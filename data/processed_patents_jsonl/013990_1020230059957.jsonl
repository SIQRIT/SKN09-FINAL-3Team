{"patent_id": "10-2023-0059957", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0162848", "출원번호": "10-2023-0059957", "발명의 명칭": "동영상 장면 검색을 위한 방법, 장치 및 저장매체", "출원인": "주식회사 엘지유플러스", "발명자": "서영민"}}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "멀티모달 딥러닝(Multimodal Deep Learning) 기반 모델을 사용하여 전체 동영상에서 장면(Scene)들을 분할하고,입력 받은 장면 묘사 문장에서 하나 이상의 주제어(Keyword) 혹은 핵심문장(Key sentence)들을 추출하고,상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하고,상기 검색된 장면들 중 하나 이상의 장면을 선택하고, 그리고검색 결과로서 하나 이상의 영상 콘텐츠를 출력하는 것을 포함하는,장면 검색 방법."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 검색된 장면들 중 하나 이상의 장면을 선택하는 것은:코사인 유사도를 이용하여 상기 검색된 장면들과 상기 추출된 하나 이상의 주제어 혹은 핵심문장 간의 유사도를계산하고, 그리고상기 유사도가 높은 순서대로 하나 이상의 장면을 선택하는 것을 포함하는,장면 검색 방법."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 출력된 하나 이상의 영상 콘텐츠는 상기 선택된 하나 이상의 장면, 혹은 이들의 조합인,장면 검색 방법."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1항에 있어서,상기 멀티모달 딥러닝 기반 모델의 학습 데이터에는 상기 전체 동영상의 이미지 정보, 음성 정보, 자막, 스크립트 및 시간축 정보가 포함되는,장면 검색 방법."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4항에 있어서,상기 멀티모달 딥러닝 기반 모델의 학습 데이터에는 쇼트(shot) 사이의 임계값이 더 포함되고,상기 쇼트 사이의 임계값은 상기 전체 동영상의 쇼트들 사이의 광 흐름(optical flow) 차이에 대한 임계값, 색상 차이에 대한 임계값, 또는 이미지 특징점 차이에 대한 임계값인,장면 검색 방법."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1항에 있어서,상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하는 것은:공개특허 10-2024-0162848-3-비디오 검색(Video retrieval) 모델을 사용하여 장면을 검색하되, 상기 비디오 검색 모델은 VLP(VideoLanguage Pre-training) 기반으로 상기 분할된 장면들이 어떤 단어 혹은 문장과 매칭되는지 학습된,장면 검색 방법."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6항에 있어서,상기 비디오 검색 모델은 광학 문자 인식 (Optical Character Recognition, OCR)을 이용하여 상기 분할된 장면들이 어떤 단어 혹은 문장과 매칭되는지 학습된,장면 검색 방법."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "장면(Scene) 검색을 위한 장치에 있어서,적어도 하나의 영상 출력부;적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서에 동작 가능하게 연결 가능한, 그리고, 실행될 때, 상기 적어도 하나의 프로세서로 하여금 동작들을 수행하도록 하는 명령(instruction)들을 저장한, 적어도 하나의 컴퓨터 메모리를 포함하며,상기 동작들은:멀티모달 딥러닝(Multimodal Deep Learning) 기반 모델을 사용하여 전체 동영상에서 장면들을 분할하고,입력 받은 장면 묘사 문장에서 하나 이상의 주제어(Keyword) 혹은 핵심문장(Key sentence)들을 추출하고,상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하고,상기 검색된 장면들 중 하나 이상의 장면을 선택하고, 그리고검색 결과로서 하나 이상의 영상 콘텐츠를 상기 적어도 하나의 영상 출력부 상에 출력하는 것을 포함하는,장면 검색 장치."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8항에 있어서,상기 검색된 장면들 중 하나 이상의 장면을 선택하는 것은:코사인 유사도를 이용하여 상기 검색된 장면들과 상기 추출된 하나 이상의 주제어 혹은 핵심문장 간의 유사도를계산하고, 그리고상기 유사도가 높은 순서대로 하나 이상의 장면을 선택하는 것을 포함하는,장면 검색 장치."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 8항에 있어서,상기 출력된 하나 이상의 영상 콘텐츠는 상기 선택된 하나 이상의 장면, 혹은 이들의 조합인,장면 검색 장치."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 8항에 있어서,상기 멀티모달 딥러닝 기반 모델의 학습 데이터에는 상기 전체 동영상의 이미지 정보, 음성 정보, 자막, 스크립트, 및 시간축 정보가 포함되는,공개특허 10-2024-0162848-4-장면 검색 장치."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11항에 있어서,상기 멀티모달 딥러닝 기반 모델의 학습 데이터에는 쇼트(shot) 사이의 임계값이 더 포함되고,상기 쇼트 사이의 임계값은 상기 전체 동영상의 쇼트들 사이의 광 흐름(optical flow) 차이에 대한 임계값, 색상 차이에 대한 임계값, 또는 이미지 특징점 차이에 대한 임계값인,장면 검색 장치."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 8항에 있어서,상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하는 것은:비디오 검색(Video retrieval) 모델을 사용하여 장면을 검색하되, 상기 비디오 검색 모델은 VLP(VideoLanguage Pre-training) 기반으로 상기 분할된 장면들이 어떤 단어 혹은 문장과 매칭되는지 학습된,장면 검색 장치."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13항에 있어서,상기 비디오 검색 모델은 광학 문자 인식 (Optical Character Recognition, OCR)을 이용하여 상기 분할된 장면들이 어떤 단어 혹은 문장과 매칭되는지 학습된,장면 검색 장치."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "컴퓨터 판독가능한 저장 매체에 있어서,상기 컴퓨터 판독가능한 저장 매체는, 적어도 하나의 프로세서에 의해 실행될 때, 상기 적어도 하나의 프로세서로 하여금 장면(Scene) 검색 장치를 위한 동작들을 수행하도록 하도록 하는 지시들을 포함하는 적어도 하나의컴퓨터 프로그램들을 저장하며, 상기 동작들은:멀티모달 딥러닝(Multimodal Deep Learning) 기반 모델을 사용하여 전체 동영상에서 장면들을 분할하고,입력 받은 장면 묘사 문장에서 하나 이상의 주제어(Keyword) 혹은 핵심문장(Key sentence)들을 추출하고,상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하고,상기 검색된 장면들 중 하나 이상의 장면을 선택하고, 그리고검색 결과로서 하나 이상의 영상 콘텐츠를 출력하는 것을 포함하는,컴퓨터 판독가능한 저장 매체."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15항에 있어서,상기 검색된 장면들 중 하나 이상의 장면을 선택하는 것은:코사인 유사도를 이용하여 상기 검색된 장면들과 상기 추출된 하나 이상의 주제어 혹은 핵심문장 간의 유사도를계산하고, 그리고상기 유사도가 높은 순서대로 하나 이상의 장면을 선택하는 것을 포함하는,공개특허 10-2024-0162848-5-컴퓨터 판독가능한 저장 매체."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 15항에 있어서,상기 멀티모달 딥러닝 기반 모델의 학습 데이터에는 상기 전체 동영상의 이미지 정보, 음성 정보, 자막, 스크립트, 및 시간축 정보가 포함되는,컴퓨터 판독가능한 저장 매체."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 17항에 있어서,상기 멀티모달 딥러닝 기반 모델의 학습 데이터에는 쇼트(shot) 사이의 임계값이 더 포함되고,상기 쇼트 사이의 임계값은 상기 전체 동영상의 쇼트들 사이의 광 흐름(optical flow) 차이에 대한 임계값, 색상 차이에 대한 임계값, 또는 이미지 특징점 차이에 대한 임계값인,컴퓨터 판독가능한 저장 매체."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 15항에 있어서,상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하는 것은,비디오 검색(Video retrieval) 모델을 사용하여 장면을 검색하되, 상기 비디오 검색 모델은 VLP(VideoLanguage Pre-training) 기반으로 상기 분할된 장면들이 어떤 단어 혹은 문장과 매칭되는지 학습된,컴퓨터 판독가능한 저장매체."}
{"patent_id": "10-2023-0059957", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 19항에 있어서,상기 비디오 검색 모델은 광학 문자 인식 (Optical Character Recognition, OCR)을 이용하여 상기 분할된 장면들이 어떤 단어 혹은 문장과 매칭되는지 학습된,컴퓨터 판독가능한 저장매체."}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "장면(Scene) 검색 방법, 이를 위한 장치 및 상기 방법을 기록한 저장매체가 제공된다. 상기 방법, 상기 장치 및 상기 저장매체는: 멀티모달 딥러닝(Multimodal Deep Learning) 기반 모델을 사용하여 전체 동영상에서 장면들을 분할하고, 입력 받은 장면 묘사 문장에서 하나 이상의 주제어(Keyword) 혹은 핵심문장(Key sentence)들을 추출하 고, 상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하고, 상기 검색된 장면들 중 하나 이상의 장면을 선택하고, 그리고 검색 결과로서 하나 이상의 영상 콘텐츠를 출력하는 것을 포함한다."}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "동영상 장면 검색을 위한 방법, 이를 위한 장치 및 저장매체에 관한 것이다."}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "비디오 검색(Video retrieval) 기술이란 동영상(Video)에서 주어진 질문 혹은 쿼리(Query)와 관련 있는 쇼트 (shot) 혹은 장면(Scene)을 찾아 다른 쇼트들 혹은 장면들과 분리해내는 것이다. 주어진 질문 혹은 쿼리와 관련 있는 키 프레임 혹은 쇼트(shot)를 찾는 것은 상대적으로 쉬운 작업(task)이지만, 문맥상 서로 의존하는 구간인 장면을 찾는 것은 어려운 작업이다. 문맥상 서로 의존하는 구간을 판단하는 기준이 사람마다 다르고 명확하게 기준을 정의하기 어렵기 때문이다. 의미 있는 구간으로 장면을 분리하기 위해 장면 검출(Scene Detect) 기술이 따로 연구되고 있다."}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "새로운 무선 인터넷 기술의 도입으로 대용량 통신이 가능해지고, 인터넷 이용이 보편화 됨에 따라, 인터넷을 이 용한 VOD(Video On Demand) 서비스 제공 및 이용이 비약적으로 증가하고 있다. VOD 서비스 사용자들은 본인이 원하는 장면만 본다거나 혹은 예고편, 하이라이트 영상 등을 보고 VOD 시청 여부 를 결정할 수 있다. VOD 서비스를 제공하는 OTT(Over The Top) 사업자들은 이러한 점을 고려하여 사용자들에게 편의를 제공하고 타사 대비 차별화된 경쟁력을 갖기 위해 다양한 사용자 취향에 맞는 하이라이트 영상, 예고편, 스틸 컷, 광고 영상 등을 제공하고자 한다. 동영상의 장면을 검색하는 방법에 있어서, 다양한 취향을 갖는 사용자들이 원하는 콘텐츠를 검색하여 얻을 수 있도록 하기 위해, 검색의 정확도를 높이기 위한 새로운 검색 방안이 요구된다. 다시 말해, 사용자들의 취향이 다양한 점, 같은 장면을 보고도 느끼는 감정 또는 해석이 다를 수 있는 점 등을 고려한 검색 방안이 요구된다. 본 명세가 해결하고자 하는 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은"}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "또 다른 기술적 과제들은 이하의 상세한 설명으로부터 본 명세와 관련된 기술분야에서 통상의 지식을 가진 자에 게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 명세의 일 양상으로, 장면(Scene)을 검색하는 방법이 제공된다. 상기 방법은: 멀티모달 딥러닝(Multimodal Deep Learning) 기반 모델을 사용하여 전체 동영상에서 장면(Scene)들을 분할하고, 입력 받은 장면 묘사 문장에 서 하나 이상의 주제어(Keyword) 혹은 핵심문장(Key sentence)들을 추출하고, 상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하고, 상기 검색된 장면들 중 하나 이상의 장면을 선택하고, 그리고 검색 결과로서 하나 이상의 영상 콘텐츠를 출력하는 것을 포함한다. 본 명세의 또 다른 양상으로, 장면(Scene)을 검색하기 위한 장치가 제공된다. 상기 장치는: 적어도 하나의 영상 출력부; 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서에 동작 가능하게 연결 가능한, 그리고, 실행 될 때, 상기 적어도 하나의 프로세서로 하여금 동작들을 수행하도록 하는 명령(instruction)들을 저장한, 적어 도 하나의 컴퓨터 메모리를 포함한다. 상기 동작들은: 멀티모달 딥러닝(Multimodal Deep Learning) 기반 모델을 사용하여 전체 동영상에서 장면들을 분할하고, 입력 받은 장면 묘사 문장에서 하나 이상의 주제어(Keyword) 혹 은 핵심문장(Key sentence)들을 추출하고, 상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하고, 상기 검색된 장면들 중 하나 이상의 장면을 선택하고, 그리고 검색 결과로서 하나 이상의 영상 콘텐 츠를 상기 적어도 하나의 영상 출력부 상에 출력하는 것을 포함한다. 본 명세의 또 다른 양상으로, 컴퓨터 판독가능한 저장 매체가 제공된다. 상기 컴퓨터 판독가능한 저장 매체는, 적어도 하나의 프로세서에 의해 실행될 때, 상기 적어도 하나의 프로세서로 하여금 장면(Scene) 검색 장치를 위 한 동작들을 수행하도록 하도록 하는 지시들을 포함하는 적어도 하나의 컴퓨터 프로그램들을 저장한다. 상기 동 작들은: 멀티모달 딥러닝(Multimodal Deep Learning) 기반 모델을 사용하여 전체 동영상에서 장면들을 분할하고, 입력 받은 장면 묘사 문장에서 하나 이상의 주제어(Keyword) 혹은 핵심문장(Key sentence)들을 추출 하고, 상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하고, 상기 검색된 장면들 중 하 나 이상의 장면을 선택하고, 그리고 검색 결과로서 하나 이상의 영상 콘텐츠를 출력하는 것을 포함한다. 본 명세의 각 양상에 있어서, 상기 검색된 장면들 중 하나 이상의 장면을 선택하는 것은: 코사인 유사도를 이용 하여 상기 검색된 장면들과 상기 추출된 하나 이상의 주제어 혹은 핵심문장 간의 유사도를 계산하고, 그리고 상 기 유사도가 높은 순서대로 하나 이상의 장면을 선택하는 것을 포함할 수 있다. 본 명세의 각 양상에 있어서, 상기 출력된 하나 이상의 영상 콘텐츠는 상기 선택된 하나 이상의 장면, 혹은 이 들의 조합일 수 있다. 본 명세의 각 양상에 있어서, 상기 멀티모달 딥러닝 기반 모델의 학습 데이터에는 상기 전체 동영상의 이미지 정보, 음성 정보, 자막, 스크립트 및 시간축 정보가 포함될 수 있다. 본 명세의 각 양상에 있어서, 상기 멀티모달 딥러닝 기반 모델의 학습 데이터에는 쇼트(shot) 사이의 임계값이 더 포함되고, 상기 쇼트 사이의 임계값은 상기 전체 동영상의 쇼트들 사이의 광 흐름(optical flow) 차이에 대 한 임계값, 색상 차이에 대한 임계값, 또는 이미지 특징점 차이에 대한 임계값일 수 있다. 본 명세의 각 양상에 있어서, 상기 추출된 하나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하는 것은: 비디오 검색(Video retrieval) 모델을 사용하여 장면을 검색하되, 상기 비디오 검색 모델은 VLP(VideoLanguage Pre-training) 기반으로 상기 분할된 장면들이 어떤 단어 혹은 문장과 매칭되는지 학습된 모델일 수 있다. 본 명세의 각 양상에 있어서, 상기 비디오 검색 모델은 광학 문자 인식 (Optical Character Recognition, OC R)을 이용하여 상기 분할된 장면들이 어떤 단어 혹은 문장과 매칭되는지 학습된 모델일 수 있다."}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 명세의 구현(들)에 의하면, 문맥상 일치하는 구간에 대한 자연스러운 장면(Scene) 분할이 가능하다. 본 명세의 구현(들)에 의하면, 정확도 높은 장면 탐지기(Scene detector)를 구축할 수 있다. 본 명세의 구현(들)에 의하면, 동영상 장면 검색의 정확도를 높일 수 있다. 본 명세에 따른 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급되지 않은 또 다른 효과는 이하의 상세"}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "한 설명으로부터 본 명세와 관련된 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 명세에 따른 구현들을 첨부된 도면을 참조하여 상세하게 설명한다. 첨부된 도면과 함께 이하에 개시될 상세한 설명은 본 명세의 예시적인 구현을 설명하고자 하는 것이고, 본 명세가 실시될 수 있는 유일한 구현 형 태를 나타내고자 하는 것이 아니다. 이하의 상세한 설명은 본 명세의 완전한 이해를 제공하기 위해서 구체적 세 부사항을 포함한다. 그러나 당업자는 본 명세가 이러한 구체적 세부사항 없이도 실시될 수 있음을 안다. 몇몇 경우, 본 명세의 개념이 모호해지는 것을 피하기 위해 공지의 구조 및 장치는 생략되거나, 각 구조 및 장 치의 핵심기능을 중심으로 한 블록도 형식으로 도시될 수 있다. 본 명세서 전체에서 동일한 구성요소에 대해서 는 동일한 도면 부호를 사용하여 설명한다. 이하의 설명 및 도면에서 사용된 용어나 단어는 통상적이거나 사전적 의미로 한정해서 해석되어서는 아니 되며, 발명자는 그 자신의 발명을 가장 최선의 방법으로 설명하기 위한 용어의 개념으로 적절하게 정의할 수 있다는 원칙에 입각하여 본 명세의 기술적 사상에 부합하는 의미와 개념으로 해석되어야 한다. 따라서 본 명세에 기재 된 예시와 도면에 도시된 구성은 본 명세의 가장 바람직한 일 실시 예에 불과할 뿐이고, 본 명세의 기술적 사상 을 모두 대변하는 것은 아니므로, 본 출원 시점에 있어서 이들을 대체할 수 있는 다양한 균등물과 변형 예들이 있을 수 있음을 이해하여야 한다. 또한, 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발 명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미 를 가진 것으로 해석되어야 하며, 본 명세에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의 미로 해석되지 않는다. 본 명세에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세에서 기술되 는 “포함한다” 또는 “가지다” 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성 요소, 부품 또 는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성 요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야한다. 본 명세에서 쇼트(shot)이라 함은 동영상(예, 영화 등)의 시간단위를 한정 짓는 표현 단위로, 카메라가 작동하 기 시작해서 끝날 때까지의 시간 동안에 빛에 노출되어 촬영된 필름의 길이이다. 본 명세에서 장면(Scene)이라 함은 동영상(예, 영화 등)의 시간단위를 한정 짓는 표현 단위로, 하나 이상의 쇼 트(들)의 집합이다. 도 1은 장면(Scene)을 검색하기 위한 방법의 흐름을 예시한 것이다. 도 1에 도시된 바와 같이, 전체 동영상(Video)에서 장면(Scene)들을 분할한다(S101). 이때, 장면 분할은 문맥상 서로 의존하는 구간에 대하여 이루어 진다. 구체적인 장면 분할 방법은 도 2 및 도 3과 함께 살핀다. 다음으로, 사용자 혹은 Chat GPT(Chat Generative Pre-trained Transformer) 등으로부터 입력 받은 문장에서 하나 이상의 주제어(Keyword)(들) 혹은 핵심 문장(Key Sentence)(들)을 추출한다(S102). 다음으로, 추출된 하나 이상의 주제어 혹은 핵심 문장과 일치하는 장면을 검색하고(S103), 검색된 장면(들) 중 하나 이상의 장면(들)을 선택한다(S104). 이때, 코사인 유사도를 이용하여 상기 검색된 장면(들)과 상기 추출된 하나 이상의 주제어(들) 혹은 핵심 문장(들) 간의 유사도를 계산하고, 상기 유사도가 높은 순서대로 하나 이상 의 장면(들)을 선택할 수 있다. 예를 들어, S104에서 코사인 유사도를 이용해 입력 받은 문장과 유사도가 높은 순서대로 5순위 내에 드는 장면들을 선택해 S105에서 출력할 수 있다. 또 다른 예로, 사용자 선택에 따라 2순위 혹은 3순위 내에 드는 장면들을 선택할 수도 있다. 한편, 사용자가 선택하는 경우를 제외한 일반적인 경우에는 유사도가 높은 1순위 장면을 선택하도록 자동화할 수 있다. 코사인 유사도는 두 벡터 간의 코사인 각도를 이용 하여 구할 수 있는 두 벡터 간의 유사도를 의미한다. 코사인 유사도는 두 벡터의 방향이 동일한 경우에 1의 값 을 가지고, 두 벡터가 90도의 각을 이루면 0의 값을 가지고, 두 벡터가 서로 반대의 방향을 가지면 -1의 값을 갖는다. 다시 말해, 코사인 유사도는 -1 이상 1 이하의 값을 가지며 1에 가까울수록 유사도가 높다고 판단할 수 있다. 두 벡터 A, B에 대한 코사인 유사도는 수학식 1 과 같이 표현할 수 있다. 수학식 1"}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "예를 들어, 상기 추출된 하나 이상의 주제어(들) 혹은 핵심 문장(들)을에 포함된 단어들을 벡터로 하여 상기 검 색된 장면(들)과 상기 추출된 하나 이상의 주제어(들) 혹은 핵심 문장(들) 간의 유사도를 계산할 수 있다. 벡터 는 비디오 검색(Video retrieval) 모델의 학습 데이터에 포함되는 단어들일 수도 있다. 상기 비디오 검색 모델 은 S101에서 분할된 장면들이 어떤 단어(들) 혹은 문장(들)과 매칭되는지 학습된 VLP(Video Language Pre- training) 기반의 모델로 이를 이용해 장면을 검색할 수 있다. 다음으로, 검색 결과로서 하나 이상의 영상 콘텐츠를 출력한다(S105). 하나 이상의 영상 콘텐츠는 S104에서 선 택된 하나 이상의 장면(들), 혹은 이들의 조합일 수 있다. 예를 들어, S104에서 코사인 유사도를 이용해 입력 받은 문장과 관련 깊은 장면들 중 5순위 내에 드는 장면들을 선택한 경우 상기 출력된 영상 콘텐츠는 5개의 장 면들 혹은 이들의 조합일 수 있다. 또 다른 예로, S104에서 사용자 선택에 따라 2순위 혹은 3순위 내에 드는 장 면들을 선택한 경우 2개 또는 3개의 장면들 혹은 이들의 조합일 수 있다. 한편, 사용자가 선택하는 경우를 제외 한 일반적인 경우에는 유사도가 높은 1순위 장면을 선택하여 출력할 수도 있다. 본 명세에서, 컴퓨터 판독가능한(readable) 저장(storage) 매체(medium)은 적어도 하나의 지시 또는 컴퓨터 프 로그램을 저장할 수 있으며, 상기 적어도 하나의 지시 또는 컴퓨터 프로그램은 적어도 하나의 프로세서에 의해 실행될 때 상기 적어도 하나의 프로세서로 하여금 본 명세의 몇몇 실시예들 또는 구현들에 따른 동작들을 수행 하도록 할 수 있다. 본 명세에서, 프로세싱 기기(device) 또는 장치(apparatus)는 적어도 하나의 프로세서와 상기 적어도 하나의 프 로세서에 연결 가능한 적어도 하나의 컴퓨터 메모리를 포함할 수 있다. 상기 적어도 하나의 컴퓨터 메모리는 지 시들 또는 프로그램들을 저장할 수 있으며, 상기 지시들 또는 프로그램들은, 실행될 때, 상기 적어도 하나의 메 모리에 작동가능하게(operably) 연결되는 적어도 하나의 프로세서로 하여금 본 명세의 몇몇 실시예들 또는 구현들에 따른 동작들을 수행하도록 할 수 있다. 이하에서, 구체적인 장면 분할 방법과 장면 검색 방법에 대하여 살펴보겠다. 도 2는 사용자로부터 입력 받은 장면 묘사 문장을 이용해 장면을 검색하는 방법을 예시한 것이다. 1. 장면(Scene) 분할 장면 분할이란 원본 동영상에서 서로 문맥상 의존하는 구간들을 분할하는 것이다. 이 단계는 도 1의 S101과 대 응된다. 원본 동영상의 자막 혹은 스크립트를 기반으로 장면들의 내용을 파악하여 장면들을 분할할 수 있다. 자 막 혹은 스크립트를 기준으로 장면들을 분할하는 경우, 문맥이 이어지는 구간으로 분할될 수 있으나 부자연스러 운 분할이 이루어질 수 있다. 다시 말해, 자막 혹은 스크립트를 기준으로 장면 분할을 하면 장면이 전환될 때의 색 변화값을 고려하지 못해 분할 사이에 존재하는 쇼트(shot)전환이 부자연스러울 수 있다. 나아가 분할된 장면 이 문맥이 유지된다는 보장도 없다. 본 명세에서는 원본 동영상의 자막, 스크립트뿐만 아니라 이미지 혹은 프레임(Frame), 시간축에 대한 정보, 또 는 쇼트 사이 임계값 등을 고려해 자연스러운 장면 분할이 가능하다. 장면 분할을 위해 멀티모달 딥러닝(Multimodal Deep Learning) 기반 모델인 장면 추출기(Scene detector)를 사 용한다. 멀티모달 딥러닝 기반 모델이란 인간의 인지적 학습법을 모방하여 다양한 형태(Multimodal)의 데이터로 학습된 딥러닝 모델을 의미한다. 상기 멀티모달 딥러닝 기반 모델의 학습 데이터에는 상기 전체 동영상의 이미 지 정보, 음성 정보, 자막, 스크립트, 시간축 정보, 쇼트 사이 임계값 등이 포함될 수 있다. 시간축 정보란 연 속된 이미지들의 집합인 동영상(Video)에서 상기 이미지들이 어떻게 바뀌는 지에 대한 정보이다. 예를 들어, 시 간축 정보에는 등장 인물들의 모션 정보, 시간축에 따른 화소값 정보 등이 있을 수 있다. 쇼트 사이의 임계값은 상기 전체 동영상의 쇼트들 사이의 광 흐름(optical flow) 차이에 대한 임계값, 색상 차이(예, RGB)에 대한 임 계값, 또는 이미지 특징점 차이에 대한 임계값일 수 있다. 이미지 특징점은 코너 검출(corner detection)을 바 탕으로 검출할 수 있다. 2. 사용자 입력 문장을 이용한 장면 검색 예를 들어, '나홀로 집에서'라는 영화에서 특정 장면만 보고 싶은 고객이 있다고 가정하자. 그 고객은 “케빈이 도둑을 쫓기 위해 함정을 설치하는 장면”을 검색어로 입력할 수 있다. 입력 받은 문장에서 주제어 혹은 핵심문 장을 추출한다. 입력 받은 문장에서 주제어 혹은 핵심문장을 추출하는 것을 통해 장면 검색의 정확도를 높일 수 있다. 입력 받은 문장에는 메인 이벤트에 해당하는 장면 묘사 부분과 서브 이벤트에 해당하는 장면 묘사 부분이 있을 수 있기 때문이다. 위 단계들은 도 1의 S102과 대응된다. 구체적인 예시로, 나홀로 집에서 케빈이 도둑을 쫓기 위해 함정을 설치하는 것이 메인 이벤트이고, 페인트 통으로 함정을 설치하는 장면, 도둑이 페인트 통으로 맞는 장면, 미끄러운 계단 등은 서브 이벤트에 해당하므로 입력 받은 문장에서 메인 이벤트에 대한 묘사 부분만 추출할 수 있다. 다음으로, 상기 추출된 주제어 혹은 핵심문장과 일치하는 장면을 검색한다. 이 단계는 도 1의 S103와 대응된다. 이때, 비디오 검색(Video retrieval) 모델을 이용해 장면을 검색할 수 있다. 상기 비디오 검색 모델은 분할된 장면들이 어떤 단어(들) 혹은 문장(들)과 매칭되는지 학습된 VLP(Video Language Pre-training) 기반의 모델이 다. 상기 비디오 검색 모델은 각 장면의 자막, 스크립트, 이미지 특징, 음성 정보, 등장 물체, 등장 인물 등을 기반으로 어떤 단어(들) 혹은 문장(들)과 매칭되는지 학습된 모델일 수 있다. 나아가, 상기 비디오 검색 모델은 광학 문자 인식 (Optical Character Recognition, OCR)을 이용하여 상기 분 할된 장면들이 어떤 단어 혹은 문장과 매칭되는지 학습된 모델일 수 있다. 예를 들어, 동영상에 특정 간판이 등 장하면 그 간판에 쓰여있는 문자를 인식하고, 그 간판이 등장하는 장면을 그 문자와 매칭되는 장면이라고 볼 수 있다. 또 다른 예로, 동영상 자막을 인식할 수 도 있다. 이를 통해 장면 검색의 정확도를 높일 수 있다. 장면 검색 시 스크립트, 자막에 나타난 단어와 검색어가 매칭되는지 여부에 따라 검색 결과로서 추출할 장면들 을 우선적으로 고려하게 되면 동영상에 정확하게 명시된 단어와 검색어가 매칭되는 경우에만 검색 성공으로 이 어질 수 있다. 본 명세에서는 자막, 스크립트뿐만 아니라 이미지 특징, 음성 정보, 등장 물체 또는 등장 인물, 문자 인식 등을 이용하여 장면을 검색하므로, 장면 검색의 정확도를 높일 수 있다. 다음으로, 검색된 장면(들) 중 하나 이상의 장면(들)을 선택하고, 검색 결과로서 하나 이상의 영상 콘텐츠를 출 력한다. 이 단계들은 도 1의 S104 및 S105과 대응된다. 상기 영상 콘텐츠는 선택된 하나 이상의 장면(들), 혹은 이들의 조합일 수 있다. 예를 들어, 나홀로 집에서 케빈이 도둑을 쫓기 위해 함정을 설치하는 장면들을 모은 숏 폼(short-form)이 출력될 수 있다. 이하에서, VLP(Video Language Pre-training)에 대해 구체적으로 살펴보겠다. VLP란 동영상(video) 데이터를 이용하여 자연어처리(Natural Language Processing, NLP)를 하기 위한 사전학습 기술이다. VLP 모델은 시각적인 정보와 자연어 정보를 연결하여 특정 이미지 혹은 동영상이 어떤 자연어와 대응 되는지 학습된 모델로, 텍스트 데이터만을 이용한 NLP 기반 모델보다 섬세한 자연어 이해와 생성이 가능하다. VLP 모델은 시각적 특징 및 문자 특징을 추출한다. 즉, VLP 모델은 동영상의 각 프레임 특징들을 추출한다. 추 출될 수 있는 특징(들)에는 대표적으로 CNN-based Grid Features(CNN-GFs), ViT-based Patch Features (ViT- PFs) 등이 있다. 그리고, VLP 모델은 입력 문장으로부터 문자 특징들을 추출한다. 이를 위해, BERT, RoBERTa, AlBERT, and XLNet 모델 등 사전 훈련된 언어 모델을 응용 또는 활용할 수 있다. VLP 모델의 구조는 2가지 관점 에서 살펴볼 수 있다. 단일 스트림 구조인지 또는 이중 스트림 구조인지 여부와, 인코더만 포함된 구조인지 또 는 인코더와 디코더가 모두 포함된 구조인지 여부에 의해 정해질 수 있다. VLP 모델이 단일 스트림 구조인 경우, 상기 추출한 시각적 특징들과 문자 특징들을 융합하여 인코딩을 위한 단일 변환기 블록으로 전송한다. VLP 모델이 이중 스트림 구조인 경우, 상기 추출한 시작적 특징들과 추출한 문자 특징들을 융합하지 않고 인코 딩을 위한 서로 다른 변환기 블록으로 각각 전송한다. VLP 모델이 인코더만 포함된 구조인 경우, 크로스 모달 표현형(cross-modal representation)들이 최종 출력(output)을 위한 출력 레이어에 직접 전송된다. 반면에, VLP 모델이 인코더와 디코더가 모두 포함된 구조인 경우, 크로스 모달 표현형들은 디코더를 거쳐 출력 레이어에 전송된다. VLP 모델을 사전 훈련하는 방법들로는 Masked language modeling (MLM), Prefix Language Modeling (PrefixLM) masked vision modeling (MVM), Vision-Language Matching (VLM), Vision-Language Contrastive Learning (VLC), Frame Order Modeling (FOM) 등이 있다. 도 3은 Chat GPT(Generative Pre-trained Transformer)를 이용해 장면을 검색하는 방법을 예시한 것이다. 도 3에 도시된 바와 같이, 도 3의 1. 장면 분할은 도 2의 1. 장면 분할 및 도 1의 S101과 대응된다. 1. 장면 분할 긴 동영상에서 특정 장면을 검색하여 출력하기 위해, 먼저 문맥이 이어지는 구간 별로 분할이 필요하다. 이 단 계는 도 2의 1. 장면 분할 및 도 1의 S101과 대응된다. 2. Chat GPT를 이용한 장면 검색 Chat GPT(Generative Pre-trained Transformer)는 프로토타입 대화형 인공지능 챗봇으로, 본 명세에 따른 구현 들은 Chat GPT를 API(Application Programming Interface)로 활용하여 사용자 혹은 고객에게 장면을 추천해줄 수도 있다. 이 경우에는 사용자가 직접 검색하는 경우와 달리, 사용자 혹은 고객이 어떤 장면이라고 명확하게 명시하지 않더라도 해당 장면을 추측하여 알려줄 수 있고, 사용자 혹은 고객의 세그먼트(segment) 별로 선추천 장면 제공이 가능하며, 사용자 혹은 고객의 시청 이력을 바탕으로 시청 이력이 있는 콘텐츠와 유사한 분위기의 장면이 있는 콘텐츠 추천이 가능하다. 빅데이터와 함께 사용자 혹은 고객의 개인화된 정보 제공이 가능하므로 맞춤형 예고편, 하이라이트 영상 등을 추출할 수 있다. 예를 들어, 시청할 영화를 선택하기 전에 영화의 하이라이트를 먼저 보고 싶은 고객이 있다고 가정하자. 그 고 객은 “나홀로 집에서 20대 남자가 좋아할만한 장면이 뭐야?”라고 질문할 수 있다. 이에 대해, Chat GPT는 해 당 영화에서 20대 남자가 좋아할만한 장면을 묘사하는 문장으로 “케빈이 부비트랩을 설치하는 장면, 케빈이 신 용카드를 쓰는 장면” 등으로 대답할 수 있다. Chat GPT의 답변에서 주제어 혹은 핵심문장을 추출한다. 다음으 로, 상기 추출된 주제어 혹은 핵심문장과 일치하는 장면을 검색한다. 그리고, 검색된 장면(들) 중 하나 이상의 장면(들)을 선택하고, 검색 결과로서 하나 이상의 영상 콘텐츠를 출력한다. 이는 도 1의 S102 내지 S105과 동일 하다. 도 4는 Chat GPT에 질문하기 위한 프롬프트 입력 예시를 도시한 것이다. 도 5는 입력 받은 문장에서 주제어(Keyword) 또는 핵심문장(Key sentence) 추출의 예를 도시한 것이다. 도 4에 도시된 바와 같이, “나홀로집에서 재미있는 장면과 감정별로 인상적인 장면, 고객 segmentation 별로 재미있어할 장면을 알려줘”라고 Chat GPT에 질문을 해서 장면을 추천 받는 경우, 도 5에 도시된 바와 같이 해당 장면을 묘사하는 문장뿐만 아니라, 개인적인 생각 혹은 감정에 대한 묘사도 함께 출력 될 수 있다. 사용자로 부터 직접 장면을 묘사하는 문장을 입력 받아 장면을 검색하는 경우에도 사용자가 어떤 화법으로 말하는 가에 따라 동일한 장면을 묘사하는 방법이 다를 수 있고, 미사여구가 많을 수 있다. 따라서 추후 장면 검색의 정확도 를 높이기 위해 검색에 방해가 되는 부분을 제외하는 것이 필요하다. 본 명세에서는, 이것들을 제외한 구, 절 혹은 문장을 장면 검색에 필요한 주제어 혹은 핵심문장이라고 한다. 예를 들어, 도 5에 도시된 바와 같이, 고객의 질문에 대한 chat GPT 대답에서 “케빈이 가족과 재회하는 영화의 마지막에 나오는 훈훈한 장면”, “케빈의 엄마가 파리에 고립된 후 마침내 아들에게 돌아가는 장면”을 주제어 혹은 핵심문장으로 추출할 수 있다. 그리고 본 명세에 구현된 방법에 따라, 추출한 주제어 혹은 핵심문장과 일 치하는 장면을 검색, 선택 및 추출할 수 있다. 본 명세에서는 주제어 혹은 핵심문장 추출하여 이를 기반으로 장면 검색을 하므로, 복잡한 문장을 입력하여도 메인 이벤트 혹은 핵심 장면에 대한 묘사만 상기 비디오 검색 모델에 입력 되도록 처리하여 장면 검색이 가능하 다. 예를 들어, 장면 검색을 위한 입력 문장으로 동영상 전체의 설명(description)을 입력하여도 동영상의 하이 라이트 장면을 추출할 수 있다. 도 6은 본 명세에 따른 장면 검색 방법을 수행하기 위한 장면 검색 장치의 예를 도시한 것이다. 도 6에 도시된 바와 같이, 장면 검색 장치는 멀티모달 딥러닝(Multimodal Deep Learning) 기반 모델을 사 용하여 전체 동영상에서 장면(Scene)들을 분할하기 위한 장면 분할부, 입력 받은 장면 묘사 문장에서 하나 이상의 주제어(Keyword) 혹은 핵심문장(Key sentence)들을 추출하기 위한 주제어 추출부, 상기 추출된 하 나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하는 장면 검색부, 상기 검색된 장면들 중 하나 이상의 장면을 선택하는 장면 선택부, 검색 결과로서 하나 이상의 영상 콘텐츠를 출력하는 영상 출력부 를 포함할 수 있다. 장면 검색 장치는 본 명세의 구현에 따른 방법을 수행하여 장면을 검색할 수 있 다. 장면 검색 장치는 적어도 하나의 영상 출력부, 적어도 하나의 프로세서 및 상기 적어도 하나의 프로세서에 동작 가능하게 연결 가능한, 그리고, 실행될 때 상기 적어도 하나의 프로세서로 하여금 동작들을 수행하도록 하 는 명령(instruction)들을 저장한, 적어도 하나의 컴퓨터 메모리를 포함할 수 있다. 상기 동작들은 멀티모달 딥 러닝(Multimodal Deep Learning) 기반 모델을 사용하여 전체 동영상에서 장면(Scene)들을 분할하고, 입력 받은 장면 묘사 문장에서 하나 이상의 주제어(Keyword) 혹은 핵심문장(Key sentence)들을 추출하고, 상기 추출된 하 나 이상의 주제어 혹은 핵심문장과 일치하는 장면을 검색하고, 상기 검색된 장면들 중 하나 이상의 장면을 선택 하는 장면 선택하고, 검색 결과로서 하나 이상의 영상 콘텐츠를 상기 적어도 하나의 영상 출력부 상에 출력하는 것을 포함할 수 있다. 상기 장면 검색 장치는 사용자로부터 장면 묘사 문장을 입력 받기 위한 사용자 입력 부를 더 포함할 수 있다. 본 명세의 몇몇 구현들에 의하면, OTT(Over The Top)서비스 또는 일반 영상에서 사용자 맞춤형 예고편 및 하이 라이트 영상 추출 서비스 제공이 가능하다. 본 명세의 몇몇 구현들에 의하면, 문맥상 일치하는 구간에 대한 자연스러운 장면(Scene) 분할이 가능하다. 본 명세의 몇몇 구현들에 의하면, 정확도 높은 장면 탐지기(Scene detector)를 구축할 수 있다. 본 명세의 몇몇 구현들에 의하면, 동영상 장면 검색의 정확도를 높일 수 있다."}
{"patent_id": "10-2023-0059957", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "상술한 바와 같이 개시된 본 명세의 예들은 본 명세와 관련된 기술분야의 통상의 기술자가 본 명세를 구현하고 실시할 수 있도록 제공되었다. 상기에서는 본 명세의 예들을 참조하여 설명하였지만, 해당 기술 분야의 통상의 기술자는 본 명세의 예들을 다양하게 수정 및 변경시킬 수 있다. 따라서, 본 명세는 여기에 기재된 예들에 제한 되려는 것이 아니라, 여기서 개시된 원리들 및 신규한 특징들과 일치하는 최광의 범위를 부여하려는 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6"}
{"patent_id": "10-2023-0059957", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 장면(Scene)을 검색하기 위한 방법의 흐름을 예시한 것이다. 도 2는 사용자로부터 입력 받은 장면 묘사 문장을 이용해 장면을 검색하는 방법을 예시한 것이다. 도 3은 Chat GPT(Generative Pre-trained Transformer)를 이용해 장면을 검색하는 방법을 예시한 것이다. 도 4는 Chat GPT에 질문하기 위한 프롬프트 입력 예시를 도시한 것이다. 도 5는 입력 받은 문장에서 주제어(Keyword) 또는 핵심문장(Key sentence) 추출의 예를 도시한 것이다. 도 6은 본 명세에 따른 장면 검색 방법을 수행하기 위한 장면 검색 장치의 예를 도시한 것이다."}
