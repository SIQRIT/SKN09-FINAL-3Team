{"patent_id": "10-2023-0098654", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0017813", "출원번호": "10-2023-0098654", "발명의 명칭": "음성 대화 내용에 따른 인공지능 기반의 가상인간 캐릭터 표정 및 제스처 표현 시스템", "출원인": "주식회사 아임클라우드", "발명자": "이두식"}}
{"patent_id": "10-2023-0098654", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "캐릭터의 표정과 제스처를 표현하는 애니메이션을 생성하는 방법으로서, 임의의 문장을 포함하는 음성신호를 텍스트 데이터로 변환하고 구문 분석하여 상기 문장을 포함하는 구문 데이터를 생성하는 단계; 상기 구문 데이터를 상황인지를 위한 딥러닝 알고리즘에 입력하여 구문 데이터에 대응하는 상황 데이터를 출력하는 단계; 상기 구문 데이터와 상황 데이터를 표정 및 제스처 생성 알고리즘에 입력 데이터로서 입력하고 상기 구문과 상황에 대응하는 얼굴 표정과 제스처를 출력하는 단계; 및 가상인간 캐릭터를 선택하고 선택된 가상인간 캐릭터에 상기 출력된 얼굴 표정과 제스처를 렌더링하여 출력하는단계;를 포함하는 것을 특징으로 하는, 가상인간 캐릭터의 표정과 제스처를 표현하는 애니메이션을 생성하는 방법."}
{"patent_id": "10-2023-0098654", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 표정 및 제스처 생성 알고리즘에서 출력되는 상기 제스처는 캐릭터의 동작을 표현하는 애니메이션이고, 상기 표정 및 제스처 생성 알고리즘에서 출력되는 상기 얼굴 표정은 캐릭터의 얼굴 표정을 표현하는 애니메이션인 것을 특징으로 하는, 가상인간 캐릭터의 표정과 제스처를 표현하는 애니메이션을 생성하는 방법."}
{"patent_id": "10-2023-0098654", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 또는 제2항에 따른 가상인간 캐릭터의 얼굴 표정과 제스처를 표현하는 애니메이션의 생성 방법을 실행시키기 위한 컴퓨터 프로그램이 기록된 컴퓨터 판독가능 기록매체."}
{"patent_id": "10-2023-0098654", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "캐릭터의 표정과 제스처를 표현하는 애니메이션을 생성하는 방법으로서, 임의의 문장을 포함하는 음성신호를 텍 스트 데이터로 변환하고 구문 분석하여 상기 문장을 포함하는 구문 데이터를 생성하는 단계; 상기 구문 데이터를 상황인지를 위한 딥러닝 알고리즘에 입력하여 구문 데이터에 대응하는 상황 데이터를 출력하는 단계; 상기 구문 데이터와 상황 데이터를 표정 및 제스처 생성 알고리즘에 입력 데이터로서 입력하고 상기 구문과 상황에 대응하 는 얼굴 표정과 제스처를 출력하는 단계; 및 가상인간 캐릭터를 선택하고 선택된 가상인간 캐릭터에 상기 출력된 얼굴 표정과 제스처를 렌더링하여 출력하는 단계;를 포함하는, 가상인간 캐릭터의 표정과 제스처를 표현하는 애 니메이션을 생성하는 방법을 개시한다."}
{"patent_id": "10-2023-0098654", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 컴퓨터를 이용한 캐릭터 애니메이션 생성 시스템에 관한 것으로, 보다 상세하게는, 음성대화의 내용 으로부터 인공지능 기반 알고리즘을 이용하여 상황을 인지하고 이에 따라 가상인간 캐릭터의 표정 및 제스처를 표현할 수 있는 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0098654", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "애니메이션(animation)에 포함되는 디지털 캐릭터(Digital Character)는 다양한 분야에 응용되며, 예를 들면 연 극 배우의 모션을 감지하고 이를 디지털 캐릭터로 애니메이션화 하기도 한다. 다른 예로는, 디지털 캐릭터는 또한 음향에 동기되어 동영상으로 재생되기도 한다. 예를 들면, 한국 공개특허 제2006-0054678호(2006년 5월 23일)에는 음향에 캐릭터를 동기화시키는 기술이 개시되어 있다."}
{"patent_id": "10-2023-0098654", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 음성 대화 내용을 텍스트로 변환하여 현재의 대화 내용이 어떤 상황인지는 인지 하는 상황인지 알고리즘을 제시한다. 본 발명에 따르면 음성대화를 텍스트로 변환한 후 자연어 처리 분석을 통해 구문을 분석하여 구문 데이터를 생 성하고, 또한 상황인지 알고리즘을 통해 상황 데이터를 생성한 후, 구문 데이터와 상황 데이터로부터 가상인간 캐릭터에 렌더링할 얼굴 표정과 제스처를 인공지능 알고리즘을 통해 산출하고 이를 가상인간 캐릭터에 렌더링하 여 가상인간의 표정과 제스처를 출력하는 방법을 개시한다. 일 실시예에서 얼굴 표정과 제스처는 각각 최소 100개 이상의 표정과 제스처 종류로 분류하고, 음성대화 내용에 따른 상황인식을 인공지능 알고리즘의 입력데이터로 입력하여 상기 기분류된 다양한 얼굴 표정과 제스처 중 가 장 적절한 표정과 제스처를 출력하도록 하고 이를 가상인간 캐릭터에 렌더링하여 애니메이션으로 디스플레이 한 다. 본 발명의 일 실시예에 따르면, 상기와 같은 인공지능 기반의 가상인간 캐릭터 표정 및 제스처 표현 방법을 실 행하기기 위한 컴퓨터 프로그램이 기록된 컴퓨터 판독가능 기록매체를 개시한다."}
{"patent_id": "10-2023-0098654", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이상의 본 발명의 목적들, 다른 목적들, 특징들 및 이점들은 첨부된 도면과 관련된 이하의 바람직한 실시예들을 통해서 쉽게 이해될 것이다. 그러나 본 발명은 여기서 설명되는 실시예들에 한정되지 않고 다른 형태로 구체화 될 수도 있다. 오히려, 여기서 소개되는 실시예들은 개시된 내용이 철저하고 완전해질 수 있도록 그리고 당업자 에게 본 발명의 사상이 충분히 전달될 수 있도록 하기 위해 제공되는 것이다. 본 명세서에서 용어 '소프트웨어'는 컴퓨터에서 하드웨어를 움직이는 기술을 의미하고, 용어 '하드웨어'는 컴퓨 터를 구성하는 유형의 장치나 기기(CPU, 메모리, 입력 장치, 출력 장치, 주변 장치 등)를 의미하고, 용어 '단계'는 소정의 목을 달성하기 위해 시계열로 연결된 일련의 처리 또는 조작을 의미하고, 용어 '컴퓨터 프로그 램', '프로그램', 또는 '알고리즘'은 컴퓨터로 처리하기에 합한 명령의 집합을 의미하고, 용어 '프로그램 기록 매체'는 프로그램을 설치하고 실행하거나 유통하기 위해 사용되는 프로그램을 기록한 컴퓨터로 읽을 수 있는 기 록매체를 의미한다. 본 명세서에서 발명의 구성요소를 지칭하기 위해 사용된 '~부', '~모듈', '~유닛', '~블록', '보드' 등의 용어 는 적어도 하나의 기능이나 동작을 처리하는 물리적, 기능적, 또는 논리적 단위를 의미할 수 있고 이는 하나 이 상의 하드웨어나 소프트웨어 또는 펌웨어로 구현되거나 또는 하나 이상의 하드웨어, 소프트웨어, 및/또는 펌웨 어의 결합으로 구현될 수 있다. 본 명세서에서 '처리장치', '컴퓨터', '컴퓨팅 장치', '서버 장치', '서버'는 윈도우, 맥, 또는 리눅스와 같은 운영체제, 컴퓨터 프로세서, 메모리, 응용프로그램들, 기억장치(예를 들면, HDD, SDD), 및 모니터를 구비한 장 치로 구현될 수 있다. 컴퓨터는 예를 들면, 데스크톱 컴퓨터나 노트북, 모바일 단말기 등과 같은 장치일 수 있 으나 이들은 예시적인 것이며 이에 한정되는 것은 아니다. 모바일 단말기는 스마트폰, 태블릿 PC, 또는 PDA와 같은 모바일 무선통신기기 중 하나일 수 있다. 애니메이션산업 진흥에 관한 법률에 따르면 애니메이션은 \"실물의 세계 또는 상상의 세계에 존재하는 스스로 움 직이지 않는 피사체를 2D, 3D, CG, 스톱모션 등 다양한 기법과 매체를 이용하여 가공함으로써 움직이는 이미지 로 창출하는 영상\"이라고 정의하고 있다. 본 명세서에서 \"애니메이션\"은 이러한 정의를 포함할 뿐만 아니라 실 존하는 캐릭터 또는 가상인간 캐릭터의 몸체의 적어도 일부가 움직이는 동영상을 포괄하여 의미하는 것으로 해 석한다. 또한 본 명세서에서 '애니메이션'은 '애니메이션 동영상', '애니메이션 영상', '동영상', '영상' 등으 로 칭하기도 한다. 본 명세서에 언급되는 '캐릭터'는 애니메이션 영상에 나타나는 실제의 또는 가공의 인물이나 동물 등 시각적 상 징물을 포괄하여 지칭하는 것이며, 예컨대 가공의 캐릭터로는 각종 만화 등의 주인공이나 아바타 등을 의미할 수 있다. 본 명세서에서 언급되는 캐릭터의 '제스처'는 애니메이션 내에서 캐릭터의 몸짓, 손짓 등 몸체의 적어도 일부의 움직임이나 동작을 의미한다. 제스처는 단순히 캐릭터 일부 몸체의 움직임을 의미할 수도 있고 캐릭터의 내면의 심리적 상태의 표현이거나 어떤 대상에 대한 신호일 수도 있다. 본 명세서에서는 특별한 제한이 없는 한 제스처 를 '움직임'이나 '동작'과 동일한 의미로 사용하기로 한다. 또한 본 명세서에서 때로는 '제스처'가 캐릭터의 얼굴 표정의 움직임이나 캐릭터 얼굴의 눈, 코, 입 등 얼굴의 적어도 일부분의 움직임까지 포괄하여 의미할 수도 있다. 이하 도면을 참조하여 본 발명을 상세히 설명하도록 한다. 아래의 특정 실시예들을 기술하는데 있어서 여러 가 지의 특정적인 내용들은 발명을 더 구체적으로 설명하고 이해를 돕기 위해 작성되었다. 하지만 본 발명을 이해 할 수 있을 정도로 이 분야의 지식을 갖고 있는 독자는 이러한 여러 가지의 특정적인 내용들이 없어도 사용될 수 있다는 것을 인지할 수 있다. 또한 발명을 기술하는 데 있어서 공지 또는 주지관용 기술이면서 발명과 크게 관련 없는 부분들은 본 발명을 설명하는 데 있어 혼돈을 막기 위해 기술하지 않음을 미리 언급해 둔다. 도1은 본 발명의 일 실시예에 따른 인공지능 기반 가상인간 캐릭터의 표정 및 제스처 표현 시스템을 설명하는 도면이다. 도1을 참조하면, 일 실시예에 따른 표정 및 제스처 표현 시스템은 음성 분석모듈 및 렌더링 모듈 을 포함한다. 또한 시스템은 상황분류 데이터, 가상인간 캐릭터 데이터, 및 제스처 및 표 정 데이터를 저장하는 데이터베이스를 더 포함할 수 있다. 음성 분석모듈(이하 간단히 '분석모듈'이라고도 함)은 사용자가 발화하는 음성을 수신하여 음성을 분석하 여 구문 데이터와 상황 데이터를 생성할 수 있다. 일 실시예에서 분석모듈은 STT(speech-to-text) 분석부, 구문 분석부, 및 상황인지 알고리즘 을 포함한다. 분석모듈은 사용자가 발화하는 음성신호를 수신하기 위해 마이크와 같은 음성수신 장치를 구비할 수 있다. 또한 일 실시예에서 사용자의 표정을 촬영하여 수신하기 위해 카메라와 같은 이미지 촬상 장치를 추가로 구비할 수 있다. 분석모듈이 수신하는 음성신호는 사용자가 발화하는 소정의 문장을 포함할 수 있다. 여기서 '문장'은 반드 시 주어와 동사를 포함해야 하는 것은 아니며 하나 이상의 단어의 집합일 수 있다. 마이크와 같은 음성수신 장 치를 통해 입력된 음성신호는 STT 분석부로 전달된다. STT 분석부는 음성신호를 해석하여 문자 데이 터로 전환하는 기능부이다. STT 분석부에서 생성된 문자 데이터는 구문 분석부로 전달된다. 구문 분석부는 문자 데이터 내 의 문장에 대한 자연어 분석 처리를 하여 단어별 및/또는 구문별 해석하여 구문 데이터를 생성할 수 있다. 일 실시예에서 '구문 데이터'는 사용자가 발화한 문장(즉, 문장을 텍스트로 전환한 텍스트 데이터) 및 상기 문 장에 대한 구문 분석된 데이터(이하 '메타 데이터'라 한다)를 포함할 수 있다. 예를 들어, 메타 데이터는 상기 문장을 구성하는 각각의 단어 및/또는 구문에 대응(매칭)하는 동작을 나타내는 동작 정보, 및 각각의 단어 및/ 또는 구문에 대응(매칭)하는 감정을 나타내는 감정 정보를 포함할 수 있다. 예를 들어 사용자가 발화한 문장 중 에 '너무 기뻐'와 같은 문장이 포함된 경우, 구문 분석부는 '기뻐'란 단어에 대응하여 '기쁨을 표시하는 동작'을 동작 정보로서 메타 데이터에 포함시킬 수 있다. 또한 구문 분석부는 '너무 기뻐'의 문장으로부터 '기쁨' 또는 '행복'이라는 감정을 매칭시킬 수 있고 이에 따라 메타 데이터는 기분류된 감정의 종류 중 '기쁨' 또는 '행복'을 감정 정보로서 메타 데이터에 포함시킬 수 있다. 일 실시예에서, 구문 분석부는 문장을 형태소 기반으로 분석할 수 있고 이에 따라 메타 데이터가 동사의 시제에 관한 시제 정보(예컨대, 미래형인지 완료형인지, 가정법인지 등을 판단하는 정보)를 더 포함할 수 있다. 예를 들어 사용자가 발화한 문장이 '~했다'라는 단어를 포함한 경우, '~했'이라는 형태소로부터 해당 동작을 이 미 완료했다고 판단할 수 있고, 다른 예로서, 문장이 '~하겠다'라는 단어를 포함한 경우 형태소 분석을 통해 해 당 동작을 아직 하지 않았다고 판단할 수 있다. 또 다른 예로서 문장이 '~할텐데'라는 단어를 포함한 경우 구문 분석부는 해당 동작을 하지 않았다고 판단할 수 있다. 일 실시예에서, 구문 분석부는 문장을 구성하는 각각의 단어 및/또는 구문에 대응(매칭)하는 동작을 캐릭 터가 수행하도록 동작 정보를 생성한다. 그러나 문장의 내용이나 뉘앙스에 따라, 캐릭터가 반드시 상기 문장 내 에 포함된 단어(동사)와 일치하는 동작을 하는 것은 아니다. 예를 들어 사용자가 발화한 문장이 '뛰어야겠다'와 같이 미래형이거나 또는 '뛰었을 텐데'와 같이 가정법을 의 미하는 경우 캐릭터가 뛰는 동작을 반드시 해야 하는 것이 아니므로, 구문 분석부는 '뛰다'는 단어에 대응 하여 뛰는 동작의 동작 정보를 생성하지 않을 수도 있고(예컨대, 이 경우 '뒤는 동작'이 아니라 '생각하는 동작' 또는 '후회하는 동작'을 동작 정보로서 생성할 수도 있고, 더 나아가 이 경우 '후회' 또는 '슬픔' 등의감정을 감정 정보로서 생성할 수도 있다), 구문 분석부는 문장의 전체적인 의미나 뉘앙스에 기초하여 동작 정보를 생성하는 것이 바람직하다. 그러므로 메타 데이터가 시제 정보를 더 포함하는 일 실시예에서, 구문 분석부는 문장의 단어나 구문 뿐만 아니라 시제 정보 및 문장의 문맥이나 뉘앙스에 기초하여 각 단어나 구문에 대응하는 동작의 동작 정보 및/또는 감정 정보를 생성할 수 있다. 상황인지 알고리즘은 예를 들어 딥러닝 알고리즘일 수 있다. 이 경우 기저장된 상황분류 데이터를 학 습데이터로 사용하여 상황인지 알고리즘을 학습시킨다. 예를 들어, 구문 분석부에서 출력되는 다양한 단어 및/또는 구문을 입력데이터로 하고 그에 대응하는 상황 데이터(즉, 상황분류 데이터에 저장된 상황 데이터)를 라벨로 사용하여 알고리즘을 학습시킬 수 있으며, 따라서 이렇게 학습된 상황인지 알고리즘 은 구문 분석부에서 분석된 단어 및/또는 구문에 대응하는 상황 데이터를 출력할 수 있다. 이와 같이 음성 분석모듈에서 생성된 구문 데이터와 상황 데이터가 렌더링 모듈로 전달될 수 있다. 렌더링 모듈은 표정 및 제스처 생성 알고리즘(이하 간단히 '생성 알고리즘'이라고도 함)을 포함한다. 생성 알고리즘은 음성 분석모듈로부터 수신한 구문 데이터와 상황 데이터를 입력받아 이에 대응하는 얼굴 표정과 제스처를 출력하고, 렌더링 모듈은 이렇게 출력된 표정과 제스처를 가상인간 캐릭터에 렌더링 하여 최종적으로 디스플레이를 통해 가상인간 캐릭터가 특정 표정과 제스처로 움직이는 애니메이션을 출력할 수 있다. 생성 알고리즘은 예를 들어 딥러닝 알고리즘일 수 있다. 이 경우 기저장된 제스처 및 표정 데이터를 학습데이터로 사용하여 알고리즘을 학습시킨다. 예를 들어, 구문 데이터와 상황 데이터를 입력데이터로 하 고 그에 대응하는 다양한 표정과 제스처(즉, 제스처 및 표정 데이터에 저장된 다양한 종류의 제스처와 표 정 데이터)를 라벨로 사용하여 알고리즘을 학습시킬 수 있고, 이렇게 학습된 알고리즘은 예를 들어 구문 데이터와 상황 데이터를 수신하면 그에 대응하는 얼굴 표정과 제스처를 출력데이터로서 출력할 수 있다. 그리고 렌더링 모듈은 이렇게 알고리즘에서 출력되는 제스처 및 얼굴 표정을 가상인간 캐릭터에 입혀 서 애니메이션으로 출력한다. 이 때 제스처 및 표정 데이터는 복수개의 애니메이션 동영상을 저장할 수 있다. 일 실시예에서 데이터 는 복수개의 제스처 동영상 및 복수개의 얼굴 동영상을 포함한다. 각각의 제스처 동영상은 캐릭터가 취할 수 있는 각기 다른 동작을 각각 표현하는 짧은 길이의 동영상이다. 예를 들어 각 제스처 동영상은 뛰다, 앉다, 손을 들다 등 기본적인 동작을 표현하거나 기쁨, 슬픔, 화남 등 감정에 따른 동작을 표현한다. 각 제스처 동영상마다 식별정보(예컨대 식별번호나 메타 데이터 등)나 라벨이 부여되어 있고 하나 이상의 동작 및/또는 감정이 하나 이상의 제스처 동영상의 식별정보나 라벨에 매칭되어 있을 수 있다. 각각의 얼굴 동영상은 캐릭터가 취할 수 있는 각기 다른 얼굴 표정을 표현하는 짧은 길이의 동영상이다. 각 얼 굴 동영상은 웃다, 울다, 찡그리다, 화내다 등 기본적인 감정 표현에 따른 얼굴 표정을 표현한다. 예를 들어 슬 픈 표정의 얼굴 동영상은 눈썹 끝과 입꼬리가 내려가는 것을 표현하고 기쁜 표정의 동영상은 눈썹이 올라가고 입꼬리가 올라가는 것을 표현할 수 있다. 각 얼굴 동영상마다 식별정보나 라벨이 부여되어 있고 하나 이상의 감 정이 하나 이상의 얼굴 동영상의 식별정보나 라벨에 매칭되어 있을 수 있다. 일 실시예에서 제스처 및 표정 데이터는 가상인간 캐릭터의 특징에 따라 다양한 동작의 제스처 동영상 및/ 또는 얼굴 동영상을 포함할 수 있다. 예를 들어 '뛰다'는 하나의 동작에 대해서도 캐릭터의 성별이나 나이 또는 외모에 따라 뛰는 동작이 다를 수 있고, '웃다' 또는 '울다' 등 감정 표현에 대해서도 캐릭터의 성별, 나이, 또 는 외모에 따라 동작이 각각 다를 수 있다. 그러므로 바람직하게는 하나의 동작이나 감정에 대해 캐릭터의 특징 에 따라 다양하고 세분화된 제스처 동영상 및/또는 얼굴 동영상을 구비하는 것이 바람직하다. 각각의 제스처 동영상 및/또는 얼굴 동영상은 상술한 바와 같이 기본적인 동작이나 감정을 나타내는 짧은 길이 의 단위 동영상이며 예를 들어 수분의 1초 내지 수초 사이의 길이를 갖는다. 일 실시예에 각각의 제스처 동영상 및/또는 얼굴 동영상은 특정 가상인간 캐릭터가 아직 입혀지지 않은 형태의 동영상일 수 있다. 예를 들어 제스처 동영상은 관절, 뼈대 등 몸체의 특징점의 움직임을 정의하는 동영상일 수 있고 얼굴 동영상은 눈, 코, 입 등 얼굴의 특징점의 움직임을 정의하는 동영상일 수 있으며, 사용자에 의해 또 는 시스템에 의해 가상인간 캐릭터 데이터에서 임의의 가상인간 캐릭터가 선택되면, 이러한 제스처 및/또는 얼굴 동영상에 상기 선택된 가상인간 캐릭터의 외관이 입혀지는 리타겟 작업이 수행되어 최종적인 가상인간 캐릭터의 애니메이션이 생성될 수 있다. 이와 같이 본 발명의 일 실시예에 따르면 사용자가 발화하는 음성신호와 표정에 따라 그에 대응하는 적절한 제 스처와 얼굴 표정을 가상인간 캐릭터에 입혀서 캐릭터 애니메이션을 생성할 수 있으므로 한 문장 내에서도 시간 적으로 다양한 감정 변화를 표현하고 그에 따른 제스처도 변화시킬 수 있다. 이상과 같이 본 발명이 속하는 분야에서 통상의 지식을 가진 자라면 이러한 명세서의 기재로부터 다양한 수정 및 변형이 가능함을 이해할 수 있다. 그러므로 본 발명의 범위는 설명된 실시예에 국한되어 정해져서는 아니되 며 후술하는 특허청구범위뿐 아니라 이 특허청구범위와 균등한 것들에 의해 정해져야 한다."}
{"patent_id": "10-2023-0098654", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도1은 본 발명의 일 실시예에 따른 인공지능 기반 가상인간 캐릭터의 표정 및 제스처 표현 시스템을 설명하는 도면이다."}
