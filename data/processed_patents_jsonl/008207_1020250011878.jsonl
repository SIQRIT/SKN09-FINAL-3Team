{"patent_id": "10-2025-0011878", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0021511", "출원번호": "10-2025-0011878", "발명의 명칭": "피사체가 촬영된 영상을 생성하는 디바이스 및 방법", "출원인": "삼성전자주식회사", "발명자": "딘쿠오칸"}}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "디바이스가 영상을 처리하는 방법에 있어서,상기 디바이스가 원시 영상을 획득하는 동작;상기 디바이스가 상기 원시 영상에 기초하여 영상의 밝기를 조절하기 위한 톤 맵 데이터를 획득하는 동작;상기 디바이스가 상기 톤 맵 데이터에 기초하여 상기 원시 영상 내의 픽셀들의 밝기를 스케일링하는 동작;상기 디바이스가 상기 밝기가 스케일링된 원시 영상 내의 복수의 특징들을 각각 나타내는 복수의 특징 영상들을획득하는 동작;영상의 특징을 보정하기 위한 적어도 하나의 설정에 기초하여, 상기 디바이스가 상기 복수의 특징 영상들을 보정하는 동작;상기 보정된 복수의 특징 영상들에 기초하여, 상기 디바이스가 출력 영상을 생성하는 동작; 및상기 디바이스가 상기 생성된 출력 영상을 저장하는 동작을 포함하는, 방법."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 톤 맵 데이터를 획득하는 동작은,상기 디바이스가 상기 원시 영상을 인공지능 모델에 입력하는 동작; 및상기 디바이스가 상기 인공지능 모델로부터 출력되는 상기 톤 맵 데이터를 획득하는 동작을 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 복수의 특징 영상들을 획득하는 동작은,상기 디바이스가 상기 밝기가 스케일링된 영상을 상기 인공지능 모델에 입력하는 동작; 및상기 디바이스가 상기 인공지능 모델로부터 출력되는 복수의 특징 영상들을 획득하는 동작을 포함하며,상기 인공지능 모델은, 입력되는 영상의 컨텍스트를 포착하기 위한 목적으로 구성된 수축 단계(contractionpath)의 레이어들 및 상기 수축 단계의 영상의 해상도를 높이기 위한 업 샘플링을 수행하는 팽창 단계(expanding path)의 레이어들을 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 톤 맵 데이터를 획득하는 동작은,상기 디바이스가 상기 원시 영상을, 영상의 밝기를 조절하도록 훈련된 제1 인공지능 모델에 입력하는 동작; 및상기 디바이스가 상기 제1 인공지능 모델로부터 출력되는 상기 톤 맵 데이터를 획득하는 동작을 포함하는 것을공개특허 10-2025-0021511-3-특징으로 하는 방법."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 복수의 특징 영상들을 획득하는 동작은,상기 디바이스가 상기 밝기가 스케일링된 원시 영상을, 영상의 특징들을 분석하도록 훈련된 제2 인공지능에 입력하는 동작; 및상기 디바이스가 상기 제2 인공지능 모델로부터 출력되는 복수의 특징 영상들을 획득하는 동작을 포함하고,상기 인공지능 모델은, 입력되는 영상의 컨텍스트를 포착하기 위한 목적으로 구성된 수축 단계(contractionpath)의 레이어들 및 상기 수축 단계의 영상의 해상도를 높이기 위한 업 샘플링을 수행하는 팽창 단계(expanding path)의 레이어들을 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 출력 영상을 생성하는 동작은,상기 디바이스가 상기 보정된 복수의 특징 영상들을, 출력 영상을 생성하도록 훈련된 제3 인공지능 모델에 입력하는 동작; 및상기 디바이스가 상기 제3 인공지능 모델로부터 출력되는 상기 출력 영상을 획득하는 동작을 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 제1 인공지능 모델, 상기 제2 인공지능 모델 및 상기 제3 인공지능 모델은, 기준 원시 영상 및 상기 기준원시 영상으로부터 기설정된 ISP 프로세싱 (Image Signal Processing)을 통해 생성되는 기준 출력 영상을 바탕으로 함께 훈련된 것을 특징으로 하는 방법."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,연속 촬영으로 인하여 생성되는 복수의 원시 영상들을 조합함으로써 생성되는 상기 기준 원시 영상으로부터 기설정된 ISP 프로세싱을 통해 생성되는 상기 기준 출력 영상, 및 상기 기준 원시 영상으로부터 상기 제1 인공지능 모델, 상기 제2 인공지능 모델 및 상기 제3 인공지능 모델을 통해 출력되는 출력 영상 간의 로스(loss) 기초하여, 상기 제1 인공지능 모델, 상기 제2 인공지능 모델 및 상기 제3 인공지능 모델이 함께 훈련된 것을 특징으로 하는 방법."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서,상기 제1 인공지능 모델, 상기 제2 인공지능 모델 및 상기 제3 인공지능 모델 중 적어도 하나를 이용하지 않고,상기 원시 영상으로부터 라이브뷰 영상을 생성하는 동작; 및공개특허 10-2025-0021511-4-상기 생성된 라이브뷰 영상을 디스플레이하는 동작을 더 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제6항에 있어서,상기 제1 인공지능 모델, 상기 제2 인공지능 모델 및 상기 제3 인공지능 모델을 리트레이닝하는 동작을 더 포함하며,상기 리트레이닝하는 동작은,상기 리트레이닝을 위한 기준 영상 및 상기 기준 영상에 대응되는 기준 원시 영상을 획득하는 동작; 및상기 획득된 기준 영상 및 상기 기준 원시 영상을 이용하여, 상기 제1 인공지능 모델, 상기 제2 인공지능 모델및 상기 제3 인공지능 모델을 리트레이닝하는 동작을 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "영상을 처리하는 디바이스에 있어서,상기 디바이스의 제어를 위한 명령어들이 저장되는 메모리; 및상기 메모리에 저장된 명령어들을 실행하기 위한 적어도 하나의 프로세서를 포함하며,상기 적어도 하나의 프로세서가 상기 메모리에 저장된 명령어들을 실행함으로써 상기 디바이스는,원시 영상을 획득하고,상기 원시 영상에 기초하여 영상의 밝기를 조절하기 위한 톤 맵 데이터를 획득하고,상기 톤 맵 데이터에 기초하여 상기 원시 영상 내의 픽셀들의 밝기를 스케일링하고,상기 밝기가 스케일링된 원시 영상 내의 복수의 특징들을 각각 나타내는 복수의 특징 영상들을 획득하고,영상의 특징을 보정하기 위한 적어도 하나의 설정에 기초하여, 상기 복수의 특징 영상들을 보정하고,상기 보정된 복수의 특징 영상들에 기초하여, 출력 영상을 생성한 후,상기 생성된 출력 영상을 저장하는, 디바이스."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 디바이스는 상기 톤 맵 데이터를 획득함에 있어서,상기 원시 영상을 인공지능 모델에 입력한 후,상기 인공지능 모델로부터 출력되는 상기 톤 맵 데이터를 획득하는 것을 특징으로 하는 디바이스."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 디바이스는 상기 복수의 특징 영상들을 획득함에 있어서,상기 밝기가 스케일링된 영상을 상기 인공지능 모델에 입력한 후,상기 인공지능 모델로부터 출력되는 복수의 특징 영상들을 획득하며,상기 인공지능 모델은, 입력되는 영상의 컨텍스트를 포착하기 위한 목적으로 구성된 수축 단계(contraction공개특허 10-2025-0021511-5-path)의 레이어들 및 상기 수축 단계의 영상의 해상도를 높이기 위한 업 샘플링을 수행하는 팽창 단계(expanding path)의 레이어들을 포함하는 것을 특징으로 하는 디바이스."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 디바이스는 상기 톤 맵 데이터를 획득함에 있어서,상기 원시 영상을, 영상의 밝기를 조절하도록 훈련된 제1 인공지능 모델에 입력한 후,상기 제1 인공지능 모델로부터 출력되는 상기 톤 맵 데이터를 획득하는 것을 특징으로 하는 디바이스."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 디바이스는 상기 복수의 특징 영상들을 획득함에 있어서,상기 밝기가 스케일링된 원시 영상을, 영상의 특징들을 분석하도록 훈련된 제2 인공지능에 입력한 후,상기 제2 인공지능 모델로부터 출력되는 복수의 특징 영상들을 획득하며,상기 인공지능 모델은, 입력되는 영상의 컨텍스트를 포착하기 위한 목적으로 구성된 수축 단계(contractionpath)의 레이어들 및 상기 수축 단계의 영상의 해상도를 높이기 위한 업 샘플링을 수행하는 팽창 단계(expanding path)의 레이어들을 포함하는 것을 특징으로 하는 디바이스."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 디바이스는 상기 출력 영상을 생성함에 있어서,상기 보정된 복수의 특징 영상들을, 출력 영상을 생성하도록 훈련된 제3 인공지능 모델에 입력한 후,상기 제3 인공지능 모델로부터 출력되는 상기 출력 영상을 획득하는 것을 특징으로 하는 디바이스."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 제1 인공지능 모델, 상기 제2 인공지능 모델 및 상기 제3 인공지능 모델은, 기준 원시 영상 및 상기 기준원시 영상으로부터 기설정된 ISP 프로세싱 (Image Signal Processing)을 통해 생성되는 기준 출력 영상을 바탕으로 함께 훈련된 것을 특징으로 하는 디바이스."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,연속 촬영으로 인하여 생성되는 복수의 원시 영상들을 조합함으로써 생성되는 상기 기준 원시 영상으로부터 기설정된 ISP 프로세싱을 통해 생성되는 상기 기준 출력 영상, 및 상기 기준 원시 영상으로부터 상기 제1 인공지능 모델, 상기 제2 인공지능 모델 및 상기 제3 인공지능 모델을 통해 출력되는 출력 영상 간의 로스(loss) 기초하여, 상기 제1 인공지능 모델, 상기 제2 인공지능 모델 및 상기 제3 인공지능 모델이 함께 훈련된 것을 특징으로 하는 디바이스.공개특허 10-2025-0021511-6-청구항 19 제16항에 있어서,상기 디바이스는,상기 제1 인공지능 모델, 상기 제2 인공지능 모델 및 상기 제3 인공지능 모델 중 적어도 하나를 이용하지 않고,상기 원시 영상으로부터 라이브뷰 영상을 생성한 후,상기 생성된 라이브뷰 영상을 디스플레이하는 것을 특징으로 하는 디바이스."}
{"patent_id": "10-2025-0011878", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제16항에 있어서,상기 디바이스는 상기 제1 인공지능 모델, 상기 제2 인공지능 모델 및 상기 제3 인공지능 모델을 리트레이닝함에 있어서,상기 리트레이닝을 위한 기준 영상 및 상기 기준 영상에 대응되는 기준 원시 영상을 획득한 후,상기 획득된 기준 영상 및 상기 기준 원시 영상을 이용하여, 상기 제1 인공지능 모델, 상기 제2 인공지능 모델및 상기 제3 인공지능 모델을 리트레이닝하는 것을 특징으로 하는 디바이스."}
{"patent_id": "10-2025-0011878", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "피사체가 촬영된 영상을 생성하는 디바이스 및 방법이 제공된다. 디바이스가 원시 영상으로부터 피사체가 촬영된 출력 영상을 생성하는 방법은, 상기 디바이스의 제어를 위한 제1 프로세서를 이용하여, 상기 디바이스 내의 카메 라 센서를 제어함으로써 피사체를 나타내는 원시 영상을 획득하는 동작; 인공지능 모델을 이용하는 상기 원시 영 상에 대한 영상 처리를 위한 제2 프로세서를 이용하여, 상기 피사체를 나타내는 상기 원시 영상을, 영상의 밝기 를 조절하기 위해 훈련된 제1 인공지능 모델에 입력하는 동작; 상기 제2 프로세서를 이용하여, 상기 제1 인공지 능 모델로부터 출력되는 톤 맵 데이터를 획득하는 동작; 및 상기 톤 맵 데이터에 기초하여 생성되는 출력 영상을 저장하는 동작;을 포함한다."}
{"patent_id": "10-2025-0011878", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공지능 모델을 이용하여 피사체가 촬영된 영상을 생성하는 디바이스 및 방법에 관한 것이다."}
{"patent_id": "10-2025-0011878", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "휴대 단말기가 널리 사용되고, 통신 네트워크 기술이 발전함에 따라 휴대 단말기와 관련된 소비자들의 요구가 다양화되고 있으며, 이에 따라 다양한 종류의 부가 장치들이 휴대 단말기에 장착되고 있다. 또한, 휴대 단말기 가 소형화되고 피사체를 촬영하기 위한 카메라의 기능이 휴대 단말기에 의해 지원되고 있다. 하지만, 휴대 단말기의 소형화로 인하여, 고성능의 카메라 센서를 휴대 단말에 장착하기 힘든 어려움이 있었으 며, 카메라 센서를 통해 생성되는 로우 데이터를 프로세싱하는데 휴대 단말의 많은 리소스가 필요한 문제가 있 었다. 이에 따라, 휴대 단말의 카메라 센서를 이용하여 다양한 영상 특징을 가지는 영상을 생성하는 기술의 필요성이 높아지고 있으며, 나아가, 높은 품질의 HDR (High Dynamic Range) 이미지를 생성할 수 있는 인공지는 기술이 요 구되고 있다."}
{"patent_id": "10-2025-0011878", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 일 실시예는, 인공지능 모델을 이용하여 피사체가 촬영된 영상을 생성할 수 있는 디바이스 및 방법을 제공할 수 있다. 또한, 본 개시의 일 실시예는, 카메라 센서로부터 생성되는 원시 영상을 인공지능 모델에 입력하여 피사체가 촬 영된 출력 영상을 획득할 수 있는 디바이스 및 방법을 제공할 수 있다. 또한, 본 개시의 일 실시예는, 톤 맵을 생성하기 위한 인공지능 모델을 이용하여 피사체가 촬영된 출력 영상을 획득할 수 있는 디바이스 및 방법을 제공할 수 있다. 또한, 본 개시의 일 실시예는, 복수의 인공지능 모델을 순차적으로 이용하여 원시 영상으로부터 피사체가 촬영 된 출력 영상을 획득할 수 있는 디바이스 및 방법을 제공할 수 있다.또한, 본 개시의 일 실시예는, 함께 훈련된 복수의 인공지능 모델을 적어도 일부를 이용하여, 피사체를 촬영하 기 위한 라이브 뷰 영상 및 피사체가 촬영된 출력 영상을 획득할 수 있는 디바이스 및 방법을 제공할 수 있다. 또한, 본 개시의 일 실시예는, 피사체가 촬영되는 상황 또는 사용자의 선호 중 적어도 하나에 관련하여 훈련된 인공지능 모델을 이용하여, 피사체를 촬영하기 위한 라이브 뷰 영상 및 피사체가 촬영된 출력 영상을 획득할 수 있는 디바이스 및 방법을 제공할 수 있다."}
{"patent_id": "10-2025-0011878", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본 개시의 제1 측면은, 디바이스의 제어를 위한 제1 프로세서를 이용하여, 상기 디바이스 내의 카메라 센서를 제어함으로써 상기 카메라 센서의 전방의 피사체를 나 타내는 원시 영상을 획득하는 동작; 인공지능 모델을 이용하는 상기 원시 영상에 대한 영상 처리를 위한 제2 프 로세서를 이용하여, 상기 카메라 센서의 전방의 상기 피사체를 나타내는 상기 원시 영상을, 영상의 밝기를 조절 하기 위해 훈련된 제1 인공지능 모델에 입력하는 동작; 상기 제2 프로세서를 이용하여, 상기 제1 인공지능 모델 로부터 출력되는 톤 맵 데이터를 획득하는 동작; 및 상기 톤 맵 데이터에 기초하여 생성되는 출력 영상을 저장 하는 동작;을 포함하는, 디바이스가 원시 영상으로부터 피사체가 촬영된 출력 영상을 생성하는 방법을 제공할 수 있다. 또한, 본 개시의 제2 측면은, 카메라 센서; 디스플레이; 상기 디바이스의 제어를 위한 제1 명령어들을 저장하는 제1 메모리; 상기 제1 메모리에 저장된 제1 명령어들을 실행하는 제1 프로세서; 상기 원시 영상에 대한 영상 처 리를 위한 적어도 하나의 인공지능 모델 및 상기 인공지능 모델의 실행에 관련된 제2 명령어들을 저장하는 제2 메모리; 및 상기 제2 메모리에 저장된 상기 적어도 하나의 인공지능 모델 및 상기 제2 명령어들을 실행하는 제2 프로세서;를 포함하며, 상기 제1 프로세서가 상기 카메라 센서를 이용하여 상기 카메라 센서의 전방의 피사체를 나타내는 원시 영상을 획득하고, 상기 제2 프로세서가 상기 카메라 센서의 전방의 상기 피사체를 나타내는 상기 원시 영상을, 영상의 밝기를 조절하기 위해 훈련된 제1 인공지능 모델에 입력하고, 상기 제2 프로세서가 상기 제1 인공지능 모델로부터 출력되는 톤 맵 데이터를 획득하고, 상기 제1 프로세서가 상기 톤 맵 데이터에 기초하 여 생성되는 출력 영상을 상기 제1 메모리에 저장하는 것인, 원시 영상으로부터 피사체가 촬영된 출력 영상을 생성하는 디바이스를 제공할 수 있다. 또한, 본 개시의 제3 측면은, 제1 측면의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체를 제공할 수 있다."}
{"patent_id": "10-2025-0011878", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 개시의 실시예를 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해서 설명과 관 계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\"되어 있는 경우도 포함한다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아 니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 본 명세서에서, AI 프로세싱 유닛은 인공지능 기술을 이용하여 이미지를 처리하기 위한 프로세싱 유닛일 수 있다. 예를 들어, AI 프로세싱 유닛은 인공지능 모델을 이용한 이미지 처리를 위하여 설계된 프로세싱 유닛 으로서, 이미지 처리를 위한 전용의 프로세싱 유닛일 수 있다. 또는, 예를 들어, AI 프로세싱 유닛은 NPU(Neural Processing Unit)에 인공지능 모델을 이용한 이미지 처리를 위한 설정을 함으로써 구현될 수 있다. 또한, 본 명세서에서, 인공지능 모델은 원시 영상(raw image)으로부터 피사체가 촬영된 결과물인 출력 영상을 생성하기 위하여 훈련된 모델로서, 복수의 하위 인공지능 모델을 포함할 수 있다. 인공 지능 모델에 포함된 복 수의 하위 인공지능 모델은, 톤 맵 생성 모델, 특징 추출 모델, 영상 보정 모델 및 영상 회복 모델을 포함할 수 있다. 톤 맵 생성 모델은 원시 영상으로부터 톤 맵을 생성하기 위하여 훈련된 인공지능 모델이고, 특징 추출 모 델은 특징 추출 모델에 입력된 원시 영상 내의 특징들을 추출하기 위하여 훈련된 인공지능 모델이며, 영상 보정 모델은 특징 추출 모델로부터 출력되는 특징 영상들을 보정하기 위하여 훈련된 인공지능 모델이며, 영상 회복 모델은 보정된 특징 영상들로부터 피사체가 촬영된 촬영 영상을 생성하기 위하여 훈련된 인공지능 모델일 수 있 다. 또한, 본 명세서에서, 톤 맵은 원시 영상 내의 픽셀들의 밝기를 스케일링하기 위한 정보를 포함하는 맵 데이터 일 수 있다. 톤 맵은, 원시 영상 내의 부분 별로 픽셀의 밝기를 스케일링하는 로컬톤 매핑 또는 원시 영상의 전 체에 대한 밝기를 스케일링하는 글로벌톤 매핑 중 적어도 하나를 위한 맵 데이터일 수 있다. 또한, 본 명세서에서, 라이브 뷰 영상은 피사체를 촬영하는 사용자가 촬영되는 피사체를 확인할 수 있도록 디바 이스의 화면 상에 출력되는 영상일 수 있다. 이하 첨부된 도면을 참고하여 본 개시를 상세히 설명하기로 한다. 도 1은 본 개시의 일 실시예에 따른, 디바이스가 피사체를 촬영하여 출력 영상을 생성하는 예시를 나타내 는 도면이다. 도 1을 참조하면, 디바이스는 피사체가 촬영된 영상을 생성하기 위한 인공지능 모델을 이용하여 피사체를 촬영할 수 있다. 디바이스는 피사체가 촬영된 영상을 생성하기 위한 AI 프로세싱 유닛을 포함할 수 있으며, 디바이스의 AI 프로세싱 유닛은 디바이스 내의 카메라 센서를 이용하여 생성되는 원 시 영상 및 영상 보정을 위한 기설정된 설정 값을 인공지능 모델에 입력하고, 인공지능 모델로부터 출력되는 출 력 영상을 획득할 수 있다. AI 프로세싱 유닛에 의해 이용되는 인공지능 모델은, 원시 영상피사체가 촬영 된 결과물인 출력 영상을 원시 영상으로부터 생성하기 위하여 훈련된 모델로서, 복수의 하위 인공지능 모델을 포함할 수 있다. 인공 지능 모델에 포함된 복수의 하위 인공지능 모델은, 톤 맵을 생성하기 위한 인공지능 모델, 영상의 특징을 추출하기 위한 인공지능 모델, 추출된 특징을 나타내는 특징 영상을 보정하기 위한 인공지 능 모델 및 출력 영상을 생성하기 위한 인공지능 모델을 포함할 수 있다. 또한, 디바이스의 AI 프로세싱 유닛은 복수의 하위 인공지능 모델 중 적어도 하나를 이용하여 피사 체 촬영을 위한 라이브뷰 영상을 생성할 수 있다. 디바이스는, 스마트폰, 태블릿 PC, PC, 스마트 TV, 휴대폰, PDA(personal digital assistant), 랩톱, 미 디어 플레이어, GPS(global positioning system) 장치, 전자책 단말기, 디지털방송용 단말기, 네비게이션, 키오 스크, 디지털 카메라, 가전기기 및 기타 모바일 또는 비모바일 컴퓨팅 장치일 수 있으나, 이에 제한되지 않는다. 또한, 디바이스는 통신 기능 및 데이터 프로세싱 기능을 구비한 시계, 안경, 헤어 밴드 및 반지 등의 웨어러블 디바이스일 수 있다. 그러나, 이에 제한되지 않으며, 디바이스는 피사체를 촬영할 수 있는 모든 종류의 기기를 포함할 수 있다. 디바이스는 피사체가 촬영된 영상을 획득하기 위하여 서버(미도시)와 네트워크를 통하여 통신할 수 있다. 네트워크는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN) 또는 부가가치 통 신망(Value Added Network; VAN) 등과 같은 유선 네트워크나 이동 통신망(mobile radio communication network) 또는 위성 통신망 등과 같은 모든 종류의 무선 네트워크로 구현될 수 있다. 또한, 네트워크는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN), 부가가치 통신망(Value Added Network; VAN), 이동 통신망(mobile radio communication network) 또는 위성 통신망 중 적어도 둘 이상의 상 호 조합을 포함할 수 있으며, 각 네트워크 구성 주체가 서로 원활하게 통신을 할 수 있도록 하는 포괄적인 의미 의 데이터 통신망이며, 유선 인터넷, 무선 인터넷 및 모바일 무선 통신망을 포함할 수 있다. 무선 통신은 예를 들어, 무선 랜(Wi-Fi), 블루투스, 블루투스 저 에너지(Bluetooth low energy), 지그비, WFD(Wi-Fi Direct), UWB(ultra wideband), 적외선 통신(IrDA, infrared Data Association), NFC(Near Field Communication) 등이 있을 수 있으나, 이에 한정되는 것은 아니다. 도 2는 본 개시의 일 실시예에 따른 디바이스의 블록도이다. 도 2를 참조하면, 본 개시의 일 실시예에 따른 디바이스는 사용자 입력부, 디스플레이부, 통 신 인터페이스, 카메라 센서부, 제1 프로세서, 제1 메모리 및 AI 프로세싱 유닛 를 포함할 수 있다. 또한, AI 프로세싱 유닛은 인공지능 기술을 이용하여 이미지를 처리하기 위한 제2 프로세서 및 제2 메모리을 포함할 수 있다. 예를 들어, AI 프로세싱 유닛은 인공지능 모 델을 이용한 이미지 처리를 위하여 설계된 프로세싱 유닛으로서, 이미지 처리를 위한 전용의 프로세싱 유닛일 수 있다. 또는, 예를 들어, AI 프로세싱 유닛은 NPU(Neural Processing Unit)에 인공지능 모델을 이용한 이미지 처리를 위한 설정을 함으로써 구현될 수 있다. 사용자 입력부는, 사용자가 디바이스를 제어하기 위한 데이터를 입력하는 수단을 의미한다. 예를 들어, 사용자 입력부는 키 패드(key pad), 돔 스위치 (dome switch), 터치 패드(접촉식 정전 용량 방식, 압력식 저항막 방식, 적외선 감지 방식, 표면 초음파 전도 방식, 적분식 장력 측정 방식, 피에조 효과 방식 등), 조그 휠 또는 조그 스위치 중 적어도 하나를 포함할 수 있으나 이에 한정되는 것은 아니다. 사용자 입력부 는 디바이스를 이용하는 사용자가 사진을 촬영하기 위한 사용자 입력을 수신할 수 있다. 디스플레이부는 디바이스에서 처리되는 정보를 표시 출력한다. 예를 들어, 디스플레이부는, 사진 촬영을 위한 GUI, 라이브 뷰 영상, 사진 촬영의 결과물로 출력되는 출력 영상을 디스플레이할 수 있다. 한편, 디스플레이부와 터치패드가 레이어 구조를 이루어 터치 스크린으로 구성되는 경우, 디스플레이부 는 출력 장치 이외에 입력 장치로도 사용될 수 있다. 디스플레이부는 액정 디스플레이(liquid crystal display), 박막 트랜지스터 액정 디스플레이(thin film transistor-liquid crystal display), 유기 발광 다이오드(organic light-emitting diode), 플렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display), 전기영동 디스플레이(electrophoretic display) 중에서 적어도 하나를 포함할 수 있다. 그리고 디바 이스의 구현 형태에 따라 디바이스는 디스플레이부를 2개 이상 포함할 수도 있다. 통신 인터페이스는, 다른 디바이스(미도시) 및 서버(미도시) 간의 통신을 하게 하는 하나 이상의 구성요 소를 포함할 수 있다. 예를 들어, 통신 인터페이스는, 근거리 통신부, 이동 통신부, 방송 수신부를 포함 할 수 있다. 근거리 통신부(short-range wireless communication unit)는, 블루투스 통신부, BLE(Bluetooth Low Energy) 통 신부, 근거리 무선 통신부(Near Field Communication unit), WLAN 통신부, 지그비(Zigbee) 통신부, 적외선 (IrDA, infrared Data Association) 통신부, WFD(Wi-Fi Direct) 통신부, UWB(ultra wideband) 통신부, Ant+ 통신부 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 이동 통신부는, 이동 통신망 상에서 기지국, 외부 의 단말, 서버 중 적어도 하나와 무선 신호를 송수신한다. 여기에서, 무선 신호는, 음성 호 신호, 화상 통화 호 신호 또는 문자/멀티미디어 메시지 송수신에 따른 다양한 형태의 데이터를 포함할 수 있다. 방송 수신부는, 방 송 채널을 통하여 외부로부터 방송 신호 및/또는 방송 관련된 정보를 수신한다. 방송 채널은 위성 채널, 지상파 채널을 포함할 수 있다. 구현 예에 따라서 디바이스가 방송 수신부를 포함하지 않을 수도 있다. 또한, 통신 인터페이스는, 사진 촬영을 위한 인공지능 모델을 관리하기 위하여 필요한 정보를, 다 른 디바이스(미도시) 및 서버(미도시)와 송수신할 수 있다. 카메라 센서부는 컬러 필터 및 이미지 센서를 포함하며, 후술할 제1 프로세서 또는 제 2 프로세서에 의해 제어됨으로써, 피사체에 대한 원시 영상(raw image)을 생성할 수 있다. 카메라 센서부 는 제1 프로세서의 제어 신호에 기초한 제2 프로세서의 제어 또는 제2 프로세서의 독 립적인 제어에 의해, 피사체에 대한 원시 영상을 생성할 수 있다. 예를 들어, 제2 프로세서가 카메라 센 서부 내에 포함되는 경우에, 제1 프로세서가 카메라 센서부에 대해서 촬영을 위한 제어 요청 신호를 보내면, 카메라 센서부 내의 제2 프로세서가 제어 요청 신호를 수신하여 카메라 센서부 를 제어하여 원시 영상을 생성할 수 있다. 또는, 예를 들어, 제2 프로세서가 카메라 센서부 의 외부에 있는 경우에, 제1 프로세서가 촬영을 위한 제어 요청 신호를 제2 프로세서에게 보내면, 제2 프로세서가 제어 요청 신호를 수신하여 카메라 센서부를 제어하여 원시 영상을 생성할 수 있다. 또는, 예를 들어, 제1 프로세서가 카메라 센서부를 제어하여 원시 영상을 획득하고 획득된 원시 영상을 제2 프로세서에게 제공할 수도 있다. CMOS(complementary metal-oxide semiconductor) 또는 CCD(charge coupled device) 등의 이미지 센서에 의해 획득되는 영상은 단색(monochrome)의 영상이므로, 이미지 센서의 전단부에 가시광선 영역에서 특정 주파수 대역 만을 통과시키는 컬러 필터가 배치될 수 있으며, 컬러 필터를 통과하여 이미지 센서 로 전달되는 광으로부터 컬러 영상이 획득될 수 있다. 컬러 필터는 피사체로부터 전달되는 광 중에 서 특정 주파수 대역을 갖는 광을 통과시킨다. 컬러 필터는 복수의 색상에 대응되는 복수의 영역으로 구 분되어 있으며, 복수의 영역 각각은, 예를 들어, 적색, 녹색, 청색의 3개의 색상 중 하나의 색상이 갖는 주파수 대역과 동일한 주파수 대역을 갖는 광만을 통과 시킬 수 있다. 컬러 필터를 통과한 광은 CMOS 또는 CCD 등의 이미지 센서로 전달되며, 이미지 센서는 전달받은 광을 전기적 신호로 변환할 수 있다. 예를 들어, 이미지 센서에 의해 변환된 전기적 신호는 적색 값, 녹색 값 및 청색 값으로 구성될 수 있으며, 적 색 값, 녹색 값 및 청색 값의 배열로 구성된 원시 영상(raw image)이 생성될 수 있다. 원시 영상은 컬러 필터 의 어레이 패턴에 따른 패턴을 가질 수 있으며, 예를 들어, 베이어(bayer) 패턴, RGBE 패턴, RYYB 패턴, CYYM 패턴, CYGM 패턴, RGBW 베이어 패턴, 및 X-trans 패턴 중 하나의 패턴을 가질 수 있다. 제1 메모리는, 후술할 제1 프로세서 또는 제2 프로세서 중 적어도 하나의 처리 및 제어를 위 한 프로그램을 저장할 수 있고, 디바이스로 입력되거나 디바이스로부터 출력되는 데이터를 저장할 수 있다. 제1 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램 (RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 제1 메모리에 저장된 프로그램들은 그 기능에 따라 복수 개의 모듈들로 분류할 수 있는데, 예를 들어, 영 상 처리 모듈, 적어도 하나의 인공지능 모델 및 모델 관리 모듈을 포함할 수 있다. 또한, 영 상 처리 모듈은 전처리 모듈, 출력 영상 생성 모듈 및 라이브 뷰 생성 모듈을 포함하 고, 인공지능 모델은 톤 맵 생성 모델(tone map generation model), 특징 추출 모델(feature extraction model), 영상 보정 모델(image modifying model) 및 영상 회복 모델(image regression model)와 같은 복수의 하위 인공지능 모델들을 포함하며, 모델 관리 모듈은 모델 선택 모듈, 다운로딩 모듈, 업데 이트 모듈 및 리트레이닝 모듈을 포함할 수 있다. 제1 프로세서는, 통상적으로 디바이스 의 전반적인 동작을 제어한다. 예를 들어, 제어부는, 메모리에 저장된 프로그램들을 실행함 으로써, 사용자 입력부, 디스플레이부, 통신 인터페이스, 카메라 센서부 및 제1 메모 리를 제어할 수 있다. 제1 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서 또는 GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 중 적어도 하나를 포함할 수 있다. AI 프로세싱 유닛은 제2 프로세서 및 제2 메모리를 포함할 수 있다. 제2 프로세서 및 제2 메모리는 인공지능 모델을 이용한 이미지 처리를 위하여 설계될 수 있으나, 이에 제한되지 않는다. 제2 메모리는 제2 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수 있다. 예를 들어, 제2 메 모리는 제 1 메모리에 저장된 인공지능 모델 중에서 선택된 인공지능 모델이 저장될 수 있으며, 제2 메모리에 저장된 인공지능 모델은 제2 프로세서에 의해 실행될 수 있다. 제2 메모리에 저장된 인공지능 모델은 톤 맵 생성 모델(tone map generation model), 특징 추출 모델(feature extraction model), 영상 보정 모델(image modifying model) 및 영상 회복 모델 (image regression model)와 같은 복수의 하위 인공지능 모델들을 포함할 수 있다. 제2 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램 (RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 제1 메모리에 저장된 영상 처리 모듈은 제1 프로세서 또는 제2 프로세서 중 적어도 하 나에 의해 실행됨으로써 피사체를 촬영하기 위한 라이브 뷰 영상 및 피사체가 촬영된 출력 영상을 생성할 수 있 다. 라이브 뷰 영상은 피사체를 촬영하는 사용자가 촬영되는 피사체를 확인할 수 있도록 디스플레이부 상 에 출력되는 영상이며, 피사체가 촬영된 출력 영상은 피사체를 촬영한 결과물로서 디바이스에 저장되는 피사체가 촬영된 영상일 수 있다. 제1 프로세서는 제1 메모리에 저장된 전처리 모듈을 실행함으로써 피사체에 대한 원시 영상 을 생성할 수 있다. 제1 프로세서는, 피사체를 촬영하는 사용자 입력이 수신됨에 따라, 사용자 입력이 수 신된 때에 카메라 센서부의 컬러 필터를 통과하여 이미지 센서로 전달되는 광에 기초한 원시 영상을 생성할 수 있다. 제1 프로세서는, 예를 들어, 적색 값, 녹색 값 및 청색 값의 배열로 구성된 원시 영상(raw image)을 생성할 수 있다. 제1 프로세서는 출력 영상 생성 모듈을 실행함으로써 피사체가 촬영된 출력 영상을 생성할 수 있다. 제1 프로세서는 원시 영상을 이용하여 출력 영상을 생성할 것을 제2 프로세서에게 요청할 수 있다. 이에 따라, 제2 프로세서는 톤 맵 생성 모델, 특징 추출 모델, 영상 보정 모델 및 영상 회복 모델을 포함하는 인공지능 모델에 원시 영상을 입력하고 인공지능 모델로부터 출력되는 출력 영상을 획득할 수 있다. 제2 프로세서는, 원시 영상을 톤 맵 생성 모델에 입력함으 로써 톤 맵을 획득하고, 생성된 톤 맵 및 원시 영상을 특징 추출 모델에 입력함으로써 피사체의 복수의 특징들에 관한 특징 영상들을 획득하고, 특징 영상들을 영상 보정 모델에 입력함으로써 복수의 특징 영상 들을 보정하고, 보정된 복수의 특징 영상들을 영상 회복 모델에 입력함으로써 디바이스에 저장될 출력 영상을 획득할 수 있다. 또한, 인공지능 모델로부터 출력되는 출력 영상은 제1 프로세서에 의 해 디스플레이부에 표시되고 제1 메모리에 저장될 수 있다. 또한, 사용자가 피사체를 촬영하는 상황에 관련된 인공지능 모델이 제1 메모리에 저장된 인공지능 모델들로부터 선택되어 제2 메모리에 로딩될 수 있으며, 이에 대하여는 후술하기로 한다.제2 프로세서는 원시 영상 내의 적색 값, 녹색 값 및 청색 값의 밝기(brightness)를 스케일링하기 위하여 이용되는 톤 맵을 생성하기 위하여, 원시 영상을 톤 맵 생성 모델에 입력할 수 있다. 예를 들어, 제2 프 로세서가 카메라 센서부 내에 포함되는 경우에, 제1 프로세서가 카메라 센서부에 대해 서 촬영을 위한 제어 요청 신호를 보내면, 카메라 센서부 내의 제2 프로세서가 제어 요청 신호를 수신하여 카메라 센서부를 제어하여 원시 영상을 생성하고, 제2 프로세서가 원시 영상을 톤 맵 생 성 모델에 입력할 수 있다. 또는, 예를 들어, 제2 프로세서가 카메라 센서부의 외부에 있는 경우에, 제1 프로세서가 촬영을 위한 제어 요청 신호를 제2 프로세서에게 보내면, 제2 프로세서 가 제어 요청 신호를 수신하여 카메라 센서부를 제어하여 원시 영상을 생성하고, 제2 프로세서 가 원시 영상을 톤 맵 생성 모델에 입력할 수 있다. 예를 들어, 제1 프로세서가 카메라 센서 부를 제어하여 원시 영상을 획득하고 획득된 원시 영상을 제2 프로세서에게 제공하며, 제2 프로세 서가 원시 영상을 톤 맵 생성 모델에 입력할 수 있다. 톤 맵은 원시 영상 내의 픽셀들의 밝기를 스 케일링하기 위한 정보를 포함하는 맵 데이터일 수 있다. 톤 맵은, 원시 영상 내의 부분 별로 픽셀의 밝기를 스 케일링하는 로컬톤 매핑 또는 원시 영상의 전체에 대한 밝기를 스케일링하는 글로벌톤 매핑 중 적어도 하나를 위한 맵 데이터일 수 있다. 또한, 예를 들어, 톤 맵은, 원시 영상 내에서 어두운 영역의 밝기를 밝은 영역의 밝 기보다 크게 스케링일하도록 생성될 수 있다. 톤 맵 생성 모델은 원시 영상으로부터 톤 맵을 생성하기 위 하여 훈련된 인공지능 모델일 수 있다. 톤 맵 생성 모델은 복수의 신경망 레이어들로 구성될 수 있으며, 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행할 수 있다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 톤 맵 생성 모델의 훈련 결과에 의해 최적화될 수 있으며, 예를 들어, 톤 맵 생성 모 델은 특징 추출 모델, 영상 보정 모델 및 영상 회복 모델과 함께 훈련될 수 있다. 또 한, 예를 들어, 톤 맵 생성 모델은 CNN (Convolutional Neural Network)를 포함할 수 있으나, 이에 제한 되지 않는다. 제2 프로세서는 원시 영상 및 톤 맵을 조합하여 특징 추출 모델에 입력하고 특징 추출 모델 로부터 출력되는 복수의 특징 영상들을 획득할 수 있다. 예를 들어, 제2 프로세서는 톤 맵을 이용하여 원 시 영상 내의 픽셀들의 밝기를 스케일링하고, 밝기가 스케일링된 원시 영상을 특징 추출 모델에 입력할 수 있다. 특징 추출 모델에 입력되는 원시 영상은, 어두운 부분의 픽셀이 밝은 값을 가지도록 스케일링될 수 있으며 이에 따라, 원시 영상 내의 어두운 부분에 위치한 피사체가 보다 효과적으로 식별될 수 있게 된다. 특징 추출 모델은 특징 추출 모델에 입력된 원시 영상 내의 특징들을 추출할 수 있다. 원시 영상 내의 복수의 특징들을 각각 나타내는 복수의 특징 영상들이 특징 추출 모델로부터 출력될 수 있다. 예를 들어, 복수의 특징 영상들은, 원시 영상 내의 엣지에 관한 특징을 나타내는 특징 영상, 원시 영상 내의 라인에 관한 특징을 나타내는 특징 영상, 원시 영상 내의 공간에 관련된 특징을 나타내는 특징 영상, 원시 영상 내의 객체의 형상 및 깊이에 관련된 특징을 나타내는 특징 영상, 원시 영상 내의 인물에 관련된 특징을 나타내는 특 징 영상 및 원시 영상 내의 사물에 관련된 특징을 나타내는 특징 영상 등을 포함할 수 있으나, 이에 제한되지 않는다. 특징 추출 모델은 특징 추출 모델에 입력된 원시 영상 내의 특징들을 추출하기 위하여 훈련된 인공 지능 모델일 수 있다. 특징 추출 모델은 복수의 신경망 레이어들로 구성될 수 있으며, 복수의 신경망 레 이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행할 수 있다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치 들은 특징 추출 모델의 훈련 결과에 의해 최적화될 수 있으며, 예를 들어, 특징 추출 모델은 톤 맵 생성 모델, 영상 보정 모델 및 영상 회복 모델과 함께 훈련될 수 있다. 또한, 예를 들어, 특 징 추출 모델은 후술할 도 5와 같이 End-to-End 방식의 Fully-Convolutional Network 기반 구조를 가지 는 U-NET에 의해 구현될 수 있으나, 이에 제한되지 않는다. 제2 프로세서는 특징 추출 모델로부터 출력된 특징 영상들을 보정할 수 있다. 제2 프로세서 는 기설정된 영상 속성들에 관련한 설정에 기초하여 특징 영상들을 보정할 수 있다. 예를 들어, 제2 프로세서 는 화이트 밸런스의 보정 및 색상 보정을 위한 기설정된 기준에 따라, 영상 보정 모델을 이용하여 특징 영상들을 보정할 수 있다. 제2 프로세서는 특징 추출 모델로부터 출력된 특징 영상들 및 영상 속성에 관련된 기설정된 속성 값들을 영상 보정 모델에 입력하고, 영상 보정 모델로부터 출력되는 보정된 특징 영상들을 획득할 수 있다. 이 경우, 제2 프로세서는 특징 영상들을 보정하기 위한 영상 속성을 결정할 수 있다. 예를 들어, 특징 영 상의 보정을 위한 영상 속성은, 디바이스의 촬영 환경에 따라 결정될 수 있다. 제2 프로세서는 카 메라 센서부를 통하여 피사체를 촬영하는 때의 디바이스 주변의 환경을 나타내는 센싱 데이터를 획 득하며, 센싱 데이터에 따라 미리 설정된 기준에 기초하여 특징 영상의 보정을 위한 화이트 밸런스 매트릭스 값 및 색상 보정 매트릭스 값을 영상 보정 모델에 특징 영상들과 함께 입력할 수 있다. 또는, 예를 들어, 제1 프로세서는 피사체 촬영의 설정을 위하여 GUI를 디바이스의 화면 상에 디스 플레이할 수 있으며, GUI를 통한 사용자 입력에 기초하여 화이트 밸런스의 보정 및 색상 보정 등을 위한 설정을 미리 수행할 수 있다. 예를 들어, 디바이스는 도 13a 및 도 13b에서와 같이 피사체를 촬영하는 디바이스의 촬영 설정을 위한 GUI를 디스플레이부 상에 디스플레이할 수 있다. 이에 따라, 사용자는 디스플레이부 상에 디 스플레이된 GUI를 통하여, ISO, 셔터 스피드, 화이트 밸런스, 색온도, 틴트, 대비, 채도, 하이라이트 효과, 쉐 도우 효과 등에 관련된 설정 값을 디바이스에 입력할 수 있다. 이 경우, 제1 프로세서는 사용자에 의해 미리 설정된 기준에 따라, 화이트 밸런스 매트릭스 값 및 색상 보정 매트릭스 값을 제1 메모리로부터 추출하고, 추출된 화이트 밸런스 매트릭스 값 및 색상 보정 매트릭 스 값을 영상 보정 모델에 특징 영상들과 함께 입력할 수 있다. 영상 보정 모델은 영상 보정 모델에 입력된 특징 영상들의 영상 속성을 보정하기 위하여 훈련된 인 공지능 모델일 수 있다. 영상 보정 모델은 복수의 신경망 레이어들로 구성될 수 있으며, 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행할 수 있다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치 들은 영상 보정 모델의 훈련 결과에 의해 최적화될 수 있으며, 예를 들어, 영상 보정 모델은 톤 맵 생성 모델, 특징 추출 모델 및 영상 회복 모델과 함께 훈련될 수 있다. 한편, 상기에서는, 화이트 밸런스 매트릭스 값 및 색상 보정 매트릭스 값 값이 영상 보정 모델에 입력되 는 것으로 설명되었지만 이에 제한되지 않는다. 복수의 인공지능 모델이 화이트 밸런스 및 색상 보정에 관한 설정 별로 각각 훈련될 수 있다. 이 경우에는, 출력 영상을 생성하기 위해 소정의 화이트 밸런스 및 색상 보정에 관한 설정에 대응되는 인공지능 모델이 제2 메모리에 로딩되어 제2 프로세서에 의해 이용될 수 있다. 제2 메모리에 로딩된 인공지능 모델에는 화이트 밸런스 매트릭스 값 및 색상 보정 매트릭스 값이 입력되지 않더라도, 제2 메모리에 로딩된 인공지능 모델로부터 소정의 화이트 밸런 스 및 색상 보정에 관한 설정을 고려한 출력 영상이 출력될 수 있다. 제2 프로세서는 보정된 특징 영상들을 영상 회복 모델에 입력하고, 영상 회복 모델로부터 출 력되는 출력 영상을 획득할 수 있다. 영상 회복 모델로부터 출력되는 출력 영상은 피사체를 촬영한 결과 물로서 디바이스에 저장될 영상일 수 있다. 또한, 피사체를 촬영하기 위한 사용자 입력이 수신되면 인공지능 모델을 통해 출력된 영상이 소정 기준에 따라 압축되고, 압축된 영상이 제1 메모리에 저장될 수 있으나 이에 제한되지 않는다. 영상 회복 모델은 특징 영상들로부터 피사체가 촬영된 촬영 영상을 생성하기 위하여 훈련된 인공지능 모 델일 수 있다. 영상 회복 모델은 복수의 신경망 레이어들로 구성될 수 있으며, 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행할 수 있다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 영상 회복 모델의 훈련 결과에 의해 최적화될 수 있으며, 예를 들어, 영상 회복 모델은, 톤 맵 생성 모 델, 특징 추출 모델 및 영상 보정 모델과 함께 훈련될 수 있다. 또한, 예를 들어, 영상 회복 모델은 CNN (Convolutional Neural Network)를 포함할 수 있으나, 이에 제한되지 않는다. 제1 프로세서는 라이브 뷰 생성 모듈을 실행함으로써, 라이브 뷰 영상을 생성할 수 있다. 피사체를 촬영하기 위한 촬영 기능을 활성화하는 사용자 입력이 수신되면, 제1 프로세서는 라이브 뷰 영상을 생성 하고 생성된 라이브뷰 영상을 디스플레이부 상에 디스플레이할 수 있다. 예를 들어, 디바이스에 설 치된 카메라 애플리케이션을 실행하는 사용자 입력이 수신되면, 제1 프로세서는 카메라 애플리케이션을 실행하고 사용자가 촬영될 피사체를 확인할 수 있도록 라이브 뷰 영상을 생성하여 디스플레이부 상에 디 스플레이할 수 있다. 제1 프로세서는, 디바이스의 촬영 기능이 활성화됨에 따라, 카메라 센서부를 통하여 입력되 는 광에 기초하여 라이브 뷰 영상을 생성하는데 이용되는 원시 영상을 생성할 수 있다. 또한, 제2 프로세서는 라이브 뷰 영상을 생성하는데 소요되는 시간을 단축시키기 위하여, 인공지능 모델 내의 모델들 중에서 적어도 하나를 이용하지 않을 수 있다. 제1 프로세서는 톤 맵 생성 모델, 특징 추출 모델 , 영상 보정 모델 또는 영상 회복 모델 중 적어도 하나를 이용하지 않을 수 있다. 이 경우, 라이브 뷰 영상의 생성을 위하여 톤 맵 생성 모델, 특징 추출 모델 및 영상 회복 모델 중 어 느 모델을 이용하지 않을 지는 미리 설정될 수 있다. 제2 프로세서는 적어도 하나의 모델이 비활성화된 인공지능 모델에 원시 영상을 입력할 수 있으며, 제2 프로세서는 인공지능 모델로부터 출력되는 라이브 뷰 영상을 제1 프로세서에게 제공하며, 제1 프로세서는 라이브 뷰 영상을 디스플레이부 상에 디스플레이할 수 있다. 이 경우, 인공지능 모델은 인공지능 모델 내의 모델들 중에서 적어도 하나가 비활성화된 상태에서 양호한 라 이브뷰 영상이 출력될 수 있도록 미리 훈련될 수 있다. 이에 따라, 디바이스의 카메라 애플리케이션이 실행되어 촬영 기능이 활성화되면, 디바이스의 적은 리소스를 사용하여 생성되는 라이브 뷰 영상이 디바이스의 화면 상에 디스플레이될 수 있으며, 피사체를 촬영하는 사용자의 캡쳐 입력이 수신되면 캡쳐 입력이 수신될 때 생성된 원시 영상으로부터 인공지능 모델 을 통해 생성되는 출력 영상이 제1 메모리에 저장될 수 있다. 제1 프로세서는 모델 관리 모듈을 실행함으로써, 제2 프로세서에 의해 이용될 인공지능 모 델을 선택하고, 제1 메모리에 저장된 인공지능 모델을 업데이트할 수 있다. 제1 프로세서는 모델 선택 모듈을 실행함으로써, 제1 메모리에 저장된 적어도 하나의 인공지 능 모델 중에서 제2 프로세서에 의해 이용될 인공지능 모델을 선택할 수 있다. 또한, 제1 프 로세서에 의해 선택된 인공지능 모듈은 AI 프로세싱 유닛의 제2 메모리에 로딩될 수 있다. 제1 메모리에는 복수의 인공지능 모델이 저장될 수 있으며, 복수의 인공지능 모델은 복수의 상황에 따라 각각 훈련된 인공지능 모델일 수 있다. 예를 들어, 카메라 필터, 카메라 렌즈, 카메라의 제 조사, 디바이스 모델, 복수의 연속 촬영된 이미지, 촬영 환경, 피사체의 종류 또는 촬영 영상의 속성 중 적어도 하나에 관련된 상황을 기준으로 훈련된 인공지능 모델들일 수 있다. 예를 들어, 카메라 필터 별로 훈련된 인공지능 모델들은, 비선명 마스크를 이용하여 촬영된 이미지들을 바탕으 로 훈련된 인공지능 모델, Contrast 조정 마스크를 이용하여 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델 및 컬러 필터 마스크를 이용하여 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델을 포함할 수 있으나, 이에 제한되지 않는다. 또한, 예를 들어, 카메라 렌즈 별로 훈련된 인공지능 모델들은, 망원 렌즈를 이용하여 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델, 광각 렌즈를 이용하여 촬영된 이미지들을 바탕으로 훈련된 인공지 능 모델, 어안 렌즈를 이용하여 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델을 포함할 수 있으나, 이에 제한되지 않는다. 또한, 예를 들어, 카메라의 제조사 별로 훈련된 인공지능 모델들은, 제조사 A의 카메라를 이 용하여 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델, B 제조사 B의 카메라를 이용하여 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델, 제조사 C의 카메라를 이용하여 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델을 포함할 수 있으나, 이에 제한되지 않는다. 또한, 예를 들어, 디바이스 별로 훈련된 인공지능 모델들은 갤럭시 S10으로 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델, 갤럭시 S20으로 촬영된 이미지들을 바탕으 로 훈련된 인공지능 모델, 갤럭시 노트 20으로 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델을 포함할 수 있으나, 이에 제한되지 않는다. 또한, 예를 들어, 촬영 환경 별로 훈련된 인공지능 모델은, 실내에서 촬영된 이미지들을 바탕으로 훈련된 인공 지능 모델, 야외에서 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델, 특정 조도 범위에서 촬영된 이미지들 을 바탕으로 훈련된 인공지능 모델을 포함할 수 있으나, 이에 제한되지 않는다. 또한, 예를 들어, 피사체의 종 류 별로 훈련된 인공지능 모델은, 인물이 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델, 음식이 촬영된 이 미지들을 바탕으로 훈련된 인공지능 모델, 건물이 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델을 포함할 수 있으나 이에 제한되지 않는다. 또한, 예를 들어, 촬영 영상의 속성 별로 훈련된 인공지능 모델은, 특정 화이 트 밸런스 값을 적용하여 촬영된 이미지들을 바탕으로 훈련된 인공지능 모델, 특정 ISO를 적용하여 촬영된 이미 지들을 바탕으로 훈련된 인공지능 모델, 특정 셔터 스피드에서 촬영된 이미지들을 바탕으로 훈련된 인공지능 모 델을 포함할 수 있으나, 이에 제한되지 않는다. 디바이스의 카메라 애플리케이션이 실행되고 카메라 기능이 활성화되면 제1 프로세서는 모델 선택 모듈을 실행하여 카메라 필터, 카메라 렌즈, 카메라의 제조사, 디바이스 모델, 복수의 연속 촬영된 이미 지, 촬영 환경, 피사체의 종류 또는 촬영 영상의 속성 중 적어도 하나에 관련된 상황을 식별할 수 있다. 예를 들어, 제1 프로세서는 디바이스의 주변 상황을 센싱한 센싱 값 및 카메라 애플리케이션의 설정 값 에 기초하여, 적어도 하나의 상황을 식별할 수 있다. 또한, 제1 프로세서는 사진 촬영의 설정을 위한 소정의 GUI를 디스플레이부 상에 디스플레이하고, GUI에 대한 사용자 입력에 기초하여 설정된 값에 기초하여, 적어도 하나의 상황을 식별할 수도 있다. 예를 들어, 제1 프로세서는 도 13a 및 도 13b에서와 같이 피사체를 촬영하는 디바이스의 촬영 설정을 위 한 GUI를 디스플레이부 상에 디스플레이할 수 있다. 이에 따라, 사용자는 디스플레이부 상에 디스 플레이된 GUI를 통하여, ISO, 셔터 스피드, 화이트 밸런스, 색온도, 틴트, 대비, 채도, 하이라이트 효과, 쉐도 우 효과 등에 관련된 설정 값을 디바이스에 입력할 수 있다. 제1 프로세서는, 제1 메모리에 저장된 인공지능 모델 중에서, 식별된 적어도 하나의 상황에 대응되는 인공지능 모델을 추출하여 제2 메모리에 로딩할 수 있다. 예를 들어, 사용자는 디바이스 의 디스플레이부 상에 디스플레이된 GUI를 통하여, ISO, 화이트 밸런스, 색온도, 틴드, 채도 및 대 비 등에 관련된 설정을 할 수 있으며, 디바이스는 사용자에 의해 설정된 값들에 기초하여, 제1 메모리 에 저장된 복수의 인공지능 모델 중에서 설정 값에 대응되는 인공지능 모델을 선택하고, 선 택된 인공지능 모델을 제2 메모리에 로딩할 수 있다. 또한, 제1 프로세서는, 예를 들어, 사용자가 소정 기준 이상 사용한 카메라 필터, 사용자가 소정 기준 이 상 사용한 카메라 렌즈, 사용자가 소정 기준 이상 사용한 카메라의 제조사, 사용자가 소정 기준 이상 사용한 디 바이스 모델, 소정 횟수 이상의 촬영 환경, 사용자가 소정 기준 이상 촬영된 피사체의 종류 또는 사용자가 소정 기준 이상 사용한 촬영 영상의 속성 중 적어도 하나에 관한 정보에 기초하여, 사용자의 선호에 대응되는 인공지 능 모델을 제1 메모리로부터 추출하여 제2 메모리에 로딩할 수 있다. 또한, 제1 프로세서는, 예를 들어, 기설정된 이미지들을 디스플레이하고, 디스플레이된 이미지들 중에서 사용자에 의해 선택된 적어도 하나의 이미지에 기초하여, 사용자에 의해 선택된 이미지와 유사한 특징의 이미지 를 출력하도록 훈련된 인공지능 모델을 제2 메모리에 로딩할 수 있다. 예를 들어, 사용자가 엣지가 강조된 이미지를 많이 선택하면 엣지가 강조된 이미지를 출력하도록 훈련된 인공지능 모델이 제2 메모리 에 로딩될 수 있다. 만약, 식별된 적어도 하나의 상황에 대응되는 인공지능 모델이 제1 메모리에 저장되어 있지 않으면, 제1 프로세서는 서버(미도시)에게 인공지능 모델을 요청하고, 서버(미도시)로부터 인공지 능 모델을 수신하여 제2 메모리에 저장할 수 있다. 제1 프로세서는 다운로딩 모듈을 실행함으로써, 리트레이닝된 인공지능 모델, 또는 인공지능 모델을 리트레이닝하기 위한 데이터를 서버(미도시)로부터 수신할 수 있다. 제1 프로세서는 리트레 이닝된 인공지능 모델을 서버(미도시)에게 요청할 수 있다. 인공지능 모델은 서버(미도시)에 의해 리트레이닝될 수 있으며, 서버(미도시)는 리트레이닝된 인공지능 모델이 존재함을 알리는 알림 정보를 디 바이스에게 제공할 수 있다. 또한, 제1 프로세서는 서버(미도시)로부터 수신된 알림 정보를 화면 상에 디스플레이하고, 인공지능 모델을 업데이트하기 위한 사용자 입력을 수신할 수 있다. 제1 프로세서 는 서버(미도시)에게 사용자가 선호하는 사진 속성에 관한 정보를 제공하면서 서버(미도시)에게 리트레이 닝된 인공지능 모델을 요청할 수도 있다. 사용자가 선호하는 사진 속성에 관한 정보는, 예를 들어, 사용 자가 소정 기준 이상 사용한 카메라 필터, 사용자가 소정 기준 이상 사용한 카메라 렌즈, 사용자가 소정 기준 이상 사용한 카메라의 제조사, 사용자가 소정 기준 이상 사용한 디바이스 모델, 소정 횟수 이상의 촬영 환경, 사용자가 소정 기준 이상 촬영된 피사체의 종류 또는 사용자가 소정 기준 이상 사용한 촬영 영상의 속성 중 적 어도 하나에 관한 정보를 포함할 수 있다. 제1 프로세서가 사용자가 선호하는 사진 속성에 관한 정보를 서버(미도시)에게 제공한 경우에는, 제1 프로세서는 사용자가 선호하는 사진 속성과 관련하여 리트레이닝 된 인공지능 모델을 서버(미도시)로부터 다운로드할 수 있다. 또한, 제1 프로세서는 업데이트 모듈 을 실행함으로써, 디바이스 내의 인공지능 모델을 서버(미도시)로부터 수신된 리트레이닝된 인공지능 모델로 대체함으로써, 디바이스 내의 인공지능 모델을 업데이트할 수 있다. 제1 프로세서는 다운로딩 모듈을 실행함으로써, 리트레이닝을 위한 기준 원시 영상 및 기준 원시 영상에 대응되는 기준 영상을 서버(미도시)로부터 다운로드할 수 있다. 기준 영상은 리트레이닝을 위한 기준 원 시 영상으로부터 생성된 영상일 수 있다. 제1 프로세서는 서버(미도시)에게 인공지능 모델의 리트레이닝을 위한 기준 원시 영상 및 기준 영상을 요청하고, 서버(미도시)로부터 리트레이닝을 위한 기준 원시 영 상 및 기준 영상을 수신할 수 있다. 이 경우, 제1 프로세서는 사용자가 선호하는 사진 속성에 관한 정보 를 서버(미도시)에게 제공할 수 있다. 사용자가 선호하는 사진 속성에 관한 정보는, 예를 들어, 사용자가 소정 기준 이상 사용한 카메라 필터, 사용자가 소정 기준 이상 사용한 카메라 렌즈, 사용자가 소정 기준 이상 사용한 카메라의 제조사, 사용자가 소정 기준 이상 사용한 디바이스 모델, 소정 횟수 이상의 촬영 환경, 사용자가 소정 기준 이상 촬영된 피사체의 종류 또는 사용자가 소정 기준 이상 사용한 촬영 영상의 속성 중 적어도 하나에 관 한 정보를 포함할 수 있다. 이 경우, 서버(미도시)는 사용자가 선호하는 사진 속성과 관련하여 생성된 기준 영 상 및 기준 원시 영상을 디바이스에게 제공할 수 있다. 제1 프로세서는 리트레이닝 모듈을 실행함으로써 리트레이닝을 위한 기준 원시 영상 및 기준 원시 영상에 대응되는 기준 영상을 획득하여 제2 프로세서가 인공지능 모델을 리트레이닝할 수 있다. 제 2 프로세서는 서버(미도시)로부터 수신된 기준 영상을 정답 영상으로 이용할 수 있으며, 제2 프로세서 는 서버(미도시)로부터 수신된 기준 원시 영상을 인공지능 모델에 입력하고 인공지능 모델로 부터 출력되는 출력 영상을 기준 영상과 비교함으로써 인공지능 모델을 리트레이닝할 수 있다. 한편, 상기에서는 디바이스가 디바이스 내의 인공지능 모델을 이용하여 피사체가 촬영된 출 력 영상을 생성하는 것으로 설명되었지만, 이에 제한되지 않는다. 디바이스는 서버(미도시)와 함께 피사 체가 촬영된 출력 영상을 생성할 수도 있다. 예를 들어, 디바이스는 원시 영상을 생성하고 생성된 원시 영상을 서버(미도시)로 전송하면서 서버(미도시)에게 출력 영상을 요청할 수 있다. 디바이스는 원시 영상 및 영상 보정을 위한 설정 정보를 서버(미도시)에게 함께 제공할 수도 있다. 이 경우, 인공지능 모델은 서버(미도시)에 포함될 수 있으며, 서버(미도시)는 서버(미도시) 내의 인공지능 모델을 이용하여 원시 영 상으로부터 출력 영상을 생성할 수 있다. 또한, 서버(미도시)는 생성된 출력 영상을 디바이스에게 제공할 수 있다. 또는, 예를 들어, 디바이스는 톤 맵 생성 모델로부터 출력된 톤 맵 및 원시 영상을 서버(미도시)에 게 제공하면서 서버(미도시)에게 출력 영상을 요청할 수 있다. 디바이스는 원시 영상, 톤 맵 및 영상 보 정을 위한 설정 정보를 서버(미도시)에게 함께 제공할 수도 있다. 이 경우, 인공지능 모델 내의 특징 추 출 모델, 영상 보정 모델 및 영상 회복 모델은 서버(미도시)에 포함될 수 있으며, 서버(미도 시)는 서버(미도시) 내의 특징 추출 모델, 영상 보정 모델 및 영상 회복 모델을 이용하여 출 력 영상을 생성할 수 있다. 또한, 서버(미도시)는 생성된 출력 영상을 디바이스에게 제공할 수 있다. 또는, 예를 들어, 디바이스는 특징 추출 모델로부터 출력된 특징 영상들을 서버(미도시)에게 제공 하면서 서버(미도시)에게 출력 영상을 요청할 수 있다. 디바이스는 특징 영상들 및 영상 보정을 위한 설 정 정보를 서버(미도시)에게 함께 제공할 수도 있다. 이 경우, 인공지능 모델 내의 영상 보정 모델 및 영상 회복 모델은 서버(미도시)에 포함될 수 있으며, 서버(미도시)는 서버(미도시) 내의 영상 보정 모 델 및 영상 회복 모델을 이용하여 출력 영상을 생성할 수 있다. 또한, 서버(미도시)는 생성된 출력 영상을 디바이스에게 제공할 수 있다. 또한, 디바이스는 디바이스의 상황에 따라, 디바이스가 혼자 출력 영상을 생성할지, 디바이 스가 서버(미도시)와 함께 출력 영상을 생성할 지를 결정할 수 있다. 예를 들어, 디바이스는 배터 리 잔량, 리소스 사용량 및 통신 상태 등을 고려하여 디바이스가 혼자서 출력 영상을 생성할지, 디바이스 가 서버(미도시)와 함께 출력 영상을 생성할 지를 결정할 수 있다. 예를 들어, 디바이스의 배터리 잔량이 임계치보다 작은 경우에, 디바이스는 서버(미도시)에게 출력 영상을 생성할 것을 요청할 수 있다. 또한, 예를 들어, 디바이스의 리소스 사용량이 임계치보다 큰 경우에, 디바이스는 서버(미도시)에 게 출력 영상을 생성할 것을 요청할 수 있다. 또한, 예를 들어, 디바이스는 디바이스의 통신 상태 가 양호한 경우에, 서버(미도시)에게 출력 영상을 생성할 것을 요청할 수 있다. 이 경우, 디바이스가 서 버(미도시)에게 출력 영상을 요청하면서, 원시 영상을 제공할지, 원시 영상 및 톤 맵을 제공할 지, 특징 영상들 을 제공할지는, 디바이스의 상황에 따른 다양한 기준에 따라 설정될 수 있다. 예를 들어, 촬영 기능이 활 성화되면, 디바이스는 배터리 잔량, 리소스 사용량 및 통신 상태 등을 식별할 수 있으며, 서버(미도시)에 게 원시 영상을 제공할지, 원시 영상 및 톤 맵을 제공할 지, 특징 영상들을 제공할 지를 결정할 수 있다. 또한, 디바이스는 결정에 기초하여 원시 영상, 톤 맵, 특징 영상들 중 적어도 하나를 서버(미도시)에게 제공하 면서 출력 영상을 제공해 줄 것을 요청할 수 있다. 도 3은 본 개시의 일 실시예에 따른 원시 영상으로부터 피사체가 촬영된 출력 영상이 생성되는 과정을 설명하는 도면이다. 도 3을 참조하면, 디바이스는, AI 프로세싱 유닛을 이용하여, 피사체에 대한 원시 영상을 톤 맵 생성 모델에 입력하고 톤 맵 생성 모델로부터 출력되는 톤 맵을 획득할 수 있다. 또한, 디바이 스는 원시 영상 및 톤 맵을 조합하여 특징 추출 모델에 입력하고 특징 추출 모델로부터 출력되는 복수의 특징 영상들을 획득할 수 있다. 이후, 디바이스는 복수의 특징 영상들을 영상 보정 모델 에 입력하고, 영상 보정 모델로부터 출력되는 보정된 특징 영상들을 획득할 수 있다. 이후, 디바이 스는 보정된 특징 영상들을 영상 회복 모델에 입력함으로써 영상 회복 모델로부터 출력되는 출력 영상을 획득할 수 있다. 톤 맵 생성 모델, 특징 추출 모델, 영상 보정 모델 및 영상 회복 모델 각각은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있 으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행한다. 복수 의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도 록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으 며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 톤 맵 생성 모델, 특징 추출 모델, 영상 보정 모델 및 영상 회복 모델은, 후술할 도 7 에서와 같이, 기준 원시 영상 및 기준 출력 영상을 이용하여 함께 훈련될 수 있다. 도 4a는 본 개시의 일 실시예에 따른 디바이스가 원시 영상으로부터 톤 맵을 생성하는 과정을 설명하 는 도면이다. 도 4a를 참조하면, 디바이스의 카메라 애플리케이션이 실행되고 촬영 기능이 활성화됨에 따라, 디바이스 는 원시 영상을 생성할 수 있다. 디바이스는 피사체를 촬영하기 위한 기능이 활성화됨에 따라 카메라 센서부를 이용하여 피사체로부터 제공되는 광에 기초한 원시 영상들을 생성할 수 있으며, 피사체 를 촬영하는 사용자 입력이 수신됨에 따라 사용자 입력이 수신된 때에 카메라 센서부에 의해 생성된 원시 영상을 획득할 수 있다. 원시 영상은, 예를 들어, 적색 값, 녹색 값 및 청색 값의 배열로 구성될 수 있 다. 또한, 디바이스는 촬영 기능이 활성화됨에 따라, 피사체 촬영을 위한 적어도 하나의 상황을 식별할 수 있 으며, 식별된 적어도 하나의 상황에 대응되는 인공지능 모델을 제1 메모리로부터 추출하여 AI 프로 세싱 유닛 내의 제2 메모리에 로딩할 수 있다. 만약, 식별된 적어도 하나의 상황에 대응되는 인공 지능 모델이 제1 메모리에 저장되어 있지 않은 경우에는, 디바이스는 식별된 적어도 하나의 상황에 대응되는 인공지능 모델을 서버(미도시)에게 요청하고, 서버(미도시)로부터 인공지능 모델 을 수신하여 제2 메모리에 로딩할 수 있다. 이후, 디바이스는 원시 영상 내의 적색 값, 녹색 값 및 청색 값의 밝기(brightness)를 스케일링하기 위하여 이용되는 톤 맵을 생성하기 위하여, 카메라 애플리케이션이 실행되고 피사체를 촬영하는 사용자 입 력을 수신하고 수신된 사용자 입력에 응답하여, 원시 영상을 톤 맵 생성 모델에 입력할 수 있다. 톤 맵은 원시 영상 내의 픽셀들의 밝기를 스케일링하기 위한 정보를 포함할 수 있다. 또한, 예를 들어, 톤 맵은, 원시 영상 내에서 어두운 영역의 밝기를 밝은 영역의 밝기보다 크게 스케링일하도록 생성될 수 있다. 톤 맵 생성 모델은 복수의 신경망 레이어들로 구성될 수 있으며, 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행할 수 있다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 톤 맵 생성 모델 의 훈련 결과에 의해 최적화될 수 있으며, 예를 들어, 톤 맵 생성 모델은 특징 추출 모델, 영상 보정 모델 및 영상 회복 모델과 함께 훈련될 수 있다. 또한, 예를 들어, 톤 맵 생성 모델 은 CNN (Convolutional Neural Network)를 포함할 수 있으나, 이에 제한되지 않는다. 도 4b는 본 개시의 일 실시예에 따른 디바이스가 원시 영상으로부터 특징들을 추출하는 과정을 설명 하는 도면이다.도 4b를 참조하면, 디바이스는 원시 영상 및 톤 맵을 조합하여 특징 추출 모델에 입력하고 특징 추출 모델로부터 출력되는 복수의 특징 영상들을 획득할 수 있다. 예를 들어, 디바이스는 톤 맵을 이용하여 원시 영상 내의 픽셀들의 밝기를 스케일링하고, 밝기가 스케일링된 원시 영상을 특징 추출 모델에 입력할 수 있다. 특징 추출 모델에 입력되는 원시 영상은, 어두운 부분의 픽 셀이 밝은 값을 가지도록 스케일링될 수 있으며 이에 따라, 원시 영상 내의 어두운 부분에 위치한 피사체가 보다 효과적으로 식별될 수 있게 된다. 특징 추출 모델은 특징 추출 모델에 입력된 원시 영상 내의 특징들을 추출할 수 있다. 원시 영 상 내의 복수의 특징들을 각각 나타내는 복수의 특징 영상들이 특징 추출 모델로부터 출력될 수 있다. 예를 들어, 복수의 특징 영상들은, 원시 영상 내의 엣지에 관한 특징을 나타내는 특징 영상, 원 시 영상 내의 라인에 관한 특징을 나타내는 특징 영상, 원시 영상 내의 공간에 관련된 특징을 나타내는 특징 영상, 원시 영상 내의 객체의 형상 및 깊이에 관련된 특징을 나타내는 특징 영상, 원시 영상 내의 인물에 관련된 특징을 나타내는 특징 영상 및 원시 영상 내의 사물에 관련된 특징을 나타내는 특징 영상 등 을 포함할 수 있으나, 이에 제한되지 않는다. 특징 추출 모델은 복수의 신경망 레이어들로 구성될 수 있으며, 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행할 수 있다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 특징 추출 모델 의 훈련 결과에 의해 최적화될 수 있으며, 예를 들어, 특징 추출 모델은 톤 맵 생성 모델, 영상 보정 모델 및 영상 회복 모델과 함께 훈련될 수 있다. 또한, 예를 들어, 특징 추출 모델 은 도 5와 같이 End-to-End 방식의 Fully-Convolutional Network 기반 구조를 가지는 U-NET에 의해 구현 될 수 있으나, 이에 제한되지 않는다. 도 4c는 본 개시의 일 실시예에 따른 디바이스가 특징 영상들을 보정하는 과정을 설명하는 도면이다. 도 4c를 참조하면, 디바이스는 특징 추출 모델로부터 출력된 특징 영상들을 보정할 수 있다. 디바이스는 특징 추출 모델로부터 출력된 특징 영상들 및 영상 속성들에 관련한 설정 값들을 영상 보정 모델에 입력하여 특징 영상들을 보정할 수 있다. 예를 들어, 디바이스는 화이트 밸 런스의 보정 및 색상 보정을 위한 기설정된 기준에 따라, 특징 영상들을 보정할 수 있다. 예를 들어, 디바이스의 제1 프로세서는 디바이스 내의 센서를 이용하여 피사체를 촬영하는 때의 디바이스 주변의 환경을 나타내는 센싱 데이터를 생성하며, 생성된 센싱 데이터에 따라 디바이스 의 주변 환경을 자동으로 식별하고, 미리 설정된 기준에 기초하여 특징 영상의 보정을 위한 화이트 밸런 스 매트릭스 및 색상 보정 매트릭스를 제1 메모리로부터 추출할 수 있다. 또한, 제1 프로세서 는 추출된 화이트 밸런스 매트릭스 및 색상 보정 매트릭스를 제2 프로세서에게 제공할 수 있다. 또한, 제2 프로세서는 제1 프로세서로부터 제공받은 화이트 밸런스 매트릭스 및 색상 보 정 매트릭스를 영상 보정 모델에 특징 영상들과 함께 입력할 수 있다. 또는, 예를 들어, 디바이스의 제1 프로세서는 촬영 기능이 활성화됨에 따라 특징 영상들의 화 이트 밸런스, 색상을 보정하기 위한 설정을 위하여 GUI를 디바이스의 디스플레이부 상에 디스플레 이할 수 있으며, GUI를 통한 사용자 입력에 기초하여 화이트 밸런스의 보정 및 색상 보정을 위한 설정을 미리 수행할 수 있다. 또한, 제1 프로세서는 사용자 입력에 따른 설정에 기초하여 제1 메모리로부터 화 이트 밸런스 매트릭스 및 색상 보정 매트릭스를 추출하고, 화이트 밸런스 매트릭스 및 색상 보정 매트릭스를 제2 프로세서에게 제공할 수 있다. 또한, 제2 프로세서는 제1 프로세서로부 터 제공받은 화이트 밸런스 매트릭스 및 색상 보정 매트릭스를 영상 보정 모델에 특징 영상들 과 함께 입력할 수 있다. 상기에서는 디바이스가 영상 보정 모델을 이용하여 특징 영상들을 보정하는 것으로 설명되었지 만, 이에 제한되지 않는다. 디바이스는 영상 보정 모델을 이용하지 않고 특징 영상들을 보정할 수 있다. 이 경우, 디바이스는 영상 속성을 보정하기 위한 매트릭스를 이용하여 특징 영상들을 보정할 수 있다. 예를 들어, 디바이스는 화이트 밸런스를 보정하기 위한 매트릭스를 특징 영상들에 각각 곱 함(multiply)으로써 특징 영상들의 화이트 밸런스를 보정할 수 있다. 또한, 디바이스는 색상을 보정 하기 위한 매트릭스를 특징 영상들에 각각 곱함으로써 특징 영상들의 색상을 보정할 수 있다. 한편, 상기에서는, 화이트 밸런스 매트릭스 값 및 색상 보정 매트릭스 값 값이 영상 보정 모델에 입력되 는 것으로 설명되었지만 이에 제한되지 않는다. 복수의 인공지능 모델이 화이트 밸런스 및 색상 보정에 관한 설정 별로 각각 훈련될 수 있다. 이 경우에는, 출력 영상을 생성하기 위해 소정의 화이트 밸런스 및 색상 보정에 관한 설정에 대응되는 인공지능 모델이 제2 메모리에 로딩되어 제2 프로세서에 의해 이용될 수 있다. 또한, 제2 메모리에 로딩된 인공지능 모델에는 화이트 밸런스 매트릭스 값 및 색 상 보정 매트릭스 값이 입력되지 않더라도, 제2 메모리에 로딩된 인공지능 모델로부터 소정의 화이 트 밸런스 및 색상 보정에 관한 설정을 고려한 출력 영상이 출력될 수 있게 된다. 도 4d는 본 개시의 일 실시예에 따른 디바이스가 보정된 특징 영상들로부터 출력 영상을 생성하는 과정을 설명하는 도면이다. *디바이스는 보정된 특징 영상들을 영상 회복 모델에 입력하고, 영상 회복 모델로부터 출력되는 출력 영상을 획득할 수 있다. 영상 회복 모델로부터 출력되는 출력 영상은 피사체를 촬 영한 결과물로서 디바이스에 저장될 영상일 수 있다. 영상 회복 모델은 복수의 신경망 레이어들로 구성될 수 있으며, 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전 (previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행할 수 있다. 복수의 신 경망 레이어들이 갖고 있는 복수의 가중치들은 영상 회복 모델의 훈련 결과에 의해 최적화될 수 있으며, 예를 들어, 영상 회복 모델은, 톤 맵 생성 모델 및 특징 추출 모델과 함께 훈련될 수 있다. 또한, 예를 들어, 영상 회복 모델은 CNN (Convolutional Neural Network)를 포함할 수 있으나, 이에 제 한되지 않는다. 도 5는 본 개시의 일 실시예에 따른 특징 추출 모델의 구조의 일례를 나타내는 도면이다. 도 5를 참조하면, 특징 추출 모델은 End-to-End 방식의 Fully-Convolutional Network 기반 구조를 가지 는 U-NET에 의해 구현될 수 있다. 특징 추출 모델은 입력된 영상의 컨텍스트를 포착하기 위한 목적으로 구성된 수축 단계(contraction path)의 레이어들 및 수축 단계의 특징 영상으로부터 높은 해상도의 결과를 얻기 위한 업 샘플링(Up-Sampling)을 수행하는 팽창 단계(expanding path)의 레이어들을 포함할 수 있다. 또한, 수축 단계(contraction path)의 레이어들 및 팽창 단계(expanding path)의 레이어들은 서로 대칭되는 구조를 가질 수 있다. 도 6은 본 개시의 일 실시예에 따른 디바이스가 피사체를 촬영하여 출력 영상을 생성하는 방법의 흐름도 이다. 동작 S600에서 디바이스는 디바이스 주변의 피사체를 촬영하는 사용자 입력을 수신하고, 동작 S605 에서 디바이스는 카메라 센서부를 통하여 입력되는 광에 기초하여, 원시 영상을 생성할 수 있다. 디바이스는 피사체를 촬영하기 위한 기능이 활성화됨에 따라 카메라 센서부를 이용하여 피사체로부 터 제공되는 광에 기초한 원시 영상들을 생성할 수 있으며, 피사체를 촬영하는 사용자 입력이 수신됨에 따라 사 용자 입력이 수신된 때에 카메라 센서부에 의해 생성된 원시 영상을 획득할 수 있다. 동작 S610에서 디바이스는 원시 영상을 톤 맵 생성 모델에 입력할 수 있다. 디바이스는 원시 영상을 톤 맵 생성 모델에 입력하고, 톤 맵 생성 모델로부터 출력되는 톤 맵을 획득할 수 있다. 동작 S615에서 디바이스는 생성된 톤 맵 및 원시 영상을 특징 추출 모델에 입력할 수 있다. 디바이 스는 톤 맵을 이용하여 원시 영상 내의 픽셀들의 밝기를 스케일링하고, 밝기가 스케일링된 원시 영상을 특징 추출 모델에 입력할 수 있다. 또한, 디바이스는 특징 추출 모델로부터 출력되는 특징 영상들 을 획득할 수 있다. 동작 S620에서 디바이스는 특징 추출 모델에 의해 생성된 특징 영상들을 기설정된 기준에 따라 보 정할 수 있다. 디바이스는 기설정된 영상 속성들에 관련한 설정에 기초하여 특징 영상들을 보정할 수 있 다. 예를 들어, 디바이스는 화이트 밸런스의 보정 및 색상 보정을 위한 기설정된 기준에 따라, 특징 영상 들을 보정할 수 있으나, 특징 영상의 보정을 위한 기설정된 기준은 이에 제한되지 않는다. 동작 S625에서 디바이스는 보정된 특징 영상들을 영상 회복 모델에 입력하고, 동작 S630에서 디바 이스는 영상 회복 모델로부터 출력되는 출력 영상을 저장할 수 있다. 도 7은 본 개시의 일 실시예에 따른 인공지능 모델이 훈련되는 예시를 나타내는 도면이다. 도 7을 참조하면, 기준 원시 영상에 대한 ISP 프로세싱을 통해 생성되는 기준 영상이 인공지능 모델 의 훈련을 위한 정답 영상(GT image: Ground Truth image)으로 이용될 수 있다. 예를 들어, 피사체가 짧은 시간 구간동안 연속 촬영됨으로써 생성되는 복수의 원시 영상들이 조합됨으로써 기준 원시 영상이 생성될 수 있 으며, 기준 원시 영상에 대한 기존의 ISP 프로세싱을 통해 기준 영상이 출력될 수 있다. 기존의 ISP 프로세 싱은 인공지능 모델을 이용하지 않고 원시 영상을 영상 처리하는 기존의 프로세싱일 수 있으며, 예를 들어, 원 시 영상에 대한 전처리, 화이트 밸런스 조정, 디모자이킹, 감마 보정, 색 변환 등의 처리를 인공지능 모델을 이 용하지 않고 수행하는 이미지 프로세싱일 수 있다. 출력된 기준 영상은 인공지능 모델의 정답 영상(GT image: Ground Truth image)으로 이용될 수 있다. 또는, 피사체가 짧은 시간 구간동안 연속 촬영됨으로써 생성 되는 복수의 원시 영상들 중 하나가 선택되고, 선택된 원시 영상으로부터 기존의 ISP 프로세싱을 통해 출력 된 영상이 정답 영상으로 이용될 수 있다. 인공지능 모델의 훈련을 위하여, 연속 촬영된 복수의 원시 영상들 중 하나 또는 기준 원시 영상이 인 공지능 모델에 입력될 수 있다. 예를 들어, 버스트 샷에 의해 8장의 원시 영상들이 생성되는 경우에, 8 장의 원시 영상을 조합하여 기준 원시 영상이 생성될 수 있으며, 기준 원시 영상 및 기준 원시 영상으로부터 기 존의 ISP 프로세싱을 통해 생성되는 기준 영상이 인공지능 모델의 훈련에 이용될 수 있다. 또는, 예를 들 어, 버스트 샷에 의해 8장의 원시 영상들이 생성되는 경우에, 8장의 원시 영상들 각각으로부터 기존의 ISP 프로 세싱을 통해 8장의 출력 영상들이 생성될 수 있다. 이 경우, 8장의 원시 영상들 중 하나가 선택되고, 선택된 원 시 영상 및 선택된 원시 영상에 대응되는 출력 영상이 인공지능 모델의 훈련을 위하여 이용될 수 있다. 또한, 특징 영상들의 영상 속성을 보정하기 위한 설정 정보가 인공지능 모델에 입력될 수 있다. 영상 속 성을 보정하기 위한 설정 정보는, 예를 들어, 기설정된 화이트 밸런스 보정을 위한 매트릭스 및 기설정된 색상 보정을 위한 매트릭스를 포함할 수 있다. 다양한 영상 속성에 대하여 인공지능 모델이 훈련될 수 있도록 인공지능 모델에 다양한 설정에 따른 설정 정보가 입력될 수 있다. 또한, 소정의 노이즈가 포함된 기준 원시 영상이 인공지능 모델의 훈련을 위하여 인공지능 모델에 입력될 수 있다. 이 경우, 점차적으로 많은 노이즈를 포함하는 기준 원시 영상들을 인공지능 모델에 입력 하여 인공지능 모델이 훈련될 수 있다. 예를 들어, 서버(미도시) 또는 디바이스는 기준 원시 영상 에 제1~n 레벨의 노이즈를 각각 포함시켜, n 개의 입력 영상을 생성하고 n개의 입력 영상을 인공지능 모델 에 각각 입력하여 인공지능 모델을 훈련시킬 수 있다. 이에 따라, 훈련된 인공지능 모델은 노이즈가 포함된 원시 영상으로부터 디노이징된 출력 영상을 출력하도록 훈련될 수 있다. 또한, 기준 영상 및 인공지능 모델로부터 출력되는 출력 영상을 비교함으로써 기준 영상와 출력 영상 간의 로스(loss)를 감소시킬 수 있도록 인공지능 모델 내의 신경망 레이어들의 가중치들이 튜닝될 수 있다. 이 경우, 인공지능 모델은 톤 맵 생성 모델, 특징 추출 모델 및 영상 회복 모델을 포함할 수 있으며, 이에 따라, 톤 맵 생성 모델 내의 신경망 레이어들의 가중치들, 특징 추 출 모델 내의 신경망 레이어들의 가중치들 및 영상 회복 모델 내의 신경망 레이어들의 가중치들이 함께 튜닝될 수 있다. 도 8은 본 개시의 일 실시예에 따른 인공지능 모델을 훈련하는 방법의 흐름도이다. 동작 S800에서 서버(미도시)는 피사체를 연속 촬영함으로써 생성되는 복수의 원시 영상들을 획득할 수 있다. 서 버(미도시)는 짧은 시간 구간동안 피사체를 연속 촬영함으로써 생성되는 복수의 원시 영상들을 획득할 수 있다. 복수의 원시 영상들은 연속 촬영을 통하여 생성되므로, 복수의 원시 영상들의 영상 정보는 서로 유사할 수 있다. 동작 S805에서 서버(미도시)는 복수의 원시 영상들을 조합하여 기준 원시 영상을 생성할 수 있다. 서버(미도 시)는 영상 융합(fusion) 기법을 이용하여 복수의 원시 영상들을 조합함으로써 하나의 기준 원시 영상을 생성할 수 있다. 동작 S810에서 서버(미도시)는 기준 원시 영상으로부터 ISP 프로세싱을 통해 생성되는 기준 영상을 획득할 수 있다. ISP 프로세싱은 인공지능 모델을 이용하지 않고 원시 영상을 영상 처리하는 기존의 프로세싱일 수 있으며, 예를 들어, 원시 영상에 대한 전처리, 화이트 밸런스 조정, 디모자이킹, 감마 보정, 색 변환 등의 처리 를 인공지능 모델을 이용하지 않고 수행하는 이미지 프로세싱일 수 있다. ISP 프로세싱을 통해 생성된 기준 영 상은 인공지능 모델의 정답 영상(GT image: Ground Truth image)으로 이용될 수 있다. 동작 S815에서 서버(미도시)는 인공지능 모델로부터 출력되는 제1 출력 영상을 획득할 수 있다. 서버(미 도시)는 복수의 원시 영상들 중 하나 또는 기준 원시 영상을 인공지능 모델에 입력하고, 인공지능 모델 로부터 출력되는 제1 출력 영상을 획득할 수 있다. 서버(미도시)에 의해 훈련될 인공지능 모델은 도 3에 기재된 인공지능 모델의 구성들을 포함할 수 있다. 동작 S820에서 서버(미도시)는 기준 영상 및 제1 출력 영상 간의 로스(loss)를 분석할 수 있다. 서버(미도시)는 기준 영상을 정답 영상(Ground Truth image)로 이용할 수 있으며, 기준 영상 및 제1 출력 영상 간의 차이를 비 교할 수 있다. 동작 S825에서 서버(미도시)는 분석된 로스에 기초하여, 인공지능 모델의 가중치를 변경할 수 있다. 서버 (미도시)는 기준 영상 및 제1 출력 영상 간의 로스(loss)를 감소시킬 수 있도록 인공지능 모델 내의 신경 망 레이어들의 가중치들을 조정할 수 있다. 이 경우, 인공지능 모델은 톤 맵 생성 모델, 특징 추출 모델 및 영상 회복 모델을 포함할 수 있으며, 이에 따라, 톤 맵 생성 모델 내의 신경망 레이 어들의 가중치들, 특징 추출 모델 내의 신경망 레이어들의 가중치들 및 영상 회복 모델 내의 신경 망 레이어들의 가중치들이 함께 조정될 수 있다. 동작 S830에서 서버(미도시)는 원시 영상들 중 적어도 하나 또는 기준 원시 영상을 가중치가 변경된 인공지능 모델에 입력하고, 동작 S835에서 서버(미도시)는 가중치가 변경된 인공지능 모델로부터 출력되는 제2 출력 영상을 획득하고, 동작 S840에서 서버(미도시)는 기준 영상 및 제2 출력 영상 간의 로스를 분석할 수 있다. 또한, 동작 S845에서 서버(미도시)는 인공지능 모델의 훈련을 종료할 지를 결정할 수 있다. 동작 S840에 서 분석된 로스가 기설정된 임계치보다 작은 경우에 서버(미도시)는 인공지능 모델의 훈련을 종료할 것을 결정할 수 있으며, 동작 S840에서 분석된 로스가 기설정된 임계치보다 큰 경우에 서버(미도시)는 인공지능 모델 의 가중치를 변경하기 위한 동작들을 반복하여 수행할 수 있다. *한편, 복수의 상황에 따라 복수의 인공지능 모델들이 훈련될 수 있다. 예를 들어, 카메라 필터, 카메라 렌즈, 카메라의 제조사, 디바이스 모델, 복수의 연속 촬영된 이미지, 촬영 환경, 피사체의 종류 또는 촬영 영상 의 속성 중 적어도 하나에 관련된 상황을 기준으로 복수의 인공지능 모델들이 훈련될 수 있다. 예를 들어, 비선명 마스크를 이용하여 촬영된 기준 원시 영상들 및 촬영 영상들을 이용하여 인공지능 모델 이 훈련되고, Contrast 조정 마스크를 이용하여 촬영된 기준 원시 영상들 및 촬영 영상들을 이용하여 인 공지능 모델이 훈련되고, 컬러 필터 마스크를 이용하여 촬영된 기준 원시 영상들 및 촬영 영상들을 이용 하여 인공지능 모델이 훈련될 수 있으나, 이에 제한되지 않는다. 또한, 예를 들어, 카메라 렌즈 별로 촬 영된 기준 원시 영상들 및 촬영 영상들을 이용하여 인공지능 모델이 훈련될 수 있으며, 카메라 렌즈들은, 예를 들어, 망원 렌즈, 광각 렌즈, 어안 렌즈 등을 포함할 수 있으나, 이에 제한되지 않는다. 또한, 예를 들어, 카메라의 제조사 별로 카메라에 의해 촬영된 기준 원시 영상들 및 촬영 영상들을 이용하여 인공지능 모델(162 0)이 훈련될 수 있으며, 디바이스의 모델 별로 디바이스에 의해 촬영된 기준 원시 영상들 및 촬영 영상들을 이 용하여 인공지능 모델이 훈련될 수 있으나, 이에 제한되지 않는다. 또한, 예를 들어, 실내에서 촬영된 기 준 원시 영상들 및 촬영 영상들을 이용하여 인공지능 모델이 훈련되고, 야외에서 촬영된 기준 원시 영상 들 및 촬영 영상들을 이용하여 인공지능 모델이 훈련되고, 특정 조도 범위에서 촬영된 기준 원시 영상들 및 촬영 영상들을 이용하여 인공지능 모델이 훈련될 수 있으나, 이에 제한되지 않는다. 또한, 예를 들어, 인물이 촬영된 기준 원시 영상들 및 촬영 영상들을 이용하여 인공지능 모델이 훈련되고, 음식이 촬영된 기준 원시 영상들 및 촬영 영상들을 이용하여 인공지능 모델이 훈련되고, 건물이 촬영된 기준 원시 영상 들 및 촬영 영상들을 이용하여 인공지능 모델이 훈련될 수 있으나 이에 제한되지 않는다. 또한, 예를 들 어, 촬영 영상의 속성 별로 촬영된 기준 원시 영상들 및 촬영 영상들을 이용하여 인공지능 모델이 훈련될 수 있으며, 예를 들어, 촬영 영상의 속성은, 화이트 밸런스, ISO, 셔터 스피드 등을 포함할 수 있으나, 이에 제 한되지 않는다. 한편, 도 8에서는 인공지능 모델의 훈련이 서버(미도시)에 의해 수행되는 것으로 설명되었지만, 이에 제 한되지 않는다. 인공지능 모델의 훈련은 디바이스에 의해 수행될 수도 있다. 이 경우, 디바이스 는 인공지능 모델의 훈련을 위하여 이용될 기준 원시 영상 및 기준 영상을 직접 생성하거나 서버 (미도시)에게 요청하여 수신할 수 있다. 예를 들어, 디바이스는 디바이스에서 촬영된 영상들을 서버(미도시)에게 제공하면서 서버(미도시) 에게 인공지능 모델의 훈련을 위한 기준 원시 영상 및 기준 영상을 요청할 수 있다. 서버(미도시)는 디바 이스로부터 수신된 영상들을 분석하여 디바이스의 사용자가 선호하는 상황에 관련된 기준 원시 영 상 및 기준 영상을 디바이스에게 제공할 수 있다. 예를 들어, 서버(미도시)는 디바이스로부터 수신 된 영상들을 분석함으로써, 사용자가 이용한 디바이스의 종류, 디바이스에서 촬영된 영상 내의 피 사체의 종류, 사용자가 선호하는 영상 스타일, 사용자가 주로 촬영하는 장소의 환경(예를 들어, 실내, 실외, 조 도, 날씨 등) 등과 같은 사용자의 선호 상황을 식별할 수 있다. 또는, 예를 들어, 디바이스는 디바이스 의 카메라의 종류, 렌즈 종류에 관한 정보를 서버(미도시)에게 제공하면서 서버(미도시)에게 인공지능 모 델의 훈련을 위한 기준 원시 영상 및 기준 영상을 요청할 수 있다. 서버(미도시)는 디바이스로부터 수신된 카메라의 종류 및 렌즈 종류에 관련된 기준 원시 영상 및 기준 영상을 디바이스에게 제공할 수 있 다. 또는, 디바이스는 사용자의 선호하는 영상 스타일, 촬영 환경 및 피사체 등에 관한 사용자 선호 정보를 서버(미도시)에게 제공하면서 서버(미도시)에게 인공지능 모델의 훈련을 위한 기준 원시 영상 및 기준 영 상을 요청할 수 있다. 서버(미도시)는 디바이스로부터 수신된 사용자 선호 정보에 관련된 기준 원시 영상 및 기준 영상을 디바이스에게 제공할 수 있다. 도 9는 본 개시의 일 실시예에 따른 디바이스가 라이브 뷰 영상을 출력하는 방법의 흐름도이다. 동작 S900에서 디바이스는 피사체를 촬영하기 위한 촬영 기능을 활성화하는 사용자 입력을 수신할 수 있 다. 예를 들어, 디바이스는 디바이스에 설치된 카메라 애플리케이션을 실행하는 사용자 입력을 수 신할 수 있다. 동작 S905에서 디바이스는 카메라 센서부를 통하여 입력되는 광에 기초하여 원시 영상을 생성할 수 있다. 디바이스는 촬영 기능이 활성화됨에 따라 라이브 뷰 영상을 생성하는데 이용되는 원시 영상을 생성 할 수 있다. 동작 S910에서 디바이스는 인공지능 모델 내의 모델들 중에서 적어도 하나를 비활성화할 수 있다. 디바이스는 톤 맵 생성 모델, 특징 추출 모델, 영상 보정 모델 또는 영상 회복 모델 중 적어도 하나를 비활성화할 수 있다. 라이브 뷰 영상의 생성을 위하여 톤 맵 생성 모델, 특징 추출 모델, 영상 보정 모델 또는 영상 회복 모델 중 어느 모델을 비활성화할 지는 미리 설정 될 수 있다. 동작 S915에서 디바이스는 적어도 하나의 모델이 비활성화된 인공지능 모델에 원시 영상을 입력할 수 있으며, 동작 S920에서 디바이스는 인공지능 모델로부터 출력되는 라이브 뷰 영상을 화면 상에 디스플레이할 수 있다. 이 경우, 인공지능 모델은 인공지능 모델 내의 모델들 중에서 적어도 하나 가 비활성화된 상태에서 양호한 라이브 뷰 영상이 출력될 수 있도록 미리 훈련될 수 있다. 한편, 상기에서는 톤 맵 생성 모델, 특징 추출 모델, 영상 보정 모델 또는 영상 회복 모델 중 적어도 하나가 비활성된 인공지능 모델을 이용하여 라이브 뷰 영상을 생성하는 것으로 설명되 었지만 이에 제한되지 않는다. 톤 맵 생성 모델, 특징 추출 모델, 영상 보정 모델 및 영상 회복 모델이 모두 활성화된 인공지능 모델을 이용하여 라이브 뷰 영상이 생성될 수도 있다. 한편, 톤 맵 생성 모델, 특징 추출 모델, 영상 보정 모델 또는 영상 회복 모델 중 적 어도 하나가 비활성된 인공지능 모델을 이용하여, 피사체가 촬영된 영상이 생성되고 저장될 수도 있다. 도 10a는 본 개시의 일 실시예에 따른 라이브 뷰 영상을 생성하기 위하여 인공지능 모델 내의 톤 맵 생성 모델을 비활성화하는 예시를 나타내는 도면이다. 도 10a를 참조하면, 인공지능 모델 내의 톤 맵 생성 모델이 비활성화된 상태에서, 인공지능 모델 에 원시 영상이 입력될 수 있다. 원시 영상은 특징 추출 모델에 입력되고, 특징 추출 모델로 부터 출력된 특징 영상들이 보정되고, 보정된 특징 영상들이 영상 회복 모델에 입력되며, 영상 회복 모델 로부터 라이브뷰 영상이 출력될 수 있다. 이 경우, 인공지능 모델은 톤 맵 생성 모델이 비활 성화된 상태에서 도 7에서와 같은 방법으로 훈련된 모델일 수 있다. 도 10b는 본 개시의 일 실시예에 따른 라이브 뷰 영상을 생성하기 위하여 인공지능 모델 내의 특징 추출 모델 및 영상 회복 모델을 비활성화하는 예시를 나타내는 도면이다. 도 10b를 참조하면, 인공지능 모델 내의 특징 추출 모델 및 영상 회복 모델이 비활성화된 상 태에서, 인공지능 모델에 원시 영상이 입력될 수 있다. 원시 영상은 톤 맵 생성 모델에 입력되고, 톤 맵 생성 모델로부터 출력된 톤 맵에 기초하여 원시 영상의 밝기가 조정되고, 밝기가 조정된 원시 영상 의 보정됨으로써 라이브뷰 영상이 생성될 수 있다. 이 경우, 인공지능 모델은 특징 추출 모델 및 영상 회복 모델이 비활성화된 상태에서 도 7에서와 같은 방법으로 훈련된 모델일 수 있다. 도 10c는 본 개시의 일 실시예에 따른 라이브 뷰 영상을 생성하기 위하여 인공지능 모델 내의 톤 맵 생성 모델, 특징 추출 모델 및 영상 회복 모델을 비활성화하는 예시를 나타내는 도면이다. 도 10c를 참조하면, 인공지능 모델 내의 톤 맵 생성 모델, 특징 추출 모델 및 영상 회복 모 델이 비활성화된 상태에서, 원시 영상이 보정됨으로써 라이브뷰 영상이 생성될 수도 있다. 한편, 도 10a 내지 도 10c에서는, 화이트 밸런스 매트릭스 값 및 색상 보정 매트릭스 값 값이 영상 보정 모델 에 입력되는 것으로 설명되었지만 이에 제한되지 않는다. 복수의 인공지능 모델이 화이트 밸런스 및 색상 보정에 관한 설정 별로 각각 훈련될 수 있다. 이 경우에는, 라이브뷰 영상을 생성하기 위해 소정의 화 이트 밸런스 및 색상 보정에 관한 설정에 대응되는 인공지능 모델이 제2 메모리에 로딩되어 제2 프 로세서에 의해 이용될 수 있다. 또한, 제2 메모리에 로딩된 인공지능 모델에는 화이트 밸런 스 매트릭스 값 및 색상 보정 매트릭스 값이 입력되지 않더라도, 제2 메모리에 로딩된 인공지능 모델 로부터 소정의 화이트 밸런스 및 색상 보정에 관한 설정을 고려한 라이브뷰 영상이 출력될 수 있게 된다. 도 11은 디바이스가 리트레이닝된 인공지능 모델을 서버(미도시)로부터 수신하여 인공지능 모델 을 업데이트하는 방법의 흐름도이다. 동작 S1100에서 디바이스는 리트레이닝된 인공지능 모델을 서버(미도시)에게 요청할 수 있다. 인공 지능 모델은 서버(미도시)에 의해 리트레이닝될 수 있으며, 서버(미도시)는 리트레이닝된 인공지능 모델 이 존재함을 알리는 알림 정보를 디바이스에게 제공할 수 있다. 또한, 디바이스는 서버(미도 시)로부터 수신된 알림 정보를 화면 상에 디스플레이하고, 인공지능 모델을 업데이트하기 위한 사용자 입 력을 수신할 수 있다. 디바이스는 서버(미도시)에게 사용자가 선호하는 사진 속성에 관한 정보를 제공하 면서 서버(미도시)에게 리트레이닝된 인공지능 모델을 요청할 수도 있다. 디바이스는 디바이스에서 촬영된 영상들을 서버(미도시)에게 제공하면서 서버(미도시)에게 리트레 이닝된 인공지능 모델을 요청할 수 있다. 서버(미도시)는 디바이스로부터 수신된 영상들을 분석하 여 디바이스의 사용자가 선호하는 상황에 관련된 기준 원시 영상 및 기준 영상을 이용하여 인공지능 모델 을 리트레이닝할 수 있다. 예를 들어, 서버(미도시)는 디바이스로부터 수신된 영상들을 분석함으로 써, 사용자가 이용한 디바이스의 종류, 디바이스에서 촬영된 영상 내의 피사체의 종류, 사용자가 선호하는 영상 스타일, 사용자가 주로 촬영하는 장소의 환경(예를 들어, 실내, 실외, 조도, 날씨 등) 등과 같은 사용자의 선호 상황을 식별할 수 있다. 또한, 서버(미도시)는 사용자의 선호 상황에 관련된 기준 원시 영상 및 기준 영상을 이용하여 인공지능 모델을 리트레이닝할 수 있다. 또는, 예를 들어, 디바이스는 디바이스의 카메라의 종류, 렌즈 종류에 관한 정보를 서버(미도시)에 게 제공하면서 서버(미도시)에게 리트레이닝된 인공지능 모델을 요청할 수 있다. 서버(미도시)는 디바이 스로부터 수신된 카메라의 종류 및 렌즈 종류에 관련된 기준 원시 영상 및 기준 영상을 이용하여 인공지 능 모델을 리트레이닝할 수 있다. 또는, 디바이스는 사용자의 선호하는 영상 스타일, 촬영 환경 및 피사체 등에 관한 사용자 선호 정보를 서버(미도시)에게 제공하면서 서버(미도시)에게 리트레이닝된 인공지능 모델을 요청할 수 있다. 서버(미 도시)는 디바이스로부터 수신된 사용자 선호 정보에 관련된 기준 원시 영상 및 기준 영상을 이용하여 인 공지능 모델을 리트레이닝할 수 있다. 동작 S1110에서 디바이스는 리트레이닝된 인공지능 모델을 서버(미도시)로부터 수신할 수 있다. 디 바이스가 사용자가 선호하는 사진 속성에 관한 정보를 서버(미도시)에게 제공한 경우에는, 서버(미도시) 는 사용자가 선호하는 사진 속성과 관련하여 리트레이닝된 인공지능 모델을 디바이스에게 제공할 수 있다. 동작 S1120에서 디바이스는 리트레이닝된 인공지능 모델에 기초하여, 디바이스 내의 인공지 능 모델을 업데이트할 수 있다. 디바이스는, 예를 들어, 디바이스 내의 인공지능 모델(162 0)을 리트레이닝된 인공지능 모델로 대체함으로써, 디바이스 내의 인공지능 모델을 업데이트할 수 있다. 도 12는 디바이스가 인공지능 모델을 리트레이닝하여 업데이트하는 방법의 흐름도이다. 동작 S1200에서 디바이스는 리트레이닝을 위한 기준 원시 영상 및 기준 원시 영상에 대응되는 기준 영상 을 획득할 수 있다. 기준 영상은 리트레이닝을 위한 기준 원시 영상으로부터 생성되는 영상일 수 있다. 디바이 스는 서버(미도시)에게 인공지능 모델의 리트레이닝을 위한 기준 원시 영상 및 기준 영상을 요청하 고, 서버(미도시)로부터 리트레이닝을 위한 기준 원시 영상 및 기준 영상을 수신할 수 있다. 이 경우, 디바이스 는 디바이스에서 촬영된 사진 또는 사용자가 선호하는 사진 속성에 관한 정보를 서버(미도시)에게 제공할 수 있으며, 서버(미도시)는 사용자가 선호하는 상황과 관련하여 생성된 기준 영상 및 기준 원시 영상을 디바이스에게 제공할 수 있다. 예를 들어, 디바이스는 디바이스에서 촬영된 영상들을 서버(미도시)에게 제공하면서 서버(미도시) 에게 인공지능 모델의 리트레이닝을 위한 기준 원시 영상 및 기준 영상을 요청할 수 있다. 서버(미도시) 는 디바이스로부터 수신된 영상들을 분석하여 디바이스의 사용자가 선호하는 상황에 관련된 기준 원시 영상 및 기준 영상을 디바이스에게 제공할 수 있다. 예를 들어, 서버(미도시)는 디바이스로부 터 수신된 영상들을 분석함으로써, 사용자가 이용한 디바이스의 종류, 디바이스에서 촬영된 영상 내의 피사체의 종류, 사용자가 선호하는 영상 스타일, 사용자가 주로 촬영하는 장소의 환경(예를 들어, 실내, 실외, 조도, 날씨 등) 등과 같은 사용자의 선호 상황을 식별할 수 있다. 또한, 서버(미도시)는 사용자의 선호 상황에 관련된 기준 원시 영상 및 기준 영상을 디바이스에게 제공할 수 있다. 또는, 예를 들어, 디바이스는 디바이스의 카메라의 종류, 렌즈 종류에 관한 정보를 서버(미도시)에 게 제공하면서 서버(미도시)에게 인공지능 모델의 리트레이닝을 위한 기준 원시 영상 및 기준 영상을 요 청할 수 있다. 서버(미도시)는 디바이스로부터 수신된 카메라의 종류 및 렌즈 종류에 관련된 기준 원시 영상 및 기준 영상을 디바이스에게 제공할 수 있다. 또는, 디바이스는 사용자의 선호하는 영상 스타일, 촬영 환경 및 피사체 등에 관한 사용자 선호 정보를 서버(미도시)에게 제공하면서 서버(미도시)에게 인공지능 모델의 리트레이닝을 위한 기준 원시 영상 및 기준 영상을 요청할 수 있다. 서버(미도시)는 디바이스로부터 수신된 사용자 선호 정보에 관련된 기준 원 시 영상 및 기준 영상을 이용하여 디바이스에게 제공할 수 있다. 동작 S1210에서 디바이스는 리트레이닝을 위한 기준 원시 영상 및 기준 원시 영상에 대응되는 기준 영상 을 이용하여, 디바이스 내의 인공지능 모델을 업데이트할 수 있다. 디바이스는 서버(미도 시)로부터 수신된 기준 영상을 정답 영상으로 이용할 수 있으며, 디바이스는 서버(미도시)로부터 수신된 기준 원시 영상을 인공지능 모델에 입력하고 인공지능 모델로부터 출력되는 출력 영상을 기준 영상 과 비교함으로써 인공지능 모델을 리트레이닝할 수 있다. 본 개시에 따른 인공지능과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등 과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인 공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인 공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미 한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도 형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경 망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 본 개시의 일 실시예에서 원시 영상이 인공지능 모델의 입력 데이터로 이용되어 인공지능 모델로부터 출력 영상 데이터가 출력될 수 있다. 인공지능 모델은 학습을 통해 만들어 질 수 있다. 여기서, 학습을 통해 만들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미한다. 인공 지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경 망 연산을 수행한다. 인공지능 모델은, 객체 인식(Object Recognition), 객체 추적(Object Tracking), 영상 검 색(Image Retrieval), 사람 인식(Human Recognition), 장면 이해(Scene Recognition), 공간 이해(3D Reconstruction/Localization), 영상 개선(Image Enhancement) 등에 이용될 수 있다. 본 개시의 일 실시예는 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행가능한 명령어를 포함 하는 기록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독 가능 매체는 컴퓨터 저장 매체 및 통신 매체를 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구현된 휘 발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독가능 명령어, 데이터 구조, 또는 프로그램 모듈과 같은 변조된 데이터 신호의 기타 데이터를 포함할 수 있다. 또한, 컴퓨터에 의해 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, ‘비일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다 는 것을 의미할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경 우를 구분하지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 또는 두개의 사용자 장치들(예: 스 마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 또한, 본 명세서에서, “부”는 프로세서 또는 회로와 같은 하드웨어 구성(hardware component), 및/또는 프로 세서와 같은 하드웨어 구성에 의해 실행되는 소프트웨어 구성(software component)일 수 있다. 또한, 본 명세서에서, “a, b 또는 c 중 적어도 하나를 포함한다”는 “a만 포함하거나, b만 포함하거나, c만 포함하거나, a 및 b를 포함하거나, b 및 c를 포함하거나, a 및 c를 포함하거나, a, b 및 c를 모두 포함하는 것 을 의미할 수 있다."}
{"patent_id": "10-2025-0011878", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본 개시의 설명은 예시를 위한 것이며, 본 개시가 속하는 기술분야의 통상의 지식을 가진 자는 본 개시 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 개시의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 개시의 범위에 포함되는 것으 로 해석되어야 한다.도면 도면1 도면2 도면3 도면4a 도면4b 도면4c 도면4d 도면5 도면6 도면7 도면8 도면9 도면10a 도면10b 도면10c 도면11 도면12 도면13a 도면13b"}
{"patent_id": "10-2025-0011878", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른, 디바이스가 피사체를 촬영하여 출력 영상을 생성하는 예시를 나타내 는 도면이다. 도 2는 본 개시의 일 실시예에 따른 디바이스의 블록도이다. 도 3은 본 개시의 일 실시예에 따른 원시 영상으로부터 피사체가 촬영된 출력 영상이 생성되는 과정을 설명하는 도면이다. 도 4a는 본 개시의 일 실시예에 따른 디바이스가 원시 영상으로부터 톤 맵을 생성하는 과정을 설명하 는 도면이다. 도 4b는 본 개시의 일 실시예에 따른 디바이스가 원시 영상으로부터 특징들을 추출하는 과정을 설명 하는 도면이다. 도 4c는 본 개시의 일 실시예에 따른 디바이스가 특징 영상들을 보정하는 과정을 설명하는 도면이다. 도 5는 본 개시의 일 실시예에 따른 특징 추출 모델의 구조의 일례를 나타내는 도면이다. 도 6은 본 개시의 일 실시예에 따른 디바이스가 피사체를 촬영하여 출력 영상을 생성하는 방법의 흐름도 이다. 도 7은 본 개시의 일 실시예에 따른 인공지능 모델이 훈련되는 예시를 나타내는 도면이다. 도 8은 본 개시의 일 실시예에 따른 인공지능 모델을 훈련하는 방법의 흐름도이다.도 9는 본 개시의 일 실시예에 따른 디바이스가 라이브 뷰 영상을 출력하는 방법의 흐름도이다. 도 10a는 본 개시의 일 실시예에 따른 라이브 뷰 영상을 생성하기 위하여 인공지능 모델 내의 톤 맵 생성 모델을 비활성화하는 예시를 나타내는 도면이다. 도 10b는 본 개시의 일 실시예에 따른 라이브 뷰 영상을 생성하기 위하여 인공지능 모델 내의 특징 추출 모델 및 영상 회복 모델을 비활성화하는 예시를 나타내는 도면이다. 도 10c는 본 개시의 일 실시예에 따른 라이브 뷰 영상을 생성하기 위하여 인공지능 모델 내의 톤 맵 생성 모델, 특징 추출 모델 및 영상 회복 모델을 비활성화하는 예시를 나타내는 도면이다. 도 11은 본 개시의 일 실시예에 따른 디바이스가 리트레이닝된 인공지능 모델을 서버(미도시)로부 터 수신하여 인공지능 모델을 업데이트하는 방법의 흐름도이다. 도 12는 본 개시의 일 실시예에 따른 디바이스가 인공지능 모델을 리트레이닝하여 업데이트하는 방 법의 흐름도이다. 도 13a는 본 개시의 일 실시예에 따른 피사체를 촬영하는 디바이스의 촬영 설정을 위한 GUI의 예시를 나 타내는 도면이다. 도 13b는 본 개시의 일 실시예에 따른 피사체를 촬영하는 디바이스의 촬영 설정을 위한 GUI의 예시를 나 타내는 도면이다."}
