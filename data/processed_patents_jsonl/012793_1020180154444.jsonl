{"patent_id": "10-2018-0154444", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0071846", "출원번호": "10-2018-0154444", "발명의 명칭": "시퀀스-투-시퀀스 요약모델의 반복 생성 제어를 위한 학습 방법", "출원인": "경북대학교 산학협력단", "발명자": "박세영"}}
{"patent_id": "10-2018-0154444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "시퀀스-투-시퀀스 요약모델의 학습 방법에 있어서,인코더가, 상기 시퀀스-투-시퀀스 요약모델이 생성해낸 요약문에 대한 히든 상태 벡터를 출력하는 단계;디코더가, 상기 히든 상태 벡터를 이용하여 각 단계마다, 상기 요약문 내의 단어들 각각이 생성될 확률을 계산하고, 계산된 확률 중에서 가장 높은 확률에 해당하는 단어를 생성하는 단계; 및상기 디코더가 생성한 단어들의 이력을 이용하여, 상기 시퀀스-투-시퀀스 요약모델이 이전에 출력한 단어를 중복해서 출력하지 않도록 학습하는 단계;를 포함하는 시퀀스-투-시퀀스 요약모델의 학습 방법."}
{"patent_id": "10-2018-0154444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 학습하는 단계는,상기 각 단계마다, 현재 생성될 단어가 정답 단어일 확률이 최대가 되도록 학습함과 동시에 이전에 생성된 단어들이 생성될 확률이 최소가 되도록 학습하는 것을 특징으로 하는 시퀀스-투-시퀀스 요약모델의 학습 방법."}
{"patent_id": "10-2018-0154444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 학습하는 단계는,상기 각 단계마다, 이전에 생성된 단어들을 이용하여 상기 이전에 생성된 단어들이 현재 생성될 확률을 추출하고, 추출된 확률을 모두 더하여 반복 손실 함수를 계산하는 단계; 및상기 반복 손실 함수의 값이 최소가 되도록 상기 인코더와 상기 디코더를 이용한 학습을 반복하는 단계;를 포함하는 시퀀스-투-시퀀스 요약모델의 학습 방법."}
{"patent_id": "10-2018-0154444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 학습하는 단계는,상기 각 단계의 반복 손실 함수를 모두 더하여 전체 반복 손실 함수를 계산하는 단계;상기 각 단계마다, 현재 생성될 단어가 정답 단어일 확률을 추출하고, 추출된 확률을 모두 더하여 교차 엔트로피 손실(cross entropy loss) 함수를 계산하는 단계; 상기 전체 반복 손실 함수와 상기 교차 엔트로피 손실 함수를 더하여 전체 손실 함수를 계산하는 단계; 및상기 전체 손실 함수의 값이 최소가 되도록 상기 인코더와 상기 디코더를 이용한 학습을 반복하는 단계;를 포함하는 시퀀스-투-시퀀스 요약모델의 학습 방법."}
{"patent_id": "10-2018-0154444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "시퀀스-투-시퀀스 요약모델을 학습하는 전자장치에 있어서,인공 신경망(artificial neural network)을 이용하여 상기 시퀀스-투-시퀀스 요약모델의 학습을 수행하는 프로세서;를 포함하고,상기 프로세서는 상기 시퀀스-투-시퀀스 요약모델이 생성해낸 요약문을 이용하여, 상기 시퀀스-투-시퀀스 요약공개특허 10-2020-0071846-3-모델이 이전에 출력한 단어를 중복해서 출력하지 않도록 학습을 수행하는 것을 특징으로 하는 전자장치."}
{"patent_id": "10-2018-0154444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 프로세서는,각 단계마다, 이전에 생성된 단어들을 이용하여 상기 이전에 생성된 단어들이 현재 생성될 확률을 추출하고, 추출된 확률을 모두 더하여 반복 손실 함수를 계산하고, 상기 반복 손실 함수의 값이 최소가 되도록 학습을 반복하는 것을 특징으로 하는 전자장치."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "모델의 반복 생성 제어를 위한 학습 방법 요 약"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "요약", "paragraph": 2, "content": "시퀀스-투-시퀀스 요약모델의 학습 방법이 개시된다. 상기 방법은, 인코더가, 상기 시퀀스-투-시퀀스 요약모델이"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "요약", "paragraph": 3, "content": "생성해낸 요약문에 대한 히든 상태 벡터를 출력하는 단계와, 디코더가, 상기 히든 상태 벡터를 이용하여 각 단계"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "요약", "paragraph": 4, "content": "마다, 상기 요약문 내의 단어들 각각이 생성될 확률을 계산하고, 계산된 확률 중에서 가장 높은 확률에 해당하는"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "요약", "paragraph": 5, "content": "단어를 생성하는 단계와, 상기 디코더가 생성한 단어들의 이력을 이용하여, 상기 시퀀스-투-시퀀스 요약모델이 이전에 출력한 단어를 중복해서 출력하지 않도록 학습하는 단계를 포함한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명의 개념에 따른 실시 예는 시퀀스-투-시퀀스 요약모델이 요약문을 생성할 때 이전에 생성된 단어가 반복"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "생성되지 않도록 상기 요약 모델을 학습하는 방법에 관한 것이다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "시퀀스-투-시퀀스(Sequence-to-Sequence) 모델은 인코더(encoder)와 디코더(decoder)로 구성되어 있는데, 인코 더는 한 문장을 단어 단위로 쪼개어 하나의 벡터로 인코딩하는 작업을 하고, 디코더는 인코딩된 벡터를 하나의 단어씩 디코딩하여 최종적으로 하나의 문장을 만든다. 최근에 시퀀스-투-시퀀스 모델은 많은 작업에서 폭넓게 응용되고 있으며, 특히 원 문서(입력 시퀀스)에서 핵심"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "을 파악하고, 핵심 내용을 담은 원문서보다 짧은 문서인 요약문(출력 시퀀스)을 생성하는 요약 작업에서 이용된 다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "그러나, 원문서를 입력으로 받아서 요약문을 출력하는 시퀀스-투-시퀀스 모델은 특정 어휘를 반복해서 출력하는"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 4, "content": "문제점이 있다. 이 경우 요약문의 품질이 떨어진다. 이런 요약문이 생성된 경우 실제 애플리케이션에서 활용하"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 5, "content": "기가 힘들다. 따라서, 요약문 생성 후 반복되는 표현을 제거하는 등 후처리가 필요하다. 그러나, 상기와 같은 방식은 후처리를 위한 알고리즘을 따로 만들어야하는 등 추가적인 비용이 소모되고, 기존"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 6, "content": "의 잘못 생성된 문장에서 반복적인 표현을 제거한다고 하여 정상적인 요약문이 되지 않는다. 등록특허공보 제10-1882096호는 다중 시상수를 갖는 GRU를 포함하는 회귀 신경망을 통해, 변환된 벡터를 내부"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 7, "content": "표현으로 생성하는 구성을 추가함으로써 긴 텍스트를 이해하고 이해한 내용을 기반으로 추상적 요약문을 생성해 내는 발명이 개시되어 있다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 8, "content": "그러나, 상기 선행기술문헌은 요약문을 생성할 때 내부적 표현 문법을 생성하고 난 후 문장 생성 기법을 적용한 후 추상적 표현을 생성하는 것으로서 여전히 특정 어휘를 반복해서 출력하는 문제점이 남아있다. 선행기술문헌 특허문헌 (특허문헌 0001) 등록특허공보 제10-1882096호"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기와 같은 문제점을 해결하기 위하여 안출된 것으로서, 본 발명의 시퀀스-투-시퀀스 요약모델의 반 복 생성 제어를 위한 학습 방법은 학습 단계에서부터 특정 어휘의 반복 생성을 제어하는 것을 목적으로 한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기와 같은 목적을 달성하기 위한 본 발명의 시퀀스-투-시퀀스 요약모델의 학습 방법은 인코더가, 상기 시퀀스"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "-투-시퀀스 요약모델이 생성해낸 요약문에 대한 히든 상태 벡터를 출력하는 단계와, 디코더가, 상기 히든 상태"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "벡터를 이용하여 각 단계마다, 상기 요약문 내의 단어들 각각이 생성될 확률을 계산하고, 계산된 확률 중에서 가장 높은 확률에 해당하는 단어를 생성하는 단계와, 상기 디코더가 생성한 단어들의 이력을 이용하여, 상기 시"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 4, "content": "퀀스-투-시퀀스 요약모델이 이전에 출력한 단어를 중복해서 출력하지 않도록 학습하는 단계를 포함한다. 상기 학습하는 단계는, 상기 각 단계마다, 현재 생성될 단어가 정답 단어일 확률이 최대가 되도록 학습함과 동 시에 이전에 생성된 단어들이 생성될 확률이 최소가 되도록 학습한다. 상기 학습하는 단계는, 상기 각 단계마다, 이전에 생성된 단어들을 이용하여 상기 이전에 생성된 단어들이 현재 생성될 확률을 추출하고, 추출된 확률을 모두 더하여 반복 손실 함수를 계산하는 단계와, 상기 반복 손실 함수 의 값이 최소가 되도록 상기 인코더와 상기 디코더를 이용한 학습을 반복하는 단계를 포함한다. 상기 학습하는 단계는, 상기 각 단계의 반복 손실 함수를 모두 더하여 전체 반복 손실 함수를 계산하는 단계와, 상기 각 단계마다, 현재 생성될 단어가 정답 단어일 확률을 추출하고, 추출된 확률을 모두 더하여 교차 엔트로 피 손실(cross entropy loss) 함수를 계산하는 단계와, 상기 전체 반복 손실 함수와 상기 교차 엔트로피 손실 함수를 더하여 전체 손실 함수를 계산하는 단계와, 상기 전체 손실 함수의 값이 최소가 되도록 상기 인코더와 상기 디코더를 이용한 학습을 반복하는 단계를 포함한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 5, "content": "본 발명의 시퀀스-투-시퀀스 요약모델을 학습하는 전자장치에 있어서, 인공 신경망(artificial neural networ"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 6, "content": "k)을 이용하여 상기 시퀀스-투-시퀀스 요약모델의 학습을 수행하는 프로세서를 포함하고, 상기 프로세서는 상기"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 7, "content": "시퀀스-투-시퀀스 요약모델이 생성해낸 요약문을 이용하여, 상기 시퀀스-투-시퀀스 요약모델이 이전에 출력한 단어를 중복해서 출력하지 않도록 학습을 수행한다. 상기 프로세서는, 각 단계마다, 이전에 생성된 단어들을 이용하여 상기 이전에 생성된 단어들이 현재 생성될 확 률을 추출하고, 추출된 확률을 모두 더하여 반복 손실 함수를 계산하고, 상기 반복 손실 함수의 값이 최소가 되 도록 학습을 반복한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기한 바와 같은 본 발명의 시퀀스-투-시퀀스 요약모델의 반복 생성 제어를 위한 학습 방법은 각 단계에서 출 력될 단어가 정답 단어일 확률이 최대가 되도록 학습함과 동시에 이전에 생성된 단어들이 생성될 확률을 패널티"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "로 학습시킴으로써 특별한 후처리 없이도 요약문 생성시 중복되는 어휘의 출력을 줄일 수 있는 효과를 제공할 수 있다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 본 발명에 따른 실시예 및 도면을 참조하여, 본 발명을 더욱 상술한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 2, "content": "도 1은 시퀀스-투-시퀀스 요약모델을 나타낸다. 도 1을 참조하면, 인코더가 원 문서(입력 시퀀스)를 형태소별로"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "벡터화하여 핵심을 파악하고, 디코더가 상기 인코더에서 생성된 벡터를 형태소별로 디코딩하여 요약문(출력 시 퀀스)을 생성한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "예컨대, 시퀀스-투-시퀀스 요약모델에 “앞서 일본 정부는 북한의 3차 핵실험과 관련하여 … 금융제제를 추진하 고 있다”라는 문장이 입력되면, 인코더는 이 문장에 대한 의미 벡터를 출력한다. 디코더는 의미 벡터를 받아서 [일본], [정부], [가], [북한], [에], [대한], [독자적], [제제], [를], [연장],"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "[하기], [로], [했다] 단어를 하나씩 출력한다. 각 출력들의 확률은 소프트 맥스 분류기(softmax classifier)를이용한 전체 연결층(fully-connected layer)을 통해 계산된다. 시퀀스-투-시퀀스 요약모델은 상기 확률을 최대 화하는 방향으로 학습한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 6, "content": "도 2는 시퀀스-투-시퀀스 요약모델의 반복 생성 문제를 설명하기 위한 도면이다. 디코더는 각 단어를 하나씩 출 력하는 단계에서, 사전 내의 단어들 각각이 생성될 확률을 출력한다. 따라서, 도 2에서 [북한]이라는 단어는 현 재 단계에서 사전 내의 단어들 중에서 생성될 확률이 가장 높은 단어가 선택된 결과이다. 종래의 시퀀스-투-시퀀스 모델은 각 단계에서 출력될 단어가 정답이 될 확률이 최대가 되도록 학습할 뿐, 현재 단계에서 이전에 출력된 단어 정보를 활용하지 않았으므로 이전에 높은 생성 확률을 가졌던 단어가 이후에도 여"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "전히 높은 생성 확률을 가진다. 따라서, 같은 단어, 구, 또는 문장이 반복 생성됨으로써 요약문의 품질이 떨어 지게 된다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 8, "content": "도 3과 도 4는 본 발명의 실시 예에 따른 시퀀스-투-시퀀스 요약모델의 학습 방법을 설명하기 위한 도면이다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 9, "content": "도 3을 참조하면 본 발명의 시퀀스-투-시퀀스 요약모델을 학습하기 위한 학습 모델은 상기 시퀀스-투-시퀀스 요"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "약모델이 생성해낸 요약문을 입력받고, 상기 입력받은 요약문의 각 단어별로 이전에 생성된 단어들을 이용하여 상기 이전에 생성된 단어들이 다음에 생성될 확률이 낮아지도록 학습한다. 예컨대, 디코더가 단어를 생성하는 단계가 t-번째일 때, 디코더는 사전 내의 단어들 각각이 현재 생성될 확률"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "(Pt)을 출력한다. 시퀀스-투-시퀀스 요약모델은 이전에 생성된 단어들의 인덱스(word index)를 참조하여 상기 이전에 생성된 단어들 각각의 현재 생성될 확률을 획득한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "시퀀스-투-시퀀스 요약모델은 상기 이전에 생성된 단어들 각각의 현재 생성될 확률을 더하여 하기의 [수학식 1]과 같이 t-번째 단계에서의 반복 손실 함수(repeat loss)를 구한다. [수학식 1] repeat loss(t) ="}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "여기서, yi는 디코더에서 i-번째 출력된 단어를 의미하고, Pt[yi]는 사전 내의 단어들 각각이 t-번째 단계에서 생성될 확률 분포 중에서 단어 yi가 생성될 확률을 의미하고, k는 디코더가 각 단계에서 고려할 이전에 생성된"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "단어의 수를 의미한다. 도 3의 실시 예에서 K=3이다. K는 요약문 내 문장의 평균 길이일 수 있으나 이에 한정되 는 것은 아니다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "상기 [수학식 1]의 반복 손실 함수는 요약문 전체 시퀀스 중에서 각 단어마다 구한 것이다. 따라서, 요약문 전 체 시퀀스에 대한 반복 손실 함수를 계산하기 위하여, 도 4에 도시된 바와 같이, 각 단계에서 계산된 반복 손실 함수를 모두 더하여 하기의 [수학식 2]와 같이 전체 반복 손실 함수(Lrepeat(y))를 구한다. [수학식 2]"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "여기서, seq는 요약문 전체 시퀀스의 길이를 의미한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "시퀀스-투-시퀀스 요약모델은 반복 손실 함수(Lrepeat(y))를 이용하여 각 단계에서 이전에 생성된 단어들이 생성 될 확률이 낮아지도록 학습한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "시퀀스-투-시퀀스 요약모델은 하기의 [수학식 3]의 전체 손실 함수(total loss)의 값이 최소가 되도록 학습한다. [수학식 3] total loss = LCE(y) + Lrepeat(y) 여기서, LCE(y)는 교차 엔트로피 손실(cross entropy loss)함수를 의미한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "시퀀스-투-시퀀스 요약모델은 교차 엔트로피 손실 함수(LCE(y))를 이용하여 각 단계에서 출력될 단어가 정답 단"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "어일 확률이 최대가 되도록 학습한다.시퀀스-투-시퀀스 요약모델은 반복 손실 함수(Lrepeat(y))와 교차 엔트로피 손실 함수(LCE(y))를 더한 전체 손실 함수(total loss)의 값이 최소가 되도록 학습함으로써, 각 단계에서 출력될 단어가 정답 단어일 확률이 최대가 되도록 학습함과 동시에 이전에 생성된 단어들이 생성될 확률이 낮아지도록 학습한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "실시 예에 따라, 시퀀스-투-시퀀스 요약모델은 교차 엔트로피 손실 함수(Lce(y))와 반복 손실 함수(Lrepeat(y)) 각 각에 특정 가중치를 두어 하기의 [수학식 4]와 같이 전체 손실 함수를 계산할 수 있다. [수학식 4]"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 22, "content": "여기서, x는 입력문서를 의미하고, 는 시퀀스-투-시퀀스 모델의 파라미터를 의미하고, 는 모델이 생성한"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 23, "content": "요약문을 의미하고,"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 24, "content": "는 모델이 생성한 요약문과 정답요약문을 이용하여 계산하는 교차 엔트로피 손실함수를 의미한다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 25, "content": "도 5는 본 발명의 실시 예에 따른 시퀀스-투-시퀀스 요약모델의 학습 방법을 설명하기 위한 플로우 차트이다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 26, "content": "도 5를 참조하면, 시퀀스-투-시퀀스 요약모델의 학습 방법은 인코더가, 상기 시퀀스-투-시퀀스 요약모델이 생성"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 27, "content": "해낸 요약문에 대한 히든 상태 벡터를 출력한다(S110)."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 28, "content": "디코더가, 상기 히든 상태 벡터를 이용하여 각 단계마다, 상기 요약문 내의 단어들 각각이 생성될 확률을 계산 하고, 계산된 확률 중에서 가장 높은 확률에 해당하는 단어를 생성한다(S120)."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 29, "content": "상기 디코더가 생성한 단어들의 이력을 이용하여, 상기 시퀀스-투-시퀀스 요약모델이 이전에 출력한 단어를 중 복해서 출력하지 않도록 학습한다(S130). 도 6은 본 발명의 실시예에 따른 전자 장치의 구성 요소를 도시한 블록도이다. 전자 장치는 인공 신경망을"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 30, "content": "이용하여 시퀀스-투-시퀀스 요약모델이 요약문을 생성할 때 이전에 생성된 단어가 반복 생성되지 않도록 상기"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 31, "content": "요약 모델을 학습하는 장치일 수 있다. 전자 장치는 컴퓨터 장치로 구현되는 고정형 단말이거나 이동형 단 말일 수 있다. 전자 장치는 예를 들어, 스마트 폰(smart phone), 휴대폰, 내비게이션, 컴퓨터, 노트북, 디 지털방송용 단말, PDA(Personal Digital Assistants), PMP(Portable Multimedia Player), 및 태블릿 PC 중 적 어도 하나일 수 있 으나, 이에 한정되지 않는다. 전자 장치는 무선 또는 유선 통신 방식을 이용하여 네트워 크를 통해 다른 전 자 장치 및/또는 서버와 통신할 수 있다. 도 6을 참조하면, 전자 장치는 프로세서, 메모리, 및 입력부를 포함할 수 있다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 32, "content": "입력부는 상기 시퀀스-투-시퀀스 요약모델이 생성해낸 요약문을 입력받는다. 프로세서는 컨볼루션 연산과 같은 산술, 로직 및 입출력 연산을 수행함으로써, 컴퓨터 프로그램의 명령을 처리하도록 구성될 수 있다. 명령(instruction)은 메모리에 의해 프로세서에 제공될 수 있다. 일 실 시예에서, 프로세서는 메모리와 같은 기록 장치에 저장된 프로그램 코드에 따라 수신되는 명령을 실 행하도록 구성될 수 있다. 프로세서는 예를 들어, 중앙 처리 장치(Central Processing Unit), 마이크로 프 로세서(microprocessor) 및 그래픽 프로세서(Graphic Processing Unit) 중 적어도 하나로 구성될 수 있으나, 이에 제한되는 것은 아니다. 일 실시예에서, 전자 장치가 스마트 폰, 태블릿 PC 등과 같은 모바일 디바이 스인 경우, 프로세서는 애플리케이션을 실행시키는 애플리케이션 프로세서(Application Processor, AP)일 수 있다. 프로세서는 뉴럴 네트워크 모델(neural network model)과 같은 딥 뉴럴 네트워크 기반의 범용 인 공 지능 알고리즘을 통해 학습(training)을 수행할 수 있다. 프로세서는 인공 신경망(artificial neural network)을 이용하여 입력부로부터 입력받은 시퀀스-투-"}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 33, "content": "시퀀스 요약모델의 학습을 수행할 수 있다."}
{"patent_id": "10-2018-0154444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 34, "content": "프로세서는 상기 프로세서는 상기 시퀀스-투-시퀀스 요약모델이 생성해낸 요약문을 이용하여, 상기 시퀀스 -투-시퀀스 요약모델이 이전에 출력한 단어를 중복해서 출력하지 않도록 학습을 수행한다. 프로세서는 각 단계마다, 이전에 생성된 단어들을 이용하여 상기 이전에 생성된 단어들이 현재 생성될 확 률을 추출하고, 추출된 확률을 모두 더하여 반복 손실 함수를 계산하고, 상기 반복 손실 함수의 값이 최소가 되 도록 학습을 반복한다. 본 발명은 도면에 도시된 일 실시 예를 참고로 설명되었으나 이는 예시적인 것에 불과하며, 본 기술 분야의 통 상의 지식을 가진 자라면 이로부터 다양한 변형 및 균등한 타 실시 예가 가능하다는 점을 이해할 것이다. 따라 서, 본 발명의 진정한 기술적 보호 범위는 첨부된 등록청구범위의 기술적 사상에 의해 정해져야 할 것이다."}
{"patent_id": "10-2018-0154444", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 시퀀스-투-시퀀스 요약모델을 나타낸다."}
{"patent_id": "10-2018-0154444", "section": "도면", "subsection": "도면설명", "item": 2, "content": "도 2는 시퀀스-투-시퀀스 요약모델의 반복 생성 문제를 설명하기 위한 도면이다."}
{"patent_id": "10-2018-0154444", "section": "도면", "subsection": "도면설명", "item": 3, "content": "도 3과 도 4는 본 발명의 실시 예에 따른 시퀀스-투-시퀀스 요약모델을 나타낸다."}
{"patent_id": "10-2018-0154444", "section": "도면", "subsection": "도면설명", "item": 4, "content": "도 5는 본 발명의 실시 예에 따른 시퀀스-투-시퀀스 요약모델의 학습 방법을 설명하기 위한 플로우 차트이다. 도 6은 본 발명의 실시예에 따른 전자 장치의 구성 요소를 도시한 블록도이다."}
