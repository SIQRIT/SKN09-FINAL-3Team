{"patent_id": "10-2021-0072282", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0164124", "출원번호": "10-2021-0072282", "발명의 명칭": "딥 러닝 모델에 기반하여 제품의 결함 여부를 유형별로 검사하기 위한 시스템", "출원인": "신신화학공업 주식회사", "발명자": "김형섭"}}
{"patent_id": "10-2021-0072282", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "비전 검사 시스템에 있어서,3차원 형태의 검사 대상 제품이 검사를 위하여 배치되며, 설정된 방향으로 상기 검사 대상 제품을 이송하도록구성되는 제1 컨베이어;상기 검사 대상 제품을 감지하도록 구성된 광 센서;상기 광 센서에 의해 트리거 되면 상기 검사 대상 제품에 대한 검사광을 조사하도록 구성되는 복수의 조명;상기 광 센서에 의해 트리거 되면 상기 검사 대상 제품에 대한 다각도의 검사 이미지를 획득하도록 구성된 복수의 카메라 - 상기 검사 이미지는 상기 검사 대상 제품에 대한 상단 이미지, 하단 이미지, 설정된 각도 별 측면이미지를 포함함 -;딥 러닝 모델을 이용하여 학습된 결함 유형 별 결함 이미지가 저장된 메모리;상기 딥 러닝 모델을 이용하여 상기 검사 이미지와 상기 결함 유형 별 상기 결함 이미지의 비교에 기반하여 상기 검사 대상 제품의 결함 여부를 판단하도록 구성된 적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서에 의하여 결함으로 판단된 상기 검사 대상 제품을 공기 압력을 이용하여 상기 제1컨베이어로부터 상기 결함 유형 별로 각기 다른 배출구에 배출하도록 구성된 분류기를 포함하는,비전 검사 시스템."}
{"patent_id": "10-2021-0072282", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 검사 대상 제품이 생산되어 배치되며, 설정된 방향으로 상기 검사 대상 제품을 이송하도록 구성되는 제2컨베이어;검사를 위해 상기 검사 대상 제품을 상기 제2 컨베이어로부터 상기 제1 컨베이어로 옮기고, 검사 후 정상으로판단된 상기 검사 대상 제품을 상기 제2 컨베이어로부터 상기 제1 컨베이어로 옮기도록 구성된 핸들러를 더 포함하는,비전 검사 시스템."}
{"patent_id": "10-2021-0072282", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 적어도 하나의 프로세서는,설정된 개수 별로 상기 검사 대상 제품의 결함 여부를 판단하고,각각의 상기 검사 대상 제품에 대하여 정상 또는 결함 여부에 따라 1 또는 0으로 표현한 바이너리 코드를 생성하며, 상기 바이너리 코드는 상기 설정된 개수의 코드로 구성되고,상기 분류기에게 상기 바이너리 코드를 전송하도록 더 구성되며,상기 분류기는 상기 바이너리 코드에 기반하여 상기 설정된 개수의 상기 검사 대상 제품 중 결함으로 판단된 상기 검사 대상 제품만을 배출하도록 더 구성된,공개특허 10-2022-0164124-3-비전 검사 시스템."}
{"patent_id": "10-2021-0072282", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 제1 컨베이어는 결함 유형 별로 검사에 필요한 상기 검사 대상 제품 간 간격, 결함 유형 별로 검사에 필요한 시간을 고려하여 이송 속도를 조정하도록 더 구성된,비전 검사 시스템."}
{"patent_id": "10-2021-0072282", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 적어도 하나의 프로세서는,상기 검사 이미지에서 상기 검사 대상 제품의 유형을 식별하는 분류(classification)와 상기 검사 이미지에서상기 검사 대상 제품의 위치를 식별하는 위치 측정(localization)을 동시에 수행함으로써 상기 검사 이미지에서상기 검사 대상 제품의 상기 결함 유형 별 검출을 단일 단계 검출(1-stage detector)로 수행하도록 구성된,비전 검사 시스템."}
{"patent_id": "10-2021-0072282", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 검사 대상 제품의 결함 여부의 판단은 상기 검사 이미지와 상기 결함 유형 별 상기 결함 이미지에 대하여산출된 IoU(intersection over union)이 설정된 임계 값을 초과하는지 여부를 판단함으로 수행되며,상기 IoU는 상기 검사 이미지와 상기 결함 유형 별 상기 결함 이미지 간 교집합 영역(area of overlap)의 넓이를 합집합 영역(area of union)의 넓이로 나눈 값인,비전 검사 시스템."}
{"patent_id": "10-2021-0072282", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 복수의 카메라는,상기 검사 대상 제품에 대하여 상단 이미지를 획득하기 위한 적어도 하나의 카메라, 상기 검사 대상 제품에 대하여 하단 이미지를 획득하기 위한 적어도 하나의 카메라, 상기 검사 대상 제품에 대하여 설정된 각도 별 측면이미지를 획득하기 위한 다수의 카메라를 포함하는,비전 검사 시스템."}
{"patent_id": "10-2021-0072282", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항에 있어서,상기 복수의 조명은,각각의 상기 복수의 카메라를 중심으로 상기 검사 대상 제품을 향하여 부채꼴 형태로 다양한 각도에서 검사광을조사하도록 구성된,공개특허 10-2022-0164124-4-비전 검사 시스템."}
{"patent_id": "10-2021-0072282", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 항에 있어서,상기 제1 컨베이어는 0.09 m/s의 속도로 상기 검사 대상 제품을 이송하도록 구성되며,상기 복수의 카메라는 30 FPS(frame per second) 이상의 FPS로 상기 검사 이미지를 획득하도록 구성된,비전 검사 시스템."}
{"patent_id": "10-2021-0072282", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항에 있어서,상기 딥 러닝 모델은,상기 결함 유형에 따른 트레이닝 이미지를 획득한 후, 설정된 해상도로 상기 트레이닝 이미지를 분할한 뒤, 상기 분할된 트레이닝 이미지에서 특징을 추출하고, 상기 추출된 특징을 학습하도록 구성된,비전 검사 시스템."}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 딥 러닝 모델에 기반하여 제품의 결함 여부를 유형별로 검사하기 위한 시스템에 관한 것이다. 구체적 으로, 본 발명은 3차원 형태의 검사 대상 제품에 대하여 획득된 다면 이미지를 딥 러닝 모델에 기반하여 결함 이 미지와 비교함으로써 결함 유형 별로 결함 제품을 분류하기 위한 시스템에 관한 것이다."}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 딥 러닝 모델에 기반하여 제품의 결함 여부를 유형별로 검사하기 위한 시스템에 관한 것이다. 구체적 으로, 본 발명은 3차원 형태의 검사 대상 제품에 대하여 획득된 다면 이미지를 딥 러닝 모델에 기반하여 결함 이미지와 비교함으로써 결함 유형 별로 결함 제품을 분류하기 위한 시스템에 관한 것이다."}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "스마트 공장은 제품의 기획부터 판매까지 모든 생산과정에 정보통신기술(information communication technology, ICT)을 적용하여 생산성 향상, 품질 개선, 비용 절감 등 기업의 운영 효율을 극대화하고 고객 맞춤 형 제품을 생산하는 지능형 공장이다. 제조업에서 품질은 매우 중요한 요소이다. 제품의 품질은 기업의 성장과 시장 경쟁력 확보에 중대한 영향을 미치며 스마트 공장을 구현하면서 가장 우선적으로 고려되고 있다. 최근, 인공지능을 도입한 검사 시스템이 활발하게 연구되고 있다. 머신 비전(Machine Vision)은 주로 산업 분야 에서 사람의 인지 판단 작업을 대신 수행하거나, 일반적으로 사람에게 어려운 작업을 수행하는 기술이다. 사람 이 수행하는 인지 판단 작업이라는 것은 눈으로 어떤 물체를 인식하고 그 물체가 올바른지 또는 문제는 없는지 판단하는 것을 의미한다. 그리고 머신 비전의 역할은 이러한 사람의 역할을 대신 수행하는 것이다. 비전 검사 시스템을 사용하면 사람의 개입 없이 제품의 수, 결함 검출 및 유형 등의 정보 수집이 가능하며 이는 생산성 향 상, 품질 개선, 비용 절감 등 기업의 운영 효율을 극대화한다. 현재 상용되고 있는 대부분의 비전 검사 시스템 은 제품에 대한 하나의 이미지를 수집 및 검사하는 단면 이미지(Single-sided Image) 검사 시스템을 채택하고 있다. 하지만 실제 제조 현장에서 단면 이미지(Single-sided Image) 검사가 유효한 제품은 PCB(Printed Circuit Board), 반도체(Semiconductor), 디스플레이(Display) 등 2차원 평판 형태의 일부 제품군만 국한되며 대부분의 생산품은 3차원 형태의 제품으로 단면 이미지(Single-sided Image) 검사 방식은 그 효용성에 한계가 있다. 또한 기존의 룰 베이스 검사 방식은 복잡한 외형을 가진 3차원 제품의 비정형 및 불규칙적인 결함을 검출 하는데 한계가 있기 때문에 딥 러닝과 같은 인공지능 기반의 검사 시스템 도입이 필요하다. 최근 기술의 발전으 로 머신 비전을 활용한 인공지능 검사 시스템에 대한 연구가 주목받고 있다. 특히 지속적인 인건비 상승으로 전 통적인 방식의 육안 검사를 인공지능 기반의 검사 시스템이 빠르게 대체하고 있다. 작업자의 업무를 대체하기 위해서는 3차원 제품의 외형 전체 검사가 필요하며 실제 제조 현장에서 요구하는 생산 속도를 충족하여야 한다. 본 발명에서는 다면 이미지(Multi-sided Image) 검사와 빠른 생산 속도에 충족하는 심층 신경망 기반의 비전 검 사 시스템을 제안한다. 복수 개의 카메라를 이용하여 3차원 형태의 제품의 다면 이미지(Multi-sided Image)를 동시에 수집한다. 또한, 딥 러닝 모델을 통해 제품의 결함을 실시간으로 빠르게 검출할 수 있다. 본 발명에서 제안된 시스템을 통해 실제 제조 현장에서 요구하는 수준의 제품 결함 검사가 가능하며 검사 과정에서 획득한 정보는 제품의 품질을 평가하고 개선하는데 매우 중요한 자료로 활용할 수 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 등록특허 제10-2172769호 (2020.10.27.) (기포에 의한 기판결함 검사시스템 및 기포 에 의한 기판결함 검사 방법)"}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 전술한 문제점을 해결하기 위하여 다음과 같은 해결 과제를 목적으로 한다. 본 발명은 3차원 형태의 제품에 대하여 획득된 다면 이미지에 기반하여 제품의 결함 여부를 판단하기 위한 시스 템을 제공하는 것을 목적으로 한다. 본 발명은 3차원 형태의 제품에 대하여 획득된 다면 이미지를 딥 러닝 모델에 기반하여 결함 이미지와 비교함으 로써 결함 유형 별로 결함 제품을 분류하기 위한 시스템을 제공하는 것을 목적으로 한다. 본 발명의 해결과제는 이상에서 언급된 것들에 한정되지 않으며, 언급되지 아니한 다른 해결과제들은 아래의 기"}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "재로부터 당해 기술분야에 있어서의 통상의 지식을 가진 자가 명확하게 이해할 수 있을 것이다."}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 다양한 실시 예들은 비전 검사 시스템을 제공한다. 상기 비전 검사 시스템은, 3차원 형태의 검사 대 상 제품이 검사를 위하여 배치되며, 설정된 방향으로 상기 검사 대상 제품을 이송하도록 구성되는 제1 컨베이어; 상기 검사 대상 제품을 감지하도록 구성된 광 센서; 상기 광 센서에 의해 트리거 되면 상기 검사 대 상 제품에 대한 검사광을 조사하도록 구성되는 복수의 조명; 상기 광 센서에 의해 트리거 되면 상기 검사 대상 제품에 대한 다각도의 검사 이미지를 획득하도록 구성된 복수의 카메라 - 상기 검사 이미지는 상기 검사 대상 제품에 대한 상단 이미지, 하단 이미지, 설정된 각도 별 측면 이미지를 포함함 -; 딥 러닝 모델을 이용하여 학 습된 결함 유형 별 결함 이미지가 저장된 메모리; 상기 딥 러닝 모델을 이용하여 상기 검사 이미지와 상기 결함 유형 별 상기 결함 이미지의 비교에 기반하여 상기 검사 대상 제품의 결함 여부를 판단하도록 구성된 적어도 하 나의 프로세서; 및 상기 적어도 하나의 프로세서에 의하여 결함으로 판단된 상기 검사 대상 제품을 공기 압력을 이용하여 상기 제1 컨베이어로부터 상기 결함 유형 별로 각기 다른 배출구에 배출하도록 구성된 분류기를 포함 한다."}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 3차원 형태의 제품에 대하여 획득된 다면 이미지에 기반하여 제품의 결함 여부를 판단하기 위한 시스 템을 제공할 수 있다. 본 발명은 3차원 형태의 제품에 대하여 획득된 다면 이미지를 딥 러닝 모델에 기반하여 결함 이미지와 비교함으 로써 결함 유형 별로 결함 제품을 분류하기 위한 시스템을 제공할 수 있다."}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 이상에서 언급된 것들에 한정되지 않으며, 언급되지 아니한 다른 효과들은 아래의 기재로부터 당해 기술분야에 있어서의 통상의 지식을 가진 자가 명확하게 이해할 수 있을 것이다."}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부한 도면을 참고로 하여 본 발명의 실시 예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 도 1은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 블록도를 도시한다. 도 1을 참고하면, 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 제1 컨베이어, 광 센서 , 조명, 카메라, 메모리, 프로세서, 분류기를 포함한다. 제1 컨베이어는, 3차원 형태의 검사 대상 제품이 검사를 위하여 배치되며, 설정된 방향으로 상기 검사 대 상 제품을 이송하도록 구성된다. 본 발명의 다양한 실시 예들에 따르면, 제1 컨베이어는 결함 유형 별로 검사에 필요한 상기 검사 대상 제 품 간 간격, 결함 유형 별로 검사에 필요한 시간을 고려하여 이송 속도를 조정하도록 더 구성될 수 있다. 본 발 명의 다양한 실시 예들에 따르면, 제1 컨베이어는 광 센서가 각각을 검사 대상 제품을 감지하는 시간 간격에 기반하여 이송 속도를 조정할 수 있다. 구체적으로, 검사 대상 제품 사이 간격이 검사에 적절하도록 설 정된 간격으로 될 수 있도록 하기 위하여 하나의 검사 대상 제품이 감지된 후 다른 검사 대상 제품이 설정된 감 지 시간보다 빨리 감지될 경우 이송 속도를 빠르게 함으로써 검사 대상 제품 간 간격을 벌릴 수 있다. 또한, 검 사 대상 제품 사이 간격이 검사에 적절하도록 설정된 간격으로 될 수 있도록 하기 위하여 하나의 검사 대상 제 품이 감지된 후 다른 검사 대상 제품이 설정된 감지 시간보다 느리게 감지될 경우 이송 속도를 느리게 함으로써 검사 대상 제품 간 간격을 좁힐 수 있다. 본 발명의 다양한 실시 예들에 따르면, 제1 컨베이어는 평균 0.09 m/s의 속도로 상기 검사 대상 제품을 이 송하도록 구성될 수 있다. 광 센서(Optical Sensor)는, 검사 대상 제품을 감지하도록 구성된다. 검사 대상 제품이 제1 컨베이어의 벨 트에서 움직일 때 광 센서가 감지하고 하드웨어 트리거 신호 또는 스프트웨어 트리거 신호를 전송하여 조 명과 카메라가 작동 가능하다. 조명은, 광 센서에 의해 트리거 되면 상기 검사 대상 제품에 대한 검사광을 조사하도록 구성된다. 조명 은 복수의 조명으로 구성될 수 있다. 본 발명의 다양한 실시 예들에 따르면, 조명은, 복수의 카메라를 중심으로 상기 검사 대상 제품을 향 하여 부채꼴 형태로 다양한 각도에서 검사광을 조사하도록 구성될 수 있다. 카메라는, 광 센서에 의해 트리거 되면 상기 검사 대상 제품에 대한 다각도의 검사 이미지를 획득하도록 구성된다. 검사 이미지는 검사 대상 제품에 대한 상단 이미지, 하단 이미지, 설정된 각도 별 측면 이미지를 포 함한다. 카메라는 복수의 카메라로 구성될 수 있다. 본 발명의 다양한 실시 예들에 따르면, 카메라는 검사 대상 제품에 대하여 상단 이미지를 획득하기 위한 적어도 하나의 카메라, 상기 검사 대상 제품에 대하여 하단 이미지를 획득하기 위한 적어도 하나의 카메라, 상 기 검사 대상 제품에 대하여 설정된 각도 별 측면 이미지를 획득하기 위한 다수의 카메라를 포함할 수 있다. 본 발명의 다양한 실시 예들에 따르면, 카메라는 30 FPS(frame per second) 이상의 FPS로 상기 검사 이미 지를 획득하도록 구성될 수 있다. 메모리는, 딥 러닝 모델을 이용하여 학습된 결함 유형 별 결함 이미지가 저장된다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 단일의 제품에 대해 복수 개, 예를 들어, 7개의 이미지를 수집 및 처리할 수 있다. 이 때, 동시에 수집되는 이미지의 크기가 크기 때문에 정상 제품의 이미지는 저장하지 않고 이미지 정보 에 대해서만 별도의 데이터베이스에 저장할 수 있다. 또한, 결함 이미지 발생 시 별도의 이미지 및 이미지 정보 저장을 통해 결함의 위치 및 유형, 원인 등을 분석할 수 있다. 메모리는 비전 검사 시스템 내 하드웨어 메 모리의 형태로 카메라 및 프로세서와 직접 전기적으로 연결될 수 있다. 또는, 메모리는 데이터 베이스의 형태로 유무선 통신 시스템을 통해 카메라 및 프로세서와 데이터 교환을 수행할 수 있도록 연결될 수 있다(connected). 본 발명의 다양한 실시 예들에 따르면, 딥 러닝 모델은, 결함 유형 별 트레이닝 이미지를 획득한 후, 설정된 해 상도로 상기 트레이닝 이미지를 분할한 뒤, 상기 분할된 트레이닝 이미지에서 특징을 추출하고, 상기 추출된 특 징을 학습하도록 구성될 수 있다. 프로세서는, 딥 러닝 모델을 이용하여 상기 검사 이미지와 상기 결함 유형 별 상기 결함 이미지의 비교에 기반하여 상기 검사 대상 제품의 결함 여부를 판단하도록 구성된다. 프로세서는 적어도 하나의 프로세서로 구성될 수 있다. 본 발명의 다양한 실시 예들에 따르면, 프로세서는, 설정된 개수 별로 상기 검사 대상 제품의 결함 여부를 판단하고, 각각의 상기 검사 대상 제품에 대하여 정상 또는 결함 여부에 따라 1 또는 0으로 표현한 바이너리 코 드를 생성하며, 상기 바이너리 코드는 상기 설정된 개수의 코드로 구성되고, 상기 분류기에게 상기 바이너리 코 드를 전송하도록 더 구성되며, 상기 분류기는 상기 바이너리 코드에 기반하여 상기 설정된 개수의 상기 검사 대 상 제품 중 결함으로 판단된 상기 검사 대상 제품만을 배출하도록 더 구성될 수 있다. 본 발명의 다양한 실시 예들에 따르면, 프로세서는, 검사 이미지에서 상기 검사 대상 제품의 유형을 식별 하는 분류(classification)와 상기 검사 이미지에서 상기 검사 대상 제품의 위치를 식별하는 위치 측정 (localization)을 동시에 수행함으로써 상기 검사 이미지에서 상기 검사 대상 제품의 상기 결함 유형 별 검출을 단일 단계 검출(1-stage detector)로 수행하도록 구성될 수 있다. 본 발명의 다양한 실시 예들에 따르면, 검사 대상 제품의 결함 여부의 판단은 상기 검사 이미지와 상기 결함 유 형 별 상기 결함 이미지에 대하여 산출된 IoU(intersection over union)이 설정된 임계 값을 초과하는지 여부를 판단함으로 수행될 수 있으며, 상기 IoU는 상기 검사 이미지와 상기 결함 유형 별 상기 결함 이미지 간 교집합 영역(area of overlap)의 넓이를 합집합 영역(area of union)의 넓이로 나눈 값이다. 분류기는, 적어도 하나의 프로세서에 의하여 결함으로 판단된 상기 검사 대상 제품을 공기 압력을 이용하 여 상기 제1 컨베이어로부터 상기 결함 유형 별로 각기 다른 배출구에 배출하도록 구성된다. 도 1에서는 도시되지 않았으나, 비전 검사 시스템은 검사 대상 제품이 생산되어 배치되며, 설정된 방향으 로 상기 검사 대상 제품을 이송하도록 구성되는 제2 컨베이어를 더 포함할 수 있다. 또한, 비전 검사 시스템은 검사를 위해 상기 검사 대상 제품을 상기 제2 컨베이어로부터 상기 제1 컨베이어로 옮기고, 검사 후 정 상으로 판단된 상기 검사 대상 제품을 상기 제2 컨베이어로부터 상기 제1 컨베이어로 옮기도록 구성된 핸들러를 더 포함할 수 있다. 도 2는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 2를 참고하면, 비전 검사 시스템은 제품 인식 장치(product recognition device), 이미지 획득 장치(image acquisition device), 이미지 처리 장치(image processing device), 제품 분류 장치(product sorting device) 를 포함한다. 제품 인식 장치는 광 센서(optical sensor)를 포함한다. 제품 인식 장치는 광 센서를 이용하여 제1 컨베이어를 통해 이송되는 검사 대상 제품이 광 센서에 접근하였음을 인식하고, 이미지 획득 장치에게 하드웨어 트리거를 한다. 이미지 획득 장치는 광 센서에 의해 트리거 되면 상기 검사 대상 제품에 대한 검사광을 조사하도록 구성되는 복 수의 조명과 광 센서에 의해 트리거 되면 상기 검사 대상 제품에 대한 다각도의 검사 이미지를 획득하도록 구성 된 복수의 카메라를 포함한다. 이미지 처리 장치는 이미지 획득 장치가 획득한 이미지를 처리하여 검사 대상 제품의 결함 여부를 판단하도록 구성된다. 이미지 처리 장치는 딥 러닝 모델을 이용하여 학습된 결함 유형 별 결함 이미지가 저장된 메모리, 상 기 딥 러닝 모델을 이용하여 상기 검사 이미지와 상기 결함 유형 별 상기 결함 이미지의 비교에 기반하여 상기 검사 대상 제품의 결함 여부를 판단하도록 구성된 적어도 하나의 프로세서를 포함한다. 프로세서는 획득된 이미 지에서 필요한 부분반을 자르고, 그레이 스케일로 필터링 후 저장하며, 트레이닝 데이터 세트에 기반한 딥 러닝 모델을 이용하여 검사 대상 제품의 결함 여부를 판단하도록 구성된다. 프로세서에 의하여 판단된 검사 결과는 통신 시스템을 이용하여 사용자 등의 단말 내 애플리케이션으로 전송될 수 있다. 애플리케이션은 모니터링부, 클라우드 및 서버부, 데이터 분석부를 포함한다. 모니터링부는 검사 대상 제품에 대한 모니터링을 수행하도록 구성되며, 클라우드 및 서버부는 데이터의 클라우드 및 서버에 대한 저장 및 조회를 수행하도록 구성되며, 데이 터 분석부는 수신한 데이터에 대한 분석을 수행하도록 구성된다. 제품 분류 장치는 이미지 처리 장치에서 제품의 결함 여부에 대한 판단에 따라 결함 있는 제품을 별도로 분류하 도록 구성된 분류기(actuator)를 포함한다. 분류기는 프로세서에 의하여 결함으로 판단된 상기 검사 대상 제품 을 공기 압력을 이용하여 상기 제1 컨베이어로부터 상기 결함 유형 별로 각기 다른 배출구에 배출하도록 구성된 다. 도 3은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 본 발명에서 제안하는 비전 검사 시스템의 검증 및 적용을 위해 국내 중견 제조 기업의 RFP(Request For Proposal)를 받아 실제 시스템을 구축하였다. 구축하는 시스템의 TRL(Technology Readiness Level)은 실용화 단계인 7단계 적용을 목표로 하였다. 해당 기업은 연 매출 약 1,000억 원의 중견기업으로 Cosmetic Case, Motors Parts를 주로 생산하는 업체이며 생산 제품의 특성상 제품 외형 디자인과 품질이 매우 중요한 '감성 제 품'으로 실제 제품의 외형 결함 검사를 위해 많은 인력이 육안으로 검사를 진행하였다. 기업의 니즈와 애로사항 을 수렴하여 시스템을 구축하였고 내용은 다음과 같다. (a) 연구 정보 일정 : 2020년 9월 장소 : S 사 조립 라인(Assembly Line) 검사 대상 제품 : 화장품 케이스 뚜껑(Cosmetic Case Cap) (b) 비전 검사 시스템 컨셉(Concept) 본 발명에서 제안하는 비전 검사 시스템의 아키텍처를 기반으로 구축하였고 생산 현장의 장소와 기 조립 설비의 연결을 고려하여 컨셉(Concept)을 구성하였다. 비전 검사 시스템의 컨셉은 검사 대상 제품을 생산하여 이송하는 제2 컨베이어, 즉, 기존 컨베이어 벨트와 검사 대상 제품을 검사를 위하여 이송하는 제1 컨베이어를 “ㄱ자”모양으로 결합하여 구성하였다. 검사 대상 제품의 연결 및 이송은 핸들러(Handler) 로봇을 사용하였다. 검사 대 상 제품을 검사하는 공간은 최적의 검사 환경을 구현하기 위해 클린룸(Clean Room)과 암실(Dark Room)을 구성하 였다. 마지막으로 검사 대상 제품의 검사 결과는 양품과 불량품으로 구분하였으며 불량품은 다시 재작업(Re- Work) 제품과 폐기 제품으로 구분하여 에어 분류기(Air Actuator)를 통해 배출하도록 설계하였다. 본 발명의 다양한 실시 예들에서 제안하는 비전 검사 시스템은 다음의 과정을 통해 검사를 진행한다. 1. 기존 컨베이어 벨트 제품 이송 2. 핸들러 로봇을 통한 제품 정렬 3. 검사 컨베이어 벨트에 제품 안착 시 제품 간 간격 조정 (6cm) 4. 광 센서(Optical Sensor)를 통해 제품 감지 및 하드웨어 트리거(H/W Trigger) 5. 제품 이미지 획득 6. 딥 러닝 모델, 예를 들어, PANet 모델을 활용한 제품 결함 검사 진행 7. 검사 결과에 따른 제품 분류(Classification) 8. 에어 분류기(Air Actuator)를 통한 제품 배출 9. 검사 결과 모니터링 및 MES(Manufacturing Execution System) 전송 도 4는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 구체적으로, 도 4는 본 발명 의 다양한 실시 예들에 따른 비전 검사 시스템의 측면 구성을 도시한다. 도 5는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 구체적으로, 도 5는 본 발명 의 다양한 실시 예들에 따른 비전 검사 시스템의 상단 구성을 도시한다. 도 6은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 구체적으로, 도 6은 본 발명 의 다양한 실시 예들에 따른 비전 검사 시스템 내 상부 카메라 및 조명의 구성을 도시한다. 도 7은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 구체적으로, 도 7은 본 발명 의 다양한 실시 예들에 따른 비전 검사 시스템 내 상부 카메라 및 조명의 구성을 도시한다. 도 8은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 구체적으로, 도 6은 본 발명 의 다양한 실시 예들에 따른 비전 검사 시스템 내 하부 카메라 및 조명의 구성을 도시한다. 도 9는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 구체적으로, 도 9은 본 발명 의 다양한 실시 예들에 따른 비전 검사 시스템내 실제 구현 예를 도시한다. 도 10은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 구체적으로, 도 10은 본 발 명의 다양한 실시 예들에 따른 비전 검사 시스템내 실제 구현 예를 도시한다. 도 11은 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 모니터링 화면을 도시한다. 비전 검사 시스템을 통해 수집된 이미지는 검사 결과에 따라 양/불 판정을 하며 검사 결과 데이터와 이미지는 엣지 컴퓨팅(Edge Computing) 기반의 로컬 서버(Local Server)로 저장되며, 시각화 스크린(VisualizationScreen)으로 송출된다. 엣지 레이어(Edge Layer)를 통해 처리된 데이터는 수집(Gathering)된 이미지와 데이터를 클라우드 컴퓨팅(Cloud Computing) 기반의 제조 실행 시스템(manufacturing execution system, MES)로 전송하 여 효율적인 데이터 애플리케이션(Data Application)을 지원한다. 본 발명에서 제안하는 시스템 효과의 검증 및 적용을 위해 국내 중견 제조 기업을 대상으로 실제 시스템을 구축 하였다. 검사 시스템 구축 후 해당 제품의 육안 검사 업무를 대체할 수 있었고 품질로 인한 클레임을 약 20% 이 상 개선할 수 있었다. 도 12는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 구체적으로, 도 12는 비전 검사 시스템을 설계하고 구현하기 위한 일반적인 구성요소와 고려사항에 대한 정보이다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 제품 인식 장치, 이미지 수집 장치, 이미지 처리 장치, 제품 분류 장치로 구성될 수 있다. 제품 인식 장치는 주로 센서(Sensor), 트리거(Trigger), 엔코더(Encoder) 등 으로 구성되고 이미지 수집 장치는 카메라(Camera), 렌즈(Lens), 광원(Light Sources), IPC(Industrial Process Control) 등으로 구성된다. 수집된 이미지는 이미지 처리 장치 단계에서 딥 러닝 모델, 예를 들어, PANet 모델을 통해 처리 및 분석되며 마지막으로 제품 분류 장치 단계에서 결과에 따라 제품을 분류한다. 도 13은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 광 센서에 기반한 트리거 모드의 구성을 도시 한다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 제품 인식 장치, 이미지 수집 장치, 이미지 처리 장치, 제품 분류 장치로 구성될 수 있다. 제품 인식 장치는 컨베이어 벨트를 통해 생산되는 제품을 감지하고 정해진 시간과 위치에서 카메라가 촬영될 수 있도록 카메라 모듈을 트리거(Trigger)하는데 사용된다. 제안된 시스템에서는 제품을 감지하는데 광 센서 (Optical Sensor)를 사용하는데 이는 제품 감지에 대한 정확도와 빠른 응답에 적합하기 때문이다. 광센서를 통 해 감지된 신호는 이미지 수집을 위해 카메라와 조명에 트리거를 준다. 컨베이어 벨트에서 생산되는 제품은 움 직이는 피사체이기 때문에 정지된 상태가 아닌 움직이는 상태에서 이미지 수집이 이루어진다. 따라서 제품의 생 산 속도를 계산하여 정해진 시간과 위치에서 카메라가 촬영될 수 있도록 신호를 주는 것이 트리거 신호(Trigger Signal)이다. 트리거는 사용자가 일정한 프레임 속도(Frame Rate)와 같이 비 주기적인 방식으로 이미지를 촬영할 때 주로 사 용한다. 트리거 모드는 두 가지 방법으로 동작할 수 있는데 하드웨어 트리거(Hardware Trigger)와 소프트웨어 트리거(Software Trigger)이다. 하드웨어 트리거는 전기 신호가 입력되어 트리거로 동작하는 것이고, 소프트웨 어 트리거는 명령어에 의해 트리거로 동작하는 것이다. 하드웨어 트리거는 소프트웨어 트리거 고유의 지연 (Latency)으로는 부적합한 초정밀 용도에 이상적이며, 따라서 비전 검사 시스템에는 하드웨어 트리거 방식을 적 용하는 것이 통상적이다. 도 13은 트리거 모드에 대한 구성의 일 예를 도시한다. 트리거 모드(Trigger Mode)는 원하는 순간에 이미지를 획득하는 방식이며 오버랩(Overlap)을 설정할 수 있다. 오버랩은 의미 그대로 겹친다는 것이다. 카메라 동작에서 오버랩은 리드아웃(Read Out) 기간에 노출(Exposure) 하는 동작을 의미한다. 트리거 신호가 입력되면, 상승 엣지(Rising Edge) 기준으로 노출이 시작된다. 노출이 일 정 시간동안 동작하며 트리거의 폭(Pulse Width)만큼 동작한다. 노출이 끝나고 리드아웃(Read Out)이 시작된다. 또한, 트리거 신호(Trigger Signal)는 카메라와 더불어 조명에게도 신호를 주는데 카메라의 노출 시간과 조명의 동기를 맞추어 광량을 확보하는 기능을 스트로브(Strobe)라고 한다. 빠른 속도로 움직이는 물체를 촬영하기 위 해서는 센서의 노출 시간이 짧아야 하는데 짧은 노출 설정은 빛이 부족하기 때문에 스트로브 신호(Strobe Signal)를 사용하면 카메라의 노출 시간에 조명이 켜져 충분한 광량 확보가 가능하게 된다. 도 14는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 복수의 카메라의 구성을 도시한다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 제품 인식 장치, 이미지 수집 장치, 이미지 처리 장치, 제품 분류 장치로 구성될 수 있다.이미지 수집 장치는 광학계를 통해 검사 제품의 이미지를 수집하는 과정이며 주로 카메라, 렌즈, 조명, IPC 등 으로 구성된다. 도 14는 이미지 수집 장치의 구조도를 보여준다. 도 14에서 도시된 예에 따르면, CMOS(Complementary Metal-Oxide Semiconductor) 카메라는 총 7대가 사용되는데 상단 뷰(Top View) 1대와 측면 어라운드 뷰(Around View) 6대로 구성이 된다. 검사 대상 제품의 피사계 심도(Depth of Focus, DoF) 기준으로 제품 전체의 이미지 사각지대가 발생하지 않는 카메라 개수는 6개이며 각 카메라의 각도는 각 60°이다. 또한 검사 대상 제품과의 거리를 계산했을 때 27cm의 초점 거리가 설정되었다. 도 15는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 복수의 카메라의 구성의 일 예를 도시한다. 구체적으로, 도 15는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템에서 사용된 카메라와 렌즈의 제원의 일 예에 대한 정보이다. 정확하고 빠른 검사를 위해서는 영상을 촬영하는 카메라가 매우 중요하다. 제품의 유형 및 생산 속도에 따라 적 절한 카메라와 렌즈가 필요하다. 따라서 수집된 정보를 기반으로 카메라와 조명 선정에 필요한 구성 요소를 얻 는다. 카메라와 조명 선정에 필요한 구성 요소는 다음과 같다. 카메라 : 센서(Sensors), 유효 픽셀(Effective Pixels), 프레임 속도(Frame Rate), 색상(Color), 셔터 (Shutter), 인터페이스(Interface) 렌즈 : 이미지 크기(Image Size), 초점 거리(Focal Length), 조리개 범위(Aperture Range), 마운트(Mount) 카메라에서 가장 중요한 것은 이미지 센서이다. 이미지 센서는 렌즈와 결합하여 빛 데이터를 받아들이는 역할을 한다. 또한 렌즈는 초점을 맞추는 초점 링(Focus Ring)과 빛의 양을 조절하는 조리개(Aperture)를 통해 빛 데이 터를 받아들인다. 따라서 카메라와 렌즈를 선정하려면 초점 거리(Focal Length, FL), 초당 프레임(Frames Per Second, FPS), 시야(Field of View, FoV), 작동 거리(Working Distance, WD)의 고려가 필수적이다. 시야(Field of View, FoV)는 구성된 광학계로 얻을 수 있는 가로(H) 또는 세로(V) 영역을 의미한다. 공식으로는 아래와 같다. [수학식 1]"}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "[수학식 2]"}
{"patent_id": "10-2021-0072282", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 3, "content": "도 16은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 복수의 조명의 구성을 도시한다. 구체적으로, 도 16은 다중 각도 직접 조명(Multi-Angle Direct Lights)의 구현 예시이다. 검사 대상 제품에서 양질의 이미지를 획득하기 위해서는 광원 또한 매우 중요하다. 조명을 통해 제품을 비추는 과정에서 제품에 그림자가 생기면 음영 부분은 결함 검사가 불가하기 때문이다. 그렇기 때문에 검사 대상 제품 이 선정되면 실험을 통해 카메라와 광원의 거리 및 각도를 설정해야 한다. 직접적으로 물체에 빛을 반사시킬 것 인지 간접적으로 할 것인지 또는 동축, 투과, 백라이트 등 조명을 어떤 위치에서 어떤 각도로 그리고 어떤 색상 의 조명을 사용하는지에 따라 검사 결과에 중대한 영향을 미친다. 따라서, 광원을 선정할 때에는 여러 조건으로 충분히 실험 후 최적에 조건을 찾는 것이 가장 중요하다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템에서는 다중 각도 직접 조명(Multi-Angle Direct Lights) 을 사용한다. 해당 조명은 플렉서블(Flexible) 기반의 반구 형상 돔 형태 조명으로 발광 다이오드(light emitting diode, LED) 광원이 검사 대상 제품에 직접 소사하도록 설계되어 그림자 없는 영상을 얻을 수 있다. 채널 별로 분리도 가능하여 각도에 따라 다른 파장 대를 혼합하여 다양한 형태의 검사가 가능한 복합조명으로도 사용이 가능하다. 도 17은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 복수의 카메라의 인터페이스 구성을 도시한다. 머신 비전 시스템의 카메라 인터페이스 표준은 카메라가 PC로 연결되는 방법을 표준화하여 효율적인 기술사용이 가능한 정의된 모델을 제공한다. 머신 비전 시스템의 카메라 인터페이스 표준은 하드웨어와 소프트웨어로 구분 된다. 비전 어플리케이션은 카메라 검색과 연결, 카메라 구성, 카메라의 이미지 그래빙(Grabbing), 카메라로 신 호를 보낸 이벤트의 처리라는 4가지의 기본적인 과정을 요구한다. 도 17은 머신 비전 시스템의 카메라 인터페이 스 표준으로 제공되는 핵심 기능을 도시한다. 두 개의 소프트웨어 레이어가 머신 비전 시스템의 카메라 인터페이스 처리에 도움을 준다. 첫 번째 레이어는 전 송 레이어(Transport Layer)로서 카메라를 열거하고, 카메라의 저수준 레지스터로 접속하여, 스트림 데이터를 검색한 후 이벤트를 전달한다. 전송 레이어는 하드웨어 인터페이스 표준으로 통제된다. 인터페이스 형식에 따라 서, 전송 레이어는 전용 프레임 그래버(Camera Link, Camera Link HS, CoaXPress) 또는 버스 어댑터(FireWire, GigE Vision, USB3 Vision)가 필요하다. 두 번째 레이어는 이미지 획득 라이브러리이며, 소프트웨어 개발 키트 (Software Development Kit)의 한 부분이다. SDK(software development kit)는 독립형 품목이 될 수 있으며, 프레임 그래버와 함께 제공되거나 이미지 처리 라이브러리에 있다. SDK는 전송 레이어를 사용하여 카메라 기능 에 접속하고, 이미지를 그래빙(Grabbing) 할 수 있다. 도 15에서 도시된 바와 같이 본 발명의 다양한 실시 예들은 GigE Vision의 인터페이스를 사용할 수 있다. GigE Vision 표준은 이더넷(IEEE 802.3)통신 표준을 사용하여 개발되고, 광범위하게 채택하는 머신 비전 인터페이스 표준이다. GigE Vision은 다중 스트림 채널을 지원하며 표준 이더넷 케이블을 사용하여 매우 긴 거리에 빠르고 오류 없는 이미지 전송이 가능하다. 다양한 공급업자의 소프트웨어와 하드웨어가 다양한 데이터 전송 속도에서 이더넷 연결을 통하여 원활하게 상호 운용될 수 있다. IEEE 1588과 같은 다른 이더넷 표준의 장점을 이용하여 결정적인 트리거를 제공한다. 본 발명의 다양한 실시 예들에서 제안하는 이미지 수집 장치를 통해 제품의 회전 및 접촉 없이 전체의 표면을 검사할 수 있다. 제품이 컨베이어 벨트에서 움직일 때 광 센서(Optical Sensor)가 감지하고 하드웨어 트리거 신 호를 주어 카메라와 조명 스트로브가 작동 가능하다. 7개의 CMOS 카메라에서 동시에 제품의 이미지를 수집하고 수집한 이미지는 이미지 처리 장치를 통해 데이터 분석 결과에 따라 결함의 존재 여부에 대한 판정을 하게 된다. 이미지 수집 장치에 의해 수집된 이미지 처리, 분석 과정을 거쳐 결함의 존재 여부에 대한 판정이 수행된다. 통 상적으로 카메라 제조사에서는 제공하는 카메라의 라이브러리를 제공한다. 이를 SDK(Software Development Ki t)라고 하는데 최근 가장 많이 사용하고 있는 SDK 컴파일러 및 언어는 비주얼 스튜디오(Visual Studio)와 C/C+ 이다. 본 발명에서 제안하는 비전 검사 시스템의 카메라 제조사에서 제공하는 SDK의 언어와 딥 러닝 모델 모두 C언어로 프로그래밍 할 수 있다. SDK를 통해 수집된 원본 이미지는 미리 세팅해놓은 608*608 해상도로 크롭 (Crop)이 되어 필요한 이미지만 남겨 둔다. 또한, 크롭(Crop)된 이미지는 그레이 스케일(Grey Scale)로 변환하 여 .jpg 파일로 저장된다. 도 18은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 딥 러닝 모델의 트레이닝 과정을 도시한다. SDK에서 지원하는 카메라의 정보를 불러와 딥 러닝 모델, 예를 들어, PANet을 사용하여 정상 제품과 결함이 있 는 제품에서 패턴을 검출할 수 있다. PANet은 다양한 필터를 라벨로 표시된 데이터에 적용하여 그 데이터에서 특징을 추출한다. 데이터에서 추출된 특징을 이용하여 딥 러닝 모델을 학습시킬 수 있다. 도 18은 제안된 딥 러 닝 모델의 트레이닝 처리(Training Process) 과정 나타낸다. 도 18을 참고하면, 광 센서(Optical Sensor)를 통해 인지된 신호는 카메라에 트리거(Trigger) 신호를 주어 제품 의 이미지를 촬영한다. 카메라 SDK를 통해 수집 및 저장된 이미지 데이터는 다크넷(Darknet) 라이브러리를 사용 하여 학습 모델을 만들고 PANet 모델로 학습을 시켰다. PANet 모델을 통해 학습된 이미지는 제품의 결함을 검사 하는데 사용되어 감지(Detection)하게 된다. 레이더 센서, 예를 들어, 광 센서에 의하여 카메라 등이 트리거 되어 검사 대상 제품에 대한 이미지를 획득하며, 이미지에 대하여 SDK에 기반하여 이미지 크롭을 수행하여 필요한 부분만 남긴 이미지로 변환하고 그 레이 스케일로 변환한 뒤, 딥 러닝 모델을 통해 트레이닝을 수행함으로써 결함의 존재 여부를 감지할 수 있다. 도 19는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 프로세서의 이미지 데이터 처리 과정을 도시 한다. 구체적으로, 도 19는 이미지 처리 장치의 데이터 프로세스이다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 제품 인식 장치, 이미지 수집 장치, 이미지 처리 장치, 제품 분류 장치로 구성될 수 있다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 단일의 제품에 대해 7개의 이미지를 수집 및 처리할 수 있으며, 동시에 수집되는 이미지의 크기가 크기 때문에 정상 제품의 이미지는 저장하지 않고 이미지 정보에 대 해서만 별도의 데이터베이스에 저장할 수 있다. 또한, 결함 이미지 발생 시 별도의 이미지 및 이미지 정보 저장 을 통해 결함의 위치 및 유형, 원인 등을 분석할 수 있도록 하였다. 우리의 실험에서는 3,048개의 데이터세트에 서 2,249개의 이미지에 대해 학습되었고 799개의 이미지에 대해 테스트되었다. 이 후, 제품 분류 장치는 이미지 처리 장치를 통해 나온 결과를 바탕으로 결함 존재 여부에 따라서 양/불량 제 품을 분류한다. PANet을 통해 감지(Detection)된 이미지는 결과에 따라 분류기로 신호를 보내는데 이 때 통신은 RS-232를 사용할 수 있다. 분류기는 검사 시스템의 결과부에 위치하여 컨베이어 벨트의 끝 점에서 에어 분류기 (Air Actuator)로 작동한다. PANet을 통해 받은 결함 존재 여부의 결과 값에 따라 정상 제품은 그대로 흐르게 되고 결함 제품은 분류기에 의해 별도의 좌측으로 분류되어 결함 제품 수집부(Defective Parts)에 도달하게 된 다. 도 20은 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 예시적인 결함 유형을 도시한다. 도 20은 예시적 인 3차원 형태의 검사 대상 제품으로서 플라스틱 성형 용기의 8가지 결함 유형을 도시한다. 본 발명의 일 실시 예에서는 검사 대상 제품의 결함을 검사하기 위해 플라스틱 성형 용기(Cosmetic Case)를 대 상으로 실험을 진행하였다. 다양한 재질의 플라스틱 성형 용기는 사출, 도장 및 조립 공정에서 발생하는 다양한 요인에 의해 스크래치(scratch), 균열(crack), 흉터(scar), 오염(pollution), 링 불량 조립(ring-poor assembly), 링 조립되지 않음(ring-unassembled), 캡 불량 조립(cap-poor assembly), 캡 조립되지 않음(cap- unassembled) 등의 결함이 발생한다. 이러한 결함은 제품 외관에만 영향을 미칠 뿐 아니라 제품의 내구성에도 영향을 미친다. 따라서, 제품의 결함을 검사하여 생산 품질을 개선하는 것이 매우 중요하다. 도 21은 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 하단에 대한 결함을 도시한다. 구체적으로, 도 21은 예시적인 3차원 형태의 검사 대상 제품으로서 플라스틱 성형 용기에 대하여 하단에서 촬영한 이미지에 기 반하여 결함 여부를 검사한 결과를 도시한다. 도 22는 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 상단에 대한 결함을 도시한다. 구체적으로, 도 22는 예시적인 3차원 형태의 검사 대상 제품으로서 플라스틱 성형 용기에 대하여 하단에서 촬영한 이미지에 기 반하여 결함 여부를 검사한 결과를 도시한다. 도 22를 참고하면, 복합 결함(complex), 링 조립되지 않음(un-ring), 캡 조립되지 않음(un-cap), 스크래치 (scratch)의 결함에 대한 예시가 도시된다. 도 23은 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 측면에 대한 결함을 도시한다. 구체적으로, 도 23은 예시적인 3차원 형태의 검사 대상 제품으로서 플라스틱 성형 용기에 대하여 측면에서 촬영한 이미지에 기반하여 결함 여부를 검사한 결과를 도시한다. 도 23을 참고하면, 복합 결함(complex), 균열(crack), 링 불량 조립(ring-poor assembly), 캡 불량 조립(cap- poor assembly)의 결함에 대한 예시가 도시된다. 도 24는 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 유형 별 결함 검사 결과를 도시한다. 구체적으로, 도 24sms 예시적인 3차원 형태의 검사 대상 제품으로서 플라스틱 성형 용기에 대하여 촬영한 이미 지에 기반하여 결함 여부를 검사한 결과를 도시한다. 도 23을 참고하면, 스크래치(scratch), 균열(crack), 흉터(scar), 오염(pollution), 링 불량 조립(ring-poor assembly), 링 조립되지 않음(ring-unassembled), 캡 불량 조립(cap-poor assembly), 캡 조립되지 않음(cap- unassembled)의 결함에 대한 예시가 도시된다. 도 25는 본 발명의 다양한 실시 예들에 따른 인공 지능의 개념을 도시한다. 머신 비전은 주로 산업 분야에서 사람의 인지 판단 작업을 대신 수행하거나 일반적으로 사람에게 어려운 작업을 수행하는 기술이다. 사람이 수행하는 인지 판단 작업이라는 것은 눈으로 어떤 물체를 인식하고 그 물체가 올바 른지 또는 문제는 없는지 판단하는 것을 의미한다. 그리고 머신 비전의 역할은 이러한 사람의 역할을 대신 수행 하는 것이다. 머신 비전을 사용하면 사람의 개입 없이 제품의 수, 결함 검출 및 유형 등의 정보 수집이 가능하 며 이는 생산성 향상, 품질 개선, 비용 절감 등 기업의 운영 효율을 극대화한다. 기존 머신 비전은 룰(Rule) 기 반의 기계 학습 알고리즘을 사용하며 이미지 전처리, 특징 추출 및 분류의 세 가지 주요 단계로 나뉜다. 그러나 이러한 알고리즘은 제품의 특징 추출을 수동으로 설계해야 하고 많은 시간이 소요된다. 기술의 발달로 인해 인공지능이 도입되어 많은 분야에서 주목을 받고 있다. 인공지능은 인간의 뇌 신경망 구조 를 모방하기 때문에 제한적이지만 사고와 판단이 가능하다. 일반적으로 인공 신경망은 DNN(Deep Neural Network)으로 구현된다. CNN(Convolution Neural Network)은 이미지의 물체를 인식하는 가장 강력하고 효과적 인 DNN이다. CNN의 딥 멀티-레이어 아키텍처(Deep Multi-Layers Architecture)는 수작업으로 제작된 기능보다 더 강력한 기능을 추출할 수 있으며 모든 기능은 역 전파(Back Propagation) 알고리즘을 사용하여 트레이닝 데 이터(Training Data)에서 자동으로 추출된다. CNN은 저 결함 이미지(Low Defect Image)에서 예측에 이르는 종 단 간 솔루션(End-to-End Solution)을 제공하여 적합한 기능을 수동으로 추출해야 하는 불편함을 최소화한다. 또한 CNN을 통해 물체의 정확한 위치 및 크기 정보를 사용하여 몇 밀리 초 내에 물체를 감지할 수 있다. 하지만 CNN은 처리 속도가 상대적으로 느리다는 단점이 있다. 최근 CNN을 개선한 PANet(Path Aggregation Network)의 등장으로 빠르고 정확한 제품 표면 결함 검사가 가능해졌다. PANet은 기존에 존재하는 피라미드형 네트워크인 FPN(Feature Pyramid Network)을 백본(Backbone)으로 삼아 추가로 세 단계의 모델을 덧붙여 정확도를 높였다. PANet의 연구는 향상된 YOLO 감지 네트워크(YOLO Detection Network)를 통해 실시간으로 스틸 스트립(Steel- Strip) 결함 검사를 했다. 개선된 딥 러닝 모델인 PANet의 등장으로 처리속도가 빨라지고 정확도가 높아지면서 비전 검사 시스템이 지속해서 발전하고 있다. 실제 제품의 생산 속도에 충족하는 기술의 발전 덕분에 머신 비전 은 거의 모든 생산 시설에서 사용할 수 있다. PANet의 연구는 한 개 이상의 카메라와 머신 비전 소프트웨어를 통해 제품의 다양한 결함을 쉽게 감지할 수 있는 방법을 제시하였다. PANet의 연구는 비전 검사 시스템을 통해 PCB의 홀 수 결함을 검사하는 연구를 제안했다. 이 연구에 따르면 95% 이상의 정확도를 보장할 수 있다. PANet 의 연구는 Otsu 임계 값(Otsu Thresholding), 캐니 엣지 감지(Canny Edge Detection), 형태학적 프로세스 (Morphological Process), 허프 서클 변환(Hough Circle Transform) 등 다양한 이미지 처리 기술을 통해 2차원 형태 제품인 인쇄 회로 기판(printed circuit board, PCB)의 결함을 검사했다. PANet은 컨베이어 벨트에서 발생 하는 결함을 연구했다. 또한 PANet은 레일 웨이(Railways)에서 발생하는 균열 및 결함을 연구했다. 본 발명은 3 차원 형태 제품의 다면 이미지(Multi-sided Image)와 빠른 생산 속도에 충족하는 비전 검사 시스템을 제안한다. IT 기술의 발달로 인해 인공지능은 광범위하게 사용되어왔다. 심층 신경망은 기계 학습(Machine Learning)의 하 위 개념이다. 심층 신경망은 음성 인식, 영상 인식, 이미지 인식, 물체 인식, 문자 인식 등을 통해 제조, 의료, 금융, 미디어, 교육의 다양한 산업 분야에서 서비스를 창출하고 있다. 심층 신경망을 통해 기존의 방식에서 탈 피한 자동화된 방법으로 높은 수준의 정확도를 구현할 수 있다. 도 25는 인공지능의 개념도이다.도 26은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델과 기계 학습 모델의 성능을 도시한다. 구체적으로, 도 26은 딥 러닝 모델과 기계 학습 모델의 성능 비교를 도시한다. 종래에, 결함 검출 및 진단은 주로 기계 학습(Machine Learning)을 통해 사용되었다. 기계 학습에서 결함 검출 이 잘 수행되려면 연산의 복잡성을 줄이고 분류(Classification)의 정확도를 높이기 위해 많은 양의 데이터를 선택해야 한다. 기계 학습의 정확도를 높이기 위하여 SVM(Support Vector Machine) 모델에 적합한 기능 선택을 위한 방법이 제안된 바 있다. 하지만 기계 학습은 도 26에 도시된 바와 같이 데이터 학습 시 일정 영역에서 학 습 성능이 정체된다는 단점이 있다. 반면, 심층 신경망 모델은 빅데이터를 학습할 때 매우 효과적이다. 도 26을 참고하면, 학습 데이터 세트의 크기 가 클수록 심층 신경망 모델의 성능이 기계 학습에 비하여 우수함을 알 수 있다. 도 27은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델과 기계 학습 모델의 과정을 도시한다. 구체적으로, 도 27은 딥 러닝 모델과 기계 학습 모델의 트레이닝 과정의 비교를 도시한다. 도 27을 참고하면, 기계 학습을 통한 검사 대상 제품의 결함 검출 과정이 도시된다. 도 27의 좌측은 검사 대상 제품의 결함 검출을 위하여 수행되어야 할 전체적인 과정을 나타내며, 도 27의 가운데는 기계 학습 모델을 통한 결함 검출 과정을 나타내고, 도 27의 우측은 심층 신경망 모델을 통한 결함 검출 과정을 나타낸다. 심층 신경망 모델은 기계 학습 모델에 비하여 상대적으로 간단한 절차를 통해 “딥 러닝 모델(Deep Learning Model)”을 거 쳐 곧바로 결함 검출 및 진단 결과가 도출되는 것을 볼 수 있다. 도 28은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델의 아키텍처를 도시한다. 구체적으로, 도 28은 예시적 인 딥 러닝 모델로서 PANet(Path Aggregation Network)의 아키텍처를 도시한다. 최근에는 인공지능 기술의 발달로 기계 학습을 통한 이미지 처리 기술이 활발하게 연구되고 있다. PANet(Path Aggregation Network)은 실시간으로 이미지를 인식하는데 최적화된 모델이다. PANet은 기존에 존재하는 피라미 드형 네트워크인 FPN(Feature Pyramid Network)을 백본(Backbone)으로 삼아 추가로 세 단계의 모델을 덧붙여놓 은 알고리즘이다. 일반적으로 먼저 추출하게 되는 낮은 레벨 특징(Low-level Feature) 일수록 결과까지 전파 (Propagate)되면 상대적으로 많은 과정을 수행하기 때문에 결과에 충분히 반영되지 않는데 이를 보완하기 위해 추출된 특징(Feature)들이 공평하게 결과에 반영되는 방안을 도입했다. 도 28은 예시적인 딥 러닝 모델로서 PANet의 구조를 도시한다. 도 28을 참고하면, PANet은 (a) FPN 백본(FPN Backbone), (b) 상향식 경로 확대(Bottom-up Path Augmentation), (c) 적응형 기능 풀링(Adaptive Feature Pooling), (d) 박스 브랜치(Box Branch), (e) 완전 연 결 퓨전(Fully-connected Fusion)으로 구성된다. (a) FPN 백본(FPN Backbone): FP(Feature Pyramid)는 다양한 스케일로 물체를 감지하기 위한 물체 감지(Object Detection)의 기본 구성 요소이다. 하지만, 최하단 레이어(Layer)가 많은 수의 레이어(Layer)를 거쳐 최상위까 지 도달하기 때문에 정확도가 떨어지는 예측 결과가 나온다는 단점이 있다. FPN의 각 레이어(Layer)는 구조도를 봤을 때 한층 같지만 사실 ResNet-50 등으로 구성되어 있기 때문에 빨간색 점선의 경로대로 최하단 레이어 (Layer)가 최상위까지 도달하기 위해서는 매우 많은 수의 레이어(Layer)를 지나가게 되고 결과적으로 전파 (Propagate)가 원활하게 이루어지지 못한다. (b) 상향식 경로 확대(Bottom-up Path Augmentation): PANet은 심플한 컨볼루션 레이어(Convolution Layer)로 이루어진 Nn 피라미드를 새로 구성한다. 새로운 피라미드는 최하단 레이어(Layer)가 최상위까지 도달하기 위해 서는 파란색 점선처럼 숏컷(Short-Cut) 하나와 몇 개의 컨볼루션 레이어(Convolution Layer)만 거치게 된다. 이 는 기존 백본 모델(Backbone Model)에 비해 원활한 정보 흐름(Information Flow)이 가능해졌음을 의미한다. 모 든 Nn 레이어(Layer)는 해상도(Resolution)는 다르지만, 똑같이 256개의 필터를 가지며, Nn-1을 스트라이드 (Stride) 2의 3x3 컨볼루션(Convolution) 연산으로 다운 샘플링(Down Sampling)한 결과를 Pn 과 연결 (Concatenate)시켜 구현한다. (c) 적응형 기능 풀링(Adaptive Feature Pooling): ROIAlign 기법을 통해 각 Nn 마다 관심영역을 추출한다. 특 징 맵(Feature Map)은 최종 결과의 해상도(Resolution) 차이가 있기 때문에 낮은 수준으로 ROI(region of interest)를 설정하면 결과에 해당하는 정확한 픽셀(Pixel)과 불일치하는 문제가 발생할 수 있는데 이를 보정해주는 기법이 ROIAlign이다. 그 다음 모든 Nn 레이어의 ROI를 요소 별(Element-Wise)로 풀링(Pooling) 해준다. (d) 박스 브랜치(Box Branch): 인스턴스(Instance)를 단순히 박스(Box)로 구분하고 싶다면 풀링(Pooling)의 결 과를 완전 연결 레이어(Fully-Connected Layer)로 연결해 예측하면 된다. 반면, 분할(Segmentation)을 하려면 다른 방식을 적용하면 된다. (e) 완전 연결 퓨전(Fully-connected Fusion): PANet은 정확한 예측을 위해 풀링(Pooling) 결과를 컨볼루션 레 이어(Convolution Layer)와 완전 연결 레이어(Fully-Connected Layer)로 분리한 뒤 통합한다. PANet은 컨볼루 션 레이어(Convolution Layer)는 공간 정보(Spatial Information)를 가지고 있고, 완전 연결 레이어(Fully- Connected Layer)는 공간 정보는 없지만 풍부한 정보량을 가지고 있기 때문에, 이 둘을 통합하면 정확한 예측이 가능하다. 도 29는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델의 예측 구현 방식을 도시한다. 구체적으로, 도 29는 완전 연결 퓨전(Fully-connected Fusion)의 구체적인 구현 방식을 도시한다. PANet은 정확한 예측을 위해 풀링(Pooling) 결과를 컨볼루션 레이어(Convolution Layer)와 완전 연결 레이어 (Fully-Connected Layer)로 분리한 뒤 통합한다. PANet은 컨볼루션 레이어(Convolution Layer)는 공간 정보 (Spatial Information)를 가지고 있고, 완전 연결 레이어(Fully-Connected Layer)는 공간 정보는 없지만 풍부 한 정보량을 가지고 있기 때문에, 이 둘을 통합하면 정확한 예측이 가능하다. 도 30은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델의 아키텍처를 도시한다. 구체적으로, 도 30은 CNN(Convolutional Neural Network)의 아키텍처를 도시한다. CNN(Convolutional Neural Network)은 이미지를 분석하기 위해 패턴을 찾는데 유용한 알고리즘으로 데이터에서 이미지를 직접 학습하고 패턴을 사용해 이미지를 분류한다. CNN은 필터링 기법을 인공 신경망에 적용함으로써 이미지를 더욱 효과적으로 처리하기 위해 제안되었다. CNN의 기본 구조도는 도 30에 도시된 바와 같다. 도 31은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델의 인공 신경망 구조와 일반적인 인공 신경망 구조의 비교를 도시한다. 일반적인 신경망은 아핀(Affine)으로 명시된 완전 연결(Fully-connected) 연산과 ReLU와 같은 비선형 활성 함수 (Non-linear Activation Function)의 합성으로 정의된 여러 계층을 쌓은 구조이다. CNN은 특징(Feature)을 추 출하는 컨볼루션 레이어(Convolution Layer)와 추출된 특징(Feature)을 서브 샘플링(Sub-Sampling)하는 풀링 레이어(Pooling Layer)로 구성되어 있다. 컨볼루션 레이어(Convolution Layer)는 이미지에 필터링 기법을 적용 하고 풀링 레이어(Pooling Layer)는 이미지의 국소적인 부분들을 하나의 대표적인 스칼라 값으로 변환함으로써 이미지 크기를 줄이는 등의 다양한 기능들을 수행한다. 도 31은 일반적인 인공 신경망 구조와 CNN의 구조 비교 를 도시한다. 도 32는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델 중 컨볼루션 레이어의 매트릭스를 도시한다. 도 30에서 도시된 바와 같이 CNN 모델은 컨볼루션 레이어(Convolution Layer), 풀링 레이어(Pooling Layer), 완전 연결 레이어(Fully-Connected Layer)로 구성되어 있다. 각 레이어(Layer)에 대한 설명은 다음과 같다. (a) 컨볼루션 레이어(Convolution Layer): 컨볼루션 레이어(Convolution Layer)는 입력 값(Input Value)과 컨 볼루션 필터(Convolution Filter)를 곱해서 얻어진 값이다. 도 30은 단일 채널(Single Channel)에 대한 컨볼루 션 레이어(Convolution Layer)의 동작을 도시한다. 4x4 매트릭스(Matrix)는 입력 데이터이고 3X3 매트릭스 (Matrix)는 필터이다. 이 필터가 데이터에서 한 칸씩 또는 두 칸씩 이동하면서 계산하는데 이 값을 스트라이드 (Stride)라 하며 이를 통해 특징 맵(Feature Map)을 만들 수 있다. 컨볼루션 레이어(Convolution Layer)를 거 치면서 이미지의 크기는 점점 작아지게 되고 이미지의 가장자리에 위치한 픽셀들의 정보는 점점 사라지게 된다. 이러한 문제점을 해결하기 위해 패딩(Padding) 기법을 사용하는데 패딩(Padding)은 이미지의 가장자리에 특정 값으로 설정된 픽셀들을 추가함으로써 입력 이미지와 출력 이미지의 크기를 같거나 비슷하게 만드는 역할을 한다. (b) 풀링 레이어(Pooling Layer): 풀링 레이어(Pooling Layer)는 컨볼루션 레이어(Convolution Layer)의 출력 데이터를 받아서 특징 맵(Feature Map)의 크기를 줄이거나 특정 데이터를 강조하는 용도로 사용된다. 입력 데이 터의 크기가 축소되고 학습하지 않기 때문에 파라미터 수가 줄어들어 오버-피팅(Over-fitting)이 발생하는 것을 방지한다. 풀링 레이어(Pooling Layer)를 처리하는 방법으로는 대상 이미지 영역에서 최대 값을 구하는 최대 풀 링(Max Pooling)과 대상 이미지 영역에서 평균값을 구하는 평균 풀링(Average Pooling)이 있다. (c) 완전 연결 레이어(Fully-Connected Layer): 완전 연결 레이어(Fully-Connected Layer)는 CNN 마지막에서 분류(Classification)를 결정하는 단계이다. 소프트맥스(Softmax) 함수를 이용해 가장 확률이 높은 클래스 (Class)를 출력(Output)으로 분류한다. 도 33은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 제1 컨베이어와 제2 컨베이어의 구성을 도시 한다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 크게 2가지 파트로 구성되어 있으며 하부검사와 상부검 사를 담당한다. 각각의 파트는 이송부, 검사부, 배출부로 구성되어 있다. 하부검사는 기 구축된 검사 대상 제품의 생산 라인, 즉, 제2 컨베이어 상의 제품을 검사 장비로 위치를 변경하 며 그 과정에서 하부 검사를 진행하게 된다. 주요 부속은 이송 핸들러, 제1 컨베이어의 벨트, 검사 장비로 구성 된다. 하부검사의 이송부에서, 생산 제품은 기 구축된 조립장비에서 1행 4열의 형태로 배출이 진행된다. 따라서 해당 라인 위에서 검사를 진행이 불가하다. 따라서 검사 장비로 제품을 이송할 수 있는 핸들러를 활용하여 생산된 제 품의 진행 방향을 변경하여 준다. 이송 핸들러는 수직이동 및 수평이동이 가능하게 구성되어 있어 제품감지 -> 핸들러하강 -> 제품 홀딩 -> 핸들러 상승 -> 핸들러 수평이동(검사 컨베이어) -> 핸들러하강 -> 제품 릴리즈 -> 핸들러 상승 -> 핸들러 수평이동(초기위치) 순으로 작동하게 된다. 근접 센서를 활용하여 광 센서에 대하여 정 해진 가이드 범위 안으로 제품이 정해진 개수, 예를 들어, 2개 이상 확인될 때 핸들러 작동할 수 있다. 제품이 생산 컨베이어, 즉, 제2 컨베이어에서 검사 컨베이어, 즉, 제1 컨베이어로 이송하는 과정에서 제품 하부를 검사 할 수 있도록 제품 감지 후 핸들러 하강 시 DW100 번지의 값을 0 -> 1로 변경하여 주며, 제품 릴리즈 시 1 -> 0 으로 다시 변경하여 신호에 구분을 주게 된다. 도 34는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템에서 결함 여부 판단에 대하여 생성한 바이너리 코드의 구성을 도시한다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 크게 2가지 파트로 구성되어 있으며 하부검사와 상부검 사를 담당한다. 각각의 파트는 이송부, 검사부, 배출부로 구성되어 있다. 하부검사의 검사부는 딥 러닝 연산이 가능한 프로세서, 컬러 이미지 획득이 가능한 카메라 1개, 링 조명 1개로 구성되어 있으며, 검사 대상 제품이 검사 컨베이어, 즉, 제1 컨베이어로 이송하는 과정에서 하부 미조립(내캡 및 외캡) 여부를 검사한다. 프로세서에서 직렬(Serial) 통신을 통해 PLC의 DW100 번지의 값을 지속적으로 조회 를 하게 되며 번지의 값이 1이 될 경우 소프트웨어 트리거(S/W Trigger) 형태로 카메라를 작동시킨다. 이 때, 핸들러의 초기 위치에서 카메라 상단으로 이송할때 걸리는 지연시간(730ms)을 계산하여 카메라를 작동시킨다. 불량품 배출은 핸들러가 제품을 릴리즈한 직후 진행되기 때문에 릴리즈 이전에 결과를 PLC의 DW104 번지에 값을 전달해주며, 4개에 대한 결과를 한 번에 전달하기 위하여 16진법의 형태로 값을 전달하게 된다. 도 34는 본 발 명의 다양한 실시 예들에 따른 비전 검사 시스템에서 결함 여부 판단에 대하여 생성한 바이너리 코드의 구성을 도시한다. 본 발명의 다양한 실시 예들에 따르면, 프로세서는, 설정된 개수 별로 검사 대상 제품의 결함 여부를 판단하고, 각각의 상기 검사 대상 제품에 대하여 정상 또는 결함 여부에 따라 1 또는 0으로 표현한 바이너리 코 드를 생성하며, 바이너리 코드는 설정된 개수의 코드로 구성되고, 분류기에게 바이너리 코드를 전송하도록 구성 되며, 분류기는 바이너리 코드에 기반하여 상기 설정된 개수의 검사 대상 제품 중 결함으로 판단된 상기 검사 대상 제품만을 배출하도록 구성될 수 있다. 하부검사의 배출부는 이송 핸들러에서 제품이 릴리즈 된 후 PLC의 DW104번지의 값이 0이 아닌 1~F 사이의 값이 들어오게 되면 결함 제품을 컨베이어에서 불량품 배출 구간으로 공기 압력을 통하여 배출한다. 배출된 제품은상부 검사로 이동되지 않으며 작업자의 분류를 통하여 제품 수선 또는 폐기의 과정을 거친다. 검사 시스템의 강 제 종료에 대비하기 위하여 신호 처리 후 PLC의 DW104 번지의 기본값을 0으로 수정한다. 통계 작성시 검사 총수 량을 기록하지 않고 불량품 총 수량을 기록/전송하게 구성되어 있다. 검사 총 수량을 활용할 경우 상부 검사와 하부검사 간 생산 수량이 맞지 않는 문제가 있다. 도 35는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 제1 컨베이어와 제2 컨베이어의 구성을 도시 한다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 크게 2가지 파트로 구성되어 있으며 하부검사와 상부검 사를 담당한다. 각각의 파트는 이송부, 검사부, 배출부로 구성되어 있다. 상부검사의 이송부는 이송 핸들러를 통해 컨베이어 위로 이동된 제품간 간격이 제품의 직경보다 2~3배 이상 좁 기 때문에 정상적인 측면 검사가 어렵다. 따라서, 이송 컨베이어간 속도차이를 이용하여 제품 간격을 벌려줄 수 있도록 컨베이어를 2개로 구성하게 된다. 상부검사의 검사부는 딥러닝 연산이 가능한 PC, 카메라(Mono) 5개, 복합조명 1개로 구성되어 있으며 제품의 상 단 및 측면 불량유형(스크래치성, 조립불량 등)을 검사하게 된다. 상단에 달린 카메라를 이용하여 제품의 정위 치 여부를 판단하며 제품이 검사 위치에 도달하였을 때 측면 4대의 카메라를 작동시켜 영상을 확보한다. 현재 생산 라인에서 제품 4개가 생산되는 시간은 약 3.5~4초의 간격으로, 또는 모든 제품이 동일한 간격으로 들어온 다는 보장을 할 수 없어 실제 제품 1개 검사에 소요되는 시간은 0.3초 내외이다. 제품 4개가 연속적으로, 예를 들어, 1.5초~2초 간격으로 들어온 후 약 2초간의 대기 시간이 있다. 검사 결과 불량이 발생한 경우 불량 위치와 명칭을 이미지 위에 덮어쓰기를 진행하며, 추후 학습에 활용할 수 있도록 원본의 이미지를 별도의 폴더에 저장 한다. 상부검사의 배출부는 상단 검사에서는 정상품과 불량품으로 분류하는 것이 아닌 정상품, 불량품(수선가능), 불 량품(수선불가)의 3가지 유형으로 분기가 필요하다. 따라서 배출부에서는 불량의 유형에 따라 불량품을 배출하 는 방법(배출구)가 달라야 한다. 특히 수선 가능제품을 분류하는 과정에 있어 제품간 충돌 및 장비와 충격이 발 생하여 추가 손상이 가해지지 않도록 시스템을 구성하여야 한다. 검사 결과가 PLC의 DW114번지에 전달이 되면 검사 위치와 배출 위치간 이송 시간(컨베이어)을 고려하여 일정시간 지연(수선가능 2.1sec, 수선불가 2.8sec) 후 배출을 수행하게 된다. 하부 검사 배출부와 동일하게 신호 수신 후 DW114번지의 값을 기본값인 0으로 초기화 한다. 배출 시 하부 검사와 동일하게 공기 압력을 이용하여 제품을 컨베이어에서 이탈시킨다. 전체 생산량은 상 부 검사수량(정상/불량)과 하부 불량품 수량의 합으로 이루어진다. 도 36는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델 별 물체 감지 능력의 비교를 도시한다. 도 36을 참고하면, YOLO v4와 기타 물체 감지(Object Detection) 모델과의 비교가 도시된다. YOLO v4는 EfficientDet과 비슷한 AP 성능을 내면서도 2배 더 빠른 FPS를 보유한다. YOLO v4는 YOLO v3에 비해서 AP는 10%, FPS는 12% 향상됐다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 물체 감지(Object Detection)를 위한 딥 러닝 모델에 대하여 다양한 모델을 선택적으로 사용할 수 있다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 딥러닝 연산에 대하여, 제품의 불량 유무를 검출하는 방법으로 컴퓨터 비전(Computer Vision)의 영역 중 하나인 물체 감지(Object Detection)를 사용한다. 특히 딥 러닝 기반의 물체 감지(Object Detection)의 경우 하드웨어의 제한으로 인하여 분석 속도를 확보하기 위해 정확 도를 희생하는 경우가 많다. YOLO 알고리즘은 다른 알고리즘과 비교하여 정확도 대비 인터페이스가 매우 빠른 편이어서 실시간 불량 검사에 활용할 수 있다. 물체 감지(Object Detection) 문제는 물체의 위치를 찾는 위치 측정(Localization)과 물체를 식별하는 분류 (Classification) 문제를 합한 문제로써 단일 단계 검출(1-Stage Detector)은 이 두 문제를 동시에 행하는 방 법이고 2단계 검출(2-Stage Detector)은 이 두 문제를 순차적으로 행하는 방법이다. 기본적으로 단일 단계 검출 (1-Stage Detector)은 2단계 검출(2-Stage Detector) 대비 빠르지만 정확도가 떨어지는 단점이 있다. 이에 사 용하려는 YOLO 알고리즘의 경우 단일 단계 검출(1-Stage Detector)로써 빠른 검사 속도를 보여준다. 단일 단계 검출(1-Stage Detector)은 위치 측정(Localization)이 2단계 검출(2-Stage Detector)에 비해 떨어지는 특징이 있다. 하지만, 검사 대상 제품의 경우 불량의 유형 검출이 중요한 편이며 발생 위치를 판단하여 활용하는 것이아니기 때문에 검사 결과에 큰 영향을 주지는 않는다. 따라서, 단일 단계 검출(1-Stage Detector)을 통해 결함 의 정확한 위치 검출을 희생하고 대신 결함의 존재 여부를 빠르게 판단할 수 있다. 도 37는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 물체 감지 과정을 도시한다. 본 발명의 다양한 실시 예들에 따르면, 검사 대상 제품의 결함 여부의 판단은 상기 검사 이미지와 상기 결함 유 형 별 상기 결함 이미지에 대하여 산출된 IoU(intersection over union)이 설정된 임계 값을 초과하는지 여부를 판단함으로 수행될 수 있으며, 상기 IoU는 상기 검사 이미지와 상기 결함 유형 별 상기 결함 이미지 간 교집합 영역(area of overlap)의 넓이를 합집합 영역(area of union)의 넓이로 나눈 값이다. 도 38은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 물체 감지 과정을 도시한다. 물체 감지(object detector) 모델은 주로 백본(Backbone)과 헤드(Head)라는 두 부분으로 구성된다. 백본은 입력 이미지를 특징 맵(feature map)으로 변형시켜주는 부분이다. ImageNet 데이터셋으로 사전 트레이닝(pre- trained) 시킨 VGG16, ResNet-50 등이 대표적인 백본(Backbone)이다. 헤드는 백본(Backbone)에서 추출한 특징 맵(feature map)의 위치(location) 작업을 수행하는 부분이다. 헤드에서 클래스 예측(predict classes)과 바운 딩 박스(bounding boxes) 작업이 수행된다. 헤드는 크게 밀도 예측(Dense Prediction), 희소성 예측(Sparse Prediction)으로 나뉘는데, 이는 물체 감지(Object Detection)의 종류인 단일 단계(1-stage)인지 2단계(2- stage)인지와 직결된다. 희소성 예측(Sparse Prediction) 헤드를 사용하는 2단계 감지(Two-Stage Detector)는 대표적으로 Faster R-CNN, R-FCN 등이 있다. 클래스 예측(predict classes)과 바운딩 박스 회귀(Bounding Box Regression) 부분이 분리되어 있는 것이 특징이다. 밀도 예측(Dense Prediction) 헤드를 사용하는 단일 단계 감 지(One-Stage Detector)는 대표적으로 YOLO, SSD 등이 있다. 2단계 감지(Two-Stage Detector)와 다르게, 단일 단계 감지(One-Stage Detector)는 클래스 예측(predict classes)과 바운딩 박스 회귀(Bounding Box Regression)가 통합되어 있는 것이 특징이다. 도 39는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 물체 감지 과정을 도시한다. 도 39를 참고하면, 물체 감지(object detector)에 대하여 BoF(Bag of Freebies)와 BoS(Bag of Specials) 방법 을 도시한다. BoF 방법은 전통적인 물체 검출은 오프라인으로 학습시킨다. BoS 방법은 학습 비용만 증가시킬 뿐 추론 (inference) 비용은 증가시키지 않는다. BoS 방법은 추론(inference) 비용을 조금은 증가시키더라도 물체 검출 정확도를 확연히 올리는 방법이다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 데이터 증강 방법에 대하여 CutMix, Mosaic 등에 기반 하여 BoF 방법을 사용할 수 있다. 도 40은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 물체 감지 과정을 도시한다. 도 40의 좌측은 기존의 활성화 함수(activation function)들을 도시하며, 도 40의 우측은 본 발명의 다양한 실 시 예들에 따른 비전 검사 시스템에 적용될 수 있는 활성화 함수(activation function)로서 Mish 함수(Mish function)을 도시한다. Mish 함수(Mish function)는 마이너스 측에서는 선형에 가깝고 플러스 측에서는 무한으로 뻗어가는 특징이 있다. 그라디언트가 다른 함수들 대비 좀더 유동적이기 때문에 분류에 유리하지만 계산 비용이 증가하는 단점이 있다. 도 41은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 유형 별 결함 판단 데이터를 도시한다. 구 체적으로, 도 41은 예시적인 3차원 형태의 검사 대상 제품인 플라스틱 성형 용기의 표면 결함 이미지에 대하여 비전 검사 시스템을 적용한 결함 판단 데이터 세트를 도시한다.제품 생산의 마지막 단계인 조립 공정에서 제품의 결함 데이터를 수집한다. 본 발명의 다양한 실시 예들에서는 총 8가지 유형의 결함 이미지를 수집하였다. 복수의 카메라, 예를 들어, 7개의 카메라를 통해 수집된 결함 이미 지는 미리 세팅해놓은 608*608 해상도로 크롭(Crop)이 되어 필요한 이미지만 남겨 둔다. 또한 크롭(Crop)된 이 미지는 그레이 스케일(Gray Scale)로 변환하여 .jpg 파일로 저장된다. 실험을 위해 사용된 데이터 세트는 총 3,048개이며 세부내용은 도 41에 도시된 바와 같다. 도 42는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 유형 별 결함 판단 데이터를 도시한다. 구 체적으로, 도 42는 예시적인 3차원 형태의 검사 대상 제품인 플라스틱 성형 용기의 표면 결함 이미지에 대하여 비전 검사 시스템을 적용한 결함 판단 데이터 세트를 도시한다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템은 3차원 형태의 검사 대상 제품의 다면 이미지(Multi- sided Image) 획득을 위해 7개의 CMOS 카메라, 다중 각도 직접 조명(Multi-Angle Direct Lights), 광 센서 (Optical Sensor), H/W 트리거(Trigger) 등을 포함할 수 있다. 또한, 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 딥 러닝 모델은 i7-9700F, DDR4-32GB, GTX 2080Ti가 장착된 Windows 10 Pro에서 PANet 알고리 즘을 학습하였으며 8가지 유형의 결함 데이터베이스를 총 16,000회 학습했다. 학습 과정에서 배치(Batch)는 64, 해상도(Resolution)는 608 * 608, 학습률(Learning Rate)은 0.00261, 활성화 함수(Activation Function)는 Leaky ReLU를 사용했다. 799개의 테스트 이미지에서 결함을 검출하는데 소요된 시간은 24.93s이며 mAP 0.96%, 정확도(Precision) 0.94%, 리콜(Recall) 0.98%, F1 점수(F1-Score) 0.96%, 평균 IoU(Average IoU) 72.89%의 결 과가 나타났다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템을 통해 3차원 형태의 검사 대상 제품의 다면 이미지 (Multi-sided Image)를 추론(Inference)하는 시간은 평균 0.224/s이며 단면 이미지를 처리하는 시간은 평균 0.032/s밖에 되지 않는다. 실제 생산 현장에서의 생산 속도는 0.09m/s, 카메라의 거리는 22~35cm이다. 또한 생 산 현장에서 요구하는 생산 속도를 충족하기 위해서는 30 이상의 FPS(frame per second)가 보장되어야 한다. 본 발명의 다양한 실시 예들에서 제안하는 PANet 알고리즘을 사용하여 분석한 결과 35 FPS의 속도를 달성하여 생산 현장에서 요구하는 검사 속도를 완전히 충족하였다. 마지막으로 정상제품과 불량제품은 별도의 분류기를 통해 분류되었다. 결함 검사의 정확도는 도 42에 도시된 바와 같다. 도 43은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 유형 별 결함 판단 데이터를 도시한다. 구 체적으로, 도 43은 각 딥 러닝 모델 별 결함 검사 결과의 비교를 도시한다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 성능을 평가하기 위해 PANet(Path Aggregation Network), CNN(Convolutional Neural Network), YOLO v3를 통해 딥 러닝 모델의 성능을 비교하였다. 알고리즘 모델 외의 모든 환경은 동일하게 구성하였으며 각 알고리즘 모델 비교에 사용된 데이터 세트는 동일하다. 도 43은 각 모델의 결함 검사 결과를 도시한다. 예시적인 3차원 형태의 검사 대상 제품인 플라스틱 성형 용기의 8가지 결함 유형에 대해 검사를 진행하였고 검사 결과 모든 결함 유형에서 PANet의 정확도가 가장 높은 것으로 확인되었다. 그 다음으로는 물체 감지(Object Detection) 모델인 YOLO v3의 정확도가 높았으며 비교된 모델 중 CNN의 정확도가 가장 낮게 나타났다. 또한 PANet 모델은 결함 이미지를 분류할 뿐만 아니라 결함에 대한 정보를 정확하게 얻을 수 있었다. 도 44는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델의 정확도 및 성능 비교 데이터를 도시한다. 구체적으 로, 도 44는 각 딥 러닝 모델 별 결함 검사의 정확도 및 성능의 비교를 도시한다. 실제 제조 현장에서 검사 시스템을 사용하기 위해서는 정확도 못지않게 제품의 결함을 처리하고 분석하는 속도 가 매우 중요하다. 환경에 따라 제품의 생산 속도가 각각 상이하며 검사 시스템은 제품 생산에 차질이 없도록 요구조건을 충족해야 한다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템에서 제안하는 PANet 모델은 3 차원 형태의 제품의 다면 이미지(Multi-sided Image)에 대하여 검사 시간이 평균 0.224/s이며 단면 이미지에 대 하여 검사 시간이 평균 0.032/s이다. YOLO v3와 CNN의 다면 이미지(Multi-sided Image)에 대한 검사 시간은 각 각 0.266/s, 1.498/s이며, 단면 이미지에 대한 검사 시간은 각각 0.038/s, 0.214/s이다. 각 모델의 처리 속도 비교 결과 PANet의 처리 속도가 가장 빠른 것으로 확인되었다. 그 다음으로는 YOLO v3의 처리 속도가 빨랐으며비교된 모델 중 CNN이 가장 느린 것으로 확인되었다. 실험의 환경에서 요구하는 생산 속도는 0.09m/s이며 이를 충족하기 위해서는 30 이상의 FPS가 보장되어야 한다. 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템에서 제안하는 PANet 모델을 활용하여 분석한 결과 35 FPS 를 달성하여 생산 현장에서 요구하는 조건을 완전히 충족하였다. 또한, YOLO v3 모델도 32FPS를 달성하여 요구 조건을 충족하였으나 정확도와 처리 속도 면에서 PANet 모델이 우수한 결과를 보여주었다. CNN 모델의 FPS은 15 미만으로 실험 환경에서의 적용은 어려운 것으로 확인되었다. 하드웨어를 이용하여 본 발명의 실시 예를 구현하는 경우에는, 본 발명을 수행하도록 구성된 ASICs(application specific integrated circuits) 또는 DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays) 등이 본 발명의 프로 세서에 구비될 수 있다. 한편, 상술한 방법은, 컴퓨터에서 실행될 수 있는 프로그램으로 작성 가능하고, 컴퓨터 판독 가능 매체를 이용 하여 상기 프로그램을 동작시키는 범용 디지털 컴퓨터에서 구현될 수 있다. 또한, 상술한 방법에서 사용된 데이 터의 구조는 컴퓨터 판독 가능한 저장 매체에 여러 수단을 통하여 기록될 수 있다. 본 발명의 다양한 방법들을 수행하기 위한 실행 가능한 컴퓨터 코드를 포함하는 저장 디바이스를 설명하기 위해 사용될 수 있는 프로그램 저장 디바이스들은, 반송파(carrier waves)나 신호들과 같이 일시적인 대상들은 포함하는 것으로 이해되지는 않 아야 한다. 상기 컴퓨터 판독 가능한 저장 매체는 마그네틱 저장매체(예를 들면, 롬, 플로피 디스크, 하드 디스 크 등), 광학적 판독 매체(예를 들면, 시디롬, DVD 등)와 같은 저장 매체를 포함한다. 이상에서 설명된 실시 예들은 본 발명의 구성요소들과 특징들이 소정 형태로 결합된 것들이다. 각 구성요소 또 는 특징은 별도의 명시적 언급이 없는 한 선택적인 것으로 고려되어야 한다. 각 구성요소 또는 특징은 다른 구 성요소나 특징과 결합되지 않은 형태로 실시될 수 있다. 또한, 일부 구성요소들 및/또는 특징들을 결합하여 본 발명의 실시 예를 구성하는 것도 가능하다. 발명의 실시 예들에서 설명되는 동작들의 순서는 변경될 수 있다. 어느 실시 예의 일부 구성이나 특징은 다른 실시 예에 포함될 수 있고, 또는 다른 실시 예의 대응하는 구성 또 는 특징과 교체될 수 있다. 특허청구범위에서 명시적인 인용 관계가 있지 않은 청구항들을 결합하여 실시 예를 구성하거나 출원 후의 보정에 의해 새로운 청구항으로 포함시킬 수 있음은 자명하다."}
{"patent_id": "10-2021-0072282", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 블록도를 도시한다. 도 2는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 3은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 4는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 5는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 6은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 7은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 8은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 9는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 10은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 11은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 모니터링 화면을 도시한다. 도 12는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템의 구성을 도시한다. 도 13은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 광 센서에 기반한 트리거 모드의 구성을 도시 한다. 도 14는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 복수의 카메라의 구성을 도시한다. 도 15는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 복수의 카메라의 구성의 일 예를 도시한다. 도 16은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 복수의 조명의 구성을 도시한다. 도 17은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 복수의 카메라의 인터페이스 구성을 도시한다. 도 18은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 딥 러닝 모델의 트레이닝 과정을 도시한다. 도 19는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 프로세서의 이미지 데이터 처리 과정을 도시 한다. 도 20은 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 예시적인 결함 유형을 도시한다. 도 21은 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 하단에 대한 결함을 도시한다. 도 22는 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 상단에 대한 결함을 도시한다. 도 23은 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 측면에 대한 결함을 도시한다. 도 24는 본 발명의 다양한 실시 예들에 따른 검사 대상 제품의 유형 별 결함 검사 결과를 도시한다. 도 25는 본 발명의 다양한 실시 예들에 따른 인공 지능의 개념을 도시한다. 도 26은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델과 기계 학습 모델의 성능을 도시한다. 도 27은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델과 기계 학습 모델의 과정을 도시한다. 도 28은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델의 아키텍처를 도시한다. 도 29는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델의 예측 구현 방식을 도시한다. 도 30은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델의 아키텍처를 도시한다.도 31은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델의 인공 신경망 구조와 일반적인 인공 신경망 구조의 비교를 도시한다. 도 32는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델 중 컨볼루션 레이어의 매트릭스를 도시한다. 도 33은 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 제1 컨베이어와 제2 컨베이어의 구성을 도시 한다. 도 34는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템에서 결함 여부 판단에 대하여 생성한 바이너리 코드의 구성을 도시한다. 도 35는 본 발명의 다양한 실시 예들에 따른 비전 검사 시스템 내 제1 컨베이어와 제2 컨베이어의 구성을 도시 한다. 도 36는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델 별 물체 감지 능력의 비교를 도시한다. 예들에 따른 비전 검사 시스템 내 제1 컨베이어와 제2 컨베이어의 구성을 도시한다. 도 37는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 물체 감지 과정을 도시한다. 도 38은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 물체 감지 과정을 도시한다. 도 39는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 물체 감지 과정을 도시한다. 도 40은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 물체 감지 과정을 도시한다. 도 41은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 유형 별 결함 판단 데이터를 도시한다. 도 42는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 유형 별 결함 판단 데이터를 도시한다. 도 43은 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델에 기반한 유형 별 결함 판단 데이터를 도시한다. 도 44는 본 발명의 다양한 실시 예들에 따른 딥 러닝 모델의 정확도 및 성능 비교 데이터를 도시한다."}
