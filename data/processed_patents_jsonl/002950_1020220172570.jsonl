{"patent_id": "10-2022-0172570", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0166853", "출원번호": "10-2022-0172570", "발명의 명칭": "인간의 전략을 인공신경망에 적용한 문장형 대수 문제 풀이 방법 및 장치", "출원인": "서울대학교산학협력단", "발명자": "권가진"}}
{"patent_id": "10-2022-0172570", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "데이터를 입력받고, 이를 연산 처리한 결과를 출력하기 위한 입출력부;문제 풀이 방법을 수행하기 위한 프로그램이 저장되는 저장부; 및적어도 하나의 프로세스를 포함하며, 상기 프로그램을 실행시킴으로써 상기 입출력부를 통해 수신된 데이터를분석하는 제어부를 포함하고,상기 제어부는,문장형 수학 문제를 입력받아 인코더-디코더 기반의 설명문 생성 모델을 통해 상기 문장형 수학 문제에 내포된숫자와 변수에 관한 설명문을 생성하여 출력하고,상기 문장형 수학 문제, 상기 생성된 설명문, 또는 이들의 조합을 입력받아 인코더-디코더 기반의 수학식 생성모델을 통해 수학식을 생성하여 출력하는, 문제 풀이 장치."}
{"patent_id": "10-2022-0172570", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 설명문 생성 모델의 상기 인코더는 상기 문장형 수학 문제를 입력받아 토큰으로 분절하고 각 토큰마다 상기 문장형 수학 문제의 맥락 정보를 나타내는 문제 맥락 벡터를 출력하고,상기 설명문 생성 모델은 상기 설명문 생성 모델의 상기 인코더 및 상기 디코더 사이에 연결된 변수 예측기를포함하고, 상기 변수 예측기는 상기 문제 맥락 벡터를 입력받아 대푯값을 변형하여 상기 문장형 수학 문제를 풀기 위해 필요한 상기 변수의 개수를 예측하고,상기 설명문 생성 모델의 상기 디코더는 상기 문제 맥락 벡터, 상기 변수의 개수를 고려한 변수, 및 상기 문장형 수학 문제의 숫자를 입력받아 이전에 생성된 설명 토큰을 기반으로 상기 디코더의 은닉 상태를 계산하고,상기 설명문 생성 모델은 상기 디코더에 연결된 포인터 생성기를 포함하고, 상기 포인터 생성기는 상기 디코더의 상기 은닉 상태를 입력받아 다음 설명 토큰을 예측하는, 문제 풀이 장치."}
{"patent_id": "10-2022-0172570", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 제어부는, 상기 설명문 생성 모델을 통해 상기 문장형 수학 문제에서 상기 숫자가 위치하는 문맥을 선별하여 생성한 상기숫자에 관한 설명문을 제1 문장으로 재구조화하고, 상기 설명문 생성 모델을 통해 상기 변수의 인덱스를 이용하여 생성한 상기 변수에 관한 설명문을 제2 문장으로재구조화하고,재구조화한 상기 제1 문장 및 상기 제2 문장을 결합하여 재결합 문제를 생성하는, 문제 풀이 장치."}
{"patent_id": "10-2022-0172570", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 제어부는,상기 재결합 문제 및 상기 문장형 수학 문제를 상기 수학식 생성 모델의 상기 인코더에 입력하고, 상기 수학식 생성 모델의 상기 인코더는 상기 재결합 문제 및 상기 문장형 수학 문제를 입력받아 재결합 맥락벡터를 출력하고, 공개특허 10-2023-0166853-3-상기 수학식 생성 모델의 상기 디코더는 상기 재결합 맥락 벡터를 입력받아 연산자와 필요한 피연산자를 그룹화한 표현식 토큰 단위를 사용하여 상기 수학식을 생성하는, 문제 풀이 장치."}
{"patent_id": "10-2022-0172570", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "문제 풀이 장치에 의한 문제 풀이 방법에 있어서,문장형 수학 문제를 입력받아 인코더-디코더 기반의 설명문 생성 모델을 통해 상기 문장형 수학 문제에 내포된숫자와 변수에 관한 설명문을 생성하여 출력하는 단계; 및 상기 문장형 수학 문제, 상기 생성된 설명문, 또는 이들의 조합을 입력받아 인코더-디코더 기반의 수학식 생성모델을 통해 수학식을 생성하여 출력하는 단계를 포함하는, 문제 풀이 방법."}
{"patent_id": "10-2022-0172570", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서,상기 설명문 생성 모델의 상기 인코더는 상기 문장형 수학 문제를 입력받아 토큰으로 분절하고 각 토큰마다 상기 문장형 수학 문제의 맥락 정보를 나타내는 문제 맥락 벡터를 출력하고,상기 설명문 생성 모델은 상기 설명문 생성 모델의 상기 인코더 및 상기 디코더 사이에 연결된 변수 예측기를포함하고, 상기 변수 예측기는 상기 문제 맥락 벡터를 입력받아 대푯값을 변형하여 상기 문장형 수학 문제를 풀기 위해 필요한 상기 변수의 개수를 예측하고,상기 설명문 생성 모델의 상기 디코더는 상기 문제 맥락 벡터, 상기 변수의 개수를 고려한 변수, 및 상기 문장형 수학 문제의 숫자를 입력받아 이전에 생성된 설명 토큰을 기반으로 상기 디코더의 은닉 상태를 계산하고, 상기 설명문 생성 모델은 상기 디코더에 연결된 포인터 생성기를 포함하고, 상기 포인터 생성기는 상기 디코더의 상기 은닉 상태를 입력받아 다음 설명 토큰을 예측하는, 문제 풀이 방법."}
{"patent_id": "10-2022-0172570", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 5 항에 있어서,상기 수학식을 생성하여 출력하는 단계는,상기 설명문 생성 모델을 통해 상기 문장형 수학 문제에서 상기 숫자가 위치하는 문맥을 선별하여 생성한 상기숫자에 관한 설명문을 제1 문장으로 재구조화하고, 상기 설명문 생성 모델을 통해 상기 변수의 인덱스를 이용하여 생성한 상기 변수에 관한 설명문을 제2 문장으로재구조화하고,재구조화한 상기 제1 문장 및 상기 제2 문장을 결합하여 재결합 문제를 생성하는, 문제 풀이 방법."}
{"patent_id": "10-2022-0172570", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서,상기 수학식을 생성하여 출력하는 단계는,상기 재결합 문제 및 상기 문장형 수학 문제를 상기 수학식 생성 모델의 상기 인코더에 입력하고,상기 수학식 생성 모델의 상기 인코더는 상기 재결합 문제 및 상기 문장형 수학 문제를 입력받아 재결합 맥락벡터를 출력하고, 상기 수학식 생성 모델의 상기 디코더는 상기 재결합 맥락 벡터를 입력받아 연산자와 필요한 피연산자를 그룹화한 표현식 토큰 단위를 사용하여 상기 수학식을 생성하는, 문제 풀이 방법."}
{"patent_id": "10-2022-0172570", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 5 항에 기재된 방법을 수행하는 프로그램이 기록된 컴퓨터 판독 가능한 기록 매체."}
{"patent_id": "10-2022-0172570", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "공개특허 10-2023-0166853-4-문제 풀이 장치에 의해 수행되며, 제 5 항에 기재된 방법을 수행하기 위해 기록 매체에 저장된 컴퓨터프로그램."}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "문제 풀이 장치 및 방법을 제시한다. 문제 풀이 장치는 데이터를 입력받고, 이를 연산 처리한 결과를 출력하기 위한 입출력부, 문제 풀이 방법을 수행하기 위한 프로그램이 저장되는 저장부, 및 적어도 하나의 프로세스를 포 함하며, 상기 프로그램을 실행시킴으로써 상기 입출력부를 통해 수신된 데이터를 분석하는 제어부를 포함하고, 상기 제어부는, 문장형 수학 문제를 입력받아 인코더-디코더 기반의 설명문 생성 모델을 통해 상기 문장형 수학 문제에 내포된 숫자와 변수에 관한 설명문을 생성하여 출력하고, 상기 문장형 수학 문제, 상기 생성된 설명문, 또는 이들의 조합을 입력받아 인코더-디코더 기반의 수학식 생성 모델을 통해 수학식을 생성하여 출력한다."}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 명세서에서 개시되는 실시예들은 문장형 대수 문제 풀이 방법 및 장치에 관한 것으로, 더욱 상세하게는, 인 간의 전략을 인코더-디코더 구조의 인공신경망 모델에 적용하여 주어진 수학 문제를 해석하고 이를 토대로 문제 속 숫자와 변수에 관한 설명을 자연어 형태로 생성하고, 수학 문제를 올바른 수학식으로 자동 생성하는 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간의 문제 풀이 전략은 크게 문제를 이해하는 단계, 수학식을 작성하는 단계, 및 답을 계산하는 단계로 구분 된다. 문제를 이해하는 단계와 수학식을 작성하는 단계에서 숫자 간의 관계와 같은 맥락 정보(contextual information)를 사용한다. 인간은 문제를 이해하는 단계에서 주어진 문제를 설명함으로써 숫자와 변수에 대한 정보를 추출한다. 인간은 수 학식을 작성하는 단계에서 연산자와 피연산자 간의 관계에 대한 정보를 보존하는 여러 템플릿을 결합함으로써 수학식을 작성한다. 문장형 문제 풀이에 관한 기계 학습은 주어진 문장형 문제를 인코딩하고 수학식으로 디코딩하도록 설계된 모델 을 구축한다. 기존에는 문장형 수학 문제를 풀기 위해서 전문가에 의해 정의된 학습 특징(feature)들을 데이터 세트에 구축한 후, 해당 특징들을 기계 학습 모델이 학습하여 문제를 유형들로 구분하고, 이어서 해당 유형에 맞는 풀이를 적 용하는 방식이 보편적이었으나 이러한 방식은 모델 구축에 전문가의 작업이 요구되는 문제가 있다. 인공신경망을 사용하게 되면 전문가가 사전에 정의한 특징을 사용할 필요가 없으나, 인공신경망이 문제를 어떻 게 해석하고 어떤 과정을 거쳐서 수학식을 추론한 것인지를 해석하기가 어렵다는 문제가 있다. 따라서, 인공신경망을 사용하여 추론하는 자동화된 모델 및 인공신경망을 해석할 수 있는 근거자료로서 문제의 주요 부분에 대한 자연스러운 줄글 설명을 생성하는 모델이 필요한 실정이다. 참고로, 특허문헌 1은 인공지능 기반 수학 문제 해결장치에 관한 발명이고, 특허문헌 2는 신경망 기반 기계번역 및 셈뭉치를 이용한 수학 문제 개념유형 예측 서비스 제공 방법에 관한 발명으로, 특허문헌 1 및 특허문헌 2는 일반적인 수학 문제 풀이 내용만을 개시하고 있을 뿐, 인공신경망의 문제 이해도를 설명하면서 설명에 관한 신 뢰도를 제공할 수 있는 자동화된 수학 문제 풀이 기술을 제공하지 아니한다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제10-2110784호(2020.05.08.) (특허문헌 0002) 한국등록특허 제10-1986721호(2019.05.31.)"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 명세서에서 개시되는 실시예들은, 문장형 수학 문제에 대한 문제 이해 과정에서 문제 속 숫자와 변수에 관한 해석 결과를 자연어 형태로 생성하고, 수학식 작성 과정에서 해석 결과를 재구성한 재결합 문제를 생성하고, 재 결합 문제와 원 문제를 기반으로 연산자와 연관되는 피연산자를 함께 묶은 단위인 표현식을 사용하여 수학식을 생성하는 문제 풀이 방법 및 장치를 제공하는 데 그 목적이 있다. 본 발명의 다른 목적 및 장점들은 하기의 설명에 의해서 이해될 수 있으며, 일 실시예에 의해 보다 분명하게 알 게 될 것이다. 또한, 본 발명의 목적 및 장점들은 특허청구범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있 음을 쉽게 알 수 있을 것이다."}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 기술적 수단으로서, 문제 풀이 장치는 데이터를 입력받고, 이를 연산 처리 한 결과를 출력하기 위한 입출력부; 문제 풀이 방법을 수행하기 위한 프로그램이 저장되는 저장부; 및 적어도 하나의 프로세스를 포함하며, 상기 프로그램을 실행시킴으로써 상기 입출력부를 통해 수신된 데이터를 분석하는 제어부를 포함하고, 상기 제어부는, 문장형 수학 문제를 입력받아 인코더-디코더 기반의 설명문 생성 모델을 통 해 상기 문장형 수학 문제에 내포된 숫자와 변수에 관한 설명문을 생성하여 출력하고, 상기 문장형 수학 문제, 상기 생성된 설명문, 또는 이들의 조합을 입력받아 인코더-디코더 기반의 수학식 생성 모델을 통해 수학식을 생 성하여 출력한다. 다른 실시예에 따르면, 문제 풀이 장치에 의한 문제 풀이 방법에 있어서, 문장형 수학 문제를 입력받아 인코더- 디코더 기반의 설명문 생성 모델을 통해 상기 문장형 수학 문제에 내포된 숫자와 변수에 관한 설명문을 생성하 여 출력하는 단계; 및 상기 문장형 수학 문제, 상기 생성된 설명문, 또는 이들의 조합을 입력받아 인코더-디코 더 기반의 수학식 생성 모델을 통해 수학식을 생성하여 출력하는 단계를 포함한다. 또 다른 실시예에 따르면, 기록매체는, 문제 풀이 방법을 수행하는 프로그램이 기록된 컴퓨터 판독 가능한 기록 매체이다. 또 다른 실시예에 따르면, 컴퓨터 프로그램은, 문제 풀이 장치에 의해 수행되며, 문제 풀이 방법을 수행하기 위 해 기록 매체에 저장된 컴퓨터 프로그램이다."}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 과제 해결 수단 중 어느 하나에 의하면, 설명문 생성 모델이 문장형 수학 문제를 해석하고 이를 토대로 문제 속 숫자와 변수에 관한 해석을 자연어 형태로 생성하는 문제 풀이 방법 및 장치를 제시할 수 있다. 또한, 전술한 과제 해결 수단 중 어느 하나에 의하면, 문제 해석이 정확한지 모델이 스스로 비교 대조할 수 있 도록 문제 해석을 원 문제와 함께 수학식 생성 모델에 입력하여 수학식을 생성하는 문제 풀이 방법 및 장치를 제시할 수 있다. 또한, 전술한 과제 해결 수단 중 어느 하나에 의하면, 수학식 생성 모델이 연산자와 연관되는 피연산자를 함께 묶은 단위인 표현식을 사용하여 맥락 정보를 보존하는 문제 풀이 방법 및 장치를 제시할 수 있다. 또한, 전술한 과제 해결 수단 중 어느 하나에 의하면, 인공신경망 모델이 맥락 정보를 보존할 수 있도록 디코딩 과 인코딩 과정을 개선하여 정답률을 향상시키는 문제 풀이 방법 및 장치를 제시할 수 있다. 개시되는 실시예들에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "른 효과들은 아래의 기재로부터 개시되는 실시예들이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 다양한 실시예들을 상세히 설명한다. 아래에서 설명되는 실시예들은 여러 가지 상이한 형태로 변형되어 실시될 수도 있다. 실시예들의 특징을 보다 명확히 설명하기 위하여, 이하의 실시"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "예들이 속하는 기술분야에서 통상의 지식을 가진 자에게 널리 알려져 있는 사항들에 관해서 자세한 설명은 생략 하였다. 그리고, 도면에서 실시예들의 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부 분에 대해서는 유사한 도면 부호를 붙였다. 명세서 전체에서, 어떤 구성이 다른 구성과 \"연결\"되어 있다고 할 때, 이는 '직접적으로 연결'되어 있는 경우뿐 아니라, '그 중간에 다른 구성을 사이에 두고 연결'되어 있는 경우도 포함한다. 또한, 어떤 구성이 어떤 구성을 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한, 그 외 다른 구성을 제외하는 것이 아니라 다른 구 성들을 더 포함할 수도 있음을 의미한다. 이하 첨부된 도면을 참고하여 실시예들을 상세히 설명하기로 한다. 아래 표 1은 문장형 수학 문제를 예시한 것이다. [표 1]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "표 1의 문제에서 쿼터는 0.25 달러(25 센트)이고, 니켈은 0.05 달러(5 센트)를 의미한다. 인공신경망 모델은 문 장형 수학 문제를 입력받아 문제로부터 적합한 수학식을 추론한다. 문장형 수학 문제에는 숫자가 기재되어 있으 나 변수는 기재되어 있지 않으므로, 인공신경망 모델은 적절한 변수를 추론해야 한다. 예컨대, 쿼터의 개수를 변수 x로 설정하고, 니켈의 개수를 변수 y로 설정하고, 풀이에 필요한 변수의 개수가 2 개에 해당하는 것을 찾 아야 한다. 인공신경망 모델은 문장형 수학 문제를 풀이할 때 인코딩 및 디코딩 과정을 거친다. 인공신경망 모델은 인코딩 과정에서 수학 문제를 올바르게 이해하고 숫자의 역할을 설명하는 숫자 정보로 변환한다. 예컨대, 인공신경망 모델은 12는 전체 동전 개수이고 2.20은 전체 동전 가격을 의미하는 것을 파악해야 한다. 인공신경망 모델은 디 코딩 과정에서 수학식의 계산 구조 및 작성된 부분을 추적하고 작은 계산 단위를 결합하여 수학식을 구성한다. 예컨대, 인공신경망 모델은 x+y 표현을 생성하고, 생성된 표현을 x+y=12에 재사용한다. 인공신경망 모델을 어떻게 설계하는지에 따라 인코딩 및 디코딩 과정에서 맥락 정보를 손실하게 된다. 인공신경 망 모델이 문장형 수학 문제를 풀 때 맥락 정보를 더 잘 보존하기 위해서 아래 표 2와 같은 관계를 고려할 필요 가 있다.[표 2]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "본 실시예에 따른 문제 풀이 장치는 수학식을 작성하는 디코딩 과정에서 수학식의 계산 구조의 맥락 정보를 포 착하기 위해서 연산자와 관련된 피연산자를 그룹화한 표현식을 사용하는 수학식 생성 모델을 설계한다. 표현식 이 연산자와 피연산자 간의 관계를 표시하므로, 수학식 생성 모델은 이전에 생성한 표현식의 맥락 정보를 식별 할 수 있고, 수학식 생성 모델은 생성한 표현식과 추가로 생성할 표현식을 쉽게 파악할 수 있다. 수학식 생성 모델은 표현식을 활용하여 정답률을 개선한다. 본 실시예에 따른 문제 풀이 장치는 문제를 이해하는 인코딩 과정에서 문제를 구성하는 숫자 등의 요소의 맥락 정보를 포착하기 위해서 숫자와 변수를 설명하는 설명문 생성 모델을 설계한다. 설명문 생성 모델은 생성된 설 명과 원 문제를 비교하여 인공신경망 모델의 이해 정도를 확인할 수 있다. 생성된 설명을 참고하여 인공신경망 모델이 문제를 오해했는지를 식별할 수 있다. 설명문 생성 모델이 올바른 설명을 예측할수록 인공신경망 모델의 풀이 정확도를 향상시킨다. 도 1은 일 실시예에 따른 문제 풀이 장치의 기능 블록도이다. 도 1을 참조하면, 일 실시예에 따른 문제 풀이 장치는 입출력부, 저장부 및 제어부를 포함 한다. 입출력부는 사용자로부터 입력을 수신하기 위한 입력부와 작업의 수행결과 또는 문제 풀이 장치의 상 태 등의 정보를 표시하기 위한 출력부를 포함할 수 있다. 즉, 입출력부는 데이터를 입력받고, 이를 연산 처리한 결과를 출력하기 위한 구성이다. 실시예에 따른 문제 풀이 장치는 입출력부를 통해 문장형 수 학 문제 등을 수신할 수 있다. 저장부는 파일 및 프로그램이 저장될 수 있는 구성으로서, 다양한 종류의 메모리를 통해 구성될 수 있다. 특히, 저장부에는 후술하는 제어부가 이하에서 제시되는 알고리즘에 따라 문제 풀이를 위한 연산을 수행할 수 있도록 하는 데이터 및 프로그램이 저장될 수 있다. 제어부는 CPU, GPU, 아두이노 등과 같은 적어도 하나의 프로세서를 포함하는 구성으로서, 문제 풀이 장치 의 전체적인 동작을 제어할 수 있다. 즉, 제어부는 문제 풀이를 위한 동작을 수행하도록 문제 풀이 장치에 포함된 다른 구성들을 제어할 수 있다. 제어부는 저장부에 저장된 프로그램을 실행함으 로써 이하에서 제시되는 알고리즘에 따라 문제 풀이를 하기 위한 연산을 수행할 수 있다. 제어부는 인공신경망 모델을 통해 문장형 수학 문제로부터 설명문을 생성한다. 제어부는 문장형 수학 문제를 입력받아 인코더-디코더 기반의 설명문 생성 모델을 통해 문장형 수학 문제에 내포된 숫자와 변수에 관 한 설명문을 생성하여 출력한다. 제어부는 문제 속의 숫자와 관련된 맥락 정보를 활용하여 설명문을 생성 하며, 생성된 설명문을 수학식 생성에 활용한다. 생성된 설명문은 인공신경망 모델이 문제를 얼마나 올바르게 이해했는지를 판단하는 기준이 될 수 있다. 제어부는 인공신경망 모델을 통해 설명문을 변환한 문제와 원래의 문장형 수학 문제를 결합한 문제로부터 수학식을 생성한다. 제어부는 문장형 수학 문제, 생성된 설명문, 또는 이들의 조합을 입력받아 인코더-디 코더 기반의 수학식 생성 모델을 통해 수학식을 생성하여 출력한다. 설명문을 변환한 문제를 원래의 문장형 수학 문제와 함께 입력하면, 설명문을 변환한 문제가 원래의 문장형 수학 문제 속의 맥락 정보를 보완하여 인공신 경망 모델의 성능을 개선할 수 있다. 도 2는 일 실시예에 따른 문제 풀이 장치에 적용된 인공신경망 모델을 예시한 도면이다. 인공신경망 모델은 설명문 생성 모델 및 수학식 생성 모델을 포함한다. 설명문 생성 모델 및 수 학식 생성 모델은 인코더(Encoder)-디코더(Decoder) 구조의 복수 레이어로 설계된다. 인코더는 고차원 정 보를 특징을 대표하는 저차원 정보로 변환하는 임베딩(Embedding) 처리를 수행한다. 디코더는 저차원 정보로부 터 고차원 정보로 변환하여 특징을 재구성한다. 설명문 생성 모델은 인코더, 디코더, 변수 예측기, 및 포인터 생성기를 포함한다. 설명문 생성 모델의 인코더는 문장형 수학 문제를 입력받아 토큰으로 분절하고 각 토큰마다 문장형 수학 문제의 맥락 정보를 나타내는 문제 맥락 벡터를 출력한다. 설명문 생성 모델의 디코더는 문제 맥락 벡터, 변수의 개수를 고려한 변수, 및 문장형 수학 문제의 숫자를 입력받아 이전에 생성된 설명 토큰을 기반으로 디코더의 은닉 상태(hidden state)를 계산한다. 변수 예측기는 설명문 생성 모델의 인코더 및 디코더 사이에 연결된다. 변수 예측기 는 문제 맥락 벡터를 입력받아 대푯값을 변형하여 문장형 수학 문제를 풀기 위해 필요한 변수의 개수를 예측한 다. 포인터 생성기는 설명문 생성 모델의 디코더에 연결되며, 포인터 생성기는 디코더의 은닉 상태를 입력받아 다음 설명 토큰을 예측한다. 제어부는 설명문 생성 모델을 통해 문장형 수학 문제에서 숫자가 위치하는 문맥을 선별하여 생성한 숫자에 관한 설명문을 제1 문장으로 재구조화한다. 제어부는 설명문 생성 모델을 통해 변수의 인덱스 를 이용하여 생성한 변수에 관한 설명문을 제2 문장으로 재구조화한다. 제어부는 재구조화한 제1 문장 및 제2 문장을 결합하여 재결합 문제를 생성한다. 수학식 생성 모델은 인코더 및 디코더를 포함한다. 제어부는 재결합 문제 및 문장형 수학 문제를 수학식 생성 모델의 인코더에 입력한다. 수학식 생성 모델의 인코더는 재결합 문제 및 원래의 문장형 수학 문제를 입력받아 재결합 맥락 벡터 를 출력한다. 수학식 생성 모델의 디코더는 재결합 맥락 벡터를 입력받아 연산자와 필요한 피연산자를 그룹화한 표 현식 토큰 단위를 사용하여 수학식을 생성한다. 도 3 및 도 4는 일 실시예에 따른 문제 풀이 장치에 적용 가능한 수학식 생성 모델의 동작 원리를 예시한 도면 이다. 문제 풀이 장치에 적용 가능한 수학식 생성 모델(Expression Pointer Transformer, EPT)은 표현식 토큰을 사용 하여 표현식 조각화 문제를 해결하고, 피연산자-맥락 정보 분리 문제를 해결한다. 도 3의 (a)를 참조하면, 표현식 조각화 문제는 수학식의 계산 구조를 나타내는 표현식 트리가 분할되는 것이다. 표현식 조각화 문제의 예시로, 기존의 연산자/피연산자 토큰을 적용하면, 연산자 'x', 피연산자 'x1'와 'N2'로 분해된다. 분해된 3 개의 토큰은 N2 x x1이라는 단일 표현으로 재결합하기 전까지는 의미가 없고, 모델을 혼동시 킨다. 피연산자-맥락 정보 분리 문제는 피연산자 및 피연산자에 관련된 숫자 간에 연결이 끊어지는 것이다. 피연산자- 맥락 정보 분리 문제의 예시로, 기존의 연산자/피연산자 토큰을 적용할 때, 숫자 '8'을 사용하지 않고 문제에 나타나지 않은 추상적인 기호 'N2'를 사용하게 된다. 추상적인 기호 'N2'에 대응하는 숫자가 어떤 숫자인지 원 문제에서 찾기 쉽지 않다. 즉, 표현식 조각화 문제 및 피연산자-맥락 정보 분리 문제로 인하여 수학식을 생성하는 과정에서 계산 구조의 맥락 정보가 손실된다. 수학식 생성 모델은 연산자 및 연산자와 관련된 피연산자를 그룹화한 표현식 토큰 단위를 적용하여, 기존의 연 산자/피연산자 토큰보다 더 많은 계산 구조의 맥락 정보를 보존할 수 있다. 그룹화한 표현식 토큰 단위는 트리 구조를 유지하고 피연산자의 맥락 정보를 직접 사용하므로, 표현식 단편화 문제 및 피연산자-맥락 정보 분리 문 제를 해결할 수 있다. 도 3의 (b)에 도시된 바와 같이, 연산자 및 관련 피연산자를 그룹화한 표현식 토큰을 적용하면, 3 개의 토큰 각 각은 표현식 트리의 하위 트리 중 하나를 나타낸다. 수학식 생성 모델은 연산자 및 관련 피연산자를 그룹화한 표현식 토큰을 이용하여 연산자 및 관련 피연산자에 대한 맥락 정보를 명시적으로 고려할 수 있다. 연산자 및 관련 피연산자를 그룹화한 표현식 토큰을 적용할 때, 문제에 쓰여진 숫자를 추상적인 기호로 변환하 지 않고, 숫자가 발생한 위치에 포인터를 생성한다. 각각의 피연산자는 쓰여진 숫자 또는 이전 출력을 직접 가 리킨다. 수학식 생성 모델은 피연산자-맥락 포인터를 이용하여 피연산자에 대한 맥락 정보를 직접 접근할 수 있 다. 도 4를 참조하면, 문제 풀이 장치에 적용 가능한 수학식 생성 모델(Expression Pointer Transformer, EPT)의 인코더는 문제를 읽고 각 토큰에 대한 인코더의 은닉 상태 벡터를 출력한다. 그런 다음 디코더는 은닉 상태 벡 터를 메모리로 사용하고 단계적으로 표현식을 생성한다. 인코더는 문제 텍스트를 읽고 인코더의 은닉 상태 벡터와 숫자의 맥락 벡터를 생성한다. 인코더는 입력된 문제 텍스트를 일련의 하위 단어 토큰으로 토큰화한다. 그런 다음 각 토큰을 임베딩 벡터로 변 환한다. 인코더는 임베딩 벡터를 사용하여 각 토큰에 대해 인코더의 은닉 상태 벡터를 계산한다. 인코더는 문제에 쓰여진 각 숫자에 대한 맥락 벡터를 획득한다. 인코더가 주어진 문제 텍스트를 하위 단어 토큰 으로 토큰화를 수행하므로, 쓰여진 숫자는 하나 이상의 은닉 상태 벡터를 가진다. 따라서 단일 벡터값을 얻기 위해 인코더는 이러한 은닉 상태 벡터의 평균을 취할 수 있다. 인코더의 초기 가중치는 사전 훈련된 언어 모델인 ALBERT(A Lite BERT for Self-supervised Learning of Language Representations, Lan et al., 2019) 모델을 참고할 수 있다. ALBERT모델은 대규모 코퍼스(corpus)에 서 배열된 단어나 문장을 예측하도록 훈련되었으므로 ALBERT 모델은 단어 문제를 해결하기 위한 지식을 제공할 수 있다. 디코더는 맥락 벡터를 사용하여 피연산자-맥락 정보 분리 문제를 해결한다. 디코더는 인코더의 은닉 상태 벡터 를 메모리로 사용하여 표현식 토큰을 생성한다. 표현식 토큰을 생성하는 전체 프로세스는 자동 회귀적으로 진행 된다. 수학식 생성 모델은 i 번째 단계 이전의 토큰을 사용하여 i 번째 토큰을 예측한다. 아래 표 3은 x0-2x1=8 및 x0+x1=20을 생성하는 8 개의 표현식 토큰 시퀀스의 예를 보여준다. [표 3]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "3개의 토큰(BEGIN, VAR 및 VAR)을 입력한 후 디코더는 연산자 ×와 피연산자 2와 R1을 동시에 예측해야 한다. 디코더는 세 개의 특수 명령을 목록에 포함한다. 'BEGIN'은 시퀀스 시작을 위한 명령이고, 'VAR'은 새 변수 생 성을 위한 명령이고, 'END'은 지금까지 생성된 모든 수학식을 수집하기 위한 명령이다. 수학식 생성 모델에 적용 가능한 디코더의 계산 구조는 트랜스포머(Transformer: Attention Is All You Need, Vaswani et al., 2017) 모델의 디코더의 계산 구조를 참고할 수 있다. 디코더가 i 번째 표현식 토큰을 예측한다고 가정하면, 디코더는 지금까지 생성된 표현식 토큰을 수신하여 임베 딩 벡터 vj(j=0, ... , i-1)로 변환한다. 임베딩 벡터 vj와 인코더의 은닉 상태 벡터 et를 기반으로 디코더는 i 번째 표현식 토큰에 대한 디코더의 은닉 상태 벡터 di를 구축한다. 그런 다음 디코더는 은닉 상태 벡터 di로 다 음 표현식 토큰을 예측한다. 트랜스포머 모델의 디코더와 달리, 수학식 생성 모델에 적용 가능한 디코더가 표현식 토큰을 수신하고 생성하도 록 입력 임베딩 부분과 출력 예측 부분을 수정한다. 입력 임베딩 부분과 관련하여, i 번째 표현식 토큰의 입력 벡터 vi는 수학식 1과 같이 연산자 임베딩 fi와 피연 산자 임베딩 aij를 결합하여 획득된다. [수학식 1]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "FF*는 피드-포워드 선형 레이어(feed-forward linear layer)이고, Concat()은 괄호 안의 모든 벡터를 결합하는 것을 의미한다. vi, fi, 및 aij는 동일한 차원 D를 가진다. i 번째 표현식의 연산자 토큰 fi에 대해서 연산자 임베딩 벡터 fi는 수학식 2와 같이 계산된다. [수학식 2]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "E*()는 임베딩 벡터를 위한 룩업 테이블, c*는 스칼라 파라미터(scalar parameter), LN*()은 레이어 정규화 (layer normalization) 및 PE()는 위치 인코딩(positional encoding)을 각각 나타낸다. i 번째 표현식의 j 번째 피연산자를 나타내는 임베딩 벡터 aij는 피연산자 aij의 소스에 따라 상이하게 계산된다. 피연산자의 맥락 정보를 반영하기 위해서 가능한 세 가지 소스는 문제 종속적인 숫자, 문제 독립적인 상수, 및 이전 표현식 토큰의 결과가 있다. 문제 종속적인 숫자는 대수 문제에서 제공된 숫자이다. 숫자 aij를 계산하기 위해 수학식 3과 같이 숫자 토큰에 해당하는 인코더의 은닉 상태 벡터를 재사용한다. [수학식 3]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "u*는 소스를 나타내는 벡터이고, 는 숫자 aij에 대응하는 맥락 벡터이다. 문제 독립적인 상수는 문제에 명시되지 않은 미리 정의된 숫자이다. 예컨대, 0.25는 쿼터의 양으로 사용된다. 상수 aij를 계산하기 위해 수학식 4과 같이 룩업 테이블 Ec를 사용한다. [수학식 4]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "이전 표현식 토큰의 결과는 현재 i 번째 단계 이전에 생성된 표현식 토큰이다. 예컨대, 표 3을 참조하면, 4 번 째 단계의 R0은 i 번째 단계 이전에 디코더의 은닉 상태 벡터에 접근할 수 있더라도. 수학식 생성 모델은 트레이 딩 과정에서 동시 디코딩(simultaneous decoding)을 유지하기 위해서 위치 인코딩을 사용한다. [수학식 5]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "k는 이전 표현식 aij가 생성된 단계의 인덱스를 나타낸다. LNa와 ca는 서로 다른 소스에서 공유된다. 입력 임베딩 부분은 표현식 조각화 문제와 피연산자-맥락 정보 분리 문제를 처리하는 데 적합하다. 표현식 단편 화 문제를 해결하기 위해 수학식 생성 모델의 디코더의 입력은 표현식의 구조를 유지한다. 따라서 연산자와 관 련 피연산자는 표현식의 입력 임베딩을 구성하는 데 사용된다. 피연산자 컨텍스트 분리 문제를 해결하기 위해 수학식 생성 모델의 디코더는 피연산자의 맥락 정보를 활용한다. 특히, 디코더는 추상적인 기호 대신 쓰여진 숫 자의 맥락 벡터를 직접 사용한다. 출력 예측 부분과 관련하여, 수학식 생성 모델의 디코더는 i 번째 표현식 토큰이 제공될 때 다음 연산자 fi+1과 피연산자 ai+1,j를 동시에 예측한다. 다음 연산자 fi+1는 수학식 6과 같이 예측된다. [수학식 6]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "는 소프트맥스(softmax) 함수 의 출력을 따르는 분포에서 항목 k를 선택할 확률이다. 피연산자를 예측할 때 피연산자의 맥락 정보를 활용하기 위해 출력 레이어는 포인터 네트워크(Pointer Networks, Vinyals et al., 2015)를 참조한 '피연산자-맥락 포인터'를 적용한다. 포인터 네트워크에서 출력 레 이어는 후보 벡터에 대한 어텐션(attention)을 사용하여 다음 토큰을 예측한다. 수학식 생성 모델은 피연산자의 소스에 따라 다른 세 가지 방법으로 다음 (i+1) 번째 표현식에 대한 후보 벡터 를 수집한다. 피연산자의 소스는 문제 내의 k 번째 숫자에 대한 , k 번째 표현식 출력에 대한 , 상수 x에대한 으로 구분된다. 수학식 생성 모델은 다음 j 번째 피연산자 ai+1,j를 예측한다. Aij를 행 벡터가 후보인 행렬이라고 가정하면, 수학 식 생성 모델은 키 행렬 Kij에서 쿼리 벡터 Qij의 어텐션을 계산하여 ai+1,j를 예측한다. [수학식 7]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "출력 예측 부분은 표현식 조각화 문제와 피연산자-맥락 정보 분리 문제를 처리하는 데 적합하다. 표현식 단편화 문제를 해결하기 위해 수학식 생성 모델의 디코더는 표현식의 모든 구성 요소를 동시에 예측한다. 연산자와 피 연산자는 동일한 디코더의 은닉 상태 벡터 di에서 생성된다. 피연산자-맥락 정보 분리 문제를 해결하기 위해 수 학식 생성 모델의 디코더는 추상적인 기호를 생성하는 대신 적절한 맥락 정보를 직접 가리킨다. 인코더의 은닉 상태 벡터와 디코더의 은닉 상태 벡터는 쓰여진 숫자의 맥락 정보와 이전 표현식 토큰을 각각 제공할 수 있다. 수학식 생성 모델은 다양한 손실 함수(연산자 손실 함수 및 피연산자 손실 함수)의 합을 최소화하도록 훈련된다. 손실 함수는 평활화된 크로스-엔트로피(smoothed cross-entropy)가 적용될 수 있다. 수학식 생성 모 델은 비가환 연산자(noncommutative operator)를 고려하여 각 피연산자 위치에 대해 피연산자 손실 함수를 별도 로 계산할 수 있다. 예컨대, 수학식 생성 모델에서 사용되는 연산자의 최대 애러티(arity)가 2이면, 하나의 연 산자 손실 함수와 두 개의 피연산자 손실 함수에 관한 세 개의 손실 함수가 적용될 수 있다. 도 5 및 도 6은 일 실시예에 따른 문제 풀이 장치에 적용 가능한 수학식 생성 모델에 대한 절제(ablation) 분석 을 예시한 도면이다. 도 4에 도시된 모델은 표현식 토큰(expression token)과 피연산자-맥락 포인터(operand-context pointer)를 모 두 적용한 모델이고, 도 5에 도시된 모델은 표현식 토큰 대신에 연산자/피연산자 토큰(op token)을 적용하고 피 연산자-맥락 포인터 대신에 추상적인 기호를 적용한 모델이고, 도 6에 도시된 모델은 표현식 토큰을 적용하고 피연산자-맥락 포인터 대신에 추상적인 기호를 적용한 모델이다. 아래 표 4는 도 4, 도 5, 및 도 6에 도시된 각 모델에 적용된 세 가지 데이터 세트의 크기 및 복잡도를 나타낸 다.[표 4]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "ALG514 및 DRAW-1K는 고복잡도 데이터 세트이고, MAWPS는 저복잡도 데이터 세트이다. ALG514 및 DRAW-1K에서의 미지수와 토큰의 수는 MAWPS보다 거의 두 배에 해당한다. 아래 표 5는 도 4, 도 5, 및 도 6에 도시된 각 모델의 정답 정확도를 나타낸다. [표 5]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "표현식 토큰을 적용한 모델(도 6)은 트랜스포머 모델(도 5)보다 ALG514 및 DRAW-1K에서 약 15% 정도, MAWPS에서 약 1% 정도로 정확도가 향상되었다. 표현식 토큰을 적용한 모델에 피연산자-맥락 포인터를 적용한 모델(도 4)은 ALG514 및 DRAW-1K에서 약 30% 정도, MAWPS에서 약 3% 정도로 정확도가 향상되었다. 이처럼 ALG514 및 DRAW-1K의 고복잡도 데이터 세트에서 두 가지 관점에서 더 높은 성능 개선을 확인할 수 있다. 첫 번째로 고복잡도 데이터 세트에서 미지수와 토큰의 수가 증가함에 따라 각 토큰에 대한 표현식 조각화 문제 가 발생할 확률이 기하급수적으로 증가하게 된다. 두 번째로 숫자와 표현식 토큰은 피연산자를 선택하기 위한 후보이므로 방정식에서 숫자와 표현식의 수가 증가함에 따라 피연산자-맥락 정보 분리 문제가 발생할 확률이 선 형적으로 증가하게 된다. 수학식 생성 모델에 표현식 토큰과 피연산자-맥락 포인터를 적용하면 각 토큰에 대한 표현식 조각화 문제와 피연산자-맥락 정보 분리 문제를 해결할 수 있기 때문에 고복잡도 데이터 세트에서 정확 도를 더 향상시킬 수 있다. 문장형 수학 문제를 풀이하는 인공신경망 모델의 정확도가 향상되더라도 인공신경망 모델이 문제의 숫자 정보를 올바르게 포착했는지 검증할 필요가 있다. 도 7은 일 실시예에 따른 문제 풀이 장치에 적용된 인공신경망 모델의 동작 원리를 예시한 도면이다. 본 실시예에 따른 인공신경망 모델(Expression Pointer Transformer with eXplanations, EPT-X)은 문제를 풀이 할 때 두 가지 기준을 충족하는 설명을 고려해야 한다. 첫 번째로 설명은 문제를 이해하는 과정에서 주어진 문 제를 철저히 반영하여 타당성(plausibility)을 충족시켜야 한다. 특히, 인간은 각 숫자 및 변수를 개별적으로 인식하므로 설명은 주어진 문제의 맥락에서 각 숫자 또는 변수가 나타내는 의미를 밝혀야 한다. 두 번째로 수학 식을 작성하는 과정에서 설명을 사용하여 충실도(faithfulness)를 충족시켜야 한다. 설명에 수학식을 작성하는 데 유용한 정보가 포함되어 있으면 연산자 또는 피연산자를 선택하는 기준으로 작용해야 한다. 타당성 있는 설명에 충실한 수학식을 작성하기 위해서 본 실시예에 따른 문제 풀이 장치는 도 4에서 설명한 수 학식 생성 모델을 변형하고 추가로 설명문 생성 모델을 설계한다. 문제 풀이 장치는 설명문 생성 모델에 기반한 숫자 또는 변수의 설명 동작과 수학식 생성 모델에 기반한 수학식 구축 동작을 각각 수행한다. 설명 동작은 원 문제(original problem)를 입력받아 선행 학습 언어 모델인 인코더를 통과하여 문제의 맥 락적 의미를 나타내는 벡터값인 문제 맥락 벡터(problem context vector)을 획득한다. 선행 학습 언어 모델로 ELECTRA(Pre-training Text Encoders as Discriminators Rather Than Generators, Clark et al., 2020) 모델 을 참고할 수 있다. 설명 동작은 생성된 벡터값을 변수 예측기(variable predictor, 230)에 입력하여 문제를 풀기 위해 필요한 변수 의 개수를 예측한다. 설명 동작은 문제 속의 각 숫자와 예측된 변수를 디코더에 각각 입력하여 숫자와 변수를 각각 설명하는 설 명문(explanation)을 획득한다. 설명을 생성하는 대상은 문제 전체가 아닌 숫자/변수와 같이 문제의 일부분이다. 수학식 구축 동작은 생성된 설명문을 문장으로 변환하고 이어 붙여서 설명을 재결합한 재결합 문제(recombined problem)를 생성한다. 생성된 재결합 문제는 원 문제의 의역(paraphrase) 결과물로서, 설명 동작에서 놓친 정보 가 있는지 모델이 스스로 원 문제와 비교해 확인할 수 있다. 수학식 구축 동작은 원 문제와 재결합 문제를 이어 붙이거나 어느 한쪽만 입력받아 선행 학습 언어 모델인 인코 더를 통과하여 새로운 벡터값인 재결합 맥락 벡터(recombined context vector)를 획득한다. 선행 학습 언 어 모델로 ELECTRA 모델을 참고할 수 있다. 필요에 따라 설명문 생성 모델의 인코더와 수학식 생성 모델의 인코더는 공유될 수 있다. 수학식 구축 동작은 새로운 벡터값을 디코더에 입력하여 주어진 문제를 풀기 위한 수학식을 예측한다. 설명 동작은 구체적으로 문제 맥락 벡터를 계산하는 동작, 변수의 개수를 예측하는 동작, 및 설명문을 생성하는 동작으로 나뉜다. 문제 맥락 벡터를 계산하는 동작은 원 문제가 선행 학습 언어 모델에 입력되면 입력된 원 문제를 토큰으로 분절 하고, 사전(dictionary)을 참조하여 각 토큰을 사전의 인덱스(index)로 변환한다. 분절 및 변환으로 생성된 인 덱스 목록을 선행 학습 언어 모델에 입력한다. 선행 학습 언어 모델의 계산 과정에 따라 각 토큰마다 벡터값을 생성하여, 문제 맥락 벡터를 획득한다. 문제 맥락 벡터를 계산하는 동작에서 설명문 생성 모델의 인코더는 자연어 문제 텍스트를 입력으로 수신하고, 문제 맥락 벡터를 계산한다. 문제 텍스트가 제공되면 인코더는 문제 텍스트를 일련의 하위 단어 토큰으로 토큰화한다. 그런 다음 인코더는 각 토큰을 임베딩 벡터로 변환한다. 인코더는 임베딩 벡터 를 사용하여 주어진 문제의 각 토큰 ws대해 문제 맥락 벡터 ws를 계산한다. 사전 훈련된 언어 모델인 ELECTRA 모델을 사용하여 인코더의 가중치를 초기화할 수 있다. ELECTRA 모델은 대규모 코퍼스에서 대체 단어를 예측하도록 훈련되었으므로 ELECTRA 모델은 주어진 문제를 이해하는 데 필요한 지식을 제공할 수 있다. 주어진 문제 텍스트는 필요한 변수의 개수 N를 숨기고 있으므로 설명문을 생성하기 전에 문제 맥락 벡터를 사용 하여 변수의 개수 N을 복구해야 한다. 변수의 개수를 예측하는 동작은 언어 모델이 생성한 여러 벡터값 중 문제를 표현할 수 있는 대푯값을 선택한다. 대푯값을 선택하는 방법은 여러 가지가 있으며, 예컨대, 첫 번째 값을 선택할 수 있다. 대푯값을 여러 개의 선 형 피드 포워드 레이어(linear feed-forward layer)와 활성화(activation) 함수를 통과하여 변형한다. 변형한 값을 기반으로 변수의 개수가 1부터 9까지 중 몇일지 그 확률을 예측한다. 변수의 개수를 예측하는 동작에서 설명문 생성 모델의 변수 예측기는 문제 맥락 벡터를 풀링 (pooling)하는 여러 방법 중에서 문제 텍스트의 전체 의미를 포함하는 첫 번째 토큰의 문제 맥락 벡터 w0를 사 용할 수 있다. 변수의 개수 N의 확률 분포는 수학식 8과 같이 계산된다. [수학식 8]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "FF()는 피드 포워드 레이어를 나타낸다. 변수의 최대 개수를 9로 설정될 수 있다. 설명문을 생성하는 동작은 선행 학습 언어 모델이 생성한 벡터와 설명을 생성할 숫자 또는 변수의 정보를 하나 씩 입력받아 디코더가 설명문을 완성한다. 설명문 생성 모델의 디코더는 트랜스포머 모델을 참 고할 수 있다. 숫자의 경우 숫자가 등장하는 문제의 문맥을 선별하여 입력하고 설명하게 한다. 변수의 경우 변 수의 인덱스를 입력하고 설명하게 한다. 모든 숫자와 모든 변수에 대한 설명을 생성하면 설명문을 생성하는 동 작이 종료된다. 설명문을 생성하는 동작에서 설명문 생성 모델의 디코더는 문제 맥락 벡터를 메모리로 사용하여 설명 문을 생성한다. 설명문 생성 모델은 트랜스포머(Transformer: Attention Is All You Need, Vaswani et al., 2017) 모델의 디코더와 포인터 생성기 네트워크(Get To The Point: Summarization with Pointer- Generator Networks, See et al., 2017)를 참고할 수 있다. 다음 설명문 토큰 xt+1을 예측하기 전에 디코더는 문제 맥락 벡터 ws와 이전에 생성된 설명문 토큰 x1, ... , xt을 기반으로 은닉 상태 ht를 계산한다. 설명문 생성하는 데 지식을 활용하기 위해 BERTGeneration(Leveraging Pre-trained Checkpoints for Sequence Generation Tasks, Rothe et al., 2020)를 적용하고, ELECTRA(Clark et al., 2020) 모델을 초기 가중치로 사용 할 수 있다. 설명문을 생성하는 동작에 따라 디코더의 임베딩은 초기화 후 고정될 수 있다. 포인터 생성기는 계산된 은닉 상태 ht를 수신하고, 다음 토큰을 예측한다. pg, Pv, 및 Pc를 각각 생성된 단 어를 사용할 확률, 어휘에서 토큰을 생성할 확률, 및 문제에서 토큰을 복사할 확률이라 하면, 다음 토큰 xt+1은 수학식 9와 같이 예측된다. [수학식 9]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "σ(), E(), 및 attn()는 각각 시그모이드(sigmoid), 임베딩(embedding) 및 단일 헤드 어텐션 점수 함수(single head attention scoring function)를 나타낸다. 는 벡터들의 결합(concatenation)을 나타낸다.설명문을 생성하는 동작에서 각 숫자/변수에 대한 설명문을 별도로 생성하여 설명문의 타당성을 확보한다. 설명 문 생성 모델은 모든 숫자와 변수에 고유한 초기 입력값을 사용한다. 트랜스포머 모델의 디코더의 초기 입 력값인 '[CLS]' 대신에 설명문 생성 모델은 “[CLS] explain: context [SEP]”를 입력한다. 여기서 'context' 부분은 숫자나 변수에 의존한다. 숫자의 경우 주어진 숫자 토큰 근처에 있는 토큰 윈도우를 사용한다. 예컨대, 윈도우 크기가 3이면, 지정된 토큰 앞에 위치한 3 개 토큰, 뒤에 위치한 3 개 토큰을 사용한 다. 변수의 경우 문제에 변수가 나타나지 않기 때문에 변수 인덱스를 사용한다. 예컨대, n번째 변수의 초기 입 력값은 “[CLS] explain: variable n [SEP]”이 된다. 수학식 구축 동작은 구체적으로 설명문을 재결합하는 동작, 재결합 맥락 벡터를 계산하는 동작, 및 수학식을 생 성하는 동작으로 나뉜다. 설명문을 재결합하는 동작은 설명문을 의역한다. 숫자에 대한 설명문의 경우 “설명은 숫자다”와 같은 일반 형 태 문장으로 재구조화하고, 변수에 대한 설명문의 경우 “설명은 무엇인가?”와 같은 질문 형태 문장으로 재구 조화한다. 숫자에 대한 설명문을 의역한 제1 문장 및 변수에 대한 설명문을 의역한 제2 문장을 순서대로 이어 붙여 재결합 문제를 획득한다. 재구조화하는 문장의 형태는 편의상 선택한 것이며, 다른 형태의 문장으로도 변 형이 가능하다. 데이터 세트를 보강하기 위해 모델의 훈련 과정에서 재결합 전에 각 숫자/변수의 참조 설명 중 하나를 임의로 선택할 수 있다. 설명문을 재결합하는 동작은 두 가지 관점에서 수학식 생성 모델의 디코더의 기능을 돕는다. 첫 번째 로 원 문제를 정제하여 주어진 문제를 해결하는 데 관련 없는 정보를 제외한다. 재결합 문제 텍스트는 원 문제 텍스트보다 관련 없는 정보를 적게 갖기 때문이다. 두 번째로 재결합 문제는 문제를 해결하는 데 필요한 맥락 정보를 보완한다. 원 문제는 필요한 변수에 관한 정보를 생략할 때가 있으나, 재결합 문제는 이러한 정보를 명 시적으로 지정한다. 재결합 맥락 벡터를 계산하는 동작은 선행 학습 언어 모델인 수학식 생성 모델의 인코더가 새로운 벡 터값인 재결합 맥락 벡터(recombined context vector)을 획득한다. 원 문제와 재결합 문제를 모두 사용하는 경 우, 원 문제와 재결합 문제를 순서대로 이어 붙여 입력으로 활용한다. 입력되는 문제의 종류가 하나인 경우, 그 문제를 그대로 입력으로 사용한다. 재결합 맥락 벡터를 계산하는 동작은 각 문제 또는 조합된 문제가 선행 학습 언어 모델에 입력되면 입력된 문제 를 토큰으로 분절하고, 사전을 참조하여 각 토큰을 사전의 인덱스로 변환한다. 분절 및 변환으로 생성된 인덱스 목록을 선행 학습 언어 모델에 입력한다. 선행 학습 언어 모델의 계산 과정에 따라 각 토큰마다 벡터값을 생성 하여, 재결합 맥락 벡터를 획득한다. 재결합 문제 텍스트를 생성한 후, 원 문제와 재결합 문제는 “[CLS] original problem [SEP] recombined problem [SEP]”와 같이 결합하여 인코더의 입력으로 제공된다. 인공신경망 모델은 원 문제를 사용하면 정 보 손실을 방지할 수 있으므로 두 유형의 문제를 모두 사용하도록 설계할 수 있다. 재결합 문제는 설명이 잘못 생성되면 문제를 풀기에 충분한 정보가 없을 수 있다. 생성된 설명이 틀리더라도 원 문제는 인공신경망 모델이 올바른 방향으로 학습되도록 도울 수 있다. 재결합 맥락 벡터를 계산하는 동작에서 수학식 생성 모델의 인코더는 각 입력 토큰 ri에 대해 재결합 맥락 벡터 ri를 계산한다. 인공신경망 모델은 설명문 생성 모델의 인코더와 수학식 생성 모델의 인코더의 유사성을 고려하여 설명문 생성 모델의 인코더와 수학식 생성 모델의 인코더에 동일한 인코더를 적 용할 수 있다. 설명문 생성 모델의 인코더의 입력 텍스트는 수학식 생성 모델의 인코더에 입력되는 하위 시퀀스인 원 문제이고, 설명문 생성 모델의 인코더와 수학식 생성 모델의 인코더의 출력은 동일한 형식을 가진다. 즉, 인코더(210, 310)의 출력은 주어진 입력 텍스트를 캡슐화하는 맥락 벡터이다. 설명 문 생성 모델의 인코더와 수학식 생성 모델의 인코더 간에 훈련 지식을 공유하면 훈련 과 정이 안정화될 수 있다. 수학식을 생성하는 동작은 생성된 선행 학습 언어 모델의 벡터값을 입력받아 수학식 생성 모델의 디코더 가 수학식을 단계적으로 생성한다.수학식 생성 모델의 디코더는 각 단계마다 연산자 1 개를 생성하고 그에 필요한 피연산자를 재결합 문제, 원 문제, 사전 정의된 상수값, 또는 이전 계산 단계의 결과 중 하나 이상에서 복사한다. 최종적으로 생성 된 수학식이 문제를 풀이하기 위한 방정식으로 변환된다. 변환된 방정식을 계산하면 수학식 풀이 라이브러리를 통해 답을 얻을 수 있다. 수학식 생성 모델의 디코더는 재결합 맥락 벡터를 메모리로 사용하여 수학식을 생성한다. 수학식 생 성 모델은 도 4에서 설명한 디코더의 동작 원리를 활용하여 연산자와 관련 피연산자의 그룹인 표현식 토큰 단위를 사용하여 수학식을 생성한다. 수학식 생성 모델의 디코더는 다음 j번째 토큰을 예측한다. 먼저 디코더는 지금까지 생성된 표 현식 토큰을 수신하여 임베딩 벡터 vk(k=0, ... , j-1)로 변환한다. 그런 다음, 이러한 임베딩 벡터 vk 및 재결 합 맥락 벡터 ri를 사용하여 디코더는 다음 j 번째 토큰에 대한 수학식 맥락 벡터 qj를 구축한다. 마지막으 로 디코더는 수학식 맥락 벡터 qj를 사용하여 다음 연산자와 관련 피연산자를 예측한다. 수학식을 생성하는 동작에서 설명문을 입력 데이터 소스로 사용하여 설명의 충실도를 확보한다. 도 4에서 설명 한 수학식 생성 모델의 디코더에서 설명문을 사용할 수 있도록 숫자와 변수의 입력 형식을 변경한다. 도 4에서 설명한 수학식 생성 모델의 디코더는 알려진 각 숫자에 대한 인코더의 은닉 상태 벡터 및 알려지지 않은 각 변 수에 대한 디코더의 은닉 상태 벡터에 해당하는 서로 다른 유형의 벡터를 입력한다. 본 실시예에 따른 인공신경망 모델의 수학식 생성 모델의 디코더는 설명문의 정보를 활용하여 수학식 을 생성하도록 모델을 학습한다. 모든 숫자와 변수가 재결합 문제에 나타나 있으므로, 본 실시예에 따른 인공신 경망 모델의 수학식 생성 모델의 디코더는 각 숫자/변수에 해당하는 재결합 맥락 벡터 ri를 사용한다. 본 실시예에 따른 인공신경망 모델의 학습 및 검증을 위해 문제 텍스트, 수학식, 및 각 문제에 대한 숫자/변수 에 대한 설명이 포함된 문장형 대수 문제 데이터 세트인 PEN(Problem with Explanation for Numbers)이라는 데 이터 세트를 설계한다. PEN 데이터 세트 구축은 데이터 세트 선택 단계, 오류 정정 준비 단계, 및 설명문 수집을 위한 주석화 단계를 거친다. 데이터 세트 선택 단계에서 소스 데이터 세트를 수집한다. 데이터 세트에는 영어 단어 문제가 포함되어야 하고, 데이터 세트에 있는 대부분의 문제는 문제를 해결하기 위해 대수 방정식을 사용해야 하고, 데이터 세트에는 각 문제에 대한 최적 표준 방정식(gold standard equation)이 포함되어야 한다. 오류 정정 준비 단계에서는 오류를 정정하고 데이터를 정리한다. 먼저, 오타, 문법 오류 또는 논리적 오류를 수 정한다. 다음으로, 수정된 텍스트에서 숫자 유형을 추출한다. 문장형 문제에는 방정식을 설정하는 데 사용 가능 한 숫자 데이터가 포함될 수 있다. 숫자 데이터는 아라비아 숫자, 분수, 서수, 또는 숫자의 다른 표현(예컨대, dozen) 등 다양한 형태로 작성될 수 있다. 마지막으로 해 방정식을 정규화한다. 아래 표 6과 같이 미리 정의된 공식을 기반으로 방정식을 재구성한다. [표 6]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "설명문 수집을 위한 주석화 단계에서는 문제의 각 숫자/변수에 대한 설명을 수집한다. 웹 기반 시스템을 사용하 여 각 숫자/변수에 대해 자연어 설명을 입력하고, 주어진 정보를 기반으로 문장을 완성한다. 설명이 문제와 일 치하도록 하기 위해 아래 표 7과 같이 정의된 규칙과 유효성 검사라는 두 가지 전략을 사용한다.[표 7]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "규칙 1은 숫자/변수가 나타내는 상황에 대한 설명을 텍스트에 나오는 단어를 사용하여 작성하도록 한다. 규칙 2 는 각 설명은 3~25개의 단어로 구성된 간단한 명사구이므로, 간결하게 작성하도록 한다. 규칙 3은 설명을 작성 할 때 문제 텍스트에 나타나는 단어를 하나 이상 사용하도록 한다. 규칙 4는 다른 개체에 대해 동일한 설명을 사용하지 않도록 한다. 규칙 5는 설명만으로 문제를 풀기 위한 방정식을 공식화할 수 있도록 한다. 규칙 6은 차 이 A-B를 “A 빼기 B의 값”으로 작성하도록 한다. 규칙 7은 비율 A/B를 “A 대 B의 비율”로 작성하도록 한다. 규칙 8은 A/B의 분자[분모]를 “A 대 B 비율의 분자[분모]”로 쓰도록 한다. 웹 기반 시스템은 작성자가 규칙을 준수하는지 여부를 지속적으로 확인하고, 처음 네 가지 규칙 중 하나가 위반 되면 시스템은 작성자가 다음 문제를 진행하기 전에 위반된 규칙을 따르도록 경고한다. 다른 네 가지 규칙의 경 우 시스템은 작성자가 수동으로 규칙을 확인할 수 있도록 힌트를 표시한다. 완성된 PEN 데이터 세트의 크기는 아래 표 8과 같다. [표 8]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "본 실시예에 따른 인공신경망 모델(EPT-X)에 대해서 PEN 데이터 세트를 이용하여 정답을 맞춘 문제의 비율인 정 답률을 측정한다. 생성된 수학식에서 변수의 순서를 고려하고, 생성된 수학식이 최적 표준 방정식과 일치하면 정답으로 간주한다. 도 4의 EPT는 74.52%의 정답률을 보이고, 도 7의 EPT-X는 설명을 추가로 출력하면서도 69.59%의 정답률을 보인 다. 도 8 및 도 9는 일 실시예에 따른 문제 풀이 장치에 적용된 인공신경망 모델에 대한 절제 분석을 예시한 도면이 다. 정확도와 충실도 간의 균형 관계를 확인하기 위해 인공신경망 모델의 수학식 생성 모델의 입력 데이터 및 데이 터 경로를 변경한 후 테스트를 진행한다. 도 8에 도시된 모델은 수학식 생성 모델에 설명문만을 입력하는 모델(EPTX+F)이다. EPTX+F는 수학식 생성 모델 의 디코더가 설명문 생성 모델의 디코더의 출력에만 의존하므로, 생성된 설명문에서 오류가 발생한 상황에 취약 할 수 있다. 도 9에 도시된 모델은 수학식 생성 모델에 원 문제만을 입력하는 모델(EPTX+U)이다. EPTX+U는 설명문을 활용하 지 않으므로 충실도를 확보할 수 없다. 아래 표 9는 EPT-X, EPTX+F, 및 EPTX+U를 비교한 결과이다. [표 9]"}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "설명문의 유사성을 측정하여 설명문의 타당성을 확인할 수 있다. BLEU(a Method for Automatic Evaluation of Machine Translation, Papineni et al., 2002), ROUGE(A Package for Automatic Evaluation of Summaries, Lin, 2004), CIDEr(Consensus-based Image Description Evaluation, Vedantam et al., 2015), 및 BLEURT(Learning Robust Metrics for Text Generation, Sellam et al., 2020)는 설명문의 유사성을 측정하는 방식이다. 생성된 설명문만 입력한 모델(EPTX+F)보다 원 문제와 생성된 설명문을 함께 입력한 모델(EPT-X)에서 정답률이 향상됨을 확인할 수 있다. 설명문이 원 문제의 맥락 정보를 보완하기 때문이다. 원 문제만 입력한 모델(EPTX+ U)에서 원 문제와 생성된 설명문을 함께 입력한 모델(EPT-X)보다 정답률이 조금 향상될 수 있으나 원 문제만 입 력한 모델(EPTX+U)는 충실도를 확보할 수 없는 한계가 있다. EPT-X, EPTX+F, 및 EPTX+U가 정답률이 높은 원인은 연산자와 피연산자를 그룹화한 표현식 토큰 단위를 사용하여 맥락 정보를 보존하기 때문이다. 도 10 및 도 11은 다른 실시예에 따른 문제 풀이 방법의 흐름도이다. 도 10 및 도 11에 도시된 실시예에 따른 문제 풀이 방법은 도 1에 도시된 문제 풀이 장치에서 시계열적으로 처 리되는 단계들을 포함한다. 따라서, 이하에서 생략된 내용이라고 하더라도, 도 1에 도시된 문제 풀이 장치에 관 하여 이상에서 기술한 내용은 도 10 및 도 11에 도시된 실시예에 따른 문제 풀이 방법에도 적용될 수 있다. 도 10을 참조하면, S1010 단계에서 문제 풀이 장치는 문장형 수학 문제를 입력받아 인코더-디코더 기반의 설명문 생성 모델을 통해 문장형 수학 문제에 내포된 숫자와 변수에 관한 설명문을 생성하여 출력한다. 설명문을 생성하여 출력하는 단계(S1010)에서 설명문 생성 모델의 인코더는 문장형 수학 문제를 입력받아 토큰 으로 분절하고 각 토큰마다 문장형 수학 문제의 맥락 정보를 나타내는 문제 맥락 벡터를 출력한다. 설명문을 생성하여 출력하는 단계(S1010)에서 설명문 생성 모델의 인코더 및 디코더 사이에 연결된 변수 예측기 는 문제 맥락 벡터를 입력받아 대푯값을 변형하여 문장형 수학 문제를 풀기 위해 필요한 변수의 개수를 예측한 다. 설명문을 생성하여 출력하는 단계(S1010)에서 설명문 생성 모델의 디코더는 문제 맥락 벡터, 변수의 개수를 고 려한 변수, 및 문장형 수학 문제의 숫자를 입력받아 이전에 생성된 설명 토큰을 기반으로 디코더의 은닉 상태를 계산한다. 설명문을 생성하여 출력하는 단계(S1010)에서 설명문 생성 모델의 디코더에 연결된 포인터 생성기는 디코더의 은닉 상태를 입력받아 다음 설명 토큰을 예측한다. S1020 단계에서 문제 풀이 장치는 문장형 수학 문제, 생성된 설명문, 또는 이들의 조합을 입력받아 인코더 -디코더 기반의 수학식 생성 모델을 통해 수학식을 생성하여 출력한다. 도 11을 참조하면, 수학식을 생성하여 출력하는 단계(S1020)는 생성된 설명문을 문장으로 변환하고 재결합 문제 를 생성한다. 수학식을 생성하여 출력하는 단계(S1020)는 설명문 생성 모델을 통해 문장형 수학 문제에서 숫자 가 위치하는 문맥을 선별하여 생성한 숫자에 관한 설명문을 제1 문장으로 재구조화하는 단계(S1110), 설명문 생 성 모델을 통해 변수의 인덱스를 이용하여 생성한 변수에 관한 설명문을 제2 문장으로 재구조화하는 단계 (S1120), 및 재구조화한 제1 문장 및 제2 문장을 결합하여 재결합 문제를 생성하는 단계(S1130)를 포함한다. 수학식을 생성하여 출력하는 단계(S1020)에서 재결합 문제 및 문장형 수학 문제를 수학식 생성 모델의 인코더에 입력한다. 수학식을 생성하여 출력하는 단계(S1020)에서 수학식 생성 모델의 인코더는 재결합 문제 및 문장형 수학 문제를 입력받아 재결합 맥락 벡터를 출력한다. 수학식을 생성하여 출력하는 단계(S1020)에서 수학식 생성 모델의 디코더는 재결합 맥락 벡터를 입력받아 연산 자와 필요한 피연산자를 그룹화한 표현식 토큰 단위를 사용하여 수학식을 생성한다. 생성된 수학식은 수학식 풀이 라이브러리를 통해 답을 얻을 수 있다. 이상의 실시예들에서 사용되는 '~부'라는 용어는 소프트웨어 또는 FPGA(field programmable gate array) 또는 ASIC 와 같은 하드웨어 구성요소를 의미하며, '~부'는 어떤 역할들을 수행한다. 그렇지만 '~부'는 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. '~부'는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 '~부'는 소프트웨어 구 성요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프 로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램특허 코드의 세그먼트들, 드라이버들, 펌웨어, 마 이크로코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들 및 변수들을 포함한다. 구성요소들과 '~부'들 안에서 제공되는 기능은 더 작은 수의 구성요소들 및 '~부'들로 결합되거나 추가적인 구 성요소들과 '~부'들로부터 분리될 수 있다. 뿐만 아니라, 구성요소들 및 '~부'들은 디바이스 또는 보안 멀티미디어카드 내의 하나 또는 그 이상의 CPU/GPU 들을 재생시키도록 구현될 수도 있다. 한편, 본 명세서를 통해 설명된 일 실시예에 따른 문제 풀이 방법은 컴퓨터에 의해 실행 가능한 명령어 및 데이 터를 저장하는, 컴퓨터로 판독 가능한 매체의 형태로도 구현될 수 있다. 이때, 명령어 및 데이터는 프로그램 코 드의 형태로 저장될 수 있으며, 프로세서에 의해 실행되었을 때, 소정의 프로그램 모듈을 생성하여 소정의 동작 을 수행할 수 있다. 또한, 컴퓨터로 판독 가능한 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터로 판독 가능한 매 체는 컴퓨터 기록 매체일 수 있는데, 컴퓨터 기록 매체는 컴퓨터 판독 가능 명령어, 데이터 구조, 프로그램 모 듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함할 수 있다. 예를 들어, 컴퓨터 기록 매체는 HDD 및 SSD 등과 같은 마그네틱 저장매체, CD, DVD 및 블루레이 디스크 등과 같은 광학적 기록 매체, 또는 네트워크를 통해 접근 가능한 서버에 포 함되는 메모리일 수 있다. 또한, 본 명세서를 통해 설명된 일 실시예에 따른 문제 풀이 방법은 컴퓨터에 의해 실행 가능한 명령어를 포함 하는 컴퓨터 프로그램(또는 컴퓨터 프로그램 제품)으로 구현될 수도 있다. 컴퓨터 프로그램은 프로세서에 의해 처리되는 프로그래밍 가능한 기계 명령어를 포함하고, 고레벨 프로그래밍 언어(High-level Programming Language), 객체 지향 프로그래밍 언어(Object-oriented Programming Language), 어셈블리 언어 또는 기계 언 어 등으로 구현될 수 있다. 또한 컴퓨터 프로그램은 유형의 컴퓨터 판독가능 기록매체(예를 들어, 메모리, 하드 디스크, 자기/광학 매체 또는 SSD(Solid-State Drive) 등)에 기록될 수 있다. 따라서, 본 명세서를 통해 설명된 일 실시예에 따른 문제 풀이 방법은 상술한 바와 같은 컴퓨터 프로그램이 컴 퓨팅 장치에 의해 실행됨으로써 구현될 수 있다. 컴퓨팅 장치는 프로세서와, 메모리와, 저장 장치와, 메모리 및 고속 확장포트에 접속하고 있는 고속 인터페이스와, 저속 버스와 저장 장치에 접속하고 있는 저속 인터페이스 중 적어도 일부를 포함할 수 있다. 이러한 성분들 각각은 다양한 버스를 이용하여 서로 접속되어 있으며, 공통 마더보드에 탑재되거나 다른 적절한 방식으로 장착될 수 있다. 여기서 프로세서는 컴퓨팅 장치 내에서 명령어를 처리할 수 있는데, 이런 명령어로는, 예컨대 고속 인터페이스 에 접속된 디스플레이처럼 외부 입력, 출력 장치상에 GUI(Graphic User Interface)를 제공하기 위한 그래픽 정 보를 표시하기 위해 메모리나 저장 장치에 저장된 명령어를 들 수 있다. 다른 실시예로서, 다수의 프로세서 및 (또는) 다수의 버스가 적절히 다수의 메모리 및 메모리 형태와 함께 이용될 수 있다. 또한 프로세서는 독립적인 다수의 아날로그 및(또는) 디지털 프로세서를 포함하는 칩들이 이루는 칩셋으로 구현될 수 있다. 또한, 메모리는 컴퓨팅 장치 내에서 정보를 저장한다. 일례로, 메모리는 휘발성 메모리 유닛 또는 그들의 집합 으로 구성될 수 있다. 다른 예로, 메모리는 비휘발성 메모리 유닛 또는 그들의 집합으로 구성될 수 있다. 또한 메모리는 예컨대, 자기 혹은 광 디스크와 같이 다른 형태의 컴퓨터 판독 가능한 매체일 수도 있다. 그리고, 저장장치는 컴퓨팅 장치에게 대용량의 저장공간을 제공할 수 있다. 저장 장치는 컴퓨터 판독 가능한 매 체이거나 이런 매체를 포함하는 구성일 수 있으며, 예를 들어 SAN(Storage Area Network) 내의 장치들이나 다른 구성도 포함할 수 있고, 플로피 디스크 장치, 하드 디스크 장치, 광 디스크 장치, 혹은 테이프 장치, 플래시 메 모리, 그와 유사한 다른 반도체 메모리 장치 혹은 장치 어레이일 수 있다."}
{"patent_id": "10-2022-0172570", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "상술한 실시예들은 예시를 위한 것이며, 상술한 실시예들이 속하는 기술분야의 통상의 지식을 가진 자는 상술한 실시예들이 갖는 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하 다는 것을 이해할 수 있을 것이다. 그러므로, 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적 이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 명세서를 통해 보호받고자 하는 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지 며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발 명의 범위에 포함되는 것으로 해석되어야 한다."}
{"patent_id": "10-2022-0172570", "section": "도면", "subsection": "도면설명", "item": 1, "content": "이하, 첨부되는 도면들은 본 명세서에 개시되는 바람직한 실시예를 예시하는 것이며,"}
