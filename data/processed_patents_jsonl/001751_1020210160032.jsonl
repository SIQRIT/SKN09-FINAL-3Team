{"patent_id": "10-2021-0160032", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0081277", "출원번호": "10-2021-0160032", "발명의 명칭": "인공지능 프로세서 및 이를 이용한 딥러닝 연산 처리 방법", "출원인": "한국전자통신연구원", "발명자": "한진호"}}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "각각 NVM(Non-Volatile Memory)에 저장된 데이터를 기반으로, 딥러닝 연산에 필요한 기본 단위 연산을 수행하는복수개의 NVM AI 코어들;상기 기본 단위 연산의 결과들 중 적어도 일부를 저장하는 SRAM; 및상기 기본 단위 연산의 결과들을 누적(accumulation) 연산하는 AI 코어를 포함하는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 AI 코어는상기 기본 단위 연산의 결과들을 누적하여 확장된 비트(expanded bit) 연산 결과를 생성하는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서,상기 기본 단위 연산은 4비트 MAC(Multiply and Accumulation) 연산이고, 상기 확장된 비트 연산 결과는 8비트에 상응하는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 2에 있어서,상기 NVM AI 코어들 및 상기 AI 코어 중 일부는상기 딥러닝 연산에 필요한 비트-폭(bit-width)에 따라 파워-게이팅(power-gating)되는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서,상기 딥러닝 연산에 필요한 비트-폭은 상기 딥러닝 연산에 상응하는 레이어별로 결정되는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 1에 있어서,DRAM에 저장된 AI 데이터를 기반으로 상기 기본 단위 연산에 필요한 웨이트(weight)들을 상기 NVM으로 제공하는AI DRAM 컨트롤러를 더 포함하는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서,상기 AI DRAM 컨트롤러는상기 DRAM에서 상기 기본 단위 연산에 필요한 피처(feature)들을 리드(read)해서 상기 SRAM에 저장하는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 7에 있어서,공개특허 10-2022-0081277-3-상기 AI DRAM 컨트롤러는0이 아닌 웨이트만을 상기 NVM에 저장하는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 8에 있어서,상기 AI DRAM 컨트롤러는상기 DRAM으로부터 0이 아닌 웨이트와 곱해지는 피처만을 리드(read)하는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 9에 있어서,상기 AI DRAM 컨트롤러는상기 DRAM으로부터의 데이터 리드 및 상기 DRAM으로의 데이터 라이트를 위한 제어를 수행하는 DMA(DirectMemory Access);상기 DMA로 0이 아닌 웨이트 정보를 제공하는 스파스 웨이트 유닛(sparse weight unit); 및상기 DMA의 제어에 기반하여 상기 DRAM으로 데이터를 쓰거나 상기 DRAM으로부터 데이터를 리드하는 DRAM 프로토콜 컨버터를 포함하는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "청구항 1에 있어서,상기 NVM AI 코어들은 각각 NVM 어레이; 및상기 NVM 어레이로부터 리드된 데이터에 기반한 MAC 연산을 수행하기 위한 MAC 연산기(MAC operator)를 포함하는, 인공지능 프로세서."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "복수개의 NVM(Non-Volatile Memory) AI 코어들 각각이, NVM에 저장된 데이터를 기반으로 딥러닝 연산에 필요한기본 단위 연산을 수행하는 단계;상기 기본 단위 연산의 결과들 중 적어도 일부를 SRAM에 저장하는 단계;AI 코어가, 상기 기본 단위 연산의 결과들을 누적(accumulation) 연산하는 단계; 및상기 누적 연산의 결과를 상기 SRAM에 저장하는 단계를 포함하는, 딥러닝 연산 처리 방법."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "청구항 12에 있어서,상기 AI 코어는상기 기본 단위 연산의 결과들을 누적하여 확장된 비트(expanded bit) 연산 결과를 생성하는, 딥러닝 연산 처리방법."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "청구항 13에 있어서,상기 기본 단위 연산은 4비트 MAC(Multiply and Accumulation) 연산이고, 상기 확장된 비트 연산 결과는 8비트에 상응하는, 딥러닝 연산 처리 방법."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "공개특허 10-2022-0081277-4-청구항 13에 있어서,상기 NVM AI 코어들 및 상기 AI 코어 중 일부는상기 딥러닝 연산에 필요한 비트-폭(bit-width)에 따라 파워-게이팅(power-gating)되는, 딥러닝 연산 처리방법."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "청구항 15에 있어서,상기 딥러닝 연산에 필요한 비트-폭은 상기 딥러닝 연산에 상응하는 레이어별로 결정되는, 딥러닝 연산 처리 방법."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "청구항 12에 있어서,상기 SRAM은상기 기본 단위 연산에 필요한 피처(feature)들을 저장하는, 딥러닝 연산 처리 방법."}
{"patent_id": "10-2021-0160032", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "청구항 17에 있어서,상기 NVM은 0이 아닌 웨이트만을 저장하는, 딥러닝 연산 처리 방법."}
{"patent_id": "10-2021-0160032", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일실시예에 따른 인공지능 프로세서는, 각각 NVM에 저장된 데이터를 기반으로, 딥러닝 연산에 필요한 기본 단위 연산을 수행하는 복수개의 NVM AI 코어들; 상기 기본 단위 연산의 결과들 중 적어도 일부를 저장하는 SRAM; 및 상기 기본 단위 연산의 결과들을 누적(accumulation) 연산하는 AI 코어를 포함한다."}
{"patent_id": "10-2021-0160032", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능(Artificial Intelligence) 프로세서에 관한 것으로, 특히 저전력 설계를 위해 연산효율성이 다른 AI 코어들을 기반으로 하는 인공지능 프로세서 구조에 관한 것이다."}
{"patent_id": "10-2021-0160032", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "반도체는 크게 메모리 반도체와 시스템 반도체의 두 종류로 구분할 수 있다. 최근 이러한 두 종류의 반도체로 빅데이터 및 사물인터넷(Internet of Things; IoT) 등의 다양한 어프리케이션들 다루는 데 한계를 느끼고, 이 둘을 하나로 합치는 연구/개발이 진행되어 왔다. 이 두 기능을 합한 반도체가 바로 PIM(Processing-In- Memory)이다. PIM은 저장 작업을 하는 메모리 반도체에 연산 작업을 하는 프로세서 기능을 더한 것이다. 한편, 딥러닝 가속기가 AIoT(Artificial Intelligence of Things) 등의 응용에 적용되기 위해서는 전력 소모를 줄이는 것은 매우 중요한 문제이다. 이를 위해 외부 메모리에 대한 접근을 최소화하여 전력 소모를 줄이고 연 산 성능을 극대화하려는 시도들이 있어 왔고, 딥러닝 가속기의 관점에서 보면, PIM 기술은 외부 메모리에서 읽 어 온 데이터를 이용하여 다시 외부 메모리로 데이터를 내보내기 전에 가능한 많은 재사용을 하는 기술이다. 따라서, 메모리 반도체와 인공지능 프로세서를 적절히 결합하여 PIM(Processing-In-Memory) 기술을 기반으로 한 저전력 동작이 가능한 인공지능 프로세서의 필요성이 절실하게 대두된다."}
{"patent_id": "10-2021-0160032", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 MRAM(Magnetic Random Access Memory) 등의 NVM(Non Volatile Memory)을 이용한 PIM(Processing-In-Memory) 기반의 딥러닝 가속기 코어(AI 코어)를 이용하여 저전력 인공지능 프로세서를 제공 하는 것이다. 또한, 본 발명의 목적은 복수개의 NVM AI 코어들과 AI 코어를 적절히 혼용함으로써, 인공지능 프로세서가 저전 력으로 동작하도록 하는 것이다.또한, 본 발명의 목적은 딥러닝 연산에 필수적인 요소들(AI 코어들)만 동작하도록 하여 인공지능 프로세서의 전 력 소모를 최소화하는 것이다. 또한, 본 발명의 목적은 0이 아닌 웨이트(weight)와 멀티플라이되는 피처(feature)만이 저장되거나 리드(read) 되도록 하여 인공지능 프로세서의 전력 소모를 최소화하는 것이다."}
{"patent_id": "10-2021-0160032", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명에 따른 인공지능 프로세서는, 각각 NVM(Non-Volatile Memory)에 저장된 데이터를 기반으로, 딥러닝 연산에 필요한 기본 단위 연산을 수행하는 복수개의 NVM AI 코어들; 상기 기본 단위 연산의 결과들 중 적어도 일부를 저장하는 SRAM; 및 상기 기본 단위 연산의 결과들을 누적(accumulation) 연산 하는 AI 코어를 포함한다. 이 때, AI 코어는 상기 기본 단위 연산(basic unit operation)의 결과들을 누적하여 확장된 비트(expanded bit) 연산 결과를 생성할 수 있다. 이 때, 기본 단위 연산은 4비트 MAC(Multiply and Accumulation) 연산이고, 상기 확장된 비트 연산 결과는 8비 트에 상응할 수 있다. 이 때, 상기 NVM AI 코어들 및 상기 AI 코어 중 일부는 상기 딥러닝 연산에 필요한 비트-폭(bit-width)에 따라 파워-게이팅(power-gating)될 수 있다. 이 때, 상기 딥러닝 연산에 필요한 비트-폭은 상기 딥러닝 연산에 상응하는 레이어별로 결정될 수 있다. 이 때, 인공지능 프로세서는 DRAM에 저장된 AI 데이터를 기반으로 상기 기본 단위 연산에 필요한 웨이트 (weight)들을 상기 NVM으로 제공하는 AI DRAM 컨트롤러를 더 포함할 수 있다. 이 때, 상기 AI DRAM 컨트롤러는 상기 DRAM에서 상기 기본 단위 연산에 필요한 피처(feature)들을 리드(read)해 서 상기 SRAM에 저장할 수 있다. 이 때, 상기 AI DRAM 컨트롤러는 0이 아닌 웨이트만을 상기 NVM에 저장할 수 있다. 이 때, 상기 AI DRAM 컨트롤러는 상기 DRAM으로부터 0이 아닌 웨이트와 곱해지는 피처만을 리드(read)할 수 있 다. 이 때, 상기 AI DRAM 컨트롤러는 상기 DRAM으로부터의 데이터 리드(read) 및 상기 DRAM으로의 데이터 라이트 (write)를 위한 제어를 수행하는 DMA(Direct Memory Access); 상기 DMA로 0이 아닌 웨이트 정보를 제공하는 스 파스 웨이트 유닛(sparse weight unit); 및 상기 DMA의 제어에 기반하여 상기 DRAM으로 데이터를 쓰거나 상기 DRAM으로부터 데이터를 리드하는 DRAM 프로토콜 컨버터를 포함할 수 있다. 이 때, 상기 NVM AI 코어들은 각각 NVM 어레이 및 상기 NVM 어레이로부터 리드된 데이터에 기반한 MAC 연산을 수행하기 위한 MAC 연산기(MAC operator)를 포함할 수 있다. 또한, 본 발명의 일실시예에 따른 딥러닝 연산 처리 방법은, 복수개의 NVM AI 코어들 각각이, NVM에 저장된 데 이터를 기반으로 딥러닝 연산에 필요한 기본 단위 연산을 수행하는 단계; 상기 기본 단위 연산의 결과들 중 적 어도 일부를 SRAM에 저장하는 단계; AI 코어가, 상기 기본 단위 연산의 결과들을 누적(accumulation) 연산하는 단계; 및 상기 누적 연산의 결과를 상기 SRAM에 저장하는 단계를 포함한다. 이 때, 상기 AI 코어는 상기 기본 단위 연산의 결과들을 누적하여 확장된 비트(expanded bit) 연산 결과를 생성 할 수 있다. 이 때, 상기 기본 단위 연산은 4비트 MAC(Multiply and Accumulation) 연산이고, 상기 확장된 비트 연산 결과 는 8비트에 상응할 수 있다. 이 때, 상기 NVM AI 코어들 및 상기 AI 코어 중 일부는 상기 딥러닝 연산에 필요한 비트-폭(bit-width)에 따라 파워-게이팅(power-gating)될 수 있다. 이 때, 상기 딥러닝 연산에 필요한 비트-폭은 상기 딥러닝 연산에 상응하는 레이어별로 결정될 수 있다. 이 때, 상기 SRAM은 상기 기본 단위 연산에 필요한 피처(feature)들을 저장할 수 있다. 이 때, 상기 NVM은 0이 아닌 웨이트만을 저장할 수 있다."}
{"patent_id": "10-2021-0160032", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, MRAM(Magnetic Random Access Memory)과 같은 NVM(Non Volatile Memory)을 이용한 PIM(Processing-In-Memory) 기반의 딥러닝 가속기 코어(AI 코어)를 이용하여 저전력 인공지능 프로세서를 제공 할 수 있다. 또한, 본 발명은 복수개의 NVM AI 코어들과 AI 코어를 적절히 혼용함으로써, 인공지능 프로세서가 저전력으로 동작하도록 할 수 있다. 또한, 본 발명은 딥러닝 연산에 필수적인 요소들(AI 코어들)만 동작하도록 하여 인공지능 프로세서의 전력 소모 를 최소화할 수 있다. 또한, 본 발명은 0이 아닌 웨이트(weight)와 멀티플라이되는 피처(feature)만이 저장되거나 리드(read)되도록 하여 인공지능 프로세서의 전력 소모를 최소화할 수 있다."}
{"patent_id": "10-2021-0160032", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2021-0160032", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 비록 \"제1\" 또는 \"제2\" 등이 다양한 구성요소를 서술하기 위해서 사용되나, 이러한 구성요소는 상기와 같은 용 어에 의해 제한되지 않는다. 상기와 같은 용어는 단지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사 용될 수 있다. 따라서, 이하에서 언급되는 제1 구성요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있다. 본 명세서에서 사용된 용어는 실시예를 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세 서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한다 (comprises)\" 또는 \"포함하는(comprising)\"은 언급된 구성요소 또는 단계가 하나 이상의 다른 구성요소 또는 단 계의 존재 또는 추가를 배제하지 않는다는 의미를 내포한다."}
{"patent_id": "10-2021-0160032", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어는 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 해석될 수 있다. 또한, 일반적으로 사용되는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예들을 상세히 설명하기로 하며, 도면을 참조하여 설명할 때 동일 하거나 대응하는 구성 요소는 동일한 도면 부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 도 1은 본 발명의 일실시예에 따른 인공지능 프로세서를 나타낸 블록도이다. 도 1을 참조하면, 본 발명의 일실시예에 따른 인공지능 프로세서는 프로세서 코어, AI DRAM 컨트롤러 , NVM AI 코어들, AI 코어, SRAM(static random access memory) 및 온-칩 네트워크를 포함한다. 이 때, NVM AI 코어들은 MRAM AI 코어들일 수 있다. 이하에서 설명의 편의상 NVM중 MRAM을 예로 들어 설명하지만, 본 발명의 기술사상은 NVM(Non-Volatile Memory) 중 MRAM(Magnetic Random Access Memory 또는 Magnetoresistive Random Access Memory)뿐만 아니라, F-RAM(Ferroelectric RAM), FeFET memory, ReRAM(Resistive Random Access Memory) 및 PRAM(Phase change Random Access Memory) 등일 수도 있다. 도 1에 도시된 실시예에서는 설명의 편의상 DRAM(dynamic random access memory)이 인공지능 프로세서 에 포함되는 것으로 기재하였으나, DRAM은 인공지능 프로세서의 외부에 구비될 수 있다. 도 1에 도시된 인공지능 프로세서는 저전력으로 동작하기 위해, 이기종(heterogeneous) AI 코어들(NVM AI 코어 및 AI 코어)을 기반으로 동작할 수 있다. MRAM(Magnetic Random Access Memory) AI(Artificial Intelligence) 코어들 등의 NVM(Non-Volatile Memory) AI 코어들은 각각 MRAM 등의 NVM에 저장된 데이터를 기반으로, 딥러닝 연산에 필요한 기본 단위 연산을 수 행한다. 예를 들어, 딥러닝 연산은 웨이트들(weights) 및 피처들(features)을 이용한 연산일 수 있다. 예를 들어, 기본 단위 연산은 4-비트 웨이트와 4비트 피처를 MAC(Multiply and Accumulation) 연산하는 것일 수 있 다. 이 때, SRAM는 기본 단위 연산 결과들 중 적어도 일부를 저장할 수 있다. AI 코어는 기본 단위 연산의 결과들을 누적(accumulation) 연산한다. 이 때, AI 코어의 연산을 위한 입력은 SRAM으로부터 제공될 수 있고, AI 코어의 연산 결과는 SRAM을 통해 DRAM으로 제공될 수 있다. 이 때, AI 코어는 기본 단위 연산의 결과들을 누적하여 확장된 비트(expanded bit) 연산 결과를 생성할 수 있다. 예를 들어, 기본 단위 연산은 4비트 MAC(Multiply and Accumulation) 연산이고, 상기 확장된 비트 연산 결과는 8비트에 상응할 수 있다. 프로세서 코어는 인공지능 프로세서의 동작을 제어하는 기능을 수행한다. 이 때, 프로세서 코어 는 인공지능 프로세서의 다른 구성요소들로 제어 명령을 제공하고, 그 결과를 수신할 수 있다. AI DRAM 컨트롤러는 DRAM에 저장된 AI 데이터를 기반으로 상기 기본 단위 연산에 필요한 웨이트 (weight)들을 MRAM 등의 상기 NVM으로 제공한다. 이 때, AI DRAM 컨트롤러는 AI 알고리즘 특성에 맞게 동작할 수 있다. 이 때, AI DRAM 컨트롤러는 0이 아닌 웨이트만을 MRAM 등의 NVM에 저장할 수 있다. 이 때, AI DRAM 컨트롤러는 DRAM에 저장된 피처(feature)들을 SRAM으로 읽어올 수 있다. 즉, AI DRAM 컨트롤러는 DRAM에서 상기 기본 단위 연산에 필요한 피처(feature)들을 리드(read)해서 SRAM에 저장할 수 있다. 이 때, AI DRAM 컨트롤러는 DRAM으로부터 0이 아닌 웨이트와 곱해지 는(multiplied) 피처만을 리드(read)할 수 있다. SRAM은 NVM AI 코어들 및 AI 코어의 입력 또는 출력 데이터를 저장할 수 있다. 이 때, NVM AI 코어들 및 AI 코어 중 일부는 상기 딥러닝 연산에 필요한 비트-폭(bit-width)에 따라 파워-게이팅(power-gating)될 수 있다. 이 때, 상기 딥러닝 연산에 필요한 비트-폭은 상기 딥러닝 연산에 상응 하는 레이어별로 결정될 수 있다. 도 1에 도시된 예에서, NVM AI 코어들은 NVM 기반 AI 연산(기본 단위 연산)을 처리하고, 이 연산을 위한 웨이트 등의 데이터는 NVM에 저장될 수 있다. 전술한 바와 같이, 이 때 NVM은 MRAM일 수 있다. 온-칩 네트워크는 인공지능 프로세서 내의 프로세서 코어, AI DRAM 컨트롤러, NVM AI 코 어들, AI 코어 및 SRAM들이 서로 통신하기 위한 온칩 통신 구조이다. 전력 효율성을 위한 NVM AI 코어들은 각각 최소 연산 단위(기본 단위 연산)를 처리할 수 있는 크기로 작게 설계되어, 인공지능 프로세서 내에 복수개의 NVM AI 코어들이 존재하게 된다. 도 2는 도 1에 도시된 NVM AI 코어의 일 예를 나타낸 블록도이다. 도 2를 참조하면, NVM AI 코어는 MRAM 어레이 및 MAC(Multiply and Accumulation) 연산기를 포함한 MRAM AI 코어일 수 있다. MRAM 어레이는 MRAM이 집적되어 기본 단위 연산에 필요한 데이터를 저장한다. 예를 들어, MRAM 어레이 는 딥러닝 연산에 사용되는 웨이트들을 저장할 수 있다. 딥러닝 연산에 이용되는 웨이트들은 딥러닝 모델 이 결정된 이후 바뀌지 않으므로, 웨이트들이 MRAM에 저장되면 이후 피처 데이터만을 바뀌가면서 지속적으로 딥 러닝 연산을 수행할 수 있다. MAC 연산기는 MRAM 어레이로부터 리드된 데이터에 기반한 MAC 연산을 수행한다. 이 때, MAC 연산에 필요한 다른 데이터(피처 데이터)는 도 1에 도시된 SRAM로부터 리드될 수 있다. 예를 들어, MRAM 어레이에는 2-비트 또는 4-비트 웨이트가 저장되어 있을 수 있다. 이 때, MAC 연산기 는 2-비트 또는 4-비트 MAC(Multiply and Accumulation) 연산을 수행할 수 있다. 이 때, MAC 연산기 의 하나의 2-비트 또는 4-비트 입력 데이터는 MRAM 어레이로부터 제공되고, 다른 하나의 2-비트 또는 4-비트 입력 데이터는 온-칩 네트워크를 통하여 SRAM으로부터 제공될 수 있다. 이 때, MAC 연산기 의 연산 결과는 다시 SRAM에 저장될 수 있다. 도 3은 도 1에 도시된 AI 코어의 일 예를 나타낸 블록도이다. 도 3을 참조하면, 도 1에 도시된 AI 코어는 ACC 연산기를 포함한다. ACC 연산기는 SRAM에 저장된 NVM AI 코어의 기본 단위 연산 결과를 입력으로 받아서, 이를 누적 연산 (accumulation operation)한 결과를 다시 SRAM으로 출력한다. 예를 들어, 기본 단위 연산 결과는 4-비트 데이터일 수 있고, AI 코어는 4-비트 데이터에 대한 누적 연산을 이용하여 8-비트의 확장된 비트 연산 결 과를 생성하여 SRAM으로 출력할 수 있다. 이 때, 기본 단위 연산 결과인 4-비트 데이터는 4-비트 웨이트 데이 터 및 4-비트 피처 데이터의 MAC 연산 결과에 상응하는 것일 수 있고, 확장된 비트 연산 결과인 8-비트 데이터 는 8-비트 웨이트 데이터 및 8-비트 피처 데이터의 MAC 연산 결과에 상응하는 것일 수 있다. 도 4는 도 1에 도시된 AI DRAM 컨트롤러의 일 예를 나타낸 블록도이다. 도 4를 참조하면, AI DRAM 컨트롤러는 DMA(Direct Memory Access), 스파스 웨이트 유닛(sparse weight unit) 및 DRAM 프로토콜 컨버터를 포함한다. DMA는 DRAM으로부터의 데이터 리드 및 DRAM으로의 데이터 라이트를 위한 제어를 수행한다. 스파스 웨이트 유닛은 DMA로 0이 아닌 웨이트 정보를 제공한다. DRAM 프로토콜 컨버터는 DMA의 제어에 기반하여 DRAM으로 데이터를 쓰거나 DRAM으로부터 데이터를 리드(read)한다. 즉, AI DRAM 컨트롤러는 외부 메모리인 DRAM에서 AI 알고리즘 처리를 위해 필요한 웨이트 데이터 및 피처 데이터를 읽어와서 NVM AI 코어들의 MRAM 등의 NVM이나, SRAM에 저장하고, SRAM에 저장된 연산 결과인 출력 피처(output feature)를 DRAM에 저장한다. 또한, AI DRAM 컨트롤러는 그 다음 연산에 필요 한 피처 데이터를 DRAM에서 읽어와서 SRAM에 저장할 수 있다. 이 때, 웨이트는 런-랭스 코딩된 데이터에 기반한 것일 수 있고, 런-랭스 코딩은 0이 아닌 웨이트만 저장하고, 0이거나 0에 가까운 웨이트는 그 개수(run-length)만 저장하는 것일 수 있다. 이 때, 0이 아닌 웨이트는 AI DRAM 컨트롤러에 의하여 MRAM 등의 NVM에 저장될 수 있다. 이 때, 스파스 웨이트 유닛은 0이 아닌 웨이트 정보 또는 반대로 0인 웨이트 정보를 DMA로 제공할 수 있고, DMA는 0이 아닌 웨이트에 곱해지 는 피처들만을 읽어올 수 있다. 도 5는 NVM AI 코어 및 AI 코어 동작의 일 예를 나타낸 블록도이다. 도 5에 도시된 예에서, NVM AI 코어들은 기본 단위 연산을 수행하여 그 결과를 SRAM에 저장하지만, AI 코 어는 비활성화된다. 즉, 도 5에 도시된 예에서, NVM AI 코어들은 각각 4-비트 MAC 연산을 수행하여 그 결과인 4-비트 데이터를 SRAM에 저장한다. 이 때, NVM AI 코어들은 MRAM AI 코어들일 수 있다. 이 때, AI 코어는 파워-게 이팅(power-gating)될 수 있다. 즉, 4비트의 비트-폭(bit-width)만이 필요한 경우에, AI 코어는 전력 절 감을 위해 파워-게이팅 되어 비활성화될 수 있다. 도 5에 도시된 예에서, FIN[3:0]은 4-비트 피처 데이터를 나타내고 이는 SRAM에서 읽어온 것일 수 있다. 이 때, W0[3:0], W1[3:0], W2[3:0] 및 W3[3:0]는 각각 MRAM 등의 SVM에서 읽어온 4-비트 웨이트 데이터일 수 있다.도 6은 NVM AI 코어 및 AI 코어 동작의 다른 예를 나타낸 블록도이다. 도 6에 도시된 예에서, NVM AI 코어들은 기본 단위 연산을 수행하여 그 결과를 SRAM에 저장하고, AI 코어 는 SRAM에서 기본 단위 연산 결과들을 리드하여 이들을 누적 연산함으로써 확장된 비트 연산 결과를 생성 한다. 이 때, NVM AI 코어들은 MRAM AI 코어들일 수 있다. 즉, 도 6에 도시된 예에서, NVM AI 코어들은 각각 4-비트 MAC 연산을 수행하여 그 결과인 4-비트 데이터를 SRAM에 저장한다. 이 때, AI 코어는 4-비트 MAC 연산 결과들을 조합(combination)하여 8-비트 MAC 연산 을 수행하고, 그 결과인 8-비트 데이터를 SRAM에 저장한다. 도 6에 도시된 예에서, AI 코어는 4개의 4-비트 MAC 연산 결과들(FIN[7:4] x W[7:4], FIN[3:0] x W[7:4], FIN[7:4] x W[3:0], FIN[3:0] x W[3:0])을 이용하여 8-비트 MAC 연산 결과(FIN[7:0] x W[7:0])를 생성하고, 이를 SRAM에 저장한다. 이 때, 4-비트 피처 데이터들(FIN[7:4], FIN[3:0])은 SRAM으로부터 리드될 수 있고, 4-비트 웨이트 데이터들(W[7:4], W[3:0])은 MRAM 등의 NVM에서 리드될 수 있다. 도 6에 도시된 '<<'는 쉬프트 연산을 나타내고, '+'는 누적(accumulation) 연산을 나타낼 수 있다. 도 5 및 도 6에 도시된 예에서, 연산에 사용되지 않는 NVM AI 코어는 연산에 사용되지 않는 AI 코어와 마찬가지 로 파워-게이팅되어 비활성화될 수 있다. 결국, 도 5는 4-비트 연산을 위한 실시예를 나타내고, 도 6은 8-비스 연산을 위한 실시예를 나타낸다. 인공지능 프로세서를 통해 수행되는 AI 알고리즘은 복수개의 레이어들에 상응할 수 있고, 각 레이어가 요구하는 데이터의 비트-폭(bit-width)은 다르게 설정될 수 있다. 따라서, 레이어별로 연산에 필요한 NVM AI 코어와 AI 코어가 결정되고, 불필요한 NVM AI 코어 또는 AI 코어는 파워-게이팅되어 비활성화될 수 있다. 도 7은 딥러닝 연산에 상응하는 레이어 수 변화에 따른 데이터 비트-폭의 변화를 나타낸 그래프이다. 도 7에 도시된 \"1% Loss\", \"5% Loss\" 및 \"10% Loss\"는 각각 정확도(accuracy)를 나타낸다. 도 7을 참조하면, 레이어 #8의 경우 필요한 데이터 비트-폭이 4비트 이내가 되고, 이 경우 도 5에 도시된 바와 같이 AI 코어가 파워-게이팅 되어 비활성화될 수 있다. 본 발명의 일실시예에 따른 이기종 AI 코어들 기반 인공지능 프로세서는 데이터-폭(data-width, bit-width 또는 bit precision)에 따라 NVM AI 코어와 AI 코어를 제어하여 필요한 최소한의 코어들만을 동작시켜서 전력 소모를 최소화한다. 또한, 본 발명의 일실시예에 따른 인공지능 프로세서는 레이어별로 최소한의 전력을 소모할 수 있는 데이터-폭 (data-width)으로 웨이트(weight)와 피처(feature)가 구성되도록 하여, 레이어 별로 최소한의 필요한 NVM AI 코어들 및 AI 코어만 동작하도록 할 수 있다. 또한, 본 발명의 일실시예에 따른 인공지능 프로세서는 0이 아닌 값에 상응하는 웨이트들만 MRAM 등의 NVM에 저 장하고, 0이 아닌 웨이트들과 곱해지는 피처들만을 리드함으로써 효율적으로 동작할 수 있다. 결국, 본 발명의 일실시예에 따른 인공지능 프로세서는 AIoT에 적용가능한 매우 낮은 전력에서 동작하는 4-비트 ~ 8-비트 데이터 프리시전(data precision)을 지원할 수 있다. 도 8은 본 발명의 일실시예에 따른 딥러닝 연산 처리 방법을 나타낸 동작 흐름도이다. 도 8을 참조하면, 본 발명의 일실시예에 따른 딥러닝 연산 처리 방법은, 복수개의 NVM AI 코어들 각각에 의하여, NVM에 저장된 데이터를 기반으로 딥러닝 연산에 필요한 기본 단위 연산을 수행한다(S810). 이 때, NVM은 MRAM일 수 있다. 이 때, 기본 단위 연산은 4비트 MAC(Multiply and Accumulation) 연산일 수 있다. 또한, 본 발명의 일실시예에 따른 딥러닝 연산 처리 방법은 상기 기본 단위 연산의 결과들 중 적어도 일부를 SRAM에 저장한다(S820). 또한, 본 발명의 일실시예에 따른 딥러닝 연산 처리 방법은, AI 코어에 의하여, 상기 기본 단위 연산의 결과들 을 누적(accumulation) 연산한다(S830).이 때, 상기 AI 코어는 상기 기본 단위 연산의 결과들을 누적하여 확장된 비트(expanded bit) 연산 결과를 생성 할 수 있다. 이 때, 확장된 비트 연산 결과는 8비트에 상응할 수 있다. 또한, 본 발명의 일실시예에 따른 딥러닝 연산 처리 방법은 상기 누적 연산의 결과를 상기 SRAM에 저장한다 (S840). 이 때, NVM AI 코어들 및 AI 코어들 중 일부는 상기 딥러닝 연산에 필요한 비트-폭(bit-width)에 따라 파워-게 이팅(power-gating)될 수 있다. 이 때, 상기 딥러닝 연산에 필요한 비트-폭은 상기 딥러닝 연산에 상응하는 레이어별로 결정될 수 있다. 이 때, 상기 SRAM은 상기 기본 단위 연산에 필요한 피처(feature)들을 저장할 수 있다. 이 때, 상기 NVM은 0이 아닌 웨이트만을 저장할 수 있다. 이상에서와 같이 본 발명에 따른 인공지능 프로세서 및 딥러닝 연산 처리 방법은 상기한 바와 같이 설명된 실시 예들의 구성과 방법이 한정되게 적용될 수 있는 것이 아니라, 상기 실시예들은 다양한 변형이 이루어질 수 있도 록 각 실시예들의 전부 또는 일부가 선택적으로 조합되어 구성될 수도 있다."}
{"patent_id": "10-2021-0160032", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른 인공지능 프로세서를 나타낸 블록도이다. 도 2는 도 1에 도시된 NVM AI 코어의 일 예를 나타낸 블록도이다. 도 3은 도 1에 도시된 AI 코어의 일 예를 나타낸 블록도이다. 도 4는 도 1에 도시된 AI DRAM 컨트롤러의 일 예를 나타낸 블록도이다. 도 5는 NVM AI 코어 및 AI 코어 동작의 일 예를 나타낸 블록도이다. 도 6은 NVM AI 코어 및 AI 코어 동작의 다른 예를 나타낸 블록도이다. 도 7은 딥러닝 연산에 상응하는 레이어 수 변화에 따른 데이터 비트-폭의 변화를 나타낸 그래프이다. 도 8은 본 발명의 일실시예에 따른 딥러닝 연산 처리 방법을 나타낸 동작 흐름도이다."}
