{"patent_id": "10-2022-7036291", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0159407", "출원번호": "10-2022-7036291", "발명의 명칭": "가중 실수-값 논리의 기능 및 러닝을 최적화 하는 방법, 프로그램 제품 및 시스템", "출원인": "인터내셔널 비지네스 머신즈 코포레이션", "발명자": "루스, 프랑수아"}}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "논리적 뉴런들(logical neurons)의 신경망(neural network)에 구현된 가중 실수값 논리(weighted real-valuedlogic)의 러닝(learning) 방법으로서,가중 실수값 논리 게이트를 구현하는 신경망의 뉴런에 대한 최대 입력 가중치와 최소 입력 가중치 사이의 비율(ratio)을 나타내는 최대 표현도(maximum expressivity)를 수신하는 단계;상기 뉴런과 연관된 연산자 에러티(operator arity)를 수신하는 단계;활성화를 위해 상기 뉴런, 참의 스레쉬홀드(threshold-of-truth), 및 뉴런 스레쉬홀드(neuron threshold)에 대한 입력들과 연관된 가중치들로서(in terms of weights associated with inputs to the neuron)로 상기 가중실수값 논리 게이트와 연관된 논리적 제약조건들(logical constraints)을 정의하는 단계;상기 논리적 제약조건들에 기초하여 공식화(formulate)된 활성화 최적화(activation optimization) - 상기 활성화 최적화는, 상기 연산자 애러티와 상기 최대 표현도가 주어질 때 상기 뉴런에 대한 입력 가중치들의 분포폭(distribution width)을 나타내는 표현도와 상기 뉴런에 대한 그래디언트 품질(gradient quality)의 곱(product)을 최대화함 - 를 해결(solve)하는 것에 기초하여, 상기 뉴런의 활성화 함수(activation function)에사용되는 파라미터로서 상기 참의 스레쉬홀드를 결정하는 단계;상기 뉴런에서 상기 활성화 함수 - 상기 활성화 함수는 상기 결정된 참의 스레쉬홀드를 사용함 - 를 사용하여상기 논리적 뉴런들의 신경망을 훈련시키는 단계((training); 를 포함하는방법."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서, 상기 활성화 함수는 시그모이드 함수(sigmoid function)를 포함하는, 방법."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서, 상기 활성화 함수는 3-피쓰 누설(3-piece leaky) 정류 선형 유닛(rectified linearunit)(ReLU)을 포함하는, 방법."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 1에 있어서, 상기 방법은, 상기 정의된 논리적 제약조건들에 기초하여,상기 뉴런 스레쉬홀드에 관한 하한(lower bound)을 정의하는 단계;상기 뉴런 스레쉬홀드에 관한 상한(upper bound)을 정의하는 단계;상기 뉴런의 입력에 관한 하한을 정의하는 단계 - 상기 뉴런의 입력에 관한 하한은 상기 뉴런의 연산 범위(operating range)의 최소점(minimum point)을 나타냄 - ;상기 표현도, 연산자 애러티 및 상기 참의 스레쉬홀드의 텀들로 최소 입력 가중치에 관한 하한을 정의하는단계;상기 뉴런의 상기 활성화 함수의 연산 범위의 폭을 나타내는 논리 대역폭(logical bandwidth)을 정의하는 단계;상기 표현도에 관한 상한을 정의하는 단계를 더 포함하며,상기 활성화 최적화는, 상기 뉴런 스레쉬홀드에 관한 하한, 상기 뉴런 스레쉬홀드에 관한 상한, 상기 뉴런의 입공개특허 10-2022-0159407-3-력에 관한 하한, 최소 입력 가중치에 관한 하한, 상기 논리 대역폭 및 상기 표현도에 관한 상한에 기초하여 공식화되는, 방법."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서, 상기 활성화 최적화는, 상기 표현도, 상기 참의 스레쉬홀드, 상기 뉴런의 입력에 관한 하한및 상기 뉴런 활성화에 예상되는 상기 최대 입력의 상한으로부터 유도되는, 방법."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 1에 있어서, 상기 방법은, 부분 선형 함수(piecewise linear function) - 상기 부분 선형 함수는, 최소연산 범위(operating range minimum)에서, 거짓의 스레쉬홀드(threshold-of-falsity) 입력에 대해서는 거짓의스레쉬홀드에서, 참의 스레쉬홀드 입력에 대해서는 참의 스레쉬홀드에서, 거짓이고, 최대 연산 범위(operatingrange maximum)에서 참인 함수임 - 를 형성함으로써 논리 연산 범위에 걸쳐 상기 활성화 함수의 그래디언트들을최적화하기 위해 상기 참의 스레쉬홀드 값에 대하여 상기 활성화 함수를 계속적으로 업데이트하는 단계를 더 포함하는,방법."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 1에 있어서, 상기 방법은,더 높은 표현도를 요구함이 없이 상기 뉴런의 입력이 제거될 수 있도록 하기 위해 상기 뉴런의 입력과 연관된슬랙 변수(slack variable)를 도입하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 1에 있어서, 상기 뉴런에 대한 상기 입력 가중치들의 합(sum)은 단지 최소 가중치와 상기 표현도가 주어질 때에만 결정되는, 방법."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 1에 있어서, 상기 참의 스레쉬홀드는 현재의 표현도에 기초하여 업데이트되는, 방법."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 1에 있어서, 상기 방법은, 상기 가중 실수-값 논리 게이트의 특성들을 시각화(visualizing)하는 단계를더 포함하는, 방법."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "컴퓨터 판독가능 스토리지 매체 - 상기 컴퓨터 판독가능 스토리지 매체는 그것으로 구현된 프로그램 명령들을가짐 - 를 포함하는 컴퓨터 프로그램 제품으로서, 상기 프로그램 명령들은 디바이스에 의해 실행가능하며, 상기디바이스로 하여금,가중 실수 값 논리 게이트(weighted real-valued logic gate)를 구현하는 신경망(neural network)의 뉴런(neuron)에 대한 최대 입력 가중치와 최소 입력 가중치 사이의 비율(ratio)을 나타내는 최대 표현도(maximumexpressivity)를 수신하고;공개특허 10-2022-0159407-4-상기 뉴런과 연관된 연산자 에러티(operator arity)를 수신하고;활성화를 위해 상기 뉴런, 참의 스레쉬홀드(threshold-of-truth), 및 뉴런 스레쉬홀드(neuron threshold)에 대한 입력들과 연관된 가중치들로서(in terms of weights associated with inputs to the neuron) 상기 가중 실수-값 논리 게이트와 연관된 논리적 제약조건들(logical constraints)을 정의하고;상기 논리적 제약조건들에 기초하여 공식화된(formulated) 활성화 최적화(activation optimization) - 상기 활성화 최적화는 상기 연산자 애러티와 상기 최대 표현도가 주어질 때 상기 뉴런에 대한 입력 가중치들의 분포 폭(distribution width)을 나타내는 표현도와 상기 뉴런에 대한 그래디언트 품질(gradient quality)의 곱(product)을 최대화함 - 를 해결(solve)하는 것에 기초하여, 상기 뉴런의 활성화 함수(activation function)에사용되는 파라미터로서 상기 참의 스레쉬홀드를 결정하고;상기 뉴런에서 상기 활성화 함수 - 상기 활성화 함수는 상기 결정된 참의 스레쉬홀드를 사용함 - 를 사용하여상기 논리적 뉴런들의 신경망을 훈련(training)시키도록 하는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "청구항 11에 있어서, 상기 활성화 함수는 시그모이드 함수(sigmoid function)를 포함하는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "청구항 11에 있어서, 상기 활성화 함수는 3-피쓰 누설(3-piece leaky) 정류 선형 유닛(rectified linearunit)(ReLU)을 포함하는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "청구항 11에 있어서, 상기 디바이스는, 상기 정의된 논리적 제약조건들에 기초하여,상기 뉴런 스레쉬홀드에 관한 하한(lower bound)을 정의하고;상기 뉴런 스레쉬홀드에 관한 상한(upper bound)을 정의하고;상기 뉴런의 입력에 관한 하한을 정의하고 - 상기 뉴런의 입력에 관한 하한은 상기 뉴런의 연산 범위(operatingrange)의 최소점(minimum point)을 나타냄 - ;상기 표현도, 연산자 애러티 및 상기 참의 스레쉬홀드들로서 최소 입력 가중치에 관한 하한을 정의하고;상기 뉴런의 상기 활성화 함수의 연산 범위의 폭을 나타내는 논리 대역폭(logical bandwidth)을 정의하고;상기 표현도에 관한 상한을 정의하고,상기 활성화 최적화는, 상기 뉴런 스레쉬홀드에 관한 하한, 상기 뉴런 스레쉬홀드에 관한 상한, 상기 뉴런의 입력에 관한 하한, 최소 입력 가중치에 관한 하한, 상기 논리 대역폭 및 상기 표현도에 관한 상한에 기초하여 공식화되는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "청구항 14에 있어서, 상기 활성화 최적화는, 상기 표현도, 상기 참의 스레쉬홀드, 상기 뉴런의 입력에 관한 하한 및 상기 뉴련 활성화에 예상되는 상기 최대 입력의 상한으로부터 유도되는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "청구항 11에 있어서, 상기 디바이스는, 부분 선형 함수(piecewise linear function) - 상기 부분 선형 함수는,최소 연산 범위(operating range minimum)에서, 거짓의 스레쉬홀드(threshold-of-falsity) 입력에 대해서는 거공개특허 10-2022-0159407-5-짓의 스레쉬홀드에서, 참의 스레쉬홀드 입력에 대해서는 참의 스레쉬홀드에서, 거짓이고, 상기 최대 연산 범위에서 참인 함수임 - 를 형성함으로써 논리 연산 범위에 걸쳐 상기 활성화 함수의 그래디언트들(gradients)을 최적화하기 위해 상기 참의 스레쉬홀드 값에 대하여 상기 활성화 함수를 계속적으로 업데이트하도록 더 동작하는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "청구항 11에 있어서, 상기 디바이스는,더 높은 표현도를 요구함이 없이 상기 뉴런의 입력이 제거될 수 있도록 하기 위해 상기 뉴런의 입력과 연관된슬랙 변수(slack variable)를 도입하도록 더 동작하는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "청구항 11에 있어서, 상기 뉴런에 대한 상기 입력 가중치들의 합(sum)은 단지 최소 가중치와 상기 표현도가 주어질 때에만 결정되는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "청구항 11에 있어서, 상기 참의 스레쉬홀드는 현재의 표현도에 기초하여 업데이트되는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2022-7036291", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "시스템으로서, 상기 시스템은,하드웨어 프로세서; 및상기 하드웨어 프로세서에 결합되는 메모리 디바이스를 포함하고,상기 하드웨어 프로세서는 적어도,가중 실수 값 로직 게이트(weighted real-valued logic gate)를 구현하는 신경망(neural network)의 뉴런(neuron)에 대한 최대 입력 가중치(weight)와 최소 입력 가중치 사이의 비율(ratio)을 나타내는 최대 표현도(maximum expressivity)를 수신하고;상기 뉴런과 연관된 연산자 에러티(operator arity)를 수신하고;활성화를 위해 상기 뉴런, 참의 스레쉬홀드(threshold-of-truth), 및 뉴런 스레쉬홀드(neuron threshold)에 대한 입력들과 연관된 가중치들로서(in terms of weights associated with inputs to the neuron)로 상기 가중실수-값 로직 게이트와 연관된 논리적 제약조건들(logical constraints)을 정의하고;상기 논리적 제약조건들에 기초하여 공식화된(formulated) 활성화 최적화(activation optimization)를 해결(solve)하는 것에 기초하여, 상기 뉴런의 활성화 함수(activation function)에 사용되는 파라미터로서 상기 참의 스레쉬홀드를 결정하고 - 상기 활성화 최적화는 상기 뉴런에 대한 입력 가중치들의 분포 폭(distributionwidth)을 나타내는 표현도와 주어진 상기 연산자 애러티와 상기 최대 표현도에 의한 상기 뉴런에 대한 그래디언트 품질(gradient quality)의 곱(product)을 최대화함 - ;상기 뉴런에서 상기 활성화 함수 - 상기 활성화 함수는 상기 결정된 참의 스레쉬홀드를 사용함 - 를 사용하여상기 논리적 뉴런들의 신경망을 훈련(training)시키도록 구성되는, 시스템."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "가중 실수-값 논리 게이트(weighted real-valued logic gate)를 구현하는 신경망(neural network)의 뉴런 (neuron)에 대한 최대 입력 가중치(weight)와 최소 입력 가중치 사이의 비(ratio)를 나타내는 최대 표현도 (maximum expressivity)가 수신될 수 있다. 상기 뉴런과 연관된 연산자 애러티(operator arity)가 수신될 수 있 (뒷면에 계속)"}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 일반적으로 컴퓨터들 및 컴퓨터 애플리케이션들과 관련되고, 더 구체적으로는 신경망들 (neural networks) 및 신경-상징망들(neuro-symbolic networks)과 관련된다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 방법론들은, 공식 논리(formal logic)의 여러 가지 시스템들, 및 데이터 중심의 그래디언트 (gradient) 최적화 신경망들 중 어떤 것을 통한, 원칙에 입각한 연역적 추론을 포함한다. 두 개의 패러다임들 모두 많은 장점들과 단점들을 갖고 있다. 예를 들어, 공식 논리는, 비록 계산 집약적(computationally intensive)이고, 광범위한 영역의 전문가 입력을 필료로 하고, 불일치에 의해 탈선될 수도 있으나, 해석가능하 고(interpretable), 검증가능하며(varifiable), 광범위하게 일반화가능(generalizable)하다. 한편, 신경망들은, 심지어 미처리되고 및/또는 노이즈가 많은 데이터에 대해서도 잘 수행되고, 인적 구성을 거의 필요 로 하지 않으며, 병렬로 효율적으로 실행될 수 있다. 하지만, 신경망들은 대규모의 훈련(training) 데이터 세트 들을 필요로 할 수 있으며, 적대적 공격들에 취약할 수 있으며, 해석할 수 없는 블랙-박스 특징을 가질 수 있다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 측면에서, 논리적 뉴런들(logical neurons)의 신경망에 구현된 가중 실수-값 논리 (weighted real-valued logic)의 러닝(learning) 방법은, 가중 실수-값 논리 게이트를 구현하는 신경망의 뉴런 에 대한 최대 입력 가중치(weight)와 최소 입력 가중치 사이의 비(ratio)를 나타내는 최대 표현도 (expressivity)를 수신하는 단계를 포함한다. 상기 방법은 또한 상기 뉴런과 연관된 연산자 에러티(operator arity)를 수신하는 단계를 포함할 수 있다. 상기 방법은 활성화를 위해 상기 뉴런, 참의 스레쉬홀드(threshold- of-truth), 및 뉴런 스레쉬홀드(neuron threshold)에 대한 입력들과 연관된 가중치들로서 상기 가중 실수-값 논 리 게이트와 연관된 논리적 제약조건들(logical constraints)을 정의하는 단계를 더 포함할 수 있다. 상기 방법 은 또한 상기 논리적 제약조건들에 기초하여 공식화된(formulated) 활성화 최적화(activation optimization)를 해결(solve)하는 것에 기초하여, 상기 뉴런의 활성화 함수(activation function)에 사용되는 파라미터로서 상기 참의 스레쉬홀드를 결정하는 단계를 포함할 수 있다. 상기 활성화 최적화는 상기 연산자 애러티와 상기 최대 표 현도가 주어질 때 상기 뉴런에 대한 입력 가중치들의 분포 폭(distribution width)을 나타내는 표현도와 상기 뉴런에 대한 그래디언트 품질(gradient quality)의 곱(product)을 최대화한다. 상기 방법은 또한 상기 뉴런에서 상기 활성화 함수- 상기 활성화 함수는 상기 결정된 참의 스레쉬홀드를 사용함 - 를 사용하여 상기 논리적 뉴런 들의 신경망을 훈련(training)시키는 단계를 포함할 수 있다. 본 발명의 일 측면에서, 시스템은 하드웨어 프로세서를 포함할 수 있다. 메모리 디바이스는 하드웨어 프 로세서와 결합될 수 있다. 하드웨어 프로세서는 가중 실수-값 논리 게이트를 구현하는 신경망의 뉴런에 대한 최 대 가중치와 최소 가중치 사이의 비를 나타내는 최대 표현도를 수신하도록 구성될 수 있다. 하드웨어 프로세서 는 또한 상기 뉴런과 연관된 연산자 에러티(operator arity)를 수신하도록 구성될 수 있다. 상기 하드웨어 프로 세서는 또한 활성화를 위해 상기 뉴런, 참의 스레쉬홀드, 및 뉴런 스레쉬홀드에 대한 입력들과 연관된 가중치들 로서 상기 가중 실수-값 논리 게이트와 연관된 논리적 제약조건들(logical constraints)을 정의도록 구성될 수 있다. 상기 하드웨어 프로세서는 또한 상기 활성화 최적화를 해결하는 것에 기초하여, 상기 뉴런의 활성화 함수 에 사용되는 파라미터로서 상기 참의 스레쉬홀드를 결정하도록 구성될 수 있다. 상기 활성화 최적화는 상기 논 리적 제약조건들에 기초하여 공식화될 수 있다. 상기 활성화 최적화는 상기 연산자 에러티와 상기 최대 표현도 가 주어질 때 상기 뉴런에 대한 입력 가중치들의 분포 폭을 나타내는 표현도와 상기 뉴런에 대한 그래디언트 품 질의 곱을 최대화한다. 상기 하드웨어 프로세서는 또한 상기 뉴런에서 상기 활성화 함수 - 상기 활성화 함수는 상기 결정된 참의 스레쉬홀드를 사용함 - 를 사용하여 상기 논리적 뉴런들의 신경망을 훈련시키도록 구성될 수 있다. 여기에 설명되는 하나 또는 그 이상의 방법들을 수행하기 위해 머신에 의해 실행가능한 명령들의 프로그 램을 저장하는 컴퓨터 판독가능 스토리지 매체가 또한 제공될 수 있다. 여러 가지 실시예들의 구조 및 동작 뿐만 아니라 추가 특징들은 첨부되는 도면들을 참조하여 이하에서 상세히 설명된다. 도면들에서, 유사한 참조부호들은 동일한 구성요소 또는 기능이 유사한 구성요소를 나타낸다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "머신 러닝 모델들, 예컨대, 신경망들을 최적화하기 위한 여러 가지 파라미터들을 결정할 수 있는 시스템 들, 방법들 및 기술들이 제시된다. 예를 들어, 방법은, 주어진 최대 표현도(maximumexpressivity)(예컨대, 최대 입력 가중치와 최소 입력 가중치 사이의 비(ratio)로 정의됨)와 연산자 크기 또는 애러티(arity)(피연산자들 (operands)의 수)에 대한 활성화 함수의 함수 그래디언트들(functional gradients)을 최적화하기 위해, 논리적 뉴런들과 같은 가중 실수-값 논리 연산자들(예컨대, 가중된 오프셋 루카시에비치(Lukasiewicz) 실수-값 강 (strong) 논리곱(conjunction) 및/또는 논리합(disjunction) 연산자들)의 참의 스레쉬홀드(threshold-of- truth)(알파), 활성화 함수(activation function) 및 논리적 제약조건들(logical constraints)(예컨대, 컨벡스 최적화 선형 프로그램(convex optimization linear program))을 결정할 수 있다. 일 측면에서, 시스템들, 방법 들 및 기술들의 실시예들에 따라 결정된 머신 러닝을 위한 여러 가지 파라미터들은 머신 러닝에서의 사라지는 (vanishing) 그래디언트 문제에 대처할 수 있다. 예를 들어 너무 작은 그래디언트들을 갖는 경우 머신 훈련을 어렵게 만든다. 인공 신경망(artificial neural network, ANN) 또는 신경망(neural network, NN)은 머신 러닝 모델이며, 이는 입력 데이터를 예측하거나 분류하도록 훈련받을 수 있다. 인공 신경망은 일련의 뉴런들의 층들 을 포함할 수 있는데, 이들은 하나의 층에 있는 뉴런들의 출력 신호들이 가중되고 그 다음 층에 있는 뉴런들로 전송되도록 상호연결(interconnect)된다. 주어진 층에서의 뉴런(Ni)은 그 다음 층에서의 하나 또는 그 이상의 뉴런들(Nj)에 연결될 수 있고, Ni에서 Nj까지 전송된 신호들을 가중시키기 위해 상이한 가중치들(Wij)이 각각의 뉴런-뉴런 연결(Ni-Nj)과 연관될 수 있다. 뉴런(Nij)은 자신의 누적된 입력들에 의존하여 출력 신호들을 생성하 고, 가중된 신호들은 입력에서 출력 뉴런 층까지 망(network)의 연속적인 층들을 통해 전파될 수 있다. 인공 신 경망 머신 러닝 모델은, 각각의 뉴런 층들과 연관된 가중치들의 세트들이 결정되는 훈련 단계(training phase) 를 거칠 수 있다. 가중치들이 훈련 데이터로부터 망 \"런들(learns)\"로서 반복적으로 업데이트되는 반복적인 훈 련 스킴(scheme)에 있어서, 망은 훈련 데이터(training data)의 세트에 노출된다. 그 결과의 훈련된 모델은, 그 훈련 연산을 통해 정의된 가중치들을 가지며, 새로운 데이터에 기초한 태스크를 수행하도록 적용될 수 있다. 신경망의 일 예는 반복 신경망(recurrent neural network)인데, 이는 언어에서 문장들과 같은 시퀀스 기 반 데이터 또는 시계열 데이터(time series data)를 처리할 수 있다. 반복 신경망 모델은 일련의 신경망 세포들 을 가질 수 있는데, 이들은 그 일련의 신경망 세포들에서의 데이터 뿐만 아니라 이전의 시간 단계에서 이전의 신경망으로부터의 활성화 정보를 입력으로 받아들인다. 퍼지 논리(fuzzy logic)는, 고전적인 논리의 부울 연산자들(Boolean operators)이 범위 [0;1]에 있는 실수들에 관해 연산하는 함수들로 대체된 논리 시스템이다. 자신들의 서브공식들(subformulae)에 기초한 여러가지 공식들(formulae)의 진리 값들을 평가하는 수단을 넘어, 퍼지 논리는 미적분학(calculus)을 제공하는 것을 목표로 하고 있으며, 그것에 의해 명제들(propositions)(또는 술어들(predicates))의 진리 값들은 공리들 (axioms)의 지식 기반(knowledge base)(이론)과 함께 서로에게서 추론될 수 있으며, 그들 자신들에게는 퍼지 진 리 값들이 할당될 수 있다. 가중된 퍼지 논리에 있어서, 논리 공식들 및/또는 그들의 컴포넌트는 중요도(importance), 영향력, 또는 어떤 관련된 개념을 나타내는 실수-값 가중치들을 갖는다. 이러한 논리의 시스템은 명제들(또는 만약 1차 논리 (first-order ligic)로 확장된다면 술어들(predicates)) 사이의 관계들의 더 유연한 표현들을 가능하게 한다. 가중된 퍼지 논리는 자동화된 러닝(learning)에 적합하다. 왜나하면 가중된 퍼지 논리는 주어진 공식에서 각각 의 피연산자(operand)의 영향을 조정하는 계속적인 수단을 제공하기 때문이다. 예를 들어, 루카시에비치 논리(Lukasiewicz logic)는, 문장들이 0 또는 1 뿐만 아니라 그 사이의 어떤 실수(예컨대, 0.3)의 진리 값을 할당받을 수 있는 실수-값 논리(real-valued logic) 또는 다치 논리(many- valued logic)이다. 루카시에비치 논리의 명제 연결사들(connectives)은 함축(implication) , 부정 (negation) , 동치(equivalence) , 약 논리곱(weak conjunction) , 강 논리곱(strong conjunction) , 약 논리합(weak disjunction) , 강 논리합(strong disjunction) , 그리고 명제 상수들 (propositional constants) 0 과 1을 포함한다. 신경-상징 표현(neuro-symbolic representation)은, 고전적 논리(또는 퍼지 논리)와 동등하게 동작하도 록 제약될 수 있는 신경망에서 가중된, 실수-값 논리이다. 실시예들에 있어서, 시스템 및 방법은 신경-상징 표 현을 위한 다양한 파라미터들을 결정할 수 있으며, 예를 들어, 신경-상징 표현은 활성화 함수들과 유사한 고전 적 함수들을 평가하는 뉴런들을 포함한다. 예를 들어, 일 실시예에서 신경망은 논리적 뉴런들을 포함한다. 논리적 뉴런은 AND, OR, IMPLIES, NOT, 및/또는 그 밖의 것들과 같은 논리 게이트를 에뮬레이트(emulate)할 수 있다. 논리적 뉴런들의 신경망은 논리 규칙을 구현하고 논리 추론(logic reasoning)을 수행하도록 훈련받을 수 있다. 논리적 뉴런들은 신경망에서 사 용된 어떤 시그모이드 뉴런(sigmodial neuron)과 유사할 수 있지만, 다음의 특성들에서 다를 있다. 논리적 뉴런 들의 가중치들은 그것들의 출력들이 예컨대, AND, OR, 및 IMPLIES 와 같은 고전적 논리에서 일어나는 연산들의 동작특성을 반영하도록 제약받는다. 그들의 입력들 및 출력들은 진리 값들에 관한 한도들(bounds)인 것으로 이 해된다. 만약 그들이 어떤(예컨대, 정의된) 스레쉬홀드 이상 또는 1 이하, 예컨대, 스레쉬홀드 α이상 또는 1- α이하로 한도가 정해진다면 비록 고전적으로 해석될 수 있지만, 진리 값들은 0에서부터 1까지의 범위이다. 그 것들은 정상적인(normal) 것으로 평가될 수 있고 긍정식(modus ponens)을 수행하는데 사용될 수 있으며, 그것에 의해 각각의 입력에 대한 한도들은 출력 및 다른 입력들에 대한 알려진 한도들에 기초하여 컴퓨팅된다. 긍정식 (modus ponens)은, 만약 조건문(\"p이면 q이다\")이 인정되고, 선행사건(antecedent)(p)이 유지(hold)된다면, 그 결과(q)가 추론될 수 있다는 논리 진술(logic stating)의 규칙을 일컫는다. 로지스틱 함수(logistic function) 및 선택된 참의 스레쉬홀드 0.5 < α < 1이 주어진다면, 크기 조정된(scaled) 오프셋 α-보존 시그모이드(preserving sigmoid)를 정의하는 것은 유용 하다. . 그것의 정의에 따라서, 이 함수는 이고 인 특성을 갖는다. 으로 색인이 만들어진 논리적 뉴런들에 대해, 정상적인 컴퓨테이션은 그러면 이다. 여기서 진리 값 입력들은 이고, 입력 가중치들은 이고, 활성화 스레쉬홀드는 이고, 활성화 출력은 이다. 논리적 뉴런들에 대한 입력들은 다른 논리적 뉴런들의 출력들일 수 있고 또는 명제들과 관련된 전용(specialized) 뉴런들의 출력들일 수 있다. 논리식들 (logical formulae)에서 모델링 연산들의 결과로서, 비록 연관 연산자들(associative operators)이 더 많은 입 력들을 가질수 있지만, 입력 세트 의 크기는, 예컨대, 바이너리 연산들에 있어서 단지 2인 것과 같이 매우 작 을 수 있다. 또 다른 측면에서, 를 더 크지만 대부분의 가중치들 이 정확히 0과 동일한 것으로 생각할 수 있다. 각각의 뉴런에서 활성화 함수는 그 뉴런 또는 노드의 출력을 컴퓨팅할 수 있고, 특정 논리 연산 또는 연 산들의 패밀리를 표현하도록 선택될 수 있다. 망에서 각각의 노드는 출력으로서 그것의 진리 값들에 대한 상한 (upper bound) 및 하한(lower bound) 값들을 컴퓨팅할 수 있다. 이들 상한 및 하한 진리 값들은 각 노드에서 연 속 변수들이고 망을 통해서 전달되었다. 이러한 한도가 정해진 진리 값 변수들은 예측 시간 또는 추론 시간에 노드에서 계산되고, 그것들은 입력들에 기초하여 컴퓨팅되고, 러닝된(learned) 중요도 가중치들(importance weights)에 관하여 컴퓨팅된다. 논리적 뉴런들에 대한 제약조건들(예컨대, 논리적 제약조건들)은, 그들이 모델링을 의도하는 연산들의 진리 테이블들로부터 비롯될 수 있고 \"참\" 및 \"거짓\" 값들을 위한 수립된 범위들로부터 비롯될 수 있다. 일 측 면에 있어서, 연속적인 진리 값들은, 만약 그것의 하한이 적어도 α이면 참(true)으로 간주되고 만약 그것의 상 한이 기껏해야 1-α이면 거짓(false)으로 간주된다. α-보존 시그모이드를 사용하는 경우, 그것은 그러한 적절 한 스레쉬홀드보다 높거나 낮게 있도록 그것의 선형 논증(linear argument)의 결과들을 제한하기에 충분할 수 있다. 도 6은 일 실시예에서 α-보존 시그모이드 활성화 함수를 나타낸다. 일 예로서, 예컨대, 바이너리 AND에 대한 진리표는 다음과 같이 주어진 제약조건들의 세트를 제시한다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 θ는 스레쉬홀드이고, w는 가중치들을 나타낸다. 예를 들어, 제약조건들을 유도함에 있어서, 거짓 논리 값은 (1-α)로 표현되고, 참 논리 값은 α 로 표현된다. 더 일반적으로는, n-ary 논리곱들(conjunctions)(양의 입력들의)은 다음과 같은 형태의 제약조건들을 갖 는다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "반면에 n-ary 논리합들(disjunctions)은 다음과 같은 형태의 제약조건들을 갖는다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "예를 들어, 은, 그들의 입력들 모두가 참이면, 참을 리턴하기 위해 논리곱들을 필요로 하고, 반면에 는, 비록 그들의 다른 입력들이 모두 1이더라도, 즉 최대로 참이더라도, 그들의 입력들 중 어떤 것이 거짓이면 거짓을 리턴하기 위해 논리곱들을 필요로 한다. 만약 논리합들의 참이라면, 그 반대로 된다. 로 정의된 부정(negation)을 고려하면, 고전적 아이덴티티 는 함축들(implications)에게 논리합들과 동일한 제약조건들을 사용하도록 허용한다. 드모르간의 법칙(DeMorgan laws), 예컨대, 하에서 제약조건들의 상기 두 개의 세트들은 등가이다. 이들 제약조건들의 결과로서, 논리적 뉴런들의 정상적인 평가는, 만약 그들의 입력들 모두가 또한 이들 범위들 내에 있다면, 참과 거짓에 대해 수립된 범위들 내에서 결과들을 낳도록 보장된다. 논리곱들을 위한 와 논리합들을 위한 형태의 활 성화 선형 입력들에 대해, 제약조건들은, 각각 및 을 가지며 위의 식 과 등가이다. 그래서 이것은 다음과 같은 형태의 제약조건들을 제시한다. 및"}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "일 측면에서, 진리 값들에 관한 한도들로서의 추론(reasoning)은, 신경망에서 논리적 추론(logical inference)을 구현함에 있어서 내재된 다수의 과제들을 줄여준다. 한도들은 추론 동안 진리 값이 차지할 수 있 는 네 개의 상태들을 표현하기 위해 명쾌하고 직관적인 메커니즘을 제공한다. 네 개의 상태들은 알려지지 않은 상태(unknown), 알려진 참(known true), 알려진 거짓(known false), 및 알려진 모순(known contradiction)이다. 한도들은 그것에 의해 긍정식(modes ponens)이 무결과(nonresult)를 리턴할 수 있는 메커 니즘을 제공한다. 한도들로서 추론하는 것은 동일 명제에 대한 다수의 증명들을 종합하는 능력을 용이하게 한다. 입력의 가중치는 정확히 0으로 드랍(drop) 가능하게 되는 것이 바람직할 수 있고, 그에 따라 1로부터 그 것을 효과적으로 제거한다. 이를 허용하기 위해 각각의 가중치에 슬랙(slack) 변수(들)이 도입될 수 있고, 이는 가중치가 0으로 드랍됨에 따라 또는 에 있어서 각각의 제약조건들이 위반될 수 있도록 한다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "(4*) (5*) 이것들은, 만약 Sk = 0 이거나 Wk = 0 이면 오리지널 제약조건들과 계속해서 일치한다. 최적화는, SkWk로 크기 조정하는 페널티 항(penalty term)을 포함하기 위해 훈련 손실 함수(training loss function)를 업데이트 함으로써 그러한 파라미터화들(parameterizations)을 선택하는 것을 포함할 수 있다. 이러한 페널티 항에 관한 계수(coefficient)는 어떻게 고전적 러닝 연산들이 있는지를 컨트롤하며, 만약 최적화가 패널티 항을 0으로 감 소시킨다면 정확한 고전적 동작은 복원된다. 도 1은 일 실시예에서 논리적 뉴런들의 망에서 뉴런 또는 논리적 뉴런을 나타낸다. 뉴런의 활성화 함수 내에 그 결과를 입력하기 전에, 뉴런에 대한 가중 입력들은 가중 합(weighted sum) 또는 선형 결합(linear combination)을 통해 뉴런 내부에 결합될 수 있다. 보여진 예에서, [0, 1]의 범위에서 논리 진리 -값들의 벡터 x에 관한 논리곱(AND) 연산에 대응되는 논리적 뉴런은 루카시에비츠 퍼지 t-놈(norm)(삼각- 놈)의 형태로 정의될 수 있다. 뉴런 활성화 함수에 대한 논거는 이하와 같다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "논리적 뉴런은 하나 또는 그 이상의 입력 피연산자들(operands) 를 수신한다. 입 력 피연산자들에서 각각의 피연산자들은 중요도(importance)를 나타내는 각각의 할당된 또는 컴퓨팅된 가중치 을 갖는다. 입력 값들은 정의된 공식 에 따라 결합되며, 뉴런 활성 화 함수 f에 입력된다. \"t\"는 뉴런 활성화에 예상되는 최대 입력을 나타내며, 논리곱의 출력이 완전히 참인 것 에 대응된다. 예로서, 논리곱(&, 또한 AND) 뉴런은 다음과 같이 정의될 수 있다. . 논리곱 뉴런에 대한 제약조건들 또는 논리적 제약조건들은 다음과 같이 정의될 수 있다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, 이다. 일 실시예에서, 시스템은 파라미터 값들을 결정하는데, 이들은 참의 스레쉬홀드(threshold of truth)와 활성화 스레쉬홀드(activation threshold)를 포함한다. 상기 파라미터 값들은 논리적 뉴런에서 활성화 함수에 대한 논거에서 계산됨에 따른 활성화 함수에 대해 예상되는 입력 분포에 대한 표현도(expressivity)와 그래디언 트 품질(gradient quality)의 곱(product)을 최대화하는 파라미터화된 활성화 f를 위한 것이다. 일 실시예에 있 어서, 제약된 뉴런들에서: 1. f는 홀수이고, 범위 [0,1]에서 단조 증가(monotonically increasing)하며, 알파 (α) 보존 상태(preserving)이고; 2. 스레쉬홀딩을 위한 진리의 정도(degree): 예컨대, 0.5 < α ≤ 1를 정의 하고; 3. 슬랙들(si)은 입력 제거(input removal)를 허용하며; 4. 예를 들어, 논리곱(AND) 뉴런에 대한 하나의 거짓 입력(false input)은 그것의 출력을 거짓으로 만들어야 한다. 신경 상징 표현은 논리식을 형성하는 복수의 논리적 뉴런들로 형성된 신경망을 가질 수 있다. 도 2A 및 2B는 일 실시예에서 논리적 뉴런들의 망의 일 예를 보여준다. 논리적 추론의 일 예는 \"수염 꼬리 (레이 저 포인터 → 추적), → 고양이\"(수염 AND 꼬리 AND (레이저 포인터 IMPLIES 추적) IMPLIES 고양이)일 수 있다. 논리적 추론의 또 하나의 예는 \"(고양이 개) → 펫(Pet)\"(고양이 OR 개) IMPLIES 펫(Pet)일 수 있다. 어떤 명제 또는 공식(formula)에 대한 알려진 진리 값들은 신경망 최적화에서 기본 진리(ground truth)로서 사용될 수 있다. 도 2A는 일 실시예에서 논리적 뉴런들을 갖는 신경망의 사용 사례를 설명하는 논리망 트리 구조(logical network tree structure)를 도시한다. 도 2B는 도 2A의 신경망의 사용 사례의 논리망 트리 구조에서 역 추론 컴 퓨테이션들(backward inference computations)을 보여준다. 도 2A는 신경망(\"네트\" 또는 \"NN\")의 사용 사례의 일 예를 보여주며, 이는 일 실시예에서 본 발명의 방 법들에 따라 형성된 논리적 뉴런들을 포함한다. 일반적으로 신경망이 1차 논리(first-order logic)로서 설명되 지만, 그 다음의 사용 사례는 먼저 명제 논리의 범위로 제한된 신경망을 설명한다. 도 2A에 보여진 바와 같이, 논리 연결사(connective) 또는 엣지 당 하나의 뉴런 플러스 명제 /술어 당 하나의 뉴런이 있다. 각각의 생성된 뉴런은 논리식들(205, 208)의 시스템과 1-대-1 대 응을 가지며, 여기서 평가는 논리적 추론과 같다. 논리 신경망들을 생성하기 위해, 컴퓨팅 시스템은 입력들, 예컨대 지식 그래프/예측 입력 데이터/사실들을 수신한다. 이것들은 어떤 뉴런, 예컨대, 통상적으로 쿼리 식 (query formula)과 관련된 뉴런에서 진리 값(상한과 하한에 의해 표현되는)을 출력하는데 사용하기 위한 것이다. 뉴런들은 공식 신택스 트리들(formula syntax trees)과 명제/술어 뉴런들을 매치시키도록 마 련되어 증명된 진리 값들을 제공하고 또한 반복적으로 종합(aggregate)한다. 일반적으로, 부정(negation)(및 1 차 한정자들(first-order quantifiers))은 어떠한 파라미터들도 없이 노드들을 통과한다. 보여지는 바와 같이 어떤 뉴런들은 0과 1사이의 값의 불확실성 한도들(uncertainty bounds)을 제공한다. 각각의 뉴런 은 잘 맞게 맞들어진 활성화 함수를 갖는데, 이것의 컴퓨테이션은 결국 진리 값들에 관한 한도들이 된다. 양방향 추론의 방법들이 더 이용될 수 있다. 도 2A에 보여진 바와 같이, 사용 사례의 논리망(네트(net))의 예는 NN 모델 생성시 논리식에 대응되도록 구성되고 상징 논리 추론(symbolic logic reasoning)에서 보여진 신택스 트리 그래프 구조를 포함한다. 논리망은, 노드들로서 논리 연결사들과 술어들을 가지며 논리식들의 대응되는 시스템의 신택스 트리 의 일부로서 엣지들을 갖는다. 도 2A에 제2 신택스 트리 구조가 보여지는데, 이는 또한 모델 생성시에 입 력 논리식에 대응되도록 구성된다. 입력 논리식들(205, 208)은 세상(world)에 관한 규칙들이고, 전문가들 에 의해 작성되거나 무계획 방식으로 생성될 수 있으며, 예컨대, 정수 선형 프로그래밍(integer linear programming, ILP) 또는 자연 언어 프로그래밍(natural language programming, NLP) 디바이스에 의해 추출될 수 있다. 공식들(205, 208)은 그것의 알려진 공식들 또는 부분들, 즉, 서브-공식들(sub-formulae)일 수 있고, 모델 생성 시에 및/또는 예측 또는 추론 시간 동안에 변경(modification) 또는 수정(tweak) 대상이 될 수 있다. 즉, 공식들에 대한 어떤 입력들 또는 서브-공식에 대한 입력들(예컨대, 피연산자들)은 그들의 중요도 (importance)에 기초하여 가중될 수 있고, 그 결과 NN의 구조의 일부가 되는 중요도 가중치들로 할당될 수 있다. 따라서, 공식에서 각각의 연결사(connective)의 각각의 피연산자에게는 중요도 가중치들이 할당되고 또는 할당될 수 있고, 그래서 만약 그것들이 최적화되지 않는다면 각각의 공식 또는 가중치의 변경들을 허용한다. 사용 사례에 있어서, 서브-그래프에 있어서, \"고양이\" 노드는 \"개\" 보다 두 배 더 중요하게 가중될 수 있고, 따라서 공식에서 \"개\"보다 \"고양이\"가 이제 더 영향력을 갖는다는 의미에서 결과들을 왜곡(skew)하는 논리합 \"OR\" 연결사의 변경된 버젼이다. 각각의 공식에 대한 신택스 트리 그래프들(204, 207) 각각은 입력 명제들의 초기 리스트에 기초하 여 구성된다. 입력 명제들의 초기 리스트는 공식에 존재하는 \"수염\"(202A), \"꼬리\"(202B), \"레이저 포인터\"(202C), \"추적\"(202D), \"고양이\"(202E1,202E2), \"개\"(202F), 및 \"펫\"(202D)을 포함하는 정의된 진리 값 (예컨대, 참 또는 거짓)을 갖는다. 이와는 다르게, 이들 입력들은 술어들(predicates), 예컨대, 진리 값이 하나 이상의 변수(들)에 의존하는 서술들(statements)일 수 있다. 논리식(205, 208)이 적용되는데, 이 논리식(205, 208)은, 각각의 추론 또는 함축(implication)의 평가(예컨대, \"수염 꼬리 (레이저 포인터 → 추적), → 고양이\") 그리고 추론 또는 함축의 평가(예컨대, \"고양이 개) → 펫\") 결과를 가져온다. 도 2A에서, 서 브-공식은 대응되는 서브-그래프(예컨대, \"고양이 개\")로 보여진다. 비록 추론 규칙들은 도시되지는 않 았으나, 특정 선택된 논리, 예컨데, 루카시에비치 논리에 기초하여 특정 노드들에 존재하는 활성화 함수들 이 도시되어 있다. 활성화 함수들은 특정 논리 연결사들, 예컨대, 공식들(205, 208)에 존재하는 노드 들(210A, 210B, 210C)에 대응되는 것으로 보여져 있다. 고전적 명제 논리 연결사들의 예는 부울 연산들(Boolean operations)을 이용한다. 예를 들어, 부울 연산들은 AND, OR, 함축(implication), 부정(negation) 및 논리적 동치(logical equivalence)를 포함하고, 진리 값들로 1들(ones) 또는 0들(zeros)을 이용한다. 반복되는(양-방향) 역 추론(역행 패스들(backwards passes))의 일 예가 도 2B에 보여지며, 이는 도 2A 에 보여진 NN의 예에 해당한다. 시스템은 집중되지 않았지만, 논리망의 역방향 횡단이 발생하여 여러가지 명제들의 진리 값들을 업데이트한다. NN에서, 뿌리 노드(root node)로부터 하나하나의 잎(leaf)까지의 모 든 경로 각각은 각 노드에서 아랫 방향으로 역 추론 규칙들을 평가하기 위해 횡단된다. 어떤 지침도 없는 일 실 시예에서, 각각의 공식에 대해 신택스 트리들에서의 잎 경로들에 대한 뿌리의 스윕(sweep)으로 깊이-제1 횡단 (depth-first traversal)이 수행된다. 예를 들어, 제1 경로는 \"고양이\"에 관한 지식에 기초하여 논리곱을 논박 (refute)하려고 시도하는 부정식(modus tollens) 엣지(edge), 그리고 만약 논리곱이 거짓이었다면 \"수 염\"을 반증(disprove)하려고 시도하는 논리곱 삼단논법(conjunctive syllogism) 엣지 242A인 것으로 평가된다. 그런 다음, 노드 210B 등에서 \"레이저 포인트는 추적(chases)을 함축한다\"는 서브-표현을 위해, 엣지 242B에서, 그런 다음 엣지 242C에서 다음의 논리곱 삼단논법 평가가 수행될 수 있다. 실시예들에 있어서, 정보는 트리 204에서 역함수 컴퓨테이션들을 업데이트하기 위해 다음과 같은 역방향 경로들을 따라 역방향으로 전파된다. 즉, 상기 역방향 경로들은, 화살표들 242, 242A로 보여진 경로 - 여기서 수염 술어(202A)의 진리 값(즉, 상한들과 하한들)이 논리곱 뉴런(210A)에서 역 활성화 함수 컴퓨테이션에 의해 업데이트됨 - ; 화살표들 242, 242B로 보여진 경로 - 여기서 꼬리 술어(202B)의 진리 값(상한들 및 하한들)이 논리곱 뉴런(210A)에서 역 활성화 함수 컴퓨테이션에 의해 업데이트됨 - ; 화살표들 242, 242C, 242D로 보여진 경로 - 여기서 레이저 포인터 술어(202C)의 진리 값(상한들과 하한들)은 논리곱 뉴런(210A) 및 함축 (implication) 뉴런(210B)에서의 역 활성화 함수 컴퓨테이션들에 의해 업데이트됨 - ; 화살표들 242, 242C, 242E에 의해 보여진 경로 - 여기서 추적 술어(202D)의 진리 값(상한들과 하한들)은 논리곱 뉴런(210A)과 함축 뉴런(210B)에서의 역 활성화 함수 컴퓨테이션에 의해 업데이트됨 - ; 및 화살표 243에 의해 보여진 경로 - 여기 서 고양이 술어(202E1)에 대한 진리 값(상한들과 하한들)은 함축 뉴런에서의 역 활성화 함수 컴퓨테이션에 의해 업데이트됨 - 이다. 이와 유사하게, 신택스 트리에서 역함수 컴퓨테이션들을 수행하기 위해 이하의 후방 경로들(backwards paths)이 횡단된다. 즉, 후방 경로들은, 화살표들 272, 272A에 의해 보여진 경로 - 여기 서 고양이 술어(202E2)에 대한 진리 값(상한들과 하한들)은 논리합 뉴런(210C)에서의 역 활성화 함수 컴퓨테이 션에 의해 업데이트됨 - ; 화살표들 272, 272B에 의해 보여진 경로 - 여기서 개 술어(202F)의 진리 값(상한들과 하한들)은 논리합 뉴런(210C)에서의 역 활성화 함수 컴퓨테이션에 의해 업데이트됨 - ; 및 화살표 273에 의해 보여진 경로 - 여기서 펫 술어(202G)에 대한 진리 값(상한들과 하한들)은 함축 뉴런에서의 역 활성화 함수 컴퓨테이션에 의해 업데이트됨 - 이다. 일 측면에서, 퍼지 논리(fuzzy logic)가 이용되는데, 퍼지 논리는 0과 1 사이의 범위에 있는 진리 값들 의 확률들을 제공한다. 즉 퍼지 논리에서는 0과 1 사이에서의 값들이 참과 거짓 사이의 애매한 혼합들이다. 일 실시예에서, 알파(α)는 예컨대, 0.5≤α≤1 사이의 범위에 있는 참의 스레쉬홀드로서 정의된다. 따라서, 어떤 평가된 진리 값은, α 위의 값들은 \"참\"으로 간주되고 양(quantity) 1-α보다 작은 어떤 진리 값은 \"거짓\"으로 간주되도록 제한된다. 활성화 함수는 그 뉴런의 출력을 컴퓨팅하기 위한 각각의 뉴런에서의 특정 함수이고, 특정 논 리 연산을 나타내도록 선택되고, 또는 활성화 함수들의 패밀리는 특정 논리 스킴, 예컨대 루카시에비치 논리와 일치하는 논리 연산들을 나타내도록 선택된다. 예를 들어, 서브-공식에서 보여지는 바와 같이 활성화 함수 는 입력들의 덧셈(합)으로 평가되는 논리합 함수(disjunction function)이다. 도 2A에 도시된 \"비가중 (unweighted)\" 사례에서, 노드 210C에서 논리합 활성화 함수는 0 또는 1 값이 되도록 고정(clamp)된 출력을 제 공한다. 일 실시예에 있어서, 실수-값 논리 연결사들에 대해, 활성화 함수들의 예는 다음과 같은 제약조건들에 따라 진리 값을 컴퓨팅한다. 논리곱(AND)에 대해:"}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기서 p와 q는 피연산자들이고, 제약조건은 두 개의 피연산자 값 입력들의 합 마이너스 1이고 0에 의해 아래로 고정되는 것이다. 논리합(OR)에 대해:"}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "즉, 두 개의 피연산자 입력 값들의 합이고 1에 의해 위로 고정된다. 함축(→)에 대해:"}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "즉, 논리합과 유사하지만, 입력들 중 하나가 부정(negate)된다(1-p). 일 예로서, 노드 210A에 보여진 바와 같이, 활성화 함수는 논리곱 함수이다. 도 2A에 도시된 \"비가중\" 사례에서, 루카시에비치 논리에 따른 논리곱 함수 활성화 함수는, 입력들의 덧셈(합) 마이너스 엘리먼트들의 수 플러스 1로 평가된다. 다른 논리 스킴들에 따른 다른 활성화 함수들이 실시예들에 이용될 수 있다. 더욱이, NN의 다른 파트들 에서 활성화 함수들과 같은 다른 로지스틱 함수들(logistic functions)이 사용될 수 있다. 예를 들어, 논리 연 결사들에 대한 \"가중\" 루카시에비치 논리 스킴이 이용되는데, \"가중\" 루카시에비치 논리 스킴에서는 활성화 함 수들이 다음과 같이 진리 값들을 컴퓨팅한다. AND에 대응되는 뉴런들의 활성화 함수에 대해, 가중 루카시에비치 t-놈(norm)은 다음과 같이 정의된다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "그런 다음, 논리합(OR)의 경우에 대해, 다음과 같이 연관된 가중 t-코놈(conorm)이 사용될 수 있다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "함축(→)에 대해,"}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "위의 제약조건들 각각에서, β는 연산을 수립하는 바이어스 항(term)이고, 가중치들(w)은 중요도 (importance)를 표현한다. wp는 입력 피연산자 p에 대한 가중치이고 wq는 입력 피연산자 q에 대한 가중치이다. 바이어스 항 β는 1의 값일 수 있지만, 또 다른 값일 수 있다. 클램프들(clamps)의 내부에는 초평면 (hyperplane)의 표현 또는 공식이 있다. 즉, 그것은 가중치들(입력들) 벡터 사이의 내적(dot product) 플러스 오프셋이다. 실시예들에서 시스템들 및 방법들은 이와 같이 위에서 설명된 신경망을 훈련시킴에 있어서 사용되는 여 러 가지 파라미터들을 결정할 수 있다. 예를 들어, 일 실시예에서 방법은 전용 인공 뉴런들로 논리를 구현하고 러닝할 수 있다. 예를 들어, 이는 관찰들(observations)로부터 논리를 알아내는 것(uncovering), 논리 프로그램 들을 교정하는 것(correcting), 신경망들 및 구별가능한 러닝 시스템들 내에 논리적 추론들을 통합하는 것에 관 한 문제에 대처할 수 있다. 상기 방법은 논-제로 그래디언트들을 제공하기 위한 근사(approximate) 뉴런 활성화 함수일 수 있다. 상기 방법은 또한 상대 그래디언트 크기를 계량화(quantify)할 수 있다. 상기 방법은 또한 뉴 런 입력 가중치들 표현도(expresivity)를 정의할 수 있다. 상기 방법은 또한 그래디언트 품질(gradient quality)과 표현도 사이의 트레이드오프(tradeoff)를 최적화할 수 있다. 상기 방법은 또한 최적의 활성화 함수, 및 논리적 뉴런들에 대한 논리적 제약조건들의 세트를 정의할 수 있다. 그래디언트 품질은 뉴런의 활성화 함수의 연산 범위에서 0이 아닌(논-제로) 및/또는 상대적으로 작지 않 은(논-스몰(non-small)) 그래디언트들의 유용성(availability)과 관련되는데, 높은 그래디언트 품질은 뉴런 파 라미터들에서의 변화에 대한 진리-값 출력들에서의 논-제로 및 상대적 논-스몰 변화가 있다는 것을 보장한다. 예를 들어, 러닝(learning)이 그래디언트 하강(gradient descent)을 통해 일어날 수 있도록 하기 위해, 그래디 언트 품질은 모든 조건들 하에서 에러 그래디언트들을 계산할 수 있는 능력을 보장하는 것과 관련된다. 일 실시 예에 있어서, 그래디언트 품질은 상대적 스칼라 척도(relative scalar measure)일 수 있고, 비율-평균 플랭크들 그래디언트(ratio-averaged flanks gradient), 비율-평균(ratio-averaged)인 플랭크 그래디언트들(flank gardients)을 수반하는 평균 그래디언트로서 표현될 수 있다. 활성화 함수(뉴런의 활성화 함수)를 최적화함에 있어서, 그래디언트 품질 척도는 상대적으로, 예컨대, 새로운 최고값(optimum)에서의 그래디언트 품질이 이전의 최고값보다 더 높은 최적화 방향을 선택하는 것으로 사용될 수 있다. 더 높은 비율-평균 측면 그래디언트를 갖 는 활성화 함수는, 더 작은 비율-평균 측면 그래디언트를 갖는 것보다 더 높은 품질을 갖도록 고려된다. 표현도 (expressivity)는 뉴런의 가장 큰 입력 가중치에 대한 가장 작은 입력 가중치의 비를 일컬으며, 논리적 뉴런들 에 대한 입력 가중치들의 분포 폭(distribution width)과 관련된다. 참의 스레쉬홀드(Threshold-of-truth)는 실수-값 진리 값들(real-valued truth values)이 그 위에서 참 인 것으로 여겨지는 한계(limit)를 의미하며, 그리고 참의 스레쉬홀드는 고전적 참/거짓 진리 값들의 외부 해석 을 위해 사용될 수 있다. 일 실시예에 있어서, 참의 스레쉬홀드는 활성화 함수의 러닝가능한 파라미터일 수 있다. 참의 스레쉬홀드를 나타내기 위한 기호는 \"α\"일 수 있다. 계속되는 진리 값은, 만약 그것의 하한이 적어도 α이면 참인 것으로 간주되고, 만약 그것의 상한이 기껏해야 1-α이면 거짓으로 간주된다. 도 5는 일 실시예에 서 참의 스레쉬홀드(또는 진리의 정도)의 일 예를 나타낸다. 일 실시예에 있어서, 최적화 프로세스는 논리적 뉴 런들의 신경망에서 머신 러닝을 위해 참의 스레쉬홀드 값을 결정한다. 일 실시예에 있어서, 논리적 뉴런들에 관 한 제약조건들은 그것들이 모델링을 의도하는 연산들의 진리표들로부터 비롯될 수 있고 \"참\" 및 \"거짓\" 값들에 대해 수립된 범위들로부터 비롯될 수 있다. 계속되는 진리 값은, 만약 그것의 하한이 적어도 α이면 참으로 간 주되고, 그것의 상한이 기껏해야 1-α이면 거짓으로 간주된다. 사용자 지정 애러티(user-specified arity) 및 사용자-지정 최대 표현도(user-specified maximum expressivity)를 갖는 연산자에서, 일 실시예에서의 방법은 사용자-지정 최대 표현도에서 표현도와 그래디언트 품질의 곱(product)을 최대화하는 알파(α)를 계산한다. 일 측면에서, 알파는 고전적 참/거짓 의미에서 진리 값 들의 외부 해석을 위해 사용될 수 있는 참의 스레쉬홀드이고, 그리고 그것을 1보다 작게 설정함으로써 그것은 논-제로 그래디언트들을 허용한다. 예를 들어, 논-제로 그래디언트들은, 예컨대, 역-전달(back-propagation)에 있어서 머신 훈련을 용이하게 하고, 사라지는 그래디언트들(vanishing gradients)의 문제를 경감 또는 완화시킬 수 있다. 여기서의 설명에서 이하의 기호들이 또한 사용된다. 에러티(arity)는 뉴런에 대한 입력 연결들의 수를 일컬으며, 여기서 가중치 파라미터(weight paramete r)는 모든 입력 각각과 연관되고 모든 입력 각각에 곱해진다. 최대 표현도(maximum expressivity)는 뉴런 입력들(뉴런에 대한 입력들)의 가중(weighting)을 통해 표 현될 수 있는 입력 중요도(input importance)에서의 가장 큰 차이를 일컬으며, 논리적 뉴런에 의해 지원될 수 있는 입력 가중치 차이들을 위한 보증을 제공한다. \"t_max\"는 뉴런 활성화에 예상되는 최대 입력의 상한을 일컬으며, 논리적 뉴런 공식화(formulation)에서 스레쉬홀드의 상한에 해당한다. 논리적 뉴런 공식화의 스레쉬홀드는 주어진 제약조건들 하에서 설정될 수 있는 최대값이다. \"t\"는 뉴런 활성화에 예상되는 최대 입력을 일컬으며, 이는 논리곱의 출력이 완전히 참인 것에 해당한다. \"t-sum(w)\"(t 마이너스 뉴런의 w의 합, 여기서 w는 입력과 연관된 가중치를 나타냄)은 논리적 뉴런의 활 성화 함수에 대한 최소 예상 입력을 일컬으며, 완전히 거짓인 출력과의 논리곱에 해당된다. 여기서 일 실시예에서의 방법은 뉴런 스레쉬홀드의 상한인 t_max를 계산하고, 제한된 선형 프로그램(업 데이트된 뉴런 파라미터들을 계산하는)에서 뉴런 스레쉬홀드인 t의 상한을 t_max로 무한정으로 설정한다. T_max 는 연산자(예컨대, AND 연산자)의 진리표로부터 비롯된 논리적 제약조건들에 기초하여 계산될 수 있다. 시스템 이 알파 < 1로 설정된 참의 스레쉬홀드를 가질 때(이것은 뉴런의 전체 연산 범위에서 논-제로 그래디언트들을 제공하므로, 보통 그러한 경우일 수 있음), 이것은 또한 t-sum(w)(1-알파) >=알파의 주어진 제약조건들을 통해 뉴런 입력 가중치들의 크기 및 합을 또한 제한할 것이다. 이것은, 뉴런 입력 가중치들에서 원하는 수준의 표현 도 또는 변화(variation)를 위해 제공하는 한편, 허용가능한 뉴런 입력 가중치 파라미터 영역을 최소화한다. 그 래서 최소의 볼록한(컨벡스) 영역(convex region)은 최적화 단계 동안 큰 급작스런 파라미터 단계들을 감소시키 는 것을 도울 수 있다. 일 실시예에 있어서, 애러티, 최대 표현도, 알파, t_max는 논리적 뉴런의 초기화 단계 동안 결정되어 모 든 장래의 뉴런 가중치 파라미터 업데이트들 동안(예컨대, 훈련 동안) 그 후에 계속 고정된 상태로 유지된다. 일 실시예에 있어서, 러닝 방법은 또한, 현재의 표현도에서 그래디언트 품질을 최적화하기 위해, 가중치들 사이 의 특정 가중치 차이 비(ratio)까지 지원하는 현재의 가중치 표현도에 기초하여, 논리적 뉴런과 연관된 참의 스 레쉬홀드 알파를 변경하는 것을 또한 볼 수 있다. 알파에 관한 이러한 변경은 현재의 표현도가 변경되었을 때, 즉 뉴런 입력 가중치 비들, 예컨대, 최소 가중치에 대한 최대 가중치의 비가 변경되었을 때, 수행될 수 있다. 일 실시예에 있어서, 모든 연산자들 각각은 자신의 3-피쓰 누설(3-piece leaky) 정류 선형 유닛 (rectified linear unit)(ReLU) 활성화 함수를 보유할 있으며, 모든 파라미터 각각이 업데이트된 후 t와 t- sum(w)에서 포화(saturate)시키도록 업데이트된다. 정의된 활성화 함수 오브젝티브(objective)에 따라, 3-피쓰 누설 ReLU는 최적이 될 수 있다. 간단히 말해, ReLU는 활성화 함수의 한 유형이며, 신경망들에서 사용될 수 있 다. 누설 ReLU에서, x < 0일 때, 누설 ReLU는 함수가 0인 대신에 작은 음의 기울기(예컨대, 0.01 정도)를 가질수 있다. 애러티, 표현도 및 알파 사이의 관계에 기초하여 관찰들이 수행될 수 있다. 예를 들어, 애러티에서의 증 가는 그래디언트 품질을 감소시킬 수 있다. 이러한 관찰에 기초하여, 일 실시예에서, 여기서의 방법은 작은 애 러티 게이트들로부터 더 큰 애러티 게이트들을 구축하기 위해 최대 허용가능 애러티를 설정할 수 있고 구성 (composition)과 연관(associativity)을 활용할 수 있다. 일 실시예에 있어서, 표현도와 러닝(그래디언트들)의 품질 사이의 트레이드오프를 최대화하기 위해 알파 에 대한 최적값이 결정된다. 최대 가중치와 최소 가중치 사이의 비를 설명하는 표현도에 추가로, 슬랙 변수들 (slack variables)이 도입될 수 있다. 슬랙들은, 더 높은 표현도를 필요로 함이 없이 입력들이 완전히 제거되도 록 여전히 허용할 수 있다(예컨대, 가중치(w)를 0으로 설정하거나 갖도록 함으로써(예컨대, w=0)). 예를 들어, 제약조건 슬랙 변수(constraint slack variable)는 러닝 단계 동안 각각의 가중치를 위해 도입될 수 있고, 그리 하여 그 제약된 선형 프로그램은 그것이 논리적 제약조건을 위반했기 때문에 실행불가능했던 가중 값들을 설정 하도록 허용되게 한다(슬랙 변수들을 통해). 유효 애러티는, 입력들이 드랍(drop)될 때 알파 및 t_max와 함께 재계산될 수 있다. 여기서의 방법은, 주어진 최대 표현도(예컨대, 최대 입력 가중치와 최소 입력 가중치 사이의 비에 의해 정의된)와 연산자 크기 또는 애러티에 대한 활성화 함수의 함수 그래디언트들을 최적화하기 위해, 논리적 뉴런 들과 같은 가중된 실수-값 논리 연산자들(예컨대, 가중 오프셋 루카시에비치 실수-값 강 논리곱 또는 논리합 연 산자들)의 참의 스레쉬홀드(알파), 활성화 함수 및 논리적 제약조건들(예컨대, 컨벡스 최적화 선형 프로그램)을 결정한다. 실수-값 논리는 진리 값들이 연속적인 범위를 포함하는 다치 논리(many-valued logic)를 일컫는다. 일 실시예에서 방법은 참의 스레쉬홀드(알파)와 거짓의 스레쉬홀드(threshold-of truth)(1-알파)를 정의 함으로써 개선된 표현도 및 머신 러닝을 가능하게 하기 위해 논리적 제약조건들에 대한 완화(relaxation)를 도 입할 수 있다. 참의 스레쉬홀드(알파)는 그것보다 높은 진리-값들이 고전적 논리 참 값들처럼 작용하는 불확실 성(uncertainty)이고, 거짓의 스레쉬홀드(1-알파)는 그것보다 낮은 값들이 고전적으로 거짓으로 작용하는 것이다. 일 실시예에서 방법은, 참의 스레쉬홀드가 주어질 때, 참의 스레쉬홀드와 거짓의 스레쉬홀드보다 각각 높고 낮은 논리적 뉴런의 활성화 함수의 연산 영역의 부분들에 대해서만 비-평균 그래디언트들(ratio-average gradients)을 획득함으로써 논리적 뉴런의 활성화 함수에 대한 그래디언트 품질의 척도(measure)를 계산할 수 있다. 일 실시예에 있어서, 그래디언트 품질은 상대적 스칼라 측정값이고, 비-평균 플랭크들 그래디언트의 텀들 로 측정될 수 있다. 일 실시예에서 방법은, 가중치들의 합이 단지 최소 가중치와 표현도만 주어진 경우 결정될 수 있도록 하 기 위해 가중치 분포를 모델링함으로써, 표현도와 참의 스레쉬홀드 사이의 관계를 수립하기 위해 논리 연산자의 표현도 또는 기능을 그것의 논리적 제약조건들 내에 포함시킬 수 있다. 도 7은 하나의 실시예에서 가중치 분포를 보여준다. 일 실시예에 있어서, 표현도는 논리적 뉴런으로의 최소 입력 가중치에 대한 최대 입력 가중치의 비로 결정된다. 예를 들어, 표현도 이다. 예를 들어, 이다. 가중치들의 합은 로 표현될 수 있고, 여기서 n은 입력 가중치들의 수 또는 애러티를 나타낸다. 일 실시예에 있어서, 균일하고 정상적인 것과 같은 대칭 가중치 분포들에 추가로, 방법은 또한 비대칭 가중치 분포들을 허용하고 허용가능한 표현도와 참의 스레쉬홀드 사이의 관계를 결정한다. 일 실시예는 간격 (interval) 당 이벤트들의 양을 설명하는 쁘와종 분포(Poisson distribution)에 따라(또는 그 반대로) 입력 가 중치들을 모델링할 수 있다. 예를 들어, 전자메일(이메일)과 관련된 사용 사례에서, 방법의 일 실시예는 간격 당 이벤트들의 양을 설명하는 쁘와종 분포에 따라(또는 그 반대로), 예컨대, 원치 않거나 부적절한 이메일을 필 터링하는 논리적 필터에서 주어진 입력 이메일(또는 이메일 메일보관함(inbox))에 대해 하루 단위로 수신된 이 메일들 또는 행해진 답신들의 수에 따라, 입력 가중치들을 모델링할 수 있다. 이메일 제공자의 논리적 필터가 그 이메일 사용자들/메일보관함들 각각에 대해 들어오는 이메일 활동/카운트들을 분석할 때, 논리 규칙은, 의심 되는 이메일이 원치 않는 이메일인 것으로 간주될 때 참이거나 또는 그렇지 않으면 거짓인 실수-값 논리 출력을 결정하기 위해, 입력으로서 이들 메일보관함들 각각에서 의심되는 이메일의 수신을 취할 수 있다(부울 조건(Boolean condition). 논리적 필터는 내부의 논리적 처리에 기초하여 이메일들의 분류 또는 라벨링을 제공할 수 있다. 예를 들어, 이메일 제공자는 이메일 스토리지를 호스팅하고, 자신의 등록된 사용자들에게 서비스들에 대 하여 서비스들을 보내고, 받으며 접근한다. 논리적 필터는, 예를 들어 이메일이 평판이 좋은 소스로부터 들어오 는지의 여부와 같은 라벨 또는 분류를 제공하기 위해, 사용자의 메일보관함에 들어오는 모든 이메일들에 자동적 으로 적용된다. 논리적 필터는 내부 처리를 가질 수 있는데, 이는, 예를 들어, 이메일 발송자 주소 도메인이 발 송 이메일 서버 도메인과 같은지의 여부와 같은, 이메일 또는 사용자에 관한 외부 조건들과 연관된 입력 진리 값들을 취하는 하나 또는 그 이상의 논리적 규칙들에 기초한다. 개시된 방법은, 이메일의 입력 명제들 및 특징 들에 기초하여, 이메일이 어떻게 의심되는 것인지를 나타내도록 하는 것과 같이, 논리적 필터로부터 실수-단위 간격(real-unit interval) [0,1] 진리-값을 생성해 낼 수 있다. 따라서 입력 이메일이 의심되는 이메일을 수신 했는지의 여부에 관한 명제들을 취하는 논리 연산자에 대한 입력 가중치는 입력의 영향을 증가시키거나 감소시 킬 수 있는 관련 정보를 포함시킬 수 있다. 예를 들어, 만약 논리 연산자 활성화들을 갖는 뉴런들의 망이 많은 들어오는 이메일들을 수신한다면, 의심되는 이메일이 예상되고, 그래서 그 입력은 하향-가중(down-weighted), 즉 그 입력(또는 뉴런)에 대한 가중치가 감소된다. 예를 들어, 만약 전형적으로 논리 필터가 의심되는 이메일들 을 수신하지 않는 많은 받은편지함들이 동일한 이메일을 모두 수신하는 것을 관찰한다면, 이는 이메일이 의심스 럽다는 더 강한 표시이다. 반면에, 만약 전형적으로 많은 이메일들을 받는 받은편지함이 또한 그 의심되는 이메 일을 수신한다면, 훨씬 더 적은 수의 이메일들을 수신하는 또 다른 받은편지함에서 온 것보다, 그 이메일을 의 심하기 위한 투표로 그것은 덜 작용할 것이다. 일 실시예에 있어서, 최소 가중치에 관한 하한은, 표현도, 연산자 애러티 및 참의 스레쉬홀드에 의해 결 정된다. 단지 균일한 표현도만이 요구되는 곳에서는, 그 계산은 시스템 전체의 표현도를 1로 설정함으로써 단순 화될 수 있다. 일 실시예에서 방법은, 예컨대, 구분 선형 함수(piecewise linear function)를 형성함으로써, 논리적 연산 범위에 걸쳐 활성화 함수의 그래디언트들을 최적화하기 위해, 주어진 참의 스레쉬홀드에 대한 활성화 함수 를 구성할 수 있고 연속적으로 업데이트할 수 있다. 구분 선형 함수는, 최소 연산 범위에서, 거짓의 스레쉬홀드 입력에 대해서는 거짓의 스레쉬홀드에서, 참의 스레쉬홀드 입력에 대해서는 참의 스레쉬홀드에서, 거짓이고, 그 리고 최대 연산 범위에서 참인 함수이다. 일 실시예에 있어서, 거짓의 연산 범위는 [0, 1-알파]이다. 일 실시예 에 있어서, 참의 연산 범위는 [알파, 1] 이다. 일 실시예에서 방법은 표현도와 최대 표현도에서의 그래디언트 품질의 곱을 최대화하는 무한정으로 동일 한 값으로 참의 스레쉬홀드를 설정할 수 있다. 참의 스레쉬홀드 값은 현재의 표현도에 기초하여 업데이트될 수 있다. 예를 들어, 표현도는 가중치들이 러닝되고 변경됨에 따라 변할 수 있다. 변경된 표현도에 기초하여, 참의 스레쉬홀드 값은 재컴퓨팅(recomputing) 또는 업데이트될 수 있다. 일 실시예에 있어서, 방법은 참의 스레쉬홀드를 결정하는 단계 및 업데이트하는 단계를 포함할 수 있다. 상기 방법은 사용자 값 선택들의 세트에 기초하여 실수-값 논리 연산자들의 다양한 측면들 및 특성들을 계산하 고, 업데이트하고 시각화하는 사용자 인터페이스를 제공하는 단계를 또한 포함할 수 있다. 표현도 및 애러티에 관한 적절한 값들은 실수-값 논리의 다양한 측면들 및 특성들에 기초하여 걸정될 수 있다. 예를 들어, 상기 방 법은 논리적 결정 표면(logical decision surface) 및 연산 영역을 시각화할 수 있고, 그것에 관한 표현도 및 애러티의 영향을 분석할 수 있으며, 또한 표현도에 관한 하한들을 보장하는 실행가능한 파라미터 영역들을 결정 할 수도 있다. 도 9는 일 실시예에서 사용자 인터페이스 시각화의 일 예를 나타낸다. 시각화는 활성화 오브젝티 브 플롯(activation objective plot)을 제공한다. 이러한 활성화 오브젝티브 플롯은, 사용자가 최대 표현도 또는 알파 값들을 설정하기를 원하는 것이 어느 x- 또는 y- 값인지를 사용자에게 결정할 수 있도록 허용 한다. 예를 들어, 플롯 또는 그래프에서 y-축은 표현도 설정을 나타내고 x-축은 알파 값 설정을 나타낸다. 플롯 에서 다양한 포인트들은 최대 표현도 및 알파 값의 상이한 값들에 대한 활성화들을 보여준다. 이러한 시각화는, 예를 들어, 시그모이드 함수 및 ReLU 함수(예컨대, 3-피쓰 누설 ReLU)와 같은(이러한 예로 한정되는 것은 아님) 상이한 활성화 함수들에 대해 보여질 수 있다. 최대 표현도 대(versus) 알파의 상이한 플롯들은 애러티(연산자 들 또는 입력들의 수)의 상이한 값들에 대해 보여질 수 있다. 일 실시에에서 방법은, 입력들이 완전히 제거되도록 허용하기 위해 정의된 표현도 값에서의 대응되는 변 화를 필요로 함이 없이, 입력들이 영(zero) 가중치를 수신하도록 허용하기 위해, 논리적 제약요건들에 슬랙들을 도입할 수 있다. 일 실시예에서 방법은, 입력들로서 참의 스레쉬홀드, 애러티 및 표현도를 취하는 특정 공식이 주어지면, 최대 논리적 오프셋(예컨대, 가중된 오프셋 루카시에비치 실수-값 강 논리곱 또는 논리합 연산자들에 대한)과 논리적 제약조건들에서의 대응되는 오프셋 상한을 동일한 값으로 무한정으로 설정할 수 있다. 일 실시예에 있어서, 방법은 활성화 함수의 러닝가능한 파라미터로 참의 스레쉬홀드를 특정한다. 일 실 시예에서, 활성화 함수 최적화는 논리적 기능성(ligical functionality)을 보장하기 위해 세부사항들 (particulars)을 특정한다. 진리 값들에 관한 한도들로서 명료하게 추론하는 것은 신경망에서 논리적 추론을 구현함에 있어서 내재 된 많은 과제들을 완화시킨다. 한도들은 고상하고 직관적인 메커니즘을 제공하여 추론 동안 진리 값이 차지할 수 있는 네 가지 상태들을 표현한다. 네 가지 상태들은 알려지지 않은(unknown), 알려진 참, 알려진 거짓, 그리 고 알려진 모순(contradiction)이다. 한도들은 그것에 의해 긍정식(modes ponens)이 무결과(nonresult)를 리턴 할 수 있는 메커니즘을 제공한다. 한도들로서 추론하는 것은 동일 명제에 대한 다수의 증명들을 종합할 수 있는 능력을 용이하게 한다. 다음은 가중 실수-값 논리 연산자 또는 논리적 뉴런의 참의 스레쉬홀드(알파), 활성화 함수 및 논리적 제약조건들(예컨대, 컨벡스 최적화 선형 프로그램)을 결정하는 것을 나타내며, 이는 AND 연산 또는 게이트를 구 현한다. 시스템들 및 방법들은 또한 다른 논리 게이트들 또는 연산들에 적용할 수도 있다. 일 실시예에 있어서, 범위 [0,1]에서 논리적 진리-값들의 벡터 x에 관한 논리곱 또는 AND 연산자에 대응 하는 논리적 뉴런은, 뉴런 활성화 함수 f에 대한 논거(argument)인 루카시에비치 퍼지 t-놈(norm)(삼각-놈)의 형태로 다음과 같이 정의될 수 있다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "다음은 일 실시예에서 표현도 및 참의 스레쉬홀드(α)를 정의하는 것을 나타낸다. 다음의 기호들이 공식 화들(formulations)에서 사용된다. t는 뉴런 스레쉬홀드 또는 바이어스를 나타내며, 이는 뉴런 활성화 함수에 대한 최대 입력에 해당한다. w는 논리적 뉴런에 대한 입력들 각각과 연관된 러닝가능한 가중치 파라미터들의 벡터를 나타낸다. α는 뉴런에 대한 참의 스레쉬홀드를 나타내며, 그것보다 높으면, 진리-값들(0과 1의 범위 사이)은 고전 적으로 참인 것으로 해석될 수 있다. n은 논리적 뉴런에 대한 애러티 또는 입력들의 수를 나타낸다. λ는 표현도 또는 최소 입력 가중치에 대한 최대 입력 가중치의 비를 나타낸다. t_min은 뉴런 스레쉬홀드에 관한 하한을 나타내며, 뉴런 논리 제약조건들을 계속해서 충족시키면서 뉴런 스레쉬홀드가 설정될 수 있는 최소값에 해당된다. w_min은 논리적 뉴런에 대한 입력 가중치들의 세트 중에서 최소 입력 가중치를 나타낸다. s는 슬랙 변수들의 벡터를 나타내며, 논리적 뉴런에 대한 입력 가중치 파라미터들 각각과 연관된 것이다. 이들 슬랙 변수들은 제약 만족도(constraint satisfaction)를 결정하고 손실 항들(loss terms)을 계산 하는데 사용되지만, 그렇지 않고 예컨대, 논리적 뉴런의 추론 단계에서는 사용되지 않는다. s_i는 정수 i에 의해 색인이 만들어진 뉴런 입력과 연관된 특정 슬랙 변수를 나타낸다. x_min은 논리적 뉴런의 연산 범위에서 최소점을 나타내며, 논리적 뉴런의 활성화 함수에 예상되는 최소 입력에 해당된다. t-max는 뉴런 스레쉬홀드에 관한 상한을 나타내며, 뉴런 논리적 제약조건들을 충족시키면서 뉴런 스레쉬 홀드가 설정될 수 있는 최대에 해당한다. 일 실시예에 있어서, 다음은 t에 관한 하한을 정의한다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "[105] 일 실시예에서, 다음은 t에 관한 상한을 정의한다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "[106] 일 실시예에서, t에 관한 상한(t_max)은 i에 대해 제약조건을 선택함으로써 계산되는데, 여기서 wi는 t가 최대 가능 값이 되도록, 즉 wi = wmin이 되도록 선택된다. 여기에 사용된 알파 값은 다른 제약조건들에 사용된 알 파 값과 동일할 수 있고, 모든 유도된 값들의 계산은 알파에 의존한다. 일 실시예에 있어서, 주어진 논리적 뉴 런에 대해 하나의 알파 값이 있을 수 있다. 다른 논리적 뉴런들에 대해서는 상이한 알파 값들이 있을 수 있다. 일 실시예에 있어서, 다음은 x에 관한 하한들을 정의하는데, 이는 논리적 뉴런의 활성화 함수에 대해 예 상된 가장 작은 입력이고 뉴런의 연산 범위의 최소점이다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "일 실시예에 있어서, 다음은 wmin에 관한 하한을 정의한다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "]0109] 일 실시예에 있어서, 다음은 논리적 대역폭(logical bandwidth)을 정의하는데, 이는 논리적 뉴런의 활성 화 함수의 연산 범위의 폭이다. 더 큰 논리적 대역폭은 필수적으로 더 작은 그래디언트 품질을 의미하는 반면, 더 작은 논리적 대역폭은 더 작은 표현도를 의미한다. 따라서, 논리적 대역폭은 시스템 최적화에서 그래디언트 품질과 표현도를 밸런싱하는데 사용되는 척도(measure)이다. 일 실시예에서, 다음은 λ(표현도)에 관한 상한을 정의한다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "위에서 정의된 t에 관한 하한, t에 관한 상한, x에 관한 하한들, wmin에 관한 하한, 논리적 대역폭, 및 λ에 관한 상한은 활성화 최적화 함수를 공식화하는데 사용된다. 다음은 일 실시예에서 활성화 최적화를 설명한다. 최적화는 비(ratio)- 평균 플랭크들 그래디언트(g로 표현됨)와 표현도(λ로 표현됨)를 최대화한다(활성화 최적화)."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "g는 그래디언트들을 나타낸다. 다음은 상기 활성화 최적화에서 비-평균 플랭크들 그래디언트(ratio-average flanks gradient)를 설명 한다. 일 실시예에서, 비-평균 플랭크들 그래디언트들은 비-평균된(ratio-averaged) 두 개의 플랭크 그래디언트들과 관련된 평균 그래디언트(average gradient)이다. 일 실시예에서, 구분 선형 활성화 함수(piecewise linear activation function)는 xmin에서 tmax의 도메인까지 그리고 0에서 1의 범위에 걸쳐 있는 세 개의 연결된 선형 라인들을 포함한다. xmin에서 1-α까지의 좌측 선형 조각은 거짓 출력과 연관되고, 1-α에서 α까지의 가운 데 선형 조각은 퍼지 출력들과 연관되고, α에서 tmax까지의 우측 선형 조각은 참 출력과 연관된다. 좌측 선형 조각(좌측 플랭크)의 그래디언트는 1-α-xmin의 도메인 폭으로 나누어진 1-α의 범위이다. 우측 선형 조각(우측 플랭크)의 그래디언트는 도메인 폭 tmax-α에 의해 나누어진 1-α의 범위이다. 좌측과 우측 선형 조각의 합에 대한 좌측 선형 조각의 도메인 폭 비는 1-α-xmin과 tmax-α의 합으로 나누 어진 1-α-xmin이다. 이와 유사하게, 좌측과 우측 선형 조각의 합에 대한 우측 선형 조각의 도메인 폭 비는 1-α -xmin과 tmax-α의 합으로 나누어진 tmax-α이다. 그래서 비-평균 플랭크들 그래디언트는 좌측 플랭크의 도메인 폭 비와 좌측 플랭크의 그래디언트 (1- α)/(1-α-xmin)의 곱, 플러스 우측 플랭크의 도메인 폭 비와 우측 플랭크의 그래디언트 (1- α)/(tmax - α)의 곱이다. 유도되는 최적화 함수, 는 함수 g의 최대화를 포함하고, 표현도 값 λ에 대한 함수를 최대화하는 참의 스레쉬홀드 α 값을 위해 해결된다. 최적화 함수에서, xmin, tmax는 알려져 있 고, 주어지며 또는 위에서 설명된 것과 같이 컴퓨팅된다. 뉴런 에러티와 표현도 람다(λ)는 사용자에 의해 특정될 수 있다. 또 다른 측면에서, 뉴런 애러티와 표 현도 람다(λ)는 미리 정의될 수 있다. 그리고 개시되는 시스템 및/또는 방법은 활성화 오브젝티브를 통해 알파 를 자동적으로 최적화할 수 있다. 애러티, 람다(λ) 및 알파(α)를 알아냄으로써, 논리적 뉴런은 완전히 설명될 수 있고, 가중치들의 러닝을 위해 준비된다. 가중치들이 시간에 따라 적응됨으로써, 표현도 람다(λ)는 현재의 가중치들에서 요구되는 더 큰 표현도를 수용하기 위해 재계산될 수 있고, 알파(α)는 람다(λ)가 변할 때(변하 는 람다(λ)에 응답하여) 재최적화(reoptimize)될 수 있다. 위의 공식은 실수-값 논리 회로들 또는 게이트들의 기능(capacity) 및 머신 러닝(그래디언트들)을 최적 화한다. 함수 g는 관심이 있는 두 개의 값들, 즉 3-피쓰 선형 정류 활성화 함수의 비-평균 플랭크들 그래디언트 및 표현도를 결합한다. 씬(thin) 플랭크 섹션이 그래디언트 품질 척도에 덜 기여하는 비-평균(ratio- averaging)을 획득하기 위해, 비-평균 플랭크들 그래디언트는 전체 연산 범위까지 3-피쓰 선형 정류 활성화 함 수의 각각의 플랭크 섹션 폭의 비에 그래디언트를 곱한다. 그런 다음, 표현도와 그래디언트 품질의 곱을 최대화 하는 활성화 함수 f는 위의 공식을 통해 결정되는데, 이는 논리적 뉴런들에게 표현도 λ까지 상이한 크기들의 가중치들을 지원할 수 있는 활성화 함수를 제공하는 한편 또한 논리적 뉴런의 전체 연산 범위에 대해 상대적으 로 작지 않은(논-스몰(non-small)) 그래디언트들을 제공한다. 일 실시예에 있어서, 위의 활성화 오브젝티브(activation objective)는 람다(λ), 알파(α) 및 애러티 (arity)로서 작성될 수 있다. 만약 람다(λ), 알파(α) 및 애러티가 주어진다면, 활성화 오브젝티브 스칼라 출 력이 계산될 수 있는데, 이는 표현도 및 그래디언트 품질의 상대적인 수를 나타낸다. 애러티는 논리적 뉴런 설 계를 위해 미리 특정될 수 있고, 논리적 뉴런에 의해 사용자가 지원하기를 원하는 최대 표현도 람다(λ)도 또한 특정될 수 있다. 람다(λ) 및 애러티가 주어질 때, 이는 활성화 오브젝티브를 최대화하기 위해 선택될 수 있는 종속 파라미터로서 참의 스레쉬홀드 알파(α)를 남기는데, 이러한 선택은 자동적으로 결정될 수 있다. 또 다른 측면에서, 사용자 인터페이스(예컨대, 도 9에 보여지는 사용자 인터페이스)와 같은 인터페이스 가 제공될 수 있는데, 이러한 사용자 인터페이스는, 예컨대 주어진 애러티에 대해 람다 대 알파의 2차원(2-D) 플롯을 제공하거나 시각화할 수 있으며, 여기서 활성화 오브젝티브를 최대화하는 람다와 알파의 결합이 선택될 수 있다. 예를 들어, 도 9는 활성화 오브젝티브 스칼라 출력 값들의 윤곽 플롯을 보여주는데, 상기 활성화 오브 젝티브 스칼라 출력값들은 축들과 단일 애러티 선택을 통해 특정된 입력 트리플렛(애러티, 람다, 알파)에 대해 계산된 것이다. 애러티에 대해 더 높은 값들을 선택하도록 하기 위해 유사한 플롯들이 생성될 수 있다. 그래픽 사용자 인터페이스는 이러한 플롯들을 시각화할 수 있고, 그래서 사용자에게 활성화 오브젝티브를 최대화하는 람다 및 알파의 조합을 선택할 수 있게 허용한다. 도 3은 가중된 실수-값 논리의 기능 및 러닝을 최적화하도록 할 수 있는 일 실시예에서의 시스템의 컴포 넌트들을 보여주는 다이어그램이다. 중앙처리장치(CPU), 그래픽 처리 유닛(graphic process unit, GPU), 및/또 는 필드 프로그램가능 게이트 어레이(Field Programmable Gate Array, FPGA), 주문형 집적회로(application specific integrated circuit, ASIC), 및/또는 또 다른 프로세서와 같은 하나 또는 그 이상의 하드웨어 프로세 서들은 메모리 디바이스와 결합될 수 있다. 메모리 디바이스는 랜덤 억세스 메모리(random access memory, RAM), 읽기 전용 메모리(read-only memory, ROM) 또는 또 다른 메모리 디바이스를 포함할 수 있고, 여기에서 설명되는 방법들 및/또는 시스템들과 연관된 다양한 기능들을 구현하기 위한 데이터 및/또는 프 로세서 명령들을 저장할 수 있다. 하나 또는 그 이상의 프로세서들은 메모리에 저장되거나 또 다른 컴퓨터 디바이스 또는 매체로부터 수신된 컴퓨터 명령들을 실행할 수 있다. 메모리 디바이스는, 예를 들어, 하나 또는 그 이상의 하드웨어 프로세서들의 기능을 제공하기 위한 명령들 및/또는 데이터를 저장할 수 있고, 운영체제 및 다른 명령들의 프로그램 및/또는 데이터를 포함할 수 있다. 예를 들어, 적어도 하나의 하 드웨어 프로세서는, 주어진 최대 표현도(예컨대, 최대 입력 가중치와 최소 입력 가중치 사이의 비에 의해 정의되는) 및 연산자 크기 또는 애러티에 대해 활성화 함수의 함수 그래디언트들을 최적화하기 위해, 가중 실수 -값 논리 연산자들 및/또는 논리적 뉴런들(예컨대, 가중 오프셋 루카시에비치 실수-값 강 논리곱 또는 논리합 연산자들)의 참의 스레쉬홀드(알파), 활성화 함수 및 논리적 제약조건들(예컨대, 컨벡스 최적화 선형 프로그 램)을 결정할 수 있다. 일 측면에서, 입력 데이터는 스토리지 디바이스에 저장되거나 네트워크 인터페이스 를 통해 원격 디바이스로부터 수신될 수 있으며, 가중 실수-값 논리의 기능 및 러닝을 최적화하기 위해 메 모리 디바이스 내로 일시적으로 로드될 수 있다. 러닝된 가중 실수-값 논리는, 예를 들어, 예측 또는 추론 을 수행하기 위해, 메모리 디바이스 상에 저장될 수 있다. 하나 또는 그 이상의 하드웨어 프로세서들(30 2)은, 예를 들어, 네트워크를 통해 원격 시스템들과 통신하기 위한 네트워크 인터페이스, 및 키보드, 마우 스, 디스플레이 등과 같은 입력 및/또는 출력 디바이스들과 통신하기 위한 입력/출력 인터페이스와 결합될 수 있다. 도 4는 일 실시예에서의 방법을 나타내는 플로우 다이어그램이다. 하나 또는 그 이상의 프로세서들, 예 컨대, 하드웨어 프로세서들은 여기에 설명되는 방법을 실행시킬 수 있다. 방법은 논리적 뉴런들의 신경망에서 구현되는 가중 실수-값 논리를 러닝(learning)할 수 있다. 단계에서, 방법은 가중 실수-값 논리 게이트를 구현하는 신경망의 뉴런에 대해 최대 입력 가중치와 최소 입력 가중치 사이의 비를 나타내는 최대 표현도를 수 신하는 단계를 포함할 수 있다. 단계에서, 방법은 뉴런과 연관된 연산자 애러티를 수신하는 단계를 포함할 수 있다. 단계에서, 방법은, 활성화를 위해, 뉴런, 참의 스레쉬홀드, 및 뉴런 스레시홀드에 대한 입력들과 연관된 가중치들로서 가중 실수-값 논리 게이트와 연관된 논리적 제약조건들을 정의하는 단계를 포함할 수 있다. 단계에서, 방법은 뉴런의 활성화 함수에 사용된 파라미터로서 참의 스레쉬홀드를 결정하는 단계를 포함할 수 있다. 참의 스레쉬홀드는 논리적 제약조건들에 기초하여 공식화된 활성화 최적화를 해결함으로써 결 정될 수 있다. 활성화 최적화는, 연산자 애러티와 최대 표현도가 주어질 때, 뉴런에 대한 입력 가중치들의 분포 폭을 나타내는 표현도와 그 뉴런에 대한 그래디언트 품질의 곱을 최대화한다. 일 실시예에 있어서, 그래디언트 품질은 상대적 스칼라 척도(relative scalar measure)이며, 비-평균 플랭크들 그래디언트의 텀들로 표현될 수 있다. 비-평균 플랭크들 그래디언트는 위에서 설명된 것과 같이 결정될 수 있다. 단계에서, 방법은 뉴런에 서 활성화 함수 - 활성화 함수는 결정된 참의 스레쉬홀드를 사용함 - 를 사용하여 논리적 뉴런들의 신경망을 훈 련시키는 단계를 포함할 수 있다. 일 실시예에 있어서, 활성화 함수는 시그모이드 함수(a sigmoid function)일 수 있다. 또 다른 실시예에 있어서, 활성화 함수는 3-피쓰 누설 정류 선형 유닛(ReLU)(a 3-piece leaky rectified linear unit (ReLU))일 수 있다. 다른 활성화 함수들이 사용되거나 선택될 수 있다. 일 실시예에 있어서, 방법은, 정의된 논리적 제약조건들, 뉴런 스레쉬홀드에 관한 하한, 뉴런 스레쉬홀 드에 관한 상한, 뉴런의 연산 범위의 최소점을 나타내는 뉴런의 입력에 관한 하한, 최소 입력 가중치에 관한 하 한을 사용하여, 표현도, 연산자 애러티 및 참의 스레쉬홀드, 뉴런의 활성화 함수의 연산 범위의 폭을 나타내는 논리적 대역폭, 및 표현도에 관한 상한을 정의 또는 유도하는 단계를 포함할 수 있다. 활성화 최적화는 뉴런 스 레쉬홀드에 관한 하한, 뉴런 스레쉬홀드에 관한 상한, 뉴런의 입력에 관한 하한, 최소 입력 가중치에 관한 하한, 논리적 대역폭, 및 표현도에 관한 상한에 기초하여 공식화될 수 있다. 활성화 최적화는 표현도, 참의 스 레쉬홀드, 뉴런의 입력에 관한 하한, 및 뉴런 활성화에 예상되는 최대 입력의 상한의 텀들로 유도될 수 있다. 방법은 또한 구분 선형 함수(piecewise linear function)를 형성함으로써 논리적 연산 범위에 걸쳐 활성 화 함수의 그래디언트들을 최적화하기 위해 참의 스레쉬홀드 값에 대한 활성화 함수를 계속적으로 업데이트하는 단계를 포함할 수 있다. 구분 선형 함수는 최소 연산 범위에서, 거짓의 스레쉬홀드 입력에 대해서는 거짓의 스 레쉬홀드에서, 참의 스레쉬홀드 입력에 대해서는 참의 스레쉬홀드에서 거짓이고, 최대 연산 범위에서 참이다. 방법은 또한 더 높은 표현도를 필요로 함이 없이 뉴런의 입력이 제거될 수 있도록 하기 위해 뉴런의 입 력과 연관된 슬랙 변수(slack variable)를 도입하는 단계를 포함할 수 있다. 일 실시예에서, 뉴런에 대한 입력 가중치들의 합은 단지 최소 가중치 및 표현도가 주어질 때에만 결정될 수 있다. 일 실시예에서, 참의 스레쉬홀 드는 현재의 표현도에 기초하여 업데이트될 수 있다. 일 실시예에서, 방법은 또한 가중 실수-값 논리 게이트의 특성들을 시각화하는 단계를 포함할 수 있다. 도 8은 하나의 실시예에서 시스템을 구현할 수 있는 컴퓨터 또는 처리 시스템의 일 예의 개념도를 나타 낸다. 컴퓨터 시스템은 단지 적절한 처리 시스템의 일 예이며 여기에 설명되는 방법론의 실시예들의 기능 또는 사용의 범위에 관한 어떤 제안을 제시하려고 의도된 것은 아니다. 보여지는 처리 시스템은 많은 다른 범용 또는 전용 컴퓨팅 시스템 환경들 또는 구성들과 함께 동작될 수 있다. 도 8에 보여진 처리 시스템과 함께 사용하기에 적합할 수 있는 잘 알려진 컴퓨팅 시스템들, 환경들 및/또는 구성들의 예들은, 개인용 컴퓨터 시스템들, 서버 컴퓨터 시스템들, 신 클리이언트들(thin clients), 시크 클라이언트들(thick clients), 핸드헬드 또는 랩탑 디 바이스들, 멀티프로세서 시스템들, 마이크로프로세서 기반 시스템들, 셋탑 박스들, 프로그램가능 소비자 전자제 품들(programmable consumer eletronics), 네트워크 PC들, 마이크로컴퓨터 시스템들, 메인프레임 컴퓨터 시스 템들, 및 분산된 클라우드 컴퓨팅 환경들 - 이는 상기 시스템들 또는 디바이스들 중 어떤 것을 포함할 수 있음 - 등이 될 수 있으나, 이러한 것들로 한정되는 것은 아니다. 컴퓨터 시스템은, 컴퓨터 시스템에 의해 실행가능한 프로그램 모듈들과 같은 컴퓨터 시스템 실행가능 명 령들의 일반적인 맥락에서 설명될 수 있다. 일반적으로, 프로그램 모듈들은 루틴들, 프로그램들, 객체들, 컴포 넌트들, 로직, 데이터 구조들 등을 포함할 수 있는데, 이들은 특정 태스크들을 수행하거나 특정의 추상적인 데 이터 유형들을 구현한다. 컴퓨터 시스템은 분산된 클라우드 컴퓨팅 환경들에서 실행될 수 있다. 분산된 클라우 드 컴퓨팅 환경들에서는, 통신 네트워크를 통해 연결되어 있는 원격 처리 디바이스들에 의해 태스크들이 수행된 다. 분산된 클라우드 컴퓨팅 환경에서, 프로그램 모듈들은 메모리 스토리지 디바이스들을 포함하는 로컬 컴퓨터 시스템 스토리지 매체와 원격 컴퓨터 시스템 스토리지 매체 둘 모두일 수 있다."}
{"patent_id": "10-2022-7036291", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 22, "content": "[130] 컴퓨터 시스템의 컴포넌트들은, 하나 또는 그 이상의 프로세서들 또는 처리 유닛들, 시스템 메모리 , 버스를 포함할 수 있으나 이러한 것으로 한정되는 것은 아니다. 버스는 시스템 메모리를 포 함하는 여러 가지 시스템 컴포넌트들을 프로세서에 결합한다. 프로세서는 여기에 설명된 방법들을 수행 하는 모듈을 포함할 수 있다. 모듈은 프로세서의 집적회로들 내에 프로그램될 수도 있고, 또는 메 모리, 스토리지 디바이스, 또는 네트워크 또는 이것들의 조합들로부터 로딩될 수 있다. 버스는 버스 구조들의 몇 가지 유형들 중 어떤 것의 하나 또는 그 이상을 나타낼 수 있다. 버스 구 조들은 메모리 버스 또는 메모리 컨트롤러, 주변 버스, 가속화 그래픽스 포트(accelerated graphics port), 및 다양한 버스 아키텍쳐들 중 어떤 것을 사용하는 로컬 버스를 포함한다. 예로서(이러한 예로 한정되는 것은 아님), 이러한 아키텍쳐들은 산업 표준 아키텍쳐(Industry Standard Architecture, ISA) 버스, 마이크로 채널 아키텍쳐(Micro Channel Architecture, MCA), 버스, 향상된(Enhanced) ISA(EISA) 버스, 비디오 전자공학 표준 협회(Video Electronics Standards Association VESA) 로컬 버스, 및 주변 컴포넌트 인터커넥츠(Peripheral Component Interconnects, PCI) 버스를 포함한다. 컴퓨터 시스템은 다양한 컴퓨터 시스템 판독가능 매체를 포함할 수 있다. 이러한 매체는 컴퓨터 시스템 에 의해 접근가능한 어떤 가용 매체일 수 있으며, 이는 휘발성 매체와 비휘발성 매체를 포함할 수 있고, 착탈식 매체와 비착탈식 매체를 포함할 수 있다. 시스템 메모리는 랜덤 억세스 메모리(RAM), 및/또는 캐시 메모리 또는 그 밖의 것들과 같은 휘발성 메모리의 형태로 된 컴퓨터 시스템 판독가능 매체를 포함할 수 있다. 컴퓨터 시스템은 다른 착탈식/비착탈식, 휘발성/비휘발성 컴퓨터 시스템 스토리지 매체를 더 포함할 수 있다. 단지 예로서, 비착탈식, 비휘발성 자기 매 체(예컨대,\"하드 드라이브\")로부터 리드하고 비착탈식, 비휘발성 자기 매체(예컨대,\"하드 드라이브\")에 라이트 하기 위해 스토리지 시스템이 제공될 수 있다. 비록 보여지지는 않았으나, 착탈식, 비휘발성 자기 디스크로 부터 리드하고 이들에 대해 라이트하기 위한 자기 디스크 드라이브(예컨대, \"플로피 디스크\"), 및 CD-ROM, DVD- ROM 또는 다른 광학 매체와 같은 착탈식, 비휘발성 광 디스크로부터 리드하거나 이들에 대해 라이트하기 위한 광 디스크 드라이브가 제공될 수 있다. 이러한 경우들에 있어서, 각각은 하나 또는 그 이상의 데이터 매체 인터페이스들에 의해 버스에 연결될 수 있다. 컴퓨터 시스템은 또한 키보드, 포인팅 디바이스, 디스플레이 등과 같은 하나 또는 그 이상의 외부 디바이스들과 통신할 수 있다. 여기서, 하나 또는 그 이상의 디바이스들은 사용자들에게 컴퓨터 시스템과 상호작용하도록 할 수 있으며, 및/또는 어떤 디바이스들(예컨대, 네트워크 카드, 모뎀 등)은 컴퓨터 시스템에게 하나 또는 그 이상의 다른 컴퓨팅 디바이스들과 통신하도록 할 수 있다. 이러한 통신은 입력/출력(I/O) 인터페 이스들을 통해서 일어날 수 있다. 계속해서, 컴퓨터 시스템은 네트워크 어댑터를 통해 근거리 통신망(LAN), 일반적인 광역 통신망 (WAN), 및/또는 공중망(예컨대, 인터넷)과 같은 하나 또는 그 이상의 네트워크들과 통신할 수 있다. 도시된 바와 같이, 네트워크 어댑터는 버스를 통해 컴퓨터 시스템의 다른 컴포넌트들과 통신한다. 비록 보여지 지는 않았으나, 다른 하드웨어 및/또는 소프트웨어 컴포넌트들이 컴퓨터 시스템과 함께 사용될 수 있다는 것이 이해되어야 한다. 예들은 마이크로 코드, 디바이스 드라이버들, 리던던트 처리 유닛들, 외부 디스크 드라이브 어레이들, RAID 시스템들, 테이프 드라이브들, 및 데이터 보존 스토리지 시스템들(data archival storage systems) 등을 포함하나, 이러한 예들로 한정되는 것은 아니다. 본 발명은 가능한 어떤 기술적 상세 수준의 통합에서 시스템, 방법, 및/또는 컴퓨터 프로그램 제품일 수 있다. 컴퓨터 프로그램 제품은 컴퓨터 판독가능 프로그램 매체(매체들)를 포함할 수 있으며, 컴퓨터 판독가능 프로그램 매체(매체들)는 프로세서로 하여금 본 발명의 측면들을 수행하도록 하기 위해 그것에 대한 컴퓨터 판 독가능 프로그램 명령들을 갖는다. 컴퓨터 판독가능 스토리지 매체는 명령 실행 디바이스에 의해 사용하기 위한 명령들을 보유 및 저장할 수 있는 유형적 디바이스일 수 있다. 컴퓨터 판독가능 스토리지 매체는, 예를 들어, 전자 스토리지 디바이스, 자기 스토리지 디바이스, 광 스토리지 디바이스, 전자기 스토리지 디바이스, 반도체 스토리지 디바이스, 또는 이것들으 어떤 적절한 조합일 수 있으나, 이러한 예들로 한정되는 것은 아니다. 컴퓨터 판독가능 스토리지 매체 의 더 구체적인 예들의 리스트(빠짐없이 모두 열거한 것은 아님)는 다음을 포함할 수 있다. 휴대용 컴퓨터 디스 켓, 하드 디스크, 랜덤 억세스 메모리(RAM), 읽기 전용 메모리(ROM), 소거 프로그램가능 읽기 전용 메모리 (EPROM 또는 플래쉬 메모리), 스태틱 랜덤 억세스 메모리(SRAM), 휴대용 컴팩트 디스크 읽기 전용 메모리(CD- ROM), 디지털 다용도 디스크(digital versatile disk, DVD), 메모리 스틱, 플로피 디스크, 펀치-카드들 또는 그 안에 기록된 명령들을 갖는 홈에서 볼록한 구조들과 같은 기계적으로 인코딩된 디바이스, 및 앞서 열거한 것 들의 어떤 적절한 조합을 포함할 수 있다. 여기서 사용되는 것과 같이, 컴퓨터 판독가능 스토리지 매체는, 무선 전파(radio waves) 또는 다른 자유롭게 전파되는 전자기파, 도파관 또는 다른 전송 매질을 통해 전파되는 전자 기파(예컨대, 광섬유 케이블을 통과하는 광 펄스), 또는 와이어를 통해 전송되는 전기적 신호와 같은 일시적인 신호들 그 자체로 구성되는 것은 아니다. 여기에 설명되는 컴퓨터 판독가능 프로그램 명령들은, 예를 들어, 인터넷, 근거리 통신망, 광역 통신망 및/또는 무선 통신망과 같은 통신망을 통해 컴퓨터 판독가능 스토리지 매체로부터 각각의 컴퓨팅/처리 디바이스 들로, 또는 외부 컴퓨터 또는 외부 스토리지 디바이스로 다운로드될 수 있다. 통신망은 동선 전송 케이블, 광 전송 섬유, 무선 전송, 라우터, 방화벽, 스위치, 게이트웨이 컴퓨터 및/또는 엣지 서버를 포함할 수 있다. 각각 의 컴퓨팅/처리 디바이스에서 네트워크 어댑터 카드 또는 네트워크 인터페이스는, 각각의 컴퓨팅/처리 디바이스 내 컴퓨터 판독가능 스토리지 매체에 저장하기 위해 통신망으로부터 컴퓨터 판독가능 프로그램 명령들을 수신하 고 컴퓨터 판독가능 프로그램 명령들을 전송한다. 본 발명의 동작들을 수행하기 위한 컴퓨터 판독가능 프로그램 명령들은 어셈블러 명령(assembler instructions), 명령-세트-아키텍쳐(instruction-set-architecture, ISA) 명령, 머신 명령(machine instructions), 머신 종속 명령(machine dependent instructions), 마이크로코드, 펌웨어 명령(firmware instructions), 상태-세팅 데이터(state-setting data), 집적된 회로부를 위한 구성 데이터, 또는 하나 또는 그 이상의 프로그래밍 언어들의 어떤 조합으로 작성된 소스 코드 또는 객체 코드(object code)일 수 있다. 상기 프로그래밍 언어들은 스몰토크(Smalltalk), C++ 등과 같은 객체 지향 프로그래밍 언어(object oriented programming language), 및 \"C\" 프로그래밍 언어 또는 유사 프로그래밍 언어들과 같은 절차형 프로그래밍 언어 들을 포함한다. 컴퓨터 판독가능 프로그램 명령들은 독립형(stand-alone) 소프트웨어 패키지로서 사용자의 컴퓨 터 상에서 전적으로, 사용자의 컴퓨터 상에서 부분적으로 실행될 수 있고, 사용자의 컴퓨터 상에서 부분적으로 그리고 원격 컴퓨터 상에서 부분적으로 실행될 수 있고, 또는 원격 컴퓨터 또는 서버 상에서 전적으로 실행될 수 있다. 후자의 시나리오에 있어서, 원격 컴퓨터는 근거리 통신망(LAN) 또는 광역 통신망(WAN)을 포함하는 어떤 유형의 통신망을 통해 사용자의 컴퓨터에 연결될 수 있고, 또는 그 연결은 외부 컴퓨터에 대해 이뤄질 수 있 다(예컨대, 인터넷 서비스 공급자(Internet Service Provider)를 사용하여 인터넷을 통해서). 몇몇 실시예들에 있어서, 본 발명의 측면들을 수행하기 위해, 예를 들어, 프로그램가능 로직 회로부(programmable logic circuitry), 필드-프로그램가능 게이트 어레이(field-programmable gate arrays, FPGA), 또는 프로그램가능 로 직 어레이(Programmable logic arrays, PLA)를 포함하는 전자 회로부는 전자 회로부를 개인에게 맞추기 위해 컴 퓨터 판독가능 프로그램 명령들의 상태 정보를 이용함으로써 컴퓨터 판독가능 프로그램 명령들을 실행할 수 있 다. 본 발명의 측면들은 여기서 발명의 실시예들에 따른 방법들, 장치들(시스템들), 및 컴퓨터 프로그램 제 품들의 플로우챠트 설명들 및/또는 블록 다이어그램들을 참조하여 설명된다. 플로우챠트 설명들 및/또는 블록 다이어그램들에서 플로우챠트 설명들 및/또는 블록 다이어그램들의 각각의 블록, 및 블록들의 조합들은 컴퓨터 판독가능 프로그램 명령들에 의해 구현될 수 있다는 것이 이해될 것이다. 이들 컴퓨터 판독가능 프로그램 명령들은 머신을 생성하기 위해 컴퓨터, 또는 다른 프로그램가능 데이터 처리 장치의 프로세서로 제공될 수 있으며, 그리하여 컴퓨터 또는 다른 프로그램가능 데이터 처리 장치의 프로 세서를 통해 실행되어 플로우챠트 및/또는 블록 다이어그램 블록 또는 블록들에 명시된 기능들/동작들을 구현하 기 위한 수단을 생성하도록 한다. 이들 컴퓨터 판독가능 프로그램 명령들은 또한 컴퓨터 판독가능 스토리지 매 체에 저장될 수 있으며, 컴퓨터 판독가능 스토리지 매체는 컴퓨터, 프로그램가능 데이터 처리 장치, 및/또는 다 른 디바이스들이 특정 방식으로 기능하도록 지시할 수 있다. 그리하여, 내부에 저장된 명령들을 갖는 컴퓨터 판 독가능 스토리지 매체가 플로우챠트 및/또는 블록 다이어그램의 블록 또는 블록들에 명시된 기능/동작의 측면들 을 구현하는 명령들을 포함하는 제조 물품을 포함하도록 한다. 컴퓨터 판독가능 프로그램 명령들은 또한 컴퓨터, 다른 프로그램가능 데이터 처리 장치, 또는 다른 다바 이스 상으로 로딩되어 일련의 동작 단계들이 컴퓨터, 다른 프로그램가능 장치 또는 다른 디바이스 상에서 수행 되도록 하여 컴퓨터 구현 프로세스를 생성하게 할 수 있다. 그리하여 컴퓨터, 다른 프로그램가능 장치, 또는 다 른 디바이스 상에서 실행되는 명령들이 플로우챠트 및/또는 블록 다이어그램의 블록 또는 블록들에 명시된 기능 들/동작들을 구현하도록 한다. 도면들에서 플로우챠트 및 블록 다이어그램들은 본 발명의 다양한 실시예들에 따른 시스템들, 방법들, 및 컴퓨터 프로그램 제품들의 가능한 구현의 아키텍쳐, 기능, 및 동작을 보여준다. 이 점에서, 플로우챠트 또는 블록 다이어그램들에서의 각각의 블록은 모듈, 세그먼트, 또는 명령들의 일부분을 나타낼 수 있으며, 이는 명시 된 논리적 기능(들)을 구현하기 위한 하나 또는 그 이상의 실행가능한 명령들을 포함한다. 몇몇 다른 구현들에 있어서, 블록들에 기재되어 있는 기능들은 도면들에 기재되어 있는 순서를 벗어나서 일어날 수도 있다. 예를 들 어, 연속해서 보여지는 두 개의 블록들은, 사실상, 하나의 단계로 수행될 수 있고, 동시에 수행될 수 있고, 실 질적으로 동시에 수행될 수 있고, 부분적으로 또는 전체적으로 임시로 오버랩 방식으로 수행될 수도 있으며, 또 는 블록들은 때로는 관련된 기능에 의존하여 역순으로 실행될 수도 있다. 또한, 블록 다이어그램들 및/또는 플 로우챠트 그림의 각각의 블록, 그리고 블록 다이어그램들 및/또는 플로우챠트 그림에서의 블록들의 조합들은 명 시된 기능들 또는 동작들을 수행하거나 전용 하드웨어와 컴퓨터 명령들의 조합들을 수행하는 전용 하드웨어 기 반 시스템들에 의해 구현될 수 있다는 것이 주목될 것이다. 여기에 사용되는 용어는 단지 구체적인 실시예들을 설명하기 위한 목적이지 발명의 범위를 한정하려고 의도된 것은 아니다. 여기에서 사용된 바와 같이, 단수 형태의 \"하나의\" 및 \"상기\"는, 만약 그 맥락에서 명확하 게 그렇지 않은 것으로 나타내지 않는다면, 복수의 형태들도 또한 포함하는 것으로 의도된다. 여기에서 사용되 는 바와 같이, \"또는\" 이라는 용어는 포괄적인 용어이며, 만약 그 맥락에서 분명하게 또는 명백히 그렇지 않은 것으로 나타내지 않는다면, \"및/또는\"을 의미할 수 있다. \"포함하다\", \"포함하는\", \"갖는\" 이라는 용어들은, 여 기서 사용될 때, 언급되는 특징들, 정수들, 단계들, 동작들, 구성요소들, 및/또는 컴포넌트들을 특정할 수 있지 만, 하나 또는 그 이상의 다른 특징들, 정수들, 단계들, 동작들, 구성요소들, 컴포넌트들, 및/또는 그것들의 그 룹들의 존재 또는 부가를 배제하는 것은 아니라는 것이 더 이해될 것이다. 여기에서 사용되는 바와 같이, \"일 실시예에서\" 라는 문구는, 비록 그것이 동일한 실시예를 지칭할 수도 있으나, 반드시 동일한 실시예를 지칭하는 것은 아니다. 여기에서 사용된느 바와 같이, \"하나의 실시예에서\" 라는 문구는, 비록 그것이 동일한 실시예를 지칭할 수도 있으나, 반드시 동일한 실시예를 지칭하는 것은 아니다. 여기에서 사용되는 바와 같이, \"다른 실시 예에서\" 라는 문구는, 비록 그것이다른 실시예를 지칭할 수도 있으나, 반드시 다른 실시예를 지칭하는 것은 아 니다. 나아가, 실시예들 및/또는 실시예들의 컴포넌트들은, 만약 그것들이 서로 배타적이 아니라면, 서로 자유롭게 결합될 수 있다. 이하의 청구항들에서 모든 수단들 또는 단계 플러스 기능 구성요소들의 해당 구조들, 재료들, 동작들, 및 균등물들은, 만약 있다면, 명시적으로 청구되는 다른 청구되는 구성요소들과 함께 그 기능을 수행하기 위한 어떤 구조, 재료, 또는 동작을 포함하는 것으로 의도된다. 본 발명의 설명은 예시 또는 설명 목적으로 제시되었 으나, 개시된 형태로 발명의 실시예들을 빠짐없이 망라하거나 발명의 범위를 한정하려고 의도된 것은 아니다. 본 발명의 범위를 벗어남이 없이 많은 수정 예들 및 변형 예들이 있을 수 있다는 것은 통상의 기술자들에게는 자명할 것이다. 실시예들은 발명의 원리들 및 실제 적용을 가장 잘 설명하기 위해, 그리고 통상의 기술자들에게 고려된 특정 용도에 적합하게 여러가지 변형 예들을 갖는 다양한 실시예들에 대해 본 발명을 이해할 수 있도록 하기 위해 선택되어 설명되었다."}
{"patent_id": "10-2022-7036291", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에서 뉴런들의 망(네트워크)에서 하나의 뉴런을 나타낸다. 도 2A 및 도 2B는 일 실시예에서 논리적 뉴런들의 네트워크의 일 예를 보여준다. 도 3은 가중 실수-값 논리(weighted real-value logic)의 기능(capacity) 및 러닝(learning)을 최적화 하기 위해 제공할 수 있는 일 실시예에서의 시스템의 컴포넌트들을 보여주는 다이어그램이다. 도 4는 일 실시예에서의 방법을 나타내는 플로우 다이어그램이다. 도 5는 일 실시예에서 참의 스레쉬홀드의 일 예를 보여주는 다이어그램이다. 도 6은 일 실시예에서 α-보존 시그모이드 활성화 함수(α- preserving sigmoid activation function) 를 나타낸다. 도 7은 일 실시예에서, 가중치 분포(weight distribution)를 보여준다. 도 8은 일 실시예에 따른 시스템을 구현할 수 있는 컴퓨터 또는 처리 시스템의 일 예의 도식을 나타낸다. 도 9는 일 실시예에서 사용자 인터페이스 시각화(user interface visualization)의 일 예를 보여준다."}
