{"patent_id": "10-2022-7008530", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0116423", "출원번호": "10-2022-7008530", "발명의 명칭": "자연스러운 데이스케일 타임랩스 생성 방법 및 컴퓨팅 장치", "출원인": "삼성전자주식회사", "발명자": "스터킨 글렙 믹하일로비치"}}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "학습된 생성적 신경망 및 학습된 머징 신경망을 이용하여 컨텐츠 이미지를 바탕으로 자연스러운 데이스케일 타임랩스 시퀀스의 하나 또는 그 이상의 이미지를 생성하는 방법에 있어서,상기 컨텐츠 이미지 및 (a) 상기 컨텐츠 이미지에 적용될 미리 정의된 하나 또는 그 이상의 스타일 또는 (b) 상기 컨텐츠 이미지에 적용될 하나 또는 그 이상의 스타일을 갖는 하나 또는 그 이상의 스타일 이미지를 수신하는단계;상기 컨텐츠 이미지를 n 개의 이미지 크롭으로 자르는 단계; 상기 하나 또는 그 이상의 스타일 각각에 따라 재-스타일화된 n 개의 이미지 크롭을 획득하기 위해, 상기 학습된 생성적 신경망을 상기 하나 또는 그 이상의 스타일 각각을 이용하여 상기 n 개의 이미지 크롭에 적용하는 단계; 및상기 컨텐츠 이미지에 대한 자연스러운 데이스케일 타임랩스 시퀀스의 하나 또는 그 이상의 이미지를 획득하기위해, 상기 하나 또는 그 이상의 스타일 각각에 대한 재-스타일화된 n 개의 이미지 크롭을 상기 학습된 머징 신경망을 이용하여 병합하는 단계;를 포함하는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 생성적 신경망은, 다음의 모드들: 스왑 모드, 랜덤 모드 및 오토인코더 모드 중 하나 또는 이들의 조합으로 학습되는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 생성적 신경망은, 컨텐츠 인코더, 스타일 인코더 및 디코더를 포함하는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 생성적 신경망은, 다수의 이터레이션에서 반복적으로 수행되는 다음의 단계들을 사용하여 스왑 모드에서학습되며:제1 이미지에서 컨텐츠의 공간적으로 더 작은 레프레센테이션인 컨텐츠 텐서를 획득하기 위해, 상기 제1 이미지를 학습 예로서 컨텐츠 인코더에 입력하는 단계;제2 이미지에서 스타일을 나타내는 스타일 벡터를 획득하기 위해, 상기 제2 이미지를 학습 예로서 스타일 인코더에 입력하는 단계;적응적 인스턴스 정규화를 이용하여 상기 제1 이미지의 컨텐츠와 상기 제2 이미지의 스타일을 갖는 제3 이미지및 상기 제1 이미지에 대응되는 세그먼테이션 마스크를 획득하기 위해, 상기 컨텐츠 텐서 및 상기 스타일 벡터를 디코더에 입력하는 단계;이미지가 주어진 스타일 하에서 충분히 자연스러운지 판단하도록 구성된 조건부 판별기에 상기 제2 이미지와 상기 스타일 벡터 또는 상기 스타일 벡터와 상기 제3 이미지를 교대로 입력하는 단계; 및적대적 방식으로 상기 생성적 신경망 및 상기 조건부 판별기의 학습을 보장하기 위해, 상기 조건부 판별기에 의한 판단 결과를 바탕으로 서로 다른 이터레이션에서 상기 생성적 신경망 및 상기 조건부 판별기의 파라미터를교대로 업데이트하는 단계,공개특허 10-2022-0116423-3-상기 생성적 신경망의 파라미터를 업데이트할 때, 상기 세그먼테이션 마스크는 추가로 고려되고,상기 제1 이미지 및 제2 이미지는 제1 해상도를 갖는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 생성적 신경망은, 다수의 이터레이션에서 반복적으로 수행되는 다음의 단계들을 사용하여 랜덤 모드에서학습되며:제1 이미지에서 컨텐츠의 공간적으로 더 작은 레프레센테이션인 컨텐츠 텐서를 획득하기 위해, 상기 제1 이미지를 학습 예로서 컨텐츠 인코더에 입력하는 단계;사전 분포로부터 랜덤 스타일 벡터를 생성하는 단계;적응적 인스턴스 정규화를 이용하여 상기 제1 이미지의 컨텐츠와 상기 랜덤 스타일 벡터에 의해 정의되는 스타일을 갖는 제3 이미지 및 상기 제1 이미지에 대응되는 세그먼테이션 마스크를 획득하기 위해, 상기 컨텐츠 텐서및 상기 스타일 벡터를 디코더에 입력하는 단계;이미지가 주어진 스타일 하에서 충분히 자연스러운지 판단하도록 구성된 조건부 판별기에 상기 랜덤 스타일 벡터 및 상기 제3 이미지를 입력하는 단계; 및적대적 방식으로 상기 생성적 신경망 및 상기 조건부 판별기의 학습을 보장하기 위해, 상기 조건부 판별기에 의한 판단 결과를 바탕으로 상기 생성적 신경망의 파라미터를 교대로 업데이트하는 단계,상기 생성적 신경망의 파라미터를 업데이트할 때, 상기 세그먼테이션 마스크가 추가로 고려되는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 생성적 신경망은, 다수의 이터레이션에서 반복적으로 수행되는 다음의 단계들을 사용하여 오토인코더 모드에서 학습되며:제1 이미지에서 컨텐츠의 공간적으로 더 작은 레프레센테이션인 컨텐츠 텐서를 획득하기 위해, 상기 제1 이미지를 학습 예로서 컨텐츠 인코더에 입력하는 단계;상기 제1 이미지의 스타일을 나타내는 스타일 벡터를 획득하기 위해, 상기 제1 이미지를 학습 예로서 스타일 인코더에 입력하는 단계;적응적 인스턴스 정규화를 이용하여 상기 제1 이미지의 컨텐츠와 스타일을 갖는 제3 이미지 및 상기 제1 이미지에 대응되는 세그먼테이션 마스크를 획득하기 위해, 상기 컨텐츠 텐서 및 상기 스타일 벡터를 디코더에 입력하는 단계;상기 제1 이미지 및 상기 제3 이미지를 비교하는 단계; 및상기 비교 결과에 기초하여 상기 생성적 신경망의 파라미터를 업데이트하는 단계,상기 생성적 신경망의 파라미터를 업데이트할 때, 상기 세그먼테이션 마스크는 추가로 고려되는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 생성된 신경망은,제1 이미지의 컨텐츠 텐서에 의해 전달되지 않은 고주파 특징을 디코더에 전달하도록 구성된 하나 또는 그 이상의 덴스 스킵 커넥션을 포함하는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,공개특허 10-2022-0116423-4-상기 생성적 신경망은,다수의 이터레이션에서 반복적으로 수행되는 다음의 단계들을 사용하여 학습되며:각각 제2 해상도를 갖는 학습 이미지들의 셋을 획득하는 단계;상기 학습 이미지들의 셋의 각 이미지를 오프셋 방향과 k 개의 픽셀(들)의 스트라이드에 의해 정의되는 미리 정의된 방식으로 n 개의 강하게 중첩되는 이미지 크롭으로 자르는 단계;상기 n 개의 이미지 크롭의 각 이미지 크롭을 제1 해상도로 다운샘플링하는 단계;각각이 학습된 생성적 신경망에 의해 생성된 변환된 이미지 크롭과 원본 학습 이미지의 대응하는 이미지 크롭간의 불일치 및 아티팩트를 캡쳐하는 n 개의 변환된 크롭을 획득하기 위해, 오토인코더 모드에서 학습된 생성적신경망을 n 개의 이미지 크롭의 각 이미지 크롭에 적용하는 단계; 및아티팩트와 불일치가 감소된 병합된 이미지를 획득하기 위해 n 개의 변환된 크롭을 상기 머징 신경망에 입력하고, 상기 병합된 이미지를 상기 학습 이미지들의 셋의 대응하는 원본 이미지와 비교하고, 상기 비교 결과에 기초하여 상기 머징 신경망의 파라미터를 업데이트하는 단계,상기 병합된 이미지는, 제2 해상도를 갖는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 하나 또는 그 이상의 스타일은, 상기 컨텐츠 이미지에 적용될 하루 중 하나 또는 그 이상의 시간에 각각대응하는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "프로세서 및 상기 프로세서에 의해 실행되면 상기 프로세서가 제1항의 방법을 수행하도록 하는 컴퓨터-실행 가능한 명령어들을 저장하는 메모리를 포함하는 컴퓨팅 장치."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "학습된 생성적 신경망을 이용하여 컨텐츠 이미지를 바탕으로 자연스러운 데이스케일 타임랩스 시퀀스의 하나 또는 그 이상의 이미지를 생성하는 방법에 있어서,상기 컨텐츠 이미지 및 (a) 상기 컨텐츠 이미지에 적용될 미리 정의된 하나 또는 그 이상의 스타일 또는 (b) 상기 컨텐츠 이미지에 적용될 하나 또는 그 이상의 스타일을 갖는 하나 또는 그 이상의 스타일 이미지를 수신하는단계;상기 컨텐츠 이미지의 가로세로 비율을 유지하면서 상기 컨텐츠 이미지의 더 작은 변 상에서 상기 컨텐츠 이미지의 해상도를 더 낮은 해상도로 축소시키는 단계; 상기 하나 또는 그 이상의 스타일 각각에 따라 재-스타일화된 하나 또는 그 이상의 축소된 컨텐츠 이미지를 획득하기 위해, 상기 학습된 생성적 신경망을 상기 하나 또는 그 이상의 스타일 각각을 이용하여 상기 축소된 컨텐츠 이미지에 적용하는 단계;상기 가로세로 비율을 유지하면서 상기 더 작은 변 상에서 더 낮은 해상도를 갖는 고주파 성분들 및 저주파 성분으로 상기 재-스타일화된 컨텐츠 이미지 각각을 분해하는 단계; 대응하는 재-스타일화된 컨텐츠 이미지의 컨텐츠를 고려하여 상기 저주파 성분을 필터링하는 단계; 및 대응하는 재-스타일화된 컨텐츠 이미지 각각의 필터링된 저주파 성분 및 고주파 성분들에 기초하여 상기 자연스러운 데이스케일 타임랩스 시퀀스의 하나 또는 그 이상의 이미지를 생성하는 단계;를 포함하는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 분해를 위해 라플라시안 피라미드가 이용되는 방법.공개특허 10-2022-0116423-5-청구항 13 제11항에 있어서,상기 필터링을 위해 가이디드 필터가 이용되고, 상기 대응하는 재-스타일화된 컨텐츠 이미지의 컨텐츠는 상기필터링을 위한 가이드로서 이용되는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 생성적 신경망은, 다음의 모드들: 스왑 모드, 랜덤 모드 및 오토인코더 모드 중 하나 또는 이들의 조합으로 학습되는 방법."}
{"patent_id": "10-2022-7008530", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 생성적 신경망은, 컨텐츠 인코더, 스타일 인코더 및 디코더를 포함하는 방법."}
{"patent_id": "10-2022-7008530", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 단일 이미지로부터 자연스러운 데이스케일 타임랩스 이미지(들) 생성 분야에 관한 것이다. 기술적 효 과는 자연스러운 타임랩스 시퀀스의 하나 또는 그 이상의 이미지를 생성하는 동안 스타일을 스타일 이미지에서 컨텐츠 이미지로 스왑할 때, 향상된 보존 디테일으로 구성된다. 이러한 효과를 위해 학습된 생성적 신경망 및 학 (뒷면에 계속)"}
{"patent_id": "10-2022-7008530", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 단일 이미지(single image)로부터 자연스러운(plausible) 데이스케일(dayscale) 타임랩스 영상 생성 분야에 관한 것으로, 특히, 자연스러운 데이스케일 타임랩스를 생성하기 위한 방법, 컴퓨터-구현 시스템, 컴퓨 팅 장치 및 컴퓨터-판독 가능 매체에 관한 것이다."}
{"patent_id": "10-2022-7008530", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "지난 몇 년 동안, 심층 신경망(deep neural networks)을 기반으로 하는 이미지-투-이미지 변환(image-to-image translation)의 문제는 두 개의 미리 정의된 페어된 도메인들(paired domains) 간의 변환에서 멀티플 (multiple) 도메인들 간의 변환을 위한 통합된 모델의 개발로 진화하였다. 이미지-투-이미지 변환에 대한 대부 분의 고전적인 접근 방식에서는 도메인 레이블(domain labels)이 필요하다. 최근의 FUNIT 모델은 이러한 제약을 완화하였다: 인퍼런스 시간(inference time)에 스타일(style)을 추출하기 위해, 타겟 도메인으로부터의 여러 이 미지들을 변환을 위한 지표(guidance)로 사용하지만(이를 few-shot setting이라 한다), 학습(training) 중에 도메인 레이블이 여전히 필요하였다. 종래 기술 해결책은 이미지-투-이미지 변환의 문제를 해결하기 위해, 페어 된 또는 도메인-레이블된 학습 이미지들을 항상 사용하였다."}
{"patent_id": "10-2022-7008530", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "이미지-투-이미지 변환 문제는 사용자-컴퓨팅 장치에서 사용자에 의해 촬영될 수 있는 단일 이미지로부터 시간 대(daytime) 타임랩스 영상을 생성하는 실질적인 태스크에 대해 개시된 본 발명에 의해 해결될 수 있다. 고해상 도(high resolution)의 다양한 데이스케일 타임랩스 이미지들 또는 영상의 데이터셋(dataset)을 획득하는 것이 고해상도의 다양한 이미지들의 데이터셋을 획득하는 것보다 훨씬 어렵기 때문에, 본 명세서에 개시된 본 발명은 이미지-투-이미지 변환 접근 방식을 기초로 한다. 정의하기 어렵고 사용자로부터 얻기 어려운 도메인 어노테이 션들(annotations)을 수집하는 대신, 명시적 도메인 지도(explicit domain supervision) 없이 데이터의 암시적 도메인 구조(implicit domain structure)를 밝히는 방법이 제안된다."}
{"patent_id": "10-2022-7008530", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 명세서에 개시된 본 발명에서 사용된 AI(Artificial Intelligence)-시스템은 도메인 레이블 없이 비정렬된 (unaligned) 이미지들의 많은 데이터셋에 대해 학습된다. 본 발명의 실시 예에서 사용된 유일한 외부(external) (약(weak)) 지도는 오프-더-쉘프 시맨틱 세그먼테이션 네트워크(off-the-shelf semantic segmentation network)를 사용하여 추정될 수 있는 코어스 세그먼테이션 맵(coarse segmentation maps)이다. 본 개시의 제1 실시 예에 따르면, 학습된 생성적 신경망(trained generative neural network) 및 학습된 머징 신경망(trained merging neural network)을 이용하여 컨텐츠 이미지를 바탕으로 자연스러운 데이스케일 타임랩 스 시퀀스의 하나 또는 그 이상의 이미지를 생성하는 방법이 제공된다. 방법은 컨텐츠 이미지 및 (a) 컨텐츠 이 미지에 적용될 하루 중 하나 또는 그 이상의 시간에 각각 대응되는 하나 또는 그 이상의 미리 정의된 스타일 또 는 (b) 컨텐츠 이미지에 적용될 하나 또는 그 이상의 스타일을 갖는 하나 또는 그 이상의 스타일 이미지를 수신 하는 단계; 컨텐츠 이미지를 n 개의 이미지 크롭(crops)으로 자르는(slicing) 단계; 하나 또는 그 이상의 스타 일 각각에 따라 재-스타일화된(re-stylized) n 개의 이미지 크롭을 획득하기 위해, 학습된 생성적 신경망을 하 나 또는 그 이상의 스타일 각각을 이용하여 n 개의 이미지 크롭에 적용하는 단계; 및 컨텐츠 이미지에 대한 자 연스러운 데이스케일 타임랩스 시퀀스의 하나 또는 이상의 이미지를 획득하기 위해, 학습된 머징 신경망을 이용 하여 하나 또는 그 이상의 스타일 각각에 대한 재-스타일화된 n 개의 이미지 크롭을 병합하는(merging) 단계를 포함한다: 본 개시의 제1 실시 예에 따른 방법에서 사용된 생성적 신경망은 다음의 모드들(following modes): 스왑(swap) 모드, 랜덤(random) 모드 및 오토인코더(autoencoder) 모드 중 하나 또는 이들의 조합으로 학습된다. 생성적 신경망은 적어도 컨텐츠 인코더(content encoder), 스타일 인코더(style encoder) 및 디코더 (decoder)를 포함한다. 본 개시의 제2 실시 예에 따르면, 프로세서 및 프로세서에 의해 실행되면 프로세서가 제1 실시 예에 따른 방법 을 수행하도록 하는 컴퓨터-실행 가능한 명령어들을 저장하는 메모리를 포함하는 컴퓨팅 장치가 제공된다. 본 개시의 제3 실시 예에 따르면, 학습된 생성적 신경망을 이용하여 컨텐츠 이미지를 바탕으로 자연스러운 데이 스케일 타임랩스 시퀀스의 하나 또는 그 이상의 이미지를 생성하는 방법이 제공된다. 방법은 컨텐츠 이미지 및 (a) 컨텐츠 이미지에 적용될 미리 정의된 하나 또는 그 이상의 스타일 또는 (b) 컨텐츠 이미지에 적용될 하나 또는 그 이상의 스타일을 갖는 하나 또는 이상의 스타일 이미지를 수신하는 단계; 컨텐츠 이미지의 가로세로 비 율(aspect ratio)을 유지하면서 컨텐츠 이미지의 더 작은 변(side) 상에서 컨텐츠 이미지의 해상도를 더 낮은 해상도로 축소시키는(reducing) 단계; 하나 또는 그 이상의 스타일 각각에 따라 재-스타일화된 하나 또는 그 이 상의 축소된 컨텐츠 이미지를 획득하기 위해, 학습된 생성적 신경망을 하나 또는 그 이상의 스타일 각각을 이용 하여 축소된 컨텐츠 이미지에 적용하는 단계; 및 가로세로 비율을 유지하면서 더 작은 변 상에서 더 낮은 해상 도를 갖는 고주파 성분들 및 저주파 성분으로 재-스타일화된 컨텐츠 이미지 각각을 분해하는(decomposing) 단계; 대응하는 재-스타일화된 컨텐츠 이미지의 컨텐츠를 고려하여 저주파 성분을 필터링하는(filtering) 단계; 및 대응하는 재-스타일화된 컨텐츠 이미지 각각의 필터링된(filtered) 저주파 성분 및 고주파 성분들에 기초하 여 자연스러운 데이스케일 타임랩스 시퀀스의 하나 또는 그 이상의 이미지를 생성하는 단계를 포함한다: 본 개시의 제4 실시 예에 따르면, 프로세서 및 프로세서에 의해 실행되면 프로세서가 제3 실시 예에 따른 방법 을 수행하도록 하는 컴퓨터-실행 가능한 명령어들을 저장하는 메모리를 포함하는 컴퓨팅 장치가 제공된다."}
{"patent_id": "10-2022-7008530", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "첫째, 제안된 방법은 데이터셋에 표현된 도메인들에 대한 지식 없이도, 이미지-투-이미지 시멘틱 프리저빙 스타 일 트랜스퍼(semantic pre-serving style transfer)를 수행하는 것을 가능하게 한다. 수집된 데이터셋의 인터 널 바이어스(internal bias), 아키텍처 바이어스 및 특별히 개발된 학습 절차는 이러한 환경에서도 스타일 변환 (style transformations)을 학습하는 것을 가능하게 한다. 둘째, 파인 디테일 프리저베이션(fine detail preservation)을 보장하기 위해, 개시된 이미지-투-이미지 변환을 위한 아키텍처는 2 개의 기술: 스킵 커넥션(skip connections)과 적응적 인스턴스 정규화(adaptive instance normalizations)(AdaIN)를 결합한다. 이러한 조합은 실현 가능하며, 스킵 커넥션 없이 현재 도미넌트한 AdaIN 아키텍처보다 디테일들을 훨씬 더 잘 보존하는 아키텍처로 이어질 수 있다. 본 명세서의 주요한 목적과는 별개 로, 제안된 발명은 멀티 도메인 이미지 스타일화(stylization)/리컬러링(recoloring)을 학습하는데 이용될 수 있고, 해당 기술 분야의 현재 상태와 동등한 품질을 달성할 수 있다. 마지막으로, 높은 해상도에서 고용량 이미지-투-이미지 변환 네트워크를 직접 학습하는 것은 산술적으로 불가능 하기 때문에, (머징 네트워크를 사용하는) 새로운 향상 스킴은 고해상도의 자연스러운 이미지를 생성하기 위해, 더 낮은 해상도에서 학습된 이미지-투-이미지 변환 네트워크의 적용을 가능하게 한다."}
{"patent_id": "10-2022-7008530", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "종래 기술의 방법에서는 이미지들 간에 독립적으로 스왑될 수 있는 \"컨텐츠\"와 \"스타일\"로 이미지를 분해하는 것에 대처하기 위해 일부 형태의 도메인/속성(attribute) 어노테이션에 의존하였다. 본 발명에서 이러한 분해는 이하에서 설명되는, 적절하게 선택된 아키텍처와 학습 절차를 사용하여 완전한 비지도(fully unsupervised) 방 식으로 용이해질 수 있다. 도 1은 본 명세서에 개시된 본 발명의 제1 실시 예에 따른 학습된 생성적 신경망 및 학습된 머징 신경망을 이용 하여 이미지로부터 자연스러운 데이스케일 타임랩스를 생성하는 방법의 흐름도를 도시한다. 방법은 이미지 및 이미지에 적용될 하루 중 하나 또는 그 이상의 시간에 각각 대응하는 하나 또는 그 이상의 미리 정의된 스타일 을 지정하는(specifying) 스타일 파라미터를 수신하는 단계(S105)를 포함한다. 대안으로, 단계 S105에서는 스타 일 파라미터를 대신하여, 컨텐츠 이미지에 적용될 하나 또는 그 이상의 스타일을 갖는 하나 또는 그 이상의 스 타일 이미지가 수신될 수 있다. 이미지는 스마트폰과 같은 사용자의 컴퓨팅 디바이스를 사용하여 사용자에 의해 촬영 또는 로딩될 수 있다. 스타일 파라미터는 사용자의 컴퓨팅 장치를 사용하여 사용자에 의해 특정될 수 있다. 예를 들어, 사용자는 타임라인 상에서 특정 시점 또는 시간 범위를 선택할 수 있고, 하나 또는 그 이상의 미리 정의된 스타일은 선택된 시점 또는 시간 범위에 기초하여 상응하게 결정될 수 있다. 만일, 타임라인 상에 서 사용자에 의해 특정 시점이 선택되면, 방법은 해당 특정 시점에 대해 생성된 단일 이미지만을 포함하는 자연 스러운 데이스케일 타임랩스를 생성한다는 점에 유의해야 한다. 미리 정의된 스타일의 수는 제한되지 않지만, 특정 스타일에 대해 방법이 적절하게 작동하려면, 그러한 스타일을 전달하는 학습 이미지를 이용하여 생성적 신 경망이 사전에 학습되어야 한다. 그리고, 방법은 이미지를 n 개의 이미지 크롭으로 자르는 단계(S110)를 포함한다. n 개의 이미지 크롭은 오프셋 방향 및 k 개의 픽셀(들)의 스트라이드(stride)에 의해 정의되는 미리 정의된 방식으로 강하게 중첩되는 (strongly overlapping) 이미지 크롭들이다. n 및 k 의 특정한 값들은 한정되지 않으며, 임의의 정수일 수 있다. 그리고, 방법은 스타일 파라미터에 따라 재-스타일화된 n 개의 이미지 크롭을 획득하기 위해, 학습된 생 성적 신경망을 스타일 파라미터를 사용하여 n 개의 이미지 크롭에 적용하는 단계(S115)와, 이미지에 대한 자연 스러운 데이스케일 타임랩스를 획득하기 위해, 학습된 머징 신경망을 사용하여 재-스타일화된 n 개의 이미지 크롭을 병합하는 단계(S120)를 포함한다. 생성된 자연스러운 데이스케일 타입랩스는 특정 스타일에 대한 단일 이 미지 또는 선택된 스타일들의 시퀀스에 대한 이미지들의 시퀀스를 포함할 수 있다. 생성된 자연스러운 데이스케 일 타임랩스의 몇 가지 이미지의 예가 도 10에 도시되어 있다. 본 발명의 제3 실시 예에 따른 방법(도시되지 않음)이 제1 실시 예에 따른 방법과 비교하여 그들의 차이점 측면 에서 설명될 것이다. 제3 실시 예에 따른 방법은 머징 네트워크를 사용하지 않고, 이미지를 이미지 크롭으로 자 르는 단계를 사용하지 않는다는 점에서, 제1 실시 예에 따른 방법과 차이가 있다. 대신, 본 발명의 제3 실시 예 에 따른 방법은 컨텐츠 이미지의 가로세로 비율을 유지하면서 컨텐츠 이미지의 더 작은 변 상에서 컨텐츠 이미 지의 해상도를 더 낮은 해상도로 축소시키는 단계, 가로세로 비율을 유지하면서 더 작은 변 상에서 더 낮은 해 상도를 갖는 고주파 성분들 및 저주파 성분으로 재-스타일화된 컨텐츠 이미지 각각을 분해하는 단계, 대응하는 재-스타일화된 컨텐츠 이미지의 컨텐츠를 고려하여 저주파 성분을 필터링하는 단계 및 대응하는 재-스타일화된 컨텐츠 이미지 각각의 필터링된 저주파 성분 및 고주파 성분들에 기초하여 자연스러운 데이스케일 타임랩스 시 퀀스의 하나 또는 그 이상의 이미지를 생성하는 단계를 포함한다. 라플라시안 피라미드(Laplacian pyramid)가 분해를 위한 제한되지 않는 실시 예에서 이용될 수 있다. 가이디드 필터(guided filter)가 필터링을 위한 제한 되지 않는 실시 예에서 이용되고, 또한, 대응하는 재-스타일화된 컨텐츠 이미지의 컨텐츠는 필터링을 위한 가이 드로서 이용될 수 있다. 더 낮은 해상도는 128과 같거나, 또는, 128 보다 낮거나 높을 수 있다. 제3 실시 예에 따른 방법을 위한 생성적 신경망의 학습은 생성적 신경망의 출력이 판별기(discriminator)에 직접 제공되지 않 지만 differentiated 가이디드 필터를 원본 이미지와 대응하는 합성(생성) 이미지에 적용한 결과가 제공된다는 점에서, 제1 실시 예에 따른 방법을 위한 생성적 신경망의 학습과 차이가 있다. 따라서, 생성적 신경망(고해상도 시간대 변환(high resolution daytime translation, HiDT) 모델이라고도 함) 은, 학습 셋으로부터의 명시적 지도 없이, 자신의 아키텍처 바이어스를 이용하여 입력 이미지 로부터 컨텐츠 및 스타일의 독립 인코딩(independent encodings)을 추출하고, 그리고, 새로운 컨텐츠 스타일 조합들을 갖는 이 미지를 구성하는 것을 목표로 한다. 따라서, 출력 이미지 는 로부터 컨텐츠를 취하고, 선택된 스타일 파라 미터에 따라 자신의 스타일을 변경한다. 따라서, 본 명세서에서 태스크는 범주형 변수(categorical variables) 를 조건으로 하는 기존의 조건부(conditional) GAN 아키텍쳐를 사용하는 대신, 스타일 이미지 으로부터의 스 타일을 컨텐츠 이미지 로 트랜스퍼링(transferring) 것으로 정의된다. 생성적 신경망은 다음의 모드들: 스왑 모드, 랜덤 모드 및 오토인코더 모드 중 하나 또는 이들의 조합으로 학습 된다. 실시 예에서, 생성적 신경망은 나타난 모드들 각각에서 학습된다. 이하에서 모드들 각각에 대한 학습 단 계를 설명하고, 한정으로 해석되지 않는 특정한 구현 세부 사항을 소개한다. 도 2는 본 명세서에 개시된 본 발명의 실시 예에 따른 스왑 모드에서 생성적 신경망의 학습의 흐름도를 도시한 다. 생성적 신경망은 적어도 컨텐츠 인코더, 스타일 인코더 및 디코더를 포함한다. 스왑 모드에서 생성적 신경 망의 학습은 다수의 이터레이션(iterations)에서 반복적으로 수행되는 다음의 단계들(following steps)을 포함 한다: 제1 이미지에서 컨텐츠의 공간적으로 더 작은 레프레센테이션(representation)인 컨텐츠 텐서(content tensor)를 획득하기 위해, 제1 이미지를 학습 예(training example)로서 컨텐츠 인코더에 입력하는 단계(S115). 그리고, 학습은 제2 이미지에서 스타일을 나타내는 스타일 벡터를 획득하기 위해, 제2 이미지를 학습 예로서 스 타일 인코더에 입력하는 단계(S160)를 포함한다. 제1 이미지 및 제2 이미지는 도 1을 참조하여 상술한 방법에 의해 생성된 자연스러운 데이스케일 타임랩스의 해상도보다 낮은 제1 해상도를 갖는다. 제1 이미지(들) 및 제2 이미지(들)은 생성적 신경망 학습의 목적을 위한 이미지들 가령, 풍경(landscape) 이미지들의 학습 데이터셋으 로부터 랜덤하게 선택될 수 있다. 그리고, 학습은 적응적 인스턴스 정규화를 이용하여 제1 이미지의 컨텐츠와 제2 이미지의 스타일을 갖는 제3 이미지 및 제1 이미지에 대응되는 세그먼테이션 마스크(segmentation mask)를 획득하기 위해, 컨텐츠 텐서, 스타일 벡터를 디코더에 입력하는 단계(S165)를 포함한다. 그리고, 학습은 이미지 가 주어진 스타일 하에서 충분히 자연스러운지 판단하도록 구성된 조건부 판별기(conditional discriminator)에 제2 이미지와 스타일 벡터 또는 스타일 벡터와 제3 이미지를 교대로(alternately) 입력하는 단계(S170) 및 적대 적 방식(adversarial manner)으로 생성적 신경망 및 조건부 판별기의 학습을 보장하기 위해, 서로 다른 이터레 이션에서 조건부 판별기에 의한 판단 결과를 바탕으로 생성적 신경망 및 조건부 판별기의 파라미터를 교대로 업 데이트하는 단계(S175)를 포함한다. 생성적 신경망의 파라미터를 업데이트하는 과정에서, 세그먼테이션 마스크 가 추가로 고려된다.도 3은 본 명세서에 개시된 본 발명의 실시 예에 따른 랜덤 모드에서 생성적 신경망의 학습의 흐름도를 도시한 다. 랜덤 모드에서 생성적 신경망의 학습은 다수의 이터레이션에서 반복적으로 수행되는 다음의 단계들을 포함 한다: 제1 이미지에서 컨텐츠의 공간적으로 더 작은 레프레센테이션인 컨텐츠 텐서를 획득하기 위해, 제1 이미 지를 학습 예로서 컨텐츠 인코더에 입력하는 단계(S180). 그리고, 학습은 사전 분포(prior distribution)로부터 랜덤 스타일 벡터를 생성하는 단계(S185) 및 적응적 인스턴스 정규화를 이용하여 제1 이미지의 컨텐츠와 랜덤 스타일 벡터에 의해 정의되는 스타일을 갖는 제3 이미지 및 제1 이미지에 대응되는 세그먼테이션 마스크를 획득 하기 위해, 컨텐츠 텐서, 랜덤 스타일 벡터를 디코더에 입력하는 단계(S190)를 포함한다. 제1 이미지 및 제3 이 미지는 도 1을 참조하여 전술한 방법에 의해 생성된 자연스러운 데이스케일 타임랩스의 해상도보다 낮은 제1 해 상도를 갖는다. 그리고, 학습은 이미지가 주어진 스타일 하에서 충분히 자연스러운지 판단하도록 구성된 조건부 판별기에 랜덤 스타일 벡터와 제3 이미지를 입력하는 단계(S195) 및 적대적 방식으로 생성적 신경망 및 조건부 판별기의 학습을 보장하기 위해, 조건부 판별기에 의한 판단 결과를 바탕으로 생성적 신경망의 파라미터를 업데 이트하는 단계(S200)를 포함한다. 생성적 신경망의 파라미터를 업데이트하는 과정에서, 세그먼테이션 마스크가 추가로 고려된다. 도 4는 본 명세서에 개시된 본 발명의 실시 예에 따른 오토인코더 모드에서 생성적 신경망의 학습의 흐름도를 도시한다. 오토인코더 모드에서 생성적 신경망의 학습은 다수의 이터레이션에서 반복적으로 수행되는 다음의 단 계들을 포함한다: 제1 이미지에서 컨텐츠의 공간적으로 더 작은 레프레센테이션인 컨텐츠 텐서를 획득하기 위해, 제1 이미지를 학습 예로서 컨텐츠 인코더에 입력하는 단계(S205). 그리고, 학습은 제1 이미지에서 스타일 을 나타내는 스타일 벡터를 획득하기 위해, 제1 이미지를 학습 예로서 스타일 인코더에 입력하는 단계(S210) 및 적응적 인스턴스 정규화를 이용하여 제1 이미지의 컨텐츠와 스타일을 갖는 제3 이미지 및 제1 이미지에 대응되 는 세그먼테이션 마스크를 획득하기 위해, 컨텐츠 텐서, 스타일 벡터를 디코더에 입력하는 단계(S215)를 포함한 다. 제1 이미지 및 제3 이미지는 도 1를 참조하여 상술한 방법에 의해 생성된 자연스러운 데이스케일 타임랩스 의 해상도보다 낮은 제1 해상도를 갖는다. 그리고, 학습은 제1 이미지와 제3 이미지를 비교하는 단계(S220) 및 비교 결과에 기초하여 생성적 신경망의 파라미터를 업데이트하는 단계(S225)를 포함한다. 한정되지 않는 실시 예에서, 비교는 픽셀 대 픽셀(pixel-by-pixel) 비교일 수 있다. 생성적 신경망의 파라미터를 업데이트하는 과정 에서, 세그먼테이션 마스크가 추가로 고려된다. 따라서, 학습 중에, 생성적 신경망의 디코더는 입력 이미지 뿐만 아니라 대응하는 세그먼테이션 마스크 (외부의 미리 학습된 네트워크에 의해 생성됨)를 예측한다. 본 명세서의 발명은 부산물로서 최신의 세그먼테이 션을 달성하는 것을 목표로 하지 않지만, 생성적 신경망에서의 세그먼테이션은 스타일 트랜스퍼(transfer)를 제 어하고, 시멘틱 레이아웃(semantic layout)을 더 잘 보전하는데 도움이 된다. 그렇지 않으면, 생성적 신경망이 풀을 물로 다시 페인팅하거나 그 반대로 페인팅하는 것을 막을 수 없다. 세그먼테이션 마스크들은 네트워크에 입력으로 제공되지 않으므로, 인퍼런스 (in-use) 단계에서 필요하지 않다. 명세서 전반에서, 입력 이미지들의 공간(space)은 로 표시되고, 그들의 세그먼테이션 마스크들은 으로 표 시되고, 세그먼테이션 마스크들을 갖는 개별 이미지들은 으로 표시된다; 레이튼트 컨텐츠 코드들 (latent content codes) 의 공간은 이고, 레이튼트 스타일 코드들(latent style codes) 의 공간은 이다( 인 반면, 는 보다 복잡한 구조를 갖는다). 이미지 로부터 와 를 추출하기 위해, 생성적 신경망은 2 개의 인코더: 입력 이미지 의 컨텐츠 레프레센테이션 를 추출하는 및 입력 이미지 의 스타일 레프레센테이션 를 추출하는 를 사용한다. 주어진 레이튼트 컨텐츠 코드 및 레이턴트 스타일 코드 에서, 생성적 신경망의 디코더(생성기(generator)) 는 새로운 이미지 및 대응하는 세그먼테이션 마스크 를 생성한다. 따라서, 생성적 신경망은 로부터의 컨 텐츠와 로부터의 스타일을 와 같이 결합할 수 있다. 따라서, 생성적 신경망은 (i) 2 개의 입력 이미지 와 로부터 시작하거나, (ii) 입력 이미지 와 이미지에 적용될 하루 중 하나 또 는 그 이상의 시간에 각각 대응되는 하나 또는 그 이상의 미리 정의된 스타일을 지정하는 스타일 파라미터로부터 시작하는, 적어도 스타일 인코더 , 컨텐츠 인코더 및 디코더 를 결합한다. 스타일 파라미터는 이미지 로부터 추출되거나, 예를 들어, \"타임라인\"을 참조하여 전술한 바와 같이, 사용자에 의해 직접 입력될 수 있다. 도 7은 본 명세서에 개시된 본 발명의 실시 예에 따른 생성적 신경망의 가능한 구현에서의 데이터 흐 름을 도시한다. 도 7은 (대칭적) 아키텍쳐의 절반을 도시한다. 은 다른 이미지 로부터 추출된 스 타일이고, 은 와 가 스왑되어 와 유사하게 얻어진다. 도면은 데이터 요소; 손실 함수(loss function); 함수(서브네트워크);를 나타낸다. 레이블이 동일한 함수들은 공유된 가중치들을 가진다. 생성적 신경망에 적용 가능한 손실 함수 적대적 손실(Adversarial loss). 생성적 신경망은 보통의 적대적 방식(adversarial fashion)에서 정의된 바와 같이, 자연스럽고 사실적인(realistic) 타임랩스 이미지(들)을 생성해야 한다. 스타일을 설명하기 위해, 2 개의 판별기, 무조건부(unconditional) 판별기 및 조건부 판별기 가 사용된다. 이들 판별기 는 예를 들어, least squares GAN 접근 방식을 사용하여, 실제 이미지와 변환된(translated) 이미지를 구별하려 고 시도한다. 실제 컨텐츠와 스타일 이미지, , 로부터 생성된 \"페이크(fake)\" 이미지는 와 같이 정의된다. 동일한 스킴이 랜덤 스타일 로 생성된 이미지들에 사용 된다. 프로젝션 컨디셔닝 스킴(projection conditioning scheme)이 이용되고, 스타일은 디코더 파라미터 업데이 트 단계 동안 스타일이 에 제공되면, 계산 그래프(computational graph)로부터 분리된다. 실제 이미지는 그들로부터 추출된 스타일을 사용하는 반면, 생성된 이미지는 그들이 변환된 스타일과 결합된다. 도 7은 적대적 손실을 도시하지 않았다. 이미지 재구성 손실(Image reconstruction loss). 이미지 재구성 손실 은 원본과 재구성된 이미지들 간의 차이의 L1-norm으로 정의된다. 이미지 재구성 손실은 생성적 신경망의 아키텍쳐에서 최소 3 번: 컨텐츠 이미지 의 재구성 , , 랜덤 스타일 이미지 의 재구성 , 및 스타일화된 이미지 의 컨텐츠와 스타일화된 이미지 의 스타일로부터 이미지 의 재구성 (cross cycle consistency): (여기에서, 이다)에 적용될 수 있다(도 7 참조). 세그먼테이션 손실(Segmentation loss). 세그먼테이션 손실 은 이미지 재구성 손실과 함께 사용되며, 원본 과 재구성된 세그먼테이션 마스크들 간의 크로스 엔트로피 로 정의된다. 세그먼테이션 손실은 생성적 신경망의 아키텍쳐에서 적어도 2 번: 변환된(translated) 이미지의 세그먼테이션 마스크 , 및 랜덤 스타일 이미지의 마스크 , 에 적용된다. 레이튼트 재구성 손실(Latent reconstruction loss). 2 개의 재구성 손실 와 는 스타일 및 컨텐츠 코드들를 처리한다: 이러한 재구성 손실들은 원래의 코드와 재구성된 코드 간의 차이에 적용되고, 생성적 신경망의 아키 텍쳐에 적어도 2 번 사용된다. 첫째로, 랜덤 스타일 이미지의 스타일 와 컨텐츠 에 대해, , 여기에서 스타일은 와 매치되고, 컨텐츠는 와 매치되어야 한다: . 둘째, 스 타일화된 이미지 의 스타일 와 컨텐츠 에 대해, , 여기에서, 스타일은 와 매치되고, 컨텐츠는 와 매치되어야 한다; L1 손실은 한정되는 것은 아니나 컨텐츠에 적용될 수 있고, , 더욱 강인한 (robust) 손실 함수는 스타일이 0으로 감소되는 것을 방지하기 위해, 스타일에 적용될 수 있다:. 스타일 분포 손실(Style distribution loss). 추출된 스타일 코드의 공간의 구조를 강화하기 위해, 스타일 분포 손실이 다수의 이전 학습 이터레이션으로부터 수집된 스타일들의 풀(pool)에 적용될 수 있다. 즉, 주어진 풀 사 이즈 에 대해, 스티일 { }은 스탑 그래디언트(stop gradient) 작업이 적용된 과거 minibatches로부터 수집될 수 있고, 추출된 스타일 와 (이들은 현재 계산 그래프의 일부)는 이러한 풀에 추가될 수 있고, 평 균 벡터(mean vector) 와 공분산 행렬(covariance matrix) 은 업데이트된 풀을 이용하여 산출될 수 있 다. 그리고, 스타일 분포 손실은 결과 분포(resulting distribution)의 경험적 모멘트(empirical moments)를 랜덤 스타일 벡터 디코더의 이론적 모멘트(theoretical moments)와 매칭시킨다 . 공간 은 저차원(low-dimensional)이고, 타겟은 표준 정규 분포(standard normal distribution) 이기 때문에, 이러한 단순화된 접근 방식은 레이튼트 코드의 공간에서 구조를 실시하기에 충분하다. 손실 값을 산출한 후, 가장 오래된 스타일을 풀에서 제거하여 풀의 사이 즈를 로 유지한다. 총 손실 함수(Total loss function). 따라서, 전체 생성적 신경망은 다음과 같은 목표에 따라, 스타일 인코더, 컨텐츠 인코더, 디코더 및 판별기를 공동으로 학습할 수 있다:"}
{"patent_id": "10-2022-7008530", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "하이퍼파라미터(Hyperparameters) λ1,...,λ7는 전체 손실 함수에서 컴포넌트의 상대적 중요도를 정의하며, 이 들은 경험적으로 결정되었다. 실험은 프로젝션 판별기(projection discriminator)가 결과를 현저히 향상시키는 반면, 세그먼테이션 손실 함수 를 제거하면 때때로 디코더에 의해 야기되는 원치 않는 \"환각(hallucinations)\"이 발생하는 것으로 나타났다. 하지만, 모델은 세그먼테이션 손실 함수 없이 여전히 성공적으로 학습되었다. 실험은 또한 스타일 분포 손실 함 수가 필요하지 않다는 것을 입증한다. 이는 학습 중에 프로젝션 판별기와 랜덤 스타일을 모두 사용하기 때문이 라는 것을 보여준다. 적응적 U-Net(Adaptive U-Net) 아키텍쳐 자연스러운 시간대 풍경 이미지를 생성하기 위해, 모델은 원본 이미지로부터 디테일들을 보존해야 한다. 따라서, 생성적 신경망은 제1 이미지의 컨텐츠 텐서에 의해 전달(convey)되지 않은 고주파 특징을 디코더에 전 달하도록 구성된 하나 또는 그 이상의 덴스 스킵 커넥션(dense skip connections)을 포함할 수 있다. 이를 구현 하기 위해, FUNIT에서 영감을 받은 인코더-디코더(FUNIT-inspired encoder-decoder)는 의 다운샘플링 (downsampling) 부분과 의 업샘플링(upsampling) 부분 간의 덴스 스킵 커넥션으로 강화될 수 있다. 불행하 게도, 레귤러 스킵 커넥션(regular skip connections)은 또한 초기 입력의 스타일을 보존하다. 그러므로, 실시 예에서, AdaIN를 갖는 추가적인 컨볼루션 블록(convolutional block)이 도입되고 스킵 커넥션에 적용될 수 있다. 도 8은 본 명세서에 개시된 본 발명의 실시 예에 따른 적응적 U-Net 아키텍쳐의 다이어그램: 덴스 스킵- 커넥션과 컨텐츠-스타일 분해 ()를 갖는 인코더-디코더 네트워크를 도시한다. 전체적인 아키텍쳐는 다음과 같은 구조를 갖는다: 컨텐츠 인코더 는 여러 개의 컨볼루션 다운샘플링 레이어 와 잔여 블록(residual blocks)을 이용하여 초기 이미지를 3D 텐서 에 맵핑한다. 한정되지 않는 실시 예에서, 스타일 인코더 는 글로벌 풀링(global pooling)과 압축(compressing) 1×1 컨볼루션 레이어로 끝 나는 풀리(fully) 컨볼루션 네트워크이다. 디코더 는 내부에 AdaIN 모듈들을 갖는 여러 개의 잔여 블록으로를 처리하고, 이후, 처리된 를 업샘플링한다. 머징 신경망을 이용한 후처리 향상 메모리 및 계산 시간에서의 하드웨어 제약으로 인해, 고해상도 이미지로 효과적으로 동작할 수 있는 네트워크를 학습하는 것은 어렵다. 풀리 컨볼루션 신경망을 고해상도 이미지에 직접 적용하거나 가이디드 필터를 이용하는 것은 고해상도 이미지를 처리하는 데 적용할 수 있는 기술이다. 비록, 이러한 기술들은 대부분의 경우에 좋은 결과를 보여주지만, 몇 가지 한계가 있다. 풀리 컨볼루션 적용은 복수의 태양이 그려지는 일몰이나 하늘과 물 표면 간의 경계가 혼동될 수 있는 워터 리플렉션(water reflections)의 경우와 같이, 한정된 수용 영역(limited receptive field)으로 인한 장면 손상(scene corruption)을 일으킬 수 있다. 반면에, 가이디드 필터는 물 또는 태양과 함께 잘 작동하지만, 잔가지와 같은 작은 디테일이 스타일 트랜스퍼 절차에 의해 변경되거나, 수평선 또 는 매우 대조적인 어떠한 다른 경계에서 강한 영향을 받아 후광 효과(halo effect)가 발생될 때, 실패한다. 이 러한 경우들은 고려할 필요가 없는 코너 케이스(corner case)로 볼 수 있지만, 시간대 변환 태스크에서 중요하 고, 이것은 시맨틱 프리저빙 업스케일(semantic preserving upscale) 방식의 필요성으로 이끈다. 또한, 이미지 투 이미지 네트워크에 의해 생성된 bicubic 다운샘플링 커널과 아티팩트(artifacts) 간의 더 큰 불일치로 인해, 슈퍼레졸루션(superresolution) 방법들과 미리 학습된 모델들의 간단한 적용은 가능하지 않다. 도 5는 본 명세서에 개시된 본 발명의 실시 예에 따른 머징 신경망의 학습의 흐름도를 도시한다. 본 명세서에 개시된 본 발명의 실시 예에 따르면, 변환된 이미지를 업스케일하고 동시에 학습 및 프로즌 (frozen) 디코더 에 대한 \"전형적인(typical)\" 아티팩트들을 제거하기 위해 별도의(seperate) 머징 신경망 (강화 네트워크(enhancement network)라고도 함)을 사용하는 것이 제안된다. 본 명세서에 개시된 본 발명 의 실시 예에서, 머징 신경망의 학습은 다수의 이터레이션에서 반복적으로 수행되는 다음의 단계들을 포함한다: 각각 제1 해상도 보다 높은 제2 해상도를 갖는 학습 이미지들의 셋을 획득하는 단계(S230). 그리고, 학습은 학 습 이미지들의 셋의 각 이미지를 오프셋 방향과 k 개의 픽셀(들)의 스트라이드에 의해 정의되는 미리 정의된 방 식으로 n 개의 강하게 중첩되는 이미지 크롭으로 자르는 단계(S235)를 포함한다. n 및 k의 값은 한정되지 않는 다. 오프셋 방향은 다른 강하게 중첩되는 이미지 크롭에 대한 하나의 이미지의 오프셋의 방향을 나타내고, 스트 라이드는 이미지 크롭들 간의 오프셋 양을 나타낸다(즉, 이미지 크롭들 간의 논-오버랩핑(non-overlapping) 영 역). 그리고, 학습은 n 개의 이미지 크롭의 각 이미지 크롭을 제1 해상도로 다운샘플링하는 단계(S240) 및 각각 이 학습된 생성적 신경망에 의해 생성된 변환된 이미지 크롭과 원본 학습 이미지의 대응하는 이미지 크롭 간의 불일치(discrepancies)와 아티팩트를 캡쳐하는 n 개의 변환된 크롭을 획득하기 위해, 오토인코더 모드에서 학습 된 생성적 신경망을 n 개의 이미지 크롭의 각 이미지 크롭에 적용하는 단계(S245)를 포함한다. 마지막으로, 학 습은 아티팩트와 불일치가 감소된 병합된 이미지를 획득하기 위해 n 개의 변환된 크롭을 머징 신경망에 입력하 고, 병합된 이미지를 학습 이미지들의 셋의 대응하는 원본 이미지와 비교하고, 비교 결과에 기초하여 머징 신경 망의 파라미터를 업데이트하는 단계(S250)를 포함한다. 종래 기술과 비교하면, 여러 RGB 이미지가 특징 맵들(feature maps) 대신 입력으로 이용되었다. 개시된 방법은 쌍을 이루는 데이터셋을 획득하기 위해 \"오토인코더\" 모드에서 디코더를 사용하고, 지도 방식으로 머징 신경망 을 학습시키고, 실제 이미지와 디코더에 의해 생성된 이미지 간의 불일치와 가장 일반적인 아티팩트를 캡쳐하는 것에 의존한다. 변환된 이미지들에 대한 일반화(generalization)를 추가적으로 향상시키기 위해, 디코더는 \"랜 덤 스타일\" 모드에서 사용되어 지도(supervised) (지각 및 특징 매칭(perceptual and feature matching)) 손실 들이 적용되지 않는 추가적인 비지도 셋(unsupervised set)을 획득할 수 있다. 간략히, \"오토인코더\"를 위한 손 실 함수는 아래에서 논의된다. 특정한 구현에서, 고해상도 이미지 (실험에서는 1024×1024)는 1 개의 픽셀의 스트라이드와 동일한 폭 (width)과 높이(height)의 강하게 중첩되는 프레임들 로 커버된다; 각 프레임은 보다 몇 픽셀 작다. 프레임은 bilinear 커널을 통해 생성적 신경망의 디코더에 적합한 해상도(한정되지 않는 실시 예에서, 스 케일 팩터(scale factor) 4를 통한 256×256)로 다운스케일되어, 다운스케일된 크롭들의 셋 이 생성된 다. 그리고, 생성적 신경망이 다운스케일된 크롭들의 셋 에 적용되어, 저해상도 이미지들 ,이 생성된다. 이러한 프레임들은 고정된 순서로 단일 텐서(single tensor)에 스 택되고, 원본 이미지 를 복원하려는 머징 신경망 에 제공되며, 의 결과를 가진다. 이러한 프로세스의 예시가 도 9에 도시되어 있다. 에 대해, pix2pixHD의 학습 세팅은 지각(perceptual), 특징 매칭(feature matching) 및 적대적 손실 함 수들(adversarial loss functions)과 함께 이용될 수 있다. 고해상도 원본 이미지들은 지도(supervision)로서 이용된다. 는 학습 과정에서 다음의 손실 함수들 중 하나 또는 그 이상을 사용할 수 있다: 과 간의 지각 재구성 손실(perceptual reconstruction loss): ; 각 판별기의 각 특징 맵을 이용한 과 간의 특징 매칭 손실(멀티 스케일 아키텍쳐에는 3 가지가 있음): , LSGAN에 기반한 적대적 손실: , . 구현 세부사항 학습 디테일(training details). 하기에 주어진 특정한 구현 세부사항은 단지 한정되지 않는 예들로 고려될 수 있다. 구현 예에서, 컨텐츠 인코더는 2 개의 다운샘플링과 4 개의 잔여 블록들을 포함할 수 있다; 각각의 다운 샘플링 후, 단지 5 개의 채널이 스킵 커넥션을 위해 이용될 수 있다. 스타일 인코더는 4 개의 다운샘플링 블록 들을 포함하고, 그리고, 다운샘플된 결과는 3차원 벡터로 공간 정보(spatial information)에 대해 평균화 (averaged)될 수 있다. 디코더는 내부에 AdaIN를 갖는 5 개의 잔여 블록과 2 개의 업샘플링 블록을 포함할 수 있다. AdaIN 파라미터들은 3-레이어 피드포워드 네트워크(three-layre feedforward network)를 통해, 스타일 벡터로부터 산출될 수 있다. 2 개의 판별기는 3 개의 다운샘플링 레이어를 갖는, 멀티 스케일(multi-scale)이다. 생성적 신경망은 배치 사이즈(batch size)가 4를 갖는 다수의 이터레이션(예를 들어, 약 45 만번의 이터레이션)에 대해 학습될 수 있다. 학습을 위해, 이미지들은 256×256의 해상도로 다운스케일될 수 있다. 특정한 예에서, 손실 가중치(loss weights)는 경험적으로 λ1=5, λ2=2, λ3=3, λ4=1, λ5=0.1, λ6=4, λ7=1와 같이 결정될 수 있다. adam 옵티마이저(optimizer)는 β1=0.5, β2=0.999 및 생성기들 및 판별기들에 대 한 초기 학습률(initial learning rate) 0.0001이 사용될 수 있고, 200000 번의 이터레이션마다 학습률이 반으 로 줄어든다. 데이터셋과 시간대 분류기(dataset and daytime classifier). 20000 개의 풍경 사진의 데이터셋이 인터넷으로 부터 수집되었다. 이러한 이미지들의 일부는 크라우드소싱 플랫폼(crowdsourcing platform)을 사용하여 수동으 로 4 개의 클래스(밤, 일몰/일출, 아침/저녁, 정오, 다만, 제한 없음)로 라벨(labeled)되었다. 다른 실시 예들 에서는 더 많거나 더 적은 클래스가 사용될 수 있다. ResNet 기반 분류기(ResNet-based classifier)는 이러한 라벨들에 대해 학습되고, 데이터 셋의 나머지에 적용될 수 있다. 예측 레이블(predicted labels)은 2 가지 방식: 시간대 클래스들에 대한 이미지 변환 모델들을 위한 학습 셋을 밸런싱하고, 베이스라인 모델들 (baseline models)에 대한 도메인 레이블을 제공하기 위해 이용될 수 있다. 세그먼테이션 마스크들은 외부 모델 에 의해 생성되었고, 9 개의 클래스: 하늘, 잔디, 땅, 산, 물, 빌딩, 나무, 도로 및 인간으로 축소되었다. 다른 실시 예에서는 더 많거나 더 적은 클래스가 사용될 수 있다. 중요하게는, 개시된 생성적 신경망의 하나의 적용 은 특정한 영상을 지표(guidance)로서 사용하는 시간대 타임랩스 생성이다. 다른 실시 예들 도 6은 본 명세서에 개시된 본 발명의 실시 예에 따른 방법을 수행할 수 있는 컴퓨팅 장치의 블록도를 도시한다. 컴퓨팅 장치(예를 들어, 스마트폰, 태블릿, 노트북, 스마트워치 등)는 프로세서 및 프로세 서에 의해 실행되면 프로세서가 제1 실시 예에 따른 방법을 수행하도록 하는 컴퓨터-실행 가능한 명령어들을 저 장하는 메모리를 포함한다. 프로세서와 메모리는 서로 통신 가능하게 연결된다. 한정되지 않는 실시 예들에서, 프로세서는 범용 프로세서(general-purpose processor), ASIC(application-specific integrated circuit), FPGA(user-programmable gate array) 또는 SoC(system-on-chip)를 포함하는 컴퓨팅 수단 으로 구현될 수 있으나, 이에 한정되는 것은 아니다. 이러한 컴퓨팅 디바이스들 또는 어떠한 다른 사용자 디바이스들은 또한 메모리(RAM, ROM 등), (터치)스크린, I/O 수단, 카메라, 통신 수단 등을 포함할 수 있다. 제안된 방법은 또한, 디바이스의 프로세싱 또는 컴퓨팅 수단에 의해 실행되면, 디바이스가 자연스러운 고해상도 데이스케일 타임랩스를 생성하는 제안된 방법의 단계(들)을 수행하도록 하는 컴퓨터-실행 가능한 명령어들이 저 장된 컴퓨터-판독 가능한 매체 상에 구현될 수 있다. 임의의 타입의 데이터는 전술한 접근 방식을 이용하여 학 습된 인공지능 시스템에 의해 처리될 수 있다. 학습 단계는 오프라인에서 수행될 수 있다. 학습 또는 인퍼런스 동안 도메인 레이블들에 의존하지 않는 새로운 이미지-투-이미지 변환 모델이 개시된다. 새 로운 향상된 스킴은 변환 출력의 해상도를 높일 수 있다. 제안된 모델은 고해상도 풍경 이미지들에 대한 시간대 변환을 학습할 수 있다. 제안된 모델은 예를 들어, 꽃, 애완동물, 인간 등의 타임랩스 이미지들을 생성하기 위 해, 다른 도메인들에 용이하게 일반화될 수 있다. 다른 도메인들을 위해, 생성적 신경망이 가령, 꽃 이미지들의 학습 데이터셋, 애완동물 이미지들의 학습 데이터셋 및 인간 이미지들의 학습 데이터셋과 같이 대응하는 학습 데이터셋들에 대해 학습되어야 한다는 것을 당업자들에게 명확할 것이다. 개시된 모델은 이전 분포로부터 샘플링된 스타일 뿐만 아니라, 이미지들로부터 추출된 스타일을 이용하여 이미 지를 생성할 수 있다. 모델의 매력적이고 간단한 적용은 단일 이미지로부터 타임랩스를 생성하는 것이다(현재 주로 쌍을 이룬 데이터셋으로 처리되는 태스크). 본 명세서에 언급된 모든 기술적 효과가 본 기술의 모든 실시 예와 각 실시 예에서 향유될 필요가 없다는 점을 분명히 이해해야 한다. 예를 들어, 본 기술의 실시 예들은 사용자가 이러한 기술적 효과 중 일부를 향유하지 않 고 구현될 수 있는 반면, 다른 실시 예들은 사용자가 다른 기술적 효과를 향유하거나 전혀 향유하지 않고 구현 될 수 있다. 본 기술의 전술한 구현에 대한 수정 및 개선은 당업자에게 명백할 수 있다. 전술한 설명은 한정되는 것이 아니 라 예시적인 것이다. 따라서, 본 기술의 범위는 첨부된 청구항들의 범위에 의해서만 제한되는 것으로 의도되었 다. 전술한 구현들은 특정 순서로 수행되는 특정한 단계들을 참조하여 설명 및 도시되었지만, 이러한 단계들은 본 기술의 교시를 벗어나지 않고 결합, 세분화 또는 재정렬될 수 있다는 것이 이해될 것이다. 따라서, 단계들의 순 서 및 그룹화는 본 기술의 한정이 아니다."}
{"patent_id": "10-2022-7008530", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 발명의 상기 및 기타 예시, 특징, 및 효과는 첨부된 도면과 함께 설명된 다음의 상세한 설명으로부터 더욱 명백해질 것이다: 도 1은 본 명세서에 개시된 본 발명의 실시 예에 따른 학습된 생성적 신경망 및 학습된 머징 신경망을 사용하여 이미지로부터 자연스러운 데이스케일 타임랩스를 생성하는 방법의 흐름도를 도시한다. 도 2는 본 명세서에 개시된 본 발명의 실시 예에 따른 스왑 모드에서 생성적 신경망의 학습의 흐름도를 도시한 다. 도 3은 본 명세서에 개시된 본 발명의 실시 예에 따른 랜덤 모드에서 생성적 신경망의 학습의 흐름도를 도시한 다. 도 4는 본 명세서에 개시된 본 발명의 실시 예에 따른 오토인코더 모드에서 생성적 신경망의 학습의 흐름도를 도시한다. 도 5는 본 명세서에 개시된 본 발명의 실시 예에 따른 머징 신경망의 학습의 흐름도를 도시한다. 도 6은 본 명세서에 개시된 본 발명의 실시 예에 따른 방법을 수행할 수 있는 컴퓨팅 장치의 블록도를 도시한다. 도 7은 본 명세서에 개시된 본 발명의 실시 예에 따른 생성적 신경망의 가능한 구현에서의 데이터 흐름을 도시 한다. 도 8은 본 명세서에 개시된 본 발명의 실시 예에 따른 적응적 U-Net 아키텍쳐의 다이어그램을 도시한다. 도 9는 본 명세서에 개시된 본 발명의 실시 예에 따른 향상 스킴의 다이어그램을 도시한다. 도 10은 본 명세서에 개시된 본 발명의 실시 예에 따른 방법에 의해 생성된 자연스러운 데이스케일 타임랩스로 부터의 예시적인 이미지를 도시한다. 이하의 설명에서, 다른 설명이 없는 한 동일한 구성요소에 대하여 서로 다른 도면에 도시된 경우에는 동일한 참 조부호를 사용하고, 중복되는 설명은 생략하기로 한다."}
