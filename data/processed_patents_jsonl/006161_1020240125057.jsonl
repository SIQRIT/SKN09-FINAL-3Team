{"patent_id": "10-2024-0125057", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0053700", "출원번호": "10-2024-0125057", "발명의 명칭": "인공지능 기반의 모션 생성 장치 및 이의 제어 방법", "출원인": "주식회사 아이리브", "발명자": "이도희"}}
{"patent_id": "10-2024-0125057", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "텍스트 태깅(tagging)된 모션을 생성하는 모션 생성 장치에 있어서,적어도 하나의 명령어(instruction)을 저장하는 메모리; 및상기 메모리에 저장된 상기 적어도 하나의 명령어를 실행하는 적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는,캐릭터를 포함하는 애니메이션 데이터를 획득하고,상기 획득한 애니메이션 데이터를 복수의 프레임들에 걸친 상기 캐릭터의 모션을 생성하기 위한 중간 데이터로변환하고,상기 변환된 중간 데이터에 기초하여 상기 캐릭터의 모션을 생성하고,상기 생성된 캐릭터의 모션에 포함된 상기 복수의 프레임들 각각의 캡션(caption)을 생성하고,언어 모델(Language Model)에 상기 생성된 복수의 캡션들을 제공하여, 상기 캐릭터의 모션에 대응되는 텍스트를생성하고,상기 캐릭터의 모션에 상기 생성된 텍스트를 레이블링(labeling)하여 상기 텍스트 태깅된 모션을 생성하는 모션생성 장치."}
{"patent_id": "10-2024-0125057", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 중간 데이터는, SMPL(Simplified Human Motion Model) 기반의 데이터인 모션 생성 장치."}
{"patent_id": "10-2024-0125057", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 캐릭터의 모션은, 상기 복수의 프레임들 각각에서의 상기 캐릭터의 관절의 위치 정보, 관절의 회전 정보또는 바닥과 발의 접촉 정보 중 적어도 하나를 포함하는 모션 데이터와, 상기 모션 데이터를 상기 캐릭터에 리타게팅(retargeting)하여 렌더링한 모션 영상을 포함하는 모션 생성 장치."}
{"patent_id": "10-2024-0125057", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,상기 적어도 하나의 프로세서는,상기 생성된 모션 영상에 포함된 복수의 프레임 영상들을 추출하고,상기 추출된 복수의 프레임 영상들에 기초하여, 상기 복수의 프레임들 각각에 대응되는 상기 복수의 캡션들을생성하는 모션 생성 장치."}
{"patent_id": "10-2024-0125057", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서,상기 적어도 하나의 프로세서는,상기 생성된 캐릭터의 모션에 대응되는 텍스트를 다시 생성할 것을 요청하는 수정 신호를 획득함에 따라,상기 언어 모델에 상기 수정 신호에 포함된 정보 및 상기 생성된 복수의 캡션들을 제공하여 상기 생성된 캐릭터공개특허 10-2025-0053700-3-의 모션에 대응되는 텍스트를 다시 생성하는 모션 생성 장치."}
{"patent_id": "10-2024-0125057", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서,상기 생성된 텍스트 태깅된 모션은, 상기 모션 데이터를 제공받아 상기 텍스트를 추론하는 인공 지능 모델을 학습시키는 데 이용되는 학습 데이터인 모션 생성 장치."}
{"patent_id": "10-2024-0125057", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "텍스트 태깅(tagging)된 모션을 생성하는 모션 생성 장치의 동작 방법에 있어서,캐릭터를 포함하는 애니메이션 데이터를 획득하는 단계;상기 획득한 애니메이션 데이터를 복수의 프레임들에 걸친 상기 캐릭터의 모션을 생성하기 위한 중간 데이터로변환하는 단계;상기 변환된 중간 데이터에 기초하여 상기 캐릭터의 모션을 생성하는 단계;상기 생성된 캐릭터의 모션에 포함된 상기 복수의 프레임들 각각의 캡션(caption)을 생성하는 단계;언어 모델(Language Model)에 상기 생성된 복수의 캡션들을 제공하여, 상기 캐릭터의 모션에 대응되는 텍스트를생성하는 단계; 및상기 캐릭터의 모션에 상기 생성된 텍스트를 레이블링(labeling)하여 상기 텍스트 태깅된 모션을 생성하는 단계를 포함하는 모션 생성 장치의 동작 방법."}
{"patent_id": "10-2024-0125057", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서,상기 중간 데이터는, SMPL(Simplified Human Motion Model) 기반의 데이터이고,상기 캐릭터의 모션은, 상기 복수의 프레임들마다 상기 캐릭터의 관절의 위치 정보, 관절의 회전 정보 또는 바닥과 발의 접촉 정보 중 적어도 하나를 포함하는 모션 데이터와, 상기 모션 데이터를 상기 캐릭터에 리타게팅(retargeting)하여 렌더링한 모션 영상을 포함하는 모션 생성 장치의 동작 방법."}
{"patent_id": "10-2024-0125057", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 모션 생성 장치의 동작 방법은,상기 생성된 캐릭터의 모션에 대응되는 텍스트를 다시 생성할 것을 요청하는 수정 신호를 획득하는 단계; 및상기 언어 모델에 상기 수정 신호에 포함된 정보 및 상기 생성된 복수의 캡션들을 제공하여 상기 생성된 캐릭터의 모션에 대응되는 텍스트를 다시 생성하는 단계를 포함하는 모션 생성 장치의 동작 방법."}
{"patent_id": "10-2024-0125057", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "컴퓨터와 결합되어, 제7 내지 제9 항 중 어느 하나의 항의 방법을 실행시키기 위한 프로그램이 저장된 컴퓨터판독 가능한 기록매체."}
{"patent_id": "10-2024-0125057", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 실시예는, 텍스트 태깅(tagging)된 모션을 생성하는 모션 생성 장치 및 모션 생성 장치의 동작 방 법을 개시한다. 모션 생성 장치는 적어도 하나의 명령어(instruction)을 저장하는 메모리 및 메모리에 저장된 적 어도 하나의 명령어를 실행하는 적어도 하나의 프로세서를 포함하고, 적어도 하나의 프로세서는, 캐릭터를 포함 하는 애니메이션 데이터를 획득하고, 획득한 애니메이션 데이터를 복수의 프레임들에 걸친 캐릭터의 모션을 생성 하기 위한 중간 데이터로 변환하고, 변환된 중간 데이터에 기초하여 캐릭터의 모션을 생성하고, 생성된 캐릭터의 모션에 포함된 복수의 프레임들 각각의 캡션(caption)을 생성하고, 언어 모델(Language Model)에 생성된 복수의 캡션들을 제공하여 캐릭터의 모션에 대응되는 텍스트를 생성하고, 캐릭터의 모션에 생성된 텍스트를 레이블링 (labeling)하여 텍스트 태깅된 모션을 생성할 수 있다."}
{"patent_id": "10-2024-0125057", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 모션 생성 장치에 관한 것으로, 보다 구체적으로는 텍스트 태깅(tagging)된 모션을 생성하는 모션 생 성 장치 및 그의 동작 방법에 관한 것이다."}
{"patent_id": "10-2024-0125057", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 들어, 기술의 발전에 따라, 텍스트의 내용 또는 텍스트의 문맥 등을 고려하여, 텍스트로부터 특징을 추출 하여 아바타 등의 캐릭터의 모션을 추정하거나 혹은 텍스트에 포함된 동작을 수행하는 캐릭터의 모션을 생성하 는 기술이 이용되고 있다. 그러한 텍스트로부터 특징을 추출하여 캐릭터의 모션을 생성하는 기술은, 최근 발전하고 있는 인공 지능 모델을 이용할 수 있다. 그러나, 입력한 텍스트로부터 특징을 추출하여 캐릭터의 모션을 추론하기 위한 인공 지능 모델 을 개발하기 위하여는, 해당 동작을 수행하도록 인공 지능 모델을 학습시키는 과정이 필요하다. 이때, 인공 지 능 모델을 학습시키기 위하여는 텍스트와 대응되는 캐릭터의 모션이 레이벨링(labeling) 된 좋은 품질의 학습 데이터가 필요하다. 선행기술문헌 특허문헌 (특허문헌 0001) 공개번호 제10-2022-0133141 호 (공개일자: 2022년 10월 04일) 비특허문헌 (비특허문헌 0001) Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5152-5161, 2022. (비특허문헌 0002) Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big data, vol. 4, number 4, pages 236-252, 2016."}
{"patent_id": "10-2024-0125057", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "입력된 캐릭터의 모션에 대한 정보를 포함하는 텍스트로부터 특징을 추출하여, 캐릭터의 모션을 추론하기 위한 인공 지능 모델을 학습시키기 위한 학습용 데이터를 생성할 수 있다. 구체적으로, 게임이나 혹은 촬영 등을 통하여 획득한 애니메이션 데이터로부터 캐릭터의 모션을 추출하고, 언어 모델을 이용하여 추출한 캐릭터의 모션에 대응되는 텍스트를 생성한 후 캐릭터의 모션에 텍스트를 레이블링하여, 텍스트 태깅(tagging)된 모션을 포함하는 학습용 데이터를 생성할 수 있다. 또한, 언어 모델을 이용하여 추출한 캐릭터의 모션에 대응되는 텍스트를 생성하는 과정에서, 적절하지 않은 텍 스트가 생성된 경우 수정 요청을 통하여 재차 텍스트를 생성하도록 할 수 있다. 본 개시가 해결하고자 하는 과제들은 이상에서 언급된 과제로 제한되지 않으며, 언급되지 않은 또 다른 과제들 은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2024-0125057", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 본 개시에 따르면, 텍스트 태깅(tagging)된 모션을 생성하는 모션 생성 장 치가 제공될 수 있다. 모션 생성 장치는 적어도 하나의 명령어(instruction)을 저장하는 메모리를 포함할 수 있 다. 모션 생성 장치는 메모리에 저장된 적어도 하나의 명령어를 실행하는 적어도 하나의 프로세서를 포함할 수 있다. 적어도 하나의 프로세서는 캐릭터를 포함하는 애니메이션 데이터를 획득할 수 있다. 적어도 하나의 프로 세서는 획득한 애니메이션 데이터를 복수의 프레임들에 걸친 캐릭터의 모션을 생성하기 위한 중간 데이터로 변 환할 수 있다. 적어도 하나의 프로세서는 변환된 중간 데이터에 기초하여 캐릭터의 모션을 생성할 수 있다. 적 어도 하나의 프로세서는 생성된 캐릭터의 모션에 포함된 복수의 프레임들 각각 또는 일정 구간의 프레임들 마다 캡션(caption)을 생성할 수 있다. 적어도 하나의 프로세서는 언어 모델(Language Model)에 생성된 복수의 캡션 들을 제공하여, 캐릭터의 모션에 대응되는 텍스트를 생성할 수 있다. 적어도 하나의 프로세서는 캐릭터의 모션 에 생성된 텍스트를 레이블링(labeling)하여 텍스트 태깅된 모션을 생성할 수 있다. 또한, 본 개시에 따르면, 텍스트 태깅(tagging)된 모션을 생성하는 모션 생성 장치의 동작 방법이 개시될 수 있 다. 모션 생성 장치의 동작 방법은 캐릭터를 포함하는 애니메이션 데이터를 획득하는 단계를 포함할 수 있다. 모션 생성 장치의 동작 방법 획득한 애니메이션 데이터를 복수의 프레임들에 걸친 캐릭터의 모션을 생성하기 위 한 중간 데이터로 변환하는 단계를 포함할 수 있다. 모션 생성 장치의 동작 방법은 변환된 중간 데이터에 기초 하여 캐릭터의 모션을 생성하는 단계를 포함할 수 있다. 모션 생성 장치의 동작 방법은 생성된 캐릭터의 모션에 포함된 복수의 프레임들 각각의 캡션(caption)을 생성하는 단계를 포함할 수 있다. 모션 생성 장치의 동작 방법 은 언어 모델(Language Model)에 생성된 복수의 캡션들을 제공하여, 캐릭터의 모션에 대응되는 텍스트를 생성하 는 단계를 포함할 수 있다. 모션 생성 장치의 동작 방법은 캐릭터의 모션에 생성된 텍스트를 레이블링 (labeling)하여 텍스트 태깅된 모션을 생성하는 단계를 포함할 수 있다. 이 외에도, 본 개시를 구현하기 위한 방법을 실행하기 위한 컴퓨터 프로그램을 기록하는 컴퓨터 판독 가능한 기 록 매체가 더 제공될 수 있다."}
{"patent_id": "10-2024-0125057", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 전술한 과제 해결 수단에 의하면, 모션 생성 장치는 텍스트 태깅된 모션을 생성함에 있어서, 애니메 이션 데이터로부터 캐릭터의 모션을 추출하여, 학습용 데이터를 생성하기 위한 캐릭터의 모션을 쉽게 획득할 수 있다. 또한, 모션 생성 장치는 언어 모델을 이용하여 추출된 캐릭터의 모션으로부터 대응되는 텍스트를 생성함 으로써, 캐릭터의 모션에 대응되는 텍스트를 쉽게 획득할 수 있다. 이를 통하여 캐릭터의 모션에 텍스트가 레이블링된, 텍스트 태깅된 모션인 학습용 데이터를 쉽게 획득할 수 있 다. 또한, 수정 요청에 따라 캐릭터 모션에 대응되는 텍스트를 재차 생성할 수 있어, 높은 품질의 학습용 데이터를 획득할 수 있다. 본 개시의 효과들은 이상에서 언급된 효과로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재로 부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2024-0125057", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시 전체에 걸쳐 동일 참조 부호는 동일 구성요소를 지칭한다. 본 개시가 실시예들의 모든 요소들을 설명"}
{"patent_id": "10-2024-0125057", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "하는 것은 아니며, 본 개시가 속하는 기술분야에서 일반적인 내용 또는 실시예들 간에 중복되는 내용은 생략한 다. 명세서에서 사용되는 '부, 모듈, 부재, 블록'이라는 용어는 소프트웨어 또는 하드웨어로 구현될 수 있으며, 실시예들에 따라 복수의 '부, 모듈, 부재, 블록'이 하나의 구성요소로 구현되거나, 하나의 '부, 모듈, 부재, 블록'이 복수의 구성요소들을 포함하는 것도 가능하다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 제1, 제2 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하기 위해 사용되는 것으로, 구성요소가 전 술된 용어들에 의해 제한되는 것은 아니다. 단수의 표현은 문맥상 명백하게 예외가 있지 않는 한, 복수의 표현을 포함한다. 각 단계들에 있어 식별부호는 설명의 편의를 위하여 사용되는 것으로 식별부호는 각 단계들의 순서를 설명하는 것이 아니며, 각 단계들은 문맥상 명백하게 특정 순서를 기재하지 않는 이상 명기된 순서와 다르게 실시될 수 있다. 이하 첨부된 도면들을 참고하여 본 개시의 작용 원리 및 실시예들에 대해 설명한다. 본 명세서에서 '본 개시에 따른 장치'는 연산처리를 수행하여 사용자에게 결과를 제공할 수 있는 다양한 장치들 이 모두 포함된다. 예를 들어, 본 개시에 따른 장치는, 컴퓨터, 서버 장치 및 휴대용 단말기를 모두 포함하거나, 또는 어느 하나의 형태가 될 수 있다. 여기에서, 상기 컴퓨터는 예를 들어, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱(desktop), 랩톱 (laptop), 태블릿 PC, 슬레이트 PC 등을 포함할 수 있다. 상기 서버 장치는 외부 장치와 통신을 수행하여 정보를 처리하는 서버로써, 애플리케이션 서버, 컴퓨팅 서버, 데이터베이스 서버, 파일 서버, 게임 서버, 메일 서버, 프록시 서버 및 웹 서버 등을 포함할 수 있다. 상기 휴대용 단말기는 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), WiBro(Wireless Broadband Internet) 단말, 스마트 폰(Smart Phone) 등과 같은 모든 종류의 핸드헬드 (Handheld) 기반의 무선 통신 장치와 시계, 반지, 팔찌, 발찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head-mounted-device(HMD) 등과 같은 웨어러블 장치를 포함할 수 있다. 본 개시에 따른 인공지능과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등 과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인 공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인 공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미 한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도 형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 도 1은 본 개시의 일 실시예에 따른 모션 생성 장치의 구성을 도시한 블록도이다. 일 실시예에서, 도 1을 참조하면, 모션 생성 장치는 캐릭터의 모션과, 캐릭터의 모션에 대응되는 텍스트를 생성하고, 생성된 캐릭터의 모션에 생성된 캐릭터를 레이블링(labeling)하여 텍스트 태깅(tagging)된 모션을 생 성하는 장치일 수 있다. 일 실시예에서, 텍스트 태깅(tagging)된 모션을 생성하기 위하여, 모션 생성 장치는 캐릭터를 포함하는 애 니메이션 데이터를 획득하고, 획득된 애니메이션 데이터에 기초하여 캐릭터 모션을 생성할 수 있다. 일 실시예 에서, 모션 생성 장치는 거대 언어 모델(Large Language Model, LLM)을 이용하여, 생성된 캐릭터 모션에대응되는 텍스트를 생성할 수 있다. 일 실시예에서, 모션 생성 장치는 애니메이션 데이터에 기초하여 생성된 캐릭터 모션에, 캐릭터 모션에 대응하여 생성된 텍스트를 레이블링하여 텍스트 태깅된 모션을 생성할 수 있다. 이때, 캐릭터는 사람, 동물, 소설이나 만화의 등장인물, 아바타, 게임의 등장 인물 등 움직임을 가질 수 있는 객체일 수 있고, 어느 하나로 제한되지 않는다. 캐릭터의 모션은 복수의 프레임들마다 캐릭터의 관절의 위치 정 보, 관절의 회전 정보 또는 바닥과 발의 접촉 정보 중 적어도 하나를 포함하는 모션 데이터를 포함할 수 있다. 캐릭터의 모션은 모션 데이터를 캐릭터에 리타게팅(retargeting)하여 렌더링하여 획득된, 복수의 프레임들에 걸 친 모션 영상을 포함할 수 있다. 일 실시예에서, 캐릭터의 모션에 대응되는 텍스트는, 캐릭터의 움직임이나 위 치, 포즈 등을 나타내는 모션을 설명하는 문장일 수 있다. 이때, 모션 생성 장치는 거대 언어 모델에 모션 영상을 제공하여, 모션 영상에 대응되는 텍스트를 생성할 수 있다. 일 실시예에서, 텍스트 태깅된 모션은 모션 영상에 대응되는 텍스트와 모션 데이터의 세트로 이루어지 는 데이터일 수 있다. 일 실시예에서, 텍스트 태깅된 모션은 모션 데이터를 제공받아 모션 데이터에 대응되는 텍스트를 추론하는 인공 지능 모델을 학습시키는 데 이용되는 학습 데이터로 사용될 수 있다. 도 1을 참조하면, 일 실시예에서, 모션 생성 장치는 메모리, 적어도 하나의 프로세서, 입/출력 인터페이스부 및 통신 인터페이스부를 포함할 수 있다. 다만, 도 1에 도시된 구성 요소들은 본 개시 에 따른 모션 생성 장치를 구현하는데 필수적인 것은 아니다. 일 실시예에서, 본 명세서 상에서 설명되는 모션 생성 장치는 위에서 열거된 구성 요소들보다 많거나, 혹은 적은 구성 요소들을 가질 수도 있다. 메모 리, 적어도 하나의 프로세서, 입/출력 인터페이스부 및 통신 인터페이스부는 각각 전기적 및/또는 물리적으로 서로 연결될 수 있다. 일 실시예에서, 메모리는 모션 생성 장치의 다양한 기능을 지원하는 데이터와, 적어도 하나의 프로세 서의 동작을 위한 프로그램을 저장할 수 있고, 입/출력되는 데이터들(예를 들어, 문장, 음악 파일, 정지 영상, 동영상 등)을 저장할 있고, 본 장치에서 구동되는 다수의 응용 프로그램(application program 또는 애플 리케이션(application)), 본 장치의 동작을 위한 적어도 하나의 데이터들, 적어도 하나의 명령어(instruction) 를 저장할 수 있다. 일 실시예에서, 메모리에는 텍스트 태깅(tagging)된 모션을 생성하는 동작을 수행하기 위한 모션 렌더링 모듈 및 텍스트 태깅 모듈이 포함될 수 있다. 모션 렌더링 모듈은 데이터 변환 모듈 및 모 션 생성 모듈이 포함될 수 있다. 메모리에 포함되는 '모듈'은 적어도 하나의 프로세서에 의해 수행되는 기능이나 동작을 처리하는 단위를 의미할 수 있다. 메모리에 포함되는 '모듈'은 명령어들 (instructions), 알고리즘, 또는 프로그램 코드와 같은 소프트웨어로 구현될 수 있다. 이러한 응용 프로그램 중 적어도 일부는, 무선 통신을 통해 외부 서버로부터 다운로드 될 수 있다. 일 실시예에서, 텍스트 태깅 모듈은 캐릭터 모션에 대응하는 텍스트를 생성하도록 미리 학습된 인공 지능 모델을 포함할 수 있다. 인공 지능 모델은 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치 들 간의 연산을 통해 신경 망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지 능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스 (loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN: Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network), 트랜스포머 (Transformer), 심층 Q-네트워크 (Deep Q-Networks) 또는 부스팅(Boosting) 알고리즘 등이 있으나, 전술한 예 에 한정되지 않는다. 텍스트 태깅 모듈은 해당 동작을 수행하기 위하여 미리 학습된 모델을 전이 학습(Transfer Learning) 및 파인 튜닝을 통하여 최적화된 인공 지능 모델을 포함할 수도 있다. 이러한, 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), SSD 타입 (Solid State Disk type), SDD 타입(Silicon Disk Drive type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(random access memory; RAM), SRAM(static random access memory), 롬(read-only memory; ROM), EEPROM(electrically erasable programmable read-only memory), PROM(programmable read-only memory), 자기 메모리, 자기 디스크 및 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 또한, 메모리는 모션 생성 장치와는 분리되어 있 으나, 유선 또는 무선으로 연결된 데이터베이스가 될 수도 있다. 일 실시예에서, 적어도 하나의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로 세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세 서일 수 있다. 적어도 하나의 프로세서는 메모리에 저장된 명령어들 또는 또는 인공지능 모델에 따라, 환자 관 련 정보를 처리하도록 제어한다. 또는, 적어도 하나의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전 용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 모션 생성 장치의 전반적인 동작들을 제어할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 메모리에 저장된 적어도 하나 이상의 명령어를 실행하여, 모 션 생성 장치의 동작을 제어할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 메모리에 포 함된 데이터 변환 모듈, 모션 생성 모듈 및 텍스트 태깅 모듈의 적어도 하나의 명령어들 또는 프로그램 코드를 실행함으로써 텍스트 태깅된 모션을 생성하는 동작을 수행할 수 있다. 일 실시예에서, 입/출력 인터페이스부는 외부로부터 정보를 입력받거나, 혹은 외부로 정보를 제공하기 위 한 것으로서, 입/출력 인터페이스를 통해 캐릭터를 포함하는 애니메이션 데이터가 입력되면, 적어도 하나 의 프로세서는 애니메이션 데이터에 기초하여 캐릭터의 모션을 생성하도록 모션 생성 장치를 제어할 수 있다. 또한, 모션 생성 장치는 생성된 텍스트 태깅된 모션을 입/출력 인터페이스부를 통하여 주변 전자 장치로 제공할 수도 있다. 일 실시예에서 통신 인터페이스부는 외부의 서버 또는 주변의 다른 전자 장치들과 모션 생성 장치 간 의 데이터 통신을 수행할 수 있다. 일 실시예에서, 통신 인터페이스부는 외부의 서버 또는 주변의 다른 전자 장치들과 통신을 가능하게 하는 하나 이상의 구성 요소를 포함할 수 있으며, 예를 들어, 무선 통신 모듈, 근거리 통신 모듈, 위치 정보 모듈 중 적어도 하나를 포함할 수 있다. 무선 통신 모듈은 와이파이(Wifi) 모듈, 와이브로(Wireless broadband) 모듈 외에도, GSM(global System for Mobile Communication), CDMA(Code Division Multiple Access), WCDMA(Wideband Code Division Multiple Access), UMTS(universal mobile telecommunications system), TDMA(Time Division Multiple Access), LTE(Long Term Evolution), 4G, 5G, 6G 등 다양한 무선 통신 방식을 지원하는 무선 통신 모듈을 포함할 수 있 다. 무선 통신 모듈은 데이터 신호를 송신하는 안테나 및 송신기(Transmitter)를 포함하는 무선 통신 인터페이스를 포함할 수 있다. 또한, 무선 통신 모듈은 적어도 하나의 프로세서의 제어에 따라 무선 통신 인터페이스를 통해 적어도 하나의 프로세서로부터 출력된 디지털 제어 신호를 아날로그 형태의 무선 신호로 변조하는 데 이터 신호 변환 모듈을 더 포함할 수 있다. 무선 통신 모듈은 데이터 신호를 수신하는 안테나 및 수신기(Receiver)를 포함하는 무선 통신 인터페이스를 포 함할 수 있다. 또한, 무선 통신 모듈은 무선 통신 인터페이스를 통하여 수신한 아날로그 형태의 무선 신호를 디 지털 제어 신호로 복조하기 위한 데이터 신호 변환 모듈을 더 포함할 수 있다. 근거리 통신 모듈은 근거리 통신(Short range communication)을 위한 것으로서, 블루투스(Bluetooth™), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), UWB(Ultra Wideband), ZigBee, NFC(Near Field Communication), Wi-Fi(Wireless-Fidelity), Wi-Fi Direct, Wireless USB(Wireless Universal Serial Bus) 기술 중 적어도 하나를 이용하여, 근거리 통신을 지원할 수 있다. 일 실시예에서, 모션 생성 장치는 통신 인터페이스부를 통하여 외부의 서버 또는 주변의 전자 장치와 통신을 수행할 수 있다. 일 실시예에서, 외부의 서버는 애플리케이션 서버, 컴퓨팅 서버, 데이터베이스 서버, 파일 서버, 게임 서버, 메일 서버, 프록시 서버 및 웹 서버 등을 포함할 수 있다. 일 실시예에서, 외부의 서버 또는 주변의 전자 장치에, 본 개시에 설명하는 캐릭터 모션에 대응하는 텍스트를 생성하는 방법을 포함하는 모델이 포함되어 있을 수도 있다. 모션 생성 장치는 입/출력 인터페이스부(15 0)를 통하여 외부의 서버 또는 주변의 전자 장치로부터 캐릭터 모션에 대응하는 텍스트를 생성하도록 미리 학습 된 인공 지능 모델을 제공받을 수도 있다. 이하, 설명의 편의를 위하여, 캐릭터 모션에 대응하는 텍스트를 생성하는 동작은 모션 생성 장치에서 이루 어지는 것으로 설명한다. 도 2는 본 개시의 일 실시예에 따른 모션 생성 장치의 동작을 설명하기 위한 순서도이다. 도 1 및 도 2를 참조하면, 일 실시예에서, 모션 생성 장치의 동작 방법은 캐릭터를 포함하는 애니메이션 데이터를 획득하는 단계(S100)를 포함할 수 있다. 일 실시예에서, 애니메이션 데이터를 획득하는 단계(S100)에 서, 적어도 하나의 프로세서는 입/출력 인터페이스부 또는 통신 인터페이스부를 통하여 애니메 이션 데이터를 획득할 수 있다. 다만, 본 개시는 이에 제한되지 않고, 적어도 하나의 프로세서는 메모리 에 이미 저장되어 있는 애니메이션 데이터를 읽어올 수도 있다. 일 실시예에서, 모션 생성 장치의 동작 방법은 획득한 애니메이션 데이터를 복수의 프레임들에 걸친 캐릭 터의 모션을 생성하기 위한 중간 데이터로 변환하는 단계(S200)를 포함할 수 있다. 일 실시예에서, 애니메이션 데이터를 중간 데이터로 변환하는 단계(S200)에서 적어도 하나의 프로세서는 데이터 변환 모듈의 명 령어들 또는 프로그램 코드를 실행함으로써, 애니메이션 데이터를 중간 데이터로 변환할 수 있다. 이하, 애니메 이션 데이터를 중간 데이터로 변환하는 동작에 대하여는 도 3 및 도 4에서 후술하도록 한다. 일 실시예에서, 모션 생성 장치의 동작 방법은 변환된 중간 데이터에 기초하여 캐릭터의 모션을 생성하는 단계(S300)를 포함할 수 있다. 일 실시예에서, 캐릭터의 모션을 생성하는 단계(S300)에서, 적어도 하나의 프로 세서는 모션 생성 모듈의 명령어들 또는 프로그램 코드를 실행함으로써, 중간 데이터에 기초하여 캐 릭터의 모션을 생성할 수 있다. 이하, 중간 데이터에 기초하여 캐릭터의 모션을 생성하는 동작에 대하여는 도 3 및 도 5에서 후술하도록 한다. 일 실시예에서, 모션 생성 장치의 동작 방법은 생성된 캐릭터의 모션에 포함된 복수의 프레임 영상들을 추 출하는 단계(S400)를 포함할 수 있다. 일 실시예에서, 캐릭터의 모션에 포함된 복수의 프레임 영상들을 추출하 는 단계(S400)에서, 적어도 하나의 프로세서는 텍스트 태깅 모듈의 명령어들 또는 프로그램 코드를 실행함으로써, 캐릭터 모션에 포함된 복수의 프레임들 영상들을 추출할 수 있다. 이하, 생성된 캐릭터의 모션에 포함된 복수의 프레임 영상들을 추출하는 동작에 대하여는 도 6 및 도 8에서 후술하도록 한다. 일 실시예에서, 모션 생성 장치의 동작 방법은 추출된 복수의 프레임 영상들 각각에 대응되는 캡션 (caption)을 생성하는 단계(S500)를 포함할 수 있다. 일 실시예에서, 복수의 프레임 영상들 각각에 대응되는 캡 션(caption)을 생성하는 단계(S500)에서, 적어도 하나의 프로세서는 텍스트 태깅 모듈의 명령어들 또 는 프로그램 코드를 실행함으로써, 복수의 프레임 영상들 각각에 대응되는 캡션을 생성할 수 있다. 이하, 복수 의 프레임 영상들 각각에 대응되는 캡션을 생성하는 동작에 대하여는 도 6 및 도 9에서 후술하도록 한다. 일 실시예에서, 도 2에는 캐릭터 모션에 포함된 복수의 프레임 영상들을 추출하는 단계(S400)와 복수의 프레임 영상들 각각에 대응되는 캡션을 생성하는 단계(S500)가 구분되어 도시되어 있지만, 본 개시는 이에 제한되지 않 는다. 일 실시예에서, 캐릭터 모션에 포함된 복수의 프레임 영상들을 추출하는 동작과 복수의 프레임 영상들 각 각에 대응되는 캡션을 생성하는 단계는 하나의 단계에서 수행될 수도 있다. 일 실시예에서, 모션 생성 장치의 동작 방법은 언어 모델(Language Model)에 생성된 복수의 캡션들을 제공 하여, 캐릭터의 모션에 대응되는 텍스트를 생성하는 단계(S600)를 포함할 수 있다. 일 실시예에서, 캐릭터의 모 션에 대응되는 텍스트를 생성하는 단계(S600)에서, 적어도 하나의 프로세서는 텍스트 태깅 모듈의 명 령어들 또는 프로그램 코드를 실행함으로써, 언어 모델(Language Model)에 생성된 복수의 캡션들을 제공하여, 캐릭터의 모션에 대응되는 텍스트를 생성할 수 있다. 이하, 캐릭터의 모션에 대응되는 텍스트를 생성하는 동작 에 대하여는 도 6 및 도 10에서 후술하도록 한다. 일 실시예에서, 모션 생성 장치의 동작 방법은 캐릭터의 모션에 생성된 텍스트를 레이블링(labeling)하여, 텍스트 태깅(tagging)된 모션을 생성하는 단계를 포함할 수 있다. 일 실시예에서, 텍스트 태깅된 모션을 생성하 는 단계에서, 적어도 하나의 프로세서는 텍스트 태깅 모듈의 명령어들 또는 프로그램 코드를 실행함 으로써, 캐릭터의 모션에 생성된 텍스트를 레이블링(labeling)하여, 텍스트 태깅된 모션을 생성할 수 있다. 이 하, 텍스트 태깅된 모션을 생성하는 동작에 대하여는 도 6 및 도 10에서 후술하도록 한다. 일 실시예에서, 언어 모델은 거대 언어 모델을 의미하는 것일 수 있다. 또한, 복수의 캡션들을 제공하여, 캐릭 터의 모션에 대응되는 텍스트를 생성하는 동작과 캐릭터의 모션에 생성된 텍스트를 레이블링(labeling)하여, 텍 스트 태깅(tagging)된 모션을 생성하는 동작은 하나의 단계에서 수행될 수도 있다. 일 실시예에서, 모션 생성 장치의 동작 방법은 생성된 캐릭터 모션에 대응되는 텍스트를 다시 생성할 것을 요청하는 수정 신호를 획득하는 단계를 더 포함할 수도 있다. 일 실시예에서, 수정 신호를 획득하는 단계에서, 적어도 하나의 프로세서는 생성된 캐릭터의 모션에 대응되는 텍스트를 다시 생성할 것을 요청하는 수정 신 호를 획득할 수 있다. 일 실시예에서, 모션 생성 장치의 동작 방법은 언어 모델에 수정 신호에 포함된 정보 및 생성된 복수의 캡 션들을 제공하여, 생성된 캐릭터의 모션에 대응되는 텍스트를 다시 생성하는 단계를 더 포함할 수도 있다. 일 실시예에서, 캐릭터의 모션에 대응되는 텍스트를 다시 생성하는 단계에서, 적어도 하나의 프로세서는 언어 모델에 수정 신호에 포함된 정보 및 생성된 복수의 캡션들을 제공하여, 생성된 캐릭터의 모션에 대응되는 텍스 트를 다시 생성할 수 있다. 이하, 수정 신호를 획득하는 동작 및 텍스트를 다시 생성하는 동작에 대하여는 도 6 및 도 10에서 후술하도록 한다. 도 3은 본 개시의 일 실시예에 따른 캐릭터의 모션을 생성하는 동작을 설명하기 위한 도면이다. 도 1 및 도 3을 참조하면, 일 실시예에서, 적어도 하나의 프로세서는 애니메이션 데이터를 획득할 수 있다. 일 실시예에서, 애니메이션 데이터는 복수의 프레임들에 걸쳐 캐릭터의 움직임에 대한 정보를 데이 터일 수 있다. 일 실시예에서, 애니메이션 데이터는 특정 게임 내에서의 캐릭터의 모션에 대한 데이터이거 나, 혹은 모션을 캡쳐하여 획득된 데이터일 수 있으나, 어느 하나로 제한되지 않는다. 일 실시예에서, 적어도 하나의 프로세서는 모션 렌더링 모듈의 명령어들 또는 프로그램 코드를 실행 함으로써, 애니메이션 데이터로부터 캐릭터의 모션을 생성할 수 있다. 일 실시예에서, 캐릭터의 모션은 복 수의 프레임들 각각에서의 캐릭터의 관절의 위치 정보, 캐릭터의 관절의 회전 정보 또는 캐릭터의 발이 바닥에 닿았는지 여부에 대한 바닥과 발의 접촉 정보 중 적어도 하나를 포함하는 모션 데이터를 포함할 수 있다. 캐릭터의 모션은 모션 데이터를 캐릭터에 리타게팅(retargeting)하여 렌더링한 모션 영상을 포함할 수 있 다. 구체적으로, 적어도 하나의 프로세서는 데이터 변환 모듈의 명령어들 또는 프로그램 코드를 실행함으 로써, 획득한 애니메이션 데이터를 모션 생성 모듈에서 이용하기 위하여, 중간 데이터로 변환할 수 있다. 적어도 하나의 프로세서는 중간 데이터를 이용하여 모션 생성 모듈을 통하여 캐릭터의 모션을 생성할 수 있다. 도 4는 본 개시의 일 실시예에 따른 데이터 변환 모듈의 동작을 설명하기 위한 도면이다. 이하, 도 3에서 설명 한 구성과 동일한 구성에 대하여는 동일한 도면 부호를 부여하고, 중복되는 설명은 생략하도록 한다. 도 1, 도 3 및 도 4를 참조하면, 일 실시예에서, 적어도 하나의 프로세서는 데이터 변환 모듈을 통하 여 획득한 애니메이션 데이터를 SMPL(Skinned Multi-Person Linear Model)에 기반하는 중간 데이터로 변 환할 수 있다. 이때, SMPL은 모션 생성 모듈에서 이용되는 데이터 타입일 수 있다. 다만, 본 개시는 이에 제한되지 않고, 필요에 따라 애니메이션 데이터는 SMPL-H(SMPL + Hands 파라미터) 또는 SMPL-X(SMPL + Hands + Face 파라미터) 기반의 중간 데이터로 변환될 수도 있다. 구체적으로, 적어도 하나의 프로세서는 애니메이션 데이터에 포함된 스켈레톤(예를 들어 캐릭터의 스 켈레톤)의 IK(Inverse Kinematics) Rig(Rigging)를 세팅할 수 있다. 적어도 하나의 프로세서는 SMPL-H 기 반의 스켈레톤의 IK-Rig를 세팅할 수 있다. 적어도 하나의 프로세서는 세팅된 SMPL-H 기반의 스켈레톤의 IK-Rig에 애니메이션 데이터에 포함된 스켈레톤의 IK-Rig를 리타겟팅(retargeting)할 수 있다. 이를 토대 로, 적어도 하나의 프로세서는 애니메이션 데이터의 데이터를 SMPL-H 기반의 스켈레톤에 대한 애니메 이션 데이터로 변환할 수 있다. 이때, SMPL-H 기반의 스켈레톤에 대한 애니메이션 데이터가 애니메이 션 데이터가 변환된 중간 데이터일 수 있다. 도 5는 본 개시의 일 실시예에 따른 모션 생성 모듈의 동작을 설명하기 위한 도면이다. 이하, 도 3 및 도 4에서 설명한 구성과 동일한 구성에 대하여는 동일한 도면 부호를 부여하고, 중복되는 설명은 생략하도록 한다. 도 1, 도 3 및 도 5를 참조하면, 적어도 하나의 프로세서는 모션 생성 모듈의 명령어들 또는 프로그 램 코드를 실행함으로써, 변환된 중간 데이터로부터 캐릭터 모션에 포함된 모션 데이터 및 모션 영상 을 생성할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 변환된 중간 데이터를 재생할 수 있다(500, 520). 이때, 적어도 하나의 프로세서는 변환된 중간 데이터를 순차적으로 재생하여, 1회차 재생 시에는 모션데이터를 생성하고, 2회차 재생 시에는 모션 영상을 생성할 수 있다. 일 실시예에서, 1회차 재생 시, 적어도 하나의 프로세서는 변환된 중간 데이터에 포함된 복수의 프레 임들 각각에서의 모션 데이터를 추출할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 매 프레 임에서의 SMPL-H 기반의 스켈레톤의 관절의 위치 정보(joint position), 관절의 회전 정보(joint rotation) 및 발이 바닥에 닿았는지 여부에 대한 정보(foot contact)를 추출할 수 있다. 이때, 추출되는 정보는 2D 또는 3D의 정보일 수 있다. 추출되는 정보가 3D일 경우, 스켈레톤의 관절의 위치 정 보(joint position)는 스켈레톤이 포함된 세계(world) 기준에서의 위치를 포함할 수 있다. 스켈레톤의 관절의 회전 정보(joint rotation)는 기준 축(x축, y축 또는 z축)에서의 local 회전 정보를 포함할 수 있다. 발이 바닥 에 닿았는지 여부에 대한 정보(foot contact)는 양쪽 발의 앞꿈치 및 발꿈치와 바닥간의 충돌 판정 알고리즘을 통하여 계산된 총 4개의 정보를 포함할 수 있다. 일 실시예에서, 1회차 재생 시, 적어도 하나의 프로세서는 변환된 중간 데이터에 포함된 복수의 프레 임들 각각에서의 모션 데이터를 추출하여 모션 데이터를 생성할 수 있다. 모션 데이터는 이때, 모션 데이터는 Json(Javascript object notation) 타입으로 생성될 수 있다. Json 타입으로 생성된 모션 데이터는 각각의 스켈레톤 관절 및 위치 정보, 회전 정보 및 바닥의 접촉 여부를 key-value의 쌍으로 저장 할 수 있다. 일 실시예에서, 2회차 재생 시, 적어도 하나의 프로세서는 추출된 모션 데이터를 캐릭터에 리타겟팅 하여 렌더링할 수 있다. 추출된 모션 데이터이 리타겟된 캐릭터를 렌더링함에 따라, 추출된 모션 데 이터에 포함된 스켈레톤의 관절의 위치 정보(joint position), 관절의 회전 정보(joint rotation) 및 발 이 바닥에 닿았는지 여부에 대한 정보(foot contact) 등에 기초하여 캐릭터의 모션이 재생될 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 재생되는 캐릭터의 모션을 저장하여 모션 영상을 생성할 수 있다. 이때, 적어도 하나의 프로세서는 재생되는 캐릭터 모션을 녹화하여 모션 녹화 영상을 모션 영상으로서 생성할 수 있다. 이때, 모션 영상은 MPEG(Moving Picture Expers Group-4)의 일부인 mp4 의 타입으로 생성될 수도 있으나, 본 개시는 이에 제한되지 않고, 다양한 타입의 동영상 파일로 생성될 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 모션 생성 모듈을 이용하여, 모션 데이터와 모션 영 상의 세트로 이루어지는 캐릭터의 모션을 생성할 수 있다. 도 6은 본 개시의 일 실시예에 따른 캐릭터의 모션에 대응되는 텍스트를 생성하고, 텍스트 태깅된 모션을 생성 하는 동작을 설명하기 위한 도면이다. 도 1 및 도 6을 참조하면, 일 실시예에서, 도 6에는 적어도 하나의 프로세서가 텍스트 태깅 모듈을 이용하여 캐릭터의 모션에 대응되는 텍스트를 생성하고, 캐릭터의 모션에 생성된 텍스트를 레이 블링하는 동작이 도시되어 있다. 이하, 설명의 편의를 위하여 캐릭터의 모션은 모션 데이터로 지칭하 여 설명하도록 한다. 일 실시예에서, 적어도 하나의 프로세서는 모션 데이터의 정보를 이용하여, 영상을 녹화할 수 있다. 이때, 적어도 하나의 프로세서는 3D 게임 엔진을 이용하여 모션 데이터를 렌더링하여 구현되는 것을 녹화하여 영상을 생성할 수 있다. 이때, 모션 데이터를 렌더링하여 구현되는 것을 녹화하 여 생성된 영상은, 도 5에 도시된 모션 영상으로 대체될 수도 있다. 이 경우, 적어도 하나의 프로세서 는 모션 데이터를 렌더링하여 녹화하는 동작 없이, 모션 영상을 이용할 수도 있다. 이하, 설명 의 편의를 위하여, 적어도 하나의 프로세서는 모션 영상을 이용하여 캐릭터의 모션에 대응되는 텍스 트를 생성하는 것으로 설명한다. 일 실시예에서, 모션 영상은 복수의 프레임들에 걸쳐서 재생되는 영상일 수 있다. 적어도 하나의 프로세서 는 모션 영상에 포함된 복수의 프레임 영상들을 추출할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 추출된 복수의 프레임 영상들 각각에서, 캡션(caption)을 생성 할 수 있다. 이때, 캡션은 각각의 프레임 영상의 내용 등을 설명하기 위한 텍스트일 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 거대 언어 모델에 복수의 캡션들을 제공하여, 복수의 프레 임들에 걸친 모션 영상에 대응되는 텍스트를 생성할 수 있다. 이때, 거대 언어 모델은 언어 모델로 지칭될 수도 있다. 이때, 텍스트를 다시 생성할 것을 요청하는 수정 신호를 획득하는 경우, 생성된 텍스트가 모션 영상 에 대응되지 않는다고 판단되어 수정 신호에 포함된 정보 및 모션 영상을 이용하여 재차 텍스트(65 0)를 생성할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 모션 영상에 텍스트를 레이블링하여 텍스트 태깅된 모션을 생성할 수 있다. 또한, 적어도 하나의 프로세서는 모션 영상에 대응되는 모션 데이터(310, 도 5 참조)에 텍스트를 레이블링하여 텍스트 태깅된 모션을 생성할 수도 있다. 이때, 모션 데이터와 텍 스트가 레이블링된 텍스트 태깅된 모션은, 특정 모션 데이터를 제공받아 대응되는 특정 텍스트를 추론하는 인공 지능 모델을 학습시키기 위한 학습 데이터로 이용될 수도 있다. 모션 렌더링 모듈 및 텍스트 태깅 모듈을 통하여 생성된 텍스트 태깅된 모션을 학습 데이터로 이용할 수 있음에 따라, 특정 모션 데이터를 제공받아 대응되는 특정 텍스트를 추론하는 인공 지능 모델을 학습시키기 위한 학습 데이터의 획득이 용이해지고, 다양한 학습 데이터를 획득할 수 있어 상기 추론 성능이 향상된 인공 지능 모델을 획득할 수도 있다. 도 7는 본 개시의 일 실시예에 따른 캐릭터의 모션을 생성하는 동작을 설명하기 위한 도면이다. 이하, 도 6에서 설명한 구성과 동일한 구성에 대하여는 동일한 도면 부호를 부여하고, 중복되는 설명은 생략하도록 한다. 도 1, 도 6 및 도 7을 참조하면, 일 실시예에서, 적어도 하나의 프로세서는 모션 데이터를 캐릭터에 리타겟팅할 수 있다. 이때, 캐릭터는 2D 캐릭터 또는 3D 캐릭터를 포함할 수 있다. 캐릭터는 애니메이션 데이터(300, 도 4 참조)에 포함된 캐릭터이거나 혹은 새로운 캐릭터로 지정될 수도 있다. 일 실시예에서, 적어도 하나의 프로세서는 모션 데이터가 리타겟팅된 캐릭터를 렌더링하여, 캐 릭터가 움직이는 영상을 생성할 수 있다. 이때, 적어도 하나의 프로세서는 생성되는 영상을 녹화하여, 복 수의 프레임들 동안 캐릭터의 움직임을 나타내는 모션 영상을 생성할 수 있다. 도 8은 본 개시의 일 실시예에 따른 모션 영상에 포함된 복수의 프레임들을 추출하는 동작을 설명하기 위한 도 면이다. 이하, 도 6에서 설명한 구성과 동일한 구성에 대하여는 동일한 도면 부호를 부여하고, 중복되는 설명은 생략하도록 한다. 도 1, 도 6 및 도 8을 참조하면, 일 실시예에서, 적어도 하나의 프로세서는 모션 영상을 구성하는 복 수의 프레임 영상들을 추출할 수 있다. 모션 영상은 복수의 프레임들에 걸쳐 캐릭터의 모션을 나타내 는 것으로, 적어도 하나의 프로세서는 모션 영상으로부터 각각의 프레임에 해당하는 프레임 영상을 추출할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 추출된 복수의 프레임 영상들 각각의 FPS(Frames Per Second)을 조정하여, 복수의 프레임 영상들이 공통된 프레임 간격을 갖도록 할 수 있다. 도 9는 본 개시의 일 실시예에 따른 복수의 프레임들 각각에 대응되는 복수의캡션들을 생성하는 동작을 설명하 기 위한 도면이다. 이하, 도 6에서 설명한 구성과 동일한 구성에 대하여는 동일한 도면 부호를 부여하고, 중복 되는 설명은 생략하도록 한다. 도 1, 도 6 및 도 9를 참조하면, 일 실시예에서, 적어도 하나의 프로세서는 이미지 기반 캡션 생성 모델 을 이용하여, 복수의 프레임 영상들 각각에 대한 캡션을 생성할 수 있다. 이때, 이미지 기반 캡션 생 성 모델은 컴퓨터 비전(Computer Vision) 모델과 자연어 처리(Natural Language Processing) 모델을 포함 할 수 있다. 이미지 기반 캡션 생성 모델은 하나의 프레임에 해당하는 영상을 제공받아, 해당 영상의 캡션 을 추론하도록 미리 학습된 인공 지능 모델을 포함할 수 있다. 일 실시예에서, 이미지 기반 캡션 생성 모델 은 CNN (Convolutional Neural Network), RNN (Recurrent Neural Network) 또는 트랜스포머 (Transformer) 등을 포함할 수 있으나, 이에 제한되지는 않는다. 일 실시예에서, 적어도 하나의 프로세서는 복수의 프레임 영상들 각각에 대하여 이미지 기반 캡션 생 성 모델을 이용하여 캡션을 생성하는 동작을 반복하여, 복수의 캡션들을 생성할 수 있다. 적어도 하 나의 프로세서는 복수의 프레임 영상들 각각에 기초하여, 복수의 프레임들 각각에 대응되는 복수의 캡션들을 생성할 수 있다. 도 10은 본 개시의 일 실시예에 따른 캐릭터의 모션에 대응되는 텍스트를 생성하고, 생성된 텍스트를 수정하는 동작을 설명하기 위한 도면이다. 이하, 도 6에서 설명한 구성과 동일한 구성에 대하여는 동일한 도면 부호를 부 여하고, 중복되는 설명은 생략하도록 한다. 도 1, 도 6 및 도 10을 참조하면, 일 실시예에서, 적어도 하나의 프로세서는 복수의 캡션들을 이용하 여, 복수의 프레임들에 걸친 모션 영상의 움직임 정보를 이해하고, 모션 영상에 대응되는 텍스트 를 생성할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 캡션 정보 통합 언어 모델 모듈을 이용하여, 복수의 캡션 들로부터 모션 영상의 전반적인 내용을 포함하는 전체 캡션을 추출할 수 있다. 일 실시예에서, 캡션 정보 통합 언어 모델 모듈은 각각의 프레임 영상에 대한 캡션인 복수의 캡션들 각각을 통합하여, 복수의 프레임들에 걸친 모션 영상의 내용에 대응되는 전체 캡션을 생성하는 모듈일 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 레이블 생성 언어 모델 모듈을 이용하여, 전체 캡션에 대 응되는 텍스트를 생성할 수 있다. 적어도 하나의 프로세서는 전체 캡션을 레이블 생성 언어 모델 모 듈에 제공하여, 모션 영상에 대응되는 텍스트를 생성할 수 있다. 이때, 적어도 하나의 프로세서는 목표 레이블 개수를 설정하고, 레이블 생성 언어 모델 모듈을 이용 하여 전체 캡션에 대응되는 텍스트를 목표 레이블 개수만큼 생성할 수 있다. 이 경우, 모션 영상에 포함된 캐릭터의 움직임을 다양한 관점에서 설명하는 복수의 텍스트들을 생성할 수 있다. 이를 통하여, 특정 모 션 데이터를 제공받아 대응되는 특정 텍스트를 추론하는 인공 지능 모델을 학습시키기 위한 학습 데이터를 생성 하는데 있어, 하나의 모션 데이터에 대응되는 복수의 텍스트들을 이용하여 복수의 학습 데이터 세트를 생 성할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 모션 영상에 대응되는 텍스트를 생성한 이후에, 텍스 트를 다시 생성할 것을 요청하는 수정 신호를 획득할 수도 있다. 이때, 수정 신호는 생성된 텍스트 와 모션 데이터를 비교하여, 생성된 텍스트가 모션 데이터에 대응되지 않는다고 판단되어 생성 된 신호일 수 있다. 일 실시예에서, 수정 신호는 미리 설정된 모션 데이터를 나타낸다고 판단된 적어도 하 나의 단어와 텍스트를 비교하여, 상기 적어도 하나의 단어가 텍스트에 포함되지 않는다고 판단되는 경우 생성되는 신호일 수 있다. 또한, 적어도 하나의 프로세서는 입/출력 인터페이스부나 통신 인터 페이스부에 의하여 수정 신호를 획득할 수도 있다. 일 실시예에서, 적어도 하나의 프로세서는 수정 신호에 포함된 신호 및 복수의 캡션들을 언어 모델 에 제공하여, 모션 데이터에 대응되는 텍스트를 다시 생성할 수도 있다. 이러한 과정을 통하여, 언어 모델에 포함된 파라미터가 개선되어, 언어 모델의 성능이 좋아질 수도 있다. 일 실시예에서, 적어도 하나의 프로세서는 생성된 텍스트를 모션 데이터에 레이블링하여, 텍스 트 태깅된 모션을 생성할 수 있다. 한편, 개시된 실시예들은 컴퓨터에 의해 실행 가능한 명령어를 저장하는 기록매체의 형태로 구현될 수 있다. 명 령어는 프로그램 코드의 형태로 저장될 수 있으며, 프로세서에 의해 실행되었을 때, 프로그램 모듈을 생성하여 개시된 실시예들의 동작을 수행할 수 있다. 기록매체는 컴퓨터로 읽을 수 있는 기록매체로 구현될 수 있다. 컴퓨터가 읽을 수 있는 기록매체로는 컴퓨터에 의하여 해독될 수 있는 명령어가 저장된 모든 종류의 기록 매체 를 포함한다. 예를 들어, ROM(Read Only Memory), RAM(Random Access Memory), 자기 테이프, 자기 디스크, 플 래쉬 메모리, 광 데이터 저장장치 등이 있을 수 있다."}
{"patent_id": "10-2024-0125057", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상에서와 같이 첨부된 도면을 참조하여 개시된 실시예들을 설명하였다. 본 개시가 속하는 기술분야에서 통상 의 지식을 가진 자는 본 개시의 기술적 사상이나 필수적인 특징을 변경하지 않고도, 개시된 실시예들과 다른 형 태로 본 개시가 실시될 수 있음을 이해할 것이다. 개시된 실시예들은 예시적인 것이며, 한정적으로 해석되어서 는 안 된다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10"}
{"patent_id": "10-2024-0125057", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 모션 생성 장치의 구성을 도시한 블록도이다. 도 2는 본 개시의 일 실시예에 따른 모션 생성 장치의 동작을 설명하기 위한 순서도이다. 도 3은 본 개시의 일 실시예에 따른 캐릭터의 모션을 생성하는 동작을 설명하기 위한 도면이다. 도 4는 본 개시의 일 실시예에 따른 데이터 변환 모듈의 동작을 설명하기 위한 도면이다. 도 5는 본 개시의 일 실시예에 따른 모션 생성 모듈의 동작을 설명하기 위한 도면이다. 도 6은 본 개시의 일 실시예에 따른 캐릭터의 모션에 대응되는 텍스트를 생성하고, 텍스트 태깅된 모션을 생성 하는 동작을 설명하기 위한 도면이다. 도 7는 본 개시의 일 실시예에 따른 캐릭터의 모션을 생성하는 동작을 설명하기 위한 도면이다. 도 8은 본 개시의 일 실시예에 따른 모션 영상에 포함된 복수의 프레임들을 추출하는 동작을 설명하기 위한 도 면이다. 도 9는 본 개시의 일 실시예에 따른 복수의 프레임들 각각에 대응되는 복수의캡션들을 생성하는 동작을 설명하 기 위한 도면이다. 도 10은 본 개시의 일 실시예에 따른 캐릭터의 모션에 대응되는 텍스트를 생성하고, 생성된 텍스트를 수정하는 동작을 설명하기 위한 도면이다."}
