{"patent_id": "10-2021-0140826", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0056959", "출원번호": "10-2021-0140826", "발명의 명칭": "자연어 처리 장치 및 방법", "출원인": "삼성에스디에스 주식회사", "발명자": "이현제"}}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "문서 내 텍스트에 스타일 정보를 가지는 문서를 수집하는 수집 모듈;상기 수집한 문서의 텍스트에서 스타일 정보를 추출하고, 추출한 스타일 정보를 상기 텍스트와 매칭하여 레이블링 하는 전처리 모듈; 및상기 스타일 정보가 레이블링 된 텍스트를 입력 받고, 입력 받은 텍스트에서 상기 스타일 정보를 가지는 단어의위치를 예측하는 제1 머신 러닝 모듈을 포함하는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서, 상기 스타일 정보는, 텍스트 색, 크기, 스타일, 및 폰트 중 하나 이상을 포함하는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서, 상기 전처리 모듈은, 상기 텍스트에서 스타일 정보를 가지는 부분의 시작 위치 및 끝 위치에 기반하여 상기 레이블링을 수행하는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 3에 있어서, 상기 제1 머신 러닝 모듈은, 입력되는 텍스트에 대해 토큰화를 수행하여 기 설정된 단위의 토큰들을 생성하는 토큰화부;상기 각 토큰들을 임베딩 벡터로 변환하는 인코딩부;상기 각 토큰에 대한 임베딩 벡터를 입력 받고, 상기 임베딩 벡터로부터 상기 텍스트 내 스타일 정보의 시작 위치가 될 확률을 예측하는 제1 예측부; 및상기 각 토큰에 대한 임베딩 벡터를 입력 받고, 상기 임베딩 벡터로부터 상기 텍스트 내 스타일 정보의 끝 위치가 될 확률을 예측하는 제2 예측부를 포함하는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서, 상기 제1 머신 러닝 모듈은, 상기 제1 예측부가 예측한 값과 상기 스타일 정보의 시작 위치에 대한 정답 값 간의 차이가 최소화 되도록하고, 상기 제2 예측부가 예측한 값과 상기 스타일 정보의 끝 위치에 대한 정답 값 간의 차이가 최소화 되도록학습되는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 5에 있어서, 상기 제1 머신 러닝 모듈을 구성하는 인공 신경망의 손실 함수(Loss)는 Cross Entropy Loss를 적용하여 하기의공개특허 10-2023-0056959-3-수학식으로 표현되는, 자연어 처리 장치.(수학식)Start' : 제1 예측부가 예측한 값Start : 스타일 정보의 시작 위치에 대한 정답 값End' : 제2 예측부가 예측한 값End : 스타일 정보의 끝 위치에 대한 정답 값"}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 5에 있어서, 상기 제1 머신 러닝 모듈은, 상기 제1 예측부가 예측한 값과 상기 제2 예측부가 예측한 값에 기반하여 상기 임베딩 벡터들을 필터링하는 필터부를 더 포함하는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 7에 있어서, 상기 필터부는, 상기 제1 예측부가 예측한 값과 상기 제2 예측부가 예측한 값에 기반하여 상기 임베딩 벡터들 중 기 설정된 임계값 이상의 임베딩 벡터를 스타일 관련 임베딩 벡터로 추출하는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 8에 있어서, 상기 자연어 처리 장치는, 상기 스타일 관련 임베딩 벡터를 이용하여 입력되는 문서 내 텍스트를 요약하는 제2 머신 러닝 모듈을 더 포함하는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 9에 있어서, 상기 제2 머신 러닝 모듈은, 입력되는 텍스트에 대해 토큰화를 수행하여 기 설정된 단위의 토큰들을 생성하는 토큰화부;상기 각 토큰들을 임베딩 벡터로 변환하는 인코딩부;상기 스타일 관련 임베딩 벡터를 상기 인코딩부에서 출력되는 벡터들 중 그에 대응하는 임베딩 벡터와 더하는합산부; 및상기 합산부에서 출력되는 임베딩 벡터에 기반하여 상기 텍스트의 요약된 내용을 출력하는 디코딩부를포함하는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "에 있어서, 상기 제2 머신 러닝 모듈의 인코딩부 및 디코딩부는, 상기 제1 머신 러닝 모듈의 학습된 인코딩부의 가중치 값으로 초기화 되는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "문서 내 텍스트에 스타일 정보를 가지는 문서를 수집하는 수집 모듈;상기 수집한 문서의 텍스트에서 스타일 정보를 추출하고, 추출한 스타일 정보를 상기 텍스트와 매칭하여 레이블링 하는 전처리 모듈; 상기 스타일 정보가 레이블링 된 텍스트를 입력 받고, 입력 받은 텍스트에서 상기 스타일 정보를 가지는 단어의위치를 예측하는 제1 머신 러닝 모듈; 및상기 제1 머신 러닝 모듈에서 예측된 결과에 기반하여 입력되는 문서 내 텍스트를 요약하는 제2 머신 러닝 모듈을 포함하는, 자연어 처리 장치."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "수집 모듈에서, 문서 내 텍스트에 스타일 정보를 가지는 문서를 수집하는 단계;전처리 모듈에서, 상기 수집한 문서의 텍스트에서 스타일 정보를 추출하고, 추출한 스타일 정보를 상기 텍스트와 매칭하여 레이블링 하는 단계; 및제1 머신 러닝 모듈에서, 상기 스타일 정보가 레이블링 된 텍스트를 입력 받고, 입력 받은 텍스트에서 상기 스타일 정보를 가지는 단어의 위치를 예측하는 단계를 포함하는, 자연어 처리 방법."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "청구항 14에 있어서, 상기 스타일 정보는, 텍스트 색, 크기, 스타일, 및 폰트 중 하나 이상을 포함하는, 자연어 처리 방법."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "청구항 14에 있어서, 상기 레이블링 하는 단계는, 상기 텍스트에서 스타일 정보를 가지는 부분의 시작 위치 및 끝 위치에 기반하여 상기 레이블링을 수행하는, 자연어 처리 방법."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "청구항 16에 있어서, 상기 단어의 위치를 예측하는 단계는, 토큰화부에서, 입력되는 텍스트에 대해 토큰화를 수행하여 기 설정된 단위의 토큰들을 생성하는 단계;인코딩부에서, 상기 각 토큰들을 임베딩 벡터로 변환하는 단계;제1 예측부에서, 상기 각 토큰에 대한 임베딩 벡터를 입력 받고, 상기 임베딩 벡터로부터 상기 텍스트 내 스타일 정보의 시작 위치가 될 확률을 예측하는 단계; 및제2 예측부에서, 상기 각 토큰에 대한 임베딩 벡터를 입력 받고, 상기 임베딩 벡터로부터 상기 텍스트 내 스타일 정보의 끝 위치가 될 확률을 예측하는 단계를 포함하는, 자연어 처리 방법."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "공개특허 10-2023-0056959-5-청구항 17에 있어서, 상기 단어의 위치를 예측하는 단계는, 상기 제1 머신 러닝 모듈에서, 상기 제1 예측부가 예측한 값과 상기 스타일 정보의 시작 위치에 대한 정답 값간의 차이가 최소화 되도록 하고, 상기 제2 예측부가 예측한 값과 상기 스타일 정보의 끝 위치에 대한 정답 값간의 차이가 최소화 되도록 학습하는 단계를 더 포함하는, 자연어 처리 방법."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "청구항 18에 있어서, 상기 자연어 처리 방법은, 필터부에서, 상기 제1 예측부가 예측한 값과 상기 제2 예측부가 예측한 값에 기반하여 상기 임베딩 벡터들을 필터링하는 단계를 더 포함하는, 자연어 처리 방법."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "청구항 19에 있어서, 상기 필터링하는 단계는, 상기 필터링부에서, 상기 제1 예측부가 예측한 값과 상기 제2 예측부가 예측한 값에 기반하여 상기 임베딩 벡터들 중 기 설정된 임계값 이상의 임베딩 벡터를 스타일 관련 임베딩 벡터로 추출하는 단계를 포함하는, 자연어처리 방법."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "청구항 20에 있어서, 상기 자연어 처리 방법은, 제2 머신 러닝 모듈에서, 상기 스타일 관련 임베딩 벡터를 이용하여 입력되는 문서 내 텍스트를 요약하는 단계를 더 포함하는, 자연어 처리 방법."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "청구항 21에 있어서, 상기 텍스트를 요약하는 단계는, 토큰화부에서, 입력되는 텍스트에 대해 토큰화를 수행하여 기 설정된 단위의 토큰들을 생성하는 단계;인코딩부에서, 상기 각 토큰들을 임베딩 벡터로 변환하는 단계;합산부에서, 상기 스타일 관련 임베딩 벡터를 상기 인코딩부에서 출력되는 벡터들 중 그에 대응하는 임베딩 벡터와 더하는 단계; 및디코딩부에서, 상기 합산부에서 출력되는 임베딩 벡터에 기반하여 상기 텍스트의 요약된 내용을 출력하는 단계를 포함하는, 자연어 처리 방법."}
{"patent_id": "10-2021-0140826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "청구항 22에 있어서, 상기 자연어 처리 방법은, 상기 합산부에서, 상기 인코딩부에서 출력되는 임베딩 벡터들 중 상기 스타일 관련 벡터와 대응하지 않는 임베딩 벡터는 상기 디코딩부로 전달하는 단계; 및상기 합산부에서, 상기 인코딩부에서 출력되는 임베딩 벡터들 중 상기 스타일 관련 벡터와 대응하는 임베딩 벡터는 상기 스타일 관련 임베딩 벡터와 더한 후 상기 디코딩부로 전달하는 단계를 더 포함하는, 자연어 처리 방법.공개특허 10-2023-0056959-6-"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "자연어 처리 장치 및 방법이 개시된다. 개시되는 일 실시예에 따른 자연어 처리 장치는, 문서 내 텍스트에 스타 일 정보를 가지는 문서를 수집하는 수집 모듈, 수집한 문서의 텍스트에서 스타일 정보를 추출하고, 추출한 스타 일 정보를 텍스트와 매칭하여 레이블링 하는 전처리 모듈, 및 스타일 정보가 레이블링 된 텍스트를 입력 받고, 입력 받은 텍스트에서 스타일 정보를 가지는 단어의 위치를 예측하는 제1 머신 러닝 모듈한다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "개시되는 실시예들은 자연어 처리 기술과 관련된다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "컴퓨터 과학에서, 자연어 이해(Natural Language Understanding: NLU)란 사람이 일반적으로 의사소통에 사용하 는 자연어(예를 들어, 한국어, 일본어, 영어 등)로 구성된 문장을 컴퓨터가 입력 받고, 입력된 문장의 의도 (Intention)를 추론하는 것을 의미한다. 컴퓨터 상에서 자연어를 이해하기 위한 기술은 여러 가지가 존재하지만, 최근에는 머신 러닝 기반의 인공 지능 모델을 이용한 기술이 주로 연구되고 있다. 한편, 종래의 딥러닝 언어 모델을 활용한 비정형 텍스트 분석 기술은 주로 텍스트가 가지고 있는 의미적 정보에 만 의존하여 진행하였다. 하지만, 문서에는 글자 외에 다양한 형태의 정보를 가지고 있으며, 이는 문서의 이해 도 향상에 매우 중요한 역할을 하는 경우가 많이 있다. 예를 들어, 밑줄이 그어져 있거나 볼드(bold)체 처리된 텍스트는 그 문서에서 가장 중요한 내용을 담고 있을 확률이 높다. 따라서, 텍스트의 글자가 가지고 있는 사전적 또는 의미론적 정보 외에 글자의 형태가 가지고 있는 스타일 정보 를 이용하여 문서 내 중요한 정보를 유추할 수 있는 방안이 요구된다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 등록특허공보 제10-2264899호(2021.06.11. 공개)"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "개시되는 실시예들은 문서 내 텍스트에서 스타일 정보를 갖는 단어의 위치를 예측할 수 있는 자연어 처리 장치 및 방법을 제공하기 위한 것이다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "개시되는 실시예들은 문서 내 텍스트의 스타일 정보를 반영하여 텍스트의 내용을 요약할 수 있는 자연어 처리 장치 및 방법을 제공하기 위한 것이다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 자연어 처리 장치는, 문서 내 텍스트에 스타일 정보를 가지는 문서를 수집하는 수집 모듈; 상 기 수집한 문서의 텍스트에서 스타일 정보를 추출하고, 추출한 스타일 정보를 상기 텍스트와 매칭하여 레이블링 하는 전처리 모듈; 및 상기 스타일 정보가 레이블링 된 텍스트를 입력 받고, 입력 받은 텍스트에서 상기 스타일 정보를 가지는 단어의 위치를 예측하는 제1 머신 러닝 모듈을 포함한다. 상기 스타일 정보는, 텍스트 색, 크기, 스타일, 및 폰트 중 하나 이상을 포함할 수 있다. 상기 전처리 모듈은, 상기 텍스트에서 스타일 정보를 가지는 부분의 시작 위치 및 끝 위치에 기반하여 상기 레 이블링을 수행할 수 있다. 상기 제1 머신 러닝 모듈은, 입력되는 텍스트에 대해 토큰화를 수행하여 기 설정된 단위의 토큰들을 생성하는 토큰화부; 상기 각 토큰들을 임베딩 벡터로 변환하는 인코딩부; 상기 각 토큰에 대한 임베딩 벡터를 입력 받고, 상기 임베딩 벡터로부터 상기 텍스트 내 스타일 정보의 시작 위치가 될 확률을 예측하는 제1 예측부; 및 상기 각 토큰에 대한 임베딩 벡터를 입력 받고, 상기 임베딩 벡터로부터 상기 텍스트 내 스타일 정보의 끝 위치가 될 확률을 예측하는 제2 예측부를 포함할 수 있다. 상기 제1 머신 러닝 모듈은, 상기 제1 예측부가 예측한 값과 상기 스타일 정보의 시작 위치에 대한 정답 값 간 의 차이가 최소화 되도록 하고, 상기 제2 예측부가 예측한 값과 상기 스타일 정보의 끝 위치에 대한 정답 값 간의 차이가 최소화 되도록 학습될 수 있다. 상기 제1 머신 러닝 모듈을 구성하는 인공 신경망의 손실 함수(Loss)는 Cross Entropy Loss를 적용하여 하기의 수학식으로 표현될 수 있다. (수학식)"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "Start' : 제1 예측부가 예측한 값 Start : 스타일 정보의 시작 위치에 대한 정답 값 End' : 제2 예측부가 예측한 값 End : 스타일 정보의 끝 위치에 대한 정답 값 상기 제1 머신 러닝 모듈은, 상기 제1 예측부가 예측한 값과 상기 제2 예측부가 예측한 값에 기반하여 상기 임 베딩 벡터들을 필터링하는 필터부를 더 포함할 수 있다. 상기 필터부는, 상기 제1 예측부가 예측한 값과 상기 제2 예측부가 예측한 값에 기반하여 상기 임베딩 벡터들 중 기 설정된 임계값 이상의 임베딩 벡터를 스타일 관련 임베딩 벡터로 추출할 수 있다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "상기 자연어 처리 장치는, 상기 스타일 관련 임베딩 벡터를 이용하여 입력되는 문서 내 텍스트를 요약하는 제2 머신 러닝 모듈을 더 포함할 수 있다. 상기 제2 머신 러닝 모듈은, 입력되는 텍스트에 대해 토큰화를 수행하여 기 설정된 단위의 토큰들을 생성하는 토큰화부; 상기 각 토큰들을 임베딩 벡터로 변환하는 인코딩부; 상기 스타일 관련 임베딩 벡터를 상기 인코딩부 에서 출력되는 벡터들 중 그에 대응하는 임베딩 벡터와 더하는 합산부; 및 상기 합산부에서 출력되는 임베딩 벡"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 4, "content": "터에 기반하여 상기 텍스트의 요약된 내용을 출력하는 디코딩부를 포함할 수 있다. 상기 합산부는, 상기 인코딩부에서 출력되는 임베딩 벡터들 중 상기 스타일 관련 벡터와 대응하지 않는 임베딩 벡터는 상기 디코딩부로 전달하고, 상기 인코딩부에서 출력되는 임베딩 벡터들 중 상기 스타일 관련 벡터와 대 응하는 임베딩 벡터는 상기 스타일 관련 임베딩 벡터와 더한 후 상기 디코딩부로 전달할 수 있다. 상기 제2 머신 러닝 모듈의 인코딩부 및 디코딩부는, 상기 제1 머신 러닝 모듈의 학습된 인코딩부의 가중치 값 으로 초기화 될 수 있다. 다른 실시예에 따른 자연어 처리 장치는, 문서 내 텍스트에 스타일 정보를 가지는 문서를 수집하는 수집 모듈; 상기 수집한 문서의 텍스트에서 스타일 정보를 추출하고, 추출한 스타일 정보를 상기 텍스트와 매칭하여 레이블 링 하는 전처리 모듈; 상기 스타일 정보가 레이블링 된 텍스트를 입력 받고, 입력 받은 텍스트에서 상기 스타일 정보를 가지는 단어의 위치를 예측하는 제1 머신 러닝 모듈; 및 상기 제1 머신 러닝 모듈에서 예측된 결과에 기"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 5, "content": "반하여 입력되는 문서 내 텍스트를 요약하는 제2 머신 러닝 모듈을 포함한다. 일 실시예에 따른 자연어 처리 방법은, 수집 모듈에서, 문서 내 텍스트에 스타일 정보를 가지는 문서를 수집하 는 단계; 전처리 모듈에서, 상기 수집한 문서의 텍스트에서 스타일 정보를 추출하고, 추출한 스타일 정보를 상 기 텍스트와 매칭하여 레이블링 하는 단계; 및 제1 머신 러닝 모듈에서, 상기 스타일 정보가 레이블링 된 텍스 트를 입력 받고, 입력 받은 텍스트에서 상기 스타일 정보를 가지는 단어의 위치를 예측하는 단계를 포함한다. 상기 스타일 정보는, 텍스트 색, 크기, 스타일, 및 폰트 중 하나 이상을 포함할 수 있다. 상기 레이블링 하는 단계는, 상기 텍스트에서 스타일 정보를 가지는 부분의 시작 위치 및 끝 위치에 기반하여 상기 레이블링을 수행할 수 있다. 상기 단어의 위치를 예측하는 단계는, 토큰화부에서, 입력되는 텍스트에 대해 토큰화를 수행하여 기 설정된 단 위의 토큰들을 생성하는 단계; 인코딩부에서, 상기 각 토큰들을 임베딩 벡터로 변환하는 단계; 제1 예측부에서, 상기 각 토큰에 대한 임베딩 벡터를 입력 받고, 상기 임베딩 벡터로부터 상기 텍스트 내 스타일 정보의 시작 위 치가 될 확률을 예측하는 단계; 및 제2 예측부에서, 상기 각 토큰에 대한 임베딩 벡터를 입력 받고, 상기 임베 딩 벡터로부터 상기 텍스트 내 스타일 정보의 끝 위치가 될 확률을 예측하는 단계를 포함할 수 있다. 상기 단어의 위치를 예측하는 단계는, 상기 제1 머신 러닝 모듈에서, 상기 제1 예측부가 예측한 값과 상기 스타 일 정보의 시작 위치에 대한 정답 값 간의 차이가 최소화 되도록 하고, 상기 제2 예측부가 예측한 값과 상기 스 타일 정보의 끝 위치에 대한 정답 값 간의 차이가 최소화 되도록 학습하는 단계를 더 포함할 수 있다. 상기 자연어 처리 방법은, 필터부에서, 상기 제1 예측부가 예측한 값과 상기 제2 예측부가 예측한 값에 기반하 여 상기 임베딩 벡터들을 필터링하는 단계를 더 포함할 수 있다. 상기 필터링하는 단계는, 상기 필터링부에서, 상기 제1 예측부가 예측한 값과 상기 제2 예측부가 예측한 값에 기반하여 상기 임베딩 벡터들 중 기 설정된 임계값 이상의 임베딩 벡터를 스타일 관련 임베딩 벡터로 추출하는 단계를 포함할 수 있다. 상기 자연어 처리 방법은, 제2 머신 러닝 모듈에서, 상기 스타일 관련 임베딩 벡터를 이용하여 입력되는 문서"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 6, "content": "내 텍스트를 요약하는 단계를 더 포함할 수 있다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 7, "content": "상기 텍스트를 요약하는 단계는, 토큰화부에서, 입력되는 텍스트에 대해 토큰화를 수행하여 기 설정된 단위의 토큰들을 생성하는 단계; 인코딩부에서, 상기 각 토큰들을 임베딩 벡터로 변환하는 단계; 합산부에서, 상기 스 타일 관련 임베딩 벡터를 상기 인코딩부에서 출력되는 벡터들 중 그에 대응하는 임베딩 벡터와 더하는 단계; 및"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 8, "content": "디코딩부에서, 상기 합산부에서 출력되는 임베딩 벡터에 기반하여 상기 텍스트의 요약된 내용을 출력하는 단계 를 포함할 수 있다. 상기 자연어 처리 방법은, 상기 합산부에서, 상기 인코딩부에서 출력되는 임베딩 벡터들 중 상기 스타일 관련 벡터와 대응하지 않는 임베딩 벡터는 상기 디코딩부로 전달하는 단계; 및 상기 합산부에서, 상기 인코딩부에서 출력되는 임베딩 벡터들 중 상기 스타일 관련 벡터와 대응하는 임베딩 벡터는 상기 스타일 관련 임베딩 벡터와 더한 후 상기 디코딩부로 전달하는 단계를 더 포함할 수 있다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "개시되는 실시예들에 따르면, 제1 머신 러닝 모듈에서 텍스트 내에서 스타일 정보를 가지는 부분의 위치를 예측"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "하도록 학습하고, 제2 머신 러닝 모듈에서 이를 이용하여 텍스트 내용을 요약하도록 함으로써, 텍스트 내 중요"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "한 키워드 및 문장을 파악할 수 있게 되고 결과적으로 문서의 요약 성능을 높일 수 있게 된다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 발명의 구체적인 실시형태를 설명하기로 한다. 이하의 상세한 설명은 본 명세서에서 기술된 방법, 장치 및/또는 시스템에 대한 포괄적인 이해를 돕기 위해 제공된다. 그러나 이는 예시에 불과하며 본 발명은 이에 제한되지 않는다. 본 발명의 실시예들을 설명함에 있어서, 본 발명과 관련된 공지기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명을 생략하기로 한다. 그리고, 후술되는 용어들은 본 발명에서의 기능을 고려하여 정의된 용어들로서 이는 사용자, 운용자의 의도 또는 관례 등에 따라 달라질 수 있다. 그러므로 그 정의는 본 명세서 전반에 걸친 내용을 토대로 내려져야 할 것이다. 상세한 설명에서 사용되 는 용어는 단지 본 발명의 실시예들을 기술하기 위한 것이며, 결코 제한적이어서는 안 된다. 명확하게 달리 사 용되지 않는 한, 단수 형태의 표현은 복수 형태의 의미를 포함한다. 본 설명에서, \"포함\" 또는 \"구비\"와 같은 표현은 어떤 특성들, 숫자들, 단계들, 동작들, 요소들, 이들의 일부 또는 조합을 가리키기 위한 것이며, 기술된 것 이외에 하나 또는 그 이상의 다른 특성, 숫자, 단계, 동작, 요소, 이들의 일부 또는 조합의 존재 또는 가능성을 배제하도록 해석되어서는 안 된다. 또한, 제1, 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로 사용될 수 있다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성 요소는 제2 구성 요소로 명 명될 수 있고, 유사하게 제2 구성 요소도 제1 구성 요소로 명명될 수 있다. 도 1은 본 발명의 일 실시예에 따른 머신 러닝 기반의 자연어 처리 장치의 구성을 나타낸 블록도이다. 도 1을 참조하면, 자연어 처리 장치는 수집 모듈, 전처리 모듈, 제1 머신 러닝 모듈, 및 제2 머신 러닝 모듈을 포함할 수 있다. 수집 모듈은 텍스트를 포함하는 문서들을 수집한다. 수집 모듈은 제1 머신 러닝 모듈 및 제2 머 신 러닝 모듈에 구비된 머신 러닝 모델들을 학습하기 위한 문서들을 수집할 수 있다. 예시적인 실시예에서, 수집 모듈은 웹 크롤링(Web Crawling)을 통해 대량의 HTML(Hyper Text Markup Language) 문 서 또는 XML(eXtensible Markup Language) 문서 등을 수집할 수 있다. 수집 모듈은 문서 내 텍스트에 스 타일 정보를 가지는 문서를 수집할 수 있다. 구체적으로, 수집 모듈은 각종 URL(Uniform Resource Locator)을 수집하고, 통신 네트워크를 통해 각 URL 에 접근하여 해당 URL의 웹 페이지를 구성하는 HTML 문서를 수집할 수 있다. 수집 모듈은 수집한 HTML 문 서를 저장할 수 있다. 이와 같이, 웹 크롤링을 통해 웹 문서들을 수집하고 이를 학습용 데이터로 사용하게 되면, 별도의 전문가가 문 서 내 중요한 내용을 표시하는 스타일 작업을 수행하지 않아도 되는 바, 제1 머신 러닝 모듈 및 제2 머신 러닝 모듈을 학습하는데 소요되는 시간 및 비용을 줄일 수 있게 된다. 전처리 모듈은 수집 모듈이 수집한 문서 내 텍스트에서 스타일 정보를 추출할 수 있다. 여기서, 스타 일 정보는 텍스트의 외면적 형태를 의미하는 것으로, 스타일 정보는 다음과 같은 것이 있을 수 있다. 1) 텍스트 색 : 텍스트의 배경 색(background color)과 텍스트 자체의 색 2) 텍스트 크기 : 폰트 사이즈(font-size)로 표현되는 텍스트의 크기 3) 텍스트 스타일 : 텍스트에 밑줄이 그어져 있거나 볼드체로 표현되어 있는가에 대한 정보 4) 텍스트 폰트 : 문서 내에서 다른 텍스트들과 다른 폰트를 사용하는가에 대한 정보 전처리 모듈은 수집 모듈이 수집한 문서가 태그를 가지는 HTML(또는 XML) 문서인 경우, HTML 문서에 서 HTML 코드를 추출하고 태깅(tagging) 작업을 수행하여 문서 내 텍스트에서 스타일 정보를 추출할 수 있다. 도 2는 본 발명의 일 실시예에서 HTML 태그에 기반하여 문서 내 텍스트의 스타일 정보를 추출하는 상태를 나타 낸 도면이다. 전처리 모듈은 추출한 스타일 정보를 해당 텍스트와 매칭하여 레이블링(labeling) 할 수 있다. 예시적인 실시예에서, 전처리 모듈은 텍스트에서 스타일 정보를 가지는 부분의 위치(예를 들어, 시작 위치 및 끝 위 치)에 기반하여 레이블링을 수행할 수 있다. 예를 들어, \"오늘 날씨는 흐리고 소나기가 내리겠습니다.\"라는 문장에서 \"소나기\"라는 단어가 밑줄이라는 스타 일 정보를 가지고 있으며 해당 문장에서 \"소나기\"라는 단어의 시작 위치는 12번째이고, 끝 위치는 14번째이므로, 전처리 모듈은 해당 문장에 대해 {12, 14}라는 레이블링을 수행할 수 있다. 제1 머신 러닝 모듈은 입력되는 문서 내 텍스트에서 스타일 정보를 가지는 단어의 위치를 예측하는 작업을 수행할 수 있다. 즉, 제1 머신 러닝 모듈은 입력되는 문서 내 텍스트에서 스타일 정보를 가지는 단어의 위 치를 예측하도록 학습되는 인공 신경망 모델을 구비할 수 있다. 도 3은 본 발명의 일 실시예에서 제1 머신 러닝 모듈의 구성을 나타낸 도면이다. 도 3을 참조하면, 제1 머 신 러닝 모듈은 토큰화부, 인코딩부, 제1 예측부, 및 제2 예측부를 포함할 수 있다. 여기서, 토큰화부, 인코딩부, 제1 예측부, 및 제2 예측부는 인공 신경망으로 이루어질 수 있다. 이때, 토큰화부, 인코딩부, 제1 예측부, 및 제2 예측부는 하나의 인공 신경망으로 구성될 수도 있고 여러 개의 인공 신경망으로 구성될 수도 있다.토큰화부는 입력되는 텍스트에 대해 토큰화(Tokenization)를 수행하여 기 설정된 단위의 토큰들을 생성할 수 있다. 예시적인 실시예에서, 토큰화부는 입력되는 텍스트에 대해 단어 단위로 토큰화를 수행하여 토큰 들을 생성할 수 있다. 도 3에서와 같이, 입력되는 텍스트가 \"코로나 확진자 대부분이 백신 미접종자 였다고 당국은 밝혔다\"인 경우, 토큰화부는 입력되는 텍스트를 \"코로나\", \"확진자\", \"대부분이\", \"백신\", \"미접종자\", \"였다고\", \"당국 은\", \"밝혔다\"와 같이 각 단어 단위로 토큰화를 수행하여 토큰들을 생성할 수 있다. 여기서, 해당 텍스트의 스 타일 정보는 예측 값과 정답 값을 비교하기 위한 레이블로 사용될 뿐, 제1 머신 러닝 모듈로 입력되지는 않는다. 인코딩부는 토큰화부에서 출력되는 각 토큰들을 임베딩 벡터(embedding vector)로 변환할 수 있다. 임베딩 벡터는 각 토큰을 기 설정된 차원의 벡터 표현으로 변환한 것일 수 있다. 인코딩부는 기 공지된 다 양한 워드 임베딩 기법을 이용하여 각 토큰으로부터 기 설정된 차원의 임베딩 벡터를 생성할 수 있다. 예를 들 어, 인코딩부는 LSTM(Long Short Term Memory), GRU(Gated Recurrent Unit), BERT(Bidirectional Encoder Representations from Transformers) 등과 같은 인공 신경망 모델이 사용될 수 있으나, 이에 한정되는 것은 아니다. 제1 예측부는 인코딩부로부터 각 토큰에 대한 임베딩 벡터를 입력 받을 수 있다. 제1 예측부는 입력되는 각 임베딩 벡터로부터 해당 텍스트 내 스타일 정보의 시작 위치가 될 확률을 예측하도록 학습될 수 있 다. 즉, 제1 예측부는 입력되는 각 임베딩 벡터에 대응하는 단어가 해당 텍스트에서 스타일 정보를 갖는 부분의 시작 위치가 될 확률이 어느 정도인지를 예측하도록 학습될 수 있다. 제1 예측부는 linear layer로 이루어지는 신경망 층일 수 있다. 제2 예측부는 인코딩부로부터 각 토큰에 대한 임베딩 벡터를 입력 받을 수 있다. 제2 예측부는 입력되는 각 임베딩 벡터로부터 해당 텍스트 내 스타일 정보의 끝 위치가 될 확률을 예측하도록 학습될 수 있다. 즉, 제2 예측부는 입력되는 각 임베딩 벡터에 대응하는 단어가 해당 텍스트에서 스타일 정보를 갖는 부분의 끝 위치가 될 확률이 어느 정도인지를 예측하도록 학습될 수 있다. 제2 예측부는 linear layer로 이루어지는 신경망 층일 수 있다. 여기서, 제1 예측부는 \"코로나\", \"확진자\", \"대부분이\", \"백신\", \"미접종자\", \"였다고\", \"당국은\", \"밝혔 다\"에 대응하는 임베딩 벡터에 대해 각각 해당 텍스트 내 스타일 정보의 시작 위치가 될 확률을 0.2, 0.5, 0.2, 0.9, 0.8, 0.2, 0.2, 0.1, 0.0으로 예측한 것을 볼 수 있다. 그리고, 제2 예측부는 \"코로나\", \"확진자\", \"대부분이\", \"백신\", \"미접종자\", \"였다고\", \"당국은\", \"밝혔 다\"에 대응하는 임베딩 벡터에 대해 각각 해당 텍스트 내 스타일 정보의 끝 위치가 될 확률을 0.2, 0.5, 0.2, 0.2, 0.7, 0.2, 0.2, 0.1, 0.0으로 예측한 것을 볼 수 있다. 또한, \"코로나\", \"확진자\", \"대부분이\", \"백신\", \"미접종자\", \"였다고\", \"당국은\", \"밝혔다\"에 대한 스타일 정 보의 시작 위치의 정답 값은 1(true), 0(false), 0, 1, 0, 0, 0, 0, 0이고, 끝 위치의 정답 값은 1(true), 0(false), 0, 0, 1, 0, 0, 0, 0이 된다. 제1 머신 러닝 모듈은 각 임베딩 벡터에 대해 제1 예측부가 예측한 값과 스타일 정보의 시작 위치에 대한 정답 값 간의 차이가 최소화 되도록 하고, 제2 예측부가 예측한 값과 스타일 정보의 끝 위치에 대한 정답 값 간의 차이가 최소화 되도록 학습할 수 있다. 이때, 제1 머신 러닝 모듈을 구성하는 인공 신경망의 손실 함수(Loss)는 Cross Entropy Loss를 적용하여 다음의 수학식 1과 같이 표현될 수 있다. (수학식 1)"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "Start' : 제1 예측부가 예측한 값 Start : 스타일 정보의 시작 위치에 대한 정답 값 End' : 제2 예측부가 예측한 값 End : 스타일 정보의 끝 위치에 대한 정답 값"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "제2 머신 러닝 모듈은 입력되는 문서 내 텍스트 중 중요 내용을 요약하는 작업을 수행할 수 있다. 제2 머 신 러닝 모듈은 입력되는 문서 내 텍스트가 의미하는 내용을 요약하도록 학습되는 인공 신경망 모델을 구 비할 수 있다. 이때, 제2 머신 러닝 모듈은 기 학습된 제1 머신 러닝 모듈에서 출력되는 정보를 기반"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "으로 텍스트 내 스타일 정보를 반영하여 해당 텍스트를 요약하도록 학습될 수 있다. 예를 들어, 제2 머신 러닝 모듈은 시퀀스-투-시퀀스(sequence-to-sequence) 모델과 같은 인공 신경망 모델로 이루어질 수 있다. 도 4는 본 발명의 일 실시예에서 제2 머신 러닝 모듈을 학습하는 과정을 개략적으로 나타낸 도면이다. 도 4를 참조하면, 제1 머신 러닝 모듈은 토큰화부, 인코딩부, 제1 예측부, 및 제2 예측부 이외에 필터부를 더 포함할 수 있다. 즉, 제1 머신 러닝 모듈은 입력되는 문서 내 텍스트에서 스타일 정보를 가지는 단어의 위치를 예측하는 태 스크를 수행하도록 학습된 이후에, 제2 머신 러닝 모듈을 학습하는 과정에서 필터부를 추가적으로 포 함할 수 있다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, \"... 대한 민국이 도쿄 올림픽 ...\"이라는 텍스트를 제2 머신 러닝 모듈이 요약하는 경우에 대해 살펴보기로 한다. \"... 대한 민국이 도쿄 올림픽 ...\"이라는 텍스트는 제1 머신 러닝 모듈에도 입력된다. 토큰화부는 텍스트의 각 단어들을 토큰화 하여 토큰들을 생성할 수 있다. 인코딩부는 토큰화부 에서 출력되는 각 토큰들을 임베딩 벡터로 변환할 수 있다. 제1 예측부는 각 임베딩 벡터로부터 해당 텍스 트 내 스타일 정보의 시작 위치가 될 확률을 예측한 값(이하, 제1 예측 값이라 지칭할 수 있음)을 출력할 수 있 다. 제2 예측부는 각 임베딩 벡터로부터 해당 텍스트 내 스타일 정보의 끝 위치가 될 확률을 예측한 값(이 하, 제2 예측 값이라 지칭할 수 있음)을 출력할 수 있다. 필터부는 제1 예측 값 및 제2 예측 값에 기반하여 임베딩 벡터들을 필터링 할 수 있다. 필터부는 제1 예측 값 및 제2 예측 값에 기반하여 임베딩 벡터들 중 일부를 제2 머신 러닝 모듈로 전달할 수 있다. 예시 적인 실시예에서, 필터부는 제1 예측 값 및 제2 예측 값의 평균 값이 기 설정된 임계 값 이상인 경우, 해 당 임베딩 벡터를 제2 머신 러닝 모듈로 전달할 수 있다. 예를 들어, \"대한\", \"민국이\", \"도쿄\", \"올림픽\" 등의 임베딩 벡터들 중 \"도쿄\" 및 \"올림픽\"에 대응하는 임베딩 벡터에 대한 제1 예측 값 및 제2 예측 값의 평균 값이 기 설정된 임계 값 이상인 경우, 필터부는 \"도쿄\" 및 \"올림픽\"에 대응하는 임베딩 벡터들을 제2 머신 러닝 모듈로 전달할 수 있다. 여기서, 제1 예측 값 및 제2 예측 값의 평균 값이 기 설정된 임계 값 이상인 임베딩 벡터는 해당 단어가 텍스트 내에서 스타일 정보를 가질 확률이 높은 것이므로, 이를 \"스타일 관련 임베딩 벡터\"라 지칭할 수 있다. 즉, 필 터부는 임베딩 벡터들 중 제1 예측 값 및 제2 예측 값의 평균 값이 기 설정된 임계 값 이상인 임베딩 벡터 를 스타일 관련 임베딩 벡터로 추출할 수 있다. 한편, 제2 머신 러닝 모듈은 토큰화부, 인코딩부, 합산부, 및 디코딩부를 포함할 수 있다. 여기서, 토큰화부, 인코딩부, 합산부, 및 디코딩부는 인공 신경망으로 이루어질 수 있다. 이때, 토큰화부, 인코딩부, 합산부, 및 디코딩부는 하나의 인공 신경망으로 구성될 수도 있고 여러 개의 인공 신경망으로 구성될 수도 있다. 토큰화부는 입력되는 텍스트에 대해 토큰화(Tokenization)를 수행하여 기 설정된 단위의 토큰들을 생성할 수 있다. 예시적인 실시예에서, 토큰화부는 입력되는 텍스트에 대해 단어 단위로 토큰화를 수행하여 토큰 들을 생성할 수 있다. 예를 들어, 토큰화부는 입력되는 텍스트를 \"대한\", \"민국이\", \"도쿄\", \"올림픽\" 등 으로 토큰화를 수행하여 토큰들을 생성할 수 있다. 인코딩부는 토큰화부에서 출력되는 각 토큰들을 임베딩 벡터로 변환할 수 있다. 합산부는 인코딩부에서 출력되는 임베딩 벡터와 제1 머신 러닝 모듈에서 전달되는 스타일 관련 임베딩 벡터를 더할 수 있다. 이때, 합산부는 제1 머신 러닝 모듈에서 전달되는 스타일 관련 임베딩 벡터를 인코딩부에서 출력되는 임베딩 벡터들 중 그에 대응하는 임베딩 벡터와 각각 더할 수 있다. 예를 들어, 제1 머신 러닝 모듈에서 \"도쿄\" 및 \"올림픽\"에 대응하는 임베딩 벡터가 전달된 경우, 합산부 는 제1 머신 러닝 모듈에서 전달된 \"도쿄\" 및 \"올림픽\"에 대응하는 임베딩 벡터들을 각각 인코딩부 에서 출력되는 인코딩 벡터들 중 \"도쿄\" 및 \"올림픽\"에 대응하는 임베딩 벡터와 각각 더할 수 있다. 합산부는 스타일 관련 임베딩 벡터를 인코딩부에서 출력되는 임베딩 벡터들 중 그에 대응하는 임베딩 벡터와 각각 더한 후에 정규화(Normalization)를 수행할 수 있다. 합산부는 인코딩부에서 출력되는 임베딩 벡터들 중 스타일 관련 임베딩 벡터와 대응하지 않는 임베딩 벡터는 그대로 디코딩부로 전달할 수 있다. 또한, 합산부는 인코딩부에서 출력되는 임베딩 벡터 들 중 스타일 관련 임베딩 벡터와 대응하는 임베딩 벡터는 스타일 관련 임베딩 벡터와 더한 후 디코딩부로 전달할 수 있다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "디코딩부는 합산부에서 출력되는 임베딩 벡터에 기반하여 텍스트의 요약된 내용을 출력하도록 학습될 수 있다. 이때, 제1 예측 값 및 제2 예측 값의 평균 값이 기 설정된 임계 값 이상인 임베딩 벡터(즉, 스타일 관 련 벡터)는 합산부에서 그에 대응하는 임베딩 벡터와 더해진 후 디코딩부로 입력되기 때문에, 텍스트"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "내 스타일 정보를 가지는 단어들을 반영하여 텍스트의 요약된 내용을 출력할 수 있게 된다. 디코딩부는 추"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "출형 요약 태스크를 수행하는 기 공지된 인공 신경망으로 구현될 수 있다. 한편, 인코딩부 및 디코딩부의 가중치 값(또는 파라미터 값)은 랜덤하게 초기화 될 수 있으나, 이에 한정되는 것은 아니며 제1 머신 러닝 모듈의 인코딩부의 가중치 값으로 초기화 될 수도 있다. 이때, 인코딩부의 가중치 값은 텍스트 내 스타일 정보를 가진 부분과 그렇지 않은 부분을 구별하도록 학습된 값"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "이므로, 랜덤화된 초기 값보다 인코딩부의 가중치 값을 이용할 때 보다 효율적으로 텍스트의 요약된 내용 을 출력할 수 있게 된다. 개시되는 실시예에서는, 제1 머신 러닝 모듈에서 텍스트 내에서 스타일 정보를 가지는 부분의 위치를 예측"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "하도록 학습하고, 제2 머신 러닝 모듈에서 이를 이용하여 텍스트 내용을 요약하도록 함으로써, 텍스트 내"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "중요한 키워드 및 문장을 파악할 수 있게 되고 결과적으로 문서의 요약 성능을 높일 수 있게 된다. 본 명세서에서 모듈이라 함은, 본 발명의 기술적 사상을 수행하기 위한 하드웨어 및 상기 하드웨어를 구동하기 위한 소프트웨어의 기능적, 구조적 결합을 의미할 수 있다. 예컨대, 상기 \"모듈\"은 소정의 코드와 상기 소정의 코드가 수행되기 위한 하드웨어 리소스의 논리적인 단위를 의미할 수 있으며, 반드시 물리적으로 연결된 코드를 의미하거나, 한 종류의 하드웨어를 의미하는 것은 아니다. 도 5는 본 발명의 일 실시예에 따른 자연어 처리 방법을 나타낸 흐름도이다. 도 5에 도시된 방법은 예를 들어, 도 1에 도시된 자연어 처리 장치에 의해 수행될 수 있다. 도 5를 참조하면, 자연어 처리 장치는 텍스트에 스타일 정보를 가지는 문서를 수집한다. 예시적인 실시예에서, 자연어 처리 장치는 웹 크롤링(Web Crawling)을 통해 HTML 문서와 같이 태그를 가지 는 웹 문서를 수집할 수 있다. 이후, 자연어 처리 장치는 수집된 문서 내 텍스트에서 스타일 정보를 추출하여 해당 텍스트와 레이블링 (labeling)을 수행한다. 예를 들어, 자연어 처리 장치는 문서 내 텍스트에서 스타일 정보를 추출하고, 해당 텍스트에서 스타일 정 보를 가지는 부분의 위치에 기반하여 레이블링을 수행할 수 있다. 이후, 자연어 처리 장치는 문서 내 텍스트에서 스타일 정보를 가지는 단어의 위치를 예측하도록 제1 머신 러닝 모듈을 학습시킨다. 자연어 처리 장치는 스타일 정보가 레이블링 된 문서를 학습 데이터로 하여 제1 머신 러닝 모듈을 학 습할 수 있다. 이때, 자연어 처리 장치는 입력되는 텍스트에서 스타일 정보를 갖는 부분의 시작 위치 및 끝 위치를 예측하도록 제1 머신 러닝 모듈을 학습시킬 수 있다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "이후, 자연어 처리 장치는 문서 내 텍스트의 중요 내용을 요약하도록 제2 머신 러닝 모듈을 학습시킨 다. 자연어 처리 장치는 기 학습된 제1 머신 러닝 모듈에서 출력되는 정보(스타일 관련 임베딩 벡터)를"}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "기반으로 텍스트 내 스타일 정보를 반영하여 해당 텍스트를 요약하도록 제2 머신 러닝 모듈을 학습시킬 수 있다. 한편, 도 5에 도시된 흐름도에서는 상기 방법을 복수 개의 단계로 나누어 기재하였으나, 적어도 일부의 단계들 은 순서를 바꾸어 수행되거나, 다른 단계와 결합되어 함께 수행되거나, 생략되거나, 세부 단계들로 나뉘어 수행 되거나, 또는 도시되지 않은 하나 이상의 단계가 부가되어 수행될 수 있다. 도 6은 예시적인 실시예들에서 사용되기에 적합한 컴퓨팅 장치를 포함하는 컴퓨팅 환경을 예시하여 설명하 기 위한 블록도이다. 도시된 실시예에서, 각 컴포넌트들은 이하에 기술된 것 이외에 상이한 기능 및 능력을 가 질 수 있고, 이하에 기술된 것 이외에도 추가적인 컴포넌트를 포함할 수 있다. 도시된 컴퓨팅 환경은 컴퓨팅 장치를 포함한다. 일 실시예에서, 컴퓨팅 장치는 자연어 처리 장치 일 수 있다. 컴퓨팅 장치는 적어도 하나의 프로세서, 컴퓨터 판독 가능 저장 매체 및 통신 버스를 포함한다. 프로세서는 컴퓨팅 장치로 하여금 앞서 언급된 예시적인 실시예에 따라 동작하도록 할 수 있 다. 예컨대, 프로세서는 컴퓨터 판독 가능 저장 매체에 저장된 하나 이상의 프로그램들을 실행할 수 있 다. 상기 하나 이상의 프로그램들은 하나 이상의 컴퓨터 실행 가능 명령어를 포함할 수 있으며, 상기 컴퓨터 실 행 가능 명령어는 프로세서에 의해 실행되는 경우 컴퓨팅 장치로 하여금 예시적인 실시예에 따른 동작 들을 수행하도록 구성될 수 있다. 컴퓨터 판독 가능 저장 매체는 컴퓨터 실행 가능 명령어 내지 프로그램 코드, 프로그램 데이터 및/또는 다 른 적합한 형태의 정보를 저장하도록 구성된다. 컴퓨터 판독 가능 저장 매체에 저장된 프로그램은 프로 세서에 의해 실행 가능한 명령어의 집합을 포함한다. 일 실시예에서, 컴퓨터 판독 가능 저장 매체는 메 모리(랜덤 액세스 메모리와 같은 휘발성 메모리, 비휘발성 메모리, 또는 이들의 적절한 조합), 하나 이상의 자 기 디스크 저장 디바이스들, 광학 디스크 저장 디바이스들, 플래시 메모리 디바이스들, 그 밖에 컴퓨팅 장치 에 의해 액세스되고 원하는 정보를 저장할 수 있는 다른 형태의 저장 매체, 또는 이들의 적합한 조합일 수 있다. 통신 버스는 프로세서, 컴퓨터 판독 가능 저장 매체를 포함하여 컴퓨팅 장치의 다른 다양한 컴 포넌트들을 상호 연결한다. 컴퓨팅 장치는 또한 하나 이상의 입출력 장치를 위한 인터페이스를 제공하는 하나 이상의 입출력 인터 페이스 및 하나 이상의 네트워크 통신 인터페이스를 포함할 수 있다. 입출력 인터페이스 및 네트워 크 통신 인터페이스는 통신 버스에 연결된다. 입출력 장치는 입출력 인터페이스를 통해 컴퓨팅 장치의 다른 컴포넌트들에 연결될 수 있다. 예시적인 입출력 장치는 포인팅 장치(마우스 또는 트랙패드 등), 키보드, 터치 입력 장치(터치패드 또는 터치스크린 등), 음성 또는 소리 입력 장치, 다양한 종류의 센서 장치 및/또는 촬영 장치와 같은 입력 장치, 및/또는 디스플레이 장치, 프린터, 스피커 및/또는 네트워크 카드와 같은 출력 장치를 포함할 수 있다. 예시적인 입출력 장치는 컴퓨팅 장치를 구성하는 일 컴포넌트로서 컴퓨팅 장치의 내부에 포함될 수도 있고, 컴퓨팅 장치와는 구별되는 별개의 장치로 컴퓨팅 장치와 연결될 수도 있다."}
{"patent_id": "10-2021-0140826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "이상에서 본 발명의 대표적인 실시예들을 상세하게 설명하였으나, 본 발명이 속하는 기술분야에서 통상의 지식 을 가진 자는 상술한 실시예에 대하여 본 발명의 범주에서 벗어나지 않는 한도 내에서 다양한 변형이 가능함을 이해할 것이다. 그러므로 본 발명의 권리범위는 설명된 실시예에 국한되어 정해져서는 안 되며, 후술하는 특허 청구범위뿐만 아니라 이 특허청구범위와 균등한 것들에 의해 정해져야 한다."}
{"patent_id": "10-2021-0140826", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 머신 러닝 기반의 자연어 처리 장치의 구성을 나타낸 블록도 도 2는 본 발명의 일 실시예에서 HTML 태그에 기반하여 문서 내 텍스트의 스타일 정보를 추출하는 상태를 나타 낸 도면 도 3은 본 발명의 일 실시예에서 제1 머신 러닝 모듈의 구성을 나타낸 도면 도 4는 본 발명의 일 실시예에서 제2 머신 러닝 모듈을 학습하는 과정을 개략적으로 나타낸 도면 도 5는 본 발명의 일 실시예에 따른 자연어 처리 방법을 나타낸 흐름도 도 6은 예시적인 실시예들에서 사용되기에 적합한 컴퓨팅 장치를 포함하는 컴퓨팅 환경을 예시하여 설명하기 위 한 블록도"}
