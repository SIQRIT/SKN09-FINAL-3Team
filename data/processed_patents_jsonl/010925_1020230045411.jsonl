{"patent_id": "10-2023-0045411", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0149628", "출원번호": "10-2023-0045411", "발명의 명칭": "초차원 컴퓨팅 방법", "출원인": "재단법인대구경북과학기술원", "발명자": "김예성"}}
{"patent_id": "10-2023-0045411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터의 초차원 컴퓨팅 방법에 있어서,학습 단계에서,각 클래스 별 다수의 학습 데이터를 입력받고, 이를 인코딩하여 하이퍼벡터(hypervector)를 산출하는 단계;상기 산출된 하이퍼벡터의 선형 트레이닝을 통해, 각 클래스 별 하이퍼벡터를 산출하여 저장하는 단계;를 포함하고,추론 단계에서,입력 데이터를 인코딩하여, 쿼리 하이퍼벡터(query hypervector)를 산출하는 단계;상기 학습 단계에 의해 저장된 상기 각 클래스 별 하이퍼벡터와 산출한 상기 쿼리 하이퍼벡터 간 유사도 체크를통해 입력 데이터의 클래스를 추론하는 단계;를 포함하되,각 클래스 별 하이퍼벡터를 산출하여 저장하는 단계는상기 하이퍼벡터와 연산되는 베이스 하이퍼벡터(base hypervector)(B)를 가변 업데이트하는, 초차원 컴퓨팅 방법."}
{"patent_id": "10-2023-0045411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,각 클래스 별 하이퍼벡터를 산출하여 저장하는 단계는상기 산출된 하이퍼벡터와 베이스 하이퍼벡터를 입력받아, 특성 인코딩하여 제1 특성 하이퍼벡터(featurehypervector)(X)를 산출하는 단계;상기 산출된 제1 특성 하이퍼벡터를 이진화(binarize)를 통해 제2 특성 하이퍼벡터(H) 및 이진화 계수(binarization factor) 추론을 통해 이진화 계수 하이퍼벡터(F_bz)를 산출하는 단계;상기 산출된 제2 특성 하이퍼벡터(H)를 기초로 오류 계수(error factor) 추론을 통해 오류 계수 하이퍼벡터(F_err)를 산출하는 단계; 및상기 산출된 이진화 계수 하이퍼벡터(F_bz)와 상기 산출된 오류 계수 하이퍼벡터(F_err)를 이용하여, 상기 베이스 하이퍼벡터를 업데이트하는 단계;를 포함하는, 초차원 컴퓨팅 방법."}
{"patent_id": "10-2023-0045411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서,상기 초차원 컴퓨팅 방법은상기 입력 데이터의 클래스를 추론하는 단계를 수행하고 난 후,상기 입력 데이터의 클래스를 추론하는 단계에 의해 추론된 클래스와 상기 입력 데이터를 기초로, 상기 학습 단계를 수행하여, 각 클래스 별 하이퍼벡터를 업데이트하는 단계;공개특허 10-2024-0149628-3-를 더 포함하는, 초차원 컴퓨팅 방법."}
{"patent_id": "10-2023-0045411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1항에 있어서,상기 학습 단계는양자화 인식 학습(quantization aware training)을 이용하여 동작을 수행하는, 초차원 컴퓨팅 방법."}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 학습하기 위한 효율적이고 효과적인 동적 인코더(Trainable HD)를 포함하는 양자화 기능을 갖춘 초차 원 컴퓨팅(HDC) 기술에 관한 것이다."}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 초차원 컴퓨팅 방법에 관한 것으로, 더욱 상세하게는 효과적인 양자화를 갖춘 동적 인코더를 이용한 초차원 컴퓨팅 방법에 관한 것이다."}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "초차원 컴퓨팅(Hyper dimensional computing, HDC)은 뇌/인지 과학에 기반하여, 학습 데이터를 고차원 벡터 데 이터로 인코딩한 후, 고차원 벡터의 연산을 통하여 분류 등의 문제를 해결하기 위한 학습을 진행하는 대안 인공 지능 기법이다. 최근, 단순한 연산량과 높은 효율로 인해, HDC를 학습 모델로 활용하려는 기술이 많이 등장하고 있다. 그렇지만, 종래의 초차원 컴퓨팅 시스템은 인코딩 과정에서, 랜덤하게 생성한 가중치를 정적으로 사용하기 때문 에, 많은 매개 변수와 낮은 정확도를 초래하는 문제점이 있다. 즉, 종래의 초차원 컴퓨팅 기법은 데이터를 초차원 벡터로 전환할 때, 고정된 값을 지닌 인코더를 사용하고 있 으며, 이는 각 데이터의 feature가 고유하게 지니고 있는 정보를 학습 모델의 학습에 사용하지 않고 있다는 것 을 의미한다. 일 예를 들자면, 가장 높은 정확도를 달성하는 것으로 알려져 있는 비선형 인코딩(non-linear encoding)의 경우, 가우시안 분포에 기반하여 부동소수점으로 이루어진 초차원 벡터를 생성한 다음, 코사인 함수 또는, 부호 비트를 적용하여 각 요소 분포의 비선형성이 증폭된 이진화된 초차원 벡터를 만들어냄으로써, 다른 인코딩 기법 대비 정확도를 향상시켰다. 그렇지만, 랜덤하게 생성된 정적 가중치를 활용하기 때문에, 인코딩되는 특징값들의 연관 관계를 파악하지 못하고, 결과적으로 정확도가 낮아지는 문제점이 있다. 이러한 정적 인코딩 과정의 문제점을 해결하기 위한 종래 기술로는 Mani HD가 있다. Mani HD는 기존 인코딩 과 정을 수행하기 전에, manifold projection을 통해 각 특징(feature) 간의 non-linear 상호 작용을 학습 과정에 반영하는 절차를 도입하여, 초차원 공간에서의 효율적인 학습을 수행하고자 하였다. 그렇지만, 이 역시도, 각 class의 특성을 충분히 고려하지 않았으며, manifold projection된 feature의 값과 결합하여 생성해 낸 인코더 역시 그 값이 훈련 과정이 반복되어도 변하지 않기 때문에, 고정된 값을 지닌 인코 더의 한계를 완전히 해결하지 못하였고, 정확도 역시 크게 개선하지 못한다는 문제점이 있다. 더불어, Mani HD에서 사용되는 manifold 학습 방법은 인코딩 과정 수행에 앞서, manifold projection에 요구되 는 추가적인 연산을 필요로 하기 때문에, 이를 포함한 Mani HD의 인코딩 시간은 기존의 초차원 컴퓨팅 기법에 적용된 인코딩에 소요되는 시간의 약 2.95배에 달하는 것으로 측정되었다. 그렇기 때문에, 기존의 초차원 컴퓨팅 기법에 적용된 인코딩 과정이 FPGA 환경에서의 추론 과정에 사용되는 총 시간의 90% 가량을 소요하고 있기에, Mani HD으로 인해 발생하는 오버헤드를 고려해야 하는 추가적인 문제점이 있다. 이와 관련해서, \"Stochastic-HD: Leveraging Stochastic Computing on the Hyper-Dimensional Computing Pipeline, FRONTIERS IN NEUROSCIENCE, 2022년, 16권, ISSN 1662-4548\"에서는 상술한 hyper dimensional computing 기술을 제안하고 있다. 또한, \"Zou, Z., Kim, Y., Najafi, M. H., & Imani, M. (2021, February). Manihd: Efficient hyper- dimensional learning using manifold trainable encoder. In 2021 Desine, Automation & Test in Europe Conference & Exhibition (DATE) (pp. 850-855). IEEE.\"에서는 상술한 Mani HD에 대한 기술을 제안하고 있다. 선행기술문헌 비특허문헌 (비특허문헌 0001) FRONTIERS IN NEUROSCIENCE, 2022, Stochastic-HD: Leveraging Stochastic Computing on the Hyper-Dimensional Computing Pipeline (비특허문헌 0002) Zou, Z., Kim, Y., Najafi, M. H., & Imani, M. (2021, February). Manihd: Efficient hyper-dimensional learning using manifold trainable encoder. In 2021 Desine, Automation & Test in Europe Conference & Exhibition (DATE) (pp. 850-855). IEEE."}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기한 바와 같은 종래 기술의 문제점을 해결하기 위하여 안출된 것으로, 본 발명의 목적은 효율적이 고 효과적인 동적 인코더를 이용한 양자화 기능을 갖춘 초차원 컴퓨팅 시스템을 제공하는 것이다. 훈련 가능한 인코더를 이용하여, 더 나은 성능을 위해 학습 과정 중에 인코더를 지속적으로 업데이트시킴으로써, 정확도 및 연산 효율성을 개선한 초차원 컴퓨팅 시스템을 제공하는 것이다."}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 컴퓨터의 초차원 컴퓨팅 방법에 있어서, 컴퓨터의 초차원 컴퓨팅 방법에 있어서, 학습 단계에서, 각 클래스 별 다수의 학습 데이터를 입력받고, 이를 인코딩하여 하이퍼벡터(hypervector)를 산 출하는 단계, 상기 산출된 하이퍼벡터의 선형 트레이닝을 통해, 각 클래스 별 하이퍼벡터를 산출하여 저장하는 단계를 포함하며, 추론 단계에서, 입력 데이터를 인코딩하여, 쿼리 하이퍼벡터(query hypervector)를 산출하는 단계, 상기 학습 단계에 의해 저장된 상기 각 클래스 별 하이퍼벡터와 산출한 상기 쿼리 하이퍼벡터 간 유사도 체크를 통해 입력 데이터의 클래스를 추론하는 단계를 포함하되, 각 클래스 별 하이퍼벡터를 산출하여 저장하는 단계는 상기 하이퍼벡터와 연산되는 베이스 하이퍼벡터(base hypervector)(B)를 가변 업데이트하는 것이 바람직 하다. 더 나아가, 각 클래스 별 하이퍼벡터를 산출하여 저장하는 단계는 상기 산출된 하이퍼벡터와 베이스 하이퍼벡터 를 입력받아, 특성 인코딩하여 제1 특성 하이퍼벡터(feature hypervector)(X)를 산출하는 단계, 상기 산출된 제 1 특성 하이퍼벡터를 이진화(binarize)를 통해 제2 특성 하이퍼벡터(H) 및 이진화 계수(binarization factor) 추론을 통해 이진화 계수 하이퍼벡터(F_bz)를 산출하는 단계, 상기 산출된 제2 특성 하이퍼벡터(H)를 기초로 오 류 계수(error factor) 추론을 통해 오류 계수 하이퍼벡터(F_err)를 산출하는 단계 및 상기 산출된 이진화 계수 하이퍼벡터(F_bz)와 상기 산출된 오류 계수 하이퍼벡터(F_err)를 이용하여, 상기 베이스 하이퍼벡터를 업데이트 하는 단계를 포함하는 것이 바람직하다. 더 나아가, 상기 초차원 컴퓨팅 방법은 상기 입력 데이터의 클래스를 추론하는 단계를 수행하고 난 후, 상기 입 력 데이터의 클래스를 추론하는 단계에 의해 추론된 클래스와 상기 입력 데이터를 기초로, 상기 학습 단계를 수 행하여, 각 클래스 별 하이퍼벡터를 업데이트하는 단계를 더 포함하는 것이 바람직하다. 더 나아가, 상기 학습 단계는 양자화 인식 학습(quantization aware training)을 이용하여 동작을 수행하는 것 이 바람직하다."}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기와 같은 구성에 의한 본 발명의 초차원 컴퓨팅 방법은 초차원 컴퓨팅의 정확도 및 연산 효율성을 모두 개선 할 수 있는 효과가 있다. 먼저, 기존 초차원 컴퓨팅과의 분류 정확도 비교에서 추가적인 계산 비용 없이 평균적으로 3.62% 높은 정확도를 얻음을 실험을 통하여 확인하였으며, 기존 초차원 컴퓨팅에서 3000차원 벡터를 사용하여 얻을 수 있는 성능을 본 발명에서는 1000차원 벡터를 사용하여 얻어낼 수 있었기 때문에, 학습에 요구되는 초차원 벡터의 차원 수를 크게 줄일 수 있는 장점이 있다. 더불어, 학습 속도는 딥러닝 모델에 비해 24.48배 빨랐으며, 양자화 인식 학습 과정을 적용하더라도 딥러닝 모 델에 비해 12.13배 빠른 학습 속도를 갖는 장점이 있다. 양자화 인식 학습을 통해 정확도 손실을 최소화하는 한편, 설계 환경, 다시 말하자면, CPU, GPU 및 FPGA 환경에 서 효율적인 추론을 가능하게 하는 장점이 있다. 딥러닝 모델과 비교할 경우, 양자화 과정을 수행하지 않을 경우, GPU 환경에서 56.4배 빠르고 73배 에너지 효율 적인 것을 알 수 있었으며, FPGA 환경에서 양자화 인식 학습을 수행하더라도 180.8배 까지 성능을 향상시킬 수 있는 장점이 있다."}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 첨부한 도면들을 참조하여 본 발명의 초차원 컴퓨팅 시스템을 상세히 설명한다. 다음에 소개되는 도면들은 당업자에게 본 발명의 사상이 충분히 전달될 수 있도록 하기 위해 예로서 제공되는 것이다. 따라서, 본 발명은 이하 제시되는 도면들에 한정되지 않고 다른 형태로 구체화될 수도 있다. 또한, 명세서 전반에 걸쳐서 동일한 참조번호들은 동일한 구성요소들을 나타낸다. 이때, 사용되는 기술 용어 및 과학 용어에 있어서 다른 정의가 없다면, 이 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 통상적으로 이해하고 있는 의미를 가지며, 하기의 설명 및 첨부 도면에서 본 발명의 요지를 불필요하게 흐릴 수 있는 공지 기능 및 구성에 대한 설명은 생략한다. 더불어, 시스템은 필요한 기능을 수행하기 위하여 조직화되고 규칙적으로 상호 작용하는 장치, 기구 및 수단 등 을 포함하는 구성 요소들의 집합을 의미한다. 초차원 컴퓨팅(HDC, Hyper-Dimensional Computing) 기술은 인간의 기억 모델에서 영감을 얻은 컴퓨팅 패러다임 이다. 최근, 초차원 컴퓨팅이 가지고 있는 특성인 낮은 연산량과 높은 효율성을 채용하기 위하여, 다양한 분야 에서 활용이 이루어지고 있다. 그렇지만, 초차원 컴퓨팅의 학습 프레임워크는 무작위로 생성되고 정적인 인코더를 사용하기 때문에, 많은 매개 변수와 낮은 정확도를 갖는 문제점이 있다. 이에 따라, 본 발명의 일 실시예에 따른 컴퓨터의 초차원 컴퓨팅 방법은, 효율성을 향상시키기 위해 효과적인 양자화를 갖춘 동적 인코더를 활용한 Trainable HD에 관한 것이다. 간단하게는, 본 발명의 일 실시예에 따른 컴퓨터의 초차원 컴퓨팅 방법은, 학습 모델(HD model)에서 얻은 오류 를 고려하여, 학습 과정에서 인코더를 동적으로 업데이트함으로써, 추가 연산량 없이도 초차원 컴퓨팅의 정확도 를 최대 22.26% 향상시킬 수 있는 기술에 관한 것이다. 이는, 실험을 통해서, 저전력 GPU 플랫폼인 NVIDIA Jetson Xavier에 대한 딥러닝에 비해 더 빠르고 에너지 효율 성이 더 높은 장점을 확인하였다. 이러한 본 발명의 일 실시예에 따른 컴퓨터의 초차원 컴퓨팅 방법은 학습 과정과 추론 과정으로 나누어 설명할 수 있다. 각 단계는 당연히, 컴퓨터를 포함하는 연산 처리 수단에 구비되어 동작을 수행하게 된다. 본 발명의 일 실시예에 따른 컴퓨터의 초차원 컴퓨팅 방법은, 뇌인지과학과 수학을 인공지능(AI)에 접목하여 인 코더를 이용하여 기존보다 연산 과정이 단순해짐으로써, 큰 장치 없이도 수준 높은 AI를 적용할 수 있는 초차원 컴퓨팅 기술에 관한 것이다. 학습 단계에서는, 각 클래스 별 다수의 학습 데이터를 비선형 인코더 및 선형 트레이닝을 통해서 각 클래스 별 하이퍼벡터를 산출하여 저장하고, 추론 단계에서는, 입력 데이터를 인코딩하여 쿼리 하이퍼벡터를 산출하고, 상 기 학습 단계에서 저장된 하이퍼벡터와 상기 쿼리 하이퍼벡터 간 유사도 체크를 통해 각 클래스를 추론할 수 있 어, 연산량을 대폭 감소시킬 수 있는 기술이다. 이 때, 비선형 인코더는 입력되는 특성 벡터(학습 데이터)에 베이스 하이퍼벡터(가중치)를 바인딩하고 번들링하 여 이진화하는 과정을 수행하는데, 종래에 적용되었던 인코더의 베이스 하이퍼벡터는 랜덤하게 생성된 정적 값 이므로 인코딩되는 특징값들의 연관 관계를 평가하지 못하고, 결과적으로 정확도가 낮아지는 문제점이 있었다. 이를 해소하기 위하여, 본 발명의 일 실시예에 따른 컴퓨터의 초차원 컴퓨팅 방법에서는, 학습 결과를 반영하여 상기 베이스 하이퍼벡터를 가변하는 과정을 수행함으로써, 추론의 정확도를 향상시킬 수 있다. 이러한 본 발명의 일 실시예에 따른 컴퓨터의 초차원 컴퓨팅 방법은 학습 단계는 각 클래스 별 다수의 학습 데 이터를 입력받고, 이를 인코딩하여 하이퍼벡터(hypervector)를 산출하며, 상기 산출된 하이퍼벡터의 선형 트레 이닝을 통해, 각 클래스 별 하이퍼벡터를 산출하여 저장하게 된다. 또한, 추론 단계는 입력 데이터를 인코딩하여, 쿼리 하이퍼벡터(query hypervector)를 산출하고, 상기 학습 단 계에 의해 저장된 상기 각 클래스 별 하이퍼벡터와 산출한 상기 쿼리 하이퍼벡터 간 유사도 체크를 통해 입력 데이터의 클래스를 추론하게 된다. 이 때, 상기 학습 단계는 상기 하이퍼벡터와 연산되는 베이스 하이퍼벡터(base hypervector)(B)를 가변 업데이 트하는 것이 바람직하다. 또한, 상기 추론 단계를 수행하고 난 후, 상기 추론된 클래스와 상기 입력 데이터를 기초로, 상기 학습 단계를 반복 수행하여, 각 클래스 별 하이퍼벡터를 업데이트하는 단계를 더 포함하는 것이 바람직하다. 도 1은 본 발명의 초차원 컴퓨팅 방법에 의해 제안된 동적 인코더(Trainable HD)의 학습 프레임워크를 나타낸 예시도이다. 학습 과정의 목표는 두가지 유형의 학습된 하이퍼벡터인 초기에 가우시안 분포에서 랜덤 생성된 인코더의 베이 스 하이퍼벡터와, 초기 값이 0인 구성 요소를 갖는 각 클래스에 대한 고차원 패턴을 나타내는 각 클래스 별 하 이퍼벡터를 식별하는 것이다. 학습하는 동안, 현재의 베이스 하이퍼벡터를 사용하여 이전에 인코딩된 특성 하이퍼벡터를 인코딩하게 된다. 특성 하이퍼벡터와 모든 클래스 하이퍼벡터의 유사도를 비교하여 학습을 진행한다. HD 모듈은 학습 데이터의 클 래스와 최대 유사도 값을 보인 클래스 하이퍼벡터로 정의하게 된다. 유사성 값과 GT(Ground Truth) 레이블 값을 기반으로 HD 모듈은 클래스 별 오류를 계산하고, 각 클래스 하이퍼 벡터를 업데이트하여 이 후 예측을 위한 오류를 줄이게 된다. 이를 위해, 상기 학습 단계는 상기 산출된 하이퍼벡터와 베이스 하이퍼벡터를 입력받아, 특성 인코딩하여 제1 특성 하이퍼벡터(feature hypervector)(X)를 산출하는 단계, 상기 산출된 제1 특성 하이퍼벡터를 이진화 (binarize)를 통해 제2 특성 하이퍼벡터(H) 및 이진화 계수(binarization factor) 추론을 통해 이진화 계수 하 이퍼벡터(F_bz)를 산출하는 단계, 상기 산출된 제2 특성 하이퍼벡터(H)를 기초로 오류 계수(error factor) 추론 을 통해 오류 계수 하이퍼벡터(F_err)를 산출하는 단계 및 상기 산출된 이진화 계수 하이퍼벡터(F_bz)와 상기 산출된 오류 계수 하이퍼벡터(F_err)를 이용하여, 상기 베이스 하이퍼벡터를 업데이트하는 단계를 포함하게 된 다. 동적 인코더(Trainable HD)의 핵심은 각 클래스 별 스칼라 오류를 각 특성 별 하이퍼벡터의 오류로 변환하여, 베이스 하이퍼벡터를 업데이트하는 것으로 이에 대해서는 자세히 후술하도록 한다. 이러한 인코더의 학습 과정은 EIT(Encoder Interval Training) 최적화 기술을 기반으로 수행하며, 여러 에포크 에 걸쳐 미니 배치가 있는 모든 입력 데이터에 대해 반복된다.또한, 학습 과정은 양자화 인식 학습(QAT, Quantization-Aware Training)를 사용하여, 8비트 정수와 같은 저정 밀 하이퍼벡터 요소로 양자화된 모델을 훈련함으로써, 성능/전력 관점에서 보다 효율적으로 추론을 수행하게 된 다. 이 역시도 관련해서 자세히 후술하도록 한다. 양자화에 의해 완전히 가속화된 추론 단계에서, 훈련된 인코더는 기존 HD 학습 솔루션에 훈련 시간의 오버헤드 없이 추론 데이터를 사용하여 쿼리 하이퍼벡터를 생성하게 된다. 이 후, HD 모델은 쿼리 하이퍼벡터와 클래스 하이퍼벡터 간의 유사성 계산을 수행하여, 입력 데이터의 클래스를 식별하게 된다. 동적 인코더의 학습 과정에 대해서 설명하기 앞서서, 인코딩 원리에 대해서 먼저 설명하자면, 인간의 뇌에 입력 자극에 따라 활성화되는 수백만 개의 뉴런과 시냅스가 존재하는 것과 마찬가지로, 초차원 컴퓨팅은 하이퍼벡터 를 사용하여 고차원 공간 또는 초공간에 있는 모든 개체를 나타낸다. 하이퍼벡터를 정보를 모든 구성 요소에 균등하게 분배하는 전체적인 표현이다. 대부분의 관련 작업에서 학습 데 이터/입력 데이터의 특성과 같은 원시 값을 하이퍼벡터에 매핑/인코딩하기 위해, 양극성 값 {-1, 1} 또는, 더 높은 정확도를 갖는 가우시안 분포(N(μ, σ2))에서 각 차원을 무작위로 샘플링하여 베이스 하이퍼벡터(base hypervector)를 생성하게 된다. 초차원 컴퓨팅에서 무작위성과 높은 차원을 필요로 하는 이유는 유사 직교성(quasi-orthogonality)을 달성하기 위함이다. 즉, 서로 다른 특성이 서로 관련이 없다고 가정하여, 거의 0에 가까운 유사성을 나타낸다. 그렇지만, 종래의 초차원 컴퓨팅에 채용된 인코더는 한번 생성된 베이스 하이퍼벡터(기본 하이퍼벡터)를 유지하 기 때문에, 서로 다른 특성이 상관관계가 있을 경우엔, 정확하게 식별할 수 없는 문제점이 있다. 일반적인 인코딩 과정에 대해서 설명하자면, 를 부호화할 스칼라의 벡터(v1, …, vp), 코드북 ( )은 D-차원 하이퍼벡터 정보를 포함하고 있다고 가정하고, 랜덤 프로젝션(random projection), 비선형 인코딩(non-linear encoding)과 같은 종래의 인코딩 방법은 하기의 수학식 1과 같이 나타낼 수 있다. 수학식 1"}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, 는 다른 정보를 요소별 곱셈과 연결하는 바인딩(binding) 작업을 의미하며, 는 서로 다른 정보를 요소 별로 추가하여 하이퍼벡터로 결함하는 번들링(bundling) 작업을 의미한다. 더불어, 인코더는 일반적으로 cos(·), sine(·)와 같은 활성화 함수를 적용한다. 원칙적으로 HD 인코딩은 서로 다른 벡터 기반으로 표현되는 도메인 간의 차원 간 맵핑으로 볼 수 있으며, 다시 말하자면, p 차원의 실제 좌표 공간의 정보를 다른 초공간으로 맵핑하는데, 이 초공간의 기본을 모든 하이퍼벡 터의 집합인 가 된다. 더불어, 본 발명에서는, 의 코드워드를 참조하여, 에 저장된 동일한 정보를 하이퍼벡터( )로 전송 하는 절차로 차원 간 맵핑 함수( )를 정의한다. 일 예를 들자면, 원래의 인코더는 원시(raw) 특성(feature)을 의 임의의 코드북으로 맵핑한다. 또한, 클래스 하이퍼벡터( )의 코드북을 사용하여, 스칼라 벡터의 오류값을 다른 인코딩( )을 사용하여 클래스 하이퍼벡터의 도메인에 매핑할 수 있다. 이러한 HD 인코딩 속성을 활용하여, 각 클래스 별 스칼라 오류 정보를 베이스 하이퍼벡터의 오류로 변환하게 된 다. 하기의 표 1은 본 발명의 초차원 컴퓨팅 방법에 활용된 인코더(Trainable HD)의 학습 단계를 설명하는 알고리즘 이다. 표 1"}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "상세하게는, Trainable HD의 학습 알고리즘은 먼저, 표 1의 Encoding과 같이, 이진화를 위한 sine(·) 함수 활성화를 사용하여 특성 하이퍼벡터(H)를 인코딩한다. 다음, 표 1의 Updating class HVs와 같이, 클래스 하이퍼벡터( )를 업데이트하고, 표 1의 Updating base HVs와 같이, 베이스 하이퍼벡터( )를 업데이트한다. 클래스 하이퍼벡터를 학습시키기 위해, 소프트맥스(softmax(·)) 정규화와 원핫 인코딩 벡터(one-hot-encoding vector)( )에 의해 주어진 정답(GT, Ground Truth) 레이블을 사용하여 초차원에서의 유사성을 사용하여 클래 스 별 오류( )를 산출하게 된다. 이 후, 각 클래스 당 오류의 양( )을 고려하여, 각 클래스에 대한 특성 하이퍼벡터의 크기를 조정하고, 학 습 속도(λ)로 클래스 하이퍼벡터를 조정한다. 도 2는 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에서, 인코더(Trainable HD)를 이용하여 베이스 하이퍼 벡터를 업데이트하는 과정을 나타낸 예시도이다. 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에서는 도 2의 과정을 통해서, 베이스 하이퍼벡터( )의 부정확 성으로 인한 클래스 별 오류의 정보를 특성 별 오류로 변환하기 위함이다. 인코더(Trainable HD)는 두 단계로 이를 수행하며, 첫 번째 단계로는, 샘플 오류 하이퍼벡터(sample error hypervector)(E)라고 불리는 하이퍼벡터 의 샘플 별 오류를 인코딩하고, 두 번째 단계로는, E에서 각 특성 별 오류 하이퍼벡터를 추정하게 된다. 상세하게는, 인코더(Trainable HD)를 이용하여 첫 번째 단계인 하이퍼벡터에서 샘플별 오류를 인코딩하는 과정 에서는, 단일 샘플에 대해 얼마나 많은 오류가 발생하는지에 대한 하이퍼벡터 유형 정보를 포함하는 샘플 오류 하이퍼벡터(E)를 인코딩한다. 인코딩된 하이퍼벡터(X)는 하기의 두가지 계산을 통해 스칼라 오류( )에 기여한 다. 이는 이진화(즉, 상기의 표 1에 기재된 알고리즘의 에서 )와, 클래스 하이퍼벡터의 불 일치(상기의 표 1에 기재된 알고리즘의 에서, )에 해당한다. 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에서의 인코더(Trainable HD)는 하이퍼벡터 형태의 두 요소인 이진화 계수 하이퍼벡터(F_bz)와 오류 계수 하이퍼벡터(F_err)를 산출하게 된다. 이는 도 2의 ②와 같다. 도 2의 ②-a와 같이, 상기 이진화 계수 하이퍼벡터(F_bz)는 에 의해 계산된다. 여기서, I는 모 든 요소가 1인 하이퍼벡터이고, tanh(·)은 쌍곡선 함수(hyperbolic tangent)이다. 이진화 함수인 sine(·)는 [-1, +1] 범위에서 X의 하이퍼벡터 요소를 증폭하므로, 요소 값이 0에 가까울수록 예 측에서 더 높은 오류를 생성할 수 있으며, 여기서, 오차에 대한 각 요소의 영상은 동일한 범위 내로 제한된다. 이에 따라, 하이퍼벡터 요소가 오류에 대해 미치는 영향을 설명하는 데 적합한 tanh(·)의 제곱을 이용하는 것 이 바람직하다. 더불어, 도 2의 ②-b와 같이, 상기 오류 계수 하이퍼벡터(F_err)는 에 의해 를 기준으 로 의 클래스 당 오류를 인코딩하게 된다. 이는 상기 오류 계수 하이퍼벡터(F_err)가 해당 오류 값으로 크기 가 조정된 모든 클래스 별 오류 하이퍼벡터를 번들링(bundle)하는 것을 의미한다. 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에서의 인코더(Trainable HD)는 상기 이진화 계수 하이퍼벡터와 상기 오류 계수 하이퍼벡터로 인코딩된 요소를 사용하여, 를 통해, 정보를 연결(결합)하여, 샘플 오류 하이퍼벡터를 계산할 수 있다. 인코더(Trainable HD)를 이용하여 두 번째 단계인 E에서 각 특성 별 오류 하이퍼벡터를 추정하는 과정은 도 2의 ③과 같다. 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에서의 인코더(Trainable HD)는 △로 표시되는 베이스 오류 하 이퍼벡터(base error hypervector)를 생성하며, 이는 각 베이스 하이퍼벡터에서 발생하는 기능별 오류를 추정한 다. 각 샘플에 대한 오류를 나타내는 샘플 오류 하이퍼벡터를 사용하여, 원시(raw) 학습 샘플의 더 높은 특성 값이 하이퍼벡터의 더 높은 오류에 기여한다고 가정하여, 각 샘플 당 오류를 특성 도메인에 분배하게 된다. 이를 설 명하자면, 하기의 수학식 2와 같다. 수학식 2"}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이 후, 베이스 오류 하이퍼벡터(△)는 f 개의 하이퍼벡터를 가지며, 각각의 하이퍼벡터는 각 특성 오류의 양에 대한 정보를 포함하게 된다. 베이스 오류 하이퍼벡터를 학습 속도와 번들링하여, 최종적으로 베이스 하이퍼벡터를 업데이트하게 된다. 즉, 정리하자면, 학습 결과를 반영하여 인코더의 값을 변화시킴으로써, 고정된 값을 지닌 종래의 인코더와 달리, 훈련 과정을 수행함에 따라 그 값을 기반으로 업데이트할 수 있어, 각 특성값들의 연관 관계를 보다 명확 하게 파악할 수 있어, 높은 정확도를 갖게 된다. 더불어, 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법은 상술한 바와 같이, 훈련 시간의 오버헤드(overhea d)를 최소화하기 위해, 인코더 인터벌 트레이닝(Encoder interval training) 기법을 적용하는 것이 바람직하다. 매번 업데이트된 베이스 하이퍼벡터로 특성 하이퍼벡터를 인코딩하면, 기존 HD 학습에 추가적인 복잡성을 더할 수 있다. 이를 해소하기 위하여, 인코더 인터벌 트레이닝(EIT, Encoder interval training) 기법을 적용하는 것 이 바람직하다. EIT는 특성 하이퍼벡터를 재사용하여, 학습 과정에서의 인코딩 시간을 단축시킬 수 있다. 도 3은 이러한 EIT의 동작 예시도이다. 첫 번째 반복에서는, 인코더는 특성 하이퍼벡터를 인코딩한다. EIT는 생성된 특성 하이퍼벡터 값을 메모리에 저장한다. 이 후, (n-1) 반복에서는, 이전에 저장된 특성 하이퍼벡터를 사용하여 학습을 수행한다. 여기서, n은 EIT 기간 을 정의하는 하이퍼 매개변수를 의미한다. 베이스 하이퍼벡터는 EIT가 반복되는 과정에서 계속 업데이트됨으로써, 더 나은 특성 하이퍼벡터를 인코딩하게 된다. 모든 n마다 학습 모델 및 베이스 하이퍼벡터의 학습이 재수행되어, 다시 인코딩되게 된다. 이는 학습이 종료될 때까지 반복된다.더불어, 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법은 양자화 과정을 적용하는 것이 바람직하다. 이는, 도 4에 도시된 바와 같이, 양자화 과정을 사용하여 비용이 큰 부동 소수점 연산을 완전히 제거하고, 양자 화 인식 학습(QAT, Quantization-Aware Training)을 사용하여, 양자화에 따른 정확도 손실을 최소화하여 하드웨 어에서의 효율적인 구현이 가능하도록 설계하였다. 이러한 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에 의한 인코더에 적용한 QAT 기술은 학습에 사용되는 하이퍼벡터들을 8비트 정수로 생성하지만, 부동 소수점 데이터의 형태로 이를 저장한다. 학습 과정에서 부동 소 수점 초차원 벡터를 이용하여 8비트 정수 하이퍼벡터 표현을 시뮬레이션 하며, 학습이 끝나면, 각각의 하이퍼벡 터를 8비트 정수 형태로 변환하여 저장한다. 이에 따라, 인코더 학습 과정에서, 32비트 실수형을 사용하더라도 결과적으로 8비트 정수의 가중치를 생성하게 되고, 성능/전력면에서 보다 효율적으로 추론을 수행할 수 있다. 즉, 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에 의한 인코더는 인코딩된 하이퍼벡터가 이진화되더라도, 높은 정확도를 위해 부동 소수점 형태의 베이스 하이퍼벡터를 활용하게 된다. 높은 추론 효율성을 제공하기 위 해, 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에 의한 인코더는 양자화를 사용하여 비용이 많이 요구되는 부동 소수점 연산을 완전히 제거하여, 정수 연산 전용 가속기 설계를 가능하게 한다. 상세하게는, 도 4에 도시된 바와 같이, 베이스 하이퍼벡터에 대한 양자화 효과를 모델링하기 위해, 원래의 입력 기능과 현재의 베이스 하이퍼벡터는 가상 양자화(fake-quantization)를 수행한다. 즉, 요소를 고정(clamp)하고 반올림(round)하여 8비트 정수(INT8)로 입력의 대략적인 버전을 생성하지만, 부동 소수점(FLOAT32) 데이터 유형으로 저장하게 된다. 각 클래스 별 하이퍼벡터에 대해서도 동일한 방식의 가상 양자화를 수행한다. 이를 통해서, 학습이 끝나면, 8비트 정수 형태로 훈련된 베이스 하이퍼벡터와 클래스 하이퍼벡터를 변환하여, INT8 도메인에서 추론을 수행하게 된다. 양자화를 사용한 부동 소수점 훈련의 한 가지 과제는 원래 입력 또는, 하이퍼벡터에 대한 양자화 매개 변수를 설정하는 것이다. 이에 따라, 본 발명에서는 아핀 변환(affine transformation)을 통해, 두 개의 매개 변수(s, z)를 사용하여, x 에 대한 양자화를 수행하게 된다. 이는 하기의 수학식 3과 같다. 수학식 3"}
{"patent_id": "10-2023-0045411", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, s는 스케일 팩터(scale factor)이고, z는 양자화된 형식에서 0으로 맵핑되는 값이며, INT8의 경우, b = 8이다. 베이스 하이퍼벡터 및 각 클래스 하이퍼벡터의 업데이트로 인해 입력 x의 상한과 하한이 변경되기 때문에, 학습 반복 중 두 개의 매개 변수도 학습을 수행하게 된다. 이를 위해, 도 4에 도시된 바와 같이, 양자화할 주어진 하이퍼벡터에 대한 최소값(⊥)과 최대값()의 이동 평균 을 관찰한다. 여기서, 이동 평균에 대한 붕괴율(decay rate)은 으로 설정하였으며, , 로 설정하는 것이 바람직하다. 즉, 이동 평균 범위([⊥, ])를 양자화 지점으로 균등하게 나누게 된다. 본 발명에서는 QAT 성능 최적화를 위해, 즉, 가상 양자화 프로세스에서 발생하는 QAT 비용을 최소화하기 위해, 가상 양자화를 수행할 시기를 결정하는 DAU(Drift-Aware Update)라는 최적화 기술을 적용한다. 상술한 바와 같이, 학습율, 즉, , 을 가진 하이퍼벡터 유형 오류를 사용하여, 베이스 하이퍼벡터 및 각 클래스 하이퍼벡터의 업데이트를 수행하게 된다. 초차원 컴퓨팅은 잡음에 강하므로, 베이스 벡터와 하이퍼벡터 간의 사소한 차이는 큰 영향을 미치지 않기 때문 에, 드리프트(drift)라고 하는 , 에 의해 누적된 많은 변화가 관찰될 때에만 가상 양자화를 수행하게 된 다. 다시 말하자면, DAU 최적화는 이전 반복된 i, j에서 가상 양자화를 수행하지 않았던, 인 경 우의 클래스 하이퍼벡터와, 인 경우의 베이스 하이퍼벡터에 대해 가상 양자화를 수행하게 된다. 이러한 본 발명의 초차원 컴퓨팅 방법을 증명하기 위해, 다양한 실험을 진행하였다. 일 예를 들자면, NVIDIA GeForce RTX 3090에서 실행되는 PyTorch를 사용하여, 본 발명의 초차원 컴퓨팅 방법에 서의 인코더(Trainable HD)의 학습 단계를 구현하였다. 추론 과정에서는, CPU(Intel Xeon Silver 4110), 저전 력 GPU(NVIDIA Jetson Xavier), FPGA(Xilinx Zynq-7000)등 부동 소수점 및 정수 벡터 연산을 모두 지원하는 다 양한 가속 플랫폼에서 구현하였다. 또한, CPU용 Intel RAPL, GPU용 NVIDIA Nsight 및 FPGA용 Xilinx Vitis 툴 킷을 사용하여 실행 시간과 전력 소비량을 측정하였다. 이를 통해서, 상술한 양자화 기능을 갖춘 초차원 컴퓨팅 학습을 위한 효율적이고 효과적인 동적 인코더인 Trainable HD에 대한 실험을 진행하였으며, Trainable HD의 더 나은 성능을 위해, 학습 과정 중에 인코더를 지 속적으로 업데이트하여 이전 학습 과정의 문제점이였던 정적 인코더의 제안 요소를 해결하였다. 실험 결과, Trainable HD는 추가 계산 비용 없이 초차원 컴퓨팅의 정확도를 최대 22.26%(평균 3.62%) 향상시킴 을 알 수 있었으며, 저전력 GPU(NVIDIA Jetson Xavier)에서 딥러닝과 비스한 정확도를 달성하고, 56.4배 더 빠 르고 에너지 효율은 73배 더 높은 것을 알 수 있었다. 이상과 같이 본 발명에서는 구체적인 구성 소자 등과 같은 특정 사항들과 한정된 실시예 도면에 의해 설명되었 으나 이는 본 발명의 보다 전반적인 이해를 돕기 위해서 제공된 것 일 뿐, 본 발명은 상기의 일 실시예에 한정 되는 것이 아니며, 본 발명이 속하는 분야에서 통상의 지식을 가진 자라면 이러한 기재로부터 다양한 수정 및 변형이 가능하다. 따라서, 본 발명의 사상은 설명된 실시예에 국한되어 정해져서는 아니 되며, 후술하는 특허 청구 범위뿐 아니라 이 특허 청구 범위와 균등하거나 등가적 변형이 있는 모든 것들은 본 발명 사상의 범주에 속한다고 할 것이다.도면 도면1 도면2 도면3 도면4"}
{"patent_id": "10-2023-0045411", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법의 학습 과정과 추론 과정에 대한 프레임워크를 나타낸 예시도이다. 도 2는 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에서, 베이스 하이퍼벡터를 업데이트하는 과정에 대한 프레임워크를 나타낸 예시도이다. 도 3은 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에서, 인코더 인터벌 트레이닝(Encoder Interval Training) 과정을 나타낸 예시도이다. 도 4는 본 발명의 일 실시예에 따른 초차원 컴퓨팅 방법에서, 양자화 인식 학습(Quantization-Aware Training) 을 이용한 학습 과정에 대한 프레임워크를 나타낸 예시도이다."}
