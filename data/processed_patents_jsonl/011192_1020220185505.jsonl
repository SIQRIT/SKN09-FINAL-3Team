{"patent_id": "10-2022-0185505", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0103386", "출원번호": "10-2022-0185505", "발명의 명칭": "광대역 라이다 스캔 기반 이미지 보간을 통한 3D 스캔 데이터 생성 방법 및 플랫폼 제공 장치", "출원인": "주식회사 럭스피엠", "발명자": "유영웅"}}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "광대역 라이다의 스캔을 기반으로 이미지 정합을 통해 텍스처가 구현된 3D 스캔 데이터를 생성하는 방법에 있어서,라이다가 스캔 대상 구역을 스캔하면, 상기 라이다로 상기 스캔 대상 구역을 스캔하여 생성된 3D 이미지인 제1데이터를 획득하는 단계;상기 스캔 대상 구역을 카메라로 촬영하여 생성된 2D 영상인 제2 데이터를 획득하는 단계;상기 제2 데이터 내에서 피사체의 구조(structure)와 연관된 제1 특징 데이터를 획득하는 단계;상기 제2 데이터 내에서 피사체의 텍스처(texture)와 연관된 제2 특징 데이터를 획득하는 단계;상기 제1 특징 데이터의 변화 값이 미리 설정된 기준 값보다 크면, 상기 제2 특징 데이터를 제1 데이터에 병합하여 제3 데이터를 생성하는 단계를 포함하는 3D 스캔 데이터 생성 방법."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 제1 데이터 획득 단계는,깊이 데이터(depth data) 프레임별로 나눈 뒤, 이를 병합하여 제1 데이터를 생성하는 3D 스캔 데이터 생성방법."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 제1 데이터는,상기 제2 데이터보다 낮은 해상도를 가지며상기 제3 데이터 생성 단계는,상기 제1 특징 데이터의 변화 값이 미리 설정된 기준 값과 같거나 기준 값보다 크면, 상기 제2 특징 데이터를제1 데이터에 병합하여 제3 데이터를 생성하는 3D 스캔 데이터 생성 방법."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,상기 제2 데이터를 분할하여 다각형 그물(Polygon Mesh)형태로 만드는 그물 분할 단계;를 더 포함하고,상기 제1 특징 데이터 획득 단계는,상기 다각형 그물(Polygon mesh)을 구성하는 요소로부터 제1 특징 데이터를 획득하는 단계를 포함하고,상기 제3 데이터 생성 단계는,상기 획득된 제1 특징 데이터의 변화 값이 미리 설정된 기준 값과 같거나 기준 값보다 크면, 상기 제2 특징 데이터를 제1 데이터에 병합하여 제3 데이터를 생성하는 단계를 포함하는 3D 스캔 데이터 생성 방법.공개특허 10-2024-0103386-3-청구항 5 제4 항에 있어서,상기 제1 데이터를 획득함과 동시에 상기 제2 데이터를 획득하는 것을 특징으로 하는 3D 스캔 데이터 생성방법."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서,UV(horizontal-vertical) 매핑 단계;를 더 포함하며,상기 다각형 그물의 제1 특징 데이터는,상기 다각형 그물의 색상, 엣지(edge)의 수, 엣지(edge)의 연결정도, 벡터(vector) 값으로 이루어진 집합에서적어도 하나의 데이터를 포함하는 것을 특징으로 하는 3D 스캔 데이터 생성 방법."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 제3 데이터를 통해 객체를 인식하는 단계;를 더 포함하는 것을 특징으로 하는 3D 스캔 데이터 생성 방법."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6 항에 있어서,상기 제1 데이터를 통해 객체를 인식하는 단계;를 더 포함하는 것을 특징으로 하는 3D 스캔 데이터 생성 방법."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 제3 데이터 생성 단계는,상기 획득된 제1 특징 데이터의 변화 값에 따라 제2 데이터의 해상도를 결정하는 단계를 포함하는 3D 스캔 데이터 생성 방법."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "컴퓨터에 제1 항 내지 제8 항 중 어느 하나의 항에 따른 방법을 실행시키기 위한 프로그램을 기록한 컴퓨터로읽을 수 있는 기록 매체."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "가상 공간 기반의 실시간 커뮤니케이션 플랫폼 서버에 있어서,제1 단말로부터 라이다로 스캔 대상 구역을 스캔하여 생성된 제1 데이터와 상기 스캔 대상 구역을 카메라로 촬영하여 생성된 제2 데이터를 수신하는 송수신부;상기 제1 데이터 및 상기 제2 데이터에 기초하여 가상 공간을 생성하고 피사체를 인식하도록 구성된 프로세서를공개특허 10-2024-0103386-4-포함하되, 상기 프로세서는,상기 제2 데이터 내에서 피사체의 구조와 연관된 제1 특징 데이터를 추출하고;상기 제2 데이터 내에서 피사체의 텍스처와 연관된 제2 특징 데이터를 추출하고;상기 제1 특징 데이터의 변화 값이 미리 설정된 기준 값보다 크면, 상기 제2 특징 데이터를 상기 제1 데이터에병합하여 상기 가상 공간을 생성하는,플랫폼 서버."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 프로세서는,상기 가상 공간 내에서 적어도 하나 이상의 객체를 인식하고;인식된 객체를 이용하여 객체 리스트를 생성하고;상기 송수신부가 상기 객체 리스트를 제2 단말에 전송하도록 제어하는,플랫폼 서버."}
{"patent_id": "10-2022-0185505", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12 항에 있어서,상기 프로세서는상기 제1 단말과 상기 제2 단말에 상기 객체 리스트를 바탕으로 한 실시간 커뮤니케이션 인터페이스를 제공하는플랫폼 서버."}
{"patent_id": "10-2022-0185505", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 3D 스캔 데이터 생성 방법에 관한 것으로, 광대역 라이다의 스캔을 기반으로 이미지 정합을 통해 텍스 처가 구현된 3D 스캔 데이터를 생성하는 방법에 있어서, 라이다가 스캔 대상 구역을 스캔하면, 라이다로 스캔 대 상 구역을 스캔하여 생성된 3D 이미지인 제1 데이터를 획득하는 단계, 스캔 대상 구역을 카메라로 촬영하여 생성 된 2D 영상인 제2 데이터를 획득하는 단계, 제2 데이터 내에서 피사체의 구조(structure)와 연관된 제1 특징 데 이터를 획득하는 단계, 제2 데이터 내에서 피사체의 텍스처(texture)와 연관된 제2 특징 데이터를 획득하는 단계, 제1 특징 데이터의 변화 값이 미리 설정된 기준 값보다 크면, 제2 특징 데이터를 제1 데이터에 병합하여 제3 데이터를 생성하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0185505", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 3D 스캔 데이터 생성 방법 및 플랫폼 제공 장치에 관한 것이다. 보다 구체적으로 광대역 라이다 스캔 데이터 및 이미지 데이터를 보간하여 보다 높은 해상도의 3D 이미지를 신속하게 생성하기 위한 3D 스캔 데이터 생성 방법에 관한 것이다."}
{"patent_id": "10-2022-0185505", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래에는 물체에 대한 3차원 모델을 제작하기 위해, 일반적으로 CAD와 같은 프로그램을 이용하여 3차원 모델링 작업을 수행하였다. 이러한 작업을 수행하기 위해서는 어느 정도 숙련된 기술이 필요하기 때문에, 대부분 전문 가에 의해 3차원 모델링 작업이 수행되었다. 이에 따라 3차원 모델링 작업에 많은 시간과 비용이 소모되었고, 제작된 3차원 모델은 작업자에 따라 품질의 차이가 크다는 문제점이 있었다. 최근에는 대상 물체를 다양한 각도에서 촬영한 사진 또는 영상을 기반으로 3차원 모델링을 자동화하는 기술이 도입되어, 짧은 시간 내에 3차원 모델을 제작할 수 있게 되었다. 다만, 일반적인 3차원 모델링 자동화 기술들은 이미지에서 특징점(feature point)들을 추출하는 과정을 거치는데, 이러한 방식에 의하면, 물체의 특징에 따라 특징점이 제대로 추출되지 않거나, 물체의 형체가 정확하게 반영되지 않은 3차원 모델이 생성된다는 문제점이있다."}
{"patent_id": "10-2022-0185505", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 기술적 과제는 고화질의 3D 스캔 데이터를 빠른 시간에 생성하기 위한 이미지 보간 방법을 제공하는 것이다. 다만, 본 발명이 해결하고자 하는 기술적 과제들은 상기 과제들로 한정되는 것이 아니며, 본 발명의 기술적 사 상 및 영역으로부터 벗어나지 않는 범위에서 다양하게 확장될 수 있다."}
{"patent_id": "10-2022-0185505", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 과제를 해결하기 위한 본 발명의 일 실시예에 따른 광대역 라이다의 스캔을 기반으로 이미지 정합을 통해 텍스처가 구현된 3D 스캔 데이터를 생성하는 방법은, 라이다로 스캔 대상 구역을 스캔하여 생성된 3D 이미지인 제1 데이터를 획득하는 단계, 스캔 대상 구역을 카메라로 촬영하여 생성된 2D 영상인 제2 데이터를 획득하는 단 계, 제2 데이터 내에서 피사체의 구조(structure)와 연관된 제1 특징 데이터를 획득하는 단계, 제2 데이터 내에 서 피사체의 텍스처(texture)와 연관된 제2 특징 데이터를 획득하는 단계, 제1 특징 데이터의 변화 값이 미리 설정된 기준 값보다 크면, 제2 특징 데이터를 제1 데이터에 병합하여 제3 데이터를 생성하는 단계를 포함한다."}
{"patent_id": "10-2022-0185505", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 3D 스캔 데이터 생성 방법은 피사체의 구조 변화 정도가 클 때, 높은 해상도를 가 지는 이미지를 3D 데이터에 병합하고, 피사체의 구조 변화 정도가 작을 때는, 기존 데이터를 그대로 사용함으로 써 3D 스캔 이미지의 해상도를 높이면서도 빠른 시간 내에 3D 스캔 데이터를 생성할 수 있도록 하였다."}
{"patent_id": "10-2022-0185505", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 출원의 상술한 목적, 특징들 및 장점은 첨부된 도면과 관련된 다음의 상세한 설명을 통해 보다 분명해질 것 이다. 다만, 본 출원은 다양한 변경을 가할 수 있고 여러 가지 실시예들을 가질 수 있는 바, 이하에서는 특정 실시예들을 도면에 예시하고 이를 상세히 설명하고자 한다. 명세서 전체에 걸쳐서 동일한 참조번호들은 원칙적으로 동일한 구성요소들을 나타낸다. 또한, 각 실시예의 도면 에 나타나는 동일한 사상의 범위 내의 기능이 동일한 구성요소는 동일한 참조부호를 사용하여 설명하며, 이에 대한 중복되는 설명은 생략하기로 한다. 본 출원과 관련된 공지 기능 혹은 구성에 대한 구체적인 설명이 본 출원의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 본 명세서의 설명 과정에서 이용되는 숫자(예를 들어, 제1, 제2 등)는 하나의 구성요소를 다른 구성요소와 구분하기 위한 식별기호에 불과하다. 또한, 이하의 실시예에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되 어 부여되거나 혼용되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 이하의 실시예에서, 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 이하의 실시예에서, 포함하다 또는 가지다 등의 용어는 명세서상에 기재된 특징, 또는 구성요소가 존재함을 의 미하는 것이고, 하나 이상의 다른 특징들 또는 구성요소가 부가될 가능성을 미리 배제하는 것은 아니다. 도면에서는 설명의 편의를 위하여 구성 요소들이 그 크기가 과장 또는 축소될 수 있다. 예컨대, 도면에서 나타 난 각 구성의 크기 및 두께는 설명의 편의를 위해 임의로 나타낸 것으로, 본 발명이 반드시 도시된 바에 한정되 지 않는다. 어떤 실시예가 달리 구현 가능한 경우에 특정한 프로세스의 순서는 설명되는 순서와 다르게 수행될 수도 있다. 예를 들어, 연속하여 설명되는 두 프로세스가 실질적으로 동시에 수행될 수도 있고, 설명되는 순서와 반대의 순 서로 진행될 수 있다. 이하의 실시예에서, 구성 요소 등이 연결되었다고 할 때, 구성 요소들이 직접적으로 연결된 경우뿐만 아니라 구 성요소들 중간에 구성 요소들이 개재되어 간접적으로 연결된 경우도 포함한다. 예컨대, 본 명세서에서 구성 요소 등이 전기적으로 연결되었다고 할 때, 구성 요소 등이 직접 전기적으로 연결 된 경우뿐만 아니라, 그 중간에 구성 요소 등이 개재되어 간접적으로 전기적 연결된 경우도 포함한다. 라이다는 광대역 라이다(Lidar) 센서를 통해 광대역 장소를 스캔하여, 스캔 구역에 대한 3D 데이터를 생성할 수 있다. 여기서, 3D 데이터는 특정 구역을 스캔하여 생성된 3D 이미지이다. 라이다는 통신 모듈을 구비하여, 단말 과 무선으로 통신하도록 구성될 수 있다. 카메라는 카메라 촬영을 통해 특정 객체를 촬영하여, 촬영 대상에 대한 2D 데이터를 생성할 수 있다. 여기서, 2D 데이터는 특정 대상을 촬영하여 생성된 2D 이미지이다. 카메라는 통신 모듈을 구비하여, 단말과 무선으로 통 신하도록 구성될 수 있다. 이하에서는 도 1 내지 도 6을 참고하여, 본 출원의 3D 스캔 데이터 생성 방법 및 가상 공간 기반의 실시간 커뮤 니케이션 플랫폼 제공 장치, 및 제공 시스템에 관하여 설명한다. 도 1은 본 출원의 일 실시예에 따른 가상 공간 기반의 실시간 커뮤니케이션 플랫폼 제공 시스템을 도시한 개략 도이다. 플랫폼 제공 시스템은 특정 영역을 3D로 스캔하고자 하는 자가 스캔 대상 구역에 관한 데이터를 플랫폼 서 버로 전송하여 3D 스캔 데이터를 생성하고, 이를 다른 사용자와 공유하며, 해당 데이터를 기반으로 다른 사용자들과 커뮤니케이션을 진행할 수 있도록 구성된 시스템이다. 특히, 본 출원에 따른 플랫폼 제공 시스템은 고화질의 3D 스캔 데이터를 빠른 시간에 제공하는 것을 기술적 특 징으로 한다. 본 출원의 일 실시예에 따른 플랫폼 제공 시스템은 제1 단말, 제2 단말 및 플랫폼 서버로 구성될 수 있다. 제1 단말은 대상 구역의 3D 스캔을 원하는 유저가 데이터를 획득하기 위해 사용하는 단말로 일반 컴퓨터, 스마트폰, 테블릿PC 등 다양한 형태일 수 있다. 제2 단말은 생성된 3D 스캔 데이터를 획득하고자 하는 유 저가 사용하는 단말로 일반 컴퓨터, 스마트폰, 테블릿PC 등 다양한 형태일 수 있다. 제1 단말은 라이다 및/또는 카메라를 이용하여 스캔 대상 구역으로부터 스캔 데이터를 획득할 수 있다. 일 실시예에 따르면, 제1 단말은 라이다 및 카메라를 포함하여 통합된 기기로 구현될 수 있으며, 또는, 제1 단말은 라이다 및/또는 카메라와 분리된 별도의 기기로 구현되고, 라이다 및/또는 카메라와 유선 또는 무선 통신망으로 연결되어 있을 수 있다. 제2 단말은 네트워크를 통해 플랫폼 서버로부터 생성된 데이터를 수신할 수 있다. 필요에 따라, 제1 단말 및 제2 단말은 생성된 데이터에 대해 실시간 대화를 생성할 수 있다. 필요에 따라, 제1 단말 및 제2 단말은 적어도 하나 이상의 단말로 구성될 수 있다. 플랫폼 서버는 제1 단말 및 제2 단말과 유선 또는 무선 통신망으로 연결된다. 플랫폼 서버(30 0)는 제1 단말로부터 입력받은 데이터를 이용하여 가상 공간에 3D 스캔 데이터를 생성한다. 필요에 따라, 플랫폼 서버는 제1 단말의 라이다로 스캔 대상 구역을 스캔하여 생성된 3D 이미지 데이터 및 카메라 로 촬영하여 생성된 2D 영상 데이터를 전송받을 수 있으며, 이를 이용하여 가상 공간에 3D 스캔 데이터를 생성 할 수 있다. 필요에 따라, 플랫폼 서버는 생성된 3D 스캔 데이터 내에서 객체를 인식하고, 인식된 객체를 이용하여 객 체 리스트를 생성하고, 객체 리스트를 바탕으로 실시간 커뮤니케이션 인터페이스를 제공할 수 있다. 도 1을 참조하면, 본 출원의 일 실시예에 따른 플랫폼 서버는 송수신부, 데이터베이스 및 프로 세서를 포함할 수 있다. 플랫폼 서버의 송수신부는 임의의 외부 기기 및 내부 서버와 통신을 수행할 수 있다. 예컨대, 플랫폼 서버는, 송수신부를 통해, 제1 단말로부터 스캔 대상 구역을 스캔하여 생성된 3D 이미지 및 카 메라로 촬영된 2D 영상 데이터를 수신할 수 있다. 또한, 플랫폼 서버는, 송수신부를 통해, 3D 이미지 및 2D 영상 데이터를 병합하여 생성된 3D 스캔 데이터를 임의의 제2 단말로 송신할 수 있다. 플랫폼 서버는, 송수신부를 통해, 네트워크에 접속하여 각종 데이터를 송수신할 수 있다. 송수신부 는 크게 유선 타입과 무선 타입을 포함할 수 있다. 유선 타입과 무선 타입은 각각의 장단점을 가지므로, 경우에 따라서 플랫폼 서버에는 유선 타입과 무선 타입이 동시에 마련될 수도 있다. 여기서, 무선 타입의 경우에는 주로 와이파이(Wi-Fi) 같은 WLAN(Wireless Local Area Network) 계열의 통신 방식을 이용할 수 있다. 또는, 무선 타입의 경우에는 셀룰러 통신, 예컨대, LTE, 5G 계열의 통신 방식을 이용할 수 있다. 다만, 무선 통 신 프로토콜이 상술한 예시에 제한되는 것은 아니며, 임의의 적절한 무선 타입의 통신 방식을 이용하는 것도 가 능하다. 유선 타입의 경우에는 LAN(Local Area Network)이나 USB(Universal Serial Bus) 통신이 대표적인 예이 며 그 외의 다른 방식도 가능하다. 플랫폼 서버는 제1 단말로부터 획득된 데이터를 데이터베이스에 저장하여 관리할 수 있으며, 플 랫폼 서버를 통해 생성된 3D 스캔 데이터를 데이터베이스에 저장하여 관리할 수 있다. 비전 검사 장 치의 메모리는 각종 정보를 저장할 수 있다. 데이터베이스의 예로는 하드 디스크(HDD: Hard Disk Drive), SSD(Solid State Drive), 플래쉬 메모리(flash memory), 롬(ROM: Read-Only Memory), 램(RAM: Random Access Memory) 등이 있을 수 있다. 데이터베이스는 플랫폼 서버에 내장되는 형태나 탈부착 가능한 형태로 제공될 수 있다. 데이터베이스에는 플랫폼 서버를 구동하기 위한 운용 프로그램(OS: Operating System)이나 플랫폼 서버의 각 구성을 동작시키기 위한 프로그램을 비롯해 플랫폼 서버의 동작에 필요한 각종 데이터가 저장될 수 있다. 프로세서는 플랫폼 서버의 전반적인 동작을 제어할 수 있다. 예컨대, 프로세서는 후술할 데이터 를 병합하는 동작 등 플랫폼 서버의 전반적인 동작을 제어할 수 있다. 구체적으로 프로세서는 데이터 베이스 로부터 플랫폼 서버의 전반적인 동작을 위한 프로그램을 로딩하여 실행할 수 있다. 프로세서 는 하드웨어나 소프트웨어 또는 이들의 조합에 따라 AP(Application Processor), CPU(Central Processing Unit), MCU(Microcontroller Unit)나 이와 유사한 장치로 구현될 수 있다. 이때, 하드웨어적으로는 전기적 신호 를 처리하여 제어 기능을 수행하는 전자 회로 형태로 제공될 수 있으며, 소프트웨어적으로는 하드웨어적 회로를 구동시키는 프로그램이나 코드 형태로 제공될 수 있다. 이하에서는 도 2 내지 도 3을 참고하여 본 출원의 일 실시예에 따른 플랫폼 서버의 동작들을 보다 구체적 으로 서술한다. 도 2는 본 출원의 일 실시예에 따른 플랫폼 서버의 동작들을 도시한 도면이며, 도 3은 본 출원의 일 실시예에 따른 이미지 보간 방법을 설명하기 위한 도면이다. 본 출원의 일 실시예에 따른 플랫폼 서버는 광대역 라이다의 스캔을 기반으로 이미지 정합을 통해 텍스처 가 구현된 3D 스캔 데이터를 생성할 수 있다. 이때, 플랫폼 서버는 라이다로 스캔 대상 구역을 스캔하여 생성된 3D 이미지와 스캔 대상 구역을 카메라로 촬영하여 생성된 2D 영상 데이터를 정합하여, 가상 공간에 3D 스캔 데이터를 생성할 수 있다. 본 출원의 일 실시예에 따른 플랫폼 서버는 외부 단말로부터 데이터를 획득할 수 있다. 보다 구체적으로 플랫폼 서버는 제1 단말로부터 라이다(Lidar) 및 카메라를 이용하여 스캔 대상 구역을 스캔한 데이터를 획 득할 수 있다. 여기서 라이다로 스캔 대상 구역을 스캔하여 생성된 데이터는 RGB-D데이터 혹은 다양한 형태의 3D 이미지 데이터일 수 있으며, 카메라로 촬영하여 생성된 데이터는 2D 영상 데이터일 수 있다. 일 실시예에 따르면, 라이다를 이용하여 스캔 대상 구역의 RGB-D(depth) 데이터를 획득할 수 있다. 필요에 따라, 플랫폼 서버는 획득된 RGB-D 데이터를 가공하여 3D 이미지 데이터를 생성할 수 있다. 본 출원의 일 실시예에 따르면, 플랫폼 서버는 획득된 RGB-D 데이터 중 깊이(depth)데이터를 이용하여 3D 이미지 데이터 를 생성할 수 있다. 이렇게 깊이 데이터를 이용하여 3D 이미지 데이터를 생성하는 경우, 보다 빠르게 스캔 대상 구역의 3D 형상을 획득할 수 있다. 이하에서는, 라이다를 통해 획득된 3D 이미지를 제1 데이터로, 카메라로 촬영하여 생성된 2D 영상은 제2 데이터 로 칭하도록 하겠다. 필요에 따라, 제1 데이터는 제2 데이터보다 낮은 해상도를 가질 수 있다. 이는 데이터 병합의 속도를 높이면서 도 높은 화질의 3D 스캔 데이터를 가지도록 하기 위함이다. 본 출원의 일 실시예에 따른 플랫폼 서버는 제2 데이터로부터 제1 및 제2 특징 데이터를 획득할 수 있다. 제1 특징 데이터는 피사체의 구조(structure)와 연관된 데이터이며, 제2 특징 데이터는 피사체의 텍스처 (texture)와 연관된 데이터이다. 제1 특징 데이터는 표면의 평평한 정도를 판단할 수 있는 데이터를 의미한다. 보다 구체적으로, 플랫폼 서버는 피사체의 구조와 연관된 데이터를 획득하기 위하여 제2 데이터의 피사체를 다 각형 그물(Polygon Mesh)형태로 분할할 수 있으며, 다각형 그물(Polygon mesh)을 구성하는 요소로부터 피사체의 구조(structure)와 연관된 제1 특징 데이터를 획득할 수 있다. 구조와 연관된 제1 특징 데이터란, 피사체와 관 련된 기하학적 정보, 다각형 그물의 색상, 엣지(edge)의 수, 엣지(edge)의 연결정도, 벡터(vector) 값, 색상변 화 등과 관련된 데이터를 의미한다. 제2 특징 데이터는 피사체의 텍스쳐와 관련된 데이터로, 피사체의 표면 이미지일 수 있다. 본 출원의 또 다른 실시예에 따르면, 플랫폼 서버는 인공지능을 기반으로 객체의 표면의 평평한 정도를 분 류할 수 있다. 즉, 인공지능은 획득된 제2 데이터를 이용하여 표면의 평평한 정도를 판단할 수 있으며, 인공지 능을 통해 판단된 표면의 평평한 정도를 수치화할 수 있다. 본 출원의 일 실시예에 따른 플랫폼 서버는 제1 특징 데이터의 변화 값이 미리 설정된 기준 값보다 크면, 제2 특징 데이터를 제1 데이터에 병합하여 가상 공간 또는 제3 데이터를 생성할 수 있다. 제1 특징 데이터의 변화 값은 피사체 표면의 평평한 정도를 나타내는 값으로, 변화 값이 클수록 표면이 평평하 지 않고, 변화 값이 작을수록 표면이 평평함을 의미한다. 이 때, 기준 값을 작게 하는 경우, 3D 스캔 데이터의 화질은 더 높아지고, 데이터를 생성하는데 필요한 시간이 길어지게 된다. 이에 사용자는 필요에 맞게 기준 값을 설정할 수 있다. 즉, 플랫폼 서버는 제1 특징 데이터의 변화 값이 미리 설정된 기준 값보다 크면, 해상도가 높은 제2 특징 데이터를 제1 데이터에 병합하여 제3 데이터를 생성하며, 그렇지 않은 경우에는 제2 특징 데이터를 제1 데이터 에 병합하지 않도록 한다. 제1 특징 데이터의 변화 값이 클수록 피사체의 구조의 변화가 크고, 평평하지 않은 것을 의미하며, 이 때 고화질의 제2 특징 데이터를 제1 데이터에 병합함으로써 해상도가 높은 3D 스캔 데이터를획득할 수 있다. 또한, 제1 특징 데이터의 변화 값이 크지 않고, 표면이 평평한 경우에는, 기존의 제1 데이터를 이용하여 3D 스캔 데이터를 구성함으로써 3D 데이터 스캔 속도를 향상시킬 수 있다. 도 5는 본 출원의 일 실시예에 따른 플랫폼 서버 내에서 생성된 가상 공간 내에서 객체를 인식하는 양상을 설명 하기 위한 도면이다. 도 5를 참조하면, 본 출원의 일 실시예에 따른 플랫폼 서버는 가상 공간이 생성되는 경우, 가상 공간 내에 서 적어도 하나 이상의 객체를 인식하고, 인식된 객체를 이용하여 객체 리스트를 생성할 수 있다. 이는 인공지 능을 기반으로 생성된 객체를 인식하는 기술로, 미리 학습된 인공신경망을 포함할 수 있다. 객체 리스트란, 스캔 대상 구역에 위치하는 피사체를 인식하여 작성한 물체의 명칭 리스트일 수 있다. 일 예로, 제1 단말은 사무실을 스캔할 수 있으며, 사무실에 있는 의자, 책상, 컴퓨터, 모니터, 키보드 등을 스캔하여 인 식하고 이들 각각을 리스트화하여 객체리스트를 생성할 수 있다. 본 발명에서, 인공지능(Artificial Intelligence, AI)은 인간의 학습능력, 추론능력, 지각능력 등을 모방하고, 이를 컴퓨터로 구현하는 기술을 의미하고, 기계 학습, 심볼릭 로직(Symbolic Logic) 등의 개념을 포함할 수 있 다. 기계 학습(Machine Learning, ML)은 입력 데이터들의 특징을 스스로 분류 또는 학습하는 알고리즘 기술이다. 인공지능의 기술은 기계 학습의 알고리즘으로써 입력 데이터를 분석하고, 그 분석의 결과를 학습하며, 그 학습의 결과에 기초하여 판단이나 예측을 할 수 있다. 또한, 기계 학습의 알고리즘을 활용하여 인 간 두뇌의 인지, 판단 등의 기능을 모사하는 기술들 역시 인공지능의 범주로 이해될 수 있다. 예를 들어, 언어 적 이해, 시각적 이해, 추론/예측, 지식 표현, 동작 제어 등의 기술 분야가 포함될 수 있다. 기계 학습은 데이터를 처리한 경험을 이용해 신경망 모델을 훈련시키는 처리를 의미할 수 있다. 기계 학습을 통 해 컴퓨터 소프트웨어는 스스로 데이터 처리 능력을 향상시키는 것을 의미할 수 있다. 신경망 모델은 데이터 사 이의 상관 관계를 모델링하여 구축된 것으로서, 그 상관 관계는 복수의 파라미터에 의해 표현될 수 있다. 신경 망 모델은 주어진 데이터로부터 특징들을 추출하고 분석하여 데이터 간의 상관 관계를 도출하는데, 이러한 과정 을 반복하여 신경망 모델의 파라미터를 최적화해 나가는 것이 기계 학습이라고 할 수 있다. 예를 들어, 신경망 모델은 입출력 쌍으로 주어지는 데이터에 대하여, 입력과 출력 사이의 매핑(상관 관계)을 학습할 수 있다. 또는, 신경망 모델은 입력 데이터만 주어지는 경우에도 주어진 데이터 사이의 규칙성을 도출하여 그 관계를 학 습할 수도 있다. 인공지능 학습모델 또는 신경망 모델은 인간의 뇌 구조를 컴퓨터 상에서 구현하도록 설계될 수 있으며, 인간의 신경망의 뉴런(neuron)을 모의하며 가중치를 가지는 복수의 네트워크 노드들을 포함할 수 있다. 복수의 네트워 크 노드들은 뉴런이 시냅스(synapse)를 통하여 신호를 주고받는 뉴런의 시냅틱(synaptic) 활동을 모의하여, 서 로 간의 연결 관계를 가질 수 있다. 인공지능 학습모델에서 복수의 네트워크 노드들은 서로 다른 깊이의 레이어 에 위치하면서 컨볼루션(convolution) 연결 관계에 따라 데이터를 주고받을 수 있다. 인공지능 학습모델은, 예 를 들어, 인공 신경망 모델(Artificial Neural Network), 컨볼루션 신경망 모델(Convolution Neural Network: CNN) 등일 수 있다. 일 실시예로서, 인공지능 학습모델은, 지도학습(Supervised Learning), 비지도 학습 (Unsupervised Learning), 강화 학습(Reinforcement Learning) 등의 방식에 따라 기계 학습될 수 있다. 기계 학습을 수행하기 위한 기계 학습 알고리즘에는, 의사결정트리(Decision Tree), 베이지안 망(Bayesian Network), 서포트 벡터 머신(Support Vector Machine), 인공 신경망(Artificial Neural Network), 에이다부스 트(Ada-boost), 퍼셉트론(Perceptron), 유전자 프로그래밍(Genetic Programming), 군집화(Clustering) 등이 사 용될 수 있다. 이중, CNN은 최소한의 전처리(preprocess)를 사용하도록 설계된 다계층 퍼셉트론(multilayer perceptrons)의한 종류이다. CNN은 하나 또는 여러 개의 합성곱 계층과 그 위에 올려진 일반적인 인공 신경망 계층들로 이루어져 있으며, 가중치와 통합 계층(pooling layer)들을 추가로 활용한다. 이러한 구조 덕분에 CNN은 2차원 구조의 입 력 데이터를 충분히 활용할 수 있다. 다른 딥러닝 구조들과 비교해서, CNN은 영상, 음성 분야 모두에서 좋은 성 능을 보여준다. CNN은 또한 표준 역전달을 통해 훈련될 수 있다. CNN은 다른 피드포워드 인공 신경망 기법들보 다 쉽게 훈련되는 편이고 적은 수의 매개변수를 사용한다는 이점이 있다. 컨볼루션 네트워크는 묶인 파라미터들을 가지는 노드들의 집합들을 포함하는 신경 네트워크들이다. 사용 가능한 트레이닝 데이터의 크기 증가와 연산 능력의 가용성이, 구분적 선형 단위 및 드롭아웃 트레이닝과 같은 알고리즘 발전과 결합되어, 많은 컴퓨터 비전 작업들이 크게 개선되었다. 오늘날 많은 작업에 사용할 수 있는 데이터 세트들과 같은 엄청난 양의 데이터 세트에서는 초과 맞춤(outfitting)이 중요하지 않으며, 네트워크의 크기를 늘리면 테스트 정확도가 향상된다. 컴퓨팅 리소스들의 최적 사용은 제한 요소가 된다. 이를 위해, 심층 신경 네 트워크들의 분산된, 확장 가능한 구현예가 사용될 수 있다. 도 6은 본 출원의 일 실시예에 따른 플랫폼 서버 내에서 인식된 객체 및 객체 리스트를 바탕으로 실시간 커뮤니 케이션 인터페이스를 설명하기 위한 도면이다. 도 6을 참조하면, 본 출원의 일 실시예에 따른 플랫폼 서버는 생성된 가상 공간 내에서 인식된 객체 및 객 체 리스트를 바탕으로 한 실시간 커뮤니케이션 인터페이스를 제공할 수 있다. 즉, 복수개의 단말이 플랫폼 서버 와 통신을 할 수 있으며, 복수개의 단말은 가상 공간 내에서 인식된 객체 및 객체 리스트를 바탕으로 실시간 커 뮤니케이션을 진행할 수 있다. 이하에서는 도 4를 참고하며, 본 출원의 또 다른 일 실시예에 따른 광대역 라이다의 스캔을 기반으로 이미지 정 합을 통해 3D 스캔 데이터를 생성하는 방법들을 구체적으로 서술한다. 도 4는 본 출원의 일 실시예에 따른 3D 스캔 데이터를 생성하는 과정을 설명하기 위한 순서도이다. 도 4를 참고하면, 먼저, S101 단계에서, 라이다로부터 제1 데이터를 획득할 수 있다. 여기서, 제1 데이터는 스 캔 대상 구역을 라이다로 스캔하여 생성된 3D 이미지이다. 구체적으로, 라이다는 사용자 조작에 의해 스캔 대상 구역이 지정되면, 스캔 대상 구역에 대한 스캔을 진행할 수 있으며, 스캔 대상 구역에 대한 스캔이 완료되면, 스캔 대상 구역을 스캔하여 생성된 3D 이미지인 제1 데이 터를 생성할 수 있다. 일 예로, 제1 단말은 스캔 대상 구역에 대한 RGB-D 데이터를 생성하고 이를 서버에 전송 할 수 있다. 필요에 따라, 서버는 획득된 데이터를 가공하여 3D 이미지를 생성할 수 있다. 보다 구체적으로, 서버는 획득된 RGB-D 데이터 중 깊이(depth)데이터를 프레임별로 나눈 뒤, 이를 병합함으로써 3D 이미지를 생성할 수 있다. 제 1 데이터는 RGB-D 데이터 혹은 가공된 3D 이미지 데이터일 수 있다. S102 단계에서, 서버는 카메라로 스캔 대상 구역을 촬영하여 생성된 2D 영상인 제2 데이터를 획득할 수 있다. 필요에 따라, 서버는 제1 단말을 통해 동시에 촬영된 3D 이미지와 2D 영상 데이터를 획득할 수 있다. 제2 데이 터는 제1 데이터에 대응되는 대상 구역에 대한 2D 영상 데이터 일 수 있으며, 필요에 따라 제1 데이터와 제2 데 이터의 이미지 원점을 맞추기 위한 과정을 더 포함할 수 있다. 필요에 따라, 제1 데이터는 제2 데이터보다 낮은 해상도를 가질 수 있다. 이는 3D 스캔 데이터 생성 과정을 보 다 빠르게 하기 위함이다. S103단계에서는, 제2 데이터 내에서 피사체의 구조(structure)와 연관된 제1 특징 데이터를 획득할 수 있다. 필 요에 따라, S103단계에서는, 제2 데이터를 분할하여 다각형 그물(Polygon Mesh)형태로 만드는 단계를 더 포함할 수 있다. 이는 제1 특징 데이터 획득의 정확성을 향상시키기 위함이다. 필요에 따라, 제1 특징 데이터 획득 단 계는 다각형 그물(Polygon mesh)을 구성하는 요소로부터 제1 특징 데이터를 획득하도록 할 수 있다. 제1 특징데 이터는 제2 데이터로부터 획득할 수 있는 피사체의 구조와 연관된 다양한 데이터일 수 있으며, 다각형 그물의 색상, 엣지(edge)의 수, 엣지(edge)의 연결정도, 벡터(vector) 값으로 이루어진 집합에서 적어도 하나의 데이터 를 포함할 수 있다. S104단계에서는, 제2 데이터 내에서 피사체의 텍스처(texture)와 연관된 제2 특징 데이터를 획득할 수 있다. 필 요에 따라, 획득된 제2 특징 데이터는 2D 이미지일 수 있다. S105단계에서는, 제1 특징 데이터의 변화 값이 미리 설정된 기준 값보다 크면, 제2 특징 데이터를 제1 데이터에 병합하여 제3 데이터를 생성할 수 있다. 또는, 제1 특징 데이터의 변화 값이 미리 설정된 기준 값과 같거나 기 준 값보다 크면, 상기 제2 특징 데이터를 제1 데이터에 병합하여 제3 데이터를 생성할 수 있다. 이 때, 제1 특 징 데이터의 변화 값이 미리 설정된 기준 값보다 작은 경우에는 제1 데이터를 그대로 사용하여 제3 데이터를 생 성한다. 제1 특징 데이터의 변화 값은 피사체 표면의 평평한 정도를 나타내는 값으로, 변화 값이 클수록 표면이 평평하 지 않고, 변화 값이 작을수록 표면이 평평함을 의미한다. 이 때, 기준 값을 작게 하는 경우, 3D 스캔 데이터의화질은 더 높아지고, 데이터를 생성하는데 필요한 시간이 길어지게 된다. 이에 사용자는 필요에 맞게 기준 값을 설정할 수 있다. 필요에 따라, 제3 데이터 생성 단계에서 획득된 제1 특징 데이터의 변화 값에 따라 제2 데이터의 해상도를 결정 하는 단계를 포함할 수 있다. 필요에 따라, 서버는 제1 특징데이터의 변화 값이 클수록 더 높은 해상도를 가지 는 제2 데이터를 이용하여 제3 데이터를 생성할 수 있다. 이를 통해 표면의 구조가 급격하게 변하는 피사체를 고해상도로 구현할 수 있다. S107단계는 제1 데이터 혹은 제3 데이터를 통해 객체를 인식하는 단계를 포함할 수 있다. 객체 인식 단계는 인 공지능을 이용하여 진행될 수 있으며, 이에 대한 자세한 설명은 생략하기로 한다. 필요에 따라, 인식된 객체의 리스트를 작성하는 단계를 더 포함할 수 있다. 필요에 따라, 인식된 객체 및 객체 리스트를 바탕으로 실시간 커뮤니케이션 인터페이스를 제공할 수 있다. 본 발명에 따른 3D 스캔 데이터 생성 방법은 피사체의 구조 변화 정도가 클 때, 높은 해상도를 가지는 이미지를 3D 데이터에 병합하고, 피사체의 구조 변화 정도가 작을 때는, 기존 데이터를 그대로 사용함으로써 3D 스캔 이 미지의 해상도를 높이면서도 빠른 시간 내에 3D 스캔 데이터를 생성할 수 있도록 하였다. 이상에서 실시 형태들에 설명된 특징, 구조, 효과 등은 본 발명의 적어도 하나의 실시 형태에 포함되며, 반드시 하나의 실시 형태에만 한정되는 것은 아니다. 나아가, 각 실시 형태에서 예시된 특징, 구조, 효과 등은 실시 형 태들이 속하는 분야의 통상의 지식을 가지는 자에 의해 다른 실시 형태들에 대해서도 조합 또는 변형되어 실시 가능하다. 따라서 이러한 조합과 변형에 관계된 내용들은 본 발명의 범위에 포함되는 것으로 해석되어야 할 것 이다. 또한, 이상에서 실시 형태를 중심으로 설명하였으나 이는 단지 예시일 뿐 본 발명을 한정하는 것이 아니며, 본 발명이 속하는 분야의 통상의 지식을 가진 자라면 본 실시 형태의 본질적인 특성을 벗어나지 않는 범위에서 이 상에 예시되지 않은 여러 가지의 변형과 응용이 가능함을 알 수 있을 것이다. 즉, 실시 형태에 구체적으로 나타 난 각 구성 요소는 변형하여 실시할 수 있는 것이다. 그리고 이러한 변형과 응용에 관계된 차이점들은 첨부된 청구 범위에서 규정하는 본 발명의 범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2022-0185505", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 출원의 일 실시예에 따른 가상 공간 기반의 실시간 커뮤니케이션 플랫폼 제공 시스템을 도시한 개략 도이다. 도 2는 본 출원의 일 실시예에 따른 플랫폼 서버의 동작들을 도시한 도면이다. 도 3은 본 출원의 일 실시예에 따른 이미지 보간 방법을 설명하기 위한 도면이다. 도 4는 본 출원의 일 실시예에 따른 3D 스캔 데이터를 생성하는 과정을 설명하기 위한 순서도이다. 도 5는 본 출원의 일 실시예에 따른 플랫폼 서버 내에서 생성된 가상 공간 내에서 객체를 인식하는 양상을 설명 하기 위한 도면이다. 도 6은 본 출원의 일 실시예에 따른 플랫폼 서버 내에서 인식된 객체 및 객체 리스트를 바탕으로 실시간 커뮤니 케이션 인터페이스를 설명하기 위한 도면이다."}
