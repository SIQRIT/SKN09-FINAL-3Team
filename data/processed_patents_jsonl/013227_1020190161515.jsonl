{"patent_id": "10-2019-0161515", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0071021", "출원번호": "10-2019-0161515", "발명의 명칭": "영상 분석 장치, 영상 분석 방법 및 기록 매체", "출원인": "(주)제이엘케이", "발명자": "김원태"}}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "하나 이상의 객체를 포함하는 영상을 획득하는 단계;상기 영상으로부터 복수의 세부 영역을 추출하는 단계;상기 복수의 세부 영역들 간의 계층 구조를 분석하는 단계; 및상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계를 포함하는 영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 영상은 3차원 영상인 영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 영상으로부터 복수의 세부 영역을 추출하는 단계는,상기 영상을 이진화하여 초기 세부 영역을 추출하는 단계; 및상기 초기 세부 영역을 세분화하여 최종 세부 영역을 추출하는 단계를 포함하는 영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 초기 세부 영역을 세분화하는 단계는,상기 초기 세부 영역에 포함된 복셀들의 값에 기초하여 수행되는 영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 초기 세부 영역을 세분화하는 단계는,상기 복셀들의 값의 분산을 이용하여 수행되는 영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 초기 세부 영역을 세분화하는 단계는,상기 복셀들의 값의 분산이 소정의 임계치보다 크면, 상기 초기 세부 영역을 둘 이상의 세부 영역으로 세분화하고,상기 세분화하여 획득된 세부 영역을 초기 세부 영역으로 하여, 상기 초기 영역을 세분화하는 단계를 수행하는영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서,상기 초기 세부 영역을 세분화하는 단계는,공개특허 10-2020-0071021-3-상기 복셀들의 값의 분산이 소정의 임계치 이하이면, 상기 초기 세부 영역을 최종 세부 영역으로 결정하는 영상분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 복수의 세부 영역들 간의 계층 구조를 분석하는 단계는,상기 복수의 세부 영역들 중 둘 이상의 세부 영역들이 하나의 영역으로 병합가능한지 판단하는 단계;병합가능한 경우, 상기 둘 이상의 세부 영역들을 상기 하나의 영역으로 병합하는 단계; 및상기 병합된 영역을 상위 영역으로 하고, 상기 둘 이상의 세부 영역들을 하위 영역으로하는 계층 구조를 결정하는 단계를 포함하는 영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 계층 구조를 분석하는 단계는,상기 복수의 세부 영역들이 하나의 영역으로 병합될 때까지 재귀적으로 수행되는 영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서,상기 복수의 세부 영역들 중 둘 이상의 세부 영역들이 하나의 영역으로 병합가능한지 판단하는 단계는,세부 영역 내 픽셀 값, 세부 영역의 위치, 세부 영역의 크기, 세부 영역의 형태, 병합된 영역의 형태 및 세부영역간 인접 부분의 유사도 중 적어도 하나에 기초하여 수행되는 영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계는,상기 객체의 상위 영역에 해당하는 객체 또는 상기 객체의 하위 영역에 해당하는 객체의 분류 결과를 이용하여수행되는 영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 객체의 상위 영역에 해당하는 객체 또는 상기 객체의 하위 영역에 해당하는 객체의 분류 결과에 따라 상기객체의 분류 결과에 적용될 확률이 기정의되고,상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계는, 상기 확률을 이용하여 상기 객체의 분류를 수행하는 영상 분석 방법."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "하나 이상의 마이크로 프로세서를 포함하는 영상 분석 장치로서,상기 프로세서는하나 이상의 객체를 포함하는 영상을 획득하는 단계;상기 영상으로부터 복수의 세부 영역을 추출하는 단계;상기 복수의 세부 영역들 간의 계층 구조를 분석하는 단계; 및상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계를 수행하는 영상 분석 장치.공개특허 10-2020-0071021-4-청구항 14 제13항에 있어서,상기 영상은 3차원 영상인 영상 분석 장치."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13항에 있어서,상기 영상으로부터 복수의 세부 영역을 추출하는 단계는,상기 영상을 이진화하여 초기 세부 영역을 추출하는 단계; 및상기 초기 세부 영역을 세분화하여 최종 세부 영역을 추출하는 단계를 포함하는 영상 분석 장치."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 초기 세부 영역을 세분화하는 단계는,상기 초기 세부 영역에 포함된 복셀들의 값에 기초하여 수행되는 영상 분석 장치."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 초기 세부 영역을 세분화하는 단계는,상기 복셀들의 값의 분산을 이용하여 수행되는 영상 분석 장치."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 초기 세부 영역을 세분화하는 단계는,상기 복셀들의 값의 분산이 소정의 임계치보다 크면, 상기 초기 세부 영역을 둘 이상의 세부 영역으로 세분화하고,상기 세분화하여 획득된 세부 영역을 초기 세부 영역으로 하여, 상기 초기 영역을 세분화하는 단계를 수행하는영상 분석 장치."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제17항에 있어서,상기 초기 세부 영역을 세분화하는 단계는,상기 복셀들의 값의 분산이 소정의 임계치 이하이면, 상기 초기 세부 영역을 최종 세부 영역으로 결정하는 영상분석 장치."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제13항에 있어서,상기 복수의 세부 영역들 간의 계층 구조를 분석하는 단계는,상기 복수의 세부 영역들 중 둘 이상의 세부 영역들이 하나의 영역으로 병합가능한지 판단하는 단계;병합가능한 경우, 상기 둘 이상의 세부 영역들을 상기 하나의 영역으로 병합하는 단계; 및상기 병합된 영역을 상위 영역으로 하고, 상기 둘 이상의 세부 영역들을 하위 영역으로하는 계층 구조를 결정하는 단계를 포함하는 영상 분석 장치.공개특허 10-2020-0071021-5-청구항 21 제20항에 있어서,상기 계층 구조를 분석하는 단계는,상기 복수의 세부 영역들이 하나의 영역으로 병합될 때까지 재귀적으로 수행되는 영상 분석 장치."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제20항에 있어서,상기 복수의 세부 영역들 중 둘 이상의 세부 영역들이 하나의 영역으로 병합가능한지 판단하는 단계는,세부 영역 내 픽셀 값, 세부 영역의 위치, 세부 영역의 크기, 세부 영역의 형태, 병합된 영역의 형태 및 세부영역간 인접 부분의 유사도 중 적어도 하나에 기초하여 수행되는 영상 분석 장치."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제13항에 있어서,상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계는,상기 객체의 상위 영역에 해당하는 객체 또는 상기 객체의 하위 영역에 해당하는 객체의 분류 결과를 이용하여수행되는 영상 분석 장치."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제23항에 있어서,상기 객체의 상위 영역에 해당하는 객체 또는 상기 객체의 하위 영역에 해당하는 객체의 분류 결과에 따라 상기객체의 분류 결과에 적용될 확률이 기정의되고,상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계는, 상기 확률을 이용하여 상기 객체의 분류를 수행하는 영상 분석 장치."}
{"patent_id": "10-2019-0161515", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "하나 이상의 객체를 포함하는 영상을 획득하는 단계;상기 영상으로부터 복수의 세부 영역을 추출하는 단계;상기 복수의 세부 영역들 간의 계층 구조를 분석하는 단계; 및상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계를 실행하는 프로그램을 기록한 컴퓨터 판독 가능한 기록 매체."}
{"patent_id": "10-2019-0161515", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "영상 분석 방법 및 영상 분석 장치가 제공된다. 본 발명의 영상 분석 방법은, 하나 이상의 객체를 포함하는 영상 을 획득하는 단계, 상기 영상으로부터 복수의 세부 영역을 추출하는 단계, 상기 복수의 세부 영역들 간의 계층 구조를 분석하는 단계, 및 상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계를 포함할 수 있다."}
{"patent_id": "10-2019-0161515", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 영상 분석 장치 및 방법에 관한 것이다. 보다 구체적으로, 본 개시는 입력 영상에 포함된 객체를 검 출하고 분류하는 장치 및 방법에 관한 것이다. 상기 입력 영상은 예컨대, 3차원 영상일 수 있다. 상기 3차원 영 상은 예컨대, 3차원 보안 CT 영상일 수 있다."}
{"patent_id": "10-2019-0161515", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "항만, 공항 또는 연구 시설 등의 보안 시설에서는 보안을 강화하고 기술 유출 등을 방지하고자, 통행자의 소지 품을 검색해야 하는 필요성이 야기된다. 이때, 다수의 통행자에 대한 소지품을 보다 빠르고 효율적으로 검색하 기 위한 기술로서, 방사선(X-ray) 등을 이용한 물품 검색 시스템이 활용되곤 한다. 이러한 물품 검색 시스템은 특히 관세 전자 통관 시스템 혹은 보안 검사 시스템 등에 널리 활용된다. 예컨데, 관세 전자 통관 시스템은 수출입 화물에 대한 통관 업무를 전산화한 것으로서, 이를 통해 다자간에 이루어지는관세행정 업무의 효율성을 제고할 수 있다. 또한, 보안 검사 시스템은 통행자의 소지품에 안전 또는 보안 상 문제가 발생 할 수 있는 물품이 있는지 여부를 판단하는 보안 검사 업무를 전산화한 것으로, 이를 통해 보안 구역의 보안 강화를 제고 할 수 있다. 한편, 딥러닝(deep learning)은 매우 방대한 양의 데이터를 학습하여, 새로운 데이터가 입력될 경우 학습 결과 를 바탕으로 확률적으로 가장 높은 답을 선택하는 것으로서, 영상에 따라 적응적으로 동작할 수 있으며, 데이터 에 기초하여 모델을 학습하는 과정에서 특성인자를 자동으로 찾아내기 때문에 최근 인공 지능 분야에서 이를 활 용하려는 시도가 늘어나고 있는 추세이다. 또한, 근래 3차원 보안 CT 영상을 이용한 보안 검색이 진행되고 있는데, 2차원 영상과 차별화되는 3차원 영상의 특성을 고려한 새로운 방법에 대한 연구가 요구된다."}
{"patent_id": "10-2019-0161515", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 기술적 과제는, 영상에 포함된 객체를 분류하는 영상 분석 방법 및 장치를 제공하는 것이다. 또한, 본 개시의 기술적 과제는, 영상에 포함된 객체들의 계층 구조를 이용하여 객체를 분류하는 영상 분석 방 법 및 장치를 제공하는 것이다. 본 개시에서 이루고자 하는 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급하지 않은"}
{"patent_id": "10-2019-0161515", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "또 다른 기술적 과제들은 아래의 기재로부터 본 개시가 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하 게 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0161515", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 양상에 따른 영상 분석 방법은 하나 이상의 객체를 포함하는 영상을 획득하는 단계, 상기 영상으 로부터 복수의 세부 영역을 추출하는 단계, 상기 복수의 세부 영역들 간의 계층 구조를 분석하는 단계, 및 상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계를 포함할 수 있다. 본 발명에 있어서, 상기 영상은 3차원 영상일 수 있다. 본 발명에 있어서, 상기 영상으로부터 복수의 세부 영역을 추출하는 단계는, 상기 영상을 이진화하여 초기 세부 영역을 추출하는 단계, 및 상기 초기 세부 영역을 세분화하여 최종 세부 영역을 추출하는 단계를 포함할 수 있 다. 본 발명에 있어서, 상기 초기 세부 영역을 세분화하는 단계는, 상기 초기 세부 영역에 포함된 복셀들의 값에 기 초하여 수행될 수 있다. 본 발명에 있어서, 상기 초기 세부 영역을 세분화하는 단계는, 상기 복셀들의 값의 분산을 이용하여 수행될 수 있다. 본 발명에 있어서, 상기 초기 세부 영역을 세분화하는 단계는, 상기 복셀들의 값의 분산이 소정의 임계치보다 크면, 상기 초기 세부 영역을 둘 이상의 세부 영역으로 세분화하고, 상기 세분화하여 획득된 세부 영역을 초기 세부 영역으로 하여, 상기 초기 영역을 세분화하는 단계를 수행할 수 있다. 본 발명에 있어서, 상기 초기 세부 영역을 세분화하는 단계는, 상기 복셀들의 값의 분산이 소정의 임계치 이하 이면, 상기 초기 세부 영역을 최종 세부 영역으로 결정할 수 있다. 본 발명에 있어서, 상기 복수의 세부 영역들 간의 계층 구조를 분석하는 단계는, 상기 복수의 세부 영역들 중 둘 이상의 세부 영역들이 하나의 영역으로 병합가능한지 판단하는 단계, 병합가능한 경우, 상기 둘 이상의 세부 영역들을 상기 하나의 영역으로 병합하는 단계, 및 상기 병합된 영역을 상위 영역으로 하고, 상기 둘 이상의 세 부 영역들을 하위 영역으로하는 계층 구조를 결정하는 단계를 포함할 수 있다. 본 발명에 있어서, 상기 계층 구조를 분석하는 단계는, 상기 복수의 세부 영역들이 하나의 영역으로 병합될 때 까지 재귀적으로 수행될 수 있다. 본 발명에 있어서, 상기 복수의 세부 영역들 중 둘 이상의 세부 영역들이 하나의 영역으로 병합가능한지 판단하 는 단계는, 세부 영역 내 픽셀 값, 세부 영역의 위치, 세부 영역의 크기, 세부 영역의 형태, 병합된 영역의 형태 및 세부 영역간 인접 부분의 유사도 중 적어도 하나에 기초하여 수행될 수 있다. 본 발명에 있어서, 상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계는, 상기 객체의 상위 영역에 해당하는 객체 또는 상기 객체의 하위 영역에 해당하는 객체의 분류 결과를 이용하여 수행될 수 있 다. 본 발명에 있어서, 상기 객체의 상위 영역에 해당하는 객체 또는 상기 객체의 하위 영역에 해당하는 객체의 분 류 결과에 따라 상기 객체의 분류 결과에 적용될 확률이 기정의되고, 상기 계층 구조를 이용하여 상기 영상에 포함된 상기 객체를 분류하는 단계는, 상기 확률을 이용하여 상기 객체의 분류를 수행할 수 있다. 본 발명의 다른 양상에 따른 영상 분석 장치는 본 개시에 따른 영상 분석 방법을 수행하는 하나 이상의 마이크 로 프로세서를 포함할 수 있다. 본 발명의 또 다른 양상에 따른 기록 매체는 본 개시에 따른 영상 분석 방법을 실행하기 위한 프로그램을 저장 할 수 있다."}
{"patent_id": "10-2019-0161515", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "본 발명에 대하여 위에서 간략하게 요약된 특징들은 후술하는 본 발명의 상세한 설명의 예시적인 양상일 뿐이며, 본 발명의 범위를 제한하는 것은 아니다."}
{"patent_id": "10-2019-0161515", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 영상에 포함된 객체를 분류하는 영상 분석 방법 및 장치를 제공될 수 있다. 또한, 본 개시에 따르면, 영상에 포함된 객체들의 계층 구조를 이용하여 객체를 분류하는 영상 분석 방법 및 장 치가 제공될 수 있다. 본 개시에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은"}
{"patent_id": "10-2019-0161515", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "아래의 기재로부터 본 개시가 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0161515", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참고로 하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나, 본 개시는 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 본 개시의 실시예를 설명함에 있어서 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그에 대한 상세한 설명은 생략한다. 그리고, 도면에서 본 개시에 대한 설명과 관계없 는 부분은 생략하였으며, 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 개시에 있어서, 어떤 구성요소가 다른 구성요소와 \"연결\", \"결합\" 또는 \"접속\"되어 있다고 할 때, 이는 직접 적인 연결관계뿐만 아니라, 그 중간에 또 다른 구성요소가 존재하는 간접적인 연결관계도 포함할 수 있다. 또한 어떤 구성요소가 다른 구성요소를 \"포함한다\" 또는 \"가진다\"고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 배제하는 것이 아니라 또 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 본 개시에 있어서, 제1, 제2 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용되 며, 특별히 언급되지 않는 한 구성요소들간의 순서 또는 중요도 등을 한정하지 않는다. 따라서, 본 개시의 범위 내에서 일 실시예에서의 제1 구성요소는 다른 실시예에서 제2 구성요소라고 칭할 수도 있고, 마찬가지로 일 실 시예에서의 제2 구성요소를 다른 실시예에서 제1 구성요소라고 칭할 수도 있다. 본 개시에 있어서, 서로 구별되는 구성요소들은 각각의 특징을 명확하게 설명하기 위함이며, 구성요소들이 반드 시 분리되는 것을 의미하지는 않는다. 즉, 복수의 구성요소가 통합되어 하나의 하드웨어 또는 소프트웨어 단위 로 이루어질 수도 있고, 하나의 구성요소가 분산되어 복수의 하드웨어 또는 소프트웨어 단위로 이루어질 수도 있다. 따라서, 별도로 언급하지 않더라도 이와 같이 통합된 또는 분산된 실시예도 본 개시의 범위에 포함된다. 본 개시에 있어서, 다양한 실시예에서 설명하는 구성요소들이 반드시 필수적인 구성요소들은 의미하는 것은 아 니며, 일부는 선택적인 구성요소일 수 있다. 따라서, 일 실시예에서 설명하는 구성요소들의 부분집합으로 구성 되는 실시예도 본 개시의 범위에 포함된다. 또한, 다양한 실시예에서 설명하는 구성요소들에 추가적으로 다른 구성요소를 포함하는 실시예도 본 개시의 범위에 포함된다. 이하, 첨부한 도면을 참조하여 본 개시의 실시예들에 대해서 설명한다. 도 1은 본 개시의 일 실시예에 따른 물품 검색 시스템을 설명하기 위한 도면이다. 물품 검색 시스템은 판독부 및/또는 학습부를 포함할 수 있다. 판독부는 영상 분석 장치 및/또는 출력 장치를 포함할 수 있다. 학습부는 데이터 베이스, 딥러닝 학습부, 알 고리즘 검증부 및/또는 학습된 모델 저장부를 포함할 수 있다. 판독부는 판독 인터페이스로서기능할 수 있으며, 학습부는 중앙 관리되는 인공지능 데이터 센터로서 기능할 수 있다. 이하에서는, 본 개시에 따른 물품 검색 시스템이 전자 통관 시스템 혹은 보안 검색 시스템에 활용되는 경우를 예를 들어 설명한다. 하지만 본 개시에 따른 물품 검색 시스템이 이러한 활용에만 한정되는 것은 아니다. 이외 에도, 본 개시에 따른 물품 검색 시스템은 다양한 목적에 따라 특정 물품을 식별하는 역할을 수행하는 시스템에 서 활용 될 수 있다. 물품 검색 시스템의 입력은 영상, 물품 정보 및/또는 제어 정보를 포함할 수 있다. 상기 영상은 적어도 하나의 객체를 포함하는 물품에 관한 영상일 수 있다. 예컨대, X-Ray 판독 기기가 촬영한 물품에 관한 X-Ray 영상일 수 있다. 상기 영상은 X-Ray 영상 기기가 촬영한 로(raw) 이미지이거나 상기 로 이미 지를 저장 또는 전송하기 위한 임의의 형태(포맷)의 이미지일 수 있다. 상기 영상은 X-Ray 판독 기기가 촬영하 여 모니터와 같은 출력 장치로 전송하는 영상 정보를 캡쳐하여 데이터화함으로써 획득될 수도 있다. 영상은 출 력 장치에 출력되기 전, 또는 영상 분석 장치에 입력되기 전에 강화될 수 있다. 영상을 강화하는 방 법에 대해서는 후술한다. 출력 장치는 영상 또는 강화된 영상을 출력할 수 있다. 영상 분석 장치는 영상 또는 강화된 영상을 입력 받아 후술하는 영상 분석 장치의 동작을 수행할 수 있다. 상기 물품 정보는 대응하는 영상에 포함된 물품에 관한 정보일 수 있다. 예를 들어, 영상 분석 장치로 입력되는 물품이 전자 통관 시스템에서의 화물인 경우, 물품 정보는 수입 신고된 정보 및/또는 통관 목록 리스트 정보를 포함할 수 있다. 다른 예로, 영상 분석 장치로 입력되는 물품이 보안 검사 시스템에서의 물품인 경우, 물품 정 보는 통행자의 식별 정보, 통행자의 보안 레벨 및/또는 통행자의 인가 품목 정보를 포함 할 수 있다. 물품 정보는 영상 분석 장치에 입력되기 전에 소정의 전처리 과정을 거칠 수 있다. 예컨대, 물품 정보에 포함된 물품 목록, 반입 정보 등에 대해 품명의 정제 작업이 수행될 수 있다. 품명의 정제 작업이란 동일 또는 유사한 물품에 대해 입력되는 다양한 물품의 명칭을 통일하는 작업을 의미할 수 있다. 물품 정보의 입력은 선택적일 수 있다. 예컨대, 본 개시의 물품 검색 시스템은 물품 정보의 입력이 없어도 영상만을 입력으로 받아 동작할 수 있다. 상기 물품은 검사 또는 판독 대상의 물품으로서 모든 종류의 물품을 포함할 수 있다. 예컨대, 본 개시에 따른 영상 분석 장치가 전자 통관 시스템에 사용되는 경우, 상기 물품은 특 송 화물, 우편 화물, 컨테이너 화물, 여행자 수송 화물 및 여행자 자신 중 적어도 하나일 수 있다. 다른 예로, 본 개시에 따른 영상 분석 장치가 보안 검색 시스템에 사용되는 경우, 상기 물품은 통행자 소지품 및 통행자 자 신 중 적어도 하나 일 수 있다. 예를 들어, 전자 통관 시스템이 여행자를 판독한 결과, 판독된 여행자가 이상이 있거나 위험한 객체를 과거에 운송한 이력이 있는 요주의 여행자인 경우, 해당 여행자의 화물에 대해서는 다른 여행자의 화물보다 높은 수준 의 분석 및/또는 판독을 수행하도록 할 수 있다. 예컨대, 특정 물품이 요주의 여행자의 화물이라는 정보를 판독 원에게 제공할 수 있다. 다른 예로, 통행자를 판독한 결과, 통행자가 보안 레벨이 높은 통행자인 경우, 해당 통행자의 소지품에 대해서 는 다른 통행자보다 높은 수준의 분석 및/또는 판독을 수행하도록 할 수 있다. 예컨대, 특정 물품이 보안 레벨 이 높은 통행자의 소지품이라는 정보를 판독원에게 제공 할 수 있다. 상기 제어 정보는 영상 판독을 제어하거나, 판독된 영상을 제어하는 정보 일 수 있다. 일예로, 제어 정보는 판 독원에 의해 입력될 수 있다. 예를 들어 제어 정보는 판독원 정보, 관리자 정보, 동작 모드 정보, 판독 민 감도 정보 및/또는 유저 인터페이스 정보를 포함 할 수 있다. 제어 정보의 구체적인 활용에 대해서는 후술 하도 록 한다. 물품 검색 시스템은 영상, 물품 정보 및/또는 제어 정보를 입력 받아 출력 장치에 전송하거나, 영상 분석 장치에 전송할 수 있다. 영상 분석 장치는 미리 학습된 딥러닝 기반의 모델을 이용하여 입 력된 영상을 분석할 수 있다. 영상 분석 장치는 분석된 결과를 출력 장치로 전송할 수 있다. 출력 장 치는 입력된 영상, 물품 정보 및/또는 제어 정보, 영상 분석 장치로부터 전송 받은 영상 분석 결과 및/또는 유저 인터페이스를 출력하고, 판독원은 출력 장치의 출력 결과를 판독할 수 있다. 전술 한 바와 같이, 물품 정보에 대해 정제 작업이 수행될 수 있으며, 또한, 영상 분석 장치에 입력되기 전 및/또는 출력 장치에 출력되기 전에 분석 대상 영상에 대해 영상 강화가 수행될 수 있다. 출력 장치는 모니터, 경고등 등의 시각 정보를 출력하는 장치, 스피커 등의 음향 정보를 출력하는 장치, 바이브레이터 등의 촉각 정보를 출력하는 장치 등 인간이 감지할 수 있는 모든 형태의 신호를 출력할 수 있는장치를 포함한다. 출력 장치를 통해 유저 인터페이스가 제공 될 수 있으며, 판독원은 상기 유저 인터페이 스를 이용하여 물품 검색 시스템의 동작을 제어 할 수 있다. 예를 들어, 판독원은 출력되는 유저 인 터페이스를 이용하여 제어 정보를 입력함으로써, 영상 분석 장치의 동작을 제어 할 수 있다. 상기 영상 분석 장치의 영상 분석 결과, 해당 영상에 검출 대상인 객체, 이상이 있는 객체 또는 위험도가 임계치 이상인 객체가 포함된 경우, 이와 관련된 정보가 영상 분석 결과로서 출력 장치를 통해 출력되고, 판독원은 이를 확인할 수 있다. 상기 영상 분석 장치는 분석 대상 영상을 분석하는 다양한 과정을 수 행할 수 있다. 예컨대, 영상 분석 장치는 분석 대상 영상을 보다 정확히 분석하기 위해, 맥락 분석을 수행 할 수 있다. 상기 영상 분석 장치가 수행하는 다양한 과정 및 맥락 분석에 대해서는 후술한다. 판독원은 출력 장치를 통해 출력된 영상 분석 결과에 기초하여 추가적인 검사의 수행 여부를 결정할 수 있다. 상기 추가적인 검사는 해당 영상에 관한 물품을 직접 열어 해당 물품에 포함된 객체를 확인하는 개장 검사를 포함할 수 있다. 본 명세서에서 검색 대상 객체는 전술한 바와 같이 이상이 있는 객체 또는 위험도가 임 계치 이상인 객체를 의미할 수 있다. 그러나 이에 한정되지 않으며, 본 개시의 시스템에 의해 검출 또는 검색하 고자 하는 다양한 객체를 포함할 수 있다. 영상 분석 장치의 영상 분석 결과, 판독원이 직접 개장 검사를 수행한 후 입력하는 개장 검사 결과 및/또는 영 상 분석 장치가 영상과 물품 정보를 매칭한 매칭 결과 정보 등은 학습부에 전송될 수 있다. 학습부는 새롭게 수신한 정보를 데이터 베이스에 저장하고, 딥러닝 학습부는 데이터 베이스에 저장된 정 보를 이용하여 딥러닝 학습을 수행할 수 있다. 또는 데이터 베이스에 저장됨이 없이, 딥러닝 학습부 가 상기 학습 데이터의 전부 또는 일부를 직접 수신할 수도 있다. 딥러닝 학습부에서 학습된 결과는 알고 리즘 검증부에서 검증되고, 검증된 모델은 학습된 모델 저장부에 업데이트된 모델로서 저장될 수 있 다. 학습된 모델 저장부에 저장된 모델은 다시 영상 분석 장치로 전송되고, 영상 분석 장치는 수신한 모델을 전술한 미리 학습된 딥러닝 기반의 모델로서 업데이트하여 이용할 수 있다. 학습부는 복수 의 영상을 수신하여 합성함으로써 하나의 합성 영상을 생성할 수 있다. 또한 상기 복수의 영상의 각각에 대한 영상 분석 결과, 개장 검사 결과 및/또는 매칭 결과 정보 등을 이용하여 상기 합성 영상에 대응하는 가상의 영 상 분석 결과, 개장 검사 결과 및/또는 매칭 결과 정보를 생성할 수 있다. 학습부는 상기 합성 영상 및 상 기 생성된 가상의 정보 등을 학습 데이터로서 이용할 수 있다. 이에 따르면 학습 데이터의 수가 절대적으로 적 다고 하더라도, 이들 학습 데이터를 합성하거나 병합함으로써, 인공 지능 모델의 학습에 필요한 충분한 양의 학 습 데이터를 생성해 낼 수 있다. 영상의 합성 및 합성 영상에 대한 가상의 정보의 생성에 대해서는 후술한다. 판독부와 학습부는 별개의 장치로 구현될 수도 있고, 동일한 장치 내에서 구현될 수도 있다. 또한 판 독부와 학습부가 포함하는 구성의 일부 또는 전부는 하드웨어로 구성되거나 소프트웨어로 구성될 수 있다. 인공지능 기술은 컴퓨터에게 데이터를 학습시켜 마치 사람처럼 스스로 의사결정을 할 수 있게 하는데, 인공 신 경망(artificial neural network)은 생물학의 신경망에서 영감을 얻은 수학적 모델로서, 시냅스의 결합으로 네 트워크를 형성한 인공 뉴런이 학습을 통해 시냅스의 결합 세기를 변화시킴으로써 문제 해결 능력을 가지는 모델 전반을 의미할 수 있다. 인공 신경망은 일반적으로 입력층(input layer), 은닉층(hidden layer) 및 출력층 (output layer)로 구성되어 있으며 각 층에 포함된 뉴런들이 가중치를 통해 연결되어 있으며, 가중치와 뉴런값 의 선형 결합과 비선형 활성화 함수를 통해 인공 신경망은 복잡한 함수를 근사화할 수 있는 형태를 가질 수 있 다. 인공 신경망 학습의 목적은 출력층에서 계산된 출력과 실제 출력의 값 차이를 최소화 시키는 가중치를 찾는 데 있다. 심층 신경망(deep neural network)은 입력층과 출력층 사이에 여러 개의 은닉층들로 이루어진 인공 신경망으로 서, 많은 은닉층을 통해 복잡한 비선형 관계들을 모델링할 수 있으며, 이처럼 층의 개수를 늘림으로써 고도화된 추상화가 가능한 신경망 구조를 딥러닝(deep learning)이라고 부른다. 딥러닝은 매우 방대한 양의 데이터를 학 습하여, 새로운 데이터가 입력될 경우 학습 결과를 바탕으로 확률적으로 가장 높은 답을 선택하기 때문에 영상 에 따라 적응적으로 동작할 수 있으며, 데이터에 기초하여 모델을 학습하는 과정에서 특성인자를 자동으로 찾아 낼 수 있다. 본 개시의 일 실시예에 따르면, 딥러닝 기반의 모델은 완전 합성곱 신경망(완전 컨볼루션 뉴럴 네트워크, fully convolutional neural network), 합성곱 신경망(컨볼루션 뉴럴 네트워크, convolutional neural network), 순 환 신경망(회귀 뉴럴 네트워크, recurrent neural network), 제한 볼츠만 머신(restricted Boltzmann machine, RBM) 및 심층 신뢰 신경망(deep belief neural network, DBN) 중 적어도 하나를 포함할 수 있으나, 이에 제한되지 않는다. 또는, 딥러닝 이외의 머신 러닝 방법도 포함할 수 있다. 또는 딥러닝과 머신 러닝을 결합한 하이 브리드 형태의 모델도 포함할 수 있다. 예컨대, 딥러닝 기반의 모델을 적용하여 영상의 특징을 추출하고, 상기 추출된 특징에 기초하여 영상을 분류하거나 인식할 때는 머신 러닝 기반의 모델을 적용할 수도 있다. 머신 러닝 기반의 모델은 서포트 벡터 머신(Support Vector Machine, SVM), 에이다부스트(AdaBoost) 등을 포함할 수 있으 나, 이에 한정되지 않는다. 또한, 본 개시의 일 실시예에 따르면, 딥러닝 기반의 모델을 학습하는 방법은 지도 학습(supervised learning), 비지도 학습(unsupervised learning) 또는 강화 학습(reinforcement learning) 중 적어도 하나를 포함할 수 있 으나, 이에 제한되지 않는다. 지도 학습은 일련의 학습 데이터와 그에 상응하는 레이블(label, 목표 출력값)을 이용하여 학습이 이루어지며, 지도 학습에 기초한 신경망 모델은 훈련용 데이터(training data)로부터 함수를 추론해내는 형태의 모델일 수 있다. 지도 학습은 일련의 학습 데이터와 그에 상응하는 목표 출력 값을 수신하고, 입력되는 데이터에 대한 실제 출력 값과 목표 출력 값을 비교하는 학습을 통해 오류를 찾아내고, 해 당 결과를 근거로 모델을 수정하게 된다. 지도 학습은 결과물의 형태에 따라 다시 회귀(regression), 분류 (classification), 검출(detection), 시멘틱 세그멘테이션(semantic segmentation) 등으로 구분될 수 있다. 지 도 학습을 통해 도출된 함수는 다시 새로운 결과값을 예측하는데 사용될 수 있다. 이처럼, 지도 학습에 기초한 신경망 모델은 수많은 학습 데이터의 학습을 통해, 신경망 모델의 파라미터를 최적화하게 된다. 본 개시의 일 실시예에 따르면, 딥러닝 기반의 모델은 입력 영상과 물품에 대한 정보를 학습에 이용할 수 있으 며, 학습된 모델을 생성한 후에도 본 개시의 장치에서 획득된 영상과 물품에 대한 정보를 이용하여 신경망 모델 을 업데이트할 수 있다. 또한, 본 개시의 일 실시예에 따른 딥러닝 기반의 모델은 본 개시의 방법에 의해 출력 되는 분석 결과, 예를 들어 식별된 객체에 대한 이상 유무 또는 위험도, 객체에 관한 정보, 식별된 객체가 검색 대상 객체인지의 여부 등의 예측 결과, 상기 예측 결과와 최종 개장 검사 결과에 대한 비교 정보, 상기 예측 결 과에 대한 평가도 또는 신뢰도 정보 등을 이용하여 신경망 모델을 업데이트할 수 있다. 도 2는 본 개시의 일 실시예에 따른 영상 분석 장치의 구성을 나타내는 블록도이다. 도 2의 영상 분석 장 치는 도 1의 영상 분석 장치의 일 실시예다. 영상 분석 장치는 영상 수신부, 물품 정보 매칭부 및/또는 영상 분석부를 포함할 수 있다. 전술한 바와 같이, 물품 정보의 입력은 선택적이므로, 영상 분석 장치는 물품 정보 매칭부를 포함하 지 않을 수 있다. 물품 정보의 입력에 관한 설명은 도 1을 참조하여 설명된 바와 같다. 영상 수신부는 하나 이상의 객체를 포함하는 물품에 관한 영상을 수신할 수 있다. 영상 수신부가 수 신하는 영상에 관한 설명은 도 1을 참조하여 설명된 바와 같다. 물품 정보 매칭부는 물품 정보 및 영상 수신부에서 수신한 영상을 입력으로 수신하여 물품 정보와 영 상의 매칭을 수행할 수 있다. 상기 물품 정보에 대한 설명은 도 1을 참조하여 설명된 바와 같다. 매칭된 영상과 물품 정보는 판독원에게 출력되어 판독원의 판독 업무를 보조할 수 있다. 또는 매칭된 영상과 물품 정보는 도 1 의 학습부에 전송되어, 딥러닝 모델의 학습에 이용될 수도 있다. 매칭된 영상과 물품 정보는 도 1의 학습 부의 데이터 베이스에 저장되고, 이후, 판독 대상별 및/또는 판독 업무별로 정제되고, 딥러닝 학습부 는 판독 대상별 및/또는 적용될 판독 업무별로 정제된 데이터를 이용하여 학습을 수행할 수 있다. 상기 판 독 대상은 특송 화물, 우편 화물, 컨테이너 화물, 여행자 수송 화물 및 여행자를 포함할 수 있다. 또한 상기 판 독 대상은 통행자 소지품 및 통행자를 포함 할 수 있다. 상기 판독 업무는 물체에 포함된 객체의 이상 유무 또 는 위험 여부 판단, 식별된 객체가 검색 대상 객체인지의 여부 판단, 식별된 객체와 물체에 대한 정보의 매칭 여부에 대한 판단, 물체가 신고된 것인지 또는 신고되지 않은 것인지에 대한 판단을 포함할 수 있다. 전술한 바와 같이 학습부에서 학습된 모델은 영상 분석부에 입력되어 기존의 모델을 업데이트할 수 있다. 이 때, 판독 대상에 따라 적합한 인공 지능이 업데이트될 수 있다. 또한, 전술한 바와 같이, 학습부는 기존의 학습 데이터를 활용하여 새로운 학습 데이터를 생성하고 이를 학습에 이용할 수도 있다. 새로운 학습 데이터는 기존의 영상의 합성 및 데이터의 병합에 의해 생성될 수 있음은 전술한 바와 같다. 영상 분석부는 영상(분석 대상 영상) 또는 영상과 물품 정보를 수신하고, 미리 학습된 딥러닝 기반의 모델 을 이용하여 상기 영상을 분석한 후, 분석된 결과를 출력 장치로 출력할 수 있다. 영상만이 수신되는 경우, 영상 분석부는 영상에 포함된 객체를 식별하고, 식별된 객체에 대한 이상 유무 또는 위험도를 판단할 수 있다. 영상 분석부는 후술하는 맥락 분석 과정을 수행하여 객체 식별의 정확도를 향상시킬 수 있다. 예컨대, 식별된 객체가 금지되거나 부적합하다고 판단되는 경우, 영상 분석부는 해당 객체는 이상이 있거 나 위험하다고 판단될 수 있다. 상기 위험도는 수치로 표현될 수 있고, 소정의 임계치와의 비교를 통해 위험한 객체인지 여부가 판단될 수 있다. 상기 위험도에 관한 수치 및/또는 상기 소정의 임계치는 판독 대상 및/또는 판독 업무에 따라 적응적으로 결정될 수 있다. 영상과 물품 정보가 함께 수신되는 경우, 영상 분석부는 영상과 물품 정보를 이용하여 영상에 포함된 객체 에 대한 분석을 보다 정밀하게 수행할 수 있다. 예컨대, 물품 목록 리스트에 기재된 물품의 종류, 수량 및/또는 크기 정보, 통행자의 보안 레벨 및/또는 통행자의 인가 품목 정보 등을 영상으로부터 객체를 식별하는데 추가적 으로 이용할 수 있다. 영상을 분석하여 식별한 객체와 물품 정보 사이에 불일치가 있는 경우, 이를 영상 분석 결과로서 출력할 수 있다. 영상 분석부가 출력하는 영상 분석 결과는 객체의 위험도, 종류, 양, 수, 크기 및 위치 중 적어도 하나를 포함할 수 있다. 영상 분석 결과가 객체의 위치인 경우, 분석 대상 영상에 해당 객체의 위치를 표시하여 출력 장치로 출력할 수 있다. 해당 객체의 위치는 좌표로 표시될 수도 있으나, 판독원이 용이하게 판독할 수 있도록 출력 영상 내의 해당 위치에 객체를 강조하여 표시할 수 있다. 예컨대, 객체의 에지를 강조하거나 객체를 둘러 싸는 사각 박스를 표시하여 객체를 강조할 수도 있다. 또한, 후술하는 영상 강화 과정을 통해 판독원이 보다 용 이하게 객체를 식별할 수 있도록 소정의 객체 영역을 강화할 수 있다. 예컨대, 소정의 색상에 해당하는 영역을 강화하여, 영역이 보다 명확히 식별될 수 있도록 영상을 변환할 수 있다. 또는, 영상 분석부는 분석 대상 영상에 검색 대상 객체(예컨대, 통관이 금지되거나 부적합한 객체)가 포함 되는지 여부를 판단할 수 있다. 이를 위해, 영상 분석부는 검색 대상 객체에 관한 정보를 수신하거나 미리 저장할 수 있다. 또한, 영상 분석부는 영상에 포함된 객체를 식별하고, 식별된 객체가 검색 대상 객체인지 여부를 판단할 수 있다. 도 3은 영상의 판독 과정을 설명하기 위한 도면이다. 도 3의 (a)는 종래의 판독 과정에 관한 흐름도이고, 도 3의 (b)는 본 개시의 일 실시예에 따른 판독 과정에 관 한 흐름도이다. 도 3의 (a)에 도시된 바와 같이, 기존의 판독 과정에 따르면, 영상 및/또는 물품 정보가 입력되면, 판독자 에게 정보로서 제공된다. 판독자는 영상 및/또는 물품 정보에 기초하여, 개장 검사가 필요한 물품을 선별 한다. 개장 검사를 수행한 결과는 검사 결과로서 입력된다. 도 3의 (b)에 도시된 바와 같이, 본 개시의 일 실시예에 따른 판독 과정에 따르면, 영상 및/또는 물품 정보가 입력되면, 영상 분석 장치는 미리 학습된 딥러닝 기반의 모델을 이용하여 영상을 분석하고, 분석된 결과를 판독자에게 정보로서 제공한다. 또한 영상 분석 장치는 학습 데이터를 인공지능 데이터 센터 로 전송하고, 인공지능 데이터 센터는 학습 데이터를 학습할 수 있다. 인공지능 데이터 센터는 추후 판독 대상별 판독 업무 보조 인공지능으로서 학습된 모델을 영상 분석 장치에 전송할 수 있다. 판독자는 상기 영상 분석 장치의 분석 결과, 영상 및/또는 물품 정보에 기초하여, 개장 검사가 필요한 물 품을 선별할 수 있다. 개장 검사를 수행한 결과는 검사 결과로서 입력될 수 있다. 상기 검사 결과는 인공지능 데이터 센터로 전송되어 학습 데이터로 이용될 수도 있다. 도 4는 본 개시의 일 실시예에 따른 영상 판독 과정에서의 인공지능의 적용 범위를 설명하기 위한 도면이다. 도 4에 도시된 바와 같이, 모든 물품 중 랜덤하게 추출된 샘플을 관리 대상으로 선별할 수 있다. 또는 관리 대상을 선별하기 위한 선별 보조 인공지능을 이용하여, 모든 물품에 대한 위험 분석 을 수행할 수 있고, 이를 통해 관리 대상을 선별할 수 있다. 인공지능의 활용은 전술한 물품의 위험 분석에만 국한되지 않는다. 예컨대, 관리 대상이 선별되면, 이후, 검사를 보조하기 위한 검사 보조 인공지능으로서 활용될 수 있다. 예컨대, 검사 보조 인공지능(46 0)을 적용함으로써, 객체의 식별, 식별된 객체의 이상 유무 또는 위험도 판단 및/또는 검색 대상 객체에 관한 정보를 판독원에게 제공함으로써 판독원의 검사를 보조할 수 있다. 판독원은 검사 보조 인공지능이 제공한 정보 를 활용하여 정밀 검사를 수행할 수 있다. 이하, 도 5 내지 도 11을 참조하여, 도 1의 영상 분석 장치에 입력되기 전 및/또는 출력 장치에 출력 되기 전에 영상을 강화하는 방법의 일 실시예를 설명한다.도 5는 본 개시에 따른 영상 강화를 수행하는 영상 강화 장치의 일 실시예를 도시한 도면이다. 도 5의 영상 강화 장치는 도 1의 영상 분석 장치과는 별개로 구성되거나 또는 그 일부로서 구성될 수 있다. 영상 강화 장치는 영상 수신부, 객체 영상 추출부, 색상 분포 분석부 및/또는 영상 강화부 를 포함할 수 있다. 다만, 이는 본 실시예를 설명하기 위해 필요한 일부 구성요소만을 도시한 것일 뿐, 영 상 강화 장치에 포함된 구성요소가 전술한 예에 한정되는 것은 아니다. 예컨대, 둘 이상의 구성부가 하나 의 구성부 내에서 구현될 수도 있고, 하나의 구성부에서 실행되는 동작이 분할되어 둘 이상의 구성부에서 실행 되도록 구현될 수도 있다. 또한, 일부 구성부가 생략되거나 부가적인 구성부가 추가될 수도 있다. 본 개시의 일 실시예에 따른 영상 강화 장치는 입력 영상을 수신하고, 입력 영상에 포함되어 있 는 객체를 추출하고, 객체를 포함하는 객체 영상을 하나 이상의 영역들로 분할하고, 하나 이상의 영역들 각각에 대해 색상 분포 정보를 획득하고, 색상 분포 정보에 기초하여, 하나 이상의 영역들 중 적어도 일부에 대해 하나 이상의 가중치를 결정하고, 결정된 하나 이상의 가중치를 하나 이상의 영역들 중 적어도 일부에 적용하여 객체 영상에 대한 제1 출력 영상을 생성할 수 있다. 영상을 구성하는 각 픽셀은 휘도(밝기)를 나타내는 휘도 값과 색상을 나타내는 색상 값의 조합에 의해 소정의 밝기와 색상을 가질 수 있다. 이때, 색상 값은 색상을 표현하는 다양한 방식에 따라, 3개 또는 그 이상의 색요 소의 값의 조합에 의해 나타내어질 수 있다. 예컨대, 색상 값은 3개의 색요소(Red(R), Green(G), Blue(B))의 조 합인 RGB 값으로 표현될 수 있다. 예컨대, R, G, B의 각각은 0 내지 255 중 하나의 값을 가짐으로써, 해당 색요 소 각각의 강도를 표현할 수 있다. R, G, B의 각각이 가질 수 있는 값의 범위는 R, G, B의 각각을 표현하는 비 트 수에 기초하여 결정될 수 있다. 예컨대, 8 비트로 표현되는 경우, R, G, B의 각각은 0 내지 255 중 하나의 값을 가질 수 있다. 색상 분포 정보를 획득한다는 것은 해당 영역에 포함된 픽셀들의 색상 값의 색요소를 분석함으로써, 그로부터 획득될 수 있는 다양한 통계값을 획득한다는 것을 의미할 수 있다. 예컨대, 상기 통계값은 해당 영역에 포함된 픽셀들의 색상 값의 색요소 중 평균적으로 가장 큰 값을 갖는 색요소가 무엇인지에 관한 정보일 수 있다. 예컨 대, 해당 영역에 포함된 모든 픽셀들의 R, G 및 B 각각의 값을 더한 값에 기초하여, R, G, B 중 총합 또는 평균 이 가장 큰 색요소가 무엇인지 결정될 수 있다. 또는, 각 픽셀마다, R, G, B 중 가장 큰 값을 갖는 색요소를 해 당 픽셀의 지배적 색상으로 결정하고, 해당 영역에 포함된 모든 픽셀들에 대해 어떤 색상이 지배적 색상으로 가 장 많이 결정되었는지를 판단할 수 있다. 이와 같은 방법으로, 소정 영역의 지배적 색상이 무엇인지가 결정될 수 있다. 예컨대, 소정 영역에 포함된 대다수의 픽셀들의 색상 값들에 대해, 3 개의 색요소(R, G, B) 중 R이 가 장 큰 값을 갖는다면, 해당 소정 영역의 지배적 색상은 적색(Red)인 것으로 판단할 수 있다. 상기 설명에서는, R, G, B의 각각을 기준으로 색상 분포 정보 또는 지배적인 색상을 분석하였다. 그러나, 이에 한정되지 않으며, R, G, B 중 둘 이상의 조합에 의해 표현되는 다양한 색상을 기준으로 분석할 수도 있다. 예컨대, 식별하고자하 는 색상이 오렌지색이라면, 오렌지색을 표현하는 R, G, B의 일부 또는 전부의 조합을 기준으로하여 해당 영역 내의 픽셀의 지배적 색상이 오렌지색인지 여부를 판단할 수 있다. 이하에서, 지배적 색상이 적색인 영역이 영상 강화의 대상인 경우를 가정하고, 가중치를 적용하여 영상을 강화 하는 과정의 일 실시예를 구체적으로 설명한다. 색상 분포 정보에 기초하여 소정 영역의 지배적 색상이 적색인 것으로 판단되면, 해당 영역에 대해 하나 이상의 가중치가 결정될 수 있다. 가중치는 R, G, B 및 휘도의 전부 또는 일부에 대해 결정될 수 있다. 예컨대, 적색을 강화하는 경우, R에 대한 가중치는 1보다 큰 값일 수 있다. 가중치를 적용한다는 것은 해당 영역의 픽셀의 색요소 값에 해당 가중치를 곱한다는 의미일 수 있다. 이 경우, G 및/또는 B에 대한 가중치는 1보다 작은 값일 수 있다. 그럼으로써, 적색이 지배적인 영역은 보다 적색인 영역 으로 강화할 수 있다. 상기에서는 영상의 특정 색상을 강화하는 것에 대해서 설명하였다. 그러나, 본 개시의 영상의 강화는 이에 한정 되지 않으며, 색상 값의 변화 또는 밝기 값의 변화를 모두 포함할 수 있다. 따라서, 필요에 따라서는 휘도 값에 대해서도 가중치를 적용하여 영상을 강화할 수 있다. 이하, 영상 강화 장치의 각 구성부에 대해 설명한다. 영상 수신부는 하나 이상의 객체를 포함하는 입력 영상을 수신할 수 있다. 입력 영상은 영상 분 석 장치에 입력되기 전의 영상 및/또는 출력 장치에 출력되기 전의 영상일 수 있다.객체 영상 추출부는 영상 수신부에서 수신된 입력 영상에 포함되어 있는 객체를 추출하고, 객체를 포 함하는 객체 영상을 하나 이상의 영역들로 분할할 수 있다. 예컨대 객체 영상 추출부는 분석 대상 영상의 픽셀값과 소정의 임계값을 비교하여 픽셀값을 이진화하고, 이진화된 픽셀값을 그룹핑함으로써 입력 영상에 포함 된 객체를 추출할 수 있다. 여기서 객체를 추출한다는 것은 객체와 배경을 구분한다는 의미일 수 있고, 객체는 영상 내의 특정한 물체를 의미하며 또한 배경은 영상에서 객체를 제외한 부분을 의미할 수 있다. 영상의 배경은 영상의 촬영 방법 또는 촬영 장치에 따라 소정의 색상으로 표현될 수 있다. 예컨대, 상기 소정의 색상은 흰색일 수 있다. 영상의 배경을 표현하는 색상이 특정된 경우, 특정된 배경 색상에 기초하여 배경과 객체를 분리할 수 도 있다. 예컨대, 특정된 배경 색상 영역을 입력 영상에서 삭제함으로써 객체를 구분할 수도 있다. 또한 예컨대 객체 영상은 객체 영역을 둘러싸는 사각형 박스(bounding box)를 특정함으로써 획득될 수 있으며, 객체 영상 추출부는 특정된 사각형 박스에 기초하여, 구분된 객체의 위치 정보를 생성할 수 있다. 즉 사각 형 박스는 객체 인식 박스를 의미 할 수 있다. 본 개시의 일 실시예에 따를 때, 입력 영상이 X-Ray 판독 기기가 촬영한 물품에 관한 X-Ray 영상이라고 하면, 물품이 아닌 배경 부분은 불필요하기 때문에 해당 배경 부분은 잘라내고 물품이 존재하는 영역만으로 분석할 수 있다. 특히, 물품들이 컨베이어 벨트를 통해 계속적으로 X-Ray 판독 기기를 통과하는 실제 환경에서는 물품에 대한 영역을 획득하는 것이 중요하다고 할 수 있다. 객체와 배경을 구분하고 객체의 위치 정보를 생성하는 구체 적인 과정은 도 6을 참고하여 자세히 설명한다. 도 6은 본 개시의 일 실시예에 따라, 단일 객체를 포함하는 영상으로부터 객체와 배경을 구분하고, 객체의 위치 정보를 생성하는 과정을 설명하기 위한 도면이다. 도 6의 객체 영상 추출부는 도 5의 객체 영상 추출부 의 일 실시예일 수 있다. 입력 영상은 도 5를 참조하여 설명한 입력 영상일 수 있으며, 예컨대, 단일 객체로서 가방을 포함하는 물품에 관한 영상일 수 있다. 객체 영상 추출부는 먼저 하나의 가방을 포함하는 입력 영상에 대해 크로핑(cropping) 연산을 수행함으로써 가방을 기준으로 주변 영역을 대략적으로(roughly) 잘라버린, 크로핑된 영상을 획득할 수 있다. 그런 다음 객체 영상 추출부는 크로핑된 영상의 픽셀값과 소정의 임계값을 비교 (thresholding)하여 픽셀값을 이진화함으로써 이진화된 영상을 획득할 수 있다. 그리고, 객체 영상 추출부 는 이진화된 영상에서 객체에 대한 부분을 선택하기 위해 근접한 픽셀끼리 그룹핑(군집화, morphology, closing)함으로써, 그룹핑된 영상을 획득할 수 있다. 그런 다음, 객체 영상 추출부는 그 룹핑된 영상에 대해 라벨링(labeling) 및 홀 채우기(hole filling) 연산을 수행하여 가장 큰 형태로 형성 된 픽셀 그룹을 객체에 대한 영역으로 결정하고, 나머지를 배경에 대한 영역으로 결정함으로써 객체 가 추출된 영상을 획득할 수 있다. 또한, 객체 영상 추출부는 추출된 객체 영상에 대한 정보를 이용 하여 입력 영상 내에서의 객체의 위치를 결정할 수 있다. 예컨대, 객체 영상 추출부는 객체 영역을 둘러싸는 사각형 박스를 특정하고, 특정된 사각형 박스에 기초하여, 객체의 위치 정보를 생성할 수 있다. 도 6 을 참조하면, 객체 영상 추출부는 가방을 둘러싸는 사각형 박스를 특정하고, 특정된 사각형 박 스에 기초하여, 가방의 위치 정보를 획득할 수 있다. 예컨대, 가방의 위치 정보는 사각형 박스 를 형성하는 네 개의 꼭지점의 위치 정보일 수 있으나, 이에 제한되지 않는다. 예컨대, 위치 정보는 사각형 박 스의 하나의 꼭지점의 좌표 (x, y) 및 사각형 박스의 가로 길이(width), 세로 길이(height)에 의해 표현될 수도 있다. 상기 하나의 꼭지점의 좌표 (x, y)는 사각형 박스의 좌측 상단 꼭지점의 좌표일 수 있다. 상기 꼭지점의 좌표 (x, y)는 입력 영상의 좌측 상단 꼭지점의 좌표 (0, 0)을 기준으로 특정될 수 있다. 다시 도 5를 참조하여, 객체 영상 추출부는 객체 영상의 크기에 기초하여 객체 영상을 하나 이상의 영역들 로 분할할 수 있다. 상기 하나 이상의 영역들의 각각은 정방형일 수 있다. 예컨대, 객체 영상 추출부는 객 체 영상의 크기에 기초하여 객체 영상을 분할하는 영역들의 개수나 크기를 결정할 수 있다. 예컨대, 객체 영상 이 상대적으로 크거나 소정의 임계치 이상의 크기를 갖는 경우, 더 많은 분할 영역을 갖도록 분할될 수 있다. 또한, 객체 영상을 분할하는 영역들 각각의 크기는 서로 동일하지 않을 수 있다. 또한 객체 영상 추출부는 객체 영상이 정방형이 아닌 경우, 객체 영상을 업샘플링(up-sampling) 또는 다운 샘플링(down-sampling)하여 객체 영상을 정방형으로 변환한 후, 객체 영상을 하나 이상의 영역들로 분할할 수 있다. 예컨대, 객체 영상은 객체 영상 추출부에서 추출된 객체에 대해 해당 객체를 둘러싸는 사각형 박스 를 기초로 획득되므로 객체 영상이 정방형이 아닐 수 있다. 이 경우에 객체 영상 추출부는 해당 객체 영상 에 대해 하나 이상의 영역들로 분할할 수도 있겠으나, 객체 영상의 가로나 세로 방향으로 업샘플링 또는 다운샘 플링 함으로써 정방형의 객체 영상을 획득하고, 획득된 정방형의 객체 영상을 하나 이상의 영역들로 분할할 수도 있다. 예컨대, 도 8을 참조하면, 객체 영상은 가로 9픽셀 및 세로 12픽셀로 구성되어 정방형이 아닐 수 있다. 이 경우, 본 개시에 따르면, 객체 영상을 3x3 크기의 정방형 영역으로 분할할 수도 있으나(이 경우, (객체 영 상의 가로 크기/분할 영역의 가로 크기) x (객체 영상의 세로 크기/분할 영역의 세로 크기) = (9/3) x (12/3) = 12, 총 12개의 영역들을 가진다), 객체 영상의 가로를 업샘플링 하여 가로 12픽셀 및 세로 12픽셀로 구성 되는 영상을 획득하고, 이를 3x3 크기의 영역으로 분할하여 총 16개의 영역으로 분할할 수도 있다. 객체 영상을 분할하는 하나 이상의 영역들의 형태는 정방형으로 한정되지 않는다. 예컨대, 상기 영역은 n과 m이 상이한 양의 정수인 n x m의 형태를 가질 수도 있다. 이러한 경우, 전술한 업샘플링 또는 다운 샘플링이 수행되지 않을 수도 있다. 다시 도 5를 참조하여, 색상 분포 분석부는 객체 영상 추출부에서 분할된 영역들 각각에 대해 색상 분포 정보를 획득하고, 색상 분포 정보에 기초하여 영역들 중 적어도 일부에 대해 하나 이상의 가중치를 결정할 수 있다. 색상 분포 정보는 n(n은 1보다 큰 정수)개의 색상 표현 범위의 각각에 대한 정보를 포함할 수 있다. 색상 표현 범위는 영상 획득 장치의 종류, 성능 등에 따라 달라질 수 있다. 또한, 예컨대, R(red)에 대한 \"색상\"은 8비트 의 영상에서 픽셀값이 (R, G, B) = (255,0,0)을 가지는 픽셀의 색상만을 의미할 수 있으나, R에 대한 \"색상 표 현 범위\"는 상기 픽셀값이 (R, G, B)= (255,0,0)인 경우뿐만 아니라 상기 픽셀값을 기준으로 소정의 범위 내에 있는 유사 색상을 포함하는 의미이다. 예컨대, R에 대한 \"색상 표현 범위\"는 (R, G, B) = (150~255, 0~100, 0~100)의 범위일 수 있다. 즉, (R, G, B) = (150, 100, 100)인 픽셀도 적색(R)의 색상 표현 범위에 포함되는 것으로 정의할 수 있다. 상기 \"색상 표현 범위\"는 식별하고자 하는 색상에 대해 정의될 수 있다. 전술한 예에서 는 적색의 색상 표현 범위를 기준으로 설명하였으나, 녹색(G) 또는 청색(B)의 색상 표현 범위가 정의될 수도 있 다. 또는 R, G, B 중 일부 또는 전부를 조합하여 표현되는 임의의 색상(황색, 주황색, 하늘색 등)에 대한 색상 표현 범위를 정의할 수도 있다. 영상에 포함된 객체 중 예컨대, 주황색으로 표현되는 객체의 영역을 강화하고자 할 경우, 색상 분포 정보의 분석 결과, 주황색의 색상 표현 범위에 포함되는 픽셀이 다수이거나 지배적인 영역 에 대해 가중치를 적용함으로써, 본 개시에 따른 영상 강화를 수행할 수 있다. 가중치를 적용하는 방법은 전술 한 바와 같다. 영상 획득 장치가 R, G(green), B(blue)의 3가지 색요소의 조합에 의해 색상을 표현한다면, 색상 분포 정보는 3 개의 색요소의 일부 또는 전부에 대한 정보를 포함할 수 있다. 색요소가 R, G, B, Y(yellow), P(purple) 5가지 라면 색상 분포 정보는 5개의 색요소의 일부 또는 전부에 대한 정보를 포함할 수 있다. X-Ray 판독 기기가 촬영한 물품에 관한 X-Ray 영상에 있어서, 영상에 포함되어 있는 객체들의 물성(예를 들어, 해당 객체가 유기물, 무기물, 금속 등인지 여부)에 따라 다른 색상 표현 범위를 적용한 X-Ray 영상이 사용되고 있다. 판독원은 색상이 부가된 X-Ray 영상을 판독함으로써, 영상에 포함된 객체의 형태뿐만 아니라, 객체의 물 성에 대해서도 어느 정도의 식별이 가능하다. 본 개시의 영상 강화는 객체의 물성에 따른 색상이 부가된 X-Ray 영상을 입력 영상으로 하여, 색상 분포 정보를 분석하고, 이에 기초하여 특정 색상의 영역을 강화함으로써, 영 상에 포함된 객체 검출의 정확도와 영상을 판독하는 판독원의 가독성을 향상시킬 수 있다. 도 7은 본 개시의 일 실시예에 따른 객체의 물성에 기초하여 색상이 표현된 영상을 나타내는 도면이다. 도 7을 참조하면, X-Ray 판독 기기에 의해 촬영된 가방 영상, 의약품용기 영상 및 여행자수화물 캐리 어 영상을 나타낸다. 가방 고리, 가방 지퍼, 의약품 및 병의 경우 각각 객체의 물성 에 따라 색상 표현 범위(적용된 색상)가 상이함을 확인할 수 있다. 한편, 가방 고리, 가방 지퍼, 의 약품 및 병은 다른 객체들과 구분될 수 있도록 비교적 선명하게 색상이 표현되어 있는 반면에, 수화 물 내의 임의의 내용물의 경우에는 여행자 수화물 영상에서 그 임의의 내용물이 무엇인지도 확 인하기 어렵고 다른 객체들과 구분하기에도 용이하지 않음을 알 수 있다. 이것은 객체의 물성에 기인한 것이다. 예컨대, 금속이나 무기물은 배경과 뚜렷이 구분될 수 있도록 비교적 선명하고 뚜렷한 색상으로 표현되는 반면, 유기물은 옅은 색상으로 표현되어 배경과의 구분이 뚜렷하지 않게 된다. 유기물을 표현하는 색상의 영역에 대해 서는 해당 색상을 강화하는 방법을 통해 배경과 뚜렷하게 구분될 수 있는 선명하고 뚜렷한 색상으로 강화할 수 있다. 다시 말해, 객체의 물성에 따라 색상 표현 범위가 상이하다는 특징을 이용하여, 색상 표현 범위에 따라 영상의 강화 정도를 다르게 할 필요가 있다. 이를 위해, 분할된 영역들 각각에 대한 색상 분포를 분석하여 적어도 일부의 영역에 대해 가중치를 적용할 수 있다. 상기 하나 이상의 가중치는 n개의 색상 표현 범위 또는 색상을 표현하는 n개의 색요소 중 적어도 일부에 대한 가중치를 포함할 수 있다. 예컨대, 하나의 영역이 n개의 색상 표현 범위 또는 색요소를 가진다고 하면, 해당 영 역에서의 가중치의 개수는 1부터 n개를 가질 수 있다. 예컨대, 하나의 영역에 대해 하나의 가중치가 결정되는 경우, 상기 하나의 영역에 포함된 모든 색요소 또는 모 든 색상 표현 범위에 대해 상기 결정된 가중치를 적용할 수 있다. 또는 상기 하나의 영역에 포함된 모든 색요소 또는 모든 색상 표현 범위 중 적어도 일부에 대해 상기 결정된 가중치를 적용할 수도 있다. 예컨대, 영상 강화 를 위해, n개의 색요소 중 소정의 색요소 또는 n 개의 색상 표현 범위 중 소정의 색상 표현 범위에 대해서만 상 기 결정된 가중치를 적용할 수 있다. 또는 예컨대, n개의 색요소 또는 n개의 색상 표현 범위의 각각에 대해 가중치가 결정될 수 있다. 즉, 하나의 영 역에 대한 가중치의 개수는 n일 수 있다. 이 경우, 상기 영역에 포함된 모든 색요소 또는 색상 표현 범위의 각 각에 대응되는 가중치를 해당하는 색요소 또는 색상 표현 범위에 적용할 수 있다. 가중치는 영상 강화의 대상이 되는 소정의 색요소 또는 색상 표현 범위에 대해 상대적으로 높은 가중치가 부여될 수 있다. 예컨대, 1보다 큰 가중치가 부여되어 해당 색요소의 값 또는 해당 색상 표현 범위에 속하는 픽셀값에 곱해질 수 있다. 또는 예컨대, 1보다 크고 n보다 작은 m개의 색요소 또는 색상 표현 범위의 각각에 대해 가중치가 결정될 수 있 다. 즉, 하나의 영역에 대한 가중치의 개수는 m일 수 있다. 이 경우, 상기 영역에 포함된 색요소 또는 색상 표 현 범위 중 가중치가 부여된 색요소 또는 색상 표현 범위에 대해서만 상기 부여된 가중치를 적용할 수 있다. 영 상 강화의 대상이 되는 소정의 색요소 또는 색상 표현 범위에 대해 상대적으로 높은 가중치가 부여되는 것은 전 술한 바와 같다. 전술한 바와 같이, n개의 색요소 또는 색상 표현 범위 중 소정의 색요소 또는 색상 표현 범위에 대해서는 가중 치를 상대적으로 높게 결정할 수 있다. 예컨대, X-Ray 영상에 포함된 객체가 유기물인 경우 다른 물성(금속, 무 기물 등)을 가진 객체에 비해 상대적으로 경계가 덜 선명하게 영상에 표현되는 경우가 많다. 이는 유기물인 객 체의 색상이 주변의 다른 객체 또는 배경과 구분될 수 있을 정도로 선명하게 표현되지 않기 때문이다. 예컨대, 연한 주황색으로 표현됨으로써, 백색의 배경과 잘 구분되지 않는 경우가 있다. 따라서, 분할 영역 중 유기물을 나타내는 색상 표현 범위에 해당하는 부분에 대해서 상대적으로 높은 가중치를 부여함으로써, 해당 색상을 강화 하여, 예컨대, 연한 주황색을 진한 주황색으로 변경할 수 있다. 이와 같은 방식으로 영상을 강화하여, 주변 객 체 또는 배경과, 강화의 대상이 되는 객체의 구분을 보다 명확히 할 수 있다. 상대적으로 높은 가중치가 부여되는 상기 소정의 색요소 또는 색상 표현 범위는 하나 이상일 수 있다. 예컨대, 전체 색요소 또는 색상 표현 범위가 n개일 때, 상대적으로 높은 가중치가 부여되는 상기 소정의 색요소 또는 색 상 표현 범위는 1 내지 n개일 수 있다. 상기 소정의 색요소 또는 색상 표현 범위가 복수 개일 때, 각각에 대해 요구되는 영상 강화의 정도는 상이할 수 있으며, 그에 따라 각각에 대해 상이한 가중치가 부여될 수 있다. 예컨 대, 금속->무기물->유기물의 순서로 영상이 선명하게 표현될 때, 유기물에 대한 색요소 또는 색상 표현 범위에 대해서만 상대적으로 높은 가중치를 부여할 수도 있으나, 무기물과 유기물에 대해 금속보다 상대적으로 높은 가 중치를 부여할 수도 있다. 이때, 무기물보다는 유기물에 상대적으로 높은 가중치가 부여될 수 있다. 분할된 영역들 각각에 대해 색상 분포 정보를 획득하고, 가중치를 결정하는 구체적인 과정은 도 8을 참고하여 자세히 설명한다. 도 8은 본 개시의 일 실시예에 따른 영상의 색상 분포 정보에 기초하여 출력 영상을 생성하는 과정을 설명하기 위한 도면이다. 도 8을 참조하면, 객체 영상은 제1 영역, 제2 영역 등 하나 이상의 영역들로 분할될 수 있다. 객체 영상에서 영역들을 분할하는 과정에 대해서는 도 5의 객체 영상 추출부에 대해서 설명한 바와 같다. 이하에서 제1 영역에서 색상 분포 정보가 획득되고 가중치가 결정되는 과정에 대해 자세히 설명한다. 제1 영역은 3x3 크기의 영역으로서 총 9개의 픽셀을 가지고, 5개(n=5)의 색상 표현 범위(이하, 색상 표현 범위는 색요소로 치환될 수 있음)들을 가지는 것으로 가정한다. 일 실시예에 따른 영상 강화 장치는 제1 영역에 대해 5개의 색상 표현 범위에 대한 정보를 포함하는 색상 분포 정보를 획득하고, 획득된 색상 분포 정보에 기초하여 3x3 크기의 영역 중 적어도 일부에 대해 하나 이상의 가중치를 결정할 수 있다. 또는 영상 강화의 대상이 되는 소정의 색상 표현 범위에 대한 정보만을 색상 분포 정보로서 획득하고 이용할 수 도 있다. 예컨대, 소정의 색상 표현 범위에 대한 분포 정보가 소정의 임계치 이상인 경우, 해당 영역은 강화의대상으로 결정되고, 해당 영역에 상대적으로 높은 가중치가 부여될 수 있다. 색상 분포 정보에 기초하여 가중치를 결정하는 일 실시예를 보다 구체적으로 살펴보면, 분할된 영역들 각각에 대해서, n개의 색상 표현 범위 각각에 대한 정보를 반영하는 n개의 색상 채널 영상들이 획득될 수 있다. 예컨대, 제1 영역은 5개(n=5)의 색상 표현 범위들에 대한 정보를 가지므로 5개의 색상 채널 영상들이 획득 될 수 있으며, 이를 제1 색상 채널 영상, 제2 색상 채널 영상, 제3 색상 채널 영상, 제4 색상 채널 영상 및 제5 색상 채널 영상이라 한다. 또한 예컨대, X-Ray 판독 기기가 R, G, B, Y, P의 5가 지 색요소를 지원하는 경우, 제1 색상 채널 영상, 제2 색상 채널 영상, 제3 색상 채널 영상, 제 4 색상 채널 영상 및 제5 색상 채널 영상은 각각 R, G, B, Y 및 P의 색요소에 대응될 수 있다. 제1 내지 제5 색상 채널 영상(830~870)의 각각은 제1 영역의 구성 픽셀들 각각의 색상 정보에 기초하여 해 당 색상 정보에 대응하는 색상 채널 영상에 각 픽셀을 매핑시킴으로써 생성될 수 있다. 예컨대, 제1 픽셀 은 제3 색상 채널 영상의 해당 위치의 픽셀에 매핑되고, 또한 제2 픽셀은 제1 색상 채널 영상 의 해당 위치의 픽셀에 매핑되고, 또한 제3 픽셀은 제5 색상 채널 영상의 해당 위치의 픽 셀에 매핑되고, 또한 제4 픽셀은 제2 색상 채널 영상의 해당 위치의 픽셀에 매핑되고, 또 한 제5 픽셀은 제5 색상 채널 영상의 해당 위치의 픽셀에 매핑되고, 또한 제6 픽셀은 제5 색상 채널 영상의 해당 위치의 픽셀에 매핑되고, 또한 제7 픽셀은 제2 색상 채널 영상의 해당 위치의 픽셀에 매핑되고, 또한 제8 픽셀은 제5 색상 채널 영상의 해당 위치의 픽셀에 매핑되고, 또한 제9 픽셀은 제5 색상 채널 영상의 해당 위치의 픽셀에 매핑됨으로써, 상기 제1 내지 제5 색상 채널 영상(830~870)이 생성될 수 있다. 한편, 색상 표현 범위가 최대 n개인 경우, n보다 적은 수의 색상 채널 영상들이 획득될 수 있는데, 예컨대, 제1 영역의 경우 제4 색상 채널 영상에 해당되는 색상을 가진 픽셀이 존재하지 않기 때문에, 제4 색상 채 널 영상을 제외한 총 4개의 색상 채널 영상들이 획득될 수 있다. 색상 채널 영상들이 획득되면, 제1 색상 채널 영상, 제2 색상 채널 영상, 제3 색상 채널 영상, 제4 색상 채널 영상 및 제5 색상 채널 영상에 가중치 a1, a2, a3, a4 및 a5를 각각 적용할 수 있다. 가중치는 각 영역을 구성하는 픽셀들의 색상 분포를 고려하여 결정될 수 있으며, 예컨대, 픽셀들의 색상 분포에 비례하도록 가중치가 결정될 수 있다. 또는, 가중치는 소정의 색상 표현 범위에 대해서는 가중치를 상대적으로 높게, 나머지 색상 표현 범위에 대해서는 가중치를 상대적으로 낮게 결정할 수 있다. 다시 도 5를 참조하여, 영상 강화부는 색상 분포 분석부에서 결정된 하나 이상의 가중치를 하나 이상 의 영역들 중 적어도 일부에 적용하여 객체 영상에 대한 제1 출력 영상을 생성할 수 있다. 도 8을 참조하면, 제1 색상 채널 영상, 제2 색상 채널 영상, 제3 색상 채널 영상, 제4 색상 채 널 영상 및 제5 색상 채널 영상에 가중치 a1, a2, a3, a4 및 a5를 적용하고, 가중치가 적용된 상기 제1 내지 제5 색상 채널 영상을 결합하여 가중치가 적용된 제1 영역(810-1)이 획득될 수 있다. 그리고, 상기 객 체 영상의 나머지 영역들에 대해서도 상기 과정을 반복함으로써, 최종적으로 제1 출력 영상이 생성될 수 있다. 가중치는 각 영역을 구성하는 픽셀들의 색상 분포를 고려하여 결정되고 또한 소정의 색상 표현 범위에 대 해서는 상대적으로 높은 가중치가, 나머지 색상 표현 범위에 대해서는 상대적으로 낮은 가중치가 결정될 수 있 다. 예컨대, 각 분할 영역에서 유기물을 나타내는 색상에 해당되는 부분은 배경과의 구분이 뚜렷하지 않아 경계 부분이 영상에서 상대적으로 명확하게 표현되지 않으므로 가중치가 상대적으로 높게 결정되고, 금속을 나타내는 색상에 해당되는 부분은 배경과의 구분이 비교적 뚜렷하므로 경계 부분이 영상에서 상대적으로 명확하게 표현되 기 때문에 가중치가 상대적으로 낮게 결정될 수 있다. 가중치를 적용한다는 것은 전술한 바와 같이, 강화되는 영역 내 픽셀을 가중치가 곱해진 새로운 픽셀값으로 치환하는 것을 의미할 수 있다. 또는 전술한 바와 같이, 객체 영상에 포함된 영역의 색상 분포 분석 결과, 영상 강화의 대상이 되는 소정의 색상 표현 범위가 지배적이거나 소정의 임계치 이상의 분포를 갖는 경우, 해당 영역에 대해 상대적 으로 높은 가중치를 설정할 수 있다. 예를 들어, 강화의 대상이 되는 색상 표현 범위가 적색(R)이고, 소정 영역의 지배적 색상이 적색일 때, 해당 영 역에 1 이상의 가중치가 적용될 수 있다. 예컨대, 상기 소정 영역 내 픽셀의 (R, G, B) = (120, 10, 10) 일 때, 가중치 2를 적용함으로써, (R, G, B) = (240, 20, 20)으로 강화할 수 있다. 또는, R, G, B 중 일부, 예컨대, 적색에만 가중치 2가 적용되어, (R, G, B) = (240, 10, 10)으로 강화할 수도 있다. 또는, R, G, B 중 일부, 예 컨대, 적색에는 가중치 2가 적용되고, 다른 일부, 예컨대, 녹색과 청색에는 가중치 0.5가 적용되어, (R, G, B)= (240, 5, 5)으로 강화할 수도 있다. 상기 예시는 적색을 강화하는 경우에 관한 것이나, 이에 한정되지 않으며, 임의의 색상을 강화의 대상이 되는 색상으로 결정할 수 있다. 상기 소정의 임계치 및/또는 가중치는 임의로 결정될 수도 있고, 축적된 영상 처리 정보에 기초하여 결정될 수 도 있다. 또는, 인공지능 기반 학습 모델을 통해 상기 임계치 및/또는 가중치에 관한 학습을 수행함으로써, 최 적의 임계치 및/또는 가중치가 계속하여 업데이트될 수 있다. 또한, 영상 강화부는 하나 이상의 영역들 중 적어도 일부에 대해 에지 기반 필터링 또는 평활화 필터링을 적용하여 객체 영상에 대한 제2 출력 영상을 생성할 수 있다. 또한, 영상 강화부는 생성된 제1 출력 영상 및 제2 출력 영상에 기초하여 객체 영상에 대한 제3 출력 영상을 생성할 수 있다. 에지 기반 필터링 또는 평활화 필터링은 영상의 컨트라스트(contrast)를 강화하기 위한 기법으로서, 예컨대 Wiener filtering, Unsharp mask filtering, Histogram equalization, linear contrast adjustment의 기법 등 을 포함하나, 이에 제한되지 않으며, 영상의 컨트라스트를 강화하기 위한 기법들을 포함할 수 있다. 도 9는 본 개시의 일 실시예에 따른 색상 분포 정보를 이용하여 획득된 영상과 에지 기반 필터링 또는 평활화 필터링을 적용하여 획득된 영상을 결합한 최종 출력 영상을 획득하는 과정을 설명하기 위한 도면이다. 도 9의 객체 영상, 제1 영역 및 가중치가 적용된 제1 영역(910-1)은 도 8의 객체 영상, 제1 영역 및 가중치가 적용된 제1 영역(810-1)에 각각 대응될 수 있다. 도 9를 참조하면, 영상 강화부는 제1 영역 에 대해 상기 필터링이 적용된 제1 영역(910-2)을 생성할 수 있으며, 가중치가 적용된 제1 영역(910-1)과 필터링이 적용된 제1 영역(910-2)을 결합하여 최종 제1 영역(910-3)을 생성할 수 있다. 그리고, 영상 강화부 는 나머지 영역들에도 상기와 같은 필터링 기법들이 적용된 제2 출력 영상 및 제1 출력 영상과 제2 출력 영상을 결합한 제3 출력 영상을 생성할 수 있다. 가중치가 적용된 영역(예컨대, 910-1), 필터링이 적용된 영역(예컨대, 910-2) 및/또는 상기 둘을 이용한 최종 영역(910-3)을 생성하는 과정은 영역 단위로 수행될 수 있다. 그러나 이에 한정되지 않으며, 상기 과정은 객체 영상 단위로 수행될 수도 있다. 예컨대, 객체 영상에 포함된 영역의 각각에 대해 가중치를 적용하는 과정을 수 행하여 가중치가 적용된 객체 영상(제1 출력 영상)을 획득할 수 있다. 또한, 객체 영상에 포함된 영역의 각각에 대해 상기 필터링을 적용하는 과정을 수행하여 객체 영상(제2 출력 영상)을 획득할 수 있다. 또한, 상기 가중치 가 적용된 객체 영상과 상기 에지가 강화된 객체 영상을 결합함으로써, 상기 최종 영상(제3 출력 영상)을 생성 할 수 있다. 한편, 예컨대 분할 영역 내 유기물이 다른 물질에 비해 적게 포함된 경우에는, 제1 출력 영상에 제2 출력 영상 을 결합함으로써 제1 출력 영상에 대한 영향이 상대적으로 적을 수 있는데, 이 경우 유기물을 나타내는 색상 분 포 정보에 대한 가중치를 상대적으로 더 높게 결정할 수 있다. 또한 예컨대, 제1 출력 영상과 제2 출력 영상을 결합함으로써 영상 내 여러 객체가 겹쳐있는 경우에도 보다 정확한 객체 인식이 가능하게 할 수 있다. 도 10은 본 개시의 일 실시예에 따른 그래피컬 모델을 이용하여 최종 출력 영상을 획득하는 과정을 설명하기 위 한 도면이다. 일 실시예에 따른 영상 강화 장치는 색상 분포 정보에 포함된 색상 표현 범위들 각각을 개별 노드로 결정하고, 결정된 각 개별 노드간의 상대적인 관계 및 제1 출력 영상, 제2 출력 영상 및 제3 출력 영상과의 상대적인 관계 를 이용하여, 계층적(hierarchical) 구조의 그래피컬 모델(graphical model)을 생성할 수 있다. 도 10을 참조하면, 각각의 분할 영역에서, 색상 분포 정보에 n개의 색상 표현 범위가 포함되어 있으면, 가장 하 위 노드는 제1 색상 분포 정보(1010-1)부터 제n 색상 분포 정보(1010-n)까지 최대 n개의 노드를 포함할 수 있다. 그런 다음, 각 색상 분포 정보에 기초하여 해당 분할 영역 또는 해당 분할 영역의 색상 표현 범위들의 각 각에 가중치를 적용하여 제1 출력 영상을 획득할 수 있다. 제1 출력 영상이 최종 출력 영상으로 결 정될 수 있다. 또는 영상의 컨트라스트 강화 기법을 적용하여 획득된 제2 출력 영상을 더 생성하고, 제1 출력 영상과 제2 출력 영상을 기초로 제3 출력 영상을 생성할 수도 있다. 도 11은 본 개시의 일 실시예에 따른 영상 강화 방법을 설명하기 위한 도면이다. 도 11의 영상 강화 방법은 도 5의 영상 강화 장치가 수행하는 방법으로서, 도 5의 영상 강화 장치에 관한 설명은 도 11의 영상 강화 방법에 적용될 수 있다. S1100 단계에서, 입력 영상을 수신할 수 있다. S1110 단계에서 입력 영상에 포함되어 있는 객체를 추출할 수 있다. 예컨대, 입력 영상의 픽셀값과 소정의 임계 값을 비교하여 픽셀값을 이진화하고, 이진화된 픽셀값을 그룹핑함으로써 분석 대상 영상에 포함된 객체를 추출 할 수 있다. S1120 단계에서 객체를 포함하는 객체 영상을 하나 이상의 영역들로 분할할 수 있다. 예컨대, 객체 영상의 크기 에 기초하여 객체 영상을 분할하는 영역들의 개수나 크기를 결정할 수 있다. 또한, 객체 영상을 분할하는 영역 들 각각의 크기는 서로 동일하지 않을 수 있다. 또한, 예컨대 객체 영상이 정방형이 아닌 경우, 객체 영상을 업 샘플링(up-sampling) 또는 다운샘플링(down-sampling)하여 객체 영상을 정방형으로 변환한 후, 객체 영상을 하 나 이상의 영역들로 분할할 수 있다. S1130 단계에서 하나 이상의 영역들 각각에 대해 색상 분포 정보를 획득할 수 있다. 예컨대, 색상 분포 정보는 n(n은 1보다 큰 정수)개의 색상 표현 범위의 각각에 대한 정보를 포함할 수 있다. S1140 단계에서, 색상 분포 정보에 기초하여, 하나 이상의 영역들 중 적어도 일부에 대해 하나 이상의 가중치를 결정할 수 있다. 예컨대, 하나 이상의 가중치는 n개의 색상 표현 범위 중 적어도 일부에 대한 가중치를 포함할 수 있다. 예컨대, 하나의 영역이 n개의 색상 표현 범위를 가진다고 하면, 해당 영역에서의 가중치의 개수는 1부 터 n개를 가질 수 있다. S1150 단계에서, 결정된 하나 이상의 가중치를 하나 이상의 영역들 중 적어도 일부에 적용하여 객체 영상에 대 한 제1 출력 영상을 생성할 수 있다. 도 11에는 도시되지 않았으나, 하나 이상의 영역들 중 적어도 일부에 대해 에지 기반 필터링 또는 평활화 필터 링을 적용하여 객체 영상에 대한 제2 출력 영상을 생성할 수 있다. 또한 예컨대, 생성된 제1 출력 영상 및 제2 출력 영상에 기초하여 객체 영상에 대한 제3 출력 영상을 생성할 수 있다 도 5 내지 도 11을 참조하여 설명한 실시예에서는 단일 객체를 포함하는 영상을 입력 받아, 객체와 배경을 분리 하는 예를 설명하였다. 그러나, 반드시 이에 한정되지 않으며, 입력 영상이 둘 이상의 객체를 포함하는 영상일 수도 있다. 이 경우, 입력 영상으로부터 둘 이상의 객체와 배경을 구분하고, 둘 이상의 객체의 각각에 대해 위 치 정보를 생성하여 이용할 수도 있다. 또한, 이 경우, 도 6을 참조한 설명에서, 복수의 픽셀 그룹이 형성된 경 우, 가장 큰 형태로 형성된 픽셀 그룹뿐만 아니라 다른 픽셀 그룹에 대해서도 각각 객체에 대한 영역인 것으로 결정할 수 있다. 각각의 결정된 객체의 위치 정보를 생성하는 과정은 하나의 객체를 포함하는 영상에 대해 설명 한 바와 동일하다. 본 개시의 영상 강화 장치의 구성부들 및 영상 강화 방법의 단계들 중 적어도 일부는 인공지능 기반 또는 딥러 닝 기반의 모델을 이용하여 수행될 수 있다. 예컨대, 객체 영상을 분할하여 생성되는 영역의 크기, 개수, 색상 분포 정보에 기초하여 결정되는 가중치, 본 개시에서 언급된 각종 임계치, 제2 출력 영상의 생성 여부 등은 인 공지능 기반 또는 딥러닝 기반의 모델을 이용하여 학습될 수 있고, 학습된 모델에 따른 정보가 이용될 수 있다. 이하, 도 12 내지 도 17을 참고하여, 영상 분석 장치가 수행하는 맥락 분석 방법의 일 실시예에 대해 설명 한다. 도 12의 영상 분석 장치는 도 1의 영상 분석 장치의 일 실시예일 수 있다. 또는 도 12의 영상 분석 장치는, 도 1 의 영상 분석 장치에 포함되거나, 별개로 구성되어 맥락 분석을 수행하는 장치일 수 있다. 도 12를 참조하면, 영상 분석 장치는 특징 추출부, 맥락 생성부 및/또는 특징 및 맥락 분석 부를 포함할 수 있다. 다만, 이는 본 실시예를 설명하기 위해 필요한 일부 구성요소만을 도시한 것일 뿐, 영상 분석 장치에 포함된 구성요소가 전술한 예에 한정되는 것은 아니다. 영상 분석 장치는 입력 영상(분석 대상 영상)의 특징을 추출하고, 추출된 특징에 기초하여 맥락 정보를 생성하고, 추출된 특징 및 생성된 맥락 정보에 기초하여 분석 대상 영상을 분석할 수 있다. 예컨대, 영상 분석 장치는 추출된 특징 및 생성된 맥락 정보를 이용하여 영상을 분류하거나 관심 객체의 위치를 찾아낼 수 있다. 영상 분석 장치의 입력 영상은 도 1의 영상 분석 장치의 입력 영상과 동일할 수 있다. 특징 추출부는 입력 영상을 분석하여 영상의 특징을 추출할 수 있다. 예컨대, 상기 특징은 영상의 각 영 역마다의 국소적인 특징일 수 있다. 일 실시예에 따른 특징 추출부는 일반적인 합성곱 신경망 (Convolutional Neural Network, CNN) 기법 또는 풀링(pooling) 기법을 이용하여 입력 영상의 특징을 추출할수 있다. 상기 풀링 기법은 맥스(max) 풀링 기법 및 평균(average) 풀링 기법 중 적어도 하나를 포함할 수 있다. 그러나, 본 개시에서 언급되는 풀링 기법은 맥스 풀링 기법 또는 평균 풀링 기법에 한정되지 않으며, 소 정 크기의 영상 영역의 대표값을 획득하는 임의의 기법을 포함한다. 예컨대, 풀링 기법에 사용되는 대표값은 최 대값 및 평균값 외에, 분산값, 표준 편차값, 중간값(mean value), 최빈값(most frequent value), 최소값, 가중 평균값 등 중 적어도 하나일 수 있다. 본 개시의 합성곱 신경망은 입력 데이터(영상)로부터 테두리, 선 색 등과 같은 \"특징들(features)\"을 추출하기 위해 이용될 수 있으며, 복수의 계층들(layers)을 포함할 수 있다. 각각의 계층은 입력 데이터를 수신하고, 해 당 계층의 입력 데이터를 처리하여 출력 데이터를 생성할 수 있다. 합성곱 신경망은 입력된 영상 또는 입력된 특징맵(feature map)을 필터 커널들(filter kernels)과 컨볼루션하여 생성한 특징맵을 출력 데이터로서 출력할 수 있다. 합성곱 신경망의 초기 계층들은 입력으로부터 에지들 또는 그레디언트들과 같은 낮은 레벨의 특징들을 추출하도록 동작될 수 있다. 신경망의 다음 계층들은 눈, 코 등과 같은 점진적으로 더 복잡한 특징들을 추출할 수 있다. 합성곱 신경망의 구체적인 동작에 대해서는 도 16을 참고하여 후술한다. 합성곱 신경망은 컨볼루션 연산이 수행되는 합성곱 계층 외에도 풀링 연산이 수행되는 풀링 계층도 포함할 수 있다. 풀링 기법은 풀링 계층에서 데이터의 공간적 크기를 축소하는데 사용되는 기법이다. 구체적으로, 풀링 기 법에는 해당 영역에서 최대값을 선택하는 맥스 풀링(max pooling) 기법과 해당 영역의 평균값을 선택하는 평균 풀링(average pooling) 기법이 있으며, 이미지 인식 분야에서는 일반적으로 맥스 풀링 기법이 사용된다. 풀링 기법에서는 일반적으로 풀링의 윈도우 크기와 간격(스트라이드, stride)을 같은 값으로 설정한다. 여기서, 스트 라이드란 입력 데이터에 필터를 적용할 때 이동할 간격을 조절하는 것, 즉 필터가 이동할 간격을 의미하며, 스 트라이드 또한 출력 데이터의 크기를 조절하기 위해 사용될 수 있다. 풀링 기법의 구체적인 동작에 대해서는 도 17을 참고하여 후술한다. 본 개시의 일 실시예에 따른 특징 추출부는 분석 대상 영상의 특징을 추출하기 위한 전처리(pre- processing)로서, 분석 대상 영상에 필터링을 적용할 수 있다. 상기 필터링은 고속 푸리에 변환(Fast Fourier Transform, FFT), 히스토그램 평활화(histogram equalization), 모션 아티팩트(motion artifact) 제거 또는 노 이즈(noise) 제거 등일 수 있다. 그러나, 본 개시의 필터링은 상기 열거한 방법으로 제한되지 않으며, 영상의 품질을 개선할 수 있는 모든 형태의 필터링을 포함할 수 있다. 또는 전처리로서 도 5 내지 도 11을 참조하여 설 명한 영상의 강화가 수행될 수도 있다. 맥락 생성부는 특징 추출부로부터 추출된 입력 영상의 특징을 이용하여 입력 영상(분석 대상 영 상)의 맥락 정보를 생성할 수 있다. 예컨대, 상기 맥락 정보는 분석 대상 영상의 전체 또는 일부 영역을 나타내 는 대표값일 수 있다. 또한 상기 맥락 정보는 입력 영상의 전역적인 맥락 정보일 수 있다. 일 실시예에 따른 맥 락 생성부는 합성곱 신경망 기법 또는 풀링 기법을 특징 추출부로부터 추출된 특징에 적용하여 맥 락 정보를 생성할 수 있다. 상기 풀링 기법은 예컨대, 평균 풀링(average pooling) 기법일 수 있다. 특징 및 맥락 분석부는 특징 추출부에서 추출된 특징 및 맥락 생성부에서 생성된 맥락 정보 에 기초하여 영상을 분석할 수 있다. 일 실시예에 따른 특징 및 맥락 분석부는 특징 추출부에서 추 출된 영상의 각 영역마다의 국소적인 특징 및 맥락 생성부에서 재구성된 전역적인 맥락을 결합 (concatenate)하는 등의 방식으로 함께 사용하여, 입력 영상을 분류하거나 입력 영상에 포함된 관심 객체의 위 치 등을 찾는데 이용할 수 있다. 입력 영상 내 특정 2차원 위치에서의 정보는 국소적인 특징 정보뿐만 아니라 전역적인 맥락 정보까지 포함하게 되므로, 특징 및 맥락 분석부는 이들 정보를 이용함으로써, 실제 내용 은 상이하지만 국소적인 특징 정보가 유사한 입력 영상들에 대해 보다 정확한 인식 또는 분류 등이 가능하게 된 다. 전술한 바와 같이, 본 개시의 일 실시예에 따른 발명은, 일반적인 합성곱 신경망 기법이 사용하는 국소적인 특 징뿐만 아니라 전역적인 맥락 정보를 함께 사용함으로써, 보다 더 정확하고 효율적인 학습 및 영상 분석이 가능 하게 된다. 이러한 관점에서 본 개시에 따른 발명이 적용된 신경망을 '맥락 분석을 통한 심층 신경망'이라 할 수 있다. 도 13은 본 개시의 일 실시예에 따른 영상의 맥락 정보를 생성하고 분석하는 과정을 나타내는 도면이다. 도 13의 특징 추출부, 맥락 생성부, 및 특징 및 맥락 분석부는 각각 도 12의 특징 추출부 , 맥락 생성부, 및 특징 및 맥락 분석부의 일 실시예일 수 있다. 도 13을 참조하면, 특징 추출부는 입력 영상을 이용하여 입력 영상으로부터 특징을 추출하고, 추출된 특징 정보를 포함하는 특징 영상을 생성할 수 있다. 상기 추출된 특징은 입력 영상의 국소 영역에 대한 특징일 수 있다. 상기 입력 영상은 영상 분석 장치의 입력 영상 또는 합성곱 신경망 모 델 내의 각 계층에서의 특징맵을 포함할 수 있다. 또한 상기 특징 영상은 입력 영상에 대해 합성곱 신경망 기법 및/또는 풀링 기법을 적용하여 획득된 특징맵 및/또는 특징 벡터를 포함할 수 있다. 맥락 생성부는 특징 추출부에서 추출된 특징 영상에 대해 합성곱 신경망 기법 및/또는 풀링 기법을 적용하여 맥락 정보를 생성할 수 있다. 예컨대, 맥락 생성부는 풀링의 간격(stride)을 다양하게 조절함으로써 영상 전체, 4등분 영역, 9등분 영역 등의 다양한 크기(scale)의 맥락 정보를 생성할 수 있다. 도 13을 참조하면, 영상 전체 크기의 영상에 대한 맥락 정보를 포함하는 전체 맥락 정보 영상, 영상 전체를 4등분한 크기의 4등분 영상에 대한 맥락 정보를 포함하는 4등분 맥락 정보 영상 및 영상 전체를 9등분한 크기의 9등분 영상에 대한 맥락 정보를 포함하는 9등분 맥락 정보 영상이 획득될 수 있다. 특징 및 맥락 분석부는 상기 특징 영상과 상기 맥락 정보 영상(1322, 1324, 1326)을 모두 이용하여 분석 대상 영상의 특정 영역에 대한 분석을 보다 정확히 수행할 수 있다. 예컨대, 자동차(car)와 유사한 형태를 갖는 보트(boat)가 포함된 영상이 입력 영상인 경우, 특징 추출부 가 추출한 국소적인 특징을 포함하는 특징 영상으로부터는 상기 식별된 객체가 자동차인지 보트인지 정확 히 판단할 수 없다. 즉, 특징 추출부는 국소적인 특징에 기초하여 객체의 형상을 인식할 수 있으나, 해당 객체의 형상만 가지고는 정확히 객체를 식별하고 분류할 수 없는 경우가 있다. 본 개시의 일 실시예에 따른 맥락 생성부는 상기 분석 대상 영상 또는 상기 특징 영상에 기초하여 맥락 정보(1322, 1324, 1326)를 생성함으로써, 보다 정확히 객체를 식별하고 분류할 수 있다. 예컨대, 전체 영 상에 대해 추출된 특징이 \"자연 경관\"으로 인식 또는 분류되고, 4등분 영상에 대해 추출된 특징이 \"호수\"로 인 식 또는 분류되고, 9등분 영상에 대해 추출된 특징이 \"물\"로 인식 또는 분류되는 경우, 상기 추출된 특징인 \"자 연 경관\", \"호수\", \"물\"을 맥락 정보로서 생성하고 활용할 수 있다. 본 개시의 일 실시예에 따른 특징 및 맥락 분석부는 상기 맥락 정보를 활용함으로써, 상기 보트 또는 자 동차의 형상을 갖는 객체를 \"보트\"로 식별할 수 있다. 도 13을 참조하여 설명한 실시예에서는 전체 영상에 대한 맥락 정보, 4등분 영상에 대한 맥락 정보, 9등분 영상 에 대한 맥락 정보를 생성하고 활용하는 것에 대해 설명하였으나, 맥락 정보를 추출하는 영상의 크기는 이에 한 정되지 않는다. 예컨대, 전술한 크기의 영상 이외의 크기를 갖는 영상에 대한 맥락 정보를 생성하고 활용할 수 도 있다. 본 개시의 일 실시예에 따른 합성곱 신경망 기법 및 풀링에 대해서는 도 16 및 도 17을 참조하여 후술한다. 도 14는 본 개시의 일 실시예에 따른 영상 분석 장치가 영상을 분석하여 객체를 식별하는 과정을 설명하기 위한 도면이다. 예컨대, 영상 분석 장치는 영상을 입력 받고, 다양한 크기의 영상 영역에 대한 정보를 생성함으로 써, 영상에 포함된 객체를 정확히 식별 및/또는 분류할 수 있다. 입력 영상은 예컨대, 가방을 포함 하는 X-ray 영상일 수 있다. 영상 분석 장치는 전술한 바에 따라 입력 영상을 분석하여, 영상 전체 에 대한 특징, 영상의 일부 영역에 대한 특징을 추출하고 이를 이용하여 영상에 포함된 객체를 정확히 식 별할 수 있다. 상기 영상 전체에 대한 특징은 예컨대, 가방의 형상에 대한 특징일 수 있다. 상기 영상의 일부 영역에 대한 특징은 예컨대, 손잡이에 대한 특징, 지퍼에 대한 특징(1426, 고리에 대한 특징 등을 포함할 수 있다. 영상 분석 장치는 상기 생성된 특징들(1422, 1424, 1426, 1428)을 맥락 정보로서 활용함으로써, 상기 영 상에 포함된 객체가 \"가방\"이라는 것을 정확히 식별할 수 있다. 만약 상기 생성된 특징들 중 일부가 \"가방\"과 관련이 없는 특징이라면, 영상 분석 장치는 상기 영상 에 포함된 객체가 \"가방\"이라고 식별할 수 없거나 또는 상기 영상에 포함된 객체를 \"가방\"으로 식 별할 수 없다는 분석 결과를 제공할 수 있다. 또는, 맥락 정보 중 일부가 다른 맥락 정보와 관련이 없는 경우, 해당 객체의 이상을 출력할 수 있다. 예컨대, \"가방\"에 대한 통상의 특징과는 관련이 없는 비정형의 공간, 일정 한 두께 이상의 공간 등이 검출되는 경우, 해당 \"가방\"은 이상이 있는 가방이라는 신호를 출력할 수 있다. 상기와 같이, 통상의 맥락 정보와는 관련이 없는 맥락 정보가 포함되는 경우, 그러한 사실은 판독원에게 출력될 수 있으며, 판독원은 이에 기초하여 해당 영상의 물품 또는 객체에 대한 정밀 검사 또는 개장 검사를 실시할 수있다. 도 15는 본 개시의 일 실시예에 따른 영상 분석 장치의 동작을 설명하기 위한 도면이다. S1500 단계에서 영상 분석 장치는 분석 대상 영상의 특징을 추출할 수 있다. 일 실시예에 따른 영상 분석 장치는 일반적인 합성곱 신경망 기법 또는 풀링 기법을 이용하여 입력 영상의 특징 을 추출할 수 있다. 상기 분석 대상 영상의 특징은 영상의 각 영역마다의 국소적인 특징일 수 있으며, 또한 상 기 풀링 기법은 맥스 풀링 기법 및 평균 풀링 기법 중 적어도 하나를 포함할 수 있다. S1510 단계에서 영상 분석 장치는 S1500 단계에서 추출된 특징에 기초하여 맥락 정보를 생성할 수 있다. 일 실시예에 따른 영상 분석 장치는 합성곱 신경망 기법 및/또는 풀링 기법을 S1500 단계에서 추출된 특징에 적 용하여 맥락 정보를 생성할 수 있다. 상기 맥락 정보는 분석 대상 영상의 전체 또는 일부 영역을 나타내는 대표 값일 수 있다. 또한 상기 맥락 정보는 입력 영상의 전역적인 맥락 정보일 수 있다. 또한, 상기 풀링 기법은 예 컨대, 평균 풀링 기법일 수 있다. S1520 단계에서 영상 분석 장치는 S1500 단계에서 추출된 특징 및 S1510 단계에서 생성된 맥락 정보에 기초하여 상기 분석 대상 영상을 분석할 수 있다. 예컨대, 영상 분석 장치는 S1500 단계에서 추출된 영상의 각 영역마다의 국소적인 특징 및 S1510 단계에서 재구 성된 전역적인 맥락을 결합하여 입력 영상을 분류하거나 입력 영상에 포함된 관심 객체의 위치 등을 찾을 수 있 다. 따라서, 입력 영상에서 특정 2차원 위치에서의 정보가 국소적인 정보부터 전역적인 맥락까지 포함됨으로써, 실제 내용은 상이하지만 국소적인 정보가 유사한 입력 영상들에 대해 보다 정확한 인식 또는 분류 등이 가능하 다. 또는 다른 맥락 정보와 관련이 없는 맥락 정보를 포함하는 객체에 대한 검출이 가능하다. 도 16은 다채널 특징맵을 생성하는 합성곱 신경망의 일 실시예를 설명하기 위한 도면이다. 합성곱 신경망 기반의 영상 처리는 다양한 분야에 활용될 수 있다. 예컨대, 영상의 객체 인식(object recognition)을 위한 영상 처리 장치, 영상 복원(image reconstruction)을 위한 영상 처리 장치, 시맨틱 세그 먼테이션(semantic segmentation)을 위한 영상 처리 장치, 장면 인식(scene recognition)을 위한 영상 처리 장 치 등에 이용될 수 있다. 입력 영상은 합성곱 신경망을 통해 처리됨으로써 특징맵 영상을 출력할 수 있다. 출력된 특징맵 영 상은 전술한 다양한 분야에 활용될 수 있다. 합성곱 신경망은 복수의 계층들(1620, 1630, 1640)을 통해 처리될 수 있으며, 각 계층은 다채널 특징맵 영상들(1625, 1635)을 출력할 수 있다. 일 실시예에 따른 복수의 계층들(1620, 1630, 1640)은 입력 받은 데이터 의 좌측 상단으로부터 우측 하단까지 일정한 크기의 필터를 적용하여 영상의 특징을 추출할 수 있다. 예를 들어, 복수의 계층들(1620, 1630, 1640)은 입력 데이터의 좌측 상단 NxM 픽셀에 가중치를 곱해서 특징맵의 좌측 상단의 한 뉴런에 매핑시킨다. 이 경우, 곱해지는 가중치도 NxM가 될 것이다. 상기 NxM은 예컨대, 3x3일 수 있 으나, 이에 한정되지 않는다. 이후, 동일한 과정으로, 복수의 계층들(1620, 1630, 1640)은 입력 데이터를 좌측 에서 우측으로, 그리고 상단에서 하단으로 k 칸씩 스캔하면서 가중치를 곱하여 특징맵의 뉴런에 매핑한다. 상기 k 칸은 합성곱 수행시 필터를 이동시킬 간격(stride)을 의미하며, 출력 데이터의 크기를 조절하기 위해 적절히 설정될 수 있다. 예컨대, k는 1일 수 있다. 상기 NxM 가중치는 필터 또는 필터 커널이라고 한다. 즉, 복수의 계 층들(1620, 1630, 1640)에서 필터를 적용하는 과정은 필터 커널과의 컨볼루션 연산을 수행하는 과정이며, 그 결 과 추출된 결과물을 \"특징맵(feature map)\" 또는 \"특징맵 영상\"이라고 한다. 또한, 컨볼루션 연산이 수행된 계 층을 합성곱 계층이라 할 수 있다. “다채널 특징맵(multiple-channel feature map)\"의 용어는 복수의 채널에 대응하는 특징맵들의 세트를 의미하 고, 예를 들어 복수의 영상 데이터일 수 있다. 다채널 특징맵들은 합성곱 신경망의 임의의 계층에서의 입력일 수 있고, 컨볼루션 연산 등의 특징맵 연산 결과에 따른 출력일 수 있다. 일 실시예에 따르면, 다채널 특징맵들 (1625, 1635)은 합성곱 신경망의 \"특징 추출 계층들\" 또는 \"컨볼루션 계층들\"이라고도 불리는 복수의 계층들 (1620, 1630, 1640)에 의해 생성된다. 각각의 계층은 순차적으로 이전 계층에서 생성된 다채널 특징맵들을 수신 하고, 출력으로서 그 다음의 다채널 특징맵들을 생성할 수 있다. 최종적으로 L(L은 정수)번째 계층에서는 L-1번째 계층(미도시)에서 생성한 다채널 특징맵들을 수신하여 미도시의 다채널 특징맵들을 생성할 수 있다. 도 16을 참조하면, 채널 K1개를 가지는 특징맵들은 입력 영상에 대해 계층 1에서의 특징맵 연산 에 따른 출력이고, 또한 계층 2에서의 특징맵 연산을 위한 입력이 된다. 또한, 채널 K2개를 가지는 특징맵들은 입력 특징맵들에 대해 계층 2에서의 특징맵 연산에 따른 출력이고, 또한 계층 3 에서의 특징맵 연산(미도시)을 위한 입력이 된다. 도 16을 참조하면, 첫 번째 계층에서 생성된 다채널 특징맵들은 K1(K1은 정수)개의 채널에 대응하 는 특징맵들을 포함한다. 또한, 두 번째 계층에서 생성된 다채널 특징맵들은 K2(K2은 정수)개의 채 널에 대응하는 특징맵들을 포함한다. 여기서, 채널의 개수를 나타내는 K1 및 K2는, 첫 번째 계층 및 두 번째 계층에서 각각 사용된 필터 커널의 개수와 대응될 수 있다. 즉, M(M은 1 이상 L-1 이하의 정수)번째 계층에서 생성된 다채널 특징맵들의 개수는 M번째 계층에서 사용된 필터 커널의 개수와 동일할 수 있다. 도 17은 풀링 기법의 일 실시예를 설명하기 위한 도면이다. 도 17에 도시된 바와 같이, 풀링의 윈도우 사이즈는 2x2, 스트라이드는 2이며, 맥스 풀링을 입력 영상에 적용하여 출력 영상을 생성할 수 있다. 도 17의 (a)에서, 입력 영상의 좌측 상단에 2x2 윈도우를 적용하고, 윈도우 영역 내의 값들 중 대표값(여기서는, 최대값 4)을 계산하여 출력 영상의 대응 위치에 입력한다. 이후, 도 17의 (b)에서, 스트라이드만큼, 즉, 2만큼 윈도우를 이동하고, 윈도우 영역 내의 값들 중 최대 값 3을 출력 영상의 대응 위치에 입력한다. 더 이상 우측으로 윈도우를 이동시킬 없는 경우, 다시 입력 영상의 좌측에서 스트라이드만큼 아래의 위치부터 상기 과정을 반복한다. 즉, 도 17의 (c)에 도시된 바와 같이, 윈도우 영역 내의 값들 중 최대값 5를 출력 영상의 대응 위치에 입력한다. 이후, 도 17의 (d)에 도시된 바와 같이, 스트라이드만큼 윈도우를 이동하고, 윈도우 영역 내의 값들 중 최대값 2를 출력 영상의 대응 위치에 입력한다. 상기 과정은 입력 영상의 우측 하단 영역에 윈도우가 위치할 때까지 반복적으로 수행됨으로써, 입력 영상 에 풀링을 적용한 출력 영상을 생성할 수 있다. 이하, 도 18 내지 도 22를 참고하여, 복수의 영상 및/또는 물품 정보를 활용하여 새로운 합성 영상 및/또는 이 에 대응하는 가상의 물품 정보를 생성하는 방법의 일 실시예에 대해 설명한다. 도 18은 본 개시의 일 실시예에 따른 영상 합성 장치의 구성을 나타내는 블록도이다. 도 18을 참조하면, 영상 합성 장치는 객체 영상 추출부, 객체 위치 정보 생성부, 영상 합성 부 및/또는 객체 검출 딥러닝 모델 학습부를 포함할 수 있다. 다만, 이는 본 실시예를 설명하기 위 해 필요한 일부 구성요소만을 도시한 것일 뿐, 영상 합성 장치에 포함된 구성요소가 전술한 예에 한정되 는 것은 아니다. 예컨대, 둘 이상의 구성부가 하나의 구성부 내에서 구현될 수도 있고, 하나의 구성부에서 실행 되는 동작이 분할되어 둘 이상의 구성부에서 실행되도록 구현될 수도 있다. 또한, 일부 구성부가 생략되거나 부 가적인 구성부가 추가될 수도 있다. 또는, 도 1의 영상 분석 장치, 도 5의 영상 강화 장치, 도 12의 영상 분석 장치 및 도 18의 영상 합성 장치의 구성요소 중, 동일한 기능 또는 유사한 기능을 수행 하는 구성부는 하나의 구성요소로서 구현될 수도 있다. 본 개시의 일 실시예에 따른 영상 합성 장치는 제1 객체를 포함하는 제1 영상 및 제2 객체를 포함하는 제 2 영상을 입력 받아, 제1 영상 및 제2 영상의 각각에 대해 객체와 배경을 구분하고, 구분된 제1 객체 및 제2 객 체의 위치 정보를 생성하고, 제1 객체의 위치 정보 및 제2 객체의 위치 정보에 기초하여, 제1 객체 및 제2 객체 를 포함하는 제3 영상을 생성하고, 제1 객체의 위치 정보, 제2 객체의 위치 정보 및 제3 영상을 이용하여 객체 검출 딥러닝 모델을 학습할 수 있다. 도 18을 참조하면, 입력 영상은 단일 객체를 포함하는 영상을 포함할 수 있다. 입력 영상에 대한 설명은 도 1 등을 참조하여 설명한 입력 영상에 대한 설명과 동일하다. 객체 영상 추출부는 단일 객체를 포함하는 영상을 수신하고 수신된 영상을 객체와 배경으로 구분할 수 있다. 객체 영상 추출부에 대한 설명은 도 5 및 도 6을 참조하여 설명한 객체 영상 추출부에 대 한 설명과 동일하다. 객체 위치 정보 생성부는 객체 영상 추출부로부터 추출된 객체의 위치를 결정할 수 있다. 예컨대, 객체 위치 정보 생성부는 객체 영역을 둘러싸는 사각형 박스(bounding box)를 특정하고, 특정된 사각형 박스에 기초하여, 객체 영상 추출부에서 구분된 객체의 위치 정보를 생성할 수 있다. 객체의 위치 정보를 생성하는 방법에 대한 설명은 도 6을 참조한 방법에 대한 설명과 동일하다. 본 개시의 일 실시예에 따르면, 영상에 포함된 객체의 위치 정보가 자동으로 생성될 수 있으므로, 인공지능 학 습을 위해 판독원이 각각의 영상마다 객체의 위치 정보를 직접 입력해야 하는 번거로움을 피할 수 있다. 다시 도 18을 참조하여, 영상 합성부는 객체 영상 추출부 및 객체 위치 정보 생성부를 거쳐 객체의 위치 정보가 획득된 복수의 단일 객체 영상을 이용하여 다중 객체 영상을 생성할 수 있다. 예컨대, 제1 객체를 포함하는 제1 영상 및 제2 객체를 포함하는 제2 영상에 대해, 각각 객체 영상 추출부 및 객체 위 치 정보 생성부를 거쳐 제1 객체의 위치 정보 및 제2 객체의 위치 정보가 획득되고, 영상 합성부는 획득된 제1 객체의 위치 정보 및 제2 객체의 위치 정보에 기초하여 제1 객체 및 제2 객체를 포함하는 제3 영상 을 생성할 수 있다. 다중 객체 영상을 생성하는 구체적인 과정에 대해 도 19를 참고하여 보다 상세히 설명한다. 도 19는 본 개시의 일 실시예에 따른 단일 객체를 포함하는 두 개의 영상을 이용하여 다중 객체 영상을 생성하 는 과정을 나타내는 도면이다. 도 19의 영상 합성부는 도 18의 영상 합성부의 일 실시예다. 도 19 를 참조하면, 영상 합성부는 객체 영상 추출부 및 객체 위치 정보 생성부를 통해 획득된 제1 단일 객체 영상, 제2 단일 객체 영상 및 제1 단일 객체 영상과 제2 단일 객체 영상의 위치 정보 를 이용하여, 제1 단일 객체 영상과 제2 단일 객체 영상이 합성된 다중 객체 영상 및 다중 객체 영 상에 포함된 객체들에 대한 위치 정보를 획득할 수 있다. 한편, 영상 합성부는 제1 단일 객 체 영상과 제2 단일 객체 영상의 합성 시 객체로부터 구분된 배경에 대한 영상도 함께 이용 할 수도 있다. 복수의 영상 합성시, 제1 단일 객체 영상의 위치 정보와 제2 단일 객체 영상의 위치 정보는 임의로 수정될 수 있다. 또한, 수정된 위치 정보에 기초하여 영상의 합성이 수행될 수 있다. 그럼으로써, 무수히 많은 양의 합성 영상과 가상의 위치 정보를 생성해 낼 수 있다. 전술한 바와 같이, 합성을 통해 학습에 필요한 만큼의 합성 영상 및/또는 그에 대응하는 가상의 위치 정보를 생 성해 낼 수 있다. 따라서, 학습에 필요한 영상 및/또는 위치 정보의 절대적인 수가 작은 경우에도 인공지능 모 델을 학습시키기에 충분한 수의 학습 데이터를 얼마든지 생성해 낼 수 있다. 다시 도 18을 참조하여, 객체 검출 딥러닝 모델 학습부는 제1 객체의 위치 정보, 제2 객체의 위치 정보 및 제3 영상을 이용하여 객체 검출 딥러닝 모델을 학습시킬 수 있다. 예컨대, 객체 검출 딥러닝 모델 학습부 는 합성곱 신경망 모델을 학습시킬 수 있다. 합성곱 신경망 모델의 학습을 위해 제1 객체의 위치 정보, 제2 객체의 위치 정보 및 제3 영상이 이용될 수 있다. 도 20은 본 개시의 일 실시예에 따른 다중 객체 영상을 이용하여 합성곱 신경망을 학습시키는 과정을 나타내는 도면이다. 도 20의 객체 검출 딥러닝 모델 학습부는 도 18의 객체 검출 딥러닝 모델 학습부의 일 실시예다. 도 20을 참조하면, 학습에 필요한 데이터로서 단일 객체 영상들과 객체들의 위치 정보를 이용하여 합 성된 다중 객체 영상을 이용할 수 있다. 객체 검출 딥러닝 모델 학습부는 다중 객체 영상에 대해 단일 객체 각각의 위치 정보를 함께 사영시킴으로써 합성곱 신경망을 학습시킬 수 있다. 일 실시예 에 따를 때, 전자 검색 시스템에서 X-Ray 검색기를 통과하는 물체 내에 복수의 객체가 존재하면, 복수의 객체들 이 겹쳐진 X-Ray 영상이 획득될 수 있는데, 본 개시에 따르면, 영상 내의 복수의 객체의 위치 정보와 함께 각각 의 객체의 형상을 이용하여 합성곱 신경망을 학습시키기 때문에, 객체 간 겹침이 발생하여도 보다 정확한 검출 결과가 획득될 수 있다. 도 21은 본 개시의 일 실시예에 따른 영상 합성 장치를 이용하여 실제 영상을 분석하는 과정을 설명하기 위한 도면이다. 도 21의 영상 합성 장치는 도 18의 영상 합성 장치의 일 실시예다. 도 21의 영상 합성 장치 가 포함하는 객체 영상 추출부, 객체 위치 정보 생성부, 영상 합성부 및 객체 검출 딥러닝 모델 학습부의 동작은 도 18의 영상 합성 장치에 포함된 객체 영상 추출부, 객체 위치 정보 생성부, 영상 합성부 및 객체 검출 딥러닝 모델 학습부의 동작과 동일하다. 따라서, 영상 합 성 장치는 복수의 단일 객체 영상에 대해 객체 영상 추출부, 객체 위치 정보 생성부, 영상 합성부 및 객체 검출 딥러닝 모델 학습부에서의 동작을 수행함으로써 학습된 합성곱 신경망 모델을 생성할 수 있다. 객체 검출 장치는 실제 환경의 다중 객체를 포함하는 영상에 대해 영상 처 리 장치에서 학습된 합성곱 신경망 모델을 이용하여 각각의 객체를 검출할 수 있다. 일 실시예에 따를 때, 물품 검색 시스템에 본 개시의 발명이 적용되는 경우, 본 개시의 영상 합성 장치는 X-Ray 영상 내 단일 객체 영역 추출을 기반으로 새롭게 다중 객체 포함 영상을 생성할 수 있다. 또한 객체 검출 장치는 X-Ray 검색기를 통과하는 물품 내 포함된 다중 객체가 존재하는 영역을 찾을 수 있다. 따라서, X- Ray 영상에 대해 객체의 위치를 자동적으로 추출함으로써, 판독원이 보다 수월하게 영상 검사 작업을 수행할 수 있도록 할 수 있고, 또한 추출된 객체와 물체 내 객체의 수량 정보 등을 포함하는 전산 정보를 비교하는 업무 등에 이용될 수 있다. 도 22는 본 개시의 일 실시예에 따른 영상 합성 방법을 설명하기 위한 도면이다. S2200 단계에서, 제1 객체를 포함하는 제1 영상 및 제2 객체를 포함하는 제2 영상을 입력 받아, 제1 영상 및 제 2 영상의 각각에 대해 객체와 배경을 구분할 수 있다. 예컨대, 입력 영상의 픽셀값과 소정의 임계값을 비교하여 픽셀값을 이진화하고, 이진화된 픽셀값을 그룹핑함으로써 입력 영상에 포함된 객체를 구분할 수 있다. S2210 단계에서, 구분된 제1 객체 및 제2 객체의 위치 정보를 생성할 수 있다. 예컨대, 객체 영역을 둘러싸는 사각형 박스를 특정하고, 특정된 사각형 박스에 기초하여, S2200 단계에서 구분된 객체의 위치 정보를 생성할 수 있다. S2220 단계에서, 제1 객체의 위치 정보 및 제2 객체의 위치 정보에 기초하여, 제1 객체 및 제2 객체를 포함하는 제3 영상을 생성할 수 있다. 예컨대, S2210 단계에서 획득된 제1 객체의 위치 정보 및 제2 객체의 위치 정보에 기초하여 제1 객체 및 제2 객체를 포함하는 제3 영상을 생성할 수 있다. S2230 단계에서, 제1 객체의 위치 정보, 제2 객체의 위치 정보 및 제3 영상을 이용하여 객체 검출 딥러닝 모델 을 학습할 수 있다. 예컨대, 합성곱 신경망 모델을 학습시킬 수 있으며, 합성곱 신경망 모델의 학습을 위해 S2210 단계에서 생성된 제1 객체의 위치 정보와 제2 객체의 위치 정보 및 S2220 단계에서 생성된 제3 영상이 이 용될 수 있다. 도 18 내지 도 22를 참조하여 설명한 실시예에서는 단일 객체를 포함하는 영상을 입력 받아, 객체와 배경을 분 리하는 예를 설명하였다. 그러나, 반드시 이에 한정되지 않으며, 입력 영상이 둘 이상의 객체를 포함하는 영상 일 수도 있다. 이 경우, 입력 영상으로부터 둘 이상의 객체와 배경을 구분하고, 둘 이상의 객체의 각각에 대해 위치 정보를 생성하여 이용할 수도 있다. 또한, 상기 설명한 실시예에서는 2개의 단일 객체 영상 및 각 객체의 위치 정보에 기초하여 제3 영상을 생성하 는 것으로 설명하였다. 그러나, 반드시 이에 한정되지 않으며, 둘 이상의 단일 객체 영상 및 각 객체의 위치 정 보를 이용하여 제3 영상을 생성할 수도 있다. 즉, 본 개시에 따른 영상 처리 방법 및 장치는 각각이 하나 이상 의 객체를 포함하는 둘 이상의 영상 및 각 객체의 위치 정보에 기초하여 제3 영상을 생성할 수 있다. 상기의 설명에서는 2차원 영상을 대상으로 하는 영상 분석 방법, 객체 검출 방법, 영상 강화 방법 등을 설명하 였다. 그러나, 2차원 영상을 대상으로 하는 본 발명에 따른 다양한 실시예는 3차원 영상을 대상으로 할 수도 있 다. 기존의 2차원 영상(예컨대, 2D X-Ray 영상)과 3차원 영상(예컨대, 3차원 CT 영상)은 아래와 같은 차이를 갖는다. 첫째, 3차원 영상은 2차원 영상에 비해, 차원(dimension)이 증가함에 따라 데이터가 방대해지고 이에 따른 연산 량이 매우 크게 증가한다. 둘째, 2차원 영상의 경우, 예컨대, 촬영하는 방향으로 객체들이 중접(superposion)되는 반면, 3차원 영상의 경 우 중첩이 발생하지 않는다. 본 발명은 상기 차이점에 착안하여, 3차원 영상에 포함된 객체를 효율적으로 검출하기 위한 다양한 실시예를 제 공한다. 3차원 영상을 대상으로 하는 본 발명의 다양한 실시예는 도 23 내지 도 25를 참조하여 구체적으로 설명된다. 예컨대, 3차원 영상이란 3차원 보안 CT 영상을 의미할 수 있다. 그러나, 이에 한정되지 않으며, 3차원의 형태로 표현되는 일체의 영상을 포함할 수 있다. 예컨대, 3차원의 좌표 (x, y, z)를 갖고, 휘도 정보 및/또는 색차 정 보를 갖는 하나 이상의 복셀(voxel)들로 구성된 된 영상이 3차원 영상일 수 있다. 복셀이란 3차원 영상을 구성 하는 화소를 의미할 수 있으며, 2차원 영상에서의 픽셀에 대응될 수 있다. 복셀과 픽셀은 휘도 정보 및/또는 색 차 정보를 포함할 수 있음은 동일하다. 복셀은 3차원 상의 위치를 규정하기 위해 추가적으로 z 좌표를 가질 수있다. 도 23은 3차원 영상에 포함된 객체를 검출하는 본 발명의 실시예를 설명하기 위한 도면이다. 단계 S2300에서 3차원 영상을 획득할 수 있다. 단계 S2310에서 영상에 대한 전처리를 수행할 수 있다. 영상에 대한 전처리는 2차원 영상에 대한 전처리 과정을 포함할 수 있다. 2차원 영상에 대한 전처리 과정은 3차원 영상에 그대로 적용되거나, 3차원 영상에 적용될 수 있도록 적응적으로 일부 수정될 수 있다. 단계 S2320에서 세부 영역 추출이 수행될 수 있다. 먼저, 초기 세부 영역이 추출될 수 있다. 초기 세부 영역이란 본 발명에 따른 알고리즘의 시작이 되는 영역들일 수 있다. 초기 세부 영역을 추출하기 위해, 영상 이진화 이후 3차원 연결 요소를 추출할 수 있다. 영상 이진화는 2차원 영상에 대한 영상 이진화와 동일 또는 유사할 수 있다. 영상 이진화를 통해 각 복셀은 0 또는 1의 값을 가질 수 있다. 1의 값을 갖는 임의의 복셀에 인접한 복셀들 중 1의 값을 갖는 복셀을 확인할 수 있다. 2차원의 픽셀은 2 차원 평면 상에서 4각형 형태로 나타내어 질 수 있다. 복셀은 3차원 공간에서의 화소의 단위이므로, 3차원 공간 상에서 육면체 형태로 나타내어 질 수 있다. 인접한 복셀이란 현재 복셀과 적어도 하나의 면을 공유하거나, 적 어도 하나의 선을 공유하거나 또는 적어도 하나의 점을 공유하는 복셀을 의미할 수 있다. 1의 값을 갖는 인접한 복셀들을 모두 확인하고, 이들 확인된 복셀들로 구성된 복셀 그룹을 초기 세부 영역으로 정의하거나 초기 세부 영역으로 추출할 수 있다. 추출된 초기 세부 영역을 기초로 세분화 작업을 진행할 수 있다 세분화 작업은 세부 영역에 포함된 복셀 값을 기준으로 수행될 수 있다. 하나의 세부 영역에 포함된 복셀 값들의 분산이 일정 값(소정의 임계값) 이하가 될 때까지 세분화 작업을 진행할 수 있다. 예컨대, 하나의 세부 영역에 포함된 복셀 값들의 분산이 일정 값 이상일 경우, 소정의 기준에 따라 하나의 세부 영역에 포함된 복셀들을 둘 이상의 세부 영역으로 다시 세분화할 수 있 다. 예컨대, 하나의 세부 영역에 포함된 복셀들의 휘도 값의 분포를 확인한 결과, 한 그룹의 복셀들은 50 내지 60의 휘도 값을 갖고, 다른 그룹의 복셀들은 200 내지 250의 휘도 값을 가지면, 해당 세부 영역에 포함된 복셀들은 제1 그룹(휘도 값 50 내지 60) 및 제2 그룹(휘도 값 200 내지 250)으로 세분화될 수 있다. 이와 같이 세분화된 각각의 복셀 그룹이 구성하는 영역은 다시 세부 영역으로 정의되고, 상기 세분화 작업이 재귀적 및/또는 반복적 으로 수행될 수 있다. 상기 세분화 작업을 수행한 결과로 획득된 세부 영역에 포함된 복셀 값들의 분산은 일정 값 이하가 된다. 따라 서, 상기 세분화 작업을 통해, 3차원 영상에서 동일한 물성을 갖는 영역들을 식별해 낼 수 있다. 상기 세분화 작업을 수행한 결과로 획득된 세부 영역은 초기 세부 영역과 구별하기 위해 최종 세부 영역으로 호칭될 수 있다. 예컨대, 3차원 보안 CT 영상의 경우 단순 연결 요소 추출법만으로 초기 세부 영역을 한정하게 되면, 분해능 등 의 장비 한계점으로 인해 다른 성질을 갖는 물체들까지 하나의 영역(객체)으로 합쳐지게 된다. 따라서, 1차로 추출된 연결 요소 기반 세부 영역 내에서, 복셀 값의 차이가 크지 않도록(즉, 분산이 일정 값 이하가 되도록) 세분화할 수 있다. 이를 통해, 해당 물체의 성질, 밀도 등을 고려한 동일/유사한 성질을 갖는 객체 영역들을 비 교적 정확히 검출해 낼 수 있다. 그러나, 상기 예에 한정되지 않으며, 3차원 전체 영역에 대해 균일하게 세부 영역을 추출할 수도 있다. 즉, 이 진화 이후 연결 요소(Connected Component) 추출법을 통해 추출된 초기 세부 영역을 그대로 최종 세부 영역으로 이용할 수 있다. 단계 S2330에서 영역 통합 및/또는 영역 분할이 수행될 수 있다. 세부 영역(초기 세부 영역 또는 최종 세부 영역) 내 복셀들의 측정 값 (예컨대, CT 장비에서 직접 인식된 원본 신호 값) 혹은 표시 값 (예컨대, 측정 값에 기반하여 디스플레이 기기에 출력하기 위한 값), 세부 영역의 3차원 위치, 크기, 형태, 후술하는 병합 이후의 형태, 영역 간 인접 부분의 유사도 등과 같은 기준에 기초하여 세부 영역들을 계층적으로 병합할 수 있다. 도 24는 세부 영역을 병합 및/또는 분할하는 실시예를 설명하기 위한 도면이다. 도 24에 있어서, A 내지 G는 영상에 포함된 세부 영역 또는 병합된 세부 영역을 지시한다. 예컨대, bottom-up 접근법의 경우, 도 24의 (a)에서 (d)의 방향으로 영역의 병합이 수행될 수 있다. 반대로, top-down 접근법의 경우, 도 24의 (d)에서 (a)의 방향으로 영역의 분할이 수행될 수 있다. 이러한 과정을 통해, 세부 영역 및 병합(분할)된 세부 영역 간의 계층 구조가 도출될 수 있다. 도 24의 (a)에 있어서, 예를 들어, 세부 영역 A와 세부 영역 B가 소정의 병합 조건에 부합하면 도 24의 (b)의 세부 영역 E로 병합될 수 있다. 새로운 세부 영역 E는 세부 영역 A 및 세부 영역 B가 포함하는 복셀들을 모두 포함할 수 있다. 또한 세부 영역 E의 하위 영역으로 세부 영역 A와 세부 영역 B가 위치하게 된다. 이러한 계층 적 관계는 도 24의 (b)에 해당하는 \"계층 구조\"로 나타낼 수 있다. 계속하여, 예를 들어, 세부 영역 E와 세부 영역 C가 소정의 병합 조건에 부합하면, 도 24의 (c)의 세부 영역 F 로 병합될 수 있다. 새로운 세부 영역 F는 세부 영역 E 및 세부 영역 C가 포함하는 복셀들을 모두 포함할 수 있 다. 또한 세부 영역 F의 하위 영역으로 세부 영역 E와 세부 영역 C가 위치하게 된다. 이러한 계층적 관계는 도 24의 (c)에 해당하는 \"계층 구조\"로 나타낼 수 있다. 계속하여, 예를 들어, 세부 영역 F와 세부 영역 D가 소정의 병합 조건에 부합하면, 도 24의 (d)의 세부 영역 G 로 병합될 수 있다. 새로운 세부 영역 G는 세부 영역 F 및 세부 영역 D가 포함하는 복셀들을 모두 포함할 수 있 다. 또한 세부 영역 G의 하위 영역으로 세부 영역 F와 세부 영역 D가 위치하게 된다. 이러한 계층적 관계는 도 24의 (d)에 해당하는 \"계층 구조\"로 나타낼 수 있다. 상기와 같이 세부 영역의 병합 과정을 반복적으로 수행함으로써, 모든 작업이 끝날 경우 세부 영역들에 대한 관 계가 트리 구조 형태로 표현될 수 있다. 소정의 병합 조건이란, 각 세부 영역들이 완전히 다른 물체에 해당하는 것이 아닌, 상관 관계를 갖는 영역들끼 리 병합되기 위한 기준이다. 소정의 병합 조건은 예컨대, 다음과 같을 수 있다. 병합은 현재 세부 영역들 중 크기가 작은 세부 영역들을 우선적으로 병합하는 형태로 진행될 수 있다. 예컨대, 도 24의 (a)에서 세부 영역 A와 세부 영역 B가 가장 작으므로, 세부 영역 A와 세부 영역 B에 대한 병합이 먼저 수행될 수 있다. 유사한 이유로, 세부 영역 A와 세부 영역 D를 결합하거나, 세부 영역 A와 세부 영역 C를 결합 하는 형태로는 진행되지 않을 수 있다. 병합은 병합되는 세부 영역들 사이의 위치(거리)에 기초하여 수행될 수 있다. 예컨대, 가까운 거리에 있는 세부 영역들이 우선적으로 병합될 수 있다. 도 24의 (a)에서 세부 영역 A와 세부 영역 B는 가깝고, 세부 영역 B와 세 부 영역 C는 멀다. 따라서, 세부 영역 A와 세부 영역 B에 대한 병합이 먼저 수행될 수 있다. 병합은 병합되는 세부 영역들 사이의 형태(병합면의 형태 또는 인접 부분의 유사도 등)에 기초하여 수행될 수 있다. 예컨대, 세부 영역 A와 세부 영역 B는 병합면이 특징적이고, 인접 부분의 형태가 유사하다. 따라서, 세부 영역 A와 세부 영역 B에 대한 병합이 우선적으로 수행될 수 있다. 병합은 병합된 후의 형태에 기초하여 수행될 수 있다. 도 24의 (a)에서 세부 영역 A와 세부 영역 B를 병합한 형 태는 거의 완벽한 원의 형태(3차원 영상에서는 구의 형태)가 된다. 이 경우, 세부 영역 A와 세부 영역 B는 동일 객체에 포함된 세부 영역들일 확률이 크다. 따라서, 세부 영역 A와 세부 영역 B의 병합이 우선적으로 수행될 수 있다. 상기 언급한 기준들 이외에도 본 발명의 취지에 부합하는 기준들(즉, 하나의 객체에 포함된다고 판단할 수 있을 만한 기준들)이 적용될 수 있다. 본 발명의 다른 실시예에 따르면, 상기 bottom-up 과정을 반대로 수행하여 전체 영역에서 세부 영역으로 분할하 는 방식(top-down 방식)으로 진행할 수 있다. 도 24의 (c)와 (d)에 있어서, 예를 들어, 영역 G에 포함된 세부 영역 F와 세부 영역 D가 소정의 병합 조건에 부 합하면, 도 24의 (d)의 영역 G는 도 24의 (c)의 세부 영역 F와 세부 영역 D로 분할될 수 있다. 계속하여, 도 24의 (b)와 (c)에 있어서, 예를 들어, 세부 영역 F에 포함된 세부 영역 E와 세부 영역 C가 소정의 병합 조건에 부합하면, 도 24의 (c)의 세부 영역 F는 도 24의 (b)의 세부 영역 E와 세부 영역 C로 분할될 수 있 다. 계속하여, 도 24의 (a)와 (b)에 있어서, 예를 들어, 세부 영역 E에 포함된 세부 영역 A와 세부 영역 B가 소정의 병합 조건에 부합하면, 도 24의 (b)의 세부 영역 E는 도 24의 (a)의 세부 영역 A와 세부 영역 B로 분할될 수 있 다. 도 24는 2차원 영상을 도시하였으나, 본 실시예는 3차원 영상을 대상으로 수행될 수 있다. Bottom-up 방식의 경 우, 3차원 영상에 포함된 3차원의 세부 영역들을 상기 기준에 따라 병합하여 나갈 수 있다. top-down 방식의 경 우, 3차원 영상으로부터 3차원의 세부 영역들로 분할해 나갈 수 있다. bottom-up 방식의 경우, 예컨대 도 24에 도시된 바와 같이, 세부 영역 A와 세부 영역 B가 세부 영역 E로 병합될 수 있다(도 24의 (b)). 그 후, 세부 영역 E와 세부 영역 C가 세부 영역 F로 병합될 수 있다(도 24의 (c)). 그 후, 세부 영역 F와 세부 영역 D가 세부 영역 G로 병합될 수 있다(도 24의 (d)). 최종적으로 3차원 영상과 동일 크기의 영역 G가 병합에 의해 획득되면, 병합에 순서에 따른 계층 구조가 획득될 수 있다. top-down 방식의 경우, 3차원 영상과 동일 크기의 영역 G를 세부 영역 F와 세부 영역 D로 분할할 수 있다(도 24 의 (c)). 그 후, 세부 영역 F를 세부 영역 E와 세부 영역 C로 분할할 수 있다(도 24의 (b)). 그 후, 세부 영역 E를 세부 영역 A와 세부 영역 B로 분할할 수 있다(도 24의 (a)). top-down 방식의 경우, 더 이상 분할할 수 없 는 단계까지 영역을 분할한 후, 분할의 순서에 따른 계층 구조가 획득될 수 있다. 동일한 객체를 포함하는 동일한 3차원 영상에 대해 bottom-up 방식에 따라 획득된 계층 구조와 top-down 방식에 따라 획득된 계층 구조는 동일할 수 있다. 단계 S2340에서 세부 영역에 대한 분류가 수행될 수 있다. 또한 단계 S2350에서 영역의 통합/분할에 따른 계층 구조가 분석될 수 있다. 세부 영역에 대한 분류 결과와 계층 구조를 분석한 결과를 이용하여 단계 S2360에서 최 종 결과가 출력될 수 있다. 최종 결과(최종 분석 결과)는 목적물(객체)의 종류를 식별하는 것일 수 있다. 보다 구체적으로 설명하면, 단계 S2340에서, 각 세부 영역에 해당하는 객체(목적물)의 종류를 자동 분류할 수 있다. 이를 위해, 전술한 다양한 객체 검출/분류 알고리즘이 적용될 수 있다. 2차원 영상으로부터 객체를 검출 하고 분류하는 본 발명의 실시예에 따른 다양한 방법들은 3차원 영상으로부터 객체를 검출하기 위해 동일하게 사용되거나 또는 차원의 증가에 따라 변형된 후 사용될 수 있다. 또는 3차원 영상으로부터 객체를 검출하고 분 류하기 위해, 3차원 신경망 구조 혹은 semi-3D 신경망 구조가 이용될 수 있다. 또는, 상기 계층적 트리 구조를 이용하기 위한 재귀 신경망(RNN, Recursive Neural Network) 구조를 적용할 수 있다. 또는, 계층적 트리 구조에 따른 목적물의 포함 관계에 관한 확률을 분류 결과에 대한 가중치로 사용함으로써 신뢰도를 보정할 수 있다. 상 기 확률은 사전에 정의될 수 있다. 본 개시에 따르면, 3차원 영상에 포함된 객체의 분류 성능을 높이기 위해, S2350의 계층적 트리 구조가 이용될 수 있다. 예컨대, 스마트폰은 리튬 배터리를 포함하고 있으므로, 트리의 하위 노드가 리튬 배터리일 때, 상위 노드는 스마트폰일 가능성이 높다. 그러나, 그 반대의 경우는 가능하지 않으므로 분류 후보에서 제외될 수 있다. 예컨대, 하위 노드가 배터리일 때, 상위 노드가 노트북이나 휴대폰일 확률은 높게 정의될 수 있다. 그러 나 상위 노드가 칼 또는 USB일 확률은 낮게 정의될 수 있다. 이러한 확률은 각 노드에 해당하는 객체의 분류를 위한 가중치로서 사용될 수 있다. 도 25는 본 발명에 따라 여행자 가방으로부터 객체를 검출하는 실시예를 설명하기 위한 도면이다. 본 발명에 따라 3차원 영상을 분석한 결과 도 25에 도시된 바와 같은 계층적 트리 구조가 획득될 수 있다. 하위 노드에 배터리와 화약을 포함하고 있는 노드는 폭발물일 확률이 높다. 폭발물에 포함된 또 다른 하위 노드는 뇌 관으로 분류될 확률이 높다. 또한 배터리, CPU, 카메라를 하위 노드로서 포함하는 노드는 스마트폰으로 분류될 확률이 높다. 반대로, 스마트폰으로 분류된 노드의 하위 노드는 스마트폰이 일반적으로 포함하는 객체(배터리, CPU, 카메라 등)들 중 하나로 분류될 확률이 높다. 이와 같이, 상위 노드의 객체와 하위 노드의 객체는 소정의 포함 관계를 가질 수 있으며, 이러한 포함 관계를 이용하여 객체의 분류를 보다 정확하게 수행할 수 있다. 예컨대, 하위 노드의 객체들의 분류 결과가 상위 노드 의 객체를 분류하는데 이용될 수 있다. 또는 상위 노드의 객체의 분류 결과가 하위 노드의 객체들을 분류하는데 이용될 수 있다. 또는 하위 노드의 객체의 분류 결과를 이용하여 상위 노드의 객체를 분류하고, 다시 상위 노드 의 객체의 분류 결과를 이용하여 분류가 어려운 하위 노드의 객체의 분류에 이용하거나, 또는 하위 노드의 객체 의 분류 결과를 검증할 수도 있다. 전술한 예 이외에, 계층적 트리 구조를 이용하여 트리의 각 노드에 해당하는 객체의 분류 결과를 향상시킬 수 있는 모든 방법들이 본 발명의 범위에 포함될 수 있다. 이와 같은 접근법은 2차원 영상에 대해서도 적용될 수 있다. 그러나, 본 발명은 3차원 영상에 대해서 최적의 효 과를 가질 수 있다. 왜냐하면 비파괴 검사법에 따를 때, 2차원 영상의 경우 세부 영역들이 중첩되는 경우가 많 으므로, 동일한 객체라고 하더라도 중첩된 부분과 중첩되지 않은 부분에 투과율의 차이가 발생할 수 있다. 투과 율에 차이가 발생하면, 각각의 부분들은 상이한 픽셀 값의 범위로 표현되므로, 결론적으로 각각의 부분들은 서 로 다른 세부 영역으로 판단될 수 있기 때문이다. 반면, 3차원 영상의 경우, 세부 영역들이 중첩되지 않으므로, 각각의 세부 영역을 명확히 분리해 낼 수 있다. 따라서, 세부 영역들 간의 포함 관계를 이용하는 본 발명의 실 시예가 효과적으로 적용될 수 있다. 본 개시의 예시적인 방법들은 설명의 명확성을 위해서 동작의 시리즈로 표현되어 있지만, 이는 단계가 수행되는 순서를 제한하기 위한 것은 아니며, 필요한 경우에는 각각의 단계가 동시에 또는 상이한 순서로 수행될 수도 있 다. 본 개시에 따른 방법을 구현하기 위해서, 예시하는 단계에 추가적으로 다른 단계를 포함하거나, 일부의 단 계를 제외하고 나머지 단계를 포함하거나, 또는 일부의 단계를 제외하고 추가적인 다른 단계를 포함할 수도 있 다. 본 개시의 다양한 실시예는 모든 가능한 조합을 나열한 것이 아니고 본 개시의 대표적인 양상을 설명하기 위한 것이며, 다양한 실시예에서 설명하는 사항들은 독립적으로 적용되거나 또는 둘 이상의 조합으로 적용될 수도 있 다. 또한, 본 개시의 다양한 실시예는 하드웨어, 펌웨어(firmware), 소프트웨어, 또는 그들의 결합 등에 의해 구현 될 수 있다. 하드웨어에 의한 구현의 경우, 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 범용 프로세서(general processor), 컨트롤러, 마이크로 컨트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 본 개시의 범위는 다양한 실시예의 방법에 따른 동작이 장치 또는 컴퓨터 상에서 실행되도록 하는 소프트웨어 또는 머신-실행가능한 명령들(예를 들어, 운영체제, 애플리케이션, 펌웨어(firmware), 프로그램 등), 및 이러한 소프트웨어 또는 명령 등이 저장되어 장치 또는 컴퓨터 상에서 실행 가능한 비-일시적 컴퓨터-판독가능 매체 (non-transitory computer-readable medium)를 포함한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17 도면18 도면19 도면20 도면21 도면22 도면23 도면24 도면25"}
{"patent_id": "10-2019-0161515", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 물품 검색 시스템을 설명하기 위한 도면이다. 도 2는 본 개시의 일 실시예에 따른 영상 분석 장치의 구성을 나타내는 블록도이다. 도 3은 영상의 판독 과정을 설명하기 위한 도면이다. 도 4는 본 개시의 일 실시예에 따른 영상 판독 과정에서의 인공지능의 적용 범위를 설명하기 위한 도면이다. 도 5는 본 개시에 따른 영상 강화를 수행하는 영상 강화 장치의 일 실시예를 도시한 도면이다. 도 6은 본 개시의 일 실시예에 따라, 단일 객체를 포함하는 영상으로부터 객체와 배경을 구분하고, 객체의 위치 정보를 생성하는 과정을 설명하기 위한 도면이다. 도 7은 본 개시의 일 실시예에 따른 객체의 물성에 기초하여 색상이 표현된 영상을 나타내는 도면이다. 도 8은 본 개시의 일 실시예에 따른 영상의 색상 분포 정보에 기초하여 출력 영상을 생성하는 과정을 설명하기 위한 도면이다. 도 9는 본 개시의 일 실시예에 따른 색상 분포 정보를 이용하여 획득된 영상과 에지 기반 필터링 또는 평활화 필터링을 적용하여 획득된 영상을 결합한 최종 출력 영상을 획득하는 과정을 설명하기 위한 도면이다. 도 10은 본 개시의 일 실시예에 따른 그래피컬 모델을 이용하여 최종 출력 영상을 획득하는 과정을 설명하기 위 한 도면이다. 도 11은 본 개시의 일 실시예에 따른 영상 강화 방법을 설명하기 위한 도면이다. 도 12는 본 개시의 일 실시예에 따른 맥락 분석을 설명하기 위한 도면이다. 도 13은 본 개시의 일 실시예에 따른 영상의 맥락 정보를 생성하고 분석하는 과정을 나타내는 도면이다. 도 14는 본 개시의 일 실시예에 따른 영상 분석 장치가 영상을 분석하여 객체를 식별하는 과정을 설명하기 위한 도면이다.도 15는 본 개시의 일 실시예에 따른 영상 분석 장치의 동작을 설명하기 위한 도면이다. 도 16은 다채널 특징맵을 생성하는 합성곱 신경망의 일 실시예를 설명하기 위한 도면이다. 도 17은 풀링 기법의 일 실시예를 설명하기 위한 도면이다. 도 18은 본 개시의 일 실시예에 따른 영상 합성 장치의 구성을 나타내는 블록도이다. 도 19는 본 개시의 일 실시예에 따른 단일 객체를 포함하는 두 개의 영상을 이용하여 다중 객체 영상을 생성하 는 과정을 나타내는 도면이다. 도 20은 본 개시의 일 실시예에 따른 다중 객체 영상을 이용하여 합성곱 신경망을 학습시키는 과정을 나타내는 도면이다. 도 21은 본 개시의 일 실시예에 따른 영상 합성 장치를 이용하여 실제 영상을 분석하는 과정을 설명하기 위한 도면이다. 도 22는 본 개시의 일 실시예에 따른 영상 합성 방법을 설명하기 위한 도면이다. 도 23은 3차원 영상에 포함된 객체를 검출하는 본 발명의 실시예를 설명하기 위한 도면이다. 도 24는 세부 영역을 병합 및/또는 분할하는 실시예를 설명하기 위한 도면이다. 도 25는 본 발명에 따라 여행자 가방으로부터 객체를 검출하는 실시예를 설명하기 위한 도면이다."}
