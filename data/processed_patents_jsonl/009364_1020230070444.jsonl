{"patent_id": "10-2023-0070444", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0082159", "출원번호": "10-2023-0070444", "발명의 명칭": "스파이킹 뉴럴 네트워크의 가속을 위한 장치 및 방법", "출원인": "한국전자기술연구원", "발명자": "김희탁"}}
{"patent_id": "10-2023-0070444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "외부 메모리 액세스 없이 멤브레인 데이터(막 전위 데이터)를 연산하기 위한 스파이킹 뉴럴 네트워크의 가속을위한 장치에서,인터페이스를 통해 상기 외부 메모리로부터 읽어온 시냅스 전 뉴런 데이터(ineu)와 시냅스 가중치를 저장하는글로벌 버퍼(gbuf);상기 글로벌 버퍼로부터 전달된 상기 시냅스 전 뉴런 데이터와 상기 시냅스 가중치에 대해 MAC(Multiply-and-Accumulate) 연산을 수행하여 부분합(psum)을 계산하는 처리 요소(PE, processing element) 어레이;상기 부분합을 누적하는 연산을 통해 계산한 시냅스 후 뉴런 데이터(oneu)를 상기 글로벌 버퍼에 저장하는 부분합 누산기; 및상기 시냅스 후 뉴런 데이터를 기반으로 멤브레인 데이터를 계산하고, 상기 멤브레인 데이터에 대응하는 스파이크(spike) 데이터를 상기 인터페이스를 통해 상기 외부 메모리로 전달하는 활성화 코어를 포함하는 스파이킹 뉴럴 네트워크의 가속을 위한 장치."}
{"patent_id": "10-2023-0070444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서,상기 글로벌 버퍼에 저장된 상기 시냅스 전 뉴런 데이터와 상기 시냅스 가중치를 특정 대역폭으로 상기 처리 요소 어레이에 전달하는 X-버스를 더 포함하는 스파이킹 뉴럴 네트워크의 가속을 위한 장치."}
{"patent_id": "10-2023-0070444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에서,상기 특정 대역폭은 256 비트/사이클인 스파이킹 뉴럴 네트워크의 가속을 위한 장치."}
{"patent_id": "10-2023-0070444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에서,상기 인터페이스는 AHB(Advanced High-performance Bus) 마스터 인터페이스인 스파이킹 뉴럴 네트워크의 가속을위한 장치."}
{"patent_id": "10-2023-0070444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에서,상기 처리 요소 어레이는 복수의 처리 요소(PE)들을 포함하고, 각 처리 요소(PE)는,상기 시냅스 전 뉴런 데이터(ineu)를 저장하는 제1 버퍼;상기 시냅스 가중치를 저장하는 제2 버퍼; 및상기 시냅스 전 뉴런 데이터와 상기 시냅스 가중치에 대해 상기 MAC 연산을 수행하기 위해 적어도 하나의 곱셈기와 적어도 하나의 덧셈기를 포함하는 MAC 연산기를 포함하는 스파이킹 뉴럴 네트워크의 가속을 위한 장치."}
{"patent_id": "10-2023-0070444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2024-0082159-3-제5항에서,상기 제1 및 제2 버퍼 각각은 256 비트의 버퍼인 스파이킹 뉴럴 네트워크의 가속을 위한 장치."}
{"patent_id": "10-2023-0070444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에서,상기 활성화 코어는,LIF(leaky integrate-fire) 스파이킹 뉴런 모델 연산(spiking neuron model operation)에 의해 상기 멤브레인데이터를 계산하는 것인 스파이킹 뉴럴 네트워크의 가속을 위한 장치."}
{"patent_id": "10-2023-0070444", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "외부 메모리 액세스 없이 멤브레인 데이터(막 전위 데이터)를 연산하기 위한 스파이킹 뉴럴 네트워크의 가속을위한 장치의 동작 방법에서,글로벌 버퍼에서, 인터페이스를 통해 외부 메모리로부터 읽어온 시냅스 전 뉴런 데이터(ineu)와 시냅스 가중치(synapse)를 저장하는 단계;처리 요소 어레이에서, 상기 시냅스 전 뉴런 데이터(ineu) 및 상기 시냅스 가중치(synapse)에 대해 MAC 연산을수행하여 부분합(psum)을 계산하는 단계;부분합 누산기에서, 상기 계산된 부분합(psum)을 누산하여 계산한 시냅스 후 뉴런 데이터(oneu)를 상기 글로벌버퍼에 저장하는 단계; 및활성화 코어에서, 상기 시냅스 후 뉴런 데이터(oneu)에 대해 LIF 스파이킹 뉴런 모델 연산을 수행하여 멤브레인데이터와 상기 멤브레인 데이터에 따른 스파이크 데이터를 계산하여, 상기 외부 메모리에 저장하는 단계를 포함하는 스파이킹 뉴럴 네트워크의 가속을 위한 장치의 동작 방법."}
{"patent_id": "10-2023-0070444", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "외부 메모리 액세스 없이 멤브레인 데이터(막 전위 데이터)를 연산하기 위한 스파이킹 뉴럴 네트워크의 가속을 위한 장치가 개시된다. 이 장치는 인터페이스를 통해 상기 외부 메모리로부터 읽어온 시냅스 전 뉴런 데이터 (ineu)와 시냅스 가중치를 저장하는 글로벌 버퍼(gbuf), 상기 글로벌 버퍼로부터 전달된 상기 시냅스 전 뉴런 데 이터와 상기 시냅스 가중치에 대해 MAC(Multiply-and-Accumulate) 연산을 수행하여 부분합(psum)을 계산하는 처 리 요소(PE, processing element) 어레이, 상기 부분합을 누적하는 연산을 통해 계산한 시냅스 후 뉴런 데이터 (oneu)를 상기 글로벌 버퍼에 저장하는 부분합 누산기 및 상기 시냅스 후 뉴런 데이터를 기반으로 멤브레인 데이 터를 계산하고, 상기 멤브레인 데이터에 대응하는 스파이크(spike) 데이터를 상기 인터페이스를 통해 상기 외부 메모리로 전달하는 활성화 코어를 포함한다."}
{"patent_id": "10-2023-0070444", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 에너지 효율적인(an energy-efficient) 스파이킹 뉴럴 네트워크의 가속을 위한 하드웨어 장치(프로세 서 또는 칩)에 관한 것으로, 보다 구체적으로는 스파이킹 뉴럴 네트워크(Spiking Neural Networks) 연산 수행 시 외부 메모리 접근을 최소화하는 하드웨어 구조에 관한 것이다."}
{"patent_id": "10-2023-0070444", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 들어 인공지능은 일상생활의 다양한 부분에서 주목을 받고 있다. 예를 들어 산업 공장이나 로봇, 자율주행 에서는 인공지능 알고리즘을 사용하여 다양한 상황에 대한 인지나 데이터 분류와 같은 일들을 수행하고 있다. 높은 성능을 성취한 CNN(Convolution Neural Network), RNN(Recurrent Neural Network) 등의 유명한 인공지능 알고리즘들은 실제 뇌의 동작 환경 기반으로 설계된다. 따라서, 다음 세대의 더 개선된 인공지능 알고리즘을 위 해서는 뉴로모픽(neuromorphic) 알고리즘에 대한 연구가 필수적이다. 뉴로모픽 알고리즘 중 가장 유명한 것은 SNN이라고 불리는 스파이킹 뉴럴 네트워크(spiking neural network)이 다. 우리의 뇌는 뉴런으로 이루어져 있고, 뉴런들은 시냅스(synapse)라는 매개체를 통하여 스파이크(spike)로 통신을 수행하며 뇌의 동작이 수행되는데, 이런 현상을 모방한 것이 SNN이다. 뉴런의 입력으로 시냅스를 통해 스파이크(Spike)가 들어오면 시간에 따라 막 전위 값(membrane potential, memb)을 저장소에 누적했다가 막 전위 값(memb)이 특정 임계치(threshold)를 넘으면 해당 뉴런이 스파이크 (Spike) 데이터를 다음 뉴런으로 내보내는 특징을 가진다. 만약 Spike가 들어오지 않으면 memb가 시간에 따라 점차 줄어들게 된다. SNN의 가장 큰 문제점은 최신 딥 뉴럴 네트워크 대비하여 정확도가 현저히 낮다는 점이다. 최근에 SNN의 정확도 를 높이기 위해 역전파(Back-propagation)를 SNN에 적용하려는 연구들이 수행되고 있으나, 스파이크(Spike)를만들 때 사용하는 함수가 미분 불가능하고, 1-bit를 사용하는 스파이크(Spike)에서 정보 손실이 발생한다. 따라서 최근에서는 스파이크(Spike)에 1-bit가 아닌 수 bit를 할당하여 정확도를 높인 연구 결과물[*]이 있으며, SNN의 정확도 문제를 해결하기 위해서는 아직 딥 뉴럴 네트워크처럼 뉴런의 출력 값에 수 bit 정도가 필요한 상황이다. 또한 SNN을 가속하기 위한 하드웨어 가속기를 설계하기 위해서는 스파이킹 뉴런(Spiking neuron) 동작을 하드웨 어로 구현해야 한다. 하지만 SNN에서 time-step에 따라 반복적으로 수행되는 막 전위 값(memb) 연산은 외부 메 모리의 액세스 빈도를 높이므로 가속기의 처리 속도를 감소시키고 에너지 소모를 증가시키게 되므로 하드웨어 성능이 떨어지게 된다. [*] W. Zhang, et al. \"Temporal spike sequence learning via backpropagation for deep spiking neural networks.\" Advances in Neural Information Processing Systems, 2020"}
{"patent_id": "10-2023-0070444", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상술한 문제점을 해결하기 위한 본 발명은 스파이킹 뉴럴 네트워크의 빠른 연산 처리를 위한 스파이킹 뉴럴 네 트워크의 가속을 위한 장치를 제공하는데 있다."}
{"patent_id": "10-2023-0070444", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 목적을 달성하기 위한 본 발명의 일면에 따른 스파이킹 뉴럴 네트워크의 가속을 위한 장치는 외부 메모 리 액세스 없이 멤브레인 데이터(막 전위 데이터)를 연산하기 위한 장치로서, 인터페이스를 통해 상기 외부 메 모리로부터 읽어온 시냅스 전 뉴런 데이터(ineu)와 시냅스 가중치를 저장하는 글로벌 버퍼(gbuf), 상기 글로벌 버퍼로부터 전달된 상기 시냅스 전 뉴런 데이터와 상기 시냅스 가중치에 대해 MAC(Multiply-and-Accumulate) 연 산을 수행하여 부분합(psum)을 계산하는 처리 요소(PE, processing element) 어레이, 상기 부분합을 누적하는 연산을 통해 계산한 시냅스 후 뉴런 데이터(oneu)를 상기 글로벌 버퍼에 저장하는 부분합 누산기 및 상기 시냅 스 후 뉴런 데이터를 기반으로 멤브레인 데이터를 계산하고, 상기 멤브레인 데이터에 대응하는 스파이크(spike) 데이터를 상기 인터페이스를 통해 상기 외부 메모리로 전달하는 활성화 코어를 포함한다. 실시 예에서, 상기 장치는 상기 글로벌 버퍼에 저장된 상기 시냅스 전 뉴런 데이터와 상기 시냅스 가중치를 특 정 대역폭으로 상기 처리 요소 어레이에 전달하는 X-버스를 더 포함한다. 실시 예에서, 상기 특정 대역폭은 256 비트/사이클이다. 실시 예에서, 상기 인터페이스는 AHB(Advanced High-performance Bus) 마스터 인터페이스이다. 실시 예에서, 상기 처리 요소 어레이는 복수의 처리 요소(PE)들을 포함한다. 실시 예에서, 각 처리 요소(PE)는, 상기 시냅스 전 뉴런 데이터(ineu)를 저장하는 제1 버퍼; 상기 시냅스 가중 치를 저장하는 제2 버퍼; 및 상기 시냅스 전 뉴런 데이터와 상기 시냅스 가중치에 대해 상기 MAC 연산을 수행하 기 위해 적어도 하나의 곱셈기와 적어도 하나의 덧셈기를 포함하는 MAC 연산기를 포함한다. 실시 예에서, 상기 제1 및 제2 버퍼 각각은, 256 비트의 버퍼이다. 실시 예에서, 상기 활성화 코어는 LIF(leaky integrate-fire) 스파이킹 뉴런 모델 연산(spiking neuron model operation)에 의해 상기 멤브레인 데이터를 계산한다. 본 발명의 다른 일면에 따른 스파이킹 뉴럴 네트워크의 가속을 위한 장치의 동작 방법은, 글로벌 버퍼에서, 인 터페이스를 통해 외부 메모리로부터 읽어온 시냅스 전 뉴런 데이터(ineu)와 시냅스 가중치(synapse)를 저장하는 단계; 처리 요소 어레이에서, 상기 시냅스 전 뉴런 데이터(ineu) 및 상기 시냅스 가중치(synapse)에 대해 MAC 연산을 수행하여 부분합(psum)을 계산하는 단계; 부분합 누산기에서, 상기 계산된 부분합(psum)을 누산하여 계 산한 시냅스 후 뉴런 데이터(oneu)를 상기 글로벌 버퍼에 저장하는 단계; 및 활성화 코어에서, 상기 시냅스 후 뉴런 데이터(oneu)에 대해 LIF 스파이킹 뉴런 모델 연산을 수행하여 멤브레인 데이터와 상기 멤브레인 데이터에따른 스파이크 데이터를 계산하여, 상기 외부 메모리에 저장하는 단계를 포함한다."}
{"patent_id": "10-2023-0070444", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 첫째, 높은 정확도를 확보한 스파이킹 뉴럴 네트워크 구조를 가속할 수 있다. 둘째, 스파이 킹 뉴런의 막 전위값 연산에 따른 연산 부하를 최소화할 수 있다."}
{"patent_id": "10-2023-0070444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부한 도면들을 참조하여, 본 발명의 바람직한 실시예를 보다 상세하게 설명하고자 한다. 본 발명을 설 명함에 있어 전체적인 이해를 용이하게 하기 위하여 도면상의 동일한 구성요소에 대해서는 동일한 참조부호를 사용하고 동일한 구성요소에 대해서 중복된 설명은 생략한다. 본 명세서에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도 가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포 함하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의 미를 가진 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 인간 뇌 환경(human brain environment)을 모방하는 스파이킹 신경망(SNN, Spiking Neural Networks)은 인간 지향 인공지능(Human-Oriented Artificial Intelligence)을 개발하는 핵심 역할로 간주된다. SNN은 최근 개발된 심층 신경망(DNN, Developed Deep Neural Networks)보다 알고리즘 정확도(algorithmic accuracy)가 낮다. 하지만 컨볼루션 연산(convolution operation)과 스파이킹 뉴런(spiking neuron)을 결합한 스파이킹 컨볼루션 신경망(SCNN, Spiking Convolutional Neural Network)은 DNN과 비슷한 정확도(comparable accuracy)를 달성한다. 그러나 SCNN에서는 반복적인 멤브레인 포텐셜(막 전위) 업데이트(repetitive membrane potential updates)를 위해 빈번한 외부 메모리 액세스와 낮은 하드웨어 처리량(low hardware throughput)으로 인해, 에너지 효율적인 SNN 가속화(SNN acceleration)가 어렵다. 이에 본 발명에서는 멤브레인 포텐셜(막 전위)에 대응하는 멤브레인 데이터(membrane data)를 재사용하여 외부 메모리 액세스를 최소화하는 새로운 데이터 흐름(a novel dataflow)이 제공되며, 이러한 새로운 데이터 흐름을 기반으로 설계된 스파이킹 뉴럴 네트워크의 가속을 위한 장치, 즉, SNN 프로세서, 더 구체적으로 SCNN 프로세서 가 제공된다. 본 발명의 실시 예에 따른 새로운 데이터 흐름을 기반으로 설계된 스파이킹 뉴럴 네트워크의 가속을 위한 장치 (SNN 프로세서 또는 SCNN 프로세서)를 설명하기에 앞서, 본 발명의 이해를 돕기 위해, 본 발명에 적용되는SCNN(Spiking Convolutional Neural Network) 알고리즘에 대해 간략히 설명하기로 한다. SCNN은 시간 영역에서 스파이크를 방출하는 생물학적으로 그럴듯한 스파이크 뉴런으로 구성된다. 시간 영역에서 스파이크 뉴런은 디지털화된 모든 시간 단계 동안 아래 수학식을 반복적으로 계산한다. 수학식 1"}
{"patent_id": "10-2023-0070444", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, t는 시간 영역(time domain)에서 디지털화된 시간 스텝(a digitized time-step)이고, l은 레이어를 나 타내고, Vth는 임계 전압, τm은 누설 계수(a leakage factor), Q는 시냅스 모델(a synaptic model)이다. 는 풀리 커넥티드 레이어(fully connected layer)에서 도트 곱(dot product) 또는 컨볼루션 레이어(convolutional layer)에서 3D 컨볼루션 연산(3D convolution operation)을 나타낸다. Al는 컨볼루션 결과 행렬(the matrices of the convolution result), Il는 시냅스 전 뉴런(pre-synaptic neuron, ineu)이고, Ol는 시냅스 후 뉴런(post-synaptic neuron, oneu)이고, Wl는 시냅스 가중치(synapse weight), Ul는 막 전위(memb)이다. 그리고 Sl은 레이어 l의 스파이크이다. 레이어 l의 oneu는 다음 레이어 l+1 의 ineu에 연결된다. 위의 수학식 1에서 의 곱하기 및 누적(MAC, Multiply-and-Accumulate) 연산 후, ~에 설명된 스파이크 뉴런 모델 연산(spiking neuron model operations)이 막 전위(memb)를 계산하는데 적용된다. 그런 다음 간단한 선형 변환(a simple linear transformation)을 포함하는 의 시냅스 모델 함수 Q를 통해 최종 oneu가 계산된 다. 스파이크가 이진 표현(binary representation)인 경우에도 첫 번째 레이어에서는 ineu는 다중 비트 정밀도를 가 지며, SCNN의 풀링 레이어 연산(pooling layer operation) 후에 다중 비트 oneu가 제공된다. 하드웨어에서 엔드-투-엔드 SCNN 작동을 지원하기 위해, ineu, oneu 및 synapse에 16비트 고정 소수점 정밀도 (16-bit fixed point precision)를 사용한다. 위 수학식 1로부터, 스파이킹 뉴런 모델 연산(spiking neuron model operation)을 제외하고 T를 1로 설정하면 SCNN에서 CNN 연산을 원활하게 수행될 수 있음을 알 수 있다. 위 수학식 1에서, 와 에 따르면, 멤브레인 데이터(막 전위)는 시간 단계가 진행에 따라 반복적으로 업데이트된다. 멤브레인 포텐셜(막 전위) 연산(memb computation)은 계산 복잡도가 낮더라도 멤브레인 데이터(memb data)를 저 장하기 위한 추가 메모리 용량이 필요하다. 멤브 데이터(memb data)가 온-칩(on-chip)에 저장되지 않는 경우, 멤브 데이터는 T 타임 스텝을 통해 외부 메모리(external memory) 사이를 오간다. 빈번한 외부 메모리 액세스는 막대한 에너지 소비(huge energy consumption)와 처리량 저하(throughput degradation)를 유발하기 때문에 멤브 데이터 이동(memb data movement)을 줄이는 것은 SCNN 프로세서를 구현 하는 데 필수적인 부분이다. 이에 본 발명에서는 외부 메모리에 액세스하지 않고 멤브 데이터(memb data)를 온-칩(on-chip)에서 계산하는 새로운 데이터 플로우와 이러한 새로운 데이터 플로우를 기반으로 설계된 장치(SNN 프로세서 또는 SCNN 프로세서)를 제안한다. 도 1은 본 발명의 실시 예에 따른 스파이킹 뉴럴 네트워크의 가속을 위한 장치의 구성도이고, 도 2는 도 1에 도 시된 단일 PE(Processing Element)의 구성도이다. 본 발명의 실시 예에 따른 스파이킹 뉴럴 네트워크의 가속을 위한 장치는, SNN 연산 또는 SCNN 연산을 효 율적으로 처리하는 반도체 칩 형태의 프로세서(이하, \"SNN 프로세서(또는 SCNN 프로세서)\"라 함)일 수 있다. 본 발명의 일 실시 예에 따른 SNN 프로세서에서는 외부 메모리 액세스에 의한 멤브레인 데이터(membrane data 또는 membrane potential data(막 전위 데이터)) 이동을 제거하기 위해, 새로운 데이터 플로우(a novel dataflow)를 기반으로 설계된다. 데이터 플로우에서 모든 oneus(시냅스 후 뉴런 데이터)를 계산하여 외부 메모 리로 후술하는 psums(partial sums, 부분합)을 이동하지 않고 후술하는 글로벌 버퍼(gbuf)에 저장한다. 또한 본 발명의 다른 실시 예에 따른 SNN 프로세서에서는, 각 시간 단계에서 새로운 스파이크 데이터와 멤 브레인 데이터는 글로벌 버퍼(gbuf)의 oneus(시냅스 후 뉴런 데이터)를 사용하여 계산되고, 글로벌 버퍼(gbuf) 의 oneus(시냅스 후 뉴런 데이터)는 새로 업데이트된 멤브레인 데이터로 대체된다. 또한 본 발명의 또 다른 실시 예에 따른 SNN 프로세서에서는, 업데이트된 멤브레인 데이터는 다음 시간 단 계의 초기 멤브레인 데이터로 사용된다. 따라서 멤브레인 데이터는 모든 시간 단계에 걸쳐 재사용된다. 또한, 본 발명의 또 다른 실시 예에 따른 SNN 프로세서에서는, 상기 글로벌 버퍼(gbuf)와 후술하는 처리 요소(PE, processing element) 사이에서 높은 대역폭(256비트/사이클)을 사용하는 240개(15x16)의 NoC를 구축 함으로써 높은 처리량을 달성할 수 있다. 또한, 본 발명의 또 다른 실시 예에 따른 SNN 프로세서에서는, 다양한 유형의 활성화 함수(activation functions)와 스파이킹 뉴런 모델을 지원한다. 또한, 최종적으로 계산된 oneus(시냅스 후 뉴런 데이터)를 글로 벌 버퍼(gbuf)에 저장한 상태에서 활성화 함수를 계산하기 때문에, 다른 활성화 함수를 쉽게 추가할 수 있다. 본 발명에서는 누설성 정류 선형 유닛(a leaky Rectified Linear Unit, ReLU)과 누설성 축적-발화(LIF, Leaky Integrate-Fire) 스파이킹 뉴런 모델이 구현된다. 구체적으로, 도 1을 참조하면, SNN 프로세서는 글로벌 컨트롤러, 제1 인터페이스, 제2 인터페이 스, 글로벌 버퍼(140, gbuf), X-버스(150, X-bus), 부분합(psums, partial sums) 누산기(accumulator, 160), 처리 요소(PE, processing element) 어레이, 활성화 코어(activation function core(활성화 함수 코어), 180) 및 풀링 코어(Pooling core, 190)를 포함한다. SNN 프로세서는 어드밴스 페리페럴 버스(Advanced Peripheral Bus, APB) 슬레이브(slave) 프로토콜 및 어 드밴스 하이-퍼포먼스 버스(Advanced High-performance Bus, AHB) 마스터 프로토콜(master protocol)과 같은 어드밴스 마이크로컨트롤러 버스 아키텍처(AMBA, Advanced Microcontroller Bus Architecture) 프로콜을 지원 하는 AMBA 버스와 인터페이싱 한다. SNN 프로세서는 AMBA 버스를 통해 도시하지 않은 외부 메모리 (예, D램(RAM))에 액세스한다. 글로벌 컨트롤러는 SNN 프로세서 내의 일부 또는 모든 주변 구성들(120~190)의 전반적인 동작을 제어 한다. 더 구체적으로, SNN 프로세서 내의 일부 또는 모든 주변 구성들(120~190)의 전반적인 동작은 글로벌 컨트롤러에 포함된 유한 상태 머신(Finite State Machines, FSM)에 의해 제어된다. 제1 인터페이스는 AMBA 버스와 인터페이싱 하며, 예를 들면, 상기 APB 슬레이브 프로토콜에 따라 AMBA 버스와 인터페이싱 한다. 이 경우, 제1 인터페이스는 \"APB 인터페이스\"로 불릴 수 있다. 이러한 제1 인터페이스에 의해 글로벌 컨트롤러는 AMBA 버스와 인터페이싱 한다. 제2 인터페이스는 AMBA 버스와 인터페이싱 하며, 예를 들면, 상기 AHB 마스터 프로토콜에 따라 AMBA 버스와 인터페이싱 한다. 이 경우, 제2 인터페이스는 \"AHB 마스터 인터페이스\"로 불릴 수 있다. 이러 한 제2 인터페이스에 의해 글로벌 버퍼(140, gbuf)는 AMBA 버스와 인터페이싱 한다. 글로벌 버퍼(140, gbuf)는 메모리 컨트롤러, 제1 메모리 및 제2 메모리를 포함한다. 제1 메모리 는 메모리 컨트롤러의 제어에 따라 제2 인터페이스(130, AHB 마스터 인터페이스)를 통해 외부 메모리 (도시하지 않음)로부터 읽어온 입력 데이터를 저장한다. 여기서, 입력 데이터는 시냅스 전 뉴런 데이터(pre-synaptic neuron, ineu)와 시냅스 가중치(synapse weight)를 포함한다. 제2 메모리는 부분합 누산기(16 0)에 의해 누산된 부분합(psums: partial sums)에 대응하는 시냅스 후 뉴런 데이터(post-synaptic neuron, oneu)를 저장한다. 제1 메모리는, 예를 들면, 2개의 1024×128 SRAM(Static Random Access Memory)(2 bank of 1024×128 SRAM)와 2개의 512×128 SRAM(2 bank of 512×128 SRAM)으로 구성되며, 제2 메모리는 10개의 512×128 SRAM(2 bank of 512×128 SRAM)으로 구성될 수 있다. 따라서, 이러한 실시 예에서는 글로벌 버퍼(140, gbuf)는 2개의 1024×128 SRAM 2개와 12개의 512×128 SRAM로 구성된다. X-버스(150, X-bus)는 높은 처리량을 위해 높은 대역폭, 예를 들면, 256비트/사이클로, 제1 메모리에 저장 된 시냅스 전 뉴런 데이터(pre-synaptic neuron, ineu)와 시냅스 가중치(synapse weight)를 포함하는 입력 데 이터를 프로세싱 엘리먼트(PE, processing element) 어레이로 전달한다. PE 어레이는 복수의 PE들을 포함하며, 예를 들면, 복수의 PE들은 15×16개의 PE들로 구성될 수 있다. 도 2를 참조하여 각 PE에 대해 상세히 설명하면, 각 PE는, 도 2에 도시된 바와 같이, X-버스를 통해 제1 메모리로부터 읽어온 시냅스 전 뉴런 데이터(pre-synaptic neuron, ineu)를 위한 제1 버퍼, 시냅스 가중치(synapse weight)를 위한 제2 버퍼 및 MAC(multiply-and-accumulate) 연산기를 포함한다. 제1 및 제2 버퍼(172 및 174) 각각은, X-버스(150, X-bus)의 높은 대역폭, 예를 들면, 256비트/사이클을 지원하 기 위해, 256 비트 버퍼로 설계될 수 있다. MAC(multiply-and-accumulate) 연산기는 제1 및 제2 버퍼(172 및 174)로부터 입력되는 시냅스 전 뉴런 데 이터(pre-synaptic neuron, ineu)와 시냅스 가중치(synapse weight)에 대해 MAC 연산을 수행하여 부분합 부분 합(psums: partial sums)을 생성한다. MAC 연산기는 MAC 연산을 수행하기 위해 적어도 하나의 덧셈기와 적 어도 하나의 곱셈기를 포함하도록 설계될 수 있으며, MAC 연산은 16 비트 고정 소수점(16-bit fixed poin) MAC 연산일 수 있다. 다시 도 1을 참조하면, 부분합 누산기(160, psum ACCM)는 PE 어레이의 각 PE로부터 출력되는 부분합 (psums: partial sums)을 누적하기 위한 연산을 수행한다. 부분합 누산기(160, psum ACCM)에 의해 누적된 부분합(psums)은 글로벌 버퍼의 제2 메모리에 저장된 다. 이때, 부분합(psums)은 시냅스 후 뉴런(post-synaptic neuron, oneu)이 계산될 때까지 글로벌 버퍼 (제2 메모리)에 시냅스 후 뉴런 데이터(post-synaptic neuron, oneu)로서 고정적으로(stationary) 저장된 다. 활성화 코어(180 또는 활성화 함수 코어(activation function core))는 글로벌 버퍼에 액세스하여 글로벌 버퍼의 제2 메모리로부터 시냅스 후 뉴런(post-synaptic neuron, oneu) 데이터를 읽어오고, 읽어온 시냅스 후 뉴런(post-synaptic neuron, oneu) 데이터를 기반으로 멤브레인 데이터(membrane data 또는 membrane potential(막 전위 데이터))를 계산한다. 멤브레인 데이터(membrane data)의 계산은 LIF 스파이킹 뉴런 모델 연산(a leaky integrate-fire(LIF) spiking neuron model operation)에 의해 수행될 수 있다. LIF 스파이킹 뉴런 모델 연산은 전술한 수학식 1의 , 및 로 나타낼 수 있다. 계산된 멤브레인 데이터는 활성화 코어의 쓰기 권한을 통해 글로벌 버퍼의 제2 메모리에 저장되 며, 이때, 제2 메모리에 저장된 시냅스 후 뉴런(post-synaptic neuron, oneu) 데이터는 상기 계산된 멤브 레인 데이터로 대체된다. 이와 같이, 활성화 코어는 SNN 프로세서에 내장되며, LIF 스파이킹 뉴런 모델로 설계된다. 또한 활성 화 코어는 SNN 프로세서에 내장된 글로벌 버퍼에 액세스하는 방식으로 멤브레인 데이터를 계산 및 업데이트한다. 즉, 활성화 코어는 SNN 프로세서에 내장된 글로벌 버퍼에 대해 읽기 및 쓰기 권한을 갖는다. 이처럼, 멤브레인 데이터의 계산 및 업데이트가 SNN 프로세서 내부에서 수행되기 때문에, 멤브레인 데이터 를 계산 및 업데이트하기 위해 외부 메모리(도시하지 않음)로부터 관련 데이터(예, oneu)를 읽어오거나 계산된 멤브레인 데이터에 대응하는 스파이크 데이터를 외부 메모리에 저장하기 위한 외부 메모리 액세스를 최소화할 수 있다.한편, 풀링 코어는 풀링 레이어(pooling layer)의 동작을 지원한다. 도 3은 본 발명의 실시 예에 따른 SNN 프로세서의 동작 방법을 나타내는 흐름도이고, 도 4는 도 3에 도시된 동 작 방법에 따른 데이터 플로우를 나타내는 의사 코드(Pseudo code)이다. 도 3 및 4를 참조하면, 먼저, S310에서, 글로벌 컨트롤러가 제1 인터페이스(예를 들면, APB(Advanced Peripheral Bus) 인터페이스)를 통해 수신된 동작 신호에 따라 SNN 프로세서의 구성들(120~190)의 동작 시작을 명령한다. 이어, S320에서, 글로벌 버퍼 또는 글로벌 버퍼의 제1 메모리가, 메모리 컨트롤러의 제어 에 따라, 제2 인터페이스(예를 들면, AHB(Advanced High-performance Bus) 인터페이스)를 통해 외부 메모 리(예, dram)로부터 읽어온 입력 데이터를 저장한다. 여기서, 입력 데이터는 시냅스 전 뉴런 데이터(ineu) 및 시냅스 가중치(synapse)를 포함한다. 이어, S330에서, 글로벌 버퍼 또는 글로벌 버퍼의 제1 메모리에 저장된 입력 데이터(시냅스 전 뉴런 데이터(ineu) 및 시냅스 가중치(synapse))가 높은 대역폭(256비트/사이클)을 제공하는 X-bus를 통해 PE 어레이로 전달된다. 이어, S340에서, PE 어레이에서 입력 데이터(시냅스 전 뉴런 데이터(ineu) 및 시냅스 가중치(synapse))에 대해 MAC(Multiply-and-Accumulate) 연산을 수행하여 부분합(psum)을 계산한다. 이어, S350에서, 부분합 누산기(160, psum ACCM)가 상기 계산된 부분합(psum)을 누산하여 계산한 시냅스 후 뉴 런 데이터(post-synaptic neuron, oneu)를 글로벌 버퍼(또는 제2 메모리)에 저장한다. 이어, S360에서, 활성화 코어(180, 활성화 함수 코어)가 상기 시냅스 후 뉴런 데이터(post-synaptic neuron, oneu)를 기반으로 멤브레인 데이터(memb)를 계산하고, 이를 글로벌 버퍼 (또는 제2 메모리)에 저장한 다. 이에 따라 글로벌 버퍼 (또는 제2 메모리)의 상기 시냅스 후 뉴런 데이터(post-synaptic neuron, oneu)는 멤브레인 데이터(memb) 또는 멤브레인 데이터(memb)에 대응하는 스파이크 데이터(수학식 1의 , Sl[t]= '1' 또는 '0')로 업데이트되어 대체된다. 이어, S370에서, 글로벌 버퍼 또는 글로벌 버퍼의 메모리 컨트롤러가 업데이트된 멤브레인 데이 터(memb 또는 스파이크) 또는 멤브레인 데이터(memb)에 대응하는 스파이크(수학식 1의 , Sl[t]= '1' 또는 '0')를 최종 시냅스 후 뉴런 데이터(post-synaptic neuron, oneu)로서 제2 인터페이스(예를 들면, AHB 인터페 이스)를 통해 외부 메모리에 전달한다. 이상 설명한 바와 같이, 본 발명의 실시 예에 따른 데이터 플로우 따르면, SNN 프로세서가 멤브레인 데이 터(memb) 또는 멤브레인 데이터(memb)에 대응하는 스파이크 데이터를 외부 메모리에 전달하는 과정에서만 외부 메모리에 액세스하고, 멤브레인 데이터(memb) 또는 멤브레인 데이터(memb)에 대응하는 스파이크 데이터는 외부 메모리와의 액세스 없이 SNN 프로세서 내부에서 연산할 수 있다. 즉, 반복적으로 수행되는 멤브레인 데이 터(막 전위 값) 연산에 비례하여 높아지는 외부 메모리의 액세스 빈도에 의해 처리 속도가 감소하는 종래의 문 제점을 개선할 수 있다. 이상, 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시 킬 수 있음을 이해할 수 있을 것이다.도면 도면1 도면2 도면3 도면4"}
{"patent_id": "10-2023-0070444", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시 예에 따른 스파이킹 뉴럴 네트워크의 가속을 위한 장치의 구성도이다. 도 2는 도 1에 도시된 단일 PE(Processing Element)의 구성도이다. 도 3은 본 발명의 실시 예에 따른 SNN 프로세서의 동작 방법을 나타내는 흐름도이다. 도 4는 도 3에 도시된 동작 방법에 따른 데이터 플로우를 나타내는 의사 코드(Pseudo code)이다."}
