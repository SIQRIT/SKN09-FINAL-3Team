{"patent_id": "10-2021-0190049", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0099487", "출원번호": "10-2021-0190049", "발명의 명칭": "멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법", "출원인": "한국전자통신연구원", "발명자": "데브라니 데비"}}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "(a) 제1 에이전트의 정책과 제2 에이전트의 정책의 유사도를 계산하고, 상기 유사도를 이용하여 최종보상을 산출하는 단계; 및(b) 상기 제1 에이전트의 정책 및 제2 에이전트의 정책의 유사도를 계산한 결과를 이용하여 리플레이 버퍼에 대한 클러스터링을 수행하고, 클러스터 내 데이터에 대한 샘플링을 수행하는 단계를 포함하는 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 (a) 단계는 조건부 상호 정보를 통해 상기 정책의 유사도를 계산하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 (a) 단계는 다른 에이전트의 영향에 따른 보상 및 신경망에 의해 학습되는 파라미터의 곱과, 환경으로부터공통적으로 주어지는 보상을 합산하여 상기 최종보상을 산출하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 (a) 단계는 내적 보상을 추가적으로 사용하여 상기 최종보상을 산출하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 (b) 단계는 상기 제1 에이전트의 정책에 따른 행동을 하고, 상기 제2 에이전트의 히스토리 중 기설정된 개수의 최근 히스토리를 모니터링하여, 상기 정책의 유사도를 계산하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 (b) 단계는 쿨백-라이블러 발산을 이용하여 상기 정책의 유사도를 계산하는 것공개특허 10-2022-0099487-3-인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 (b) 단계는 계산된 쿨백-라이블러 발산값을 상기 리플레이 버퍼에 저장하고, 계층병합군집을 사용하여 총클러스터의 개수가 기설정 개수보다 작아지도록 상기 클러스터링을 수행하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 (b) 단계는 전체 데이터의 수와 클러스터 내의 데이터 수를 이용한 기준 및 상기 리플레이 버퍼의 쿨백-라이블러 발산값의 합과 클러스터 내의 쿨백-라이블러 발산값의 합을 이용한 기준 중 적어도 어느 하나의 기준에따라 상기 샘플링을 수행하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 에이전트 및 제2 에이전트의 정책을 수신하는 입력부; 상기 정책의 유사도를 비교하여 최종 보상을 산출하고, 경험 데이터에 대한 우선순위를 결정하는 프로그램이 저장된 메모리; 및상기 프로그램을 실행하는 프로세서를 포함하고, 상기 프로세서는 상기 정책의 유사도에 기반하여 상기 제2 에이전트의 영향에 따른 보상과 환경으로부터 공통적으로 주어지는 보상을 합산한 상기 최종 보상을 산출하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 장치."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 프로세서는 조건부 상호 정보를 통해 상기 정책의 유사도를 계산하고, 상기 최종 보상을 산출하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 장치."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 상기 프로세서는 상기 제1 에이전트의 정책에 따른 행동과, 상기 제2 에이전트의 히스토리에 대한 모니터링 결과를 이용하여 상기 정책의 유사도를 계산하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 장치."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "공개특허 10-2022-0099487-4-제11항에 있어서, 상기 프로세서는 상기 정책의 유사도를 계산한 결과를 이용하여 리플레이 버퍼에 대한 클러스터링을 수행하고,클러스터 내 데이터에 대한 샘플링을 수행하여 상기 경험 데이터에 대한 우선순위를 결정하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 장치."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 프로세서는 계층병합군집을 사용하여 총 클러스터의 개수가 기설정 개수보다 작아지도록 상기 클러스터링을 수행하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 장치."}
{"patent_id": "10-2021-0190049", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서, 상기 프로세서는 전체 데이터의 수와 클러스터 내의 데이터 수를 이용한 기준과, 상기 리플레이 버퍼의 쿨백-라이블러 발산값의 합과 클러스터 내의 쿨백-라이블러 발산값의 합을 이용한 기준 중 적어도 어느 하나의 기준에따라 상기 샘플링을 수행하는 것인 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 장치."}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법에 관한 것이다. 본 발명에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법은 (a) 제1 에 이전트의 정책과 제2 에이전트의 정책의 유사도를 계산하고, 유사도를 이용하여 최종보상을 산출하는 단계 및 (b) 제1 에이전트의 정책 및 제2 에이전트의 정책의 유사도를 계산한 결과를 이용하여 리플레이 버퍼에 대한 클 러스터링을 수행하고, 클러스터 내 데이터에 대한 샘플링을 수행하는 단계를 포함한다."}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법에 관한 것이다."}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래 기술에 따른 멀티에이전트 강화학습은 다른 에이전트의 행동에 의한 영향을 계산함에 있어서 신경망에만 의존하며, 다른 에이전트의 행동에 따른 최적의 정책을 학습에 반영하지 못하는 문제점이 있다. 또한, 종래 기술에 따른 멀티에이전트 강화학습은 리플레이 버퍼(메모리)의 크기가 커짐에 따라 소수의 샘플만 을 선택하므로, 에이전트가 다양한 시도를 할 수 없고 복잡한 문제를 푸는 경우 제한된 접근방법만을 찾는 한계 가 있다."}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 전술한 문제점을 해결하기 위해 제안된 것으로, 보상이 희박한 멀티에이전트 강화학습에서 제1 에이 전트가 제2 에이전트(즉, 다른 에이전트)의 정책을 고려하여 자신(제1 에이전트)의 정책을 학습하고, 이를 기반 으로 리플레이 버퍼(메모리)에 있는 경험 데이터를 추출하는 방법을 제공하는데 그 목적이 있다."}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법은 (a) 제1 에 이전트의 정책과 제2 에이전트의 정책의 유사도를 계산하고, 유사도를 이용하여 최종보상을 산출하는 단계 및(b) 제1 에이전트의 정책 및 제2 에이전트의 정책의 유사도를 계산한 결과를 이용하여 리플레이 버퍼에 대한 클 러스터링을 수행하고, 클러스터 내 데이터에 대한 샘플링을 수행하는 단계를 포함한다. 상기 (a) 단계는 조건부 상호 정보를 통해 상기 정책의 유사도를 계산한다. 상기 (a) 단계는 다른 에이전트의 영향에 따른 보상 및 신경망에 의해 학습되는 파라미터의 곱과, 환경으로부터 공통적으로 주어지는 보상을 합산하여 상기 최종보상을 산출한다. 상기 (a) 단계는 내적 보상을 추가적으로 사용하여 상기 최종보상을 산출한다. 상기 (b) 단계는 상기 제1 에이전트의 정책에 따른 행동을 하고, 상기 제2 에이전트의 히스토리 중 기설정된 개 수의 최근 히스토리를 모니터링하여, 상기 정책의 유사도를 계산한다. 상기 (b) 단계는 쿨백-라이블러 발산을 이용하여 상기 정책의 유사도를 계산한다. 상기 (b) 단계는 계산된 쿨백-라이블러 발산값을 상기 리플레이 버퍼에 저장하고, 계층병합군집을 사용하여 총 클러스터의 개수가 기설정 개수보다 작아지도록 상기 클러스터링을 수행한다. 상기 (b) 단계는 전체 데이터의 수와 클러스터 내의 데이터 수를 이용한 기준 및 상기 리플레이 버퍼의 쿨백-라 이블러 발산값의 합과 클러스터 내의 쿨백-라이블러 발산값의 합을 이용한 기준 중 적어도 어느 하나의 기준에 따라 상기 샘플링을 수행한다. 본 발명에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 장치는 제1 에이전 트 및 제2 에이전트의 정책을 수신하는 입력부와, 정책의 유사도를 비교하여 최종 보상을 산출하고, 경험 데이 터에 대한 우선순위를 결정하는 프로그램이 저장된 메모리 및 프로그램을 실행하는 프로세서를 포함하고, 프로 세서는 정책의 유사도에 기반하여 제2 에이전트의 영향에 따른 보상과 환경으로부터 공통적으로 주어지는 보상 을 합산한 최종 보상을 산출한다. 상기 프로세서는 조건부 상호 정보를 통해 상기 정책의 유사도를 계산하고, 상기 최종 보상을 산출한다. 상기 프로세서는 상기 제1 에이전트의 정책에 따른 행동과, 상기 제2 에이전트의 히스토리에 대한 모니터링 결 과를 이용하여 상기 정책의 유사도를 계산한다. 상기 프로세서는 상기 정책의 유사도를 계산한 결과를 이용하여 리플레이 버퍼에 대한 클러스터링을 수행하고, 클러스터 내 데이터에 대한 샘플링을 수행하여 상기 경험 데이터에 대한 우선순위를 결정한다. 상기 프로세서는 계층병합군집을 사용하여 총 클러스터의 개수가 기설정 개수보다 작아지도록 상기 클러스터링 을 수행한다. 상기 프로세서는 전체 데이터의 수와 클러스터 내의 데이터 수를 이용한 기준과, 상기 리플레이 버퍼의 쿨백-라 이블러 발산값의 합과 클러스터 내의 쿨백-라이블러 발산값의 합을 이용한 기준 중 적어도 어느 하나의 기준에 따라 상기 샘플링을 수행한다."}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 제2 에이전트의 정책이 제1 에이전트의 정책에 미치는 정도를 정량화하고, 영향이 큰 행동을 중심으로 학습을 수행함으로써 멀티에이전트 환경에서 우수한 학습 성능을 확보하며, 이를 리플레이 버퍼의 데 이터 샘플링에 적용하여 다양한 학습 데이터를 학습하는 효과가 있다."}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 이상에서 언급한 것들에 한정되지 않으며, 언급되지 아니한 다른 효과들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 전술한 목적 및 그 이외의 목적과 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있"}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "으며, 단지 이하의 실시예들은 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 발명의 목적, 구성 및 효과를 용이하게 알려주기 위해 제공되는 것일 뿐으로서, 본 발명의 권리범위는 청구항의 기재에 의해 정의 된다. 한편, 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한 다(comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성소자, 단계, 동작 및/또는 소자가 하나 이상의 다른 구성소자, 단계, 동작 및/또는 소자의 존재 또는 추가됨을 배제하지 않는다. 이하에서는, 당업자의 이해를 돕기 위하여 본 발명이 제안된 배경에 대하여 먼저 서술하고, 본 발명의 실시예에 대하여 서술하기로 한다. 강화학습은 인공지능 기술영역에서 지속적인 보상을 통해 어떤 일을 수행하도록 학습하는 방법이다. 하나의 에 이전트가 주어진 환경에서 자신의 보상을 최대화하는 행동 또는 행동순서를 학습하는 강화학습은 딥러닝의 출연 이후 다양한 연구들이 진행되어 왔다. 강화학습은 에이전트의 수에 따라 싱글에이전트 강화학습 및 멀티에이전트 강화학습으로 나눌 수 있는데, 강화 학습이 실제로 적용되는 로봇, 자동차, 전장 등 다양한 분야의 특성을 고려할 때, 다수의 에이전트에 대한 고려 는 필수적이다. 싱글에이전트 강화학습에서, 에이전트는 오직 환경과 상호작용하면서 동작한다. 반면, 멀티에이 전트 강화학습에서는 하나의 에이전트가 복수의 다른 에이전트 및 환경과 상호작용해야 한다. 종래 기술에 따른 강화학습 기술은 동시행동(joint actions) 확률을 계산하여 탐색을 효율적으로 하는 데 집중 한다. 그러나, 종래 기술에 따르면 각 에이전트는 자신이 실제로 결과에 기여했는지 여부와 관계없이, 환경으로 부터 동일한 보상을 받게 된다. 따라서, 에에전트 별로 결과에 대한 기여도에 따른 보상을 줄 수 없다는 문제가 있다. 멀티에이전트 강화학습에서 보상(reward)은 학습에 있어 매우 중요한 역할을 하지만, 방대한 학습영역과 각 에이전트들의 행동이 복합적으로 작용하여, 그 보상이 각 상태/행동에 대해 매우 드물게 주어진다. 이러한 문제를 해결하기 위해, 종래 기술에 따르면 LIIR(Learning Individual Intrinsic Reward)은 에이전트의 내적 보상이라는 개념을 도입하였다. 즉, 각 에이전트가 결과에 미치는 영향(Contribution)을 내적 보상의 형태 로 추가한다. 내적 보상(Intrinsic Reward)은 모든 에이전트에게 공통으로 환경으로부터 주어지는 외적 보상 (Extrinsic Reward)과 달리 각 에이전트별로 주어지는 보상을 의미한다. 종래 기술에 따르면, 에이전트의 내적 보상을 계산하고, 에이전트는 환경으로부터 얻어진 외적 보상과 개별적으로 생성한 내적 보상의 합으로 전체 보 상을 계산하게 된다. 이 때, 두 보상 간의 가중치는 별도로 설정되어야 한다. LIIR 알고리즘은 이중 수준 최적 화(Bi-level Optimization)를 수행하며, 외적 보상과 내적 보상의 합을 최대로 하는 정책의 파라미터를 일차적 으로 찾고, 그렇게 얻어진 정책의 파라미터를 기반으로 외적 보상만을 최대로 하는 개별 보상을 위한 인공 신경 망의 파라미터를 최적화한다. 내적 보상의 형태로 에이전트마다 개별적으로 보상을 부여하기 때문에 높은 성능 의 정책을 찾을 수 있고, 다양한 패턴의 정책을 찾을 수 있다. 이러한 접근방법은 보상이 매우 드물게 주어지는 멀티에이전트 강화학습 환경에서 잘 작동하는 것으로 알려졌으 나, 이는 다른 에이전트의 행동과 이로 인한 영향을 계산함에 있어서 신경망에만 의존하는 한계가 있고, 다른 에이전트의 행동에 따른 최적의 정책을 학습에 반영하지 못하는 한계가 있다. 전술한 문제점 외에, 종래 기술에 따른 멀티에이전트 강화학습은 리플레이 버퍼(메모리)에서 상태전이 데이터 샘플링과 관련되는 문제점이 있다. 리플레이 버퍼는 강화학습에 있어서 사용되는 샘플 데이터의 상관관계로 인해 학습이 급격하게 변하고 수렴 속 도가 느려지는 문제를 해결하기 위한 구성이다. 즉, 실행하면서 발생하는 상태전이 데이터를 바로 학습에 사용하지 않고, 일단 저장한 후에 사용함으로써 데이터 사이의 correlation을 줄인다. 이때 버퍼 상의 상태전이 데 이터는 랜덤 샘플링을 통해 추출되나, 이러한 방식은 방대한 데이터의 양으로 인해 수렴 속도가 늦다는 단점이 있다. 각 상태전이에 weight를 주는 방식이 제안되어, weight가 높은 샘플을 사용할 확률을 높임으로써 수렴속도를 개 선하고자 하였으나, 이 때 사용되는 TD(Temporal Difference) error는 현재 상태에서 보상과 다음 상태에서의 값의 차이가 가장 큰 샘플을 택함으로써 수렴속도를 높이는 방식을 사용한다. 그러나 이 방법은 리플레이 버퍼 의 크기가 커짐에 따라 소수의 샘플만을 선택하여 에이전트가 다양한 시도를 할 수 없게 하는 문제가 있다. 따 라서, 스타크래프트와 같이 복잡한 문제를 푸는 경우 제한된 접근방법만을 찾아내는 한계가 있다. 본 발명은 전술한 문제점을 해결하기 위해 제안된 것으로, 제2 에이전트(다른 에이전트)의 행동(정책)을 고려하 여 제1 에이전트(자신)의 정책을 학습하고, 다른 에이전트의 정책을 고려하여 리플레이 버퍼에서 샘플링되는 데 이터를 선택함으로써 다양한 경우를 탐색하고 효과적인 해를 찾는 것이 가능한 방법을 제안한다. 종래 기술에 따른 멀티에이전트 강화학습은 매우 큰 행동공간(Action Space)의 문제점이 있어, 다른 에이전트의 행동에 의해 변화되는 모든 상황을 환경에서 오는 것으로 인식한다. 본 발명의 실시예에 따르면, 최대의 보상을 줄 것으로 보이는 행동을 Greedy하게 수행하는 방식에서 벗어나, 다 른 에이전트의 정책이 자신에게 영향을 주는 행동을 중심으로 학습을 수행함으로써, 복잡한 문제를 효율적으로 해결하는 것이 가능한 효과가 있다. 또한, 본 발명의 실시예에 따르면, 과거 경험의 이용에 있어서도 쿨백-라이블러 발산(Kullback-Leibler Divergence)을 이용하여, 다른 에이전트의 정책에 의해 자신에게 영향을 주는 샘플들에 대해 학습을 수행하고, 클러스터링을 이용하여 희귀한 경우에 대한 학습도 수행하여 다양한 학습을 수행하는 것이 가능한 효과가 있다. 도 1 및 도 2는 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순 위 결정 방법의 동작을 도시한다. 전술한 바와 같이, 종래 기술에 따르면 다른 에이전트의 행동(정책)에 대한 영향을 고려하지 않았는데, 일반적 으로 멀티에이전트 강화학습에서는 복잡한 환경을 만들어내는 다른 에이전트의 영향이 더 중요하게 작용할 수 있다. 본 발명의 실시예에 따르면, 다른 에이전트가 정책에 미치는 영향을 보상의 형태로 적용하는 기술적 특징이 있 다. 본 발명의 실시예에 따르면, 서로 다른 에이전트(제1 에이전트, 제2 에이전트)의 정책을 비교하고, 서로 관계성 이 높은 정책에 대해 추가 보상(호기심)을 제공하여 보다 빠르고 다양한 학습을 수행한다. 서로 다른 두 에이전트에 있어서 정책의 유사함은 [수학식 1]과 같은 조건부 상호 정보(conditional mutual information)을 통해 계산된다. [수학식 1]"}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "[수학식 1]은 상태 에 condition된 에이전트 i 와 j 가 상태 에서의 조건부 상호 정보이다(st는 state at time을 의미함). 와 는 시간 t에서 에이전트 i와 j의 정책이다. 또한, 는 시간 에서 에이전트 j의 정책이다. 이를 반영한 최종보상은 [수학식 2]와 같다. [수학식 2] 여기서 최종보상은 이고, 환경에서 공통적으로 받는 보상은 , 그리고 다른 에이전트의 영향에 따른 보 상 은 신경망에 의해 학습되는 파라미터 와의 곱으로 표현 된다. 본 발명의 실시예에 따르면, 내적 보상으로 사용되는 를 추가적으로 사용하는 것이 가능하다. [수학식 2]를 참고하면, 두 에이전트의 정책이 서로 독립적일 때(즉, 서로 영향을 주지 못할 때), 환경에서 받 은 외적 보상만을 사용하고, 두 에이전트의 정책이 서로 영향을 받는 경우, 서로의 영향에 대해 더 빠르고 다양 하게 학습하기 위한 추가적인 보상을 제공함으로써, 다른 에이전트에서 오는 불확실성을 최소화한다. 본 발명의 실시예에 따르면, 리플레이 버퍼를 클러스터링하고 각 클러스터 내의 엔트리에 대해 우선순위를 결정 한다. 이 때, 클러스터링은 유사한 정책을 가진 데이터를 클러스터링하고, 클러스터별로 데이터를 추출함으로써, 다양 한 경우를 학습한다. 클러스터 내 엔트리의 우선순위는 클러스터 내에서 자신과 다른 정책을 가진 에이전트에 영향을 주는 샘플을 더 많이 사용함으로써, 다른 에이전트의 정책을 반영하여 학습한다. 제1 에이전트는 자신의 정책을 따른 행동을 하면서, 제2 에이전트의 최근 T개의 히스토리(Trajectory)를 모니터 링하고, 이를 바탕으로 각 에이전트간 정책에 대한 유사도를 계산한다. 이 때 유사도의 계산에는 쿨백-라이블러 발산(KL Divergence)을 사용한다. Trajectory _T = (s0; a0; ...; sT ; aT ) 에 대해 에이전트 i(제1 에이전트), j(제2 에이전트)간 쿨백-라이블 러 발산은 다음 [수학식 3]과 같이 계산된다. [수학식 3]"}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이때, 샘플 데이터에 대한 KL값은 에이전트들의 평균값, 최고값 등을 사용할 수 있으며, 계산된 KL값은 (Si,t, ai,t, ri,t+, KL) 형태로 리플레이 버퍼에 저장된다. 클러스터링은 온라인 알고리즘을 사용할 수 있는데, 본 발명의 실시예에 따르면 계층병합군집(Hierarchical Agglomerative Clustering)을 사용한다. 즉, 초기에 각 데이터는 하나의 클러스터가 되고, 총 클러스터의 수가 특정 개수의 클러스터보다 작아질 때까지, 클러스터는 지속적으로 병합된다. 클러스터의 병합에는 KL값을 사용하여 두 값의 차이가 가장 작은 클러스터를 우선적으로 병합하고, 클러스터 내 에 데이터가 복수일 경우는 거리의 평균값을 이용한다. 클러스터링이 완료되면 각 데이터를 이용하여 샘플링 값이 계산된다. 각 샘플 데이터의 KL값은 다음 [수학식 4]를 통해 샘플링 확률로 변환된다. [수학식 4]"}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이 때, m은 클러스터 사이즈이고, 는 0보다 큰 양수로 어떤 데이터도 샘플링 가능하도록 하기 위한 상수다. 이렇게 계산된 확률은 바로 사용될 수도 있고 바이어스(bias) 해소를 위해 중요도 샘플링(importance samplin g)을 사용할 수 있다. 샘플링 확률()은 [수학식 5]와 같다. [수학식 5]"}
{"patent_id": "10-2021-0190049", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "이때, β는 샘플링을 제어하는 파라메터가 된다. 본 발명의 실시예에 따른 리플레이 버퍼(메모리)의 데이터 선택은 다음의 두 단계로 구성된다. 먼저 클러스터를 선택하고, 그 다음 클러스터 내에서 데이터를 선택한다. 라운드로빈 방식에 따라 클러스터를 선택할 수 있고, 클러스터 내의 데이터 수를 기준으로 m/N 의 확률로, 또는 클러스터내의 KL값의 합을 기준으로 KLm/KLtotal샘플링을 수행한다. 여기서, N은 전체 데이터의 수이고 m은 클러스터 내의 데이터 수, KLm 과 KLtotal은 클러스터 내의 KL값의 합, 전 체 메모리의 KL값의 합이다. 도 2를 참조하면, 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선 순위 결정 방법은 제1 에이전트의 정책과 제2 에이전트의 정책의 유사도를 계산하고, 유사도를 이용하여 최종보 상을 산출하는 단계(S210) 및 제1 에이전트의 정책 및 제2 에이전트의 정책의 유사도를 계산한 결과를 이용하여 리플레이 버퍼에 대한 클러스터링을 수행하고, 클러스터 내 데이터에 대한 샘플링을 수행하는 단계(S220)를 포 함한다. S210 단계는 조건부 상호 정보를 통해 상기 정책의 유사도를 계산한다. S210 단계는 다른 에이전트의 영향에 따른 보상 및 신경망에 의해 학습되는 파라미터의 곱과, 환경으로부터 공 통적으로 주어지는 보상을 합산하여 상기 최종보상을 산출한다. S210 단계는 내적 보상을 추가적으로 사용하여 상기 최종보상을 산출한다. S220 단계는 상기 제1 에이전트의 정책에 따른 행동을 하고, 상기 제2 에이전트의 히스토리 중 기설정된 개수의 최근 히스토리를 모니터링하여, 상기 정책의 유사도를 계산한다. S220 단계는 쿨백-라이블러 발산을 이용하여 상기 정책의 유사도를 계산한다. S220 단계는 계산된 쿨백-라이블러 발산값을 상기 리플레이 버퍼에 저장하고, 계층병합군집을 사용하여 총 클러 스터의 개수가 기설정 개수보다 작아지도록 상기 클러스터링을 수행한다. S220 단계는 전체 데이터의 수와 클러스터 내의 데이터 수를 이용한 기준 및 상기 리플레이 버퍼의 쿨백-라이블 러 발산값의 합과 클러스터 내의 쿨백-라이블러 발산값의 합을 이용한 기준 중 적어도 어느 하나의 기준에 따라 상기 샘플링을 수행한다. 도 3은 종래 기술 및 본 발명의 실시예에 따른 테스트 결과를 도시한다. 종래 기술에 따른 멀티에이전트 강화학습은 에이전트 자신의 동기를 학습에 반영하거나, 다른 에이전트의 평균 행동을 가정하고 학습을 수행하한다. 반면, 본 발명의 실시예에 따르면 다른 에이전트의 정책이 자신의 정책에 영향을 미치는 정도를 정량화(조건부 상호정보)하여 영향이 큰 행동을 중심으로 학습을 수행함으로써, 다른 에이전트의 영향이 큰 멀티에이전트 환경 에서 우수한 학습성능을 확보하는 효과가 있다. 즉, 학습에 필요한 시간은 짧아지고, 학습의 결과 게임에서 승리하는 확률은 높아지는 장점이 있다. 본 발명의 실시예에 따르면, 리플레이 버퍼의 데이터 샘플링에도 해당 방법을 적용함으로써, 학습성능을 높이고 클러스터링을 이용하여 다양한 학습데이터를 학습하는 장점이 있다. 본 발명의 실시예에 따른 학습 성능은 스타크래프트 마이크로매니지먼드(Starcraft Micromanagement)환경을 이 용하여 확인하였다. 성능의 평가를 위해 각 시나리오별로 5번의 실험을 수행하였고, 그 평균값을 지표로 사용하였다. 도 3의 (a), (b), (c)는 각각 쉬운 시나리오(2질럿 3마린), 어려운 시나리오(20저글링 4맹독충), 매우 어려운 시나리오(1의료선, 3약탈자, 8마린)에서 학습 후 테스트하여 게임에서 승리하는 비율을 나타낸다. 도 3을 참조하면, 파란색 선은 본 발명의 실시예에 따른 조건부 상호정보 기반 학습 및 리플레이버퍼에 계층 병 합 군집을 이용한 결과이고, 오렌지색 선은 조건부 상호정보 기반 학습 및 종래 기술에 따른 TD Error기반 리플 레이 버퍼를 이용한 결과이고, 초록색 선은 조건부 상호정보 기반 학습 및 계층병합 리플레이 버퍼(클러스터 내 에서는 랜덤선택)만을 사용한 결과이다. 빨간색 선은 조건부 상호정보 기반 학습 및 쿨백-라이블러 발산 기반 리플레이 버퍼 우선 정책만을 이용한 결과 이고, 보라색 선은 조건부 상호정보 기반 학습만을 이용한 결과이며, 갈색 선은 종래 기술에 따른 LIIR을 이용 한 결과이다. 도 3을 참조하면, 모든 실험에서 본 발명의 실시예에 따르는 경우가 가장 높은 승률을 보였고, 가장 빠른 학습 속도 성능을 보이는 것을 확인할 수 있다. 한편, 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방 법은 컴퓨터 시스템에서 구현되거나, 또는 기록매체에 기록될 수 있다. 컴퓨터 시스템은 적어도 하나 이상의 프 로세서와, 메모리와, 사용자 입력 장치와, 데이터 통신 버스와, 사용자 출력 장치와, 저장소를 포함할 수 있다. 전술한 각각의 구성 요소는 데이터 통신 버스를 통해 데이터 통신을 한다. 컴퓨터 시스템은 네트워크에 커플링된 네트워크 인터페이스를 더 포함할 수 있다. 프로세서는 중앙처리 장치 (central processing unit (CPU))이거나, 혹은 메모리 및/또는 저장소에 저장된 명령어를 처리하는 반도체 장치 일 수 있다. 메모리 및 저장소는 다양한 형태의 휘발성 혹은 비휘발성 저장매체를 포함할 수 있다. 예컨대, 메모리는 ROM 및 RAM을 포함할 수 있다. 따라서, 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법은 컴퓨터에서 실행 가능한 방법으로 구현될 수 있다. 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법이 컴퓨터 장치에서 수행될 때, 컴퓨터로 판독 가능한 명 령어들이 본 발명에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법을 수행할 수 있다. 한편, 상술한 본 발명에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법 은 컴퓨터로 읽을 수 있는 기록매체에 컴퓨터가 읽을 수 있는 코드로서 구현되는 것이 가능하다. 컴퓨터가 읽을 수 있는 기록 매체로는 컴퓨터 시스템에 의하여 해독될 수 있는 데이터가 저장된 모든 종류의 기록 매체를 포함 한다. 예를 들어, ROM(Read Only Memory), RAM(Random Access Memory), 자기 테이프, 자기 디스크, 플래시 메 모리, 광 데이터 저장장치 등이 있을 수 있다. 또한, 컴퓨터로 판독 가능한 기록매체는 컴퓨터 통신망으로 연결 된 컴퓨터 시스템에 분산되어, 분산방식으로 읽을 수 있는 코드로서 저장되고 실행될 수 있다. 도 4는 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 장치를 도시한다. 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 장치는 제1 에이전트 및 제2 에이전트의 정책을 수신하는 입력부와, 정책의 유사도를 비교하여 최종 보상을 산출 하고, 경험 데이터에 대한 우선순위를 결정하는 프로그램이 저장된 메모리 및 프로그램을 실행하는 프로세 서를 포함하고, 프로세서는 정책의 유사도에 기반하여 제2 에이전트의 영향에 따른 보상과 환경으로 부터 공통적으로 주어지는 보상을 합산한 최종 보상을 산출한다. 상기 프로세서는 조건부 상호 정보를 통해 상기 정책의 유사도를 계산하고, 상기 최종 보상을 산출한다. 상기 프로세서는 상기 제1 에이전트의 정책에 따른 행동과, 상기 제2 에이전트의 히스토리에 대한 모니터 링 결과를 이용하여 상기 정책의 유사도를 계산한다. 상기 프로세서는 상기 정책의 유사도를 계산한 결과를 이용하여 리플레이 버퍼에 대한 클러스터링을 수행 하고, 클러스터 내 데이터에 대한 샘플링을 수행하여 상기 경험 데이터에 대한 우선순위를 결정한다. 상기 프로세서는 계층병합군집을 사용하여 총 클러스터의 개수가 기설정 개수보다 작아지도록 상기 클러스 터링을 수행한다. 상기 프로세서는 전체 데이터의 수와 클러스터 내의 데이터 수를 이용한 기준과, 상기 리플레이 버퍼의 쿨 백-라이블러 발산값의 합과 클러스터 내의 쿨백-라이블러 발산값의 합을 이용한 기준 중 적어도 어느 하나의 기 준에 따라 상기 샘플링을 수행한다. 한편, 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방 법은 컴퓨터 시스템에서 구현되거나, 또는 기록매체에 기록될 수 있다. 컴퓨터 시스템은 적어도 하나 이상의 프 로세서와, 메모리와, 사용자 입력 장치와, 데이터 통신 버스와, 사용자 출력 장치와, 저장소를 포함할 수 있다. 전술한 각각의 구성 요소는 데이터 통신 버스를 통해 데이터 통신을 한다. 컴퓨터 시스템은 네트워크에 커플링된 네트워크 인터페이스를 더 포함할 수 있다. 프로세서는 중앙처리 장치 (central processing unit (CPU))이거나, 혹은 메모리 및/또는 저장소에 저장된 명령어를 처리하는 반도체 장치 일 수 있다. 메모리 및 저장소는 다양한 형태의 휘발성 혹은 비휘발성 저장매체를 포함할 수 있다. 예컨대, 메모리는 ROM 및 RAM을 포함할 수 있다. 따라서, 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법은 컴퓨터에서 실행 가능한 방법으로 구현될 수 있다. 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법이 컴퓨터 장치에서 수행될 때, 컴퓨터로 판독 가능한 명 령어들이 본 발명에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법을 수행할 수 있다. 한편, 상술한 본 발명에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 방법 은 컴퓨터로 읽을 수 있는 기록매체에 컴퓨터가 읽을 수 있는 코드로서 구현되는 것이 가능하다. 컴퓨터가 읽을 수 있는 기록 매체로는 컴퓨터 시스템에 의하여 해독될 수 있는 데이터가 저장된 모든 종류의 기록 매체를 포함 한다. 예를 들어, ROM(Read Only Memory), RAM(Random Access Memory), 자기 테이프, 자기 디스크, 플래시 메 모리, 광 데이터 저장장치 등이 있을 수 있다. 또한, 컴퓨터로 판독 가능한 기록매체는 컴퓨터 통신망으로 연결 된 컴퓨터 시스템에 분산되어, 분산방식으로 읽을 수 있는 코드로서 저장되고 실행될 수 있다.도면 도면1 도면2 도면3 도면4"}
{"patent_id": "10-2021-0190049", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1 및 도 2는 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순 위 결정 방법의 동작을 도시한다. 도 3은 종래 기술 및 본 발명의 실시예에 따른 테스트 결과를 도시한다. 도 4는 본 발명의 실시예에 따른 멀티에이전트 강화학습에서 호기심 기반 탐색 및 경험 데이터 우선순위 결정 장치를 도시한다."}
