{"patent_id": "10-2023-0172110", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0083836", "출원번호": "10-2023-0172110", "발명의 명칭": "특히 지능형 증강 현실 애플리케이션을 위해 적어도 하나의 머신러닝 모델을 제공하는 컴퓨터", "출원인": "람블르 게엠베하", "발명자": "슈베르트 필립"}}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 머신러닝 알고리즘을 사용하는 프로세스에 사용하기 위한 적어도 하나의 머신러닝 모델을 제공하는 컴퓨터 구현 방법으로서,적어도 하나의 컴퓨팅 장치에 의해, 이미지 센서에 의해 캡처된 적어도 하나의 이미지를 수신하는 것을 포함하여 적어도 하나의 센서로부터 센서 데이터를 수신하는 단계; 및적어도 하나의 컴퓨팅 장치에 의해, 처리 모듈의 제1 모듈부터 제5 모듈까지, 하나 이상의 피드백 루프를 사용하여 일련의 처리 모듈을 차례대로 또는 반복적인 프로세스로 수행하는 단계를 포함하고,- 제1 모듈은 센서 데이터를 수신하고, 센서 데이터의 각각의 유입 소스를 결정하며, 메타 데이터 정보를 포함하는 소스 정보를 파싱하고, 센서 데이터를 복수의 표준화된 데이터 아이템으로 변환하며, 표준화된 데이터 아이템 중 적어도 일부를 메타 데이터 정보와 연관시키고, 원격 액세스에 적합한 데이터 웨어하우스에서 관련 메타 데이터 정보와 함께 표준화된 데이터 아이템 중 적어도 일부를 수집하는 것을 포함하며,- 제2 모듈은 데이터 웨어하우스에 액세스하고 데이터 웨어하우스의 표준화된 데이터 아이템당 적어도 하나의이미지 키프레임을 식별하며, 적어도 하나의 이미지 키프레임을 나타내는 추가 정보를 각각의 표준화된 데이터아이템과 연관시키는 것을 포함하고,- 제3 모듈은 하나 이상의 기초 모델에 의해 자동으로 생성된 하나 이상의 초기 어노테이션이 수행되는 어노테이션을 위해 휴먼 머신 인터페이스를 통해 휴먼 어노테이터에게 적어도 하나의 이미지 키프레임 및 각 데이터아이템의 관련 추가 정보를 제시하고, 휴먼 머신 인터페이스를 통해 어노테이터로부터 어노테이션을 수신하며적어도 하나의 어노테이션이 달린 데이터세트에 대한 해당 어노테이션 정보로 적어도 하나의 이미지 키프레임을보강하는 것을 포함하고,- 제4 모듈은 교정되지 않은 프레임에 대해 업데이트된 초기 어노테이션도 제공하기 위해 적어도 하나의 어노테이션이 달린 데이터세트를 사용하여 적어도 하나의 머신러닝 모델을 생성하고 업데이트하는 것 중 적어도 하나를 포함하며,- 제5 모듈은 적어도 하나의 머신러닝 알고리즘을 사용하는 하나 이상의 프로세스가 액세스하기 위해 작업별 머신러닝 모델로서 적어도 하나의 머신러닝 모델을 저장 장치에 업로드하는 것을 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 적어도 하나의 머신러닝 모델은 증강 현실 애플리케이션의 하나 이상의 프로세스에 의해 사용되도록 구성되고,센서 데이터는 웨어러블 컴퓨팅 장치의 사용자 관점에서 수집된 에고센트릭 센서 데이터를 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 또는 제2항에 있어서, 센서 데이터는 하나 이상의 이미지; 하나 이상의 비디오 스트림; 지리적 위치 데이터; 오디오 데이터; 적외선데이터; 수집기의 시선 데이터, 특히 각막 반사, 입체 기하학, 눈 방향 및/또는 움직임 데이터; 및 관성 측정데이터, 특히 가속도계, 자이로스코프 및/또는 자력계 데이터 중 하나 이상을 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 증강 현실 애플리케이션의 하나 이상의 프로세스가 적어도 하나의 머신러닝 모델을 사용하도록 구성되고, 증강공개특허 10-2024-0083836-3-현실 애플리케이션의 사용자가 착용한 하나 이상의 각각의 센서에 의해 데이터가 캡처되는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항 내지 제4항 중 어느 한 항에 있어서, 표준화된 데이터 아이템 중 적어도 일부를 메타 데이터 정보와 연관시키는 단계는 미디어 속성, 특히 비디오 또는 이미지 속성, 위치 정보, 스키마 및 버전 정보, 및 다양한 센서 입력 양식 또는 연결된 정보에 대한 하나 이상의 추가 데이터 채널 중 하나 이상을 포함하는 표준화된 데이터 아이템 중 적어도 일부에 각각의 메타 데이터컨테이너를 할당하는 단계를 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제5항 중 어느 한 항에 있어서, 제1 모듈은 적어도 하나의 규제 방식에 따라 특정 데이터 카테고리의 난독화, 특히 데이터 대상 얼굴, 차량 번호판 및/또는 음성 오디오 필터링의 블러링을 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 내지 제6항 중 어느 한 항에 있어서, 표준화된 데이터 아이템 중 적어도 일부를 메타 데이터 정보와 연관시키는 단계는 상황, 이벤트, 관심 객체 및/또는 위치 태그에 관한 메타 데이터를 포함하는 표준화된 데이터 아이템에 추가적인 메타 데이터 정보를 할당하는 단계를 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항 내지 제7항 중 어느 한 항에 있어서, 제1 모듈은 기능적 및 법적 작업 중 적어도 하나를 포함하는 작업 요구사항에 대해 수집된 데이터의 수동 확인과, 뷰에 있는 객체 카테고리를 통한 자동 작업 확인을 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제8항 중 어느 한 항에 있어서, 제3 모듈은 머신 러닝 모델에 의해 반환된 객체 인스턴스가 수동 개정 및 수정을 위한 시작점으로 사용되는 인스턴스 분할을 포함하여 하나 이상의 어노테이션을 수동으로 다듬는 하나 이상의 프로세스를 포함하는 컴퓨터구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제9항 중 어느 한 항에 있어서, 제3 모듈은 하나 이상의 객체 인스턴스에 머신 러닝 모델 출력에서 비롯되고 휴먼 어노테이터에게 어노테이션제안을 디스플레이하는 데 사용되는 하나 이상의 추정 카테고리가 할당되는 분류를 포함하여 하나 이상의 어노테이션을 수동으로 다듬는 하나 이상의 프로세스를 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항 내지 제10항 중 어느 한 항에 있어서, 어노테이션 제안, 어노테이션 정정 제안, 두드러진 객체 강조 및/또는 추정 오류 표시와 같은 가이드 신호를 제공함으로써 어노테이션이 더 확장되는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항 내지 제11항 중 어느 한 항에 있어서, 제3 모듈은 수동(2인 1조 원칙) 또는 자동으로 실행될 수 있는 높은 품질을 보장하기 위해 사람이 어노테이션을다는 동안 검증 단계를 포함하는 컴퓨터 구현 방법.공개특허 10-2024-0083836-4-청구항 13 제1항 내지 제12항 중 어느 한 항에 있어서, 제3 모듈은 어노테이션 오류가 발생할 가능성이 높은 이미지 프레임 또는 프레임 영역을 강조 표시하기 위한 안내와 함께 적어도 하나의 수동 개선 프로세스를 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1항 내지 제13항 중 어느 한 항에 있어서, 표준화된 데이터 아이템당 적어도 하나의 이미지 키프레임을 식별하는 단계는 콘트라스트가 거의 또는 전혀 없거나 선명하지 않은 객체를 포함하는 이미지 프레임을 제외하고 풍부한 컨텍스트를 가진 이미지 프레임을 승격시키는 단계를 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1항 내지 제14항 중 어느 한 항에 있어서, 표준화된 데이터 아이템당 적어도 하나의 이미지 키프레임을 식별하는 단계는 시선 추적, 위치 감지, 관성 측정데이터 및 컨텍스트 정보 중 하나 이상으로부터 추가 센서 정보를 파싱하는 단계를 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-0172110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "적어도 하나의 컴퓨터가 프로그램을 실행할 때, 적어도 하나의 컴퓨터가 제1항 내지 제15항 중 어느 한 항의 방법을 수행하게 하는 명령어를 포함하는 컴퓨터 프로그램."}
{"patent_id": "10-2023-0172110", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 적어도 하나의 머신러닝 알고리즘을 사용하는 프로세스에 사용하기 위한 적어도 하나의 머신러닝 모델 을 제공하는 컴퓨터 구현 방법으로서, 적어도 하나의 컴퓨팅 장치에 의해, 적어도 하나의 이미지를 포함하여 적 어도 하나의 센서로부터 센서 데이터를 수신하는 단계; 및 제1 모듈부터 제5 모듈까지, 하나 이상의 피드백 루프 (뒷면에 계속)"}
{"patent_id": "10-2023-0172110", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 특히 지능형 증강 현실 애플리케이션을 위한 데이터 파이프라인의 형태로 적어도 하나의 머신러닝 모 델을 제공하는 컴퓨터 구현 방법 및 이에 대응하는 컴퓨터 프로그램에 관한 것이다."}
{"patent_id": "10-2023-0172110", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "스마트 안경, 헤드셋 등 미래의 지능형 웨어러블은 증강 현실(AR) 및 가상 현실(VR)을 일상 생활에 유비쿼터스 하게 만들 계획이다. 이러한 장치에는 컴퓨터 생성 콘텐츠, 이미지, 비디오, 홀로그램, 오디오 등이 자연스러운 환경을 강화시키는 관점에서 사용자의 실제 세상의 장면 위에 놓이면서 사용자가 실시간으로 가상 및 현실 세계 와 상호 작용할 수 있도록 하는 고급 이미징 및 감지 기술이 포함되어 있다. 사용자를 위한 몰입형 AR 경험을 만들기 위해, 비디오 카메라와 같은 얼굴 또는 머리 장착 센서는 사용자의 실제 환경에서 자연스러운 장면, 예 를 들어, 이미지, 사운드는 물론 사용자의 움직임과 주의를 스캔하고 캡처한다. 이후 장치는 캡처된 장면을 스 캔하고 분석하여 실제 환경을 기준으로 사전 프로그래밍된 가상 요소를 중첩하거나 재생할 위치를 결정한다. 다 음으로, 가상 요소를 불러와 실제 장면에 실시간으로 겹쳐 놓거나 재생한다. 차세대 스마트 안경에는 엄청난 잠재력이 있다. 예를 들어, 이러한 장치는 AI 기술을 활용하여 시야에 있는 모 든 것을 인식하고 이러한 객체 및/또는 움직임을 이전에 수집했거나 인터넷을 통해 온라인으로 검색한 지식과 연결한다. 그러면 상황에 맞는 정보가 사용자 시야의 올바른 위치에 디스플레이되어 (또는 큰 소리로 재생되어) 사용자에게 잠재적인 위험을 경고하거나 잠재적인 오해의 위험을 줄일 수 있다. 예를 들어, HEIN, D. & RAUSCHNABEL, P., Augmented Reality Smart Glasses and Knowledge Management: A Conceptual Framework for Enterprise Social Networks. January 2016. 10.1007/978-3-658-12652-0_5 페이지 85를 참조하라. 그러나, 지능형 AR 기술은 아직 충분한 운영 성숙도에 도달하지 못했고 현재 진행이 저지되고 있다. 예를 들어, 사용자 관점에서 보는, 고급 \"에고센트릭\" 또는 1인칭 인식이 지능형 AR이 선호되는 기반이지만, 이를 위해서는 수많은 탐지, 분할 및 분류 작업을 자동으로 해결하는 알고리즘이 필요하다. 기술 상태가 끊임없이 발전함에 따 라, 이러한 작업을 미리 완전히 지정할 수는 없다. 하드웨어가 점점 정교해지면서 더욱 복잡한 머신러닝 모델을 배포할 수 있게 되었고 아키텍처 개선으로 예측 성능이 향상될 수 있지만, 둘 다 모델 생성 시 광범위한 조정이 필요하다. 현재 모델 생성에 대한 단일 접근 방식이 없고 훈련 데이터가 상황 및 업무별로 다르므로, 모델 배포가 반응성이 떨어지고 느리다. 또한 AI 엔진(예를 들어, 복잡한 처리 계산을 위한 컴퓨터 프로그램 또는 알고리즘의 일부)은 일반적으로 머신 러닝 모델을 생성하기 위해 정확하고 신뢰할 수 있는 훈련 데이터 스트림이 필요하다. 특히, 스마트 안경의 충 분한 가능성은 에고센트릭 관점에서 AI가 세상과 상호작용하도록 훈련함으로써만 실현될 수 있다. 이를 위해서 는 단일 또는 바람직하게는 다중 센서 양식, 예를 들어, (RGB 카메라, IR, LIDAR, 위치정보(GPS), 시선 및 관성 측정 장치(IMU)와 같은) 하나 또는 다수의 서로 다른 센서를 포함하는 이미지 센싱으로 구성된 에고센트릭 훈련 데이터가 필요하다. 지능형 AR 개발의 발전은 (다중)센서 에고센트릭 데이터 표준화 및 처리의 비효율성으로 인 해 방해를 받는다. 훈련 데이터 수집과 관련하여, 장애물에는 센서 변형, 파일 형식 차이, 개인 정보 보호법 준 수, 품질, 분석, 버전 관리 및 데이터 저장 문제가 포함된다. 효과적인 모델에는 고품질 데이터가 필요하기 때 문에 이러한 문제는 머신러닝 노력에 상당한 다운스트림 영향을 미친다. 예를 들어, WO 2019/245618 A 및 US 2022/107652 A1은 차량의 센서를 사용하여 캡처한 이미지를 수신하여 복수 의 구성요소 이미지들로 분해하는 자율 주행을 위한 데이터 파이프라인 및 딥러닝 시스템을 개시한다. 복수의 구성요소 이미지 중 각각의 구성요소 이미지는 인공신경망의 복수의 레이어 중 서로 다른 레이어에 서로 다른 입력으로 제공되어 결과를 결정한다. 인공신경망의 결과는 적어도 부분적으로 차량을 자율적으로 작동시키는 데 사용된다. US 10 691 943 B1은 다중 모드 센서 데이터를 기반으로 한 이미지에 어노테이션을 추가하는 것을 개시한다. 카 메라를 사용하여 캡처된 이미지 데이터나 기타 데이터는 열화상 카메라, 방사선 카메라, 자외선 카메라 등 다른 센서를 사용하여 캡처된 데이터를 기준으로 분류될 수 있으며, 이러한 센서는 장면에서 동시에 데이터를 캡처하 여 그 안에 있는 하나 이상의 객체를 감지하도록 각각의 캡처된 데이터를 처리할 수 있다. 데이터가 하나 이상 의 관심 객체를 묘사할 확률은 다양한 방식으로 작동하는 보정된 센서에서 캡처된 데이터를 기반으로 향상될 수 있다. 관심 객체가 충분한 신뢰도로 감지되는 경우, 객체가 감지된 어노테이션이 달린 데이터는 객체 또는 유사 한 객체를 인식하거나 다른 목적을 위해 하나 이상의 분류기를 훈련하는 데 사용될 수 있다. US 2021/390702 A1은 머신러닝(ML) 어노테이션 모델 재학습을 트리거하기 위한 시스템 및 방법을 개시한다. 이 방법에는 제1 모양을 둘러싸는 거친 경계가 있는 제1 모양의 정의되지 않은 변형이 있는 미가공 데이터 이미지 를 수용하는 어노테이션 모델이 포함된다. 어노테이션 모델은 제1 모양을 둘러싸는 다듬어진 경계를 형성하는 어노테이션 표시가 있는 어노테이션이 달린 이미지를 생성한다. 에이전트 사용자 인터페이스(UI)는 다듬어진 경 계를 수정하여 수정된 이미지를 제공한다. 중개 소프트웨어 애플리케이션은 어노테이션이 달린 이미지를 해당 수정된 이미지와 비교하고 어노테이션이 달린 이미지를 수정된 이미지와 비교한 것에 응답하여 제1 모양에 대한 자원 분석을 재학습하는 어노테이션 모델을 제공한다. 그러나, 위에서 언급한 바와 같이 에고센트릭 데이터가 선호되는 지능형 증강 현실을 위한 (다중)센서 데이터 처리 문제를 충분히 해결하기에는 이러한 기술들이 적합하지 않다."}
{"patent_id": "10-2023-0172110", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "따라서, 캡쳐된 센서 데이터로부터 주요 객체 정보를 극대화하고, 딥러닝 분석을 위한 딥러닝 네트워크에 보다 높은 수준의 안정적인 문맥 정보를 제공할 수 있는 맞춤형 컴퓨터 구현 방법이 필요하다."}
{"patent_id": "10-2023-0172110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시는 첨부된 청구범위에 따른 컴퓨터 구현 방법 및 컴퓨터 프로그램에 관한 것이다. 실시예는 종속항에 개 시되어 있다. 일 양태에 따르면, 적어도 하나의 머신러닝 알고리즘을 사용하는 프로세스에 사용하기 위한 적어도 하나의 머신 러닝 모델을 제공하는 컴퓨터 구현 방법으로서, 적어도 하나의 컴퓨팅 장치에 의해, 이미지 센서에 의해 캡처된 적어도 하나의 이미지를 수신하는 것을 포함하 여 적어도 하나의 센서로부터 센서 데이터를 수신하는 단계; 및 적어도 하나의 컴퓨팅 장치에 의해, 처리 모듈의 제1 모듈부터 제5 모듈까지, 하나 이상의 피드백 루프를 사용 하여 일련의 처리 모듈을 차례대로 또는 반복적인 프로세스로 수행하는 단계를 포함하고,- 제1 모듈은 센서 데이터를 수신하고, 센서 데이터의 각각의 유입 소스를 결정하며, 메타 데이터 정보를 포함 하는 소스 정보를 파싱하고, 센서 데이터를 복수의 표준화된 데이터 아이템으로 변환하며, 표준화된 데이터 아 이템 중 적어도 일부를 메타 데이터 정보와 연관시키고, 원격 액세스에 적합한 데이터 웨어하우스에서 관련 메 타 데이터 정보와 함께 표준화된 데이터 아이템 중 적어도 일부를 수집하는 것을 포함하며, - 제2 모듈은 데이터 웨어하우스에 액세스하고 데이터 웨어하우스의 표준화된 데이터 아이템당 적어도 하나의 이미지 키프레임을 식별하며, 적어도 하나의 이미지 키프레임을 나타내는 추가 정보를 각각의 표준화된 데이터 아이템과 연관시키는 것을 포함하고, - 제3 모듈은 하나 이상의 기초 모델에 의해 자동으로 생성된 하나 이상의 초기 어노테이션이 수행되는 어노테 이션을 위해 휴먼 머신 인터페이스를 통해 휴먼 어노테이터에게 적어도 하나의 이미지 키프레임 및 각 데이터 아이템의 관련 추가 정보를 제시하고, 휴먼 머신 인터페이스를 통해 어노테이터로부터 어노테이션을 수신하며 적어도 하나의 어노테이션이 달린 데이터세트에 대한 해당 어노테이션 정보로 적어도 하나의 이미지 키프레임을 보강하는 것을 포함하고, - 제4 모듈은 교정되지 않은 프레임에 대해 업데이트된 초기 어노테이션도 제공하기 위해 적어도 하나의 어노테 이션이 달린 데이터세트를 사용하여 적어도 하나의 머신러닝 모델을 생성하고 업데이트하는 것 중 적어도 하나 를 포함하며, - 제5 모듈은 적어도 하나의 머신러닝 알고리즘을 사용하는 하나 이상의 프로세스가 액세스하기 위해 작업별 머 신러닝 모델로서 적어도 하나의 머신러닝 모델을 저장 장치에 업로드하는 것을 포함하는 컴퓨터 구현 방법이 개 시된다. 따라서, 본 발명의 양태는 분석 기술을 위한 소프트웨어 어셈블리 라인 또는 모델 공장으로서 역할을 하여, 다 양한 모델 아키텍처 및 목표로 삼은 특정 어노테이션 작업의 신속한 개발 및 적용을 가능하게 한다. 데이터 파 이프라인은 센서 에고센트릭 데이터를 수집, 표준화, 익명화, 변환, 버전 지정, 강화하고 다운스트림 AI 훈련, 조정 및 추론에 전송하는 데 필요한 처리를 신속하게 처리하고 반자동화하고 확장할 수 있다. 에고센트릭 데이 터에 대한 용량, 다양성 및 속도 요구 사항이 증가함에 따라, 지능형 AR 개발자가 클라우드, 하이브리드 클라우 드 및 엣지 컴퓨팅 환경 내에서 지리적으로나 성능 측면에서 모두 확장할 수 있는 데이터 파이프라인 방안을 찾 는 것이 점점 더 중요해질 것이다. 예를 들어, https://www.technologyreview.com/2021/12/06/1040716/evolution-of-intelligent-data-pipelines/에서 SCHMARZO, BILL: Evolution of Intelligent Data Pipelines. MIT Technology Review Online. December 6, 2021, Accessed on 25 November 2022를 참조하라. 일 양태에서, 본 발명의 향상된 기능은 처리 모듈의 결합 및 상호 작용에 의해 달성된다. 특히, 본 발명은 처리 모듈의 집합체로서, 각 모듈은 이를 통과하는 데이터에 대해 특정 작업 또는 일련의 작업을 수행한다. 모듈은 직렬로 구성된다. 예를 들어, 제1 모듈은 미가공 데이터 스트림을 수신하고 이를 처리한 후 그 출력을 순서에 따라 다음 모듈에서 사용할 수 있도록 한다. 예를 들어, US 2005/0154696 A1, in particular Col. 1 [003]을 참조하라. 그러나, 프로세스는 하나 이상의 피드백 루프를 통해 반복될 수도 있다. 처리 모듈은 함께 머신러닝 모델 생성을 위한 데이터 처리 시간과 오류를 줄이다. 본 발명은 증강 현실을 넘어서 자율 주행과 같은 다른 사용 분야에도 적용될 수 있다. 실시예에 따르면, 위에서 설명한 과제 외에도, 머신러닝 훈련을 위한 에고센트릭 데이터 처리와 관련하여 다음과 같은 과제를 해결할 수 있다: - 수집된 데이터가 매우 다양하며 여러 도메인에 걸쳐 있다. - 센서 장비 카메라는 가령 조명 변화를 처리하여 이미지 해상도와 품질이 달라지는 등 다양한 기능을 갖추고 있다. - 비디오 데이터 스트림은 빠르고 빈번한 위치 및 주의 변화를 보여준다(예를 들어, 머리 움직임으로 인해 모션 블러가 발생할 수 있다). - 예를 들어, 자율 주행을 위한 데이터 캡처(제한된 개체 카테고리가 있는 정적인, 자동차 장착형 카메라)와 달 리 에고센트릭 데이터 캡처는 사전 정의된 규칙 세트(광범위한 객체 카테코기가 있는 동적인, 머리 장착형 센서 장비)를 따르지 않는다.- 데이터 개인 정보 보호 규정은 데이터 사용 및 보유를 방해한다. 실시예에 따르면, 적어도 하나의 머신러닝 모델은 증강 현실 애플리케이션의 하나 이상의 프로세스에 의해 사용 되도록 구성되고, 센서 데이터는 웨어러블 컴퓨팅 장치의 사용자의 관점에서 수집된 에고센트릭 센서 데이터를 포함한다. 실시예에 따르면, 센서 데이터는 하나 이상의 센서에 의해 제공되는 다중 센서 데이터이다. 예를 들어, AR 애플리케이션 없이 웨어러블 하드웨어 장치(예를 들어, \"Tobii Pro 스마트 안경\")를 통해 데이터 가 수집될 수 있다. 그러나, 다른 실시예에 따르면, 웨어러블 하드웨어 장치에서 실행되는 AR 애플리케이션은 데이터 파이프라인에 삽입할 에고센트릭 센서 데이터를 수집하는 동시에 AR 경험을 제공할 수 있다. 실시예에 따르면, 센서 데이터는 하나 이상의 이미지; 하나 이상의 비디오 스트림; 지리적 위치 데이터; 오디오 데이터; 적외선 데이터; 수집기의 시선 데이터(특히 각막 반사, 입체 기하학, 눈 방향 및/또는 움직임); 및 관 성 측정 데이터( 특히 가속도계, 자이로스코프 및/또는 자력계 데이터) 중 하나 이상을 포함한다. 실시예에 따르면, 증강 현실 애플리케이션의 하나 이상의 프로세스가 적어도 하나의 머신러닝 모델을 사용하도 록 구성되고, 증강 현실 애플리케이션의 사용자가 착용한 하나 이상의 각각의 센서에 의해 데이터가 캡처된다. 일 실시예에 따르면, 표준화된 데이터 아이템 중 적어도 일부를 메타 데이터 정보와 연관시키는 단계는 미디어 속성, 특히 비디오 또는 이미지 속성, 위치 정보, 스키마 및 버전 정보, 및 다양한 센서 입력 양식 또는 연결된 정보에 대한 하나 이상의 추가 데이터 채널 중 하나 이상을 포함하는 표준화된 데이터 아이템 중 적어도 일부에 각각의 메타 데이터 컨테이너를 할당하는 단계를 포함한다. 실시예에 따르면, 연결된 정보는 반드시 장치로부터 의 센서 정보일 필요가 없을 수 있는 임의의 정보이거나 반드시 장치로부터 나올 필요는 없는 데이터 확인 로그, 법적 메타데이터 또는 임의의 수반 데이터와 같은 어노테이션 정보일 수 있다. 실시예에 따르면, 적어도 하나의 규제 방식에 따라 특정 데이터 카테고리의 난독화, 특히 데이터 대상 얼굴, 차 량 번호판 및/또는 음성 오디오 필터링의 블러링을 포함한다. 일 실시예에 따르면, 표준화된 데이터 아이템 중 적어도 일부를 메타 데이터 정보와 연관시키는 단계는 상황, 이벤트, 관심 객체 및/또는 위치 태그에 관한 메타 데이터를 포함하는 표준화된 데이터 아이템에 추가적인 메타 데이터 정보를 할당하는 단계를 포함한다. 실시예에 따르면, 제1 모듈은 제1 모듈은 기능적 및 법적 작업 중 적어도 하나를 포함하는 작업 요구사항에 대 해 수집된 데이터의 수동 확인과, 뷰에 있는 객체 카테고리를 통한 자동 작업 확인을 포함한다. 예를 들어, 작 업 요구 사항/검증은 (예를 들어, 얼굴 인스턴스에 대한 자동 확인이 수행되고 법적 동의 수와 비교되는 경우) 단순한 기능 확인 이상의 것일 수 있다. 실시예에 따르면, 제3 모듈은 머신 러닝 모델에 의해 반환된 객체 인스턴스가 수동 개정 및 수정을 위한 시작점 으로 사용되는 인스턴스 분할을 포함하여 하나 이상의 어노테이션을 수동으로 다듬는 하나 이상의 프로세스를 포함한다. 실시예에 따르면, 제3 모듈은 하나 이상의 객체 인스턴스에 머신 러닝 모델 출력에서 비롯되고 휴먼 어노테이터 에게 어노테이션 제안을 디스플레이하는 데 사용되는 하나 이상의 추정 카테고리가 할당되는 분류를 포함하여 하나 이상의 어노테이션을 수동으로 다듬는 하나 이상의 프로세스를 포함한다. 실시예에 따르면, 어노테이션 제안, 어노테이션 정정 제안, 두드러진 객체 강조 및/또는 추정 오류 표시와 같은 가이드 신호를 제공함으로써 어노테이션이 더 확장된다. 실시예에 따르면, 제3 모듈은 수동(2인 1조 원칙) 또는 자동으로 실행될 수 있는 높은 품질을 보장하기 위해 사 람이 어노테이션을 다는 동안 검증 단계를 포함한다. 예를 들어, 유효성 검사는 QA 검토(아래 참조)의 일부로 간주되는 경우가 많지만, 사람이 어노테이션을 추가하는 동안 품질을 개선하는 데 도움이 될 수도 있다. 일 실시예에 따르면, 제3 모듈은 어노테이션 오류가 발생할 가능성이 높은 이미지 프레임 또는 프레임 영역을 강조 표시하기 위한 안내와 함께 적어도 하나의 수동 개선 프로세스를 포함한다. 실시예에 따르면, 표준화된 데이터 아이템당 적어도 하나의 이미지 키프레임을 식별하는 단계는 콘트라스트가 거의 또는 전혀 없거나 선명하지 않은 객체를 포함하는 이미지 프레임을 제외하고 풍부한 컨텍스트를 가진 이미 지 프레임을 승격시키는 단계를 포함한다.실시예에 따르면, 표준화된 데이터 아이템당 적어도 하나의 이미지 키프레임을 식별하는 단계는 시선 추적, 위 치 감지, 관성 측정 데이터 및 컨텍스트 정보 중 하나 이상으로부터 추가 센서 정보를 파싱하는 단계를 포함한 다. 일 실시예에 따르면, 표준화된 데이터 아이템 중 적어도 일부를 메타 데이터 정보와 연관시키는 단계는 표준화 된 타임 스탬프를 각 센서 데이터, 특히 생성 날짜 시간 및/또는 수정 날짜 시간에 할당하는 단계를 포함한다. 예를 들어, 센서 데이터(예를 들어, 시선이 있는 비디오 프레임)를 동기화하는 맥락에서 타임스탬프가 바람직하 다. 추가 양태에 따르면, 적어도 하나의 컴퓨터가 프로그램을 실행할 때, 적어도 하나의 컴퓨터가 본 명세서에 기술 된 바와 같이 본 발명에 따른 방법을 수행하게 하는 명령어를 포함하는 컴퓨터 프로그램이 제공된다."}
{"patent_id": "10-2023-0172110", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 내용에 포함됨."}
{"patent_id": "10-2023-0172110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이제 도면을 참조하여 본 발명의 양태를 더 자세히 설명할 것이다. 본 발명은 주로 예시적인 방식으로 본 발명 의 특정 양태을 설명하기 위해 제공되는 개시된 실시예에 국한되지 않는다는 점에 유의해야 한다. 본 발명의 하 나 이상의 실시예에 대한 상세한 설명이 본 발명을 예시하는 첨부 도면과 함께 아래에 제공된다. 본 발명은 이 러한 실시예와 관련하여 설명되지만, 본 발명은 임의의 실시예에 국한되지 않는다. 명확히 하기 위해, 본 발명 이 불필요하게 모호해지지 않도록 당업자에게 알려진 기술 자료나 용어는 상세히 설명하지 않을 것이다. 본 명세서에서 사용되는 머신러닝(ML)은 해당 분야에서 일반적으로 알려진 용어로 이해된다. ML과 관련된 기술 은 딥러닝(DL), 인공 지능(AI) 등으로도 지정된다. ML은 머신러닝 또는 딥러닝을 기반으로 하는 다양한 형태의 알고리즘을 모두 설명하는 용어로 사용된다. 이는 이미지 분류, 객체 감지 또는 센서, 작업 및/또는 프로세스 데이터를 해석하는 기타 방법일 수 있다. 예를 들어, 인공신경망과 같은 머신러닝 모델을 이용하여 딥러닝 분석 을 수행한다. 머신러닝 모델은 AI 또는 ML 알고리즘에서 사용할 준비가 된 모델로 이해되며, 예를 들어 인공 신경망이다. 컨텍스트 또는 컨텍스트 정보는 적절한 머신러닝 모델을 생성하기 위해 ML 프로세스와 관련된 상황, 사용 사례, 환경 및/또는 의도를 나타내는 정보 모음으로 여기에서 사용된다. 데이터는 다양한 유형의 하나 이상의 센서를 사용하여 감지할 수 있는 이미지, 객체, 환경, 동작, 제스처 또는 기타 일반적인 (시각적, 수치적, 청각적 등) 특징 중 하나 이상을 의미한다. 프레임은 단일 정지 이미지에 사용 되는 기술 분야에서 알려진 일반적인 용어이다. 키프레임은 예를 들어 특정 임계값을 초과하는 콘트라스트와 같 은 하나 이상의 속성을 갖는 프레임이며, 이는 여기에 설명된 본 발명의 양태에 따른 추가 절차를 위해 복수의 프레임 중에서 선택하기에 적합하다. 예를 들어, 비디오의 경우, 비디오의 키프레임은 가령 일반적으로 비디오 의 정확하거나 가장 정확한 표현을 제공하는 프레임 또는 가장 작은 프레임 세트를 목표로 한다. 어노테이션 또는 어노테이션이 달린 데이터는 ML 기반 프로세스 또는 해당 프로세스의 구성요소를 생성하는 데 사용할 수 있도록 하는 구성을 가진 데이터를 의미한다. 특히, 어노테이션 또는 어노테이션이 달린 데이터는 예 를 들어 머신러닝 모델을 훈련하는 데 사용되는 데이터에 대한 기계 판독 가능 정보를 포함한다. 일반적으로, 이는 머신에서 추가로 처리될 수 있는 포맷으로 캡처된 데이터에 대한 설명(어노테이션)으로 구성된다. 잠재적인 구현에서, 컴퓨팅 장치 및/또는 일부 데이터 소스(예를 들어, 하나 이상의 센서, 일반적으로 도 1 에서 데이터 소스라고 함)는 소프트웨어 및/또는 하드웨어로 개별적으로 또는 분산된 방식으로 로컬 장치,예를 들어 모바일 또는 웨어러블 컴퓨터 시스템 및/또는 로컬 네트워크 및/또는 인터넷을 통해 액세스 가능한 하나 이상의 서버 컴퓨터의 하나 이상의 마이크로프로세서와 같은 임의의 적절한 처리 장치에서 구현될 수 있다. 달리 명시하지 않는 한, 작업을 수행하도록 구성되는 것으로 설명된 프로세서, 메모리, 스토리지 장치등 과 같은 구성요소는 주어진 시간에 작업을 일시적으로 수행하도록 구성되는 일반적인 구성요소 또는 작업을 수 행하기 위해 제조되는 특정 구성요소로 구현될 수 있다. 본 명세서에 사용된 바와 같이, 프로세서, 처리 장치 또는 컴퓨팅 장치라는 용어는 각각 컴퓨터 프로그램 명령과 같은 데이터를 처리하도록 구성된 하나 이상의 장치, 회로, 마이크로프로세서, 서버 및/또는 처리 코어를 말한다. 본 발명의 양태를 적용할 때, (비배타적인) 잠재적인 AR 적용 예는 다음과 같다: 응용 프로그램 1: 운동 중 피트니스 장비와 인체에 대한 비디오, 시선 및 IMU 데이터는 인체 운동 위치, 인간 움직임 및 상황별 정보(예를 들어, 배경 시각 및 청각 \"노이즈\")의 스펙트럼을 인식하기 위해 처리된다. 최종 사용자 앱 제작자는 이를 활용하여 클럽 회원을 위해 피트니스 스튜디오에서 제공하는 운동 연습의 올바른 실행 을 추적할 수 있다. 가령, 사용자가 착용한 스마트 안경과 같은 소비자 기기가 운동을 시작하기 전에 거울 앞에 서 켜져 있다. 애플리케이션 2: 음성 설명과 함께 음식, 사람, 주방 도구 및 식사 준비에 대한 비디오, IMU, 시선 및 오디오 데이터가 요리 도구 및 가전제품, 음식 카테고리, 요리 활동 및 상황별 정보를 감지하기 위해 처리된다. 최종 사용자 애플리케이션은 이러한 모델을 활용하여 취미 요리사에게 식사 준비 방법을 안내할 수 있다. 이를 위해, 스마트 안경을 통해 최종 사용자에게 표시되는 일련의 비디오에는 장치를 통해 재생되는 음성 지침이 함께 수반 된다. 애플리케이션 3: 가구, 사람 및 조립 활동의 비디오, IMU, 시선 및 오디오 데이터가 가구 및 하드웨어 카테고리, 사람의 움직임, 조립 작업 및 상황별 정보를 감지하기 위해 처리된다. 최종 사용자 애플리케이션은 사용자에게 가구 조립 방법을 안내할 수 있다. 사용하는 동안, 스마트 안경의 월드 페이싱 카메라가 활성화되어 사용자 설명서의 관련 단계를 감지하고 가상 콘텐츠가 스마트 안경에 겹쳐져 다음 단계에 대한 안내를 제공한다. 시선 추적은 사용자가 올바른 단계에 집중하고 있는지 이해하는 데 사용된다. 애플리케이션 4: 인간과 가전제품 작동 및 서비스의 비디오, IMU, 시선, 오디오 데이터가 가전제품 부품, 인간 움직임, 유지보수 활동 및 상황별 정보를 감지하기 위해 처리된다. 최종 사용자 애플리케이션은 중첩된 가상 정 보를 통해 자동 커피 머신 물때 제거, 오븐 조명 교체, 에어프라이어 청소 등 서비스 및 유지 관리 작업을 수행 하는 방법을 사용자에게 안내할 수 있다. 월드 페이싱 카메라를 사용하여 지원 엔지니어에게 전화해야 할 시기 를 감지할 수 있다. 오류 발생 시 추가 비디오, IMU, 시선 및 오디오 데이터를 기록하여 자동화된 지침을 개선 할 수 있다. 겹쳐진 비디오는 Q&A 사용자 매뉴얼과 사용 설명서를 향상시킬 수 있다. 애플리케이션 5: 외부 풍경 및 인간과 외부 환경의 상호 작용에 대한 비디오, GPS, IMU 및 시선 데이터가 인간 의 움직임 및 상황별 정보로 위치 정보를 감지하고 자전거 타는 사람의 관점에서 야생 동물, 고르지 않은 지형, 사람 및 교통과 같은 환경 위험을 인식하기 위해 처리된다. 가능한 최종 사용자 애플리케이션의 경우, 시선 데 이터는 자전거를 타는 동안 안전 기능을 제공하는 데 사용된다. 이러한 안전 기능에는 실시간으로 눈에 띄지 않 는 우회로, 위험, 자동차 및 보행자에 대한 중첩된 이미지 또는 오디오 경고가 포함될 수 있다. 적어도 하나의 컴퓨팅 장치 또는 컴퓨터 시스템 또는 컴퓨터 기기(개략적으로 1로 표시됨)에 의해 구현되는 본 발명의 양태에 따른 방법은 또한 아래에 더 자세히 설명된 바와 같이 도 1에 도시된 적어도 하나의 센서에 포함된 이미지 센서에 의해 캡처된 적어도 하나의 이미지를 수신하는 것을 포함하여 하나 이상의 센서(멀티센서 데이터라고도 함)로부터의 실시예에 따른, 적어도 하나의 컴퓨팅 장치에 의해 하나 이상의 센서로부터 센서 데이터를 수신하는 단계를 포함한다. 하나 이상의 컴퓨팅 장치가 도면의 실시예에 따라 도시되고 아래에서 더 자세히 설명된 바와 같이 처리 모듈 의 제1 모듈부터 제5 모듈까지 연속적으로 또는 하나 이상의 피드백 루프를 사용하는 반복 프로세스로 일련의 처리 모듈을 수행한다. 이하에서는, 제1 모듈의 실시예를 설명한다. 일반적으로, 제1 모듈의 기능은 \"데이터 수집 및 사전 처리\"로 이 어질 수 있다: 일반적으로, 위에서 설명한 바와 같이, 제1 모듈(M1)은 도 1에 도시된 바와 같이 센서 데이터를 수신하고, 센서 데이터의 각 수신 소스를 결정하며, 메타 데이터 정보를 포함하는 소스 정보를 파싱하고, 센서 데이터를 복수의 표준화된 데이터 아이템으로 변환시키며, 표준화된 데이터 아이템 중 적어도 일부를 메타 데이터 정보와 연관시키고, 원격 액세스에 적합한 데이터 웨어하우스에서 표준화된 데이터 아이템 중 적어도 일부를 연관된 메타 데 이터 정보와 함께 수집한다. 예를 들어, 실시예에 따르면, 제1 모듈(M1)은 예를 들어 이미지 및 비디오 스트림, GPS 감지, 오디오, 적외선, 수집기 시선(예를 들어, 각막, 반사, 입체 기하학, 눈 방향 및/또는 움직임) 및/또는 IMU(예를 들어, 가속도계, 자이로스코프 및/또는 자력계) 데이터를 포함하는 데이터 소스로부터 미가공 센서 데이터를 수신한다 (단계 1.1). 이러한 데이터를 사용자의 관점에서 수집하므로, '에고센트릭 데이터'라고 한다. 일반적으로, 데이 터는 예를 들어 당업계에 공지된 머리 장착형 센서 장치(예를 들어 스마트 안경)를 인간 데이터 수집기에 장착 함으로써 캡처될 수 있다. (멀티센서 에고센트릭 데이터 실시예에 따른) 센서 데이터는 각각의 센서로부터 직접적으로, 또는 내부 컬 렉션, 오픈 소스 공개 데이터세트, 또는 제3자 독점 데이터세트를 통해와 같이 수많은 소스 또는 입력 스트림으 로부터 나올 수 있다. 이러한 다양한 스트림에서 들어오는 데이터는 일반적으로 상호 연관되거나 표준화되지 않 을 수 있는 다양한 형식의 수많은 데이터 파일로 구성된다. 처음에, 전처리(1.1, 1.2 및 1.3단계)에는 다음과 같은 표준화 및 변환 작업이 포함될 수 있다: 데이터 수집: 센서 데이터의 유입 소스를 확인하고, 소스 정보를 파싱하며, 임시 스토리지에 분류한다. 추출 및 변환: 데이터 무결성, 검색 가능성, 접근성, 상호 운용성 및 재사용성을 보장하기 위해, 메타 데이터 컨테이너에는 미디어 속성(예를 들어, 비디오 또는 이미지), 사용 가능한 위치 정보, 스키마 및 버전 정보, 다 양한 센서 입력 방식 또는 연결된 정보에 대한 다른 추가 데이터 채널이 할당된다. 표준화된 타임스탬프를 (예를 들어, 절대 날짜 시간을 소스 날짜 시간, 예를 들어, 내부 처리 작업을 기록한 소 스 날짜/시간(가령, 센서 장비에서 가져온 데이터), 생성 날짜 시간, 예를 들어, 내부 섭취 날짜, 수정 날짜 시 간)에 대한 타임스탬프로 변환함으로써) 모든 다양한 센서 스트림에 할당한다. 센서 데이터를 비슷한 크기와 포맷의 표준화되고 처리 가능한 데이터 단위(예를 들어, 표준화된 픽셀 포맷과 비 슷한 길이의 비디오 인코딩을 사용하는 MP4 비디오 파일)로 변환한다. 추가적인 선택적 단계는 GDPR 및/또는 기타 개인 정보 보호 규제 제도에 따라 데이터 주체의 얼굴과 차량 번호 판을 블러링하거나 음성 오디오 필터링하는 등 특정 데이터 카테고리를 난독화하는 것일 수 있다(단계 1.4). 데이터 아이템(예를 들어, 상황, 이벤트, 관심 객체 및/또는 위치 태그)에 추가 메타데이터 정보를 할당한다. 추가 선택 단계로는 작업 요구 사항(예를 들어, 법적 및/또는 기능적 작업)에 대해 수집된 데이터를 수동으로 확인하고 뷰에 있는 객체 카테고리를 통해 자동 작업 확인을 수행할 수 있다. 본 실시예에서, 변환 활동의 결과는 메타 데이터 컨테이너를 갖는 표준화된 데이터 아이템이다. 이들은 특 정 목적(예를 들어, 에고센트릭, 개방형 시나리오를 포함)을 제공하는 데이터 세트 또는 데이터 세트 컬렉션으 로 묶일 수 있다. 변환 후, 단계 1.4에서, 데이터는 시각화, 어노테이션 및 교정을 위한 검색 가능성 및 분산 액세스를 허용하는 데이터 웨어하우스로 수집된다. 스토리지는 특정 요구 사항에 맞게 여러 데이터베이스 아키텍처를 기반으로 하는 클라우드 네이티브 접근 방식의 조합을 사용할 수 있다. 모든 데이터 및 모델 스토리지 구성 요소는 미사 용 암호화 기술 및 액세스 제어를 지원할 수 있다. 스토리지는 전 세계적으로 배포 가능하다. 미사용 암호화 및 전송 중 암호화 외에도, 개별 클라이언트 측 암호화가 사용될 수 있다. 데이터는 비정형 데이터부터 정형 데이터까지 매우 다양하므로 스토리지를 사용하면 데이터 구조에 유연성을 부 여할 수 있다. 제안된 아키텍처는 버전이 지정된 스키마 적용은 물론 거버넌스 및 감사 필요성도 지원한다. 데이터 웨어하우스는 또한 증강 및 분석된 데이터 아이템을 포함하여 파이프라인을 통해 생성되거나 파생된 데이터를 저장하여 파이프라인 결과의 참조 및 추가 반복은 물론 비교 및 개선을 가능하게 한다(도 1, 데이터 웨어하우스 참조). 위에서 설명한 추출 및 변환 단계를 고려한 추가 센서 데이터 외에, 비디오 또는 이미지와 같은 센서 데이터 가 정의된 기준(예를 들어, 결과 만족도에 대한 사용자 피드백 또는 감지 신뢰도)을 기반으로 데이터 웨어 하우스에 추가되고 모델 교육을 더욱 활성화하기 위해 저장된다. 예를 들어, 위의 애플리케이션 4에서, 지원 엔지니어를 고용해야 하는 경우, 사용자는 센서 데이터를 기록한 후 변환하여 데이터 웨어하우스에 저장하는 데 동의할 수 있다. 이러한 데이터 아이템은 개별 상황(예를 들어, 어두운 조명 조건에서 부분적으로 난독화되는 커피 머신)에서의 특정 사용 사례(예를 들어, 커피 머신 석회질 제거)의 모델 생성을 개선하는 데 사용될 수 있다. 이하에서는, 제2 모듈의 실시예를 설명한다. 일반적으로 제2 모듈의 기능은 \"훈련 및 교정 준비\"로 이어질 수 있다. 일반적으로, 위에 설명된 바와 같이, 도 1에 도시된 바와 같이, 제2 모듈(M2)은 데이터 웨어하우스에 액세스하 고 데이터 웨어하우스의 표준화된 데이터 아이템당 적어도 하나의 이미지 키프레임을 식별하며 적어도 하나의 이미지 키프레임을 나타내는 추가 정보를 각각의 표준화된 데이터 아이템과 연관시키는 것을 포함한다. 특히, 실시예에 따르면, 훈련 및 교정을 위해 도식화된 데이터를 사용하기 위한 준비에서, 메타 데이터는 추가 적인 파라미터로 확장될 것이다. 추가 파라미터에는 (예를 들어, 빠른 머리 움직임을 통해) 콘트라스트가 없거 나 거의 없거나 선명하지 않은 객체를 포함하는 프레임을 제외하고 풍부한 컨텍스트가 있는 프레임을 승격시키 는 등의 방식으로 식별되는 키프레임에 대한 정보가 포함된다(자세한 내용은 아래의 도 3을 참조하라). 시선 추 적의 추가 센서 정보, 위치 센서, IMU 또는 상황별 정보와 같은 추가 정보가 고려될 수 있다. 추출된 미디어 속 성은 수동 개선을 위해 키프레임만 제시하여 교정을 촉진하거나 콘텐츠별로 훈련 중에 제시된 샘플에 가중치를 부여하는 데 사용될 수 있다. 위에 설명된 예제 애플리케이션에서, IMU 데이터는 어노테이션 프로세스에 대한 정보가 적은 프레임을 필터링하 는 데 사용될 수 있다. 예를 들어, 머리를 크게 움직이면 모션 블러가 발생하여 해당 프레임이 제외될 것이라고 판단될 수 있다. 또한, 위의 애플리케이션 1에서는 움직임이 거의 없어 일시 중지를 결정하고 해당 섹션을 제외 할 수 있다. 이러한 키프레임을 선택하면 그 키프레임에만 어노테이션이 달아지므로 효율적인 어노테이션의 기 반이 구축되어 상당한 시간과 리소스가 절약된다. 시선 데이터(예를 들어, 주의 마커)를 사용하여 장면에서 객체의 돌출성을 확인할 수 있으므로, 교정 중에 영역 의 초점과 우선 순위를 높일 수 있다. 이 메커니즘을 사용하면 현재 컨텍스트와 관련이 없는 객체(예를 들어, 배경에 있거나 가려져 부분적으로만 보이는 객체)를 필터링할 수 있다. 유효한 훈련 데이터 세트를 위한 선택적 단계는 데이터 세트에 대한 초기 추론 작업으로 인해 교정 단계에 대한 권장 사항을 생성하는 것이다. 이는 교정 중에 표시될 키프레임 선택의 우선순위를 추가로 지정하는 데 사용될 수 있다. 데이터 아이템은 프레임의 집합으로 저장된 각 이미지 프레임에 대한 초기 어노테이션(2.2)을 얻기 위해 하 나 이상의 기초 모델에 제공된다(2.1). 이미지 프레임은 키프레임(2.3-2.4)을 감지하여 필터링되고 결과 키 프레임 정보는 프레임 컬렉션(2.5)을 확장하는 데 사용된다. 계산 비용이 필터링에 대한 추가 정보보다 중요한 경우 하나 이상의 기초 모델에 의해 제공되는 초기 어노테이션또 또한 키프레임 필터링 후에 제공될 수도 있다 것에 유의하라. 이하에서는, 제3 모듈의 실시예를 설명한다. 일반적으로, 제3 모듈의 기능은 \"어노테이션\"으로 이어질 수 있다. 일반적으로, 위에 설명된 바와 같이, 제3 모듈(M3)은 도 1에 도시된 바와 같이 컴퓨터 장치의 디스플레이와 같 은 휴먼 머신 인터페이스를 통해 적어도 하나의 이미지 키프레임 및 각 데이터 아이템의 관련 추가 정보를 하나 이상의 기초 모델에 의해 자동으로 생성된 하나 이상의 초기 어노테이션이 수행되는 어노테이션을 위해 휴먼 어 노테이터에게 제공하고, 휴먼 머신 인터페이스를 통해 어노테이터로부터 어노테이션을 수신하며, 해당 어노테이 션 정보를 갖는 적어도 하나의 이미지 키프레임을 적어도 하나의 어노테이션이 달린 데이터세트에 보강하는 것 을 포함한다. 일반적으로, 어노테이션 및 어노테이션 품질을 위한 데이터 큐레이팅은 모델 훈련용 데이터를 준비하는 회사의 주요 과제이다. 고품질의 데이터에 어노테이션을 추가하지 못하면 모델 성능이 저하되는 경우가 많다. 특히, 실시예에 따르면, 어노테이션의 시작점은 단계 2.6에서 제공되는 프레임의 컬렉션에서 감지된 키프레 임이며, 이는 초기 어노테이션이 수반될 수 있으며, 각 어노테이션은 이미지 데이터 및 대상 작업에 따라 필요 한 어노테이션 도구를 제공하는 어노테이션 프레임워크를 통해 수동으로 어노테이션 달아진다. 이러한 작업에는 개별 객체 마스크의 픽셀 수준 어노테이션, 기존 객체 마스크의 분류 및 속성 결정 또는 장면 속성의 라벨링이 포함될 수 있다. 단계 3.1에서 어노테이션 도구에 의해 어노테이터(또는 교정자)에게 제시된 키프레임은 기초 모델 에 의해 생성된 자동으로 생성된 초기 어노테이션에 의해 수행된다. 일반적으로, 기술 분야에 알려진 바와 같이, 기초 모델은 다양한 데이터세트의 대규모 풀에서 훈련되고 여기서는 일련의 일반적인 상위 카테고리(예를 들어, 사람, 음식, 가구, 바닥, 하늘 등) 세트로 분류된 객체의 인스턴스를 출력하는 모델을 참조한다. 어노테이션 프로세스는 어노테이션 정정 제안, 두드러진 객체 강조 및/또는 추정 오류 표시와 같은 안내 신호 (단계 6.2)에 의해 더 확장될 수 있다(도 1, 안내 시스템 참조). 예를 들어, 카테고리 제안은 최근 수동 어 노테이션으로 업데이트되어 거의 실시간으로 객체 카테고리를 제안하는 온라인 학습 시스템으로 구현될 수 있다. 데이터는 어노테이션이 달린 데이터세트로부터 단계 6.1을 통해 안내 시스템에 제공된다. 실시예 에 따르면, 안내 시스템은 과분할 또는 다른 개선 프로세스와 같은 선택적인 개선 프로세스의 묶음으로 구 현될 수 있다. 실시예에 따르면, 안내 시스템은 잠재적인 오류에 관한 정정 제안 및 개선 도움말과 같은 일 부 프로세스도 포함할 수 있다. 예제 애플리케이션 1: 각 운동 연습은 운동선수의 다양한 신체 부위(다리, 팔, 머리, 몸통 등)와 바벨, 케틀벨, 머신과 같은 운동 장 비를 대상으로 하는 키프레임과 어노테이션으로 설명된다. 수동 어노테이션은 두 가지 기초 모델에 의해 지원된 다. 그 중 하나는 사람별 마스크(기초 모델)를 출력하고 다른 하나는 사람 마스크 내의 다양한 신체 부위에 대 한 제안을 제공하는 과분할(개선 프로세스)을 출력한다. 과분할은 프레임을 일관된 영역으로 분할하고 기초 모 델보다 유사한 텍스처로 인해 더 편향된다. 예를 들어 셔츠, 반바지, 팔, 다리는 과분할에서 독립적인 영역이지 만, 모두 동일한 사람 마스크에 속한다. 그런 다음 각 신체 부위 영역은 사람 어노테이션 작성자에 의해 수정되고 해당 카테고리로 라벨이 지정된다. 카 테고리 제안은 객체별 통계를 고려하고 레이블이 지정되지 않은 객체를 이전에 분류된 객체와 일치시키는 온라 인 학습 접근 방식을 통해 최적화된다. 이하에서는, 제4 모듈의 실시예를 설명한다. 일반적으로 제4 모듈의 기능은 \"모델 생성\"으로 이어질 수 있다. 일반적으로, 위에서 설명한 바와 같이, 제4 모듈(M4)은 도 1에 도시된 바와 같이 적어도 하나의 어노테이션이 달린 데이터세트를 사용하여 적어도 하나의 머신러닝 모델을 생성하고 업데이트하는 것 중 적어도 하나를 포함 한다. 실시예에 따르면, (단계 3.3에서 업데이트된) 어노테이션이 달린 데이터세트의 어노테이션이 달린 데이터는 작업별 모델을 처음부터 생성하거나 기존 모델을 업데이트하고 대상 작업에 맞게 미세 조정하는 데 사용될 수 있다(단계 4.1). 작업별 모델은 이후에 나머지 데이터 요소에 적용될 수 있다(단계 4.2). 새로 어노테이션 이 달린 데이터로 기존 모델을 업데이트하는 것은 작은 단계를 통해 점진적이고 반복적으로 수행할 수 있다: 제 1 최소 비디오 세트(예를 들어, 100개 이상의 프레임)에 어노테이션을 추가한 후, 이미지-어노테이션 쌍을 사용 하여 시작 모델을 미세 조정하여, 작업별 객체 인스턴스 분할(모양 식별) 및 객체 카테고리(분류)를 출력하며, 이는 차례로 원래 기초 모델에서 사용할 수 있는 것보다 인간 교정을 위한 더 자세한 시작점을 제공한다. 미세 조정된 모델은 원래 기초 모델에서 사용할 수 있는 것보다 더 자세하게 나머지 요소에 적용된다. 실시예에 따르면, 두 가지 주요 유형의 어노테이션을 수동으로 개선(교정)하는 두 가지 프로세스가 아래에서 더 자세히 설명된다: - 인스턴스 분할: 모델에서 반환된 객체 인스턴스는 수동 개정 및 수정을 위한 시작점으로 사용된다. 수정 프로 세스는 브러시 도구를 통해 특정 객체에 픽셀 덩어리를 재할당하거나 전체 슈퍼픽셀(이미지를 분할하는 서브-객 체 조각)을 병합하거나 재할당하는 것을 포함할 수 있다. - 분류: 모든 객체 인스턴스에는 모델 출력에서 비롯된 추정 카테고리가 할당되고 인간 어노테이터에게 어노테 이션 제안을 표시하는 데 사용된다. 어노테이션 제안은 장면 컨텍스트 및 어노테이션 기록을 기반으로 확장되거 나 개선될 수 있다(단계 3.2 참조). 교정되고 어노테이션이 달린 데이터세트를 사용하여 원래 기초 모델을 업데이트하여 다음 데이터세트에 대한 보 다 정확한 초기 예측을 제공할 수 있다. 이를 위해, 작업별 및 세부 카테고리가 일반적인 적용 가능성을 제공하 기 위해 대략적인 수준으로 다시 매핑된다. 위의 예제 애플리케이션의 경우, 애플리케이션 1과 4의 시작점인 기 초 모델은 애플리케이션 2(주방 도구, 음식, 사람 등에 대한 객체 마스크)에서 제공하는 추가 훈련 데이터의 이 점을 누릴 수 있다.기초 모델과 작업별 모델이 객체 마스크와 카테고리 예측을 생성한다(단계 4.2). 이하에서는, 제5 모듈의 실시예를 설명한다. 실시예에 따르면, 제5 모듈의 기능은 \"추론/품질 보증(QA) 검토\"로 이어질 수 있다. 일반적으로, 위에서 설명한 바와 같이, 제5 모듈(M5)은 도 1에 도시된 바와 같이 하나 이상의 머신러닝 알고리 즘을 사용하는 하나 이상의 프로세스에 의해 액세스하기 위해 작업별 머신러닝 모델로서 적어도 하나의 머신러 닝 모델을 데이터 웨어하우스와 같은 스토리지 장치에 업로드하는 것을 포함한다. 실시예에 따르면, 기초 모델과 작업별 모델은 객체 마스크 및 카테고리 예측을 생성할 수 있다(단계 4.2). 모델 실행은 분산 클라우드 컴퓨팅을 사용하여 병렬로 수행된다. 배포를 위해, 데이터세트는 임시 컴 퓨팅 노드에 의해 처리되는 여러 청크(처리를 용이하게 하기 위해 더 작은 단위로 분할된 데이터세트의 조각)로 분할될 수 있으며, 여기에는 머신러닝 모델 입력을 제공하기 위한 (특히 데이터 세트를 모델에서 요구하는 포맷 (크기/해상도)으로 이미지를 조정하는) 전처리, 실제 추론 및 데이터 웨어하우스에 후처리한 후 출력된 최 종 데이터의 업로딩이 포함된다. 추론은 클라우드 호스팅 환경의 임시 컴퓨팅 노드에서 발생할 수 있으며, 이에 따라 노드가 추론 작업을 위해 필요에 따라 할당할 수 있다. 추론은 강력한 예측을 생성하기 위한 테스트 시간 확대와 함께 검토 지침 및 모델 앙상블(다중 모델 변형)을 위 한 불확실한 모델 예측에 대해 알리기 위해 출력 통계를 활용할 수 있다. 일부 실시예에서 추론은 머신러닝 모델을 새로운 데이터세트(즉, 이전에 훈련에 사용되지 않은 데이터세트)에 적용하고 출력 또는 \"예측\"을 생성하는 것을 포함한다. 이러한 프로세스를 일반적으로 \"ML 모델 운영화\" 또는 \"ML 모델을 생산에 투입\"이라고도 한다. 이 단계에 동안, 추론 시스템은 콘텐츠 제공자(예를 들어, 데이터 세트 에 대한 어노테이션을 서비스로 요청하는 제3자 회사)로부터 새로운 입력을 받아들이고, 데이터를 처리하고, 이 를 ML 모델에 공급하고, 출력을 다시 제공자에게 제공하여 객체 마스크 및 카테고리 예측으로 어노테이션이 달 린 데이터 세트를 얻는다. 실시예에 따르면, 출력 \"객체 마스크\"는 픽셀별 객체 감지를 나타내며 이미지의 객체 위에 컬러 필터로 시각화 될 수 있다. 일부 실시예에 따르면, 의미론적 분할을 통해 \"카테고리\"(예를 들어, \"배경\" 대 \"거리\" 대 \"객 체\")가 예측된다. 인스턴스 분할을 통해 개별 인스턴스(예를 들어, 동일한 객체의 세 인스턴스) 예측이 이루어 진다. 판옵틱 세그멘테이션(Panoptic Segmentation)에는 객체 인스턴스와 객체 카테고리 검색이 모두 포함된다. 품질 보증(QA) 검토(5.1단계 참조): 실시예에 따르면, 추론을 통해, 보이지 않는 데이터세트에는 인스턴스 및 카테고리 마스크/라벨이 어노테이션으 로 추가된다. 마스크가 실제 객체 경계와 정렬되지 않는 등 추론 결과나 예측이 잘못되거나 '어긋나는' 경우가 있다. 실시예에 따르면, 품질 보증 검토의 일부로서, 데이터 샘플이 추가되거나 제거될 수 있고(데이터 큐레이션) 모 델 하이퍼파라미터 또는 마스크 자체가 재검토 및 수정되어 예를 들어 데이터 웨어하우스에 저장된 최종 어 노테이션이 달린 데이터세트를 얻을 수 있다. 이런 식으로, 피드백 루프가 있다. \"모델 앙상블\"은 혼합 모 델로, 기본 모델 예측이 집계되어 보이지 않는 데이터에 대한 하나의 최종 예측이 생성된다. 테스트 시간 확대 에는 모델을 다시 실행하고 예측을 개선하기 위해 어떤 방식으로든 입력 이미지를 변경하는 작업(예를 들어, 왜 곡, 자르기, 뒤집기)이 포함된다. 따라서, 본 발명의 양태는 전술한 바와 같이 각각의 방법 단계를 구현하는 처리 모듈(M1 내지 M5)을 자동화하여 오류를 줄이고 에고센트릭 비디오 및 이미지 데이터에 어노테이션을 달기 위해 필요한 시간을 줄이는 데 중점을 둔다. 최근까지 에고센트릭 데이터 수집, 처리, 교정, 어노테이션 달기 및 모델 훈련 활동은 어렵고 오류가 발 생하기 쉬우며 시간이 많이 소요되었다. 실시예에 따르면, 개시된 데이터 파이프라인은 데이터 표준화, 어노테이션 및 모델 개선을 묶고, 또한 가이드 신호와 함께 자동화된 어노테이션을 고도로 통합하여 수동 어노테이션 프로세스를 최소화된 시간 요구 사항으로 안내된 교정 프로세스로 전환한다. 주목할만한 구성요소 중 하나는 기존 기반 모델을 신속하게 수정하거나 특정 작업을 더욱 정확하게 해결하기 위한 생성을 가능하게 하는 유연한 훈련 환경이다. 모델 출력은 시각화 및 어노 테이션 도구에 통합되어 휴먼 어노테이터에게 강화 및 최종 수정을 위한 초기의 정확한 어노테이션을 제공할 수 있다. 카테고리 어노테이션을 위한 모델 출력과 가이드 신호의 통합에 대한 한 가지 예는 어휘 데이터베이스를 활용하 여 예측 모델 카테고리를 계층적 분류에 포함시켜 휴먼 어노테이터에게 세분화된 제안을 표시할 수 있도록 하는 것이다. 이를 통해 궁극적으로 특수 객체 카테고리에 대한 빠른 모델 구축이 가능해진다. 도 3은 본 발명의 실시예에 따른 방법의 예시적인 사용 사례의 예시적인 시나리오를 도시한다. 이는 사이클링 사용 사례에 적용되는 훈련 및 교정을 위한 개략적인 데이터를 예시한 것으로, 센서 데이터는 반투명 안경과 실 제 환경의 이미지 또는 이미지(비디오) 스트림을 캡처하기 위한 적어도 하나의 카메라, 및 오디오, 시선 감지 센서 및/또는 IMU와 같은 추가 센서를 포함하는 머리 장착형 센서 장비에 의해 제공된다. 이미지를 제공하는 것 외에도, 추가 정보가 추출 및 변환되고, 메타 데이터 컨테이너에는 미디어 속성(예를 들어, 비디오 또는 이미지)과 위치 정보, 시선(시선 오버레이 이미지로 도 3에 도시됨), 스키마 및 버전 정보와 같은 기타 정보가 할당된다. 어노테이션 도구에 의해 어노테이터(또는 교정자)에게 제시된 키프레임 중 하나는 기초 모델에 의해 생성된 자동으로 생성된 초기 어노테이션(단계 2.2)이 수반된다. 키프레임은 전술한 바와 같이 이미지 프레임에서 필터링된다. 예를 들어, IMU 데이터는 어노테이션 프로세스에 대해 덜 유익한 초기 프레임 25(프레임 1에서 n)에서 프레임을 필터링하는 데 사용될 수 있다. 예를 들어, IMU에 의해 감지된 중요한 머리 움직임은 모션 블러로 이어질 것이 며 따라서 해당 프레임은 필터링된 키프레임 26(키프레임 1에서 m)의 감소된 세트를 얻기 위해 제외될 것이라고 결정될 수 있다."}
{"patent_id": "10-2023-0172110", "section": "도면", "subsection": "도면설명", "item": 1, "content": "이제 예시적인 실시예를 예시하는 다음의 도면과 관련하여 본 발명의 양태를 더 설명한다. 도 1은 본 개시 내용의 실시예에 따른 컴퓨터로 구현되는 방법을 도시한다. 도 2a, 2b는 도 1에 따른 방법의 각각의 확대도를 도시한다. 도 3은 본 발명의 실시예에 따른 방법의 예시적인 사용 사례의 예시적인 시나리오를 도시한다."}
