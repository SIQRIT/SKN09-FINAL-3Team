{"patent_id": "10-2024-0135095", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0053716", "출원번호": "10-2024-0135095", "발명의 명칭": "올플래시 어레이에서 컴퓨팅 스토리지를 가속화하기 위한 방법 및 아키텍처", "출원인": "삼성전자주식회사", "발명자": "베르만 아밋"}}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "링 토폴로지로 연결된 복수의 SSD(Solid State Drive)들로 구성된 올플래시 어레이 네트워크에서 컴퓨팅 스토리지를 가속화하는 방법에 있어서,상기 복수의 SSD들 중 각 SSD는 컨트롤러와 상기 컨트롤러에 연결된 DRAM을 포함하고,제1 SSD 컨트롤러가 상기 제1 SSD에 연결된 DRAM으로부터 데이터 읽기 또는 쓰기 요청을 수신하는 단계;상기 링 토폴로지의 상기 제1 SSD에 대한 식별자, 패킷에 대한 식별자 및 요청을 식별하는 읽기/쓰기 플래그를포함하는 패킷을 생성하는 단계; 및상기 링 토폴로지의 다음 SSD로 패킷을 전송하는 단계를 포함하고,상기 읽기/쓰기 플래그는 읽기 데이터 어드레스가 상기 DRAM에 위치하지 않는 것에 응답하는 읽기 요청이고, 상기 읽기/쓰기 플래그는 상기 DRAM이 가득 찬 것에 대한 응답하는 쓰기 요청이고, 상기 패킷에 쓰게될 데이터가포함되어 있는 것을 특징으로 하는 컴퓨팅 스토리지를 가속화하는 방법."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 읽기/쓰기 플래그가 읽기 요청을 나타내고, 읽기 데이터 어드레스가 상기 DRAM에 위치하는지 확인하는 단계; 및상기 DRAM의 상기 읽기 데이터 어드레스에서 데이터를 읽는 단계를 더 포함하는 컴퓨팅 스토리지를 가속화하는방법."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 읽기/쓰기 플래그가 쓰기 요청을 나타내고, 상기 DRAM이 가득 차지 않았는지 확인하는 단계; 및상기 DRAM으로부터 쓰기 어드레스에 데이터를 쓰는 단계를 더 포함하는 컴퓨팅 스토리지를 가속화하는 방법."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 링 토폴로지의 다음 SSD가 패킷을 수신하는 단계;상기 링 토폴로지의 다음 SSD는 상기 링 토폴로지의 제1 SSD와 다르고, 상기 읽기/쓰기 플래그가 쓰기 요청을나타내며 다음 SSD와 연관된 DRAM이 가득 차지 않음을 확인하는 단계;다음 SSD의 컨트롤러에 다음 SSD와 연관된 DRAM에 대한 쓰기 어드레스어드레스를 요청하는 단계;상기 패킷의 데이터를 상기 DRAM의 쓰기 어드레스에 쓰는 단계;쓰기 승인을 나타내기 위해 상기 읽기/쓰기 플래그를 설정함으로써 상기 패킷을 업데이트하는 단계; 및상기 링 토폴로지의 다음 SSD로 패킷을 전송하는 단계를 더 포함하는 컴퓨팅 스토리지를 가속화하는 방법."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 링 토폴로지의 다음 SSD가 상기 패킷을 수신하는 단계;공개특허 10-2025-0053716-3-상기 링 토폴로지의 다음 SSD는 상기 제1 SSD와 다르고, 상기 읽기/쓰기 플래그가 읽기 요청을 나타내는지 확인하는 단계;상기 링 토폴로지의 상기 제1 SSD에 대한 식별자 및 상기 패킷의 식별자에 기초하여, 다음 SSD의 컨트롤러에 다음 SSD와 연관된 DRAM에 대한 읽기 어드레스를 요청하는 단계;쓰기 승인을 나타내기 위해 읽기/쓰기 플래그를 설정하여 상기 패킷을 업데이트하고, 쓰여질 데이터를 상기 읽기 어드레스로부터 읽은 데이터로 교체하는 단계; 및상기 링 토폴로지의 다음 SSD로 상기 패킷을 전송하는 단계를 더 포함하는 컴퓨팅 스토리지를 가속화하는 방법."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 링 토폴로지의 다음 SSD가 상기 패킷을 수신하는 단계;상기 링 토폴로지의 다음 SSD가 상기 링 토폴로지의 상기 제1 SSD인지 확인하는 단계;쓰기 승인을 나타내는 상기 읽기/쓰기 플래그에 응답하여, 상기 제1 SSD 컨트롤러의 입출력 패킷 테이블을 업데이트하는 단계;쓰기 요청을 나타내는 상기 읽기/쓰기 플래그 및 상기 제1 SSD와 연관된 DRAM이 가득 찼음에 응답하여, DRAM 공간 부족 절차를 수행하는 단계; 및읽기 승인을 나타내는 상기 읽기/쓰기 플래그에 응답하여, 상기 패킷의 데이터를 상기 올플래시 어레이의 호스트로 출력하는 단계를 더 포함하는 컴퓨팅 스토리지를 가속화하는 방법."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 패킷에는 SSD 식별자, 패킷 식별자, 읽기/쓰기 승인 플래그 및 SSD와 연결된 DRAM에 쓰거나 SSD와 연결된DRAM에 읽을 데이터에 대한 필드를 포함하는 것을 특징으로 하는 컴퓨팅 스토리지를 가속화하는 방법."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "올플래시 어레이에서 컴퓨팅 스토리지를 가속화하기 위한 아키텍처에 있어서,입출력(I/O) 허브에 연결된 복수의 SSD(Solid State Drive)들; 및상기 허브에 연결된 프로세서를 포함하고,상기 복수의 SSD들 중 각각의 SSD 컨트롤러, 상기 컨트롤러에 연결된 DRAM 및 상기 컨트롤러에 연결된 복수의NAND 메모리를 포함하고,상기 복수의 SSD들은 각각의 컨트롤러가 입력 포트와 출력 포트를 포함하는 링 토폴로지로 연결되며, 각 SSD의각 입력 포트는 커넥터를 통해 이전 SSD의 출력 포트에 연결되고, 제1 SSD의 입력 포트는 커넥터를 통해 상기복수의 SSD들 중 마지막 SSD의 출력 포트에 연결되는 것을 특징으로 하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,각 SSD는 DRAM에 연결된 하드웨어 가속기를 더 포함하고,상기 하드웨어 가속기는 각 SSD의 DRAM에 대한 읽기 또는 쓰기 이외의 동작을 가속화하는 것을 특징으로 하는아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8 항에 있어서,인공지능(AI) 서버를 더 포함하고, 공개특허 10-2025-0053716-4-상기 인공지능 서버는 프로세서, 상기 프로세서에 연결된 DRAM 및 상기 프로세서에 연결된 복수의 NAND 메모리를 포함하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8 항에 있어서,상기 커넥터는 케이블인 것을 특징으로 하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8 항에 있어서,각 SSD의 상기 컨트롤러는,상기 DRAM 및 상기 출력 포트와 연결된 DRAM 매니저;상기 입력 포트, 상기 DRAM 및 상기 출력 포트와 연결된 패킷 버퍼;상기 패킷 버퍼, 상기 입력 포트 및 상기 DRAM 매니저와 연결된 SSD 간 FSM(finite state machine);상기 DRAM 매니저와 상기 패킷 버퍼를 상기 출력 포트에 연결하는 제1 멀티플렉서;상기 DRAM 매니저와 상기 패킷 버퍼를 상기 DRAM에 연결하는 제2 멀티플렉서; 및상기 DRAM 매니저를 호스트 및 상기 복수의 NAND 메모리에 연결하는 추가 구성 요소를 포함하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12 항에 있어서,상기 패킷 라우팅은,제1 SSD 컨트롤러가 상기 제1 SSD에 연결된 DRAM으로부터 데이터 읽기 또는 쓰기 요청을 수신하는 단계;상기 링 토폴로지의 상기 제1 SSD에 대한 식별자, 패킷에 대한 식별자 및 요청을 식별하는 읽기/쓰기 플래그를포함하는 패킷을 생성하는 단계; 및상기 링 토폴로지의 다음 SSD로 패킷을 전송하는 단계를 포함하고,상기 읽기/쓰기 플래그는 읽기 데이터 어드레스가 상기 DRAM에 위치하지 않는 것에 응답하는 읽기 요청이고, 상기 읽기/쓰기 플래그는 상기 DRAM이 가득 찬 것에 대한 응답하는 쓰기 요청이고, 상기 패킷에 쓰게 될 데이터가포함되어 있는 것을 특징으로 하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12 항에 있어서,상기 패킷 핸들링은,상기 링 토폴로지의 다음 SSD가 패킷을 수신하는 단계;상기 링 토폴로지의 다음 SSD는 상기 링 토폴로지의 제1 SSD와 다르고, 상기 읽기/쓰기 플래그가 쓰기 요청을나타내며 다음 SSD와 연관된 DRAM이 가득 차지 않음을 확인하는 단계;다음 SSD의 컨트롤러에 다음 SSD와 연관된 DRAM에 대한 쓰기 어드레스를 요청하는 단계;상기 패킷의 데이터를 상기 DRAM의 쓰기 어드레스에 쓰는 단계;쓰기 승인을 나타내기 위해 상기 읽기/쓰기 플래그를 설정함으로써 상기 패킷을 업데이트하는 단계; 및상기 링 토폴로지의 다음 SSD로 패킷을 전송하는 단계를 포함하는 것을 특징으로 하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12 항에 있어서,공개특허 10-2025-0053716-5-상기 패킷을 핸들링은,상기 링 토폴로지의 다음 SSD가 상기 패킷을 수신하는 단계;상기 링 토폴로지의 다음 SSD는 상기 제1 SSD와 다르고, 상기 읽기/쓰기 플래그가 읽기 요청을 나타내는지 확인하는 단계;상기 링 토폴로지의 상기 제1 SSD에 대한 식별자 및 상기 패킷의 식별자에 기초하여, 다음 SSD의 컨트롤러에 다음 SSD와 연관된 DRAM에 대한 읽기 어드레스를 요청하는 단계;쓰기 승인을 나타내기 위해 읽기/쓰기 플래그를 설정하여 상기 패킷을 업데이트하고, 쓰여질 데이터를 상기 읽기 어드레스로부터 읽은 데이터로 교체하는 단계; 및상기 링 토폴로지의 다음 SSD로 상기 패킷을 전송하는 단계를 포함하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제12 항에 있어서,상기 DRAM 매니저는,SSD의 식별자에 대한 필드와 들어오는 패킷의 식별자에 대한 필드를 포함하는 들어가는(incoming) 패킷 테이블;및나가는 패킷의 식별자에 대한 필드와 DRAM 데이터 링크에 대한 필드를 포함하는 나가는(outgoing) 패킷 테이블을 포함하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "올플래시 어레이에서 컴퓨팅 스토리지를 가속화하기 위한 아키텍처에 있어서,입출력 허브에 연결된 복수의 SSD들; 및상기 입출력 허브에 연결된 프로세서를 포함하고,상기 복수의 SSD들 중 각 SSD는 컨트롤러, 상기 컨트롤러에 연결된 DRAM, 상기 컨트롤러에 연결된 복수의 NAND메모리들을 포함하고,상기 입출력 허브는 인커밍 DRAM 데이터와 승인 패킷을 지원하도록 구성되고, 각 SSD 컨트롤러는 DRAM 읽기 및쓰기 요청을 라우팅하고, 수신된 패킷을 핸들링하도록 구성되는 것을 특징으로 하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17 항에 있어서,상기 패킷 라우팅은,제1 SSD 컨트롤러가 상기 제1 SSD에 연결된 DRAM으로부터 데이터 읽기 또는 쓰기 요청을 수신하는 단계;상기 링 토폴로지의 상기 제1 SSD에 대한 식별자, 패킷에 대한 식별자 및 요청을 식별하는 읽기/쓰기 플래그를포함하는 패킷을 생성하는 단계; 및상기 링 토폴로지의 다음 SSD로 패킷을 전송하는 단계를 포함하고,상기 읽기/쓰기 플래그는 읽기 데이터 어드레스가 상기 DRAM에 위치하지 않는 것에 응답하는 읽기 요청이고, 상기 읽기/쓰기 플래그는 상기 DRAM이 가득 찬 것에 대한 응답하는 쓰기 요청이고, 상기 패킷에 쓰게 될 데이터가포함되어 있는 것을 특징으로 하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18 항에 있어서,상기 패킷 핸들링은,상기 링 토폴로지의 다음 SSD가 패킷을 수신하는 단계;공개특허 10-2025-0053716-6-상기 링 토폴로지의 다음 SSD는 상기 링 토폴로지의 제1 SSD와 다르고, 상기 읽기/쓰기 플래그가 쓰기 요청을나타내며 다음 SSD와 연관된 DRAM이 가득 차지 않음을 확인하는 단계;다음 SSD의 컨트롤러에 다음 SSD와 연관된 DRAM에 대한 쓰기 어드레스를 요청하는 단계;상기 패킷의 데이터를 상기 DRAM의 쓰기 어드레스에 쓰는 단계;쓰기 승인을 나타내기 위해 상기 읽기/쓰기 플래그를 설정함으로써 상기 패킷을 업데이트하는 단계; 및상기 링 토폴로지의 다음 SSD로 패킷을 전송하는 단계를 포함하는 것을 특징으로 하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제18 항에 있어서,상기 패킷을 핸들링은,상기 링 토폴로지의 다음 SSD가 상기 패킷을 수신하는 단계;상기 링 토폴로지의 다음 SSD는 상기 제1 SSD와 다르고, 상기 읽기/쓰기 플래그가 읽기 요청을 나타내는지 확인하는 단계;상기 링 토폴로지의 상기 제1 SSD에 대한 식별자 및 상기 패킷의 식별자에 기초하여, 다음 SSD의 컨트롤러에 다음 SSD와 연관된 DRAM에 대한 읽기 어드레스를 요청하는 단계;쓰기 승인을 나타내기 위해 읽기/쓰기 플래그를 설정하여 상기 패킷을 업데이트하고, 쓰여질 데이터를 상기 읽기 어드레스로부터 읽은 데이터로 교체하는 단계; 및상기 링 토폴로지의 다음 SSD로 상기 패킷을 전송하는 단계를 포함하는 아키텍처."}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "올플래시 어레이에서 컴퓨팅 스토리지를 가속화하기 위한 방법 및 아키텍처가 개시된다. 본 개시의 예시적인 실 시예에 따른 링 토폴로지로 연결된 복수의 SSD(Solid State Drive)들로 구성된 올플래시 어레이 네트워크에서 컴 퓨팅 스토리지를 가속화하는 방법에 있어서, 복수의 SSD들 중 각 SSD는 컨트롤러와 컨트롤러에 연결된 DRAM을 포 함하고, 제1 SSD 컨트롤러가 제1 SSD에 연결된 DRAM으로부터 데이터 읽기 또는 쓰기 요청을 수신하는 단계, 링 토폴로지의 제1 SSD에 대한 식별자, 패킷에 대한 식별자 및 요청을 식별하는 읽기/쓰기 플래그를 포함하는 패킷 을 생성하는 단계 및 링 토폴로지의 다음 SSD로 패킷을 전송하는 단계를 포함하고, 읽기/쓰기 플래그는 읽기 데 이터 어드레스가 DRAM에 위치하지 않는 것에 응답하는 읽기 요청이고, 읽기/쓰기 플래그는 DRAM이 가득 찬 것에 대한 응답하는 쓰기 요청이고, 패킷에 쓰게될 데이터가 포함되어 있는 것을 특징으로 할 수 있다."}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시의 실시예는 스토리지 내 컴퓨팅을 가속화하기 위해 DRAM이 다수의 SSD에서 공유되는 컴퓨터 시스템의 스토리지 시스템 스택에 관한 것이다."}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "스토리지 시스템 스택은 장치, 프로토콜 및 레이어를 포함한다. 성능 중심의 대용량 스토리지 시스템에서는 플 래시 기반 SSD(Solid-State Drive) 장치는 단일 호스트 인터페이스 프로토콜로 액세스하고 맵핑하고, 파일 시스 템 섹터로 구성하며, 전용 소프트웨어 레이어로 균형을 맞추고 관리하는 올플래시 어레이(all-flash array, AFA)를 형성하기 위해 그룹화된다. SSD 장치는 싱글 호스트와의 독립 실행형 작동에 최적화된 프로세서, 내장형 메모리 및 RAM(Random Access Memory) 리소스를 갖춘 포괄적인 스토리지 시스템을 구성한다. 여러 개의 SSD를 모아서 하나의 스토리지 엔터티 (entity)로 동작하는 경우, 하나의 SSD에 병목 현상이 발생하는 특정 구성 요소가 있는 반면 다른 장치에는 동 일한 리소스가 충분히 활용되지 않는 경우가 있을 수 있다. 이러한 시나리오는 전체 시스템 성능을 저하시킨다. 필요한 하드웨어가 있음에도 불구하고 SSD 간 통신이 부족하여 사용할 수 없다. 플래시 기반 스토리지 시스템에는 여러 SSD가 포함되어 있으며, 각 SSD는 독립적으로 활성화된다. SSD 내부 구 성 요소는 호스트 프로토콜에서 제공하는 특정 워크로드에 고유하게 사용된다. 그러나 여러 개의 SSD를 플래시 어레이로 그룹화하면, 내부 프로세스로 인해 스토리지 작업 할당 관리의 균형이 유지되기 어렵다. DRAM(Dynamic RAM)은 동적 플래시 변환 레이어(flash translation layer, FTL) 관리 및 데이터 버퍼링을 활성 화하여 읽기/쓰기 성능과 안정성을 향상시키는 SSD의 필수 구성 요소이다. 그러나 특정 워크로드 시나리오에서 는, DRAM이 오버플로우 될 수 있고, 해당 데이터가 NAND 플래시 메모리로 이동하여 심각한 성능 저하가 발생할 수 있다. 따라서, RAM 공간이 부족한 경우에 대처하기 위한 알고리즘이 필요하다. 게다가, 계속되는 비용 경쟁 으로 인해 DRAM 공간을 줄이려는 시도가 발생하고 상황이 더욱 어려워지고 있다. 데이터 센터 및 AFA 제품 수요가 증가함에 따라 기본 빌딩 블록에 더 많은 구성 요소가 채워진다. 본 개시의 실 시예에 따른 SSD 연결 구조는 SSD 통합 구성 요소와 개방형 연구 방향을 리소스 공유 및 내결함성으로 효율적으 로 활용한다. 본 개시의 실시예는 효율적은 요소 활용을 통해 비용 및 성능 문제를 해결할 글로벌 시장에서 경 쟁력 있는 제품을 확보할 수 있다. 도 1a은 종래의 실시예에 따른 SSD 네트워크의 아키텍처를 나타내는 블록도이다. 도 1a는 SSD 1, SSD2, SSD 3, SSD 4 각각에 연결되는 입/출력 컨트롤러 허브, 입/출력 컨트롤러 허브 에 연결된 프로세서를 도시한다. 각각의 SSD는 컨트롤러, 컨트롤러에 연결되는 DRAM, 컨트롤러에 연 결되는 복수의 NAND를 포함할 수 있다. 비록 도 1a는 4개의 SSD가 허브에 연결되는 것을 도시하였지만, 이 는 설명의 편의를 위한 것으로 본 개시의 실시예는 이에 제한되지 않는다."}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 기술적 사상이 해결하고자 하는 과제는, 다수의 SSD를 연결함으로써 AFA의 SSD 간에 DRAM을 공유하고, 특정 SSD DRAM의 공간이 부족하면 해당 데이터를 사용 가능한 RAM이 있는 다른 SSD에 저장할 수 있는 올플래시 어레이 네트워크에서 컴퓨팅 스토리지를 가속화하기 위한 방법 및 아키텍처를 제공하는데 있다. 본 개시의 기술적 사상이 해결하고자 하는 과제는, DRAM 리소스를 효율적으로 활용하고 단일 SSD 공간이 부족한 DRAM이 있는 반면 다른 SSD에는 스페어(spare)가 있는 경우를 제거하는 것이다. 올플래시 어레이 계산은 SSD DRAM 리소스를 공유할 수 있다. 또한, AFA는 고성능 계산을 수행할 수 있으며 더 나은 성능을 위해 SSD 내부의 DRAM 리소스를 공유할 수 있다. 분석 결과, SSD 수, DRAM 오버플로우 확률 및 성능 감소 요인에 따라 기존 방식 에 비해 AFA 성능이 최대 2배 향상된 것으로 나타났다."}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 예시적인 실시예에 따른 링 토폴로지로 연결된 복수의 SSD(Solid State Drive)들로 구성된 올플래시 어레이 네트워크에서 컴퓨팅 스토리지를 가속화하는 방법에 있어서, 상기 복수의 SSD들 중 각 SSD는 컨트롤러와 상기 컨트롤러에 연결된 DRAM을 포함하고, 제1 SSD 컨트롤러가 상기 제1 SSD에 연결된 DRAM으로부터 데이터 읽 기 또는 쓰기 요청을 수신하는 단계, 상기 링 토폴로지의 상기 제1 SSD에 대한 식별자, 패킷에 대한 식별자 및 요청을 식별하는 읽기/쓰기 플래그를 포함하는 패킷을 생성하는 단계 및 상기 링 토폴로지의 다음 SSD로 패킷을 전송하는 단계를 포함하고, 상기 읽기/쓰기 플래그는 읽기 데이터 어드레스가 상기 DRAM에 위치하지 않는 것에 응답하는 읽기 요청이고, 상기 읽기/쓰기 플래그는 상기 DRAM이 가득 찬 것에 대한 응답하는 쓰기 요청이고, 상 기 패킷에 쓰게 될 데이터가 포함되어 있는 것을 특징으로 할 수 있다. 본 개시의 예시적인 실시예에 따른 올플래시 어레이에서 컴퓨팅 스토리지를 가속화하기 위한 아키텍처에 있어서, 입출력(I/O) 허브에 연결된 복수의 SSD(Solid State Drive)들 및 상기 허브에 연결된 프로세서를 포함 하고, 상기 복수의 SSD들 중 각각의 SSD 컨트롤러, 상기 컨트롤러에 연결된 DRAM 및 상기 컨트롤러에 연결된 복 수의 NAND 메모리를 포함하고, 상기 복수의 SSD들은 각각의 컨트롤러가 입력 포트와 출력 포트를 포함하는 링 토폴로지로 연결되며, 각 SSD의 각 입력 포트는 커넥터를 통해 이전 SSD의 출력 포트에 연결되고, 제1 SSD의 입 력 포트는 커넥터를 통해 상기 복수의 SSD들 중 마지막 SSD의 출력 포트에 연결되는 것을 특징으로 할 수 있다. 본 개시의 예시적인 실시예에 따른 올플래시 어레이에서 컴퓨팅 스토리지를 가속화하기 위한 아키텍처에 있어서, 입출력 허브에 연결된 복수의 SSD들 및 상기 입출력 허브에 연결된 프로세서를 포함하고, 상기 복수의 SSD들 중 각 SSD는 컨트롤러, 상기 컨트롤러에 연결된 DRAM, 상기 컨트롤러에 연결된 복수의 NAND 메모리들을 포함하고, 상기 입출력 허브는 인커밍 DRAM 데이터와 승인 패킷을 지원하도록 구성되고, 각 SSD 컨트롤러는 DRAM 읽기 및 쓰기 요청을 라우팅하고, 수신된 패킷을 핸들링하도록 구성되는 것을 특징으로 할 수 있다."}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 예시적인 실시예에 따르면, 다수의 SSD를 연결함으로써 AFA의 SSD 간에 DRAM을 공유하여 순환 구조를 생성함으로써, 전반적인 SSD 성능을 향상시킬 수 있다. 데이터 센터 및 AFA 제품 수요가 증가함에 따라 기본 빌딩 블록에 더 많은 구성 요소가 채워진다. 본 개시의 실 시예에 따른 SSD 연결 구조는 SSD 통합 구성 요소와 개방형 연구 방향을 리소스 공유 및 내결함성으로 효율적으 로 활용한다. 본 개시의 실시예는 효율적은 요소 활용을 통해 비용 및 성능 문제를 해결할 글로벌 시장에서 경쟁력 있는 제품을 확보할 수 있다. 본 개시의 예시적 실시예들에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 아니하며, 언급되지"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "아니한 다른 효과들은 이하의 설명으로부터 본 개시의 예시적 실시예들이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 도출되고 이해될 수 있다. 즉, 본 개시의 예시적 실시예들을 실시함에 따른 의도하지 아"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "니한 효과들 역시 본 개시의 예시적 실시예들로부터 당해 기술분야의 통상의 지식을 가진 자에 의해 도출될 수 있다."}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시의 실시예에는 두 가지 구현 방식인 하드웨어 및 배선 수정을 포함하는 SSD 간 연결을 갖춘 I/O 컨트롤 러 독립 방식과 I/O 허브 드라이버 및 SSD 컨트롤러에서 소프트웨어만 변경하는 대체 방식이 포함될 수 있다. 또한, 본 개시의 실시예는 패킷 네트워킹 통신을 위한 알고리즘을 제공할 수 있다. 도 1b는 본 개시의 예시적 실시예에 따른 SSD 네트워크를 도시한 블록도이다. 도 1b는 각 SSD 컨트롤러에는 \"입 력\"과 \"출력\"이라는 두 개의 추가 포트가 있고, 각각의 SSD 컨트롤러 사이에 새 커넥터가 추가되어 링 토폴로지 를 형성한다는 점에서 도 1a과 차이점이 있다. 링 토폴로지는 각 노드가 정확히 두 개의 다른 노드에 연결되어 각 노드를 통과하는 신호에 대한 단일 연속 경로를 형성하고 링 네트워크를 형성하는 네트워크 토폴로지이다. 예를 들어, 각 SSD의 각 입력 포트는 이전 SSD의 출력 포트에 커넥터로 연결되고, 복수의 SSD 중 첫 번째 SSD의 입력 포트는 상기 복수의 SSD 중 마지막 SSD의 출력 포트의 커넥터로 연결될 수 있다. 특정 SSD에서 DRAM의 공 간이 부족한 경우, 데이터는 다른 장치에 있는 올플래시 어레이(AFA)의 다른 DRAM으로 라우팅될(routed) 수 있 다. 그러한 장치의 위치가 발견되지 않는다면, 패킷은 순환 연결을 통해 원래 SSD로 반환되며 종래 구조와 동일 하게 처리될 수 있다. 일부 실시예에서, 하나 이상의 SSD에는 각 SSD의 DRAM에 대한 읽기 또는 쓰기 이외의 작 업을 가속화하는 하드웨어(HW) 가속기를 포함할 수 있고, 하나 이상의 SSD는 프로세서, 프로세서에 연결된 DRAM, 프로세서에 연결된 복수의 NAND 메모리들을 포함하는 인공지능(AI) 서버일 수 있다. 비록 도 1b는 허브 에 연결된 4개의 SSD를 도시하나, 이는 예시의 편의를 위한 것으로, 본 개시의 실시예가 반드시 이에 한정 되는 것은 아니다. 통신은 DRAM 오버플로우로 인해 컨트롤러 출력 포트에서 RAM 요청이 전송되는 패킷 스위칭을 기반으로 할 수 있 다. 각 SSD에는 식별을 위해 일련 제조 번호와 같은 고유 식별자(ID)가 할당될 수 있다. 각 컨트롤러에는 컨트 롤러 간 통신을 위한 입력 및 출력이라는 두 개의 추가 포트를 포함할 수 있다. 도 1b를 참조하면, SSD 1은 SSD 2에, SSD 2는 SSD 3에, SSD 3은 SSD 4에, SSD 4는 다시 SSD 1에 연결되어 링 토폴로지를 형성한다. 특정 SSD DRAM 공간이 필요하지만 사용할 수 없는 경우, 컨트롤러는 입력 포트를 통해 SSD-ID 및 데이터가 포함된 패킷을 링 네트워크의 다음 SSD로 전송할 수 있다. 수신 컨트롤러는 DRAM 가용성을 확인하고 요청이 발행되면 해당 ID 와 함께 승인 패킷을 전송할 수 있다. 그렇지 않고 DRAM 공간을 사용할 수 없는 경우, 원본 패킷은 출력 포트를통해 다음 SSD로 전달될 수 있다. SSD 중 어느 것도 요청을 이행할 수 없는 경우, 이는 종래의 독립형 SSD에서 처럼 상황을 처리하는 원래 SSD로 다시 전달될 수 있다. 허브에는 SSD 기능 확장을 위한 추가 리소스가 포함될 수 있다. DRAM 할당이 입/출력 컨트롤러에 의해 소프트웨어로 관리되는 대체적인 연결이 아래의 도 3a 내지 도 3b에 도시 되어 있다. 이 경우, SSD 컨트롤러는 허브에서 RAM 공간을 요청하고, 허브 연결을 통해 데이터를 다른 SSD로 라 우팅할 수 있다. 상기 아키텍처에는 추가 배선이나 SSD 컨트롤러 포트가 필요하지 않고, 입/출력 컨트롤러와 SSD 컨트롤러의 드라이버 및 소프트웨어 수정만 필요할 수 있다. DRAM 공유 기능을 사용하면 메모리 감소, 플래시 변환 레이어(FTL, flash translation layer) 압축, 가비지 컬 렉션(garbage collection) 동작, 쓰기 버퍼 및 내부 가속기를 활성화하고 확장할 수 있다. SSD 간 연결은 에러 정정 디코더 모듈(error-correction decoder module) 또는 프로세서와 같은 기타 컨트롤러 리소스를 추가로 공 유할 수 있다. Ⅰ. 표기법 및 정의 표기법 1(AFA의 SSD 수): 단일 AFA 플랫폼의 SSD 용량은 N으로 표시될 수 있다. 도 1b를 참조하면, AFA는 단일 허브에 연결되거나 호스트 프로세서에 연결되는 입/출력 컨트롤러를 관리하는 여러 SSD를 포함하는 것으로 가정 할 수 있다. 표기법 2(DRAM 오버플로우 확률): 사전 정의된 SSD 파라미터에서, DRAM 공간이 완전히 활용될 때의 공간 수요 확률은 p로 표시될 수 있다. 표기법 3(SSD 대역폭): 읽기/쓰기 작업의 평균 GB/초 데이터 속도는 B로 표시될 수 있고, DRAM 오버플로우가 있 거나 없는 경우(r<1), rB로 표시될 수 있다. SSD 성능 저하의 원인은 DRAM 데이터를 NAND 플래시에 저장하는 데 서 비롯될 수 있다. 결과적으로 예상되는 대역폭은 수학식 1과 같다. 수학식 1"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "정규화 목적을 위해, BW를 상수 B로 나누고 r은 분석에 따라 정적으로 결정될 수 있다. 표기법 4(AFA 대역폭): AFA의 성능은 AFA의 SSD 수와 단일 SDD의 대역폭을곱한 값으로 정의될 수 있다. 수학식 2"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "각 SSD는 독립적으로 동작하므로, 전체 시스템 성능을 위해 대역폭을 추가할 수 있다. Ⅱ. SSD 간 통신 A. 물리적 링크 본 개시의 실시예에서, 각각의 컨트롤러는 들어오고 나가는 패킷을 위한 2개의 추가 포트 \"입력\" 및 \"출력\"을 포함할 수 있다. 링크 폭과 주파수는 물론 상호 작용 프로토콜도 임의로 결정할 수 있다. B. 패킷 구조 패킷 크기는 패킷이 데이터를 전달하는지 또는 승인을 전달하는지에 따라 20 비트(bit) 또는 84 비트가 될 수 있다. 도 2를 참조하면, 어드레스, 섹터 쓰기/읽기 데이터 또는 메타 데이터와 같은 데이터의 종류에 상관없이 패킷에는 읽기 또는 쓰기 동작을 나타내는 2비트 표시인 고유 ID가 포함되고, 쓰기 요청 또는 읽기 응답을 나타 내는 2비트 표시, SSD-ID와 함께 DRAM 페이지 크기(일반적으로 64 비트) 크기의 데이터를 포함할 수 있다. 도 2 는 본 개시의 실시예에 따른 SSD 간 패킷 구조를 나타내는 도면이다. 도 2를 참조하면, 콘텐츠에는 SSD 네트워 크의 각 SSD를 고유하게 식별하는 SSD 식별자(SSD_ID), 패킷을 고유하게 식별하는 고유한 패킷 식별자(Packet- ID), 첨부된 데이터의 요청 또는 승인 표시를 포함하는 2 비트 읽기/쓰기 명령 및 원본 데이터는 DRAM 페이지크기(일반적으로 64 비트)로 DRAM에 기록될 수 있다. 실시예에서, SSD 식별자(SSD_ID)는 8 비트이고, 고유 패킷 식별자(Packet-ID)는 10 비트이다. 전송은 버스 폭에 따라, 몇 사이클에 걸쳐 분산될 수 있다. 패킷 크기는 연 결 버스 폭으로 나누어 전송에 필요한 클록 주기의 양을 결정할 수 있다. 다른 SSD와 연결된 DRAM에서 데이터를 읽기 위한 SSD 요청의 경우, 패킷 R/W 표시는 {R, 0}인 반면, 읽기 데이터가 대상 SSD로 가는 도중에 연결되면, 패킷 R/W 표시는 {R, 1}일 수 있다. SSD의 쓰기 승인이 발생한 경우, 패킷 R/W 표시는 {W, 0}이고, 다른 SSD에 데이터를 프로그래밍하라는 SSD 요청의 경우, 패킷 R/W 표시는 {W, 1}일 수 있다. C. 라우팅 및 서비스 SSD DRAM이 가득 찼을 때 올플래시 어레이(AFA)의 다른 DRAM에 대한 읽기 또는 쓰기 요청을 하는 실시예에 따른 방법은 아래 알고리즘 1에서 자세히 설명된다. 패킷 식별 할당은 증분 순서로 수행되고 나가는 패킷 식별은 일 반 DRAM 데이터 관리 외에 전송된 패킷 식별을 저장하려고 시도하는 SSD 컨트롤러의 추가 DRAM 관리자 메커니즘 에서 수행될 수 있다. 데이터가 다른 SSD에 저장되어 있는 DRAM 읽기의 경우는 ~ 단계에서 다루며, 이 경 우 DRAM 관리자로부터 패킷 식별자를 요청하고, AFA의 현재 SSD 식별자, 패킷 식별자 및 읽기 요청 명령으로부 터 패킷을 구성함으로써 패킷이 구성될 수 있다. DRAM 공간을 사용할 수 없는 DRAM 쓰기의 경우는 ~ 단계 에서 다루며, AFA의 현재 SSD의 식별자, 패킷 식별자 및 쓰기 요청 명령으로부터 패킷이 구성될 수 있다. 각 경 우에 패킷은 AFA 링의 다음 SSD로 전송될 수 있다. 알고리즘 1: DRAM에 읽기/쓰기 요청"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "들어오는 패킷을 처리하기 위한 실시예에 따른 방법이 아래의 알고리즘 2에서 설명하기로 한다. 이하에서,\"다른 SSD\"라는 문구는 SSD 네트워크 내의 다른 SSD를 의미하며, 네트워크의 각 SSD는 고유한 SSD 식별자(SSD_ID)로 식별 및 구별될 수 있다. 서로 다른 SSD의 DRAM에 쓰기 또는 읽기를 수행하는 경우는 ~ 단계에서 다루며, 이 경우 패킷을 수신한 SSD의 SSD 식별자(SSD_ID)는 수신된 패킷의 SSD 식별자(SSD_ID)와 다르며, 이는 전송 SSD를 식별할 수 있다. 서로 다른 SSD의 DRAM에 쓰는 경우는 ~ 단계에서 다루고, 서로 다른 SSD에서 DRAM을 읽는 경우는 ~ 단계에서 다룬다. DRAM이 꽉 찼거나 읽기 패킷 번호를 찾을 수 없는 경우, 패킷은 단계에서 단순히 링의 다음 SSD로 라우팅될 수 있다. 패킷이 원래 SSD로 반환된 경우는 ~ 단계에 서 처리될 수 있다. 원본 SSD는 패킷의 SSD 식별자(SSD_ID)와 동일할 수 있다. 쓰기 승인의 경우는 ~ 단계에 표시될 수 있고, 이 경우 DRAM 매니저는 들어오고 나가는 패킷 테이블을 업데이트될 수 있다. 쓰기 요청 이 포함된 패킷이 모든 링 노드(완전한 순환)를 통과하여 원래 SSD로 돌아오는 경우 및 DRAM이 여전이 가득 찬 경우에는, ~ 단계에 표시된 것처럼 기존 DRAM 공간 외로 처리될 수 있다. 해당 사례는 그림과 같이 기 존 DRAM 공간 부족으로 처리된다. 읽기 승인이 발생한 경우, ~ 단계에서 설명한 바와 같이, 읽은 데이 터가 원래 SSD로 다시 전송되고, 데이터가 호스트로 출력될 수 있다.알고리즘 2: 패킷 핸들링(handling)"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "D. 대체 SSD 간 연결 상호 연결 비용을 줄이기 위해, 실시예는 관리 입/출력 컨트롤러 허브를 통해 대체 SSD 간 통신을 제공할 수 있 다. 예를 들어, AFA 링의 다음 SSD로 패킷을 전송하는 대신, 전송 SSDF는 패킷을 허브로 전송하고, 허브로 패킷 을 다음 SSD로 전송할 수 있다. 허브-SSD 인터페이스는 스토리지 프로토콜 이상으로 확장되고, SATA(Serial AT Attachment), PCIe(Peripheral Component Interconnect express), NVMe(Non-Volatile Memory express) 또는 이와 유사한 것, 내부 DRAM 데이터 전송 및 승인 명령을 포함하고, 위의 섹션 II.B 에 설명된 바와 같고, 아래 도 6을 참조하여 설명된 바와 같다. 이 구성에는 외부 링크가 필요하지 않으며 다음 섹션에서 설명하기로 한다. 도 3a 및 도 3b는 실시예에 따른 입/출력 컨트롤러 허브를 통한 대체적인 SSD 간 통신을 도시한 블록도이다. 도 3a 및 도 3b를 참조하면, SSD 컨트롤러에는 도 1b에 도시된 추가 입력 포트 및 출력 포트를 포함하지 않을 수 있다. 도 3a를 참조하면 입/출력 컨트롤러 허브로부터 RAM 공간을 요청하는 SSD 2를 도시하고, 도 3b 를 참조하면 허브가 SSD 3에서 이용 가능한 DRAM을 할당하고 프로토콜에 따라 데이터를 라우팅하는것을 도시한다. E. RAM 확장 및 기타 리소스 공유 본 개시의 실시예에 따른 접근법에 따르면, 추가 DRAM과 같은, 추가 AFA 리소스를 허브에 할당하여 전반적인 SSD 성능을 향상시킬 수 있다. SSD는 위에서 설명한 것과 유사한 아키텍처를 사용하여 에러 정정 디코더와 같은 다른 리소스를 공유할 수 있다. Ⅲ. SSD 간 통신 상호 연결 비용을 줄이기 위해, 본 개시의 실시예는 다음을 통해 대체 SSD 간 통신을 제공할 수 있다. A. SSD 간 링 연결 본 개시의 실시예에 따른, 수정 사항은 아래와 같다. ● SSD 컨트롤러 하드웨어(HW) 및 소프트웨어(SW) 도 1b에 도시된 바와 같이, 컨트롤러에는 2개의 추가 입력 포트 및 출력 포트를 포함할 수 있다. 위의 알고리즘 1 및 2에 설명된 바와 같이, 컨트롤러는 패킷을 보내고 받기 위해 전용 하드웨어 또는 소프트웨어가 필요할 수 있다. 추가 하드웨어는 도 4에 도시되어 있고, 도 4는 SSD 간 통신 모듈에서의 데이터 흐름을 도시한다. 도 4는 멀티플렉서(210, 211), DRAM 매니저, 패킷 버퍼, SSD 간 FSM() 및 추가 컨트롤러 구성 요소 를 포함하는 예시적인 SSD 컨트롤러를 도시한다. 라우팅 및 핸들링 알고리즘은 SSD 간 FSM에서 구현될 수 있다. DRAM 관리는 SSD-ID 및 패킷 번호로 식별되는 다른 SSD에서 들어오는 DRAM 데이터와 일반 테이 블 인덱스로 식별되는 공간 부족 DRAM의 결과로 나가는 패킷 등록 리스트와 함께 기존 구조가 포함된다. 추가 하드웨어는 라우팅에 사용되는 멀티플렉서(210, 211)와 패킷 버퍼(213, 214)를 더 포함할 수 있다. 도 5는 본 개시의 실시예에 따른 DRAM 매니저를 도시하고, DRAM 매니저는 DRAM 데이터를 검출하고 복원하기 위 한 들어오고 나가는 패킷 테이블(301, 302)과 함께 종래의 DRAM 매니저를 포함할 수 있다. ● 링크 케이블 링 토폴로지를 상호 연결하려면 N개의 SSD 사이의 N개의 케이블이 필요하다. 무선 통신을 사용하여 링크를 설정 할 수도 있지만, 이러한 연결의 현재 전송 속도와 이러한 연결의 하드웨어/소프트웨어 복잡성은 고속 DRAM 데이 터 전송에 적합하지 않다. B. 입/출력 컨트롤러 허브 관리를 통한 SSD 간 통신 본 개시의 실시예에 따른 대체 방식을 지원하려면, 소프트웨어 수정만이 필요하다. ● 입/출력 컨트롤러 허브 소프트웨어 본 개시의 실시예에 따른 허브 드라이버는 저장 프로토콜 이외에도 들어오는 DRAM 데이터/승인 패킷을 지원할 수 있다. 필요한 변경 사항은 내부 허브 드라이버 소프트웨어에서 구현할 수 있다. ● SSD 컨트롤러 소프트웨어 본 개시의 실시예에 따른 컨트롤러는 위의 섹션 Ⅱ. B에서 설명된 패킷 구성요소와 함께, DRAM 데이터를 외부 입/출력으로 전달할 수 있다. 이러한 수정은 내부 SSD 프로세서 소프트웨어에 의해 구현될 수 있다. 도 6은 본 개시의 실시예에 따른 입/출력 컨트롤러 허브를 통한 대안적인 SSD 간 통신을 예시한다. 도 6은 컨트 롤러로부터 DRAM 공간을 요청하는 SSD-2를 도시하고, 그 후 입/출력 컨트롤러 허브는 SSD-3에 사용 가능한 DRAM을 할당하고 프로토콜에 따라 데이터를 라우팅할 수 있다. 도 6을 참조하면, 도 5 및 도 6에 도시된 입/출력 컨트롤러 허브의 DRAM 관리 소프트웨어와 SSD 컨트롤러의 패킷 송수신 소프트웨어(50 3)에서 구현될 수 있다. 또한, SSD는 요청SSD인 SSD-2와 수신 SSD인 SSD SSD-3를 모두 도시할 수 있다. Ⅳ. 분석 본 개시의 실시예에서, 단일 SSD의 정규화된 종래 대역폭은 DRAM 오버플로우 p의 확률에 따라 달라질 수 있다. 수학식 3"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "SSD는 독립적이므로 AFA의 종래 대역폭은 대역폭의 곱이다. 수학식 4"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "DRAM 공유의 경우, 모든 SSD에 전체 DRAM이 있는 경우에만 성능이 저하된다. 그러나, 모든 SSD는 서로의 메모리 를 사용하기 때문에 오버플로우 p가 발생할 확률이 증가할 수 있다. 본 개시의 실시예에 따른 분석에서는 특정 SSD DRAM이 공간을 벗어나는 경우를 가정하면, 동일한 AFA의 다른 SSD가 독립적인 워크로드에서 오버플로우되지 않는 한 DRAM을 제공할 수 있다. 이 경우, DRAM 오버플로우가 있는 모든 SSD에는 사용 가능한 DRAM 공간으로 활 성화되는 동료 SSD가 필요할 수 있다. 단순화를 위해, 본 개시의 실시예에서는 짝수 개의 SSDs(N)을 가정할 것 이다. DRAM 공유(DS)를 사용하는 AFA에 있는 2개의 SSD의 경우, 둘 다 오버플로우되는 경우에만 성능이 저하된다. 단 일 SSD의 정규화된 성능은 전체 AFA 성능을 N으로 나눈 값이다. 수학식 5"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "4개의 SSD의 경우, 오버플로우가 있는 최대 2개의 SSD는 성능을 변경하지 않는다. SSD 3개가 오버플로우되면, DRAM 공유를 지원할 수 있는 SSD가 하나 남아 있기 때문에 SSD 2개의 성능이 저하된다. 4개의 SSD가 오버플로우 되면, 모든 SSD의 성능이 저하된다. 수학식 6"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "AFA에서 N SSD에 대한 일반화는 수학식 7과 같다. 수학식 7 수학식 8"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "p=10-2~5·10-1, r=0.1, N=2, 4, 8, 16 및 32개의 SSD에 대한 분석 결과는 도 7의 종래의 AFA 아키텍처에 대한 분석 결과와 비교하여 표시되어 있고, 도 7은 정규화된 AFA 대역폭(성능)과 단일 SSD DRAM 오버플로우 확률을 도시한다. AFA에 높은 오버플로우 확률과 SSD 수가 많은 경우 DRAM이 높을수록 성능이 최대 2배 향상되는 것으 로 나타났다. Ⅴ. 시스템 구현 본 개시의 실시예는 다양한 형태의 하드웨어, 소프트웨어, 펌웨어, 특수 목적 프로세스 또는 이들의 조합으로 구현될 수 있다는 것이 이해되어야 한다. 일 실시예에서, 본 개시 내용의 실시예에 따른 읽기/쓰기 요청 및 패 킷 처리 알고리즘은 ASIC(application-Specific Integrated Circuit) 또는 FPGA(Field Programmable Gate Array)와 같은 하드웨어에서 구현될 수 있다. 다른 실시예에서, 본 개시의 실시예에 따른 알고리즘은 컴퓨터 판 독 가능 프로그램 저장 장치에 구현된 유형의 애플리케이션 프로그램으로서 소프트웨어로 구현될 수 있다. 애플 리케이션 프로그램은 임의의 적합한 아키텍처를 포함하는 기계에 업로드 되고 실행될 수 있다. 도 8은 본 개시의 실시예에 따른 읽기/쓰기 요청 및 패킷 처리 알고리즘을 구현하기 위한 시스템의 블록도이다. 도 8에 도시된 바와 같이, 컴퓨터 시스템은 특히 중앙 처리 장치(CPU) 또는 컨트롤러, 메모리 및 입/출력(I/O) 인터페이스를 포함할 수 있다. 컴퓨터 시스템은 일반적으로 I/O 인터페이스를 통해 디스플레이 및 마우스 및 키보드와 같은 다양한 입력 장치에 연결될 수 있다. 지원 회로에는 캐시, 전 원 공급 장치, 클록 회로 및 통신 버스와 같은 회로가 포함될 수 있다. 메모리는 RAM(Random Access Memory), ROM(Read Only Memory), 디스크 드라이브, 테이프 드라이브 또는 이들의 조합을 포함할 수 있다. 본 개시의 실시예에 따른 알고리즘은 신호 소스로부터의 신호를 처리하기 위해 메모리에 저장되고 CPU 또 는 컨트롤러에 의해 실행되는 루틴으로 구현될 수 있다. 이처럼, 컴퓨터 시스템은 본 개시의 루틴 이 실행될 때 특정 목적의 컴퓨터 시스템이 되는 범용 컴퓨터 시스템일 수 있다. 대안적으로, 전술한 바와 같이, 본 개시의 실시예는 신호 소스로부터의 신호를 처리하기 위해 CPU 또는 컨트롤러와 신호 통신하 는 ASIC 또는 FPGA로서 구현될 수 있다. 컴퓨터 시스템은 또한 운영 체제와 마이크로 명령어 코드를 포함할 수 있다. 여기에 설명된 다양한 프로세 스 및 기능은 마이크로 명령 코드의 일부이거나 운영 체제를 통해 실행되는 응용 프로그램(또는 이들의 조합)의 일부일 수 있다. 또한, 추가적인 데이터 저장 장치, 인쇄 장치 등 다양한 기타 주변 장치를 컴퓨터 플랫폼에 연 결할 수 있다. 이상에서와 같이 도면과 명세서에서 예시적인 실시예들이 개시되었다. 본 명세서에서 특정한 용어를 사용하여 실시예들을 설명되었으나, 이는 단지 본 개시의 기술적 사상을 설명하기 위한 목적에서 사용된 것이지 의미 한"}
{"patent_id": "10-2024-0135095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "정이나 특허청구범위에 기재된 본 개시의 범위를 제한하기 위하여 사용된 것은 아니다. 그러므로 본 기술분야의 통상의 지식을 가진 자라면 이로부터 다양한 변형 및 균등한 타 실시예가 가능하다는 점을 이해할 것이다. 따라 서, 본 개시의 진정한 기술적 보호범위는 첨부된 특허청구범위의 기술적 사상에 의해 정해져야 할 것이다.도면 도면1a 도면1b 도면2 도면3a 도면3b 도면4 도면5 도면6 도면7 도면8"}
{"patent_id": "10-2024-0135095", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1a는 종래 실시예에 따른 현재 SSD 간 채널의 아키텍처를 나타내는 블록도이다. 도 1b는 본 개시의 예시적 실시예에 따른 SSD들 간의 새로운 연결을 나타내는 블록도이다. 도 2는 본 개시의 예시적 실시예에 따른 SSD 간 패킷 구조를 나타내는 도면이다. 도 3a 및 도 3b는 본 개시의 예시적 실시예에 따른 입/출력 컨트롤러 허브에 의해 DRAM 할당이 관리되는 연결을 나타내는 블록도이다. 도 4는 본 개시의 예시적 실시예에 따른 SSD 간 컨트롤러의 데이터 흐름을 나타내는 블록도이다. 도 5는 도 4의 DRAM 매니저의 구조를 나타내는 블록도이다. 도 6은 본 개시의 예시적 실시예에 따른 입/출력 컨트롤러 허브를 통한 대안적인 SSD 간 통신을 나타내는 블록 도이다. 도 7은 본 개시의 예시적 실시예에 따른 정규화된 AFA 대역폭 대 단일 SSD DRAM 오버플로우 확률의 그래프를 도 시한 도면이다. 도 8은 본 개시의 예시적 실시예에 따른 읽기/쓰기 요청 및 패킷 처리 알고리즘을 구현하기 위한 시스템의 블록 도이다."}
