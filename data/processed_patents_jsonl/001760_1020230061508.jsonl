{"patent_id": "10-2023-0061508", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0082157", "출원번호": "10-2023-0061508", "발명의 명칭": "로봇 팔의 제어를 위한 객체 위치 인식방법 및 장치", "출원인": "충북대학교 산학협력단", "발명자": "황영배"}}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇 팔의 객체 위치 인식방법에 있어서,딥러닝 객체검출 모델을 이용하여 객체 영상의 프레임에서 대상 객체를 검출하고 대상 객체 영역에 대한 검출정보를 획득하는 단계;상기 대상 객체 영역 중에서 미리 설정된 부분영역에 대한 깊이 정보를 획득하는 단계;상기 깊이 정보에 대한 이상 발생 여부를 판단하고, 판단 결과에 따라 상기 로봇 팔로부터 상기 대상 객체까지의 거리를 결정하는 단계; 및 상기 대상 객체 영역에 대한 검출정보를 기초로 상기 로봇 팔에서의 상기 대상 객체의 방향을 결정하는 단계를 포함하는 객체 위치 인식방법."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 객체 영상은,상기 로봇 팔에 구비된 뎁스 카메라를 이용하여 촬영한 영상인, 객체 위치 인식방법."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 미리 설정된 부분영역에 대한 깊이 정보는,상기 객체 영상의 프레임상에서 상기 미리 설정된 부분영역에 대응되는 복수의 픽셀위치에 대한 복수의 개별 깊이값을 포함하는, 객체 위치 인식방법."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 깊이 정보에 대한 이상 발생 여부를 판단하고, 판단 결과에 따라 상기 로봇 팔로부터 상기 대상 객체까지의 거리를 결정하는 단계는,상기 깊이 정보에 대한 이상이 발생하지 않았다고 판단되는 경우,상기 미리 설정된 부분영역에 대한 깊이 정보를 기초로 상기 로봇 팔로부터 상기 대상 객체까지의 거리를 결정하는 단계인, 객체 위치 인식방법."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서,상기 깊이 정보에 대한 이상 발생 여부를 판단하고, 판단 결과에 따라 상기 로봇 팔로부터 상기 대상 객체까지의 거리를 결정하는 단계는,상기 깊이 정보에 대한 이상이 발생하였다고 판단되는 경우,상기 대상 객체 영역에 대한 검출정보를 기초로 상기 대상 객체 영역의 기준크기에 대한 상대적인 크기를 결정하는 단계;상기 대상 객체 영역의 기준크기에 대응되는 거리값 및 상기 상대적인 크기를 기초로 상기 로봇 팔로부터 상기공개특허 10-2024-0082157-3-대상 객체까지의 거리를 결정하는 단계를 포함하는, 객체 위치 인식방법."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3항에 있어서,상기 깊이 정보에 대한 이상 발생 여부를 판단하는 단계는,상기 미리 설정된 부분영역에 대응되는 복수의 픽셀위치에 대한 복수의 개별 깊이값의 분포와 미리 저장된 정상깊이값 분포간의 차이를 계산하는 단계; 및상기 차이가 미리 설정된 임계값 이상이면 상기 깊이 정보에 대한 이상이 발생하였다고 판단하는 단계를 포함하는, 객체 위치 인식방법."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제3항에 있어서,상기 깊이 정보에 대한 이상 발생 여부를 판단하는 단계는,상기 객체 영상의 프레임의 전체 영역에 대응되는 복수의 픽셀위치에 대한 복수의 개별 깊이값 중에서 가장 큰분포의 깊이값에 대응되는 어느 하나의 기준픽셀을 결정하는 단계; 상기 기준픽셀 주변의 미리 설정된 픽셀영역에 대한 픽셀 중에서 0이 아닌 개별 깊이값을 갖는 픽셀의 수를 획득하는 단계;상기 객체 영상의 프레임의 전체 영역에 대응되는 복수의 픽셀위치의 개수에 대한 상기 픽셀의 수의 비율이 미리 설정된 비율 미만이면 상기 깊이 정보에 대한 이상이 발생하였다고 판단하는 단계를 포함하는, 객체 위치 인식방법."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 대상 객체 영역에 대한 검출정보는,상기 객체 영상의 프레임 상에서 상기 대상 객체 영역에 대한 바운딩박스의 중심점 좌표정보 및 상기 대상 객체영역에 대한 바운딩박스의 크기정보를 포함하는, 객체 위치 인식방법."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제3항에 있어서,상기 대상 객체 영역에 대한 검출정보를 기초로 상기 로봇 팔에서의 상기 대상 객체의 방향을 결정하는 단계는,상기 대상 객체 영역에 대한 바운딩박스의 중심점 좌표정보를 기초로 상기 객체 영상의 프레임의 중심점에 대한상기 바운딩박스의 중심점의 상대 픽셀거리를 계산하는 단계; 및상기 상대 픽셀거리에 대응되는 시야각을 기초로 상기 대상 객체의 방향을 결정하는 단계를 포함하는, 객체 위치 인식방법."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "컴퓨터 프로그램을 저장하고 있는 컴퓨터 판독 가능 기록매체로서,상기 컴퓨터 프로그램은,제1항 내지 제9항 중 어느 한 항에 따른 방법을 프로세서가 수행하도록 하기 위한 명령어를 포함하는, 컴퓨터판독 가능한 기록매체."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "하나 이상의 인스트럭션을 저장하는 메모리; 및 공개특허 10-2024-0082157-4-상기 메모리에 저장된 상기 하나 이상의 인스트럭션을 실행하는 프로세서를 포함하되, 상기 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써,딥러닝 객체검출 모델을 이용하여 객체 영상의 프레임에서 대상 객체를 검출하여 대상 객체 영역에 대한 검출정보를 획득하고,상기 대상 객체 영역 중에서 미리 설정된 부분영역에 대한 깊이 정보를 획득하고,상기 깊이 정보에 대한 이상 발생 여부를 판단하고, 판단 결과에 따라 로봇 팔로부터 상기 대상 객체까지의 거리를 결정하고,상기 대상 객체 영역에 대한 검출정보를 기초로 상기 로봇 팔에서의 상기 대상 객체의 방향을 결정하는객체 위치 인식장치."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 미리 설정된 부분영역에 대한 깊이 정보는,상기 객체 영상의 프레임상에서 상기 미리 설정된 부분영역에 대응되는 복수의 픽셀위치에 대한 복수의 개별 깊이값을 포함하는, 객체 위치 인식방법."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 프로세서는,상기 미리 설정된 부분영역에 대응되는 복수의 픽셀위치에 대한 복수의 개별 깊이값의 분포와 미리 저장된 정상깊이값 분포간의 차이를 계산하고,상기 차이가 미리 설정된 임계값 이상이면 상기 깊이 정보에 대한 이상이 발생하였다고 판단하는, 객체 위치 인식장치."}
{"patent_id": "10-2023-0061508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서,상기 프로세서는,상기 미리 설정된 부분영역에 대응되는 복수의 픽셀위치에 대한 복수의 개별 깊이값 중에서 가장 큰 분포의 깊이값에 대응되는 어느 하나의 기준픽셀을 결정하고, 상기 기준픽셀 주변의 미리 설정된 픽셀영역에 대한 픽셀 중에서 0이 아닌 개별 깊이값을 갖는 픽셀의 수를 획득하고,상기 복수의 픽셀위치의 개수에 대한 상기 픽셀의 수의 비율이 미리 설정된 비율 미만이면 상기 깊이 정보에 대한 이상이 발생하였다고 판단하는, 객체 위치 인식장치."}
{"patent_id": "10-2023-0061508", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "로봇 팔의 객체 위치 인식방법 및 장치를 개시한다. 본 개시의 일 측면에 의하면, 딥러닝 객체검출 모델을 이용하여 객체 영상의 프레임에서 대상 객체를 검출하고 대상 객체 영역에 대한 검출정보를 획득하는 단계; 상기 대상 객체 영역 중에서 미리 설정된 부분영역에 대한 깊 이 정보를 획득하는 단계; 상기 깊이 정보에 대한 이상 발생 여부를 판단하고, 판단 결과에 따라 상기 로봇 팔로 부터 상기 대상 객체까지의 거리를 결정하는 단계; 및 상기 대상 객체 영역에 대한 검출정보를 기초로 상기 로봇 팔에서의 상기 대상 객체의 방향을 결정하는 단계를 포함하는, 객체 위치 인식방법을 제공한다."}
{"patent_id": "10-2023-0061508", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 로봇 팔의 제어를 위한 객체 위치 인식방법 및 장치에 관한 것으로, 더 자세하게는, 로봇 팔이 파지 하고자 하는 대상 객체의 위치를 정확하게 인식하는 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0061508", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이하에 기술되는 내용은 단순히 본 실시예와 관련되는 배경 정보만을 제공할 뿐 종래기술을 구성하는 것이 아니 다. 사물인터넷(Internet of Things, IoT), 로봇, 인공지능(Artificial Intelligence, AI) 등 ICT(Information and Communications Technology)의 발전에 따라, 산업 자동화 시스템이 공장, 농장 등과 같은 다양한 산업 현장의 자동화 시스템에서 이용되고 있다. 이러한 자동화 시스템에서는 기존 인간 노동력에 의존하던 다양한 작업을 로봇 팔이 효율적으로 수행한다. 예를 들면, 로봇 팔은 특정한 대상 객체를 용접하거나, 파지하여 지정된 장소로 옮기는 등의 작업을 수행한다. 여기 서, 로봇 팔이 높은 정밀도와 효율로 작업을 수행하기 위해서는 작업 대상 객체의 위치를 정확하게 인식할 필요 가 있다. 정확한 객체 위치 인식을 위하여, 로봇 팔에서 카메라 및 영상처리 기술이 로봇 팔에 이용될 수 있다. 예를 들 면, 로봇 팔에 장착된 뎁스 카메라를 통하여 객체 촬영 이미지의 깊이 정보를 획득하고, 깊이 정보를 기초로 로 봇 팔과 대상 객체간의 거리를 계산함으로써 대상 객체의 위치정보를 획득할 수 있다. 그러나, 카메라를 이용한 영상처리 기반의 객체 인식 방법은 주변 환경 상황에 따라 정확도가 크게 저하될 수 있는 문제가 있다. 예를 들면, 객체 주변에 발생한 잡음, 빛 반사, 조명 간섭 등 다양한 환경적 요인으로 인하 여 객체 촬영 이미지상에 블러(blur)와 같은 아티팩트가 발생할 수 있으며, 이러한 아티팩트로 인하여 객체의 정확한 검출이 어려워질 수 있다. 특히, 뎁스 카메라는 대상 객체가 카메라로부터 근거리에 위치할수록 깊이 정 보의 정확도가 저하되는 문제점이 있다."}
{"patent_id": "10-2023-0061508", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는, 딥러닝 객체검출 모델을 이용하여 검출된 대상 객체에 대한 깊이 정보에 이상 발생 여부를 판단하며, 깊이 정보에 이상이 발생한 경우, 딥러닝 객체검출 모델을 이용하여 획득한 대상 객체의 바운딩 박스 에 관한 정보를 기초로 대상 객체의 위치를 정확하게 인식할 수 있는 로봇 팔의 객체 위치 인식방법 및 장치를 제공하는 데 일 목적이 있다. 본 발명이 해결하고자 하는 과제들은 이상에서 언급한 과제들로 제한되지 않으며, 언급되지 않은 또 다른 과제 들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0061508", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면에 의하면, 로봇 팔의 객체 위치 인식방법에 있어서, 딥러닝 객체검출 모델을 이용하여 객체 영상의 프레임에서 대상 객체를 검출하고 대상 객체 영역에 대한 검출정보를 획득하는 단계; 상기 대상 객체 영 역 중에서 미리 설정된 부분영역에 대한 깊이 정보를 획득하는 단계; 상기 깊이 정보에 대한 이상 발생 여부를 판단하고, 판단 결과에 따라 상기 로봇 팔로부터 상기 대상 객체까지의 거리를 결정하는 단계; 및 상기 대상 객 체 영역에 대한 검출정보를 기초로 상기 로봇 팔에서의 상기 대상 객체의 방향을 결정하는 단계를 포함하는 객 체 위치 인식방법을 제공한다. 본 개시의 또 다른 측면에 의하면, 하나 이상의 인스트럭션을 저장하는 메모리; 및 상기 메모리에 저장된 상기 하나 이상의 인스트럭션을 실행하는 프로세서를 포함하되, 상기 프로세서는, 상기 하나 이상의 인스트럭션을 실 행함으로써, 딥러닝 객체검출 모델을 이용하여 객체 영상의 프레임에서 대상 객체를 검출하고 대상 객체 영역에 대한 검출정보를 획득하고, 상기 대상 객체 영역 중에서 미리 설정된 부분영역에 대한 깊이 정보를 획득하고, 상기 깊이 정보에 대한 이상 발생 여부를 판단하고, 판단 결과에 따라 상기 로봇 팔로부터 상기 대상 객체까지 의 거리를 결정하고, 상기 대상 객체 영역에 대한 검출정보를 기초로 상기 로봇 팔에서의 상기 대상 객체의 방 향을 결정하는 객체 위치 인식장치를 제공한다."}
{"patent_id": "10-2023-0061508", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 실시예에 의하면, 대상 객체의 위치나 대상 객체 주변의 외부 환경 요인으로 인하여 대상 객체에 대 한 깊이 정보의 정확도가 저하되는 경우에도 대상 객체의 정확한 위치정보를 획득할 수 있으며, 로봇 팔의 정확 한 제어가 가능한 효과가 있다. 본 개시의 효과들은 이상에서 언급한 효과들로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재 로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0061508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 일부 실시예들을 예시적인 도면을 이용해 상세하게 설명한다. 각 도면의 구성 요소들에 참조 부호를 부가함에 있어서, 동일한 구성 요소들에 대해서는 비록 다른 도면 상에 표시되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 개시를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략한다. 본 개시에 따른 실시예의 구성요소를 설명하는 데 있어서, 제1, 제2, i), ii), a), b) 등의 부호를 사용할 수 있다. 이러한 부호는 그 구성요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 부호에 의해 해당 구성요소의 본질 또는 차례나 순서 등이 한정되지 않는다. 명세서에서 어떤 부분이 어떤 구성요소를 '포함' 또는 '구비'한 다고 할 때, 이는 명시적으로 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 첨부된 도면과 함께 이하에 개시될 상세한 설명은 본 개시의 예시적인 실시형태를 설명하고자 하는 것이며, 본 개시가 실시될 수 있는 유일한 실시형태를 나타내고자 하는 것이 아니다. 도 1은 본 개시의 일 실시예에 따른 객체 위치 인식장치를 개략적으로 나타낸 블록구성도이다. 도 1을 참조하면, 객체 위치 인식장치는 카메라부(camera unit, 110), 입출력 인터페이스(input-output interface, 120), 프로세서(processor, 130) 및 메모리(memory, 140) 중 일부 또는 전부를 포함할 수 있다. 객 체 위치 인식장치는 로봇 팔의 제어장치의 일부로서 탑재되는 컴퓨팅 장치뿐만 아니라 로봇 팔에 장착되는 카메라 모듈에 포함되는 컴퓨팅 장치일 수도 있다. 도 1에 도시된 모든 블록은 본 개시의 실시 예들과 관련된 구성요소들만이 도시되어 있으며, 객체 위치 인식장 치의 필수 구성요소는 아니다. 다른 실시예에서, 객체 위치 인식장치에 포함된 일부 블록이 변경 또 는 삭제되거나, 또 다른 구성요소 블록이 객체 위치 인식장치에 더 포함될 수 있다. 카메라부는 로봇 팔의 주변에 대한 영상을 촬영한다. 여기서, 카메라부는 파지하고자 하는 객체를 인 식할 수 있도록 로봇 팔의 미리 설정된 위치, 예를 들면 로봇 팔의 그립퍼(gripper) 상단의 미리 설정된 위치에 설치될 수 있다. 일 실시예에 따라, 카메라부는 로봇 팔의 주변에 대한 영상 및 깊이정보를 획득할 수 있는 하나 이상의 뎁 스 카메라를 포함할 수 있다. 예를 들면, 카메라부는 객체 영상 데이터뿐만 아니라 해당 영상의 프레임상 의 복수의 지점에 대응되는 깊이 정보를 획득할 수 있다. 여기서, 뎁스 카메라는 촬영된 영상 및 영상 내 미리 설정된 복수의 지점에 대한 거리정보를 생성할 수 있도록 스테레오(stereo) 방식, ToF(Time of Flight) 방식 및 구조광(structured light) 방식 중 적어도 하나로 구현된 카메라일 수 있다. 입출력 인터페이스는 카메라부 또는 외부의 장치와 연결되어 데이터 입출력을 위한 인터페이스를 제 공한다. 예를 들면, 입출력 인터페이스는 카메라부가 획득한 객체 영상 데이터 및 깊이정보 데이터를 프로세서에 전송할 수 있다. 입출력 인터페이스는 키보드, 마우스 또는 터치 인터페이스 등과 같은 입력장치, 디스플레이 등과 같은 출 력 장치를 포함하는 외부 장치와의 인터페이스를 제공할 수도 있다. 예를 들면, 사용자는 입출력 인터페이스에 통신적으로 연결된 별도의 컴퓨터를 통해 객체 위치 인식장치가 인식한 객체정보나 로봇 팔의 파 지동작 과정을 확인할 수 있다. 여기서, 객체 위치 인식장치가 인식한 객체정보는 객체 영상의 특정 프레 임 상에 존재하는 객체 영역을 시각화한 정보, 또는 프로세서가 결정한 객체의 위치를 수치화한 정보 등을 포함할 수 있다. 입출력 인터페이스는 통신 연결을 구성하기 위하여 네트워크 인터페이스 카드, 네트워크 인터페이스 칩 및 네트워킹 인터페이스 포트 등과 같은 하드웨어 모듈 또는 네트워크 디바이스 드라이버(driver) 또는 네트워킹 프로그램과 같은 소프트웨어 모듈을 더 포함할 수 있다. 프로세서는 적어도 하나의 명령어들을 실행할 수 있는 적어도 하나의 코어를 포함할 수 있다. 프로세서 는 단일 프로세서 또는 복수의 프로세서들일 수 있다. 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행할 수 있다. 프로세서에는 카메라부가 획득한 객체 영상 데이터가 전송된다. 여기서, 객체 영상은 로봇 팔이 파지 하고자 하는 하나 이상의 대상 객체를 포함하는 로봇 팔 전방의 촬영 영상일 수 있다. 프로세서는 딥러닝 객체검출 모델을 이용하여 객체 영상의 특정 프레임에서 대상 객체를 검출하고 대상 객 체 영역에 대한 검출정보를 생성할 수 있다. 여기서, 딥러닝 객체검출 모델은 다양한 위치에 존재하는 대상 객 체를 포함하는 다수의 이미지 데이터를 학습 데이터로 이용하여, 객체 영상 프레임에 존재하는 대상 객체를 검 출할 수 있도록 미리 학습된 딥러닝 알고리즘 기반의 신경망 모델일 수 있다. 미리 학습된 딥러닝 객체검출 모델은 하나의 특정 대상 객체에 대한 영역을 검출할 수 있도록 학습될 수 있으나 이에 한정되는 것은 아니며, 프레임상에서 미리 설정된 복수의 대상 객체에 해당하는 영역을 검출하고, 해당 영 역의 객체가 미리 학습된 복수의 서로 다른 객체 클래스 중에서 어느 클래스에 해당하는지를 예측할 수 있는 딥 러닝 알고리즘 기반의 객체검출모델을 포함할 수 있다. 예를 들면, 딥러닝 객체검출 모델은 R-CNN(Region with Convolutional Neural Network), Fast R-CNN, Faster R-CNN, Mask R-CNN, YOLO(You Only Look Once) 및 SSD 등과 같은 딥러닝 객체검출 신경망 모델을 포함할 수 있 다. 이 외에도 딥러닝 객체검출 모델이 다양한 알고리즘 기반의 딥러닝 객체검출 모델을 포함할 수 있음은 해당"}
{"patent_id": "10-2023-0061508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야의 통상의 기술자에게 자명한 것이다. 프로세서는 객체 검출모델이 출력한 대상 객체에 대한 정보를 기초로 대상 객체 영역에 대한 검출정보를 생성한다. 여기서, 객체 검출모델이 출력한 대상 객체 영역에 대한 검출정보는 검출된 대상 객체에 해당하는 영 역에 대한 위치 및 크기 데이터일 수 있다. 예를 들면, 검출정보는 검출된 대상 객체 영역에 대한 바운딩 박스 의 중심점 좌표와 같은 위치정보, 높이 및 너비와 같은 크기정보를 포함할 수 있다. 프로세서는 카메라부가 획득한 객체 영상의 프레임에 대한 깊이정보 데이터를 기초로, 대상 객체 영 역 중에서 미리 설정된 부분영역에 대한 깊이 정보를 획득한다. 객체 영상의 프레임에 대한 깊이 정보 중에서 대상 객체 영역에 해당하는 깊이 정보를 이용하면 객체까지의 거 리정보를 계산할 수 있다. 프로세서는 대상 객체 영역 중에서 미리 설정된 부분영역에 대응되는 깊이 정보 를 획득한다. 여기서, 미리 설정된 부분영역은 대상 객체 영역에 포함된 일부분, 예를 들면, 중심점 근처의 일 부분 영역일 수 있으나 이에 한정되는 것은 아니다. 다른 실시예에 따라, 프로세서는 대상 객체 영역 전체 에 대응되는 깊이 정보를 획득할 수 있다. 구체적으로, 프로세서는 객체 영상 프레임을 구성하는 복수의 픽셀 각각에 대응되는 깊이 정보 중에서 대 상 객체 영역에 포함된 미리 설정된 부분영역에 해당하는 영역의 하나 이상의 픽셀에 대응되는 깊이 정보를 획 득할 수 있다. 예를 들면, 프로세서는 대상 객체 영역의 중심점 주변의 가로 5 픽셀 및 세로 5 픽셀로 구 성된 픽셀영역에 대한 깊이 정보, 즉 총 25 개의 픽셀에 대한 깊이값을 포함하는 매트릭스 데이터를 획득할 수 있다. 이러한 깊이 정보는 로봇 팔의 주변 환경 상황에 따라 정확도가 크게 저하될 수 있다. 예를 들면, 대상 객체와 가까운 위치에 강한 조명 빛이 존재하거나, 대상 객체의 표면 재질로 인하여 깊이 정보가 과도하게 큰 값으로 나타나거나 0 의 값으로 나타나는 등의 측정 오류가 발생할 수 있다. 특히, 대상 객체가 로봇 팔로부터 근거리 에 위치할수록 이러한 측정 오류가 더 크고 빈번하게 나타날 수 있다. 프로세서는 깊이 정보에 대한 이상 발생 여부를 판단하고, 깊이 정보를 이용하여 대상 객체까지의 거리를 결정할 수 있는지 여부를 결정한다. 구체적으로, 획득한 깊이 정보가 측정 오류 없는 신뢰할 수 있는 정보로 판단되면 미리 설정된 부분영역에 대한 깊이 정보를 기초로 로봇 팔로부터 상기 대상 객체까지의 거리를 결정한다. 그러나, 획득한 깊이 정보에 측정 오류가 발생하여 신뢰하기 어렵다고 판단되는 경우에는, 깊이 정보 대신 객체 영역에 대한 검출정보를 이용하여 대상 객체까지의 거리를 결정한다. 일 실시예에 따라, 프로세서는 미리 설정된 부분영역에 대응되는 복수의 픽셀위치에 대한 깊이값의 분포를 이용하여 깊이 정보에 대한 이상 발생 여부를 판단한다. 구체적으로, 프로세서는 미리 설정된 부분영역에 대응되는 복수의 픽셀위치에 대한 복수의 개별 깊이값의 분포와 미리 저장된 정상인 경우의 깊이값 분포간의 차 이를 계산한다. 여기서, 프로세서는 쿨백 라이블러 발산(Kullback Leibler-divergence) 또는 젠슨 새넌 발산(Jensen shannon-divergence)등을 이용하여 두 깊이값 분포간의 차이를 계산할 수 있다. 깊이 정보에 측정 오류에 의한 이상이 발생하면 깊이 정보의 깊이값이 이상 분포를 나타내므로, 두 깊이값 분포 간의 차이는 커진다. 따라서, 프로세서는 두 깊이값 분포간의 차이가 미리 설정된 임계값 이상이면, 측정 오류에 의하여 깊이값의 이상 분포 깊이 정보에 이상이 발생하였다고 판단하고, 임계값 미만이면 정상이라고 판 단한다. 다른 실시예에 따라, 프로세서는 객체 영상의 프레임의 전체 영역에 대한 개별 픽셀위치의 깊이값들의 분 포의 최대값의 픽셀위치 근처에 발생한 이상분포를 검출하여 깊이 정보에 대한 이상 발생 여부를 판단한다. 구체적으로, 프로세서는 객체 영상의 프레임의 전체 영역에 대응되는 복수의 픽셀위치에 대한 복수의 개별 깊이값을 획득하고, 그 중에서 가장 큰 분포의 최대값에 대응되는 어느 하나의 픽셀위치를 기준픽셀로 결정한다. 깊이 정보에 측정 오류에 의한 이상이 발생하는 부분은 대상 객체의 일부 영역에 한정되거나, 객체 영상의 프레 임의 전체 영역에 걸쳐서 나타날 수 있다. 예를 들면, 객체 영상의 프레임의 전체 영역 중에서 개별 픽셀에 대 한 깊이값의 이상 분포는 대상 객체 영역에는 경미한 수준으로 나타나는 반면, 대상 객체 영역 밖의 영역 또는 대상 객체 영역과 일부 중첩되는 영역에서 두드러지게 나타날 수 있다. 또는, 대상 객체 영역에 해당하는 개별 픽셀에 대한 깊이값의 분포는 정상분포를 보이나, 그 깊이값들이 모두 잘못 측정된 값일 수 있다. 따라서, 프로세서는 객체 영상의 프레임의 전체 영역에 대응되는 모든 픽셀위치 중에서 가장 큰 깊이값의 분포의 최대값을 나타내는 어느 하나의 픽셀위치를 기준픽셀로 결정하고, 그 주변부에 발생한 이상분포를 검출 한다. 프로세서는 기준픽셀 주변의 미리 설정된 픽셀영역에 대한 픽셀 중에서 0이 아닌 개별 깊이값을 갖는 픽셀 의 수를 획득한다. 여기서, 미리 설정된 픽셀영역은 기준픽셀위치를 중심으로 하는 미리 설정된 개수의 픽셀을 포함하는 영역일 수 있다. 예를 들면, 미리 설정된 픽셀영역은 대상 객체 영역 중에서 미리 설정된 부분영역과 같은 크기로 설정될 수 있으나, 이에 한정되는 것은 아니며, 더 넓은 픽셀영역으로 설정될 수 있다. 프로세서는 미리 설정된 픽셀영역에 포함된 전체 픽셀 중에서 깊이값이 0인 픽셀을 제외한 나머지 픽셀들 의 수를 획득한다. 여기서, 프로세서는 깊이값이 0인 픽셀을 제외하고 있으나, 다른 실시예에 따라, 미리 설정된 깊이값, 즉, 0의 깊이값을 갖는 것과 동일한 의미로 볼 수 있는 0보다 큰 임의의 값 미만의 픽셀을 제외 하도록 설정될 수 있다. 프로세서는 객체 영상의 프레임의 전체 영역에 대응되는 복수의 픽셀위치의 개수에 대한 깊이값이 0인 픽 셀을 제외한 나머지 픽셀들의 수의 비율을 계산한다. 최대의 깊이값을 나타내는 어느 하나의 픽셀에 인접한 다 른 픽셀에 대한 깊이값이 0으로 나타나는 경우는 이례적이며, 픽셀 차원에서의 깊이값은 점진적으로 변화한다. 따라서, 미리 설정된 픽셀영역 내에서 0의 깊이값을 나타내는 픽셀은 일정 수준 이상 나타나기 어려우며, 0의 깊이값을 나타내는 픽셀이 많이 나타날수록 깊이 정보에 대한 측정 오류가 발생하였을 가능성은 높아진다. 프로세서는 계산된 나머지 픽셀들에 대한 수의 비율이 미리 설정된 비율 미만이면, 0의 깊이값을 나타내 는 픽셀이 비정상적으로 많이 발생하였다고 판단하고, 깊이 정보에 대한 이상이 발생하였다고 최종적으로 판단 한다. 깊이 정보에 대한 이상이 발생하지 않았다고 판단되는 경우, 프로세서는 상기 미리 설정된 부분영역에 대 한 깊이 정보를 기초로 로봇 팔로부터 대상 객체까지의 거리를 결정한다. 예를 들면, 미리 설정된 부분영역에 대응되는 복수의 깊이 값의 평균값을 기초로 대상 객체까지의 거리를 결정할 수 있다. 깊이 정보에 대한 이상이 발생하였다고 판단되는 경우, 프로세서는 대상 객체 영역에 대한 검출정보를 기 초로 대상 객체 영역의 기준크기에 대한 상대적인 크기를 결정한다. 구체적으로, 프로세서는 대상 객체 영역에 대한 검출정보에 포함된 크기정보, 즉, 대상 객체 영역 바운딩 박스의 높이 및 너비정보를 기초로 바운딩 박스의 크기를 계산한다. 프로세서는 대상 객체 영역 바운딩박스의 크기와 미리 저장된 대상 객체 영역의 기준크기와의 비율을 계산 한다. 여기서, 미리 저장된 대상 객체 영역의 기준크기는 대상 객체가 기준위치에 존재하는 경우의 바운딩 박스 크기이다. 예를 들면, 대상 객체와 동일한 객체가 로봇 팔 전방의 30 cm 지점에 존재하는 경우 검출된 대상 객 체 영역 바운딩박스의 크기가 기준 크기로서 미리 저장될 수 있다. 프로세서는 대상 객체 영역의 기준크기에 대응되는 거리값 및 기준크기에 대한 상대적인 크기를 기초로 로 봇 팔로부터 대상 객체까지의 거리를 결정한다. 즉, 대상 객체 영역의 크기와 거리간의 선형적인 관계를 이용하 여 검출된 대상 객체의 거리를 추정할 수 있다. 예를 들면, 기준 크기에 대응되는 거리값이 30 cm일때, 계산된 대상 객체 영역의 바운딩 박스 크기가 기준크기보다 크면 대상 객체의 거리는 30 cm보다 작은 거리로 결정된다. 반면에, 계산된 대상 객체 영역의 바운딩 박스 크기가 기준크기보다 작으면 대상 객체의 거리는 30 cm보다 먼 거리로 결정된다. 여기서, 프로세서가 바운딩 박스의 상대적 크기를 이용하여 결정한 대상 객체까지의 거 리는 대상 객체가 존재하는 지점을 포함하는 평면, 즉, 객체 평면까지의 수직 거리일 수 있다. 로봇 팔로부터 상기 대상 객체까지의 거리가 결정되면, 프로세서는 대상 객체 영역에 대한 검출정보를 기 초로 상기 로봇 팔에서의 상기 대상 객체의 방향을 결정한다. 카메라부의 카메라는 카메라의 시야각(field of view)에 해당하는 범위의 영상을 촬영한다. 따라서, 객체 영상의 프레임은 카메라의 광학축을 기준으로 수평방향 및 수직방향 각각의 시야각에 대응되는 뷰를 포함한다. 여기서, 시야각은 카메라의 광학 구성에 따라 달라질 수 있으며, 동일 카메라에서 촬영되는 프레임의 초점거리 에 따라 달라질 수 있다. 객체 영상의 프레임을 구성하는 단위 픽셀은 각각 수평방향 단위 시야각 및 수직방향 단위 시야각에 해당하는 뷰를 나타낼 수 있다. 따라서, 객체 영상의 프레임 상에서 카메라의 광학축에 대응되는 지점, 즉, 객체 영상의 프레임의 중심점으로부터 어느 하나의 지점간의 픽셀거리를 이용하면 해당 지점의 방향각을 계산할 수 있다. 프로세서는 대상 객체 영역에 대한 바운딩박스의 중심점 좌표정보를 기초로 객체 영상의 프레임의 중심점 에 대한 바운딩박스의 중심점의 상대 픽셀거리를 계산한다. 여기서, 상대 픽셀거리는 수평방향 픽셀거리 및 수 직방향 픽셀거리를 포함할 수 있다. 프로세서는 상대 픽셀거리에 대응되는 시야각을 기초로 대상 객체의 방향을 결정한다. 구체적으로, 프로세 서는 객체 영상의 프레임을 구성하는 단위 픽셀에 대응되는 수평방향 시야각 및 수평방향 픽셀거리를 기초 로 바운딩박스의 중심점의 수평방향 방향각을 결정하고, 단위 픽셀에 대응되는 수직방향 시야각 및 수직방향 픽 셀거리를 기초로 바운딩박스의 중심점의 수직방향 방향각을 결정한다. 프로세서는 로봇 팔로부터 상기 대상 객체까지의 거리 및 대상 객체의 방향각을 기초로 대상 객체의 위치 를 최종적으로 결정할 수 있다. 로봇 팔은 최종적으로 결정된 대상 객체의 위치정보를 기초로 대상 객체의 파지 를 위한 동작을 수행할 수 있다. 예를 들면, 로봇 팔이 지지 고정되어 있는 위치를 중심축으로 하여 회전하며, 다수의 관절부를 이용하여 특정 거리까지 그립퍼를 직선 이동할 수 있는 경우, 로봇 팔은 대상 객체의 수평방향 방향각에 따라 회전한 뒤, 대상 객체까지의 거리 및 수직방향 방향각에 따라 객체의 위치까지 그립퍼를 직선 이 동하여 대상 객체를 파지할 수 있게 된다. 메모리는 객체 위치 인식장치에 의해 사용되거나 그에 의해 출력되는 정보를 저장하기 위한 단일의 메모리 또는 복수의 메모리들일 수 있다. 객체 위치 인식장치에 의해 사용되거나 그에 의해 출력되는 정보 는 단일 메모리에 저장되거나 복수의 메모리들에 나뉘어 저장될 수 있다. 메모리는 휘발성 메모리, 비휘발성 메모리, 가상 또는 다른 종류의 메모리를 포함할 수 있다. 예를 들면, 메모리는 랜덤 액세스 메모리(random access memory, RAM) 또는 다이내믹 RAM(dynamic RAM, DRAM), 플래 시 메모리(flash memory)를 포함할 수 있다. 메모리는 객체 위치 인식장치의 동작을 위한 기본 프로그램, 응용 프로그램, 네트워크 설정 정보 등 을 저장한다. 그리고, 메모리는 프로세서의 요청에 따라 저장된 정보를 프로세서에 제공할 수 있다메모리는 프로세서의 처리 또는 제어를 위한 다양한 데이터를 저장할 수 있다. 예를 들면, 메모리 에는 객체 영상 데이터, 객체 프레임에 대한 깊이 정보 데이터, 객체 영역에 대한 바운딩박스의 기준 크기 데이터, 객체 영상의 프레임으로부터 대상 객체를 검출하기 위한 학습이 완료된 딥러닝 객체검출 모델 등이 저 장될 수 있다. 또한, 메모리는 프로세서가 생성한 다양한 데이터를 저장할 수 있다. 예를 들면, 메모리에는 프 로세서가 생성한 대상 객체의 거리정보, 객체의 수평방향 방향각 정보, 객체의 수직방향 방향각 정보 등이 저장될 수 있다. 도 2는 본 개시의 일 실시예에 따른 객체 위치 인식장치가 객체 영상의 프레임에서 대상 객체에 대한 정보를 획 득하는 과정을 설명하기 위한 도면이다. 도 2를 참조하면, 객체 위치 인식장치는 딥러닝 객체검출 모델을 이용하여 대상 객체 영상의 프레임 상에 존재하는 대상 객체를 검출한다. 객체검출 모델은 대상 객체를 검출하고 객체 영상의 프레임의 전체 영역 중에서 대상 객체에 해 당하는 영역에 대한 바운딩 박스의 중심점의 좌표 및 크기에 관한 정보를 출력한다. 예를 들면, 객체검출 모델은 객체 영상의 프레임 상에 설정된 x-y 좌표평면에서 중심점의 x, y 좌표값, 중심점에서의 바운딩 박스의 폭 및 높이를 출력한다. 객체 위치 인식장치는 중심점의 x, y 좌표 및 바운딩 박스 의 폭과 높이를 기초로 바운딩 박스 영역을 결정할 수 있다. 객체 위치 인식장치는 뎁스 카메라를 이용하여 대상 객체 영역 중에서 미리 설정된 부분영역에 대한 깊이 정보를 획득한다. 여기서, 미리 설정된 부분영역은 중심점 주변의 25 개 픽셀에 대응되는 영역으로 설정될 수 있다. 구체적으로, 객체 위치 인식장치는 대상 객체를 검출하여 중심점의 좌표를 획득하면, 중심점을 포함하는 그 주변영역, 즉, 중심점을 중심으로 한 5 5 픽셀영역에 대한 깊이 값 을 획득할 수 있다. 객체 위치 인식장치는 5 5 픽셀영역에 대한 깊이 값의 분포를 기초로 깊이 정보에 대한 이상 발생 여부를 판단 한다. 깊이 정보에 이상이 발생하지 않은 경우, 객체 위치 인식장치는 깊이 정보, 즉 25 개 픽셀에 대한 깊이값 의 평균값을 기초로 대상 객체까지의 거리를 결정한다. 깊이 정보에 이상이 발생한 경우에는 바운딩 박스 의 기준크기에 대한 상대적 크기 비율을 기초로 대상 객체까지의 거리를 결정한다. 도 3은 본 개시의 일 실시예에 따른 객체 위치 인식장치가 상대 픽셀거리를 계산하는 과정을 설명하기 위한 도 면이다. 도 3을 참조하면, 객체 위치 인식장치는 대상 객체 영상의 프레임 상에서 대상 객체에 해당하는 객체 영역에 대한 바운딩박스의 중심점 좌표정보를 획득한다. 객체 위치 인식장치는 객체 영상의 프레임의 중심점과 바운딩박스의 중심점 간의 수평 방향으로 의 픽셀개수, 즉, 수평방향 상대 픽셀거리를 계산한다. 마찬가지로, 객체 위치 인식장치는 객체 영상의 프 레임의 중심점과 바운딩박스의 중심점 간의 수직 방향으로의 픽셀개수, 즉, 수직방향 상대 픽셀 거리를 계산한다. 객체 영상의 프레임에서, 하나의 픽셀은 단위 픽셀에 대응되는 수평방향 단위 시야각 및 수직방향 단위 시 야각을 가질 수 있다. 이러한 단위 시야각은 객체 영상의 프레임 시야각을 기초로 결정될 수 있다. 예를 들면, 수평방향 상대 픽셀거리가 픽셀이고 수직방향 상대 픽셀거리가 픽셀이라고 가정한 다. 객체 영상의 프레임에서의 수평방향 단위 시야각이 , 수직방향 단위 시야각이 인 경우, 대상 객체 의 수평 방향각은 현재 로봇 팔의 정면 방향으로부터 오른쪽으로 인 각도로 결정되며, 수직 방향각 은 현재 로봇 팔의 정면 방향으로부터 아래쪽으로 인 각도로 결정될 수 있다. 도 4은 본 개시의 일 실시예에 따른 객체 위치 인식장치가 대상 객체의 위치를 인식하는 과정을 설명하기 위한 도면이다. 도 4를 참조하면, 객체 영상 프레임은 xy 좌표평면 상에 존재하고, 객체 영상 프레임의 중심점 은 xy 좌표평면의 원점 상에 존재한다. 로봇 팔의 기준점은 x'y' 좌표평면의 원점 상에 존재한다. 여기서, xy 좌표평면 및 x'y' 좌표평면은 서로 평행하며, 로봇 팔의 기준점에서 xy 좌표평면상으로의 정사영은 객체 영상 프레임의 중심점에 해당한다. 즉, z'축은 로봇 팔의 기준점으로부터 객체 영상 프레임의 중심점으로의 로봇 팔 카 메라의 광학축, 즉, 로봇 팔의 정면 방향축을 의미한다. 객체 위치 인식장치는 대상 객체에 관한 깊이 정보 또는 대상 객체 영역에 대한 검출정보를 기초로 로봇 팔로부터 대상 객체까지의 거리를 결정할 수 있다. 객체 위치 인식장치는 객체 영상의 프레임의 중심점과 바운딩박스의 중심점 간의 수평방향 상대 픽셀거리 및 단위 픽셀에 대한 수평방향 단위 시야각을 기초로 대상 객체의 수평 방향각을 계산하고, 수직방향 상대 픽셀거리 및 단위 픽셀에 대한 수직방향 단위 시야각을 기초로 대상 객체의 수직 방향각 을 계산하여 대상 객체의 방향을 결정할 수 있다. 객체 위치 인식장치는 로봇 팔로부터 상기 대상 객체까지의 거리 및 대상 객체의 방향각을 기초로 대상 객체의 위치를 최종적으로 결정할 수 있다. 구체적으로, 객체 위치 인식장치는 로봇 팔로부터 대상 객체까지의 거리 및 수평 방향각을 기초로, 객체 영상의 프레임의 중심점으로부터 대상 객체의 x축 상에서의 위치까지의 거리 및 로봇 팔의 기준점으로부터 대상 객체의 x축 상에서의 위치까지의 거리 를 각각 계산한다. 이후, 로봇 팔의 기준점으로부터 대상 객체의 x축 상에서의 위치까지의 거리 및 수직 방향각을 기초로 대상 객체의 x축 상에서의 위치로부터 대상 객체의 중심점까지의 거리를 계산할 수 있다. 결과적으로, 객체 위치 인식장치는 객체 영상의 프레임의 중심점으로부터 대상 객체의 바운딩박 스 중심점까지의 실제 거리를 아래의 수학식 1을 이용하여 계산할 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0061508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, 는 객체 영상의 프레임의 중심점으로부터 대상 객체의 바운딩박스 중심점까지 의 실제 수평거리이고, 는 객체 영상의 프레임의 중심점으로부터 대상 객체의 바운딩박스 중 심점까지의 실제 수직거리이다. d는 로봇 팔로부터 대상 객체까지의 거리이고, 는 수평 방향각 , 는 수직 방향각이다. 객체 위치 인식장치는 실제 거리 기반의 대상 객체의 위치좌표를 결정할 수 있다. 예를 들면, x'y'z' 좌표 계 상에서의 원점, 로봇 팔의 기준점에 로봇 팔이 위치한다고 가정하면, 객체 위치 인식장치는 x'y'z' 좌 표계 상에서의 ( , , d)인 지점을 대상 객체의 위치로 결정할 수 있다. 도 5는 본 개시의 일 실시예에 따른 객체 위치 인식방법을 나타내는 흐름도이다. 도 5를 참조하면, 객체 위치 인식장치는 딥러닝 객체검출 모델을 이용하여 객체 영상의 프레임에서 대상 객체를 검출하고 대상 객체 영역에 대한 검출정보를 획득한다(S510). 로봇 팔은 로봇 팔의 미리 설정된 위치에 장착된 카메라를 이용하여 로봇 팔의 주변에 대한 객체 영상을 촬영한 다. 여기서, 카메라는 로봇 팔의 주변에 대한 영상 및 깊이정보를 획득할 수 있는 하나 이상의 뎁스 카메라를 포함할 수 있다. 객체 영상은 로봇 팔이 파지하고자 하는 하나 이상의 대상 객체를 포함하는 로봇 팔 전방의 촬 영 영상이다. 딥러닝 객체검출 모델은 객체 영상 프레임에 존재하는 대상 객체를 검출할 수 있도록 미리 학습된 딥러닝 알고 리즘 기반의 신경망 모델이다. 딥러닝 객체검출 모델은 하나의 특정 대상 객체에 대한 영역을 검출할 수 있도록 학습될 수 있으나 이에 한정되는 것은 아니며, 프레임상에서 미리 설정된 복수의 대상 객체에 해당하는 영역을 검출하고, 해당 영역의 객체가 미리 학습된 복수의 서로 다른 객체 클래스 중에서 어느 클래스에 해당하는지를 예측할 수 있는 딥러닝 알고리즘 기반의 객체검출모델을 포함할 수 있다. 객체 위치 인식장치는 딥러닝 객체 검출모델이 출력한 대상 객체에 대한 정보를 기초로 대상 객체 영역에 대한 검출정보를 생성한다. 여기서, 객체 검출모델이 출력한 대상 객체 영역에 대한 검출정보는 검출된 대상 객체 영 역에 대한 바운딩 박스의 중심점 좌표와 같은 위치정보, 높이 및 너비와 같은 크기정보를 포함할 수 있다. 객체 위치 인식장치는 대상 객체 영역 중에서 미리 설정된 부분영역에 대한 깊이 정보를 획득한다(S520). 객체 영상의 프레임에 대한 깊이 정보 중에서 대상 객체 영역에 해당하는 깊이 정보를 이용하면 객체까지의 거 리정보를 계산할 수 있다. 객체 위치 인식장치는 대상 객체 영역 중에서 미리 설정된 부분영역에 대응되는 깊이 정보를 획득한다. 여기서, 미리 설정된 부분영역은 대상 객체 영역에 포함된 일부분, 예를 들면, 중심점 근처의 일부분 영역일 수 있으나 이에 한정되는 것은 아니다. 다른 실시예에 따라, 객체 위치 인식장치는 대상 객체 영 역 전체에 대응되는 깊이 정보를 획득할 수 있다. 구체적으로, 객체 위치 인식장치는 객체 영상 프레임을 구성하는 전체 픽셀 각각에 대응되는 깊이 정보 중에서 대상 객체 영역에 포함된 미리 설정된 부분영역에 해당하는 영역의 하나 이상의 픽셀에 대응되는 깊이 정보를 획득할 수 있다. 예를 들면, 객체 위치 인식장치는 대상 객체 영역의 중심점 주변의 가로 5 픽셀 및 세로 5 픽 셀로 구성된 픽셀영역에 포함된 각각의 픽셀에 대한 깊이값을 획득한다. 이러한 깊이 정보는 로봇 팔의 주변 환경 상황에 따라 정확도가 크게 저하될 수 있으며, 정확도가 저하된 깊이 정보를 기초로 객체의 위치를 인식하는 경우에는 로봇 팔이 객체를 정확하게 파지하는 것이 불가능하다. 따라서, 객체 위치 인식장치는 획득한 깊이 정보에 정확도를 저하시킬 수 있는 대한 이상 발생 여부를 판단하여 객체의 위치를 인식하기 위하여 신뢰할 수 있는 정보인지 여부를 결정한다. 객체 위치 인식장치는 획득한 깊이 정보가 신뢰할 수 있는 정보로 판단되면 미리 설정된 부분영역에 대한 깊이 정보를 기초로 로봇 팔로부터 상기 대상 객체까지의 거리를 결정하지만, 획득한 깊이 정보를 신뢰하기 어렵다고 판단되는 경우에는, 깊이 정보 대신 객체 영역에 대한 검출정보를 이용하여 대상 객체까지의 거리를 결정한다. 일 실시예에 따라, 객체 위치 인식장치는 미리 설정된 부분영역에 대응되는 복수의 픽셀위치에 대한 깊이값의 분포를 이용하여 깊이 정보에 대한 이상 발생 여부를 판단한다. 구체적으로, 객체 위치 인식장치는 미리 설정된 부분영역에 대응되는 복수의 픽셀위치에 대한 복수의 개별 깊이값의 분포와 미리 저장된 정상인 경우의 깊이값 분포간의 차이를 계산한다. 객체 위치 인식장치는 두 깊이값 분포간의 차이가 미리 설정된 임계값 이상이면, 측정 오류에 의하여 깊이값의 이상 분포 깊이 정보에 이상이 발생하였다고 판단하고, 임계값 미만이면 정상이라고 판단한다. 다른 실시예에 따라, 객체 위치 인식장치는 객체 영상의 프레임의 전체 영역에 대한 개별 픽셀위치의 깊이값들 중에서 최대값의 픽셀위치 근처에 발생한 이상분포를 검출하여 깊이 정보에 대한 이상 발생 여부를 판단한다. 구체적으로, 객체 위치 인식장치는 객체 영상의 프레임의 전체 영역에 대응되는 복수의 픽셀위치에 대한 복수의 개별 깊이값을 획득하고, 그 중에서 가장 큰 깊이값에 대응되는 어느 하나의 픽셀위치를 기준픽셀로 결정한다. 객체 위치 인식장치는 기준픽셀 주변의 미리 설정된 픽셀영역에 대한 픽셀 중에서 0이 아닌 개별 깊이값을 갖는 픽셀의 수를 획득한다. 여기서, 미리 설정된 픽셀영역은 기준픽셀위치를 중심으로 하는 미리 설정된 개수의 픽 셀을 포함하는 영역이다. 객체 위치 인식장치는 미리 설정된 픽셀영역에 포함된 전체 픽셀 중에서 깊이값이 0인 픽셀을 제외한 나머지 픽 셀들의 수를 획득한다. 다른 실시예에 따라, 객체 위치 인식장치는 0의 깊이값을 갖는 것과 동일한 의미로 볼 수 있는 0보다 큰 임의의 값 미만의 픽셀을 제외하도록 설정될 수 있다. 객체 위치 인식장치는 객체 영상의 프레임의 전체 영역에 대응되는 복수의 픽셀위치의 개수에 대한 깊이값이 0 인 픽셀을 제외한 나머지 픽셀들의 수의 비율을 계산한다. 계산된 나머지 픽셀들에 대한 수의 비율이 미리 설정 된 비율 미만이면, 객체 위치 인식장치는 깊이 정보에 대한 이상이 발생하였다고 최종적으로 판단한다. 깊이 정보에 이상이 발생하지 않았다고 판단된 경우, 객체 위치 인식장치는 깊이 정보를 기초로 로봇 팔로부터 대상 객체까지의 거리를 결정한다(S540a). 여기서, 객체 위치 인식장치는 미리 설정된 부분영역에 대응되는 복수의 깊이 값의 평균값을 기초로 대상 객체까지의 거리를 결정할 수 있다. 깊이 정보에 이상이 발생하였다고 판단된 경우, 객체 위치 인식장치는 대상 객체 영역에 대한 검출정보를 기초 로 로봇 팔로부터 대상 객체까지의 거리를 결정한다(S540b). 객체 위치 인식장치는 대상 객체 영역에 대한 검출정보에 포함된 크기정보, 즉, 대상 객체 영역 바운딩박스의 높이 및 너비정보를 기초로 바운딩 박스의 크기를 계산한다. 객체 위치 인식장치는 대상 객체 영역 바운딩박스의 크기와 미리 저장된 대상 객체 영역의 기준크기와의 비율을 계산한다. 여기서, 미리 저장된 대상 객체 영역의 기준크기는 대상 객체가 기준위치에 존재하는 경우에 검출된 대상 객체의 바운딩 박스 크기이다. 객체 위치 인식장치는 대상 객체 영역의 기준크기에 대응되는 거리값 및 기준크기에 대한 상대적인 크기를 기초 로 로봇 팔로부터 대상 객체까지의 거리를 결정한다. 즉, 대상 객체 영역의 크기와 거리간의 선형적인 관계를 이용하여 검출된 대상 객체의 거리를 추정할 수 있다. 객체 위치 인식장치는 대상 객체 영역에 대한 검출정보를 기초로 로봇 팔에서의 대상 객체의 방향을 결정한다 (S550). 객체 영상의 프레임은 카메라의 광학축을 기준으로 수평방향 및 수직방향 각각의 시야각에 대응되는 뷰를 포함 한다. 따라서, 객체 영상의 프레임을 구성하는 단위 픽셀은 각각 수평방향 단위 시야각 및 수직방향 단위 시야 각에 해당하는 뷰를 나타낼 수 있다. 객체 위치 인식장치는 객체 영상의 프레임 상에서 카메라의 광학축에 대응 되는 지점, 즉, 객체 영상의 프레임의 중심점으로부터 대상 객체의 기준위치까지의 픽셀거리를 이용하여 대상 객체의 방향각을 계산할 수 있다. 여기서, 대상 객체의 기준위치는 대상 객체 영역에 대한 바운딩박스의 중심점 으로 설정될 수 있다. 객체 위치 인식장치는 대상 객체 영역에 대한 바운딩박스의 중심점 좌표정보를 기초로 객체 영상의 프레임의 중 심점에 대한 바운딩박스의 중심점의 상대 픽셀거리를 계산한다. 여기서, 상대 픽셀거리는 수평방향 픽셀거리 및 수직방향 픽셀거리를 포함한다. 객체 위치 인식장치는 상대 픽셀거리에 대응되는 시야각을 기초로 대상 객체의 방향을 결정한다. 구체적으로, 객체 위치 인식장치는 객체 영상의 프레임을 구성하는 단위 픽셀에 대응되는 수평방향 시야각 및 수평방향 픽셀 거리를 기초로 바운딩박스의 중심점의 수평방향 방향각을 결정하고, 단위 픽셀에 대응되는 수직방향 시야각 및 수직방향 픽셀거리를 기초로 바운딩박스의 중심점의 수직방향 방향각을 결정한다. 객체 위치 인식장치는 로봇 팔로부터 상기 대상 객체까지의 거리 및 대상 객체의 방향각을 기초로 대상 객체의 위치를 최종적으로 결정할 수 있다. 로봇 팔은 최종적으로 결정된 대상 객체의 위치정보를 기초로 대상 객체의 파지를 위한 동작을 수행할 수 있다. 본 발명에 따른 장치 또는 방법의 각 구성요소는 하드웨어 또는 소프트웨어로 구현되거나, 하드웨어 및 소프트 웨어의 결합으로 구현될 수 있다. 또한, 각 구성요소의 기능이 소프트웨어로 구현되고 마이크로프로세서가 각 구성요소에 대응하는 소프트웨어의 기능을 실행하도록 구현될 수도 있다. 본 명세서에 설명되는 시스템들 및 기법들의 다양한 구현예들은, 디지털 전자 회로, 집적회로, FPGA(field programmable gate array), ASIC(application specific integrated circuit), 컴퓨터 하드웨어, 펌웨어, 소프 트웨어, 및/또는 이들의 조합으로 실현될 수 있다. 이러한 다양한 구현예들은 프로그래밍가능 시스템 상에서 실 행 가능한 하나 이상의 컴퓨터 프로그램들로 구현되는 것을 포함할 수 있다. 프로그래밍가능 시스템은, 저장 시 스템, 적어도 하나의 입력 디바이스, 그리고 적어도 하나의 출력 디바이스로부터 데이터 및 명령들을 수신하고 이들에게 데이터 및 명령들을 전송하도록 결합되는 적어도 하나의 프로그래밍가능 프로세서(이것은 특수 목적 프로세서일 수 있거나 혹은 범용 프로세서일 수 있음)를 포함한다. 컴퓨터 프로그램들(이것은 또한 프로그램들, 소프트웨어, 소프트웨어 애플리케이션들 혹은 코드로서 알려져 있음)은 프로그래밍가능 프로세서에 대한 명령어 들을 포함하며 \"컴퓨터가 읽을 수 있는 기록매체\"에 저장된다. 컴퓨터가 읽을 수 있는 기록매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기 록장치를 포함한다. 이러한 컴퓨터가 읽을 수 있는 기록매체는 ROM, CD-ROM, 자기 테이프, 플로피디스크, 메모 리 카드, 하드 디스크, 광자기 디스크, 스토리지 디바이스 등의 비휘발성(non-volatile) 또는 비일시적인(non- transitory) 매체일 수 있으며, 또한 데이터 전송 매체(data transmission medium)와 같은 일시적인 (transitory) 매체를 더 포함할 수도 있다. 또한, 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터시스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수도 있다. 본 명세서의 흐름도/타이밍도에서는 각 과정들을 순차적으로 실행하는 것으로 기재하고 있으나, 이는 본 개시의 일 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것이다. 다시 말해, 본 개시의 일 실시예가 속하는 기 술 분야에서 통상의 지식을 가진 자라면 본 개시의 일 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 흐 름도/타이밍도에 기재된 순서를 변경하여 실행하거나 각 과정들 중 하나 이상의 과정을 병렬적으로 실행하는 것으로 다양하게 수정 및 변형하여 적용 가능할 것이므로, 흐름도/타이밍도는 시계열적인 순서로 한정되는 것은 아니다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2023-0061508", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 객체 위치 인식장치를 개략적으로 나타낸 블록구성도이다. 도 2는 본 개시의 일 실시예에 따른 객체 위치 인식장치가 객체 영상의 프레임에서 대상 객체에 대한 정보를 획 득하는 과정을 설명하기 위한 도면이다. 도 3은 본 개시의 일 실시예에 따른 객체 위치 인식장치가 상대 픽셀거리를 계산하는 과정을 설명하기 위한 도 면이다. 도 4은 본 개시의 일 실시예에 따른 객체 위치 인식장치가 대상 객체의 위치를 인식하는 과정을 설명하기 위한 도면이다. 도 5는 본 개시의 일 실시예에 따른 객체 위치 인식방법을 나타내는 흐름도이다."}
