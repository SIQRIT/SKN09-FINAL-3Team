{"patent_id": "10-2019-0011748", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0094354", "출원번호": "10-2019-0011748", "발명의 명칭": "버스트 스파이크 기반한 스파이킹 신경망 생성 방법 및 스파이킹 신경망 기반 추론 장치", "출원인": "서울대학교산학협력단", "발명자": "윤성로"}}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터 장치가 학습된 인공신경망 모델의 가중치를 스파이킹 신경망(SNN)에 매핑하는 단계; 및 상기 컴퓨터 장치가 상기 가중치가 매핑된 SNN을 최적화하는 단계를 포함하되,상기 최적화하는 단계는 상기 SNN의 뉴런 상태에 따라 발화를 결정하는 임계값을 조정하는 버스트 스파이크 기반한 스파이킹 신경망 생성 방법."}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 최적화하는 단계는 최적화를 위한 입력 데이터를 상기 SNN에 입력하는 단계;상기 SNN 각 뉴런에 대하여 입력값을 수신한 뉴런이 발화할 상태인 타깃 뉴런인 경우 스파이크를 생성하는단계;상기 타깃 뉴런에 대한 임계값을 조절하는 단계; 및상기 타깃 뉴런에 대하여 상기 가중치를 조절하는 단계를 포함하는 버스트 스파이크 기반한 스파이킹 신경망 생성 방법."}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 SNN은 정보 전달을 위한 버스트 코딩을 사용하되, 상기 버스트 코딩은 아래의 수식으로 표현되는 버스트함수에 따른 버스트 스파이킹을 수행하는 버스트 스파이크 기반한 스파이킹 신경망 생성 방법.(는 시간 t에서 l번째 계층에 있는 뉴런에 대한 버스트 함수, β는 버스트를 위한 상수이고, 는 시간 t-1에서 생성된 스파이크임)"}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 버스트 함수가 적용된 뉴런에 대한 임계값은 아래 수학식으로 결정되는 버스트 스파이크 기반한 스파이킹신경망 생성 방법.( 는 시간 t에서 l번째 계층에 i번째 뉴런에 대한 임계값, 는 해당 뉴런에 이전에 설정된 임계값임)"}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서,상기 버스트 함수가 적용된 뉴런에 대한 가중치는 아래 수학식으로 결정되는 버스트 스파이크 기반한 스파이킹공개특허 10-2020-0094354-3-신경망 생성 방법.(는 시간 t에서 l-1번째 계층에 i번째 뉴런과 l번째 계층에 있는 j 번째 뉴런 사이의 시냅스 가중치,는 해당 뉴런에 대하여 이전에 설정된 가중치)"}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 SNN은 정보 전달을 위한 신경 코딩을 사용하되, 상기 신경 코딩은 특정 뉴런이 입력값에 따라 발화하게 되는 경우 기준값에 따라 상기 특정 뉴런에 대한 임계값을 기준값만큼 낮추고, 상기 기준값에 따라 상기 특정 뉴런에 대한 가중치를 조절하는 버스트 스파이크 기반한 스파이킹 신경망 생성 방법."}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 컴퓨터 장치는 상기 SNN의 정확도가 기준값 이상이 될 때까지 상기 최적화 과정을 반복하는 버스트 스파이크 기반한 스파이킹 신경망 생성 방법."}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "입력 데이터를 입력받는 입력장치;입력값에 따라 상태가 변경되는 뉴런에 대하여 기준값에 따라 상기 뉴런에 대한 임계값을 조절하고, 상기 기준값에 따라 상기 뉴런에 대한 가중치를 조절하는 신경 코딩을 사용하는 스파이킹 신경망(SNN)을 저장하는 저장장치; 및상기 입력 데이터를 상기 SNN에 입력하여 추론을 수행하는 연산장치를 포함하는 스파이킹 신경망 기반 추론 장치."}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 SNN은 학습 데이터로 학습된 DNN의 가중치를 적용한 초기 SNN에 대하여 최적화를 위한 훈련 데이터를 이용하여 상기 초기 SNN의 가중치가 최적화되는 과정을 거쳐 마련되는 스파이킹 신경망 기반 추론 장치."}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서,상기 SNN은 정보 전달을 위한 버스트 코딩을 사용하되, 상기 버스트 코딩은 아래의 수식으로 표현되는 버스트함수에 따른 버스트 스파이킹을 수행하는 스파이킹 신경망 기반 추론 장치.(는 시간 t에서 l번째 계층에 있는 뉴런에 대한 버스트 함수, β는 버스트를 위한 상수이고, 는 시간 t-1에서 생성된 스파이크임)"}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,공개특허 10-2020-0094354-4-상기 버스트 함수가 적용된 뉴런에 대한 임계값은 아래 수학식으로 결정되는 스파이킹 신경망 기반 추론 장치.( 는 시간 t에서 l번째 계층에 i번째 뉴런에 대한 임계값, 는 해당 뉴런에 이전에 설정된 임계값임)"}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,상기 버스트 함수가 적용된 뉴런에 대한 가중치는 아래 수학식으로 결정되는 스파이킹 신경망 기반 추론 장치.(는 시간 t에서 l-1번째 계층에 i번째 뉴런과 l번째 계층에 있는 j 번째 뉴런 사이의 시냅스 가중치,는 해당 뉴런에 대하여 이전에 설정된 가중치)"}
{"patent_id": "10-2019-0011748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "컴퓨터에서 상기 제1항 내지 제7항 중 어느 하나의 항에 기재된 버스트 스파이크 기반한 스파이킹 신경망 생성방법을 실행하기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록 매체."}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "스파이크 기반한 스파이킹 신경망 생성 방법은 컴퓨터 장치가 학습된 인공신경망 모델의 가중치를 스파이킹 신경 망(SNN)에 매핑하는 단계 및 상기 컴퓨터 장치가 상기 가중치가 매핑된 SNN을 최적화하는 단계를 포함한다. 상기 최적화하는 단계는 상기 SNN의 뉴런 상태에 따라 발화를 결정하는 임계값을 조정한다."}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "이하 설명하는 기술은 스파이킹 신경망 모델을 구축하는 방법에 관한 것이다."}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 AI(인공지능) 분야가 차세대 유망 기술로 주목받고 있다. 특히 딥 러닝(deep learning)이 이미지 인식 등 의 분야에서 큰 성과를 거두며 많은 관심을 받고 있다. 대부분의 딥 러닝의 학습 및 추론 과정은 다수의 CPU (Central Processing Unit)와 GPU로 구성되어 컴퓨팅 자원과 전력이 풍부한 대규모 서버 컴퓨팅 환경에서 이뤄 지게 된다. 많은 양의 컴퓨팅 자원과 전력 소모 요구량은 딥 러닝 모델을 모바일(mobile) 및 임베디드 (embedded) 환경에 적용하는데 장애가 된다. 이러한 문제를 해결하기 위해 딥 러닝 모델을 경량화 하여 모바일 환경 적용하는 방법인 양자화(quantization)와 전정(pruning) 방법에 대해 여러 연구가 진행되고 있다. 나아가 SNN (spiking neural network)으로 구성되어 저 전력으로 동작할 수 있는 뉴로모픽 칩(neuromorphic chip)이 개발되었다. SNN은 뉴런(neuron)의 막 전위(membrane potential)가 문턱 전압(threshold voltage) 보 다 높을 때만 발화(fire)하고, 발화된 스파이크(spike)를 통해 시냅스(synapse) 간의 정보를 전달하기 때문에 이벤트 기반(event-driven)으로 동작할 수 있어 다른 인공 신경망에 비해 저 전력 동작이 가능하다. 선행기술문헌 특허문헌 (특허문헌 0001) 미국공개특허 US 2015-0235124호"}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "SNN은 아직 적용 애플리케이션이 많지 않다. 이는 SNN을 학습하기가 어렵기 때문이다. SNN에서는 뉴런과 시냅스 가 미분이 안 되기 때문에 경사 강하법(gradient descent)과 오차 역전파 방법(error back-propagation)을 사용하여 SNN을 학습할 수 없다. SNN의 학습 방법으로 가장 널리 알려진 방법이 STDP (Spiking Timing Dependent Plasticity)이다. 그러나 STDP를 이용한 deep SNN에 대한 연구는 미진한 상태이다. 종래 DNN에 대한 학습 방법 을 SNN에 적용한 연구가 있었다. 이하 설명하는 기술은 DNN의 학습 결과를 SNN에 적용하여 SNN을 학습하는 방법을 제공하고자 한다. 이하 설명하 는 기술은 DNN 학습 결과가 적용된 SNN을 최적화하는 방법을 제공하고자 한다. 이하 설명하는 기술은 버스트 스 파이크 방식을 적용한 신경 코딩 기법을 제공하고자 한다."}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "버스트 스파이크 기반한 스파이킹 신경망 생성 방법은 컴퓨터 장치가 학습된 인공신경망 모델의 가중치를 스파 이킹 신경망(SNN)에 매핑하는 단계 및 상기 컴퓨터 장치가 상기 가중치가 매핑된 SNN을 최적화하는 단계를 포함 한다. 상기 최적화하는 단계는 상기 SNN의 뉴런 상태에 따라 발화를 결정하는 임계값을 조정하한다. 스파이킹 신경망 기반 추론 장치는 입력 데이터를 입력받는 입력장치, 입력값에 따라 상태가 변경되는 뉴런에 대하여 기준값에 따라 상기 뉴런에 대한 임계값을 조절하고, 상기 기준값에 따라 상기 뉴런에 대한 가중치를 조 절하는 신경 코딩을 사용하는 스파이킹 신경망(SNN)을 저장하는 저장장치 및 상기 입력 데이터를 상기 SNN에 입 력하여 추론을 수행하는 연산장치를 포함한다."}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이하 설명하는 기술은 버스트 스파이크 기반의 신경 코딩을 적용한 SNN을 구축한다. 따라서 이하 설명하는 기술 은 SNN의 장점을 유지하면서, 종래 기법에 비하여 빠르고 정확한 추론이 가능한 신경망 모델을 제공할 수 있다."}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 설명하는 기술은 다양한 변경을 가할 수 있고 여러 가지 실시례를 가질 수 있는 바, 특정 실시례들을 도면 에 예시하고 상세하게 설명하고자 한다. 그러나, 이는 이하 설명하는 기술을 특정한 실시 형태에 대해 한정하려 는 것이 아니며, 이하 설명하는 기술의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하 는 것으로 이해되어야 한다. 제1, 제2, A, B 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 해당 구성요소들은 상기 용어 들에 의해 한정되지는 않으며, 단지 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예 를 들어, 이하 설명하는 기술의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 및/또는 이라는 용어는 복수의 관련된 기재된 항목들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다. 본 명세서에서 사용되는 용어에서 단수의 표현은 문맥상 명백하게 다르게 해석되지 않는 한 복수의 표현을 포함 하는 것으로 이해되어야 하고, \"포함한다\" 등의 용어는 설시된 특징, 개수, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함을 의미하는 것이지, 하나 또는 그 이상의 다른 특징들이나 개수, 단계 동작 구성요 소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 배제하지 않는 것으로 이해되어야 한다. 도면에 대한 상세한 설명을 하기에 앞서, 본 명세서에서의 구성부들에 대한 구분은 각 구성부가 담당하는 주기 능 별로 구분한 것에 불과함을 명확히 하고자 한다. 즉, 이하에서 설명할 2개 이상의 구성부가 하나의 구성부로합쳐지거나 또는 하나의 구성부가 보다 세분화된 기능별로 2개 이상으로 분화되어 구비될 수도 있다. 그리고 이 하에서 설명할 구성부 각각은 자신이 담당하는 주기능 이외에도 다른 구성부가 담당하는 기능 중 일부 또는 전 부의 기능을 추가적으로 수행할 수도 있으며, 구성부 각각이 담당하는 주기능 중 일부 기능이 다른 구성부에 의 해 전담되어 수행될 수도 있음은 물론이다. 또, 방법 또는 동작 방법을 수행함에 있어서, 상기 방법을 이루는 각 과정들은 문맥상 명백하게 특정 순서를 기 재하지 않은 이상 명기된 순서와 다르게 일어날 수 있다. 즉, 각 과정들은 명기된 순서와 동일하게 일어날 수도 있고 실질적으로 동시에 수행될 수도 있으며 반대의 순서대로 수행될 수도 있다. 이하 설명하는 기술은 인공신경망을 구축하는 기법에 관련된다. 이하 컴퓨터 장치가 인공신경망을 학습하고 구 축한다고 설명한다. 컴퓨터 장치는 학습된 인공신경망을 이용하여 일정한 서비스를 제공할 수도 있다. 컴퓨터 장치는 일정한 프로그램 내지 소스코드에 따라 일정한 연산을 처리하는 장치를 의미한다. 컴퓨터 장치는 서버, 개인 PC, 스마트 기기 또는 영상처리 전용 칩셋 등을 포함한다. 응용 분야에 따라서 컴퓨터 장치는 영상 처리 장치일 수도 있다. 먼저 SNN에 대하여 간략하게 설명한다. SNN은 스파이크 트레인(spike train)을 생성하는 뉴런과 시냅스로 구성된다. 뉴런은 일련의 스파이크인 스파이 크 트레인을 통하여 다른 뉴런으로 정보를 전달한다. 다양한 신경 모델 중에서 IF(Integrate-and-Fire) 모델이 가장 널리 사용된다. IF 신경 모델은 시냅스 후 전위(post-synaptic potential, PSP)를 막 전위(Vmem)로 통합한 다. 시간 t에서 l번째 계층에 있는 j 번째 뉴런에서 통합된 PSP를 아래 수학식 1과 같이 표현할 수 있다. 이하 설명에서 동일한 파라미터 내지 기호는 수학식에서 동일한 의미로 사용한다. 수학식 1"}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서 는 PSP의 합이다. 는 l-1번째 계층에 i번째 뉴런과 l번째 계층에 있는 j 번째 뉴런 사 이의 시냅스 가중치이다. 는 l-1번째 계층에 i번째 뉴런에 있는 스파이크이다. 는 편향(bias)이다. 막전위가 일정한 임계값 를 초과하면, 뉴런은 스파이크를 생성한다. 시간 t에서 l번째 계층에 있는 i 번째 뉴런의 스파이크 생성 과정은 아래의 수학식 2와 같이 표현될 수 있다. 수학식 2"}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 2에서 는 스파이크 출력을 표현하는 단위 계단 함수(unit step function)이다. z는 PSP의 합계이 고, Vth는 임계값이다. IF 뉴런이 스파이크를 발화(fire)한다면, 막전위는 휴지 전위(resting potential, Vrest)로 설정되고, 스파이크 가 발화되기 전이라만 이전 전위를 유지한다. 막전위의 동적 변화는 아래의 수학식 3과 같이 표현될 수 있다. 수학식 3"}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "SNN은 스파이크 트레인을 통하여 정보가 전송된다. 스파이크라는 이벤트라 발생할 때만 정보가 전송되므로, SNN 은 에너지 효율적인 모델이 된다. SNN 학습 SNN을 위한 학습 방법에 대하여 설명한다. STDP 또는 오차 역전파 방법으로 학습된 SNN은 DNN과 같은 성능을 보 이지 못한다. 최근 DNN을 이용한 SNN 학습 방법이 연구되었다. 도 1은 DNN을 이용한 SNN 학습 방법에 대한 예이다. 컴퓨터 장치는 SNN에 대응하는 DNN을 구성하고, 먼저 DNN을 학습한다. SNN과 동일한(또는 유사한) 구조를 갖는 DNN이 오차 역전파 방법으로 학습된다. 컴퓨터 장치는 학습된 DNN의 가중치를 SNN에 적용한다. 컴퓨터 장치는 DNN의 각 계층 노드(뉴런)에 대한 가중치를 대응하는 SNN의 각 계층 노드에 적용한다. 즉 컴퓨터 장치가 DNN에서 대응하는 가중치를 SNN에 매핑(mapping)한 다고 할 수 있다. 이 과정을 수행하면 학습된 SNN이 마련된다. 이후 컴퓨터 장치는 학습된 SNN을 이용하여 일정 한 추론을 하고, 관련된 서비스를 제공할 수 있다. SNN은 DNN과는 구조적으로 차이가 있다. 따라서 DNN의 학습 결과를 그대로 SNN에 적용하면 일정 부분 성능 감소 가 따른다. 이를 해결하기 위한 다양한 연구가 있었다. 예를 들어, 가중치를 SNN에 맞게 조절 내지 정규화 (normalization)하는 연구가 있었다. 가중치 조절을 위한 다양한 모델에 대한 연구가 있었다. 최근 연구는 새로운 정규화 모델이 제안하였다. 새로운 정규화 모델은 막 전위의 초기화로 인한 성능 저하를 막 기 위하여 감산 방식으로 초기화(reset-by-subtraction)를 구현하였다. 이 경우 휴지 전위(Vrest)는 아래의 수학 식 4와 같이 표현될 수 있다. 수학식 4"}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "그리고 뉴런 간의 정보 손실을 줄이기 위하여 PSP의 합계를 아래의 수학식 5와 같이 표현할 수 있다. 수학식 5 는 수학식 1과 같이 시간 t에서 l번째 계층에 있는 j 번째 뉴런에서의 통합된 PSP를 나타낸다.수학식 5"}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "이와 같은 모델을 사용한 경우 MNIST 및 CIFAR-10 데이터 세트에 대해서는 DNN와 유사한 성능을 보였다. 이하 설명하는 기술도 이와 DNN의 학습 결과를 SNN에 적용하는 학습 방법을 사용한다고 전제한다. 신경 코딩(neural coding) 신경 코딩은 하나의 뉴런에서 다른 뉴런으로 정보를 전송하는 인코딩 구조(encoding scheme)이다. 다양한 유형 의 신경 코딩이 연구되었다. 대표적으로 발화율 코딩(rate coding)이 사용된다. 도 2는 발화율 코딩에 대한 예이다. 도 2에서 A 영역은 스파 이크 트레인, B영역은 PSP(시냅스 후 전위) 및 C 영역은 ISIH(inter-spike interval histogram)을 나타낸다. ISIH는 스파이크 사이의 간격에 대한 히스토그램이다. 발화율 코딩은 스파이크 발화율에 기반한 코딩이다. 발 화율 코딩은 일정 시간 동안 발생하는 스파이크 개수에 따라 결정된다. 따라서 발화율 코딩은 정보의 전송 용량 및 전송 속도라는 측면에서 비효율적이다. 예를 들어, 8비트 정보를 전송하기 위하여 256(=28) 번의 동작이 필 요하다. 도 2를 살펴보면, 매번의 스파이크마다 같은 크기의 PSP가 유도되는 것을 알 수 있다. 결국 deep SNN에 발화율 코딩을 적용하면 많은 대기 시간(latency)과 높은 에너지 소비를 가져온다. 도 3은 버스트 스파이킹(burst spiking)에 대한 예이다. 버스트 스파이킹은 매우 다양한 스파이크 패턴을 갖는 다. 스파이크 패턴은 짧은 ISI(inter-spike interval)를 갖는 스파이크 그룹을 포함할 수 있다. 버스트 스파이 킹은 일반적인 스파이크 트레인보다 많은 정보를 담을 수 있다. 예컨대, 도 3의 C에 도시한 바와 같이 버스트 스파이킹은 정보 전달 효율을 높이는 막전위를 유도할 수 있다. 이와 같은 버스트 스파이킹을 deep SNN에 적용 할 수 있다면 추론(inference) 시간을 줄여 성능 향상을 가져올 수 있을 것이다. 이하 설명하는 기술은 버스트 스파이크를 deep SNN에 적용하여 효율적이고 빠른 추론이 가능한 모델을 제공한다. 이는 종래 생물학 분야에서 정보 전달에 효율적이라고 알려진 버스트 스파이크를 인공지능 모델에 적 용하는 것이다. 버스트 스파이킹을 함수적으로 구현하기 위하여 아래의 수학식 6과 같은 버스트 함수(burst function)을 정의할 수 있다. 수학식 6"}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "는 시간 t에서 l번째 계층에 있는 뉴런 i에 대한 버스트 함수이다. β는 버스트 상수(burst constant)이다. 는 수학식 2에서 설명한 바와 같이 시간 t-1에서 생성된 스파이크를 의미한다. 효율적인 버스트 스파이크를 얻기 위하여 전술한 수학식 5에 버스트 함수를 적용하면 임계값이 아래의 수학식 7 과 같이 표현될 수 있다.수학식 7"}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "는 시간 t에서 l번째 계층에 i번째 뉴런에 대한 임계값이다. 는 뉴런에 설정된 초기 임계값이다. 수학식 5와 수학식 7로부터 아래의 수학식 8과 같이 가중치를 결정할 수 있다. 아래 수학식 8은 시간 t에서 l-1 번째 계층에 i번째 뉴런과 l번째 계층에 있는 j 번째 뉴런 사이의 시냅스 가중치이다. 수학식 8"}
{"patent_id": "10-2019-0011748", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "수학식 8의 가중치는 버스트 스파이크에 기반한 시냅스 강화 작용(synaptic potentiation)이라고 해석될 수 있 다. 이를 통해 SNN은 효율적인 정보 전달이 가능하다. 수학식 8에서 는 각 뉴런에 초기 설정된 가중치를 의 미한다. 즉, DNN의 가중치가 매핑되어 설정된 값이다. 도 4는 임계값에 따라 버스트 스파이킹이 발생하는 확률에 대한 예이다. 도 4는 서로 다른 임계값(Vth)에 따라 발생하는 버스트 스파이크의 개수 및 그 길이(burst length)를 나타낸다. 임계값이 작아지면 버스트 스파이크가 증가하고, 긴 길이의 버스트가 자주 나타나는 것을 알 수 있다. 이와 같은 버스트 스파이크를 이용한 신경 코딩을 버스트 코딩(burst coding)이라고 명명한다. 버스트 코딩은 뉴런의 상태에 따라 동적으로 시냅스의 크기를 변화시킬 수 있다. 전달해야 하는 정보의 양은 누적된 막전위 및 연속된 스파이크의 수로 판단할 수 있다. 스파이크가 연속적으로 발생하는 경우 버스트 함수를 통해 시냅스를 강화하고, 스파이크가 발생하지 않는 경우 시냅스 세기를 원상태로 만드는 방법을 사용한다. 도 5는 버스트 스파이크를 사용하는 SNN 학습 과정에 대한 예이다. 컴퓨터 장치는 SNN에 대응하는 DNN을 구성하고, 먼저 DNN을 학습한다. SNN과 동일한(또는 유사한) 구조를 갖는 DNN이 오차 역전파 방법으로 학 습된다. 컴퓨터 장치는 학습된 DNN의 가중치를 SNN에 적용한다. 컴퓨터 장치는 DNN의 각 계층 노드(뉴런) 에 대한 가중치를 대응하는 SNN의 각 계층 노드에 매핑한다. 컴퓨터 장치는 가중치가 설정된 SNN에 버스트 스파 이크를 적용하여 SNN 구조를 최적화할 수 있다. 이후 컴퓨터 장치는 최적화된 SNN을 이용하여 일정한 추론 을 할 수 있다. 버스트 스파이크를 이용한 구조 최적화는 특정 입력 데이터에 대한 추론 과정에서 수 행될 수도 있다. 도 6은 버스트 스파이크에 기반한 SNN 최적화 과정에 대한 예이다. 도 6은 도 5에서의 230 단계에 해당한 다. 도 6은 기본적으로 SNN이 학습된 DNN의 가중치로 설정된 상태를 가정한다. 예컨대, SNN이 영상 분류를 목적 으로 하다면, 최적화 과정을 위한 훈련용 영상을 사용할 수 있다. 컴퓨터 장치는 훈련용 영상을 SNN에 입력하고, 각 뉴련의 상태에 따라 해당 뉴런의 임계값 및 가중치를 조정하면서 최종 결과가 정확한지 판단한다. 정확도는 사전에 구축한 DNN의 추론 결과와 SNN의 결과를 비교하여 판단할 수도 있다. SNN은 일반적인 DNN과 같 이 피드 포워드 방식으로 동작한다고 가정한다. 컴퓨터 장치는 먼저 최적화를 위하여 각 계층에 대한 인덱스를 부여할 수 있다. 컴퓨터 장치는 계층 인덱스에 대한 초기 설정을 한다. 컴퓨터 장치는 훈련용 영상을 SNN에 입력하고, 이후 모델의 구조에 따라 SNN 각 계층은 이전 계층의 노드(뉴런)로부터 일정한 입력값(feed forward input)을 수신한다. 입력값을 수신한 뉴런은 상태가 변경될 수 있다(상태 업데이트). 전술한 바와 같이 뉴런은 시냅스 후 전위(PSP)를 입력받아 이를 통합한 값이 일정한 임계값을 초과하면 발화하게 된다. 따라서 컴퓨터 장치는 현재 입력값 수신에 따라 뉴런이 상태 변화 여부(발화 진행 여부)를 판단한다. 현재의 뉴런이 발화해야 하는 상태라면(330의 YES), 컴퓨터 장치는 스파이크를 생성하고 뉴런 상태를 초기화한 다. 뉴런 상태 초기화는 뉴런의 막전위 초기화를 의미한다. 뉴런의 상태에 따라 컴퓨터 장치는 임계값을 업데이트한다. 예컨대, 컴퓨터 장치는 현재 스파이크가 발생한 뉴런에 대한 임계값을 기준값만큼 줄일 수 있다. 임계값을 줄이는 정도(기준값)는 SNN의 적용 애플리케이션 종류 내지 SNN이 구축된 시스템의 성능에 따라 달라질 수 있다. 기준값은 전술한 수학식 6에서의 버스트 함수에 따라 결정될 수 있다. 관련하여 컴퓨터 장치는 현재 뉴런에 대한 시냅스 연결 강화를 수행할 수 있다. 임계값 변경 및 시냅스 연결 강화는 전 술한 버스트 함수를 통해 구현될 수 있다. 따라서 현재 발화된 뉴런은 임계값이 일정하게 조절(일반적으로 임계 값 감소)되고, 또한 가중치도 변경된다. 구체적인 과정은 전술한 수학식 6 내지 8에 따른다. 컴퓨터 장치는 현재 뉴런이 발화되는 상태가 아니라면 시냅스 연결을 초기화한다. 이 과정은 스파이크가 발생하지 않는 경우 초기 학습 과정에서 설정된 값(DNN의 가중치가 매핑된 값)으로 가중치를 환원하는 것입니다. 따라서 버스트 함수에 따라 임계값이 변경되고 스파이크가 많이 발생하게 되는 시냅스 강화는 일정한 기간(단기)에만 활성화 된다고 할 수 있다. 컴퓨터 장치는 현재 계층이 마지막 계층인지 여부를 판단한다. 마지막 계층이 아니라면(360의 NO), 계층 인덱스를 증가하여 다음 계층으로 이동한다. 다음 계층에 대해서도 각 뉴런에 대하여 발화 여부를 확인하 고, 경우에 따라 시냅스 연결을 강화하여 가중치를 조정하는 작업을 수행한다. 현재 계층이 마지막 계층이라면(360의 YES), 컴퓨터 장치는 SNN의 추론 결과의 정확도가 기준값보다 큰지 판단 한다. 컴퓨터 장치는 학습된 DNN의 추론 결과와 비교하여 정확도가 기준값보다 큰지 판단할 수 있다. 예컨 대, 컴퓨터 장치는 학습된 DNN의 추론 결과와 SNN의 추론 결과가 동일하다면 정확도가 기준값보다 크다라고 판 단할 수 있다. 여기서 기준값은 SNN의 최적화를 위한 목표값이라고 할 수 있다. 만약 정확도가 기준값 이하라면 컴퓨터 장치는 전술한 최적화 과정을 반복할 수 있다(380의 NO). 도 6에 도시하지 않았지만, 컴퓨터 장치는 최 적화 과정을 무한 반복하지 않기 위하여 일정한 횟수의 최적화 과정을 반복하였다면, 최적화 과정을 종료할 수 도 있다. 이후 컴퓨터 장치는 최적화된 SNN를 이용하여 일정한 추론 및 서비스를 제공할 수 있다. 뉴런은 그 역할 및 뇌에서의 위치에 따라 서로 다른 신경 코딩을 사용한다고 알려져 있다. 이러한 모델에 기반 하여 단일 신경 코딩을 사용하지 않고, SNN의 계층의 특징에 따라 서로 다른 코딩 전략을 사용할 수도 있을 것 이다. SNN을 입력 계층(input layer)과 은닉 계층(hidder layer)으로 구분할 수 있다. 영상 분류를 예로 설명하면, 입 력 계층은 입력 영상의 연속적인 값을 이산된 스파이크로 변환한다. 영상 분류에서 입력 값은 한정적이고 정적 인 값을 갖는다. 예를 들어, 입력 계층은 위상 코딩(phase coding) 등이 적절할 수 있다. 위상 코딩은 동적 코 딩(temporal coding)의 일종이다. 한편 버스트 코딩은 동적으로 전송 속도를 결정할 수 있다. 따라서 버스트 코 딩은 입력 계층보다는 은닉 계층에 적절할 수 있다. SNN은 그 용도에 따라 서로 다른 종류의 신경 코딩 기법을 사용하는 것이 보다 높은 효율을 가져올 수 있다. 복수의 신경 코딩 기법을 사용한다는 의미에서 이를 하이브리 드(hybrid) 신경 코딩이라고 할 수 있다. 도 7은 SNN 모델에 기반한 추론 장치에 대한 예이다. SNN 기반 추론 장치는 도 6에서 설명한 SNN 최 적화 과정을 수행하는 장치일 수 있다. 또 SNN 기반 추론 장치는 구축된 SNN을 이용하여 일정한 추론 서비스를 제공하는 장치일 수 있다. 예컨대, SNN 기반 추론 장치는 영상에 대한 분류를 수행하는 장치일 수 있다. SNN 기반 추론 장치는 물리적으로 다양한 형태로 구현될 수 있다. 예컨대, SNN 기반 추론 장치는 스 마트 기기, PC, IoT 디바이스, 네트워크의 서버, 영상 처리 전용 칩셋 등의 형태를 가질 수 있다. SNN 기반 추론 장치는 저장장치, 메모리, 연산장치, 인터페이스 장치 및 통신장치 를 포함한다. 저장장치는 SNN 학습 내지 최적화 을 위한 프로그램 내지 소스코드를 저장한다. 저장장치는 학습을 위한 훈련용 영상을 저장할 수 있다. 저장장치는 학습 대상인 초기 SNN 모델 및 DNN 모델을 저장할 수 있 다. 저장장치는 학습된 DNN 모델 및 SNN 모델을 저장할 수 있다. 또 저장장치는 학습된 SNN을 이용한 추론 방법 내지 서비스를 위한 프로그램 내지 소스코드를 저장할 수 있다. 메모리는 SNN 기반 추론 장치가 SNN을 학습 내지 최적화 과정에서 생성되는 데이터, 파라미터, 영상 정보 등을 저장할 수 있다. 또 메모리는 SNN을 이용한 추론 과정에서 생성되는 데이터 등을 저장할 수 있 다. 인터페이스 장치는 외부로부터 일정한 명령 및 데이터를 입력받는 장치이다. 인터페이스 장치는 물리 적으로 연결된 입력 장치 또는 외부 저장 장치로부터 훈련용 영상, 입력 영상 등을 입력받을 수 있다. 인터페이 스 장치는 DNN 모델, SNN 모델 및 영상 처리를 위한 각종 프로그램 내지 명령을 입력받을 수 있다. 통신 장치는 유선 또는 무선 네트워크를 통해 일정한 정보를 수신하고 전송하는 구성을 의미한다. 통신 장 치는 외부 객체로부터 훈련용 영상 또는 입력 영상을 수신할 수 있다. 통신 장치는 SNN 모델, DNN 모 델, SNN 학습용 프로그램, SNN 기반 응용 서비스 제공 프로그램 등을 수신할 수도 있다. 나아가 통신 장치(45 0)는 학습된 SNN 이용한 추론 결과를 외부 객체로 송신할 수 있다. 통신 장치 내지 인터페이스 장치는 외부로부터 일정한 데이터 내지 명령을 전달받는 장치이다. 통신 장치 내지 인터페이스 장치를 입력장치라고 명명할 수 있다. 연산 장치는 저장장치에 저장된 프로그램을 이용하여 SNN을 학습할 수 있다. 또 연산 장치는 저 장장치에 저장된 프로그램을 이용하여 SNN을 최적화할 수 있다. 연산 장치는 최종적으로 마련된 SNN 을 이용하여 일정한 추론을 할 수 있다. 연산 장치는 데이터를 처리하고, 일정한 연산을 처리하는 프로세 서, AP, 프로그램이 임베디드된 칩과 같은 장치일 수 있다. 또한, 상술한 바와 같은 SNN 학습 방법, 학습된 SNN 생성 방법 및 학습된 SNN의 동작 방법은 컴퓨터에서 실행될 수 있는 실행가능한 알고리즘을 포함하는 프로그램(또는 어플리케이션)으로 구현될 수 있다. 상기 프로그램은 비일시적 판독 가능 매체(non-transitory computer readable medium)에 저장되어 제공될 수 있다. 비일시적 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체가 아니 라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 구체적으로는, 상 술한 다양한 어플리케이션 또는 프로그램들은 CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등과 같은 비일시적 판독 가능 매체에 저장되어 제공될 수 있다. 본 실시례 및 본 명세서에 첨부된 도면은 전술한 기술에 포함되는 기술적 사상의 일부를 명확하게 나타내고 있 는 것에 불과하며, 전술한 기술의 명세서 및 도면에 포함된 기술적 사상의 범위 내에서 당업자가 용이하게 유추 할 수 있는 변형 예와 구체적인 실시례는 모두 전술한 기술의 권리범위에 포함되는 것이 자명하다고 할 것이다."}
{"patent_id": "10-2019-0011748", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 DNN을 이용한 SNN 학습 방법에 대한 예이다. 도 2는 발화율 코딩에 대한 예이다. 도 3은 버스트 스파이킹에 대한 예이다. 도 4는 임계값에 따라 버스트 스파이킹이 발생하는 확률에 대한 예이다. 도 5는 버스트 스파이크를 사용하는 SNN 학습 과정에 대한 예이다. 도 6은 버스트 스파이크에 기반한 SNN 최적화 과정에 대한 예이다. 도 7은 SNN 모델에 기반한 추론 장치에 대한 예이다."}
