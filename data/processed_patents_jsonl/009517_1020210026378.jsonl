{"patent_id": "10-2021-0026378", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0122141", "출원번호": "10-2021-0026378", "발명의 명칭": "학습데이터 수집장치, 학습데이터 수집방법, 및 음성인식장치", "출원인": "윤혜진", "발명자": "윤혜진"}}
{"patent_id": "10-2021-0026378", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "입력 데이터를 처리하여 음성 데이터를 출력하는 음성 처리부;상기 입력 데이터를 처리하여 상기 음성 데이터와동일한 정보를 의미하는 텍스트 데이터를 출력하는 텍스트 처리부; 및상기 음성 데이터와 상기 텍스트 데이터를동기화한 학습데이터를 출력하는 동기화부를 포함하는 학습데이터수집장치."}
{"patent_id": "10-2021-0026378", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 음성 처리부는,음성인식을 해야 하는 대상인 화자를 식별하는 화자 식별부; 및상기 화자가 발화한 음성데이터 및 시간데이터를 추출하는 음성/시간 데이터 추출부를 포함하는 학습데이터수집장치."}
{"patent_id": "10-2021-0026378", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서, 상기 텍스트 처리부는,상기 입력데이터로부터 이미지를 추출하는 이미지 추출부;추출된 이미지 내의 글자를 텍스트로 바꾸는 OCR처리부; 및상기 OCR처리부에서 받은 텍스트 정보를 추출하는 텍스트 데이터추출부를 포함하는 학습 데이터 수집 장치."}
{"patent_id": "10-2021-0026378", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서, 상기 텍스트 처리부는,상기 입력데이터로부터, 상기 음성데이터와 같은 시간의 텍스트 정보를 추출하는 텍스트 데이터 추출부를 포함하는 학습 데이터 수집 장치.`"}
{"patent_id": "10-2021-0026378", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서, 상기 학습 데이터 수집 장치는, 상기 입력데이터의 형식을 식별하는 식별부를 포함하는 학습데이터 수집 장치."}
{"patent_id": "10-2021-0026378", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서, 상기 식별부는, 상기 입력데이터에 상기 텍스트 데이터가 포함되는지를 판단하는 학습데이터수집장치."}
{"patent_id": "10-2021-0026378", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항 내지 제 6 항 중의 어느 한 항의 상기 학습데이터 수집장치에서 수집된 학습데이터를, 인공지능을 이용하여 학습한 학습모델을 탑재하는 음성인식부를 포함하는 음성인식장치."}
{"patent_id": "10-2021-0026378", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서, 상기 학습데이터 수집장치는, 상기 음성인식부와 같은 기기 내에 있거나, 원격지에서 상기음성인식부와 온라인으로 연결되는 음성인식 장치."}
{"patent_id": "10-2021-0026378", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "자막 및 음성이 포함되는 영상 데이터를 입력하는 데이터 입력 단계; 상기 데이터 입력단계에서 입력된 데이터중 음성 데이터를 확인하는 음성데이터 확인 단계; 상기 음성데이터 확인 단계에서 입력된 음성 데이터 중 선택된 음성 데이터를 특정하고 추출하는 화자 특정 단계;상기 화자 특정 단계에서 특정된 화자가 있는 시간 정보를추출하는 시간정보 추출 단계; 상기 시간정보 추출 단계로부터 전달된 시간정보에 있는 이미지를 추출하는 이미지 추출 단계; 상기 이미지 추출단계에서 추출한 이미지에 있는 자막을 OCR과정을 거쳐 텍스트화 하는 텍스트처리 단계; 상기 텍스트 처리 단계로부터 전달된 텍스트 데이터를 추출하는 텍스트 추출 단계; 및음성 데이터와텍스트 데이터를 동기화하는 데이터 동기화 단계를 포함하는 학습 데이터 수집방법."}
{"patent_id": "10-2021-0026378", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "공개특허 10-2022-0122141-3-자막 및 음성이 포함될 수 있는 영상 데이터를 입력하는 데이터 입력 단계; 상기 데이터 입력단계에서 입력된데이터 중 음성 데이터를 확인하는 음성데이터 확인 단계; 상기 음성데이터 확인 단계에서 입력된 음성 데이터중 필요한 음성 데이터를 특정하고 추출하는 화자 특정 단계; 상기 화자 특정 단계에서 특정 화자가 있는 시간정보를 추출하는 시간정보 추출 단계; 상기 시간 정보와 같은 시간의 텍스트 데이터를 추출하는 텍스트 추출 단계; 및상기 음성 데이터와 텍스트 데이터를 동기화하는 데이터 동기화 단계를 포함하는 학습데이터 수집 방법."}
{"patent_id": "10-2021-0026378", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명에 따른 학습데이터 수집장치는 시간데이터를 추출하는 음성 처리부, 입력 데이터로부터 받은 텍스트 데 이터 혹은 받은 입력 데이터를 텍스트 데이터화하여 추출하는 텍스트 처리부, 및 상기 추출된 음성 데이터와 텍 스트 데이터를 동기화는 동기화부를 포함한다."}
{"patent_id": "10-2021-0026378", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 학습데이터 세트를 수집할 수 있는 학습데이터 수집장치, 학습데이터 수집방법, 및 음성인식장치에 관한 것이다."}
{"patent_id": "10-2021-0026378", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "음성인식(Speech Recognition)이란 사람이 말하는 음성 언어를 컴퓨터가 해석해 그 내용을 문자 데이터로 전환 하는 처리를 말한다. 상기 음성인식은 STT(Speech-to-Text)라고도 할 수 있다. 상기 음성인식처리를 하는 장치 를 음성인식 장치라고 할 수 있다. 상기 음성인식장치는 인공지능을 이용하여 구현할 수 있다. 상기 인공지능(또는 AI)은 인간의 학습능력, 추론 능력, 지각능력, 그 외에 인공적으로 구현한 컴퓨터 프로그램 또는 이를 포함한 컴퓨터 시스템이다. 상기 인공 지능을 이용하여 음성인식장치를 구현하기 위해서는 많은 학습데이터를 학습하는 것이 바람직하다. 특히, 노인, 사투리, 및 발성장애인 등의 음성인식을 위해서는 더 많은 학습데이터가 필요하다. 그럼에도 더더 욱 학습데이터를 얻기가 어려운 문제점이 있다. 인공지능을 이용하는 음성인식장치의 학습모델을 얻기 위해서는 수천 수만시간의 음성데이터가 필요하다. 상기 음성데이터의 참 값(ground truth)은 텍스트 데이터로 표시될 수 있다. 종래에는, 음성 데이터를 녹음기 등의 수작업을 통해 직접 수집해야 했다. 또 상기의 참 값을 확인하기 위하여, 검수자가 일일이 검수를 하여야 했다. 그러나, 이는 비효율적이며 covid19로 언택트가 뉴노멀(newnormal)이 된 사회에서는 실현하기 어려운 방식이다. 또한, 다양한 화자의 음성을 음성인식장치가 인식하기 위해서는 더 많은 학습데이터의 확보가 필요하다. 그럼에 도 불구하고, 위에서 이미 설명한 바와 같이, 충분한 학습데이터를 얻는 것이 많이 어려운 것이 현실이다. 종래 기술로서 특허문헌 1에는, 음성 데이터와 자막 데이터를 자동으로 추출하여 작업 시간을 줄이고자 하는 기 술을 개시한다. 상기 작업 시간은 콘텐츠를 제작하기 위한 시간, 자료를 검색하기 위한 시간 등이다. 그러나 엄 밀히 말하자면, 이는 메타데이터를 추출하기 위한 것이지, 상기 학습 데이터를 추출하기 위한 것이 아니다. 메 타데이터란 다른 데이터를 설명해주는 데이터를 이야기한다. 메타데이터는 데이터를 사용하는 사람에게는 보이 지 않으며, 컴퓨터가 이러한 메타데이터의 내용을 이해하고 이를 이용한다. 또한 종래 기술은 추출 대상을 설정 할 수 없다. 예를 들어 A가 발화할 때, A의 음성으로 식별하거나, 두 명이 발화할 때 어느 사람의 발화인지를 식별할 수 없다. 선행기술문헌 특허문헌 (특허문헌 0001) 특허문헌 1 KR 10-2007-0057478 (공개일: 07.06.12)동영상의 음성 인식과 자막 인식을 통한 메타데이터 추출방법, 메타데이터를 이용한 동영상 탐색 방법 및 이를 기록한 기록매체"}
{"patent_id": "10-2021-0026378", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기되는 문제를 해소하기 위하여 제안되는 것으로서, 종래 기술에 비해 효율적으로 음성인식장치의 학습을 위한 학습데이터 손쉽게 구하는 장치 및 방법을 제안한다.본 발명은 음성인식의 효율이 높은 음성인식장치를 제안한다."}
{"patent_id": "10-2021-0026378", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명에 따른 학습데이터 수집장치는, 시간데이터를 추출하는 음성 처리부, 입력 데이터로부터 받은 텍스트 데이터 혹은 받은 입력 데이터를 텍스트 데이터화하여 추출하는 텍스트 처리부, 및 상기 추출된 음성 데이터와 텍스트 데이터를 동기화하는 동기화부를 포함한다. 본 발명에 따르면, 음성인식 장치의 학습을 위한 학습데이터를 종래에 비해 효율적으로 수집할 수 있다."}
{"patent_id": "10-2021-0026378", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 음성과 텍스트 데이터인 학습데이터 세트를 종래 기술에 비해 훨씬 효율적으로 수집할 수 있 다. 본 발명에 따르면, 더 많은 학습데이터를 수집하고 학습에 이용함으로써 높은 인식률의 음성인식장치를 만들 수 있다."}
{"patent_id": "10-2021-0026378", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 도면을 참조하여 본 발명의 구체적인 실시예를 상세하게 설명한다. 본 발명의 사상은 이하에 제시되 는 실시예에 제한되지 아니하고, 본 발명의 사상을 이해하는 당업자는 동일한 사상의 범위 내에 포함되는 다른 실시예를 구성요소의 부가, 변경, 삭제, 및 추가 등에 의해서 용이하게 제안할 수 있을 것이나, 이 또한 본 발 명 사상의 범위 내에 포함된다고 할 것이다. 발명자는, 어느 멀티미디어 데이터에는 음성데이터와 영상데이터가 포함될 수 있고, 그 영상 데이터에 자막정보 가 포함되는 것을 알았다. 여기서 상기 자막정보는 영상데이터의 일부분으로서 자막이 어떠한 의미 또는 내용인 지는 식별되지 않을 수 있다. 이 경우에 상기 자막정보는 OCR처리를 통하여 텍스트 데이터로 변환될 수 있다. 이 경우에는 상기 음성 데이터와 상기 자막정보를 동기화하여 음성인식을 위한 학습데이터를 얻을 수 있는 것을 착안하였다. 물론 상기 학습데이터를 학습한 학습모델을 제공하여 음성인식장치 제공할 수 있다. 이 실시예를 제 1 실시예라고 할 수 있다. 발명자는, 다른 멀티미디어 데이터에는 음성 데이터와 영상데이터와 자막데이터가 포함되는 것을 알았다. 여기 서, 상기 자막데이터는 상기 영상데이터의 일부분으로서 영상으로 표현되기는 하지만, 상기 영상데이터와는 다 른 정보로서 그 의미 또는 내용이 식별된 텍스트 데이터일 수 있다, 이 경우에는 상기 음성데이터와 상기 자막 데이터를 바로 동기화하여 음성인식을 위한 학습데이터를 얻을 수 있는 것을 착안하였다. 물론 상기 학습데이터 를 학습한 학습모델을 제공하여 음성인식장치를 제공할 수 있다. 이 실시예를 제 2 실시예라고 할 수 있다. 실시예의 설명에 있어서, 상기 학습데이터는 음성인식장치를 위한 학습모델을 구축하기 위한 음성데이터와 텍스 트데이터가 서로 동기화된 정보일 수 있다. 상기 텍스트 데이터는 상기 음성데이터의 참 값 정보일 수 있다. 상 기 텍스트 데이터는 상기 음성 데이터와 동일한 정보를 의미하고, 그 형식만 음성데이터는 소리이고, 텍스트인 차이가 있다. 실시예는 상기 텍스트 데이터를 손쉽게 수집하는 것을 그 일 특징으로 할 수 있다. 도 1은 제 1 실시예에 따른 학습데이터 수집장치와 음성인식장치의 구성을 설명하는 도면이다. 도 1을 참조하여 제 1 실시예에 따른 학습데이터수집장치를 설명한다. 제 1 실시예에 따른 학습데이터 수집장치에는, 입력데이터에서 화자를 식별하는 화자 식별부, 상기 화자 의 음성데이터와 현재 시간데이터를 추출하는 음성/시간데이터 추출부, 상기 시간데이터와 동기화되는 동시 간의 이미지를 상기 영상데이터에서 추출하는 이미지 추출부, 이미지 추출부에서 추출된 이미지를 텍스트 로 변환해주는 OCR 처리부, 상기 변환된 텍스트 데이터를 추출하는 텍스트 데이터 추출부, 및 상기 추출된 음성 데이터와 텍스트 데이터를 동기화하는 동기화부가 포함될 수 있다. 상기 제 1 실시예에 따른 학습데이터 수집장치의 각 구성을 상세하게 설명한다. 상기 입력데이터에는, 음성데이터, 영상데이터 및 텍스트 데이터가 포함될 수 있다. 상기 화자 식별부는, 입력데이터로부터 음성데이터를 제공하는 화자를 식별할 수 있다. 예를 들어, 현재 '나는 공부를 하고 있다'라는 발성을 하는 화자를 식별해 해는 방법이다. 상기 화자식별부는 음성데이터가 현재 나오는 영상에서 화자(예를 들어, 가)를 의미할 수 있다. 동일한 영상에서 화자가 두 명 이상이 있는 경우에도, 다른 사람이 동일한 시각에 함께 음성을 발화하지 않는 경우에는 두 명의 화자를 구분할 필요가 없다. 시간으로 화자를 구분할 수 있기 때문이다. 동일한 영상에서 화자가 두 명 이상 있고, 다른 사람이 동일한 시각에 함께 음성을 발화하는 경우에도 화자를 분리할 수 있다. 예를 들어, 영상의 좌측과 우측에 각각 다른 사람이 있는 경우에, 좌측의 사람(가)과 우측의 사람(나)는 다채널(예를 들어, 2채널 스테레오)정보로 입력되는 음성데이터에서 상기 (가)와 (나)를 구분하여 화자를 구분할 수 있다. 구분되는 각 화자의 음성데이터는 특징을 식별하여 동일한 시각에 같이, 복수의 화자가 같이 발화하더라도 음성데이터를 구분할 수 있다. 그 외에, 각 화자의 특징을 분석하는 다른 방법(HMM, GMM)을 이용하여, 복수의 화자가 같이 발화하더라도 음성데이터를 구분할 수 있다. 상기 화자는 음성이 부정확한 사람으로 특정할 수 있다. 예를 들어, 사투리, 발성장애인, 유아, 및 노인 등과 같이, 기존의 음성인식시스템을 통해서는 어떤 음성인지 알기가 어려운 화자일 수 있다. 이를 통하여 부정확한 음성에 해당하는 학습데이터를 더 많이 수집할 수 있다. 상기 음성/시간 데이터추출부는, 현재 화자의 음성데이터를 추출할 수 있다. 또한 상기 음성/시간 데이터추 출부는 현재 음성데이터가 발화되는 시간을 추출할 수 있다. 상기 음성 데이터와 상기 시간 데이터가 추출되 도록 하기 위하여, 음성데이터와 함께 상기 음성데이터가 발화되는 시간 데이터가 포함될 수 있다. 상기 이미지 추출부는 상기 음성/시간데이터 추출부로부터 전달된 시간 데이터를 이용하여 현재 시간과 동기화되는 이미지를 상기 영상데이터로부터 추출할 수 있다. 다른 실시형태로, 상기 이미지 추출부는 상기 음성/시간 데이터추출부로부터 전달된 음성데이터를 이용하 여 현재 음성과 동기화되는 이미지를 상기 영상데이터로부터 추출할 수 있다. 이 경우에는 상기 입력데이터에는 상기 음성데이터와 상기 영상데이터가 동기화되어 있을 수 있다. 이 경우에는 별도의 시간데이터가 포함되지 않 을 수도 있다. 상기 OCR 처리부는 상기 이미지 추출부로부터 전달된 이미지 데이터를 텍스트 데이터로 변환할 수 있다. 즉, 이미지 속에 있는 문자를 찾아 그 형식을 텍스트 데이터 형식으로 변환할 수 있다. 상기 텍스트 데이터 추출부는 상기 OCR 처리부를 통해 변환된 텍스트 데이터를 획득할 수 있다. 또한 상 기에서 획득한 필요 텍스트 데이터를 동기화부로 전달할 수 있다. 상기 동기화부는 상기 처리부와 추출부 들에서 전달된 음성데이터와 텍스트 데이터의 작업들 사이의 수행시 기를 동기화 할 수 있다. 즉, 동기화 작업을 할 수 있다. 상기 동기화부를 통해서는, 학습데이터가 출력될 수 있다. 상기 학습데이터에는, 상기 화자의 음성데이터, 및 상기 화자의 텍스트 데이터가 동기화되어 있을 수 있다. 상 기 학습데이터에는, 식별된 화자, 상기 화자의 음성데이터, 상기 화자의 텍스트 데이터, 상기 음성데이터의 시 간데이터, 상기 음성데이터의 지속시간이 동기화되어 포함될 수 있다. 상기 학습데이터에는, 다른 정보가 더 포 함될 수 있다. 상기되는 과정을 거치는 것에 의해서 수집이 어려운 음성인식장치용 학습데이터를 편리하게 수집할 수 있다. 상기 학습데이터수집장치에 의해서 수집된 상기 학습데이터는, 인공지능 학습부로 제공될 수 있다. 상기 인 공지능 학습부에는, 기계학습, CNN, 및 딥러닝 등의 다양한 학습 알고리즘을 사용할 수 있다. 상기 인공지 능 학습부는 상기 학습데이터를 상기 학습알고리즘에 적용하여 학습된 모델(학습모델)을 제공할 수 있다. 상기 학습모델은 음성인식장치에 탑재될 수 있다. 더 정확하게 상기 학습모델은 음성인식부에 탑재될 수 있다. 상기 음성인식장치는 장치의 동작에 필요한 하드웨어, IO장치, 및 프로세서를 포함하는 장치로 이해할 수 있다. 이 경우에, 상기 학습데이터수집장치는 원격지에서 상기 음성인식부 또는 음성인식장치와 온라인으로 연결될 수 있다. 상기 학습데이터 수집장치, 화자의 음성을 인식할 수 있는 음성 인식부, 및 상기 인공지능학습부를 포 함하는 전체구성이, 음성인식장치가 될 수도 있다. 이 경우에는, 단일의 하드웨어가 큰 연산용량, 및 저장용량 을 가질 수 있는 경우로서, 실시간 또는 계속해서 또는 연속해서 음성인식장치의 학습모델을 업데이트하여 동작 이 향상될 수도 있다. 이 경우에 상기 학습데이터 수집장치는 상기 음성인식부에 같은 기기 내에 포함될 수 있 다. 상기 구성을 참조하여, 제 1 실시예에 따른 학습데이터수집장치 및 음성인식장치의 작용을 설명한다. 상기 화자식별부는 상기 입력데이터에서 입력되는 데이터 정보 중, 필요한 화자를 식별할 수 있다. 예를 들어, 화자가 다수인 경우 그 중 데이터를 추출하고자 하는 화자만을 특정할 수 있다. 물론, 화자가 1 인일 경 우에도 그 과정이 적용될 수 있다. 즉, 화자 식별부를 거쳐 입력 데이터 중 어떤 화자가 필요한 지, 그 화자가 어디에 존재하는 지, 음성/시간데이터 추출부에 어떤 데이터를 보낼 지 식별할 수 있다. 상기 데이터 는 음성 데이터, 시간 데이터를 포함할 수 있다. 상기 음성/시간데이터 추출부는 화자 식별부에서 전송된 데이터를 추출할 수 있다. 상기 추출된 시간 데 이터를 이미지 추출부가 입력데이터에서 이미지를 추출하는 과정에 활용할 수 있다. 추출된 상기 시간 데 이터의 영상 데이터만을 추출하는 방식이다. 또, 상기 음성/시간데이터 추출부는 추출한 필요 음성 데이터 및 필요 시간 데이터를 동기화부로 전송할 수 있다. 상기 OCR 처리부는 상기 이미지 추출부에서 추출된 이미지 데이터 내에서 텍스트를 변환할 수 있다. 상기 OCR(광학적 문자 판독 장치)은, 이미지 데이터를 컴퓨터가 편집 가능한 문자코드(아스키 코드, 유니코드 등)의 형식, 즉 텍스트 데이터로 변환하는 기술을 말할 수 있다. 대표적인 변환방법으로는 빛을 비추어 그 반사 광선 을 전기 신호로 바꾸어 컴퓨터에 입력하는 방식을 사용할 수 있다. 상기 이미지 데이터는 이미지 스캐너를 통 하여 획득한 것일 수도 있고, 전용 소프트웨어를 사용한 것일 수도 있다. 필요하다면, 이 과정을 여러 번 거칠 수도 있다. 상기 텍스트 데이터 추출부는 OCR 처리부에서 전송된 텍스트 데이터 중에서 화자가 말한 음성데이터에 해 당하는 텍스트 데이터만을 추출할 수 있다. 상기 텍스트 데이터 추출부는 화면의 특정위치, 예를 들어, 화면 정 중앙의 아래에 있는 자막의 텍스트 데이터만을 추출할 수 있다. 또, 추출된 필요 텍스트 데이터를 동기화부 로 전송할 수 있다. 상기 동기화부는 전송된 음성 데이터와 텍스트 데이터 간의 수행시기를 동기화할 수 있다. 즉, 전송된 상기 시간 데이터를 이용해 음성 데이터와 텍스트 데이터가 동일한 시간 데이터에 수행되었는지를 확인하는 과정일 수 있다. 동기화부는, 음성 데이터와 텍스트 데이터를 같은 파일에 동기화하여 저장하거나, 음성 데이터와 텍스트 데이터 상호 호출이 가능하게 동기화 저장하는 등의 작업을 수행할 수 있다. 상기 동기화부는 상기 동기화된 데이터를 인공지능 학습부로 제공할 수 있다. 상기 인공지능 학습부는 여러 방법으로 상기 학습데이터를 학습할 수 있다. 기계 학습은 작업 수행 방법을 컴퓨터가 데이터를 바탕으로 스스로 학습하도록 하는 기술이다. 이를 통해 상기 인공지능 학습부는 상기 학 습 알고리즘을 적용하여 학습데이터를 학습한 학습 모델을 음성인식장치로 전달할 수 있다. 상기 학습모델은 상기 음성 인식부에 저장될 수 있다. 상기 음성인식장치는 외부의 음성데이터를 입력받고, 음성 인식부 를 통해 음성데이터를 인식할 수 있다. 인식된 음성데이터는 텍스트 데이터로 출력될 수 있다. 본 발명의 제 2 실시예를 설명한다. 본 발명의 제 2 실시예는 다른 구성은 제 1 실시예의 설명과 동일하고, 멀티미디어 데이터에 별도로 자막데이터 가 포함되어 있는 경우에 적용되는 정보이다. 제 1 실시예와 동일한 설명은 제 1 실시예의 설명이 그대로 적용 되는 것으로 하고, 설명을 생략한다. 도 2는 제 2 실시예에 따른 학습데이터 수집장치와 음성인식장치의 구성을 설명하는 도면이다. 도 2를 참조하여 제 2 실시예에 따른 학습데이터 수집장치를 설명한다. 제 2 실시예에 따른 학습데이터 수집장치에는, 입력데이터에서 화자를 식별하는 화자 식별부, 상기 화자 의 음성데이터와 현재 시간데이터를 추출하는 음성/시간데이터 추출부, 입력데이터로부터 텍스트 데이터 를 추출하는 텍스트 데이터 추출부, 및 상기 추출된 음성 데이터와 텍스트 데이터를 동기화하는 동기화부가 포함될 수 있다. 상기 구성을 참조하여, 2 실시예에 따른 학습데이터수집장치의 작용을 설명한다. 본 발명의 제 2 실시예는 다른 작용은 제 1 실시예의 설명과 동일하다. 단, 제 2 실시예에 따른 학습 데이터 수 집장치는 입력 데이터가 이미지 추출부 및 OCR 처리부를 통하지 않고, 텍스트 데이터 추출부로 직 접 전송될 수 있다. 상기 텍스트 데이터를 전송하는 과정에 음성/시간 데이터추출부에서 전송된 시간 데이터 를 참고할 수 있다. 상기 입력데이터는 상기 제 1 실시예와 같이 음성 데이터, 영상데이터, 텍스트 데이터를 포함할 수 있다. 텍스트 데이터가 입력데이터로부터 텍스트 데이터 추출부로 바로 전달될 수 있다. 이 때 텍스트 데이터는 이미 텍스트화 되어 존재하는 입력데이터일 수 있다. 그러므로 OCR과정이 필요 없을 수 있 다. 본 발명의 제 3 실시예를 설명한다. 도 3은 제 3 실시예에 따른 학습데이터 수집장치와 음성인식장치의 구성을 설명하는 도면이다. 도 3을 참조하여 제 3 실시예에 따른 학습데이터 수집장치를 설명한다. 본 발명의 제 3 실시예는, 제 3 실시예에 따른 학습데이터 수집장치에는, 입력데이터로부터 음성 데이터 및 시간 데이터를 추출할 수 있는 음성 처리부, 입력데이터로부터 텍스트 데이터를 추출할 수 있는 텍스트 처리부, 및 상기 추출된 음성 데이터와 텍스트 데이터를 동기화하는 동기화부, 입력데이터가 어떠한 데 이터인지를 식별하는 식별부가 포함될 수 있다. 상기 식별부는, 상기 입력데이터가 상기 제 1 실시예 및 상 기 제 2 실시예 중의 어느 실시예에 적합한지를 식별할 수 있다. 상기 제 3 실시예에 따른 학습데이터 수집장치의 각 구성을 상세하게 설명한다. 상기 음성 처리부는 제 1 또는 제 2 실시예에서, 화자 식별부또는 음성/시간 데이터 추출부, 또는 그 둘의 역할을 처리할 수 있다. 상기 화자 식별부와 데이터 추출부의 구성은 제 1실시예의 설명과 동일하다. 상기 텍스트 처리부는 제 1 또는 2실시예에서, 이미지 추출부또는 OCR 처리부, 또는 텍스트 데이터 추출부, 또는 상기 구성들 중 일부로 이루어진 구성의 역할을 처리할 수 있다. 상기의 이미지 추출부와 OCR 처리부, 텍스트 데이터 추출부의 구성은 제 1실시예의 설명과 동일하다. 상기 구성을 참조하여, 제 3 실시예에 따른 학습데이터 수집장치의 작용을 설명한다. 본 발명의 제 3 실시예의 다른 작용은 제 1 실시예의 설명과 동일하다. 도 3을 참조하면, 상기 입력 데이터는 식별부를 거친다. 입력 데이터가 식별부를 거치는 과정을 통해, 입력 데이터에 텍스트 데이터가 포함되는지 또는 자막이 텍스트 데이터 화 되어 있는지를 식별할 수 있다. 만약 텍스트 데이터가 포함되거나, 텍스트 데이터화 되어있지 않다면, 텍스트 처리부에서 OCR처리 과정을 거쳐 텍스트 데이터를 추출할 수 있다. 상세작용은 제 1 실시예를 참조할 수 있다. 만약 텍스트 데이터화 되어 있다면, 텍스트 처리부을 통해 데이터를 추출할 수 있다. 상세 작용은 제 2 실 시예를 참조할 수 있다. 도 4는 영상자료로부터 학습 데이터세트를 추출하는 방법을 설명하는 도면이다. 상기 방법의 설명에 있어서, 부 족한 설명은 상기 도 1 내지 도 3의 설명을 참조할 수 있다. 도 4를 참조하면, 먼저, 입력 데이터를 입력한다(ST1). 입력 데이터는 상기 실시예에서 기술한 바와 같이 영상, 음성, 자막 데이터를 포함할 수 있다. 전송된 입력데이터 내부에 다수의 화자가 존재하는 음성을 확인할 수 있 다(ST2). 그러나 상기 실시예와 같이 전송된 입력 데이터에는 한 명의 화자만 존재하는 경우도 있다. 다음에서는 데이터를 추출하고자 하는 화자를 특정할 수 있다(ST3). 입력된 데이터 내에 화자가 다수라면 화자 를 구분할 필요가 있을 것이다. 이 때 사용하는 방법은 음성 분리(Voice separation) 또는 음성 필터(Voice Filter)가 될 수 있다. 음성 분리(Voice separation)에 관한 논문은 Cornell university arXiv:2003.01531v4에 실린 Eliya Nachmani의 Voice separation with an unKnown Number of Multiple Speakers를 참조할 수 있다. 또 한 음성 필터(Voice Filter)에 관한 논문은 Cornell university arXiv:1810.04826v6에 실린 Quan Wang의 VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking을 참조할 수 있다. 그외에 다양한 방법으로 화자를 식별할 수 있다. 다음으로, 상기 화자 특정단계(ST3)의 특정 화자가 발화하는 시간 정보들을 추출한다(ST4). 다음으로, 시간정보 추출단계(ST4)에서 전송된 시간 정보를 토대로 특정 화자가 발화하는 시간의 이미지들을 추 출한다(ST5). 예를 들어, 영상은 많은 프레임을 포함할 수 있다. 따라서 특정 시간의 프레임을 캡처하는 방식으 로 이미지를 추출할 수 있다. 이 이미지 내에는 자막이 포함되어 있을 수 있다. 다음으로, 이미지 추출단계(ST5)에서 추출한 이미지에 있는 자막을 OCR과정을 거쳐 텍스트화 할 수 있다(ST6). 이후에, 텍스트화 단계(ST6)을 거쳐 텍스트화 된 데이터를 추출한다(ST7). 상기 도 1의 제 1 실시예에 따르면, 이 텍스트는 OCR과정 및 텍스트화 단계(ST 5, ST 6)을 거친 텍스트 일 수 있다. 상기 도 2의 제 2 실시예에 따르면, 이 텍스트는 입력데이터에서 바로 전송이 된 텍스트 데이터일 수 있다. 이 경우에는 상기 OCR과정 및 텍스트화 단계(ST 5, ST 6)을 거치치 않을 수도 있다. 다음으로, 음성 데이터와 텍스트 데이터를 동기화한다(ST8). 즉, 상기 실시예에서 기술한 대로, 추출된 음성 데 이터와 텍스트 데이터 사이의 수행시기를 맞출 수 있다. 상기 기술한 과정들을 거쳐서 음성 데이터 세트가 확 보된다. 도 5는 도 4의 이미지-텍스트화 단계(ST6)를 상세하게 설명하는 도면이다. 도 5를 참조하면, 먼저 상기 입력데이터로부터 획득한 이미지 데이터를 OCR 처리부에 입력한다(ST11). 상기 이미지 파일 입력 단계(ST11)에서 입력된 이미지 데이터를 이미지 처리한 후 분할한다(ST12). 분할 과정을 통해 이미지 데이터 내의 텍스트는 단락, 음절 별로 분할될 수 있다. 상기 단계를 이미지 처리를 통한 분할 단 계(ST12)라고 할 수 있다. 상기 이미지 처리를 통한 분할 단계(ST12)를 거친 이미지에서 데이터를 추출하기 전, 전처리를 해준다(ST13). 상기 전처리 과정은 이진화(Binarization)가 될 수도 있고, 얼룩을 제거하거나 라인을 제거해주는 기술일 수 있 다. 언급한 일련의 전처리 기술들을 통해 컴퓨터가 이미지 데이터 중 텍스트인 영역을 쉽게 인식할 수 있다. 즉 텍스트의 인식률을 높일 수 있다. 상기의 단계를 이미지 처리를 통한 전처리 단계(ST13)라고 할 수 있다. 상기 전처리 단계(ST13)를 거친 이미지 데이터를 CNN(Convolutional neural networks)에 입력해 이미지 데이터 내 텍스트의 획 또는 특징을 추출할 수 있다(ST14). 상기의 단계를 획 또는 특징 추출(ST14) 단계라고 한다. 상기 과정에서 추출된 획 또는 특징을 활용하여 CNN에서 입력된 이미지 데이터에 어떤 텍스트가 있는지 확률적 으로 계산을 한다. 상기 과정을 통계 및 모델을 통한 매칭 단계(ST15)라고 할 수 있다. 다음으로, 통계 및 모델을 통한 매칭 단계(ST15)를 거친 이미지 데이터는 후처리과정을 거쳐 출력할 수 있다. 상기의 단계를 후처리 후 인식결과 출력(ST16) 단계라고 할 수 있다. 후처리는 텍스트 데이터의 정확도를 향상 시키는 단계이다. 예를 들어, CNN은 출력전 데이터의 의미 혹은 맥락을 따져 불분명하거나 부정확한 부분이 있 는지 확인하게 된다. '이미지 출력'이 '아미지 출력'이라고 되었다면, 내용에 맞게 그 부분을 '이미지 출력'으 로 수정해주는 단계이다. 상기 과정들을 거쳐 그림파일에서 텍스트 파일을 추출할 수 있다. 이 과정을 통해 학습데이터 중 텍스트 데이터 를 효율적으로 추출할 수 있다. 상기 이미지-텍스트화 단계는 다른 방법이 사용될 수도 있다. 산업상 이용가능성 본 발명에 따르면, 높은 음성 인식률을 통하여 특정인(노년층 또는 사투리를 사용하는 사람)들의 인터넷 서비스 이용이 증가할 수 있다. 본 발명에 따르면, 상기의 장치 또는 방법은 음성인식 서비스를 제공하는 기업에게 공급될 수 있다. 도면 도면1 도면2 도면3 도면4 도면5"}
{"patent_id": "10-2021-0026378", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 제 11 실시예에 따른 장치의 기능 블록을 나타낸 도면. 도 2는 제 2 실시예에 따른 장치의 기능 블록을 나타낸 도면. 도 3은 제 3 실시예에 따른 장치의 기능 블록을 나타낸 도면. 도 4는 제 1 실시예에 따른 영상 자료를 음성 데이터셋으로 가공하는 프로세스를 개략적으로 나타낸 도면. 도 5는 본 발명의 이미지 - 텍스트 변 환을 위해 사용하는 OCR 프로세스를 나타낸 도면."}
