{"patent_id": "10-2024-0108876", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0131944", "출원번호": "10-2024-0108876", "발명의 명칭": "입모양을 기반으로 하는 얼굴 이미지 생성 방법, 모델의 트레이닝 방법 및 기기", "출원인": "베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니", "발명자": "판 시루이"}}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "입모양을 기반으로 하는 얼굴 이미지 생성 방법에 있어서, 인식할 오디오 데이터와 기설정된 얼굴 이미지를 획득하는 단계; 상기 인식할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 상기 오디오 특징은 발화 속도 특징과 의미 특징을 포함하는 단계; 상기 발화 속도 특징과 상기 의미 특징을 기초로 상기 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는얼굴 이미지를 생성하는 단계를 포함하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 인식할 오디오 데이터의 오디오 특징을 결정하는 단계는, 기설정된 제1 특징 추출 모델을 기초로, 상기 인식할 오디오 데이터의 발화 속도 특징을 결정하며; 여기서, 상기 제1 특징 추출 모델은 인식할 오디오 데이터로부터 발화 속도 특징을 추출하기 위한 것인 단계; 기설정된 제2 특징 추출 모델을 기초로, 상기 인식할 오디오 데이터의 의미 특징을 결정하며; 여기서, 상기 제2특징 추출 모델은 인식할 오디오 데이터로부터 의미 특징을 추출하기 위한 것인 단계를 포함하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서, 상기 기설정된 제1 특징 추출 모델을 기초로, 상기 인식할 오디오 데이터의 발화 속도 특징을 결정하는 단계는, 상기 인식할 오디오 데이터를 기설정된 제1 특징 추출 모델에 입력하여 특징 추출을 수행하여, 상기 인식할 오디오 데이터의 음성 사후 확률 특징을 획득하며; 여기서, 상기 음성 사후 확률 특징은 인식할 오디오 데이터의음소 카테고리의 정보를 나타내는 단계; 상기 인식할 오디오 데이터의 음성 사후 확률 특징을 기초로, 상기 인식할 오디오 데이터의 발화 속도 특징을결정하는 단계를 포함하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서, 상기 인식할 오디오 데이터의 음성 사후 확률 특징을 기초로, 상기 인식할 오디오 데이터의 발화 속도 특징을결정하는 단계는, 상기 음성 사후 확률 특징에 대해 고속 푸리에 변환 처리를 수행하여, 주파수 영역 신호 특징을 획득하며; 여기서, 상기 주파수 영역 신호 특징은 인식할 오디오 데이터의 음소 카테고리의 정보를 나타내는 단계; 기설정된 주파수 대역 크기를 기초로, 상기 주파수 영역 신호 특징을 적어도 두 개의 주파수 대역의 주파수 영역 신호 특징으로 분할하는 단계; 상기 적어도 두 개의 주파수 대역의 주파수 영역 신호 특징에 대해 적분 처리를 수행하여, 상기 인식할 오디오데이터의 발화 속도 특징을 획득하는 단계를 포함하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2 항에 있어서, 공개특허 10-2024-0131944-3-상기 기설정된 제2 특징 추출 모델을 기초로, 상기 인식할 오디오 데이터의 의미 특징을 결정하는 단계는, 상기 인식할 오디오 데이터를 기설정된 제2 특징 추출 모델에 입력하여 특징 추출을 수행하여, 상기 인식할 오디오 데이터의 의미 특징을 출력받는 단계를 포함하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서, 상기 발화 속도 특징과 상기 의미 특징을 기초로 상기 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는얼굴 이미지를 생성하는 단계는, 상기 발화 속도 특징과 상기 의미 특징을 기설정된 얼굴 입모양 결정 모델에 입력하여 처리하고, 처리하여 얻은결과와 상기 기설정된 얼굴 이미지를 기초로, 입모양을 갖는 얼굴 이미지를 생성하는 단계를 포함하는 입모양을기반으로 하는 얼굴 이미지 생성 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서, 상기 발화 속도 특징과 상기 의미 특징을 기설정된 얼굴 입모양 결정 모델에 입력하여 처리하고, 처리하여 얻은결과와 상기 기설정된 얼굴 이미지를 기초로, 입모양을 갖는 얼굴 이미지를 생성하는 단계는, 상기 기설정된 얼굴 입모양 결정 모델을 기반으로 상기 발화 속도 특징과 상기 의미 특징에 대해 스플라이싱 처리를 수행하여, 상기 인식할 오디오 데이터의 스플라이싱 특징을 획득하며; 여기서, 상기 스플라이싱 특징은 발화 속도 특징과 의미 특징을 나타내는 단계; 상기 기설정된 얼굴 입모양 결정 모델 중의 컨벌루션층을 기초로, 상기 스플라이싱 특징에 대해 특징 추출을 수행하여, 얼굴 구동 파라미터를 획득하며; 여기서, 상기 얼굴 구동 파라미터는 얼굴 이미지 중의 입모양이 변화되도록 구동하는 데 필요한 파라미터를 나타내는 단계; 상기 얼굴 구동 파라미터를 기초로 상기 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 생성하는 단계를 포함하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서, 상기 얼굴 구동 파라미터는 블랜드 쉐입 가중치 파라미터이고; 상기 얼굴 구동 파라미터를 기초로 상기 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 생성하는 단계는, 상기 블랜드 쉐입 가중치 파라미터를 기초로, 상기 기설정된 얼굴 이미지에 대응되는 얼굴 3차원 메시 데이터를결정하며; 여기서, 상기 얼굴 3차원 메시 데이터는 얼굴 이미지 상의 얼굴 표면을 나타내는 3차원 메시 모델의데이터인 단계; 상기 얼굴 3차원 메시 데이터를 기초로 상기 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을갖는 얼굴 이미지를 생성하는 단계를 포함하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 항에 있어서, 상기 인식할 오디오 데이터의 발화 속도 특징이 나타내는 수치가 기설정된 발화 속도 임계값보다 작은 것으로결정되면, 상기 의미 특징을 기초로 상기 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를생성하는 단계를 더 포함하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "얼굴 입모양 결정 모델의 트레이닝 방법에 있어서, 트레이닝할 이미지 데이터와 기설정된 얼굴 이미지를 획득하며; 여기서, 상기 트레이닝할 이미지 데이터는 트레이닝할 오디오 데이터와 트레이닝할 얼굴 이미지를 포함하고, 상기 트레이닝할 얼굴 이미지는 트레이닝할 오디공개특허 10-2024-0131944-4-오 데이터에 대응되는 입모양을 갖는 단계; 상기 트레이닝할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 상기 오디오 특징은 발화 속도 특징과 의미특징을 포함하는 단계; 상기 발화 속도 특징, 상기 의미 특징 및 상기 기설정된 얼굴 이미지를 기초로, 초기의 얼굴 입모양 결정 모델에 대해 트레이닝하여, 입모양을 갖는 얼굴 이미지를 획득하는 단계; 입모양을 갖는 얼굴 이미지와 상기 트레이닝할 얼굴 이미지가 일치하면, 트레이닝 완성된 얼굴 입모양 결정 모델이 획득된 것으로 결정하는 단계를 포함하는 얼굴 입모양 결정 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10 항에 있어서, 상기 트레이닝할 오디오 데이터의 오디오 특징을 결정하는 단계는, 기설정된 제1 특징 추출 모델을 기초로, 상기 트레이닝할 오디오 데이터의 발화 속도 특징을 결정하며; 여기서,상기 제1 특징 추출 모델은 트레이닝할 오디오 데이터로부터 발화 속도 특징을 추출하기 위한 것인 단계; 기설정된 제2 특징 추출 모델을 기초로, 상기 트레이닝할 오디오 데이터의 의미 특징을 결정하며; 여기서, 상기제2 특징 추출 모델은 트레이닝할 오디오 데이터로부터 의미 특징을 추출하기 위한 것인 단계를 포함하는 얼굴입모양 결정 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서, 상기 기설정된 제1 특징 추출 모델을 기초로, 상기 트레이닝할 오디오 데이터의 발화 속도 특징을 결정하는 단계는, 상기 트레이닝할 오디오 데이터를 기설정된 제1 특징 추출 모델에 입력하여 특징 추출을 수행하여, 상기 트레이닝할 오디오 데이터의 음성 사후 확률 특징을 획득하며; 여기서, 상기 음성 사후 확률 특징은 트레이닝할 오디오 데이터의 음소 카테고리의 정보를 나타내는 단계; 상기 트레이닝할 오디오 데이터의 음성 사후 확률 특징을 기초로, 상기 트레이닝할 오디오 데이터의 발화 속도특징을 결정하는 단계를 포함하는 얼굴 입모양 결정 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12 항에 있어서, 상기 트레이닝할 오디오 데이터의 음성 사후 확률 특징을 기초로, 상기 트레이닝할 오디오 데이터의 발화 속도특징을 결정하는 단계는, 상기 음성 사후 확률 특징에 대해 고속 푸리에 변환 처리를 수행하여, 주파수 영역 신호 특징을 획득하며; 여기서, 상기 주파수 영역 신호 특징은 트레이닝할 오디오 데이터의 음소 카테고리의 정보를 나타내는 단계; 기설정된 주파수 대역 크기를 기초로, 상기 주파수 영역 신호 특징을 적어도 두 개의 주파수 대역의 주파수 영역 신호 특징으로 분할하는 단계; 상기 적어도 두 개의 주파수 대역의 주파수 영역 신호 특징에 대해 적분 처리를 수행하여, 상기 트레이닝할 오디오 데이터의 발화 속도 특징을 획득하는 단계를 포함하는 얼굴 입모양 결정 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11 항에 있어서, 상기 기설정된 제2 특징 추출 모델을 기초로, 상기 트레이닝할 오디오 데이터의 의미 특징을 결정하는 단계는, 상기 트레이닝할 오디오 데이터를 기설정된 제2 특징 추출 모델에 입력하여 특징 추출을 수행하여, 상기 트레이닝할 오디오 데이터의 의미 특징을 출력받는 단계를 포함하는 얼굴 입모양 결정 모델의 트레이닝 방법.공개특허 10-2024-0131944-5-청구항 15 제10 항에 있어서, 상기 발화 속도 특징, 상기 의미 특징 및 상기 기설정된 얼굴 이미지를 기초로, 초기의 얼굴 입모양 결정 모델에 대해 트레이닝하여, 입모양을 갖는 얼굴 이미지를 획득하는 단계는, 상기 초기의 얼굴 입모양 결정 모델을 기반으로 상기 발화 속도 특징과 상기 의미 특징에 대해 스플라이싱 처리를 수행하여, 상기 트레이닝할 오디오 데이터의 스플라이싱 특징을 획득하며; 여기서, 상기 스플라이싱 특징은발화 속도 특징과 의미 특징을 나타내는 단계; 상기 초기의 얼굴 입모양 결정 모델 중의 컨벌루션층을 기초로, 상기 스플라이싱 특징에 대해 특징 추출을 수행하여, 얼굴 구동 파라미터를 획득하며; 여기서, 상기 얼굴 구동 파라미터는 얼굴 이미지 중의 입모양이 변화되도록 구동하는 데 필요한 파라미터를 나타내는 단계; 상기 얼굴 구동 파라미터를 기초로 상기 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 획득하는 단계를 포함하는 얼굴 입모양 결정 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15 항에 있어서, 상기 얼굴 구동 파라미터는 블랜드 쉐입 가중치 파라미터이고; 상기 얼굴 구동 파라미터를 기초로 상기 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 획득하는 단계는, 상기 블랜드 쉐입 가중치 파라미터를 기초로, 상기 기설정된 얼굴 이미지에 대응되는 얼굴 3차원 메시 데이터를결정하며; 여기서, 상기 얼굴 3차원 메시 데이터는 얼굴 이미지 상의 얼굴 표면을 나타내는 3차원 메시 모델의데이터인 단계; 상기 얼굴 3차원 메시 데이터를 기초로 상기 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을갖는 얼굴 이미지를 생성하는 단계를 포함하는 얼굴 입모양 결정 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제10 항에 있어서, 상기 트레이닝할 이미지 데이터를 획득하는 단계는, 상기 트레이닝할 오디오 데이터를 획득하는 단계; 상기 트레이닝할 오디오 데이터를 기초로 얼굴 이미지의 3차원 재구성 처리를 수행하여, 상기 트레이닝할 오디오 데이터에 대응되는 얼굴 3차원 메시 데이터를 획득하는 단계; 상기 트레이닝할 오디오 데이터에 대응되는 얼굴 3차원 메시 데이터를 기초로, 상기 트레이닝할 얼굴 이미지를획득하는 단계를 포함하는 얼굴 입모양 결정 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "입모양을 기반으로 하는 얼굴 이미지 생성 장치에 있어서, 인식할 오디오 데이터와 기설정된 얼굴 이미지를 획득하는 데이터 획득 유닛; 상기 인식할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 상기 오디오 특징은 발화 속도 특징과 의미 특징을 포함하는 특징 결정 유닛; 상기 발화 속도 특징과 상기 의미 특징을 기초로 상기 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는얼굴 이미지를 생성하는 이미지 생성 유닛을 포함하는 입모양을 기반으로 하는 얼굴 이미지 생성 장치."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "얼굴 입모양 결정 모델의 트레이닝 장치에 있어서, 트레이닝할 이미지 데이터와 기설정된 얼굴 이미지를 획득하며; 여기서, 상기 트레이닝할 이미지 데이터는 트레공개특허 10-2024-0131944-6-이닝할 오디오 데이터와 트레이닝할 얼굴 이미지를 포함하고, 상기 트레이닝할 얼굴 이미지는 트레이닝할 오디오 데이터에 대응되는 입모양을 갖는 이미지 획득 유닛; 상기 트레이닝할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 상기 오디오 특징은 발화 속도 특징과 의미특징을 포함하는 특징 추출 유닛; 상기 발화 속도 특징, 상기 의미 특징 및 상기 기설정된 얼굴 이미지를 기초로, 초기의 얼굴 입모양 결정 모델에 대해 트레이닝하여, 입모양을 갖는 얼굴 이미지를 획득하는 모델 트레이닝 유닛; 입모양을 갖는 얼굴 이미지와 상기 트레이닝할 얼굴 이미지가 일치하면, 트레이닝 완성된 얼굴 입모양 결정 모델이 획득된 것으로 결정하는 모델 획득 유닛을 포함하는 얼굴 입모양 결정 모델의 트레이닝 장치."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "전자기기에 있어서,적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하되; 여기서, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행될 수 있는 명령이 저장되어 있고, 상기 명령은 상기적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서가 제1 항 내지 제9 항 중 어느 한 항에따른 입모양을 기반으로 하는 얼굴 이미지 생성 방법을 수행할 수 있도록 하거나; 또는, 상기 적어도 하나의 프로세서가 제10 항 내지 제17 항 중 어느 한 항에 따른 얼굴 입모양 결정 모델의 트레이닝 방법을 수행할 수 있도록 하는 전자기기."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장매체에 있어서, 상기 컴퓨터 명령은 컴퓨터가 제1 항 내지 제9 항 중 어느 한 항에 따른 입모양을 기반으로 하는 얼굴 이미지생성 방법을 수행하도록 하기 위한 것이거나; 또는, 상기 컴퓨터 명령은 상기 컴퓨터가 제10 항 내지 제17 항중 어느 한 항에 따른 얼굴 입모양 결정 모델의 트레이닝 방법을 수행하도록 하기 위한 것인 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장매체."}
{"patent_id": "10-2024-0108876", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "컴퓨터 판독 가능 저장매체에 저장된 컴퓨터 프로그램에 있어서, 상기 컴퓨터 프로그램은 프로세서에 의해 실행될 때 제1 항 내지 제9 항 중 어느 한 항에 따른 입모양을 기반으로 하는 얼굴 이미지 생성 방법의 단계를 구현하거나; 또는, 상기 컴퓨터 프로그램은 상기 프로세서에 의해 실행될 때 제10 항 내지 제17 항 중 어느 한 항에 따른 얼굴 입모양 결정 모델의 트레이닝 방법의 단계를 구현하는 컴퓨터 판독 가능 저장매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2024-0108876", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 입모양을 기반으로 하는 얼굴 이미지 생성 방법, 모델의 트레이닝 방법 및 기기를 제공하며, 인공지능 분야에 관한 것으로, 특히 클라우드 컴퓨팅과 디지털 휴먼 분야에 관한 것이다. 구체적인 구현방안에 따르면, 인 식할 오디오 데이터와 기설정된 얼굴 이미지를 획득하고; 상기 인식할 오디오 데이터의 오디오 특징을 결정하고; 여기서, 상기 오디오 특징은 발화 속도 특징과 의미 특징을 포함하고; 상기 발화 속도 특징과 상기 의미 특징을 기초로 상기 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를 생성하는 것을 포함한다. 오 디오 데이터의 의미 특징과 발화 속도 특징을 결합하여, 임의의 발화 속도에서 얼굴 이미지 중의 입모양을 정확 하게 구동하는 것을 지원하고, 얼굴 이미지의 결정 정밀도를 향상시킨다."}
{"patent_id": "10-2024-0108876", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공지능 분야 중의 클라우드 컴퓨팅과 디지털 휴먼 분야에 관한 것으로, 특히 입모양을 기반으로 하 는 얼굴 이미지 생성 방법, 모델의 트레이닝 방법 및 기기에 관한 것이다."}
{"patent_id": "10-2024-0108876", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간지능 기술의 급속한 발전에 따라, 디지털 휴면 애플리케이션은 현재 연구하는 주요 대상이다. 디지털 휴먼 의 얼굴은 음성에 따라 변화될 수 있는데, 예를 들어, 디지털 휴먼의 얼굴 이미지 중의 표정과 입모양 등은 음 성의 변화에 따라 변화될 수 있다. 디지털 휴먼 애플리케이션 중의 하나의 핵심 기술은 오디오로 얼굴 입모양을 구동하는 기술인 바, 얼굴 이미지 중의 입모양을 어떻게 오디오 데이터와 정확하게 매칭시킬지는 시급히 해결해야 할 기술적 과제로 대두되고 있"}
{"patent_id": "10-2024-0108876", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "다.발명의 내용"}
{"patent_id": "10-2024-0108876", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 입모양을 기반으로 하는 얼굴 이미지 생성 방법, 모델의 트레이닝 방법 및 기기를 제공한다."}
{"patent_id": "10-2024-0108876", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 제1 측면에 따르면, 입모양을 기반으로 하는 얼굴 이미지 생성 방법을 제공하며, 상기 방법은, 인식할 오디오 데이터와 기설정된 얼굴 이미지를 획득하는 단계; 상기 인식할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 상기 오디오 특징은 발화 속도 특징과 의미 특 징을 포함하는 단계; 상기 발화 속도 특징과 상기 의미 특징을 기초로 상기 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를 생성하는 단계를 포함한다. 본 개시의 제2 측면에 따르면, 얼굴 입모양 결정 모델의 트레이닝 방법을 제공하며, 상기 방법은, 트레이닝할 이미지 데이터와 기설정된 얼굴 이미지를 획득하며; 여기서, 상기 트레이닝할 이미지 데이터는 트레 이닝할 오디오 데이터와 트레이닝할 얼굴 이미지를 포함하고, 상기 트레이닝할 얼굴 이미지는 트레이닝할 오디 오 데이터에 대응되는 입모양을 갖는 단계; 상기 트레이닝할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 상기 오디오 특징은 발화 속도 특징과 의미 특징을 포함하는 단계; 상기 발화 속도 특징, 상기 의미 특징 및 상기 기설정된 얼굴 이미지를 기초로, 초기의 얼굴 입모양 결정 모델 에 대해 트레이닝하여, 입모양을 갖는 얼굴 이미지를 획득하는 단계; 입모양을 갖는 얼굴 이미지와 상기 트레이닝할 얼굴 이미지가 일치하면, 트레이닝 완성된 얼굴 입모양 결정 모 델이 획득된 것으로 결정하는 단계를 포함한다. 본 개시의 제3 측면에 따르면, 입모양을 기반으로 하는 얼굴 이미지 생성 장치를 제공하며, 상기 장치는, 인식할 오디오 데이터와 기설정된 얼굴 이미지를 획득하는 데이터 획득 유닛; 상기 인식할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 상기 오디오 특징은 발화 속도 특징과 의미 특 징을 포함하는 특징 결정 유닛; 상기 발화 속도 특징과 상기 의미 특징을 기초로 상기 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를 생성하는 이미지 생성 유닛을 포함한다. 본 개시의 제4 측면에 따르면, 얼굴 입모양 결정 모델의 트레이닝 장치를 제공하며, 상기 장치는, 트레이닝할 이미지 데이터와 기설정된 얼굴 이미지를 획득하며; 여기서, 상기 트레이닝할 이미지 데이터는 트레 이닝할 오디오 데이터와 트레이닝할 얼굴 이미지를 포함하고, 상기 트레이닝할 얼굴 이미지는 트레이닝할 오디 오 데이터에 대응되는 입모양을 갖는 이미지 획득 유닛; 상기 트레이닝할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 상기 오디오 특징은 발화 속도 특징과 의미 특징을 포함하는 특징 추출 유닛; 상기 발화 속도 특징, 상기 의미 특징 및 상기 기설정된 얼굴 이미지를 기초로, 초기의 얼굴 입모양 결정 모델 에 대해 트레이닝하여, 입모양을 갖는 얼굴 이미지를 획득하는 모델 트레이닝 유닛; 입모양을 갖는 얼굴 이미지와 상기 트레이닝할 얼굴 이미지가 일치하면, 트레이닝 완성된 얼굴 입모양 결정 모 델이 획득된 것으로 결정하는 모델 획득 유닛을 포함한다. 본 개시의 제5 측면에 따르면, 전자기기를 제공하며, 상기 전자기기는, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하되; 여기서, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행될 수 있는 명령이 저장되어 있고, 상기 명령은 상기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서가 본 개시의 제1 측면과 제2 측면에 따 른 방법을 수행할 수 있도록 한다. 본 개시의 제6 측면에 따르면, 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장매체를 제공하며, 여기서, 컴퓨터 명령은 상기 컴퓨터가 본 개시의 제1 측면과 제2 측면에 따른 방법을 수행하도록 하기 위한 것 이다. 본 개시의 제7 측면에 따르면, 컴퓨터 프로그램을 제공하며, 상기 컴퓨터 프로그램은 판독 가능 저장매체에 저 장되고, 전자기기의 적어도 하나의 프로세서는 상기 판독 가능 저장매체로부터 상기 컴퓨터 프로그램을 읽을 수 있으며, 상기 적어도 하나의 프로세서는 상기 컴퓨터 프로그램을 실행하여 전자기기가 제1 측면에 따른 방법을 수행하도록 하거나; 또는, 상기 적어도 하나의 프로세서는 상기 컴퓨터 프로그램을 실행하여 전자기기가 제2 측 면에 따른 방법을 수행하도록 한다."}
{"patent_id": "10-2024-0108876", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 기술에 따르면, 입모양을 기반으로 하는 얼굴 이미지의 생성 정밀도가 향상된다. 응당 이해해야 할 것은, 본 부분에 기재되는 내용은 본 개시의 실시예의 키포인트 또는 중요특징을 표시하려는 목적이 아니고, 본 개시의 범위를 제한하기 위한 것도 아니다. 본 개시의 기타 특징은 이하의 명세서에 의해 쉽 게 이해된다."}
{"patent_id": "10-2024-0108876", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 첨부 도면을 결합하여 본 개시의 예시적 실시예를 설명하며, 여기서 이해의 편의를 위하여 본 개시의 실시 예의 다양한 세부 사항을 포함하는 바, 이들은 단지 예시적인 것으로 간주하여야 한다. 따라서, 본 분야의 통상 의 지식을 가진 자라면, 여기서 설명하는 실시예에 대해 다양한 변경과 수정을 가할 수 있으며, 본 개시의 범위 와 정신을 벗어나지 않는다는 것을 인식하여야 한다. 마찬가지로, 명확성과 간략성을 위해, 이하의 설명에서는 공지 기능과 구조에 대한 설명을 생략한다. 기존의 디지털 휴먼 애플리케이션에서, 하나의 핵심 기술은 오디오로 얼굴 입모양을 구동하는 것으로서, 즉, 오 디오 데이터를 통해 얼굴 이미지 중의 입모양을 변화시켜, 얼굴 이미지 중의 입모양이 오디오 데이터와 일치하 도록 한다. 따라서, 얼굴 입모양을 어떻게 보다 실감있고 정확하게 구동할지는 시급히 해결해야 할 기술적 과제 이다. 관련 입모양을 기반으로 하는 얼굴 이미지 생성 방법에서, 발화 속도의 변화를 처리하기 어렵고, 오디오 데이터 의 발화 속도는 입모양에 매우 큰 영향을 미친다. 같은 한마디의 말이 다른 발화 속도로 발화될 때, 대응되는 입모양이 완전히 다를 수 있다. 발화 속도가 보다 느릴 때, 각각의 글자의 입모양은 모두 발음과 완전히 맞추어 질 수 있다. 그러나 발화 속도가 빨라질 때, 얼굴 이미지 중의 입모양은 등비례로 가속되는 것이 아니며, 하나 의 입모양을 미처 완성하지 못한 채 다음 글자를 발음해야 할 수 있다. 이에 따라 많은 글자들의 입모양이 모두 변화되면서, “말을 삼키기”와 “연독”과 같은 다양한 현상이 발생하게 되는데, 많은 입모양들이 누락, 융합 또는 간소화되어, 얼굴 이미지의 생성 정밀도에 영향을 미친다. 본 개시는 인공지능 분야 중의 클라우드 컴퓨팅과 디지털 휴먼 분야에 적용되어, 입모양을 갖는 얼굴 이미지의 생성 정밀도를 향상시키는 입모양을 기반으로 하는 얼굴 이미지 생성 방법, 모델의 트레이닝 방법 및 기기를 제 공한다. 설명할 필요가 있는 것은, 본 실시예 중의 모델은 어떤 특정 사용자를 대상으로 하는 것이 아니며, 어떤 특정 사용자의 개인 정보를 반영할 수 없다. 설명할 필요가 있는 것은, 본 실시예 중의 얼굴 이미지는 공개된 데이터 세트로부터 유래된다. 본 개시의 기술방안에서, 관련되는 사용자 개인 정보의 수집, 저장, 사용, 가공, 전송, 제공과 공개 등의 처리 는, 모두 관련 법률 법규의 규정에 부합되며, 공서 양속에 위배되지 않는다. 읽는 자가 본 개시의 구현 원리를 더욱 깊이 이해할 수 있도록 하기 위하여, 아래에서는 도 1 내지 도 10을 결 합하여 실시예를 추가적으로 세분화한다. 도 1은 본 개시에 따른 실시예에서 제공하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법의 흐름도이며, 해 당 방법은 입모양을 기반으로 하는 얼굴 이미지 생성 장치에 의해 수행될 수 있다. 도 1에 도시된 바와 같이, 해당 방법은 아래의 단계를 포함한다. S101, 인식할 오디오 데이터와 기설정된 얼굴 이미지를 획득한다. 예시적으로, 디지털 휴먼의 얼굴을 미리 설계하되, 예를 들어, 디지털 휴먼의 얼굴형, 눈, 코와 입 등을 설계하 여, 하나의 기설정된 얼굴 이미지를 생성할 수 있다. 디지털 휴먼은 기설정된 얼굴 이미지의 기초 상에서 입모 양을 변화를 수행할 수 있으며, 예를 들어, 기설정된 얼굴 이미지에서, 디지털 휴먼의 입은 다물어진 상태이고, 디지털 휴먼의 입모양은 오디오 데이터가 전송됨에 따라 변할 수 있다. 인식할 오디오 데이터는 미리 준비한 오디오 데이터이고, 디지털 휴먼의 얼굴 이미지에서, 입모양은 인식할 오 디오 데이터에 따라 변해야 한다. 기설정된 인식할 오디오 데이터, 및 기설정된 얼굴 이미지를 획득한다. 인식 할 오디오 데이터는 오디오 스트림이고, 기설정된 얼굴 이미지는 2차원 또는 3차원 이미지일 수 있다. S102, 인식할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 오디오 특징은 발화 속도 특징과 의미 특징을 포함한다. 예시적으로, 인식할 오디오 데이터를 획득한 후, 인식할 오디오 데이터에 대해 특징 추출을 수행하여, 인식할 오디오 데이터의 오디오 특징을 획득한다. 오디오 특징은 발화 속도 특징과 의미 특징 등을 포함할 수 있다. 발 화 속도 특징은 인식할 오디오 데이터 중 음소의 변화 속도를 나타낼 수 있는 바, 예를 들어, 발화 속도 특징은 1초 내에 출력되는 음소의 수량으로 나타낼 수 있으며, 즉 인식할 오디오 데이터 중의 음소 수량과 인식할 오디 오 데이터의 시간을 결정하고, 음소 수량을 인식할 오디오 데이터의 시간으로 나누어, 인식할 오디오 데이터의 발화 속도 크기를 획득하여 발화 속도 특징으로 한다. 본 실시예에서, 음소 수량을 인식할 오디오 데이터의 시 간으로 나누어 인식할 오디오 데이터의 평균적인 발화 속도 특징을 결정할 수 있고, 인식할 오디오 데이터 중 상이한 음소에 대응되는 발화 속도 특징을 결정할 수도 있다. 의미 특징은 인식할 오디오 데이터 중의 음소가 표현하는 의미를 나타낼 수 있다. 인식할 오디오 데이터는 복수 의 음소를 포함할 수 있으며, 인식할 오디오 데이터에 대해, 각 음소의 의미 특징을 결정할 수 있다. 즉, 인식 할 오디오 데이터에 대해 음소 분할을 수행하여, 인식할 오디오 데이터 중의 각 음소를 획득하고, 음소에 대해 의미 인식을 수행하여, 의미 특징을 결정할 수 있다. 예를 들어, 기설정된 의미 인식 모델을 이용하여 의미 인 식을 수행할 수 있으며, 의미 인식 모델은 뉴럴 네트워크 모델일 수 있다. 음소와 의미 사이의 연관 관계를 미 리 설정하고, 기설정된 연관 관계를 기초로, 인식할 오디오 데이터 중 각 음소의 의미 특징을 찾아, 인식할 오 디오 데이터의 의미 특징으로 할 수도 있다. S103, 발화 속도 특징과 의미 특징을 기초로 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미 지를 생성한다. 예시적으로, 발화 속도 특징과 의미 특징을 획득한 후, 발화 속도 특징과 의미 특징을 기초로, 기설정된 얼굴 이미지에 대해 처리하여, 기설정된 얼굴 이미지 중의 입모양이 변하도록 제어하여, 입모양을 갖는 얼굴 이미지 를 획득할 수 있다. 예를 들어, 인식할 오디오 데이터에서 나오는 소리가 “아”일 경우, 얼굴 이미지 상의 입 모양은 “아”의 입모양이다. 본 실시예에서, 의미 특징과 발화 속도 특징을 기초로 얼굴 이미지 중의 입모양을 결정하여, 인식할 오디오 데이터에 대응되는 복수의 얼굴 이미지를 획득할 수 있다. 또한, 복수의 얼굴 이미지 를 기초로, 인식할 오디오 데이터의 얼굴 동영상을 결정할 수 있다. 입모양과 발화 속도 특징 사이의 연관 관계, 입모양과 의미 특징의 연관 관계를 미리 설정할 수 있고, 입모양과 발화 속도 특징 및 의미 특징 사이의 연관 관계를 미리 설정할 수도 있다. 기설정된 연관 관계를 기초로, 발화 속도 특징 및 의미 특징에 대응되는 입모양을 결정하여, 입모양을 갖는 얼굴 이미지를 생성한다. 입모양을 결정 하기 위한 하나의 뉴럴 네트워크 모델을 미리 트레이닝하고, 발화 속도 특징과 의미 특징을 입력 데이터로 하여, 해당 뉴럴 네트워크 모델로 입력하여, 입모양을 갖는 얼굴 이미지를 출력할 수도 있다. 본 실시예에서, 해당 방법은, 인식할 오디오 데이터의 발화 속도 특징이 나타내는 수치가 기설정된 발화 속도 임계값보다 작은 것으로 결정되면, 의미 특징을 기초로 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를 생성하는 단계를 더 포함한다. 구체적으로, 말하는 발화 속도가 보다 느릴 때, 각 글자의 입모양이 발음과 완전히 맞추어질 수 있지만, 발화 속도가 보다 빠를 때 하나의 입모양을 미처 하지 못한 채 다음 글자를 발음해야 할 수 있으므로, 많은 입모양들 의 누락, 융합 및 간소화 등의 상황이 발생하게 된다. 하나의 발화 속도 임계값을 미리 설정하고, 발화 속도 특징을 획득한 후, 발화 속도 특징이 나타내는 수치와 기 설정된 발화 속도 임계값을 비교할 수 있다. 인식할 오디오 데이터의 발화 속도 특징이 나타내는 수치가 기설정 된 발화 속도 임계값보다 같거나 큰 것으로 결정되면, 발화 속도가 보다 빠르다는 것을 의미하며, 발화 속도 특 징과 의미 특징을 기초로 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를 생성할 수 있다. 인식할 오디오 데이터의 발화 속도 특징이 나타내는 수치가 기설정된 발화 속도 임계값보다 작은 것으로 결정되 면, 인식할 오디오 데이터의 발화 속도가 보다 느린 것으로 결정하고, 의미 특징만 이용하여 기설정된 얼굴 이 미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를 생성할 수 있다. 예를 들어, 의미 특징만 기설정된 뉴럴 네트워크 모델의 입력 데이터로 하고, 의미 특징에 대해 컨벌루션 등의 처리를 수행하여, 얼굴 이미지 처리 시 의 연산량을 줄일 수 있다. 이러한 구성은, 인식할 오디오 데이터의 발화 속도가 보다 느릴 때, 의미 특징만 이용하여서도 정확한 입모양을 얻고, 연산량을 줄일 수 있으며, 얼굴 이미지의 생성 효율이 향상되는 유리한 효과가 있다. 본 개시의 실시예에서, 인식할 오디오 데이터를 획득하고, 인식할 오디오 데이터로부터 발화 속도 특징과 의미 특징을 결정한다. 발화 속도 특징과 의미 특징을 결합하여, 기설정된 얼굴 이미지에 대해 처리한다. 여기서, 기 설정된 얼굴 이미지는 입모양이 변화될 때 의거하는 초기 이미지로서, 얼굴의 외모를 나타낼 수 있다. 발화 속 도 특징과 의미 특징을 기초로, 다른 입모양을 갖는 얼굴 이미지를 생성하여, 얼굴 이미지의 입모양이 인식할 오디오 데이터와 매칭되도록 한다. 발화 속도가 보다 빠를 때, 얼굴 이미지의 입모양이 말을 삼키는 것과 연독 하는 문제를 해결한다. 얼굴 이미지 중 입모양을 정확하게 구동하고, 얼굴 이미지의 결정 정밀도를 향상시킨다. 도 2는 본 개시의 실시예에서 제공하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법의 흐름도로서, 해당 실 시예는 상술한 실시예를 기초로 하는 선택적인 실시예이다. 본 실시예에서, 인식할 오디오 데이터의 오디오 특징을 결정하는 단계는, 기설정된 제1 특징 추출 모델을 기초 로, 인식할 오디오 데이터의 발화 속도 특징을 결정하며; 여기서, 제1 특징 추출 모델은 인식할 오디오 데이터 로부터 발화 속도 특징을 추출하기 위한 것인 단계; 기설정된 제2 특징 추출 모델을 기초로, 인식할 오디오 데 이터의 의미 특징을 결정하며; 여기서, 제2 특징 추출 모델은 인식할 오디오 데이터로부터 의미 특징을 추출하 기 위한 것인 단계로 세분화할 수 있다. 도 2에 도시된 바와 같이, 해당 방법은 아래의 단계들을 포함한다. S201, 인식할 오디오 데이터와 기설정된 얼굴 이미지를 획득한다. 예시적으로, 본 단계는 상술한 단계(S101)를 참조할 수 있으며, 반복되는 설명을 생략한다. S202, 기설정된 제1 특징 추출 모델을 기초로, 인식할 오디오 데이터의 발화 속도 특징을 결정하며; 여기서, 제 1 특징 추출 모델은 인식할 오디오 데이터로부터 발화 속도 특징을 추출하기 위한 것이다. 예시적으로, 하나의 제1 특징 추출 모델을 미리 구성하며, 제1 특징 추출 모델은 사전에 결정된 뉴럴 네트워크 모델로서, 인식할 오디오 데이터로부터 발화 속도 특징을 추출하기 위한 것일 수 있다. 인식할 오디오 데이터를 제1 특징 추출 모델에 입력하여 처리하여, 인식할 오디오 데이터의 발화 속도 특징을 획득한다. 예를 들어, 제1 특징 추출 모델은 컨벌루션층, 풀링층 등의 네트워크 계층이 포함될 수 있고, 인식할 오디오 데이터에 대해 컨 벌루션 처리와 특징 추출을 수행하여, 인식할 오디오 데이터의 발화 속도 특징을 획득할수 있다. 본 실시예에서, 제1 특징 추출 모델의 네트워크 구조에 대해 구체적으로 한정하지 않는다. 본 실시예에서, 기설정된 제1 특징 추출 모델을 기초로, 인식할 오디오 데이터의 발화 속도 특징을 결정하는 단 계는, 인식할 오디오 데이터를 기설정된 제1 특징 추출 모델에 입력하여 특징 추출을 수행하여, 인식할 오디오 데이터의 음성 사후 확률 특징을 획득하며; 여기서, 음성 사후 확률 특징은 인식할 오디오 데이터의 음소 카테 고리의 정보를 나타내는 단계; 인식할 오디오 데이터의 음성 사후 확률 특징을 기초로, 인식할 오디오 데이터의 발화 속도 특징을 결정하는 단계를 포함한다. 구체적으로, 제1 특징 추출 모델은 ASR(Automatic Speech Recognition, 자동 음성 인식) 모델일 수 있고, ASR 모델은 복수층의 네트워크 계층을 포함할 수 있는 바, 예를 들어, 컨벌루션층, 풀링층과 전체 연결층을 포함할 수 있다. 인식할 오디오 데이터를 기설정된 ASR 모델에 입력하여 특징을 추출하며, 예를 들어, 컨벌루션층에 의 해 특징을 추출하여, 인식할 오디오 데이터의 PPG(Phonetic Posteriorgram, 음성 사후 확률) 특징을 획득할 수 있다. PPG 특징은 하나의 카테고리에 대한 시간 매트릭스로서, 하나의 발화의 각 특정 시간 프레임에 대해, 각 음성 카테고리의 사후 확률을 나타낼 수 있다. PPG 특징은 2차원 좌표축의 이미지로 나타낼 수 있으며, 인식할 오디오 데이터의 음소 카테고리의 정보를 나타내고, 횡좌표는 시간을 나타내고, 종좌표는 음소 카테고리를 나타 낸다. PPG 특징을 획득한 후, 기설정된 발화 속도 결정 알고리즘을 기초로, PPG 특징에 대해 연산하여, PPG 특징을 인 식할 오디오 데이터의 발화 속도 특징으로 변환할 수 있다. 음소의 변화 속도를 산출하여, 발화 속도의 크기로 할 수 있으며, 발화 속도 특징에 대한 명시적 모델링(Explicit Modeling)을 구현한다. 본 실시예에서, 기설정된 발화 속도 결정 알고리즘에 대해 구체적으로 한정하지 않는다. 이러한 구성은, 인식할 오디오 데이터를 자동 음성 인식 모델에 입력하고 처리하여, 인식할 오디오 데이터의 PPG 특징을 획득하고, PPG 특징에 대해 추가적인 연산을 수행하여 발화 속도 특징을 획득한다. 발화 속도에 대 한 명시적 모델링을 구현함으로써, 발화 속도 특징을 도입하여, 발화 속도가 변화될 때 오디오로 입모양을 구동 하는 정밀도와 진실성을 대폭 향상시키는 유리한 효과가 있다. 본 실시예에서, 인식할 오디오 데이터의 음성 사후 확률 특징을 기초로, 인식할 오디오 데이터의 발화 속도 특 징을 결정하는 단계는, 음성 사후 확률 특징에 대해 고속 푸리에 변환 처리를 수행하여, 주파수 영역 신호 특징 을 획득하며; 여기서, 주파수 영역 신호 특징은 인식할 오디오 데이터의 음소 카테고리의 정보를 나타내는 단계; 기설정된 주파수 대역 크기를 기초로, 주파수 영역 신호 특징을 적어도 두 개의 주파수 대역의 주파수 영 역 신호 특징으로 분할하는 단계; 적어도 두 개의 주파수 대역의 주파수 영역 신호 특징에 대해 적분 처리를 수 행하여, 인식할 오디오 데이터의 발화 속도 특징을 획득하는 단계를 포함한다. 구체적으로, PPG 특징은 시간 영역 신호이고, 인식할 오디오 데이터의 PPG 특징을 획득한 후, PPG 특징에 대해 고속 푸리에 변환 처리를 수행할 수 있다. 즉, FFT(Fast Fourier Transform, 고속 푸리에 변환)에 의해, PPG 특징을 주파수 영역으로 변환하여, PPG 특징에 대응되는 주파수 영역 신호 특징을 획득한다. 해당 주파수 영역 신호 특징은 인식할 오디오 데이터의 음소 카테고리의 정보로 표시될 수도 있다. 주파수 영역 신호 특징에 대해 주파수 대역별로 적분하여, 원하는 주파수를 산출하고, 발화 속도 크기로 하면, 인식할 오디오 데이터의 발화 속도 특징이 획득된다. 발화 속도 특징을 계산할 때, 주파수 대역 크기를 미리 설 정하고, 기설정된 주파수 대역 크기를 기초로, 주파수 영역 신호 특징에 대해 분할하여, 복수의 주파수 대역 크 기의 주파수 영역 신호 특징을 획득할 수 있다. 각각의 주파수 대역 크기의 주파수 영역 신호 특징에 대해 하나 씩 적분 처리하며, 적분 결과는 인식할 오디오 데이터 중 음소 변화 속도를 반영할 수 있으며, 즉 발화 속도 특징이 된다. 이러한 구성은, FFT 처리와 적분 계산을 수행하여, PPG 특징을 구체적인 발화 속도 크기로 변환하여, 발화 속도 특징을 결정하는 것을 구현할 수 있으며, 이에 따라 얼굴 이미지의 생성 정밀도가 향상되는 유리한 효과가 있다. S203, 기설정된 제2 특징 추출 모델을 기초로, 인식할 오디오 데이터의 의미 특징을 결정하며; 여기서, 제2 특 징 추출 모델은 인식할 오디오 데이터로부터 의미 특징을 추출하기 위한 것이다. 예시적으로, 제2 특징 추출 모델은 사전에 트레이닝된 뉴럴 네트워크 모델일 수도 있으며, 예를 들어, 제2 특징 추출 모델은 기설정된 의미 인식 모델이다. 제2 특징 추출 모델은 특징 추출 네트워크를 포함하고, 기설정된 제 2 특징 추출 모델을 기초로, 인식할 오디오 데이터에 대해 의미 특징 추출을 수행하여, 인식할 오디오 데이터의 의미 특징을 획득할 수 있다. 제1 특징 추출 모델과 제2 특징 추출 모델에 의해, 발화 속도 특징과 의미 특징을 빠르게 얻을 수 있으며, 발화 속도 특징과 의미 특징을 각각 추출하여, 특징 추출의 효율을 향상시키고, 나아가 얼굴 이미지의 생성 효율을 향상시킨다. 본 실시예에서, 기설정된 제2 특징 추출 모델을 기초로, 인식할 오디오 데이터의 의미 특징을 결정하는 단계는, 인식할 오디오 데이터를 기설정된 제2 특징 추출 모델에 입력하여 특징 추출을 수행하여, 인식할 오디오 데이터 의 의미 특징을 출력받는 단계를 포함한다. 구체적으로, 제2 특징 추출 모델은 의미 인식 모델일 수 있고, 의미 인식 모델은 복수층의 컨벌루션층 등의 네 트워크 계층이 포함될 수 있으며, 특징 추출 네트워크를 구성한다. 인식할 오디오 데이터를 기설정된 의미 인식 모델에 입력하여 처리하며, 예를 들어, 컨벌루션층에 의해 특징을 추출하여, 인식할 오디오 데이터의 의미 특징 을 획득할 수 있다. 인식할 오디오 데이터는 스트리밍 데이터로서, 추출해 낸 의미 특징은 스트리밍 특징일 수 있다. 본 실시예에서, 의미 인식 모델의 모델 구조에 대해 구체적으로 한정하지 않는다. 이러한 구성은, 입력받은 오디오 스트림 데이터에 대해 의미 특징을 자동으로 추출하여, 의미 특징의 결정 효율 과 정밀도를 향상시키고, 나아가 얼굴 이미지의 생성 효율과 정밀도를 향상시키는 유리한 효과가 있다. S204, 발화 속도 특징과 의미 특징을 기초로 상기 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를 생성한다. 예시적으로, 본 단계는 상술한 단계(S103)를 참조할 수 있으며, 반복되는 설명을 생략한다. 본 개시의 실시예에서, 인식할 오디오 데이터를 획득하고, 인식할 오디오 데이터로부터 발화 속도 특징과 의미 특징을 결정한다. 발화 속도 특징과 의미 특징을 결합하여, 기설정된 얼굴 이미지에 대해 처리한다. 여기서, 기 설정된 얼굴 이미지는 입모양이 변화될 때 의거하는 초기 이미지로서, 얼굴의 외모를 나타낼 수 있다. 발화 속 도 특징과 의미 특징을 기초로, 다른 입모양을 갖는 얼굴 이미지를 생성하여, 얼굴 이미지의 입모양이 인식할 오디오 데이터와 매칭되도록 한다. 발화 속도가 보다 빠를 때, 얼굴 이미지의 입모양이 말을 삼키는 것과 연독 하는 문제를 해결한다. 얼굴 이미지 중 입모양을 정확하게 구동하고, 얼굴 이미지의 결정 정밀도를 향상시킨다. 도 3은 본 개시의 실시예에서 제공하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법의 흐름도로서, 해당 실 시예는 상술한 실시예를 기초로 하는 선택적인 실시예이다. 본 실시예에서, 발화 속도 특징과 의미 특징을 기초로 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를 생성하는 단계는, 발화 속도 특징과 의미 특징을 기설정된 얼굴 입모양 결정 모델에 입력하여 처 리하고, 처리하여 얻은 결과와 기설정된 얼굴 이미지를 기초로, 입모양을 갖는 얼굴 이미지를 생성하는 단계로 세분화할 수 있다. 도 3에 도시된 바와 같이, 해당 방법은 아래의 단계들을 포함한다. S301, 인식할 오디오 데이터와 기설정된 얼굴 이미지를 획득한다. 예시적으로, 본 단계는 상술한 단계(S101)를 참조할 수 있으며, 반복되는 설명을 생략한다. S302, 인식할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 오디오 특징은 발화 속도 특징과 의미 특징을 포함한다. 예시적으로, 본 단계는 상술한 단계(S102)를 참조할 수 있으며, 반복되는 설명을 생략한다. S303, 발화 속도 특징과 의미 특징을 기설정된 얼굴 입모양 결정 모델에 입력하여 처리하고, 처리하여 얻은 결 과와 기설정된 얼굴 이미지를 기초로, 입모양을 갖는 얼굴 이미지를 생성한다. 예시적으로, 하나의 얼굴 입모양 결정 모델을 사전에 모델링하고 트레이닝하며, 얼굴 입모양 결정 모델은 하나 의 뉴럴 네트워크 모델로서, 입모양을 갖는 얼굴 이미지를 출력하는 데 사용될 수 있다. 발화 속도 특징과 의미 특징을 입력 데이터로 하여, 기설정된 얼굴 입모양 결정 모델에 입력하여 처리한다. 얼굴 입모양 결정 모델은 처리한 후, 처리 결과를 기초로, 기설정된 얼굴 이미지에 대해 입모양을 변경시켜, 입모양을 갖는 얼굴 이미지 를 획득할 수 있다. 예를 들어, 얼굴 입모양 결정 모델이 발화 속도 특징과 의미 특징을 기초로 결정한 처리 결 과는 입모양의 크기와 형상 정보일 수 있고, 결정된 입모양의 크기와 형상 정보를 기초로, 기설정된 얼굴 이미 지에 대해 렌더링하여, 해당 입모양을 포함하는 얼굴 이미지를 생성한다. 얼굴 입모양 결정 모델을 사용함으로 써, 신속하게 얼굴 이미지를 획득할 수 있고, 발화 속도 특징과 의미 특징을 결합함으로써, 발화 속도 변화로 인해 오디오가 얼굴 입모양을 구동하는 효과가 저하되는 문제를 방지하고, 얼굴 이미지의 생성 효율과 정밀도가 향상된다. 본 실시예에서, 발화 속도 특징과 의미 특징을 기설정된 얼굴 입모양 결정 모델에 입력하여 처리하고, 처리하여 얻은 결과와 기설정된 얼굴 이미지를 기초로, 입모양을 갖는 얼굴 이미지를 생성하는 단계는, 기설정된 얼굴 입 모양 결정 모델을 기반으로 발화 속도 특징과 의미 특징에 대해 스플라이싱 처리를 수행하여, 인식할 오디오 데 이터의 스플라이싱 특징을 획득하며; 여기서, 스플라이싱 특징은 발화 속도 특징과 의미 특징을 나타내는 단계; 기설정된 얼굴 입모양 결정 모델 중의 컨벌루션층을 기초로, 스플라이싱 특징에 대해 특징 추출을 수행하여, 얼 굴 구동 파라미터를 획득하며; 여기서, 얼굴 구동 파라미터는 얼굴 이미지 중의 입모양이 변화되도록 구동하는 데 필요한 파라미터를 나타내는 단계; 얼굴 구동 파라미터를 기초로 기설정된 얼굴 이미지에 대해 이미지 렌더 링을 수행하여, 입모양을 갖는 얼굴 이미지를 생성하는 단계를 포함한다. 구체적으로, 발화 속도 특징과 의미 특징을 기설정된 얼굴 입모양 결정 모델에 입력한다. 얼굴 입모양 결정 모 델을 기초로, 발화 속도 특징과 의미 특징에 대해 스플라이싱 처리를 수행할 수 있으며, 예를 들어, 발화 속도 특징이 나타내는 매트릭스와 의미 특징이 나타내는 매트릭스를 병합할 수 있다. 스플라이싱된 데이터를 인식할 오디오 데이터의 스플라이싱 특징으로 결정한다. 즉, 스플라이싱 특징은 발화 속도 특징과 의미 특징을 나타낼 수 있다. 얼굴 입모양 결정 모델에는 컨벌루션층 등의 네트워크 계층이 설치되어 있으며, 스플라이싱 특징이 얼굴 입모양 결정 모델의 컨벌루션층을 통과할 때, 컨벌루션층을 기초로 스플라이싱 특징에 대해 특징 추출을 수행하여, 얼 굴 구동 파라미터를 계산해낼 수 있다. 얼굴 구동 파라미터는 얼굴 이미지 중의 입모양이 변화되도록 구동할 때 필요한 파라미터이다. 예를 들어, 얼굴 구동 파라미터는 얼굴 이미지에서의, 입모양을 포함하는 타겟 프레임의 위치 정보와 크기 정보 등일 수 있다. 얼굴 구동 파라미터를 획득한 후, 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 기설정된 얼굴 이미지 중의 입모양이 원래의 입을 다문 형상으로부터 얼굴 구동 파라미터에 대응되는 형상으로 변경되도록 하여, 입모양을 갖는 얼굴 이미지를 획득한다. 한 토막의 인식할 오디오 데이터 에 대하여, 다른 입모양을 갖는 복수의 얼굴 이미지를 생성할 수 있다. 이러한 구성은, 발화 속도 특징과 의미 특징을 스플라이싱하고, 얼굴 입모양 결정 모델의 구동 네트워크에 의해, 얼굴 입모양을 구동하는 데 필요한 파라미터를 획득하여, 생성된 얼굴 이미지 중의 입모양이 인식할 오디 오 데이터와 매칭되도록 하고, 발화 속도가 얼굴 이미지 중의 입모양에 주는 영향을 줄여, 얼굴 이미지의 생성 효율과 정밀도를 향상시키는 유리한 효과가 있다. 본 실시예에서, 얼굴 구동 파라미터는 블랜드 쉐입 가중치 파라미터이고; 얼굴 구동 파라미터를 기초로 기설정 된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 생성하는 단계는, 블랜드 쉐입 가중치 파라미터를 기초로, 기설정된 얼굴 이미지에 대응되는 얼굴 3차원 메시 데이터를 결정하며; 여기서, 얼 굴 3차원 메시 데이터는 얼굴 이미지 상의 얼굴 표면을 나타내는 3차원 메시 모델의 데이터인 단계; 얼굴 3차원 메시 데이터를 기초로 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 생성하는 단계를 포함한다. 구체적으로, 얼굴 구동 파라미터는 blend shape(블랜드 쉐입) 가중치일 수 있고, 얼굴 입모양 결정 모델 중의 구동 네트워크에 의해, blend shape 가중치를 획득한다. blend shape 가중치의 파라미터를 기초로, 기설정된 렌 더링 엔진에 기반하여, 기설정된 얼굴 이미지의 기초 상에서, 입모양을 갖는 얼굴 이미지를 획득할 수 있다. 예 를 들어, 기설정된 렌더링 엔진은 Unreal(언리얼) 렌더링 엔진일 수 있다. 이미지 렌더링을 수행할 때, 우선 blend shape 가중치를 기초로, 얼굴 3차원 mesh(메시) 데이터를 결정할 수 있 다. 얼굴 3차원 mesh 데이터는 얼굴 이미지 상의 얼굴 표면의 3차원 메시 모델의 데이터를 나타낼 수 있다. blend shape 가중치와 blend shape 베이스를 기초로, 얼굴 3차원 mesh를 결정할 수 있다. 여기서, blend shape 베이스는 인물 사진 바인딩과 연관되며, 고정 불변 기설정 파라미터이다. 얼굴 3차원 mesh 데이터를 획득한 후, 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 획득한다. 이러한 구성은, 우선 blend shape 가중치를 기초로 얼굴 3차원 mesh를 획득한 후, 얼굴 3차원 mesh를 기초로 얼 굴 이미지를 획득한다. 얼굴 이미지를 정확하게 생성하고, 디지털 휴먼에 대한 사용자의 체험 편의를 도모하는 유리한 효과가 있다. 본 개시의 실시예에서, 인식할 오디오 데이터를 획득하고, 인식할 오디오 데이터로부터 발화 속도 특징과 의미 특징을 결정한다. 발화 속도 특징과 의미 특징을 결합하여, 기설정된 얼굴 이미지에 대해 처리한다. 여기서, 기 설정된 얼굴 이미지는 입모양이 변화될 때 의거하는 초기 이미지로서, 얼굴의 외모를 나타낼 수 있다. 발화 속 도 특징과 의미 특징을 기초로, 다른 입모양을 갖는 얼굴 이미지를 생성하여, 얼굴 이미지의 입모양이 인식할 오디오 데이터와 매칭되도록 한다. 발화 속도가 보다 빠를 때, 얼굴 이미지의 입모양이 말을 삼키는 것과 연독 하는 문제를 해결한다. 얼굴 이미지 중 입모양을 정확하게 구동하고, 얼굴 이미지의 결정 정밀도를 향상시킨다. 도 4는 본 개시의 실시예에서 제공하는 얼굴 입모양 결정 모델의 트레이닝 방법의 흐름도로서, 해당 방법은 얼 굴 입모양 결정 모델의 트레이닝 장치에 의해 수행될 수 있다. 도 4에 도시된 바와 같이, 해당 방법은 아래의 단계들을 포함한다. S401, 트레이닝할 이미지 데이터와 기설정된 얼굴 이미지를 획득하며; 여기서, 트레이닝할 이미지 데이터는 트 레이닝할 오디오 데이터와 트레이닝할 얼굴 이미지를 포함하고, 트레이닝할 얼굴 이미지는 트레이닝할 오디오 데이터에 대응되는 입모양을 갖는다. 예시적으로, 입모양을 갖는 얼굴 이미지를 결정할 때, 딥러닝 기반 얼굴 입모양 결정 모델을 사용할 수 있다. 얼굴 입모양 결정 모델은 상술한 임의의 하나의 실시예에 따른 얼굴 이미지 생성 방법을 구현할 수 있고, 얼굴 입모양 결정 모델은 사전에 트레이닝한 후 사용해야 한다. 사전에 수집한 트레이닝할 이미지 데이터와 기설정된 얼굴 이미지를 획득한다. 트레이닝할 이미지 데이터는 트레이닝할 오디오 데이터와 트레이닝할 얼굴 이미지를 포함할 수 있고, 트레이닝할 오디오 데이터는 모델을 트레이닝하기 위한 오디오 스트림이고, 트레이닝할 얼굴 이미지는 트레이닝할 오디오 데이터와 매칭되는 입모양을 갖는다. 기설정된 얼굴 이미지는 사전에 설계한, 입을 가진 디지털 휴면의 얼굴 이미지이고, 기설정된 얼굴 이미지는 눈, 코 등의 오관이 더 포함될 수 있다. 디지털 휴먼의 얼굴형, 눈, 코와 입 등을 설계하여, 하나의 기설정된 얼굴 이미지를 생성할 수 있다. 디지털 휴먼은 기설정된 얼굴 이미지의 기초 상에서 입모양이 변할 수 있으며, 예를 들어, 기설정된 얼굴 이미지에서, 디지털 휴먼의 입은 다물어진 상태이고, 디지털 휴먼의 입모양은 오디오 데이터의 발송에 따라 변화될 수 있다. 트레이닝할 얼굴 이미지가 기설정된 얼굴 이미지에 비해 차별되는 점은 입모양이 변했다는 것이다. 본 실시예에서, 트레이닝할 이미지 데이터를 획득하는 단계는, 트레이닝할 오디오 데이터를 획득하는 단계; 트 레이닝할 오디오 데이터를 기초로 얼굴 이미지의 3차원 재구성 처리를 수행하여, 트레이닝할 오디오 데이터에 대응되는 얼굴 3차원 메시 데이터를 획득하는 단계; 트레이닝할 오디오 데이터에 대응되는 얼굴 3차원 메시 데 이터를 기초로, 트레이닝할 얼굴 이미지를 획득하는 단계를 포함한다. 구체적으로, 사전에 수집한 트레이닝 세트를 획득하며, 트레이닝 세트는 트레이닝할 오디오 데이터일 수 있다. 트레이닝할 오디오 데이터를 기초로, 트레이닝할 얼굴 이미지를 생성한다. 트레이닝할 얼굴 이미지는 입모양을 가지고, 트레이닝할 얼굴 이미지 중의 입모양은 트레이닝할 오디오 데이터와 매칭된다. 트레이닝할 오디오 데이터를 기초로 얼굴 이미지의 3차원 재구성 처리를 수행할 수 있으며, 예를 들어, 트레이 닝할 오디오 데이터의 각 음소를 기초로, 각 프레임의 얼굴 이미지에 대해 3차원 재구성을 수행할 수 있다. 본 실시예에서, 3차원 재구성의 처리 과정에 대해 구체적으로 한정하지 않는다. 프레임별로 얼굴 3차원 mesh 데이 터를 결정하면, 즉 트레이닝할 오디오 데이터에 대응되는 복수의 프레임의 얼굴 이미지의 얼굴 3차원 mesh를 획 득할 수 있다. 트레이닝할 오디오 데이터에 대응되는 얼굴 3차원 mesh를 기초로, 복수의 프레임의 트레이닝할 얼굴 이미지를 획득한다. 이러한 구성은, 트레이닝할 오디오 데이터에 대응되는 얼굴 이미지를 사전에 결정함으로써, 얼굴 입모양 결정 모델에 대한 트레이닝을 용이하게 하고, 얼굴 입모양 결정 모델의 트레이닝 효율과 정밀도를 향상시키는 유리한 효과가 있다. S402, 트레이닝할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 오디오 특징은 발화 속도 특징과 의미 특 징을 포함한다. 예시적으로, 트레이닝할 오디오 데이터를 획득한 후, 트레이닝할 오디오 데이터에 대해 특징 추출을 수행하여, 트레이닝할 오디오 데이터의 오디오 특징을 획득한다. 오디오 특징은 발화 속도 특징과 의미 특징 등을 포함할 수 있다. 발화 속도 특징은 트레이닝할 오디오 데이터 중, 음소의 변화 속도를 나타낼 수 있는 바, 예를 들어, 발화 속도 특징은 1초 내에 출력되는 음소의 수량으로 나타낼 수 있으며, 즉 트레이닝할 오디오 데이터 중의 음 소 수량과, 트레이닝할 오디오 데이터의 시간을 결정하고, 음소 수량을 트레이닝할 오디오 데이터의 시간으로 나누어, 트레이닝할 오디오 데이터의 발화 속도 크기를 획득하여 발화 속도 특징으로 한다. 본 실시예에서, 음 소 수량을 트레이닝할 오디오 데이터의 시간으로 나누어 트레이닝할 오디오 데이터의 평균적인 발화 속도 특징 을 결정할 수 있고, 트레이닝할 오디오 데이터 중 상이한 음소에 대응되는 발화 속도 특징을 결정할 수도 있다. 의미 특징은 트레이닝할 오디오 데이터가 표현하는 의미를 나타낼 수 있다. 트레이닝할 오디오 데이터는 복수의 음소가 포함될 수 있으며, 트레이닝할 오디오 데이터에 대해, 각 음소의 의미 특징을 결정할 수 있다. 즉, 트레 이닝할 오디오 데이터에 대해 음소 분할을 수행하여, 트레이닝할 오디오 데이터 중의 각 음소를 획득하고, 음소 의 의미를 인식하여, 의미 특징을 결정할 수 있다. 예를 들어, 기설정된 의미 인식 모델을 이용하여 의미 인식 을 수행할 수 있으며, 의미 인식 모델은 뉴럴 네트워크 모델일 수 있다. 음소와 의미 사이의 연관 관계를 미리 설정하고, 기설정된 연관 관계를 기초로, 트레이닝할 오디오 데이터 중 모든 음소의 의미 특징을 찾아내어, 트 레이닝할 오디오 데이터의 의미 특징으로 할 수도 있다. S403, 발화 속도 특징, 의미 특징 및 기설정된 얼굴 이미지를 기초로, 초기의 얼굴 입모양 결정 모델에 대해 트 레이닝하여, 입모양을 갖는 얼굴 이미지를 획득한다. 예시적으로, 트레이닝할 오디오 데이터의 발화 속도 특징과 의미 특징을 트레이닝할 얼굴 입모양 결정 모델에 입력하여 반복 트레이닝을 수행한다. 매번 반복할 때마다, 처리하여 얻은 결과와 기설정된 얼굴 이미지를 기초 로, 입모양을 갖는 얼굴 이미지를 생성한다. 사전에 하나의 트레이닝할 얼굴 입모양 결정 모델을 모델링하고, 발화 속도 특징과 의미 특징을 입력 데이터로 하여, 트레이닝할 얼굴 입모양 결정 모델에 입력하여 처리한다. 얼굴 입모양 결정 모델은 처리한 후, 처리 결과 를 기초로, 기설정된 얼굴 이미지에 대해 입모양 변경을 수행하여, 다른 입모양을 갖는 얼굴 이미지를 획득할 수 있다. 예를 들어, 얼굴 입모양 결정 모델이 발화 속도 특징과 의미 특징을 기초로 결정한 처리 결과는 입모 양의 크기와 형상 정보일 수 있고, 결정된 입모양의 크기와 형상 정보를 기초로, 기설정된 얼굴 이미지에 대해 렌더링하여, 해당 입모양을 포함한 얼굴 이미지를 생성한다. 트레이닝할 오디오 데이터는 복수의 음소를 포함하 며, 각 음소에 대응되는 입모양을 갖는 얼굴 이미지를 생성할 수 있다. S404, 입모양을 갖는 얼굴 이미지와 트레이닝할 얼굴 이미지가 일치하면, 트레이닝 완성된 얼굴 입모양 결정 모 델이 획득된 것으로 결정한다. 예시적으로, 모델에서 출력되는 입모양을 갖는 얼굴 이미지를 획득한 후, 음소에 대응되는 입모양을 갖는 얼굴 이미지와, 해당 음소에 대응되는 트레이닝할 얼굴 이미지를 비교하여, 양자가 일치하면, 얼굴 입모양 결정 모델 이 트레이닝 완성된 것으로 결정하고; 양자가 일치하지 않으면, 얼굴 입모양 결정 모델을 더 트레이닝해야 할 필요가 있다고 결정하고, 계속하여 얼굴 입모양 결정 모델에 트레이닝할 오디오 데이터의 의미 특징과 발화 속 도 특징을 입력하여, 출력된 입모양을 갖는 얼굴 이미지가 대응되는 트레이닝할 얼굴 이미지와 일치할 때까지 기설정된 역전파 알고리즘을 기반으로 트레이닝한다. 하나의 유사도 임계값을 미리 설정할 수도 있으며, 유사도 임계값은 얼굴 입모양 결정 모델의 트레이닝 완성 여 부를 판단하는 데 사용될 수 있다. 입모양을 갖는 얼굴 이미지를 획득한 후, 입모양을 갖는 얼굴 이미지와 이에 대응되는 트레이닝할 얼굴 이미지 사이의 유사도를 결정한다. 결정된 유사도가 기설정된 유사도 임계값보다 같 거나 클 경우, 얼굴 입모양 결정 모델이 트레이닝 완성된 것으로 결정되고; 유사도가 기설정된 유사도 임계값보 다 작을 경우, 얼굴 입모양 결정 모델이 트레이닝 완성되지 않은 것으로 결정한다. 본 개시의 실시예에서, 트레이닝할 오디오 데이터와 트레이닝할 얼굴 이미지를 획득하고, 트레이닝할 오디오 데 이터로부터 발화 속도 특징과 의미 특징을 결정한다. 발화 속도 특징과 의미 특징을 결합하여, 트레이닝할 얼굴 입모양 결정 모델에 대해 트레이닝한다. 발화 속도 특징과 의미 특징을 기초로, 다른 입모양을 갖는 얼굴 이미지를 생성하고, 트레이닝을 통해 출력된 얼굴 이미지 중의 입모양이 트레이닝할 오디오 데이터와 매칭되도록 한 다. 상이한 발화 속도가 입모양에 주는 영향을 모델이 학습하도록 하여, 발화 속도가 변화될 때 오디오로 입모 양을 구동하는 정밀도와 진실성을 대폭 향상시켜, 추후에 얼굴 입모양 결정 모델을 사용할 때, 얼굴 이미지의 결정 정밀도를 향상시키는데 편리하다. 도 5는 본 개시의 실시예에서 제공하는 얼굴 입모양 결정 모델의 트레이닝 방법의 흐름도로서, 해당 실시예는 상술한 실시예를 기초로 하는 선택적인 실시예이다. 본 실시예에서, 트레이닝할 오디오 데이터의 오디오 특징을 결정하는 것은, 기설정된 제1 특징 추출 모델을 기 초로, 트레이닝할 오디오 데이터의 발화 속도 특징을 결정하며; 여기서, 제1 특징 추출 모델은 트레이닝할 오디 오 데이터로부터 발화 속도 특징을 추출하기 위한 것인 단계; 기설정된 제2 특징 추출 모델을 기초로, 트레이닝 할 오디오 데이터의 의미 특징을 결정하며; 여기서, 제2 특징 추출 모델은 트레이닝할 오디오 데이터로부터 의 미 특징을 추출하기 위한 것인 단계로 세분화할 수 있다. 도 5에 도시된 바와 같이, 해당 방법은 아래의 단계들을 포함한다. S501, 트레이닝할 이미지 데이터와 기설정된 얼굴 이미지를 획득하며; 여기서, 트레이닝할 이미지 데이터는 트 레이닝할 오디오 데이터와 트레이닝할 얼굴 이미지를 포함하고, 트레이닝할 얼굴 이미지는 트레이닝할 오디오 데이터에 대응되는 입모양을 갖는다. 예시적으로, 본 단계는 상술한 단계(S401)를 참조할 수 있으며, 반복되는 설명을 생략한다. S502, 기설정된 제1 특징 추출 모델을 기초로, 트레이닝할 오디오 데이터의 발화 속도 특징을 결정하며; 여기서, 제1 특징 추출 모델은 트레이닝할 오디오 데이터로부터 발화 속도 특징을 추출하기 위한 것이다. 예시적으로, 하나의 제1 특징 추출 모델을 사전에 구성하며, 제1 특징 추출 모델은 사전에 결정된 뉴럴 네트워 크 모델로서, 트레이닝할 오디오 데이터로부터 발화 속도 특징을 추출하기 위한 것일 수 있다. 트레이닝할 오디 오 데이터를 제1 특징 추출 모델에 입력하고 처리하여, 트레이닝할 오디오 데이터의 발화 속도 특징을 획득한다. 예를 들어, 제1 특징 추출 모델은 컨벌루션층, 풀링층 등의 네트워크 계층이 포함될 수 있고, 트레이 닝할 오디오 데이터에 대해 컨벌루션 처리와 특징 추출을 수행하여, 트레이닝할 오디오 데이터의 발화 속도 특 징을 획득할 수 있다. 본 실시예에서, 제1 특징 추출 모델의 네트워크 구조에 대해 구체적으로 한정하지 않는다. 본 실시예에서, 기설정된 제1 특징 추출 모델을 기초로, 트레이닝할 오디오 데이터의 발화 속도 특징을 결정하 는 단계는, 트레이닝할 오디오 데이터를 기설정된 제1 특징 추출 모델에 입력하여 특징 추출을 수행하여, 트레 이닝할 오디오 데이터의 음성 사후 확률 특징을 획득하며; 여기서, 음성 사후 확률 특징은 트레이닝할 오디오 데이터의 음소 카테고리의 정보를 나타내는 단계; 트레이닝할 오디오 데이터의 음성 사후 확률 특징을 기초로, 트레이닝할 오디오 데이터의 발화 속도 특징을 결정하는 단계를 포함한다. 구체적으로, 제1 특징 추출 모델은 ASR 모델일 수 있고, ASR 모델은 복수층의 네트워크 계층을 포함할 수 있는 바, 예를 들어, 컨벌루션층, 풀링층과 전체 연결층을 포함할 수 있다. 트레이닝할 오디오 데이터를 기설정된 ASR 모델에 입력하여 특징을 추출하며, 예를 들어, 컨벌루션층에 의해 특징을 추출하여, 트레이닝할 오디오 데 이터의 PPG 특징을 획득할 수 있다. PPG 특징은 하나의 카테고리에 대한 시간 매트릭스로서, 하나의 발화의 각 특정 시간 프레임에 대해, 각 음성 카테고리의 사후 확률을 나타낼 수 있다. PPG 특징은 2차원 좌표축의 이미지 로 나타낼 수 있으며, 트레이닝할 오디오 데이터의 음소 카테고리의 정보를 나타내고, 횡좌표는 시간을 나타내 고, 종좌표는 음소 카테고리를 나타낸다. PPG 특징을 획득한 후, 기설정된 발화 속도 결정 알고리즘을 기초로, PPG 특징에 대해 연산하여, PPG 특징을 트 레이닝할 오디오 데이터의 발화 속도 특징으로 변환할 수 있다. 음소의 변화 속도를 산출하여, 발화 속도의 크 기로 할 수 있으며, 발화 속도 특징에 대한 명시적 모델링(Explicit Modeling)을 구현한다. 본 실시예에서, 기 설정된 발화 속도 결정 알고리즘에 대해 구체적으로 한정하지 않는다. 이러한 구성은, 트레이닝할 오디오 데이터를 자동 음성 인식 모델에 입력하고 처리하여, 트레이닝할 오디오 데 이터의 PPG 특징을 획득하고, PPG 특징에 대해 추가적인 연산을 수행하여, 발화 속도 특징을 획득한다. 발화 속 도에 대한 명시적 모델링을 구현함으로써, 발화 속도 특징을 도입하여, 발화 속도가 변화될 때 오디오로 입모양 을 구동하는 정밀도와 진실성을 대폭 향상시키는 유리한 효과가 있다. 본 실시예에서, 트레이닝할 오디오 데이터의 음성 사후 확률 특징을 기초로, 트레이닝할 오디오 데이터의 발화 속도 특징을 결정하는 단계는, 음성 사후 확률 특징에 대해 고속 푸리에 변환 처리를 수행하여, 주파수 영역 신 호 특징을 획득하며; 여기서, 주파수 영역 신호 특징은 트레이닝할 오디오 데이터의 음소 카테고리의 정보를 나 타내는 단계; 기설정된 주파수 대역 크기를 기초로, 주파수 영역 신호 특징을 적어도 두 개의 주파수 대역의 주 파수 영역 신호 특징으로 분할하는 단계; 적어도 두 개의 주파수 대역의 주파수 영역 신호 특징에 대해 적분 처 리를 수행하여, 트레이닝할 오디오 데이터의 발화 속도 특징을 획득하는 단계를 포함한다. 구체적으로, PPG 특징은 시간 영역 신호이고, 트레이닝할 오디오 데이터의 PPG 특징을 획득한 후, PPG 특징에 대해 고속 푸리에 변환 처리를 수행할 수 있다. 즉, FFT에 의해 PPG 특징을 주파수 영역으로 변환하여, PPG 특 징에 대응되는 주파수 영역 신호 특징을 획득한다. 해당 주파수 영역 신호 특징은 트레이닝할 오디오 데이터의 음소 카테고리의 정보로 표시될 수도 있다. 주파수 영역 신호 특징에 대해 주파수 대역별로 적분하여, 원하는 주파수를 산출하여, 발화 속도 크기로 하면, 트레이닝할 오디오 데이터의 발화 속도 특징이 획득된다. 발화 속도 특징을 계산할 때, 주파수 대역 크기를 미 리 설정하고, 기설정된 주파수 대역 크기를 기초로, 주파수 영역 신호 특징에 대해 분할하여, 복수의 주파수 대 역 크기의 주파수 영역 신호 특징을 획득할 수 있다. 각각의 주파수 대역 크기의 주파수 영역 신호 특징에 대해 하나씩 적분 처리하며, 적분 결과는 트레이닝할 오디오 데이터 중 음소 변화 속도를 반영할 수 있으며, 즉 발화 속도 특징이 된다. 이러한 구성은, FFT 처리와 적분 계산을 수행하여, PPG 특징을 구체적인 발화 속도 크기로 변환하여, 발화 속도 특징을 결정하는 것을 구현할 수 있으며, 이에 따라 얼굴 입모양 결정 모델의 트레이닝 정밀도가 향상되는 유리 한 효과가 있다. S503, 기설정된 제2 특징 추출 모델을 기초로, 트레이닝할 오디오 데이터의 의미 특징을 결정하며; 여기서, 제2 특징 추출 모델은 트레이닝할 오디오 데이터로부터 의미 특징을 추출하기 위한 것이다. 예시적으로, 제2 특징 추출 모델은 사전에 트레이닝된 뉴럴 네트워크 모델일 수도 있으며, 예를 들어, 제2 특징 추출 모델은 기설정된 의미 인식 모델이다. 제2 특징 추출 모델은 특징 추출 네트워크를 포함하고, 제2 특징 추 출 모델 중의 특징 추출 네트워크를 기초로, 트레이닝할 오디오 데이터에 대해 의미 특징 추출을 수행하여, 트 레이닝할 오디오 데이터의 의미 특징을 획득할 수 있다. 제1 특징 추출 모델과 제2 특징 추출 모델에 의해, 발화 속도 특징과 의미 특징을 빠르게 얻을 수 있으며, 발화 속도 특징과 의미 특징을 각각 추출하여, 특징 추출의 효율을 향상시키고, 나아가 얼굴 입모양 결정 모델의 트 레이닝 효율을 향상시킨다. 본 실시예에서, 기설정된 제2 특징 추출 모델을 기초로, 트레이닝할 오디오 데이터의 의미 특징을 결정하는 단 계는, 트레이닝할 오디오 데이터를 기설정된 제2 특징 추출 모델에 입력하여 특징 추출을 수행하여, 트레이닝할 오디오 데이터의 의미 특징을 출력받는 단계를 포함한다. 구체적으로, 제2 특징 추출 모델은 의미 인식 모델일 수 있고, 의미 인식 모델은 복수층의 컨벌루션층 등의 네 트워크 계층이 포함될 수 있으며, 특징 추출 네트워크를 구성한다. 트레이닝할 오디오 데이터를 기설정된 의미 인식 모델에 입력하여 처리하며, 예를 들어, 컨벌루션층에 의해 특징을 추출하여, 트레이닝할 오디오 데이터의 의미 특징을 획득할 수 있다. 트레이닝할 오디오 데이터는 스트리밍 데이터로서, 추출해 낸 의미 특징은 스트리 밍 특징일 수 있다. 본 실시예에서, 의미 인식 모델의 모델 구조에 대해 구체적으로 한정하지 않는다. 이러한 구성은, 입력받은 오디오 스트림 데이터에 대해 의미 특징을 자동으로 추출하여, 의미 특징의 결정 효율 과 정밀도를 향상시키고, 나아가 얼굴 입모양 결정 모델의 트레이닝 효율과 정밀도를 향상시키는 유리한 효과가 있다. S504, 발화 속도 특징, 의미 특징 및 기설정된 얼굴 이미지를 기초로, 초기의 얼굴 입모양 결정 모델에 대해 트 레이닝하여, 입모양을 갖는 얼굴 이미지를 획득한다. 예시적으로, 발화 속도 특징과 의미 특징을 트레이닝할 얼굴 입모양 결정 모델에 입력하여 트레이닝한다. 트레 이닝할 얼굴 입모양 결정 모델은 의미 특징과 발화 속도 특징에 대해 처리하고, 처리하여 얻은 결과와 기설정된 얼굴 이미지를 기초로, 입모양을 갖는 얼굴 이미지를 생성한다. 본 실시예에서, 발화 속도 특징, 의미 특징 및 기설정된 얼굴 이미지를 기초로, 초기의 얼굴 입모양 결정 모델 에 대해 트레이닝하여, 입모양을 갖는 얼굴 이미지를 획득하는 단계는, 초기의 얼굴 입모양 결정 모델을 기반으 로 발화 속도 특징과 의미 특징에 대해 스플라이싱 처리를 수행하여, 트레이닝할 오디오 데이터의 스플라이싱특징을 획득하며; 여기서, 스플라이싱 특징은 발화 속도 특징과 의미 특징을 나타내는 단계; 초기의 얼굴 입모 양 결정 모델 중의 컨벌루션층에 의해, 스플라이싱 특징에 대해 특징 추출을 수행하여, 얼굴 구동 파라미터를 획득하며; 여기서, 얼굴 구동 파라미터는 얼굴 이미지 중의 입모양이 변화되도록 구동하는 데 필요한 파라미터 를 나타내는 단계; 얼굴 구동 파라미터를 기초로 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모 양을 갖는 얼굴 이미지를 획득하는 단계를 포함한다. 구체적으로, 발화 속도 특징과 의미 특징을 트레이닝할 얼굴 입모양 결정 모델에 입력한다. 얼굴 입모양 결정 모델을 기초로, 발화 속도 특징과 의미 특징에 대해 스플라이싱 처리를 수행할 수 있으며, 예를 들어, 발화 속 도 특징이 나타나는 매트릭스와 의미 특징이 나타내는 매트릭스를 병합할 수 있다. 스플라이싱된 데이터를 트레 이닝할 오디오 데이터의 스플라이싱 특징으로 결정한다. 즉, 스플라이싱 특징은 발화 속도 특징과 의미 특징을 나타낼 수 있다. 얼굴 입모양 결정 모델에는 컨벌루션층 등의 네트워크 계층이 설치되어 있으며, 스플라이싱 특징이 얼굴 입모양 결정 모델의 컨벌루션층을 통과할 때, 컨벌루션층을 기초로 스플라이싱 특징에 대해 특징 추출을 수행하여, 얼 굴 구동 파라미터를 계산해낼 수 있다. 얼굴 구동 파라미터는 얼굴 이미지 중의 입모양이 변화되도록 구동할 때 필요한 파라미터이다. 예를 들어, 얼굴 구동 파라미터는 얼굴 이미지에서의, 입모양을 포함한 타겟 프레임의 위 치 정보와 크기 정보 등일 수 있다. 얼굴 구동 파라미터를 획득한 후, 기설정된 얼굴 이미지에 대해 이미지 렌 더링을 수행하여, 기설정된 얼굴 이미지 중의 입모양이 원래의 입을 다문 형상으로부터 얼굴 구동 파라미터에 대응되는 형상으로 변경되도록 하여, 입모양을 갖는 얼굴 이미지를 획득한다. 이러한 구성은, 발화 속도 특징과 의미 특징을 스플라이싱하고, 얼굴 입모양 결정 모델의 구동 네트워크에 의해, 얼굴 입모양을 구동하는 데 필요한 파라미터를 획득하고, 트레이닝을 거쳐, 생성된 얼굴 이미지 중의 입 모양이 트레이닝할 오디오 데이터와 매칭되도록 하고, 발화 속도가 얼굴 이미지 중의 입모양에 주는 영향을 줄 여, 얼굴 입모양 결정 모델의 트레이닝 정밀도를 향상시키는 유리한 효과가 있다. 본 실시예에서, 얼굴 구동 파라미터는 블랜드 쉐입 가중치 파라미터이고; 얼굴 구동 파라미터를 기초로 기설정 된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 획득하는 단계는, 블랜드 쉐입 가중치 파라미터를 기초로, 기설정된 얼굴 이미지에 대응되는 얼굴 3차원 메시 데이터를 결정하며; 여기서, 얼 굴 3차원 메시 데이터는 얼굴 이미지 상의 얼굴 표면을 나타내는 3차원 메시 모델의 데이터인 단계; 얼굴 3차원 메시 데이터를 기초로 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 생성하는 단계를 포함한다. 구체적으로, 얼굴 구동 파라미터는 blend shape(블랜드 쉐입) 가중치일 수 있고, 얼굴 입모양 결정 모델 중의 구동 네트워크에 의해, blend shape 가중치를 획득한다. blend shape 가중치의 파라미터를 기초로, 기설정된 렌 더링 엔진에 기반하여, 기설정된 얼굴 이미지의 기초 상에서, 입모양을 갖는 얼굴 이미지를 획득할 수 있다. 예 를 들어, 기설정된 렌더링 엔진은 Unreal(언리얼) 렌더링 엔진일 수 있다. 이미지 렌더링을 수행할 때, 우선 blend shape 가중치를 기초로, 얼굴 3차원 mesh(메시) 데이터를 결정할 수 있 다. 얼굴 3차원 mesh 데이터는 얼굴 이미지 상의 얼굴 표면의 3차원 메시 모델의 데이터를 나타낼 수 있다. blend shape 가중치와 blend shape 베이스를 기초로, 얼굴 3차원 mesh를 결정할 수 있다. 여기서, blend shape 베이스는 인물 사진 바인딩과 연관되며, 고정 불변 기설정 파라미터이다. 얼굴 3차원 mesh 데이터를 획득한 후, 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 획득한다. 이러한 구성은, 우선 blend shape 가중치를 기초로 얼굴 3차원 mesh를 획득한 후, 얼굴 3차원 mesh를 기초로 얼 굴 이미지를 획득하여, 얼굴 이미지를 정확하게 생성하여, 얼굴 입모양 결정 모델의 트레이닝 정밀도를 향상시 키는 유리한 효과가 있다. S505, 입모양을 갖는 얼굴 이미지와 트레이닝할 얼굴 이미지가 일치하면, 트레이닝 완성된 얼굴 입모양 결정 모 델이 획득된 것으로 결정한다. 예시적으로, 본 단계는 상술한 단계(S404)를 참조할 수 있으며, 반복되는 설명을 생략한다. 본 개시의 실시예에서, 트레이닝할 오디오 데이터와 트레이닝할 얼굴 이미지를 획득하고, 트레이닝할 오디오 데 이터로부터 발화 속도 특징과 의미 특징을 결정한다. 발화 속도 특징과 의미 특징을 결합하여, 트레이닝할 얼굴 입모양 결정 모델에 대해 트레이닝한다. 발화 속도 특징과 의미 특징을 기초로, 다른 입모양을 갖는 얼굴 이미 지를 생성하고, 트레이닝을 통해 출력된 얼굴 이미지 중의 입모양이 트레이닝할 오디오 데이터와 매칭되도록 한 다. 상이한 발화 속도가 입모양에 주는 영향을 모델이 학습하도록 하여, 발화 속도가 변화될 때 오디오로 입모양을 구동하는 정밀도와 진실성을 대폭 향상시켜, 추후에 얼굴 입모양 결정 모델을 사용할 때, 얼굴 이미지의 결정 정밀도를 향상시키는데 편리하다. 도 6은 본 개시의 실시예에서 제공하는 입모양을 기반으로 하는 얼굴 이미지 생성 장치의 구조 블록도이다. 설 명의 편리를 위해, 본 개시의 실시예와 관련되는 부분만 도시하였다. 도 6을 참조하면, 입모양을 기반으로 하는 얼굴 이미지 생성 장치는 데이터 획득 유닛, 특징 결정 유닛과 이미지 생성 유닛을 포함한 다. 데이터 획득 유닛은 인식할 오디오 데이터와 기설정된 얼굴 이미지를 획득하고; 특징 결정 유닛은 상기 인식할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 상기 오디오 특징은 발 화 속도 특징과 의미 특징을 포함하고; 이미지 생성 유닛은 상기 발화 속도 특징과 상기 의미 특징을 기초로 상기 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를 생성한다. 도 7은 본 개시의 실시예에서 제공하는 입모양을 기반으로 하는 얼굴 이미지 생성 장치의 구조 블록도로서, 도 7에 도시된 바와 같이, 입모양을 기반으로 하는 얼굴 이미지 생성 장치는 데이터 획득 유닛, 특징 결 정 유닛과 이미지 생성 유닛을 포함하되, 여기서, 특징 결정 유닛은 제1 결정 모듈과 제2 결정 모듈을 포함한다. 제1 결정 모듈은 기설정된 제1 특징 추출 모델을 기초로, 인식할 오디오 데이터의 발화 속도 특징을 결정 하며; 여기서, 상기 제1 특징 추출 모델은 인식할 오디오 데이터로부터 발화 속도 특징을 추출하기 위한 것이고; 제2 결정 모듈은 기설정된 제2 특징 추출 모델을 기초로, 상기 인식할 오디오 데이터의 의미 특징을 결정 하며; 여기서, 상기 제2 특징 추출 모델은 인식할 오디오 데이터로부터 의미 특징을 추출하기 위한 것이다. 하나의 예시에서, 제1 결정 모듈은, 상기 인식할 오디오 데이터를 기설정된 제1 특징 추출 모델에 입력하여 특징 추출을 수행하여, 상기 인식할 오 디오 데이터의 음성 사후 확률 특징을 획득하며; 여기서, 상기 음성 사후 확률 특징은 인식할 오디오 데이터의 음소 카테고리의 정보를 나타내는 특징 추출 서브 모듈; 상기 인식할 오디오 데이터의 음성 사후 확률 특징을 기초로, 상기 인식할 오디오 데이터의 발화 속도 특징을 결정하는 특징 결정 서브 모듈을 포함한다. 하나의 예시에서, 특징 결정 서브 모듈은 구체적으로, 상기 음성 사후 확률 특징에 대해 고속 푸리에 변환 처리를 수행하여, 주파수 영역 신호 특징을 획득하며; 여기 서, 상기 주파수 영역 신호 특징은 인식할 오디오 데이터의 음소 카테고리의 정보를 나타내고; 기설정된 주파수 대역 크기를 기초로, 상기 주파수 영역 신호 특징을 적어도 두 개의 주파수 대역의 주파수 영 역 신호 특징으로 분할하고; 상기 적어도 두 개의 주파수 대역의 주파수 영역 신호 특징에 대해 적분 처리를 수행하여, 상기 인식할 오디오 데이터의 발화 속도 특징을 획득한다. 하나의 예시에서, 제2 결정 모듈은 구체적으로, 상기 인식할 오디오 데이터를 기설정된 제2 특징 추출 모델에 입력하여 특징 추출을 수행하여, 상기 인식할 오 디오 데이터의 의미 특징을 출력받는다. 하나의 예시에서, 이미지 생성 유닛은, 상기 발화 속도 특징과 상기 의미 특징을 기설정된 얼굴 입모양 결정 모델에 입력하여 처리하고, 처리하여 얻은 결과와 상기 기설정된 얼굴 이미지를 기초로, 입모양을 갖는 얼굴 이미지를 생성하는 이미지 생성 모듈을 포함 한다. 하나의 예시에서, 이미지 생성 모듈은, 상기 기설정된 얼굴 입모양 결정 모델을 기반으로 상기 발화 속도 특징과 상기 의미 특징에 대해 스플라이싱 처 리를 수행하여, 상기 인식할 오디오 데이터의 스플라이싱 특징을 획득하며; 여기서, 상기 스플라이싱 특징은 발 화 속도 특징과 의미 특징을 나타내는 특징 스플라이싱 서브 모듈; 상기 기설정된 얼굴 입모양 결정 모델 중의 컨벌루션층을 기초로, 상기 스플라이싱 특징에 대해 특징 추출을 수 행하여, 얼굴 구동 파라미터를 획득하며; 여기서, 상기 얼굴 구동 파라미터는 얼굴 이미지 중의 입모양이 변화 되도록 구동하는 데 필요한 파라미터를 나타내는 파라미터 결정 서브 모듈; 상기 얼굴 구동 파라미터를 기초로 상기 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖 는 얼굴 이미지를 생성하는 이미지 렌더링 서브 모듈을 포함한다. 하나의 예시에서, 얼굴 구동 파라미터는 블랜드 쉐입 가중치 파라미터이고; 이미지 렌더링 서브 모듈은 구체적 으로, 상기 블랜드 쉐입 가중치 파라미터를 기초로, 상기 기설정된 얼굴 이미지에 대응되는 얼굴 3차원 메시 데이터를 결정하며; 여기서, 상기 얼굴 3차원 메시 데이터는 얼굴 이미지 상의 얼굴 표면을 나타내는 3차원 메시 모델의 데이터이고; 상기 얼굴 3차원 메시 데이터를 기초로 상기 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 생성한다. 하나의 예시에서, 상기 인식할 오디오 데이터의 발화 속도 특징이 나타내는 수치가 기설정된 발화 속도 임계값보다 작은 것으로 결정되면, 상기 의미 특징을 기초로 상기 기설정된 얼굴 이미지에 대해 처리하여, 입모양을 갖는 얼굴 이미지를 생성하는 의미 처리 유닛을 더 포함한다. 도 8은 본 개시의 실시예에서 제공하는 얼굴 입모양 결정 모델의 트레이닝 장치의 구조 블록도이다. 설명의 편 의를 위해, 본 개시의 실시예와 관련되는 부분만 도시하였다. 도 8을 참조하면, 얼굴 입모양 결정 모델의 트레 이닝 장치는 이미지 획득 유닛, 특징 추출 유닛, 모델 트레이닝 유닛과 모델 획득 유닛 을 포함한다. 이미지 획득 유닛은 트레이닝할 이미지 데이터와 기설정된 얼굴 이미지를 획득하며; 여기서, 상기 트레이 닝할 이미지 데이터는 트레이닝할 오디오 데이터와 트레이닝할 얼굴 이미지를 포함하고, 상기 트레이닝할 얼굴 이미지는 트레이닝할 오디오 데이터에 대응되는 입모양을 갖고; 특징 추출 유닛은 상기 트레이닝할 오디오 데이터의 오디오 특징을 결정하며; 여기서, 상기 오디오 특징은 발화 속도 특징과 의미 특징을 포함하고; 모델 트레이닝 유닛은 상기 발화 속도 특징, 상기 의미 특징 및 상기 기설정된 얼굴 이미지를 기초로, 초 기의 얼굴 입모양 결정 모델에 대해 트레이닝하여, 입모양을 갖는 얼굴 이미지를 획득하고; 모델 획득 유닛은 입모양을 갖는 얼굴 이미지와 상기 트레이닝할 얼굴 이미지가 일치하면, 트레이닝 완성 된 얼굴 입모양 결정 모델이 획득된 것으로 결정한다. 하나의 예시에서, 특징 추출 유닛은, 기설정된 제1 특징 추출 모델을 기초로, 상기 트레이닝할 오디오 데이터의 발화 속도 특징을 결정하며; 여기서, 상기 제1 특징 추출 모델은 트레이닝할 오디오 데이터로부터 발화 속도 특징을 추출하기 위한 것인 제1 추출 모 듈; 기설정된 제2 특징 추출 모델을 기초로, 상기 트레이닝할 오디오 데이터의 의미 특징을 결정하며; 여기서, 상기 제2 특징 추출 모델은 트레이닝할 오디오 데이터로부터 의미 특징을 추출하기 위한 것인 제2 추출 모듈을 포함 한다. 하나의 예시에서, 제1 추출 모듈은, 상기 트레이닝할 오디오 데이터를 기설정된 제1 특징 추출 모델에 입력하여 특징 추출을 수행하여, 상기 트레이 닝할 오디오 데이터의 음성 사후 확률 특징을 획득하며; 여기서, 상기 음성 사후 확률 특징은 트레이닝할 오디 오 데이터의 음소 카테고리의 정보를 나타내는 확률 결정 서브 모듈; 상기 트레이닝할 오디오 데이터의 음성 사후 확률 특징을 기초로, 상기 트레이닝할 오디오 데이터의 발화 속도 특징을 결정하는 발화 속도 결정 서브 모듈을 포함한다. 하나의 예시에서, 발화 속도 결정 서브 모듈은 구체적으로, 상기 음성 사후 확률 특징에 대해 고속 푸리에 변환 처리를 수행하여, 주파수 영역 신호 특징을 획득하며; 여기 서, 상기 주파수 영역 신호 특징은 트레이닝할 오디오 데이터의 음소 카테고리의 정보를 나타내고; 기설정된 주파수 대역 크기를 기초로, 상기 주파수 영역 신호 특징을 적어도 두 개의 주파수 대역의 주파수 영 역 신호 특징으로 분할하고; 상기 적어도 두 개의 주파수 대역의 주파수 영역 신호 특징에 대해 적분 처리를 수행하여, 상기 트레이닝할 오 디오 데이터의 발화 속도 특징을 획득한다. 하나의 예시에서, 제2 추출 모듈은 구체적으로, 상기 트레이닝할 오디오 데이터를 기설정된 제2 특징 추출 모델에 입력하여 특징 추출을 수행하여, 상기 트레이 닝할 오디오 데이터의 의미 특징을 출력받는다. 하나의 예시에서, 모델 트레이닝 유닛은, 상기 초기의 얼굴 입모양 결정 모델을 기반으로 상기 발화 속도 특징과 상기 의미 특징에 대해 스플라이싱 처리 를 수행하여, 상기 트레이닝할 오디오 데이터의 스플라이싱 특징을 획득하며; 여기서, 상기 스플라이싱 특징은 발화 속도 특징과 의미 특징을 나타내는 특징 스플라이싱 모듈; 상기 초기의 얼굴 입모양 결정 모델 중의 컨벌루션층을 기초로, 상기 스플라이싱 특징에 대해 특징 추출을 수행 하여, 얼굴 구동 파라미터를 획득하며; 여기서, 상기 얼굴 구동 파라미터는 얼굴 이미지 중의 입모양이 변화되 도록 구동하는 데 필요한 파라미터를 나타내는 파라미터 결정 모듈; 상기 얼굴 구동 파라미터를 기초로 상기 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖 는 얼굴 이미지를 획득하는 이미지 렌더링 모듈을 포함한다. 하나의 예시에서, 얼굴 구동 파라미터는 블랜드 쉐입 가중치 파라미터이고; 이미지 렌더링 모듈은, 상기 블랜드 쉐입 가중치 파라미터를 기초로, 상기 기설정된 얼굴 이미지에 대응되는 얼굴 3차원 메시 데이터를 결정하며; 여기서, 상기 얼굴 3차원 메시 데이터는 얼굴 이미지 상의 얼굴 표면을 나타내는 3차원 메시 모델의 데이터인 데이터 결정 서브 모듈; 상기 얼굴 3차원 메시 데이터를 기초로 상기 기설정된 얼굴 이미지에 대해 이미지 렌더링을 수행하여, 입모양을 갖는 얼굴 이미지를 생성하는 이미지 렌더링 서브 모듈을 포함한다. 하나의 예시에서, 이미지 획득 유닛은, 상기 트레이닝할 오디오 데이터를 획득하는 데이터 획득 모듈; 상기 트레이닝할 오디오 데이터를 기초로 얼굴 이미지의 3차원 재구성 처리를 수행하여, 상기 트레이닝할 오디 오 데이터에 대응되는 얼굴 3차원 메시 데이터를 획득하는 3차원 재구성 모듈; 상기 트레이닝할 오디오 데이터에 대응되는 얼굴 3차원 메시 데이터를 기초로, 상기 트레이닝할 얼굴 이미지를 획득하는 이미지 획득 모듈을 포함한다. 도 9는 본 개시의 실시예에서 제공하는 전자기기의 구조 블록도로서, 도 9에 도시된 바와 같이, 전자기기 는 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하 되; 여기서, 메모리는 상기 적어도 하나의 프로세서에 의해 실행될 수 있는 명령이 저장되어 있고, 상기 명령은 상기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서가 본 개시의 입 모양을 기반으로 하는 얼굴 이미지 생성 방법과 모델의 트레이닝 방법을 수행할 수 있도록 한다. 전자기기는 수신기와 송신기를 더 포함한다. 수신기는 기타 기기에서 송신되는 명령과 데 이터를 수신하기 위한 것이고, 송신기는 외부 기기에 명령과 데이터를 송신하기 위한 것이다. 본 개시의 실시예에 따르면, 본 개시는 전자기기, 판독 가능 저장매체와 컴퓨터 프로그램 제품을 더 제공한다. 본 개시의 실시예에 따르면, 본 개시는 컴퓨터 프로그램 제품을 더 제공하며, 컴퓨터 프로그램 제품은 컴퓨터 프로그램을 포함하고, 컴퓨터 프로그램은 판독 가능 저장매체에 저장되고, 전자기기의 적어도 하나의 프로세서는 판독 가능 저장매체로부터 컴퓨터 프로그램을 판독할 수 있고, 적어도 하나의 프로세서는 컴퓨터 프로그램을 실행하여 전자기기가 상술한 어느 하나의 실시예에서 제공하는 방안을 수행하도록 한다. 도 10은 본 개시의 실시예를 실시할 수 있는 예시적 전자기기의 예시적 블록도를 도시한다. 전자기기는 다양한 형태의 디지털 컴퓨터를 나타내려는 것으로서, 랩톱 컴퓨터, 데스크톱 컴퓨터, 워크 테이블, 개인 휴대 정보 단말기, 서버, 블레이드 서버, 대형 컴퓨터와 기타 적합한 컴퓨터를 들 수 있다. 전자기기는 다양한 형태 의 이동 장치를 나타낼 수도 있는 바, 개인 휴대 정보 단말기, 셀룰러폰, 스마트폰, 웨어러블 디바이스와 기타 유사한 컴퓨팅 장치를 들 수 있다. 본문에 나타나는 부재, 이들의 연결과 관계, 및 이들의 기능은 단지 예시적 인 것일 뿐, 본문에 기재된 것 및/또는 요구하는 본 개시의 구현을 제한하려는 의도는 아니다. 도 10에 도시된 바와 같이, 기기는 컴퓨팅 유닛을 포함하고, 이는 판독 전용 메모리(Read Only Memory, ROM, 1002)에 저장되어 있는 컴퓨터 프로그램 또는 저장유닛으로부터 랜덤 액세스 메모리(Random Access Memory, RAM, 1003)에 로딩된 컴퓨터 프로그램을 기초로, 다양한 적당한 동작과 처리를 수행할 수 있다. RAM에서, 기기 조작에 수요되는 다양한 프로그램과 데이터를 저장할 수도 있다. 컴퓨팅 유닛 , ROM 및 RAM은 버스에 의해 서로 연결된다. 입력/출력(Input/Output, I/O) 인터페이 스도 버스에 연결된다. 기기 중의 복수의 부재는 I/O 인터페이스에 연결되며, 예를 들어 키보드, 마우스 등과 같은 입력유 닛; 예를 들어 다양한 유형의 디스플레이, 스피커 등과 같은 출력유닛; 예를 들어 자기 디스크, 시 디롬 등과 같은 저장유닛; 및 예를 들어 랜 카드, 모뎀, 무선 통신 송수신기 등과 같은 통신유닛을 포함한다. 통신유닛은 기기가 인터넷과 같은 컴퓨터 네트워크 및/또는 다양한 전기 통신 네트워크 에 의해 기타 기기와 정보/데이터를 교환하는 것을 허용한다. 컴퓨팅 유닛은 처리와 연산 능력을 구비하는 다양한 범용 및/또는 전용 처리 어셈블리일 수 있다. 컴퓨팅 유닛의 일부 예시는 중앙 처리 유닛(Central Processing Unit, CPU), 그래픽 처리 유닛(Graphics Processing Unit, GPU), 다양한 전용 인공지능(Artificial Intelligence, AI) 컴퓨팅 칩, 기계 학습 모델 알고 리즘을 실행하는 다양한 컴퓨팅 유닛, 디지털 신호 프로세서(Digital Signal Processor, DSP), 및 임의의 적당 한 프로세서, 컨트롤러, 마이크로컨트롤러 등을 포함하나 이에 제한되지 않는다. 컴퓨팅 유닛은 상술한 각각의 방법과 처리를 수행하며, 예를 들어 입모양을 기반으로 하는 얼굴 이미지 생성 방법과 모델의 트레이닝 방법을 수행한다. 예를 들어, 일부 실시예에서, 입모양을 기반으로 하는 얼굴 이미지 생성 방법과 모델의 트레 이닝 방법은 컴퓨터 소프트웨어 프로그램으로 구현될 수 있으며, 이는 유형적으로 기계 판독 가능 매체, 예를 들어 저장유닛에 포함될 수 있다. 일부 실시예에서, 컴퓨터 프로그램의 일부분 또는 전부는 ROM 및 /또는 통신유닛을 거쳐 기기 상에 로딩 및/또는 설치될 수 있다. 컴퓨터 프로그램은 RAM에 로딩되고 컴퓨팅 유닛에 의해 실행될 때, 상술한 입모양을 기반으로 하는 얼굴 이미지 생성 방법과 모델 의 트레이닝 방법의 하나 또는 복수의 단계를 수행할 수 있다. 대안적으로, 기타 실시예에서, 컴퓨팅 유닛 은 기타 임의의 적당한 방식으로(예를 들어, 펌웨어에 의해) 입모양을 기반으로 하는 얼굴 이미지 생성 방법과 모델의 트레이닝 방법을 수행하도록 구성될 수 있다. 본문에서 상술한 시스템과 기술의 다양한 실시형태는 디지털 전자 회로 시스템, 집적 회로 시스템, 필드 프로그 래머블 게이트 어레이(Field-Programmable Gate Array, FPGA), 전용 집적 회로(Application Specific Integrated Circuit, ASIC), 특정 용도 표준 제품(Application Specific Standard Product, ASSP), 시스템 온 칩의 시스템(System On Chip, SOC), 복합 프로그래머블 논리 소자(Complex Programmable Logic Device, CPLD), 컴퓨터 하드웨어, 펌웨어, 소프트웨어, 및/또는 이들의 조합에서 구현될 수 있다. 이러한 다양한 실시형태는 하 나 또는 복수의 컴퓨터 프로그램에서 실시되는 것을 포함할 수 있고, 해당 하나 또는 복수의 컴퓨터 프로그램은 적어도 하나의 프로그래머블 프로세서를 포함하는 프로그래머블 시스템 상에서 실행 및/또는 해석될 수 있고, 해당 프로그래머블 프로세서는 전용 또는 범용 프로그래머블 프로세서일 수 있고, 저장 시스템, 적어도 하나의 입력 장치 및 적어도 하나의 출력 장치로부터 데이터와 명령을 수신하고, 데이터와 명령을 해당 저장 시스템, 해당 적어도 하나의 입력 장치 및 해당 적어도 하나의 출력 장치에 전송할 수 있다. 본 개시의 방법을 실시하기 위한 프로그램 코드는 하나 또는 복수의 프로그래밍 언어의 임의의 조합으로 프로그 래밍할 수 있다. 이러한 프로그램 코드는 범용 컴퓨터, 전용 컴퓨터 또는 기타 프로그래머블 데이터 처리 장치 의 프로세서 또는 컨트롤러에 제공되어, 프로그램 코드가 프로세서 또는 컨트롤러에 의해 실행될 때 흐름도 및/ 또는 블록도에서 규정하는 기능/조작이 실시되도록 할 수 있다. 프로그램 코드는 완전히 기계 상에서 실행되거 나, 부분적으로 기계 상에서 실행될 수 있고, 개별적인 소프트웨어 패키지로서 부분적으로 기계 상에서 실행되고 부분적으로 원격 기계 상에서 실행되거나 완전히 원격 기계 또는 서버 상에서 실행될 수 있다. 본 개시의 문맥에서, 기계 판독 가능 매체는 유형적인 매체일 수 있는 바, 이는 명령 실행 시스템, 장치 또는 기기가 사용하거나 명령 실행 시스템, 장치 또는 기기와 결합하여 사용하는 프로그램을 포함하거나 저장할 수 있다. 기계 판독 가능 매체는 기계 판독 가능 신호매체 또는 기계 판독 가능 저장매체일 수 있다. 기계 판독 가 능 매체는 전자적, 자기적, 광학적, 전자기적, 적외선적, 또는 반도체 시스템, 장치 또는 기기거나, 상술한 내 용의 임의의 적합한 조합을 포함할 수 있으나 이에 제한되지 않는다. 기계 판독 가능 저장매체의 더 구체적인 예시는 하나 또는 복수의 선에 기반하는 전기적 연결, 휴대용 컴퓨터 디스크, 하드 디스크, 랜덤 액세스 메모리 (RAM), 판독 전용 메모리(ROM), 소거 가능 프로그래머블 판독 전용 메모리(Erasable Programmable Read Only Memory, EPROM) 또는 플래시 메모리, 광섬유, 휴대용 콤팩트 디스크 판독 전용 메모리(Compact Disc Read Only Memory, CD-ROM), 광학 기억 장치, 자기 기억 장치, 또는 상술한 내용의 임의의 적합한 조합을 포함한다. 사용자와의 인터랙션을 제공하기 위해, 컴퓨터에서 여기서 설명한 시스템과 기술을 실시할 수 있으며, 해당 컴 퓨터는, 사용자에게 정보를 표시하기 위한 표시장치(예를 들어, 음극선관(Compact Disc Read Only Memory, CRT) 또는 액정 디스플레이(Liquid Crystal Display, LCD) 모니터); 및 키보드와 지향성 장치(예를 들어, 마우 스 또는 트랙볼)를 구비하며, 사용자는 해당 키보드와 해당 지향성 장치를 통해 입력을 컴퓨터에 제공할 수 있 다. 기타 종류의 장치는 사용자와의 인터랙션을 제공하는 데 사용될 수 있으며; 예를 들어, 사용자에게 제공되 는 피드백은 임의의 형태의 감지 피드백(예를 들어, 시각적 피드백, 청각적 피드백, 또는 촉각적 피드백)일 수 있고; 또한 임의의 형태(사운드 입력, 음성 입력 또는 촉각적 입력을 포함)로 사용자의 입력을 수신할 수 있다. 여기서 언급되는 시스템과 기술은 백그라운드 부재를 포함하는 컴퓨팅 시스템(예를 들어, 데이터 서버), 또는 미들웨어 부재를 포함하는 컴퓨팅 시스템(예를 들어, 애플리케이션 서버), 또는 프론트엔드 부재를 포함하는 컴 퓨팅 시스템(예를 들어, 그래픽 사용자 인터페이스 또는 웹 브라우저를 구비하는 사용자 컴퓨터, 사용자는 해당 그래픽 사용자 인터페이스 또는 해당 웹 브라우저를 통해 여기서 언급한 시스템과 기술의 실시형태와 인터랙션 할 수 있다), 또는 이러한 백그라운드 부재, 미들웨어 부재, 또는 프론트엔드 부재를 포함하는 임의의 조합의 컴퓨팅 시스템에서 실시될 수 있다. 임의의 형태나 매체의 디지털 데이터 통신(예를 들어, 통신 네트워크)에 의 해 시스템의 부재를 서로 연결시킬 수 있다. 통신 네트워크의 예시는 근거리 통신망(Local Area Network, LAN), 광역 통신망(Wide Area Network, WAN)과 인터넷을 포함한다. 컴퓨터 시스템은 클라이언트와 서버를 포함할 수 있다. 클라이언트와 서버는 일반적으로 서로 멀리 이격되어 있 으며 통상적으로 통신 네트워크에 의해 인터랙션한다. 상응한 컴퓨터에서 실행되며 서로 클라이언트-서버 관계 를 갖는 컴퓨터 프로그램을 통해 클라이언트와 서버의 관계가 발생된다. 서버는 클라우드 서버일 수 있고, 클라 우드 컴퓨팅 서버 또는 클라우드 호스트로도 불리우며, 클라우드 컴퓨팅 서비스 시스템 중의 하나의 호스트 제 품으로서, 기존의 물리적 호스트와 VPS 서버(\"Virtual Private Server\", 또는 \"VPS\"로 약칭)에 존재하는 관리 어려움이 크고, 업무 확장성이 약한 흠결을 해결한다. 서버는 분포식 시스템의 서버일 수도 있고, 블록체인과 결합한 서버일 수도 있다. 이해해야 할 점은, 상술한 다양한 형태의 프로세스를 사용하여, 단계를 다시 정렬, 추가 또는 삭제할 수 있다. 예를 들어, 본 개시에서 기재한 각 단계는 병행으로 수행될 수도 있고 순차적으로 수행될 수도 있으며 다른 순 서로 수행될 수도 있는 바, 본 개시에서 개시하는 기술적 해결수단에서 원하는 효과를 구현할 수만 있으면 되고, 본문에서는 제한하지 않는다. 상술한 구체적인 실시형태는 본 개시의 보호 범위를 제한하지 않는다. 본 분야의 통상의 지식을 가진 자는 설계 요구와 기타 요소에 따라, 다양한 수정, 조합, 서브 조합 또는 대체를 가할 수 있다는 것을 이해하여야 한다. 본 개시의 정신과 원칙 내에서 이루어진 모든 수정, 등가적 대체와 개선 등은 모두 본 개시의 보호 범위 내에 포함되어야 한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10"}
{"patent_id": "10-2024-0108876", "section": "도면", "subsection": "도면설명", "item": 1, "content": "첨부 도면은 본 방안을 더욱 잘 이해하기 위한 것이지, 본 개시를 한정하지 않는다. 여기서, 도 1은 본 개시의 실시예에서 제공하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법의 흐름도이다. 도 2는 본 개시의 실시예에서 제공하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법의 흐름도이다. 도 3은 본 개시의 실시예에서 제공하는 입모양을 기반으로 하는 얼굴 이미지 생성 방법의 흐름도이다. 도 4는 본 개시의 실시예에서 제공하는 얼굴 입모양 결정 모델의 트레이닝 방법의 흐름도이다. 도 5는 본 개시의 실시예에서 제공하는 얼굴 입모양 결정 모델의 트레이닝 방법의 흐름도이다. 도 6은 본 개시의 실시예에서 제공하는 입모양을 기반으로 하는 얼굴 이미지 생성 장치의 구조 블록도이다. 도 7은 본 개시의 실시예에서 제공하는 입모양을 기반으로 하는 얼굴 이미지 생성 장치의 구조 블록도이다. 도 8은 본 개시의 실시예에서 제공하는 얼굴 입모양 결정 모델의 트레이닝 장치의 구조 블록도이다. 도 9는 본 개시의 실시예의 입모양을 기반으로 하는 얼굴 이미지 생성 방법과 모델의 트레이닝 방법을 구현하는 전자기기의 블록도이다. 도 10은 본 개시의 실시예의 입모양을 기반으로 하는 얼굴 이미지 생성 방법과 모델의 트레이닝 방법을 구현하 는 전자기기의 블록도이다."}
