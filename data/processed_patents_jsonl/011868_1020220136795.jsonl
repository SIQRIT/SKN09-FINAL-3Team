{"patent_id": "10-2022-0136795", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0056307", "출원번호": "10-2022-0136795", "발명의 명칭": "객체 중심의 분할 및 병합에 기반한 대용량 학습데이터 태깅 장치 및 방법", "출원인": "한국전자통신연구원", "발명자": "윤호섭"}}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능을 이용하여 동영상 내에 포함된 적어도 하나의 객체를 검출하고 추적하는 단계; 상기 검출된 객체의 추적 결과를 통해 분할할 객체가 존재하는 경우 객체 단위로 객체 분할을 수행하는 단계;및상기 분할된 객체 각각에 대하여 병합할 동일 객체가 존재하는 경우 객체 병합을 수행하는 단계를 포함하는, 학습데이터 태깅 방법."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 객체 분할을 수행하는 단계는,상기 검출된 객체의 추적 결과에 대하여, 상기 검출된 객체 각각의 첫 번째 영상과 마지막 영상 간의 유사도 차이를 계산하고, 상기 계산된 유사도 차이가 미리 설정된 값 이하인 경우 해당 객체에 대하여 객체 분할을 수행하는, 학습데이터 태깅 방법."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 객체 분할을 수행하는 단계는,상기 계산된 유사도 차이가 미리 설정된 값 이하인 경우 나머지 영상 각각과 상기 첫 번째 영상 간의 유사도와상기 나머지 영상 각각과 상기 마지막 영상 간의 유사도를 계산하여, 상기 마지막 영상 간의 유사도가 상기 첫번째 영상 간의 유사도보다 높은 영상의 위치를 찾아 상기 해당 객체에 대하여 객체 분할을 수행하는, 학습데이터 태깅 방법."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 객체 분할을 수행하는 단계는,상기 해당 객체에서 객체 분할된 영상들에 대하여, 새로운 객체 아이디(Identity)를 할당하는, 학습데이터 태깅방법."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 객체 병합을 수행하는 단계는, 상기 분할된 객체 각각에 대하여, 어느 하나 객체의 대표 영상과 다른 객체의 대표 영상 간의 유사도를 계산하고, 상기 계산된 유사도가 미리 설정된 일정 값 이상인 경우 대상 객체를 상기 어느 하나 객체로 객체병합하는, 학습데이터 태깅 방법.공개특허 10-2024-0056307-3-청구항 6 제1항에 있어서,상기 객체 병합을 수행하는 단계는, 상기 대상 객체의 객체 아이디(Identity)를 상기 어느 하나 객체의 객체 아이디(Identity)로 변경함으로써, 상기 대상 객체를 상기 어느 하나 객체로 객체 병합하는, 학습데이터 태깅 방법."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 객체 병합을 수행하는 단계는, 다수의 카메라 각각에 대하여, 상기 카메라 각각에서의 객체 병합이 수행되면, 상기 다수의 카메라 각각에서 병합된 객체를 통합적으로 병합 처리하는 단계를 포함하는, 학습데이터 태깅 방법."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 검출 결과, 상기 추적 결과, 상기 분할 결과와 상기 병합 결과는,미리 설정된 파일 형태로 생성되어 저장되고,상기 검출 결과는,상기 검출된 적어도 하나의 객체의 종류, (x, y) 좌표, 폭과 높이 정보를 포함하는, 학습데이터 태깅 방법."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "인공지능을 이용하여 동영상 내에 포함된 적어도 하나의 객체를 검출하고 추적하는 추적부; 상기 검출된 객체의 추적 결과를 통해 분할할 객체가 존재하는 경우 객체 단위로 객체 분할을 수행하는분할부; 및상기 분할된 객체 각각에 대하여 병합할 동일 객체가 존재하는 경우 객체 병합을 수행하는 병합부를 포함하는, 학습데이터 태깅 장치."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 분할부는,상기 검출된 객체의 추적 결과에 대하여, 상기 검출된 객체 각각의 첫 번째 영상과 마지막 영상 간의 유사도 차이를 계산하고, 상기 계산된 유사도 차이가 미리 설정된 값 이하인 경우 해당 객체에 대하여 객체 분할을 수행하는, 학습데이터 태깅 장치."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2024-0056307-4-제10항에 있어서,상기 분할부는,상기 계산된 유사도 차이가 미리 설정된 값 이하인 경우 나머지 영상 각각과 상기 첫 번째 영상 간의 유사도와상기 나머지 영상 각각과 상기 마지막 영상 간의 유사도를 계산하여, 상기 마지막 영상 간의 유사도가 상기 첫번째 영상 간의 유사도보다 높은 영상의 위치를 찾아 상기 해당 객체에 대하여 객체 분할을 수행하는, 학습데이터 태깅 장치."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 분할부는,상기 해당 객체에서 객체 분할된 영상들에 대하여, 새로운 객체 아이디(Identity)를 할당하는, 학습데이터 태깅장치."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,상기 병합부는,상기 분할된 객체 각각에 대하여, 어느 하나 객체의 대표 영상과 다른 객체의 대표 영상 간의 유사도를 계산하고, 상기 계산된 유사도가 미리 설정된 일정 값 이상인 경우 대상 객체를 상기 어느 하나 객체로 객체병합하는, 학습데이터 태깅 장치."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제9항에 있어서,상기 병합부는,상기 대상 객체의 객체 아이디(Identity)를 상기 어느 하나 객체의 객체 아이디(Identity)로 변경함으로써, 상기 대상 객체를 상기 어느 하나 객체로 객체 병합하는, 학습데이터 태깅 장치."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서,상기 병합부는,다수의 카메라 각각에 대하여, 상기 카메라 각각에서의 객체 병합이 수행되면, 상기 다수의 카메라 각각에서 병합된 객체를 통합적으로 병합 처리하는, 학습데이터 태깅 장치."}
{"patent_id": "10-2022-0136795", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제9항에 있어서,상기 검출 결과, 상기 추적 결과, 상기 분할 결과와 상기 병합 결과는,미리 설정된 파일 형태로 생성되어 저장되고,상기 검출 결과는,공개특허 10-2024-0056307-5-상기 검출된 적어도 하나의 객체의 종류, (x, y) 좌표, 폭과 높이 정보를 포함하는, 학습데이터 태깅 장치."}
{"patent_id": "10-2022-0136795", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 학습데이터 태깅 방법은, 인공지능을 이용하여 동영상 내에 포함된 적어도 하나의 객체를 검출하고 추적하는 단계; 상기 검출된 객체의 추적 결과를 통해 분할할 객체가 존재하는 경우 객체 단위 로 객체 분할을 수행하는 단계; 및 상기 분할된 객체 각각에 대하여 병합할 동일 객체가 존재하는 경우 객체 병 합을 수행하는 단계를 포함한다"}
{"patent_id": "10-2022-0136795", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 대용량 학습데이터 태깅 장치 및 방법에 관한 것이며, 보다 구체적으로 카메라 내에 존재하는 다양한 객체의 검출, 추적 및 재인식을 위한 딥러닝 학습에 필요한 대용량의 학습 데이터를 효율적으로 생성하기 위한 학습데이터 태깅 장치 및 방법에 대한 것이다."}
{"patent_id": "10-2022-0136795", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "대규모 IT 기업들이 인공지능 기술의 도입에 있어서 가장 큰 제약사항은 대용량의 학습데이터의 부족이다. MIT 테크놀로지 리뷰의 설문조사에 따르면 글로벌 AI 기업들의 48%는 활용 가능한 데이터 확보에 어려움을 겪는 것으로 조사됐다. 그러므로 AI 기술 개발의 경쟁력의 핵심은 학습데이터에 확보에 있다. 성능 좋은 알고리즘을 채 택했다 하더라도 모델 학습에 필요한 데이터가 충분치 않아 기대하던 성능을 만들어내지 못한 경우가 대부분이 다. 따라서 대용량 학습데이터의 확보는 AI 프로젝트를 성공적으로 추진하는데 있어서 가장 먼저 해결되어야 한다. 그러나, 대용량 학습데이터의 확보는 학습데이터의 수가 많아질수록 시간과 경비가 기하급수적으로 늘어나므로 쉽지 않은 문제이다."}
{"patent_id": "10-2022-0136795", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 기술적 과제는, 카메라 내에 존재하는 다양한 객체의 검출, 추적 및 재인식을 위한 딥러닝 학습에 필 요한 대용량의 학습데이터를 효율적으로 생성하기 위한 학습데이터 태깅 장치 및 방법을 제공하는데 그 목적이 있다. 본 개시에서 이루고자 하는 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급하지 않은"}
{"patent_id": "10-2022-0136795", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "또 다른 기술적 과제들은 아래의 기재로부터 본 개시가 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하 게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0136795", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 학습데이터 태깅 방법은, 인공지능을 이용하여 동영상 내에 포함된 적어도 하나의 객체를 검출하고 추적하는 단계; 상기 검출된 객체의 추적 결과를 통해 분할할 객체가 존재하는 경우 객체 단위 로 객체 분할을 수행하는 단계; 및 상기 분할된 객체 각각에 대하여 병합할 동일 객체가 존재하는 경우 객체 병 합을 수행하는 단계를 포함한다. 이때, 상기 객체 분할을 수행하는 단계는, 상기 검출된 객체의 추적 결과에 대하여, 상기 검출된 객체 각각의 첫 번째 영상과 마지막 영상 간의 유사도 차이를 계산하고, 상기 계산된 유사도 차이가 미리 설정된 값 이하인 경우 해당 객체에 대하여 객체 분할을 수행할 수 있다. 이때, 상기 객체 분할을 수행하는 단계는, 상기 계산된 유사도 차이가 미리 설정된 값 이하인 경우 나머지 영상 각각과 상기 첫 번째 영상 간의 유사도와 상기 나머지 영상 각각과 상기 마지막 영상 간의 유사도를 계산하여, 상기 마지막 영상 간의 유사도가 상기 첫 번째 영상 간의 유사도보다 높은 영상의 위치를 찾아 상기 해당 객체 에 대하여 객체 분할을 수행할 수 있다. 이때, 상기 객체 분할을 수행하는 단계는, 상기 해당 객체에서 객체 분할된 영상들에 대하여, 새로운 객체 아이 디(Identity)를 할당할 수 있다.이때, 상기 객체 병합을 수행하는 단계는, 상기 분할된 객체 각각에 대하여, 어느 하나 객체의 대표 영상과 다 른 객체의 대표 영상 간의 유사도를 계산하고, 상기 계산된 유사도가 미리 설정된 일정 값 이상인 경우 대상 객 체를 상기 어느 하나 객체로 객체 병합할 수 있다. 이때, 상기 객체 병합을 수행하는 단계는, 상기 대상 객체의 객체 아이디(Identity)를 상기 어느 하나 객체의 객체 아이디(Identity)로 변경함으로써, 상기 대상 객체를 상기 어느 하나 객체로 객체 병합할 수 있다. 이때, 상기 객체 병합을 수행하는 단계는, 다수의 카메라 각각에 대하여, 상기 카메라 각각에서의 객체 병합이 수행되면, 상기 다수의 카메라 각각에서 병합된 객체를 통합적으로 병합 처리하는 단계를 포함할 수 있다. 이때, 상기 검출 결과, 상기 추적 결과, 상기 분할 결과와 상기 병합 결과는, 미리 설정된 파일 형태로 생성되 어 저장되고, 상기 검출 결과는, 상기 검출된 적어도 하나의 객체의 종류, (x, y) 좌표, 폭과 높이 정보를 포함 할 수 있다. 본 발명의 다른 실시예에 따른 학습데이터 태깅 장치는, 인공지능을 이용하여 동영상 내에 포함된 적어도 하나 의 객체를 검출하고 추적하는 추적부; 상기 검출된 객체의 추적 결과를 통해 분할할 객체가 존재하는 경우 객체 단위로 객체 분할을 수행하는 분할부; 및 상기 분할된 객체 각각에 대하여 병합할 동일 객체가 존재하는 경우 객체 병합을 수행하는 병합부를 포함한다."}
{"patent_id": "10-2022-0136795", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "본 개시에 대하여 위에서 간략하게 요약된 특징들은 후술하는 본 개시의 상세한 설명의 예시적인 양상일 뿐이며, 본 개시의 범위를 제한하는 것은 아니다."}
{"patent_id": "10-2022-0136795", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 카메라 내에 존재하는 다양한 객체의 검출, 추적 및 재인식을 위한 딥러닝 학습에 필요한 대 용량의 학습데이터를 효율적으로 생성하기 위한 학습데이터 태깅 장치 및 방법을 제공할 수 있다. 본 개시에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은 아래의 기재로부터 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0136795", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참고로 하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나, 본 개시는 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 본 개시의 실시예를 설명함에 있어서 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그에 대한 상세한 설명은 생략한다. 그리고, 도면에서 본 개시에 대한 설명과 관계없 는 부분은 생략하였으며, 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 개시에 있어서, 어떤 구성요소가 다른 구성요소와 \"연결\", \"결합\" 또는 \"접속\"되어 있다고 할 때, 이는 직접 적인 연결 관계 뿐만 아니라, 그 중간에 또 다른 구성요소가 존재하는 간접적인 연결관계도 포함할 수 있다. 또 한 어떤 구성요소가 다른 구성요소를 \"포함한다\" 또는 \"가진다\"고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 배제하는 것이 아니라 또 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 본 개시에 있어서, 제1, 제2 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용되 며, 특별히 언급되지 않는 한 구성요소들 간의 순서 또는 중요도 등을 한정하지 않는다. 따라서, 본 개시의 범 위 내에서 일 실시예에서의 제1 구성요소는 다른 실시예에서 제2 구성요소라고 칭할 수도 있고, 마찬가지로 일 실시예에서의 제2 구성요소를 다른 실시예에서 제1 구성요소라고 칭할 수도 있다. 본 개시에 있어서, 서로 구별되는 구성요소들은 각각의 특징을 명확하게 설명하기 위함이며, 구성요소들이 반드 시 분리되는 것을 의미하지는 않는다. 즉, 복수의 구성요소가 통합되어 하나의 하드웨어 또는 소프트웨어 단위 로 이루어질 수도 있고, 하나의 구성요소가 분산되어 복수의 하드웨어 또는 소프트웨어 단위로 이루어질 수도 있다. 따라서, 별도로 언급하지 않더라도 이와 같이 통합된 또는 분산된 실시예도 본 개시의 범위에 포함된다. 본 개시에 있어서, 다양한 실시예에서 설명하는 구성요소들이 반드시 필수적인 구성요소들을 의미하는 것은 아 니며, 일부는 선택적인 구성요소일 수 있다. 따라서, 일 실시예에서 설명하는 구성요소들의 부분집합으로 구성 되는 실시예도 본 개시의 범위에 포함된다. 또한, 다양한 실시예에서 설명하는 구성요소들에 추가적으로 다른 구성요소를 포함하는 실시예도 본 개시의 범위에 포함된다. 본 개시에 있어서, 본 명세서에 사용되는 위치 관계의 표현, 예컨대 상부, 하부, 좌측, 우측 등은 설명의 편의 를 위해 기재된 것이고, 본 명세서에 도시된 도면을 역으로 보는 경우에는, 명세서에 기재된 위치 관계는 반대 로 해석될 수도 있다. 본 개시에 있어서, \"A 또는 B\", \"A 및 B 중 적어도 하나\", \"A 또는 B 중 적어도 하나\", \"A, B 또는 C\", \"A, B 및 C 중 적어도 하나\", 및 \"A, B, 또는 C 중 적어도 하나\"와 같은 문구들 각각은 그 문구들 중 해당하는 문구에 함께 나열된 항목들 중 어느 하나, 또는 그들의 모든 가능한 조합을 포함할 수 있다. 동영상이나 이미지를 다루는 업체라면 학습을 위한 데이터를 만들기 위해 각 동영상과 이미지에 원하는 객체마 다 고유의 라벨을 다는 게 일반적이다. 이러한 작업을 보통 태깅 작업이라 정의하며, 태깅이란 분류 및 클러스 터링 알고리즘의 학습에 용이하도록 동영상 혹은 이미지에 인식 대상에 고유의 라벨 정보를 입력하는 것이다. 이때, 방대한 동영상에 일일이 태깅을 하는 것은 매우 시간 소모적이고 비효율적이기 때문에 학습데이터화에 커 다란 제약으로 작용한다. 그러므로, 이러한 태깅 작업을 자동으로 해줄 수 있다면 학습데이터를 훨씬 쉽게 생성 할 수 있으므로. 이를 가능케 하는 자동 태깅 플랫폼의 개발이 시도되었다. 이중 대표적인 예로 추치 AI(Chooch AI)를 들 수 있다. 이 플랫폼은 API를 통해 입력된 동영상/이미지의 프레임마다 등장사물, 사운드와 관련된 태 그를 자동으로 달아 학습용 데이터로 만들어주는 플랫폼이다. 이러한 종류의 플랫폼은 인공지능 기술 개발에 필 수적인 고품질의 학습데이터를 효율적으로 생성할 수 있기 때문에 데이터를 확보하는 전략으로 주로 채택되어 왔다. 그러나, 기존에 객체(휴먼) 재인식 학습데이터 생성을 위해 개발된 자동화된 AI 알고리즘을 사용한 태깅툴은 사 람이 겹쳐지는 경우가 수시로 발생시 다수의 에러가 발생된다. 이러한 에러들은 대부분 일일이 수동으로 한 프 레임씩 자동 태깅된 결과를 수정하는 검수 작업이 필수적으로 발생하여, 이로 인해 3분의 짧은 동영상을 태깅할 때도 화면 내에 다수의 객체(예를 들어, 20명 이상의 휴먼)이 존재할 경우엔 최소 4시간 이상의 수동 태깅 보조 작업이 필요한 문제점이 있어왔다. 결국, 이러한 태깅의 난이도는 휴먼 재인식 알고리즘을 개발하기 위해 필수적인 대용량 학습데이터 생성에 있어 해결하기 어려운 문제로 인식되어 왔다. 이러한 문제를 해결하긴 위해 최근에는 사람의 검수없이 AI 스스로 학습데이터를 태깅하는 방식으로 알고리즘이 개발되고 있으나, 간단한 영상을 제외하고는 실환경에서 아직까지는 AI가 스스로 태깅된 결과는 신뢰성이 부족 하다는 단점이 존재하여 활발히 활용되지 못하고 있다. 본 개시의 실시예들은, AI 기술 중에서도 대용량의 학습데이터가 필요한 객체(휴먼) 검출, 추적 및 인식 기술 개발을 위해, 사전에 이미 개발된 AI 기술로 객체(휴먼) 검출 및 추적용 학습데이터를 라벨링하고, AI가 라벨링 한 데이터를 휴먼 라벨러가 객체 중심의 분할 및 병합하는 툴을 사용하여 최종적으로 검출, 추적 및 인식 기술 학습용 데이터 테깅을 완료하는 방식을 제공하는 것을 그 요지로 한다. 이와 같은 방식으로 객체 검출, 추적 및 재인식 학습데이터를 생성하는 방법을 사용하면, 일일이 모든 데이터를 라벨러(휴먼)가 매 프레임마다 라벨링 할 때에 비해 현재 약 10배 이상의 라벨링 시간과 자원(resource) 절약이 가능하여, 학습에 필요한 대용량 데이터의 수집이 경제적으로 가능해진다. 또한, 본 개시의 실시예들에 의해 생 성된 대용량 학습데이터를 이용해 객체 검출, 추적 및 재인식 AI의 성능이 높아지면 높아질수록 향후 학습데이 터 추가 시 라벨러(휴먼)가 관여하는 태깅 작업도 기하급수적으로 줄어들 수 있다. 따라서, 본 개시의 실시예들은, AI가 스스로 태깅한 결과를 기반으로 수동으로 태깅하는 난이도를 획기적으로 개선할 수 있는 방식으로, 수동으로 태깅할 때 한 프레임 씩 라벨러가 일일이 검수하는 것이 아니라, 영상의 시 작부터 끝까지 존재하는 각각의 객체(휴먼)에 대해 AI가 스스로 검출하고 추적해서 생성한 태깅 결과를 일정 파 일 예를 들어, .csv 형태의 파일화 저장한 후, 라벨러는 저장된 동영상.csv 파일을 읽어서 각각의 객체들의 검 출, 추적 정보를 확인한 후, 간단한 마우스 동작만으로 분할 및 병합 하는 방식으로 태깅할 수 있다. 이와 같은 태깅 방식은 기존의 라벨러가 한 프레임씩 일일이 검수하는 방식보다 10배이상 효율적이며, AI가 스스로 태깅한 결과보다 학습에 활용 가능한 신뢰성 높은 태깅 결과를 신속하게 획득할 수 있다. 도 1은 본 개시의 일 실시예에 따른 학습데이터 태깅 방법에 대한 동작 흐름도를 나타낸 것이다. 도 1을 참조하면, 본 개시의 일 실시예에 따른 학습데이터 태깅 방법은, 동영상이 입력되면 미리 학습된 인공지 능(AI)를 이용하여 동영상 내에 포함된 객체를 검출하고, 이러한 객체 검출 결과를 미리 설정된 형태의 파일로 생성하여 저장한다(101, 102). 여기서, 동영상은 카메라에서 획득된 태깅이 필요한 입력 동영상 파일을 의미할 수 있으며, 이 동영상 내에는 다양한 객체가 포함되어 있기에, 본 개시의 방법을 통해 각 객체에 대한 검출, 추적 및 재인식 정보를 태깅할 수 있다 단계 101에서 객체를 검출하기 위한 인공지능은, ResNet, RFCN, DenseNet, YOLO, FPN, Mask RCNN, RetinaNet, ComeNet 등을 포함하는 딥러닝 기술을 기반으로 하는 다양한 모델의 객체 검출 알고리즘 중 어느 하나일 수 있 다. 예를 들어, 객체를 검출하기 위한 AI는 상술한 AI 알고리즘 방식 중 YOLO V.5 모델을 채택하여 동영상에서 객체를 검출할 수 있다. 단계 101에 의해 검출된 객체 검출 결과는 detect.csv 파일로 생성되어 저장될 수 있으며, 이때 생성된 detect.csv 파일은 도 2에 도시된 예와 같이, 다양한 정보를 저장할 수 있다. 도 2의 Frame number 테이블은 동영상을 프레임단위로 나누었을 때 생성되는 ID로 만일 동영상이 1초에 초당 30 frame으로 저장되고 10초 길이라면 1~300까지의 ID가 생성된다. 다음 Id Type 테이블은 태깅하려는 객체의 종류 (예를 들어, 사람, 자동차, 자전거, 오토바이 등)를 의미할 수 있다. Id Number 테이블은 각 프레임에 존재하는 객체들에 할당된 고유 Number를 의미할 수 있다. 다만, detect.csv 파일은 검출된 영역정보만 존재하고 Id가 부 여되지 않았기 때문에 모두 0으로 세팅될 수 있다. 다음 x, y, width, height 테이블은 검출된 객체의 x, y 위 치, 폭과 높이를 의미하며 이정보를 사용하여 검출된 객체의 MBR(Minimum Bounding Rectangle)를 표시할 수 있 다. 마지막으로 confidence 스코어는 AI 검출모델에서 검출된 객체의 신뢰도 값을 의미할 수 있는데, confidence 스코어 값이 높을수록 노이즈가 아니고 실제 객체일 확률이 높음을 의미한다. 도 2와 같이 객체 검출 AI 모델을 사용하여 detect.csv 파일이 생성되면 입력된 동영상과 검출된 객체 결과 즉, detect.csv 파일을 기반으로 검출된 객체를 추적하고, 객체 추적 결과를 미리 설정된 형태의 파일로 생성하여저장한다(103, 104). 여기서, 단계 103은 AI 기반 객체 추적 알고리즘을 이용하여 detect.csv 파일에 저장된 객체를 동영상에서 추적 할 수 있다. 이때, AI 기반 객체 추적 알고리즘은 DeepSort, AP-HWDPL, RAN, MHT-bLSTM 등의 알고리즘 중 어느 하나의 알고리즘을 이용할 수 있다. 예를 들어, AI 기반 객체 추적 알고리즘은 DeepSort모델을 사용할 수 있으 며, 객체 추적 결과를 track.csv 파일로 생성하여 저장할 수 있다. 이때, track.csv 파일은 도 3에 도시된 예와 같이, 다양한 정보를 저장할 수 있다. 도 3에 도시된 바와 같이, track.csv 파일과 detect.csv 파일의 차이점은 Id number 테이블에 있다. AI 기반 객체 추적 알고리즘 예를 들어, DeepSort 모델은 검출된 Type 및 박스 정보를 이용하여 앞선 프레임에 존재하는 동일 객체에 대해 추적을 수행하여 순차적으로 이후 프레임에 나타난 동일 객체에 대해 앞선 프레임의 Id와 동 일한 Id를 할당한다. 단계 104의 과정을 통해 검출된 객체의 추적이 종료되면, 사전 처리된 track.csv 파일을 통해 잘못 태깅된 객체 별 ID(identity)를 각 프레임단위가 아니라 객체단위로 분할하고, 객체단위 분할이 완료되면, 객체단위 분할 완 료 결과를 바탕으로 객체 병합을 수행함으로써, 학습데이터를 태킹하는 과정을 완료한다(105~112) 예를 들어, 도 4는 분할과 병합용 GUI 형태에 대한 일 예시도를 나타낸 것으로, AI 모델에 의해 사전 레이블링 된 객체를 확인하는 GUI 및 기능을 나타낸 것이다. 해당 GUI는 비록 도 4에 도시되진 않았지만, 동영상과 CSV 파일의 path를 선택하는 인터페이스를 제공하고, 해 당 인터페이스를 통해 동영상과 CSV 파일의 path를 선택한 후, 실행 버튼 예를 들어, Go 버튼이 입력되면 입력 동영상에 존재하는 각 ID에 대해 ID별 대표 영상을 출력한다. 이때, 각 ID별 대표 영상은 “Id number_대 표 frame number).bmp” 예를 들어, 0014_155.bmp와 같이 표시될 수 있는데, 각 ID별 대표 영상은 각 ID가 첫 번째 나타난 영상, 마지막에 나타난 영상 또는 중간에 나타난 영상 중 어느 하나의 영상으로 선택할 수 있다. 도 4의 경우에는 각 ID별 대표 영상을 첫 번째 나타난 영상으로 선택한 경우로, 각각의 Id number는 각 ID가 나 타난 첫 번째 프레임에서 마지막으로 나타난 프레임까지의 전체 정보를 대표하게 된다. 나아가, 해당 GUI는 사용자 또는 라벨러에 의해 특정 ID의 대표 영상(예를 들어, 410)을 선택한 후, 프리뷰 윈 도우의 Play 버튼(미도시)을 선택하면, 선택된 ID가 발생된 동영상의 시작부터 끝 위치까지의 정보를 고유 색상의 MBR의 출력과 더불어 확인 가능하다. 즉, 사용자 또는 라벨러에 의해 선택된 특정 ID가 동일한 프레임은, 프리뷰 윈도우를 통해 출력됨으로써, 특정 ID가 나타난 프레임들을 순차적으로 확인할 수 있다. 다음은, 도 4에 도시된 각각의 ID를 일정 횟수 클릭, 예를 들어, 2회 클릭으로 선택하면, 도 5에 도시된 바와 같은 분할(Split) GUI를 제공할 수 있다. 도 5에 도시된 분할 GUI 기능은, 추적(tracking) 모델이 중첩이나 노 이즈에 의해 서로 다른 객체를 하나의 객체로 인식했을 때 이를 쉽게 분리할 수 있도록 지원하는 GUI 기능이다. 도 5의 분할 GUI는 하나의 객체로 할당된 ID에 대해 첫 번째 발생 프레임에서부터 마지막으로 발생된 프레임을 순차적으로 화면에 보여준 것으로, 라벨러는 이 결과를 보고 모든 프레임이 동일한 ID를 포함하는지 확인하고, 만일 도 5와 같이 서로 다른 객체가 결합되어 있다면 그 부분을 마우스 클릭으로 처음과 끝 부분으로 선택(51 0)한 후 GUI의 Split 버튼을 선택함으로써, 다른 객체를 분할할 수 있다. 이때, 다른 객체는, 자동적으로 새로 운 ID가 할당되며, 변경된 정보는 split.csv 파일에 반영될 수 있다. 이러한 작업은 전체 ID에 대해 수행될 수 있으며, 분할(Split) 태깅이 라벨러를 통해 수동으로 완성될 수 있다. 이러한 분할 작업은 사용자 또는 라벨러의 수동 작업에 의해 수행되는 것으로 제한되거나 한정되지 않으며, 미 리 학습된 인공지능을 이용하여 이루어질 수도 있으며, 이를 통해 수동으로 분할(Split)해야 하는 ID를 효율적 으로 줄일 수 있다. 분할(Split) 작업을 자동으로 수행하는 AI 기술은 각 ID의 첫 번째 영상과 마지막 영상 간 의 유사도 차이를 계산하고 이 유사도의 차이가 일정치 이하의 작은 값이라면 분할(Split)이 필요한 ID로 결정 할 수 있다. 그리고, 영상을 순차적으로 검색하면서 첫 번째 영상과 마지막 영상을 대상으로 유사도를 측정해서, 첫 번째 영상보다 마지막 영상이 더 유사도가 높게 나오는 위치를 찾아 자동으로 분할(Split)되어야 하는 시작 위치로 할당할 수 있다. 이를 c++ 알고리즘으로 표현하면 아래와 같이 나타낼 수 있다. n = Id count; i = Id number; i.frame_start = frame count; i.frame_end = frame count; j = frame number; For (i = 1, I < n, i++) { If (similarity(i.frame_start, i.frame_end) < Threshod_value(0.7) { For (j = z_start+1, j < z_end-1, j++) If (similarity(j, z_start) < similarity(j, z_end) break; i.split_point = j; else i.split_point = 0; } 여기서, n은 한 동영상 내에 존재하는 ID 개수를 의미하고, i는 0 ~ n 사이의 값을 의미하며, i.frame_start는 동일 ID가 처음 나타난 프레임 number를 의미하고, i.frame_end는 동일 ID가 마지막으로 나타난 프레임 number 를 의미하며, j는 i.frame_start부터 i.frame_end를 의미할 수 있다. 상기 알고리즘에서 similarity(i.frame_start, i.frame_end)는 track.csv를 읽어서 얻은 하나의 i번째 ID가 나 타난 첫 번째 프레임(i.frame_start) 내의 MBR 영상과 마지막 프레임(i.frame_end)의 MBR영상간의 유사도를 계 산하는 함수를 의미한다. 만일 이 두 MBR 영상이 동일하다면 유사도는 1.0 값을 가지며, 완전히 다르다면 0.0의 값을 가지게 된다. 이러한 두 영상간의 유사도를 계산하는 방법은 가장 간단하게는 두영상의 차를 계산하는 방 법 등이 있으며, 본 개시의 방법에서는 유사도를 계산하는 방법으로 특정하지는 않는다. 상술한 방법을 통해 첫 번째 프레임 영상과 마지막 프레임 영상의 유사도가 일정 임계치 이하의 값 예를 들어, 0.7(Threshold_value)이하의 값을 갖는 ID는 두 개의 서로 다른 ID로 분할(Split)될 수 있다. 이때, 분할(Split)되는 위치를 찾기 위해 본 개시에서는 동일 ID를 갖는 모든 frame MBR에 대해 첫 번째 MBR과 마지막 MBR 간의 유사도 비교를 통해, 어떤 프레임 순서에서 첫 번째 MBR이 아닌 마지막 MBR과 더 유사한 값이 나왔는지 similarity(j, z_start), similarity(j, z_end) 값들간의 비교를 통해 검색할 수 있다. 이를 통해 최종적으로 i.split_point에 결과가 저장되는데 i.split_point가 0이면 분할(Split)이 필요하지 않은 ID이고 i.split_point가 0가 아닌 값이라면 이 값은 i번째 ID가 몇 번째 프레임에서 두 개로 분리된 새로운 ID 로 할당해야 하는지를 의미한다. 분할(Split) 검수가 완수되면 라벨러가 병합될 ID를 찾고 이를 수동으로 병합하는 과정을 수행한다. 도 6은 병 합용 GUI 형태에 대한 일 예시도를 나타낸 것으로, 도 4의 GUI를 병합(merge) 작업을 위해 좀더 세밀하게 표현 한 것이다. 현재 split,csv 파일에 포함된 고유 ID 들은 영상의 복잡도, 겹침, 조명, AI 기술의 한계 등으로 동일 객체임에 도 불구하고 서로 다른 여러 ID로 할당되어 나타난다. 병합 작업은 여러 다른 ID로 할당된 객체들을 하나의 통 합된 ID로 할당하는 작업이다. 예를 들어, 도 6에 도시된 바와 같이, 라벨러는 화면내에 존재하는 동일 객체이 나 다른 ID로 할당된 객체를 모두 찾아서 선택한 후, Merge 버튼(미도시)을 선택하면 선택된 ID 번호 중 가장 작은 번호로 모든 ID를 변경한 후, 이를 merge.csv 파일에 반영한다. 분할(Split) 작업과 마찬가지로 병합(merge) 작업도 AI 기법을 사용해서 자동화가 가능하다. 본 개시의 방 법은, 각각의 대표 ID 영상을 기준으로 순차적으로 다른 대표 ID 영상과 비교하여 유사도가 일정치(threshold value) 보다 큰지 확인한 후, 일정치 이상 되는 ID는 현재 ID로 변경하는 방식을 사용하면 된다.이를 C++ 언어의 알고리즘으로 표현하면 아래와 같이 나타낼 수 있다. n = id count; i = ID number; For (i = 1, I < n, i++) i.flag = i; For (i = 1, I < n, i++) { if (i.flag == i) For (j = i+1, j < n, j++) If (similarity(i, j) > Threshold_value) j.flag = i; } 여기서, n은 고유 ID 개수를 의미하고, i는 1과 n사이의 값을 의미할 수 있다. 상기 병합 알고리즘을 시행한 후, i번째 ID와 i번째 I,flag 값이 동일하다면 GUI 내 서로 다른 ID와 비교하여 유사한 ID가 없었다는 것을 의미하며, i번째 ID와 i번째 I,flag 값이 동일하지 않다면 서로 다른 ID 간의 유사 도 값이 기준치(Threshold_value = 0.7) 이상인 조건에 부합되어, 동일한 ID 값을 같도록 병합(merge) 되었음을 알 수 있다. 이때, 유사도 측정은 분할(Split) 방식과 같은 방식이 사용할 수도 있고 기타 다양한 방법으로 유사도 측정이 가능하다. 마지막으로 이 작업의 AI 기술에 의해 자동적으로 수행이 완료되면 최종적으로 merge.csv 파일에 그 결과를 저장한다. 하나의 카메라로부터 얻어진 객체 태깅이 종료되면 도 7에 도시된 바와 같이, 시공간 배치가 다른 다중 카메라 로 입력된 동일 객체 간의 재인식 태깅이 가능하다. 도 7에서 시공간 배치가 다른 A, B, C를 포함하는 다수 개의 동영상과 이미 처리된 merge.csv파일이 존재한다면, 다중 카메라 동영상 및 merge.csv 통합입력 처리를 수행하여 GUI에 표시한다. 201 단계의 처 리 결과는 도 6에서 설명한 바와 같은 병합(merge)용 GUI를 통해 표시될 수 있으며, 기존 단일 영상 내의 병합 (merge)과 다른 점은 A(Camera name)_0014(Id number)_155(frame number).bmp와 같이 각 ID의 이름에 어떤 카 메라에서 입력되었는지 카메라 정보가 추가 포함되어 있게 된다. 다음 단계는 202~205 단계를 거치는데, 이는 하나의 카메라 안의 ID 병합(merge) 단계로 도 1에 도시된 단계 109~112와 동일한 단계를 거치게 된다. 즉, 하나의 카메라 안에서 동일한 ID를 병합(merge)하는 태깅 방법과 다 중 카메라 안에서 동일한 ID를 병합(merge)하는 방법은 입력 형태만 다를 뿐 기능적으론 동일한 처리를 거치게 된다. 카메라간 통합 merge.csv 파일은 도 8에 도시된 바와 같이, 카메라별 ID를 포함할 수 있다. 기존의 객체 검출, 추적 및 재인식을 위한 대용량 학습데이터를 생성하기 위해선 전체 태깅하는 데이터의 양을 줄이기 위해 사전에 객체 검출 및 추적 AI 기술을 적용한 후, 이 작업을 통해 얻어진 라벨링된 정보를 라벨러 (휴먼)가 동영상을 한 프레임 단위로 읽어 들이면서 잘못 라벨링된 정보를 수정 및 교정하는 방식이 주로 사용 되어 왔다. 이러한 방법은 사전에 AI 기술을 사용하지 않고, 전체를 매 프레임 마다 일일이 손으로 라벨링하는 방법에 비해 시간과 노력을 줄이는 장점이 있으나, 그럼에도 불구하고 동영상의 길이가 길거나 한 프레임안에 라벨링해야 하 는 객체가 많을 때는 태깅에 많은 시간과 노력이 요구되어, 필드에서 원하는 대용량 학습데이터 생성에 장애가 되어왔다. 본 개시의 실시예에 따른 방법은, 휴먼 라벨러가 매 프레임마다 존재하는 복수의 객체를 일일이 확인하고 수정 하는 방식이 아니라, AI 기술 중에서도 대용량의 학습데이터가 필요한 객체(휴먼) 검출, 추적 및 인식 기술 개 발을 위해, 사전에 이미 개발된 AI 기술로 객체(휴먼) 검출 및 추적용 학습데이터를 라벨링하고, AI가 라벨링한 데이터를 휴먼 라벨러가 객체 중심의 분할 및 병합하는 툴을 사용하여 최종적으로 검출, 추적 및 인식 기술 학 습용 데이터 테깅을 완료할 수 있다. 또한, 본 개시의 실시예에 따른 방법은, 기존의 프레임 단위의 태깅 방식에 비해 태깅 작업시간이 1/10이하로 줄게 되어 약 10배 이상의 라벨링 시간과 자원(resource) 절약이 가능하여 학습에 필요한 대용량 데이터의 수집 이 경제적으로 가능하며, 수동으로 태깅하는 분할(Split) 및 병합(merge) 태깅 또한 성능이 우수한 AI 기술로 대치한다면, 기존의 객체기반 수동 태깅 작업 시간을 다시 1/10 이하로 줄일 수 있다. 도 9는 본 개시의 다른 실시예에 따른 학습데이터 태깅 장치에 대한 구성을 나타낸 것으로, 도 1 내지 도 8의 방법을 수행하는 장치의 개념적인 구성을 나타낸 것이다. 도 9를 참조하면, 본 개시의 다른 실시예에 따른 학습데이터 태깅 장치는, 추적부, 분할부와 병 합부를 포함하며, 이 뿐만 아니라 본 개시의 장치에서 필요로 하는 기본적인 구성 예를 들어, 메모리, 통 신 수단 등을 포함할 수 있다. 추적부는, 인공지능을 이용하여 동영상 내에 포함된 적어도 하나의 객체를 검출하고 추적한다. 이때, 추적부는, 검출된 객체와 입력된 동영상을 이용한 미리 설정된 객체 추적 알고리즘을 통해 검출된 객체 각각에 대하여 추적할 수 있다. 분할부는, 검출된 객체의 추적 결과를 통해 분할할 객체가 존재하는 경우 객체 단위로 객체 분할을 수행한 다. 이때, 분할부는, 상기 검출된 객체의 추적 결과에 대하여, 상기 검출된 객체 각각의 첫 번째 영상과 마지 막 영상 간의 유사도 차이를 계산하고, 상기 계산된 유사도 차이가 미리 설정된 값 이하인 경우 해당 객체에 대 하여 객체 분할을 수행할 수 있다. 이때, 분할부는, 상기 계산된 유사도 차이가 미리 설정된 값 이하인 경우 나머지 영상 각각과 상기 첫 번 째 영상 간의 유사도와 상기 나머지 영상 각각과 상기 마지막 영상 간의 유사도를 계산하여, 상기 마지막 영상 간의 유사도가 상기 첫 번째 영상 간의 유사도보다 높은 영상의 위치를 찾아 상기 해당 객체에 대하여 객체 분 할을 수행할 수 있다. 이때, 분할부는, 상기 해당 객체에서 객체 분할된 영상들에 대하여, 새로운 객체 아이디(Identity)를 할당 할 수 있다. 병합부는, 분할된 객체 각각에 대하여 병합할 동일 객체가 존재하는 경우 객체 병합을 수행한다. 이때, 병합부는, 상기 분할된 객체 각각에 대하여, 어느 하나 객체의 대표 영상과 다른 객체의 대표 영상 간의 유사도를 계산하고, 상기 계산된 유사도가 미리 설정된 일정 값 이상인 경우 대상 객체를 상기 어느 하나 객체로 객체 병합할 수 있다. 이때, 병합부는, 상기 대상 객체의 객체 아이디(Identity)를 상기 어느 하나 객체의 객체 아이디 (Identity)로 변경함으로써, 상기 대상 객체를 상기 어느 하나 객체로 객체 병합할 수 있다. 이때, 병합부는, 다수의 카메라 각각에 대하여, 상기 카메라 각각에서의 객체 병합이 수행되면, 상기 다수 의 카메라 각각에서 병합된 객체를 통합적으로 병합 처리할 수 있다. 본 개시의 장치에서, 객체의 검출 결과, 추적 결과, 분할 결과와 병합 결과는, 미리 설정된 파일 형태로 생성되 어 저장될 수 있으며, 상기 검출 결과는, 상기 검출된 적어도 하나의 객체의 종류, (x, y) 좌표, 폭과 높이 정 보를 포함할 수 있다. 비록, 도 9의 장치에서 그 설명이 생략되더라도, 본 개시의 실시예에 따른 장치는 도 1 내지 도 8의 방법에서 설명한 모든 내용을 포함할 수 있으며, 이는 해당 기술 분야에 종사하는 당업자에게 있어서 자명하다.도 10은 본 개시의 다른 실시예에 따른 학습데이터 태깅 장치가 적용되는 디바이스의 구성도를 나타낸 것이다. 예를 들어, 도 9의 본 개시의 다른 실시예에 따른 학습데이터 태깅 장치는 도 10의 디바이스가 될 수 있 다. 도 10을 참조하면, 디바이스는 메모리, 프로세서, 송수신부 및 주변 장치 를 포함할 수 있다. 또한, 일 예로, 디바이스는 다른 구성을 더 포함할 수 있으며, 상술한 실시예로 한정 되지 않는다. 이때, 상기 디바이스는 예를 들어 이동 가능한 사용자 단말기(예를 들어, 스마트 폰, 노트 북, 웨어러블 기기 등) 이거나 고정된 관리 장치(예를 들어, 서버, PC 등) 일 수 있다. 보다 상세하게는, 도 10의 디바이스는 학습데이터 태깅 툴, 데이터 라벨링 시스템 등과 같은 예시적인 하 드웨어/소프트웨어 아키텍처일 수 있다. 이때, 일 예로, 메모리는 비이동식 메모리 또는 이동식 메모리일 수 있다. 또한, 일 예로, 주변 장치는 디스플레이, GPS 또는 다른 주변기기들을 포함할 수 있으며, 상술 한 실시예로 한정되지 않는다. 또한, 일 예로, 상술한 디바이스는 상기 송수신부와 같이 통신 회로를 포함할 수 있으며, 이에 기 초하여 외부 디바이스와 통신을 수행할 수 있다. 또한, 일 예로, 프로세서는 범용 프로세서, DSP(digital signal processor), DSP 코어, 제어기, 마이크 로제어기, ASIC들(Application Specific Integrated Circuits), FPGA(Field Programmable Gate Array) 회로들, 임의의 다른 유형의 IC(integrated circuit) 및 상태 머신과 관련되는 하나 이상의 마이크로프로세서 중 적어도 하나 이상일 수 있다. 즉, 상술한 디바이스를 제어하기 위한 제어 역할을 수행하는 하드웨어적 /소프트웨어적 구성일 수 있다. 또한 상기 프로세서는 전술한 도 9의 추적부, 분할부와 병합부 의 기능을 모듈화하여 수행할 수 있다. 이때, 프로세서는 학습데이터 태깅 장치의 다양한 필수 기능들을 수행하기 위해 메모리에 저장된 컴퓨터 실행가능한 명령어들을 실행할 수 있다. 일 예로, 프로세서는 신호 코딩, 데이터 처리, 전력 제어, 입출력 처리 및 통신 동작 중 적어도 어느 하나를 제어할 수 있다. 또한, 프로세서는 물리 계층, MAC 계층, 어플리케이션 계층들을 제어할 수 있다. 또한, 일 예로, 프로세서는 액세스 계층 및/또는 어플 리케이션 계층 등에서 인증 및 보안 절차를 수행할 수 있으며, 상술한 실시예로 한정되지 않는다. 일 예로, 프로세서는 송수신부를 통해 다른 장치들과 통신을 수행할 수 있다. 일 예로, 프로세서 는 컴퓨터 실행가능한 명령어들의 실행을 통해 학습데이터 태깅 장치가 네트워크를 통해 다른 장치들과 통신을 수행하게 제어할 수 있다. 즉, 본 개시에서 수행되는 통신이 제어될 수 있다. 일 예로, 송수신부 는 안테나를 통해 RF 신호를 전송할 수 있으며, 다양한 통신망에 기초하여 신호를 전송할 수 있다. 또한, 일 예로, 안테나 기술로서 MIMO 기술, 빔포밍 등이 적용될 수 있으며, 상술한 실시예로 한정되지 않는다. 또한, 송수신부를 통해 송수신한 신호는 변조 및 복조되어 프로세서에 의해 제어될 수 있으며, 상 술한 실시예로 한정되지 않는다. 본 개시의 예시적인 방법들은 설명의 명확성을 위해서 동작의 시리즈로 표현되어 있지만, 이는 단계가 수행되는 순서를 제한하기 위한 것은 아니며, 필요한 경우에는 각각의 단계가 동시에 또는 상이한 순서로 수행될 수도 있 다. 본 개시에 따른 방법을 구현하기 위해서, 예시하는 단계에 추가적으로 다른 단계를 포함하거나, 일부의 단 계를 제외하고 나머지 단계를 포함하거나, 또는 일부의 단계를 제외하고 추가적인 다른 단계를 포함할 수도 있 다. 본 개시의 다양한 실시예는 모든 가능한 조합을 나열한 것이 아니고 본 개시의 대표적인 양상을 설명하기 위한 것이며, 다양한 실시예에서 설명하는 사항들은 독립적으로 적용되거나 또는 둘 이상의 조합으로 적용될 수도 있 다. 또한, 본 개시의 다양한 실시예는 하드웨어, 펌웨어(firmware), 소프트웨어, 또는 그들의 결합 등에 의해 구현 될 수 있다. 하드웨어에 의한 구현의 경우, 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 범용 프로세서(general processor), 컨트롤러, 마이크로 컨트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 본 개시의 범위는 다양한 실시예의 방법에 따른 동작이 장치 또는 컴퓨터 상에서 실행되도록 하는 소프트웨어 또는 머신-실행가능한 명령들(예를 들어, 운영체제, 애플리케이션, 펌웨어(firmware), 프로그램 등), 및 이러한소프트웨어 또는 명령 등이 저장되어 장치 또는 컴퓨터 상에서 실행 가능한 비-일시적 컴퓨터-판독가능 매체 (non-transitory computer-readable medium)를 포함한다."}
{"patent_id": "10-2022-0136795", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 학습데이터 태깅 방법에 대한 동작 흐름도를 나타낸 것이다. 도 2는 객체 검출 결과에 대한 일 예시도를 나타낸 것이다. 도 3은 객체 추적 결과에 대한 일 예시도를 나타낸 것이다. 도 4는 분할과 병합용 GUI 형태에 대한 일 예시도를 나타낸 것이다. 도 5는 분할용 GUI 형태에 대한 일 예시도를 나타낸 것이다. 도 6은 병합용 GUI 형태에 대한 일 예시도를 나타낸 것이다. 도 7은 시공간 배치가 다른 다중 카메라로 입력된 동일 객체간의 재인식 태깅 과정에 대한 일 실시예의 동작 흐 름도를 나타낸 것이다. 도 8은 카메라간 통합 결과에 대한 일 예시도를 나타낸 것이다. 도 9는 본 개시의 다른 실시예에 따른 학습데이터 태깅 장치에 대한 구성을 나타낸 것이다. 도 10은 본 개시의 다른 실시예에 따른 학습데이터 태깅 장치가 적용되는 디바이스의 구성도를 나타낸 것이다."}
