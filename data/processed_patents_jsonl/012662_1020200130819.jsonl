{"patent_id": "10-2020-0130819", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0048085", "출원번호": "10-2020-0130819", "발명의 명칭": "머신 러닝 및 위치/환경 정보에 기반한 객체 영상 분류 방법", "출원인": "박교열", "발명자": "박교열"}}
{"patent_id": "10-2020-0130819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "촬영된 객체 영상에 기초하여 객체의 종류를 식별 및 분류하기 위한, 객체 영상 분류에 관한 컴퓨터 구현 방법(Computer implemented method)으로서,객체 영상 분류를 요청한 서비스 이용자로부터, 소정 객체가 촬영된 객체 영상을 수신하는 단계; 및제1 분류 모델에 따른 출력 결과 및 제2 분류 모델에 따른 출력 결과를 영상 분류 통합 모델의 입력으로 하여,상기 영상 분류 통합 모델에 따른 출력 결과인 영상 분류 결과를 획득하는 단계;를 포함하고,상기 제1 분류 모델로는, 상기 촬영된 객체 영상을 입력으로 하여 1차 영상 분류 결과를 출력하는 제1 인공지능학습 모델이 활용되고,상기 제2 분류 모델로는, 상기 촬영된 객체 영상의 촬영 장소 및 촬영 시기 중 적어도 하나와 관련된 환경 특징정보를 추출하는 환경 특징 추출 모델이 활용되고,상기 영상 분류 통합 모델로는, 상기 1차 영상 분류 결과 및 상기 환경 특징 정보를 입력으로 하여 최종 영상분류 결과를 출력하는 머신 러닝 모델로서, 상기 제1 인공지능 학습 모델과 상이한 제2 인공지능 학습 모델이활용되는 것을 특징으로 하는, 객체 영상 분류 방법."}
{"patent_id": "10-2020-0130819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 분류 모델로서 제1 인공지능 학습 모델은, 메타 학습(meta learning)에 의해 훈련된 퓨삿 학습 모델(Few shot learning model) 또는 연합 학습 모델(Federated learning model)이 활용되는 것을 특징으로 하는,객체 영상 분류 방법."}
{"patent_id": "10-2020-0130819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 제2 분류 모델로서 환경 특징 추출 모델은, 상기 객체 영상의 촬영 장소에 따른 위치 정보, 상기 객체 영상의 촬영 시기에 따른 일시 정보, 날씨 정보, 기온 정보, 수온 정보 중 적어도 하나의 환경 파라미터를 반영하였을 때 해당 객체의 출현 분포 특징에 관한 추출 결과를 출력하는 것을 특징으로 하는, 객체 영상 분류 방법."}
{"patent_id": "10-2020-0130819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 객체 영상의 분류 서비스를 제공하는 플랫폼인 서비스 서버, 상기 객체 영상 분류 서비스를 제공받는 서비스 이용자 단말, 상기 서비스 서버로부터 제공된 데이터에 근거하여 상기 객체 영상의 분류를 위한 인공지능 학습 모델의 학습 및 업데이트를 수행하는 분석 서버를 포함하는 서비스 시스템 구조에서, 상기 제1 분류 모델, 상기 제2 분류 모델, 상기 영상 분류 통합 모델에 따른 처리 과정이 상기 분석 서버 또는상기 서비스 이용자 단말에 설치된 서비스 앱에 의해 단독 실행되거나, 또는 상기 분석 서버 및 상기 서비스 앱의 연계 실행되는 것을 특징으로 하는, 객체 영상 분류 방법.공개특허 10-2022-0048085-3-청구항 5 제4항에 있어서,상기 제1 분류 모델로서 연합 학습 모델(Federated learning model)이 활용되는 경우, 상기 제1 분류 모델에 의한 1차 영상 분류는 상기 서비스 이용자 단말에 설치된 서비스 앱에 의해 실행되고,상기 1차 영상 분류 결과에 따른 메타 데이터는 상기 서비스 서버와 연동하는 데이터베이스(DB) 서버로 전송되어 저장되며, 상기 분석 서버는 상기 DB 서버에 누적된 메타 데이터를 이용하여 제1 분류 모델의 학습을 수행하고 업데이트된 분류 모델을 상기 서비스 서버를 통해서 상기 서비스 이용자에게 배포하는 것을 특징으로 하는,객체 영상 분류 방법."}
{"patent_id": "10-2020-0130819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,상기 객체 영상이 물고기 사진인 경우,상기 제2 분류 모델로서 환경 특징 추출 모델은, 해양수산부 API를 통해서 획득되는 위치 연관의 어종 출현 정보, 기상청 API를 통해서 획득되는 기온 또는 수온 연관의 어종 출현 정보, 상기 DB 서버에 구축된 어종 특성구축 정보로부터 추론되는 어종 출현 분포 특징 정보를 추출 결과로서 출력하는 것을 특징으로 하는, 객체 영상분류 방법."}
{"patent_id": "10-2020-0130819", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 실시예에 의하면, 어류의 식별, 탐지, 분류 과정에서 어류 영상 데이터의 부족 문제 및 분류 정확도 문제를 해결할 수 있는 방법으로서, 머신 러닝 기반의 학습 및 위치/환경 데이터를 함께 사용하고, 어류 분류 서 비스 이용 과정에서 사용자의 개인정보 보호가 가능한 객체 영상 분류 모델이 제공된다. 본 발명의 실시예에 따 른 객체 영상 분류 모델은 어류뿐만 아니라 다양한 식물, 동물의 식별, 탐지, 분류 과정에서도 적용될 수 있다."}
{"patent_id": "10-2020-0130819", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 객체 영상 분류 방법에 관한 것으로서, 머신 러닝 기반 및 위치/환경 정보 기반의 객체 영상 분류 방 법에 관한 것이다."}
{"patent_id": "10-2020-0130819", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "낚시를 하는 인구수가 급증함에 따라 낚시 관련 서비스의 수요가 증가하고 있다. 그중 많은 수의 서비스 이용자 는 다양한 종류의 어류로 인해 낚시로 포획한 물고기가 어떤 종류의 물고기인지 모르며, 어떻게 조리하는지 등 에 대한 방법을 알지 못한다. 이와 관련하여, 어류 자체를 분류하는 분류 데이터는 데이터베이스화되어 존재하나, 사용자가 낚시로 낚은 물고 기를 촬영한 경우 그 촬영한 물고기를 식별 및 분류할 수 있는 적절한 방법이 제안되지 못하고 있다. 종래 기술로서, 한국공개특허 제10-2016-0119997호에 따르면, 영상 처리 장치가 물고기 이미지를 포함하는 소스 영상을 입력받는 단계, 상기 영상 처리 장치가 상기 물고기 이미지에서 분석 영역을 결정하는 단계 및 상기 영 상 처리 장치가 상기 분석 영역의 색상 특징, 질감 특징 및 모양 특징 중 적어도 하나를 상기 물고기 이미지의 특징 정보로 추출하는 단계를 포함하는 물고기 영상 특징 정보 추출 방법이 제안되고 있다. 또한 한국공개특허 제10-2018-0032488호에 따르면, 낚시와 같은 어로행위를 통해 수득되는 어류의 종류 및 어류의 길이를 측정할 수 있는 이미지를 이용한 어류 종류 및 체장 측정 방법이 제안되고 있다. 그러나 상술한 종래 기술에 의할 때에도, 어류 영상 데이터의 부족의 문제를 해결할 수 있는 방법과 높은 정확 도의 분류 결과를 서비스로 제공하기 위한 방법까지는 구체적으로 제안되지 못하고 있는 실정이다."}
{"patent_id": "10-2020-0130819", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 어류의 식별, 탐지, 분류 과정에서 어류 영상 데이터의 부족 문제 및 분류 정확도 문제를 해결할 수 있는 방법으로서, 인공지능 기반의 학습 및 위치/환경 데이터를 함께 사용하고, 어류 분류 서비스 이용 과정에 서 사용자의 개인정보 보호가 가능한 신규의 객체 영상 분류 모델을 제공하고자 한다."}
{"patent_id": "10-2020-0130819", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 측면에 따르면, 촬영된 객체 영상에 기초하여 객체의 종류를 식별 및 분류하기 위한, 객체 영상 분류에 관한 컴퓨터 구현 방법(Computer implemented method)은, 객체 영상 분류를 요청한 서비스 이용자로부터, 소정 객체가 촬영된 객체 영상을 수신하는 단계; 및 제1 분류 모델에 따른 출력 결과 및 제2 분 류 모델에 따른 출력 결과를 영상 분류 통합 모델의 입력으로 하여, 상기 영상 분류 통합 모델에 따른 출력 결 과인 영상 분류 결과를 획득하는 단계;를 포함한다. 이때, 상기 제1 분류 모델로는 상기 촬영된 객체 영상을 입력으로 하여 1차 영상 분류 결과를 출력하는 제1 인 공지능 학습 모델이 활용되고, 상기 제2 분류 모델로는 상기 촬영된 객체 영상의 촬영 장소 및 촬영 시기 중 적 어도 하나와 관련된 환경 특징 정보를 추출하는 환경 특징 추출 모델이 활용된다. 여기서, 상기 제1 분류 모델로서 제1 인공지능 학습 모델은, 메타 학습(Meta learning)에 의해 훈련된 퓨삿 학 습 모델(Few shot learning model) 또는 연합 학습 모델(Federated learning model)이 활용될 수 있다. 여기서, 상기 제2 분류 모델로서 환경 특징 추출 모델은, 상기 객체 영상의 촬영 장소에 따른 위치 정보, 상기 객체 영상의 촬영 시기에 따른 일시 정보, 날씨 정보, 기온 정보, 수온 정보 중 적어도 하나의 환경 파라미터를 반영하였을 때 해당 객체의 출현 분포 특징에 관한 추출 결과를 출력할 수 있다. 이때, 상기 객체 영상이 물고 기 사진인 경우, 상기 제2 분류 모델로서 환경 특징 추출 모델은, 해양수산부 API를 통해서 획득되는 위치 연관 의 어종 출현 정보, 기상청 API를 통해서 획득되는 기온 또는 수온 연관의 어종 출현 정보, 상기 DB 서버에 구 축된 어종 특성 구축 정보로부터 추론되는 어종 출현 분포 특징 정보를 추출 결과로서 출력할 수 있다. 또한, 상기 영상 분류 통합 모델로는, 상기 1차 영상 분류 결과 및 상기 환경 특징 정보를 입력으로 하여 최종 영상 분류 결과를 출력하는 인공지능 학습 모델로서, 상기 제1 인공지능 학습 모델과 상이한 제2 인공지능 학습 모델이 활용된다. 여기서, 상기 영상 분류 통합 모델로는 얕은 신경망 모델(Shallow neural network model)이 활용될 수 있다."}
{"patent_id": "10-2020-0130819", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예에 의하면, 어류의 식별, 탐지, 분류 과정에서 어류 영상 데이터의 부족 문제 및 분류 정확도 문제를 해결할 수 있는 방법으로서, 머신 러닝 기반의 학습 및 위치/환경 데이터를 함께 사용하고, 어류 분류 서비스 이용 과정에서 사용자의 개인정보 보호가 가능한 객체 영상 분류 모델이 제공될 수 있는 효과가 있다. 또한 본 발명의 실시예에 따른 객체 영상 분류 모델은 어류뿐만 아니라 다양한 식물, 동물의 식별, 탐지, 분류 과정에서도 적용될 수 있는 효과가 있다."}
{"patent_id": "10-2020-0130819", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변환을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변환, 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 본 발명을 설명함에 있어서, 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 본 명세서의 설명 과정에서 이용되는 숫자(예를 들어, 제1, 제2 등)는 하나의 구성요소를 다른 구성요소와 구분하기 위한 식별기호에 불과하다. 또한, 명세서 전체에서, 일 구성요소가 다른 구성요소와 \"연결된다\" 거나 \"접속된다\" 등으로 언급된 때에는, 상 기 일 구성요소가 상기 다른 구성요소와 직접 연결되거나 또는 직접 접속될 수도 있지만, 특별히 반대되는 기재가 존재하지 않는 이상, 중간에 또 다른 구성요소를 매개하여 연결되거나 또는 접속될 수도 있다고 이해되어야 할 것이다. 또한, 명세서 전체에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또 한 본 명세서에서 인공지능 학습 모델이란 머신 러닝 및 딥 러닝을 모두 포괄하는 개념 용어로서 사용한다. 본 발명은, 촬영된 객체 영상에 기초하여 객체의 종류를 식별 및 분류하기 위한, 객체 영상 분류에 관한 컴퓨터 구현 방법(Computer implemented method)으로서, 객체 영상 분류를 요청한 서비스 이용자로부터, 소정 객체가 촬영된 객체 영상을 수신하는 단계; 및 제1 분류 모델에 따른 출력 결과 및 제2 분류 모델에 따른 출력 결과를 영상 분류 통합 모델의 입력으로 하여, 상기 영상 분류 통합 모델에 따른 출력 결과인 영상 분류 결과를 획득하 는 단계;를 포함하는 객체 영상 분류 방법에 관한 것이다. 이때, 상기 제1 분류 모델로는 상기 촬영된 객체 영상을 입력으로 하여 1차 영상 분류 결과를 출력하는 제1 인 공지능 학습 모델이 활용되고, 상기 제2 분류 모델로는 상기 촬영된 객체 영상의 촬영 장소 및 촬영 시기 중 적 어도 하나와 관련된 환경 특징 정보를 추출하는 환경 특징 추출 모델이 활용된다. 또한, 상기 영상 분류 통합 모델로는, 상기 1차 영상 분류 결과 및 상기 환경 특징 정보를 입력으로 하여 최종 영상 분류 결과를 출력하는 머신 러닝 모델로서, 상기 제1 인공지능 학습 모델과 상이한 제2 인공지능 학습 모델이 활용된다. 상술한 바와 같은 본 발명에 따른 객체 영상 분류 방법은, 그 전체 과정이, 객체 영상 분류를 위한 분석 목적인 분석 서버에 의해 실행될 수도 있고, 동일 목적의 서비스를 제공하기 위한 애플리케이션 프로그램(예를 들어, 사용자(후술할 서비스 이용자) 단말에 설치되는 모바일 앱(App))에 의해 실행될 수도 있다. 또한, 일부 분류 과 정은 모바일 앱에서 실행되고 그 외 분류 과정은 분석 서버에 의해 실행될 수도 있다. 즉, 상기 제1 분류 모델, 제2 분류 모델, 영상 분류 통합 모델에 의한 예측(prediction) 과정 전체가 분석 서버 또는 모바일 앱에서 실행 될 수도 있지만, 제1 분류 모델 및 제2 분류 모델 중 적어도 하나가 모바일 앱에서 실행되고 영상 분류 통합 모 델을 포함하는 그 외의 분류 과정은 분석 서버에 의해서 실행될 수도 있는 것이다. 본 발명에 따른 객체 영상 분류 방법은, 특별한 제한 없이 다양한 객체 영상의 객체 식별, 탐지, 분류에 활용될 수 있다. 다만, 본 명세서에서는 발명 설명의 편의 및 집중을 위해, 사용자가 촬영한 물고기 사진으로부터 해당 물고기의 어종을 분류/판별하는 케이스에 본 발명이 적용되는 경우를 주로 설명하기로 한다. 이하, 첨부된 도면들을 참조하여 본 발명의 실시예를 상세히 설명한다. 여기서, 도 1은 본 발명의 객체 영상 분 류 방법이 실행되는 일 실시예의 서비스 플로우를 도시한 도면이고, 도 2는 본 발명의 일 실시예의 객체 영상 분류 방법에 관한 분석 모델을 설명하기 위한 도면이며, 도 3은 본 발명의 객체 영상 분류 방법이 실행되는 다 른 실시예의 서비스 플로우를 도시한 도면이고, 도 4는 본 발명의 다른 실시예의 객체 영상 분류 방법에 관한 분석 모델을 설명하기 위한 도면이다. 먼저, 도 1 및 도 2를 참조하여, 본 발명의 제1 실시예를 설명하면 다음과 같다. 도 1에서, 서비스 서버는 객체 영상 분류 서비스를 제공하는 서비스 플랫폼에 해당한다. 또한, 분석 서버는 객 체 영상 분류에 따른 실제 분석을 수행하며, 객체 영상 분류를 위한 분류 모델을 검증/업데이트하는 역할을 수 행한다. 도 1의 경우, 서비스 이용자 단말로부터 객체 영상에 관한 분류 요청이 수신되면[S10], 서비스 서버가 분석 서 버로 객체 영상 관련 메타 데이터를 전송하고[S11], 이에 따라 분석 서버에 의해 분석된 객체 영상 분류 결과 (객체 식별/분류 결과)를 서비스 서버를 통해서 서비스 이용자 단말로 전송[S12, S13]하는 서비스 플로우가 예 시되고 있다. 즉, 도 1은 분석 서버에 의해 객체 영상 분류에 관한 모든 분석 과정(후술할 도 2의 분석 과정)이 실행되는 케이스가 예시되고 있다. 다만, 앞서도 설명한 바와 같이, 서비스 설계 방식에 따라, 서비스 이용자 단말에 설치되는 서비스 앱을 통한 분석 또는 해당 서비스 앱과 분석 서버가 연계되는 방식의 분석도 가능할 수 있음은 물론이다. 이와 같은 경우, 분석 서버에 의해 객체 영상 분류 모델이 업데이트되는 경우, 서비스 서버를 통해서 그 업데이트된 분류 모델은 서비스 앱에도 반영되게 된다. [Few Shot Learning과 GPS(위치) 기반의 어종 탐지와 분류 (도 2)] 도 2의 경우, 제1 분류 모델로서, 퓨샷 학습 모델(Few shot learning model)이 활용된다. Few shot learning은 딥러닝 모델 중에서도 소량의 데이터를 이용하여 객체를 인식(분류)하는 문제를 풀도록 학습된 모델로서, N-way K-shot 문제(여기서, N은 클래스(분류 범주)의 수, K는 클래스별 서포트 데이터(support data)의 수)로 표현될수 있다. 따라서, Few shot learning의 성능은 클래스의 수인 N과는 반비례 관계를 갖고 서포트 데이터의 수인 K와는 비례 관계를 갖는다. 이때, Few shot learning이 기존의 학습 과정에 사용된 데이터 셋(즉, 서포트 데이터(support data) 및 이의 검증 테스트를 위한 쿼리 데이터(query data)) 이외의 완전히 새롭게 주어진 데이터에서도 잘 작동하도록 만들 기 위해서는 메타 학습(Meta learning)에 의해 훈련이 필요하다. 이에 따라 Few shot learning에서는 에피소딕 훈련(Episodic training) 방식으로 Meta learning을 실행한다. 이와 같은 Episodic training을 통해서 모델 스 스로 학습 규칙을 도출할 수 있게 함으로써 일반화 성능을 높일 수 있게 된다. 이와 같은 학습(훈련)은 그 학습 솔루션으로서 메트릭(metric) 기반의 메타 학습, 그래프 신경망(graph neural network) 기반의 메타 학습, 그래 디언트(gradient) 최적화 기반의 메타 학습 등과 같은 다양한 방식이 이용될 수 있는데, 최종적으로는 예측(분 류) 손실(loss)를 줄일 수 있도록 하는(즉, 분류 결과(지표)를 가장 좋게 만들어 주는) 손실 함수의 매개 변수 값(가중치)을 탐색하는 것이다. 이를 위해, Few shot learning을 위해 어종 별 10~20장의 사진을 모델에 등록하고, 가지고 있는 데이터에 기반 하여 어종을 구별하기 위해 Meta learning을 진행할 수 있다. 이때, Meta learning에서는 국내에서 자주 등장하 는 어종(일 예로, 약 23개의 어종)을 포함하여 다수 어종(일 예로, 1000여개의 어종)의 약 2만장 내외의 사진을 학습하도록 함으로써, 메타 학습에 의해 훈련된 퓨삿 학습 모델을 구축할 수 있다. 본 발명의 일 실시예에서는 상술한 바와 같이 구축된 Few shot learning 모델을 제1 분류 모델로서 이용하여 서 비스 이용자가 촬영한 물고기 사진이 어떤 어종의 물고기일지에 관한 1차 영상 분류 결과를 획득한다. 이때, 1 차 영상 분류 결과는 표현 벡터(representation vector)로 생성될 수 있으며, 분류 클래스에 따라 representation vector의 크기는 조정될 수 있다. 예를 들어, 1000여종의 어종을 대상으로 할 경우 500 ~ 1000 차원의 벡터(즉, 500 ~ 1000개의 파라미터(매개 변수)에 따라 표현되는 특징 벡터(feature vector))가 생성될 수 있다. 이와 같은 표현 벡터가 생성되면, 해당 벡터가 각각의 분류 클래스 중 어떤 분류 클래스에 속할 가능 성이 높은지에 관한 분류 확률이 도출될 수 있게 된다. 그러나 Few shot learning 모델은 어종 별 10~20장의 사진을 모델에 등록하여 이용하는 것이므로, 그 분류 정확 도가 떨어질 수 있다. 따라서, 본 발명에서는 이를 보완하기 위해, 촬영된 객체 영상의 촬영 장소 및 촬영 시기 중 적어도 하나와 관련된 환경 특징 정보(지역/시기/날씨 등과 같은 환경 상황 별 어종 분포/출현 특성)를 추출 하는 환경 특징 추출 모델을 제2 분류 모델로서 추가 활용한다. 이하, 제2 분류 모델은 설명상 편의를 위해 GPS 기반 추론 모델이라 명명하기로 한다. GPS 기반 추론 모델은, 지역 및 기간별 GPS 위치 기준의 물고기 출현 빈도를 이용한다. 해당 출현 빈도 데이터 는 예를 들어, 해양수산부의 API 또는/및 자체 구축 DB의 활용을 통해서 획득할 수 있다. 이에 따라, GPS 기반 추론 모델은, 물고기 사진의 촬영 장소에 따른 위치 정보, 물고기 사진의 촬영 시기에 따른 일시 정보, 날씨 정 보, 기온 정보, 수온 정보 중 적어도 하나의 환경 파라미터를 반영하였을 때 해당 객체인 물고기의 출현 분포 특징에 관한 추출 결과를 출력한다. 이때, 어종 특성 구축을 위한 DB 구축 정보로는, 어종 식별정보(속, 계명, 계수형질, 목, 문, 학명, 어, 계통, 생물종, 몸체길이, 생명주기 등), 어종 분포정보(분포지역, 평태, 채집 해역, 군락, 분포지역정보, 서식지, 서 식지위도, 서식지경도, 원산지 등), 어종 특징 정보(몸체길이, 생명주기, 산란기 등) 등이 포함될 수 있다. 상술한 제1 분석 모델에 따른 1차 영상 분류 결과(즉, 표현 벡터에 따른 어종별 분류 확률값) 및 상술한 제2 분 석 모델에 따른 GPS 기반의 어종 출현 분포 특징 정보는 영상 분류 통합 모델(도 2의 케이스에서는 Shallow neural network)의 입력 레이어(layer)로 입력되고, 이에 따라 영상 분류 통합 모델에 따른 출력 결과인 최종 분류 결과가 획득된다. 이때, 실시에에 따라, Shallow neural network의 최종 어종 확률값을 최종 출력값으로 하며, 가장 확률이 높은 3개의 어종의 사진과 확률이 서비스 이용자에게 제공(반환)될 수 있다. 도 2의 경우, 영상 분류 통합 모델로서 Shallow neural network가 활용되는 경우를 예시하고 있지만, 이외에도 다양한 딥 러닝 모델 또는 SVM(Support vector machine), Light gradient boosting machine의 앙상블 모델, Random forest 등과 같은 다양한 머신 러닝 모델이 대체 활용될 수 있음은 물론이다(도 4도 동일함). 상술한 바와 같은 방법에 의하면, 인공지능 기반의 학습 모델 및 위치/환경 데이터에 기반한 어종 출현 분포 특 징 정보를 함께 이용함으로써, 어류(어종)의 식별, 탐지, 분류 과정에서 어류 영상 데이터의 부족 문제 및 분류 정확도 문제를 해결할 수 있다. 또한 상술한 방법에 의하면, 서비스 서버는, 분류 정확도가 향상된 어종 분류 결과에 기초하여, 해당 어종의 추 천 요리, 조리 방법 등과 같은 다양한 정보를 서비스 이용자에게 제공해 줄 수 있다. 다음으로, 도 3 및 도 4를 참조하여, 본 발명의 제2 실시예를 설명하면 다음과 같다. 도 3의 경우, 서비스 이용자 단말의 서비스 앱으로부터 객체 영상에 관한 1차 영상 분류 결과가 획득되고 이에 관한 정보가 서비스 서버로 전송되면[S20], 서비스 서버는 분석 서버로 객체 영상 관련 메타 데이터를 전송하고 [S22], 이에 따라 분석 서버에 의해 분석된 객체 영상 분류 결과(객체 식별/분류 결과)를 서비스 서버를 통해서 서비스 이용자 단말로 전송[S23, S24]하는 서비스 플로우가 예시되고 있다. 즉, 도 3은 서비스 앱과 분석 서버 가 연계하여 객체 영상 분류를 실행하는 케이스가 예시되고 있다. 이때, 서비스 앱으로부터 획득된 1차 영상 분 류 결과에 관한 정보는 DB 서버로도 전송되어 저장되며[S21], 이러한 정보는 향후 분석 서버에 의해 객체 영상 분류를 위한 분류 모델을 검증/업데이트하는데 활용된다. 이와 같이 분석 서버에 의해 업데이트된 분류 모델은 서비스 서버를 통해서 서비스 이용자에게 배포된다. 이하, 도 4를 참조하여 본 발명의 제2 실시예를 설명하되, 설명의 편의 및 집중을 위해 앞선 제1 실시예(도 2) 의 설명에서와 중복될 수 있는 부분에 관한 설명은 생략하기로 한다. [Federated learning과 GPS(위치) 기반의 어종 탐지와 분류 (도 4)] 앞서 설명한 도 2에서는 제1 분류 모델로서 Few shot learning 모델을 사용한 반면, 도 4에서는 제1 분류 모델 로서 연합 학습 모델(Federated learning model)을 사용하고 있다. Federated learning은 사용자 측의 디바이스에서 학습을 하게 한 다음, 학습 정보를 취합해서 모델을 업데이트 하는 방식을 채용하므로, 사용자 측 디바이스에 저장된 데이터를 외부로 전송할 필요가 없어 개인정보 보호에 이점이 있다. 따라서, 본 발명의 제2 실시예에서는 개인정보(사용자의 사진, 위치 등) 보호와 학습 데이터 부족 을 해결하기 위해 Federated learning 모델을 제1 분류 모델로서 사용한다. 즉, 본 발명의 제2 실시예는, 사용 자가 서버로 자신이 촬영한 사진을 업로드하는 것과 자신의 GPS 위치 정보를 제공하는 것을 원하지 않는 경우에 활용되기 적합하다. 이에 따라, 도 4에서, 제1 분류 모델(즉, Federated learning model)에 의한 1차 영상 분류는 서비스 이용자 단말에 다운로드 설치된 서비스 앱에 의해 실행된다. 이를 위해, 서비스 이용자는, 자신의 단말기에 Federated learning model에 따른 어종 분류 모델을 앱으로 다운 로드 받은 후, 그 다운로드된 Federated learning model을 통해서 추론된 물고기 사진에 관한 1차 영상 분류 결 과를 서비스 앱의 화면을 통해 확인할 수 있다. 이때, 서비스 이용자가 어떤 물고기인지 아는 경우라면, 어떤 물고기인지를 선택하여 등록할 수도 있다. 즉, 제1 분류 모델의 분류 결과(예측)가 서비스 앱 화면에 보여졌을 때, 서비스 이용자가 어종 선택(정답 선택)을 하여 등록할 수도 있다. 상술한 바와 같이 획득된 1차 영상 분류 결과에 관한 메타 데이터(예를 들어, 어종별 분류 확률값, 손실값, Federated learning을 통해 학습된 매개 변수 가중치(weights), 사용자 선택값 등)이 DB 서버로 저장될 수 있다. 이때, 사용자 동의가 있는 경우, GPS 위치 정보와 등록 시간도 DB 서버로 저장될 수 있다. 이와 같은 메타 데이터는 향후 분석 서버를 통한 분류 모델 학습 및 업데이트에 활용된다. DB 서버에 저장된 weights는 일정 이상 데이터가 누적되면 일괄 학습되며, 학습 및 검증을 통해 정확도가 향상된 분류 모델은 서 비스 서버를 통해 다시 서비스 이용자에게 배포된다. 본 발명의 제2 실시예에서도, 앞서 설명한 제1 실시예에서와 유사하게, 기상청 API를 통해 획득되는 기상 상황, 기온, 수온 등과 같은 외부 환경 요인에 따른 정보와, 자체 구축 DB를 통해 획득되는 어종별 특성 정보에 기반 하여 추출되는 물고기 출현 분포 특징 정보를 출력하는 제2 분류 모델(GPS 기반 추론 모델)이 함께 이용된다. 이때, 서비스 이용자가 GPS 위치 정보의 외부 제공에 동의하지 않는 케이스라면 제2 분류 모델에 의한 특징 추 출도 서비스 이용자 단말의 서비스 앱에서 처리하게 될 것이나, GPS 위치 정보의 외부 제공에 동의하는 케이스 라면 제2 분류 모델에 의한 특징 추출은 서비스 앱이 아닌 분석 서버에서 처리하여도 무방하다. 이후, 제1 분석 모델인 Federated learning model에 따른 1차 영상 분류 결과(즉, 어종별 분류 확률값) 및 제2 분석 모델에 따른 GPS 기반의 어종 출현 분포 특징 정보는 영상 분류 통합 모델(도 4의 케이스에서는 Shallow neural network)의 입력 레이어(layer)로 입력되고, 이에 따라 영상 분류 통합 모델에 따른 출력 결과인 최종 분류 결과가 획득된다. 이때, 실시에에 따라, Shallow neural network의 최종 어종 확률값을 최종 출력값으로 하며, 가장 확률이 높은 3개의 어종의 사진과 확률이 서비스 이용자에게 제공(반환)될 수 있음은 앞서도 설명한바이다. 상술한 바와 같은 방법에 의하면, 어류(어종)의 식별, 탐지, 분류 과정에서, 어류 분류 서비스 이용자의 개인정 보 보호가 가능한 객체 영상 분류 모델이 제공될 수 있는 효과가 있다. 이상에서는 본 발명의 실시예를 참조하여 설명하였지만, 해당 기술 분야에서 통상의 지식을 가진 자라면 하기의 특허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수 정 및 변경시킬 수 있음을 쉽게 이해할 수 있을 것이다."}
{"patent_id": "10-2020-0130819", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 객체 영상 분류 방법이 실행되는 일 실시예의 서비스 플로우를 도시한 도면. 도 2는 본 발명의 일 실시예의 객체 영상 분류 방법에 관한 분석 모델을 설명하기 위한 도면. 도 3은 본 발명의 객체 영상 분류 방법이 실행되는 다른 실시예의 서비스 플로우를 도시한 도면. 도 4는 본 발명의 다른 실시예의 객체 영상 분류 방법에 관한 분석 모델을 설명하기 위한 도면."}
