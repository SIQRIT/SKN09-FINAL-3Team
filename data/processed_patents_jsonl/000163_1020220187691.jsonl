{"patent_id": "10-2022-0187691", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0104947", "출원번호": "10-2022-0187691", "발명의 명칭": "이기종 프로세서 기반 인공지능 모델 추론 장치 및 방법", "출원인": "(주)네오와인", "발명자": "이효승"}}
{"patent_id": "10-2022-0187691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "이기종 프로세스 기반 인공지능 모델 추론 장치에 있어서,학습 완료된 ONNX 모델로부터 오퍼레이터 정보, 가중치 정보 및 속성 정보를 추출하여 상기 ONNX 모델을 적어도3개 이상의 이기종 프로세서들로 구성된 추론 환경에 적용 가능한 CONNX 모델로 변환하는 모델 변환부와,상기 모델 변환부에서 출력된 CONNX 모델을 파싱하여 상기 이기종 프로세서를 구성하는 특정 프로세서별로 워크로드(workload)를 분배하고 추론을 위한 입력 데이터를 입력받아 추론을 수행하는 추론부를 포함하는 인공지능모델 추론장치."}
{"patent_id": "10-2022-0187691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제2항에 있어서,상기 추론부는 상기 CONNX 모델로부터 오퍼레이터(Operator) 정보를 읽어 CPU 오퍼레이터, FPGA 기반 PIM 오퍼레이터 및 FPGA 기반 NPU 오퍼레이터를 구분하고, 오퍼레이터의 가중치 정보를 메모리에 쓰기(write)하여 추론준비를 수행하는 모델 파싱부와,상기 PIM 또는 NPU 연산을 위한 FPGA의 메모리 영역을 입력 데이터 영역, 가중치 데이터 영역 및 출력 데이터영역으로 구분하고 연산을 위한 입력 데이터, 가중치 데이터 및 출력 데이터를 벡터 형태로 변환하여 병렬로 처리하는 모델 실행부를 포함하는 것을 특징으로 하는 인공지능 모델 추론 장치."}
{"patent_id": "10-2022-0187691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "프로세서에 의해 수행되는 이기종 프로세서 기반 인공지능 모델 추론 방법에 있어서,학습 완료된 ONNX 모델을 CONNX 파일, 텍스트 파일 및 데이터 파일로 구성된 CONNX 모델로 변환하는 단계와, 상기 CONNX 파일을 읽어 그래프에 포함된 각 오퍼레이터(Operator)를 적어도 3개 이상의 이기종 프로세서를 통해 병렬적으로 계산하여 추론을 수행하는 단계를 포함하는 인공지능 모델 추론 방법."}
{"patent_id": "10-2022-0187691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 추론을 수행하는 단계는 상기 CONNX 파일의 메타데이터를 읽어 버전 정보 및 그래프 개수 정보를 확인하는단계와,상기 확인한 그래프 개수에 따라 상기 n개의 텍스트 파일을 읽어 그래프 내부의 메모리 설정 정보 및 오퍼레이터 구성 정보를 확인하는 단계와, 각 그래프의 가중치 정보를 포함하는 데이터 파일의 리스트를 확인하는 단계와,추론을 위한 입력 데이터를 입력받아 메모리에 할당하는 단계와,상기 데이터 파일의 리스트에서 각 항목의 바이너리 데이터를 읽어 그래프의 가중치를 초기화하는 단계와, 그래프의 각 오퍼레이터를 특정 프로세서를 통해 연산 수행하는 단계를 포함하는 것을 특징으로 하는 인공지능모델 추론 방법."}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공지능 모델 추론 기술에 관한 것으로서, 상세하게는 기존 딥 러닝 프레임워크(Deep-Learning Framework)로 학습된 인공지능(AI) 모델이 FPGA 기반 이기종 프로세서 환경에서 효율적인 연산을 수행할 수 있도 록 한 이기종 프로세서 기반 인공지능 모델 추론 장치 및 방법에 관한 것이다. 이를 위해 본 발명에 따른 인공지 능 모델 추론 장치는 이기종 프로세스 기반 인공지능 모델 추론 장치로서,학습 완료된 ONNX 모델로부터 오퍼레이 터 정보, 가중치 정보 및 속성 정보를 추출하여 상기 ONNX 모델을 적어도 3개 이상의 이기종 프로세서들로 구성 된 추론 환경에 적용 가능한 CONNX 모델로 변환하는 모델 변환부와, 상기 모델 변환부에서 출력된 CONNX 모델을 파싱하여 상기 이기종 프로세서를 구성하는 특정 프로세서별로 워크로드(workload)를 분배하고 추론을 위한 입력 데이터를 입력받아 추론을 수행하는 추론부를 포함한다."}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 모델 추론 기술에 관한 것으로서, 상세하게는 기존 딥 러닝 프레임워크(Deep-Learning Framework)로 학습된 인공지능(AI) 모델이 FPGA 기반 이기종 프로세서 환경에서 효율적인 연산을 수행할 수 있도록 한 이기종 프로세서 기반 인공지능 모델 추론 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "딥 러닝 사용을 위한 소프트웨어(S/W) 기술은 크게 학습용 S/W와 추론용 S/W로 나뉘며, AI 모델 연산을 위한 프 로세서 GPU 또는 CPU 환경 중 하나의 프로세서를 활용하여 AI 모델을 추론한다. 최근에는 NPU 또는 PIM 등 AI 모델 연산 전용 프로세서가 등장하고 있으며, 추론에 사용되는 프로세서 또한 한 가지의 프로세서가 아닌 서로 다른 이기종 프로세서를 다중으로 구성하여 추론하는 연구가 진행되고 있다. NPU는 GPU와 다르게 AI 모델의 추론 연산에 최적화되어 설계된 H/W로 연산속도는 GPU보다 매우 빠르지만 GPU처 럼 모든 AI모델을 추론할 수 있는 구조가 아니고, 컨벌루션(Convolution) 등 특정 AI 모델 연산에만 최적화 되 어 있어서 설계 내용에 따라 실행할 수 있는 연산 범위가 제한적임 PIM(Processor-In-Memory)은 AI 모델 추론 연산 중 메모리 접근이 자주 발생하는 GEMV(GEneral Matrix Vector multiplication) 또는 GEMM(GEneral Matrix to Matrix multiplication) 연산에 최적화 되어 설계되어 있으며, 대규모의 언어처리 AI 모델 등의 활용에 적합하다. 이러한 이기종 프로세서들을 다중으로 구성하여 AI 모델 추론 수행 시 각 프로세서에 연산 특징에 맞는 워크로 드를 전달 해줘야하므로 추론 S/W에서 이를 잘 분배할 수 있는 기술이 필요하다. 이에 따라 기존 딥 러닝 S/W(또는 프레임워크)를 활용하여 학습된 모델에 대하여 CPU, FPGA 기반 NPU, FPGA 기 반 PIM으로 구성된 이기종 프로세서 환경에서 AI 모델 추론 시 프로세서 특징별로 워크로드 분배 및 추론 연산 을 수행할 수 있는 방법에 대한 개발이 필요한 상황이다. 선행기술문헌 특허문헌 (특허문헌 0001) (선행기술 1) 한국공개특허공보 제10-2020-0006903호"}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기 상황을 감안하여 창안된 것으로서, 본 발명의 목적은 기존 인공지능 학습 프레임워크(ONNX)를 사용하여 획득한 모델이 CPU, FPGA 기반 NPU, 및 FPGA 기반 PIM으로 구성된 이기종 프로세서 환경에서 빠르게 인공지능 추론을 수행할 수 있도록 하는 것이다."}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "이를 위해 본 발명에 따른 인공지능 모델 추론 장치는 이기종 프로세스 기반 인공지능 모델 추론 장치로서,학습 완료된 ONNX 모델로부터 오퍼레이터 정보, 가중치 정보 및 속성 정보를 추출하여 상기 ONNX 모델을 적어도 3개 이상의 이기종 프로세서들로 구성된 추론 환경에 적용 가능한 CONNX 모델로 변환하는 모델 변환부와, 상기 모델 변환부에서 출력된 CONNX 모델을 파싱하여 상기 이기종 프로세서를 구성하는 특정 프로세서별로 워크로드 (workload)를 분배하고 추론을 위한 입력 데이터를 입력받아 추론을 수행하는 추론부를 포함한다. 상기 CONNX 모델은 CONNX 파일, 텍스트 파일 및 데이터 파일로 구성된 것을 특징으로 한다. 상기 CONNX 파일은 상기 ONNX 모델의 메타정보를 포함하고, 상기 텍스트 파일은 상기 ONNX 모델의 그래프 구조 정보 및 메모리 정보를 포함하고, 상기 데이터 파일은 상기 그래프 구조에서 각 오프레이터가 사용하는 가중치 정보를 바이너리 데이터 형식으로 포함하는 것을 특징으로 한다. 상기 추론부는 상기 CONNX 모델로부터 오퍼레이터(Operator) 정보를 읽어 CPU 오퍼레이터, FPGA 기반 PIM 오퍼 레이터 및 FPGA 기반 NPU 오퍼레이터를 구분하고, 오퍼레이터의 가중치 정보를 메모리에 쓰기(write)하여 추론 준비를 수행하는 모델 파싱부와, 상기 PIM 또는 NPU 연산을 위한 FPGA의 메모리 영역을 입력 데이터 영역, 가 중치 데이터 영역 및 출력 데이터 영역으로 구분하고 연산을 위한 입력 데이터, 가중치 데이터 및 출력 데이터를 벡터 형태로 변환하여 병렬로 처리하는 모델 실행부를 포함하는 것을 특징으로 한다. 또한 본 발명에 따른 인공지능 모델 추론 방법은 프로세서에 의해 수행되는 이기종 프로세서 기반 인공지능 모 델 추론 방법으로서. 학습 완료된 ONNX 모델을 CONNX 파일, 텍스트 파일 및 데이터 파일로 구성된 CONNX 모델로 변환하는 단계와, 상기 CONNX 파일을 읽어 그래프에 포함된 각 오퍼레이터(Operator)를 적어도 3개 이상의 이 기종 프로세서를 통해 병렬적으로 계산하여 추론을 수행하는 단계를 포함한다. 상기 추론을 수행하는 단계는 상기 CONNX 파일의 메타데이터를 읽어 버전 정보 및 그래프 개수 정보를 확인하는 단계와, 상기 확인한 그래프 개수에 따라 상기 n개의 텍스트 파일을 읽어 그래프 내부의 메모리 설정 정보 및 오퍼레이터 구성 정보를 확인하는 단계와, 각 그래프의 가중치 정보를 포함하는 데이터 파일의 리스트를 확인하 는 단계와, 추론을 위한 입력 데이터를 입력받아 메모리에 할당하는 단계와, 상기 데이터 파일의 리스트에서 각 항목의 바이너리 데이터를 읽어 그래프의 가중치를 초기화하는 단계와, 그래프의 각 오퍼레이터를 특정 프로세 서를 통해 연산 수행하는 단계를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, ONNX 모델을 CONNX 모델로 변환하고 이를 CONNX 엔진(Engine)을 통해 모델을 실행 및 추론함 으로써 CPU, FPGA 기반 NPU, 및 FPGA 기반 PIM으로 구성된 이기종 프로세서 환경에서 활용 가능하도록 하며, CPU, NPU, PIM을 동시에 활용하여 특정 AI 모델 추론 시 병렬 처리를 통해 기존 다른 딥러닝 프레임워크(Deep Learning Framework)보다 빠른 속도의 인공지능 모델 추론이 가능한 효과가 있다."}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명에 따른 실시예를 상세하게 설명한다. 본 발명의 구성 및 그에 따른 작용 효과는 이하의 상세한 설명을 통해 명확하게 이해될 것이다. 본 발명의 상세한 설명에 앞서, 동일한 구성요소에 대해서는 다른 도면상에 표시되더라도 가능한 동일한 부호로 표시하며, 공지된 구성에 대해서는 본 발명의 요지를 흐릴 수 있다고 판단되는 경우 구체적인 설명은 생략하기 로 함에 유의한다. ONNX(Open Neural Network Exchange)는 Tensorflow, PyTorch 등과 같은 서로 다른 DNN(Deep Neural Network) 프레임워크(framework) 환경에서 만들어진 모델들이 호환되도록 한 공유 플랫폼을 말한다. Tensorflow에서 모델 을 만들고 이를 ONNX 모델로 변환하면 이후에 PyTorch 등과 같은 다른 프레임워크에서도 Tensorflow에서 만든 모델을 사용할 수 있다. 그리고 CONNX는 C 언어로 구현한 ONNX를 말한다. 본 발명은 서로 다른 이기종 프로세서가 FPGA기반으로 구축된 환경에서 인공지능 모델 추론 시 연산 워크로드를 효율적으로 분배하여 연산을 수행함으로써 추론 성능을 향상시킬 수 있다. 즉, 본 발명의 실시예에서는 기존의 딥러닝 프레임워크(PyTorch, Tensorflow 등)로 학습된 ONNX 모델의 구조를 분석하여 PIM 또는 NPU에 최적화된 연산 워크로드(workload)를 이기종 프로세서를 구성하는 해당 프로세서로 분 배하여 추론 속도를 향상시킬 수 있다. 도 1은 본 발명에 따른 이기종 프로세서 기반 인공지능 모델 추론 장치의 개략적인 구성을 나타낸 것이다. 도 1을 참조하면, 본 발명에 따른 이기종 프로세서 기반 인공지능 모델 추론 장치는 ONNX 모델을 CONNX 모델로 변환하는 모델 변환부 및 CONNX 모델을 파싱하여 추론하는 추론부로 구성된다. CONNX 모델은 이기종 프 로세서를 구성하는 프로세서별로 워크로드를 분배하는데 적합하도록 C 언어로 구현된 ONNX 모델이다. 모델 변환부는 학습이 완료된 ONNX 모델을 입력받아 기정의된 ONNX의 프로토콜 버퍼(protocol buffer) 구조 에 따라 표 1과 같이 주요 정보를 파싱한다. 표 1"}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "표 1에 나타낸 ONNX 모델의 주요 정보 외에 버전과 관련한 메타데이터(metadata) 및 각 정보에 대한 설명 (human-readable information)이 포함되어 있다. 모델 변환부는 주요 정보 외에 불필요한 정보는 사용하지 않고 모델 추론에 필요한 주요 정보만을 사용하여 ONNX 모델을 CONNX 모델로 변환한다. 이때 ONNX 모델을 CONNX 모델 구조로 변환하는 세부 절차는 3가지의 파일 생성 프로세스를 포함한다. 1) CONNX 파일 생성 프로세스 CONNX 파일은 CONNX의 버전 정보, ONNX의 opset 버전 정보, 그래프(Graph)의 개수 정보를 포함한다. CONNX의 버전 정보는 ONNX 모델에서 획득한 정보가 아닌 별도로 지정된 버전 정보를 사용한다. opset 버전 정보는 학습 된 ONNX 모델의 opset 정보를 파싱하여 활용한다. 그래프의 개수 정보는 학습된 ONNX 모델의 GraphProto로부터 정보를 파싱하여 활용한다. CONNX 파일은 아래와 같이 지정된 포맷으로 작성된다."}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "2) TEXT 파일 생성 프로세스 TEXT 파일은 그래프의 구성 정보를 포함한다. 그래프의 구성 정보는 그래프 내의 오퍼레이터(Operator) 구성 정 보 및 메모리 설정 정보를 가진다. 오퍼레이터 구성 정보는 학습된 ONNX 모델에서 NodeProto로부터 정보를 파싱 하여 활용한다. TEXT 파일은 아래와 같은 지정된 포맷으로 작성된다."}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "3) DATA 파일 생성 프로세스 DATA 파일은 학습된 ONNX 모델의 가중치가 바이너리 형식으로 작성된 리스트를 포함한다. 리스트는 ONNX 모델에 서 TensorProto로부터 정보를 파싱하여 활용한다. DATA 파일은 별도의 정해진 포맷이 없다. 상술한 3가지의 파일 생성 프로세스를 통해, 도 2에 도시된 바와 같이, 모델 변환부는 ONNX 모델 (model.onnx)로부터 CONNX 파일(model.connx), n개의 텍스트 파일({n}.text), 텍스트 파일당 m개의 데이터 파 일({n}_{m}.data)을 생성한다. 추론부는 CONNX 모델을 파싱하고 PIM 또는 NPU에 적합한 연산들의 워크로드를 전달하여 추론을 위한 입력 데이터에 대한 연산을 수행한다. 추론부는 모델 변환부로부터 출력된 CONNX 모델의 파일을 이기종 프로 세서에서 추론하기 위해 모델 파싱부와 모델 실행부로 구성된다. 모델 파싱부는 CONNX 모델의 파일을 파싱한다. 모델 파싱부는 CONNX 파일을 읽어 버전 정보(CONNX의 버 전 정보 및 ONNX의 opset 버전 정보) 및 그래프 개수 정보를 출력한다. 또한 모델 파싱부는 텍스트 파일을 읽어 ONNX 그래프의 구조 정보를 출력하고, 데이터 파일을 읽어 ONNX 그 래프의 각 오퍼레이터 정보를 출력한다. 즉, 모델 파싱부는 ONNX 그래프 내의 오퍼레이터(Operator) 구성 정보 및 메모리 설정 정보를 추출하고, ONNX 그래프의 각 오퍼레이터에 의해 사용되는 가중치 정보가 바이너리 데이터로 구성된 리스트를 추출한다. 이에 따라 모델 파싱부는 CONNX 모델로부터 오퍼레이터(Operator) 정보를 읽어 CPU가 처리할 오퍼레이터, PIM이 처리할 오퍼레이터 및 NPU가 처리할 오퍼레이터를 구분하고, 오퍼레이터의 가중치 정보를 메모리에 쓰기 (write)하여 추론 준비를 수행하게 된다. 모델 실행부는 추론을 위한 입력 데이터를 입력받아 메모리에 할당하고, 리스트에서 바이너리 데이터를 읽 어 그래프의 가중치를 초기화한 후 그래프의 각 오퍼레이터를 수행한다. 모델 실행부는 PIM 또는 NPU 연산을 위한 FPGA의 메모리 영역을 입력 데이터 영역, 가중치 데이터 영역 및 출력 데이터 영역으로 구분하고 연산을 위한 데이터를 벡터 형태로 변환하여 처리한다. 모델 실행부(CONNX Executor)는 도 3에 도시된 구조로 설계된다. 모델 실행부는 CONNX 모델의 3가지 파 일의 파싱 결과에 근거해 모델 실행을 위한 가중치 초기화 기능 및 메모리 관리 기능을 수행한다. 구체적으로 모델 실행부에는 PIM 또는 NPU 환경에서 메모리 사용을 위해 시스템 콜 명령어 연동을 위한 HAL(hardware Abstraction Layer), CONNX 모델 실행 시 입출력 데이터 정의를 위한 Tensor, 그래프의 각 노드 (Node)에 대한 연산 시 오퍼레이터(Operator) 설정을 위한 Opset 등이 설계되어 있다. 도 3에서 CONNX Engine은 CONNX 모델을 파싱하기 위한 Parser를 더 포함하고 있는데, 이 경우 CONNX Engine은 파싱 및 추론을 수행하는 본 발명의 추론부에 해당한다. 도 4는 본 발명에 따른 이기종 프로세서 기반 인공지능 모델 추론 방법의 순서도를 나타내고, 도 5는 파싱 이후 의 실제 추론 과정을 나타낸 것이다. 도 4 및 도 5에 도시된 각 처리 단계는 본 발명에 따른 추론부에서 수 행된다. 도 4에서, 먼저 CONNX 모델이 로딩되면(S10), CONNX 파일을 파싱하여 버전 정보 및 그래프 개수 정보를 확인하 는 모델 파싱(Model Parsing) 단계(S20)를 수행한다. 다음, 그래프 개수 정보에 따라 텍스트 파일을 파싱하여 그래프 구조 정보를 확인하고, 그래프 구조 정보에 따 라 데이터 파일을 파싱하여 그래프의 각 오퍼레이터에 의해 사용되는 가중치 정보가 바이너리 데이터로 구성된 리스트를 확인하는 그래프 파싱(Graph parsing) 단계(S30)를 수행한다. 파싱이 완료되면 실제 추론 과정 (Run)(S40)이 수행된다 도 5에서, 실제 추론 과정(S40)은 추론을 위한 입력 데이터(Inference Data)가 입력되면, 입력 데이터를 메모리 에 할당하는 단계(Set Input), 리스트에서 바이너리 데이터를 읽어 그래프의 가중치를 초기화하는 단계 (Initialize value_info), 입력 데이터 및 가중치를 이용하여 그래프의 각 오퍼레이터를 수행하는 단계(Execute operators), 추론된 결과값을 출력하는 단계(Set Output), 그래프의 가중치를 리셋하는 단계(Clear value_info)를 포함한다. 이와 같이 도 4 및 도 5에서 상술한 CONNX 모델 추론 프로세스는 다음과 같이 정리될 수 있다."}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 5, "content": "도 6은 본 발명의 실시예에 따라 CONNX 모델로 변환하기 전 MNIST 데이터셋을 학습한 ONNX 모델의 구조를 시각 화한 것이고, 도 7은 도 6의 학습된 ONNX 모델을 CONNX 모델로 변환한 결과를 나타낸 것이다. 도 6 및 도 7을 참조하면, 도 6의 구조를 가진 ONNX 모델을 CONNX 모델로 변환했을 때, 파일 폴더 안에 1개의 CONNX 파일(model.ponnx), 1개의 텍스트 파일(0.text), 8개의 데이터 파일(0_1.data ~ 0_8.data)이 생성되어 있음을 알 수 있다. 도 8은 본 발명의 실시예에 따라 변환된 CONNX 모델의 text 파일의 출력 내용을 나타내고, 도 9는 본 발명의 실 시예에 따라 CONNX 모델을 추론한 결과를 나타낸 것이다. 도 9에 도시된 추론 결과는 MNIST 데이터셋이 학습된 CONNX 모델을 추론한 결과로서, 여기서 MNIST 데이터셋은 0~9로 이루어진 숫자 이미지 데이터셋을 말하며, 필기체 숫자 인식을 위한 데이터 셋이다. 이러한 CONNX 모델에 입력 데이터로 숫자 2에 해당되는 영상 데이터를 입력한 결과, 3번째 요소의 값이 6573.3687로 가장 높게 나왔으며, 이는 배열 인덱스 2에 해당되므로 정상적인 추론 결과가 나왔음을 알 수 있다. 도 10은 본 발명에 따른 이기종 프로세서 기반 인공지능 모델 추론 과정을 나타낸 것이다. 도 10에 도시된 이기종 프로세서 기반 인공지능 모델 추론 과정은 FPGA 기반의 이기종 프로세서 환경에서 CONNX 모델을 통한 추론 시 워크로드를 분배하여 추론하는 과정이다. 즉, 상술한 바와 같이, 학습 완료된 ONNX 모델을 모델 변환부(ONNX-CONNX Convertor)를 통해 CONNX 파일, 텍스 트 파일 및 데이터 파일로 구성된 CONNX 모델로 변환하고, 추론부(CONNX Engine)을 통해 CONNX 모델을 파싱하여 그래프에 포함된 각 오퍼레이터(Operator)를 적어도 3개 이상의 이기종 프로세서(CPU, FPGA 기반 NPU 및 PIM)를 통해 병렬적으로 계산하여 추론을 수행한다."}
{"patent_id": "10-2022-0187691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "이상의 설명은 본 발명을 예시적으로 설명한 것에 불과하며, 본 발명이 속하는 기술분야에서 통상의 지식을 가 진 자에 의해 본 발명의 기술적 사상에서 벗어나지 않는 범위에서 다양한 변형이 가능할 것이다. 따라서 본 발명의 명세서에 개시된 실시예들은 본 발명을 한정하는 것이 아니다. 본 발명의 범위는 아래의 특허 청구범위에 의해 해석되어야 하며, 그와 균등한 범위 내에 있는 모든 기술도 본 발명의 범위에 포함되는 것으로 해석해야 할 것이다."}
{"patent_id": "10-2022-0187691", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 이기종 프로세서 기반 인공지능 모델 추론 장치의 개략적인 구성도. 도 2는 본 발명에 따른 모델 변환부에서 ONNX 모델이 CONNX 모델로 변환되었을 때 생성되는 파일을 나타낸 도면. 도 3은 본 발명에 따른 추론부의 내부 구성을 나타낸 도면. 도 4는 본 발명에 따른 이기종 프로세서 기반 PIM에 적용되는 인공지능 모델 추론 방법의 순서도. 도 5는 본 발명에 따른 이기종 프로세서 기반 인공지능 모델 추론 방법에서 파싱 이후의 실제 추론 과정을 나타 낸 순서도. 도 6은 본 발명의 실시예에 따라 CONNX 모델로 변환하기 전 MNIST 데이터셋을 학습한 ONNX 모델의 구조를 시각 화한 도면. 도 7은 본 발명의 실시예에 따라 도 6의 학습된 ONNX 모델을 CONNX 모델로 변환한 결과를 나타낸 도면 도 8은 본 발명의 실시예에 따라 변환된 CONNX 모델의 text 파일의 내용을 출력한 도면 도 9는 본 발명의 실시예에 따라 CONNX 모델을 추론한 결과를 나타낸 도면. 도 10은 본 발명에 따른 이기종 프로세서 기반 인공지능 모델 추론 과정을 나타낸 순서도."}
