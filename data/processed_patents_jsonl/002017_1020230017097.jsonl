{"patent_id": "10-2023-0017097", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0044303", "출원번호": "10-2023-0017097", "발명의 명칭": "2단계 사전 학습 기반의 인공지능을 이용한 음성 인식 장치", "출원인": "경북대학교 산학협력단", "발명자": "정호영"}}
{"patent_id": "10-2023-0017097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "제1 인공지능 모델을 이용하여 학습 음성 데이터로부터 제1 특징 표현을 추출하는 제1 사전 학습부;제2 인공지능 모델을 이용하여 상기 제1 특징 표현으로부터 제2 특징 표현을 추출하는 제2 사전 학습부; 및제3 인공지능 모델을 이용하여 상기 제2 특징 표현으로부터 예측 텍스트를 추출하는 음성 인식부를 포함하는음성 인식 장치."}
{"patent_id": "10-2023-0017097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 사전 학습부는 상기 학습 음성 데이터를 마스킹한 데이터를 이용하여 상기 제1 인공지능 모델을 학습시키고,상기 제2 사전 학습부는 상기 제1 특징 표현의 상호 의존 정보를 이용하여 제2 인공지능 모델을 학습시키는음성 인식 장치."}
{"patent_id": "10-2023-0017097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제1 사전 학습부는,상기 학습 음성 데이터를 주파수 영역인 스펙트럼 데이터로 변환하는 데이터 처리부;상기 스펙트럼 데이터를 마스킹(masking)하는 데이터 차폐부;마스킹된 스펙트럼 데이터를 상기 제1 인공지능 모델에 입력시키는 제1 딥러닝 네트워크부; 및상기 제1 인공지능 모델의 학습을 위해, 상기 제1 인공지능 모델의 출력 데이터와 상기 스펙트럼 데이터의 오차를 산출하는 제1 오차 측정부를 포함하는음성 인식 장치."}
{"patent_id": "10-2023-0017097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 데이터 차폐부는 상기 스펙트럼 데이터의 제1 주파수 구간의 값을 0으로 하거나, 제1 시간 구간의 값을 0으로 하거나, 노이즈를 추가하거나, 제2 시간 구간의 값을 제3 시간 구간의 값으로 대체함으로써 마스킹을 수행하는음성 인식 장치."}
{"patent_id": "10-2023-0017097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서,공개특허 10-2024-0044303-3-상기 제1 오차 측정부는,상기 제1 인공지능 모델의 출력 데이터와 상기 스펙트럼 데이터의 차이값의 크기를 합하거나, 상기 제1 인공지능 모델의 출력 데이터와 상기 스펙트럼 데이터의 차이값을 제곱한 값을 합하여 오차를 산출하는음성 인식 장치."}
{"patent_id": "10-2023-0017097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 제2 사전 학습부는,상기 제1 사전 학습부로부터 상기 제1 특징 표현을 획득하고, 상기 제1 특징 표현을 이용하여 긍정 샘플 및 부정 샘플을 생성하는 데이터 레이블링부;상기 제1 특징 표현으로부터 상호 의존 정보를 추출하는 제2 딥러닝 네트워크부; 및상기 제2 인공지능 모델의 학습을 위해, 상기 제2 인공지능 모델의 출력 데이터와 상기 긍정 샘플 및 상기 부정샘플의 오차를 산출하는 제2 오차 측정부를 포함하는음성 인식 장치."}
{"patent_id": "10-2023-0017097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 상호 의존 정보는,상기 제1 특징 표현에 포함된 제1 시점의 제1 데이터에 기초하여 생성된 제1 문맥 정보 및 상기 제2 특징 표현에 포함된 제2 시점의 제2 데이터에 기초하여 생성된 제2 문맥 정보를 포함하는음성 인식 장치."}
{"patent_id": "10-2023-0017097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 제2 오차 측정부는,상기 제2 인공지능 모델의 출력 데이터 중 기준 시점보다 미래 시점의 데이터와 상기 긍정 샘플 및 상기 부정샘플 사이의 차이를 산출하는음성 인식 장치."}
{"patent_id": "10-2023-0017097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 음성 인식부는,상기 제2 특징 표현 및 정답 텍스트를 이용하여 상기 제3 인공지능 모델을 학습시키는 제3 딥러닝 네트워크부를포함하는음성 인식 장치.공개특허 10-2024-0044303-4-청구항 10 적어도 하나 이상의 프로세서에 의해 수행되는 음성 인식 방법에 있어서,마스킹된 데이터의 마스킹 부분을 예측하도록 학습된 제1 인공지능 모델을 이용하여 학습 음성 데이터로부터 제1 특징 표현을 추출하는 단계;상호 의존 정보를 이용하여 정답을 예측하도록 학습된 제2 인공지능 모델을 이용하여 상기 제1 특징 표현으로부터 제2 특징 표현을 추출하는 단계;제3 인공지능 모델을 이용하여 상기 제2 특징 표현으로부터 예측 음성 데이터를 획득하는 단계; 및상기 예측 음성 데이터를 이용하여 예측 텍스트를 추출하는 단계를 포함하는음성 인식 방법."}
{"patent_id": "10-2023-0017097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 기재된 음성 인식 방법을 실행시키도록 컴퓨터로 판독 가능한 기록 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0017097", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 음성 인식 장치는 제1 인공지능 모델을 이용하여 학습 음성 데이터로부터 제1 특징 표현을 추출하는 제1 사전 학습부; 제2 인공지능 모델을 이용하여 상기 제1 특징 표현으로부터 제2 특징 표현을 추출하는 제2 사 전 학습부; 및 제3 인공지능 모델을 이용하여 상기 제2 특징 표현으로부터 예측 텍스트를 추출하는 음성 인식부 를 포함할 수 있다."}
{"patent_id": "10-2023-0017097", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능을 이용한 음성 인식 장치에 관한 것으로, 보다 상세하게는, 텍스트를 이용한 본 학습 이전 에 2단계 사전 학습을 거진 인공지능을 이용한 음성 인식 장치에 관한 것이다."}
{"patent_id": "10-2023-0017097", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 인공지능을 이용한 음성 인식 기술이 발달함에 따라 음성 인식 시스템을 기반으로 하는 인공지능 스피커가 널리 사용되고 있다. 그럼에도 불구하고 인공지능 스피커를 사용한 경험이 있는 소비자의 만족률은 절반을 넘지 못하고 있다. 이러한 사용자의 불만족은 구체적으로 음성 명령이 잘 적용되지 않고, 자연스러운 대화가 곤란하 고, 소음을 음성 명령으로 오인하고, 사용자에게 맞는 정확한 정보 및 콘텐츠를 제공하지 못하는 등 다양한 이 유에서 기인하고 있다. 대부분의 딥러닝 기반 음성 인식 시스템은 음성과 이에 대한 정답인 텍스트의 쌍 형태로 구축된 데이터를 기반 으로 학습이 되고 있다. 그러나, 데이터의 양이 한정적이기 때문에 이러한 방법은 최적 학습에는 한계가 있고, 이를 극복하기 위한 추가적인 데이터 구축에는 많은 노력과 비용이 소모된다는 단점이 있다. 지도 학습 기반의 음성 인식 모델은 많은 양의 정답을 보유한 데이 터셋으로 기인하여 학습이 진행된다. 이에, 모델의 학습을 위해서는 입력 음성과 그에 대한 텍스트가 필요하지만, 학습 데이터 생성에는 많은 인력과 비용 이 소요된다. 실제로 다양한 사람들이 이용하는 온라인 서비스에는 이러한 문제를 해결할 수 있는 방대한 데이 터가 있다. 그러나, 온라인 서비스에서 제공되는 데이터에는 정답이 없는 비전사 음성 데이터가 대부분이다. 따 라서, 음성 인식 모델에 대한 비 지도 학습 및 이에 기반하여 지속적으로 확장이 가능한 음성 인식 시스템이 필 요한 실정이다."}
{"patent_id": "10-2023-0017097", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 과제는 텍스트를 이용한 본 학습 이전에 2단계 사전 학습을 거진 인공지능을 이용한 음성 인식 장 치에 관한 것이다."}
{"patent_id": "10-2023-0017097", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 음성 인식 장치는 제1 인공지능 모델을 이용하여 학습 음성 데이터로부터 제1 특징 표현을 추 출하는 제1 사전 학습부; 제2 인공지능 모델을 이용하여 상기 제1 특징 표현으로부터 제2 특징 표현을 추출하는 제2 사전 학습부; 및 제3 인공지능 모델을 이용하여 상기 제2 특징 표현으로부터 예측 텍스트를 추출하는 음성인식부를 포함할 수 있다. 여기서, 상기 제1 사전 학습부는 상기 학습 음성 데이터를 마스킹한 데이터를 이용하여 상기 제1 인공지능 모델 을 학습시키고, 상기 제2 사전 학습부는 상기 제1 특징 표현의 상호 의존 정보를 이용하여 제2 인공지능 모델을 학습시킬 수 있다. 여기서, 상기 제1 사전 학습부는, 상기 학습 음성 데이터를 주파수 영역인 스펙트럼 데이터로 변환하는 데이터 처리부; 상기 스펙트럼 데이터를 마스킹(masking)하는 데이터 차폐부; 마스킹된 스펙트럼 데이터를 상기 제1 인 공지능 모델에 입력시키는 제1 딥러닝 네트워크부; 및 상기 제1 인공지능 모델의 학습을 위해, 상기 제1 인공지 능 모델의 출력 데이터와 상기 스펙트럼 데이터의 오차를 산출하는 제1 오차 측정부를 포함할 수 있다. 여기서, 상기 데이터 차폐부는 상기 스펙트럼 데이터의 제1 주파수 구간의 값을 0으로 하거나, 제1 시간 구간의 값을 0으로 하거나, 노이즈를 추가하거나, 제2 시간 구간의 값을 제3 시간 구간의 값으로 대체함으로써 마스킹 을 수행할 수 있다. 여기서, 상기 제1 오차 측정부는, 상기 제1 인공지능 모델의 출력 데이터와 상기 스펙트럼 데이터의 차이값의 크기를 합하거나, 상기 제1 인공지능 모델의 출력 데이터와 상기 스펙트럼 데이터의 차이값을 제곱한 값을 합하 여 오차를 산출할 수 있다. 여기서, 상기 제2 사전 학습부는, 상기 제1 사전 학습부로부터 상기 제1 특징 표현을 획득하고, 상기 제1 특징 표현을 이용하여 긍정 샘플 및 부정 샘플을 생성하는 데이터 레이블링부; 상기 제1 특징 표현으로부터 상호 의 존 정보를 추출하는 제2 딥러닝 네트워크부; 및 상기 제2 인공지능 모델의 학습을 위해, 상기 제2 인공지능 모 델의 출력 데이터와 상기 긍정 샘플 및 상기 부정 샘플의 오차를 산출하는 제2 오차 측정부를 포함할 수 있다. 여기서, 상기 상호 의존 정보는, 상기 제1 특징 표현에 포함된 제1 시점의 제1 데이터에 기초하여 생성된 제1 문맥 정보 및 상기 제2 특징 표현에 포함된 제2 시점의 제2 데이터에 기초하여 생성된 제2 문맥 정보를 포함할 수 있다. 여기서, 상기 제2 오차 측정부는, 상기 제2 인공지능 모델의 출력 데이터 중 기준 시점보다 미래 시점의 데이터 와 상기 긍정 샘플 및 상기 부정 샘플 사이의 차이를 산출할 수 있다. 여기서, 상기 음성 인식부는, 상기 제2 특징 표현 및 정답 텍스트를 이용하여 상기 제3 인공지능 모델을 학습시 키는 제3 딥러닝 네트워크부를 포함할 수 있다. 일 실시예에 따른 음성 인식 방법은 적어도 하나 이상의 프로세서에 의해 수행되는 음성 인식 방법에 있어서, 마스킹된 데이터의 마스킹 부분을 예측하도록 학습된 제1 인공지능 모델을 이용하여 학습 음성 데이터로부터 제 1 특징 표현을 추출하는 단계; 상호 의존 정보를 이용하여 정답을 예측하도록 학습된 제2 인공지능 모델을 이용 하여 상기 제1 특징 표현으로부터 제2 특징 표현을 추출하는 단계; 제3 인공지능 모델을 이용하여 상기 제2 특 징 표현으로부터 예측 음성 데이터를 획득하는 단계; 및 상기 예측 음성 데이터를 이용하여 예측 텍스트를 추출 하는 단계를 포함할 수 있다. 여기서, 상기 음성 인식 방법을 실행시키도록 컴퓨터로 판독 가능한 기록 매체에 저장된 컴퓨터 프로그램이 제 공될 수 있다."}
{"patent_id": "10-2023-0017097", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면 텍스트를 이용한 본 학습 이전에 2단계 사전 학습을 거진 인공지능을 이용한 음 성 인식 장치가 제공될 수 있다."}
{"patent_id": "10-2023-0017097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에 기재된 실시예는 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 본 발명의 사상을 명 확히 설명하기 위한 것이므로, 본 발명이 본 명세서에 기재된 실시예에 한정되는 것은 아니며, 본 발명의 범위 는 본 발명의 사상을 벗어나지 아니하는 수정예 또는 변형예를 포함하는 것으로 해석되어야 한다. 본 명세서에서 사용되는 용어는 본 발명에서의 기능을 고려하여 가능한 현재 널리 사용되고 있는 일반적인 용어 를 선택하였으나 이는 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자의 의도, 판례 또는 새로운 기술 의 출현 등에 따라 달라질 수 있다. 다만, 이와 달리 특정한 용어를 임의의 의미로 정의하여 사용하는 경우에는 그 용어의 의미에 관하여 별도로 기재할 것이다. 따라서 본 명세서에서 사용되는 용어는 단순한 용어의 명칭이 아닌 그 용어가 가진 실질적인 의미와 본 명세서의 전반에 걸친 내용을 토대로 해석되어야 한다. 본 명세서에 첨부된 도면은 본 발명을 용이하게 설명하기 위한 것으로 도면에 도시된 형상은 본 발명의 이해를 돕기 위하여 필요에 따라 과장되어 표시된 것일 수 있으므로 본 발명이 도면에 의해 한정되는 것은 아니다. 본 명세서에서 본 발명에 관련된 공지의 구성 또는 기능에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우에 이에 관한 자세한 설명은 필요에 따라 생략하기로 한다. 도 1은 일 실시예에 따른 음성 인식 장치의 내부 구성 요소간의 흐름을 나타내는 구성도이다. 도 1을 참조하면, 일 실시예에 따른 음성 인식 장치는 제1 사전 학습부, 제2 사전 학습부 및 음성 인식부를 포함할 수 있다. 도 1은 음성 인식 장치에 포함되는 세 가지 구성 요소를 도시하고 있으나, 도 시된 구성 요소들이 필수적인 것은 아니고, 음성 인식 장치는 그보다 많은 구성 요소를 갖거나 그보다 적은 구 성 요소를 가질 수 있다. 또한, 음성 인식 장치의 각 구성 요소는 물리적으로 하나의 장치에 포함될 수도 있고, 각각의 기능 별로 분산된 분산 장치일 수 있다. 또는 음성 인식 장치의 각 구성 요소의 기능은 물리적으로 하나의 프로세서에 의해 수행 될 수도 있고, 각각의 기능 별로 분산된 복수의 프로세서에 의해 수행될 수도 있다. 예를 들어, 제1 사전 학습 부 및 제2 사전 학습부는 하나의 사전 학습 장치에 포함될 수 있으나, 이에 한정되지 않는다. 음성 인식 장치는 장치의 동작을 총괄하는 제어 프로세서를 포함할 수 있다. 구체적으로, 제어 프로세서는 제1 사전 학습부, 제2 사전 학습부 및 음성 인식부에 제어 명령을 보내 각 부서의 동작을 실행할 수 있다. 이하에서 특별한 언급이 없는 경우에는, 음성 인식 장치의 동작은 제어 프로세서의 제어에 의해 수행되는 것으 로 해석될 수 있다. 제1 사전 학습부는 제1 인공지능 모델을 이용하여 학습 음성 데이터로부터 제1 특징 표현을 추출할 수 있 다. 구체적으로, 제1 사전 학습부는 학습 음성 데이터를 마스킹한 데이터를 이용하여 학습된 제1 인공지 능 모델을 이용하여, 입력 데이터의 마스킹 부분을 예측하여 제1 특징 표현을 추출할 수 있다.제2 사전 학습부는 제2 인공지능 모델을 이용하여 제1 사전 학습부로부터 추출된 제1 특징 표현으 로부터 제2 특징 표현을 추출할 수 있다. 구체적으로, 제2 사전 학습부는 제1 특징 표현의 상호 의존 정 보를 이용하여 학습된 제2 인공지능 모델을 이용하여, 기준 시점의 미래 데이터를 예측하여 제2 특징 표현을 추 출할 수 있다. 음성 인식부는 제3 인공지능 모델을 이용하여 제2 사전 학습부로부터 추출된 제2 특징 표현으로부 터 예측 텍스트를 추출할 수 있다. 음성 인식부는 제1 사전 학습부 및 제2 사전 학습부에 의 해 수행된 2단계 사전 학습 이후 본 학습을 수행하는 구성일 수 있다. 구체적으로, 음성 인식부는 2단계 사전 학습 이후, 정답 텍스트를 이용하여 학습된 제3 인공지능 모델을 이용하여, 음성 데이터를 분석한 예측 텍 스트를 추출할 수 있다. 제1 사전 학습부, 제2 사전 학습부 및 음성 인식부에 대한 구체적인 설명은 도 2 내지 도 4 를 참조하여 이하에서 설명한다. 도 2는 일 실시예에 따른 제1 사전 학습부의 블록도이다. 도 2를 참조하면, 일 실시예에 따른 제1 사전 학습부는 데이터 처리부, 데이터 차폐부, 제1 딥러닝 네트워크부 및 제1 오차 측정부를 포함할 수 있다. 도 2는 제1 사전 학습부에 포함되 는 네 가지 구성 요소를 도시하고 있으나, 도시된 구성 요소들이 필수적인 것은 아니고, 제1 사전 학습부(100 0)는 그보다 많은 구성 요소를 갖거나 그보다 적은 구성 요소를 가질 수 있다. 또한, 제1 사전 학습부의 각 구성 요소의 기능은 물리적으로 하나의 프로세서에 의해 수행될 수도 있고, 각각의 기능 별로 분산된 복수의 프로세서에 의해 수행될 수도 있다. 예를 들어, 데이터 처리부 및 데이 터 차폐부는 데이터 전처리부와 같이 하나의 구성 요소로 병합될 수도 있으나, 이에 한정되지 않는다. 데이터 처리부는 학습 음성 데이터를 주파수 영역인 스펙트럼 데이터로 변환할 수 있다. 구체적으로, 데 이터 처리부는 입력된 음성 데이터를 주파수 영역의 데이터로 변환할 수 있다. 예를 들어, 데이터 처리부 는 Mel-filter Bank를 이용하여 학습 음성 데이터를 주파수 영역인 스펙트럼 데이터로 변환할 수 있으나, 이에 한정되지 않는다. 데이터 차폐부는 데이터 처리부에 의해 생성된 스펙트럼 데이터를 마스킹(masking)할 수 있다. 구 체적으로, 데이터 차폐부는 스펙트럼 데이터의 일부를 삭제하거나, 일부 데이터를 다른 데이터로 대체하 거나, 스펙트럼 데이터에 노이즈를 추가함으로써 스펙트럼 데이터를 마스킹할 수 있다. 데이터 차폐부의 마스킹에 대해서는 도 7 내지 도 9를 참조하여 이하에서 자세히 설명한다. 제1 딥러닝 네트워크부는 데이터 차폐부에 의해 마스킹된 스펙트럼 데이터를 제1 인공지능 모델에 입력시켜 제1 인공지능 모델을 학습시킬 수 있다. 구체적으로, 제1 딥러닝 네트워크부는 제1 인공지능 모 델이 마스킹된 스펙트럼 데이터의 마스킹 부분을 예측하도록 제1 인공지능 모델을 학습시킬 수 있다. 제1 오차 측정부는 제1 인공지능 모델의 학습을 위해, 제1 인공지능 모델의 출력 데이터와 스펙트럼 데이 터의 오차를 산출할 수 있다. 구체적으로, 제1 오차 측정부는 제1 인공지능 모델이 예측한 마스킹 부분이 마스킹되지 않은 원래의 스펙트럼 데이터와 얼마나 차이나는지에 대한 수치를 산출할 수 있다. 제1 딥러닝 네트 워크부는 제1 오차 측정부가 산출한 오차가 제1 임계값보다 작아지도록 계속적으로 제1 인공지능 모델을 학습시킬 수 있다. 구체적으로, 제1 오차 측정부는 제1 인공지능 모델의 출력 데이터와 스펙트럼 데이터의 차이값의 크기를 합하여 오차를 산출할 수 있다. 또는 제1 오차 측정부는 제1 인공지능 모델의 출력 데이터와 스펙트럼 데 이터의 차이값을 제곱한 값을 합하여 오차를 산출할 수 있다. 이때, 제1 임계값은 제1 인공지능 모델의 성능, 어플리케이션의 목적, 입력한 학습 음성 데이터의 속성 등에 기 초하여 설정될 수 있다. 예를 들어, 제1 인공지능 모델의 성능이 좋을수록 제1 임계값은 작아질 수 있다. 또한 예를 들어, 음성 인식 장치가 적용되는 어플리케이션의 환경이 고성능을 요구할수록 제1 임계값은 작아질 수 있 다. 또한 예를 들어, 입력한 학습 음성 데이터의 음질이 좋을수록 제1 임계값은 작아질 수 있다. 제1 사전 학습부는 마스킹된 부분을 예측하도록 제1 인공지능 모델을 학습시켜, 음성 인식을 위한 1단계 사전 학습을 수행할 수 있다. 제1 사전 학습부는 결과적으로 오차가 제1 임계값보다 작도록 학습된 제1인공지능 모델을 이용하여 학습 음성 데이터로부터 제1 특징 표현을 추출할 수 있다. 제1 사전 학습부는 추출한 제1 특징 표현을 제2 사전 학습부로 전달할 수 있다. 전달된 제1 특징 표현은 2단계 사전 학습에 이용될 수 있다. 도 3은 일 실시예에 따른 제2 사전 학습부의 블록도이다. 도 3을 참조하면, 일 실시예에 따른 제2 사전 학습부는 데이터 레이블링부, 제2 딥러닝 네트워크부 및 제2 오차 측정부를 포함할 수 있다. 도 3은 제2 사전 학습부에 포함되는 세 가지 구성 요소를 도시하고 있으나, 도시된 구성 요소들이 필수적인 것은 아니고, 제2 사전 학습부는 그보다 많은 구성 요소를 갖거나 그보다 적은 구성 요소를 가질 수 있다. 예를 들어, 제2 사전 학습부는 제1 사전 학 습부가 전달한 제1 특징 표현을 획득하는 구성 요소를 더 포함할 수 있으나, 이에 한정되지 않는다. 또한, 제2 사전 학습부의 각 구성 요소의 기능은 물리적으로 하나의 프로세서에 의해 수행될 수도 있고, 각각의 기능 별로 분산된 복수의 프로세서에 의해 수행될 수도 있다. 예를 들어, 제2 딥러닝 네트워크부 및 제2 오차 측정부는 학습부와 같이 하나의 구성 요소로 병합될 수도 있으나, 이에 한정되지 않는다. 데이터 레이블링부는 제1 사전 학습부로부터 제1 특징 표현을 전달받을 수 있다. 또한, 데이터 레 이블링부는 제1 특징 표현을 이용하여 긍정 샘플 및 부정 샘플을 생성할 수 있다. 구체적으로, 데이터 레 이블링부는 기준 시점을 설정하고, 기준 시점보다 미래 시점의 데이터에 대한 샘플을 생성할 수 있다. 예를 들어, 제1 특징 표현이 '안녕하세요'에 관한 것일 경우, 데이터 레이블링부는 '하'에 대응되는 시점 을 기준 시점으로 설정할 수 있다. 이에, 데이터 레이블링부는 기준 시점보다 미래 시점에 대응되는 '세 요'에 대한 긍정 샘플 및 부정 샘플을 생성할 수 있다. 구체적인 예를 들어, 데이터 레이블링부는 긍정 샘플을'세요'로 설정하고, 부정 샘플을 '시요', '스요', '수요' 등으로 설정할 수 있다. 또한 예를 들어, 제1 특징 표현이 '오늘 날씨 알려줘'에 관한 것일 경우, 데이터 레이블링부는 '씨'에 대 응되는 시점을 기준 시점으로 설정할 수 있다. 이에, 데이터 레이블링부는 기준 시점보다 미래 시점에 대 응되는 '알려줘'에 대한 긍정 샘플 및 부정 샘플을 생성할 수 있다. 구체적인 예를 들어, 데이터 레이블링부 는 긍정 샘플을 '알려줘'로 설정하고, 부정 샘플을 '일려줘', '일러줘', '알랴줘' 등으로 설정할 수 있다. 제2 딥러닝 네트워크부는 제1 특징 표현으로부터 상호 의존 정보를 추출할 수 있다. 구체적으로, 제2 딥 러닝 네트워크부는 기준 시점보다 과거 시점에 대응되는 데이터로부터 상호 의존 정보를 추출할 수 있다. 이때, 상호 의존 정보는 과거 시점의 데이터 간의 문맥 정보를 포함할 수 있다. 위 '안녕하세요'의 예시에서, 상호 의존 정보는 '안'에 기초하여 생성된 제1 문맥 정보 및 '녕'에 기초하여 생성된 제2 문맥 정보를 포함할 수 있다. 또는 위 '오늘 날씨 알려줘'의 예시에서, 상호 의존 정보는 '오','늘','날' 각각에 기초하여 생성된 문맥 정보들을 포함할 수 있다. 제2 오차 측정부는 제2 인공지능 모델의 학습을 위해, 제2 인공지능 모델의 출력 데이터와 데이터 레이블 링부에 의해 생성된 긍정 샘플 및 부정 샘플의 오차를 산출할 수 있다. 구체적으로, 제2 오차 측정부 는 제2 인공지능 모델이 상호 의존 정보를 이용하여 예측한 기준 시점보다 미래 시점의 데이터와 긍정 샘 플이 얼마나 차이나는지에 대한 수치를 산출할 수 있다. 제2 딥러닝 네트워크부는 제2 오차 측정부(230 0)가 산출한 오차가 제2 임계값보다 작아지도록 계속적으로 제2 인공지능 모델을 학습시킬 수 있다. 이때, 제2 임계값은 제2 인공지능 모델의 성능, 어플리케이션의 목적, 입력한 학습 음성 데이터의 속성 등에 기 초하여 설정될 수 있다. 구체적인 내용은 제1 임계값에 대한 내용과 중복될 수 있으므로, 자세한 내용은 생략한 다. 제2 사전 학습부는 미래 데이터를 예측하도록 제2 인공지능 모델을 학습시켜, 음성 인식을 위한 2단계 사 전 학습을 수행할 수 있다. 제2 사전 학습부는 결과적으로 오차가 제2 임계값보다 작도록 학습된 제2 인 공지능 모델을 이용하여 제1 특징 표현으로부터 제2 특징 표현을 추출할 수 있다. 제2 사전 학습부는 추 출한 제2 특징 표현을 음성 인식부로 전달할 수 있다. 전달된 제2 특징 표현은 음성 인식을 위한 본 학습 에 이용될 수 있다. 이때, 본 학습은 텍스트를 이용한 학습으로, 텍스트를 이용하지 않는 제1 사전 학습부 에 의한 1단계 사전 학습 및 제2 사전 학습부에 의한 2단계 사전 학습과 상이할 수 있다.도 4는 일 실시예에 따른 음성 인식부의 블록도이다. 도 4를 참조하면, 일 실시예에 따른 음성 인식부는 제3 딥러닝 네트워크부, 제3 오차 측정부 및 결과 출력부를 포함할 수 있다. 도 4는 음성 인식부에 포함되는 세 가지 구성 요소를 도시하고 있으나, 도시된 구성 요소들이 필수적인 것은 아니고, 음성 인식부는 그보다 많은 구성 요소를 갖거나 그 보다 적은 구성 요소를 가질 수 있다. 예를 들어, 음성 인식부는 제2 사전 학습부가 전달한 제2 특 징 표현을 획득하는 구성 요소를 더 포함할 수 있으나, 이에 한정되지 않는다. 또한, 음성 인식부의 각 구성 요소의 기능은 물리적으로 하나의 프로세서에 의해 수행될 수도 있고, 각각 의 기능 별로 분산된 복수의 프로세서에 의해 수행될 수도 있다. 예를 들어, 제3 딥러닝 네트워크부 및 제3 오차 측정부는 학습부와 같이 하나의 구성 요소로 병합될 수도 있으나, 이에 한정되지 않는다. 제3 딥러닝 네트워크부는 제2 특징 표현 및 정답 텍스트를 이용하여 제3 인공지능 모델을 학습시킬 수 있 다. 제3 오차 측정부는 제3 인공지능 모델의 학습을 위해, 제3 인공지능 모델의 출력 데이터와 정답 텍스 트 사이의 오차를 산출할 수 있다. 제3 딥러닝 네트워크부는 제3 오차 측정부가 산출한 오차가 제3 임계값보다 작아지도록 계속적으로 제3 인공지능 모델을 학습시킬 수 있다. 이때, 제3 임계값은 제3 인공지능 모델의 성능, 어플리케이션의 목적, 입력한 학습 음성 데이터의 속성 등에 기 초하여 설정될 수 있다. 구체적인 내용은 제1 임계값에 대한 내용과 중복될 수 있으므로, 자세한 내용은 생략한 다. 결과 출력부는 학습된 제3 인공지능 모델을 이용하여, 입력된 음성 데이터에 대응되는 예측 텍스트를 출 력할 수 있다. 결과적으로, 결과 출력부의 결과값인 예측 텍스트는 음성 인식 장치의 결과가 될 수 있다. 음성 인식 장치는 예측 텍스트를 이용하여 사용자의 음성에 대한 응답을 출력할 수 있다. 도 5는 일 실시예에 따른 음성 인식 장치의 음성 인식 방법을 나타내는 순서도이다. 도 5를 참조하면, 일 실시예에 따른 음성 인식 장치의 음성 인식 방법은 음성 데이터로부터 제1 특징 표현을 추 출하는 단계(S100), 제1 특징 표현을 이용하여 제2 특징 표현을 추출하는 단계(S200) 및 제2 특징 표현을 이용 하여 예측 텍스트를 추출하는 단계(S300)를 포함할 수 있다. 도 5에는 단계 S100 내지 단계 S300이 순서대로 수 행되는 것이 도시되었으나, 이에 한정되지 않고 각 단계 사이에 다른 단계가 추가될 수 있다. 또는 각 단계가 다른 단계와 병합되거나 생략될 수도 있다. 음성 데이터로부터 제1 특징 표현을 추출하는 단계(S100)는 제1 사전 학습부에 의해 수행될 수 있다. 구 체적으로, 데이터 처리부가 입력받은 학습 음성 데이터를 주파수 영역인 스펙트럼 데이터로 변환할 수 있 다. 데이터 차폐부는 스펙트럼 데이터를 마스킹하여 마스킹된 스펙트럼 데이터를 생성할 수 있다. 제1 딥 러닝 네트워크부는 마스킹된 스펙트럼 데이터의 마스킹부분을 예측하도록 제1 인공지능 모델을 학습시킬 수 있다. 제1 오차 측정부는 제1 인공지능 모델의 학습을 위해, 제1 인공지능 모델의 결과 데이터와 마스 킹되지 않은 원래의 스펙트럼 데이터 사이의 오차를 측정할 수 있다. 단계 S100에 대한 구체적인 내용은 도 6 내지 도 11을 참조하여 설명한다. 제1 특징 표현을 이용하여 제2 특징 표현을 추출하는 단계(S200)는 제2 사전 학습부에 의해 수행될 수 있 다. 구체적으로, 데이터 레이블링부가 제1 특징 표현으로부터 긍정 샘플 및 부정 샘플을 설정 및/또는 생 성할 수 있다. 제2 딥러닝 네트워크부는 제2 특징 표현의 기준 시점보다 미래인 시점의 데이터를 예측하 도록 제2 인공지능 모델을 학습시킬 수 있다. 제2 오차 측정부는 제2 인공지능 모델의 학습을 위해, 제2 인공지능 모델의 결과 데이터와 긍정 샘플 및 부정 샘플 사이의 오차를 측정할 수 있다. 단계 S200에 대한 구체 적인 내용은 도 12 및 도 13을 참조하여 설명한다. 제2 특징 표현을 이용하여 예측 텍스트를 추출하는 단계(S300)는 음성 인식부에 의해 수행될 수 있다. 구 체적으로, 제3 딥러닝 네트워크부는 제2 특징 표현을 텍스트로 예측하도록 제3 인공지능 모델을 학습시킬 수 있다. 제3 오차 측정부는 제3 인공지능 모델의 학습을 위해, 제3 인공지능 모델의 결과 데이터와 정답 텍스트 사이의 오차를 측정할 수 있다. 결과 측정부는 학습된 제3 인공지능 모델을 이용하여 제2 특징 표 현으로부터 예측 데이터를 추출할 수 있다. 단계 S300에 대한 구체적인 내용은 도 15 및 도 16을 참조하여 설명한다. 도 6은 일 실시예에 따른 제1 사전 학습부의 인공지능 학습 및 특징 표현 추출 방법의 순서도이다. 도 6을 참조하면, 일 실시예에 따른 제1 사전 학습부의 인공지능 학습 및 특징 표현 추출 방법은 학습 음 성 데이터를 스펙트럼 데이터로 변환하는 단계(S110), 스펙트럼 데이터를 마스킹하는 단계(S120), 제1 인공지능 모델을 학습시키는 단계(S130), 제1 인공지능 모델의 오차를 산출하는 단계(S140), 오차를 제1 임계값과 비교하 는 단계(S150) 및 제1 인공지능 모델을 이용하여 제1 특징 표현을 추출하는 단계(S160)를 포함할 수 있다. 도 6 에는 단계 S110 내지 단계 S160이 순서대로 수행되는 것이 도시되었으나, 이에 한정되지 않고 각 단계 사이에 다른 단계가 추가될 수 있다. 또는 각 단계가 다른 단계와 병합되거나 생략될 수도 있다. 방법은 학습 음성 데이터를 스펙트럼 데이터로 변환하는 단계(S110)는 데이터 처리부가 입력받은 학습 음 성 데이터를 주파수 영역의 스펙트럼 데이터로 변환하는 단계일 수 있다. 예를 들어, 데이터 처리부는 Mel-filter Bank를 이용하여 학습 음성 데이터를 주파수 영역인 스펙트럼 데이터로 변환할 수 있으나, 이에 한 정되지 않는다. 스펙트럼 데이터는 도 9(a)의 데이터와 같이 시간-주파수 좌표 상에 주파수의 세기를 나타내는 데이터일 수 있다. 스펙트럼 데이터를 마스킹하는 단계(S120)는 데이터 차폐부가 스펙트럼 데이터의 일부 데이터를 삭제하거 나, 일 영역의 데이터를 다른 영역의 데이터로 대체하거나, 노이즈를 추가함으로써 스펙트럼 데이터를 마스킹 (masking)하는 단계일 수 있다. 도 7 내지 도 9를 참조하여 이하에서 자세히 설명한다. 도 7 내지 도 9는 제1 사전 학습부의 학습 음성 데이터 마스킹을 설명하기 위한 도면이다. 도 7을 참조하면, 데이터 차폐부는 A 내지 K로 표현된 데이터들 중 일부 데이터(C, F, G, I)를 삭제할 수 있다. 데이터 차폐부는 데이터 중 일부의 값을 0으로 하여 스펙트럼 데이터를 마스킹할 수 있다. 이때, 마스킹된 일부 데이터는 주파수 대역 중 일부 주파수에 대응될 수 있다. 도 8을 참조하면, 데이터 차폐부는 스펙트럼 데이터에 대해 일정 비율로 마스킹할 수 있다. 구체적으로, 데이터 차폐부는 도 8과 같이 원래 스펙트럼 데이터의 80%에 해당하는 데이터의 값을 삭제하여 0으로 설 정하고, 10%에 해당하는 데이터를 다른 영역의 데이터로 대체하고, 나머지 10%는 그대로 유지함으로써 스펙트럼 데이터의 마스킹을 수행할 수 있다. 그러나, 도 8의 수치에 한정되지 않고 마스킹을 수행하는 데이터의 비율은 경우에 따라 상이할 수 있다. 도 9는 Mel-filter Bank를 거친 주파수 영역의 스펙트럼 데이터를 시각적으로 나타낸 것이다. 도 9(a)는 마스킹 되지 않은 스펙트럼 데이터를 나타낸 도면이고, 도 9(b)는 일 주파수 영역의 데이터를 삭제함으로써 마스킹된 스펙트럼 데이터를 나타낸 도면이고, 도 9(c)는 일 시간 영역의 데이터를 삭제함으로써 마스킹된 스펙트럼 데이 터를 나타낸 도면이고, 도 9(d)는 노이즈를 추가함으로써 마스킹된 스펙트럼 데이터를 나타낸 도면이고, 도 9(e)는 일 영역의 데이터가 다른 영역의 데이터로 대체됨으로써 마스킹된 스펙트럼 데이터를 나타낸 도면이고, 도 9(f)는 도 9(b) 내지 도 9(e)의 마스킹이 모두 합산된 마스킹된 스펙트럼 데이터를 나타낸 도면이다. 도 9(b) 내지 도 9(f)와 같이, 데이터 차폐부는 가로 축 또는 세로 축을 기준으로 일 영역의 데이터를 삭 제하거나, 가우시안 노이즈를 추가하거나, 다른 영역의 데이터를 대체하거나 이들 모두를 조합함으로써 스펙트 럼 데이터를 마스킹할 수 있다. 다시 도 6을 참조하면, 제1 인공지능 모델을 학습시키는 단계(S130)는 제1 딥러닝 네트워크부가 마스킹된 스펙트럼 데이터를 제1 인공지능 모델에 입력시킴으로써 제1 인공지능 모델이 마스킹된 부분을 예측할 수 있도 록 학습시키는 단계일 수 있다. 제1 인공지능 모델의 오차를 산출하는 단계(S140)는 제1 오차 측정부가 제1 인공지능 모델의 결과 데이터 와 마스킹되지 않은 스펙트럼 데이터 사이의 오차를 산출하는 단계일 수 있다. 오차를 제1 임계값과 비교하는 단계(S150)는 제1 오차 측정부가 산출한 오차가 줄어드는 방향으로 제1 인 공지능 모델이 학습될 수 있도록, 제1 임계값과 비교하는 단계일 수 있다. 오차가 제1 임계값보다 크거나 같으 면 단계 S130이 다시 수행되어 제1 인공지능 모델이 재학습될 수 있다. 오차가 제1 임계값보다 작으면 제1 인공 지능 모델의 학습이 완료된 것으로 판단되어, 제1 인공지능 모델을 이용하여 제1 특징 표현을 추출(S160)할 수 있다. 이때, 제1 특징 표현은 음성 데이터의 속성을 나타내는 벡터일 수 있으나, 이에 한정되지 않는다.제1 사전 학습부는 단계 S160 이후에 추출한 제1 특징 표현을 제2 사전 학습부로 전달하는 단계를 더 포함할 수 있다. 도 10은 제1 인공지능 모델의 학습 및 예측을 설명하기 위한 도면이다. 도 10을 참조하면, 제1 인공지능 모델은 데이터 차폐부에 의해 마스킹된 스펙트럼 데이터에 기초하여 학 습될 수 있다. 학습된 제1 인공지능 모델은 마스킹된 입력 데이터가 입력되면 마스킹된 부분을 예측할 수 있다. 제1 인공지능 모델은 최종적으로 마스킹된 부분을 예측하여 제1 특징 표현을 추출할 수 있다. 도 11은 제1 사전 학습부의 오차 산출 과정을 설명하기 위한 도면이다. 도 11을 참조하면, 제1 사전 학습부의 제1 인공지능 모델은 마스킹된 스펙트럼 데이터(Masked Speech Frames)를 입력받고, 마스킹된 부분을 예측하여 결과 데이터(Predicted Features)를 출력할 수 있다. 제1 사전 학습부의 제1 오차 측정부는 결과 데이터(Predicted Features)와 마스킹되지 않은 스펙트럼 데이터 (Original Speech Features) 사이의 오차를 측정할 수 있다. 측정된 오차를 이용하여 제1 딥러닝 네트워크부 는 제1 인공지능 모델을 학습시킬 수 있다. 도 12는 일 실시예에 따른 제2 사전 학습부의 인공지능 학습 및 특징 표현 추출 방법의 순서도이다. 도 12를 참조하면, 일 실시예에 따른 제2 사전 학습부의 인공지능 학습 및 특징 표현 추출 방법은 제1 특 징 표현의 기준 시점을 설정하는 단계(S210), 상호 의존 정보를 생성하는 단계(S220), 긍정 샘플 및 부정 샘플 을 생성하는 단계(S230), 상호 의존 정보를 이용하여 미래 시점의 데이터를 예측하는 단계(S240), 미래 시점의 데이터와 샘플들 사이의 오차를 산출하는 단계(S250), 오차가 제2 임계값보다 작은지 여부를 판단하는 단계 (S260) 및 제2 특징 표현을 추출하는 단계(S270)를 포함할 수 있다. 도 12에는 단계 S210 내지 단계 S270이 순 서대로 수행되는 것이 도시되었으나, 이에 한정되지 않고 각 단계 사이에 다른 단계가 추가될 수 있다. 또는 각 단계가 다른 단계와 병합되거나 생략될 수도 있다. 제1 특징 표현의 기준 시점을 설정하는 단계(S210)는 제1 사전 학습부에서 추출된 제1 특징 표현을 이용 하여 제2 인공지능 모델을 학습시키기 위한 기준 시점을 설정하는 단계일 수 있다. 단계 S210 이전에 제2 사전 학습부가 제1 사전 학습부로부터 제1 특징 표현을 수신 또는 획득하는 단계가 수행될 수도 있다. 기준 시점에 대해서는 도 13을 참조하여 자세히 설명한다. 도 13은 제2 인공지능 모델의 학습 및 예측을 설명하기 위한 도면이다. 도 13을 참조하면, 제1 사전 학습부로부터 획득한 제1 특징 표현은 시간에 따른 데이터를 포함할 수 있다. 데이터 레이블링부는 제1 특징 표현에 대해 기준 시점을 설정할 수 있다. 기준 시점을 기준으 로 제2 인공지능 모델은 기준 시점보다 과거 시점의 데이터에 대한 상호 의존 정보를 추출하고, 추출한 상호 의 존 정보에 기초하여 기준 시점보다 미래 시점의 데이터를 예측하게 된다. 상호 의존 정보를 생성하는 단계(S220)는 제2 딥러닝 네트워크부가 제1 특징 표현에서 기준 시점보다 과 거 시점의 데이터를 제2 인공지능 모델에 입력하여 상호 의존 정보를 생성하는 단계일 수 있다. 이때, 상호 의 존 정보는 과거 시점의 데이터들의 문맥 정보를 포함할 수 있다. 구체적으로, 상호 의존 정보는 제1 과거 시점 의 데이터에 기초하여 생성된 제1 문맥 정보, 제2 과거 시점의 데이터에 기초하여 생성된 제2 문맥 정보, 제3 과거 시점의 데이터에 기초하여 생성된 제3 문맥 정보 및 기준 시점의 데이터에 기 초하여 생성된 제4 문맥 정보를 포함할 수 있다. 긍정 샘플 및 부정 샘플을 생성하는 단계(S230)는 데이터 레이블링부가 기준 시점보다 미래 시점의 특징 표현 데이터에 기초하여 제2 인공지능 모델의 학습을 위한 긍정 샘플 및 부정 샘플을 생성하는 단계일 수 있다. 예를 들어, 제1 특징 표현이 '안녕하세요'에 관한 것이고, '하'에 대응되는 시점을 기준 시점인 경우, 데이터 레이블링부는 기준 시점보다 미래 시점에 대응되는 '세요'에 대한 긍정 샘플 및 부정 샘플을 생성할 수 있다. 구체적인 예를 들어, 데이터 레이블링부는 긍정 샘플을'세요'로 설정하고, 부정 샘플을 '시요', ' 스요', '수요' 등으로 설정할 수 있다. 단계 S220과 단계 S230의 순서는 변경될 수도 있고, 두 단계가 동시에수행될 수도 있다. 상호 의존 정보를 이용하여 미래 시점의 데이터를 예측하는 단계(S240)는 제2 딥러닝 네트워크부가 상호 의존 정보 및 제1 특징 표현을 이용하여 제2 인공지능 모델을 통해 기준 시점보다 미래 시점의 데이터를 추출하 는 단계일 수 있다. 미래 시점의 데이터와 샘플들 사이의 오차를 산출하는 단계(S250)는 제2 오차 측정부가 제2 인공지능 모 델의 결과 데이터와 긍정 샘플 및 부정 샘플 사이의 오차를 산출하는 단계일 수 있다. 오차가 제2 임계값보다 작은지 여부를 판단하는 단계(S260)는 제2 오차 측정부가 산출하는 오차가 줄어드 는 방향으로 제2 인공지능 모델이 학습될 수 있도록, 오차를 제2 임계값과 비교하는 단계일 수 있다. 오차가 제 2 임계값보다 크거나 같으면 단계 S240이 다시 수행되어 제2 인공지능 모델이 재학습될 수 있다. 오차가 제2 임 계값보다 작으면 제2 인공지능 모델의 학습이 완료된 것으로 판단되어, 제2 인공지능 모델을 이용하여 제2 특징 표현을 추출(S270)할 수 있다. 이때, 제2 특징 표현은 음성 데이터의 속성을 나타내는 벡터일 수 있으나, 이에 한정되지 않는다. 제2 사전 학습부는 단계 S270 이후에 추출한 제2 특징 표현을 음성 인식부로 전달하는 단계를 더 포함할 수 있다. 도 14는 제1 사전 학습부 및 제2 사전 학습부에 의해 수행되는 2단계 사전 학습을 설명하기 위한 도면이다. 도 14를 참조하면, 일 실시예에 따른 음성 인식 장치는 텍스트를 이용하지 않은 사전 학습을 2단계로 수행할 수 있다. 구체적으로, 음성 인식 장치는 제1 사전 학습부에 의해 수행되는 1단계 사전 학습 및 제2 사 전 학습부에 의해 수행되는 2단계 사전 학습을 수행할 수 있다. 1단계 사전 학습은 마스킹된 데이터를 생성 및 이용하여 마스킹된 부분을 예측하도록 학습된 제1 인공지 능 모델에 의해 결과적으로 제1 특징 표현을 추출하는 학습일 수 있다. 2단계 사전 학습은 과거 시점의 상호 의존 정보에 기초하여 미래 시점의 데이터를 예측하도록 학습된 제2 인공지능 모델에 의해 결과적으로 제2 특징 표현을 추출하는 학습일 수 있다. 본원 발명의 음성 인식 장치는 2단계의 사전 학습을 통해 음성 인식의 정확도 및 효율성을 향상시킬 수 있다. 또한, 본원 발명은 비 지도 학습 기반으로, 종래의 지도 학습에서의 학습 데이터 생성에 대한 자원 문제를 해결 할 수 있다. 도 15는 일 실시예에 따른 음성 인식부의 인공지능 모델 학습 및 음성 인식 방법의 순서도이다. 도 15를 참조하면, 일 실시예에 따른 음성 인식부의 인공지능 모델 학습 및 음성 인식 방법은 제2 특징 표현 및 정답 텍스트를 이용하여 제3 인공지능 모델을 학습시키는 단계(S310), 제3 인공지능 모델의 출력 데이 터와 정답 텍스트의 오차를 산출하는 단계(S320), 오차가 제3 임계값보다 작은지 여부를 판단하는 단계(S330) 및 제3 인공지능 모델을 이용하여 예측 텍스트를 출력하는 단계(S340)를 포함할 수 있다. 제2 특징 표현 및 정답 텍스트를 이용하여 제3 인공지능 모델을 학습시키는 단계(S310)는 제3 딥러닝 네트워크 부가 제2 특징 표현 및 정답 텍스트를 제3 인공지능 모델에 입력하여 제3 인공지능 모델이 제2 특징 표현 의 텍스트를 예측하도록 학습시키는 단계일 수 있다. 2단계 사전 학습과 달리 음성 인식부는 텍스트를 이 용하여 제3 인공지능 모델을 학습시킬 수 있다. 제3 인공지능 모델의 출력 데이터와 정답 텍스트의 오차를 산출하는 단계(S320)는 제3 오차 측정부가 제3 인공지능 모델의 결과 데이터와 정답 텍스트 사이의 오차를 산출하는 단계일 수 있다. 오차가 제3 임계값보다 작은지 여부를 판단하는 단계(S330)는 제3 오차 측정부가 산출하는 오차가 줄어드 는 방향으로 제3 인공지능 모델이 학습될 수 있도록, 오차를 제3 임계값과 비교하는 단계일 수 있다. 오차가 제 3 임계값보다 크거나 같으면 단계 S310이 다시 수행되어 제3 인공지능 모델이 재학습될 수 있다. 오차가 제3 임 계값보다 작으면 제3 인공지능 모델의 학습이 완료된 것으로 판단되어, 제3 인공지능 모델을 이용하여 예측 텍 스트를 출력(S340)할 수 있다. 이때, 예측 텍스트는 제3 인공지능 모델에 입력된 제2 특징 표현이 문자화된 결 과 데이터일 수 있다.도 16은 제3 인공지능 모델의 학습 및 예측을 설명하기 위한 도면이다. 도 16을 참조하면, 제3 인공지능 모델에 2단계 사전 학습된 특징 표현이 입력될 수 있다. 예를 들어, 제3 인공지능 모델에 제2 사전 학습부에서 추출된 제2 특징 표현이 입력될 수 있다. 학습된 제3 인공지능 모 델은 양방향 LSTM(Bi-LSTM) 및 셀프 어텐션(Self-Attention)을 거쳐 제2 특징 표현에 대한 예측 텍스트를 출력 할 수 있다. 구체적으로, 제3 인공지능 모델은 선형화(Linear) 및 소프트맥스(Softmax) 함수를 이용하여 예측 텍스트를 출력할 수 있다. 음성 인식부는 제1 사전 학습부 및 제2 사전 학습부에 의해 텍스트 없이 수행된 2단계 사전 학습의 결과인 제2 특징 표현을 획득하고, 이로부터 제3 인공지능 모델을 이용하여 예측 텍스트를 추출할 수 있 다. 본원 발명의 음성 인식 장치는 정답이 없는 비전사 음성 데이터를 이용하여 2단계 사전 학습을 수행하고, 이를 기반으로 지속적으로 확장이 가능한 음성 인식 시스템을 포함할 수 있다. 따라서, 본원 발명의 음성 인식 장치는 학습의 효율성을 증가시킬 수 있고, 학습 데이터 생성을 위해 많은 인력과 비용이 소요되는 문제를 해결 할 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계 되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가 능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD- ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 실시예의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도 록 구성될 수 있으며, 그 역도 마찬가지이다"}
{"patent_id": "10-2023-0017097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2023-0017097", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 음성 인식 장치의 내부 구성 요소간의 흐름을 나타내는 구성도이다. 도 2는 일 실시예에 따른 제1 사전 학습부의 블록도이다. 도 3은 일 실시예에 따른 제2 사전 학습부의 블록도이다. 도 4는 일 실시예에 따른 음성 인식부의 블록도이다.도 5는 일 실시예에 따른 음성 인식 장치의 음성 인식 방법을 나타내는 순서도이다. 도 6은 일 실시예에 따른 제1 사전 학습부의 인공지능 학습 및 특징 표현 추출 방법의 순서도이다. 도 7 내지 도 9는 제1 사전 학습부의 학습 음성 데이터 마스킹을 설명하기 위한 도면이다. 도 10은 제1 인공지능 모델의 학습 및 예측을 설명하기 위한 도면이다. 도 11은 제1 사전 학습부의 오차 산출 과정을 설명하기 위한 도면이다. 도 12는 일 실시예에 따른 제2 사전 학습부의 인공지능 학습 및 특징 표현 추출 방법의 순서도이다. 도 13은 제2 인공지능 모델의 학습 및 예측을 설명하기 위한 도면이다. 도 14는 제1 사전 학습부 및 제2 사전 학습부에 의해 수행되는 2단계 사전 학습을 설명하기 위한 도면이다. 도 15는 일 실시예에 따른 음성 인식부의 인공지능 모델 학습 및 음성 인식 방법의 순서도이다. 도 16은 제3 인공지능 모델의 학습 및 예측을 설명하기 위한 도면이다."}
