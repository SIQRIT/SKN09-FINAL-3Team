{"patent_id": "10-2023-0034241", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0135532", "출원번호": "10-2023-0034241", "발명의 명칭": "집단지성화 알고리즘을 이용한 정보처리방법", "출원인": "유한회사 닥터다비드", "발명자": "김행철"}}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "제 1 기초 영상정보에 기반하여 GNN 및 GAN에 의해 영상정보를 출력(output) 및 생성하는 제1 영상정보 출력(output) 및 생성 단계;상기 영상정보에 대한 제1 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 분류모델 유도 단계;상기 영상정보 및 제2 기초 영상정보를 기반하여 영상정보를 출력(output) 및 생성하는 제2 영상정보 출력(output) 및 생성 단계;상기 영상정보에 대한 제2 선택라벨링 정보를 수신하여 분류모델을 유도하는 분류모델 2 유도 단계;를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서상기 제1, 2 선택라벨링은 상기 제 1, 2 계층라벨링을 포함하되,상기 제1 ,2 선택라벨링에서의 상기 제1 ,2 계층라벨링 정보 수신은 사용자 또는 컴퓨팅장치에 의해 수행되는특징으로 하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 또는 2항에 있어서,상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 시계열 분할 선택 라벨링 정보를 수신하는 단계를 포함하는 집단지성화알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 또는 2항에 있어서,상기 영상정보는 제2 영상정보이고,상기 제2 선택라벨링 정보 수신 단계는 신체부위별 선택 라벨링 정보를 수신하는 단계를 포함하는 집단지성화알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항 또는 2항에 있어서,상기 영상정보는 NFT가 부여된 영상정보이고, 상기 영상정보의 플랫폼 제공 구조는 사용자 및 참여자 및 기업들이 이익을 창출하고 돈을 벌면서 재미요소를배가하는 플랫폼으로서의 원순환 구조를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법.공개특허 10-2023-0135532-3-청구항 6 제1항 또는 2항에 있어서,상기 영상정보는 제2 영상정보이고,상기 동영상정보에 대한 정보 발신의 단계는 사용자가 행하는 사소한 실수나 치명적인 실수에 대해 보정 및 수행 중지를 경보로 개입하는 단계를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 또는 2항에 있어서,상기 영상정보는 제1 영상정보 또는 제2 영상정보이고,상기 영상정보에 대한 정보 발신의 단계는,제 1 기초 로보틱스 영상정보에 기반하여 GAN 및 GNN에 의해 영상정보를 출력(output) 및 생성하는 제1 로보틱스 영상정보 출력(output) 및 생성 단계;상기 영상정보에 대한 사용자의 제1 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 로보틱스분류모델 유도 단계;상기 영상정보 및 제2 기초 로보틱스 영상정보를 기반하여 영상정보를 출력(output) 및 생성하는 제2 로보틱스영상정보 출력(output) 및 생성단계; 및상기 영상정보에 대한 제2 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제2 로보틱스 분류모델유도 단계;를 포함하되,상기 영상정보는 제2 로보틱스 영상정보이고,상기 영상정보에 대한 정보 발신의 단계는 사용자가 행하는 실수에 대해 보정 동작을 하거나 및 자율동작을 하는 단계를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 기초 영상정보에 기반하여 GNN에 의해 영상정보를 출력(output)하는 제1 영상정보 출력(output) 단계;상기 영상정보에 대한 제1 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 분류모델 유도 단계;상기 영상정보 및 제2 기초 영상정보를 기반하여 영상정보를 출력(output)하는 제2 영상정보 출력(output)단계;및상기 영상정보에 대한 제2 선택라벨링 정보를 수신하여 분류모델을 유도하는 분류모델 2 유도 단계;를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 제1, 2 선택라벨링은 상기 제 1, 2 계층적 라벨링을 포함하되,상기 제1 ,2 선택라벨링에서의 상기 제1 ,2 계층라벨링 정보 수신은 사용자 또는 컴퓨팅장치에 의해 수행되는특징으로 하는 집단지성화 알고리즘을 이용한 정보처리방법.공개특허 10-2023-0135532-4-청구항 10 제8항 또는 제9항에 있어서,상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 시계열 분할 선택 라벨링 정보를 수신하는 단계를 포함하는 집단지성화알고리즘을 이용한 정보처리방법"}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항 또는 제9항에 있어서,상기 영상정보는 제2 영상정보이고,상기 제2 선택라벨링 정보 수신 단계는 신체부위별 선택 라벨링 정보를 수신하는 단계를 포함하는 집단지성화알고리즘을 이용한 정보처리방법"}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항 또는 제9항에 있어서,상기 영상정보는 NFT가 부여된 영상정보이고, 상기 영상정보의 플랫폼 제공 구조는 사용자 및 참여자 및 기업들이 이익을 창출하고 돈을 벌면서 재미요소를배가하는 플랫폼으로서의 원순환 구조를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법"}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8항 또는 제9항에 있어서,상기 영상정보는 제2 영상정보이고,상기 동영상정보에 대한 정보 발신의 단계는 사용자가 행하는 사소한 실수나 치명적인 실수에 대해 보정 및 수행 중지를 경보로 개입하는 단계를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법"}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8항 또는 제9항에 있어서,상기 영상정보는 제1 영상정보 또는 제2 영상정보이고,상기 제1 영상정보 또는 제2 영상정보에 대한 정보 발신의 단계는,제 1 기초 로보틱스 영상정보에 기반하여 GNN에 의해 영상정보를 출력(output)하는 제1 로보틱스 영상정보 출력(output) 단계;상기 영상정보에 대한 사용자의 제1 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 로보틱스분류모델 유도 단계; 상기 영상정보 및 제2 기초 로보틱스 영상정보를 기반하여 영상정보를 출력(output) 하는 제2 로보틱스 영상정보 출력(output) 단계;상기 영상정보에 대한 제2 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제2 로보틱스 분류모델유도 단계;를 포함하되,공개특허 10-2023-0135532-5-상기 영상정보는 제2 로보틱스 영상정보이고,상기 동영상정보에 대한 정보 발신의 단계는 사용자가 행하는 실수에 대해 보정 동작을 하거나 및 자율동작을하는 단계를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 1 기초 영상정보에 기반하여 GAN에 의해 영상정보를 생성하는 제1 영상정보 생성 단계;상기 영상정보에 대한 제1 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 분류모델 유도 단계;상기 영상정보 및 제1 기초영상정보를 기반하여 영상정보를 생성하는 제2 영상정보 생성단계; 및상기 영상정보에 대한 제2 선택라벨링 정보를 수신하여 분류모델을 유도하는 분류모델 2 유도 단계;를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 제1, 2 선택라벨링은 상기 제 1, 2 계층적 라벨링을 포함하되,상기 제1 ,2 선택라벨링에서의 상기 제1 ,2 계층라벨링 정보 수신은 사용자 또는 컴퓨팅장치에 의해 수행되는특징으로 하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제15항 또는 16항에 있어서,상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 시계열 분할 선택 라벨링 정보를 수신하는 단계를 포함하는 집단지성화알고리즘을 이용한 정보처리방법"}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제15항 또는 16항에 있어서,상기 영상정보는 제2 영상정보이고,상기 제2 선택라벨링 정보 수신 단계는 신체부위별 선택 라벨링 정보를 수신하는 단계를 포함하는 집단지성화알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제15항 또는 16항에 있어서,상기 영상정보는 제1 영상정보 또는 제2 영상정보이고,상기 영상정보에 대한 정보 발신의 단계는,제 제 1 기초 로보틱스 영상정보에 기반하여 GAN에 의해 영상정보를 생성하는 제1 로보틱스 영상정보 생성단계;상기 영상정보에 대한 사용자의 제1 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 로보틱스공개특허 10-2023-0135532-6-분류모델 유도 단계;상기 영상정보 및 제2 기초 로보틱스 영상정보를 기반하여 영상정보를 생성하는 제2 로보틱스 영상정보 생성단계; 및상기 영상정보에 대한 제2 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제2 로보틱스 분류모델유도 단계;를 포함하되,상기 영상정보는 제2 로보틱스 영상정보이고,상기 동영상정보에 대한 정보 발신의 단계는 사용자가 행하는 실수에 대해 보정 동작을 하거나 및 자율동작을하는 단계를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제15항 또는 16항에 있어서,상기 영상정보는 NFT가 부여된 영상정보이고, 상기 영상정보의 플랫폼 제공 구조는 사용자 및 참여자 및 기업들이 이익을 창출하고 돈을 벌면서 재미요소를배가하는 플랫폼으로서의 원순환 구조를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제15항 또는 16항에 있어서,상기 영상정보는 제2 영상정보이고,상기 동영상정보에 대한 정보 발신의 단계는 사용자가 행하는 사소한 실수나 치명적인 실수에 대해 보정 및 수행 중지를 경보로 개입하는 단계를 포함하는 집단지성화 알고리즘을 이용한 정보처리방법."}
{"patent_id": "10-2023-0034241", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "집단지성화 알고리즘을 이용한 정보처리방법이 개시된다. 이러한 본 발명은, 제 1 기초 영상정보에 기반하여 GNN 및 GAN에 의해 영상정보를 출력(output) 및 생성하는 제1 영상정보 출력(output) 및 생성 단계; 상기 영상정보에 대한 제1 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 분류모델 유도 단계; 상기 영상정보 및 제2 기초 영상정보를 기반하여 영상정보를 출력(output) 및 생성하는 제2 영상정보 출력(output) 및 생성 단계; 상기 영상 정보에 대한 제2 선택라벨링 정보를 수신하여 분류모델을 유도하는 분류모델 2 유도 단계; 를 포함하여 구성될 수 있다."}
{"patent_id": "10-2023-0034241", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 집단지성화 알고리즘을 이용한 정보처리방법에 관한 것으로, 더욱 상세하게는, 가상의 아바타, 아이 템 및 로보틱스 등의 소프트웨어를 컴퓨팅장치에서 구현하여 사용자 단말에 제공하되, 동영상 정보에 정보처리 라벨을 붙임과 아울러 라벨링된 정보를 인공지능 추론에 적용할 수 있도록 한 집단지성화 알고리즘을 이용한 정 보처리방법에 관한 것이다."}
{"patent_id": "10-2023-0034241", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일반적으로, 웹 공간에 널리 분포하는 사용자들의 집단지성은 그 방대한 규모와 풍부한 지식으로 각광받고 있다. 그러나, 종래의 정보 데이터베이스 기술로는 단순한 아바타, 아이템 및 로보틱스 등의 소프트웨어를 컴퓨팅장치 에서 구현하는 것이 곤란하기 때문에 집단 지성의 장점을 새로운 지식 서비스로 연결하는 데에는 한계가 있다. 따라서, 가상의 아바타 및 아이템 및 로보틱스 등의 소프트웨어를 컴퓨팅장치에서 구현하여 사용자단말에 제공 하기 위한 집단지성화 알고리즘을 이요한 정보처리방법에 대한 기술이 시급히 요구되고 있다. 선행기술문헌특허문헌 (특허문헌 0001) KR 등록특허공보 제10-1859198호(2018.05.11) (특허문헌 0002) KR 공개특허공보 제10-2010-0073793호(2010.07.01) 비특허문헌 (비특허문헌 0001) Action Genome : Actions as Composition of Spatio-temporal Scene Graphs (비특허문헌 0002) Semi-Supervised Classification with Graph Convolution Networks (비특허문헌 0003) Graph Autoencoder for Graph Compression and Representation Learning"}
{"patent_id": "10-2023-0034241", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "따라서, 이러한 문제점을 해결하기 위한 본 발명의 목적은, 가상의 아바타, 아이템 및 로보틱스 등의 소프트웨 어를 컴퓨팅장치에서 구현하여 사용자 단말에 제공하되, 동영상 정보에 정보처리 라벨을 붙임과 아울러 라벨링 된 정보를 인공지능 추론에 적용할 수 있도록 한 집단지성화 알고리즘을 이용한 정보처리방법을 제공하기 위한 것이다. 또한, 본 발명은, 아바타 및/또는 디지털카데바 제공 방법을 이용하여 사용자들에게 메타버스상에서 가상수술 오디션 게임 , 가상 댄스 오디션, 축구 게임 등을 진행할 수 있도록 하고, 실제 현실의 동영상 정보 및/또는 가상 시뮬레이셩의 동작을 통해 수집된 동작 패턴을 군집화하여 인공지능 모델을 제공하기 위한 것이다. 아울러, 본 발명은, 더욱 고도화된 인공지능을 통해 사용자가 행하는 사소한 실수나 치명적인 실수를 보정 및 수행 중지 경보로 개입하거나 자율동작을 함으로서 현실세계에서 사용자의 행동에 도움을 줄 수 있도록 하기 위 한 것이다. 그리고, 본 발명은, 아바타의 생성 및 합성 및 라벨링 기술은 가상 수술 및 가상 댄스 오디션 게임 및 가상 비 행 게임, 각종 전투 게임 등에 응용이 가능하고 상기 아바타의 동작은 디지털트윈으로 로보틱스 프로그래밍하여 제공하기 위한 것이다."}
{"patent_id": "10-2023-0034241", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명에 따른 집단지성화 알고리즘을 이용한 정보처리방법은, 제 1 기초 영상정보 에 기반하여 GNN 및 GAN에 의해 영상정보를 출력(output) 및 생성하는 제1 영상정보 출력(output) 및 생성 단계; 상기 영상정보에 대한 제1 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 분류모델 유도 단계; 상 기 영상정보 및 제2 기초 영상정보를 기반하여 영상정보를 출력(output) 및 생성하는 제2 영상정보 출력 (output) 및 생성 단계; 상기 영상정보에 대한 제2 선택라벨링 정보를 수신하여 분류모델을 유도하는 분류모델 2 유도 단계; 를 포함하여 구성될 수 있다. 상기 제1, 2 선택라벨링은 상기 제 1, 2 계층라벨링을 포함하되, 상기 제1 ,2 선택라벨링에서의 상기 제1 ,2 계 층라벨링 정보 수신은 사용자 또는 컴퓨팅장치에 의해 수행될 수 있다. 상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 시계열 분할 선택 라벨링 정보를 수 신하는 단계를 포함할 수 있다. 상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 신체부위별 선택 라벨링 정보를 수 신하는 단계를 포함할 수 있다. 상기 영상정보는 NFT가 부여된 영상정보이고, 상기 영상정보의 플랫폼 제공 구조는 사용자 및 참여자 및 기업들 이 이익을 창출하고 돈을 벌면서 재미요소를 배가하는 플랫폼으로서의 원순환 구조를 포함할 수 있다.상기 영상정보는 제2 영상정보이고, 상기 동영상정보에 대한 정보 발신의 단계는 사용자가 행하는 사소한 실수 나 치명적인 실수에 대해 보정 및 수행 중지를 경보로 개입하는 단계를 포함할 수 있다. 상기 영상정보는 제1 영상정보 또는 제2 영상정보이고, 상기 영상정보에 대한 정보 발신의 단계는, 제 1 기초 로보틱스 영상정보에 기반하여 GAN 및 GNN에 의해 영상정보를 출력(output) 및 생성하는 제1 로보틱스 영상정보 출력(output) 및 생성 단계; 상기 영상정보에 대한 사용자의 제1 로보틱스 선택라벨링 정보를 수신하여 분류모 델을 유도하는 제1 로보틱스 분류모델 유도 단계; 상기 영상정보 및 제2 기초 로보틱스 영상정보를 기반하여 영상정보를 출력(output) 및 생성하는 제2 로보틱스 영상정보 출력(output) 및 생성단계; 및 상기 영상정보에 대한 제2 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제2 로보틱스 분류모델 유도 단계; 를 포 함하되, 상기 영상정보는 제2 로보틱스 영상정보이고, 상기 영상정보에 대한 정보 발신의 단계는 사용자가 행하 는 실수에 대해 보정 동작을 하거나 및 자율동작을 하는 단계를 포함할 수 있다. 본 발명에 따른 집단지성화 알고리즘을 이용한 정보처리방법에 의하면, 제 1 기초 영상정보에 기반하여 GNN에 의해 영상정보를 출력(output)하는 제1 영상정보 출력(output) 단계; 상기 영상정보에 대한 제1 선택라벨링 정 보를 수신하여 분류모델을 유도하는 제1 분류모델 유도 단계; 상기 영상정보 및 제2 기초 영상정보를 기반하여 영상정보를 출력(output)하는 제2 영상정보 출력(output)단계; 및 상기 영상정보에 대한 제2 선택라벨링 정보를 수신하여 분류모델을 유도하는 분류모델 2 유도 단계; 를 포함하여 구성될 수 있다. 상기 제1, 2 선택라벨링은 상기 제 1, 2 계층적 라벨링을 포함하되, 상기 제1 ,2 선택라벨링에서의 상기 제1 ,2 계층라벨링 정보 수신은 사용자 또는 컴퓨팅장치에 의해 수행될 수 있다. 상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 시계열 분할 선택 라벨링 정보를 수 신하는 단계를 포함할 수 있다. 상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 신체부위별 선택 라벨링 정보를 수 신하는 단계를 포함할 수 있다. 상기 영상정보는 NFT가 부여된 영상정보이고, 상기 영상정보의 플랫폼 제공 구조는 사용자 및 참여자 및 기업들 이 이익을 창출하고 돈을 벌면서 재미요소를 배가하는 플랫폼으로서의 원순환 구조를 포함할 수 있다. 상기 영상정보는 제2 영상정보이고, 상기 동영상정보에 대한 정보 발신의 단계는 사용자가 행하는 사소한 실수 나 치명적인 실수에 대해 보정 및 수행 중지를 경보로 개입하는 단계를 포함할 수 있다. 상기 영상정보는 제1 영상정보 또는 제2 영상정보이고, 상기 제1 영상정보 또는 제2 영상정보에 대한 정보 발신 의 단계는, 1 기초 로보틱스 영상정보에 기반하여 GNN에 의해 영상정보를 출력(output)하는 제1 로보틱스 영상 정보 출력(output) 단계; 상기 영상정보에 대한 사용자의 제1 로보틱스 선택라벨링 정보를 수신하여 분류모델 을 유도하는 제1 로보틱스 분류모델 유도 단계; 상기 영상정보 및 제2 기초 로보틱스 영상정보를 기반하여 영 상정보를 출력(output) 하는 제2 로보틱스 영상정보 출력(output) 단계; 상기 영상정보에 대한 제2 로보틱스 선 택라벨링 정보를 수신하여 분류모델을 유도하는 제2 로보틱스 분류모델 유도 단계; 를 포함하되, 상기 영상정보 는 제2 로보틱스 영상정보이고, 상기 동영상정보에 대한 정보 발신의 단계는 사용자가 행하는 실수에 대해 보정 동작을 하거나 및 자율동작을 하는 단계를 포함하여 구성될 수 있다. 본 발명에 따른 집단지성화 알고리즘을 이용한 정보처리방법에 의하면, 제 1 기초 영상정보에 기반하여 GAN에 의해 영상정보를 생성하는 제1 영상정보 생성 단계; 상기 영상정보에 대한 제1 선택라벨링 정보를 수신하여 분 류모델을 유도하는 제1 분류모델 유도 단계; 상기 영상정보 및 제1 기초영상정보를 기반하여 영상정보를 생성하 는 제2 영상정보 생성단계; 및 상기 영상정보에 대한 제2 선택라벨링 정보를 수신하여 분류모델을 유도하는 분 류모델 2 유도 단계; 를 포함하여 구성될 수 있다. 상기 제1, 2 선택라벨링은 상기 제 1, 2 계층적 라벨링을 포함하되, 상기 제1 ,2 선택라벨링에서의 상기 제1 ,2 계층라벨링 정보 수신은 사용자 또는 컴퓨팅장치에 의해 수행될 수 있다. 상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 시계열 분할 선택 라벨링 정보를 수 신하는 단계를 포함할 수 있다. 상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 신체부위별 선택 라벨링 정보를 수 신하는 단계를 포함할 수 있다. 상기 영상정보는 제1 영상정보 또는 제2 영상정보이고, 상기 영상정보에 대한 정보 발신의 단계는, 제 제 1 기 초 로보틱스 영상정보에 기반하여 GAN에 의해 영상정보를 생성하는 제1 로보틱스 영상정보 생성 단계; 상기 영 상정보에 대한 사용자의 제1 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 로보틱스 분류모델 유도 단계; 상기 영상정보 및 제2 기초 로보틱스 영상정보를 기반하여 영상정보를 생성하는 제2 로보틱스 영상 정보 생성단계; 및 상기 영상정보에 대한 제2 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제2 로보틱스 분류모델 유도 단계; 를 포함하되, 상기 영상정보는 제2 로보틱스 영상정보이고, 상기 동영상정보에 대한 정보 발신의 단계는 사용자가 행하는 실수에 대해 보정 동작을 하거나 및 자율동작을 하는 단계를 포함할 수 있다. 상기 영상정보는 NFT가 부여된 영상정보이고, 상기 영상정보의 플랫폼 제공 구조는 사용자 및 참여자 및 기업들 이 이익을 창출하고 돈을 벌면서 재미요소를 배가하는 플랫폼으로서의 원순환 구조를 포함할 수 있다. 상기 영상정보는 제2 영상정보이고, 상기 동영상정보에 대한 정보 발신의 단계는 사용자가 행하는 사소한 실수 나 치명적인 실수에 대해 보정 및 수행 중지를 경보로 개입하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0034241", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기에서 설명한 본 발명의 집단지성화 알고리즘을 이용한 정보처리방법에 의하면, 사용자에게 아이템을 아바타 에게 대입 및 다양한 조합으로 생성 및/또는 출력(Output)해 보는 서비스를 제공할 수 있다. 또한, 본 발명에 의하면, 동영상 정보에 라벨링하는 기술을 사용하여 인공지능의 추론 능력을 향상시킬 수 있으 며, 이러한 방식으로 처리된 동영상 정보를 인공지능 추론 알고리즘에 반복적으로 적용하는 기술을 개발하여 인 공지능의 추론 능력을 향상시킬 수 있다 아울러, 본 발명에 의하면, 병원게임에서 치과의사 사용자에게는 의료기기, 의료장비, 재료 등의 아이템을 디지 털카데바(환자의 아바타)의 얼굴 및 몸에 대입 및 다양한 조합으로 생성 및/또는 출력(Output)해 보는 서비스 를 제공할 수 있다. 그리고, 본 발명에 의하면, 메타버스 게임에서 사용자에게는 화장품, 패션 아이템 및 의류를 나의 얼굴 및 몸에 대입 및 다양한 조합으로 생성 및 합성해 보는 서비스를 제공할 수 있다. 또한, 본 발명에 의하면, 아이템을 제공하는 기업에게는 마케팅 광고 플랫폼 및 온라인 구매 연결 플랫폼을 제 공할 수 있으며, 인플루언서에게는 자신의 다양한 이미지 및 영상을 SNS 경유하여 구매로 유도시키고, 이 를 트래킹 하는 일련의 마케팅 활동을 수익으로 환원해 주는 플랫폼을 제공할 수 있다. 아울러, 본 발명에 의하면, 메타버스의 데이터는 로보틱스의 데이터로 사용되어 아바타에 대한 디지털트윈이 이 루어질 수 있으며, 아바타와 아이템은 메타버스 국가 플랫폼에 제공될 수 있는 등 다양한 효과를 구현할 수 있 다."}
{"patent_id": "10-2023-0034241", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한 다(comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성요소 외에 하나 이상의 다른 구성요소의 존재 혹 은 추가를 배제하지 않는다. 명세서 전체에 걸쳐 동일한 도면 부호는 동일한 구성 요소를 지칭하며, \"및/또는\" 은 언급된 구성요소들의 각각 및 하나 이상의 모든 조합을 포함한다. 비록 \"제1\", \"제2\" 등이 다양한 구성요소 들을 서술하기 위해서 사용되나, 이들 구성요소들은 이들 용어에 의해 제한되지 않음은 물론이다. 이들 용어들 은 단지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사용하는 것이다. 따라서, 이하에서 언급되는 제1 구성요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있음은 물론이다. 명세서에서 사용되는 “모듈”이라는 용어는 소프트웨어, FPGA 혹은 ASIC과 같은 하드웨어 구성요소를 의미하며, “모듈”은 어떤 역할들을 수행한다. 그렇지만 “모듈”은 소프트웨어 혹은 하드웨어에 한정되는 의 미는 아니다. “모듈”은 어드레싱하는 저장 매체에 있도록 구성될 수도 있고 하나 혹은 그 이상의 프로세서들 을 재생시키도록 구성될 수도 있다. 본 발명의 실시예에서, “모듈”은 소프트웨어 구성요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소 들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그 램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로 코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이 블들, 어레이들 및 변수들을 포함한다. 구성요소들과 “모듈”들 안에서 제공되는 기능은 더 작은 수의 구성요 소들 및 “모듈”들로 결합되거나 추가적인 구성요소들과 “모듈”들로 더 분리된다. 본 명세서에서, 컴퓨터는 적어도 하나의 프로세서를 포함하는 모든 종류의 하드웨어 장치를 의미하는 것이고, 실시 예에 따라 해당 하드웨어 장치에서 동작하는 소프트웨어적 구성도 포괄하는 의미로서 이해된다. 본 발명의 실시예에서, 컴퓨터는 스마트폰, 태블릿 PC, 데스크톱, 노트북 및 각 장치에서 구동되는 사용자 클 라이언트 및 애플리케이션을 모두 포함하는 의미로서 이해될 수 있으며, 또한 이에 제한되는 것은 아니다. 본 명세서에서 설명되는 각 단계들은 컴퓨터에 의하여 수행되는 것으로 설명되나, 각 단계의 주체는 이에 제한 되는 것은 아니며, 실시 예에 따라 각 단계들의 적어도 일부가 서로 다른 장치에서 수행될 수도 있다. 본 발명에서의 컴퓨팅 장치는 GNN(Graph Neural Network), GAN(Generative Adversarial Network) 을 이용한다. 인공지능 알고리즘으로 GAN과 GNN의 조합이 있을 수 있고 GAN을 제외한 GNN 단독 적용이 있을 수 있고, GNN을 제외한 GAN 단독 적용이 있을 수 있다. GAN을 단독으로 사용하는 경우에는, 'GNN 회귀모델 1형' 및 'GNN 회귀모 델 2형'을 사용하지 않고 속성과 타겟속성의 예측값을 구할때, 딥러닝 및 연관규칙을 사용한다. GAN은 정지영상 이나 동영상의 표현이나 화질의 자연스러움이나 정교함을 보강한다. 다음 동작의 예측을 위해서는 동작 패턴의 군집을 연관규칙 등으로 추론한다. 본 발명의 일 실시예에서, 컴퓨팅장치는 Cloud Computing, Grid Computing, Server-Based Computing , Utility Computing, Network Computing, Qumntum Cloud Computing 등이 있을 수 있다. 이하, 본 발명의 집단지성화 알고리즘을 이용한 정보처리방법에 대한 바람직한 일 실시예를 첨부된 도면을 참조 하여 상세히 설명하면 다음과 같다. 1.제1 기초 영상정보 기반하여 (GNN 및 GAN 중 적어도 어느 하나에 의해) 영상정보를 출력(output) 혹은 생성하 거나, 출력(output) 및 생성하는 제1 영상정보 출력(output) 및/또는 생성 단계 도 20 및 도 23을 참조하면, 시각세트장치가 연결된 사용자단말을 통하여 도 1 내지 도 2의 컴퓨팅 장치는 실제 현실의 데이터를 획득한다. 실제 현실의 영상정보를 수집하는 영상 정보 수집장치를 '시각세트장치'로 정의한다. 도 20의 시각세트장치는 기초 영상정보를 실시예와 같은 방식으로 획득한다. 도 9의 실제 현실 데 이터는 기초 영상정보이다. 도 9은 K1개의 군집이다. 도 23을 참조하면, '제1 기초 영상정보, 제2 기초 영상정보, 제 3 기초 영상정보…'는 기초 영상정 보이고 1명의 사용자로부터 획득한 annotation 단계의 복수의 입력(input) 데이터이다. 제1 영상정보는 제1 기초 영상정보로부터 생성 및/또는 출력(output)된 가상의 아바타 및 아이템 이다. 제1 영상정보의 정지영상정보는 제1 속성이고 동영상정보는 제1 타겟속성이다. 제1 영상정보는 계층적 군집화 라벨링에 의해 군집화된 제1 속성 및 제1 타겟속성이 된다. 제1 속성은 도 8의 정지영상정보의 출력에서 생성 및/또는 출력(output)되고 제1 타겟속성은 도 7의 동영상정보의 출력에서 생성 및/또는 출력(output)된다. 본 발명의 일 실시예에서, 시각세트장치는 라이다, 아이트래커, 모션 캡쳐 및 모션트래커, 의료장비(CT, 스캐너, MRI, 의료용 초음파 등) 등을 포함하며, 수집된 영상정보는 합병하여 사용될 수 있다. 본 발명의 일 실시예에서, 도 9의 실제 현실의 데이터를 이용해 생성데이터(Augmentation 데이터) 제공한 다. 본 발명의 일 실시예는 가상 수술 시뮬레이션 및 가상 치아삭제 시뮬레이션에 적은 양의 실제 수술 수집 데이터 (실제 현실 데이터)를 사용하여 성능을 극대화 하는데 1차 목표가 있다. 이를 위해, 생성되거나 출력(output)된 가상 디지털카데바 생성데이타(Augmentation 데이터)를 트레이닝 및 시 뮬레이션 단계에서 제공하고, 이를 의사가 선택라벨링을 하는 방식으로 인공지능을 지도학습시킬 수 있다. 본 발명의 일 실시예에서, 지멘스 헬시니어스가 마이크로소프트 홀로렌즈 2를 활용해 개발한 의료 보조 애플리 케이션 시네마틱 리얼리티는 의료용 CT, MRI로 얻어지는 복셀 데이터를 랜더링하는 기능을 가지고 있다. 본 발명의 일 실시예에서, 시네마틱 리얼리티로 랜더링 된 데이터는 디지털카데바 및 3D 프린팅 인공카데바를 제작을 위한 데이터 셋으로 사용된다. 위 복셀 데이터는 GNN 형태의 포인트 클라우드 데이터와 합병하여 사용된 다. 본 발명의 일 실시예에서, 도 7 내지 도8의 애플리케이션 화면(700, 800)에서 출력되는 시각데이터는 사용자 단 말이 제공하는 아바타(인간)의 가상현실, 증강현실, 확장현실에서의 동작 화면이다. 본 발명의 일 실시예에서, 디지털카데바는 환자의 아바타이다. 디지털카데바의 단점인 균일한 디지털 속성 상 개별 환자의 상이한 신체구조(변이)를 반영하기 힘든 것을 보완하기 위해 의료 현장에서의 정보 수집 장치(CT, X-RAY, 초음파 장치, 구강스캐너 등)로 수집된 의료정보와 전문인력의 경험 및 지식 등을 활용한다. 본 발명의 일 실시예에서, 이런 종합적인 정보를 사용하여 특정 환자의 변이가 반영된 디지털카데바 및 인공카 데바 사용을 병행하여 VR 및 3D 시뮬레이터 가상 치료 및 가상 수술을 진행한다. 도 1을 참조하면, 본 발명의 실시 예에 따른 가상 아바타에 대한 GAN에 의한 생성 및/또는 GNN에 의한 출력 (output) 플랫폼 제공 시스템은 서비스 제공 장치인 컴퓨팅장치, 사용자 단말 및 외부 서버를 포함한다. 여기서, 도 1에 도시된 GAN 및/또는 GNN을 이용한 가상 아바타 생성 및/또는 출력(output)플랫폼 제공 시스템 은 일 실시 예에 따른 것이고, 그 구성 요소가 도 1에 도시된 실시 예에 한정되는 것은 아니며, 필요에 따라 부 가, 변경 혹은 삭제된다. 도 16을 참조하면,서비스 제공 장치인 컴퓨팅장치는 사용자로부터 사용자 정보를 획득하고, 획득된 사용자 정보를 기초로, 가상의 아바타를 GAN에 의해 생성하거나 GNN에 의해 출력(output)하고, 상기 아바 타를 메타버스 상에 제공하고, 상기 아바타를 이용한 메타버스 게임 등을 진행한다. 본 발명의 실시예에서, 메타버스 국가에서 할 수 있는 게임의 아바타를 제작한다. 재판 게임, 경찰게임, 소방관 게임, 예술품 창작 게임, 농업게임, 무역 게임, 토지개발 게임, 건축게임, 금융투자 게임, 에너지 발전 게임, 병원 게임, 치과 병원 게임, 남녀 미팅 게임, 회사 운영 게임, 국가기관 운영 게임, 전쟁 및 전투 게임, 슈팅 게임, 전략 게임, 아케이드 게임, 스포츠 게임, 오디션 게임 등이 메타버스 국가 내에서 일어날 수 있는 경쟁 게임의 일부분이다 . 메타버스 병원 게임은 메타버스 국가 게임의 일부 이고 디지털카데바는 아바타의 일종이다. 사용자 단말은 네트워크를 통해 웹사이트에 접속하며, 서비스 제공 장치인 컴퓨팅장치로부터 GAN 및/ 또는 GNN을 이용한 가상 아바타 및 아이템의 생성 및/또는 출력(output) 플랫폼을 제공받을 수 있다. 브라우저를 실행하는 과정에서 서비스 제공 장치인 컴퓨팅장치로부터 제공되는 GAN 및/또는 GNN을 이용한 가상 아바타의 생성 및/또는 출력(output) 플랫폼을 제공받을 수 있다. 본 발명의 일 실시예에서, 사용자 단말은 사용자 단말의 적어도 일부분에 도 20 내지 도 21의 디스플 레이 장치(를 구비하는 스마트폰, 태플릿 PC, 데스크톱 및 노트북, VR 시뮬레이터, 로봇 중 적어도 하나를 포함 하며, 이에 한정되지 않는다. 사용자 단말은 서비스 제공 장치인 컴퓨팅장치가 제공하는 각종 서비스 및 플랫폼에 관한 어플리케이 션을 다운로드, 설치 및 실행함으로써, 어플리케이션을 통해 서비스 제공 장치인 컴퓨팅장치가 제공하는 각종 서비스를 제공받을 수 있다. 이를 위해, 사용자 단말은 어플리케이션의 구동이 가능한 운영체제를 탑재한다. 사용자 단말은 어플 리케이션의 구동이 가능한 다른 범용적인 장치들이 적용된다. 본 발명의 일 실시예에서, 사용자 단말은 어플리케이션 외에 웹 기반으로도 서비스를 제공하며, 사용자 단 말이 서비스를 제공하는 방법은 특정한 형식으로 제한되지 않는다. 외부 서버는 네트워크를 통해 서비스 제공 장치인 컴퓨팅장치와 연결될 수 있으며, 서비스 제공 장치 인 컴퓨팅장치가 GAN 및/또는 GNN을 이용한 가상 아바타 의 생성 및/또는 출력(output)플랫폼 제공 방법 을 수행하기 위한 각종 정보를 저장 및 관리한다. 또한, 외부 서버는 서비스 제공 장치인 컴퓨팅장치가 GAN 및/또는 GNN을 이용한 가상 아바타 의 생 성 및/또는 출력(output)플랫폼 제공 방법을 수행함에 따라 생성 및/또는 출력(output)되는 각종 정보 및 데 이터를 제공받아 저장한다. 본 발명의 일 실시예에서, 외부 서버는 서비스 제공 장치인 컴퓨팅장치 외부에 별도로 구비되는 저장 서버이다. 도 2을 참조하여, 서비스 제공 장치인 컴퓨팅장치의 하드웨어 구성에 대해 설명하도록 한다. 도 2는 본 발명의 실시 예에 따른 서비스 제공 장치인 컴퓨팅장치의 하드웨어 구성도이다. 도 2을 참조하면, 본 발명의 실시 예에 따른 서비스 제공 장치인 컴퓨팅장치(이하, 컴퓨팅 장치)는 하나 이상의 프로세서, 프로세서에 의하여 수행되는 컴퓨터 프로그램을 로드(Load)하는 메모리, 버스, 통신 인터페이스 및 컴퓨터 프로그램을 저장하는 스토리지를 포함한다. 여기서, 도 2에는 본 발명의 실시예와 관련 있는 구성요소들만 도시되어 있다."}
{"patent_id": "10-2023-0034241", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "따라서, 본 발명이 속한 기술분야의 통상의 기술자라면 도 2에 도시된 구성요소들 외에 다른 범용적인 구성 요 소들이 더 포함될 수 있음을 알 수 있다. 본 발명의 일 실시예에서, 프로세서는 CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit) 혹은 본 발명의 기술 분야에 잘 알려진 임의의 형 태의 프로세서를 포함하여 구성된다. 또한, 프로세서는 본 발명의 실시예들에 따른 방법을 실행하기 위한 적어도 하나의 애플리케이션 혹은 프 로그램에 대한 연산을 수행하며, 컴퓨팅 장치는 하나 이상의 프로세서를 구비한다. 본 발명의 일 실시예에서, 프로세서는 프로세서 내부에서 처리되는 신호(또는, 데이터)를 일시적 및/ 또는 영구적으로 저장하는 램(RAM: Random Access Memory, 미도시) 및 롬(ROM: Read-Only Memory, 미도시)을 더 포함한다. 또한, 프로세서는 그래픽 처리부, 램 및 롬 중 적어도 하나를 포함하는 시스템온칩(SoC: system on chip) 형태로 구현된다. 메모리는 각종 데이터, 명령 및/또는 정보를 저장한다. 메모리는 본 발명의 본 발명의 일 실시예에 따른 방법/동작을 실행하기 위하여 스토리지로부터 컴퓨터 프로그램을 로드한다. 메모리에 컴퓨터 프로그램이 로드되면, 프로세서는 컴퓨터 프로그램을 구성하는 하나 이상 의 인스트럭션들을 실행함으로써 상기 방법/동작을 수행한다. 메모리는 RAM과 같은 휘발성 메모리로 구현될 수 있을 것이나, 본 개시의 기술적 범위가 이에 한정되는 것 은 아니다. 버스는 컴퓨팅 장치의 구성 요소 간 통신 기능을 제공한다. 버스는 주소 버스(address Bus), 데 이터 버스(Data Bus) 및 제어 버스(Control Bus) 등 다양한 형태의 버스로 구현된다. 통신 인터페이스는 컴퓨팅 장치의 유무선 인터넷 통신을 지원한다. 또한, 통신 인터페이스는 인 터넷 통신 외의 다양한 통신 방식을 지원할 수도 있다. 이를 위해, 통신 인터페이스는 본 발명의 기술 분야에 잘 알려진 통신 모듈을 포함하여 구성된다. 몇몇 실시예에서, 통신 인터페이스는 생략될 수도 있다.스토리지는 컴퓨터 프로그램을 비 임시 적으로 저장한다. 컴퓨팅 장치를 통해 GAN을 이용한 가상 아바타의 생성 및/또는 GNN에 의한 출력(output)플랫폼 제공 방법 을 수행하는 경우, 스토리지는 GAN을 이용한 가상 아바타 GAN에 의한 생성 및/또는 GNN에 의한 출력 (output)플랫폼 제공 방법을 제공하기 위하여 필요한 각종 정보를 저장한다. 스토리지는 ROM(Read Only Memory), EPROM(Erasable Programmable ROM), EEPROM(Electrically Erasable Programmable ROM), 플래시 메모리 등과 같은 비휘발성 메모리, 하드 디스크, 착탈형 디스크, 혹은 본 발명이 속하는 기술 분야에서 잘 알려진 임의의 형태의 컴퓨터로 읽을 수 있는 기록 매체를 포함하여 구성된다. 컴퓨터 프로그램은 메모리에 로드될 때 프로세서로 하여금 본 발명의 본 발명의 일 실시예에 따 른 방법/동작을 수행하도록 하는 하나 이상의 인스트럭션들을 포함한다. 즉, 프로세서는 상기 하나 이상의 인스트럭션들을 실행함으로써, 본 발명의 본 발명의 일 실시예에 따른 상기 방법/동작을 수행한다. 도 16은 본 발명의 실시 예에 따른GNN 및/또는 GAN을 이용한 가상 아바타 및 아이템의 생성 및/또는 출력 (output) 플랫폼 제공 방법을 나타내는 도면이다. 도 16에서 컴퓨터 프로그램은 사용자로부터 사용자 정보를 획득하는 단계와, 획득된 사용자 정보를 기초로, 가상의 아바타 및 아이템을 생성하거나 출력(output)하는 단계와, 상기 아바타를 메타버스 상에 제공하는 단계와, 상기 아바타를 이용한 가상 게임 등을 진행하는 단계를 포함하는 가상 아바타의 생성 및/또는 출력(output)에 관한 플랫폼 제공 방법을 수행하도록 하는 하나 이상의 인스트럭션을 포함한다. 본 발명의 실시예와 관련하여 설명된 방법 혹은 알고리즘의 단계들은 하드웨어로 직접 구현되거나, 하드웨어에 의해 실행되는 소프트웨어 모듈로 구현되거나, 혹은 이들의 결합에 의해 구현된다. 본 발명의 일 실시예로, 소프트웨어 모듈은 RAM(Random Access Memory), ROM(Read Only Memory), EPROM(Erasable Programmable ROM), EEPROM(Electrically Erasable Programmable ROM), 플래시 메모리(Flash Memory), 하드 디스크, 착탈형 디스크, CD-ROM, 혹은 본 발명이 속하는 기술 분야에서 잘 알려진 임의의 형태의 컴퓨터 판독가능 기록매체에 상주할 수도 있다. 본 발명의 구성 요소들은 하드웨어인 컴퓨터와 결합되어 실행되기 위해 프로그램(혹은 애플리케이션)으로 구현 되어 매체에 저장된다. 본 발명의 구성 요소들은 소프트웨어 프로그래밍 혹은 소프트웨어 요소들로 실행될 수 있다. 본 발명의 일 실시예로, 데이터 구조, 프로세스들, 루틴들 혹은 다른 프로그래밍 구성들의 조합으로 구현되는 다양한 알고리즘을 포함하여, C, C++, 자바(Java), 어셈블러(assembler) 등과 같은 프로그래밍 혹은 스크립팅 언어로 구현된다. 기능적인 측면들은 하나 이상의 프로세서들에서 실행되는 알고리즘으로 구현된다. 도 16을 참조하면, 컴퓨팅 장치는 사용자로부터 사용자 정보를 획득한다. 사용자 정보는 성별, 나이, 체형, 인종, 사용자의 얼굴 이미지를 포함하나, 이에 한정되는 것은 아니다. 획득된 사용자 정보를 기초로 가상의 아바타 및 아이템을 생성 혹은 출력한다.아바타를 메타버스 상에 지공하고, 아바타를 이용해 각종 게임을 진행한다. 컴퓨팅 장치는 획득된 사용자 정보를 기초로 GAN을 포함하나, 도 17을 참조하면, 동영상이나 정지영상 속 의 객체는 노드(x1~x4, z1~z4)로 표현된다. 각 객체들은 서로 연관되어 있고, 그 관계가 상호영향을 주는 시계열적인 움직임 패턴이 있다. 입력층( 1701)와 출력층는 다수의 층(layer)이 겹쳐있고 GNN은 입력층와 출력층의 중간에 존 재하는 감쳐진 층이다. 입력(Input)이 들어가면 다음 출력(output)이 예측된다. 기존의 GAN은 3D voxel 방식을 이용하였다. 즉, 공간을 3D voxel화 하는 경우 X*, Y*, Z*, 4차원(dimension)의 정보가 수백 메가바이트에 달하므로, 매우 많은 HW, GPU, 메모리 리소스를 필요로 하고, 트레이닝 시간이 매우 많이 소요되는 문제가 있다. 이러한 문제로 인해 최근에는 포인트 클라우드 방식을 주로 이용하고 있다. 포인트 클라우드는 라이다(Lidar) 등을 이용하여 실물 공간 측정이 가능하고, xyz 좌표의 상대값을 물리적으로 측정 및 자료구조화하는 것이 가능하여 3D voxel 방식에 비하여 효과적인 측면이 있다. 그러나, 포인트(point)는 정렬되어 있지 않고, 정형화되어 있지 않아, 인공지능에게 객체의 특성을 매우 일부만 표현해주는 단점이 있어, 상대적인 값, 특징(feature)에 대한 정렬된 정보를 표현할 필요가 있다. 본 발명의 실시 예에 따른 GAN은 동영상, 포즈, 움직임의 특성에 대한 정보를 특징 내에 표현하기 위해, 포인트 와 포인트의 연결에 있어서, 관절의 특성(본 발명의 일 실시예에서, 안으로만 접힐 수 있음, 각도, 거리, 랜드 마크 포인트(land mark point)를 추가로 표현한다. 본 발명의 일 실시예에서, 포인트 클라우드는 필수적 특징을 벗어나지 않는 범위에서 포인트 클라우드는 다른 자료 구조로 구체화 될 수 있다.즉, 본 발명의 실시 예에 따른 GAN은 관절, 구조물의 특성을 벡터(vector)적으로 표현하고, 포인트 클라우드에 서 GNN(Graph Neural Network) 형태로의 치환된 자료구조를 가져갈 수 있다. 또한, 3D 공간, 3D motion, 체형, 동작에 대한 GAN 활용에 있어, 공간의 정보를 포인트가 아닌 복수의 포인트가 결합된 오브젝트(object)로 생성하며, 이를 GNN 형태로 자료구조화 하여 처리한다. GNN 형태로 처리함에 있어 부가 메타(meta) 정보를 입력값(input)의 또 다른 특징(feature)으로 사용하고 있으 며, 특징의 형태가 상이한 경우, 이를 단순하게 변경할 수 없으므로, layer를 나누어 합병(merge)하여 사용한다. 이와 같이, 합병하여 또 다른 입력값으로 사용하는 메타 정보는 사용자 정보 및 아이템 정보를 포함한다. 메타 정보는 unsupervised GAN의 트레이닝에 있어, supervised 라벨의 보완 정보로 사용되며, 조건 (Conditional) 정보로 사용된다. 해당 메타 정보는 각종 시각적 트레이닝 시 유사 정도를 GAN이 기억하게 되고, 향후 특정 속성 정보가 바뀔때, 시각정보가 그에 맞추어 가변적으로 인위적 개입을 함에 있어 도움을 주는 입력값 정보로 활용된다. 본 발명의 실시예에서, 근육량 수치를 늘리거나, 연령을 낮추는 경우 생성된 가상의 아바타의 모양은 해당 값에 따라 달라지게 될 수 있는 것이다. GNN는 특정 파라미터간 매핑된 데이터를 기초로 모델링된 모델링 데이터를 이용하여, 모델링 데이터 간의 유사 도와 특징점을 도출하는 방식으로 구현된 인공신경망 구조를 나타낼 수 있다. 여기에서 망라된 알고리즘 이외에 도 다른 것의 사용도 가능하고 언급된 알고리즘에 국한하지 않는다. 본 발명의 일 실시예에서, 사용자 정보는 얼굴 및 몸의 형태 및 색상, 연령, 성별, 헤어, 인종, fat 정도, 근육 질 정도, 기타 각종 카테고리 정보, 숫자(numeric) 정보, 기타 사용자 속성 정보를 포함하고, 아이템 정보는 브 랜드, 생성자ID, 광고주ID, NFT ID, 상품그룹ID, 기타 아이템 속성 정보를 포함한다. 디지털카데바에서 활용되는 경우는 각 부위명, 혈액형, 나이, 성별, 발병종류, 진행상태 등의 정보이다. 도 18을 참조하면, 컴퓨팅 장치는Conditional GAN의Condition 메타 정보를 수정하여, 아바타의 날 씬해지기, 근육맨 등 체형특성정보를 수정한다. 도 18을 참조하면, 다양한 게임에서 메타정보의 수정 이 가능하다. 본 발명의 일 실시예에서, 댄싱 퍼포먼스 및 가상 수술, 가상 축구게임, 가상 전투기 조정 등으로 응용된다. 본 발명의 일 실시예에서, 디지털카데바는 치과 수술시 보철, 임플란트 등이 교체 가능한 외부 객체 일 수 있으 며, 이를 교체하여 수술 전 시뮬레이션 해 볼 수 있으며, 성형에서는 성형 후 시뮬레이션으로, 일반 수술에서는 3D 입체적인 크기와 구조에 따른 물리적 결합 시뮬레이션 용도로 활용해 볼 수 있다. 이를 통해, 기 학습된 객체에 대한 특성(본 발명의 일 실시예에서, 각종 의료장비의 열리고 닫힘, 의사의 손과 발은 몸에서 떨어질 수 없고 안쪽으로 굽어질 수 있음, 의료장비 및 기구는 디지털카데바에서 떨어질 수 있고, 붙어질 수 있음)을 트레이닝 특징으로 활용한다. 또한, 기 학습된 객체에 대한 특성(본 발명의 일 실시예에서, 치과 핸드피스의 치과용 날(버)는 돌아갈 수 있음, 수술용 칼에 의해 조직이 열림, 치아는 치아 잇몸에서 뽑힐 수 있음, 인체 장기는 대체될수 있음 등)을 트레이닝 특징으로 활용한다. 이를 통해, 기 학습된 객체에 대한 특성(본 발명의 일 실시예에서, 자동차의 바퀴는 돌아갈 수 있음, 집의 현관 문은 열림, 손과발은 몸에서 떨어질 수 없고 안쪽으로 굽어질 수 있음, 모자는 머리에서 떨어질 수 있고, 씌워 질 수 있음)을 트레이닝 특징으로 활용한다. 아울러, 컴퓨팅 장치는 Conditional GAN의 Condition 메타 정보를 수정하여, 아바타의 일종인 디지 털카데바의 변이, 증례에 따른 각종질환정보를 수정한다. 본 발명의 일 실시예로서, 사용자단말은 다양한 형태의 VR시뮬레이터일 수 있다. GAN 및/또는 GNN 예측모 델에 의해 시각랜더링이 제공되는 VR 시뮬레이터에는 햅틱랜더링이 동시에 제공된다. VR 시뮬레이터에는 시각세트장치 및 다양한 형태의 햅틱 디바이스가 연결된다. VR 시뮬레이터 의 형태에 따른 종류는 다음과 같다. VR 시뮬레이터 의 형태는 이에 국한하지 않는다. 치아삭제 VR 시뮬레이터, 수술 VR 시뮬레이터, VEHICLE VR 시 뮬레이터, VR 트레드밀 등이 있다. 본 발명의 일 실시예에서, 치아삭제 VR 시뮬레이터의 장비는 HEAD MOUNTED DISPLAY, 햅틱 디바이스, 치과용 체 어에 사용되는 풋페달 시스템 등이 필요하다. 3D 프린팅을 이용하여 디지털카데바를 가상현실에서 만들고 HD 촉각을 구현한 인공카데바를 만든다. VR 및 3D 시뮬레이터를 이용하여 가상의 치과치료 및 외과수술을 진행한다. 본 발명의 일 실시예에서, 수술 VR 시뮬레이터는 다음과 같다. 환자의 병변에 대한 3D 모델을 작성하여 병변의 위치 및 상태, 영상정보를 기반으로 3D 환자 좌표계와 수술대 위에 놓인 환자의 좌표계를 정합함으로서 보이지 않는 병변의 위치를 예측하여 수술을 수행하게 하는 방식이다. CT 자료, vision probe를 통한 실시간 영상 제공, 초음파, 방사선, 가상의 영상과 실제 장면을 하나의 시스템으로 통합한다. 본 발명의 일 실시예에서, 다양한 형태의 VR 시뮬레이터(VEHICLE 예시 : 잠수함, 탱크, 드론, 전투기 등)에서 아바타(인간)가 VEHICLE 형 VR 시뮬레이터의 조종장치를 이용하여 운전하는 방식을 데이터화하여 아바타를 생성 및/또는 출력(output)할 수 있다. VR VEHICLE 시뮬레이터는 조종사 자신의 아바타 팔로 시뮬레이션 한다. 좌표계는 메타버스 월드 상에서 시작부 터 종료까지 규칙에 따라 동기화 된다. 높은 수준의 시각 랜더링을 구현하기 위해 라이다 및 적외선 트래킹 및 모션트레킹을 장착하여 인체의 모션데이터 정합 알고리즘을 갖추어야 하고 메타버스 월드에서 시뮬레이터의 위 치에 대한 정합 알고리즘도 필요하다. 예를 들어, 가상 비행기 조종에 의해 획득된 시각데이터는 도 15의 유도알고리즘의 초기 모델의 데이터 셋이 된 다. 도 15의 유도알고리즘은 도 23의 부분 유도 알고리즘(제1,2 유도알고리즘)의 합이다. 이 데이터는 인공지능이 비행 시뮬레이터를 작동시킬 수 있게 하는 기초 데이터가 된다. 인공지능의 가상 비행 조종에 있어서 오류와 오차가 많이 발생하면 사용자(비행기 조종사)는 도7 내지 도8의 사용자 인터페이스를 통 해 라벨링을 진행한다 본 발명의 일 실시예에서, VR 트레드밀을 이용한 아바타 컨트롤 시스템 (HEAD MOUNTED DISPLAY 착용)은 다음과 같은 기술이 필요하다. 사용자와 아바타의 이동, 행동, 무한 보행, 회전 등의 정합 알고리즘, 자세 제어 시스템, VIVE 트래커를 활용한 모션 및 이동 제어 시스템, 라이다 및 적외선 트래킹(신발의 압력값과 적외 선 센서값을 이용)을 이용한 무한보 행 및 인체 모션데이터 정합 알고리즘,인체의 거의 모든 움직임 등이 가능하도록 설계된 VR 트레드밀 본체, 메 타버스 세상의 좌표기준과 환경 변이에 따라 반응 기술, 모션 데이터 동기화와 전용서버, 사용자의 네트워크 플 레이가 가능하도록 하는 동기화 시스템 등이 필요하다. 2.상기 영상정보에 대한 계층적 군집화 라벨링 정보를 수신하는 제1 계층라벨링 정보 수신 단계 사용자에 의한 계층적 군집화 라벨링을 '계층 라벨링'이라고 정의한다. 본 발명의 일 실시예에서, 제1 계층라벨링은 제1 선택라벨링에 포함되어 실시될 수 있다. 도 7 내지 도8의 어플리케이션 화면(700, 800)에서 상기 영상정보인 제1 영상정보에 대해 계층적 군집화 라벨링 정보를 수신하는 제1 계층라벨링을 한다. 특정 아바타(인간)의 동작별 및/또는 특정 방식별 및/또는 특정 단계별 및/또는 세부 동작 단계별로 아바타(인 간)의 동작을 계층적으로 분류한 라벨분류를 참조하여 라벨값을 입력하는 방법을 계층라벨링으로 정의한다. 제1 계층적 군집화 라벨링, 제2 계층적 군집화 라벨링을 제1, 2 계층라벨링이라 한다. 본 발명의 일 실시예에서, 어떤 특정 환자와 비슷한 해부학적 구조(특정 변이)를 가진 환자들의 치료 및/또는 수술은 특정 아바타(인간)의 특정 동작에 포함된다. 치과 시술 혹은 외과수술의 계층적 군집화에서 표 2 내지 표 3의'특정 증례의 한가지의 특정 방식'의 라벨분류 는 표 7 내지 표 9 혹은 표12 내지 표14에서 '특정 아바타(인간)의 특정 동작별, 특정 동작의 방식별'의 라벨분 류에 포함된다. 본 발명의 일 실시예에서, 계층적 군집화 라벨링 정보를 수신하는 제1 계층라벨링 정보 수신 단계를 생략한다. 제1, 2 계층라벨링과 같은 사용자에 의한 input feature engineering을 생략하고 컴퓨팅 장치가 스스로 input feature를 구한다. 제1 계층적군집이 컴퓨팅장치에 의해 스스로 생성될 수 있다. 사용자에 의한 제1,2 계층라벨링이 일부 혹은 전체를 사용자가 수행하지 않을 경우에, 인공지능이 input feature를 스스로 구하는 것을 제1,2 계층라벨링이 컴퓨터 장치에 의해 수행되는 것으로 정의한다. 도 15에서, 제1 계층적군집은 제1 영상정보가 제1 계층라벨링에 의해 군집화 된 것을 의미한 다. 제1 계층적 군집은 전산화되어 도 10의 데이터단위 3 기준으로 전산화된 계층적군집 혹은 도 12의 데이터단위 4 기준으로 전산화된 계층적군집이 된다. 본 발명에 있어서 계층적 군집은 컴퓨팅 장치가 스스로 input feature를 구하는 방식을 포함한다. 도 15의 제1 계층라벨링은 도 7 내지 도8의 어플리케이션 화면(700, 800)에 생성 및/또는 출력(output) 된 제1 영상정보에 대해 계층적 군집화 라벨링을 진행하는 것으로 제1 영상정보에 대해 계층적 군 집화 라벨값을 입력하기 위해 사용자는 실시예에 따른 표1 내지 표16의 라벨분류를 참조한다. 라벨분류는 인공 지능이 학습할 정답 데이터셋이다. 본 발명의 일실시예에서, 계층라벨링의 라벨값을 학습한 인공지능이 사용자에게 라벨값 또는 영상정보를 반환하 고 사용자는 이에 대해 ACCEPT, REJECT 라벨을 붙일 수 있다. 본 발명의 일 실시예에서, 사용자는 각 분야의 직업을 갖는 도메인 전문가, 치과의사 ,의사, 축구선수, 댄서 등 이다. 도 3 의 직육면체는 도 4내 지 도6의 타겟속성이고 k번째 단계의 분할된 동작의 동영상 혹은 L번째 단계의 분할된 동작의 동영상 혹은 f번째 단계의 분할된 동작의 동영상을 의미한다. 시작부분 yz평 면 혹은 끝부분 yz평면(은 속성을 의미한다. 도4 내지 도 14를 참조하면, m1, m2, m3는 임의의 양의 정수, s1, s2, s3는 변수, 1 s1 m1, 1"}
{"patent_id": "10-2023-0034241", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "s2 m2, 1 s3 m3, k는 변수, 1 k n, n은 양의 정수, n', N는 임의의 양의 정수, L, f는 변수, 1 L n', 1 f N 과 같다. 도 7의 동영상 정보의 출력 혹은 도 8의 정지영상의 출력에는 아바타(인간)의 동작 등에 관련된 영 상정보(속성 및 타겟속성)가 출력 및/또는 생성된다. 사용자는 도 7의 동영상정보의 출력 및 도 8의 정지영상정보의 출력을 확인하고 라벨분류를 참조하여 계층적 군집화 관련 변수값(라벨값)을 도 7 내지 도 8의 제1입 력창 및 제2 입력창 및 제3 입력창 및 제4 입력창 및 제5 입력창 및 제6 입력창에 입력한다 제1 입력창 및 제4 입력창 에는 제1 계층(901, 1001, 1101, 1201, 1301, 1401)의 변수 s1의 변수값 (라벨값)이 입력되고, 제2 입력창 및 제 5 입력창 에는 제2 계층(902, 1002, 1102, 1202, 1302, 1402)의 변수 s2의 변수값(라벨값)이 입력되고 제3 입력창 및 제 6입력창 에는 제 3계층(903, 1003, 1103, 1203, 1303, 1403)의 변수 s3의 변수값(라벨값)이 입력된다. 계층의 개수에 따라 입력창의 개수가 늘어난 다. 제4 계층(904, 1004, 1104, 1204, 1304, 1404) 혹은 제5 계층(1205, 1305, 1405) 혹은 제6 계층의 동 영상정보는 라벨분류의 라벨값(k, L, f)과 같은 순서대로 분할된다. 본 발명의 일 실시예에서, 사용자가 단계를 분할하는 방법은 도 7의 시간축 , 시각을 나타내는 표를 조작하여 가능한다. 사용자는 도 7의 시각을 나타내는 표를 시간축상에서 마우스로 이동시키고 분할 시키고자 하는 동영상의 시각을 확인한 다음 마우스로 선택한다.본 발명의 실시예에서, 사용자는 동영상을 분할하는 특정단계별 및/또는 세부동작 단계별 라벨링을 하지 않는다. 컴퓨팅장치에 의해 '데이터단위 3 혹은 데이터단위 4로 동영상이 분할된다. 도 7 내지 도 8의 어플리케이션 화면(700, 800)상에서 입력되는 변수값은 인공지능 추론에 있어서 라벨값으로 사용된다. 본 발명의 실시예에서, 도 7내지 도 8의 어플리케이션 화면(700, 800) 에 의사 및 치과의사들이 표 1 내지 표 6 의 라벨분류를 참조하여 변수값(라벨값)을 입력하거나.(k, L, f, s1, s2, s3는 변수) 아바타(인간)의 동작과 관 련하여 사용자들이 표 7 내지 표16의 라벨분류를 참조하여 변수값(라벨값)을 입력하는 곳이다. 본 발명의 실시예에 따라, 라벨분류 표 5 및 표 10 및 표 15는 사용자에 의해 동영상이1~3 초 내외로 짧게 분할 될 수 있도록, 특징적인 세부 동작 단계별로 분류된 라벨분류이다. 표 6, 표 11, 표 16은 아바타(인간)의 신체 부위에 라벨을 붙인 것으로 제2 계층라벨링에 사용되며 전문가 집단이 임으로 정한다.세부 동작 단계는 동영상을 분할하는 방식에 따라 세부 동작 단계 1 혹은 세부 동작 단계 2로 나뉜다. 세부 동작 단계 1은 시계열 분할 선택라벨링에 의한 동작 단계의 세부 분할이고 세부 동작 단계 2는 신체부위별 선택라벨링에 의한 동작 단계의 세부 분할이다. 표 1 변수값(라벨값) 특정 아바타(외과수술 분야) 정보형태 1 설암수술 문서 등 2 양악수술(BSSRO) 문서 등 3 대장암수술 문서 등 .. .. 문서 등 s1 치과임플란트수술 문서 등 .. .. 문서 등 m1 간이식수술 문서 등 표 2 변수값(라벨값) 특정 아바타의 특정 동작 (특정 변이를 갖은 환자들의 치과임플란트 수술의 증례 혹은 특정변이를 갖은 디지털카데바의 증례)정보형태 1 상악 구치부 골폭이 좁은 증례 영상 등 2 전치부 골폭이 좁고 치조골 소실이 많은 증례 영상 등 .. .. 영상 등 s2 하악 구치부 골폭이 좁은 증례 영상 등 .. .. 영상 등 m2 .. 영상 등 표 3 변수값(라벨값) 특정 동작의 특정 방식(하악 구치부골폭이 좁은 증례의 수술 방식)정보형태 1 ridge split을 한 후, .. 영상 등 2 3D stent를 이용하여 드릴이 안전하게 삽입.. 영상 등 .. .. 영상 등 s3 block bone을 이식한 수술, 영상 등 .. .. 영상 등 m3 .. 영상 등표 4 변수값(라벨값) 특정 방식의 특정 단계(block bone 이식 수술 방식의 수술 단계) 정보형태 1 절개 및 피판을 형성한다. .. 영상 등 2 block bone을 공여 부위에서 채취한다. .. 영상 등 .. .. 영상 등 K 이식부위에 block bone을 고정한다. 영상 등 .. .. 영상 등 n 봉합 및 소독을 한다. .. 영상 등 표 5 변수값(라벨값) 세부 동작 단계 1( 상악중절치 라미네이트 11번(치식) 삭제를 진행하는 30초 동영상)정보형태 1 치아삭제 전에 미리 제작한 치아삭제용 인덱스를 구강 및 치 아에 위치시킨다.영상 등 2 구강 및 치아에 위치되어진 인덱스를 치과의사가 눈으로 확인 하고 삭제량을 측정한다영상 등 .. .. 영상 등 L 치아 절단부 3분의 1의 예상 삭제 깊이를 depth gage bur로 삭제한다.영상 등 .. .. 영상 등 n' 상악 중절치 전체 치아를 핸드피스의 트리밍 bur(다듬는 칼날)로 다듬고 미세하게 삭제한다영상 등 표 6 변수값(라벨값) 세부 동작 단계 2(치식) 정보형태 1 11(오른쪽 상악 중절치에 해당됨) 영상 등 2 12 … f 35(좌측 하악 두번째 작은 어금이 해당됨) … … … N 48(40번대 치열에서 사랑이에 해당됨) 표 7 변수값(라벨값) 특정 아바타 (딥페이크 할 캐릭터)정보형태 1 김형일 영상 등 .. 김남일 영상 등 s1 손홍민 영상 등 .. 영상 등 m1 마라도나 영상 등표 8 변수값(라벨값) 특정 아바타의 특정 동작 (손홍민 아바타의 동작)정보형태 1 슛 영상 등 .. 쎄레머니 영상 등 s2 패스 영상 등 .. 영상 등 m2 드리블 영상 등 표 9 변수값(라벨값) 특정 동작의 특정 방식 (손흥민 아바타의 드리블 동작)정보형태 1 카타르 월드컵 브라질전 전반전 36분 12초~ 36분 16초(5초간)영상 등 .. 영상 등 s3 2019년 번리전 프리미어 홈경기 70m 폭풍 드리블(11초간)영상 등 .. 영상 등 m3 카타르 월드컵 가나전 전반전 42분 33초~ 42분 35초(4초간)영상 등 표 10 변수값(라벨값) 특정 동작의 특정 단계 (2019년 번리전 프리미어 홈경기 (70m 폭풍 드리블, 11초) 정보형태 1 인프런트 드리블 1스텝 숏동작 영상 등 .. 영상 등 k 인스텝 드리블 3스텝 롱동작 영상 등 .. 영상 등 … 13 아웃사이드 드리블 1 스텝 숏동작 영상 등 표 11 변수값(라벨값) 세부 동작 단계 2 (손흥민이 인스텝 드리블3 스텝 롱동작, 발에 공이 닿는 부위의 순서와 터치 후 달리기)정보형태 1 발가락 터치 영상 등 2 터치 후 1 스텝 달리기 3 발목 터치 4 터치 후 2 스텝 달리기 표 12 변수값(라벨값) 특정 아바타(딥페이크 할 게임 캐 릭터)정보형태 1 BTS 진 영상 등.. BTS 슈가 영상 등 s1 블랙핑크 제니 영상 등 .. 영상 등 m1 블랙핑크 지수 영상 등 표 13 변수값(라벨값) 특정 아바타의 특정 동작 (제니의 댄스 동작 및 노래 종류)정보형태 1 Shut Down(4분10초) 영상 등 .. 영상 등 s2 자 오늘 밤이야(3분 55초) 영상 등 .. 영상 등 m2 마지막처럼 (3분 14초) 영상 등 표 14 변수값(라벨값) 특정 동작의 특정 방식 (제니의 마지막처럼 방송 목록) )정보형태 1 뮤직 뱅크 2022년 3월 14일 방송 (3분 20초) 영상 등 .. 영상 등 s3 열린음악회 2022년 7월 8일 방송(3분 34초) 영상 등 .. 영상 등 m3 콘서트 2022년 6월 3일 녹화 (3분 17초)영상 등 표 15 변수값(라벨값) 특정 동작의 특정 단계 (열린음 악회 2022년 7월 8일 방송)정보형태 1 좌측 그루브 영상 등 .. 영상 등 k 앞뒤 웨이브 영상 등 .. 영상 등 n 상체 팝핀 및 골반 튕기기 영상 등 표 16 변수값(라벨값) 세부 동작 단계 2(제니가 앞뒤 웨이브할때 가장 많이 움직이는 신체 부위의 순서)정보형태 1 왼쪽 팔을 든다 영상 등 2 오른쪽 팔을 든다 .. 3 가슴을 앞으로 내민다 .. 4 배를 앞으로 내민다 .. 5 골반을 앞으로 내민다 6 다리를 앞으로 내민다도 4는 기다란 입체도형이 n개로 분할된다.도 5에서 세분화 되어 n'개 된다. 도 6에서 세분화 되어 N개 된 다.(1 k n, k는 변수, k는 양의 정수, 1 L n', L은 변수, L은 양의 정수, 1 f N, f은 변 수, N은 양의 정수) 도 4 내지 도6은 아바타(사람)의 동작 1회를 입체도형으로 표현한 것이다. 첫번째 사각형(붉은색)은 동작의 시 작부분 정지영상정보(401, 501, 601)이고 마지막 사각형(붉은색)은 동작의 끝부분 정지영상정보(404, 504, 60 4)이고 x축은 시간이고, yz평면(사각형)은 정지영상정보이고 분할된 직육면체는 분할된 동영상정보이다. 도 4 혹은 도5 혹은 도6의 분할된 입체도형 1개는 도 3의 직육면체에 해당된다. 도 5의 분할된 입체도형 1개는 기다란 직육면체가 n'개로 분할된 것이다. 도 6의 분할된 입체도형 1개는 기다란 직육면체가 N개로 분할 된 것이다. 도 4 내지 도6의 기다란 막대모양의 직육면체는 아바타의 전체 동작 관련 동영상정보를 입체도형으 로 나타낸 것이다. 도 3을 참조하면, 붉은색 사각형인 끝부분 yz평면은 속성이고 직육면체는 분할된 동작의 동영상이고 타겟속성이다. 다음은 소괄호의 순서대로 매칭된다. (도 4 , 도 5, 도6)의 (k, L, f)번째 시작부분 정지영상정보(402, 502, 602)는 도3의 시작부분 정지영상정보를 의미하고 (k, L, f)번째 끝부분 정지영상정보(403, 503, 603)는 끝 부분 정지영상정보를 의미하고 ((k, L, f)번째 시작부분 정지영상정보(402, 502, 602)와 (k-1, L-1, f- 1)번째 끝부분 정지영상(403, 503, 603)는 동일하다. 도4 내지 도6의 (k, L, f)번째 끝부분 정지영상정보(403, 503, 603)은 속성이고 분할된 동작 동영상의 (k, L, f)번째 단계(405, 505, 605)는 타겟속성이다. 도 3 내지 도 6에서 데이터단위 1은 아바타(인간)의 1회 동작 전체 동영상의 시작부분 정지영상정보(401, 501, 601)와 전 체 동영상의 끝부분 정지영상정보(404, 504, 604)의 합이 전산화된 값으로 정의한다. 데이터단위 2는 아바타(인 간)의 분할된 동작 동영상의 최초 단계 정지영상정보부터 마지막 단계 정지영상정보까지의 정지영상정보의 전체 합이 전산화된 값으로 정의한다. 도 4에서 데이터단위 3은 아바타(인간)의 분할된 동작 동영상의 k번째 단계의 동영상정보와 끝부분 정지 영상정보의 합이 전산화된 값으로 정의한다. 도 5에서 데이터단위 4는 아바타(인간)의 분할된 동작 동영상의 L번째 단계의 동영상정보와 끝부분 정지 영상정보의 합이 전산화된 값으로 정의한다.. 도 10은 데이터단위 1, 2, 3 을 기준으로 하는 계층적 군집화가 전산화된 것이고 도 12는 데이터단위 1, 2, 3, 4를 기준으로 하는 계층적 군집화가 전산화된 것이다. 도 15의 유도알고리즘에 사용되는 데이터단위 3, 데이터단위 4는 도 3의 분할된 직육면체 형태이다. 데이터단위 3)에서 속성은 아바타(인간)의 분할된 동작 동영상 의 k번째 단계의 끝부분 정지영상정보 이고 도 4의 붉은색 사각형이다. 데이터단위 4에서 속성은 아바타(인간)의 분할된 동작 동영상의 L번째 단계의 끝부분 정지영상정보 이고 도 5의 붉은색 사각형이다. 도 9는 실제 현실의 데이터에 대한 계층적 군집화의 계통도로써, 도 9은 K1개의 군집을 의미한다. 도 10 혹은 도 12는 데이터단위에 의해 동영상의 단계가 분할될 때 붙여진 라벨값에 의해 만들어진 계층적 군 집화의 계통도이다. 도 10는 데이터단위 3 에 근거한 K2개의 군집이다. 도 12은 데이터단위 4에 근 거한 K4개의 군집이다 본 발명의 실시예에서, 시작부분 정지영상정보도 속성이고 타겟속성과의 합으로 데이터단위가 되고 순방향의 동 영상 출력 및 생성에 사용된다. 본 발명의 실시예에서, 증례, 방식, 단계별 계층적 군집화와 관련된 라벨링 방식은 다음과 같다. 표 1 내지 표 6는 의사 및 치과의사들의 의료분야 전문지식을 근거하여 제작된 것으로,도 7 내지 도 8의 어플리케이션 화면 (700, 800)에 변수값(라벨값)을 입력하기 위해 제시된 라벨분류들의 예시이다. 표 1은 제1 입력창 및/또는 제 4 입력창 에 입력되는 변수값(라벨값)과 수술 분야의 예시이고, 표 2는 제2 입력창 및/또는 제 5 입력창 에 입력되는 변수값(라벨값)과 수술 증례의 예시이고, 표 3은 제 3 입력창 및/또는 제 6 입력창 에 입력되는 변수값(라벨값)과 수술 방식의 예시이고 표 4은 시계열 분할에 따라 입력되는 변수값(라 벨값)에 의한 수술 단계의 예시이다. 이것은 임상적인 기준(증례, 방식, 단계)을 사용자(의사, 치과의사)가 참 조하여 변수값(라벨값)을 입력하는 방식이다. 표 5는 수술단계를 더욱 세분화하여 분류한 예시이다. 표6은 아바 타(인간)의 신체부위에 라벨값을 지정한 분류의 예시이다. 외과수술에 대한, '증례, 방식, 단계별 분류 기준'이 계층적 군집 라벨값 입력에 의해 도 10 및 도12의 데이터 단위의 기준으로 전산화된다. 의료동영상 정보 및 기타 정보들을 증례, 방식, 단계별 계층적 군집화를 하는 것으로 기본으로 하나, 위 정보들을 임의의 방식으로 세부적으로 라벨링하고 동영상을 분할하여 세분화된 계층적 군집화를 한다면 계층의 종류 및 개수(3개층, 4개층, 5개층 등)와 동영상 분할의 방식과 상관없이 도 15의 유도 알고리즘이 똑같이 적용된다. 3.상기 영상정보에 대한 제1 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 분류모델 유도 단계 도7 내지 도8의 어플리케이션 화면(700, 800)에서, 상기 영상정보인 제1 영상정보에 대한 사용자의 선택 라벨링 정보를 수신하여 분류모델을 유도하는 제1 분류모델의 유도 단계이다. 제1 분류모델은 도10 및/또는 도 12의 각 K2 혹은 K4개의 계층적 군집중의 하나인, 특정군집에 속한 ' 제1 속성 및 제1 타겟속성'을 분류한 것이다. 특정군집에 속한 제1 속성 및 제1 타겟속성 에 대해 사용자들이 선택라벨링을 하였다면 라벨링된 데이터에 대해 제1 분류모델이 유도된 다. 본 발명의 실시예에서, 도 9와 표 1 내지 표 6을 참조하면, 치과 시술 혹은 외과수술의 계층적 군집화에 있어 서,특정 증례의 한가지의 특정 방식이란, 어떤 특정 환자와 비슷한 해부학적 구조(특정 변이)를 가진 환자들의 치료 및/또는 수술에 있어서 같은 방식의 수술 및 치료 기법 및 같은 의료 장비 및 재료를 사용하여 치료를 한 경우라도 정의할 수 있다. 이를 기반으로 특정변이를 포함한 디지털카데바를 3D 프린팅기법으로 생성 및/또는 출력(output)한다. 분류 모델의 유도는 다음과 같을 수 있다. 도11 내지 도14의 총 K2 부터 총K6개의 군집중 하 나인 ‘특정 군집’에 총 K7개의 ‘제1 속성, 제1 타겟속성 ’이 존재하고 여기에 총 K8명의 플랫 폼 사용자(치과의사, 외과의사)가 치과 시술 및/또는 외과수술을 하고 ACCEPT(705, 805), REJECT(706, 806)라 벨 값을 붙였다고 가정할 수 있다(K7, K8는 임으로 정한 양의 정수). 총 K7×K8(곱하기)개의 ‘제1 속성, 제1 타겟속성’에 대해서, 인공지능 추론 및/또는 GNN 회귀모델 및/또는 데이터 마이닝 기법(유사도, 이웃, 군집, 계층확률추정, 모델링 등)을 통한 분류 모델의 유도를 진행할 수 있다.K8이 1인 경우, 1명의 사용 자가 총 K7번의 반복적인 치료 및/또는 수술을 진행한다.K8이 복수인 경우, 여러명의 사용자가 총 K7번의 반복 적인 치료 및/또는 수술을 진행한다.(K7, K8은 임의의 양의 정수). 사용자들은, 도 7 내지 도 8에서 출력되는 영상정보에서 사용자 자신의 전문지식으로 판단하여 잘못된 행위와 관련된 부분이 보이면, 그 부분에는 REJECT 버튼(706, 806)을 눌러 REJECT 라벨을 붙이고, 잘된 행위와 관련된 부분에는 ACCEPT 버튼(705, 805)을 눌러 ACCEPT 라벨을 붙인다. 영상정보에 드래그를 하거나 태그를 붙여서 경계선과 경계면을 자동인식하는 객체인식기법을 이용하여 2분법 및 /또는 3분법 및/또는 다분법의 방식으로 도 7 내지 도8의 어플리케이션 화면(700, 800)에 라벨을 붙이는 라벨 링 방법을 '선택라벨링'이라고 정의한다 해당 객체인식(object detection)을 위한 인공신경망은 사용자가 도 7 내지 도 8의 어플리케이션 화면(700, 800)에서 영상정보에 드래그를 하거나 태그를 붙이면 잘못된 동작 부위 및 동작 들을 탐지하여 이미지를 분리하 고 분석한다. 인공지능 추론 과정을 통해 사용자에게 추론 결과를 제공한다. 도 7의 '마우스의 화살표, 버튼(705, 706), 기타입력장치' 및 도 8의 '마우스의 화살표, 버튼 (805, 806), 기타입력장치' 는 도 7 내지 도 8에서 출력되는 영상정보에 변수값을 입력하여 라벨을 붙일 때 사용되는 것으로 영상정보에 태그를 붙이거나 드래그를 하여 선택라벨링을 할 때 사용된다. 본 발명의 실시예에서, 사용자가 사용자 인터페이스의 동영상에서 도 7의 시각을 나타내는 표를 이동하여 시간을 정지하여 정지영상을 캡처한 다음, 해당 시각의 정지영상에 경계선과 경계면이 자동 인식되게 마우스의 버튼과 화살표로 태그를 붙인다. 동영상 중에서 캡쳐한 여러 장의 3D 정지 영상에 태그를 붙이면 동영상 전체의경계면과 경계선이 자동 인식되도록 코딩한다. 본 발명의 실시예에서, 도 7내지 도 8에서 출력되는 영상정보 전체에 대해 ACCEPT, REJECT 라벨을 붙일 때는 인 터페이스 화면의 ACCEPT(705, 805), REJECT(706, 806) 버튼을 직접 누르는 방식으로 가능하고 세밀한 부분을 지 정하여 ACCEPT, REJECT 변수값을 입력하여 라벨을 붙이고자 할 때는 마우스 드래그를 이용하여 경계선(직선이나 곡선)이나 경계면(폐곡선)을 지정하거나 다수개의 포인트를 마우스 버튼으로 지정한 다음에 ACCEPT(705, 805), REJECT(706, 806) 버튼을 누른다. ACCEPT 라벨이 붙지 않은 속성과 타겟속성에는 not ACCEPT 라벨이 붙고 REJECT 라벨이 붙지 않은 속성과 타겟속성에는 not REJECT 라벨이 붙는다. 본 발명의 실시예에서, 객체인식의 방법에서 객체탐지, 위치 측정 및 객체 및 인스턴스 분할 , 자세 추정 등이 적용되고 동영상 분석을 위한 인스턴스 추적, 행동 인식, 움직임 추정 등에도 동일하게 적용된다. 동영상 클립 에 포함된 동작을 감지하기 위해 합성곱 신경망과 결합하여 사용한다. 동작 감지, 장면 추출, 다음 프레임 예측, 객체 추적 등이 사용된다. 자동 인식된 경계선과 경계면을 기준으로 인터페이스에서 출력되는 객체 및 동 작의 잘된 부분과 잘못된 부분에 각각 ACCEPT(705, 805), REJECT(706, 806) 버튼을 누르는 방식으로 해당 라벨 을 붙인다. 본 발명의 실시예에서, 2D 영상정보 및 3D 영상정보의 복수의 포인트 클라우드에 마우스의 왼쪽 버튼을 눌러 태 그를 붙이거나 및/또는 드래그를 하여 태그를 붙이면 경계선 및 경계면의 자동 인식이 가능하도록 프로그램을 코딩한다. 3D 정지 영상 중에서 x, y, z좌표상에 존재하는 복수의 포인트 클라우드에 마우스 왼쪽버튼을 눌러 태그를 붙이거나 드래그를 하여 태그를 붙이면 잘된 정보와 잘못된 정보의 경계선이 자동인식이 되고 폐곡선에 의해 경계면이 자동 인식되도록 코딩한다. 본 발명의 실시예에서, 라벨링 단계에서 아바타(인간)의 동작을 ACCEPT, REJECT 로 분류하는 라벨링은 지도학습 형태로 진행되며, 이는 분류모델에 해당된다. ACCEPT, REJECT 2진분류는 보편적으로 사용하는 2진분류(Binary Classification) 모델로 사용가능하며 수술 및 동작의 성공 실패를 5단계의 척도로 표현시에는 각 class의 확률 값이 도출되는 multiple classification 모델로 구현 가능하다. 본 발명의 실시예에서, 도 7내지 도 8의 사용자 인터페이스에서 ACCEPT, REJECT를 선택하는 이분법으로 영상정 보에 라벨을 붙일 수 있지만, ACCEPT, NORMAL, REJECT로 분류하여 3단계로 영상정보에 라벨을 붙이는 것도 가능 하다. 잘된 동작과 잘못된 동작의 단계를 정도로 나누어서 5단계, 6단계 라벨로 세분화하여 라벨링 할 수도 있 다. 5단계, 6단계와 같이 라벨의 세분화가 클 경우, 잘된점수를 5점부터 1점까지 점수를 매긴다. 잘된 점수가 일정 이상(4점이상)인 경우에는 ACCEPT로 간주하고 잘못된 점수가 일정 이하(2점이하)인 경우에는 REJECT로 간 주하여 분류한다. 3점은 NORMAL로 분류한다. 본 발명의 실시예에서, 도 7 내지 도 8 에서 '기타입력장치'는 다음과 같다. 마우스로 태그를 붙이거나 드래그 를 하는 방법은 콘트롤러, 아이트레커,데이터 글러브, 음성인식 인터페이스, 브레인 컴퓨터 인터페이스, 손 추 적 기술 등으로 응용된다. 다음은 '기타입력장치'의 사용방법의 예시이다. 마우스(화살표, 버튼)를 음성인식 인터페이스와 브레인 컴퓨터 인터페이스로 작동시켜 태그를 붙이거나 드래그 를 하고 라벨을 붙이는 방법, 광선이 나오는 컨트롤러를 음성인식 인터페이스와 브레인 컴퓨터 인터페이스로 작 동시켜 태그를 붙이거나 드래그를 하고 라벨을 붙이는 방법, 아이트래커를 음성인식 인터페이스와 브레인 컴퓨 터 인터페이스로 작동시켜 태그를 붙이거나 드래그를 하고 라벨을 붙이는 방법, 데이터글러브를 손 추적 기술 ,음성인식 인터페이스, 브레인 컴퓨터 인터페이스로 작동시켜 태그를 붙이거나 드래그를 하고 라벨을 붙이는 방법, 이상의 방법을 병행해서 사용하는 방법 등이다. 본 발명의 실시예에서, 음성인식 인터페이스로 컴퓨터의 마우스나 버튼을 직접 움직인다. 콘트롤러의 광선을 움 직여서 태그를 붙이거나 드래그를 할 수도 있다. 또한, 아이트래커를 사용하여 사용자의 응시를 감지해 시야각 의 중심부에 태그를 붙이는 방법으로 분류하고자하는 객체를 식별하여 객체에 라벨링을 한다. 또한, 데이터 글로브 및 손동작의 상호작용(손동작 추적 기술)을 이용하여 사용자 인터페이스상의 영상에 태그 를 붙이거나 객체에 경계선과 경계면을 만든다. 플랫폼 사용자(각 분야 전문가 집단)가 사람의 뇌와 컴퓨터를 연결하는 브레인 컴퓨터 인터페이스(Brain-Computer Interface)기술을 이용한다면 플랫폼 사용자는 영상(정지영 상, 동영상)을 보고 경계선과 경계면에 마우스로 태그를 붙이거나 드래그를 한 다음, 자신의 의지(생각)만으로 정지영상이나 동영상에 도 7 내지 도8에서 ACCEPT(705, 805), REJECT(706, 806) 버튼을 눌러 라벨을 붙인다. 더 나아가, 자신의 의지(생각)만으로 정지영상이나 동영상에서 경계선과 경계면을 만들기 위한 태그를 붙이거나 드래그를 한 다음, 구분된 정지영상과 동영상에 선택라벨링을 한다. 본 발명의 실시예에서, 정지영상에서 잘못된 부분을 지정하는 방식은 다음과 같다. 치과의사가 환자의 구강에 식립된 교정용 미니 임플란트의 위치가 자신의 의학 지식에 근거하여 적절한 위치보다 다소 높거나 낮다고 판단 되면, 마우스 드래그를 이용하여 경계선(직선이나 곡선)이나 경계면(폐곡선)을 지정하거나 다수개의 포인트를 마우스 버튼으로 지정하고 REJECT(706, 806) 버튼을 누를 수 있다. 이부분은 REJECT 라벨이 붙게 된다. 본 발명의 실시예에서, 도 7의 수술 동영상에서 잘못된 부분을 지정하고 라벨을 붙이는 방식은 다음과 같다. 먼 저, 잘못된 의료행위 및/또는 동작이 행해진 동영상 구간을 시계열에서 도 7의 시각을 나타내는 표를 이 용하여 한정한다. 시각을 나타내는 표를 이동하여 선택된 시각과 시각의 사이에 존재하는 동영상정보가 라 벨을 붙일 정보로 한정된다. 본 발명의 실시예에서, 교정용 미니 임플란트를 식립하는 동영상에서 선택라벨링을 하고자 한다면, 동영 상의 정지 화면 및/또는 동영상 화면에서 마우스 드래그나 마우스 버튼을 이용한 다수개의 태그를 사용하여 경 계선(곡선이나 직선) 및/또는 경계면(폐곡선)이 되는 포인트 클라우드 들을 지정한다. 이 때, 선택하고자 하는 동영상이 자동인식되고 그 다음 순서로 인식된 동영상에 ACCEPT(705, 805), REJECT(706, 806) 버튼을 누를 수 있다 4.상기 영상정보 또는 제2 기초 영상정보를 기반하여 영상정보를 출력(output) 혹은 생성하거나, 출력(output) 및 생성하는 제2 영상정보 출력(output) 또는 생성 단계 제2 영상정보는 제2 유도알고리즘의 예측값인 영상으로, 본 발명에 있어서 유도알고리즘의 반복된 적용으로 고도화된 예측모델의 예측값으로 정의한다. '제1 기초 영상정보, 제2 기초 영상정보, ...'는 도 15의 유도알고리즘에 입력되는 것으로 시각세트장치 를 통해 계속적으로 수집된 실제 현실의 시각데이터이다. 상기 영상정보는 제1 분류모델에 의해 분류된 제1 영상정보이다. 도 8에서 출력 및/또는 생성되는 정지영상정보는 제2 속성이고 도 7에서 출력 및/또는 생성되는 동영상 정보는 제2 타겟속성이다. 도 10 내지 도 14 의 총 K2개 내지 K6개의 군집 중의 하나인, 특정군집에 속한, 속성에 대한 정지영상의 좌표 상대값과 각종 물리속성 강도 값에 대한 회귀 모델을 'GNN 회귀 모델 1형'으로 정의하고 타겟속성에 대한 동영 상의 좌표 상대값과 각종 물리속성 강도 값에 대한 회귀모델을 'GNN 회귀모델 2형' 으로 정의한다. 도 17을 참조하여, 아바타의 동작행위에 대한 특정 시점의 상대적 영상 정보 및 상태 값을 예측하는 모델이 GNN 형태로 구조화 되고, 이의 각 수치를 예측한 모델을 'GNN 회귀모델 1형 및 2형'으로 정의한다. GAN 단독 사용시, 제1 연관규칙 1형 및 제1 연관규칙 2형이 제2 속성 및 제2 타겟속성 을 예측한다. 연관규칙 1형 및 2형은 GNN을 사용하지 않고, 각각 정지영상 및 동영상을 연관규칙 및 딥러 닝(GNN 회귀모델을 제외한 Sequential 형태의 input이 들어오는 모델)으로 추론하는 모델이고 GNN 형태의 구조 화를 사용하지 않는 점을 제외하고 도 17의 GNN회귀모델 1형 및 2형과 동일한 형태의 모델이다. 'GNN회귀모델 1형 및 2형' 혹은 '연관규칙 1형 및 2형'은 sliding window기법을 사용하고. Sequential 형태의 input이 들어오는 모델이다. 'GNN회귀모델 1형 및 2형' 혹은 '연관규칙 1형 및 2형'은 도 20의 'GAN 및/또는 GNN 예측모델'이다. 도 20을 참조하면, 시각세트장치가 연결된 사용자단말의 사용자 인터페이스에서 사용자는 선 택라벨링을 하고 라벨링된 시각데이터는 GAN 및/또는 GNN 예측모델에서 사용된다. GAN 및/또는 GNN 예측모델은 아바타의 동작을 생성하거나 출력(output)하기 위해 시각데이터를 시뮬레이 션 엔진에 전달한다. 도 20에서 시각데이터는 시뮬레이션 엔진 및 그래픽스 엔진 및 디스플레이 장치 및 제어알고 리즘의 순서대로 전달되어 사용자 인터페이스를 통해 출력(output)및/또는 생성된다. 도 7 내지 도8의 어플리케이션 화면(700, 800)은 사용자 인터페이스가 사용자단말에서 화면으로 구 현된 것이다. 도 20의 GAN 및/또는 GNN 예측모델은 인터페이스 API과정을 포함한다. 본 발명의 실시예에서, 인터페이스API의 예시는 다음과 같다. IoT Edge(아두이노,라즈베리파이 등) 디바이스가 받아들이는 데이터는 입력(input) 데이터 자체일 수도 있고, Edge 상에서 구동된 인공지능 추론 의 결과 출력(output) 일 수도 있다. 파이썬 등으로 만들어진 인공지능 모델은 ONNX등의 오픈소스 라이브러리를 통하여 IoT Edge 디바이스 용으로 컨 버팅 가능하고, 이를 통해 Edge 상에서 1차 추론된 출력 결과(output Result) 데이터 및 입력(Input) 데이터는 Server API 호출을 통해 보다 복잡한 집단지성 모델로 재 추론된다. 디지털단위는 인공지능과 사용자의 상호작용(시계열분할 선택라벨링, 1901)(신체부위별 선택라벨링, 1902)에 의 해 분할전 동영상 단위를 의미한다. 도 10 혹은 도 12의 계층적 군집에서 K2 혹은 K4개의 각 군집별로 선택라벨링 된 제1 속성을 분 류하여 제1 GNN회귀모델 1형 혹은 제1 연관규칙 1형 을 유도한다.. 도 10 혹은 도 12의 계층적 군집에서 K2 혹은 K4 개의 각 군집화된 제1 속성과 제1 타겟속성에서 제1 GNN회귀모델 2형 혹은 제1 연관규칙 2형을 유도한다. 제1 GNN회귀모델 1형 혹은 제1 연관규칙 1형 에 정지영상정보(데이터단위 1, 2 혹은 제1 속성, 1524)의 시계열 시퀀스를 입력하면 제1 GNN회귀모델 1형 ) 혹은 제1 연관규칙 1형 은 도 8의 어플 리케이션 화면으로 제2 속성 의 시계열 시퀀스를 반환한다. 제2 속성은 제1 GNN회귀모델 1형 ) 혹은 제1 연관규칙 1형 의 예측값(1506, 1516)이고 동작 동영상의 k번째 혹은 L번째 혹은 f번 째 단계의 정지영상정보에 대한 특징벡터표현(feature vector representation)이다. 제1 GNN회귀모델 2형 ) 혹은 제1 연관규칙 2형 에 '제2 속성 의 시계열 시퀀스가 입력되면, 제1 GNN회귀모델 2형 혹은 제1 연관규칙 2형 은 도 7의 어플리케이션 화면으로 제1 GNN회귀 모델 2형 혹은 제1 연관규칙 2형 의 예측값인 '제2 타겟속성'을 출력(output) 및/또 는 생성한다. 제2 타겟속성은 동작 동영상의 k번째 혹은 L번째 혹은 f번째 단계의 동영상정보에 대한 특 징벡터표현(feature vector representation)이다 도 15 및 도 23을 참조하면, 제1 유도알고리즘은 다음과 같다.제1 계층적 군집의 데이터는 제1 선 택라벨링되어 제1 분류모델이 유도되고 분류된 제1 속성 및 제1 타겟속성은 제1 GAN 및/또는 GNN 예측모델의 유도에 사용된다. 제2 유도알고리즘은 다음과 같다. 제1 GAN 및/또는 GNN 예측모델의 예측값은 제2 계층적 군집이 된다. 제2 계층적 군집의 데 이터는 제2 선택라벨링되어 제2 분류모델이 유도되고 분류된 제2 속성 및 제2 타겟속성은 제 2 GAN 및/또는 GNN 예측모델의 유도에 사용된다. 이하 알고리즘의 유도는 반복된다. 제1 GAN 및/또는 GNN 예측모델은 제1 GNN회귀모델 1형 및 제1 GNN회귀모델 2형이거나 제1 연관규칙 1형 및 제1 연관규칙 2형이다. 제2 GAN 및/또는 GNN 예측모델은 제2 계층적 군집이 제2 선택라벨링되고 제2 분류모델 에 분류된 제2 속성과 제2 타겟속성에 의해 유도된 모델이다. 본 발명의 실시예에서, 제2 계층적군집은 제1 GNN회귀모델 1형의 예측값 및 제1 GNN회귀모델 2형의 예측값이 제2 계층라벨링되거나 제1 연관규칙 1형의 예측값 및 제1 연관규칙 2형의 예측값 이 제2 계층라벨링되어 군집화 된 것이다. 도 23을 참조하면, 제1 계층적 군집의 제1 영상정보 및 제2 기초 영상정보가 단일모델로 학습되고 각 군 집별로 제2 영상정보가 생성 및/또는 출력(output)된다. 제2 영상정보는 제2 속성 및 제2타겟속성이고 제2 계층라벨링에 의해 도 23의 제 2 계층적군집이 된다. 본 발명의 실시예에서, 잘된 동작의 결과인 동영상정보를 예측하는 모델이 제1 GNN회귀모델 2형 혹은 제 1 연관규칙 2형이다. 본 발명의 실시예에서, '제1 GNN회귀모델 2형'은 연관규칙을 포함할 수 있다. 도 11 및/또는 도 13 및/ 또는 도 14 의, 특정군집에 속한, 디지털단위의 정지영상정보(속성)에서 객체의 패턴, 물리적 속성 값을 포함 한 동영상정보(타겟속성)를 연관규칙으로 예측한다. 본 발명의 실시예에서, GNN 회귀모델 2가 포함할 수 있는 연관규칙은 역방향, 순방향, 양방향의 방식이 있다. 도 23에서 모델 관점에서 제2 기초 영상정보와 제1 유도알고리즘의 예측값인 제1 영상정보는 정확도나 정교함의 장단점이 서로 상이 할 수 있는데, 제2 유도알고리즘의 학습 데이타로 사용된다. 제2 유도알고리즘는 제2 영상정보를 출력 및/또는 생성한다. 1차 라벨링 된 제1 영상정보는 라벨링에 의해 2차 라벨링 되고, 이 과정은 계속하여 반복된다. 이 과정을 과거 라벨 된 데이터(제1 영상정보,2301)와 다른 데이터(제2 기초 영상정보, 2305)가 함께 반복 수행 하게 되며, 한번 학습했던 데이타 및/또는 유사한 레이블 값 또한 매 반복 학습(epoch)에 계속 등장하여, 여러 번의 실험을 거치는 과정이 필요하다. 매 epoch 는 누적된 단위 레이블의 총 갯수(batch size) 만큼을 학습 연산 단위(mini batch size)로 분할하여 다양한 실험을 하게 되며 해당과정에서 집단지성의 라벨값은, 취사 선택 및 평균화 되어 모델에 반영 된다. 도 23에서 '제1 유도알고리즘, 제2 유도알고리즘, ...'은 도 15의 유도 알고리즘이고 전체 알고리즘을 부분 알고리즘의 합으로 표현한 것이다. 본 발명의 실시예에서, 가상 공간 상 3D프린팅 시뮬레이션으로 디지털카데바를 손쉽게 제작 및 초기화 가능하고, 이의 사용은 가상공간의 제약을 덜기 위해 가상수술 오디션 게임을 진행한다. 실제 의료기관 및/또는 가상수술 오디션을 통해 수집된 수술 패턴을 군집화 및 패턴화 하여 인공지능의 초기 모델을 만든다. 수술의 정 교함의 정도 및 성공 여부에 대하여는 검증된 전문의의 패턴을 별도 추출하여 지도학습한다. 본 발명의 실시예에서, 슬라이딩 윈도우는 다음과같다. 동영상 정보들에 대해서 각 windows size 의 단위 단위 를 classification 한다.위 방식으로 총 50초 짜리 동영상들이 10초씩 5개로 나뉜다고 할 때, A,B,Z,A,B 의 순 서로 input 이 들어오면 다음이 Z 라는 걸 연관규칙으로 예측한다. 본 발명의 실시예에서, 잘 알려진 딥러닝 알고리즘인 RNN, LSTM 은 bidirectional LSTM 이라는 변형 알고리즘을 통해 순방향을 역방향 및 양방향으로 확대하고 추가적인 성능 형상을 꾀할수 있는데, 제안하는 디지털단위 또한 bidirectional 과 마찬가지로 역방향 및 양방향으로 확대 가능하다. 제안하는 디지털단위는 RNN 및 LSTM 과 달리 복합적인 input feature 가 결합되어 있다는 차이점이 있다. 동영상 장면 프레임은 특징적인 패턴으로 군집화 되어, A동작, B동작, C동작 형태로 군집되어질 수 있다. 수술 동작, 및/또는 게임 내 특정 캐릭터의 특별한 동작은 모두 학습된 동작 군집(A, B, C, ...)의 일련의 순 서 패턴으로 다시 연관도가 만들어 질 수 있다. 이는, 본 발명의 실시예에서 A→B→D, A→B→F 등의 Sequential 연관 패턴이 학습 데이터 내에서 순서 연관도 높게 빈발하게 관찰될 경우 그 패턴이 순서와 함께 학습되게 된다. 특정 반복 동작은 동영상 군집의 Sequential 연관 패턴으로 학습 및 재현되어 질 수 있다. 여기서 말하는 재현은 앞부분의 일부 패턴이 input으로 입력되었을 때 뒤이을 패턴이 어떤 군집의 동작 패턴인 지 연관규칙으로 유추 추론 가능하다는 의미이다. 본 발명의 실시예로, 시계열 역방향의 연관규칙을 설명한다. GNN 회귀모델 1형을 사용하여 정지영상정보인 Output 데이터 (흔적 및 결과)을 예측하고 Output 데이터 (흔적 및 결과)에 대한 원인이 되는 포인트 클라우드의 벡터적이 역방향의 제1 GNN회귀모델 2형의 예측값이다. 결과 값을 분석하여 시간의 변화에 따른 GNN 프레임워크 상의 포인트 클라우드의 벡터적을 찾아내어 플랫폼 사 용자에게 반환한다. 결과 값이 있으면 포인트 클라우드의 벡터적이 있다는 규칙을 발견하여 'GNN 회귀모델 1형이 임의의 예측값(결 과 값) 및/또는 정지영상정보를 제시하면, GNN 회귀모델 2형은 시간의 변화에 따른 포인트 클라우드의 벡터적 (원인 값)을 반환한다. 연관규칙 추론을 위해 결과 값들 간의 의미 있는 관계를 찾아내기 위하여 결과값 의 데 이터 집합과 포인트 클라우드의 벡터적(원인값)을 반환하는 트랜잭션의 집합을 구축한다. 연관규칙은 선행사건과 후행사건이 있으며 결과 값과 원인 값의 집합에 각각 포함되는 것으로 이는 연관규칙 추 론의 결과로 얻게 되며 벡터적은 complexity이 있는 정보이기 때문에 많은 연관규칙이 존재하게 된다. 의미 있는 연관 규칙을 찾아내는 평가기준이 필요한다. 평가척도로는 지지도, 신뢰도, 향상도 등을 사용한다. 연관 규칙 알고리즘에서 결과 값과 원인 값의 각각의 집합이 디지털단위 4 및 디지털단위 5에서 정 지영상정보의 군집과 동영상정보 군집을 의미한다. 5.상기 영상정보에 대한 계층적 군집화 라벨링 정보를 수신하는 제2 계층라벨링 정보 수신 단계 상기 영상정보는 제2 영상정보이다. 도 23의 제1 GNN 및/또는 GAN 예측모델의 예측값인 제2 영상정보에 대해 제2 계층라벨링을 하면 제2 계층적 군집이 된다. 본 발명의 실시예에서, 계층적 군집화 라벨링 정보를 수신하는 제2 계층라벨링 정보 수신 단계를 생략한 다. 사용자에 의한 input feature engineering을 생략하고 컴퓨팅 장치가 스스로 input feature를 구한다. 제 2 계층적 군집이 컴퓨팅장치에 의해 스스로 생성될 수 있다. 본 발명의 실시예에서, 제2 계층라벨링은 제2 선택라벨링에 포함되어 실시될 수 있다. 6.상기 영상정보에 대한 제2 선택라벨링 정보를 수신하여 분류모델을 유도하는 제2 분류모델 유도 단계 제2 영상정보에 대한 사용자의 선택라벨링 정보를 수신하여 분류모델을 유도하는 제2 분류모델 의 유도 단계이다. 제1 GAN 및/또는 GNN 예측모델에 의해 예측된 값이 제2 영상정보이다. 제2 영상정보의 정지 영상정보는 제2 속성이고 동영상정보는 제2 타겟속성이다. 제2 분류모델은 제2 계층적군집중의 하나인, 특정군집에 속한 '제2 속성 및 제2 타겟속성 '을 분류한 것이다. 특정군집에 속한 제2 속성 및 제2 타겟속성에 대해 사용자들이 '선택라 벨링'을 하면 라벨링된 데이터에 대한 분류모델을 유도한다. 본 발명의 실시예에서, 계층적 군집화 라벨링 정보를 수신하는 사용자에 의한 제2 계층라벨링 정보 수신 단계를 생략할때, 제2 계층적군집은 컴퓨팅장치에 의해 스스로 생성된다. 7.시계열분할 선택라벨링 정보를 수신하는 단계 상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 시계열분할 선택라벨링을 포 함한다. 시계열분할 선택라벨링을 포함한 제2 선택라벨링 정보는 도 11 혹은 도13의 계층적 군집과 관련된 것이다. 본 발명의 실시예에서, 계층적 군집화 라벨링이 생략되는 경우에도 시계열 분할 선택라벨링은 제 2 선택 라벨링에 포함되어 실행되기도 하고 제 2 선택라벨링전후에도 실행될 수 있다. 제2 계층적 군집은 도 11 혹은 도13의 디지털단위 3 혹은 디지털단위 4 를 기준으로 전산화된 것이다. 도 11은 디지털단위 3을 기준으로 전산화된 계층적군집이고 도 13은 디지털단위 4를 기준으로 전산화된 계층적군집이다. '제1 GNN회귀모델 1형의 예측값 혹은 제1 연관규칙 1형의 예측값'에 대해 사용자가 ACCEPT 혹은 REJECT의 라벨을 붙여 분할시점(정지영상정보 혹은 속성의 라벨값)을 선택하거나 거부하는 방식을 도 19의 '시 계열분할 선택라벨링이라고 정의한다. 사용자가 라벨분류를 참조하여 제2 영상정보에 도 19의 시계열분할 선택라벨링을 한 다음, 제2 분 류모델을 유도한다. 인공지능이 분할시점을 학습하여 이를 사용자에게 반환하면 사용자는 ACCEPT(705, 805), REJECT(706, 806) 버튼 을 통해 선택한다. 분류모델은 라벨링이 된 정보를 다시 분류하고 'GNN 회귀모델 1형 혹은 연관규칙 1형'은 집단지성화된 예측값을 반환하고 사용자는 반복하여 도19의 시계열분할 선택라벨링을 한다. 라벨분류를 참조한 사용자가 도 7 내 지 도 8의 어플리케이션 화면(700, 800)을 통해 라벨값을 입력한다. 본 발명의 실시예에서, 시계열분할 선택라벨링에서 사용자가 REJECT(706, 806) 버튼을 눌렀다면, 도 7의 시각을 나타내는 표를 이동하여 동영상을 분할하고자 하는 시점의 정지영상정보를 캡처한 후, ACCEPT(705, 805) 버튼을 누르는 방법으로 시계열 분할에 따라 라벨링되는 라벨값을 직접 입력한다. 도 15의 유도알고리즘에 사용되는 디지털단위 3, 디지털단위 4는 도 3의 분할된 직육면체 이다. 디지털단위 3에서 속성은 아바타(인간)의 분할된 동작 동영상의 k번째 단계의 끝부분 정지영상정보 이고 도 4 붉은색 사각형이다. 도 4를 참조하면, 분할된 동작 동영상의 n번째 단계의 끝부분 정지영상정보 또한 속성에 해당되고 도 4의 마지 막 붉은색 사각형이다. 디지털단위 4에서 속성은 아바타(인간)의 분할된 동작 동영상의 L번째 단계의 끝부분 정지영상정보 이고 도 5의 붉은색 사각형이다. 도 4 내지 도 5을 참조하면, 분할된 동작 동영상의 (k, L)번째 단계의 끝부분 정지영상정보 또한 속성에 해당되 고 도 4 내지 도 5의 마지막 붉은색 사각형이다. 도11, 도 13은 디지털단위 3 , 디지털단위 4에 근거한 K3, K5개의 군집이다. 본 발명의 실시예에서, 시작부분 정지영상정보도 속성이고 타겟속성인 동영상정보와의 합으로 디지털단위 3 혹 은 4가 된다. 본 발명의 실시예에서, 도 11 혹은 도 13은 도 7 내지 도 8의 입력창에 입력된 변수값에 의해 동영상의 단계가 분할될 때 붙여진 라벨값에 의해 만들어진 계층적 군집화의 계통도이다. 도 19의 시계열분할 선택라벨링은 '데이터단위 3 혹은 데이터단위 4'를 '디지털단위 3 혹은 디지털단위 4'를 기준으로 동영상을 분할한다. 본 발명의 실시예에서, 디지털단위 3 혹은 디지털단위 4는 시계열분할 선택라벨링과 데이터 정렬을 병행하여 동영상을 분할한 것이다. 사용자는 정렬된 동영상의 순서를 정렬하기 위해 동영상 혹은 동영상 의 순서를 나타내는 라벨값에 대해 ACCEPT 혹은 REJECT의 라벨을 붙여 분할된 동영상의 순서를 선택하거나 거 부한다 디지털단위 4는 사용자가 라벨분류를 참조하여 아바타(인간)의 동작을 약 0.5~3초 내외의 특징적인 세부 동작 단계들로 분할하는 시계열분할 선택라벨링에 의해 분할된 동작 동영상정보이다. 본 발명의 실시예에서, 아바타(인간)의 동작을 약 0.5~3초 내외의 특징적인 세부 동작 단계들로 분류한 라벨분 류는 표 5 혹은 표 10 혹은 표 15이다.디지털단위 3 은 데이터단위 3과 동일한 방식으로 속성(정지영상정보)과 타겟속성(동영상정보)의 합이 전산화 된 것이다. 디지털단위 4는 데이터단위 4와 동일한 방식으로 속성(정지영상정보)과 타겟속성(동영상정보)의 합 이 전산화 된 것으로 정의한다. 본 발명의 실시예에서, 다수의 사용자(공군 사관생도 및/또는 전투기 조종사 등)가 영화 '탑건'의 전투기 조종 장면 약 1분간을 가상 전투기 시뮬레이터(VEHICLE VR 시뮬레이터)를 이용하여 따라서 조종하고 라벨링을 진행하 여, 도 15의 유도알고리즘에 사용하는 데이터단위 및 디지털단위별 데이터 셋을 획득한다. 전투기 조정은 조종의 세부적인 조종술마다 각 방식의 특징적인 동작이 있기 때문에 다수의 사용자가 유사한 가 상 비행을 하면 전체동영상이 1~2초 내외의 짧은 동영상으로 분할된다. 본 발명의 실시예에서, 다수의 사용자가 영화 '라이언 일병 구하기'의 전투 장면 약 1분간을 VR 트레드밀을 이 용하여 전동콘트롤러형 무기를 격발하고 라벨링을 진행한다. 영화 속 보병이나 공병의 움직임(소총 격발 및 슈 류탄 던지기 등의 연속동작)도 1~2초 내외의 짧은 동영상으로 분할이 가능하다. 본 발명의 실시예에서, 디지털단위 3의 시계열 분할 방식은 다음과 같다. 표1 내지 표 4와 같은 라벨분류 등을 참조하여 치과의사 및 의사들의 라벨링을 하여, 도 15 의 유도알고리즘 이 고도화 된다면, 'GNN 회귀모델 1형'은 정지영상정보를 반환하면서 플랫폼 사용자(의사, 치과의사 등) 에게 분할시점 및 라벨값 (s1, s2, s3, k)을 반환한다. 반환값에 대해 사용자는 시계열분할 선택라벨링을 한다. 본 발명의 실시예에서, 표 17는 상악중절치 라미네이트 11번(치식) 삭제를 진행하는 30초 동영상을 10단계로 분 할하여 30초를 약 2~4초 간격으로 분할하여 설명한다.사용자의 시계열분할 선택라벨링을 통해 디지털단위 4로 동영상의 분할이 가능하다. 표 17 변수값(라벨값) 상악 중절치 라미네이트 치료를 위한 11번 치아 삭제 방법(세부 동작 단 계)정보형태 1 치아삭제 전에 미리 제작한 치아삭제용 인덱스를 구강 및 치아에 위치시 킨다영상 등 2 구강 및 치아에 위치되어진 인덱스를 치과의사가 눈으로 확인하고 삭제 량을 측정한다. 3 치과의사는 자신이 판단으로 삭제량을 정하고 치과용 핸드피스의 depth gage bur(삭제할 깊이를 치아에 표시하는 치아 삭제용 핸드피스 버, 칼 날)를 체크하여 핸드피스에 장착한다. 4 치경부 3분의 1의 예상 삭제 깊이를 depth gage bur로 삭제한다. 5 치아 중앙부 3분의 1의 예상 삭제 깊이를 depth gage bur로 삭제한다. 6 치아 절단부 3분의 1의 예상 삭제 깊이를 depth gage bur로 삭제한다. 7 치아 치경부 3분의 1를 실제 치아 삭제용 핸드피스 bur로 삭제한다. 8 치아 중앙부 3분의 1를 실제 치아 삭제용 핸드피스 bur로 삭제한다. 9 치아 절단부 3분의 1를 실제 치아 삭제용 핸드피스 bur로 삭제한다 10 상악 중절치 전체 치아를 핸드피스의 트리밍 bur(다듬는 칼날)로 다듬고 미세하게 삭제한다 위와 같은 10단계로 분할되는 여러 환자(아바타 및 디지털카데바)들을 대상으로 한 동영상정보들이 도 12에서 같은 특정군집에 속해 있다고 하더라도 해당 동영상에서 세밀한 수술과 시술의 순서는 집도하는 의사의 술기에 따라 다를 수가 있다. 다른 순서는 표 5의 라벨 순서를 기준으로 전처리하여 분류모델에 적용시킬 수 있다. 또한, 단계가 순서가 다르거나 생략된 부분 및/또는 추가된 부분에 대해서 표 5의 라벨 순서를 기준으로 동영 상 정보를 정렬하여 군집화한다.라벨분류(표 5)를 참조하여 치과의사가 라벨링을 한다면 인공지능의 반환에 대해 치과의사는 ACCEPT, REJECT 라 벨을 붙여 분할시점(정지영상정보)에 대해 시계열분할 선택라벨링을 하면, 분류모델은 라벨링이 된 정보 를 다시 분류하고 'GNN 회귀모델 1형' 은 더욱 집단지성화된 분할시점(정지영상정보)과 라벨값을 반환하게 된다. 인공지능의 반환에 대해 치과의사는 ACCEPT(705, 805), REJECT(706, 806) 버튼으로 선택한다. 위 방식으로 치과의사가 라벨링을 한다면, 동영상정보를 분할하여 정지영상정보를 반환하는 GNN 회귀모델 1형이 정지영상을 반환하면서 도 15의 유도알고리즘은 치과의사에게 분할시점(속성값) 및 라벨값 반환한다. 인공지능의 예측값에 대해 치과의사가 ACCEPT(705, 805), REJECT(706, 806) 버튼을 눌러 라벨을 붙여 분할시점 (속성 혹은 속성의 라벨값)을 선택하거나 거부하면 하면, 분류모델은 라벨링이 된 정보를 다시 분류하고 'GNN 회귀모델 1형'은 더욱 집단지성화된 분할시점(정지영상정보)과 라벨값을 반환한다. 결국, 충분히 집단지성화 된 디지털단위 4가 생성된다. 본 발명의 실시예에서, 다수의 사용자가 VR 시뮬레이터 (VR 트레드밀)을 사용하여 표 10의 2019년 번리전 프리 미어 홈경기 70m 폭풍 드리블 11초간을 최대한 유사하게 재현할지라도 사용자마다 인프런트 드리블과 아웃프런 트 드리블의 순서를 잘못할 수 있다. 이런 경우, BCI를 이용하여 시계열분할 선택라벨링을 하고 정렬을 통해서 순서를 보정할 수 있으며 노이 즈 데이터를 제거할 수도 있다. 8. 신체부위별 선택라벨링을 수신하는 단계 상기 영상정보는 제2 영상정보이고, 상기 제2 선택라벨링 정보 수신 단계는 신체부위별 선택라벨링 을 포함한다. 신체부위별 선택라벨링이 포함된 제2 선택라벨링 정보는 도 14의 계층적군집이 된다. 제2 계층적 군집은 도 14의 디지털단위 5를 기준으로 전산화된 계층적군집이다. 본 발명의 실시예에서, 계층적 군집화 라벨링이 생략되는 경우에도 신체 부위별 선택라벨링은 제 2 선택 라벨링에 포함되어 실행되기도 하고 2 선택라벨링 전후에 실행될 수 있다. 디지털단위 5는 디지털단위 4와 동일한 방식으로 속성(정지영상정보)과 타겟속성(동영상정보)의 합 이 전산화 된 것이다. '데이터단위 3 혹은 데이터단위 4 혹은 디지털단위 3 혹은 디지털단위 4'는 신체부위 별 선택라벨링에 의해 '디지털단위 5'로 전산화된다. 아바타(인간)의 동작에서 신체부위별로 동작 순서를 정하기 위해 신체부위별로 순서를 정하는 라벨을 붙여 실제 동영상에서의 동작 순서를 바꾸기 위한 라벨링을 하는 것을 '신체부위별 선택라벨링'이라고 정의한다. 본 발명의 실시예에서, '신체 부위별 선택라벨링'을 한 후, 동영상 데이터 정렬에 따른 전처리작업(삭제, 추가 등)등에 대해서 ACCEPT, REJECT 라벨을 붙여 선택하거나 거부할 수 있다. 사용자가 라벨분류를 참조하여 제2 영상정보에 신체부위별 선택라벨링을 한 다음, 제2 분류모델 을 유도한다. 사용자가 라벨분류를 참조하여 제2 영상정보에 신체부위별 선택라벨링 을 하고 이로 인해 디지털단 위 5로 동영상이 분할되는 단계이다. 디지털단위 5에서 속성은 아바타(인간)의 분할된 동작 동영상 의 f번째 단계의 끝부분 정지영상정보이고 도 6의 붉은색 사각형이다. 도 6을 참조하면, 분할된 동작 동영상의 f번째 단계의 끝부분 정지영상정보 또한 속성에 해당되고 도 6의 마지 막 붉은색 사각형이다. 도 14는 디지털단위 5에 근거한 K6 개의 군집이다. 도 15의 유도알고리즘에 사용되는 디지털단위 5은 도 3의 분할된 직육면체이다. '신체부위별 선택라벨링'에 의해 디지털단위 5로 아바타(인간)의 동작 동영상이 분할된다. 본 발명의 실시예에서, 시작부분 정지영상정보도 속성이고 타겟속성인 동영상정보와의 합으로 디지털단위 5가 된다. 본 발명의 실시예에서, 도 14는 도 7 내지 도 8의 입력창에 입력된 변수값(라벨값)에 의해 동영상의 단계가 분 할될 때 붙여진 라벨값에 의해 만들어진 계층적 군집화의 계통도이다. 본 발명의 실시예에서, 라미네이트 치아 11번 삭제에 대해서도 대부분의 치과의사들이 치아 삭제용 인덱스를 제 작하여 치아삭제를 하지만 인덱스를 제작하지 않는 치과의사도 있고, depth gage bur를 사용하지 않는 사람도 있다. 위 차이를 기준으로 계층적 군집화를 하고 동영상 정보를 정렬하여 전처리한다. 치아삭제시 순서(치경부, 중앙 부, 절단부 순서 등)를 라벨분류의 기준으로 하지 않고 자신만의 순서대로 진행하는 사람도 있을 수 있다. 이런 경우, 상악 중절치 치아의 중앙부, 절단부, 치경부와 같은 신체 부위에 대한 세부적인 순서를 지정하는 라 벨링을 통해서 분할된 동영상의 순서를 지정하고 라벨링 순서에 맞는 라벨분류를 만든다. 또한, 위 라벨링의 순서대로 동영상정보들을 정렬한다. 동일한 특정 군집(본 발명의 실시예에서, 11번 치아를 인덱스를 사용하지 않고 삭제하는 방식)에 포함되었으나 치아삭제의 순서(치경부, 중앙부, 절단부 삭제 순서)가 다른 동영상 및 정지영상 정보들에 대해 신체부위의 순서 라벨링과 동영상정보의 정렬 등의 전처리작업을 통해 서 분류모델의 오차값들을 줄이고 분류모델의 정확도를 높힌다. 본 발명의 실시예에서, 어떤 치과의사가 치경부, 중앙부, 절단부 순으로 치아삭제를 하고 어떤 치과의사가 중앙 부,절단부, 치경부 순으로 치아를 삭제했다면 전부 치경부,중앙부, 절단부 삭제 순서와 같은 방식으로 동영상정 보를 정렬하고 이 순서대로 분할하여 군집화 한다. 또한, 치과의사가 마우스의 화살표를 이용하여 신체의 특정 부분을 가리키거나 신체의 특정 부위를 생각만 한다 면 사용자가 마우스로 가리키거나 생각한 부위의 경계선 및 경계면를 객체인식(OBJECT DETECTION)을 통해 인공 지능이 반환한다. 또한, 인공지능은 치료순서에 대한 정렬된 정보를 사용자에게 반환하다. 이에 대해 사용자는 '자신이 의도하거 나 생각한 부위가 맞다, 틀리다' 및/또는 '자신이 의도한 순서가 맞다, 틀리다' 및/또는 '순서에 대한 라벨값 이 맞다, 틀리다'를 판단한다. 이와 같이 브레인 컴퓨터 인터페이스를 이용한 판단만으로 동영상 및 정지영상 등에 ACCEPT, REJECT 라벨을 붙 이고 정렬을 한다. 위 라벨링을 반복하여 15의 유도알고리즘에 적용한다. 본 발명의 실시예에서, 표 6에서 건강한 성인의 구강에는 약 28개의 치아가 있고 치아마다 치식(치아번호)가 있 다. 상악 우측 중절치는 11번이다. 치식 22, 21, 11, 12 치아 4개를 라미네이트 치료를 위해 삭제하는 술식을 하는 경우에, 모든 치과의사들이 라 미네이트를 하기 위해 치아삭제를 할 때, 일정한 치아번호(치식)의 순서대로 진행하는 것이 아니므로 위 동영상 정보를 일정한 순서(치식)대로 정렬하고 삭제되거나 추가된 동영상정보에 대해서도 전처리를 한다. 시계열 분할을 할 경,우 계층적 군집화와 더불어 신체 부위의 구체적 시술 순서에 대한 정렬(치식순서)를 동시 에 진행하면 좀더 정확한 군집화가 가능하다(계층적 군집화 및 신체 부위별 선택). 또한, 좀 더 세부적인 신체부위별 선택라벨링을 하고자 한다면, 상악중절치 라미네이트 치아삭제(11번 치 아)의 방식과 순서도 치과의사마다 다를 수 있으므로 라벨분류(일정한 기준)을 근거로 동영상정보를 정렬하고 생략되거나 추가된 동영상정보에 대해서 전처리를 한다. 본 발명의 실시예에서, 메타버스 축구게임의 동영상에서 0.5~2초 이하의 순간적인 작은 데이터 크기의 디지털단 위로 분할된 동영상을 얻고자한다면 신체부위별 선택라벨링과 정렬을 이용하여 동영상을 세분화하여 분할 한다. 손흥민이 인스텝 드리블 3스텝 롱동작을 할때, 축구공이 표 11의 발가락 터치, 1 스텝 달리기, 발목 터치, 2 스 텝 달리기 순서대로 축구공이 손흥민의 발에 순간적인 터치를 하였는데 이것을 재현한 특정 사용자가 발목 터치, 1 스텝 달리기, 발가락 , 2 스텝 달리기 순으로 터치하고 달리기를 하였다면 특정 사용자의 인프런트 드리블의 터치순서 및 달리기의 순서에 대해 BCI를 이용하여 신체부위별 선택라벨링을 한다. 본 발명의 실시예에서, 표 15에서 열린음악회 2022년 7월 8일 방송의 제니의 k번째 동작이 앞뒤 웨이브라고 했 을 때, 표 16에서 블랙핑크 제니의 앞뒤 웨이브 동작은 왼쪽 팔 들기, 오른쪽 팔 들기, 가슴 움직이기, 배 움직 이기 , 골반 움직이기, 다리 움직이기 순으로 들거나 앞뒤로 움직인다. 특정 사용자가 다리 움직이기, 골반 움 직이기, 배 움직이기, 가슴 움직이기, 오른쪽 팔 들기, 왼쪽 팔들기 순서대로 앞뒤 웨이브를 했다면 BCI를 이용 하여 신체부위별 선택라벨링을 한다. 특정사용자의 동작 동영상을 블랙핑크 제니의 동작 순서대로 정렬을 한다. 또한, 총 3분 14초의 동영상은 1~2초 내외의 동영 상, 약 200개로 시계열 분할이 가능하다. 춤동작은 머리, 손, 발, 몸통의 움직임의 조합이 연속된 것이다.신체부위별 선택라벨링을 하지 않고 시계열분할 선택라벨링을 할 수 있다. 9. 이하 반복하는 단계 10. 제1 기초 로보틱스 영상정보에 기반하여 제1 로보틱스 영상정보 출력(output) 또는 생성 단계 상기 영상정보는 제1 영상정보 또는 제2 영상정보이고, 상기 제1 영상정보 및/또는 제2 영상정보에 대한 정보 발신의 단계는 제1 기초 로보틱스 영상정보에 기반하여 GNN 및 GAN 중 적어도 어느 하나에 의해 영상정보를 출 력(output) 혹은 생성하거나 출력(output) 및 생성하는 제1 로보틱스 영상정보 출력(output) 또는 생성 단계를 포함한다. '사용자단말'과 연결하여 로봇의 움직임에 대한 공간적 시간적 좌표를 정확히 계측하면서 도7 내지 도8의 어플리케이션 화면(700, 800) 형태의 디스플레이 장치 및 사용자 인터페이스를 통해 로봇의 움직임 을 평가하여 사용자가 로보틱스 선택라벨링을 하는 방식으로 고도화하여 컴퓨팅장치상에서 작동 하는 집단지성 모델로 추론된 로보틱스 프로그래밍을 '집단지성 로보틱스'라 정의한다. 본 발명의 실시예에서, 도 7 내지 도8의 애플리케이션 화면(700, 800)에서 출력되는 시각데이터는 사용자 단말 이 제공하는 로봇의 가상현실, 증강현실, 확장현실에서의 동작 화면이다. 로보틱스 영상정보는 도 3 내지 도 6의 속성 및 타겟속성에 해당되고 집단지성 로보틱스에 의해 출 력(output) 및/또는 생성되는 시각데이터이다. 제 1 집단지성 로보틱스는 제1 기초 로보틱스 영상정보가 입력되어 프로그래밍 되고 제1 로보틱스 영상정보를 출력(output) 및/또는 생성한다. 도 21의 기초 로보틱스 영상정보는 메타버스 사용자의 행동 정보 및 위치정보가 가상환경의 좌표에 정합 되도록 동기화된 상태에서 사용자단말로부터 확보된 아바타(인간)의 동작 데이터 가 컴퓨팅장치 에서 로봇 동작 데이터로 재구성된 것이다. 아바타(인간) 동작의 시각데이터 는 도 20의 GAN 및/또는 GNN 예측모델의 예측값이고 제1 영상정 보 및/또는 제2 영상정보이다. 본 발명의 실시예에서, 아바타(인간) 동작의 시각데이터는 아바타가 조작하여 작동하는 vehicle 움직임의 시각데이터가 포함한다. 사용자 단말 의 사용자 인터페이스상의 좌표계와 로봇 동작에서의 좌표계의 오차를 줄이기 위해 로 봇 크기 기반 실제 거리 좌표계를 추정하고 로봇 관절 별 각도를 추출하며 제어한다. 본 발명의 실시예에서, '치아삭제 VR 시뮬레이터, 수술 VR 시뮬레이터, VEHICLE VR 시뮬레이터, VR 트레드밀의 시각 데이터를 이용하여 로봇팔 형태 혹은 휴모노이드 형태 혹은 VEHICLE 형태의 로봇을 제작한다.)의 일종이다. 도21을 참조하면, 기초 로보틱스 영상정보는 집단지성 로보틱스에 입력된다. 집단지성 로보틱스 에 포함된 GAN 및/또는 GNN 로보틱스 예측모델은 인터페이스 API과정을 포함하고 예측모델은 도7 내지 도 8의 어플리케이션 화면(700, 800)에 로보틱스 영상정보를 출력(output) 및/또는 생성한다.GAN 및/또는 GNN 로보틱스 예측모델은 GAN 및/또는 GNN 예측모델와 동일한 방식의 로보틱스 동작에 관 한 시각데이터의 모델이다. 집단지성 로보틱스에서 출력된 시각데이터는 로봇 시뮬레이션 엔진으로 전달되고 로봇과의 API 통 신을 통해 로봇을 작동시키고 그래픽스 엔진을 거쳐 디스플레이 장치 및 사용자 인터 페이스를 통해 출력된다. 본 발명의 실시예에서, 컴퓨팅장치에서의 로보틱스의 프로그래밍은 다음과 같다. ROS와 OpenCV, PCL을 활 용하여 비전 센서를 ROS와 인터페이스하고 OpenCV(Open Source Computer Vision) 및 PCL(Point Cloud Librar y)과 같은 라이브러리를 이용하여 프로그래밍한다. 본 발명의 실시예에서, 메타버스 병원 및 치과병원 게임에서 사용자단말은 환자의 병변에 대한 3D 모델을 작성하여 병변의 위치 및 상태, 영상정보를 기반으로 3D 환자 좌표계와 수술대 위에 놓인 환자의 좌표계를 정합 되도록 정합한다. '실제 의료기관의 데이터 수집 및 가상 치아 시뮬레이터 및 가상 수술 시뮬레이터'에서 수집 된 군집의 활용에 있어서, 연관규칙과 앞에서 예측한 것이 뒤에 입력으로 가면서 반복되는 sequencial 모델을 통해 '자동화 수술 및 시술이 가능한 인공지능'의 초기 모델을 만든다. 제1 로보틱스 영상정보 는 도 4 내지 도 6의 제1 속성 및 제1 타겟속성이다. 기초 로보틱스 영상정보 는 도 10 내지 도 14의 군집중의 하나인, 특정군집에 속하는 데이터이다. '로보 틱스 영상정보 또한 같은 특정군집에 속하는 데이터가 된다. 11.상기 영상정보에 대한 제1 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제1 로보틱스 분류모 델 유도 단계 상기 영상정보인 제1 로보틱스 영상정보 에 대해 제1 로보틱스 선택라벨링 정보를 수신 한다. 제1 로보틱스 분류모델 유도 방식은 도15의 제1 분류모델의 방식과 동일하다. 제1 집단지성 로보틱스의 출력(output)인 제1 로보틱스 영상정보에 대해 제1 로보틱스 선택라벨링 을 한다. 제1 로보틱스 선택라벨링을 하여 획득된 시각데이터를 분류한 분류모델을 '제1 로보틱스 분류모델'라 정의한다. ,'이하' 로보틱스 분류모델은 반복된다. 로보틱스 선택라벨링은 도 20의 선택라벨링와 동일한 방식이다 도7 내지 도8의 어플리케이션 화면(700, 800)형태의 도 21의 사용자 인터페이스에서 출력되는 로봇의 동 작에 대해 사용자가 도 21의 로보틱스 선택라벨링을 한다. 로봇의 동작은 로보틱스 영상정보이다. ‘집단지성 로보틱스’의 초기 모델의 로봇은 동작에 오류가 많을 수 있다. 다소 부정확한 로봇의 움직임에 대해 로보틱스 개발자는 로보틱스 선택라벨링 및 분류를 통해 지도학습을 한다. 가상 시뮬레이 션에서 생성 및 출력(output)된 아바타와 아이템, 그리고 공간 환경, 서사 등을 집단지성 로보틱스에 제 공하고 사용자가 로보틱스 선택라벨링을 하는 방식으로 인공지능에 지도학습을 한다.도 15의 유도알고리 즘은 집단지성 로보틱스을 고도화한다. 본 발명의 실시예에서, 로보틱스 선택라벨링은 메타버스의 영상정보처리와 동일한 방식의 계층라벨링 또 는 시계열분할선택라벨링 또는 신체부위별 선택라벨링를 포함한다. 도 21에서 사용자 인터페이스에 출력(output)되거나 생성된 영상정보는 로보틱스 선택라벨링에 라 벨링되어 로보틱스 분류모델을 통해 분류된다. 분류된 시각데이터는 집단지성 로보틱스에 전달되는 라벨링된 로보틱스 라벨정보이다. 제1 로보틱스 분류모델에 분류된 정보가 제1 로보틱스 라벨정보이다. 본 발명의 실시예에서, 가상 시뮬레이션 게임의 각 분야 전문가에 해당되는 다수의 사용자들이 사용자단말(20 0)의 인터페이스를 통해 로보틱스 선택라벨링을 진행하여 충분한 시각데이터가 확보되면 VR 시뮬레 이터를 조작하는 인공지능 로봇이 제작된다. 로봇 관절과 팔과 다리 등을 이용하여 VR 시뮬레이터를 조작하는 집단지성 로보틱스의 초기 모델이 개발 된 경우에 사용자들의 라벨링을 통해 집단지성 로보틱스 모델의 능력을 지도학습으로 고도화한다. 고도화가 되면 실제 현실에서 동작이 가능한 집단지성 로보틱스 초기 모델이 개발될 수 있고 이 경우에도 사용자 및 전문가들의 로보틱스 선택라벨링을 통해 집단지성 로보틱스를 지도학습으로 고도화한다. 도21 및 도22의 집단지성 로보틱스는 도15의 유도알고리즘을 반복 적용하고 라벨링을 반복하여 집 단지성 로보틱스의 모델을 고도화 한다. 집단지성 로보틱스의 고도화에 따라 로봇팔을 이용한 실제 의료 현장에서의 자동화 시술 및 수술이 가능한 인공지능 초기 모델이 개발될 수 있고 이 경우에도 의사들의 로 보틱스 선택라벨링을 통해 인공지능 초기모델의 자동화 능력을 지도학습으로 고도화한다. . 본 발명의 실시예에서, 로봇머리, 로봇팔, 로봇다리, 로봇 몸, 로봇 관절 등을 사용하는 휴머노이드 형 로봇을 제작하고 자율주행형 차량, 드론, 비행기 등의 비히클 로봇 및 인공지능 치과의사로봇 및 인공지능 의사로봇을 제작한다. 12.상기 영상정보 또는 제2 기초 로보틱스 영상정보에 기반하여 영상정보를 출력(output) 혹은 생성하거나 출력 (output) 및 생성하는 제2 로보틱스 영상정보 출력(output) 및/또는 생성 단계 제2 로보틱스 영상정보는 제2 집단지성 로보틱스의 예측값인 영상으로, 본 발명에 있어서 반복된 유도알고리즘의 반복된 적용으로 고도화된 예측모델의 예측값으로 정의한다. 제2 로보틱스 영상정보는 도 4 내지 도 6에서 제2 속성과 제2 타겟속성이다. 제1 로보틱스 분류모델에 의해 분류된, 상기 영상정보인 제1 로보틱스 라벨정보는 제2 집단지성 로 보틱스 에 입력되고 제2 기초 로보틱스 영상정보도 제 2 집단지성 로보틱스에 입력되어 단일 모델로 프로그래밍 되고 제2 로보틱스 영상정보를 출력(output) 및/또는 생성한다. 13. 상기 영상정보에 대한 제2 로보틱스 선택라벨링 정보를 수신하여 분류모델을 유도하는 제2 로보틱스 분류모 델 유도 단계 상기 영상정보인 제2 로보틱스 영상정보에 대해 제2 로보틱스 선택라벨링 정보를 수신한다. 제2 로보틱스 분류모델 유도 방식은 도15의 제2 분류모델의 방식과 동일하다. 제2 로보틱스 영상정보는 사용자에 의해 제2 로보틱스 선택라벨링 방식으로 라벨링되고 라벨링된 데이터에 의해 제2 로보틱스 분류모델이 유도된다. 분류된 제2 로보틱스 라벨정보은 제3 집단지성 로보틱스에 입력된다.이하 반복된다 제1 로보틱스 라벨정보 및 제2 기초 로보틱스 영상정보는 제 2 집단지성 로보틱스의 단일 모 델로 학습된다. 제1 기초 로보틱스 영상정보 입력에 의해 제1 집단지성 로보틱스가 프로그래밍 되고, 제2 기초 로 보틱스 영상정보와 제1 로보틱스 라벨정보의 입력에 의해 제2 집단지성 로보틱스가 프로그래 밍 되고, '이하' 반복된다. 도22를 참조하면, 모델 관점에서 메타버스의 출력(output) 및/또는 생성 데이터인 제2 기초 로보틱스 영상정보 와 제1 집단지성 로보틱스의 출력(output) 및/또는 생성 데이터인 제1 로보틱스 라벨정보의 정확도나 정교함의 장단점이 서로 상이 할 수 있는데, 두 과정이 서로 다른 형태의 라벨링 과정이지만, 모델이 두 과정의 장점만을 수용하게 하기 위해서는 양 라벨 접근방법의 수정된 제2 기초 로보틱스 영상정보와 출력(output) 및/또는 생성 데이터인 제1 로보틱스 라벨정보는 서로 다른 모델이 어닌 동일 모델(단일모 델)의 학습 데이타로 사용된다. 제1 로보틱스 선택라벨링 및 제1 로보틱스 분류모델로 1차 라벨링 된 이후 제안한 출력(output) 및 /또는 생성 데이터인 제1 로보틱스 라벨정보는 제2 로보틱스 선택라벨링에 의해 2차 라벨링 되고, 이 과정은 계속하여 반복된다. 과거 라벨된 데이터(제1 로보틱스 라벨정보, 2205)는 다른 라벨 데이터(제2 기초 로보틱스 영상정보, 2201)와 함께 반복 수행하게 되며, 한번 학습했던 데이타 및/또는 유사한 레이블 값 또한 매 반복 학습(epoch)에 계속 등장하여, 여러번의 실험을 거치는 과정이 필요하며 해당과정에서 집단지성의 레이블 값은, 취사 선택 및 평균 화 되어 모델에 반영된다. 본 발명의 실시예에서, 움직임이나 조합의 범위가 한정되 있는 자동 수술 및 치과시술 로봇 경우에는 계층적 군 집화를 할 필요없이 로보틱스 선택라벨링을 통하여 영상정보의 잘된 부분과 잘못된 부분을 하나하나 세밀 하게 라벨링을 한다. 집단지성 로보틱스의 로보틱스 선택라벨링은 아바타(인간)의 선택라벨링과 동일한 방식이다. 자유도가 높은 휴모노이드 로봇 이나 비히클 로봇 (예를들어 댄스하는 로봇, 축구하는 로봇, 2족 보행 로봇)의 경우에는, 계층적 군집화를 위한 도 19에서 시계열분할 라벨링, 신체부위별 선택라벨링 등을 통하 여 디지털단위 3 및/또는 디지털단위 4 및/또는 디지털단위 5로 로보틱스 영상정보를 분 할 한 후, 로보틱스 선택라벨링을 한다. 14. 이하 반복되는 단계 15. 상기 영상정보는 NFT가 부여된 영상정보이고, 상기 NFT가 부여된 영상정보 플랫폼 제공 시스템은 사용자 및 참여자 및 기업들이 이익을 창출하고 돈을 벌면서 재미요소를 배가하는 원순환 구조 본 도 24를 참조하면, GAN 및/또는 GNN을 이용한 가상 아바타 생성 및/또는 출력(output) 플랫폼 제공 시스 템은 사용자(user), 참여자(인플루언서 또는 SNS에 자신의 캐릭터를 홍보하는 개개인), 기업(광고주 및/ 또는 제조사)들이 서로서로 이익을 창출하고 돈을 벌면서 재미요소를 배가하는 플랫폼으로서의 flywheel(원순 환 구조)이다. 도 24를 참조하면, GNN 및/또는 GAN 예측모델은 도 1의 컴퓨팅장치에서 작동한다.GNN 및/또는 GAN 예측모델은 인플루언서로부터 제공 받은 기초 영상정보(도 23의 제1 기초 영상정보,2301 제 2 기초 영상정보, 2305)를 활용하여 마케팅 플랫폼에 NFT 아바타 및 아이템을 생성하거나 출력 (output)하고 국내외 NFT 마켓에 자동등록 할 수 있도록 프로그래밍 된다. 기업 및 투자자는 인플루언서의 프로필(동영상, 사진 등)NFT 및 상품 NFT를 소유할 수 있고 마케팅 및/또 는 기업 홍보에 활용한다. GNN 및/또는 GAN 예측모델은 도 20의 GAN 및/또는 GNN 예측모델(200 5)과 동일한 것이다. 프로필(동영상, 사진 등)은 생성된 아바타이고 상품은 아이템이다. 메타버스상에서 NFT는 아바타와 아이템을 현실세계의 소유자, 생성자, 광고주, 실물 상품 등과의 디지털 트윈 (digital twin)을 위한 매개로 연결된다 또한, 참여자에게는 홍보비를 제공하고, 사용자에게는 아바타 및 아이템에 대한 NFT를 발급함으로써 유일성이 부여되어 가치가 측정되고, 가치에 따른 비용이 환급됨으로써, 이익을 창출한다. 도 18를 참조하면, 컴퓨팅 장치는 사람 몸을 별도 객체화하고, 성별, 나이, 체형, 동양인 등 정보를 메타 정보와 연결한다. 아이템(상품 등)을 별도 객체화하여 메타정보와 연결한다.이때, 각 아바타 ID 는 유저ID 및 아이템 ID 및 NFT ID와 연결된다. 다양한 현실의 가치 및 재화 정보는 Meta 데이터 형태로 포함되어 NFT 화 될 수 있으며, 이는 아이템 NFT형태로 유일성이 보장되면서 매매 및 거래가 될수 있다. 플랫폼은 해당 NFT 소유가 현실의 가치 이용의 사용권이 될 수 있도록 보장하며, 서비스의 사용 내역 및 단계는 플랫폼 데이터base 와 연동되어, NFT 메타정보가 갱신되고 참 조된다. 본 발명의 실시예에서, NFT 소유에 대한 현실의 가치는 환자의 아바타인 디지털카데바 사용권등이 있다 도 24를 참조하면, 도 1의 컴퓨팅 장치는 실제 판매되는 제품을 메타버스 내 아이템으로 생성할 수 있고, 현실에서 실제 제품을 구매할 수 있도록 인스트럭션을 제공할 수 있다. 본 발명의 실시예에서, 본 발명의 서비스를 이용하는 인플루언서는 본인의 아바타나 본 발명의 서비스를 본인의 네트워크 상의 SNS에 홍보할 수 있고, 컴퓨팅 장치는 네트워크 상의 SNS 채널에 업로드된 홍보 관 련 콘텐츠를 획득할 수 있다. 컴퓨팅 장치는 네트워크 상의 SNS 채널을 통해 유입된 사용자를 분석할 수 있고, 분석된 결과를 기초로 네 트워크 상의 SNS 에게 제공할 홍보비용을 정산할 수 있다. 컴퓨팅 장치는 각 인플루언서별로 상이한 링크를 생성하여 제공할 수 있고, 해당 링크를 통해 유입 되는 사용자에 대한 보상을 인플루언서에게 제공할 수 있다. 또한, 사용자의 가입 여부, 아이템 구매 금 액 등을 분석하여 인플루언서에게 추가 보상을 제공할 수도 있다. 본 발명의 실시예에서, 인플루언서는 연예인, 배우, 운동선수 등을 포함한다. 본 발명의 실시예에서, 메타버스 내의 땅이나 바다, 건물을 포함하는 각각의 영역에도 NFT가 부여되어, 부동산 등기부와 같은 역할을 수행하도록 한다. 사용자들은 NFT를 이용하여 각각의 영역을 거래한다. 본 발명의 실시예에서, 메타버스 게임에서 각 객체는 무늬, 색, 재질, 디자인 등의 복합 요소로 구성될 수 있고, 컴퓨팅 장치는 브랜드, 상품ID, 판매자ID, 생성자ID, 광고자ID, 소유자ID 등과 메타 정보를 연동하 고 NFT화 한다. 또한, 컴퓨팅 장치는 모자, 악세서리, 의상을 별도 객체화하고, 각 객체는 사용자, 생성자, 유일성ID 혹은 대표객체ID의 메타 정보와 연결한다. 이때, 각 아이템ID는 NFT ID와 연결될 수있다. 또한, 컴퓨팅 장치는 액세서리 등 사용자가 구매한 아이템에 NFT를 부여하고, 이에 기반한 거래가 메타버 스 내에서 가능하도록 구성한다. 본 발명의 실시예에서, 컴퓨팅 장치는 구매 완료된 아이템이 합성된 아바타에 NFT를 발급한다. 사용자는 해당 아바타에 대한 NFT를 발급받을 수 있으며, 이를 판매하여 수익을 얻는 것이 가능한다. 즉, 본 발명의 실시 예에 따르면, GAN 및/또는 GNN 예측모델을 통해 아바타에게 다양한 조합의 아이템을 코디네이션 하면서, 재미요소를 제공하고, 합성이 완료된 아바타에 대해NFT를 발급함으로써, 유일성을 제공하며, 이를 통한 수익을 얻을 수도 있다. 또한, 컴퓨팅 장치는 액세서리 등 사용자가 구매한 아이템에NFT를 부여하고, 이에 기반한 거래가 메타버스 내에서 가능하도록 구성한다. 본 발명의 실시예에서, 컴퓨팅 장치는 화장해보기, 옷입어 보기, 화장스타일, 패션스타일 추천받기, 연예 인의 영상에 나의 얼굴을 대입, 스타일 확인해보기 등의 서비스를 제공한다. 16. 상기 영상정보는 제2 영상정보이고, 상기 영상정보에 대한 정보 발신의 단계는 사용자가 행하는 사소한 실 수나 치명적인 실수에 대해 보정 및 수행 중지 Alert(경보)으로 개입하는 단계 도 20을 참조하면, 사용자의 판단에 의한 잘된 것과 잘못된 것에 대해 선택라벨링에 따라 인공지능에게 지도학습을 한다. 사용자단말에서 사용자가 행하는 사소한 실수나 치명적인 실수에 대해 보정 및 수행 중지 Alert(경보)으로 개입한다. 본 발명의 실시예에서, 워닝 시그널을 하는 방식으로 인간과 집단지성 로보틱스는 상호 작용을 한다. 수술 인공지능은 의사의 수술 과정에서 정교한 수술을 돕는 로봇팔 조향 장치에 햅틱 개념으로 포함되어, 잘못 된 수술을 시술하려 하는 경우, 진동등의 워닝 시그널을 줌으로 인하여, 의사와 상호 작용 및 개입이 가능하다. 해당 Alert 시그널을 무시하고, 수술을 집행하는 경우에는, 해당 상황에서 그렇게 행동하는 것이 올바른 정답이 다라는 별도의 라벨 데이타로 사용될 수 있으며, 이를 통해, 가상세계의 인공지능은 실존세계의 의사의 수술에개입하고 도움을 주면서, 그 피드백에 따라 사용자가 많아 질 수록 정교함이 배가 된다. 본 발명의 실시예에서, not ACCEPT 라벨 혹은 REJECT 라벨이 붙여진 동영상에서의 아바타(인간)의 행위에 대해 서, 인공지능은 지도학습하여 경보를 보낸다. 경보는 VR 시뮬레이터에서의 가상 수술이나 가상 운전, 비행 등에 도 가능하고 실제 수술이나 실제 운전, 비행 등에서도 가능하다. 본 발명의 실시예에서, 경보는 영상정보 혹은 음성정보 혹은 햅틱디바이스를 통해서도 가능하다. 본 발명의 실시예에서, 사용자가 가상의 전쟁게임의 전투기 조정에서 전투기를 조정하는 경우 및/또는 적기에 의해 격추되는 경우에, 사용자가 이 동영상에 ACCEPT 혹은 REJECT 라벨을 붙인다면 인공지능은 이에 대해 지도 학습 을 하게 되고 실제 전투기 조종사의 비행전투에서 잘못된 조정을 감지하여 경보를 보낸다. 예를들면, VR 트레드밀의 가상의 경찰 게임에서 사용자(도둑 역할)가 물건을 훔치거나 범죄를 저지르는 행위에 대해 REJECT 라벨을 붙인다면, 인공지능은 이를 지도학습 하게 되고 실제 경비 시스템에서 도둑의 행위를 감지 하여 경보를 보낸다. 17. 상기 영상정보는제2 로보틱스 영상정보이고, 상기 영상정보에 대한 정보 발신의 단계는 사용자가 행하는 실 수에 대해 보정 동작을 하거나 및 자율동작을 하는 단계 도 21 내지 도 22의 집단지성 로보틱스는 로보틱스 선택라벨링된 시각데이터에 대해 지도학습하고 인공지능 로봇은 스스로 동작한다. 사용자단말에서 사용자가 행하는 실수에 대해 보정 동작을 하거나 및 자율동작을 한다. 본 발명의 실시예에서, VR 시뮬레이터를 인공지능 로봇팔이 작동하고 작동 의사가 수술정보에 대해 라벨링을 하 는 것을 보상하는 방식으로 게임화한다. 추가로 라벨링 된 수술정보에 대해서는 기존 알고리즘 모델을 추가 fine tunning 하는 방식으로 의료인공지능을 고도화한다. 궁극적으로는 인공지능 로봇팔이 실제 인체에 대해 수술을 진행하고 이에 대해 의사가 라벨링을 할 수 있다. 본 발명의 실시예에서, ACCEPT 라벨 혹은 RJECT 라벨 혹은 not ACCEPT 라벨 혹은 not REJECT 라벨이 이 붙 은 정보는 인공지능이 사용자에게 경보를 하는데 사용되고 발생한 문제를 해결하거나 회피하기 위해 인공지능 스스로 동작하는데 사용된다. 집단지성 로보틱스의 자율동작은 VR 시뮬레이터에서도 가능하고 실제 현실 에서도 가능하다. 본 발명의 실시예에서, 가상의 전투기 비행게임에서 비히클 로봇이 전투기를 조정하여 적기를 격추하는 동영상 에 사용자가 ACCEPT(705, 805)버튼을 눌러 ACCEPT 라벨을 붙인다면 인공지능은 이에 대해 지도학습을 하게 되고 가상의 혹은 실제 전투기 조종사의 비행기 조정을 학습한다. 가상 전투기 비행게임에서 혹은 실제의 전투기 비 행에서 적극적인 작동으로 회피기동이나 공격기동을 할 수 있다 본 발명의 실시예에서, 가상의 자동차 조정에서 VEHICLE 로봇이 VR 시뮬레이터상에서 자율주행을 하는 것에 대 해 실제 사람이 선택라벨링을 하면 인공지능은 이를 지도학습하게 되고 VEHICLE 로봇은 점차적으로 고도 화된다. 고도화된 로봇은 실제 주행을 자동으로 할 수 있게 되고 이를 실제 사람이 다시 한번 선택라벨링을 하 여 인공지능은 더욱 고도화 된다 본 발명의 실시예에서, VR 트레드밀의 가상의 댄스 경연에서 휴모노이드 로봇이 댄스 경연을 하는 동영상에 실 제 댄스 전문가 혹은 도메인 전문가 혹은 로보틱스 개발자 혹은 사용자가 선택라벨링을 하면 인공지능은 이에 대해 지도학습을 하게 되고 휴노노이드 로봇의 동작은 점차적으로 고도화된다. 이상에서는 본 발명의 바람직한 실시예를 예시적으로 설명하였으나, 본 발명의 권리 범위는 이같은 특정 실시예 에만 한정되는 것이 아니며, 본 발명의 사상을 이해하는 통상의 기술자에게 자명한 범위 내에서, 구성요소의 부 가, 변경, 삭제, 추가 등에 의해서 다른 실시예를 제안할 수 있을 것이나, 이 또한 본 발명의 특허청구범위내에기재된 범주내에 속하는 것으로 해석하여야 할 것이다."}
{"patent_id": "10-2023-0034241", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 GAN과 GNN을 이용한 가상 아바타 및 아이템 의 생성 및/또는 출력(outpu t)과 집단지성 로보틱스 프로그래밍에 관한 플랫폼 제공 시스템을 도시한 도면이다. 도 2은 본 발명의 일 실시예에 따른 서비스 제공 장치인 컴퓨팅장치의 하드웨어 구성도이다. 도 3 은 본 발명의 도 4 내지 도 6의 분할된 동작 동영상에서 입체도형의 정의를 나타낸 개념도이다. 도 4는 본 발명의 아바타(인간) 및/또는 로보틱스의 동작에 대한 n개의 정보 수집을 입체도형으로 나타낸 개념 도이다 도 5는 본 발명의 아바타(인간) 및/또는 로보틱스의 동작에 대한 n'개의 정보 수집을 입체도형으로 나타낸 개 념도이다. 도 6는 본 발명의 아바타(인간) 및/또는 로보틱스의 동작에 대한 N개의 정보 수집을 입체도형으로 나타낸 개념 도이다. 도 7 내지 도 8은 본 발명의 일 실시예에 따른 사용자단말에서의 애플리케이션 화면(사용자 인터페이스)을 도시한 개념도이다. 도 9는 본 발명의 실제 현실데이터의 계층적 군집화의 계통도이다. 도 10는 본 발명의 데이터단위 3 를 기준으로 전산화된 계층적 군집화화의 계통도이다. 도 11는 본 발명의 디지털단위 3을 기준으로 전산화된 계층적 군집화의 계통도이다. 도 12는 본 발명의 데이터단위 4를 기준으로 전산화된 계층적 군집화의 계통도이다. 도 13은 본 발명의 디지털단위 4를 기준으로 전산화된 계층적 군집화의 계통도이다 도14는 본 발명의 디지털단위 5를 기준으로 전산화된 계층적 군집화의 계통도이다 도 15는 본 발명의 유도알고리즘에 정보처리된 데이터가 어떤 방식으로 적용되는 지를 나타낸 순서도이다. 도 16 은 본 발명의 일 실시예에 따른 GAN 및/또는 GNN을 이용한 가상 아바타 및 아이템의 생성 및/또는 출력 (output)플랫폼 제공 방법을 나타내는 도면이다. 도 17은 본 발명에 대한 GNN 회귀모델의 원리를 나타내는 도면이다. 도 18는 본 발명의 일 실시예에 따른 GAN을 이용한 가상의 아바타 및 아이템의 생성 방법을 나타내는 도면이다. 도 19은 본 발명의 라벨링에 의해 디지털단위가 생성되는 원리를 나타내는 도면이다. 도 20은 본 발명의 컴퓨팅장치에서 작동하여 사용자단말에서 출력 및 생성되는 시각랜더링의 원리를 나타내는 도면이다. 도 21 본 발명의 집단지성 로보틱스가 컴퓨팅장치에서 작동되는 원리를 나타내는 도면이다. 도 22는 본 발명의 로보틱스 라벨링에 의해 집단지성 로보틱스가 고도화되는 원리를 나타내는 도면이다. 도 23 은 본 발명의 기초 영상정보가 계속 수집될 경우, 기존 데이터와 같이 하나의 모델로 적용되는 원리를 나 타내는 도면이다. 도 24는 본 발명의 사용자 및 참여자 및 기업들이 이익을 창출하고 돈을 벌면서 재미요소를 배가하는 플랫폼으 로서의 원순환 구조를 나타내는 도면이다."}
