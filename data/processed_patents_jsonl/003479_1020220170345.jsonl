{"patent_id": "10-2022-0170345", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0085458", "출원번호": "10-2022-0170345", "발명의 명칭": "SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템 및 방법", "출원인": "재단법인대구경북과학기술원", "발명자": "이성진"}}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "각각이 제1 연산장치를 포함하는 복수의 에스에스디(SSD)들을 포함하고, 데이터를 저장하는 스토리지 서버;상기 스토리지 서버와 네트워크를 통해 연결되고, 주기적인 학습을 통해 업데이트되는 인공지능 모델을 생성하는 전이학습 서버;상기 전이학습 서버로부터 수신한 상기 인공지능 모델을 이용해 상기 데이터에 대한 메타데이터를 추출하는 추론 서버; 및상기 추출된 메타데이터를 저장하는 데이터베이스를 포함하고,상기 전이학습 서버는 제2 연산장치를 포함하고, 상기 학습의 제1 부분은 상기 제1 연산장치를 통해 수행하고,상기 학습의 상기 제1 부분을 제외한 나머지 학습부분을 포함하는 제2 부분은 상기 제2 연산장치를 통해 수행하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 제1 부분은 학습에 필요한 중간 데이터를 생성하고,상기 제2 부분은 상기 중간 데이터를 사용하여 직접적인 학습을 통해 상기 인공지능 모델의 생성을 위한 결과데이터를 생성하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 전이학습 서버는 상기 학습을 제어하는 전이학습부를 더 포함하고, 상기 전이학습부는 상기 데이터를 가진 상기 스토리지 서버에 상기 인공지능 모델의 생성 또는 업데이트를 위한 요청을 송신하고,상기 요청을 받은 상기 스토리지 서버는 상기 제1 연산장치를 이용하여 상기 중간 데이터를 생성하기 위한 특징추출 작업을 수행하고,상기 전이학습부는 상기 중간 데이터를 수신하여 상기 제2 연산장치에 전달하고 상기 제2 연산장치는 상기 결과데이터를 생성하기 위한 네트워크의 가중치 업데이트를 수행하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습시스템."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 전이학습 서버는 상기 메타데이터의 업데이트를 제어하는 오프라인 추론부를 더포함하고,상기 오프라인 추론부는 상기 데이터베이스 내에 부실 메타데이터가 존재하는지 판단하여 상기 부실 메타데이터가 존재하는 것으로 판단되면 상기 부실 메타데이터를 업데이트하기 위해 상기 스토리지 서버에 오프라인 추론요청을 송신하고,상기 요청을 받은 상기 스토리지 서버는 상기 제1 연산장치를 이용하여 상기 오프라인 추론을 수행하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 전이학습 서버는 상기 스토리지 서버로부터 상기 오프라인 추론을 통해 추출된 업데이트된 메타데이터를 수신하고, 상기 데이터베이스에서 상기 부실 메타데이터를 식별하여 상기 업데이트된 메타데이터로 갱신하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3항에 있어서, 상기 전이학습 서버는 생성된 상기 업데이트된 인공지능 모델에서 기존 인공지능 모델과 차이공개특허 10-2024-0085458-3-점만을 추출하여 상기 추론 서버 및 상기 스토리지 서버로 전달하는 SSD 오프로딩을 이용한 인공지능 추론 및학습 시스템."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "각각이 제1 연산장치를 포함하는 복수의 에스에스디들(SSD)을 포함하는 스토리지 서버가 사용자로부터 수신한데이터를 저장하는 단계;상기 스토리지 서버와 네트워크를 통해 연결되고 제2 연산장치를 포함하는 전이학습 서버가 주기적인 학습을 통해 인공지능 모델을 업데이트하는 단계;추론 서버가 상기 인공지능 모델을 이용하여 상기 데이터에 대한 메타데이터를 추출하여 데이터베이스에 저장하는 단계를 포함하고,상기 전이학습 서버가 인공지능 모델을 업데이트하는 단계는,상기 스토리지 서버의 상기 제1 연산장치가 학습 데이터의 특징 추출을 통해 중간 데이터를 생성하는 제1 단계;및상기 전이학습 서버의 상기 제2 연산장치가 상기 중간 데이터를 사용하여 학습을 통해 업데이트된 인공지능 모델을 생성하는 제2 단계를 포함하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 전이학습 서버가 상기 업데이트된 인공지능 모델을 상기 추론 서버 및 상기 스토리지 서버로 배포하는 단계를 더 포함하고,상기 업데이트된 인공지능 모델을 배포하는 단계는 상기 업데이트된 인공지능 모델과 기존 인공지능 모델의 차이점만을 추출하여 배포하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서, 상기 전이학습 서버가 상기 데이터베이스 내에 부실 메타데이터가 존재하는지 판단하는 단계;상기 부실 메타데이터가 존재한다고 판단되는 경우 상기 전이학습 서버가 상기 부실 메타데이터의 업데이트를위해 상기 부실 메타데이터가 가리키는 타겟 데이터가 저장된 스토리지 서버에 오프라인 추론 요청을 송신하는단계; 및상기 요청에 따라 상기 스토리지 서버가 상기 제1 연산장치를 이용하여 상기 타겟 데이터에 대해 상기 오프라인추론을 수행하는 단계를 더 포함하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 스토리지 서버가 상기 오프라인 추론을 수행하는 단계는 상기 오프라인 추론을 통해 추출된 업데이트된 메타데이터만을 상기 전이학습 서버로 다시 전달하는 단계를 포함하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 상기 전이학습 서버가 상기 타겟 데이터를 식별하고 상기 데이터베이스에서 상기 부실 메타데이터를 상기 업데이트된 메타데이터로 갱신하는 단계를 더 포함하는 SSD 오프로딩을 이용한 인공지능 추론 및학습 방법."}
{"patent_id": "10-2022-0170345", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제7항에 있어서, 상기 전이학습 서버가 인공지능 모델을 업데이트하는 단계는,상기 복수의 에스에스디들은 제1 에스에스디 및 제2 에스에스디를 포함하고,상기 제1 에스에스디의 제1 연산장치가 상기 제1 단계를 마치면 상기 제2 에스에스디의 제1 연산장치가 상기 제1 단계를 시작하고, 공개특허 10-2024-0085458-4-상기 제2 에스에스디의 상기 제1 단계는 상기 제1 에스에스디에 대한 상기 제2 연산장치의 상기 제2 단계와 동시에 병렬적으로 수행되는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법."}
{"patent_id": "10-2022-0170345", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 각각이 제1 연산장치를 포함하는 복수 의 에스에스디(SSD)들을 포함하고, 데이터를 저장하는 스토리지 서버, 상기 스토리지 서버와 네트워크를 통해 연 결되고, 주기적인 학습을 통해 업데이트되는 인공지능 모델을 생성하는 전이학습 서버, 상기 전이학습 서버로부 터 수신한 상기 인공지능 모델을 이용해 상기 데이터에 대한 메타데이터를 추출하는 추론 서버 및 상기 추출된 메타데이터를 저장하는 데이터베이스를 포함하고, 상기 전이학습 서버는 제2 연산장치를 포함하고, 상기 학습의 제1 부분은 상기 제1 연산장치를 통해 수행하고, 상기 학습의 상기 제1 부분을 제외한 나머지 학습부분을 포함하 는 제2 부분은 상기 제2 연산장치를 통해 수행한다."}
{"patent_id": "10-2022-0170345", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 SSD(Solid-State Drive) 오프로딩(off-loading)을 이용한 인공지능 추론 및 학습 시스템 및 방법에 관한 것으로, 보다 상세하게는 학습 및 추론 성능을 가속화하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2022-0170345", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "많은 양의 데이터가 사용자에 의해 생성되고 데이터 센터에 축적되면서, 이를 활용할 수 있는 인공지능 애플리 케이션을 위한 데이터 센터 시스템이 등장하였다. 인공지능을 위한 시스템은 애플리케이션의 품질을 효율적으로 진보시키기 위해 진화하고 있다. 종래의 시스템에서 사용하고 있는 데이터 센터의 구성은 training server, inference server, database 및 storage server의 네 가지 구성 요소로 이루어져 있다. Storage server에 있는 training dataset을 사용하여 training server는 인공지능 모델을 주기적으로 학습(training)한다. 학습 과정을 거치고 더 나은 품질로 모델 이 업데이트되면 이후 수행될 inference 과정을 위해 inference server로 모델이 전송된다. 데이터 저장 요청이 오면, 해당 데이터의 메타데이터(예: 이미지의 tag)를 추출하기 위해 inference 과정이 수행된다. 메타데이터는 사용자에게 검색 및 인공지능 애플리케이션 등을 제공하기 위해 사용된다. 앞선 training 과정을 통해 생성된 모델을 활용하여, inference server에서 inference 과정이 수행된다. 이를 통해 데이터의 메타데이터를 추출한 뒤, 데이터는 storage server에 저장되고 메타데이터는 database에 인덱싱된다. 인공지능 모델은 더 큰 모델을 가지고, 더 많은 training data를 사용하면 더 높은 정확도를 달성할 수 있다. 따라서 사용자에게 더 나은 품질의 서비스를 제공하기 위해, 많은 데이터 센터에서는 더 큰 모델과 더 많은 training data를 사용한다. 하지만 이러한 경향은 training에 수행되는 시간을 길어지게 하고, 더 높은 컴퓨팅 성능을 요구한다. 이는 인공지능 모델을 training하고 배포할 때, 두 가지 문제점을 발생시킨다. 첫 번째는 stale model problem 이다. 인공지능 모델은 생성된 직후에는 예상되는 품질을 제공할 수 있지만, re-training되지 않으면 시간이 지 남에 따라 정확도가 점차 저하된다. 모델을 더 자주 re-training한다면 정확도 하락을 방지할 수 있다. 그러나 모델이 거대해짐에 따라 자주 re-training하는 것은 현실적으로 불가능하다. 두 번째는 stale metadata problem이다. Inference는 일반적으로 storage server에 업로드되는 새로운 데이터 에 대해 online으로 이루어진다. (Online inference) 그러나 모델이 업데이트되어 더 나은 품질의 메타데이터가 생성될 수 있거나, 외부적 요인으로 인해 기존에 저장되어 있던 메타데이터가 정확하지 않게 되는 경우가 발생 할 수 있다. 이 부정확한 메타데이터를 stale metadata라고 정의한다."}
{"patent_id": "10-2022-0170345", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 이루고자 하는 기술적 과제는 연산장치를 포함하는 스토리지 서버 및 전이학습 서버를 포함하여 상기 된 문제점들을 해소하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템 및 방법을 제공하는 것이다. 본 발명의 일 실시예는 각각이 연산장치를 포함하는 복수의 SSD들을 가진 스토리지 서버를 이용하여 오프라인에 서 능동적으로 오래된 부실 메타데이터를 업데이트하는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템 및 방법을 제공하는 것을 목적으로 한다. 본 발명의 일 실시예는 전이학습 서버를 통해 인공지능 모델의 일부를 주기적으로 업데이트하여 인공지능 모델 의 업데이트에 걸리는 긴 시간을 보조하고 더 나은 품질의 인공지능 모델을 제공하는 SSD 오프로딩을 이용한 인 공지능 추론 및 학습 시스템 및 방법을 제공하는 것을 목적으로 한다.본 발명이 이루고자 하는 기술적 과제는 이상에서 언급한 기술적 과제로 제한되지 않으며, 언급되지 않은 또 다 른 기술적 과제들은 아래의 기재로부터 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0170345", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 기술적 과제를 달성하기 위하여, 본 발명의 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학 습 시스템은 각각이 제1 연산장치를 포함하는 복수의 에스에스디들(SSD)을 포함하고, 데이터를 저장하는 스토리 지 서버, 상기 스토리지 서버와 네트워크를 통해 연결되고, 주기적인 학습을 통해 업데이트되는 인공지능 모델 을 생성하는 전이학습 서버, 상기 전이학습 서버로부터 수신한 상기 인공지능 모델을 이용해 상기 데이터에 대 한 메타데이터를 추출하는 추론 서버 및 상기 추출된 메타데이터를 저장하는 데이터베이스를 포함하고, 상기 전 이학습 서버는 제2 연산장치를 포함하고, 상기 학습의 제1 부분은 상기 제1 연산장치를 통해 수행하고, 상기 학 습의 상기 제1 부분을 제외한 나머지 학습부분을 포함하는 제2 부분은 상기 제2 연산장치를 통해 수행한다. 일 실시예에서, 상기 제1 부분은 학습에 필요한 중간 데이터를 생성하고, 상기 제2 부분은 상기 중간 데이터를 사용하여 직접적인 학습을 통해 상기 인공지능 모델의 생성을 위한 결과 데이터를 생성할 수 있다. 일 실시예에서, 상기 전이학습 서버는 상기 학습을 제어하는 전이학습부를 더 포함하고, 상기 전이학습부는 상 기 데이터를 가진 상기 스토리지 서버에 상기 인공지능 모델의 생성 또는 업데이트를 위한 요청을 송신하고, 상 기 요청을 받은 상기 스토리지 서버는 상기 제1 연산장치를 이용하여 상기 중간 데이터를 생성하기 위한 특징 추출 작업을 수행하고, 상기 전이학습부는 상기 중간 데이터를 수신하여 상기 제2 연산장치에 전달하고 상기 제 2 연산장치는 상기 결과 데이터를 생성하기 위한 네트워크의 가중치 업데이트를 수행할 수 있다. 일 실시예에서, 상기 전이학습 서버는 상기 메타데이터의 업데이트를 제어하는 오프라인 추론부를 더 포함하고, 상기 오프라인 추론부는 상기 데이터베이스 내에 부실 메타데이터가 존재하는지 판단하여 상기 부실 메타데이터 가 존재하는 것으로 판단되면 상기 부실 메타데이터를 업데이트하기 위해 상기 스토리지 서버에 오프라인 추론 요청을 송신하고, 상기 요청을 받은 상기 스토리지 서버는 상기 제1 연산장치를 이용하여 상기 오프라인 추론을 수행할 수 있다. 일 실시예에서, 상기 전이학습 서버는 상기 스토리지 서버로부터 상기 오프라인 추론을 통해 추출된 업데이트된 메타데이터를 수신하고, 상기 데이터베이스에서 상기 부실 메타데이터를 식별하여 상기 업데이트된 메타데이터 로 갱신할 수 있다. 일 실시예에서, 상기 전이학습 서버는 생성된 상기 업데이트된 인공지능 모델에서 기존 인공지능 모델과 차이점 만을 추출하여 상기 추론 서버 및 상기 스토리지 서버로 전달할 수 있다. 실시예들 중에서, SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법은 각각이 제1 연산장치를 포함하는 복수 의 에스에스디(SSD)들을 포함하는 스토리지 서버가 사용자로부터 수신한 데이터를 저장하는 단계, 상기 스토리 지 서버와 네트워크를 통해 연결되고 제2 연산장치를 포함하는 전이학습 서버가 주기적인 학습을 통해 인공지능 모델을 업데이트하는 단계, 추론 서버가 상기 인공지능 모델을 이용하여 상기 데이터에 대한 메타데이터를 추출 하여 데이터베이스에 저장하는 단계를 포함하고, 상기 전이학습 서버가 인공지능 모델을 업데이트하는 단계는, 상기 스토리지 서버의 상기 제1 연산장치가 학습 데이터의 특징 추출을 통해 중간 데이터를 생성하는 제1 단계 및 상기 전이학습 서버의 상기 제2 연산장치가 상기 중간 데이터를 사용하여 네트워크 가중치 업데이트를 포함 하는 학습을 통해 업데이트된 인공지능 모델을 생성하는 제2 단계를 포함한다. 일 실시예에서, SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법은 상기 전이학습 서버가 상기 업데이트된 인공지능 모델을 상기 추론 서버 및 상기 스토리지 서버로 배포하는 단계를 더 포함하고, 상기 업데이트된 인공 지능 모델을 배포하는 단계는 상기 업데이트된 인공지능 모델과 기존 인공지능 모델의 차이점만을 추출하여 배 포할 수 있다. 일 실시예에서, SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법은 상기 전이학습 서버가 상기 데이터베이스 내에 부실 메타데이터가 존재하는지 판단하는 단계, 상기 부실 데이터가 존재한다고 판단되는 경우 상기 전이학 습 서버가 상기 부실 메타데이터의 업데이트를 위해 상기 부실 메타데이터가 가리키는 타겟 데이터가 저장된 스 토리지 서버에 오프라인 추론 요청을 송신하는 단계 및 상기 요청에 따라 상기 스토리지 서버가 상기 제1 연산장치를 이용하여 상기 오프라인 추론을 수행하는 단계를 더 포함할 수 있다. 일 실시예에서, 상기 스토리지 서버가 상기 오프라인 추론을 수행하는 단계는 상기 오프라인 추론을 통해 추출 된 업데이트된 메타데이터만을 상기 전이학습 서버로 다시 전달하는 단계를 포함할 수 있다. 일 실시예에서, SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법은 상기 전이학습 서버가 상기 타겟 데이터 를 식별하고 상기 데이터베이스에서 상기 부실 메타데이터를 상기 업데이트된 메타데이터로 갱신하는 단계를 더 포함할 수 있다. 일 실시예에서, 상기 전이학습 서버가 인공지능 모델을 업데이트하는 단계는, 상기 복수의 에스에스디들은 제1 에스에스디 및 제2 에스에스디를 포함하고, 상기 제1 에스에스디의 제1 연산장치가 상기 제1 단계를 마치면 상 기 제2 에스에스디의 제1 연산장치가 상기 제1 단계를 시작하고, 상기 제2 에스에스디의 상기 제1 단계는 상기 제1 에스에스디에 대한 상기 제2 연산장치의 상기 제2 단계와 동시에 수행될 수 있다."}
{"patent_id": "10-2022-0170345", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템 및 방법은 각각이 연산장치를 포함하는 복수의 SSD들을 가진 스토리지 서버를 이용하여 오프라인에서 능동적으로 오래된 부실 메타데이터를 업데이트하 고 추론의 정확성을 향상시킬 수 있다. 본 발명의 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템 및 방법은 전이학습 서버를 통해 인공지능 모델의 일부를 주기적으로 업데이트하여 인공지능 모델의 업데이트에 걸리는 긴 시간을 보조하고 더 나은 품질의 인공지능 모델을 제공할 수 있다."}
{"patent_id": "10-2022-0170345", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 상기한 효과로 한정되는 것은 아니며, 본 발명의 설명 또는 청구범위에 기재된 발명의 구성으 로부터 추론 가능한 모든 효과를 포함하는 것으로 이해되어야 한다."}
{"patent_id": "10-2022-0170345", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참조하여 본 발명을 설명하기로 한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며, 따라서 여기에서 설명하는 실시예로 한정되는 것은 아니다. 그리고 도면에서 본 발명을 명확 하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유 사한 도면 부호를 붙였다. 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결(접속, 접촉, 결합)\"되어 있다고 할 때, 이는 \"직접적으로 연 결\"되어 있는 경우뿐 아니라, 그 중간에 다른 부재를 사이에 두고 \"간접적으로 연결\"되어 있는 경우도 포함한다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 구비할 수 있다는 것을 의미한다. 본 명세서에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도 가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들 을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요 소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 명세서에서, \"모듈\"은 하드웨어, 소프트웨어 또는 펌웨어로 구성된 유닛을 포함하며, 예컨대 로직, 논리 블 록, 부품, 또는 회로 등의 용어와 상호 호환적으로 사용될 수 있다. 모듈은 일체로 구성된 부품 또는 하나 또는 그 이상의 기능을 수행하는 최소 단위 또는 그 일부가 될 수 있다. 예컨대 모듈은ASIC(application-specific integrated circuit)으로 구성될 수 있다. 이하, 첨부된 도면을 참고하여 본 발명의 실시예를 상세히 설명하기로 한다. 도 1은 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 개략도이다. 도 1을 참조하면, SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 네트워크를 통해 데이터 센터 및 인공지능 애플리케이션 서버와 연결될 수 있다. 인공지능 애플리케이션 서버는 인공지능을 이용한 다양한 종류의 애플리케이션을 포함할 수 있다. 인공지 능 애플리케이션은 음성 인식, 비전, 의사 결정 논리 및 인간적인 이유를 모방하는 여러 지능형 기능을 제공할 수 있다. 데이터 센터는 인공지능 애플리케이션 서버에서 활용할 수 있는 데이터를 저장한다. 데이터 센터 는 복수의 스토리지 서버들을 포함할 수 있다. 일 실시예에서, 데이터 센터 및 스토리지 서버는 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템에 포함될 수 있다. SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 인공지능 애플리케이션의 품질을 높이기 위해 인 공지능 애플리케이션에 사용되는 데이터에 대한 추론 및 학습을 수행할 수 있다. SSD 오프로딩을 이용한 인공지 능 추론 및 학습 시스템은 데이터 센터에 데이터 저장 요청이 오면, 해당 데이터의 메타데이터(예: 이미지의 tag)를 추출하기 위해 추론(inference) 과정을 수행할 수 있다. 메타데이터는 사용자에게 검색 및 인 공지능 애플리케이션 등을 제공하기 위해 사용될 수 있다. SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스 템은 학습 및 전이학습의 과정을 통해 생성된 인공지능 모델을 활용하여, 추론 서버에서 추론 과정을 수행 할 수 있다. SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 이를 통해 데이터의 메타데이터를 추 출한 뒤, 데이터는 스토리지 서버에 저장되고 메타데이터는 데이터베이스에 인덱싱 될 수 있다. SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 인-스토리지 컴퓨팅을 활용하여 학습 및 추론 성 능을 가속화할 수 있다. 인-스토리지 컴퓨팅은 데이터 센터 또는 스토리지 서버에 경량 에지(edge) GPU 장치가 있는 SSD를 배포하고 집단 지능을 사용하여 데이터 근처에서 추론 및 학습을 수행하는 것일 수 있다. 일 실시예에서, SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 인공지능 모델의 학습에 있어서 전이학습(transfer learning)의 기술을 활용하여 학습에 걸리는 긴 시간을 보조할 수 있다. 따라서, 인공지능 모델의 정확도 하락에 빠르게 대응하고 높은 품질의 인공지능 모델을 제공할 수 있다. SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 스토리지 서버의 연산장치를 이용하는 오프로딩(off-loading) 기술을 활 용하여 데이터베이스의 메타데이터를 업데이트할 수 있다. 자세한 설명은 후술한다. 도 2는 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 블록도이다. 도 2를 참조하면, SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 스토리지 서버, 전이학습 서버, 추론 서버 및 데이터베이스를 포함할 수 있다. 도 2에는 도시되지 않았으나, SSD 오프로 딩을 이용한 인공지능 추론 및 학습 시스템은 전이학습 서버 외에 메인 학습 서버를 더 포함할 수 있 다. 메인 학습 서버는 스토리지 서버로부터 학습 데이터셋을 받아 인공지능 모델을 주기적으로 업데이트하 여 추론 서버에 전송할 수 있다. 스토리지 서버는 인공지능 애플리케이션의 활용을 위한 수많은 데이터들을 저장할 수 있다. 스토리지 서버 는 인공지능 모델의 학습을 위한 학습 데이터셋을 학습 서버에 제공할 수 있다. 스토리지 서버는 도 1의 데이터 센터를 대체할 수 있다. 즉, 스토리지 서버는 전이학습 서버 , 추론 서버 및 데이터베이스와 네트워크를 통해 연결될 수 있다. 스토리지 서버는 사용자 로부터 또는 인공지능 애플리케이션으로부터 수신한 데이터(예를 들어, 이미지 파일)를 저장할 수 있다. 스토리 지 서버는 복수의 에스에스디(이하, SSD, Solid-State Drive)들을 포함할 수 있다. 일 실시예에서, 스토리지 서버의 복수의 SSD들 각각은 제1 연산장치를 포함할 수 있다. 제1 연산장치는 GPU일 수 있다. GPU는 경량 엣지 GPU 가속기를 포함할 수 있다. 전이학습 서버는 메인 학습 서버를 보조할 수 있다. 전이학습 서버는 인공지능 모델의 학습 단계 또 는 학습 과정을 효율적으로 분할하고, 복수의 SSD의 제1 연산장치들을 이용하는 오프로딩을 통해 학습하기 때문 에 높은 학습 처리량을 달성할 수 있다. 전이학습 서버는 복수의 SSD들을 포함하는 스토리지 서버를 제어하는 역할을 수행할 수 있다. 예를 들어, 전이학습 서버는 필요한 경우, 오프라인 추론 및 전이학습의 일부(제1 부분)를 수행하도록 스토리지 서버에 명령할 수 있다. 스토리지 서버의 제1 연산장치를 통해 수행되는 전이학습의 일부는 학습이 이루어지지 않는 부분일 수 있다. 전이학습 서버는 스토리지 서버를 제어하는 동안 전이학습 서버에 포함된 제2 연산장치를 이용 하여 전이학습의 나머지 일부(제2 부분)를 수행할 수 있다. 제2 연산장치는 GPU일 수 있고, GPU는 경량 엣지 GPU 가속기를 포함할 수 있다. 전이학습 서버의 제2 연산장치를 통해 수행되는 전이학습의 나머지 일부는 학습이 실제 이루어지는 부분일 수 있다. 즉, 전이학습 서버는 주기적인 학습을 통해 인공지능 모델을 업데이트할 수 있다. 전이학습 서버에서 업데이트를 통해 생성되는 인공지능 모델은 추론 서버에 전달될 수 있다. 추론 서버는 수신한 인공지능 모델을 이용하여 데이터에 대한 메타데이터를 추출하는 온라인 추론을 수행 할 수 있다. 데이터베이스는 추출된 메타데이터를 저장할 수 있다. 전이학습 서버는 추론 서버에 의해 추출되는 메타데이터들 중 오래되어 정확하지 않은 부실 메타데이 터가 존재하는지 여부를 검출할 수 있다. 전이학습 서버는 부실 메타데이터가 존재한다고 판단된 경우, 스 토리지 서버의 복수의 SSD들에게 부실 메타데이터가 가리키는 타겟 데이터에 대한 오프라인 추론을 수행하 도록 요청할 수 있다. 스토리지 서버는 오프라인 추론을 통해 추출된 메타데이터를 전이학습 서버로 반환할 수 있다. 여기 에서 추출된 메타데이터는 기존 부실 메타데이터와 다른 업데이트된 메타데이터일 수 있다. 전이학습 서버(12 0)는 업데이트된 메타데이터를 가지고 데이터베이스의 오래된 부실 메타데이터를 업데이트할 수 있다. 도 3은 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 예시도이다. 도 3은 추론 서버 및 데이터베이스 외에 스토리지 서버(110, SOFA-SSD) 및 전이학습 서버(120, SOFA-TL)의 두 가지 주요 구성 요소를 더 포함하는 본 발명의 일 실시예에 따른 SSD 오프로딩을 이용한 인공지 능 추론 및 학습 시스템(100, SOFA)을 보여준다. 본 명세서에서 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템(100, 이하 인공지능 추론 및 학습 시스템)은 SOFA로 지칭될 수 있고, 스토리지 서버의 SSD들 은 SOFA-SSD로 지칭될 수 있고, 전이학습 서버는 SOFA-TL로 지칭될 수 있다. 도 3을 참조하면, 스토리지 서버는 복수의 SSD들을 포함하고 복수의 SSD들 각각은 제1 연산장치(11 4)를 포함할 수 있다. 전이학습 서버는 오프라인 추론부, 전이학습부, 제2 연산장치, 모델 분배부, 인공지 능 모델을 포함할 수 있다. 전이학습부는 전이학습을 제어할 수 있다. 전이학습 서버는 전이학습부에 의해서 전이학습의 필 요성을 감지할 수 있다. 전이학습부는 전이학습이 필요하다고 판단되는 경우 스토리지 서버의 복수의 에스에스디들로 인공지능 모델의 생성 또는 업데이트를 위한 요청을 보낼 수 있다. 스토리지 서버 는 요청을 받아 학습을 위한 데이터셋을 보유하고 있는 SSD들에게 전달할 수 있다. SSD들은 제1연산장치를 이용하여 데이터셋의 특징 추출 등의 학습 이외의 작업을 수행하여 중간 데이터를 생성하여 전 이학습부에 전달할 수 있다. 전이학습부는 중간 데이터를 받아 제2 연산장치를 이용하여 가중치 업데 이트 등을 수행하여 결과 데이터를 생성하고 결과 데이터를 통해 인공지능 모델을 업데이트하고 최신 인공지능 모델을 생성할 수 있다. 모델 분배부은 최신 인공지능 모델을 스토리지 서버의 복수의 SSD들에 분배할 수 있다. 모 델 분배부는 업데이트된 인공지능 모델에서 기존 인공지능 모델과 차이점만을 추출하여 추론 서버 및 스토리지 서버로 분배할 수 있다. 오프라인 추론부는 메타데이터의 업데이트를 제어할 수 있다. 오프라인 추론부는 데이터베이스 내에서 부실 메타데이터가 존재하는지 판단할 수 있다. 오프라인 추론부는 부실 메타데이터가 존재하는 것으로 판단되면 부실 메타데이터를 업데이트하기 위해 스토리지 서버에 오프라인 추론 요청을 송신할 수 있 다. 요청을 받은 스토리지 서버는 제1 연산장치를 이용하여 오프라인 추론을 수행할 수 있다. 오프라인 추론부는 업데이트된 메타데이터를 수신하고 데이터베이스 내에 부실 메타데이터를 식별하 여 업데이트된 메타데이터를 통해 이를 업데이트할 수 있다. 보다 구체적으로, 도 3에서, 스토리지 서버의 SSD들은 기존 SSD와 유사하지만 경량 엣지 GPU 가속기 를 포함하는 제1 연산장치를 탑재할 수 있다. 전이학습 서버는 제1 연산장치를 사용하는 스토리 지 서버를 제어하는 역할을 하며, 필요한 경우 오프라인 추론 및 전이 학습을 트리거할 수 있다. 스토리지 서버를 제어하는 동안 전이학습 서버는 호스트 측 GPU, 즉, 제2 연산장치를 사용하여 전이 학습 의 일부도 수행할 수 있다. 즉, 인공지능 추론 및 학습 시스템은 추론 프로세스를 SSD로 오프로드하도록 설계된다. 전이학습 서 버는 데이터베이스에서 부실 메타데이터가 있을 수 있는 후보 데이터(후보 이미지)의 정보를 정기적 으로 검색한다. 전이학습 서버는 대상 데이터가 있는 스토리지 서버에 추론 요청을 보낸다. 요청은 SSD로 전달된다. 최신 DNN(Deep Learning Network) 모델을 사용하여 개별 SSD는 로컬에서 오프라인 추론을 수행하고 추출된 메타데이터(예: 레이블)를 전이학습 서버로 반환한다. 마지막으로, 전이학습 서버 는 오래된 레이블이 있는 이미지를 식별하고 데이터베이스에서 오래된 메타데이터를 업데이트할 수 있다. 추론된 레이블만 전이학습 서버에 전달되므로 인공지능 추론 및 학습 시스템은 오프라인 추론 을 위해 네트워크 트래픽을 제거할 수 있다. 오프라인 추론과 달리 SSD 내에서 전이 학습 프로세스를 완전히 오프로드하는 것은 높은 계산 및 동기화 비용 때문에 실행 불가능하다. 인공지능 추론 및 학습 시스템은 교육 프로세스를 두 부분(제1 부분(FE) 및 제2 부분(CL))으로 나눈다. 각 부분은 SSD 또는 전이학습 서버에서 별도로 실행된다. 전이학습 서버 는 학습 이미지를 보유하고 있는 스토리지 서버에 요청을 보낸다. 이후, SSD는 이미지를 읽고 경량 축소 작업을 수행하여 작은 중간 데이터를 생성한다(제1 부분(FE)). SSD에서 제공된 중간 데이터를 사용하여 전이학습 서버는 네트워크의 가중치를 업데이트하여 최신 DNN 모델을 생성할 수 있다(제2 부분 (CL)). 추론 서버는 온라인 추론을 통해, 네트워크에서 도착한 데이터(DAT)에서 레이블(메타데이터)을 직접 추출 한다. 따라서, 전이 학습의 전체 프로세스가 완료되면 전이학습 서버는 추론 서버의 인공지능 모델을 업데이트한다. 즉, 인공지능 추론 및 학습 시스템에서 오프라인 추론과 전이 학습의 일부는 SSD에 의 해 수행된다. SSD가 최신 모델을 갖도록 하려면 전이학습 서버가 SSD를 통해 최신 모델을 재배 포해야 하므로 추가 네트워크 트래픽이 발생할 수 있다. 인공지능 추론 및 학습 시스템의 전체 디자인은 간단하고 추론 및 학습을 위한 거의 모든 데이터 이동을 제거하는 데 합리적이다. 인공지능 추론 및 학습 시스템은 기존 딥러닝(DL) 시스템을 대체하기 위해 Edge GPU를 사용하여 빠른 추론 및 학습 성능을 제공한다. 즉, 최신 모델을 재배포하기 위한 추가 네트워크 비용을 최소화한다. 일 실시예에서, 높은 추론 처리량을 실현하기 위해 스토리지 서버의 SOFA-SSD는 CPU 집약적 작업(예: 이미지 디코딩)을 강력한 CPU가 있는 서버로 오프로드 한다. 또한, SOFA-SSD는 NAND 칩, CPU 코어 및 GPU 의 특성을 고려하여 추론 작업을 최적으로 할당한다. 이러한 방식으로 SOFA-SSD는 충분히 높은 이미지 추 론 처리량을 달성할 수 있다. 전이 학습에는 추론보다 더 높은 컴퓨팅 성능이 필요하다. 또한 많은 SOFA-SSD에 학습 작업을 분산하면 동 기화 비용으로 인해 확장 가능한 처리량이 제공되지 않을 수 있다. SOFA는 모델을 분할하고 가중치 고정 레이어를 SOFA-SSD에 할당하고 학습 가능한 레이어를 SOFA-TL에 할당한다. 가중치 고정 레이어는 가 중치 동기화가 필요하지 않으며 에지 GPU에 의해 빠르게 가속될 수 있다. SOFA는 여러 SSD에서 여러 작업자를 병렬로 실행하여 데이터 병렬 처리를 극대화하여 높은 집계 처리량을 달성할 수 있다. 동기화가 필요 한 훈련 가능한 계층은 SOFA-TL에서 로컬로 실행된다. 따라서 SOFA는 동기화를 위한 네트워크 트래픽 없이 최신 인공지능 모델을 생성할 수 있다. SOFA는 또한 SOFA-SSD와 SOFA-TL을 동시에 실행하 여 모델 병렬 처리를 활용하여 학습 처리량을 더욱 향상시킨다. 오프라인 추론의 경우 최신 인공지능 모델은 SOFA-SSD로 다시 전달되어야 한다. SOFA-SSD에 최신 모 델을 그대로 배포하면 필연적으로 일반적인 딥러닝 시스템에서는 발생하지 않는 많은 네트워크 트래픽이 발생한 다. SOFA는 인접 모델 간의 차이점만 SOFA-SSD로 보내는 델타 압축 개념을 채택한다. 또한, SOFA는 DNN 모델 버전의 효율적인 관리를 통해 필요할 때 최신 델타를 SOFA-SSD로 전송합니다. 이러 한 방식으로 SOFA는 네트워크 트래픽을 크게 줄일 수 있다. SOFA는 에너지 효율적이다. 동일한 성능 을 위해 SOFA는 고급 GPU를 사용하여 서버 측에서 모든 작업을 수행하는 대응 설계에 비해 훨씬 적은 전력 을 소비한다. 도 4는 일 실시예에 따른 스토리지 서버를 보여주는 도면이다. 도 4는 스토리지 서버에 포함된 SOFA-SSD의 하드웨어 아키텍처를 보여준다. 예를 들어, 각각의 SOFA- SSD는 NAND 칩, DRAM 모듈, ARM CPU(8코어 ARMv8.2) 및 에지 GPU(512코어 Volta GPU)로 구성될 수 있다. 일반적으로 ARM 코어는 스토리지 펌웨어를 실행하지만 이미지를 추론하는 데에도 사용된다. Edge GPU는 이미지 추론 처리 전용이다. SOFA는 NAND 칩에 DNN 모델을 유지한다. 추론을 실행하려면 SOFA-SSD에 해당 모 델을 로드하고 실행하기에 충분한 DRAM이 있어야 한다. 필요한 메모리 크기는 모델에 따라 다를 수 있다. 예를 들어, ResNet50에서는 1.9GB DRAM이면 충분하다. SSD가 대용량 DRAM(예: 2GB-16GB DRAM[13-15, 31])을 사용한 다는 점을 고려하면 SSD 내에서 추론을 실행하는 것이 저렴하다. 일 실시예에 따른 오프라인 추론은 이미지 로딩, 디코딩, 크기 조정, 정규화, 특징 추출(FE), 분류(CL)의 6단계 로 구성될 수 있다. 이미지 로딩 단계는 이미지를 NAND에서 메모리로 로드한다. 디코딩 단계에서 이미지(예: JPEG)를 압축 해제된 이진 형식으로 변환한 후 크기 조정 단계에서는 DNN 모델에 적합한 디코딩된 이미지의 해 상도를 변경한다. 인공지능 모델이 정규화된 이미지를 사용하여 훈련된 경우 SOFA는 정규화 단계를 수행할 수 있다. 마지막으로 정규화된 데이터는 특징 추출(FE) 및 분류 학습(CL)을 통해 레이블을 유추하기 위해 GPU에 공급된다. 디코딩, 크기 조정 및 정규화는 전처리의 일부이며 CPU에 의해 수행된다. 많은 계산을 포함하는 FE 및 CL 단계는 GPU에 의해 수행된다. 도 5는 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 동작을 보여주는 예시도이다. 도 5는 SOFA가 인공지능 모델을 분할하고 많은 SSD에서 작업을 실행하는 방법을 보여준다. SOFA(10 0)는 모델의 레이어가 가중치 고정 및 학습 가능한 레이어의 두 가지로 분할되는 전이 학습의 고유한 속성을 활 용한다. SOFA는 weight-freeze 레이어(FE)의 복제본을 복수의 SOFA-SSD들의 복수의 제1 연산장치들 에 할당하고 그 위에 여러 작업자를 배포하여 로컬 배치(batch)에서 기능을 추출한다. 반대로 훈련 가능한 계층(CL)은 SOFA-TL에 할당되고 단일의 제2 연산장치가 처리한다. weight-freeze 레이어(FE)를 처리 하는 것은 추론이 하는 것과 동일하고 가중치 업데이트 및 동기화가 필요하지 않다. 또한 각 SOFA-SSD는 이미 인공지능 모델을 로컬에 유지하고 입력 배치를 처리하기에 충분한 컴퓨팅 기능을 갖추고 있다. 따라서 여 러 SOFA-SSD가 종속성 없이 병렬로 수행되어 데이터 병렬 처리를 통해 높은 집계 처리량을 달성할 수 있다. SOFA-TL은 SOFA-SSD에서 작은 크기의 중간 결과만 수집하고 강력한 GPU를 사용하여 학습 가능한 레이어를 학습 또는 훈련한다. 가중치 업데이트가 SOFA-TL 측에서 로컬로 수행되므로 동기화를 위한 통신 비용이 최소화될 수 있다. 도 6은 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 학습 과정을 보여주는 예시도 이다. 일 실시예에서, SOFA-TL은 SOFA-SSD가 가지고 있는 모든 로컬 배치에 대한 중간 결과를 수신한 후 훈 련 네트워크를 시작할 수 있다. 이는 각 Epoch에 대해 DNN 알고리즘이 훈련 데이터 세트의 모든 배치(batch)를 모델에 공급해야 하기 때문이다. 그 결과 도 6(a)와 같이 SOFA-SSD와 SOFA-TL이 직렬로 실행될 수 있다. 도 6(a)의 실시예는 전이 학습 시간이 비교적 짧은 경우에 허용된다. 그러나, 모델과 입력 데이터의 크기가 커질수록 전이 학습 시간은 계속 늘어난다. 예를 들어, 대형 모델(예: ResNeXt101 32x48d)의 경우 전이 학습이 42시간 이상 걸릴 수 있다. 다른 일 실시예에서, SOFA는 인공지능 모델의 분할 파티션을 동시에 실행하는 모델 병렬 처리에서 영감을 받아 파이프라인 전달 학습 전략을 제안한다. 파이프라인 전달 학습은 SOFA-SSD(112, 가중치 고정 레이어(FE)) 와 SOFA-TL(120, 훈련 가능한 레이어(CL))을 병렬로 실행한다. 도 6(b)와 같이 일 실시예에 따른 SOFA는 전이 학습을 여러 단계로 나눌 수 있다. SOFA-SSD가 모든 로컬 배치(LB)를 처리하기 전에 SOFA-TL에 서 네트워크 학습을 시작한다. 이것은 훈련 시간을 효과적으로 줄일 수 있다. 즉, 복수의 SSD들 전부가 제 1 연산장치를 통해 가중치 고정 레이어(FE)의 학습의 제1 단계를 모두 마치기 전에 전이학습 서버는 제2 연산장치를 통해 학습 단계인 제2 단계를 시작할 수 있다. 예를 들어, 복수의 SSD들 중 제1 SSD의 제1 로컬 배치(LB1)가 제1 연산장치를 통해 제1 단계를 처리하고 나면, 제1 로컬 배치(LB1)의 제2 연산장치를 통한 제2 단계를 시작하기 전에, 제2 SSD의 로컬 배치(LB2)가 제1 연산장치를 통해 제1 단계를 시작할 수 있다. 즉, 제2 로컬 배치(LB2)의 제1 단계와 제1 로컬 배치(LB1)의 제2 단계는 동시에 병렬적으로 수행될 수 있다. SOFA는 훈련 가능한 소수 계층의 가중치만 업데이트한다. SOFA는 학습률을 조정할 수 있다. 학습률은 각 반복에서 모델의 가중치가 얼마나 변경되는지를 결정하는 하이퍼 매개변수이다. 훈련 후반기에 학습률을 낮 추어 훈련 중 과거의 지식이 잊혀지는 것을 방지한다. 예를 들어, 학습을 3단계(3 stage)로 나눌 때 SOFA 는 모델 정확도를 희생하지 않고 훈련 시간을 25% 향상시킬 수 있다. 도 7은 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 델타 알고리즘을 보여주는 도 면이다. SOFA-TL에서 전이 학습이 완료되면 최신 모델을 추론 서버로 다시 보내어 새 인공지능 모델을 통해 들어오는 이미지에 레이블을 지정해야 한다. SOFA-SSD는 추론과 학습을 담당하므로 SOFA-TL는 SOFA- SSD에 새 인공지능 모델을 배포한다. SOFA-TL는 새 인공지능 모델이 생성될 때마다 최신 인공지능 모 델을 SOFA-SSD에 보낼 수 있다. 그러나 네트워크를 통해 수백 개의 SOFA-SSD에 거대한 크기의 인공지 능 모델(예: 115MB)을 전달하면 상당한 오버헤드가 발생할 수 있다. 이러한 오버헤드를 완화하기 위해 SOFA는 추론 또는 학습이 실제로 일어나는 소수의 SOFA-SSD에만 선택적으로 최신 인공지능 모델을 보 낸다. SOFA는 또한 인접 모델의 유사성을 활용하여 네트워크 트래픽을 줄일 수 있다. 즉, 모델 간에 압축 된 델타만 SOFA-SSD로 보내고 모델을 로컬에서 재구성함으로써 DNN 모델을 전송할 데이터의 양을 줄일 수 있다. 도 7은 SOFA가 델타 압축을 통해 DNN 모델을 배포하는 방법을 보여준다. 델타만 전송하는 것은 SOFA에서 특히 효과적이다. SOFA는 학습 가능한 계층의 가중치를 업데이트하고 다른 계층은 변경되지 않은 상태로 유지하는 전이 학습을 수행한다. 또한 인공지능 모델의 구성(예: 레이어 수)을 변경하지 않는다. SOFA는 전이 학습을 통해 마이너 업데이트된 DNN 모델에 대해 최대 427.4배의 압 축률을 달성할 수 있다. 인공지능 모델이 전체 교육을 통해 주요 업데이트되면 많은 가중치가 변경될 수 있다. 전체 압축 비율을 낮추지만 SOFA는 여전히 주요 업데이트 모델에 대해 충분히 높은 압축 효율(4.32배)을 제공할 수 있다. SOFA-TL은 추론 또는 학습이 발생하는 선택된 SSD에 최신 DNN 모델을 전송하기 때문에, 개별 SOFA- SSD에는 다른 버전의 모델이 있을 수 있다. SOFA-TL은 SOFA-SSD가 로컬 모델을 사용하여 최신 모델을 다시 빌드할 수 있도록 올바른 델타를 보낸다. 도 7은 SOFA가 서로 다른 버전의 모델로 SOFA- SSD를 관리하는 방법을 보여줍니다. SOFA-TL은 이전 DNN 모델을 로컬 저장소에 보관한다. 저장 효율 성을 위해 SOFA-TL은 이전 모델의 압축된 델타만 유지한다. 일 실시예에서, 동일한 스토리지 서버에 속한 SOFA-SSD의 그룹은 동일한 DNN 모델을 가질 수 있다. 델타를 보내기 전에 SOFA-TL은 먼저 SOFA-SSD에 있는 모델 버전을 스토리지 서버에서 검색할 수 있다. 그런 다음, 해당 버전부터 SOFA- TL이 누적된 델타를 집계하고 결과를 SOFA-SSD로 보낼 수 있다. DNN 모델의 히스토리를 유지하는 것은 엄청난 저장 공간을 차지할 수 있다. 또한 시간이 지남에 따라 이전 모델 의 델타가 누적됨에 따라 전체 압축률이 낮아지는 경향이 있으며 압축/감압 시간이 증가할 가능성이 있다. 이를 방지하기 위해 SOFA-TL은 압축되지 않은 체크포인트 모델을 정기적으로 기록하고 로컬 스토리지에서 DNN 모델을 제거한다. 여전히 체크포인트 모델보다 오래된 모델을 유지하는 SOFA-SSD의 경우 SOFA-TL은 해당 모델을 업데이트한다. 그렇지 않으면 델타를 사용하여 최신 모델을 다시 작성할 수 없다. 예를 들어, ResNet50을 사용한 시뮬레이션에 따르면 DNN 모델이 차지하는 저장 공간은 3개월 동안 약 225MB일 수 있다. 모델 압축 및 압축 해제에 소요되는 경과 시간은 6초 미만일 수 있다. 도 8은 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법의 전이학습 과정을 나타내는 흐름 도이다. SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법은 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템에 의해서 수행될 수 있다. 도 8에서, SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 인공지능 모델의 정확도 하락이 감지되 면 전이학습 서버를 통해 스토리지 서버에 인공지능 모델의 업데이트를 요청할 수 있다(단계 S810). SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 스토리지 서버를 통해 학습 데이터의 특징 추출 등을 수행하여 학습을 위한 중간 데이터를 생성하여 전이학습 서버에 전달할 수 있다(단계 S820). SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 중간 데이터를 받은 전이학습 서버를 통해 직접적 인 학습을 수행하여 인공지능 모델을 업데이트하여 최신 인공지능 모델을 생성하고 추론 서버에 제공할 수 있다 (단계 S830). 즉, SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 수행되는 인공지능 모델의 업데이트를 위한 학습을 두 단계로 분할하고, 학습을 포함하지 않는 제1 단계는 스토리지 서버의 복수의 GPU들을 사용하고, 학습 을 포함하는 제2 단계는 전이학습 서버의 GPU를 사용할 수 있다. 도 9는 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법의 오프라인 추론 과정을 나타내는 흐름도이다. 오프라인 추론 과정은 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템에 의해서 수행될 수 있다. 도 9에서, SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 데이터베이스의 오래된 메타데이터를 정기적으로 검색할 수 있다(단계 S910). SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 오래된 메타데이터들 중 데이터를 정확하게 가리 키지 못하는 부실 메타데이터가 존재하는지 여부를 판단할 수 있다(단계 S920) 예를 들어, SSD 오프로딩을 이용 한 인공지능 추론 및 학습 시스템은 부실 메타데이터의 후보들을 검출할 수 있다. SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 부실 메타데이터가 가리키는 데이터가 있는 스토 리지 서버에 오프라인 추론을 요청할 수 있다(단계 S930). 예를 들어, SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 특정 SSD에 저장된 데이터에 대해서 오프라인 추론을 요청할 수 있다. SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 스토리지 서버의 복수의 GPU를 이용하여 오프라인 추론을 수행하여 메타데이터를 생성하고 전이학습 서버로 전달할 수 있다(단계 S940). SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 전이학습 서버를 통해 메타데이터를 가지고 부실 메타데이터를 업데이트할 수 있다(단계 S950). 예를 들어, 전이학습 서버는 부실 메타데이터에 대한 메타데이터 만을 업데이트할 수 있다. SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 추론 서버는 이후, 저장된 데이터(이미지)에 대한 추론 요청에 대하여 부실 메타데이터 대신 업데이트된 메타데이터를 추출할 수 있다(단계 S960). SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템은 스토리지 서버에 포함된 제2 연산장치들을 이용하 여 오프라인으로 추론을 수행하고 메타데이터를 업데이트한다. 즉, SSD 오프로딩을 이용한 인공지능 추론 및 학 습 시스템은 기존의 온라인 방식에서 방대한 데이터 양으로 인한 과도한 연산량에 따라 할 수 없었던 부실 메타데이터에 대한 업데이트를 가능하게 할 수 있다. 도 10 내지 도 12는 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 효과를 보여주는 그래프들이다. 도 10은 본 발명의 일 실시예에 따른 인공지능 모델의 업데이트에 대한 타임라인을 보여주는 그래프이다. 인공지능 추론 및 학습 시스템(100, SOFA)은 제1 연산장치를 탑재한 SOFA-SSD를 통해 네트워크 트래 픽을 크게 발생시키지 않고도 오프라인 추론(offline inference)를 수행할 수 있다. 이를 통해, 인공지능 추론 및 학습 시스템은 사용자에게 더 나은 인공지능 서비스를 제공할 수 있다. 또한, 인공지능 추론 및 학습 시스템은 SOFA-SSD들의 병렬처리와 경량 에지 GPU를 통해 추론 서버와 동일한 성능을 제공 하면서도 절반 수준의 에너지만 사용하여, 적은 비용으로 부실 메타데이터 문제(stale metadata problem)를 완화할 수 있다. 도 10에서 확인 할 수 있듯이, 종래의 시스템은 오랜 시간이 소요되는 메인 모델 업데이트만을 수행하여, 부실 메타데이터 문제를 발생시킨다. 이와 다르게, 인공지능 추론 및 학습 시스템은 전이학습 서버라는 구 성요소를 추가하여, 전이학습(transfer learning)을 통해 정확도 하락을 방어할 수 있다. 종래기술에서는 도 10 의 “Stale model”에서 볼 수 있듯 메인 모델 업데이트가 진행되는 동안 정확도 하락을 방어할 수 있는 요소가 없다. 본 발명의 SOFA-SSD와 SOFA-TL에서 함께 수행하는 전이학습 (도 10의 TL)을 활용한다면 정확도 하락을 방어할 수 있다. 도 10을 참조하면, 종래기술에서는 메인 모델 업데이트가 진행되는 동안 3%의 정확도 하락이 발생했으나, 본 발 명이 제안하는 인공지능 추론 및 학습 시스템에서는 1.05%의 정확도 하락만 관측된다. 도 11은 각각의 인공지능 모델에 따른 추론 처리량 결과를 보여준다. 도 12는 각각의 인공지능 모델에 따른 추 론시 전력 소비량을 보여준다. 도 11 및 도 12는 호스트 측에서 추론을 수행하는 세 가지 설정을 SOFA와 모델별로 비교한다. SRV-I는 전처리된 데이터를 입력으로 사용하며 추론할 모든 데이터는 서버에 보관된다. SRV-P는 SRV-I와 유사하지만 40GbE를 통해 스토리지에서 데이터를 가져온다. SRV-C는 압축된 데이터를 로드하여 네트워크 트래픽을 줄이며 압축 해제 과정 이 필요하다. 도 11에서, SOFA는 하나의 SSD만 사용할 때 최악의 추론 처리량을 보여주지만 성능은 SSD가 많을수록 선형 적으로 향상되는 것을 알 수 있다. 각 점(P1-P3)은 SOFA가 각 서버 환경보다 성능이 우수할 때 SSD의 수를 나타낸다. 보다 구체적으로, 도 11은 SOFA를 서버 측에서 추론을 수행하는 세 가지 서버 기반 추론 시스템인 SRV-I, SRV- P, SRV-C와 비교한다. SRV-I 사전 처리된 이미지 바이너리를 입력으로 사용하며 추론할 모든 바이너리는 서버에 보관된다. 따라서 SRV-I는 네트워크에서 이미지 바이너리를 로드할 필요가 없다. SRV-I는 비교 목적으로 참조할 수 있다. SRV-P는 SRV-I와 유사하지만 40GbE를 통해 스토리지에서 사전 처리된 이미지 바이너리를 로드한다. SRV-C는 압축된 바이너리를 로드하여 네트워크 트래픽을 줄이지만 GPU에 입력하기 전에 입력 바이너리의 압축을 풀어야 한다. 너무 많은 코어가 사용되는 것을 방지하기 위해 2개의 코어를 압축 해제에 사용할 수 있다. 서버 시스템에는 2개의 RTX-3090 GPU가 있을 수 있다. 도 11은 SSD 또는 SOFA-SSD의 수를 1에서 30까지 다양하게 변경하여 시스템의 처리량(초당 이미지 수)을 비교하고 결과를 보여준다. SRV-I는 다른 시스템에 비해 상당히 높은 처리량을 나타낸다. SRV-I는 네트워크의 병목 현상 없이 사용 가능한 GPU 처리량을 최대한 활용할 수 있다. 또한 입력 바이너리를 압축 해제할 필요가 없으므로 CPU 사이클을 많이 절약할 수 있다. SRV-P에서 SSD는 처음에 병목 현상으로 작동한다. 각 SSD는 1.1GB/s를 제공하여 초당 1,915개의 이미지를 제공 한다. 이 수치는 40GbE의 네트워크 대역폭과 초당 최대 21,275개의 이미지를 처리할 수 있는 2개의 RTX-3090 GPU의 컴퓨팅 성능보다 훨씬 낮은 수치이다. 더 많은 SSD가 추가되면 SRV-P의 처리량이 선형적으로 향상된다. 그러나 SSD의 수가 4개를 초과하면 네트워크 대역폭이 포화되어 더 이상 개선되지 않는 것을 알 수 있다. SRV-C는 스토리지에서 압축된 바이너리를 로드하여 네트워크의 대역폭 제한을 극복하려고 시도한다. 압축을 통 해 각 SSD는 초당 6,961개의 더 많은 이미지를 제공할 수 있다. 불행히도 2개의 코어는 초당 2,525개의 이미지 만 압축 해제할 수 있어 CPU에 주요 병목 현상이 발생한다. 이것이 SRV-C가 연결된 SSD 수에 관계없이 동일한 처리량을 나타내는 이유일 수 있다. SOFA는 SOFA-SSD를 하나만 사용할 때 최악의 성능을 보인다. 각 SOFA-SSD는 ResNet50, InceptionV3 및 ResNeXt101에 대해 각각 680, 880 및 188 IPS를 제공한다. 그러나 SOFA-SSD가 많을수록 성능이 선형적 으로 향상된다. 7개의 SOFA-SSD가 배포된 P1 지점에서 SOFA는 SRV-C를 능가한다. SOFA-SSD도 내부적 으로 압축 해제를 수행하지만 작업과 겹치면서 대기 시간을 숨길 수 있다. 11개의 SOFA-SSD가 사용되는 P2 지점에서 SOFA는 SRV-P를 능가한다. SOFA는 서버에 작은 크기의 레이블만 보내므로 네트워크에 병목 현상이 발생하지 않는다. SOFA는 24개의 SOFA-SSD가 있을 때 SRV-I보다 훨씬 더 높은 처리량을 나타 낸다. 이는 24개의 SOFA-SSD의 총 처리량이 2개의 RTX-3090 GPU의 처리량을 초과함을 나타낼 수 있다. 도 11에서, ResNeXt101에서 다양한 처리량 경향을 관찰할 수 있다. ResNeXt101은 계층을 처리하는 데 높은 컴퓨 팅 성능이 필요한 거대한 모델을 가지고 있다. 2개의 RTX-3090 GPU는 많은 SSD와 CPU에서 공급되는 데이터를 처리할 능력이 충분하지 않아 주요 병목 현상이 발생한다. 이것이 SRV-I, SRV-P 및 SRV-C가 SSD 수에 관계없이 유 사한 처리량을 나타내는 이유일 수 있다. 반면에, SOFA의 처리량은 22개 이상의 SOFA-SSD가 있는 경 우 서버 기반 시스템보다 성능이 선형적으로 향상되는 것을 알 수 있다. 도 12는 P1-P3의 각 지점에서 각 시스템의 평균 전력 소비량을 표시한다. 도 12에서, SOFA는 전력 효율적 인 구성요소의 이점을 얻기 때문에 다른 모든 서버 환경보다 높은 전력 효율을 보여준다. 도 12는 SOFA 및 다양한 시스템의 전력 소비를 비교한다. 여기에서, 서버 기반 시스템용 gpustat 및 powerstats 유틸리티를 사용하여 GPU 및 CPU 전력 소비를 측정할 수 있다. SOFA의 경우 Xavier 보드에서 GPU 및 CPU의 전력 소비를 보고하는 jtop 유틸리티를 사용할 수 있다. SOFA는 각각 SRV-P, SRV-C 및 SRV-I와 유사한 성 능을 나타내는 P1, P2 및 P3에서 평균 전력 소비량을 표시한다. SOFA는 네트워크와 CPU에서 각각 병목 현 상이 발생하는 SRV-P 및 SRV-C보다 높은 전력 효율을 보인다. SRV-P, SRV-C 및 SRV-I는 사용 가능한 GPU 및 CPU 리소스를 완전히 활용할 수 없어 유휴 상태로 유지되며 전력 낭비가 발생한다. SOFA는 전력 효율적인 구성 요소의 이점을 활용하기 때문에 SRV-I보다 높은 전력 효율성을 보여줄 수 있다. 본 발명의 실시예에 따른 구성요소, 예컨대 모듈 또는 프로그램 각각은 단수 또는 복수의 서브 구성요소로 구성 될 수 있으며, 이러한 서브 구성요소들 중 일부 서브 구성요소가 생략되거나, 또는 다른 서브 구성요소가 더 포 함될 수 있다. 일부 구성요소들(모듈 또는 프로그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 본 발명의 실시예에 따른 모듈, 프로그 램 또는 다른 구성요소에 의해 수행되는 동작들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어 도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다."}
{"patent_id": "10-2022-0170345", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본 발명의 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자는 본 발명 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 발명의 범위는 후술하는 청구범위에 의하여 나타내어지며, 청구범위의 의미 및 범위 그리고 그 균등 개념으 로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으로 해석되어야 한다."}
{"patent_id": "10-2022-0170345", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 개략도이다. 도 2는 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 블록도이다. 도 3은 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 예시도이다. 도 4는 일 실시예에 따른 스토리지 서버를 보여주는 도면이다. 도 5는 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 동작을 보여주는 예시도이다. 도 6은 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 학습 과정을 보여주는 예시도 이다. 도 7은 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 델타 알고리즘을 보여주는 도 면이다. 도 8은 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법의 전이학습 과정을 나타내는 흐름 도이다. 도 9는 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 방법의 오프라인 추론 과정을 나타내는 흐름도이다. 도 10 내지 도 12는 일 실시예에 따른 SSD 오프로딩을 이용한 인공지능 추론 및 학습 시스템의 효과를 보여주는 그래프들이다."}
