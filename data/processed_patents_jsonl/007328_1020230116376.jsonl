{"patent_id": "10-2023-0116376", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0033799", "출원번호": "10-2023-0116376", "발명의 명칭": "스마트 차량 제어 장치 및 방법", "출원인": "현대자동차주식회사", "발명자": "황상현"}}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자 단말로부터 디지털키 애플리케이션을 통해 입력되는 사용자 입력을 수신하는 사용자 입력 수신부;차량으로부터 차량에 탑승한 사용자에 의해 입력되는 차량 입력을 수신하는 차량 입력 수신부;상기 사용자 입력 및 상기 차량 입력을 기초로 강화학습을 통해 상기 차량의 주행 준비를 위한 차량 제어 정책을 생성하는 강화학습 모델; 및상기 차량 제어 정책에 따라 차량을 제어하는 차량 제어부를 포함하는 스마트 차량 제어 장치."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 사용자 입력 및 상기 차량 입력에 의한 요인 외에 차량 정보 및 환경 정보를 포함하는 정적 요인들을 기초로 지도학습을 통해 상기 차량 제어 정책의 적어도 일부를 업데이트하는 지도학습 모델을 더포함하는 스마트 차량 제어 장치."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 지도학습 모델은, 상기 차량 제어 정책의 스케줄에 대한 제1 가중치와 상기 정적 요인들각각에 대한 제2 가중치를 결정하고,상기 제2 가중치가 상기 제1 가중치보다 큰 경우에만 상기 스케줄을 업데이트 하는 스마트 차량 제어 장치."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 강화학습 모델은 사용자의 스케줄 및 일상패턴에 관한 정보를 포함하는 상기 사용자 입력을 기초로 규칙 기반(rule-based) 모델링을 통해 초기 차량 제어 정책을 생성하는 스마트 차량 제어 장치."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 사용자 입력 수신부는 상기 디지털키 애플리케이션을 통해 사용자에게 제공되는 차량의주행 준비와 관련된 질문에 대한 사용자의 답변을 수신하고,상기 강화학습 모델은 수신된 상기 답변을 기초로 생성되는 제1 상태 및 제1 보상을 기초로 학습하여 상기 초기차량 제어 정책을 업데이트하고,상기 제1 보상은 0보다 작은 값을 가지고, 상기 사용자의 답변이 수신되지 않으면 상기 제1 보상의 값은 0인 스마트 차량 제어 장치."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 사용자 입력 수신부는 상기 디지털키 애플리케이션을 통해 상기 차량의 주행 준비와 관련된 사용자의 실시간 입력을 수신하고,상기 강화학습 모델은 상기 실시간 입력에 따라 생성되는 제2 상태 및 제2 보상을 기초로 학습하여 상기 초기차량 제어 정책을 업데이트하며,상기 제2 보상은 상기 제1 보상보다 작은 값을 가지고, 상기 사용자의 실시간 입력이 수신되지 않으면 상기 제2보상의 값은 0인 스마트 차량 제어 장치."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 차량 입력 수신부는 차량에 탑승한 사용자가 차량을 통해 입력하는 차량 입력을수신하고,공개특허 10-2025-0033799-3-상기 강화학습 모델은 상기 차량 입력에 따라 생성된 제3 상태 및 제3 보상을 기초로 학습하여 상기 초기 차량제어 정책을 업데이트하며,상기 제3 보상은 상기 제2 보상보다 작은 값을 가지고, 상기 차량 입력이 수신되지 않으면 상기 제3 보상의 값은 0인 스마트 차량 제어 장치."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 강화학습 모델은 상기 제1 내지 제3 보상들을 포함하는 보상이 최대값을 가지는 방향으로상기 초기 차량 제어 정책을 반복적으로 업데이트하여 상기 차량 제어 정책을 생성하고,상기 사용자의 답변, 상기 사용자의 실시간 입력 및 상기 차량 입력이 수신되지 않는 경우에 상기 보상은 최대값을 가지는 스마트 차량 제어 장치."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 강화학습 모델은 DQN(Deep Q Network)을 포함하는 스마트 차량 제어 장치."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 강화학습 모델은 상기 DQN에 따른 상기 차량 제어 정책의 실제값(Q-target)과 상기 반복적으로 업데이트 되는 상기 초기 차량 제어 정책들 각각에 대한 예측값(Qpredict)의 차이인 손실값(loss)을 줄이는 방향으로 학습하고,상기 손실값이 최소값을 가지는 경우에 상기 초기 차량 제어 정책을 상기 차량 제어 정책으로 결정하는 스마트차량 제어 장치."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "스마트 차량 제어 장치가 사용자 단말로부터 디지털키 애플리케이션을 통해 입력되는 사용자 입력을 수신하는단계;상기 스마트 차량 제어 장치가 차량으로부터 차량에 탑승한 사용자에 의해 입력되는 차량 입력을 수신하는단계;상기 스마트 차량 제어 장치가 강화학습 모델을 이용하여 상기 사용자 입력 및 상기 차량 입력을 기초로 강화학습을 하고, 상기 차량의 주행 준비를 위한 차량 제어 정책을 생성하는 단계; 및상기 스마트 차량 제어 장치가 상기 차량 제어 정책에 따라 차량을 제어하는 단계를 포함하는 스마트 차량 제어방법."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 사용자 입력 및 상기 차량 입력에 의한 요인 외에 차량 정보 및 환경 정보를 포함하는정적 요인들을 기초로 지도학습을 통해 상기 차량 제어 정책의 적어도 일부를 업데이트하는 단계를 더 포함하는스마트 차량 제어 방법."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 차량 제어 정책을 생성하는 단계는 강화학습 모델이 사용자의 스케줄 및 일상패턴에 관한 정보를 포함하는 상기 사용자 입력을 기초로 규칙 기반(rule-based) 모델링을 통해 초기 차량 제어 정책을생성하는 단계를 포함하는 스마트 차량 제어 방법."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 강화학습 모델은 DQN(Deep Q Network)을 포함하는 스마트 차량 제어 방법."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 상기 사용자 입력을 수신하는 단계는 상기 디지털키 애플리케이션을 통해 사용자에게 제공되는 차량의 주행 준비와 관련된 질문에 대한 사용자의 답변을 수신하는 단계를 포함하고,공개특허 10-2025-0033799-4-상기 차량 제어 정책을 생성하는 단계는 상기 강화학습 모델이 수신된 상기 답변을 기초로 생성되는 제1 상태및 제1 보상을 기초로 학습하여 상기 초기 차량 제어 정책을 업데이트하는 단계를 더 포함하는 스마트 차량 제어 방법."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서, 상기 사용자 입력을 수신하는 단계는 상기 디지털키 애플리케이션을 통해 상기 차량의 주행준비와 관련된 사용자의 실시간 입력을 수신하는 단계를 더 포함하고,상기 차량 제어 정책을 생성하는 단계는 상기 강화학습 모델이 상기 실시간 입력에 따라 생성되는 제2 상태 및제2 보상을 기초로 학습하여 상기 초기 차량 제어 정책을 업데이트하는 단계를 더 포함하는 스마트 차량 제어방법."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서, 상기 사용자 입력을 수신하는 단계는 차량에 탑승한 사용자가 차량을 통해 입력하는 차량 입력을 수신하는 단계를 더 포함하고,상기 차량 제어 정책을 생성하는 단계는 상기 강화학습 모델을 통해 상기 차량 입력에 따라 생성된 제3 상태 및제3 보상을 기초로 학습하여 상기 초기 차량 제어 정책을 업데이트하는 단계를 더 포함하는 스마트 차량 제어방법."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서, 상기 제1 보상은 0보다 작고, 상기 제2 보상은 상기 제1 보상보다 작고, 상기 제3 보상은 상기 제2 보상보다 작으며, 상기 사용자의 답변이 수신되지 않으면 상기 제1 보상은 0이고, 상기 사용자의 실시간 입력이 수신되지 않으면상기 제2 보상은 0이고, 상기 차량 입력이 수신되지 않으면 상기 제3 보상은 0인 스마트 차량 제어 방법."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서, 상기 차량 제어 정책을 생성하는 단계는 상기 제1 내지 제3 보상들을 포함하는 보상이 최대값을 가지는 경우에 상기 초기 차량 제어 정책을 상기 차량 제어 정책으로 최종 결정하고, 상기 사용자의 답변, 상기 사용자의 실시간 입력 및 상기 차량 입력이 수신되지 않는 경우에 상기 보상은 최대값을 가지는 스마트 차량 제어 방법."}
{"patent_id": "10-2023-0116376", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서, 상기 차량 제어 정책을 생성하는 단계는 상기 최종 결정된 차량 제어 정책에 대해 상기 DQN에따라 미리 산출된 실제값(Q-target)과 상기 초기 제어 정책에 대해 상기 DQN에 따라 예측된 예측값(Qpredict)간의 차이인 손실(loss)를 최소화하는 방향으로 상기 강화학습을 진행하는 스마트 차량 제어 방법."}
{"patent_id": "10-2023-0116376", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "일 실시예에 따른 스마트 차량 제어 장치는 사용자 단말로부터 디지털키 애플리케이션을 통해 입력되는 사용자 입력을 수신하는 사용자 입력 수신부, 차량으로부터 차량에 탑승한 사용자에 의해 입력되는 차량 입력을 수신하 는 차량 입력 수신부, 상기 사용자 입력 및 상기 차량 입력을 기초로 강화학습을 통해 상기 차량의 주행 준비를 위한 차량 제어 정책을 생성하는 강화학습 모델 및 상기 차량 제어 정책에 따라 차량을 제어하는 차량 제어부를 포함한다."}
{"patent_id": "10-2023-0116376", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 스마트 차량 제어 장치 및 방법에 관한 것으로, 보다 상세하게는 사용자 패턴 학습을 통해 차량에서 능동적으로 이루어지는 스마트 차량 제어 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0116376", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "디지털키는 전통적인 물리적 키를 대체하는 기술로, 스마트폰이나 다른 디지털 장치를 사용하여 자동차를 잠그 거나 잠금 해제하고, 엔진을 시작하는 등의 기능을 수행할 수 있게 해준다. 디지털키는 스마트폰이나 다른 디지 털 장치를 사용하여 차량에 접근하고, 차량을 시작하거나 제어하는 기능을 제공할 수 있다. 디지털키 기술은 자 체적으로 인공지능(AI)을 내재하지는 않지만, AI와 결합하면 차량 사용에 더욱 많은 편의성과 개인화를 제공할 가능성이 있다. 종래에는 디지털키를 사용해도 차량의 출차를 수행하기 위해서 직접 주차장까지 가서 스마트폰을 태그해야 하는 등의 불편함이 있다. 따라서, 인공지능과 디지털키의 사용자 인증의 기술을 접목시켜, 사용자가 출차를 원하는 경우, 차량이 능동적으로 출차를 준비하는 등의 사용자 편의를 도모하는 기술에 대한 니즈가 있다."}
{"patent_id": "10-2023-0116376", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 실시예는 사용자의 일상 패턴에 대한 강화학습을 통해 차량의 주행 준비를 위한 차량 제어를 능동 적으로 수행할 수 있는 스마트 차량 제어 장치 및 방법을 제공하고자 한다. 본 발명의 일 실시예는 사용자가 사용자의 스케줄을 입/출력할 수 있는 디지털키 애플리케이션을 통해 사용자의 스케줄을 입력하면 이를 기초로 차량의 주행 준비를 위한 차량의 능동 제어 스케줄을 출력하고, 출력된 능동 제 어 스케줄을 인공지능을 통한 강화학습을 이용하여 반복적으로 업데이트하여 최적화하는 스마트 차량 제어 장치 및 방법을 제공하고자 한다."}
{"patent_id": "10-2023-0116376", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "실시예들 중에서, 스마트 차량 제어 장치는 사용자 단말로부터 디지털키 애플리케이션을 통해 입력되는 사용자 입력을 수신하는 사용자 입력 수신부, 차량으로부터 차량에 탑승한 사용자에 의해 입력되는 차량 입력을 수신하 는 차량 입력 수신부, 상기 사용자 입력 및 상기 차량 입력을 기초로 강화학습을 통해 상기 차량의 주행 준비를 위한 차량 제어 정책을 생성하는 강화학습 모델 및 상기 차량 제어 정책에 따라 차량을 제어하는 차량 제어부를 포함한다. 일 실시예에 따른 스마트 차량 제어 장치는 상기 사용자 입력 및 상기 차량 입력에 의한 요인 외에 차량 정보 및 환경 정보를 포함하는 정적 요인들을 기초로 지도학습을 통해 상기 차량 제어 정책의 적어도 일부를 업데이 트하는 지도학습 모델을 더 포함할 수 있다. 상기 지도학습 모델은, 상기 차량 제어 정책의 스케줄에 대한 제1 가중치와 상기 정적 요인들 각각에 대한 제2 가중치를 결정하고, 상기 제2 가중치가 상기 제1 가중치보다 큰 경우에만 상기 스케줄을 업데이트 할 수 있다. 상기 강화학습 모델은 사용자의 스케줄 및 일상패턴에 관한 정보를 포함하는 상기 사용자 입력을 기초로 규칙 기반(rule-based) 모델링을 통해 초기 차량 제어 정책을 생성할 수 있다. 상기 사용자 입력 수신부는 상기 디지털키 애플리케이션을 통해 사용자에게 제공되는 차량의 주행 준비와 관련 된 질문에 대한 사용자의 답변을 수신하고, 상기 강화학습 모델은 수신된 상기 답변을 기초로 생성되는 제1 상 태 및 제1 보상을 기초로 학습하여 상기 초기 차량 제어 정책을 업데이트하고, 상기 제1 보상은 0보다 작은 값 을 가지고, 상기 사용자의 답변이 수신되지 않으면 상기 제1 보상의 값은 0일 수 있다. 상기 사용자 입력 수신부는 상기 디지털키 애플리케이션을 통해 상기 차량의 주행 준비와 관련된 사용자의 실시 간 입력을 수신하고, 상기 강화학습 모델은 상기 실시간 입력에 따라 생성되는 제2 상태 및 제2 보상을 기초로 학습하여 상기 초기 차량 제어 정책을 업데이트하며, 상기 제2 보상은 상기 제1 보상보다 작은 값을 가지고, 상 기 사용자의 실시간 입력이 수신되지 않으면 상기 제2 보상의 값은 0일 수 있다. 상기 차량 입력 수신부는 차량에 탑승한 사용자가 차량을 통해 입력하는 차량 입력을 수신하고, 상기 강화학습 모델은 상기 차량 입력에 따라 생성된 제3 상태 및 제3 보상을 기초로 학습하여 상기 초기 차량 제어 정책을 업 데이트하며, 상기 제3 보상은 상기 제2 보상보다 작은 값을 가지고, 상기 차량 입력이 수신되지 않으면 상기 제 3 보상의 값은 0일 수 있다. 상기 강화학습 모델은 상기 제1 내지 제3 보상들을 포함하는 보상이 최대값을 가지는 방향으로 상기 초기 차량 제어 정책을 반복적으로 업데이트하여 상기 차량 제어 정책을 생성하고, 상기 사용자의 답변, 상기 사용자의 실 시간 입력 및 상기 차량 입력이 수신되지 않는 경우에 상기 보상은 최대값을 가질 수 있다. 상기 강화학습 모델은 DQN(Deep Q Network)을 포함할 수 있다. 상기 강화학습 모델은 상기 DQN에 따른 상기 차량 제어 정책의 실제값(Q-target)과 상기 반복적으로 업데이트 되는 상기 초기 차량 제어 정책들 각각에 대한 예측값(Qpredict)의 차이인 손실값(loss)을 줄이는 방향으로 학 습하고, 상기 손실값이 최소값을 가지는 경우에 상기 초기 차량 제어 정책을 상기 차량 제어 정책으로 결정할 수 있다.실시예들 중에서 스마트 차량 제어 방법은 스마트 차량 제어 장치가 사용자 단말로부터 디지털키 애플리케이션 을 통해 입력되는 사용자 입력을 수신하는 단계, 상기 스마트 차량 제어 장치가 차량으로부터 차량에 탑승한 사 용자에 의해 입력되는 차량 입력을 수신하는 단계, 상기 스마트 차량 제어 장치가 강화학습 모델을 이용하여 상 기 사용자 입력 및 상기 차량 입력을 기초로 강화학습을 하고, 상기 차량의 주행 준비를 위한 차량 제어 정책을 생성하는 단계 및 상기 스마트 차량 제어 장치가 상기 차량 제어 정책에 따라 차량을 제어하는 단계를 포함할 수 있다. 스마트 차량 제어 방법은 상기 사용자 입력 및 상기 차량 입력에 의한 요인 외에 차량 정보 및 환경 정보를 포 함하는 정적 요인들을 기초로 지도학습을 통해 상기 차량 제어 정책의 적어도 일부를 업데이트하는 단계를 더 포함할 수 있다. 상기 차량 제어 정책을 생성하는 단계는 강화학습 모델이 사용자의 스케줄 및 일상패턴에 관한 정보를 포함하는 상기 사용자 입력을 기초로 규칙 기반(rule-based) 모델링을 통해 초기 차량 제어 정책을 생성하는 단계를 포함 할 수 있다. 상기 강화학습 모델은 DQN(Deep Q Network)을 포함할 수 있다. 상기 사용자 입력을 수신하는 단계는 상기 디지털키 애플리케이션을 통해 사용자에게 제공되는 차량의 주행 준 비와 관련된 질문에 대한 사용자의 답변을 수신하는 단계를 포함하고, 상기 차량 제어 정책을 생성하는 단계는 상기 강화학습 모델이 수신된 상기 답변을 기초로 생성되는 제1 상태 및 제1 보상을 기초로 학습하여 상기 초기 차량 제어 정책을 업데이트하는 단계를 더 포함할 수 있다. 상기 사용자 입력을 수신하는 단계는 상기 디지털키 애플리케이션을 통해 상기 차량의 주행 준비와 관련된 사용 자의 실시간 입력을 수신하는 단계를 더 포함하고, 상기 차량 제어 정책을 생성하는 단계는 상기 강화학습 모델 이 상기 실시간 입력에 따라 생성되는 제2 상태 및 제2 보상을 기초로 학습하여 상기 초기 차량 제어 정책을 업 데이트하는 단계를 더 포함할 수 있다. 상기 사용자 입력을 수신하는 단계는 차량에 탑승한 사용자가 차량을 통해 입력하는 차량 입력을 수신하는 단계 를 더 포함하고, 상기 차량 제어 정책을 생성하는 단계는 상기 강화학습 모델을 통해 상기 차량 입력에 따라 생 성된 제3 상태 및 제3 보상을 기초로 학습하여 상기 초기 차량 제어 정책을 업데이트하는 단계를 더 포함할 수 있다. 상기 제1 보상은 0보다 작고, 상기 제2 보상은 상기 제1 보상보다 작고, 상기 제3 보상은 상기 제2 보상보다 작 으며, 상기 사용자의 답변이 수신되지 않으면 상기 제1 보상은 0이고, 상기 사용자의 실시간 입력이 수신되지 않으면 상기 제2 보상은 0이고, 상기 차량 입력이 수신되지 않으면 상기 제3 보상은 0일 수 있다. 상기 차량 제어 정책을 생성하는 단계는 상기 제1 내지 제3 보상들을 포함하는 보상이 최대값을 가지는 경우에 상기 초기 차량 제어 정책을 상기 차량 제어 정책으로 최종 결정하고, 상기 사용자의 답변, 상기 사용자의 실시 간 입력 및 상기 차량 입력이 수신되지 않는 경우에 상기 보상은 최대값을 가질 수 있다. 상기 차량 제어 정책을 생성하는 단계는 상기 최종 결정된 차량 제어 정책에 대해 상기 DQN에 따라 미리 산출된 실제값과 상기 초기 제어 정책에 대해 상기 DQN에 따라 예측된 예측값 간의 차이인 손실을 최소화하는 방향으로 상기 강화학습을 진행할 수 있다."}
{"patent_id": "10-2023-0116376", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 스마트 차량 제어 장치 및 방법은 디지털키 애플리케이션을 통해 입력되는 사용자의 스 케줄 및 일상 패턴에 대한 강화학습을 통해 차량의 주행 준비를 위한 차량 제어를 능동적으로 수행할 수 있다. 본 발명의 일 실시예에 스마트 차량 제어 장치 및 방법은 강화학습 모델을 이용한 반복적인 학습을 통해 사용자 의 스케줄 또는 일상 패턴과 최적으로 대응하는 차량 제어 스케줄을 생성할 수 있다."}
{"patent_id": "10-2023-0116376", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 발명의 실시 예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식 을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해 서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 명세서 및 청구범위 전체에서, 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재 가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적 으로만 사용된다. 명세서에 기재된 \"...부\", \"...기\", \"모듈\" 등의 용어는 본 명세서에서 설명되는 적어도 하나의 기능이나 동작 을 처리할 수 있는 단위를 의미할 수 있으며, 이는 하드웨어 또는 회로나, 소프트웨어, 또는 하드웨어 또는 회 로와 소프트웨어의 결합으로 구현될 수 있다. 이하, 도면을 참조하여 본 발명의 실시예들을 설명한다. 도 1은 본 발명의 일 실시예에 따른 스마트 차량 제어 시스템을 개략적으로 보여준다. 도 1을 참조하면, 스마트 차량 제어 시스템은 스마트 차량 제어 장치, 사용자 단말, 차량 단말, 및 데이터베이스를 포함할 수 있다. 스마트 차량 제어 장치는 사용자의 스케줄 및 일상 패턴에 대해 인공지능을 통한 강화학습을 통해 차량을 능동적으로 제어할 수 있다. 스마트 차량 제어 장치는 강화학습을 이용하여 시간이 지날수록 진화하고 최 적화되는 스마트 차량 제어 기능을 제공할 수 있다. 스마트 차량 제어 장치는 사용자 단말 및 차량 단말을 통해서 각각 입력되는 사용자 입력 및 차량 입력을 기초로 강화학습을 통해서 최적화되는 차량의 주행 준비를 위한 차량 제어 정책을 생성할 수 있다. 스마트 차량 제어 장치는 최적화된 차량 제어 정책을 가지고 차량을 능동적으로 제어할 수 있다. 스마트 차량 제어 장치는 차량의 차량 내부의 컴퓨터 시스템을 포함하는 차량의 내장 시스템과 연결될 수 있다. 차량 내부의 컴퓨터 시스템은 스마트 차량 제어 장치로부터 신호 또는 명령을 받아들여 자동차를 잠 그거나 잠금 해제하고, 엔진을 시작하는 등의 작업을 수행한다. 스마트 차량 제어 장치는 차량의 CAN(Controller Area Network) 버스를 통해 연결되고, 차량의 다른 부분들과 통신할 수 있다. 사용자 단말은 스마트폰, 태블릿 PC, 노트 PC, 데스크탑과 같은 프로세서, 메모리, 통신 기능을 갖춘 스마 트 장치가 될 수 있다. 사용자 단말은 차량을 무선 제어하는 디지털키 기능을 하는 앱 또는 애플리케이션 을 사용자에게 제공할 수 있다. 디지털키는 사용자의 편의를 위하여 실제 자동차 키를 대신하거나 보조할 수 있 다. 예를 들어, 디지털키는 사용자 단말의 무선 통신을 이용하여 차량의 도어를 열고 시동을 걸 수 있다. 사용자 단말은 주로 스마트폰 또는 태블릿 PC를 포함하는 사용자의 모바일 기기일 수 있다. 사용자 단말 은 디스플레이 및 입력 기능을 갖추고 있다. 사용자 단말은 디지털키 애플리케이션을 통해서 사용자 에게 차량의 시동을 키거나, 도어를 열거나, 음악을 틀거나 또는 에어컨을 키는 등의 차량 제어를 위한 인터페 이스를 제공할 수 있다. 즉, 사용자는 사용자 단말의 디지털키 애플리케이션을 통해서 차량을 수동으로 제 어할 수 있다.일 실시예에서, 사용자 단말은 디지털키 애플리케이션을 통해서 사용자에게 사용자의 스케줄 및/또는 일상 패턴과 관련된 정보를 입력할 수 있는 인터페이스를 제공할 수 있다. 사용자 단말은 스마트 차량 제어 장 치와 유무선 통신을 포함하는 네트워크를 통해 연결되고, 디지털키 애플리케이션에 입력된 사용자의 스케 줄 및/또는 일상 패턴과 관련된 정보를 스마트 차량 제어 장치에 제공할 수 있다. 차량 단말은 차량 내부의 컴퓨터 시스템을 포함하는 차량의 내장 시스템을 포함할 수 있다. 차량 단말 은 차량의 도어를 열거나, 시동을 키거나, 음악을 틀거나 에어컨을 조작하는 등의 차량을 직접 조작하는 매뉴얼 입력을 위한 환경을 사용자에게 제공할 수 있다. 즉, 차량 단말는 자동차의 내장된 컴퓨터 시스템 으로 스마트 차량 제어 장치 및 사용자 단말과 네트워크를 통해 연결될 수 있다. 데이터베이스는 스마트 차량 제어 장치와 네트워크를 통해 연결되어 스마트 차량 제어 장치에 필요한 사용자의 스케줄, 패턴 정보, 날씨를 포함하는 환경 정보 및 차량과 관련된 차량 스펙, 차량 위치 등의 정보들을 저장할 수 있다. 데이터베이스는 인공지능을 통한 강화학습 및 지도학습에 필요한 다양한 데이터 들을 저장할 수 있다. 도 2는 본 발명의 일 실시예에 따른 스마트 차량 제어 장치의 블록도이다. 도 3은 도 2의 블록도를 구체적으로 보여주는 도면이다. 도 2 및 도 3을 참조하면, 스마트 차량 제어 장치는 사용자 입력 수신부, 차량 입력 수신부, 강 화학습 모델, 지도학습 모델 및 차량 제어부를 포함한다. 사용자 입력 수신부는 디지털키 애플리케이션을 통해 입력된 사용자 스케줄, 일상패턴 등의 사용자와 관련 된 정보를 포함하는 사용자 입력을 수신할 수 있다. 예를 들어, 사용자 입력 수신부는 스케줄 및 일상 패 턴 정보(T)로서, 차량 승차 정보(T1), 공조기 설정 정보(T2), AVN(Audio, Video, Nevigation) 정보(T3), 및 차 량 도어 자동 개폐(walk access/away) 정보(T4)를 수신할 수 있다. 사용자 입력 수신부는 디지털키 애플리케이션을 통해 사용자에게 제공되는 차량의 주행 준비와 관련된 질 문에 대한 사용자의 답변을 수신할 수 있다. 사용자 입력 수신부는 강화학습 모델로부터 디지털키 애 플리케이션을 통해 사용자에게 제공되는 질문에 대해서 사용자가 답변을 입력하면 사용자의 답변을 수신한다. 또한, 사용자 입력 수신부는 디지털키 애플리케이션을 통해 차량의 주행 준비와 관련된 사용자의 실시간 입력을 수신할 수 있다. 사용자의 실시간 입력은 사용자가 미리 입력한 스케줄과 별도로 사용자에 의해 상시 입 력되는 목적지 변동, 차량 승차 시간 변동 등을 포함하는 스케줄 업데이트 정보일 수 있다. 예를 들어, 사용자 상시 입력(F)은 사용자의 차량 운행 또는 탑승 전의 입력(F1) 및 운행 후 피드백(F2)을 위한 입력을 포함할 수 있다. 차량 입력 수신부는 사용자가 차량을 통해 입력하는 차량 입력을 수신할 수 있다. 차량 입력은 차량의 문 을 열거나(N1), 시동을 키거나(N2), 음악을 틀거나 에어컨을 키는(N3) 등의 차량에 탑승한 사용자의 수동 조작 에 따른 매뉴얼 입력(N)에 해당할 수 있다. 강화학습 모델은 사용자 입력 및 차량 입력을 기초로 강화학습을 통해 차량의 주행 준비를 위한 차량 제어 정책을 생성할 수 있다. 강화학습(Reinforcement Learning)은 기계 학습의 한 형태로, 소프트웨어 에이전트(agent)가 환경에서 행동을 선택하여 그 결과로 얻는 보상(reward)을 극대화하는 방법을 배우는 것을 의미할 수 있다. 강화학습은 일련의 시행착오를 통해 이루어질 수 있다. 에이전트는 초기에는 임의로 행동을 선택하지만, 시간이 지남에 따라 유용 한 정보(즉, 보상)를 축적하면서 더 나은 행동을 선택하는 방법을 학습한다. 여기에서, 에이전트는 강화학습 모 델을 지칭할 수 있다. 강화학습의 주요 개념 중 하나는 \"정책(policy)\"이다. 정책은 주어진 상태(state)에서 에이전트가 어떤 행동 (action)을 선택할지 결정하는 방법을 정의할 수 있다. 정책은 고정된 것일 수도 있고, 시간이 지남에 따라 변 화할 수도 있다. 강화학습의 목표는 보상을 최대화하는 정책을 찾는 것일 수 있다. 강화학습의 다른 중요한 개념으로는 \"가치 함수(value function)\"가 있다. 가치 함수는 특정 상태나 행동의 \"가 치\"를 예측하는 함수로, 보통 미래의 예상 보상을 기반으로 한다. 가치 함수를 사용하여 에이전트는 여러 가능 한 행동 중에서 가장 가치가 높은 것을 선택할 수 있다. 즉, 에이전트는 단기적으로만 보상을 추구하는 것이 아 니라, 장기적으로 최대의 보상을 얻을 수 있는 전략 또는 정책을 선택할 수 있다. 일 실시예에서, 강화학습 모델은 사용자 입력 및/또는 차량 입력에 따라 주어지는 다양한 상태(state)들에 서 보상(reward)을 최대로 하는 최적의 행동(action)을 선택하기 위한 차량 제어 정책을 생성할 수 있다. 강화 학습 모델은 가치 함수를 사용하여 복수의 행동들 중에서 가장 가치가 높은 행동을 선택하는 차량 제어 정 책을 생성할 수 있다. 강화학습 모델은 사용자의 스케줄 및 일상패턴에 관한 정보를 포함하는 사용자 입력을 기초로 규칙 기반 (rule-based) 모델링을 통해 초기 차량 제어 정책을 생성할 수 있다. 강화학습 모델은 초기 차량 제어 정 책을 기초로 반복적인 강화학습을 통해서 최종적으로 차량 제어 정책을 생성할 수 있다. 초기 차량 제어 정책은 사용자의 디지털키 애플리케이션을 통해 맨 처음 입력한 사용자의 스케줄 및 일상 패턴에 대한 정보를 기초로 생성될 수 있다. 규칙 기반(rule-based) 모델링은 강화학습에서 주로 사용되는 전략 중 하나이다. 규칙 기반(rule-based) 모델링 은 사용자 및 차량과 관련된 사전 지식을 활용하여 인공지능을 통해 차량의 초기 행동 정책 또는 초기 제어 정 책을 정의할 수 있다. 즉, 초기 차량 제어 정책은 이미 알고 있는 사용자의 스케줄 및 사용자의 행동 패턴, 일 상 패턴 및 차량 사용 패턴을 기초로 생성될 수 있다. 규칙 기반(rule-based) 모델링은 강화학습의 초기에 에이전트(agent)가 무작위로 행동을 선택하고 학습 곡선을 통해 보상을 최적화하는 것보다 효율적일 수 있다. 규칙 기반(rule-based) 모델링은 사용자의 일상 패턴 및 그 에 따른 차량 이용 패턴에 대한 사전에 입력된 정보를 기반으로 하기 때문이다. 예를 들어, 규칙 기반(rule- based) 모델링은 사용자의 출근 스케줄을 기초로 사용자가 차량에 탑승하는 시간 및 출발하는 시간을 합리적으 로 결정할 수 있다. 따라서, 규칙 기반(rule-based) 모델링은 차량의 문을 여는 시간, 시동을 켜는 시간을 빠르 고 적절하게 설정할 수 있다. 일 실시예에서, 강화학습 모델은 DQN(Deep Q Network)을 포함할 수 있다. 강화학습 모델은 DQN을 통 해서 초기 차량 제어 정책으로부터 최종 차량 제어 정책을 생성할 수 있다. 즉, 강화학습 모델은 DQN을 이 용하여 사용자 입력 및 차량 입력을 기초로 최적의 차량 제어 정책을 생성할 수 있다. DQN은 강화학습의 한 방법으로, 딥러닝(deep learning)과 큐러닝(Q-Learning)을 결합한 것이다. DQN의 핵심 아 이디어는 Q-함수의 근사치를 얻기 위해 심층 신경망을 사용하는 것이다. Q-함수는 주어진 상태에서 특정 행동을 취할 때 얻을 수 있는 미래의 총 보상의 예상치를 나타낸다. 원래 큐러닝에서는 모든 가능한 상태(state)-행동(action) 쌍에 대한 Q-값(Q-value)을 테이블로 저장하고 업데 이트하는 방식을 사용한다. 그러나, DQN은 신경망을 사용하여 Q-함수를 근사하고, 주어진 상태에서 각 행동의 Q-값을 예측한다. DQN에서는 신경망이 이 Q-값를 예측하며, 이것을 Qpredict 또는 예측된 Q-value라고 한다. Q-target은 에이전트 가 수행한 실제 행동에 대한 보상과 그 다음 상태에서 가장 높은 Q-value를 합친 값으로, 최적의 Q-value에 가 깝다고 가정한다. 여기에는 할인 계수(discounting factor)가 적용되어, 미래의 보상이 현재의 보상보다 약간 덜 중요하게 취급될 수 있다. 즉, 보상의 순위는 시간 순서에 따라서 나중에 발생한 보상이 먼저 발생한 보상보 다 크기가 작을 수 있다. 예를 들어, 차량에 탑승하기 전 사용자의 상시 입력에 대한 제1 보상의 크기는 차량에 탑승한 이후의 차량 입력을 통한 제2 보상의 크기보다 크다. Q-target은 특정 상태에서 특정 행동에 대한 Q-value의 실제값일 수 있다. Qpredict는 DQN에 의해서 동일한 특 정 상태에서 특정 행동에 대한 Q-value의 예측값일 수 있다. Qpredict와 Q-target 사이의 차이는 에이전트의 행동에 대한 신경망의 예측이 얼마나 잘 이루어지고 있는지를 나타내는 측정치인 손실(loss)을 생성하는데 사용된다. 이 손실을 최소화하는 것이 DQN 학습의 목표이다. 이에 따라, 에이전트는 시간이 지남에 따라 더 나은 행동을 예측하고 선택할 수 있다. 강화학습 모델은 DQN을 통한 강화학습을 이용하여 사용자 입력 및 차량 입력을 기초로 최적의 차량 제어를 위한 행동을 예측하고 선택하기 위한 차량 제어 정책을 생성할 수 있다. 예를 들어, 강화학습 모델은 DQN 에 따라 산출되는 차량 제어 정책의 실제값(Q-target)과 반복적으로 업데이트 되는 상기 초기 차량 제어 정책들 각각에 대한 예측값(Qpredict)의 차이인 손실값(loss)을 줄이는 방향으로 학습한다. 즉, 강화학습 모델은 반복적인 초기 차량 제어 정책의 업데이트를 통해서 초기 차량 제어 정책이 최적의 차량 제어를 제공하는 최종 차량 제어 정책과 동일하게 되는 것을 목표로 학습할 수 있다. 지도학습 모델은 사용자 입력 및 차량 입력에 의한 요인 외에 차량 정보 및 환경 정보를 포함하는 정적 요 인들을 기초로 지도학습을 통해 차량 제어 정책의 적어도 일부를 업데이트할 수 있다. 지도학습(SupervisedLearning)은 기계 학습의 한 종류로, 모델이 입력과 그에 상응하는 출력이 주어진 학습 데이터를 통해 학습하고, 새로운 입력에 대한 적절한 출력을 예측하는 방식이다. 지도학습은 모델이 예측을 할 때 정답이라고 할 수 있는 레이블(label)이 제공되는 학습 방식이다. 예를 들어, 지도학습 모델은 사용자 입력 및 차량 입력과 상관없이, 차량의 위치, 날씨의 요인에 따라서 차량의 시동을 키는 시간 및 공조기를 작동하는 시간을 가변적으로 결정할 수 있다. 즉, 정적 요인은 고정된 환 경요인으로 차량 위치, 외기 온도, 날씨 등을 포함할 수 있다. 지도학습 모델은 차량 위치와 이에 대응하 는 레이블인 시동을 키는 시간에 대한 학습 데이터를 가지고 학습한다. 지도학습 모델은 차량 스펙, 날씨 및 차량 위치에 따른 사용자와 차량 사이의 거리에 대한 데이터들을 데 이터베이스(400, 도 1 참조)로부터 수신하고, 이를 기초로 학습하고, 이를 반영하여 차량 제어 정책을 업데이트 할 수 있다. 지도학습 모델은 학습을 통해 외기온도에 대한 차량 냉방온도를 가변적으로 결정할 수 있다. 또는, 지도학 습 모델은 외기온도에 따라 차량 엔진 예열시간 설정값을 가변적으로 결정할 수 있다. 지도학습 모델(14 0)은 가변적으로 결정된 상기 냉방온도 및/또는 엔진 예열시간을 기초로 차량 제어 정책을 변경할 수 있다. 일 실시예에서, 지도학습 모델은 차량 제어 정책의 스케줄에 대한 제1 가중치와 정적 요인들 각각에 대한 제2 가중치를 결정할 수 있다. 예를 들어, 지도학습 모델은 차량 엔진 시동 스타트 스케줄에 대한 제1 가 중치를 외기온도에 따라 가변된 엔진 예열시간의 제2 가중치보다 큰 값으로 결정할 수 있다. 지도학습 모델은 제2 가중치가 제1 가중치보다 큰 경우에만 차량 제어 정책의 기존 스케줄을 정적 요인을 반영하여 업데이트할 수 있다. 즉, 차량 엔진 시동 스타트 스케줄의 제1 가중치가 외기온도에 따른 엔진 예열시 간에 대한 제2 가중치보다 크면, 지도학습 모델은 지도학습에 따라 차량 제어 정책을 업데이트 하지 않는 다. 지도학습 모델은 학습에 따라 가변적으로 결정된 외기온도에 따른 냉방온도 설정값을 차량 제어 정책에 반 영할 때 가중치를 고려할 수 있다. 예를 들어, 지도학습 모델은 기존 차량 제어 정책의 차량 스케줄 중 하 나인 엔진 시동 시간에 관한 제1 가중치와 외기온도에 따른 냉방온도에 대해 설정된 제2 가중치를 비교하고, 제 2 가중치가 제1 가중치보다 큰 경우에 외기온도에 따라 냉방온도를 업데이트할 수 있다. 지도학습 모델은 제2 가중치를 (1 - 제1 가중치)로 설정할 수 있다. 예를 들어, 제1 가중치가 0.4이면 제2 가중치는 0.6으로 결정된다. 지도학습 모델은 (제2 가중치 x 정적요인 + 제1 가중치 x 차량스케줄)의 수식에 따라서 냉방온도 또는 엔 진 예열 시간을 업데이트 할 수 있다. 즉, 외기온도를 40% 반영하고, 기존 차량스케줄을 60% 반영하여서 차량 정책을 업데이트할 수 있다. 차량 제어부는 강화학습 모델을 통해 생성된 차량 제어 정책에 따라 차량을 제어할 수 있다. 차량 제 어부는 강화학습 모델 및 지도학습 모델에 따라서 생성 및 업데이트된 최종의 차량 제어 정책을 기초로 차량을 제어할 수 있다. 즉, 차량 제어부는 강화학습 모델을 통한 차량 제어 정책에 지도학습 모델에 따른 학습 결과를 반영하여 결정된 최종 차량 제어 정책을 기초로 차량을 제어한다. 도 4는 본 발명의 일 실시예에 따른 스마트 차량 제어 장치에 의한 차량 제어 프로세스를 보여주는 도면이다. 도 4의 보상 및 손실의 값은 설명을 위해 임의로 가정한다. 강화학습 모델은 디지털키 애플리케이션을 통해 사용자에게 차량의 주행 준비와 관련된 질문을 제공할 수 있다. 예를 들어, 강화학습 모델은 초기 차량 제어 정책을 생성하기 전에, 사용자에게 에어컨 등 공조기 설정, 미디어 설정, 및 차량 승차 시간 등에 관한 질문을 제공할 수 있다. 강화학습 모델은 상기 질문에 대한 사용자의 답변에 따라 이를 반영하여 초기 차량 제어 정책을 생성할 수 있다. 또는, 강화학습 모델은 초기 차량 제어 정책을 생성한 이후에 사용자 단말을 통해 사용자에게 질문을 제공하고 답변을 수신할 수도 있다(단계 S10). 질문에 대한 사용자의 답변이 수신되면, 강화학습 모델은 수신된 답변을 기초로 생성되는 제1 상태 및 제1 보상을 기초로 학습하여 초기 차량 제어 정책을 업데이트할 수 있다. 업데이트된 초기 차량 제어 정책은 제1 상태에서 제1 보상을 생성하는 제1 행동을 선택한다. 예를 들어, 강화학습 모델은 제1 상태가 입력되면 가장 큰 보상으로 나타나는 제1 보상을 가지는 제1 행동 을 선택하도록 초기 차량 제어 정책을 업데이트할 수 있다. 여기에서, 제1 보상은 0보다 작은 값(예를 들어 -1)을 가지고, 상기 사용자의 답변이 수신되지 않으면 상기 제1 보상의 값은 0이다. 강화학습 모델은 디지털키 애플리케이션을 통해 실시간으로 입력되는 사용자의 상시 입력이 수신되면, 실 시간 입력에 따라 생성되는 제2 상태 및 제2 보상을 기초로 학습하여 초기 차량 제어 정책을 업데이트할 수 있 다. 예를 들어, 강화학습 모델은 사용자 단말을 통해 사용자의 목적지 변경, 차량 승차 시간 변경 등 의 입력이 수신되면 이를 반영하여 초기 차량 제어 정책을 업데이트할 수 있다(단계 S20). 강화학습 모델은 사용자의 상시 입력에 따른 제2 상태가 생성되면 가장 큰 보상으로 나타나는 제2 보상을 가지는 제2 행동을 선택하도록 초기 차량 제어 정책을 업데이트할 수 있다. 여기에서, 제2 보상은 제1 보상보다 작은 값을 가지고(예를 들어, -2), 사용자의 실시간 입력이 수신되지 않으면 제2 보상의 값은 0이다. 강화학습 모델은 사용자가 차량에 탑승하여 수동으로 입력한 차량 입력이 수신되면, 생성된 제3 상태 및 제3 보상을 기초로 학습하여 상기 초기 차량 제어 정책을 업데이트한다(단계 S30). 강화학습 모델은 기 설 정된 스케줄과 다른 시간에 사용자가 매뉴얼로 차량의 시동을 킨 경우, 제3 상태가 생성되면 가장 큰 보상으로 나타나는 제3 보상을 가지는 제3 행동을 선택하도록 초기 차량 제어 정책을 업데이트할 수 있다. 여기에서, 제3 보상은 제2 보상보다 작은 값(예를 들어 -3)을 가지고, 차량의 매뉴얼 입력이 수신되지 않으면 제3 보상의 값은 0일 수 있다. 강화학습 모델은 사용자 입력 및 차량 입력을 기초로 최적의 행동(action)을 수행하는 차량 제어 정책을 생성하고(단계 S40), 차량 제어부는 이를 기초로 차량을 제어할 수 있다(단계 S50). 강화학습 모델은 차량의 운행이 종료되고 사용자 단말의 디지털키 애플리케이션을 통해 차량 제어와 관련한 피드백이 입력되면, 제4 상태 및 제4 보상을 기초로 학습하여 차량 제어 정책을 업데이트할 수 있다(단 계 S60). 강화학습 모델은 사용자의 피드백 입력에 따른 제4 상태가 생성되면 가장 큰 보상으로 나타나는 제4 보상을 가지는 제4 행동을 선택하도록 초기 차량 제어 정책을 업데이트할 수 있다. 여기에서, 제4 보상은 제3 보상보다 작은 값을 가지고(예를 들어, -4), 사용자의 피드백 입력이 수신되지 않으면 제4 보상의 값은 0이 다. 강화학습 모델은 모든 보상을 0보다 작은 값으로 결정할 수 있다. 또한, 강화학습 모델은 제1 내지 제4 상태들 중 시간순으로 가장 빠른 상태인 제1 상태에 대한 제1 보상의 크기를 가장 크게 결정하고, 가장 늦 은 상태인 제4 상태에 대한 제4 보상의 크기를 가장 작게 결정할 수 있다. 즉, 보상의 크기는 시간과 반비례할 수 있다. 강화학습 모델은 업데이트된 초기 차량 제어 정책을 기초로 최종적으로 차량 제어 정책을 생성하고, 차량 스케줄 정보를 초기화 할 수 있다(단계 S70). 즉, 강화학습 모델은 강화학습을 통해 생성된 차량 제어 정 책을 가지고 다음 번의 차량 제어를 위한 차량 스케줄 정보를 초기화할 수 있다. 강화학습 모델은 초기 차량 제어 정책이 그대로 최종 차량 제어 정책과 동일한 경우에 사용자의 스케줄에 따른 최적의 맞춤 차량 제어 스케줄을 생성한 것으로 간주한다. 예를 들어, 초기 차량 제어 정책을 생성한 이후 에 복수의 상태들 및 복수의 보상들이 발생하면 보상의 크기는 점점 줄어들 수 있다. 강화학습 모델은 보 상의 크기를 최대화하는 방향으로 강화학습을 진행한다. 즉, 강화학습 모델은 제1 내지 제3 보상들을 포함하는 보상이 최대값을 가지는 방향으로 상기 초기 차량 제어 정책을 반복적으로 업데이트하여 상기 차량 제어 정책을 생성할 수 있다. 보상은 초기 차량 제어 정책이 생성된 이후에, 사용자의 답변, 사용자의 실시간 입력 및 차량 입력이 전혀 수신되지 않는 경우에 최대값을 가 질 수 있다. 일 실시예에서, 강화학습 모델은 DQN에 따른 손실값(loss)이 최소값을 가지는 때의 초기 차량 제어 정책을 차량 제어 정책으로 결정할 수 있다. 차량 제어 정책의 실제값(Qtarget)은 최대값을 가지는 보상을 반영하여 결 정되고, 초기 차량 제어 정책들 각각에 대한 예측값(Qpredict)은 제1 내지 제3 보상들 중 적어도 하나를 반영하 여 결정될 수 있다. 따라서, 차량 제어 정책의 실제값과 초기 차량 제어 정책들 각각의 예측값의 차이에 해당하 는 손실값은 차량 제어 정책의 실제값과 초기 차량 제어 정책들 각각의 예측값이 동일한 경우에 최소인 0일 수 있다. 손실값의 크기는 시간과 비례할 수 있다. 예를 들어, 사용자 질의 답변 입력을 수신한 경우에 업데이트되는 차 량 제어 정책에 대한 실제값과 예측값의 차이는 1일 수 있다. 사용자 상시 입력을 수신한 경우에 업데이트되는 차량 제어 정책에 대한 손실값은 2이고, 차량 제어 입력을 수신한 경우 손실값은 3일 수 있다. 차량 운행이 종료된 이후 사용자 피드백 입력이 수신되는 경우 이에 따라 업데이트되는 차량 제어 정책에 대한 예측값과 실제 값의 차이는 4일 수 있다. 즉, 사용자 질의 답변, 사용자 상시 입력, 차량 입력 및 피드백 입력이 수신되지 않 은 경우에 손실값은 최소일 수 있다. 도 5는 본 발명의 일 실시예에 따른 스마트 차량 제어의 일 예를 보여주는 도면이다. 도 6은 본 발명의 일 실시 예에 따른 사용자 스케줄 입력 및 출력 화면을 보여준다. 도 5 및 도 6에서, A스케줄은 최초 사용자가 디지털키 애플리케이션을 통해 입력한 스케줄이고, B스케줄은 차량 제어 정책의 반복적 업데이트에 따라 새로 생성된 스케줄일 수 있다. 도 5에서, A스케줄에 따라서 강화학습 모델은 제1 제어정책을 생성할 수 있다. 강화학습 모델은 제1 제어정책의 생성 이후, 사용자의 실시간 입력에 따라 목적지가 A에서 B로 변경되면, 이를 기초로 제1 제어정책 을 업데이트할 수 있다. 운행이 종료되면, 강화학습 모델은 제2 제어정책을 생성한다. 제2 제어정책은 업 데이트에 따라 목적지B를 설정할 수 있다. 강화학습 모델은 제2 제어정책에 따른 스케줄을 생성한다. 제2 제어정책이 생성된 이후, 사용자의 차량 매 뉴얼 입력에 따른 도어 오픈 시간에 1분 일찍 발생하면, 강화학습 모델은 이를 기초로 제2 제어정책을 업 데이트할 수 있다. 운행이 종료되면 강화학습 모델은 제3 제어정책을 생성한다. 제3 제어정책은 업데이트 에 따라 도어 오픈시간은 6:29으로 재설정할 수 있다. 강화학습 모델은 제3 제어정책에 따라서 B스케줄을 생성할 수 있다. 도 7은 본 발명의 일 실시예에 따른 스마트 차량 제어 방법을 보여주는 흐름도이다. 스마트 차량 제어 방법은 스마트 차량 제어 장치를 통해 수행될 수 있다. 도 1 내지 도 3을 참조하여 설명한다. 도 7에서, 스마트 차량 제어 장치는 사용자 입력 수신부를 통해 사용자 단말로부터 디지털키 애 플리케이션을 통해 입력되는 사용자 입력을 수신할 수 있다(단계 S100). 사용자 입력은 사용자 스케줄 및 일상 패턴과 관련된 정보를 포함할 수 있다. 스마트 차량 제어 장치는 강화학습 모델을 이용하여, 수신된 사용자 스케줄 정보를 기초로 Rule- based 초기 강화학습을 통해 차량의 주행 준비를 위한 초기 차량 제어 정책을 생성할 수 있다(단계 S200). 스마트 차량 제어 장치는 강화학습 모델을 이용하여, 디지털키 애플리케이션을 통해 입력되는 사용자 의 상시 입력과 차량에서 입력되는 매뉴얼 조작을 기초로 강화학습을 통해 상기 초기 차량 제어 정책을 업데이 트할 수 있다(단계 S300). 강화학습 모델은 DQN(Deep Q Network)을 포함할 수 있다. DQN은 이상적인 차량 제어 정책의 실제값과 업데이트되는 초기 차량 제어 정책의 예측값을 서로 비교하고, 그 차이에 해당하는 손실 이 최소가 되도록 학습을 수행한다. 강화학습 모델은 초기 차량 제어 정책의 생성 이후에 사용자 입력 및 차량 입력이 발생하지 않고 주행이 완료된 경우에 손실이 최소인 경우로 간주한다. 강화학습 모델은 손실 이 최소인 경우의 차량 제어 정책에 따라 차량 제어 스케줄을 업데이트할 수 있다. 스마트 차량 제어 장치는 상기 업데이트 과정을 반복한 후 생성된 최종 차량 제어 정책에 따라서 차량을 제어할 수 있다(단계 S400). 도 8은 본 발명의 일 실시예에 따른 스마트 차량 제어 방법을 보여주는 흐름도이다. 도 8은 도 7의 초기 차량 제어 정책을 업데이트하는 단계를 상세하게 보여준다. 도 8에서, 스마트 차량 제어 장치는 디지털키 애플리케이션을 통해 사용자에게 차량 주행 준비와 관련한 질문을 하고 답변에 따라 생성된 제1 상태 및 제1 보상에 따라 예측되는 제1 행동을 기초로 초기 차량 제어 정 책을 업데이트할 수 있다(단계 S310). 스마트 차량 제어 장치는 사용자 입력 또는 차량 입력이 발생하면, 그에 따른 제2 상태 및 제2 보상에 따 라 예측되는 제2 행동을 기초로 초기 차량 제어 정책을 업데이트할 수 있다(단계 S320). 스마트 차량 제어 장치는 초기 차량 제어 정책의 반복적인 업데이트에 따라서 사용자 입력 및 차량 입력이 더 이상 발생하지 않는 경우의 초기 차량 제어 정책을 최종 차량 제어 정책으로 결정할 수 있다(단계 S330). 스마트 차량 제어 장치는 차량 정보 및 환경 정보를 포함하는 정적 요인들을 기초로 지도학습을 통해 차량 제어 정책의 적어도 일부를 업데이트할 수 있다(단계 S340). 도 9는 본 발명의 일 실시예에 따른 컴퓨팅 장치를 설명하기 위한 도면이다. 도 9를 참조하면, 실시예들에 따른 스마트 차량 제어 장치 및 방법은 컴퓨팅 장치를 이용하여 구현될 수 있다. 컴퓨팅 장치는 버스를 통해 통신하는 프로세서, 메모리, 사용자 인터페이스 입력 장치 , 사용자 인터페이스 출력 장치 및 저장 장치 중 적어도 하나를 포함할 수 있다. 컴퓨팅 장치 는 또한 네트워크에 전기적으로 접속되는 네트워크 인터페이스를 포함할 수 있다. 네트워크 인터 페이스는 네트워크를 통해 다른 개체와 신호를 송신 또는 수신할 수 있다. 프로세서는 MCU(Micro Controller Unit), AP(Application Processor), CPU(Central Processing Unit), GPU(Graphic Processing Unit), NPU(Neural Processing Unit) 등과 같은 다양한 종류들로 구현될 수 있으며, 메모리 또는 저장 장치에 저장된 명령을 실행하는 임의의 반도체 장치일 수 있다. 프로세서는 도 1 내지 도 8과 관련하여 전술한 기능 및 방법들을 구현하도록 구성될 수 있다. 메모리 및 저장 장치는 다양한 형태의 휘발성 또는 비 휘발성 저장 매체를 포함할 수 있다. 예를 들 어, 메모리는 ROM(read-only memory) 및 RAM(random access memory)를 포함할 수 있다. 본 실시예에 서 메모리는 프로세서의 내부 또는 외부에 위치할 수 있고, 메모리는 이미 알려진 다양한 수단 을 통해 프로세서와 연결될 수 있다. 몇몇 실시예에서, 실시 예들에 따른 스마트 차량 제어 장치 및 방법 중 적어도 일부 구성 또는 기능은 컴퓨팅 장치에서 실행되는 프로그램 또는 소프트웨어로 구현될 수 있고, 프로그램 또는 소프트웨어는 컴퓨터로 읽 을 수 있는 매체에 저장될 수 있다. 몇몇 실시예에서, 실시 예들에 따른 스마트 차량 제어 장치 및 방법 중 적어도 일부 구성 또는 기능은 컴퓨팅 장치의 하드웨어 또는 회로를 사용하여 구현되거나, 컴퓨팅 장치와 전기적으로 접속될 수 있는 별도 의 하드웨어 또는 회로로 구현될 수도 있다. 이상에서 본 발명의 실시 예들에 대하여 상세하게 설명하였지만 본 발명의 권리 범위가 이에 한정되는 것은 아 니고, 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한, 본 발명이 속하는 기술 분야에서 통 상의 지식을 가진 자의 여러 변형 및 개량 형태 또한 본 발명의 권리 범위에 속한다."}
{"patent_id": "10-2023-0116376", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 스마트 차량 제어 시스템을 개략적으로 보여준다. 도 2는 본 발명의 일 실시예에 따른 스마트 차량 제어 장치의 블록도이다. 도 3은 도 2의 블록도를 구체적으로 보여주는 도면이다.도 4는 본 발명의 일 실시예에 따른 스마트 차량 제어 장치에 의한 차량 제어 프로세스를 보여주는 도면이다. 도 5는 본 발명의 일 실시예에 따른 스마트 차량 제어의 일 예를 보여주는 도면이다. 도 6은 본 발명의 일 실시예에 따른 사용자 스케줄 입력 및 출력 화면을 보여준다. 도 7은 본 발명의 일 실시예에 따른 스마트 차량 제어 방법을 보여주는 흐름도이다. 도 8은 본 발명의 일 실시예에 따른 스마트 차량 제어 방법을 보여주는 흐름도이다. 도 9는 본 발명의 일 실시예에 따른 컴퓨팅 장치를 설명하기 위한 도면이다."}
