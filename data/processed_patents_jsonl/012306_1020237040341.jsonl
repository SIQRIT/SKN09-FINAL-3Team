{"patent_id": "10-2023-7040341", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0000576", "출원번호": "10-2023-7040341", "발명의 명칭": "운영 가드레일을 제공하기 위해 유사한 시나리오를 사용하는 명시적인 윤리적 머신", "출원인": "레이던 컴퍼니", "발명자": "우, 페기"}}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서를 사용하여, 머신 러닝(machine learning, ML) 또는 인공 지능(artificialintelligence, AI) 알고리즘에 의해 평가되는 현재 시나리오와 연관된 정보를 획득하는 단계 - 상기 정보는 상기 현재 시나리오와 연관된 초기 리워드 함수(initial reward function)을 포함함 -;상기 적어도 하나의 프로세서를 사용하여, 상기 현재 시나리오와 유사한(analogous) 하나 이상의 이전 시나리오와 연관된 하나 이상의 정책(policies)을 식별하는 단계;상기 적어도 하나의 프로세서를 사용하여, 상기 초기 리워드 함수 및 상기 하나 이상의 이전 시나리오와 연관된하나 이상의 리워드 함수에 의해 최적화되는 파라미터 간의 하나 이상의 차이(differences)를 결정하는 단계;상기 적어도 하나의 프로세서를 사용하여, 새로운 리워드 함수를 생성하기 위해 결정된 상기 하나 이상의 차이중 적어도 하나에 기초하여 상기 초기 리워드 함수를 수정하는 단계; 및상기 적어도 하나의 프로세서를 사용하여, 상기 새로운 리워드 함수에 기초하여 상기 현재 시나리오에 대한 새로운 정책을 생성하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 현재 시나리오에 대해 선택된 조치 과정(course of action)을 결정하기 위해 상기 ML 또는 AI 알고리즘을사용하여 상기 현재 시나리오에 상기 새로운 정책을 적용하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 새로운 리워드 함수를 상기 초기 리워드 함수로 사용하여 상기 획득, 식별, 결정, 수정, 및 생성하는 동작들 중 적어도 일부를 반복하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 현재 시나리오와 연관된 상기 정보는 제1 마르코프 결정 프로세스(a first Markov decision process)를 포함하고; 및데이터베이스는 상기 하나 이상의 이전 시나리오와 연관된 하나 이상의 제2 마르코프 결정 프로세스를저장하는,방법.공개특허 10-2024-0000576-3-청구항 5 제1항에 있어서,최적화되는 상기 파라미터 간의 상기 하나 이상의 차이를 결정하는 단계는,상기 현재 시나리오에 적용되는 상기 하나 이상의 이전 시나리오에서 사용된 추론(reasoning)을 식별하기 위해역 강화 러닝(Inverse Reinforcement Learning)을 사용하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 획득, 식별, 결정, 수정, 및 생성하는 동작들은 다수의 에이전트(multiple agents)를 사용하여 수행되고;및적합성(conformance)을 위해 상기 다수의 에이전트에 의해 생성되는 상기 새로운 정책을 비교하는 단계; 및적합하지 않은(not conforming) 상기 다수의 에이전트에 의해 생성되는 상기 새로운 정책에 응답하여, 상기 다수의 에이전트 중 적어도 하나에 의해 사용되는 상기 초기 리워드 함수를 수정하고 상기 획득, 식별, 결정, 수정, 및 생성하는 동작들을 반복하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 초기 리워드 함수 및 상기 새로운 리워드 함수 각각은,작업(task)에 관계없이 하나 이상의 윤리적 경계(ethical boundaries)를 시행하도록(enforce) 구성되는 작업 독립적 부분(task-agnostic portion); 및연관된 리워드 함수의 맥락적 행동(contextual behavior)을 구동하도록 구성되는 작업 종속적 부분(task-dependent portion)을 포함하는, 방법."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "머신 러닝(machine learning, ML) 또는 인공 지능(artificial intelligence, AI) 알고리즘에 의해 평가되는 현재 시나리오와 연관된 정보를 저장하도록 구성되는 적어도 하나의 메모리 - 상기 정보는 상기 현재 시나리오와연관된 초기 리워드 함수(initial reward function)을 포함함 -; 및상기 현재 시나리오와 유사한(analogous) 하나 이상의 이전 시나리오와 연관된 하나 이상의 정책(policies)을식별하고;상기 초기 리워드 함수 및 상기 하나 이상의 이전 시나리오와 연관된 하나 이상의 리워드 함수에 의해 최적화되는 파라미터 간의 하나 이상의 차이(differences)를 결정하고;새로운 리워드 함수를 생성하기 위해 결정된 상기 하나 이상의 차이 중 적어도 하나에 기초하여 상기 초기 리워드 함수를 수정하고; 및상기 새로운 리워드 함수에 기초하여 상기 현재 시나리오에 대한 새로운 정책을 생성하도록 구성되는 적어도 하나의 프로세서공개특허 10-2024-0000576-4-를 포함하는, 장치."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 적어도 하나의 프로세서는,상기 현재 시나리오에 대해 선택된 조치 과정(course of action)을 결정하기 위해 상기 ML 또는 AI 알고리즘을사용하여 상기 현재 시나리오에 상기 새로운 정책을 적용하도록 더 구성되는, 장치."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서,상기 적어도 하나의 프로세서는,상기 새로운 리워드 함수를 상기 초기 리워드 함수로 사용하고 상기 식별, 결정, 수정, 및 생성하는 동작들 중적어도 일부를 반복하도록 더 구성되는, 장치."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항에 있어서,상기 현재 시나리오와 연관된 상기 정보는 제1 마르코프 결정 프로세스(a first Markov decision process)를 포함하고; 및상기 적어도 하나의 프로세서는 상기 하나 이상의 이전 시나리오와 연관된 하나 이상의 제2 마르코프 결정 프로세스를 저장하도록 구성되는 데이터베이스에 액세스하도록 구성되는, 장치."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항에 있어서,상기 하나 이상의 프로세서는,최적화되는 상기 파라미터 간의 상기 하나 이상의 차이를 결정하기 위해, 상기 현재 시나리오에 적용되는 상기하나 이상의 이전 시나리오에서 사용된 추론(reasoning)을 식별하기 위해 역 강화 러닝(Inverse ReinforcementLearning)을 사용하도록 구성되는 장치."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8항에 있어서,상기 적어도 하나의 프로세서는,상기 식별, 결정, 수정, 및 생성하는 동작들을 다수의 에이전트(multiple agents)를 사용하여 수행하고;적합성(conformance)을 위해 상기 다수의 에이전트에 의해 생성되는 상기 새로운 정책을 비교하고; 및적합하지 않은(not conforming) 상기 다수의 에이전트에 의해 생성되는 상기 새로운 정책에 응답하여, 상기 다수의 에이전트 중 적어도 하나에 의해 사용되는 상기 초기 리워드 함수를 수정하고 상기 획득, 식별, 결정, 수정, 및 생성하는 동작들을 반복하도록 더 구성되는, 장치.공개특허 10-2024-0000576-5-청구항 14 제8항에 있어서,상기 초기 리워드 함수 및 상기 새로운 리워드 함수 각각은,작업(task)에 관계없이 하나 이상의 윤리적 경계(ethical boundaries)를 시행하도록(enforce) 구성되는 작업 독립적 부분(task-agnostic portion); 및연관된 리워드 함수의 맥락적 행동(contextual behavior)을 구동하도록 구성되는 작업 종속적 부분(task-dependent portion)을 포함하는, 장치."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "실행될 때, 적어도 하나의 프로세서가,머신 러닝(machine learning, ML) 또는 인공 지능(artificial intelligence, AI) 알고리즘에 의해 평가되는 현재 시나리오와 연관된 정보를 획득하고- 상기 정보는 상기 현재 시나리오와 연관된 초기 리워드 함수(initialreward function)을 포함함 -;상기 현재 시나리오와 유사한(analogous) 하나 이상의 이전 시나리오와 연관된 하나 이상의 정책(policies)을식별하고;상기 초기 리워드 함수 및 상기 하나 이상의 이전 시나리오와 연관된 하나 이상의 리워드 함수에 의해 최적화되는 파라미터 간의 하나 이상의 차이(differences)를 결정하고;새로운 리워드 함수를 생성하기 위해 결정된 상기 하나 이상의 차이 중 적어도 하나에 기초하여 상기 초기 리워드 함수를 수정하고; 및상기 새로운 리워드 함수에 기초하여 상기 현재 시나리오에 대한 새로운 정책을 생성하도록 야기하는 명령어를포함하는, 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,실행될 때, 상기 적어도 하나의 프로세서가,상기 현재 시나리오에 대해 선택된 조치 과정(course of action)을 결정하기 위해 상기 ML 또는 AI 알고리즘을사용하여 상기 현재 시나리오에 상기 새로운 정책을 적용하도록 야기하는 명령어를 더 포함하는, 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제15항에 있어서,실행될 때, 상기 적어도 하나의 프로세서가,상기 새로운 리워드 함수를 상기 초기 리워드 함수로 사용하고 상기 획득, 식별, 결정, 수정, 및 생성하는 동작들 중 적어도 일부를 반복하도록 야기하는 명령어를 더 포함하는, 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제15항에 있어서,공개특허 10-2024-0000576-6-상기 현재 시나리오와 연관된 상기 정보는 제1 마르코프 결정 프로세스(a first Markov decision process)를 포함하고,상기 명령어들은 실행될 때 상기 적어도 하나의 프로세서가 상기 하나 이상의 이전 시나리오와 연관된 하나 이상의 제2 마르코프 결정 프로세스를 저장하도록 구성되는 데이터베이스에 액세스하도록 야기하는,비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제15항에 있어서,실행될 때 상기 적어도 하나의 프로세서가 최적화되는 상기 파라미터 간의 상기 하나 이상의 차이를 결정하도록야기하는 상기 명령어들은,실행될 때 상기 적어도 하나의 프로세서가 상기 현재 시나리오에 적용되는 상기 하나 이상의 이전 시나리오에서사용된 추론(reasoning)을 식별하기 위해 역 강화 러닝(Inverse Reinforcement Learning)을 사용하도록 야기하는 명령어들을 포함하는,비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7040341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제15항에 있어서,실행될 때 상기 적어도 하나의 프로세서가,다수의 에이전트(multiple agents)를 사용하여 상기 획득, 식별, 결정, 수정, 및 생성하는 동작들을 수행하고;적합성(conformance)을 위해 상기 다수의 에이전트에 의해 생성되는 상기 새로운 정책을 비교하고; 및적합하지 않은(not conforming) 상기 다수의 에이전트에 의해 생성되는 상기 새로운 정책에 응답하여, 상기 다수의 에이전트 중 적어도 하나에 의해 사용되는 상기 초기 리워드 함수를 수정하고 상기 획득, 식별, 결정, 수정, 및 생성하는 동작들을 반복하도록 야기하는 명령어를 더 포함하는, 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7040341", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "장치는 머신 러닝(machine learning, ML)/인공 지능(artificial intelligence, AI) 알고리즘에 의해 평가 되는 현재 시나리오(302a, 402a)와 연관된 정보를 저장하도록 구성되는 적어도 하나의 메모리(204, 210, 212)를 포함하고, 상기 정보는 상기 현재 시나리오와 연관된 초기 리워드 함수(302b, 402b)를 포함한다. 상기 장치는 (뒷면에 계속)"}
{"patent_id": "10-2023-7040341", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 일반적으로 머신 러닝(machine learning, ML) 시스템 및 기타 인공 지능(artificial intelligence, AI) 시스템에 관한 것이다. 더 구체적으로, 본 개시는 운영 가드레일(operational guardrails)을 제공하기 위 해 유사한(analogous) 시나리오를 사용하는 명시적인 윤리적 머신(explicit ethical machine)에 관한 것이다."}
{"patent_id": "10-2023-7040341", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "다양한 영역에서 자율(autonomous) 및 반자율(semi-autonomous) 의사 결정 알고리즘의 채택이 증가함에 따라, 알고리즘이 윤리적 딜레마에 직면하는 것이 흔하다. 윤리적 딜레마의 일 예는 자주 \"트롤리 문제(trolley problem)\"라고 불리며, 일반적으로 폭주하는(runaway) 트롤리가 하나의 트랙에서 여러 사람을 향해 가고 있지만 한 사람이 점유하는 다른 트랙으로 방향을 바꿀 수 있는 가상의 상황을 고려한다."}
{"patent_id": "10-2023-7040341", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시는 운영 가드레일을 제공하기 위해 유사한 시나리오를 사용하는 명시적인 윤리적 머신에 관한 것이다. 제1 실시예에서, 방법은 적어도 하나의 프로세서를 사용하여, 머신 러닝(machine learning, ML) 또는 인공 지능 (artificial intelligence, AI) 알고리즘에 의해 평가되는 현재 시나리오와 연관된 정보를 획득하는 단계 - 상 기 정보는 상기 현재 시나리오와 연관된 초기 리워드 함수(initial reward function)을 포함함 -를 포함한다. 상기 방법은 또한 상기 적어도 하나의 프로세서를 사용하여, 상기 현재 시나리오와 유사한(analogous) 하나 이 상의 이전 시나리오와 연관된 하나 이상의 정책(policies)을 식별하는 단계를 포함한다. 상기 방법은 상기 적 어도 하나의 프로세서를 사용하여, 상기 초기 리워드 함수 및 상기 하나 이상의 이전 시나리오와 연관된 하나 이상의 리워드 함수에 의해 최적화되는 파라미터 간의 하나 이상의 차이(differences)를 결정하는 단계를 더 포 함한다. 상기 방법은 또한 상기 적어도 하나의 프로세서를 사용하여, 새로운 리워드 함수를 생성하기 위해 결 정된 상기 하나 이상의 차이 중 적어도 하나에 기초하여 상기 초기 리워드 함수를 수정하는 단계를 포함한다. 또한, 상기 방법은 상기 적어도 하나의 프로세서를 사용하여, 상기 새로운 리워드 함수에 기초하여 상기 현재 시나리오에 대한 새로운 정책을 생성하는 단계를 포함한다. 제2 실시예에서, 장치는 머신 러닝(machine learning, ML) 또는 인공 지능(artificial intelligence, AI) 알고 리즘에 의해 평가되는 현재 시나리오와 연관된 정보를 저장하도록 구성되는 적어도 하나의 메모리 - 상기 정보 는 상기 현재 시나리오와 연관된 초기 리워드 함수(initial reward function)을 포함함 -를 포함한다. 상기 장 치는 또한 상기 현재 시나리오와 유사한(analogous) 하나 이상의 이전 시나리오와 연관된 하나 이상의 정책 (policies)을 식별하고, 상기 초기 리워드 함수 및 상기 하나 이상의 이전 시나리오와 연관된 하나 이상의 리워 드 함수에 의해 최적화되는 파라미터 간의 하나 이상의 차이(differences)를 결정하고, 새로운 리워드 함수를 생성하기 위해 결정된 상기 하나 이상의 차이 중 적어도 하나에 기초하여 상기 초기 리워드 함수를 수정하고, 및 상기 새로운 리워드 함수에 기초하여 상기 현재 시나리오에 대한 새로운 정책을 생성하도록 구성되는 적어도 하나의 프로세서를 포함한다. 제3 실시예에서, 비일시적 컴퓨터 판독 가능 매체는 실행될 때, 적어도 하나의 프로세서가, 머신 러닝(machine learning, ML) 또는 인공 지능(artificial intelligence, AI) 알고리즘에 의해 평가되는 현재 시나리오와 연관 된 정보를 획득하도록 야기하는 명령어를 포함한다- 상기 정보는 상기 현재 시나리오와 연관된 초기 리워드 함 수(initial reward function)을 포함함 -. 상기 매체는 또한 실행될 때, 상기 적어도 하나의 프로세서가, 상기 현재 시나리오와 유사한(analogous) 하나 이상의 이전 시나리오와 연관된 하나 이상의 정책(policies)을 식별하 도록 야기하는 명령어를 포함한다. 상기 매체는 실행될 때, 상기 적어도 하나의 프로세서가, 상기 초기 리워드 함수 및 상기 하나 이상의 이전 시나리오와 연관된 하나 이상의 리워드 함수에 의해 최적화되는 파라미터 간의 하나 이상의 차이(differences)를 결정하도록 야기하는 명령어를 더 포함한다. 상기 매체는 또한 실행될 때, 상기 적어도 하나의 프로세서가, 새로운 리워드 함수를 생성하기 위해 결정된 상기 하나 이상의 차이 중 적어도 하나에 기초하여 상기 초기 리워드 함수를 수정하도록 야기하는 명령어를 포함한다. 또한, 상기 매체는 실행될 때, 상기 적어도 하나의 프로세서가, 상기 새로운 리워드 함수에 기초하여 상기 현재 시나리오에 대한 새로운 정책을 생성하도록 야기하는 명령어를 포함한다. 다른 기술적 특징은 다음의 도면, 설명 및 청구범위로부터 당업자에게 쉽게 명백할 수 있다."}
{"patent_id": "10-2023-7040341", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에 설명되는 도 1 내지 도 6 및 본 개시의 원리를 설명하기 위해 사용되는 다양한 실시예는 단지 예시일 뿐 이며 어떠한 방식으로 본 개시의 범위를 제한하는 것으로 해석되어서는 안 된다. 당업자는 본 개시의 원리가 적절하게 배열된 임의의 유형의 장치 또는 시스템에서 구현될 수 있다는 것을 이해할 것이다. 위에서 언급한 바와 같이, 다양한 영역에서 자율 및 반자율 의사 결정 알고리즘의 채택이 증가함에 따라, 알고 리즘이 윤리적 딜레마에 직면하는 것이 흔하다. 윤리적 딜레마의 일 예는 자주 \"트롤리 문제\"라고 불리며, 일 반적으로 폭주하는 트롤리가 하나의 트랙에서 여러 사람을 향해 가고 있지만 한 사람이 점유하는 다른 트랙으로 방향을 바꿀 수 있는 가상의 상황을 고려한다. 이러한 유형 및 기타 유형의 윤리적 딜레마는 자율주행차를 위 한 제어 시스템 설계와 같은 다양한 의사결정 알고리즘에서 발생할 수 있다. 불행하게도, 복잡한 자율 및 반자 율 시스템은 운영자(operators)에게 제한된 투명성을 제공하는 머신 러닝(ML) 알고리즘 또는 기타 인공 지능 (AI) 알고리즘을 일상적으로 사용하여 본질적으로 \"블랙 박스\" 역할을 한다. 결과적으로 시스템 엔지니어는 요 구사항 및 시스템 행동을 정의된 파라미터로 설계하는 경우가 많으며, 이렇게 정의된 파라미터는 사용 시 올바 르게 동작할 것으로 예상된다. 자율 및 반자율 시스템에 사용되는 현재 접근 방식은 자주 주어진 시나리오에 대해 원하는 조치 과정(courses of action)을 인코딩하는 폐쇄 세계 가정(closed-world assumptions)에 의존한다. 그러나 이러한 접근 방식은 매우 취약하며 환경이나 시나리오에서 사소한 변화에도 확장될 수 없는 경우가 많으므로, 이러한 시스템을 신뢰 할 수 없게 만들고 본질적으로 치명적인 오류를 일으키기 쉽다. 미국 국방부는 일반적으로 \"책임감 있는 (responsible)\", \"공평한(equitable)\", \"추적 가능한(traceable)\", \"신뢰 가능한(reliable)\", \"통제 가능한 (governable)\"으로 분류되는 5가지 주요 영역을 포괄하는 인공 지능 시스템의 5가지 핵심 \"'윤리 원칙(ethical principles)\"을 채택했다. 그러나 현장에서 이러한 5가지 원칙을 실제로 구현하는 것은 현재 오류와 편견 (bias)이 발생하기 쉬운 무정형의(amorphous) 인간의 판단에 달려 있다. 더욱이, 자율 또는 반자율 시스템이 취하는 조치 과정이 윤리적 의미를 갖는지 여부가 항상 명확한 것은 아니다. 본 개시는 운영 가드레일(operational guardrails)을 제공하기 위해 유사한 시나리오를 사용하여 이러한 유형의 원리 중 적어도 일부를 만족할 수 있는 명시적인 윤리적 머신을 구현하기 위한 다양한 기술을 제공한다. 아래 에 자세히 설명된 것처럼 이러한 기술은 목표별(goal-specific) 리워드 함수를 인코딩하는 마르코프 결정 프로 세스(Markov decision processes)를 사용하는 운영 시나리오를 나타낸다. 동작(operation) 동안 초기 리워드 함수를 인코딩하는 마르코프 결정 프로세스를 사용하여 현재 시나리오가 표현될 수 있다. 현재 시나리오와 유 사한 적어도 하나의 이전 시나리오가 식별될 수 있으며, 유사한 시나리오(들)와 연관된 적어도 하나의 정책도 식별될 수 있다. 하나 이상의 유사한 시나리오와 연관된 하나 이상의 정책이 현재 시나리오에 적용될 수 있으 며, 이는 하나 이상의 유사한 시나리오와 연관된 하나 이상의 정책을 기반으로 현재 시나리오와 연관된 초기 리 워드 함수를 수정하는 것을 포함한다. 이로 인해 현재 시나리오에 대한 새로운 정책이 생성되며, 여기서 새로 운 정책은 현재 시나리오에 대한 수정된 리워드 함수를 포함한다. 그런 다음 현재 시나리오에서 취해야 할 조 치 과정을 식별하기 위해 현재 시나리오에 대한 새로운 정책이 사용될 수 있다. 윤리적 결정의 특징에 대한 지 속적인 러닝을 지원하기 위해 한 명 이상의 인간 운영자와의 상호 작용이 발생할 수도 있다. 예를 들어, 주어 진 시나리오에서 선택된 조치 과정에 대한 하나 이상의 머신 생성 정당화(machine-generated justifications)는 머신의 \"도덕적 성숙도(moral maturity)\" 수준을 추정하기 위해 하나 이상의 인간의 정당화(human justifications)에 대해(against) 평가될 수 있다. 이러한 방식으로, 유사한 이전 시나리오 형태의 이전 경험은 새로운(이전에 조사되지 않은(previously- unexamined)) 시나리오에 대한 애플리케이션에 대한 유사성(analogies) 및 유사점(parallels)을 도출하기 위해 통계적 기술을 사용하는 등을 통해 수집될 수 있다. 결과적으로, 이러한 기술은 알고리즘이 이전에 훈련되지 않은 새로운 시나리오와 관련된 결정을 내리려고 할 때 ML/AI 알고리즘이 다른 시나리오로부터 학습하도록 설계 되도록 할 수 있다. 유사한 이전 시나리오는 현재 시나리오에 대한 가드레일 역할을 하며 현재 시나리오에 대 해 선택된 조치 과정이 심각하게 부정확하지 않도록(not catastrophically incorrect) 보장하는 데 도움이된다. 근본적으로 이는 ML/AI 알고리즘이 일관되고 투명한 방식으로 윤리를 식별하고 추론할 수 있도록 하는 프레임워크를 제공한다. 또한 이러한 기술은 자율 또는 반자율 시스템이 어떻게 주어진 결론에 도달하고 주어 진 시나리오에 대한 특정 조치 과정을 선택하는지 명확하게 식별하기 위해 사용될 수 있다. 이는 자율 또는 반 자율 시스템의 추적 가능하고(traceable) 신뢰 가능하며 통제 가능한(governable) 동작을 지원하는 데 도움이 될 수 있다. 또한 이러한 기술을 통해 엔지니어, 설계자, 운영자 및 기타 직원은 시스템에서 자율성이 어떻게 사용되는지, 그리고 자율성에 대한 의존도가 과도한지 또는 낮은지 여부에 대한 통찰력을 얻을 수 있다. 또한, 자율 및 반자율 시스템의 투명하고 신뢰할 수 있으며 윤리적으로 명시적인 동작은 다양한 애플리케이션에 대한 시스템의 채택을 높이는 데 도움이 될 수 있다. 이 기능(functionality)의 특정 예로서, 자율 차량 제어 시스템은 그 신뢰성(reliability)에 대한 95% 신뢰 (confidence)를 입증하기 위해 50억 마일 이상의 동작을 요구할 수 있다. 여기의 접근 방식을 사용하면, 희귀 한 이벤트가 시뮬레이션 될 수 있으며 유사한 시나리오의 이전 이벤트가 자율 차량 제어 시스템이 어떻게 반응 할지 결정하기 위해 사용될 수 있다. 무엇보다도 이를 통해 자율 차량 제어 시스템 또는 기타 자율/반자율 시 스템이 검증 및 보증 평가를 자체 평가하고 가속화하도록 할 수 있다. 또한 이를 통해 자율 차량 제어 시스템 또는 기타 자율/반자율 시스템과 연관된 엔지니어, 설계자, 운영자 또는 기타 인력은 현재 시나리오가 임의의 윤리적 고려사항을 포함하지 않는 것처럼 보이더라도 이전 시나리오를 기반으로 가능한 조치 과정의 윤리적 영 향을 예측할 수 있도록 할 수 있다. 도 1은 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 예시적인 아키텍처 를 도시한다. 도 1에 도시된 바와 같이, 아키텍처는 작업 기반 및 윤리적 결정 모두에 대해 동시에 추론하기(reason) 위해 적어도 하나의 ML/AI 알고리즘을 훈련하는 데 사용될 수 있는 데이터베이스 및 다양한 동작(102-108)을 포함한다. 여기에서 동작(102-108)은 반복적으로 수행될 수 있으며, 이는 점점 더 복잡한 추론(reasoning)을 수행하기 위해 ML/AI 알고리즘이 반복적으로 훈련될 수 있도록 한다. 이는 이 상적으로 ML/AI 알고리즘이 점점 더 복잡한 작업 기반 및 윤리적 의사 결정 문제를 다룰 수 있도록 한다. 동작은 일반적으로 ML/AI 알고리즘에 의해 평가되는 현재 시나리오와 연관된 초기 정책을 식별, 생성 또는 획득하는 것을 포함하는 정책 식별 동작을 나타낸다. 일부 실시예에서, 현재 시나리오는 가능한 상태 세 트(S), 가능한 조치 세트(A), 및 조치로 인한 상태 간 전환(transitioning)에 대한 리워드를 정의하는 리워드 함수(R)를 포함하는 마르코프 결정 프로세스(MDP)를 사용하여 표현될 수 있다. 초기 정책은 지정된 상태에 있 을 때 취할 조치를 지정하는 초기 함수를 나타낸다. 초기 정책은 일반적으로 리워드 함수에 의해 정의된 대로 최적화되는 하나 이상의 파라미터의 초기 세트를 기반으로 한다. 마르코프 결정 프로세스와 정책의 조합은 프 로세스의 각각의 상태에 대한 조치를 식별하며, 이상적으로는 정책이 리워드의 기능(function of the rewards) 을 최대화하도록 선택될 수 있다. 특정 실시예에서, 현재 시나리오를 나타내는 마르코프 결정 프로세스 및 최 적화되는 하나 이상의 파라미터를 식별하는 초기 리워드 함수가 하나 이상의 사용자로부터 획득될 수 있다. 동작은 일반적으로 ML/AI 알고리즘에 의해 평가되는 현재 시나리오와 연관된 초기 정책과 일부 측면 에서 유사한 하나 이상의 이전 시나리오를 식별하는 것을 포함하는 유사성 식별 동작을 나타낸다. 이전 시나리 오는 이전 시나리오의 마르코프 결정 프로세스, 이전 시나리오의 최적화되는 파라미터 세트 또는 둘 다의 측면 에서 초기 정책과 유사하다. 임의의 유사한 이전 시나리오의 식별은 적절한 통계적 상관 기술(statistical correlation technique)을 사용하는 등의 임의의 적절한 방식으로 발생할 수 있다. 동작은 현재 시나리오 와 유사한 하나 이상의 이전 시나리오와 관련된 적절한 정보를 출력할 수 있다. 이 예에서, 동작은 이전 시나리오에 대한 다양한 정보를 저장할 수 있는 데이터베이스의 사용을 포함 할 수 있다. 데이터베이스의 정보는 ML/AI 알고리즘(또는 다른 ML/AI 알고리즘)에 의해 직면하게 된 실제 이전 시나리오에 대한 정보를 포함할 수 있다. 데이터베이스의 정보는 주어진 환경에서 직면하게 될 수 있는 시나리오와 같은 시뮬레이션된 시나리오에 대한 정보를 추가적으로 또는 대안적으로 포함할 수 있다. 데이터베이스는 이전 시나리오의 마르코프 결정 프로세스, 리워드 함수 및 이전 시나리오 정책의 기타 정 보와 같은 각각의 이전 시나리오에 대한 임의의 적절한 정보를 저장할 수 있다. 동작의 출력은 각각의 유 사한 이전 시나리오의 정책 또는 데이터베이스에 포함된 기타 정보를 포함할 수 있다. 동작은 일반적으로 현재 시나리오와 연관된 초기 정책 및 각각의 유사한 이전 시나리오와 연관된 정책 간 의 차이(differences)(충돌(conflicts))를 식별하는 것을 포함하는 충돌 식별 동작을 나타낸다. 예를 들어, 현 재 시나리오와 관련된 초기 정책 및 유사한 이전 시나리오와 연관된 정책은 비슷한(similar) 문제와 관련될 수 있지만 최적화되는 다른 매개변수 세트를 포함할 수 있다(따라서 다른 리워드 함수를 가짐). 따라서 여기서의동작은 현재 시나리오에 적용될 수 있는 하나 이상의 유사한 시나리오에 사용되는 추론(reasoning)을 식별 할 수 있다. 동작은 유사한 리워드 함수의 형태와 같이, 그 추론을 식별하는 정보를 출력할 수 있다. 일부 실시예에서, 동작은 관찰된 행동에 기초하여 리워드 함수를 추출하는 프로세스인 역 강화 러닝(IRL) 의 사용을 포함한다. 즉, 동작은 여기서 IRL을 사용하여 하나 이상의 유사한 이전 시나리오에서 발생하는 행동에 기초하여 리워드 함수를 식별할 수 있다. 그 다음, 동작은 평가되는 현재 시나리오에 대해 유사한 리워드 함수를 생성할 수 있다. 여기서 유사한 리워드 함수는 기존(original) 리워드 함수에 포함되지 않은 하 나 이상의 파라미터를 포함하여 및/또는 기존 리워드 함수에 포함되었던 하나 이상의 파라미터를 생략하여 최적 화되는 다양한 파라미터를 포함할 수 있다. 동작은 일반적으로 임의의 차이를 식별하기 위해 초기 리워드 함수를 유사한 리워드 함수와 비교하는 것을 포함하는 리워드 함수 수정 동작을 나타낸다. 그런 뒤에 동작은 상기 비교에 기초하여 새로운 리워드 함 수를 생성할 수 있다. 예를 들어, 동작은 유사한 리워드 함수에 최적화된 하나 이상의 특정 파라미터가 기존 리워드 함수에 포함되지 않았으며 포함되어야 한다고 결정할 수 있다. 따라서 동작은 하나 이상의 특정 파라미터를 포함하기 위해 업데이트된 새로운 리워드 함수를 생성 및 출력할 수 있다. 평가되는 현재 시 나리오와 연관된 업데이트된 정책을 형성하기 위해 새로운 리워드 함수가 사용될 수 있다. 이 시점에서, 업데이트된 정책이 수용 가능(acceptable)한지 확인하기 위해 선택적으로 인간의 상호 작용이 발 생할 수 있다. 예를 들어, 업데이트된 정책은 현재 시나리오에 대해 제안된 조치 과정(proposed course of action)을 식별하는 데 사용하기 위해 ML/AI 알고리즘에 제공될 수 있으며, 한 명 이상의 사람은 제안된 조치 과정이 수용 가능한지 여부를 평가할 수 있다. 업데이트된 정책은 또한 현재 시나리오에 대한 다른 정보 와 함께 데이터베이스에 저장될 수 있으며, 이는 업데이트된 정책이 후속 시나리오와 함께 사용될 수 있도 록 한다. 업데이트된 정책은 여기에 표시된 프로세스의 또 다른 반복(iteration)을 통해 추가로 피드백 될 수 있으며, 이를 통해 정책의 반복적인(iterative) 업데이트가 가능하다. 이는 이전 시나리오의 수가 증가함에 따 라 더 복잡한 상황이 학습될 수 있도록 한다. 전반적으로, 이 아키텍처는 반복에 더 많은 파라미터를 포함함으로써와 같이, 점점 더 복잡한 추론을 기반 으로 서로 다른 리워드 함수를 반복적으로 적응시킬 수 있는 머신 러닝 프레임워크를 제공한다. 이는 콜버그 (Kohlberg)의 도덕 발달 이론(theory of moral development)에서 정의한 인간의 도덕 발달과 일반적으로 일치 (consistent)할 수 있다. 콜버그 이론은 일반적으로 도덕 발달이 6단계로 발생하는 것으로 모델링한다: 1단계 - 처벌 및 복종 지향(처벌을 피하기 위해 규칙에 복종) 2단계 - 도구-상대주의(Instrumental-Relativist) 지향(리워드를 받거나 호의를 얻기 위해 순응(conform)) 3단계 - 착한 소년/소녀 지향(타인의 불승인(disapproval)을 피하기 위해 순응) 4단계 - 법과 질서 지향(당국의 처벌을 피하기 위해 순응) 5단계 - 사회 계약 지향(커뮤니티 유지를 위해 순응) 6단계 - 보편적인 윤리 원칙 지향(결정이 다른 사람들에게 어떻게 영향을 미치는지 고려) 첫 번째 및 두 번째 단계를 자주 \"전인습적(pre-conventional)\", 세 번째와 네 번째 단계를 \"인습적 (conventional)\", 다섯 번째와 여섯 번째 단계를 자주 \"후인습적(post-conventional)\"이라고 한다. 점점 더 복잡해지는 추론을 수행하는 아키텍처의 능력은 ML/AI 알고리즘의 능력이 시간이 지남에 따라 이러한 유형의 모델과 일치하는 윤리적 결정을 내리는 데 더욱 효과적이게 되도록 지원한다(물론, 이 원리를 설명하기 위해 다른 모델이 사용될 수도 있음). ML/AI 알고리즘의 결정과 그 결정의 근거(bases)를 인간의 정당화 와 비교함으로써, ML/AI 알고리즘의 도덕적 성숙도 수준을 평가하는 것이 가능하다. 더욱이, 여기서 아키텍처는 이전 경험으로부터 추가적인 파라미터를 식별하기 위해 유비 추론(analogical reasoning)(아마도 휴리스틱(heuristics)과 함께)의 사용을 지원하며, 선택된 조치의 윤리적 정당화 또는 설명 이 제공될 수 있다. 이는 ML/AI 알고리즘에 의한 투명하고 신뢰할 수 있으며 윤리적으로 명시적인 동작의 사용을 지원하는 데 도움이 된다. 또한, 아키텍처는 ML/AI 알고리즘의 선택된 조치를 단순히 평가하 는 것이 아니라 ML/AI 알고리즘의 도덕적 추론을 평가하기 위한 프레임워크를 제공한다. 이는 \"도둑질하 지 마십시오\"의 결과가 낮은 도덕적 발달(처벌을 피하기 위해) 및 높은 도덕적 발달(보편적인 윤리 원칙을 고수 하기 위해)에서 비롯된 경우와 같이, 동일한 관찰 가능한 결과가 서로 다른 도덕적 발달 수준에서 나타날 수 있 기 때문에 유용하다. 아키텍처는 ML/AI 알고리즘이 \"올바른(correct)\" 조치 과정을 선택했는지 여부를 단순히 평가하는 것이 아니라 ML/AI 알고리즘이 선택된 조치 과정을 선택한 이유를 결정하거나 설명하 는 능력을 제공한다. 이는 ML/AI 알고리즘에 의한 치명적인 오류를 줄이거나 제거하는 데 도움이 될 수 있다. 또한, 여기의 아키텍처는 완전히 규칙 기반(rule-based)이 아닌 명시적인 윤리적 에이전트를 훈련 하기 위해 사용될 수 있으며, 이는 암시적(implicit) 에이전트와 연관된 함정을 피하는 데 도움이 된다. 예를 들어, 명시적인 윤리적 에이전트는 스케일링 가능(scalable)하며, 모든 가능한 시나리오를 예상하기 위해 설계 자에게 의존하지 않을 수 있으며(따라서 인간이 예측하지 못한 위험을 지원하는 데 도움이 됨), 그러한 결정을 정당화하기 위해 결정에 대한 구체적인 설명을 제공할 수 있다. 도 1을 참조하여 전술한 동작(102-108)은 임의의 적절한 방식으로 하나 이상의 장치에서 구현될 수 있다는 점에 유의한다. 예를 들어, 일부 실시예에서, 동작(102-108)은 전용 하드웨어 또는 하드웨어와 소프트웨어/펌웨어 명령어의 조합을 사용하여 구현될 수 있다. 또한, 일부 실시예에서, 동작(102-108)은 하나 이상의 기능 (functions)을 수행하기 위해 하나 이상의 ML/AI 알고리즘을 사용하는 시스템과 같은, 더 큰 시스템에 내 장된 하드웨어 또는 하드웨어 및 소프트웨어/펌웨어 명령어를 사용하여 구현될 수 있다. 그러나 본 개시는 동 작(102-108)의 임의의 특정 물리적 구현에 제한되지 않는다. 도 1은 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 아키텍처의 일 예를 도시하지 만, 도 1에 다양한 변경이 이루어질 수 있다. 예를 들어, 아키텍처에 의해 수행되는 프로세스의 임의의 적절한 반복 횟수가 발생할 수 있다. 또한, 아키텍처는 현재 시나리오와 유사한 이전 시나리오가 식별되 지 않는 경우 아키텍처가 동작(106-108)을 건너뛸 수 있는 경우와 같이, 각각의 반복에서 모든 동작(102- 108)을 반드시 수행할 필요는 없다. 도 2는 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 지원하는 예시적인 장치를 도 시한다. 장치의 하나 이상의 인스턴스는, 예를 들어, 도 1에 도시된 동작(102-108)을 적어도 부분적으로 구현하는 데 사용될 수 있다. 그러나 동작(102-108)은 임의의 다른 적절한 방식으로 구현될 수 있다. 일부 실 시예에서, 도 2에 도시된 장치는 데스크탑, 랩탑, 서버 또는 태블릿 컴퓨터와 같은 컴퓨팅 시스템의 적어 도 일부를 형성할 수 있다. 그러나, 임의의 다른 적절한 장치 또는 장치들이 동작(102-108)을 수행하는데 사용 될 수 있다. 도 2에 도시된 바와 같이, 장치는 적어도 하나의 처리 장치, 적어도 하나의 저장 장치, 적어도 하나의 통신 유닛 및 적어도 하나의 입력/출력(I/O) 유닛을 포함하는 컴퓨팅 장치를 나타낸다. 처리 장치는 메모리에 로드될 수 있는 명령어를 실행할 수 있다. 처리 장치는 임의의 적절한 수(들) 및 유형(들)의 임의의 적절한 배열의 프로세서 또는 다른 처리 장치를 포함한다. 처리 장치의 예시적인 유형은 하나 이상의 마이크로프로세서, 마이크로컨트롤러, 디지털 신호 프로세서(DSP), 주문형 집적 회로 (ASIC), 필드 프로그래밍 가능 게이트 어레이(FPGA), 또는 개별 회로를 포함한다. 메모리 및 영구 저장소는 저장 장치의 예이며, 이는 정보(데이터, 프로그램 코드 및/또는 임시 적이거나 영구적인 기타 적절한 정보 등)를 저장하고 검색을 용이하게 할 수 있는 임의의 구조(들)를 나타낸다. 메모리는 랜덤 액세스 메모리 또는 임의의 다른 적절한 휘발성 또는 비휘발성 저장 장치(들)를 나타낼 수 있다. 영구 저장소는 읽기 전용 메모리, 하드 드라이브, 플래시 메모리 또는 광 디스크와 같이 데이터의 장기 저장을 지원하는 하나 이상의 구성요소 또는 장치를 포함할 수 있다. 통신 유닛은 다른 시스템 또는 장치와의 통신을 지원한다. 예를 들어, 통신 유닛은 유선 또는 무선 네트워크 또는 직접 연결을 통한 통신을 용이하게 하는 무선 트랜시버 또는 네트워크 인터페이스 카드를 포함할 수 있다. 통신 유닛은 임의의 적절한 물리적 또는 무선 통신 링크(들)를 통한 통신을 지원할 수 있다. I/O 유닛은 데이터의 입력 및 출력을 허용한다. 예를 들어, I/O 유닛은 키보드, 마우스, 키패드, 터 치스크린, 또는 다른 적절한 입력 장치를 통한 사용자 입력에 대한 연결을 제공할 수 있다. I/O 유닛은 또한 디스플레이 또는 다른 적절한 출력 장치에 출력을 보낼 수 있다. 그러나, 장치가 원격으로 액세스될 수 있는 서버 또는 다른 장치를 나타내는 경우와 같이 장치가 로컬 I/O를 요구하지 않는 경우 I/O 장치 는 생략될 수 있다는 점에 유의한다. 일부 실시예에서, 처리 장치에 의해 실행되는 명령어는 위에서 설명된 동작(102-108)을 구현하는 명령어를 포함한다. 따라서, 예를 들어 처리 장치에 의해 실행되는 명령어는 처리 장치가 현재 시나리오에 대 한 초기 정책을 획득하고 현재 시나리오와 유사한 임의의 이전 시나리오를 식별하도록 할 수 있다. 처리 장치 에 의해 실행되는 명령어는 또한 처리 장치가 유사한 리워드 함수를 식별하고 현재 시나리오에 대한업데이트된 정책 및 새로운 리워드 함수를 생성하도록 할 수 있다. 업데이트된 정책은 현재 시나리오에 대해 ML/AI 알고리즘에 의해 선택된 조치 과정을 식별하기 위해 ML/AI 알고리즘에 제공될 수 있다. 업데 이트된 정책은 다른 적절한 방식으로 사용될 수도 있다. 도 2는 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 지원하는 장치의 일 예를 도시하지만, 도 2에 다양한 변경이 이루어질 수 있다. 예를 들어, 컴퓨팅 및 통신 장치와 시스템은 매우 다양한 구성으로 제공되며, 도 2는 본 개시를 임의의 특정 컴퓨팅 또는 통신 장치 또는 시스템으로 제한하지 않는다. 도 3은 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 단일 에이전트 아키 텍처의 예시적인 사용을 도시한다. 여기서 단일 에이전트 아키텍처는 아키텍처의 단일 인스턴 스를 사용하여 적어도 하나의 ML/AI 알고리즘의 사용을 지원한다. 도 3에 도시된 바와 같이, 현재 시나리 오와 관련된 입력은 아키텍처의 단일 인스턴스의 정책 식별 동작에 제공된다. 여기의 입력은 마르코 프 결정 프로세스(MDPoriginal)(302a) 및 기존의 리워드 함수(Roriginal)(302b)로 표현될 수 있는 최적화하는 하나 이상의 파라미터의 초기 세트를 포함한다. 입력은 사용자와 같은, 임의의 적절한 소스(들)로부터 및 임의의 적 절한 방식으로 획득될 수 있다. 정책 식별 동작은 이 정보를 포함하는 초기 정책을 유사성 식별 동작에 제공하는데, 이는 현재 시나 리오와 유사한 임의의 이전 시나리오를 식별하기 위해 데이터베이스에 액세스한다. 유사성 식별 동작 은 하나 이상의 유사한 이전 시나리오와 연관된 하나 이상의 정책을 나타내는 하나 이상의 유사한 정책(π analog)을 출력할 수 있다. 위에서 언급한 바와 같이, 하나 이상의 유사한 이전 시나리오의 식별은 이전 시 나리오와 연관된 정보와 현재 시나리오의 정보의 통계적 상관관계(statistical correlation)를 통해서와 같이, 임의의 적절한 방식으로 발생할 수 있다. 충돌 식별 동작은 하나 이상의 유사한 정책이 기존 리워드 함수(302b)에 의해 최적화되지 않은 임의 의 파라미터를 최적화하는지 여부를 식별할 수 있다. 충돌 식별 동작은 또한 또는 대안적으로 하나 이상 의 유사한 정책이 기존 리워드 함수(302b)에 의해 최적화된 임의의 파라미터를 최적화하지 않는지 여부를 식별할 수 있다. 일부 실시예에서, 충돌 식별 동작은 기존 리워드 함수(302b)에 의해 최적화되거나 최적 화되지 않은(또는 그 반대) 유사한 이전 시나리오와 연관된 임의의 파라미터를 식별하기 위해 마르코프 결정 프 로세스(302a) 및 하나 이상의 유사한 정책을 사용하여 IRL을 수행할 수 있다. 결과는 유사한 리워드 함수 (Ranalog)로서 충돌 식별 동작으로부터 출력될 수 있다. 리워드 함수 수정 동작은 기존 리워드 함수(302b)와 유사한 리워드 함수 사이의 차이를 조사한다. 이에 기초하여, 리워드 함수 수정 동작은 최적화를 위해 기존 리워드 함수(302b)에 삽입될 수 있는 유사한 이전 시나리오로부터 하나 이상의 파라미터를 결정하고/결정하거나 리워드 함수 수정 동작은 기존 리워드 함수(302b)에서 제거될 수 있는 기존 리워드 함수(302b)의 하나 이상의 파라미터를 결정한다. 이는 현재 시나 리오에 대한 리워드 함수를 수정함으로써 현재 시나리오를 이전 시나리오에 대해 유추(analogize)하는 데 도움 이 된다. 리워드 함수 수정 동작은 새로운 리워드 함수(Rnew)를 생성하기 위해 이러한 방식으로 기존 리워드 함수(302b)를 업데이트할 수 있다. 정책 식별 동작은 마르코프 결정 프로세스(302a)를 새로운 리 워드 함수와 결합하여 새로운 정책(πnew)을 생성할 수 있으며, 이는 (이상적으로는) 하나 이상의 이 전 시나리오로부터 하나 이상의 유사성을 통합함으로써 수정된 초기 정책을 나타낸다. 새로운 정책은 임의의 적절한 방식으로 사용될 수 있다. 예를 들어, 새로운 정책은 프로세스의 또 다른 반복을 위해 정책 식별 동작으로부터 유사성 식별 동작으로 제공될 수 있다(다음 반복의 Roriginal 은 이전 반복으로부터의 Rnew를 나타냄). 새로운 정책은 다른 입력 정책에 대한 유추사성을 찾는데 사용하 기 위해 데이터베이스에 저장될 수 있다. ML/AI 알고리즘이 현재 시나리오에 대해 제안된 조치 과정 을 식별할 수 있도록 새로운 정책이 ML/AI 알고리즘에 제공될 수 있다. 일부 경우에, 제안된 조치 과정은 새로운 정책이 올바른 이유(들)에 대해 올바른 조치 과정이 선택되도록 허용하는지 여부를 결정하 기 위해 인간의 정당화와 비교하여 평가될 수 있다. 도 3에 도시된 단일 에이전트 접근 방식(single-agent approach)은 다음 예와 같이, 다양한 애플리케이션에 사 용될 수 있다. 아래의 예는 단지 설명을 위한 것이며 본 개시의 범위를 아래에 설명된 특정 예 또는 예의 유형 으로 제한하지 않는다는 점에 유의한다. 하나의 애플리케이션에서, ML/AI 알고리즘은 다수의 환자 중 투석을 받아야 하는 환자를 선택하도록 훈련 될 수 있다. 아키텍처는 입력(302a-302b)으로서 주어진 지역에서 이용 가능한 투석 기계의 수의 표시 (indication), 주어진 지역에서 투석을 받아야 하는 환자의 목록(인구통계학적 정보 포함), 및 선택이 환자 연 령에 맞춰 최적화되어야 한다는 표시를 수신할 수 있다. 통계적 상관관계 또는 다른 기술을 사용하여, 아키텍 처는 투석을 위한 환자를 선택하는 것이 신장 이식을 위한 환자를 선택하는 것과 비슷한 특성을 갖고 있으 며 이전 시나리오가 신장 이식을 위한 환자를 선택하는 것과 관련하여 정의되었음을 결정할 수 있다. 신장 이 식을 위한 환자 선택과 관련된 정책은 유사한 정책으로 선택될 수 있으며, 아키텍처는 유사한 정책 을 분석하고 유사한 정책이 기존의 리워드 함수(302b)에 포함되지 않고 기존의 리워드 함수(302b)에 의해 최적화되는 2개의 파라미터(흡연 및 약물 사용)를 포함한다고 결정할 수 있다. 따라서 이러한 파라미터는 유사한 리워드 함수에 포함될 수 있고, 다른 또는 추가적인 파라미터가 포함될 수 있거나 기존의 리워드 함수(302b)로부터의 일부 기존(existing) 파라미터가 유사한 리워드 함수에서 제외될 수 있다. 그러면 아 키텍처는 새로운 리워드 함수가 유사한 정책으로부터의 파라미터(흡연 및 약물 사용) 중 하나 또는 둘 다를 포함하는 경우와 같이, 기존의 리워드 함수(302b) 및 유사한 리워드 함수에 기초한 새로운 리워드를 생성할 수 있다. 새로운 리워드 함수는 새로운 정책을 생성하는 데 사용되며, ML/AI 알고 리즘은 여러 환자 중 투석을 받아야 하는 환자를 선택하는 문제에 새로운 정책을 적용할 수 있다. ML/AI 알고리즘은 또한 유사한 정책이 선택되었고 유사한 정책으로부터의 2개의 파라미터가 새 로운 정책에 포함되었음을 나타내는 정보를 출력하는 등의 선택된 환자가 투석을 위해 선택된 이유에 대한 설명을 제공할 수 있다. 다른 애플리케이션에서, ML/AI 알고리즘은 특정 지역에서 병원이 어디에 지어질지 결정하도록 훈련될 수 있다. 아키텍처는 입력(302-302b)으로 지어지는 병원 수의 표시, 특정 지역의 인구 통계 및 상기 결정이 인구 규모에 기초하여 최적화되어야 한다는 표시를 수신할 수 있다. 통계적 상관관계 또는 다른 기술을 사용하 여, 아키텍처는 병원이 어디에 지어질 것인지를 식별하는 것이 학교가 어디에 지어질 것인지를 식별하는 것과 비슷한 특성을 갖고 있으며, 이전 시나리오가 학교가 어디에 지어지는 것인지를 식별하는 것과 관련하여 정의되었음을 결정할 수 있다. 학교가 어디에 지어질 것인지를 식별하는 것과 관련된 정책은 유사한 정책(30 4)으로 선택될 수 있고, 아키텍처는 유사한 정책을 분석하고 유사한 정책이 기존의 리워드 함수 (302b)에 포함되지 않고 기존의 리워드 함수(302b)에 의해 최적화되는 하나의 파라미터(다른 도시에 대한 연결 성(connectivity))를 포함한다고 결정할 수 있다. 따라서 이 파라미터는 유사한 리워드 함수에 포함될 수 있고, 다른 또는 추가적인 파라미터가 포함될 수 있거나 기존의 리워드 함수(302b)로부터의 일부 기존 (existing) 파라미터가 유사한 리워드 함수에서 제외될 수 있다. 그러면 아키텍처는 새로운 리워드 함수가 유사한 정책으로부터의 새로운 파라미터를 포함하는 경우와 같이, 기존의 리워드 함수(302b) 및 유사한 리워드 함수에 기초한 새로운 리워드 함수를 생성할 수 있다. 새로운 리워드 함수는 새로운 정책을 생성하는 데 사용될 수 있고, ML/AI 알고리즘은 병원이 어디에 지어지는지 식별하는 문제에 새로운 정책을 적용할 수 있다. ML/AI 알고리즘은 또한 유사한 정책이 선택되었고 유사 한 정책으로부터의 파라미터가 새로운 정책에 포함되었음을 나타내는 정보를 출력하는 등의 병원에 대해 식별된 위치가 선택된 이유에 대한 설명을 제공할 수 있다. 도 4는 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 멀티 에이전트 아키 텍처(multi-agent architecture)의 예시적인 사용을 도시한다. 멀티 에이전트 아키텍처는 여기서 아키텍처의 다수의 인스턴스를 사용하여 적어도 하나의 ML/AI 알고리즘의 사용을 지원하지만, 아키텍 처의 다수의 인스턴스는 동일한 데이터베이스를 공유할 수 있다(그러나, 이것이 필수는 아니라는 점 에 유의한다). 아키텍처의 서로 다른 인스턴스는 작업이 수행되는 방식의 측면에서 서로 다른 편견이나 선호를 가질 수 있다. 도 4에 도시된 바와 같이, 현재 시나리오와 관련된 입력은 아키텍처의 각각의 인스 턴스의 각각의 정책 식별 동작에 제공된다. 여기서 입력은 마르코프 결정 프로세스(402a) 및 기존의 리워 드 함수(402b)로 표현될 수 있는, 최적화하기 위한 하나 이상의 파라미터의 초기 세트(initial set)를 포함한다. 입력은 사용자와 같은, 임의의 적절한 소스(들)로부터 임의의 적절한 방식으로 획득될 수 있다. 각각의 정책 식별 동작은 현재 시나리오와 유사한 임의의 이전 시나리오를 식별하기 위해 데이터베이스 에 액세스하는, 연관된 유사성 식별 동작에 이 정보를 제공한다. 각각의 유사성 식별 동작에 의해 액세스되는 이전 시나리오는 아키텍처의 특정 인스턴스에만 관련될 수 있거나, 이전 시나리오는 아키 텍처의 다수의 인스턴스에 걸쳐 공유될 수 있다는 점에 유의한다. 각각의 유사성 식별 동작은 하나 이상의 유사한 이전 시나리오와 연관된 하나 이상의 정책을 나타내는, 하나 이상의 유사한 정책을 출력할 수 있다. 위에서 언급한 바와 같이, 하나 이상의 유사한 이전 시나리오의 식별은, 현재 시나리오의 정보와 이전 시나리오와 연관된 정보의 통계적 상관관계를 통해서와 같이, 임의의 적절한 방식으로 발생할 수 있다. 각각의 충돌 식별 동작은 하나 이상의 유사한 정책이 연관된 기존의 리워드 함수(402b)에 의해 최적 화되지 않은 임의의 파라미터를 최적화하는지 여부를 식별할 수 있다. 각각의 충돌 식별 동작은 하나 이 상의 유사한 정책이 기존의 리워드 함수(402b)에 의해 최적화된 임의의 파라미터를 최적화하지 않는지 여 부를 또한 또는 대안적으로 식별할 수 있다. 일부 실시예에서, 각각의 충돌 식별 동작은 기존의 리워드 함수(402b)에 의해 최적화되거나 최적화되지 않은(또는 그 반대) 유사한 이전 시나리오와 연관된 임의의 파라미 터를 식별하기 위해 마르코프 결정 프로세스(402a) 및 하나 이상의 유사한 정책을 사용하여 IRL을 수행할 수 있다. 결과는 각각의 충돌 식별 동작으로부터 유사한 리워드 함수로서 출력될 수 있다. 각각의 충돌 식별 동작은 자신의 유사한 리워드 함수를 출력할 수 있거나, 다수의 충돌 식별 동작이 공 통의(common) 유사한 리워드 함수를 공유할 수 있다는 점에 유의한다. 각각의 리워드 함수 수정 동작은 그의 기존의 리워드 함수(402b) 및 그의 유사한 리워드 함수 사이의 차이를 조사한다. 이를 기반으로, 각각의 리워드 함수 수정 동작은 그의 유사한 이전 시나리오로부터 그 의 기존의 리워드 함수(402b)에 삽입될 수 있는 하나 이상의 파라미터를 결정하고/결정하거나 각각의 리워드 함 수 수정 동작은 그의 기존의 리워드 함수(402b)에서 제거될 수 있는 그의 기존의 리워드 함수(402b)의 하 나 이상의 파라미터를 결정한다. 이는 현재 시나리오에 대한 각각의 리워드 함수를 수정하여 현재 시나리오를 이전 시나리오(들)와 유추하는 데 도움이 된다. 각각의 리워드 함수 수정 동작은 이러한 방식으로 그의 기존의 리워드 함수(402b)를 업데이트하여 새로운 리워드 함수를 생성할 수 있다. 각각의 정책 식별 동작 은 마르코프 결정 프로세스(402a)를 그의 새로운 리워드 함수와 결합하여 새로운 정책(πnew)을 생성 할 수 있으며, 이는 (이상적으로) 하나 이상의 이전 시나리오로부터 하나 이상의 유사성을 통합하여 수정된 초 기 정책을 나타낸다. 아키텍처의 다수의 인스턴스로부터의 새로운 정책은 평가 동작에 제공될 수 있으며, 이는 아키 텍처의 다수의 인스턴스가 조치 과정이 선택되는 방법의 측면에서 합의(consensus)에 도달하는지 여부를 결정하기 위해 새로운 정책을 평가한다. 여기서 합의는 동일한 조치 과정을 식별하는 정책을 생성하 는 아키텍처의 모든 인스턴스, 동일한 조치 과정을 식별하는 정책을 생성하는 아키텍처의 대다 수 또는 다른 지정된 수/퍼센트의 인스턴스, 또는 임의의 다른 적절한 기준으로 정의될 수 있다. 아키텍처 의 다수의 인스턴스가 동일한 조치 과정을 선택하기 위해, 그러나 혹시라도(possibly) 다른 이유로 인해 사용될 수 있다는 점에 유의한다. 합의가 이루어지지 않으면, 평가 동작은 아키텍처의 하나 이상의 인스턴스에 의해 사용되는 기존의 리워드 함수(402b) 중 하나 이상을 조정할 수 있으며, 프로세스가 반복될 수 있다. 합의가 얻어지면, 새로운 정책은 ML/AI 알고리즘(들)에 대한 최종 정책 세트로서 사용될 수 있다. 새로운 정책은 임의의 적절한 방식으로 사용될 수 있다. 예를 들어, 새로운 정책은 프로세스의 또 다른 반복을 위해 정책 식별 동작으로부터 유사성 식별 동작으로 제공될 수 있다. 새로운 정책(41 4)은 다른 입력 정책에 대한 유사성을 찾는 데 사용하기 위해 데이터베이스에 저장될 수 있다. ML/AI 알 고리즘(들)이 현재 시나리오에 대해 제안된 조치 과정을 식별할 수 있도록, 새로운 정책이 ML/AI 알 고리즘(들)에 제공될 수 있다. 일부의 경우에, 제안된 조치 과정은 새로운 정책이 올바른 이유(들) 에 대해 올바른 조치 과정이 선택되도록 허용하는지 여부를 결정하기 위해 인간의 정당화와 비교하여 평가될 수 있다. 도 4에 도시된 멀티 에이전트 접근방식은 다음 예와 같이, 다양한 애플리케이션에 사용될 수 있다. 아래의 예 는 단지 설명을 위한 것이며 본 개시의 범위를 아래에 설명된 특정 예 또는 예의 유형으로 제한하지 않는다는 점에 유의한다. 하나의 애플리케이션에서, 적어도 하나의 ML/AI 알고리즘은 미사일 방어 시스템을 동작 시키고 미사일 방 어 시스템에 의해 교전 및 파괴될 들어오는(incoming) 미사일을 선택하도록 훈련될 수 있다. 서로 다른 위치들 이 들어오는 미사일의 타겟이 될 수 있으며, 각각의 위치는 아키텍처의 자체 인스턴스를 가질 수 있다. 아키텍처는 입력(402a-402b)으로서, 들어오는 미사일의 수에 대한 표시와 교전할 들어오는 미사일의 선택 이 민간인을 보호하기 위해 최적화되어야 한다는 표시를 수신할 수 있다. 통계적 상관관계 또는 다른 기술을 사용하여, 아키텍처의 각각의 인스턴스는 현재 시나리오와 관련하여 또다른 이전 시나리오가 정의되었음을 결정할 수 있다. 이전 시나리오와 관련된 정책은 유사한 정책으로 선택될 수 있으며, 아키텍처의 각 각의 인스턴스는 유사한 정책을 분석하고 유사한 정책이 그 기존 리워드 함수(402b)에 포함되지 않은하나 이상의 파라미터를 포함한다고 결정할 수 있다. 이들 파라미터는 각각의 유사한 리워드 함수에 포함 될 수 있고, 다른 또는 추가적인 파라미터가 포함될 수 있거나 각각의 기존 리워드 함수(402b)로부터의 일부 기 존의 파라미터가 연관된 유사한 리워드 함수에서 제외될 수 있다. 그러면 아키텍처의 각각의 인스턴 스는 그 기존 리워드 함수(402b)에 기초하여 새로운 리워드 함수를 생성할 수 있고, 새로운 리워드 함수 는 새로운 정책을 생성하기 위해 사용될 수 있으며, 새로운 정책은 합의를 찾기 위해 비교될 수 있다. 합의가 얻어진 경우, ML/AI 알고리즘(들)은 어떤 들어오는 미사일과 교전할지 선택하는 문제에 대 한 최종 정책 세트로서 새로운 정책을 적용할 수 있다. 합의가 얻어지지 않은 경우, 평가 동작(41 2)은 하나 이상의 기존 리워드 함수(402b)를 조정하고 합의가 얻어질 때까지(또는 다른 특정 기준(들)이 충족될 때까지) 프로세스를 반복할 수 있다. ML/AI 알고리즘(들)은 또한 프로세스를 설명하는 적절한 정보를 출 력하는 등의, 선택된 들어오는 미사일이 교전을 위해 선택된 이유에 대한 설명을 제공할 수 있다. 도 3 및 도 4는 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 단일 에이전트 및 멀티 에 이전트 아키텍처(300, 400)의 사용 예를 도시하지만, 도 3 및 도 4에 다양한 변경이 이루어질 수 있다. 예를 들어, 다수의 데이터베이스는 아키텍처(300 또는 400)에서 사용될 수 있으며, 하나 이상의 데이터베이스 는 아키텍처의 인스턴스(들)를 구현하는 장치(들)에 대해 로컬이거나 그(들)로부터 원격일 수 있다. 또한, 임의의 적절한 수의 아키텍처의 인스턴스가 멀티 에이전트 시스템에서 사용될 수 있고, 아키텍처 의 인스턴스로부터의 결과는 임의의 다른 적절한 방식으로 결합되거나 달리 사용될 수 있다. 도 5는 본 개시에 따른 유사한 시나리오를 사용하는 명시적 윤리 머신을 제공하기 위한 아키텍처에서 역 강화 러닝을 사용하는 예시적인 기술을 도시한다. 예를 들어, 도 5에 도시된 기술은 유사한 시나리오에 기초하여 리워드 함수를 개선(refine)하기 위해 동작에 의해 사용될 수 있다. 그러나 동작은 임의의 다른 적절한 방식으로 구현될 수 있다는 것에 유의한다. 도 5에 도시된 바와 같이, 기술은 시간이 지남에 따라 발생하는 일련의 작업(502a-502n)을 포함하며, 여기 서 작업(502a)이 먼저 발생하고 다른 작업(502b-502n)이 순차적으로 뒤따른다. 작업(502a-502n) 각각은 리워드 함수(504a-504n)의 버전과 각각 연관되며, 여기서 리워드 함수(504a-504n)는 시간이 지남에 따라 변화하는 공통 의 리워드 함수를 나타낼 수 있다. 여기서 리워드 함수(504a-504n)의 각각의 버전은 각각 작업 독립적 부분(task-agnostic portion)(506a-506n) 및 작업 종속적 부분(task-dependent portion)(508a-508n)을 포함하거나 이와 연관된다. 리워드 함수(504a- 504n)의 작업 독립적 부분(506a-506n)은 수행되는 작업에 관계없이 윤리적 경계를 시행하는(enforces) 리워드 시스템을 정의하는 데 도움을 주기 위해 사용될 수 있다. 리워드 함수(504a-504n)의 작업 종속적 부분(508a- 508n)은 리워드 시스템의 맥락적 행동(contextual behavior)을 구동하는 리워드 시스템을 정의하는 데 도움이 될 수 있다. 각각의 리워드 함수(504a-504n)는 여기서 개별적인 작업(502a-502n)과 연관된 데이터에 대한 리워 드 함수(504a-504n)의 애플리케이션을 나타내는 결과(510a-510n)를 생성하기 위해 사용될 수 있다. 각각의 리 워드 함수(504a-504n)의 작업 독립적 부분(506a-506n)과 작업 종속적 부분(508a-508n)은 연관된 결과(510a- 510n)를 생성하기 위해 사용될 수 있다. 여기서 라인은 시간이 지남에 따라 작업 독립적 부분(506a-506n)에 발생할 수 있는 윤리적 행동의 개선 (refinements)을 정의한다. 예를 들어, 이는 더 높은 수준의 도덕적 발전을 달성하기 위해 리워드의 윤리적 구 성요소의 점진적인 계승(inheritance) 및 개선을 허용한다(예: 콜버그 모델(Kohlberg model)의 단계로 올라가는 것). 여기서 라인은 시간이 지남에 따라 작업 종속적 부분(508a-508n)에 발생할 수 있는 유비 추론 및 전 이(transfer) 러닝을 정의한다. 예를 들어, 이는 데이터베이스를 사용하여 위에서 설명된 방식으로 발생 할 수 있는, 시간이 지남에 따라 러닝을 촉진하기 위해 이전 시나리오와의 유사성을 사용하는 것을 허용한다. 이전 시나리오와 유추하는 능력은 러닝을 늘리고 훈련 요구사항을 줄이는 데 도움이 될 수 있다. 여기서 베이 지안 공식은 불확실성 하에서 학습하고 조치(acting)하는 이론적으로 건전한 프레임워크를 제공할 수 있으며, 충돌하는 도덕적 규범 및 리워드에 대한 절충적(trade-off) 분석을 가능하게 할 수 있다. 도 5는 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 아키텍처에서 역 강화 러닝을 사용 하는 기술의 일 예를 도시하지만, 도 5에 다양한 변경이 이루어질 수 있다. 예를 들어, 리워드 함수는 그 작업 독립적 부분 및/또는 그 작업 종속적 부분의 변경에 기초하여 시간이 지남에 따라 다양한 방식으로 발전할 수 있다. 도 6은 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 예시적인 방법(60 0)을 도시한다. 설명의 편의를 위해, 방법은 아키텍처에 의해 수행되는 것으로 설명되며, 이는 도 2의 적어도 하나의 장치를 사용하여 구현될 수 있다. 그러나 방법은 임의의 적절한 장치(들) 및 임의 의 적절한 시스템(들)에 의해 수행될 수 있다. 도 6에 도시된 바와 같이, 단계에서 현재 시나리오 및 초기 리워드 함수를 정의하는 입력 정보가 획득되고, 단계에서 현재 시나리오에 대한 초기 정책을 생성하기 위해 사용된다. 이는 예를 들어, 사용자 또는 다른 소스(들)로부터 마르코프 결정 프로세스 및 기존 리워드 함수를 정의하는 정보를 수신하기 위해 정책 식별 동작을 수행하는 처리 장치를 포함할 수 있다. 이는 또한 현재 시나리오에 대한 초기 정책으로 서 마르코프 결정 프로세스 및 기존 리워드 함수를 사용하기 위해, 또는 현재 시나리오에 대한 초기 정책을 생 성하기 위해 정책 식별 동작을 수행하는 처리 장치를 포함할 수 있다. 기존 리워드 함수는 현재 시 나리오에 대한 조치 과정을 선택할 때 최적화될 하나 이상의 파라미터를 식별할 수 있다. 단계에서 현재 시나리오와 유사한 임의의 이전 시나리오에 대해 데이터베이스가 검색되고, 단계에서 각각의 유사한 이전 시나리오와 연관된 유사한 정책이 식별된다. 이는 예를 들어, 현재 시나리오와 유사한 임 의의 이전 시나리오에 대해 데이터베이스를 검색하는 유사성 식별 동작을 수행하는 처리 장치를 포함할 수 있다. 일부 경우에, 현재 및 이전 시나리오의 마르코프 결정 프로세스 및 리워드 함수의 유사점 (similarities)을 기반으로 하는 등, 유사성(analogous)이 통계적으로 결정될 수 있다. 이는 또한 데이터베이스 로부터 각각의 식별된 유사한 이전 시나리오와 연관된 정책 정보를 추출하기 위해, 그리고 정책 정보를 하 나 이상의 유사한 정책으로서 사용하기 위해 유사성 식별 동작을 수행하는 처리 장치를 포함할 수 있 다. 단계에서 역 강화 러닝이 하나 이상의 유사한 이전 시나리오와 연관된 정책 또는 정책들에 적용되고, 단계 에서 IRL 결과에 기초하여 유사한 리워드 함수가 생성된다. 이는 예를 들어, 처리 충돌 식별 동작을 수행하는 장치를 포함할 수 있으며, 이는 유사한 리워드 함수를 식별하기 위해, 현재 시나리오에 대한 마 르코프 결정 프로세스 및 유사한 정책에 대한 정보를 사용할 수 있다. 유사한 리워드 함수는 기존 리워드 함수 에 포함되지 않은 하나 이상의 파라미터를 포함할 수 있고/있거나, 유사한 리워드 함수는 기존 리워드 함수에 포함된 하나 이상의 파라미터를 생략할 수 있다. 단계에서 초기 및 유사한 리워드 함수가 비교되고, 단계에서 현재 시나리오에 대한 새로운 리워드 함 수가 생성된다. 이는 예를 들어, (i) 기존 리워드 함수에 포함되지 않은 유사한 리워드 함수의 하나 이상의 파 라미터 및/또는 (ii) 유사한 리워드 함수에 포함되지 않은 기존 리워드 함수의 하나 이상의 파라미터를 식별하 기 위한 기존 및 유사한 리워드 함수들 간의 차이를 식별하기 위해 리워드 함수 수정 동작을 수행하는 처 리 장치를 포함할 수 있다. 이는 또한 새로운 리워드 함수를 생성하기 위해 리워드 함수 수정 동작 을 수행하는 처리 장치를 포함할 수 있으며, 이는 (i) 유사한 리워드 함수로부터의 적어도 하나의 파라미 터를 포함하고/포함하거나 (ii) 기존 리워드 함수로부터의 적어도 하나의 파라미터를 제외하기 위해 수정된 기 존 리워드 함수를 나타낼 수 있다. 단계에서 새로운 리워드 함수에 기초한 새로운 정책이 획득된다. 이는 예를 들어, 현재 시나리오에 대한 새로운 정책으로서 현재 시나리오에 대한 마르코프 결정 프로세스 및 현재 시나리오에 대한 새로운 리워드 함수 를 사용하기 위해 정책 식별 동작을 수행하는 처리 장치를 포함할 수 있다. 이 시점에서, 단계(62 0)의 프로세스를 반복할지 여부가 선택적으로(optionally) 결정될 수 있다. 일부 경우, 단계 의 결정은 각각 단계(602-618)를 수행하는 다수의 에이전트(아키텍처의 다수의 인스턴스)가 있고 다수의 에이전트로 부터의 결과적인 정책이 일치(conform)하지 않을 때 사용된다. 다른 경우에, 단계의 결정은 새로운 정책 이 프로세스의 또 다른 반복 대상인지 여부를 결정하기 위해 사용된다. 예를 들어, 이를 통해 점점 더 많은 파 라미터가 정책에 추가되고 추론 중에 고려될 수 있다. 어떤 이유로든 반복(repetition)이 필요한 경우, 프로세 스는 하나 이상의 동작을 반복하기 위해 단계(또는 다른 단계)로 돌아간다. 단계에서 새로운 정책이 ML/AI 알고리즘을 사용하여 적용될 수 있거나, 어떤 방식으로든 저장, 출력 또는 사용될 수 있다. 이는 예를 들어, ML/AI 알고리즘에 새로운 정책을 제공하는 처리 장치를 포함할 수 있으며, 이는 동일한 처리 장치 또는 다른 처리 장치에 의해 실행될 수 있다. 이는 또한 현재 시나리오의 결과로 발생할 선택된 조치 과정을 식별하기 위해 새로운 정책을 사용하는 ML/AI 알고리즘을 포함할 수 있 다. 그러나, 새로운 정책이 위에서 논의된 다양한 방식을 포함하여 임의의 다른 적절한 방식으로 저장, 출력 또는 사용될 수 있다는 점에 유의한다. 도 6은 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 방법의 일 예를 도시하지만, 도 6에 다양한 변경이 이루어질 수 있다. 예를 들어, 일련의 단계들로 도시되어 있지만, 도 6의 다양한 단계들은 겹치거나(overlap), 병렬적으로 발생하거나, 다른 순서로 발생하거나, 여러 번 발생할 수 있다. 일부 실시예에서, 본 특허 문서에 설명된 다양한 함수(functions)는 컴퓨터 판독 가능 프로그램 코드로 구성되 고 컴퓨터 판독 가능 매체에 구현된 컴퓨터 프로그램에 의해 구현되거나 지원된다. \"컴퓨터 판독 가능 프로그 램 코드\"라는 문구는 소스 코드, 목적 코드, 실행 코드를 포함한 모든 유형의 컴퓨터 코드를 포함한다. \"컴퓨 터 판독 가능 매체\"라는 문구는 읽기 전용 메모리(ROM), 랜덤 액세스 메모리(RAM), 하드 디스크 드라이브(HDD), 컴팩트 디스크(CD), 디지털 비디오 디스크(DVD) 또는 임의의 다른 유형의 메모리와 같이, 컴퓨터에 의해 액세스 될 수 있는 임의의 유형의 매체를 포함한다. \"비일시적\" 컴퓨터 판독 가능 매체는 일시적인 전기 또는 다른 신 호를 전송하는 유선, 무선, 광학 또는 다른 통신 링크를 제외한다. 비일시적 컴퓨터 판독 가능 매체는 재기록 가능한 광학 디스크 또는 지울 수 있는 저장 장치와 같이, 데이터가 영구적으로 저장될 수 있는 매체 및 데이터 가 저장되고 나중에 덮어 쓰일 수 있는 매체를 포함한다. 본 특허 문서 전반에 걸쳐 사용된 특정 단어 및 문구의 정의를 설명하는 것이 유리할 수 있다. \"애플리케이션\" 및 \"프로그램\"이라는 용어는 하나 이상의 컴퓨터 프로그램, 소프트웨어 구성요소, 명령어 세트, 절차 (procedures), 함수(functions), 개체(objects), 클래스, 인스턴스, 관련 데이터 또는 적절한 컴퓨터 코드(소 스 코드, 목적 코드 또는 실행 코드 포함)로 구현되도록 조정된 그 일부를 의미한다. \"통신하다(communicat e)\"라는 용어와 그 파생어는 직간접적 통신을 모두 포함한다. \"포함하다(include)\" 및 \"포함하다(comprise)\"라 는 용어와 그 파생어는 제한 없는 포함을 의미한다. \"또는\"이라는 용어는 포괄적이며, 및/또는을 의미한다. \"연관된(associated with)\"이라는 문구와 그 파생어는 포함하다(to include), 안에 포함되다(be included within), 상호 연결하다(interconnect with), 포함하다(contain), 안에 포함되다(be contained within), 연결 하다(connect to or with), 연결하다(couple to or with), 통신 가능하다(be communicable with), 협력하다 (cooperate with), 인터리빙하다(interleave), 병치하다(juxtapose), 근접하다(be proximate to), 속박되다(be bound to or with), 가지다(have), 속성을 가지다(have a property of), 관계를 가지다(have a relationship to or with) 등을 의미할 수 있다. 항목 리스트와 함께 사용되는 \"적어도 하나\"라는 문구는 나열된 항목 중 하 나 이상의 서로 다른 조합이 사용될 수 있으며 목록에서 항목 하나만 필요할 수 있음을 의미한다. 예를 들어, \"A, B, 및 C 중 적어도 하나\"는 A, B, C, A 및 B, A 및 C, B 및 C, 그리고 A 및 B 및 C의 조합 중 임의의 것을 포함한다. 본 개시의 설명은 임의의 특정 요소, 단계 또는 기능(function)이 클레임 범위에 포함되어야 하는 필수적이거나 중요한 요소임을 암시하는 것으로 해석되어서는 안 된다. 특허 대상의 범위는 허용된 클레임에 의해서만 정의 된다. 더욱이, 어떠한 클레임도 \"~을 위한 수단(means for)\" 또는 \"~을 위한 단계(step for)\"라는 정확한 단어 가 특정 클레임에서 명시적으로 사용된 후 기능을 식별하는 분사구(participle phrase)가 뒤따르지 않는 한, 첨 부된 클레임 또는 클레임 요소와 관련하여 35 U.S.C. § 112(f)를 발동하지(invokes) 않는다. 클레임 내 \"메커 니즘\", \"모듈\", \"장치(device)\", \"유닛\", \"구성요소\", \"요소\", \"멤버\", \"장치(apparatus)\", \"머신\", \"시스템\" 과 같은 용어(그러나 이에 국한되지 않음)의 사용은 관련 기술 분야의 당업자에게 알려진 구조를 의미하는 것으 로 이해되고 의도되며, 클레임 자체의 특징에 의해 추가로 수정되거나 강화되는 것으로, 35 U.S.C. § 112(f)를 발동하도록 의도되지 않는다. 본 개시는 특정 실시예 및 일반적으로 연관된 방법을 설명했지만, 이들 실시예 및 방법의 변경(alterations) 및 치환(permutations)은 당업자에게 명백할 것이다. 따라서, 예시적인 실시예에 대한 위의 설명은 본 개시를 정 의하거나 제한하지 않는다. 다음의 클레임에 의해 정의된 바와 같이, 본 개시의 정신 및 범위를 벗어나지 않고 다른 변경(changes), 대체 및 변경(alterations)도 가능하다."}
{"patent_id": "10-2023-7040341", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 개시의 보다 완전한 이해를 위해, 이제 첨부 도면과 함께 다음의 설명을 참조한다: 도 1은 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 예시적인 아키텍처 를 도시하고; 도 2는 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 지원하는 예시적인 장치를 도시하 고; 도 3은 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 단일 에이전트 (single-agent) 아키텍처의 예시적인 사용을 도시하고; 도 4는 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 다수의 에이전트 (multi-agent) 아키텍처의 예시적인 사용을 도시하고;도 5는 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 아키텍처에서 역 강 화 러닝(Inverse Reinforcement Learning)을 사용하는 예시적인 기술을 도시하고; 및 도 6은 본 개시에 따른 유사한 시나리오를 사용하는 명시적인 윤리적 머신을 제공하기 위한 예시적인 방법을 도 시한다."}
