{"patent_id": "10-2021-0117979", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0124103", "출원번호": "10-2021-0117979", "발명의 명칭": "음성 합성 모델의 속성 등록 방법, 장치, 전자 기기, 저장 매체 및 컴퓨터 프로그램 제품", "출원인": "베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니", "발명자": "왕 웬푸"}}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 합성 모델의 속성 등록 방법으로서,등록될 속성에 대응하는 복수의 데이터를 수집하는 단계, 및상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하는 단계를 포함하되, 상기 음성 합성 모델은 이미 사전에 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것인,음성 합성 모델의 속성 등록 방법."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 속성에 음색기 포함되는 경우, 등록될 속성에 대응하는 복수의 데이터를 수집하는 단계는 등록될 음색에대응하는 복수의 등록 데이터를 수집하는 것을 포함하고,각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 음색의 음색 정보, 상기 텍스트 콘텐츠의 오디오 정보, 및상기 등록될 음색을 사용하여 상기 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보를 포함하는, 음성 합성 모델의 속성 등록 방법."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하는 단계는,각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 텍스트 콘텐츠의 오디오 정보, 상기 음색 정보에 대해, 상기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 추출기, 음색 인코더 및 디코더를 사용하여, 예측 음향학 특징정보를 생성하는 것,상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하는 것,재구성 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 음색 인코더와 상기 디코더의파라미터를 조정하는 것을 포함하는, 음성 합성 모델의 속성 등록 방법."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 속성에 스타일이 포함되는 경우, 등록될 속성에 대응하는 복수의 데이터를 수집하는 단계는, 등록될 스타일에 대응하는 복수의 등록 데이터를 수집하는 것을 포함하고,각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 스타일에 대해 사전 정의된 스타일 식별자 및 상기 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보를 포함하는, 음성 합성 모델의 속성 등록 방법."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하는 단계는, 각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 스타일 식별자 및 상기 스타일을 사용하여 대응하는 상기텍스트 콘텐츠를 표현하는 오디오 정보에 대해, 상기 음성 합성 모델의 스타일 추출기와 스타일 인코더를 사용공개특허 10-2021-0124103-3-하여, 각각 목표 스타일 인코딩 특징 및 예측 스타일 인코딩 특징을 생성하는 것,상기 예측 스타일 인코딩 특징 및 상기 목표 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축하는 것,상기 스타일 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 스타일 추출기와 상기스타일 인코더의 파라미터를 조정하는 것을 포함하는, 음성 합성 모델의 속성 등록 방법."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 속성에 스타일 및 음색이 포함되는 경우, 등록될 속성에 대응하는 복수의 데이터를 수집하는 단계는, 등록될 스타일 및 등록될 스타일에 대응하는 복수의 등록 데이터를 수집하는 것을 포함하고, 각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 스타일에 대해 사전 정의된 스타일 식별자, 상기 등록될 음색의 음색 정보, 상기 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보, 및 상기 등록될 음색 및상기 등록될 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보를 포함하는, 음성 합성모델의 속성 등록 방법."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하는 단계는,각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오정보, 상기 스타일 식별자, 상기 음색 정보에 대해, 상기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더,음색 인코더, 스타일 추출기 및 디코더를 사용하여, 목표 스타일 인코딩 특징, 예측 스타일 인코딩 특징 및 예측 음향학 특징 정보를 생성하는 것,상기 목표 스타일 인코딩 특징, 상기 예측 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 종합 손실 함수를 구축하는 것,상기 종합 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 음색 인코더, 상기 스타일인코더, 상시 스타일 추출기 및 상기 디코더의 파라미터를 조정하는 것을 포함하는, 음성 합성 모델의 속성 등록 방법."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 목표 스타일 인코딩 특징, 상기 예측 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 종합 손실 함수를 구축하는 것은,상기 목표 스타일 인코딩 특징 및 상기 예측 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축하는 것,상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하는 것,상기 스타일 손실 함수 및 상기 재구성 손실 함수에 기반하여, 상기 종합 손실 함수를 구축하는 것을 포함하는, 음성 합성 모델의 속성 등록 방법."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "음성 합성 모델의 속성 등록 장치로서,등록될 속성에 대응하는 복수의 데이터를 수집하기 위한 수집 모듈, 및상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하기 위한 등록 모듈을 포함하되, 상기 음성 합성 모델은 이미 사전에 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것인,공개특허 10-2021-0124103-4-음성 합성 모델의 속성 등록 장치."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 수집 모듈은, 상기 속성에 음색이 포함되는 경우, 등록될 음색에 대응하는 복수의 등록 데이터를 수집하기위한 것이고,각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 음색의 음색 정보, 상기 텍스트 콘텐츠의 오디오 정보, 및상기 등록될 음색을 사용하여 상기 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보를 포함하는, 음성 합성 모델의 속성 등록 장치."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 등록 모듈은,각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 텍스트 콘텐츠의 오디오 정보, 상기 음색 정보에 대해, 상기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 추출기, 음색 인코더 및 디코더를 사용하여, 예측 음향학 특징정보를 생성하기 위한 생성부,상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하기 위한 구축부,재구성 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 음색 인코더와 상기 디코더의파라미터를 조정하기 위한 조정부를 포함하는, 음성 합성 모델의 속성 등록 장치."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 수집 모듈은, 또한, 상기 속성에 스타일이 포함되는 경우, 등록될 스타일에 대응하는 복수의 등록 데이터를 수집하기 위한 것이고,각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 스타일에 대해 사전 정의된 스타일 식별자 및 상기 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보를 포함하는, 음성 합성 모델의 속성 등록 장치."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 생성부는, 또한, 각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 스타일 식별자 및 상기 스타일을 사용하여 대응하는 상기 텍스트 콘텐츠를 표현하는 오디오 정보에 대해, 상기 음성 합성 모델의 스타일 추출기와스타일 인코더를 사용하여, 각각 목표 스타일 인코딩 특징 및 예측 스타일 인코딩 특징을 생성하기 위한것이고,상기 구축부는, 또한, 상기 예측 스타일 인코딩 특징 및 상기 목표 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축하기 위한 것이며,상기 조정부는, 또한, 상기 스타일 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기스타일 추출기와 상기 스타일 인코더의 파라미터를 조정하기 위한 것인, 음성 합성 모델의 속성 등록 장치."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 수집 모듈은, 또한, 등록될 스타일 및 등록될 스타일에 대응하는 복수의 등록 데이터를 수집하기 위한 것이고,공개특허 10-2021-0124103-5-각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 스타일에 대해 사전 정의된 스타일 식별자, 상기 등록될 음색의 음색 정보, 상기 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보, 및 상기 등록될 음색 및상기 등록될 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보를 포함하는, 음성 합성모델의 속성 등록 장치."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 생성부는, 또한, 각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보, 상기 스타일 식별자, 상기 음색 정보에 대해, 상기 음성 합성 모델 중의 콘텐츠인코더, 스타일 인코더, 음색 인코더, 스타일 추출기 및 디코더를 사용하여, 목표 스타일 인코딩 특징, 예측 스타일 인코딩 특징 및 예측 음향학 특징 정보를 생성하기 위한 것이고,상기 구축부는, 또한, 상기 목표 스타일 인코딩 특징, 상기 예측 스타일 인코딩 특징, 상기 예측 음향학 특징정보 및 상기 목표 음향학 특징 정보에 기반하여, 종합 손실 함수를 구축하기 위한 것이며,상기 조정부는, 또한, 상기 종합 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 음색 인코더, 상기 스타일 인코더, 상기 스타일 추출기 및 상기 디코더의 파라미터를 조정하기 위한 것인, 음성합성 모델의 속성 등록 장치."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 구축부는, 구체적으로,상기 목표 스타일 인코딩 특징 및 상기 예측 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축하고,상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하며,상기 스타일 손실 함수 및 상기 재구성 손실 함수에 기반하여, 상기 종합 손실 함수를 구축하기 위한 것인, 음성 합성 모델의 속성 등록 장치."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "적어도 하나의 프로세서, 및상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하는 전자 기기로서,상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령어가 저장되어 있고, 상기 명령어가 상기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제8항 중 어느한 항의 방법을 실행하게 하는, 전자 기기."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "컴퓨터 명령어가 저장되어 있는 비휘발성 컴퓨터 판독 가능 저장 매체로서,상기 컴퓨터 명령어가 프로세서로 하여금 제1항 내지 제8항 중 어느 한 항의 방법을 구현하게 하기 위한 것인,비휘발성 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2021-0117979", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "프로세서에 의해 실행될 때 제1항 내지 제8항 중 어느 한 항의 방법을 구현하는 컴퓨터 프로그램을 포함하는 컴퓨터 프로그램 제품."}
{"patent_id": "10-2021-0117979", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 출원은 딥 러닝 및 스마트 음성 등 인공 지능"}
{"patent_id": "10-2021-0117979", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것으로, 음성 합성 모델의 속성 등록 방법, 장치, 전자 기기 및 저장 매체를 개시한다. 구체적인 구현 방안은, 등록될 속성에 대응하는 복수의 데이터를 수 집하며, 상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하는 것을 포함하 (뒷면에 계속) 대 표 도 - 도1"}
{"patent_id": "10-2021-0117979", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2021-0124103 고, 상기 음성 합성 모델은 이미 사전에 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것이다. 본 출원 의 기술 방안에 의하면, 음성 합성 모델에 속성을 등록할 수 있어, 음성 합성 모델로 하여금 등록된 속성에 기반 하여 음성 합성을 실행 가능하게 할 수 있고, 사용자의 맞춤형 요구를 충족시킬 수 있다. 또한, 사용자 장면의 음성 합성의 다양성과 흥미성을 최대적으로 풍부화시키고, 제품의 근접성과 의인화를 향상하며, 사용자의 사용 체험을 현저하게 향상시킬 수 있다. 또한, 본 출원의 기술 방안은 다양한 맞춤형 음성 인터랙션 장면에 적용될 수 있으며, 보편적인 보급성을 갖고 있다. CPC특허분류 G10L 25/30 (2013.01) 발명자 쑨 타오 중국 베이징 하이디엔 디스트릭트 샹디 10번가 넘 버 10 바이두 캠퍼스 2층 위엔 한 중국 베이징 하이디엔 디스트릭트 샹디 10번가 넘 버 10 바이두 캠퍼스 2층가오 정쿤 중국 베이징 하이디엔 디스트릭트 샹디 10번가 넘 버 10 바이두 캠퍼스 2층 지아 레이 중국 베이징 하이디엔 디스트릭트 샹디 10번가 넘 버 10 바이두 캠퍼스 2층명 세 서 청구범위 청구항 1 음성 합성 모델의 속성 등록 방법으로서, 등록될 속성에 대응하는 복수의 데이터를 수집하는 단계, 및 상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하는 단계 를 포함하되, 상기 음성 합성 모델은 이미 사전에 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것인, 음성 합성 모델의 속성 등록 방법. 청구항 2 제1항에 있어서, 상기 속성에 음색기 포함되는 경우, 등록될 속성에 대응하는 복수의 데이터를 수집하는 단계는 등록될 음색에 대응하는 복수의 등록 데이터를 수집하는 것을 포함하고, 각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 음색의 음색 정보, 상기 텍스트 콘텐츠의 오디오 정보, 및 상기 등록될 음색을 사용하여 상기 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보를 포함하는, 음성 합성 모 델의 속성 등록 방법. 청구항 3 제2항에 있어서, 상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하는 단계는, 각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 텍스트 콘텐츠의 오디오 정보, 상기 음색 정보에 대해, 상 기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 추출기, 음색 인코더 및 디코더를 사용하여, 예측 음향학 특징 정보를 생성하는 것, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하는 것, 재구성 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 음색 인코더와 상기 디코더의 파라미터를 조정하는 것 을 포함하는, 음성 합성 모델의 속성 등록 방법. 청구항 4 제1항에 있어서, 상기 속성에 스타일이 포함되는 경우, 등록될 속성에 대응하는 복수의 데이터를 수집하는 단계는, 등록될 스타 일에 대응하는 복수의 등록 데이터를 수집하는 것을 포함하고, 각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 스타일에 대해 사전 정의된 스타일 식별자 및 상기 스타일 을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보를 포함하는, 음성 합성 모델의 속성 등록 방법. 청구항 5 제4항에 있어서, 상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하는 단계는, 각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 스타일 식별자 및 상기 스타일을 사용하여 대응하는 상기 텍스트 콘텐츠를 표현하는 오디오 정보에 대해, 상기 음성 합성 모델의 스타일 추출기와 스타일 인코더를 사용하여, 각각 목표 스타일 인코딩 특징 및 예측 스타일 인코딩 특징을 생성하는 것, 상기 예측 스타일 인코딩 특징 및 상기 목표 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축하는 것, 상기 스타일 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 스타일 추출기와 상기 스타일 인코더의 파라미터를 조정하는 것 을 포함하는, 음성 합성 모델의 속성 등록 방법. 청구항 6 제1항에 있어서, 상기 속성에 스타일 및 음색이 포함되는 경우, 등록될 속성에 대응하는 복수의 데이터를 수집하는 단계는, 등록 될 스타일 및 등록될 스타일에 대응하는 복수의 등록 데이터를 수집하는 것을 포함하고, 각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 스타일에 대해 사전 정의된 스타일 식별자, 상기 등록될 음 색의 음색 정보, 상기 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보, 및 상기 등록될 음색 및 상기 등록될 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보를 포함하는, 음성 합성 모델의 속성 등록 방법. 청구항 7 제6항에 있어서, 상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하는 단계는, 각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보, 상기 스타일 식별자, 상기 음색 정보에 대해, 상기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더, 음색 인코더, 스타일 추출기 및 디코더를 사용하여, 목표 스타일 인코딩 특징, 예측 스타일 인코딩 특징 및 예 측 음향학 특징 정보를 생성하는 것, 상기 목표 스타일 인코딩 특징, 상기 예측 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표 음향 학 특징 정보에 기반하여, 종합 손실 함수를 구축하는 것, 상기 종합 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 음색 인코더, 상기 스타일 인코더, 상시 스타일 추출기 및 상기 디코더의 파라미터를 조정하는 것 을 포함하는, 음성 합성 모델의 속성 등록 방법. 청구항 8 제7항에 있어서, 상기 목표 스타일 인코딩 특징, 상기 예측 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표 음향 학 특징 정보에 기반하여, 종합 손실 함수를 구축하는 것은, 상기 목표 스타일 인코딩 특징 및 상기 예측 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축하는 것, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하는 것, 상기 스타일 손실 함수 및 상기 재구성 손실 함수에 기반하여, 상기 종합 손실 함수를 구축하는 것 을 포함하는, 음성 합성 모델의 속성 등록 방법. 청구항 9 음성 합성 모델의 속성 등록 장치로서, 등록될 속성에 대응하는 복수의 데이터를 수집하기 위한 수집 모듈, 및 상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하기 위한 등록 모듈 을 포함하되, 상기 음성 합성 모델은 이미 사전에 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것인,음성 합성 모델의 속성 등록 장치. 청구항 10 제9항에 있어서, 상기 수집 모듈은, 상기 속성에 음색이 포함되는 경우, 등록될 음색에 대응하는 복수의 등록 데이터를 수집하기 위한 것이고, 각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 음색의 음색 정보, 상기 텍스트 콘텐츠의 오디오 정보, 및 상기 등록될 음색을 사용하여 상기 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보를 포함하는, 음성 합성 모 델의 속성 등록 장치. 청구항 11 제10항에 있어서, 상기 등록 모듈은, 각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 텍스트 콘텐츠의 오디오 정보, 상기 음색 정보에 대해, 상 기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 추출기, 음색 인코더 및 디코더를 사용하여, 예측 음향학 특징 정보를 생성하기 위한 생성부, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하기 위한 구 축부, 재구성 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 음색 인코더와 상기 디코더의 파라미터를 조정하기 위한 조정부 를 포함하는, 음성 합성 모델의 속성 등록 장치. 청구항 12 제11항에 있어서, 상기 수집 모듈은, 또한, 상기 속성에 스타일이 포함되는 경우, 등록될 스타일에 대응하는 복수의 등록 데이터 를 수집하기 위한 것이고, 각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 스타일에 대해 사전 정의된 스타일 식별자 및 상기 스타일 을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보를 포함하는, 음성 합성 모델의 속성 등록 장치. 청구항 13 제12항에 있어서, 상기 생성부는, 또한, 각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 스타일 식별자 및 상기 스타일을 사 용하여 대응하는 상기 텍스트 콘텐츠를 표현하는 오디오 정보에 대해, 상기 음성 합성 모델의 스타일 추출기와 스타일 인코더를 사용하여, 각각 목표 스타일 인코딩 특징 및 예측 스타일 인코딩 특징을 생성하기 위한 것이고, 상기 구축부는, 또한, 상기 예측 스타일 인코딩 특징 및 상기 목표 스타일 인코딩 특징에 기반하여, 스타일 손 실 함수를 구축하기 위한 것이며, 상기 조정부는, 또한, 상기 스타일 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 스타일 추출기와 상기 스타일 인코더의 파라미터를 조정하기 위한 것인, 음성 합성 모델의 속성 등록 장치. 청구항 14 제11항에 있어서, 상기 수집 모듈은, 또한, 등록될 스타일 및 등록될 스타일에 대응하는 복수의 등록 데이터를 수집하기 위한 것 이고,각 상기 등록 데이터는 텍스트 콘텐츠, 상기 등록될 스타일에 대해 사전 정의된 스타일 식별자, 상기 등록될 음 색의 음색 정보, 상기 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보, 및 상기 등록될 음색 및 상기 등록될 스타일을 사용하여 상기 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보를 포함하는, 음성 합성 모델의 속성 등록 장치. 청구항 15 제14항에 있어서, 상기 생성부는, 또한, 각 상기 등록 데이터 중의 상기 텍스트 콘텐츠, 상기 스타일을 사용하여 상기 텍스트 콘 텐츠를 표현하는 오디오 정보, 상기 스타일 식별자, 상기 음색 정보에 대해, 상기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더, 음색 인코더, 스타일 추출기 및 디코더를 사용하여, 목표 스타일 인코딩 특징, 예측 스 타일 인코딩 특징 및 예측 음향학 특징 정보를 생성하기 위한 것이고, 상기 구축부는, 또한, 상기 목표 스타일 인코딩 특징, 상기 예측 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 종합 손실 함수를 구축하기 위한 것이며, 상기 조정부는, 또한, 상기 종합 손실 함수가 수렴되는 방향으로 향하도록, 상기 음성 합성 모델 중의 상기 음 색 인코더, 상기 스타일 인코더, 상기 스타일 추출기 및 상기 디코더의 파라미터를 조정하기 위한 것인, 음성 합성 모델의 속성 등록 장치. 청구항 16 제15항에 있어서, 상기 구축부는, 구체적으로, 상기 목표 스타일 인코딩 특징 및 상기 예측 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축하고, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하며, 상기 스타일 손실 함수 및 상기 재구성 손실 함수에 기반하여, 상기 종합 손실 함수를 구축하기 위한 것인, 음 성 합성 모델의 속성 등록 장치. 청구항 17 적어도 하나의 프로세서, 및 상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하는 전자 기기로서, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령어가 저장되어 있고, 상기 명령어가 상 기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제8항 중 어느 한 항의 방법을 실행하게 하는, 전자 기기. 청구항 18 컴퓨터 명령어가 저장되어 있는 비휘발성 컴퓨터 판독 가능 저장 매체로서, 상기 컴퓨터 명령어가 프로세서로 하여금 제1항 내지 제8항 중 어느 한 항의 방법을 구현하게 하기 위한 것인, 비휘발성 컴퓨터 판독 가능 저장 매체. 청구항 19 프로세서에 의해 실행될 때 제1항 내지 제8항 중 어느 한 항의 방법을 구현하는 컴퓨터 프로그램을 포함하는 컴 퓨터 프로그램 제품. 발명의 설명 기 술 분 야 본 출원은 컴퓨터 기술 분야에 관한 것으로서, 구체적으로는, 딥 러닝 및 스마트 음성 등 인공지능 기술 분야에 관한 것이고, 특히, 음성 합성 모델의 속성 등록 방법, 장치, 전자 기기 및 저장 매체에 관한 것이다."}
{"patent_id": "10-2021-0117979", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "문자 언어 전환(Text-to-Speech: TTS)라고도 하는 음성 합성은, 컴퓨터를 통해 텍스트 정보를 음질이 좋고, 자 연스럽고 유창도가 높은 음성 정보로 전환시키는 과정을 의미하며, 스마트 음성 인터랙션 기술의 핵심 기술 중 하나이다. 최근 몇 년간, 딥 러닝 기술의 발전 및 음성 합성 분야에서의 광범위한 응용과 함께, 음성 합성의 음질 및 자연 유창도는 전례없는 발전을 거듭하였다. 현재 주류적 음성 합성 모델은 주로 단일 발음자(즉, 단일 음색), 단일 스타일의 음성 합성을 구현하는데 사용되고 있다. 멀티 스타일, 멀티 음색 합성을 구현하려면, 각 발음자가 녹 음한 복수의 스타일의 훈련 데이터를 수집하여, 음성 합성 모델을 훈련할 수 있다. 본 출원은 음성 합성 모델의 속성 등록 방법, 장치, 전자 기기 및 매체를 제공한다. 본 출원의 일 측면에 의하면, 음성 합성 모델의 속성 등록 방법을 제공하며, 상기 방법은, 등록될 속성에 대응하는 복수의 데이터를 수집하는 것, 및 상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하는 것을 포함하고, 상기 음성 합성 모델은 이미 사전에 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것이다. 본 출원의 다른 측면에 의하면, 음성 합성 모델의 속성 등록 장치를 제공하며, 상기 장치는, 등록될 속성에 대응하는 복수의 데이터를 수집하기 위한 수집 모듈, 및 상기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하기 위한 등록 모듈을 포 함하고, 상기 음성 합성 모델은 이미 사전에 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것이다. 본 출원의 또 다른 측면에 의하면, 전자 기기를 제공하며, 적어도 하나의 프로세서, 및 상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하며, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령어가 저장되어 있고, 상기 명령어가 상 기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서로 하여금 상기 방법을 실행하게 한 다. 본 출원의 또 다른 측면에 의하면, 컴퓨터 명령어가 저장되어 있는 비휘발성 컴퓨터 판독 가능 저장 매체에 있 어서, 상기 컴퓨터 명령어가 프로세서로 하여금, 상기 방법을 구현하게 하기 위한 것인 비휘발성 컴퓨터 판독 가능 저장 매체를 제공한다. 본 출원의 기술 방안에 의하면, 음성 합성 모델 중의 속성 등록을 구현할 수 있어, 나아가 음성 합성 모델로 하 여금 등록된 속성에 기반하여 음성 합성을 실행할 수 있게 함으로써, 사용자의 맞춤형 요구를 충족시킬 수 있다. 또한, 사용자 장면의 음성 합성의 다양성과 흥미성을 최대적으로 풍부화시키고, 제품의 근접성과 의인화 를 향상하며, 사용자의 사용 체험을 현저하게 향상시킬 수 있다. 또한, 본 출원의 기술 방안은 다양한 맞춤형 음성 인터랙션 장면에 적용될 수 있으며, 보편적인 보급성을 갖고 있다. 본 명세서에 기술된 내용은 본 개시의 실시예의 핵심 또는 중요한 특징을 지정하기 위한 것이 아니고, 또한, 본 출원의 범위는 이에 한정되지 않음을 이해하여야 한다. 본 출원의 다른 특징들은 하기 설명으로부터 용이하게 이해할 수 있을 것이다."}
{"patent_id": "10-2021-0117979", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 출원의 예시적인 실시예들을 설명한다. 쉽게 이해할 수 있도록, 본 명세서에서 설명 한 각 실시예의 세부사항을 포함하게 되는데, 이들은 단지 예시적인 것에 불과하다. 따라서, 당업자라면 본 출 원의 범위 및 취지를 벗어나지 않으면서 본 출원의 실시예에 대해 여러가지 변경 및 수정이 이루어질 수 있음을 이해할 것이다. 또한, 명확성과 간결성을 위해 하기의 설명에 있어서, 공지된 기능 및 구성에 대한 설명은 생략 한다. 최근 몇 년간, 음성 합성 기술의 다양한 분야에서의 관범위한 사용 및 응용 장면의 끊이 없는 풍부화에 따라, 멀티 스타일, 멀티 음색 합성과 같은 음성 합성의 다양화는 갈수록 사용자들의 사랑을 받고 있다. 그러나, 분명 한 것은, 기존의 음성 합성 모델은 합성 훈련 데이터 세트 중의 스타일과 음색만을 지원할 수 있기에, 많은 응 용 장면의 수요를 만족시키기 어렵다. 예를 들어, 훈련 데이터 세트 외의 새로운 스타일, 새로운 음색을 사용하 여 음성 방송을 진행하는 경우, 특히 크로스-스타일, 크로스-음색 합성이 일반 사용자를 향하는 경우, 일반 사 용자는 자신의 스타일과 음색을 사용하여 음성 방송을 구현할 수 없다. 상기와 같은 기술 과제를 기반으로, 본 출원은 소량의 사용자 데이터를 사용하여 스타일 및/또는 음색을 등록함 으로써, 스타일, 음색이 다양화된 맞춤형 주문을 구현하여, 사용자의 맞춤형 요구를 충족시킬 수 있고, 실용적 가치가 높다. 본 출원에 따른 스타일 및/또는 음색 등록의 기본 사상은, 수백건의 데이터와 같은 소량의 사용자 데이터를 사용하여, 훈련 데이터 세트에 기반하여 사전 훈련된 음성 합성 모델에 대해 미세 조정함으로써, 음성 합성 모델로 하여금 등록될 스타일 및/또는 음색을 인식할 수 있게 하고, 나아가, 음성 합성 모델이 등록된 음 성 및/또는 스타일을 사용하여 합성 가능하도록 할 수 있으므로, 사용자의 맞춤형 수요를 만족시킬 수 있다. 음 성 합성 모델에 스타일 및/또는 음색을 등록함으로써, 사용자 장면의 음성 합성의 다양성과 흥미성을 최대적으 로 풍부화시키고, 제품의 근접성과 의인화를 향상하며, 사용자의 사용 체험을 현저하게 향상시킬 수 있다. 또한, 본 출원의 기술 방안은 다양한 맞춤형 음성 인터랙션 장면에 적용될 수 있으며, 보편적인 보급성을 갖고 있다. 우선, 본 출원의 실시예에 따른 크로스-언어, 크로스-스타일, 크로스-음색의 음성 합성 모델의 훈련 방안을 설 명한다. 도 1은 본 출원에 따른 제1 실시예의 개략도이다. 도 1에 도시된 바와 같이, 본 실시예에 따르면, 음성 합성 모 델의 훈련 방법을 제공하고, 구체적으로는 하기 단계들을 포함할 수 있다. 단계S101에서는, 복수의 훈련 데이터를 포함하는 훈련 데이터 세트를 수집하고, 각 훈련 데이터에는 합성될 음 성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 훈련 스타일 정보에 대응하는 훈련 스 타일을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보와, 훈련 스타일 정보에 대응하는 훈 련 스타일 및 훈련 음색 정보에 대응하는 훈련 음색을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 목표 음 향학 특징 정보를 포함한다. 단계S102에서는, 훈련 데이터 세트 중의 복수의 훈련 데이터를 사용하여, 음성 합성 모델을 훈련한다. 본 출원의 실시예에 따른 음성 합성 모델의 훈련 방법의 실행 주체는 음성 합성 모델의 훈련 장치이다. 상기 장 치는 전자 엔티티일 수 있고, 또는, 소프트웨어로 집적된 애플리케이션으로서, 사용될 때 퓨터 디바이스 상에서실행되어, 음성 합성 모델을 훈련한다. 본 실시예에 따른 훈련에서, 수집된 훈련 데이터의 수량은 백만 자릿수에 달할 수 있어, 음성 합성 모델을 보다 정확하게 훈련할 수 있다. 각 훈련 데이터에는 합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트 의 콘텐츠 정보를 포함할 수 있다. 본 실시예에 따른 훈련 스타일 정보는 스타일 ID와 같은 스타일 식별자일 수 있거나, 상기 스타일에 의해 표현 된 오디오 정보에서 추출된 다른 포맷일 수도 있다. 그러나, 실제 응용에서, 사용할 때, 스타일에 의해 표현된 오디오 정보는 멜(mel) 스펙트럼 시퀀스의 형태로 표현될 수 있다. 본 출원의 실시예에 따른 훈련 음색 정보는, 또한, 상기 음색에 의해 표현된 오디오 정보에 기반하여 추출될 수 있거나, 상기 음색 정보는 멜(mel) 스펙트럼 시퀸스의 형태로 표현될 수도 있다. 본 출원의 실시예에 따른 훈련 스타일 정보는 유머, 기쁨, 슬픔, 전통 등과 같은 음성 표현의 스타일을 한정하 기 위한 것이다. 본 실시예에 따른 훈련 음색 정보는 음성의 소리의 음색을 한정하여 표현하기 위한 것이다. 예 를 들어, 스타(A), 방송인(B), 카툰 동물(C) 등일 수 있다. 본 실시예에 따른 훈련 텍스트의 콘텐츠 정보는 텍스트 형태이다. 훈련 텍스트의 콘텐츠 정보는 구체적으로 훈 련 텍스트의 음자 시퀸스일 수 있다. 예를 들어, 훈련 텍스트가 중국어인 경우, 이 콘텐츠 정보는 훈련 텍스트 의 음조를 가진 음자 시퀸스일 수 있고, 중국어 문자의 발음에는 음조를 가져 있으므로, 중국어에 대해, 음조를 가진 음자 시퀸스를 획득해야 하다. 또한, 다른 언어에 대해, 훈련 텍스트의 콘텐츠 정보는 훈련 텍스트의 음자 시퀸스이다. 예를 들어, 훈련 텍스트가 중국어인 경우, 음자는 자음 또는 모음과 같은 중국어 병음 중의 음절일 수 있다. 또한, 각 훈련 데이터에는 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보와, 훈련 스타일 정보에 대응하는 훈련 스타일 및 훈련 음색 정보에 대응하는 훈련 음색을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 포함한다. 이 두가지 정보는 지도 훈련의 참조로 사용함으로써, 음성 합성 모델이 보다 효율적으로 학습될 수 있도록 한다. 본 출원의 실시예에 따른 음성 합성 모델의 훈련 방법에 따르면, 상술한 방법을 통해 음성 합성 모델을 효과적 으로 훈련할 수 있으므로, 음성 합성 모델로 하여금 훈련 데이터에 기반하여, 콘텐츠, 스타일 및 음색에 따라 음성을 합성하는 과정을 학습하게 하고, 나아가, 학습된 음성 합성 모델의 음성 합성의 다양성을 풍부하게 할 수 있다. 도 2는 본 출원에 따른 제2 실시예의 개략도이다. 도 2에 도시된 바와 같이, 본 실시예의 음성 합성 모델의 훈 련 방법에 따라, 상기 도 1에 도시된 실시예의 기술 방안의 기초에, 본 출원의 기술 방안을 보다 더 구체적으로 설명한다. 도 2에 도시된 바와 같이, 본 실시예에 따른 음성 합성 모델의 훈련 방법은, 구체적으로 하기 단계들 을 포함할 수 있다. 단계S201에서는, 복수의 훈련 데이터를 수집하고, 각 훈련 데이터에는 합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트 의 콘텐츠 정보를 표현하는 스타일 특징 정보와, 훈련 스타일 정보에 대응하는 훈련 스타일 및 훈련 음색 정보 에 대응하는 훈련 음색을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 포함한다. 단계S202에서는, 각 훈련 데이터에 대해, 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더 및 음색 인코더를 사용하여, 훈련 데이터 중의 훈련 텍스트의 콘텐츠 정보, 훈련 스타일 정보 및 훈련 음색 정보에 대해 각각 인 코딩하여, 훈련 콘텐츠 인코딩 특징, 훈련 스타일 인코딩 특징 및 훈련 음색 인코더 특징을 순차적으로 획득한 다. 구체적으로, 음성 합성 모델 중의 콘텐츠 인코더를 사용하여 훈련 데이터 중의 훈련 텍스트의 콘텐츠 정보에 대 해 인코딩하여, 훈련 콘텐츠 인코딩 특징을 획득한다. 음성 합성 모델 중의 스타일 인코더를 사용하여 훈련 데 이터 중의 훈련 스타일 정보 및 훈련 텍스트의 콘텐츠 정보에 대해 인코딩하여, 훈련 스타일 인코딩 특징을 획 득한다. 음성 합성 모델 중의 음색 인코더를 사용하여 훈련 데이터 중의 훈련 음색 정보에 대해 인코딩하여, 훈 련 음색 인코딩 특징을 획득한다. 구현 과정에서, 구체적으로는 음성 합성 모델 중의 콘텐츠 인코더를 사용하여 훈련 데이터 중의 훈련 텍스트의 콘텐츠 정보에 대해 인코딩하여, 훈련 콘텐츠 인코딩 특징을 획득한다. 처리될 텍스트의 콘텐츠 정보는 음자 시 퀸스의 형태를 사용하므로, 대응하게 획득된 훈련 콘텐츠 인코딩 특징도 대응하는 시퀸스 형태일 수 있으며, 이를 훈련 콘텐츠 인코딩 시퀸스라고 할 수 있다. 여기서, 각 음자는 각각 하나의 인코딩 벡터에 대응한다. 상기 콘텐츠 인코더는 각 음자가 어떻게 발음할 것인지를 결정한다. 구체적으로, 음성 합성 모델 중의 스타일 인코더 를 사용하여 훈련 텍스트의 콘텐츠 정보 및 훈련 스타일 정보에 대해 인코딩하여, 훈련 스타일 인코딩 특징을 획득한다. 즉, 상기 스타일 인코더는 훈련 텍스트의 콘텐츠 정보에 대해 인코딩하는 동시에 훈련 스타일 정보를 사용하여 인코딩 스타일을 제어하고, 대응하는 훈련 스타일 인코딩 매트릭스를 생성하며, 마찬가지로 이를 훈련 스타일 인코딩 시퀸스라고도 할 수 있다. 각 음자는 각각 하나의 인코딩 벡터에 대응한다. 상기 스타일 인코더 는 각 음자의 발음 방식, 즉 스타일을 결정한다. 또한, 구체적으로, 음성 합성 모델 중의 음색 인코더를 사용하 여 훈련 음색 정보에 대해 인코딩하여, 훈련 음색 인코딩 특징을 획득할 수 있다. 훈련 음색 정보는 또한 멜 (mel) 스펙트럼 시퀸스를 사용할 수 있기 때문에, 즉, 음색 인코더는 멜(mel) 스펙트럼 시퀀스에 대해 인코딩하 여, 대응하는 음색 벡터를 생성할 수 있다. 상기 음색 인코더는 음색A, 음색B 또는 음색C 등과 같은 합성된 음 성의 음색을 결정한다. 단계S203에서는, 음성 합성 모델 중의 스타일 추출기를 사용하여, 훈련 텍스트의 콘텐츠 정보 및 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보에 기반하여, 목표 훈련 스타일 인코딩 특징을 추출한다. 설명해야 할 것은, 이 훈련 텍스트의 콘텐츠 정보는 상기 스타일 인코더의 훈련중에 입력된 훈련 텍스트의 콘텐 츠 정보와 동일하다. 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현 하는 스타일 특징 정보는 멜(mel) 스펙트럼 시퀀스의 형태일 수 있다. 도 3은 본 실시예에 따른 음성 합성 모델의 훈련 아키텍처 개략도이다. 도 4는 본 실시예에 따른 음성 합성 모 델의 응용 아키텍처 개략도이다. 도 4에 도시된 바와 같이, 본 실시예에 따른 음성 합성 모델은, 콘텐츠 인코더, 스타일 인코더, 음색 인코더 및 디코더 등 여러 부분을 포함하여 구성될 수 있다. 종래의 음성 합성 모 델 Tacotron과 비교하면, 도 4에 도시된 음성 합성 모델 중의 콘텐츠 인코더 및 스타일 인코더, 음색 인코더는 단독적인 3개의 유닛이며, 3개의 유닛은 분리된 상태에서 각자 서로 다른 역할을 실행하고, 각각 대응하는 기능 을 가지고 있으며, 크로스-스타일, 크로스-음색, 크로스-언어 합성의 핵심 부분이다. 따라서, 본 실시예는 더 이상 단일 음색 또는 단일 스타일의 음성을 합성하는 것에 제한되지 않고, 크로스-언어, 크로스-스타일, 크로스 -음색의 음성 합성을 구현할 수 있다. 예를 들어, 가수A가 유머스러운 스타일로 영어 세그먼트X를 방송하는 것 을 구현할 수 있고, 카툰 동물C가 슬픈 스타일로 중국어 세그먼트Y를 방송하는 것을 구현할 수도 있다. 도 3에 도시된 바와 같이, 상기 도 4에 도시된 상기 음성 합성 모델의 응용 아키텍처 개략도와 비교하면, 이 음 성 합성 모델은 훈련 과정에서 훈련 효과를 향상시키기 위해 스타일 추출기를 추가했다. 그러나, 사용 과정에는 이 스타일 추출기가 필요 없어 도 4에 도시된 바와 같은 아키텍처를 직접 사용한다. 도 3에 도시된 바와 같이, 이 스타일 추출기는 스타일 벡터를 텍스트 차원으로 압축하기 위해, 참조 스타일 인코더, 참조 콘텐츠 인코더 및 어텐션 메커니즘(Attention Mechanism) 모듈을 포함할 수 있고, 획득된 목표 훈련 스타일 인코딩 특징은 스 타일 인코더의 학습 목표이다. 여기서, 콘텐츠 인코더는 잔여 연결을 가지는 복수 층의 합성곱 신경망 네트워크(Convolutional Neural Networks: CNN) 및 한 층의 양방향 장단기 메모리 네트워크(Long Short-Term Memory: LSTM)로 구성된다. 음색 인코더는 복수 층의 CNN 및 한 층의 게이트 순환 유닛(Gated Recurrent Unit: GRU)으로 구성된다. 디코더는 어 텐션 메커니즘에 기반한 자동 회귀 구조이다. 스타일 인코더는 복수 층의 CNN 및 복수 층의 양방향 GRU로 구성 된다. 예를 들어, 도 5는 본 실시예에 따른 음성 합성 모델 중의 스타일 인코더의 개략도이다. 도 5에 도시된 바와 같이, N층의 CNN 및 N층의 GRU의 경우를 예로 들어, 스타일 인코더가 인코딩할 경우, 처리될 텍스트의 콘 텐츠 정보에 대해, 예를 들어 처리될 텍스트가 중국어인 경우, 상기 콘텐츠 정보는 음조를 가진 음자 시퀸스일 수 있고, 직접 CNN에 입력될 수 있으며, 스타일 ID와 같은 스타일 정보는 직접 GRU에 입력되며, 스타일 인코더 의 인코딩을 거쳐 최종적으로 스타일 인코딩 특징을 출력할 수 있고, 입력된 것은 음조를 가진 음자 시퀸스이므 로, 스타일 인코딩 시퀸스라고도 할 수 있다. 구체적으로, 훈련 단계에서, 스타일 추출기는 비지도 방식으로 스타일 표현을 학습하고, 이 스타일 표현은 또한 스타일 인코더의 목표 구동 스타일 인코더로 학습된다. 음성 합성 모델에 대한 훈련이 끝나면, 스타일 인코더는 스타일 추출기와 동일한 기능을 가진다. 응용 단계에서, 스타일 인코더는 스타일 추출기를 대체한다. 따라서, 스타일 추출기는 훈련 단계에만 존재한다. 유의해야 할 것은, 스타일 추출기의 강력한 효과에 기인하여 전체 음 성 합성 모델은 양호한 분리 성능을 가지고 있으며, 즉, 콘텐츠 인코더, 스타일 인코더, 음색 인코더는 각자 맡 은바 소임을 실행하고, 업무 분할이 명확하다. 콘텐츠 인코더는 어떻게 발음하는지를 담당하고, 스타일 인코더는 발음의 스타일을 담당하며, 음색 인코더는 누구의 음색으로 발음할 것인지를 담당한다. 단계S204에서는, 음성 합성 모델 중의 디코더를 사용하여, 훈련 콘텐츠 인코딩 특징, 목표 훈련 스타일 인코딩 특징 및 훈련 음색 인코딩 특징에 기반하여, 디코딩하여 훈련 텍스트의 예측 음향학 특징 정보를 생성한다. 단계S205에서는, 훈련 스타일 인코딩 특징, 목표 훈련 스타일 인코딩 특징, 예측 음향학 특징 정보 및 목표 음 향학 특징 정보에 기반하여, 종합 손실 함수를 구축한다. 예를 들어, 상기 단계의 구현 방법은 구체적으로는 하기 단계를 포함할 수 있다. （a1）훈련 스타일 인코딩 특징 및 목표 훈련 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축한다. （b1）예측 음향학 특징 정보 및 목표 음향학 특징 정보에 기반하여, 음향학 특징 손실 함수를 구축한다. （c1）스타일 손실 함수 및 재구성 손실 함수에 기반하여, 종합 손실 함수를 생성한다. 구체적으로, 스타일 손실 함수와 재구성 손실 함수에 대한 특정 가중치를 구성할 수 있고, 스타일 손실 함수와 재구성 손실 함수의 가중 합계를 최종 종합 손실 함수로 결정할 수 있다. 구체적인 가중치의 비율은 실제 수요 에 따라 설정할 수 있다. 예를 들어, 스타일을 강조하려는 경우, 상대적으로 큰 가중치를 설정할 수 있다. 예를 들어, 재구성 손실 함수의 가중치를 1로 설정할 경우, 스타일 손실 함수의 가중치를 1-10 사이의 특정 값으로 설정할 수 있으며, 값이 클수록, 스타일 손실 함수의 비중이 크고, 훈련 중 스타일이 전체에 대한 영향이 더 크 다. 단계S206에서는, 종합 손실 함수가 수렴되는지를 판단하고, 수렴되지 않은 경우, 단계S207을 실행하고, 수렴되 면, 단계S208을 실행한다. 단계S207에서는, 종합 손실 함수가 수렴되는 방향으로 향하도록, 콘텐츠 인코더, 스타일 인코더, 음색 인코더, 스타일 추출기 및 디코더의 파라미터를 조정하고, 단계S202로 돌아가서 다음 훈련 데이터를 획득하고 계속 훈련 한다. 단계S208에서는, 연속적인 미리 설정된 차례수의 훈련 중, 종합 손실 함수가 항상 수렴되는지를 판단하고, 수렴 되지 않은 경우, 단계S202로 돌아가서 다음 훈련 데이터를 획득하고 계속 훈련하며, 반면에, 항상 수렴되면, 음 성 합성 모델의 파라미터를 결정확인하여, 나아가 음성 합성 모델을 결정하고 훈련을 종료한다. 이 단계는 훈련 종료의 조건으로 할 수 있고, 연속적인 미리 설정된 차례수는 연속적인 100차례, 200차례 또는 다른 차례수와 같이, 실제 수요에 따라 설정될 수 있다. 미리 설정된 차례수의 연속 훈련 중, 종합 손실 함수가 항상 수렴되면, 상기 음성 합성 모델이 이미 완벽하게 훈련되었음을 의미하고, 훈련을 종료해도 된다. 또한, 선 택적으로, 실제 훈련에서, 음성 합성 모델은 수렴으로 무한히 향하지만, 미리 설정된 차례수의 연속 훈련에서 절대적으로 수렴되지 않은 가능성도 있고, 이때, 미리 설정된 차례수 임계치를 설정하여 훈련 종료의 조건으로 할 수 있으며, 훈련 차례수가 미리 설정된 차례수 임계치에 달하면, 훈련은 종료되고, 훈련 종료 시의 음성 합 성 모델의 파라미터를 획득하여, 상기 음성 합성 모델의 최종 파라미터로 결정할 수 있고, 상기 음성 합성 모델 은 최종 파라미터에 기반하여 사용될 수 있다. 반면에, 훈련 차례수가 미리 설정된 차례수 임계치에 달할 때까 지 계속 훈련한다. 상기 단계S202 내지 단계S207은 상기 도 1에 도시한 실시예의 단계S102의 하나의 구현 방식이다. 본 실시예는 훈련 과정에서 음성 합성 모델 중의 각 유닛을 설명하였지만, 전체 음성 합성 모델의 훈련 과정은 종단간(End-to-End) 훈련이다. 상기 음성 합성 모델의 훈련에서, 총 두 부분의 손실 함수를 포함한다. 하나는 디코더의 출력에 기반하여 구성된 재구성 손실 함수이고, 다른 하나는 스타일 인코더의 출력과 스타일 추출기의 출력에 기반하여 구성된 스타일 손실 함수이다. 두 부분의 손실 함수은 모두 L2 노름(norm)의 손실 함수를 사용 할 수 있다. 본 실시예의 음성 합성 모델의 훈련 방법은, 상기 방안을 통하여, 훈련 과정에 콘텐츠, 스타일 및 음색의 완전 한 분리를 효과적으로 보장할 수 있어, 나아가, 훈련된 음성 합성 모델은 크로스-스타일, 코르스-음색, 크로스- 언어의 음성 합성을 구현할 수 있고, 음성 합성의 다양성을 최대적으로 풍부화시키고, 장기간 방송으로 인한 지 루함을 감소시킬 수 있으며, 사용자의 체험을 현저히 향상시킬 수 있다. 상기 설명은, 훈련 데이터 세트 중의 훈련 데이트를 사용하여 음성 합성 모델을 훈련하는 원리에 대한 설명이다. 또한, 상기 훈련된 음성 합성 모델이 음성 합성할 경우에는, 훈련 세트에 합성된 음색, 스타일 및 언어의 기초에, 크로스-언어, 크로스-스타일 및 크로스-음색의 음성 합성만 구현하는 것을 지원할 수 있다. 추가 적으로, 훈련 데이터 세트 외의 새로운 스타일, 새로운 음색을 사용하여 음성 방송을 진행하기 위해, 특히 크로 스-스타일, 크로스-음색 합성이 일반 사용자를 향하는 경우, 일반 사용자들이 자신의 스타일 및 음색을 사용하 여 음성 방송하는 것을 구현할 수 있도록, 음성 합성 모델에 자신이 원하는 음색 및/또는 스타일을 추가 등록하 는 것이 필요하다. 도 6은 본 출원에 따른 제3 실시예의 개략도이다. 도 6에 도시된 바와 같이, 본 실시예에 따른 음성 합성 모델 의 속성 등록 방법은, 구체적으로는 하기 단계들을 포함할 수 있다. 단계S601에서는, 등록될 속성에 대응하는 복수의 등록 데이터를 수집한다. 단계S602에서는, 상기 속성에 대응하는 복수의 등록 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록한 다. 상기 음성 합성 모델은 이미 사전에 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것이다. 본 출원의 실시예에 따른 음성 합성 모델의 속성 등록 방법의 실행 주체는 음성 합성 모델의 속성 등록 장치이 다. 상기 장치는 전자 엔티티일 수도 있고, 또는, 애플리케이션으로 집적된 엔티티로서 속성을 음성 합성 모델 에 등록하기 위해 사용할 수도 있다. 본 실시예에 따른 속성 등록은 음색 등록 및/또는 스타일 등록을 포함할 수 있다. 본 실시예의 속성 등록 과정 은 훈련 과정과 유사하며, 즉, 이 속성에 대응하는 복수의 데이터를 사용하여 사전에 훈련 데이터 세트 중의 훈 련 데이터에 의해 훈련된 음성 합성 모델을 미세 조정하는 과정이다. 이로써, 음성 합성 모델에 음색 및/또는 스타일과 같은 대응하는 속성들을 등록하고, 나아가, 음성 합성 모델로 등록된 음색 및/또는 스타일을 사용하여 음성 합성을 실행할 수 있다. 본 실시예의 속성 등록 과정에서, 등록될 속성의 데이터량에 대한 요구는, 음성 합성 모델의 훈련 데이터 세트 중의 훈련 데이터의 데이터량과 다르다. 예를 들어, 음성 합성 모델을 성공적으로 훈련하려면, 훈련 데이터 세 트 중의 훈련 데이터의 수량은 백만 자릿수 또는 더 큰 자릿수에 달해야 한다. 실험 검증을 통해, 음성 합성 모 델에 음색 및/또는 스타일 속성을 등록할 경우, 해당 속성의 등록 데이터는 소량의 데이터만 수집하면 된다. 예 를 들어, 일부 장면에서, 등록을 실행하는데 10마디의 등록 데이터만 필요하다. 본 실시예에 따른 음성 합성 모델의 속성 등록 방법은, 등록될 속성에 대응하는 복수의 데이터를 수집하고, 상 기 속성에 대응하는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하며, 상기 음성 합성 모델 은 이미 사전에 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것이며, 음성 합성 모델에 속성을 등록 하는 것을 구현할 수 있어, 나아가 음성 합성 모델로 하여금 등록된 속성에 기반하여 음성 합성을 실행 가능하 게 할 수 있고, 사용자의 맞춤형 요구를 충족시킬 수 있다. 또한, 사용자 장면의 음성 합성의 다양성과 흥미성 을 최대적으로 풍부화시키고, 제품의 근접성과 의인화를 향상하며, 사용자의 사용 체험을 현저하게 향상시킬 수 있다. 또한, 본 출원의 실시예에 따른 기술 방안은 다양한 맞춤형 음성 인터랙션 장면에 적용될 수 있으며, 보 편적인 보급성을 갖고 있다. 도 7은 본 출원에 따른 제4 실시예의 개략도이다. 도 7에 도시된 바와 같이, 본 실시예에 따른 음성 합성 모델 의 속성 등록 방법은, 상기 도 6에 도시된 실시예의 기술 방안의 기초에, 음성 합성 모델에 음색을 등록하는 것 을 예로 들어, 본 출원의 기술 방안을 보다 더 구체적으로 설명한다. 도 7에 도시된 바와 같이, 본 실시예에 따 른 음성 합성 모듈의 속성 등록 방법은, 구체적으로는 하기 단계들을 포함할 수 있다. 단계S701에서는, 등록될 음색에 대응하는 복수의 등록 데이터를 수집하고, 각 등록 데이터는 텍스트 콘텐츠, 등 록될 음색의 음색 정보, 텍스트 콘텐츠의 오디오 정보, 및 등록될 음색을 사용하여 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보를 포함한다. 구체적으로, 본 실시예에 따르면, 각 등록 데이터는 동일한 음색을 등록하는데 사용되어, 각 등록 데이터에 포 함되는 등록될 음색의 음색 정보는 동일하다. 또한, 도 3에 도시된 바와 같은 음성 합성 모델의 훈련 아키텍처 개략도를 참조하여 본 실시 예실시예에 따른 음색 등록 방법을 설명할 수 있다. 도 3에 도시된 바와 같이, 음색 을 등록하는 경우, 스타일을 고려하지 않아도 되므로, 데이터 수집 시, 스타일 인코더와 관련된 입력 정보를 고 려하지 않아도 된다. 따라서, 본 실시예에서 수집된 등록 데이터는 등록될 음색의 음색 정보를 포함할 수 있고, 예를 들어, 상기 등록될 음색의 음색 정보는 등록될 음색의 사용자의 한마디 음성으로부터 대응하는 멜(mel) 스 펙트럼 시퀸스를 추출할 수 있다. 구체적으로, 상기 한 마디의 음성은 등록될 음색의 사용자가 텍스트 콘텐츠 또는 다른 콘텐츠에 대한 표현일 수 있다. 상기 등록될 음색의 음색 정보는 음색 인코더의 입력으로 사용된다.또한, 음성 합성 모델의 정상적인 작업을 확보하기 위해서는 스타일 추출기 및 콘텐츠 인코더의 입력 정보도 고 려해야 한다. 여기서, 콘텐츠 인코더의 입력은 텍스트 콘텐츠이다. 스타일 추출기의 입력은 텍스트 콘텐츠 및 텍스트 콘텐츠의 오디오 정보이며, 여기서는 스타일 정보를 고려하지 않는다. 구체적으로, 상기 텍스트 콘텐츠 의 오디오 정보는 멜(mel) 스펙트럼 시퀸스의 형태일 수 있다. 여기서, 등록 데이터 중의 등록될 음색을 사용하여 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보는, 상기 음색에 의해 표현된 텍스트 콘텐츠의 오디오로부터 추출된 멜(mel) 스펙트럼 시퀸스일 수 있고, 지도 훈련을 구 현하는데 사용된다. 상기 단계S701은, 상기 도 6에 도시된 실시예의 단계S601의 하나의 구현 방식이다. 단계S702에서는, 각 텍스트 콘텐츠, 각 텍스트 콘텐츠의 오디오 정보, 음색 정보에 대해, 음색 합성 모델 중의 콘텐츠 인코더, 스타일 추출기, 음색 인코더 및 디코더를 사용하여, 예측 음향학 특징 정보를 생성한다. 예를 들어, 구체적으로는 도 3에 도시된 훈련 구조를 참조할 수 있고, 콘텐츠 인코더는 입력된 텍스트 콘텐츠를 인코딩하여 콘텐츠 인코딩 특징을 획득할 수 있다. 음색 인코더는 입력된 음색 정보를 인코딩하여 음색 인코딩 특징을 획득한다. 스타일 추출기는 입력된 텍스트 콘텐츠 및 텍스트 콘텐츠의 오디오 정보에 기반하여, 목표 스 타일 인코딩 특징을 추출한다. 여기서, 목표 스타일 인코딩 특징으로 표현하지만, 이때 그 특징은 스타일과 관 련이 없으며, 단지 상기 스타일 추출기에 의해 추출된 특징을 참조하여 명명된 것 뿐이다. 디코더는 콘텐츠 인 코더에 의해 획득된 콘텐츠 인코딩 특징, 음색 인코더에 의해 획득된 음색 인코딩 특징 및 스타일 추출기에 의 해 추출된 목표 스타일 인코딩 특징을 모아 이은 종합 특징에 기반하여, 디코딩하여 예측 음향학 특징 정보를 획득할 수 있다. 마찬가지로, 상기 예측 음향학 특징 정보는 또한 멜(mel) 스펙트럼 시퀸스 형태를 사용할 수 있고, 즉, 예측된 상기 음색을 사용하여 텍스트 콘텐츠를 표현하는 오디오에서 추출된 멜(mel) 스펙트럼 시퀸스 를 의미한다. 단계S703에서는, 예측 음향학 특징 정보 및 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축한다. 단계S704에서는, 재구성 손실 함수가 수렴되는 방향으로 향하도록, 음성 합성 모델 중의 음색 인코더와 디코더 의 파라미터를 조정한다. 선택적으로, 단계S704를 실행하기 전, 또한 재구성 손실 함수가 수렴되는지를 판단할 수 있고, 수렴되지 않은 것으로 확인된 경우, 단계S704를 실행한다. 또한, 수집된 등록 데이터의 수량은 백 자릿수, 심지어 백 이하와 같이 매우 적기 때문에, 소량의 등록 데이터는 음성 합성 모델을 조정할 때, 이론적으로 모델로 하여금 빠르게 수렴하도록 하지 않을 것이다. 본 실시예의 음성 등록은, 훈련 데이터 세트를 사용하여 사전에 훈련된 음성 합 성 모델에 등록하는 점을 고려하여, 상기 과정은 단지 음성 합성 모델에 대해 미세 조정만을 하므로, 손실 함수 의 수렴 여부에 대해 판단하지 않고, 음색 합성 모델 중의 음색 인코더와 디코더의 파라미터를 직접 조정하여, 재구성 손실 함수가 수렴되도록 하면 된다. 상기 단계S702 내지 단계S704는, 상기 도 6에 도시된 실시예의 단계S602의 하나의 구현 방식이다. 본 실시예에 의한 상기 모드는 단지 사용자의 음색만 등록하는데, 즉, 스타일과 관련 없이 사용자의 음색만 필 요하다. 전형적인 응용 장면은 사용자가 훈련 데이터 세트 중의 사람의 말을 모방하는 것을 포함한다. 일반적으 로, 훈련 데이터 세트 중의 사람의 스타일은 스토리 스타일, 뉴스 스타일, 감정 스타일 등과 같이 판이하다. 사 용자가 다양한 스타일로 방송하기를 원하는 경우, 상기 사용자의 \"일인 천면\" 합성을 실현하기 위해, 즉, 사용 자는 훈련 데이터 세트 중의 임의의 스타일을 모방하기 위해, 상기 사용자의 음색만 등록하면 된다. 본 실시예에 따른 음성 합성 모델의 속성 등록 방법에 따르면, 상기 방법을 통해 음색을 음성 합성 모델에 등록 할 수 있어, 나아가 음성 합성 모델로 하여금 등록된 음색에 기반하여 음성 합성을 실행 가능하게 할 수 있고, 사용자의 맞춤형 요구를 충족시킬 수 있다. 또한, 사용자 장면의 음성 합성의 다양성과 흥미성을 최대적으로 풍 부화시키고, 제품의 근접성과 의인화를 향상하며, 사용자의 사용 체험을 현저하게 향상시킬 수 있다. 도 8은 본 출원에 따른 제5 실시예의 개략도이다. 도 8에 도시된 바와 같이, 본 출원의 실시예에 따른 음성 합 성 모델의 속성 등록 방법은, 상기 도 6에 도시된 실시예의 기술 방안의 기초에, 음성 합성 모델에 스타일을 등 록하는 것을 예로 들어, 본 출원의 기술 방안을 보다 더 구체적으로 설명한다. 도 8에 도시된 바와 같이, 본 실 시예에 따른 음성 합성 모듈의 속성 등록 방법은, 구체적으로는 하기 단계들을 포함할 수 있다. 단계S801에서는, 등록될 스타일에 대응하는 복수의 등록 데이터를 수집하고, 각 등록 데이터는 텍스트 콘텐츠, 등록될 스타일에 대해 사전 정의된 스타일 식별자 및 스타일을 사용하여 텍스트 콘텐츠를 표현하는 오디오 정보를 포함한다. 구체적으로, 본 실시예에 따르면, 각 등록 데이터는 동일한 스타일을 등록하는데 사용되므로, 각 등록 데이터에 포함되는 등록될 스타일에 대해 사전 정의된 스타일 식별자는 동일하다. 또한, 도 3에 도시된 음성 합성 모델의 훈련 아키텍처 개략도를 참조하여 본 실시예에 따른 스타일 등록 방법을 설명할 수 있다. 도 3에 도시된 바와 같이, 스타일을 등록하는 경우, 음색을 고려하지 않아도 되므로, 데이터 수집 시, 음색 인코더와 관련된 입력 정보를 고려하지 않아도 된다. 따라서, 본 실시예에 의해 수집된 등록 데이터에는 텍스트 콘텐츠, 등록될 스타 일에 대해 사전 정의된 스타일 식별자, 및 스타일을 사용하여 텍스트 콘텐츠를 표현하는 오디오 정보를 포함할 수 있고, 예를 들어, 스타일을 사용하여 텍스트 콘텐츠를 표현하는 오디오 정보는 등록될 스타일을 사용하여 상 기 텍스트 콘텐츠를 표현하는 음성으로부터 대응하는 멜(mel) 스펙트럼 시퀸스를 추출하고, 텍스트 콘텐츠와 함 께 스타일 인코더의 입력으로 사용될 수 있다. 상기 단계S801은, 상기 도 6에 도시된 실시예의 단계S601의 하나의 구현 방식이다. 단계S802에서는, 각 등록 데이터 중의 텍스트 콘텐츠, 스타일 식별자 및 스타일을 사용하여 대응하는 텍스트 콘 텐츠를 표현하는 오디오 정보에 대해, 음색 합성 모델의 스타일 추출기 및 스타일 인코더를 사용하여, 각각 목 표 스타일 인코딩 특징 및 예측 스타일 인코딩 특징을 생성한다. 예를 들어, 구체적으로는 도 3에 도시된 훈련 구성을 참조할 수 있고, 스타일 인코더는 입력된 텍스트 콘텐츠와 스타일 식별자에 기반하여 인코딩하여 예측 스타일 인코딩 특징을 획득한다. 스타일 추출기는 입력된 텍스트 콘 텐츠와 스타일을 사용하여 대응하는 텍스트 콘텐츠를 표현하는 오디오 정보에 기반하여, 목표 스타일 인코딩 특 징을 추출한다. 단계S803에서는, 예측 스타일 인코딩 특징 및 목표 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축한 다. 단계S804에서는, 스타일 손실 함수가 수렴되는 방향으로 향하도록, 음성 합성 모델 중의 스타일 추출기와 스타 일 인코더의 파라미터를 조정한다. 마찬가지로, 선택적으로, 단계S804를 실행하기 전, 또한 스타일 손실 함수가 수렴되는지를 판단할 수 있고, 수 렴되지 않은 것으로 확인된 경우, 단계S804를 실행한다. 또한, 수집된 등록 데이터의 수량은 백 자릿수, 심지어 백 이하와 같이 매우 적기 때문에, 소량의 등록 데이터는 음성 합성 모델을 조정할 때, 이론적으로 모델로 하여 금 빠르게 수렴하게 하지 않을 것이다. 본 실시예의 스타일 등록은, 훈련 데이터 세트를 사용하여 사전에 훈련 된 음성 합성 모델에 등록하는 점을 고려하여, 이 과정은 단지 음성 합성 모델에 대해 미세 조정만을 하므로, 스타일 손실 함수가 수렴되는지에 대해 판단하지 않고, 음색 합성 모델 중의 스타일 추출기와 스타일 인코더의 파라미터를 직접 조정하여, 스타일 손실 함수가 수렴하도록 하면 된다. 그러나, 설명해야 할 것은, 음성 합성 모델의 훈련 및 사용과 같이, 음성 합성 모델을 사용하는 경우, 스타일 인코더만을 사용하고, 스타일 추출기는 사용되지 않는다. 상기 단계S802 내지 단계S804는, 상기 도 6에 도시된 실시예의 단계S602의 하나의 구현 방식이다. 본 실시예에 의한 상기 모드는 단지 사용자의 스타일만을 등록하는데, 즉, 음색과 관련 없이 사용자의 스타일만 필요하다. 전형적인 응용 장면은 훈련 데이터 세트 중의 사람이 사용자를 모방하여 말하게 하는 것을 포함한다. 일반적으로, 훈련 데이터 중의 사람은 다양한 음색을 가지며, 예를 들어, 발음자마다 하나의 음색을 갖고 있고, 맑고 깨끗하거나, 묵직하거나, 달콤하거나, 성숙하는 등이다. 사용자는 이렇게 다양한 음색이 자신의 스타일로 방송하기를 원하는 경우, 상기 사용자의 \"천인 일면\" 합성을 실현하기 위해, 즉, 훈련 데이터 세트 중의 사람이 사용자의 스타일을 모방하게 하기 위해, 상기 사용자의 스타일만 등록하면 된다. 본 실시예에 따른 음성 합성 모델의 속성 등록 방법에 따르면, 상기 방법을 통해 음성 합성 모델에 스타일을 등 록하는 것을 구현할 수 있어, 나아가 음성 합성 모델로 하여금 등록된 스타일에 기반하여 음성 합성을 실행 가 능하게 할 수 있고, 사용자의 맞춤형 요구를 충족시킬 수 있다. 또한, 사용자 장면의 음성 합성의 다양성과 흥 미성을 최대적으로 풍부화시키고, 제품의 근접성과 의인화를 향상하며, 사용자의 사용 체험을 현저하게 향상시 킬 수 있다. 도 9는 본 출원에 따른 제6 실시예의 개략도이다. 도 9에 도시된 바와 같이, 본 출원의 실시예에 따른 음성 합 성 모델의 속성 등록 방법은, 상기 도 6에 도시된 실시예의 기술 방안의 기초에, 음성 합성 모델에 동시에 스타 일과 음색을 등록하는 것을 예로 들어, 본 출원의 기술 방안을 보다 더 구체적으로 설명한다. 도 9에 도시된 바와 같이, 본 실시예에 따른 음성 합성 모듈의 속성 등록 방법은, 구체적으로는 하기 단계들을 포함할 수 있다. 단계S901에서는, 등록될 스타일 및 등록될 스타일에 대응하는 복수의 등록 데이터를 수집하고, 각 등록 데이터 는 텍스트 콘텐츠, 등록될 스타일에 대해 사전 정의된 스타일 식별자, 등록될 음색의 음색 정보, 스타일을 사용 하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보, 및 등록될 음색 및 등록될 스타일을 사용하여 텍스트 콘텐츠 를 표현하는 목표 음향학 특징 정보를 포함한다. 구체적으로, 본 실시예에서 음색 및 스타일에 대한 동시 등록이 구현된다. 여기서 각 파라미터에 대한 해석은 상기 도 7과 도 8에 도시된 실시예에 관련한 파라미터들의 해석을 참조할 수 있다. 또한, 상기 도 7과 도 8에 도시된 실시예를 함께 결합하여, 음색과 스타일의 동시 등록을 구현할 수도 있다. 또는, 이 음색과 스타일의 동 시 등록 과정은 상기 도 2에 도시된 실시예에서 설명된 모델 훈련 과정과 동일하므로, 상세한 과정은 또한 상기 도 2에 도시된 실시예의 설명을 참조할 수 있다. 단계S902에서는, 각 등록 데이터 중의 텍스트 콘텐츠, 스타일을 사용하여 텍스트 콘텐츠를 표현하는 오디오 정 보, 스타일 식별자, 음색 정보에 대해, 음색 합성 모델 중의 콘텐츠 인코더, 스타일 인코더, 음색 인코더, 스타 일 추출기 및 디코더를 사용하여, 목표 스타일 인코딩 특징, 예측 스타일 인코딩 특징 및 예측 음향학 특징 정 보를 생성한다. 여기서, 선택적으로, 목표 스타일 인코딩 특징은 스타일 추출기에 의해 추출된 것이고, 예측 스타일 인코딩 특 징은 스타일 인코더에 의해 인코딩하여 획득된 것이며, 예측 음향학 특징 정보는 디코더에 의해 디코딩하여 획 득된 것이다. 이 단계의 구체적인 구현 과정은 상기 도 7 및 도 8에 도시된 실시예의 관련 설명을 참조할 수 있 으며, 여기서는 설명을 생략한다. 단계S903에서는, 목표 스타일 인코딩 특징, 예측 스타일 인코딩 특징, 예측 음향학 특징 정보 및 목표 음향학 특징 정보에 기반하여, 종합 손실 함수를 구축한다. 예를 들어, 상기 단계의 구체적인 구현 시, 상기 도 2에 도시된 실시예의 단계S205 중의 종합 손실 함수의 구축 과정을 참조할 수 있고, 여기서는 설명을 생략한다. 단계S904에서는, 종합 손실 함수가 수렴되는 방향으로 향하도록, 음성 합성 모델 중의 음색 인코더, 스타일 인 코더, 스타일 추출기 및 디코더의 파라미터를 조정한다. 마찬가지로, 선택적으로, 단계S904를 실행하기 전, 또한 스타일 손실 함수가 수렴되었는지를 판단하는 것을 포 함할 수 있고, 수렴되지 않은 것으로 확인된 경우, 단계S904를 실행한다. 또한, 수집된 등록 데이터 수량은 백 자릿수, 심지어 백 이하와 같이 매우 적은 데이터일 수 있으므로, 소량의 등록 데이터는 음성 합성 모델을 조정 할 때, 이론적으로 모델이 빠르게 수렴되도록 하지 않을 것이다. 본 실시예의 스타일 등록과 음색 등록은, 훈련 데이터 세트를 사용하여 사전 훈련된 음성 합성 모델에 등록하는 점을 고려하여, 이 등록 과정은 단지 음성 합 성 모델에 대해 미세 조정만을 하므로, 종합 손실 함수의 수렴 여부에 대해 판단하지 않고, 음색 합성 모델 중 의 음색 인코더, 스타일 인코더, 스타일 추출기 및 디코더의 파라미터를 직접 조정하여, 종합 손실 함수로 하여 금 수렴되는 방향으로 향하게 하면 된다. 마찬가지로, 음성 합성 모델의 훈련 및 사용과 같이, 음성 합성 모델 을 사용하는 경우, 스타일 인코더만을 사용하고, 스타일 추출기는 사용되지 않는다. 상기 단계S902 내지 단계S904는, 상기 도 6에 도시된 실시예의 단계S602의 하나의 구현 방식이다. 본 실시예에 의한 상기 모드는 사용자의 음색과 스타일을 동시에 등록한다. 전형적인 응용 장면은 사용자가 훈 련 데이터 세트 중의 사람을 모방하여 말할 수도 있고, 훈련 데이터 중의 사람이 사용자를 모방하여 말하게 할 수도 있으며, 동시에, 사용자 자신의 음색, 자신의 스타일로 방송하는 것도 지원한다. 이 모드는 상기 도 7에 도시된 음색 등록 모드와 도 8에 도시된 스타일 등록 모드에 대한 종합이고 일반화이며, 상기 사용자의 \"일인 천면\", \"천인 일면\"을 동시에 구현할 수 있으며, 즉, 훈련 데이터 세트 중의 사람과 사용자 사이의 상호 모방을 구현한다. 본 실시예에 따른 음성 합성 모델의 속성 등록 방법에 따르면, 상기 방법을 통해 음성 합성 모델에 음색 등록 및 스타일을 등록하는 것을 구현할 수 있어, 나아가 음성 합성 모델로 하여금 등록된 음색 및 스타일에 기반하 여 음성 합성을 실행 가능하게 할 수 있고, 사용자의 맞춤형 요구를 충족시킬 수 있다. 또한, 사용자 장면의 음 성 합성의 다양성과 흥미성을 최대적으로 풍부화시키고, 제품의 근접성과 의인화를 향상하며, 사용자의 사용 체 험을 현저하게 향상시킬 수 있다. 도 10은 본 출원에 따른 제7 실시예의 개략도이다. 도 10에 도시된 바와 같이, 본 출원의 실시예에 따른 음성 합성 모델의 속성 등록 장치는, 구체적으로는, 수집 모듈 및 등록 모듈을 포함할 수 있다. 수집 모듈은 등록될 속성에 대응하는 복수의 데이터를 수집하기 위한 것이다. 등록 모듈은 속성에 대응하는 복수의 데이터를 사용하여, 음성 합성 모델에 속성을 등록하기 위한 것이다. 음성 합성 모델은 이미 사전 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것이다. 본 실시예에 따른 음성 합성 모델의 속성 등록 장치는, 상기 모듈을 사용하여 음성 합성 모델의 속성 등 록의 구현 원리 및 기술 효과를 구현하고, 이는 상기 관련 실시예의 구현 방식과 동일하며, 상세한 설명은 상기 관련 방법 실시예를 참조할 수 있으며, 여기서는 설명을 생략한다. 도 11은 본 출원에 따른 제8 실시예의 개략도이다. 도 11에 도시된 바와 같이, 본 실시예에 따른 음성 합성 모 델의 속성 등록 장치는, 상기 도 10에 도시된 실시예의 기술 방안의 기초에, 본 출원의 기술 방안을 보다 더 구체적으로 설명하기 위한 것이다. 본 실시예에 따른 음성 합성 모듈의 속성 등록 장치에 있어서, 수집 모듈은, 속성에 음색이 포함되는 경우, 등록될 음색에 대응하는 복수의 등록 데이터를 수집하기 위한 것이고, 각 등록 데이터는 텍스트 콘텐츠, 등록될 음색의 음색 정보, 텍스트 콘텐츠의 오디오 정보, 및 등록될 음색을 사용하여 텍스트 콘텐츠를 표현하는 목표 음향학 특징 정보를 포함한다. 나아가 선택적으로, 도 11에 도시된 바와 같이, 본 실시예에 따른 음성 합성 모듈의 속성 등록 장치에 있 어서, 등록 모듈은, 생성부(1002a), 구축부(1002b) 및 조정부(1002c)를 포함할 수 있다. 생성부(1002a)는 각 등록 데이터 중의 텍스트 콘텐츠, 텍스트 콘텐츠의 오디오 정보, 음색 정보에 대해, 음성 합성 모델 중의 콘텐츠 인코더, 스타일 추출기, 음색 인코더 및 디코더를 사용하여, 예측 음향학 특징 정보를 생성하기 위한 것이다. 구축부(1002b)는, 예측 음향학 특징 정보 및 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하기 위한 것이다. 조정부(1002c)는 재구성 손실 함수가 수렴되는 방향으로 향하도록, 음성 합성 모델 중의 음색 인코더 및 디코더 의 파라미터를 조정하기 위한 것이다. 나아가 선택적으로, 수집 모듈은, 또한, 속성에 스타일이 포함되는 경우, 등록될 스타일에 대응하는 복수의 등록 데이터를 수집하고, 각 등록 데이터는 텍스트 콘텐츠, 등록될 스타일에 대해 사전 정의된 스타일 식별자 및 스타일을 사용하여 텍스트 콘텐츠를 표현 하는 오디오 정보를 포함한다. 대응하게, 생성부(1002a)는, 각 등록 데이터 중의 텍스트 콘텐츠, 스타일 식별자 및 스타일을 사용하여 대응하 는 텍스트 콘텐츠를 표현하는 오디오 정보에 대해, 음성 합성 모델 중의 스타일 추출기 및 스타일 인코더를 사 용하여, 각각 목표 스타일 인코딩 특징 및 예측 스타일 인코딩 특징을 생성하기 위한 것이다. 구축부(1002b)는, 또한 예측 스타일 인코딩 특징 및 목표 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축하기 위한 것이다. 조정부(1002c)는, 스타일 손실 함수가 수렴되는 방향으로 향하도록, 음성 합성 모델 중의 스타일 추출기 및 스 타일 인코더의 파라미터를 조정하기 위한 것이다. 나아가 선택적으로, 수집 모듈은, 또한, 등록될 스타일 및 등록될 스타일에 대응하는 복수의 등록 데이터를 수집하기 위한 것이고, 각 상기 등록 데이터 는 텍스트 콘텐츠, 등록될 스타일에 대해 사전 정의된 스타일 식별자, 등록될 음색의 음색 정보, 스타일을 사용 하여 상기 텍스트 콘텐츠를 표현하는 오디오 정보, 및 등록될 음색 및 등록될 스타일을 사용하여 텍스트 콘텐츠 를 표현하는 목표 음향학 특징 정보를 포함한다. 대응하게, 생성부(1002a)는, 또한 각 등록 데이터 중의 텍스트 콘텐츠, 스타일을 사용하여 텍스트 콘텐츠를 표 현하는 오디오 정보, 스타일 식별자, 음색 정보에 대해, 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더, 음색 인코더, 스타일 추출기 및 디코더를 사용하여, 목표 스타일 인코딩 특징, 예측 스타일 인코딩 특징 및 예 측 음향학 특징 정보를 생성하기 위한 것이다.구축부(1002b)는, 또한 목표 스타일 인코딩 특징, 예측 스타일 인코딩 특징, 예측 음향학 특징 정보 및 목표 음 향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하기 위한 것이다. 조정부(1002c)는, 또한 종합 손실 함수가 수렴되는 방향으로 향하도록, 음성 합성 모델 중의 음색 인코더, 스타 일 인코더, 스타일 추출기 및 디코더의 파라미터를 조정하기 위한 것이다. 나아가 선택적으로, 구축부(1002b)는, 구체적으로는, 목표 스타일 인코딩 특징 및 예측 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구축하고, 예측 음향학 특징 정보 및 목표 음향학 특징 정보에 기반하여, 재구성 손실 함수를 구축하며, 스타일 손실 함수 및 재구성 손실 함수에 기반하여, 종합 손실 함수를 구축하기 위한 것이다. 본 출원의 실시예에 따른 음성 합성 모델의 속성 등록 장치는, 상기 모듈을 사용하여 음성 합성 모델의 속성 등록의 구현 원리 및 기술 효과를 구현하고, 상기 관련 실시예의 구현 방식과 동일하며, 상세한 설명은 상 기 관련 방법 실시예를 참조할 수 있으며, 여기서는 설명을 생략한다. 본 출원의 실시예에 따르면, 전자 기기 및 판독 가능 저장 매체를 더 제공한다. 도 12에 도시된 바와 같이, 본 실시예에 따른 음성 합성 모델의 속성 등록 방법을 구현하기 위한 전자 기기의 블로도이다. 전자 기기는 예를 들어, 랩톱 컴퓨터, 데스크톱 컴퓨터, 워크스테이션, 개인용 디지털 비서 (Personal Digital Assistants: PDA), 서버, 블레이드 서버, 메인프레임 컴퓨터, 및 기타 적절한 컴퓨터와 같 은 다양한 형태의 디지털 컴퓨터를 의미한다. 전자 기기는 예를 들어, 개인용 디지털 비서(Personal Digital Assistants: PDA), 셀룰러 폰, 스마트 폰, 웨어러블 기기, 및 기타 유사한 컴퓨팅 기기와 같은 다양한 형태의 모바일 기기를 포함할 수 있다. 본 명세서에 기재된 부품, 이들의 연결 및 관계, 그리고 이들의 기능은 단지 예 시적인 것에 불과하며, 본 명세서에서 설명 및/또는 요구하는 본 출원의 범위를 한정하기 위한 것이 아니다. 도 12에 도시된 바와 같이, 상기 전자 기기는, 하나 또는 복수의 프로세서, 메모리, 및 각 부품을 연결하기 위한 인터페이스를 포함하고, 상기 인터페이스에는 고속 인터페이스 및 저속 인터페이스가 포함된다. 각 부품들은 서로 다른 버스를 통해 서로 연결되고, 공공 메인보드에 장착되거나 또는 수요에 따라 기타 방식으 로 장착될 수 있다. 프로세서는 전자 기기에서 실행되는 명령어들을 실행할 수 있고, 상기 명령어는 메모리에 저장되어 외부 입력/출력 장치(예를 들어, 인터페이스에 접속된 표시 장치)에 GUI의 그래픽 정보를 표시하기 위 한 명령어를 포함할 수 있다. 다른 실시예에서는, 수요에 따라 복수의 프로세서 및/또는 복수의 버스를 복수의 메모리와 함께 사용될 수 있다. 마찬가지로, 복수의 전자 기기를 연결하고, 각 전자 기기에 의해 일부 필요한 동작을 제공할 수 있다(예를 들어, 서버 어레이, 한 세트의 블레이드 서버, 또는 멀티 프로세서 시스템으로 한 다). 도 12에서는, 하나의 프로세서의 경우를 예로 들어 설명한다. 메모리는 본 출원에 의해 제공되는 비휘발성 컴퓨터 판독 가능 저장 매체이다. 여기서, 상기 메모리에는 적어도 하나의 프로세서에 의해 실행될 수 있는 명령어가 저장되어 있어, 상기 적어도 하나의 프로세서로 하여 금, 본 출원에 의해 제공되는 음성 합성 모델의 속성 등록 방법을 실행하게 한다. 본 출원의 비휘발성 컴퓨터 판독 가능 저장 매체에는 컴퓨터에 의해 실행될 경우 컴퓨터로 하여금 본 출원에 의해 제공되는 음성 합성 모델 의 속성 등록 방법을 실행하도록 하기 위한 컴퓨터 명령어가 저장되어 있다. 메모리는 비휘발성 컴퓨터 판독가능 저장 매체로서, 예를 들어, 본 출원의 실시예에 따른 음성 합성 모델 의 속성 등록 방법에 대응하는 프로그램 명령어/모듈(예를 들어, 도 10 및 도 11에 도시된 관련 모듈)과 같은 비휘발성 소프트웨어 프로그램, 비휘발성 컴퓨터 실행가능 프로그램 및 모듈을 저장할 수 있다. 프로세서(120 1)는 메모리에 저장된 비휘발성 소프트웨어 프로그램, 명령어 및 모듈을 실행함으로써, 서버의 다양한 기 능 및 데이터 처리를 실행한다. 즉, 상기 방법 실시예에 따른 음성 합성 모델의 속성 등록 방법을 구현한다. 메모리는 프로그램 저장 영역 및 데이터 저장 영역을 포함할 수 있다. 여기서, 프로그램 저장 영역은 OS 시스템 및 적어도 하나의 기능에 필요한 앱을 저장할 수 있고, 데이터 저장 영역은 음성 합성 모델의 속성 등록 방법을 구현하는 전자 기기의 사용에 따라 생성된 데이터 등을 저장할 수 있다. 또한, 메모리는 고속 RAM(Random Access Memory)를 포함할 수도 있고, 예를 들어, 적어도 하나의 디스크 저장 디바이스, 플래시 메모 리 디바이스, 또는 기타 비휘발성 고체 저장 디바이스와 같은 비휘발성 메모리를 포함할 수도 있다. 일부 실시 예에 따르면, 메모리는 프로세서에 대해 원격으로 설치된 메모리를 포함할 수 있고, 이러한 원격 메모리는 네트워크를 통해 전자 기기에 연결될 수 있다. 상기 네트워크의 실예로는 인터넷, 인트라넷, 근거리통신망(LAN), 이동 통신망 및 이들의 조합을 포함할 수 있는데, 이에 한정되지는 않는다. 음성 합성 모델의 속성 등록 방법을 구현하는 전자 기기는, 입력 장치 및 출력 장치를 더 포함할 수 있다. 프로세서, 메모리, 입력 장치 및 출력 장치는 버스 또는 기타 방식으로 연결 될 수 있는데, 도 12에서는 버스를 통해 연결되는 경우를 예로 한다. 입력 장치는 입력된 숫자 또는 문자 부호 정보를 수신할 수 있고, 또한 음성 합성 모델의 속성 등록 방법 을 구현하는 전자 기기의 사용자 설정 및 기능 제어와 연관된 키 신호 입력을 생성할 수 있으며, 예를 들어, 터 치 스크린, 키 패드, 마우스, 트랙 패드, 터치 패드, 인디케이터 로드, 하나 또는 복수의 마우스 버튼, 트랙 볼, 콘트롤러 로드 등과 같은 입력 장치를 포함할 수 있다. 출력 장치는 표시 장치, 보조 조명 장치(예를 들어, LED) 및 촉각 피드백 장치(예를 들어, 진동모터) 등을 포함할 수 있다. 상기 표시 장치는 액정 디스플레 이(LCD), 발광 다이오드(LED) 디스플레이 및 플라스마 디스플레이를 포함할 수 있는데, 이에 한정되지는 않는다. 일부 실시 형태에 따르면, 표시 장치는 터치 스크린일 수 있다. 여기서 설명한 시스템 및 기술의 다양한 실시 형태는 디지털 전자 회로 시스템, 집적 회로 시스템, 전용 ASIC (전용 집적 회로), 컴퓨터 하드웨어, 펌웨어, 소프트웨어, 및/또는 이들의 조합에서 구현될 수 있다. 이러한 다 양한 실시 형태는 하나 또는 복수의 컴퓨터 프로그램을 통해 구현되는 것을 포함할 수 있고, 상기 하나 또는 복 수의 컴퓨터 프로그램은 적어도 하나의 프로그램 가능 프로세서를 포함하는 프로그램 가능 시스템에서 실행 및/ 또는 해석될 수 있으며, 상기 프로그램 가능 프로세서는 전용 또는 범용 프로그램 가능 프로세서일 수 있고, 저 장 시스템, 적어도 하나의 입력 장치, 및 적어도 하나의 출력 장치로부터 데이터 및 명령어를 수신하고, 데이터 및 명령어를 저장 시스템, 적어도 하나의 입력 장치, 및 적어도 하나의 출력 장치로 송신할 수 있다. 이러한 컴퓨터 프로그램(프로그램, 소프트웨어, 소프트웨어 애플리케이션, 또는 코드라고도 함)은 프로그램 가 능 프로세서의 기계 명령어를 포함하고, 하이 라벨 프로세스 및/또는 객체 지향 프로그래밍 언어, 및/또는 어셈 블러/기계언어를 사용하여 이러한 컴퓨터 프로그램을 실시할 수 있다. 본 명세서에서 사용되는 \"기계 판독가능 매체 및 \"컴퓨터 판독가능 매체\" 등과 같은 용어는, 기계 명령어 및/또는 데이터를 프로그램 가능 프로세서의 임의의 컴퓨터 프로그램 제품, 기기, 및/또는 장치(예를 들어, 디스크, CD-ROM, 메모리, 프로그램 가능 논리 장 치(PLD))에 제공하기 의한 것이고, 기계 판독 가능 신호로서의 기계 명령어를 수신하는 기계 판독가능 매체를 포함한다. \"기계 판독가능 신호\"라는 용어는 기계 명령어 및/또는 데이터를 프로그램 가능 프로세서에 제공하기 위한 임의의 신호를 의미한다. 사용자와의 인터랙션을 제공하기 위해서는, 컴퓨터를 통해 본 명세서에서 설명한 시스템 및 기술을 구현할 수 있는데, 상기 컴퓨터는, 사용자에게 정보를 표시하기 위한 표시 장치(예를 들어, CRT(음극선관) 또는 LCD(액정 디스플레이) 모니터), 및 사용자가 상기 컴퓨터에 입력을 제공할 수 있는 키보드 및 포인팅 디바이스(예를 들어, 마우스 또는 트랙 볼)를 포함한다. 기타 유형의 디바이스도 사용자와의 인터랙션을 제공하기 위한 것일 수 있다. 예를 들어, 사용자에게 제공되는 피드백은 임의의 형태의 센싱 피드백(예를 들어, 시각 피드백, 청각 피드백, 또는 촉각 피드백)일 수 있고, 임의의 형태(소리 입력, 음성 입력, 또는 촉각 입력을 포함)로 사용자로 부터의 입력을 수신할 수 있다. 본 명세서에서 설명한 시스템 및 기술은, 백 그라운드 부품을 포함하는 컴퓨팅 시스템(예를 들어, 데이터 서 버), 또는 미들웨어 부품을 포함하는 컴퓨팅 시스템(예를 들어, 애플리케이션 서버), 또는 프론트 앤드 부품을 포함하는 컴퓨팅 시스템(예를 들어, GUI 또는 웹 브라우저를 갖는 사용자 컴퓨터이며, 사용자는 상기 GUI 또는 상기 웹 브라우저를 통하여 본 명세서에서 설명한 상기 시스템 및 기술의 실시 형태와 인터랙션을 할 수 있음), 또는 이러한 백 그라운드 부품, 미들웨어 부품, 또는 프론트 앤드 부품의 임의의 조합을 포함하는 컴퓨팅 시스 템에서 구현될 수 있다. 시스템의 부품은 임의의 형태 또는 매체의 디지털 데이터 통신(예를 들어, 통신 네트워 크)을 통해 서로 연결될 수 있다. 통신 네트워크는 예를 들어 근거리 통신망(LAN), 광역 통신망(WAN), 인터넷 및 블록 체인 네트워크를 포함할 수 있다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트 및 서버는 일반적으로 서로 멀리 떨어져 있 고, 통상적으로 통신 네트워크를 통해 인터랙션을 진행한다. 클라이언트와 서버의 관계는 대응하는 컴퓨터에서 실행되고 서로 클라이언트-서버의 관계를 갖는 컴퓨터 프로그램에 의해 생성된다. 서버는 클라우드 컴퓨팅 서버 또는 클라우드 호스트라고도 하는 클라우드 서버일 수 있고, 클라우드 컴퓨팅 서비스 체계 중 하나의 호스트 제 품으로서, 전통적인 물리적 호스트 및 VPS 서비스 (\"Virtual Private Server\"，또는 \"VPS\")에서 관리가 어렵고 업무 확장성이 약한 단점을 해결한다.본 출원의 실시예에 따른 기술 방안은, 등록될 속성에 대응하는 복수의 데이터를 수집하고, 상기 속성에 대응하 는 복수의 데이터를 사용하여, 상기 속성을 음성 합성 모델에 등록하며, 상기 음성 합성 모델은 이미 사전 훈련 데이터 세트 중의 훈련 데이터를 사용하여 훈련된 것임으로써, 음성 합성 모델에 속성을 등록하는 것을 구현할 수 있어, 음성 합성 모델로 하여금 등록된 속성에 기반하여 음성 합성을 실행 가능하게 할 수 있고, 사용자의 맞춤형 요구를 충족시킬 수 있다. 또한, 사용자 장면의 음성 합성의 다양성과 흥미성을 최대적으로 풍부화시키 고, 제품의 근접성과 의인화를 향상하며, 사용자의 사용 체험을 현저하게 향상시킬 수 있다. 또한, 본 출원의 실시예에 따른 기술 방안은 다양한 맞춤형 음성 인터랙션 장면에 적용될 수 있으며, 보편적인 보급성을 갖고 있 다. 상기에서 설명한 다양한 흐름을 사용하여 각 단계를 다시 순서 배열, 추가 또는 삭제할 수 있다는 점을 이해하 여야 한다. 예를 들어, 본 출원이 개시된 기술 방안이 원하는 결과를 구현할 수 있는 한, 본 출원에 기재된 다 양한 단계는 병렬적으로 또는 순차적으로, 또는 서로 다른 순서로 실행될 수 있고, 본 출원은 이에 대해 특별히 한정하지 않는다. 본 출원의 보호범위는 상기 다양한 실시 형태에 의해 제한되지 않는다. 당업자라면, 설계 요구 및 기타 요인에 의해, 다양한 수정, 조합, 서브 조합 및 교체가 이루어질 수 있음을 이해할 것이다. 본 출원의 취지 및 원칙 내 에서 이루어진 임의의 수정, 등가 교체 및 개선 등은 모두 본 출원의 보호범위에 속한다."}
{"patent_id": "10-2021-0117979", "section": "도면", "subsection": "도면설명", "item": 1, "content": "첨부된 도면은 본 출원을 보다 쉽게 이해하도록 하기 위한 것이고, 본 출원은 이에 한정되지 않는다. 도 1은 본 출원에 따른 제1 실시예의 개략도이다. 도 2는 본 출원에 따른 제2 실시예의 개략도이다. 도 3은 본 출원의 실시예에 따른 음성 합성 모델의 훈련 아키텍처 개략도이다.도 4는 본 출원의 실시예에 따른 음성 합성 모델의 응용 아키텍처 개략도이다. 도 5는 본 출원의 실시예에 따른 음성 합성 모델 중의 스타일 인코더의 개략도이다. 도 6은 본 출원에 따른 제3 실시예의 개략도이다. 도 7은 본 출원에 따른 제4 실시예의 개략도이다. 도 8은 본 출원에 따른 제5 실시예의 개략도이다. 도 9는 본 출원에 따른 제6 실시예의 개략도이다. 도 10은 본 출원에 따른 제7 실시예의 개략도이다. 도 11은 본 출원에 따른 제8 실시예의 개략도이다. 도 12는 본 출원의 실시예에 따른 음성 합성 모델의 속성 등록 방법을 구현하기 위한 전자 기기의 블로도이다."}
