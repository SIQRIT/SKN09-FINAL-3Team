{"patent_id": "10-2024-0174713", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0020375", "출원번호": "10-2024-0174713", "발명의 명칭": "3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템", "출원인": "주식회사 메이사", "발명자": "이용주"}}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "대상체를 촬영한 입력 이미지를 수신하는 데이터 수집 모듈;상기 입력 이미지로부터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 상기 누락 영역을 마스킹(Masking)처리함으로써 마스킹 영역이 포함된 마스킹 이미지를 생성하는 전처리 모듈;상기 입력 이미지 및 상기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하기 위한 대체 이미지를 생성하는 생성 모듈; 및상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성하는 후처리 모듈을 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 입력 이미지는, 상기 대상체를 다각도에서 촬영한 멀티뷰(Multi-view) 이미지를 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 전처리 모듈은,상기 입력 이미지를 기초로 생성된 사전 3D 모델로부터 상기 누락 영역을 식별하고,상기 누락 영역을 다각도에서 캡쳐(Capture)함으로써 복수의 상기 마스킹 이미지를 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,상기 전처리 모듈은,상기 사전 3D 모델로부터 음영(Shadow)이 존재하는 음영 영역을 식별하고,상기 식별된 음영 영역을 상기 누락 영역으로 결정하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 생성 모듈은,상기 마스킹 이미지 및 상기 마스킹 영역을 설명하는 텍스트 가이드(Text Guide)에 기초하여 제1 예비 이미지를공개특허 10-2025-0020375-3-생성하는 제1 생성부와,상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 제2 예비 이미지를 생성하고, 생성된 제2 예비 이미지를상기 대체 이미지로 결정하는 제2 생성부를 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서,상기 제1 생성부는, 미리 학습된 제1 생성 모델에 상기 마스킹 이미지 및 상기 텍스트 가이드를 입력함으로써상기 제1 예비 이미지를 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 제1 생성 모델은, 뉴럴 네트워크(Neural Network)에 기반한 이미지 생성 모델을 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서,상기 제1 생성 모델은, 컨디셔널 디퓨전 모델(Conditional Diffusion Model)을 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 제1 생성 모델은, 상기 마스킹 이미지 및 상기 텍스트 가이드가 입력되면, 상기 마스킹 이미지에 포함된 상기 마스킹 영역 및 상기 텍스트 가이드를 조건(Condition)으로 하여 상기 제1 예비 이미지를 출력하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제5 항에 있어서,상기 제2 생성부는,상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 상기 마스킹 영역을 다각도에서 바라본 이미지인 복수의상기 제2 예비 이미지를 생성하는 3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2025-0020375-4-제10 항에 있어서,상기 제2 생성부는,미리 학습된 제2 생성 모델에 상기 입력 이미지 및 상기 제1 예비 이미지를 입력함으로써 상기 복수의 제2 예비이미지를 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 제2 생성 모델은,가우시안 스플래팅(Gaussian Splatting) 기법을 통해 상기 입력 이미지 및 상기 제1 예비 이미지로부터 상기 복수의 제2 예비 이미지를 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1 항에 있어서,상기 후처리 모듈은,상기 대체 이미지에 대한 캘리브레이션(Calibration)을 수행함으로써 최종 합성 데이터를 생성하는 캘리브레이션부와,상기 생성된 최종 합성 데이터에 기초하여 상기 3D 모델을 생성하는 완성부를 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13 항에 있어서,상기 캘리브레이션부는,상기 대체 이미지와 복수의 상기 입력 이미지 간의 위치 관계 및 상기 대체 이미지에 상응하는 촬영 파라미터를포함하는 캘리브레이션 데이터를 추정하고,상기 입력 이미지, 상기 대체 이미지 및 상기 캘리브레이션 데이터를 상기 최종 합성 데이터로 결정하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14 항에 있어서,상기 촬영 파라미터는, 상기 대체 이미지에 상응하는 카메라의 위치, 각도 및 렌즈왜곡 계수 중 적어도 하나를포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "공개특허 10-2025-0020375-5-제14 항에 있어서,상기 캘리브레이션부는,미리 학습된 캘리브레이션 모델(Calibration Model)에 상기 입력 이미지와 상기 대체 이미지를 입력함으로써 상기 캘리브레이션 데이터를 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16 항에 있어서,상기 캘리브레이션 모델은, SFM(Structure From Motion) 알고리즘에 기초한 모델을 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제13 항에 있어서,상기 완성부는,미리 학습된 매핑 모델(Mapping Model)에 상기 최종 합성 데이터를 입력함으로써 상기 3D 모델을 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18 항에 있어서,상기 매핑 모델은, 상기 최종 합성 데이터를 미리 정의된 레퍼런스 3D 모델에 투영(Projection)시켜 상기 3D 모델을 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제13 항에 있어서,상기 3D 텍스쳐 복원 서버는,상기 최종 합성 데이터에 기초하여 상기 생성 모듈에 포함된 제1 생성 모델 및 제2 생성 모델을 학습시키는 학습 모듈을 더 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "적어도 하나의 인스트럭션(instruction)을 저장하는 메모리; 및상기 적어도 하나의 인스트럭션을 실행하는 적어도 하나의 프로세서를 포함하고,상기 프로세서는,대상체를 촬영한 입력 이미지를 수신하고,상기 입력 이미지로부터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 상기 누락 영역을 마스킹(Masking)공개특허 10-2025-0020375-6-처리함으로써 마스킹 영역이 포함된 마스킹 이미지를 생성하고,상기 입력 이미지 및 상기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하기 위한 대체 이미지를 생성하고,상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제21 항에 있어서,상기 입력 이미지는, 상기 대상체를 다각도에서 촬영한 멀티뷰(Multi-view) 이미지를 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제21 항에 있어서,상기 프로세서는,상기 입력 이미지를 기초로 생성된 사전 3D 모델로부터 상기 누락 영역을 식별하고,상기 누락 영역을 다각도에서 캡쳐(Capture)함으로써 복수의 상기 마스킹 이미지를 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제23 항에 있어서,상기 프로세서는,상기 사전 3D 모델로부터 음영(Shadow)이 존재하는 음영 영역을 식별하고,상기 식별된 음영 영역을 상기 누락 영역으로 결정하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제21 항에 있어서,상기 프로세서는,상기 마스킹 이미지 및 상기 마스킹 영역을 설명하는 텍스트 가이드(Text Guide)에 기초하여 제1 예비 이미지를생성하고,상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 제2 예비 이미지를 생성하고, 생성된 제2 예비 이미지를상기 대체 이미지로 결정하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제25 항에 있어서,공개특허 10-2025-0020375-7-상기 프로세서는, 미리 학습된 제1 생성 모델에 상기 마스킹 이미지 및 상기 텍스트 가이드를 입력함으로써 상기 제1 예비 이미지를 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제26 항에 있어서,상기 제1 생성 모델은, 뉴럴 네트워크(Neural Network)에 기반한 이미지 생성 모델을 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "제27 항에 있어서,상기 제1 생성 모델은, 컨디셔널 디퓨전 모델(Conditional Diffusion Model)을 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_29", "content": "제28 항에 있어서,상기 제1 생성 모델은, 상기 마스킹 이미지 및 상기 텍스트 가이드가 입력되면, 상기 마스킹 이미지에 포함된 상기 마스킹 영역 및 상기 텍스트 가이드를 조건(Condition)으로 하여 상기 제1 예비 이미지를 출력하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_30", "content": "제25 항에 있어서,상기 프로세서는,상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 상기 마스킹 영역을 다각도에서 바라본 이미지인 복수의상기 제2 예비 이미지를 생성하는 3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_31", "content": "제30 항에 있어서,상기 프로세서는,미리 학습된 제2 생성 모델에 상기 입력 이미지 및 상기 제1 예비 이미지를 입력함으로써 상기 복수의 제2 예비이미지를 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_32", "content": "공개특허 10-2025-0020375-8-제31 항에 있어서,상기 제2 생성 모델은,가우시안 스플래팅(Gaussian Splatting) 기법을 통해 상기 입력 이미지 및 상기 제1 예비 이미지로부터 상기 복수의 제2 예비 이미지를 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_33", "content": "제21 항에 있어서,상기 프로세서는,상기 대체 이미지에 대한 캘리브레이션(Calibration)을 수행함으로써 최종 합성 데이터를 생성하고,상기 생성된 최종 합성 데이터에 기초하여 상기 3D 모델을 생성하는 3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_34", "content": "제33 항에 있어서,상기 프로세서는,상기 대체 이미지와 복수의 상기 입력 이미지 간의 위치 관계 및 상기 대체 이미지에 상응하는 촬영 파라미터를포함하는 캘리브레이션 데이터를 추정하고,상기 입력 이미지, 상기 대체 이미지 및 상기 캘리브레이션 데이터를 상기 최종 합성 데이터로 결정하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_35", "content": "제34 항에 있어서,상기 촬영 파라미터는, 상기 대체 이미지에 상응하는 카메라의 위치, 각도 및 렌즈왜곡 계수 중 적어도 하나를포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_36", "content": "제34 항에 있어서,상기 프로세서는,미리 학습된 캘리브레이션 모델(Calibration Model)에 상기 입력 이미지와 상기 대체 이미지를 입력함으로써 상기 캘리브레이션 데이터를 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_37", "content": "제36 항에 있어서,공개특허 10-2025-0020375-9-상기 캘리브레이션 모델은, SFM(Structure From Motion) 알고리즘에 기초한 모델을 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_38", "content": "제33 항에 있어서,상기 프로세서는,미리 학습된 매핑 모델(Mapping Model)에 상기 최종 합성 데이터를 입력함으로써 상기 3D 모델을 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_39", "content": "제38 항에 있어서,상기 매핑 모델은, 상기 최종 합성 데이터를 미리 정의된 레퍼런스 3D 모델에 투영(Projection)시켜 상기 3D 모델을 생성하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_40", "content": "제33 항에 있어서,상기 프로세서는,상기 최종 합성 데이터에 기초하여 상기 대체 이미지의 생성에 이용된 제1 생성 모델 및 제2 생성 모델을 학습시키는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_41", "content": "위성 영상을 저장하는 외부 데이터베이스;상기 외부 데이터베이스와 통신하는 3D 텍스쳐 복원 서버; 및상기 외부 데이터베이스와 상기 3D 텍스쳐 복원 서버 간의 통신을 수행하는 통신망을 포함하되, 상기 3D 텍스쳐 복원 서버는, 적어도 하나의 인스트럭션(instruction)을 저장하는 메모리와,상기 적어도 하나의 인스트럭션을 실행하는 적어도 하나의 프로세서를 포함하고,상기 프로세서는,대상체를 촬영한 입력 이미지를 수신하고,상기 입력 이미지로부터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 상기 누락 영역을 마스킹(Masking)처리함으로써 마스킹 영역이 포함된 마스킹 이미지를 생성하고,상기 입력 이미지 및 상기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하기 위한 대체 이미지를 생성하고,상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성하는공개특허 10-2025-0020375-10-3D 텍스쳐 복원 시스템."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_42", "content": "제41 항에 있어서,상기 프로세서는,상기 입력 이미지를 기초로 생성된 사전 3D 모델로부터 상기 누락 영역을 식별하고,상기 누락 영역을 다각도에서 캡쳐(Capture)함으로써 복수의 상기 마스킹 이미지를 생성하는3D 텍스쳐 복원 시스템."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_43", "content": "제41 항에 있어서,상기 프로세서는,상기 마스킹 이미지 및 상기 마스킹 영역을 설명하는 텍스트 가이드(Text Guide)에 기초하여 제1 예비 이미지를생성하고,상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 제2 예비 이미지를 생성하고, 생성된 제2 예비 이미지를상기 대체 이미지로 결정하는3D 텍스쳐 복원 시스템."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_44", "content": "제43 항에 있어서,상기 프로세서는, 미리 학습된 제1 생성 모델에 상기 마스킹 이미지 및 상기 텍스트 가이드를 입력함으로써 상기 제1 예비 이미지를 생성하는3D 텍스쳐 복원 시스템."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_45", "content": "제44 항에 있어서,상기 제1 생성 모델은, 뉴럴 네트워크(Neural Network)에 기반한 이미지 생성 모델을 포함하는3D 텍스쳐 복원 시스템."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_46", "content": "제45 항에 있어서,상기 제1 생성 모델은, 상기 마스킹 이미지 및 상기 텍스트 가이드가 입력되면, 상기 마스킹 이미지에 포함된 상기 마스킹 영역 및 상기 텍스트 가이드를 조건(Condition)으로 하여 상기 제1 예비 이미지를 출력하는3D 텍스쳐 복원 시스템.공개특허 10-2025-0020375-11-청구항 47 제43 항에 있어서,상기 프로세서는,상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 상기 마스킹 영역을 다각도에서 바라본 이미지인 복수의상기 제2 예비 이미지를 생성하는 3D 텍스쳐 복원 시스템."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_48", "content": "제47 항에 있어서,상기 프로세서는, 미리 학습된 제2 생성 모델에 상기 입력 이미지 및 상기 제1 예비 이미지를 입력함으로써 상기 복수의 제2 예비 이미지를 생성하고, 상기 제2 생성 모델은, 가우시안 스플래팅(Gaussian Splatting) 기법을 통해 상기 입력 이미지 및 상기 제1 예비 이미지로부터 상기 복수의 제2 예비 이미지를 생성하는3D 텍스쳐 복원 시스템."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_49", "content": "제41 항에 있어서,상기 프로세서는,상기 대체 이미지에 대한 캘리브레이션(Calibration)을 수행함으로써 최종 합성 데이터를 생성하고,상기 생성된 최종 합성 데이터에 기초하여 상기 3D 모델을 생성하는 3D 텍스쳐 복원 시스템."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_50", "content": "제49 항에 있어서,상기 프로세서는,상기 대체 이미지와 복수의 상기 입력 이미지 간의 위치 관계 및 상기 대체 이미지에 상응하는 촬영 파라미터를포함하는 캘리브레이션 데이터를 추정하고,상기 입력 이미지, 상기 대체 이미지 및 상기 캘리브레이션 데이터를 상기 최종 합성 데이터로 결정하는3D 텍스쳐 복원 시스템."}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_51", "content": "3D 텍스쳐 복원 서버에 의해 수행되는 3D 텍스쳐 복원 방법에 있어서,대상체를 촬영한 입력 이미지를 수신하는 단계;상기 입력 이미지로부터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 상기 누락 영역을 마스킹(Masking)처리함으로써 마스킹 영역이 포함된 마스킹 이미지를 생성하는 단계;상기 입력 이미지 및 상기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하기 위한 대체 이미지를 생성공개특허 10-2025-0020375-12-하는 단계; 및상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성하는 단계를 포함하는3D 텍스쳐 복원 방법"}
{"patent_id": "10-2024-0174713", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_52", "content": "프로세서; 상기 프로세서에 의해 실행되는 컴퓨터 프로그램을 로드하는 메모리;상기 컴퓨터 프로그램의 실행과정에서 발생되는 데이터를 외부 데이터베이스와 교환하는 인터페이스; 및 상기 인터페이스를 통해 교환되는 데이터를 저장하는 스토리지를 포함하되,상기 컴퓨터 프로그램은,대상체를 촬영한 입력 이미지를 수신하는 단계와,상기 입력 이미지로부터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 상기 누락 영역을 마스킹(Masking)처리함으로써 마스킹 영역이 포함된 마스킹 이미지를 생성하는 단계와,상기 입력 이미지 및 상기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하기 위한 대체 이미지를 생성하는 단계와,상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성하는 단계를 포함하는3D 텍스쳐 복원 서버."}
{"patent_id": "10-2024-0174713", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 대상체에 대한 3D 모델을 생성할 때, 대상체에 대한 텍스쳐(Texture), 즉 질감을 정확히 표현하기 위 해 딥러닝(Deep Learning) 기반의 생성 모델을 이용함으로써 3D 모델의 시각적 완성도와 정확도를 대폭 향상시킬 수 있는 3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템에 관한 것이다. 상기 3D 텍스쳐 복원 서버는, 대상체를 촬영한 입력 이미지를 수신하는 데이터 수집 모듈, 상기 입력 이미지로부 터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 상기 누락 영역을 마스킹(Masking) 처리함으로써 마스킹 영 역이 포함된 마스킹 이미지를 생성하는 전처리 모듈, 상기 입력 이미지 및 상기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하기 위한 대체 이미지를 생성하는 생성 모듈 및 상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성하는 후처리 모듈을 포함할 수 있다."}
{"patent_id": "10-2024-0174713", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템에 관한 것이다. 구체적으로, 본 발명은 대상체에 대한 3D 모델을 생성할 때, 대상체에 대한 텍스쳐(Texture), 즉 질감을 정확히 표현하기 위해 딥러닝(Deep Learning) 기반의 생성 모델을 이용함으로써 3D 모델의 시각적 완성도와 정확도를 대폭 향상시킬 수 있는 3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템에 관한 것이다."}
{"patent_id": "10-2024-0174713", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이 부분에 기술된 내용은 단순히 본 실시예에 대한 배경 정보를 제공할 뿐 종래기술을 구성하는 것은 아니다. 대상체에 대한 3D 모델, 즉 3차원 모델을 생성하는 과정에서, 높이가 서로 다른 오브젝트를 포함하는 대상체, 예컨대 도심 지역의 경우 건물의 높이가 서로 상이하고, 해당 건물 들 사이에 음영지역이 복잡하게 발생하게 된 다. 이러한 음영 지역 등에 대하여는 3D 모델에서 텍스쳐를 원활히 표현할 수 없게 된다. 만약 대상체에 대한 텍스쳐를 충분히 표현하지 못하는 경우, 3D 모델에서는 시각적 완성도가 떨어지고, 많은 노이즈가 포함될 수 있 다. 이를 해결하기 위해, 대상체를 촬영할 때, 충분한 중첩률과 옆면을 추가로 촬영하는 등 복잡하고 정밀한 촬영이 요구된다. 다만, 촬영 조건, 촬영 장비의 위치 등으로 인해 모든 음영 지역 등을 면밀히 촬영하는 것은 많은 노 력, 시간, 비용을 발생하게 하여 효율성이 떨어지는 문제가 존재한다. 이에 따라, 최근 인공지능(Artificial Intelligence) 기술을 활용하여 음영 지역 등에 대한 텍스쳐를 표현함으 로써 부족한 텍스쳐를 고정밀로 보간하고자 하는 니즈가 존재한다."}
{"patent_id": "10-2024-0174713", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2024-0174713", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는, 대상체에 대한 3D 모델을 생성할 때, 대상체에 대한 텍스쳐(질감)를 정확히 표현하기 위해 딥러닝 기반의 생성 모델을 이용함으로써 3D 모델의 시각적 완성도와 정확도를 대폭 향상시킬 수 있는 3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템을 제공하는 것이다. 보다 상세하게, 본 발명이 해결하고자 하는 과제는, 3D 모델에서 텍스쳐가 누락된 영역이 존재하는 경우, 해당 영역의 이미지 데이터를 딥러닝 기반의 생성 모델을 이용하여 생성하여 대체함으로써, 3D 모델에서 누락된 텍스 쳐를 고정밀로 보간할 수 있는 3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템을 제공하는 것이다. 또한, 본 발명이 해결하고자 하는 과제는, 텍스쳐 보간을 위해 생성된 최종 데이터를 이용하여 생성 모델을 학 습시킴으로써 순환 학습 구조를 달성할 수 있는 3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템을 제공하 는 것이다. 본 발명의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 발명의 다른 목적 및 장점들 은 하기의 설명에 의해서 이해될 수 있고, 본 발명의 실시예에 의해 보다 분명하게 이해될 것이다. 또한, 본 발 명의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것 이다."}
{"patent_id": "10-2024-0174713", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버는, 대상체를 촬영한 입력 이미지를 수신하는 데이터 수집 모듈, 상기 입력 이미지로부터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 상기 누락 영역을 마스킹 (Masking) 처리함으로써 마스킹 영역이 포함된 마스킹 이미지를 생성하는 전처리 모듈, 상기 입력 이미지 및 상 기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하기 위한 대체 이미지를 생성하는 생성 모듈 및 상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성하는 후처리 모듈을 포함할 수 있다. 또한, 상기 입력 이미지는, 상기 대상체를 다각도에서 촬영한 멀티뷰(Multi-view) 이미지를 포함할 수 있다. 또한, 상기 전처리 모듈은, 상기 입력 이미지를 기초로 생성된 사전 3D 모델로부터 상기 누락 영역을 식별하고, 상기 누락 영역을 다각도에서 캡쳐(Capture)함으로써 복수의 상기 마스킹 이미지를 생성할 수 있다. 또한, 상기 전처리 모듈은, 상기 사전 3D 모델로부터 음영(Shadow)이 존재하는 음영 영역을 식별하고, 상기 식 별된 음영 영역을 상기 누락 영역으로 결정할 수 있다. 또한, 상기 생성 모듈은, 상기 마스킹 이미지 및 상기 마스킹 영역을 설명하는 텍스트 가이드(Text Guide)에 기 초하여 제1 예비 이미지를 생성하는 제1 생성부와, 상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 제2 예비 이미지를 생성하고, 생성된 제2 예비 이미지를 상기 대체 이미지로 결정하는 제2 생성부를 포함할 수 있다. 또한, 상기 제1 생성부는, 미리 학습된 제1 생성 모델에 상기 마스킹 이미지 및 상기 텍스트 가이드를 입력함으 로써 상기 제1 예비 이미지를 생성할 수 있다. 또한, 상기 제1 생성 모델은, 뉴럴 네트워크(Neural Network)에 기반한 이미지 생성 모델을 포함할 수 있다. 또한, 상기 제1 생성 모델은, 컨디셔널 디퓨전 모델(Conditional Diffusion Model)을 포함할 수 있다. 또한, 상기 제1 생성 모델은, 상기 마스킹 이미지 및 상기 텍스트 가이드가 입력되면, 상기 마스킹 이미지에 포 함된 상기 마스킹 영역 및 상기 텍스트 가이드를 조건(Condition)으로 하여 상기 제1 예비 이미지를 출력할 수 있다. 또한, 상기 제2 생성부는, 상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 상기 마스킹 영역을 다각도에 서 바라본 이미지인 복수의 상기 제2 예비 이미지를 생성할 수 있다. 또한, 상기 제2 생성부는, 미리 학습된 제2 생성 모델에 상기 입력 이미지 및 상기 제1 예비 이미지를 입력함으 로써 상기 복수의 제2 예비 이미지를 생성할 수 있다. 또한, 상기 제2 생성 모델은, 가우시안 스플래팅(Gaussian Splatting) 기법을 통해 상기 입력 이미지 및 상기 제1 예비 이미지로부터 상기 복수의 제2 예비 이미지를 생성할 수 있다. 또한, 상기 후처리 모듈은, 상기 대체 이미지에 대한 캘리브레이션(Calibration)을 수행함으로써 최종 합성 데 이터를 생성하는 캘리브레이션부와, 상기 생성된 최종 합성 데이터에 기초하여 상기 3D 모델을 생성하는 완성부 를 포함할 수 있다. 또한, 상기 캘리브레이션부는, 상기 대체 이미지와 복수의 상기 입력 이미지 간의 위치 관계 및 상기 대체 이미 지에 상응하는 촬영 파라미터를 포함하는 캘리브레이션 데이터를 추정하고, 상기 입력 이미지, 상기 대체 이미 지 및 상기 캘리브레이션 데이터를 상기 최종 합성 데이터로 결정할 수 있다. 또한, 상기 촬영 파라미터는, 상기 대체 이미지에 상응하는 카메라의 위치, 각도 및 렌즈왜곡 계수 중 적어도 하나를 포함할 수 있다. 또한, 상기 캘리브레이션부는, 미리 학습된 캘리브레이션 모델(Calibration Model)에 상기 입력 이미지와 상기 대체 이미지를 입력함으로써 상기 캘리브레이션 데이터를 생성할 수 있다. 또한, 상기 캘리브레이션 모델은, SFM(Structure From Motion) 알고리즘에 기초한 모델을 포함할 수 있다. 또한, 상기 완성부는, 미리 학습된 매핑 모델(Mapping Model)에 상기 최종 합성 데이터를 입력함으로써 상기 3D 모델을 생성할 수 있다. 또한, 상기 매핑 모델은, 상기 최종 합성 데이터를 미리 정의된 레퍼런스 3D 모델에 투영(Projection)시켜 상기 3D 모델을 생성할 수 있다. 또한, 상기 3D 텍스쳐 복원 서버는, 상기 최종 합성 데이터에 기초하여 상기 생성 모듈에 포함된 제1 생성 모델 및 제2 생성 모델을 학습시키는 학습 모듈을 더 포함할 수 있다. 본 발명의 다른 몇몇 실시예에 따른 3D 텍스쳐 복원 서버는, 적어도 하나의 인스트럭션(instruction)을 저장하 는 메모리 및 상기 적어도 하나의 인스트럭션을 실행하는 적어도 하나의 프로세서를 포함하고, 상기 프로세서는, 대상체를 촬영한 입력 이미지를 수신하고, 상기 입력 이미지로부터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 상기 누락 영역을 마스킹(Masking) 처리함으로써 마스킹 영역이 포함된 마스킹 이미지를 생성 하고, 상기 입력 이미지 및 상기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하기 위한 대체 이미지를 생성하고, 상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성할 수 있다. 또한, 상기 입력 이미지는, 상기 대상체를 다각도에서 촬영한 멀티뷰(Multi-view) 이미지를 포함할 수 있다. 또한, 상기 프로세서는, 상기 입력 이미지를 기초로 생성된 사전 3D 모델로부터 상기 누락 영역을 식별하고, 상 기 누락 영역을 다각도에서 캡쳐(Capture)함으로써 복수의 상기 마스킹 이미지를 생성할 수 있다. 또한, 상기 프로세서는, 상기 사전 3D 모델로부터 음영(Shadow)이 존재하는 음영 영역을 식별하고, 상기 식별된 음영 영역을 상기 누락 영역으로 결정할 수 있다. 또한, 상기 프로세서는, 상기 마스킹 이미지 및 상기 마스킹 영역을 설명하는 텍스트 가이드(Text Guide)에 기 초하여 제1 예비 이미지를 생성하고, 상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 제2 예비 이미지를 생성하고, 생성된 제2 예비 이미지를 상기 대체 이미지로 결정할 수 있다. 또한, 상기 프로세서는, 미리 학습된 제1 생성 모델에 상기 마스킹 이미지 및 상기 텍스트 가이드를 입력함으로 써 상기 제1 예비 이미지를 생성할 수 있다. 또한, 상기 제1 생성 모델은, 뉴럴 네트워크(Neural Network)에 기반한 이미지 생성 모델을 포함할 수 있다. 또한, 상기 제1 생성 모델은, 컨디셔널 디퓨전 모델(Conditional Diffusion Model)을 포함할 수 있다. 또한, 상기 제1 생성 모델은, 상기 마스킹 이미지 및 상기 텍스트 가이드가 입력되면, 상기 마스킹 이미지에 포 함된 상기 마스킹 영역 및 상기 텍스트 가이드를 조건(Condition)으로 하여 상기 제1 예비 이미지를 출력할 수 있다. 또한, 상기 프로세서는, 상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 상기 마스킹 영역을 다각도에서 바라본 이미지인 복수의 상기 제2 예비 이미지를 생성할 수 있다. 또한, 상기 프로세서는, 미리 학습된 제2 생성 모델에 상기 입력 이미지 및 상기 제1 예비 이미지를 입력함으로 써 상기 복수의 제2 예비 이미지를 생성할 수 있다.또한, 상기 제2 생성 모델은, 가우시안 스플래팅(Gaussian Splatting) 기법을 통해 상기 입력 이미지 및 상기 제1 예비 이미지로부터 상기 복수의 제2 예비 이미지를 생성할 수 있다. 또한, 상기 프로세서는, 상기 대체 이미지에 대한 캘리브레이션(Calibration)을 수행함으로써 최종 합성 데이터 를 생성하고, 상기 생성된 최종 합성 데이터에 기초하여 상기 3D 모델을 생성할 수 있다. 또한, 상기 프로세서는, 상기 대체 이미지와 복수의 상기 입력 이미지 간의 위치 관계 및 상기 대체 이미지에 상응하는 촬영 파라미터를 포함하는 캘리브레이션 데이터를 추정하고, 상기 입력 이미지, 상기 대체 이미지 및 상기 캘리브레이션 데이터를 상기 최종 합성 데이터로 결정할 수 있다. 또한, 상기 촬영 파라미터는, 상기 대체 이미지에 상응하는 카메라의 위치, 각도 및 렌즈왜곡 계수 중 적어도 하나를 포함할 수 있다. 또한, 상기 프로세서는, 미리 학습된 캘리브레이션 모델(Calibration Model)에 상기 입력 이미지와 상기 대체 이미지를 입력함으로써 상기 캘리브레이션 데이터를 생성할 수 있다. 또한, 상기 캘리브레이션 모델은, SFM(Structure From Motion) 알고리즘에 기초한 모델을 포함할 수 있다. 또한, 상기 프로세서는, 미리 학습된 매핑 모델(Mapping Model)에 상기 최종 합성 데이터를 입력함으로써 상기 3D 모델을 생성할 수 있다. 또한, 상기 매핑 모델은, 상기 최종 합성 데이터를 미리 정의된 레퍼런스 3D 모델에 투영(Projection)시켜 상기 3D 모델을 생성할 수 있다. 또한, 상기 프로세서는, 상기 최종 합성 데이터에 기초하여 상기 대체 이미지의 생성에 이용된 제1 생성 모델 및 제2 생성 모델을 학습시킬 수 있다. 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 시스템은, 위성 영상을 저장하는 외부 데이터베이스, 상기 외부 데이터베이스와 통신하는 3D 텍스쳐 복원 서버 및 상기 외부 데이터베이스와 상기 3D 텍스쳐 복원 서버 간의 통 신을 수행하는 통신망을 포함하되, 상기 3D 텍스쳐 복원 서버는, 적어도 하나의 인스트럭션(instruction)을 저 장하는 메모리와, 상기 적어도 하나의 인스트럭션을 실행하는 적어도 하나의 프로세서를 포함하고, 상기 프로세 서는, 대상체를 촬영한 입력 이미지를 수신하고, 상기 입력 이미지로부터 텍스쳐(Texture)가 누락된 누락 영역 을 식별하고, 상기 누락 영역을 마스킹(Masking) 처리함으로써 마스킹 영역이 포함된 마스킹 이미지를 생성하고, 상기 입력 이미지 및 상기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하기 위한 대체 이미 지를 생성하고, 상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성할 수 있다. 또한, 상기 프로세서는, 상기 입력 이미지를 기초로 생성된 사전 3D 모델로부터 상기 누락 영역을 식별하고, 상 기 누락 영역을 다각도에서 캡쳐(Capture)함으로써 복수의 상기 마스킹 이미지를 생성할 수 있다. 또한, 상기 프로세서는, 상기 마스킹 이미지 및 상기 마스킹 영역을 설명하는 텍스트 가이드(Text Guide)에 기 초하여 제1 예비 이미지를 생성하고, 상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 제2 예비 이미지를 생성하고, 생성된 제2 예비 이미지를 상기 대체 이미지로 결정할 수 있다. 또한, 상기 프로세서는, 미리 학습된 제1 생성 모델에 상기 마스킹 이미지 및 상기 텍스트 가이드를 입력함으로 써 상기 제1 예비 이미지를 생성할 수 있다. 또한, 상기 제1 생성 모델은, 뉴럴 네트워크(Neural Network)에 기반한 이미지 생성 모델을 포함할 수 있다. 또한, 상기 제1 생성 모델은, 상기 마스킹 이미지 및 상기 텍스트 가이드가 입력되면, 상기 마스킹 이미지에 포 함된 상기 마스킹 영역 및 상기 텍스트 가이드를 조건(Condition)으로 하여 상기 제1 예비 이미지를 출력할 수 있다. 또한, 상기 프로세서는, 상기 입력 이미지 및 상기 제1 예비 이미지에 기초하여 상기 마스킹 영역을 다각도에서 바라본 이미지인 복수의 상기 제2 예비 이미지를 생성할 수 있다. 또한, 상기 프로세서는, 미리 학습된 제2 생성 모델에 상기 입력 이미지 및 상기 제1 예비 이미지를 입력함으로 써 상기 복수의 제2 예비 이미지를 생성하고, 상기 제2 생성 모델은, 가우시안 스플래팅(Gaussian Splatting) 기법을 통해 상기 입력 이미지 및 상기 제1 예비 이미지로부터 상기 복수의 제2 예비 이미지를 생성할 수 있다. 또한, 상기 프로세서는, 상기 대체 이미지에 대한 캘리브레이션(Calibration)을 수행함으로써 최종 합성 데이터 를 생성하고, 상기 생성된 최종 합성 데이터에 기초하여 상기 3D 모델을 생성할 수 있다. 또한, 상기 프로세서는, 상기 대체 이미지와 복수의 상기 입력 이미지 간의 위치 관계 및 상기 대체 이미지에 상응하는 촬영 파라미터를 포함하는 캘리브레이션 데이터를 추정하고, 상기 입력 이미지, 상기 대체 이미지 및 상기 캘리브레이션 데이터를 상기 최종 합성 데이터로 결정할 수 있다. 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버에 의해 수행되는 3D 텍스쳐 복원 방법은, 대상체를 촬영한 입력 이미지를 수신하는 단계, 상기 입력 이미지로부터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 상기 누락 영역을 마스킹(Masking) 처리함으로써 마스킹 영역이 포함된 마스킹 이미지를 생성하는 단계, 상기 입력 이미지 및 상기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하기 위한 대체 이미지를 생성하는 단계 및 상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성하는 단계를 포함할 수 있다. 본 발명의 또 다른 몇몇 실시예에 따른 3D 텍스쳐 복원 서버는, 프로세서, 상기 프로세서에 의해 실행되는 컴퓨 터 프로그램을 로드하는 메모리, 상기 컴퓨터 프로그램의 실행과정에서 발생되는 데이터를 외부 데이터베이스와 교환하는 인터페이스 및 상기 인터페이스를 통해 교환되는 데이터를 저장하는 스토리지를 포함하되, 상기 컴퓨 터 프로그램은, 대상체를 촬영한 입력 이미지를 수신하는 단계와, 상기 입력 이미지로부터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 상기 누락 영역을 마스킹(Masking) 처리함으로써 마스킹 영역이 포함된 마스킹 이미지를 생성하는 단계와, 상기 입력 이미지 및 상기 마스킹 이미지에 기초하여 상기 마스킹 이미지를 대체하 기 위한 대체 이미지를 생성하는 단계와, 상기 입력 이미지 및 상기 대체 이미지에 기초하여 3D 모델을 생성하 는 단계를 포함할 수 있다."}
{"patent_id": "10-2024-0174713", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템은, 대상체에 대한 3D 모델 을 생성할 때, 대상체에 대한 텍스쳐(질감)를 정확히 표현하기 위해 딥러닝 기반의 생성 모델을 이용함으로써 3D 모델의 시각적 완성도, 정확도, 사용자 경험, 3D 모델의 운영 효율 및 유지보수 효율성 등을 개선 또는 향상 시킬 수 있다. 보다 상세하게, 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템은, 3D 모 델에서 텍스쳐가 누락된 영역이 존재하는 경우, 해당 영역의 이미지 데이터를 딥러닝 기반의 생성 모델을 이용 하여 생성하여 대체함으로써, 3D 모델에서 누락된 텍스쳐를 고정밀로 보간할 수 있고, 그에 따라 3D 모델의 텍 스쳐 완성도와 시각적 품질을 향상시킬 수 있다. 또한, 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템은, 텍스쳐 보간을 위해 생성된 최종 데이터를 이용하여 생성 모델을 학습시킴으로써 순환 학습 구조를 달성할 수 있고, 그에 따라 텍스쳐 생성의 정확도를 더욱 향상시킬 수 있다. 상술한 내용과 더불어 본 발명의 몇몇 실시예에 따른 구체적인 효과는 이하"}
{"patent_id": "10-2024-0174713", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서 및 특허청구범위에서 사용된 용어나 단어는 일반적이거나 사전적인 의미로 한정하여 해석되어서는 아 니된다. 발명자가 그 자신의 발명을 최선의 방법으로 설명하기 위해 용어나 단어의 개념을 정의할 수 있다는 원 칙에 따라, 본 발명의 기술적 사상과 부합하는 의미와 개념으로 해석되어야 한다. 또한, 본 명세서에 기재된 실 시예와 도면에 도시된 구성은 본 발명이 실현되는 하나의 실시예에 불과하고, 본 발명의 기술적 사상을 전부 대 변하는 것이 아니므로, 본 출원시점에 있어서 이들을 대체할 수 있는 다양한 균등물과 변형 및 응용 가능한 예 들이 있을 수 있음을 이해하여야 한다. 본 명세서 및 특허청구범위에서 사용된 제1, 제2, A, B 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성 요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 본 명세서 및 특허청구범위에서 사용된 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서 \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해서 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의미를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식 적인 의미로 해석되지 않는다. 또한, 본 발명의 각 실시예에 포함된 각 구성, 과정, 공정 또는 방법 등은 기술적으로 상호 간 모순되지 않는 범위 내에서 공유될 수 있다. 이하에서는, 도 1 내지 도 32를 참조하여 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버 및 방법과 이를 포함하는 시스템에 대해 살펴보도록 한다. 도 1은 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 시스템을 도시한 것이다. 도 1을 참조하면, 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 시스템은, 외부 데이터베이스, 3D 텍 스쳐 복원 서버(200, 이하 “서버”라 한다) 및 통신망을 포함할 수 있다. 외부 데이터베이스는 대상체에 대한 3D 모델 생성, 3D 텍스쳐 복원의 기초가 되는 대상체를 촬영한 적어도 하나의 이미지 데이터를 저장, 보관, 관리 및/또는 전송하는 데이터베이스일 수 있다. 이때, 대상체는 지상, 해상, 상공 등에 존재하는 형태를 가진 오브젝트 및/또는 오브젝트의 집합을 포함할 수 있다. 일 예로, 대상체는 건물, 구조물, 산(Mountain) 및/또는 이들을 포함하는 도심 지역, 산림 지역 등을 포 함할 수 있으나, 본 발명의 실시예가 이에 제한되는 것은 아니다. 몇몇 예로, 외부 데이터베이스는 카메라 및/또는 카메라가 탑재된 드론, 비행기, 인공위성 등을 통해 촬영 된 이미지 데이터를 수신하고, 수신된 이미지 데이터들을 서버로 전달할 수 있다. 이후, 외부 데이터베이 스는 서버로부터 3D 텍스쳐 복원이 완료되면, 이미지 데이터에 기초하여 생성된 3D 모델을 수신하여 저장할 수 있다. 이때, 외부 데이터베이스는 다양한 종류의 이미지 데이터를 저장, 보관, 관리, 전송할 수 있다. 일 예로, 외부 데이터베이스가 취급하는 이미지 데이터는 대상체를 다각도에서 촬영한 멀티뷰(Multi-view) 이미지를 포함할 수 있으나, 본 발명의 실시예가 이에 제한되는 것은 아니다. 한편, 외부 데이터베이스는 컴퓨터, 노트북 PC, 모바일 기기, 웨어러블 기기 등의 다양한 형태의 전자 기 기, 워크스테이션(workstation), 데이터 센터, 인터넷 데이터 센터(internet data center(IDC)), DAS(direct attached storage) 시스템, SAN(storage area network) 시스템, NAS(network attached storage) 시스템 및 RAID(redundant array of inexpensive disks, or redundant array of independent disks) 시스템 등의 형태일 수 있으나, 본 발명의 실시예가 이에 제한되는 것은 아니다. 다만, 전술한 바와 달리, 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 시스템에서 외부 데이터베이스(10 0)는 생략되어 실시될 수 있다. 이때, 외부 데이터베이스가 저장, 관리 및 전송하는 이미지 데이터는 서버 에 포함된 데이터베이스 모듈에 의해 저장 및 관리될 수도 있다. 서버는 외부 데이터베이스로부터 이미지 데이터, 즉 입력 이미지를 수신한 후, 수신된 입력 이미지에 기초하여 3D 모델을 생성할 수 있다. 이때, 서버는 3D 모델에서 텍스쳐가 누락된 영역이 존재하는 경우, 이를 복원할 수 있다. 다시 말하면, 서 버는 입력 이미지를 기초로 3D 텍스쳐 복원을 수행함으로써 3D 모델을 생성할 수 있다. 몇몇 예로, 서버는 외부 데이터베이스로부터 수신된 이미지 데이터에서 텍스쳐가 누락된 누락 영역을 식별하고, 식별된 누락 영역을 마스킹하여 마스킹 이미지를 생성하고, 마스킹 이미지를 대체할 대체 이미지를 생성하고, 생성된 대체 이미지를 이용하여 텍스쳐를 복원함으로써 3D 모델을 생성할 수 있다. 이하, 도 2 및 도 3을 참조하여 본 발명의 몇몇 실시예에 따른 3D 모델에서 텍스쳐가 누락되는 것에 대하여 자 세히 설명하기로 한다. 도 2 및 도 3은 텍스쳐 누락을 설명하기 위한 도면이다. 도 1 및 도 2를 참조하면, 도 2에는 대상체로써 산림 지역이 예시되어 있고, 제1 위치(P1)와 제2 위치(P2) 각각 에 배치된 드론이 해당 대상체를 촬영하는 것이 도시되어 있다. 이때, 드론은 카메라를 포함할 수 있고, 카메라 를 이용하여 산림 지역을 촬영할 수 있다. 도 2에는 제1 위치(P1)에서 촬영된 이미지 데이터가 커버하는 영역, 즉 가시 영역(Visible Region, 이하 “VR” 이라 한다)과, 제1 위치(P1)에서 촬영된 이미지 데이터가 커버하지 못하는 영역, 즉 비가시 영역(Non-Visible Region, 이하 “NVR”이라 한다)이 구분된 제1 이미지 데이터 분석 결과(Visible at P1)와, 제2 위치(P2)에서 촬영된 이미지 데이터가 커버하는 영역, 즉 가시 영역(VR)과 제2 위치(P2)에서 촬영된 이미지 데이터가 커버하 지 못하는 영역, 즉 비가시 영역(NVR)이 구분된 제2 이미지 데이터 분석 결과(Visible at P2)와, 제1 위치(P1) 및 제2 위치(P2) 중 적어도 하나에서 촬영된 이미지 데이터에 대한 가시 영역(VR)과 비가시 영역(NVR)이 구분된 종합 이미지 데이터 분석 결과(Visible at P1 or P2)가 도시되어 있다. 도 2에는 설명의 편의를 위해 가시 영역 (VR)은 녹색으로, 비가시 영역(NVR)은 적색으로 표시되어 있다. 즉, 도 2에 도시된 바와 같이, 종합 이미지 데이터 분석 결과(Visible at P1 or P2)에서의 비가시 영역(NVR)의 경우, 각 드론의 카메라의 촬영 범위에 존재하지 않아 해당 영역에 대한 이미지 데이터가 결여된 상태이다. 이 러한 비가시 영역(NVR)이 존재하는 경우, 가시 영역(VR)만을 이용하여 생성된 3D 모델에서 해당 비가시 영역 (NVR)에 상응하는 위치에서의 텍스쳐가 누락될 수 있다. 보다 상세하게, 도 1 내지 도 3을 참조하면, 도 3에는 이미지 데이터, 즉 입력 이미지(Input Image, 이하 “II ”라 한다)에 기초하여 생성된 3D 모델(3-Dimension Model, 이하 “3DM”이라 한다)이 도시되어 있다. 이때, 입 력 이미지(II)에 도 2에서 전술한 바와 같은 비가시 영역(NVR)이 존재하는 경우, 해당 입력 이미지(II)에 기초 하여 생성된 3D 모델(3DM)은 도 3에 도시된 바와 같이 그 형태가 명확히 표현되지 않고 노이즈가 많이 포함될 수 있다. 이에 따라, 본 발명의 몇몇 실시예에 따른 서버는 이러한 텍스쳐 누락 문제를 해결하기 위해, 3D 모델 (3DM)을 생성할 때 텍스쳐 복원 작업을 수행할 수 있는 것이다. 서버의 텍스쳐 복원 과정에 대한 자세한 설명은 후술하기로 한다. 다시 도 1을 참조하면, 한편, 서버는 워크스테이션(workstation), 데이터 센터, 인터넷 데이터 센터 (internet data center(IDC)), DAS(direct attached storage) 시스템, SAN(storage area network) 시스템, NAS(network attached storage) 시스템 및 RAID(redundant array of inexpensive disks, or redundant array of independent disks) 시스템 등의 형태일 수 있으나, 본 발명의 실시예가 이에 제한되는 것은 아니다. 통신망은 외부 데이터베이스와 서버를 연결하는 역할을 수행한다. 즉, 통신망은 서버(20 0)가 외부 데이터베이스로부터 데이터를 송수신할 수 있도록 접속 경로를 제공하는 통신망을 의미한다. 통 신망은 예컨대 LANs(Local Area Networks), WANs(Wide Area Networks), MANs(MetRoFolitan Area Networks), ISDNs(Integrated Service Digital Networks) 등의 유선 네트워크나, 무선 LANs, CDMA, 블루투스, 위성 통신 등의 무선 네트워크를 망라할 수 있으나, 본 발명의 범위가 이에 한정되는 것은 아니다. 이하, 도 4 내지 도 21을 참조하여 서버의 동작에 대하여 더 자세히 설명하기로 한다. 도 4는 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버의 블록도이다. 도 5는 본 발명의 몇몇 실시예에 따 른 3D 텍스쳐 복원 서버의 동작을 설명하기 위한 개념도이다. 도 1, 도 4 및 도 5를 참조하면, 본 발명의 몇몇 실시예에 따른 서버는 데이터 수집 모듈, 전처리 모 듈, 생성 모듈, 후처리 모듈 및 학습 모듈을 포함할 수 있다. 다만, 본 발명의 실시예가 이에 제한되는 것은 아니며, 서버에 포함된 데이터 수집 모듈, 전처리 모듈, 생성 모듈, 후처리 모듈 및 학습 모듈 중 어느 하나가 생략되어 실시되거나 또는 도 4에 도시되지 않은 다른 구 성이 서버에 포함되어 실시될 수도 있다. 데이터 수집 모듈은 입력 이미지(II)를 수신할 수 있다. 일 예로, 데이터 수집 모듈은 외부 데이터베 이스로부터 입력 이미지(II)를 수신할 수 있다. 입력 이미지(II)는 지상, 해상, 상공 등에 위치한 카메라가 탑재된 드론, 비행기, 인공위성 등을 통해 촬영된 이미지 데이터를 포함할 수 있다. 이때, 입력 이미지(II)는 다양한 종류의 이미지 데이터를 포함할 수 있다. 일 예로, 입력 이미지(II)는 대상체를 다각도에서 촬영한 멀티뷰(Multi-view) 이미지를 포함할 수 있으나, 본 발명 의 실시예가 이에 제한되는 것은 아니다. 데이터 수집 모듈은 수신된 입력 이미지(II)를 서버 내의 다른 구성에 전달할 수 있다. 일 예로, 데 이터 수집 모듈은 입력 이미지(II)를 전처리 모듈, 생성 모듈, 후처리 모듈 등에 전달할 수 있다. 전처리 모듈은 입력 이미지(II)에 기초하여 누락 영역을 식별하고, 식별된 누락 영역에 기초하여 마스킹 이미지(Masking Image, 이하 “MI”라 한다)를 생성할 수 있다. 몇몇 예로, 전처리 모듈은 입력 이미지(II)로부터 텍스쳐가 누락된 누락 영역을 식별하고, 누락 영역을 마 스킹(Masking) 처리함으로써 마스킹 영역(Masking Region, 이하 “MR”이라 한다)이 포함된 마스킹 이미지(MI) 를 생성할 수 있다. 이하, 도 6 및 도 7을 참조하여, 본 발명의 몇몇 실시예에 따른 전처리 모듈의 구조 및 동작에 대해 더 자 세히 설명하기로 한다. 도 6 및 도 7은 본 발명의 몇몇 실시예에 따른 전처리 모듈의 구조 및 동작을 설명하기 위한 도면이다. 도 1, 도 4 내지 도 7을 참조하면, 본 발명의 몇몇 실시예에 따른 전처리 모듈은 입력 이미지(II)와 레퍼 런스 3D 모델(Reference 3-Dimension Model, 이하 “REF”라 한다)을 이용하여 마스킹 이미지(MI)를 생성할 수 있다. 이때, 레퍼런스 3D 모델(REF)은 서버의 데이터베이스에 미리 저장된 데이터일 수 있다. 몇몇 예로, 전처리 모듈은 입력 이미지(II)와 레퍼런스 3D 모델(REF)을 이용하여 사전 3D 모델(Pre 3- Dimension Model, 이하 “3DM_pre”라 한다)을 생성한 후, 생성된 사전 3D 모델(3DM_pre)로부터 누락 영역을 식별함으로써 마스킹 이미지(MI)를 생성할 수 있다. 보다 상세하게, 우선, 전처리 모듈은 입력 이미지(II)와 레퍼런스 3D 모델(REF)을 매핑 모델(Mapping Model, 이하 “MM”이라 한다)에 입력함으로써 사전 3D 모델(3DM_pre)을 생성할 수 있다. 이때, 매핑 모델(MM) 은 입력 이미지(II)를 레퍼런스 3D 모델(REF)에 투영(Projection) 시킴으로써 사전 3D 모델(3DM_pre)을 생성할 수 있다. 이어서, 전처리 모듈은 사전 3D 모델(3DM_pre)로부터 텍스쳐가 누락된 누락 영역을 식별할 수 있다. 예컨 대, 전처리 모듈은 사전 3D 모델(3DM_pre)로부터 음영(Shadow)이 존재하는 음영 영역을 식별하고, 식별된 음영 영역을 텍스쳐가 누락된 누락 영역으로 결정할 수 있다. 이어서, 전처리 모듈은 누락 영역을 마스킹 처리함으로써 마스킹 영역(MR)이 포함된 마스킹 이미지(MI)를 생성할 수 있다. 이때, 전처리 모듈은 결정된 누락 영역을 다각도에서 캡쳐(Capture)함으로써 복수의 마스 킹 이미지(MI)를 생성할 수 있다. 다시 도 1, 도 4 및 도 5를 참조하면, 전처리 모듈은 생성된 적어도 하나의 마스킹 이미지(MI)를 생성 모 듈에 전달할 수 있다. 생성 모듈은 입력 이미지(II) 및 마스킹 이미지(MI)에 기초하여 마스킹 이미지(MI)를 대체하기 위한 대체 이미지(Replacement Image, 이하 “RI”라 한다)를 생성할 수 있다. 이하, 도 8 내지 도 14를 참조하여, 본 발명의 몇몇 실시예에 따른 생성 모듈의 구조 및 동작에 대해 더 자세히 설명하기로 한다. 도 8 내지 도 14는 본 발명의 몇몇 실시예에 따른 생성 모듈의 구조 및 동작을 설명하기 위한 도면이다. 도 1, 도 4, 도 5, 도 8 내지 도 14를 참조하면, 본 발명의 몇몇 실시예에 따른 생성 모듈은 입력 이미지 (II)와 마스킹 이미지(MI)에 기초하여 대체 이미지(RI)를 생성하는 모듈로서, 제1 생성부와 제2 생성부 를 포함할 수 있다. 제1 생성부는 마스킹 이미지(MI)에 기초하여 제1 예비 이미지(1st Pre Image, 이하 “Pre1”이라 한다)를 생성할 수 있다. 몇몇 예로, 제1 생성부는 마스킹 이미지(MI) 및 마스킹 이미지(MI)에 포함된 마스킹 영역(MR)을 설명하는 텍스트 가이드(Text Guide, 이하 “TG”라 한다)에 기초하여 제1 예비 이미지(Pre1)를 생성할 수 있다. 텍스트 가이드(TG)는 마스킹 영역(MR)을 설명하는 텍스트 데이터일 수 있다. 다시 말하면, 텍스트 가이드(TG)는 마스킹 이미지(MI)에 대응하는 제1 예비 이미지(Pre1) 및/또는 대체 이미지(RI)를 생성하기 위해, 마스킹 영역 (MR)을 대체할 수 있도록 해당 마스킹 영역(MR)의 특징, 구성, 배경 등을 설명하는 텍스트 데이터를 포함할 수 있다. 일 예로, 텍스트 가이드(TG)는 “둥근 창문의 대리석 벽으로 이루어진 건물의 부분”과 같은 텍스트 데이 터를 포함할 수 있으나, 본 발명의 실시예가 이에 제한되지 않음은 당연하다. 이때, 텍스트 가이드(TG)는 서버 의 데이터베이스에 미리 저장되거나 또는 서버의 관리자로부터 입력된 데이터일 수 있다. 예컨대, 제1 생성부는 도 10에 도시된 바와 같이 미리 학습된 제1 생성 모델(1st Generation Model, 이하 “GM1”이라 한다)을 이용하여 제1 예비 이미지(Pre1)를 생성할 수 있다. 예를 들어, 제1 생성부는 제1 생 성 모델(GM1)에 마스킹 이미지(MI)와 텍스트 가이드(TG)를 입력함으로써 제1 예비 이미지(Pre1)를 생성할 수 있 다. 제1 생성 모델(GM1)은 마스킹 이미지(MI)와 텍스트 가이드(TG)가 입력되면 제1 예비 이미지(Pre1)를 생성하여 출력할 수 있다. 이때, 제1 생성 모델(GM1)은 AI(Artificial Intelligence) 기술을 기초로 제1 예비 이미지(Pre1)를 생성할 수 있다. 일 예로, 제1 생성 모델(GM1)은 딥러닝(Deep Learning) 방식 및 구조를 이용하여 마스킹 이미지(MI)와 텍 스트 가이드(TG)로부터 제1 예비 이미지(Pre1)를 생성할 수 있다. 예컨대, 제1 생성 모델(GM1)은 미리 학습된 뉴럴 네트워크 모델(Neural Network Model)을 이용하여 제1 예비 이미지(Pre1)를 생성할 수 있다. 보다 자세히 설명하자면, 머신 러닝의 일종인 딥러닝(Deep Learning) 기술은 데이터를 기반으로 다단계로 깊은 수준까지 내려가 학습하는 것이다. 즉, 딥러닝은, 단계를 높여가면서 복수의 데이터들 로부터 핵심적인 데이터 를 추출하는 머신 러닝 알고리즘의 집합을 나타낸다. 몇몇 예로, 뉴럴 네트워크는 공지된 다양한 딥러닝 구조를 이용할 수 있다. 예를 들어, 뉴럴 네트워크는 CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), DBN(Deep Belief Network), GNN(Graph Neural Network), GAN (Generative Adversarial Network), Transformer, Autoencoder 등의 구조를 이용할 수 있다. 구체적으로, CNN(Convolutional Neural Network)은 사람이 물체를 인식할 때 물체의 기본적인 특징들을 추출한 다음 뇌 속에서 복잡한 계산을 거쳐 그 결과를 기반으로 물체를 인식한다는 가정을 기반으로 만들어진 사람의 뇌 기능을 모사한 모델이다. CNN은 공지된 LeNet, AlexNet, VGGNet, GoogleNet, ResNet 등의 구조를 포함할 수 있으나, 이에 제한되는 것은 아니다. RNN(Recurrent Neural Network)은 자연어 처리 등에 많이 이용되며, 시간의 흐름에 따라 변하는 시계열 데이터 (Time-series data) 처리에 효과적인 구조로 매 순간마다 레이어를 쌓아 올려 인공신경망 구조를 구성할 수 있 다. DBN(Deep Belief Network)은 딥러닝 기법인 RBM(Restricted Boltzman Machine)을 다층으로 쌓아 구성되는 딥러 닝 구조이다. RBM(Restricted Boltzman Machine) 학습을 반복하여 일정 수의 레이어가 되면, 해당 개수의 레이 어를 가지는 DBN(Deep Belief Network)이 구성될 수 있다. GNN(Graphic Neural Network, 그래픽 인공신경망, 이하, GNN)는 특정 파라미터 간 매핑된 데이터를 기초로 모델 링된 모델링 데이터를 이용하여, 모델링 데이터 간의 유사도와 특징점을 도출하는 방식으로 구현된 인공신경망 구조를 나타낸다. GAN(Generative Adversarial Network, 적대적 생성 신경망, 이하 GAN)은 생성신경망과 구별신경망을 이용하여, 입력된 데이터와 유사한 형태의 새로운 데이터를 만들어내는 인공신경망 구조를 나타낸다. GAN은 공지된 DCGAN(Deep Convolutional GAN), CGAN (Conditional GAN), WGAN (Wasserstein GAN), StyleGAN (Style-Based GAN), CycleGAN 등을 포함할 수 있으나, 본 발명의 실시예가 이에 제한되는 것은 아니다. Transformer(트랜스포머)는 어텐션을 활용한 인코더-디코더 구조의 인공신경망으로서, 입력 시퀀스와 출력 시퀀 스 간의 전체적인 의미를 파악할 수 있다. 트랜스포머는, 어텐션(Attention) 메커니즘을 이용하여 입력 시퀀스 의 모든 요소가 출력 시퀀스에 영향을 주도록 하며, 이를 통해 인코더와 디코더 모두 시퀀스 전체를 고려할 수 있다. 트랜스포머는, 자연어, 시계열 데이터뿐만 아니라, 이미지를 패치화 하여 입력으로 사용할 수 있다. Auto-encoder(오토 인코더)는 데이터의 특징을 추출하고 재구성하는 역할을 수행하는 딥러닝 구조이다. 대표적 으로 오토 인코더는 입력값을 압축하는 인코더와, 압축된 데이터를 복원하는 디코더를 포함한다. 인코더는 입력 값을 저차원의 잠재 표현(latent representation)으로 변환하고, 디코더는 잠재 표현을 입력값과 동일한 차원으 로 복원한다. 이 때 인코더와 디코더는 각각 다층 퍼셉트론(MLP)으로 구성될 수 있다. 오토인코더를 학습할 때 에는 입력 데이터를 입력하고, 출력값과 입력값 간의 차이를 최소화하는 방향으로 가중치와 편향을 학습시킨다. 이렇게 학습된 오토 인코더는 입력 데이터의 특징을 잘 추출하고, 잡음이 있는 입력 데이터를 복원할 수 있다. 오토 인코더는 주로 데이터 압축, 차원 축소, 잡음 제거, 데이터 생성 등의 분야에서 활용되며, 또한, 이미지 인식, 자연어 처리, 음성 인식 등의 분야에서도 활용될 수 있다. 몇몇 예로, 제1 생성 모델(GM1)은 디퓨전 모델(Diffusion Model)을 포함할 수 있다. 일 예로, 제1 생성 모델 (GM1)은 컨디셔널 디퓨전 모델(Conditional Diffusion Model)을 포함할 수 있으나, 본 발명의 실시예가 이에 제한되는 것은 아니다. 이때, 제1 생성 모델(GM1)은 후술하는 바와 같이 마스킹 이미지(MI)에 포함된 마스킹 영 역(MR)과 텍스트 가이드(TG)를 조건(Condition)으로 하여 마스킹 이미지(MI)로부터 제1 예비 이미지(Pre1)를 생 성할 수 있다. 한편, 제1 생성 모델(GM1)이 이용하는 뉴럴 네트워크 모델의 인공신경망 학습은 주어진 입력에 대하여 원하는 출력이 나오도록 노드간 연결선의 웨이트(weight)를 조정(필요한 경우 바이어스(bias) 값도 조정)함으로써 이루 어질 수 있다. 또한, 인공신경망은 학습에 의해 웨이트(weight) 값을 지속적으로 업데이트 시킬 수 있다. 또한, 인공신경망의 학습에는 역전파(Back Propagation) 등의 방법이 사용될 수 있다. 이때, 인공신경망의 머신 러닝 방법으로는 자율학습(unsupervised learning), 준지도학습(semi-supervised learning), 지도학습(supervised learning)등이 사용될 수 있다. 또한, 뉴럴 네트워크 모델은 설정에 따라 학습 후 분석 데이터를 출력하기 위한 인공신경망 구조를 자동 업데이트하도록 제어될 수 있다. 본 발명의 몇몇 실시예에 따른 제1 생성 모델(GM1)에 포함된 뉴럴 네트워크 구조는 도 11에 도시되어 있다. 도 11에 도시된 바와 같이, 본 발명의 몇몇 실시예에 따른 제1 생성 모델(GM1)은 뉴럴 네트워크 구조를 이용하여 제1 예비 이미지(Pre1)를 생성할 수 있다. 몇몇 예로, 제1 생성 모델(GM1)은 입력 레이어(input)와, 출력 레이어(Output)와, 입력 레이어와 출력 레이어 사이에 배치되는 M 개의 히든 레이어를 포함할 수 있다. 여기서, 각 레이어들의 노드를 연결하는 에지(edge)에 는 가중치가 설정될 수 있다. 이러한 가중치 혹은 에지의 유무는 학습 과정에서 추가, 제거, 또는 업데이트 될 수 있다. 따라서, 학습 과정을 통하여, k개의 입력노드와 i개의 출력노드 사이에 배치되는 노드들 및 에지들의 가중치는 업데이트될 수 있다. 제1 생성 모델(GM1)이 학습을 수행하기 전에는 모든 노드와 에지는 초기값으로 설정될 수 있다. 그러나, 누적하여 정보가 입력될 경우, 노드 및 에지들의 가중치는 변경되고, 이 과정에서 학 습인자로 입력되는 파라미터와 출력노드로 할당되는 값 사이의 매칭이 이루어질 수 있다. 추가적으로, 클라우드 서버를 이용하는 경우, 제1 생성 모델(GM1)은 많은 수의 파라미터들을 수신하여 처리할 수 있다. 따라서, 제1 생성 모델(GM1)은 방대한 데이터에 기반하여 학습을 수행할 수 있다. 제1 생성 모델(GM1)을 구성하는 입력노드 와 출력노드 사이의 노드 및 에지의 가중치는 뉴럴 네트워크의 학습 과정에 의해 업데이트될 수 있다. 또한, 제 1 생성 모델(GM1)에서 입력 또는 출력되는 파라미터는 마스킹 이미지(MI), 텍스트 가이드(TG) 및 제1 예비 이미 지(Pre1) 이외에도 다양한 데이터로 추가 확장될 수도 있다. 이러한 제1 생성 모델(GM1)은 마스킹 이미지(MI)와 텍스트 가이드(TG)가 입력되면, 텍스트 가이드(TG)에 기초하 여 마스킹 이미지(MI)를 가공, 복원, 재구성, 및/또는 처리함으로써 제1 예비 이미지(Pre1)를 생성하도록 미리 학습될 수 있다. 다시 말하면, 제1 생성 모델(GM1)은 학습 단계(Learning Phase)를 거쳐 학습될 수 있고, 수행 단계(Inferencing Phase)에서는 학습 단계에서의 학습 결과를 기초로 연산 작업을 수행할 수 있다. 보다 상세하게, 우선 도 12에 도시된 바와 같이, 제1 생성 모델(GM1)은 학습 단계에서, 학습용 텍스트 가이드 (TG_Learn) 및 학습용 마스킹 이미지(MI_Learn)가 입력되면, 해당 학습용 텍스트 가이드(TG_Learn)에 기초하여 학습용 마스킹 이미지(MI_Learn)를 가공, 복원, 재구성 및/또는 처리함으로써 학습용 제1 예비 이미지 (Pre1_Learn)를 출력하도록 학습될 수 있다. 즉, 제1 생성 모델(GM1)은 학습 단계에서, 학습용 텍스트 가이드 (TG_Learn), 학습용 마스킹 이미지(MI_Learn) 및 학습용 제1 예비 이미지(Pre1_Learn)를 학습 데이터 셋으로써 이용할 수 있다. 이때, 학습용 제1 예비 이미지(Pre1_Learn)는 서버의 관리자로부터 입력된 데이터일 수 있다. 다시 말하면, 학습용 제1 예비 이미지(Pre1_Learn)는 학습용 데이터로써의 학습용 텍스트 가이드(TG_Learn) 및 학습 용 마스킹 이미지(MI_Learn)의 조합에 부합하도록 서버의 관리자로부터 입력된 데이터일 수 있다. 이때, 학습용 제1 예비 이미지(Pre1_Learn)가 정답 데이터, 즉 레이블링(labeling) 데이터로서 이용될 수 있다. 다시 말하면, 제1 생성 모델(GM1)의 학습 단계에서는 서버의 관리자로부터 입력된 학습용 제1 예비 이미지 (Pre1_Learn)가 레이블링 데이터로서 이용될 수 있다. 즉, 제1 생성 모델(GM1)은 학습용 텍스트 가이드(TG_Learn) 및 학습용 마스킹 이미지(MI_Learn) 각각이 입력 단 자에 입력되고, 학습용 제1 예비 이미지(Pre1_Learn)가 출력 단자에 인가되는 지도학습 방식으로 학습될 수 있 다. 다만, 이는 하나의 예시에 불과하며 본 발명이 이에 한정되는 것은 아니다. 일 예로, 제1 생성 모델(GM1)은 전술한 바와 같이 컨디셔널 디퓨전 모델을 포함할 수 있고, 이때, 제1 생성 모 델(GM1)은 학습용 마스킹 이미지(MI_Learn)에 포함된 마스킹 영역(MR)과 학습용 텍스트 가이드(TG_Learn)를 조 건(Condition)으로 하여 학습용 마스킹 이미지(MI_Learn)로부터 학습용 제1 예비 이미지(Pre1_Learn)를 출력하 도록 학습될 수 있다. 이후, 도 13에 도시된 바와 같이, 제1 생성 모델(GM1)은 수행 단계에서, 텍스트 가이드(TG_Inference)와 마스킹 이미지(MI_Inference)가 입력 데이터로서 입력되면, 해당 텍스트 가이드(TG_Inference)를 통해 마스킹 이미지 (MI_Inference)를 가공, 복원, 재구성 및/또는 처리함으로써 제1 예비 이미지(Pre1_Inference)를 출력할 수 있다. 일 예로, 제1 생성 모델(GM1)은 전술한 바와 같이 컨디셔널 디퓨전 모델을 포함할 수 있고, 이때, 제1 생성 모 델(GM1)은 마스킹 이미지(MI_Inference)에 포함된 마스킹 영역(MR)과 텍스트 가이드(TG_Inference)를 조건 (Condition)으로 하여 마스킹 이미지(MI_Inference)로부터 제1 예비 이미지(Pre1_Inference)를 생성하여 출력 할 수 있다. 제2 생성부는 입력 이미지(II)와 제1 예비 이미지(Pre1)에 기초하여 제2 예비 이미지(2nd Pre Image, 이하 “Pre2”라 한다)를 생성하고, 생성된 제2 예비 이미지(Pre2)를 대체 이미지(RI)로 결정할 수 있다. 이때, 제2 예비 이미지(Pre2)는 마스킹 영역(MR)을 다각도에서 바라본 이미지일 수 있다. 다시 말하면, 제2 예 비 이미지(Pre2)는 마스킹 영역(MR)에 상응하는 누락 영역을 다각도에서 바라본 이미지일 수 있다. 이를 통해, 제2 예비 이미지(Pre2)는 의미론적, 시각적, 기하학적으로 유의미한 정보를 포함할 수 있다. 즉, 제1 예비 이미 지(Pre1)의 경우, 마스킹 영역(MR)을 하나의 각도에서 바라본 가상의 이미지인데, 제2 생성부는 이러한 제 1 예비 이미지(Pre1)와 입력 이미지(II)를 통해 마스킹 영역(MR)을 여러 각도에서 바라본 이미지인 제2 예비 이 미지(Pre2)를 생성함으로써 대상체에 관련한 기하학적 제약을 만족하는 이미지를 생성할 수 있는 것이다. 구체적으로, 제2 생성부는 도 14에 도시된 바와 같이 미리 학습된 제2 생성 모델(2nd Generation Model, 이하 “GM2”이라 한다)을 이용하여 제2 예비 이미지(Pre2)를 생성할 수 있다. 예를 들어, 제2 생성부는 제2 생성 모델(GM2)에 입력 이미지(II)와 제1 예비 이미지(Pre1)를 입력함으로써 제2 예비 이미지(Pre2)를 생성 할 수 있다. 제2 생성 모델(GM2)은 입력 이미지(II)와 제1 예비 이미지(Pre1)가 입력되면 제2 예비 이미지(Pre2)를 생성하여 출력할 수 있다. 이때, 제2 생성 모델(GM2)은 가우시안 스플래팅(Gaussian Splatting) 기법을 통해 입력 이미지(II)와 제1 예비 이미지(Pre1)로부터 제2 예비 이미지(Pre2)를 생성할 수 있다. 이때, 입력 이미지(II)는 원본 이미지이므로, 캘 리브레이션 데이터, 예컨대 입력 이미지(II)에 대한 촬영 파라미터(카메라 위치, 카메라 각도, 카메라 렌즈왜곡 계수 등) 등이 미리 정의되어 있을 수 있다. 보다 상세하게, 제2 생성 모델(GM2)은 3D 포인트 클라우드 생성을 통해 기하학적 3D 구조를 재구성하고, 3D 구 조를 가우시안 분포로 표현하고, 3D 가우시안을 다양한 각도에서 2D 이미지로 투영하는 가우시안 스플래팅 기법 을 통해 입력 이미지(II)와 제1 예비 이미지(Pre1)로부터 마스킹 영역(MR)을 다각도에서 바라본 복수의 제2 예 비 이미지(Pre2)를 생성할 수 있다. 가우시안 스플래팅 기법에 대하여는 공지된 알고리즘이 이미 존재하는 바, 자세한 설명은 생략하기로 한다. 다시 도 1, 도 4 및 도 5를 참조하면, 생성 모듈은 생성된 대체 이미지(RI)를 후처리 모듈에 전달할 수 있다. 후처리 모듈은 입력 이미지(II) 및 대체 이미지(RI)에 기초하여 3D 모델(3DM)을 생성할 수 있다. 몇몇 예로, 후처리 모듈은 대체 이미지(RI)에 대한 캘리브레이션을 수행함으로써 최종 합성 데이터를 생성 하고, 최종 합성 데이터에 기초하여 3D 모델(3DM)을 생성할 수 있다. 이하, 도 15 내지 도 20을 참조하여, 본 발명의 몇몇 실시예에 따른 후처리 모듈의 구조 및 동작에 대해 더 자세히 설명하기로 한다. 도 15 내지 도 20은 본 발명의 몇몇 실시예에 따른 후처리 모듈의 구조 및 동작을 설명하기 위한 도면이다. 도 1, 도 4, 도 5, 도 15 내지 도 20을 참조하면, 본 발명의 몇몇 실시예에 따른 후처리 모듈은 캘리브레 이션부와 완성부를 포함할 수 있다. 캘리브레이션부는 대체 이미지(RI)에 대한 캘리브레이션(Calibration)을 수행함으로써 최종 합성 데이터 (Final Synthesis Data, 이하 “FSD”라 한다)를 생성할 수 있다. 몇몇 예로, 캘리브레이션부는 도 17에 도시된 바와 같이 입력 이미지(II), 대체 이미지(RI) 및 대체 이미 지(RI)에 대한 캘리브레이션 결과인 캘리브레이션 데이터(Calibration Data, 이하 “CD”라 한다)를 최종 합성데이터(FSD)로 결정할 수 있다. 캘리브레이션이란, 특정 이미지에 대하여 3차원 희소점(3-Dimension Sparse Point)을 획득하는 과정일 수 있다. 다시 말하면, 캘리브레이션이란 특정 이미지를 통해 3D 모델(3DM)을 생성할 때 필요한 3차원 희소점을 추정하는 과정을 포함할 수 있다. 구체적으로 설명하면, 입력 이미지(II)의 경우 원본 이미지이므로, 해당 입력 이미지 (II)를 촬영한 카메라의 위치, 각도, 렌즈왜곡 계수 등이 미리 정의된 상태, 즉 캘리브레이션이 완료된 상태일 수 있다. 이에 반하여, 대체 이미지(RI)의 경우 전술한 바와 같이 생성 모델에 의해 생성된 가상의 이미지인 바, 대체 이미지(RI)를 통해 3D 모델(3DM)을 생성하기 위해서는 해당 대체 이미지(RI)에 대한 캘리브레이션을 수행할 필요가 있다. 이를 위해, 캘리브레이션부는 도 18에 도시된 바와 같이 대체 이미지(RI)와 입력 이미지(II) 간의 위치 관 계(CD1) 및 대체 이미지(RI)에 대한 촬영 파라미터(CD2)를 포함하는 캘리브레이션 데이터(CD)를 추정할 수 있다. 다시 말하면, 캘리브레이션부에 의해 생성되는 캘리브레이션 데이터(CD)는 대체 이미지(RI)와 입력 이미지(II) 간의 위치 관계(CD1)와, 대체 이미지(RI)에 대한 촬영 파라미터(CD2)를 포함할 수 있다. 위치 관계(CD1)란 적어도 하나의 대체 이미지(RI)가 적어도 하나의 입력 이미지(II)에서 어느 위치에 속하는지 에 대한 데이터일 수 있다. 다시 말하면, 위치 관계(CD1)란, 대상체를 3D 모델(3DM)로 정의하기 위해 필요한 대 체 이미지(RI)와 입력 이미지(II) 사이의 순서 정보를 포함할 수 있다. 촬영 파라미터(CD2)는, 대체 이미지(RI)에 상응하는 카메라의 카메라 파라미터를 포함할 수 있다. 예를 들어, 촬영 파라미터(CD2)는 대체 이미지(RI)에 상응하는 카메라의 위치, 각도 및 렌즈왜곡 계수 중 적어도 하나를 포 함할 수 있다. 다시 말하면, 촬영 파라미터(CD2)는 대상체에서 대체 이미지(RI)에 상응하는 영역을 화각(View Angle) 범위로 포함하고 있는 카메라의 위치, 각도 및 렌즈왜곡 계수를 추정한 결과를 포함할 수 있다. 몇몇 예로, 캘리브레이션부는 도 19에 도시된 바와 같이 미리 학습된 캘리브레이션 모델(Calibration Model, 이하 “CM”이라 한다)에 입력 이미지(II)와 대체 이미지(RI)를 입력함으로써 캘리브레이션 데이터(CD) 를 생성할 수 있다. 이때, 캘리브레이션 모델(CM)은 SFM(Structure From Motion) 알고리즘에 기초한 모델일 수 있으나, 본 발명의 실시예가 이에 제한되는 것은 아니다. 몇몇 예로, 캘리브레이션 모델(CM)은 SFM 알고리즘을 통해 입력 이미지(II)와 대체 이미지(RI)를 비교함으로써 대체 이미지(RI)와 입력 이미지(II) 간의 위치 관계 (CD1) 및 대체 이미지(RI)에 대한 촬영 파라미터(CD2)를 포함하는 캘리브레이션 데이터(CD)를 생성할 수 있다. SFM 알고리즘은 공지된 알고리즘이므로, 자세한 설명은 생략한다. 완성부는 최종 합성 데이터(FSD)와 레퍼런스 3D 모델(REF)을 이용하여 3D 모델(3DM)을 생성할 수 있다. 몇몇 예로, 완성부는 도 20에 도시된 바와 같이 최종 합성 데이터(FSD)와 레퍼런스 3D 모델(REF)을 매핑 모델(MM)에 입력함으로써 3D 모델(3DM)을 생성할 수 있다. 이때, 매핑 모델(MM)은 최종 합성 데이터(FSD)에 포 함된 입력 이미지(II), 대체 이미지(RI) 및 캘리브레이션 데이터(CD)를 레퍼런스 3D 모델(REF)에 투영 (Projection) 시킴으로써 3D 모델(3DM)을 생성할 수 있다. 한편, 학습 모듈은 최종 합성 데이터(FSD)에 기초하여 생성 모듈을 학습시킬 수 있다. 이하, 도 21을 참조하여 학습 모듈의 동작에 대해 더 자세히 설명하기로 한다. 도 21은 본 발명의 몇몇 실시예에 따른 학습 모듈의 구조 및 동작을 설명하기 위한 도면이다. 도 1, 도 4, 도 5 및 도 21을 참조하면, 학습 모듈은 캘리브레이션부로부터 최종 합성 데이터(FSD)를 수신한 후, 이를 통해 생성 모듈에 포함된 제1 생성 모델(GM1)과 제2 생성 모델(GM2)을 학습시킬 수 있다. 즉, 학습 모듈은 캘리브레이션이 완료된 최종 합성 데이터(FSD)를 제1 생성 모델(GM1)과 제2 생성 모델 (GM2)에 전달함으로써, 제1 생성 모델(GM1)과 제2 생성 모델(GM2)이 각각의 입력 데이터로부터 캘리브레이션이 완료된 최종 합성 데이터(FSD)에 상응하거나 또는 유사한 데이터를 생성하도록 하는 제어할 수 있다. 다시 말하면, 학습 모듈은 캘리브레이션이 완료된 최종 합성 데이터(FSD)를 대체 이미지(RI)의 생성에 이 용된 제1 생성 모델(GM1)과 제2 생성 모델(GM2)에 전달하는 학습 제어 과정을 수행함으로써, 순환 학습 구조를 달성할 수 있다.도 22는 본 발명의 다른 몇몇 실시예에 따른 3D 텍스쳐 복원 서버의 블록도이다. 도 1, 도 4 내지 도 22를 참조하면, 본 발명의 다른 몇몇 실시예에 따른 3D 텍스쳐 복원 서버는 메모리 (Memory, 이하 “M”이라 한다)와 프로세서(Processor, 이하 “P”라 한다)를 포함할 수 있다. 메모리(M)는 비-일시적인 임의의 컴퓨터 판독 가능한 기록매체를 포함할 수 있다. 일 예로서, 메모리(M)는 RAM(random access memory), ROM(read only memory), 디스크 드라이브, SSD(solid state drive), 플래시 메모 리(flash memory) 등과 같은 비소멸성 대용량 저장 장치(permanent mass storage device)를 포함할 수 있다. 다른 예로서, ROM, SSD, 플래시 메모리, 디스크 드라이브 등과 같은 비소멸성 대용량 저장 장치는 메모리와는 구분되는 별도의 영구 저장 장치일 수 있다. 또한, 메모리(M)에는 운영체제(OS)와 적어도 하나의 프로그램 코드 가 저장될 수 있다. 이러한 소프트웨어 구성요소들은 메모리(M)와는 별도의 컴퓨터에서 판독 가능한 기록매체로부터 로딩될 수 있다. 이러한 별도의 컴퓨터에서 판독 가능한 기록매체는 컴퓨터에 직접 연결될 수 있는 기록 매체일 수 있고, 예를 들어, 플로피 드라이브, 디스크, 테이프, DVD/CD-ROM 드라이브, 메모리 카드 등의 컴퓨터에서 판독 가능한 기록매체를 포함할 수 있다. 또는, 소프트웨어 구성요소들은 컴퓨터에서 판독 가능한 기록매체가 아닌 통신 장 치를 통해 메모리(M)에 로딩될 수도 있다. 예를 들어, 적어도 하나의 프로그램은 개발자들 또는 어플리케이션의 설치 파일을 배포하는 파일 배포 시스템이 통신 장치를 통해 제공하는 파일들에 의해 설치되는 컴퓨터 프로그램 에 기반하여 메모리(M)에 로딩될 수 있다. 메모리(M)는 서버에 포함된 각 구성요소들의 동작과 연관된 명령, 정보 및/또는 데이터를 저장할 수 있다. 일 예로, 메모리(M)는, 실행 시에, 프로세서(P)가 본 문서에 기재된 다양한 동작을 수행할 수 있도록 하는 인스 트럭션들(instructions)을 저장할 수 있다. 다른 예로, 메모리(M)에는 프로세서(P)가 입력 이미지(II)에 기초하 여 3D 모델(3DM)을 생성할 때 이용될 수 있는 각종 알고리즘(Algorithm) 또는 모델(Model), 예컨대 제1 생성 모 델(GM1), 제2 생성 모델(GM2), 캘리브레이션 모델(CM) 및 매핑 모델(MM) 등이 저장될 수 있다. 프로세서(P)는 기본적인 산술, 로직 및 입출력 연산을 수행함으로써, 컴퓨터 프로그램의 명령을 처리할 수 있다. 여기에서, 명령은 메모리(M) 또는 외부 디바이스로부터 제공될 수 있다. 명령은 전술한 '인스트럭션'이라 는 명칭으로 호칭될 수도 있다. 이때, 프로세서(P)는 서버의 전반적인 기능을 수행하기 위하여 메모리(M) 와 작동적으로(operatively) 연결될 수 있다. 또한, 프로세서(P)는 서버에 포함된 다른 구성요소들의 동작 을 전반적으로 제어할 수 있다. 이때, 프로세서(P)에 포함된 각각의 모듈들이 수행하는 기능은 하나의 프로세서에 의해 수행되거나 또는 각각 별도의 프로세서에 의해 수행될 수도 있다. 프로세서(P)는 서버의 적어도 하나의 다른 구성요소들의 제어 및/또는 통신에 관한 연산이나 데이터 처리를 실행할 수 있다. 또한, 프로세서(P)는 다수의 논리 게이트들의 어 레이로 구현될 수도 있고, 범용적인 마이크로 프로세서와 이 마이크로 프로세서에서 실행될 수 있는 프로그램이 저장된 메모리의 조합으로 구현될 수도 있다. 예를 들어, 프로세서(P)는 범용 프로세서, 중앙 처리 장치(CPU), 마이크로프로세서, 디지털 신호 프로세서(DSP), 제어기, 마이크로제어기, 상태 머신 등을 포함할 수 한다. 일부 환경에서, 프로세서(P)는 주문형 반도체(ASIC), 프로그램 가능 로직 디바이스(PLD), 필드 프로그램 가능 게이트 어레이(FPGA) 등을 포함할 수도 있다. 예를 들어, 프로세서(P)는 디지털 신호 프로세서(DSP)와 마이크로프로세 서의 조합, 복수의 마이크로프로세서들의 조합, 디지털 신호 프로세서(DSP) 코어와 결합된 하나 이상의 마이크 로프로세서들의 조합, 또는 임의의 다른 그러한 구성들의 조합과 같은 처리 디바이스들의 조합을 지칭할 수도 있다. 프로세서(P)는 외부 데이터베이스로부터 입력 이미지(II)를 수신한 후, 수신된 입력 이미지(II)를 기초로 3D 모델(3DM)을 생성할 수 있다. 예를 들어, 프로세서(P)는 외부 데이터베이스로부터 입력 이미지(II)를 수신한 후, 수신된 입력 이미지 (II)에 기초하여 3D 텍스쳐 복원을 수행함으로써 3D 모델(3DM)을 생성할 수 있다. 다시 말하면, 프로세서(P)는 외부 데이터베이스로부터 수신된 입력 이미지(II)에서 텍스쳐가 누락된 영역을 복원함으로써 3D 모델(3D M)을 생성할 수 있다. 구체적으로 설명하면, 우선, 프로세서(P)는 입력 이미지(II)를 수신할 수 있다. 일 예로, 프로세서(P)는 외부 데이터베이스로부터 입력 이미지(II)를 수신할 수 있다. 이때, 입력 이미지(II)는 지상, 해상, 상공 등에 위치한 카메라가 탑재된 드론, 비행기, 인공위성 등을 통해 촬영된 이미지 데이터를 포함할 수 있다. 이때, 입 력 이미지(II)는 다양한 종류의 이미지 데이터를 포함할 수 있다. 일 예로, 입력 이미지(II)는 대상체를 다각도에서 촬영한 멀티뷰(Multi-view) 이미지를 포함할 수 있으나, 본 발명의 실시예가 이에 제한되는 것은 아니다. 이어서, 프로세서(P)는, 입력 이미지(II)에 기초하여 누락 영역을 식별하고, 식별된 누락 영역에 기초하여 마스 킹 이미지(MI)를 생성할 수 있다. 몇몇 예로, 프로세서(P)는, 입력 이미지(II)로부터 텍스쳐가 누락된 누락 영 역을 식별하고, 누락 영역을 마스킹(Masking) 처리함으로써 마스킹 영역(MR)이 포함된 마스킹 이미지(MI)를 생 성할 수 있다. 보다 상세하게, 프로세서(P)는 입력 이미지(II)와 레퍼런스 3D 모델(REF)을 매핑 모델(MM)에 입 력함으로써 사전 3D 모델(3DM_pre)을 생성할 수 있다. 이때, 매핑 모델(MM)은 입력 이미지(II)를 레퍼런스 3D 모델(REF)에 투영(Projection) 시킴으로써 사전 3D 모델(3DM_pre)을 생성할 수 있다. 이어서, 프로세서(P)는 사 전 3D 모델(3DM_pre)로부터 텍스쳐가 누락된 누락 영역을 식별할 수 있다. 예컨대, 프로세서(P)는 사전 3D 모델 (3DM_pre)로부터 음영(Shadow)이 존재하는 음영 영역을 식별하고, 식별된 음영 영역을 텍스쳐가 누락된 누락 영 역으로 결정할 수 있다. 이어서, 프로세서(P)는 누락 영역을 마스킹 처리함으로써 마스킹 영역(MR)이 포함된 마 스킹 이미지(MI)를 생성할 수 있다. 이때, 프로세서(P)는 결정된 누락 영역을 다각도에서 캡쳐(Capture)함으로 써 복수의 마스킹 이미지(MI)를 생성할 수 있다. 이어서, 프로세서(P)는 입력 이미지(II) 및 마스킹 이미지(MI)에 기초하여 마스킹 이미지(MI)를 대체하기 위한 대체 이미지(RI)를 생성할 수 있다. 프로세서(P)가 대체 이미지(RI)를 생성하는 과정을 보다 상세하게 설명하면, 우선, 프로세서(P)는 마스킹 이미 지(MI)에 기초하여 제1 예비 이미지(Pre1)를 생성할 수 있다. 몇몇 예로, 프로세서(P)는 마스킹 이미지(MI) 및 마스킹 이미지(MI)에 포함된 마스킹 영역(MR)을 설명하는 텍스트 가이드(TG)에 기초하여 제1 예비 이미지(Pre 1)를 생성할 수 있다. 텍스트 가이드(TG)는 마스킹 영역(MR)을 설명하는 텍스트 데이터일 수 있다. 다시 말하면, 텍스트 가이드(TG)는 마스킹 이미지(MI)에 대응하는 제1 예비 이미지(Pre1) 및/또는 대체 이미지(RI)를 생성하기 위해, 마스킹 영역(MR)을 대체할 수 있도록 해당 마스킹 영역(MR)의 특징, 구성, 배경 등을 설명하는 텍스트 데이터를 포함할 수 있다. 일 예로, 텍스트 가이드(TG)는 “둥근 창문의 대리석 벽으로 이루어진 건물의 부분”과 같은 텍스트 데이터를 포함할 수 있으나, 본 발명의 실시예가 이에 제한되지 않음은 당연하다. 이때, 텍스트 가이드(TG)는 서버의 메모리(M)에 미리 저장되거나 또는 서버의 관리자로부터 입력된 데이터 일 수 있다. 예컨대, 프로세서(P)는 도 10에 도시된 바와 같이 미리 학습된 제1 생성 모델(GM1)을 이용하여 제1 예비 이미지(Pre1)를 생성할 수 있다. 예를 들어, 프로세서(P)는 제1 생성 모델(GM1)에 마스킹 이미지(MI)와 텍 스트 가이드(TG)를 입력함으로써 제1 예비 이미지(Pre1)를 생성할 수 있다. 이때, 제1 생성 모델(GM1)은 디퓨전 모델(Diffusion Model)을 포함할 수 있다. 일 예로, 제1 생성 모델(GM1)은 컨디셔널 디퓨전 모델(Conditional Diffusion Model)을 포함할 수 있으나, 본 발명의 실시예가 이에 제한되는 것은 아니다. 이때, 제1 생성 모델 (GM1)은 전술한 바와 같이 마스킹 이미지(MI)에 포함된 마스킹 영역(MR)과 텍스트 가이드(TG)를 조건 (Condition)으로 하여 마스킹 이미지(MI)로부터 제1 예비 이미지(Pre1)를 생성할 수 있다. 이어서, 프로세서(P)는 입력 이미지(II)와 제1 예비 이미지(Pre1)에 기초하여 제2 예비 이미지(Pre2)를 생성하 고, 생성된 제2 예비 이미지(Pre2)를 대체 이미지(RI)로 결정할 수 있다. 이때, 제2 예비 이미지(Pre2)는 마스 킹 영역(MR)을 다각도에서 바라본 이미지일 수 있다. 다시 말하면, 제2 예비 이미지(Pre2)는 마스킹 영역(MR)에 상응하는 누락 영역을 다각도에서 바라본 이미지일 수 있다. 이를 통해, 제2 예비 이미지(Pre2)는 의미론적, 시 각적, 기하학적으로 유의미한 정보를 포함할 수 있다. 즉, 제1 예비 이미지(Pre1)의 경우, 마스킹 영역(MR)을 하나의 각도에서 바라본 가상의 이미지인데, 프로세서(P)는 이러한 제1 예비 이미지(Pre1)와 입력 이미지(II)를 통해 마스킹 영역(MR)을 여러 각도에서 바라본 이미지인 제2 예비 이미지(Pre2)를 생성함으로써 대상체에 관련 한 기하학적 제약을 만족하는 이미지를 생성할 수 있는 것이다. 구체적으로, 프로세서(P)는 도 14에 도시된 바 와 같이 미리 학습된 제2 생성 모델(GM2)을 이용하여 제2 예비 이미지(Pre2)를 생성할 수 있다. 예를 들어, 프 로세서(P)는 제2 생성 모델(GM2)에 입력 이미지(II)와 제1 예비 이미지(Pre1)를 입력함으로써 제2 예비 이미지 (Pre2)를 생성할 수 있다. 제2 생성 모델(GM2)은 입력 이미지(II)와 제1 예비 이미지(Pre1)가 입력되면 제2 예 비 이미지(Pre2)를 생성하여 출력할 수 있다. 이때, 제2 생성 모델(GM2)은 가우시안 스플래팅(Gaussian Splatting) 기법을 통해 입력 이미지(II)와 제1 예비 이미지(Pre1)로부터 제2 예비 이미지(Pre2)를 생성할 수 있다. 이어서, 프로세서(P)는 입력 이미지(II) 및 대체 이미지(RI)에 기초하여 3D 모델(3DM)을 생성할 수 있다. 몇몇 예로, 프로세서(P)는 대체 이미지(RI)에 대한 캘리브레이션을 수행함으로써 최종 합성 데이터(FSD)를 생성하고, 최종 합성 데이터(FSD)에 기초하여 3D 모델(3DM)을 생성할 수 있다. 프로세서(P)가 입력 이미지(II) 및 대체 이미지(RI)에 기초하여 3D 모델(3DM)을 생성하는 과정을 보다 상세하게 설명하면, 우선, 프로세서(P)는 대체 이미지(RI)에 대한 캘리브레이션(Calibration)을 수행함으로써 최종 합성 데이터(FSD)를 생성할 수 있다. 몇몇 예로, 프로세서(P) 도 17에 도시된 바와 같이 입력 이미지(II), 대체 이미 지(RI) 및 대체 이미지(RI)에 대한 캘리브레이션 결과인 캘리브레이션 데이터(CD)를 최종 합성 데이터(FSD)로 결정할 수 있다. 캘리브레이션이란, 특정 이미지에 대하여 3차원 희소점(3-Dimension Sparse Point)을 획득하는 과정일 수 있다. 다시 말하면, 캘리브레이션이란 특정 이미지를 통해 3D 모델(3DM)을 생성할 때 필요한 3차원 희소점을 추정하는 과정을 포함할 수 있다. 구체적으로 설명하면, 입력 이미지(II)의 경우 원본 이미지이므로, 해당 입력 이미지(II)를 촬영한 카메라의 위치, 각도, 렌즈왜곡 계수 등이 미리 정의된 상태, 즉 캘리브레이션 이 완료된 상태일 수 있다. 이에 반하여, 대체 이미지(RI)의 경우 전술한 바와 같이 생성 모델에 의해 생성된 가상의 이미지인 바, 대체 이미지(RI)를 통해 3D 모델(3DM)을 생성하기 위해서는 해당 대체 이미지(RI)에 대한 캘리브레이션을 수행할 필요가 있다. 이를 위해, 프로세서(P)는 도 18에 도시된 바와 같이 대체 이미지(RI)와 입력 이미지(II) 간의 위치 관계(CD1) 및 대체 이미지(RI)에 대한 촬영 파라미터(CD2)를 포함하는 캘리브레이션 데이터(CD)를 추정할 수 있다. 다시 말하면, 프로세서(P)에 의해 생성되는 캘리브레이션 데이터(CD)는 대체 이 미지(RI)와 입력 이미지(II) 간의 위치 관계(CD1)와, 대체 이미지(RI)에 대한 촬영 파라미터(CD2)를 포함할 수 있다. 이때, 프로세서(P)는 도 19에 도시된 바와 같이 미리 학습된 캘리브레이션 모델(CM)에 입력 이미지(II)와 대체 이미지(RI)를 입력함으로써 캘리브레이션 데이터(CD)를 생성할 수 있다. 이어서, 프로세서(P)는 최종 합성 데이터(FSD)와 레퍼런스 3D 모델(REF)을 이용하여 3D 모델(3DM)을 생성할 수 있다. 몇몇 예로, 프로세서(P)는 도 20에 도시된 바와 같이 최종 합성 데이터(FSD)와 레퍼런스 3D 모델(REF)을 매핑 모델(MM)에 입력함으로써 3D 모델(3DM)을 생성할 수 있다. 이때, 매핑 모델(MM)은 최종 합성 데이터(FSD) 에 포함된 입력 이미지(II), 대체 이미지(RI) 및 캘리브레이션 데이터(CD)를 레퍼런스 3D 모델(REF)에 투영 (Projection) 시킴으로써 3D 모델(3DM)을 생성할 수 있다. 한편, 프로세서(P)는 최종 합성 데이터(FSD)를 통해 제1 생성 모델(GM1)과 제2 생성 모델(GM2)을 학습시킬 수 있다. 즉, 프로세서(P)는 캘리브레이션이 완료된 최종 합성 데이터(FSD)를 제1 생성 모델(GM1)과 제2 생성 모델 (GM2)에 전달함으로써, 제1 생성 모델(GM1)과 제2 생성 모델(GM2)이 각각의 입력 데이터로부터 캘리브레이션이 완료된 최종 합성 데이터(FSD)에 상응하거나 또는 유사한 데이터를 생성하도록 하는 제어할 수 있다. 다시 말하 면, 프로세서(P)는 캘리브레이션이 완료된 최종 합성 데이터(FSD)를 대체 이미지(RI)의 생성에 이용된 제1 생성 모델(GM1)과 제2 생성 모델(GM2)에 전달하는 학습 제어 과정을 수행함으로써, 순환 학습 구조를 달성할 수 있다. 도 23은 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 방법의 흐름도이다. 도 24는 본 발명의 몇몇 실시예에 따른 도 23의 단계(S200)의 세부 흐름도이다. 도 25는 본 발명의 몇몇 실시예에 따른 도 24의 단계(S220)의 세 부 흐름도이다. 도 26은 본 발명의 몇몇 실시예에 따른 도 23의 단계(S300)의 세부 흐름도이다. 도 27은 본 발 명의 몇몇 실시예에 따른 도 26의 단계(S310)의 세부 흐름도이다. 도 28은 본 발명의 몇몇 실시예에 따른 도 26 의 단계(S320)의 세부 흐름도이다. 도 29는 본 발명의 몇몇 실시예에 따른 도 23의 단계(S400)의 세부 흐름도이 다. 도 30은 본 발명의 몇몇 실시예에 따른 도 29의 단계(S410)의 세부 흐름도이다. 도 31은 본 발명의 몇몇 실 시예에 따른 도 29의 단계(S420)의 세부 흐름도이다. 도 23 내지 도 31의 각 단계(S100, S200, S210 내지 S230, S221 및 S222, S310 및 S320, S311 및 S312, S321 및 S322, S410 및 S420, S411 및 S412, S421 및 S422)는 도 1, 도 4 및 도 22의 서버에 의해 수행될 수 있다. 이하, 전술한 내용과 중복되는 내용은 제외 하고 간략하게 서술한다. 도 1, 도 4 내지 도 31을 참조하면, 우선, 서버는 대상체를 촬영한 입력 이미지(II)를 수신할 수 있다 (S100). 일 예로, 서버는 외부 데이터베이스로부터 입력 이미지(II)를 수신할 수 있다. 이때, 입력 이미지 (II)는 지상, 해상, 상공 등에 위치한 카메라가 탑재된 드론, 비행기, 인공위성 등을 통해 촬영된 이미지 데이 터를 포함할 수 있다. 이때, 입력 이미지(II)는 다양한 종류의 이미지 데이터를 포함할 수 있다. 일 예로, 입력 이미지(II)는 대상체를 다각도에서 촬영한 멀티뷰(Multi-view) 이미지를 포함할 수 있으나, 본 발명의 실시예가 이에 제한되는 것은 아니다. 이어서, 서버는 입력 이미지(II)로부터 텍스쳐(Texture)가 누락된 누락 영역을 식별하고, 누락 영역을 마 스킹(Masking) 처리함으로써 마스킹 영역(MR)이 포함된 마스킹 이미지(MI)를 생성할 수 있다(S200).보다 상세하게, 우선, 서버는 입력 이미지(II)를 기초로 사전 3D 모델(3DM_pre)을 생성할 수 있다(S210). 보다 상세하게, 서버는 입력 이미지(II)와 레퍼런스 3D 모델(REF)을 매핑 모델(MM)에 입력함으로써 사전 3D 모델(3DM_pre)을 생성할 수 있다. 이때, 매핑 모델(MM)은 입력 이미지(II)를 레퍼런스 3D 모델(REF)에 투영 (Projection) 시킴으로써 사전 3D 모델(3DM_pre)을 생성할 수 있다. 이어서, 서버는 생성된 사전 3D 모델 (3DM_pre)로부터 누락 영역을 식별할 수 있다(S220). 구체적으로, 서버는, 사전 3D 모델(3DM_pre)로부터 음영(Shadow)이 존재하는 음영 영역을 식별할 수 있고(S221), 식별된 음영 영역을 누락 영역으로 결정할 수 있 다(S222). 이어서, 서버는 누락 영역을 다각도에서 캡쳐(Capture)함으로써 복수의 마스킹 이미지를 생성할 수 있다(S230). 이어서, 서버는 입력 이미지(II) 및 마스킹 이미지(MI)에 기초하여 마스킹 이미지(MI)를 대체하기 위한 대 체 이미지(RI)를 생성할 수 있다(S300). 보다 상세하게, 서버는 마스킹 이미지(MI) 및 마스킹 영역(MR)을 설명하는 텍스트 가이드(TG)에 기초하여 제1 예비 이미지(Pre1)를 생성할 수 있다(S310). 구체적으로, 서버 는 미리 학습된 제1 생성 모델(GM1)에 마스킹 이미지(MI) 및 텍스트 가이드(TG)를 입력할 수 있고(S311), 제1 생성 모델(GM1)로부터 출력된 제1 예비 이미지(Pre1)를 획득할 수 있다(S312). 이어서, 서버는 입력 이미지(II) 및 제1 예비 이미지(Pre1)에 기초하여 제2 예비 이미지(Pre2)를 생성하고, 생성된 제2 예비 이미지 (Pre2)를 대체 이미지(RI)로 결정할 수 있다(S320). 구체적으로, 서버는 미리 학습된 제2 생성 모델(GM2) 에 입력 이미지(II) 및 제1 예비 이미지(Pre1)를 입력할 수 있고(S321), 이후 제2 생성 모델(GM2)로부터 출력된 제2 예비 이미지(Pre2)를 획득할 수 있다(S322). 이어서, 서버는 입력 이미지(II) 및 대체 이미지(RI)에 기초하여 3D 모델(3DM)을 생성할 수 있다(S400). 보다 상세하게, 서버는 입력 이미지(II) 및 대체 이미지(RI)에 대한 캘리브레이션(Calibration)을 수행함 으로써 최종 합성 데이터(FSD)를 생성할 수 있다(S410). 구체적으로, 서버는 대체 이미지(RI)와 복수의 입 력 이미지(II) 간의 위치 관계(CD1) 및 대체 이미지(RI)에 상응하는 촬영 파라미터(CD2)를 포함하는 캘리브레이 션 데이터(CD)를 추정할 수 있고(S411), 입력 이미지(II), 대체 이미지(RI) 및 캘리브레이션 데이터(CD)를 최종 합성 데이터(FSD)로 결정할 수 있다(S412). 이어서, 서버는 생성된 최종 합성 데이터(FSD)에 기초하여 3D 모델(3DM)을 생성할 수 있다(S420). 구체적으로, 서버는 미리 학습된 매핑 모델(MM)에 최종 합성 데이터 (FSD)를 입력할 수 있고(S421), 매핑 모델(MM)로부터 출력된 3D 모델(3DM)을 획득할 수 있다(S422). 도 32는 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버의 하드웨어 구현을 설명하기 위한 도면이다. 도 1 및 도 32를 참조하면, 본 발명의 몇몇 실시예들에 따른 서버는 전자 장치로 구현될 수 있다. 전자 장치는 컨트롤러(1010, controller), 입출력 장치(1020, I/O), 메모리 장치(1030, memory device), 인터페이스(1040, interface) 및 버스(1050, bus)를 포함할 수 있다. 컨트롤러, 입출력 장치, 메 모리 장치 및/또는 인터페이스는 버스를 통하여 서로 결합될 수 있다. 이때, 버스는 데이터들이 이동되는 통로(path)에 해당한다. 구체적으로, 컨트롤러는 CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit), 마이크로프로세서, 디지털 신호 프로세스, 마이크로컨트롤 러, 어플리케이션 프로세서(AP, application processor) 및 이들과 유사한 기능을 수행할 수 있는 논리 소자들 중에서 적어도 하나를 포함할 수 있다. 입출력 장치는 키패드(keypad), 키보드, 터치스크린 및 디스플레이 장치 중 적어도 하나를 포함할 수 있 다. 메모리 장치는 데이터 및/또는 프로그램 등을 저장할 수 있다. 인터페이스는 통신 네트워크로 데이터를 전송하거나 통신 네트워크로부터 데이터를 수신하는 기능을 수행 할 수 있다. 인터페이스는 유선 또는 무선 형태일 수 있다. 예컨대, 인터페이스는 안테나 또는 유 무선 트랜시버 등을 포함할 수 있다. 도시하지 않았지만, 메모리 장치는 컨트롤러의 동작을 향상시 키기 위한 동작 메모리로서, 고속의 디램 및/또는 에스램 등을 더 포함할 수도 있다. 메모리 장치는 내부 에 프로그램 또는 어플리케이션을 저장할 수 있다. 본 발명의 실시예들에 따른 서버는 복수의 전자 장치가 네트워크를 통해서 서로 연결되어 형성된 시 스템일 수 있다. 이러한 경우에는 각각의 모듈 또는 모듈의 조합들이 전자 장치로 구현될 수 있다. 단,본 실시예가 이에 제한되는 것은 아니다. 추가적으로, 서버는 워크스테이션(workstation), 데이터 센터, 인터넷 데이터 센터(internet data center(IDC)), DAS(direct attached storage) 시스템, SAN(storage area network) 시스템, NAS(network attached storage) 시스템, RAID(redundant array of inexpensive disks, or redundant array of independent disks) 시스템, 및 EDMS(Electronic Document Management) 시스템 중 적어도 하나로 구현될 수 있으나, 본 실 시예가 이에 제한되는 것은 아니다. 또한, 서버는 네트워크를 통해서 외부 데이터베이스에 데이터를 전송할 수 있다. 네트워크는 유선 인 터넷 기술, 무선 인터넷 기술 및 근거리 통신 기술에 의한 네트워크를 포함할 수 있다. 유선 인터넷 기술은 예 를 들어, 근거리 통신망(LAN, Local area network) 및 광역 통신망(WAN, wide area network) 중 적어도 하나를 포함할 수 있다. 무선 인터넷 기술은 예를 들어, 무선랜(Wireless LAN: WLAN), DMNA(Digital Living Network Alliance), 와이브 로(Wireless Broadband: Wibro), 와이맥스(World Interoperability for Microwave Access: Wimax), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), IEEE 802.16, 롱 텀 에볼루션(Long Term Evolution: LTE), LTE-A(Long Term Evolution-Advanced), 광대역 무선 이동 통신 서비스 (Wireless Mobile Broadband Service: WMBS) 및 5G NR(New Radio) 기술 중 적어도 하나를 포함할 수 있다. 단, 본 실시예가 이에 제한되는 것은 아니다. 근거리 통신 기술은 예를 들어, 블루투스(Bluetooth), RFID(Radio Frequency Identification), 적외선 통신 (Infrared Data Association: IrDA), UWB(Ultra-Wideband), 지그비(ZigBee), 인접 자장 통신(Near Field Communication: NFC), 초음파 통신(Ultra Sound Communication: USC), 가시광 통신(Visible Light Communication: VLC), 와이 파이(Wi-Fi), 와이 파이 다이렉트(Wi-Fi Direct), 5G NR (New Radio) 중 적어도 하 나를 포함할 수 있다. 단, 본 실시예가 이에 제한되는 것은 아니다. 네트워크를 통해서 통신하는 서버는 이동통신을 위한 기술표준 및 표준 통신 방식을 준수할 수 있다. 예를 들어, 표준 통신 방식은 GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), CDMA2000(Code Division Multi Access 2000), EV-DO(Enhanced Voice-Data Optimized or Enhanced Voice-Data Only), WCDMA(Wideband CDMA), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), LTE(Long Term Evolution), LTEA(Long Term Evolution-Advanced) 및 5G NR(New Radio) 중 적어도 하 나를 포함할 수 있다. 단, 본 실시예가 이에 제한되는 것은 아니다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17 도면18 도면19 도면20 도면21 도면22 도면23 도면24 도면25 도면26 도면27 도면28 도면29 도면30 도면31 도면32"}
{"patent_id": "10-2024-0174713", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 시스템을 도시한 것이다. 도 2 및 도 3은 텍스쳐 누락을 설명하기 위한 도면이다. 도 4는 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버의 블록도이다. 도 5는 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버의 동작을 설명하기 위한 개념도이다. 도 6 및 도 7은 본 발명의 몇몇 실시예에 따른 전처리 모듈의 구조 및 동작을 설명하기 위한 도면이다. 도 8 내지 도 14는 본 발명의 몇몇 실시예에 따른 생성 모듈의 구조 및 동작을 설명하기 위한 도면이다. 도 15 내지 도 20은 본 발명의 몇몇 실시예에 따른 후처리 모듈의 구조 및 동작을 설명하기 위한 도면이다. 도 21은 본 발명의 몇몇 실시예에 따른 학습 모듈의 구조 및 동작을 설명하기 위한 도면이다.도 22는 본 발명의 다른 몇몇 실시예에 따른 3D 텍스쳐 복원 서버의 블록도이다. 도 23은 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 방법의 흐름도이다. 도 24는 본 발명의 몇몇 실시예에 따른 도 23의 단계(S200)의 세부 흐름도이다. 도 25는 본 발명의 몇몇 실시예에 따른 도 24의 단계(S220)의 세부 흐름도이다. 도 26은 본 발명의 몇몇 실시예에 따른 도 23의 단계(S300)의 세부 흐름도이다. 도 27은 본 발명의 몇몇 실시예에 따른 도 26의 단계(S310)의 세부 흐름도이다. 도 28은 본 발명의 몇몇 실시예에 따른 도 26의 단계(S320)의 세부 흐름도이다. 도 29는 본 발명의 몇몇 실시예에 따른 도 23의 단계(S400)의 세부 흐름도이다. 도 30은 본 발명의 몇몇 실시예에 따른 도 29의 단계(S410)의 세부 흐름도이다. 도 31은 본 발명의 몇몇 실시예에 따른 도 29의 단계(S420)의 세부 흐름도이다. 도 32는 본 발명의 몇몇 실시예에 따른 3D 텍스쳐 복원 서버의 하드웨어 구현을 설명하기 위한 도면이다."}
