{"patent_id": "10-2022-0132468", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0052441", "출원번호": "10-2022-0132468", "발명의 명칭": "추적기용 DQN 모델 학습 방법 및 이에 의해 학습된 DQN 모델 기반 추적기", "출원인": "주식회사 디엠스튜디오", "발명자": "권기룡"}}
{"patent_id": "10-2022-0132468", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "추적기용 DQN 모델 학습 방법으로서,상기 추적기용 DQN 모델은 가상 현실 환경에서 작동하는 클라이언트를 사용하여 가상 시뮬레이션 플랫폼에 연결되며, 순차적 비디오 프레임을 입력으로 학습하되,상기 추적기용 DAN 모델은 순환 신경망 기반으로 가상 시뮬레이션 환경에 대한 특징과 정보를 학습하여 가상 환경 시뮬레이션에서 시각적 객체 추적을 수행할 수 있도록 학습하는, 추적기용 DQN 모델 학습 방법."}
{"patent_id": "10-2022-0132468", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 추적용 DQN 모델은 매니 배치를 효과적으로 샘플링하고 정확한 상태 표현을 생성하기 위해, 가상 시뮬레이션 플랫폼에 연결되는 응답 버퍼 메모리 유닛을 이용하여 학습 및 추적을 수행하는, 추적기용 DQN 모델 학습 방법."}
{"patent_id": "10-2022-0132468", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,LSTM(Long Short-Term Memory) 레이어가 상기 추적용 DQN 모델에는 적용되어 상기 추적용 DQN 모델이 입력되는순차적 비디오 프레임을 학습하는, 추적기용 DQN 모델 학습 방법."}
{"patent_id": "10-2022-0132468", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,상기 추적용 DQN 모델의 관찰 부분은 각 프레임에서 예측된 경계 상자 위치로 출력을 제공하고, 경계 상자의 결합에 대한 교차점의 계산에 따라 추적 단위가 통합되는, 추적기용 DQN 모델 학습 방법."}
{"patent_id": "10-2022-0132468", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "가상 시뮬레이션 플랫폼에 연결되고, 순차적 비디오 프레임을 입력으로 하여 학습된 DQN 모델을 탑재하는 추적기로서,상기 DQN 모델은 제 1 항 내지 제 4 항 중 어느 하나의 학습 방법에 따라 학습된, DQN 모델 기반 추적기."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 추적기용 DQN 모델 학습 방법에 관한 것으로, 추적기용 DQN 모델은 가상 현실 환경에서 작동하는 클라 이언트를 사용하여 가상 시뮬레이션 플랫폼에 연결되며, 순차적 비디오 프레임을 입력으로 학습하되, 순환 신경 망 기반으로 가상 시뮬레이션 환경에 대한 특징과 정보를 학습하여 가상 환경 시뮬레이션에서 시각적 객체 추적 을 수행할 수 있도록 학습할 수 있다."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 추적기에 이용되는 DQN 모델의 학습 방법 및 이에 의해 학습된 DQN 모델 기반의 추적기에 관한 것이다."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "시각적 객체 추적(Visual Object Tracking)은 모든 프레임에서 객체를 인식하고 다른 객체와 구별하는 기술과 관련된다. 객체는 순차 프레임에서 정적 또는 동적일 수 있다. 시각적 객체 추적은 폐색, 모션 블리, 배경 노이 즈, 주변 조명의 변화 등 많은 것을 고려해야 한다는 어려움이 있다. 이러한 어려움을 해결하기 위한 가장 일반적인 추적 접근 방식은 다양한 기능 학습 알고리즘을 활용하여 특정 객체 클래스를 모니터링하는 것이다.몇 가지 추적 기술은 뛰어난 효율성과 경쟁력 있는 결과를 제공하기는 하지만, 높은 정확도와 더 빠른 추적 성 능을 달성하기 위해서는 해결해야 하는 제약 조건이 여전히 존재한다. 기존 방법 중 경쟁력 있는 방식으로 일부 추정 필터와 객체 감지 기반 모델이 있지만, 더 뛰어난 성능을 얻기 위해서는 여전히 여러 단점을 가지고 있다는 한계가 있다. 심층 CNN(Deep convolutional neural network) 기반 시각적 객체 추적 모델(CNN 기반 추적 모델)은 최근 많은 관심을 받고 있으며, 추적 견고성이 우수하고, 매우 효율적인 기능 표현이 가능하다는 점에서 이점이 있다. CNN 기반 추적 모델은 표적 추적 상황에서 물체를 찾고 분류하거나 자르고 회귀하는 방법을 위해 사전 훈련된 CNN 분류기를 사용한다. 그러나 분류를 위한 CNN 기반 기능 표현과 추적 알고리즘 사이의 차이는 최종 추적 결과의 출력에 영향을 준다 는 문제점이 있고, 사전 훈련된 CNN 분류기는 훈련 중에 포착된 공간 특징이 철저히 학습되지 않는 까다로운 환 경에서 제대로 작동하지 못한다는 문제점이 있다. CNN 기반 추적 모델은 종래 추적기와 비교하여 우수한 효과를 보인다는 것이 입증되기는 하였으나, 수많은 폐색 프레임과, 약간의 상관 거리가 있고 시각적으로 혼합한 장면에서는 대상을 놓칠 수 있다는 문제점이 있다. 따라서, 수많은 폐색 프레임과, 약간의 상관 거리가 있고 시각적으로 혼합한 장명에서도 우수한 추적 성능을 보 일 수 있는 기술에 대한 연구가 요구되고 있다. 예를 들어, 가상환경 시뮬레이션에서도 객체(ex, 드론)에 대한 추정 성능이 우수한 추적 기술에 대한 연구가 요구된다."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "이러한 배경기술은 발명자가 본 발명의 도출을 위해 보유하고 있었거나, 본 발명의 도출 과정에서 습득한 기술 정보로서, 반드시 본 발명의 출원 전에 일반 공중에게 공개된 공지기술이라 할 수는 없다."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 명세서에서 개시되는 실시예들은 전술한 문제점을 해결하기 위하여 안출된 것으로, 심층 강화 학습과 인공지 능 네트워크 모델을 사용하여 대상을 학습하고 추적할 수 있는 DQN 모델의 학습 방법을 제공하는 것을 기술적 과제로 한다. 또한, 실시예들은 본 발명의 실시예에 따른 학습 방법을 이용하여 학습된 DQN 모델을 기반으로 가상 환경 시뮬 레이션 플랫폼 상의 시각적 객체를 추적하는 추적기를 제공하는 것을 기술적 과제로 한다. 본 발명의 기술적 과제는 이상에서 언급한 사항에 제한되지 않으며, 이하의 기재로부터 본 발명이 속하는 기술 분야의 통상의 지식을 가진 자라면 본 발명이 의도하는 기타의 과제들 또한 명료하게 이해할 수 있을 것이다."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 기술적 수단으로서, 실시예에 따른 추적기용 DQN 모델 학습 방법에 따르면, 추적기용 DQN 모델은 가상 현실 환경에서 작동하는 클라이언트를 사용하여 가상 시뮬레이션 플랫폼에 연결되며, 순차적 비디오 프레임을 입력으로 학습할 수 있다. 실시예에 따르면, 추적기용 DAN 모델은 순환 신경망 기반으로 가상 시뮬레이션 환경에 대한 특징과 정보를 학습 하여 가상 환경 시뮬레이션에서 시각적 객체 추적을 수행할 수 있도록 학습할 수 있다. 실시예에 따르면, 추적용 DQN 모델은 매니 배치를 효과적으로 샘플링하고 정확한 상태 표현을 생성하기 위해, 가상 시뮬레이션 플랫폼에 연결되는 응답 버퍼 메모리 유닛을 이용하여 학습 및 추적을 수행할 수 있다. 실시예에 따르면, LSTM(Long Short-Term Memory) 레이어가 추적용 DQN 모델에는 적용되어 추적용 DQN 모델이 입력되는 순차적 비디오 프레임을 학습할 수 있다. 실시예에 따르면, 추적용 DQN 모델의 관찰 부분은 각 프레임에서 예측된 경계 상자 위치로 출력을 제공하고, 경"}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "계 상자의 결합에 대한 교차점의 계산에 따라 추적 단위가 통합될 수 있다.위에서 언급된 과제의 해결 수단 이외의 본 발명의 다양한 예에 따른 구체적인 사항들은 아래의 기재 내용 및 도면들에 포함되어 있다."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 명세서에서 개시되는 실시예에 따르면, 심층 강화 학습과 인공지능 네트워크 모델을 사용하여 대상을 학습하 고 추적할 수 있는 DQN 모델의 학습 방법이 제공될 수 있다. 본 발명의 학습 방법에 의해 학습된 DQN 모델 기반의 추적기는 가상 환경 시뮬레이션 플랫폼 상의 시각적 객체 를 추적할 수 있다."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 이상에서 언급한 효과에 제한되지 않으며, 언급되지 않은 또 다른 효과는 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다. 위에서 언급된 해결하고자 하는 과제, 과제 해결 수단, 효과의 내용은 청구범위의 필수적인 특징을 특정하는 것"}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "은 아니므로, 청구범위의 권리 범위는 발명의 내용에 기재된 사항에 의하여 제한되지 않는다."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나, 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서 로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명 은 청구항의 범주에 의해 정의될 뿐이다. 본 발명의 실시예를 설명하기 위한 도면에 개시된 형상, 크기, 비율, 각도, 개수 등은 예시적인 것이므로 본 발 명이 도시된 사항에 한정되는 것은 아니다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 또한, 본 발명을 설명함에 있어서, 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐 릴 수 있다고 판단되는 경우 그 상세한 설명은 생략한다. 본 명세서 상에서 언급한 \"포함한다\", \"갖는다\", \"이 루어진다\" 등이 사용되는 경우 \"만\"이 사용되지 않는 이상 다른 부분이 추가될 수 있다. 구성 요소를 단수로 표 현한 경우에 특별히 명시적인 기재 사항이 없는 한 복수를 포함하는 경우를 포함한다. 구성 요소를 해석함에 있어서, 오차 범위에 대한 별도의 명시적 기재가 없더라도 오차 범위를 포함하는 것으로 해석한다. 시간 관계에 대한 설명일 경우, \"후에\", \"이어서\", \"다음에\", \"전에\" 등으로 시간적 선후 관계가 설명되는 경우, \"바로\" 또는 \"직접\"이 사용되지 않는 이상 연속적이지 않은 경우도 포함할 수 있다. 제1, 제2 등이 다양한 구성요소들을 서술하기 위해서 사용되나, 이들 구성요소들은 이들 용어에 의해 제한되지 않는다. 이들 용어들은 단지 하나의 구성 요소를 다른 구성요소와 구별하기 위하여 사용하는 것이다. 따라서, 이하에서 언급되는 제1 구성요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있다. 본 발명의 구성 요소를 설명하는 데에 있어서, 제 1, 제 2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이러 한 용어는 그 구성 요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성 요소의 본질, 차례, 순서 또는 개수 등이 한정되지 않는다. 어떤 구성 요소가 다른 구성요소에 \"연결\", \"결합\" 또는 \"접속\"된 다고 기재된 경우, 그 구성 요소는 그 다른 구성요소에 직접적으로 연결되거나 또는 접속될 수 있지만, 특별히 명시적인 기재 사항이 없는 간접적으로 연결되거나 또는 접속될 수 있는 각 구성 요소 사이에 다른 구성 요소가 \"개재\"될 수도 있다고 이해되어야 할 것이다. \"적어도 하나\"는 연관된 구성요소의 하나 이상의 모든 조합을 포함하는 것으로 이해되어야 할 것이다. 예를 들 면, \"제1, 제2, 및 제3 구성요소의 적어도 하나\"의 의미는 제1, 제2, 또는 제3 구성요소뿐만 아니라, 제1, 제2, 및 제3 구성요소의 두 개 이상의 모든 구성요소의 조합을 포함한다고 할 수 있다. 본 명세서의 여러 실시예들의 각각 특징들이 부분적으로 또는 전체적으로 서로 결합 또는 조합 가능하고, 기술 적으로 다양한 연동 및 구동이 가능하며, 각 실시예들이 서로에 대하여 독립적으로 실시 가능할 수도 있고 연관 관계로 함께 실시할 수도 있다. 이하, 첨부된 도면 및 실시예를 통해 본 발명의 실시예를 살펴보면 다음과 같다. 도면에 도시된 구성요소들의 스케일은 설명의 편의를 위해 실제와 다른 스케일을 가지므로, 도면에 도시된 스케일에 한정되지 않는다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예에 따른 추적기용 DQN 모델 학습 방법 및 이에 의해 학습된 DQN 모델 기반 추적기에 대해서 설명한다. 도 1은 본 발명의 실시예에 따른 DQN 기반 추적기가 가상 도시 환경 시뮬레이션 플랫폼과 연동되어 구축된 상태 의 프레임워크를 도시하고 있다. 본 발명에서는, 비디오 프레임의 런타임 시퀀스를 얻고 에피소드의 각 이미지에서 대상 개체를 찾기 위해 가상 환경에 연결된 네트워크를 구축할 것이 제안된다. 본 발명의 추적기는 성공적인 작업에서 영감을 받은 행동 결정 기법과 통합된 장기 시간 시퀀스에서 학습 하고 예측하기 위해 가장 잘 알려진 RNN(Recurrent Neural Network, 순환 신경망) 중 하나인 DQN(Deep Q- Network) 모델을 이용한다. RNN은 VCE(Virtual Computing Environment) 시뮬레이션 플랫폼이 시간 순서를 따라 직접 또는 간접 그래프의 노드를 연결하여 시간 동적 동작을 나타낼 수 있도록 한다. 본 발명의 DQN 모델은 심층 강화 학습 기반으로 하며, 심층 강화 학습 기반의 DQN 모델은 객체 추적 절차의 출 력으로 행동 결정을 위한 순환 신경망 기반 훈련 시퀀스를 사용하여 훈련할 수 있다. 도 1에서와 같이, 시뮬레이션 플랫폼(ex, AirSim 시뮬레이션 플랫폼)에는 AirSim Python Client가 연결되어 접 근 방식을 테스트할 수 있으며, Multirotor(드론)와 Car 옵션이 있다. 본 발명에서는 추적기(또는 DQN 모델)가 도 1의 구조를 사용하여 다양한 시뮬레이션 환경 조건에서 정책을 제어하여 순차적 비디오 프레임을 학습할 수 있다. 이와 같이, 본 발명의 추적기에 탑재되는 신경망 모델은 강화 학습과 반복 신경망 방법을 통합하여 구현될 수 있다. RNN은 순차적 상황을 적용하고 환경 목표 속성을 예측하는 데에 효과적인 것으로, 가상환경 시뮬레이션에서의 시각적 객체 추적을 위한 본 발명의 추적기에 RNN 기반의 DQN 모델이 적용되는 것이다. 본 발명의 추적기는 DQN 모델로 구현되어, UAV(Unmanned Air Vehicle) 에이전트의 전략을 탐색하고 학습하 기 시작하는 동안 VCE 모델을 비교적 빠르게 학습할 수 있다. 도 2는 본 발명의 실시예에 따른 DQN 모델이 학습하는 절차를 도시하고 있다. 취한 조취 및 상태 값은 학습의 다음 단계를 위해 초기 압력 값으로 저장되고, 학습된 정보를 보존하면 동일한 위치를 두 번 조사할 필요가 없다. 도 2는 주어진 반복에 대한 샘플 또는 미니 배치를 업데이트하기 위한 Q-러닝의 일부를 보여준다. 학습 과정에서 업데이트된 Q-값은 대상 Q-learning 값이 가장 높은 액션을 선택하는 에이전트의 액션을 결정한 다. 업데이트는 하기와 같이 정의된 Bellman 방정식(이하, 수학식 1)을 사용하여 수행될 수 있다. [수학식 1]"}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "위 [수학식 1]에서, 는 업데이트된 Q-network 반복 값의 결과이고, 초기 이전 값이 추가될 는 훈련 네트워크의 학습 속도이고, 는 보상 값이며, 는 보상을 시간 영역과 연관시키며, 는 미래 추정치이다. 갱신된 반복 값은 이전 값을 더하고 계산 시간의 차이를 나타내는 로 학습 속도를 곱하여 공식화할 수 있다. 이 수학식의 최대값은 에이전트가 VCE 자체의 위쪽 호를 취하도록 도와주는 최대화된 작업을 나타낸다. 드론 에이전트가 어떤 조치를 취했다고 가정해보자. 이러한 조치 때문에 환경은 드론 에이전트를 상 태 중 하나에 착륙시킬 수 있으며, 이러한 상태로부터 조치를 최대화할 수 있다. 이 값들 중에서 드론 에 이전트는 최대 값( )을 갖는 동작을 선택한다. 의도된 새로운 값은 할인 계수 에 최대 값 을 곱하고 보상 값 을 더하여 계산할 수 있다. 본 발명의 DQN 모델에서, DQN 에이전트는 동적 모드에서 환경을 추적하는 재생 메모리 클래스 단위를 포함할 수 있다. 모든 상태( ), 동작( ), 새 상태( ), 보상( ) 및 완료된 전환이 기억될 수 있다. 이 재생 메모리 접 근 방식을 사용하면 보존된 값에서 미니 배치를 효과적으로 샘플링하고 정확한 상태 표현을 생성할 수 있다. 저 장된 값을 최대한 활용하고 동적으로 추적하기 위해 응답 버퍼 메모리가 VCE 시뮬레이션 플랫폼에 통합될 수 있 다. 도 3은 본 발명의 실시예에 따라 응답 버퍼 메모리 유닛이 적용된 경우의 DQN 모델에 있어서의 데이터 흐름을 나타낸다. 실시예에 따르면, 응답 버퍼 메모리 유닛은 가상 환경에 직접 연결되며, 추적기 내에 구비되거나, 추 적기 외부에 구비될 수도 있다. 응답 버퍼 메모리는 가상 환경에 직접 연결되어 필요한 전환을 메모리 장치에 추가할 수 있다. 샘플링 프로세스 동안 다양한 크기의 랜덤한 수의 맵 인덱스가 메모리에 생성되고 반환된 인덱스는 AirSim 파이썬 클라이언트의 \"get state\" 함수를 사용하여 검색될 수 있다. DQN 모델은 AirSim Python 클라이언트를 통해 가상 시뮬레이션 플랫폼에 연결되며, 에이전트 평가에 활용될 N 프레임의 추적을 유지한다. 또한, DQN 모델은 예측된 Q-값과 목표 Q-값을 결합하여 DQN 손실 함수를 계산할 수 있으며, 그레이디언트 손실도 출력도 있다. DQN 모델은 반복 시 Q-learning을 업데이트하기 위해 하기의 [수학 식 2]와 같이 정의되는 손실 함수를 사용할 수 있다.[수학식 2]"}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 는 에이전트 지평선의 할인 계수 값을 나타내고, 는 반복 i에서 Q-네트워크의 매개 변수이며, 는 반복 i에서 대상을 계산하는 데 사용되는 네트워크 매개 변수이다. 대상 네트워크 매개 변수 는 정의된 각 단 계에서 Q-네트워크 매개 변수( )로만 업데이트되며 개별 업데이트 간에 일정하게 유지된다. 이전 상태가 N개인 기본 버퍼는 응답 버퍼 메모리 유닛의 첫 번째 축을 따라 적층되어 상태 보존에 추가된다. 또한 재설정 기능을 사용하여 전체 메모리 유닛을 기본 버퍼로 재설정하고 모든 인덱스를 0으로 설정한다. 이와 같이, 본 발명의 추적기에 탑재되는 DQN 모델은 반복 신경망 모델을 활용하여 학습하며, 반복 계층은 가상 시뮬레이션 환경에 대한 특징과 정보를 학습하는 데에 사용된다. 순차적 환경 학습 프로세스에서 반복 계 층은 정책에서 일반화된 데이터 생성으로 상태 및 작업 값의 정확한 예측을 제공할 수 있다. 본 발명의 DQN 모 델에서, 최종 행동 가치의 최종 결과를 결정하기 위해 추가적인 접근법이 사용될 수 있다. 도 4는 본 발명의 실시예에 따른 RNN 기반의 DQN 모델에 있어서 추적 과정을 개략적으로 나타낸 도면이다. 실시예에 따르면, 먼저 DQN 모델은 가상 시뮬레이션 환경과 상호 작용하기 위해 동작 값 모델을 사용하는 초기 설정으로 매개 변수를 구성할 것이다. 실시예에 따른 DQN 모델은 활성화-릴루 기능으로 벡터의 치수를 64에서 32로 변경하기 위해 심층적으로 연결된 밀도 레이어가 있는 LSTM 레이어가 적용되어 구축될 수 있다. DQN 모델은 상태, Q-값 및 출력 값을 작업 값으로 제공할 수 있으며, 여러 Q-value 작업을 통해 환경의 현재 상 태와 관련하여 수행할 다음 작업을 선택할 수 있다. DQN 모델은 관찰 유닛을 통해 이전 상태의 동작 기능을 통해 Q-값의 수를 관찰할 수 있으며, 이 작업이 완료되"}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "면, 단기 메모리에 재설정되며, 학습 절차의 1회 탐색 에피소드 요약에 대한 요약 결과를 제공하고 장기 메모리 에 추가될 수 있다."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "DQN 모델의 훈련에 대한 출력 요약에는 전체 훈련 에피소드의 총 보상, 평균 MaxQ, 기간, 평균 손실 및 시간 단 계가 포함될 수 있다. Q-값은 서로 다른 궤도의 물체를 추적한다는 측면에서 서로 다른 유형의 움직임을 강조한 다. DQN 모델에서 주어진 기호는 드론 에이전트가 객체 위치 및 이동과 상호작용하는 방향을 나타낸다. DQN 모델은 환경 역학을 더 잘 이해하고 다음 상태(t+1)에 대한 예상 보상을 계산하도록 스스로 훈련할 수 있다. 또한, DQN 모델은 첫번째 훈련 결과에 따라 t단계에서 예상되는 보상을 업데이트할 수 있다. 목표 기대치는 행동의 목표 Q-값을 통해 계산될 수 있으며, 이에 따라 훈련 안정성을 높일 수 있다. 훈련 과정 후, DQN 네트워크는 모든 양의 보상을 1에서 클리핑하고, 모든 음의 보상을 -1에서 클리핑하고, 0의 보상을 변경하지 않고, 다음 에피소드를 위해 다시 훈련하고, 최종적으로 네트워크 출력 파일을 고정 경로에 저 장한다. DQN 모델의 훈련 전에 심층 Q-network에 대한 하이퍼 매개 변수와 해당 매개 변수의 기본값이 설정될 수 있다."}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "최종적으로, 총 보상, 평균 MaxQ 지속 시간 및 평균 손실의 모든 학습된 값이 요약되고 조정된 경로 위치에 저 장될 수 있다. 본 발명에서, DQN 모델의 관찰 유닛은 가상 환경 시퀀스 및 반복적인 네트워크 기반 아키텍처 계층을 나타내며, 각 프레임에서 예측된 경계 상자 위치로 출력을 제공할 수 있다. 실시예에 따른 DQN 모델은 학습 과정의 출력 예측을 제공하며, 경계 상자의 결합에 대한 교차점(IoU)을 계산하 여 추적 단위를 통합할 수 있다. 본 발명에서, 보상 함수는 경계 상자의 중심과 프레임의 중심 사이의 유클리드 거리, 경계 상자의 결합에 대한 교차점 및 매개 변수 임계값 높이와 가중치를 중심으로 한 가상 상자의 척도화된 합으로 계산될 수 있다. 실시예에 따른 추적기는 실제 환경 시뮬레이션에서 자유롭게 탐색하는 데 매우 유용하고 유리한 것으로 잘 알려진 시뮬레이션 플랫폼인 AirSim을 사용하여 탐색 및 평가되었다. 그리고, 추적기는 마이크로소프트 개 발자가 수행한 AirSim Python Client를 사용하여 VCE 시뮬레이션 플랫폼에 연결되었다. 추적기의 학습은 오브젝트 클래스의 특성과 세부 정보를 파악하기 위해 오프라인 모드로 진행되었고, 사실 적인 가상 모델 사진만 학습에 사용되었다. 도 5는 본 발명의 실시예에 따른 DQN 추적기의 학습 속도 및 손실 함수의 출력을 나타낸 그래프를 도시하고 있 다. 도 5에서 (a)는 학습 속도에 대한 그래프이고, (b)는 손실 함수에 대한 그래프이다. 그리고, 도 5에서 파란색 선은 실제 학습 값을 나타내고, 빨간색 선은 목표 학습 값을 나타낸다. 도 5에서 확인할 수 있는 바와 같이, 본 발명의 실시예에 따른 추적기는 학습 속도와 손실 기능에서 목표 를 달성한다는 것을 알 수 있다. 도 6은 본 발명의 실시예에 따른 DQN 추적기의 학습 과정 동안의 보류 상태에 대해 계산된 평균 예측 작업 값 평균(MeanQ) 및 최대 Q 값(MaxQ)에 대한 그래프를 도시하고 있다. 도 6에서 (a)는 평균 예측 작업 값 평균(MeanQ)에 대한 그래프이고, (b)는 최대 Q 값(MaxQ)에 대한 그래프이다. 그리고, 도 6에서 빨간색 선은 목표 값을 나타내고, 파란색 선은 DQN 값 손실을 나타낸다. 하기 표 1은 가상 도시 환경에서 드론 에이전트를 훈련하는 동안의 매개 변수를 보여준다. 훈련에서 무작위로 선택된 모든 에피소드에서 위치 매개 변수는 가상 환경에서 드론 에이전트의 추적 경로를 보 여준다. 리워드 XY, Z 및 +T 매개 변수는 순차적 동작으로 분석하면서 드론 에이전트의 환경 조건 적응을 강조 한다. [표 1]"}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "하기 표 2는 DQN 추적기의 테스트 과정 중 조치 Q-값을 보여준다. [표 2]"}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "시간 단계, 지속 단계, 총 보상, 평균 최대 Q-값 및 검사 절차의 평균 손실 결과와 같이 무작위로 선택한 증상"}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "의 요약 결과가 표 2에 표시된다. 드론은 임의의 지역을 이동하며 테스트 단계에서 추적할 항목 사이트를 식별 하는데, 이는 드론이 임의의 위치로 비행하고 추적할 대상 위치를 식별하는 것을 의미한다. 발명자들은 본 발명의 실시예에 따른 추적기를 VisDrone2019 및 OTB-100에서 제안된 데이터 셋을 기반으로 ADNet 및 ASRL Track과 성능을 비교하였다. [표 3]"}
{"patent_id": "10-2022-0132468", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "표 3에서 확인할 수 있는 바와 같이, 정확도, 초당 프레임 수(FPS), 및 결합에 대한 교차 결과(IoU) 각각의 항 목에 있어서 본 발명의 실시예에 따른 추적기의 성능이 우수하다는 것을 알 수 있다. 본 발명에서는, 가상 현실 환경에서 작동하는 AirSim Python Client를 드론 에이전트로 사용하여 가상 시뮬레이 션 플랫폼 VCE를 통합하는 새로운 추적 기법이 제안된다. 본 발명에서는, 객체를 예측하고 추적하고 환경을 학습하기 위해 여러 가상 이미지 기반 비디오 시퀀스로 훈련 된 상호 통합된 반복 신경망 기반 DQN 추적기가 제안된다. AirSim 시뮬레이션 플랫폼은 가상 환경에서 모델을 테스트하고, 중요한 기능 정보를 수집하고, 객체 클래스를 자동으로 식별할 수 있도록 하며, AirSim API는 DQN 에이전트 기반 추적기를 가상 드론 시뮬레이션 플랫폼에 직 접 연결하여 쉽게 테스트할 수 있도록 도와준다. 3차원 가상 현실 모델 환경 측면에서 몇 가지 과제가 있지만, 본 발명에서 제안된 반복 신경망 기반 DQN 모델은 행동 결정 기법과 통합된 반복적인 예측 기반 네트워크를 통해 성공적으로 훈련되고 더 나은 성능을 달성하는 것이 확인되었다. 도 7은 본 발명의 실시예에 따른 DQN 기반 추적기의 일례의 구성을 나타낸 도면이다. 본 발명의 실시예에 따른 DQN 기반 추적기는 가상 현실 환경에서 작동하는 AirSim Python Client를 드론 에이전트로 사용하여 가상 현실 시뮬레이션 플랫폼(여기서는, AirSim 시뮬레이션 플랫폼)에 연결되고, 순차적 비디오 프레임을 입력으로 학습할 수 있다. 실시예에 따른 추적기에는 반복 신경망 모델을 활용하여 학습하는 DQN 모델이 탑재될 수 있으며, 반복 신 경망 모델의 반복 계층은 가상 시뮬레이션 환경에 대한 특징과 정보를 학습할 수 있다. 실시예에 따르면, 가상 환경에 직접 연결되는 응답 버퍼 메모리 유닛이 구비되며, 예를 들어, 응답 버퍼 메모리 유닛은 추적기 내에 구비될 수 있으며, 응답 버퍼 메모리 유닛의 위치가 이에 한정되는 것은 아니다. 추적기는 응답 버퍼 메모리를 이용하여 재생 메모리 접근 방식으로 동작하며, 이에 따라, 응답 버퍼 메모리에 보존된 값에서 미니 배치를 효과적으로 샘플링하고 정확한 상태 표현을 생성할 수 있다. 추적기 내에 탑재되는 신경망 모델에는 입력되는 순차적 비디오 프레임을 이용한 학습이 가능하도록 LSTM(Long Short-Term Memory) 레이어가 적용된다. 실시예에 따르면, 추적기는 메모리, 프로세서, 및 입출력 인터페이스를 포함할 수 있으며, 추적기의 구성이 이에 한정되는 것은 아니다. 실시예에 따른 메모리는 본 발명의 응답 버퍼 메모리 유닛을 구성할 수 있으며, 본 실시예에서는 추 적기 내에 구비되는 메모리가 응답 버퍼 메모리 유닛을 구성하는 것을 예시하나, 이에 한정되는 것은 아니며, 추적기 외부의 별도 메모리가 응답 버퍼 메모리 유닛으로 이용될 수 있다. 실시예에 따른 프로세서는 본 발명의 실시예에 따라 훈련되는 신경망 모델이 탑재되고, 훈련된 신경망 모 델의 동작을 구현할 수 있다. 입출력 인터페이스는 입출력 장치와의 인터페이스를 위한 수단일 수 있다. 예를 들어, 입력 장치는 키보드 또는 마우스 등의 장치를, 그리고 출력 장치는 프로세서의 동작 수행 결과를 표시하기 위한 디스플레이와 같은 장치를 포함할 수 있다. 이상 첨부된 도면을 참조하여 본 발명의 실시예들을 더욱 상세하게 설명하였으나, 본 발명이 반드시 이러한 실 시예로 국한되는 것은 아니고, 본 발명의 기술사상을 벗어나지 않는 범위 내에서 다양하게 변형 실시될 수 있다. 따라서, 본 명세서에 개시된 실시예들은 본 발명의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 발명의 기술 사상의 범위가 한정되는 것은 아니다. 그러므로, 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 본 발명의 보호 범위 는 청구 범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 발명의 권리 범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2022-0132468", "section": "도면", "subsection": "도면설명", "item": 1, "content": "이하에 첨부되는 도면들은 본 발명의 실시예에 관한 이해를 돕기 위한 것으로, 상세한 설명과 함께 실시예들을 제공한다. 다만, 본 실시예의 기술적 특징이 특정 도면에 한정되는 것은 아니며, 각 도면에서 개시하는 특징들 은 서로 조합되어 새로운 실시예로 구성될 수 있다. 도 1은 본 발명의 실시예에 따른 DQN 기반 추적기가 가상 도시 환경 시뮬레이션 플랫폼과 연동되어 구축된 상태 의 프레임워크를 도시하고 있다. 도 2는 본 발명의 실시예에 따른 DQN 모델이 학습하는 절차를 도시하고 있다. 도 3은 본 발명의 실시예에 따라 응답 버퍼 메모리 유닛이 적용된 경우의 DQN 모델에 있어서의 데이터 흐름을 나타낸다. 도 4는 본 발명의 실시예에 따른 RNN 기반의 DQN 모델에 있어서 추적 과정을 개략적으로 나타낸 도면이다. 도 5는 본 발명의 실시예에 따른 DQN 추적기의 학습 속도 및 손실 함수의 출력을 나타낸 그래프를 도시하고 있 다. 도 6은 본 발명의 실시예에 따른 DQN 추적기의 학습 과정 동안의 보류 상태에 대해 계산된 평균 예측 작업 값 평균(MeanQ) 및 최대 Q 값(MaxQ)에 대한 그래프를 도시하고 있다. 도 7은 본 발명의 실시예에 따른 DQN 기반 추적기의 일례의 구성을 나타낸 도면이다."}
