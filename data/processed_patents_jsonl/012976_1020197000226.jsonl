{"patent_id": "10-2019-7000226", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0016539", "출원번호": "10-2019-7000226", "발명의 명칭": "신경망 및 신경망 트레이닝 방법", "출원인": "프로그레스, 인코포레이티드", "발명자": "페치안시, 드미트리"}}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "트레이닝 이미지들을 수신하도록 구성되되, 트레이닝 이미지들은 신경망의 트레이닝동안 트레이닝 입력 값 어레이로서 수신되고 그리고 트레이닝 입력 값 어레이로서 코딩된 것의 하나인, 신경망에 대한 다수의 입력들;각각의 시냅스가 다수의 입력들 중 하나에 연결되고 그리고 다수의 보정 가중치들을 포함하며, 그리고 각각의보정 가중치가 가중치 값에 의해 정의되고, 그리고 다수의 시냅스들의 보정 가중치들이 보정 가중치 어레이로조직화되는, 다수의 시냅스들;각각의 뉴런이 적어도 하나의 출력을 갖고 그리고 다수의 시냅스들 중 적어도 하나를 통해 다수의 입력들 중 적어도 하나와 연결되며, 그리고 각각의 뉴런이 각각의 뉴런에 연결된 대응하는 각각의 시냅스의 보정 가중치들의가중치 값들을 합산하도록 구성됨으로써, 다수의 뉴런들이 뉴런 합 어레이를 생성하도록 하는, 다수의 뉴런들;및 원하는 출력 값 어레이로 조직화된 원하는 이미지들을 수신하고;원하는 출력 값 어레이로부터 뉴런 합 어레이의 편차를 결정하며 그리고 편차 어레이를 생성하고; 그리고결정된 편차 어레이를 이용하여 보정 가중치 어레이를 변경함으로써, 뉴런 합 어레이를 결정하기 위해 변경된보정 가중치 값들을 합산하면 트레이닝된 보정 가중치 어레이를 생성하도록 원하는 출력 값으로부터 뉴런 합 어레이의 편차를 감소시켜 신경망의 동시 트레이닝을 용이하게 하는, 컨트롤러를 포함하는, 신경망."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,트레이닝된 신경망에서:신경망에 대한 다수의 입력들은 입력 이미지들을 수신하도록 구성되되, 입력 이미지들은 신경망에 의해 이미지들의 인식동안 입력 값 어레이로서 수신되고 그리고 입력 값 어레이로서 코딩되는 것중 하나이고;각각의 시냅스는 트레이닝된 보정 가중치 어레이 중 다수의 트레이닝된 보정 가중치들을 포함하며; 그리고각각의 뉴런은 각각의 뉴런에 연결된 각각의 시냅스에 대응하는 트레이닝된 보정 가중치들 중 가중치 값들을 가산하도록 구성됨으로써, 다수의 뉴런들이 인식된 이미지들 어레이를 생성하여, 입력 이미지들의 인식을 제공하는, 신경망."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,분배기들의 세트를 더 포함하되, 분배기들의 세트는 각각의 트레이닝 이미지들 그리고 각각의 트레이닝 입력 값어레이와 입력 값 어레이로서 입력 이미지들을 코딩하도록 구성되고, 분배기들의 세트는 각각의 트레이닝 이미지들 및 입력 이미지들을 수신하기 위해 다수의 입력들에 동작 가능하게 연결되는, 신경망."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,컨트롤러는 원하는 출력 값 어레이로부터 뉴런 합 어레이의 타겟 편차의 어레이로 부가적으로 프로그래밍되고,그리고 컨트롤러는 원하는 출력 값 어레이로부터 뉴런 합 어레이의 편차가 타겟 편차 어레이의 수용 가능한 범위 내에 있을 때, 신경망의 트레이닝을 완료하도록 추가로 구성되는, 신경망."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서,공개특허 10-2019-0016539-3-트레이닝 입력 값 어레이, 입력 값 어레이, 보정 가중치 어레이, 뉴런 합 어레이, 원하는 출력 값 어레이, 편차어레이, 트레이닝된 보정 가중치 어레이, 인식된 이미지 어레이, 및 타겟 편차 어레이는, 각각, 트레이닝 입력값 매트릭스, 입력 값 매트릭스, 보정 가중치 매트릭스, 뉴런 합 매트릭스, 원하는 출력 값 매트릭스, 편차 매트릭스, 트레이닝된 보정 가중치 매트릭스, 인식된 이미지 매트릭스 및 타겟 편차 매트릭스로 조직화되는, 신경망."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,다수의 데이터 프로세서들을 더 포함하되, 컨트롤러는 각각의 입력 값, 트레이닝 입력 값, 보정 가중치, 뉴런합 및 원하는 출력 값 매트릭스들 중 적어도 하나를 각각의 서브-매트릭스들로 분할하고 그리고 서브-매트릭스로 별도의 병렬 수학 연산을 위해 다수의 결과적인 서브-매트릭스들이 다수의 데이터 프로세서들과 통신하도록추가로 구성되어, 동시 데이터 프로세싱을 용이하게 하고 입력 값 매트릭스의 이미지 인식 및 신경망의 트레이닝 중 하나의 속도를 향상시키는, 신경망."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서,컨트롤러는 트레이닝 입력 값 매트릭스 및 보정 가중치 매트릭스에 대수 매트릭스 연산을 적용함으로써 보정 가중치 매트릭스를 변경하여, 신경망을 트레이닝하는, 신경망."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,수학적 매트릭스 연산은 트레이닝 입력 값과 보정 가중치 매트릭스들의 수학적 곱의 결정을 포함하여, 현재의트레이닝 시기 가중치 매트릭스를 형성하는, 신경망."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,컨트롤러는 뉴런 합들의 편차 매트릭스를 생성하기 위해 원하는 출력 값 매트릭스로부터 뉴런 합 매트릭스를 감산하고; 그리고뉴런 입력 당 편차 매트릭스를 생성하기 위해 뉴런 합들의 편차 매트릭스를 각각의 뉴런에 연결된 입력들의 수로 나누도록, 추가적으로 구성된, 신경망."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서,컨트롤러는 각각의 보정 가중치가 신경망에서 하나의 트레이닝 시기동안 사용된 횟수를 결정하고;각각의 보정 가중치가 하나의 트레이닝 시기동안 사용된 결정된 횟수를 사용하여 하나의 트레이닝 시기에 대한평균 편차 매트릭스를 형성하며; 그리고하나의 트레이닝 시기에 대한 평균 편차 매트릭스를 보정 가중치 매트릭스에 가산하도록 추가적으로 구성되어,트레이닝된 보정 가중치 매트릭스를 생성하고 하나의 트레이닝 시기를 완료하는, 신경망."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "신경망에 대한 다수의 입력들을 통해 트레이닝 이미지들을 수신하되, 트레이닝 이미지들은 신경망의 트레이닝동안 트레이닝 입력 값 어레이로서 수신되고 그리고 트레이닝 입력 값 어레이로서 코딩된 것의 하나인, 단계;보정 가중치 어레이에서 다수의 시냅스들의 보정 가중치들을 조직화하되, 각각의 시냅스가 다수의 입력들 중 하나에 연결되고 그리고 다수의 보정 가중치들을 포함하며, 그리고 각각의 보정 가중치가 가중치 값에 의해 정의되는, 단계;다수의 뉴런들을 통해 뉴런 합 어레이를 생성하되, 각각의 뉴런이 적어도 하나의 출력을 갖고 그리고 다수의 시공개특허 10-2019-0016539-4-냅스들 중 적어도 하나를 통해 다수의 입력들 중 적어도 하나와 연결되며, 그리고 각각의 뉴런이 각각의 뉴런에연결된 대응하는 각각의 시냅스의 보정 가중치들의 가중치 값을 합산하도록 구성된, 단계;컨트롤러를 통해, 원하는 출력 값 어레이로 조직화된 원하는 이미지들을 수신하는 단계;컨트롤러를 통해, 원하는 출력 값 어레이로부터 뉴런 합 어레이의 편차를 결정하고 그리고 편차 어레이를 생성하는 단계; 및컨트롤러를 통해, 결정된 편차 어레이를 이용하여 보정 가중치 어레이를 변경함으로써, 뉴런 합 어레이를 결정하기 위해 변경된 보정 가중치 값들을 합산하면 트레이닝된 보정 가중치 어레이를 생성하도록 원하는 출력 값으로부터 뉴런 합 어레이의 편차를 감소시켜 신경망의 동시 트레이닝을 용이하게 하는, 단계를 포함하는, 신경망을 동작시키는 방법."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,트레이닝된 신경망에서:신경망에 대한 다수의 입력들을 통해 입력 이미지들을 수신하되, 입력 이미지들은 신경망에 의해 이미지들의 인식동안 입력 값 어레이로서 수신되고 그리고 입력 값 어레이로서 코딩되는 것중 하나이고;각각의 시냅스에 트레이닝된 보정 가중치 어레이 중 다수의 트레이닝된 보정 가중치들을 부여하되, 각각의 트레이닝된 보정 가중치는 보정치 값으로 정의되며; 및각각의 뉴런에 연결된 각각의 시냅스에 대응하는 트레이닝된 보정 가중치들의 가중치 값들을 가산함으로써, 다수의 뉴런들이 인식된 이미지들 어레이를 생성하여, 입력 이미지들의 인식을 제공하는, 신경망을 동작시키는방법."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,분배기들의 세트를 통하여, 각각의 트레이닝 입력 값 어레이 및 입력 값 어레이로서 트레이닝 이미지들 및 입력 이미지들의 각각을 코딩하는 단계를 더 포함하되, 분배기들의 세트는 각각의 트레이닝 이미지들 및 입력 이미지들을 수신하기 위해 다수의 입력들에 동작 가능하게 연결되는, 신경망을 동작시키는 방법."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,컨트롤러는 원하는 출력 값 어레이로부터 뉴런 합 어레이의 타겟 편차의 어레이로 부가적으로 프로그래밍되고,그리고 방법은, 컨트롤러를 통하여, 원하는 출력 값 어레이로부터 뉴런 합 어레이의 편차가 타겟 편차 어레이의수용 가능한 범위 내에 있을 때, 신경망의 트레이닝을 완료하는 단계를 더 포함하는, 신경망을 동작시키는방법."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12항에 있어서,트레이닝 입력 값 어레이, 입력 값 어레이, 보정 가중치 어레이, 뉴런 합 어레이, 원하는 출력 값 어레이, 편차어레이, 트레이닝된 보정 가중치 어레이, 인식된 이미지 어레이, 및 타겟 편차 어레이는, 각각, 트레이닝 입력값 매트릭스, 입력 값 매트릭스, 보정 가중치 매트릭스, 뉴런 합 매트릭스, 원하는 출력 값 매트릭스, 편차 매트릭스, 트레이닝된 보정 가중치 매트릭스, 인식된 이미지 매트릭스 및 타겟 편차 매트릭스로 조직화하는 단계를 더 포함하는, 신경망을 동작시키는 방법."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,공개특허 10-2019-0016539-5-신경망은 부가적으로 다수의 데이터 프로세서들을 더 포함하고, 방법은, 컨트롤러를 통하여, 각각의 입력 값,트레이닝 입력 값, 보정 가중치, 뉴런 합 및 원하는 출력 값 매트릭스들 중 적어도 하나를 각각의 서브-매트릭스들로 분할하는 단계, 그리고 서브-매트릭스로 별도의 병렬 수학 연산을 위해 다수의 결과적인 서브-매트릭스들이 다수의 데이터 프로세서들과 통신하는 단계를 더 포함하여, 동시 데이터 프로세싱을 용이하게 하고 입력값 매트릭스의 이미지 인식 및 신경망의 트레이닝 중 하나의 속도를 향상시키는, 신경망을 동작시키는 방법."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제15항에 있어서,컨트롤러에 의해, 트레이닝 입력 값 매트릭스 및 보정 가중치 매트릭스에 대수 매트릭스 연산을 적용함으로써보정 가중치 매트릭스를 변경하는 단계를 더 포함하여, 신경망을 트레이닝하는, 신경망을 동작시키는 방법."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,수학적 매트릭스 연산을 적용하는 것은 트레이닝 입력 값과 보정 가중치 매트릭스들의 수학적 곱을 결정함을 포함하여, 현재의 트레이닝 시기 가중치 매트릭스를 형성하는, 신경망을 동작시키는 방법."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,컨트롤러를 통하여, 뉴런 합들의 편차 매트릭스를 생성하기 위해 원하는 출력 값 매트릭스로부터 뉴런 합 매트릭스를 감산하는 단계; 그리고컨트롤러를 통하여, 뉴런 입력 당 편차 매트릭스를 생성하기 위해 뉴런 합들의 편차 매트릭스를 각각의 뉴런에연결된 입력들의 수로 나누는 단계를 더 포함하는, 신경망을 동작시키는 방법."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서,컨트롤러를 통하여, 각각의 보정 가중치가 신경망에서 하나의 트레이닝 시기동안 사용된 횟수를 결정하는 단계;컨트롤러를 통하여, 각각의 보정 가중치가 하나의 트레이닝 시기동안 사용된 결정된 횟수를 사용하여 하나의 트레이닝 시기에 대한 평균 편차 매트릭스를 형성하는 단계; 및컨트롤러를 통하여, 하나의 트레이닝 시기에 대한 평균 편차 매트릭스를 보정 가중치 매트릭스에 가산하는 단계를 더 포함하여, 트레이닝된 보정 가중치 매트릭스를 생성하고 하나의 트레이닝 시기를 완료하는, 신경망을 동작시키는 방법."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "인공 신경망을 동작시키는 비 일시적 컴퓨터 판독 가능 저장 장치로서, 저장 장치가 실행 가능한 명령들로 인코딩되어, 신경망에 대한 다수의 입력들을 통해 트레이닝 이미지들을 수신하되, 트레이닝 이미지들은 신경망의 트레이닝동안 트레이닝 입력 값 어레이로서 수신되고 그리고 트레이닝 입력 값 어레이로서 코딩된 것의 하나이고;보정 가중치 어레이에서 다수의 시냅스들의 보정 가중치들을 조직화하되, 각각의 시냅스가 다수의 입력들 중 하나에 연결되고 그리고 다수의 보정 가중치들을 포함하며, 그리고 각각의 보정 가중치가 가중치 값에 의해 정의되고;다수의 뉴런들을 통해 뉴런 합 어레이를 생성하되, 각각의 뉴런이 적어도 하나의 출력을 갖고 그리고 다수의 시냅스들 중 적어도 하나를 통해 다수의 입력들 중 적어도 하나와 연결되며, 그리고 각각의 뉴런이 각각의 뉴런에연결된 대응하는 각각의 시냅스의 보정 가중치들의 가중치 값을 합산하도록 구성되고;원하는 출력 값 어레이로 조직화된 원하는 이미지들을 수신하며;원하는 출력 값 어레이로부터 뉴런 합 어레이의 편차를 결정하고 그리고 편차 어레이를 생성하며;공개특허 10-2019-0016539-6-결정된 편차 어레이를 이용하여 보정 가중치 어레이를 변경함으로써, 뉴런 합 어레이를 결정하기 위해 변경된보정 가중치 값들을 합산하면 트레이닝된 보정 가중치 어레이를 생성하도록 원하는 출력 값으로부터 뉴런 합 어레이의 편차를 감소시켜 신경망의 동시 트레이닝을 용이하게 하는, 인공 신경망을 동작시키는 비 일시적 컴퓨터판독 가능 저장 장치."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제21항에 있어서,저장 장치가 실행 가능한 명령들로 더 인코딩되어, 신경망에 대한 다수의 입력들을 통해 입력 이미지들을 수신하되, 입력 이미지들은 신경망에 의해 이미지들의 인식동안 입력 값 어레이로서 수신되고 그리고 입력 값 어레이로서 코딩되는 것중 하나이고;각각의 시냅스에 트레이닝된 보정 가중치 어레이 중 다수의 트레이닝된 보정 가중치들을 부여하되, 각각의 트레이닝된 보정 가중치는 보정치 값으로 정의되며; 그리고각각의 뉴런에 연결된 각각의 시냅스에 대응하는 트레이닝된 보정 가중치들의 가중치 값들을 가산함으로써, 다수의 뉴런들이 인식된 이미지들 어레이를 생성하여, 입력 이미지들의 인식을 제공하는, 인공 신경망을 동작시키는 비 일시적 컴퓨터 판독 가능 저장 장치."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "신경망에 대한 다수의 입력들을 통해 트레이닝 이미지들을 수신하되, 트레이닝 이미지들은 신경망의 트레이닝동안 트레이닝 입력 값 어레이로서 수신되고 그리고 트레이닝 입력 값 어레이로서 코딩된 것의 하나인, 수단;보정 가중치 어레이에서 다수의 시냅스들의 보정 가중치들을 조직화하되, 각각의 시냅스가 다수의 입력들 중 하나에 연결되고 그리고 다수의 보정 가중치들을 포함하며, 그리고 각각의 보정 가중치가 가중치 값에 의해 정의되는, 수단;다수의 뉴런들을 통해 뉴런 합 어레이를 생성하되, 각각의 뉴런이 적어도 하나의 출력을 갖고 그리고 다수의 시냅스들 중 적어도 하나를 통해 다수의 입력들 중 적어도 하나와 연결되며, 그리고 각각의 뉴런이 각각의 뉴런에연결된 대응하는 각각의 시냅스의 보정 가중치들의 가중치 값을 합산하도록 구성된, 수단;원하는 출력 값 어레이로 조직화된 원하는 이미지들을 수신하는 수단;원하는 출력 값 어레이로부터 뉴런 합 어레이의 편차를 결정하고 그리고 편차 어레이를 생성하는 수단; 및결정된 편차 어레이를 이용하여 보정 가중치 어레이를 변경함으로써, 뉴런 합 어레이를 결정하기 위해 변경된보정 가중치 값들을 합산하면 트레이닝된 보정 가중치 어레이를 생성하도록 원하는 출력 값으로부터 뉴런 합 어레이의 편차를 감소시켜 신경망의 동시 트레이닝을 용이하게 하는, 수단을 포함하는, 인공 신경망을 동작시키기위한 장치."}
{"patent_id": "10-2019-7000226", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제23항에 있어서,트레이닝된 신경망에서:신경망에 대한 다수의 입력들을 통해 입력 이미지들을 수신하되, 입력 이미지들은 신경망에 의해 이미지들의 인식동안 입력 값 어레이로서 수신되고 그리고 입력 값 어레이로서 코딩되는 것중 하나인, 수단;각각의 시냅스에 트레이닝된 보정 가중치 어레이 중 다수의 트레이닝된 보정 가중치들을 부여하되, 각각의 트레이닝된 보정 가중치는 보정치 값으로 정의되는, 수단; 및각각의 뉴런에 연결된 각각의 시냅스에 대응하는 트레이닝된 보정 가중치들의 가중치 값들을 가산함으로써, 다수의 뉴런들이 인식된 이미지들 어레이를 생성하여, 입력 이미지들의 인식을 제공하는, 수단인, 인공 신경망을동작시키기 위한 장치."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "신경망은 입력 신호들을 수신하기 위한 입력들, 및 입력들에 연결되고 어레이로 조직화된 보정 가중치들을 갖는 시냅스들을 포함한다. 트레이닝 이미지들은 망의 트레이닝 과정동안 입력들에 의해 어레이로 수신되거나 코딩된 다. 망은 또한 각각 하나의 시냅스를 통해 적어도 하나의 입력과 연결된 출력을 가지며 각각의 뉴런에 연결된 각 시냅스에서 선택된 보정 가중치를 합산하여 뉴런 합계 어레이를 생성하는 뉴런을 포함한다. 또한, 망은 어레이에 서 원하는 이미지들을 수신하고, 원하는 출력 값 어레이로부터 뉴런 합 어레이 편차를 결정하며, 편차 어레이를 생성하는 컨트롤러를 포함한다. 컨트롤러는 편차 어레이를 사용하여 보정 가중치 어레이를 변경한다. 뉴런 합 어 레이를 결정하기 위해 변경된 보정 가중치를 더하면 대상 편차가 줄어들고 동시 망 트레인닝을 위한 트레이닝된 보정 가중치 어레이가 생성된다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 2017년 3월 3일자로 출원된 미국 실용신안 출원 제15/449614호 및 2016년6월 9일자로 출원된 미국 실 용신안 출원 제15/178,137호의 이익을 주장하며, 이들 각각은 2015년 9월 23일에 출원된 미국 우회 실용신안출 원 제14/ 862,337호의 일부 계속 출원이고, 이는 2015년 3월 6일에 출원된 국제출원번호 PCT/US2015/19236로서, 이는 2014년 3월 6일에 출원된 미국가출원 제 61/949,210호와 2015년 1월 22일에 출원 된 미국 가출원 제62/106,389호의 이익을 주장하고, 또한 2015년 6월 9일자로 출원된 미국 가출원 제62/173,163 호의 이익을 주장하며, 그 전체 내용은 참고 문헌으로 유사하게 포함된다. 본 개시는 인공 신경망 및 트레이닝 방법에 관한 것이다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "기계 학습에서, \"신경망\"이라는 용어는 일반적으로 소프트웨어 및/또는 컴퓨터 아키텍처, 즉 그것을 실행하는데 필요한 하드웨어 및 소프트웨어를 포함하는 컴퓨터 시스템 또는 마이크로프로세서의 전체 디자인 또는 구조를 지칭한다. 인공 신경망들은 동물의 중추 신경계라고 알려진, 특히 뇌의 생물학적 신경 네트워크에서 영감을 얻 은 통계적 학습 알고리즘 계열이다. 인공 신경망들은 주로 많은 입력에 의존할 수 있는 일반적으로 알려지지 않 은 기능을 추정하거나 근사하는데 사용된다. 이러한 신경망들은 컴퓨터 비전 및 음성 인식을 포함하여 일반적인 규칙 기반 프로그래밍을 사용하여 해결하기 어려운 다양한 작업에 사용되었다. 인공 신경망들은 일반적으로 입력들로부터 값들을 계산할 수 있는 \"뉴런들\"의 시스템들로서 표현되고, 그 적응 적 특성의 결과로서 패턴 인식뿐만 아니라 기계 학습을 할 수 있다. 각 뉴런은 종종 시냅스 가중치들(synaptic weights)을 갖는 시냅스들을 통해 여러 입력들과 연결된다. 신경망들은 통상적인 소프트웨어 및 하드웨어로서 프로그램되지 않고, 다만 트레이닝된다. 이러한 트레이닝은 통상적으로 충분한 수의 대표적인 예시들의 분석을 통해 그리고 시냅스 가중치의 통계적인 또는 알고리즘적 선 택에 의해, 주어진 입력 이미지들 세트가 주어진 출력 이미지들 세트에 대응되도록 수행된다. 전통적인 신경망 들의 공통적인 비판은 상당한 시간 및 다른 리소스들이 그들의 트레이닝에 자주 필요하다는 것이다. 다양한 인공 신경망들이 후술하는 미국 특허 4,979,124; 5,479,575; 5,493,688; 5,566,273; 5,682,503; 5,870,729; 7,577,631; 및 7,814,038에서 개시된다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 인공 신경망 및 트레이닝 방법을 제공한다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에서, 신경망은 트레이닝 이미지들을 수신하도록 구성된 신경망에 대한 다수의 입력들을 포함한다. 트 레이닝 이미지들은 다수의 입력들에 의해 신경망의 트레이닝동안, 즉, 다수의 입력들에 의해 수신된 후에, 트레 이닝 입력 값 어레이로서 수신되거나 또는 트레이닝 입력 값 어레이로서 코딩된 것의 하나이다. 신경망은 또한 다수의 시냅스들을 포함한다. 각각의 시냅스는 다수의 입력들 중 하나에 연결되고 그리고 다수의 보정 가중치들 을 포함한다. 각각의 보정 가중치가 가중치 값에 의해 정의되고, 그리고 다수의 시냅스들의 보정 가중치들이 보 정 가중치 어레이로 조직화된다. 신경망은 부가적으로 다수의 뉴런들을 포함한다. 각각의 뉴런이 적어도 하나의 출력을 갖고 그리고 다수의 시냅 스들 중 적어도 하나를 통해 다수의 입력들 중 적어도 하나와 연결된다. 각각의 뉴런이 각각의 뉴런에 연결된 대응하는 각각의 시냅스의 보정 가중치들의 가중치 값들을 합산하도록 구성됨으로써, 다수의 뉴런들이 뉴런 합 어레이를 생성한다. 신경망은 또한 원하는 출력 값 어레이로 조직화된 원하는 이미지들을 수신하도록 구성된 컨 트롤러를 포함한다. 컨트롤러는 또한 원하는 출력 값 어레이로부터 뉴런 합 어레이의 편차를 결정하며 그리고 편차 어레이를 생성하 도록 구성된다. 컨트롤러는 부가적으로 결정된 편차 어레이를 이용하여 보정 가중치 어레이를 변경하도록 구성 된다. 뉴런 합 어레이를 결정하기 위해 변경된 보정 가중치 값들을 합산하면 원하는 출력 값으로부터 뉴런 합 어레이의 편차를 감소시키고, 즉, 트레이닝 도중 뉴런망에 의해 생성된 오류를 보상하고, 트레이닝된 보정 가중치 어레이를 생성하여, 신경망의 동시 및 병렬 트레이닝을 용이하게 한다. 트레이닝된 신경망에서, 신경망에 대한 다수의 입력들은 입력 이미지들을 수신하도록 구성될 수 있다. 이러한 입력 이미지들은 신경망에 의해 이미지들의 인식동안 입력 값 어레이로서 수신되거나 또는 입력 값 어레이로서 코딩되는 것중 하나일 수 있다. 각각의 시냅스는 트레이닝된 보정 가중치 어레이 중 다수의 트레이닝된 보정 가 중치들을 포함할 수 있다. 부가적으로, 각각의 뉴런은 각각의 뉴런에 연결된 각각의 시냅스에 대응하는 트레이 닝된 보정 가중치들 중 가중치 값들을 가산하도록 구성될 수 있음으로써, 다수의 뉴런들이 인식된 이미지들 어 레이를 생성하여, 이러한 입력 이미지들의 인식을 제공한다. 신경망은 또한 분배기들의 세트를 포함할 수 있다. 이러한 실시예에서, 분배기들의 세트는 각각의 트레이닝 이 미지들 그리고 각각의 트레이닝 입력 값 어레이와 입력 값 어레이로서 입력 이미지들을 코딩하도록 구성될 수 있다. 이러한 분배기들의 세트는 각각의 트레이닝 이미지들 및 입력 이미지들을 수신하기 위해 다수의 입력들에 동작 가능하게 연결될 수 있다. 컨트롤러는 부가적으로 원하는 출력 값 어레이로부터 뉴런 합 어레이의 타겟 편차의 어레이로 프로그래밍될 수 있다. 또한, 컨트롤러는 원하는 출력 값 어레이로부터 뉴런 합 어레이의 편차가 타겟 편차 어레이의 수용 가능 한 범위 내에 있을 때, 신경망의 트레이닝을 완료하도록 구성될 수 있다. 트레이닝 입력 값 어레이, 입력 값 어레이, 보정 가중치 어레이, 뉴런 합 어레이, 원하는 출력 값 어레이, 편차 어레이, 트레이닝된 보정 가중치 어레이, 인식된 이미지 어레이, 및 타겟 편차 어레이는, 각각, 트레이닝 입력 값 매트릭스, 입력 값 매트릭스, 보정 가중치 매트릭스, 뉴런 합 매트릭스, 원하는 출력 값 매트릭스, 편차 매 트릭스, 트레이닝된 보정 가중치 매트릭스, 인식된 이미지 매트릭스 및 타겟 편차 매트릭스로 조직화될 수 있다. 신경망은 부가적으로 다수의 데이터 프로세서들을 포함할 수 있다. 이러한 실시예에서, 컨트롤러는 부가적으로 각각의 입력 값, 트레이닝 입력 값, 보정 가중치, 뉴런 합 및 원하는 출력 값 매트릭스들 중 적어도 하나를 각 각의 서브-매트릭스들로 분할하고 그리고 서브-매트릭스로 별도의 병렬 수학 연산을 위해 다수의 결과적인 서브 -매트릭스들이 다수의 데이터 프로세서들과 통신하도록 구성될 수 있다. 이러한 분할 즉, 임의의 대상 매트릭스 들을 각각의 서브-매트릭스들로 분할하는 것은 동시 또는 병렬 데이터 프로세싱을 용이하게 하고 입력 값 매트 릭스의 이미지 인식 및 신경망의 트레이닝 중 하나의 속도를 향상시킨다. 이러한 동시 또는 병렬 데이터 프로세 싱은 또한 신경망의 확장성을 허용한다. 컨트롤러는 트레이닝 입력 값 매트릭스 및 보정 가중치 매트릭스에 대수 매트릭스 연산을 적용하여 보정 가중치 매트릭스를 변경할 수 있음으로써, 신경망을 트레이닝한다. 수학적 매트릭스 연산은 트레이닝 입력 값과 보정 가중치 매트릭스들의 수학적 곱의 결정을 포함할 수 있음으로 써, 현재의 트레이닝 시기 가중치 매트릭스를 형성한다. 컨트롤러는 또한 뉴런 합들의 편차 매트릭스를 생성하기 위해 원하는 출력 값 매트릭스로부터 뉴런 합 매트릭스 를 감산할 수 있다. 부가적으로, 컨트롤러는 뉴런 입력 당 편차 매트릭스를 생성하기 위해 뉴런 합들의 편차 매 트릭스를 각각의 뉴런에 연결된 입력들의 수로 나누도록 구성될 수 있다. 컨트롤러는 각각의 보정 가중치가 신경망에서 하나의 트레이닝 시기동안 사용된 횟수를 결정하도록 구성될 수 있다. 컨트롤러는 부가적으로 각각의 보정 가중치가 하나의 트레이닝 시기동안 사용된 결정된 횟수를 사용하여 하나의 트레이닝 시기에 대한 평균 편차 매트릭스를 형성하도록 구성될 수 있다. 또한, 컨트롤러는 하나의 트레 이닝 시기에 대한 평균 편차 매트릭스를 보정 가중치 매트릭스에 가산하도록 구성될 수 있어, 트레이닝된 보정 가중치 매트릭스를 생성하고 하나의 트레이닝 시기를 완료한다. 이러한 신경망을 동작시키는, 즉 트레이닝 및 이미지 인식을 위한 방법이 또한 개시된다. 또한, 인공 신경망을 동작시키기 위한 비 일시적 컴퓨터 판독 가능 저장 장치 및 인공 신경망을 동작시키기 위 한 장치가 개시된다. 다른 실시예에서, 신경망은 다수의 망 입력들을 포함함으로써, 각 입력이 입력 값을 갖는 입력 신호를 수신하도 록 구성된다. 신경망은 또한 다수의 시냅스를 포함하되, 각 시냅스는 다수의 입력들 중 하나에 연결되고 그리고 다수의 보정 가중치를 포함하되, 각각의 보정 가중치는 각각의 가중치를 유지하는 메모리 소자에 의해 설정된다. 신경망은 부가적으로 분배기들의 세트를 포함한다. 각각의 분배기는 각각의 입력 신호를 수신하기 위 해 다수의 입력들 중 하나에 동작 가능하게 연결되고, 입력 값과 상관하여 다수의 보정 가중치들로부터 하나 이상의 보정 가중치들을 선택하도록 구성된다. 신경망은 또한 뉴런들의 세트를 포함한다. 각각의 뉴런은 적어도 하나의 출력을 가지며, 다수의 시냅스들 중 하나의 시냅스를 통해 다수의 입력들 중 적어도 하나와 연결되고, 각각의 뉴런에 연결된 각각의 시냅스로부터 선택된 보정 가중치들 중 가중치 값을 가산하도록 구성되어, 뉴런 합을 생성한다. 각 뉴런의 출력은 신경망의 작동 출력 신호를 설정하기 위해 각각의 뉴런 합을 제공한다. 신경망은 또한 값을 갖는 원하는 출력 신호를 수신하고, 원하는 출력 신호 값으로부터의 뉴런 합의 편차를 결정 하고, 결정된 값을 사용하여 대응하는 메모리 소자에 의해 설정된 각각의 보정 가중치를 변경(수정)하도록 구성 된 가중치 보정 계산기를 포함할 수 있다. 이러한 경우에, 뉴런 합을 결정하기 위해 변경된 보정 가중치 값들을 가산하는 것은 원하는 출력 신호 값으로부터 뉴런 합의 편차를 최소화하여 트레이닝된 신경망을 생성하기 위한 것이다. 트레이닝된 신경망은 값 및 대응하는 부가적인 원하는 출력 신호를 갖는 보충 입력 신호만을 사용하여 보충 트 레이닝을 수신하도록 구성될 수 있다. 신경망의 트레이닝 동안 또는 보충 트레이닝 이전에, 다수의 시냅스들 각각은 각각의 메모리 소자들에 의해 설 정된 하나 이상의 추가적인 보정 가중치들을 수용하도록 구성될 수 있다. 신경망은 신경망의 트레이닝 동안 또는 후에 각각의 시냅스들로부터 각각의 메모리 소자에 의해 설정된 하나 이 상의 보정 가중치를 제거하도록 구성될 수 있다. 이러한 보정 가중치의 제거는 신경망이 신경망을 동작시키는데 필요한 다수의 메모리 소자만을 보유하게 할 수 있다. 신경망은 신경망의 트레이닝 전 또는 도중에 추가 입력, 추가 시냅스 및 추가 뉴런 중 적어도 하나를 수용하여 신경망의 동작 파라미터를 확장하도록 구성될 수 있다. 신경망은 신경망의 트레이닝 전, 도중 또는 후에 입력, 시냅스 및 뉴런 중 적어도 하나를 제거하도록 구성될 수 있다. 망에 의해 사용되지 않는 신경망 소자(요소)를 제거하는 그러한 능력은 망의 출력 품질 손실없이 구조를 단순화하고 신경망의 동작 파라미터를 수정하도록 의도된다. 각각의 메모리 소자는 각각의 가중치를 정의하도록 구성된 전기적 및/또는 자기적 특성을 특징으로 하는 전기 디바이스에 의해 설정될 수있다. 이러한 특성은 저항, 임피던스, 용량, 자기장, 인덕션, 전계 강도 등일 수 있 다. 각 디바이스의 각각의 전기적 및/또는 자기적 특성은 신경망의 트레이닝동안 변화되도록 구성될 수 있다. 또한, 보정 가중치 계산기는 대응하는 전기 디바이스의 전기적 및 자기 적 특성 각각을 변화시킴으로써 각각의 보정 가중치 값을 변경할 수 있다. 전기 디바이스는 저항, 메미스터, 멤리스터, 트랜지스터, 캐패시터, 전계효과트랜지스터, 광의존성 저항(LDR) 또는 자기의존성 저항(MDR)과 같은 광소자 저항 중 하나로서 구성될 수 있다. 각각의 메모리 소자는 전기 저항들 블록에 의해 설정될 수 있으며, 각각의 보정 가중치를 설정할 수 있도록 결 정된 편차를 사용하여 블록으로부터 하나 이상의 전기 저항들을 선택하도록 구성된 선택기 디바이스를 포함한다. 전기 저항들의 블록은 전기 커패시터들을 추가로 포함할 수 있다. 즉, 각각의 메모리 소자는 전기 저항들 및 전 기 커패시터들을 모두 갖는 블록에 의해 형성 될 수 있다. 선택기 디바이스는 각각의 가중치를 설정하도록 결정 된 편차를 사용하여 커패시터들을 선택하도록 추가로 구성될 수 있다. 신경망은 아날로그, 디지털 및 디지털-아날로그 망 중 하나로 구성될 수 있다. 이러한 망에서, 다수의 입력들, 다수의 시냅스들, 메모리 소자들, 분배기들의 세트, 뉴런들의 세트, 가중치 보정 계산기 및 원하는 출력 신호 중 적어도 하나는 아날로그, 디지털 및 디지털-아날로그 형식으로 구성된다. 신경망이 아날로그 망으로 구성된 경우, 각각의 뉴런은 전기적 와이어, 또는 직렬 또는 병렬 버스와 같은, 직렬 및 병렬 통신 채널중 하나에 의해 설정될 수 있다. 가중치 보정 계산기는 차동 증폭기들의 세트로서 설정될 수 있다. 또한, 각각의 차동 증폭기는 각각의 보정 신 호를 생성하도록 구성 될 수 있다. 각각의 분배기는 수신된 입력 신호에 응답하여 다수의 보정 가중치들로부터 하나 이상의 보정 가중치들을 선택 하도록 구성된 디멀티플렉서 일 수 있다. 각각의 분배기는 수신된 입력 신호를 이진 코드로 변환하고 이진 코드와 관련하여 다수의 보정 가중치들로부터 하나 이상의 보정 가중치를 선택하도록 구성 될 수 있다. 신경망은 메모리를 갖는 전자 디바이스에 프로그래밍될 수 있되, 각각의 메모리 소자는 전자 디바이스의 메모리 에 저장된다. 유틸리티 신경망을 동작시키는 방법이 또한 개시된다. 방법은 트레이닝 중에 별도의 유사 신경망에 의해 설정된 변경된 보정 가중치를 사용하여 유틸리티 신경망을 통해 데이터를 처리하는 단계를 포함한다. 방법은 또한 별도 의 유사 신경망에 의해 설정된 변경된 보정 가중치 값들을 사용하여 유틸리티 신경망의 동작 출력 신호를 설정 하는 단계를 포함한다. 유틸리티 신경망에 의해 변경된 보정 가중치 값들의 사용을 위해, 별도의 유사 신경망은 트레이닝 입력 값을 갖 는 트레이닝 입력 신호를 신경망으로의 입력을 통해 수신하는 단계; 트레이닝 입력 신호를 입력에 작동 가능하 게 연결된 분배기에 전달하는 단계; 다수의 보정 가중치들로부터 하나 이상의 보정 가중치를, 트레이닝 입력 값 에 관련하여, 분배기를 통하여 선택하되, 보정 가중치 각각은 가중치 값에 의해 정의되고 입력에 연결된 시냅스 상에 위치하는, 단계; 뉴런 합을 생성하도록 시냅스를 통해 입력과 연결된 뉴런을 통해 적어도 하나의 출력을 갖는 선택된 보정 가중치의 가중치를 가산하는 단계; 가중치 보정 계산기를 통해 값을 갖는 원하는 출력 신호를 수신하는 단계; 가중치 보정 계산기를 통해, 원하는 출력 신호 값으로부터 뉴런 합의 편차를 결정하는 단계; 및 변경된 보정 가중치 값들을 설정하기 위해 결정된 편차를 사용하여 각각의 보정 가중치 값들을, 가중치 보정 계 산기를 통해, 변경하여, 뉴런 합을 결정하기 위해 변경된 보정 가중치 값들을 합산하여 원하는 출력 값으로부터 뉴런 합의 편차를 최소화하여, 신경망을 트레이닝하는, 단계에 의해 트레이닝된다. 유틸리티 신경망 및 트레이닝된 개별 유사 신경망은 다수의 입력들, 보정 가중치들, 분배기들, 뉴런들 및 시냅 스들을 포함하는 매칭 신경망을 포함할 수 있다. 유틸리티 신경망 및 트레이닝된 개별 아날로그 신경망 각각에서, 각각의 보정 가중치는 각각의 가중치를 유지하 는 메모리 소자에 의해 설정될 수 있다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 1, "content": "도 1에 도시된 바와 같이, 종래의 인공 신경망은 전형적으로 입력 디바이스들, 시냅스 가중치를 갖 는 시냅스들, 가산기와 활성 함수 디바이스를 포함하는 뉴런들, 뉴런 출력들 및 가중치 보 정 계산기를 포함한다. 각 뉴런은 두 개 이상의 입력 디바이스들에 대해 시냅스들을 통해 연결 된다. 시냅스 가중치의 값들은 통상적으로 전기적 저항, 전도도, 전압, 전하, 자성 특성, 또는 다른 파라미 터들을 사용하여 표현된다. 전통적인 신경망의 통제된 트레이닝(supervised training)은 일반적으로 트레이닝쌍의 세트의 적용에 기초한다. 각 트레이닝쌍은 통상적으로 입력 이미지(28-1)와 감시 신호(supervisory signal)로도 알려진 원 하는 출력 이미지(28-2)로 구성된다. 전통적인 신경망의 트레이닝은 일반적으로 다음과 같이 제공된다. 입 력 신호들(Ii-Im)의 세트의 형태인 입력 이미지가 입력 디바이스로 들어가고, 초기 가중치(Wi)로 시냅스 가 중치로 이동된다. 입력 신호의 값은 가중치에 의해, 통상적으로 각 가중치에 의해 각 신호(Ii-Im)을 곱하거 나 나눔으로써 변경된다. 시냅스 가중치로부터, 변경된 입력 신호들은 각 뉴런으로 이동된다. 각 뉴런 은 대상 뉴런(subject neuron)에 관련된 시냅스의 그룹으로부터 신호들의 세트를 수신한다. 뉴런 에 포함된 가산기는 가중치에 의해 변경되고 대상 뉴런에 의해 수신된 모든 입력 신호들을 합한다. 활 성 함수 디바이스는 각 결과적인 뉴런 합을 수신하고, 수학적 함수(들)에 따라 합을 변경하여, 뉴런 출력 신호들(∑Fi...∑F)의 세트로서 출력 각 출력 이미지를 형성한다. 뉴런 출력 신호들(∑Fi...∑F)에 의해 정의된 획득된 뉴런 출력 이미지는 미리 설정된 원하는 출력 이미지들 (Oi-On)과 가중치 보정 계산기에 의해 비교된다. 획득된 뉴런 출력 이미지 ∑Fn 과 원하는 출력 이미지 On 사이의 결정된 차이에 근거하여, 시냅스 가중치를 변경하는 보정 신호들이 미리 프로그램된 알고리즘을 이 용하여 형성된다. 보정이 모든 시냅스 가중치에 대해 이루어진 이후, 입력 신호(Ii-Im)의 세트는 신경망 에 재도입되고, 새로운 보정이 이루어진다. 위의 사이클은 획득된 뉴런 출력 이미지 ∑Fn 과 원하는 출력 이미지 On 의 사이의 차이가 어떤 미리 결정된 에러에 비해 작은 것으로 결정될 때까지 반복된다. 모든 개별적 인 이미지를 갖는 망 트레이닝의 하나의 사이클은 통상적으로 \"트레이닝 시기(training epoch)\"로서 통상적으로 식별된다. 일반적으로, 각 트레이닝 시기에서, 에러의 크기는 감소된다. 그러나, 입력 및 출력의 수 뿐만 아니 라 개별적인 입력(Ii-Im)의 수에 따라, 전통적인 신경망의 트레이닝은 많은 수의 트레이닝 시기를 필요로 할 수 있고, 일부 경우에서, 수십만번 만큼 클 수 있다. 홉필드 네트워크(Hopfield network), 제한된 볼츠만 머신(Restricted Boltzmann Machine), 방사기저함수 네트 워크(Radial basis function network) 및 순환형 신경망(recurrent neural network)을 포함하는 다양한 전통적 인 신경망이 존재한다. 분류 및 클러스터링의 특정한 태스크들은 특정한 타입의 신경망, 네크워크 입력 트레이 닝 정보로서 오직 입력 이미지만을 사용하고, 특정 입력 이미지에 대응하는 원하는 출력 이미지는 최대값을 갖 는 출력 신호를 갖는 단일 위닝 뉴런(winning neuron)에 기초한 트레이닝 과정 동안 직접 형성되는 자기조직화 맵(Self-Organizing Maps) 을 필요로 한다. 위에서 언급한 것처럼, 신경망과 같은 존재하는 전통적인 신경망들의 주요 관심사들 중 하나는 그 성공적인 트레이닝이 많은 소요 시간을 필요로 할 수 있다는 것이다. 전통적인 네트워크들의 일부 추가적인 관심사는 컴 퓨팅 리소스들의 많은 소모일 수 있고, 이것은 결과적으로 강력한 컴퓨터들을 필요하게끔 유도한다. 추가적인 관심사들은 네트워크의 전체적인 재교육이 없이는 네트워크의 크기를 증가시키기 불가능함, \"네트워크 마비 (network paralysis)\" 및 \"극솟값(local minimum)에서의 프리징(freezing)\"과 같은 이러한 현상들에 대한 성향 이고, 이것은 특정한 신경망이 특정한 시퀀스에서 특정한 이미지들의 세트로 트레이닝될 수 있는지를 예측하기 불가능하게 만든다. 또한, 트레이닝 동안 도입되는 특정한 이미지들의 시퀀싱과 관련한 한정들이 있을 수 있고, 트레이닝 이미지들의 도입의 순서를 변경하는 것은 이미 학습된 네트워크의 추가적인 트레이닝을 수행하는 것에 대한 불가능 뿐만 아니라 네트워크 프리즈로 이어질 수 있다. 유사한 도면 부호들은 유사한 구성을 언급하는 나머지 도면들을 참조하면, 도 2는 프로그래시브 신경망, 따라서 \"프로그래시브 네트워크\", 또는 \"p-net\"의 개략도를 나타낸다. P-net은 다수의 또는 p-net의 입력들 의 세트를 포함한다. 각 입력은 입력 신호를 수신하도록 구성되고, 입력 신호들은 도 2에서 I1, I2 … Im 로서 표시된다. 각 입력 신호 I1, I2 … Im 는 예를 들어, 크기, 주파수, 위상, 신호 편광각, 또는 입력 이미지의 다른 부분들과의 관련성과 같은 입력 이미지의 일부 특성(들)의 값을 나타낸다. 각 입력 신 호는 입력값을 갖고, 다수의 입력 신호와 함께 일반적으로 입력 이미지를 설명한다. 각 입력값은 -∞ 와 +∞의 사이에 있는 값 범위 내에 있을 수 있고, 디지털 및/또는 아날로그 형태로 설정될 수 있다. 입력값의 범위는 트레이닝 이미지들의 세트에 종속할 수 있다. 가장 단순한 경우에서, 입력값들의 범위는 모든 트레이닝 이미지들을 위한 입력값들의 가장 작은 값과 가장 큰 값의 사이의 차이일 수 있다. 실제적인 이 유로서, 입력값들의 범위는 너무 높은 것으로 여겨지는 입력값들을 제거함으로써 한정될 수 있다. 예를 들어, 입력값들의 범위의 이러한 한정은 중요도 샘플링(importance sampling)과 같은 분산 감소(variance reductio n)를 위한 알려진 통계적인 방법들을 통해 수행될 수 있다. 입력 값들의 범위를 한정하는 다른 예는 특정한 최 소값에 대해 미리 설정된 최소 레벨에 비해 낮은 모든 신호들의 지정과 특정한 최대값에 대해 미리 설정된 최대 레벨을 초과하는 모든 신호들의 지정일 수 있다. p-net은 또한 다수의 또는 시냅스들의 세트를 포함할 수 있다. 각 시냅스는 도 2에 도시된 바와 같이 다수의 입력들 중 하나와 연결되고 다수의 보정 가중치를 포함하고, 시냅스 가중치를 또한 포함할 수 있다. 각 보정 가중치는 각 가중치값에 의해 정의된다. P-net은 또한 분배기의 세트를 포함한다. 각 분배기는 각 입력 신호를 수신하기 위해 다수의 입력들의 하나와 동작적으 로 연결된다. 추가적으로, 각 분배기는 입력값과 관련해서 다수의 보정 가중치들로부터 하나 이상의 보정 가중치들을 선택하도록 구성된다. p-net은 추가적으로 뉴런들의 세트를 포함한다. 각 뉴런은 적어도 하나의 출력을 갖고 하 나의 시냅스를 통해 다수의 입력들 중 적어도 하나와 연결된다. 각 뉴런은 각 뉴런에 대해 연결된 각 시냅스로부터 선택된 보정 가중치의 보정 가중치 값들을 합산하거나 합하도록 구성되고, 따라서 ∑n 으로 지정된 뉴런 합을 작성하고 출력한다. 분리된 분배기는 도 3a, 3b, 3c에서 도시된 것처럼 특정한 입력의 각 시냅스를 위해 사용될 수 있거나, 또는 도 4a, 4b 및 4c에서 도시된 것처럼 단일 분배기가 모든 이러한 시냅스들을 위해 사용될 수 있다. p-net의 형성 또는 설정 동안, 모든 보정 가 중치들은 초기값들을 할당받고, 이것은 p-net 트레이닝의 과정 동안 변경될 수 있다. 보정 가중치의 초기값은 예를 들어, 가중치들이 무작위로 선택될 수 있는 전통적인 신경망에서처럼 할당될 수 있고, 미리 설정된 템플릿 등으로부터 선택된 미리 설정된 수학적 함수의 도움으로 계산될 수 있다. p-net은 또한 가중치 보정 계산기를 포함한다. 가중치 보정 계산기는 신호값을 갖고 출력 이미 지의 부분을 표시하는 원하는, 예를 들어, 미리 설정된 출력 신호를 수신하도록 구성된다. 가중치 보 정 계산기는 또한 트레이닝 에러로도 알려진 원하는 출력 신호의 값으로부터 뉴런 합의 편차 를 결정하고, 결정된 편차를 사용하여 각 보정 가중치 값을 변경하도록 구성된다. 따라서, 뉴런 합 을 결정하기 위한 변경된 보정 가중치 값들을 합하는 것은 원하는 출력 신호의 값으로부터 대상 뉴런 합의 편차를 최소화하고, 그 결과로서, p-net을 트레이닝하기에 효과적이다. 도 1에 대해 언급된 전통적인 네트워크로 유추하면, 편차는 결정된 뉴런 합와 원하는 출력 신호 의 값의 사이에서 트레이닝 에러로서 설명될 수 있다. 도 1과 관련하여 논의된 전통적인 신경망과 비 교하여, p-net에서 입력 신호의 입력 값들은 단지 일반적인 네트워크 설정의 과정에서만 변하고, p- net의 트레이닝동안 변하지 않는다. 입력 값을 변경하는 대신, p-net의 트레이닝은 보정 가중치의 값 을 변경하여 제공된다. 추가적으로, 비록 각 뉴런이 합 함수를 포함하나, 뉴런은 보정 가중치 값들을 합산하고, 뉴런은 필요하지 않으며, 또한 실제로, 전통적인 신경망에서 활성 함수 디바이스에 의 해 제공되는 것과 같은 활성 함수의 부재에 의해 특징된다. 전통적인 신경망에서, 트레이닝 중 가중치 보정은 시냅스 가중치를 변경함에 의해 수행되나, 반면, p- net에서 대응되는 가중치 보정은 도 2에서 도시된 것처럼, 보정 가중치 값을 변경함에 의해 제공된다. 각 보정 가중치는 전체 또는 일부 시냅스들 상에 위치된 가중치 보정 블록들에 포함 될 수 있다. 신경망 컴퓨터 에뮬레이션에서, 각 시냅시스 및 보정 가중치는 예를 들어 메모리 셀과 같은 디지털 디바이스 및/또는 아날로그 디바이스에 의해 표시될 수 있다. 신경망 소프트웨어 에뮬레이션에서, 보정 가중치 의 값들은 적절하게 프로그램된 알고리즘을 통해 제공될 수 있고, 반면 하드웨어 에뮬레이션에서, 메모리컨트롤을 위해 알려진 방법들이 사용될 수 있다. p-net에서, 원하는 출력 신호로부터 뉴런 합의 편차는 그 사이에서 수학적으로 계산된 차 이로서 표시될 수 있다. 추가적으로, 각 변경된 보정 가중치의 작성은 뉴런 합을 작성하기 위해 사용 된 각 보정 가중치에 대한 계산된 차이의 분배를 포함할 수 있다. 이러한 실시예에서, 각 변경된 보정 가중치 의 작성은 뉴런 합이 작은 수의 시기 내에서 원하는 출력 신호 값 상에 수렴하도록 허용할 것이고, 단지 하나의 시기만 필요로 하는 일부 경우에서, p-net을 빠르게 트레이닝하는 것을 허용할 것이다. 특정 경우에서, 뉴런 합을 작성하는데 사용되는 보정 가중치의 사이에서의 수학적 차이의 분배는 각 뉴런 합을 작성하기 위해 사용된 각 보정 가중치의 사이에서 결정된 차이를 동일하게 나누는 것을 포함할 수 있 다. 분리된 실시예에서, 원하는 출력 신호 값으로부터의 뉴런 합의 편차의 결정은 이에따라 편차 계수를 작성하기 위해 뉴런 합에 의해 원하는 출력 신호 값을 나누는 것을 포함할 수 있다. 이러한 특정한 경우에서, 각 변형된 보정 가중치의 변형은 편차 계수에 의해 뉴런 합을 작성하는데 사용된 각 보정 가중치의 곱을 포함한다. 각 분배기는 추가적으로 다수의 보정 가중치에 대해 다수의 영향 계수들을 할당 하도록 구성될 수 있다. 본 실시예에서, 각 영향 계수는 각 뉴런 합을 작성하기 위해 일부 미리 결정 된 비율에서 다수의 보정 가중치들의 하나를 할당할 수 있다. 각각의 보정 가중치와 대응을 위해, 각 영향 계수는 도면들에서 도시된 바와 같이 \"Ci,d,n\" 명명이 할당될 수 있다. 특정 시냅스에 대응되는 다수의 영향 계수들 각각은 각 영향 분포 함수에 의해 정의된다. 영향 분포 함수는 모든 영향 계수들에 대해 또는 단지 특정 시냅스에 대응되는 다수의 영향 계수들 에 대해 동일할 수 있다. 다수의 입력 값들의 각각은 간격들 또는 간격 분포 함수에 따라 서브 디비 전 \"d\"로 나누어진 값 범위로 수신될 수 있어서, 각 입력값은 각 간격 \"d\" 내에 수신되고, 각 보정 가중치 는 이러한 간격들의 하나에 대응한다. 각 분배기는 각 간격 \"d\"를 선택하고 선택된 각 간격 \"d\"에 대해 대 응하는 보정 가중치와 예를 들어 Wi,d+i,n 또는 Wi,d-i,n와 같이 선택된 각 간격에 인접한 간격에 대응되는 적 어도 하나의 보정 가중치에 각각의 다수의 영향 계수들을 할당한다. 다른 비제한적인 예에서, 영향 계수 의 미리 결정된 비율은 통계적 분포에 따라 정의될 수 있다. 뉴런 합을 작성하는 것은 입력값에 따라 각 보정 가중치에 각 영향 계수들을 초기적으로 할당하고, 그리고 나서 각 채택된 보정 가중치의 값에 의해 대상 영향 계수들을 곱하는 것을 포함할 수 있 다. 그리고 나서, 그에 연결된 모든 시냅스들에 대해 각 뉴런을 통해 보정 가중치 및 할당된 영 향 계수의 개별적인 곱들을 합한다. 가중치 보정 계산기는 각 변경된 보정 가중치를 작성하기 위해 각 영향 계수들을 적용하도록 구 성될 수 있다. 특히, 가중치 보정 계산기는 각 영향 계수에 의해 정해진 비율에 따라 뉴런 합을 작성하기 위해 사용된 각 보정 가중치에 대해 뉴런 합과 원하는 출력 신호의 사이의 계산된 수 학적 차이의 비율을 적용할 수 있다. 추가적으로, 뉴런 합을 작성하기 위해 사용된 보정 가중치들 사 이에서 나누어진 수학적 차이는 각 영향 계수에 의해 더 나누어질 수 있다. 그 이후, 각 영향 계수에 의해 뉴런 합의 나누어진 결과는 원하는 출력 신호값 상에 뉴런 합을 수렴시키기 위해 보정 가중치 에 더해질 수 있다. p-net의 전형적인 형성은 p-net의 트레이닝이 시작되기 전에 발생할 수 있다. 그러나, 개별적인 실시예에 서, 트레이닝하는 동안 p-net이 초기 보정 가중치가 없는 입력 신호를 수신하면, 적절한 보정 가중치 가 작성될 수 있다. 이러한 경우, 특정 분배기는 특정 입력 신호를 위한 적절한 간격 \"d\"을 결 정할 것이고, 초기값들을 갖는 보정 가중치의 그룹은 특정 입력, 특정 간격 \"d\" 및 모든 각 뉴런들 에 대해 작성될 것이다. 추가적으로, 대응하는 영향 계수는 각각 새로 작성된 보정 가중치에 할 당될 수 있다. 각 보정 가중치는 p-net 상에 각 보정 가중치의 위치를 식별하기 위해 구성된 인덱스들의 세트에 의 해 정의될 수 있다. 인덱스들의 세트는 특히 특정 입력에 대응하는 보정 가중치를 식별하기 위한 입 력 인덱스 \"i\", 각 보정 가중치에 대해 앞서 언급한 선택된 간격을 명시하기 위한 간격 인덱스 \"d\", 명명 \"Wi,d,n\"을 갖는 특정 뉴런에 대응하는 보정 가중치를 명시하기 위해 구성된 뉴런 인덱스 \"n\"을 포함할 수 있다. 따라서, 특정 입력에 대해 대응하는 각 보정 가중치는 대상 위치를 지정하기 위해 첨자가 있는 특정 인덱스 \"i\"를 할당받는다. 유사하게, 특정 뉴런 및 각 시냅스에 대응하는 각 보정 가중치\"W'는 p-net 상에 보정 가중치의 대상 위치를 지정하기 위해 첨자가 있는 특정 인덱스들 \"n\" 및 \"d\"를 할 당받는다. 인덱스들의 세트는 또한 p-net의 트레이닝 동안 각 보정 가중치가 입력 신호에 의해 액세스된 횟수를 총계하도록 구성된 액세스 인덱스 \"a\"를 포함할 수 있다. 액세스 인덱스 \"a\"는 명명 \"Wi,d,n,a\"을 채택함에 의해 각 보정 가중치의 현재 상태를 더 지정하거나 정의하는데 사용될 수 있다. \"i\", \"d\", \"n\" 및 \" a\"의 각 인덱스는 0 에서 +∞의 범위에서 수치값을 가질 수 있다. 입력 신호의 범위를 간격 d0, di ... dm으로 나누는 것의 다양한 가능성이 도 5에 도시된다. 특정 간격 분 포는 예를 들어, 동일한 크기의 모든 간격 \"d\"를 지정하는 것에 의해 달성될 수 있는 것처럼 균일하거나, 선형 적일 수 있다. 미리 결정된 최하 레벨에 비해 낮은 입력 신호 값을 갖는 모든 입력 신호들은 0 값을 갖도 록 고려될 수 있고, 미리 결정된 최상 레벨에 비해 큰 입력 신호 값을 갖는 모든 입력 신호들은 도 5에서 도시 된 바와 같이 이러한 최대 레벨을 할당받을 수 있다. 특정 간격 분포는 또한 대칭, 비대칭 또는 무제한과 같이 비균일 또는 비선형적일 수 있다. 간격 \"d\"의 비선형적 분포는 입력 신호들의 범위가 비실제적으로 큰 것으로 고려되는 때 유용할 수 있고, 범위의 특정 부분은 범위의 처음, 중간 또는 끝에서와 같이 가장 치명적인 것으로 고려되는 입력 신호들을 포함할 수 있다. 특정 간격 분포는 또한 무작위 함수에 의해 설명될 수 있다. 모든 선행하는 예시들은 비제한적인 것으로서, 간격 분포의 다른 변형들도 역시 가능하다. 입력 신호의 선택된 범위 내의 간격 \"d\"의 수는 p-net을 최적화하도록 증가될 수 있다. 이러한 p- net의 최적화는 예를 들어, 입력 이미지를 트레이닝하는 것의 복잡성에서 증가가 있는 때 바람직하다. 예를 들어, 더 많은 수의 간격은 단색 이미지들과 비교할 때 다색 이미지들에 대해 필요로 할 수 있고, 더 많은 수의 간격들은 단순한 그래픽들에 비해 복잡한 장식에 대해 필요로 할 수 있다. 증가된 수의 간 격들은 트레이닝 이미지의 보다 큰 전체 수를 위해서 뿐만 아니라, 윤곽에 의해 설명되는 이미지들과 비교할 때 복잡한 색상 구배를 갖는 이미지의 정확한 인식을 위해 필요할 수 있다. 간격 \"d\"의 수에서 감소는 노이즈의 높 은 크기, 트레이닝 이미지에서 큰 분산 및 컴퓨팅 리소스들에서 과도한 소모의 경우 역시 필요할 수 있다. p-net에 의해 다루어지는 태스크 또는 정보의 종류에 따라, 예를 들어, 화상 또는 텍스트 데이터, 다양한 속성의 센서로부터의 데이터, 다양한 수의 간격들 및 그 분포의 타입이 할당될 수 있다. 각 입력 신호값 간격 \"d\"에 대해, 인덱스 \"d\"를 갖는 특정 시냅스의 대응하는 보정 가중치가 할당될 수 있다. 따라서, 특정 간격 \" d\"는 특정한 입력에 대해 관련하여 인덱스 \"i\", 특정 간격에 대해 관련된 인덱스 \"d\" 및 0부터 n까지 인덱스 \"n\"에 대한 모든 값들을 갖는 모든 보정 가중치를 포함할 것이다. p-net의 트레이닝의 과정에서, 분 배기는 각 입력 신호 값을 정의하고 따라서 대응하는 간격 \"d\"에 대해 대상 입력 신호를 연관시킨다. 예를 들어, 0부터 100까지의 입력 신호들의 범위 내에 10개의 동일한 간격들 \"d\"가 있다면, 30과 40의 사이에서 값을 갖는 입력 신호는 예를 들어 \"d\"=3인 간격 3과 연관될 수 있다. 주어진 입력과 연결된 각 시냅스의 모든 보정 가중치에 대해, 분배기는 특정 입력 신호에 관련된 간격 \"d\"와 부합하여 영향 계수의 값을 할당할 수 있다. 분배기는 또한 정현, 정규, 대수 분 포 곡선, 또는 무작위 분포 함수와 같은 영향 계수(도 6에 도시됨)의 값들의 미리 결정된 분포와 부합하는 영향 계수의 값들을 할당할 수 있다. 많은 경우에서, 각 시냅스에 관련된 특정 입력 신호에 대 한 영향 계수 또는 Ci,d,n의 합 또는 적분은 1(one)의 값을 가질 것이다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "가장 단순한 경우에서, 입력 신호값에 가장 근접하게 대응하는 보정 가중치는 영향 계수(Ci,d,n)에 대 해 1(one)의 값을 할당받을 수 있고, 반면 다른 간격들에 대한 보정 가중치는 0(zero)의 값을 받을 수 있다. p-net은 전형적인 신경망과 비교할 때, p-net의 트레이닝 동안 소요 시간 및 다른 리소스들의 사용의 절감에 포커스된다. 비록 p-net의 부분으로서 여기서 기재된 구성들의 일부는 전통적인 신경망에서의 그것 과 유사한 특정 명칭 또는 식별자가 지정되었지만, 특정 명칭들은 명확성을 위해 사용되었고, 전통적인 신경망 에서 그 상대로부터 다르게 적용될 수 있다. 예를 들어, 입력 신호들(Ii-Im)의 크기를 컨트롤하기 위한 시냅스 가중치는 전통적인 신경망의 전반적인 설정의 과정 동안 설립되고 전통적인 네트워크의 트레이닝 동안 변경된다. 반면, p-net의 트레이닝은 보정 가중치를 변경하는 것에 의해 수행되고, 반면 시냅스 가중 치는 트레이닝 동안 변하지 않는다. 추가적으로, 위에서 논의한 것처럼, 각 뉴런은 합하거나 부가하는 구성을 포함하나, 전통적인 신경망에 대해 통상적인 활성 함수 디바이스는 포함하지 않는다. 전반적으로, p-net은 각 뉴런과 특정 뉴런과 대상 뉴런에 연결된 모든 각 시냅스 및 보정 가중 치를 포함하는 모든 연결 시냅스들을 포함하는 뉴런 유닛을 트레이닝함으로써 트레이닝된다. 따 라서, p-net의 트레이닝은 각 뉴런에 대해 기여하는 보정 가중치를 변경하는 것을 포함한다. 보 정 가중치에 대한 변경은 이하에서 상세히 설명될 방법에 포함된 그룹-트레이닝 알고리즘에 기반하여 발생한다. 설명된 알고리즘에서, 예를 들어, 편차와 같은 트레이닝 에러는 보정값이 각 뉴런에 의해 획득된 합을 결정하는데 사용된 가중치의 각각에 대해 결정되고 할당되는 것에 근거하여 각 뉴런에 대해 결정된다. 트레이닝 동안 이러한 보정의 도입은 대상 뉴런에 대한 편차를 0으로 줄이도록 의도된다. 추가적인 이미지들을 구비한 트레이닝 동안, 이전에 사용된 이미지들에 관련된 새로운 에러들이 다시 발생할 수 있다. 이러한 추가적인 에러들을 제거하기 위해, 한 트레이닝 시기의 완료 이후, p-net의 모든 트레이닝 이미지들에 대한 에러들이 계산될 수 있고, 민약 이러한 에러들이 미리 결정된 값들 보다 크다면, 하나 이상의 추가적인 트레이닝 시기들이 에러가 타겟 또는 미리 설정된 값 보다 작게 될 때까지 이루어진다. 도 2-22와 관련하여 앞에서 설명한 것과 같이, 도 23은 p-net을 트레이닝하는 방법을 도시한다. 방법 은 방법이 입력 값을 갖는 입력 신호를 입력을 통해 수신하는 것을 포함하는 프레임에서 시작된다. 프레임을 뒤이어, 방법은 프레임으로 진행한다. 프레임에서, 방법은 입력에 동 작적으로 연결된 분배기에 대해 입력 신호를 전달하는 것을 포함한다. 프레임 또는 프레임(20 4)에서, 방법은 인덱스들의 세트에 의해 각 보정 가중치를 정의하는 것을 포함할 수 있다. p- net의 구조에 대해 앞서 설명한 것처럼, 인덱스들의 세트는 입력에 대응하는 보정 가중치를 식 별하기 위해 구성된 입력 인덱스 \"i\"를 포함할 수 있다. 인덱스들의 세트는 또한 각 보정 가중치에 대해 선택된 간격을 특정하기 위해 구성된 간격 인덱스 \"d\", 및 \"Wi,d,n\"으로서 특정 뉴런에 대응하는 보정 가중 치를 특정하기 위해 구성된 뉴런 인덱스 \"n\"을 포함할 수 있다. 인덱스들의 세트는 추가적으로 각 보정 가 중치가 p-net의 트레이닝 동안 입력 신호에 의해 접근된 횟수를 총계하도록 구성된 액세스 인덱 스 \"a\"를 포함할 수 있다. 따라서, 각 보정 가중치의 현재 상태는 명명 \"Wi,d,n,a\"를 채택할 수 있다. 프레임의 이후, 방법은 프레임으로 진행하고, 방법은 분배기를 통해, 입력 값과 관련하여, 대상 입력에 연결된 시냅스 상에 위치한 다수의 보정 가중치들로부터 하나 이상의 보정 가중치를 선 택하는 것을 포함한다. 앞서 설명한 것처럼, 각 보정 가중치는 그 각각의 가중치 값에 의해 정의된다. 프 레임(206에서, 방법은 추가적으로 분배기를 통해, 다수의 영향 계수들을 다수의 보정 가중치로 할당하는 것을 포함할 수 있다. 프레임에서 방법은 뉴런 합을 작성하기 위해 미리 결정된 비율에서 다수의 보정 가중치 중 하나에 대해 각 영향 계수를 할당하는 것을 포함할 수도 있다. 또한, 프레임 에서 방법은 뉴런을 통해, 그에 연결된 모든 시냅스들에 대해 영향 계수를 할당하는 것을 포함할 수 있다. 추가적으로, 프레임에서 방법은 각 영향 계수에 의해 정해진 비율에 따라 뉴런 합 을 작성하기 위해 사용된 각 보정 가중치에 대해 결정된 차이의 비율을 가중치 보정 계산기를 통해 적용하는 것을 포함할 수 있다. p-net의 구조에 대해 앞서 설명한 것처럼, 다수의 영향 계수들은 영향 분포 함수에 의해 정의될 수 있다. 이러한 경우에서, 방법은 추가적으로 간격 분포 함수에 따라 간격 \"d\"로 나누어진 값 범위 로 입력값을 수신하는 것을 포함할 수 있어서, 입력값은 각 간격 내에 수신되고, 각 보정 가중치는 간격들 의 하나에 대응한다. 또한, 방법은 각 간격 \"d\"를 선택하고 다수의 영향 계수들을 선택된 각 간격 \"d\"에 대응하는 보정 가중치와 선택된 각 간격 \"d\"에 대해 인접한 간격에 대응하는 적어도 하나의 보정 가중치에 할당하기 위해 분배기를 통해 수신된 입력값을 사용하는 것을 포함할 수 있다. p-net의 구조에 대해 앞에서 설명한 것처럼, 선택된 각 간격 \"d\"에 인접한 간격에 대응하는 보정 가중치는 예를 들어 Wi,d+i,n 또 는 Wi,d i,n과 같이 식별될 수 있다. 프레임에 후속하여, 방법은 프레임으로 진행한다. 프레임에서, 방법은 뉴런 합을 작성하기 위해 시냅스를 통해 입력에 연결된 특정 뉴런에 의해 선택된 보정 가중치의 가중치 값들을 합하는 것을 포함할 수 있다. p-net의 구조에 대해 앞서 설명한 것처럼, 각 뉴런은 적어도 하나의 출 력을 포함한다. 프레임의 이후, 방법은 프레임으로 진행하고, 방법은 신호값을 갖는 원하는 출 력 신호를 가중치 보정 계산기를 통해 수신하는 것을 포함한다. 프레임에 후속하여, 방법은 방 법이 원하는 출력 신호의 값으로부터 뉴런 합의 편차를 가중치 보정 계산기를 통해 결정하는 것을 포함하는 프레임으로 진행한다. p-net의 설명에서 앞서 설명한 것처럼, 원하는 출력 신호 값으로부터 뉴런 합의 편차의 결정은 그 사이에서 수학적 차이를 결정하는 것을 포함할 수 있다. 추가적으로, 각 보정 가중치의 변형은 뉴런 합 을 작성하도록 사용된 각 보정 가중치에 대해 수학적 차이를 배분하는 것을 포함할 수 있다. 대안으로, 수 학적 차이의 배분은 뉴런 합을 작성하기 위해 사용된 각 보정 가중치의 사이에서 결정된 차이를 동일 하게 나누는 것을 포함할 수 있다. 다른 개별적인 실시예에서, 편차의 결정은 또한 그에 따라 편차 계수를 작성하기 위해 뉴런 합에 의해 원하는 출력 신호의 값을 나누는 것을 포함할 수 있다. 또한, 이러한 경우에서, 각 보정 가중치의 변형은 편차 계수를 작성하기 위해 뉴런 합을 작성하는데 사용된 각 보 정 가중치를 곱하는 것을 포함할 수 있다. 프레임의 이후, 방법은 프레임으로 진행한다. 프레임에서 방법은 결정된 편차를 사용하여 각 보정 가중치 값을 가중치 보정 계산기를 통해 변형하는 것을 포함한다. 변형된 보정 가중치 값은 이후 합산되거나 합해지고, 그리고 새로운 뉴런 합을 결정하는데 사용될 수 있다. 합산되어 변형된 보정 가중치 값은 원하는 출력 신호의 값으로부터 뉴런 합의 편차를 최소화하기 위해 제공하고 따라서 p- net을 트레이닝할 수 있다. 프레임에 후속하여, 방법은 원하는 출력 신호의 값으로부터 뉴 런 합의 편차가 충분히 최소화될 때까지 추가적인 트레이닝 시기를 수행하기 위해 프레임으로 돌아갈 수 있다. 다시 말해서, 추가적인 트레이닝 시기들이 원하는 출력 신호 상의 뉴런 합이 미리 결정된 편차 또는 에러 값 내로 수렴하도록 수행될 수 있고, p-net은 트레이닝된 것으로 고려되고 새로운 이미지 들과 동작을 위해 준비할 수 있다. 전반적으로, 입력 이미지는 p-net의 트레이닝을 위해 준비될 필요가 있다. 트레이닝을 위한 p- net의 준비는 전반적으로 입력 이미지들, 다수의 경우, 대상 입력 이미지들에 대응하는 원하는 출력 이미지들을 포함하는 트레이닝 이미지들의 세트의 형성으로 시작된다. p-net의 트레이닝을 위해 입력 신호들 I1, I2 … Im 에 의해 정의된 입력 이미지들(도 2에 도시)는 예를 들어, 사람 이미지 또는 다른 객 체들의 인식, 특정 행동의 인식, 클러스터링 또는 데이터 분류, 통계학적 데이터의 분석, 패턴 인식, 예측, 또 는 특정 과정들의 컨트롤과 같이 p-net이 다루도록 할당된 태스크들과 부합하여 선택된다. 따라서, 입력 이미지 는 예를 들어, jpeg, gif, 또는 pptx 포맷을 사용하여 컴퓨터에 도입되기 적합한 어느 형태, 테이블, 차트, 다이어그램과 그래픽, 다양한 문서 포맷 또는 기호들의 세트의 형태로 표시될 수 있다. p-net의 트레이닝을 위한 준비는 예를 들어, 모든 이미지들을 동일한 수의 신호들을 갖는 포맷으로, 또는 사진의 경우, 동일한 수의 화소로 변환하는 것과 같이, p-net에 의해 대상 이미지의 프로세싱을 위해 편리 한 그 통일화를 위해 선택된 입력 이미지의 변환을 포함할 수 있다. 색상 이미지들은 예를 들어, 세 개의 기본 색상의 조합으로서 표현될 수 있다. 이미지 변환은 예를 들어, 공간에서 이미지를 들어내고, 기호, 숫자, 또는 메모를 추가하는 것뿐만 아니라, 해상도, 밝기, 명암, 색상, 관점, 시각, 초점거리 및 초점, 이미지의 시 각적 특징을 변경하는 특성의 변환을 포함할 수 있다. 간격들의 개수의 선택 이후, 특정 입력 이미지는 즉, 실제 신호값들이 각 대상 개별적 신호들이 속한 간격들의 숫자로서 기록될 수 있는 간격 포맷의 입력 이미지로 컨버팅될 수 있다. 이러한 절차는 특정 이미지에 대해 각 트레이닝 시기에서 수행될 수 있다. 그러나, 이미지는 간격 숫자들의 세트로서 한번 형성될 수도 있다. 예를 들 어, 도 7에서 초기 이미지는 사진으로서 표시되고, 반면 \"디지털 포맷의 이미지\"의 테이블에서 동일한 이미지는 디지털 코드의 형태로 표시되며, \"간격 포맷의 이미지\" 테이블에서 이미지는 간격 숫자의 세트로서 표시되며, 개별적인 간격은 디지털 코드의 각 10개 값들을 위해 할당된다. 설명된 것처럼 p-net의 설명된 구조와 트레이닝 알고리즘 또는 방법은 p-net의 계속되거나 반복되는 트레이닝을 허용하고, 따라서, 트레이닝 과정의 시작에서 입력 이미지의 트레이닝의 완료된 세트를 형성할 필요가 없다. 상대적으로 작은 트레이닝 이미지들의 시작 세트를 형성하는 것이 가능하고, 이러한 시작 세트는 필요한만큼 확장될 수 있다. 입력 이미지들은 예를 들어, 한 사람의 사진의 세트, 고양이의 사진의 세트, 또는 자동차의 사진의 세트와 같이 구분되는 카테고리로 나뉠 수 있고, 각 카테고리는 사람의 이름이나 특정 라 벨과 같은 단일 출력 이미지에 대응한다. 원하는 출력 이미지는 필드 또는 디지털의 테이블을 나타내고, 각 포인트는 -∞에서 +∞까지 특정 수치값에 대응한다. 원하는 출력 이미지의 각 포인트는 p-net의 뉴런들의 하나의 출력에 대응한다. 원하는 출력 이미지는 이미지, 텍스트, 공식, 바코드와 같은 기호의 세 트 또는 소리의 디지털 또는 아날로그 코드로 인코딩될 수 있다. 가장 간단한 경우에서, 각 입력 이미지는 출력 이미지에 대응할 수 있고, 대상 입력 이미지를 인코딩할 수 있다. 이러한 출력 이미지의 포인트들의 하나는 예를 들어 100%인 최대 가능한 값을 할당받을 수 있고, 모든 다 른 포인트들은 예를 들어 0의 최소 가능한 값을 할당받을 수 있다. 이러한 경우, 후속하는 트레이닝에서, 트레 이닝 이미지들과 유사한 가능성의 형태에서 다양한 이미지들의 확률적 인식이 가능하게 될 것이다. 도 8은 사각 형과 원의 두 이미지들의 인식에 대해 트레이닝된 p-net이 각 그림이 합이 반드시 100%는 아닌 확률로서 표현된 각 그림의 일부 구성들을 포함한 사진을 인식할 수 있는지 예를 도시한다. 트레이닝을 위해 사용된 다른 이미지들의 사이에서 유사도의 확률을 정의하는 것에 의해 패턴 인식의 과정는 특정 이미지를 분류하는데 사용 될 수 있다. 정확도를 개선하고 에러를 배제하기 위해, 코딩이 하나의 출력 보다는 여러 뉴럴 출력의 세트를 사용하여 수행 될 수 있다(이하 참조). 가장 단순한 경우에서, 출력 이미지들이 트레이닝에 앞서 준비될 수 있다. 그러나, 트 레이닝 동안 p-net에 의해 형성되는 출력 이미지들을 갖는 것 역시 가능하다. p-net에서, 입력 및 출력 이미지들을 인버팅하는 가능성도 역시 있다. 다시 말해서, 입력 이미지는 디지털 또는 아날로그 값들의 필드 또는 테이블의 형태로 있을 수 있고, 각 포인트는 p-net의 하나의 입력에 대 응하며, 반면, 출력 이미지는 예를 들어, jpeg, gif, pptx 포맷을 사용하여 컴퓨터에 도입되기 적합한 어느 형 태, 테이블, 차트, 다이어그램과 그래픽, 다양한 문서 포맷 또는 기호들의 세트의 형태로 표시될 수 있다. 결과 적인 p-net은 이미지의 관련된 검색, 음악적 표현, 수식 또는 데이터 세트 뿐만 아니라 시스템을 보존하는 데 상당히 적합할 수 있다. 입력 이미지의 준비에 후속하여, 통상적으로 p-net은 형성될 필요가 있고 및/또는 존재하는 p-net의 파라미터들은 특정 태스크(들)를 핸들링하기 위해 설정되어야 한다. p-net의 형성은 후속하는 목적을 포함 할 수 있다. ● 입력 및 출력의 개수에 의해 정의되는 p-net의 차원(dimensions); ● 모든 입력들에 대한 시냅스 가중치; ● 보정 가중치의 개수 ● 입력 신호의 다른 값들을 위한 보정 가중치 영향 계수들(Ci,d,n)의 분포; 및 ● 원하는 트레이닝의 정확도 입력의 개수는 입력 이미지의 크기에 기초하여 결정된다. 예를 들어, 다수의 화소들이 사진에 사용될 수 있고, 반면 출력의 선택된 개수는 원하는 출력 이미지의 크기에 의존할 수 있다. 일부 경우에서, 출력의 선택된 개수는 트레이닝 이미지의 카테고리의 개수에 의존할 수 있다. 개별적인 시냅스 가중치의 값들은 -∞에서 +∞의 범위에 있을 수 있다. 0 보다 작은 시냅스 가중치는 신호 증폭을 나타낼 수 있고, 특정 입력으로부터 또는 예를 들어, 많은 수의 다른 개별 또는 객체들을 포함하는 사진에서 사람 얼굴의 더 효과적인 인식을 위해 특정 이미지로부터 영향을 향상시키는데 사용될 수 있다. 반면, 0보다 큰 시냅스 가중치의 값들은 신호 감쇄를 나타내는데 사용될 수 있고, p-net의 필요한 계산의 수를 줄이고 동작 속도를 증가시키기 위해 사용될 수 있다. 전반적으로, 시냅스 가중치가 커질수록, 보다 감쇄된 신호가 대응하는 뉴런으로 전송된다. 만약 모든 입력들에 대해 대응하는 모든 시냅스 가중치가 동 일하고 뉴런들이 모든 입력들에 동일하게 연결된다면, 신경망은 범용적으로 될 것이고, 사전에 이미지의 특성에 대해 거의 알려져 있지 않은 때와 같이 통상적인 태스크들에 대해 가장 효과적으로 될 것이다. 그러나, 이러한 구조는 트레이닝 및 동작 동안 요구되는 계산의 수가 점차적으로 증가하게 될 것이다. 도 9는 입력과 각 뉴런들의 연관이 통계학적 정규 분포와 부합하여 줄어드는 p-net의 일 실시예를 도시한 다. 시냅스 가중치의 고르지 않은 분포는 전체 입력 신호가 특정 입력에 대해 타겟 또는 \"중앙\" 뉴런에 전 달되는 것을 유도할 것이고, 따라서 대상 시냅스 가중치에 대해 0의 값을 할당한다. 추가적으로, 시냅스 가중치 의 고르지 않은 분포는 다른 뉴런들이 예를 들어, 정규(normal), 대수-정규(log-normal), 정현(sinusoidal) 또 는 다른 분포를 사용하여 감소된 입력 신호값들을 수신하도록 유도할 수 있다. 감소된 입력 신호값들을 수신하 는 시냅스 가중치의 값들은 \"중앙\" 뉴런으로부터 그 거리의 증가를 따라 함께 증가할 수 있다. 이러한 경 우에서, 계산의 개수는 줄어들 수 있고 p-net의 동작은 속도가 빨라질 수 있다. 이러한 네트워크에서, 완전하게 연결되고 불완전하게 연결된 것으로 알려진 신경망들의 조합은 예를 들어 사람 얼굴 또는 영화의 연속적인 프레 임과 같은 강한 내부 패턴들을 갖는 이미지들의 분석을 위해 대단히 효과적일 수 있다.도 9는 로컬 패턴의 인식을 위해 효과적인 p-net의 실시예를 도시한다. 공통 패턴들의 식별을 개선하기 위 해, 시냅스 가중치의 값들이 작거나 0인 강한 연결의 10-20%는 그리드(grid)의 형태와 같이 결정론적인 또 는 무작위적인 접근에서 전체 p-net을 통해 분포될 수 있다. 특정 태스크를 다루기 위해 의도된 p- net의 실제 형성은 소프트웨어 객체와 같이 시냅스, 시냅스 가중치, 분배기, 보정 가중치, 뉴런 등과 같은 p-net의 주요 구성들을 작성하는 예를 들어 객체-지향 프로그래밍 언어로 작성된 프로그램을 사용하여 수행될 수 있다. 이러한 프로그램은 지목된 객체와 그 행동을 특정하는 알고리즘 사이의 관례를 할당할 수 있다. 특히, 시냅틱 및 보정 가중치들은 그 초기값으로 설정됨과 함께 p-net의 형성의 시작에서 형성될 수 있다. p- net은 그 트레이닝의 시작 전에 완전하게 형성될 수 있고 예를 들어, 네트워크의 수용력이 고갈되는 때 또 는 치명적인 에러의 경우 필요에 따라 이후 프레임에서 변경되거나 추가될 수 있다. p-net의 완성은 또한 트레이닝이 계속되는 동안 가능할 수도 있다. p-net이 미리 형성되면, 특정 시냅스 상의 선택된 보정 가중치의 개수는 입력 신호들의 범위 내의 간격들 의 개수와 동일할 수 있다. 추가적으로, 보정 가중치들은 개별적인 간격의 모양에 대응한 신호들로서 p- net의 형성 이후 작성될 수 있다. 전통적인 신경망과 유사하게, p-net의 파라미터의 선택과 설정 은 목적된 실험의 일련을 통해 제공된다. 이러한 실험은 모든 입력들에서 동일한 시냅스 가중치를 갖 는 p-net의 형성 및 선택된 이미지를 위한 입력 신호값들의 평가와 간격들의 개수의 초기적 선택을 포함할 수 있다. 예를 들어, 바이너리(하나의 색상) 이미지의 인식을 위해, 단지 2개의 간격을 갖는 것으로 충분할 수 있고; 8비트 이미지의 질적인 인식을 위해 최대 256개의 간격들이 사용될 수 있고; 복잡한 통계적 의존성의 근 사치는 수십 또는 심지어 수백개의 간격을 필요로할 수 있고; 큰 데이터베이스를 위해 간격들의 개수는 수천일 수 있다. p-net의 트레이닝의 과정에서, 입력 신호들의 값들은 특정 간격들의 사이에서 분포됨에 따라 통합 (rounded)될 수 있다. 따라서, 간격의 개수에 의해 나누어진 범위의 폭보다 큰 입력 신호들의 정확도는 요구되 지 않을 수 있다. 예를 들어, 입력 값 범위가 100 유닛에 대해 설정되고 간격의 개수가 10이면, ± 5 이상의 정 확도는 요구되지 않을 것이다. 이러한 실험은 특정 입력 신호를 위해 간격에 대응하는 보정 가중치를 위해 1과 동일하게 설정될 수 있고, 반면 모든 잔존하는 보정 가중치들을 위한 보정 가중치 영향은 0으로 설정될 수 있는 입력 신호들의 값의 전체 범위를 통한 간격들의 균일한 분포와 보정 가중치 영향 계수들 Ci,d,n을 위한 가장 단순한 분포의 선택을 포함할 수 있다. 이러한 실험은 추가적으로 미리 결정된 정확도로 하나, 그 이상, 또 는 전체 준비된 트레이닝 이미지들을 갖는 트레이닝 p-net를 포함할 수 있다. 미리 결정된 정확도를 위한 p-net의 트레이닝 시간은 실험에 의해 설정될 수 있다. 만약 p-net의 정 확도 및 트레이닝 시간이 충분하다면, 선택된 설정들은 유지되거나 변경될 수 있고, 반면 검색은 보다 효과적인 분산을 위해 계속된다. 만약, 요구된 정확도가 달성되지 않았다면, 취적화 목적을 위해 특정 변형의 영향이 평 가될 것이고, 이것은 그때 하나 또는 그룹으로 수행될 수 있다. 이러한 변형의 평가는 간격들의 개수를 증가시 키거나 줄이는 변경; 보정 가중치 영향 계수 (Ci,d,n)의 분포 타입을 변경하고, 정규, 파워, 대수 또는 대수 정규 분포를 사용하는 것처럼 간격들의 비정규 분포로 분산을 테스팅; 및 예를 들어 비정규 분포에 대한 전이와 같이 시냅스 가중치의 값을 변경하는 것을 포함할 수 있다. 만약, 정확한 결과를 위해 요구되는 트레이닝 시간이 과도한 것 같다면, 증가된 수의 간격들을 갖는 트레이닝은 트레이닝 시간 상에 그 효과에 대해 평가될 수 있다. 만약, 그 결과, 트레이닝 시간이 감소되면, 간격들의 개수 에서 증가는 바람직한 트레이닝 시간이 필요한 정확도의 손실없이 획득될 때까지 반복될 수 있다. 만약, 트레이 닝 시간이 간격들의 증가하는 개수에 따라 줄어드는 대신 증가한다면, 추가적인 트레이닝은 줄어든 개수의 간격 들로 수행될 것이다. 만약, 간격들의 줄어든 개수가 줄어든 트레이닝 시간을 유도한다면, 간격들의 개수는 바람 직한 트레이닝 시간이 획득될 때까지 더 감소할 수 있다. p-net 설정의 형성은 미리 결정된 트레이닝 시간 및 트레이닝 정확도의 실험적 결정으로 트레이닝을 통할 수 있다. 파라미터들은 위에서 설명한 것들과 유사한 실험적 변경을 통해 개선될 수 있다. 다양한 p-net을 갖는 실제 실행은 설정 선택의 절차는 전반적으로 간단하고 시간을 소모하지 않음을 보여주고 있다. 도 23에서 도시된 방법의 일부로서 p-net의 실제 트레이닝은 입력 이미지 신호들(I1, I2 … Im)을 그 들이 시냅스에 대해 전송된 곳으로부터 네트워크 입력 디바이스에 대해 공급하는 것으로 시작하고, 시냅스 가중치를 통과하고 분배기(또는 분배기들의 그룹)에 입력한다. 입력 신호값에 기초하여, 분배기 는 특정 입력 신호가 대응하는 간격 “d”의 숫자를 설정하고, 각 입력에 연결된 모든 시냅스들의 가중치 보정 블록의 모든 보정 가중치에 대해 보정 가중치 영향 계수 Ci,d,n를 할당한다. 예를 들어, 만약 간격 “d”가 제 1 입력에 대해 3으로 설정될 수 있다면, W1,3,n Ci,3,n = 1인 모든 가중치에 대해 1로 설정되고, 반면 i≠ 1이고 d≠ 3인 모든 다른 가중치들에 대해 Ci,d,n은 0으로 설정될 수 있다. 아래의 관계에서 “n”으로서 식별된 각 뉴런에 대해, 뉴런 출력 합 ∑1, ∑2.. .∑n은 아래 관계에서 Wi,d,n로서 식별된 각 보정 가중치를 특정 뉴런에 대해 기여하는 모든 시냅스들에 대해 대응하는 보정 가중치 영향 계수 Ci,d,n를 곱하고, 획득된 값들을 모두 더함에 의해 형성된다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "Wi,d,n x Ci,d,n의 곱은 예를 들어, 분배기와 같은 저장된 가중치를 갖는 다양한 디바이스들에 의해 또는 뉴런 에 의해 직접 수행될 수 있다. 합들은 뉴런 출력을 통해 가중치 보정 계산기로 전달된다. 원하 는 출력 이미지를 설명하는 원하는 출력 신호들 O1, O2 … On은 역시 계산기로 공급된다. 위에서 논의한 것처럼, 가중치 보정 계산기는 출력 신호 O1, O2 … On 를 갖는 뉴런 출력 합 ∑1,∑2 ... ∑n의 비교에 의해 보정 가중치의 변형된 값들을 계산하기 위한 연산 디바이스이다. 도 11은 뉴런 출력 합 ∑1 에 대해 기여하는 보정 가중치 Wi,d,1의 세트가 대응하는 보정 가중치 영향 계수 Ci,d,1에 의해 곱해지고 이러한 곱 들이 뉴런 출력 합 ∑1에 의해 더해지는 것을 보여준다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "트레이닝이 시작함에 따라, 예를 들어, 제 1 시기 동안, 보정 가중치 Wi,d,1는 트레이닝에 대해 사용된 입력 이미 지에 대응하지 않으며, 따라서, 뉴런 출력 합들 ∑1는 대응하는 원하는 출력 이미지와 동일하지 않다. 최초 보정 가중치 Wi,d,1에 기초하여, 가중치 보정 시스템은 뉴런 출력 합 ∑1에 기여하는 모든 보정 가중 치들(Wi,d,1)을 변경하기 위해 사용되는 보정값 Δ1을 계산한다. P-net은 특정된 뉴런에 기여하는 모든 보정 가중치들 Wi,d,n에 대한 집단적인 보정 신호들의 그 형성과 사용에 대해 다양한 옵션들과 변형들을 허용한다. 이하에는 집단적인 보정 신호들의 형성과 사용에 대한 2개의 예시적이고 비제한적인 변형들이 있다. 변형 1 - 원하는 출력 신호와 획득된 출력 합들의 차이에 기초하여 보정 신호들의 형성 및 사용은 다음과 같다: ● 수식에 따라 뉴런 “n”에 기여하는 모든 보정 가중치들에 대해 동일한 보정값 Δn의 계산 *"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, On - 뉴런 출력 합 ∑n에 대응하는 원하는 출력 신호 S - 뉴런 “n”에 연결된 시냅스들의 개수 ● 수식에 따라 뉴런 “n”에 기여하는 모든 보정 가중치 Wi,d,n의 변경"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "변형 2 - 획득된 출력 합에 대한 원하는 출력 신호의 비에 기초한 보정 신호들의 형성 및 사용은 다음과 같다: ● 수식에 따라 뉴런 “n”에 기여하는 모든 보정 가중치들에 대해 동일한 보정값 Δn의 계산"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "● 수식에 따라 뉴런 “n”에 기여하는 모든 보정 가중치들 Wi,d,n의 변형"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "어떠한 적용가능한 변형에 의한 보정 가중치들 Wi,d,n의 변형은 원하는 출력 신호의 값 상에 그 출력 합이 수렴함 에 의해 각 뉴런에 대한 트레이닝 에러를 줄이기 위해 의도된다. 이러한 방식에서, 특정 이미지에 대한 트 레이닝 에러는 이러한 것이 0과 동일하거나 가까워질 때까지 줄어들 수 있다. 트레이닝 동안 보정 가중치 Wi,d,n의 예시적인 변형이 도 11에 도시된다. 보정 가중치 Wi,d,n 의 값은 트레이닝이 시작하기 전에 보정 가중치 범위로부터 0 ± 10%로 설정된 가중치 값으로 무작위 가중치 분포의 형태로 설정되 고, 트레이닝 이후 최종 가중치 분포에 도달한다. 집합적 신호들의 설명된 계산은 p-net에서 모든 뉴런들 을 위해 이루어진다. 하나의 트레이닝 이미지를 위한 설명된 트레이닝 절차는 모든 다른 트레이닝 이미지 들을 위해 반복될 수 있다. 일부 보정 가중치 Wi,d,n는 여러 이미지에 참여할 수 있기 때문에, 이러한 절차는 이 전에 트레이닝된 이미지들의 일부에 대해 트레이닝 에러들의 출현을 유도할 수 있다. 따라서, 다른 이미지를 트 레이닝하는 것은 이전의 이미지들을 위해 형성된 보정 가중치 Wi,d,n의 분포를 부분적으로 지장을 줄 수 있다. 그 러나, 각 시냅스는 보정 가중치 Wi,d,n의 세트를 포함하기 때문에, 가능한 트레이닝 에러를 증가시키면서 새 로운 이미지를 트레이닝하는 것은 p-net이 이미 트레이닝됐던 이미지들을 삭제하지 않는다. 게다가, 더 많 은 시냅스가 각 뉴런에 기여하고 각 시냅스에 더 많은 보정 가중치 Wi,d,n의 개수가 기여할수록, 특정 이미지에 대한 더 적은 트레이닝이 다른 이미지들에 대한 트레이닝에 영향을 미치게 된다. 전반적으로 각 트레이닝 시기는 전체 트레이닝 이미지들을 위한 전체 트레이닝 에러 및/또는 로컬 트레이닝 에 러들의 대체적인 수렴으로 종료한다. 에러들은 예를 들어, 평균 제곱 오차(Mean Squared Error (MSE)), 평균 절 대 오차(Mean Absolute Error (MAE)), 또는 표준 평균 오차 평균(Standard Error Mean (SEM))과 같은 알려진 통계적 방법을 사용하여 평가될 수 있다. 만약 전체 에러 또는 로컬 에러들의 일부가 너무 크면, 추가적인 트레 이닝 시기가 에러가 미리 결정된 에러 값에 비해 작도록 줄어들 때까지 이루어질 수 있다. 트레이닝(도 8에 도 시)을 위해 사용된 다른 이미지들 사이의 유사도의 퍼센티지를 정의하는 이미지 인식의 이전에 언급한 과정은 그 자체로 이전에 정의된 카테고리를 따라 이미지의 분류를 하는 과정이다. 예를 들어, 이미지들을 이전에 특정되지 않았던 특성 분류 또는 그룹으로 나누는 것과 같이 클러스터링을 위해, 방법의 기초적 트레이닝 알고리즘은 수정된 자가-조직 지도(Organizing Maps (SOM)) 접근으로 수정될 수 있다. 특정 입력 이미지에 대응하는 원하는 출력 이미지는 출력 뉴런 합의 최대값으로 위닝 뉴런들의 세트에 기초한 p-net을 트레이닝하는 과정에서 직접 형성될 수 있다. 도 22는 어떻게 방법의 기초 알 고리즘의 사용이 출력 뉴런 합들의 프라이머리 세트를 작성하는지를 도시하고, 세트는 또한 컨버팅되어 여러 더 큰 합들이 그들의 값을 유지하거나 또는 증가하며, 반면 모든 다른 합들은 0과 동일하게 고려된다. 이러한 출력 뉴런 합들의 변환된 세트는 원하는 출력 이미지로서 받아들여진다. 위에서 설명한 것처럼 형성된, 원하는 출력 이미지의 세트는 클러스터들 또는 그룹들을 포함한다. 그러한 원하는 출력 이미지의 세트는 선형적으로 불가분의 이미지들의 클러스터링을 허용하며, 이것은 전통적인 네트워크과 차별된다. 도 13은 설명된 접근법이 복합적인 가상의 이미지 \"고양이-자동차\"를 클러스터링하는 것을 보조하는 방법을 보여주며, 이미지들의 다른 구성들은 다른 클러스터들-고양이들과 자동차들에 대해 할당 된다. 원하는 출력 이미지들의 세트는 예를 들어 클러스터링의 결과로서 형성된 기준에 기초하여 다른 분 류, 통계적 분석, 이미지 선택을 만들기 위해 사용될 수 있다. 또한, p-net에 의해 작성된 원하는 출력 이 미지들은 다른 또는 추가적인 p-net을 위한 입력 이미지로서 사용될 수 있고, 대상 p-net을 위해 설 명된 라인들을 따라 역시 형성될 수 있다. 따라서, 형성된 원하는 출력 이미지들은 다층(multi-layer) p- net의 후속 층을 위해 사용될 수 있다.전통적인 신경망 트레이닝은 예비적으로 준비된 입력 이미지와 원하는 출력 이미지의 쌍에 기초하여 감시된 트레이닝 방법을 통해 제공된다. 동일한 범용적 방법이 p-net의 트레이닝을 위해 역시 사용되나, p- net의 증가된 트레이닝 속도는 또한 외부 트레이너로 트레이닝하는 것을 허용한다. 외부 트레이너의 역할 을 예를 들어, 개별적으로(by an individual) 또는 컴퓨터 프로그램에 의해 수행될 수 있다. 외부 트레이너로서 동작하는 것은 개체(individual)가 물리적 태스크를 수행하는데 포함되거나 게이밍 환경(gaming environment)에 서 동작할 수 있다. p-net은 특정 환경에 관한 데이터의 형태로 입력 신호를 수신하고 이에 대해 변경한다. 트레이너의 신호 반사 동작(signals reflecting actions)은 원하는 출력 이미지들로서 도입되고, p-net이 기초 알고리즘에 따라 트레이닝되도록 허용한다. 이러한 방식으로, 다양한 과정들의 모 델링이 실시간으로 p-net에 의해 작성될 수 있다. 예를 들어, p-net은 도로 환경 및 드라이버의 동작에 관한 수신하는 정보에 의해 차량을 구동하도록 트레 이닝될 수 있다. 비록 매우 다양한 치명적인 환경을 모델링하는 것이나, p-net은 어떠한 단일 운전자에 의 해 일반적으로 가능한 것에 비해 많은 다른 운전자들에 의해 트레이닝되고 보다 많은 운전 기술들을 누적할 수 있다. p-net은 0.1초 또는 이보다 빨리 특정 도로 컨디션을 평가할 수 있고, 다양한 환경에서 교통 안전을 향상시킬 수 있는 \"운전 경험\"을 축적할 수 있다. p-net은 또한 예를 들어 체스-경기 머신과 같이 컴퓨터 와 협업하도록 트레이닝될 수도 있다. p-net이 외부 트레이너에 의해 트레이닝되는 때, 트레이닝 모드로부 터 인식 모드로 및 그 반대로 쉽게 변환하는 p-net의 능력은 \"실수로부터 학습\"의 현실화를 가능하게 한다. 이러한 경우에서, 다수의 트레이닝된 p-net은 예를 들어, 기술적 과정을 컨트롤하는 것과 같은 고유 의 동작을 작성할 수 있다. 트레이너는 p-net의 동작을 컨트롤하고 필요할 때 그 동작들을 수정할 수 있다. 따라서, p-net의 추가적인 트레이닝이 제공될 수 있다. p-net의 정보 수용력은 매우 크지만, 무제한적이지는 않다. p-net이 트레이닝되는 p-net의 입력 들, 출력들 및 간격들의 개수와 같은 세트 차원과, 이미지들의 수에서의 증가로, 특정 수의 이미지들 이후, 트 레이닝 에러들의 수와 크기가 역시 증가할 것이다. 에러 작성에서 이러한 증가가 감지되는 때, 에러들의 수 및/ 또는 크기는 p-net의 크기를 증가시키는 것에 의해 줄어들 수 있고, 이것은 p-net은 뉴런들의 수와 p-net을 가로지르는 신호 간격들 \"d\" 또는 트레이닝 시기들의 사이에서 그 구성들에서의 수를 증가시키는 것을 허용하기 때문이다. p-net 확장은 새로운 뉴런을 추가하고, 새로운 입력과 시냅스들을 추 가하고, 보정 가중치 영향 계수 Ci,d,n의 분포를 변경하고, 존재하는 간격 \"d\"로 나눔으로써 제공될 수 있다. 최근 경우에서, p-net은 이미지, 패턴 및 이미지 또는 이미지들의 세트들에 내재된 연관성을 인식하는 그 능력을 보장하기 위해 트레이닝될 수 있다. 가장 간단한 경우에서 인식 과정는 방법의 부분으로서 개시된 기초 알고리즘에 따른 트레이닝 과정의 제 1 단계들을 반복한다. 특히, ● 직접 인식은 포맷 이미지들에 대해 트레이닝을 하기 위해 사용된 동일한 룰에 따라 이미지의 포맷팅으로 시 작한다; ● 이미지는 트레이닝된 p-net의 입력으로 보내지고, 분배기들은 트레이닝 동안 설정되었던 입력 신호의 값들에 대응되는 보정 가중치 Wi,d,n을 할당하고, 뉴런들은 도 8에 도시된 것처럼, 각 뉴런 합을 작성한다. ● 만약 출력 이미지를 표시하는 결과 출력 합들이 p-net이 트레이닝된 이미지들의 하나와 완전하게 충족하면, 객체의 정확한 인식이 존재한다; 그리고 ● 만약 출력 이미지가 p-net이 트레이닝된 여러 이미지들과 부분적으로 충족하면, 결과는 퍼센티지 로서 다른 이미지들과 매칭율을 보여준다. 도 13은 고양이와 차량의 이미지들의 조합에 기초하여 만들어진 복잡 한 이미지의 인식 과정동안, 출력 이미지는 특정 이미지 조합을 나타내고, 조합에 대한 각 최초 이미지들 의 기여의 퍼센티지를 나타낸다. 예를 들어, 만약 특정 사람의 여러 사진이 트레이닝에 사용되었다면, 인식된 이미지는 첫번째 사진에 대해 90%, 두번째 사진에 대해 60%, 그리고 세번째 사진에 대해 35%로 대응할 수 있다. 인식된 이미지는 다른 사람 또는 심지어 동물들의 사진들에 대해 특정한 확률로 대응할 수 있고, 이것은 사진들 간에 일부 유사성이 있음을 의미 한다. 그러나, 이러한 유사성의 확률은 더 낮기 쉽다. 이러한 가능성에 기초하여, 예를 들어, 베이즈의 정리 (Bayes' theorem)에 근거하여 인식의 신뢰성이 결정될 수 있다. p-net으로, 또한 알고리즘적 및 신경망 인식 방법들의 장점들을 조합한 멀티-스테이지 인식을 구현하는 것 이 가능하다. 이러한 멀티-스테이지 인식은 다음을 포함할 수 있다.● 여기서 \"기초 입력들\"로 지정된, 입력들의 전체가 아닌 단지 1%-10%를 이용하는 것을 통해 미리 트레이닝된 네트워크에 의해 이미지의 초기 인식. 이러한 입력들의 비율을 균일하게, 무작위로 또는 어떤 다른 분포 함수에 의해 p-net 내에서 분포될 수 있다. 예를 들어, 사진에서 사람의 인식은 다수의 다른 객체를 포함할 수 있 다; ● 더 자세한 인식을 위해 가장 유용한 정보를 주는 객체들 또는 객체들의 부분을 선택하는 것. 이러한 선택은 알고리즘적 방법에서처럼 메모리 내에서 미리 설정된 특정 객체들의 구조에 따라 제공되거나 또는 색상의 구배, 밝기 및/또는 이미지의 깊이에 따라 제공된다. 예를 들어, 초상화의 인식에서, 후속하는 인식 영역들이 선택될 수 있다: 문신, 차량 번호 또는 가옥 번호 등과 같은 어떠한 특정 구성들 뿐만 아니라 눈, 입의 가장자리, 코 형상은 또한 유사 접근을 사용하여 선택되고 인식될 수 있다; 그리고 ● 선택된 이미지들의 상세한 인식은 필요한다면 역시 가능하다. p-net과 그 트레이닝의 컴퓨터 에뮬레이션의 형성은 어느 프로그래밍 언어를 사용하여서도 위의 언급에 기 초하여 제공될 수 있다. 예를 들어, 객체 지향 프로그래밍이 사용될 수 있고, 여기서 시냅스 가중치, 보정 가중치, 분배기 및 뉴런들은 프로그래밍 객체들 또는 객체들의 분류를 나타내며, 관련성은 링크 나 메시지를 통한 객체 분류를 통해 세워지고, 상호작용의 알고리즘은 객체들의 사이와 객체 분류들의 사이에서 설정된다. p-net의 형성과 트레이닝 소프트웨어 에뮬레이션은 다음을 포함할 수 있다: 1. p-net의 형성과 트레이닝의 준비, 특히: ● 특정 태스크와 부합하는 디지털 형태로 트레이닝 입력 이미지들의 세트의 컨버전; ● 예를 들어, 주파수, 크기, 위상 또는 좌표와 같은 트레이닝을 위해 사용되기 위한 입력 신호들의 파라미터들 의 선택을 포함하는 결과적인 디지털 이미지들의 분석; 그리고 ● 트레이닝 신호를 위한 범위, 대상 범위 내의 간격들의 수, 보정 가중치 영향 계수 Ci,d,n의 분포를 설정 2. p-net 소프트웨어 에뮬레이션의 형성은 이하를 포함한다. ● p-net의 입력들의 세트의 형성. 예를 들어, 입력들의 개수는 트레이닝 입력 이미지에서 신호들의 개수 와 같을 수 있다; ● 뉴런들의 형성, 각 뉴런은 추가하는 디바이스를 나타낸다; ● 시냅스 가중치를 갖는 시냅스의 세트의 형성, 각 시냅스는 하나의 p-net과 하나의 뉴런에 연결된다; ● 각 시냅스에서 가중치 보정 블록들의 형성, 가중치 보정 블록들은 분배기들과 보정 가중치들을 포함하고, 각 보정 가중치는 다음의 특징을 갖는다: ○ 보정 가중치 입력 인덱스(i); ○ 보정 가중치 뉴런 인덱스(n); ○ 보정 가중치 간격 인덱스(d); 그리고 ○ 보정 가중치 초기값(Wi,d,n). ● 간격들과 보정 가중치들의 사이에서 연관을 지정 3. 다음을 포함하여 하나의 출력 이미지로 각 뉴런을 트레이닝: ● 다음을 포함하는 보정 가중치 영향 계수 Ci,d,n를 지정: ○ 각 입력에 의해 수신된 트레이닝 입력 이미지의 입력 신호에 대응하는 간격을 결정; 그리고 ○ 모든 시냅스들에 대한 모든 보정 가중치들에 보정 가중치 영향 계수 Ci,d,n의 크기를 결정. ● 대응하는 보정 가중치 영향 계수 Ci,d,n에 의해 곱해진 뉴런에 기여하는 모든 시냅스 가중치들의 보정 가중치 값 Wi,d,n을 더하는 것에 의해 각 뉴런 \"n\"에 대한 뉴런 출력 합(∑n)을 계산: ● 대응하는 원하는 출력 신호 On으로부터 뉴런 출력 합 ∑n의 뺄셈을 통해 트레이닝 에러(Tn)의 편차를 계산:"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "● 뉴런 \"n\"에 연결된 시냅스들 \"S\"의 개수에 의해 트레이닝 에러를 나눔으로써 뉴런 \"n\"에 기여하는 모든 보정 가중치들에 대한 균등한 보정값(Δn)을 계산:"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "● 대응하는 보정 가중치 영향 계수들 Ci,d,n에 의해 나누어진 보정값 Δn을 각 보정 가중치에 더함으로써 각 뉴 런에 기여하는 모든 보정 가중치들 Wi,d,n을 변형:"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "균등한 보정값 (Δn)을 계산하고 뉴런 \"n\"에 기여하는 모든 보정 가중치에 대한 보정 가중치 Wi,d,n를 변형하는 다른 방법은 다음을 포함할 수 있다. ● 뉴런 출력 합 ∑n에 의해 원하는 출력 이미지 On의 신호를 나눔"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "● 보정 값 Δn에 의해 보정 가중치들을 곱함에 의해 뉴런에 기여하는 보정 가중치들 Wi,d,n을 변형:"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "4. 다음을 포함하여 모든 트레이닝 이미지들을 이용하여 p-net을 트레이닝: ● 하나의 트레이닝 시기에서 포함된 모든 선택된 트레이닝 이미지들에 대해 위에서 설명한 과정을 반복; 그리 고 ● 특정 트레이닝 시기의 에러 또는 에러들을 결정하고, 이러한 에러(들)을 미리 결정된 허용 가능한 에러 레벨 과 비교하고, 트레이닝 에러들이 미리 결정된 허용 가능한 에러 레벨보다 작게 될 때까지 트레이닝 시기들을 반 복하는 것. 객체 지향 프로그래밍을 이용한 p-net의 소프트웨어 에뮬레이션의 실제 예시는 이하에서 설명되고 도 1-21 에서 도시된다. 뉴런유닛(NeuronUnit) 객체 분류의 형성은 다음의 형성을 포함할 수 있다: ● 시냅스 분류의 객체들의 세트 ● 변수를 나타내는 뉴런, 추가는 트레이닝 동안 수행된다; 그리고 ● 변수를 나타내는 계산기, 바람직한 뉴런 합의 값은 저장되고 보정 값 Δn의 계산은 트레이닝 과정 동안 수행된다. p-net을 제공하는 분류 뉴런유닛(Class NeuronUnit)은 다음을 포함할 수 있다: ● 뉴런 합의 형성; ● 원하는 합들의 설정; ● 보정값 Δn 의 계산; 그리고 ● 계산된 보정값 Δn을 보정 가중치 Wi,n,d에 더함. 객체 분류 시냅스(object class Synapse)의 형성은 다음을 포함할 수 있다: ● 보정 가중치 Wi,n,d의 설정; 그리고 ● 시냅스에 연결된 입력을 나타내는 포인터. 분류 시냅스(Class Synapse)는 다음의 기능을 수행할 수 있다: ● 보정 가중치 Wi,n,d의 초기화; ● 계수 Ci,d,n 에 의해 가중치 Wi,n,d를 곱하고; 그리고 ● 가중치 Wi,n,d의 보정 분류 입력신호(class InputSignal)의 형성은 다음을 포함할 수 있다: ● 특정 입력에 연결된 시냅스들 상의 인덱스들의 설정 ● 입력 신호의 값을 포함하는 변수 ● 가능한 최소 및 최대 입력 신호의 값들 ● 간격 \"d\"의 수; 그리고 ● 간격 길이. 분류 입력신호(Class InputSignal)는 다음의 기능을 제공할 수 있다. ● 다음을 포함하는 p-net의 형성 ○ 입력과 시냅스들의 사이의 링크 추가 및 제거; 그리고 ○ 특정 입력의 시냅스들을 위한 간격 \"d\"의 수를 설정 ● 최소 및 최대 입력 신호들의 파라미터들의 설정 ● p-net의 동작에 대한 기여: ○ 입력 신호를 설정; 및 ○ 보정 가중치 임팩트 Ci,d,n 의 상수들을 설정 객체 분류들의 세트를 포함하는 객체 분류 PNET(object class PNet)의 형성: ● 뉴런유닛(NeuronUnit); 그리고 ● 입력신호(InputSignal) 분류 PNET은 다음의 기능을 제공함: ● 입력신호 분류의 객체의 수를 설정; ● 뉴런유닛 분류의 객체의 수를 설정; 그리고 ● 객체들 뉴런유닛과 입력신호의 함수의 그룹 요청 트레이닝 과정 동안 사이클들이 형성될 수 있고: ● 사이클이 시작하기 전에 0과 동일한 뉴런 출력 합이 형성되고 ● 특정 뉴런유닛에 대해 기여하는 모든 시냅스들이 검토되고, 각 시냅스에 대해: ○ 입력 신호에 기초하여, 분배기는 보정 가중치 영향 계수들 Ci,d,n의 설정을 형성한다; ○ 시냅스들의 모든 가중치들 Wi,n,d이 검토되고, 각 가중치에 대해: ■ 가중치 Wi,n,d의 값이 대응하는 보정 가중치 영향 계수 Ci,d,n에 의해 곱해진다; ■ 곱의 결과가 뉴런 출력 합을 형성하기 위해 더해진다; ● 보정값 Δn이 계산된다; ● 예를 들어, Δn / Ci,d,n와 같이, 보정값 Δn이 보정 가중치 영향 계수 Ci,d,n에 의해 나누어진다. ● 특정 뉴런유닛에 대해 기여하는 모든 시냅스들이 검토된다. 각 시냅스에 대해, 대상 시냅스의 모 든 가중치 Wi,n,d가 검토되고, 모든 가중치에 대해 그 값은 대응되는 보정값 Δn에 대해 변경된다. p-net의 추가적인 트레이닝의 이전에 언급된 가능성은 트레이닝 과정이 속도를 빠르게 하고 그 정확도를 개선하는 것을 가능하게 하는 이미지의 인식을 통한 트레이닝과 조합을 허용한다. 서로 약간 다른 필름의 연속 적인 프레임들 상에서의 트레이닝과 같이 순차적으로 변경하는 이미지들의 세트 상의 p-net을 트레이닝 할 때, 추가적인 트레이닝은 다음을 포함할 수 있다: ● 제 1 이미지로 트레이닝 ● 다음 이미지의 인식 및 새로운 이미지와 네트워크가 초기적으로 트레이닝된 이미지 사이의 유사도의 퍼센티 지를 식별. 추가적인 트레이닝은 인식 에러가 미리 결정된 값 보다 작다면 필요하지 않다; 그리고 ● 인식 에러가 미리 결정된 값을 초과하면, 추가적인 트레이닝이 제공된다. 위의 기본적인 트레이닝 알고리즘에 의한 p-net의 트레이닝은 이미지 인식의 문제들을 해결하기에 효과적 이지만, 오버랩핑되는 이미지들에 기인한 데이터의 손실 또는 변형을 배제하지 못한다. 따라서, 메모리 목적을 위한, 비록 가능하지만, p-net의 사용은 완전히 신뢰적이지 않을 수 있다. 본 실시예는 정보의 손실 또는 변형에 반해 보호를 제공하는 p-net의 트레이닝을 설명한다. 추가적인 제한은 모든 보정 가중치 Wi,n,d가 단 지 한번에 트레이닝될 수 있는 것을 필요로하는 기초 네트워크 트레이닝 알고리즘으로 도입될 수 있다. 제 1 트 레이닝 사이클의 이후, 가중치 Wi,n,d는 고정되거나 상수로 남아있는다. 이것은 각 보정 가중치에 대해 추가적인 접근 인덱스 \"a\"를 입력함에 의해 달성될 수 있고, 이것은 트레이닝 과정 동안 대상 보정 가중치 Wi,n,d 에 대한 접근들의 수를 나타내는 앞서 설명한 인덱스이다. 앞서 언급한 것처럼, 각 보정 가중치는 \"a\"가 트레이닝 과정 동안 대상 가중치에 대한 접근의 수인 명칭 Wi,n,d,a 으로 채택될 수 있다. 예를 들어, 고정되지 않은 가장 단순한 경우에서, 가중치, a=0, 반면 언급한 기초 알고리 즘에 의해 변경되거나 고정된 가중치에 대해서는 a = 1. 게다가, 기초 알고리즘을 적용하는 동안, 고정된 값 a =1을 갖는 보정 가중치 Wi,n,d,a는 보정이 이루어지는 가중치들로부터 배제될 수 있다. 이러한 경우, 수식 [5],"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "[6] 및 [7]은 아래와 같이 변형될 수 있다. 표 1 값 기초 알고리즘 고정된 가중치들을 갖는 트레이닝 알고리즘 동일한 보정 값 - 변형 1 S0 - 대상 뉴런에 대해 기여하고 인덱스 a=0을 갖는 모든 보정 가중치들 Wi,n,d,a의 합 Ci,d,n,a 변형된 보정 가중치 - 변형 1 Wi,n,d,0은 대상 뉴런에 대해 기여하고 인덱스 a=0을 갖는 가중치 들이고, Ci,d,n,0는 대상 뉴런에 대해 기여하고 인덱스 a=0을 갖는 보정 가중치들에 대한 보정 가중치 영향 계수들변형된 보정 가중치 - 변형 2 앞의 제한은 앞에서 트레이닝된 보정 가중치 Wi,n,d,a의 보정에 부분적으로 적용될 수 있으나, 가장 중요한 이미지 를 형성하는 가중치들에만 그런 것은 아니다. 예를 들어, 단일한 사람의 초상화의 세트 상에서 트레이닝을 하는 내에서, 하나의 특정 이미지가 일차적으로 선언되고 우선권을 할당받을 수 있다. 이러한 우선권 이미지 상에서 트레이닝의 이후, 트레이닝의 과정 내에서 변경될 수 있는 모든 보정 가중치들 Wi,n,d,a 은 예를 들어 인덱스 a=1 로 고정될 수 있고, 따라서, Wi,n,d,1로서 가중치를 지정할 수 있고 동일한 사람의 다른 이미지들은 변경 가능하게 남아 있을 수 있다. 이러한 우선권은 예를 들어, 암호화 키에 사용되거나 및/또는 치명적인 수치적 데이터를 포 함하는 것과 같이 다른 이미지들을 포함할 수 있다.보정 가중치 Wi,n,d,a 에 대한 변경은 또한 완전하게 금지될 수 있으나, 인덱스 \"a\"의 성장에 대해서는 제한될 수 있다. 즉, 가중치 Wi,n,d,a의 각 후속 사용은 변경하는 그 능력 을 줄이는데 사용될 수 있다. 보다 자주 특정 보정 가중치 Wi,n,d,a가 사용될수록, 각 접근별로 더 적게 가중치가 변화하고, 따라서, 후속 이미지들 상에서의 트레이닝 동안 이전의 저장된 이미지들은 더 적게 변경되고 줄어든 변형을 겪게 된다. 예를 들어, 만약 a=0이면, 가중치 Wi,n,d,a에 대한 어떠한 변화도 가능하고; a=1인 때 가중치 에 대한 변화의 가능성은 가중치값의 ± 50%로 감소할 수 있으며, a=2일 때 변경의 가능성은 가중치값의 ± 25% 로 줄어들 수 있다. 미리 결정된 접근의 수에 도달한 이후, 인덱스 \"a\"에 의해 의미되는 것처럼, 예를 들어 a=5일 때, 가중치 Wi,n,d,a의 추가적인 변경은 금지될 수 있다. 이러한 접근은 단일한 p-net 내에서 높은 지능 및 정보의 조합 을 제공할 수 있다. 네트워크 에러 계산 메커니즘을 사용하여, 허용되는 에러들의 레벨이 설정될 수 있어서 미 리 결정된 정확도 범위 내에서 손실을 갖는 정보가 보호될 수 있고, 정확도 범위는 특정 태스크에 따라 할당될 수 있다. 다시 말하면, 시각 이미지로 동작하는 p-net에 대해, 에러는 육안에 의해 포착될 수 없는 레벨로 설정될 수 있고, 이것은 \"저장 용량에서의 증가의 중대한 \"요인\"을 제공한다. 앞의 것은 예를 들어, 영화와 같 은 시각적 정보의 높은 효율적 저장의 작성을 가능하게 할 수 있다. 선택적으로 컴퓨터 메모리를 청소하는 능력은 p-net의 연속된 하이레벨 기능에 대해 가치가 있을 수 있다. 이러한 메모리의 선택적인 청소는 저장된 정보의 나머지의 손실이나 변형없이 특정 이미지를 제고함에 의해 이 루어질 수 있다. 이러한 청소는 이하와 같이 제공될 수 있다. ● 예를 들어, 네트워크 상에 이미지를 도입하거나 각 이미지의 사용된 보정 가중치들의 리스트를 컴파일링함에 의해 이미지 형성에 참여한 모든 보정 가중치들 Wi,n,d,a 의 식별; ● 각 보정 가중치들 Wi,n,d,a에 대해 인덱스 \"a\"를 축소; 그리고 ● 인덱스 \"a\"가 0으로 축소된 때 대상 가중치에 대해 보정 가중치들 Wi,n,d,a 를 0 또는 가능한 값들의 범위의 중 간에 근접한 임의의 값으로 대체. 인덱스 \"a\"의 축소의 적절한 순서 및 연속은 이미지의 시퀀스 내에서 숨겨진 강한 패턴들을 식별하도록 실험적 으로 선택될 수 있다. 예를 들어, \"a\"가 0 값으로 도달할 때까지 트레이닝 동안 p-net으로 도입된 모든 100개의 이미지들에 대해, 하나의 카운트에 의해 인덱스 \"a\"의 축소가 있을 수 있다. 이러한 경우, \"a\"의 값은 새로운 이미지들의 도입에 대응하여 증가할 수 있다. \"a\"의 증가와 축소 사이의 경쟁은 무작위 변경들이 메모리 로부터 점차적으로 제거되는 상황을 유도할 수 있고, 여러번 사용되고 확인되어 왔던 보정 가중치들 Wi,n,d,a 은 저장될 수 있다. p-net이 예를 들어, 동일하거나 유사한 환경과 같은 유사한 속성을 갖는 많은 이미지들 상에서 트레이닝되는 때, 자주 사용된 보정 가중치들 Wi,n,d,a 은 끊임없이 이러한 영역들에서 그들의 값과 정보를 확인하고 매우 안정하게 된다. 또한, 무작위 노이즈는 점차적으로 사라질 것이다. 다시 말하면, 인덱스 \"a\"에서 점차적인 감소를 갖는 p-net은 효과적인 노이즈 플터로서 제공될 수 있다. 정보의 손실이 없는 p-net 트레이닝의 설명한 실시예들은 높은 용량 및 신뢰성을 갖는 p-net 메모리를 작 성하는 것을 허용한다. 이러한 메모리는 심지어 \"캐시 메모리\" 시스템에 비해 더 높은 속도를 제공하는 대용량 의 고속 컴퓨터 메모리로서 사용될 수 있으나, \"캐시 메모리\" 시스템을 갖는 종래와 같이 컴퓨터 비용과 복잡도를 증가시키지 않을 것이다. 공개된 데이터에 따라, 일반적으로, 신경망으로 영화를 녹화하는 동안, 메모리는 녹화 품질의 중대한 손실 없이 수십 또는 수백번 압축될 수 있다. 다시 말하면, 신경망은 매우 효과적인 기록 프로그램으로서 동작할 수 있다. p-net의 고속 트레이닝 능력을 갖는 신경망의 이러한 능력을 조합하는 것 은 고속 데이터 전송 시스템, 고용량을 갖는 메모리, 예를 들어 코덱스(codex)와 같은 고속 해독 프로그램 멀티 미디어 파일의 작성을 허용할 것이다. p-net에서 데이터가 보정 가중치들 Wi,n,d,a의 세트로서 저장되는 사실에 기인하여, 존재하는 방법들을 통하 고 동일한 네트워크 및 키의 사용을 하지 않는 코드 레코딩, 디코딩 또는 p-net에 대한 미인가된 접근은 허용되 지 않는다. 따라서, p-net은 데이터 보호의 상당한 정도를 제공할 수 있다. 또한, 종래의 컴퓨터 메모리와 달리, 다른 구성들은 잃어버린 기능들을 보상하기 때문에 p-net의 개별적인 저장 소자들에 대한 손상은 중 요하지 않은 해로운 효과를 나타낸다. 이미지 인식 과정에서, 사용된 이미지의 내재된 패턴들은 하나 이상의 구 성들에 대한 손상의 결과로서 실제적으로 왜곡되지 않는다. 앞의 것은 컴퓨터의 신뢰성을 획기적으로 향상시키 고, 평상시 컨디션이 결함이 있는 것으로 고려될 수 있는 특정 메모리 블록을 사용하는 것을 허용한다. 또한, 이러한 메모리의 타입은 p-net에서 치명적인 바이트들에 대해 영구적인 어드레스들의 부재로 인해 해커 공 격에 대해 덜 취약하고, 다양한 컴퓨터 바이러스들에 의한 이러한 시스템의 공격에 대해 영향을 받지 않도록 할 수 있다. 트레이닝에서 사용된 다른 이미지들 사이의 유사도의 퍼센티지의 결정을 갖는 이미지 인식의 이전에 언급된 과 정은 위에서 언급한 것처럼 이전에 정의된 카테고리들에 따라 이미지 분류의 과정으로서 채택될 수 있다. 미리 정의되지 않은 속성 또는 그룹으로 이미지들을 분배하는 클러스터링을 위해 기초적인 트레이닝 과정은 변경될 수 있다. 본 실시예는 다음을 포함할 수 있다: ● 준비된 출력 이미지들을 포함하지 않고 트레이닝에 대한 입력 이미지들의 세트의 준비 ● 기초 알고리즘에 따라 이루어진 대로 뉴런 출력 합들의 형성을 갖는 네트워크의 형성 및 트레이닝 ● 예를 들어, 코호넨 네트워크(Kohonen network)와 유사하게 조직화될 수 있는 위너 출력(winner output), 위 너 출력들의 그룹과 같은 최대 출력 합을 갖는 출력의 출력 이미지를 도출하는 선택 ● 위너 출력 또는 위너 출력들의 그룹에서 최대 값을 수신하는 원하는 출력 이미지의 작성. 이와 동시에: ○ 선택된 위너 출력들의 수가 예를 들어 1 내지 10의 범위에서 미리 결정되거나, 위너 출력들이 \"최대 뉴런 합의 N%보다 작은 것이 없는\" 률에 따라 선택될 수 있고, 여기서 \"N\"은 예를 들어 90-100%일 수 있고; 그리고 ○ 모든 다른 출력들은 0과 동일하게 설정될 수 있다. ● 작성된 원하는 출력 이미지를 사용한 기초 알고리즘에 따른 트레이닝, 도 13; 그리고 ● 다른 위너들 또는 위너 그룹의 각 이미지에 대한 형성과 함께 다른 이미지에 대한 모든 절차들을 반복. 위의 방식에서 형성된 원하는 출력 이미지들의 세트는 다수의 입력 이미지들이 자연스럽게 분리되도록 클러스터 들 또는 그룹들로 설명하는데 사용될 수 있다. 이러한 원하는 출력 이미지들의 세트는 수립된 기준에 따라 및 통계적 분석에서 이미지들을 선택하는 것과 같이 다양한 분류들을 작성하는데 사용될 수 있다. 앞의 것은 또한 앞서 언급한 입력 및 출력 이미지들의 전환을 위해 사용될 수 있다. 다시 말하면, 원하는 출력 이미지들은 예를 들어, 추가적인 네트워크와 같은 다른 것을 위해 입력 이미지들로서 사용될 수 있고, 추가적인 네트워크의 출력 은 컴퓨터 입력을 위해 적합한 어떠한 형태로 표현된 이미지일 수 있다. p-net에서, 설명된 앞의 알고리즘으로 트레이닝의 단일 사이클의 이후, 원하는 출력 이미지들은 작은 출력 합 변화로 작성될 수 있고, 이것은 트레이닝 과정을 느리게 할 수 있고 또한 그 정확도를 줄일 수 있다. 도 21 에 도시된 것처럼, p-net의 트레이닝을 개선하기 위해, 포인트들의 초기 변화는 인위적으로 증가하거나 연 장될 수 있어서, 포인트들의 크기의 변화는 예를 들어 -50에서 +50까지 가능한 출력값들의 전체 범위를 커버할 수 있다. 이러한 포인트들의 초기 변화의 연장은 선형이거나 비선형일 수 있다. 상황은 예를 들어, 노이즈의 징후와 같이 특정 출력의 최대값이 이상점(outlier)이거나 또는 실수인 경우, 영향 을 받을 수 있다. 이러한 것들은 작은 신호들의 크기에 의해 둘러싸인 최대값의 출현에 의해 분명해질 수 있다. 위닝 출력들이 선택된 때, 작은 신호값들은 위너로서 다른 큰 신호들에 의해 둘러싸인 가장 큰 신호들을 선택하 는 것을 통해 무시될 수 있다. 이러한 목적을 위해, 중요도 표집(importance sampling)과 같은 분산 축소의 알 려진 통계적 기법들이 사용될 수 있다. 이러한 접근은 기본적이고 가치있는 패턴들을 유지하면서 노이즈를 제거하는 것을 허용할 수 있다. 위너 그룹들의 작성은 예를 들어 도 13에서 도시된 것처럼, 하나의 이상의 클러스터 에 관련된 이미지와 같은 선형적으로 불가분인 이미지들의 클러스터링을 가능하게 한다. 앞의 것은 정확도에서 의 중대한 개선을 제공할 수 있고 클러스터링 에러들의 수를 줄일 수 있다. p-net의 과정에서 보정해야 할 통상적인 에러들은 다음과 같다: 표 2 신경망의 통상적인 에러 p-net 보정의 방법 트레이닝 이미지의 선택에서의 에러. 예를 들어, 사람 이미지의 세트가 고양이의 이미지를 포함함.대응되는 원하는 출력 이미지를 지우거나 그 입증 (demonstration)을 제한 트레이닝 동안 보정되지 않은 네트워크 에러. 예를 들어, 네트워크가 객체의 일부 특징들을 나눌 수 없기 때문에 특정 이미지가 부정확하게 인식됨(선 형적 불가분의 영향)에러가 감지된 이후 p-net의 추가적인 트레이닝; 추가적인 원하는 출력 이미지의 도입 네트워크 정보 용량의 한계에 도달함에 기인하여 정확도가 감소 P-net 확장 에러 보정은 또한 외부 트레이너와 함께 트레이닝할 때 상술한 알고리즘의 도움으로 가능하다. p-net의 하드웨어 부분은 디지털, 아날로그 또는 결합된 디지털-아날로그 마이크로칩으로 구비될 수 있다. 대표적인 p-net 마이크로칩은 정보의 저장 및 처리 모두에 사용될 수 있다. p-net 마이크로칩은 다양 한 가변 저항들, 전계효과트랜지스터들, 멤리스터들, 커패시터들, 스위칭 소자들, 전압 발생기들, 비선형 광-셀 들 등을 기반으로 할 수 있다. 가변 저항들은 시냅스 가중치들 및/또는 보정 가중치들로서 이용될 수 있다. 다수의 이러한 저항들은 병렬, 직렬 또는 직렬 병렬로 연결될 수 있다. 각각의 저항들이 병렬로 연결된 경우, 신호는 전류 값에 의해 코딩될 수 있는데, 이는 전류의 자동화된 아날로그 합산을 용이하게 할 수 있다. 양성 또는 음성 신호들을 얻기 위해, 흥분성 및 억제성인 두 세트의 저항들이 각 시냅스 상에 제공될 수 있다. 그러한 하드웨어 구조에서, 억제성 신호들은 흥분성 신호로부터 감산될 수 있다. 각각의 보정 가중치는 멤리스터형 디바이스(멤리스터)로서 구현될 수 있다. 당업자에게 이해되는 바와 같 이, 멤리스터는 회로 내의 전류 또는 전위 또는 전하에 의해 제어되는 저항을 갖는 가변 저항이다. 적절한 멤리 스터 기능은 실제 멤리스터 디바이스, 그것의 소프트웨어 또는 물리적 에뮬레이션을 통해 수행될 수 있다. 저전 압 전위에서의 p-net의 동작에서, 멤리스터는 간단한 저항으로서 동작할 수 있다. 트레인 모드 동안, 멤리 스터의 저항은, 예를 들어, 강한 전압 펄스에 의해 변화될 수 있다. 멤리스터의 값 변화(저항의 증가 또는 감소)는 전압의 극성에 따라 달라질 수 있지만, 값 변화의 크기는 전압 펄스의 크기에 따라 달라질 수 있다. 도 24는 p-net(100A)으로 라벨링된 p-net의 일 실시예를 도시하는데, 이는 후술될 특정 소자들(요소들)을 갖는 것 이외에, 앞에서 설명한 p-net와 모든면에서 유사하다. p-net(100A)에서, 각각의 보정 가중치(11 2)는 특정 보정 가중치의 각 가중치를 유지하는 메모리 소자에 의해 설정된다. p-net(100A)에서, 가중치 보정 계산기는 대응하는 메모리 소자에 의해 설정되는 보정 가중치들의 각각의 보정 가중치를 변경(수정)하도록 구성될 수 있다. p-net(100A)의 다른 실시예와 일치하여, 보정 가중치는 결정된 편차 를 사용하여 대응하는 메모리 소자에서 설정된다. 도 25에 도시된 p-net(100A)의 동작동안, 모든 뉴 런 각 출력은 p-net(100A)의 동작 출력 신호를 설정하도록 각각의 뉴런 합을 제공한다. 동 작 출력 신호는 동작 출력 이미지의 일부 또는 전부를 나타내는 신호 값을 갖는다. p-net(100A)의 트레이닝동안, 가중치 보정 계산기는 출력 이미지의 일부 또는 전체를 나타내는 원하 는 출력 신호를 수신하고, 원하는 출력 신호의 값으로부터 뉴런 합의 편차를 결정하고, 그 리고 결정된 편차를 사용하여 대응하는 메모리 소자에 의해 설정된 각각의 보정 가중치를 변경한다. 또한, 뉴런 합을 결정하기 위해 대응하는 메모리 소자들에 의해 설정된 보정 가중치들의 변경된 보정 가중치 값 들을 합산하면 원하는 출력 신호 값 으로부터 뉴런 합의 편차를 최소화시킬 것이다. 원하는 출력 신 호 값으로부터 뉴런 합의 편차를 최소화하는 것이 p-net(100A)을 트레이닝시키는데 사용된다. 도 24의 가상선으로 도시된 바와 같이, 임의의 개시된 실시예의 트레이닝된 p-net(100A)는 대응하는 부가적인 원하는 출력 신호와 함께 값을 갖는 보조 입력 신호만을 사용하여 보충 트레이닝을 수신하도록 구성 될 수 있다. 즉, 이전에 트레이닝된 p-net(100A)은 원래의 입력 신호들 및 p-net(100A)을 초기에 트레이닝 시키는 데 사용된 원하는 출력 신호들의 일부 또는 전부로 재트레이닝되지 않고 보충 트레이닝을 수신할수 있다. p-net(100A) 내의 다수의 시냅스들 각각은 각각의 메모리 소자들에 의해 설정된 하나 이상 의 추가 보정 가중치들을 수용하도록 구성될 수 있다. 이러한 추가적인 보정 가중치들은 트레이닝동 안 또는 p-net(100A)의 보충 트레이닝 이전에 시냅스에 추가될 수 있다. 그러한 추가적인 보정 가중치들은 p-net(100A)을 트레이닝시키고 동작시키기 위해 이용 가능한 다수의 메모리 소자의 수를 확장하는데 사용 될 수 있다. p-net(100A)은 p-net(100A)의 트레이닝동안 또는 후에 각각의 시냅스들로부터 각각의 메모리 소자들 에 의해 설정된 하나 이상의 보정 가중치들을 제거하도록 구성될 수도 있다. 일부 보정 가중치들의 제거는 신경망이 신경망을 동작시키는데 필요한 수의 메모리 소자만을 보유하도록 허용할 수 있다. 보정 가중치 들을 제거하는 이러한 능력은 p-net을 더욱 소형화하여 트레이닝 및 후속 동작에보다 효율적으로 하기 위 한 것이다. p-net(100A)은 또한 p-net의 트레이닝 이전 또는 트레이닝 중에 각각의 추가 뉴런 출력들 및 추가 시냅스들과 함께 추가 입력들, 추가 뉴런들을 허용하도록 구성될 수 있음으로써, p-net의 동작 파라미터들을 확장한다. 이러한 p-net(100A) 에의 추가들은 용량, 출력의 정밀도 및 p-net에 의해 처리될 수 있는 태스크의 개수와 같은 능력을 향상시킬 수 있다. p-net(100A)은 초기 트레이닝 또는 보충 트레이닝 이전, 도중 또는 후에 각각의 추가 뉴런 출력들 및 시냅 스들과 함께 임의의 수의 사용되지 않은 입력들, 뉴런들을 제거하도록 추가로 구성 될 수 있다. 이와 같이 사용되지 않는 p-net(100A)의 요소를 제거하는 능력은 구조를 단순화하고 p-net의 출력 품질 저하없 이 p-net의 동작 파라미터를 변경하는 즉, p-net을 압축(condense )하는 것을 목적으로 한다. 도 26에 도시된 바와 같이, 저항, 임피던스, 용량, 자기장, 인덕션, 또는 전계 강도와 같은 각각의 보정 가중치 의 가중치 값을 정의하도록 구성된 전기적 및/또는 자기적 특성을 특징으로 하는 전기 부품 또는 디바이스 에 의해 각각의 메모리 소자가 설정될 수 있다. 이러한 전기 디바이스는, 예를 들어, 멤리스터 (도 26 내지도 28에 도시됨), 저항(도 29 내지 도 32에 도시됨), 트랜지스터, 커패시터(도 29 내지 도 32에 도 시됨), 전계효과트랜지스터, 광저항 또는 광 의존성 저항(LDR), 자성의존성저항(MDR), 또는 메미스터(memisto r)를 포함할 수있다. 당업자에 의해 이해되는 바와 같이, 메미스터는 논리 동작을 수행하고 정보를 저장할 수 있는 메모리를 갖는 저항이며, 일반적으로 멤리스터의 3단자 구현이다. 메모리 소자의 이러한 실시예에서, 각각의 전기 디바이스의 각각의 전기적 및/또는 자기적 특성은 p- net(100A)의 트레이닝 동안 변경되도록 구성될 수 있다. 또한, 메모리 소자의 전기 디바이스 실시예 를 사용하는 p-net(100A)에서, 가중치 보정 계산기는 p-net(100A)에 의해 채용된 대응하는 디바이스들의 각각의 전기적 및/또는 자기적 특성을 변화시킴으로써 각각의 보정 가중치들의 값들을 변경할 수 있다. 각 각의 전기 디바이스는 또한 전술한 바와 같이 p-net(100A)의 트레이닝동안 변경된 각각의 보정 가중치 의 값에 대응하는 전기적 및/또는 자기적 특성을 유지 또는 보유하도록 구성될 수 있으며, 트레이닝 이후 p-net의 동작 중에 사용될 수 있다. 적절한 멤리스터의 특정 실시예들은 소프트웨어 또는 전기 회로 기능적 표현물 또는 그의 균등물 뿐만 아니라 디바이스의 공지된 물리적 표현물일 수 있다. 도 26-28은 그러한 물리적 멤리스터들을 사용하는 대표적인 p- net(100A)의 실시예를 도시한 것이다. 도 26에 도시된 바와 같이, 각각의 입력은 아날로그 입력 신호(10 4)를 수신하도록 구성되되, 이미지 센서, 감광 소자들 또는 마이크로폰들, 디지털-아날로그 컨버터 등과 같은 외부 소스로부의 입력 신호들은 전압들 V1, V2 ... Vm으로 표현된다. 모든 입력 신호들은 함께 대응하는 입력 이미지를 일반적으로 기술한다. 각각의 메모리 소자는 또한 전기 저항들을 갖는 블록에 의해 설정될 수 있다. 전기 저항들(16 4)을 갖는 이러한 블록은 선택기 디바이스를 포함할 수 있다. 선택기 디바이스는 전술한 바와 같이 각각의 보정 가중치를 설정하기 위해 원하는 출력 신호의 값으로부터 뉴런 합의 결정된 편 차를 사용하여 블록으로부터 하나 이상의 전기 저항들을 선택하도록 구성된다. 또한, 전기 저항 들을 갖는 블록에 의해 설정된 각각의 메모리 소자는 전기 커패시터들을 또한 포함할 수 있다. 다른 말로, 각각의 메모리 소자는 전기적 커패시터들 뿐만 아니라 전기 저항들을 갖는 블 록에 의해 설정될 수 있다. 이러한 경우에, 선택기 디바이스는 각각의 보정 가중치를 설정하기 위해 결정된 편차를 사용하여, 전기 저항들 뿐만 아니라 커패시터들을 선택하도록 부가적으로 구성될 수 있다. 도 24 및 도 25에 도시된 p-net(100A)의 실시예 각각은 아날로그, 디지털 및 디지털-아날로그 신경망 중 하나로 구성될 수 있다. p-net(100A)의 그러한 실시예에서, 임의의 다수의 입력들, 다수의 시냅스들, 메모리소자들, 분배기들의 세트, 뉴런들의 세트, 가중치 보정 계산기, 및 원하는 출력 신호들 은 아날로그, 디지털 및 디지털-아날로그 포맷으로 동작하도록 구성될 수 있다. 도 26은 원하는 출력 신호 의 값으로부터 뉴런 합의 편차를 결정하는 것으로 결론을 내리는 트레이닝의 제 1 단계에서의 p-net(100A)을 도시하고, 도 27은 제 2 단계에서 p-net(100A)이 전기 디바이스들에 의해 설정된 보정 가중 치들에 대한 보정 신호들의 형성으로 결론나는 것을 도시한다. 도 26에 도시된 바와 같이, 각각의 시 냅스는 다수의 입력들 중 하나에 연결되고, 보정 가중치들로서 기능하는, 멤리스터들로 도시된, 다수의 전기 디바이스들을 포함한다. 도 26에 도시된 p-net(100A)는 또한 분배기들 세트를 포함한다. 각각의 분배기는 적절한 보정 가중치 를 설정하는, 멤리스터들로 구성된, 전기 디바이스들의 세트를 통해 각각의 입력 신호를 수신하 기 위해 다수의 입력들 중 하나에 동작 가능하게 연결된다. 또한, 각각의 분배기는 입력 전압과 상관 하여 이용 가능한 다수의 보정 가중치들로부터 멤리스터들에 의해 구현된 하나 이상의 보정 가중치들을 선 택하도록 구성된다. 도 28은 쌍둥이 평행 분기들로 배열된 멤리스터들로서 구성된 전기 디바이스들을 사용 하는 p-net(100A)을 도시한다. 상기 언급된 p-net(100A)의 구성을 위한 다른 해결책과 관련하여, 도 29-31은 p- net(100A)에서 적절한 저항을 정의하기 위한 공통 저항으로 구성된 전기 디바이스들을 도시한다; 반면 도 32는 p-net(100A)에서 임피던스를 정의하도록 구성된 전기 디바이스들을 도시한다. 아날로그 또는 아날로그-디지털 망으로서 구성된 p-net(100A)에서, 각각의 입력은 아날로그 또는 디지털 입력 신호를 수신하도록 구성되되, 입력 신호들은 도 24 및 도 25에서 I1, I2 ... Im으로 표시된다. 각각의 입력 신호 I1, I2 ... Im은 입력 이미지의 일부 아날로그 특성(들 값), 예를 들어, 크기, 주파수, 위상, 신 호 편광 각도 등을 나타낸다. 아날로그 p-net(100A)에서, 각각의 입력 신호는 입력 아날로그 값을 갖되, 함께 다수의 입력 신호들은 일반적으로 아날로그 입력 이미지를 나타낸다. 아날로그 망으로서 구성된 p-net(100A)에서, 각각의 뉴런은 직렬 또는 병렬 통신 채널, 예를 들어, 전기 와이어 또는 직렬 또는 병렬 버스 중 어느 하나에 의해 설정될 수 있다. 통신 채널의 대표적인 버스 실시예는 당업자가 이해하는 바와 같이, 병렬 및 비트 직렬 연결들 모두를 사용할 수 있다. 예를 들어, 대응 아날로그 신호들이 전류를 통해 제공되는 경우, 통신 채널은 직렬 전류 버스일 수 있고, 대응 아날로그 신호들이 각각의 보정 가중치 상의 전위를 통해 제공되는 경우, 대표적인 통신 채널은 병렬 버스일 수 있다. 보정 가중치의 아날로그 실시예가 도 26-32에 도시되어있다. 각각의 아날로그 보정 가중치는 특정 보 정 가중치의 각 가중치를 유지하고 순방향 신호들 (입력에서 출력으로) 또는 역방향 신호들 (출력에서 입력 으로) 상에 작용하여 변경하는 메모리 소자, 또는 제어 신호를 통해 흐르는 추가적인 제어 신호에 의해 정 의된다. 또한, 각각의 아날로그 보정 가중치는, 예를 들어, 이미지 인식 동안 p-net(100A)의 실제 동작 동 안 그 각각의 가중치를 유지할 수 있다. 또한, 각각의 아날로그 보정 가중치의 값은 순방향 신호들 또는 역방향 신호들 또는 추가 제어 신호 중 어느 하나를 통해 p-net(100A)의 트레이닝 중에 변경될 수 있다. 결 과적으로, p-net(100A)의 아날로그 실시예는 다양한 컴퓨터 애플리케이션의 지원을 허용하는 마이크로칩의 생성 을 가능하게 할 수 있다. 또한, 전체 p-net(100A)은 마이크로 칩, 비디오 카드 등과 같은 메모리를 갖는 전자 디바이스에 프로그래밍될 수 있다. 따라서, 각 메모리 소자의 적절한 실시예는 또한 대상 전자 디바이스의 메모리에 저장될 것이다. 상기와 일치하여, 아날로그 p-net(100A) 내의 각각의 전기 디바이스는 p-net의 트레이닝 후에 각각의 보정 가중치의 변경된 값에 대응하는 전기적 및/또는 자기적 특성을 복원하도록 구성될 수 있다. 가중치 보정 계산기는 원하는 출력 신호의 값으로부터 뉴런 합의 결정된 편차를 나타내는 하나 이상의 보정 신호를 생성하도록 구성될 수 있다. 또한, 생성된 보정 신호들 각각은 적어도 하나의 전기 디바이스의 전 기적 및/또는 자기적 특성, 즉 변경되는 각각의 디바이스에 사용되는 개별 보정 신호를, 변경하는데 사용될 수 있다. 또한, 가중치 보정 계산기는 각각의 전기 디바이스의 전기적 및/또는 자기적 특성을 변화시키 는 데 사용되는 단일 보정 신호를 생성하도록 구성될 수 있다. 즉, 변경되는 모든 전기 디바이스에 대해 하나의 보정 신호가 사용될 수 있다. 가중치 보정 계산기의 특정 실시예들은 프로그래밍된 소프트웨어로서 p-net(100A)에 내장되거나 또는 외부 디바이스들 또는 접근 가능한 컴퓨터 프로그램을 통해 설정될 수 있다. 예를 들어, 가중치 보정 계산기는 한 세트의 차동 증폭기들로서 설정될 수 있다. 전체적인 개시와 일치하여, 이러한 각각의 차동 증폭기 는 원하는 출력 신호의 값으로부터 뉴런 합의 결정된 편차를 나타내는 각각의 보정 신호를생성하도록 구성될 수 있다. 각 전기 디바이스는 p-net(100A)의 트레이닝이 완료된 후에 각각의 보정 가중 치의 변경된 값에 대응하는 전기적 및/또는 자기적 특성을 유지하도록 구성될 수 있다. 또한, p- net(100A)는 p-net의 동작 동안, 즉 트레이닝이 완료된 후에도 유지 된 전기적 및/또는 자기적 특성을 사용하도 록 구성될 수 있다. 이러한 p-net(100A)의 구조는 보정 가중치의 병렬 또는 일괄 트레이닝을 용이하게 하 고, 이에 의해 전술한 종래의 신경망과 비교하여 p-net을 트레이닝시키는데 필요한 시간의 상당한 감소를 가능 하게한다. p-net(100A)의 각각의 분배기는 단일 입력 신호를 취하여 하나 이상의 다중 데이터-출력-라인들을 선 택하는 아날로그, 디지털 또는 아날로그-디지털 디바이스로서 구성될 수 있는데, 이는 단일 입력, 즉, 디멀티플 렉서에 연결된다. 이러한 디멀티플렉서는 수신된 입력 신호에 응답하여 다수의 보정 가중치들로 부터 하나 이상의 보정 가중치를 선택하도록 구성될 수 있다. 각각의 분배기는 수신된 입력을 바이너 리 코드로 변환하고 바이너리 코드와 관련하여 다수의 보정 가중치들로부터 하나 이상의 보정 가중치들을 선택하도록 구성될 수 있다. 도 33은 (도 25에 도시된) 유틸리티 신경망(100B)을 동작시키는 방법을 도시한다. 방법은 도 2 내지 22 및 24 내지 32에 관한 상기 개시 내용에 따라 동작한다. 방법은 프레임에서 시작하며, 이 방법은 유틸리티 신경망(100B)을 제공하는 단계를 포함한다. 프레임 이후, 방법은 프레임으로 진행한다. 프 레임에서, 방법은 트레이닝 동안 p-net(100A)과 같은 별도의 유사 신경망에 의해 설정된 보정 가중치(11 2)의 변경된 값을 사용하여 유틸리티 신경망(100B)을 통한 데이터 프로세싱을 포함한다. 유사한 p-net(100A)의 트레이닝은 도 24와 관련하여 전술한 바와 같은 보충 트레이닝을 포함할 수 있다. 유틸리티 신경망(100B) 및 트레이닝된 별도의 p-net(100A)는, 예를 들어 도 25에 나타낸 바와 같이 매칭 신경망 구조를 가짐으로써 유사하게 만들어질 수 있어서, 유틸리티 신경망(100B)은 유틸리티 신경망을 트레이닝하도록 가중치 보정 계산기 및 대응 능력을 배제할 수 있다. 따라서, 매칭 신경망 구조는 동일한 개수의 입력들 , 보정 가중들, 분배기들, 뉴런들, 뉴런 출력들 및 시냅스들을 포함할 수 있 다. 환언하면, 보정된 보정 가중치들을 설정함으로써 트레이닝될 p-net(100A)의 능력인 가장 큰 기능적 차이를 갖는, 망의 동작 파라미터들을 설정하는 모든 특징에 대해 p-net(100A)과 유틸리티 신경망(110B)은 실질적으로 동일할 수 있다. 유사 p-net(100A) 및 유틸리티 신경망(100B)은 각각 예를 들면 아날로그 및/또는 디지털 포맷 뿐만 아니라 하드웨어 및/또는 소프트웨어의 다양한 형태로 구별되는 구성들로 구현될 수 있어, 유틸리티 신경 망 및 유사한 p-net이 다른 캐리어들로 대표된다. 그러한 경우에, 트랜스레이터(도시되지 않음)는 변경된 보정 가중치로 데이터를 변환하거나 해석하기 위해 사용될 수 있다. 유틸리티 신경망(100B) 및 트레이닝된 별도의 p-net(100A) 각각에서, 각각의 보정 가중치는 메모리 소자 에 의해 설정될 수 있다. 구체적으로는, p-net(100A)에서, 메모리 소자는 p-net의 트레이닝에 후속하 는 보정 가중치에 대응하는 각각의 수정된 가중치를 보유할 수 있다. 유틸리티 신경망(100B)에 제공되는 각각의 입력 이미지는 도 2와 관련한 설명과 동일한 I1, I2 ... Im으로 표현된 결합 입력 신호들로 표 현될 수 있다. 도 2와 관련하여 추가로 논의된 바와 같이, 각각의 입력 신호 I1, I2, ..., Im은 대응 입력 이미 지의 몇몇 특성 (들)의 값을 나타낸다. 프레임에서 방법은 프레임으로 진행한다. 프레임에서 방법은 도 25와 관련하여 설명된 바와 같 이 변경된 보정 가중치를 사용하여 동작 출력 신호를 설정하는 단계를 포함한다. 따라서, 유틸리티 신경망(100B)을 통한 입력 이미지와 같은 입력 데이터의 프로세싱은 그러한 데이터의 인식 및 유틸리티 신 경망의 연산 출력 신호와 함께 완성될 수 있다. 그런 다음 유틸리티 신경망(100B)의 동작 출력 신호 는 방법을 완료하기 위해 유틸리티 신경망 자체 또는 프레임의 일부로서 유틸리티 망의 오퍼레이터에 의해 해석되거나 디코딩될 수 있다. p-net(100A)에서 변경된 보정 가중치들의 설정은 도 23과 관련하여 기술된 바와 같이 p-net를 트레이닝하는 방법에 따라 달성될 수 있다. 도 34 및 도 35는 p-net(100B)으로 라벨링된 p-net의 일 실시예를 도시하며, 이는 후술될 특정 소자(요 소)들을 갖는 것 이외에는 전술한 p-net과 모든 면에서 유사하다. 또한, 도 34 및 도 35에 도시된 p- net(100B)은 어레이 구조를 사용하여 다른 이미지들의 후속 인식을 위해 선택된 이미지들로 동작하도록, 즉, 트 레이닝되도록 구성된다. 본 명세서에서 사용되는 용어 \"이미지\"는 신경망에 의해 처리되거나 생성되도록 수신된 임의의 유형의 정보 또는 데이터를 나타내는 것으로 의도된다. 도 36에서, 트레이닝된 p-net(100B)는 도면 부호100C를 통해 지정된다. p-net(100B)이 트레이닝되는 경우, 입력 이미지는 트레이닝 이미지로서 정의되고, 트레이닝된 p-net(100C)에서는 입력 이미지가 인식되도록 의도된다. p-net(100B)의 트레이닝 동안, 즉, 다 수의 입력에 의해 수신된 후, 트레이닝 이미지들이 트레이닝 입력 값 어레이로서 다수의 입력들(10 2)에 의해 수신되거나, 또는 트레이닝 입력 값 어레이로서 코드화(코딩)된다. 다른 실시예들과 유사하게, 도 34-36에 도시된 p-net(100B) 및 p-net(100C) 각각은 또한 시냅스 를 포함 하되, 각각의 시냅스는 다수의 입력들 중 하나에 연결되고, 다수의 보정 가중치들을 포함할 수 있고, 또한 시냅스 가중치를 포함할 수 있다. 모든 시냅스의 보정 가중치는 즉, 보정 가중치 어 레이(119A)의 형태로 구성된다. 따라서, 도 34 내지 도 36에서, 보정 가중치 어레이(119A)는 점선 박스(119A) 내의 모든 보정 가중치를 포함한다. p-net(100B)은 한 세트의 분배기들을 포함할 수도 있다. 이러한 실시 예에서, 각각의 분배기는 각각의 입력 신호를 수신하기 위해 다수의 입력들 중 하나에 동 작 가능하게 연결된다. 보정 가중치 어레이(119A)를 포함하는 p-net(100B)의 실시예는 또한 뉴런 유닛의 구성 요소들 중 일부를 유지하면서 별개의 뉴런 유닛이 부재함으로써 특징될 수 있다. 이전에 기술된 p-net 및 p-net(100A)과 유사하게, p-net(100B)은 추가로 한 세트의 뉴런을 포함하며, 이하에서 상세히 설명되는 동작들을 실행하기위한 수단이다. 각각의 뉴런은 또한 적어도 하나의 출력을 가지며 하나의 시냅스를 통해 다수의 입력들 중 적어도 하나와 연결된다. 각각의 뉴런 은 마찬가지로 각각의 뉴런에 연결된 각 시냅스로부터 선택된 보정 가중치의 값을 합산하 여 뉴런 합 어레이(120A)를 생성하여 출력하고, 그렇치 않으면 ∑n으로 지정된다. 본 실시예에서, 도 34 내지 도 36에 도시된 바와 같이, 별개의 분배기가 주어진 입력의 각 시냅스에 유사하게 사용될 수 있 다. 대안으로, 하나의 분배기가 모든 시냅스(도시 생략)에 대해 사용될 수 있다. p-net(100B)의 형성 또는 설정 동안, 모든 보정 가중치는 초기 값으로 할당되는데, 이는 도 35에 도시된 바와 같이, p-net 트레이닝의 프 로세스 동안 변경될 수 있다. 보정 가중치의 초기 값은 사전 결정된 수학 함수의 도움으로 계산되고, 미리 결정된 템플리트로부터 선택되는 등 임의적으로 선택될 수 있다. 보정 가중치들의 초기 값들은 각각의 보 정 가중치에 대해 동일하거나 별개일 수 있고 또한 0 일 수도 있다. 도 34 및 도 35에 도시된 바와 같이, p-net(100B)는 또한 p-net(100B)의 트레이닝을 조절하도록 구성된 컨트롤 러(122A)를 포함하며, 이하에서 상세히 설명되는 동작을 실행하기 위한 수단이다. 컨트롤러(122A)는 다른 실시 예들에 대해 전술한 가중치 보정 계산기를 포함할 수 있다. 이하에서 상세하게 설명되는 태스크를 적절하 게 수행하기 위해, 컨트롤러(122A)는 적어도 일부가 실재적이고 일시적이지 않은 메모리를 포함한다. 컨트롤러 (122A)의 메모리는 컴퓨터 판독 가능 데이터 또는 처리 명령을 제공하는데 참여하는 기록 가능한 매체일 수 있 다. 그러한 매체는 비 휘발성 매체 및 휘발성 매체를 포함 하나 이에 한정되지 않는 많은 형태를 취할 수 있다. 컨트롤러(122A)용 비 휘발성 매체는 예를 들어 광 또는 자기 디스크 및 다른 영구 메모리를 포함할 수 있다. 휘 발성 매체는 예를 들어 주 메모리를 구성 할 수 있는 DRAM(dynamic random access memory)을 포함할 수 있다. 이러한 명령은 컴퓨터의 프로세서에 연결된 시스템 버스를 포함하는 동축 케이블, 구리 와이어 및 광섬유를 포 함하는 하나 이상의 전송 매체에 의해 전송될 수 있다. 컨트롤러(122A)의 메모리는 또한 적절한 매체, 예를 들어 자기 또는 광학 매체를 포함할 수 있다. 컨트롤러 (122A)는 고속 클록, 필수 아날로그-디지털(A/D) 및/또는 디지털-아날로그(D/A) 회로, 필요한 입력/출력 회로 및 디바이스들(I/O)은 물론 적절한 신호 컨디셔닝 및/또는 버퍼 회로를 포함할 수 있다. 컨트롤러(122A)에 의해 요구되거나 이에 의해 액세스 가능한 알고리즘은 메모리에 저장되고 자동적으로 실행되어 이하에 상세히 설명된 요구되는 기능을 제공할 수 있다. 컨트롤러(122A)는 보정 가중치들을 보정 가중치 어레이(119A)로 조직하도록 프로그램될 수 있다. 컨트롤러 (122A)는 또한 원하는 출력값 어레이(126A)로서 조직화된 원하는 이미지들 또는 출력 신호들을 수신하도록 구성된다. 컨트롤러(122A)는 추가로 원하는 출력값 어레이로부터 뉴런 합 어레이(120A)의 편차를 결정하고 편차 어레이를 생성하도록 구성된다. 컨트롤러(122A)는 또한 결정된 편차 어레이를 사용하여 보정 가 중치 어레이(119A)를 변경하도록 구성된다. 이러한 경우에, 뉴런 합 어레이(120A)를 결정하기 위해 변경된 보정 가중치 값을 가산함으로써 트레이닝된 보정 가중치 어레이(134A)를 생성하기 위해 원하는 출력값 어레이(126A) 로부터 뉴런 합 어레이(120A)의 편차가 감소된다(도 36에 도시됨). 도 34 및 도 35에 도시된 보정 가중치 어레이(119A)와 유사하게, 트레이닝 된 보정 가중치 어레이(134A)는 점선 박스(134A) 내의 모든 보정 가중치들 을 포함한다. 또한, 도 36에 도시되고 도 34 및 도 35의 보정 가중치 어레이(119A)와 유사하게, 트레이닝 된 보정 가중치 어레이(134A)는 점선 박스(119A) 내의 모든 트레이닝된 보정 가중치들(112A)을 포함하고 그것과 관련된 분배기들을 포함할 수 있다. 따라서, 뉴런 합 어레이(120A)의 최소화된 편차는 p-net(100B)에의해 생성된 에러를 보상한다. 또한, 생성된 트레이닝된 보정 가중치 어레이(134A)는 p-net(100B)의 동시 또는 병렬 트레이닝을 용이하게 한다. 도 35에 도시된 트레이닝된 p-net(100C)에서, p-net에 대한 다수의 입력들 은 입력 이미지들을 수신 하도록 구성될 수 있다. 이러한 입력 이미지들는 입력 값 어레이(107A)로서 수신되거나 또는 p- net(100B)에 의한 이미지의 인식 중에 입력 값 어레이(107A)로서 코드화(코딩)될 수 있다. 각각의 시냅스 는 다수의 트레이닝된 보정 가중치(112A)를 포함할 수 있다. 또한, 각각의 뉴런은 각각의 뉴런에 연결된 각 시냅스에 대응하는 트레이닝된 보정 가중치들(112A)의 가중치 값들을 합산하도록 구성될 수 있어, 다수 의 뉴런들이 인식된 이미지 어레이를 생성하도록 하여, 입력 이미지들의 인식을 제공한다. 분배기 를 포함하는 트레이닝된 p-net(100B) 및 실시예에서, 분배기는 트레이닝 및 입력 이미지들을 각각의 트레이닝 입력 값 어레이 및 입력 값 어레이(107A)로서 코드화하도록 구성될 수 있다. 따라서, 그러한 세 트의 분배기는 각각의 트레이닝 및 입력 이미지들 각각을 수신하기 위해 다수의 입력들에 동작 가능하게 연결된다. 전술한 동작은 구조화된 매트릭스들, 특히 트레이닝된 보정 가중치 어레이(134A) 대신에 트 레이닝된 보정 가중치 매트릭스를 사용하여 수행될 수 있으며, 이는 이하에서 상세히 설명될 것이다. 컨트롤러(122A)는 추가로 원하는 출력값 어레이(126A)로부터 뉴런 합 어레이(120A)의 타겟 편차 또는 타겟 편차 어레이의 어레이로 프로그래밍될 수 있다. 또한, 컨트롤러(122A)는 원하는 출력값 어레이(126A)로부터 뉴 런 합 어레이(120A)의 편차가 타겟 편차 어레이의 수용 가능한 범위 내에 있을 때 p- net(100B)의 트레이닝을 완료하도록 구성될 수 있다. 수용 가능한 범위는 타겟 편차 어레이의 최대 또는 최소값 또는 타겟 편차 어레이의 평균값에 대해 참조될 수 있다. 대안으로, 컨트롤러(122A)는 편차 의 감소 속도 또는 트레이닝 입력 값 어레이와 원하는 출력값 어레이(126A)의 수렴 속도가 소정의 속 도 값으로 떨어질 때 p-net(100B)의 트레이닝을 완료하도록 구성될 수 있다. 수용 범위 및/또는 소정 속도 값 140)은 컨트롤러(122A)에 프로그래밍 될 수 있다. 트레이닝 입력 값 어레이, 입력 값 어레이(107A), 보정 가중치 어레이(119A), 뉴런 합 어레이(120A), 원하 는 출력값 어레이(126A), 편차 어레이, 트레이닝된 보정 가중치 어레이(134A), 인식된 이미지 어레이 및 타겟 편차 어레이, 즉, 그것들에서의 파라미터 값들은, 각각, 트레이닝 입력 값 매트릭스, 입력 값 매트릭스(141A), 보정 가중치 매트릭스, 뉴런 합 매트릭스, 원하는 출력값 매트릭스, 편차 매트릭스, 트레이닝된 보정 가중치 매트릭스, 인식된 이미지 매트릭스, 및 타겟 편차 매트릭스 로 조직화될 수 있다. 각각의 어레이(107, 107A, 119, 120, 126, 132, 134, 136 및 138)에서, 각각의 파 라미터들의 값은 예를 들어 프로세서 액세스 가능한 데이터 테이블의 형태로 구성될 수 있으며, 각각의 매트릭 스들(141, 141A, 142, 143, 144, 145, 146, 147 및 148)의 값들은 그들의 결합들 뿐만 아니라 각각의 매트릭스 에 대수 매트릭스 연산을 개별적으로 적용할 수 있도록 특별히 구성되어 있다. 매트릭스들(141, 141A, 142, 143, 144, 145, 146, 147 및 148)은 도면들에 구체적으로 도시되지는 않았지만, 그렇게 조직화될 때 각각의 어 레이들(107, 119, 120, 126, 132, 134, 136, 및 138)에서 발생되는 것으로 이해되어야 한다. 아래의 예에서 설명의 편의를 위해 특정 매트릭스가 임의의 수의 열과 행으로 표시된다. 예를 들어, 트레이닝 이미지는 입력 트레이닝 매트릭스 |I|에서 수신 및/또는 조직될 수 있다 : 표 3 입력 1 입력 2 입력 3 이미지 1I11 I21 I31 이미지 2I12 I22 I32 이미지 3I13 I23 I33 이어서, 트레이닝 입력 이미지 매트릭스 컨트롤러(122A)를 통해 트레이닝 입력 값 매트릭스로 변환될 수 있고, 이는 매트릭스 |C|로 표현된다. 각 매트릭스 |C|는 입력 \"I\"의 수에 해당하는 수의 열을 갖지만 특정 수 의 간격들 \"i\"와 이미지들의 수에 해당하는 행 수를 고려한다. 매트릭스 |C|에서, 간격들 \"i\"는 트레이닝 중에 사용될 특정 보정 가중치로 식별된다. 간격들 \"i\"에 대응 하는 열에서, 신호의 값들은 특정 신호가 특정 간격에서 사용될 것이라는 것을 나타내기 위해 일로 대체될 수 있는 반면, 대상 신호에 대한 다른 간격에서는 신호의 값이 특정 간격이 고려되지 않음을 나타 내기 위해 영으로 대체된다. 예시적인 보정 가중치 매트릭스은 아래에 도시된 매트릭스 |W|로서 형성될 수 있다: 표 4 간격 들출력 1출력 2출력 3 입력 1i1W111 W112 W113 i2W121 W122 W123 i3W131 W132 W133 i4W141 W142 W143 입력 2i1W211 W212 W213 i2W221 W222 W223 i3W231 W232 W233 i4W241 W242 W243 입력 3i1W311 W312 W313 i2W321 W322 W323 i3W331 W332 W333 i4W341 W342 W343 뉴런 합 매트릭스는 아래 도시된 매트릭스 |Σ|로 표현될 수 있다:"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "원하는 출력값 매트릭스은 아래 도시된 매트릭스 |O|로서 형성될 수 있다: 표 5 출력 1 출력 2 출력 3 이미지 1O11 O12 O13 이미지 2O21 O22 O23 이미지 3O31 O32 O33 뉴런 합 매트릭스의 편차는 아래 매트릭스 |E|로 표현되는 편차 매트릭스를 생성할 수 있도록 원하는 출력값 매트릭스로부터 결정될 수 있다: 아래 매트릭스 |W|로 표시되는 보정 가중치 매트릭스는, 결정된 편차 매트릭스를 이용하여 변경될 수 있는데, 이는 변경된 보정 가중치 값을 가산하여 뉴런 합 매트릭스를 결정하도록 하여 매트릭스 |W trained|로서 표시되는, 트레이닝된 보정 가중치 매트릭스를 생성하도록 원하는 출력 값 매트릭스로부터 뉴런 합 매트릭스의 편차를 최소화한다. 매트릭스 |W trained|는 식 |W trained| = |W| + |∇W| 에 따라 유도된 다(여기서, 인자 |∇W|는 이하에서 상세히 기술될 것이다): 표 6 간격 들출력 1 출력 2 출력 3 입력 1i1W111 + ∇W111 W112 + ∇W112 W113 + ∇W113 i2W121 + ∇W121 W122 + ∇W122 W123 + ∇W123 i3W131 + ∇W131 W132 + ∇W132 W133 + ∇W133 i4W141 + ∇W141 W142 + ∇W142 W143 + ∇W143 입력 2i1W211 + ∇W211 W212 + ∇W212 W213 + ∇W213 i2W221 + ∇W221 W222 + ∇W222 W223 + ∇W223 i3W231 + ∇W231 W232 + ∇W232 W233 + ∇W233 i4W241 + ∇W241 W242 + ∇W242 W243 + ∇W243 입력 3i1W311 + ∇W311 W312 + ∇W312 W313 + ∇W313 i2W321 + ∇W321 W322 + ∇W322 W323 + ∇W323 i3W331 + ∇W331 W332 + ∇W332 W333 + ∇W333 i4W341 + ∇W341 W342 + ∇W342 W343 + ∇W343 전술한 바와 같이, 트레이닝된 보정 가중치 어레이(134A) 및 트레이닝된 보정 가중치 매트릭스의 형성은 p-net(100B)의 동시 트레이닝을 용이하게 한다. 트레이닝된 p-net(100C)를 이용하는 이미지 인식(도 36에 도시됨)의 실시예에서, 입력 이미지들의 배치의 동시 인식은 전술한 매트릭스 동작을 이용하여 제공될 수 있다. 특히, 트레이닝된 p-net(100C)은 2차원 n × k 매트릭스 |W|로 표현될 수 있는 보정 가중치 어레이이며, 여기서 \"n\"은 뉴런의 수이고, \"k\"는 특정 뉴런에 서 보정 가중치의 수이다. 매트릭스 |W|는 일반적으로 다음과 같이 나타낼 수 있다: 표 7 W11 W12W13...W1k W21W22W23...W2k W31W32W33...W3k W41W42W43...W4k............... Wn1Wn2Wn3...Wnk 일괄 입력 이미지들의 동시 인식을 위해, 인식될 입력 이미지들은 vxk 매트릭스 |Ir|로서 나타낼 수 있으 며, 여기서 \"v\"는 인식 가능한 이미지들의 수이고, \"k\"는 특정 뉴런에서 보정 가중치의 수이다. 인식 을 위한 입력 이미지들의 매트릭스 |Ir|은 일반적으로 다음과 같이 표현될 수 있다. 표 8 Ir11 Ir21Ir31...Irv1 Ir12Ir22Ir32...Irv2 Ir13Ir23Ir33...Irv3 Ir14Ir24Ir34...Irv4 ............... Ir1kIr2kIr3k...Irvk 매트릭스 |Ir|에서, 매트릭스의 각 행은 인식 대상이 되는 단일 이미지이다. 일괄 입력 이미지들의 동시 인식은 아래와 같은 심볼 \"|Y|\"로 표시되는 인식된 이미지 매트릭스을 생성하기 위해, 전치 매트릭스 |I|T에 의한 매트릭스 |W|의 승산에 의해 제공될 수 있다:"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "매트릭스 |Y|는 n x v의 차원을 갖는다. 매트릭스 |Y|의 각 열은 트레이닝된 p-net(100C)에 의해 획득된 단일 출력 또는 인식된 이미지이다. 매트릭스 |Y|는 일반적으로 다음과 같이 묘사될 수 있다: 표 9 Y11 Y12Y13...Y1v Y21Y22Y23...Y2v Y31Y32Y33...Y3v Y41Y42Y43...Y4v ............... Yn1Yn2Yn3...Ynv p-net(100B 및 100C) 각각은 컨트롤러(122A)의 서브-유닛일 수 있는 데이터 프로세서를 추가로 포함할 수 있다. 이러한 실시예에서, 컨트롤러(122A)는 추가적으로 각각의 트레이닝 입력 값 매트릭스, 입력 값 매트 릭스(141A), 보정 가중치 매트릭스, 뉴런 합 매트릭스 및 원하는 출력값 매트릭스 중 적어도 하 나를 각각의 서브-매트릭스로 분할하거나 잘라내도록(partition or cut-up) 구성될 수 있다. 또한, 컨트롤러 (122A)는 다수의 결과 서브-매트릭스 또는 서브-매트릭스를 데이터 프로세서와 통신하여 별도의 수학적 연 산을 수행하도록 구성될 수 있다. 대상 매트릭스들(141, 142, 143 및 144) 중 임의의 것을 각각의 서브-매트릭 스로 분할함으로써, 동시 또는 병렬 데이터 프로세싱 및 입력 값 매트릭스(141A)의 이미지 인식 또는 p- net(100B)의 트레이닝중 어느 하나의 속도 증가를 용이하게 한다. 이러한 동시 또는 병렬 데이터 프로세싱은 또 한 p-net(100B 또는 100C)의 확장성(scalability)를 허용함으로써, 즉, 특정 프로세서상에서 대수 조작을 받는 각각의 매트릭스의 크기를 제한 및/또는 예시된 프로세서와 같은 다수의 프로세서들 사이에서 매트릭스를 분해함으로써, p-net의 크기를 변화시키는 능력을 제공한다. 도 34 내지 도 36에 도시된 바와 같이, p-net(100B 및 100C)의 이러한 실시예에서, 컨트롤러(122A)와 통신하는 다수의 데이터 프로세서가 컨트롤러(122A)의일부로서 또는 그곳으로부터 멀리 떨어져서 배열될 수 있으며, 분리되어 병렬로 작동하도록 구성될 수 있다. 컨트롤러(122A)는 트레이닝 입력 값 매트릭스(141A) 및 보정 가중치 매트릭스에 대수 매트릭스 연산을 적용하여 보정 가중치 매트릭스를 변경함으로써 p-net(100B)을 트레이닝시킨다. 이러한 수학적 매트릭스 연산은 입력 값 매트릭스(141A)와 보정 가중치 매트릭스의 수학적 곱의 결정을 포 함할 수 있음으로써 현재의 트레이닝 시기 가중치 매트릭스를 형성한다. 컨트롤러(122A)는 위에서 설명된 매트릭스 |E|로 도시된 뉴런 합들의 편차 매트릭스를 생성하기 위해 원하 는 출력 값 매트릭스로부터 뉴런 합 매트릭스를 감산하도록 구성될 수도 있다. 또한, 컨트롤러(122A)는, 다음과 같이, 심볼 \"|ΔW|\"로 아래에 표시되는, 뉴런 입력 당 편차 매트릭스를 생성하기 위해, 각각의 뉴런에 연결된, 아래에서 문자 \"m\"으로 식별되는, 시냅시스들의 개수에 의해 뉴런 합들 편차 매트릭스가 나눠지도록 구성될 수 있다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "컨트롤러(122A)는 심볼 \"|S|\"로 아래에 식에서 표현된 p-net(100B)의 하나의 트레이닝 시기 동안 각 보정 가중 치가 사용된 횟수를 결정하도록 부가적으로 구성될 수 있다. 아래에서 볼 수 있듯이, 매트릭스 |S|는 트레 이닝 입력 값 매트릭스(141A)에 단위 벡터를 곱하여 얻어진다:"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "컨트롤러(122A)는 각각의 보정 가중치가 하나의 트레이닝 시기 동안 사용된 결정된 횟수를 사용하여 하나의 트 레이닝 시기에 대해 아래에 심볼 \"|∇W|\"로 표시되는, 평균 편차 매트릭스를 형성하도록 더 구성될 수 있 다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "또한, 컨트롤러(122A)는 보정 가중치 매트릭스에 하나의 트레이닝 시기에 대한 평균 편차 매트릭스을 가산하도록 구성될 수 있음으로써, 아래에 |W trained|로 표시된, 트레이닝된 보정 가중치 매트릭스를 생성하 고, 그리고 아래에 도시된 바와 같이 하나의 트레이닝 시기를 완료한다."}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "표 10 간격 들출력 1 출력 2 출력 3 입력 1i1W111 + ∇W111 W112 +∇W112 W113 + ∇W113 i2W121 +∇W121 W122 + ∇W122 W123 + ∇W123 i3W131 + ∇W131 W132 + ∇W132 W133 + ∇W133 i4W141 + ∇W141 W142 + ∇W142 W143 + ∇W143 입력 2i1W211 + ∇W211 W212 + ∇W212 W213 + ∇W213 i2W221 + ∇W221 W222 + ∇W222 W223 + ∇W223 i3W231 + ∇W231 W232 + ∇W232 W233 + ∇W233 i4W241 + ∇W241 W242 + ∇W242 W243 + ∇W243입력 3i1W311 + ∇W311 W312 + ∇W312 W313 + ∇W313 i2W321 + ∇W321 W322 + ∇W322 W323 + ∇W323 i3W331 + ∇W331 W332 + ∇W332 W333 + ∇W333 i4W341 + ∇W341 W342 + ∇W342 W343 + ∇W343 도 37은 도 34-36과 관련하여 전술한 바와 같이 p-net(100B)를 동작시키는 방법을 도시한다. 방법은 프로세서와 같은 하나 이상의 데이터 프로세서를 사용하여 감독된 트레이닝을 구현하는데 사용되는 컴퓨터 시스템 또는 컴퓨터 시스템과 같은 디바이스의 동작을 향상시키도록 구성된다. 방법은 p-net(100B)을 동작 시키기 위한 일시적 컴퓨터 판독 가능 저장 디바이스로 프로그램될 수 있으며, 상기 방법을 수행하도록 실행 가 능한 명령들로 인코딩될 수 있다. 방법은 프레임에서 시작하는데, 이 방법은, 다수의 입력들을 통해, 트레이닝 이미지들을 수신하는 단계를 포함한다. 도 34 및 도 35에 도시된 p-net(100B)의 구조와 관련하여 상술한 바와 같이, 트레이 닝 이미지들은 대상 트레이닝 단계의 시작 이전에 트레이닝 입력 값 어레이로서 수신되거나 또는 실 제 트레이닝 단계 동안 트레이닝 입력 값 어레이로서 코드화될 수 있다. 프레임 다음에, 방법은 프레임 으로 진행한다. 프레임에서, 이 방법은 보정 가중치 어레이(119A)에서 다수의 시냅스들의 보정 가중치를 조직하는 단계를 포함한다. p-net(100B)의 구조와 관련하여 상술한 바와 같이, 각각의 시냅스 는 다수의 입력들 중 하나에 연결되고 다수의 보정 가중치들을 포함한다. 프레임 후에, 방법은 프레임으로 진행하는데, 이 방법은 다수의 뉴런을 통해 뉴런 합 어레이 (120A)를 생성하는 단계를 포함한다. p-net(100B)의 구조와 관련하여 상술한 바와 같이, 각 뉴런은 적어도 하나의 출력을 가지며, 다수의 시냅스들 중 하나를 통해 다수의 입력들 중 적어도 하나와 연결 된다. 또한, 각각의 뉴런은 각각의 뉴런에 연결된 각 시냅스에 대응하는 보정 가중치들의 가중 치 값들을 합산하도록 구성된다. 프레임 이후에, 프레임에서, 방법은, 컨트롤러(122A)를 통해, 원하 는 출력값 어레이(126A)로 조직화된 원하는 이미지를 수신하는 단계를 포함한다. 프레임 후에, 방법 은 프레임으로 진행하고, 이 방법은, 컨트롤러(122A)를 통해, 원하는 출력값 어레이(126A)로부터 뉴런 합 어레이(120A)의 편차를 결정하는 단계를 포함하고 그리고 따라서 편차 어레이를 생성한다. 프레임 이후에, 방법은 프레임으로 진행한다. 프레임에서, 이 방법은 결정된 편차 어레이 를 사용하여 보정 가중치 어레이(119A)를, 컨트롤러(122A)를 통해, 변경하는 단계를 포함한다. 변경된 보정 가 중치 어레이(119A)의 변경된 보정 가중치 값은 이후에 가산되거나 합산되어 새로운 뉴런 합 어레이(120A)를 결 정하는데 사용될 수 있다. 변경된 보정 가중치 어레이(119A)의 합산된 변경된 보정 가중치 값들은 원하는 출력 값 어레이(126A)로부터의 뉴런 합 어레이(120A)의 편차를 감소시키거나 최소화하고 트레이닝된 보정 가중치 어 레이(134A)를 생성하는 역할을 한다. 편차 어레이는, p-net(100C)의 구조와 관련하여 상술한 바와 같이, 원하는 출력값 어레이(126A)로부터의 뉴런 합 어레이(120A)의 편차가 타겟 편차 어레이의 수용 가능 한 범위 내에 있을 때 충분히 최소화된 것으로 결정될 수 있다. 트레이닝된 보정 가중치 어레이(134A)는 편차 어레이를 사용하여 결정된 트레이닝된 보정 가중치(112A)를 포함하고, 이에 의해 p-net(100B)를 트레 이닝시킨다. p-net(100B)의 구조와 관련하여 상술한 바와 같이, 트레이닝 입력 값 어레이, 보정 가중치 어레이(119A), 뉴런 합 어레이(120A), 원하는 출력값 어레이(126A), 편차 어레이, 트레이닝된 보정 가중치 어레이(134A) 및 타겟 편차 어레이는 각각 트레이닝 입력 값 매트릭스, 보정 가중치 매트릭스, 뉴런 합 매트 릭스, 원하는 출력값 매트릭스, 편차 매트릭스, 트레이닝된 보정 가중치 매트릭스 및 타겟 편차 매트릭스로 조직화될 수 있다. 프레임에서, 이 방법은, 컨트롤러(122A)를 통하여, 각각의 트레 이닝 입력 값 매트릭스, 입력 값 매트릭스(141A), 보정 가중치 매트릭스, 뉴런 합 매트릭스 및 원하는 출력값 매트릭스 중 적어도 하나를 분할하는 단계를 포함할 수 있다. 이러한 결과적인 서브-매트릭 스는 별도의 수학적 연산을 위해 데이터 프로세서에 전달되어 동시 데이터 처리 및 p-net(100B)의 트레이 닝 속도의 향상을 용이하게 한다. 프레임에서, 방법은 또한, 컨트롤러(122A)를 통해, 대수 매트릭스 연산을 트레이닝 입력 값 매트릭스 및 보정 가중치 매트릭스에 적용함으로써 보정 가중치 매트릭스를 변경하는 단계를 포함할 수 있고, 따라 서 p-net(100B)를 트레이닝한다. 이러한 수학적 매트릭스 연산은 트레이닝 입력 값 매트릭스 및 보정 가중치 매트릭스의 수학적 곱을 결정하는 단계를 포함하고 따라서 현재 트레이닝 시기 가중치 매트릭스을 형성한다. 프레임에서, 이 방법은, 컨트롤러(122A)를 통해, 뉴런 합들의 편차 매트릭스를 생성하기 위해 원하는 출력값 매트릭스로부터 뉴런 합 매트릭스을 감산하는 단계를 추가로 포함할 수 있다. 또 한, 프레임에서, 이 방법은, 컨트롤러(122A)를 통해, 뉴런 입력 당 편차 매트릭스를 생성하기 위해 뉴런 합들의 편차 매트릭스를 각각의 뉴런에 연결된 입력들의 갯수로 나누는 단계를 포함할 수 있다. 또한, 프레임에서, 방법은, 컨트롤러(122A)를 통해, 각각의 보정 가중치가 p-net(100B)의 하나의 트 레이닝 시기 동안 사용된 횟수를 결정하는 단계를 포함할 수 있다. 또한, 이 방법은, 컨트롤러(122A)를 통해, 각각의 보정 가중치가 특정 트레이닝 시기 동안 사용된 결정된 횟수를 사용하여 하나의 트레이닝 시기에 대한 평균 편차 매트릭스를 형성하는 단계를 포함할 수 있다. 예를 들어, 이러한 동작은, 하나의 트레이닝 시기 동안 사용된 각각의 보정 가중치에 대한 평균 편차를 얻기 위해, 각각의 보정 가중치가 특정 트레이 닝 시기 동안 사용된 결정된 횟수로 뉴런 입력 당 편차 매트릭스를, 요소별로(element-by-element), 나누는 단 계를 포함함으로써, 하나의 트레이닝 시기에 대한 평균 편차 매트릭스를 형성한다. 또한, 예를 들어, 산술 평균, 기하 평균, 조화 평균, 제곱 평균 제곱근 등을 사용하여 하나의 트레이닝 시기에 대한 평균 편차 매트릭스를 형성하도록, 프레임에서 다른 매트릭스 기반 연산이 사용될 수 있다. 또 한, 프레임에서, 이 방법은, 컨트롤러(122A)를 통해, 하나의 트레이닝 시기에 대한 평균 편차 매트릭스 를 보정 가중치 매트릭스에 추가하는 단계를 포함할 수 있음으로써, 트레이닝된 가중치 매트릭스 를 생성하고 특정 트레이닝 시기를 완료한다. 따라서, 매트릭스 연산들이 모든 보정 가중치들에 병렬 로 적용되도록 함으로써, 방법은 동시 발생을 용이하게 하고, 따라서 트레이닝된 p-net(100C)를 생성하는 데 있어서의 p-net(100B)의 향상된 속도의 트레이닝을 제공한다. 프레임 다음에, 방법은 편차 어레이가 충분히 최소화될 때까지 추가적인 트레이닝 시기를 수행 하도록 프레임으로 복귀하는 단계를 포함할 수 있다. 환언하면, 부가적인 트레이닝 시기들은 소정의 편차 또는 에러 값 내에서 원하는 출력값 어레이(126A) 상의 뉴런 합 어레이(120A)를 수렴하도록 수행될 수 있고, 이 에 따라 p-net(100B)은 트레이닝되어 새로운 입력 이미지로 동작할 준비가 되어 있는 것으로 간주 될 수 있다. 따라서, 프레임 이후에, 본 방법은 트레이닝된 p-net(100C)(도 36에 도시됨)를 사용하여 이미지 인 식을 위한 프레임으로 진행할 수 있다. 트레이닝된 p-net(100C)를 이용한 이미지 인식의 실시예에서, 프레임에서, 방법은 다수의 입력들 을 통해 입력 이미지들를 수신하는 단계를 포함한다. p-net(100C)의 구조와 관련하여 전술한 바와 같 이, 입력 이미지들은 입력 값 어레이(107A)로서 수신되거나 또는 p-net(100C)에 의해 이미지들을 인식하는 동안 입력 값 어레이로서 코드화될 수 있다. 프레임의 다음에, 프레임에서, 본 방법은 트레이닝된 보 정 가중치 어레이(134A)의 다수의 트레이닝된 보정 가중치(112A)를 각 시냅스에 부여하는(attributing) 단 계를 포함한다. 프레임 이후에, 본 방법은 프레임로 진행한다. 프레임에서, 방법은 각각의 뉴런에 연결된 각 시냅스에 대응하는 트레이닝된 보정 가중치(112 A)의 가중치 값들을 가산하는 단계를 포함한다. p-net(100B)의 구조와 관련하여 상술한 바와 같이, 트레니닝된 보정 가중치들(112A)의 가중치 값들의 그러한 합산은 다수의 뉴런이 인식된 이미지 어레이를 생성하 게 하고, 이에 의해 입력 이미지들의 인식을 제공한다. p-net(100C)의 구조와 관련하여 전술한 바와 같이, 트레이닝에 사용된 매트릭스들(141, 142, 143, 144, 145, 146 및 148)에 추가적으로, 입력 값 어레이(107A) 및 인식된 이미지들 어레이는 입력 값 매트릭스(141A) 및 인식된 이미지들 매트릭스로서 각각 조직화될 수 있다. 프레임에서, 본 방법은 또한, 컨트롤러(122A)를 통해, 입력 값 매트릭스(141A)와 같은 사용된 매트릭스들 중 임의의 매트릭스를 각각의 서브-매트릭스들로 분할하는 단계를 포함할 수 있다. 이러한 결과적인 서브-매트 릭스들은 별도의 수학 연산을 수행하기 위해 데이터 프로세서와 통신할 수 있음으로써, 동시 데이터 처리 및 p-net(100C)의 이미지 인식 속도의 향상을 촉진한다. 프레임들(202-212)에서 방법의 트레이닝 부분에 부여되는 효과 매트릭스 동작과 유사하게, 대수 매트릭스 연산들이 트레이닝된 p-net(100C)의 매트릭스들 또는 서브-매트릭스들에 병렬로 적용될 때 프레임(214-218)의 이미지 인식 영역은 향상된 속도 이득을 갖는다. 따라 서, 매트릭스 연산들이 모든 트레이닝된 보정 가중치(112A)에 병렬로 적용되도록 함으로써, 본 방법은 동 시 수행, 따라서 p-net(100C)을 사용한 향상된 속도, 이미지 인식을 용이하게 한다. 프레임 다음에, 달성 된 이미지 인식이 불충분하게 정밀하다고 판단되면, 도 34 내지 도 36을 참조하여 설명한 바와 같이, 방법은 추"}
{"patent_id": "10-2019-7000226", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "가적인 트레이닝을 위해 프레임으로 복귀하거나, 또는 방법은 프레임에서 종결될 수 있다.상세한 설명 및 도면들(drawings) 또는 도면들(figures)은 본 발명의 내용을 뒷받침하고 설명하는 것이지만, 본 발명의 범위는 청구 범위에 의해서만 정의된다. 청구된 개시를 수행하기 위한 최상의 모드들 및 다른 실시예들 중 일부가 상세히 설명되었지만, 첨부된 청구항들에 정의된 개시를 실시하기 위한 다양한 대안적인 설계들 및 실시예들이 존재한다. 또한, 도면들에 도시된 실시예들 또는 본 명세서에서 언급된 다양한 실시예의 특성들은 반드시 서로 독립적인 실시예들로서 이해될 필요는 없다. 오히려, 일 실시예의 예들 중 하나에 설명된 각각의 특성이 다른 실시예들로부터의 하나 또는 다수의 다른 바람직한 특성들과 결합될 수 있어, 단어들 또는 도면들 을 참조하여 기술되지 않은 다른 실시 예들을 초래할 수 있다. 따라서, 이러한 다른 실시 예들은 첨부된 청구 범위의 범주 내에 있다."}
{"patent_id": "10-2019-7000226", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 종래의 고전적인 인공 신경망의 개략도이다. 도 2는 다수의 시냅스들들, 분배기들 세트, 및 각 시냅스와 연관된 다수의 보정 가중치들을 갖는 “프로그래시 브 신경망”(p-net)의 개략도이다. 도 3a는 다수의 시냅스들과 각 분배기의 업스트림에 위치한 하나의 시냅스 가중치를 갖는 도 2에서 도시된 p- net의 부분의 개략도이다. 도 3b는 다수의 시냅스들과 다수의 보정 가중치들의 각각의 다운스트림에 위치한 시냅스 가중치들 세트를 갖는 도 2에 도시된 p-net의 부분의 개략도이다. 도 3c는 다수의 시냅스들과 각 분배기의 업스트림에 위치한 하나의 시냅스 가중치와 다수의 보정 가중치들의 각 각의 다운 스트림에 위치한 시냅스 가중치들의 세트를 갖는 도 2에 도시된 p-net의 부분의 개략도이다. 도 4a는 특정한 입력의 모든 시냅스들을 위한 단일 분배기와 각 분배기의 업스트림에 위치된 하나의 시냅스 가 중치를 갖는 도 2에 도시된 p-net의 부분의 개략도이다. 도 4b는 특정한 입력의 모든 시냅스들을 위한 단일 분배기와 다수의 보정 가중치들의 각각의 다운스트림에 위치 한 시냅스 가중치들의 세트를 갖는 도 2에 도시된 p-net의 부분의 개략도이다. 도 4c는 특정한 입력의 모든 시냅스들을 위한 단일 분배기를 갖고, 각 분배기의 업스트립에 위치한 하나의 시냅 스 가중치와 다수의 보정 가중치들의 각각의 다운스트림에 위치한 시냅스 가중치들의 세트를 갖는 도 2에 도시 된 p-net의 부분의 개략도이다. 도 5는 도 2에 도시된 p-net의 개별적인 간격들로 입력 신호값 범위를 나눈 것을 도시한 것이다. 도 6a는 도 2에 도시된 p-net에서 보정 가중치들의 영향 계수값들에 대한 분포의 일 실시예의 개략도이다. 도 6b는 도 2에서 도시된 p-net에서 보정 가중치들의 영향 계수값들에 대한 분포의 다른 실시예의 개략도이다. 도 6c는 도 2에 도시된 p-net에서 보정 가중치들의 영향 계수값들에 대한 분포의 또 다른 실시예의 개략도이다.도 7은 디지털 코드의 형태에서 이미지를 대표하는 하나의 대응되는 테이블과 각 간격들의 세트로서 동일한 이 미지를 대표하는 다른 대응되는 테이블 뿐만 아니라 도 2에 도시된 p-net을 위한 입력 이미지의 개략도이다. 도 8은 두 개의 구별되는 이미지들의 인식을 위해 트레이닝된 p-net의 일 실시예의 도시이고, p-net은 각 이미 지의 일부 구성들을 포함하는 사진응 인식하도록 구성된다. 도 9는 \"중앙\" 뉴런 근처의 시냅스 가중치들의 분포의 예시를 갖는 도 2에서 도시된 p-net의 일 실시예의 개략 도이다. 도 10은 보정 가중치들의 사이에서 트레이닝 편차의 균일한 분포를 나타내는 도 2에 도시된 p-net의 일 실시예 의 개략도이다. 도 11은 p-net 트레이닝 동안 보정 가중치들의 변형을 채택하는 도 2에 도시된 p-net의 일 실시예의 개략도이다. 도 12는 베이직 알고리즘이 출력 뉴런 합들의 일차적인 세트를 작성하고, 작성된 세트가 보유되거나 증가된 값 들을 갖는 여러 \"승자\" 합들을 작성하는데 사용되고, 남아있는 합들의 공헌이 무효화된 도 2에 도시된 p-net의 일 실시예의 예시이다. 도 13은 다수의 이미지의 원소들을 갖는 복합 이미지를 인식하는 도 2에 도시된 p-net의 일 실시예의 예시이다. 도 14는 통일 모델링 언어(Unified Modeling Language (UML))를 사용하는 도 2에 도시된 p-net을 위한 객체 지 향 프로그래밍을 위한 모델의 개략도이다. 도 15는 도 2에 도시된 p-net의 일반적인 형성 시퀀스의 개략도이다. 도 16은 도 2에 도시된 p-net의 형성을 위한 데이터의 대표적인 분석 및 준비의 개략도이다. 도 17은 트레이닝 및 p-net 적용 동안 입력 데이터를 갖는 도 2에 도시된 p-net의 대표적인 입력 작성 허용 상 호작용(input creation permitting interaction)의 개략도이다. 도 18은 도 2에 도시된 p-net을 위한 뉴런 유닛들의 대표적인 작성의 개략도이다. 도 19는 뉴런 유닛들과 연결된 각 시냅스의 대표적인 작성의 개략도이다. 도 20은 도 2에 도시된 p-net을 트레이닝하는 것의 개략도이다. 도 21은 도 2에 도시된 p-net에서 뉴런 유닛 트레이닝의 개략도이다. 도 22는 도 2에 도시된 p-net의 트레이닝 동안 뉴런 합들의 확장의 개략도이다. 도 23은 도 2-22에서 도시된 신경망을 트레이닝하는데 사용되는 방법의 흐름도이다. 도 24는 메모리 소자에 의해 확립된 다수의 교정 가중치들 각각을 갖는 p- net의 특정 실시예의 개략도로서, p- net은 네트워크 트레이닝 과정에서 설명된다. 도 25는 메모리 소자에 의해 확립된 다수의 교정 가중치들 각각을 갖는 p- net의 특정 실시예의 개략도로서, p- net은 이미지 인식 과정에서 설명된다. 도 26은 트레이닝의 제1단계 동안 멤리스터들을 사용하는 대표적인 p-net의 개략도이다. 도 27은 트레이닝의 제2단계 동안 멤리스터들을 사용하는 대표적인 p-net의 개략도이다. 도 28은 대표적인 p-net에서 멤리스터들의 쌍둥이 병렬 분기들의 개략도이다. 도 29는 저항들을 사용하는 대표적인 p-net의 개략도이다. 도 30은 p-net에서 저항으로 구성된 메모리 소자의 일 실시예의 개략도이다. 도 31은 p-net에서 저항으로 구성된 메모리 소자의 다른 실시예의 개략도이다. 도 32는 p-net에서 가변 임피던스로 구성된 메모리 소자의 다른 실시예의 개략도이다. 도 33은 도 2-22 및 도 24-32에 도시된 신경망을 동작시키는데 사용되는 방법의 흐름도이다. 도 34는 본 개시에 따른, 다수의 시냅스들 및 각각의 시냅스와 관련된 다수의 보정 가중치들을 갖는 \"프로그래시브 신경망\"(p-net)의 예시이다. 도 35는 본 개시에 따른, 트레이닝 과정에서의 p-net의 예시이다. 도 36은 본 개시에 따른, 이미지 인식 과정에서의 p-net의 예시이다. 도 37은 도 34-36에 도시된 신경망을 동작시키는 방법의 흐름도이다."}
