{"patent_id": "10-2019-0160008", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0070029", "출원번호": "10-2019-0160008", "발명의 명칭": "반복적 생성을 통해 출력 콘텐트를 향상시키기 위한 디바이스, 방법, 및 프로그램", "출원인": "삼성전자주식회사", "발명자": "백서현"}}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "반복적 생성 (iterative generation) 을 통해 출력 콘텐트를 향상시키기 위한 디바이스로서:인스트럭션들을 저장하는 메모리; 및프로세서를 포함하고, 상기 프로세서는 상기 인스트럭션들을 실행하여:자연어 입력을 수신하고;자연어 이해 (natural language understanding; NLU) 모델을 이용하여, 상기 자연어 입력에 기초하여 사용자의도 정보를 획득하고;제 1 사용자 입력에 기초하여 베이스 콘텐트에서 타겟 영역을 설정하고;상기 사용자 의도 정보 또는 제 2 사용자 입력에 기초하여 입력 콘텐트를 결정하고;신경망 (neural network) 모델을 이용하여, 상기 입력 콘텐트, 상기 타겟 영역, 및 상기 사용자 의도 정보에 기초하여 상기 베이스 콘텐트에 연관되는 출력 콘텐트를 생성하고;이미지 캡셔닝 (image captioning) 모델을 이용하여, 상기 출력 콘텐트에 대한 캡션을 생성하고;상기 자연어 입력의 텍스트 및 상기 캡션 간 유사도를 계산하고;상기 유사도가 미리 결정된 조건을 충족하지 않는 경우 상기 출력 콘텐트의 상기 생성을 반복하도록 구성되는디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 베이스 콘텐트, 상기 입력 콘텐트, 및 상기 출력 콘텐트는 이미지이고, 상기 입력 콘텐트를 상기 베이스 콘텐트의 상기 타겟 영역에 합성함으로써, 상기 출력 콘텐트가 생성되는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 베이스 콘텐트는 복수의 영역들을 포함하고,상기 타겟 영역은 상기 복수의 영역들 중 상기 제 1 사용자 입력에 의해 선택되는 영역을 포함하는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,상기 자연어 입력은 음성 입력을 포함하고, 상기 자연어 입력의 상기 텍스트는 자동 음성 인식 (automatic speech recognition; ASR) 을 통해 상기 음성입력으로부터 변환되는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서,상기 입력 콘텐트는 상기 사용자 의도 정보에 포함되는 콘텐트 정보에 기초하여 결정되는 디바이스.공개특허 10-2021-0070029-3-청구항 6 제 5 항에 있어서,상기 입력 콘텐트는 상기 콘텐트 정보에 대응하는 복수의 콘텐트들로부터 결정되는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 복수의 콘텐트들의 속성들은 서로 상이한 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서,상기 입력 콘텐트의 자세, 표정, 메이크업, 헤어, 의상, 및 액세서리 중 적어도 하나를 포함하는 상기 입력 콘텐트의 속성은 상기 사용자 의도 정보에 포함되는 콘텐트 속성 정보에 기초하여 결정되는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항에 있어서,상기 신경망은 생성적 적대 신경망 (generative adversarial network; GAN) 에 연관되고, 상기 출력 콘텐트는 상기 생성적 적대 신경망의 생성자 (generator) 에 의해 생성되는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1 항에 있어서,상기 출력 콘텐트의 확률 분포는 진정 (real) 콘텐트의 확률 분포에 대응하는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 1 항에 있어서,상기 출력 콘텐트를 포함하는 상기 베이스 콘텐트의 확률 분포는 진정 콘텐트의 확률 분포에 근사하는디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 1 항에 있어서,상기 신경망은 생성적 적대 신경망에 연관되고, 상기 유사도가 상기 미리 결정된 기준을 충족하지 않는 경우, 상기 생성적 적대 신경망의 감별자(discriminator) 는 상기 출력 콘텐트를 위조 (fake) 로 감별하는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 1 항에 있어서,상기 출력 콘텐트는 제 1 출력 콘텐트이고, 상기 유사도가 상기 미리 결정된 기준을 충족하지 않는 경우, 상기 프로세서는 상기 인스트럭션들을 실행하여:상기 신경망 모델을 이용하여, 상기 입력 콘텐트, 상기 타겟 영역, 및 상기 사용자 의도 정보에 기초하여 상기제 1 출력 콘텐트와 상이한 제 2 출력 콘텐트를 생성하도록 더 구성되는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 1 항에 있어서,상기 입력 콘텐트는 제 1 입력 콘텐트이고, 상기 출력 콘텐트는 제 1 출력 콘텐트이고, 공개특허 10-2021-0070029-4-상기 유사도가 상기 미리 결정된 기준을 충족하지 않는 경우, 상기 프로세서는 상기 인스트럭션들을 실행하여:상기 제 1 입력 콘텐트와 상이한 제 2 입력 콘텐트를 결정하고;상기 신경망 모델을 이용하여, 상기 제 2 입력 콘텐트 및 상기 타겟 영역에 기초하여 상기 제 1 출력 콘텐트와상이한 제 2 출력 콘텐트를 생성하도록 더 구성되는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 1 항에 있어서,상기 프로세서는 상기 인스트럭션들을 실행하여:상기 출력 콘텐트의 일부에 대한 사용자 피드백을 수신하고;상기 신경망 모델을 이용하여, 상기 출력 콘텐트의 상기 일부를 변형하도록 더 구성되는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 1 항에 있어서,상기 베이스 콘텐트는 애플리케이션의 작업영역 (workspace) 을 포함하고,상기 입력 콘텐트는 상기 작업영역에 배치되는 작업 객체를 포함하는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16 항에 있어서,상기 출력 콘텐트는 상기 작업 객체에 연관된 애니메이션을 포함하고, 상기 애니메이션은 상기 작업 객체, 상기 사용자 의도 정보, 및 상기 애플리케이션의 API (applicationprogramming interface) 에 기초하여 생성되고, 상기 출력 콘텐트에 대한 상기 캡션은, 상기 애니메이션에 대한 캡션을 포함하는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 1 항에 있어서,상기 자연어 이해 모델, 상기 신경망 모델, 및 상기 이미지 캡셔닝 모델은 상기 메모리에 저장되는 디바이스."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "반복적 생성 (iterative generation) 을 통해 출력 콘텐트를 향상시키기 위한 방법으로서:자연어 입력을 수신하는 동작;자연어 이해 (natural language understanding; NLU) 모델을 이용하여, 상기 자연어 입력에 기초하여 사용자의도 정보를 획득하는 동작;제 1 사용자 입력에 기초하여 베이스 콘텐트에서 타겟 영역을 설정하는 동작;상기 사용자 의도 정보 또는 제 2 사용자 입력에 기초하여 입력 콘텐트를 결정하는 동작;신경망 (neural network) 모델을 이용하여, 상기 입력 콘텐트, 상기 타겟 영역, 및 상기 사용자 의도 정보에 기초하여 출력 콘텐트를 생성하는 동작;이미지 캡셔닝 (image captioning) 모델을 이용하여, 상기 출력 콘텐트에 대한 캡션을 생성하는 동작;상기 자연어 입력의 텍스트 및 상기 캡션 간 유사도를 계산하는 동작; 및상기 유사도가 미리 결정된 조건을 충족하지 않는 경우 상기 출력 콘텐트의 상기 생성을 반복하는 동작을 포함하는 방법."}
{"patent_id": "10-2019-0160008", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "공개특허 10-2021-0070029-5-인스트럭션들을 저장하는 컴퓨터 판독가능 매체로서, 상기 인스트럭션들은 프로세서에 의해 실행되는 경우 상기 프로세서로 하여금:자연어 입력을 수신하고;자연어 이해 (natural language understanding; NLU) 모델을 이용하여, 상기 자연어 입력에 기초하여 사용자의도 정보를 획득하고;제 1 사용자 입력에 기초하여 베이스 콘텐트에서 타겟 영역을 설정하고;상기 사용자 의도 정보 또는 제 2 사용자 입력에 기초하여 입력 콘텐트를 결정하고;신경망 (neural network) 모델을 이용하여, 상기 입력 콘텐트, 상기 타겟 영역, 및 상기 사용자 의도 정보에 기초하여 출력 콘텐트를 생성하고;이미지 캡셔닝 (image captioning) 모델을 이용하여, 상기 출력 콘텐트에 대한 캡션을 생성하고;상기 자연어 입력의 텍스트 및 상기 캡션 간 유사도를 계산하고;상기 유사도가 미리 결정된 조건을 충족하지 않는 경우 상기 출력 콘텐트의 상기 생성을 반복하도록 구성되는컴퓨터 판독가능 매체."}
{"patent_id": "10-2019-0160008", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시에 의해, 반복적 생성 (iterative generation) 을 통해 출력 콘텐트를 향상시키기 위한 디바이스로서: 인 스트럭션들을 저장하는 메모리; 및 프로세서를 포함하고, 상기 프로세서는 상기 인스트럭션들을 실행하여: 자연 어 입력을 수신하고; 자연어 이해 (natural language understanding; NLU) 모델을 이용하여, 상기 자연어 입력 (뒷면에 계속)"}
{"patent_id": "10-2019-0160008", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 딥러닝 등의 기계 학습 알고리즘을 활용하여 인간 두뇌의 인지, 판단 등의 기능을 모사하는 인공지능 (AI) 시스템 및 그 응용에 관한다. 본 개시는 구체적으로 인공지능을 이용한 반복적 생성을 통해 출력 콘텐트를 향상시키는 것에 관한다."}
{"patent_id": "10-2019-0160008", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(Artificial Intelligence, AI) 시스템은 인간 수준의 지능을 구현하는 컴퓨터 시스템이며, 기존 Rule 기반 스마트 시스템과 달리 기계가 스스로 학습하고 판단하며 똑똑해지는 시스템이다. 인공지능 시스템은 사용 할수록 인식률이 향상되고 사용자 취향을 보다 정확하게 이해할 수 있게 되어, 기존 Rule 기반 스마트 시스템은 점차 딥러닝 기반 인공지능 시스템으로 대체되고 있다. 인공지능 기술은 기계학습(딥러닝) 및 기계학습을 활용한 요소 기술들로 구성된다. 기계학습은 입력 데이터들의 특징을 스스로 분류/학습하는 알고리즘 기술이며, 요소기술은 딥러닝 등의 기계학 습 알고리즘을 활용하여 인간 두뇌의 인지, 판단 등의 기능을 모사하는 기술로서, 언어적 이해, 시각적 이해, 추론/예측, 지식 표현, 동작 제어 등의 기술 분야로 구성된다. 인공지능 기술이 응용되는 다양한 분야는 다음과 같다. 언어적 이해는 인간의 언어/문자를 인식하고 응용/처리 하는 기술로서, 자연어 처리, 기계 번역, 대화시스템, 질의 응답, 음성 인식/합성 등을 포함한다. 시각적 이해 는 사물을 인간의 시각처럼 인식하여 처리하는 기술로서, 객체 인식, 객체 추적, 영상 검색, 사람 인식, 장면 이해, 공간 이해, 영상 개선 등을 포함한다. 추론 예측은 정보를 판단하여 논리적으로 추론하고 예측하는 기술 로서, 지식/확률 기반 추론, 최적화 예측, 선호 기반 계획, 추천 등을 포함한다. 지식 표현은 인간의 경험정보 를 지식데이터로 자동화 처리하는 기술로서, 지식 구축(데이터 생성/분류), 지식 관리(데이터 활용) 등을 포함 한다. 동작 제어는 차량의 자율 주행, 로봇의 움직임을 제어하는 기술로서, 움직임 제어(항법, 충돌, 주행), 조 작 제어(행동 제어) 등을 포함한다. 일 실시예에 따르면, 사용자의 의도에 부합하는 콘텐트가 생성될 수 있다. 일 실시예에 따르면, 콘텐트의 생성 과정의 효율이 향상될 수 있다. 본 개시에 의해, 반복적 생성 (iterative generation) 을 통해 출력 콘텐트를 향상시키기 위한 디바이스로서: 인스트럭션들을 저장하는 메모리; 및 프로세서를 포함하고, 상기 프로세서는 상기 인스트럭션들을 실행하여: 자연어 입력을 수신하고; 자연어 이해 (natural language understanding; NLU) 모델을 이용하여, 상기 자연어 입 력에 기초하여 사용자 의도 정보를 획득하고; 제 1 사용자 입력에 기초하여 베이스 콘텐트에서 타겟 영역을 설 정하고; 상기 사용자 의도 정보 또는 제 2 사용자 입력에 기초하여 입력 콘텐트를 결정하고; 신경망 (neural network) 모델을 이용하여, 상기 입력 콘텐트, 상기 타겟 영역, 및 상기 사용자 의도 정보에 기초하여 상기 베 이스 콘텐트에 연관되는 출력 콘텐트를 생성하고; 이미지 캡셔닝 (image captioning) 모델을 이용하여, 상기 출 력 콘텐트에 대한 캡션을 생성하고; 상기 자연어 입력의 텍스트 및 상기 캡션 간 유사도를 계산하고; 상기 유사 도가 미리 결정된 조건을 충족하지 않는 경우 상기 출력 콘텐트의 상기 생성을 반복하도록 구성되는 디바이스가 제공될 수 있다. 상기 베이스 콘텐트, 상기 입력 콘텐트, 및 상기 출력 콘텐트는 이미지이고, 상기 입력 콘텐트를 상기 베이스 콘텐트의 상기 타겟 영역에 합성함으로써, 상기 출력 콘텐트가 생성되는 디바이스가 제공될 수 있다. 상기 베이스 콘텐트는 복수의 영역들을 포함하고, 상기 타겟 영역은 상기 복수의 영역들 중 상기 제 1 사용자 입력에 의해 선택되는 영역을 포함하는 디바이스가 제공될 수 있다. 상기 자연어 입력은 음성 입력을 포함하고, 상기 자연어 입력의 상기 텍스트는 자동 음성 인식 (automatic speech recognition; ASR) 을 통해 상기 음성 입력으로부터 변환되는 디바이스가 제공될 수 있다. 상기 입력 콘텐트는 상기 사용자 의도 정보에 포함되는 콘텐트 정보에 기초하여 결정되는 디바이스가 제공될 수 있다. 상기 입력 콘텐트는 상기 콘텐트 정보에 대응하는 복수의 콘텐트들로부터 결정되는 디바이스가 제공될 수 있다. 상기 복수의 콘텐트들의 속성들은 서로 상이한 디바이스가 제공될 수 있다. 상기 입력 콘텐트의 자세, 표정, 메이크업, 헤어, 의상, 및 액세서리 중 적어도 하나를 포함하는 상기 입력 콘 텐트의 속성은 상기 사용자 의도 정보에 포함되는 콘텐트 속성 정보에 기초하여 결정되는 디바이스가 제공될 수 있다. 상기 신경망은 생성적 적대 신경망 (generative adversarial network; GAN) 에 연관되고, 상기 출력 콘텐트는 상기 생성적 적대 신경망의 생성자 (generator) 에 의해 생성되는 디바이스가 제공될 수 있다. 상기 출력 콘텐트의 확률 분포는 진정 (real) 콘텐트의 확률 분포에 대응하는 디바이스가 제공될 수 있다. 상기 출력 콘텐트를 포함하는 상기 베이스 콘텐트의 확률 분포는 진정 콘텐트의 확률 분포에 근사하는 디바이스 가 제공될 수 있다. 상기 신경망은 생성적 적대 신경망에 연관되고, 상기 유사도가 상기 미리 결정된 기준을 충족하지 않는 경우, 상기 생성적 적대 신경망의 감별자 (discriminator) 는 상기 출력 콘텐트를 위조 (fake) 로 감별하는 디바이스 가 제공될 수 있다. 상기 출력 콘텐트는 제 1 출력 콘텐트이고, 상기 유사도가 상기 미리 결정된 기준을 충족하지 않는 경우, 상기 프로세서는 상기 인스트럭션들을 실행하여: 상기 신경망 모델을 이용하여, 상기 입력 콘텐트, 상기 타겟 영역, 및 상기 사용자 의도 정보에 기초하여 상기 제 1 출력 콘텐트와 상이한 제 2 출력 콘텐트를 생성하도록 더 구성 되는 디바이스가 제공될 수 있다. 상기 입력 콘텐트는 제 1 입력 콘텐트이고, 상기 출력 콘텐트는 제 1 출력 콘텐트이고, 상기 유사도가 상기 미 리 결정된 기준을 충족하지 않는 경우, 상기 프로세서는 상기 인스트럭션들을 실행하여: 상기 제 1 입력 콘텐트 와 상이한 제 2 입력 콘텐트를 결정하고; 상기 신경망 모델을 이용하여, 상기 제 2 입력 콘텐트 및 상기 타겟 영역에 기초하여 상기 제 1 출력 콘텐트와 상이한 제 2 출력 콘텐트를 생성하도록 더 구성되는 디바이스가 제공 될 수 있다. 상기 프로세서는 상기 인스트럭션들을 실행하여: 상기 출력 콘텐트의 일부에 대한 사용자 피드백을 수신하고; 상기 신경망 모델을 이용하여, 상기 출력 콘텐트의 상기 일부를 변형하도록 더 구성되는 디바이스가 제공될 수 있다. 상기 베이스 콘텐트는 애플리케이션의 작업영역 (workspace) 을 포함하고, 상기 입력 콘텐트는 상기 작업영역에 배치되는 작업 객체를 포함하는 디바이스가 제공될 수 있다. 상기 출력 콘텐트는 상기 작업 객체에 연관된 애니메이션을 포함하고, 상기 애니메이션은 상기 작업 객체, 상기 사용자 의도 정보, 및 상기 애플리케이션의 API (application programming interface) 에 기초하여 생성되고, 상기 출력 콘텐트에 대한 상기 캡션은, 상기 애니메이션에 대한 캡션을 포함하는 디바이스가 제공될 수 있다. 상기 자연어 이해 모델, 상기 신경망 모델, 및 상기 이미지 캡셔닝 모델은 상기 메모리에 저장되는 디바이스가 제공될 수 있다. 나아가, 본 개시에 의해, 반복적 생성 (iterative generation) 을 통해 출력 콘텐트를 향상시키기 위한 방법으 로서: 자연어 입력을 수신하는 동작; 자연어 이해 (natural language understanding; NLU) 모델을 이용하여, 상기 자연어 입력에 기초하여 사용자 의도 정보를 획득하는 동작; 제 1 사용자 입력에 기초하여 베이스 콘텐트 에서 타겟 영역을 설정하는 동작; 상기 사용자 의도 정보 또는 제 2 사용자 입력에 기초하여 입력 콘텐트를 결 정하는 동작; 신경망 (neural network) 모델을 이용하여, 상기 입력 콘텐트, 상기 타겟 영역, 및 상기 사용자 의도 정보에 기초하여 출력 콘텐트를 생성하는 동작; 이미지 캡셔닝 (image captioning) 모델을 이용하여, 상기 출력 콘텐트에 대한 캡션을 생성하는 동작; 상기 자연어 입력의 텍스트 및 상기 캡션 간 유사도를 계산하는 동 작; 및 상기 유사도가 미리 결정된 조건을 충족하지 않는 경우 상기 출력 콘텐트의 상기 생성을 반복하는 동작 을 포함하는 방법이 제공될 수 있다. 나아가, 본 개시에 의해, 인스트럭션들을 저장하는 컴퓨터 판독가능 매체로서, 상기 인스트럭션들은 프로세서에 의해 실행되는 경우 상기 프로세서로 하여금: 자연어 입력을 수신하고; 자연어 이해 (natural language understanding; NLU) 모델을 이용하여, 상기 자연어 입력에 기초하여 사용자 의도 정보를 획득하고; 제 1 사용 자 입력에 기초하여 베이스 콘텐트에서 타겟 영역을 설정하고; 상기 사용자 의도 정보 또는 제 2 사용자 입력에 기초하여 입력 콘텐트를 결정하고; 신경망 (neural network) 모델을 이용하여, 상기 입력 콘텐트, 상기 타겟 영 역, 및 상기 사용자 의도 정보에 기초하여 출력 콘텐트를 생성하고; 이미지 캡셔닝 (image captioning) 모델을 이용하여, 상기 출력 콘텐트에 대한 캡션을 생성하고; 상기 자연어 입력의 텍스트 및 상기 캡션 간 유사도를 계 산하고; 상기 유사도가 미리 결정된 조건을 충족하지 않는 경우 상기 출력 콘텐트의 상기 생성을 반복하도록 구 성되는 컴퓨터 판독가능 매체가 제공될 수 있다."}
{"patent_id": "10-2019-0160008", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고하여 실시예들에 대하여 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시 할 수 있도록 상세히 설명한다. 그러나, 실시예들은 다양한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 그리고, 도면에서 실시예들을 명확하게 설명하기 위해 설명과 관계없는 부분은 생략하 였으며, 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수개의 표현을 포함한다. \"포함하다\" 또는 \"가지다\" 등의 용어는 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 특히, 숫자들은 이해를 돕기 위한 예로서, 기재된 숫자들에 의해 실시예들이 한정되는 것으로 이해되지 말아야 한다. 여기에서, \"콘텐트\"는 전자 디바이스에 입력되거나, 전자 디바이스에 의해 생성되거나, 전자 디바이스에서 출력 될 수 있는 임의의 데이터를 가리킨다. 예를 들어, 콘텐트는 이미지, 벡터 이미지, 비디오, 애니메이션 (animation), 배경(background), 작업영역(workspace), 작업 객체 (work object), 오디오, 텍스트, 진동, 등일 수 있고, 또는 이들의 임의의 조합일 수 있다. 실시예들에 따른 방법들의 각각의 동작에서 언급되는 콘텐트들을 구별하기 위해, 베이스 콘텐트, 입력 콘텐트, 출력 콘텐트, 재구성된 콘텐트, 타겟 콘텐트, 위조 콘텐트, 진정 콘텐트 등의 용어들이 사용되었고, 각 용어가 가리키는 대상은 문맥에 의해 당업자에게 적절히 이해될 것이다. 예를 들어, 베이스 콘텐트는 편집, 변형, 합성 등의 대상이 되는 콘텐트를 가리킨다. 베이스 콘텐트는 애플리케 이션의 작업영역일 수 있다. 예를 들어, 문서 편집 애플리케이션의 작업영역인 문서, 프레젠테이션 편집 애플리 케이션의 작업영역인 슬라이드, 스프레드시트 편집 애플리케이션의 작업영역인 스프레드시트, 게임 애플리케이 션에서 사용자 창작 모드, 드로잉 애플리케이션의 드로잉 문서 등 일 수 있다. 한편, 콘텐트들을 나타내는 용어 들 각각은 동일한 종류의 콘텐트, 예를 들어, 이미지를 가리킬 수 있으나, 이에 제한되지 않고 서로 상이한 종 류의 콘텐트를 가리킬 수 있다. 예를 들어, 베이스 콘텐트는 작업영역이고, 입력 콘텐트는 이미지이고, 출력 콘 텐트는 이미지의 애니메이션일 수 있다. 여기에서, \"사용자 입력\"은 사용자에 의해 전자 디바이스에서 수신되는 모든 종류의 입력을 가리키지만, 특정 사용자에 의한 입력으로 한정되는 것은 아니다. 사용자 입력은 화면의 하나 이상의 좌표에 연관될 수 있으나, 이에 제한되지 않는다. 예를 들어, 사용자 입력은 음성 입력, 텍스트 입력, 또는 이들의 조합일 수 있다. 좌표 에 연관되는 입력은 터치 입력, 클릭 입력, 제스처 입력, 등일 수 있다. 여기에서, \"자연어 입력\"은 사람들이 일상적으로 사용하는 언어의 형태로 전자 디바이스에서 수신되는 입력을 가리키고, 자연어 입력은, 음성 입력, 텍스트 입력, 또는 이들의 조합일 수 있다. 도 1은 일 실시예에 따른 콘텐트의 반복적 생성을 개략적으로 설명하기 위한 도면이다. 도 1을 참조하면, 전자 디바이스는 사용자의 자연어 입력 및 사용자 입력에 기초하여 출력 콘텐트를 생성할 수 있다. 출력 콘텐트는, 베이스 콘텐트에 입력 콘텐트를 합성함으로써 생성될 수 있고, 베이 스 콘텐트, 입력 콘텐트, 및 출력 콘텐트는 모두 이미지일 수 있으나, 이에 제한되지 않는다. 출력 콘텐트를 생성하는 구체적인 방법은 도 7a 및 7b 를 참조하여 후술할 것이다. 일 실시예에서, 출력 콘텐트 생성에 사용되는 입력 콘텐트는 사용자의 자연어 입력에 기초하여 결정될 수 있다. 예를 들어, 도 1에 도시된 바와 같이, 사용자의 자연어 입력 \"draw a cat perching on here\" 에 기초하 여 고양이, 또는 걸터앉은(perching) 고양이의 이미지가 입력 콘텐트로서 결정될 수 있다. 입력 콘텐트는 전자 디바이스에 저장된 콘텐트들 중에서 결정되거나, 인터넷 검색을 통해 획득된 이미지들 중에서 결정될 수 있다. 자연어 입력에 기초하여 입력 콘텐트를 결정하는 방법은 도 5를 참조하여 후술할 것이다. 도 1을 참조하면, 베이스 콘텐트의 타겟 영역에서 출력 콘텐트가 생성될 수 있다. 출력 콘텐트 는, 베이스 콘텐트의 타겟 영역에 입력 콘텐트를 합성함으로써 생성될 수 있다. 타겟 영역은, 베이스 콘텐트에서 입력 콘텐트와 합성될 영역을 가리키고, 타겟 영역은 베이스 콘텐트의 전체 영역 또는 일부 영역일 수 있다. 합성된 베이스 콘텐트에서 타겟 영역에 대응하는 영역은 생성된 출력 콘텐트를 포함할 수 있다. 일 실시예에 따르면, 베이스 콘텐트의 타겟 영역에 입력 콘텐트를 합성함으로써, 베이스 콘텐트의 전체 영역에 입력 콘텐트를 합성하는 경우보다 합성이 필요한 픽 셀의 수가 감소하여, 합성 과정의 효율이 향상될 수 있다. 한편, 타겟 영역은, 베이스 콘텐트에서 검출되거나 로컬라이즈된(localized) 객체, 예를 들어, 도 1 의 베이스 콘텐트에서 책상, 의자, 또는 벤치를 포함하는 바운딩 박스(bounding box)와 동일할 수 있다. 베이스 콘텐트는 복수의 영역들, 예를 들어, 베이스 콘텐트 내 객체를 각각 포함하는 복수의 바운딩 박스들을 포함할 수 있고, 타겟 영역은 복수의 바운딩 박스들 중 사용자 입력에 의해 선택될 수도 있다. 타겟 영역의 크기나 형태는 사용자 입력, 예를 들어, 드래그 입력에 기초하여 조절될 수 있다. 타겟 영역 은 미리 결정된 크기나 형태를 가질 수 있다. 도 1을 참조하면, 생성된 출력 콘텐트에 대한 캡션이 생성될 수 있다. 출력 콘텐트에 대한 캡션은, 출력 콘텐트에 관한 텍스트로서, 이미지 캡셔닝(image captioning) 모델을 이용하여 생성될 수 있다. 캡션 은 출력 콘텐트를 묘사하기 위한 텍스트일 수 있다. 일 실시예에서, 자연어 입력의 텍스트와 출력 콘텐트 에 대한 캡션 간 유사도가 계산되고, 유사도가 소정의 기준을 충족하는 경우, 생성된 출력 콘텐트가 전자 디바이스에서 디스플레이될 수 있다. 일 실시예에서, 유사도가 소정의 기준을 충족하지 않는 경우, 출력 콘텐트의 생성 과정이 반복될 수 있다. 예를 들어, 다른 입력 콘텐트를 베이스 콘텐트의 타겟 영역에 합성함으로써, 출력 콘텐트의 생성 과정이 반복될 수 있다. 예를 들어, 동일한 입력 콘텐트를 타겟 영역에 합성하되 이전의 출력 콘텐트와 상이한 출력 콘텐트를 생성함으로써, 출력 콘텐트 의 생성 과정이 반복될 수 있고, 이는 도 7a 및 7b 를 참조하여 후술할 것이다. 일 실시예에 따르면, 사용자의 자연어 입력과 출력 콘텐트에 대한 캡션 간의 유사도에 기초하여 출력 콘텐 트의 생성 과정을 반복함으로써, 사용자의 의도에 부합하는 출력 콘텐트가 생성될 수 있다. 한편, 본 개시에서 설명되는 다양한 동작들, 예를 들어, 사용자의 자연어 입력의 해석, 출력 콘텐트의 생 성, 출력 콘텐트에 대한 캡션의 생성, 자연어 입력의 텍스트와 캡션 간의 유사도 계산, 등은 인공지능 모 델을 통해 수행될 수 있다. 인공지능 모델은 신경망 모델로 지칭될 수 있다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행한다. 복수의 신경 망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수 의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으며, 예 를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 도 2는 일 실시예에 따른 전자 디바이스의 블록도이다. 전자 디바이스는 스마트폰, 태블릿, 휴대폰, PDA (personal digital assistant), 미디어 플레이어, PMP (Portable Multimedia Player), 전자책 단말기, 디지털방송용 단말기, PC (Personal Computer), 노트북 (laptop), 마이크로 서버, GPS (global positioning system) 장치, 네비게이션, 키오스크, MP3 플레이어, 스마 트 TV, 디지털 카메라 및 기타 모바일, 또는, 비모바일 컴퓨팅 장치일 수 있으나, 이에 제한되지 않는다. 전자 디바이스는 엔드 유저 디바이스일 수 있으나, 이에 제한되지 않는다. 도 2를 참조하면, 전자 디바이스는 사용자 의도 획득부, 타겟 영역 설정부, 입력 콘텐트 결정부 , 출력 콘텐트 생성부, 캡션 생성부, 및 유사도 계산부를 포함할 수 있다. 실시예들에 따 라, 전자 디바이스는 전술된 유닛들의 수보다 더 많거나 더 적은 유닛들을 포함할 수 있다. 전자 디바이스 의 유닛들에 대한 명명은, 전자 디바이스에서 수행되는 동작들을 구별하여(distinctively) 설명하기 위한 것이므로, 특정 동작이 반드시 특정 유닛에서 수행될 필요는 없다. 예를 들어, 전자 디바이스의 특정 유닛에서 수행되는 것으로 묘사된 동작이 다른 유닛에서 수행될 수 있고, 전자 디바이스의 하나의 유닛에 서 수행되는 것으로 묘사된 동작이 복수의 유닛들에서 수행될 수 있고, 전자 디바이스의 복수의 유닛들 간 의 상호적 처리 (interactive processing) 에 의해 수행되는 것으로 묘사된 동작이 하나의 유닛에 의해 수행되 더라도, 실질적으로 동일한 기능성이 제공될 수 있다. 나아가, 전자 디바이스에서 수행되는 것으로 묘사된동작이 다른 디바이스에서 수행되거나, 다른 디바이스의 도움을 받아 수행되더라도, 실질적으로 동일한 기능성 이 제공될 수 있다. 전자 디바이스의 유닛들은 소프트웨어 모듈들로 구현될 수 있고, 소프트웨어 모듈들이 프로세서에 의해 실 행됨으로써 유닛들의 기능들이 수행될 수 있다. 한편, 소프트웨어 모듈들은 인스트럭션들의 집합으로서 메모리 에 저장될 수 있고, 메모리에 저장된 인스트럭션들이 프로세서에 의해 실행될 수 있다. 전자 디바이스의 유닛들은 하드웨어 모듈, 또는 하드웨어 모듈과 소프트웨어 모듈의 조합으로 구현될 수도 있다. 전자 디바이스 는 메모리 및 프로세서를 포함할 수 있다. 전자 디바이스의 각 유닛의 기능을 설명하기 위해 도 3을 더 참조한다. 도 3은 일 실시예에 따른 방법의 흐름도이다. 동작 S310에서, 수신된 자연어 입력에 기초하여 사용자 의도 정보가 획득될 수 있다. 전자 디바이스의 사 용자 의도 획득부가, 수신된 자연어 입력에 기초하여 사용자 의도 정보를 획득할 수 있다. 여기에서, 사용자 의도 정보는, 사용자의 자연어 입력(또는 자연어 입력의 텍스트)로부터 전자 디바이스에 의해 이해되는 사용자의 의도에 관한다. 사용자 의도 정보는, 전자 디바이스에서 수행 가능한 액션에 관한 액션 정보를 포함할 수 있다. 액션 정보는, 자연어 입력의 텍스트 중 동사로부터 추론될 수 있다. 예를 들어, 자연어 입력 \"draw a cat perching on here\" 중 동사인 \"draw\"로부터, 콘텐트 생성이 액션 정보로서 식별될 수 있다. 콘텐트 생성을 가리키는 단어는 \"draw\"에 제한되지 않고 다양할 수 있다. 전자 디바이스는, 식별된 액션 정보에 기초하여, 그 액션 정보가 가리키는 액션을 수행하도록 구현될 수 있다. 예를 들어, 전자 디바이스 는, 사용자 의도 정보 중 콘텐트 생성을 나타내는 액션 정보를 식별한 것에 응답하여, 출력 콘텐트를 생성 하도록 구현될 수 있다. 일 실시예에서, 인공지능 모델을 이용하여 사용자 의도 정보가 획득될 수 있고, 이에 대해서는 도 5를 참조하여 후술할 것이다. 동작 S320에서, 사용자 입력에 기초하여 베이스 콘텐트에서 타겟 영역이 설정될 수 있다. 전자 디바이스의 타겟 영역 설정부가 사용자 입력에 기초하여 베이스 콘텐트에서 타겟 영역을 설정할 수 있다. 타겟 영역은 베이스 콘텐트의 일부 영역 또는 전체 영역일 수 있다. 나아가, 타겟 영역은 사용자 입력에 의해 베이스 콘텐트 에서 조절 가능한 영역일 수 있다. 타겟 영역은 베이스 콘텐트에서 인식되는 바운딩 박스에 대응할 수 있다. 일 실시예에서, 바운딩 박스의 인식은 인공지능 모델에 의해 수행될 수 있다. 일 실시예에서, 사용자 입력 및 자연어 입력은 전자 디바이스에서 함께 수신될 수 있다. 예를 들어, 사용 자가 발화하면서 전자 디바이스에 디스플레이된 베이스 콘텐트 상을 터치하는 경우, 사용자의 발화에 의한 자연어 입력 및 사용자의 터치에 의한 사용자 입력이 전자 디바이스에 동시에 수신될 수 있다. 동작 S330에서, 입력 콘텐트가 결정될 수 있다. 전자 디바이스의 입력 콘텐트 결정부가 입력 콘텐트 를 결정할 수 있다. 일 실시예에서, 입력 콘텐트는 동작 S320에서 획득된 사용자 의도 정보에 기초하여 결정될 수 있다. 예를 들어, 사용자 의도 정보에 콘텐트 정보가 포함되는 경우, 콘텐트 정보에 기초하여 입력 콘텐트가 결정될 수 있다. 콘텐트 정보는, 자연어 입력의 텍스트 중 명사로부터 추론될 수 있다. 예를 들어, 자연어 입력 \"draw a cat perching on here\" 중 명사인 \"cat\"이 콘텐트 정보로서 식별될 수 있다. 일 실시예에서, 콘텐트 정보로서, 개체명 인식 (named entity recognition; NER) 을 통해 인식된 개체명이 식별될 수 있다. 전자 디 바이스는, 자연어 입력의 텍스트 중 콘텐트 정보로서 \"cat\"을 식별한 것에 응답하여, 고양이의 이미지를 입력 콘텐트로서 결정할 수 있다. 일 실시예에서, 콘텐트 정보는 자연어 입력의 텍스트 중 동사의 목적어로부터 추론될 수 있다. 예를 들어, 자연어 입력 \"draw a cat perching on here\" 중 동사인 \"draw\"의 목적어인 \"a cat perching\"으로부터, \"perching cat\"이 콘텐트 정보로서 식별될 수도 있고, 전자 디바이스는 걸터앉은 고양 이의 이미지를 입력 콘텐트로서 결정할 수 있다. 입력 콘텐트는 전자 디바이스에 저장된 콘텐트로부터 결 정되거나, 인터넷 검색을 통해 획득된 콘텐트로부터 결정될 수 있다. 콘텐트 정보는 액션 정보의 액션을 수행하 기 위해 인공지능 모델에 입력될 대상을 나타낼 수 있다. 일 실시예에서, 입력 콘텐트는 사용자 입력에 기초하여 결정될 수 있다. 예를 들어, 전자 디바이스에서 사 용자에 의해 선택된 콘텐트가 입력 콘텐트로서 결정될 수 있다. 입력 콘텐트를 결정하는 구체적인 방법은 도 6a 및 6b 를 참조하여 후술할 것이다. 한편, 일 실시예에서, 사용자 의도 정보가 베이스 콘텐트에서 타겟 영역을 설정하는데 이용될 수 있다. 사용자 의도 정보는 영역 정보를 포함할 수 있다. 자연어 입력의 텍스트 중 장소나 위치를 나타내는 표현, 예를 들어, \"here\", \"there\", \"everywhere\", \"somewhere\", 위치를 나타내는 전치사나 조사, 위치를 나타내는 전치사나 조사에 연결된 단어, 등이 영역 정보로서 식별될 수 있다. 예를 들어, 베이스 콘텐트에서 사용자 입력에 의해 임 의의 위치가 선택되는 경우, 선택된 위치를 기준으로 영역 정보에 의해 지시되는 영역이 타겟 영역으로서 설정 될 수 있다. 사용자 입력에 의해 베이스 콘텐트에서 바운딩 박스가 선택되는 경우, 선택된 바운딩 박스를 기준 으로 영역 정보에 의해 지시되는 영역이 타겟 영역으로서 설정될 수 있다. 일 실시예에 따르면, 동작 S320에서 사용자 입력 없이도, 사용자 의도 정보에 기초하여 베이스 콘텐트에서 타겟 영역이 설정될 수 있다. 한편, 일 실시예에서, 사용자 의도 정보는 콘텐트 속성 정보, 예를 들어, 자연어 입력 \"draw a cat perching on here\" 중 \"cat\"을 나타내는 콘텐트 정보가 식별되는 경우, 해당 콘텐트 정보를 수식하는 단어인 \"perching\"이 콘텐트의 속성을 나타내는 콘텐트 속성 정보로서 식별될 수 있고, 이에 대해서는 도 8, 9, 및 10을 참조하여 후 술할 것이다. 동작 S340에서, 입력 콘텐트, 타겟 영역, 및 사용자 의도 정보에 기초하여 출력 콘텐트가 생성될 수 있다. 출력 콘텐트는 베이스 콘텐트에 연관되어 생성될 수 있다. 전자 디바이스의 출력 콘텐트 생성부가 입력 콘 텐트, 타겟 영역, 및 사용자 의도 정보에 기초하여 출력 콘텐트를 생성할 수 있다. 여기에서, 출력 콘텐트는, 사용자의 자연어 입력에 의해 생성되는 콘텐트를 가리키고, 실제로 전자 디바이스에 의해 출력되어 사용자 에게 보여지는 콘텐트뿐만 아니라, 생성되었지만 실제로는 사용자에게 보여지지 않는 콘텐트를 포함할 수 있다. 일 실시예에서, 인공지능 모델을 이용하여 출력 콘텐트가 생성될 수 있고, 이에 대해서는 도 7a 및 7b 를 참조 하여 후술할 것이다. 동작 S350에서, 출력 콘텐트에 대한 캡션이 생성될 수 있다. 전자 디바이스의 캡션 생성부가 출력 콘 텐트에 대한 캡션을 생성할 수 있다. 일 실시예에서, 인공지능 모델을 이용하여 출력 콘텐트에 대한 캡션이 생 성될 수 있다. 예를 들어, 이미지 캡셔닝 (image captioning) 모델을 이용하여 출력 콘텐트에 대한 캡션이 생성 될 수 있다. 예를 들어, 비디오 캡셔닝 (video captioning) 모델을 이용하여 출력 콘텐트의 애니메이션에 대한 캡션이 생성될 수 있다. 공지된 다양한 이미징 캡셔닝 모델 및 비디오 캡셔닝 모델이 이용될 수 있으므로, 이에 대한 설명은 생략한다. 동작 S360에서, 자연어 입력의 텍스트 및 캡션 간의 유사도가 계산될 수 있다. 전자 디바이스의 유사도 계 산부가 자연어 입력의 텍스트 및 캡션 간의 유사도를 계산할 수 있다. 여기에서, 자연어 입력의 텍스트는, 동작 310에서 자연어 입력으로부터 사용자 의도 정보를 획득하는 과정에서 결정될 수 있다. 일 실시예에서, 인공지능 모델을 이용하여 자연어 입력의 텍스트 및 캡션 간의 유사도가 계산될 수 있다. 유사 도는 벡터 유사도일 수 있다. 자연어 입력의 텍스트 및 캡션을 의미적 벡터 (semantic vector) 로 인코딩함으로 써, 각각에 대응하는 벡터들이 생성될 수 있고, 벡터들 간의 유사도가 계산될 수 있다. 동작 S370에서, 유사도가 조건을 충족하는지 여부가 결정될 수 있다. 일 실시예에서, 유사도가 기설정된 임계치 를 초과하는 경우 조건이 충족될 수 있다. 일 실시예에서, 유사도가 기설정된 범위 내인 경우 조건이 충족될 수 있다. 동작 S370에서 유사도가 조건을 충족하는 경우, 생성된 출력 콘텐트가 사용자의 의도에 부합하는 것으로 결정될 수 있다. 동작 S370에서 유사도가 조건을 충족하지 않는 경우, 생성된 출력 콘텐트가 사용자의 의도에 부합하지 않는 것으로 결정될 수 있다. 일 실시예에서, 생성된 출력 콘텐트가 사용자의 의도에 부합하지 않은 경우, 즉, 유사도가 조건을 충족하지 않 는 경우, 사용자의 의도에 부합하는 출력 콘텐트를 생성하기 위해, 출력 콘텐트의 생성 과정이 반복될 수 있다. 예를 들어, 동작 S340, S350, S360, 및 S370이 반복될 수 있다. 일 실시예에서, 생성된 출력 콘텐트가 사용자의 의도에 부합하지 않는 경우, 즉, 유사도가 조건을 충족하지 않 는 경우, 사용자의 의도에 부합하는 출력 콘텐트를 생성하기 위해, 동작 S330에서 선택된 입력 콘텐트가 아닌 다른 입력 콘텐트가 결정되어 동작 S340, S350, S360, 및 S370이 반복될 수 있다. 일 실시예에 따르면, 사용자의 자연어 입력과 출력 콘텐트의 캡션을 비교함으로써 사용자의 의도에 더 부합하는 출력 콘텐트가 생성될 수 있다. 나아가, 출력 콘텐트의 생성 과정의 반복을 통해, 출력 콘텐트의 품질이 향상될 수 있다. 도 4는 일 실시예에 따라 생성된 예시적인 콘텐트들을 설명하기 위한 도면이다. 일 실시예에서, 베이스 콘텐트에서 설정되는 타겟 영역은 도 1에서 직사각형으로 도시되나, 이에 제한되지 않으 며, 원, 타원, 삼각형, 오각형, 다각형, 자유형, 등일 수 있다. 도 4를 참조하면, \"put Amy's face here\"의 자연어 입력 및 사용자 입력에 의해 설정된 타겟 영역(412a)에 기초 하여, \"Amy's face\"의 입력 콘텐트를 타겟 영역(412a)에 합성함으로써, 출력 콘텐트가 생성되고, 출력 콘텐트가 합성된 베이스 콘텐트(410a)가 사용자에게 제공될 수 있다. 여기에서, 입력 콘텐트는 인터넷을 검색함으로써 획 득될 수 있다. 일 실시예에서, 입력 콘텐트는 인공지능 모델을 이용하여, 전자 디바이스, 또는 클라우드 서버에 저장된 사진들 로부터 획득될 수 있다. 사용자에 의해 촬영된 사진들은, 전자 디바이스 또는 클라우드 서버에서 안면인식 인공 지능 모델에 의해 처리될 수 있고, 인식된 안면은 사용자에 의해 직접, 또는 사용자의 소셜 네트워크 서비스 (social network service; SNS) 의 사용 기록 등에 기초하여, 명명될 수 있다. 이에 따라, 인식된 안면들 중 \"Amy\"로 명명된 안면이 입력 콘텐트로서 결정될 수 있다. 한편, 입력 콘텐트를 타겟 영역(412a)에 합성함으로써 생성된 출력 콘텐트의 캡션은 \"Amy's face between friends\"일 수 있고, 출력 콘텐트의 캡션 및 사용자의 자연어 입력의 텍스트 간의 비교에 기초하여, 출력 콘텐 트의 생성 과정을 반복할 것인지가 결정될 수 있다. 일 실시예에서, 인공지능 모델을 이용하여, 베이스 콘텐트(410a)에서 안면을 포함하는 바운딩 박스가 인식될 수 있고, 인식된 바운딩 박스에 기초하여 타겟 영역(412a)이 설정될 수 있다. 사용자의 자연어 입력이 \"replace Bell's face with Amy\"이고 베이스 콘텐트(410a)에서 \"Bell\"의 안면이 인식되는 경우, \"Bell\"의 안면이 타겟 영역(412a)으로서 설정될 수 있다. 도 4를 참조하면, \"put a jumping cat here\"의 자연어 입력에 기초하여 \"cat\" 또는 \"jumping cat\"의 입력 콘텐 트(402b)를 타겟 영역에 합성함으로써, 출력 콘텐트가 생성될 수 있다. 일 실시예에서, 입력 콘텐트(402b)는 인 공지능 모델을 이용하여 베이스 콘텐트(400b)에서 결정될 수 있다. 예를 들어, 자연어 입력의 콘텐트 정보가, 베이스 콘텐트(400b)에서 인식된 객체에 대응하는 경우, 해당 객체가 입력 콘텐트(402b)로서 결정될 수 있다. 일 실시예에서, 베이스 콘텐트(400b)에서 인식된 입력 콘텐트(402b)는, 사용자 의도 정보에 기초하여 그 속성이 변경될 수 있다. 사용자 의도 정보는 콘텐트 속성 정보를 포함할 수 있고, 콘텐트 속성 정보는 포즈 (pose) 에 관할 수 있다. 나아가, 콘텐트 속성 정보가 지시하는 포즈에 대응하는 포즈 애니메이션이 획득될 수 있다. 콘텐 트 속성 정보의 포즈의 포즈 애니메이션은 포즈 애니메이션 라이브러리로부터 획득될 수 있다. 포즈 애니메이션 라이브러리는, 인공지능 모델을 이용하여 이미지 또는 동영상에서 인간 또는 동물 신체의 키포인트를 검출하여 생성될 수 있다. 일 실시예에서, 입력 콘텐트(402b)는, 사용자 의도 정보에 기초하여 획득된 포즈 애니메이션일 수 있다. 인공지 능 모델을 이용하여, 타겟 영역의 객체 및 포즈 애니메이션에 기초하여 출력 콘텐트가 생성될 수 있다. 베이스 콘텐트(400b)가 동영상인 경우, 인공지능 모델을 이용하여 베이스 콘텐트(400b)의 각 프레임의 타겟 영역을, 포 즈 애니메이션에 기초하여 변형함으로써, 출력 콘텐트가 생성될 수 있고, 출력 콘텐트가 합성된 베이스 콘텐트 (410b)가 사용자에게 제공될 수 있다. 타겟 영역의 크기 및 형태는 포즈 애니메이션에 따라 조절될 수 있다. 한편, 입력 콘텐트(402b)를 타겟 영역에 합성함으로써 생성된 출력 콘텐트의 캡션은 \"a jumping Russian Blue cat\"일 수 있고, 출력 콘텐트의 캡션 및 사용자의 자연어 입력의 텍스트 간의 비교에 기초하여, 출력 콘텐트의 생성 과정을 반복할 것인지가 결정될 수 있다. 여기에서 출력 콘텐트의 캡션을 생성하기 위해 인공지능 모델, 예를 들어, 비디오 캡셔닝 모델이 이용될 수 있다. 일 실시예에서, 인공지능 모델을 이용하여, 베이스 콘텐트(400b)에서 객체를 포함하는 바운딩 박스가 인식될 수 있고, 인식된 바운딩 박스에 기초하여 타겟 영역이 설정될 수 있다. 여기에서, 입력 콘텐트(402b)는 베이스 콘 텐트(400b)의 바운딩 박스 또는 타겟 영역에서 결정될 수 있다. 도 4를 참조하면, \"draw the sun in black and white here\"의 자연어 입력에 기초하여 \"sun\", 또는 \"sun in the black and white\"의 입력 콘텐트를 타겟 영역(412c)에 합성함으로써, 출력 콘텐트가 생성될 수 있고, 출력 콘텐트가 합성된 베이스 콘텐트(410c)가 사용자에게 제공될 수 있다. 일 실시예에서, 인공지능 모델을 이용하여, 출력 콘텐트가 베이스 콘텐트(410c)와 동일한 형식을 가지도록 생성 될 수 있다. 예를 들어, 베이스 콘텐트(410c)가 카툰 형식의 이미지인 경우, 출력 콘텐트 또한 카툰 형식의 이 미지로 생성될 수 있다. 카툰 형식의 이미지는 벡터 이미지일 수 있다. 출력 콘텐트는 이미지로부터 변환된 벡 터 이미지일 수 있다. 일 실시예에서, 출력 콘텐트는 벡터 드로잉 프로그램에서 지원되는 커맨드들에 기초하여 생성될 수도 있다. 출력 콘텐트는 프로그램의 API (application programming interface) 에 기초하여 생성될수 있다. 프로그램의 API 에 기초하여 출력 콘텐트를 생성하는 방법은 도 15, 및 16을 참조하여 후술할 것이다. 일 실시예에 따르면, 이미지 합성뿐만 아니라, 형식의 제한 없이 다양한 플랫폼에서 출력 콘텐트가 생성될 수 있다. 한편, 입력 콘텐트를 타겟 영역(412c)에 합성함으로써 생성된 출력 콘텐트의 캡션은 \"black hole\"일 수 있고, 출력 콘텐트의 캡션 및 사용자의 자연어 입력의 텍스트 간의 비교에 기초하여, 출력 콘텐트의 생성 과정을 반복 할 것인지가 결정될 수 있다. 출력 콘텐트의 캡션 및 사용자의 자연어 입력의 텍스트의 유사도가 소정 기준을 충족하지 않는 경우, 출력 콘텐트의 생성 과정이 반복될 수 있다. 출력 콘텐트의 반복적 생성은, 자연어 입력의 텍스트인 \"the sun in black and white\"과 유사한 캡션을 갖는 출력 콘텐트가 생성될 때까지 계속될 수 있다. 도 4를 참조하면, \"make a door here\"의 자연어 입력에 기초하여 \"door\"의 입력 콘텐트를 타겟 영역(412d)에 어 울리게 합성함으로써, 출력 콘텐트가 생성되고, 생성된 출력 콘텐트를 포함하는 베이스 콘텐트(410d)가 사용자 에게 제공될 수 있다. 일 실시예에서, 인공지능 모델을 이용하여, 출력 콘텐트는 베이스 콘텐트(410d)의 플랫 폼과 동일한 플랫폼에서 생성될 수 있다. 즉, 출력 콘텐트는 베이스 콘텐트(410d)의 플랫폼의 제약 하에서 생성 될 수 있다. 예를 들어, 베이스 콘텐트(410d)의 플랫폼이 게임인 경우, 게임에서 허용되는 방식으로 출력 콘텐 트가 생성될 수 있다. 일 실시예에서, 출력 콘텐트는 게임에서 지원되는 커맨드들에 기초하여 생성될 수 있다. 출력 콘텐트는 게임 프로그램의 API (application programming interface) 에 기초하여 생성될 수 있다. 일 실 시예에 따르면, 이미지 합성뿐만 아니라, 형식의 제한 없이 다양한 플랫폼에서 출력 콘텐트가 생성될 수 있다. 도 5는 일 실시예에 따라 사용자 의도 정보를 획득하는 방법을 설명하기 위한 도면이다. 사용자 의도 정보는 사용자의 자연어 입력에 기초하여 획득될 수 있다. 도 5를 참조하면, 전자 디바이스의 사용 자 의도 획득부는 자동 음성 인식 (automatic speech recognition; ASR) 모델 및 자연어 이해 (natural language understanding; NLU) 모델 을 포함할 수 있다. 도 5를 참조하면, 자연어 입력으로서 음성 입력이 수신되는 경우, 자연어 음성 입력의 음성은 ASR 모델을 통해 텍스트로 변환될 수 있고, 변환된 텍스트를 NLU 모델이 처리함으로써, 사용자 의도 정보가 획득될 수 있다. 음성 입력은 전자 디바이스의 마이크로폰을 통해 수신될 수 있다. ASR 모델을 통해 변환된 텍스트는, 이후 출력 콘텐트의 캡션과 서로 비교될 수 있고, 그 비교 결과 자연어 입력의 텍스트 및 출력 콘텐 트의 캡션 간의 유사도가 계산될 수 있다. 일 실시예에서, 사용자 의도 획득부는 음성 언어 이해 (spoken language understanding; SLU) 모델을 포 함할 수 있고, 마이크로폰을 통해 수신된 음성 입력이 SLU 모델에 의해 처리되어 사용자 의도 정보가 획득될 수 있다. 일 실시예에서, 자연어 입력은 다른 디바이스로부터 전송될 수 있다. 일 실시예에서, 자연어 입력으로서 텍스트 입력이 수신되는 경우, NLU 모델이 텍스트 입력의 텍스트를 처 리함으로써, 사용자 의도 정보가 획득될 수 있다. 텍스트 입력은 전자 디바이스의 입력부, 예를 들어, 키보드, 터치스크린, 터치스크린의 키패드 등을 통해 수신될 수 있다. 일 실시예에서, 텍스트 입력은 다른 디바이스로부 터 전송될 수 있다. 텍스트 입력의 텍스트는 이후 출력 콘텐트의 캡션과 서로 비교될 수 있고, 그 비교 결과 자 연어 입력의 텍스트 및 출력 콘텐트의 캡션 간의 유사도가 계산될 수 있다. NLU 모델에 자연어 입력의 텍스트가 입력되면, NLU 모델은 그 텍스트를 분석하여 사용자 의도 정보를 획득할 수 있다. NLU 모델은 텍스트로부터, 액션 정보, 콘텐트 정보, 콘텐트 속성 정보, 영역 정보 등을 획득할 수 있고, 획득된 정보는 출력 콘텐트의 생성에 사용될 수 있다. 사용자 의도 정보는, 전자 디바이스가 사용자의 의도를 더 정확히 이해할 수 있도록, 전술된 정보 이외에 다양한 카테고리에 따라 획득될 수 있다. 도 6a는 일 실시예에 따라 입력 콘텐트를 결정하는 방법의 흐름도이다. 도 6b는 일 실시예에 따라 입력 콘텐트 의 선택을 위한 예시적인 그래픽 유저 인터페이스 (graphic user interface; GUI) 를 도시한다. 도 6a를 참조하면, 도 3에서 설명된 동작 S330은 입력 콘텐트를 선택하는 사용자 입력의 수신 여부에 기초하여 상이하게 동작할 수 있다. 동작 S330은 베이스 콘텐트에서 타겟 영역을 설정하기 위한 사용자 입력 및 자연어 입력이 전자 디바이스에 수신된 이후에 수행될 수 있으나, 이에 제한되지 않는다. 예를 들어, 입력 콘텐트는, 베이스 콘텐트에서 타겟 영역을 설정하기 위한 사용자 입력 및 자연어 입력이 전자 디바이 스에 수신되기 이전에, 결정될 수 있다. 동작 S331에서 입력 콘텐트를 선택하는 사용자 입력이 수신되는지 여부가 결정될 수 있다. 동작 S331에서 입력 콘텐트를 선택하는 사용자 입력이 수신되는 경우, 동작 S332에서, 사용자 입력에 의해 선택된 입력 콘텐트가 결정될 수 있다. 예를 들어, 도 6b를 참조하면, 입력 콘텐트를 선택하기 위한 GUI 가 전자 디바이스의 디스 플레이 상에서 디스플레이되고, 사용자 입력에 의해 GUI 에서 선택된 콘텐트가 입력 콘텐트로서 결정될 수 있다. 동작 331에서 입력 콘텐트를 선택하는 사용자 입력이 수신되지 않은 경우, 동작 S333에서 사용자 의도 정보에 콘텐트 정보가 포함되는지 여부가 결정될 수 있다. 나아가, 동작 S333에서 사용자 의도 정보에 콘텐트 정보가 포함되는 경우, 동작 S334에서 콘텐트 정보에 기초하여 입력 콘텐트가 결정될 수 있다. 예를 들어, 도 6b를 참 조하면, \"draw a cat perching on here\"의 자연어 입력에 기초하여, \"cat\"이 콘텐트 정보로서 식별될 수 있고, 전자 디바이스는, 자연어 입력의 텍스트 중 콘텐트 정보로서 \"cat\"을 식별한 것에 응답하여, 고양이의 이미지를 입력 콘텐트로서 결정할 수 있다. 일 실시예에서, 사용자의 확인 하에, 콘텐트 정보에 기초하여 입력 콘텐트가 결정될 수 있다. 예를 들어, 콘텐트 정보에 기초하여 검색된 콘텐트가 전자 디바이스에 디스플레이되고, 사용자 가 검색된 콘텐트에 동의하는 경우, 해당 콘텐트가 입력 콘텐트로 결정될 수 있다. 콘텐트 정보에 기초하여 복 수의 콘텐트들이 검색되는 경우, 복수의 콘텐트들 중 입력 콘텐트를 선택하기 위한 GUI를 전자 디바이스에 서 디스플레이함으로써, 사용자로 하여금 입력 콘텐트를 선택하게 할 수 있다. 동작 S333에서 사용자 의도 정보에 콘텐트 정보가 포함되지 않은 경우, 동작 S335에서 복수의 콘텐트들이 디스 플레이될 수 있다. 즉, 복수의 콘텐트들 중 입력 콘텐트를 선택하기 위한 GUI 가 전자 디바이스에서 디스 플레이될 수 있고, 동작 S336에서 사용자 입력에 기초하여 복수의 콘텐트들 중 입력 콘텐트가 결정될 수 있다. 사용자 의도 정보에 콘텐트 정보가 포함되지 않은 경우뿐만 아니라, 자연어 입력에 기초하여 콘텐트 정보가 획 득되지 않은 경우에도, 입력 콘텐트를 결정하기 위한 GUI가 디스플레이되도록 구현될 수 있다. 도 7a는 일 실시예에 따른 생성적 적대 신경망 (generative adversarial network; GAN) 을 설명하기 위한 도면 이다. 출력 콘텐트 생성부는 출력 콘텐트를 생성하는 인공지능 모델을 포함할 수 있다. 도 7a를 참조하면, 출력 콘텐트 생성부는 GAN 모델 중 생성 (generative) 모델, 즉, 생성자를 포함할 수 있다. 출력 콘텐트 생성부는 GAN 모델 중 감별 (discriminative) 모델, 즉, 감별자를 더 포함할 수 있다. GAN 모델에서 생성자와 감별자, 두 개의 네트워크는 서로 적대적으로 트레이닝될 수 있다. 감별자는 생성자 에 의해 생성된 콘텐트를 위조로 감별할 수 있도록, 진정 콘텐트와 위조 콘텐트에 기초하여 트레이닝된다. 생성자는, 감별자가 위조로 감별하지 않게 속일 수 있는 콘텐트를, 입력된 데이터에 기초하여 생성한 다. 생성자와 감별자는 서로 균형점에 도달할 때까지 트레이닝될 수 있다. 두 개의 네트워크가 균형 점에 도달하면서도, 생성자가 진정 (real) 콘텐트의 데이터 확률 분포를 충분히 모사할 수 있도록, GAN의 손실함수가 적절히 수정될 수 있다. 생성자는 동작 S742에서 입력 콘텐트 및 베이스 콘텐트의 타겟 영역에 기초하여 출력 콘텐트를 생성할 수 있다. 균형점에 도달하였거나 가까워진 생성자에 의해 생성되는 출력 콘텐트는, 진정 콘텐트의 데이터 확 률 분포에 근사하는 데이터 확률 분포를 가질 수 있다. 출력 콘텐트가 감별자에 의해 진정 콘텐트로 감별 될 수 있도록, 생성자는 진정 콘텐트에 근사하는 확률 분포를 가지는 출력 콘텐트를 생성할 수 있다. 한편, 감별자는 동작 S744에서 타겟 영역을 기준으로, 출력 콘텐트의 위조 여부를 감별할 수 있다. 감별자 는 동작 S745에서 베이스 콘텐트의 전체 영역을 기준으로, 출력 콘텐트의 위조 여부를 감별할 수 있다. S780에서, 감별자의 감별 결과에 기초하여 생성자가 트레이닝될 수 있다. 감별자는 타겟 영역 및 전체 영역을 기준으로 위조 여부를 감별하도록 트레이닝되므로, 이러한 감별자를 속이기 위해 트레이닝 을 통해 생성자는, 타겟 영역 및 전체 영역 모두에서 진정 콘텐트로 감별될 수 있는 출력 콘텐트를 생성하 게 되므로, 이에 따라, 생성되는 출력 콘텐트의 품질이 향상될 수 있다. 도 4를 참조하면, 생성자에 의해 생성된 출력 콘텐트들이 감별자에 의해 진정 콘텐트로 감별될 확률 은 0.5에 수렴할 수 있다. 즉, 생성자에 의해 생성되는 출력 콘텐트들의 데이터 확률 분포는 진정 데이터 의 데이터 확률 분포에 근사할 수 있다. 출력 콘텐트들의 기준이 되는 진정 데이터는 베이스 콘텐트와 동일한 형식, 또는 동일한 플랫폼을 가질 수 있다. 즉, 베이스 콘텐트와 동일한 형식 또는 동일한 플랫폼에서 수집된 진정 데이터를 이용하여, 감별자가 트레이닝될 수 있다. 도 7b는 일 실시예에 따라 캡션을 이용하는 방법을 설명하기 위한 도면이다. 일 실시예에서, 감별자는 콘텐트의 캡션 및 자연어 입력의 텍스트 간의 유사도에 기초하여 더 트레이닝될 수 있다. GAN 모델의 생성자와 감별자가 모두 균형점에 도달한 이후, 생성자에 의해 동작 S742에서 생성된 출력 콘텐트를, 감별자가 위조로 감별할 확률은 0.5 에 수렴할 수 있다. 도 7b를 참조하면, 감별자는 동작 S746에서 출력 콘텐트의 캡션과, 자연어 입력의 텍스트 간의 유사도를 기준으로 출력 콘텐 트의 위조 여부를 감별할 수 있다. 구체적으로, 생성자에 의해 생성된 출력 콘텐트에 대한 캡션이 캡션 생 성부에 의해 생성될 수 있고, 생성된 캡션 및 사용자의 자연어 입력의 텍스트 간의 유사도가 유사도 계산부에 의해 계산될 수 있다. 동작 S746에서, 유사도가 미리 결정된 조건을 충족하지 않는 경우, 감별자는 생성된 출력 콘텐트를 위조 콘텐트로 감별할 수 있다. 동작 S746에서, 유사도가 미리 결정된 조건을 충족하는 경우, 감 별자는 생성된 출력 콘텐트를 진정 콘텐트로 감별할 수 있다. 일 실시예에 따르면, 감별자가 자연어 입력의 텍스트 및 출력 콘텐트의 캡션 간의 유사도에 기초하여 더 트레이닝될 수 있다. 나아가, 이러한 감별자 를 속이기 위해 동작 S780에서 생성자 또한 더 트레이닝되므로, 생성자가 이전의 출력 콘텐트와 는 다른 출력 콘텐트를 생성하는 과정이 반복될 수 있고, 이는 출력 콘텐트의 향상으로 이어질 수 있다. 도 8은 일 실시예에 따라 입력 콘텐트의 속성을 변경하는 방법을 설명하기 위한 도면이다. 도 8을 참조하면, 사용자 입력 에 의해 부모님의 사진이 입력 콘텐트로서 선택될 수 있다. 베이스 콘 텐트에서 사용자 입력 에 기초하여 타겟 영역이 설정될 수 있다. \"make them stand here\"의 자 연어 입력이 전자 디바이스에서 수신되는 경우, 전자 디바이스는 자연어 입력에 기초하여 사용자 의도 정보를 획득할 수 있다. 자연어 입력 중 \"make\"는 액션 정보에 대응할 수 있다. 자연어 입력 중 \"them\"은 콘텐트 정보 에 대응할 수 있다. 자연어 입력 중 콘텐트 정보를 수식하는 단어인 \"stand\"가 콘텐트의 속성을 나타내는 속성 정보로서 식별될 수 있다. 일 실시예에서, 출력 콘텐트는 콘텐트 속성 정보에 기초하여 생성될 수 있다. 출력 콘텐트는 베이스 콘텐트의 타겟 영역에 생성될 수 있다. 출력 콘텐트의 속성은 콘텐트 속성 정보에 의해 표현되 는 속성과 동일할 수 있다. 출력 콘텐트의 속성은 입력 콘텐트의 속성으로부터 변경될 수 있다. 콘텐 트의 속성은, 콘텐트의 자세, 표정, 메이크업, 헤어, 의상, 착용 액세서리, 스타일, 등일 수 있으나, 이에 제한 되지 않는다. 콘텐트의 속성은, 콘텐트 내 객체의 자세, 표정, 메이크업, 헤어, 의상, 착용 액세서리, 스타일, 등일 수 있다. 예를 들어, 도 8을 참조하면, 입력 콘텐트의 속성, 예를 들어, 자세는 앉은 자세이고, 출력 콘텐트의 속성은 선 자세일 수 있다. 입력 콘텐트의 속성을 변경하는 방법은 도 9 및 10을 참조하여 후술할 것이다. 일 실시예에서, 출력 콘텐트는 기존 베이스 콘텐트와 조화되도록 생성될 수 있다. 출력 콘텐트 는 베이스 콘텐트의 타겟 영역 또는 전체 영역을 참조함으로써, 베이스 콘텐트의 속성, 또는 베 이스 콘텐트 내 객체의 속성과 동일한 속성을 갖는 출력 콘텐트가 생성될 수 있다. 예를 들어, 도 8 을 참조하면, 베이스 콘텐트 내 객체들은 모두 하얀 옷을 입고 있으므로, 출력 콘텐트 내 객체들 또 한 하얀 옷을 입고 있는 것으로 생성될 수 있다. 즉, 입력 콘텐트 내 객체들은 다른 색상의 옷을 입고 있 지만, 출력 콘텐트가 생성된 베이스 콘텐트 또는 그 타겟 영역에서는, 객체들이 하얀 옷을 입고 있는 것으로 표현될 수 있다. 예를 들어, 도 8을 참조하면, 베이스 콘텐트 내 객체들은 모두 서있으므로, 출력 콘텐트 내 객체들 또한 서있는 것으로 생성될 수 있다. 즉, 입력 콘텐트 내 객체들은 앉아 있지 만, 출력 콘텐트가 생성된 베이스 콘텐트 또는 그 타겟 영역에서는, 객체들이 서있는 것으로 표 현될 수 있다. 출력 콘텐트로 하여금, 베이스 콘텐트와 동일한 속성을 가질 수 있게 하는 방법은, 도 9 및 10을 참조하여 후술할 것이다. 도 9는 일 실시예에 따른 GAN의 콘텐트 생성 방법을 설명하기 위한 도면이다. 일 실시예에서, 콘텐트 속성 정보는 콘텐트 정보를 수식하는 자연어 표현으로부터 획득될 수 있다. 콘텐트 속성 정보는, 입력 콘텐트에서 사용자가 변경하고자 하는 일부 속성에 관할 수 있다. 예를 들어, 입력 콘텐트가 사람 인 경우, 사람의 자세, 표정, 메이크업, 헤어, 의상, 착용 액세서리, 스타일, 등 중에서 사용자가 변경하고자 하는 속성이, 콘텐트 속성 정보로서 식별될 수 있다. 도 9를 참조하면, 출력 콘텐트 생성부는 2개의 생성자, 즉, 제 1 생성자(242a)와 제 2 생성자(242b)를 포함할 수 있다. 나아가, 출력 콘텐트 생성부는 2개의 감별자, 즉, 제 1 감별자(244a)와 제 2 감별자(244b)를 포함할 수 있다. 동작 S942a에서 제 1 생성자(242a)는 A속성의 입력 콘텐트에 기초하여 B속성의 출력 콘텐트를 생성할 수 있다. 즉, 제 1 생성자(242a)는 A속성의 진정 콘텐트에 기초하여 B속성의 위조 콘텐트를 생성할 수 있다. 제 1 생성자 (242a)는 B속성의 출력 콘텐트를 생성하도록 트레이닝되어 있을 수 있다. 동작 S942b에서 제 1 생성자(242a)에 의해 생성된 B속성의 출력 콘텐트에 기초하여, 제 2 생성자(242b)는 다시 A속성의 콘텐트를 재구성할 수 있다. 즉, 제 2 생성자(242b)는, 위조 콘텐트인 B속성의 출력 콘텐트로부터 최초 콘텐트를 재구성할 수 있다. 제 2 생성자(242b)는 A속성의 출력 콘텐트를 생성하도록 트레이닝되어 있을 수 있 다. 제 2 생성자(242b)가 최초 콘텐트, 즉, A속성의 입력 콘텐트에 가까운 콘텐트를 재구성할 수 있도록, 제 1 생성자(242a)는 출력 콘텐트 생성 시 입력 콘텐트에서 A속성을 B속성으로만 변경하고 그 외의 부분들에 대한 변 경은 억제될 수 있다. 최초 콘텐트를 재구성함으로써, 최초 콘텐트와 무관한 콘텐트가 생성되는 것이 방지될 수 있다. 재구성된 콘텐트가 입력 콘텐트에 근사할수록 손실값이 감소될 수 있고, 콘텐트 간 순환 일관성이 유지되 는 것으로 해석될 수 있다. 한편, 제 1 생성자(242a)에 의해 생성된 B속성의 출력 콘텐트의 위조 여부는, 동작 S944a에서 제 1 감별자 (244a)에 의해 감별될 수 있다. 제 1 감별자(244a)는 위조 감별을 위해, B속성의 진정 콘텐트를 이용하여 트레 이닝되어 있을 수 있다. 동작 S980a에서 제 1 감별자(244a)의 감별 결과에 기초하여, 제 1 생성자(242a)가 트레 이닝될 수 있다. 제 2 생성자(242b)에 의해 재구성된 A속성의 콘텐트의 위조 여부는 동작 S944b에서 제 2 감별자(244b)에 의해 감별될 수 있다. 제 2 감별자(244b)는 위조 감별을 위해, A속성의 진정 콘텐트를 이용하여 트레이닝되어 있을 수 있다. 동작 980b에서 제 2 감별자(244b)의 감별 결과에 기초하여, 제 2 생성자(242b)가 트레이닝될 수 있다. 일 실시예에 따르면, 입력 콘텐트에서 A속성이 B속성으로 변경되는 동안 나머지 속성들의 일관성이 유지될 수 있다. 일 실시예에 따르면, 콘텐트의 속성에 대한 라벨링 없이도, 입력 콘텐트에서 A속성이 B속성으로 변경될 수 있다. 도 10은 일 실시예에 따른 GAN의 콘텐트 생성 방법을 설명하기 위한 도면이다. 도 10을 참조하면, 출력 콘텐트 생성부는 생성자 및 감별자를 포함할 수 있다. 동작 S1042에서 생성자는 속성 라벨 및 입력 콘텐트에 기초하여 출력 콘텐트를 생성할 수 있다. 속성 라벨 은 출력 콘텐트가 감별자에 의해 분류될 수 있는 속성의 라벨을 가리킨다. 즉, 생성자는 속성 라벨 및 입력 콘텐트를 입력받아 출력 콘텐트를 생성할 수 있다. 생성자는, 감별자에 의해 진정 콘텐트로 감별되고, 해당 속성 라벨로 분류될 수 있는 콘텐트를 생성하도록 트레이닝될 수 있다. 동작 S1043에서 생성자는, 동작 S1042과 동일한 속성 라벨 및 출력 콘텐트에 기초하여 콘텐트를 재구성할 수 있다. 즉, 생성자는 최초의 속성 라벨 및 생성된 출력 콘텐트를 입력받아 콘텐트를 재구성할 수 있다. 생성자가 최초의 콘텐트, 즉, 입력 콘텐트에 가까운 콘텐트를 재구성할 수 있도록, 생성자는 생성된 출력 콘텐트가 해당 속성 라벨로 분류될 수 있는 정도로만 입력 콘텐트를 변형할 수 있다. 다른 속성 라벨로 분 류될 정도까지 입력 콘텐트가 변형되지 않을 수 있으므로, 특정 속성으로 분류되는 출력 콘텐트의 생성이 가능 해질 수 있다. 재구성된 콘텐트가 입력 콘텐트에 근사할수록 손실값이 감소될 수 있다. 생성자에 의해 생성된 출력 콘텐트의 위조 여부는, 동작 S1044에서 감별자에 의해 감별될 수 있다. 출력 콘텐트가 진정 콘텐트로 감별되는 경우, 동작 S1045에서 감별자는 출력 콘텐트의 속성을 분류할 수 있다. 감별자는 위조 감별을 위해, 진정 콘텐트 및 위조 콘텐트를 입력받아 감별하고, 진정 콘텐트인 경우 콘텐트의 속성을 분류하여 라벨링함으로써, 트레이닝될 수 있다. 동작 S1080에서 감별자의 감별 결과에 기초하여, 생성자가 트레이닝될 수 있다. 일 실시예에 따르면, 입력 콘텐트에서 A속성이 B속성으로 변경되는 동안 나머지 속성들의 일관성이 유지될 수 있다. 설명의 편의를 위해, 도 10은 하나의 속성을 기준으로 설명하였으나, 감별자가 분류하는 속성의 개 수에 따라, 입력 콘텐트에서 변형할 수 있는 속성의 개수도 증가할 수 있다. 도 11은 일 실시예에 따라 사용자의 피드백을 이용하는 방법을 설명하기 위한 도면이다. 도 11을 참조하면, 전자 디바이스는 출력 콘텐트가 합성된 베이스 콘텐트, 출력 콘텐트, 또는 입력 콘텐 트의 타겟 영역에 대응하는 출력 콘텐트의 일 영역에 대한 사용자 피드백을 수신할 수 있다. 일 실시예에 서, 사용자 피드백이 수신되는 경우, 출력 콘텐트의 생성 과정이 반복될 수 있다. 예를 들어, 부정적인 사용자 피드백이 수신되는 경우, 전술된 출력 콘텐트의 캡션 및 자연어 입력 간의 유사도가 소정 조건을 충족하지 않는 경우와 마찬가지로, 출력 콘텐트의 생성 과정이 반복될 수 있다. 일 실시예에 따르면, 사용자 피드백이 인공지능 모델에 반영될 수 있어, 사용자에게 개인화된 결과가 제공될 수 있다. 도 12는 일 실시예에 따라 사용자의 피드백에 대응하는 예시적인 GUI를 도시한다. 도 12를 참조하면, 전자 디바이스는 출력 콘텐트가 합성된 베이스 콘텐트, 출력 콘텐트, 또는 입력 콘텐 트의 타겟 영역에 대응하는 출력 콘텐트의 영역에 대한 사용자 피드백을 수신할 수 있다. 일 실시 예에서, 사용자 피드백이 수신되는 경우, 콘텐트의 속성을 변경하기 위한 GUI를 디스플레이할 수 있다. 예를 들어, 출력 콘텐트의 일 속성에 대한 부정적인 사용자 피드백이 수신되는 경우, 출력 콘텐트에서 해당 속 성만을 변경하기 위해, 사용자가 원하는 속성을 선택할 수 있는 GUI 가 디스플레이될 수 있다. 도 13은 일 실시예에 따라 애플리케이션의 작업영역에서 생성된 콘텐트를 설명하기 위한 도면이다. 도 13을 참조하면, 검은 구름의 콘텐트가 입력 콘텐트로서 결정되어 있고, 전자 디바이스가 \"put this cloud to fit in here\"의 자연어 입력을 수신할 수 있다. 나아가, 타겟 영역들 (1312a, 1312b, 1312c, 및, 1312d) 에 대한 사용자 입력이 수신되는 경우, 전자 디바이스는 해당 타겟 영역들 (1312a, 1312b, 1312c, 및, 1312d) 에서 출력 콘텐트를 생성할 수 있다. 인공지능 모델을 이용하여, 출력 콘텐트는 베이스 콘텐트에 연관되 어 생성될 수 있다. 도 13에 도시된 바와 같이, 작업영역이 다른 객체, 예를 들어, 하얀 원을 포함하는 경우, 출력 콘텐트는 해당 객체를 포함하는 작업영역에 연관되어 생성될 수 있다. 출력 콘텐트와 작업영 역의 연관 관계는, 자연어 입력으로부터 결정되는 사용자 의도 정보에 기초하여 획득될 수 있다. 예를 들 어, \"to fit in here\"에 기초하여, 작업영역의 하얀 원을 참조하여, 그에 어울리는 하얀 구름이 출력 콘 텐트로서 생성될 수 있다. 즉, 검은 구름의 최초의 입력 콘텐트으로부터 색상이 변경된 출력 콘텐트가 생 성될 수 있다. 도 14는 일 실시예에 따라 애플리케이션의 작업영역에 적응적으로 생성된 콘텐트를 설명하기 위한 도면이다. 도 14를 참조하면, 가방을 든 여성 실루엣이 입력 콘텐트로서 결정되어 있고, 전자 디바이스가 \"make her to hold an arrow like the left one\"의 자연어 입력을 수신할 수 있다. 나아가, 타겟 영역 에 대한 사 용자 입력이 수신되는 경우, 전자 디바이스는 인공지능 모델을 이용하여 해당 타겟 영역 에서 출력 콘텐 트를 생성할 수 있다. 도 14에 도시된 바와 같이, 작업영역이 이미 다른 객체, 예를 들어, 화살표를 든 남성 실루엣을 포함하는 경우, 출력 콘텐트는 해당 객체를 포함하는 작업영역에 연관되어 생성될 수 있다. 예를 들어, \"to hold an arrow like the left one\"에 기초하여, 화살표를 든 남성 실루엣을 참조하여, 그에 어울리 는 출력 콘텐트가 생성될 수 있다. 도 15는 일 실시예에 따라 콘텐트의 애니메이션을 생성하는 방법을 설명하기 위한 도면이다. 일 실시예에 따르면, 베이스 콘텐트가 애플리케이션의 작업영역인 경우, 그 애플리케이션의 API에 기초하 여 출력 콘텐트가 생성될 수 있다. 애플리케이션의 API는 애니메이션에 연관된 커맨드들을 불러올 수 있다. 애 니메이션에 연관된 커맨드들은 예를 들어, 애니메이션의 동작 파라미터 (나타내기, 강조, 끝내기, 이동), 동작 파라미터의 시작점과 종료점, 동작 파라미터의 이동 방향, 동작 파라미터의 이동 시점, 동작 파라미터의 이동 속도, 등을 설정하기 위한 커맨드들일 수 있다. 출력 콘텐트는 입력 콘텐트와 연관된 애니메이션일 수 있다. 입력 콘텐트는 작업 객체일 수 있다. 전술된 인공지능 모델, 예를 들어, GAN 모델을 이용하여 입력 콘텐트와 연관된 애니메이션이 생성될 수 있다. GAN 모델의 생성자는, 입력 콘텐트, 애플리케이션의 API, 및 사용자 의도 정보를 입력받아, 입력 콘텐트가 사용자의 의도에 따라 움직일 수 있도록, API를 통해 제공되는 커맨드들을 조합하여, 입력 콘텐 트에 연관된 애니메이션을 생성할 수 있다. 예를 들어, 도 15를 참조하면, 작업영역에서 입력 콘텐트가 선택되고, \"apply an animation where this number is counted\"의 자연어 입력이 전자 디바이스에서 수신되는 경우, \"apply an animation\"로부터 애 니메이션 생성이 액션 정보로서 식별될 수 있다. 애니메이션 생성을 나타내는 액션 정보를 식별하는 것에 응답 하여, 해당 입력 콘텐트에 연관된 애니메이션이 생성될 수 있다. 타겟영역은 작업영역에서 입력 콘 텐트가 배치된 영역으로 설정될 수 있으나, 이에 제한되지 않는다. 사용자의 의도에 부합하는 애니메이션 은 콘텐트 속성 정보로부터 추론될 수 있다. 상기 자연어 입력에서 \"this number\"가 콘텐트 정보로서 식별될 수 있고, 콘텐트 정보를 수식하는 \"is counted\"가 콘텐트 속성 정보로서 식별될 수 있다. 애니메이션은, 입력 콘텐 트, 애플리케이션의 API, 및 콘텐트 속성 정보로서 식별된 \"is counted\"에 기초하여 생성될 수 있다. 전자 디바이스는, 애니메이션이 적용된 콘텐트에 대한 캡션을 생성할 수 있다. 비디오 캡셔닝 모델을 이 용하여, 콘텐트의 애니메이션을 묘사하는 캡션이 생성될 수 있다. 생성된 캡션 및 자연어 입력의 텍스트 간의 유사도에 기초하여, 애니메이션 생성 과정이 반복될 수 있다. 이에 따라, 사용자의 의도에 부합하는 애니 메이션이 생성될 수 있다. 일 실시예에 따르면, 사용자가 애플리케이션의 커맨드들을 알지 못하더라도, 자연어 입력을 통해 편리하게 콘텐트에 애니메이션을 적용할 수 있다. 콘텐트의 애니메이션은 작업영역 의 프리젠테이션 동안에 동작할 수 있다. 도 16은 일 실시예에 따라 생성된 예시적인 애니메이션을 설명하기 위한 도면이다. 베이스 콘텐트는 애플리케이션의 작업영역일 수 있고, 입력 콘텐트는 작업 객체일 수 있다. 예를 들어, 작업 객체는 애플리케이션에 의해 지원되는 도구에 의해 생성된 것일 수 있다. 도 16을 참조하면, 작업영역에서 작업 객체가 선택되고, \"apply an animation where this graph grows laterally\"의 자연어 입력이 전자 디바이스에서 수신되는 경우, \"apply an animation\"로부터 애니메이션 생성이 액션 정보로서 식별될 수 있다. 애니메이션 생성을 나타내는 액션 정보를 식별하는 것에 응답하여, 해당 입력 콘텐트에 연관된 애니메이션이 생성될 수 있다. 사용자가 원하는 애니메이션은 콘텐트 속성 정보로부터 추론될 수 있다. 상기 자연어 입력에서 \"this graph\"가 콘텐트 정보로서 식별될 수 있고, 콘텐트 정보를 수식하 는 \"grows laterally\"가 콘텐트 속성 정보로서 식별될 수 있다. 애니메이션은, 입력 콘텐트, 애플리케이 션의 API, 및 콘텐트 속성 정보로서 식별된 \"grows laterally\"에 기초하여 생성될 수 있다. 도 17은 일 실시예에 따른 전자 디바이스의 블록도이다. 전자 디바이스 또는 전자 디바이스 상에서 실행되는 소프트웨어는 여기에서 설명되거나 묘사된 방 법의 동작 또는 단계를 수행할 수 있고, 여기에서 설명되거나 묘사된 기능성을 제공할 수 있다. 전자 디바이스 에 대한 언급은 하나 이상의 전자 디바이스들을 적절하게 포함할 수 있다. 하나 이상의 전자 디바 이스들은 여기에서 설명되거나 묘사된 방법의 동작 또는 단계를 실질적인 공간적 또는 시간적 제약 없이 수행할 수 있다. 하나 이상의 전자 디바이스들은 방법의 동작 또는 단계를 실시간 또는 일괄 (batch) 모 드로 수행할 수 있다. 전자 디바이스는 스마트폰, 태블릿, 휴대폰, PDA (personal digital assistant), 미디어 플레이어, PMP (Portable Multimedia Player), 전자책 단말기, 디지털방송용 단말기, PC (Personal Computer), 노트북 (laptop), 마이크로 서버, GPS (global positioning system) 장치, 네비게이션, 키오스크, MP3 플레이어, 스마 트 TV, 디지털 카메라 및 기타 모바일, 또는, 비모바일 컴퓨팅 장치일 수 있으나, 이에 제한되지 않는다. 전자 디바이스는 엔드 유저 디바이스일 수 있다. 전자 디바이스는 프로세서 및 메모리를 포함할 수 있다. 전자 디바이스의 소프트웨어 모듈들, 예를 들어, 프로그램 모듈들은 인스트럭션들의 집합으로서 메모리에 저장될 수 있고, 인스트럭션 들이 프로세서에 의해 실행됨으로써 대응하는 기능들이 수행될 수 있다. 전자 디바이스는 전술된 구성요소들의 수보다 더 많거나 더 적은 구성요소들을 포함할 수 있다. 예를 들어, 도 17을 참조하면, 전자 디 바이스는 입력부, 출력부, 프로세서, 통신 인터페이스, 마이크로폰, 및 메모리를 포함할 수 있다. 전자 디바이스의 프로세서는 학습 네트워크 모델을 생성하기 위한 AI 프로세서를 포함할 수 있으나, 이에 한정되는 것은 아니다. 일 실시예에 의하면, AI 프로세서는 프로세서와 별도의 칩으 로 구현될 수도 있다. 프로세서는 전술된 실시예들의 방법의 각 동작을 수행할 수 있다. 전자 디바이스의 입력부는 사용자 입력을 수신할 수 있다. 입력부는 사용자와 인터랙션하기 위한 유닛을 포함할 수 있다. 예를 들어, 입력부는, 키 패드(key pad), 돔 스위치 (dome switch), 터치 패드(접촉식 정전 용량 방식, 압력식 저항막 방식, 적외선 감지 방식, 표면 초음파 전도 방식, 적분식 장력 측 정 방식, 피에조 효과 방식 등), 터치 스크린, 조그 휠, 조그 스위치 등일 수 있으나 이에 한정되는 것은 아니 다. 전자 디바이스의 출력부는 오디오 신호, 비디오 신호, 또는 진동 신호 등 사용자에게 제공될 정보 를 출력할 수 있다. 출력부는 디스플레이부, 음향 출력부, 진동 모터 등을 포함할 수 있다. 음향 출력부는 통신 인터페이스로부터 수신되거나 메모리에 저장된 오디오 데이터를 출력한다. 또한, 음향 출력부는 전자 디바이스에서 수행되는 기능(예를 들어, 호신호 수신음, 메시 지 수신음, 알림음)과 관련된 음향 신호를 출력한다. 이러한 음향 출력부에는 스피커(speaker), 버저(Buzzer) 등이 포함될 수 있다. 진동 모터는 진동 신호를 출력할 수 있다. 예를 들어, 진동 모터는 오디오 데이터 또는 비디오 데이터(예컨대, 호신호 수신음, 메시지 수신음 등)의 출력에 대응하는 진동 신호를 출력할 수 있다. 또한, 진동 모터는 터치스 크린에 터치가 입력되는 경우 진동 신호를 출력할 수도 있다. 출력부는 대화형 인터페이스를 제공할 수 있다. 대화형 인터페이스는 메시지 창 또는 채팅 창 형태로 제 공될 수도 있고, 음성을 입/출력하는 형태로 제공될 수도 있으나, 이에 한정되는 것은 아니다. 전자 디바이스의 통신부는, 전자 디바이스와 다른 장치, 예를 들어, 서버 간의 통신을 하게 하는 하나 이상의 구성요소를 포함할 수 있다. 예를 들어, 통신 인터페이스는, 근거리 통신부, 이동 통신부 및 방송 수신부를 포함할 수 있다. 근거리 통신부(short-range wireless communication unit)는, 블루투스 통신부, BLE(Bluetooth Low Energy) 통 신부, 근거리 무선 통신부(Near Field Communication unit), WLAN(와이파이) 통신부, 지그비(Zigbee) 통신부, 적외선(IrDA, infrared Data Association) 통신부, WFD(Wi-Fi Direct) 통신부, UWB(ultra wideband) 통신부, Ant+ 통신부 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 이동 통신부는, 이동 통신망 상에서 기지국, 외부의 단말, 서버 중 적어도 하나와 무선 신호를 송수신한다. 여 기에서, 무선 신호는, 음성 호 신호, 화상 통화 호 신호 또는 문자/멀티미디어 메시지 송수신에 따른 다양한 형 태의 데이터를 포함할 수 있다. 방송 수신부는, 방송 채널을 통하여 외부로부터 방송 신호 및/또는 방송 관련된 정보를 수신한다. 방송 채널은 위성 채널, 지상파 채널을 포함할 수 있다. 마이크로폰은, 외부의 음향 신호를 입력 받아 전기적인 음성 데이터로 처리한다. 예를 들어, 마이크로폰 은 외부 디바이스 또는 화자로부터 음향 신호를 수신할 수 있다. 마이크로폰는 외부의 음향 신호를 입력 받는 과정에서 발생 되는 잡음(noise)를 제거하기 위한 다양한 잡음 제거 알고리즘을 이용할 수 있다. 메모리는, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수도 있고, 입/출력되는 데이터들을 저장할 수도 있다. 메모리에 저장된 프로그램들은 그 기능에 따라 복수의 모듈들로 분류될 수 있다. 본 개시에 따른 인공지능과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그 래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세 서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도 록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기에서, 학습을 통해 만들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으 로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의 미한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비 지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습 (reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 실시예들에 따른 방법들은 하드웨어, 소프트웨어, 또는 하드웨어와 소프트웨어의 조합의 형태로 구현될 수 있다. 실시예가 소프트웨어를 이용하여 구현되는 경우, 하나 이상의 프로그램(소프트웨어, 소프트웨어 모듈)이 이용될 수 있다. 프로그램은 컴퓨터 판독 가능 기록 매체에 포함될 수 있으나, 이에 제한되지 않는다. 프로그램 은 컴퓨터 프로그램 제품에 포함될 수도 있다. 프로그램을 저장하는 컴퓨터 판독 가능 기록 매체가 컴퓨터 프로 그램 제품에 포함될 수도 있다. 프로그램은, 전자 디바이스 내의 하나 이상의 프로세서에 의해 실행 가능하도록 구성된다. 하나 이상의 프로그램은 인스트럭션들을 포함할 수 있고, 인스트럭션들은 전자 디바이스에 포함된 하 나 이상의 프로세서에 의해 실행되어, 전자 디바이스로 하여금 실시예들에 따른 방법들을 실행하게 할 수 있다. 여기에서, 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독가능 매체는 컴퓨터 저장 매체 및통신 매체를 모두 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 인스트럭션, 데이터 구조, 프로그램 모 듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독가능 인스트럭션, 데이터 구조, 프로그 램 모듈, 또는 반송파와 같은 변조된 데이터 신호의 기타 데이터, 또는 기타 전송 메커니즘을 포함하며, 임의의 정보 전달 매체를 포함한다. 또한, 일부 실시예는 컴퓨터에 의해 실행되는 컴퓨터 프로그램과 같은 컴퓨터에 의해 실행가능한 인스트럭션을 포함하는 컴퓨터 프로그램 또는 컴퓨터 프로그램 제품으로도 구현될 수 있다. 컴 퓨터가 읽을 수 있는 복수의 기록 매체가 네트워크로 연결된 컴퓨터 시스템들에 분산되어 있을 수 있으며, 분산 된 기록 매체들에 저장된 데이터, 예를 들면 프로그램의 인스트럭션들 및 코드가 적어도 하나의 컴퓨터에 의해 실행될 수 있다. 도면들에 도시된 다양한 요소들의 기능들은 적절한 소프트웨어와 관련되어 소프트웨어를 실행할 수 있는 하드웨 어뿐만 아니라 전용 하드웨어의 이용을 통해 제공될 수 있다. 프로세서에 의해 제공될 때, 이런 기능은 단일 전 용 프로세서, 단일 공유 프로세서, 또는 일부가 공유될 수 있는 복수의 개별 프로세서에 의해 제공될 수 있다. 여기에서, \"프로세서\" 또는 \"제어부\" 등의 용어는, 소프트웨어를 실행할 수 있는 하드웨어를 배타적으로 가리키 는 것으로 해석되지 말아야 하며, 제한 없이, 디지털 신호 프로세서(DSP) 하드웨어, 소프트웨어를 저장하기 위 한 판독 전용 메모리(ROM), 랜덤 액세스 메모리(RAM), 및 비휘발성 저장 디바이스를 묵시적으로 포함할 수 있다. 여기에서, \"쪋부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 프로세 서 또는 회로와 같은 하드웨어 컴포넌트, 또는 소프트웨어 컴포넌트이거나, 하드웨어 컴포넌트와 소프트웨어 컴 포넌트의 결합으로 구현될 수 있다. 여기에서, \"적어도 하나의\"와 같은 표현은, 구성요소들의 리스트 전체를 수식하고, 그 리스트의 구성요소들을 개별적으로 수식하지 않는다. 예를 들어, \"A, B, 및 C 중 적어도 하나\"는 오직 A, 오직 B, 오직 C, A와 B 모두, B와 C 모두, A와 C 모두, A와 B와 C 전체, 또는 그 조합을 가리킨다. 여기에서, 실시예에 따른 방법을 구성하는 동작들이나 단계들은, 이들의 순서가 기재되거나 반하는 기재가 없다 면, 적당한 순서로 수행될 수 있고, 이들이 기재된 순서로 실시예가 한정되는 것은 아니다. 모든 예들 또는 예 시적인 용어 (예들 들어, 등등) 의 사용은 단순히 실시예들을 상세히 설명하기 위한 것으로서 특허청구범위에 의해 한정되지 않는 이상 상기 예들 또는 예시적인 용어로 인해 본 개시의 범위가 한정되는 것은 아니다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가지로 분산된 것으로 설명 되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 개시의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 개시의 범위에 포함되는 것으 로 해석되어야 한다. 도면 도면1 도면2 도면3 도면4 도면5 도면6a 도면6b 도면7a 도면7b 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17"}
{"patent_id": "10-2019-0160008", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 콘텐트의 반복적 생성을 개략적으로 설명하기 위한 도면이다. 도 2는 일 실시예에 따른 전자 디바이스의 블록도이다. 도 3은 일 실시예에 따른 방법의 흐름도이다. 도 4는 일 실시예에 따라 생성된 예시적인 콘텐트들을 설명하기 위한 도면이다. 도 5는 일 실시예에 따라 사용자 의도 정보를 획득하는 방법을 설명하기 위한 도면이다. 도 6a는 일 실시예에 따라 입력 콘텐트를 결정하는 방법의 흐름도이다. 도 6b는 일 실시예에 따라 입력 콘텐트의 선택을 위한 예시적인 그래픽 유저 인터페이스 (graphic user interface; GUI) 를 도시한다. 도 7a는 일 실시예에 따른 생성적 적대 신경망 (generative adversarial network; GAN) 을 설명하기 위한 도면 이다. 도 7b는 일 실시예에 따라 캡션을 이용하는 방법을 설명하기 위한 도면이다. 도 8은 일 실시예에 따라 입력 콘텐트의 속성을 변경하는 방법을 설명하기 위한 도면이다. 도 9는 일 실시예에 따른 GAN의 콘텐트 생성 방법을 설명하기 위한 도면이다. 도 10은 일 실시예에 따른 GAN의 콘텐트 생성 방법을 설명하기 위한 도면이다. 도 11은 일 실시예에 따라 사용자의 피드백을 이용하는 방법을 설명하기 위한 도면이다. 도 12는 일 실시예에 따라 사용자의 피드백에 대응하는 예시적인 GUI를 도시한다. 도 13은 일 실시예에 따라 애플리케이션의 작업영역에서 생성된 콘텐트를 설명하기 위한 도면이다.도 14는 일 실시예에 따라 애플리케이션의 작업영역에 적응적으로 생성된 콘텐트를 설명하기 위한 도면이다. 도 15는 일 실시예에 따라 콘텐트의 애니메이션을 생성하는 방법을 설명하기 위한 도면이다. 도 16은 일 실시예에 따라 생성된 예시적인 애니메이션을 설명하기 위한 도면이다. 도 17은 일 실시예에 따른 전자 디바이스의 블록도이다."}
