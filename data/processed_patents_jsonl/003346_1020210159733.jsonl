{"patent_id": "10-2021-0159733", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0081276", "출원번호": "10-2021-0159733", "발명의 명칭": "AI 가속기, 캐시 메모리 및 이를 이용한 캐시 메모리 동작 방법", "출원인": "한국전자통신연구원", "발명자": "한진호"}}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "각각 복수개의 쓰레드들을 이용하여 딥러닝 연산을 수행하는 프로세서들; 및상기 프로세서들로 인스트럭션(instruction)을 제공하기 위한 제1 계층 인스트럭션 캐시(L0 Instruction Cache)및 맵핑된 메모리(mapped memory)의 복수개의 영역들과 맵핑되는 제2 계층 캐시(L1 Cache)를 포함하는 캐시 메모리를 포함하는, AI 가속기(AI accelerator)."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 프로세서들은 각각프로세서 코어(processor core) 및 GEMM 연산기(GEneral Matrix Multiplication operator)를 포함하는, AI 가속기."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서,상기 GEMM 연산기는복수개의 GEMM-ALU(GEneral Matrix Multiplication - Arithmetic Logic Unit)들을 포함하는, AI 가속기."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 3에 있어서,상기 복수개의 영역들은상기 프로세서들이 공유하는 공통 영역; 및상기 프로세서들 각각의 상기 쓰레드들 중 일부에 상응하는 프라이빗 영역을 포함하고,상기 공통 영역은 인스트럭션 영역, 메타데이터 영역, 로컬 영역 및 글로벌 영역을 포함하는, AI 가속기."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서,상기 제1 계층 인스트럭션 캐시 및 상기 제2 계층 캐시는복수개의 캐시 뱅크들로 구성되는, AI 가속기."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 5에 있어서,상기 제1 계층 인스트럭션 캐시는상기 제2 계층 캐시의, 상기 인스트럭션 영역에 상응하는 캐시 뱅크들에 대응되는, AI 가속기."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "에 있어서,상기 인스트럭션 영역, 상기 메타데이터 영역, 상기 로컬 영역, 상기 글로벌 영역 및 상기 프라이빗 영역 중 적어도 하나 이상에 상응하는 상기 캐시 뱅크들의 개수는 상기 연산의 종류에 따라 달라지는, AI 가속기."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 1에 있어서,상기 제1 계층 인스트럭션 캐시는각각 상기 쓰레드들 각각에 상응하는 복수개의 캐시 뱅크들을 포함하는, AI 가속기."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 3에 있어서,상기 캐시 메모리는인스트럭션 버스, 메타데이터 버스, 프라이빗 버스, 로컬 버스 및 글로벌 버스를 통하여 상기 프로세서들과 연결되는, AI 가속기."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "청구항 10에 있어서,상기 인스트럭션 버스는 상기 프로세서 코어의 인스트럭션 포트에 연결되고,상기 메타데이터 버스 및 프라이빗 버스는 상기 프로세서 코어 내의 ALU(Arithmetic Logic Unit)의 로드 포트(load port)에 연결되고,상기 프라이빗 버스 및 로컬 버스는 상기 프로세서 코어 내의 ALU의 스토어 포트(store port)에 연결되고,상기 로컬 버스 및 글로벌 버스는 상기 GEMM-ALU들의 입력 포트들과 연결되는, AI 가속기."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "청구항 11에 있어서,상기 인스트럭션 버스는 상기 제1 계층 인스트럭션 캐시에 연결되고,상기 메타데이터 버스는 상기 제2 계층 캐시 내의 상기 캐시 뱅크들 중 상기 메타데이터 영역에 상응하는 일부와 연결되고,상기 프라이빗 버스는 상기 제2 계층 캐시 내의 상기 캐시 뱅크들 중 상기 프라이빗 영역에 상응하는 일부와 연결되고,상기 로컬 버스는 상기 제2 계층 캐시 내의 상기 캐시 뱅크들 중 상기 로컬 영역에 상응하는 일부와 연결되고,상기 글로벌 버스는 상기 제2 계층 캐시 내의 상기 캐시 뱅크들 중 상기 글로벌 영역에 상응하는 일부와 연결되는, AI 가속기."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "딥러닝 연산을 수행하기 위한 프로세서들로 인스트럭션(instruction)을 제공하기 위한 제1 계층 인스트럭션 캐시(L0 Instruction Cache); 및 맵핑된 메모리(mapped memory)의 복수개의 영역들과 맵핑되는 제2 계층 캐시(L1 Cache)를 포함하는 캐시메모리."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "청구항 13에 있어서,공개특허 10-2022-0081276-4-상기 복수개의 영역들은상기 프로세서들이 공유하는 공통 영역; 및상기 프로세서들 각각의 상기 쓰레드들 중 일부에 상응하는 프라이빗 영역을 포함하고,상기 공통 영역은 인스트럭션 영역, 메타데이터 영역, 로컬 영역 및 글로벌 영역을 포함하는, 캐시 메모리."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "청구항 14에 있어서,상기 제1 계층 인스트럭션 캐시 및 상기 제2 계층 캐시는 복수개의 캐시 뱅크들로 구성되고,상기 제1 계층 인스트럭션 캐시는 상기 제2 계층 캐시의, 상기 인스트럭션 영역에 상응하는 캐시 뱅크들에 대응되는, 캐시 메모리."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "청구항 15에 있어서,상기 인스트럭션 영역, 상기 메타데이터 영역, 상기 로컬 영역, 상기 글로벌 영역 및 상기 프라이빗 영역 중 적어도 하나 이상의 주소는 수행되는 연산의 종류에 따라 달라지는, 캐시 메모리."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "청구항 16에 있어서,상기 인스트럭션 영역, 상기 메타데이터 영역, 상기 로컬 영역, 상기 글로벌 영역 및 상기 프라이빗 영역 중 적어도 하나 이상에 상응하는 상기 캐시 뱅크들의 개수는 상기 연산의 종류에 따라 달라지는, 캐시 메모리."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "청구항 13에 있어서,상기 제1 계층 인스트럭션 캐시는각각 상기 쓰레드들 각각에 상응하는 복수개의 캐시 뱅크들을 포함하고, 상기 캐시 메모리는 인스트럭션 버스,메타데이터 버스, 프라이빗 버스, 로컬 버스 및 글로벌 버스를 통하여 상기 프로세서들과 연결되는, 캐시 메모리."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "맵핑된 메모리의 복수개의 영역들을 설정하는 단계;상기 복수개의 영역들과 제2 계층 캐시(L1 Cache) 내의 캐시 뱅크들을 맵핑하는 단계;복수개의 프로세서들이, 버스를 통해 상기 제2 계층 캐시(L1 Cache) 및 상기 제2 계층 캐시의 인스트럭션 영역에 맵핑되는 제1 계층 인스트럭션 캐시(L0 Instruction Cache)를 포함하는 캐시 메모리를 이용하여 딥러닝 연산을 수행하는 단계를 포함하는 캐시 메모리 동작 방법."}
{"patent_id": "10-2021-0159733", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "청구항 19에 있어서,상기 복수개의 영역들은상기 프로세서들이 공유하는 공통 영역; 및상기 프로세서들 각각의 상기 쓰레드들 중 일부에 상응하는 프라이빗 영역을 포함하고,상기 공통 영역은 인스트럭션 영역, 메타데이터 영역, 로컬 영역 및 글로벌 영역을 포함하는, 캐시 메모리 동작방법."}
{"patent_id": "10-2021-0159733", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일실시예에 따른 AI 가속기는, 각각 복수개의 쓰레드들을 이용하여 딥러닝 연산을 수행하는 프로세서 들; 및 상기 프로세서들로 인스트럭션(instruction)을 제공하기 위한 제1 계층 인스트럭션 캐시(L0 Instruction Cache) 및 맵핑된 메모리(mapped memory)의 복수개의 영역들과 맵핑되는 제2 계층 캐시(L1 Cache)를 포함하는 캐시 메모리를 포함한다."}
{"patent_id": "10-2021-0159733", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능(Artificial Intelligence) 가속기 및 이를 위한 캐시 메모리에 관한 것으로, 특히 딥러닝 가속을 위한 복수개의 프로세서 코어들을 위한 연산 효율성 및 병렬성이 높은 캐시 구조에 관한 것이다."}
{"patent_id": "10-2021-0159733", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "딥러닝 가속기 설계시 연산 효율성을 높이기 위해, 하나의 칩 내의 제한된 면적에 많은 연산기들과 캐시들을 포 함시키려는 노력들이 있어 왔다. 특히, 외부 메모리에 대한 접근을 줄여서 연산 성능을 높이려는 접근법들이 있어 왔고, 이러한 접근법들 중 하나는 딥러닝 가속에 특화된 이기종 코어(heterogeneous core)들을 칩 내에 집 적하고, OpenCL(Open Computing Language) 등의 프레임워크를 이용하여 이기종 코어들의 병렬 연산을 위한 코딩 을 수행하는 것이다. 특히, OpenCL은 메모리 영역을 프라이빗(Private), 로컬(Local) 및 글로벌(Global) 메모리 영역들로 나누어 관 리하고 있고, 이 영역들의 데이터에 효율적으로 접근하는 것은 연산 성능 및 연산 효율성 향상에 중요한 요인이 된다. 따라서, 복수의 프로세서 코어들이 칩 내에 집적된 상황에서, 제한된 메모리 용량으로 병렬 성능을 극대화할 수 있는 새로운 캐시 메모리 기술의 필요성이 절실하게 대두된다."}
{"patent_id": "10-2021-0159733", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 제한된 메모리 용량으로 AI 가속기의 연산효율성 및 병렬성을 극대화하는 것이다. 또한, 본 발명의 목적은 각각 GEMM(GEneral Matrix Multiplication) 연산기를 포함하고 있는 딥러닝 가속을 위 한 복수개의 프로세서들(프로세서 코어들)을 위한 효율적이고 병렬성이 높은 캐시 메모리를 제공하는 것이다."}
{"patent_id": "10-2021-0159733", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명에 따른 AI 가속기는, 각각 복수개의 쓰레드들을 이용하여 딥러닝 연산을 수행하는 프로세서들; 및 상기 프로세서들로 인스트럭션(instruction)을 제공하기 위한 제1 계층 인스트럭션 캐 시(L0 Instruction Cache) 및 맵핑된 메모리(mapped memory)의 복수개의 영역들(areas)과 맵핑되는 제2 계층 캐시(L1 Cache)를 포함하는 캐시 메모리를 포함한다. 이 때, 프로세서들은 각각 프로세서 코어(processor core) 및 GEMM 연산기(GEneral Matrix Multiplication operator)를 포함할 수 있다. 이 때, GEMM 연산기는 복수개의 GEMM-ALU(GEneral Matrix Multiplication - Arithmetic Logic Unit)들을 포함 할 수 있다. 이 때, 복수개의 영역들(areas)은 상기 프로세서들이 공유하는 공통 영역 및 상기 프로세서들 각각의 상기 쓰레 드들 중 일부에 상응하는 프라이빗 영역을 포함할 수 있다. 이 때, 공통 영역은 인스트럭션 영역, 메타데이터 영역, 로컬 영역 및 글로벌 영역을 포함할 수 있다. 이 때, 상기 제1 계층 인스트럭션 캐시 및 상기 제2 계층 캐시는 복수개의 캐시 뱅크들로 구성될 수 있다. 이 때, 상기 제1 계층 인스트럭션 캐시는 상기 제2 계층 캐시의 상기 인스트럭션 영역에 상응하는 캐시 뱅크들 에 대응될 수 있다. 이 때, 상기 인스트럭션 영역, 상기 메타데이터 영역, 상기 로컬 영역, 상기 글로벌 영역 및 상기 프라이빗 영 역 중 적어도 하나 이상의 주소는 수행되는 연산의 종류에 따라 달라질 수 있다. 이 때, 상기 인스트럭션 영역, 상기 메타데이터 영역, 상기 로컬 영역, 상기 글로벌 영역 및 상기 프라이빗 영 역 중 적어도 하나 이상에 상응하는 상기 캐시 뱅크들의 개수는 상기 연산의 종류에 따라 달라질 수 있다. 이 때, 상기 제1 계층 인스트럭션 캐시는 각각 상기 쓰레드들 각각에 상응하는 복수개의 캐시 뱅크들을 포함할 수 있다.이 때, 상기 캐시 메모리는 인스트럭션 버스, 메타데이터 버스, 프라이빗 버스, 로컬 버스 및 글로벌 버스를 통 하여 상기 프로세서들과 연결될 수 있다. 이 때, 상기 인스트럭션 버스는 상기 프로세서 코어의 인스트럭션 포트에 연결되고, 상기 메타데이터 버스 및 프라이빗 버스는 상기 프로세서 코어 내의 ALU(Arithmetic Logic Unit)의 로드 포트(load port)에 연결되고, 상기 프라이빗 버스 및 로컬 버스는 상기 프로세서 코어 내의 ALU의 스토어 포트(store port)에 연결되고, 상기 로컬 버스 및 글로벌 버스는 상기 GEMM-ALU들의 입력 포트들과 연결될 수 있다. 이 때, 상기 인스트럭션 버스는 상기 제1 계층 인스트럭션 캐시에 연결되고, 상기 메타데이터 버스는 상기 제2 계층 캐시 내의 상기 캐시 뱅크들 중 상기 메타데이터 영역에 상응하는 일부와 연결되고, 상기 프라이빗 버스는 상기 제2 계층 캐시 내의 상기 캐시 뱅크들 중 상기 프라이빗 영역에 상응하는 일부와 연결되고, 상기 로컬 버 스는 상기 제2 계층 캐시 내의 상기 캐시 뱅크들 중 상기 로컬 영역에 상응하는 일부와 연결되고, 상기 글로벌 버스는 상기 제2 계층 캐시 내의 상기 캐시 뱅크들 중 상기 글로벌 영역에 상응하는 일부와 연결될 수 있다. 또한, 본 발명의 일실시예에 따른 캐시 메모리는, 딥러닝 연산을 수행하기 위한 프로세서들로 인스트럭션 (instruction)을 제공하기 위한 제1 계층 인스트럭션 캐시(L0 Instruction Cache); 및 맵핑된 메모리(mapped memory)의 복수개의 영역들과 맵핑되는 제2 계층 캐시(L1 Cache)를 포함한다. 이 때, 상기 복수개의 영역들은 상기 프로세서들이 공유하는 공통 영역; 및 상기 프로세서들 각각의 상기 쓰레 드들 중 일부에 상응하는 프라이빗 영역을 포함할 수 있다. 이 때, 상기 공통 영역은 인스트럭션 영역, 메타데 이터 영역, 로컬 영역 및 글로벌 영역을 포함할 수 있다. 이 때, 상기 제1 계층 인스트럭션 캐시 및 상기 제2 계층 캐시는 복수개의 캐시 뱅크들로 구성되고, 상기 제1 계층 인스트럭션 캐시는 상기 제2 계층 캐시의, 상기 인스트럭션 영역에 상응하는 캐시 뱅크들에 대응될 수 있 다. 이 때, 상기 인스트럭션 영역, 상기 메타데이터 영역, 상기 로컬 영역, 상기 글로벌 영역 및 상기 프라이빗 영 역 중 적어도 하나 이상의 주소는 수행되는 연산의 종류에 따라 달라질 수 있다. 이 때, 상기 인스트럭션 영역, 상기 메타데이터 영역, 상기 로컬 영역, 상기 글로벌 영역 및 상기 프라이빗 영 역 중 적어도 하나 이상에 상응하는 상기 캐시 뱅크들의 개수는 상기 연산의 종류에 따라 달라질 수 있다. 이 때, 상기 제1 계층 인스트럭션 캐시는 각각 상기 쓰레드들 각각에 상응하는 복수개의 캐시 뱅크들을 포함하 고, 상기 캐시 메모리는 인스트럭션 버스, 메타데이터 버스, 프라이빗 버스, 로컬 버스 및 글로벌 버스를 통하 여 상기 프로세서들과 연결될 수 있다. 또한, 본 발명의 일실시예에 따른 캐시 메모리 동작 방법은, 맵핑된 메모리의 복수개의 영역들을 설정하는 단계; 상기 복수개의 영역들과 제2 계층 캐시(L1 Cache) 내의 캐시 뱅크들을 맵핑하는 단계; 및 복수개의 프로 세서들이, 버스를 통해 상기 제2 계층 캐시(L1 Cache) 및 상기 제2 계층 캐시의 인스트럭션 영역에 맵핑되는 제 1 계층 인스트럭션 캐시(L0 Instruction Cache)를 포함하는 캐시 메모리를 이용하여 딥러닝 연산을 수행하는 단 계를 포함한다. 이 때, 상기 복수개의 영역들은 상기 프로세서들이 공유하는 공통 영역; 및 상기 프로세서들 각각의 상기 쓰레드들 중 일부에 상응하는 프라이빗 영역을 포함할 수 있다. 이 때, 상기 공 통 영역은 인스트럭션 영역, 메타데이터 영역, 로컬 영역 및 글로벌 영역을 포함할 수 있다."}
{"patent_id": "10-2021-0159733", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 제한된 메모리 용량으로도 AI 가속기의 연산효율성 및 병렬성을 극대화할 수 있다. 또한, 본 발명은 각각 GEMM(GEneral Matrix Multiplication) 연산기를 포함하고 있는 딥러닝 가속을 위한 복수 개의 프로세서들(프로세서 코어들)을 위한 효율적이고 병렬성이 높은 캐시 메모리를 제공할 수 있다."}
{"patent_id": "10-2021-0159733", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2021-0159733", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 비록 \"제1\" 또는 \"제2\" 등이 다양한 구성요소를 서술하기 위해서 사용되나, 이러한 구성요소는 상기와 같은 용 어에 의해 제한되지 않는다. 상기와 같은 용어는 단지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사 용될 수 있다. 따라서, 이하에서 언급되는 제1 구성요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있다. 본 명세서에서 사용된 용어는 실시예를 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세 서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한다 (comprises)\" 또는 \"포함하는(comprising)\"은 언급된 구성요소 또는 단계가 하나 이상의 다른 구성요소 또는 단 계의 존재 또는 추가를 배제하지 않는다는 의미를 내포한다."}
{"patent_id": "10-2021-0159733", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어는 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 해석될 수 있다. 또한, 일반적으로 사용되는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예들을 상세히 설명하기로 하며, 도면을 참조하여 설명할 때 동일 하거나 대응하는 구성 요소는 동일한 도면 부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 도 1은 본 발명의 일실시예에 따른 AI 가속기를 나타낸 블록도이다. 도 1을 참조하면, 본 발명의 일실시예에 따른 AI 가속기는 캐시 메모리 및 복수개의 프로세서들(110, ..., 180)을 포함한다. 프로세서들(110, ..., 180)은 각각 복수개의 쓰레드들을 이용하여 딥러닝 연산을 수행한다. 캐시 메모리는 프로세서들(110, ..., 180)로 인스트럭션(instruction)을 제공하기 위한 제1 계층 인스트럭 션 캐시(L0 Instruction Cache) 및 맵핑된 메모리(mapped memory)의 복수개의 영역들과 맵핑되는 제2 계층 캐 시(L1 Cache)를 포함한다. 프로세서들(110, ..., 180)은 캐시 메모리로부터 데이터를 읽어오거나 캐시 메모리로 데이터를 라이 트(write)하여 딥러닝 연산을 수행하고, 디램(DRAM) 등에서 직접 데이터를 읽어오거나 디램 등으로 직접 데이터 를 쓰는 것에 비해 효율적으로 동작이 가능하다. 도 1에 도시된 예에서는 AI 가속기가 8개의 프로세서들(110, ..., 180)을 포함하는 경우를 예로 들었으나, AI 가속기은 8개 이하의 프로세서들을 포함할 수도 있고, 8개 이상의 프로세서들을 포함할 수도 있다. 이 때, 프로세서들(110, ..., 180)은 각각 복수개의 쓰레드들을 수행할 수 있다. 예를 들어, 프로세서들(110, ..., 180)은 각각 8개의 쓰레드들을 수행할 수 있다. 프로세서는 프로세서 코어(processor core) 및 GEMM 연산기(GEneral Matrix Multiplication operator)를 포함한다. 이 때, GEMM 연산기는 복수개의 GEMM-ALU(GEneral Matrix Multiplication - Arithmetic Logic Unit)들을 포함할 수 있다. 이 때, 프로세서의 프로세서 코어가 인스트럭션(instruction)을 읽기 위해, 프로세서의 프로세 서 코어의 ALU(Arithmetic Logic Unit)에서 로드/스토어(load/store)로 데이터를 읽거나 쓰기 위해, 프로 세서의 GEMM 연산기에서 다수의 GEMM-ALU들에게 연산에 사용되는 두 개의 입력들을 제공하기 위해 캐시 메모리에 접근할 수 있다. 마찬가지로, 프로세서는 프로세서 코어(processor core) 및 GEMM 연산기(GEneral Matrix Multiplication operator)를 포함한다. 이 때, GEMM 연산기는 복수개의 GEMM-ALU(GEneral Matrix Multiplication - Arithmetic Logic Unit)들을 포함할 수 있다. 이 때, 프로세서의 프로세서 코어가 인스트럭션(instruction)을 읽기 위해, 프로세서의 프로세 서 코어의 ALU(Arithmetic Logic Unit)에서 로드/스토어(load/store)로 데이터를 읽거나 쓰기 위해, 프로 세서의 GEMM 연산기에서 다수의 GEMM-ALU들에게 연산에 사용되는 두 개의 입력들을 제공하기 위해 캐 시 메모리에 접근할 수 있다. 예를 들어, 8개의 프로세서들(110, ..., 180)이 캐시 메모리를 사용하는 경우, 캐시 메모리는 최소한 총 8개의 인스트럭션 포트들, 8개의 로드/스토어(load/store) 포트들 및 16개의 데이터 포트들이 필요할 수 있 다. 이 때, 제2 계층 캐시에 맵핑되는 복수개의 영역들은 프로세서들(110, ..., 180)이 공유하는 공통 영역 및 프로 세서들(110, ..., 180) 각각의 상기 쓰레드들 중 일부에 상응하는 프라이빗 영역을 포함할 수 있다. 이 때, 공 통 영역은 인스트럭션 영역, 메타데이터 영역, 로컬 영역 및 글로벌 영역을 포함할 수 있다. 도 2는 도 1에 도시된 캐시 메모리의 일 예를 나타낸 블록도이다. 도 2를 참조하면, 캐시 메모리는 제1 계층 인스트럭션 캐시(L0 Instruction Cache) 및 제2 계층 캐 시(L1 Cache)를 포함한다. 제1 계층 인스트럭션 캐시는 프로세서들로 인스트럭션을 제공하기 위한 것으로, 복수개의 캐시 뱅크들 (ICACHE #0 ~ ICACHE #7)을 포함할 수 있다. 이 때, 복수개의 캐시 뱅크들(ICACHE #0 ~ ICACHE #7)은 프로세 서 코어들 내의 복수개의 쓰레드들(THREAD #0 ~ THREAD #7)에 대응될 수 있다. 제2 계층 캐시는 맵핑된 메모리(mapped memory)의 복수개의 영역들과 맵핑될 수 있다. 전술한 바와 같이 복수개의 영역들은 인스트럭션 영역, 메타데이터 영역, 로컬 영역, 글로벌 영 역 및 프라이빗 영역들을 포함할 수 있다. 예를 들어, 맵핑된 메모리(mapped memory)는 메모리 맵에 의하여 맵핑될 수 있고, DRAM(Dynamic Random Access Memory)에 상응하는 것일 수 있다. 도 2에 도시된 예에서, 맵핑된 메모리의 인스트럭션 영역, 메타데이터 영역, 로컬 영역 및 글로 벌 영역은 8개의 프로세서들에 의하여 공유될 수 있다. 이 때, 프라이빗 영역들은 각각 8개의 프로 세서 코어들 각각의 8개의 쓰레드들 각각에 상응하는 것일 수 있다. 이 때, 프라이빗 영역들은 8개의 프 로세서 코어들 및 8개의 쓰레드들의 조합(combination)에 해당하여 총 64개의 영역들일 수 있다. 이 때, 도 2에 도시된 인스트럭션 영역, 메타데이터 영역, 로컬 영역, 글로벌 영역 및 프 라이빗 영역들은 도 2에 도시된 바와 같이 _instr0, _instr1, _meta, _provate0 ~ _private63, _local, _global의 어드레스들(addresses)들로 설정될 수 있고, 이 어드레스들은 AI 가속기에서 수행되는 딥러닝 연산의 종류에 따라 다르게 설정될 수 있다. 즉, 인스트럭션 영역, 메타데이터 영역, 로컬 영역, 글로 벌 영역 및 프라이빗 영역들 중 적어도 하나 이상의 주소는 수행되는 연산의 종류에 따라 달라질 수 있다. 나아가, 전술한 어드레스들이 다르게 설정되어 복수개의 영역들의 크기가 달라지면, 캐시 메모리 내의 각 영역을 담당하는 캐시 뱅크들의 개수가 달라질 수 있다. 본 발명의 일실시예는 캐시 메모리와 복수개의 프로세서들 사이의 연결관계를 효율적으로 구성하기 위해 캐시 메모리를 2계층으로 구성하고 메모리 맵을 유동적으로 설정하여 각 영역을 담당하는 캐시 사이즈가 가변될 수 있다. 도 2에 도시된 제1 계층 인스트럭션 캐시 및 제2 계층 캐시는 각각 복수개의 캐시 뱅크들로 구성될 수 있다. 예를 들어, 캐시 뱅크들(cache banks)은 각각 32KB의 사이즈를 가질 수 있고, 독립된 캐시 컨트롤러(cache controller)를 가질 수 있다. 도 2에 도시된 예와 같이, 제1 계층 인스트럭션 캐시는 인스트럭션 영역에 상응하는 제2 계층 캐시 의 캐시 뱅크들(CACHE #0, CACHE #1)에 대응될 수 있다. 도 2에 도시된 예에서, 제1 계층 인스트럭션 캐시는 각각 프로세서 코어들의 8개의 쓰레드들(THREAD #0, ..., THREAD #7)에 상응하는 8개의 캐시 뱅크들(ICACHE #0, ..., ICACHE #7)을 포함할 수 있다. 이와 같 이 제1 계층 인스트럭션 캐시가 8개의 독립된 캐시 뱅크들로 구성되고, 각각의 독립된 캐시 뱅크들은 8개 의 프로세서들에서 특정 쓰레드에 의하여 공유되면, 캐시의 미스 레이트(miss rate)를 감소시킬 수 있다. 예를 들어, 제2 계층 캐시는 256개의 캐시 뱅크들(CACHE #0 ~ CACHE #255)로 구성될 수 있다. 이 때, 각 각의 캐시 뱅크들이 담당할 맵핑된 메모리 상의 영역은 가변적으로 설정가능할 수 있다. 즉, {_instr1 - _instr0}, {_meta - _instr1}, {_private[63~1] - _private0}, {_local - _private[63~1]} 및 {_global - _local}의 영역들의 크기에 따라 {_instr1 - _instr0}, {_meta - _instr1}, {_private[63~1] - _private0}, {_local - _private[63~1]} 및 {_global - _local} 영역들을 담당하는 제2 계층 캐시 내의 캐시 뱅크들과 각 영역들을 담당하는 캐시 뱅크들의 개수는 달라질 수 있다. 즉, 인스트럭션 영역, 메타데이터 영역 , 로컬 영역, 글로벌 영역 및 프라이빗 영역들 중 적어도 하나 이상에 상응하는 캐시 뱅크 들의 개수는 수행되는 연산의 종류에 따라 달라질 수 있다. 아래 표 1은 도 2에 도시된 바와 같이 복수개의 영역들과 제2 계층 캐시 내의 캐시 뱅크들을 맵핑한 예를 나타낸다. 표 1 AREA ADDRESS ASSIGNED CACHE _instr0 ~ _instr1 CACHE #0 ~ #1 _instr1 ~ _meta CACHE #2 ~ #3 _private0 ~ _private63 CACHE #4 ~ #67 _private63 ~ _local CACHE #68 ~ #252 _local ~ _global CACHE #253 ~ #255 복수개의 프로세서들과 캐시 메모리 사이의 포트들을 줄이고 효율적인 동작을 보장하기 위해 버스가 사용될 수 있다.본 발명의 일실시예에서 캐시 메모리는 인스트럭션 버스, 메타데이터 버스, 프라이빗 버스, 로컬 버스 및 글로벌 버스를 통하여 상기 프로세서들과 연결될 수 있다. 하기 표 2는 프로세서 포트들과 버스 사이의 연결관계의 일 예이다. 표 2 PROCESSOR PORT ASSIGNED BUS INSTRUCTION PORT INSTRUCTION BUS (IB) LOAD PORT METADATA BUS (MB), PRIVATE BUS (PB) STORE PORT PRIVATE BUS (PB), LOCAL BUS (LB) GEMM INPUT PORT #1 LOCAL BUS (LB), GLOBAL BUS (GB) GEMM INPUT PORT #2 LOCAL BUS (LB), GLOBAL BUS (GB) 표 2의 예에서, 인스트럭션 버스(INSTRUCTION BUS; IB)는 프로세서 코어들의 인스트럭션 포트들에 연결되고, 메 타데이터 버스(METADATA BUS; MB) 및 프라이빗 버스(PRIVATE BUS; PB)는 프로세서 코어들 내의 ALU들 (Arithmetic Logic Units)의 로드 포트들(load ports)에 연결되고, 프라이빗 버스(PRIVATE BUS; PB) 및 로컬 버스(LOCAL BUS; LB)는 프로세서 코어들 내의 ALU들의 스토어 포트들(store ports)에 연결되고, 로컬 버스 (LOCAL BUS; LB) 및 글로벌 버스(GLOBAL BUS; GB)는 GEMM-ALU들의 입력 포트들과 연결된다.이 때, 인스트럭션 버스(INSTRUCTION BUS; IB)는 제1 계층 인스트럭션 캐시에 연결되고, 메타데이터 버스(METADATA BUS; MB)는 제 2 계층 캐시 내의 캐시 뱅크들 중 메타데이터 영역에 상응하는 일부와 연결되고, 프라이빗 버스(PRIVATE BUS; PB)는 제2 계층 캐시 내의 캐시 뱅크들 중 프라이빗 영역에 상응하는 일부와 연결되고, 로컬 버스(LOCAL BUS; LB)는 제2 계층 캐시 내의 캐시 뱅크들 중 로컬 영역에 상응하는 일부와 연결되고, 글로벌 버스(GLOBAL BUS; GB)는 제2 계층 캐시 내의 캐시 뱅크들 중 글로벌 영역에 상응하는 일부와 연결될 수 있다. 즉, 인스트럭션 버스는 8개의 프로세서 코어들의 인스트럭션 포트들에 연결되고, 쓰레드 아이디에 따라 제1 계 층 인스트럭션 캐시의 해당 쓰레드 캐시 뱅크에 접근할 수 있도록 연결될 수 있다. 즉, 메타데이터 버스는 8의 프로세서 코어들의 로드 포트들에 연결되고, 메타데이터 영역에 대응되는 제2 계층 캐시 내의 256개의 캐시 뱅크들 중 일부에 연결될 수 있다. 즉, 프라이빗 버스는 8개의 프로세서 코어들의 로드/스토어 포트들에 연결되고, 프라이빗 영역에 대응되는 제2 계층 캐시 내의 256개의 캐시 뱅크들 중 일부에 연결될 수 있다. 즉, 로컬 버스는 8개의 프로세서 코어들의 스토어 포트와 각각 8개씩의 GEMM 첫 번째 입력 포트들 및 GEMM 두 번째 입력 포트들에 연결되고, 로컬 영역에 대응되는 제2 계층 캐시 내의 256개의 캐시 뱅크들 중 일부에 연결 될 수 있다. 즉, 글로벌 버스는 각각 8개씩의 GEMM 첫 번째 입력 포트들 및 GEMM 두 번째 입력 포트들에 연결되고, 글로벌 영역에 대응되는 제2 계층 캐시 내의 256개의 캐시 뱅크들 중 일부에 연결될 수 있다. 이와 같이, 각각의 버스가 제1 계층 인스트럭션 캐시 및 제2 계층 캐시 내의 독립된 총 264(8 + 256)개의 캐시 뱅크들에 모두 연결되면 버스 동작 주파수가 낮아질 수 밖에 없고, 따라서 264개의 캐시 뱅크들이 인스트럭션/ 메타데이터/프라이빗/로컬/글로벌 영역을 제한적으로 담을 수 있도록 설계되고 캐시 뱅크들 중 필요한 일부분만 이 버스에 연결되도록 하여 버스의 복잡도를 줄이고 캐시 메모리와 프로세서 사이의 통신 대역폭을 충분히 확보 할 수 있다. 도 3은 본 발명의 일실시예에 따른 버스를 통한 캐시 메모리와 복수개의 프로세서들의 연결 관계를 나타낸 블록 도이다. 도 3을 참조하면, 캐시 메모리 및 복수개의 프로세서들(110, ..., 180)은 버스를 통하여 연결되고, 버스는 인스트럭션 버스, 메타데이터 버스, 프라이빗 버스, 로컬 버스 및 글로벌 버 스를 포함한다. 제1 계층 인스트럭션 캐시는 인스트럭션 버스를 통해 프로세서 코어들과 연결되고, 제2 계층 개시 내의 캐시 뱅크들은 적절히 메타데이터 버스, 프라이빗 버스, 로컬 버스 및 글로벌 버스 를 통해 프로세서 코어들이나 GEMM 연산기들과 연결된다. 본 발명의 일실시예와 같은 캐시 메모리 구조 및 버스에 의하면 GEMM 연산기를 포함하는 딥러닝 가속을 위한 여 러 개의 프로세서들이 효율적으로 병렬 연산을 수행할 수 있다. 특히, 본 발명은 멀티-코어 및 멀티-GEMM 환경에서 효율성을 제공하고, Open치에서 정의한 프라이빗/로컬/글로 벌 메모리 영역과 인스트럭션/메타데이터 영역이 유동적으로 변할 수 있는 상황에서 제한된 캐시의 효율적 운용 이 가능하도록 한다. 나아가, 본 발명에 따르면 멀티-쓰레드 환경에서 쓰레드 스위치에 따른 캐시의 비효율적인 동작을 방지할 수 있 다. 도 4는 본 발명의 일실시예에 따른 캐시 메모리 동작 방법의 일 예를 나타낸 동작 흐름도이다. 도 4를 참조하면, 본 발명의 일실시예에 따른 캐시 메모리 동작 방법은 맵핑된 메모리의 복수개의 영역들을 설 정한다(S410). 이 때, 복수개의 영역들은 상기 프로세서들이 공유하는 공통 영역 및 상기 프로세서들 각각의 상기 쓰레드들 중 일부에 상응하는 프라이빗 영역을 포함할 수 있다. 이 때, 공통 영역은 인스트럭션 영역, 메타데이터 영역, 로 컬 영역 및 글로벌 영역을 포함할 수 있다. 또한, 본 발명의 일실시예에 따른 캐시 메모리 동작 방법은 상기 복수개의 영역들과 제2 계층 캐시(L1 Cache) 내의 캐시 뱅크들을 맵핑한다(S420). 또한, 본 발명의 일실시예에 따른 캐시 메모리 동작 방법은 복수개의 프로세서들이, 버스를 통해 상기 제2 계층 캐시(L1 Cache) 및 상기 제2 계층 캐시의 인스트럭션 영역에 맵핑되는 제1 계층 인스트럭션 캐시(L0 Instruction Cache)를 포함하는 캐시 메모리를 이용하여 딥러닝 연산을 수행한다(S430). 이상에서와 같이 본 발명에 따른 AI 가속기, 캐시 메모리 및 이를 이용한 캐시 메모리 동작 방법은 상기한 바와 같이 설명된 실시예들의 구성과 방법이 한정되게 적용될 수 있는 것이 아니라, 상기 실시예들은 다양한 변형이이루어질 수 있도록 각 실시예들의 전부 또는 일부가 선택적으로 조합되어 구성될 수도 있다."}
{"patent_id": "10-2021-0159733", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른 AI 가속기를 나타낸 블록도이다. 도 2는 도 1에 도시된 캐시 메모리의 일 예를 나타낸 블록도이다.도 3은 본 발명의 일실시예에 따른 버스를 통한 캐시 메모리와 복수개의 프로세서들의 연결 관계를 나타낸 블록 도이다. 도 4는 본 발명의 일실시예에 따른 캐시 메모리 동작 방법의 일 예를 나타낸 동작 흐름도이다."}
