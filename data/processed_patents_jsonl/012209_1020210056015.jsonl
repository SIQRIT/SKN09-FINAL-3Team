{"patent_id": "10-2021-0056015", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0148647", "출원번호": "10-2021-0056015", "발명의 명칭": "다중 소자 기반의 시냅스를 이용한 신경망 학습 장치 및 방법", "출원인": "포항공과대학교 산학협력단", "발명자": "황현상"}}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "다중 소자 기반의 시냅스를 이용한 신경망 학습 장치에 있어서,신경망의 가중치를 제1정밀도에 기반하여 업데이트 하기 위한 복수의 제1저항 소자를 포함하는 제1시냅스부; 및상기 신경망의 가중치를 상기 제1정밀도 대비 높은 정밀도로 업데이트 하기 위한 복수의 제2저항 소자를 포함하는 제2시냅스부,를 포함하는, 신경망 학습 장치."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1저항 소자의 컨덕턴스 값은 상기 제2저항 소자의 컨덕턴스 값 대비 큰 것인, 신경망 학습 장치."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 가중치는,상기 신경망의 학습 진행 수준에 기초하여 상기 제1시냅스부 또는 상기 제2시냅스부에 기초하여 선택적으로 업데이트 되는 것을 특징으로 하는, 신경망 학습 장치."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 제1시냅스부는 상기 학습 진행 수준에 기초하여 상대적으로 상기 신경망의 초반부 학습에 관여하고,상기 제2시냅스부는 상기 학습 진행 수준에 기초하여 상대적으로 상기 신경망의 후반부 학습에 관여하는 것인,신경망 학습 장치."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서,상기 신경망은 미리 설정된 복수의 에포크만큼 반복 학습되는 것을 특징으로 하고,상기 복수의 에포크 중 어느 하나의 에포크가 완료될 때마다 상기 신경망의 정확도(Accuracy) 변화를 산출하여상기 학습 진행 수준을 평가하는 학습 평가부,를 더 포함하는 것인, 신경망 학습 장치."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 어느 하나의 에포크를 통해 상기 제1시냅스부에 의해 상기 가중치가 업데이트된 후 상기 학습 평가부에 의해 평가된 상기 정확도 변화가 미리 설정된 임계값 이하이면, 상기 어느 하나의 에포크 이후의 에포크에서는 상기 제2시냅스부에 의해 상기 가중치가 업데이트 되는 것인, 신경망 학습 장치."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제2항에 있어서,상기 제1저항 소자의 컨덕턴스 값은 상기 제2저항 소자의 컨덕턴스 값에 미리 설정된 이득 계수를 곱한 것인,공개특허 10-2022-0148647-3-신경망 학습 장치."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 복수의 제1저항 소자 및 상기 복수의 제2저항 소자 중 적어도 하나는 교차 구조(Crossbar array)로 구비되는 것인, 신경망 학습 장치."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "다중 소자 기반의 시냅스를 이용한 신경망 회로에 있어서,복수의 인공 뉴런; 및상기 복수의 인공 뉴런 간의 가중치를 제1정밀도에 기반하여 업데이트 하기 위한 복수의 제1저항 소자 및 상기가중치를 상기 제1정밀도 대비 높은 정밀도로 업데이트 하기 위한 복수의 제2저항 소자를 포함하는 적어도 하나의 시냅스 유닛,을 포함하는 신경망 회로."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "다중 소자 기반의 시냅스를 이용한 신경망 학습 방법에 있어서,신경망의 가중치에 기초하여 추론을 수행하는 단계;상기 추론 결과에 기초하여 오류를 계산하는 단계; 및상기 오류에 기초하여 상기 가중치를 업데이트 하는 단계,를 포함하고,상기 업데이트 하는 단계는,상기 가중치를 제1정밀도에 기반하여 업데이트 하기 위한 복수의 제1저항 소자를 포함하는 제1시냅스부 또는 상기 가중치를 상기 제1정밀도 대비 높은 정밀도로 업데이트 하기 위한 복수의 제2저항 소자를 포함하는 제2시냅스부를 선택적으로 이용하여 상기 가중치를 업데이트 하는 것인, 신경망 학습 방법."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 제1저항 소자의 컨덕턴스 값은 상기 제2저항 소자의 컨덕턴스 값 대비 큰 것인, 신경망 학습 방법."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 업데이트 하는 단계는,상기 신경망의 학습 진행 수준에 기초하여 상대적으로 상기 신경망의 초반부 학습에 상기 제1시냅스부를 이용하고, 상기 학습 진행 수준에 기초하여 상대적으로 상기 신경망의 후반부 학습에 상기 제2시냅스부를 이용하는 것인, 신경망 학습 방법."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 신경망 학습 방법은 미리 설정된 복수의 에포크만큼 반복 수행되는 것을 특징으로 하고,상기 복수의 에포크 중 어느 하나의 에포크가 완료될 때마다 상기 신경망의 정확도(Accuracy) 변화를 산출하여상기 학습 진행 수준을 평가하는 단계,를 더 포함하는 것인, 신경망 학습 방법.공개특허 10-2022-0148647-4-청구항 14 제13항에 있어서,상기 평가하는 단계에 의해, 상기 어느 하나의 에포크를 통해 상기 제1시냅스부에 의해 상기 가중치가 업데이트된 후 상기 정확도 변화가 미리 설정된 임계값 이하인 것으로 평가되면,상기 업데이트 하는 단계는,상기 어느 하나의 에포크 이후의 에포크에서는 상기 제2시냅스부를 이용하여 상기 가중치를 업데이트 하는것인, 신경망 학습 방법."}
{"patent_id": "10-2021-0056015", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 제1저항 소자의 컨덕턴스 값은 상기 제2저항 소자의 컨덕턴스 값에 미리 설정된 이득 계수를 곱한 것인,신경망 학습 방법."}
{"patent_id": "10-2021-0056015", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "다중 소자 기반의 시냅스를 이용한 신경망 학습 장치 및 방법이 개시되며, 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치는, 신경망의 가중치를 제1정밀도에 기반하여 업데이트 하기 위한 복수 의 제1저항 소자를 포함하는 제1시냅스부 및 상기 신경망의 가중치를 상기 제1정밀도 대비 높은 정밀도로 업데이 트 하기 위한 복수의 제2저항 소자를 포함하는 제2시냅스부를 포함할 수 있다."}
{"patent_id": "10-2021-0056015", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본원은 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치 및 방법에 관한 것이다. 예를 들면, 본원은 RRAM 기반 하이브리드 시냅스를 이용한 신경망 훈련 가속화 기법에 관한 것이다."}
{"patent_id": "10-2021-0056015", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(Artificial Intelligence, AI) 기술은 컴퓨터 비전, 자연어 인식, 의료와 같은 다양한 분야에서 널리 보급되고 있다. 이러한 AI 기술의 발전은 딥러닝 알고리즘의 발전을 통해 이루어졌으나 폰 노이만(von Neumann) 아키텍처를 기반으로 하는 종래의 디지털 컴퓨팅 방식은 지속적으로 증가하는 신경망과 연산의 크기와 복잡성을 견딜 수 없어 에너지 효율성 측면에서 한계를 보이고 있다. 한편, 이러한 신경망의 크기 증가 및 연산 복잡도를 극복하기 위하여 하드웨어 신경망(HNN)과 같이 뇌에서 영감 을 받은 뉴로모픽 컴퓨팅의 개발이 이루어졌으며, 특히 저항 메모리(Resistive RAM, RRAM)는 컨덕턴스 값으로 다중 레벨의 가중치를 저장할 수 있어 시냅스 소자로 활용될 수 있으며, 이러한 저항 메모리 어레이의 병렬 업 데이트 방식은 벡터 매트릭스 곱셈(vector-matrix multiplication, VMM)과 함께 신경망 훈련을 가속화할 수 있 는 잠재력을 가지고 있다. 그러나, 저항 메모리는 제한된 수의 컨덕턴스 상태만 나타낼 수 있으며, 보다 많은 가중치 비트를 저항 메모리 를 통해 나타내기 위하여 종래의 다양한 연구에서는 아날로그 뉴로모픽 시스템의 시냅스에 다수의 셀(multiple cells)을 활용하였으나, 여러 장치가 하나의 시냅스로 작동하기 때문에 기존의 병렬 업데이트 방식을 완전히 적 용할 수 없었고, 뉴로모픽 시스템이 각 시냅스에 대해 업데이트 할 장치를 결정하고 해당하는 가중치 업데이트 양을 계산해야 하기 때문에 결과적으로 시냅스 유닛 아키텍처의 가중치 업데이트 프로세스에는 과도한 시간과 리소스가 요구되었다. 따라서, 빠르고 정확한 하드웨어 기반 신경망의 학습을 위해서는 피드백 정보의 양을 잃지 않고 병렬 업데이트 방법을 사용하여 시냅스 단위 아키텍처를 훈련할 수 있는 기법의 개발이 요구된다. 본원의 배경이 되는 기술은 한국공개특허공보 제10-2017-0080441호에 개시되어 있다."}
{"patent_id": "10-2021-0056015", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본원은 전술한 종래 기술의 문제점을 해결하기 위한 것으로서, 저항 소자를 포함하는 시냅스 유닛으로 신경망의 가중치를 업데이트하되, 유닛 내의 특정 시냅스 어레이만 선택적으로 업데이트 함으로써 뉴로모픽 하드웨어의 학습을 가속하고, 정확도를 높일 수 있는 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치 및 방법을 제공 하는 것을 목적으로 한다.본원은 전술한 종래 기술의 문제점을 해결하기 위한 것으로서, 컨덕턴스(전도도) 값의 정밀도 측면에서 물리적 한계가 있는 저항성 소자를 시냅스로 활용하여 신경망의 학습을 높은 정확도로 달성하려는 것을 목적으로 한다. 다만, 본원의 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2021-0056015", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치는, 신경망의 가중치를 제1정밀도에 기반하여 업데이트 하기 위한 복수의 제1저항 소자 를 포함하는 제1시냅스부 및 상기 신경망의 가중치를 상기 제1정밀도 대비 높은 정밀도로 업데이트 하기 위한 복수의 제2저항 소자를 포함하는 제2시냅스부를 포함할 수 있다. 또한, 상기 제1저항 소자의 컨덕턴스 값은 상기 제2저항 소자의 컨덕턴스 값 대비 큰 것일 수 있다. 또한, 상기 가중치는, 상기 신경망의 학습 진행 수준에 기초하여 상기 제1시냅스부 또는 상기 제2시냅스부에 기 초하여 선택적으로 업데이트 되는 것일 수 있다. 또한, 상기 제1시냅스부는 상기 학습 진행 수준에 기초하여 상대적으로 상기 신경망의 초반부 학습에 관여할 수 있다. 또한, 상기 제2시냅스부는 상기 학습 진행 수준에 기초하여 상대적으로 상기 신경망의 후반부 학습에 관여할 수 있다. 또한, 상기 신경망은 미리 설정된 복수의 에포크만큼 반복 학습되는 것일 수 있다. 또한, 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치는, 상기 복수의 에포크 중 어느 하나의 에포크가 완료될 때마다 상기 신경망의 정확도(Accuracy) 변화를 산출하여 상기 학습 진행 수준을 평가하는 학습 평가부를 포함할 수 있다. 또한, 상기 어느 하나의 에포크를 통해 상기 제1시냅스부에 의해 상기 가중치가 업데이트된 후 상기 학습 평가 부에 의해 평가된 상기 정확도 변화가 미리 설정된 임계값 이하이면, 상기 어느 하나의 에포크 이후의 에포크에 서는 상기 제2시냅스부에 의해 상기 가중치가 업데이트 되는 것일 수 있다. 또한, 상기 제1저항 소자의 컨덕턴스 값은 상기 제2저항 소자의 컨덕턴스 값에 미리 설정된 이득 계수를 곱한 것일 수 있다. 또한, 상기 복수의 제1저항 소자 및 상기 복수의 제2저항 소자 중 적어도 하나는 교차 구조(Crossbar array)로 구비될 수 있다. 한편, 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 회로는, 복수의 인공 뉴런 및 상기 복수의 인공 뉴런 간의 가중치를 제1정밀도에 기반하여 업데이트 하기 위한 복수의 제1저항 소자 및 상기 가중 치를 상기 제1정밀도 대비 높은 정밀도로 업데이트 하기 위한 복수의 제2저항 소자를 포함하는 적어도 하나의 시냅스 유닛을 포함할 수 있다. 한편, 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 방법은, 신경망의 가중치에 기 초하여 추론을 수행하는 단계, 상기 추론 결과에 기초하여 오류를 계산하는 단계 및 상기 오류에 기초하여 상기 가중치를 업데이트 하는 단계를 포함할 수 있다. 또한, 상기 업데이트 하는 단계는, 상기 가중치를 제1정밀도에 기반하여 업데이트 하기 위한 복수의 제1저항 소 자를 포함하는 제1시냅스부 또는 상기 가중치를 상기 제1정밀도 대비 높은 정밀도로 업데이트 하기 위한 복수의 제2저항 소자를 포함하는 제2시냅스부를 선택적으로 이용하여 상기 가중치를 업데이트 할 수 있다. 또한, 상기 업데이트 하는 단계는, 상기 신경망의 학습 진행 수준에 기초하여 상대적으로 상기 신경망의 초반부 학습에 상기 제1시냅스부를 이용하고, 상기 학습 진행 수준에 기초하여 상대적으로 상기 신경망의 후반부 학습 에 상기 제2시냅스부를 이용할 수 있다. 또한, 상기 신경망 학습 방법은 미리 설정된 복수의 에포크만큼 반복 수행될 수 있다. 또한, 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 방법은, 상기 복수의 에포크 중 어느 하나의 에포크가 완료될 때마다 상기 신경망의 정확도(Accuracy) 변화를 산출하여 상기 학습 진행 수준을평가하는 단계를 포함할 수 있다. 또한, 상기 업데이트 하는 단계는, 상기 평가하는 단계에 의해, 상기 어느 하나의 에포크를 통해 상기 제1시냅 스부에 의해 상기 가중치가 업데이트된 후 상기 정확도 변화가 미리 설정된 임계값 이하인 것으로 평가되면, 상 기 어느 하나의 에포크 이후의 에포크에서는 상기 제2시냅스부를 이용하여 상기 가중치를 업데이트 할 수 있다. 상술한 과제 해결 수단은 단지 예시적인 것으로서, 본원을 제한하려는 의도로 해석되지 않아야 한다. 상술한 예 시적인 실시예 외에도, 도면 및 발명의 상세한 설명에 추가적인 실시예가 존재할 수 있다."}
{"patent_id": "10-2021-0056015", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본원의 과제 해결 수단에 의하면, 저항 소자를 포함하는 시냅스 유닛으로 신경망의 가중치를 업데이트하 되, 유닛 내의 특정 시냅스 어레이만 선택적으로 업데이트 함으로써 뉴로모픽 하드웨어의 학습을 가속하고, 정 확도를 높일 수 있는 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치 및 방법을 제공할 수 있다. 전술한 본원의 과제 해결 수단에 의하면, 컨덕턴스(전도도) 값의 정밀도 측면에서 물리적 한계가 있는 저항성 소자를 시냅스로 활용하여 신경망의 학습을 높은 정확도로 달성할 수 있다. 전술한 본원의 과제 해결 수단에 의하면, 단일 소자의 경우 컨덕턴스(전도도) 값의 정밀도 측면에 물리적 한계 가 있는 저항성 소자를 시냅스로 이용하여 뉴럴 네트워크 하드웨어의 높은 정확도의 학습을 달성함으로써 저항 메모리 어레이 기반의 뉴럴 네트워크, 뉴로모픽 하드웨어를 자율주행 및 이미지 처리와 같은 다양한 분야의 인 공지능 시스템에 적용할 수 있다. 다만, 본원에서 얻을 수 있는 효과는 상기된 바와 같은 효과들로 한정되지 않으며, 또 다른 효과들이 존재할 수 있다."}
{"patent_id": "10-2021-0056015", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본원이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본원의 실시예를 상세히 설명한다. 그러나 본원은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본원을 명확하게 설명하기 위해서 설명과 관계없는 부분 은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본원 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\" 또는 \"간접적으로 연결\"되어 있는 경우 도 포함한다. 본원 명세서 전체에서, 어떤 부재가 다른 부재 \"상에\", \"상부에\", \"상단에\", \"하에\", \"하부에\", \"하단에\" 위치 하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재가 존 재하는 경우도 포함한다. 본원 명세서 전체에서, 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 본원은 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치 및 방법에 관한 것이다. 예를 들면, 본원은 RRAM 기반 하이브리드 시냅스를 이용한 신경망 훈련 가속화 기법에 관한 것이다. 도 1은 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치를 포함하는 신경망 회로 의 개략적인 구성도이다. 도 1을 참조하면, 본원의 일 실시예에 따른 신경망 회로는 인공 뉴런 및 시냅스 유닛을 포함할 수 있다. 여기서, 시냅스 유닛은 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치(이하, '신경망 학습 장치'라 한다.)에 대응하는 것일 수 있다. 구체적으로, 도 1을 참조하면, 본원의 일 실시예에 따른 신경망 회로는 복수의 인공 뉴런 및 복수의 인공 뉴런 간의 가중치를 제1정밀도에 기반하여 업데이트 하기 위한 복수의 제1저항 소자 및 복수의 인공 뉴런 간의 가중치를 제1정밀도 대비 높은 정밀도로 업데이트 하기 위한 복수의 제2저항 소자를 포함하는 적어도 하나의 시냅스 유닛을 포함할 수 있다. 본원의 일 실시예에 따르면, 신경망 회로는 하드웨어 신경망(Hardware Neural Network, HNN)로서 신호 전파 (signal propagation)를 기초로 하여 동작하며, 여기서 신경 신호(neuronal signal)는 신경망 회로 내의 소정의 위치의 전압 값에 매핑될 수 있다. 한편, 도 1의 (a)를 참조하면, 신경망 회로에 포함된 복수의 인공 뉴런은 신호 전파 방향을 기초로 입 력 측 뉴런과 출력 측 뉴런을 포함할 수 있으며, 시냅스 유닛인 신경망 학습 장치는 입력 측 뉴런과 출력 측 뉴런 사이에서 입력 측 뉴런으로부터 전달된 전압 신호에 옴의 법칙을 통해 컨턱턴스 값을 곱하여 출력 측 뉴런으로 전달하는 저항성 소자의 집합(어레이)일 수 있다. 이와 관련하여, 출력 측 뉴런에서는 시냅스 유닛을 통해 연결된 적어도 하나의 입력 측 뉴런으로부터 시냅스 유 닛에 의해 연결 가중치가 곱해져 전파된 신호 키르히호프의 법칙(Kirchhoff's Law)에 의해 축적될 수 있다. 참고로, 이러한 하드웨어 신경망(HNN)의 동작 방식에 관한 사항은 통상의 기술자에게 자명하므로 구체적 인 설명은 생략하도록 한다. 또한, 도 1의 (b)를 참조하면, 본원의 일 실시예에 따르면, 신경망 학습 장치는 교차 구조(Crossbar array)로 구비될 수 있다. 달리 말해, 신경망 학습 장치는 신경망 학습 장치에 포함된 복수의 제1저 항 소자 및 복수의 제2저항 소자 중 적어도 하나는 교차 구조(Crossbar array)로 구비되는 저항변화 메모리 (RRAM)를 포함할 수 있다. 이하에서는, 신경망 학습 장치의 하이브리드 시냅스 구조 및 기능에 대해 설명하도록 한다. 도 2a는 제1시냅스부 및 제2시냅스부를 설명하기 위한 개념도이다. 도 2a를 참조하면, 신경망 학습 장치는 신경망의 가중치를 제1정밀도에 기반하여 업데이트 하기 위한 복수 의 제1저항 소자를 포함하는 제1시냅스부 및 신경망의 가중치를 제1정밀도 대비 높은 정밀도(예를 들면, 제2정밀도 등으로 지칭될 수 있다.)로 업데이트 하기 위한 복수의 제2저항 소자를 포함하는 제2시냅스부를 포함할 수 있다. 참고로, 이하에서는 서로 다른 정밀도에 각각 대응하는 제1시냅스부 및 제2시냅스부를 포함하는 신경 망 학습 장치에 대하여 설명하나, 본원의 다양한 구현예에 따라 신경망 학습 장치는 서로 다른 복수 의 레벨로 결정되는 정밀도 각각에 대응하는 둘 이상의 복수의 시냅스부(예를 들면, 제1시냅스부 내지 제3시냅 스부 등)를 포함하도록 구현될 수 있음은 물론이다. 한편, 복수개의 시냅스부를 포함하는 신경망 학습 장치 와 관련하여 '제1시냅스부' 및 '제2시냅스부'는 복수개의 시냅스부 중 어느 하나의 시냅스부와 또 다른 하나의 시냅스부를 각각 지칭하는 것으로 이해될 수 있다. 또한, 제1시냅스부의 제1저항 소자의 컨덕턴스 값은 제2시냅스부의 제2저항 조사의 컨덕턴스 값 대비 큰 것일 수 있다. 이와 관련하여, 시냅스 유닛인 신경망 학습 장치에 의해 업데이트 되는 가중치와관련하여, 저항 소자의 컨덕턴스 값은 시냅스의 연결 강도에 대응하는 것일 수 있다. 이에 따라, 제1시냅스부 는 제2시냅스부 대비 큰 컨덕턴스 값을 갖는 복수의 저항 소자(제1저항 소자)를 활용하여 신경망의 가중치를 업데이트 함으로써 가중치를 상대적으로 큰 단위로 업데이트 하는 것일 수 있다. 이와 관련하여, 제1저항 소자의 컨덕턴스 값은 제2저항 소자의 컨덕턴스 값에 미리 설정된 이득 계수(Gain Factor; k)를 곱한 것일 수 있으며, 이는 하기 식 1로 표현될 수 있다. [식 1]"}
{"patent_id": "10-2021-0056015", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, G는 제1저항 소자의 컨덕턴스 값이고, g는 제2저항 소자의 컨덕턴스 값이고, k는 이득 계수일 수 있다. 한편, 본원의 일 실시예에 따르면, 신경망 학습 장치에 의해 업데이트 되는 신경망의 가중치는 신경망의 학습 진행 수준에 기초하여 제1시냅스부 또는 제2시냅스부에 기초하여 선택적으로 업데이트 되는 것 일 수 있다. 도 2b는 이상적인 소프트웨어 네트워크의 학습 과정을 예시적으로 나타낸 도면이다. 도 2b를 참조하면, 소프트웨어 네트워크의 시냅스를 통한 가중치 조정 프로세스는 주로 가중치가 크게 업데이트 되는 동적 조정(Dynamic tuning) 단계와 가중치가 높은 정밀도로 세밀하게 업데이트 되는 미세 조정(Fine tuning) 단계로 구분되는 것을 확인할 수 있다. 이와 관련하여, 본원에서 개시하는 신경망 학습 장치는 동적 조정 단계에 관여하여 가중치를 크게 업데이 트(달리 말해, 낮은 정밀도로 업데이트)하는 제1시냅스부와 미세 조정 단계에 관여하여 가중치를 상대적으 로 세밀하게 업데이트(달리 말해, 높은 정밀도로 업데이트)하는 제2시냅스부를 구비하고, 제1시냅스부 와 제2시냅스부를 각각 이루는 저항 소자 간의 이득 계수(k)를 적절히 조정함으로써 제1시냅스부 와 제2시냅스부의 정밀도 차이(배율)를 설정할 수 있다. 여기서, 이득 계수(k)가 커질수록 제2시냅스 부에 의한 업데이트를 통해 표현할 수 있는 가중치의 정밀도가 높아지는 것일 수 있다. 달리 말해, 신경망 학습 장치에 포함된 복수의 시냅스 유닛 중 i번째 행 및 j번째 열에 배치된 시냅 스 유닛의 가중치는 하기 식 2와 같이 표현될 수 있다."}
{"patent_id": "10-2021-0056015", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, +/- 부호는 음극/양극에 대응하는 저항 소자의 컨턱턴스 값을 각각 나타내는 것일 수 있다. 한편, 제1시냅스부와 제2시냅스부 간의 저항 소자 간 이득 계수(k)의 조정은, 입력 전압 신호에 스케 일링(Scaling)을 적용하는 방식, 주변 회로의 이득 값을 조정하는 방식 등을 통해 달성되는 것일 수 있다. 다른 예로 본원의 일 실시예에 따르면, 제1시냅스부와 제2시냅스부 간의 저항 소자 간 이득 계수(k) 는 제1저항 소자와 제2저항 소자 간의 영역 기반 컨덕턴스 스케일링(area-dependent conductance scaling)에 의해 달성되는 것일 수 있다. 이러한 영역 기반 컨덕턴스 스케일링 방식의 경우, 운영 체계에 대한 수정 없이도 각 저항 소자(장치)가 차지하는 영역을 확장함으로써 정밀도 배율을 조정할 수 있는 이점이 있다. 예를 들어, 영역 기반 컨덕턴스 스케일링 방식에 의할 때, k값이 10인 경우, 제2시냅스부의 제2저항 소자의 장치 면적 이 제1시냅스부의 제1저항 소자 대비 10배 축소되는 것으로 이해될 수 있다. 도 3a 내지 도 3b는 추론 프로세스, 오류 계산 프로세스 및 가중치 업데이트 프로세스에서 제1시냅스부 및 제2 시냅스부의 동작을 각각 나타낸 도면이다. 구체적으로, 도 3a는 신경망의 추론 프로세스(순전파)에서의 신경망 학습 장치의 상태를 나타내고, 도 3b 는 신경망의 오류 계산 프로세스(역전파)에서의 신경망 학습 장치의 상태를 나타내고, 도 3c는 신경망의 가중치 업데이트 프로세스에서 신경망 학습 장치의 상태는 나타낸 것이다. 도 3a 및 도 3b를 참조하면, 신경망 학습 장치는 추론 프로세스와 오류 계산 프로세스에서 제1시냅스부 및 제2시냅스부의 가중치를 모두 활용하여 벡터-행렬 연산을 수행할 수 있고, 도 3c를 참조하면, 신 경망 학습 장치는 가중치 업데이트 프로세스에서 제1시냅스부 또는 제2시냅스부를 선택적으로활용하여 가중치를 업데이트할 수 있다. 이와 관련하여, 제1시냅스부는 신경망의 학습 진행 수준에 기초하여 상대적으로 신경망의 초반부 학습에 관여하고, 제2시냅스부는 신경망의 학습 진행 수준에 기초하여 상대적으로 신경망의 후반부 학습에 관여하 는 것일 수 있다. 보다 구체적으로, 신경망 학습 장치에 의해 학습되는 신경망은 미리 설정된 복수의 에포크(Epoch)만큼 반 복 수행되는 것을 특징으로 하고, 학습 평가부는 복수의 에포크 중 어느 하나의 에포크가 완료될 때마다 신경망의 정확도(Accuracy) 변화를 산출하여 학습 진행 수준을 평가할 수 있다. 이와 관련하여, 학습 평가부에 의해 평가된 신경망의 에포크 간의 정확도 변화(향상) 수준이 미리 설정된 임계 수준 이하로 도출되는 경우, 스위치 회로 등을 통해 제1시냅스부가 오프(Off)되고, 제2시냅스부(12 0)가 온(On)되어 이후의 에포크에 대하여 제2시냅스부를 활용한 미세 조정(Fine tuning)이 수행되는 것일 수 있다. 달리 말해, 본원의 일 실시예에 따르면, 신경망 학습 장치는 어느 하나의 에포크를 통해 제1시냅스부(11 0)에 의해 신경망의 가중치가 업데이트된 후 학습 평가부에 의해 평가된 정확도 변화가 미리 설정된 임계 값(임계 수준) 이하이면, 해당 에포크(전술한 어느 하나의 에포크) 이후의 에포크에서는 제2시냅스부에 의 해 가중치가 업데이트 되도록 할 수 있다. 이렇듯, 본원에서 개시하는 신경망 학습 장치는 복수 개의 시냅스부로 구성된 시냅스 유닛을 통한 신경망 의 학습 시, 복수 개의 시냅스부 중 특정 시냅스부(예를 들면, 제1시냅스부, 제2시냅스부 등)를 선택 적으로 활용하여 가중치 업데이트가 이루어지도록 함으로써, 기존의 병렬 업데이트 방식을 적용할 수 있기 때문 에 종래 기법 대비 신경망의 학습이 보다 정확하고 빠르게 가속화되도록 할 수 있다. 도 4는 제1시냅스부를 이용한 가중치의 동적 조정(Dynamic-tuning) 및 제2시냅스부를 이용한 가중치의 미세 조 정(Fine-tuning)을 설명하기 위한 개념도이다. 도 4를 참조하면, 본원에서 개시하는 신경망 학습 장치는 서로 다른 정밀도 수준을 가지는 시냅스 파트를 구비한 하이브리드 시냅스 유닛을 통해 단순한 스위칭 로직만으로 개별 저항성 소자가 전도도 단계가 제한적인 한계를 극복하고 향상된 훈련 정확도를 달성할 수 있다. 도 5a 및 도 5b는 제1저항 소자와 제2저항 소자의 컨덕턴스 값과 연계된 이득 계수의 변화에 따른 학습 성능 변 화를 설명하기 위한 도면이다. 도 5a 및 도 5b를 참조하면, 이득 계수(k)는 본원에서 개시하는 하이브리드 시냅스를 통한 가중치 업데이트의 정밀도를 조정하여 신경망의 성능을 결정하는데 중요한 역할을 하며, 이러한 이득 계수(k)의 최적 값을 분석하 기 위해 서로 다른 이득 계수(k) 값에 대한 가중치 업데이트의 오류를 평가한 결과, 도 5a는 이득 계수(k)가 각 각 1, 10, 100인 세 가지 경우 상대적 정밀도를 나타내며, k 가 1인 경우는 제1시냅스부의 정밀도와 제2시 냅스부의 정밀도가 동일한 경우인 반면, k 가 10으로 증가하면 제2시냅스부의 정밀도는 제1시냅스부 의 정밀도에 비하여 10 배로 커지게 되므로, 제2시냅스부에 의해 가중치는 제1시냅스부 대비 10 배의 서로 다른 상태를 표현할 수 있게 된다. 그 결과 신경망이 제1시냅스부에 의한 대략적인 훈련을 수행 한 후, 제2시냅스부를 통해10배 이상 정밀하게 튜닝하여 Werror를 더욱 줄일 수 있으나, k가 100과 같이 과 도하게 커지는 경우, 제2시냅스부의 과도하게 스케일링 된 정밀도로 인해 에포크가 진행되더라도 업데이트 되는 가중치의 변화가 너무 미비하여 신경망의 학습 성능이 오히려 감소할 수 있다. 이와 관련하여, 도 5b는 k의 함수로서 신경망의 오류율(Error rate)의 변화를 나타내며, 도 5b를 참조하면, k의 과도한 스케일링으로 인한 오류율의 증가는 제2시냅스부의 상태(state)의 수가 커짐에 따라 완화될 수 있 다. 이하에서는 상기에 자세히 설명된 내용을 기반으로, 본원의 동작 흐름을 간단히 살펴보기로 한다. 도 6a 및 도 6b는 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 기법과 연계된 실험 예의 학습 결과를 도시한 그래프이다. 도 6a의 (a)를 참조하면, 부동 소수점 시냅스('Ideal(FP)')는 미세 조정 프로세스를 통해 99.98% 수준의 훈련 정확도까지 점진적으로 증가하는 반면, 제한된 수의 상태(state)를 가지는 단일 시냅스('Single')를 통한 훈련 은 전체 신경망이 최적의 상태로 수렴하지 못하는 것을 확인할 수 있다. 이에 비하여, 본원에서 개시하는 하이브리드 시냅스 기반의 학습 기법에 의하면('Hybrid') 미세 조정으로 전환되기 전의 동적 조정 상태에서는 정확 도가 단일 시냅스 구현과 동등한 수준으로 유지되다가 미세 조정으로 전환(Switch)된 후 정확도가 단일 시냅스 구현과 달리 점진적으로 향상되는 것을 확인할 수 있다. 또한, 도 6a의 (b)를 참조하면, 신경망의 평균 제곱 오차(Mean squared error, MSE는 가중치의 업데이트에 사용 되는 시냅스가 제1시냅스부에서 제2시냅스부로 전환된 후 유의미하게 감소하는 것을 확인할 수 있다. 또한, 도 6b는 제1시냅스부 및 제2시냅스부 각각의 동작에 의한 여러 가중치의 변화 정도를 시계열적 으로 나타낸 것으로서, 도 6b를 참조하면, 제1시냅스부와 제2시냅스부 사이의 스위칭이 발생한 Epoch 6 이전의 에포크에서는 제1시냅스부에 의해 가중치가 큰 폭으로 변화(Dynamic tuning)하고, Epoch 6 이후 의 에포크에서는 제2시냅스부에 의해 가중치가 미세한 폭으로 변화(Fine tuning)하는 것을 확인할 수 있다. 이와 관련하여 도 6b를 참조하면, 제1시냅스부는 큰 시냅스(Big synapse)로, 제2시냅스부는 작 은 시냅스(Small synapse)로 각각 달리 지칭될 수 있다. 도 7은 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치의 개략적인 구성도이다. 도 7을 참조하면, 신경망 학습 장치는 제1시냅스부, 제2시냅스부 및 학습 평가부를 포함할 수 있다. 이하에서는 상기에 자세히 설명된 내용을 기반으로, 본원의 동작 흐름을 간단히 살펴보기로 한다. 도 8은 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 방법에 대한 동작 흐름도이다. 도 8에 도시된 다중 소자 기반의 시냅스를 이용한 신경망 학습 방법은 앞서 설명된 신경망 학습 장치에 의 하여 수행될 수 있다. 따라서, 이하 생략된 내용이라고 하더라도 신경망 학습 장치에 대하여 설명된 내용 은 다중 소자 기반의 시냅스를 이용한 신경망 학습 방법에 대한 설명에도 동일하게 적용될 수 있다. 도 8을 참조하면, 단계 S11에서 신경망 학습 장치는 신경망의 가중치에 기초하여 추론을 수행할 수 있다. 다음으로, 단계 S12에서 신경망 학습 장치는 단계 S11의 추론 결과에 기초하여 오류를 계산할 수 있다. 다음으로, 단계 S13에서 학습 평가부는 신경망의 정확도(Accuracy) 변화를 산출하여 학습 진행 수준을 평 가할 수 있다. 다음으로, 단계 S14에서 신경망 학습 장치는 평가된 정확도 변화를 미리 설정된 임계값(임계 수준)과 비교 할 수 있다. 만일, 단계 S14의 판단 결과 정확도 변화가 임계값을 초과하면, 단계 S151에서 신경망 학습 장치는, 제1시 냅스부에 기초하여 가중치를 업데이트 하는 동적 조정을 수행할 수 있다. 반대로, 단계 S14의 판단 결과 정확도 변화가 임계값 이하이면, 단계 S152에서 신경망 학습 장치는 제2시 냅스부에 기초하여 가중치를 업데이트 하는 미세 조정을 수행할 수 있다. 달리 말해, 단계 S151 내지 단계 S152를 통해 신경망 학습 장치는 계산된 오류에 기초하여 신경망의 가중 치를 업데이트 하되, 단계 S13를 통해 평가된 신경망의 학습 수준에 기초하여 제1시냅스부 또는 제2시냅스 부를 선택적으로 활용하여 가중치를 업데이트할 수 있다. 상술한 설명에서, 단계 S11 내지 S152는 본원의 구현예에 따라서, 추가적인 단계들로 더 분할되거나, 더 적은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있 다. 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가 능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체 에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자 에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티 컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모 리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하 나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 또한, 전술한 다중 소자 기반의 시냅스를 이용한 신경망 학습 방법은 기록 매체에 저장되는 컴퓨터에 의해 실행 되는 컴퓨터 프로그램 또는 애플리케이션의 형태로도 구현될 수 있다."}
{"patent_id": "10-2021-0056015", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "전술한 본원의 설명은 예시를 위한 것이며, 본원이 속하는 기술분야의 통상의 지식을 가진 자는 본원의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본원의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본원의 범위에 포함되는 것으로 해 석되어야 한다."}
{"patent_id": "10-2021-0056015", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치를 포함하는 신경망 회로 의 개략적인 구성도이다. 도 2a는 제1시냅스부 및 제2시냅스부를 설명하기 위한 개념도이다. 도 2b는 이상적인 소프트웨어 네트워크의 학습 과정을 예시적으로 나타낸 도면이다. 도 3a 내지 도 3c는 추론 프로세스, 오류 계산 프로세스 및 가중치 업데이트 프로세스에서 제1시냅스부 및 제2 시냅스부의 동작을 각각 나타낸 도면이다. 도 4는 제1시냅스부를 이용한 가중치의 동적 조정(Dynamic-tuning) 및 제2시냅스부를 이용한 가중치의 미세 조 정(Fine-tuning)을 설명하기 위한 개념도이다. 도 5a 및 도 5b는 제1저항 소자와 제2저항 소자의 컨덕턴스 값과 연계된 이득 계수의 변화에 따른 학습 성능 변 화를 설명하기 위한 도면이다. 도 6a 및 도 6b는 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 기법과 연계된 실험 예의 학습 결과를 도시한 그래프이다. 도 7은 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 장치의 개략적인 구성도이다. 도 8은 본원의 일 실시예에 따른 다중 소자 기반의 시냅스를 이용한 신경망 학습 방법에 대한 동작 흐름도이다."}
