{"patent_id": "10-2021-7026491", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0114521", "출원번호": "10-2021-7026491", "발명의 명칭": "스피치 애니메이션의 실시간 생성", "출원인": "소울 머신스 리미티드", "발명자": "사가르, 마크"}}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "의사소통 표명(communicative utterance)을 애니메이션화하기 위한 방법으로서,수신하는 단계로서,애니메이션화될 스트링 - 상기 스트링은 복수의 의사소통 표명 원자들을 포함함 -;복수의 콜렉션들 - 각각의 콜렉션은 고유 원자 스트링들을 포함하는 복수의 항목들을 포함하고, 각각의 콜렉션은 상이한 길이들의 항목들을 저장함 -; 및각각의 항목 - 이는 상기 항목의 적어도 하나의 애니메이션 스니펫(Snippet)을 포함함 - 을 수신하는, 상기 단계;상기 스트링의 서브스트링들과 매칭하는 항목들에 대한 상기 콜렉션들을 계층구조적으로 검색하는 단계;의사소통 표명 원자들을 커버하기 위해 매칭된 항목들에 대한 애니메이션 스니펫들을 취출하는 단계; 및상기 취출된 애니메이션 스니펫들을 조합하여 상기 스트링을 애니메이션화하는 단계를 포함하는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 의사소통 표명은 스피치인, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 또는 제2항에 있어서, 계층구조적 순서는 더 긴 항목들을 선호하는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 내지 제3항 중 어느 한 항에 있어서, 적어도 하나의 항목은 복수의 애니메이션 스니펫들을 포함하고, 애니메이션 스니펫은 무작위로 취출되는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항 내지 제3항 중 어느 한 항에 있어서, 적어도 하나의 항목은 복수의 애니메이션 스니펫들을 포함하고, 애니메이션 스니펫은 그의 지속기간에 기초하여 취출되는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제3항 중 어느 한 항에 있어서, 적어도 하나의 항목은 복수의 애니메이션 스니펫들을 포함하고, 애니메이션 스니펫은 대응하는 스피치 특징부들에 기초하여 취출되는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 내지 제6항 중 어느 한 항에 있어서, 애니메이션 스니펫들은 상기 애니메이션에 대응하는 사운드와 연관되는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 애니메이션에 대응하는 상기 사운드와 매칭되도록 애니메이션 스니펫들을 압축하고/하거나 신장시키는 단계를 포함하는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제8항 중 어느 한 항에 있어서, 상기 복수의 콜렉션들 각각 내의 상기 항목들은 하나의 항목 유형의것이고, 상기 항목 유형은 좌측 하프포(halfpho), 우측 하프포, 다이포(dipho), 문장 경계 다이포(sentence-공개특허 10-2021-0114521-3-boundary dipho), 모음 중심 트라이포(vowel-centered tripho), 자음 중심 트라이포(consonant-centeredtripho), 문장 경계 트라이포, 음절 또는 단음절 단어, 및 다음절 단어로 이루어진 그룹으로부터 선택되는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제9항 중 어느 한 항에 있어서, 상기 항목들은 부분 음소(part-phoneme) 스트링들을 포함하는,방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항 내지 제10항 중 어느 한 항에 있어서, 애니메이션 스니펫들은 뼈 기반 애니메이션 리그(rig)의 변형 파라미터들을 저장하는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항 내지 제10항 중 어느 한 항에 있어서, 애니메이션 스니펫들은 근육 기반 서술자 가중치들을 저장하는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항 내지 제10항 중 어느 한 항에 있어서, 애니메이션 스니펫들은 형태혼합(blendshape) 가중치들을저장하는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "스피치를 애니메이션화하기 위한 방법으로서,수신하는 단계로서,애니메이션화될 음소들의 스트링; 및음소의 가장 안정한 부분에서 또는 그 주변에서 연결되도록 구성된 다음가(polyphone)들 및 부분 음소들의 스트링들을 포함하는 복수의 애니메이션 스니펫들을 수신하는, 상기 단계;상기 음소들의 스트링을 커버하기 위해 상기 음소들의 스트링의 서브스트링들을 매칭시키는 애니메이션 스니펫들을 검색하고 취출하는 단계;및 상기 취출된 애니메이션 스니펫들을 조합하여 상기 스트링을 애니메이션화하는 단계를 포함하는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 부분 음소들은 반음소들인 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "콘텍스트 내의 음소를 애니메이션화하기 위한 방법으로서,수신하는 단계로서,모델 비짐(Model Viseme); 및콘텍스트에서 발음되는 상기 음소의 애니메이션 가중치들의 시계열에 대응하는 애니메이션 스니펫을 수신하는,상기 단계; 및상기 모델 비짐의 애니메이션 가중치들과 상기 애니메이션 스니펫의 애니메이션 가중치들 사이에서 혼합하여 콘텍스트 내의 상기 음소를 애니메이션화하는 단계를 포함하는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서, 상기 모델 비짐은 입술 판독가능 방식으로 상기 음소를 묘사하는 입술 판독가능 비짐인,방법.공개특허 10-2021-0114521-4-청구항 18 제16항에 있어서, 상기 모델 비짐은 뚜렷한 입 모양이고, 상기 입 모양은 p, b, m, f, w로 이루어진 그룹으로부터 선택되는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제16항에 있어서, 상기 모델 비짐은 스피치의 근육 기반 서술자로서 표현되는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제16항에 있어서, 상기 모델 비짐은 애니메이션 시퀀스로서 표현되는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제16항 내지 제20항 중 어느 한 항에 있어서, 시간 경과에 따른 상기 모델 비짐의 혼합 정도는 가우스 함수에의해 모델링되며, 가우스의 피크는 상기 음소 발음의 피크에 또는 그 주변에 있는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제21항에 있어서, 상기 가우스는 플랫-톱(flat-top) 함수인 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제21항 또는 제22항에 있어서, 상기 가우스는 좌측으로 편위(skew)되는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "표현적인 스피치 애니메이션(expressive speech animation)을 위한 방법으로서,수신하는 단계로서,근육 기반 서술자 정보와 연관된 제1 애니메이션 입력; 및근육 기반 서술자 정보와 연관된 제2 애니메이션 입력을 수신하는, 상기 단계;상기 제1 애니메이션 입력 및 상기 제2 애니메이션 입력을, 상기 표현적인 스피치 애니메이션을 애니메이션화하기 위해 상기 애니메이션 입력들을 근육 기반 서술자 가중치들에 맵핑시키도록 구성된 출력 가중화 함수 내의인수(argument)들로서 사용하는 단계- 상기 출력 가중화 함수는 상기 제1 및 제2 애니메이션 입력들로부터의 근육 기반 서술자 정보를 조화시키도록구성됨 -; 및상기 맵핑된 근육 기반 서술자 가중치들을 사용하여 애니메이션화하는 단계를 포함하는, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제24항에 있어서, 각각의 근육 기반 서술자에 대한 적어도 하나의 근육 기반 서술자 클래스 가중치(Muscle-Based Descriptor Class Weighting)를 정의하는 단계를 포함하고, 상기 출력 가중화 함수는 상기 근육 기반 서술자 클래스 가중치의 함수인, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제24항 또는 제25항에 있어서, 각각의 애니메이션 입력에 대한 우선순위 가중치를 수신하는 단계를 포함하고,상기 출력 가중화 함수는 상기 우선순위 가중치의 함수인, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제24항 내지 제26항 중 어느 한 항에 있어서, 상기 제1 애니메이션 입력은 스피치를 애니메이션화하기 위한 것인, 방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "공개특허 10-2021-0114521-5-제24항 내지 제27항 중 어느 한 항에 있어서, 상기 제2 애니메이션 입력은 표정을 애니메이션화하기 위한 것인,방법."}
{"patent_id": "10-2021-7026491", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_29", "content": "컴퓨터로 하여금 제1항 내지 제28항 중 어느 한 항의 방법을 구현하게 하는 프로그램을 저장하는 비일시적 컴퓨터 판독가능 매체."}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "스트링(예컨대, 문장)을 사실적으로 애니메이션화하기 위해, 계층구조적 검색 알고리즘이, 스트링의 서브스트링 들의 저장된 예들(애니메이션 스니펫들)을 서브스트링 길이의 내림차순으로 검색하도록, 그리고 취출된 서브스트 링들을 연결하여 스피치 애니메이션의 스트링을 완료하도록 제공된다. 일 실시예에서, 스피치 애니메이션의 실 시간 생성은 모델 비짐들을 사용하여 비짐들의 시작부들에서의 애니메이션 시퀀스들을 예측하고, 룩업 테이블 기 반(데이터 구동식) 알고리즘을 사용하여 비짐들의 전이들에서의 동역학을 예측한다. 구체적으로 제기된 모델 비 짐들은, 비짐들이 표현될 때 애니메이션 내의 대응하는 시점들에서 다른 방법을 이용하여 생성된 스피치 애니메 이션과 혼합될 수 있다. 출력 가중화 함수는 스피치 입력 및 표정 입력을 근육 기반 서술자 가중치에 맵핑하는 데 사용된다."}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명의 실시예들은 스피치 애니메이션(speech animation)의 실시간 생성에 관한 것이다."}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "스피치를 애니메이션화하기 위한 컴퓨터 얼굴 애니메이션 기법들은 절차상, 데이터 구동식, 또는 퍼포먼스 캡처 (performance-capture) 기반 기법들을 포함한다. 코퍼스(corpus)(텍스트의 콜렉션) 내의 각각의 문장은 음소(phoneme)들(별개의 스피치/사운드의 단위들)의 시퀀 스로서 그리고 비짐(viseme)들(음소들이 발음되는 것을 보여주는 음소들의 시각적 등가물)의 시퀀스로서 표현될 수 있다. 절차상 스피치 애니메이션은 음소들을 비짐들로 변환하는 규칙들 또는 룩업 테이블들을 포함한다. 스피치에 대한 3D 애니메이션 곡선들의 온라인 실시간 생성은 음소들을 사용하여 비짐들의 애니메이션 시퀀스들 을 검색함으로써 수행될 수 있다. 그러한 접근법들은 작은 세트의 애니메이션 시퀀스들에 의해 제한되는데, 이 는 로봇식, 비현실적, 및 반복적 애니메이션들이, 특히 비짐들의 전이들에서 빈번하게 관찰되기 때문이다. 데이터 구동식 (통계) 방법들은 대용량 코퍼스로부터의 얼굴 애니메이션 데이터의 스니펫(snippet)들을 스티칭 하여 입력 스피치 트랙과 매칭시킨다. 얼굴 동역학은 다차원 모핑가능 모델(multi-dimensional morphable model)들, 은닉 마르코프 모델(hidden Markov model)들, 및 능동 외관 모델(active appearance model, AAM)들 에 의해 캡처된다. 데이터 구동식 접근법들의 품질은 종종, 이용가능한 데이터에 의해 제한된다. 통계 모델들 은 얼굴을 직접 드라이빙하여, 애니메이터로부터 제어권을 가져온다. 퍼포먼스 캡처 기반 스피치 애니메이션은 인간 행위자로부터 획득된 모션 데이터를 디지털 얼굴 모델로 변환한 다. 하나의 접근법은 사전캡처된 데이터베이스를 사용하여, 실시간으로 오디오 입력으로부터 음소 확률들을 추 출하도록 트레이닝된 심층 신경 네트워크로 퍼포먼스 캡처를 보정한다. 그러한 모델들을 트레이닝하는 것은 핸 드 크래프트 애니메이션 시퀀스들 또는 퍼포먼스들의 대용량 데이터세트들을 필요로 한다. 생성된 모델들은 종 종 복잡하여, 재생 및 추론을 실시간으로 달성하기가 어렵게 한다. 애니메이션 품질은 캡처된 연기자의 능력들 에 의해 제한되는데, 이는 애니메이터가 개량하는 것이 어렵기 때문이다. 이전 스피치 애니메이션 기법들은 또한 스피치 애니메이션과 감정 표현 애니메이션의 조합을 적절히 수용하지 못한다."}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 스피치 애니메이션의 실시간 생성을 개선하는 것 또는 적어도 대중이나 산업에 유용한 선택을 제공하는 것이다."}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "스피치를 애니메이션화하기 위한 방법으로서, 수신하는 단계로서, 애니메이션화될 스트링 - 스트링은 복수의 의 사소통 표명(communicative utterance)(예컨대, 스피치) 원자들을 포함함 -; 복수의 콜렉션들 - 각각의 콜렉션 은 고유 원자 스트링들을 포함하는 복수의 항목들을 포함하고, 각각의 콜렉션은 상이한 길이들의 항목들을 저장 함 -; 및 각각의 항목 - 이는 항목의 적어도 하나의 애니메이션 스니펫을 포함함 - 을 수신하는, 상기 단계; 스 트링의 서브스트링들과 매칭하는 항목들에 대한 콜렉션들을 계층구조적으로 검색하는 단계; 스피치 원자들을 커 버하기 위해 매칭된 항목들에 대한 애니메이션 스니펫들을 취출(retrieve)하는 단계; 및 취출된 애니메이션 스니펫들을 조합하여 스트링을 애니메이션화하는 단계를 포함하는, 방법이 제공된다. 스피치를 애니메이션화하기 위한 방법으로서, 수신하는 단계로서, 애니메이션화될 음소들의 스트링; 및 음소의 가장 안정한 부분에서 또는 그 주변에서 연결되도록 구성된 다음가(polyphone)들 및 부분 음소들의 스트링들을 포함하는 복수의 애니메이션 스니펫들을 수신하는, 상기 단계; 음소들의 스트링을 커버하기 위해 음소들의 스트 링의 서브스트링들을 매칭시키는 애니메이션 스니펫들을 검색하고 취출하는 단계; 및 취출된 애니메이션 스니펫 들을 조합하여 스트링을 애니메이션화하는 단계를 포함하는, 방법이 제공된다. 콘텍스트 내의 음소를 애니메이션화하기 위한 방법으로서, 수신하는 단계로서, 모델 비짐(Model Viseme); 및 콘 텍스트에서 발음되는 음소의 애니메이션 가중치들의 시계열에 대응하는 애니메이션 스니펫을 수신하는, 상기 단 계; 및 모델 비짐의 애니메이션 가중치들과 애니메이션 스니펫의 애니메이션 가중치들 사이에서 혼합하여 콘텍 스트 내의 음소를 애니메이션화하는 단계를 포함하는, 방법이 제공된다. 표현적인 스피치 애니메이션을 위한 방법으로서, 수신하는 단계로서, 근육 기반 서술자 정보와 연관된 제1 애니 메이션 입력; 및 근육 기반 서술자 정보와 연관된 제2 애니메이션 입력을 수신하는, 상기 단계; 제1 애니메이션 입력 및 제2 애니메이션 입력을, 표현적인 스피치 애니메이션을 애니메이션화하기 위해 애니메이션 입력들을 근 육 기반 서술자 가중치들에 맵핑시키도록 구성된 출력 가중화 함수 내의 인수(argument)들로서 사용하는 단계 - 출력 가중화 함수는 제1 및 제2 애니메이션 입력들로부터의 근육 기반 서술자 정보를 조화시키도록 구성됨 -; 및 맵핑된 근육 기반 서술자 가중치들을 사용하여 애니메이션화하는 단계를 포함하는, 방법이 제공된다."}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "파트 1 \"계층구조적 룩업 및 다음가 연결(Hierarchical Look-Up & Polyphone Concatenation)\" 하에서 기술되는 바와 같이, 애니메이션 시퀀스들은 기록된 퍼포먼스들로부터 재생되어, 비짐 시퀀스들을 개발하고 룩업 테이블 을 채운다. 이것은 비짐 전이들의 동역학에 변화(variation)를 도입하고, 개인 스타일들을 통합한다. 기록된 애니메이션 시퀀스들의 선택은 (계층구조적 검색 알고리즘을 사용하는) 계산 효율적 연결 유닛 선택 모델에 기 초한다. 입술 독해(lip-reading)를 위해 시각적 단서(cue)들의 완전한 개발을 보장하기 위해 (파트 1 하에서 기술되는 방법들로부터 획득된 애니메이션 시퀀스들과 혼합되는) 음소들의 시작부(onset)들에서 수동으로 생성되거나 구 체적으로 선택된 비짐들의 (모델) 애니메이션 시퀀스들이 도입될 수 있다. 이는 파트 2 \"모델 비짐들과의 혼합 (Blending with Model Visemes)\" 하에서 상세하게 기술된다. 생성된 스피치 애니메이션 시퀀스들은 시각적 아 티팩트들을 감소시키기 위해 평활화되고 혼합될 수 있다. 파트 3은 파트 1 및 파트 2 하에 개시된 방법들이 스 피치의 애니메이션을 넘어 제스처들의 애니메이션에 어떻게 적용될 수 있는지를 설명한다. 파트 4는 얼굴 표정들이 스피치 애니메이션 상에 중첩되어 스피치에 대한 감정 변화들을 도입할 수 있게 하기 위한 기법들을 기술한다. 출력 가중화 함수는 스피치 및 표정 근육들이 충돌하는 방식으로 표현되지 않음을 보 장한다. 본 명세서에 기술된 기법들은 아바타, 디지털 캐릭터, 또는 자율 에이전트(autonomous agent)로도 알려진, 가상 캐릭터 또는 디지털 엔티티의 스피치를 애니메이션화하는 데 사용될 수 있다. 1.계층구조적 룩업 및 다음가 연결 기술적 문제 절차상 스피치 애니메이션 및 3D 애니메이션 곡선들의 온라인 실시간 생성은 작은 세트들의 애니메이션 시퀀스 들에 의해 제한되는데, 이는 로봇식, 비현실적, 및 반복적 애니메이션들이, 특히 비짐들의 전이들에서 빈번하게 관찰되기 때문이다. 스피치를 애니메이션화하는 데 있어서의 추가의 과제는, 인간들이 동시 분절발화(co-articulate)하여, 다시 말 해서, 비짐들이 서로 중첩되고 밀어내어(crowd out); 스피치의 시각적 표현을 복잡하게 할 수 있다는 것이다. 단지 비짐 시퀀스들을 함께 스티칭하는 것은 스피치에서 발생하는 자연스러운 변동을 캡처하지 않는데, 이는 입 움직임들이 (선행 및/또는 후속 비짐들과 같은) 그들의 콘텍스트에 따라 변할 수 있기 때문이다. 즉, 음소에 대응하는 입 모양은 스피치된 음소 자체뿐만 아니라 순간 음소 전 및/또는 후에 스피치된 음소들에 의존한다. 동시 분절발화 효과들을 고려하지 않는 애니메이션 방법은 관찰자에게 인위적인 것으로 인지될 것인데, 그 이유 는 입 모양들이 그들 입 모양들의 사용과 불일치하는 콘텍스트에서 스피치된 음소와 함께 사용될 수 있기 때문 이다. 더욱이, 이전 스피치 애니메이션 기법들은 스피치 스타일들; 즉, 스피치의 의도적 변동들에 의해 드러나는 비짐 형상들의 연속체(continuum), 및/또는 개인 종속 스피치하기 스타일들(시각적 악센트와 유사함)을 명시적으로 모델링하지 못한다. 기술적 해법 스피치의 스트링(예컨대 문장)을 사실적으로 애니메이션화하기 위해, 계층구조적 검색 알고리즘이, 스트링의 서 브스트링들의 저장된 예들(애니메이션 스니펫들)을 서브스트링 길이의 내림차순으로 검색하도록, 그리고 취출된 서브스트링들을 함께 스티칭하여 그들이 스피치 애니메이션의 스트링을 완료하게 하도록 제공된다. 서브스트링들은 부분(또는 절반) 음소들 및 다음가들의 고유 부분 음소 스트링들일 수 있다. 서브스트링들은, 대체적으로 (인접한 음소들과는 상관없이 가장 적게 변화하는) 음소의 가장 안정한 부분인 음소들의 중간점에서 연결되어, 연결된 서브스트링들 사이의 평활한 전이의 기회를 증가시킬 수 있다. 상세한 설명 모델 생성 음성학상으로 풍부한 코퍼스는 룩업 테이블 내의 각각의 항목에 대한 애니메이션 스니펫들의 다수의 예들을 갖 는 조밀하게 채워진 룩업 테이블을 생성한다. 도 2는 룩업 테이블을 생성하기 위한 흐름도를 도시한다. 단계 에서, 연기자에 의한 음소-풍부 코퍼스(phoneme-rich corpus)의 스피치 퍼포먼스의 모션 캡처가 수행된다. 연기자는 중립적 얼굴 표정에서 스피치 퍼포먼스를 수행한다(예컨대, 코퍼스를 판독함). 스피치 퍼포먼스 동안 얼굴 랜드마크들이 추적될 수 있다. 누군가 스피치할 때 얼굴 근육들의 움직임들을 추적하는 기준 랜드마크들 은, 예를 들어 밝은 컬러들의 페이스페인트를 사용하여, 식별되고 연기자의 얼굴에 마킹될 수 있다. 기준 랜드 마크들은 바람직하게는, 특히 입 영역에서 얼굴의 양호한 커버리지, 및 비디오 기록물들에서 연기자의 얼굴 특 징부들에 대해 양호한 콘트라스트를 갖는다. 기준 랜드마크들은 비디오 기록물들의 모든 프레임에서 수동으로 또는 자동으로 추적될 수 있다. 단계는 타임스탬프들에 의한, 예컨대 스피치의 콘텍스트 정보, 예를 들어 운율학 및 품사(part-of-speech) 태그들을 생성하는 자연어 프로세싱에 의한 스피치 퍼포먼스의 자동화된 음소 라벨링을 포함한다. 단계는 형태혼합(blendshape) 가중치들의 시계열을 생성하기 위해 스피치 퍼포먼스에 대한 3D 형태혼합 리그(rig)의 얼 굴 리타깃팅을 포함한다. 얼굴 기준 랜드마크들의 움직임들은 얼굴 액션 코딩 시스템(Facial Action Coding System, FACS) 액션 단위(action unit, AU)들 상에 리타깃팅된다. WO2017044499A1호(본 출원인에 의해 소유되 고 본 명세서에 참고로 포함됨)에 기재된 바와 같은 이미지 규칙화 및 리타깃팅 시스템은 연결 모델을 위한 데 이터를 생성할 때 얼굴 퍼포먼스를 리타깃팅하는 데 사용될 수 있다. 단계는 얼굴 형태혼합 가중치들의 시계열을 저장하는 룩업 테이블의 생성을 포함한다. 룩업 테이블 생성 스피치 애니메이션 모델의 룩업 테이블은 코퍼스 내의 음소들, 음절들, 단어들 및 문장들에 기초하여 계층구조 적으로 조직화되어 구성된다. 그러한 정보는, 토큰화기(tokeniser), 텍스트 정규화, 품사 태거 및 음소화 (phonemisation)로 구성되는 텍스트 프로세싱 모듈로부터 획득될 수 있다. 일 실시예에서, 룩업 테이블은 9개 의 항목 콜렉션들, 즉, 좌측 하프포(halfpho), 우측 하프포, 다이포(dipho), 문장 경계 다이포, 모음 중심 트라 이포, 자음 중심 트라이포, 문장 경계 트라이포, 음절, 및 단어 콜렉션들을 포함한다. 룩업 테이블의 각각의 콜렉션에서, 콜렉션의 설명에 맞는 하나 이상의 항목들이 있을 수 있다. 룩업 테이블 내의 항목들은 부분(예컨 대, 절반)의 음소 스트링들일 수 있다. 2음가(diphone) 연결 시스템들은 스피치 합성을 특징으로 한다: 음소는 좌측 및 우측 반음소들(하프포들)로 분 할될 수 있다. 2음가(또는 다이포)는 각각의 부분 음소(또는 반음소) 사이의 전이를 포함한, 하나의 음소의 중 간으로부터 다음 음소의 중간까지 연장되는 (즉, 앞쪽에 있는 우측 하프포(반음소) 및 뒤쪽에 있는 좌측 하프포 로 구성됨) 음향 단위이다. 2음가들의 연결을 사용한 합성은 양호한 음성 품질을 제공하는데, 이는 각각의 2음 가가 시작 및 끝 음소들이 정상 상태에 도달한 인접한 2음가들과 연결되기 때문이고, 각각의 2음가가 음소로부 터 음소로의 실제 전이를 기록하기 때문이다. 유사한 방식으로, 스피치 애니메이션에서, 비짐 이미지들을 2음 가에 맵핑하기 위해, 입술, 치아 및 혀 포지셔닝을 동적으로 묘사하는 일련의 이미지들을 포함하는 \"2 의의소 (diseme)\"가 2개의 비짐들 사이의 전이를 캡처한다. (2음가와 유사한) 2 의의소가 하나의 비짐(음가) 동안의 어딘가에서 시작되고, 후속 비짐(음가) 동안의 어딘가에서 끝난다. 다음가들 및 반음소들을 포함하는 콜렉션들에 의해 카테고리화된 항목들로의 문장의 분해를 예시하는 예가 도 4 에 주어진다. 도 4는 룩업 테이블에 애니메이션 스니펫들을 추가하기 위한 문장 \"Author of the danger trail Phillip Steels et cetera\"의 텍스트 프로세싱을 도시한다. 룩업 테이블을 구성할 때, 코퍼스 내의 각각의 문 장은 스피치 분석을 거쳐서, 그 문장의 사운드를 생성할 시에 수반되는 음소들의 시퀀스를 드러낸다. 음소들의 시퀀스는 룩업 테이블 항목들의 각각의 콜렉션에 예시적인 애니메이션 스니펫들을 제공하기 위해 철저히 검색된 다. 룩업 테이블의 각각의 콜렉션 내의 항목들은 그들의 지속기간에 따라 분류된 애니메이션 스니펫들의 예들 을 함유하지 않거나, 하나 또는 다수 개를 함유할 수 있다. 일 실시예에서, 콜렉션들은 상이한 다음가 단위들의 항목들을 포함한다. 다음가들은 2개의 음가들(2음가들), 3 개의 음가들(3음가(triphone)들), 또는 그 초과의 음가들을 포함하고, 안정된 영역들(대체적으로 중간 음소)에 서 음가들의 원하는 그룹들을 세그먼트화함으로써 저장될 수 있다. 따라서, 다음가들을 기본 서브단위들로서 취급함으로써, 2개의 인접한 음가들 사이의 전이는 기록된 서브단위들에 보존되고, 연결은 유사한 스피치 포즈 들 사이에서 수행된다. 따라서, 룩업 테이블 내의 애니메이션 스니펫들의 첫 번째 및 마지막 음소들은 각자 우 측 및 좌측 하프포들인데, 이는 상이한 시점들로부터의 애니메이션 스니펫들이 음소들의 중간점에서 또는 그 주 변에서 혼합되기 때문이다. 하나의 음소로부터 다른 음소로의 전이는 연속적인 피스(piece)의 애니메이션 스니 펫들로부터 획득되어, 생성된 애니메이션의 평활한 흐름을 보장한다. 단어 콜렉션은 하나 초과의 음절의 단어들에 대한 애니메이션 스니펫들을 함유한다. 예시적인 문장 \"Author of the danger trail Philip Steels et cetera\"는 하나 초과의 음절을 갖는 4개의 단어들을 가졌다. 이들 단어들 의 애니메이션 스니펫들은 단어 콜렉션 내의 별개의 룩업 테이블 항목들이다. 이러한 예에서 부재한, 동일한 발음을 갖는 단어들과 단어들을 반복하는 것은, 상이한 예들과 동일한 룩업 항목에 들어갈 것이다. 음절은 하 나 이상의 음소들을 포함하는 발음의 단위이지만, 그들 중 단 하나만이 모음이다. 음절 콜렉션은 음절들 및 단 일 음절의 단어들에 대한 애니메이션 스니펫들을 함유한다. 도 4의 예시적인 문장은 단일 음절의 5개의 단어들을 갖는다. 이들 단어들의 애니메이션 스니펫들은 음절 콜렉 션 내의 별개의/분리된 룩업 테이블 항목들이다. 다수의 음절들을 갖는 단어들은 또한 음절들로 분해되어, 음 절 룩업 항목들에 대한 예시적인 애니메이션 스니펫들을 제공한다. 예를 들어, 단어 \"author\"는 음절들 \"O\" 및\"D @\"로 분해된다. 이들 음절들 각각은 상이한 룩업 항목에 들어갔다. 동일한 발음을 갖는 음절들은 상이한 예들과 동일한 룩업 항목 내로 들어갈 것이다. 트라이포는 앞쪽에 있는 우측 하프포, 중간에 있는 전체 음소, 및 뒤쪽에 있는 우측 하프포로 구성된 사운드의 단위이다. 모음 중심 트라이포 콜렉션은 중심 음소가 모음인 모든 트라이포들에 대한 애니메이션 스니펫들을 함유한다. 예시적인 문장 내의 음소들의 시퀀스는 모음 중심 트라이포들에 대해 철저히 검색된다. 음소들을 갖는 예시적인 문장 내의 첫 번째 모음 중심 트라이포 \"/D @ O/\"는 단어 \"author\"로부터의 모음 음소 \"@\"를 가 졌다. \"@\" 앞 및 뒤의 음소들은 각자 단어 \"author\"로부터의 \"D\" 및 단어 \"of\"로부터의 \"O\"이다. 자음 중심 트라이포 콜렉션은 중심 음소가 자음인 모든 트라이포들에 대한 애니메이션 스니펫들을 함유한다. 문장 경계 트라이포 콜렉션은 문장들의 시작 또는 끝에 트라이포들에 대한 애니메이션 스니펫들을 함유한다. 이러한 콜렉 션에서, 묵음(silence) 앞 또는 뒤의 묵음은 하프포로 간주된다. 각각의 문장에는 2개의 문장 경계 트라이포가 있고, 그것은 문장 내의 첫 번째 및 마지막 음소들로부터 온다. 예시적인 문장에서, 그들은 단어 \"author\"로부 터의 /O T/ 및 단어 \"cetera\"로부터의 /r @/이다. 문장 앞 또는 뒤의 묵음이 또한 이러한 콜렉션 내의 하프포 로서 간주되기 때문에, /O T/ 예는 문장 앞의 묵음의 우측 절반, 음소 /O/의 전체, 및 /T/의 좌측 하프포를 포 함했다. 유사하게, /r @/ 예는 /r/의 우측 하프포, 음소 /@/의 전체, 및 문장 뒤의 묵음의 좌측 절반을 포함했 다. 다이포 콜렉션은 모든 다이포들에 대한 애니메이션 스니펫들을 함유한다. 문장 경계 다이포 콜렉션은 문장의 시작 또는 끝에 애니메이션 스니펫들을 함유한다. 이러한 콜렉션에서, 묵음 앞 및 뒤의 묵음은 하프포로 간주 된다. 따라서, 이러한 콜렉션은 문장들의 시작에 있는 첫 번째 하프포들에 대한 애니메이션 스니펫들 및 문장 들의 끝에 있는 마지막 우측 하프포들에 대한 애니메이션 스니펫들을 함유한다. 각각의 문장에는 2개의 문장 경계 다이포들이 있고, 그들은 문장 내의 첫 번째 및 마지막 음소들로부터 온다. 예시적인 문장에서, 첫 번째 문장 경계 다이포는 문장 앞의 묵음의 좌측 절반, 단어 \"author\"로부터의 우측 하프포 /O/이다. 제2 문장 경계 다이포는 단어 \"cetera\"로부터의 우측 하프포 /@/ 및 문장 뒤의 묵음의 좌측 절반이다. 좌측 하프포 콜렉션은 모든 좌측 하프포들에 대한 애니메이션 스니펫들을 함유하고, 우측 하프포 콜렉션은 모든 우측 하프포들에 대한 애니메이션 스니펫들을 함유한다. 전술된 콜렉션들의 카테고리화는 단 하나의 가능한 카테고리화이고; 다른 실시예들에서는, 콜렉션들은 문장 콜 렉션, 또는 다중-단어 콜렉션들과 같은 더 적은 입도 콜렉션들을 포함할 수 있다. 애니메이션의 실시간 생성 도 3은 스피치를 애니메이션화하는 방법의 흐름도를 도시한다. 단계에서, 음소 타임스탬프들 및 입력 문 장들의 콘텍스트 정보가 생성된다. 콘텍스트 정보는 피치, 강세, 음절, 및 단어 정보 등을 포함한다. 단계 에서, 단계에서 제공된 정보에 기초하여 선택된, 시계열의 얼굴 형태혼합 가중치들이 연결되어, \"스 피치 애니메이션의 생성(Generation of speech animation)\" 하에 설명된 바와 같은 일정 피스의 연속적인 '데이 터 구동식' 스피치 애니메이션을 형성한다. 단계에서, 뚜렷한 입 모양들(예컨대, p, b, m, f, w)과 관련 된 음소들이 더 양호한 시각적 단서들에 대한 수동으로 제기된 비짐 예들과 혼합된다(파트 2 \"모델 비짐들과의 혼합\"에서 더 상세히 기술된 바와 같음). 단계에서, 애니메이션은 평활화되고 혼합된다. 단계에서, 스피치 애니메이션은 에너지에 기초하여 변조된다. 단계에서, 애니메이션이 재생되어 오디오에 동기화된 다. 스피치 애니메이션의 생성 음소들의 시퀀스는 스피치로 변환될 텍스트를 수신한 후에 스피치 분석 소프트웨어에 의해 생성된다. 룩업 테 이블로부터의 애니메이션 스니펫들의 선택은 계층구조적 알고리즘을 사용하여 수행된다. 가능할 때마다, 더 큰 연속적인 피스들의 애니메이션 스니펫이 사용된다. 룩업 테이블 내의 콜렉션의 검색은 철저하며, 어떠한 추가 적인 매치도 발견될 수 없을 때, 단지, 계층구조에서 다음 콜렉션으로 내려간다. 도 5 및 도 7은 1231개의 문장들의 코퍼스를 사용하여 구성된 룩업 테이블로부터의 애니메이션 스니펫 연결의 예들을 도시한다. 도 7은 문장 \"And we will go meet her Wednesday at the train station\"을 도시한다. 702에서, 단어들을 매 칭시키기 위해 룩업 테이블의 단어 콜렉션이 검색된다. 하나 초과의 음절의 단어들이 예시적인 문장들에서 식 별된다. 단어들 \"wednesday\" 및 \"station\"은 코퍼스에 존재하고, 그들의 대응하는 애니메이션 스니펫들이 선택 된다. 이들 애니메이션 스니펫들의 첫 번째 및 마지막 반음소들에서 혼합이 수행된다. 704에서, 단계에서 발견되었던 단어들에 의해 이미 커버된 음절들 이외의 음절들에 대해 룩업 테이블의 음절 콜렉션이 검색된다. 다음절 단어들로부터의 개별 음절들 및 단음절의 단어들은 계층구조적 검색 프로세스를 거친다. 매 칭되는 음절들은 도면의 \"음절\" 행에 열거되어 있다. 706에서, 모음 중심 트라이포 콜렉션이 검색된다. 음소 들의 대부분에 대해 단어 및 음절 콜렉션들을 사용하여, 매칭되는 애니메이션 스니펫들이 발견되었지만, 음절 콜렉션에서 단어 \"train\" (/t r EI n/)에 대해 매칭을 찾을 수 없었다. 따라서, 검색 계층적구조는 모음 중심 트라이포 콜렉션으로 계속되었고, /r EI n/에 대한 예를 발견했다. 예시적인 문장들에서 선택된 애니메이션 스 니펫들을 갖지 않는 남은 하프포들로부터, 매칭되는 자음 중심 트라이포 시퀀스 /@ s d/가 발견된다. 문 장 경계 트라이포 콜렉션 내의 어떠한 매칭되는 예들도 취출되지 않는데, 그 이유는 문장들의 첫 번째 음소의 우측 하프포 및 마지막 음소에 대한 좌측 하프포에 대한 애니메이션이 이미 할당되었기 때문이다. 앞에 있는 음절의 마지막 음소의 우측 절반 및 뒤에 있는 음절의 첫 번째 음소의 좌측 절반을 포함한, 각 쌍의 2개의 연속적인 음절들 사이의 갭은 다이포 콜렉션 내의 매칭되는 애니메이션 스니펫들에 의해 채워진다. 2개의 예시적인 문장들의 첫 번째 및 마지막 하프포들은 문장 경계 다이포 콜렉션 내의 매칭되는 애니메이션 스 니펫들에 의해 채워진다. 2개의 예시적인 문장에 어떠한 갭도 남기지 않을 때, 계층구조적 검색이 완료된다. 좌측 및 우측 하프포 콜렉션들에서의 검색은 요구되지 않는데, 그 이유는 모든 갭들이 카테고리에서 상위에 있 는 콜렉션들로부터 애니메이션 스니펫들로 완전히 채워지기 때문이다. 애니메이션 스니펫들의 선택 스피치 퍼포먼스의 기록 시의 기록 인덱스, 시작 시간, 및 종료 시간이 룩업 테이블 내의 각각의 애니메이션 스 니펫마다 기록된다. 2개 이상의 애니메이션 스니펫들이 질의되는 주어진 항목들에 대해 존재할 때, 애니메이션 스니펫이 임의의 적합한 방식으로 취출될 수 있다. 일 실시예에서, 애니메이션 스니펫이 무작위로 취출된다. 다른 실시예에서, 애니메이션 스니펫은 대응하는 스피치 스니펫에 가장 가까운 지속기간(종료 시간 - 시작 시간)을 갖는다. 애니메이션 스니펫은 스피치 특징부들, 또는 생성될 입력 문장들의 콘텍스트 정보에 기초하여 선택될 수 있다. 콘텍스트 정보는 피치, 강세, 음절, 및 단어 정보 등을 포함할 수 있다. 일 실시예에서, 애 니메이션 스니펫의 선택은 일정 범위 내의 대응하는 스피치 스니펫의 지속기간을 매칭시키지만 소정의 대응하는 스피치 스니펫 길이를 고려하면 동일한(즉, 가장 가까운) 애니메이션 스니펫을 결정론적으로 되돌리지 않도록 의사랜덤화될 수 있다. 소스(연관된 애니메이션 스니펫 기록들) 스피치 및 타깃(합성된) 스피치는 대부분의 경우들에 있어서, 상이한 오디오 지속기간들을 가질 것이다. 따라서, 소스 애니메이션은 타깃 오디오 지속기간의 내부에 맞춰지도록 신 장(또는 압축)된다. 이러한 신장(또는 압축)은 소스 애니메이션 상의 음소 경계들이 타깃 오디오와 매칭되도록 변환되는 피스별 다항 보간법(piecewise polynomial interpolation)을 사용하여 행해질 수 있다. 일 실시예에서, 애니메이션 스니펫들은 그들이 묘사하는 원래의 스피치 및/또는 다른 사운드/오디오와 연관된다. 이는, 예를 들어, 단계에서 기술된 바와 같이, 연기자가 스피치 퍼포먼스를 수행할 때 비디오 와 함께 오디오를 캡처함으로써 달성될 수 있다. 고품질 마이크로폰 붐들은 카메라 또는 카메라들이 연기자의 입 움직임들을 기록하는 것을 차단하는 방식으로 연기자 주변에 포지셔닝될 수 있다. 대안적으로, 그리고/또는 추가적으로, 전체 이미지/비디오 캡처 장치는 오디오 기록을 위해 구성된 방음실 내에 있을 수 있다. 따라서, 애니메이션 스니펫들은 연결되는 시각적 및 오디오 정보의 조합으로서 저장될 수 있거나, 또는 애니메이션 스니 펫들은 그들의 대응하는 소스 오디오 정보와 연관될 수 있다. 비터비 기반(Viterbi-based) 동적 프로그래밍이 또한, 타깃 비용 및 조인 비용(join cost)을 공동으로 최소화시 키도록 적용될 수 있다. 이러한 경우에 있어서의 타깃 비용은 소스(콜렉션) 스피치와 타깃(합성된) 스피치 사 이의 음소 지속기간, 에너지 및 피치 등의 차이로서 정의될 수 있다. 조인 비용은 2개의 음소들을 연결할 때의 근육 채널 차이들의 합이다. 2.모델 비짐들과의 혼합 기술적 문제 스피치 애니메이션의 일부 절차상 모델들, 예를 들어, 중첩되는 지배 함수(dominance function)들을 사용하여 음소들의 그의 이웃을 고려하여 주어진 비짐이 그의 타깃 형상에 얼마나 가깝게 도달하는지를 나타내는 값들을 제공하는 지배 모델은 현실적인 동시 분절발화 스킴들을 포함할 수 있다. 그러나, 지배 모델은 양순음 (bilabial)들(/m b p/)의 입술 폐쇄 및 소정의 다른 음소들의적절한 묘사를 충분하게 보장하지 못한다.기술적 해법 일 실시예에서, 스피치 애니메이션의 실시간 생성은 모델 비짐들을 사용하여 비짐들의 시작부들에서의 애니메이 션 시퀀스들을 예측하고, 룩업 테이블 기반(데이터 구동식) 알고리즘을 사용하여 비짐들의 전이들에서의 동역학 을 예측한다. 구체적으로 제기된 모델 비짐들은, 비짐들이 표현될 때 애니메이션 내의 대응하는 시점들에서 다 른 방법(예컨대, 파트 1 \"계층구조적 룩업 및 다음가 연결\" 하에 기술된 것)을 이용하여 생성된 스피치 애니메 이션과 혼합될 수 있다. 상세한 설명 모델 비짐들 비짐들로도 알려진 하나 이상의 음소들의 시각적 예들은, 비짐들을 그들의 시작부에서 현실적으로 보여주는 모 델 시각적 예들(이하, \"모델 비짐들\")로서 수동으로 제기되거나 의도적으로 선택될 수 있다. 모델 비짐들은 뚜 렷한 입 모양들에 관한 음소들에 대해 생성된 입술 독해가능 비짐들일 수 있고, 입술 독해의 목적을 위해 시각 적 단서들의 완전한 개발을 묘사하는 것을 도울 수 있다. 비짐들의 정적 포즈들은 단일 프레임에 대한 표현 가 중치들을 변경함으로써 경험있는 아티스트에 의해 수동으로 생성될 수 있다. 도 9는 비짐 포즈 예들(좌측에서 우측으로): 중립, 음소 /m/, 음소 /f/, 음소 /w/를 도시한다. 비짐의 정적 포즈는 표정 (FACS) 가중치들을 변경함으로써 또는 실제 주체를 스캐닝하고 형태혼합 모델에서 증 분적 조합으로서 형태혼합을 추가함으로써 경험있는 아티스트에 의해 수동으로 생성될 수 있다. 일 실시예에서, 입술들 또는 치아들이 함께 눌리는 것을 요구하는 음소들, 예를 들어 /b/, /p/, /m/, /f/, 및 /v/; 및 입술들이 오므리는 것을 요구하는 음소들, 예를 들어 /o/ 및 /w/에 대해 비짐들이 생성된다. 그들의 대응하 는 음소들의 지속기간에 걸친 이들 스냅숏들의 활성화의 레벨은 플랫-톱(flat-top) 및 1의 피크(완전 활성화)를 갖는 수정된 가우스 함수에 의해 기술된다. 플랫-톱 가우스 함수는, 음소의 시각적 묘사가 그것이 사용자에게 가시적이 되도록 소정 지속기간(예컨대, 적어 도 하나의 프레임) 동안 그의 완전한 활성화로 유지됨을 보장한다. 이러한 수정된 가우스 함수는 편위된 (skewed) 채로 남겨질 수 있다. 이는 비짐의 완전한 활성화가 사운드 동안의 임의의 지점에서 발생할 수 있다 는 사실을 반영하는 것이다. 예를 들어, 입술들은 음소 \"b\" 또는 \"p\"의 사운드가 발생되기 전에 함께 완전히 눌린다. 가우스의 편위도(skewness) 및 스프레드(spread)는 현재 음소뿐만 아니라 현재 음소의 앞(좌측으로) 및 뒤(우측으로)에 있는 음소의 지속기간에 기초하여 자동으로 조정된다. 활성화 곡선 그들의 대응하는 음소들의 지속기간에 걸친 이들 스냅숏들의 활성화의 레벨은 플랫-톱(이는 플랫-톱 가우스 함 수와는 상이함에 유의함) 및 1의 피크(완전 활성화)를 갖는 수정된 가우스 함수에 의해 기술된다. 수정된 가우 스 함수의 가중치들은 또한, 연결된 애니메이션 스니펫들과 비짐들의 수동으로 제기된 스냅숏들 사이의 혼합 가 중치들로서 역할한다. 프레임에서의 수정된 가우스 함수의 가중치가 1일 때, 이러한 프레임에 대한 표정 가중 치들은 단지 대응하는 비짐의 수동으로 생성된 스냅숏으로부터만 나온다. 수정된 가우스 함수의 가중치가 0일 때, 표정 가중치들은 연결된 애니메이션 스니펫들로부터만 나온다. 다수의 가우스 함수들이 중첩될 때, 더 낮은 우선순위 형상들의 세기를 조정하기 위해 정규화 단계가 수행된다. 이러한 정규화는 사용자가 각각의 음소에 배정한 우선순위 가중치들에 기초하여 수행된다. 예를 들어, /b/로부 터의 비짐 가우스 곡선이 /o/에 대한 비짐 가우스 곡선과 중첩될 때, 그들의 강도들은 /b/가 애니메이션에서 지 배적이어서 /b/ 음소의 표명 동안 입술들의 폐쇄를 유지시키도록 조정될 수 있다. 도 11은 (a) 정규화 전 및 (b) 정규화 후의 2개의 중첩하는 가우스 곡선들의 예를 도시한다. 분절발화의 위치와 같은 이들 파라미터들은 경험을 통해 또는 생리학적 및 해부학적 관찰들에 기초하여 배정될 수 있다. 일 실시예에서, 애니메이션은 FACS AU들과 같은 근육 기반 서술자들의 형태혼합에 기초하고, 모델 비짐들은 전 술된 \"계층구조적 룩업 및 다음가 연결\" 기법을 사용하여 생성된 애니메이션 시퀀스와 혼합된다. 생성된 음소 가우스 곡선은 아티스트에 의해 정의된 맵핑들을 사용하여 FACS 상에 맵핑되고, 연결된 애니메이션 스니펫들에 혼합된다. 수정된 가우스 함수의 가중치들은, 연결된 애니메이션 스니펫들과 비짐들의 수동으로 제기된 스냅숏 들 사이의 혼합 가중치들로서 역할한다. 프레임에서의 수정된 가우스 함수의 가중치가 1일 때, 이러한 프레임 에 대한 표정 가중치들은 단지 대응하는 비짐의 수동으로 생성된 스냅숏으로부터만 나온다. 수정된 가우스 함 수의 가중치가 0일 때, 표정 가중치들은 연결된 애니메이션 스니펫들로부터만 나온다.보다 대체적인 접근법에서, 음소의 형성의 각각의 단계에서 입술 형상을 설명하는 파라미터화된 스플라인 모델 이 수정된 가우스 함수 대신에 사용될 수 있다. 연결된 애니메이션의 평활화 일 실시예에서, 생성된 FACS 애니메이션은 2단계 평탄화 및 증강 프로세스를 거친다. 평활화의 제1 단계는 음 절들의 도메인에 걸친 표정 가중치들에 대해 동작한다. 추가적인 평활화가 요구되는 경우, 평활화의 제2 단계 는 문장들의 도메인에 걸쳐 표정 가중치들에 대해 동작한다. 평활화의 제1 단계는 계층구조적 필터링 전략을 이용하는데, 여기서 저역 통과 버터워스(Butterworth) 필터가 각각의 음절에, 이어서 각각의 단어에, 이어서 각각의 구절(phrase)에 적용된다. 각각의 레벨에서, 버터워스 필터의 컷오프 빈도는 이전 레벨로부터 증가된다. 이는, 음절 경계들에 비해 음절 내에 더 높은 평활화가 적용 됨을 보장하고, 유사하게, 단어 경계들에 비해 단어 내에 더 높은 평활화가 적용됨을 보장한다. 게다가, 음절 들 및 단어들의 컷오프 빈도는 합성 음소들의 평균 지속시간에 기초하여 조정된다. 이는, 평활도가 스피치 속 도와는 독립적으로 유지됨을 보장한다. 평활화의 제2 단계는 표준 애니메이션 클린업 동작들의 콜렉션, 예컨대 애니메이션 가중치들의 경계를 짓는 것, 이상치(outlier)들을 제거하기 위해 스플라인 곡선들을 맞추는 것, 및 스피치가 끝난 후에 입 모양이 원하는 휴 지 포지션(resting position)으로 되돌아갔음을 보장하기 위해 시그모이드 윈도잉 동작을 적용하는 것으로 이루 어진다. 또한, 애니메이션 가중치들은 스피치의 에너지에 기초하여 추가로 증강된다. 예를 들어, 더 큰 스피 치는 턱(jaw) 열기와 같은 일부 애니메이션 채널들에 대해 더 큰 움직임으로 변환될 것이다. 3.연결을 통한 머리 및 눈썹 애니메이션의 생성 머리 움직임들 및 눈썹 애니메이션들은 \"계층구조적 룩업 및 다음가 연결\" 하에 기술된 바와 유사한 방식으로 생성된다. 머리 움직임 애니메이션의 경우, 기울기, 피치 및 요우(yaw)뿐만 아니라 어깨 병진이동들이 연결된 다. 눈썹 애니메이션의 경우, (속눈썹 및 겉눈썹 올리기(raiser), 내리기(lowerer) 등과 같은) 눈썹 모션에 관 련된 AU들이 연결된다. 그러나, 립싱크 애니메이션과는 상이하게, 머리 및 눈썹 애니메이션들은 구절 단위로 동작한다. 머리 및 눈썹 애니메이션의 실시간 생성은 하기의 단계들을 수반한다: 1. 문장 및 단어 타임스탬프들 및 입력 문장들의 콘텍스트 정보의 생성. 2. 단계 1에서 제공된 정보에 기초하여 선택되는, 머리 회전 및 병진이동의 시계열의 연결. 3. 단계 1에서 제공된 정보에 기초하여 선택되는, 눈썹 애니메이션의 시계열의 연결. 4. 애니메이션의 평활화 및 혼합. 5. 애니메이션 신호들에의 감정들의 추가. 6. 오디오에 동기화되는 애니메이션의 재생. 구절 콜렉션 각각의 구절들 내의 음절들의 수는 매칭되는 애니메이션 스니펫들을 찾기 위해 수집된다. 다수의 애니메이션 스니펫들이 발견되는 경우, 그들은 구절 내의 강세 음절 포지션들의 유사성에 기초하여 순위매김된다. 매칭되 는 강세 음절 포지션들을 갖는 다수의 구절들이 발견되는 경우, 그들은 구절들의 지속기간에 의해 다시 순위매 김된다. 입력 구절 상에서 어떠한 매칭되는 애니메이션 스니펫들도 발견되지 않는 경우, 구절은 결합 단어 (conjunction word)의 단어 경계에서 하위구절들로 분리된다. 어떠한 결합 단어도 발견되지 않은 경우, 시스템 은 구절에서의 강세들의 개수에만 매칭하도록 전환한다. 여전히 어떠한 매칭도 발견되지 않은 경우, 시스템은 구절의 중간점에 가장 가까운 단어 경계에서 구절을 분해하기 시작할 것이다(즉, 이진 분열). 키워드 콜렉션 좋은, 나쁜, 예, 및 아니오 등과 같은 소정의 키워드들은 종종, 특정 머리 및 눈썹 움직임들(즉, 고개 끄덕임, 머리 흔듦 및 들어올려지는 눈썹들 등)과 연관된다. 이들 키워드들이 구절 내에서 발견되는 경우, 그들 단어들 에 대한 애니메이션들은 이러한 콜렉션에서 발견되는 애니메이션 스니펫들에 의해 대체된다. 일단 애니메이션 이 생성되면, 애니메이션 시퀀스는 필터링되어, 잡음을 평활화시키고 연결 아티팩트들을 제거한다.예시적인 포즈들로부터의 혀 애니메이션의 생성 정상 스피치 동안 혀 움직임들을 모션 캡처하는 것이 어렵기 때문에, 혀 애니메이션은 각각의 음소에 대해 수동 으로 생성된 예시적인 포즈들로부터 생성된다. 예시적인 포즈들은 \"모델 비짐들과의 혼합\"에 기술된 바와 같이 애니메이션과 혼합될 수 있다. 유사하게, 정규화 가중치들이 그들 음소들에 대한 분절발화들의 위치에 기초하 여 도출될 수 있다. 4.감정 스피치 기술적 문제 이전 접근법들은, 상이한 감정 상태들에서 취해지고, 원하는 감정을 갖는 스피치 애니메이션을 선택함으로써 감 정 스피치를 생성한 스피치 애니메이션의 여러 예들을 사전기록했다. 그러나, 그러한 애니메이션 모델을 생성 하는 것은 시간 소모적인데, 그 이유는 기록되는 스피치의 양이, 스피치 동안 표현될 수 있는 감정 상태들의 수 와 곱해질 것이기 때문이다. 이것은 확장불가능하며, 뉘앙스가 있는 감정 상태들이 스피치에 용이하게 내재되 고 통합될 수 있게 하지 않는다. 다른 접근법들은 얼굴을 스피치하기 영역과 감정 표현 영역들로 세그먼트화했 고, 영역들을 개별적으로 제어하여 감정 및 스피치 둘 모두를 애니메이션화했다. 그 결과는 자연스럽거나 현실 적으로 보이지 않는데, 그 이유는 전체 얼굴이 감정을 표현할 수 있고; 수반되는 근육들이 상호 배타적이지 않 기 때문이다. 기술적 해법 출력 가중화 함수는 스피치 입력 및 표정 입력을 근육 기반 서술자 가중치에 맵핑하는 데 사용된다. 상세한 설명 스피치 애니메이션은 표정 애니메이션들과 합성되어 표현적인 스피치 애니메이션들을 형성할 수 있다. 도 12는 애니메이션 시스템을 도시한다. 단순화된 실시예 하에서, 애니메이션 작성기는 스피치 애니메이션 및 표 정 애니메이션을 포함하는 2개의 애니메이션 입력들을 수신한다. 애니메이션 작성기는 출력 가중화 함수 를 사용하여, 수신된 애니메이션 입력들을 조화시킨다. 예를 들어, 스피치 애니메이션이 표정 애니메이션과 동 시에 수신될 때마다, \"스피치 지배\"되는 것으로 정의되는 애니메이션 채널들(액션 단위들)이 억압되어(또는 제 약되어, 다시 말해, 가중된 단위로 억제되고/하향가중화되어), 그러한 애니메이션 채널들의 출력 애니메이션이 스피치 애니메이션 엔진으로부터의 그들 각자의 입력들에 의해서만 또는 주로 그들에 의해 영향을 받게 한다. 표정 애니메이션은 임의의 적합한 방식으로 생성될 수 있다. 일 실시예에서, 애니메이션화되는 주체의 내부 감 정 상태를 모델링하는 신경거동 모델/가상 중추신경계를 사용하여 애니메이션들이 생성된다. 가상 객체 또는 디지털 엔티티를 애니메이션화하기 위한 신경거동 모델의 사용은 본 발명의 양수인에게 또한 양도된 WO2015016723A1호에 추가로 개시되며, 본 명세서에 참고로 포함된다. 다른 실시예에서, 상호작용 동안 주체의 공감적 미러링을 통해 애니메이션이 제공될 수 있다. 대안적으로 그리고/또는 추가적으로, 감정적 또는 의사소 통 표현의 사전기록된 애니메이션이 제공될 수 있다. 전술된 바와 같은 또는 달리 설명되는 바와 같은 임의의 적합한 애니메이션 입력들의 조합이 제공될 수 있다. 표정 애니메이션들은 스피치(입술 동기화) 애니메이션에 추가되는 시변(time-varying) FACS AU 가중치들의 콜렉션으로서 제시될 수 있다. 근육 기반 서술자 클래스 가중치 표정 애니메이션들이 스피치 입술 동기화 애니메이션들과의 간섭하는 것 또는 그 역을 방지하기 위해, 2개의 근 육 기반 서술자 클래스들, 즉 표정 AU들 및 스피치 AU들이 정의된다. 이어서, 각각의 AU들에는 2개의 클래스 가중치(최대 1.0으로 합산됨)들이 배정되어, 상이한 유형들의 애니메이션 시퀀스(예컨대, 스피치 시퀀스들 또는 표정 시퀀스들)를 표현함에 있어서 각각의 AU의 상대적 중요성을 결정한다. 일부 실시예들에서, 근육 기반 서 술자 클래스 가중치들은 출력 가중화 함수에서 스피치 및 표정 애니메이션 입력들에 대한 계수들일 수 있다. 일부 실시예들에서, 애니메이션에 대한 최종 근육 가중치들이 1 이하로, 즉, Sum(AU)<=1로 제약되도록 제약들이 적용될 수 있다. 예를 들어, 스피치 구동식 광대뼈 주요부(Major) AU12의 활성화와 조합되는 미소의 전체 활성 화(광대뼈 주요부 근육 AU12를 활성화시킴)는 감정 가중치 및 스피치 가중치 둘 모두의 활성화들을 이용하여 최 종 애니메이션을 구동하지만 광대뼈 주요부 근육의 최종 활성화를 1로 제약함으로써 활성화될 수 있다. 예를 들어, 희미한 미소(AU12 = 0.5), 및 단어 \"sheep\"을 스피치하는 것(AU12= 0.75)은 Sum(AU12) = 0.5+0.75 = 1.25를 생성할 것인데, 이는 1로 제약된다.우선순위 가중치 애니메이션 구성 동안, 각각의 애니메이션 입력은 각각의 클래스에 대한 우선순위 값을 가질 수 있다(또한 최대 1.0으로 합산됨). 이러한 우선순위 값은, 더 명확하게 우선순위화되거나 묘사되는 것이 요구되는 애니메이션 시퀀스의 유형으로서 간주될 수 있다. 예를 들어, 애니메이션이 명확하고 이해가능한 방식으로 스피치를 보여 주도록 의도될 때, 스피치에 대한 우선순위 가중치가 증가될 수 있다. 반대로, 애니메이션이 디지털 캐릭터의 스피치를 방해하는 정도로 감정적인 디지털 캐릭터를 보여주도록 의도될 때, 표정 또는 감정에 대한 우선순위 가중치는 스피치의 것보다 더 클 수 있거나, 적어도 증가될 수 있다. 애니메이션 작성기 애니메이션 작성기는 우선순위 가중치들을 수신하고, 애니메이션 작성기는 입력 시퀀스들을 증강하기 위해 그것 이 사용하는 승수를 결정한다. 일 실시예에서, 함수들은 하기와 같이 정의된다:"}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서,"}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "= 입력 스피치 가중치"}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "= 입력 표정 가중치"}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "= 스피치에 대한 우선순위 가중치"}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "= 표정에 대한 우선순위 가중치"}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "= 스피치에 대한 근육 기반 서술자 클래스 가중치(카테고리화 가중치)"}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "= 표정에 대한 근육 기반 서술자 클래스 가중치"}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "= 스피치에 대한 출력 승수"}
{"patent_id": "10-2021-7026491", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "= 표정에 대한 출력 승수 및 및 는 0 내지 1에서 경계지어진다. 도 13은 AU12, AU22 및 AU26(각자, 입술 코너 당기기(puller), 오므리기(funneler) 및 턱 열기 AU들)의 애니메 이션 우선순위들을 도시한다. 이러한 예에서, AU12는 높은 표정 클래스 가중치를 갖고 어떠한 스피치 클래스가중치도 갖지 않는 반면, AU22는 높은 스피치 클래스 가중치를 갖고 어떠한 표정 클래스 가중치도 갖지 않는다. 한편, AU26은 이 둘의 혼합물이다. 표정 애니메이션들에 대해 우선순위가 주어질 때, 표정 애니메이 션 엔진으로부터의 AU12는 (단위 승수로) 간과되는 반면, 스피치 애니메이션 엔진으로부터의 AU12는 완전히 억 압되어, 그것이 표정 지배 애니메이션들과 간섭하는 것을 방지한다(예컨대, 이것은 주체가 또한 슬픔을 표현하 고 있을 때 주체가 입술들의 코너를 당기는 것을 방지할 것임). 그러나, 스피치 애니메이션 엔진으로부터의 AU22는 통과되어, 주체가 스피치된 단어들을 형성하려고 시도하게 할 것이다(예컨대, 울면서 스피치하려고 시도 함). AU22가 감정과 충돌하지 않기 때문에, 그것은 방해받지 않는다. 클래스 가중치들이 동일할 때(즉, AU26), 스피치 채널은 또한, 그것을 이중 활성화로부터 방지하고 표정 애니메이션을 파괴하는 것을 방지하도록 억제될 것이다. 스피치 애니메이션들에 우선순위가 주어질 때, 표정 및 스피치 애니메이션 엔진들 둘 모두로부터의 AU12가 간과 된다. 이는, AU12가 표정 지배 AU이고 스피치 애니메이션을 방해하지 않기 때문이다. 스피치 애니메이션 엔진 으로부터의 AU22는 통과되어, 스피치된 단어들을 형성할 것이지만, 표정 애니메이션 엔진은 간섭을 방지하기 위 해 억제될 것이다. 유사하게, 표정 애니메이션 엔진으로부터의 AU26이 또한 억제될 것이지만, 스피치 애니메이 션 엔진으로부터의 AU26은 간과될 것이다. 다른 클래스 가중치 조합들(예컨대, AU24, 입술 누르기(pressor)가 표정 애니메이션 및 스피치 애니메이션에 대 해 각각 0.2 및 0.8의 클래스 가중치들을 가짐)의 경우, 애니메이션 작성기는 0 내지 1의 비-단위 승수들을 입 력 애니메이션 시퀀스들에 적용하여 간섭을 완화시킬 것이다. 도 14는 AU24에 대한 애니메이션 작성기의 증강의 예를 도시한다. 도 15는 스피치에 우선순위를 갖는 행복한 표정과 혼합되는 비짐 포즈 예들, (상부-좌측) 행복한 표정, (상부-우측) 음소 /m/, (하부-좌측) 음소 /f/, (하 부-우측) 음소 /w/를 도시한다. AU26(턱 열기)은 억압되지만, AU06(볼 올리기) 및 AU12(입술 코너 당기기)는 비짐 형상들을 형성할 때 계속 존재한다. 도 16은 스피치에 우선순위를 갖는 걱정하는 표정과 혼합되는 비짐 포즈 예들, (상부-좌측) 걱정하는 표정, (상부-우측) 음소 /m/, (하부-좌측) 음소 /f/, (하부-우측) 음소 /w/를 도시한다. AU24(입술 누르기)가 억압되지만, AU04(눈썹 내리기)는 비짐 형상들을 형성할 때 계속 존재한다. 스피치 애니메이션을 위한 \"악센트들\"의 맞춤화 비짐 예시적인 포즈들 및 가우스 함수 수정자들의 맞춤화는 사용자가 아바타들의 스피치하는 스타일들 및 개성 들을 조정할 수 있게 한다. 이는, 해부학적 구조물, 얼굴 표정들뿐만 아니라 피부 질감들을 변형할 수 있는 디 지털 캐릭터 혼합 시스템(가출원 NZ747626호에 기술된 바와 같음)과 조합하여, 새로운 캐릭터들이 고유의 스피 치하기 개성들에 따라 생성될 수 있게 할 것이다. 또한, 이 시스템은, 상이한 사람이 상이한 언어, 악센트, 또는 분절발화 스타일로 스피치하는 것으로부터의 캡 처들인 다수의 룩업 테이블들을 포함할 수 있다. 애니메이션 생성 단계 동안, 사용자는 혼합된 아바타의 시각 적 외관과 매칭시키기 위해 어느 테이블로부터 애니메이션을 재구성할지 선택할 수 있다. 보간과의 조합 생성된 스피치 애니메이션은 조합 및 증분 형상들의 비선형 보간으로 믿을 수 있는 형태혼합 애니메이션들을 생 성하기 위해 형태혼합 보간 및 애니메이션 프레임워크(Blendshape Interpolation and Animation Framework)[NZ 가출원 제747627호]에 공급된다. 또한, 비짐 예시적 포즈들이 연기자의 3D 스캔으로부터 생성되거나 디지털 아 티스트들에 의해 조각될 수 있다. 이어서, 이들 예시적인 포즈들은 증분 조합 형상들로서 추가되어, 이들 음소 들에 대해 생성된 입술 모양들에 대한 추가적인 맞춤화를 허용할 수 있다. 더욱이, FACS 형태혼합들에 대해 동작하는 대신에, 비심 기반 형태혼합들이 사용될 수 있다. NZ747627호에서 정의된 네이밍 스킴들을 사용하여, 애니메이션 단계 동안 비짐 기반 형태혼합들을 FACS AU들로 분해하는 데 애 니메이션 프레임워크가 사용될 수 있다. 이러한 접근법의 이점은 그것이 사용자에게 더 직관적인 제어들을 제 공할 것이라는 점이다. 추가적으로, 이는 또한, 비짐 기반 형태혼합들에 대해서만 해결할 얼굴 리타깃팅 시스 템을 제약하여, 따라서, 스피치 시퀀스들에 대해 더 명료한 해결 결과들을 생성할 것이다. 예시적인 실시예 일 실시예에서, AU 채널들은 하기의 그룹들로 카테고리화된다: 스피치 입 AU들, 예컨대: AU08liptowardeachother, AU18lippuckerer, AU22lipfunneler 등 감정 입 AU들, 예컨대: AU12lipcornerpuller, AU15lipcornerdepressor, AU21necktightener 다른 입 AU들, 예컨대: AU16lowerlipdepressor, AU25lipspart, AU35cheeksuck 등 입이 아닌 AU들, 예컨대: AU01innerbrowraiser, AU05upperlidraiser, AU09nosewrinkler 등 일 실시예에서, 애니메이션화되는 디지털 캐릭터/가상 엔티티가 스피치하기 시작할 때, 시간적으로 평활화된 억 제 신호가 활성화된다. 억제 신호는 초기에 0이고, 아바타가 스피치하기를 계속함에 따라 최대치 1까지 점진적 으로 증가한다(증가 속도는 약 100ms로 설정될 수 있는 조정가능한 파라미터이다). 억제 신호는 감정 스트림 (표정 애니메이션 입력)으로부터 나오는 소정의 AU 그룹들의 기여를 감소시킨다. 억제 백분율은 다른 네트워크 들에 의해 수동으로 설정되고/되거나 동적으로 변경될 수 있다. 일 실시예에서, 위에서 정의된 AU 카테고리들 은 하기와 같이 감소된다: 스피치 입 AU들 - 100%만큼 감소됨 감정 입 AU들 - 50%만큼 감소됨 다른 입 AU들 - 100%만큼 감소됨 입이 아닌 AU들 - 10%만큼 감소됨 AU 그룹들에 더하여, 개별 AU들 상의 특정 감소 인자들이 설정될 수 있다. 아바타가 스피치하기를 완료함에 따 라, 억제 신호는 점진적으로 감소하고 0으로 되돌아간다. 감소 속도는 보통, 더 느린 페이스(대략 500ms)로 설 정되어, 아바타가 스피치하기를 완료한 후에 전체 표현력이 얼굴로 되돌아갈 수 있게 한다. 참조 부호 목록 1 애니메이션 스니펫 2 룩업 테이블 3 콜렉션 4 항목 5 사례 6 모델 비짐 7 스트링 8 근육 기반 서술자 클래스 가중치 9 우선순위 가중치 10 출력 가중화 함수 11 스피치 12 표정 13 근육 기반 서술자 14 애니메이션 작성기 해석 전술된 방법들 및 기법들이 영어 언어를 참조하여 기술되었지만, 본 발명은 이 점에 있어서 제한되지 않는다. 실시예들은 임의의 언어의 스피치 애니메이션을 용이하게 하도록 수정될 수 있다. 뼈 기반 애니메이션 리깅 (rigging) 또는 임의의 다른 적합한 애니메이션 기법이 형태혼합 애니메이션 대신에 사용될 수 있다. 전술된 실시예들 중 일부에서, 근육 변형 서술자들은 FACS에 의해 식별되는 액션 단위(AU)들이다. 액션 단위들의 예들은 '속눈썹 올리기', '겉눈썹 올리기', '입술 코너 당기기', '턱 열기' 및 '입술 코너 당기기 및턱 열기'를 포함한다. 그러나, 근육 변형 서술자에 대한 임의의 적합한 분류체계가 사용될 수 있다. 예를 들 어, 근육 변형 서술자들은 또한 통계적으로 계산될 수 있다. 예를 들어, 애니메이션들 내의 프레임들의 메시 형상 변동의 주요 컴포넌트들은 주요 컴포넌트 분석(principal component analysis, PCA)을 사용하여 계산될 수 있다. 관심 근육들만이 애니메이션에 수반될 때, 계산된 주요 컴포넌트들은 근육 변형 서술자들로서 사용될 수 있다. 기술된 방법들 및 시스템들은 임의의 적합한 전자 컴퓨팅 시스템 상에서 활용될 수 있다. 후술되는 실시예들에 따르면, 전자 컴퓨팅 시스템은 다양한 모듈들 및 엔진들을 사용하여 본 발명의 방법을 활용한다. 전자 컴퓨팅 시스템은, 적어도 하나의 프로세서, 하나 이상의 메모리 디바이스들에의 접속을 위한 하나 이상의 메모리 디바이스들 또는 인터페이스, 시스템이 하나 이상의 사용자들 또는 외부 시스템들로부터의 명령어들을 수신하고 그에 따라 동작할 수 있게 하기 위한 외부 디바이스들에의 접속을 위한 입력 및 출력 인터페이스들, 다양한 컴포넌트들 사이의 내부 및 외부 통신을 위한 데이터 버스, 및 적합한 전력 공급원을 포함할 수 있다. 또한, 전자 컴퓨팅 시스템은 외부 및 내부 디바이스들과 통신하기 위한 하나 이상의 통신 디바이스들(유선 또는 무선), 및 디스플레이, 포인팅 디바이스, 키보드 또는 프린팅 디바이스와 같은 하나 이상의 입력/출력 디바이스 들을 포함할 수 있다. 프로세서는 메모리 디바이스 내의 프로그램 명령어들로서 저장된 프로그램의 단계들을 수행하도록 배열된다. 프로그램 명령어들은 본 명세서에 기술된 바와 같은 본 발명을 수행하는 다양한 방법들이 수행될 수 있게 한다. 프로그램 명령어들은, 예를 들어 C 기반 언어 및 컴파일러와 같은 임의의 적합한 소프트웨어 프로그래밍 언어 및 툴키트를 사용하여 개발되거나 구현될 수 있다. 또한, 프로그램 명령어들은, 예를 들어 컴퓨터 판독가능 매 체 상에 저장되는 것과 같이, 그들이 메모리 디바이스로 전달되거나 프로세서에 의해 판독될 수 있도록 하는 임 의의 적합한 방식으로 저장될 수 있다. 컴퓨터 판독가능 매체는, 예를 들어 솔리드 스테이트 메모리, 자기 테 이프, 콤팩트 디스크(CD-ROM 또는 CD-R/W), 메모리 카드, 플래시 메모리, 광 디스크, 자기 디스크 또는 임의의 다른 적합한 컴퓨터 판독가능 매체와 같은, 프로그램 명령어들을 유형적으로 저장하기 위한 임의의 적합한 매체 일 수 있다. 전자 컴퓨팅 시스템은 관련있는 데이터를 취출하기 위해 데이터 저장 시스템들 또는 디바이스들 (예를 들어, 외부 데이터 저장 시스템들 또는 디바이스들)과 통신하도록 배열된다. 본 명세서에 기술된 시스템은 본 명세서에 기술된 바와 같은 다양한 기능들 및 방법들을 수행하도록 배열되는 하나 이상의 요소들을 포함한다는 것이 이해될 것이다. 본 명세서에 기술된 실시예들은, 시스템의 요소들을 구 성하는 다양한 모듈들 및/또는 엔진들이 기능들이 구현될 수 있게 하기 위해 어떻게 상호접속될 수 있는지의 예 들을 독자에게 제공하는 것을 목표로 한다. 또한, 본 설명의 실시예들은, 시스템 관련 상세에서, 본 명세서에 기술된 방법의 단계들이 어떻게 수행될 수 있는지를 설명한다. 개념도들은 다양한 데이터 요소들이 다양한 상 이한 모듈들 및/또는 엔진들에 의해 상이한 단계들에서 어떻게 프로세싱되는지를 독자에게 나타내기 위해 제공 된다. 따라서, 모듈들 또는 엔진들의 배열 및 구성은 시스템 및 사용자 요건들에 따라 적응되어, 다양한 기능들이 본 명세서에 기술된 것들과는 상이한 모듈들 또는 엔진들에 의해 수행될 수 있도록 할 수 있는 것, 및 소정 모듈들 또는 엔진들이 단일 모듈들 또는 엔진들로 조합될 수 있는 것이 이해될 것이다. 기술된 모듈들 및/또는 엔진들은 임의의 적합한 형태의 기술을 사용하여 구현되고 명령어들을 제공받을 수 있다 는 것이 이해될 것이다. 예를 들어, 모듈들 또는 엔진들은 임의의 적합한 언어로 기록된 임의의 적합한 소프트 웨어 코드를 사용하여 구현되거나 생성될 수 있으며, 여기서 코드는 이어서 임의의 적합한 컴퓨팅 시스템 상에 서 구동될 수 있는 실행가능한 프로그램을 생성하도록 컴파일된다. 대안적으로, 또는 실행가능한 프로그램과 함께, 모듈들 또는 엔진들은 하드웨어, 펌웨어 및 소프트웨어의 임의의 적합한 혼합물을 사용하여 구현될 수 있 다. 예를 들어, 모듈들의 부분들은 ASIC(application specific integrated circuit), SoC(system-on-a-chip), FPGA(field programmable gate array) 또는 임의의 다른 적합한 적응가능 또는 프로그래밍가능 프로세싱 디바이 스를 사용하여 구현될 수 있다. 본 명세서에 기술된 방법들은 기술된 단계들을 수행하도록 특별히 프로그래밍된 범용 컴퓨팅 시스템을 사용하여 구현될 수 있다. 대안적으로, 본 명세서에 기술된 방법들은 데이터 분류 및 시각화 컴퓨터, 데이터베이스 질의 컴퓨터, 그래픽 분석 컴퓨터, 데이터 분석 컴퓨터, 제조 데이터 분석 컴퓨터, 비즈니스 지능 컴퓨터, 인공지능 컴퓨터 시스템 등과 같은 특정 전자 컴퓨터 시스템을 사용하여 구현될 수 있으며, 여기서 컴퓨터는 특정 분야와 연관된 환경으로부터 캡처된 특정 데이터에 대해 기술된 단계들을 수행하도록 특별히 적응되었다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16"}
{"patent_id": "10-2021-7026491", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 스피치 애니메이션을 생성하기 위한 프로세스의 흐름도를 도시한다. 도 2는 룩업 테이블을 생성하는 흐름도를 도시한다. 도 3은 스피치를 애니메이션화하는 방법의 흐름도를 도시한다. 도 4는 룩업 테이블의 구성에 대한 텍스트 프로세싱을 도시한다. 도 5는 상이한 콜렉션들로부터의 서브스트링들을 문장에 매칭시키는 예를 도시한다. 도 6은 도 5의 스트링을 애니메이션화하기 위한 취출된 애니메이션 스니펫들의 조합을 도시한다. 도 7은 상이한 콜렉션들로부터의 서브스트링들을 문장에 매칭시키는 예를 도시한다. 도 8은 도 7의 스트링을 애니메이션화하기 위한 취출된 애니메이션 스니펫들의 조합을 도시한다. 도 9는 비짐 포즈 예들을 도시한다. 도 10은 수정된 가우스의 예를 도시한다. 도 11은 2개의 중첩하는 가우스 곡선들의 예를 도시한다. 도 12는 애니메이션 시스템을 도시한다. 도 13은 일 실시예에 따른 애니메이션 우선순위 값들의 표를 도시한다. 도 14는 애니메이션 작성기에서 사용되는 승수 값들의 표를 도시한다. 도 15는 스피치에 대한 우선순위로 행복한 표정과 혼합된 비짐 포즈 예들을 도시한다. 도 16은 스피치에 대한 우선순위로 걱정하는 표정과 혼합된 비짐 포즈 예들을 도시한다."}
