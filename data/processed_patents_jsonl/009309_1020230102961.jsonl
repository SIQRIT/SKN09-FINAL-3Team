{"patent_id": "10-2023-0102961", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0021824", "출원번호": "10-2023-0102961", "발명의 명칭": "음성합성 장치 및 방법", "출원인": "주식회사 라온데이터", "발명자": "고준서"}}
{"patent_id": "10-2023-0102961", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성데이터를 수집하는 수집부;상기 수집된 음성데이터를 전처리하는 전처리부; 및전처리된 음성데이터를 이용하여 음성합성 모델을 생성하고 훈련시키는 모델 관리부,를 포함하는 음성합성 장치."}
{"patent_id": "10-2023-0102961", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 수집부에서 수집된 음성데이터는,기 획득된 복수의 사용자의 음성정보를 음성 분할한 후 검증함으로써 수집되는 상기 복수의 사용자의 음성데이터인 것인, 음성합성 장치."}
{"patent_id": "10-2023-0102961", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 전처리부는,상기 전처리의 수행시, 기 생성된 발음변환기를 이용하여 상기 수집된 음성데이터에 대응하는 텍스트를 발음으로 변환시키는 발음 변환 처리를 수행하는 것인, 음성합성 장치."}
{"patent_id": "10-2023-0102961", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 모델 관리부는,상기 음성합성 모델의 생성을 위해, 전처리된 음성데이터를 다중화자 음성합성 모델로 훈련시키는 제1 훈련, 및전처리된 음성데이터를 화자별로 구분하고, 화자별 전처리된 음성데이터를 기반으로 전이학습(transferlearning)을 하는 제2 훈련을 수행하는 것인, 음성합성 장치."}
{"patent_id": "10-2023-0102961", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,사용자로부터 음성합성의 수행을 위한 문장 데이터를 입력받는 입력부; 및상기 문장 데이터를 상기 음성합성 모델에 적용시켜 음성합성을 수행함으로써, 상기 문장 데이터에 대응하는 합성음을 제공하는 음성합성 처리부,를 더 포함하는 음성합성 장치."}
{"patent_id": "10-2023-0102961", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항의 음성합성 장치에 의한 음성합성 방법으로서,수집부에서 음성데이터를 수집하는 단계;전처리부에서 상기 수집된 음성데이터를 전처리하는 단계; 및모델 관리부에서 전처리된 음성데이터를 이용하여 음성합성 모델을 생성하고 훈련시키는 단계,공개특허 10-2025-0021824-3-를 포함하는 음성합성 방법."}
{"patent_id": "10-2023-0102961", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 음성합성 장치에 관한 것이며, 음성합성 장치는 음성데이터를 수집하는 수집부; 상기 수집된 음성데이 터를 전처리하는 전처리부; 및 전처리된 음성데이터를 이용하여 음성합성 모델을 생성하고 훈련시키는 모델 관리 부를 포함할 수 있다."}
{"patent_id": "10-2023-0102961", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 음성합성 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0102961", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "음성합성(speech synthesis)은 말소리의 음파를 기계가 자동으로 만들어 내는 기술로, 간단히 말하면 모델로 선 정된 한 사람의 말소리를 녹음하여 일정한 음성 단위로 분할한 다음, 부호를 붙여 합성기(speech computer, speech synthesizer)에 입력하였다가 지시에 따라 필요한 음성 단위만을 다시 합쳐 말소리를 인위로 만들어내는 기술을 의미한다. 즉, 음성합성은 컴퓨터의 프로그램을 통해 사람의 목소리를 구현해 내는 기술(달리 말해, 컴퓨터가 전달하고자 하는 데이터를 인간이 이해할 수 있는 음성 언어로 전달하기 위해 음성 데이터를 합성해 출력시키는 기술)을 말 하며, 텍스트를 음성으로 변환한다는 점에서 텍스트 음성 변환(text-to-speech, TTS) 기술이라고도 부르며, 발 음 기호를 음성으로 변환하는 기술도 있다. 음성합성 기술은 문자를 읽기 어려운 장애인이나 문자를 읽을 수 없는 사람(아이, 외국인 등)에게 화면 읽기 소 프트웨어(스크린 리더)로서 오랫동안 쓰이고 있어 말을 발표하는 것이 곤란한 사람이 대체 수단으로서 이용하는 경우도 많다. 뿐만 아니라, 이러한 음성합성 기술은 스크린 리더, 전화안내멘트(일예로, 콜센터의 안내음성, 기 상청 일기예보 안내(ARS) 서비스 등), 횡단보도의 안내 음성, 각 포털 사이트의 기계 번역, 내비게이션, 음성 인식 비서 서비스(일예로 Siri) 등 음성파일을 필요로 하는 다양한 분야에서 활용되고 있으며, 또한 음성 인식 과 함께 번역 기계, 로봇 제조 기술, 교육용 등 여러 곳에서 다양하게 쓰이고 있다. 그런데, 종래의 음성합성 기술은 발음과 억양이 부자연스럽고, 목소리의 다양성이 부족하며, 처리속도나 정확도 등의 성능이 떨어지는 단점이 있었다. 또한, 음성합성 기술의 품질을 높이기 위해서는 보다 많은 양의 음성데이터를 수집해 분석하고 학습하는 기술이 필요하다 할 것인데, 종래에는 이러한 작업들이 작업자에 의해 수작업으로 이루어졌음에 따라 많은 시간과 노력 을 필요로 하고 비용이 많이 드는 단점이 있었다. 본 발명의 배경이 되는 기술은 한국등록특허공보 제10-0948644호에 개시되어 있다."}
{"patent_id": "10-2023-0102961", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 전술한 종래 기술의 문제점을 해결하기 위한 것으로서, 보다 적은 노력과 비용과 시간으로 음성합성 모델을 빠르게 생성할 수 있으면서, 발음과 억양이 자연스럽고, 다양한 목소리(합성음)의 제공이 가능하며, 음 성합성의 처리속도나 정확도 등의 측면에서 높은 성능을 제공할 수 있는, 음성합성 장치 및 방법을 제공하려는 것을 목적으로 한다. 다만, 본 발명의 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2023-0102961", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본 발명의 일 실시예에 따른 음성합성 장치는, 음성데 이터를 수집하는 수집부; 상기 수집된 음성데이터를 전처리하는 전처리부; 및 전처리된 음성데이터를 이용하여 음성합성 모델을 생성하고 훈련시키는 모델 관리부를 포함할 수 있다. 또한, 상기 수집부에서 수집된 음성데이터는, 기 획득된 복수의 사용자의 음성정보를 음성 분할한 후 검증함으 로써 수집되는 상기 복수의 사용자의 음성데이터인 것일 수 있다. 또한, 상기 전처리부는, 상기 전처리의 수행시, 기 생성된 발음변환기를 이용하여 상기 수집된 음성데이터에 대 응하는 텍스트를 발음으로 변환시키는 발음 변환 처리를 수행할 수 있다.또한, 상기 모델 관리부는, 상기 음성합성 모델의 생성을 위해, 전처리된 음성데이터를 다중화자 음성합성 모델 로 훈련시키는 제1 훈련, 및 전처리된 음성데이터를 화자별로 구분하고, 화자별 전처리된 음성데이터를 기반으 로 전이학습(transfer learning)을 하는 제2 훈련을 수행할 수 있다. 또한, 본 발명의 일 실시예에 따른 음성합성 장치는, 사용자로부터 음성합성의 수행을 위한 문장 데이터를 입력 받는 입력부; 및 상기 문장 데이터를 상기 음성합성 모델에 적용시켜 음성합성을 수행함으로써, 상기 문장 데이 터에 대응하는 합성음을 제공하는 음성합성 처리부를 더 포함할 수 있다. 한편, 본 발명의 일 실시예에 따른 음성합성 장치에 의한 음성합성 방법은, 수집부에서 음성데이터를 수집하는 단계; 전처리부에서 상기 수집된 음성데이터를 전처리하는 단계; 및 모델 관리부에서 전처리된 음성데이터를 이 용하여 음성합성 모델을 생성하고 훈련시키는 단계를 포함할 수 있다. 상술한 과제 해결 수단은 단지 예시적인 것으로서, 본 발명을 제한하려는 의도로 해석되지 않아야 한다. 상술한 예시적인 실시예 외에도, 도면 및 발명의 상세한 설명에 추가적인 실시예가 존재할 수 있다."}
{"patent_id": "10-2023-0102961", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본 발명의 과제 해결 수단에 의하면, 음성합성 장치 및 방법을 제공함으로써, 보다 적은 노력과 비용과 시간으로 음성합성 모델을 빠르게 생성할 수 있으면서, 발음과 억양이 자연스럽고, 다양한 목소리(합성음)의 제 공이 가능하며, 음성합성의 처리속도나 정확도 등의 측면에서 높은 성능을 제공할 수 있다. 다만, 본 발명에서 얻을 수 있는 효과는 상기된 바와 같은 효과들로 한정되지 않으며, 또 다른 효과들이 존재할 수 있다."}
{"patent_id": "10-2023-0102961", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 발명의 실시예를 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관 계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 발명의 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\" 또는 \"간접적으로 연결\"되어 있는 경우도 포함한다. 본 발명의 명세서 전체에서, 어떤 부재가 다른 부재 \"상에\", \"상부에\", \"상단에\", \"하에\", \"하부에\", \"하단에\" 위치하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재 가 존재하는 경우도 포함한다. 본 발명의 명세서 전체에서, 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 본 발명의 명세서 전체에서, 단말, 장치 또는 디바이스가 수행하는 것으로 기술된 동작이나 기능 중 일부는 해 당 단말, 장치 또는 디바이스와 연결된 서버에서 대신 수행될 수도 있다. 이와 마찬가지로, 서버가 수행하는 것으로 기술된 동작이나 기능 중 일부도 해당 서버와 연결된 단말, 장치 또는 디바이스에서 수행될 수도 있다.본 발명의 명세서 전체에서, '적어도 하나의' 라는 용어는 단수 및 복수를 포함하는 용어로 정의될 수 있고, 적 어도 하나의 라는 용어가 존재하지 않더라도 각 구성요소가 단수 또는 복수로 존재할 수 있으며, 단수 또는 복 수를 의미할 수 있음은 자명하다 할 것이다. 또한, 각 구성요소가 단수 또는 복수로 구비되는 것은, 실시예에 따라 변경 가능할 수 있다. 도 1은 본 발명의 일 실시예에 따른 음성합성 장치의 개략적인 구성을 나타낸 도면이다. 이하에서는 설명의 편의상, 본 발명의 일 실시예에 따른 음성합성 장치를 본 장치라 하기로 한다. 또한, 이하 본 발명을 설명함에 있어서, 도 1을 포함하여 도면들에 도시(기재)된 사항은 이하 생략된 내용이라 하더라도 본 장치 에 대한 설명에도 동일하게 적용될 수 있다. 도 1을 참조하면, 본 장치는 음성합성 장치로서, 수집된 음성데이터를 전처리하여 이를 토대로 음성합성 모 델을 생성할 수 있으며, 생성된 음성합성 모델을 이용하여 음성합성 관련 서비스를 사용자에게 제공할 수 있다. 본 장치는 음성합성과 관련된 웹 페이지, 앱 페이지, 프로그램 또는 애플리케이션을 사용자에게 제공하는 장치 또는 서버일 수 있다. 이하에서는 설명의 편의상, 본 장치에 의해 제공되는 음성합성과 관련된 프로그 램, 애플리케이션(어플, 앱), 서비스 및 플랫폼이 각각 본 프로그램, 본 앱, 본 서비스 및 본 플랫폼으로 달리 지칭될 수 있다. 사용자는 본 장치를 이용하는 사용자로서, 본 장치를 통해 음성합성을 수행하고 음성합성 관련 서비스 를 제공받고자 하는 사용자일 수 있으며, 남녀노소 누구나 일 수 있다. 사용자는 일예로 자신이 소지한 사용자 단말을 이용해 본 프로그램 또는 본 앱을 설치하거나 설치하지 않고 본 서비스를 이용할 수 있으며, 또한 본 프 로그램 또는 본 앱을 통해 회원가입을 하거나 하지 않고 본 서비스를 이용할 수 있다. 이에 따르면, 일예로 본 장치는 사용자 단말에 설치되는 프로그램 또는 애플리케이션(어플, 앱)의 형태로 구현 가능한 장치일 수 있으며, 이를 달리 표현하면, 본 장치를 통해 제공되는 음성합성 방법은 프로그램 또는 애플리케이션의 형태로 구현되어 사용자 단말을 통해 사용자에게 제공될 수 있다. 도 1을 참조한 설명에서 는 본 장치가 일예로 사용자 단말 내에 포함(설치)되어 있는 형태로 마련된 것(즉, 본 장치가 본 프로 그램 또는 본 앱으로 구현된 것)을 예로 들어 설명하기로 한다. 다만, 이에 한정되는 것은 아니고, 다른 일예로 본 장치는 사용자 단말과 데이터 송수신이 가능한 서버의 형태로 마련될 수 있으며, 서버의 형태로 마련된 본 장치는, 본 프로그램 또는 본 앱에 접속한 사용자 단말 의 작동(일예로 화면 표시)을 제어할 수 있다. 본 장치가 일예로 서버의 형태로 마련된 경우, 본 장치 는 서버, 음성합성 서버 등의 용어로 달리 지칭될 수 있으며, 이러한 경우 일예로 본 발명의 일 실시예에 따른 음성합성 시스템(미도시)은 본 장치(10, 서버) 및 사용자 단말을 포함할 수 있다. 이때, 본 발명의 일 실시예에 따른 음성합성 시스템은 본 시스템이라 달리 지칭될 수 있다. 본 시스템 내 본 장치는, 서버의 형태로 마련 된 경우, 사용자 단말과 네트워크를 통해 연동되어 데이터를 송수신할 수 있고, 이를 통해 사용자 단말의 작동 을 제어할 수 있다. 또한, 본 시스템 내에는 복수의 사용자가 소지한 복수개의 사용자 단말을 포함할 수 있는 데, 이때 사용자 단말에 대하여 설명된 내용은 이하 생략된 내용이라 하더라도 복수개의 사용자 단말 각각에 대 한 설명에도 동일하게 적용될 수 있다. 여기서, 네트워크는 일예로 3GPP(3rd Generation Partnership Project) 네트워크, LTE(Long Term Evolution) 네트워크, WIMAX(World Interoperability for Microwave Access) 네트워크, 인터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), 블루투스(Bluetooth) 네트워크, NFC(Near Field Communication) 네트워크, 위성 방송 네트워크, 아 날로그 방송 네트워크, DMB(Digital Multimedia Broadcasting) 네트워크 등을 포함할 수 있으나, 이에 한정된 것은 아니고, 다양한 유/무선 통신 네트워크를 포함할 수 있다. 사용자 단말은 본 장치를 이용하는 사용자가 소지한 단말을 의미할 수 있다. 사용자 단말은 일예로 PCS(Personal Communication System), GSM(Global System for Mobile communication), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(WCode Division Multiple Access), Wibro(Wireless Broadband Internet) 단말, 스마트폰(Smartphone), 스마트패드(SmartPad), 태블릿 PC, 노트북, 웨어러블 디바이스, 데스크탑 PC 등과 같은 모든 종류의 유무선 통신 장치를 포함할 수 있으며, 이 에 한정되는 것은 아니다. 이하 본 장치에 대한 구체적인 설명은 다음과 같다. 도 2는 본 발명의 일 실시예에 따른 음성합성 장치 내 수집부에 의한 음성데이터의 수집 과정을 설명하 기 위한 도면이다. 특히나, 도 2는 수집부에 의한 음성데이터의 자동수집 구조(즉, 음성데이터가 자동으로 수집되는 구조/과정)를 설명하기 위한 도면이다. 도 3은 음성합성 모델의 기존(일반적인) 훈련 과정을 설명하기 위한 도면이다. 도 1 내지 도 3을 참조하면, 본 장치는 수집부, 전처리부, 모델 관리부, 입력부, 음성합성 처리부, 데이터베이스부 및 제어부를 포함할 수 있다. 수집부는 음성데이터를 수집할 수 있다. 이때, 수집부에서 수집된 음성데이터는, 기 획득된 복수의 사 용자의 음성정보를 음성 분할한 후 검증함으로써 수집되는 상기 복수의 사용자의 음성데이터일 수 있다. 특히나, 수집부에서 수집된 음성데이터는, 인공지능 모델을 이용하여 상기 복수의 사용자의 음성정보를 자 동으로 음성 분할한 후 검증함으로써 수집되는 상기 복수의 사용자의 음성데이터일 수 있다. 다시 말해, 수집부에서 수집된 음성데이터는, 일예로 기 획득된 복수의 사용자의 음성정보(원본 데이터, 원 본 음성정보)에 대하여 인공지능 모델을 적용함으로써, 도 2에 도시된 것과 같이, 자동으로 상기 복수의 사용자 의 음성정보를 음성인식(Speech-to-Text, STT)을 이용(활용)하여 문장단위로 스크립트를 생성하고, 생성된 스크 립트를 기반으로 음성을 분할한(자른) 후(즉, 복수의 사용자의 음성정보를 문장단위로 하여 복수개로 음성 분할 을 시킨 후), 분할된 음성정보 각각을 검증(음성 검증)함으로써 자동 수집되는 복수의 사용자의 음성데이터(즉, 검증이 이루어진 검증된 복수의 사용자의 음성정보)를 의미할 수 있다. 여기서, 기 획득된 복수의 사용자의 음성정보는, 원본 데이터, 원본 음성정보 등의 용어로 달리 지칭될 수 있다. 이러한 기 획득된 복수의 사용자의 음성정보는 일예로 인터넷에서 웹크롤링(wep crawling)을 통하여 기 획득(수집)되는 복수의 방송인(유명인 등)의 연설 자료, 방송 자료, 및 강의 자료 중 적어도 하나에 해당(대 응)하는 음성정보(음성데이터)를 포함할 수 있다. 여기서, 자료는 영상 콘텐츠(동영상) 등을 의미할 수 있다. 일반적으로 음성데이터를 수집하는 방법에는 2가지가 존재한다. 여기서, 첫번째 방법(제1 방법)으로는 실제 성 우를 섭외하여 대본을 주고 해당 대본을 읽도록 하여 데이터를 수집하는 방법이 있다. 두번째 방법(제2 방법)으 로는 특정인의 연설, 방송, 강의 자료 등을 기반으로 데이터를 수집하고 어노테이션(annotation)하여 데이터를 수집하는 방법이 있다. 여기서, 어노테이션은 프로그램의 소스코드 안에 다른 프로그램을 위한 정보를 미리 약 속된 형식으로 포함시킨 것을 말하며, 주석을 의미한다. 이때, 상술한 첫번째 방법으로 음성데이터를 수집하면, 가장 고품질의 음성데이터를 획득(수집)하는 것이 가능 하나, 성우를 통해 직접 녹음을 해야 한다는 번거로움이 존재하며, 유명인의 목소리를 직접 녹음하는 것은 어려 운 일이라 할 수 있다. 상술한 두번째 방법으로 음성데이터를 수집하면, 직접 녹음을 하지 않아도 된다는 장점이 있으나, 이 방법은 방 송이나 연설 등의 자료를 사람이 직접 확인하며 음성을 자르고 그에 맞는 스크립트를 사람이 수작업으로 직접 입력해야 하는 과정이 필요하므로, 매우 오랜 시간이 소요되는 작업이라 할 수 있으며, 최소 2천 문장을 수집해 야 단일 화자로 할 수 있는 음성합성이 가능하기 때문에 최소 2주 이상의 긴 시간이 소요되는 특성(단점)이 있 다. 따라서, 수집부는, 복수의 사용자의 음성데이터(음성데이터들)를 수집할 때, 일예로 상술한 두번째 방법(즉, 제2 방법)을 인공지능(Artificial Intelligence, AI) 모델을 통해 보완하여 자동으로 수집된 음성데이 터들의 음성을 자르고 텍스트(분할한 음성에 대응하는 텍스트)를 입력할 수 있는 소프트웨어를 이용하여 상기 복수의 사용자의 음성데이터를 수집할 수 있다. 여기서, 상기 소프트웨어는 본 장치를 관리하는 관리자(즉, 본 장치를 생성하여 배포하는 관리자, 혹은 관리자가 속한 회사)에 의해 자체 개발(생성, 제 작)된 것일 수 있다. 상기 소프트웨어는, 특정인의 음성(원본 데이터)이 들어왔을 때, 음성인식(Speech-to- Text, STT)을 활용하여 문장단위로 스크립트를 만들고(생성하고) 이를 기반으로 음성을 자르도록 마련될 수 있 다. 즉, 달리 표현하면, 상기 소프트웨어는 웹크롤링을 통해 획득(수집)된 복수의 사용자(특정인, 유명인 등)의 음성정보(원본 데이터)가 입력으로 주어지면(즉, 입력값으로 입력되면), 입력된 음성정보에 대하여 음성인식 (STT)을 이용해 문장단위로 스크립트를 생성하고, 생성된 스크립트를 기반으로 입력된 음성정보를 문장단위로 음성 분할 시키도록 마련될 수 있다. 이후, 수집부는, 문장단위의 스크립트가 잘리게 되었으면, 해당 음성이 우리가 요구하는 특정인의 목소리인 지, 품질은 괜찮은지 등을 화자 인식 모델(인공지능 모델)을 통해 수행할 수 있다. 즉, 수집부는 음성 분할 이 이루어진 후, 분할된 음성정보들 각각에 대해 음성 검증을 수행하여, 분할된 음성정보들 중 사용 가능으로 음성 검증이 된 일부 음성정보들만을 복수의 사용자의 음성데이터로서 수집하고, 분할된 음성정보들 중 사용 불가로 음성 검증이 된 나머지 음성정보들(즉, 분할된 음성정보들 중 상기 일부 음성정보들을 제외한 나머지 음성 정보들)의 경우 수집하지 않을 수 있다. 여기서, 음성 검증은 분할된 음성정보가 사용 가능한 음성정보인지 또는 사용 불가한 음성정보인지를 검증(판단, 분석)하는 과정을 의미하며, 구체적으로 분할된 음성정보에 대하여 기 요구되는 특정인의 목소리인 지 여부, 음성정보의 품질이 기 설정된 품질 허용 범위에 속하는지 여부, 두명 이상의 사용자의 발화 목소리가 중복되어 있는지 여부 등을 판단(분석, 검증)하는 과정을 의미할 수 있다. 이러한 음성 검증은 일예로 기 마련 된 화자 인식 모델(인공지능 모델)을 통해 수행될 수 있다. 여기서, 수집부는 음성정보의 품질이 기 설정 된 품질 허용 범위에 속하는지 여부를 판단할 때, 일예로 종래에 기 공지되었거나 향후 개발되는 다양한 음성정 보의 품질 판단 기준(방식, 방법)을 적용하여 품질을 판단할 수 있다. 일예로, 수집부는, 분할된 음성정보들 중 어느 한 음성정보에 대하여 음성 검증을 수행한 결과, 상기 어느 한 음성정보가 '기 요구되는 특정인의 목소리가 맞고, 음성정보의 품질이 기 설정된 품질 허용 범위에 속하며, 두명 이상의 사용자의 발화 목소리가 중복되는것 없이 단일 사용자의 발화 목소리만 존재하는 것으로 판단'되는 경우, 상기 어느 한 음성정보를 사용 가능으로 음성 검증이 된 음성정보(즉, 사용 가능한 음성정보)로 식별할 수 있다. 반면, 수집부는 상기 어느 한 음성정보에 대하여 음성 검증을 수행한 결과, 상기 어느 한 음성정 보가 '기 요구되는 특정인의 목소리가 아닌 경우, 음성정보의 품질이 기 설정된 품질 허용 범위에 속하지 않는 경우, 및 두명 이상의 사용자의 발화 목소리가 중복되어 있는 경우 중 적어도 하나에 속하는 것으로 판단'되는 경우, 상기 어느 한 음성정보를 사용 불가로 음성 검증이 된 음성정보(즉, 사용 불가한 음성정보)로 식별할 수 있다. 이후, 수집부는 분할된 음성정보들 중 사용 가능한 음성정보로 식별된 음성정보들만을 복수의 사용자 의 음성데이터로서 수집할 수 있다. 일예로, 수집부가 복수의 사용자로서 100명의 사용자의 음성정보(즉, 연설 자료, 방송 자료와 같은 100개의 음성정보)를 획득한 후 음성인식(STT)을 통해 문장단위로 음성 분할을 하였더니, 100개의 음성정보 각각마다 음 성이 10개씩 분할이 됨에 따라, 획득된 음성정보들이 총 1000개로 음성으로 분할되었다고 하자(즉, 분할된 음성 정보들이 총 1000개 마련되었다고 하자). 이때, 1000개의 분할된 음성정보들 각각을 음성 검증을 하였더니, 800 개의 분할된 음성정보가 사용 가능 음성정보로 식별되고, 200개의 분할된 음성정보가 사용 불가 음성정보로 식 별되었다고 하자. 이러한 경우, 수집부는 사용 가능 음성정보로 식별된 800개의 분할된 음성정보를 복수의 사용자(100명의 사용자)의 음성데이터(음성 검증이 완료된 검증된 복수의 사용자의 음성데이터)로서 수집할 수 있다. 수집부는 상술한 과정을 통해 복수의 사용자의 음성데이터를 수집함(특히, 복수의 사용자의 사용 가능 음성 정보만을 수집함)으로써, 이를 통해 후술하는 모델 관리부에서 이용되는 음성합성 모델의 훈련(학습)에 바 로 사용할 수 있는 형태의 음성데이터(음성과 텍스트 데이터)의 수집이 가능하며, 일예로 2천 문장을 수집하는 데에 2시간 내외(이내)의 짧은 시간이 소요되도록 할 수 있다. 이러한 수집부는 상술한 방식으로 음성데이터를 수집함에 따라, 종래에 사람이 음성데이터의 수집을 직접 수행할 경우에 1명의 작업자가 최소 2주 이상의 긴 시간이 소요됐던 작업 방식과 대비하여, 보다 적은 노력과 비용, 및 시간으로 음성합성 모델을 바로 빠르게 만들어낼 수 있다(생성할 수 있다)는 장점(효과)을 제공할 수 있다. 전처리부는, 수집부에서 수집된 음성데이터(특히, 검증된 복수의 사용자의 음성데이터)를 전처리할 수 있다. 전처리부는 상기 전처리의 수행시, 기 생성된(개발된) 발음변환기를 이용하여 상기 수집부에서 수집된 음성데이터에 대응하는 텍스트를 발음(음성)으로 변환시키는 발음 변환 처리를 수행할 수 있다. 여기서, 발음변환기는 본 장치를 관리하는 관리자(즉, 본 장치를 생성하여 배포하는 관리자, 혹은 관리자가 속 한 회사)에 의해 자체 개발(생성, 제작)된 것일 수 있다. 구체적으로, 전처리부는, 수집된 복수의 사용자의 음성데이터(음성과 텍스트 데이터)에 대하여 전처리를 수 행할 수 있다. 일반적으로 음성데이터를 전처리하기 위해서는 인공지능 모델이 활용할 수 있는 형태로 음성데이터의 샘플레이 트(sample rate, 샘플속도), 비트레이트(bit rate, 비트율, 전송속도) 등을 적절하게 변경해 주어야 할 필요가 있다. 이에, 본 장치의 전처리부는, 상술한 과정(변경의 과정)을 자동화하여, 음성이 들어왔을 때(수집 된 음성데이터가 입력되었을 때), 자동으로 모델(음성합성 모델이 훈련할 수 있는 형태로 음성(음성데이터)의 포맷을 변경할 수 있다. 즉, 전처리부는, 수집부에서 수집된 음성데이터가 자동으로 음성합성 모델이훈련할 수 있는 형태로의 포맷(음성데이터의 포맷)으로 변경되도록 할 수 있다. 음성합성 모델에는 일반적으로 텍스트를 그대로 넣는 자소(Grapheme)(문자소, 의미를 나타내는 최소 문자 단위) 입력방식과, 음성을 읽는 방법으로 변경하여 음소를 넣는 음소(Phoneme) 입력 방식이 존재한다. 여기서, 자소 (Grapheme)는 문자소로도 불리우며, 의미를 나타내는 최소 문자 단위를 의미한다. 음소(Phoneme)는 어떤 언어에 서 의미 구별 기능을 갖는 음성상의 최소 단위를 의미하며, 예를 들어 영어에서는 sip에 쓰인 /s/와 zip에 쓰인 /z/가 두 개의 다른 음소가 된다. 최근에는 음성합성 모델에 Phoneme 방법을 통해 입력하는 방식이 선호되고 있으며, 이는 Grapheme 대비 적은 기호의 개수만을 가지고도 발음을 표현할 수 있어 효율적이기 때문이라 할 수 있다. 그런데, 영어의 경우에는 기존 공개된 ‘Phonemize’라는 오픈소스를 활용하면, 큰 어려움 없이 Phoneme 으로의 변경이 가능하지만, 한국어의 경우에는 해당 오픈소스의 활용시 성능이 떨어지고, 최신어 및 동음이의어 의 발음법에 대한 처리가 부족하여 해당 오픈소스를 활용(이용)하기 어려운 측면이(문제가) 있다. 이에 따라, 본 장치의 전처리부는 상술한 문제를 해소하기 위해, 본 장치를 관리하는 관리자에 의 해 기 생성(개발)된 발음변환기를 이용하여 수집된 음성데이터에 대응하는 텍스트를 발음으로 변환시키는 발음 변환 처리를 수행할 수 있다. 이때, 기 생성된 발음변환기는, 후술하여 설명하는 제1 과정 내지 제4 과정의 수행을 통해 발음 변환 처리를 수 행할 수 있다. 여기서, 제1 과정은 수집된 음성데이터 내 텍스트에 대한 정제 과정으로서, 입력된 텍스트를 처리하여 인코딩, 더블스페이스 등, 변환에 문제를 발생시키는 데이터를 정제하는 과정을 의미할 수 있다. 즉, 상기 발음변환기는, 발음 변환 처리의 수행을 위해 먼저, 수집된 음성데이터에 대응하는 텍스트에 대하여, 상기 텍스 트 내 포함되어 있는 기 정의된 문제 발생 데이터를 정제시키는 제1 과정을 수행할 수 있다. 여기서, 기 정의된 문제 발생 데이터는 발음 변환 처리의 수행시 문제를 발생시키는 것으로 미리 정의되어 있는 데이터를 의미하는 것으로서, 인코딩 데이터, 드블세피에스 데이터 등을 포함할 수 있다. 제2 과정은, 정제 처리된 문장의 문법사항을 분석하는 과정을 의미할 수 있으며, 특히 문장 내의 조사, 명사, 동사, 어미 등을 분석하는 과정을 의미할 수 있다. 즉, 상기 발음변환기는, 제1 과정의 수행 이후, 제1 과정에 서 정제된 음성데이터에 대응되는 문장에 대하여 문법사항을 분석하는 제2 과정을 수행할 수 있다. 이때, 발음 변환기는 제2 과정에서 문법사항의 분석시, 문장 내에 포함되어 있는 조사, 명사, 동사, 어미 등에 관한 문법의 특성(구조, 종류, 규칙 등)을 분석함으로써 문법사항 분석 결과정보를 도출할 수 있다. 제3 과정은 문법 데이터(즉, 문법사항 분석 결과정보)와 텍스트를 기반으로 한국어 음운론에 기반하여 발음 변 형(변환)을 수행하는 과정을 의미할 수 있다. 즉, 상기 발음변환기는, 제2 과정이 수행되고 나면, 제2 과정에서 도출된 정제된 음성데이터의 문법사항 분석 결과정보인 문법 데이터와, 정제된 음성데이터에 대응하는 텍스트를 기반으로 하여, 한국어 음운론에 기반하여 정제된 음성데이터에 대응하는 텍스트 중 한국어를 발음으로 변환시 키는 제3 과정을 수행할 수 있다. 음운론(音韻論)은 언어학의 하위 분야의 하나로 특정 개별 언어 또는 여러 언어의 소리 체계를 연구하는 분야이 며, 음성학이 말소리의 물리적인 발성과 인지를 연구하는 데에 비해 음운론은 주어진 언어 내에서 또는 범언어 적으로 소리가 어떻게 기능하는가를 기술한다. 음운론의 중요한 연구 분야 중의 하나는 한 개별 언어 내에서 어떠한 소리가 변별적 단위를 이루는가를 연구하 는 것이다. 예를 들어, 한국어에서 /ㅂ/, /ㅍ/, /ㅃ/는 변별적인 소리 단위이며 이들을 음소라고 한다. 이들이 서로 다른 음소라는 것은 ‘불’, ‘풀’, ‘뿔’과 같은 서로 다른 의미를 지칭하는 최소대립쌍의 존재를 통해 확인할 수 있다. 음운론은, 음소에 관한 연구 뿐만 아니라, 음절 구조, 악센트, 억양, 성조, 리듬 등을 연구한다. 또한 음운론의 목적은 연구 대상 언어에 존재하는 음형(音型, 영어: sound patterns)을 연구한다. 음운론이라는 용어 대신 음 소론이라는 용어가 사용되는 경우도 있는데, 이 경우 동일한 분야를 지칭할 수도 있으나 음소론과 운소론을 개 념 상 구별하기 위하여 사용할 수도 있다. 이러한 의미에서 음소론은 음소만을 연구 대상으로 하는 분야를 지칭 하며, 운소론은 운율적 특징인 소리의 높낮이(고저), 길이(장단), 세기(강약) 등의 악센트와 억양 등 운소를 연 구하는 분야를 지칭한다. 음운론에서 의미를 구별하는 소리의 제일 작은 단위를 음소라고 한다. 상기 발음변환기는, 제3 과정에서 한국어 음운론에 기반하여 정제된 음성데이터에 대응하는 텍스트를 발음으로 변환시킬 수 있다.제4 과정은 한국어가 아닌 영어 및 숫자에 대하여 사전에 가지고 있는 발음셋을 기반으로 발음 변형(변환)을 수 행하는 과정을 의미할 수 있다. 즉, 상기 발음변환기는, 제3 과정을 수행하고 나면, 이후 정제된 음성데이터에 대응하는 텍스트 내에서 한국어를 제외한 나머지 텍스트들(일예로 영어, 숫자 등)을 관리자에 의해 사전에 기 마련된 발음셋(발음 데이터셋)을 이용해 발음(음성)으로 변환시키는 제4 과정을 수행할 수 있다. 발음변환기는, 상술한 제1 과정 내지 제4 과정을 수행함으로써, 수집된 음성데이터에 대응하는 텍스트를 발음 (음성)으로 변환시키는 발음 변환 처리를 수행하고, 이를 통해 발음 변환 처리된 데이터(즉, 수집된 음성데이터 에 대응하는 텍스트의 발음(음성) 데이터)를 제공(출력)할 수 있다. 즉, 전처리부는 수집된 음성데이터에 대하여 전처리를 수행하되, 전처리로서 기 생성된 발음변환기를 이용 한 발음 변환 처리를 수행할 수 있으며, 이를 통해 전처리된 음성데이터를 제공할 수 있다. 여기서, 전처리된 음성데이터는 수집된 음성데이터에 대응하는 텍스트를 발음으로 변환시킨 발음(음성) 데이터를 의미할 수 있으 며, 이러한 전처리된 음성데이터는 수집된 음성데이터에 대응하는 발음 데이터(발음열 데이터), 발음 변환 처리 된 데이터 등의 용어로 달리 지칭될 수 있다. 본 장치에서 고려되는 발음변환기는 다음과 같은 효과를 제공할 수 있다. 즉, 본 장치 내 전처리부(1 2)는, 기 생성된 발음변환기로 전처리를 수행함으로써, 모델 관리부에서 생성되고 훈련되는 음성합성 모델 의 성능 및 정확도를 종래의 음성합성 모델과 대비하여 높은 수준으로 향상시킴과 동시에, 문법사항을 직접적으 로 고려하여 발음 변환을 수행함으로써 종래 대비 보다 높은 정확도로 텍스트를 발음(음성)으로 변환시킬 수 있 다. 이러한 본 장치에서 고려되는 기 생성된 발음변환기는, 종래(기존)의 오픈소스(일예로 g2pK, KoG2P 등)와 대비(비교)했을 때, 처리속도에서 30%, 정확도에서 40% 이상의 높은 성능을 가지는 장점(특성)이 있다. 이러한 종래의 오픈소스는, 예시적으로, Python 환경에서 작동하고 문자열을 발음열로 변환해주는 발음 변환기 로서, Park에서 공개한 ‘g2pk’, Cho에서 공개한 ‘KoG2P’ 등을 의미할 수 있다. 이후, 모델 관리부는, 전처리부에서 전처리된 음성데이터를 이용하여 음성합성 모델을 생성하고 훈련 (학습)시킬 수 있다. 이때, 모델 관리부에서 생성되는 음성합성 모델은, 후술하여 설명하는 제1 훈련과 제2 훈련을 포함한 새로 운 훈련 기법(방법)을 적용시킴으로써 새롭게(신규하게) 생성(개발)된 신규 음성합성 모델(즉, 새로운 훈련 기 법이 적용된 성능 좋은 음성합성 모델)일 수 있다. 즉, 모델 관리부는, 제1 훈련과 제2 훈련으로 훈련(학습)시킨 음성합성 모델을 생성(개발)하고 관리할 수 있다. 다시 말해, 모델 관리부에서 생성된 음성 합성 모델은, 제1 훈련과 제2 훈련의 수행을 기반으로 생성된 음성합성 모델일 수 있다. 모델 관리부는, 상기 음성합성 모델(신규 음성합성 모델)의 생성을 위해, 전처리된 음성데이터(즉, 발음 변 환 처리된 복수의 사용자의 음성데이터)를 다중화자 음성합성 모델로 훈련시키는 제1 훈련, 및 전처리된 음성데 이터를 화자별로 구분하고, 화자별 전처리된 음성데이터를 기반으로 전이학습(transfer learning)을 하는 제2 훈련을 수행할 수 있다. 모델 관리부는 일예로 기존(종래) 음성합성 모델에 대하여 제1 훈련과 제2 훈련을 수행함(적용시킴)으로써, 제1 훈련과 제2 훈련이 이루어진 학습(훈련)된 음성합성 모델을 생성(마련, 개발)할 수 있다. 전이학습(transfer learning)은, 학습 데이터가 부족한 분야의 모델 구축을 위해 데이터가 풍부한 분야에서 훈 련된 모델을 재사용하는 머신러닝 학습 기법, 특정 분야에서 학습된 신경망의 일부 능력을 유사하거나 전혀 새 로운 분야에서 사용사용되는 신경망의 학습에 이용하는 학습 방법, 데이터 세트가 유사한 분야에 학습치를 전이 하여 파인튜닝(Fine Tuning) 기반으로 신경망을 학습 하고 재사용하는 기법을 의미한다. 즉, 전이학습은 사전 학습된 모델(pre-trained model)을 활용하여 새로운 모델을 학습하는 과정이며, 기존에 학습되어져 있는 모델을 기반으로 새로운 목적에 맞게(데이터에 맞게) 변형하고 이미 학습된 모델 웨이트(Weights)로부터 학습을 업데이 트하는 방법을 의미한다. 전이학습의 절차는, 사전 학습 모델을 선택하는 제1 절차, 데이터 크기와 유사성을 분석하는 제2 절차, 새로운 모델을 위한 파인튜닝을 수행하는 제3 절차로 이루어질 수 있다. 제1 절차에서는 사전 학습 모델 중 새로운 문 제 해결에 적합해보이는 모델을 선택할 수 있다. 제2 절차에서는 새로운 문제는 어떤 상황인지, 어떤 방법으로 파인튜닝을 해야하는지에 대한 결저잉 이루어질 수 있다. 전이학습의 방법은, 파인튜닝(미세조정) 과정, 특성 추출 과정, 조인트 트레이닝(joint training) 과정, 및 망 각없는 학습(learning without forgetting, lwf) 수행 과정으로 이루어질 수 있다. 파인튜닝 과정에서는 새로운 데이터로 다시 한번 가중치를 세밀하게 조정하도록 학습하며, 기존 데이터는 기존대로 분류한다. 특성 추출 과정은 기존 가중치는 그대로 놔둔 뒤, 새로운 레이어를 추가해서 이를 학습하고 최종 결과를 내도록 학습하는 과 정이다. 조인트 트레이닝 과정은 새로운 데이터를추가하여 처음부터 다시 시작하는 과정이다. 망각없는 학습 수 행 과정은 새로운 데이터로 가중치를 세밀하게 조정하며, 기존 데이터 분류 결과 또한 개선이 가능하다. 전이학 습에 대한 설명은 종래에 기 공지된 학습법이므로, 본 발명에서 구체적인 설명은 생략하기로 한다. 모델 관리부는 음성합성 모델을 생성(개발)하고, 이를 훈련시키며 테스트할 수 있다. 모델 관리부는 음 성합성 모델의 생성을 위해, VITS 라는 오픈소스 인공지능 모델을 활용(이용)하여 음성합성 모델을 생성할 수 있다. 모델 관리부는 음성합성 모델의 생성을 위해 기존 모델(기존 음성합성 모델)을 활용(이용)할 수 있으며, 기 존 모델을 토대로 제1 훈련과 제2 훈련을 적용시킴으로써 새로운 훈련 기법(방법)으로 훈련된 음성합성 모델을 생성할 수 있다. 본 발명에서는, 소량의 음성합성 데이터(소량의 전처리된 음성데이터)만으로도 성능 좋은 음성합성 모델을 생성 (개발)하기 위해 새로운 훈련 기법(상술한 제1 훈련과 제2 훈련을 포함한 훈련 기법)을 고안한다. 여기서, 음성 합성 데이터는 전처리된 음성데이터를 의미할 수 있고, 일예로 5분 내외의 100개 문장이 포함되어 있는 음성데 이터를 의미할 수 있다. 종래에 음성합성 모델을 훈련시키는 과정(기존 훈련 과정)은, 일예로 도 3에 도시된 것과 같이 음성 생성기를 통해 음성합성 모델이 텍스트를 기반으로 하여 음성을 생성하면, 음성 평가기를 통해 정답 음성과 생성된 생성 음성을 비교하여 점수를 평가하고 이를(평가된 점수를) 다시 음성 생성기로 전달하여 평가점수를 참고하여 음성 생성기를 업데이트하는 과정으로 훈련이 이루어진다. 그러나, 이러한 기존 훈련 과정에서는, 음성 평가기와 음 성 생성기를 모두 훈련시키는 데에 많은 양의 데이터(학습 데이터)와 많은 시간을 필요로 하는 문제가 있다. 따라서, 모델 관리부는, 적은 양의 데이터(학습 데이터, 즉 전처리된 음성데이터)로도 고품질의 음성합성 모델을 생성할 수 있도록 하기 위해 다음과 같은 훈련 기법을 이용(즉, 제1 훈련과 제2 훈련을 수행, 적용)할 수 있다. <제1 훈련과 관련하여> 모델 관리부는, 제1 훈련의 수행시에, 일예로 3000명 이상의 사용자(화자)가 발화한 1000만 문장(즉, 1000 만개의 전처리된 음성데이터)을 기반으로 대형 다중화자 음성합성 모델을 훈련시킬 수 있다. 이때, 모델 관리부 는 제1 훈련의 수행시에, 총 2주간(2주동안) 훈련을 수행(진행)할 수 있고, 음성 생성기와 음성 평가기를 모두 훈련시킬 수 있다. 이때, 상술한 본 규모의 데이터(즉, 3000명 이상의 사용자가 발화한 1000만 문장)는 음성 생성기와 음성 평가기를 모두 충분하게 학습(훈련)시킬 수 있는 분량의 데이터(학습 데이터)라 할 수 있다. 이때, 제1 훈련의 수행 과정에서 훈련되는 음성 생성기는 화자 정보를 임베딩할 수 있는 기능도 포함하고 있을 수 있다. 모델 관리부는 제1 훈련의 수행을 통해 일예로 3000명 이상의 사용자(화자)의 전처리된 음성 데이터를 하나의 모델(즉, 1개로 마련된 대형 다중화자 음성합성 모델)로 훈련시킬 수 있다. <제2 훈련과 관련하여> 모델 관리부는, 제2 훈련의 수행시에, 일예로 한명의 화자의 목소리(음성)로 구성된 100문장(5분 내외)의 데이터를 기반으로 전이학습(fine-tuning)을 수행할 수 있다. 즉, 모델 관리부는 제2 훈련의 수행시에, 1명 의 사용자가 발화한 5분 내외의 100개 문장이 포함된 음성데이터(전처리된 음성데이터, 학습 데이터)를 기반으 로 전이학습을 수행할 수 있다. 이때, 전체 모델을 다 훈련(즉, 음성 생성기와 음성 평가기를 모두 훈련)시키는 경우 데이터(학습 데이터)의 부족으로 인해 충분한 학습이 이루어지지 않을 수 있다. 따라서, 이러한 문제를 해소하기 위해, 모델 관리부는 제2 훈련의 수행시에 음성 생성기는 훈련시키고, 음성 평가기는 훈련시키지 않을 수 있다(즉, 음성 생성기와 음성 평가기 중 음성 생성기만을 훈련시킬 수 있다). 즉, 모델 관리부는 제2 훈련의 수행시에, 음성 생성기는 훈련되도록 하나, 음성 평가기는 훈련되지 않도록 하여, 우수한 데이터(학 습 데이터)로 훈련된 음성 생성기가 활용(이용)되도록 할 수 있다. 모델 관리부는 제2 훈련의 과정에서 이 처럼 음성 생성기를 훈련시킴으로써, 적은양(소량)의 데이터(소량의 학습데이터, 즉 소량의 음성합성 데이터)로 도 충분하게 음성합성 모델의 생성(개발)이 가능하도록 할 수 있다. 또한, 앞서 말한 제1 훈련 과정에서 훈련되는 대형 다중화자 음성합성 모델은 재활용될 수 있으며, 이처럼 재활 용될 수 있음에 따라 모델 관리부는 기 훈련된 대형 다중화자 음성합성 모델을 이용해 새로운 목소리(새로 운 화자의 목소리)를 쉽게 만들어 내어(생성하여) 제공되도록 할 수 있으며, 여러 명의 화자(사용자)가 발화한 문장을 100개씩을 사용한다면 동시에 여려 명의 화자에 대한 음성합성 데이터에 대한 훈련이 가능할 수 있다.이에 따르면, 모델 관리부는, 전처리부에서 전처리된 음성데이터를 이용하여 기존 모델(기존 음성합성 모델)을 상술한 제1 훈련 및 제2 훈련을 포함한 새로운 훈련 방법으로 훈련시킴으로써, 제1 훈련 및 제2 훈련이 이루어진 음성합성 모델을 생성할 수 있다. 달리 말해, 모델 관리부는 제1 훈련과 제2 훈련의 수행을 통해 음성 생성기와 음성 평가기 중 적어도 하나를 훈련(학습)시킬 수 있고, 이를 토대로 이후 훈련된 음성 생성기와 훈련된 음성 평가기를 이용해 음성합성을 수행하는(즉, 합성음을 생성해 제공하는) 음성합성 모델을 생성할 수 있다. 입력부는 사용자로부터 음성합성의 수행을 위한 문장 데이터를 입력받을 수 있다. 여기서, 사용자는 본 장 치를 이용하는 사용자로서, 본 장치를 통해 음성합성을 수행하고 음성합성 관련 서비스를 제공받고자 하는 사용자일 수 있다. 여기서, 문장 데이터는 문자열 데이터, 텍스트 등을 의미할 수 있다. 이후, 음성합성 처리부는, 입력부에서 입력된 문장 데이터를 모델 관리부에서 생성되고 훈련된 음 성합성 모델(즉, 모델 관리부에 의해 기 생성되고 훈련된 음성합성 모델)에 적용시켜 음성합성을 수행함으 로써, 상기 문장 데이터에 대응하는 합성음(음성, 음성합성이 된 합성된 음성)을 제공할 수 있다. 즉, 음성합성 처리부는 문장 데이터를 음성합성 모델의 입력으로 적용시킴으로써, 음성합성 모델이 입력된 문장 데이터에 대하여 음성합성을 수행하도록 하고, 이를 통해 음성합성 모델로부터 출력되는 출력값인 문장 데이터에 대응하 는 음성(합성음, 오디오 데이터, 음성합성 결과물)을 사용자에게 제공할 수 있다. 즉, 음성합성 처리부는 사용자가 입력한 문장 데이터(텍스트 데이터)를 모델 관리부에서 생성된 음성합 성 모델을 이용해 음성으로 합성하여 제공(즉, 문장 데이터를 음성으로 합성하여 음성화한 음성합성 데이터를 제공)할 수 있다. 또한, 상술한 설명에서는 일예로 입력부가 사용자로부터 문장 데이터(텍스트)를 입력받으면, 음성합성 처리 부가 음성합성 모델을 이용하여 문장 데이터에 대응하는 합성음(음성)을 제공하는 것으로 예시하였으나, 이 는 본 발명의 이해를 돕기 위한 하나의 예시일 뿐, 이에만 한정되는 것은 아니다. 다른 예로, 입력부는 사 용자로부터 음성발화 데이터를 입력받을 수 있고, 이후 음성합성 처리부는 일예로 입력된 음성발화 데이터 에 대하여 STT를 적용하여 문자열 데이터(텍스트 데이터)로 변환시키고, 사용자가 기 입력한 요구조건을 고려하 여 상기 요구조건에 해당하는 응답 텍스트를 생성하고, 상기 생성된 응답 텍스트를 음성합성 모델에 적용함으로 써 상기 응답 데이터에 대응하는 음성을 합성(응답 텍스트에 음성을 합성)한 합성음(음성합성 결과물)을 제공할 수 있다. 여기서, 기 입력한 요구조건은 일예로 다양한 종류의 세계 언어들 중 제1 언어의 음성을 제2 언어의 음성으로 바꾸어 출력되도록 하는 언어 번역 서비스의 제공을 요청하는 제1 요구조건, 사용자가 질의를 하면 질 의에 대응하는 응답의 제공을 요청하는 제2 요구조건 등을 포함할 수 있다. 데이터베이스부는 본 장치에서 고려되는 각종 정보를 저장할 수 있다. 제어부는 본 장치에 포 함된 각 부의 작동을 제어할 수 있다. 또한, 제어부는 일예로 본 장치가 서버의 형태로 마련된 경우, 본 장치와 네트워크를 통해 연동된 사용자 단말의 작동(화면표시 등)을 제어할 수 있다. 이러한 본 장치는 사용자가 입력한 문자 데이터 또는 음성발화 데이터를 토대로 그에 대응되는 음성합성 결 과물을 기 학습된 음성합성 모델을 이용하여 보다 정확하고 빠르게 도출하여 사용자에게 제공되도록 할 수 있다. 또한, 일예로 본 장치 내 제어부는 사용자로부터 음성발화 데이터가 입력된 경우, 일예로 음성발화 데 이터에서 음성 강약에 따른 자막 파일을 생성하여 사용자 단말의 화면(혹은, 본 장치에 자체 마련된 디 스플레이부의 화면)에 제공할 수 있다. 또한, 제어부는 일예로 사용자로부터 입력되는 음성발화 데이터가 두명 이상의 사용자가 대화를 나누는 상황에 대응하는 음성발화 데이터인 경우, 입력되는 사용자의 음성발화 데 이터를 음성합성 모델을 이용해 실시간으로 분석하고, 이후 음성발화 데이터에서 음성 강약에 따른 자막 파일을 생성해 화면에 제공하되, 이때 제공된 자막 파일 상에서 음성 강약에 따라서 음성발화 데이터 내 상기 두명 이 상의 사용자가 서로 강조하는 부분의 자막(텍스트) 상에 스타카토(.)로 표시하거나, 하이라이트로 표시하거나, 애니메이션 효과를 적용해 화면에 표시할 수 있다. 이때, 일예로 음성발화 데이터에서의 음성의 강약의 식별 (판단)은 종래에 기 공지되었거나 향후 개발되는 음성 강약 식별 방법(기준)을 적용함으로써 이루어질 수 있다. 이러한 본 장치는 자막 파일을 제공함으로써, 사용자가 타 사용자(상대방)와 대화를 할 때에 서로가 어느 내용을 강조하고 있는지를 보다 직관적으로 인식 가능하도록 할 수 있다. 또한, 본 장치는 음성합성 모델을 이용하여 사용자에게 언어학습 서비스를 제공할 수 있으며, 이는 도 4를 참조하여 보다 쉽게 이해될 수 있다.도 4는 본 발명의 일 실시예에 따른 음성합성 시스템의 개략적인 구성을 나타낸 도면이다. 이하에서는 설 명의 편의상, 본 발명의 일 실시예에 따른 음성합성 시스템을 본 시스템이라 하기로 한다. 도 5는 본 발명의 일 실시예에 따른 음성합성 장치가 입력받는 학습용 문서의 예를 나타낸 도면이다. 도 4 및 도 5를 참조하면, 본 시스템은 음성합성 장치, 사용자 단말 및 기능성 펜을 포함할 수 있다. 음성합성 장치는 상술한 도 1을 참조하여 설명된 음성합성 장치와 동일한 구성으로서 본 장치를 의 미할 수 있다. 다만, 도 1에 도시된 본 장치는 사용자 단말 내에 설치(포함)된 형태로 마련된 예를 설 명하고, 도 4에 도시된 본 장치는 서버의 형태로 마련된 예를 설명한다는 점에서만 차이가 있을 수 있다. 따라서 상술한 본 장치에 대하여 설명된 내용은 이하 생략된 내용이라 하더라도 도 4에 도시된 본 장치(1 0)에 대한 설명에도 동일하게 적용될 수 있다. 사용자 단말은 사용자가 소지한 단말로서, 스마트폰, 태블릿PC 등의 휴대 단말일 수 있다. 사용자 단말(2 0)에 대한 설명은 앞서 자세히 설명했으므로, 이하 중복되는 설명은 생략한다. 기능성 펜은 본 장치에 의해 제공되는 언어학습 서비스를 이용하는 사용자의 학습 편의를 돕기 위해 제 작된 펜으로서, 일예로 본 장치에 의해 제작되고 배포되는 펜일 수 있다. 사용자는 일예로 일정 금액을 지 불하고 본 장치를 통해 기능성 펜을 구매하여 이용 가능할 수 있다. 일예로 사용자 단말 및 기능성 펜 각각에는 본 장치에 의해 제공되는 본 프로그램 또는 본 앱이 기 설치되어 있을 수 있다. 기능성 펜은 도면에 도시하지는 않았으나 펜 제어부(미도시)를 포함할 수 있다. 본 장치는 사용자 단말 및 기능성 펜 각각과 네트워크를 통해 연동되어 데이터를 송수신할 수 있다. 네트워크에 대한 설명은 앞서 자세히 설명했으므로, 이하 중복되는 설명은 생략한다. 또한, 본 장치 의 제어부는 사용자 단말 및 기능성 펜 각각의 작동을 제어할 수 있다. 펜 제어부(미도시)는 본 장치로부터 제공받은 제어신호에 따라 기능성 펜의 작동을 제어할 수 있다. 이하에서는 기능성 펜을 설명함에 있어서, 일예로 도 4의 도면을 기준으로 9시-3시 방향을 전후방향, 12시- 6시 방향을 상하방향이라 하기로 하며, 이러한 방향 설정은 본 발명의 이해를 돕기 위한 예시일 뿐, 이에만 한 정되는 것은 아니다. 기능성 펜은 터치부, 제1 버튼, 제2 버튼, 제1 화면표시부, 제2 화면표시부, 제1 조 작부, 제2 조작부, 스피커부 및 펜 제어부(미도시)를 포함할 수 있다. 또한, 도면에 도시하지는 않 았으나, 기능성 펜은 제1 버튼의 상면에 마련되는 제1 압력센서(미도시) 및 제2 버튼의 상면에 마 련되는 제2 압력센서(미도시)를 포함할 수 있다. 터치부는 일예로 정전식 터치 형태로 마련될 수 있다. 제어부는 일예로 사용자가 사용자 단말의 화면의 일영역에 마련된 언어학습 버튼을 클릭한 경우, 사용 자 단말을 언어학습 모드로 제어함으로써 사용자에게 언어학습 서비스를 제공 가능할 수 있다. 제어부 는 사용자 단말이 언어학습 모드로 제어되면, 이후 다음과 같이 제어할 수 있다. 구체적으로, 사용자 단말이 상기 언어학습 모드로 제어되면, 이후 제어부는 일예로 사용자로부터 학습 용 문서를 입력받아 사용자 단말의 화면에 표시할 수 있다. 학습용 문서는 사용자가 언어학습을 수행하고자 하는 대상이 되는 문서를 의미하는 것으로서, 예시적으로 교과서나 가이드북 등의 실물책을 사용자 단말로 촬영한 이미지(사진), e북(전자책) 등일 수 있다. 제어부는, 사용자가 사용자 단말에 기 저장된 이미 지를 불러오기하거나, 신규하게 촬영한 이미지를 불러오기하거나, 또는 사용자 단말에 기 등록된 복수의 e 북 콘텐츠 중 어느 한 콘텐츠를 불러오기를 함으로써 획득되는 상기 학습용 문서를 입력받을 수 있다. 일예로 도 5에는 사용자로부터 입력받은 학습용 문서가 실물 책을 촬영한 이미지(사진)인 경우의 예가 도시되어 있다. 즉, 도 5에는 일예로 사용자 단말의 화면 상에 학습용 문서로서 실물책을 촬영한 이미지가 표시되어 있는 상태의 예가 도시되어 있다. 제어부는 학습용 문서가 사용자 단말의 화면에 표시된 이후, 기능성 펜을 통한 입력 신호에 따라 다음과 같이 제어할 수 있다. 제어부는 기능성 펜의 전원이 on 상태로 감지될 때 기능성 펜의 작동 을 제어할 수 있다. <제1 실시예> 학습용 문서가 사용자 단말의 화면에 표시된 이후, 일예로 본 발명의 제1 실시예에 따르면, 제어부는, 사용자가 기능성 펜의 제1 압력센서(미도시)에 손가락을 접촉시킨 상태에서 사용자 단말의 화면에 표시 된 학습용 문서 상에서의 어느 한 지점인 제1 지점(p1)을 터치부로 터치한 것으로 감지되는 경우, 상기 학 습용 문서 내에서 상기 제1 지점(p1)에 대응하는 단어를 식별하고, 이후 음성합성 처리부가 상기 식별된 단 어를 음성합성 모델에 적용시켜 음성합성을 수행하도록 제어할 수 있으며, 이후 음성합성 처리부에 의해 제 공되는 음성인 상기 식별된 단어에 대응하는 음성(합성음)이 스피커부를 통해 출력되도록 기능성 펜의 작동을 제어할 수 있다. 도 5에 도시된 일예에 따르면, 일예로 '제1 버튼과 제2 버튼이 모두 미눌림 상태이고, 제1 압력센서에 만 사용자의 손가락이 접촉 상태' 일 때 제1 지점(p1)에 대하여 터치부에 의한 터치가 이루어진 경우, 제어 부는 제1 지점(p1)에 대응하는 단어로 'normal'를 식별할 수 있고, 이후 식별된 'normal'이라는 단어에 대 응하는 음성(즉, 'normal'이라는 단어를 읽을 때의 음성으로서, 일예로 '놀멀'이라는 음성)이 스피커부를 통해 출력되도록 제어할 수 있다. <제2 실시예> 학습용 문서가 사용자 단말의 화면에 표시된 이후, 일예로 본 발명의 제2 실시예에 따르면, 제어부는, 사용자가 기능성 펜의 제1 버튼을 누름으로써 제1 버튼을 눌림 상태가 되도록 한 상태에서 상기 제 1 지점(p1)을 터치부로 터치한 것으로 감지되는 경우, 상기 학습용 문서 내에서 상기 제1 지점(p1)에 대응 하는 단어를 포함하는 한 문장(하나의 문장)을 식별하고, 이후 음성합성 처리부가 상기 식별된 한 문장을 음성합성 모델에 적용시켜 음성합성을 수행하도록 제어할 수 있으며, 이후 음성합성 처리부에 의해 제공되 는 음성인 상기 식별된 한 문장에 대응하는 음성(합성음)이 스피커부를 통해 출력되도록 기능성 펜의 작동을 제어할 수 있다. 이때, 제2 실시예일 때, 제1 압력센서(미도시)에 대한 사용자의 손가락은 접촉 상태일 수도 있고, 미접촉 상태 일 수도 있다. 도 5에 도시된 일예에 따르면, 일예로 제1 버튼만 눌림 상태일 때 제1 지점(p1)에 대하여 터치부에 의 한 터치가 이루어진 경우, 제어부는 제1 지점(p1)에 대응하는 단어를 포함하는 한 문장으로서, 일예로 'How might the wild robot react to normal robots and to humans?'라는 문장을 식별할 수 있고, 이후 식별된 문장 에 대응하는 음성이 스피커부를 통해 출력되도록 제어할 수 있다. <제3 실시예> 학습용 문서가 사용자 단말의 화면에 표시된 이후, 일예로 본 발명의 제3 실시예에 따르면, 제어부는, 사용자가 기능성 펜의 제1 버튼을 누름으로써 제1 버튼을 눌림 상태가 되도록 한 상태에서, 사용자 단말의 화면에 표시된 학습용 문서 상에서 일부 영역(s)에 대하여 터치부로 드래그 입력을 통해 영역 지정을 하고(즉, 일부 영역(s)을 드래그 입력을 통해 영역 지정을 하고), 이후 영역 지정이 된 상기 일부 영역 (s) 내의 어느 한 지점인 제2 지점(p2)을 터치부로 터치한 것으로 감지되는 경우, 상기 학습용 문서 내에서 상기 일부 영역(s)에 포함(기재)되어 있는 일부 영역 내 텍스트들을 식별하고, 이후 음성합성 처리부가 상 기 식별된 일부 영역 내 텍스트들을 음성합성 모델에 적용시켜 음성합성을 수행하도록 제어할 수 있으며, 이후 음성합성 처리부에 의해 제공되는 음성인 상기 식별된 일부 영역 내 텍스트들에 대응하는 음성(합성음)이 스피커부를 통해 출력되도록 기능성 펜의 작동을 제어할 수 있다. 이때, 일부 영역(s)은 사용자가 원하는 크기의 범위로 설정 가능하므로, 학습용 문서 상에서의 일부 영역(s)의 크기는 사용자가 원하는 대로 사용자에 의해 다양한 크기로 설정될 수 있다. 따라서, 일부 영역 내에 포함(기재)되어 있는 일부 영역 내 텍스트들에는, 적어도 하나의 단어가 포함되어 있을 수 있고, 또는 적어도 하나의 문장이 포함되어 있을 수 있다. 도 5에는 일예로 일부 영역(s) 내에 5개의 문장이 포함되어 있는 경우의 예가 도시되어 있다. 도 5에 도시된 일예에 따르면, 일예로 제1 버튼만 눌림 상태에서 도 5에 도시된 것과 같이 일부 영역(s)에 대하여 드래그 입력으로 영역 지정이 이루어진 경우, 제어부는 영역 지정된 일부 영역(s) 내에 포함되어 있 는 5개의 문장을 식별하고, 이후 식별된 5개의 문장에 대응하는 음성이 스피커부를 통해 출력되도록 제어할 수 있다. 상술한 제1 실시예 내지 제3 실시예에 따르면, 제1 버튼은 터치부를 통하여 사용자에게 학습용 문서 내 에서의 특정 단어 내지 적어도 일부의 문장에 대한 음성을 제공해 주기 위해 마련되는 버튼일 수 있다.반면, 후술하여 설명하는 제4 실시예 내지 제6 실시예에 따르면, 제2 버튼은 터치부를 통하여 사용자에 게 학습용 문서 내에서의 특정 단어 내지 적어도 일부의 문장에 대한 번역 음성을 제공해 주기 위해 마련되는 버튼일 수 있다. 즉, 일예로 제1 버튼은 음성 제공 기능을 가지며, 제2 버튼은 번역 음성 제공 기능을 가질 수 있다. 제4 실시예 내지 제6 실시예를 설명하기에 앞서, 제어부는 사용자가 제2 버튼에 의해 제공되는 기능(번 역 음성 제공 기능)을 이용하고자 할 경우, 인식 언어 종류 및 번역 언어 종류를 기능성 펜에 미리 설정해 두도록 유도할 수 있다. 일예로 언어의 종류에는 한국어, 영어, 독일어, 베트남어, 태국어 등 다양한 나라의 언어의 종류가 포함될 수 있다. 여기서, 인식 언어 종류는 번역 대상이 되는 언어의 종류를 의미하고, 번역 언어 종류는 번역된 영어의 종류를 의미할 수 있다. 예시적으로, 사용자는 영어를 한국어로 번역해주는 서비스를 이용하고자 할 경우, 기능성 펜 에 대하여 인식 언어 종류를 영어로 미리 설정해두고, 번역 언어 종류를 한국어로 미리 설정해둘 수 있다. 기능성 펜에서 제1 화면표시부는 인식 언어 종류의 설정을 위해 마련되는 구성이고, 제2 화면표시부 는 번역 언어 종류의 설정을 위해 마련되는 구성일 수 있다. 제1 조작부와 제2 조작부는 언어의 종 류를 변경 가능하도록 하기 위해 마련되는 구성일 수 있다. 일예로, 인식 언어 종류는 다음과 같은 과정을 통해 설정될 수 있다. 제어부는 사용자가 제1 화면표시부 를 터치한 경우, 사용자가 인식 언어 종류의 설정을 희망하는 것으로 인식하여, 이후 일예로 기본으로 설정 되어 있는 언어인 기본 언어로서 '한국어'라는 텍스트가 제1 화면표시부 상에 표시되도록 제어할 수 있고, 이후 사용자가 제2 조작부를 클릭할 때마다 기 등록된 복수의 언어 종류의 목록 내에서 기본 언어 다음에 위치하는 언어 종류가 제1 화면표시부 상에 표시되도록 제어할 수 있다. 일예로 목록 내에 복수의 언어 종 류가 '한국어-영어-베트남어-태국어-독일어 등'의 순서로 미리 등록(설정)되어 있다고 하자. 이러한 경우, 제어 부는 기본 언어로 한국어를 제1 화면표시부 상에 표시한 이후, 사용자가 제2 조작부를 클릭하면 그 에 응답하여 목록 내에서 한국어 다음에 위치해 있는 언어 종류인 '영어’'가 제1 화면표시부 상에 표시되 도록 하고, 이후 또 다시 제2 조작부를 클릭하면 목록 내에서 '영어' 다음에 위치해 있는 언어 종류인 '베 트남어'가 제1 화면표시부 상에 표시되도록 할 수 있으며, 이후 일에로 제1 조작부를 클릭하면 목록 내 에서 '베트남어' 이전에 위치해 있는 언어 종류인 '영어'가 제1 화면표시부 상에 표시되도록 할 수 있다. 이에 따르면, 제1 조작부는 목록 내 복수의 언어 종류 중 제1 화면표시부 상에 현재 표시되어 있는 언 어 종류의 이전에 위치해 있는 언어 종류가 제1 화면표시부 상에 표시되도록 하기 위해 마련되는 버튼일 수 있다. 반대로 제2 조작부는 목록 내 복수의 언어 종류 중 제1 화면표시부 상에 현재 표시되어 있는 언 어 종류의 이후에 위치해 있는 언어 종류가 제1 화면표시부 상에 표시되도록 하기 위해 마련되는 버튼일 수 있다. 이처럼, 제1 조작부 및 제2 조작부를 통해 사용자는 인식 언어 종류를 설정할 수 있다. 번역 언어 종류는 상술한 인식 언어 종류와 유사한 과정을 통해 설정될 수 있다. 간단히 살펴보면, 제어부 는 사용자가 제2 화면표시부를 터치한 경우, 사용자가 번역 언어 종류의 설정을 희망하는 것으로 인식하여, 이후 일예로 기본 언어로서 '한국어'라는 텍스트가 제2 화면표시부 상에 표시되도록 제어할 수 있고, 이후 사용자가 제1 조작부 또는 제2 조작부에 대한 입력을 통해 번역 언어 종류로의 설정을 희망하는 언어 종류로 설정 가능하도록 할 수 있다. 이하에서는 일예로 사용자가 영어를 한국어로 번역해주는 서비스를 이용하고자 하여, 기능성 펜에 대하여 인식 언어 종류를 영어로 미리 설정해두고, 번역 언어 종류를 한국어로 미리 설정해둔 경우를 예로 들어 설명하 기로 한다. <제4 실시예> 학습용 문서가 사용자 단말의 화면에 표시된 이후, 일예로 본 발명의 제4 실시예에 따르면, 제어부는, 사용자가 기능성 펜의 제2 압력센서(미도시)에 손가락을 접촉시킨 상태에서 사용자 단말의 화면에 표시 된 학습용 문서 상에서의 어느 한 지점인 제1 지점(p1)을 터치부로 터치한 것으로 감지되는 경우, 상기 학 습용 문서 내에서 상기 제1 지점(p1)에 대응하는 단어를 식별하고, 이후 상기 식별된 단어가 사용자에 의해 미 리 설정된 인식 언어 종류인 영어가 맞는지 여부를 판단하여 맞으면, 식별된 단어에 대하여 사용자가 미리 설정 해둔 번역 언어 종류인 한국어로 기계번역을 수행하고, 이후 기계번역의 수행을 통해 도출된 기계번역된 단어를음성합성 모델에 적용시켜 음성합성을 수행하도록 제어할 수 있으며, 이후 음성합성 처리부에 의해 제공되 는 음성인 상기 기계번역된 단어에 대응하는 음성(합성음)이 스피커부를 통해 출력되도록 기능성 펜의 작동을 제어할 수 있다. 도 5에 도시된 일예에 따르면, 일예로 '제1 버튼과 제2 버튼이 모두 미눌림 상태이고, 제2 압력센서에 만 사용자의 손가락이 접촉 상태' 일 때 제1 지점(p1)에 대하여 터치부에 의한 터치가 이루어진 경우, 제어 부는 제1 지점(p1)에 대응하는 단어로 'normal'를 식별할 수 있고, 이후 식별된 단어에 대하여 기계번역을 수행함으로써 기계번역된 단어로서 일예로 '보통의, 평범한, 정상적인'를 도출하고, 이후 기계번역된 단어에 대 응하는 음성(즉, '보통의, 평범한, 정상적인'에 해당하는 한국어 음성)이 스피커부를 통해 출력되도록 제어 할 수 있다. <제5 실시예> 학습용 문서가 사용자 단말의 화면에 표시된 이후, 일예로 본 발명의 제5 실시예에 따르면, 제어부는, 사용자가 기능성 펜의 제2 버튼을 누름으로써 제2 버튼을 눌림 상태가 되도록 한 상태에서 상기 제 1 지점(p1)을 터치부로 터치한 것으로 감지되는 경우, 상기 학습용 문서 내에서 상기 제1 지점(p1)에 대응 하는 단어를 포함하는 한 문장(하나의 문장)을 식별하고, 이후 상기 식별된 한 문장이 사용자에 의해 미리 설정 된 인식 언어 종류인 영어가 맞는지 여부를 판단하여 맞으면, 상기 식별된 한 문장에 대하여 사용자가 미리 설 정해둔 번역 언어 종류인 한국어로 기계번역을 수행하고, 이후 기계번역의 수행을 통해 도출된 기계번역된 한 문장을 음성합성 모델에 적용시켜 음성합성을 수행하도록 제어할 수 있으며, 이후 음성합성 처리부에 의해 제공되는 음성인 상기 기계번역된 한 문장에 대응하는 음성(합성음)이 스피커부를 통해 출력되도록 기능성 펜의 작동을 제어할 수 있다. 도 5에 도시된 일예에 따르면, 일예로 제2 버튼만 눌림 상태일 때 제1 지점(p1)에 대하여 터치부에 의 한 터치가 이루어진 경우, 제어부는 제1 지점(p1)에 대응하는 단어를 포함하는 한 문장으로서, 일예로 'How might the wild robot react to normal robots and to humans?'라는 문장을 식별할 수 있고, 이후 식별된 상기 한 문장을 한국어로 기계번역함으로써 '야생 로봇은 일반 로봇과 인간에게 어떻게 반응할까요?'와 같은 기계번 역된 한 문장을 도출할 수 있고, 이후 기계번역된 한 문장에 대응하는 음성(한국어 음성)이 스피커부를 통 해 출력되도록 제어할 수 있다. <제6 실시예> 학습용 문서가 사용자 단말의 화면에 표시된 이후, 일예로 본 발명의 제6 실시예에 따르면, 제어부는, 사용자가 기능성 펜의 제2 버튼을 누름으로써 제2 버튼을 눌림 상태가 되도록 한 상태에서, 사용자 단말의 화면에 표시된 학습용 문서 상에서 일부 영역(s)에 대하여 터치부로 드래그 입력을 통해 영역 지정을 하고(즉, 일부 영역(s)을 드래그 입력을 통해 영역 지정을 하고), 이후 영역 지정이 된 상기 일부 영역 (s) 내의 어느 한 지점인 제2 지점(p2)을 터치부로 터치한 것으로 감지되는 경우, 상기 학습용 문서 내에서 상기 일부 영역(s)에 포함(기재)되어 있는 일부 영역 내 텍스트들을 식별하고, 이후 상기 식별된 일부 영역 내 텍스트들이 사용자에 의해 미리 설정된 인식 언어 종류인 영어가 맞는지 여부를 판단하여 맞으면, 상기 식별된 일부 영역 내 텍스트들에 대하여 사용자가 미리 설정해둔 번역 언어 종류인 한국어로 기계번역을 수행하고, 이 후 기계번역의 수행을 통해 도출된 기계번역된 일부 영역 내 텍스트들을 음성합성 모델에 적용시켜 음성합성을 수행하도록 제어할 수 있으며, 이후 음성합성 처리부에 의해 제공되는 음성인 상기 기계번역된 일부 영역 내 텍스트들에 대응하는 음성(합성음)이 스피커부를 통해 출력되도록 기능성 펜의 작동을 제어할 수 있 다. 도 5에 도시된 일예에 따르면, 일예로 제2 버튼만 눌림 상태에서 도 5에 도시된 것과 같이 일부 영역(s)에 대하여 드래그 입력으로 영역 지정이 이루어진 경우, 제어부는 영역 지정된 일부 영역(s) 내에 포함되어 있 는 5개의 문장(영어 문장)을 식별하고, 이후 식별된 5개의 문장들을 한국어로 기계번역한 후 기계번역된 한국어 문장들에 대응하는 음성이 스피커부를 통해 출력되도록 제어할 수 있다. 상술한 바에 따르면, 본 장치는 기능성 펜과의 연계를 통해 사용자에게 언어학습 서비스를 제공할 수 있다. 또한, 본 장치는 기능성 펜을 이용한 언어학습 서비스의 제공을 통해, 사용자가 학습용 문서 내 에 포함되어 있는 특정 단어 내지 적어도 하나의 문장과 관련하여, 이를 그대로 음성으로 읽어주거나 혹은 번역 된 음성으로 읽어주는 서비스를 사용자에게 제공할 수 있고, 사용자가 다양한 종류의 언어에 대하여 효율적이고 빠르게 학습 가능하도록 할 수 있다. 이하에서는 상기에 자세히 설명된 내용을 기반으로, 본 발명의 동작 흐름을 간단히 살펴보기로 한다. 도 6은 본 발명의 일 실시예에 따른 음성합성 방법에 대한 동작 흐름도이다. 도 6에 도시된 음성합성 방법은 앞서 설명된 본 장치에 의하여 수행될 수 있다. 따라서, 이하 생략된 내용 이라고 하더라도 본 장치에 대하여 설명된 내용은 음성합성 방법에 대한 설명에도 동일하게 적용될 수 있다. 도 6을 참조하면, 단계S11에서 수집부는 음성데이터를 수집할 수 있다. 다음으로, 단계S12에서 전처리부는 상기 수집된 음성데이터를 전처리할 수 있다. 다음으로, 단계S13에서 모델 관리부는 전처리된 음성데이터를 이용하여 음성합성 모델을 생성하고 훈련시킬 수 있다. 상술한 설명에서, 단계 S11 내지 S13은 본 발명의 구현예에 따라서, 추가적인 단계들로 더 분할되거나, 더 적은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있 다. 본 발명의 일 실시 예에 따른 음성합성 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형 태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 발 명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수 도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체 (magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장 하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작 동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 또한, 전술한 음성합성 방법은 기록 매체에 저장되는 컴퓨터에 의해 실행되는 컴퓨터 프로그램 또는 애플리케이 션의 형태로도 구현될 수 있다."}
{"patent_id": "10-2023-0102961", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본 발명의 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자는 본 발명 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 발명의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으 로 해석되어야 한다.도면 도면1 도면2 도면3 도면4 도면5 도면6"}
{"patent_id": "10-2023-0102961", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 음성합성 장치의 개략적인 구성을 나타낸 도면이다. 도 2는 본 발명의 일 실시예에 따른 음성합성 장치 내 수집부에 의한 음성데이터의 수집 과정을 설명하기 위한 도면이다. 도 3은 음성합성 모델의 기존 훈련 과정을 설명하기 위한 도면이다. 도 4는 본 발명의 일 실시예에 따른 음성합성 시스템의 개략적인 구성을 나타낸 도면이다. 도 5는 본 발명의 일 실시예에 따른 음성합성 장치가 입력받는 학습용 문서의 예를 나타낸 도면이다. 도 6은 본 발명의 일 실시예에 따른 음성합성 방법에 대한 동작 흐름도이다."}
