{"patent_id": "10-2025-7001758", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0033228", "출원번호": "10-2025-7001758", "발명의 명칭": "사이니지 제공을 위한 에지 및 클라우드 간 협업 플랫폼", "출원인": "엘지전자 주식회사", "발명자": "이진상"}}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "차량으로부터 운행 컨텍스트 데이터를 수신하는 단계;외부 데이터 소스로부터 사이니지 관련 정보를 수집하는 단계;상기 운행 컨텍스트 데이터에 기반하여 상기 수집된 사이니지 관련 정보 중에서 상기 차량에 해당하는 제 1 사이니지 관련 정보를 추출하는 단계;상기 운행 컨텍스트 데이터에 기반하여 상기 차량에서 디스플레이될 미디어 컨텐츠에 대한 디스플레이 정책을결정하는 단계;상기 운행 컨텍스트 데이터 및 제 1 사이니지 관련 정보에 기반하여 제 1 사이니지 관련 정보의 프로세싱 위치를 결정하는 단계; 및상기 결정된 프로세싱의 위치에 기반하여 제 1 사이니지 관련 정보를 전송하는 단계;를 포함하는, 차량용 디스플레이 장치를 위한 데이터 제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 디스플레이 정책은, 상기 운행 컨텍스트 데이터 내의 상기 차량의 사용자의 프로파일에 기반하여 제 1 사이니지 관련 정보 중에서 상기 사용자가 선호할 것으로 추정되는 제 2 사이니지 관련 정보를 필터링하기 위한조건을 포함하는 것을 특징으로 하는 데이터 제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 프로세싱 위치가 클라우드 서버로 정해지는 경우, 상기 디스플레이 정책에 기반하여 상기 클라우드 서버에서 제 1 사이니지 관련 정보 중에서 제 2 사이니지 관련 정보가 필터링되는 단계; 및상기 클라우드 서버에서 제 2 사이니지 관련 정보가 상기 미디어 컨텐츠로 변환되는 단계;를 포함하는 데이터제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,제 2 사이니지 관련 정보를 상기 차량으로 제공하거나, 상기 미디어 컨텐츠를 상기 차량으로 제공하는 단계;를포함하는 것을 특징으로 하는 데이터 제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서,상기 프로세싱 위치가 상기 클라우드 서버만으로 정해지는 경우, 상기 미디어 컨텐츠가 상기 차량으로 제공될수 있고,상기 프로세싱 위치가 상기 클라우드 서버 및 상기 차량으로 정해지는 경우 제 2 사이니지 관련 정보가 상기 차량으로 제공될 수 있는 것을 특징으로 하는 데이터 제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1 항에 있어서,상기 프로세싱 위치가 상기 차량으로 정해지는 경우, 제 1 사이니지 관련 정보를 상기 차량으로 제공하는 단공개특허 10-2025-0033228-3-계;를 포함하는 것을 특징으로 하는 데이터 제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항에 있어서,상기 프로세싱 위치는 제 1 사이니지 관련 정보의 변동 가능성에 기반하여 결정되는 것을 특징으로 하는 데이터제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서,상기 프로세싱 위치는 제 1 사이니지 관련 정보의 데이터 크기에 기반하여 결정되는 것을 특징으로 하는 데이터제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항에 있어서,상기 프로세싱 위치는 상기 운행 컨텍스트 데이터에 따른 사이니지 표시 가능 영역의 넓이에 기반하여 결정되는것을 특징으로 하는 데이터 제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1 항에 있어서,상기 프로세싱 위치는 상기 차량과의 통신 상황에 기반하여 결정되는 것을 특징으로 하는 데이터 제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 1 항에 있어서,상기 프로세싱 위치는 상기 운행 컨텍스트 데이터에 따른 상기 차량의 주행 안정성에 기반하여 결정되는 것을특징으로 하는 데이터 제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 4 항에 있어서,상기 차량에서 디스플레이되는 상기 미디어 컨텐츠를 상기 차량으로부터 피드백 받는 단계;를 포함하고,상기 피드백 받은 미디어 컨텐츠와 동일한 미디어 컨텐츠는 상기 차량으로 제공되지 않는 것을 특징으로 하는데이터 제공 방법."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "클라우드 서버와 통신 가능한 차량용 디스플레이 장치로서,차량에서 수집되는 에지 데이터로부터 운행 컨텍스트 데이터를 생성하기 위한 운행 컨텍스트 데이터 매니저;상기 클라우드 서버로부터 디스플레이 정책 및 제 1 사이니지 관련 데이터를 수신하는 경우, 상기 디스플레이정책에 기반하여 제 1 사이니지 관련 정보 중에서 제 2 사이니지 관련 정보를 필터링하기 위한 사이니지 데이터필터링 모듈을 포함하는 에지 사이니지 데이터 프로세서;상기 클라우드 서버로부터 미디어 컨텐츠를 수신하는 경우, 상기 미디어 컨텐츠를 상기 미디어 컨텐츠의 해당위치에 매칭시키기 위한 랜더링 모듈; 및상기 미디어 컨텐츠가 상기 해당 위치에 매칭되도록 주행 영상과 함께 디스플레이 하기 위한 디스플레이부를 포함하는 차량용 디스플레이 장치."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13 항에 있어서,공개특허 10-2025-0033228-4-상기 클라우드 서버로부터 수신할 수 있는 디스플레이 정책과는 별도의 디스플레이 정책을 상기 운행 컨텍스트데이터에 기반하여 결정하기 위한 에지 정책 매니저를 더욱 포함하는 것을 특징으로 하는 차량용 디스플레이 장치."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서, 상기 에지 정책 매니저는,상기 운행 컨텍스트 데이터 내의 상기 차량의 사용자의 프로파일에 기반하여 제 1 사이니지 관련 정보 중에서상기 사용자가 선호할 것으로 추정되는 제 2 사이니지 관련 정보를 필터링하기 위한 조건을 상기 디스플레이 정책으로서 결정하는 것을 특징으로 하는 차량용 디스플레이 장치."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 13 항에 있어서, 상기 에지 사이니지 데이터 프로세서는,제 2 사이니지 관련 정보를 상기 미디어 컨텐츠로 변환하기 위한 미디어 컨텐츠 프로세싱 모듈을 더욱 포함하는것을 특징으로 하는 차량용 디스플레이 장치."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16 항에 있어서, 상기 에지 사이니지 데이터 프로세서는,상기 변환된 미디어 컨텐츠가 상기 주행 영상과 함께 디스플레이될 수 있도록 상기 랜더링 모듈로 제공하는 것을 특징으로 하는 차량용 디스플레이 장치."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 17 항에 있어서, 상기 차량용 디스플레이 장치는,상기 디스플레이된 미디어 컨텐츠를 저장하기 위한 메모리를 더욱 포함하고, 상기 에지 정책 매니저는,상기 저장된 미디어 컨텐츠를 상기 클라우드 서버로 피드백하는 것을 특징으로 하는 차량용 디스플레이 장치."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 18 항에 있어서, 상기 에지 사이니지 데이터 프로세서는,상기 차량이 동일 경로를 반복적으로 운행하는 경우, 상기 저장된 미디어 컨텐츠 중에서 상기 디스플레이 정책에 부합하는 미디어 컨텐츠를 상기 랜더링 모듈로 제공하는 것을 특징으로 하는 차량용 디스플레이 장치."}
{"patent_id": "10-2025-7001758", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 13 항에 있어서, 상기 운행 컨텍스트 데이터는,주행 경로 관련 데이터, 안전운행 관련 데이터, 상기 차량의 연산 자원량 정보, 사이니지 표시 가능 영역 정보,통신 환경 정보, 사용자 정보, 및 사용자 프로파일 중 적어도 하나를 포함하는 것을 특징으로 하는 차량용 디스플레이 장치."}
{"patent_id": "10-2025-7001758", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 에지 디바이스의 상황 및 에지 데이터의 특성 중 적어도 하나에 따라서 에지 및 클라우드가 적절히 협 력하여 효율적으로 사용자에게 사이니지를 제공할 수 있는 에지 및 클라우드 간 협업 시스템 및 방법에 관한 것 이다. 본 개시는, 차량으로부터 운행 컨텍스트 데이터를 수신하는 단계, 외부 데이터 소스로부터 사이니지 관련 (뒷면에 계속)"}
{"patent_id": "10-2025-7001758", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 사이니지 제공을 위한 에지 및 클라우드 간 협업 플랫폼에 관한 것으로, 좀더 자세하게는 에지 디바 이스에서 수집된 에지 데이터에 기반하여 에지 디바이스와 클라우드 서버 간에 협업으로 사용자에게 사이니지를 제공할 수 있는 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2025-7001758", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근, 다양한 IoT(Internet of Things) 장치와 같은 에지 디바이스의 보급화와 클라우드 컴퓨팅 기술의 발전으 로 인해, 에지 디바이스에서 수집된 에지 데이터가 클라우드 서버로 보내지고 클라우드 서버가 에지 데이터를분석하는 클라우드 서비스가 널리 활용되고 있다. 이러한 클라우드 서비스에서, 에지 디바이스가 모든 에지 데이터를 클라우드 서버에 보내고, 클라우드 서버가 모든 에지 데이터를 처리하는 것은 적어도 클라우드 통신 트래픽 및 레이턴시(latency) 측면에서 비효율적일 수 있다. 이러한 문제를 해결하기 위해, 데이터 분석을 위해 클라우드 서버에 에지 데이터를 보내는 대신에 에지 데이터 를 수집한 에지 디바이스 자체에서 또는 별도의 다른 에지 디바이스에서 에지 데이터를 분석하는 에지 컴퓨팅 기술이 활용될 수도 있다. 그러나, 이 경우 원활한 에지 데이터의 처리를 위해서는 고사양의 에지 디바이스가 활용되어야 하므로 비용 측 면에서 비효율적일 수 있다. 한편, 차량과 같은 에지 디바이스의 경우, 차량을 이용하는 사용자의 안전 및 편의를 위해, 차량에는 각종 센서 와 장치가 구비되고 있으며, 차량의 기능이 다양화 되고 있다. 이러한 차량의 기능은 운전자의 편의를 도모하기 위한 편의 기능, 그리고 운전자 및/또는 보행자의 안전을 도모하기 위한 안전 기능으로 나뉠 수 있다. 차량의 편의 기능은 차량에 인포테인먼트(information + entertainment) 기능을 부여하고, 부분적인 자율 주행 기능을 지원하거나, 야간 시야나 사각 대와 같은 운전자의 시야 확보를 돕는 등의 운전자 편의와 관련될 수 있 다. 예를 들어, 적응 순향 제어(active cruise control, ACC), 스마트주자시스템(smart0020parking assist system, SPAS), 나이트비전(night vision, NV), 헤드 업 디스플레이(head up display, HUD), 어라운드 뷰 모니터 (around view monitor, AVM), 적응형 상향등 제어(adaptive headlight system, AHS) 기능 등이 있다. 최근에는, 차량의 윈드쉴드, HUD(Head Up Display)를 통해 그래픽 객체를 출력하거나, 카메라로 촬영되는 영상 에 그래픽 객체를 출력하여 실제 세계에 그래픽 객체를 추가로 출력시키는 증강현실(Augmented Reality, AR)에 대한 기술개발이 활발히 이루어지고 있다. 특히, 이러한 증강현실(AR) 기술을 활용하여, 운전자에게 증강현실 (AR) 기술을 통해 경로를 안내하거나 경로 상에 존재하는 POI 와 관련된 다양한 부가 정보나 광고를 노출하는 기술들의 개발이 확대되고 있다. 이와 같은 증강현실(AR) 기술을 통한 다양한 안내 또는 광고의 경우, 적절한 위치 및/또는 시점에 제공되지 않 는다면 현실과는 다소 이질적으로 보일 뿐만 아니라 심지어 운전에 방해가 될 수도 있다."}
{"patent_id": "10-2025-7001758", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 이와 같은 문제점을 해결하기 위해 제안되는 것으로서, 에지 디바이스의 상황 및 에지 데이터의 특성 중 적어도 하나에 따라서 에지 및 클라우드가 적절히 협력하여 효율적으로 사용자에게 사이니지를 제공할 수 있 는 에지 및 클라우드 간 협업 시스템 및 방법을 제공하는 것을 목적으로 한다."}
{"patent_id": "10-2025-7001758", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위해 본 개시는, 차량으로부터 운행 컨텍스트 데이터를 수신하는 단계, 외부 데이터 소스 로부터 사이니지 관련 정보를 수집하는 단계, 상기 운행 컨텍스트 데이터에 기반하여 상기 수집된 사이니지 관 련 정보 중에서 상기 차량에 해당하는 제 1 사이니지 관련 정보를 추출하는 단계, 상기 운행 컨텍스트 데이터에 기반하여 상기 차량에서 디스플레이될 미디어 컨텐츠에 대한 디스플레이 정책을 결정하는 단계, 상기 운행 컨텍 스트 데이터 및 제 1 사이니지 관련 정보에 기반하여 제 1 사이니지 관련 정보의 프로세싱 위치를 결정하는 단 계, 및 상기 결정된 프로세싱의 위치에 기반하여 제 1 사이니지 관련 정보를 전송하는 단계를 포함하는, 차량용 디스플레이 장치를 위한 데이터 제공 방법을 제공할 수 있다. . 상기 디스플레이 정책은, 상기 운행 컨텍스트 데이터 내의 상기 차량의 사용자의 프로파일에 기반하여 제 1 사 이니지 관련 정보 중에서 상기 사용자가 선호할 것으로 추정되는 제 2 사이니지 관련 정보를 필터링하기 위한 조건을 포함할 수 있다. 상기 데이터 제공 방법은, 상기 프로세싱 위치가 클라우드 서버로 정해지는 경우, 상기 디스플레이 정책에 기반 하여 상기 클라우드 서버에서 제 1 사이니지 관련 정보 중에서 제 2 사이니지 관련 정보가 필터링되는 단계, 및상기 클라우드 서버에서 제 2 사이니지 관련 정보가 상기 미디어 컨텐츠로 변환되는 단계를 더욱 포함할 수 있 다. 상기 데이터 제공 방법은, 제 2 사이니지 관련 정보를 상기 차량으로 제공하거나, 상기 미디어 컨텐츠를 상기 차량으로 제공하는 단계를 더욱 포함할 수 있다. 상기 프로세싱 위치가 상기 클라우드 서버만으로 정해지는 경우, 상기 미디어 컨텐츠가 상기 차량으로 제공될 수 있고, 상기 프로세싱 위치가 상기 클라우드 서버 및 상기 차량으로 정해지는 경우 제 2 사이니지 관련 정보 가 상기 차량으로 제공될 수 있다. 상기 데이터 제공 방법은, 상기 프로세싱 위치가 상기 차량으로 정해지는 경우, 제 1 사이니지 관련 정보를 상 기 차량으로 제공하는 단계를 더욱 포함할 수 있다. 상기 프로세싱 위치는 제 1 사이니지 관련 정보의 변동 가능성에 기반하여 결정될 수 있다. 상기 프로세싱 위치는 제 1 사이니지 관련 정보의 데이터 크기에 기반하여 결정될 수 있다. 상기 프로세싱 위치는 상기 운행 컨텍스트 데이터에 따른 사이니지 표시 가능 영역의 넓이에 기반하여 결정될 수 있다. 상기 프로세싱 위치는 상기 차량과의 통신 상황에 기반하여 결정될 수 있다. 상기 프로세싱 위치는 상기 운행 컨텍스트 데이터에 따른 상기 차량의 주행 안정성에 기반하여 결정될 수 있다. 상기 데이터 제공 방법은 상기 차량에서 디스플레이되는 상기 미디어 컨텐츠를 상기 차량으로부터 피드백 받는 단계를 더욱 포함할 수 있고, 상기 피드백 받은 미디어 컨텐츠와 동일한 미디어 컨텐츠는 상기 차량으로 제공되 지 않을 수 있다. 또한, 상기 목적을 달성하기 위해 본 개시는, 클라우드 서버와 통신 가능한 차량으로서, 차량에서 수집되는 에 지 데이터로부터 운행 컨텍스트 데이터를 생성하기 위한 운행 컨텍스트 데이터 매니저, 상기 클라우드 서버로부 터 디스플레이 정책 및 제 1 사이니지 관련 데이터를 수신하는 경우, 상기 디스플레이 정책에 기반하여 제 1 사 이니지 관련 정보 중에서 제 2 사이니지 관련 정보를 필터링하기 위한 사이니지 데이터 필터링 모듈을 포함하는 에지 사이니지 데이터 프로세서, 상기 클라우드 서버로부터 미디어 컨텐츠를 수신하는 경우, 상기 미디어 컨텐 츠를 상기 미디어 컨텐츠의 해당 위치에 매칭시키기 위한 랜더링 모듈, 및 상기 미디어 컨텐츠가 상기 해당 위 치에 매칭되도록 주행 영상과 함께 디스플레이 하기 위한 디스플레이부를 포함하는 차량용 디스플레이 장치를 제공할 수 있다. 상기 차량용 디스플레이 장치는, 상기 클라우드 서버로부터 수신할 수 있는 디스플레이 정책과는 별도의 디스플 레이 정책을 상기 운행 컨텍스트 데이터에 기반하여 정하기 위한 에지 정책 매니저를 더욱 포함할 수 있다. 상기 에지 정책 매니저는, 상기 운행 컨텍스트 데이터 내의 상기 차량의 사용자의 프로파일에 기반하여 제 1 사 이니지 관련 정보 중에서 상기 사용자가 선호할 것으로 추정되는 제 2 사이니지 관련 정보를 필터링하기 위한 조건을 상기 디스플레이 정책으로서 결정할 수 있다. 상기 에지 사이니지 데이터 프로세서는, 제 2 사이니지 관련 정보를 상기 미디어 컨텐츠로 변환하기 위한 미디 어 컨텐츠 프로세싱 모듈을 더욱 포함할 수 있다. 상기 에지 사이니지 데이터 프로세서는, 상기 변환된 미디어 컨텐츠가 상기 주행 영상과 함께 디스플레이될 수 있도록 상기 랜더링 모듈로 제공할 수 있다. 상기 차량용 디스플레이 장치는, 상기 디스플레이된 미디어 컨텐츠를 저장하기 위한 메모리를 더욱 포함하고, 상기 에지 정책 매니저는, 상기 저장된 미디어 컨텐츠를 상기 클라우드 서버로 피드백할 수 있다. 상기 에지 사이니지 데이터 프로세서는, 상기 차량이 동일 경로를 반복적으로 운행하는 경우, 상기 저장된 미디 어 컨텐츠 중에서 상기 디스플레이 정책에 부합하는 미디어 컨텐츠를 상기 랜더링 모듈로 제공할 수 있다. 상기 운행 컨텍스트 데이터는, 주행 경로 관련 데이터, 안전운행 관련 데이터, 상기 차량의 연산 자원량 정보, 사이니지 표시 가능 영역 정보, 통신 환경 정보, 사용자 정보, 및 사용자 프로파일 중 적어도 하나를 포함할 수 있다."}
{"patent_id": "10-2025-7001758", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따른 사이니지 제공을 위한 에지 및 클라우드 간 협업 플랫폼 의 효과에 대해 설명하면 다음과 같다. 본 개시의 실시예들 중 적어도 하나에 의하면, 에지 디바이스의 상황 및에지 데이터의 특성 중 적어도 하나에 따라서 에지 및 클라우드가 적절히 협력하여 효율적으로 사용자에게 사이니지를 제공할 수 있다는 장점이 있다."}
{"patent_id": "10-2025-7001758", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 개시에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 개시 작성의 용이함만이 고려되어 부여되거나 혼용되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 개시에 개시된 실시 예를 설명 함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 개시에 개시된 실시 예의 요지를 흐릴 수 있다고 판 단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 개시에 개시된 실시 예를 쉽게 이해할 수 있 도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 개시에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사 상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 이들 구성요소들은 각각 별도의 개별 하드웨어 모듈로 구성되거나 둘 이상의 하드웨어 모듈로 구현될 수도 있고, 둘 이상의 구성요소들이 하나의 하드웨어 모듈로 구현될 수도 있으며, 경우에 따라서는 소프트웨어로도구현될 수 있음은 물론이다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"포함한다\" 또 는 \"가지다\" 등의 용어는 개시상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이 들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시에서, \"A 및 B 중 적어도 하나\"라는 표현은 \"A\"를 의미할 수도 있고, \"B\"를 의미할 수도 있고, \"A\"와 \"B\"를 모두 의미할 수도 있다. 이하, 본 개시에서 활용될 수 있는 인공 지능(AI: Artificial Intelligence)에 대해 설명하겠다. 인공 지능은 인공적인 지능 또는 이를 만들 수 있는 방법론을 연구하는 분야를 의미하며, 머신 러닝(기계 학습, Machine Learning)은 인공 지능 분야에서 다루는 다양한 문제를 정의하고 그것을 해결하는 방법론을 연구하는 분야를 의미한다. 머신 러닝은 어떠한 작업에 대하여 꾸준한 경험을 통해 그 작업에 대한 성능을 높이는 알고리 즘으로 정의하기도 한다. 인공 신경망(ANN: Artificial Neural Network)은 머신 러닝에서 사용되는 모델로서, 시냅스의 결합으로 네트워 크를 형성한 인공 뉴런(노드)들로 구성되는, 문제 해결 능력을 가지는 모델 전반을 의미할 수 있다. 인공 신경 망은 다른 레이어의 뉴런들 사이의 연결 패턴, 모델 파라미터를 갱신하는 학습 과정, 출력값을 생성하는 활성화 함수(Activation Function)에 의해 정의될 수 있다. 인공 신경망은 입력층(Input Layer), 출력층(Output Layer), 그리고 선택적으로 하나 이상의 은닉층(Hidden Layer)를 포함할 수 있다. 각 층은 하나 이상의 뉴런을 포함하고, 인공 신경망은 뉴런과 뉴런을 연결하는 시냅 스를 포함할 수 있다. 인공 신경망에서 각 뉴런은 시냅스를 통해 입력되는 입력 신호들, 가중치, 편향에 대한 활성 함수의 함숫값을 출력할 수 있다. 모델 파라미터는 학습을 통해 결정되는 파라미터를 의미하며, 시냅스 연결의 가중치와 뉴런의 편향 등이 포함된 다. 그리고, 하이퍼파라미터는 머신 러닝 알고리즘에서 학습 전에 설정되어야 하는 파라미터를 의미하며, 학습 률(Learning Rate), 반복 횟수, 미니 배치 크기, 초기화 함수 등이 포함된다. 인공 신경망의 학습의 목적은 손실 함수를 최소화하는 모델 파라미터를 결정하는 것으로 볼 수 있다. 손실 함수 는 인공 신경망의 학습 과정에서 최적의 모델 파라미터를 결정하기 위한 지표로 이용될 수 있다. 머신 러닝은 학습 방식에 따라 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning)으로 분류할 수 있다. 지도 학습은 학습 데이터에 대한 레이블(label)이 주어진 상태에서 인공 신경망을 학습시키는 방법을 의미하며, 레이블이란 학습 데이터가 인공 신경망에 입력되는 경우 인공 신경망이 추론해 내야 하는 정답(또는 결과 값)을 의미할 수 있다. 비지도 학습은 학습 데이터에 대한 레이블이 주어지지 않는 상태에서 인공 신경망을 학습시키 는 방법을 의미할 수 있다. 강화 학습은 어떤 환경 안에서 정의된 에이전트가 각 상태에서 누적 보상을 최대화 하는 행동 혹은 행동 순서를 선택하도록 학습시키는 학습 방법을 의미할 수 있다. 인공 신경망 중에서 복수의 은닉층을 포함하는 심층 신경망(DNN: Deep Neural Network)으로 구현되는 머신 러닝 을 딥 러닝(심층 학습, Deep Learning)이라 부르기도 하며, 딥 러닝은 머신 러닝의 일부이다. 이하에서, 머신 러닝은 딥 러닝을 포함하는 의미로 사용된다. 기계 학습을 이용한 객체 감지 모델은 단일 단계 방식의 YOLO(you Only Look Once) 모델, 이단계 방식의 Faster R-CNN(Regions with Convolution Neural Networks) 모델 등이 있다.YOLO(you Only Look Once) 모델은 이미지 내에 존재하는 객체와 해당 객체의 위치가 이미지를 한번만 보고 예측 할 수 있는 모델이다. YOLO(you Only Look Once) 모델은 원본 이미지를 동일한 크기의 그리드(grid)로 나눈다. 그리고, 각 그리드에 대해 그리드 중앙을 중심으로 미리 정의된 형태로 지정된 경계 박스의 개수를 예측하고 이를 기반으로 신뢰도가 계산된다. 그 후, 이미지에 객체가 포함되어 있는지, 또는 배경만 단독으로 있는지에 대한 여부가 포함되며, 높은 객체 신 뢰도를 가진 위치가 선택되어 객체 카테고리가 파악될 수 있다. Faster R-CNN(Regions with Convolution Neural Networks) 모델은 RCNN 모델 및 Fast RCNN 모델보다 더 빨리 객체를 감지할 수 있는 모델이다. Faster R-CNN(Regions with Convolution Neural Networks) 모델에 대해 구체적으로 설명한다. 먼저, CNN(Convolution Neural Network) 모델을 통해 이미지로부터 특징 맵이 추출된다. 추출된 특징 맵에 기초 하여, 복수의 관심 영역(Region of Interest, RoI)들이 추출된다. 각 관심 영역에 대해 RoI 풀링이 수행된다. RoI 풀링은 관심 영역이 투사된 피쳐 맵을 미리 정해 놓은 H x W 크기에 맞게 끔 그리드를 설정하고, 각 그리드 에 포함된 칸 별로, 가장 큰 값을 추출하여, H x W 크기를 갖는 피쳐 맵을 추출하는 과정이다. H x W 크기를 갖는 피쳐 맵로부터 특징 벡터가 추출되고, 특징 벡터로부터 객체의 식별 정보가 얻어질 수 있다. 이하, 도 1을 참조하여, 본 개시에서 에지 디바이스로 사용될 수 있는 AI 장치에 대해 설명하겠다. 도 1은 본 개시의 일 실시 예에 따른 AI 장치를 나타낸다. AI 장치는 TV, 프로젝터, 휴대폰, 스마트폰, 데스크탑 컴퓨터, 노트북, 디지털방송용 단말기, PDA(personal digital assistants), PMP(portable multimedia player), 네비게이션, 태블릿 PC, 웨어러블 장치, 셋톱박스(STB), DMB 수신기, 라디오, 세탁기, 냉장고, 데스크탑 컴퓨터, 디지털 사이니지, 로봇, 차량 등 과 같은, 고정형 기기 또는 이동 가능한 기기 등으로 구현될 수 있다. 도 1을 참조하면, AI 장치는 통신부, 입력부, 러닝 프로세서, 센싱부, 출력부, 메모리 및 프로세서 등을 포함할 수 있다. 통신부는 유무선 통신 기술을 이용하여 다른 AI 장치(100a 내지 100e)나 AI 서버 등의 외부 장치들과 데이터를 송수신할 수 있다. 예컨대, 통신부는 외부 장치들과 센서 정보, 사용자 입력, 학습 모델, 제어 신호 등을 송수신할 수 있다. 이때, 통신부가 이용하는 통신 기술에는 GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), LTE(Long Term Evolution), 5G, WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), 블 루투스(Bluetooth쪠), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), ZigBee, NFC(Near Field Communication) 등이 있다. 입력부는 다양한 종류의 데이터를 획득할 수 있다. 이때, 입력부는 영상 신호 입력을 위한 카메라, 오디오 신호를 수신하기 위한 마이크로폰, 사용자로부터 정보를 입력 받기 위한 사용자 입력부 등을 포함할 수 있다. 여기서, 카메라나 마이크로폰을 센서로 취급하여, 카메라나 마이크로폰으로부터 획득한 신호를 센싱 데이터 또는 센서 정보라고 할 수도 있다. 입력부는 모델 학습을 위한 학습 데이터 및 학습 모델을 이용하여 출력을 획득할 때 사용될 입력 데이터 등을 획득할 수 있다. 입력부는 가공되지 않은 입력 데이터를 획득할 수도 있으며, 이 경우 프로세서 또는 러닝 프로세서는 입력 데이터에 대하여 전처리로써 입력 특징점(input feature)을 추출할 수 있다. 러닝 프로세서는 학습 데이터를 이용하여 인공 신경망으로 구성된 모델을 학습시킬 수 있다. 여기서, 학습 된 인공 신경망을 학습 모델이라 칭할 수 있다. 학습 모델은 학습 데이터가 아닌 새로운 입력 데이터에 대하여 결과 값을 추론해 내는데 사용될 수 있고, 추론된 값은 어떠한 동작을 수행하기 위한 판단의 기초로 이용될 수 있다. 이때, 러닝 프로세서는 AI 서버의 러닝 프로세서과 함께 AI 프로세싱을 수행할 수 있다. 이때, 러닝 프로세서는 AI 장치에 통합되거나 구현된 메모리를 포함할 수 있다. 또는, 러닝 프로세서 는 메모리, AI 장치에 직접 결합된 외부 메모리 또는 외부 장치에서 유지되는 메모리를 사용하 여 구현될 수도 있다. 센싱부는 다양한 센서들을 이용하여 AI 장치 내부 정보, AI 장치의 주변 환경 정보 및 사용자 정보 중 적어도 하나를 획득할 수 있다. 이때, 센싱부에 포함되는 센서에는 근접 센서, 조도 센서, 가속도 센서, 자기 센서, 자이로 센서, 관성 센 서, RGB 센서, IR 센서, 지문 인식 센서, 초음파 센서, 광 센서, 마이크로폰, 라이다, 레이더 등이 있다. 출력부는 시각, 청각 또는 촉각 등과 관련된 출력을 발생시킬 수 있다. 이때, 출력부에는 시각 정보를 출력하는 디스플레이부, 청각 정보를 출력하는 스피커, 촉각 정보를 출력하 는 햅틱 모듈 등이 포함될 수 있다. 메모리는 AI 장치의 다양한 기능을 지원하는 데이터를 저장할 수 있다. 예컨대, 메모리는 입력 부에서 획득한 입력 데이터, 학습 데이터, 학습 모델, 학습 히스토리 등을 저장할 수 있다. 프로세서는 데이터 분석 알고리즘 또는 머신 러닝 알고리즘을 사용하여 결정되거나 생성된 정보에 기초하 여, AI 장치의 적어도 하나의 실행 가능한 동작을 결정할 수 있다. 그리고, 프로세서는 AI 장치(10 0)의 구성 요소들을 제어하여 결정된 동작을 수행할 수 있다. 이를 위해, 프로세서는 러닝 프로세서 또는 메모리의 데이터를 요청, 검색, 수신 또는 활용할 수 있고, 상기 적어도 하나의 실행 가능한 동작 중 예측되는 동작이나, 바람직한 것으로 판단되는 동작을 실행 하도록 AI 장치의 구성 요소들을 제어할 수 있다. 이때, 프로세서는 결정된 동작을 수행하기 위하여 외부 장치의 연계가 필요한 경우, 해당 외부 장치를 제 어하기 위한 제어 신호를 생성하고, 생성한 제어 신호를 해당 외부 장치에 전송할 수 있다. 프로세서는 사용자 입력에 대하여 의도 정보를 획득하고, 획득한 의도 정보에 기초하여 사용자의 요구 사 항을 결정할 수 있다. 이때, 프로세서는 음성 입력을 문자열로 변환하기 위한 STT(Speech To Text) 엔진 또는 자연어의 의도 정 보를 획득하기 위한 자연어 처리(NLP: Natural Language Processing) 엔진 중에서 적어도 하나 이상을 이용하여, 사용자 입력에 상응하는 의도 정보를 획득할 수 있다. 이때, STT 엔진 또는 NLP 엔진 중에서 적어도 하나 이상은 적어도 일부가 머신 러닝 알고리즘에 따라 학습된 인 공 신경망으로 구성될 수 있다. 그리고, STT 엔진 또는 NLP 엔진 중에서 적어도 하나 이상은 러닝 프로세서 에 의해 학습된 것이나, AI 서버의 러닝 프로세서에 의해 학습된 것이거나, 또는 이들의 분산 처리에 의해 학습된 것일 수 있다. 프로세서는 AI 장치의 동작 내용이나 동작에 대한 사용자의 피드백 등을 포함하는 이력 정보를 수집 하여 메모리 또는 러닝 프로세서에 저장하거나, AI 서버 등의 외부 장치에 전송할 수 있다. 수 집된 이력 정보는 학습 모델을 갱신하는데 이용될 수 있다. 프로세서는 메모리에 저장된 응용 프로그램을 구동하기 위하여, AI 장치의 구성 요소들 중 적어 도 일부를 제어할 수 있다. 나아가, 프로세서는 상기 응용 프로그램의 구동을 위하여, AI 장치에 포 함된 구성 요소들 중 둘 이상을 서로 조합하여 동작시킬 수 있다. 이하, 본 개시에서 클라우드 서버로 사용될 수 있는 AI 서버에 대해 설명하겠다. 도 2는 본 개시의 일 실시 예 에 따른 AI 서버를 나타낸다. 도 2를 참조하면, AI 서버는 머신 러닝 알고리즘을 이용하여 인공 신경망을 학습시키거나 학습된 인공 신 경망을 이용하는 장치를 의미할 수 있다. 여기서, AI 서버는 복수의 서버들로 구성되어 분산 처리를 수행 할 수도 있고, 5G 네트워크로 정의될 수 있다. 이때, AI 서버는 AI 장치의 일부의 구성으로 포함되어, AI 프로세싱 중 적어도 일부를 함께 수행할 수도 있다. AI 서버는 통신부, 메모리, 러닝 프로세서 및 프로세서 등을 포함할 수 있다. 통신부는 AI 장치 등의 외부 장치와 데이터를 송수신할 수 있다. 메모리는 모델 저장부를 포함할 수 있다. 모델 저장부는 러닝 프로세서을 통하여 학습 중 인 또는 학습된 모델(또는 인공 신경망, 231a)을 저장할 수 있다. 러닝 프로세서는 학습 데이터를 이용하여 인공 신경망(231a)을 학습시킬 수 있다. 학습 모델은 인공 신경 망의 AI 서버에 탑재된 상태에서 이용되거나, AI 장치 등의 외부 장치에 탑재되어 이용될 수도 있다. 학습 모델은 하드웨어, 소프트웨어 또는 하드웨어와 소프트웨어의 조합으로 구현될 수 있다. 학습 모델의 일부 또는 전부가 소프트웨어로 구현되는 경우 학습 모델을 구성하는 하나 이상의 명령어(instruction)는 메모리 에 저장될 수 있다. 프로세서는 학습 모델을 이용하여 새로운 입력 데이터에 대하여 결과 값을 추론하고, 추론한 결과 값에 기 초한 응답이나 제어 명령을 생성할 수 있다. 이하, 도 3을 참조하여, 본 개시의 일실시예에 따른 AI 시스템에 대해 설명하겠다. 도 3은 본 개시의 일 실 시 예에 따른 AI 시스템을 나타낸다. 도 3을 참조하면, AI 시스템은 AI 서버, 로봇(100a), 자율 주행 차량(100b), XR 장치(100c), 스마트폰 (100d) 또는 가전(100e) 중에서 적어도 하나 이상이 클라우드 네트워크와 연결되어 구성될 수 있다. 여기서, AI 기술이 적용된 로봇(100a), 자율 주행 차량(100b), XR 장치(100c), 스마트폰(100d) 또는 가전 (100e) 등을 AI 장치(100a 내지 100e)라 칭할 수 있다. 클라우드 네트워크는 클라우드 컴퓨팅 인프라의 일부를 구성하거나 클라우드 컴퓨팅 인프라 안에 존재하는 네트워크를 의미할 수 있다. 여기서, 클라우드 네트워크는 3G 네트워크, 4G 또는 LTE(Long Term Evolution) 네트워크 또는 5G 네트워크 등을 이용하여 구성될 수 있다. 즉, AI 시스템을 구성하는 각 장치들(100a 내지 100e, 200)은 클라우드 네트워크를 통해 서로 연결될 수 있다. 특히, 각 장치들(100a 내지 100e, 200)은 기지국을 통해서 서로 통신할 수도 있지만, 기지국을 통하지 않 고 직접 서로 통신할 수도 있다. AI 서버는 AI 프로세싱을 수행하는 서버와 빅 데이터에 대한 연산을 수행하는 서버를 포함할 수 있다. AI 서버는 AI 시스템을 구성하는 AI 장치들인 로봇(100a), 자율 주행 차량(100b), XR 장치(100c), 스 마트폰(100d) 또는 가전(100e) 중에서 적어도 하나 이상과 클라우드 네트워크을 통하여 연결되고, 연결된 AI 장치들(100a 내지 100e)의 AI 프로세싱을 적어도 일부를 도울 수 있다. 이때, AI 서버는 AI 장치(100a 내지 100e)를 대신하여 머신 러닝 알고리즘에 따라 인공 신경망을 학습시킬 수 있고, 학습 모델을 직접 저장하거나 AI 장치(100a 내지 100e)에 전송할 수 있다. 이때, AI 서버는 AI 장치(100a 내지 100e)로부터 입력 데이터를 수신하고, 학습 모델을 이용하여 수신한 입력 데이터에 대하여 결과 값을 추론하고, 추론한 결과 값에 기초한 응답이나 제어 명령을 생성하여 AI 장치 (100a 내지 100e)로 전송할 수 있다. 또는, AI 장치(100a 내지 100e)는 직접 학습 모델을 이용하여 입력 데이터에 대하여 결과 값을 추론하고, 추론 한 결과 값에 기초한 응답이나 제어 명령을 생성할 수도 있다. 이하에서는, 상술한 기술이 적용되는 AI 장치(100a 내지 100e)의 다양한 실시 예들을 설명한다. 여기서, 도 3에 도시된 AI 장치(100a 내지 100e)는 도 1에 도시된 AI 장치의 구체적인 실시 예로 볼 수 있다. 로봇(100a)은 AI 기술이 적용되어, 안내 로봇, 운반 로봇, 청소 로봇, 웨어러블 로봇, 엔터테인먼트 로봇, 펫 로봇, 무인 비행 로봇 등으로 구현될 수 있다. 로봇(100a)은 동작을 제어하기 위한 로봇 제어 모듈을 포함할 수 있고, 로봇 제어 모듈은 소프트웨어 모듈 또는 이를 하드웨어로 구현한 칩을 의미할 수 있다. 로봇(100a)은 다양한 종류의 센서들로부터 획득한 센서 정보를 이용하여 로봇(100a)의 상태 정보를 획득하거나, 주변 환경 및 객체를 검출(인식)하거나, 맵 데이터를 생성하거나, 이동 경로 및 주행 계획을 결정하거나, 사용 자 상호작용에 대한 응답을 결정하거나, 동작을 결정할 수 있다. 여기서, 로봇(100a)은 이동 경로 및 주행 계획을 결정하기 위하여, 라이다, 레이더, 카메라 중에서 적어도 하나 이상의 센서에서 획득한 센서 정보를 이용할 수 있다. 로봇(100a)은 적어도 하나 이상의 인공 신경망으로 구성된 학습 모델을 이용하여 상기한 동작들을 수행할 수 있 다. 예컨대, 로봇(100a)은 학습 모델을 이용하여 주변 환경 및 객체를 인식할 수 있고, 인식된 주변 환경 정보 또는 객체 정보를 이용하여 동작을 결정할 수 있다. 여기서, 학습 모델은 로봇(100a)에서 직접 학습되거나, AI 서버 등의 외부 장치에서 학습된 것일 수 있다. 이때, 로봇(100a)은 직접 학습 모델을 이용하여 결과를 생성하여 동작을 수행할 수도 있지만, AI 서버 등 의 외부 장치에 센서 정보를 전송하고 그에 따라 생성된 결과를 수신하여 동작을 수행할 수도 있다. 로봇(100a)은 맵 데이터, 센서 정보로부터 검출한 객체 정보 또는 외부 장치로부터 획득한 객체 정보 중에서 적 어도 하나 이상을 이용하여 이동 경로와 주행 계획을 결정하고, 구동부를 제어하여 결정된 이동 경로와 주행 계 획에 따라 로봇(100a)을 주행시킬 수 있다. 맵 데이터에는 로봇(100a)이 이동하는 공간에 배치된 다양한 객체들에 대한 객체 식별 정보가 포함될 수 있다. 예컨대, 맵 데이터에는 벽, 문 등의 고정 객체들과 화분, 책상 등의 이동 가능한 객체들에 대한 객체 식별 정보 가 포함될 수 있다. 그리고, 객체 식별 정보에는 명칭, 종류, 거리, 위치 등이 포함될 수 있다. 또한, 로봇(100a)은 사용자의 제어/상호작용에 기초하여 구동부를 제어함으로써, 동작을 수행하거나 주행할 수 있다. 이때, 로봇(100a)은 사용자의 동작이나 음성 발화에 따른 상호작용의 의도 정보를 획득하고, 획득한 의도 정보에 기초하여 응답을 결정하여 동작을 수행할 수 있다. 자율 주행 차량(100b)은 AI 기술이 적용되어, 이동형 로봇, 차량, 무인 비행체 등으로 구현될 수 있다. 자율 주행 차량(100b)은 자율 주행 기능을 제어하기 위한 자율 주행 제어 모듈을 포함할 수 있고, 자율 주행 제 어 모듈은 소프트웨어 모듈 또는 이를 하드웨어로 구현한 칩을 의미할 수 있다. 자율 주행 제어 모듈은 자율 주 행 차량(100b)의 구성으로써 내부에 포함될 수도 있지만, 자율 주행 차량(100b)의 외부에 별도의 하드웨어로 구 성되어 연결될 수도 있다. 자율 주행 차량(100b)은 다양한 종류의 센서들로부터 획득한 센서 정보를 이용하여 자율 주행 차량(100b)의 상 태 정보를 획득하거나, 주변 환경 및 객체를 검출(인식)하거나, 맵 데이터를 생성하거나, 이동 경로 및 주행 계 획을 결정하거나, 동작을 결정할 수 있다. 여기서, 자율 주행 차량(100b)은 이동 경로 및 주행 계획을 결정하기 위하여, 로봇(100a)과 마찬가지로, 라이다, 레이더, 카메라 중에서 적어도 하나 이상의 센서에서 획득한 센서 정보를 이용할 수 있다. 특히, 자율 주행 차량(100b)은 시야가 가려지는 영역이나 일정 거리 이상의 영역에 대한 환경이나 객체는 외부 장치들로부터 센서 정보를 수신하여 인식하거나, 외부 장치들로부터 직접 인식된 정보를 수신할 수 있다. 자율 주행 차량(100b)은 적어도 하나 이상의 인공 신경망으로 구성된 학습 모델을 이용하여 상기한 동작들을 수 행할 수 있다. 예컨대, 자율 주행 차량(100b)은 학습 모델을 이용하여 주변 환경 및 객체를 인식할 수 있고, 인 식된 주변 환경 정보 또는 객체 정보를 이용하여 주행 동선을 결정할 수 있다. 여기서, 학습 모델은 자율 주행 차량(100b)에서 직접 학습되거나, AI 서버 등의 외부 장치에서 학습된 것일 수 있다. 이때, 자율 주행 차량(100b)은 직접 학습 모델을 이용하여 결과를 생성하여 동작을 수행할 수도 있지만, AI 서 버 등의 외부 장치에 센서 정보를 전송하고 그에 따라 생성된 결과를 수신하여 동작을 수행할 수도 있다. 자율 주행 차량(100b)은 맵 데이터, 센서 정보로부터 검출한 객체 정보 또는 외부 장치로부터 획득한 객체 정보 중에서 적어도 하나 이상을 이용하여 이동 경로와 주행 계획을 결정하고, 구동부를 제어하여 결정된 이동 경로 와 주행 계획에 따라 자율 주행 차량(100b)을 주행시킬 수 있다. 맵 데이터에는 자율 주행 차량(100b)이 주행하는 공간(예컨대, 도로)에 배치된 다양한 객체들에 대한 객체 식별 정보가 포함될 수 있다. 예컨대, 맵 데이터에는 가로등, 바위, 건물 등의 고정 객체들과 차량, 보행자 등의 이 동 가능한 객체들에 대한 객체 식별 정보가 포함될 수 있다. 그리고, 객체 식별 정보에는 명칭, 종류, 거리, 위 치 등이 포함될 수 있다. 또한, 자율 주행 차량(100b)은 사용자의 제어/상호작용에 기초하여 구동부를 제어함으로써, 동작을 수행하거나 주행할 수 있다. 이때, 자율 주행 차량(100b)은 사용자의 동작이나 음성 발화에 따른 상호작용의 의도 정보를 획득하고, 획득한 의도 정보에 기초하여 응답을 결정하여 동작을 수행할 수 있다. XR 장치(100c)는 AI 기술이 적용되어, HMD(Head-Mount Display), 차량에 구비된 HUD(Head-Up Display), 텔레비 전, 휴대폰, 스마트 폰, 컴퓨터, 웨어러블 디바이스, 가전 기기, 디지털 사이니지, 차량, 고정형 로봇이나 이동형 로봇 등으로 구현될 수 있다. XR 장치(100c)는 다양한 센서들을 통해 또는 외부 장치로부터 획득한 3차원 포인트 클라우드 데이터 또는 이미 지 데이터를 분석하여 3차원 포인트들에 대한 위치 데이터 및 속성 데이터를 생성함으로써 주변 공간 또는 현실 객체에 대한 정보를 획득하고, 출력할 XR 객체를 렌더링하여 출력할 수 있다. 예컨대, XR 장치(100c)는 인식된 물체에 대한 추가 정보를 포함하는 XR 객체를 해당 인식된 물체에 대응시켜 출력할 수 있다. XR 장치(100c)는 적어도 하나 이상의 인공 신경망으로 구성된 학습 모델을 이용하여 상기한 동작들을 수행할 수 있다. 예컨대, XR 장치(100c)는 학습 모델을 이용하여 3차원 포인트 클라우드 데이터 또는 이미지 데이터에서 현실 객체를 인식할 수 있고, 인식한 현실 객체에 상응하는 정보를 제공할 수 있다. 여기서, 학습 모델은 XR 장 치(100c)에서 직접 학습되거나, AI 서버 등의 외부 장치에서 학습된 것일 수 있다. 이때, XR 장치(100c)는 직접 학습 모델을 이용하여 결과를 생성하여 동작을 수행할 수도 있지만, AI 서버 등의 외부 장치에 센서 정보를 전송하고 그에 따라 생성된 결과를 수신하여 동작을 수행할 수도 있다. 로봇(100a)은 AI 기술 및 자율 주행 기술이 적용되어, 안내 로봇, 운반 로봇, 청소 로봇, 웨어러블 로봇, 엔터 테인먼트 로봇, 펫 로봇, 무인 비행 로봇 등으로 구현될 수 있다. AI 기술과 자율 주행 기술이 적용된 로봇(100a)은 자율 주행 기능을 가진 로봇 자체나, 자율 주행 차량(100b)과 상호작용하는 로봇(100a) 등을 의미할 수 있다. 자율 주행 기능을 가진 로봇(100a)은 사용자의 제어 없이도 주어진 동선에 따라 스스로 움직이거나, 동선을 스 스로 결정하여 움직이는 장치들을 통칭할 수 있다. 자율 주행 기능을 가진 로봇(100a) 및 자율 주행 차량(100b)은 이동 경로 또는 주행 계획 중 하나 이상을 결정 하기 위해 공통적인 센싱 방법을 사용할 수 있다. 예를 들어, 자율 주행 기능을 가진 로봇(100a) 및 자율 주행 차량(100b)은 라이다, 레이더, 카메라를 통해 센싱된 정보를 이용하여, 이동 경로 또는 주행 계획 중 하나 이상 을 결정할 수 있다. 자율 주행 차량(100b)과 상호작용하는 로봇(100a)은 자율 주행 차량(100b)과 별개로 존재하면서, 자율 주행 차 량(100b)의 내부에서 자율 주행 기능에 연계되거나, 자율 주행 차량(100b)에 탑승한 사용자와 연계된 동작을 수 행할 수 있다. 이때, 자율 주행 차량(100b)과 상호작용하는 로봇(100a)은 자율 주행 차량(100b)을 대신하여 센서 정보를 획득 하여 자율 주행 차량(100b)에 제공하거나, 센서 정보를 획득하고 주변 환경 정보 또는 객체 정보를 생성하여 자 율 주행 차량(100b)에 제공함으로써, 자율 주행 차량(100b)의 자율 주행 기능을 제어하거나 보조할 수 있다. 또는, 자율 주행 차량(100b)과 상호작용하는 로봇(100a)은 자율 주행 차량(100b)에 탑승한 사용자를 모니터링하 거나 사용자와의 상호작용을 통해 자율 주행 차량(100b)의 기능을 제어할 수 있다. 예컨대, 로봇(100a)은 운전 자가 졸음 상태인 경우로 판단되는 경우, 자율 주행 차량(100b)의 자율 주행 기능을 활성화하거나 자율 주행 차 량(100b)의 구동부의 제어를 보조할 수 있다. 여기서, 로봇(100a)이 제어하는 자율 주행 차량(100b)의 기능에는 단순히 자율 주행 기능뿐만 아니라, 자율 주행 차량(100b)의 내부에 구비된 네비게이션 시스템이나 오디오 시스 템에서 제공하는 기능도 포함될 수 있다. 또는, 자율 주행 차량(100b)과 상호작용하는 로봇(100a)은 자율 주행 차량(100b)의 외부에서 자율 주행 차량 (100b)에 정보를 제공하거나 기능을 보조할 수 있다. 예컨대, 로봇(100a)은 스마트 신호등과 같이 자율 주행 차 량(100b)에 신호 정보 등을 포함하는 교통 정보를 제공할 수도 있고, 전기 차량의 자동 전기 충전기와 같이 자 율 주행 차량(100b)과 상호작용하여 충전구에 전기 충전기를 자동으로 연결할 수도 있다. 로봇(100a)은 AI 기술 및 XR 기술이 적용되어, 안내 로봇, 운반 로봇, 청소 로봇, 웨어러블 로봇, 엔터테인먼트 로봇, 펫 로봇, 무인 비행 로봇, 드론 등으로 구현될 수 있다. XR 기술이 적용된 로봇(100a)은 XR 영상 내에서의 제어/상호작용의 대상이 되는 로봇을 의미할 수 있다. 이 경 우, 로봇(100a)은 XR 장치(100c)와 구분되며 서로 연동될 수 있다. XR 영상 내에서의 제어/상호작용의 대상이 되는 로봇(100a)은 카메라를 포함하는 센서들로부터 센서 정보를 획 득하면, 로봇(100a) 또는 XR 장치(100c)는 센서 정보에 기초한 XR 영상을 생성하고, XR 장치(100c)는 생성된 XR 영상을 출력할 수 있다. 그리고, 이러한 로봇(100a)은 XR 장치(100c)를 통해 입력되는 제어 신호 또는 사용자의 상호작용에 기초하여 동작할 수 있다.예컨대, 사용자는 XR 장치(100c) 등의 외부 장치를 통해 원격으로 연동된 로봇(100a)의 시점에 상응하는 XR 영 상을 확인할 수 있고, 상호작용을 통하여 로봇(100a)의 자율 주행 경로를 조정하거나, 동작 또는 주행을 제어하 거나, 주변 객체의 정보를 확인할 수 있다. 자율 주행 차량(100b)은 AI 기술 및 XR 기술이 적용되어, 이동형 로봇, 차량, 무인 비행체 등으로 구현될 수 있 다. XR 기술이 적용된 자율 주행 차량(100b)은 XR 영상을 제공하는 수단을 구비한 자율 주행 차량이나, XR 영상 내 에서의 제어/상호작용의 대상이 되는 자율 주행 차량 등을 의미할 수 있다. 특히, XR 영상 내에서의 제어/상호 작용의 대상이 되는 자율 주행 차량(100b)은 XR 장치(100c)와 구분되며 서로 연동될 수 있다. XR 영상을 제공하는 수단을 구비한 자율 주행 차량(100b)은 카메라를 포함하는 센서들로부터 센서 정보를 획득 하고, 획득한 센서 정보에 기초하여 생성된 XR 영상을 출력할 수 있다. 예컨대, 자율 주행 차량(100b)은 HUD를 구비하여 XR 영상을 출력함으로써, 탑승자에게 현실 객체 또는 화면 속의 객체에 대응되는 XR 객체를 제공할 수 있다. 이때, XR 객체가 HUD에 출력되는 경우에는 XR 객체의 적어도 일부가 탑승자의 시선이 향하는 실제 객체에 오버 랩되도록 출력될 수 있다. 반면, XR 객체가 자율 주행 차량(100b)의 내부에 구비되는 디스플레이에 출력되는 경 우에는 XR 객체의 적어도 일부가 화면 속의 객체에 오버랩되도록 출력될 수 있다. 예컨대, 자율 주행 차량 (100b)은 차로, 타 차량, 신호등, 교통 표지판, 이륜차, 보행자, 건물 등과 같은 객체와 대응되는 XR 객체들을 출력할 수 있다. XR 영상 내에서의 제어/상호작용의 대상이 되는 자율 주행 차량(100b)은 카메라를 포함하는 센서들로부터 센서 정보를 획득하면, 자율 주행 차량(100b) 또는 XR 장치(100c)는 센서 정보에 기초한 XR 영상을 생성하고, XR 장 치(100c)는 생성된 XR 영상을 출력할 수 있다. 그리고, 이러한 자율 주행 차량(100b)은 XR 장치(100c) 등의 외 부 장치를 통해 입력되는 제어 신호 또는 사용자의 상호작용에 기초하여 동작할 수 있다. 도 4는 본 개시의 일 실시 예에 따른 AI 장치를 나타낸다. 도 1과 중복되는 설명은 생략한다. 도 4를 참조하면, 입력부는 영상 신호 입력을 위한 카메라(Camera, 121), 오디오 신호를 수신하기 위한 마 이크로폰(Microphone, 122), 사용자로부터 정보를 입력 받기 위한 사용자 입력부(User Input Unit, 123)를 포 함할 수 있다. 입력부에서 수집한 음성 데이터나 이미지 데이터는 분석되어 사용자의 제어 명령으로 처리될 수 있다. 입력부는 영상 정보(또는 신호), 오디오 정보(또는 신호), 데이터, 또는 사용자로부터 입력되는 정보의 입 력을 위한 것으로서, 영상 정보의 입력을 위하여, AI 장치는 하나 또는 복수의 카메라들을 구비할 수 있다. 카메라는 화상 통화모드 또는 촬영 모드에서 이미지 센서에 의해 얻어지는 정지영상 또는 동영상 등의 화 상 프레임을 처리한다. 처리된 화상 프레임은 디스플레이부(Display Unit, 151)에 표시되거나 메모리에 저 장될 수 있다. 마이크로폰은 외부의 음향 신호를 전기적인 음성 데이터로 처리한다. 처리된 음성 데이터는 AI 장치 에서 수행 중인 기능(또는 실행 중인 응용 프로그램)에 따라 다양하게 활용될 수 있다. 한편, 마이크로폰 에는 외부의 음향 신호를 입력 받는 과정에서 발생되는 잡음(noise)을 제거하기 위한 다양한 잡음 제거 알고리 즘이 적용될 수 있다. 사용자 입력부는 사용자로부터 정보를 입력 받기 위한 것으로서, 사용자 입력부를 통해 정보가 입력 되면, 프로세서는 입력된 정보에 대응되도록 AI 장치의 동작을 제어할 수 있다. 사용자 입력부는 기계식 (mechanical) 입력수단(또는, 메커니컬 키, 예컨대, 단말기의 전/후면 또는 측면에 위치하는 버튼, 돔 스위치 (dome switch), 조그 휠, 조그 스위치 등) 및 터치식 입력수단을 포함할 수 있다. 일 예로서, 터치식 입력수단은, 소프트웨어적인 처리를 통해 터치스크린에 표시되는 가상 키(virtual key), 소프트 키(soft key) 또는 비주얼 키(visual key)로 이루어지거나, 상기 터치스크린 이외의 부분에 배치 되는 터치 키(touch key)로 이루어질 수 있다.출력부는 디스플레이부(Display Unit, 151), 음향 출력부(Sound Output Unit, 152), 햅틱 모듈(Haptic Module, 153), 광 출력부(Optical Output Unit, 154) 중 적어도 하나를 포함할 수 있다. 디스플레이부는 AI 장치에서 처리되는 정보를 표시(출력)한다. 예컨대, 디스플레이부는 AI 장치 에서 구동되는 응용 프로그램의 실행화면 정보, 또는 이러한 실행화면 정보에 따른 UI(User Interface), GUI(Graphic User Interface) 정보를 표시할 수 있다. 디스플레이부는 터치 센서와 상호 레이어 구조를 이루거나 일체형으로 형성됨으로써, 터치 스크린을 구현 할 수 있다. 이러한 터치 스크린은, AI 장치와 사용자 사이의 입력 인터페이스를 제공하는 사용자 입력부 로써 기능함과 동시에, 단말기와 사용자 사이의 출력 인터페이스를 제공할 수 있다. 음향 출력부는 호신호 수신, 통화모드 또는 녹음 모드, 음성인식 모드, 방송수신 모드 등에서 통신부(11 0)로부터 수신되거나 메모리에 저장된 오디오 데이터를 출력할 수 있다. 음향 출력부는 리시버(receiver), 스피커(speaker), 버저(buzzer) 중 적어도 하나 이상을 포함할 수 있다. 햅틱 모듈(haptic module)은 사용자가 느낄 수 있는 다양한 촉각 효과를 발생시킨다. 햅틱 모듈이 발 생시키는 촉각 효과의 대표적인 예로는 진동이 될 수 있다. 광출력부는 AI 장치의 광원의 빛을 이용하여 이벤트 발생을 알리기 위한 신호를 출력한다. AI 장치 에서 발생 되는 이벤트의 예로는 메시지 수신, 호 신호 수신, 부재중 전화, 알람, 일정 알림, 이메일 수신, 애플리케이션을 통한 정보 수신 등이 될 수 있다.이하에서는 상기 에지 디바이스가 차량(vehicle) 또는 상기 차량에 장착될 수 있는 차량용 디스플레이 장치인 것으로 가정하고 설명하겠다. 상기 차량의 상기 센싱부에 포함되어 있는 적어도 하나의 센서들은 상기 차량과 관련된 각종 데 이터를 수집할 수 있다. 상기 적어도 하나의 센서들에 의해 수집되는 상기 차량과 관련된 데이터가 에지 데이터로 이해될 수 있다. 상기 센싱부는 위치 탐색부(미도시), 차량상태 수집부(미도시), 및 차량 내외 상황 수집부(미도시) 중 적 어도 하나를 포함할 수 있다. 상기 위치 탐색부는 상기 차량의 현재위치를 탐색하여 상기 차량의 프로세서에 제공할 수 있다. 여기서, 상기 위치 탐색부는 위성항법과 추측항법 중 어느 하나 이상을 통해 차량의 현재위치를 탐색할 수 있다. 이때, 위성항법은 GNSS(Global Navigation Satellite System)를 기반으로 차량의 위치정보를 측정하는 것으로, GPS(Global Positioning System), GLONASS(Global Navigation Satellite System), 갈릴레오(Galileo), 베이더 우(Beidou) 등의 시스템으로부터 위치정보를 수신하여 현재 위치를 탐색할 수 있다. 또한, 추측항법은 차량의 속도계(미도시), 자이로센서(미도시), 및 지자기센서(미도시) 등으로부터 획득한 속도 및 차량의 DR(Dead Reckoning) 정보를 기반으로 차량의 위치정보를 측정하여 현재 위치를 탐색할 수 있다. 상기 차량상태 수집부는 상기 차량의 구동상태를 수집하여 상기 차량의 프로세서에 제공할 수 있다. 예를 들어, 차량의 자세, 조향상태, 제동상태, 가속상태 및 주행상태 등을 수집하여 제공할 수 있다. 상기 차량 내외 상황 수집부는 차량의 외부 및 내부 상황에 대한 데이터를 수집하여 상기 차량의 프로세서(18 0)에 제공할 수 있다. 여기서 차량 내외 상황 수집부는, 카메라 (예를 들면, 차량 외부 카메라 (즉, 전방 카메라, 좌측방 카메라, 우 측방 카메라, 후방 카메라 등) 및 차량 내부 카메라 (운전자 카메라, 탑승자 카메라 등), 레이더, 및 라이더 중 어느 하나 이상을 포함하여, 자율주행에 필요한 모든 주변상황, 즉, 차선, 신호등, 주변차량, 보행자, 장애물, 및 탑승자 등을 검출할 수 있다. 한편, 도 4에서 도시되지는 않았지만, 상기 차량은 내비게이션 시스템을 포함할 수 있다. 상기 내비게이션 시스템은, 상기 차량의 운행을 보조하기 위한 내비게이션 정보를 제공할 수 있다. 상기 내비게이션 정보는, 맵(map) 정보, 설정된 목적지 정보, 상기 목적지 설정 따른 경로 정보, 경로 상의 다양한 오브젝트에 대한 정보, 차선 정보 및 차량의 현재 위치 정보 중 적어도 어느 하나를 포함할 수 있다.상기 내비게이션 시스템은, 내비게이션 정보를 저장하기 위한 별도의 메모리, 및 상기 내비케이션 시스템의 동 작을 제어하기 위한 전용 프로세서를 포함할 수 있다. 상기 내비게이션 시스템은, 상기 통신부를 통해, 외부 디바이스로부터 정보를 수신하여, 기 저장된 정보를 업데이트 할 수 있다. 상기 내비케이션 시스템은, 사용자로부터 명령 또는 정보를 입력을 받기 위해 상기 사용자 입력부과 연동 될 수 있다. 이하, 도 5를 참조하여, 본 개시의 일실시예에 따른 에지 디바이스 및 클라우드 서버 간의 협업을 위한 시스템 을 소프트웨어 관점에서 살펴보겠다. 도 5는 본 개시의 일실시예에 따른 에지 디바이스(예를 들면, 차량) 및 클 라우드 서버 간의 협업 시스템의 소프트웨어 관점의 블록도이다. 상기 차량에서 수집되는 각종 에지 데이터는 운행 컨텍스트 데이터 매니저(driving context data manager)로 전송될 수 있다. 상기 에지 데이터는 상기 차량의 운행 중에 수집될 수도 있고, 상기 차 량이 정지되어 있을 때 수집될 수도 있다. 상기 전송은 상기 에지 데이터가 수집됨에 따라 실시간으로 행 해질 수 있다. 즉, 상기 에지 데이터가 상기 차량에서 수집되면 사실상 바로 상기 운행 컨텍스트 데이터 매니저로 전송될 수 있다. 상기 에지 데이터는 상기 차량과 같은 에지 디바이스에서 수집할 수 있는 데이터를 의미하는 것으로서 그 예시는 아래와 같다. 다만 이에 한정되는 것은 아니다. 상기 에지 데이터는 상기 위치 탐색부, 상기 차량상태 수집부, 및 상기 차량 내외 상황 수집부에 의해 수집될 수 있는 데이터일 수 있다. 예를 들면, 상기 에지 데이터는 상기 차량의 외부 카메라과 같은 비전 센서를 통해 실시간으로 입력되는 외부 카메라 촬영 영상(예를 들면, 주행 영상)을 포함할 수 있다. 상기 외부 카메라 촬영 영상을 통해 주변 빌 딩에 관한 정보가 파악될 수 있다. 또한, 상기 에지 데이터는, 상기 내비게이션 시스템에 탑재되어 있는 맵 정보에 기반한 도로 종류(예를 들면, 고속도로, 국도, 이면도로 등), 차선 개수 및 너비, 빌딩 좌표 정보 존재 여부 등 상기 차량이 주행하는 도로의 지형 및/또는 지물에 관한 정보를 포함할 수 있다. 또한, 상기 에지 데이터는 상기 차량이 주행되는 중에 상기 차량상태 수집부를 통해 수집되는 차량의 외부 및 내부 상황에 대한 데이터를 포함할 수 있다. 예를 들면, 센싱되는 주행 방향 및 속도, 주변 차량과의 이격 거리, 차량 자세 등이 있을 수 있다. 상기 센싱부는 ADAS 센서를 포함할 수 있고, 상기 에지 데이터는 상기 ADAS 센서를 통해 획득된 주행 방향 및 속도, 차선과의 거리 등의 주행 관련 센싱 데이터 뿐만 아니라, 차량 주변 장애물이나 보행자에 관한 데이터를 포함할 수 있다. 상기 통신부는 V2X통신부를 포함할 수 있고, 상기 에지 데이터는 상기 V2X 통신부를 통해 수집되는 차량 트래픽 관련 데이터를 포함할 수있다. 상기 에지 데이터는 상기 차량의 사용자(예를 들면, 운전자 및/또는 탑승자)의 개인 프로파일 정보를 포함 할 수 있다. 상기 운행 컨텍스트 데이터 매니저는 상기 에지 데이터로부터 운행 컨텍스트 데이터를 추출 또는 생성할 수 있다. 상기 운행 컨텍스트 데이터는 주행 경로 관련 데이터, 안전 운행 관련 데이터, 및 상기 차량의 연산 자원량 등 중 적어도 하나를 포함할 수 있다. 상기 운행 컨텍스트 데이터는 상기 에지 데이터로부터 상기 차량에 구비된 상기 운행 컨텍스트 데이터 매니저에 의해 변환된 것이므로, 상기 운행 컨텍스트 데이 터 또한 에지 데이터의 일종으로 이해될 수도 있다. 상기 운행 컨텍스트 정보에 기반하여 사이니지 표시 가능 영역, 최대 사이니지 표시 개수, 사이니지 노출 시간 등 중 적어도 하나가 결정될 수 있다. 상기 에지 데이터 및 이로부터 상기 운행 컨텍스트 데이터가 추출 또는 변환되는 것에 대해서는 나중에 도 6을 참조하여 다시 설 명하겠다. 상기 운행 컨텍스트 데이터 매니저는 상기 운행 컨텍스트 데이터를 클라우드 정책 매니저으로 전송할 수 있다. 상기 클라우드 정책 매니저는 클라우드 서버에 구현되는 것으로서 그 동작에 대해서는 나중 에 다시 설명하겠다. 한편, 상기 클라우드 서버 내에 구현되어 있는 사이니지 데이터 매니저는 적어도 하나의 외부 데이터 소스로부터 각종 사이니지 관련 정보를 취합 또는 수집하고, 상기 취합된 사이니지 관련 정보를 상기 클라우드 서버(200 내에 구비된 데이터베이스에 저장할 수 있다. 상기 저장되는 사이니지 관련 정보는 계속 상기 사 이니지 데이터 매니저가 상기 외부 데이터 소스로부터 새로운 정보를 취합함에 따라 주기적/비주기적으로 업데이트될 수 있다. 상기 외부 데이터 소스는 예를 들면 외부 서버일 수 있다. 상기 사이니지 관련 정보는 쿠폰 및 결제와 같은 거래 서비스 데이터, 광고 관련 데이터 등(이에 한정되지 않음)과 같은 사이니지 디스플레이 정보를 포함할 수 있다. 또한, 상기 사이니지 관련 정보는 지도 관련 데이터, 및 빌딩과 관련 지형 지물 데이터 등과 같은 상기 사이니지 디스플레이 정보를 디스플레이하기 위한 위 치 정보, 즉, 사이니지 위치 정보를 포함할 수 있다. 상기 사이니지 관련 정보는 디지털 사이니지 정보를 포함 할 수 있다. 상기 디지털 사이니지 정보는 AR(Augmented Reality) 정보를 포함할 수 있다. 상기 사이니지 데이터 매니저는 상기 클라우드 정책 매니저로부터 운행 컨텍스트 데이터를 수신하여 상기 데이터베이스에 저장할 수 있다. 상기 운행 컨텍스트 데이터 전부가 상기 데이터베이스에 저장 될 수 있고, 상기 운행 컨텍스트 데이터 중에서 사용자 프로파일이 선별적으로 상기 데이터베이스에 저장 될 수도 있다. 그리고, 상기 사이니지 데이터 매니저는 상기 운행 컨텍스트 데이터에 기반하여 상기 취합된 사이니지 관 련 정보로부터, 상기 차량의 현재 위치 및/또는 예상 이동 경로에 해당하는 위치에 해당하는 여부에 부합 하는지 여부 등을 고려한 상기 차량에 적절한 사이니지 관련 정보를 추출할 수 있다. 이하, 상기 사이니지 데이터 매니저에 의해 추출된 사이니지 관련 정보를 제 1 사이니지 관련 정보라고도 호칭될 수 있다. 상기 사이니지 데이터 매니저는 제 1 사이니지 관련 정보를 상기 데이터베이스에 저장할 수 있다. 상기 데이터베이스에는 상기 외부 데이터 소스로부터 취합된 각종 사이니지 관련 정보 및 제 1 사이니지 관련 정보가 데이터 특성 별로 구분되어 각각 별도의 영역에서 저장되어 관리될 수 있다. 상기 클라우드 정책 매니저는 상기 운행 컨텍스트 데이터와 상기 추출된 사이니지 관련 정보 (즉, 제 1 사 이니지 관련 정보) 중 적어도 하나에 기반하여, 상기 추출된 사이니지 관련 정보가 상기 차량 및 상기 클 라우드 서버 중 어디에서 프로세싱되는 것이 적절한 것인지, 즉 프로세싱 위치에 대해 판단할 수 있다. 상 기 프로세싱 위치에서의 \"프로세싱\"은, 제 1 사이니지 관련 정보를 상기 차량에서 최종적으로 디스플레이 될 미디어 컨텐츠로 변환하기 위한 데이터 처리를 의미할 수 있다. 상기 프로세싱 위치를 정하는 것은, 제 1 사이니지 관련 정보를 상기 클라우드 서버 및 상기 에지 디바이 스 간에 어떻게 분배하는 것이 좋을 것인지를 추론하는 것으로 이해될 수 있으며, 이를 위해 상기 클라우드 정 책 매니저에는 상기 추론을 위한 인공지능 모델이 탑재되어 있을 수 있다. 즉, 상기 추출된 사이니지 관련 정보 및 상기 운행 컨텍스트 데이터가 상기 인공지능 모델에 대한 추론용 입력 데이터가 되고, 상기 인공지능 모델을 통해 추론 결과로서 제 1 사이니지 관련 정보가 출력될 수 있다. 예를 들면, 상기 클라우드 정책 매니저는 상기 사이니지 관련 정보가 상기 차량에 의해서만 프로세싱 되는 것으로 정할 수 있다. 이 경우 상기 프로세싱 위치는 상기 차량이 된다. 또는, 상기 클라우드 정책 매니저는 상기 사이니지 관련 정보가 상기 클라우드 서버에 의해서만 프로 세싱되는 것으로 정할 수 있다. 이 경우 상기 프로세싱 위치는 상기 클라우드 서버이 된다. 또는, 상기 클라우드 정책 매니저는 상기 사이니지 관련 정보가 상기 차량 및 상기 클라우드 서버 모두에 의해 협력되어 프로세싱되는 것으로 정할 수도 있다. 이 경우 상기 프로세싱 위치는 상기 차량 및 상기 클라우드 서버가 된다. 상기 프로세싱 위치가 결정되는 로직에 대해서는 나중에 다시 설명하겠다. 상기 클라우드 정책 매니저는, 상기 프로세싱 위치를 정하는 것 이외에도, 상기 운행 컨텍스트 데이터(특 히, 상기 사용자의 프로파일)에 기반하여 상기 차량을 통해 디스플레이될 사이니지 데이터(즉, 이하에서 설명되는 미디어 컨텐츠)의 디스플레이 정책을 도출할 수 있다. 상기 사이니지 데이터의 디스플레이 정책이라고 함은, 상기 사이니지 관련 정보 (즉, 제 1 사이니지 관련 정보) 중에서 상기 사용자가 관심 있을 것으로 여겨지 는 사이니지 관련 정보 (이하, 제 2 사이니지 관련 정보)를 필터링하기 위한 조건 또는 추론용 입력 데이터로 이해될 수 있다. 상기 클라우드 정책 매니저는 상기 디스플레이 정책을 도출함에 있어 상기 차량의 운행 컨텍스트 데 이터 뿐만 아니라, 상기 차량에 근접하여 이동 중인 다른 차량의 운행 컨텍스트 데이터도 함께 고려할 수도 있다. 이와 같이 상기 클라우드 정책 매니저에 의해 정해지는 상기 프로세싱 위치 및 상기 디스플레이 정책에 대 해 좀더 구체적으로 살펴 보겠다. 상기 클라우드 정책 매니저에 의해 상기 프로세싱 위치가 상기 클라우드 서버로 정해지는 경우에는, 상기 클라우드 정책 매니저는 상기 프로세싱 위치가 상기 클라우드 서버로 정해졌음을 알리는 제어 신호를 상기 사이니지 데이터 매니저 및 상기 클라우드 서버에 구비된 클라우드 사이니지 데이터 프 로세서로 전송할 수 있다. 상기 제어 신호에 반응하여, 상기 사이니지 데이터 매니저는 상기 추출된 제 1 사이니지 관련 정보를 상기 클라우드 사이니지 데이터 프로세서로 전송할 수 있다. 그리고, 상기 클라우드 정책 매니저는 상기 디스플레이 정책을 상기 클라우드 사이니지 데이터 프로세서 로 전송할 수 있다. 상기 클라우드 사이니지 데이터 프로세서는 사이니지 데이터 필터링 모듈과 미디어 컨텐츠 프로세싱 모듈을 포함할 수 있다. 상기 사이니지 데이터 필터링 모듈은 상기 디스플레이 정책에 기반하여 제 1 사이니지 관련 정보 중에서 상기 사용자가 관심 있을 것으로 추론되는 제 2 사이니지 관련 정보를 필터링할 수 있다. 상기 사이니지 데이 터 필터 모듈에는 제 2 사이니지 관련 정보를 추론하기 위한 인공지능 모델이 탑재되어 있을 수 있다. 즉, 상기 디스플레이 정책 및 제 1 사이니지 관련 정보가 상기 인공지능 모델에 대한 추론용 입력 데이터가 되고, 상기 인공지능 모델을 통해 추론 결과로서 제 2 사이니지 관련 정보가 출력될 수 있다. 그리고, 상기 미디어 컨텐츠 프로세싱 모듈는 상기 필터링된 제 2 사이니지 관련 정보를 AR 그래픽 데이터 와 같은 미디어 컨텐츠로 변환하고, 상기 변환된 미디어 컨텐츠에 상기 미디어 컨텐츠가 디스플레이될 위치에 관한 디스플레이 위치 정보를 태깅할 수 있다. 예를 들면, 제 2 사이니지 관련 정보 중에서 상기 사이니지 디스 플레이 정보가 상기 AR 그래픽 데이터로 변환될 수 있고, 이 때 상기 사이니지 디스플레이 정보는 상기 사이니 지 위치 정보가 고려되어 상기 AR 그래픽 데이터가 디스플레이될 빌딩 등과 같은 지형 또는 지물에 부합하도록 변환될 수 있다. 상기 클라우드 사이니지 데이터 프로세서는 상기 디스플레이 위치 정보가 태깅된 상기 미디어 컨텐츠를 상 기 차량에 구비된 랜더링 모듈로 전송할 수 있다. 상기 랜더링 모듈는 상기 차량이 상기 미디어 컨텐츠의 해당 위치에 소정 거리 이내로 접근하는 것으 로 파악되거나, 상기 주행 영상을 통해 상기 해당 지형 또는 지물이 인식되면, 상기 미디어 컨텐츠를 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭시킬 수 있다. 상기 미디어 컨텐츠는 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭되도록 상기 주행 영상과 함께 디 스플레이부를 통해 디스플레이될 수 있다. 상기 디스플레이부는 상기 차량 내의 CID (Center Information Display), 클러스터, HUD(Head-up Display), RSE(Rear Seat Entertainment), RMD(Room Mirror Display) 중 적어도 하나를 포함할 수 있다. 한편, 상기 클라우드 정책 매니저에 의해 상기 프로세싱 위치가 상기 차량로 정해지는 경우에는, 상 기 클라우드 정책 매니저는 상기 프로세싱 위치가 상기 차량으로 정해졌음을 알리는 제어 신호를 상 기 사이니지 데이터 매니저 및 상기 차량에 구비된 에지 사이니지 데이터 프로세서로 전송할 수 있다. 상기 제어 신호에 반응하여, 상기 사이니지 데이터 매니저는 상기 추출된 제 1 사이니지 관련 정보를 상기 에지 사이니지 데이터 프로세서로 전송할 수 있다. 그리고, 상기 클라우드 정책 매니저는 상기 디스플레이 정책을 상기 에지 사이니지 데이터 프로세서(100 0)로 전송할 수 있다. 상기 에지 사이니지 데이터 프로세서는, 상기 클라우드 사이니지 데이터 프로세서과 유사하게, 사이 니지 데이터 필터링 모듈과 미디어 컨텐츠 프로세싱 모듈을 포함할 수 있다. 상기 사이니지 데이터 필터링 모듈는 상기 디스플레이 정책에 기반하여 제 1 사이니지 관련 정보 중에서 상기 사용자가 관심 있을 것으로 추론되는 제 2 사이니지 관련 정보를 필터링할 수 있다. 상기 사이니지 데이터 필터 모듈에는 제 2 사이니지 관련 정보를 추론하기 위한 인공지능 모델이 탑재되어 있을 수 있다. 즉, 상기 디스플레이 정책 및 제 1 사이니지 관련 정보가 상기 인공지능 모델에 대한 추론용 입력 데이터가 되고, 상기 인공지능 모델을 통해 추론 결과로서 제 2 사이니지 관련 정보가 출력될 수 있다. 그리고, 상기 미디어 컨텐츠 프로세싱 모듈는 상기 필터링된 제 2 사이니지 관련 정보를 AR 그래픽 데이 터와 같은 미디어 컨텐츠로 변환하고, 상기 변환된 미디어 컨텐츠에 상기 미디어 컨텐츠가 디스플레이될 위치에 관한 디스플레이 위치 정보를 태깅할 수 있다. 예를 들면, 제 2 사이니지 관련 정보 중에서 상기 사이니지 디스 플레이 정보가 상기 AR 그래픽 데이터로 변환될 수 있고, 이 때 상기 사이니지 디스플레이 정보는 상기 사이니 지 위치 정보가 고려되어 상기 AR 그래픽 데이터가 디스플레이될 빌딩 등과 같은 지형 또는 지물에 부합하도록 변환될 수 있다. 상기 에지 사이니지 데이터 프로세서는 상기 디스플레이 위치 정보가 태깅된 상기 미디어 컨텐츠를 상기 랜더링 모듈로 전송할 수 있다. 상기 랜더링 모듈는 상기 차량이 상기 미디어 컨텐츠의 해당 위치에 소정 거리 이내로 접근하는 것으 로 파악되거나, 상기 주행 영상을 통해 상기 해당 지형 또는 지물이 인식되면, 상기 미디어 컨텐츠를 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭시킬 수 있다. 상기 미디어 컨텐츠는 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭되도록 상기 주행 영상과 함께 디 스플레이부를 통해 디스플레이될 수 있다. 한편, 상기 클라우드 정책 매니저에 의해 상기 프로세싱 위치가 상기 차량 및 상기 클라우드 서버 로 정해질 수도 있다. 이 경우에는, 상기 클라우드 정책 매니저는 상기 프로세싱 위치가 상기 차량 및 상기 클라우드 서버로 정해졌음을 알리는 제어 신호를 상기 사이니지 데이터 매니저, 상기 클라우드 사이니지 데이터 프로세서, 및 상기 에지 사이니지 데이터 프로세서로 전송할 수 있다. 상기 제어 신호에 반응하여, 상기 사이니지 데이터 매니저는 상기 추출된 제 1 사이니지 관련 정보를 상기 클라우드 사이니지 데이터 프로세서로 전송할 수 있다. 그리고, 상기 클라우드 정책 매니저는 상기 디스플레이 정책을 상기 클라우드 사이니지 데이터 프로세서 로 전송할 수 있다. 상기 클라우드 사이니지 데이터 프로세서의 상기 사이니지 데이터 필터링 모듈은 상기 디스플레이 정 책에 기반하여 제 1 사이니지 관련 정보 중에서 상기 사용자가 관심 있을 것으로 추론되는 제 2 사이니지 관련 정보를 필터링할 수 있다. 상기 클라우드 사이니지 데이터 프로세서는 상기 필터링된 제 2 사이니지 관련 정보를 상기 에지 사이니지 데이터 프로세서로 전송할 수 있다. 상기 에지 사이니지 데이터 프로세서의 상기 미디어 컨텐츠 프로세싱 모듈는 상기 필터링된 제 2 사이니지 관련 정보를 AR 그래픽 데이터와 같은 미디어 컨텐츠로 변환하고, 상기 변환된 미디어 컨텐츠에 상기 미디어 컨텐츠가 디스플레이될 위치에 관한 디스플레이 위치 정보를 태깅할 수 있다. 예를 들면, 제 2 사이니지 관련 정보 중에서 상기 사이니지 디스플레이 정보가 상기 AR 그래픽 데이터로 변환될 수 있고, 이 때 상기 사이 니지 디스플레이 정보는 상기 사이니지 위치 정보가 고려되어 상기 AR 그래픽 데이터가 디스플레이될 빌딩 등과 같은 지형 또는 지물에 부합하도록 변환될 수 있다. 상기 에지 사이니지 데이터 프로세서의 상기 미디어 컨텐츠 프로세싱 모듈은 상기 디스플레이 위치 정보가 태깅된 상기 미디어 컨텐츠를 상기 랜더링 모듈로 전송할 수 있다. 상기 랜더링 모듈는 상기 차량이 상기 미디어 컨텐츠의 해당 위치에 소정 거리 이내로 접근하는 것으 로 파악되거나, 상기 주행 영상을 통해 상기 해당 지형 또는 지물이 인식되면, 상기 미디어 컨텐츠를 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭시킬 수 있다. 상기 미디어 컨텐츠는 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭되도록 상기 주행 영상과 함께 디 스플레이부를 통해 디스플레이될 수 있다. 한편, 상기 차량은 에지 정책 매니저를 포함할 수도 있다. 상기 에지 정책 매니저는 상기 클 라우드 정책 매니저와 유사한 역할을 하는 것으로서, 예를 들면, 상기 차량이 상기 클라우드 서버과 통신이 원활하지 않을 때, 또는 상기 차량이 동일 경로를 반복적으로 운행할 때 상기 클라우드 정 책 매니저의 역할을 맡을 수 있다. 예를 들면, 상기 운행 컨텍스트 데이터 매니저는 상기 차량이 상기 클라우드 서버과 통신이 원 활하지 않을 때, 또는 상기 차량이 동일 경로를 반복적으로 운행할 때, 상기 에지 정책 매니저가 상 기 클라우드 정책 매니저의 역할을 맡도록 하는 제어 신호를 상기 에지 정책 매니저로 전송할 수 있 다. 상기 제어 신호에 반응하여, 상기 에지 정책 매니저는 상기 운행 컨텍스트 데이터 매니저로부터 수 신하는 상기 운행 컨텍스트 데이터 및 상기 차량 내의 상기 메모리 (예를 들면, 캐시 메모리 (이에 한정되지 않음)) 내에 기저장되어 있는 제 2 사이니지 관련 정보 또는 상기 미디어 컨텐츠를 상기 에지 사이니 지 데이터 프로세서로 전송할 수 있다. 또는, 상기 에지 정책 매니저는 상기 에지 사이니지 데이터 프로세서가 직접 상기 메모리에 접근하여 제 2 사이니지 관련 정보를 독출하도록 제어할 수 있다. 또한, 상기 에지 정책 매니저는 상기 운행 컨텍스트 데이터(특히, 상기 사용자의 프로파일)에 기반하여 상기 차량을 통해 디스플레이될 사이니지 데이터의 디스플레이 정책을 도출하고 상기 사이니지 데이터 프 로세서로 전송할 수 있다. 상기 메모리에 저장되어 있는 제 2 사이니지 관련 정보를 활용하는 경우, 상기 사이니지 데이터 프로세서 은 상기 디스플레이 정책에 부합하는 제 2 사이니지 관련 정보를 미디어 컨텐츠로 변환하여 상기 랜더링 모듈로 전송할 수 있다. 또는 상기 메모리에 저장되어 있는 제 2 사이니지 관련 정보를 활용하는 경 우, 상기 사이니지 데이터 프로세서은 상기 디스플레이 정책에 부합하는 미디어 컨텐츠를 상기 랜더링 모 듈로 전송할 수 있다. 상기 랜더링 모듈로 전송되는 미디어 컨텐츠는 상기 디스플레이 위치 정보가 태깅된 것이다. 상기 랜더링 모듈는 상기 차량이 상기 미디어 컨텐츠의 해당 위치에 소정 거리 이내로 접근하는 것으 로 파악되거나, 상기 주행 영상을 통해 상기 해당 지형 또는 지물이 인식되면, 상기 미디어 컨텐츠를 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭시킬 수 있다. 상기 미디어 컨텐츠는 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭되도록 상기 주행 영상과 함께 디 스플레이부를 통해 디스플레이될 수 있다. 상기 에지 정책 매니저, 상기 에지 사이니지 데이터 프로세서, 상기 랜더링 모듈, 및 상기 디 스플레이부는 상기 차량에 구비되는 상기 차량용 디스플레이 장치를 구성하는 것으로 이해될 수 있다. 상기 운행 컨텍스트 데이터 매니저는 상기 차량용 디스플레이 장치에 속하는 구성요소로 이해될 수 도 있고, 그렇지 않을 수도 있다. 이하, 도 6을 더욱 참조하여, 상기 운행 컨텍스트 데이터에 대해 좀더 구체적으로 살펴보겠다. 도 6은 본 개시 의 일실시예에 따른 에지 데이터 및 이로부터 추출되는 운행 컨텍스트 데이터를 도시한다. 상기 운행 컨텍스트 데이터 매니저는 상기 운행 컨텍스트 데이터를 추출하기 위한 에지 데이터는 다음과 같을 수 있다. 먼저, 상기 에지 데이터로서 사용자의 정보가 활용될 수 있다. 상기 사용자의 정보는 사용자의 성별, 연령, 거 주지 등 본 개시의 서비스 가입을 위해 등록되는 개인 정보를 포함할 수 있다. 상기 에지 데이터로서 지도 또는 내비게이션 안내 정보가 활용될 수 있다. 상기 지도 또는 내비게이션 안내 정 보는 i) 목적지 및/또는 경유지 정보, ii) 목적지 및/또는 경유지까지의 내비이게션 경로 정보, iii) 목적지 및 /또는 경유지의 안내 정보(turn-by-turn), iv) 적어도 하나의 지도 정보 및 그 속성, v) 지도 내 도로 정보 (예 를 들면, 종별/속성, 도로 및 차선 폭, 곡률, 경사도, 제한속도 등)를 포함할 수 있다. 상기 에지 데이터로서 다이내믹 (또는 실시간) 정보가 활용될 수 있다. 상기 다이내믹 정보는 i) 교통 트래픽 정보 (도로 단위 트래픽 정보, 차선 단위 트래픽 정보), ii) 사고, 위험 경고와 같은 이벤트 정보, iii) 날씨 정보, 및 iv) 현재 위치 주변에서 사용되었거나 사용 가능한 쿠폰 정보 (다른 사용자의 쿠폰 정보 포함), iv) 통신 환경 상황 정보(예를 들면, 다른 차량 또는 클라우드 서버와의 통신 트래픽 및/또는 통신 가능 유무)를 포 함할 수 있다. 상기 에지 데이터로서 차량 센서 정보가 활용될 수 있다. 상기 차량 센서 정보는 i) 현재 위치 정보 (GPS 정보 및 DR(Deduced Reckoning)-GPS 정보 포함), ii) 카메라 입력 정보 (ADAS 정보 및 객체 인식 정보 포함), iii) V2X 정보 (V2V와 V2I를 통해 수집 가능한 실시간 주변 상황 정보 포함), iv) 잔여 주유 또는 충전량 및 소모품 교환 시기 정보 등을 포함할 수 있다. 상기 에지 데이터로서 사용자 프로파일이 활용될 수 있다. 상기 사용자 프로파일은 i) 과거/최근 주행 경로, ii) 사용자가 등록한 선호 브랜드, 관심 카테고리 정보 (등록된 해시태그 등), iii) 과거/최근 목적지, 경유지, 검색지, 등록지점 정보, iv) 주요 활동 거점 지역 정보, v) 다운로드 받은 쿠폰 사용 히스토리 정보를 포함할 수 있다. 상기 에지 데이터로서 기타 주행 관련 정보가 활용될 수 있다. 상기 기타 주행 관련 정보는 i) 주행 모드 (수동, 자율주행, 반자율주행, ADAS 기능 작동 여부 등), 및 ii) 일렉트로닉 호라이즌(electronic Horizon) 정 보 (전방 주행 가능 경로 정보)를 포함할 수 있다. 상기 운행 컨텍스트 데이터 매니저가 상기 에지 데이터로부터 추출할 수 있는 운행 컨텍스트 데이터는, i) 예상 주행 경로 및 경로상 사이니지 노출 가능 위치 및/또는 정보 등과 같은 주행 경로 관련 데이터, ii) 주행 구간에 대한 안전 등급과 같은 안전운행 관련 데이터, iii) 상기 차량에서 실시간으로 연산 처리 가능한 리소스 정보와 같은 연산 자원량 정보, iv) 사이니지 표시 가능 영역 정보, v) 통신 환경 정보, 및 vi) 사용자 정보 및 프로파일 중 적어도 하나를 포함할 수 있다. 이하, 도 7을 참조하여, 본 개시의 일실시예에 따른 에지 디바이스 (즉 차량 및 클라우드 서버 간의 협업을 위한 방법에 대해 살펴 보겠다. 도 7은 본 개시의 일실시예에 따른 에지 디바이스 및 클라우드 서버 간 의 협업 방법에 관한 순서도이다. 먼저, 상기 차량의 상기 운행 컨텍스트 데이터 매니저는 상기 차량과 관련된 각종 에지 데이터 를 수집하여 운행 컨텍스트 정보를 추출할 수 있다[S71]. 상기 수집되는 에지 데이터 및 상기 추출되는 운행 컨 텍스트 정보의 예시는 도 6과 관련하여 전술한 바와 같다. 상기 운행 컨텍스트 정보는 상기 클라우드 정책 매니저를 경유하여 상기 사이니지 데이터 매니저로 전송될 수 있다. 상기 사이니지 데이터 매니저는 적어도 하나의 외부 데이터 소스로부터 각종 사이니지 관련 정보를 취합하 고, 상기 운행 컨텍스트 데이터에 기반하여 상기 취합된 사이니지 관련 정보로부터 상기 차량에 적합한 제 1 사이니지 관련 정보를 추출할 수 있다[S72]. 상기 차량에 적합한 제 1 사이니지 관련 정보는, 예를 들 면 상기 취합된 사이니지 관련 정보 중에서 상기 차량의 현재 위치 및/또는 예상 이동 경로에 해당하는 위 치에 해당하는 정보(이에 한정되지 않음)일 수 있다. 그 다음, 상기 클라우드 정책 매니저는 상기 운행 컨텍스트 데이터와 제 1 사이니지 관련 정보에 기반하여, 제 1 사이니지 관련 정보가 상기 차량 및 상기 클라우드 서버 중 어디에서 프로세싱되는 것이 적절한 것인지, 즉 프로세싱 위치에 대해 판단할 수 있다. 이는 상기 클라우드 서버 및 상기 에지 디 바이스 (즉, 상기 차량) 간의 데이터 프로세싱 분배를 추론하는 것으로 이해될 수 있다[S73]. 상기 프로세싱 위치가 상기 클라우드 서버로 결정되는 경우에는 제 1 사이니지 관련 정보가 상기 클라우드 사이니지 데이터 프로세서로 전달되어 데이터 가공될 수 있다[S74]. 즉, 상기 클라우드 정책 매니저 에 의해 정해지는 상기 디스플레이 정책에 기반하여, 상기 클라우드 사이니지 데이터 프로세서에 의해 제 1 사이니지 관련 정보 중에서 상기 사용자가 관심 있을 것으로 추론되는 제 2 사이니지 관련 정보가 필터링될 수 있다. 또한, 제 2 사이니지 관련 정보가 AR 그래픽 데이터와 같은 미디어 컨텐츠로 변환되고, 상기 변환된 미디어 컨텐츠에 상기 미디어 컨텐츠가 디스플레이될 위치에 관한 디스플레이 위치 정보가 태깅될 수 있다. 상기 프로세싱 위치가 상기 차량로 결정되는 경우에는 제 1 사이니지 관련 정보가 상기 에지 사이니지 데 이터 프로세서로 전달되어 데이터 가공될 수 있다[S75]. 즉, 상기 클라우드 정책 매니저에 의해 정 해지는 상기 디스플레이 정책에 기반하여, 상기 에지 사이니지 데이터 프로세서에 의해 제 1 사이니지 관 련 정보 중에서 상기 사용자가 관심 있을 것으로 추론되는 제 2 사이니지 관련 정보가 필터링될 수 있다. 또한, 제 2 사이니지 관련 정보가 AR 그래픽 데이터와 같은 미디어 컨텐츠로 변환되고, 상기 변환된 미디어 컨텐츠에 상기 미디어 컨텐츠가 디스플레이될 위치에 관한 디스플레이 위치 정보가 태깅될 수 있다. 상기 프로세싱 위치가 상기 차량 및 상기 클라우드 서버로 결정되는 경우에는 제 1 사이니지 관련 정 보가 상기 클라우드 사이니지 데이터 프로세서로 전달되어 데이터 가공될 수 있다[S74]. 즉, 상기 클라우드 정책 매니저에 의해 정해지는 상기 디스플레이 정책에 기반하여, 상기 클라우드 사이니지 데이터 프로 세서에 의해 제 1 사이니지 관련 정보 중에서 상기 사용자가 관심 있을 것으로 추론되는 제 2 사이니지 관 련 정보가 필터링될 수 있다. 상기 클라우드 사이니지 데이터 프로세서에 의해 필터링된 제 2 사이니지 관련 정보는 상기 에지 사이니지 데이터 프로세서로 전달되어 데이터 가공될 수 있다[S75]. 즉, 상기 에지 사이니지 데이터 프로세서 에 의해 제 2 사이니지 관련 정보가 AR 그래픽 데이터와 같은 미디어 컨텐츠로 변환되고, 상기 변환된 미 디어 컨텐츠에 상기 미디어 컨텐츠가 디스플레이될 위치에 관한 디스플레이 위치 정보가 태깅될 수 있다. 상기 디스플레이 위치 정보가 태깅된 상기 미디어 컨텐츠는 상기 차량의 상기 랜더링 모듈로 전송되 어, 상기 차량이 상기 미디어 컨텐츠의 해당 위치에 소정 거리 이내로 접근하는 것으로 파악되거나, 상기 주행 영상을 통해 상기 해당 지형 또는 지물이 인식되면, 상기 미디어 컨텐츠가 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭되도록 랜더링될 수 있다[S76]. 상기 미디어 컨텐츠는 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭되도록 상기 주행 영상과 함께 상 기 디스플레이부를 통해 디스플레이될 수 있다[S77]. 한편, 상기 차량에서의 사이니지 관련 정보의 프로세싱 상황은 상기 클라우드 정책 매니저로 피드백 될 수 있다[S78]. 이는 상기 차량에서 이미 프로세싱되어 상기 메모리 (예를 들면, 캐시 메모리(이에 한정되지 않음))에 저장되어 있는 사이니지 관련 정보가 또다시 프로세싱되는 것을 방지하기 위한 것으로서, 예 를 들어 상기 에지 사이니지 데이터 프로세서에서의 추론 결과 및/또는 제 2 사이니지 관련 정보가 상기 클라우드 정책 매니저로 피드백될 수 있다. 비록 도 7에서 도시되지는 않았지만, 상기 S76 단계에서의 상 기 랜더링된 미디어 컨텐츠 또는 상기 S77 단계에서 디스플레이되는 미디어 컨텐츠가 중복 프로세싱을 방지하기 위해 상기 클라우드 정책 매니저로 피드백될 수도 있다. 상기 클라우드 정책 매니저는 상기 프로세싱 위치를 판단함에 있어 상기 피드백되는 정보 또는 데이터를 참조할 수 있다. 예를 들어, 상기 피드백되는 정보 또는 데이터가 새롭게 프로세싱되어야 할 사이니지 관련 정 보와 중복되는 부분이 일정 수준 이상이 된다면, 상기 클라우드 정책 매니저는 상기 프로세싱 위치를 상기 차량으로 정하고, 상기 차량은 상기 정보 또는 데이터를 재활용함으로써, 데이터 가공이 중복적으로 수행되지 않도록 할 수 있다. 이하, 도 8 내지 도 10을 참조하여, 상기 AR 그래픽 데이터와 같은 미디어 컨텐츠가 상기 디스플레이부를 통해 디스플레이되는 것에 대해 설명하겠다. 도 8은 본 개시의 일실시예에 따른 에지 디바이스 및 클라우드 서 버 간의 협업 시스템을 통해 프로세싱된 미디어 컨텐츠가 디스플레이되는 순서도이고, 도 9 및 도 10은 본 개시 의 일실시예에 따른 에지 디바이스 및 클라우드 서버 간의 협업 시스템을 통해 프로세싱된 미디어 컨텐츠가 디 스플레이되는 예시를 도시한다. 본 개시에서 언급되는 본 명세서에 개시된 \"맵 정보\"는 카메라 등과 같은 비전센서를 통해 촬영된 영상, 2차원 지도정보, 3차원지도정보, 디지털 트윈(Digital Twin) 3차원 맵, 실제/가상 공간상의 지도정보를 포함하는 의미 로 지칭될 수 있다. 본 개시에서 언급되는 \"POI(Point of Interest, POI) 정보\"는 상기 맵 정보를 기초로 선택된 관심지점으로, 기 등록된 POI 정보(클라우드 서버의 맵 지도에 저장된 POI), 사용자 설정 POI 정보(예, 우리집, 학교, 회사 등), 주행 관련 POI 정보(예, 목적지, 경유지, 주유소, 휴게소, 주차장 등), 및 상위 검색 POI 정보(예, 최근 클릿/ 방문수가 많은 POI, 핫 플레이스 등)를 포함할 수 있다. 이러한 POI 정보는, 상기 차량의 현재 위치를 기 준으로 실시간 업데이트될 수 있다. 본 개시에서 언급되는 \"주행 영상\"은 상기 차량 또는 상기 차량 주변의 비전센서를 통해 획득된 것으 로, 예를 들어 차량의 주행 동안 비전센서(외부 카메라, 영상용 레이저센서 등)를 통해 획득되거나 투사되는 영 상, 차량의 윈드쉴드에 투사되는 현실의 이미지 자체/가상공간의 영상을 포함할 수 있다. 즉, 상기 주행 영상은 디스플레이를 통해 출력되는 영상, 레이저 센서 등을 통해 투사되는 영상, 또는 차량의 윈드쉴드를 통해 보여지 는 현실의 이미지 자체를 모두 포함하는 의미로 지칭될 수 있다. 상기 차량의 상기 랜더링 모듈는 상기 맵 정보로부터 적어도 복수의 POI 정보를 포함하는 빌딩영역에 대한 공간좌표를 산출하여, 복수의 POI 정보 각각에 대한 층수 정보를 획득할 수 있다[S81]. 즉, 상기 맵 정보 로부터 복수의 POI를 포함하는 빌딩이 검출/인지되면, 그에 대한 x, y, z 좌표정보가 공간좌표로서 산출될 수있다. 상기 맵 정보가 상기 클라우드 서버로부터 수신된 2차원/3차원 지도정보인 경우, 복수의 POI 정보를 포함 하는 빌딩영역의 위치, 높이, 층수에 관한 정보를 기초로 클라우드 서버로부터 수신하여 공간좌표를 산출할 수 있다. 또는, 상기 맵 정보가 차량에 구비된 카메라과 같은 비전 센서를 통해 획득된 주행 영상인 경우, 주행 영상을 기초로 빌딩영역의 위치, 높이, 층수가 추정되고, 이로부터 공간좌표가 추정될 수 있다. 이와 같이, 빌딩영역에 대한 공간좌표가 산출되면, 빌딩영역에 포함된 복수의 POI 정보 각각에 대한 층수 정보 가 산출/추정할 수 있다. 예를 들어, 빌딩영역에 대한 공간좌표에 포함된 빌딩의 높이 및 층수를 기초로 층별 높이값, 즉 높이 오프셋(offset)이 산출될 수 있고, 이를 기초로 복수의 POI 정보에 대응되는 각 층수의 계산하 거나 추정될 수 있다. 상기 렌더링 모듈은 상기 차량의 센싱 데이터와 빌딩영역의 공간좌표 및 층수 정보에 기초하여, 복수 의 POI 정보와 관련된 AR 사이니지 그래픽 데이터와 같은 미디어 컨텐츠를 표시할 기준점을 산출할 수 있다 [S82]. 상기 미디어 컨텐츠는, 예를 들어 POI와 관련된 광고 노출을 위한 브랜드 아이콘, 3D 모델, 스틸 이미지, 및 동영상 이미지 등을 의미할 수 있다. 상기 미디어 컨텐츠 정보를 표시할 빌딩영역의 기준점은 차량 센싱 데이터(차량 주행 속도 및 방향)에 관한 정 보와 빌딩영역의 공간좌표 및 각 POI 정보의 층수정보를 주행 영상에 대응되게 매칭시킴으로써, 산출될 수 있다. 상기 기준점은 빌딩영역의 공간 좌표 중 차량의 주행 방향을 기초로 현재 위치에서 가까운 좌표 지점으로 결정 될 수 있다. 상기 복수의 POI 정보에 대한 각 표시영역은, 상기 기준점을 기준으로 복수의 POI 정보 각각에 대응되는 층수정 보에 높이 오프셋(offset)을 적용시킨 것으로, 상기 기준점으로부터 해당 층수 + 높이 오프셋(offset)만큼 이동 한 위치로 설정될 수 있다. 여기에서, 상기 차량의 현재 위치에서 가까운 좌표 지점이란, 맵 정보에 포함된 좌표 데이터나 차량에 구 비된 카메라 센싱 정보를 이용하여 파악되는 빌딩의 에지(edge) 검출을 통해 획득되거나 또는 추정될 수 있다. 이러한 경우, 복수의 POI 정보에 대응되는 각 미디어 컨텐츠에 대한 기준점(이하, '층별 기준점')은, 상기 기준 점 + 각 POI 정보에 매칭되는 층수만큼의 높이 오프셋(off)으로 설정될 수 있다. 즉, 복수의 미디어 컨텐츠 각 각에 대해 서로 다른 층별 기준점이 산출 및 적용될 수 있다. 상기 랜더링 모듈는 상기 산출된 기준점에 기초하여 결정된 표시영역에, 상기 미디어 컨텐츠에 대응되는 AR 사이니지 그래픽 데이터를 표시하기 위한 랜더링을 수행할 수 있다[S83]. 상기 랜더링 모듈는 상기 복수의 POI 정보에 매칭되는 복수의 AR 사이니지 그래픽 데이터를 빌딩의 층별로 매핑하기 위한 표시영역을 각각 결정할 수 있다. 구체적으로, 빌딩영역에 대한 공간좌표에 기초하여 산출된 빌딩영역의 높이 정보와 복수의 POI 정보 각각에 매 칭되는 층수정보를 이용하여, 복수의 POI 정보에 대응되는 복수의 AR 사이니지 그래픽 데이터들의 표시영역이 각각 결정될 수 있다. 상기 맵 정보에 기초하여 빌딩영역에 대한 공간좌표를 획득할 수 없으면, 상기 랜더링 모듈는, 상기 차량 의 센싱 데이터에 포함된 카메라 센싱 정보를 이용하여 추정된 상기 빌딩영역의 높이 정보와 층수 정보를 이용하여, 미디어 컨텐츠 정보를 매핑할 표시영역을 결정할 수 있다. 상기 랜더링 결과는 상기 주행 영상 내 빌딩의 표시 영역에 층별 매칭되도록 출력되어 상기 디스플레이부 에 디스플레이될 수 있다[S84]. 이하, 도 9 및 도 10을 참조하여 자세히 설명하겠다. 도 9를 참조하면, 복수의 POI 정보를 포함하는 빌딩영역이 검출되면, 맵 정보(예, 2차원/3차원 지도정보, 빌딩영역의 층수, 높이정보 등)나 상기 차량에 구비된 외부 카메라와 같은 비전 센서를 통해 획득된 주행 영상에 기초하여 빌딩영역에 대한 공간좌표(1211, 1212, 1213, P)가 산출(또는, 추정)될 수 있다. 상기 차량의 센싱 데이터(예, 차량 주행 방향)와 상기 공간좌표(1211, 1212, 1213, P) 및 맵 정보에 포함 된(또는, 주행영상의 에지(edge) 검출을 통해 추정된) 빌딩의 높이 및 층수 정보를 이용함으로써, 상기 주행 영 상에 AR 사이니지 그래픽 데이터를 층별로 매핑하기 위한 원점(P)이 추출될 수 있다. 즉, 빌딩영역의 공간좌표(1211, 1212, 1213, P) 중에서 상기 차량의 주행 방향을 고려하여, 상기 차 량의 센싱 데이터에 포함된 상기 차량의 위치에서 가장 가까운 기준좌표가 원점(P)으로 설정될 수 있 다. 예를 들어, 상기 맵 정보로부터 빌딩영역의 층수 및 높이에 관한 정보가 획득될 수 있다면, 상기 차량 의 주행 방향을 고려하여 원점(P)이 계산될 수 있다. 다만, 상기 빌딩영역에 대한 맵 정보가 존재하 지 않거나(또는, 인식되지 않는 경우) 또는 층수 및 높이에 관한 정보를 획득할 수 없는 경우라면, 상기 차량 에 구비된 상기 비전 센서의 센싱 정보를 이용한 빌딩의 이미지로부터 빌딩의 높이와 층수가 추정되고, 이 미지의 에지(edge)를 검출하여 빌딩영역에 대한 원점이 추출될 수 있다. 이때, 층별 높이는 미리정의된 값을 사 용할 수 있을 것이다. 한편, 복수의 AR 사이니지 그래픽 데이터가 층별로 표시되기 위해서는, 복수의 AR 사이니지 그래픽 데이터 각각 에 대한 '층별 기준점'이 산출될 필요가 있다.. 이와 관련하여, 도 10을 참조하면, 상기 복수의 AR 사이니지 그래픽 데이터에 대한 '층별 기준점'은 빌딩영역의 공간좌표(1211, 1212, 1213, P)와 원점(P)을 기준으로 해당 POI 정보의 층수 만큼 높이를 이동시킨 위치가 된다. 즉, 원점(P) + 각 POI 정보에 매칭되는 층수만큼의 높이 오프셋(off)을 적용한 위치가 '층별 기준점'으 로 결정하고, 여기에 대응 AR 사이니지 그래픽 데이터가 매핑될 수 있다. 도 10에서, 빌딩영역의 1층에 표시되는 제1 AR 사이니지 그래픽 데이터의 기준점은 원점(P)으로 결 정될 수 있다. 빌딩영역의 2층에 표시되는 제2 AR 사이니지 그래픽 데이터의 층별 기준점은 원점 (P)에서 2층만큼의 높이 오프셋이 적용된 지점(P1)으로 결정될 수 있다. 그리고, 빌딩영역의 3층 및 4층 에 각각 표시되는 제3 및 제4 AR 사이니지 그래픽 데이터(1303, 1304)의 층별 기준점들은 원점(P)에서 각각 3층 및 4층만큼의 높이 오프셋을 적용시킨 각 지점(P2, P3)으로 결정될 수 있다. 빌딩영역의 높이 정보를 보다 정확하게 산출하기 위해, 위와 같이 층별 기준점을 기초로 결정된 각 표시영역의 높이정보(예, 층간 높이 오프셋)와 상기 맵 정보(예, 2차원/3차원 지도정보)에 포함된 높이 데이터를 비교하고, 비교 결과에 근거하여, 각 표시영역의 위치 및 형상 중 적어도 하나가 보정될 수 있다. 또한, 초기에 계산한 층 간 높이 셋의 값과 높이정보가 상기 데이터베이스를 통해 획득한 빌딩영역의 높이 정보가 임계값(통상의 오차범위) 이상의 차이가 검출되면, 상기 데이터베이스의 높이 정보를 이용하여 각 표시영역의 위치를 보 정할 수 있다. 예를 들어, 도 10에서 설명한 층간 높이 오프셋이 변경될 수 있다. 또는, 예를 들어, 도 10에서 빌딩영역(120 0)의 전면이 아닌 측면에 복수의 AR 사이니지 그래픽 데이터들(1301, 1302, 1303, 1304)이 층별로 매핑될 수 있 다. 한편, 빌딩영역에 층별 매핑되는 복수의 AR 사이니지 그래픽 데이터들(1301, 1302, 1303, 1304)은 주행영 상 내 빌딩의 각 표시영역의 적어도 일 면을 감싸도록 매핑될 수 있다. 예를 들어, 도 10에서, 복수의 AR 사이 니지 그래픽 데이터들(1301, 1302, 1303, 1304)은 각 층별로 빌딩영역의 전면을 감싸도록 형성되었다. 그 에 따라, 하나의 빌딩에 층별로 복수의 광고 노출 효과를 제공하며, 실제 사이니지 전광판이 올려져 있는 것과 같은 사실감이 부여될 수 있다. 이와 같은 방식으로 상기 주행 영상 상에 적어도 하나의 AR 사이니지 그래픽 데이터가 디스플레이되는 것에 대 해 도 11을 더욱 참조하여 설명하겠다. 도 11은 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스 플레이되는 주행 영상을 도시한다. 도 11에 도시된 바와 같이, 상기 주행 영상이 상기 디스플레이부를 통해 출력되거나, 레이저 센서 등을 통해 투사되거나, 상기 차량의 윈드 쉴드를 통해 보여질 수 있다. 이 때, 적어도 하나의 AR 사이니지 그래픽 데이터가 상기 주행 영상 상에 해당 위치 또는 해당 지형 또는 지물에 매칭되도록 오버레이되어 디스플레이될 수 있다. 도 11에서는 복수의 AR 사이니지 그래픽 데이터(2010, 2020, 2025, 2030, 2035, 2040, 2050)가 디스플레이되는 것이 예시되어 있다. 상기 적어도 하나의 AR 사이니지 그래픽 데이터는 광고 사이니지(2010, 2020, 2030), 쿠폰 사이니지(2025, 2035), 주행 안내 사이니지(2040, 2050) 등을 포함할 수 있다. 상기 쿠폰 사이니지(2025, 2035)는 해당 광고 사이니지(2025, 2035))와 함께 디스플레이될 수 있다. 상기 복수의 AR 사이니지 그래픽 데이터(2010, 2020, 2025, 2030, 2035, 2040, 2050) 중에서 예를 들면 일부 (2010, 2020, 2025, 2030, 2035)의 프로세싱 위치는 상기 클라우드 서버이고, 나머지(2040, 2050)의 프로 세싱 위치는 상기 차량 일 수 있다. 상기 주행 영상에서는 프로세싱 위치가 상기 클라우드 서버인 AR 사이니지 그래픽 데이터만이 디스 플레이될 수도 있고, 프로세싱 위치가 상기 차량인 AR 사이니지 그래픽 데이터만이 디스플레이될 수 있고, 이들이 서로 혼재되어 디스플레이될 수 있다. 이하, 상기 프로세싱 위치가 결정되는 로직에 대해 설명하겠다. 먼저, 도 12 및 도 13를 참조하여, 상기 프로세싱 위치가 결정되는 로직에 대해 설명하겠다. 도 12는 본 개시의 일실시예에 따라 프로세싱 위치가 결정되는 로직을 설명하는 순서도이고, 도 13은 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스플레이되는 주행 영상을 도시한다. 앞서 설명한 바와 같이, 상기 차량은 운행 또는 정지 시에 각종 에지 데이터를 수집할 수 있다. 상기 차량에서 수집된 에지 데이터는 상기 운행 컨텍스트 데이터로 변환되어 상기 클라우드 서버로 제공될 수 있다. 한편, 상기 클라우드 서버의 상기 사이니지 데이터 매니저는 적어도 하나의 외부 데이터 소스로부터 각종 사이니지 관련 정보를 취합할 수 있다. 그리고, 상기 사이니지 데이터 매니저는 상기 운행 컨텍스트 데이터에 기반하여 상기 차량의 현재 위치 및/또는 예상 이동 경로에 해당하는 위치에 해당하는 여부에 부 합하는지 여부 등을 고려한 상기 차량에 적절한 사이니지 관련 정보, 즉 제 1 사이니지 관련 정보를 추출 하고, 상기 추출된 제 1 사이니지 관련 정보를 상기 클라우드 정책 매니저로 제공할 수 있다. 그러면, 상기 클라우드 정책 매니저는 상기 운행 컨텍스트 데이터와 상기 추출된 제 1 사이니지 관련 정보 중 적어도 하나에 기반하여, 상기 추출된 제 1 사이니지 관련 정보가 상기 차량 및 상기 클라우드 서버 중 어디에서 프로세싱되는 것이 적절한 것인지, 즉 프로세싱 위치에 대해 판단할 수 있다. 먼저, 제 1 사이니지 관련 정보의 성격에 따라서 상기 클라우드 정책 매니저가 제 1 사이니지 관련 정보의 프로세싱 위치를 정할 수 있다[S71]. 예를 들면, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 정함에 있어서 제 1 사이니지 관련 정보의 변동 가능성을 고려할 수 있다[S121]. 여기서 제 1 사이니지 관련 정보의 상기\"정보 변동 가능성\"라 함은, 제 1 사이니지 관련 정보가 소정 단위 시간 이내에 변동할 가능성을 의미할 수 있다. 예를 들면, 제 1 사이니지 관련 정보가 특정 브랜드에 관한 광고 (또는 쿠폰) 사이니지인 경우, 소정 단위 시간 (예를 들면, 30초) 이내에 상기 광고의 컨텐츠가 변동할 가능성이 있는지 여부 또는 상기 광고가 다른 광고로 변경될 가능성이 있는지 여부로 상기 정보 변동 가능성이 결정될 수 있다. 또는, 제 1 사이니지 관련 정보가 상기 사이니지 위치 정보에 관한 경우, 소정 단위 시간 이내에 상기 사이니지 위치 정보가 변동할 가능성이 있는지 여부에 기반하여 상기 정보 변경 가능성이 결정될 수 있다. 상기 정보 변경 가능성이 높은 경우, 도 13의 (13-1)에 도시된 바와 같이 상기 주행 영상과 함께 디스플 레이되는 제 1 사이니지 관련 정보에 해당하는 제 1 AR 사이니지 그래픽 데이터(2100-1)가 상기 소정 시간 이내 에 도 13의 (13-2)에 도시된 바와 같이 제 2 AR 사이니지 그래픽 데이터(2100-2)로 변경될 수 있다. 이 경우, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 상기 클라우드 서버 로 정할 수 있다[S122]. 상기 정보 변경 가능성이 높은 경우 상기 프로세싱 위치를 상기 클라우드 서버 로 정함으로써 제 1 사이니지 관련 정보의 정보 변경 가능성을 좀더 신속하고 효율적으로 대응하기 위한 것이다. 한편, 상기 정보 변경 가능성이 낮은 경우, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로 세싱 위치를 상기 차량로 정할 수 있다[S123]. 상기 정보 변경 가능성이 낮은 경우 상기 프로세싱 위치를 상기 차량으로 정함으로써, 향후 상기 클라우드 서버와 상기 차량 간의 통신이 일시적으로 불안 정해지더라도 제 1 사이니지 관련 정보에 해당하는 AR 사이니지 그래픽 데이터가 안정적으로 디스플레이될 수있도록 하기 위한 것이다. 상기 클라우드 정책 매니저에 의해 상기 프로세싱 위치가 상기 클라우드 서버 또는 상기 차량으 로 정해진 경우, 제 1 사이니지 관련 정보가 프로세싱되는 과정에 대해서는 앞서 설명되었으므로[S74, S75], 자 세한 설명은 생략하도록 하겠다. 이상에서 제 1 사이니지 관련 정보의 성격으로서 상기 변동 가능성을 고려하여, 상기 프로세싱 위치가 결정되는 로직에 대해 설명하였다. 그런데, 제 1 사이니지 관련 정보의 성격으로서 상기 변동 가능성 이외에 데이터 크기 를 고려하여, 상기 프로세싱 위치가 결정될 수도 있다. 이에 대해 도 14 및 도 15를 더욱 고려하여 설명하겠다. 도 14는 본 개시의 일실시예에 따라 프로세싱 위치가 결정되는 로직을 설명하는 순서도이고, 도 15은 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스플레이되는 주행 영상을 도시한다. 앞서 설명한 바와 같이, 상기 차량은 운행 또는 정지 시에 각종 에지 데이터를 수집할 수 있다. 상기 차량에서 수집된 에지 데이터는 상기 운행 컨텍스트 데이터로 변환되어 상기 클라우드 서버로 제공될 수 있다. 한편, 상기 클라우드 서버의 상기 사이니지 데이터 매니저는 적어도 하나의 외부 데이터 소스로부터 각종 사이니지 관련 정보를 취합할 수 있다. 그리고, 상기 사이니지 데이터 매니저는 상기 운행 컨텍스트 데이터에 기반하여 상기 차량의 현재 위치 및/또는 예상 이동 경로에 해당하는 위치에 해당하는 여부에 부 합하는지 여부 등을 고려한 상기 차량에 적절한 사이니지 관련 정보, 즉 제 1 사이니지 관련 정보를 추출 하고, 상기 추출된 제 1 사이니지 관련 정보를 상기 클라우드 정책 매니저로 제공할 수 있다. 그러면, 상기 클라우드 정책 매니저는 상기 운행 컨텍스트 데이터와 상기 추출된 제 1 사이니지 관련 정보 중 적어도 하나에 기반하여, 상기 추출된 제 1 사이니지 관련 정보가 상기 차량 및 상기 클라우드 서버 중 어디에서 프로세싱되는 것이 적절한 것인지, 즉 프로세싱 위치에 대해 판단할 수 있다. 먼저, 제 1 사이니지 관련 정보의 성격에 따라서 상기 클라우드 정책 매니저가 제 1 사이니지 관련 정보의 프로세싱 위치를 정할 수 있다[S71]. 예를 들면, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 정함에 있어서 제 1 사이니지 관련 정보의 데이터 크기를 고려할 수 있다[S141]. 제 1 사이니지 관련 정보의 상기 데이터 크기는, 제 1 사이니지 관련 정보를 처리함에 필요한 연산 리소스 양 및 프로세싱 복잡도와 비례할 수 있다. 제 1 사이니지 관련 정보의 데이터 크기는 제 1 사이니지 관련 정보의 데이터 종류 및/또는 데이터 개수에 따라 달라질 수 있다. 예를 들어 제 1 사이니지 관련 정보의 데이터의 종류가 3D인 경우 2D일 때보다 데이터 크기가 클 수 있다. 또한, 제 1 사이니지 관련 정보의 데이터(예를 들면, 사이니지 표시 개수)가 많을수록 데이터 크기가 클 수 있 다. 예를 들어, 도 15의 (15-1)의 제 1 사이니지 관련 정보의 데이터(2110 내지 2160)가 도 15의 (15-2)의 제 1 사이니지 관련 정보의 데이터(2120, 2160)보다 더 많을 수 있다. 제 1 사이니지 관련 정보의 상기 데이터 크기가 소정 값 이상인 경우, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 상기 클라우드 서버로 정할 수 있다[S142]. 상기 데이터 크기가 소 정 값 이상인 경우 상기 프로세싱 위치를 상기 클라우드 서버로 정함으로써 제 1 사이니지 관련 정보에 따 른 연산 리소스 및 프로세싱 복잡도를 상기 차량이 아닌 상기 클라우드 서버가 맡도록 하기 위한 것 이다. 한편, 제 1 사이니지 관련 정보의 상기 데이터 크기가 소정 값 미만인 경우, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 상기 차량로 정할 수 있다[S143]. 상기 데이터 크기가 소정 값 미만인 경우 상기 프로세싱 위치를 상기 차량으로 정함으로써, 향후 상기 클라우드 서버와 상기 차량 간의 통신이 일시적으로 불안정해지더라도 제 1 사이니지 관련 정보에 해당하는 AR 사이니지 그래픽 데이터가 안정적으로 디스플레이될 수 있도록 하기 위한 것이다. 상기 클라우드 정책 매니저에 의해 상기 프로세싱 위치가 상기 클라우드 서버 또는 상기 차량으 로 정해진 경우, 제 1 사이니지 관련 정보가 프로세싱되는 과정에 대해서는 앞서 설명되었으므로[S74, S75], 자세한 설명은 생략하도록 하겠다. 이상에서 제 1 사이니지 관련 정보의 성격을 고려하여, 상기 프로세싱 위치가 결정되는 로직에 대해 설명하였다. 그런데, 상기 운행 컨텍스트 정보에 기반하여 사이니지 표시 가능 영역을 고려하여, 상기 프로세싱 위치가 결정될 수도 있다. 이에 대해 도 16 및 도 17를 더욱 고려하여 설명하겠다. 도 16는 본 개시의 일실시예 에 따라 프로세싱 위치가 결정되는 로직을 설명하는 순서도이고, 도 17은 본 발명의 일실시예에 따라 AR 사이니 지 그래픽 데이터가 디스플레이되는 주행 영상을 도시한다. 상기 차량에서 수집된 에지 데이터는 상기 운행 컨텍스트 데이터로 변환되어 상기 클라우드 서버로 제공될 수 있다. 한편, 상기 클라우드 서버의 상기 사이니지 데이터 매니저는 적어도 하나의 외부 데이터 소스로부터 각종 사이니지 관련 정보를 취합할 수 있다. 그리고, 상기 사이니지 데이터 매니저는 상기 운행 컨텍스트 데이터에 기반하여 상기 차량의 현재 위치 및/또는 예상 이동 경로에 해당하는 위치에 해당하는 여부에 부 합하는지 여부 등을 고려한 상기 차량에 적절한 사이니지 관련 정보, 즉 제 1 사이니지 관련 정보를 추출 하고, 상기 추출된 제 1 사이니지 관련 정보를 상기 클라우드 정책 매니저로 제공할 수 있다. 그러면, 상기 클라우드 정책 매니저는 상기 운행 컨텍스트 데이터와 상기 추출된 제 1 사이니지 관련 정보 중 적어도 하나에 기반하여, 상기 추출된 제 1 사이니지 관련 정보가 상기 차량 및 상기 클라우드 서버 중 어디에서 프로세싱되는 것이 적절한 것인지, 즉 프로세싱 위치에 대해 판단할 수 있다. 상기 운행 컨텍스트 데이터에 따른 사이니지 표시 가능 영역에 따라서 상기 클라우드 정책 매니저가 제 1 사이니지 관련 정보의 프로세싱 위치를 정할 수 있다[S71, S161]. 예를 들면, 상기 차량이 도 17의 (17-1)에 도시된 바와 같이, 예컨대 도심을 운행하고 있는 경우 상기 사 이니지 표시 가능 영역이 특정 건물의 특정 층 벽면으로 정해질 수 있다. 상기 사이니지 표시 가능 영역이 특정 건물의 특정 층 벽면과 같이 소정 넓이 미만인 경우, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 상기 클라우드 서버로 정할 수 있다[S162]. 상기 사이니지 표시 가능 영역이 특정 건물의 특정 층 벽면과 같이 소정 넓이 미만인 경우 제 1 사이니지 관련 정보에 따른 연산 리소스를 많이 필요하거나 프로세싱 복잡도가 높을 수 있기 때문에, 상기 연산 리소스 및 상 기 프로세싱 복잡도를 상기 차량이 아닌 상기 클라우드 서버가 맡도록 하기 위한 것이다. 그러나, 상기 차량이 도 17의 (17-2)에 도시된 바와 같이, 예컨대 터널 속을 운행하고 있는 경우 상기 사 이니지 표시 가능 영역이 상기 터널의 사이드 벽면이라고 정해질 수 있다. 상기 사이니지 표시 가능 영역이 상기 터널의 사이드 벽면과 같이 소정 넓이 이상인 경우, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 상기 차량로 정할 수 있다[S163]. 상기 사이니 지 표시 가능 영역이 상기 터널의 사이드 벽면과 같이 소정 넓이 이상인 경우 제 1 사이니지 관련 정보에 따른 연산 리소스를 많이 필요하지 않거나 프로세싱 복잡도가 낮을 수 있기 때문에, 이를 상기 차량이 감당하더 라도 큰 문제가 없을 수 있기 때문이다. 상기 클라우드 정책 매니저에 의해 상기 프로세싱 위치가 상기 클라우드 서버 또는 상기 차량으 로 정해진 경우, 제 1 사이니지 관련 정보가 프로세싱되는 과정에 대해서는 앞서 설명되었으므로[S74, S75], 자 세한 설명은 생략하도록 하겠다. 이상에서는 상기 운행 컨텍스트 정보에 기반하여 사이니지 표시 가능 영역을 고려하여, 상기 프로세싱 위치가 결정되는 로직에 대해 설명하였다. 그런데, 상기 클라이언트 및 상기 차량 간의 통신 트래픽 또는 통 신 가능 여부가 고려되어 상기 프로세싱 위치가 결정될 수도 있다. 이에 대해 도 18을 더욱 참조하여 설명하겠 다. 도 18는 본 개시의 일실시예에 따라 프로세싱 위치가 결정되는 로직을 설명하는 순서도이다. 상기 차량에서 수집된 에지 데이터는 상기 운행 컨텍스트 데이터로 변환되어 상기 클라우드 서버로 제공될 수 있다. 한편, 상기 클라우드 서버의 상기 사이니지 데이터 매니저는 적어도 하나의 외부 데이터 소스로부터 각종 사이니지 관련 정보를 취합할 수 있다. 그리고, 상기 사이니지 데이터 매니저는 상기 운행 컨텍스트 데이터에 기반하여 상기 차량의 현재 위치 및/또는 예상 이동 경로에 해당하는 위치에 해당하는 여부에 부 합하는지 여부 등을 고려한 상기 차량에 적절한 사이니지 관련 정보, 즉 제 1 사이니지 관련 정보를 추출하고, 상기 추출된 제 1 사이니지 관련 정보를 상기 클라우드 정책 매니저로 제공할 수 있다. 그러면, 상기 클라우드 정책 매니저는 상기 운행 컨텍스트 데이터와 상기 추출된 제 1 사이니지 관련 정보 중 적어도 하나에 기반하여, 상기 추출된 제 1 사이니지 관련 정보가 상기 차량 및 상기 클라우드 서버 중 어디에서 프로세싱되는 것이 적절한 것인지, 즉 프로세싱 위치에 대해 판단할 수 있다. 상기 운행 컨텍스트 데이터에 따른 상기 클라이언트 및 상기 차량 간의 통신 트래픽 또는 통신 가능 여부에 따라서 상기 클라우드 정책 매니저가 제 1 사이니지 관련 정보의 프로세싱 위치를 정할 수 있다 [S71, S181]. 상기 클라우드 정책 매니저가 상기 통신 트래픽 또는 상기 통신 가능 여부에 기반하여 제 1 사이니지 관련 정보의 프로세싱 위치를 정할 때 반드시 상기 운행 컨텍스트 데이터를 참고해야 하는 것은 아니다. 상기 클라우 드 정책 매니저 또는 상기 클라우드 서버가 자체적으로 상기 차량과의 통신 트래픽 상황 또는 통신 가능 여부를 파악하고 이에 기반하여 제 1 사이니지 관련 정보의 프로세싱 위치가 정해질 수도 있다. 예를 들면, 상기 차량이 도 17의 (17-1)에 도시된 바와 같이, 예컨대 터널 밖 도심을 운행하고 있는 경우 상기 클라이언트 및 상기 차량 간에 통신이 원활할 수 있다. 상기 클라이언트 및 상기 차량 간에 통신이 원활한 경우, 상기 클라우드 정책 매니저는 제 1 사 이니지 관련 정보의 프로세싱 위치를 상기 클라우드 서버로 정할 수 있다[S182]. 그러나, 상기 차량이 도 17의 (17-2)에 도시된 바와 같이, 예컨대 터널 안을 운행하고 있는 경우 상기 클 라이언트 및 상기 차량 간에 통신 트래픽이 통신이 원활하지 않을 수 있다. 상기 클라이언트 및 상기 차량 간에 통신이 원활하지 않은 경우, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 상기 차량로 정할 수 있다[S183]. 상기 클라우드 정책 매니저에 의해 상기 프로세싱 위치가 상기 클라우드 서버 또는 상기 차량으 로 정해진 경우, 제 1 사이니지 관련 정보가 프로세싱되는 과정에 대해서는 앞서 설명되었으므로[S74, S75], 자 세한 설명은 생략하도록 하겠다. 이상에서 상기 운행 컨텍스트 정보에 기반하여 사이니지 표시 가능 영역을 고려하거나 상기 클라이언트 및 상기 차량 간의 통신 트래픽 또는 통신 가능 여부를 고려하여, 상기 프로세싱 위치가 결정되는 로직에 대 해 설명하였다. 그런데, 상기 차량의 주행안정성에 기반하여 상기 프로세싱 위치가 결정될 수도 있다. 이 에 대해 도 19 및 도 20를 더욱 참조하여 설명하겠다. 도 19는 본 개시의 일실시예에 따라 프로세싱 위치가 결 정되는 로직을 설명하는 순서도이고, 도 20는 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스플 레이되는 주행 영상을 도시한다. 상기 차량에서 수집된 에지 데이터는 상기 운행 컨텍스트 데이터로 변환되어 상기 클라우드 서버로 제공될 수 있다. 한편, 상기 클라우드 서버의 상기 사이니지 데이터 매니저는 적어도 하나의 외부 데이터 소스로부터 각종 사이니지 관련 정보를 취합할 수 있다. 그리고, 상기 사이니지 데이터 매니저는 상기 운행 컨텍스트 데이터에 기반하여 상기 차량의 현재 위치 및/또는 예상 이동 경로에 해당하는 위치에 해당하는 여부에 부 합하는지 여부 등을 고려한 상기 차량에 적절한 사이니지 관련 정보, 즉 제 1 사이니지 관련 정보를 추출 하고, 상기 추출된 제 1 사이니지 관련 정보를 상기 클라우드 정책 매니저로 제공할 수 있다. 그러면, 상기 클라우드 정책 매니저는 상기 운행 컨텍스트 데이터와 상기 추출된 제 1 사이니지 관련 정보 중 적어도 하나에 기반하여, 상기 추출된 제 1 사이니지 관련 정보가 상기 차량 및 상기 클라우드 서버 중 어디에서 프로세싱되는 것이 적절한 것인지, 즉 프로세싱 위치에 대해 판단할 수 있다. 상기 운행 컨텍스트 데이터에 따른 상기 차량의 주행 안정성에 따라서 상기 클라우드 정책 매니저가 제 1 사이니지 관련 정보의 프로세싱 위치를 정할 수 있다[S71, S191]. 예를 들면, 상기 차량이 도 20의 (20-1)에 도시된 바와 같이, 예컨대 상기 차량이 도심을 운행하고 있는 경우 상기 차량의 주행 안정성이 유지된다고 볼 수 없을 수 있다. 왜냐하면, 도심의 경우 복잡한 도 로망, 교통 트래픽 및 각종 교통신호등 등으로 인해 좌회전, 우회전, 정지, 가속의 반복을 자주해야 할 수 있기 때문이다.상기 차량의 주행 안정성을 유지할 수 없는 경우, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 상기 클라우드 서버로 정할 수 있다[S192]. 따라서, 상기 클라우드 서버에 의해 제 1 사이니지 관련 정보에 따른 AR 사이니지 그래픽 데이터가 프로세싱될 수 있다. 상기 차량의 주행 안정성이 유지될 수 없는 경우 상기 차량의 안정 운행을 위해 상기 차량의 연산 리 소스가 많이 필요할 수 있고, 이 때문에 제 1 사이니지 관련 정보를 위한 연산 리소스는 상기 차량이 아닌 상기 클라우드 서버가 맡도록 하기 위한 것이다. 그러나, 상기 차량이 도 20의 (20-2)에 도시된 바와 같이, 예컨대 상기 차량이 고속도로 및 자동차 전용도로를 정속으로 주행하고 있는 경우 상기 차량의 주행안정성이 유지된다고 볼 수 있다. 왜냐하면, 도 심과는 달리 좌회전, 우회전, 정지, 가속의 반복을 자주할 필요가 없기 때문이다. 상기 차량의 주행 안정성을 유지할 수 있는 경우, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 상기 차량로 정할 수 있다[S193]. 따라서, 상기 차량에 의해 제 1 사이니지 관련 정보에 따른 AR 사이니지 그래픽 데이터(2230, 2240)가 프로세싱될 수 있다. 상기 AR 사이니지 그래픽 데 이터(2230, 2240)의 표시 가능 영역은 고속도로 또는 자동차 전용 도로의 가장자리 영역이 될 수 있다. 즉, 상 기 AR 사이니지 그래픽 데이터(2230, 2240)의 표시 가능 영역이 특정 건물의 벽면이 될 필요는 없다. 상기 차량의 주행 안정성이 유지될 수 있는 경우 상기 차량의 안정 운행을 위해 상기 차량의 연산 리 소스가 많이 필요하지 않을 수 있고, 이 때문에 제 1 사이니지 관련 정보를 위한 연산 리소스는 상기 차량(10 0)이 맡더라도 큰 문제가 없을 수 있기 때문이다. 상기 클라우드 정책 매니저에 의해 상기 프로세싱 위치가 상기 클라우드 서버 또는 상기 차량으 로 정해진 경우, 제 1 사이니지 관련 정보가 프로세싱되는 과정에 대해서는 앞서 설명되었으므로[S74, S75], 자 세한 설명은 생략하도록 하겠다. 전술한 도 14 및 도 15와 관련하여, 제 1 사이니지 관련 정보의 데이터의 크기가 큰 경우 제 1 사이니지 관련 정보가 상기 클라우드 서버에서 프로세싱되는 것에 대해 설명하였다. 그런데, 이 경우 제 1 사이니지 관 련 정보가 상기 차량 및 상기 클리우드 서버 간에 분산되어 처리될 수도 있다. 이에 대해 도 21 및 도 22을 더욱 참조하여 설명하겠다. 도 21는 본 개시의 일실시예에 따라 프로세싱 위치가 결정되는 로직을 설명 하는 순서도이고, 도 22는 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스플레이되는 주행 영상 을 도시한다. 전술한 바와 같이, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보의 프로세싱 위치를 정함에 있어 서 제 1 사이니지 관련 정보의 데이터 크기를 고려할 수 있다[S141]. 제 1 사이니지 관련 정보의 상기 데이터 크기가 소정 값 이상인 경우, 상기 클라우드 정책 매니저는 제 1 사이니지 관련 정보를 분산하여, 제 1 사이니지 관련 정보의 프로세싱 위치를 상기 차량 및 상기 클라우드 서버 모두로 정할 수 있다[S211]. 제 1 사이니지 관련 정보는 데이터 타입에 따라 나뉘어 질 수 있다. 예를 들어, 제 1 사이니지 관련 정보 중에 서 광고에 대한 AR 사이니지 그래픽과 관련된 데이터는 제 1 데이터로 구분되고, 제 1 사이니지 관련 정보 중에 서 POI (Point Of Interest)에 대한 AR 사이니지 그래픽과 관련된 데이터는 제 2 데이터로 구분될 수 있다. 이 와 같은 구분은 단지 예시적인 것으로 다른 기준에 따라 제 1 사이니지 관련 정보가 나뉘어 질 수 있음은 물론 이다. 상기 클라우드 정책 매니저는 제 1 데이터의 프로세싱 위치는 상기 클라우드 서버로 정하고, 제 2 데 이터의 프로세싱 위치는 상기 차량으로 정할 수 있다. 도 22에서는 제 1 데이터로 구분된 관계로 상기 클라우드 서버에 이해 프로세싱된 AR 사이니지 그래픽 데 이터(2110, 2120)와, 제 2 데이터로 구분된 관계로 상기 차량에 의해 프로세싱된 AR 사이니지 그래픽 데이 터(2170, 2180, 2190)이 상기 주행 영상과 함께 디스플레이되는 것이 예시되어 있다. 이상에서는 제 1 사이니지 관련 정보가 분산되어 상기 클라우드 서버 및 상기 차량 모두에 의해 프로 세싱되는 것에 대해 설명하였다. 그런데, 앞서 설명된 바와 같이, 제 1 사이니지 관련 정보가 제 2 사이니지 관 련 정보로 필터링되는 것은 상기 클라우드 서버에서 행해지고, 상기 필터링된 제 2 사이니지 관련 정보를 AR 그래픽 데이터와 같은 미디어 컨텐츠로 변환되는 것은 상기 차량에서 행해지는 방식으로 상기 클라우드 서버와 상기 차량이 협력하여 제 1 사이니지 관련 정보를 프로세싱할 수도 있다. 이와 같은 협력 방식의 제 1 사이니지 관련 정보 프로세싱은, 제 1 사이니지 관련 정보의 데이터의 크기가 큰 경우에 또는 제 1 사이니지 관련 정보의 특정 데이터 타입(예를 들면, 리콜 정보 또는 타임 세일과 같은 긴급 알림과 관련된 타입)에 대해 행해질 수 있다. 이와 같은 협력 방식의 제 1 사이니지 관련 정보 프로세싱은 앞서 설명되었으므 로 자세한 설명은 생략하겠다. 한편, 상기 차량이 어떤 경로를 처음 주행하는 경우에 상기 경로 중에 디스플레이되어야 할 미디어 컨텐츠 를 프로세싱해야 함은 당연하겠지만, 그 이후 상기 경로를 반복적으로 주행하는 경우에는 상기 미디어 컨텐츠를 또다시 프로세싱할 필요 없이 기존에 디스플레이되었던 상기 미디어 컨텐츠를 그대로 재활용할 수도 있다. 이에 대해 도 5를 참조하여 설명하겠다. 상기 주행 영상과 함께 디스플레이된 미디어 컨텐츠가 상기 메모리(예를 들면, 캐시 메모리 (이에 한정되 지 않음))에 저장될 수 있다. 상기 차량의 상기 에지 정책 매니저는 상기 메모리에 저장되어 있는 상기 미디어 컨텐츠 및/또 는 그 식별자를 상기 클라우드 정책 매니저로 실시간으로, 주기적으로, 또는 특정 이벤트 발생시 비주기적 으로 상기 피드백할 수 있다. 상기 \"특정 이벤트\"의 예시로는 상기 차량의 동일 경로의 반복적 운행이 감 지되는 것일 수 있다 (이에 한정되지 않음). 그러면, 상기 클라우드 정책 매니저는 상기 차량으로부터 실시간으로 수신되는 상기 운행 컨텍스트 데이터를 통해 상기 차량이 상기 동일 경로를 운행하고 있는 것으로 감지되면, 상기 동일 경로 중에 디스 플레이되어야 할 미디어 컨텐츠 또는 이에 관한 사이니지 관련 데이터 중에서 상기 피드백된 미디어 컨텐츠와 관련된 데이터와 동일한 것은 상기 차량으로 제공하지 않도록 할 수 있다. 대신에 상기 클라우드 정책 매니저는 상기 차량이 상기 동일 경로를 주행할 때 상기 메모리에 저장되어 있는 상기 미디어 컨텐츠가 상기 주행 영상과 함께 디스플레이되도록 하는 제어 신호를 상기 차량 의 상기 에지 정책 매니저로 전송할 수 있다. 다만, 상기 동일 경로 중에 디스플레이되어야 할 미디어 컨텐츠 또는 이에 관한 사이니지 관련 데이터 중에서 상기 피드백된 미디어 컨텐츠와 관련된 데이터와 동일하지 않은 것(예를 들면, 새롭게 업데이트 되거나 추가된 데이터)은 앞서 설명된 것과 같은 방식으로 상기 클라우드 서버 및/또는 상기 차량에 의해 프로세싱 될 수 있음은 물론이다. 상기 제어 신호에 반응하여, 상기 에지 정책 매니저는 상기 운행 컨텍스트 데이터 매니저로부터 수 신하는 상기 운행 컨텍스트 데이터 및 상기 차량 내의 상기 메모리 (예를 들면, 캐시 메모리 (이에 한정되지 않음)) 내에 기저장되어 있는 제 2 사이니지 관련 정보 또는 상기 미디어 컨텐츠를 상기 에지 사이니 지 데이터 프로세서로 전송할 수 있다. 또는, 상기 에지 정책 매니저는 상기 에지 사이니지 데이터 프로세서가 직접 상기 메모리에 접근하여 제 2 사이니지 관련 정보를 독출하도록 제어할 수 있다. 또한, 상기 에지 정책 매니저는 상기 운행 컨텍스트 데이터(특히, 상기 사용자의 프로파일)에 기반하여 상기 차량을 통해 디스플레이될 사이니지 데이터의 디스플레이 정책을 도출하고 상기 사이니지 데이터 프 로세서로 전송할 수 있다. 상기 메모리에 저장되어 있는 제 2 사이니지 관련 정보를 활용하는 경우, 상기 에지 사이니지 데이터 프로 세서은 상기 디스플레이 정책에 부합하는 제 2 사이니지 관련 정보를 미디어 컨텐츠로 변환하여 상기 랜 더링 모듈로 전송할 수 있다. 또는 상기 메모리에 저장되어 있는 제 2 사이니지 관련 정보를 활용하 는 경우, 상기 사이니지 데이터 프로세서은 상기 디스플레이 정책에 부합하는 미디어 컨텐츠를 상기 랜더 링 모듈로 전송할 수 있다. 상기 랜더링 모듈로 전송되는 미디어 컨텐츠는 상기 디스플레이 위치 정 보가 태깅된 것이다. 상기 랜더링 모듈는 상기 차량이 상기 미디어 컨텐츠의 해당 위치에 소정 거리 이내로 접근하는 것으 로 파악되거나, 상기 주행 영상을 통해 상기 해당 지형 또는 지물이 인식되면, 상기 미디어 컨텐츠를 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭시킬 수 있다. 상기 미디어 컨텐츠는 상기 해당 위치 또는 상기 해당 지형 또는 지물에 매칭되도록 상기 주행 영상과 함께 디 스플레이부를 통해 디스플레이될 수 있다. 전술한 본 개시는, 프로그램이 기록된 매체에 컴퓨터가 읽을 수 있는 코드로서 구현하는 것이 가능하다. 컴퓨터 가 읽을 수 있는 매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를포함한다. 컴퓨터가 읽을 수 있는 매체의 예로는, HDD(Hard Disk Drive), SSD(Solid State Disk), SDD(Silicon Disk Drive), ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있다. 또한, 상기 컴 퓨터는 인공 지능 기기의 프로세서를 포함할 수도 있다."}
{"patent_id": "10-2025-7001758", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시 예에 따른 AI 장치를 나타낸다. 도 2는 본 개시의 일 실시 예에 따른 AI 서버를 나타낸다. 도 3은 본 개시의 일 실시 예에 따른 AI 시스템을 나타낸다. 도 4는 본 개시의 또 다른 실시 예에 따른 AI 장치를 나타낸다. 도 5는 본 개시의 일실시예에 따른 에지 디바이스 및 클라우드 서버 간의 협업 시스템의 소프트웨어 관점의 블 록도이다. 도 6은 본 개시의 일실시예에 따른 에지 데이터 및 이로부터 추출되는 운행 컨텍스트 데이터를 도시한다. 도 7은 본 개시의 일실시예에 따른 에지 디바이스 및 클라우드 서버 간의 협업 방법에 관한 순서도이다. 도 8은 본 개시의 일실시예에 따른 에지 디바이스 및 클라우드 서버 간의 협업 시스템을 통해 프로세싱된 미디 어 컨텐츠가 디스플레이되는 순서도이다. 도 9 및 도 10은 본 개시의 일실시예에 따른 에지 디바이스 및 클라우드 서버 간의 협업 시스템을 통해 프로세 싱된 미디어 컨텐츠가 디스플레이되는 예시를 도시한다. 도 11은 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스플레이되는 주행 영상을 도시한다. 도 12는 본 개시의 일실시예에 따라 프로세싱 위치가 결정되는 로직을 설명하는 순서도이다. 도 13은 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스플레이되는 주행 영상을 도시한다. 도 14는 본 개시의 일실시예에 따라 프로세싱 위치가 결정되는 로직을 설명하는 순서도이다. 도 15은 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스플레이되는 주행 영상을 도시한다. 도 16는 본 개시의 일실시예에 따라 프로세싱 위치가 결정되는 로직을 설명하는 순서도이다. 도 17은 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스플레이되는 주행 영상을 도시한다. 도 18은 본 개시의 일실시예에 따라 프로세싱 위치가 결정되는 로직을 설명하는 순서도이다. 도 19는 본 개시의 일실시예에 따라 프로세싱 위치가 결정되는 로직을 설명하는 순서도이다. 도 20는 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스플레이되는 주행 영상을 도시한다. 도 21는 본 개시의 일실시예에 따라 프로세싱 위치가 결정되는 로직을 설명하는 순서도이다. 도 22는 본 발명의 일실시예에 따라 AR 사이니지 그래픽 데이터가 디스플레이되는 주행 영상을 도시한다."}
