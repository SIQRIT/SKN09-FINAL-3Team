{"patent_id": "10-2023-0041884", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0146822", "출원번호": "10-2023-0041884", "발명의 명칭": "의료 이미지의 생성 방법 및 시스템", "출원인": "주식회사 더로벨", "발명자": "정호원"}}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공 신경망을 학습하고, 학습된 인공 신경망을 사용자 단말로 전송하는 서버; 상기 인공 신경망 및 상기 인공 신경망의 학습에 필요한 의료 이미지를 저장하는 데이터베이스; 및학습된 인공신경망을 통해 이미지 처리를 수행하고, 처리된 의료 이미지를 출력하는 사용자 단말;을 포함하고,상기 서버는,제1 인공 신경망에 기초하여 제1 의료 이미지에 포함된 해부학적 구조를 분류한 후, 상기 해부학적 구조에 기초하여 상기 제1 의료 이미지로부터 분리된 제2 의료 이미지 각각을 상기 데이터베이스에 나누어 저장하고,상기 사용자 단말은,사용자가 생성할 의료 이미지의 종류 및 형태에 관한 정보를 입력할 수 있는 설정 창을 출력하고,상기 설정 창에 입력된 상기 의료 이미지의 종류 및 형태에 관한 정보와 제2 인공 신경망에 기초하여 제3 의료이미지의 일부에 바운딩 박스를 생성하고,상기 의료 이미지의 종류 및 형태에 관한 정보에 기초하여 상기 데이터베이스에 저장된 제2 의료 이미지 중 상기 바운딩 박스에 삽입될 제2 의료 이미지를 선택하고,상기 제3 의료 이미지 내에 생성된 바운딩 박스의 크기 및 각도에 기초하여, 상기 선택된 제2 의료 이미지를 삽입하고, 상기 제2 의료 이미지가 삽입된 제4 의료 이미지를 출력하는 의료 이미지 생성 시스템."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 서버는,상기 제2 의료 이미지가 삽입될 위치를 지정한 라벨링을 통해 상기 제2 인공 신경망을 학습하거나, HITL (HumanIn The Loop) 방식을 통해 상기 제2 인공 신경망을 학습하고,상기 사용자 단말은,상기 학습된 제2 인공 신경망에 기초하여 상기 제3 의료 이미지에 삽입될 상기 바운딩 박스의 위치를 결정하는의료 이미지 생성 시스템."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 제1 인공신경망은,Unet 기반 모델 또는 YoLo 모델 중 적어도 하나에 기초하여 상기 제1 의료 이미지에 포함된 해부학적 구조에 대한 객체 인식을 수행하는 의료 이미지 생성 시스템."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3항에 있어서,상기 서버는,상기 객체 인식된 해부학적 구조에 기초하여 상기 제1 의료 이미지에서 바운딩 모양으로 상기 제2 의료 이미지를 구분하고, 상기 바운딩 모양 내에서 객체 인식된 제2 의료 이미지와 여백 이미지를 상기 데이터베이스에 저공개특허 10-2024-0146822-3-장하는 의료 이미지 생성 시스템."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4항에 있어서,상기 사용자 단말은,상기 생성된 바운딩 박스의 크기 및 각도에 기초하여 상기 제2 의료 이미지가 포함된 바운딩 모양의 크기 및 각도를 조정하고,상기 크기 및 각도가 조정된 바운딩 모양과 함께 상기 제2 의료 이미지를 상기 바운딩 박스에 삽입하는 의료 이미지 생성 시스템."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1항에 있어서,상기 사용자 단말은,제3 인공 신경망에 기초하여 제4 의료 이미지내에서 부자연스러운 이미지 영역을 탐지하고,상기 탐지된 영역의 이미지를 수정하는 의료 이미지 생성 시스템."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6항에 있어서,상기 서버는,부자연스러운 이미지 영역 및 자연스러운 이미지 영역을 라벨링한 학습 데이터를 통해 상기 제3 인공 신경망을학습하거나, HITL 방식을 통해 상기 제3 인공 신경망을 학습하는 의료 이미지 생성 시스템."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서,상기 사용자 단말은,상기 제1 인공 신경망에 기초하여 상기 제4 의료 이미지에 삽입된 상기 제2 의료 이미지의 객체 인식을 수행하고, 객체 인식된 상기 제2 의료 이미지의 외곽을 미리 설정된 두께로 마스킹하고,제4 인공 신경망에 기초하여 상기 마스킹된 부분의 이미지 리터칭(Image Retouching)을 수행하는 의료 이미지생성 시스템."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8항에 있어서,상기 서버는,GAN 모델 또는 Diffusion 모델에 기초하여 상기 제4 인공 신경망을 학습하는 의료 이미지 생성 시스템."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 인공 신경망에 기초하여 제1 의료 이미지에 포함된 해부학적 구조를 분류하고;상기 해부학적 구조에 기초하여 상기 제1 의료 이미지로부터 분리된 제2 의료 이미지 각각을 데이터베이스에 나누어 저장하고;사용자가 생성할 의료 이미지의 종류 및 형태에 관한 정보를 입력할 수 있는 설정 창을 출력하고;상기 설정 창에 입력된 상기 의료 이미지의 종류 및 형태에 관한 정보와 미리 학습된 제2 인공 신경망에 기초하여 제3 의료 이미지의 일부에 바운딩 박스를 생성하고;공개특허 10-2024-0146822-4-상기 설정 창에 입력된 의료 이미지의 종류 및 형태에 관한 정보에 기초하여 상기 바운딩 박스에 삽입될 제2 의료 이미지를 선택하고;상기 제3 의료 이미지 내의 바운딩 박스의 크기 및 각도에 기초하여, 상기 선택된 제2 의료 이미지를 삽입하고,상기 제2 의료 이미지가 삽입된 제4 의료 이미지를 출력하는 것;을 포함하는 의료 이미지 생성 방법."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10항에 있어서,상기 제2 의료 이미지를 삽입하는 것은,사용자가 수동으로 입력한 바운딩 박스를 인식하고;상기 제2 인공 신경망에 기초하여 상기 바운딩 박스의 위치 또는 크기를 조정하는 것;을 포함하는 의료 이미지생성 방법."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 10항에 있어서,상기 제2 의료 이미지 각각을 상기 데이터베이스에 나누어 저장하는 것은,상기 제1 의료 이미지로부터 분리된 제2 의료 이미지를 바운딩 모양과 함께 저장하는 것;을 포함하고,상기 선택된 제2 의료 이미지를 삽입하는 것은,상기 생성된 바운딩 박스의 크기 및 각도에 기초하여 상기 데이터베이스에 저장된 바운딩 모양의 크기 및 각도를 조정하고상기 조정된 바운딩 모양과 함께 상기 제2 의료 이미지를 상기 바운딩 박스에 삽입하는 것;을 포함하는 의료 이미지 생성 방법."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12항에 있어서,상기 선택된 제2 의료 이미지를 삽입하는 것은,상기 생성된 바운딩 박스의 일부가 상기 제3 의료 이미지를 벗어나면, 벗어난 바운딩 박스의 일부에 대응하여상기 데이터베이스 내에 저장된 제2 의료 이미지의 일부를 포함한 제4 의료 이미지를 생성하는 것;을 포함하는의료 이미지 생성 방법."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 10항에 있어서,상기 제1 인공 신경망에 기초하여 상기 제4 의료 이미지에 삽입된 상기 제2 의료 이미지의 객체 인식을 수행하고;객체 인식된 상기 제2 의료 이미지의 외곽을 미리 설정된 두께로 마스킹하고;GAN 모델 또는 Diffusion 모델을 통해 학습한 제4 인공 신경망에 기초하여 상기 마스킹된 부분의 이미지 인페인팅을 수행하는 것;을 더 포함하는 의료 이미지 생성 방법."}
{"patent_id": "10-2023-0041884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 10항에 있어서,상기 삽입될 제2 의료 이미지를 선택하는 것은,미리 저장된 통계적 수치에 기초하여 상기 데이터베이스에 저장된 제2 의료 이미지 중 삽입될 제2 의료 이미지를 선택하는 의료 이미지 생성 방법.공개특허 10-2024-0146822-5-"}
{"patent_id": "10-2023-0041884", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "개시된 일 실시예에 따른 의료 이미지 생성 시스템은, 인공 신경망을 학습하고, 학습된 인공 신경망을 사용자 단 말로 전송하는 서버; 상기 인공 신경망 및 상기 인공 신경망의 학습에 필요한 의료 이미지를 저장하는 데이터베 이스; 및 학습된 인공신경망을 통해 이미지 처리를 수행하고, 처리된 의료 이미지를 출력하는 사용자 단말;을 포 (뒷면에 계속)"}
{"patent_id": "10-2023-0041884", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 비식별화된 의료 이미지의 생성 방법 및 시스템에 관련된 것으로, 보다 구체적으로는, 개인정보는 보 호하면서 인공지능 학습, 교육 등을 위한 무수히 많은 비식별화된 의학 영상 이미지를 생성하는 합성 데이터 생 성 방법 및 합성 데이터를 생성하는 시스템에 관련된 것이다."}
{"patent_id": "10-2023-0041884", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "방사선사진(x-ray)은 의료 분야에서 사용하는 영상 검사 중 방사선을 인체에 투과하여 내부 조직의 상태를 확인 하는 검사 방법이다. 방사선사진 촬영 시 미량의 방사선의 조사를 통해서 장기와 뼈, 치아와 같은 해부학적 구 조물에 위치한 병소와 같은 이상소견, 수술 후에 남아 있는 보철물, 뼈의 정렬 상태 및 안정성 등의 상황을 알 수 있다. 예를 들어, 유방촬영술 (mammography)을 통한 작은 크기의 종괴나 석회화 침착 현상의 발견은 유방암 에 대해서 조직검사를 의뢰할 수 있는 근거가 된다. 뼈에 있어서 정렬 상태안정성, 골절 여부 등의 문제가 의심 될 때나 소아청소년의 성장 발달 단계를 검사 또는 진단 시 방사선사진을 촬영한다. 치과에서 사용되는 교익 (Bitewing) 및 치근단(Periapical) 방사선사진은 입 안에 디지털 센서 또는 필름을 넣고 사진을 찍는 구내촬영 과 구강 악안면 영역 전체를 한 번에 촬영하는 파노라마(Panorama), 그리고 두부규격 방사선사진(Cephalometric radiograph)과 같은 구외 촬영으로 구분되며, 치아와 구강악안면 구조의 상태를 방사선사진을 통해 관찰 및 진 단한다. 이때, 방사선사진은 신원을 추정 또는 식별할 수 있는 개인정보에 속한다. 예를 들어서 치아의 상태나 치료 후 에 남아있는 보철물은 법의학적 근거로써 의료기록과 대조를 통해서 신원불상자의 신원 확인을 가능하게 한다. 이 때문에, 방사선사진을 치아 상태를 자가진단 하는 시스템의 레퍼런스 이미지로 무단 사용하거나 방사선사진 이 저장되어 있는 컴퓨터 또는 장비 등이 도난된 경우, 개인정보 유출로 인하여, 법적인 제재를 받을 수 있다. 방사선사진은 개인 식별 정보에 해당하며, 방사선사진 이외에 CT 사진, MRI 사진 및 초음파 사진도 모두 개인 식별 정보를 포함한 이미지에 해당한다. 따라서, 개인정보의 비식별화 없이는 개인 정보 유출의 위험이 상존하 고, 상기 방사선사진을 인공지능 학습을 위한 자료로 사용하는 것도 어렵다. 인공지능을 학습하기 위해서는 대량의 학습 데이터가 필요한데, 개인 정보가 포함된 의료 이미지는 개인 정보 보호법 등에 기초하여 외부로 반출이 어렵다. 따라서 의료계에서 인공지능을 통해 기술 발전을 꾀하려해도, 인 공지능의 학습에 필요한 의료 데이터를 구하는 것이 쉽지 않다. 특히 골육종과 같은 희귀질환의 경우 절대적인 데이터의 수의 부족으로 인공지능의 학습 정확도 및 성능의 신뢰도를 높이는 데에는 분명한 한계가 있다."}
{"patent_id": "10-2023-0041884", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "개시된 실시예는 개인 정보를 보호하면서, 의료기기 또는 의료진단을 수행하는데 사용되는 인공지능을 학습하는 데 필요한 대량의 학습 데이터를 생성할 수 있는 의료 이미지 생성 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2023-0041884", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "개시된 일 실시예에 따른 의료 이미지의 생성 방법 및 시스템은 인공 신경망을 학습하고, 학습된 인공 신경망을 사용자 단말로 전송하는 서버; 상기 인공 신경망 및 상기 인공 신경망의 학습에 필요한 의료 이미지를 저장하는 데이터베이스; 및 학습된 인공신경망을 통해 이미지 처리를 수행하고, 처리된 의료 이미지를 출력하는 사용자 단말;을 포함하고, 상기 서버는, 제1 인공 신경망에 기초하여 제1 의료 이미지에 포함된 해부학적 구조를 분류한 후, 상기 해부학적 구조에 기초 하여 상기 제1 의료 이미지로부터 분리된 제2 의료 이미지 각각을 상기 데이터베이스에 나누어 저장하고, 상기 사용자 단말은, 사용자가 생성할 의료 이미지의 종류 및 형태에 관한 정보를 입력할 수 있는 설정 창을 출력하고, 상기 설정 창에 입력된 상기 의료 이미지의 종류 및 형태에 관한 정보와 제2 인공 신경망에 기초하여 제3 의료 이미지의 일부에 바운딩 박스를 생성하고, 상기 의료 이미지의 종류 및 형태에 관한 정보에 기초하여 상기 데이터베이스에 저장된 제2 의료 이미지 중 상기 바운딩 박스에 삽입될 제2 의료 이미지를 선택하고, 상기 제3 의료 이미지 내에 생성된 바운딩 박스의 크기 및 각도에 기초하여, 상기 선택된 제2 의료 이미지를 삽입하고, 상기 제2 의료 이미지가 삽입된 제4 의료 이미지를 출력한다. 상기 서버는, 상기 제2 의료 이미지가 삽입될 위치를 지정한 라벨링을 통해 상기 제2 인공 신경망을 학습하거나, HITL (Human In The Loop) 방식을 통해 상기 제2 인공 신경망을 학습하고, 상기 사용자 단말은, 상 기 학습된 제2 인공 신경망에 기초하여 상기 제3 의료 이미지에 삽입될 상기 바운딩 박스의 위치를 결정할 수 있다. 상기 제1 인공신경망은, Unet 기반 모델 또는 YoLo 모델 중 적어도 하나에 기초하여 상기 제1 의료 이미지에 포 함된 해부학적 구조에 대한 객체 인식을 수행할 수 있다. 상기 서버는, 상기 객체 인식된 해부학적 구조에 기초하여 상기 제1 의료 이미지에서 바운딩 모양으로 상기 제2 의료 이미지를 구분하고, 상기 바운딩 모양 내에서 객체 인식된 제2 의료 이미지와 여백 이미지를 상기 데이터 베이스에 저장할 수 있다. 상기 사용자 단말은, 상기 생성된 바운딩 박스의 크기 및 각도에 기초하여 상기 제2 의료 이미지가 포함된 바운 딩 모양의 크기 및 각도를 조정하고, 상기 크기 및 각도가 조정된 바운딩 모양과 함께 상기 제2 의료 이미지를 상기 바운딩 박스에 삽입할 수 있다. 상기 사용자 단말은, 제3 인공 신경망에 기초하여 제4 의료 이미지내에서 부자연스러운 이미지 영역을 탐지하고, 상기 탐지된 영역의 이미지를 수정할 수 있다. 상기 서버는, 부자연스러운 이미지 영역 및 자연스러운 이미지 영역을 라벨링한 학습 데이터를 통해 상기 제3 인공 신경망을 학습하거나, HITL 방식을 통해 상기 제3 인공 신경망을 학습할 수 있다. 상기 사용자 단말은, 상기 제1 인공 신경망에 기초하여 상기 제4 의료 이미지에 삽입된 상기 제2 의료 이미지의 객체 인식을 수행하고, 객체 인식된 상기 제2 의료 이미지의 외곽을 미리 설정된 두께로 마스킹하고, 제4 인공 신경망에 기초하여 상기 마스킹된 부분의 이미지 리터칭(Image Retouching)을 수행할 수 있다. 상기 서버는, GAN 모델 또는 Diffusion 모델에 기초하여 상기 제4 인공 신경망을 학습할 수 있다. 개시된 다른 실시예에 따른 의료 이미지 생성 방법은 제1 인공 신경망에 기초하여 제1 의료 이미지에 포함된 해 부학적 구조를 분류하고; 상기 해부학적 구조에 기초하여 상기 제1 의료 이미지로부터 분리된 제2 의료 이미지 각각을 데이터베이스에 나누어 저장하고; 사용자가 생성할 의료 이미지의 종류 및 형태에 관한 정보를 입력할 수 있는 설정 창을 출력하고; 상기 설정 창에 입력된 상기 의료 이미지의 종류 및 형태에 관한 정보와 미리 학 습된 제2 인공 신경망에 기초하여 제3 의료 이미지의 일부에 바운딩 박스를 생성하고; 상기 설정 창에 입력된 의료 이미지의 종류 및 형태에 관한 정보에 기초하여 상기 바운딩 박스에 삽입될 제2 의료 이미지를 선택하고; 상기 제3 의료 이미지 내의 바운딩 박스의 크기 및 각도에 기초하여, 상기 선택된 제2 의료 이미지를 삽입하고, 상기 제2 의료 이미지가 삽입된 제4 의료 이미지를 출력하는 것;을 포함한다. 상기 제2 의료 이미지를 삽입하는 것은, 사용자가 수동으로 입력한 바운딩 박스를 인식하고; 상기 제2 인공 신 경망에 기초하여 상기 바운딩 박스의 위치 또는 크기를 조정하는 것;을 포함할 수 있다. 상기 제2 의료 이미지 각각을 상기 데이터베이스에 나누어 저장하는 것은, 상기 제1 의료 이미지로부터 분리된 제2 의료 이미지를 바운딩 모양과 함께 저장하는 것;을 포함하고, 상기 선택된 제2 의료 이미지를 삽입하는 것 은, 상기 생성된 바운딩 박스의 크기 및 각도에 기초하여 상기 데이터베이스에 저장된 바운딩 모양의 크기 및 각도를 조정하고; 상기 조정된 바운딩 모양과 함께 상기 제2 의료 이미지를 상기 바운딩 박스에 삽입하는 것;을 포함할 수 있다. 상기 선택된 제2 의료 이미지를 삽입하는 것은, 상기 생성된 바운딩 박스의 일부가 상기 제3 의료 이미지를 벗 어나면, 벗어난 바운딩 박스의 일부에 대응하여 상기 데이터베이스 내에 저장된 제2 의료 이미지의 일부를 포함 한 제4 의료 이미지를 생성하는 것;을 포함할 수 있다. 상기 제1 인공 신경망에 기초하여 상기 제4 의료 이미지에 삽입된 상기 제2 의료 이미지의 객체 인식을 수행하 고; 객체 인식된 상기 제2 의료 이미지의 외곽을 미리 설정된 두께로 마스킹하고; GAN 모델 또는 Diffusion 모델을 통해 학습한 제4 인공 신경망에 기초하여 상기 마스킹된 부분의 이미지 인페인팅을 수행하는 것;을 더 포 함할 수 있다. 상기 삽입될 제2 의료 이미지를 선택하는 것은, 미리 저장된 통계적 수치에 기초하여 상기 데이터베이스에 저장 된 제2 의료 이미지 중 삽입될 제2 의료 이미지를 선택할 수 있다."}
{"patent_id": "10-2023-0041884", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "개시된 실시예에 따르면, 공개되는 의료 영상 이미지로부터 신원을 유추하는 것이 원천 차단되도록, 개인정보 식별이 불가능한 2차원 의학 영상 이미지 합성 데이터를 생성함으로써, 개인정보를 상당한 수준으로 보호할 수 있는, 비식별화된 의료 영상 이미지 합성 데이터 생성 방법 및 시스템이 제공될 수 있다. 또한, 충분한 양의 개인정보가 비식별화된 인공지능 학습용 의료 영상 이미지 합성 데이터를 확보할 수 있으며, 이를 통하여, 인공지능의 성능을 향상시킬 수 있는, 비식별화된 의료 영상 이미지 합성 데이터 생성 방법 및 시 스템이 제공될 수 있다."}
{"patent_id": "10-2023-0041884", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면들을 참조하여 본 발명의 바람직한 실시 예를 상세히 설명할 것이다. 그러나 본 발명의 기술 적 사상은 여기서 설명되는 실시 예에 한정되지 않고 다른 형태로 구체화될 수도 있다. 오히려, 여기서 소개되 는 실시 예는 개시된 내용이 철저하고 완전해질 수 있도록 그리고 당업자에게 본 발명의 사상이 충분히 전달될 수 있도록 하기 위해 제공되는 것이다. 본 명세서에서, 어떤 구성요소가 다른 구성요소 상에 있다고 언급되는 경우에 그것은 다른 구성요소 상에 직접 형성될 수 있거나 또는 그들 사이에 제3의 구성요소가 개재될 수도 있다는 것을 의미한다. 또한, 도면들에 있어 서, 형상 및 크기는 기술적 내용의 효과적인 설명을 위해 과장된 것이다. 또한, 본 명세서의 다양한 실시 예 들에서 제1, 제2, 제3 등의 용어가 다양한 구성요소들을 기술하기 위해서 사 용되었지만, 이들 구성요소들이 이 같은 용어들에 의해서 한정되어서는 안 된다. 이들 용어들은 단지 어느 구성 요소를 다른 구성요소와 구별시키기 위해서 사용되었을 뿐이다. 따라서, 어느 한 실시 예에 제1 구성요소로 언 급된 것이다른 실시 예에서는 제2 구성요소로 언급될 수도 있다. 여기에 설명되고 예시되는 각 실시 예는 그것 의 상보적인 실시 예도 포함한다. 또한, 본 명세서에서 '및/또는'은 전후에 나열한 구성요소들 중 적어도 하나를 포함하는 의미로 사용되었다. 명세서에서 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한 복수의 표현을 포함한다. 또한, \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 특징, 숫자, 단계, 구성요소 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징이나 숫자, 단계, 구성요소 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 배제하는 것으로 이해되어서는 안 된다. 또한, 본 명세서에서 \"연결\"은 복수의 구성 요소를 간접적으로 연결하는 것, 및 직접적으로 연결하는 것을 모두 포함하는 의미로 사용된다. 또한, 하기에서 본 발명을 설명함에 있어 관련된 공지 기능 또는 구성에 대한 구체적인 설명이 본 발명의 요지 를 불필요하게 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략할 것이다. 도 1 은 개시된 의료 이미지의 생성 시스템의 개략적인 구성을 설명하기 위한 도면이고, 도 2는 의료 이미지의 생성 시스템의 각 구성에 대한 제어 블록도이다. 중복되는 설명을 피하기 위해서 이하 함께 설명한다. 도 1을 먼저 참조하면, 의료 이미지 생성 시스템은, 사용자 단말과 서버를 포함하고, 네트워크를 통 해 데이터를 송수신한다. 구체적으로 사용자 단말은 개인정보가 식별되지 않는 최종적인 의료 이미지(이하 제4 의료 이미지)를 생성 하고, 생성된 제4 의료 이미지를 사용자에게 출력하는 기기이다. 여기서 의료 이미지는, CT 사진, MRI 사진, 초음파 사진(ultrasonography) 및 방사선사진(2D X-ray)을 포함할 수 있다. 이하에서는 생성할 제4 의료 이미지를 방사선 사진으로 한정하여 설명하나, 반드시 이에 제한되는 것 은 아니다. 서버는 제4 의료 이미지의 생성에 필요한 기초적인 의료 이미지(이하 제1 의료 이미지 및 제2 의료 이미 지)를 생성 또는 저장한다. 또한, 서버는 제4 의료 이미지 생성에 필요한 다양한 인공 신경망을 미리 저장 하고, 학습 데이터를 통해 제4 의료 이미지 생성에 필요한 인공 신경망을 학습시키는 기기이다. 서버는 학 습된 인공 지능을 사용자 단말로 전달하고, 사용자 단말은 수신된 인공 지능의 출력값을 이용하여 제4 의료 이미지를 생성한다. 사용자 단말은 서버로부터 학습된 인공 신경망과 제4 의료 이미지의 생성에 필요한 애플리케이션을 내 려(다운로드)받을 수 있으며, 애플리케이션을 통해 사용자에게 제4 의료 이미지 생성에 필요한 다양한 UI(User Interface)를 제공하며, 이하에서는 이러한 UI를 설정 창 형태로 표시하는 것으로 설명한다. 한편, 사용자 단말은 서버로부터 대용량의 인공 신경망 또는 학습 데이터를 수신하여 실행하는 기기이 나, 사용자 단말의 제어부(도 2의 12) 또는 저장부(도 2의 13)의 하드웨어적 구성에 따라 인공 신경망을 스 스로 학습시키거나, 학습에 필요한 데이터를 저장할 수도 있다. 도 1에서는 사용자 단말을 테블릿 PC로 표현하였으나, 반드시 이에 제한되는 것은 아니고, 네트워크를 통해 서버에 접속할 수 있는 컴퓨터나 휴대용 단말기로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 웹 브라우 저(WEB Browser)가 탑재된 노트북, 데스크톱(desktop), 랩톱(laptop), 태블릿 PC, 슬레이트 PC 등을 포함하고, 휴대용 단말기는 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), WiBro(Wireless Broadband Internet) 단말, 스마트 폰(Smart Phone) 등과 같은 모든 종류의 핸드헬드(Handheld) 기반의 무선 통신 장치와 시계, 반지, 팔찌, 발찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head-mounted- device(HMD) 등과 같은 웨어러블 장치를 포함할 수 있다. 서버는 여러 개의 CPU, 메모리 및 I/O포트 등으로 구성된 서버 모듈이 수십 또는 수백 개 이상으로 합쳐진 기기로, HTTP 서버 또는 클라우드 서버를 포함한다. 서버는 클라이언트, 즉 사용자 단말이 요청을 통해 동작하는 하드웨어적 구성이면, 명칭에 한정되는 것이 아니다. 서버 소프트웨어는 이하에서 구체적으로 후술하 는 인공 신경망을 학습시키고, 합성 데이터(Synthetic Data)를 생성하는데 필요한 다양한 데이터를 저장, 처리 하는데 필요한 프로그램이며, 서버는 인공 신경망을 계속적으로 업데이트시킬 수 있다. 한편, 개시된 의료 생성 시스템에서 서버는 반드시 국내에 마련될 필요는 없다. 즉, 서버는 국외에 마련하여 설치할 수도 있으며, 서버가 제공하는 다양한 데이터 및 인공 신경망의 데이터가 사용자 단말에 전송되어 처리되면 개시된 발명이 특허법상 실시에 해당할 수 있다. 도 2를 참조하면, 사용자 단말은 네트워크를 통해 서버와 통신하기 위한 통신부, 제4 의료 이미지 를 처리하기 위한 다양한 동작을 수행하는 제어부, 서버로부터 학습된 인공 신경망을 저장하거나, 제4 의료 이미지의 생성에 필요한 제3 의료 이미지 등을 저장하는 저장부 및 생성된 제4 의료 이미지 등을 출력 하기 위한 디스플레이를 포함한다. 서버는 사용자 단말의 통신부와 유선 또는 무선 통신을 수행하면서, 의료 이미지의 생성 동작에 필 요한 다양한 데이터를 주고 받는 네트워크부, 의료 이미지 생성에 필요한 적어도 하나 이상의 인공 신경망 을 학습하는 인공지능 학습부 및 사용자 단말이 개인 정보 비식별을 위해서 삽입할 가공된 이미지를 처 리 또는 분류하는 이미지 처리부가 포함된다. 데이터베이스는 서버의 이미지 처리부가 처리 및 분류한 의료 이미지(이하에서는 제2 의료 이미 지)를 해부학적 구조에 따라 폴더별로 저장하는 기기이다. 데이터베이스는 제2 의료 이미지 이외에도 서버 의 동작에 필요한 다양한 데이터를 저장할 수도 있다. 도 2에서는 서버와 데이터베이스를 서로 다 른 구성으로 도시하였으나, 반드시 이에 한정되는 것은 아니고, 데이터베이스는 서버 내에 포함되는 구 성일 수 있다. 구체적으로 사용자 단말의 통신부 및 서버의 네트워크부는 네트워크를 이용하여 대용량의 데이 터, 학습된 인공 신경망 및 의료 이미지 생성에 필요한 어플리케이션 등을 송수신할 수 있다. 이를 위해서 통신부 및 네트워크부는 외부 장치와 통신을 가능하게 하는 하나 이상의 구성 요소를 포함 할 수 있으며, 예를 들어 근거리 통신 모듈, 유선 통신 모듈 및 무선 통신 모듈 중 적어도 하나를 포함할 수 있 다. 근거리 통신 모듈은 블루투스 모듈, 적외선 통신 모듈, RFID(Radio Frequency Identification) 통신 모듈, WLAN(Wireless Local Access Network) 통신 모듈, NFC 통신 모듈, 직비(Zigbee) 통신 모듈 등 근거리에서 무선 통신망을 이용하여 신호를 송수신하는 다양한 근거리 통신 모듈을 포함할 수 있다. 유선 통신 모듈은 지역 통신(Local Area Network; LAN) 모듈, 광역 통신(Wide Area Network; WAN) 모듈 또는 부가가치 통신(Value Added Network; VAN) 모듈 등 다양한 유선 통신 모듈뿐만 아니라, USB(Universal Serial Bus), HDMI(High Definition Multimedia Interface), DVI(Digital Visual Interface), RS-232(recommended standard232), 전력선 통신, 또는 POTS(plain old telephone service) 등 다양한 케이블 통신 모듈을 포함할 수 있다. 무선 통신 모듈은 와이파이(Wifi) 모듈, 와이브로(Wireless broadband) 모듈 외에도, GSM(global System for Mobile Communication), CDMA(Code Division Multiple Access), WCDMA(Wideband Code Division Multiple Access), UMTS(universal mobile telecommunications system), TDMA(Time Division Multiple Access), LTE(Long Term Evolution) 등 다양한 무선 통신 방식을 지원하는 무선 통신 모듈을 포함할 수 있다. 사용자 단말의 제어부는 제4 의료 이미지를 생성하는 동작을 수행한다. 구체적으로 제어부는 사용 자가 생성할 제4 의료 이미지의 종류 및 형태에 관한 정보를 입력할 수 있는 어플리케이션을 디스플레이를 통해 출력한다. 어플리케이션은 이하의 다른 도면에서 후술하는 설정 창 및 의료 이미지가 표시되는 형태로, 장기 또는 치아 등 다양한 조직의 형태 및 이미지의 종류에 관한 정보를 입력받도록 사용자를 유도한다. 사용자는 설정 창을 통해 서 원하는 유형이 선택되도록, 교익, 치근단, 파노라마 방사선사진 및 두부규격 방사선사진 중 어느 하나에 대 한 클릭을 통하여 의료 이미지의 종류 및 형태를 설정할 수 있다. 제어부는 설정 창을 통해 입력받은 정보에 기초하여, 흉부, 유방, 및 복부 촬영 방사선사진, 뼈와 관련된 사진 및 척추 방사선사진, 치과와 관련된 교익, 치근단, 파노라마 방사선사진 및 두부규격 방사선사진 중 어느 하나를 포함하는 제4 의료 이미지를 생성할 수 있다. 제4 의료 이미지에 포함되는 의료 용 이미지는 반드시 전 술한 조직 및 뼈 등에 제한되는 것은 아니다. 개시된 일 실시예에 따라, 제어부는 의료 이미지의 종류 및 형태에 관한 정보와, 학습된 인공 신경망에 기 초하여 제3 의료 이미지의 일부에 바운딩 박스를 생성한다. 여기서 제3 의료 이미지는, 제4 의료 이미지의 바탕 이 되는 의료 이미지이다. 개시된 다른 실시예에서 사용자는 바운딩 박스를 제어부가 생성하도록 동작하는 자동 모드 대신, 사용자가 직접 바운딩 박스를 그리는 형태로, 바운딩 박스를 생성하는 수동 모드를 선택할 수도 있다. 이 경우, 제어부 는 사용자가 수동으로 입력한 바운딩 박스를 인식하고 학습된 인공 신경망에 기초하여 상기 바운딩 박스의 위치 또는 크기를 조정할 수도 있다. 제어부는 의료 이미지의 종류 및 형태에 관한 정보에 기초하여 서버가 미리 생성한 제2 의료 이미지를 선택한다. 여기서 제2 의료 이미지는 개인정보가 비식별화될 수 있도록, 서버가 미리 데이터베이스에 저장한 이미지로써, 흉부, 유방, 및 복부 촬영 방사선사진, 뼈와 관련된 사진 및 척추 방사선사진, 치과와 관련 된 교익, 치근단, 파노라마 방사선사진 및 두부규격 방사선사진을 해부학적 구조에 의해서 분류된 이미지이다. 제어부는 데이터베이스에 미리 저장된 제2 의료 이미지를 서버로 요청하고, 네트워크부로부터 수신한 제2 의료 이미지를 생성된 바운딩 박스에 삽입함으로써, 제4 의료 이미지를 생성한다. 여기서 제2 의료 이미지는 반드시 서버로부터 수신되는 것 뿐만 아니라, 미리 서버로부터 수신하여 저장부에 저장된 이미지일 수도 있다. 제어부는 제2 의료 이미지가 삽입된 제3 의료 이미지에서 바운딩 박스를 제거한 후, 이미지를 조정 (adjustment) 및 편집(editing)한다. 이 때, 제어부는 서버에서 미리 학습된 인공 신경망을 통해 제2 의료 이미지가 삽입됨으로써 생긴 이미지 내의 부자연스러운 이미지 영역을 탐지하고, 탐지된 부자연스러운 이 미지 영역을 수정할 수 있다. 이미지의 수정 및 편집이 완료되면, 제어부는 학습된 인공 신경망을 통해 제4 의료 이미지에서 삽입된 제2 의료 이미지의 객체 인식을 수행한다. 삽입된 제2 의료 이미지가 인식되면, 제어부는 인식된 제2 의료 이미 지의 외곽을 마스킹하고, 마스킹된 영역을 제3 의료 이미지과 자연스럽게 연결하는 이미지 리터칭(Image retouching)기술을 수행할 수도 있다. 이미지 리터칭은 GAN 모델 또는 Diffusion 모델을 통해 학습된 인공 신경 망을 통해서 수행될 수 있다. 제어부가 수행하는 다양한 이미지 처리와 이에 사용되는 적어도 하나 이상의 인공 신경망은 이하의 다른 도 면을 통해 구체적으로 후술한다. 제어부는 사용자 단말 내 구성요소들의 동작을 제어하기 위한 알고리즘 또는 알고리즘을 재현한 프로그 램에 대한 데이터를 저장하는 메모리(미도시), 및 메모리에 저장된 데이터를 이용하여 전술한 동작을 수행하는 프로세서(미도시)로 구현될 수 있다. 이때, 메모리와 프로세서는 각각 별개의 칩으로 구현될 수 있다. 또는, 메 모리와 프로세서는 단일 칩으로 구현될 수도 있다. 저장부는 제어부의 동작에 필요한 다양한 데이터를 저장하고, 서버로부터 수신한 학습된 인공 신경 망(또는 제어부에 의해서 학습될 수 있는 인공 신경망), 제2 의료 이미지 및 사용자의 입력에 따라 수신하 는 제3 의료 이미지 등 다양한 데이터를 저장한다. 저장부는 캐쉬, ROM(Read Only Memory), PROM(Programmable ROM), EPROM(Erasable Programmable ROM), EEPROM(Electrically Erasable Programmable ROM) 및 플래쉬 메모리(Flash memory)와 같은 비휘발성 메모리 소 자 또는 RAM(Random Access Memory)과 같은 휘발성 메모리 소자 또는 하드디스크 드라이브(HDD, Hard Disk Drive), CD-ROM과 같은 저장 매체 중 적어도 하나로 구현될 수 있으나 이에 한정되지는 않는다. 저장부는 제어부와 관련하여 전술한 프로세서와 별개의 칩으로 구현된 메모리일 수 있고, 프로세서와 단일 칩으로 구 현될 수도 있다. 디스플레이는 제어부에 의해서 표시되는 설정 창, 제3 의료 이미지, 제3 의료 이미지에 생성되는 바운 딩 박스, 바운딩 박스 내에 삽입되는 제2 의료 이미지, 제3 의료 이미지내에 제2 의료 이미지가 삽입된 최종적 인 제4 의료 이미지 등 개시된 의료 이미지 생성 시스템의 결과물을 출력한다. 디스플레이는 디지털 광원 처리(Digital Light Processing: DLP) 패널, 플라즈마 디스플레이 패널(Plasma Display Penal), 액정 디스플레이(Liquid Crystal Display: LCD) 패널, 전기 발광(Electro Luminescence: EL) 패널, 전기영동 디스플레이(Electrophoretic Display: EPD) 패널, 전기변색 디스플레이(Electrochromic Display: ECD) 패널, 발광 다이오드(Light Emitting Diode: LED) 패널, 음극선관(Cathode Ray Tube: CRT), 또 는 유기 발광 다이오드(Organic Light Emitting Diode: OLED) 패널 등으로 마련될 수 있으나, 이에 한정되지는 않는다. 한편, 사용자 단말은 도 2에 도시되지 않은 다양한 구성을 더 포함할 수 있다. 예를 들어 사용자 단말 은 사용자의 입력을 받는 입력부를 더 포함할 수 있다. 여기서 입력부는 유저 입력을 위해 터치 패드(touch pad) 등과 같은 GUI(Graphical User interface), 즉 소프트웨어인 장치를 포함할 수도 있으며, 터치 패드는 터 치 스크린 패널(Touch Screen Panel: TSP)로 구현되어 디스플레이와 상호 레이어 구조를 이룰 수 있다. 서버의 인공지능 학습부는 사용자 단말이 수행하는 합성 데이터, 즉 제4 의료 이미지를 생성하는데 필요한 다양한 인공 신경망을 학습한다. 개시된 일 실시예에서 서버는 적어도 4가지의 인공 신경망을 다양한 방법 또는 모델로 학습시킬 수 있다. 먼저, 제2 의료 이미지를 생성하기 위해서 서버는 개인정보를 식별할 수 있는 제1 의료 이미지로부터 해부 학적 구조물을 분류한다. 이 때 서버는 제1 의료 이미지에서 해부학적 구조물을 분류할 수 있는 인공 신경 망(이하 제1 인공 신경망)을 인공지능 학습부를 통해 학습시킨다. 일 예로, 인공지능 학습부는 segmentation 모델 중 일부인 Unet 기반 모델 또는 YoLo 모델 등 을 이용하여 해부학적 구조물을 자동으로 분할 (segment) 및 객체 인식(object detection)하는 제1 인공 신경망을 학습시킬 수 있다. 학습된 제1 인공 신경망을 통해 분류되어 객체화된 제2 의료 이미지가 데이터베이스에 저장된 후, 사용자 단말은 제2 의료 이미지를 삽입함으로써 제4 의료 이미지를 생성한다. 제4 의료 이미지를 생성하기 위해서 사용자 단말은 제2 인공 신경망을 통해 제3 의료 이미지에 바운딩 박스를 생성한다. 즉, 제2 인공 신경망은 사용자가 입력하는 제3 의료 이미지에서 적절한 위치에 바운딩 박스를 생성할 수 있도록 훈련된다. 인공지능 학 습부는 사용자가 입력할 수 있는 다양한 의료 이미지의 종류 및 형태에 따라, 제3 의료 이미지에 적절한 위 치 및 크기로 바운딩 박스가 생성될 수 있도록, 제2 인공 신경망을 학습시킨다. 일 예로, 인공지능 학습부는 다양한 의료 이미지에서 적절한 위치에 바운딩 박스가 생성되도록 위치 및 크 기를 지정하는 라벨링 방식을 통해서 제2 인공 신경망을 학습시킬 수 있다. 다른 예로, 인공지능 학습부는 제2 인공 신경망이 바운딩 박스를 생성한 후, 인간이 스스로 바운딩 박스의 위치를 조정해주는 명령어를 입력하 는 피드백 방식, 즉 HITL (Human In The Loop) 방식으로 제2 인공 신경망을 학습시킬 수도 있다. 사용자 단말은 제2 의료 이미지가 삽입된 제4 의료 이미지가 자연스러운 의료 이미지가 될 수 있도록, 이미 지 수정 및 편집을 진행한다. 여기서 사용자 단말은 제3 인공 신경망을 사용한다. 일 예로, 인공지능 학습부는 자연스러운 이미지와 부자연스러운 이미지를 라벨링한 학습 데이터를 통해 제3 인공 신경망이 이미지 수정 및 편집하는 방법을 학습시키거나, 다른 예로 HITL 방식을 통해 이미지 수정 및 편 집 방법을 학습시킬 수도 있다. 사용자 단말은 이미지 수정 및 편집한 후, 이미지 리터칭을 수행한다. 구체적으로 사용자 단말은 인공지능 학습부를 통해 객체 인식하는 방법을 학습한 제1 인공 신경망을 통 해서 이미지 리터칭을 수행할 제2 의료 이미지를 인식한다. 즉, 이미지 리터칭을 수행하기 전, 사용자 단말(1 0)은 서버에서 학습한 제1 인공 신경망을 사용하여 제4 의료 이미지에서 삽입된 제2 의료 이미지를 다시 한 번 객체 인식을 수행할 수 있다. 이미지 리터칭은 객체 인식된 제2 의료 이미지의 외각을 따라 일정 두께의 마스크 데이터를 생성(마스킹)하는 작업을 포함한다. 이미지 리터칭은 마스킹된 부분을 제2 의료 이미지 주변의 제3 의료 이미지의 픽셀 값에 기초 하여 채워나감으로써 수행된다. 인공지능 학습부는 이러한 이미지 인페인팅을 수행하는 제4 인공 신경망을 GAN 또는 Diffusion 방식을 통해 학습시킨다. 한편, 전술한 복수 개의 인공 신경망의 학습 모델은 다양할 수 있다. 예를 들어, 제1 인공 신경망은 이미지 전 체를 분류하는 이미지 분류(Image Classification)와 달리, 제1 의료 이미지 내의 해부학적 구조물을 분류하는 이미지 세그멘테이션(Image Segmentation)이므로, Unet 모델을 주로 사용할 수 있다. Unet 모델은 오토인코더(Autoencoder)와 같은 인코더-디코더(Encoder-Decoder) 기반 모델에 속하는 것으로, 인 코딩 단계의 각 레이어에서 얻은 특징을 디코딩 단계의 각 레이어에 합치는(Concatenation) 방법을 통해서 이미 지의 특징을 추출함과 동시에 정확한 위치를 파악하는 모델이다. 그러나 개시된 실시예에서 제1 인공 신경망을 학습하는 모델이 반드시 Unet 모델만 한정되는 것은 아니며, YoLo 또는 Unet 모델 이외의 다양한 Segmentation 모델을 사용할 수도 있다 제2 인공 신경망과 제3 인공 신경망이 사용하는 HITL(Human in the Loop) 방식은 인공 신경망이 학습한 결과를 인간이 지도하는 방식으로, 사용자가 직접 인공 신경망의 분류 결과의 피드백을 제공하는 방법이다. 이러한HITL 방식은 대화방식을 학습하는 생성 AI(Generative Artificial Intelligence)을 통해서도 수행될 수 있다. GAN(Generative adversarial network)은 새로운 샘플을 생성하기 위한 딥러닝(Deep Learning) 모델로 생성자 (Generator)와 구분자(Discriminator) 두 개의 신경망으로 구성되는 모델이다. 생성자는 입력 데이터에서 새로 운 샘플을 생성하려고 하고, 구분자는 생성자가 생성한 샘플이 진짜인지 가짜인지를 구분하기 위해 사용된다. GAN은 경쟁 관계를 이루는 생성자와 구분자가 계속해서 각각을 최적화하는 과정을 거친다. 생성자가 새로운 샘 플을 생성할 때마다, 구분자는 진짜와 가짜를 구분하기 위해 업데이트된다. 이 과정은 생성자가 새로운 샘플을 생성할 때마다 계속되고, 이렇게 경쟁 관계를 이루는 생성자와 구분자가 계속해서 서로를 최적화하게 되면, 생 성자는 진짜와 구별할 수 없는 새로운 샘플을 생성하게 된다. Diffusion 모델은 새로운 샘플을 생성하기 위한 딥러닝 모델로 data에 노이즈를 더해가는 forward diffusion process와 노이즈가 있는 이미지에서 data를 생성해내는 reverse denoising process로 이루어져 있으며, 이 프 로세서들을 통해 새로운 데이터를 만들면서, 신경망을 학습하는 모델이다. 개시된 인공지능 학습부는 각각의 인공 신경망을 서로 다른 모델을 통해 학습하는 것으로 설명하였으나, 반 드시 이에 한정되는 것은 아니다. 각각의 인공 신경망이 출력해내는 결과물의 우수성을 위해서 다양한 업데이트 모델 또는 다른 모델로 학습될 수도 있다. 이미지 처리부는 제1 의료 이미지에서 객체 인식된 제2 의료 이미지를 분리하여 데이터베이스에 저장한 다. 이미지 처리부는 학습된 제1 인공 신경망을 통해서 다양한 의료 이미지로부터 필요한 해부학적 구조물 을 구분한 후, 사용자 단말에서 생성될 바운딩 박스의 모양을 포함하도록 제2 의료 이미지를 생성할 수 있 다. 또한, 이미지 처리부는 분류된 해부학적 명칭으로 생성된 제2 의료 이미지를 라벨링한 후, 이를 데이터 베이스에 나누어 저장하는 역할을 수행한다. 데이터베이스는 이미지 처리부가 수행한 해부학적 구조물을 바운딩 모양과 함께 구분하여 저장하고, 사 용자 단말의 요청에 의해서 선택된 제2 의료 이미지를 로딩한다. 데이터베이스는 사용자 단말의 저 장부에서 언급한 다양한 형태의 하드웨어적 구성으로 마련될 수 있으며, 서버와 일체로 구성될 수도 있 다. 이미지 처리부 및 데이터베이스가 제1 의료 이미지로부터 제2 의료 이미지를 구분하여 분류하는 실시예 는 이하 다른 도면을 통해 후술한다. 도 2에 도시된 의료 생성 시스템의 구성들은 전술한 성능에 대응하여 적어도 하나의 구성요소가 추가되거나 삭제될 수 있다. 또한, 구성들의 상호 위치는 시스템의 성능 또는 구조에 대응하여 변경될 수 있다는 것은 당해 기술 분야에서 통상의 지식을 가진 자에게 용이하게 이해될 것이다. 한편, 도 2에서 도시된 각각의 구성요소는 소프트웨어 및/또는 Field Programmable Gate Array(FPGA) 및 주문형 반도체(ASIC, Application Specific Integrated Circuit)와 같은 하드웨어 구성요소를 의미할 수 있다. 도 3은 개시된 의료 생성 방법을 설명하기 위한 순서도이다. 도 3을 참조하면, 의료 생성 방법은 의료 이미지 획득 및 인공지능의 학습 단계와 비식별화된 의료 이미지 생성 단계를 포함한다. 구체적으로 의료 이미지 획득 및 인공지능의 학습 단계는 주로 서버에서 동작한다. 본 단계에서 학습 에 필요한 의료 이미지는 환자가 공개를 동의하거나 의학연구윤리위원회의 승인을 받은 방사선사진을 의원 또는 병원같은 진료 기관, 의학 교육기관 등에서 제공받을 수 있다. 인공지능의 학습 단계는 제1 인공 신경망 내지 제4 인공 신경망을 학습하는 단계를 의미한다. 비식별화된 의료 이미지 생성 단계는 사용자 단말에서 수행되는 의료 이미지 생성방법이다. 비식별화 된 의료 이미지 생성 단계는 사용자 단말이 출력하는 설정 창을 통해서 사용자가 생성할 의료 이미지 의 종류 및 형태에 관한 정보를 수신한 후, 의료 이미지의 종류 및 형태에 관한 정보와 제2 인공 신경망에 기초 하여 제3 의료 이미지에 바운딩 박스를 생성한다. 이후, 비식별화된 의료 이미지 생성 단계는 제2 의료 이 미지를 바운딩 박스에 삽입한 후, 제2 의료 이미지가 삽입된 제3 의료 이미지, 즉 제4 의료 이미지를 생성한다. 비식별화된 의료 이미지 생성 단계는 미리 학습된 제1 인공 신경망, 제3 인공 신경망 또는 제4 인공 신경 망에 기초하여 제4 의료 이미지를 수정 및 편집을 수행하고, 개인 정보가 식별되지 않은 합성 데이터를 생성한 다.한편, 각 단계 중, 비식별화된 의료 이미지 생성 단계는 사용자의 의도에 따라 자동 모드 및 수동 모드로 나누어 동작할 수 있다. 예를 들어, 수동 모드에서 사용자는 사용자 단말을 대신해 바운딩 박스를 직접 제3 의료 이미지에 그리거나 생성할 수 있다. 또한, 수동 모드에서 사용자 단말은 제3 인공 신경망의 출력값을 사용하지 않고, 바운딩 박스 내에 삽입된 제2 의료 이미지의 크기 및 위치를 직접 조정함으로써, 이미지를 수정 또는 편집할 수도 있다. 다만, 이러한 수동 모드는, 사용자의 편의 또는 니즈를 충족시키기 위함이며, 개시된 의료 생성 방법은 사용자의 입력을 최소화하면서, 다양한 합성 데이터를 생성시킬 수 있다. 도 4는 도 3의 의료 이미지 획득 및 인공지능의 학습 단계를 구체적으로 설명하기 위한 순서도이다. 도 5는 제2 의료 이미지를 분류하는 방법을 설명하기 위한 도면이다. 도 6은 분류된 제2 의료 이미지가 생성되는 방법을 설 명하기 위한 도면이다. 도 7은 분류된 제2 의료 이미지가 데이터베이스에 저장되는 방법을 설명하기 위한 도면 이다. 중복되는 설명을 피하기 위해서 이하 함께 설명한다. 도 4를 먼저 참조하면, 서버는 제1 인공 신경망을 학습시킨다.. 도 2 등에서 전술한 바와 같이, 제1 인공 신경망은 이미지 내에서 객체를 인식하기 위한 segmentation 모델 중, Unet 모델을 통해서 학습되고, 구현될 수 있다. 도 5를 참조하면, 복수 개의 제1 의료 이미지(도 5의 111)은 사람의 치아 및 잇몸을 포함하는 방사선 사진일 수 있다. 복수 개의 제1 인공 신경망은 서버가 제공하는 다양한 사람의 치아 사진을 학습 데이터로 이용 되며 도 5의 113과 같은 치아 분류를 수행하는 법을 학습할 수 있다. 도 4를 참조하면, 서버는 제1 의료 이미지에 포함된 해부학적 구조를 분류하고, 복수 개의 제2 의료 이미지에 대한 객체 인식을 수행한다. 서버가 제1 의료 이미지의 해부학적 구조물을 분류하는 방법은 학습된 제1 인공 신경망을 통해서 수행된다. 도 6을 참조하면, 학습된 제1 인공 신경망은 학습에서 사용되지 않은 새로운 제1 의료 이미지에서 각각의 치아를 분류하고, 작은 어금니, 큰 어금니, 사랑니, 소구치, 대구치 등을 분류할 수 있다. 해부학적 구조를 분류한 후 객체 인식을 수행하면, 도 6의 제1 의료 이미지와 같이, 서버는 제1 의료 이미지에 각각의 치아를 인식할 수 있다. 객체 인식의 결과는 도 6의 제1 의료 이미지에서 표시된 대 로, 미리 설정된 데이터 표시값(-17_BWX 등)으로 표시될 수 있으며, 분류된 결과를 표시하는 바운딩 모양이 중 복되어 표시될 수도 있다. 도 4를 참조하면, 서버는 제1 의료 이미지에서 제2 의료 이미지를 분리한다. 일 예로, 서버는 객체 인식이 수행된 도 6의 제1 의료 이미지에서 둘째 큰 어금니를 분리할 수 있다. 이 때, 서버는 둘째 큰 어금니를 분리하면서, 바운딩 모양을 그대로 유지한다. 다만, 서버 는 바운딩 모양과 제2 의료 이미지을 분리하면서, 제1 의료 이미지로부터 포함된 여백 이미 지를 삭제한다. 도 4를 참조하면, 서버는 분리된 제2 의료 이미지를 데이터베이스에 저장한다. 도 7를 참조하면, 서버는 여백 이미지를 제거하고 바운딩 모양을 포함하는 제2 의료 이미지를 데이터 베이스에 저장한다. 도 7에서 도시된 각각의 폴더는 데이터베이스에 저장된 제2 의료 이미지들을 UI(User Interface)으로 표현한 예시이다. 도 7의 예시와 같이, 데이터베이스에 저장된 복수 개의 제2 의료 이미지들은 제1 인공 신경망으로부터 추론된 해부학적 명칭으로 라벨링되어 저장될 수도 있다. 한편, 서버가 분류하여 저장시킨 복수 개의 제2 의료 이미지들은 사용자 단말로 전송되어, 도 8 등에서 설명하는 제4 의료 이미지를 생성하는데 사용된다. 도 8은 도 3의 비식별화된 의료 이미지 생성 단계를 구체적으로 설명하기 위한 순서도이다. 도 9는 설정 창을 출력하는 사용자 단말의 동작을 설명하기 위한 도면이다. 도 10은 제3 의료 이미지에서 바운딩 박스가 생성되는 동작을 설명하기 위한 도면이다. 도 11은 제3 의료 이미지에서 제4의료 이미지가 생성되는 동작을 설명하기 위 한 도면이다. 중복되는 설명을 피하기 위해서 이하 함께 설명한다. 도 8을 참조하면, 사용자 단말은 제3 의료 이미지를 수신한다. 사용자 단말은 사용자 또는 외부로부터 수신한 제3 의료 이미지를 디스플레이를 통해 출력한다. 제3 의 료 이미지는 제4 의료 이미지의 기초가 되는 도면으로, CT 사진, MRI 사진, 초음파 사진 및 방사선사진 등 다양한 형태를 포함한다. 사용자 단말은 제1 설정 창을 출력한다. 도 9 내지 도 11을 참조하면, 사용자 단말은 제3 의료 이미지를 디스플레이를 통해 출력하면서, 설정 창을 함께 출력할 수 있다. 일 예에 따라, 설정 창은 제4 의료 이미지의 자동 합성 또는 수동 생성을 설정하는 제1 부분 설정 창(도번 미도시), 생성할 제4 의료 이미지의 개수를 지정하는 제2 부분 설정 창(도번 미도시), 제2 의료 이미지의 종류 를 설정하는 제3 부분 설정 창(도번 미도시), 생성하고자 하는 제2 의료 이미지 또는 해부학적 구조물을 데이터 베이스에서 선택하거나 드래그 & 드롭으로 위치를 설정하는 제4 부분 설정 창(도 10 참조) 및 삽입된 제2 의학 영상 이미지를 자연스럽게 변환시키기 위하여, 이미지의 수정 또는 편집 여부를 결정하는 제5 부분 설정 창(도 11 참조)를 포함할 수 있다. 사용자는 설정 창에서 출력되는 다양한 UI에 입력 명령(도 9 내지 도 11의 체크 표시)을 수행함으로써, 사 용자 단말의 동작에 관여할 수 있다. 한편, 도 9 내지 도 11에서 도시된 설정 창은 일 예에 불과하고, 다양한 형태의 UI를 포함할 수 있다. 도 8을 참조하면, 사용자 단말은 의료 이미지의 종류 및 형태에 관한 정보에 대한 사용자 입력을 수신한다 . 도 9의 예시와 같이, 사용자는 삽입할 제2 의료 이미지의 종류(the type of image)를 Bitewing으로 선택하고, 도 10의 예시와 같이, 사용자는 삽입할 제2 의료 이미지(image of darg and drop)를 선택한 후, 삽입될 위치 (location or surface를 Occlusal으로 선택할 수 있다. 이렇게 선택된 의료 이미지의 종류 및 형태에 관한 정보 는 사용자 단말이 선택할 제2 의료 이미지의 기초가 되며, 바운딩 박스가 삽입될 위치 및 크기의 기초가 된 다. 한편, 사용자는 설정 창에서 수동 모드 또는 자동 모드를 선택할 수 있으며, 도 8의 241 이하의 순서는 자 동 모드에 따른 동작 방법이며, 도 8의 251 이하의 순서는 수동 모드에 따른 동작 방법이다. 먼저 자동 모드를 설명하면, 사용자 단말은 제2 인공 신경망을 통해 바운딩 박스를 생성한다. 도 10을 참조하면, 사용자 단말은 미리 학습된 제2 인공 신경망과 의료 이미지의 종류 및 형태에 관한 정보 에 기초하여 제3 의료 이미지의 치아에 Amalgam이 삽입될 바운딩 박스를 생성할 수 있다. 여기서 제2 인 공 신경망은 서버에서 라벨링을 통해서 미리 Amalgam이 삽입될 위치를 학습하거나, HITL 방식을 통해 바 운딩 박스를 생성할 위치를 학습한 상태이다. 사용자 단말은 사용자가 설정 창을 통해 입력한 의료 이 미지의 종류 및 형태에 관한 정보 및 제3 의료 이미지를 입력값으로 학습된 제2 인공 신경망을 통해 적절한 위 치의 바운딩 박스를 생성하는 추론을 수행한다. 한편, 도 10에서 도시된 바운딩 박스는 네모 형태로 도시되었다. 다만, 생성되는 바운딩 박스의 모양이 반드시 네모 모양으로만 한정되는 것은 아니고, 다각형 등 다양한 변형례가 포함될 수 있다. 도 8을 참조하면, 사용자 단말은 데이터베이스 내에 저장된 제2 의료 이미지를 선택한다. 사용자 단말이 제2 의료 이미지를 선택하는 기준은 미리 저장된 통계적 기준과 사용자가 입력한 의료 이미 지의 종류 및 형태에 관한 정보이다. 다만, 데이터베이스에 미리 저장된 여러 제2 의료 이미지 중 하나의 제2 의료 이미지를 선택하는 것은, 난수 등에 의해 랜덤하게 선택될 수 있다. 이에 관한 구체적인 설명은 도 13 에서 후술한다. 사용자 단말은 선택된 제2 의료 이미지를 제3 의료 이미지 내의 바운딩 박스에 삽입한다. 도 6 에서 전술한 바와 같이, 데이터베이스에 저장된 제2 의료 이미지는 해부학적 구조물 뿐만 아니라, 바 운딩 모양과 함께 저장된다. 사용자 단말이 제2 의료 이미지를 선택하면, 바운딩 모양도 함께 선택되며, 바 운딩 모양은 제3 의료 이미지에 생성된 바운딩 박스의 크기 및 각도에 맞게 변경된다. 사용자 단말은 조정 된 바운딩 모양과 함께, 변경된 제2 의료 이미지를 제3 의료 이미지의 바운딩 박스에 삽입시킨다. 도 11을 참조하면, 사용자 단말은 바운딩 박스(도 10의 212)에 선택된 Amalgam이미지를 삽입한다. 일 예로, 사용자 단말이 삽입한 Amalgam은 바운딩 박스 내에 삽입되나, 도 11과 같이 반듯한 네모 모양이 아닌 미리 제2 의료 이미지로 저장된 가운데가 찌그러진 형태로 삽입될 수 있다.사용자 단말은 제3 인공 신경망을 통해 제4 의료 이미지를 수정한다. 구체적으로 제3 인공 신경망은 바운딩 박스가 삭제된 제4 의료 이미지내에서 부자연스러운 이미지 영역을 탐지 하고, 탐지된 영역의 이미지를 수정할 수 있다. 구체적으로 제3 인공 신경망은 서버에서 미리 바운딩 박스를 삽입하면서 발생할 수 있는 부자연스러운 이미 지를 라벨링 또는 HITL 방식을 통해 학습한 상태이다. 도 11과 같이, Amalgam이 삽입된 제3 의료 이미지에서 제 3 인공 신경망은 Amalgam 이 삽입되었으나 부자연스러운 영역을 추론한 후, 이미지를 수정 또는 편집할 수 있다. 사용자 단말은 제4 인공 신경망을 통해 이미지 리터칭을 수행한다. 도 8의 245 단계에서 제3 인공 신경망을 통해 이미지를 수정하였으나, 사용자는 추가적인 이미지 리터칭 기술을 통해 자연스러운 제4 의료 이미지(도 11의 213)의 생성을 원할 수 있다. 도 11과 같이, 사용자가 설정 창 을 통해 이미지의 추가 수정에 대한 동작 명령을 입력하였다면, 사용자 단말은 제4 인공 신경망을 통해 이 미지 리터칭을 실행한다. 구체적으로 사용자 단말은 제1 인공 신경망을 통해 제4 의료 이미지 내에 객체 인식을 다시 수행한 후, 삽 입된 제2 의료 이미지의 외곽을 마스킹한다. GAN 모델 또는 Diffusion 모델을 통해 미리 학습된 제4 인공 신경 망은 마스킹된 영역을 이미지 인페인팅 기술로 채워서 이미지 리터칭을 실행할 수 있다. 이미지 리터칭 동작이 완료되면, 사용자 단말은 제4 의료 이미지를 출력한다. 사용자가 수동 모드를 선택하면, 사용자 단말은 사용자의 입력에 의해서 바운딩 박스를 생성한다. 자동 모드와 비교하면, 사용자 단말은 제2 인공 신경망을 통해서가 아니라, 사용자가 그리는 바운딩 박스를 제3 의료 이미지 상에 출력할 수 있다. 사용자가 데이터베이스 내에 저장된 제2 의료 이미지를 선택한다. 본 단계에서 사용자 단말에 데이터베이스에 저장된 복수 개의 제2 의료 이미지를 데이터 형태로 전송받 은 후, 사용자가 인식할 수 있도록 폴더 별 또는 다른 UI를 통해 출력하여 사용자에게 선택을 유도할 수 있다. 사용자 단말은 선택된 제2 의료 이미지를 제3 의료 이미지 내의 바운딩 박스에 삽입한다. 개시된 일 실시예에서는 수동 모드에서도 사용자 단말이 사용자에 의해서 선택된 제2 의료 이미지를 제3 이 미지 내의 바운딩 박스에 삽입한다. 구체적으로 수동 모드에서 사용자는 드레그 앤 드롭 방식으로 제3 의료 이 미지에 제2 의료 이미지를 끌어올 수 있다. 일 예로, 사용자가 마우스를 통해서 제2 의료 이미지를 바운딩 박스 내에 끌고온 후, 클릭을 해제하면, 사용자 단말은 제3 의료 이미지 내의 바운딩 박스의 위치 및 크기에 맞 춰 제2 의료 이미지와 바운딩 모양을 변경한 후, 제2 의료 이미지를 삽입할 수 있다. 사용자 단말은 제2 설정 창를 출력한다. 여기서 제2 설정 창은은 사용자에게 사용자 단말이 이미지 수정 또는 편집을 진행할지 여부에 대한 UI를 의 미한다. 일 예로, 제2 설정 창은 도 11에서 도시된 제5 부분 설정 창일 수 있다. 다른 예로, 제2 설정 창은, 이 미지 수정에 필요한 다양한 도구를 제공하는 UI일 수 있다. 사용자 단말은 사용자가 선택하는 다양한 이미 지 도구에 기초하여 이미지 수정을 진행할 수도 있다. 사용자가 이미지 부분의 수정을 진행한다. 여기서 이미지의 수정은, 자동 모드에서 제3 인공 신경망을 통해서 제공하는 이미지의 수정 또는 편집과 다르다. 사용자는 디스플레이에 출력되는 이미지 처리에 대한 다양한 도구(UI)를 통해서 직접 제2 의료 이 미지가 삽입된 제4 의료 이미지를 수정할 수 있다. 사용자 단말은 이미지 리터칭 기술로 제4 의료 이미지를 수정한다. 수동 모드에서도 사용자 단말은 제1 인공 신경망 및 제4 인공 신경망에 기초하여 자동으로 이미지 리터칭을 수행할 수도 있다. 다만, 사용자는 수동 모드에서 도 11의 제5 부분 설정 창을 통해 사용자의 입력 명령을 수신 한 경우에만 이미지 리터칭을 수행할 수 있다. 246 단계에서 제공하는 제4 인공 신경망 또한, GAN 모델 또는 Diffusion 모델을 통해 서버에서 미리 학습된 것이며, 사용자 단말은 제4 의료 이미지 내에서 제2 의료 이미지를 인식 한 후, 이미지 리터칭을 수행할 수있다. 이미지 리터칭에 대한 구체적인 동작은 도 15를 통해서 후술한다. 이미지 수정이 완료되면, 사용자 단말은 제4 의료 이미지를 출력한다. 도 12 내지 도 14는 제2 의료 이미지를 선택 및 삽입하는 일련의 동작을 설명하기 위한 도면이다. 중복되는 설 명을 피하기 위해서 이하 함께 설명한다. 도 12를 먼저 참조하면, 사용자 단말은 설정 창을 통해 입력된 사용자가 생성할 의료 이미지의 종류 및 형 태에 관한 정보에 기초하여 제2 의료 이미지를 선택한다. 사용자 단말은 매복 사랑니를 제2 의료 이미지로 선택할 수 있다. 다만, 데이터베이스에 저장되는 제2 의료 이미지는 도 12에서 도시된 바와 같이, 바운딩 모양을 함께 저장하고 있으며, 바운딩 모양 에는 여백 이미지가 함께 포함될 수 있다. 한편, 자동 모드에서 사용자 단말은 제2 의료 이미지를 선택할 때, 미리 저장된 통계적 수치에 기초하 여 제2 의료 이미지를 선택할 수 있다. 도 13은 개시된 일 예에 따른 매복 사랑니의 위치와 관련된 표로써, 위턱(Maxillary)와 아래턱(Mandibular)에서 매복 사랑니가 위치하는 사람의 분포를 각도 및 형태(Vertical, Mesioangular, Horizontal, Distoangular, Inverted, Buccolingual)로 구분하여 조사된 통계적 자료이다. 도 13에서 수직(Vertiacal)으로 부분 매복치 (Partially Imapcted)가 의료 이미지를 형성할 확률은 104명으로 전체 156명 중 66.7%를 차지한다. 사용자 단말 은 설정 창에서 선택된 제2 의료 이미지의 종류 및 형태가 위턱의 매복 사랑니(Maxillary third molars)로 선택된 경우, 통계적 수치에 기초하여 Vertialcal(66.7%), Mesioangular(30.1%) 또는 Distoangular(3.2%) 중에 서 하나의 부분 매복치(Partially impacted)를 선택할 수 있다. 한편, 도 13은 통계적 수치의 일 예에 불과하며, 다양한 기준 및 통계표가 저장될 수 있다. 다시 도 12를 참조하면, 사용자 단말은 사용자가 입력한 의료 이미지의 종류 및 형태에 관한 정보에 기초하 여 제3 의료 이미지 내에 바운딩 박스를 생성한다. 일 예로, 사용자는 의료 이미지의 종류 및 형태에 관한 정보로써, 아래턱의 매복 사랑니(Mandibular third molars)를 입력할 수 있으며, 사용자 단말은 제2 의료 신경망에 기초하여 제3 의료 이미지의 왼쪽 하 단에 바운딩 박스를 생성할 수 있다. 여기서 사용자 단말은 의료 이미지의 종류가 매복 사랑니 (Mandibular third molars)인 것을 감안하여, 바운딩 박스의 일부가 제3 의료 이미지를 벗어나도록 생성할 수도 있다. 도 14를 참조하면, 사용자 단말은 바운딩 박스 내에 제2 의료 이미지를 삽입한다. 사용자 단말은 선택된 제2 의료 이미지를 삽입할 때, 바운딩 모양의 각도 및 크기를 바운딩 박스 에 맞추고, 제2 의료 이미지를 삽입한다. 다만, 사용자 단말은 제2 의료 이미지를 삽입할 때, 제3 의료 이미지를 벗어나는 제2 의료 이미지의 영역을 삭제하면서, 제2 의료 이미지를 바운딩 박스 에 삽입한다. 또한, 사용자 단말은 바운딩 모양 내에 포함된 여백 이미지를 제3 의료 이미지의 바운딩 박스 내에 이미지 픽셀과 동기화시킴으로써, 자연스러운 합성 데이터가 생성될 수 있도록 한다. 도 15는 이미지 리터칭 기술을 구체적으로 설명하기 위한 도면이다. 도 15를 참조하면, 사용자 단말은 삽입한 부분의 객체 인식 및 인식된 객체를 마스킹한다. 구체적으로 사용자 단말은 생성된 제4 의료 이미지에서 제1 인공 신경망을 통해 삽입된 의료 이미지의 객체 인식을 수행한다. 도 15의 일 예에서와 같이, 사용자 단말이 아래턱의 매복 사랑니(Mandibular third molars)의 객체 인식을 완료하면, 사용자 단말은 아래턱의 매복 사랑니의 외곽을 미리 설정된 두께로 마스킹한다. 사용자 단말은 이미지 수정 또는 리터칭을 수행한다. 구체적으로 사용자 단말은 제4 인공 신경망을 통해서 마스킹된 매복 사랑니의 영역을 제4 의료 이미지 의 주변 픽셀로 채우는 이미지 인페인팅을 수행할 수 있다. 여기서 이미지 인페인팅은 이미지 리터칭 기술 의 일 예로, GAN 모델 또는 Diffusion 모델을 통해 학습된 제4 인공 신경망을 통해 실행될 수 있다.이러한 이미지 수정을 통해서 개시된 의료 이미지 생성 시스템은, 자연스럽고, 개인 정보가 비식별화되면서 동 시에 높은 수준의 의료 합성 데이터를 생성할 수 있다. 도 16은 치아 이외의 다른 의료 이미지 중 팔 골절과 관련된 합성 데이터를 생성하는 일 예를 설명하기 위한 도 면이다. 사용자 단말은 제3 의료 이미지를 사용자 또는 외부로부터 수신한다. 도 16에서 도시된 바와 같이, 제3 의료 이미지는 골절이 없는 일반적인 의료 이미지일 수 있다. 사용자 단말은 바운딩 박스를 생성한다 사용자 단말은 골절 영상을 삽입하기 위해 사용자가 입력한 의료 이미지의 종류 및 형태에 관한 정보와 제2 인공 신경망을 이용한다. 사용자 단말은 제2 인공 신경망의 추론 결과에 기초하여 제3 의료 이미지에 서 골절이 일어날 수 있는 위치에 바운딩 박스를 생성한다. 바운딩 박스가 생성된 후, 사용자 단말은 팔 골절 합성 이미지를 생성한다. 사용자 단말은 바운딩 박스가 생성된 위치와 생성될 골절에 적합한 제2 의료 이미지를 선택한 후, 바 운딩 박스에 제2 의료 이미지를 삽입한 후, 바운딩 박스를 제거한 제4 의료 이미지을 생성한다. 새로운 합성 데이터, 즉 제4 의료 이미지가 생성된 후에도, 사용자 단말은 설정 창으로부터 입력되는 사용자의 입력 명령에 따라, 인공 신경망을 통해서 이미지 수정 및 편집을 추가적으로 진행할 수도 있다. 이를 통해서 개시된 의료 이미지 생성 시스템은 공개되는 의료 영상 이미지로부터 신원을 유추하는 것이 원천 차단되도록, 개인정보 식별이 불가능한 2차원 의학 영상 이미지 합성 데이터를 생성함으로써, 개인정보를 상당 한 수준으로 보호할 수 있으며, 충분한 양의 개인정보가 비식별화된 인공지능 학습용 의료 영상 이미지 합성 데 이터를 확보할 수 있으며, 이를 통하여, 의료 기기에 적용되는 인공지능의 성능을 향상시킬 수도 있다."}
{"patent_id": "10-2023-0041884", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1 은 개시된 의료 이미지의 생성 시스템의 개략적인 구성을 설명하기 위한 도면이다. 도 2는 의료 이미지의 생성 시스템의 각 구성에 대한 제어 블록도이다. 도 3은 개시된 의료 생성 방법을 설명하기 위한 순서도이다. 도 4는 도 3의 의료 이미지 획득 및 인공지능의 학습 단계를 구체적으로 설명하기 위한 순서도이다. 도 5는 제2 의료 이미지를 분류하는 방법을 설명하기 위한 도면이다. 도 6은 분류된 제2 의료 이미지가 생성되는 방법을 설명하기 위한 도면이다. 도 7은 분류된 제2 의료 이미지가 데이터베이스에 저장되는 방법을 설명하기 위한 도면이다. 도 8은 도 3의 비식별화된 의료 이미지 생성 단계를 구체적으로 설명하기 위한 순서도이다. 도 9는 설정 창을 출력하는 사용자 단말의 동작을 설명하기 위한 도면이다. 도 10은 제3 의료 이미지에서 바운딩 박스가 생성되는 동작을 설명하기 위한 도면이다. 도 11은 제3 의료 이미지에서 제4의료 이미지가 생성되는 동작을 설명하기 위한 도면이다. 도 12 내지 도 14는 제2 의료 이미지를 선택 및 삽입하는 일련의 동작을 설명하기 위한 도면이다. 도 15는 이미지 리터칭 기술을 구체적으로 설명하기 위한 도면이다. 도 16은 치아 이외의 다른 의료 이미지 중 팔 골절과 관련된 합성 데이터를 생성하는 일 예를 설명하기 위한 도 면이다."}
