{"patent_id": "10-2021-0128340", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0059908", "출원번호": "10-2021-0128340", "발명의 명칭": "데이터 처리 장치 및 데이터 처리 방법", "출원인": "주식회사 메디트", "발명자": "이동훈"}}
{"patent_id": "10-2021-0128340", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "구강 이미지 처리 방법에 있어서,치아 영역이 보이는 객체의 얼굴 이미지를 제1 이미지로 획득하는 단계; 상기 제1 이미지에 포함된 상기 치아 영역을 가상 치아 영역으로 대체하여 제2 이미지를 획득하는 단계; 및입력된 얼굴 이미지로부터 상기 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하도록 학습된 신경망을 이용하여, 상기 제2 이미지로부터 상기 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴을 포함하는 제3 이미지를 획득하는 단계를 포함하는, 구강 이미지 처리 방법."}
{"patent_id": "10-2021-0128340", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴은 상기 제2 이미지에 포함된 얼굴과포즈, 표정, 치아 영역이 보이는 정도, 스타일 중 적어도 하나가 다른 얼굴인, 구강 이미지 처리 방법."}
{"patent_id": "10-2021-0128340", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서, 상기 신경망은 상기 입력된 얼굴 이미지의 속성을 변환하여 다중 도메인에서의 얼굴 이미지를획득하는 심층 신경망(DNN: Deep neural network)이고, 상기 심층 신경망은 StarGAN(Star GenerativeAdversarial Networks)을 포함하는, 구강 이미지 처리 방법."}
{"patent_id": "10-2021-0128340", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서, 객체의 구강을 스캔하여 3차원 구강 모델을 획득하는 단계;상기 3차원 구강 모델로부터 목표가 되는 가상의 구강 모델을 획득하는 단계; 및상기 가상의 구강 모델에서 상기 제1 이미지에 포함된 상기 치아 영역에 대응하는 영역을 상기 가상 치아 영역으로 획득하는 단계를 더 포함하는, 구강 이미지 처리 방법."}
{"patent_id": "10-2021-0128340", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서, 상기 3차원 구강 모델은 상기 객체의 구강 중 상악, 하악, 교합 중 적어도 하나에 대한 3차원스캔 데이터를 포함하는, 구강 이미지 처리 방법."}
{"patent_id": "10-2021-0128340", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4 항에 있어서, 상기 제2 이미지를 획득하는 단계는 상기 제1 이미지에 포함된 상기 객체의 얼굴 이미지에 포함된 입술 라인을 검출하는 단계를 포함하고,상기 치아 영역은 상기 검출된 입술 라인의 내부 영역을 포함하고,상기 가상 치아 영역은, 상기 가상의 구강 모델에서 상기 검출된 입술 라인의 내부 영역에 대응하는 영역을 포함하는, 구강 이미지 처리 방법."}
{"patent_id": "10-2021-0128340", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4 항에 있어서, 상기 제2 이미지를 획득하는 단계는상기 제1 이미지에 포함된 상기 치아 영역에서 제1 특징 점을 획득하는 단계;상기 가상의 구강 모델에서 제2 특징 점을 획득하는 단계; 및상기 제1 특징 점과 상기 제2 특징 점을 정렬하는 단계를 포함하는, 구강 이미지 처리 방법. 공개특허 10-2022-0059908-3-청구항 8 제7 항에 있어서, 상기 제2 이미지를 획득하는 단계는사용자로부터 상기 치아 영역과 상기 가상의 구강 모델에서 각각 상기 제1 특징 점 및 상기 제2 특징 점을 선택받는 단계를 더 포함하는, 구강 이미지 처리 방법."}
{"patent_id": "10-2021-0128340", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 항에 있어서,상기 제1 이미지보다 더 큰 치아 영역을 포함하는 객체의 얼굴 이미지를 부가 이미지로 획득하는 단계를 더 포함하고,상기 제2 이미지를 획득하는 단계는상기 부가 이미지에 포함된 상기 더 큰 치아 영역을 가상의 치아 템플릿으로 대체하는 단계;상기 가상의 치아 템플릿으로 대체된 더 큰 치아 영역에서, 상기 제1 이미지에 포함된 상기 치아 영역에 대응하는 영역을 상기 가상 치아 영역으로 획득하는 단계; 및상기 제1 이미지에 포함된 상기 치아 영역을 상기 가상 치아 영역으로 대체하여 상기 제2 이미지를 획득하는 단계를 포함하는, 구강 이미지 처리 방법."}
{"patent_id": "10-2021-0128340", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "구강 이미지 처리 장치에 있어서,적어도 하나의 인스트럭션을 실행하는 프로세서를 포함하고,상기 프로세서는 상기 적어도 하나의 인스트럭션을 실행함으로써,치아 영역이 보이는 객체의 얼굴 이미지를 제1 이미지로 획득하고, 상기 제1 이미지에 포함된 상기 치아 영역을 가상 치아 영역으로 대체하여 제2 이미지를 획득하고, 입력된 얼굴 이미지로부터 상기 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하도록 학습된 신경망을 이용하여, 상기 제2 이미지로부터 상기 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴을 포함하는 제3 이미지를 획득하는, 구강 이미지 처리 장치."}
{"patent_id": "10-2021-0128340", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "치아 영역이 보이는 객체의 얼굴 이미지를 제1 이미지로 획득하는 단계, 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체하여 제2 이미지를 획득하는 단계 및 입력된 얼굴 이미지로부터 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하도록 학습된 신경망을 이용하여, 제2 이미지로부터 제2 이미지에 포함된 얼굴 과 다른 속성을 갖는 얼굴을 포함하는 제3 이미지를 획득하는 단계를 포함하는, 구강 이미지 처리 방법이 개시된 다."}
{"patent_id": "10-2021-0128340", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "개시된 실시 예는 데이터 처리 장치 및 데이터 처리 방법에 대한 것으로, 보다 구체적으로, 구강 이미지를 처리 또는 가공하기 위한 장치 및 그 방법에 관한 것이다."}
{"patent_id": "10-2021-0128340", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "환자의 치과 치료를 위해서 3차원 스캐너가 이용되고 있다. 3차원 스캐너는 환자의 구강 내에 인입 및 인출이 가능한 핸드헬드(handheld) 형태이거나 또는 테이블의 회전을 이용하여 테이블 위에 배치된 석고 모형을 스캔할 수 있는 테이블 스캐너 형태일 수 있다. 3차원 스캐너에 연결된 PC 등의 데이터 처리 장치는, 3차원 스캐너가 획득한 로우 데이터를 이용하여 3차원 구 강 이미지를 생성할 수 있다. 치과의 등의 사용자는 환자의 구강에 대해 보철 치료나 교정 치료 등의 치과적 치료를 수행할 수 있다. 이러한 치과적 치료는 다양한 형태의 심미 치료를 포함할 수 있다. 환자는 치과적 치료가 모두 끝났을 때의 본인의 구강이 어떤 상태일지를 미리 알고 싶어할 수 있다. 또한, 환자 는 치료가 끝난 구강이 얼굴에서 어떻게 보이는지를 알고 싶어할 수 있다. 이에, 치료를 마친 가상의 구강이 다양한 표정이나 포즈를 갖는 환자의 얼굴에서 어떻게 보이는지를 시뮬레이션하여 환자에게 제공하는 기술이 요구 된다."}
{"patent_id": "10-2021-0128340", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "다양한 실시 예들은 환자의 얼굴 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체하는 데이터 처리 장치 및 데이터 처리 방법을 제공하기 위한 것이다. 다양한 실시 예들은 치아 영역이 가상 치아 영역으로 대체된 얼굴 이미지로부터 원래의 얼굴과 다른 속성을 갖 는 다양한 얼굴 이미지를 획득하는 데이터 처리 장치 및 데이터 처리 방법을 제공하기 위한 것이다."}
{"patent_id": "10-2021-0128340", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "실시 예에 따른 구강 이미지 처리 방법은 치아 영역이 보이는 객체의 얼굴 이미지를 제1 이미지로 획득하는 단 계, 상기 제1 이미지에 포함된 상기 치아 영역을 가상 치아 영역으로 대체하여 제2 이미지를 획득하는 단계 및 입력된 얼굴 이미지로부터 상기 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하도록 학습된 신경 망을 이용하여, 상기 제2 이미지로부터 상기 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴을 포함하는 제 3 이미지를 획득하는 단계를 포함할 수 있다. 실시 예에서, 상기 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴은 상기 제2 이미지에 포함된 얼굴과 포 즈, 표정, 치아 영역이 보이는 정도, 스타일 중 적어도 하나가 다른 얼굴일 수 있다. 실시 예에서, 상기 신경망은 상기 입력된 얼굴 이미지의 속성을 변환하여 다중 도메인에서의 얼굴 이미지를 획 득하는 심층 신경망(DNN: Deep neural network)이고, 상기 심층 신경망은 StarGAN(Star Generative Adversarial Networks)을 포함할 수 있다. 실시 예에서, 상기 방법은 객체의 구강을 스캔하여 3차원 구강 모델을 획득하는 단계, 상기 3차원 구강 모델로 부터 목표가 되는 가상의 구강 모델을 획득하는 단계 및 상기 가상의 구강 모델에서 상기 제1 이미지에 포함된 상기 치아 영역에 대응하는 영역을 상기 가상 치아 영역으로 획득하는 단계를 더 포함할 수 있다. 실시 예에서, 상기 3차원 구강 모델은 상기 객체의 구강 중 상악, 하악, 교합 중 적어도 하나에 대한 3차원 스 캔 데이터를 포함할 수 있다. 실시 예에서, 상기 제2 이미지를 획득하는 단계는 상기 제1 이미지에 포함된 상기 객체의 얼굴 이미지에 포함된 입술 라인을 검출하는 단계를 포함하고, 상기 치아 영역은 상기 검출된 입술 라인의 내부 영역을 포함하고, 상 기 가상 치아 영역은, 상기 가상의 구강 모델에서 상기 검출된 입술 라인의 내부 영역에 대응하는 영역을 포함 할 수 있다. 실시 예에서, 상기 제2 이미지를 획득하는 단계는 상기 제1 이미지에 포함된 상기 치아 영역에서 제1 특징 점을 획득하는 단계, 상기 가상의 구강 모델에서 제2 특징 점을 획득하는 단계 및 상기 제1 특징 점과 상기 제2 특징 점을 정렬하는 단계를 포함할 수 있다. 실시 예에서, 상기 제2 이미지를 획득하는 단계는 사용자로부터 상기 치아 영역과 상기 가상의 구강 모델에서 각각 상기 제1 특징 점 및 상기 제2 특징 점을 선택 받는 단계를 더 포함할 수 있다. 실시 예에서, 상기 방법은 상기 제1 이미지보다 더 큰 치아 영역을 포함하는 객체의 얼굴 이미지를 부가 이미지 로 획득하는 단계를 더 포함하고, 상기 제2 이미지를 획득하는 단계는 상기 부가 이미지에 포함된 상기 더 큰 치아 영역을 가상의 치아 템플릿으로 대체하는 단계, 상기 가상의 치아 템플릿으로 대체된 더 큰 치아 영역에서, 상기 제1 이미지에 포함된 상기 치아 영역에 대응하는 영역을 상기 가상 치아 영역으로 획득하는 단 계 및 상기 제1 이미지에 포함된 상기 치아 영역을 상기 가상 치아 영역으로 대체하여 상기 제2 이미지를 획득 하는 단계를 포함할 수 있다. 실시 예에 따른 구강 이미지 처리 장치는 적어도 하나의 인스트럭션을 실행하는 프로세서를 포함하고, 상기 프 로세서는 상기 적어도 하나의 인스트럭션을 실행함으로써, 치아 영역이 보이는 객체의 얼굴 이미지를 제1 이미 지로 획득하고, 상기 제1 이미지에 포함된 상기 치아 영역을 가상 치아 영역으로 대체하여 제2 이미지를 획득하 고, 입력된 얼굴 이미지로부터 상기 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하도록 학습된신경망을 이용하여, 상기 제2 이미지로부터 상기 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴을 포함하 는 제3 이미지를 획득할 수 있다."}
{"patent_id": "10-2021-0128340", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "일 실시 예에 따른 데이터 처리 장치 및 데이터 처리 방법은 환자의 얼굴 이미지에 포함된 치아 영역을 가상 치 아 영역으로 대체할 수 있다. 일 실시 예에 따른 데이터 처리 장치 및 데이터 처리 방법은 치아 영역이 가상 치아 영역으로 대체된 얼굴 이미 지로부터 원래의 얼굴과 다른 속성을 갖는 다양한 얼굴 이미지를 획득할 수 있다."}
{"patent_id": "10-2021-0128340", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서는 본 출원의 권리범위를 명확히 하고, 본 출원이 속하는 기술분야에서 통상의 지식을 가진 자가 본 출원을 실시할 수 있도록, 본 출원의 원리를 설명하고, 실시 예들을 개시한다. 개시된 실시 예들은 다양한 형태 로 구현될 수 있다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성요소를 지칭한다. 본 명세서가 실시 예들의 모든 요소들을 설"}
{"patent_id": "10-2021-0128340", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "명하는 것은 아니며, 본 출원이 속하는 기술분야에서 일반적인 내용 또는 실시 예들 간에 중복되는 내용은 생략 한다. 명세서에서 사용되는 '부'(part, portion)라는 용어는 소프트웨어 또는 하드웨어로 구현될 수 있으며, 실 시 예들에 따라 복수의 '부'가 하나의 요소(unit, element)로 구현되거나, 하나의 '부'가 복수의 요소들을 포함 하는 것도 가능하다. 이하 첨부된 도면들을 참고하여 본 출원의 작용 원리 및 실시 예들에 대해 설명한다. 본 명세서에서 이미지는 적어도 하나의 치아, 또는 적어도 하나의 치아를 포함하는 구강, 또는 구강에 대한 석 고 모형을 나타내는 이미지(이하, '구강 이미지')를 포함할 수 있다. 또한, 본 명세서에서 이미지는 대상체에 대한 2차원 이미지 또는 대상체를 입체적으로 나타내는 3차원 구강 이 미지를 포함할 수 있다. 3차원 구강 이미지는 로우 데이터에 근거하여 구강의 구조를 3차원적으로 모델링 (modeling)하여 생성될 수 있으므로, 3차원 구강 모델로 호칭될 수도 있다. 또한, 3차원 구강 모델은 3차원 스 캔 모델 또는 3차원 스캔 데이터로도 호칭될 수 있다. 이하, 본 명세서에서 구강 이미지는 구강을 2차원 또는 3차원적으로 나타내는 모델 또는 이미지를 통칭하는 의 미로 사용하기로 한다. 또한, 본 명세서에서 데이터는 대상체를 2차원 또는 3차원적으로 표현하기 위해서 필요한 정보, 예를 들어, 적 어도 하나의 카메라를 이용하여 획득된 로우 데이터(raw data) 등을 의미할 수 있다. 구체적으로, 로우 데이터는 구강 이미지를 생성하기 위해서 획득되는 데이터로, 3차원 스캐너를 이용하여 대상 체를 스캔(scan)할 때 3차원 스캐너에 포함되는 적어도 하나의 이미지 센서에서 획득되는 데이터(예를 들어, 2 차원 데이터)가 될 수 있다. 3차원 스캐너에서 획득되는 로우 데이터는, 2차원 이미지 데이터로 언급될 수도 있 다. 로우 데이터는, 3차원 스캐너를 이용하여 대상체를 스캔할 때 복수의 카메라들에 의해 획득되는 서로 다른 시점의 2차원 이미지들을 의미할 수 있다. 위에서는, 로우 데이터가 2차원 이미지인 것으로 서술하였으나, 이에 제한하지 않고 로우 데이터는 3차원 이미 지 데이터일 수도 있다. 본 명세서에서 대상체(object)는 촬영의 대상이 되는 것으로서 신체의 일부이거나, 또는 신체의 일부를 본뜬 모 형을 포함할 수 있다. 대상체는 구강, 구강을 본 뜬 석고 모형이나 임프레션 모형, 구강에 삽입 가능한 인공 구 조물, 또는 인공 구조물을 본 뜬 석고 모형이나 임프레션 모형을 포함할 수 있다. 예컨대, 대상체는 치아나 치 은이거나, 치아나 치은에 대한 석고 모형이나 임프레션 모형이거나, 및/또는 구강 내에 삽입 가능한 인공 구조 물, 또는 이러한 인공 구조물에 대한 석고 모형이나 임프레션 모형을 포함할 수 있다. 여기서, 구강에 삽입 가 능한 인공 구조물은 예를 들어, 교정 장치, 임플란트, 크라운, 인레이, 온레이, 인공 치아, 구강 내 삽입되는 교정 보조 도구 중 적어도 하나를 포함할 수 있다. 또한, 교정 장치는 브라켓, 어태치먼트(attachment), 교정용 나사, 설측 교정 장치, 및 가철식 교정 유지 장치 중 적어도 하나를 포함할 수 있다. 환자는 구강에 대해 보철 치료나 교정 치료 등의 치과적 치료를 받을 수 있다. 치과적 치료는, 예컨대, 위아래 치아의 맞물림의 상태가 비 정상적일 때 상악과 하악의 맞물림 위치를 바로잡거나, 비 심미적으로 배치된 치아 의 배열 위치를 바로잡거나, 변색된 치아의 색조 등을 수정하거나, 결손이 있는 부위나 치아를 뽑은 자리에 임 플란트 등의 인공 치아를 식립하거나, 또는 라미네이트 치료를 수행하는 것과 같은 다양한 형태의 치료를 포함 할 수 있다. 이러한 치과적 치료를 통해 구강 영역은 치료 전보다 더 심미적으로 보이는 효과를 가질 수 있다. 환자는 구강 치료가 모두 끝났을 때의 본인의 구강, 즉, 가상의 구강이 얼굴에서 어떻게 보이는지를 미리 확인 하고 싶어할 수 있다. 환자는 하나의 얼굴 이미지만이 아니라, 다양한 속성의 얼굴 이미지에서 가상의 구강이 어떻게 보이는지를 알고 싶어할 수 있다. 예컨대, 환자는 본인의 얼굴이 다양한 포즈를 취하거나 다양한 표정을 지을 때, 또는 얼굴에서 치아가 보이는 정도가 다를 때, 치료 후의 치아가 다양한 얼굴에서 어떻게 보이는지를 확인하고 싶어할 수 있다. 이에, 가상의 구강 이미지를 다양한 상황에서의 환자의 얼굴과 합성하여 제공하는 기 술이 요구된다. 개시된 실시 예는 전술한 기술의 필요성을 인식한 데서 비롯된 것으로, 환자의 얼굴 이미지에서 치아 영역을 가 상 치아 영역으로 대체하고, 신경망을 통해 가상 치아 영역으로 대체된 환자의 얼굴 이미지로부터 복수의 속성 을 갖는 얼굴 이미지를 획득하는 방법 및 장치를 제공하기 위한 것이다. 이하에서는 도면을 참조하여 실시 예들을 상세히 설명한다. 도 1은 실시 예에 따른 구강 이미지 처리 시스템을 설명하기 위한 도면이다. 도 1을 참조하면, 구강 이미지 처리 시스템은 3차원 스캐너(100, 110), 및 3차원 스캐너(100, 110)와 통신망 을 통해 결합된 데이터 처리 장치를 포함할 수 있다. 3차원 스캐너(100, 110)는 대상체의 이미지를 획득하는 의료 장치일 수 있다. 3차원 스캐너(100, 110)는 구강이나 인공 구조물, 또는 구강이나 인공 구조물을 본 뜬 석고 모형 중 적어도 하 나에 대한 이미지를 획득할 수 있다. 3차원 스캐너(100, 110)는 구강 스캐너와 테이블 스캐너 중 적어도 하나를 포함할 수 있다. 실시 예에서, 3차원 스캐너(100, 110)는 구강 스캐너를 포함할 수 있다. 구강 스캐너는 사용자가 손 으로 잡고 이동하면서 구강을 스캔하는 핸드 헬드(handheld)형일 수 있다. 구강 스캐너는 구강 내에 삽입 되어 비 접촉식으로 치아를 스캐닝함으로써, 적어도 하나의 치아를 포함하는 구강에 대한 이미지를 획득할 수 있다. 구강 스캐너는 본체와 팁을 포함할 수 있다. 본체는 광을 투사하는 광 조사부(미도시)와 대상체를 촬영하여 이미지를 획득하는 카메라(미도시)를 포함할 수 있다. 팁은 구강 내에 삽입되는 부분으로, 탈부착이 가능한 구조로 본체에 장착될 수 있다. 팁은 광 경로 변경 수단을 포함하여, 본체로부터 조사된 광을 대상체로 향하게 하고, 대상체로부터 수신된 광을 본 체로 향하게 하도록 할 수 있다. 구강 스캐너는 구강 내부의 치아, 치은 및 구강 내에 삽입 가능한 인공 구조물(예를 들어, 브라켓 및 와이 어 등을 포함하는 교정 장치, 임플란트, 인공 치아, 구강 내 삽입되는 교정 보조 도구 등) 중 적어도 하나의 표 면을 이미징하기 위해서, 대상체에 대한 표면 정보를 로우 데이터(raw data)로 획득할 수 있다. 실시 예에서, 3차원 스캐너(100, 110)는 테이블 스캐너를 포함할 수 있다. 테이블 스캐너는 테이블 의 회전을 이용하여 대상체를 스캔함으로써 대상체에 대한 표면 정보를 로우 데이터(raw data) 로 획득하는 스캐너일 수 있다. 테이블 스캐너는 구강을 본 뜬 석고 모형이나 임프레션 모형, 구강에 삽입 가능한 인공 구조물, 또는 인공 구조물을 본 뜬 석고 모형이나 임프레션 모형 등의 대상체의 표면을 스캔 할 수 있다. 테이블 스캐너는 하우징의 내측 방향으로 함몰되어 형성되는 내부 공간을 포함할 수 있다. 내부 공간은 제1 내측면, 제2 내측면, 제3 내측면(바닥면), 및 제4 내측면(미도시)(천장면)에 의해 형성될 수 있다. 내부 공간에는 대상체를 거치할 수 있으며, 대상체를 이동시킬 수 있는 이동부가 형성될 수 있다. 이동부는 z축 방향을 따라 상하 방향으로 이동할 수 있다. 이동부는 제1 내측면에 고 정되어 제1 회전부와 연결된 고정 베이스, 고정 베이스 상의 일 지점을 중심축으로, 예컨대, x 축을 중심축으로 한 제1 회전 방향(M1)으로 회전 가능한 제1 회전부, 및 제1 회전부와 연결되어 회전 부로부터 돌출되어 형성된 빔부(beam portion, 133)를 포함할 수 있다. 빔부는 x축 방향으로 연장 또 는 단축될 수 있다. 빔부의 타단에는 z축을 회전축으로 하는 제2 회전 방향(M2)으로 회전할 수 있는 원통 형상의 제2 회전부 가 결합될 수 있다. 제2 회전부의 일면 상에는 제2 회전부와 함께 회전하는 테이블이 형성 될 수 있다. 하우징의 내부 공간 상의 제2 내측면에는 광학부가 형성될 수 있다. 광학부는 대상체 에 패턴 광을 조사(project)하는 광 조사부와, 대상체로부터 반사된 광을 수용하여 복수의 2차 원 프레임들을 획득하는 적어도 하나의 카메라(142a, 142b)를 포함할 수 있다. 광학부는 제2 내측면 에 결합된 상태에서, 광 조사부의 중심을 회전축으로 하여 회전하는 제2 회전부(미도시)를 더 포함할 수 있다. 제2 회전부는 광 조사부, 제1 카메라(142a), 그리고 제2 카메라(142b)를 제3 회전 방향(M3)으로 회 전시킬 수 있다. 3차원 스캐너(100, 110)는 획득한 로우 데이터를 통신망를 통하여 데이터 처리 장치로 전송할 수 있 다. 데이터 처리 장치는 3차원 스캐너(100, 110)와 유선 또는 무선 통신망을 통하여 연결될 수 있다. 데 이터 처리 장치는 3차원 스캐너(100, 110)로부터 로우 데이터를 수신하고, 수신된 로우 데이터에 근거하여 구강 이미지를 생성, 처리, 디스플레이 및/또는 전송할 수 있는 모든 전자 장치가 될 수 있다. 예컨대, 데이터 처리 장치는 스마트 폰(smart phone), 랩탑 컴퓨터, 데스크탑 컴퓨터, PDA, 태블릿 PC 등의 컴퓨팅 장치가 될 수 있으나, 이에 한정되지 않는다. 또한, 데이터 처리 장치는 구강 이미지를 처리하기 위한 서버(또는 서버 장치) 등의 형태로 존재할 수도 있다. 데이터 처리 장치는 3차원 스캐너(100, 110)에서 수신된 2차원 이미지 데이터에 근거하여, 2차원 이미지 데이터를 처리하여 3차원 구강 이미지를 생성하거나, 또는 부가 정보를 생성할 수 있다. 데이터 처리 장치(12 0)는 3차원 구강 이미지 및/또는 부가 정보를 디스플레이를 통하여 디스플레이 하거나, 이를 외부 장치로 출력하거나 전송할 수 있다. 또 다른 예로, 3차원 스캐너(100, 110)가 구강 스캔을 통하여 로우 데이터를 획득하고, 획득된 로우 데이터를 가공하여 3차원 데이터를 생성하고, 이를 데이터 처리 장치로 전송할 수 있다. 이 경우, 데이터 처리 장치 는 3차원 스캐너(100, 110)로부터 3차원 구강 모델을 수신할 수 있다. 또는 데이터 처리 장치는 외부 서버나 외부 장치 등으로부터 유선 또는 무선 통신망을 통해 3차원 구강 이미지를 수신할 수도 있다. 실시 예에서, 데이터 처리 장치는 치아 영역이 보이는 객체의 얼굴 이미지를 제1 이미지로 획득할 수 있다. 데이터 처리 장치는 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체하여 제2 이미지를 획득할 수 있다. 데이터 처리 장치는 입력된 얼굴 이미지로부터 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하도록 학습된 신경망을 이용하여, 제2 이미지로부터 제2 이미지에 포함된 얼굴과 다른 속성 을 갖는 얼굴을 포함하는 제3 이미지를 획득할 수 있다. 데이터 처리 장치는 획득한 제3 이미지를 디스플 레이를 통하여 디스플레이하거나, 또는 외부 장치로 전송하거나 출력할 수 있다. 실시 예에서, 3차원 스캐너(100, 110)는 다양한 방법을 이용하여 대상체에 대한 3차원 데이터를 획득할 수 있다. 예컨대, 3차원 스캐너(100, 110)는 공초점 (confocal) 방식을 이용하여 대상체에 대한 3차원 데이터를 획 득할 수 있다. 공초점 방식은 대상체에 조사되는 빛을 통과시키는 렌즈의 굴절률에 따라서, 반사된 빛의 최대 강도를 통해 알아낸 점의 위치를 기초로 대상체의 3차원적 정보를 획득하는 방식이다. 3차원 스캐너(100, 110) 는 핀홀 구조를 이용하여 공간해상도가 높은 광학 단면 이미지를 획득할 수 있다. 3차원 스캐너(100, 110)는 축 방향을 따라 획득한 2차원 이미지를 스택(stack)하여 3차원 데이터를 획득할 수 있다. 또는, 실시 예에서, 3차원 스캐너(100, 110)는 광 삼각법 (triangulation technique) 방식을 이용하여 대상체 의 3차원적 정보를 획득할 수도 있다. 광 삼각법은 광원, 광원으로부터 조사된 빛이 조사되는 대상체, 대상체로 부터 반사된 빛이 입력되는 이미지 센서에 의해 형성되는 삼각형을 이용하여 삼각 계산을 통해 대상체의 3차원 적 정보를 획득하는 기술이다. 3차원 스캐너(100, 110)는 대상체에 패턴 광을 조사(project)하고 패턴 광이 조 사된 대상체를 스캔함으로써, 패턴의 변형에 의한 삼각 계측의 원리를 이용하여 대상체의 형상을 나타내는 3차 원 데이터를 획득할 수 있다. 다만, 이는 하나의 실시 예로, 3차원 스캐너(100, 110)는 공초점 방식 또는 광 삼각법 방식 외에도 다양한 방식 으로 3차원 데이터를 획득하고, 이를 데이터 처리 장치로 전송할 수 있다. 데이터 처리 장치는 수신 된 3차원 데이터를 분석, 처리, 가공, 디스플레이 및/또는 전송할 수 있다. 도 2는 실시 예에 따른 데이터 처리 장치의 내부 블록도이다. 실시 예에서, 데이터 처리 장치는 구강 이미지 처리 장치로도 호칭될 수 있다. 도 2의 데이터 처리 장치는 도 1의 데이터 처리 장치의 일 실시 예일 수 있다. 따라서, 도 1의 데이 터 처리 장치에 대해 설명한 내용과 중복된 부분에 대한 설명은 생략한다. 데이터 처리 장치는 3차원 스캐너(100, 110)로부터 수신한 로우 데이터를 이용하여 구강 이미지를 생성, 처리, 가공, 디스플레이 및/또는 전송할 수 있는 전자 장치일 수 있다. 도 2를 참조하면, 데이터 처리 장치는 프로세서, 메모리 및 디스플레이를 포함할 수 있다. 데이터 처리 장치는 3차원 스캐너(100, 110)로부터 수신된 로우 데이터를 기반으로 3차원 구강 모델을 생 성할 수 있다. 또는 데이터 처리 장치는3차원 스캐너(100, 110)로부터 3차원 구강 모델을 수신할 수 있다. 또는 데이터 처리 장치는 외부 서버나 외부 장치 등으로부터 유선 또는 무선 통신망을 통해 3차원 구강 이 미지를 수신할 수도 있다. 실시 예에 따른 메모리는 적어도 하나의 인스트럭션을 저장할 수 있다. 메모리는 프로세서가 실 행하는 적어도 하나의 인스트럭션이나 프로그램을 저장하고 있을 수 있다. 메모리는 3차원 스캐너(100, 110)로부터 수신되는 데이터, 예를 들어, 구강이나 구강 모형을 스캔하여 획득된 로우 데이터 등을 저장할 수 있다. 또한, 메모리는 데이터 처리 장치가 생성하거나, 또는 3차원 스캐너(100, 110)로부터 수신하거 나, 외부 서버나 외부 장치 등으로부터 수신한 3차원 구강 이미지를 저장할 수 있다. 실시 예에서, 메모리는 치아 영역을 포함하는 이미지를 저장할 수 있다. 실시 예에서, 메모리는 치아 영역을 포함하는 객체의 얼굴 이미지를 저장할 수 있다. 실시 예에서, 메모리는 얼굴 이미지에서 치아 영역을 식별하기 위한 하나 이상의 인스트럭션을 포함할 수 있다. 실시 예에서, 메모리는 치아 영역을 가상 치아 영역으로 대체하기 위한 하나 이상의 인스트럭션을 포함할 수 있다. 실시 예에서, 메모리는 3차원 구강 모델을 목표로 하는 가상의 구강 모델로 대체할 수 있는 전용 소프트웨 어를 저장할 수 있다. 실시 예에서, 메모리는 치아가 보이는 객체의 2차원 얼굴 이미지에 포함된 치아 영역을 가상의 치아 템플 릿으로 대체할 수 있는 전용 소프트웨어를 저장할 수 있다. 실시 예에서, 메모리는 이미지에서 치아 영역을 검출하고, 치아 영역을 가상 치아 영역으로 대체하기 위한 전용 소프트웨어를 저장할 수 있다. 치아 영역을 가상 치아 영역으로 대체하기 위한 전용 소프트웨어는 전용 프 로그램, 전용 툴(tool), 또는 전용 어플리케이션 등으로 호칭될 수 있다. 실시 예에서, 메모리는 입력된 얼굴 이미지로부터 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하도록 학습된 신경망을 포함할 수 있다. 실시 예에 따른 프로세서는 데이터 처리 장치 전반을 제어할 수 있다. 프로세서는 적어도 하나 의 인스트럭션을 실행하여 의도하는 동작이 수행되도록 제어할 수 있다. 여기서, 적어도 하나의 인스트럭션은 프로세서와 별도로 데이터 처리 장치 내에 포함되는 메모리 또는 프로세서내에 포함되는 내부 메모리(미도시)에 저장되어 있을 수 있다. 구체적으로, 프로세서는 적어도 하나의 인스트럭션을 수행하여, 의도하는 동작이 수행되도록 데이터 처리 장치 내부에 포함되는 적어도 하나의 구성들을 제어할 수 있다. 따라서, 프로세서가 소정 동작들을 수행하는 경우를 예로 들어 설명하더라도, 프로세서가 소정 동작들이 수행되도록 데이터 처리 장치 내부에 포함된 적어도 하나의 구성들을 제어하는 것을 의미할 수 있다. 실시 예에서, 프로세서는 치아 영역이 보이는 객체의 얼굴 이미지를 획득할 수 있다. 실시 예에서, 객체의 얼굴 이미지는, 객체의 얼굴 전체 또는 객체의 얼굴의 일부를 포함하는 이미지를 의미할 수 있다. 또한, 치아 영역이 보이는 객체의 얼굴 이미지는 객체의 얼굴에서 객체의 입술 사이로 치아 영역이 드러난 이미지를 의미할 수 있다. 치아는 구강 내의 여러 구조물 중 하나로, 상악과 하악의 이틀뼈에 박혀있는 구강 구조 중 하나이다. 실시 예에서, 치아 영역은 입술 사이로 보이는 치아 또는 그 중 일부를 포함하는 영역을 의미할 수 있다. 이하, 설명의 편의를 위하여, 치아 영역이 보이는 객체의 얼굴 이미지를 제1 이미지로 호칭하기로 한다. 실시 예에서, 프로세서는 제1 이미지를 획득하고, 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체할 수 있다. 가상 치아 영역은 객체의 구강을 치료했을 때, 객체의 구강에 포함될 미래의 치아 영역을 의미 할 수 있다. 즉, 가상 치아 영역은 향후 객체의 구강 치료가 끝났을 때 제1 이미지에 포함된 현재의 객체의 치 아 영역이 향후 어떻게 바뀔지를 나타낼 수 있다. 이하, 설명의 편의를 위하여, 제1 이미지에 포함된 치아 영역 이 가상 치아 영역으로 대체된 이미지를 제2 이미지로 호칭하기로 한다. 실시 예에서, 프로세서는 입력된 얼굴 이미지로부터 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지 를 생성하도록 학습된 신경망을 이용하여, 제2 이미지로부터 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼 굴을 포함하는 이미지를 획득할 수 있다. 이하, 설명의 편의를 위하여, 신경망에 제2 이미지가 입력되었을 때 신경망에서 출력된 이미지를 제3 이미지로 호칭하기로 한다. 실시 예에서, 프로세서는 제2 이미지로부터, 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴을 포함하 는 제3 이미지를 획득할 수 있다. 실시 예에서, 다른 속성을 갖는 얼굴은, 얼굴의 포즈가 바뀐 경우, 얼굴의 표 정이 바뀐 경우, 입을 벌린 정도나 얼굴의 각도 등에 따라 보이는 치아 영역이 바뀌거나 또는 보이는 치아 영역 의 크기가 바뀐 경우, 나이, 객체의 머리 색이나 헤어 스타일, 메이크업, 피부 톤 등의 스타일이 변한 경우 중 적어도 하나를 포함할 수 있다. 프로세서는 적어도 하나의 신경망을 이용하여, 제2 이미지에 포함된 얼굴 과 포즈, 표정, 얼굴 각도, 치아 영역이 보이는 정도, 나이, 스타일 중 적어도 하나가 다른 얼굴을 포함하는 제 3 이미지를 획득할 수 있다. 실시 예에서, 프로세서가 이용하는 신경망은 입력된 얼굴 이미지의 속성을 변환하여 다중 도메인에서의 얼 굴 이미지를 획득하는 심층 신경망(DNN: Deep neural network)일 수 있다. 실시 예에서, 프로세서가 이용하는 심층 신경망은 StarGAN(Star Generative Adversarial Networks)을 포 함할 수 있다. 실시 예에서, 프로세서는 객체의 구강을 스캔하여 획득된 3차원 구강 모델로부터 목표가 되는 가상의 구강 모델을 획득할 수 있다. 3차원 구강 모델은 객체의 구강 중 상악, 하악, 교합 중 적어도 하나에 대한 3차원 스 캔 데이터를 포함할 수 있다. 실시 예에서, 프로세서는 가상의 구강 모델에서, 제1 이미지에 포함된 치아 영역에 대응하는 영역을 가상 치아 영역으로 획득할 수 있다. 실시 예에서, 프로세서는 제1 이미지에서 객체의 얼굴에 포함된 입술 라인을 검출하고, 검출된 입술 라인 의 내부 영역을 치아 영역으로 식별할 수 있다. 실시 예에서, 프로세서는 가상의 구강 모델에서, 객체의 얼굴 이미지에서 검출된 입술 라인의 내부 영역에 대응하는 영역을 가상 치아 영역으로 획득할 수 있다. 프로세서는 제1 이미지에서 검출된 입술 라인 내부 를 가상 치아 영역으로 대체하여 제2 이미지를 획득할 수 있다. 실시 예에서, 프로세서는 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체하기 위하여, 제1 이 미지에 포함된 치아 영역에서 제1 특징 점을 획득할 수 있다. 프로세서는 가상의 구강 모델에서 제2 특징 점을 획득할 수 있다. 프로세서는 제1 특징 점과 제2 특징 점을 정렬시켜, 제1 이미지에 포함된 치아 영역 을 가상 치아 영역으로 대체할 수 있다. 실시 예에서, 프로세서는 자동으로, 제1 이미지에 포함된 치아 영역과 가상의 구강 모델에서 각각 제1 특 징 점 및 상기 제2 특징 점을 찾아 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체할 수 있다. 또는, 실시 예에서, 프로세서는 수동으로, 사용자로부터 치아 영역과 가상의 구강 모델에서 각각 제1 특징 점 및 상기 제2 특징 점을 선택 받고, 제1 특징 점과 제2 특징 점을 정렬시킴으로써 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체할 수 있다. 다른 실시 예에서, 프로세서는 제2 이미지를 획득하기 위해, 3차원 구강 모델을 이용하는 대신 가상의 치 아 템플릿(template)을 이용할 수 있다. 이를 위해, 프로세서는 제1 이미지보다 더 큰 치아 영역을 포함하 는 객체의 얼굴 이미지를 이용할 수 있다. 실시 예에서, 제1 이미지보다 더 큰 치아 영역을 포함하는 객체의 얼 굴 이미지는, 객체의 얼굴에서 치아 영역이 차지하는 비율이 제1 이미지에 포함된 객체의 얼굴 대비 치아 영역 의 비율보다 더 큰 경우를 의미할 수 있다. 예컨대, 제1 이미지보다 더 큰 치아 영역을 포함하는 객체의 얼굴 이미지는, 객체의 얼굴에, 제1 이미지에서 보이는 객체의 치아 영역과 함께, 제1 이미지에서는 보이지 않던 치 아 영역이 더 포함된 이미지를 의미할 수 있다. 이하, 설명의 편의를 위해, 제1 이미지보다 더 큰 치아 영역을 포함하는 객체의 얼굴 이미지를 부가 이미지로 호칭하기로 한다. 프로세서는 부가 이미지에 포함된, 더 큰 치아 영역에 가상의 치아 템플릿을 정렬할 수 있다. 프로세서 는 가상의 치아 템플릿을 부가 이미지에 포함된 치아 영역에 정렬할 수 있다. 프로세서는 가상의 치 아 템플릿이 정렬된 더 큰 치아 영역에서, 제1 이미지에 포함된 치아 영역에 대응하는 영역을 가상 치아 영역으 로 획득할 수 있다. 프로세서는 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체함으로써 제2 이미지를 획득할 수 있다. 실시 예에 따른 디스플레이는 구강 이미지를 화면에 출력할 수 있다. 디스플레이가 터치 스크린으로 구현되는 경우, 디스플레이는 출력 장치 이외에 사용자 인터페이스와 같은 입력 장치로 사용될 수 있다. 예를 들어, 디스플레이는 액정 디스플레이(liquid crystal display), 박막 트랜지스터 액정 디스플레이(thin film transistor-liquid crystal display), 유기 발광 다이오드 (organic light-emitting diode), 플렉서블 디스플레이(flexible display), 3차원 디스플레이(4D display), 전 기 영동 디스플레이(electrophoretic display) 중에서 적어도 하나를 포함할 수 있다. 디스플레이는 구현 형태에 따라, 둘 이상 포함될 수 있다. 실시 예에서, 디스플레이는 치아 영역이 보이는 객체의 얼굴 이미지, 즉, 제1 이미지를 화면에 출력할 수 있다. 실시 예에서, 디스플레이는 객체의 구강을 스캔하여 획득한 3차원 구강 모델을 화면에 출력할 수 있다. 또 한, 실시 예에서, 디스플레이는 3차원 구강 모델로부터 획득된, 목표가 되는 가상의 구강 모델을 화면에 출력할 수 있다. 실시 예에서, 디스플레이는 가상의 구강 모델에 기반하여 획득된, 제1 이미지에 포함된 치아 영역에 대응 하는 영역인 가상 치아 영역이 제1 이미지에 포함된 치아 영역을 대체하는 제2 이미지를 화면에 출력할 수 있다. 다른 실시 예에서, 프로세서가 제2 이미지를 획득하기 위해, 가상의 치아 템플릿을 이용하는 경우, 디스플 레이는 제1 이미지보다 더 큰 치아 영역을 포함하는 객체의 얼굴 이미지, 즉, 부가 이미지를 제1 이미지와함께 하나의 화면에 출력하거나, 또는 제1 이미지와는 다른 화면에 각각 출력할 수 있다. 실시 예에서, 디스플레이는 치아 템플릿을 화면에 출력할 수 있다. 실시 예에서, 디스플레이는 부가 이미지에 포함된 더 큰 치아 영역에 치아 템플릿에 기반한 가상 템플릿이 정렬된 상태를 화면에 출력할 수 있다. 실시 예에서, 디스플레이는 가상의 치아 템플릿이 정렬된 더 큰 치아 영역에서 획득된 가상 치아 영 역으로 제1 이미지에 포함된 치아 영역이 대체되어 획득된 제2 이미지를 화면에 출력할 수 있다. 실시 예에서, 디스플레이는 제2 이미지가 신경망에 입력되었을 때 신경망을 통해 획득된, 제2 이미지에 포 함된 얼굴과는 다른 속성을 갖는 얼굴을 포함하는 제3 이미지를 화면에 출력할 수 있다. 실시 예에서, 디스플레이는 사용자 입력을 위한 사용자 인터페이스 화면을 출력할 수 있다. 도 3은 도 2의 데이터 처리 장치의 일 예를 도시한 도면이다. 도 3을 참조하면, 데이터 처리 장치는 프로세서, 메모리, 및 디스플레이 외에 통신 인터페 이스, 영상 처리부 및 사용자 입력부를 더 포함할 수 있다. 도 3의 데이터 처리 장치에 포함된 프로세서, 메모리, 디스플레이는 도 2의 데이터 처리 장치에 포함된 프로세서, 메모리, 디스플레이와 수행하는 기능이 동일하므로 동일한 도면 부호를 사용하였다. 이하, 도 2의 데이터 처리 장치에 대해 설명한 내용과 중복되는 부분에 대한 설명은 생략한다. 실시 예에 따른 사용자 입력부는 데이터 처리 장치를 제어하기 위한 사용자 입력을 수신할 수 있다. 사용자 입력부는 사용자의 터치를 감지하는 터치 패널, 사용자의 푸시 조작을 수신하는 버튼, 사용자 인터 페이스 화면 상의 일 지점을 지정 또는 선택하기 위한 마우스(mouse) 또는 키보드(key board) 등을 포함하는 사 용자 입력 디바이스를 포함할 수 있으나 이에 제한되지 않는다. 또한, 사용자 입력부는 음성 인식을 위한 음성 인식 장치를 포함할 수 있다. 예를 들어, 음성 인식 장치는 마이크로폰이 될 수 있으며, 음성 인식 장치는 사용자의 음성 명령 또는 음성 요청을 수신할 수 있다. 그에 따라서, 프로세서는 음성 명령 또는 음성 요 청에 대응되는 동작이 수행되도록 제어할 수 있다. 실시 예에서, 사용자 입력부는 치과의 등의 사용자로부터 제1 이미지의 치아 영역이 가상 치아 영역으로 대체된 제2 이미지를 획득할 것을 요청하는 사용자 입력을 수신할 수 있다. 실시 예에서, 사용자 입력부는 사용자로부터 제1 이미지의 치아 영역과 가상 치아 영역을 자동으로, 또는 수동으로 정렬할 것을 요청하는 사용자 입력을 수신할 수 있다. 실시 예에서, 사용자 입력부는 사용자로부터 제1 이미지의 치아 영역과 가상 치아 영역을 수동으로 정렬하 기 위해, 치아 영역과 가상 치아 영역으로부터 각각 특징 점을 선택 받는 사용자 입력을 수신할 수 있다. 실시 예에서, 치아 템플릿이 복수개인 경우, 사용자 입력부는 사용자로부터 복수개의 치아 템플릿 중 하나 를 선택 받는 사용자 입력을 수신할 수 있다. 실시 예에서, 사용자 입력부는 선택된 하나의 치아 템플릿을 환자의 구강에 맞게 조절하는 사용자 입력을 수신할 수 있다. 실시 예에서, 사용자 입력부는 선택된 하나의 치아 템플릿을 목표로 하는 가상의 치아 템플릿으로 조절하 는 사용자 입력을 수신할 수 있다. 실시 예에서, 사용자 입력부는 사용자로부터 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴을 갖는 제3 이미지를 획득할 것을 요청하는 사용자 입력을 수신할 수 있다. 실시 예에서, 사용자 입력부는 사용자로부터 제2 이미지에 포함된 얼굴을 어떤 속성을 갖는 이미지로 변환 할 것인지를 선택 받는 사용자 입력을 수신할 수 있다. 실시 예에 따른 영상 처리부는 이미지의 생성 및/또는 처리를 위한 동작들을 수행할 수 있다. 실시 예에서, 영상 처리부는 3차원 스캐너(100, 110)로부터 수신한 로우 데이터를 기반으로 3차원 구강 모 델을 생성할 수 있다. 실시 예에서, 영상 처리부는 3차원 구강 모델로부터 목표가 되는 가상의 구강 모델을 생성할 수 있다. 실시 예에서, 영상 처리부는 가상의 구강 모델에서, 제1 이미지에 포함된 치아 영역에 대응하는 영역인 가 상 치아 영역을 획득하고, 제1 이미지의 치아 영역이 가상 치아 영역으로 대체된 제2 이미지를 생성할 수 있다. 실시 예에서, 영상 처리부는 부가 이미지에 포함된 더 큰 치아 영역에 가상의 치아 템플릿이 오버랩된 이 미지를 생성할 수 있다. 실시 예에서, 영상 처리부는 가상의 치아 템플릿이 정렬된 부가 이미지 포함된, 제1 이미지에 포함된 치아 영역에 대응하는 가상 치아 영역이, 제1 이미지에 포함된 치아 영역을 대체한 제2 이미지를 생성할 수 있다. 실시 예에 따른 통신 인터페이스는 적어도 하나의 외부 전자 장치와 유선 또는 무선 통신 네트워크를 통하 여 통신을 수행할 수 있다. 예컨대, 통신 인터페이스는 프로세서의 제어에 따라서 3차원 스캐너(100, 110)와 통신을 수행할 수 있다. 통신 인터페이스는 3차원 스캐너(100, 110)로부터 로우 데이터를 수신하거나, 3차원 스캔 데이터를 획득할 수 있다. 실시 예에서, 통신 인터페이스는 3차원 스캐너(100, 110) 외의 다른 외부 전자 장치, 외 부 서버 등과도 통신을 수행하여 3차원 스캔 데이터를 획득할 수 있다. 통신 인터페이스는 블루투스, 와이파이, BLE(Bluetooth Low Energy), NFC/RFID, 와이파이 다이렉트(Wifi Direct), UWB, 또는 ZIGBEE 등의 통신 규격에 따른 통신을 수행하는 적어도 하나의 근거리 통신 모듈을 포함할 수 있다. 또한, 통신 인터페이스는 원거리 통신 규격에 따라서 원거리 통신을 지원하기 위한 서버와 통신을 수행하 는 원거리 통신 모듈을 더 포함할 수 있다. 구체적으로, 통신 인터페이스는 인터넷 통신을 위한 네트워크 를 통하여 통신을 수행하는 원거리 통신 모듈을 포함할 수 있다. 예컨대, 통신 인터페이스는 3G, 4G, 및/ 또는 5G 등의 통신 규격에 따르는 통신 네트워크를 통하여 통신을 수행하는 원거리 통신 모듈을 포함할 수 있다. 또한, 통신 인터페이스는 3차원 스캐너(100, 110)나 외부 서버, 외부 전자 장치 등과 유선으로 통신할 수 도 있다. 이를 위해 통신 인터페이스는 3차원 스캐너(100, 110)나 외부 전자 장치와 유선 케이블로 연결되 기 위한 적어도 하나의 포트를 포함할 수 있다. 통신 인터페이스는 적어도 하나의 포트를 통하여 유선 연 결된 3차원 스캐너(100, 110)나 외부 전자 장치와 통신을 수행할 수 있다. 실시 예에서, 통신 인터페이스는 제1 이미지에 포함된 치아 영역이 가상 치아 영역으로 대체된 제2 이미지 를 외부 전자 장치나 외부 서버 등으로 전송할 수 있다. 실시 예에서, 통신 인터페이스는 제2 이미지로부터 획득된, 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴을 포함하는 제3 이미지를 외부 전자 장치나 외부 서버 등으로 전송할 수 있다. 도 4는 실시 예에 따라, 데이터 처리 장치가 3차원 구강 모델로부터 가상의 구강 모델을 획득하는 방법을 설명 하기 위한 도면이다. 데이터 처리 장치는 3차원 스캐너로부터 로우 데이터를 수신하고, 로우 데이터에 근거하여 구강의 구조를 3차원 적으로 모델링하여 3차원 구강 모델을 생성할 수 있다. 또는 데이터 처리 장치는3차원 스캐너로부터 3차원 구강 모델을 수신할 수 있다. 또는 데이터 처리 장치는 외부 서버나 외부 장치 등으로부터 유선 또는 무선 통신망을 통해 3차원 구강 모델을 수신할 수도 있다. 3차원 구강 모델은 3차원 스캔 데이터로도 호칭될 수 있다. 3차원 구강 모델은 객체, 즉, 환자의 현재의 구강에 대한 3차원 스캔 데이터를 포함할 수 있다. 3차원 구강 모델은 객체의 구강 중 상악, 하악, 교합 중 적어도 하 나에 대한 3차원 스캔 데이터를 포함할 수 있다. 도 4를 참조하면, 데이터 처리 장치는 화면에 3차원 구강 모델을 출력할 수 있다. 실시 예에서, 데이터 처리 장치는 3차원 구강 모델을 목표로 하는 가상의 구강 모델로 대체할 수 있 는 전용 소프트웨어를 실행할 수 있다. 실시 예에서, 데이터 처리 장치는 치아의 위치나 형태 등을 조절하는 전용 소프트웨어를 실행함으로써, 3차원 구강 모델을 이상적인 형태로 변형할 수 있다. 데이터 처리 장치는 자동으로, 또는 사용자 입력에 따라 수 동으로 3차원 구강 모델에 포함된 치아의 위치나 형태를 조절할 수 있다. 예컨대, 데이터 처리 장치는 3차 원 구강 모델에 포함된 치아의 위치를 좌, 우, 상, 하 방향으로 조절하거나, 치아를 회전시키거나, 치아의크기를 조절하거나, 치아의 간격을 조절하거나, 치아의 일부를 발치하고 남은 치아들의 크기를 조절하여 치아들 이 재배치되도록 하거나, 상악이나, 하악, 교합 중 적어도 하나의 치아의 정중선을 설정해 악궁의 정면을 재 설 정하거나, 치아의 색상을 조절하는 것 중 적어도 하나를 수행함으로써 3차원 구강 모델을 변형할 수 있다. 이하, 설명의 편의를 위하여, 데이터 처리 장치가 3차원 구강 모델을 변형하여 생성한 모델을 가상의 구강 모델로 호칭하기로 한다. 가상의 구강 모델은 3차원 구강 모델로부터 생성된 것으로, 환자의 구 강에 대해 치과적 치료를 수행했을 때의 향후 목표가 되는 가상의 모델을 의미할 수 있다. 실시 예에서, 데이터 처리 장치는, 사용자가 설정한 정보에 따라 치아의 위치나 형태, 배열 상태 등을 조절하여, 조절 전과 조절 후의 치아의 상태를 비교한 시뮬레이션을 함께 보여줄 수 있다. 예컨대, 도 4에 도시 된 바와 같이, 데이터 처리 장치는 3차원 구강 모델과 가상의 구강 모델을 하나의 화면에 함께 출력 할 수 있다. 또는 데이터 처리 장치는 3차원 구강 모델과 가상의 구강 모델을 각각 별개의 화면에 출 력할 수 있다. 도 5는 일 실시 예에 따라, 데이터 처리 장치가 제1 이미지로부터 제2 이미지를 획득하는 방법을 설명하기 위한 도면이다. 도 5를 참조하면, 데이터 처리 장치는 제1 이미지를 획득할 수 있다. 실시 예에서, 제1 이미지는 치아가 보이는 객체의 얼굴 이미지를 의미할 수 있다. 데이터 처리 장치에 장착되거나 또는 데이터 처리 장치와 유무선 통신을 통해 연결된 카메라(미도시)는 치아가 보이는 객체의 얼굴을 촬영하여 제1 이미지를 획득할 수 있다. 또는 데이터 처리 장치는 통신망을 통해 외 부 서버나 컴퓨팅 장치 등으로부터 제1 이미지를 수신할 수 있다. 또는 데이터 처리 장치는 USB나 HDMI 케 이블 등을 통해 외부 전자 기기나 외부 저장 매체 등으로부터 제1 이미지를 수신할 수 있다. 데이터 처리 장치는 데이터 처리 장치의 화면에 제1 이미지를 출력할 수 있다. 실시 예에서, 데이터 처리 장치는 제1 이미지에서 치아 영역을 검출할 수 있다. 치아 영역은 제 1 이미지에서 객체의 치아를 포함하는 영역으로, 객체의 입술 사이로 보이는 치아의 전부 또는 일부를 포 함하는 영역을 의미할 수 있다. 치아 영역은 객체의 표정이나 포즈, 객체가 입을 벌린 정도 등에 따라 드 러나는 위치나 영역 등이 달라질 수 있다. 실시 예에서, 데이터 처리 장치는 객체의 현재의 구강에 대한 스캔 데이터인, 3차원 구강 모델을 획득할 수 있 다. 또한, 데이터 처리 장치는 도 4에서 설명한 방법에 따라, 3차원 구강 모델로부터 가상의 구강 모델을 획득할 수 있다. 실시 예에서, 데이터 처리 장치는 가상의 구강 모델에서, 제1 이미지에서 검출한 치아 영역과 오버랩되는 영역을 획득할 수 있다. 이하, 가상의 구강 모델에 포함된, 제1 이미지의 치아 영역과 오버랩되는 영역을 가상 치아 영 역으로 호칭하기로 한다. 전술한 바와 같이, 가상 치아 영역은 객체의 구강에 포함될 미래의 치아 영역으로, 향 후 객체의 구강 치료가 끝났을 때 제1 이미지에 포함된 객체의 치아 영역이 어떻게 보이는지를 나타낼 수 있다. 실시 예에서, 데이터 처리 장치는 참조 데이터를 이용하여 가상의 구강 모델에서 치아 영역에 대응하 는 가상 치아 영역을 획득할 수 있다. 참조 데이터는 스캔을 통해 획득된 대상체의 표면 데이터를 의미할 수 있 다. 참조 데이터는 이미지에서 서로 매칭되는 영역을 얼라인(align)하는 데 이용될 수 있다. 실시 예에서, 데이터 처리 장치는 자동으로, 가상의 구강 모델에서, 제1 이미지의 치아 영역과 오버랩되는 가상 치아 영역을 획득하고, 가상 치아 영역을 제1 이미지의 치아 영역에 정렬함으로써, 제2 이미지를 획득할 수 있다. 예컨대, 데이터 처리 장치는 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체하기 위한 전용 소프트웨어 를 실행함으로써, 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체할 수 있다. 데이터 처리 장치는 자동으로 제1 이미지의 치아 영역과 가상의 구강 모델에서 매칭되는 특징 점을 검출하고, 검출된 특징 점들을 정렬시켜 치아 영역이 가상 치아 영역으로 대체되도록 할 수 있다. 또 는 데이터 처리 장치는 자동으로 제1 이미지에서 입술 라인을 검출하고, 검출된 입술 라인의 내부 영역에 서 특징 점을 검출하고, 입술 라인의 내부에서 검출된 특징 점과 매칭되는, 가상의 구강 모델에서 매칭되 는 특징 점을 검출하여 특징 점끼리 정렬함으로써 치아 영역이 가상 치아 영역으로 대체되도록 할 수있다. 또는, 실시 예에서, 데이터 처리 장치는 수동으로 가상 치아 영역을 제1 이미지의 치아 영역에 정렬 할 수도 있다. 예컨대, 사용자는 데이터 처리 장치에 출력된 제1 이미지의 치아 영역에서 제1 특징 점의 위치를 선택하고, 가상의 구강 모델에서 제2 특징 점의 위치를 선택할 수 있다. 데이터 처 리 장치는 사용자가 선택한 위치에서의 제1 특징 점과 제2 특징 점을 서로 매칭시켜 정렬함으로써 제 1 이미지 내의 치아 영역이 가상 치아 영역으로 대체되도록 할 수 있다. 데이터 처리 장치는 제2 이미지을 화면에 출력할 수 있다. 도 5를 참조하면, 제2 이미지는 제1 이미 지와 동일한 객체의 얼굴 이미지를 포함하나, 객체의 치아 부분이 제1 이미지와 서로 다른 것을 알 수 있다. 즉, 제2 이미지는 치아 영역이 가상의 구강 모델에서 획득된 가상 치아 영역으로 대체 되어 있는 것을 알 수 있다. 이와 같이, 실시 예에 따르면, 데이터 처리 장치는 3차원 구강 모델로부터 획득된, 가상의 구강 모델에서 가상 치아 영역을 획득하고, 이를 제1 이미지의 치아 영역에 정렬하여 제2 이미지를 생성할 수 있다. 객체, 즉, 환자는 제2 이미지를 이용하여, 구강 치료가 모두 끝났을 때의 가상의 치아가 본인의 얼 굴에서 어떻게 보이는지를 용이하게 확인할 수 있다. 도 6과 도 7은 다른 실시 예에 따라, 데이터 처리 장치가 제1 이미지로부터 제2 이미지를 획득하는 방법을 설명 하기 위한 도면이다. 도 6을 참조하면, 데이터 처리 장치는 제1 이미지를 획득할 수 있다. 제1 이미지는 치아 영역이 보이 는 객체의 얼굴 이미지를 의미할 수 있다. 실시 예에서, 데이터 처리 장치는 제1 이미지 외에 부가 이미지를 더 획득할 수 있다. 부가 이미지 는 제1 이미지에 포함된 객체와 동일한 객체에 대한 이미지이나, 제1 이미지보다 객체의 치아가 더 많이 보이는 이미지를 의미할 수 있다. 즉, 부가 이미지는 제1 이미지에 포함된 치아 영역을 포함하 고, 나아가, 제1 이미지에 포함된 치아 영역에는 포함되지 않은 치아 영역을 더 포함하는 이미지일 수 있 다. 또한, 부가 이미지는 객체가 제1 이미지와 동일한 각도에서 촬영되어 획득되거나, 또는 제1 이미 지와 소정 각도 차이 이내에서 촬영되어 획득된 이미지일 수 있다. 데이터 처리 장치는 데이터 처리 장치에 장착되거나 또는 데이터 처리 장치와 유무선 통신을 통해 연결된 카메 라 등을 통해 제1 이미지 및 제1 이미지보다 객체의 치아가 더 많이 보이는 제2 이미지를 획득 할 수 있다. 또는 데이터 처리 장치는 통신망을 통해 외부 서버나 컴퓨팅 장치 등으로부터 제1 이미지 및/ 또는 제2 이미지를 수신하거나, USB나 HDMI 케이블 등을 통해 외부 전자 기기나 외부 저장 매체 등으로부 터 제1 이미지 및/또는 제2 이미지를 수신할 수 있다. 실시 예에서, 데이터 처리 장치는 이미지에 포함된 치아 영역을 가상의 치아 템플릿으로 대체하는 소프트웨어를 실행할 수 있다. 도 7을 참조하면, 데이터 처리 장치는 도 6에 도시된 부가 이미지를 데이터 처리 장치의 화면에 출력 할 수 있다. 또한, 데이터 처리 장치는 메모리 등에 저장되어 있는, 다양한 형태의 치아 템플릿을 화면에 출력할 수 있다. 치아 템플릿은 다양한 형태나 크기를 갖는 치아들의 배열 상태를 나타내는 정보를 의미할 수 있 다. 도 7에 도시된 바와 같이, 치아 템플릿은 다양한 형태의 템플릿을 포함할 수 있다. 예컨대, 치아 템플릿 은 치아의 모양이나 배열 상태에 따라 Natural 형태, Oval 형태, Square 형태, Natural M 형태, Oval M 형태, Square M 형태 등의 템플릿을 포함할 수 있다. 다만, 이는 하나의 실시 예로, 치아 템플릿은 위에서 언급하지 않은 또 다른 형태의 템플릿을 더 포함할 수도 있고, 위에서 언급한 템플릿 중 일부만을 포함할 수도 있다. 사용자는 데이터 처리 장치를 이용하여 치아 템플릿에 포함된 복수의 템플릿 중 하나를 선택할 수 있다. 또는 데이터 처리 장치는 치아 템플릿에 포함된 복수의 템플릿 중 부가 이미지에 포함된 객체의 치아 영역과 유사도가 가장 큰 템플릿을 자동으로 선택할 수도 있다. 데이터 처리 장치는 선택된 템플릿을 부가 이미 지에 포함된 치아 영역 위에 정렬할 수 있다. 데이터 처리 장치는 사용자 입력에 따라, 또는 자동으로, 부가 이미지에 포함된 치아 영역 위에 정렬된 템 플릿을 추가로 조절할 수 있다. 예컨대, 데이터 처리 장치는 사용자 입력에 따라, 또는 자동으로, 템플릿에 포 함된 치아의 모양이나 크기, 위치, 각도, 치아 간 간격, 복수 치아들의 배열 상태, 악궁의 정면, 치아의 재질이 나 색상 중 적어도 하나를 조절할 수 있다. 치아의 재질은 예컨대, 치아를 세라믹으로 할 것인지, 금으로 할 것 인지 등을 나타내는 정보일 수 있다. 치아의 색상은 치아의 미백 정도를 나타내는 정보, 예컨대, 치아의 채도, 밝기, 투명도, 그림자 표시 등을 포함할 수 있다. 데이터 처리 장치는 템플릿을 부가 이미지에 포함된 객체의 치아 영역에 맞게 조절함과 동시에, 객체의 구 강을 치료했을 때 목표로 하는 구강 형태에 맞게 조절함으로써 가상의 치아 템플릿을 획득할 수 있다. 이하, 선택된 치아 템플릿으로부터 획득된, 목표로 하는 이상적인 형태를 갖는 치아 템플릿을 가상의 치아 템플 릿이라고 호칭하기로 한다. 가상의 치아 템플릿은 치아나 치아 배열, 치아 색상 등이 목표로 하는 형상을 가지 는 경우를 나타내는 모델 데이터일 수 있다. 가상의 치아 템플릿은 선택된 치아 템플릿이 조절되어 획득될 수도 있고, 또는 선택된 치아 템플릿과 동일할 수도 있다. 도 7을 참조하면, 데이터 처리 장치는 부가 이미지 위에 가상의 치아 템플릿이 정렬된 상태를 화면 에 출력할 수 있다. 즉, 데이터 처리 장치는 부가 이미지에 포함된, 더 큰 치아 영역을 가상의 치아 템플릿으로 대체한 상태를 화면에 출력할 수 있다. 데이터 처리 장치는 가상의 치아 템플릿으로 대체된, 부가 이미지 내의 치아 영역에서, 제1 이미지 에 포함된 치아 영역에 대응하는 영역을 가상 치아 영역으로 획득할 수 있다. 가상 치아 영역은 가상의 치 아 템플릿으로 대체된 부가 이미지 내의 치아 영역 중, 제1 이미지에 포함된 치아 영역의 크기 나 위치에 맞는 영역을 의미할 수 있다. 부가 이미지와 제1 이미지 자체의 크기가 다른 경우, 가상 치아 영역은 부가 이미지와 제1 이미지의 크기 차이를 고려하여 획득될 수 있다. 데이터 처리 장치는 제1 이미지에 포함된 치아 영역을 가상의 치아 영역으로 대체함으로써 제2 이미지 를 획득할 수 있다. 데이터 처리 장치는 제2 이미지를 화면에 출력할 수 있다. 도 7에 도시된 바와 같이, 제2 이미지는 제1 이미지와 동일한 객체의 얼굴 이미지에 대한 것이나, 객 체의 치아 부분이 제1 이미지와 다른 것을 알 수 있다. 이는 부가 이미지의 더 큰 치아 영역에 정렬 된 가상의 치아 템플릿으로부터 획득된 가상 치아 영역으로 제2 이미지에 포함된 치아 영역이 대체되 어 있기 때문이다. 이와 같이, 실시 예에 의하면, 데이터 처리 장치는 부가 이미지에 포함된 더 큰 치아 영역을 목표로 하는 가상의 치아 템플릿으로 대체할 수 있다. 또한, 실시 예에 의하면 데이터 처리 장치는 가상의 치아 템플릿 으로 대체된 부가 이미지의 치아 영역에서 제1 이미지의 치아 영역에 대응하는 가상 치아 영역 을 획득하고, 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체함으로써, 제2 이미지를 획득할 수 있다. 도 8은 실시 예에 따라, 신경망을 통해 입력된 얼굴 이미지에서 다른 속성을 갖는 얼굴 이미지가 획득되는 것을 설명하기 위한 도면이다. 도 8을 참조하면, 맨 왼쪽에 Inpput으로 표기된 이미지가 원본 이미지이고, 원본 이미지 우측으로, 4x7 행렬로 표기된 이미지가 출력 이미지이다. 도 8은 원본 이미지가 두 개의 도메인으로 출력된 것을 도시한다. 즉, 도 8 에서 원본 이미지는 신경망을 통해 Blond hair, Hair style, Aged, Pale Skin의 도메인과, Angry, Happy, Fearful의 도메인으로 각각 변환될 수 있다. 도 8은 단일 모델에서 두 개의 데이터 셋을 동시에 학습하여 출력한 결과이다. 다만, 도메인은 학습한 데이터 셋에 따라 바뀔 수 있으며, 학습한 데이터 셋이 더 많아질 경우, 도메인 또한 더 증가할 수 있다. 실시 예에서, 데이터 처리 장치는 제2 이미지를 신경망에 입력시킬 수 있다. 이 경우, 도 8의 4x7 행렬에 도시된 이미지와 마찬가지로 제2 이미지에 포함된 객체의 얼굴은 원래의 헤어 색과는 다른, 금발의 머리를 갖거나, 다른 헤어 스타일을 갖거나, 나이든 얼굴로 바뀌거나, 또는 얼굴 색이 바뀐 스타일을 갖는 형태로 출력 될 수 있다. 또한, 제2 이미지에 포함된 객체의 얼굴은 원래의 얼굴과는 다른 표정, 예컨대, 화가 난 얼굴, 행복한 얼굴, 두려운 얼굴 표정 등과 같이 다른 속성을 갖는 이미지로 변환될 수 있다. 도 9는 실시 예에 따라, 입력된 얼굴 이미지에서 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하 도록 학습된 신경망을 설명하기 위한 도면이다. 도 9를 참조하면, 인공지능을 이용하여 입력된 얼굴 이미지에서 다른 속성을 갖는 얼굴 이미지를 생성하는 과정 은 크게 두 가지 과정으로 이루어질 수 있다. 먼저 훈련 과정에서는 복수의 훈련 데이터를 입력으로 하여 신경망, 즉, 뉴럴 네트워크 모델을 훈련시킬 수 있다. 각 훈련 결과인 출력 데이터는 다시 뉴럴 네트워크 모델로 피드백되어 뉴럴 네트 워크 모델의 가중치를 업데이트하는데 이용될 수 있다. 뉴럴 네트워크 모델은 복수의 훈련 데이터가 입력된 것에 응답하여, 복수의 훈련 데이터로부터 다른 속성 을 갖는 데이터를 검출하는 방법을 학습 및 또는 훈련할 수 있으며, 학습 및/또는 훈련된 결과에 기초하여 생성 될 수 있다. 보다 구체적으로, 훈련 데이터는 복수의 얼굴 이미지를 포함할 수 있다. 복수의 얼굴 이미지는 다양한 포 즈나 표정을 짓는 얼굴 이미지, 치아 영역이 보이는 정도가 다른 이미지, 머리 모양이나 화장을 한 정도, 피부 색 등과 같은 스타일이 다양한 이미지 중 적어도 하나를 포함할 수 있다. 뉴럴 네트워크 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행할 수 있다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화 되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN: Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 뉴럴 네트워크 모델은 매핑 네트워크(mapping network)를 포함할 수 있다. 매핑 네트워크는 비선형으로, 특징들 사이의 편중된 상관관계를 줄여 줄 수 있다. 매핑 네트워크는 복수 개의 레이어들을 포함할 수 있다. 각 레이어들은 적어도 하나의 노드로 표현될 수 있고, 계층 간 노드들은 엣지로 연결된다. 노드들은 이전 및 이후 의 레이어들에 포함된 노드들과 완전 연결(Fully Connected)되어 있을 수 있다. 뉴럴 네트워크 모델은 입력된 정보를 매핑 네트워크에 통과시켜 중간 벡터를 획득할 수 있다. 중간 벡터는 다른 스타일의 속성 정보를 담고 있는 웨이트일 수 있다. 예를 들어, 속성 정보로부터 추출된 특징 벡터가 화내 는 표정에 대응하는 특징에 관한 것이면, 뉴럴 네트워크 모델은 이러한 특징을 갖는 중간 벡터를 생성할 수 있다. 예를 들어, 속성 정보로부터 추출된 특징 벡터가 헤어 스타일과 관련된 속성에 대응하는 특징에 관한 것이면, 뉴럴 네트워크 모델은 이러한 헤어 스타일의 특징을 갖는 중간 벡터를 생성할 수 있다. 뉴럴 네트워크 모델은 생성한 중간 벡터를 이용하여, 복수의 레이어들 마다 스타일 정보를 입히는 방식으 로 이미지를 합성할 수 있다. 뉴럴 네트워크 모델은 텐서(tensor)를 입력 받을 수 있다. 텐서는 딥러닝 모 델의 정보를 담고 있는 데이터 스트럭쳐일 수 있다. 텐서는 학습 데이터들의 스타일이 반영되지 않은 베이스 이 미지로, 평균적인 이미지를 표현하는 정보일 수 있다. 실시 예에서, 텐서는 기본 스타일을 갖는 부가 정보 영역 의 레이아웃을 의미할 수 있다. 뉴럴 네트워크 모델에는 4X4X512의 텐서로 시작해서 1024X1024X3으로 끝나는 복수 개의 레이어들을 포함할 수 있다. 각 레이어들은 컨벌루션, 업샘플링을 통해 다음 레이어로 연결될 수 있다. 웨이트는 뉴럴 네트워크 모델의 각각의 레이어들로 입력될 수 있다. 뉴럴 네트워크 모델은 중간 벡터, 즉, 웨이트를 이용하여 각각의 레이어들에 대한 속성이나 스타일을 표현하도록 학습될 수 있다. 계층의 깊이가 얕을수록 이미지의 하위 레벨 특징, 즉, coarse한 특징들이 추출되고, 계층의 깊이가 깊어질수록 디테일한 상위 레벨 특징이 추출될 수 있다. 뉴럴 네트워크 모델은 하위 레벨부터 상위 레벨에서 획득한 특징들에 기반하여 결과 이미지를 획득할 수 있다. 결과 이미지는 다른 도메인 상의 이미지일 수 있다. 각 훈련 결과는 뉴럴 네트워크 모델로부터 출력 데이터로 나타날 수 있다. 출력 데이터는 입력 된 얼굴 이미지와는 다른 속성을 갖는 얼굴 이미지일 수 있다. 출력 데이터는 뉴럴 네트워크 모델의 가중 치들을 업데이트하는데 이용될 수 있다. 훈련 결과가 일정 신뢰도를 넘도록 뉴럴 네트워크 모델이 훈련되 면 이 모델은 훈련된 뉴럴 네트워크 모델로서 이용될 수 있다. 적용 과정에서는 적용 데이터를 훈련된 뉴럴 네트워크 모델에 입력하여, 입력된 적용 데이터 로부터 입력된 적용 데이터와는 다른 속성을 갖는 결과 데이터를 획득할 수 있다. 실시 예에서, 적용 데이터는 제1 이미지의 치아 영역이 가상 치아 영역으로 대체된 제2 이미지일 수 있고, 뉴럴 네트워크 모델로부터 출력되는 결과 데이터는 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼 굴을 포함하는 제3 이미지일 수 있다. 실시 예에서, 뉴럴 네트워크 모델은 GAN(Generative Adversarial Network)일 수 있다. 보다 구체적으로, 뉴럴 네트워크 모델은 Star GAN(Star Generative Adversarial Networks)일 수 있다. Star GAN은 주어진 이미지의 속성을 하나의 도메인에서 다른 도메인으로 바꾸는 이미지 변환 모델이다. 속성은 이미지에 있는 의미 있는 특징들을 의미할 수 있다. 예컨대, 속성은 얼굴 표정이나 얼굴 포즈, 얼굴 포즈에 따른 얼굴 각도, 치아가 보이는 정도, 나이, 얼굴 색이나 머리 색, 머리 스타일 등을 포함할 수 있다. 도메인은 속성 값을 공유하는 이 미지들의 집합을 호칭할 수 있다. 예컨대, 화난 표정과 금발 헤어 색상은 서로 다른 도메인을 구성할 수 있다. 뉴럴 네트워크 모델은 변환하고자 하는 도메인 정보와 원본 이미지, 즉, 제2 이미지를 입력 값으로 받을 수 있다. Star GAN은 모든 가능한 도메인들 사이의 매핑을 하나의 generator를 통해 학습할 수 있다. 예컨대, Star GAN은 이미지와 도메인 정보를 모두 훈련 데이터로 입력 받고, 유연하게 이미지를 알맞은 도메인으로 바꾸는 것을 학습할 수 있다. Star GAN은 이러한 도메인 정보를 표현하기 위해 binary나 one-hot vector와 같은 형식을 사용할 수 있다. Star GAN은 뉴럴 네트워크 모델을 학습하는 동안 랜덤하게 타겟 도메인 라벨을 만 들어내고 모델이 유연하게 이미지를 타겟 도메인으로 변환하도록 학습될 수 있다. 이를 통해, 도메인 라벨이 제 어될 수 있고, 이미지가 원하는 도메인으로 변환될 수 있다. Star GAN은 mask vector를 도메인 라벨에 추가함으로써, joint 학습이 가능하다. 즉, Star GAN은 뉴럴 네트워 크 모델이 모르는 라벨을 무시할 수 있고 특정 데이터셋의 라벨들에 초점을 맞출 수 있다. 이러한 맥락에 서, 뉴럴 네트워크 모델은 얼굴 표정 합성과 같은 업무를 잘 수행할 수 있다. 뉴럴 네트워크 모델을 이용하여 이미지로부터 다른 속성을 갖는 이미지를 획득하는 방법을 학습하는 동작 은, 데이터 처리 장치에서 수행될 수 있다. 또는 이러한 학습 동작은 데이터 처리 장치와는 별개의 외부의 컴퓨 팅 장치에서 수행될 수 있다. 예를 들어, 뉴럴 네트워크 모델을 이용하여 이미지로부터 다른 속성을 갖는 이미지를 획득하는 방법을 학습하는 동작은, 상대적으로 복잡한 연산량을 필요로 할 수 있다. 이에 따라, 외부 의 컴퓨팅 장치가 학습하는 동작을 수행하고, 데이터 처리 장치는 외부 컴퓨팅 장치로부터 학습이 끝난 뉴럴 네 트워크 모델을 수신함으로써, 데이터 처리 장치에서 수행되어야 하는 연산량을 줄일 수 있다. 데이터 처리 장치는 뉴럴 네트워크 모델을 외부 서버로부터 수신하여 메모리에 저장하고, 저장된 뉴럴 네트워크 모델 을 이용하여 이미지로부터 다른 속성을 갖는 이미지를 획득할 수 있다. 도 10은 실시 예에 따른 CNN 기반의 신경망을 도시한다. 구체적으로, 도 10은 복수개의 계층들을 포함하여 복수의 심도(depth)를 갖는 DCNN(Deep Convolution Neural Network)을 도시한다. 데이터 처리 장치는 CNN 기반의 신경망을 통하여 제2 이미지에서 제3 이미지 를 획득할 수 있다. 실시 예에서, CNN 기반의 신경망의 입력층(input layer)으로 제2 이미지가 입력될 수 있다. CNN 기반의 신경망은 컨볼루션 계층(convolution layer)과 풀링 계층(pooling layer)이 번갈아 가면서 배치 되며, 각 계층 필터(filter)의 심도(depth)는 왼쪽에서 오른쪽으로 갈수록 증가하게 된다. 또한, CNN 기반의 신 경망의 최종 단은 완전 연결 계층(fully connected layer)로 형성될 수 있다. 컨볼루션 계층(convolution layer)은 컨볼루션 연산에 따라서 생성되는 데이터들의 계층이며, 풀링 계층 (pooling layer)은 서브 샘플링(subsampling) 또는 풀링이라는 연산을 통하여 데이터의 숫자 또는 크기를 줄이 기 위한 계층이다. 컨볼루션 계층(convolution layer)과 풀링 계층(pooling layer)을 통과하면서, 입력된 이미 지의 특징을 나타내는 데이터들이 생성되게 된다. 그리고, 이러한 컨볼루션 계층 및 풀링 계층을 통과하여 생성 된 데이터들을 완전 연결 계층(fully connected layer)으로 형성되는 숨겨진 계층(hidden layer)을 통하여 특 징들로부터 인식되는 객체에 대한 결과 데이터를 출력할 수 있다. 예를 들어, CNN 기반의 신경망은 입력층(input layer), 제1 컨볼루션 계층(convolution layer), 제1 풀링 계층(pooling layer), 제2 컨볼루션 계층(convolution layer), 제2 풀링 계층(pooling layer), 숨겨진 계층(hidden layer) 및 출력 계층(output layer)을 포함할 수 있다. 여기서, 컨볼루션 계층 및 풀링 계층의 심도(depth)는 가변될 수 있으며, 숨겨진 계층(hidden layer)의 심도도 가변될 수 있다. 또한, 컨볼루션 계층 및 풀링 계층의 심도(depth)가 깊어질수록 보다 정확한 출력 데이 터가 획득될 수 있다. 이는 컨볼루션 계층 및 풀링 계층의 심도(depth)가 깊어질수록 입력된 이미지의 특징들을 나타내는 정보들이 더욱 구체적인 형태를 가지기 때문에 해당 특징들로부터 인식되는 객체 또한 보다 정확히 인 식될 수 있기 때문이다. 또한, 신경망의 심도 및 형태는 결과의 정확도, 결과의 신뢰도, 프로세서의 연산 처리 속도 및 용량 등을 고려하여 매우 다양하게 설계될 수 있다. 도 11은 실시 예에 따른 구강 이미지 처리 방법을 도시한 순서도이다. 도 11을 참조하면, 데이터 처리 장치는 제1 이미지를 획득할 수 있다(단계 1110). 제1 이미지는 치아 영역이 보 이는 객체의 얼굴 이미지를 포함할 수 있다. 데이터 처리 장치는 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체할 수 있다(단계 1120). 일 실시 예에서, 데이터 처리 장치는 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체하기 위해, 3차 원 스캔 데이터, 즉, 3차원 구강 모델을 이용할 수 있다. 데이터 처리 장치는 객체의 구강에 대한 3차원 구강 모델을 획득하고, 이로부터 목표가 되는 가상의 구강 모델을 획득할 수 있다. 데이터 처리 장치는 가상의 구강 모델에서, 제1 이미지에 포함된 치아 영역과 오버랩되는 영역을 가상 치아 영역으로 획득할 수 있다. 다른 실시 예에서, 데이터 처리 장치는 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체하기 위해, 치아 템플릿을 이용할 수 있다. 또한, 데이터 처리 장치는 제1 이미지와 함께 부가 이미지를 더 이용할 수 있다. 부가 이미지는 제1 이미지보다 객체의 치아 영역이 더 큰 치아 영역을 포함하는 객체의 얼굴 이미지를 의 미할 수 있다. 데이터 처리 장치는 부가 이미지에 포함된 더 큰 치아 영역에 가상의 치아 템플릿을 정렬할 수 있다. 가상의 치아 템플릿은 데이터 처리 장치에 저장된 치아 템플릿으로부터 획득된, 객체의 치아가 목표로 하 는 치아 템플릿을 의미할 수 있다. 데이터 처리 장치는 가상의 치아 템플릿으로 부가 이미지의 치아 영역을 대 체하고, 이 중 제1 이미지에 포함된 치아 영역에 대응하는 영역을 가상 치아 영역으로 획득할 수 있다. 데이터 처리 장치는 제1 이미지에 포함된 치아 영역을 가상 치아 영역으로 대체하여 제2 이미지를 획득할 수 있다. 데이터 처리 장치는 제2 이미지로부터 다른 속성의 얼굴을 포함하는 제3 이미지를 획득할 수 있다(단계 1130). 데이터 처리 장치는 입력된 얼굴 이미지로부터 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하도 록 학습된 신경망을 이용하여, 제2 이미지로부터 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴을 포함하 는 제3 이미지를 획득할 수 있다. 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴은 제2 이미지에 포함된 얼굴과 포즈, 표정, 치아 영역이 보 이는 정도, 피부 색, 헤어 스타일, 메이크업 등의 스타일 중 적어도 하나가 다른 얼굴일 수 있다. 데이터 처리 장치가 이용하는 신경망은 입력된 얼굴 이미지의 속성을 변환하여 다중 도메인에서의 얼굴 이미지 를 획득하는 심층 신경망(DNN: Deep neural network)일 수 있다. 또한, 심층 신경망은 StarGAN(Star Generative Adversarial Networks)일 수 있다. 본 개시의 일 실시 예에 따른 구강 이미지 처리 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 또한, 본 개시의 실시 예는, 구강 이미지 처리 방법을 실행하는 적어도 하나의 인스트럭션을 포함하는 하나 이상의 프로그램이 기록된 컴퓨터로 읽을 수 있는 저장 매체가 될 수 있다. 또한, 전술한 본 개시의 실시 예에 따른 구강 이미지 처리 방법은 치아 영역이 보이는 객체의 얼굴 이미지를 제 1 이미지로 획득하는 단계, 상기 제1 이미지에 포함된 상기 치아 영역을 가상 치아 영역으로 대체하여 제2 이미 지를 획득하는 단계 및 입력된 얼굴 이미지로부터 상기 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하도록 학습된 신경망을 이용하여, 상기 제2 이미지로부터 상기 제2 이미지에 포함된 얼굴과 다른 속성을 갖는 얼굴을 포함하는 제3 이미지를 획득하는 단계를 포함하는, 구강 이미지 처리 방법을 구현하기 위한 프로그 램이 기록된 컴퓨터로 판독 가능한 기록 매체를 포함하는 컴퓨터 프로그램 제품으로 구현될 수 있다. 상기 컴퓨터 판독 가능 저장 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포 함할 수 있다. 여기서, 컴퓨터 판독 가능 저장 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 구성된 하드웨어 장치가 포함될 수 있다. 여기서, 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기 서, '비일시적 저장매체'는 저장 매체가 실재(tangible)하는 장치임을 의미할 수 있다. 또한, '비일시적 저장매 체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시 예에 따르면, 본 문서에 개시된 다양한 실시 예들에 따른 데이터 처리 방법은 컴퓨터 프로그램 제품 (computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매 체(예: compact disc read only memory (CD-ROM))의 형태로 배포될 수 있다. 또는, 어플리케이션 스토어(예: 플레이 스토어 등)를 통해 또는 두 개의 사용자 장치들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운 로드 또는 업로드)될 수 있다. 구체적으로, 개시된 실시 예에 따른 컴퓨터 프로그램 제품은 개시된 실시 예에 따른 데이터 처리 방법을 수행하기 위해 적어도 하나의 인스트럭션을 포함하는 프로그램이 기록된 저장 매체를 포함할 수 있다. 이상에서 실시 예들에 대하여 상세하게 설명하였지만 본 발명의 권리 범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11"}
{"patent_id": "10-2021-0128340", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 실시 예에 따른 구강 이미지 처리 시스템을 설명하기 위한 도면이다. 도 2는 실시 예에 따른 데이터 처리 장치의 내부 블록도이다. 도 3은 도 2의 데이터 처리 장치의 일 예를 도시한 도면이다. 도 4는 실시 예에 따라, 데이터 처리 장치가 3차원 구강 모델로부터 가상의 구강 모델을 획득하는 방법을 설명 하기 위한 도면이다. 도 5는 일 실시 예에 따라, 데이터 처리 장치가 제1 이미지로부터 제2 이미지를 획득하는 방법을 설명하기 위한 도면이다. 도 6과 도 7은 다른 실시 예에 따라, 데이터 처리 장치가 제1 이미지로부터 제2 이미지를 획득하는 방법을 설명 하기 위한 도면이다. 도 8은 실시 예에 따라, 신경망을 통해 입력된 얼굴 이미지에서 다른 속성을 갖는 얼굴 이미지가 획득되는 것을 설명하기 위한 도면이다. 도 9는 실시 예에 따라, 입력된 얼굴 이미지에서 입력된 얼굴 이미지와 다른 속성을 갖는 얼굴 이미지를 생성하 도록 학습된 신경망을 설명하기 위한 도면이다. 도 10은 실시 예에 따른 CNN 기반의 신경망을 도시한다. 도 11은 실시 예에 따른 구강 이미지 처리 방법을 도시한 순서도이다."}
