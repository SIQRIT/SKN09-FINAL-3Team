{"patent_id": "10-2021-0182830", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0093857", "출원번호": "10-2021-0182830", "발명의 명칭": "도구 정보를 활용하는 행동 인식 방법 및 장치", "출원인": "한양대학교 산학협력단", "발명자": "서영민"}}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "동영상에 등장하는 사람의 행동을 인식하는 방법으로서,입력 동영상을 받아들이고, 상기 입력 동영상에 등장하는 사람과 도구를 검출하는 단계;상기 사람의 하나 이상의 관절의 영상 내 좌표와 상기 도구의 영상 내 좌표를 사용하여, 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하는 단계; 및상기 사람과 상기 도구의 상호작용하는 경우, 상기 도구의 존재 사실을 참고하여 상기 입력 동영상 내에서 상기사람의 상기 행동을 결정하는 단계;를 포함하는 행동 인식 방법."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서, 상기 사람과 상기 도구를 검출하는 단계는상기 입력 동영상의 상기 도구가 검출된 프레임들 사이에 있는 하나 이상의 프레임에서 도구가 미검출된 프레임이 존재하는지 판단하는 단계; 및상기 도구가 검출된 프레임들 사이에서 상기 도구가 미검출된 프레임이 존재하는 것으로 판단된 경우, 상기 도구가 미검출된 프레임을 보간하여 도구 좌표를 결정하는 단계;를 포함하는 행동 인식 방법."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서, 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하는 단계는상기 사람의 하나 이상의 관절 중 각 관절에 대하여 해당 관절과 상기 도구의 상호작용 여부를 결정하는 단계;를 포함하는 행동 인식 방법."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 3에 있어서, 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하는 단계는각 관절과 상기 도구의 상호작용 여부를 나타내는 인접 행렬을 구성하는 단계;를 더 포함하며, 상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정하는 단계에서는 상기 인접 행렬을 고려하여 상기 사람의 상기 행동을 결정하는 행동 인식 방법."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서, 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하는 단계는관절들 및 뼈들의 연결 관계를 나타내는 골격 그래프를 조정하는 단계;를 더 포함하며, 상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정하는 단계에서는 상기 인접 행렬과 상기 골격 그래프를 고려하여 상기 사람의 상기 행동을 결정하는 행동 인식 방법."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 1에 있어서, 상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정하는 단계는상기 사람과 상기 도구의 상호작용하지 않는 경우 상기 도구에 관한 정보는 무시하고 상기 사람의 하나 이상의관절에 관한 정보만으로 상기 사람의 상기 행동을 결정하는 단계;공개특허 10-2023-0093857-3-를 포함하는 행동 인식 방법."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "동영상에 등장하는 사람의 행동을 인식하는 장치로서,프로그램 명령들을 저장하는 메모리와; 상기 메모리에 접속되고 상기 메모리에 저장된 상기 프로그램 명령들을실행하는 프로세서;를 구비하며,상기 프로그램 명령들은 상기 프로세서에 의해 실행될 때 상기 프로세서로 하여금:입력 동영상을 받아들이고, 상기 입력 동영상에 등장하는 사람과 도구를 검출하고;상기 사람의 하나 이상의 관절의 영상 내 좌표와 상기 도구의 영상 내 좌표를 사용하여, 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하고;상기 사람과 상기 도구의 상호작용하는 경우, 상기 도구의 존재 사실을 참고하여 상기 입력 동영상 내에서 상기사람의 상기 행동을 결정하는 동작을 수행하게 하는 행동 인식 장치."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 7에 있어서, 상기 프로세서로 하여금 상기 사람과 상기 도구를 검출하게 하는 프로그램 명령들은 상기프로세서로 하여금상기 입력 동영상의 상기 도구가 검출된 프레임들 사이에 있는 하나 이상의 프레임에서 도구가 미검출된 프레임이 존재하는지 판단하고,상기 도구가 검출된 프레임들 사이에서 상기 도구가 미검출된 프레임이 존재하는 것으로 판단된 경우 상기 도구가 미검출된 프레임을 보간하여 도구 좌표를 결정하게 하는 행동 인식 장치."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 7에 있어서, 상기 프로세서로 하여금 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하게 하는 프로그램 명령들은 상기 프로세서로 하여금상기 사람의 하나 이상의 관절 중 각 관절에 대하여 해당 관절과 상기 도구의 상호작용 여부를 결정하게 하는행동 인식 장치."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 7에 있어서, 상기 프로세서로 하여금 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하게 하는 프로그램 명령들은 상기 프로세서로 하여금각 관절과 상기 도구의 상호작용 여부를 나타내는 인접 행렬을 구성하게 하는 동작을 더 수행하게 하며,상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정하는 동작을 수행할 때에는 상기 인접 행렬을 고려하여상기 사람의 상기 행동을 결정하는 행동 인식 장치."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "청구항 10에 있어서, 상기 프로세서로 하여금 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하게하는 프로그램 명령들은 상기 프로세서로 하여금관절들 및 뼈들의 연결 관계를 나타내는 골격 그래프를 조정하는 동작을 더 수행하게 하며,상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정하는 동작을 수행할 때에는 상기 인접 행렬과 상기 골격그래프를 고려하여 상기 사람의 상기 행동을 결정하는 행동 인식 장치."}
{"patent_id": "10-2021-0182830", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "청구항 7에 있어서, 상기 프로세서로 하여금 상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정하게 하는프로그램 명령들은 상기 프로세서로 하여금공개특허 10-2023-0093857-4-상기 사람과 상기 도구의 상호작용하지 않는 경우 상기 도구에 관한 정보는 무시하고 상기 사람의 하나 이상의관절에 관한 정보만으로 상기 사람의 상기 행동을 결정하게 하는 행동 인식 장치."}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "동영상 내에서 인간이 사용하고 있거나 그 근처에 존재하는 도구 또는 상호작용 가능성이 높은 도구에 대한 정보 를 반영하여, 높은 정확도로 인간의 행동을 인식할 수 있는 행동 인식 방법 및 장치를 제공한다. 본 발명의 행 동 인식 방법은 입력 동영상을 받아들이고, 상기 입력 동영상에 등장하는 사람과 도구를 검출하는 단계; 상기 사 람의 하나 이상의 관절의 영상 내 좌표와 상기 도구의 영상 내 좌표를 사용하여, 상기 사람과 상기 도구가 상호 작용하는 것인지 여부를 결정하는 단계; 및 상기 사람과 상기 도구의 상호작용하는 경우, 상기 도구의 존재 사실 을 참고하여 상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정하는 단계를 포함한다."}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 영상 분석 방법 및 장치에 관한 것으로서, 특히, 동영상 내에 있는 인간의 행동을 인식하는 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간 행동 인식은 인간과 컴퓨터의 상호 작용에 필수적이며, 여러 응용 분야에서 중요한 역할을 한다. 다양한 인식 방법 중에서, 골격 기반의 GCN(Graph Convolutional Network) 행동 인식은 속도, 카메라 시점, 배경에 의 한 간섭 등의 동적 변화에 대처할 수 있고 기존 RGB 이미지 기반 행동 인식보다 더 강건하기 때문에 주목받고 있다. 골격 데이터는 인간 관절의 좌표 시퀀스로 표현되며 자세 추정 알고리즘 또는 동역학적 깊이 센서를 통 해 획득할 수 있다. 골격 데이터 기반 행동 인식을 위한 초기 딥러닝 접근 방식은 컨벌루션 신경망(CNN: Convolutional neural network) 또는 순환 신경망(RNN: Recurrent neural network)을 사용하는 것이었다. CNN 기반 방법들은 수동으로 설계된 변환 규칙에 따라 골격 데이터를 이미지로 변환하고 인식하며, 병렬성이 우수하 고 훈련이 쉽다는 장점이 있다. RNN 기반 방법들은 골격 데이터를 공간 및 시간 차원에 대한 좌표 벡터 시퀀스 로 모델링하며, 이때 각 좌표 벡터는 인체 관절의 특성을 나타낸다. 그러나 CNN이나 RNN을 이용하는 행동 인식 은 속도, 카메라의 시점, 배경에 의한 간섭 등의 동적 변화에 강건하게 대처하지 못한다는 단점이 있다. 그리 고 골격 데이터는 그 특성상 벡터 시퀀스나 2차원 그리드로 표현할 때 관절 사이의 관계를 정확히 표현하는 것 이 불가능하다. 즉, 골격 데이터는 본질적으로 구조화되어 있어서, 비유클리드 공간의 그래프와 같은 임의의 형태로 일반화하기 어렵다. 그래프 컨벌루션 신경망(GCN: Graph convolution network) 기법은 골격 관절을 노드로, 뼈를 엣지 즉 선분으로 표현하여 비유클리드 공간에서의 그래프를 구성하고, 관절의 속성과 관절 간의 연결을 고려한 골격 구조를 기반 으로 행동을 인식한다. 예를 들어, 공간-시간 GCN(ST-GCN: Spatial temporal GCN)은 행동 인식에서 GCN으로 골격 데이터를 해석하기 위한 최초의 시도로서, 관절의 자연스러운 연결을 기반으로 한다(아래 비특허 선행기술 문헌 참조). ST-GCN은 속도, 카메라의 시점, 배경에 의한 간섭 등의 동적 변화에 강건하게 대처가능하기 때문 에 주목을 받았지만, 관절들 간의 관계를 국부적 영역에서만 찾는 것이 가능하고 행동 인식 정확도가 높지 않다 는 단점이 있다. ST-GCN과 같은 GCN의 정확도가 낮은 이유는 인간의 골격에만 집중할 뿐이고 주변 환경을 고려하지 않기 때문인 것으로 보인다. 영상 내에서 사람이 도구를 들고 있을 경우 등장할 수 있는 행동은 환경이나 도구에 의해 결정 가능한 범위가 제한되는 경우가 많음에도 불구하고 기존 기법들은 주변 환경이나 도구를 고려하지 않는다. 단 순히 관절과 뼈의 움직임만으로 사람의 행동을 판단하는 것은 처음부터 부족한 정보를 가지고 판단하는 것이다. 예를 들어, 얼굴 가까이에 위치하다가 하강하는 컵의 손잡이를 잡고 있는 사람은 무언가를 마시고 컵을 내려놓 는 것으로 생각되지, 춤추거나 샤워하고 있을 가능성은 지극히 낮다. ST-GCN을 위시한 GCN은 주변 환경이나 또 는 상호작용 가능성이 있는 도구를 고려하지 않기 때문에, 유사한 동작에 대해서도 정확한 결론을 도출해내지 못하고 낮은 정확도를 보이게 된다. ST-GCN의 단점을 해결하기 위하여 행동-구조 GCN(AS-GCN: Actional-structure GCN)이나 2-스트림 적응적 GCN(2S-AGCN: Two-stream adaptive GCN)과 같은 후속 기법이 제시되었지만, 이 모델들도 여전히 인간의 골격에 만 집중하여 보완되었을 뿐이며, 주변 환경을 고려하지 않는 근본적인 문제는 여전히 해결되지 않았다. 이와 관련하여, 등록특허공보 10-1758693호(발명의 명칭: 물체-행동 관계 모델에 기반한 행동 인식 방법 및 그 장치)는 영상 내에서 사람의 행동이 발생하는 공간이나 장소 또는 행동에 관련된 물체의 종류를 감지하고, 사전 에 정의된 물체-행동 간의 관계 모델을 이용하여 물체의 의미를 감안해서 사람의 행동의 의미를 인식하는 장치 및 방법을 제시하고 있다. 그렇지만, 동영상 내에서 물체를 식별하고 그 의미를 추론하여 사람의 행동을 분류 하는 것은 현 시점에서 실현 가능성이 높다고 하기 어렵다. 그 이유는 훈련 데이터를 많이 확보하여 반복적 훈 련을 실시한다 하더라도 물체의 신경망의 물체 식별 능력은 제한적이고, 물체 종류의 식별과 그 의미를 추론할 수 있는 물체의 종류는 그 종류가 극히 제한적이기 때문이다.선행기술문헌 특허문헌 (특허문헌 0001) 등록특허공보 10-1758693호, 2017. 7. 11. 비특허문헌 (비특허문헌 0001) Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial temporal graph convolutional networks for skeleton-based action recognition. arXiv preprint arXiv:1801.07455"}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 동영상 내에서 인간의 환경, 특히 인간이 사용하고 있거나 그 근처에 존재하는 도구 또는 상호작용 가능성이 높은 도구에 대한 정보를 반영하여, 높은 정확도로 인간의 행동을 인식할 수 있는 행동 인식 방법 및 장치를 제공하는 것을 기술적 과제로 한다."}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 기술적 과제를 달성하기 위한 본 발명의 행동 인식 방법은 동영상에 등장하는 사람의 행동을 인식하기 위 한 것으로서, 입력 동영상을 받아들이고, 상기 입력 동영상에 등장하는 사람과 도구를 검출하는 단계; 상기 사 람의 하나 이상의 관절의 영상 내 좌표와 상기 도구의 영상 내 좌표를 사용하여, 상기 사람과 상기 도구가 상호 작용하는 것인지 여부를 결정하는 단계; 및 상기 사람과 상기 도구의 상호작용하는 경우, 상기 도구의 존재 사 실을 참고하여 상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정하는 단계를 포함한다. 상기 사람과 상기 도구를 검출하는 단계는 상기 입력 동영상의 상기 도구가 검출된 프레임들 사이에 있는 하나 이상의 프레임에서 도구가 미검출된 프레임이 존재하는지 판단하는 단계; 및 상기 도구가 검출된 프레임들 사이 에서 상기 도구가 미검출된 프레임이 존재하는 것으로 판단된 경우, 상기 도구가 미검출된 프레임을 보간하여 도구 좌표를 결정하는 단계를 포함할 수 있다. 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하는 단계는 상기 사람의 하나 이상의 관절 중 각 관 절에 대하여 해당 관절과 상기 도구의 상호작용 여부를 결정하는 단계를 포함할 수 있다. 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하는 단계는 각 관절과 상기 도구의 상호작용 여부를 나타내는 인접 행렬을 구성하는 단계를 더 포함할 수 있다. 이 경우, 상기 입력 동영상 내에서 상기 사람의 상 기 행동을 결정하는 단계에서는 상기 인접 행렬을 고려하여 상기 사람의 상기 행동을 결정할 수 있다.상기 사람 과 상기 도구가 상호작용하는 것인지 여부를 결정하는 단계는 관절들 및 뼈들의 연결 관계를 나타내는 골격 그 래프를 조정하는 단계를 더 포함할 수 있다. 이 경우, 상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정 하는 단계에서는 상기 인접 행렬과 상기 골격 그래프를 고려하여 상기 사람의 상기 행동을 결정할 수 있다. 상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정하는 단계는 상기 사람과 상기 도구의 상호작용하지 않 는 경우 상기 도구에 관한 정보는 무시하고 상기 사람의 하나 이상의 관절에 관한 정보만으로 상기 사람의 상기 행동을 결정하는 단계를 포함할 수 있다. 본 발명의 행동 인식 장치는 동영상에 등장하는 사람의 행동을 인식하기 위한 것으로서,프로그램 명령들을 저장 하는 메모리와, 상기 메모리에 접속되고 상기 메모리에 저장된 상기 프로그램 명령들을 실행하는 프로세서를 구 비한다. 상기 프로그램 명령들은 상기 프로세서에 의해 실행될 때 상기 프로세서로 하여금: 입력 동영상을 받 아들이고 상기 입력 동영상에 등장하는 사람과 도구를 검출하고, 상기 사람의 하나 이상의 관절의 영상 내 좌표 와 상기 도구의 영상 내 좌표를 사용하여 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하고, 상기 사람과 상기 도구의 상호작용하는 경우 상기 도구의 존재 사실을 참고하여 상기 입력 동영상 내에서 상기 사람 의 상기 행동을 결정하는 동작을 수행하게 하게 한다."}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 동영상 내에서 인간을 인식함과 아울러, 그 인간이 사용하고 있거나 그 근처에 존재하는 도구 또는 상호작용 가능성이 높은 도구를 인식하고, 인식된 도구에 관한 정보를 감안하여 상기 인간 이 어떤 행동을 하고 있는지 판단한다. 사람의 골격 움직임과 함께 주변 도구들을 고려하는 것은 자연스러운 인식흐름이라 할 수 있다. 본 발명의 인식 방법을 활용하면 도구를 사용하는 행동 클래스에 대한 인식의 정확 도를 높이고 인식 장치의 성능 향상을 기할 수 있다. 특히 본 발명은 도구의 정확한 분류를 전제로 하는 것이 아니라 도구의 위치를 토대로 인간과의 상호작용과 인 간의 행동을 판단하기 때문에, 객체 검출기의 성능이 낮아 도구의 종류를 정확히 식별할 수 없는 경우에도 그래 프 구성 및 인간 행동 인식이 가능하다. 다시 말해서, 본 발명은 행동을 수행할 때의 인간 관절은 작은 국부적 그룹 단위로 즉, 신체 부위가 전체적으로 동일한 패턴을 보이면서 움직인다는 특징을 이용한 것이라고 할 수 있 다. 본 발명은 프로그램의 큰 변동을 요함이 없이 기존 모델에 쉽게 적용할 수 있어서 범용성이 뛰어나다는 장 점이 있다. 본 발명의 행동 인식 방법은 실시간으로 입력되는 동영상에 대하여 적용할 수 있다. 이와 같은 행동 인식 방법 은 실시간 지능형 CCTV의 위험 예지 시스템에 적용하여 위급상황에 신속히 대처할 수 있게 해주고, 인간의 안전 과 시설 보안을 제고할 수 있다. 또한, 행동 인식 방법은 가정용 비서 로봇의 가사보조가 더욱 정밀하게 이루 어질 수 있게 해줄 수 있다. 한편, 드라마나 영화와 같은 영상 매체에서 등장인물들의 행동을 정밀하게 데이터 로 구축 가능하게 해줌으로써, 장면 검색이나 등장인물간 상호작용 관계분석에 활용될 수 있다. 예를 들어, 본 발명은 '특정 배우가 편지 쓰는 장면'과 같이 구체적인 자연어 쿼리 텍스트에 응답하여 검색 결과를 제공할 수 있게 해준다. 그밖에도, 다양한 학술 연구나 상업 서비스에 적용이 가능하다."}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 각 도면을 설명하면서 유사한 구성요소에 대해서는 유사한 참조부호를 사용하였다. 제1, 제2, 등의 서수가 다양한 구성요소들을 설명하는 데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용 된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. \"및/또는\"이라는 용어는 복수의 관련된 기재된 항목 들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다.어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 본 출원에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조 합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 달리 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속 하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일반 적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의미 를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 이하, 본 발명에 따른 바람직한 실시예를 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 본 발명의 일 실시예에 따른 행동 인식 장치의 기능적 블록도이다. 행동 인식 장치는 입력되는 동영상으로부터 사람의 관절 정보와 도구 정보를 획득하는 데이터 획득부, 상 기 관절 정보와 도구 정보를 토대로 각 관절과 도구의 상호작용 여부를 분류하는 상호작용 분류부, 각 관 절과 도구의 상호작용 여부를 토대로 상기 동영상에서 사람의 행동을 인식하는 행동 인식부를 포함한다. 상기 데이터 획득부는 동영상의 각 프레임에서 사람의 관절들의 좌표를 추출하고 사람 주변에 있는 물체 또는 도구의 좌표를 검출하는 객체 검출부와, 속된 영상 프레임들에서 도구의 위치를 보간하는 영상 보간 부를 포함할 수 있다. 상기 상호작용 분류부는 각 관절과 도구의 상호작용 여부를 검출하는 분류기 와, 영상에서 검출된 사람의 관절들과 도구의 상호작용 여부를 나타내는 인접 행렬을 구성하고 상기 분류 기의 검출 결과에 따라 상기 인접 행렬을 재구성하는 데이터 조정부를 포함한다. 객체 검출부는 입력 동영상에 등장하는 사람의 관절들의 위치 좌표를 검출하고, 상기 사람의 주변에 있고 상기 사람과 관련성이 있을 수 있는 도구의 위치 좌표를 검출한다. 상기 도구와 사람의 관련성은 해당 도구와 사람간의 거리 또는 해당 도구와 사람의 일부 관절간의 거리를 토대로 결정될 수 있다. 특히, 여러 사람 또는 여러 도구들이 검출되는 경우, 각 사람의 손 또는 발에 가장 가까운 도구를 최우선으로 고려할 수 있다. 이와 같이 사람과 도구의 상대적 위치를 토대로 유효한 탐지 범위에 있는 도구를 선택함으로써, 관련없는 도구를 일 차적으로 배제할 수 있다. 상기 입력 동영상은 RGB 영상 포맷으로 되어 있을 수 있는데, 본 발명이 이에 한정되는 것은 아니며 입력 동영 상이 다른 영상 포맷으로 되어 있을 수도 있다. 한편, 입력 동영상은 스트리밍 방식으로 입력될 수도 있다. 객체 검출부는 입력 동영상의 압축 해제, 패킷 해제, 및/또는 스트림 해제를 위한 디코더를 포함할 수 있 다. 일 실시예에 있어서, 객체 검출부가 위치 정보를 추출하는 관절은 총 25개로서, 도 2에 도시된 바와 같이 척추 기저부, 척추 중간, 목, 머리 왼쪽 어깨, 왼쪽 팔꿈치, 왼쪽 손목 왼쪽 손, 오른쪽 어깨, 오른쪽 팔꿈치, 오른쪽 손목, 오른손, 왼쪽 엉덩이, 왼쪽 무 릎, 왼쪽 발목, 왼발, 오른쪽 엉덩이, 오른쪽 무릎, 오른쪽 발목, 오른발, 척추, 왼손 끝, 왼손 엄지, 오른손 끝, 오른손 엄지를 포함할 수 있다. 그렇지만, 객체 검출부가 위치 정보를 추출하는 관절의 개수나 위치가 이에 한정되는 것은 아니다. 입력 동영상에 등장하는 사람의 관절들의 좌표와, 주변의 도구에 대한 좌표는 사전 훈련된 딥러닝 기반 객체 검 출기를 통해 검출할 수 있다. 일 예로, 상기 객체 검출기는 사전 훈련된 Yolo9000을 도구 검출 모델로 사용할 수 있다. 일 실시예에서, Yolo9000은 COCO 검출 데이터 세트와 ImageNet 분류 데이터 세트로 훈련되었으며, 이 데이터 세트들은 총 9418개의 클래스를 가지고 있었다. Yolo9000의 신뢰도 값은 0.4로 설정하여, 임계값 이하 의 객체는 객체로 판단하지 않도록 하였다. 그리고 Yolo9000 모델의 테스트 과정에서는 NTU-RGB+D 데이터 세트 와 NTU-RGB+D120 데이터 세트를 사용하였다. NTU-RGB+D 데이터 세트는 골격 기반 동작 인식에서 널리 사용되는 것으로서, RGB 비디오, 깊이 맵 시퀀스, 3D 골격 데이터, 및 적외선 비디오의 4가지 다른 양식이 있다. 실험예에서는 골격 데이터와 RGB 데이터를 사용하였다. 골격 데이터에는 60개의 클래스로 분류된 56,880개의 골격 시 퀀스가 포함된다. NTU-RGB+D120은 현 시점에서 가장 큰 3D 골격 데이터 세트로서, 상기 NTU-RGB+D 데이터 세 트에 60개의 동작 클래스와 57,600개의 골격 시퀀스가 추가되어, 120개의 동작 클래스와 114,480개의 골격 시퀀 스가 있다. 이 데이터 세트에는 NTU-RGB+D 데이터 세트와 같이 4가지 다른 양식이 있는데, 실험예에서는 RGB 영 상과 3D 골격 데이터만을 사용하였다. 객체 검출 과정에서, 객체 검출부는 객체 정보, 신뢰도 값(CF), 그리고 객체 바운딩 박스의 상단 좌측 꼭 지점(UL)과 하단 우측 꼭지점(LR)의 x-좌표 및 y-좌표 값들을 출력할 수 있다. 라고 할 때, 사람이 존재한다는 가정 하에 도구가 존재할 수 있는 범위는 다 음 수학식 1 내지 3으로 정의될 수 있다. 수학식 1"}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 2"}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 3"}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "C는 바운딩 박스의 중심점 위치를 의미하고, x와 y는 각각 x-좌표 및 y-좌표를 의미한다. HO와 VE는 각각 사람 에 대한 바운딩 박스의 수평 길이와 수직 길이를 나타낸다. 그리고 TRx 및 TRy는 도구의 잠정적인 수평방향 및 수직방향 존재범위를 나타낸다. 따라서 수학식 1 내지 3에 의해 이 범위 내에서만 도구가 검출되도록 설정할 수 있다. 이 경우 가 된다. 연산 중에 생성되는 소수점 이하의 값은 버려질 수 있다. 객체 검출부의 성능이 항상 완벽한 것은 아니므로 보간이 필요할 수 있다. 예를 들어 사람이 컵을 들고 물 을 마시는 영상이 객체 검출부에 입력되면, 컵이 일시적으로 손에 가려지거나 빠르게 움직여서 컵의 형태 를 놓치고 위치를 되돌리지 못할 수 있다. 영상 보간부는 검출된 도구가 일부 프레임에서 일시적으로 검 출되지 않거나 사람 신체 일부에 가려져서 불완전하게 검출되는 경우 도구의 위치 내지 도구 바운딩 박스를 보 간하여 검출 데이터를 보정한다. 일 실시예에 있어서, 영상 보간부는 일정 수의 프레임 범위, 예컨대 연 이어진 다섯 프레임의 범위 내에서, 도구가 검출되었다가 다시 검출되는 경우 해당 도구가 불완전하게 검출된 것으로 보고, 상기 도구가 미검출된 프레임들에서 도구 좌표를 선형 보간을 실시한다. 즉, 직전에 처음 검출된 프레임과 마지막으로 검출된 프레임 사이의 프레임에 대하여 시간 흐름에 따라 각 프레임 내에서 도구 위치 정 보를 삽입할 수 있다. 이와 같은 보간에 의하여 도구 위치가 구간 내의 모든 프레임에 존재하게 되며, 관절들 과 도구가 동기화됨으로써 사람과 도구간의 상호작용 판단과 행동 인식이 원활히 이루어질 수 있게 된다. 위의 예에서와 같이 사람이 컵을 들고 물을 마시는 영상에서 컵이 일시적으로 손에 가려지거나 빠르게 움직여서 컵의 형태가 일부 프레임에서 표시되지 않는 경우, 영상 보간부는 프레임 와 프레임 사이에 존재하 는 컵의 위치를 보간에 의해 생성할 수 있다. 여기서, s는 도구가 처음 검출된 프레임 단계를 의미하고, l은 동일 객체가 검출된 마지막 단계를 의미한다. 그러므로 검출할 수 없는 프레임의 수는 로 정해질 수 있다. 보간은 다음 수학식 4와 5에 따라 이루어질 수 있다. 수학식 4"}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수학식 5"}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "DI는 한 프레임에 대한 차분 값을 의미한다. b는 1에서 B까지의 정수로 구성되며 이다. 도구의 움직임 은 수학식 5를 통해 자연스럽게 반영되며, 이는 후술하는 LSTM 분류기의 학습에 필요하다. 도 3은 선형 보간의 일 예를 보여준다. 도면에서, 첫 번째 프레임과 여섯 번째 프레임에는 도구인 옷이 검출되 었지만, 두 번째 내지 다섯 번째 프레임에서는 사람에 가려서 옷이 검출되지 않거나 불완전하게 검출될 수 있다. 이 경우, 선형 보간에 의해 첫 번째 프레임과 여섯 번째 프레임 사이의 도구 좌표의 차이를 비례부분의 법칙에 의해 각 프레임 구간 사이에 균등하게 배치하여 두 번째 내지 다섯 번째 프레임에서 도구 좌표를 결정하 게 된다. 상호작용 분류부는 객체 검출부에 의해 검출된 도구가 사람과 상호작용 가능한 것인지, 상호작용한다 면 해당 도구가 사람의 행동과 관련있는 도구인지 분류한다. 일 실시예에 따른 상호작용 분류부에 있어서, 분류기는 연속된 프레임들에서의 각 관절의 좌표와 도구 좌표를 토대로 각 관절과 도구의 상호작 용 여부를 판단한다. 일 실시예에 있어서, 분류기는 사전에 지도학습된 Long short-term memory(LSTM) 기반의 딥러닝 모델을 활 용하여 구현될 수 있다. LSTM 분류기는 다량의 학습용 데이터를 토대로, 다수의 도구 사용 동작에 해당하 는 입력과 직접 레이블링된 정답 데이터에 따라 미리 학습이 이루어질 수 있다. 예를 들어, 물을 마시는 장면 이 있는 다수의 입력 동영상을 사용하여 반복적으로 학습을 하여, 손의 움직임과 도구의 움직임에 대한 패턴을 배울 수 있다. 즉, 예컨대 손과 도구가 아래에서 머리 쪽으로 움직이는 공통 동작 패턴을 학습함으로써, 손의 움직임과 도구의 움직임에 대한 패턴을 배울 수 있다. 분류기는 입력 동영상의 각 프레임에 대하여 영상속 도구와 관절들의 상호작용 가능성이 높다고 판단되면, 해당 도구의 좌표를 메모리에 저장하여 보존할 수 있다. 한편, 영상속 도구와 관절들의 상호작용 가능성이 높 지 않다고 판단되면, 상호작용 분류부는 도구의 좌표를 0으로 채움(zero-padding)으로써 실질적으로 무효 화 내지 삭제할 수 있다. 도 4는 본 발명의 일 실시예에 따른 분류기의 블록도이다. 입력 데이터를 정규화하기 위하여 각 LSTM 레 이어 뒤에 배취 정규화(BN: batch normalization) 레이어가 추가되어 있다. 두 개의 LSTM 레이어를 통과하는 출 력은 1D 합성곱 레이어와 소프트맥스 분류기를 거쳐 전체연결(FC: fully-connected) 레이어를 통과하게 된다. 각 관절과 도구 사이에 관련성이 있는 것으로 분류되면 분류기는 1을 출력하고, 관련성이 없는 것으로 분 류되면 0을 출력하며, 이진 교차 엔트로피가 손실함수로 사용된다. 즉, 이고, 이 Y가 동영상 내에 서 행동과 연관된 도구가 있는지 여부를 나타내는 레이블을 나타낸다고 가정한다. 입력 데이터는 연속적인 프레임 시퀀스 데이터이므로, 관절들과 도구 움직임 간의 상관관계를 학습하기 위하여 LSTM-기반 분류를 사용한다. 보간이 적용된 도구 정보( )가 일 때, 정규화 방정식은 다음과 같 다.수학식 6"}
{"patent_id": "10-2021-0182830", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "25개의 관절 중에서, 도구와 관련이 있는 관절은 손이나 발과 관련된 관절일 가능성이 높다. 또한, 오른손잡이 와 왼손잡이는 다른 관절들을 사용한다. 아울러, 양손잡이는 두 손을 모두 사용하고, 두 손으로 공을 받을 때 와 같이 오른손잡이와 왼손잡이를 불문하고 두 손을 모두 사용하는 경우도 많다. 이를 감안하여, 일 실시예에 따르면, 도구 정보를 사용하여 행동을 인식함에 있어서는 25개의 관절 중에서 왼손 끝, 오른손 끝, 왼발, 오른 발의 관절의 좌표를 중심적으로 활용할 수 있다. 이에 따라, 각 프레임에 대하여 LSTM 분류기에 입력되는 데이터는 정규화된 도구 좌표()와, 왼손 끝 좌표( ), 오른손 끝 좌표 ( ), 왼발 좌표( ), 오른발 좌표( )이다. 여기서 z는 깊이를 의미 하며, 도구의 깊이를 검출할 수 없으므로 으로 정해질 수 있다. 다시 도 1을 참조하면, 데이터 조정부는 영상에서 검출된 사람의 관절들의 좌표와 도구 좌표를 관리하면서 분류기가 각 관절과 도구간의 상호작용 판단시 참고할 수 잇게 해준다. 즉, 객체 검출부에 의해 사 람의 위치를 기준으로 유효한 탐지 범위 밖에 있어서 관련성이 낮을 수 있는 도구가 일차적으로 배제된 상태에 서, 데이터 조정부는 도구와 관련성이 높을 수 있는 낮은 관절 정보와 도구 정보를 분류기에 제공하 여 상호작용 여부를 분류하도록 할 수 있다. 또한, 데이터 조정부는 분류기에 의해 분류된 각 관절 과 도구간의 상호작용 판단 결과를 토대로 인접 행렬을 구성하고 갱신할 수 있다. 특히, 데이터 조정부는 사람과 관련성이 높은 도구를 기존 골격 데이터의 마지막 관절로써 추가하여, 해당 도구의 정보를 인접 행렬에 포함시켜 관리할 수 있다. 즉, 일 실시예에 따르면, 행동과 관련있다고 판단된 도구 정보 를 기존의 골격 데이터와 병합하여 새로운 관절로 정의할 수도 있다. 결과적으로 관절 수(n)는 기존 관절 수보다 +1 증가한 값 을 가지게 되고, 행동 인식부의 입력 특징에 반영된다. 데이터 조정부가 수정하고 유지하는 인접 행렬은 관절과 관절이 서로 이어졌는지 끊어져 있는지 정의해주 는 행렬로서, 개념적으로 골격 그래프에 대응하여 행과 열이 정의된다고 할 수 있다. 도 5는 종래의 골격 그래 프와, 본 발명의 일 실시예에 따른 골격 그래프와, 각 관절과 도구간의 상호작용 판단에 따라 연결 강도를 반영 하여 수정한 골격 그래프를 대비하여 보여준다. 도 5(a)는 종래의 골격 그래프이다. 골격 그래프는 와 같이 V와 E를 포함하는데, 여기서 V=n개의 관절의 집합이고 E는 m개의 뼈의 집합이다. 구체적 으로, 골격 관절은 노드로, 뼈는 엣지로 표현되어, 뼈로 연결된 관절들이 엣지로 연결된 노드들로 묘사된다. 도 5(b)는 본 발명의 일 실시예에 따른 골격 그래프로서, 종래의 골격 그래프에 도구가 골격계의 관절 중 하나 인 것처럼 즉, 골격의 마지막 관절로써 추가되어 있다. 도 5(c)는 각 관절과 도구간의 상호작용 판단 후에 수 정된 골격 그래프로서, 분류기에 의해 분류된 상호작용 여부에 대한 정보를 반영하여 엣지 중 일부가 점선 으로 표시되어, 낮아진 연결 강도를 반영하고 있다. 점선으로 표시된 엣지는 삭제될 수도 있다. 특히, 도 5(b)와 (c)에는 도 2에 표시된 25개의 관절이 모두 포함되어 있지만, 왼손 끝, 오른손 끝, 왼발, 오른발과 같이 도구 사용과 직접 관련된 관절만이 포함될 수도 있다. 도 6은 인접 행렬의 예를 보여준다. 인접 행렬은 관절들이 서로 연결되어 있는지 여부를 나타내는 대각 행렬이 며, 와 같이 정의될 수 있다. 따라서 연결은 0 또는 1로 정의되며, i-번째 관절과 j-번째 관절에 따라서 는 1 또는 0의 값을 가질 수 있다. 이에 따라 인접 행렬 A는 골격의 전체적인 형태에 대한 정보를 담게 된다. 도 6에는 왼손 끝, 오른손 끝, 왼발, 오른발과 같이 도구 사용과 직접 관련된 관절들에 대응하는 행과 열이 마련되어 있지만, 다른 관절에 대한 행과 열이 추가될 수도 있다. 특히, 본 발명에 따르면, 골격 정보 이외에 도구 정보가 인접 행렬에 포함된다. 각 행렬 요소의 값은 행이 나 타내는 관절 또는 도구와 열이 나타내는 관절 또는 도구의 관련성을 나타낸다. 요소의 값이 0이면 관련된 관절 또는 도구가 관련이 전혀 없음을 의미하고, 요소가 1이면 관련된 관절 또는 도구가 관련이 높은 것을 의미하며, 0과 1 사이의 값을 가지면 그에 상응한 관련도가 있음을 의미한다. 도 6(a)는 도 5(b)의 골격 그래프에 대응하 는 인접 행렬로서, 초기에는 도구가 손발에 연결되어 있다고 가정하고 도구에 관련된 행과 열에 있는 요소 값들 이 모두 1로 초기화되어 있다. 하지만, LSTM 분류기의 동작에 의해 각 관절과 도구 간의 관련도가 변경됨 에 따라 구에 관련된 행과 열에 있는 요소 값들이 달라질 수 있다. 도 6(b)는 도 5(c)의 골격 그래프에 대응하 는 인접 행렬이다. 인접 행렬의 각 요소 값은 직전의 값에 가중치를 곱해서 산출될 수 있다. LSTM 분류기의 학습 과정은 이 가중치를 학습하는 과정이라고 할 수 있다. 움직임이 큰 관절과 도구가 등장하면, 즉, 관절 좌표와 도구 좌표 에서 가장 큰 움직임이 동시에 등장할 때 해당 관절이 도구에 관련된 관절이라고 결정할 수 있다. 이러한 동작 이 진행함에 따라, 도구에 관련된 관절과 도구의 인접 관계를 나타내는 요소 값은 1로 수렴하고, 나머지 요소 값은 0으로 수렴할 수 있다. 상호작용 분류부의 출력인 인접 행렬은 실제 인간 행동과 관련된 도구의 존재 여부를 표시할 수 있다. 예 를 들어, 컵을 들고 물을 마시는 행동에서 컵은 행동과 관련이 있으므로, 도구가 존재하고 인간의 행동과 관련 이 있다는 판단이 이루어질 수 있다. 판단 이유는 컵의 위치 변화와 손에 상응한 관절의 변화가 유사한 형태를 보이기 때문이다. 다른 예로서, 박수치는 행동 시퀀스를 취하고 있는 사람이 착용한 모자는 행동에 영향을 미 치지 않는다. 행동과 관련된 도구가 없다고 판단되면, 는 0으로 대체된다. 아울러, 데이터 조정부는 도구를 관절의 일부로서 포함시킨 골격 그래프의 정보를 출력함으로써, 행동 인식부의 딥러닝 모델이 골격 그래프 정보를 토대로 행동을 인식할 수 있게 해준다. 위에서 언급한 바와 같이, 도구가 특정 관절과 관련이 없는 경우, 골격 그래프에서 해당 관절 노드와 도구 노드를 연결하는 엣지는 삭제될 수 있다. 도구가 어느 관 절과도 관련이 없는 경우에는, 도구 노드 자체와 도구 노드에 연결되는 엣지가 모두 삭제될 수 있다. 이에 따 라, 행동 인식부에서의 인식 속도와 정확성이 높아질 수 있다. 이와 같이, 영상속 도구와 어느 한 관절이 상호작용한다고 분류기에 의해 판단되는 경우 새로운 관절 그래 프가 재구성되고 인접 행렬이 갱신된다. 갱신된 관절 그래프와 인접 행렬은 행동 인식부에 제공된다. 한 편, 영상속 도구가 사람의 행동과 관련 없는 경우, 기존의 관절 그래프와 인접 행렬이 유지되고, 도구 정보가 없는 상태로 행동 인식부에 제공에 제공될 수 있다. 행동 인식부는 관절들의 좌표와 도구의 좌표를 토대로 영상 속에 있는 사람의 행동을 분류할 수 있다. 즉, 행동 인식부는 복수의 프레임에 걸친 미세한 관절 움직임과 도구의 움직임을 토대로, 사람이 어떤 행 동을 하는지 분류할 수 있다. 여기서, 관절들의 움직임과 관련이 있는 도구가 존재하는 경우에는, 행동 인식부 가 사전에 학습된 GCN 모델을 기반으로 하여 해당 도구에 대한 좌표 정보를 고려하여 행동을 분류한다. 그렇지만, 관절들의 움직임과 관련이 있는 도구가 존재하지 않는 경우에는, 행동 인식부가 도구에 대한 정 보가 없이 관절들의 좌표만을 기반으로 하여 GCN 모델을 사용하여 행동을 분류할 수 있다. 기존 GCN 모델에 대 한 구체적인 내용은 당업자가 용이하게 구현할 수 있으므로 구체적인 설명은 생략하기로 한다. 따라서 도구 정 보 사용시에, 도구 정보가 없음으로 인한 성능 저하는 없게 된다. 도 1에 도시된 행동 인식 장치는 프로세서와 메모리를 구비하는 범용 데이터 처리 장치에 의해 구현될 수 있다. 도 7은 본 발명의 일 실시예에 따른 행동 인식 장치의 물리적 구성 예를 보여준다. 행동 인식 장치는 적어도 하나의 프로세서, 메모리, 및 저장 장치를 구비할 수 있다. 객체 인식 장치의 구성요소들 은 버스(bus)에 의해 연결되어 데이터를 교환할 수 있다. 프로세서는 메모리 및/또는 저장 장치에 저장된 프로그램 명령들을 실행할 수 있다. 프로세서 는 적어도 하나의 중앙 처리 장치(central processing unit, CPU), 그래픽 처리 장치(graphics processing unit, GPU), 또는 본 발명에 따른 방법을 수행할 수 있는 여타의 프로세서를 포함할 수 있다. 메모 리는 예컨대 RAM(Random Access Memory)와 같은 휘발성 메모리와, ROM(Read Only Memory)과 같은 비휘발 성 메모리를 포함할 수 있다. 메모리는 저장 장치에 저장된 프로그램 명령들을 로드하여, 프로세서 에 제공함으로써 프로세서가 이를 실행할 수 있도록 할 수 있다. 저장 장치는 프로그램 명령들 과 데이터를 저장하기에 적합한 기록매체로서, 예컨대 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매 체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기-광 매체(Magneto-Optical Media), 플래시메모리나 EPROM(Erasable Programmable ROM) 또는 이들을 기반으로 제작되는 SSD와 같은 반도체 메모리를 포함 할 수 있다. 상기 프로그램 명령들은 프로세서에 의해 실행될 때 프로세서로 하여금 후술하는 객체 인식 방법을 구현하는데 필요한 동작을 수행하게 할 수 있다. 예컨대, 상기 프로그램 명령들은 프로세서에 의해 실행 될 때 프로세서로 하여금: 입력 동영상을 받아들이고 상기 입력 동영상에 등장하는 사람과 도구를 검출하 는 동작; 상기 사람의 하나 이상의 관절의 영상 내 좌표와 상기 도구의 영상 내 좌표를 사용하여 상기 사람과 상기 도구가 상호작용하는 것인지 여부를 결정하는 동작; 및 상기 사람과 상기 도구의 상호작용하는 경우, 상기 도구의 존재 사실을 참고하여 상기 입력 동영상 내에서 상기 사람의 상기 행동을 결정하는 동작을 수행하게 할 수 있다. 도 8은 본 발명의 일 실시예에 따른 행동 인식 과정을 보여주는 흐름도이다. 먼저, 객체 검출부는 입력 동영상에 등장하는 사람의 관절들과 도구의 위치 좌표를 검출한다(제400단계). 그리고, 영상 보간부는 도구가 검출된 프레임들 사이에 있는 하나 이상의 프레임에서 도구가 미검출된 프 레임이 존재하는지 판단한다(제402단계). 만약 제402단계에서 일부 프레임에서 미검출된 도구 좌표가 있다고 판단되면, 영상 보간부는 미검출된 프레임들에서 선형 보간에 의해 도구 좌표를 결정하여 삽입할 수 있다 (제404단계). 이후, 상호작용 분류부의 분류기는, 사람의 각 관절에 대하여, 검출된 도구가 해당 관절과 상호작용 하는 것인지 여부를 분류한다(제406단계). 각 관절에 대한 분류 결과에 따라서, 데이터 조정부는 관절 그 래프 데이터와 인접 행렬을 재구성할 수 있다(제408단계). 행동 인식부는 관절들의 좌표와 상호작용가능한 도구의 좌표를 토대로 영상 속 사람의 행동을 분류한다(제 410단계). 즉, 행동 인식부는 복수의 프레임에 걸친 미세한 관절 움직임과 도구의 움직임을 토대로, 사람 이 어떤 행동을 하는지 분류할 수 있다. 이때, 관절들의 움직임과 관련이 있는 도구가 존재하는 경우에는, 행 동 인식부가 해당 도구에 대한 좌표 정보를 고려하여 행동을 분류하여 인식한다. 그렇지만, 관절들의 움 직임과 관련이 있는 도구가 존재하지 않는 경우에는, 행동 인식부가 도구에 대한 정보가 없이 관절들의 좌 표만을 기반으로 하여 기존 GCN 모델을 사용하여 행동을 인식할 수 있다. 도 9는 도구 정보를 적용하지 않은 Baseline 모델과, 도구 정보를 적용한 본 발명의 실시예에 따른 모델의 인식 성능을 대비시켜 보여준다. 도구 정보를 적용하지 않은 Baseline 모델의 경우, 사람의 골격 내에 있는 관절들 의 정보만을 토대로 가장 확률이 높은 행동을 선택하기 때문에 잘못 인식하게 될 가능성이 높다. 도시된 예의 경우, 사람이 컵에 담긴 물을 마시고 있음에도 불구하고, \"서있기\"로 검출하게 될 가능성이 가장 높다. 이에 반하여, 본 발명의 실시예에 따른 모델의 경우, 도구 정보를 활용하고 그 도구로써 가능한 행동들을 선별하리기 때문에 높은 정확성을 가진다. 이 방법에 따르면, 먼저 인체와의 상호작용 가능성을 먼저 판정한 후, \"도구가 머리로 올라와서 입 근처에서 손목 및 머리와 함께 움직인다\"는 도구 움직임 정보를 토대로 \"물 마시기(Drink water)\" 행동으로 판정하게 될 가능성이 높다. 도 10a 내지 도 10c는 본 발명의 일 실시예에 따른 행동 인식 장치에서의 동작 검출 예들을 보여준다. 도시된 예들은 제한된 도구 정보를 토대로 사람의 행동을 정확히 예측하기 위해 수행되는 구체적인 과정을 보여준다. 도 10a의 예에서는, 컵을 잡고 물을 마시는 행동이 영상에 담겨 있다. '컵'이라는 객체가 올바르게 검출되었지 만 일부 프레임에서 검출되지 않았기 때문에, 컵이 검출되지 않는 프레임에서 보간에 의해 컵의 위치 정보가 추 가된다. 컵이 사람과 상호작용 가능한 도구로 인식된 뒤, 행동 인식부는 사람이 \"물 마시기(Drink water)\" 행동을 하고 있다는 것을 인식한다. 특히, 학습이 진행됨에 따라, 모델에서 행동 인식에 영향을 미치 는 관절들이 강조되는 것을 볼 수 있다. 도 10b의 예에서는, 도 10a와 같이 컵을 잡고 물을 마시는 행동이 영상에 담겨 있지만, 카메라 촬영 각도로 말 미암아 컵이 손으로 가려져서 모든 프레임에서 객체 검출부가 '컵'이라는 객체의 검출에 실패한다. 이에 따라 도구 정보가 없게 되지만, 행동 인식부는 기존 GCN 모델에 의해 골격 시퀀스로만으로 행동 인식에 성 공할 수 있다. 도 10c의 예는 입력 동영상에서 상호작용 가능한 도구는 검출되었지만 해당 도구가 어떤 도구인지 정확하게 특 정할 수 없는 경우를 보여준다. 즉, 객체 검출부가 검출한 도구가 어떠한 종류의 도구인지 알 수 없지만, 손과 같은 관절과 상호작용 가능한 도구인 것으로 인식된 후, 행동 인식부는 사람이 \"음식 먹기(EatMeal)\"라는 행동을 하고 있다는 것을 인식할 수 있다. 만약 구체적인 도구 클래스 정보를 사용해야 한다면 이 러한 경우에 대처하기 어렵겠지만, 본 발명의 일 실시예에 따르면 도구의 유무와 움직임만을 골격 정보에 결합 하여 판단하기 때문에 이러한 경우 올바르게 행동 인식을 할 수 있게 된다. 위에서 언급한 바와 같이 본 발명의 실시예에 따른 장치와 방법은 컴퓨터로 읽을 수 있는 기록매체에 컴퓨터가 읽을 수 있는 프로그램 또는 코드로서 구현하는 것이 가능하다. 컴퓨터가 읽을 수 있는 기록매체는 컴퓨터 시스 템에 의해 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 또한 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어 분산 방식으로 컴퓨터로 읽을 수 있는 프로그램 또는 코드가 저장되고 실행될 수 있다. 상기 컴퓨터가 읽을 수 있는 기록매체는 롬(ROM), 램(RAM), 플래시 메모리(Flash memory) 등과 같이 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치를 포함할 수 있다. 프로그램 명령은 컴파일러 (compiler)에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터(interpreter) 등을 사용해서 컴퓨 터에 의해 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 본 발명의 일부 측면들은 장치의 문맥에서 설명되었으나, 그것은 상응하는 방법에 따른 설명 또한 나타낼 수 있 고, 여기서 블록 또는 장치는 방법 단계 또는 방법 단계의 특징에 상응한다. 유사하게, 방법의 문맥에서 설명된 측면들은 또한 상응하는 블록 또는 아이템 또는 상응하는 장치의 특징으로 나타낼 수 있다. 방법 단계들의 몇몇 또는 전부는 예를 들어, 마이크로프로세서, 프로그램 가능한 컴퓨터 또는 전자 회로와 같은 하드웨어 장치에 의 해(또는 이용하여) 수행될 수 있다. 몇몇의 실시예에서, 가장 중요한 방법 단계들의 하나 이상은 이와 같은 장 치에 의해 수행될 수 있다. 실시예들에서, 프로그램 가능한 로직 장치(예를 들어, 필드 프로그래머블 게이트 어레이)가 여기서 설명된 방법 들의 기능의 일부 또는 전부를 수행하기 위해 사용될 수 있다. 실시예들에서, 필드 프로그래머블 게이트 어레이 는 여기서 설명된 방법들 중 하나를 수행하기 위한 마이크로프로세서와 함께 작동할 수 있다. 일반적으로, 방법 들은 어떤 하드웨어 장치에 의해 수행되는 것이 바람직하다. 위에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10a 도면10b 도면10c"}
{"patent_id": "10-2021-0182830", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 행동 인식 장치의 기능적 블록도이다. 도 2는 객체 검출부가 위치 정보를 추출하는 관절의 위치를 예시적으로 보여주는 도면이다. 도 3은 선형 보간의 일 예를 보여주는 도면이다. 도 4는 본 발명의 일 실시예에 따른 분류기의 블록도이다. 도 5는 종래의 골격 그래프와, 본 발명의 일 실시예에 따른 골격 그래프와, 각 관절과 도구간의 상호작용 판단 에 따라 연결 강도를 반영하여 수정한 골격 그래프를 대비하여 보여준다. 도 6은 인접 행렬의 예를 보여주는 도면이다. 도 7은 본 발명의 일 실시예에 따른 행동 인식 장치의 물리적 블록도이다. 도 8은 본 발명의 일 실시예에 따른 행동 인식 과정을 보여주는 흐름도이다. 도 9는 도구 정보를 적용하지 않은 베이스라인 모델과, 도구 정보를 적용한 본 발명의 실시예에 따른 모델의 인 식 성능을 대비시켜 보여주는 도면이다. 도 10a 내지 도 10c는 본 발명의 일 실시예에 따른 행동 인식 장치에서의 동작 검출 예들을 보여주는 도면이다."}
