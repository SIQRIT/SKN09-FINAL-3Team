{"patent_id": "10-2025-7006373", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0048719", "출원번호": "10-2025-7006373", "발명의 명칭": "비디오 스트림의 생성을 위한 시스템 및 방법", "출원인": "라이브아레나 테크놀로지스 에이비", "발명자": "비에르크만, 안드레아스"}}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "출력 디지털 비디오 스트림(230)을 제공하는 방법에 있어서, 상기 방법은:실시간 제1 기본 디지털 비디오 스트림(210, 301)을 지속적으로 수집하는 단계;상기 제1 기본 디지털 비디오 스트림(210, 301)의 제1 디지털 이미지 분석을 수행하여 상기 제1 기본 디지털 비디오 스트림(210, 301)에서 적어도 하나의 제1 이벤트(211) 또는 패턴(212)을 식별하는 단계 - 상기 제1 디지털이미지 분석에 의하면 상기 제1 이벤트(211) 또는 패턴(212)의 상기 감지에 기초하여 제1 생성 제어 매개변수가설정되고, 상기 제1 디지털 이미지 분석은 수행하는 데 특정 시간이 걸려 상기 제1 생성 제어 매개변수가 상기제1 기본 디지털 비디오 스트림(210, 301)의 상기 제1 이벤트(211) 또는 패턴(212)의 발생 시간과 관련하여 제1시간 지연 후에 설정되게 함 - ;상기 제1 생성 제어 매개변수를 상기 실시간 제1 기본 디지털 비디오 스트림(210, 301)에 적용하는 단계 - 상기제1 생성 제어 매개변수의 상기 적용에 의하면 상기 제1 기본 디지털 비디오 스트림(210, 301)이 상기 제1 시간지연에 의해 지연되지 않고 상기 제1 생성 제어 매개변수에 기초하여 수정되어 제1 생성된 디지털 비디오 스트림을 생성함 - 및상기 출력 디지털 비디오 스트림(230)을 적어도 하나의 참가자 클라이언트(121)에 지속적으로 제공하는 단계 -상기 출력 디지털 비디오 스트림(230)은 상기 제1 생성된 디지털 비디오 스트림의 형태로 또는 이에 기초하여제공됨 -를 포함하는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 제1 기본 디지털 비디오 스트림(210, 301)과 상기 제1 생성된 디지털 비디오 스트림에 기반하여 상기 출력 디지털 비디오 스트림(230)을 생성하는 단계를 더욱 포함하는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 수집하는 단계는 제1 디지털 오디오 스트림을 지속적으로 캡처하는 단계를 더욱포함하고, 상기 제1 디지털 오디오 스트림은 상기 제1 기본 디지털 비디오 스트림(210, 301)과 연관되고, 상기방법은:상기 제1 디지털 오디오 스트림을 상기 제1 생성된 비디오 스트림과 시간 동기화하는 단계; 및상기 시간 동기화된 제1 디지털 오디오 스트림을 상기 적어도 하나의 참가자 클라이언트(121)에 상기 출력 디지털 비디오 스트림(230)과 함께 또는 그 일부로 제공하는 단계를 더욱 포함하는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "선행 항들 중 어느 한 항에 있어서, 상기 제1 기본 디지털 비디오 스트림(210, 301)은 상기 참가자 클라이언트(121)와 관련하여 로컬 배치된 카메라에 의해 지속적으로 캡처되는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 제1 기본 디지털 비디오 스트림(210, 301)은 카메라에 의해 지속적으로 캡처되어 상기 제1 기본 디지털 비디오 스트림(210, 301)의 상기 참가자 클라이언트(121)의 참가 사용자를 보여주는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2025-0048719-3-제4항 또는 제5항에 있어서, 상기 제1 기본 디지털 비디오 스트림(210, 301)은 상기 제1 생성 제어 매개변수의상기 적용을 수행하는 컴퓨터 장치와 관련하여 로컬 배치된 카메라에 의해 지속적으로 캡처되는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "선행 항들 중 어느 한 항에 있어서, 상기 제1 생성 제어 매개변수는:a) 상기 제1 기본 디지털 비디오 스트림(210, 301)의 고정 또는 이동하는 객체 또는 사람의 위치 또는 추적 정보 - 상기 위치 또는 추적 정보는 디지털 이미지 처리를 사용하여 자동으로 감지됨 - ;b) 미리 결정된 이벤트(211) 또는 패턴(212)의 상기 감지를 기반으로 자동으로 생성되고 및/또는 미리 결정된또는 가변적인 생성 일정을 기반으로 자동으로 생성된 개별 생성 명령;c) 가상 패닝 및/또는 줌잉 명령; 및d) 카메라 움직임 감지를 기반으로 자동으로 생성된 카메라 안정화 정보중 하나 또는 여러 개를 포함하는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "선행 항들 중 어느 한 항에 있어서, 상기 제1 디지털 이미지 분석은 상기 적어도 하나의 참가자 클라이언트에게상기 출력 디지털 비디오 스트림(230)을 제공하도록 구성된 컴퓨터 장치에 의해 수행되는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 제1 디지털 이미지 분석 및 상기 출력 디지털 비디오 스트림(230)의 상기 제공은 별도의프로세스 또는 스레드에서 수행되는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항 또는 제9항에 있어서, 상기 출력 디지털 비디오 스트림(230)의 상기 제공이 상기 제1 디지털 이미지 분석보다 프로세서 우선순위를 갖도록 상기 제1 디지털 이미지 분석을 수행하는 상기 컴퓨터 장치의 현재 프로세서 부하의 기능으로 상기 제1 디지털 이미지 분석을 프로세서 조절하는 단계를 더욱 포함하는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 제1 디지털 이미지 분석의 상기 프로세서 조절은 상기 제1 디지털 이미지 분석을 상기 제1 기본 디지털 비디오 스트림(210, 301)의 모든 비디오 프레임의 하위 부분에만 제한하여 수행되는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "선행 항들 중 어느 한 항에 있어서, 상기 제1 기본 디지털 비디오 스트림(210, 301) 및/또는 상기 디지털 오디오 스트림의 적어도 하나의 제2 이벤트(211) 또는 패턴(212)을 식별하기 위해서, 상기 제1 기본 디지털 비디오 스트림(210, 301)의 제2 디지털 이미지 분석, 및/또는 상기 제1 기본 디지털 비디오 스트림(210, 301)과 지속적으로 캡처되고 연관된 디지털 오디오스트림의 제2 디지털 오디오 분석을 수행하는 단계 - 상기 제2 디지털 이미지 또는 오디오 분석은 수행하는 데특정 시간이 걸려 상기 제1 기본 디지털 비디오 스트림(210, 301)의 상기 제2 이벤트(211) 또는 패턴(212)의 발생 시간과 관련하여 제2 시간 지연 후에 제2 생성 제어 매개변수가 설정되고, 상기 제2 시간 지연은 제1 시간지연보다 김 - ; 상기 실시간 제1 기본 디지털 비디오 스트림(210, 301)에 상기 제2 생성 제어 매개변수를 적용하는 단계 - 상기제2 생성 제어 매개변수의 상기 적용에 의하면 상기 제1 기본 디지털 비디오 스트림(210, 301)이 상기 제2 시간지연 만큼 지연되지 않고 상기 제2 생성 제어 매개변수에 기초하여 수정되어, 상기 제1 생성된 디지털 비디오스트림을 생성함 - 를 더욱 포함하는, 방법.공개특허 10-2025-0048719-4-청구항 13 제12항에 있어서, 상기 제2 디지털 이미지 분석은 상기 제2 생성 제어 매개변수의 상기 적용을 수행하는 컴퓨터장치와 관련하여 원격에 위치하는 컴퓨터 장치에 의해 수행되는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항 또는 제13항에 있어서, 상기 제2 생성 제어 매개변수는 상기 제1 디지털 이미지 분석에 대한 입력을 구성하는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12항 내지 제14항 중 어느 한 항에 있어서, 상기 제2 생성 제어 매개변수는: a) 제2 기본 비디오 스트림(210, 301); 및b) 상기 제1 생성된 디지털 비디오 스트림에서 특정 참가 사용자를 표시할지 여부에 대한 명령중 하나 또는 여러 개를 포함하고, 상기 참가 사용자는 디지털 이미지 처리에 기반하여 자동으로 식별되는, 방법."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "컴퓨터에 의해 프로그램이 실행될 때 출력 디지털 비디오 스트림(230)을 제공하기 위해, 상기 컴퓨터로 하여금선행하는 청구항들 중 어느 한 항에 따른 상기 방법을 수행하도록 하는 명령어를 포함하는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "출력 디지털 비디오 스트림(230)을 제공하기 위한 시스템(100)에 있어서, 상기 시스템(100)은:실시간 제1 기본 디지털 비디오 스트림(210, 301)을 지속적으로 수집하도록 배열되는 수집 기능(131, 131',131\", 131\"'); 상기 제1 기본 디지털 비디오 스트림(210, 301)의 제1 디지털 이미지 분석을 수행하여 상기 제1 기본 디지털 비디오 스트림(210, 301)의 적어도 하나의 제1 이벤트(211) 또는 패턴(212)을 식별하도록 구성된 생성 기능(135,135', 135\", 135\"') - 상기 제1 디지털 이미지 분석에 의하면 상기 제1 이벤트(211) 또는 패턴(212)의 상기 감지에 기초하여 제1 생성 제어 매개변수가 설정되고, 상기 제1 디지털 이미지 분석은 수행하는 데 특정 시간이걸리므로 상기 제1 생성 제어 매개변수는 상기 제1 기본 디지털 비디오 스트림(210, 301)의 상기 제1 이벤트(211) 또는 패턴(212)의 발생 시간과 관련하여 제1 시간 지연으로 설정되고, 상기 생성 기능(135, 135', 135\",135\"')은 상기 제1 생성 제어 매개변수를 상기 실시간 제1 디지털 비디오 스트림(210, 301)에 적용하도록 구성되고, 상기 제1 생성 제어 매개변수의 상기 적용에 의하면 상기 제1 생성된 디지털 비디오 스트림을 생성하기위해서 상기 제1 기본 디지털 비디오 스트림(210, 301)이 상기 제1 시간 지연 만큼 지연되지 않고 상기 제1 생성 제어 매개변수에 기초하여 수정됨 - ; 및상기 출력 디지털 비디오 스트림(230)을 적어도 하나의 참가자 클라이언트(121)에 지속적으로 제공하도록 배열되는 게시 기능(136, 136', 136\", 136\"') - 상기 출력 디지털 비디오 스트림(230)은 상기 제1 생성된 디지털 비디오 스트림의 형태로 또는 이에 기반하여 제공됨 -을 포함하는, 시스템."}
{"patent_id": "10-2025-7006373", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 시스템(100)은 여러 카메라를 더욱 포함하며, 각 카메라는 각각의 지연되지 않은 기본 디지털 비디오 스트림(210, 301)을 캡처하도록 배열되고; 상기 생성 기능(135, 135', 135\", 135\"')은 상기 캡처된 기본 디지털 비디오 스트림(210, 301) 각각에 기반하여지연되지 않은 제1 생성 디지털 비디오 스트림을 생성하도록 배열되는, 시스템.공개특허 10-2025-0048719-5-"}
{"patent_id": "10-2025-7006373", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "출력 디지털 비디오 스트림을 제공하는 방법을 개시한다. 상기 방법은 실시간 제1 기본 디지털 비디오 스트 림(210, 301)을 지속적으로 수집하는 단계; 제1 기본 디지털 비디오 스트림의 제1 디지털 이미지 분석을 수행하 여 적어도 하나의 제1 이벤트 또는 패턴을 식별하여 제1 기본 디지털 비디오 스트림의 제1 이벤트 또 는 패턴의 발생 시간과 관련하여 제1 시간 지연에서 제1 생성 제어 매개변수가 설정되게 하는 단계; 제1 생성 제 어 매개변수를 제1 기본 디지털 비디오 스트림에 적용하여 제1 기본 디지털 비디오 스트림이 제1 시간 지연으로 인해 지연되지 않고 수정되어 제1 생성된 디지털 비디오 스트림을 생성하는 단계; 및 제1 생성된 디지털 비디오 스트림을 기반으로 출력 디지털 비디오 스트림을 지속적으로 제공하는 단계를 포함한다. 본 발명은 또한 시스템 및 컴퓨터 프로그램 제품에 관한 것이다."}
{"patent_id": "10-2025-7006373", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 디지털 비디오 스트림을 생성하기 위한 시스템, 컴퓨터 소프트웨어 제품 및 방법에 관한 것으로, 특 히 서로 다른 디지털 입력 비디오 스트림을 기반으로 디지털 비디오 스트림을 생성하기 위한 방법에 관한 것이다. 바람직한 실시 예에서, 디지털 비디오 스트림은 특히 복수의 서로 다른 동시 사용자를 포함하는 디지털 비 디오 회의 또는 디지털 비디오 회의 또는 회의 시스템의 컨텍스트에서 생성된다. 생성된 디지털 비디오 스트림 은 외부로 공개되거나 디지털 비디오 회의 또는 디지털 비디오 회의 시스템 내에서 공개될 수 있다."}
{"patent_id": "10-2025-7006373", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "다른 실시 예에서, 본 발명은 디지털 화상 회의는 아니지만, 여러 디지털 비디오 입력 스트림이 동시에 처리되 어 생성된 디지털 비디오 스트림에 결합되는 컨텍스트에서 적용된다. 예를 들어, 이러한 컨텍스트는 교육적이거 나 지시적일 수 있다. 두 명 이상의 참가자가 현장에서 녹화된 디지털 비디오 및 오디오를 사용하여 가상으로 회의하고 모든 참가자에 게 방송하여 실제 회의와 같은 느낌을 받을 수 있도록 하는, Microsoft® Teams®, Zoom® 및 Google® Meet®와 같이 많은 알려진 디지털 화상 회의 시스템이 있다. 특히, 누구에게 언제 어떤 배포 채널을 통해 무엇을 보여줄지와 같이, 시청되는 콘텐츠의 생성과 관련하여 이러 한 디지털 화상 회의 솔루션을 개선해야 할 필요성이 대두되고 있다. 예를 들어, 일부 시스템은 현재 대화 중인 참가자를 자동으로 감지하고, 대화 중인 참가자의 해당 비디오 피드 를 다른 참가자에게 보여준다. 많은 시스템에서 현재 표시되는 화면, 보기 창 또는 디지털 프레젠테이션과 같은 그래픽을 공유할 가능성이 있다. 그러나 가상 회의가 더 복잡해짐에 따라 서비스에서 각 시점에서 각 참가자에 게 현재 사용 가능한 모든 정보 중에서 무엇을 보여줄지를 알기가 더 어려워진다. 다른 예에서 발표하는 참가자는 디지털 프레젠테이션에서 슬라이드에 대해 이야기하는 동안 무대를 돌아다닌다. 그런 다음 시스템은 프레젠테이션, 발표자 또는 둘 다 보여줄지 또는 둘 사이를 전환할지를 결정해야 한다. 자동 생성 프로세스를 통해 여러 입력 디지털 비디오 스트림을 기반으로 하나 또는 여러 개의 출력 디지털 비디 오 스트림을 생성하고, 이렇게 생성된 디지털 비디오 스트림을 하나 또는 여러 개의 소비 개체에 제공하는 것이 바람직할 수 있다. 그러나 많은 경우, 이러한 디지털 비디오 컨퍼런스 시스템이 직면한 여러 가지 기술적 어려움으로 인해, 동적 컨퍼런스 화면 레이아웃 관리자 또는 기타 자동화된 생성 기능이 어떤 정보를 표시할지 선택하기가 어렵다. 첫째, 디지털 비디오 회의는 실시간 측면이 있으므로 대기 시간이 낮아야 한다. 이는 서로 다른 하드웨어를 사 용하여 참가하는 서로 다른 참가자와 같이 서로 다른 수신 디지털 비디오 스트림이 서로 다른 대기 시간, 프레 임 속도, 종횡비 또는 해상도와 연관되어 있는 경우 문제가 된다. 이러한 수신 디지털 비디오 스트림은 잘 구성 된 사용자 경험을 위해 처리해야 할 필요가 있다. 둘째, 비디오 이미지 처리, 선택 및 포맷팅 측면에서의 생성에는 참가자 간 실시간 비디오 통신에 바람직하지 않은 지연이 발생할 수 있다. 셋째, 시간 동기화에 문제가 있다. 대기 시간이 너무 긴 경우와 마찬가지로, 동기화되지 않은 디지털 비디오 피 드는 결과적으로 사용자 경험이 좋지 않게 된다. 이러한 문제는 예를 들어, 많은 참가자가 참가하는 경우; 참가자가 서로 다른 하드웨어 및/또는 소프트웨어를 사용하여 연결하는 경우; 외부에서 제공되는 디지털 비디오 스트림; 화면 공유; 또는 여러 호스트와 같은, 더 복잡한 회의 상황에서 증폭된다. 이러한 문제는 특히 많은 참석자가 화상 회의 등에 참가하는 상황, 모든 참석자가 같은 방이나 장소에 로컬로 참석하거나, 일부 참석자가 로컬로 참석하고 일부 참석자는 원격으로 참가하는 상황에서 발생한다. 해당 문제는 예를 들어 교육 및 지시를 위한 디지털 비디오 생성 시스템에서와 같이, 여러 입력 디지털 비디오 스트림을 기반으로 출력 디지털 비디오 스트림을 생성해야 하는 다른 컨텍스트에서 발생한다.본 출원의 발효일에는 공개되지 않은 스웨덴 출원 SE 2151267-8은 상술된 문제에 대한 다양한 솔루션을 공개한 다. 본 출원의 발효일에는 공개되지 않은 스웨덴 출원 2151461-7은 예를 들어, 다른 참가자 그룹이 다른 일반적인 대기 시간과 연관되는 경우와 같이, 다중 참가자 디지털 비디오 환경에서 대기 시간을 처리하는 것과 관련된 다 양한 솔루션을 공개한다. 본 출원의 발효일에는 공개되지 않은 스웨덴 출원 2250113-4는 한 명 또는 여러 명의 사람을 추적하기 위해 하 나 또는 여러 대의 카메라를 사용하는 것과 관련된 다양한 솔루션을 공개한다."}
{"patent_id": "10-2025-7006373", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상술된 문제 중 하나 이상을 해결한다."}
{"patent_id": "10-2025-7006373", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "따라서, 본 발명은 출력 디지털 비디오 스트림을 제공하는 방법에 관ㄹ한 것으로, 상기 방법은 실시간 제1 기본 디지털 비디오 스트림을 지속적으로 수집하는 단계; 상기 제1 기본 디지털 비디오 스트림의 제1 디지털 이미지 분석을 수행하여 상기 제1 기본 디지털 비디오 스트림에서 적어도 하나의 제1 이벤트 또는 패턴을 식별하는 단 계 - 상기 제1 디지털 이미지 분석의 결과로 상기 제1 이벤트 또는 패턴의 상기 감지에 기초하여 제1 생성 제어 매개변수가 설정되고, 상기 제1 디지털 이미지 분석은 수행하는 데 특정 시간이 걸려 상기 제1 생성 제어 매개 변수가 상기 제1 기본 디지털 비디오 스트림에서 상기 제1 이벤트 또는 패턴의 발생 시간과 관련하여 제1 시간 지연 후에 설정되게 함 - ; 상기 제1 생성 제어 매개변수를 상기 실시간 제1 기본 디지털 비디오 스트림에 적용 하는 단계 - 상기 제1 생성 제어 매개변수의 상기 적용은 상기 제1 기본 디지털 비디오 스트림이 상기 제1 시간 지연에 의해 지연되지 않고 상기 제1 생성 제어 매개변수에 기초하여 수정되어 제1 생성된 디지털 비디오 스트 림을 생성함 - ; 및 상기 출력 디지털 비디오 스트림을 적어도 하나의 참가자 클라이언트에 지속적으로 제공하 는 단계 - 상기 출력 디지털 비디오 스트림은 상기 제1 생성된 디지털 비디오 스트림의 형태로 또는 이에 기초 하여 제공됨 - 을 포함한다. 또한, 본 발명은 컴퓨터에 의해 프로그램이 실행될 때 출력 디지털 비디오 스트림을 제공하기 위해, 상기 컴퓨터로 하여금 선행하는 청구항들 중 어느 한 항에 따른 상기 방법을 수행하도록 하는 명령어를 포함하는, 컴 퓨터 프로그램 제품에 관한 것이다. 또한, 본 발명은 출력 디지털 비디오 스트림을 제공하기 위한 시스템에 관한 것으로, 상기 시스템은 실시간 제1 기본 디지털 비디오 스트림을 지속적으로 수집하도록 배열되는 수집 기능; 상기 제1 기본 디지털 비디오 스트림 의 제1 디지털 이미지 분석을 수행하여 상기 제1 기본 디지털 비디오 스트림의 적어도 하나의 제1 이벤트 또는 패턴을 식별하도록 구성된 생성 기능 - 상기 제1 디지털 이미지 분석에 의하면 상기 제1 이벤트 또는 패턴의 상 기 감지에 기초하여 제1 생성 제어 매개변수가 설정되고, 상기 제1 디지털 이미지 분석은 수행하는 데 특정 시 간이 걸리므로 상기 제1 생성 제어 매개변수는 상기 제1 기본 디지털 비디오 스트림의 상기 제1 이벤트 또는 패 턴의 발생 시간과 관련하여 제1 시간 지연되어 설정되고, 상기 생성 기능은 상기 제1 생성 제어 매개변수 를 상기 실시간 제1 디지털 비디오 스트림에 적용하도록 구성되고, 상기 제1 생성 제어 매개변수의 상기 적용에 의하면 상기 제1 생성된 디지털 비디오 스트림을 생성하기 위해서 상기 제1 기본 디지털 비디오 스트림이 상기 제1 시간 지연 만큼 지연되지 않고 상기 제1 생성 제어 매개변수에 기초하여 수정됨 - 및 상기 출력 디지털 비 디오 스트림을 적어도 하나의 참가자 클라이언트에 지속적으로 제공하도록 배열되는 게시 기능 - 상기 출력 디 지털 비디오 스트림은 상기 제1 생성된 디지털 비디오 스트림의 형태로 또는 이에 기반하여 제공됨 - 을 포함한 다."}
{"patent_id": "10-2025-7006373", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "모든 도면은 동일하거나 대응하는 부분에 대한 참조 번호를 공유한다. 도 1은 본 발명에 따른 시스템을 도시하며, 출력 디지털 비디오 스트림, 예를 들어 공유 디지털 비디오 스 트림을 제공하기 위한 발명에 따른 방법을 수행하도록 배열되어 있다. 시스템은 비디오 통신 서비스을 포함할 수 있지만, 비디오 통신 서비스은 일부 실시 예에서 시 스템 외부에 있을 수도 있다. 설명되는 바와 같이, 하나 이상의 비디오 통신 서비스이 있을 수 있다. 시스템은 하나 이상의 참가자 클라이언트을 포함할 수 있지만, 하나, 일부 또는 모든 참가자 클라이 언트는 일부 실시 예에서 시스템 외부에 있을 수도 있다. 시스템은 중앙 서버을 포함할 수 있다. 본 명세서에서 사용되는 바와 같이, \"중앙 서버\"는 예를 들어 잘 정의된 API(애플리케이션 프로그래밍 인터페이 스)를 통해, 논리적으로 중앙화된 방식으로 액세스되도록 구성된 컴퓨터 구현 기능이다. 이러한 중앙 서버의 기 능은 순수하게 컴퓨터 소프트웨어로 구현되거나 소프트웨어와 가상 및/또는 물리적 하드웨어의 조합으로 구현될 수 있다. 이것은 독립형 물리적 또는 가상 서버 컴퓨터에서 구현되거나 여러 개의 상호 연결된 물리적 및/또는 가상 서버 컴퓨터에 분산될 수 있다. 아래에 예시되는 바와 같이, 일부 실시 예에서 중앙 서버는 하나 이상의 참가자 클라이언트와 관련하여 로 컬 배열된 하드웨어를 포함하거나 하드웨어 그 자체이다. 본 명세서에서 사용되는 바와 같이, 두 엔터티가 서로 관련하여 \"로컬하게 배열\"된다는 것은 이들이 동일한 건물, 예를 들어 동일한 방과 같이 동일한 구내에 배열되 고, 바람직하게는 개방형 인터넷을 통한 것과는 반대로, 전용 케이블 또는 로컬 영역 네트워크 연결을 사용하여 로컬 통신을 위해 상호 연결된 것을 의미한다. 중앙 서버가 실행하는, 즉 중앙 서버의 기능을 정의하는 컴퓨터 소프트웨어가 실행되는 물리적 또는 가상 하드웨어는, 그 자체로 기존의 CPU, 기존의 GPU, 기존의 RAM/ROM 메모리, 기존의 컴퓨터 버스, 인터넷 연 결과 같은 기존의 외부 통신 기능을 포함할 수 있다. 각 비디오 통신 서비스는 사용되는 범위 내에서, 상기 의미에서 또한 중앙 서버이며, 이는 중앙 서버(13 0)와 다른 중앙 서버이거나 중앙 서버의 일부일 수 있다. 특히, 각 비디오 통신 서비스는 참가자 클 라이언트 중 하나, 여러 명 또는 전체와 관련하여 로컬 배치될 수 있다. 이에 따라, 각 참가자 클라이언트는 상기 의미에서 해당 해석으로 중앙 서버일 수 있으며, 각 참가자 클라 이언트가 실행하는, 즉 참가자 클라이언트의 기능을 정의하는 컴퓨터 소프트웨어가 실행하는 물리적 또는 가상 하드웨어는, 또한 그 자체로 기존 CPU/GPU, 그 자체로 기존 RAM/ROM 메모리, 그 자체로 기존 컴퓨터 버스 및 인터넷 연결과 같은 그 자체로 기존 외부 통신 기능을 포함할 수 있다. 각 참가자 클라이언트는 또한 일반적으로 진행 중인 비디오 통신의 일부로 참가자 클라이언트에 제공 된 비디오 콘텐츠를 표시하도록 배열된 컴퓨터 화면; 상기 비디오 통신의 일부로 참가자 클라이언트에 제 공된 사운드 콘텐츠를 방출하도록 배열된 라우드스피커; 비디오 카메라; 및 해당 비디오 통신에 인간 참가자 에게 로컬로 소리를 녹음하도록 배열된 마이크로폰을 포함하거나 이와 통신하고, 이 때 참가자는 해 당 참가자 클라이언트을 사용하여 해당 비디오 통신에 참가한다.다시 말해, 각 참가자 클라이언트의 각각의 인간-기계 인터페이스는 각각의 참가자가 비디오 통신에 서, 다른 참가자 및/또는 다양한 소스에서 제공하는 오디오/비디오 스트림으로, 해당 클라이언트과 상호 작용할 수 있도록 한다. 일반적으로, 각 참가자 클라이언트는 해당 비디오 카메라, 마이크, 키보드; 컴퓨터 마우스 또는 트랙패드; 및/또는 디지털 비디오 스트림, 디지털 오디오 스트림 및/또는 기타 디지털 데이터를 수신하기 위한 API를 포함 할 수 있는 해당 입력 수단을 포함한다. 입력 수단은 비디오 통신 서비스 및/또는 중앙 서버 과 같은 중앙 서버로부터 비디오 스트림 및/또는 오디오 스트림을 수신하도록 특별히 배열되어 있으며, 이 러한 비디오 스트림 및/또는 오디오 스트림은 비디오 통신의 일부로 제공되며, 바람직하게는 이러한 디지털 데 이터 입력 스트림의 적어도 두 개의 소스, 예를 들어 참가자 클라이언트 및/또는 외부 소스(아래 참조)로 부터 상기 중앙 서버에 제공된 해당 디지털 데이터 입력 스트림을 기반으로 생성된다. 또한 일반적으로, 각 참가자 클라이언트는 상기 컴퓨터 화면을 포함할 수 있는 각각의 출력 수단; 상 기 라우드스피커; 및 디지털 비디오 및/또는 오디오 스트림을 방출하는 API를 포함하고, 이러한 스트림은 해당 참가자 클라이언트을 사용하여 참가자에게 현장에서 캡처된 비디오 및/또는 오디오를 나타낸다. 실제로, 각 참가자 클라이언트는 화면, 라우드스피커, 마이크 및 인터넷 연결이 배치된 모바일 폰과 같은 모바일 장치일 수 있으며, 모바일 장치는 현장에서 컴퓨터 소프트웨어를 실행하거나 원격으로 실행되는 컴퓨터 소프트웨어에 액세스하여 해당 참가자 클라이언트의 기능을 수행한다. 이에 상응하여, 참가자 클라이언트 는 또한 로컬 설치된 애플리케이션을 실행하고, 웹 브라우저를 통해 원격으로 액세스되는 기능을 사용하는 등 경우에 따라 두껍거나 얇은 노트북 또는 고정 컴퓨터일 수 있다. 현재 유형의 동일한 비디오 통신에 사용되는 참가자 클라이언트는 하나 이상, 적어도 3명 또는 심지어 적 어도 4명일 수 있다. 적어도 두 개의 서로 다른 그룹의 참가자 클라이언트가 있을 수 있다. 각 참가자 클라이언트는 해당 그룹에 할 당될 수 있다. 그룹은 참가자 클라이언트의 다른 역할, 참가자 클라이언트의 다른 가상 또는 물리적 위치 및/또 는 참가자 클라이언트의 다른 상호 작용 권한을 반영할 수 있다. 사용 가능한 이러한 역할은 예를 들어 \"리더\" 또는 \"회의 진행자\", \"발표자\", \"패널 참가자\", \"상호 작용하는 청중\" 또는 \"원격 청취자\"일 수 있다. 이러한 다양한 물리적 위치는 예를 들어 \"물리적으로 실내\", \"원격 청취중\", \"무대 위\", \"패널 내\", \"물리적으 로 현장에 있는 청중\" 또는 \"물리적으로 멀리 떨어진 청중\"일 수 있다. 가상 위치는 물리적 위치의 관점에서 정의될 수 있지만, 상기 물리적 위치와 부분적으로 겹칠 수 있는 가상 그 룹화를 포함할 수도 있다. 예를 들어, 물리적으로 현장에 있는 청중을 제1 가상 그룹과 제2 가상 그룹으로 나눌 수 있으며, 물리적으로 현장에 있는 청중 참가자 중 일부는 물리적으로 멀리 떨어져 있는 청중 참가자와 함께 하나의 동일한 가상 그룹으로 그룹화될 수 있다. 사용 가능한 이러한 다양한 상호작용 권한은 예를 들어 \"전체 상호작용\"(제한 없음), \"말할 수 있지만 마이크를 요청한 후에만 가능\"(예를 들어, 화상 회의 서비스에서 가상적으로 손을 드는 것), \"말할 수 없지만 일반 채팅 에서 쓸 수 있음\" 또는 \"보기/듣기 전용\"일 수 있다. 일부 경우에, 정의된 각 역할 및/또는 물리적/가상 위치가 특정 사전 결정된 상호작용 권한에 따라 정의될 수 있다. 다른 경우에는 동일한 상호작용 권한을 가진 모든 참가자가 그룹을 형성한다. 따라서 정의된 역할, 위치 및/또는 상호작용 권한은 다양한 그룹 할당을 반영할 수 있으며, 경우에 따라 다른 그룹은 분리되거나 겹칠 수 있다. 비디오 통신은 비디오 통신 서비스에 의해 적어도 부분적으로 제공될 수 있고, 중앙 서버에 의해 적 어도 부분적으로 제공될 수 있으며, 이는 본 명세서에서 설명되고 예시될 것이다. 본 명세서에서 사용되는 용어에 따르면, \"비디오 통신\"은 하나 또는 여러 개의 혼합 또는 공동 디지털 비디오/ 오디오 스트림을 생성하는 데 사용되며, 다음에 비디오 및/또는 오디오를 통해 비디오 통신에 기여할 수도 있고 그렇지 않을 수도 있는 하나 또는 여러 소비자(예를 들어, 논의된 유형의 참가자 클라이언트)에 의해 소비되는 적어도 두 개, 바람직하게는 적어도 세 개 또는 심지어 적어도 네 개의 비디오 스트림 및 바람직하게는 또한 일 치하는 오디오 스트림을 포함하는 대화형 디지털 통신 세션이다. 이러한 비디오 통신은 특정 대기 시간 또는 지 연이 있거나 없는 실시간이다. 이러한 비디오 통신에 대한 적어도 한 명, 바람직하게는 적어도 두 명 또는 심지어 적어도 네 명의 참가자가 비디오 통신에 대화형 방식으로 참가하여, 비디오/오디오 정보를 제공하고 소 비한다. 참가자 클라이언트 중 적어도 하나 또는 참가자 클라이언트 모두는 아래에서 더 자세히 설명되는 로 컬 동기화 소프트웨어 기능을 포함할 수 있다. 비디오 통신 서비스은 이하 더 자세히 설명되는 바와 같이, 공통 시간 참조를 포함하거나 이에 액세스할 수 있다. 적어도 하나의 중앙 서버 각각은 해당 중앙 서버 외부의 개체와 디지털로 통신하기 위한 해당 API를 포함할 수 있다. 이러한 통신은 입력과 출력을 모두 포함할 수 있다. 시스템, 예를 들어 상기 중앙 서버는, 또한 예를 들어 외부에서 제공된 비디오 스트림과 같은 외부 정보 소스으로부터 오디오 및/또는 비디오 스트림 데이터와 같은 디지털 정보를 디지털 방식으로 통신하도 록, 특히 수신하도록 구성될 수 있다. 정보 소스가 \"외부\"라는 것은 중앙 서버에서 제공되거나 중앙 서버의 일부로 제공되지 않는다는 것을 의미한다. 바람직하게는, 외부 정보 소스이 제공하는 디지털 데이터는 중앙 서버과 독립적이며, 중앙 서버은 그 정보 내용에 영향을 미칠 수 없다. 예를 들어, 외 부 정보 소스는 예를 들어, 공개 스포츠 이벤트 또는 진행 중인 뉴스 이벤트 또는 보고의 라이브 캡처 비 디오 및/또는 오디오일 수 있다. 외부 정보 소스는 웹 카메라 또는 유사한 장비에 의해 캡처될 수 있지만, 참가자 클라이언트 중 어느 누구에 의해서도 캡처할 수 없다. 따라서 이러한 캡처된 비디오는 참가자 클라 이언트 중 어느 것과 동일한 로컬티를 묘사할 수 있지만, 참가자 클라이언트 자체의 활동의 일부로 캡처될 수는 없다. 외부에서 제공되는 정보 소스과 내부에서 제공되는 정보 소스 간의 한 가지 가능 한 차이점은 내부적으로 제공되는 정보 소스는 위에서 정의된 유형의 비디오 통신에 참가자로서, 그리고 참가자 로서의 역량으로 제공될 수 있는 반면, 외부에서 제공되는 정보 소스는 그렇지 않고, 해당 비디오 컨퍼런 스 외부의 컨텍스트의 일부로 제공된다는 것이다. 다른 실시 예에서, 하나 또는 여러 개의 외부 제공 정보 소스 는 참가자 클라이언트 중 하나 또는 여러 개 및/또는 해당 사용자가 존재하는 동일한 지역에서, 그리고 중앙 서버에 의해 제어되는 방식으로, 해당 디지털 이미지/비디오 및/또는 오디오 스트림을 캡처하 도록 배열되어 있는, 해당 디지털 카메라 또는 마이크의 형태이다. 따라서 중앙 서버는 이러한 디지털 이 미지/비디오/오디오 캡처링 장치의 켜짐/꺼짐 상태 및/또는 현재 적용된 물리적 또는 가상적 패닝 (panning) 또는 줌잉(zooming)과 같은 다른 캡처링 상태를 제어할 수 있다. 또한 오디오 및/또는 비디오 스트림과 같은 해당 유형의 디지털 정보를 중앙 서버에 병렬로 제공하는 여러 외부 정보 소스가 있을 수 있다. 도 1에 도시된 바와 같이, 각 참가자 클라이언트는 설명된 바와 같이 해당 참가자 클라이언트에 의해 비디오 통신 서비스에 제공되는, 각각의정보(비디오 및/또는 오디오) 스트림의 소스를 구성할 수 있 다. 중앙 서버와 같은 시스템은 외부 소비자과 디지털로 통신하고, 특히 외부 소비자에게 디지 털 정보를 방출하도록 추가로 구성될 수 있다. 예를 들어, 중앙 서버에 의해 생성된 디지털 비디오 및/또 는 오디오 스트림은 해당 API를 통해 하나 또는 여러 외부 소비자에게 실시간 또는 거의 실시간으로 지속적으로 제공될 수 있다. 다시 말하지만, 소비자가 \"외부적\"이라는 것은 소비자가 중앙 서버(13 0)의 일부로 제공되지 않고/않거나, 해당 비디오 통신의 당사자가 아니라는 것을 의미한다. 달리 명시되지 않는 한, 본 명세서의 모든 기능과 통신은 디지털 및 전자적으로 제공되며, 적절한 컴퓨터 하드 웨어에 의해 실행되는 컴퓨터 소프트웨어에 의해 수행되고 인터넷과 같은 로컬 또는 글로벌 디지털 통신 네트워 크 또는 채널을 통해 전달된다. 따라서, 도 1에 도시된 시스템 구성에서, 다수의 참가자 클라이언트는 비디오 통신 서비스이 제 공하는 디지털 비디오 통신에 참가한다. 따라서 각 참가자 클라이언트는 비디오 통신 서비스에 대한 지속적인 로그인, 세션 또는 이와 유사한 것을 가질 수 있으며, 비디오 통신 서비스이 제공하는 동일한 하 나의 지속적인 비디오 통신에 참가할 수 있다. 다시 말해, 비디오 통신은 참가자 클라이언트 간에 \"공유\" 되고 따라서 해당 인간 참가자에 의해서도 공유된다. 도 1에서, 중앙 서버는 자동 참가자 클라이언트를 포함하는데, 이는 참가자 클라이언트에 대응 하지만 인간 참가자와 연관되지 않은 자동 클라이언트이다. 대신, 자동 참가자 클라이언트는 참가자 클라이언트와 동일한 공유 비디오 통신에 참가하기 위해 비디오 통신 서비스에 참가자 클라이언트로추가된다. 이러한 참가자 클라이언트로서, 자동 참가자 클라이언트은 비디오 통신 서비스에 의해 진 행 중인 비디오 통신의 일부로 제공되는 지속적으로 생성된 디지털 비디오 및/또는 오디오 스트림에 대한 액세 스가 부여되고, 자동 참가자 클라이언트을 통해 중앙 서버에서 소비될 수 있다. 바람직하게, 자동 참 가자 클라이언트는 비디오 통신 서비스으로부터, 각 참가자 클라이언트에 배포되거나 배포될 수 있는 공통 비디오 및/또는 오디오 스트림; 각 참가자 클라이언트 중 하나 또는 여러 개에서 비디오 통신 서비스에 제공되고 비디오 통신 서비스에 의해 모든 또는 요청하는 참가자 클라이언트에 원시 또는 수정된 형태로 중계되는 각각의 비디오 및/또는 오디오 스트림; 및/또는 공통 시간 참조를 수신한다. 중앙 서버는 아래에 설명된 대로 처리하기 위해 자동 참가자 클라이언트, 및 가능하게는 외부 정보 소스로부터 상기 유형의 비디오 및/또는 오디오 스트림을 수신한 다음에, API을 통해 생성된, 예를 들어 공유된 비디오 스트림을 제공하도록 구성된 수집 기능을 포함할 수 있다. 예를 들어, 이 생성된 비디 오 스트림은 외부 소비자 및/또는 비디오 통신 서비스에 의해 소비되어 다음에 비디오 통신 서비스 에 의해 참가자 클라이언트의 모두 또는 요청한 것에 배포될 수 있다. 도 2는 도 1과 유사하지만 자동 클라이언트 참가자를 사용하는 대신에, 중앙 서버는 비디오 통신 서 비스의 API를 통해 진행 중인 비디오 통신에서 비디오 및/또는 오디오 스트림 데이터를 수신한다. 도 3은 또한 도 1과 유사하지만, 비디오 통신 서비스은 표시하지 않는다. 이 경우, 참가자 클라이언트 은 중앙 서버의 API와 직접 통신하여, 예를 들어 중앙 서버에 비디오 및/또는 오디오 스트 림 데이터를 제공하고/하거나 중앙 서버에서 비디오 및/또는 오디오 스트림 데이터를 수신한다. 그런 다음 생성된 공유 스트림은 외부 소비자 및/또는 클라이언트 참가자 중 하나 또는 여러 개에 제공될 수 있 다. 도 4는 중앙 서버을 보다 자세히 설명한다. 설명된 바와 같이, 상기 수집 기능은 하나 또는 바람직하 게는 여러 개의 형식별 수집 기능(131a)을 포함할 수 있다. 상기 포맷별 수집 기능(131a) 각각은 미리 정해진 포맷, 예를 들어 미리 정해진 바이너리 인코딩 포맷 및/또는 미리 정해진 스트림 데이터 컨테이너를 갖는 비디 오 및/또는 오디오 스트림을 수신하도록 구성될 수 있으며, 상기 포맷의 바이너리 비디오 및/또는 오디오 데이 터를 개별 비디오 프레임, 비디오 프레임 시퀀스 및/또는 시간 슬롯으로 구문 분석하도록 특별히 배열될 수 있 다. 중앙 서버는 또한 수집 기능로부터 비디오 및/또는 오디오 스트림 데이터(예를 들어, 바이너리 스트 림 데이터)를 수신하고 수신된 각 개별 데이터 스트림에 대해 해당 이벤트 감지를 수행하도록 배열되는 이벤트 감지 기능를 포함할 수 있다. 이벤트 감지 기능는 해당 이벤트 감지를 수행하기 위한 AI(인공지능) 구성 요소(132a)를 포함할 수 있다. 이벤트 감지는 수집된 개별 스트림을 먼저 시간 동기화하지 않고도 수행될 수 있다. 중앙 서버는 수집 기능에서 제공하고 이벤트 감지 기능에 의해 처리되었을 수 있는 데이터 스트 림을 시간 동기화하도록 배열된 동기화 기능을 더 포함한다. 동기화 기능은 해당 시간 동기화를 수행 하기 위한 AI 구성 요소(133a)를 포함할 수 있다. 중앙 서버는 수신된 데이터 스트림 중 적어도 하나, 그러나 많은 경우 적어도 두 개, 예를 들어 적어도 세 개 또는 심지어 적어도 네 개, 예를 들어 모든 데이터 스트림의 조합을 기반으로 패턴 감지를 수행하도록 배열 되는 패턴 감지 기능을 더 포함할 수 있다. 패턴 감지는 이벤트 감지 기능에 의해 상기 데이터 스트 림 중 각각에 대해 감지된 하나 또는 일부 경우 적어도 두 개 이상의 이벤트를 기반으로 할 수 있다. 패턴 감지 기능에 의해 고려된 이러한 감지된 이벤트는 수집된 각 개별 스트림에 대해 시간에 따라 분포될 수 있다. 패턴 감지 기능은 상기 패턴 감지를 수행하기 위한 AI 구성 요소(134a)를 포함할 수 있다. 패턴 감지는 또 한 상기 논의된 그룹화를 기반으로 할 수 있으며, 특히 하나의 그룹에 대해서만; 모든 그룹이 아닌 일부 그룹에 대해서만; 모든 그룹에 대해서 발생하는 특정 패턴을 감지하도록 배열될 수 있다. 중앙 서버는 수집 기능에서 제공된 데이터 스트림 또는 스트림을 기반으로 하며, 또한 감지된 이벤트 및/또는 패턴을 기반으로 하여 공유 디지털 비디오 스트림과 같은 생성된 디지털 비디오 스트림을 생성하도록 구성된 생성 기능을 더 포함한다. 이러한 생성된 비디오 스트림은 적어도 수집 기능에서 제공된 하나 이상의 비디오 스트림을 포함하도록 생성된 비디오 스트림을 포함할 수 있으며, 원시, 재포맷 또는 변환된 비디 오 스트림을 포함할 수 있으며, 또한 해당 오디오 스트림 데이터를 포함할 수 있다. 아래에서 예시하는 바와 같 이, 여러 개의 생성된 비디오 스트림이 있을 수 있으며, 여기서 이러한 생성된 비디오 스트림 중 하나는 위에서논의한 방식으로 생성될 수 있지만, 이미 생성된 다른 비디오 스트림을 기반으로 할 수 있다. 모든 생성된 비디오 스트림은 바람직하게는 연속적으로, 바람직하게는 거의 실시간으로(아래에서 논의되는 유형 의 대기 시간과 지연을 할인한 후) 생성된다. 중앙 서버는 상술된 바와 같이, 예를 들어 API을 통해 해당 생성생성된 디지털 비디오 스트림을 게시 하도록 배열된 게시 기능을 포함할 수 있다. 도 1, 2 및 3은 중앙 서버을 사용하여 본 명세서에 설명된 원리를 구현하고 특히 본 발명에 따른 방법을 제공하는 방법에 대한 세 가지 다른 예를 설명하지만, 하나 또는 여러 개의 비디오 통신 서비스을 사용하 거나 사용하지 않는 다른 구성도 가능한다. 도 5는 생성된 디지털 비디오 스트림을 제공하는 방법을 설명한다. 도 6a-6f는 도 5에 설명된 방법 단계에서 발 생하는 다양한 디지털 비디오/오디오 데이터 스트림 상태를 설명한다. 제1 단계 S500에서, 이 방법이 시작된다. 후속의 수집 단계 S501에서, 각각의 기본 디지털 비디오 스트림(210, 301)은 상기 수집 기능에 의해 상기 디지털 비디오 소스(120, 300) 중 적어도 두 개에서 수집된다. 이러한 각 기본 데이터 스트림(210, 301)은 오디 오 부분 및/또는 비디오 부분를 포함할 수 있다. 이 컨텍스트에서 \"비디오\"는 이러한 데이터 스트림 의 움직이는 이미지 및/또는 정지 이미지 콘텐츠를 의미한다. 각 기본 데이터 스트림(210, 301)은 (해당 기본 스트림(210, 301)을 제공하는 개체에서 사용하는 해당 코덱을 사용하는) 모든 비디오/오디오 인코딩 사양에 따 라 인코딩될 수 있다. 인코딩 형식은 동일한 비디오 통신에서 동시에 사용되는 다른 기본 스트림(210, 301)에서 다를 수 있다. 기본 데이터 스트림(210, 301) 중 적어도 하나, 예를 들어 모든 것이 바이너리 데이터 스트림으 로 제공되는 것이 바람직하며, 이는 본래의 기존 데이터 컨테이너 데이터 구조로 제공될 수 있다. 기본 데이터 스트림(210, 301) 중 적어도 하나, 예를 들어 적어도 두 개, 또는 모든 것이 각각의 라이브 비디오 녹화로 제공 되는 것이 바람직하다. 기본 스트림(210, 301)은 수집 기능에서 수신될 때 시간 측면에서 동기화되지 않을 수 있다는 것에 유의한 다. 이는 서로에 대해 서로 다른 대기 시간 또는 지연과 연관되어 있음을 의미할 수 있다. 예를 들어, 두 개의 기본 비디오 스트림(210, 301)이 라이브 녹화인 경우, 이것은 수집 기능에서 수신될 때 녹화 시간과 관련 하여 서로 다른 대기 시간과 연관되어 있음을 의미할 수 있다. 또한 기본 스트림(210, 301) 자체가 웹 카메라의 해당 라이브 카메라 피드; 현재 공유되는 화면 또는 프레젠테 이션; 시청된 필름 클립 또는 이와 유사한 것; 또는 이러한 것들을 하나의 동일한 화면에 다양한 방식으로 배열 한 것의 조합일 수 있다는 것에 유의한다. 수집 단계 S501는 도 6a 및 도 6b에 도시되어 있다. 도 6a에서, 수집 기능이 각 기본 비디오 스트림(210, 301)을 묶음 오디오/비디오 정보 또는 연관된 비디오 스트림 데이터와 분리된 오디오 스트림 데이터로 저장할 수 있는 방법도 도시되어 있다. 도 6b는 기본 비디오 스트림(210, 301) 데이터가 개별 프레임 또는 프레임 의 컬렉션/클러스터로 저장되는 방법을 도시하고, 여기서 \"프레임\"은 이미지 데이터 및/또는 연관된 오디오 데 이터의 시간 제한 부분을 의미하며, 예를 들어 각 프레임은 개별 정지 이미지 또는 연속된 이미지 시리즈(예를 들어, 최대 1초의 움직이는 이미지를 구성하는 이러한 시리즈)가 함께 움직이는 이미지 비디오 콘텐츠를 형성한 다. 이벤트 감지 기능에 의해 수행되는 후속 이벤트 감지 단계 S502에서, 상기 기본 디지털 비디오 스트림 (210, 301)은 상기 이벤트 감지 기능, 예를 들어 상기 AI 구성 요소(132a)에 의해 분석되어, 제1 이벤트 세트에서 선택된 적어도 하나의 이벤트을 감지한다. 이는 도 6c에서 설명된다. 이 이벤트 감지 단계 S502는 적어도 하나, 예를 들어 적어도 두 개, 예를 들어 모든 기본 비디오 스트림(210, 301)에 대해 수행될 수 있으며, 이러한 기본 비디오 스트림(210, 301) 각각에 대해 개별적으로 수행될 수 있는 것이 바람직하다. 다시 말해, 이벤트 감지 단계 S502는 바람직하게 해당 특정 기본 비디오 스트림(210, 301)의 일부로 포함된 정보만을 고려하고, 특히 다른 기본 비디오 스트림의 일부로 포함된 정보는 고려하지 않고, 해당 개별 기본 비디오 스트림(210, 301)에 대해 발생한다. 또한, 이벤트 감지는 바람직하게 여러 기본 비디오 스트 림(210, 301)과 관련된 공통 시간 참조을 고려하지 않고 발생한다. 반면에, 이벤트 감지는 바람직하게는 0초, 예를 들어 적어도 0.1초, 예를 들어 적어도 1초보다 긴, 특정 시간 간격, 예를 들어 기본 비디오 스트림의 과거 시간 간격에 걸쳐 해당 개별 분석된 기본 비디오 스트림의 일부로포함된 정보를 고려한다. 이벤트 감지는 해당 기본 비디오 스트림(210, 301)의 일부로 포함된 오디오 및/또는 비디오 데이터에 포함된 정 보를 고려할 수 있다. 제1 이벤트 집합은 여러 유형의 이벤트, 예를 들어, 해당 기본 비디오 스트림(210, 301)을 구성하거나 해당 기 본 비디오 스트림의 일부인 슬라이드 프레젠테이션의 슬라이드 변경; 해당 기본 비디오 스트림(210, 301)을 제 공하는 소스(120, 300)의 연결 품질 변경, 이에 의해 이미지 품질이 변경되거나 이미지 데이터가 손실되거나 이 미지 데이터가 다시 확보됨; 및 해당 기본 비디오 스트림(210, 301)에서 감지된 움직임 물리적 이벤트, 예를 들 어, 비디오에서 사람이나 물체의 움직임, 비디오의 조명 변경, 오디오의 갑자기 날카로운 노이즈 또는 오디오 품질 변경을 포함할 수 있다. 이는 전체 목록인 것으로 의도하지는 않았고, 이러한 예는 현재 설명된 원리의 적 용 가능성을 이해하기 위해 제공한 것이다. 동기화 기능에 의해 수행되는 후속 동기화 단계 S503에서, 기본 디지털 비디오 스트림은 시간 동기화 도리 수 있다. 이 시간 동기화는 공통 시간 참조에 대한 것일 수 있다. 도 6d에 도시된 바와 같이, 시간 동기화는 예를 들어, 공통 시간 참조을 사용하여 기본 비디오 스트림(210, 301)을 서로에 대해 정렬하는 것을 포함할 수 있으므로, 이들을 결합하여 시간 동기화된 컨텍스트를 형성할 수 있다. 공통 시간 참조는 데이터 스트림, 하트비트 신호 또는 기타 펄스 데이터 또는 각 개별 기본 비디오 스트림(210, 301)에 적용 가능 한 시간 앵커일 수 있다. 공통 시간 참조는 해당 기본 비디오 스트림(210, 301)의 정보적 내용이 공통 시간 축 에 대해 공통 시간 참조와 명확하게 관련될 수 있는 방식으로 각 개별 기본 비디오 스트림(210, 301)에 적용될 수 있다. 즉, 공통 시간 참조는 시간 이동을 통해 기본 비디오 스트림(210, 301)이 정렬되어 현재 의미에서 시 간 동기화되도록 할 수 있다. 다른 실시 예에서, 시간 동기화는 해당 기본 비디오 스트림(210, 301) 간의 시간 차이에 대해 알려진 정보에 기반, 예를 들어 측정에 기반할 수 있다. 도 6d에 도시된 바와 같이, 시간 동기화는 각 기본 비디오 스트림(210, 301)에 대해, 하나 또는 여러 개의 타임 스탬프을, 예를 들어, 공통 시간 참조과 관련하여 또는 각 비디오 스트림(210, 301)에 대해 다른 비 디오 스트림(210, 301) 또는 다른 비디오 스트림(210, 301)과 관련하여 결정하는 것을 포함할 수 있다. 패턴 감지 기능에 의해 수행되는 후속 패턴 감지 단계 S504에서, 이렇게 시간 동기화된 기본 디지털 비디 오 스트림(210, 301)은 제1 패턴 세트에서 선택된 적어도 하나의 패턴를 감지하기 위해 분석된다. 이것은 도 6e에 도시되어 있다. 이벤트 감지 단계 S502와 대조적으로, 패턴 감지 단계 S504는 공동으로 고려되는 시간 동기화된 기본 비디오 스 트림(210, 301) 중 적어도 두 개의 일부로 포함된 비디오 및/또는 오디오 정보를 기반으로 수행될 수 있다. 상기 제1 패턴 세트는 여러 유형의 패턴, 예를 들어, 몇 명의 참가자가 번갈아가며 또는 동시에 말하는 것; 또 는 다른 참가자가 말하는 것과 같이 프레젠테이션 슬라이드 변경이 다른 이벤트로 동시에 발생하는 것을 포함할 수 있다. 이 목록은 포괄적인 것은 아니고, 설명을 위한 것이다. 일부 실시 예에서, 감지된 패턴는 여러 개의 상기 기본 비디오 스트림(210, 301)에 포함된 정보가 아니라 상기 기본 비디오 스트림(210, 301) 중 하나에만 포함된 정보와 관련될 수 있다. 이러한 경우, 이러한 패턴 는 적어도 두 개의 감지된 이벤트, 예를 들어 두 개 이상의 연속 감지된 프레젠테이션 슬라이드 변경 또는 연결 품질 변경에 걸쳐 있는 해당 단일 기본 비디오 스트림(210, 301)에 포함된 비디오 및/또는 오디오 정 보를 기반으로 감지되는 것이 바람직하다. 예를 들어, 시간이 지남에 따라 서로 빠르게 이어지는 여러 개의 연 속적인 슬라이드 변경이 감지된 각각의 슬라이드 변경 이벤트에 대한 하나의 개별적인 슬라이드 변경 패턴과 대 조적으로, 하나의 단일 슬라이드 변경 패턴으로 감지될 수 있다. 다른 예는 표시된 개체나 사람의 움직임; 및 참가 사용자가 말한 음성 문구의 인식을 포함한다. 제1 이벤트 집합과 제1 패턴 집합은 매개변수 집합과 매개변수 간격의 각각의 집합을 사용하여 정의된 미리 정 해진 유형의 이벤트/패턴을 포함할 수 있다는 것을 알 수 있다. 아래에서 설명하는 바와 같이, 해당 집합의 이 벤트/패턴은 다양한 AI 도구를 사용하여 정의되고 감지될 수도 있다. 생성 기능에 의해 수행되는 후속 생성 단계 S505에서, 공유 디지털 비디오 스트림은 가능하게 시간 동기화 된 기본 디지털 비디오 스트림(210, 301)의 연속적으로 고려된 프레임, 및 또한 상기 감지된 이벤트 및/또는 상기 감지된 패턴를 기반으로 출력 디지털 비디오 스트림으로 생성된다. 이하에서 설명하고 자세히 설명하는 바와 같이, 본 발명은 출력 디지털 비디오 스트림과 같은 비디오 스트 림의 완전 자동적 생성을 허용한다. 예를 들어, 이러한 생성은 어떤 기본 비디오 스트림(210, 301)에서 어떤 비디오 및/또는 오디오 정보를 선택하 여 이러한 출력 비디오 스트림에서 어느 정도 사용할 것인지; 출력 비디오 스트림의 비디오 화면 레 이아웃; 시간에 따른 이러한 다양한 용도 또는 레이아웃 간의 전환 패턴; 등을 포함할 수 있다. 이는 또한 (예를 들어, 상기 공통 시간 참조에) 시간 동기화되어 시간 동기화된 기본 비디오 스트림(210, 301)과 함께 출력 비디오 스트림의 생성에 사용될 수 있는 추가의 디지털 비디오 정보 스트림과 같은, (공 통 시간 참조과 관련될 수 있는) 하나 이상의 추가 시간 관련 디지털 비디오 정보를 또한 도시하는 도 6f에 도시되어 있다. 예를 들어, 추가 스트림은 감지된 패턴을 기반으로 동적으로 사용할 수 있는 비디 오 및/또는 오디오 특수 효과; 비디오 통신을 위한 계획된 시간 일정 등과 관련된 정보를 포함할 수 있다. 게시 기능에 의해 수행되는 후속 게시 단계 S506에서, 생성된 출력 디지털 비디오 스트림은 상술된 바와 같이 생성된 디지털 비디오 스트림의 소비자(110, 150)에게 지속적으로 제공된다. 생성된 디지털 비디오 스트림은 비디오 통신 서비스 등을 통해 하나 이상의 참가자 클라이언트에게 제공될 수 있다. 후속 단계 S507에서, 방법은 종료된다. 그러나 먼저 이 방법은 출력 비디오 스트림을 지속적으로 제공되는 스트림으로 생성하기 위해서, 도 5에 도시된 바와 같이 어떤 회수라도 반복할 수 있다. 바람직하게는, 출력 비 디오 스트림은 실시간 또는 거의 실시간으로 (모든 단계에서 추가된 총 대기 시간을 고려함), 및 지속적으 로(추가 정보가 제공될 때 즉시 게시되지만, 의도적으로 추가된 대기 시간이나 지연은 계산하지 않음, 아래 참 조) 소비되도록 생성된다. 이런 방식으로, 출력 비디오 스트림은 대화형 방식으로 소비될 수 있으므로, 출 력 비디오 스트림은 비디오 통신 서비스으로 또는 수집 기능에 다시 공급되는 기본 비디오 스트 림을 생성하기 위한 기반을 형성하는 다른 컨텍스트로 다시 피드백되어 닫힌 피드백 루프를 형성할 수 있 거나; 출력 비디오 스트림이 다른(시스템 외부 또는 적어도 중앙 서버 외부) 컨텍스트에서 소비 될 수 있지만, 이를 통해 실시간 대화형 비디오 통신의 기반을 형성할 수 있다. 상술한 바와 같이, 일부 실시 예에서는 상기 기본 디지털 비디오 스트림(210, 301) 중 적어도 두 개, 예를 들어 적어도 세 개, 예를 들어 적어도 네 개, 또는 적어도 다섯 개가 상기 비디오 통신 서비스에서 제공하는 것 과 같은 공유 디지털 비디오 통신의 일부로 제공되고, 해당 비디오 통신에는 해당 기본 디지털 비디오 스트림 을 제공하는 각각의 원격으로 연결된 참가자 클라이언트이 포함된다. 이러한 경우, 수집 단계 S501는 예를 들어, 해당 비디오 통신 서비스 내부로부터 비디오 및/또는 오디오 스트림 데이터에 대한 액세스가 부여되는 자동 참가자 클라이언트을 통해 및/또는 비디오 통신 서비스의 API를 통해, 공유 디지 털 비디오 통신 서비스 자체로부터 적어도 하나의 상기 기본 디지털 비디오 스트림을 수집하는 것을 포함할 수 있다. 또한, 이 경우 및 그 외 경우에서, 수집 단계 S501는 상기 기본 디지털 비디오 스트림(210, 301) 중 적어도 하 나를 공유 디지털 비디오 통신 서비스 외부에 있는 정보 소스에서 수집된 각각의 외부 디지털 비디오 스트림로 수집하는 것을 포함할 수 있다. 사용된 이러한 외부 비디오 소스 중 하나 또는 여러 개가 중앙 서버 외부에 있을 수도 있다는 점에 유의한다. 일부 실시 예에서, 기본 비디오 스트림(210, 301)은 동일한 방식으로 포맷되지 않는다. 이러한 서로 다른 포맷 은 서로 다른 유형의 데이터 컨테이너(예를 들어, AVI 또는 MPEG)의 수집 기능에 전달되는 형태일 수 있지 만, 바람직한 실시 예에서 기본 비디오 스트림(210, 301) 중 적어도 하나는 (해당 기본 비디오 스트림(210, 301) 중 적어도 하나와 비교했을 때) 편차 기본 디지털 비디오 스트림(210, 301)이 편차 비디오 인코딩, 편차 고정 또는 가변 프레임 속도, 편차 종횡비, 편차 비디오 해상도 및/또는 편차 오디오 샘플 속도를 갖는 측면에 서 편차 포맷에 따라 포맷된다. 수집 기능은 모든 수집된 기본 비디오 스트림(210, 301)에서 발생하는 모든 인코딩 형식, 컨테이너 표준 등을 읽고 해석하도록 미리 구성되는 것이 바람직하다. 이렇게 하면 본 명세서에서 설명한 대로 처리를 수행할 수 있으며, 프로세스의 비교적 후반부까지 (예를 들어, 해당 기본 스트림이 해당 버퍼에 들어간 후까지; 이벤트 감지 단계 S502 후까지; 또는 심지어 이벤트 감지 단계 S502 후까지) 디코딩이 필요하지 않다. 그러나 기본 비 디오 피드(210, 301) 중 하나 이상이 수집 기능이 디코딩 없이는 해석할 수 없는 코덱을 사용하여 인코딩 되는 드문 경우, 수집 기능은 이러한 기본 비디오 스트림(210, 301)의 디코딩 및 분석을 수행하고, 이어서 예를 들어 이벤트 감지 기능에서 처리할 수 있는 형식으로의 변환을 수행하도록 배열될 수 있다. 이 경우에도, 이 단계에서는 재인코딩을 수행하지 않는 것이 바람직한 것에 유의한다.예를 들어, 비디오 통신 서비스에 의해 제공되는 것과 같이, 다자간 비디오 이벤트로부터 기본 비디오 스 트림을 가져오면 일반적으로 낮은 대기 시간에 대한 요구 사항을 가지므로 참가자가 효과적인 통신을 할 수 있도록 하는 가변 프레임 속도 및 가변 픽셀 해상도와 일반적으로 연관된다. 다시 말해, 전체 비디오 및 오디오 품질은 낮은 대기 시간을 위해 필요에 따라 감소된다. 반면 외부 비디오 피드는 일반적으로 더 안정적인 프레임 속도, 더 높은 품질을 갖지만 따라서 더 높은 대 기 시간을 가질 수 있다. 따라서 비디오 통신 서비스은 각 시점에서 외부 비디오 소스와는 다른 인코딩 및/또는 컨테이너를 사 용할 수 있다. 따라서 이 경우 본 명세서에서 설명된 분석 및 비디오 생성 프로세스는 결합된 경험을 위해 서로 다른 형식의 스트림(210, 301)을 새로운 스트림에 결합해야 한다. 상술된 바와 같이, 수집 기능은 형식별 수집 기능(131a)의 집합을 포함할 수 있으며, 각각은 특정 유형의 형식의 기본 비디오 스트림(210, 301)을 처리하도록 배열되어 있다. 예를 들어, 이러한 형식별 수집 기능(131a) 각각은 Windows® Media® 또는 DivX®와 같은 다른 비디오 각각의 인코딩 방법/코덱을 사용하여 인코딩된 기본 비디오 스트림(210, 301)을 처리하도록 배열될 수 있다. 그러나 일부 실시 예에서, 수집 단계 S501는 기본 디지털 비디오 스트림(210, 301) 중 적어도 두 개, 예를 들어 모든 것을 공통 프로토콜로 변환하는 단계를 포함한다. 이 컨텍스트에서 사용되는 용어 \"프로토콜\"은 디지털 비디오/오디오 스트림에 포함된 정보를 저장하는 방법을 지정하는 정보 구조화 표준 또는 데이터 구조를 말한다. 그러나 공통 프로토콜은 디지털 비디오 및/또는 오디오 정보를 예를 들어 바이너리 수준(즉, 사운드와 이미지 자체를 지시하는 인코딩/압축된 데이터)으로 저장하는 방 법을 지정하지 않고, 대신 이러한 데이터를 저장하기 위한 미리 결정된 형식의 구조를 형성한다. 다시 말해, 공 통 프로토콜은 이러한 저장과 관련하여 디지털 비디오 디코딩이나 디지털 비디오 인코딩을 수행하지 않고 원시 바이너리 형태로 디지털 비디오 데이터를 저장하도록 규정한다. 기존 바이너리 형태를 전혀 수정하지 않고 바이 너리 형태 바이트 시퀀스를 연결 및/또는 분할할 수 있다. 대신에, 이 원시 바이너리 데이터를 프로토콜에 의해 정의된 데이터 구조로 다시 패키징하면서, 해당 기본 비디오 스트림(210, 301)의 원시 (인코딩/압축된) 바이너 리 데이터 내용은 유지된다. 일부 구현예에서, 공통 프로토콜은 비디오 파일 컨테이너 형식을 정의한다. 도 7은 예를 들어, 도 6a에 도시된 기본 비디오 스트림(210, 301)을 설명하는데, 이는 해당 형식별 수집 기능 (131a)에 의해 재구성되고 상기 공통 프로토콜을 사용한다. 따라서, 공통 프로토콜은 디지털 비디오 및/또는 오디오 데이터를, 바람직하게는 해당 기본 비디오 스트림 (210, 301)에 관한 시간선을 따라 이산적이고 연속적인 데이터 세트로 나뉘는, 데이터 세트에 저장하도록 규정한다. 각각의 이러한 데이터 세트는 하나 이상의 비디오 프레임과 연관된 오디오 데이터를 포함할 수 있다. 공통 프로토콜은 또한 저장된 디지털 비디오 및/또는 오디오 데이터 세트와 관련하여 지정된 시간 지 점과 연관된 메타데이터를 저장하도록 규정할 수 있다. 메타데이터는 예를 들어, 해당 원시 바이너리 데이터; 비디오 데이터의 해상도, 비디오 프레임 속도, 프레 임 속도 가변성 플래그, 비디오 해상도, 비디오 종횡비, 오디오 압축 알고리즘 또는 오디오 샘플링 속도를 생성 하는 데 사용된 디지털 비디오 인코딩 방법 또는 코덱에 관하여, 해당 기본 디지털 비디오 스트림의 원시 바이너리 형식에 대한 정보를 포함할 수 있다. 메타데이터는 또한 해당 기본 비디오 스트림(210, 301)의 시간 참조와 관련하여 또는 상술된 바와 같은 다른 비디오 스트림과 관련하여, 저장된 데이터의 타임스탬프에 대한 정보를 포함할 수 있다. 해당 공통 프로토콜과 함께 해당 형식별 수집 기능(131a)를 사용하면 수신된 비디오/오디오 데이터를 디코 딩/재인코딩하여 대기 시간을 추가하지 않고 기본 비디오 스트림(210, 301)의 정보 콘텐츠를 빠르게 수집할 수 있다. 따라서, 수집 단계 S501는 해당 기본 비디오 스트림(210, 301)을 구문 분석하고 구문 분석된 원시 및 이진 데이 터를 공통 프로토콜을 사용하여 데이터 구조에 저장하고 관련 메타데이터를 함께 저장하기 위해서, 서로 다른 이진 비디오 및/또는 오디오 인코딩 형식을 사용하여 인코딩된 기본 디지털 비디오 스트림(210, 301)을 수집하 기 위한 상기 형식별 수집 기능(131a) 중 서로 다른 기능을 사용하는 것을 포함할 수 있다. 당연히, 어떤 기본 비디오 스트림(210, 301)에 어떤 형식별 수집 기능(131a)를 사용할지 결정하는 것은 해당 기본 비디오 스트림(210, 301) 각각의 미리 결정되거나 동적으로 감지된 속성에 기반하여 수집 기능에 의해 수행될 수 있다. 따라서 수집된 각 기본 비디오 스트림(210, 301)은 중앙 서버의 RAM 메모리 버퍼와 같은 자체 별도 메모리 버퍼에 저장될 수 있다. 따라서 각 형식별 수집 기능(131a)에 의해 수행되는 기본 비디오 스트림(210, 301)의 변환은 이렇게 변환된 각 기본 디지털 비디오 스트림(210, 301)의 원시 이진 데이터를 정렬된 더 작은 데이터 세트로 분할하는 것을 포함할 수 있다. 또한, 변환은 예를 들어, 공통 시간 참조과 관련하여, 상기 적은 세트의 각각 (또는 해당 기본 스트 림(210, 301)의 각각의 시간선을 따라 규칙적으로 분포된 부분 집합과 같은 부분 집합)을 공유 시간선을 따라 각각의 시간과 연관시키는 단계를 포함할 수 있다. 이 연관은 후술되는 주요 방법 중 하나 또는 다른 방법으로 원시 바이너리 비디오 및/또는 오디오 데이터를 분석하여 수행할 수 있으며, 기본 비디오 스트림(210, 301)의 후속 시간 동기화를 수행할 수 있기 위해 수행될 수 있다. 사용된 공통 시간 참조의 유형에 따라, 각 데이터 세 트의 이 연관의 적어도 일부는 동기화 기능에 의해 또한 또는 대신에 수행될 수 있다. 후자의 경우, 수집 단계 S501는 더 작은 세트의 각각이나 그 하위 집합을 해당 기본 스트림(210, 301)에 대해 특정적인 타임라인의 각각의 시간과 연관시키는 단계를 포함할 수 있다. 일부 실시 예에서, 수집 단계 S501는 또한 기본 비디오 스트림(210, 301)에서 수집된 원시 바이너리 비디오 및/ 또는 오디오 데이터를 균일한 품질 및/또는 업데이트 주파수로 변환하는 것을 포함한다. 이것은 필요에 따라 기 본 디지털 비디오 스트림(210, 301)의 해당 원시 바이너리 디지털 비디오 및/또는 오디오 데이터를 공통 비디오 프레임 속도, 공통 비디오 해상도 또는 공통 오디오 샘플링 속도로 다운샘플링 또는 업샘플링하는 것을 포함할 수 있다. 이러한 재샘플링은 해당 형식별 수집 기능(131a)이 올바른 바이너리 인코딩 대상 형식에 따라 원시 바 이너리 데이터를 직접 처리할 수 있기 때문에 전체 디코딩/재인코딩을 수행하지 않고 또는 전혀 디코딩을 수행 하지 않고도 수행될 수 있다는 점에 유의한다. 상기 기본 디지털 비디오 스트림(210, 301)의 각각은 개별 프레임 또는 상기 설명된 프레임의 시퀀스 로서 개별 데이터 저장 버퍼에 저장될 수 있으며, 또한 각각은 상기 공통 시간 참조과 차례로 연관된 해당 타임 스탬프와 연관될 수 있다. 이러한 원리를 설명하기 위해 제공되는 구체적인 예에서, 비디오 통신 서비스는 동시 참가자를 포함 하는 화상 회의를 실행하는 Microsoft® Teams®이다. 자동 참가자 클라이언트은 Teems® 회의에서 회의 참 가자로 등록된다. 그런 다음, 기본 비디오 입력 신호는 자동 참가자 클라이언트을 통해 수집 기능에 사용할 수 있 고 이에 의해 수집된다. 이들은 H264 형식의 원시 신호이며 모든 비디오 프레임에 대한 타임스탬프 정보를 포함 한다. 관련 형식별 수집 기능(131a)은 구성 가능한 사전 정의된 TCP 포트에서 IP(LAN 네트워크)를 통해 원시 데이터를 수집한다. 모든 Teems® 회의 참가자와 관련 오디오 데이터는 별도의 포트와 연결된다. 그런 다음 수집 기능 은 오디오 신호(50Hz)의 타임스탬프를 사용하고 비디오 스트림을 해당 개별 버퍼에 저장하기 전 에 비디오 데이터를 25Hz의 고정 출력 신호로 다운샘플링한다. 언급한 바와 같이, 공통 프로토콜은 원시 바이너리 형태로 데이터를 저장할 수 있다. 매우 낮은 수준으로 설계되어 비디오/오디오 데이터의 원시 비트와 바이트를 처리할 수 있다. 바람직한 실시 예에서 데이터는 공통 프로토콜에 간단한 바이트 배열 또는 해당 데이터 구조(예를 들어, 슬라이스)로 저장된다. 이는 데이터를 기존 비디오 컨테이너에 넣을 필요가 전혀 없다는 것을 의미한다(이 컨텍스트에서 해당 공통 프로토콜은 기존 컨테이너를 구성하지 않음). 또한, 비디오 인코딩 및 디코딩은 계산량이 많고, 이는 지연이 발생하고 값비 싼 하드웨어를 필요로 하는 것을 의미한다. 게다가 이 문제는 참가자 수에 따라 증가한다. 공통 프로토콜을 사용하면, 각 Teams® 회의 참가자와 관련된 기본 비디오 스트림 및 모든 외부 비디오 소스에 대해 메모리를 수집 기능에서 예약한 다음에, 프로세스 중에 할당된 메모리 양을 즉시 변경하는 것이 가능하게 된다. 이렇게 하면 입력 스트림의 수를 변경하고 결과적으로 각 버퍼를 효과적으로 유 지하는 것이 가능하게 된다. 예를 들어 해상도, 프레임 속도 등의 정보는 가변적일 수 있지만 공통 프로토콜 에 메타데이터로 저장되기 때문에, 이 정보를 사용하여 필요에 따라 각 버퍼의 크기를 빠르게 조정할 수있다. 다음은 현재 유형의 공통 프로토콜 사양의 예이다. 바이트 예제 설명 1바이트 1 0=비디오; 1=오디오 4바이트 1234567 버퍼 길이(정수) 8바이트 424234234 수신오디오/비디오버퍼의 타임스탬프, 틱 단위로 측정, 1틱 = 100ns. (긴 정수) 1바이트 0 VideoColorFormat { NV12 = 0, Rgb24 = 1, Yuy2 = 2, H264 = 3 } 4바이트 720 비디오 프레임 픽셀 높이(정수) 4바이트 640 비디오 프레임 픽셀 너비(정수) 4바이트 25.0 비디오 프레임 속도 초당 프레임 수(부동소수점) 1바이트 0 오디오가 없는가? 1 = true; 0 = false 1바이트 0 AudioFormat { 0 = Pcm16K 1 = Pcm44KStereo } 1바이트 0 있는 경우 감지된 이벤트 0 = 이벤트 없음 1, 2, 3 등=지정된유형의 이벤트 감지 30바이트 향후 사용을 위해 예약 8바이트 1000000 바이트 단위의 이진 데이터 길이(long int) 변수 0x87A879… 이 프레임(들)의 원시 이진 비디오/오디오 데이터 4바이트 1234567 우세한 화자 포트 4바이트 1234567 활동 화자 상기에서, \"있는 경우 감지된 이벤트\" 데이터는 공통 프로토콜 사양의 일부로 포함된다. 그러나 일부 실시 예에서, (감지된 이벤트에 관한) 이 정보는 대신 별도의 메모리 버퍼에 저장될 수 있다. 일부 실시 예에서, 오버레이 또는 효과일 수 있는 적어도 하나의 추가 디지털 비디오 정보은 또한 해당 개 별 버퍼에, 해당 공통 시간 참조과 차례로 연관된 해당 시간 스탬프와 각각 연관되는 개별적 프레임 또는 프레임 시퀀스로 저장된다. 위에서 예시한 바와 같이, 이벤트 감지 단계 S502는 해당 이벤트이 감지된 기본 디지털 비디오 스트림 (210, 301)과 연관된 감지된 이벤트을 설명하는 메타데이터를 상기 공통 프로토콜을 사용하여 저장하는 것을 포함할 수 있다. 이벤트 감지는 다양한 방식으로 수행될 수 있다. AI 구성 요소(132a)에 의해 수행되는 일부 실시 예에서, 이벤 트 감지 단계 S502는 제1 훈련된 신경망 또는 기타 머신 러닝 구성 요소가 상기 이벤트 중 하나를 자동으 로 감지하기 위해 상기 기본 디지털 비디오 스트림(210, 301) 중 하나 이상, 예를 들어 여러 개 또는 심지어 전 부를 개별적으로 분석하는 것을 포함한다. 이것은 AI 구성 요소(132a)가 관리된 분류에서 기본 비디오 스트림 (210, 301) 데이터를 미리 정의된 이벤트 집합으로 분류하거나, 관리되지 않은 분류에서 동적으로 결정된 이벤 트 집합으로 분류하는 단계를 포함할 수 있다. 일부 실시 예에서, 감지된 이벤트는 프레젠테이션의 프레젠테이션 슬라이드의 변경이 해당 기본 비디오 스 트림(210, 301)에 있거나 이에 포함되는 것이다. 예를 들어, 프레젠테이션의 발표자가 그 당시 청중에게 제공하고 있는 프레젠테이션의 슬라이드를 변경하기로 결정하면, 이것은 주어진 시청자에게 흥미로운 것이 변경될 수 있음을 의미한다. 새로 표시된 슬라이드는 소위 \"나비\" 모드에서 간략하게 가장 잘 볼 수 있는 높은 수준의 그림일 수 있다(예를 들어, 출력 비디오 스트림 에서 발표자의 비디오와 나란히 슬라이드를 표시하는 것). 또는, 슬라이드는 많은 세부 정보, 작은 글꼴 크기의 텍스트 등을 포함할 수 있다. 후자의 경우, 슬라이드는 대신 전체 화면으로 표시되어야 하며, 일반적으 로 발생하는 것보다 다소 긴 시간 동안 표시되어야 한다. 나비 모드는 적절하지 않을 수 있는데, 이 경우 슬라 이드가 발표자의 얼굴보다 프레젠테이션을 보는 사람에게 더 흥미로울 수 있기 때문이다. 실제로, 이벤트 감지 단계 S502는 다음 중 하나 이상을 포함할 수 있다. 첫째, 이벤트는 감지된 슬라이드의 제1 이미지와 감지된 슬라이드의 후속 제2 이미지 간의 차이에 대한 이 미지 분석을 기반으로 감지될 수 있다. 슬라이드를 보여주는 주 비디오 스트림(220, 301)의 특성은 예를 들어 광학 문자 인식(OCR)과 함께 동작 감지를 사용하여, 기존의 디지털 이미지 처리를 사용하여 자동으로 결정될 수 있다. 이것은 자동 컴퓨터 이미지 처리 기술을 사용하여 감지된 슬라이드가 실제로 슬라이드 변경으로 분류할 만큼 상 당히 변경되었는지 확인하는 단계를 포함할 수 있다. 이것은 RGB 색상 값과 관련하여 현재 슬라이드와 이전 슬 라이드 간의 델타를 확인하여 수행될 수 있다. 예를 들어, 해당 슬라이드가 덮은 화면 영역에서 RGB 값이 전역 적으로 얼마나 변경되었는지 평가하고, 함께 속하고 함께 변경되는 픽셀 그룹을 찾는 것이 가능한지 확인할 수 있다. 이런 식으로 관련 슬라이드 변경 사항을 감지하는 동시에 화면에서 표시된 컴퓨터 마우스 움직임과 같은 무관한 변경 사항은 필터링할 수 있다. 이 접근 방식은 또한 완전한 구성 가능성을 허용하며, 예를 들어 발표자 가 여러가지를 가리키면서 컴퓨터 마우스를 사용하여 무언가를 자세히 발표하려는 경우, 컴퓨터 마우스 움직임 을 캡처할 수 있는 것이 바람직한 경우가 있다. 둘째, 이벤트는 상기 제2 이미지 자체의 정보적 복잡성에 대한 이미지 분석을 기반으로 감지하여 이벤트 유형을 보다 구체적으로 결정할 수 있다. 이것은 예를 들어, 해당 슬라이드의 총 텍스트 정보, 뿐만 아니라 관련 글꼴 크기를 평가하는 것을 포함할 수 있다. 이것은 딥 러닝 기반 문자 인식 기술과 같은 기존 OCR 방법을 사용하여 수행할 수 있다. 평가된 비디오 스트림(210, 301)의 원시 바이너리 형식이 알려져 있기 때문에, 이것은 비디오 데이터를 먼저 디코딩하거나 재인코딩하지 않고 바이너리 도메인에서 직접 수행될 수 있다는 것에 유의한다. 예를 들어, 이벤 트 감지 기능는 이미지 해석 서비스에 대한 관련 형식별 수집 기능을 호출할 수 있거나, 이벤트 감지 기능 자체가 여러 지원되는 원시 바이너리 비디오 데이터 형식에 대한 개별 픽셀 수준과 같은 이미지 정보를 평가하는 기능을 포함할 수 있다. 다른 예에서, 감지된 이벤트는 참가자 클라이언트과 디지털 비디오 통신 서비스의 통신 연결이 끊어진 것이다. 그런 다음, 감지 단계 S502는 해당 참가자 클라이언트에 해당하는 기본 디지털 비디오 스트림의 일련의 후속 비디오 프레임에 대한 이미지 분석에 근거하여, 해당 참가자 클라이언트이 통신 연결이 끊어졌음을 감지하는 것을 포함할 수 있다. 참가자 클라이언트가 서로 다른 물리적 위치와 서로 다른 인터넷 연결과 연관도리 수 있기 때문에, 누군가 비디오 통신 서비스 또는 중앙 서버과의 연결이 끊어지는 일이 발생할 수 있다. 그러한 상황에서, 생 성된 출력 비디오 스트림에 검은색 또는 빈 화면을 표시하는 것은 피하는 것이 좋다. 대신, 이러한 연결 손실은 예를 들어, 사용된 2개의 클래스가 연결/연결되지 않은(데이터 없음) 2-클래스 분류 알고리즘을 적용하여, 이벤트 감지 기능에 의해 이벤트로 감지될 수 있다. 이 경우, \"데이터 없음\"은 발표 자가 의도적으로 검은색 화면을 보내는 것과 다르다는 것이 이해되어야 한다. 1개 또는 2개 프레임과 같은 짧은 검은색 화면은 최종 생성 스트림에서 눈에 띄지 않을 수 있으므로, 시간에 따라 상기 2-클래스 분류 알고 리즘을 적용하여 시계열을 생성할 수 있다. 그런 다음 연결 중단에 대한 최소 길이를 지정하는 임계값을 사용하 여 연결이 끊어졌는지 여부를 결정할 수 있다. 다른 예에서, 이벤트는 해당 기본 디지털 비디오 스트림(210, 301)의 하나 또는 여러 이미지에서 참가하는 인간 사용자의 존재 또는 움직임을 감지하는 것이다. 다른 예에서, 이벤트는 해당 기본 디지털 비디오 스트림(210, 301)을 생성하는 데 사용된 카메라의 움직임(회전, 줌잉, 패닝 등)을 감지하는 것으로, 이러한 움직임의 일반적 인 움직임 구성 요소 및/또는 노이즈 움직임 구성 요소에 대한 정보를 포함한다. 노이즈 움직임 구성 요소는 예 를 들어 카메라가 수동으로 움직이는 것에 기인할 수 있다. 인간 사용자의 존재/움직임 감지, 및/또는 해당 카 메라의 움직임 감지는 예를 들어 위에서 예시한 바와 같이 기존의 디지털 이미지 처리 기술을 사용하여 달성될 수 있다. 이하 설명하는 바와 같이, 이러한 예시된 유형의 감지된 이벤트는 패턴 감지 기능에 의해 적합하고 원하는 대로 다양한 조치를 취하는 데 사용될 수 있다. 언급된 바와 같이, 개별 기본 비디오 스트림(210, 301)은 각각 공통 시간 참조 또는 시간 영역에서 서로 관련될 수 있으며, 이에 의해 동기화 기능이 서로에 대해 시간 동기화할 수 있다. 일부 실시 예에서, 공통 시간 참조은 공통 오디오 신호를 기반으로 하거나 이를 포함하며(도 1-3 참 조), 공통 오디오 신호는 위에서 설명한 바와 같이 적어도 두 개의 원격으로 연결된 참가자 클라이언트 을 포함하는 공유 디지털 비디오 통신 서비스에 공통이며, 각각은 해당 기본 디지털 비디오 스트림 중 각각의 것을 제공한다. 위에서 논의한 Microsoft® Teams®의 예에서, 공통 오디오 신호는 자동 참가자 클라이언트 및/또는 API를 통해 중앙 서버에 의해 생성되어 캡처될 수 있다. 이 예와 다른 예에서, 이러한 공통 오디오 신호는 하트비트 신호로 사용되어 이 하트비트 신호를 기반으로 각각을 특정 시간 지점에 바인딩하여 개별 기본 비디오 스트림을 시간 동기화할 수 있다. 이러한 공통 오디오 신호는 (각 다른 기본 비디오 스트림과 관련하여) 별도의 신호로 제공될 수 있으며, 이로 인해 해당 다른 기본 비디오 스트림에 포함된 오디오를 기반으로 하거나 심지어 여기에 포함된 이미지 정보를 기반으로 하여 다른 기본 비디오 스트림은 각각 공 통 오디오 신호와 개별적으로 시간 상관될 수 있다(예를 들어, 자동 이미지 처리 기반 립싱크 기술 사용하여). 다시 말해, 개별 기본 비디오 스트림과 관련된 모든 변수 및/또는 다른 대기 시간을 처리하고, 결합된 비 디오 출력 스트림에 대한 시간 동기화를 달성하기 위해서, 이러한 공통 오디오 신호는 중앙 서버의 모든 기본 비디오 스트림에 대한 하트비트로 사용될 수 있다(하지만 외부 기본 비디오 스트림은 아닐 수 있음). 다시 말해서, 다른 모든 신호는 모두 시간 동기화되는 것을 확실하게 하도록 이 공통 오디오 시간 하 트비트에 매핑될 수 있다. 다른 예에서, 시간 동기화는 출력 디지털 비디오 스트림에 도입되고 참가자 클라이언트 중 하나 또는 여러 개의 개별적인 것의 일부로 제공되는 해당 로컬 시간 동기화 소프트웨어 기능에 의해 감지되는 시간 동기화 요소을 사용하여 달성되며, 로컬 소프트웨어 기능는 출력 비디오 스트림에서 시간 동기 화 요소의 도착 시간을 감지하도록 배열된다. 이해되는 바와 같이, 이러한 구현예에서 출력 비디오 스트림 은 비디오 통신 서비스으로 다시 공급되거나 다르게는 각 참가자 클라이언트 및 해당 로컬 소프 트웨어 기능에 이용하게 된다. 예를 들어, 시간 동기화 요소는 시각적 마커, 예를 들어, 정기적인 시간 간격으로 출력 비디오에 배 치되거나 업데이트되는 미리 정해진 순서 또는 방식으로 색상을 변경하는 픽셀; 출력 비디오에서 업데이트되고 표시되는 시각적 시계; (예를 들어, 충분히 낮은 진폭 및/또는 충분히 높은 주파수를 갖는 방식으로 참가 자에게 들리지 않도록 설계될 수 있는) 출력 비디오 스트림의 오디오 형성 부분에 추가되는 사운드 신호일 수 있다. 로컬 소프트웨어 기능은 적절한 이미지 및/또는 오디오 처리를 사용하여 각각의 시간 동 기화 요소의 각각의 도착 시간을 자동으로 감지하도록 배열된다. 그런 다음, 공통 시간 참조는 적어도 상기 감지된 도착 시간을 기준으로 결정될 수 있다. 예를 들어, 각 로컬 소프트웨어 기능는 상기 감지된 도착 시간을 나타내는 각각의 정보를 중앙 서버에 전달할 수 있 다. 이러한 통신은 해당 참가자 클라이언트과 중앙 서버 간의 직접 통신 링크를 통해 이루어질 수 있다. 그러나, 통신은 또한 해당 참가자 클라이언트와 연관된 기본 비디오 스트림을 통해 이루어질 수도 있 다. 예를 들어, 참가자 클라이언트는 중앙 서버에 의한 자동 감지를 위해 해당 참가자 클라이언트 에 의해 생성되고 공통 시간 참조을 결정하는 데 사용되는 기본 비디오 스트림에 상술된 유형과 같은 시각적 또는 청각적 코드를 도입할 수 있다. 또 다른 예에서, 각 참가자 클라이언트는 상술된 것과 일치하는 방식으로, 모든 참가자 클라이언트에 의해 비디오 통신 서비스에서 볼 수 있는 공통 비디오 스트림에서 이미지 감지를 수행하고 이러한 이미지 감지의 결과를 중앙 서버에 중계할 수 있으므로, 시간이 지남에 따라 각 참가자 클라이언트의 서로에 대한 각각의 오프셋을 결정하는 데 사용된다. 이런 식으로 공통 시간 참조가 개별 상대 오프셋 세트로 결 정될 수 있다. 예를 들어, 일반적으로 사용 가능한 비디오 스트림의 선택된 참조 픽셀은 여러 또는 모든 참가자 클라이언트(예를 들어, 로컬 소프트웨어 기능)에 의해 모니터링될 수 있으며, 해당 픽셀의 현재 색상 은 중앙 서버에 전달될 수 있다. 중앙 서버는 다수의 (또는 모든) 참가자 클라이언트 각각으로 부터 연속적으로 수신된 이러한 색상 값을 기반으로 각각의 시간 시리즈를 계산하고, 상호 상관을 수행하여 서 로 다른 참가자 클라이언트에 대한 상대적 시간 오프셋의 추정 세트를 생성한다. 실제로, 비디오 통신 서비스에 공급되는 출력 비디오 스트림은 해당 비디오 통신의 모든 참가자 클라 이언트에 대한 공유 화면의 일부로서 포함될 수 있고, 이에 따라 참가자 클라이언트와 관련된 시간 오프셋 을 평가하는 데 사용될 수 있다. 특히, 비디오 통신 서비스에 공급되는 출력 비디오 스트림은 자동 참가자 클라이언트 및/또는 API를 통해 중앙 서버에 다시 이용 가능해질 수 있다. 일부 실시 예에서, 공통 시간 기준은 상기 기본 디지털 비디오 스트림(210, 301) 중 제1 것의 오디오 부분 과 상기 기본 디지털 비디오 스트림(210, 301)의 이미지 부분 사이의 감지된 불일치에 적어도 부분적 으로 기초하여 결정될 수 있다. 이러한 불일치는 예를 들어 해당 상기 제1 기본 디지털 비디오 스트림(210, 301)에서 보이는 말하는 참가자의 디지털 립싱크 비디오 이미지 분석에 기초할 수 있다. 이러한 립싱크 분 석은 그 자체로 기존 방식이며, 예를 들어 훈련된 신경망을 사용할 수 있다. 분석은 이용 가능한 공통 오디오 정보와 관련하여 각 기본 비디오 스트림(210, 301)에 대해 동기화 기능에 의해 수행될 수 있고, 개별 기본 비디오 스트림(210, 301)에 걸친 상대적인 오프셋은 이 정보에 기초하여 결정될 수 있다. 일부 실시 예에서, 동기화 단계 S503는 의도적으로 최대 30초, 예를 들어 최대 5초, 최대 1초, 최대 0.5초이지 만, 0초보다 긴 지연(이 컨텍스트에서 용어 \"지연\"과 \"대기 시간\"은 동일한 의미임)을 도입하는 것으로 구성되 므로, 출력 디지털 비디오 스트림에는 적어도 상기 지연이 제공된다. 여하튼, 의도적으로 도입된 지연은 수집 단계 S501에서 리샘플링 후에 저장된 프레임(또는 개별 이미지)의 수와 같이, 적어도 몇개의 비디오 프레 임, 예를 들어, 적어도 3개, 심지어는 적어도 5개 또는 심지어 10개의 비디오 프레임이다. 본 명세서에 사용된 바와 같이, \"의도적으로\"라는 용어는 동기화 문제 등을 기반으로 이러한 지연을 도입할 필요성과 관계없이 지연 이 도입되는 것을 의미한다. 다시 말해서, 의도적으로 도입된 지연은 하나를 다른 것에 대해 시간 동기화하기 위해서 기본 비디오 스트림(210, 301)의 동기화의 일부로 도입된 지연에 추가로 도입된다. 의도적으로 도입된 지연은 공통 시간 기준과 관련하여 미리 결정되거나 고정되거나 가변적일 수 있다. 지연 시간은 기본 비디 오 스트림(210, 301) 중 최소 잠재 스트림과 관련하여 측정될 수 있으므로, 상기 시간 동기화의 결과로 이들 스 트림(210, 301) 중 더 많은 잠재적인 것이 상대적으로 더 작은 의도적으로 추가된 지연과 연관된다. 일부 실시 예에서, 0.5초 이하와 같이 상대적으로 작은 지연이 발생한다. 이러한 지연은 출력 비디오 스트림 을 사용하는 비디오 통신 서비스에 대한 참가자가 거의 인지할 수 없을 것이다. 다른 실시 예에서, 예를 들어 출력 비디오 스트림이 대화형 컨텍스트에서 사용되지 않고 대신 외부 소비자에 대한 단방 향 통신으로 게시되는 경우, 더 큰 지연이 도입될 수 있다. 이러한 의도적으로 도입된 지연은 동기화 기능이 수집된 개별 기본 스트림(210, 301) 비디오 프레임을 올 바른 공통 시간 기준 타임스탬프에 매핑하는 데 충분한 시간을 달성하기에 충분할 수 있다. 또한 손 실된 기본 스트림(210, 301) 신호, 슬라이드 변경, 해상도 변경 등을 감지하기 위해서, 상술된 이벤트 감지를 수행하는 데 충분한 시간을 허용하기에 충분할 수 있다. 더욱이, 의도적으로 상기 지연을 도입하는 것은 다음에 설명되는 바와 같이 개선된 패턴 감지 기능을 허용하기에 충분할 수 있다. 상기 지연의 도입은 해당 버퍼링된 프레임을 사용하여 출력 비디오 스트림을 게시하기 전에 수집되고 시간 동기화된 기본 비디오 스트림(210, 301) 각각을 버퍼링하는 것을 포함할 수 있다는 것이 인식된다. 다시 말해서, 기본 비디오 스트림(210, 301) 중 적어도 하나, 여러 개 또는 심지어 모두의 비디오 및/또는 오디 오 데이터는 캐시와 마찬가지로 버퍼링된 방식으로 중앙 서버에 존재할 수 있지만 다양한 대역폭 상황을 처리할 수 있는 의도로 (기존 캐시 버퍼와 같이) 사용되지는 않고 상기 이유로 특히 패턴 감지 기능에 의 해 사용된다. 일부 실시 예에서, 상기 패턴 감지 단계 S504는 기본 디지털 비디오 스트림(210, 301) 중 적어도 하나, 예를 들 어 여러 개, 적어도 4개 또는 심지어 전부의 특정 정보를 고려하는 것을 포함하고, 특정 정보는 아직 출력 디지 털 비디오 스트림의 생성에 사용될 시간 동기화된 기본 디지털 비디오 스트림의 프레임보다 나중 프 레임에 존재한다. 따라서 새로 추가된 프레임은 출력 비디오 스트림의 일부(또는 기초)를 형성 하기 전에 특정 대기 시간 동안 해당 버퍼에 존재할 것이다. 이 기간 동안, 해당 프레임에 있는 정보 는 출력 비디오 스트림의 현재 프레임을 생성하기 위해 현재 사용되는 프레임과 관련된 \"미래\"의 정보를 구성할 것이다. 출력 비디오 스트림 타임라인이 해당 프레임에 도달하면, 이는 출력 비디오 스트림 의 해당 프레임을 생성하는 데 사용되고 그 후에 폐기될 수 있다. 다시 말해, 패턴 감지 기능은 출력 비디오 스트림을 생성하기 위해 아직 사용되지 않은 비디오/오디 오 프레임 세트를 처리할 수 있고, 이 데이터를 사용하여 상기 패턴을 감지할 수 있다. 패턴 감지는 다양한 방식으로 수행될 수 있다. AI 구성요소(134a)에 의해 수행되는 일부 실시 예에서, 패턴 감 지 단계 S504는 상기 패턴을 자동으로 감지하기 위해서 제2 훈련된 신경망 또는 상기 기본 디지털 비디오 스트림(120, 301) 중 적어도 두 개, 예를 들어, 적어도 3개, 적어도 4개 또는 심지어 모두를 분석하는 다른 기 계 학습 구성요소를 포함한다. 일부 실시 예에서, 감지된 패턴은 각각의 참가자 클라이언트와 각각 연관된 적어도 2명, 예를 들어 적어도 3명, 적어도 4명의 서로 다른 발언 참가자를 공유 비디오 통신 서비스에 연루시키는 말하기 패턴을 포함하고, 상기 발언 참가자 각각은 가능하게 상기 주요 디지털 비디오 스트림(210, 301) 각각의 것에서 시각적으로 보여진다. 생성 단계 S505는 바람직하게는 출력 비디오 스트림의 현재 생성 상태를 결정하고, 추적하고, 업데이트하 는 것을 포함한다. 예를 들어, 이러한 상태는 참가자가 출력 비디오 스트림에서 볼 수 있는 것과 화 면의 어디에 있는지; 외부 비디오 스트림이 출력 비디오 스트림에서 볼 수 있는 것과 화면의 어디에 있는지; 슬라이드나 공유 화면이 전체 화면 모드로 라이브 비디오 스트림과 함께 표시되는지 등을 지시할 수 있 다. 더욱이, 이러한 상태는 주요 디지털 비디오 스트림(210, 301) 중 어느 하나에 대한 자르기 또는 가상적 패 닝/줌잉을 언제든지 사용될 수 있도록 지시할 수 있다. 그러므로, 생성 기능은 생성된 출력 비디오 스트림 에 관한 상태 머신으로 볼 수 있다. 예를 들어 최종 소비자가 볼 수 있는 결합된 비디오 경험으로 출력 비디오 스트림을 생성하려면, 개 별 기본 비디오 스트림(210, 301)과 관련된 개별 이벤트를 단순히 감지하는 것보다 더 깊은 수준에서 일어나는 것을 중앙 서버가 이해할 수 있는 것이 유리하다. 제1 예에서, 발표하는 참가자 클라이언트는 현재 보고 있는 슬라이드를 변경하고 있다. 이 슬라이드 변화 는 전술한 바와 같이 이벤트 감지 기능에 의해 감지되고, 메타데이터는 슬라이드 변화를 나타내는 해 당 프레임에 추가된다. 이는 프레젠테이션 참가자 클라이언트가 빠르게 연속해서 앞으로 여러 슬라이드를 건너뛰는 것으로 밝혀졌기 때문에 여러 번 발생하고, 결과적으로 일련의 \"슬라이드 변경\" 이벤트가 이벤트 감지 기능에 의해 감지되고 해당 기본 비디오 스트림에 대한 개별 버퍼에 해당 메타데이터와 함 께 저장된다. 실제로 이렇게 빠르게 앞으로 건너뛴 슬라이드는 단 몇 분의 1초 동안만 표시될 수 있다. 이들 감지된 슬라이드 변화 중 몇 개에 걸쳐 있는, 해당 버퍼에 있는 정보를 살펴보는 패턴 감지 기능 은 다수의 또는 빠르게 수행되는 슬라이드 변경보다는, 하나의 단일 슬라이드 변경에 해당하는 패턴을 감지하는 것이다(즉, 앞으로 건너뛰기의 마지막 슬라이드까지, 빠른 건너뛰기가 끝나면 슬라이드가 여전히 보임). 다시 말해서, 패턴 감지 기능은 예를 들어 매우 짧은 시간 내에 10개의 슬라이드 변경이 있음을 알 수 있 으며, 이들은 단일 슬라이드 변경을 나타내는 감지된 패턴으로 처리된다. 결과적으로, 패턴 감지 기능에 의해 감지된 패턴에 액세스하는 생성 기능은 몇 초 동안 출력 비디오 스트림에서 전체 화면 모드로 최종 슬라이드를 표시하도록 선택할 수 있는데, 왜냐하면 이 슬라이드가 상기 상태 머신에서 잠재적으로 중요하 다고 판단하기 때문이다. 또한 출력 스트림에서 중간에 본 슬라이드를 전혀 표시하지 않도록 선택할 수도 있다. 여러 개의 빠르게 변화하는 슬라이드의 패턴 탐지는 간단한 규칙 기반 알고리즘에 의해 감지될 수 있지만, 분류 에 따라 움직이는 이미지에서 이러한 패턴을 감지하도록 설계되어 훈련된 훈련 신경망을 사용하여 대안적으로 감지할 수 있다. 비디오 통신이 토크쇼, 패널 토론 또는 이와 유사한 경우에 유용할 수 있는 다른 예에서, 조용하고 부드러운 출 력 비디오 스트림을 생성하고 게시함으로써, 현재 화자와 여전히 소비자에게 적절한 시청 경험을 제 공하는 것 사이에서 시각적 주의를 빠르게 전환하는 것이 바람직할 수 있다. 이 경우, 이벤트 감지 기능은 특정 기본 비디오 스트림(210, 301)에서 보여지는 사람이 현재 말하고 있는지를 항상 결정하기 위해서 각 기본 비디오 스트림(210, 301)을 지속적으로 분석할 수 있다. 이는 예를 들어, 그 자체로 종래의 이미지 처리 도구를 사용하여, 상술된 바와 같이 수행될 수 있다. 다음에, 패턴 감지 기능은 상기 주요 비디오 스트림(210, 301) 중 몇몇을 포함하여, 특정 전체 패턴을 감지하도록 동작 가능할 수 있고, 상기 패턴은 원활한 출력 비디오 스트림을 생성하는 데 유용하다. 예를 들어, 패턴 감지 기능은 현재 화자 사이의 매우 빈번한 전환 패턴 및/또는 여러 동시 화자와 관련된 패턴을 감지할 수 있다. 그러면, 생성 기능은 예를 들어 0.5초 동안만 말하고 다시 침묵하는 화자에게 시각적 초점을 자동으로 전 환하지 않거나, 두 사람이 번갈아 가면서 또는 동시에 말하는 경우 일정 시간 동안 여러 명의 화자가 나란히 표 시되는 상태로 전환하기 위해서, 상기 생성 상태와 관련하여 자동화된 결정을 내릴 때 이러한 감지된 패턴을 고 려할 수 있다. 이러한 상태 결정 프로세스는 시계열 패턴 인식 기술을 사용하거나 훈련된 신경망을 사용하여 자 체적으로 수행될 수 있다. 그러나 미리 결정된 규칙 세트에 적어도 부분적으로 기초할 수도 있다. 일부 실시 예에서, 병렬로 감지되어 생성 기능 상태 머신에 대한 입력을 형성하는 여러 패턴이 있을 수 있 다. 이러한 다중 패턴은 생성 기능에 의해 다양한 AI 구성 요소, 컴퓨터 비전 감지 알고리즘 등에 의해 사 용될 수 있다. 일 예로서, 일부 참가자 클라이언트의 불안정한 연결을 동시에 감지하는 동시에 영구적인 슬라이드 변경을 감지할 수 있는 반면, 다른 패턴은 현재 주요 발언 참가자를 감지한다. 이러한 사용 가능 한 모든 패턴 데이터를 사용하여, 그러한 패턴 데이터의 시계열 분석을 위해, 분류기 신경망이 훈련될 수 있고/ 있거나 규칙 세트가 개발될 수 있다. 이러한 분류는 적어도 부분적으로, 예를 들어 완전히 감독되어 결과적으로 결정된 원하는 상태 변경이 상기 생성에 사용된다. 예를 들어, 다양하고 다른 생성 스타일 및 요구에 따라 출력 비디오 스트림을 자동으로 생성하도록 구체적으로 배열된 서로 다른 미리 결정된 분류기가 생성될 수 있다. 훈련은 원하는 출력으로 알려진 생성 상태 변경 시퀀스와 훈련 데이터로 알려진 패턴 시계열 데이터를 기 반으로 할 수 있다. 일부 실시 예에서, 베이지안 모델을 사용하여 이러한 분류기를 생성할 수 있다. 구체적인 예로서, 정보는 숙련된 생성자로부터 선험적으로 수집되어 \"토크쇼에서 나는 화자 A에서 화자 B로 바로 전환하 지 않고 다른 화자가 매우 우세하여 큰 소리로 말하지 않는 한 항상 다른 화자에 집중하기 전에 먼저 개요를 보 여준다.\"과 같은 입력을 제공할 수 있다. 이 생성 논리는 일반 형식 \"X가 참이면 | Y가 참이라는 사실을 고려하 여 | Z를 수행한다\"의 베이지안 모델로 표현된다. (누군가 큰 소리로 말하고 있는지 등에 대한) 실제 감지는 분 류기 또는 임계값 기반 규칙을 사용하여 수행될 수 있다. (패턴 시계열 데이터의) 대규모 데이터 세트의 경우, 딥 러닝 방법을 사용하면 비디오 스트림의 자동화된 생성 에 사용할 정확하고 매력적인 생성 형식을 개발할 수 있다. 일부 실시 예에서, 생성 기능은 출력 비디오 스트림에서 어떤 객체 또는 인간 참가자를 보여줄 것인 지에 대한 정보를 포함할 수 있다. 예를 들어, 특정 설정에서 하나나 여러 참가자는 출력 비디오 스트림 의 소비자에게 보여지기를 원하지 않거나 허용하지 않을 수 있다. 그런 다음, 생성 기능은 해당 객체 또는 인간을 인식하기 위한 디지털 이미지 처리 기술에 기반하여, 하나나 여러 객체 또는 인간 참가자를 표시하 지 않도록 출력 비디오 스트림을 생성할 수 있고 해당 객체 또는 인간이 출력 비디오 스트림에 부분 으로 추가되기 전에 포함되지 않도록 기본 비디오 스트림(210, 301)을 자동으로 잘라내거나; 현재 해당 객체 또 는 인간을 보여주는 기본 비디오 스트림(210, 301)을 출력 비디오 스트림에 포함하지 않도록 하는 생성 결정을 내릴 수 있다. 일부 실시 예에서, 생성 기능은 하나 또는 여러 개의 상기 기본 디지털 비디오 스트림(210, 301)의 감지된 패턴에 대한 응답으로, 기본 디지털 비디오 스트림 또는 다른 유형의 외부 제공 데이터와 같은 형태로 외 부에서 제공된 정보를 도입하도록 구성될 수 있다. 예를 들어, 생성 기능은 상기 기본 비디오 스트림(210, 301)에 포함된 이미지 및/또는 사운드의 디지털 처리를 통해 토론 주제 또는 (사전 결정된 트리거 구문과 같은) 사전 결정된 트리거 이벤트 또는 패턴을 자동으로 감지하도록 구성될 수 있다. 구체적인 예에서, 이는 참가 사 용자가 현재 토론하고 있는 주제에 대한 원격 소스에서 업데이트된 텍스트 또는 차트 정보를 출력 비디오 스트림에 자동으로 도입하는 것을 포함할 수 있다. 일반적으로, 이러한 트리거 이벤트 또는 패턴의 감지에 의하면 생성 기능을 감지된 트리거 이벤트 또는 패턴의 유형 또는 특성의 기능으로 현재 사용되는 생성 상 태를 어떤 식으로든 수정할 수 있다."}
{"patent_id": "10-2025-7006373", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "요약하자면, 개별적 기본 비디오 스트림(210, 301); 의도적으로 도입된 지연; 여러 시간 동기화된 기본 비디오 스트림(210, 301) 및 감지된 이벤트에 기초한 패턴 감지; 감지된 패턴을 기반으로 생성 공정에 기초한 이벤트 감지의 조합을 사용하게 되면 다양한 취향과 스타일 선택에 따라 출력 디지털 비디오 스트림의 자동화된 생성을 달성하는 것을 가능하게 한다. 이 결과는 이벤트 감지 기능, 패턴 감지 기능 및 생성 기능 에 의해 사용되는 광범위한 가능한 신경망 및/또는 규칙 기반 분석 기술에 걸쳐 유효하다. 상기 예시한 바와 같이, 생성 단계 S505는 상기 출력 디지털 비디오 스트림에 있는 상기 기본 디지털 비디 오 스트림(210, 301) 중 개별적인 가시성에 관한 사전 결정된 및/또는 동적으로 가변적인 매개변수 세트; 시각 적 및/또는 청각적 비디오 콘텐츠 배열; 시각 또는 청각 효과 사용; 및/또는 출력 디지털 비디오 스트림의 출력 모드에 기초하여 출력 디지털 비디오 스트림을 생성하는 것을 포함할 수 있다. 이러한 매개변수는 상 기 생성 기능 상태 기계에 의해 자동으로 결정될 수 있고 및/또는 이 생성을 제어하는 조작자에 의해 설정 될 수 있고(반자동으로 만듬) 및/또는 특정한 선험적 구성 요구(예를 들어 출력 비디오 스트림 레이아웃 변경 또는 위에서 예시된 유형의 상태 변경 사이의 최단 시간)에 기초하여 미리 결정될 수 있다. 실제적인 예에서는, 상태 머신은 출력 비디오 스트림에 적용될 수 있는 미리 결정된 표준 레이아웃 세트, 예를 들어, 전체 화면 발표자 보기(현재 말하고 있는 참가자를 전체 화면으로 표시함); 슬라이드 보기(현 재 공유된 프레젠테이션 슬라이드를 전체 화면으로 표시함); 현재 말하고 있는 참가자와 현재 공유된 프리 젠테이션 슬라이드를 나란히 표시하는 \"나비 보기\"; 참가자의 전체 또는 선택된 하위 집합을 나란히 또는 매트릭스 레이아웃으로 보여주는 다중 화자 보기; 등을 지원할 수 있다. 다양한 이용 가능한 생성 포맷은 이용 가능한 상태 세트(에를 들어 상기 표준 레이아웃 세트)와 함께 (위에 예시된 바와 같이) 상태 머신 상태 변경 규칙 세트에 의해 정의될 수 있다. 예를 들어, 이러한 생성 형식 중 하나는 \"패널 토론\", 또 다른 \"발표\" 등일 수 있다. GUI 또는 중앙 서버에 대한 다른 인터페이스를 통해 특정 생성 형식을 선택함으로써, 시스템 의 운영자는 미리 정의된 생성 형식 세트 중 하나를 신속하게 선택한 다음에, 중앙 서버가 위에서 설 명된 이용 가능한 정보에 기초하여 해당 생성 포맷에 따라 출력 비디오 스트림을 완전 자동으로 생성하도 록 허용한다. 게다가 생성 과정에서, 상술한 바와 같이, 각각의 미팅 참가자 클라이언트 또는 외부 비디오 소스에 대해 각각의 인메모리 버퍼가 생성되고 유지될 수 있다. 이러한 버퍼는 즉시 쉽게 제거, 추가 및 변경할 수 있 다. 그러면 중앙 서버는 추가/탈퇴된 참가자 클라이언트 및 연설 예정인 참가자; 프레젠테이션 의 계획된 또는 예상치 못한 일시 중지/재개; 현재 사용되는 생성 형식에 대한 원하는 변경 사항 등과 관련하여, 출력 비디오 스트림을 생성하는 동안 정보를 수신하도록 배열될 수 있다. 이러한 정보는 예를 들어, 전술한 바와 같이 운영자 GUI 또는 인터페이스를 통해 중앙 서버에 공급될 수 있다. 위에 예시된 바와 같이, 일부 실시 예에서 주요 디지털 비디오 스트림(210, 301) 중 적어도 하나가 디지털 비디 오 통신 서비스에 제공되고, 게시 단계 S506는 상기 출력 디지털 비디오 스트림을 동일한 통신 서비 스에 제공하는 단계를 포함할 수 있다. 예를 들어, 출력 비디오 스트림은 비디오 통신 서비스의 참가자 클라이언트에 제공될 수 있거나, API를 통해 외부 비디오 스트림으로서 비디오 통신 서비스 에 제공될 수 있다. 이러한 방식으로, 출력 비디오 스트림은 현재 비디오 통신 서비스에 의해 달성되고 있는 비디오 통신 이벤트에 대한 일부 또는 모든 참가자에게 이용 가능하게 될 수 있다. 또한 상술된 바와 같이, 추가적으로 또는 대안적으로 출력 비디오 스트림은 하나 또는 여러 외부 소비자 에게 제공될 수 있다.일반적으로, 생성 단계 S505는 중앙 서버에 의해 수행될 수 있으며, API를 통해 라이브 비디오 스트 림으로서 하나 이상의 동시 소비자에게 상기 출력 디지털 비디오 스트림을 제공할 수 있다. 도 8은 본 발명의 제1 측면에 따른, 출력 디지털 비디오 스트림을 제공하는 방법을 도시하고 있으며, 이 방법은 위에서 설명한 내용을 참조하여 이하에서 설명될 것이다. 따라서 도 8에 도시된 방법에서, 디지털 비디오 스트 림 수집, 이벤트 감지, 동기화, 패턴 감지, 생성 및 게시와 관련하여 위에서 설명한 모든 메커니즘 및 원리가 적용될 수 있다. 또한, 도 9 및 도 10은 각각 도 8에 도시된 방법을 수행하도록 구성된 시스템의 개략도이다. 도 9에는, 세 개의 서로 다른 중앙 서버(130', 130\", 130\"')가 도시되어 있다. 이러한 중앙 서버(130', 130\", 130'\")는 상술된 유형의 하나의 단일 통합 중앙 서버일 수도 있거나, 별도의 그러한 중앙 서버일 수도 있다. 이 들은 동일한 물리적 또는 가상 하드웨어에서 실행될 수도 있고 그렇지 않을 수도 있다. 어쨌든, 이들은 서로 통 신하도록 배열된다. 일부 실시 예에서, 중앙 서버(130' 및 130\")는 예를 들어 본질적으로 기존 컴퓨터 장치와 같은 개별적 하드웨어 어플라이언스의 형태로 하나의 동일한 물리적 하드웨어(도 9에서 점선 사각형으로 표시)에서 실행되도록 배열될 수 있다. 일부 실시 예에서, 이러한 개별적 하드웨어 어플라이언스는 회의실에 배치되거나 회의실 과 물리적으로 연결되어 있으며, 특히 해당 회의실에서 디지털 비디오 회의를 진행하도록 배치된 컴퓨터 장치 (402')(도 10 참조)이다. 다른 실시 예에서, 개별적인 하드웨어 어플라이언스는 개별 인간 회의 참가자(122, 122\", 122\"')에 의해 해당 디지털 비디오 회의에 사용되는, 랩톱 컴퓨터와 같은 개인용 컴퓨터(402\", 402\"')이 고, 참가자(122, 122\", 122\"')는 해당 회의실에 있거나 원격으로 존재한다. 각 중앙 서버(130', 130\", 130\"')는 일반적으로 상술한 바와 같을 수 있는 각각의 수집 기능(131', 131\", 131\"')를 포함한다. 수집 기능(131')은 일반적으로 위에서 설명한 유형의 비디오 카메라와 같은 디지털 카 메라로부터 디지털 비디오 스트림을 수집하도록 배열된다. 이러한 디지털 카메라는 해당 개별 하드웨어 어 플라이언스의 통합 부분이거나 적절한 유선 또는 무선 디지털 통신 채널을 사용하여 하드웨어 어플라이언 스에 연결된 별도의 카메라일 수 있다. 어쨌든 카메라는 하드웨어 어플라이언스와 관련하여 로컬하게 배열되는 것이 바람직한다. 각 수집 기능(131\", 131\"')은 해당 디지털 카메라 또는 수집 기능(131')에서 직접 디지털 비디오 스트림에 해당하는 디지털 비디오 신호를 수집할 수 있다. 각 중앙 서버(130', 130\", 130\"')는 또한 각각의 생성 기능(135', 135\", 135\"')을 포함할 수 있다. 이러한 각 생성 기능(135', 135\", 135\"')은 위에서 설명한 생성 기능에 해당하며, 생성 기능과 관련하여 위에서 언급한 내용은 생성 기능(135', 135\" 및 135\"')에도 동일하게 적용된다. 중앙 서버(130', 130\", 130\"')의 세부 구성에 따라 3개 이상의 생성 기능이 있을 수도 있다. 생성 기능(135', 135\", 135\"')와 다른 엔터티 간의 다양 한 디지털 통신은 적절한 API를 통해 이루어질 수 있다. 또한, 각 중앙 서버(130', 130\", 130\"')는 해당 게시 기능(136', 136\", 136\"')을 포함할 수 있다. 각각의 이러 한 게시 기능(136', 136\", 136\"')은 상술된 게시 기능에 대응하고, 게시 기능과 관련하여 위에서 언 급한 내용은 게시 기능(136', 136\", 136\"')에도 동일하게 적용된다. 게시 기능(136', 136\", 136\"')은 중앙 서 버(130', 130\", 130'\")의 세부 구성에 따라 여러 기능을 가진 하나의 단일 논리적 기능으로 구별되거나 공동 배 열될 수 있으며, 세 개 이상의 게시 기능이 있을 수도 있다. 게시 기능(136', 136\", 136\"')은 경우에 따라 하나 의 동일한 게시 기능의 다른 기능적 측면일 수 있다. 반면, 게시 기능(136\" 및 136\"')은 선택 사항이며, 게시 기능(136')에 의해 출력된 비디오 스트림과 다른(아마 도 더 정교하고, 각각의 시간 지연과 연관됨) 비디오 스트림을 출력하도록 구성될 수 있지만, 게시 기능(136') 은 본 발명에 따라 출력 디지털 비디오 스트림을 출력하도록 구성된다. 일반적으로 말하면, 생성 기능(135\" 및 135\"')은 들어오는 비디오 스트림을 처리하여 생성 기능(135')에 의해 사용될 생성 제어 매개변수를 생성하여 본 발명에 따라 해당 출력 비디오 스트림을 생성하도록 구성된다. 도 9는 또한 각각 상술된 외부 소비자에 대응하는 3개의 외부 소비자(150', 150\", 150\"')를 도시한다. 이 러한 외부 소비자(150', 150\", 150\"')는 3개 미만일 수도 있고 3개 초과일 수도 있음을 알 수 있다. 예를 들어, 두 개 이상의 게시 기능(136', 136\", 136\"')이 하나의 동일한 외부 소비자(150', 150\", 150\"')에게 출력할 수 있고, 게시 기능(136', 136\", 136\"') 중 하나가 두 개 이상의 해당 외부 소비자(150', 150\", 150\"')에게 출력할 수 있다. 또한, 최소한 게시 기능(136')이 생성된 비디오 스트림을 수집 기능(131')에 다시 게시할 수 있음 에 유의한다. 또한, 각각의 게시 기능(136', 136\", 136\"')는 해당 생성된 비디오 스트림을 위에서 논의한 일반 유형의 참가자 클라이언트에 게시하도록 구성될 수 있다. 소비자(150')는 중앙 서버(130')을 또한 포함하는 참가자 클라이언트일 수 있으며, 예를 들어, 랩탑 컴퓨 터(402\", 402\"')가 중앙 서버(130')(및 아마도 중앙 서버(130\"))의 기능으로 구성되고, 해당 인간 사용자(122\", 122\"')에게 인간 사용자(122\", 122\"')가 참가하는 비디오 통신 서비스의 일부로서 상기 랩탑 컴퓨터(402\", 402\"')의 화면에 향상된 실시간 출력 비디오 스트림을 제공할 수 있다. 또한, 도 9는 세 개의 외부 정보 소스(300', 300\", 300\"')를 도시하는데, 각각은 위에서 설명한 외부 정보 소스 에 대응하고 해당 수집 기능(131', 131\", 131\"') 중 각각의 것에 정보를 제공한다. 이러한 외부 정보 소스 (300', 300\", 300\"')가 세 개 미만일 수도 있고, 세 개 초과일 수도 있다는 것을 알 수 있다. 예를 들어, 이러 한 외부 정보 소스(300', 300\", 300\"') 중 하나가 하나 이상의 수집 기능(131', 131\", 131\"')에 공급될 수 있 고; 각 수집 기능(131', 131\", 131\"')은 하나 이상의 외부 정보 소스(300', 300\", 300\"')로부터 공급될 수 있 다. 도 9는 단순하게 하기 위해 비디오 통신 서비스을 도시하지 않았지만, 상술한 일반 유형의 비디오 통신 서 비스는 상술된 방식으로 중앙 서버(130', 130\", 130\"')를 사용하여 참가자 클라이언트에 공유 비디오 통신 서비스를 제공하는 것과 같이, 중앙 서버(130', 130\", 130\"')와 함께 사용될 수 있다는 것을 알 수 있다. 일부 실시 예에서, 중앙 서버(130\"')는 비디오 통신 서비스을 구성하거나, 포함하거나, 포함된다. 도 10은 세 가지 다른 예시의 하드웨어 어플라이언스, 즉 회의 장소, 방 또는 행사장에서 한 명 이상의 인간 회 의 참가자(122\", 122\"', 122\"\")를 보여주는 이미지를 캡처하도록 배열된 디지털 카메라(401')를 포함하는 회의 실 하드웨어 어플라이언스(402'); 및 각각이 해당 랩톱 컴퓨터(402\"., 402\"')를 사용하여 상기 인간 회의 참가 자(122\", 122\"')의 각각을 보여주는 이미지를 캡처하도록 배열된 디지털 웹 카메라(401\", 401\"')를 각각 포함하 는 두 개의 랩톱 컴퓨터(402\", 402\"')를 도시한다. 도 10은 본 발명의 원리를 설명하는 목적으로 여러 가지 다 른 구성 중 하나를 보여주고 있으며, 다른 유형의 구성도 가능하다는 것이 이해되어야 한다. 예를 들어, 본 명 세서에서 예시된 바와 같이 일부 참가자(122\", 122\"', 122\"\")만 카메라(401')에 보일 수 있고; 추가 참가 사용 자(도 10에 표시되지 않음)는 원격으로 비디오 통신 서비스에 참가할 수 있고; 외부 정보 소스가 사용될 수 있 는 등이다. 본 명세서에서 사용된 용어 \"원격으로\"는 \"로컬\"을 의미하지 않는다. 서로에 대한 관계에서 \"원격으 로\" 배열된 두 개체는 개방형 인터넷(WAN)을 통해 통신하도록 배열하는 것이 바람직하다. 참가 사용자(122\"\") 중 하나의 예시는 카메라(410\", 401\"')에는 보이지 않지만, 카메라(401')에서만 보인다. 각 하드웨어 장치(402', 402\", 402\"')는 도 9에서 더 자세히 표시된 장치에 해당할 수 있으며, 각 하드웨 어 장치(402', 402\", 402\"')는 인터넷 또는 다른 디지털 통신 네트워크를 통해 비디오 통신 서비스과 통신하도록 구성될 수 있다. 비디오 통신 서비스은 위에서 설명한 일반 유형의 참가 사용자로서 장치 (402', 402\", 402\"')를 사용하여 공유 비디오 통신 서비스를 제공할 수 있다. 일부 실시 예에서, 중앙 서버 (130\"')와 같은 비디오 통신 서비스는 중앙 서버(130', 130\")와 관련하여 원격이다. 도 8로 다시 돌아가, 제1 단계 S800에서 방법이 시작된다. 후속 수집 단계 S801에서, 하나 또는 여러 개의 실시간 제1 기본 디지털 비디오 스트림(210, 301)이 지속적으로 수집된다. 도 9 및 10에 도시된 경우, 비디오 스트림(210, 301)은 수집 기능(131')(및/또는 외부 정보 소스 의 수집의 경우 131\", 131\"')에 의해 카메라(401', 401\", 401\"') 중 하나에서 지속적으로 수집되는, 스트 림이다. 제1 기본 디지털 비디오 스트림이 \"실시간\" 스트림이라는 것은 캡처링 카메라에서 해당 수집 기능로 지연 없이, 그리고 수집 기능에 도달하기 전에 시간 소모적인 이미지 처리 없이 제공된다는 것을 의미한다. 예 를 들어, 카메라 센서의 이미지 프레임 캡처링과 해당 디지털 비디오 스트림이 수집 기능에 저장될 때까지 의 모든 데이터 처리 및/또는 통신은 0.1초 미만, 예를 들어 0.05초 미만일 수 있다. 상술된 바와 같이, 제1 기본 디지털 비디오 스트림(210, 301)은 장치(402, 402', 402', 402\") 자체와 같은, 출 력 비디오 스트림을 소비하는 참가자 클라이언트와 관련하여 로컬 배치된 카메라(410', 401\", 401\"')에 의 해 지속적으로 캡처될 수 있다. 또한, 제1 기본 디지털 비디오 스트림(210, 301)은 해당 장치(402\", 402\"')(참가자 클라이언트)의 참가 사 용자(122\", 122\"')를 보여주는 이미지를 캡처하도록 배치된 카메라(401', 401\", 401\"')에 의해 지속적으로 캡처 될 수 있다. 이러한 경우 및 기타 경우, 제1 기본 디지털 비디오 스트림(210, 301)은 제1 생성 제어 매개변수(아래 참조)의 적용을 수행하는 컴퓨터 장치(402', 402\", 402\"')와 관련하여 물리적으로 로컬(위에서 정의한 대로 \"로컬\") 배 열된, 카메라(401', 401\", 401\"')에 의해 지속적으로 캡처될 수 있다. 후속 생성 단계 S804에서, 수집된 제1 기본 디지털 비디오 스트림 또는 스트림(210, 301) 중 적어도 하나에 대 해 제1 디지털 이미지 분석이 수행된다. 이 디지털 이미지 분석의 결과, 상술된 일반 유형의 적어도 하나의 제1 이벤트 또는 패턴가 해당 제1 기본 디지털 비디오 스트림(210, 301)에서 감지된다. 특히, 이 제1 디 지털 이미지 분석의 결과, 후속 단계 S805에서, 상기 제1 이벤트 또는 패턴의 감지를 기반으로, 제1 생성 제어 매개변수가 설정된다. 디지털 이미지 분석 자체는 해당 기술 분야에서 자체적으로 잘 알려진 것과 같은 적절한 방식으로 수행될 수 있 다. 해당 이벤트 또는 패턴의 감지는 다른 목적을 가질 수 있다. 일반적으로, 비디오가 비디오 통신 서비 스의 참가 사용자에게 표시되는 것에 영향을 미치는 것이 바람직할 수 있다. 이러한 영향은 캡처된 기본 디지털 비디오 스트림의 가상적 자르기 및/또는 패닝 및/또는 줌잉 및/또는 틸팅을 동적으로 선택하여 해 당 기본 디지털 비디오 스트림에 표시된 물리적 객체의 참가자 사용자를 하이라이트하거나 뒤따르고 및/또 는 기본 디지털 비디오 스트림에 표시되고 현재 논의 중인 현재 화자 또는 물리적 객체를 하이라이트하는 것을 포함할 수 있다. 이러한 영향을 미치는 것은 또한 예를 들어, 회의장의 오디오를 흡수하여 자연어 해석을 포함 한 디지털 사운드 처리 단계를 사용하여 식별한 것과 같이, 해당 기본 비디오 스트림에 표시된 참가자 사용자 가 현재 논의하고 있는 내용을 기반으로 하여, 기본 디지털 비디오 스트림에 메타데이터 또는 외부에서 제 공된 정보와 같은 추가 정보를 추가하는 것을 포함할 수 있다. 따라서, 제1 생성 제어 매개변수는 제1 기본 디지털 비디오 스트림(210, 301)에서 고정 또는 이동하는 객체 또 는 참가 사용자에 대한 시각적 위치 또는 시각적 추적 정보를 포함할 수 있으며, 위치 또는 추적 정보는 디지털 이미지 처리를 사용하여 자동으로 감지된다. 따라서 이 경우 감지된 이벤트 또는 패턴는 기본 디지털 비디오 스트림(210, 301)에서 해당 객체 또는 사용자의 위치 또는 이동이다. 이러한 시각적 추적 정보는 해당 객체 또는 참가 사용자를 해당 기본 디지털 비디오 스트림(210, 301)의 하위 부분으로 보여주는 이미 지를 얻기 위해서, 기본 디지털 비디오 스트림(210, 301)에 적용될 가상적 자르기, 패닝 또는 줌잉에 대한 정보 를 포함할 수 있다. 따라서, 제1 생성 제어 매개변수는 일반적으로 물리적 동작 또는 문제의 기본 디지털 비디 오 스트림(210, 301)을 캡처하는 카메라에 대한 정보를 포함하지 않고, 오히려 객체 또는 참가 사용자를 표시하도록 제1 기본 디지털 비디오 스트림(210, 301)을 수정하는 방법에 대한 명령어를 포함한다. 이는 일반적 으로 본 명세서에서 설명된 생성 제어 매개변수는 시스템에 속하는 하드웨어 장비의 물리적 이동에 관한 명령을 포함하는 것이 아니라, 하나 또는 여러 대의 카메라(401', 401\", 401\"')에서 촬영한 이미지의 디지털 사 후 캡처링 처리에 대한 명령어를 포함한다는 의미에서 사실이다. 제1 생성 제어 매개변수는 또한 미리 정해진 이벤트 또는 패턴의 감지에 따라 자동으로 생성되고, 및 /또는 미리 정해진 또는 가변적인 생성 일정에 따라 자동으로 생성되는 개별 생성 명령을 포함할 수 있다. 예를 들어, 이벤트는 기본 디지털 비디오 스트림(210, 301)에서 관심 있는 특정 미리 정해진 객체의 자동 감지 일 수 있으며, 개별적 생성 명령은 출력 디지털 비디오 스트림에서 해당 미리 정해진 객체에 대한 간략한 명령 비디오를 표시하는 것일 수 있다. 생성 명령이 \"개별적\"이라는 것은 생성 명령이 시간 범위가 아닌 개별적인 시 점에서 한 번만 적용되어야 함을 의미한다. 예를 들어, 생성 명령은 이러한 명령 비디오를 시작하는 것일 수 있 다. 제1 생성 제어 매개변수는 예를 들어 상술된 유형의 참가 사용자 또는 객체를 뒤따르거나 하이라이트하기 위해 자르기/패닝/틸팅/줌잉을 설명하는 매개변수 데이터와 같은, 가상적 자르기, 패닝, 틸팅 및/또는 줌잉 명 령을 또한 포함할 수 있으다. 가상적 자르기, 패닝, 틸팅 및/또는 줌잉 명령은 개별 프레임 사이에서와 같이, 기본 디지털 비디오 스트림의 타임라인을 따라 정적이거나 동적으로 변경될 수 있다. 제1 생성 제어 매개변수는 또한 카메라(401', 401\", 401\"')의 움직임 감지를 기반으로 자동으로 생성되는 카메 라 안정화 정보를 포함할 수 있으며, 이는 해당 주요 디지털 비디오 스트림을 제공한다. 따라서, 이 경우 디지 털 비디오 분석은 카메라(401', 401\", 401\"')가 참가 사용자(122\", 122\"', 122\"\") 중 한 명에 의해 손으로 조작하는 것에 의해서, 카메라(401', 401\", 401\"')의 흔들림이나 다른 움직임의 형태로 이벤트 또는 패턴 을 동적으로 감지하는 것을 목표로 하고, 제1 생성 제어 매개변수는 적용시 감지된 움직임을 최소한 부분 적으로 상쇄하고 및/또는 시간이 지남에 따라 기본 디지털 비디오 스트림(210, 301)을 안정화하는 것을 목표로 하는 기본 디지털 비디오 스트림(210, 301)에 대한 패닝/회전/틸팅/줌잉 명령일 수 있다. 문제의 카메라(401', 401\", 401\"')의 움직임은 예를 들어 프레임 간 이미지 변환을 감지하기 위한 픽셀 상관 관계 기술과 같은 기존 의 이미지 처리 기술을 사용하여, 기본 디지털 비디오 스트림의 이미지 처리에 전적으로 기반하여 감지될 수 있 다. 이러한 모든 예에서, 제1 디지털 이미지 분석은 관련된 계산으로 인해 수행하는 데 일정 시간이 걸린다. 결과적 으로, 제1 생성 제어 매개변수는 제1 기본 디지털 비디오 스트림(210, 301)에서 상기 제1 이벤트 또는 패 턴의 발생 시간과 관련하여 제1 시간 지연으로 설정된다. 이 제1 시간 지연은 해당 제1 시간 지연을 갖는 기본 디지털 비디오 스트림이 해당 캡처된 디지털 사운드 스트림과 시간 동기화되지 않은 경우 눈에 띄는 사운 드 지연을 생성할 만큼 충분히 길 수 있고; 및/또는 제1 시간 지연은 해당 디지털 비디오 통신 서비스을 사용하여 서로 상호 작용하는 참가 사용자(122\", 122\"', 122\"\")에 상호 작용 어려움을 일으키지 않을 만큼 충분 히 작을 수 있다. 구체적으로, 제1 시간 지연은 0.1초 이상일 수 있다. 이 실시 예 및 다른 실시 예에서, 제1 시간 지연은 1초 미만, 예를 들어 0.5초 미만, 예를 들어 0.3초 미만일 수 있다. 후속 생성 단계 S806에서, 상기 제1 생성 제어 매개변수는 디지털 출력 비디오 스트림의 생성의 일부로서 실시 간 제1 기본 디지털 비디오 스트림(210, 301)에 적용된다. 제1 생성 제어 매개변수를 적용하게 되면 결과적으로 제1 기본 디지털 비디오 스트림(210, 301)이 제1 생성 제어 매개변수를 기반으로 수정되어 제1 생성된 디지털 비디오 스트림을 생성한다. 그러나 이러한 수정은 기본 디지털 비디오 스트림 자체가 상기 제1 시간 지연에 의 해 지연되지 않고 이루어진다. 따라서, 제1 생성 제어 매개변수가 제1 시간 지연 이후에만 결정되는 경우에도(제1 생성 제어 매개변수를 확립 하는 데 특정 시간이 걸렸기 때문임), 제1 생성 제어 매개변수의 적용에 의해 영향을 받는 것은 지연되지 않은 제1 기본 디지털 비디오 스트림(210, 301)이다. 다시 말해, 제1 생성 제어 매개변수는 제1 기본 디지털 비디오 스트림(210, 301)의 타임라인을 따라 이벤트 또는 패턴이 감지된 타임라인을 따른 지점보다 적어도 제1 시간 지연만큼 늦은 시점에 제1 기본 디지털 비디오 스트림(210, 301)에 적용된다. 따라서, 예를 들어 이벤 트 또는 패턴이 제1 기본 디지털 비디오 스트림(210, 301)의 프레임 x에서 감지된 객체의 움직임인 경우, 제1 생성 제어 매개변수는 제1 기본 디지털 비디오 스트림(210, 301)의 가상 자르기를 변환하여 객체를 새로운 위치로 따라가도록 할 수 있다. 그러나 제1 생성 제어 매개변수는 제1 기본 디지털 비디오 스트림(210, 301)의 y 프레임(예: y=10 프레임)과 같은, 제1 시간 지연에서만 설정된다. 따라서 제1 생성 제어 매개변수가 자르기의 해당 움직임을 실행하도록 적용되는 경우, 제1 기본 디지털 비디오 신호(210, 301)의 자르기의 이 움 직임은 제1 기본 디지털 비디오 신호(210, 301)의 프레임 x + 1 y와 관련된다. 일부 실시 예에서, 제1 디지털 이미지 분석은 출력 디지털 비디오 스트림을 생성하고 게시하도록 구성된 컴퓨터 장치에 의해 수행된다. 특히, 이것은 중앙 서버(130')를 포함하는 동일한 물리적 컴퓨터 장치(40 2)의 일부인 중앙 서버(130\")의 생성 기능(135\")일 수 있으며, 이는 제1 디지털 이미지 분석을 수행하고 제1 생 성 제어 매개변수를 설정한다. 그런 다음 제1 생성 제어 매개변수를 적용하는 것은 중앙 서버(130')(생성 기능 (135')를 통해)일 수 있다. 이에 의하면 제1 시간 지연이 최소화되는데, 이는 제1 생성 제어 매개변수를 설정하고 적용하는 데 외부 또는 주변 컴퓨터 장치와의 통신이 필요하지 않기 때문이다. 이를 통해 회의실이나 장소에 이미 설치된 카메라 지원 하드웨어 장치(402') 및/또는 회의 참가자가 사용 하는 랩톱(402\", 402\"') 등이 향상된 공유 디지털 화상 회의 서비스를 생성하는 데 사용되고 다음에 다른 회의 참가자에 의해 제1 기본 디지털 비디오 스트림(210, 301)이 시작된 맥락에서 동일한 대화형 디지털 비디오 통신 세션의 일부로 액세스되어 사용되는 것을 가능하게 한다. 다른 예에서, 이러한 여러 하드웨어 장치(402', 402\", 402\"')는 각각 도 9에 도시된 유형의 장치일 수 있 으며, 각각은 다른 참가자 사용자가 소비하거나 공통 중앙 비디오 통신 서비스이 결합하여 사용할 수 있는 각각의 향상된 출력 디지털 비디오 스트림을 동시에 생성하고 게시하여 공통 디지털 비디오 통신 서 비스 경험의 훨씬 더 정교한 생성을 위해 사용할 수 있다. 이러한 출력 디지털 비디오 스트림은 상기 제1 시간 지연 없이 실시간으로 제공되기 때문에, 중앙 비디오 통신 서비스의 생성은 중앙 비디오 통신이 생성 목적으로 상기 설명한 대로 의도적으로 대기 시간을 추가하더라도 비교적 적은 대기 시간의 비디오 통신을결과되게 한다. 예를 들어, 교실에서 광각 웹 카메라(401')을 갖춘 고정 컴퓨터 장치(402')는 교실에 있는 모든 학생 또는 적어 도 몇몇 학생을 보여주는 이미지를 캡처하는, 도 9에 도시된 유형의 컴퓨터 장치일 수 있다. 동시에, 한 명 또는 여러 명의 학생이 웹캠 지원 노트북(402\", 402\"')의 형태로 자신의 컴퓨터 장치를 실행하여 해당 학생만 보여주는 이미지만 캡처할 수 있다. 모든 장치(402', 402\", 402\"')는 각각의 장치(402', 402\", 402\"') 에서 볼 수 있는 각각의 향상된 실시간 출력 디지털 비디오 스트림을 생성하고 게시할 수 있으므로, 매우 낮은 대기 시간과 관련된 제1 참가자 그룹을 형성할 수 있고, 및/또한 중앙 비디오 통신 서비스에 의 해 사용되어 약간의 지연으로 외부 참가자 또는 시청자가 사용할 수 있는 보다 정교한 공통 비디오 통신 경험을 생성하여 더 큰 지연 시간과 관련된 제2 참가자 그룹을 형성할 수 있다. 이러한 경우 모든 관련된 수집, 분석 및 생성은 시스템에 대한 매개변수 입력을 기반으로 완전히 자동화된 방식으로 구성될 수 있으며, 이를 통해 자동적이고 동적으로 적용되는 생성 단계가 결과된다. 제1 디지털 이미지 분석 및 출력 디지털 비디오 스트림의 게시는 출력 디지털 비디오 스트림을 생성 하기 위해 적어도 제1 디지털 이미지 분석 및 제1 생성 제어 매개변수의 적용과 관련하여, 동일한 물리적 컴퓨 터 장치에서 적어도 부분적으로 별도의 프로세스 또는 스레드에서 작동하는 컴퓨터 소프트웨어를 사용하여 수행될 수 있다. 고품질, 저지연 출력 디지털 비디오 스트림을 제공하기 위해, 일부 실시 예에서, 제1 디지털 이미지 분석 을 수행하는 컴퓨터 장치의 현재 프로세서 부하의 기능으로서 제1 디지털 이미지 분석에 대한 프로세서 조 절을 수행할 수 있다. 이는 특히 중앙 서버(130', 130\")가 하나의 동일한 중앙 프로세서 장치에서 실행되는 경 우에 해당한다. 이러한 프로세서 조절은 출력 디지털 비디오 스트림(중앙 서버(130'))의 제공 및 게시가 제1 디지털 이미지 분석(중앙 서버(130\"))보다 프로세서 우선 순위를 갖도록 수행될 수 있다. 즉, 제한된 CPU 조건에서, 생성 기능(135') 및 게시 기능(136')은 생성 기능(135\")에 비해 CPU 우선 순위를 갖는다. 예를 들어, 제1 디지털 이미지 분석의 프로세서 조절은 제1 디지털 이미지 분석을 제1 기본 디지털 비디오 스트림(210, 301)의 모든 비디오 프레임의 하위 부분으로만 제한하여 수행할 수 있다. 예를 들어, 제1 디지털 이미지 분석은 다른 모든 프레임이나 제3 프레임에 대해서만 수행되거나, 제1 디지털 이미지 분석은 주어진 시간 단위마다 한 프레임에 대해서만 수행되며, 예를 들어 0.1초당 한 프레임 또는 그보다 덜 자주 수행된다. 많은 경우, 이는 출 력 디지털 비디오 스트림에서 높은 비디오 품질을 유지하면서도 충분히 정확한 이벤트 또는 패턴 감지를 제공할 수 있다. 조절은 항상 사용할 수도 있거나, 필요에 따라 스위치 온되거나 오프될 수 있다. 후자의 경우, 사용 가능한 CPU 용량이 너무 작아서 원하는 최소 비디오 품질로 출력 디지털 비디오 스트림(23 0)을 제공하고 게시할 수 없다는 감지의 결과로 조절이 스위치 온될 수 있다. 후속 게시 단계 S807에서, 출력 디지털 비디오 스트림은 게시 기능(136')에 의해 적어도 하나의 참가자 클 라이언트에 지속적으로 제공(게시)된다. 참가자 클라이언트는 출력 디지털 비디오 스트림의 생 성을 수행하는 동일한 컴퓨터 장치일 수 있으므로, 컴퓨터 장치의 사용자에게 시청을 위한 향상 된 대기 시간이 짧은 비디오 스트림을 제공할 수 있다. 참가자 클라이언트는 또한 다른 컴퓨터 장치(402', 402\", 402\"')일 수 있으며, 또한 상술된 바와 같이 2차 출력 디지털 비디오 스트림을 생성하기 위해 생성 및 게 시된 출력 디지털 비디오 스트림을 사용하는 비디오 통신 서비스일 수도 있다. 출력 디지털 비디오 스트림은 상기 제1 생성된 디지털 비디오 스트림의 형태로 또는 이에 기초하여 제공 및 게시되고, 이는 다시 생성 기능(135')에 의해 생성된다. 상술된 바와 같이, 출력 디지털 비디오 스트림(23 0)은 여러 계층 또는 단계에서 생성되고 유용하게 될 수 있다. 예를 들어, 출력 디지털 비디오 스트림은 상기 제1 기본 디지털 비디오 스트림(210, 301)과 상기 제1 생성된 디지털 비디오 스트림을 기반으로 하여 동일 하거나 다른 하드웨어 장치에 의해 생성되어 해당 제1 생성된 디지털 비디오 스트림을 생성할 수 있다. 후속 단계 S808에서, 이 방법은 종료된다. 그러나 도 8에 도시된 바와 같이 반복이 일반적으로 발생한다. 생성 기능(135')에서 수행되는 생성은 가능한 한 빨리 이루어져야 하며, 생성 기능(135\")에서 설정한 생성 제어 매개변수만 제1 기본 디지털 비디오 스트림(210, 301)에 적용하고, 결과된 제1 생성된 디지털 비디오 스트림을 지속적으로 실시간으로 출력한다. 위에서 설명한 대로, 추가 기능(예를 들어, 외부에서 제공된 비디오 자료 또 는 메타데이터 형태)을 포함하는, 기본 디지털 비디오 스트림(210, 301)을 자르거나 조정하는 방법에 대한 모든 정보가 생성 기능(135\")에서 수신되고, 그 결과 제1 생성 제어 매개변수의 값이 처음에 설정된 것에 기초하여, 제1 생성 제어 매개변수의 적용과 제1 기본 비디오 스트림(210, 301)의 내용에 대한 시간적 불일치가 발생한다.그런 다음, 제1 생성된 디지털 비디오 스트림은 모든 수집 기능(131, 131', 131\", 131\"')에 대한 실시간으로 향 상된 기본 비디오 스트림 입력으로 노출될 수 있다. 디지털 도메인에서 비디오 프레임의 자르기 또는 줌잉을 수행하는 것과 같은 제1 생성 제어 매개변수의 적용은 매우 빠르며, 결과적으로 시간 지연이 최소화된다. 예를 들어, 제1 생성 제어 매개변수의 적용으로 인해 (제1 기본 디지털 비디오 스트림과 관련하여) 제1 생성된 디지털 비디오 스트림이 최대 0.2초 지연될 수 있다. 그런 다음, 수집 기능(131')은 또한 제1 기본 디지털 비디오 스트림(210, 301)에 더하여 또는 그 일부로서, 제1 디지털 오디오 스트림을 지속적으로 수집 또는 캡처하는 것을 포함할 수 있으며, 이 때 제1 디지털 오디오 스트 림은 제1 기본 디지털 비디오 스트림(210, 301)과 연관된다. 수집/캡처되면, 제1 디지털 오디오 스트림은 실제 로 제1 기본 디지털 오디오 스트림을 약간 지연시켜 제1 생성된 비디오 스트림(210, 301)과 시간 동기화될 수 있으며, 시간 동기화된 제1 디지털 오디오 스트림은 그런 다음 적어도 하나의 참가자 클라이언트 또는 수 집 기능에 제1 생성된 디지털 비디오 스트림 및/또는 출력 디지털 비디오 스트림과 함께 또는 그 일 부로서 제공될 수 있다. 이런 식으로 비디오와 해당 오디오는 모두 수집 기능(131')에 입력될 수 있고, 동기화된 비디오와 해당 오디오 의 향상된 번들은 향상 후 최소한의 시간 지연으로 게시 기능(136')에 의해 출력될 수 있다. 이런 식으로 중앙 서버(130')는 중앙 서버(130\")의 도움을 받아 \"가상 비디오 케이블\"로 볼 수 있으며, 최소한의 시간 지연으로만 작동하지만 전단부에서 입력된 입력 비디오/오디오 정보의 향상된 버전을 후단부에서 출력할 수 있습니다. 생성 기능(135\")에서, 상기 향상을 생성하는 비디오 분석이 발생하여, 위에서 논의한 바와 같이 제1 생성 제어 매개변수의 설정이 결과된다. 여기에는 이미지에서 특정 사람을 감지하는 것, 이미지에서 특정 움직이는 사람을 따라가거나 초점을 맞추기 위해 자르기 또는 가상 줌잉을 선택하는 것, 카메라의 부드러운 패닝/움직임을 달성 하기 위해 카메라를 안정화하기 위해 반대 가상 카메라 움직임을 식별하는 것 등이 포함될 수 있다. 이 분석은 생성 기능(135')에 대한 여러 가지 결정/명령을 생성하여, 결과적으로 해당 결정/명령을 적용하기 전에 약간의 시간 지연이 적용될 수 있다. 이 작은 시간 지연은 일반적으로 카메라 추적의 경우에도 출력 디지털 비디오 스 트림을 소비하는 사용자의 경험에 부정적인 영향을 미치지 않는 것으로 밝혀졌다. 인간 카메라 조작 자가 움직이는 사람이나 물체에 대해 카메라 추적을 수행하더라도 조작자는 특정 최소 반응 시간을 갖는다. 또 한, 생성 기능(135\")은 추적되는 사람이나 물체의 작은 움직임을 무시하고 이벤트 또는 패턴를 감지 하고, 더 큰 움직임에 대한 반응으로 가상 카메라 패닝 또는 움직임만 수행하도록 구성될 수 있다. 상술된 것과 같은 동일한 물리적 컴퓨터 하드웨어에서 멀티스레드 소프트웨어 구현은 단일 하드웨어 어플라이언스를 사 용하여 생성 복잡성이 광범위한 가능한 하드웨어 플랫폼에서 예를 들어 제1 생성된 디지털 비디오 스트림의 이 미지 또는 사운드 품질에 영향을 미치지 않는 방식으로 달성하는 것을 가능하게 한다. 특히 제1 생성 제어 매개변수는 제1 기본 디지털 비디오 스트림(210, 301)의 디지털 변환을 제어하도록만 배열 된다는 점에 유의해야 한다. 제1 생성 기능(135')이 해당 카메라에 물리적 패닝, 줌잉 또는 이와 유사한 작업을 수행하라는 명령을 제공하는 것이 가능할 수 있지만, 이것은 본 발명의 범위를 벗어난다. 도 8에 도시된 바와 같이, 이 방법은 또한 제1 기본 디지털 비디오 스트림(210, 301)의 제2 디지털 이미지 분석 을 수행하는 단계 S802를 포함할 수 있다. 일부 실시 예에서, 이 방법은 제1 기본 디지털 비디오 스트림(210, 301)과 지속적으로 캡처되고 연관된 디지털 오디오 스트림의 제2 디지털 오디오 분석을 수행하는 단계 S802를 포함한다. 제2 이미지/오디오 분석은 제1 기본 디지털 비디오 스트림(210, 301) 및/또는 해당 디지털 오디오 스트림에서 적어도 하나의 제2 이벤트 또는 패턴를 식별하고, 후속 단계 S803에서 제2 생성 제어 매개변수를 설 정하기 위해 수행된다. 제2 생성 제어 매개변수는 일반적으로 위에서 설명한 제1 생성 제어 매개변수와 유사할 수 있다. 제1 이미지 분석과 유사한 방식으로, 제2 디지털 이미지/오디오 분석은 수행하는 데 특정 시간이 걸리므로, 제2 생성 제어 매개변수는 제1 기본 디지털 비디오 스트림(210, 301)에서 해당 제2 이벤트 또는 패턴의 발생 시간과 관련하여 제2 시간 지연 후에 설정된다. 그러나 제2 시간 지연은 제1 시간 지연보다 길다. 그런 다음, 상기 제2 생성 제어 매개변수는 실시간 제1 기본 디지털 비디오 스트림(210, 301)에 적용되어, 제1 기본 디지털 비디오 스트림(210, 301)이 상기 제2 시간 지연에 의해 지연되지 않고 상기 제2 생성 제어 매개변 수에 따라 수정되어 상기 제1 생성된 디지털 비디오 스트림을 생성하게 된다. 실제로, 제2 생성 제어 매개변수는 생성 기능(135')에 의해 제1 생성 제어 매개변수의 적용에 해당하는 방식으 로 제1 기본 디지털 비디오 스트림(210, 301)에 직접 적용될 수 있다. 다른 실시 예에서, 제1 생성 제어 매개변 수보다 더 큰 지연 시간으로 설정된 제2 생성 제어 매개변수는 제1 생성 제어 매개변수가 제1 기본 디지털 비디 오 스트림(210, 301)에 적용되기 전에 제1 생성 제어 매개변수의 값에 영향을 미치게 하는 등에 의해, 제1 기본 디지털 비디오 스트림(210, 301)에 간접적으로만 적용될 수 있다. 예를 들어, 제2 생성 제어 매개변수는 현재 표시된 카메라 각도 또는 프레젠테이션 슬라이드 선택과 같은 더 광범위한 생성 측면과 관련될 수 있는 반면, 제1 생성 제어 매개변수는 현재 적용된 정확한 가상 패닝 또는 자르기와 같은 더 자세한 생성 측면과 관련될 수 있다. 제2 디지털 이미지 분석은 제1 생성 제어 매개변수의 적용을 수행하는 컴퓨터 장치와 관련하여 원격인 컴퓨터 장치 및/또는 제2 생성 제어 매개변수의 적용을 수행하는 컴퓨터 장치와 관련하여 원격인 컴퓨터 장치에 의해 수행될 수 있다. 도 9에 도시된 예에서, 컴퓨터 장치는 제1 생성 제어 매개변수를 적용하고, 아마도 제2 생성 제어 매개변수도 적용하며, 해당 컴퓨터 장치는 중앙 서버(130\"')가 실행되는 컴퓨터 장치에 대해 원 격이다. 일부 실시 예에서, 제2 생성 제어 매개변수는 제1 디지털 이미지 분석에 대한 입력을 구성한다. 예를 들어, 제2 생성 제어 매개변수는 제1 기본 디지털 비디오 스트림(210, 301)에서 보여지고 있는 특정 사람 또는 물체를 식 별하는 정보를 포함할 수 있으며, 이 식별 정보는 제1 디지털 이미지 분석에 대한 입력으로 사용되어 식별된 사 람 또는 물체가 제1 기본 디지털 비디오 스트림(210, 301)을 구성하는 이미지 프레임을 통과할 때 (가상 패닝 및/또는 줌잉에 의해) 추적을 수행할 수 있다. 따라서, 제2 생성 제어 매개변수는 영향을 받지 않은 캡처된 이미지에 표시된 특정 참가 사용자를 제1 생성된 디지털 비디오 스트림에서 표시할지 여부에 대한 명령을 포함할 수 있으며, 해당 참가 사용자는 디지털 이미지 처리를 기반으로 자동으로 식별된다. 이 실시 예 및 기타 실시 예에서, 제2 생성 제어 매개변수는 제2 기본 비디오 스트림(210, 301), 예를 들어 제1 생성된 디지털 비디오 스트림에 통합될 제2 기본 비디오 스트림(210, 301)을 포함할 수 있다. 제2 시간 지연이 제1 시간 지연보다 큰 것은 제2 이미지 및/또는 오디오 처리가 제1 이미지 처리보다 더 많은 시간이 걸리거나, 중앙 서버(130', 130\")와 중앙 서버(130\"') 간의 통신에 시간이 걸리기 때문일 수 있다. 일반적으로, 제2 디지털 이미지 처리로 인해 제2 처리 제어 매개변수가 설정되면 하드웨어 장치가 용량 한 계에 도달하는 경우와 같이 제1 디지털 이미지 처리에서 부하 완화를 구성할 수 있다. 제1 이미지 분석의 일부 를 중앙 서버(130\"')로 오프로드하는 것은 제2 생성 제어 매개변수를 적용하는 데 더 큰 시간 지연을 수용하는 동시에 더 완전한 이미지 분석을 얻는 측면에서 합리적인 트레이드오프가 될 수 있다. 그러나 일반적으로 중앙 서버(130\"')는 클라우드 리소스이거나, 중앙 서버(130\"')의 경우보다 훨씬 더 많은 처 리 능력 및/또는 더 큰 외부 데이터 데이터베이스에 대한 빠른 액세스를 제공하는 다른 외부 컴퓨팅 리소스일 수 있다(위에서 설명한 대로 로컬로 배치될 수 있음). 따라서 제2 이미지 및/또는 오디오 분석은 제1 이미지 분 석보다 더 향상되고 이에 따라 더 많은 처리가 필요한 작업을 포함하는 것이 바람직하다. 예를 들어, 제2 이미지 처리는 감지할 잠재적 인물의 데이터베이스와 그들의 얼굴 특징을 기반으로 하는 고급 얼굴 인식을 포함할 수 있다. 반면, 제1 이미지 처리는 제1 기본 디지털 비디오 스트림(210, 301)에서 이미 식 별된 사람의 얼굴을 찾아 추적하는 알고리즘을 포함할 수 있다. 제2 이미지 또는 오디오 처리는 분석된 이미지 및/또는 오디오(보이는 것 또는 이야기되는 것)에서 자동으로 해 석된 정보를 외부 데이터에 연결하는 것도 포함할 수 있다. 예를 들어, 자동 오디오 처리가 자연어 감지, 구문 분석 및 해석 구성 요소를 포함하는 경우, 해당 오디오에서 들리는 화자가 특정 꽃 종에 대해 이야기하고 있다 는 결론에 도달할 수 있다. 그런 다음 제2 생성 제어 매개변수는 해당 꽃 종의 꽃을 보여주는 특정 이미지를 제 1 생성된 디지털 비디오 스트림에 통합하는 정보를 포함할 수 있다. 이에 따라 제2 이미지 처리는 특정 록 밴드 의 로고가 제1 기본 디지털 비디오 스트림에 표시되고 제2 생성 제어 매개변수는 해당 록 밴드의 이미지를 제1 생성된 디지털 비디오 스트림에 표시하라는 명령을 포함할 수 있다. 제1 및 제2 생성 제어 매개변수의 설정은 가능한 그러한 생성 제어 매개변수와 연관된 값의 사용 가능한 공간을 기반으로 수행될 수 있으며, 그러한 공간은 비디오 통신 서비스의 사용자가 정의할 수 있는 구성 매개변수 에 의해 정의될 수 있다. 예를 들어, 지리에 대한 강의를 제공하는 데 사용되는 비디오 통신 서비스에서,생성 기능(135\"')은 그러한 구성 매개변수를 통해, (예를 들어 표시된 화이트보드의 텍스트에서) 언급되거나 언 급된 국가를 모니터링하고 제1 생성된 디지털 비디오 스트림의 특정 미리 결정된 하위 스트림에서 해당 플래그 를 표시하도록 지정하는 제2 생성 제어 매개변수를 자동으로 생성하도록 지시될 수 있다. 일부 실시 예에서는, 디지털 오디오 스트림만 전송되고 제1 기본 디지털 비디오 스트림(210, 301)은 전송되지 않는다. 이렇게 하면 (위에서 설명한 대로 원격으로 배치될 수 있는) 중앙 서버(130\"')에 전달해야 하는 데이터 양이 상당히 줄어든다. 구체적인 예에서, 제2 생성 제어 매개변수는 얼굴 인식을 수행하는 제2 디지털 이미지 처리를 기반으로 지속적 으로 설정되어, 생성 기능(135')에 하나 또는 여러 개의 기본 디지털 비디오 스트림을 선택하고 가상으로 패닝/ 자르기/줌잉하여 출력 디지털 비디오 스트림에 교사를 표시하지만 학생은 표시하지 않도록 지시한다. 이런 식으 로, 학생의 얼굴을 보여주는 완결성을 위험에 빠뜨리지 않는 출력 디지털 비디오 스트림이 수행된다. 위에서 설 명한 대로, 가상 패닝/자르기/줌잉 작업은 실제로 제1 이미지 처리와 제1 생성 제어 매개변수를 통해 제2 생성 제어 매개변수 형태의 입력을 사용하여 수행될 수 있다. 이 경우, 제1 생성된 디지털 비디오 스트림과 연관된 연관된 디지털 오디오 스트림이 또한 제1 및/또는 제2 생 성 제어 매개변수의 영향을 받을 수 있다. 여기에는 참가자 사용자의 다양한 마이크로폰에서 캡처한 것과 같은 여러 사용 가능한 기본 디지털 오디오 스트림 세트 중에서 선택하는 것이 포함될 수 있지만, 특정 사운드를 디 지털로 억제하거나 강화하는 것도 포함될 수 있다. 교사와 학생의 예에서, 제1 생성된 디지털 비디오 스트림에 서는 학생이 제기한 질문은 흐릿하게 처리될 수 있는 반면, 교사의 답변은 영향을 받지 않을 수 있다. 본 발명은 또한 컴퓨터가 프로그램을 실행할 때 컴퓨터가 임의의 선행 청구항에 따라 출력 디지털 비디오 스트 림을 제공하는 방법을 수행하도록 하는 명령어를 포함하는 컴퓨터 프로그램 제품에 관한 것이다. 컴퓨터 프로그램 제품은 시스템의 컴퓨터 하드웨어 장치 중 적어도 하나에 위치한 하나 이상의 하드웨어 프로세 서가 본 명세서에서 설명된 방법 단계를 수행하도록 하는 명령어를 인코딩하는 비일시적 컴퓨터 판독 가능 매체 에 의해 구현될 수 있다. 본 발명은 또한 상기 시스템에 관한 것으로, 차례로 상기 하나 또는 여러 개의 수집 기능(131, 131', 131\", 131\"')을 포함하고, 차례로 상기 실시간 제1 기본 디지털 비디오 스트림(210, 301)을 지속적으로 수집하 도록 배열되고; 상기 하나 또는 여러 개의 생성 기능(135, 135', 135\", 135\"')은 차례로 상기 제1 디지털 이미 지 분석을 수행하여 본 명세서에서 설명된 대로 상기 제1 생성 제어 매개변수를 확립하도록 배열되고; 상기 하 나 또는 여러 개의 게시 기능(136, 136', 136\", 136\"')은 본 명세서에서 기술된 바와 같이 상기 출력 디지털 비 디오 스트림을 지속적으로 제공하도록 배열된다. 시스템은 또한 여러 카메라를 포함할 수 있으며, 각각은 각각의 비지연 기본 디지털 비디오 스트림(210, 301)을 캡처하도록 배열된다. 그런 다음, 생성 기능(135')은 캡처된 기본 디지털 비디오 스트림(210, 301) 각각 에 따라 상기 비지연 제1 생성 디지털 비디오 스트림을 생성하도록 배열될 수 있다. 위에서, 바람직한 실시 예가 설명되었다. 그러나, 당업자라면 본 발명의 기본 사상에서 벗어나지 않고 개시된 실시 예에 많은 수정이 이루어질 수 있다는 것을 알 수 있다. 예를 들어, 많은 추가 기능이 본 명세서에서 설명된 시스템의 일부로 제공될 수 있으며, 이것은 본 명세서 에서 설명되고 있지 않다. 일반적으로, 현재 설명된 솔루션은 비디오 데이터 스트림이 통신에 사용되는 다양한 구체적인 애플리케이션을 충족시키기 위해 자세한 기능 및 특징을 구축할 수 있는 프레임워크를 제공한다. 한 가지 예는 출력 디지털 비디오 스트림이 현재 또는 다른 유형의 자동 디지털 비디오 생성 방법 또는 시스템 에 대한 입력 기본 디지털 비디오 스트림을 형성할 수 있다는 것이다. 제1 및 제2 이벤트/패턴은 도 8 내지 10과 관련하여 위에서 구체적으로 예시되었지만, 많은 다른 유형의 이벤트 와 패턴이 생각될 수 있다는 것을 알 수 있다. 가능한 유형의 이벤트와 패턴은 본 명세서 전체에서 논의되었으 며, 당업자라면 이러한 예와 논의가 설명 목적을 위한 것이며 제한적인 목록을 구성하려는 것이 아니라는 것을 이해할 것이다. 일반적으로, 현재 방법과 관련하여 언급된 모든 내용은 현재 시스템 및 컴퓨터 소프트웨어 제품에 적용 가능하 며, 그 반대의 경우도 마찬가지이다. 따라서, 본 발명은 설명된 실시 예에 제한되지 않고 첨부된 청구범위의 범위 내에서 변경될 수 있다. 도면 도면1 도면2 도면3 도면4 도면5 도면6a 도면6b 도면6c 도면6d 도면6e 도면6f 도면7 도면8 도면9 도면10"}
{"patent_id": "10-2025-7006373", "section": "도면", "subsection": "도면설명", "item": 1, "content": "이하에서, 본 발명은 본 발명의 예시의 실시 예와 관련하여 첨부된 도면을 참조하여 자세히 설명한다: 도 1은 제1 예시의 시스템을 도시한다; 도 2는 제2 예시의 시스템을 도시한다;도 3은 제3 예시의 시스템을 도시한다; 도 4는 중앙 서버를 도시한다; 도 5는 제1 방법을 도시한다; 도 6a-6f는 도 5에 도시된 방법의 다른 방법 단계와 관련하는 후속 상태를 도시한다; 도 7은 개념적으로 공통 프로토콜을 도시한다; 도 8은 제2 방법을 도시한다; 도 9는 제4 예시의 시스템을 도시한다; 도 10은 제5 예시의 시스템을 도시한다."}
