{"patent_id": "10-2022-0147451", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0066542", "출원번호": "10-2022-0147451", "발명의 명칭": "멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법", "출원인": "(주)바이브컴퍼니", "발명자": "안창원"}}
{"patent_id": "10-2022-0147451", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "정보시스템이, 사람의 안면을 촬영한 영상 및 열상과 녹음된 음성을 tensor fusion 방법을 이용한 알고리즘으로분석하여 사람의 피로도 수준을 판단하는 방법으로서,대상자에 대하여 일정 시간 동안 동시에 촬영 및 녹음된 영상, 열상 및 음성을 상기 대상자의 분석대상으로 입력받는 제1단계;상기 분석대상으로 입력받은 영상 및 열상을 프레임 단위로 분해하는 제2단계;상기 제2단계에서 분해된 영상 및 열상의 프레임 중에서 분석대상 영상프레임 및 분석대상 열상프레임을 선별하는 제3단계;상기 분석대상 영상프레임으로부터 영상특징 텐서 를 생성하는 제4단계;상기 분석대상으로 입력받은 음성데이터로부터 음성특징 텐서 를 생성하는 제5단계;상기 분석대상 열상프레임으로부터 열상특징 텐서 를 생성하는 제6단계;상기 영상특징 텐서 , 상기 음성특징 텐서 및 상기 열상특징 텐서 의 차원을 확장하는 제7단계;상기 제7단계에서 차원이 확장된 각 텐서들을 Tensor Fusion Network에 입력하여 Cartesian Product 방식으로연산하여 융합텐서 을 생성해 내는 제8단계; 및 상기 융합텐서 를 완전연결층(Fully Connected Layer)으로 보내 피로도 수준을 분류시키는 제9단계;를 포함하는 것을 특징으로 하는 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법"}
{"patent_id": "10-2022-0147451", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제4단계는, - EfficientNet을 이용하는 영상 임베딩 서브모델이, 상기 분석대상 영상프레임 을 3채널 이미지로 축소한 후벡터 형식으로 영상 특징요소 를 추출하여 벡터 임베딩을 진행하고, 상기 영상 특징요소 모두를 하나로 연결하여 상기 영상 특징텐서 를 다음과 같이 생성하는 것을 특징으로 하는 멀티모달 텐서퓨전을 이용한 피로도수준 판단방법"}
{"patent_id": "10-2022-0147451", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제5단계는, - 음성 임베딩 서브모델이, 상기 음성데이터 중 번째 음성데이터 에 대한 특정 구간 별 F0-mean 값을 추출하여 벡터 임베딩을 진행하고 이들을 하나로 연결하여 상기 음성특징 텐서 를 다음과 같이 생성하는 것을 특징으로 하는 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법공개특허 10-2024-0066542-3-"}
{"patent_id": "10-2022-0147451", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 제6단계는,- EfficientNet을 이용하는 열상 임베딩 서브모델이, 상기 분석대상 열상프레임 에서 벡터 형식으로 열상 특징요소 를 추출하여 벡터 임베딩을 진행하고 열상 특징요소 모두를 하나로 연결하여 상기 열상 특징텐서 를 다음과 같이 생성하는 것을 특징으로 하는 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법"}
{"patent_id": "10-2022-0147451", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 제4단계는,영상 임베딩 서브넷이, 상기 분석대상 영상프레임 에 대하여 DenseNet을 통한 채널확장과 Global AveragePooling (GAP) layer를 적용한 연산을 통하여 1차원 벡터 로 축소한 후 상기 1차원 벡터 모두를 하나로 연결하여 상기 영상 특징텐서 를 다음과 같이 생성하는 것을 특징으로 하는 멀티모달 텐서퓨전을 이용한 피로도수준 판단방법"}
{"patent_id": "10-2022-0147451", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 제5단계는, - 음성 임베딩 서브넷이, 상기 음성데이터 중 번째 음성데이터 에 대한 데이터를 완전연결층(Fully ConnectedLayer)으로 보내 상기 음성특징 텐서 를 생성하는 것을 특징으로 하는 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법"}
{"patent_id": "10-2022-0147451", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 제6단계는,열상 임베딩 서브넷이, 상기 분석대상 영상프레임 에 대하여 DenseNet을 통한 채널확장과 Global AveragePooling (GAP) layer를 적용한 연산을 통하여 1차원 벡터 로 축소한 후 상기 1차원 벡터 모두를 하나로 연결하여 상기 열상 특징텐서 를 다음과 같이 생성하는 것을 특징으로 하는, 멀티모달 텐서퓨전을 이용한 피로도수준 판단방법"}
{"patent_id": "10-2022-0147451", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "공개특허 10-2024-0066542-4-제1항 내지 제7항 중 어느 한 항에 있어서,상기 제7단계에서, 상기 영상특징 텐서 , 상기 음성특징 텐서 및 열상특징 텐서 의 차원을 확장할 때, 1의 값으로 해당 차원을 채우는 것을 특징으로 하는, 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법"}
{"patent_id": "10-2022-0147451", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제7항 중 어느 한 항에 있어서,상기 제3단계에서 상기 분석대상 영상프레임 또는 상기 분석대상 열상프레임의 선별은, - 상기 제2단계에서 프레임 단위로 분해된 M개의 프레임 이미지() 각각에 대하여 칼라 히스토그램을 생성하여시간 순서에 따른 M개의 칼라 히스토그램()을 만드는 제1과정;-m = 1에서 m = M이 될 때까지 아래의 과정을 반복하는 제2과정; 및-- 상기 M개의 칼라 히스토그램() 중 m번째 칼라 히스토그램()을 m+n번째(n은 1부터 시작) 칼라 히스토그램()과 비교한 유사도를 m번째 프레임 이미지()에 대한 유사도()로 하는 제2-1과정;-- 상기 유사도()가 임계값보다 큰 경우 m+n번째 프레임()을 유사프레임으로 정한 후, n+1을 n으로 하여 상기 제2-1과정부터 다시 반복하는 제2-2과정;-- 상기 유사도()가 상기 임계값보다 작은 경우 상기 m번째 프레임 이미지()를 키 프레임()으로 선별하고 n을 상기 유사프레임의 개수로 한 후, m + n을 m으로 하고 n을 다시 1로 하여 상기 제2-1과정부터 다시 반복하는 제2-3과정;- 상기 제2과정에서 선별된 상기 키 프레임()을 상기 분석대상 영상프레임 또는 상기 분석대상 열상프레임으로 제공하는 제3과정;을 포함하는 것을 특징으로 하는, 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법"}
{"patent_id": "10-2022-0147451", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제7항 중 어느 한 항에 있어서,상기 제3단계에서 상기 분석대상 영상프레임 또는 상기 분석대상 열상프레임의 선별은, - 상기 제2단계에서 프레임 단위로 분해된 상기 프레임 이미지() 각각에 대하여 Gaussian MixtureModel(GMM)을 이용하여 객체(object)를 제거하고 배경을 추출하는 제1a과정;- 상기 프레임 이미지() 각각의 원본 프레임에서 상기 배경의 픽셀 값만큼 차감하여 객체중심 프레임을 추출하는 제2a과정;- 상기 프레임 이미지() 각각의 객체중심 프레임에서 모든 픽셀을 합 연산하여 상기 프레임 이미지() 각각에 대한 행동정보()를 정의하는 제3a과정;- 상기 프레임 이미지() 각각에 대한 행동정보()들을 일정 개수 단위로 묶어 윈도우(windows)를 만드는제4a과정;- 상기 윈도우에 포함된 상기 행동정보()들의 묶음 중에서 가장 큰 지역 최대값(local maxima)을 가지는 행동정보()에 대응되는 m번째 프레임 이미지를 키 프레임()으로 선별하는 제5a과정; - 상기 윈도우 내에서 상기 키 프레임()의 뒤프레임이 존재하는 경우 상기 뒤프레임을 상기 키 프레임()의 유사프레임으로 하는 제6a과정;- 상기 윈도우 내에서 상기 키 프레임()의 앞프레임이 존재하는 경우 상기 앞프레임을 상기 키 프레임()공개특허 10-2024-0066542-5-이전 키 프레임()의 유사프레임으로 하는 제7a과정; 및- 상기 제5a과정에서 선별된 상기 키 프레임()들을 상기 분석대상 영상프레임 또는 상기 분석대상 열상프레임으로 제공하는 제8a과정;를 포함하는 것을 특징으로 하는, 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법"}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법에 관한 것이다. 보다 상세하게로는, 텐서퓨전 네트 워크(Tensor Fusion Network)를 포함하는 딥러닝 알고리즘을 가지는 정보시스템이, 대상자의 안면이 포함되어 촬 영된 영상 및 열상과 이와 동시에 녹음된 대상자의 음성을 멀티모달로 입력받아 분석한 후 그 사람의 현재 피로 도 수준을 판단하는 방법에 관한 것이다. 본 발명은 대상자에 대하여 일정시간 동안 동시에 촬영 및 녹음된 영상, 열상 및 음성을 분석대상 멀티모달로 입력받아 입력된 영상, 열상 및 음성을 각각의 특징텐서로 만든 후 이들의 차원을 확장하여 텐서퓨전 네트워크에 입력하여 융합텐서를 만들고, 만들어진 융합텐서를 완전연결층에 보내어 대상자의 피로도 수준을 판단하게 된다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법에 관한 것이다. 보다 상세하게로는, 텐서퓨전 네 트워크(Tensor Fusion Network)를 포함하는 딥러닝 알고리즘을 가지는 정보시스템이, 대상자의 안면이 포함되어 촬영된 영상 및 열상과 이와 동시에 녹음된 대상자의 음성을 멀티모달로 입력받아 분석한 후 그 사람의 현재 피 로도 수준을 판단하는 방법에 관한 것이다. 본 발명은 대상자에 대하여 일정시간 동안 동시에 촬영 및 녹음된 영상, 열상 및 음성을 분석대상 멀티모달로 입력받아 입력된 영상, 열상 및 음성을 각각의 특징텐서로 만든 후 이들의 차원을 확장하여 텐서퓨전 네트워크에 입력하여 융합텐서를 만들고, 만들어진 융합텐서를 완전연결층에 보내어 대상자의 피로도 수준을 판단하게 된다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "세상에서 발생되는 수많은 사건들은 시간적인 변화로 이루어져 있다. 이러한 사건들은 특정한 상황에서 인간이 감각을 인지하는 방법과 유사한 방법으로 데이터화되는데, 이 영역을 양상(modal)이라 한다. 한 사건의 데이터 는 다양한 모달(modal)로 관찰될 수 있으며, 이 모달들의 종합적인 판단에는 감정이나 외부상태 등 정량적으로 표현하기 어려운 정성적 요소들이 부가되어 사건의 결과로 나타난다. 이러한 데이터를 분석하여 그 결과를 예측 하고자 하는 시도가 계속되고 있으나, 일반적으로는 하나의 모달을 수집한 데이터로부터 특정한 목적을 가진 분 석을 통해 사건을 설명하고 보다 효과적으로 예측하는 방식으로 접근하고 있다. 최근에는 멀티모달(multimodal) 학습이 등장하였는데, 이 방법은 복수의 모달을 이용하여 주어진 사건에 대한 더 나은 이해방법을 제공하는 것을 목적으로 하고 있다. 주로 특정한 상황에서 하나의 모달을 다른 모달로 전이 하여 학습하거나 복수의 모달을 함께 학습하여 특정한 분석 목적을 달성하고자 하는데 주로 사용되어 왔는데, 주요한 연구로는 텍스트 데이터를 통한 이미지 판단, 이미지 데이터를 통한 텍스트 판단, 음성 모달 주관 모델 등이 있으며, 감성분석, 자연어 처리 등으로 그 연구 영역이 확장되고 있다. 한편, 현대 사회를 살아가는 사람들은 현실 공간은 물론 가상의 공간에서도 다양한 상호작용을 통하여 교류하게 되다 보니 충분한 쉼이 없이 바쁜 하루하루가 계속된다. 따라서 현대인에게 피로는 필연적으로 발생하게 되는데, 피로는 사람들이 경험하는 환경과 상호작용만큼이나 원인과 형태도 다양하다. 특히 현대인의 생활패턴 과 업무형태가 매우 중대한 영향을 끼치게 되는데, 생리적 특성, 수면장애, 개인의 생활 특성, 스트레스, 생활 환경, 건강 유지를 위한 다양한 활동 등이 그 원인으로 알려져 있다. 그중 수면장애는 피로의 가장 큰 원인으로 작용하고 있는데, 수면시간은 집중력, 의사결정 능력 반응시간 등의 수행능력에 영향을 주고 이러한 수행능력이 떨어지는 상태를 피로 상태로 나타낼 수 있다. 항공기나 전투기의 조종사 또는 정밀 무기체계의 운용자 등과 같이 업무수행 시 고도의 집중력이 필요한 전문 종사자들이나 함정의 승조원 또는 장거리 운전기사 등과 같이 장시간 동안 근무해야 하는 반면 돌발상황 발생 시 신속하고 정확한 판단력을 발휘해야 하는 고위험 직종에 근무하는 종사자들의 경우, 피로도가 높은 상태에서 근무나 전투에 투입되면 인지기능이 저하되어 안전사고 발생의 가능성이 높아지게 될 뿐만 아니라, 전투 인력의 경우에는 전투력 저하에도 크게 영향을 끼치기 때문에 피로도가 높은 사람들을 사전에 가려내어 근무에서 배제 할 수 있도록 하는 제도를 엄격하게 운영할 필요성이 있다. 따라서, 근무자나 전투 인력 각 개인에 대한 피로도를 객관적으로 측정할 수 있는 방법이나 시스템이 필요하다. 그러나 사람에게 있어서 피로라는 개념은 명확하게 구분되지 않는 개념일 뿐만 아니라, 피로도가 높은지 여부는 본인조차도 쉽게 판단할 수 없기 때문에 객관적 기준으로 피로도가 높은 사람들을 걸러내는 것은 쉬운 일이 아 니다. 피로에 관한 가장 객관적인 개념 구분은 피로를 중추 피로 또는 정신적 피로라고 할 수 있는 주관적 피로와 어 떤 과업을 수행하기에 적절한 수준의 신체적 각성을 유지하고 있는가가 기준이 되는 임무 수행 피로 즉, 말초 피로로 구분하는 것이다. 주관적 피로를 측정하는 도구로서, 임상적인 척도로 사용되는 자기 보고식 평가도구는여러 가지 버전으로 개발되어 있다. 또한, 피로도 수준을 측정하는 기존의 방법에는 설문조사, 생화학 및 생리 학적검사, 반응속도 테스트 등이 있다. 설문에 의한 방법은 다원적 피로척도(Multidimensional Fatigue Inventory, MFI)의 내용을 토대로 피로 측정 도구의 개발에 통계적으로 유의미한 결과를 보이고 있다. 그리고, Actigraphy를 활용한 일주기 리듬 교란 현상 에 대해 측정하여 평가하는 경우도 있으며, 심박수, 맥파 등의 지표를 활용하여 측정하는 ECG(ElectrocardioGraphy) 방법, 피부의 온도와 같은 지표를 이용하는 생태 순간 평가(Ecological Momentary Assessment) 등의 방법도 있으며 혈액, 타액 등의 채취를 통해 측정하는 등의 생리학적 방법도 있다. 그러나 이러한 종래의 측정방법들은 질병의 관점에서 피로의 정도를 판단하는데 주된 목적이 있다 보니, 병적 피로의 원인과 정도를 특정하는 데는 용이하나, 다양한 원인과 형태로 나타나는 피로 정도를 측정하는 데는 제 한을 받는다. 또한, 피로도 측정방법에 있어서 주관적 판단을 사용하거나 개인의 상태와 상황에 따라 편차가 존 재하여 피로도 산정에 절대적 기준으로 사용하기는 어려울 뿐만 아니라 조사에 기반한 피로 측정방법은 시간과 비용이 소요되고 식사 또는 운동, 감염 등 피로 측정에 영향을 미치는 요소들이 많다. 한편, 머신러닝(Machine learning) 및 인공지능(AI) 기술의 발전으로 인해 수많은 데이터를 분석하여 다양한 분 야에서 활용하고자 하는 시도가 계속되고 있다. 특히 컴퓨터비전(Computer vision) 분야의 영상분석 기술의 발 전에 따라 특정 분야에서는 인간의 판단 영역까지 도달하는 분석결과를 나타내고 있는데, 행동분석 등 영상분석 기술은 인간의 상태 및 행동 등을 측정하여 차후 이어지는 행동이나 감정 등을 효과적으로 분석하고 있다. 최근에는 피로와 관련된 데이터를 수집하고 딥러닝 모델을 학습하여 피로를 측정하는 연구가 진행되고 있다. 대 표적으로 센서를 통해 신체의 움직임 데이터를 수집하여 피로를 측정하거나, 장시간 운전자 등을 지속적으로 촬 영하여 눈의 깜빡임 등 동공, 홍채의 형태변화를 학습하여 피로를 측정하는 연구 등이 있다. 그러나 이러한 방 법들은 장시간 동안 대상자를 촬영한 영상을 기반으로 하기 때문에 전투기 조종사 등에 대하여 근무 투입 전에 실시하는 피로도 검사 등에는 사용할 수가 없는 문제점이 있었다. 따라서 본 발명에서는 비교적 짧은 시간에 대상자의 얼굴 부분을 촬영한 영상 및 열상과 같은 시간대에 대상자 의 목소리를 녹음한 음성을 멀티모달로 입력받아 tensor fusion 방법을 이용한 알고리즘으로 분석하여 사람의 피로도 수준을 판단하는 기법을 제안하고, 영상 및 열상의 분석에 따른 컴퓨터 자원의 효율적 활용과 분석시간 의 단축 등을 위하여 영상 및 열상을 구성하는 프레임 이미지 중에서 유의미한 이미지를 가지는 키 프레임만을 추출하여 분석하는 방법도 제공한다. 선행기술문헌 비특허문헌 (비특허문헌 0001) Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. 2021. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems 34 . (비특허문헌 0002) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision. 2425-2433. (비특허문헌 0003) Yusuf Aytar, Carl Vondrick, and Antonio Torralba. 2016. Soundnet: Learning sound representations from unlabeled video. Advances in neural information processing systems 29 . (비특허문헌 0004) Chongqing Chen, Dezhi Han, and Jun Wang. 2020. Multimodal encoder-decoder attention networks for visual question answering. IEEE Access 8 , 35662-35671. (비특허문헌 0005) Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human detection. In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR’ 05), Vol. 1. Ieee, 886-893. (비특허문헌 0006) David F Dinges. 1995. An overview of sleepiness and accidents. Journal of sleep research 4 , 4-14. (비특허문헌 0007) Desmond Elliott, Douwe Kiela, and Angeliki Lazaridou. 2016. Multimodal learning andreasoning. In Proceedings of the 54th annual meeting of the association for computational linguistics: Tutorial abstracts. (비특허문헌 0008) Jianlong Fu and Yong Rui. 2017. Advances in deep learning approaches for image tagging. APSIPA Transactions on Signal and Information Processing 6 . (비특허문헌 0009) Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang, and Hsin-Min Wang. 2018. Audio-visual speech enhancement using multimodal deep convolutional neural networks. IEEE Transactions on Emerging Topics in Computational Intelligence 2, 2 , 117-128. (비특허문헌 0010) Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. 2017. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 . (비특허문헌 0011) Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. 2018. Efficient low-rank multimodal fusion with modality-specific factors. arXiv preprint arXiv:1806.00064 . (비특허문헌 0012) Yaxiong Ma, Yixue Hao, Min Chen, Jincai Chen, Ping Lu, and Andrej Koˇsir. 2019. Audio-visual emotion fusion (AVEF): A deep efficient weighted approach. Information Fusion 46 , 184-192. (비특허문헌 0013) Pranay Mathur, Aman Gill, Aayush Yadav, Anurag Mishra, and Nand Kumar Bansode. 2017. Camera2Caption: a real-time image caption generator. In 2017 International Conference on Computational Intelligence in Data Science (ICCIDS). IEEE, 1-6. (비특허문헌 0014) Louis-Philippe Morency, Rada Mihalcea, and Payal Doshi. 2011. Towards multimodal sentiment analysis: Harvesting opinions from the web. In Proceedings of the 13th international conference on multimodal interfaces. 169-176. (비특허문헌 0015) Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William T Freeman, Michael Rubinstein, and Wojciech Matusik. 2019. Speech2face: Learning the face behind a voice. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7539-7548. (비특허문헌 0016) Soujanya Poria, Erik Cambria, and Alexander Gelbukh. 2015. Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis. In Proceedings of the 2015 conference on empirical methods in natural language processing. 2539-2544. (비특허문헌 0017) Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao. 2019. Mirrorgan: Learning text-to-image generation by redescription. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1505-1514. (비특허문헌 0018) Dhanesh Ramachandram and Graham W Taylor. 2017. Deep multimodal learning: A survey on recent advances and trends. IEEE signal processing magazine 34, 6 , 96-108. (비특허문헌 0019) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International Conference on Machine Learning. PMLR, 8821-8831. (비특허문헌 0020) Mohammad Naim Rastgoo, Bahareh Nakisa, Frederic Maire, Andry Rakotonirainy, and Vinod Chandran. 2019. Automatic driver stress level classification using multimodal deep learning. Expert Systems with Applications 138 , 112793. (비특허문헌 0021) EMA Smets, Bert Garssen, B de Bonke, and JCJM De Haes. 1995. The Multidimensional Fatigue Inventory (MFI) psychometric qualities of an instrument to assess fatigue. Journal of psychosomatic research 39, 3 , 315-325. (비특허문헌 0022) Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012. UCF101: A dataset of 101human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 . (비특허문헌 0023) Gaurav Verma, Rohit Mujumdar, Zijie J Wang, Munmun De Choudhury, and Srijan Kumar. 2022. Overcoming Language Disparity in Online Content Classification with Multimodal Learning. arXiv preprint arXiv:2205.09744 . (비특허문헌 0024) Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3156-3164."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상술한 문제점을 해결하기 위하여 창안된 본 발명은, 정보시스템이 텐서퓨전 네트워크를 포함하는 딥러닝 알고 리즘으로 대상자를 촬영한 영상 및 열상과 이와 동시에 녹음된 대상자의 음성을 멀티모달로 입력받아 분석한 후 그 사람의 현재 피로도 수준을 판단하는 방법을 제공하는 것을 목적으로 한다. 본 발명의 또 다른 목적은, 장시간 동안 대상자를 촬영하거나 관찰한 영상이나 데이터를 이용하지 않고 비교적 짧은 시간 동안 대상자의 안면을 촬영한 영상, 열상 및 녹음된 음성을 이용하여 피로도 수준을 측정하는 방법을 제공하는 것이다. 본 발명의 또 다른 목적은, 동영상 분석에 따른 컴퓨터 자원의 효율적 활용과 분석시간 단축을 위하여 영상 및 열상 속에 포함되어 있는 많은 프레임 이미지 중에서 피로 측정에 사용할 수 있는 의미 있는 키 프레임 이미지 만을 추출하여 피로도를 측정하는 방법을 제공하는 것이다. 본 발명이 이루고자 하는 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 다른 기술적 과제들은 아래의 기재로부터 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "이와 같은 목적을 달성하기 위하여 본 발명은, 정보시스템이, 사람의 안면을 촬영한 영상 및 열상과 녹음된 음 성을 tensor fusion 방법을 이용한 알고리즘으로 분석하여 사람의 피로도 수준을 판단하는 방법으로서, 대상자 에 대하여 일정 시간 동안 동시에 촬영 및 녹음된 영상, 열상 및 음성을 상기 대상자의 분석대상으로 입력받는 제1단계; 상기 분석대상으로 입력받은 영상 및 열상을 프레임 단위로 분해하는 제2단계; 상기 제2단계에서 분해 된 영상 및 열상의 프레임 중에서 분석대상 영상프레임 및 분석대상 열상프레임을 선별하는 제3단계; 상기 분석 대상 영상프레임으로부터 영상특징 텐서 를 생성하는 제4단계; 상기 분석대상으로 입력받은 음성데이터로부터 음성특징 텐서 를 생성하는 제5단계; 상기 분석대상 열상프레임으로부터 열상특징 텐서 를 생성하는 제6단계; 상기 영상특징 텐서 , 상기 음성특징 텐서 및 열상특징 텐서 의 차원을 확장하는 제7단계; 상기 제7단계에서 차원이 확장된 각 텐서들을 Tensor Fusion Network에 입력하여 Cartesian Product 방식으로 연산 하여 융합텐서 을 생성해 내는 제8단계; 및 상기 융합텐서 를 완전연결층(Fully Connected Layer)으 로 보내 피로도 수준을 분류시키는 제9단계;를 포함하는 것을 특징으로 하는 멀티모달 텐서퓨전을 이용한 피로 도 수준 판단방법으로 하는 것이 바람직하다. 본 발명은 또한 상술한 특징들에 더하여 상기 제4단계는, - EfficientNet을 이용하는 영상 임베딩 서브모델이, 상기 분석대상 영상프레임 을 3채널 이미지로 축소한 후 벡터 형식으로 영상 특징요소 를 추출하여 벡터 임 베딩을 진행하고, 상기 영상 특징요소 모두를 하나로 연결하여 상기 영상 특징텐서 를 다음과 같이 생성하 는 것을 특징으로 하는 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법으로 하거나,"}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "또는 상기 제4단계는, 영상 임베딩 서브넷이, 상기 분석대상 영상프레임 에 대하여 DenseNet을 통한 채널확장 과 Global Average Pooling (GAP) layer를 적용한 연산을 통하여 1차원 벡터 로 축소한 후 상기 1차원 벡터 모두를 하나로 연결하여 상기 영상 특징텐서 를 다음과 같이 생성하는 것을 특징으로 하는 멀티모달 텐서퓨 전을 이용한 피로도 수준 판단방법으로 하는 것도 바람직하다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "본 발명은 또한 상술한 특징들에 더하여 상기 제5단계는, - 음성 임베딩 서브모델이, 상기 음성데이터 중 번째 음성데이터 에 대한 특정 구간 별 F0-mean 값을 추출하여 벡터 임베딩을 진행하고 이들을 하나로 연결하여 상 기 음성특징 텐서 를 다음과 같이 생성하는 것을 특징으로 하는 멀티모달 텐서퓨전을 이용한 피로도 수준 판단 방법으로 하거나,"}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 4, "content": "또는 상기 제5단계는, - 음성 임베딩 서브넷이, 상기 음성데이터 중 번째 음성데이터 에 대한 데이터를 완전 연결층(Fully Connected Layer)으로 보내 상기 음성특징 텐서 를 생성하는 것을 특징으로 하는 멀티모달 텐서 퓨전을 이용한 피로도 수준 판단방법으로 하는 것도 바람직하다. 본 발명은 또한 상술한 특징들에 더하여 상기 제6단계는, - EfficientNet을 이용하는 열상 임베딩 서브모델이, 상기 분석대상 열상프레임 에서 벡터 형식으로 열상 특징요소 를 추출하여 벡터 임베딩을 진행하고 열상 특 징요소 모두를 하나로 연결하여 상기 열상 특징텐서 를 다음과 같이 생성하는 것을 특징으로 하는 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법으로 하거나,"}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 5, "content": "또는 상기 제6단계는, 열상 임베딩 서브넷이, 상기 분석대상 영상프레임 에 대하여 DenseNet을 통한 채널확 장과 Global Average Pooling (GAP) layer를 적용한 연산을 통하여 1차원 벡터 로 축소한 후 상기 1차원 벡터 모두를 하나로 연결하여 상기 열상 특징텐서 를 다음과 같이 생성하는 것을 특징으로 하는, 멀티모달 텐서 퓨전을 이용한 피로도 수준 판단방법으로 하는 것도 가능하다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 6, "content": "본 발명은 또한 상술한 특징들 모두에 더하여, 상기 제7단계에서, 상기 영상특징 텐서 , 상기 음성특징 텐서 및 열상특징 텐서 의 차원을 확장할 때, 1의 값으로 해당 차원을 채우는 것을 특징으로 하는, 멀티모달 텐 서퓨전을 이용한 피로도 수준 판단방법으로 하는 것도 바람직하다. 본 발명은 또한 상술한 특징들 모두에 더하여, 상기 제3단계에서 상기 분석대상 영상프레임 또는 상기 분석대상 열상프레임의 선별은, - 상기 제2단계에서 프레임 단위로 분해된 M개의 프레임 이미지( ) 각각에 대하여 칼라 히스토그램을 생성하여 시간 순서에 따른 M개의 칼라 히스토그램( )을 만드는 제1과정; -m = 1에서 m = M이 될 때까지 아래의 과정을 반복하는 제2과정; 및 -- 상기 M개의 칼라 히스토그램( ) 중 m번째 칼라 히스토그램 ( )을 m+n번째(n은 1부터 시작) 칼라 히스토그램( )과 비교한 유사도를 m번째 프레임 이미지( )에 대한 유사도( )로 하는 제2-1과정; -- 상기 유사도( )가 임계값보다 큰 경우 m+n번째 프레임( )을 유사프레임 으로 정한 후, n+1을 n으로 하여 상기 제2-1과정부터 다시 반복하는 제2-2과정; -- 상기 유사도( )가 상기 임계값보다 작은 경우 상기 m번째 프레임 이미지( )를 키 프레임( )으로 선별하고 n을 상기 유사프레임의 개 수로 한 후, m + n을 m으로 하고 n을 다시 1로 하여 상기 제2-1과정부터 다시 반복하는 제2-3과정; - 상기 제2 과정에서 선별된 상기 키 프레임( )을 상기 분석대상 영상프레임 또는 상기 분석대상 열상프레임으로 제공하 는 제3과정;을 포함하는 것을 특징으로 하는, 멀티모달 텐서퓨전을 이용한 피로도 수준 판단방법으로 하는 것도 바람직하다. 본 발명은 또한 상술한 특징들 모두에 더하여, 상기 제3단계에서 상기 분석대상 영상프레임 또는 상기 분석대상 열상프레임의 선별은, - 상기 제2단계에서 프레임 단위로 분해된 상기 프레임 이미지( ) 각각에 대하여 Gaussian Mixture Model(GMM)을 이용하여 객체(object)를 제거하고 배경을 추출하는 제1a과정; - 상기 프레임 이미지( ) 각각의 원본 프레임에서 상기 배경의 픽셀 값만큼 차감하여 객체중심 프레임을 추출하는 제2a과정; - 상기 프레임 이미지( ) 각각의 객체중심 프레임에서 모든 픽셀을 합 연산하여 상기 프레임 이미지( ) 각각 에 대한 행동정보( )를 정의하는 제3a과정; - 상기 프레임 이미지( ) 각각에 대한 행동정보( )들을 일 정 개수 단위로 묶어 윈도우(windows)를 만드는 제4a과정; - 상기 윈도우에 포함된 상기 행동정보( )들의 묶 음 중에서 가장 큰 지역 최대값(local maxima)을 가지는 행동정보( )에 대응되는 m번째 프레임 이미지를 키 프레임( )으로 선별하는 제5a과정; - 상기 윈도우 내에서 상기 키 프레임( )의 뒤프레임이 존재하는 경우 상기 뒤프레임을 상기 키 프레임( )의 유사프레임으로 하는 제6a과정; - 상기 윈도우 내에서 상기 키 프레임 ( )의 앞프레임이 존재하는 경우 상기 앞프레임을 상기 키 프레임( ) 이전 키 프레임( )의 유사프레 임으로 하는 제7a과정; 및 - 상기 제5a과정에서 선별된 상기 키 프레임( )들을 상기 분석대상 영상프레임 또 는 상기 분석대상 열상프레임으로 제공하는 제8a과정;를 포함하는 것을 특징으로 하는, 멀티모달 텐서퓨전을 이 용한 피로도 수준 판단방법으로 하는 것도 바람직하다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상에서 살펴본 바와 같이 본 발명은, 비교적 짧은 시간 동안 사람의 안면을 촬영한 영상 및 열상과 동시에 녹 음된 음성을 정보시스템이 분석하여 사람의 현재 피로도 수준을 판단하는 방법을 제공함으로써, 대상자의 피로 도 수준을 신속하고 정확하게 측정할 수 있는 효과가 있다. 본 발명은 또한, 하나의 모달(modal)만 분석하지 않고 사람의 피로도를 감지해낼 수 있는 영상, 열상, 음성 등 복수의 모달을 입력받아 분석하기 때문에 대상자의 피로도 수준을 더욱 정확하게 판단해 낼 수 있는 효과가 있 다. 본 발명은 또한 입력받은 복수의 모달에 대하여 late fusion방법을 사용하여 분석하지 않고 tensor fusion network로 분석하기 때문에 피로도 수준을 더욱 정확하게 분석해 낼 수 있는 효과가 있다. 본 발명은 또한 대상자에 대하여 일정시간 동안 동시에 촬영 및 녹음된 영상, 열상 및 음성을 멀티모달로 입력 받아 입력된 영상, 열상 및 음성 등의 모달을 각각의 특징텐서로 만든 다음에 이들의 차원을 확장하여 텐서퓨전 네트워크에 입력하여 융합텐서를 만들고, 만들어진 융합텐서를 완전연결층에 보내어 피로도 수준을 판단하기 때 문에, 입력데이터의 의미를 가능한한 최대로 유지하면서 해당 데이터를 축소하고 가공하여 fusion을 위한 텐서 를 정의할 수 있으며, 이에 따라 영상, 음성, 열상 각 입력 데이터의 형태와 크기에는 큰 차이가 존재함에도 불 구하고 입력 데이터의 불균등성 문제까지 해결할 수 있는 효과가 있다. 본 발명은 또한, 각 모달에 해당하는 텐서의 차원을 확장할 때, 1의 값으로 해당 차원을 채움으로써 기존의 각 Unimodal 텐서와 Biomodal 텐서의 값을 훼손시키지 않을 수 있으며, 이에 따라 fusion된 텐서는 세 가지가 결합 되어 새롭게 생성된 Trimodal 텐서 뿐만 아니라 Unimodal 및 Biomodal에 해당하는 기존 정보를 그대로 유지할 수 있는 효과가 있다. 본 발명은 또한, 분석대상 영상의 수많은 프레임 중에서 안면의 움직임이나 자세 등에 변화가 있는 시점의 프레 임 이미지를 키 프레임으로 하여 따로 추출하여 키 프레임만을 분석하고, 키 프레임이 아닌 이미지 즉, 변화가 없어서 키 프레임과 동일한 내용을 가지는 프레임 이미지에 대하여는 분석하지 않기 때문에, 컴퓨터 자원을 효 율적으로 활용할 수 있고 신속하게 피로도를 분석해 낼 수 있는 효과가 있다.본 발명은 또한, 분석대상 영상 중 분석의 의미가 있는 이미지들만 골라내어 키 프레임으로 하여 분석하기 때문 에 일부 프레임에 대하여만 분석하더라도 모든 프레임 이미지에 대하여 분석하는 것과 동일한 효과를 거둘 수 있게 된다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서 상술한 목적과 특징이 분명해지도록 본 발명을 상세하게 설명할 것이며, 이에 따라 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 본 발명의 기술적 사상을 용이하게 실시할 수 있을 것이다. 또한 본 발 명을 설명함에 있어서 본 발명과 관련한 공지기술 중 이미 그 기술 분야에 익히 알려져 있는 것으로서, 그 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에 그 상세한 설명을 생략하기로 한다. 아울러, 본 발명에서 사용되는 용어는 가능한 한 현재 널리 사용되는 일반적인 용어를 선택하였으나, 특정한 경 우는 출원인이 임의로 선정한 용어도 있으며 이 경우는 해당되는 발명의 설명부분에서 상세히 그 의미를 기재하 였으므로, 단순한 용어의 명칭이 아닌 용어가 가지는 의미로서 본 발명을 파악하여야 함을 밝혀두고자 한다. 실 시 예들에 대한 설명에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 실시 예들을 한 정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 실시 예들은 여러 가지 형태로 변경을 가할 수 있고 다양한 부가적 실시 예들을 가질 수 있는데, 여기에서는 특 정한 실시 예들이 도면에 표시되고 관련된 상세한 설명이 기재되어 있다. 그러나 이는 실시 예들을 특정한 형태 에 한정하려는 것이 아니며, 실시 예들의 사상 및 기술 범위에 포함되는 모든 변경이나 균등물 내지 대체물을 포함하는 것으로 이해되어야 할 것이다. 다양한 실시 예들에 대한 설명 가운데 “제1”, “제2”, “첫째” 또는“둘째”등의 표현들이 실시 예들의 다 양한 구성요소들을 수식할 수 있지만, 해당 구성요소들을 한정하지 않는다. 예를 들어, 상기 표현들은 해당 구 성요소들의 순서 및/또는 중요도 등을 한정하지 않는다. 상기 표현들은 한 구성요소를 다른 구성요소와 구분 짓 기 위해 사용될 수 있다. 이하에서는 첨부된 도면 등을 참조하여 본 발명을 설명한다. 그동안 멀티모달 학습은 일반적으로 정량적 데이터를 분석하여 하나의 모달을 다른 모달에 대한 판단 기준으로 사용하였다. 텍스트 데이터를 통해 이미지를 생성하는 것과 같은 정성적 결과에 대한 연구도 존재하지만, 이는 분석 과정에서 발견된 사례로서 이를 평가하고자 연구된 것은 아니다.본 발명에서는 정량적 데이터의 분석을 통해 정성적 데이터로 표현되는 사건의 특정한 상태를 분류하는 방법을 제안하고 실험을 통해 검증하였다. 사건의 표현으로 가장 대표적인 비디오, 음성 및 열화상 데이터를 각각 하나 의 모달로 정의하고, 이들을 Tensor Fusion Networks(TFN)을 통해 통합적으로 분석하여 복합적인 정성적 데이터 라 할 수 있는 대상자의 피로 수준을 분류하는 모델을 제시하였다. 실험 결과 멀티모달 모델이 각 단일 모달을 활용한 모델(Unimodal)을 능가하는 성능을 보여주었는데, 이는 멀티모달을 이용한 분석방법이 동일한 데이터를 사용한 단일모달보다 정성적 판단을 보다 효과적으로 추출할 수 있음을 의미한다. 이러한 현상을 검토하기 위하 여 데이터 입력 단계를 분할하여 각 모달의 효과와 그 조합효과를 비교 분석하여 멀티모달의 차원이 높을수록 그 분석효과가 상승함을 실험적으로 보였다. 본 발명을 통해 얻을 수 있는 성과를 살펴보면, 먼저, 단일모달 측면의 데이터에서 얻을 수 있는 효과를 복수로 조합함으로써 주어진 문제를 복합적인 시각에서 바라보고 해결할 수 있는지, 특히 정량적 분석결과를 통해 정성 적 평가기준에 관한 지표를 달성하는데 효과가 있는지 확인하는 방법론을 실험을 통해 입증하게 되며, 이는 특 히 인간의 복합적인 지각을 입력으로 이용하는 경험적, 지성적 판단 등의 AI 연구에 시사점을 제공할 수 있다. 또한, 입력 데이터를 해당 모달의 SOTA(State of the Art) 모델로 분석한 결과를 조합한 효과와 입력 데이터의 표현(representation)을 조합 분석한 효과의 비교를 통해 관점(viewpoint), 의도 (intention)와 같은 새로운 분석지향점을 발견할 수 있는 방법을 제공한다. 멀티모달 학습 혹은 멀티모달 AI는 연구 영역을 확장해나가고 있다. Image tagging, image captioning, text- to-image, visual question answering, Scene recognition by sound, and Speech2Face 등의 많은 아이디어와 모델을 제시한 연구들이 있다. 다양한 연구들의 주어진 문제는 다른 모달의 추가를 통해 기존 모달의 효과를 향 상시키거나 기존 모달의 분석을 통해 새로운 목표가 되는 모달에 도달할 수 있도록 하는 것이다. 여러 모달을 융합하는 것은 멀티모달 학습을 통한 문제 해결의 기초적인 접근 방법이라고 할 수 있다. 가장 기본적인 방법은 각 모달로 부터 도출된 특징 벡터들을 연결하는 방법으로 초기 융합(Early Fusion)이라고 할 수 있다. 그러나, 멀티모달 모델 학습은 각 모달을 나타내는 데이터 간의 형식 또는 표현의 차이 특징 공간의 차이 등으 로 인해 어려운 작업이며 특정한 모달에 편향된 결과를 가져올 수 있다. 멀티모달 학습에 대한 최근의 시도들은 멀티모달 표현을 위한 텐서의 적절한 사용의 중요성에 대해 제시하고 있다. 본 발명에서는 Tensor Fusion Networks(TFN) 방법을 적용하여 텐서의 의미있는 조작을 통해 멀티모달을 표현하고 제어하고자 하였으며 관련 모델을 제안하고 실험을 진행하였다. 도 1은 TFN의 멀티모달 텐서 표현 방법을 나타낸다. 한편, 피로는 인간의 생리적 특성, 수면 장애, 생활 습관, 스트레스 등 기타 신체 환경과 같은 다양한 원인으로 발생할 수 있다. 가장 중요한 요인은 수면장애로, 수면 시간이 집중력과 의사 결정에 영향을 미치기 때문에 이 러한 성능이 저하되는 상황은 피로한 것으로 추정할 수 있다. 피로 수준을 측정하기 위해서는 설문지, 생화학적 / 생리적 테스트 및 반응 테스트와 같은 다양한 방법이 사용되고 있다. 설문지의 경우 Multidimensional Fatigue Inventory (MFI)에 기반한 피로 측정 도구를 개발하였을 때 통계적으로 의미 있는 결과를 보였다. 또한 액티그래피, Electrocardiography (ECG), 피부 온도를 이용한 생태순간평가 등도 이용되고 있다. 이러한 방법에 는 주관적인 판단과 개인적 상황의 차이에 의한 수준 변화가 포함될 수 있으며, 피로의 범주적 수준을 거의 설 정하지 않는다. 게다가 조사와 설문지를 통해 측정하는 것은 많은 시간과 비용이 들고 영양, 운동, 감염과 같은 외부 요인에 의한 영향을 무시할 수 없다. 최근에는 운전자 졸음 감지 데이터셋과 같은 피로와 관련된 데이터셋 수집과 분석에 대한 연구가 진행되고 있다. 그러나 이러한 연구들은 주로 멀티모달 접근법이나 피로도의 검출이 아닌 유니모달 방식의 동작 검출에 초점이 맞추어져 있다. 본 발명에서는 각각의 모달 데이터의 효율성을 확인하기 위해 멀티모달 접근법의 피로 수준 측정 방법을 제공한다. 특정한 의미는 행동으로 나타난다. 영상에서의 동작 검출은 영상 또는 이를 구성하는 프레임에서 특정 의미 또 는 감정 등을 찾기 위해 사용된다. HOG는 정면에서 본 얼굴 이미지 뿐만 아니라 자연스러운 포즈의 이미지도 사 용했다. UCF101 데이터셋의 경우 101개의 활동 클래스를 정의하였고 13,000개의 클립과 27시간의 영상데이터가 포함되어 있으며, 이는 인간의 활동을 분석하는데 사용된다. 최근에는 Convolution - 3D가 사용되고 있다. 본 발명에서는 행동/활동 인식과 같은 실용적으로 일반화된 방법이 아니라 피로와 같은 포괄적인 의미를 찾기위한 정량적 분석 방법을 제안한다. 본 발명에서는 tensor fusion 방법을 통한 피로 수준 측정 모델(fTFN)을 제공한다. 정성적 상태로 표현되는 데 이터인 피로 수준을 도출하기 위해서 정량적 데이터로 표현되는 3가지의 모달인 영상, 음성 및 열상을 포함하는 멀티모달 학습 방법을 구성하여 본 발명을 구현했다. 본 발명을 통해 각 입력 데이터에 대한 SubModel 혹은SubNet을 통해 추출된 feature 혹은 가공된 세 가지 모달의 입력 텐서를 적절히 fusion하고 모델을 학습하여 피 로 수준을 도출한다. 본 발명에 의한 방법은 TFN의 tensor fusion 방법을 활용하였지만 데이터의 유형과 각 입력에 대한 특징 추출 또는 데이터 가공 방법에 차이가 있다. TFN의 텐서는 각 모달에 대해 서브모델(SubModel)을 적용하고 vector embedding을 통해 도출된 feature이다. 그러나 본 발명에서는 별도의 SubNet을 적용하여 각 모달의 입력 데이터 에 대해 feature를 추출하는 것이 아니라 단지 입력 데이터의 차원을 축소하고 원본 데이터를 있는 그대로 표현 하는 것에 중점을 두었다. 이러한 방법은 텐서로 표현하기 힘든 정성적 데이터를 포함하여 실제 문제를 해결하 는 데 의미가 있다. 따라서 기존 TFN에서 제안하는 입력 처리 과정을 SubModel 방법으로 정의하여 본 발명의 첫 번째 모델(fTFN 1)을 정의하였으며, 이와 더불어 SubNet 방법을 적용하는 두 번째 모델(fTFN 2)도 제공한다. 본 발명의 성과 입증을 위해 두 가지 모델을 정의하고 비교 실험을 진행했다. 또한 본 발명에서는 단일 모달을 이 용해 피로 수준을 도출하는 Unimodal 방식에서의 실험을 통해 각 체계를 비교 분석했다. 그리고 Unimodal 체계 에서는 모달 간의 간섭 없이 각 모달이 별개로 피로 수준을 도출해 낼 수 있는지를 파악하기 위한 것으로 모달 의 입력 데이터를 텐서화 한 후 FCL을 통해 피로 수준을 도출했다. 이를 위하여 먼저 피로 수준을 측정할 대상자에 대하여 일정 시간 동안 동시에 촬영 및 녹음된 영상, 열상 및 음성을 상기 대상자의 분석대상으로 입력받는 제1단계를 수행한 후, 상기 분석대상으로 입력받은 영상 및 열상 을 프레임 단위로 분해하는 제2단계를 수행하도록 하는 것이 바람직하다. 그리고 상기 제2단계에서 분해된 영상 및 열상의 프레임 중에서 분석대상 영상프레임 및 분석대상 열상프레임을 선별하는 제3단계를 수행하도록 하는 것이 바람직하다. 상기 분석대상 영상프레임 및 상기 분석대상 열상프레임을 선별하는 상기 제3단계에서는, 일정 시간 단위로 분 석대상 프레임을 추출하도록 하는 것도 바람직하다. 그러나, 무작위 추출로 하거나 Key frame 추출방법을 적용 하는 것도 가능하다. 그 중 Key frame 추출방법은 동일 또는 유사한 프레임들 모두까지 분석할 필요가 없기 때 문에, 하나의 동영상에 포함된 모든 프레임 전체를 대상으로 하지 않고, 안면의 움직임이나 자세 등에 일정 크 기 이상의 변화가 있는 시점의 프레임 등과 같이 분석의 의미가 있는 키 프레임 이미지만을 따로 골라내어 피로 도를 측정하는 방법이다. 이에 관하여는 후술하기로 한다. 분석대상 영상프레임 및 분석대상 열상프레임을 선별하는 제3단계를 수행한 뒤에는, 각 모달별 SubModel 또는 SubNet을 이용하여 상기 분석대상 영상프레임으로부터 영상특징 텐서 를 생성하는 제4단계, 상기 분석대상으로 입력받은 음성데이터로부터 음성특징 텐서 를 생성하는 제5단계 및 상기 분석대상 열상프레임으로부터 열상특 징 텐서 를 생성하는 제6단계를 수행하도록 하는 것이 바람직하다. 각 모달에 대한 특징텐서들이 생성된 뒤에는 상기 영상특징 텐서 , 상기 음성특징 텐서 및 열상특징 텐서"}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "의 차원을 확장하는 제7단계를 수행하도록 하는 것이 바람직하다. 그리고 상기 제7단계에서 차원이 확장된 각 텐서들을 Tensor Fusion Network에 입력하여 Cartesian Product 방식으로 연산하여 융합텐서 을 생성해 내는 제8단계와 상기 융합텐서 를 완전연결층(Fully Connected Layer)으로 보내 피로도 수준을 분류시키는 제9단계를 수행하도록 하는 것이 바람직하다. 상기 제7단계에서, 상기 영상특징 텐서 , 상기 음성특징 텐서 및 열상특징 텐서 의 차원을 확장하는 것은 TFN 연산을 위한 것인데, 차원을 확장할 때는 확장된 차원의 값을 1로 채우도록 하는 것이 바람직하다. 이와 같이 각 모달의 입력데이터는 해당 SubModel 또는 SubNet으로 입력되어 가공된 텐서로 변환하는 것이 바람 직하며, 각 텐서는 TFN 연산을 위해 차원을 확장하되 확장된 차원의 값은 1로 채우도록 하는 것이 바람직하다. 그리고 차원이 확장된 각 모달의 텐서들은 TFN으로 입력시키는 것이 바람직하다, TFN 내부에서는 Cartesian Product 방식으로 세 입력에 대한 연산을 통해 fusion된 새로운 텐서 을 생성해 내도록 하는 것이 바람직 하다. 본 발명에서는 각 모달에 해당하는 텐서의 차원을 확장할 때, 1의 값으로 해당 차원을 채움으로써 기존의 각 Unimodal 텐서와 Biomodal 텐서의 값을 훼손시키지 않도록 하였다. 따라서 fusion된 텐서는 세 가지가 결합 되어 새롭게 생성된 Trimodal 텐서 뿐만 아니라 Unimodal 및 Biomodal에 해당하는 기존 정보를 그대로 유지하고 있다. 이후 fusion 된 텐서는 FCL에 입력되어 최종적으로 single sigmoid out 레이어를 적용하여 최종 피로 수 준을 도출할 수 있게 된다. 한편, 상기 제4단계 내지 상기 제6단계에서 각 모달의 입력데이터는 해당 SubModel(fTFN 1 모델) 또는 SubNet(fTFN 2 모델)으로 입력되어 가공된 텐서로 변환되는데, 먼저 fTFN 1모델 즉 SubModel로 입력받는 모델에 대하여 설명한다. 정량적 데이터로 표현되는 각 모달들은 각각의 다른 형태와 차원을 가지고 있다. 이러한 상황에서 합리적인 tensor fusion이 이루어지도록 하기 위해서 본 발명에서는 각 모달의 원본 데이터에서 feature를 추출하는 SubModel을 활용하는 방법을 제공한다. SubModel 방법은 입력에 대해 특정 모델을 적용하여 적절한 feature를 추출하고 vector embedding 한 후 입력 텐서로써 정의하는 방법이다. 영상, 음성, 열상 세 가지 입력에 대해 SubModel은 세 모달의 fusion을 위한 특징센서인 상기 영상특징 텐서 , 상기 음성특징 텐서 및 열상특징 텐 서 를 생성한다. 도 1에는 SubModel을 적용한 tensor fusion 과정과 tensor fusion을 통해 새롭게 표현되는 융합 텐서 이 도시되어 있다. 상기 서브모델 중 영상 임베딩 서브모델(Video Embedding SubModel)에서는 피로도 측정 대상자가 앉은채 정면을 바라보도록 한 후 일반적인 카메라를 통해 1분 정도를 촬영한 것을 사용하는 것이 바람직하다. 영상의 원본 해 상도는 1280 × 720로 하는 것도 가능하다. 그러나 그 이상 또는 그 이하의 해상도를 가진 영상으로 촬영하여 분석하는 것도 가능하다. 그리고 분석을 위하여 하나의 영상 데이터를 일정 시간 단위로 선별한 일정 개수(k 개)의 프레임으로 분해하여 분석하도록 하는 것이 바람직한데, 각 프레임은 분석을 위해 (H, W, 3) 크기의 3-채 널 이미지로 축소하도록 하는 것이 더욱 바람직하다. 그리고 서브모델은 영상의 특징을 추출하기 위해 CNN 계열 중 이미지 분석에서 최상위 성능을 보이고 있는 EfficientNet을 적용하도록 하는 것이 바람직하다. 각 영상에서 분해한 모든 프레임의 특징을 추출하고 vector embedding을 진행한 후 이들을 연결하여 전체 영상 하나에 대한 영상특징 텐서 를 생성해 내도록 하는 것이 바람직하다. 도 2에는 영상 임베딩 서브모델의 구성도가 도시되어 있다. 즉, 영상특징 텐서 를 생성하는 상기 제4단계는, 도 2에 도시된 바와 같이 EfficientNet을 이용하는 영상 임 베딩 서브모델이 상기 분석대상 영상프레임 을 3채널 이미지로 축소한 후, 벡터 형식으로 영상 특징요소 를 추출하여 벡터 임베딩을 진행하고, 상기 영상 특징요소 모두를 하나로 연결하여 상기 영상 특징텐서 를 다 음과 같이 생성하도록 하는 것이 바람직하다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "그리고 상기 서브모델 중 음성 임베딩 서브모델(Voice Embedding SubModel)에서, 음성데이터는 영상데이터에서 음성부분만 추출하여 사용하는 것도 바람직하다. 그러나, 별도의 녹음장비로 녹음한 음성으로부터 음성데이터를 추출하는 것도 가능하다. 1분간의 원본(raw) 음성 데이터는 T = 220,000 크기의 벡터로 나타 내어 진다. 음성 데이터의 경우 영상과 열상 데이터에 비해 그 크기가 매우 크기 때문에 음성 데이터의 축소 비율이 가용 리소스 안에서 tensor fusion을 위해 가장 중요하다고 할 수 있다. 음성 데이터에서는 Submodel을 활용한 방법의 경우 음성 데이터의 feature를 추출하기 위해 음성 데이터의 대표적인 지표인 F0-mean 값을 추출하여 사용하는 모델 을 제안한다. 도 3에 도시된 바와 같이 번째 음성 입력데이터 에 대해 특정 구간 별 F0-mean 값을 추출하는 것이다. 즉, 본 발명에서 음성특징 텐서 를 생성하는 제5단계는, 도 3에 도시된 바와 같이 상기 음성데이터 중 번째 음성데이터 에 대한 특정 구간 별 F0-mean 값을 추출하여 벡터 임베딩을 진행하고 이들을 하나로 연결하여 상 기 음성특징 텐서 를 다음과 같이 생성하도록 하는 것이 바람직하다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "상기 서브모델 중 열상 임베딩 서브모델(Thermal Image SubModel/SubNet)에서 열상 데이터는 열상 카메라를 사 용하여 촬영한 이미지 데이터이다. 그리고 분석을 위하여 하나의 영상 데이터를 일정 시간 단위로 선별한 일정 개수(k개)의 프레임으로 분해하여 분석하도록 하는 것이 바람직한데, 열상 이미지중 영상 데이터에서의 프레임 수와 동일한 k개의 열상 이미지를 랜덤 추출하여 사용하도록 하는 것이 바람직하다. 열상 이미지의 경우 영상 데이터의 프레임과 동일한 데이터 형태를 가지고 있어 열상 데이터의 SubModel은 영상 데이터의 방법과 동일하게 구성하여 상기 열상 특징텐서 를 생성하도록 하는 것이 바람직하다. 즉, 본 발명에서 상기 열상특징 텐서 를 생성하는 제6단계는, EfficientNet을 이용하는 열상 임베딩 서브모델 이, 상기 분석대상 열상프레임 에서 벡터 형식으로 열상 특징요소 를 추출하여 벡터 임베딩을 진행하고 열 상 특징요소 모두를 하나로 연결하여 상기 열상 특징텐서 를 다음과 같이 생성하도록 하는 것이 바람직하다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "한편 상술한 바와 같이 본 발명에서는, SubModel을 이용하여 특징텐서를 추출하여 tensor fusion하는 방법인 fTFN 1모델 외에 SubNet을 이용하여 특징텐서를 추출하는 fTFN 2모델도 제공한다. fTFN 1의 경우 각 모달 입력 에 대해 특징을 추출하고 있지만, fTFN 2 모델의 경우 SubNet을 활용하여 입력 데이터의 의미를 가능한 한 최대 로 유지하면서 해당 데이터를 축소하고 가공하여 fusion을 위한 텐서를 정의 하는 것에 그 목표가 있다. 영상, 음성, 열상 각 입력 데이터의 형태와 크기에는 큰 차이가 존재한다. 따라서 입력 데이터의 불균등성 문제 또한 고려되어야 하기 때문에 본 발명에서는 이러한 SubNet을 활용한 모델 fTFN 2를 제공하는 것이다. 상기 fTFN 2 모델에 포함되는 서브넷(SubNet) 중 영상 임베딩 서브넷(Video Embedding SubNet)은 입력 영상에 서 분해한 k개의 프레임에 대해서 도 5와 같이 DenseNet을 통한 채널확장과 Global Average Pooling (GAP) layer를 적용한 비교적 단순한 형태의 연산을 통해 분해된 모든 프레임을 1차원 벡터 로 축소한 후 해당 영상 의 모든 프레임으로 생성된 1차원 벡터들을 연결(concatenation)하여 전체 영상을 축소한 가공된 영상 특징텐서 를 생성하는 것이다. 즉, 상기 fTFN 2 모델에서 영상특징 텐서 를 생성하는 상기 제4단계는, 영상 임베딩 서브넷(Video Embedding SubNet)이, 상기 분석대상 영상프레임 에 대하여 DenseNet을 통한 채널확장과 Global Average Pooling (GAP) layer를 적용한 연산을 통하여 1차원 벡터 로 축소한 후 상기 1차원 벡터 모두를 하나로 연결하여 상기 영 상 특징텐서 를 다음과 같이 생성하도록 하는 것이 바람직하다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "그리고 상기 fTFN 2 모델에 포함되는 서브넷(SubNet) 중 음성 임베딩 서브넷(Voice Embedding SubNet)은, 입력 데이터를 축소하여 원 정보를 최대한으로 유지시킬 수 있는 가공된 입력 텐서를 생성하기 위해 도 6에 나타낸 간단한 형태의 Fully Connected Layer (FCL)만을 적용하도록 하는 것이 바람직하다. 음성특징 텐서 의 크기 | |가 SubModel 방법에서의 출력 텐서 크기와 동일하게 생성될 수 있도록 FCL의 출력층을 구성하도록 하는 것 이 바람직하다. 즉, 상기 fTFN 2 모델에서 음성특징 텐서 를 생성하는 상기 제5단계는, 도 6에서 보는 바와 같이 상기 음성 임베딩 서브넷이, 상기 음성데이터 중 번째 음성데이터 에 대한 데이터를 완전연결층(Fully Connected Laye r)으로 보내 상기 음성특징 텐서 를 생성하도록 하는 것이 바람직하다. 그리고 상기 fTFN 2 모델에 포함되는 서브넷(SubNet) 중 열상 임베딩 서브넷(Thermal Image SubNet)은 영상 데 이터의 방법과 동일하게 구성하여 열상 데이터에 대한 상기 영상특징 텐서 를 생성하도록 하는 것이 바람직하 다. 즉, 상기 fTFN 2 모델에서 열상특징 텐서 를 생성하는 상기 제5단계는, 열상 임베딩 서브넷이, 상기 분석대상 영상프레임 에 대하여 DenseNet을 통한 채널확장과 Global Average Pooling (GAP) layer를 적용한 연산을 통하여 1차원 벡터 로 축소한 후 상기 1차원 벡터 모두를 하나로 연결하여 상기 열상 특징텐서 를 다음과같이 생성하도록 하는 것이 바람직하다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "한편, 본 발명 중 분석대상 영상프레임 및 분석대상 열상프레임을 정하는 상기 제3단계에서는, 상술한 바 있는 ‘일정 간격마다 임의의 프레임을 추출하는 방법’이 아닌, ‘키 프레임을 사용하는 방법’을 사용하는 것도 바 람직하다. 상기 분석대상 영상프레임 및 분석대상 열상프레임을 정할 때 키 프레임을 사용하는 방법을 적용하게 되면 영상 및 열상 분석에 따른 컴퓨터 자원을 보다 효율적으로 활용할 수 있고, 분석시간을 단축할 수 있을 뿐 만 아니라 전체 영상 및 열상에서 피로도 수준을 측정할 때 더욱 정확한 값으로 측정할 수 있게 된다. 본 발명에서 키 프레임을 사용하는 방법은, 각각의 프레임 중에서 이전 프레임과 같거나 유사한 프레임은 분석 대상에서 제외하고 이전 프레임과 차이가 나기 때문에 분석의 의미가 있는 프레임만 선별하여 분석하되, 선별된 키 프레임 다음에 위치하는 동일 또는 유사한 프레임을 몇 개씩 가지는지도 파악하여 제공함으로써 전체 동영상 에서 피로도 평균 산출 시, 보다 정확한 값이 산출될 수 있도록 한다. 즉, 대상자가 표정의 변화도 없고, 움직이지도 않는 동안에 촬영된 각각의 프레임들은 모두 동일하거나 유사한 프레임이기 때문에 얼굴 영역도 동일하게 검출되고, 특징요소도 모두 동일하게 검출될 것이기 때문에 피로도를 측정하면 모두 동일한 값으로 나올 것이다. 따라서 본 발명에서는 프레임 이미지 중에서 피로도 측정에 사용하 기에 의미 있는, 움직임이 있거나 표정의 변화가 있어서 이전 프레임과 다르게 나타나는 프레임을 키 프레임으 로 하여 키프레임에 대한 이미지만을 추출하여 피로도를 측정하되, 키 프레임 뒤에 반복되는 동일 또는 유사한 프레임이 몇 개씩인지도 파악하여 피로도 평균 산출에 반영하도록 함으로써 피로도 평균의 산출의 정확도를 높 이도록 한 것이다. 이를 위하여 본 발명에서는 칼라 히스토그램(color histogram)을 이용하는 방법과 함께 Gaussian Mixture Model (GMM)을 이용하는 방법도 제공한다. 먼저 칼라 히스토그램을 이용하는 키 프레임 생성방법은, 도 7에서 보는 바와 같이 칼라 히스토그램(Color Histogram) 색상분포도를 활용하여 프레임들을 검사하고, 이전 프레임과의 유사도 차이가 크게 나타나는 프레임 을 키 프레임으로 선정하고, 선정된 키 프레임들에 대하여는 동일 또는 유사한 프레임의 개수까지도 같이 제공 하는 것이다. 색상분포도는 화상 내부의 픽셀이 가질 수 있는 모든 픽셀 값들에 대해서, 각 값들의 출현 빈도를 나타내는데 사용한다. 이러한 색상분포도는 화상 내 픽셀들의 대략적인 분포를 확인할 수 있고 다른 화상과의 비교 시 활용될 수 있다. 칼라 히스토그램을 이용하여 키 프레임을 찾아내기 위해서는 먼저 영상의 프레임에 해당하는 색상분포도를 생성 하는데, 프레임의 흐름인 영상 데이터는 색상분포도의 흐름으로 확인할 수 있다. 이 흐름에서 큰 변화가 발생하 는 시점을 키 프레임으로 추출하게 되는데, cosine 유사도를 활용하여 전후 프레임의 색상분포도의 유사도를 계 산하고, 설정된 임계값(threshold)에 따라 프레임의 변화가 의미 있는 것인지 판단하도록 하는 것이 바람직하다. 이같이 본 발명은 프레임 단위의 색상분포도를 활용하므로 처리 속도가 빠르고, 사전에 특정 데이 터에 의존되는 함수나 전처리 방법을 사용하지 않으므로 여러 데이터셋의 활용을 위한 구현이 용이하다. 이를 위하여, 입력된 동영상을 각각의 프레임으로 분리한 후 각 프레임 이미지에 대한 색상분포도를 생성한다. 이후 생성된 색상분포도를 영상의 시간 순서에 따라 나열하게 되고, 각 색상분포도를 이전의 것과 비교하여 유 사도를 측정하게 된다. 여기서 측정된 유사도가 설정된 임계값보다 작은 경우, 이전 영상과 비교해 볼 때 유사 성이 낮은 경우 해당 프레임은 키 프레임으로 추출된다. 도 8에는 본 발명에 포함되는 칼라 히스토그램(Color Histogram)을 이용한 키 프레임 생성방법이 수행되는 구체 적인 절차 흐름이 도시되어 있다. 이하에서는 도 8을 참조하여 설명한다. 먼저, 대상자의 안면을 포함하는 동영 상을 분석대상 영상으로 입력받는 상기 제1단계(s110)를 수행한 뒤에, 상기 분석대상 영상을 프레임 단위로 분 해하는 상기 제2단계(s120)를 수행한 뒤에, 상기 제2단계에서 프레임 단위로 분해된 M개의 프레임 이미지( ) 각각에 대하여 칼라 히스토그램을 생성하여 시간 순서에 따른 M개의 칼라 히스토그램( )을 만드는 제1과정 (s130)을 수행하도록 하는 것이 바람직하다. 그리고 상기 정보시스템은, m = 1에서 m = M이 될 때까지 아래의 과정을 반복하는 제2과정(s140 ~ s148)을 수행 하도록 하는 것이 바람직하다. 즉, 상기 M개의 칼라 히스토그램( ) 중 m번째 칼라 히스토그램( )을 m+n번째(n은 1부터 시작) 칼라 히스토그 램( )과 비교한 유사도를 m번째 프레임 이미지( )에 대한 유사도( )로 하는 제2-1과정(s141)을 수행한 뒤, 상기 유사도( )가 임계값보다 큰 경우(s142) m+n번째 프레임( )을 유사프레임으로 정한 후, n+1을 n 으로 하여(s143) 상기 제2-1과정부터 다시 반복하는 제2-2과정을 수행하도록 하는 것이 바람직하다. 그러나 상기 유사도( )가 상기 임계값보다 작은 경우에는(s142) 상기 m번째 프레임 이미지( )를 키 프레임 ( )으로 선별하고 n을 상기 유사프레임의 개수로 한 후(s144), m + n을 m으로 하고(s146), n을 다시 1로 하 여(s147) 상기 제2-1과정부터 다시 반복하는 제2-3과정을 수행하되, m이 M이 되기까지 수행하도록 하는 것이 바 람직하다(s148). 그리고, 상기 제2과정이 완료된 후에는 상기 제2과정에서 선별된 상기 키 프레임( )을 상기 분석대상 프레임 으로 제공하는 제3과정(s150)을 수행하도록 하는 것이 바람직하다. 여기서 상기 유사도( )는 코사인 유사도를 이용하여 아래 식과 같이 측정되도록 하는 것이 바람직하다."}
{"patent_id": "10-2022-0147451", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "한편, 상기 제9단계에서, 피로도 수준을 분류하는 경우에는 상기 키 프레임( )별로 상기 유사프레임의 개수 (n)을 반영하여 최종적인 피로도 수준을 산출하는 것도 바람직하다. 예를 들어 상기 피로도 수준의 평균을 구할 때는, 각각의 키 프레임( )의 피로도 수준에 각각의 유사프레임의 개수(n)까지 감안하여 각각의 키 프레임 ( )과 그에 따른 유사프레임에 대한 피로도 수준의 합계를 낸 후( ), 모든 키 프레임( )별로 상 기 유사프레임의 개수(n)까지 감안한 상기 피로도 수준의 합계를 다 더한 값에 영상 내 모든 프레임 수( )로 나눈 값을 상기 피로도 수준의 평균으로 하는 것이다. 또한, 가장 많이 나타나는 피로도 수준으로 피로도 수준 을 정하는 경우에도 같은 방식으로 각각의 키 프레임( )의 피로도 수준에 각각의 유사프레임의 개수(n)까지 감안한 피로도 수준의 개수를 산출한 후 가장 많이 나타나는 피로도 수준을 구하는 것이다. 한편, 상술한 바와 같이, 상기 제3단계에서 상기 분석대상 프레임을 선별하는 또 다른 방법으로 가우시안 혼합 모델(Gaussian Mixture Model, GMM)을 활용하는 방법도 있다. 이에 대하여 도 9 및 도 10을 이용하여 설명한다. 이 방법 또한, 사람의 안면을 포함하여 촬영한 동영상 정보에서 안면의 움직임에 일정 크기 이상의 변화가 있는 시점의 프레임 이미지만 추출하여 분석용 정보로 제공하는 방법으로서, GMM을 활용하여 프레임의 흐름을 분석한 후, 프레임 간의 전후 차이가 급격하게 나타나는 부분을 발견하여 해당 프레임을 키 프레임으로 추출하게 된다. 도 9에는 이에 대한 개념이 도시되어 있다. 우선 입력영상에 대해 프레임으로 분할한 후 GMM을 이용하여 각 프 레임에서 객체(object)를 제거하고 배경을 추출하게 된다. 여기서 추출된 배경의 픽셀 값만큼 원본 프레임에서 차감함으로써 배경이 제거된 객체 중심의 프레임을 추출할 수 있다. 그리고, 객체만 존재하는 각 프레임에서 모 든 픽셀을 합 연산하여 해당 프레임의 행동정보(Motion Information)를 정의한다. 여기서 프레임의 묶음 (Window)을 활용하여 순차적으로 정해진 수의 크기만큼 프레임들이 가진 행동정보를 확인하고 해당 묶음에서 가 장 큰 지역 최대값(local maxima)을 선정하게 된다. 이와 같은 방법으로 각 묶음당 최대값을 가지는 프레임이 키프레임으로 추출된다. 일반적인 기술에 의한 키 프레임 추출에 있어서는, 프레임의 흐름에서 큰 변화가 나타 나는 것을 감지하기 힘든 경우가 있는데, 본 발명에서는 움직임의 정보를 활용하기 때문에 이를 보완하는 효과 를 확인할 수 있다. 도 10에는 가우시안 혼합 모델을 이용한 영상정보 분석용 키 프레임 생성방법이 수행되는 절차 흐름이 도시되어 있다. 먼저, 대상자의 안면을 포함하는 동영상을 분석대상 영상으로 입력받는 상기 제1단계(s210)와 상기 분석 대상 영상을 프레임 단위로 분해하는 상기 제2단계(s220)를 수행하고 난 뒤에, 상기 제2단계에서 프레임 단위로 분해된 상기 프레임 이미지( ) 각각에 대하여 Gaussian Mixture Model(GMM)을 이용하여 객체(object)를 제거 하고 배경을 추출하는 제1a과정을 수행하도록 하는 것이 바람직하다(s230). 그리고, 상기 프레임 이미지( ) 각각의 원본 프레임에서 상기 배경의 픽셀 값만큼 차감하여 객체중심 프레임을 추출하는 제2a과정(s240)을 수행하고, 그 다음에는 상기 프레임 이미지( ) 각각의 객체중심 프레임에서 모든픽셀을 합 연산하여 상기 프레임 이미지( ) 각각에 대한 행동정보( )를 정의하는 제3a과정(s250)을 수행하 도록 하는 것이 바람직하다. 그리고, 상기 프레임 이미지( ) 각각에 대한 행동정보( )들을 일정 개수(N개) 단위로 묶어 윈도우 (windows)를 만드는 제4a과정을 수행한 뒤(s261), 상기 윈도우에 포함된 상기 행동정보( )들의 묶음 중에서 가장 큰 지역 최대값(local maxima)을 가지는 행동정보( )에 대응되는 m번째 프레임 이미지를 키 프레임 ( )으로 선별하는 제5a과정(s262)을 수행하도록 하는 것이 바람직하다. 상기 제5a과정(s262)을 수행하고 난 뒤에는 상기 윈도우 내에서 상기 키 프레임( )의 뒤프레임이 존재하는 경 우 상기 뒤프레임을 상기 키 프레임( )의 유사프레임으로 하는 제6a과정(s263) 및 상기 윈도우 내에서 상기 키 프레임( )의 앞프레임이 존재하는 경우 상기 앞프레임을 상기 키 프레임( ) 이전 키 프레임( )의 유사프레임으로 하는 제7a과정(s264)과정을 수행하도록 하는 것이 바람직하다. 그리고, 마지막으로 상기 제5a과정에서 선별된 상기 키 프레임( )들을 상기 분석대상 프레임으로 제공하는 제 8a과정(s265)을 수행한 뒤, 그 다음 N개의 프레임으로 상기 제4a과정 내지 상기 제7a과정 수행을 반복하는 것이 바람직하다. 그리고 상기 제9단계에서, 상기 피로도 수준의 평균 또는 상기 가장 많이 나타나는 피로도 수준을 산출하는 경우에는, 상기 히스토그램을 이용하는 방법에서 설명한 바와 같이, 상기 키 프레임( )별 상기 유사 프레임의 개수(n)을 반영하여 산출하도록 하는 것이 바람직하다. 이같이 칼라 히스토그램 또는 가우시안 혼합모델을 이용하여 선별된 키 프레임들을 상기 분석대상 프레임으로 제공하게 되면, 움직임이 없는 동일한 프레임에 대한 분석은 생략하고 의미있는 프레임 이미지에 대하여만 피로 도 측정이 가능하므로 컴퓨터 자원을 효율적으로 사용할 수 있고 빠른 시간 내에 분석이 가능해진다. 반면에 키 프레임 뒤에 반복되는 동일 또는 유사한 프레임이 몇 개씩인지도 파악하여 피로도 평균 산출에 반영할 수 있으 므로 피로도 평균의 산출 시 그 정확도를 높일 수 있게 된다. 이하에서는 실험예를 통하여 본 발명을 보다 상세하게 설명한다. 이하에서 설명되는 실험예는 본 발명의 이해를 돕기 위하여 제시된 것이며, 본 발명은 여기서 설명되는 실험예와 다르게 다양하게 변형되어 실시될 수 있음이 이해되어야 할 것이다. 이같이 본 발명의 범주 및 기술사상 범위 내에서 다양한 변경 및 수정이 가능함은 본 기 술분야에서 통상의 지식을 가진 자에게 있어서 명백한 것이며, 이러한 변형 및 수정이 첨부된 특허청구범위에 속하는 것도 당연한 것이다. &lt;실험예&gt; 1. Fatigue Level Dataset 본 실험을 위해 피로와 관련된 인간의 신호를 수집하기 위해 개발된 생체 신호 수집 시스템을 사용했다. 본 실 험을 위해서 수집 대상자들을 신중하게 선정하였고, 해당 대상자들로부터 피로 상태와 관련된 그들의 데이터를 수집하도록 요청했다. 도 11에는 데이터 수집 프로세스가 도시되어 있다. 데이터 수집을 위한 정보는 도 12와 같이 데이터 수집 장치의 윈도우에 표시된다. 대상자들은 약 1분 간의 주어 진 대본을 읽도록 요청받으며 시작 후 영상 및 열화상이 녹화된다. 녹화가 끝난 후 대상자들은 주관적 피로 수 준을 1(Excellent)에서 5(Extreme fatigue)까지 선택한다. 다음으로 Daily Multiphasic Fatigue Inventory (DFMI), Psychomotor Cognitive Test (PCT), 혈액 및 타액 샘 플을 이용해 실제 피로도를 분석했다. 이러한 과정을 통해 데이터를 수집하여 실험에 사용하였으며, 1개의 데이 터셋은 1분간의 영상, 음성, 열상 그리고 피로 수준(1~5)을 포함한다. 본 실험에서는 피로 수준 1과 5에 해당하 는 데이터만을 사용하였으며, 총 n=3,450개의 데이터셋을 실험에 사용했다. 2. Model Tunning 본 실험은 피로 수준 1과 5를 분류하기 위한 것으로 모든 모델은 binary cross-entropy loss를 사용하여 학습했 다. Optimizer로써는 Adam을 적용하였으며 학습은 배치크기 16으로 100 Epoch 동안 진행했다. 구체적인 SubModel/SubNet 설정 환경은 다음과 같다. Video/Thermal Embedding SubModel/SubNet: 영상 데이터는 1분간 데이터 수집 대상자가 앉은 채 정면에서 일반 적인 카메라를 통해 촬영한 것으로 그 해상도는 1280 × 720이다. 열상 데이터 또한 영상 데이터와 유사한 형태 를 가지고 있으므로 같은 방법을 적용했다. 본 실험에서는 영상 및 열상 데이터의 각 데이터당 사용 프레임(이 미지) 수를 k=60개로 설정하여 분석을 진행하였으며, 각 프레임(이미지)를 EfficientNet 입력층에 맞추어 W=224, H=224 크기의 3채널 이미지로 축소하여 사용했다. SubModel 방법에서는 각 이미지별 가공된 특징텐서의 크기가 16이 되도록 FCL의 출력층을 조정했다. Video SubNet의 경우 DenseNet-121 모델을 적용하여 W’=7, H’ =7, C’=16의 크기로 프레임의 채널을 확장했고 GAP를 통해 최종 출력 벡터의 크기를 결정했다. Voice Embedding SubModel/SubNet: 음성 데이터의 경우 원본 데이터의 크기가 T= 220,000으로 매우 큰 크기를 가지고 있다. 이 경우 fusion시 리소스 한계 문제 해결을 위해 다른 두 모달인 영상과 열상에 비해 매우 높은 축소 비율을 필요로 한다. 본 실험에서는 SubModel의 경우 F0-mean 값의 추출 범위를 t=10,000으로 설정하여 feature를 생성했다. SubNet에서는 FCL의 출력 크기를 22로하여 음성 데이터의 가공된 음성특징 텐서 크기는 | | = 22가 되도록 했다. Experimental Results 피로 수준 분류 실험에서 세 모달 영상(V), 음성(A), 열상(H)의 조합에 따른 Unimodal 환경에서의 성능, Multimodal(TFN) 환경에서의 모델 성능은 도 13과 같다. 모델 평가는 20%의 테스트 데이터셋을 대상으로 실시했 다. 실험 결과로는 멀티모달 체계에서의 분류 성능이 각 단일 모달 분류 체계에서의 성능보다 크게 증가한 것을 확인할 수 있었다. 인간의 피로는 정성적 상태의 값으로 데이터 수집 대상자 본인의 감각에 의해 주관적인 판단 이 개입할 확률이 매우 크다. 이러한 관점에서 실험 결과를 통해 멀티모달 체계가 인간의 감각이나 감정을 더욱 현실적으로 반영할 수 있는 것으로 볼 수 있다. 또한 정량적 상태의 영상, 이미지 그리고 음성 데이터를 통해 새로운 정성적 상태를 분류해 낼 수 있는점에서 멀티모달 체계의 가능성을 파악할 수 있었다. 상술한 여러 가지 예로 본 발명을 설명하였으나, 본 발명은 반드시 이러한 예들에 국한되는 것이 아니고, 본 발 명의 기술사상을 벗어나지 않는 범위 내에서 다양하게 변형 실시될 수 있다. 따라서 본 발명에 개시된 예들은 본 발명의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위한 것이고, 이러한 예들에 의하여 본 발명의 기 술 사상의 범위가 한정되는 것은 아니다. 본 발명의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술사상은 본 발명의 권리범위에 포함되는 것으로 해석되어야 한다."}
{"patent_id": "10-2022-0147451", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 포함되는 서브모델로 각 모달의 특징텐서를 추출하여 Tensor Fusion으로 융합텐서를 생성하는 개념을 보여주는 개념도이다. 도 2는 본 발명에 포함되는 영상 임베딩 서브모델에 대한 구성도이다. 도 3은 본 발명에 포함되는 음성 임베딩 서브모델에 대한 구성도이다. 도 4는 본 발명에 포함되는 서브넷으로 각 모달의 특징텐서를 추출하여 Tensor Fusion으로 융합텐서를 생성하는 개념을 보여주는 개념도이다. 도 5는 본 발명에 포함되는 영상 임베딩 서브넷에 대한 구성도이다. 도 6은 본 발명에 포함되는 음성 임베딩 서브넷에 대한 구성도이다. 도 7은 본 발명에서, 칼라 히스토그램을 이용하여 키 프레임이 생성되는 프레임웍을 도시한 것이다. 도 8은 본 발명에서, 칼라 히스토그램을 이용하여 키 프레임 생성이 수행되는 절차 흐름도이다. 도 9는 본 발명에서, 가우시안 혼합모델을 이용하여 키 프레임이 생성되는 프레임웍을 도시한 것이다. 도 10은 본 발명에서, 가우시안 혼합모델을 이용하여 키 프레임 생성이 수행되는 절차 흐름도이다. 도 11은 본 발명을 검증하기 위한 실험에서 데이터 수집절차를 도시한 것이다. 도 12은 본 발명을 검증하기 위한 실험에서 영상, 열상 및 음성 데이터 수집 사례를 도시한 것이다. 도 13는 본 발명을 검증하기 위한 실험에서 각 모달의 측정결과를 비교한 것이다."}
