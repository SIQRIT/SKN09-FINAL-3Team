{"patent_id": "10-2023-0042994", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0147282", "출원번호": "10-2023-0042994", "발명의 명칭": "가상 및 현실 공간의 동기화를 위한 전자 장치 및 그 동작 방법", "출원인": "국방과학연구소", "발명자": "황규환"}}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서,적어도 하나의 표식에 대한 소정의 센서가 장착된 무인 자율 장치의 접근을 감지하는 단계;상기 감지에 기초하여 상기 센서로부터 측정된 상기 무인 자율 장치 주변 공간에 관한 제1센싱 정보를 획득하는단계;소정의 3D 시뮬레이터를 이용하여 적어도 하나의 객체, 상기 무인 자율 장치에 대응하는 가상 장치, 및 상기 표식에 대응하는 가상 표식을 포함하는 가상 공간을 생성하는 단계;상기 제1센싱 정보, 및 상기 센서에 대응하는 상기 가상 장치에 장착된 가상 센서로부터 측정된 상기 가상 공간에 관한 제2센싱 정보를 혼합하는 단계; 및상기 혼합된 정보를 상기 무인 자율 장치로 제공하는 단계를 포함하는, 동작 방법."}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 적어도 하나의 표식은,기 설정된 위치에 배치되고,상기 감지하는 단계는,상기 무인 자율 장치로부터 수신한 gps 신호에 기초하여 상기 무인 자율 장치가 상기 적어도 하나의 표식으로부터 기 설정된 거리 내에 접근하는지를 감지하는 단계를 포함하고,상기 획득하는 단계는,상기 무인 자율 장치가 상기 적어도 하나의 표식으로부터 기 설정된 거리 내에 접근한 경우, 상기 제1센싱 정보를 획득하는 단계를 포함하는, 동작 방법."}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 적어도 하나의 객체는,소정의 시나리오에 따라 행동하도록 생성되고,상기 혼합된 정보는,상기 시나리오에 대한 상기 무인 자율 장치의 대응을 테스트하기 위해 사용되는, 동작 방법."}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 3D 시뮬레이터는,가제보(Gazebo) 시뮬레이터, 언리얼(Unreal) 시뮬레이터, 및 이노(Inno) 시뮬레이터 중 하나를 포함하는, 동작방법."}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,공개특허 10-2024-0147282-3-상기 소정의 센서는,라이더(LiDAR) 센서 및 카메라 센서를 포함하고,상기 제1센싱 정보는,상기 라이더 센서로부터 측정된 상기 주변 공간에 관한 제1포인트 클라우드(Point Cloud) 정보, 및 상기 카메라센서로부터 측정된 상기 주변 공간에 관한 제1이미지 정보를 포함하고,상기 제2센싱 정보는,상기 라이더 센서에 대응하는 가상 라이더 센서로부터 측정된 상기 가상 공간에 관한 제2포인트 클라우드 정보,및 상기 카메라 센서에 대응하는 가상 카메라 센서로부터 측정된 상기 가상 공간에 관한 제2이미지 정보를 포함하고,상기 혼합하는 단계는,상기 제1포인트 클라우드 정보 및 상기 제2포인트 클라우드 정보를 혼합하고, 상기 제1이미지 정보 및 상기 제2이미지 정보를 혼합하는 단계를 포함하는, 동작 방법."}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 소정의 센서는,gps 센서, IMU(Inertial Measurement Unit) 센서, 및 주행기록계(Odometry) 센서를 더 포함하고,상기 획득하는 단계는,상기 gps 센서, IMU 센서, 및 주행기록계 센서를 이용하여 상기 무인 자율 장치의 위치에 관한 정보, 및 상기무인 자율 장치의 회전에 관한 정보를 획득하는 단계를 포함하고,상기 생성하는 단계는,상기 3D 시뮬레이터를 이용하여 상기 위치에 관한 정보 및 상기 회전에 관한 정보를 기초로 상기 표식의 중심의좌표를 기준으로 상기 가상 공간을 생성하는 단계를 포함하고,상기 가상 공간을 상기 주변 공간과 동기화하기 위해, 상기 제1이미지 정보 내의 표식과 상기 제2이미지 정보내의 가상 표식을 비교하여 상기 가상 센서를 조절하는 단계를 더 포함하는, 동작 방법."}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 회전에 관한 정보는,요(yaw)에 관한 정보 및 피치(pitch)에 관한 정보를 포함하고,상기 조절하는 단계는,상기 제1이미지 정보 내 상기 표식의 중심의 위치와 상기 제2이미지 정보 내 상기 가상 표식의 중심의 위치가일치하도록 상기 가상 장치의 요 및 피치 중 적어도 하나를 조정함으로써 상기 가상 센서를 조절하는 단계를 포함하는, 동작 방법."}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 회전에 관한 정보는,롤(roll)에 관한 정보, 요에 관한 정보, 및 피치에 관한 정보를 포함하고,상기 적어도 하나의 표식은,소정의 개수 이상의 축을 포함하는 적어도 하나의 표식을 포함하고,공개특허 10-2024-0147282-4-상기 조절하는 단계는,상기 제1이미지 정보 내 상기 표식의 각각의 축과 상기 제2이미지 정보 내 상기 가상 표식의 각각의 축이 일치되도록 상기 가상 장치의 롤, 요, 및 피치 중 적어도 하나를 조정함으로써 상기 가상 센서를 조절하는 단계를포함하는, 동작 방법."}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서,상기 가상 센서의 조절이 완료된 경우, 상기 제2이미지 정보 내에서 상기 가상 표식을 제거하는 단계를 더 포함하는, 동작 방법."}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "가상 및 현실 공간의 동기화를 위한 전자 장치로서,적어도 하나의 프로그램이 저장된 메모리; 및상기 적어도 하나의 프로그램을 실행함으로써, 적어도 하나의 표식에 대한 소정의 센서가 장착된 무인 자율 장치의 접근을 감지하고,상기 감지에 기초하여 상기 센서로부터 측정된 상기 무인 자율 장치 주변 공간에 관한 제1센싱 정보를획득하고,소정의 3D 시뮬레이터를 이용하여 적어도 하나의 객체, 상기 무인 자율 장치에 대응하는 가상 장치, 및 상기 표식에 대응하는 가상 표식을 포함하는 가상 공간을 생성하고,상기 제1센싱 정보, 및 상기 센서에 대응하는 상기 가상 장치에 장착된 가상 센서로부터 측정된 상기 가상 공간에 관한 제2센싱 정보를 혼합하고, 및상기 혼합된 정보를 상기 무인 자율 장치로 제공하는 프로세서를 포함하는, 전자 장치."}
{"patent_id": "10-2023-0042994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한컴퓨터로 읽을 수 있는 비일시적 기록매체로서,상기 동작 방법은,적어도 하나의 표식에 대한 소정의 센서가 장착된 무인 자율 장치의 접근을 감지하는 단계;상기 감지에 기초하여 상기 센서로부터 측정된 상기 무인 자율 장치 주변 공간에 관한 제1센싱 정보를 획득하는단계;소정의 3D 시뮬레이터를 이용하여 적어도 하나의 객체, 상기 무인 자율 장치에 대응하는 가상 장치, 및 상기 표식에 대응하는 가상 표식을 포함하는 가상 공간을 생성하는 단계;상기 제1센싱 정보, 및 상기 센서에 대응하는 상기 가상 장치에 장착된 가상 센서로부터 측정된 상기 가상 공간에 관한 제2센싱 정보를 혼합하는 단계; 및상기 혼합된 정보를 상기 무인 자율 장치로 제공하는 단계를 포함하는, 비일시적 기록매체."}
{"patent_id": "10-2023-0042994", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서, 적어도 하나의 표식에 대한 소정의 센서가 장착된 무인 자율 장치의 접근을 감지하는 단계; 감지에 기초하여 센서로부터 측정된 무인 자율 장치 주변 공간에 관한 제1센싱 정보를 획득하는 단계; 소정의 3D 시뮬레이터를 이용하여 적어도 하나의 객체, 무인 자율 장치에 대응하는 가상 장치, 및 표식에 대응하는 가상 표식을 포함하는 가상 공간을 생성하는 단계; 제1센싱 정보, 및 센서에 대응하는 가상 장치에 장착된 가상 센서로부터 측정된 가상 공간에 관한 제2센싱 정보 를 혼합하는 단계; 및 혼합된 정보를 무인 자율 장치로 제공하는 단계를 포함하는 동작 방법이 제공될 수 있다."}
{"patent_id": "10-2023-0042994", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법에 관한 것이다."}
{"patent_id": "10-2023-0042994", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "근래 무인 자율 플랫폼(본원에서 \"무인 자율 장치\"라고도 지칭됨)에 대한 관심이 크게 증가하는 추세이다. 개발 자들은 보다 안전하고 효율적인 인공지능 알고리즘을 개발하여 이를 실제 플랫폼을 대상으로 검증하고 있다. 다만, 실제 플랫폼을 대상으로 개발된 알고리즘을 다양한 시나리오에 따라 검증하는 것은 안전 관련 위험 요소가 크고 막대한 비용이 소모될 수 있다. 특히 군용 무인 자율 플랫폼은 기존의 민간 무인 자율 플랫폼과는 달리 야 외 험지 등 전장에 노출될 수 있어 새로운 테스트 방법이 요구된다. 이러한 요구에 따라, 가상 현실에서 무인 자율 플랫폼을 검증하려는 다양한 방법이 제시되고 있다. 가상 현실 기술은 개발 대상 무인 자율 플랫폼과 그 운용환경을 모두 가상화할 수 있다. 그러나 가상 현실 기술만을 사용 할 경우, 가상화한 모델과 실제 플랫폼의 근접 정도 및 가상화한 모델의 실험 결과에 관한 신뢰성에 대해 입증 이 필요하다는 문제점이 존재한다. 이에 비해, 증강 현실 기술은 일부 객체 만을 가상화하는 기술로 가상 현실 기술만을 사용하는 경우 보다 높은 신뢰성을 부여할 수 있다. 따라서 증강 현실 기술을 이용한 무인 자율 플랫 폼 테스트를 고려할 수 있다. 증강 현실 기술을 이용하여 무인 자율 플랫폼을 테스트할 경우, 가상 및 현실 공 간의 높은 수준의 동기화가 필요할 수 있다. 이를 위해, 실시간으로 가상 및 현실 공간을 동기화하여 혼합된 영 상을 플랫폼에게 제공하는 장치의 개발에 대한 필요성이 요구된다."}
{"patent_id": "10-2023-0042994", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 명세서의 실시예들은 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법을 개시하고자 한다. 보다 구체적으로 본 명세서의 실시예는 소정의 표식을 이용하여 가상 센서를 조절함으로써 가상 및 현실 공간을 동기화를 위한 전자 장치의 동작 방법을 제공하는 것을 목적으로 한다. 본 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 이하의 실시예 들로부터 또 다른 기술적 과제들이 유추될 수 있다."}
{"patent_id": "10-2023-0042994", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서, 적어도 하나의 표식에 대 한 소정의 센서가 장착된 무인 자율 장치의 접근을 감지하는 단계; 감지에 기초하여 센서로부터 측정된 무인 자 율 장치 주변 공간에 관한 제1센싱 정보를 획득하는 단계; 소정의 3D 시뮬레이터를 이용하여 적어도 하나의 객 체, 무인 자율 장치에 대응하는 가상 장치, 및 표식에 대응하는 가상 표식을 포함하는 가상 공간을 생성하는 단 계; 제1센싱 정보, 및 센서에 대응하는 가상 장치에 장착된 가상 센서로부터 측정된 가상 공간에 관한 제2센싱 정보를 혼합하는 단계; 및 혼합된 정보를 무인 자율 장치로 제공하는 단계를 포함하는 동작 방법이 제공될 수 있다. 일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서, 적어도 하나의 표식은 기 설정된 위치에 배치되고, 감지하는 단계는, 무인 자율 장치로부터 수신한 gps 신호에 기초하여 무인 자율 장치 가 적어도 하나의 표식으로부터 기 설정된 거리 내에 접근하는지를 감지하는 단계를 포함하고, 획득하는 단계는, 무인 자율 장치가 적어도 하나의 표식으로부터 기 설정된 거리 내에 접근한 경우, 제1센싱 정보를 획득 하는 단계를 포함하는 동작 방법이 제공될 수 있다. 일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서, 적어도 하나의 객체는, 소 정의 시나리오에 따라 행동하도록 생성되고, 혼합된 정보는, 시나리오에 대한 무인 자율 장치의 대응을 테스트 하기 위해 사용되는 동작 방법이 제공될 수 있다. 일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서, 3D 시뮬레이터는, 가제보 (Gazebo) 시뮬레이터, 언리얼(Unreal) 시뮬레이터, 및 이노(Inno) 시뮬레이터 중 하나를 포함하는 동작 방법이 제공될 수 있다. 일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서, 소정의 센서는, 라이더 (LiDAR) 센서 및 카메라 센서를 포함하고, 제1센싱 정보는, 라이더 센서로부터 측정된 주변 공간에 관한 제1포 인트 클라우드(Point Cloud) 정보, 및 카메라 센서로부터 측정된 주변 공간에 관한 제1이미지 정보를 포함하고, 제2센싱 정보는, 라이더 센서에 대응하는 가상 라이더 센서로부터 측정된 가상 공간에 관한 제2포인트 클라우드 정보, 및 카메라 센서에 대응하는 가상 카메라 센서로부터 측정된 가상 공간에 관한 제2이미지 정보를 포함하고, 혼합하는 단계는, 제1포인트 클라우드 정보 및 제2포인트 클라우드 정보를 혼합하고, 제1이미지 정보 및 제2이미지 정보를 혼합하는 단계를 포함하는 동작 방법이 제공될 수 있다.일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서, 소정의 센서는, gps 센서, IMU(Inertial Measurement Unit) 센서, 및 주행기록계(Odometry) 센서를 더 포함하고, 획득하는 단계는, gps 센서, IMU 센서, 및 주행기록계 센서를 이용하여 무인 자율 장치의 위치에 관한 정보, 및 무인 자율 장치의 회 전에 관한 정보를 획득하는 단계를 포함하고, 생성하는 단계는, 3D 시뮬레이터를 이용하여 위치에 관한 정보 및 회전에 관한 정보를 기초로 표식의 중심의 좌표를 기준으로 가상 공간을 생성하는 단계를 포함하고, 가상 공간 을 주변 공간과 동기화하기 위해, 제1이미지 정보 내의 표식과 제2이미지 정보 내의 가상 표식을 비교하여 가상 센서를 조절하는 단계를 더 포함하는 동작 방법이 제공될 수 있다. 일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서, 회전에 관한 정보는 요 (yaw)에 관한 정보 및 피치(pitch)에 관한 정보를 포함하고, 조절하는 단계는, 제1이미지 정보 내 표식의 중심 의 위치와 제2이미지 정보 내 가상 표식의 중심의 위치가 일치하도록 가상 장치의 요 및 피치 중 적어도 하나를 조정함으로써 가상 센서를 조절하는 단계를 포함하는 동작 방법이 제공될 수 있다. 일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서, 회전에 관한 정보는, 롤 (roll)에 관한 정보, 요에 관한 정보, 및 피치에 관한 정보를 포함하고, 적어도 하나의 표식은, 소정의 개수 이 상의 축을 포함하는 적어도 하나의 표식을 포함하고, 조절하는 단계는, 제1이미지 정보 내 표식의 각각의 축과 제2이미지 정보 내 가상 표식의 각각의 축이 일치되도록 가상 장치의 롤, 요, 및 피치 중 적어도 하나를 조정함 으로써 가상 센서를 조절하는 단계를 포함하는 동작 방법이 제공될 수 있다. 일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법으로서, 가상 센서의 조절이 완료 된 경우, 제2이미지 정보 내에서 가상 표식을 제거하는 단계를 더 포함하는 동작 방법이 제공될 수 있다. 일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치로서, 적어도 하나의 프로그램이 저장된 메모리; 및 적어도 하나의 프로그램을 실행함으로써, 적어도 하나의 표식에 대한 소정의 센서가 장착된 무인 자 율 장치의 접근을 감지하고, 감지에 기초하여 센서로부터 측정된 무인 자율 장치 주변 공간에 관한 제1센싱 정 보를 획득하고, 소정의 3D 시뮬레이터를 이용하여 적어도 하나의 객체, 무인 자율 장치에 대응하는 가상 장치, 및 표식에 대응하는 가상 표식을 포함하는 가상 공간을 생성하고, 제1센싱 정보, 및 센서에 대응하는 가상 장치 에 장착된 가상 센서로부터 측정된 가상 공간에 관한 제2센싱 정보를 혼합하고, 및 혼합된 정보를 무인 자율 장 치로 제공하는 프로세서를 포함하는 전자 장치가 제공될 수 있다. 일 실시예에 따른 가상 및 현실 공간의 동기화를 위한 전자 장치의 동작 방법을 컴퓨터에서 실행시키기 위한 프 로그램을 기록한 컴퓨터로 읽을 수 있는 비일시적 기록매체로서, 동작 방법은, 적어도 하나의 표식에 대한 소정 의 센서가 장착된 무인 자율 장치의 접근을 감지하는 단계; 감지에 기초하여 센서로부터 측정된 무인 자율 장치 주변 공간에 관한 제1센싱 정보를 획득하는 단계; 소정의 3D 시뮬레이터를 이용하여 적어도 하나의 객체, 무인 자율 장치에 대응하는 가상 장치, 및 표식에 대응하는 가상 표식을 포함하는 가상 공간을 생성하는 단계; 제1센 싱 정보, 및 센서에 대응하는 가상 장치에 장착된 가상 센서로부터 측정된 가상 공간에 관한 제2센싱 정보를 혼 합하는 단계; 및 혼합된 정보를 무인 자율 장치로 제공하는 단계를 포함하는 비일시적 기록매체가 제공될 수 있 다."}
{"patent_id": "10-2023-0042994", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면 전자 장치는 소정의 표식에 무인 자율 장치가 접근한 경우, 가상의 객체를 포함하는 가상 공 간을 생성하고, 무인 자율 장치 주변 환경에 관한 제1센싱 정보와 가상 공간에 관한 제2센싱 정보를 혼합한 정 보를 무인 자율 장치에게 제공할 수 있다. 전자 장치는 소정의 표식과 가상 공간 내의 소정의 표식에 대응하는 가상 표식을 비교하여 가상 공간 및 현실 공간을 동기화할 수 있다. 전자 장치는 혼합된 이미지 정보뿐만 아니라 혼합된 포인트 클라우드 정보를 무인 자율 장치에게 제공하므로, 보다 신뢰성 높은 무인 자율 장치의 테스트 수행이 가능할 수 있다."}
{"patent_id": "10-2023-0042994", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "발명의 효과는 이상에서 언급된 효과로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 청구범위의 기재로부"}
{"patent_id": "10-2023-0042994", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "터 당해 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0042994", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "실시예들에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의 미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\" 한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"..부\", \"..모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 명세서 전체에서 기재된 \"a, b, 및 c 중 적어도 하나\"의 표현은, 'a 단독', 'b 단독', 'c 단독', 'a 및 b', 'a 및 c', 'b 및 c', 또는 'a,b,c 모두'를 포괄할 수 있다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 이하에서는 도면을 참조하여 본 개시의 실시예들을 상세히 설명한다. 최근 게임 엔진을 활용한 그래픽 기술이 다양한 영역에서 사용되고 있다. 무인 자율 플랫폼을 테스트를 위한 증 강 현실 기술도 이러한 게임 엔진 활용의 일 예시일 수 있다. 게임 엔진을 활용할 경우 현실감 있는 가상 객체 생성이 가능하므로 무인 자율 플랫폼에 대해 보다 신뢰성 높은 테스트를 수행할 수 있기 때문이다. 그러나, 게 임 엔진을 활용하는 경우 문제점이 존재할 수 있다. 실제 센서의 위치와 자세가 변화하는 경우, 가상 객체의 위 치 및 자세 또한 적절히 센서에 동기화되어 변화해야 한다는 것이다. 게임 환경에서는 마우스나 키보드를 사용 하여 관찰자의 시점을 변경하지만, 증강 현실에서는 실제 센서의 위치 및 자세가 센서의 시점을 변화시킨다. 따 라서, 가상 객체를 실제 이미지에 삽입하고자 하는 경우 현실감 유지를 위해 센서와 객체간의 정확한 거리 및 센서의 정확한 관찰 시점을 고려하는 것이 중요할 수 있다. 본 개시는 보다 신뢰성 높은 무인 자율 장치의 테스 트 수행이 가능하도록, 소정의 표식과 가상 공간 내의 소정의 표식에 대응하는 가상 표식을 비교하여 가상 공간 및 현실 공간을 동기화하는 방법을 개시하고자 한다. 도 1은 일 실시예에 따른 전자 장치와 무인 자율 장치의 동작 방법을 설명하기 위한 도면이다. 도 1을 참조하면, 단계 101에서 전자 장치는 무인 자율 장치로부터 gps 신호를 수신할 수 있다. 여기 서 무인 자율 장치는 다양한 실시예들에 따른 인간의 개입 없이 주위의 환경을 감지하고 스스로 동작 가능 한 장치를 포함할 수 있다. 예를 들어, 무인 자율 장치는 무인 자율 자동차, 무인 자율 항공기, 및 무인 자율 선박을 포함할 수 있으나 이에 제한되지 않는다. 일 실시예에 따라, 무인 자율 장치는 소정의 센서가장착될 수 있다. 구체적으로, 무인 자율 장치는 gps 센서, IMU(Inertial Measurement Unit) 센서, 주행기 록계(Odometry) 센서, 카메라 센서, 및 라이더(LiDAR) 센서를 포함할 수 있다. 일 실시예에 따라, 무인 자율 장 치에 장착된 카메라 센서 및 라이더 센서는 동일한 축을 가질 수 있다. 카메라 센서 및 라이더 센서는 동 일한 축을 가짐으로써, 무인 자율 장치의 위치 및 자세가 변화되면 카메라 센서 및 라이더 센서의 시점이 동일하게 변화할 수 있다. 일 실시예에 따라, 전자 장치는 무인 자율 장치에 장착된 센서를 이용하여 gps 신호를 지속적으로 수신함으로써, 무인 자율 장치가 소정의 표식에 접근하는지를 감지할 수 있다. 전 자 장치가 소정의 표식에 대한 무인 자율 장치의 접근을 감지하는 실시예는 이하 도 3 및 도 4에서 상세히 설명될 것이다. 단계 102에서, 전자 장치는 혼합된 정보를 무인 자율 장치로 제공할 수 있다. 구체적으로, 전자 장치 는 무인 자율 장치가 소정의 표식에 기 설정된 거리 내에 접근한 경우, 제1센싱 정보 및 제2센싱 정 보가 혼합된 정보를 무인 자율 장치에게 제공할 수 있다. 다시 말해, 전자 장치는 소정의 표식에 대 한 무인 자율 장치의 접근에 기초하여, 제1센싱 정보 및 제2센싱 정보를 혼합하고 혼합된 정보를 무인 자 율 장치로 전송할 수 있다. 제1센싱 정보는 무인 자율 장치에 장착된 센서로부터 측정된 정보를 포함 할 수 있다. 예를 들어, 제1센싱 정보는 무인 자율 장치의 카메라 센서로부터 측정된 무인 자율 장치 주변 공간에 관한 제1이미지 정보를 포함할 수 있다. 제2센싱 정보는 무인 자율 장치에 대응하는 가상 장 치에 장착된 가상 센서로부터 측정된 정보를 포함할 수 있다. 예를 들어, 제2센싱 정보는 가상 장치의 가상 라 이더 센서로부터 측정된 가상 공간에 관한 제2포인트 클라우드 정보를 포함할 수 있다. 전자 장치가 제1센 싱 정보 및 제2센싱 정보를 혼합하여 제공하는 실시예는 이하 도 2 내지 도 9에서 상세히 설명될 것이다. 도 2는 무인 자율 장치가 소정의 표식에 접근한 경우, 전자 장치와 무인 자율 장치의 동작 방법을 설명하기 위 한 도면이다. 도 2를 참조하면, 전자 장치는 제1프로세서, 제2프로세서, 및 허브를 포함할 수 있고, 무 인 자율 장치는 라이더 센서 및 카메라 센서를 포함할 수 있다. 도 2에 도시된 전자 장치 및 무인 자율 장치는 본 실시예들과 관련된 구성요소들만이 도시되어 있다. 따라서, 전자 장치 및 무 인 자율 장치에는 도 2에 도시된 구성요소들 외에 범용적인 구성요소들이 더 포함될 수 있음은 당해 기술 분야의 통상의 기술자에게 자명하다. 제1프로세서 및 제2프로세서는 전자 장치의 전반적인 기능들을 제어하는 역할을 한다. 예를 들 어, 제1프로세서 및 제2프로세서는 전자 장치 내의 메모리(미도시)에 저장된 프로그램들을 실행 함으로써, 전자 장치를 전반적으로 제어한다. 제1프로세서 및 제2프로세서는 전자 장치 내 에 구비된 CPU(central processing unit), GPU(graphics processing unit), AP(application processor), SBC(single board computer) 등으로 구현될 수 있으나, 이에 제한되지 않는다. 본 실시예에서는 설명의 편의를 위해 전자 장치가 2개의 프로세서(210 및 220)를 포함하는 것으로 도시 되었으나, 이에 제한되지 않고 전 자 장치는 적어도 하나의 프로세서를 포함할 수 있다. 허브는 전자 장치의 전자 장치 내에서 전송되는 정보의 통로 역할을 수행한다. 예를 들어, 제1 프로세서는 허브를 통해 제2이미지 정보를 제2프로세서로 전송할 수 있다. 본 실시예에서는 제1 프로세서와 제2프로세서가 제한된 포트의 개수를 갖는 경우를 가정하여 전자 장치가 허브 를 포함하는 것으로 도시 되었으나, 데이터 교환을 위해 필요한 프로세서의 포트가 충분한 경우 전자 장치(20 0)는 허브를 포함하지 않을 수 있다. 일 실시예에 따라, 제1프로세서는 무인 자율 장치로부터 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보를 수신할 수 있다. 무인 자율 장치의 위치에 관한 정보는 무인 자율 장치의 좌표에 관한 정보를 포함할 수 있고, 무인 자율 장치의 회전에 관한 정보는 무인 자율 장치의 요(yaw)에 관한 정보, 피치(pitch)에 관한 정보, 및 롤(roll)에 관한 정보를 포함할 수 있다. 요에 관한 정보는 무인 자율 장치의 이동 방향에 대해 수직의 수직면에 위치하는 축의 회전에 관한 정보를 의미 할 수 있고, 피치에 관한 정보는 무인 자율 장치의 이동 방향에 대해 수직의 수평면에 위치하는 축의 회전 에 관한 정보를 의미할 수 있고, 롤에 관한 정보는 무인 자율 장치의 이동방향에 대해 평행한 수평면에 위 치하는 축의 회전에 관한 정보를 의미할 수 있다. 제1프로세서는 무인 자율 장치의 요에 관한 정보, 피치에 관한 정보, 및 롤에 관한 정보를 기초로 무인 자율 장치의 자세에 관한 정보를 확인할 수 있다. 일 실시예에 따라, 제1프로세서는 무인 자율 장치에 포함된 gps 센서, IMU 센서, 및 주행기록계 센서를이용하여 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보를 획득할 수 있 다. 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보는 무인 자율 장치 에 대응하는 가상 장치 생성 시 이용될 수 있다. 일 실시예에 따라, 제1프로세서는 가상 공간을 생성할 수 있다. 구체적으로, 제1프로세서는 소정의 3D 시뮬레이터를 이용하여 적어도 하나의 객체, 무인 자율 장치에 대응하는 가상 장치, 및 소정의 표식에 대응하는 가상 표식을 포함하는 가상 공간을 생성할 수 있다. 일 실시예에 따라, 3D 시뮬레이터는 가제보 (Gazebo) 시뮬레이터, 언리얼(Unreal) 시뮬레이터, 및 이노(Inno) 시뮬레이터 중 하나를 포함할 수 있다. 예를 들어, 제1프로세서는 가제보 시뮬레이터를 이용하여 가상 공간을 생성할 수 있다. 적어도 하나의 객체는 소정의 시나리오에 따라 행동하도록 생성될 수 있다. 예를 들어, 제1프로세서는 무인 자율 장치에 빠 른 속도로 접근하도록 행동하는 적어도 하나의 객체를 생성될 수 있다. 적어도 하나의 객체는 인간, 차량, 항공 기 등의 임의의 객체를 포함할 수 있다. 예를 들어, 제1프로세서는 무인 자율 장치에 접근하는 적어 도 하나의 차량을 생성할 수 있다. 일 실시예에 따라, 제1프로세서는 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보를 기초로 가상 공간을 생성할 수 있다. 구체적으로, 제1프로 세서는 위치에 관한 정보 및 회전에 관한 정보를 3D 시뮬레이터에 입력하여 소정의 표식의 중심의 좌표를 기준점으로 하여 가상 공간을 생성할 수 있다. 제1프로세서는 무인 자율 장치의 위치에 관한 정보 및 회전에 관한 정보를 이용하므로, 가상 공간 내에 생성된 가상 장치는 무인 자율 장치와 동일한 위치 및 자 세를 갖도록 생성될 수 있다. 일 실시예에 따라, 제1프로세서는 제2센싱 정보를 확인할 수 있다. 구체적으로, 제1프로세서는 가상 장치에 장착된 가상 센서로부터 측정된 가상 공간에 관한 제2센싱 정보를 획득할 수 있다. 가상 센서는 라이더 센서에 대응하는 가상 라이더 센서 및 카메라 센서에 대응하는 가상 카메라 센서를 포함할 수 있다. 제2센싱 정보는 제2포인트 클라우드 정보 및 제2이미지 정보를 포함할 수 있다. 제2포인트 클라우드 정보는 가 상 라이더 센서로부터 측정된 가상 장치 주변의 가상 공간에 관한 포인트 클라우드 정보를 포함할 수 있고, 제2 이미지 정보는 가상 카메라 센서로부터 측정된 가상 장치 주변 가상 공간에 관한 이미지 정보를 포함할 수 있다. 여기서 포인트 클라우드 정보란, 라이더 센서에 의해 측정되는 3차원 공간에 관한 데이터 점(point)들의 집합으로서 2차원 이미지와는 달리 측정 대상 객체의 깊이(depth) 정보를 포함하는 정보를 의미할 수 있다. 제2 이미지 정보는 가상 공간 내 가상 장치에 장착된 가상 카메라 센서에 의해 측정된 이미지 정보이므로, 제2이미 지 정보는 적어도 하나의 가상 객체의 이미지와 가상 표식의 이미지를 포함할 수 있다. 마찬가지로 제2포인트 클라우드 정보는 가상 공간 내 가상 장치에 장착된 가상 라이더 센서에 의해 측정된 포인트 클라우드이므로, 가 상 객체의 포인트 클라우드와 가상 표식의 포인트 클라우드를 포함할 수 있다. 제1프로세서는 확인한 제2 포인트 클라우드 정보 및 제2이미지 정보를 허브를 통해 제2프로세서로 전송할 수 있다. 일 실시예에 따라, 제2프로세서는 허브를 통해 무인 자율 장치로부터 제1센싱 정보를 수신할 수 있다. 구체적으로, 제2프로세서는 허브를 통해 무인 자율 장치에 장착된 라이더 센서 및 카메라 센서로부터 제1센싱 정보를 전송 받을 수 있다. 일 실시예에 따라, 라이더 센서는 벨로다인 (Velodyne) 센서를 포함할 수 있다. 제1센싱 정보는 제1포인트 클라우드 정보 및 제1이미지 정보를 포함할 수 있다. 제1포인트 클라우드 정보는 라이더 센서로부터 측정된 무인 자율 장치 주변 공간에 관한 포인 트 클라우드 정보를 포함할 수 있고, 제1이미지 정보는 카메라 센서로부터 측정된 무인 자율 장치 주 변 공간에 관한 이미지 정보를 포함할 수 있다. 일 실시예에 따라, 제2프로세서는 제1센싱 정보 및 제2센싱 정보를 혼합할 수 있다. 구체적으로, 제2프로 세서는 제1포인트 클라우드 정보 및 제2포인트 클라우드 정보를 혼합하고, 제1이미지 정보 및 제2이미지 정보를 혼합할 수 있다. 혼합된 정보는 무인 자율 장치 주변 공간의 정보, 가상 객체의 정보, 및 가상 표 식의 정보를 포함할 수 있다. 예를 들어, 혼합된 이미지 정보는 무인 자율 장치의 주변 배경, 무인 자율 장치에게 정해진 시나리오에 따라 행동하는 가상 객체, 및 가상 표식의 이미지를 나타낼 수 있다. 일 실시 예에 따라, 제2프로세서는 혼합된 정보를 허브를 통해 무인 자율 장치에게 제공할 수 있다. 예 를 들어, 제2프로세서는 무인 자율 장치에 포함된 ECU(Electronic Control Unit)으로 혼합된 정보를 전송할 수 있다. 무인 자율 장치는 혼합된 정보를 통해 카메라 센서 및 라이더 센서로는 확인할 수 없는 가상의 객체를 확인할 수 있다. 사용자는 시나리오에 따라 행동하는 가상 객체를 확인한 무인 자율 장 치의 대응을 확인함으로써 무인 자율 장치의 성능을 테스트할 수 있다. 이 때 혼합된 정보는 혼합된 이미지 정보 뿐만 아니라 혼합된 포인트 클라우드 정보도 포함할 수 있으므로, 무인 자율 장치는 보다 현 실감 높은 가상 객체를 확인할 수 있고, 이를 통해 보다 신뢰도 높은 테스트 결과를 획득할 수 있다.가상 공간에 포함되는 가상 장치는 무인 자율 장치로부터 획득한 무인 자율 장치의 위치에 관한 정보 및 회전에 관한 정보를 반영하여 생성될 수 있다. 따라서, 가상 장치는 무인 자율 장치의 위치나 자세와 극히 유사하게 생성되므로 가상 장치의 가상 센서의 시점은 무인 자율 장치의 라이더 센서 및 카메라 센서의 시점과 유사할 수 있다. 다만, 무인 자율 장치의 보다 신뢰성 높은 테스트를 위해서는 가상 센서와 무인 자율 장치의 센서의 시점이 동일해야 할 수 있다. 이를 위해, 제2프로세서는 제1이미지 정보가 나타내는 적어도 하나의 표식과 제2이미지 정보가 나타내는 적어도 하나의 표식에 대응하는 가상 표식을 비교하여 가상 센서를 조절함으로써 무인 자율 장치 주변의 현실 공간과 가상 공간을 동기화할 수 있다. 가상 센서를 조절하여 주변 공간과 가상 공간을 동기화하는 실시예는 이하 도 7a 및 7b에서 상세히 설명될 것이다. 도 3은 일 실시예에 따른 무인 자율 장치가 표식에 접근한 경우 전자 장치의 동작 방법을 설명하기 위한 도면이 다. 도 3을 참조하면, 전자 장치는 표식에 대한 무인 자율 장치의 접근을 감지할 수 있다. 구체적으 로, 전자 장치는 무인 자율 장치로부터 무인 자율 장치의 gps 신호를 수신하여 무인 자율 장치 가 표식에 접근하는지를 확인할 수 있다. 표식은 적어도 하나의 표식을 포함할 수 있다. 표식 은 기 설정된 위치에 배치될 수 있다. 전자 장치는 표식의 기 설정된 위치에 관한 정보를 사용 자로부터 전달 받거나, 메모리(미도시)로부터 획득할 수 있다. 전자 장치는 무인 자율 장치의 gps 신 호를 기초로 무인 자율 장치의 위치에 관한 정보를 확인하고, 무인 자율 장치의 위치에 관한 정보 및 표식의 위치에 관한 정보를 기초로 무인 자율 장치가 표식으로부터 기 설정된 거리 내 접근하는지를 확인할 수 있다. 일 실시예에 따라, 무인 자율 장치가 표식으로부터 기 설정된 거리 내 접근한 경우, 전자 장치 는 무인 자율 장치로부터 제1센싱 정보를 획득할 수 있다. 제1센싱 정보는 무인 자율 장치에 장착된 카메라 센서로부터 측정된 무인 자율 장치 주변 공간에 관한 제1이미지 정보 및 무인 자율 장치에 장 착된 라이더 센서로부터 측정된 무인 자율 장치 주변 공간에 관한 제1포인트 클라우드 정보를 포함할 수 있다. 예를 들어, 제1이미지 정보는 표식의 이미지 및 무인 자율 장치 주변 공간의 이미지(예를 들어, 배경 이미지)를 나타낼 수 있다. 이 때, 제1이미지 정보는 가상 객체에 관한 이미지는 나타내지 않 을 수 있다. 일 실시예에 따라, 전자 장치는 가상 공간을 생성할 수 있다. 도 3을 참조하면, 전자 장치는 소정의 3D 시뮬레이터를 이용하여 가상 객체, 무인 자율 장치에 대응하는 가상 장치, 및 표식에 대응하 는 가상 표식을 포함하는 가상 공간을 생성할 수 있다. 이 때 가상 공간은 표식의 중심을 좌표축의 기준으 로 하여 생성될 수 있다. 가상 객체는 소정의 시나리오에 따라 행동할 수 있다. 예를 들어, 가상 객체 는 가상 공간 내에서 정해진 경로에 따라 움직이는 인간일 수 있다. 가상 장치는 무인 자율 장치에 장착된 센서에 대응하는 가상 센서를 포함할 수 있다. 전자 장치는 가상 센서로부터 측정된 가상 공간에 관한 제2센싱 정보를 확인할 수 있다. 제2센싱 정보는 가상 공간에 관한 제2포인트 클라우드 정보 및 가상 공간 에 관한 제2이미지 정보를 포함할 수 있다. 가상 공간은 무인 자율 장치 주변의 현실 공간과 달리 별도의 배경 없이 가상 장치, 가상 객체, 및 가상 표식을 포함할 수 있다. 따라서, 가상 장치에 장착된 가상 센서 로부터 측정된 제2포인트 클라우드 정보 및 제2이미지 정보는 가상 객체 및 가상 표식에 대한 포인트 클라우드 정보 및 이미지 정보를 포함할 수 있다. 일 실시예에 따라, 전자 장치는 제1센싱 정보 및 제2센싱 정보를 혼합하여 무인 자율 장치로 제공할 수 있다. 예를 들어, 전자 장치는 무인 자율 장치 주변 공간에 관한 제1이미지 정보 및 가상 공간에 관한 제2이미지 정보를 혼합한 이미지 정보를 무인 자율 장치로 제공할 수 있다. 무인 자율 장치는 혼합된 이미지 정보를 통해 실제 주변 공간에서 특정 시나리오에 따라 행동하는 가상 객체를 확인할 수 있 고, 사용자는 이러한 가상 객체에 대한 무인 자율 장치의 대응을 확인함으로써 무인 자율 장치 의 성능 테스트가 가능할 수 있다. 일 실시예에 따라, 전자 장치는 무인 자율 장치와 표식의 거리에 기초하여 혼합된 정보를 무인 자율 장치로 제공할 수 있다. 구체적으로, 전자 장치는 무인 자율 장치로부터 수신한 gps 신호 에 기초하여, 무인 자율 장치와 표식 간의 거리를 확인하고, 무인 자율 장치와 표식 간의 거리가 기 설정된 거리 이상인 경우, 혼합된 정보를 무인 자율 장치에게 제공하지 않을 수 있다. 무인 자율 장치와 표식의 거리에 따라 전자 장치가 동작하므로 전자 장치에 가중되는 부하가 감소될 수 있다. 도 4는 일 실시예에 따른 전자 장치의 동작 방법을 나타낸다. 도 4를 참조하면, 단계 S400에서 전자 장치는 무인 자율 장치가 적어도 하나의 표식에 대해 기 설정된 거리 내 접근하는 지를 확인할 수 있다. 무인 자율 장치가 적어도 하나의 표식으로부터 기 설정된 거리 내에 접근한 경 우, 전자 장치는 무인 자율 장치로부터 무인 자율 장치 주변 공간에 관한 제1센싱 정보를 획득할 수 있다. 일 실시예에 따라 기 설정된 거리는 10m일 수 있다. 제1센싱 정보는 무인 자율 장치에 장착된 카메라 센서로부터 측정된 무인 자율 장치의 주변 공간에 관한 제1이미지 정보와 무인 자율 장치에 장착된 라이더 센서로부터 측정 된 무인 자율 장치의 주변 공간에 관한 제1포인트 클라우드 정보를 포함할 수 있다. 단계 S410에서, 전자 장치는 무인 자율 장치의 위치에 관한 정보 및 회전에 관한 정보를 기초로 표식의 중심의 좌표를 기준으로 가상 공간을 생성할 수 있다. 가상 공간은 적어도 하나의 객체, 무인 자율 장치에 대응하는 가 상 장치, 및 적어도 하나의 표식에 대응하는 가상 표식을 포함할 수 있다. 무인 자율 장치의 위치에 관한 정보 및 회전에 관한 정보를 기초로 가상 장치가 생성되므로, 가상 장치에 장착된 가상 센서의 측정 시점은 무인 자 율 장치에 장착된 센서의 시점과 동일할 수 있다. 단계 S420 및 S421에서, 전자 장치는 제2포인트 클라우드 정보 및 제2이미지 정보를 확인할 수 있다. 전자 장치 는 가상 장치에 장착된 가상 카메라 센서로부터 측정된 가상 공간에 관한 제2이미지 정보를 확인하고, 가상 장 치에 장착된 가상 라이더 센서로부터 측정된 가상 공간에 관한 제2포인트 클라우드 정보를 확인할 수 있다. 단계 S430 및 단계 S431에서, 전자 장치는 포인트 클라우드 정보 각각과 이미지 정보 각각을 혼합할 수 있다. 구체적으로, 전자 장치는 제1포인트 클라우드 정보 및 제2포인트 클라우드 정보를 혼합하고, 제1이미지 정보 및 제2이미지 정보를 혼합할 수 있다. 단계 S440에서, 전자 장치는 혼합된 정보를 무인 자율 장치로 제공할 수 있다. 혼합된 정보는 혼합된 포인트 클 라우드 정보를 포함하므로, 혼합된 이미지 정보만을 무인 자율 장치에게 제공하는 경우보다 신뢰성 높은 무인 자율 장치의 성능 테스트 수행이 가능할 수 있다. 도 5는 다른 실시예에 따른 전자 장치의 동작 방법을 나타낸다. 도 5를 참조하면, 단계 S500에서 전자 장치는 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보를 확인할 수 있다. 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보는 무 인 자율 장치에 장착된 gps 센서, IMU 센서, 및 주행기록계 센서를 이용하여 확인할 수 있다. 구체적으로 전자 장치는 무인 자율 장치의 gps 센서, IMU 센서, 및 주행기록계 센서 각각으로부터 측정된 gps 센싱 정보, IMU 센 싱 정보, 및 주행기록계 센싱 정보를 무인 자율 장치로부터 전송받고, gps 센싱 정보, IMU 센싱 정보, 및 주행 기록계 센싱 정보를 기초로 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보를 획득 할 수 있다. 일 실시예에 따라, 무인 자율 장치의 회전에 관한 정보는 무인 자율 장치의 롤(roll)에 관한 정보, 요(yaw)에 관한 정보, 및 피치(pitch)에 관한 정보를 포함할 수 있다. 단계 S510 및 단계 S511에서, 전자 장치는 무인 자율 장치에 장착된 카메라 센서로부터 측정된 제1이미지 정보 및 무인 자율 장치에 장착된 라이더 센서로부터 측정된 제1포인트 클라우드 정보를 확인할 수 있다. 제1이미지 정보 및 제1포인트 클라우드 정보는 무인 자율 장치 주변 공간에 관한 정보일 수 있다. 단계 S520에서, 전자 장치는 가상 공간을 생성할 수 있다. 구체적으로, 전자 장치는 소정의 3D 시뮬레이터를 이 용하여 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보를 기초로 소정의 표식의 중 심의 좌표를 기준으로 가상 공간을 생성할 수 있다. 이 때 3D 시뮬레이터는 가제보 시뮬레이터, 언리얼 시뮬레 이터, 및 이노 시뮬레이터를 포함할 수 있고, 가상 공간은 아무 배경 없이 무인 자율 장치에 대응하는 가상 장 치, 적어도 하나의 가상 객체, 및 소정의 표식에 대응하는 가상 표식을 포함할 수 있다. 단계 S521에서, 전자 장치는 가상 객체를 확인할 수 있다. 구체적으로, 전자 장치는 가상 공간에 포함된 적어도 하나의 객체를 가상 객체로서 확인할 수 있다. 가상 객체는 3D 시뮬레이터를 이용하여 소정의 시나리오에 따라 행동하도록 생성된 현실감 있는 객체를 의미할 수 있다. 예를 들어, 가상 객체는 무인 자율 장치에 충돌하도록생성된 가상의 무인 자율 장치를 포함할 수 있다. 소정의 시나리오는 기 설정될 수 있다. 사용자는 무인 자율 장치의 성능 테스트를 위해 필요한 기 설정된 시나리오 정보를 전자 장치에 전송할 수 있다. 전자 장치는 3D 시 뮬레이터를 통해 기 설정된 시나리오 정보를 기초로 가상 객체를 생성할 수 있다. 단계 S530 및 단계 S531에서, 전자 장치는 제2이미지 정보를 확인할 수 있고, 제2포인트 클라우드 정보를 확인 할 수 있다. 구체적으로, 전자 장치는 가상 공간을 렌더링하여 가상 공간에 관한 제2이미지 정보를 확인할 수 있고, 레이캐스팅(Raycasting) 기법을 이용하여 전자 장치에 대응하는 가상 장치에 장착된 가상 라이더 센서를 통해 제2포인트 클라우드 정보를 확인할 수 있다. 제2이미지 정보 및 제2포인트 클라우드 정보는 가상 공간에 관한 정보를 의미할 수 있다. 제2이미지 정보 및 제2포인트 클라우드 정보는 무인 자율 장치에 대응하는 가상 장치에 장착된 센서의 시점으로 측정되므로, 이들 정보에는 아무런 배경 없이 가상 객체 및 가상 표식에 관한 이미지 정보 및 포인트 클라우드 정보가 포함될 수 있다. 단계 S540 및 단계 S541에서, 전자 장치는 이미지 정보를 혼합하고 포인트 클라우드 정보를 혼합할 수 있다. 구 체적으로, 전자 장치는 제1이미지 정보 및 제2이미지 정보를 혼합하고, 제1포인트 클라우드 정보 및 제2포인트 클라우드 정보를 혼합할 수 있다. 혼합된 정보는 가상 공간에 관한 정보와 주변 공간에 관한 정보를 모두 포함 할 수 있다. 예를 들어, 제1이미지 정보 및 제2이미지 정보가 혼합된 이미지 정보는 무인 자율 장치의 주변 공 간에서 정해진 시나리오 대로 행동하는 가상 객체가 혼합된 이미지를 나타낼 수 있다. 전자 장치는 무인 자율 장치의 위치 및 자세를 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보를 통해 확인하고, 이를 가상 공간의 가상 장치에 반영할 수 있다. 따라서, 가상 장치에 장착된 가상 센서의 시점은 무인 자율 장치에 장착된 센서의 시점과 동일할 수 있다. 다만, 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보에 오차가 있을 경우, 가상 센서의 시점과 무인 자율 장치의 센서의 시점이 상이할 수 있다. 무인 자율 장치의 신뢰성 높은 테스트 결과를 획득하기 위해서는 가상 센서의 시점과 무인 자율 장치의 센서의 시점이 동일해야 하므로, 이를 위해 가상 표식과 소정의 표식을 비교함으로써 가상 공 간의 가상 장치와 무인 자율 장치를 동기화할 수 있다. 이 때, 가상 라이더 센서와 가상 카메라 센서의 축은 일 치할 수 있고, 무인 자율 장치의 카메라 센서의 화각(Field of View)와 가상 카메라 센서의 화각은 동일할 수 있다. 결과적으로 가상 장치의 자세를 조정하면 그에 맞추어 가상 센서의 시점이 변화될 수 있고, 가상 센서의 시점을 적절히 조절하여 가상 공간과 무인 자율 장치 주변 공간을 동기화할 수 있다. 단계 S550에서 전자 장치의 각각의 표식을 비교할 수 있다. 구체적으로, 전자 장치의 가상 공간을 주변 공간과 동기화 하기 위해 제1이미지 정보 내의 표식과 제2이미지 정보 내의 가상 표식을 비교할 수 있다. 단계 S551에서, 전자 장치는 표식 간의 오프셋을 확인할 수 있다. 구체적으로, 전자 장치는 제1이미지 정보 내 의 표식과 제2이미지 정보 내의 가상 표식의 좌표 값의 차이를 계산하고, 상기 차이를 오프셋으로 확인할 수 있 다. 일 실시예에 따라, 전자 장치는 표식 간의 오프셋이 기 설정된 값 이상인지 여부에 기초하여 가상 센서를 조절할 수 있다. 단계 S552에서, 전자 장치는 가상 장치를 조정할 수 있다. 구체적으로, 전자 장치는 표식 간의 오프셋이 기 설 정된 값 이상인 경우, 가상 장치를 조정함으로써 가상 센서를 조절할 수 있다. 일 실시예에 따라, 전자 장치는 제1이미지 정보 내 표식의 중심의 위치와 제2이미지 정보 내 가상 표식의 중심의 위치가 일치하도록 가상 장치 의 요 및 피치 중 적어도 하나를 조정할 수 있다. 다른 실시예에 따라, 표식이 소정의 개수 이상의 축을 포함하 는 경우, 전자 장치는 제1이미지 정보 내 표식의 각각의 축과 제2이미지 정보 내 가상 표식의 각각의 축이 일치 되도록 가상 장치의 롤, 요, 및 피치 중 적어도 하나를 조정할 수 있다. 가상 장치를 조정하는 경우 가상 센서 가 조절되므로 가상 센서가 측정하는 제2이미지 정보 및 제2포인트 클라우드 정보가 변화할 수 있다. 따라서, 전자 장치는 단계 S530 및 단계 S531로 다시 나아갈 수 있다. 도 6은 3D 시뮬레이터를 이용하여 가상 공간을 생성하는 일 실시예를 나타낸다. 도 6을 참조하면, 전자 장치는 가상 공간을 생성할 수 있다. 구체적으로, 전자 장치는 소정의 3D 시뮬레이 터를 이용하여 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보를 기초로 가상 공간 을 생성할 수 있다. 다시 말해, 전자 장치는 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회 전에 관한 정보를 소정의 3D 시뮬레이터에 입력하여 생성된 가상 공간을 확인할 수 있다. 무인 자율 장치 의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보의 데이터 형식은 x, y, z, roll, pitch, 및 yaw를 포함할 수 있다. 가상 공간은 무인 자율 장치에 대응하는 가상 장치, 가상 장치에 장착된 가상 센서,및 적어도 하나의 가상 객체를 포함할 수 있다. 가상 장치는 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보를 반영하여 생성되므로, 무인 자율 장치의 위치 및 자세에 동기화될 수 있다. 따 라서, 무인 자율 장치에 장착된 가상 센서로부터 측정되는 제2포인트 클라우드 정보 및 제2이미지 정 보의 시점은 무인 자율 장치에 장착된 센서로부터 측정되는 제1포인트 클라우드 정보 및 제1이미지 정보의 시점과 동일할 수 있다. 일 실시예에 따라, 제2포인트 클라우드 정보 및 제2이미지 정보 각각은 가상 센서(60 1)가 가상 공간을 측정한 포인트 클라우드 정보 및 이미지 정보 각각을 포함할 수 있다. 가상 공간은 가상 장치, 가상 센서, 가상 객체, 및 가상 표식만이 포함되므로, 결과적으로 가상 센서가 측정한 제 2포인트 클라우드 정보는 가상 객체 및 가상 표식에 관한 포인트 클라우드 정보를 포함할 수 있고, 가상 센서가 측정한 제2이미지 정보는 가상 객체 및 가상 표식에 관한 이미지 정보를 포함할 수 있다. 일 실시예에 따라, 가상 객체는 소정의 시나리오에 따라 행동하도록 생성될 수 있다. 예를 들어, 제 2이미지 정보가 걷고 있는 인간 객체를 나타내는 것을 확인할 수 있다. 이와 같이, 가상 장치는 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보가 반영되어 생성되므로, 무인 자율 장치의 실제 위치 및 자세와 동기화될 수 있다. 가상 센서는 가상 장치에 부착되어 있으므로 실제 무인 자율 장치의 센서의 시점과 동일하게 포인트 클라우드 정보 및 이미지 정보를 측정할 수 있 다. 다만, 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보에 오차가 있는 경우 가상 장치가 무인 자율 장치의 실제 위치 및 자세를 반영하지 못할 수 있다. 이 경우, 가상 센서의 시점을 조절 하기 위해 가상 장치의 위치 및 자세를 조정할 수 있다. 실제 표식과 가상 표식을 비교하여 가상 장치를 조정하 는 실시예는 이하 도 7a 및 도 7b에서 상세히 설명될 것이다. 도 7a 및 도 7b는 실시예들에 따른 표식을 이용하여 가상 공간과 주변 공간을 동기화하는 과정을 설명하기 위한 도면이다. 도 7a 및 도 7b를 참조하면, 식별 번호 700 및 식별번호 710 각각은 원형 표식을 사용한 경우의 제1이미지 정보 및 제2이미지 정보를 나타낼 수 있고, 식별 번호 730 및 식별 번호 740 각각은 소정의 개수 이상의 축을 포함하 는 표식을 사용한 경우의 제1이미지 정보 및 제2이미지 정보를 나타낼 수 있다. 일 실시예에 따라, 전자 장치는 각각의 표식을 비교하여 가상 장치에 장착된 가상 센서를 조절할 수 있다. 구체 적으로, 전자 장치는 가상 공간을 무인 자율 장치의 주변 공간과 동기화하기 위해, 제1이미지 정보 내의 표식과 제2이미지 정보 내의 가상 표식을 비교하여 가상 센서를 조절할 수 있다. 제1가상 센서 조절 방안으로서, 원형 표식을 사용하는 경우 전자 장치는 제1이미지 정보 내 표식의 중심의 위치 와 제2이미지 정보 내 가상 표식의 중심의 위치가 일치하도록 가상 센서를 조절할 수 있다. 제1가상 센서 조절 방안은 원형 표식을 사용하는 경우의 단순화된 가상 센서 조절 방안일 수 있다. 도 7a를 참조하면, 제1이미지 정보 내의 실제 표식의 중심의 위치는 (x, y)이고 제2이미지 정보 내 가상 표식의 위치는 (x', y')임 을 확인할 수 있다. 제1이미지 정보 내 실제 표식의 중심의 위치와 제2이미지 정보 내 가상 표식의 중심의 위치가 다르다는 것은 무인 자율 장치에 장착된 센서의 시점과 무인 자율 장치에 대응하는 가상 장치에 장착된 가상 센서의 시점이 상이하다는 것을 의미할 수 있다. 전자 장치는 가상 장치의 요 및 피치 중 적어도 하나를 조정함으로써 가상 센서를 조절할 수 있다. 다시 말해, 도 7a에서 전자 장치는 가상 장치의 요 및 피치 중 적어도 하나를 조정하여 제2이미지 정보가 나타내는 가상 표식의 중심(x', y')이 제1이미지 정보 가 나타내는 실제 표식의 중심(x, y)에 위치하도록 가상 센서를 조절할 수 있다. 제2가상 센서 조절 방안으로서, 소정의 개수 이상의 축을 포함하는 표식을 사용하는 경우 전자 장치는 제1이미 지 정보 내 표식의 각각의 축과 제2이미지 정보 내 가상 표식의 각각의 축이 일치되도록 가상 센서를 조절할 수 있다. 예를 들어, 도 7b를 참조하면 전자 장치는 제1이미지 정보 내 실제 표식의 3개의 축과 제2이미지 정 보 내 가상 표식의 3개의 축이 일치되도록 가상 센서를 조절할 수 있다. 구체적으로, 전자 장치는 가상 장 치의 롤, 요, 및 피치 중 적어도 하나를 조정하여 제2이미지 정보가 나타내는 가상 표식의 3개의 축이 제1 이미지 정보가 나타내는 실제 표식의 3개의 축과 일치하도록 가상 센서를 조절할 수 있다. 일 실시예에 따라, 전자 장치는 가상 센서의 조절이 완료된 경우 제2이미지 정보 내에서 가상 표식을 제거할 수 있다. 다시 말해, 전자 장치는 무인 자율 장치의 주변 공간과 가상 공간의 동기화가 완료된 후에 제2이미지 정 보 중 적어도 하나의 객체에 관한 이미지 정보만을 선택할 수 있다. 예를 들어, 도 7a를 참조하면 가상 센서의 조절 후 전자 장치는 제2이미지 정보에서 가상 표식을 제거하여 제2이미지 정보가 가상 객체만을 나타내도록 가공할 수 있다. 이러한 가공 과정을 거친 제2이미지 정보가 제1이미지 정보에 혼합되면, 혼합된 이미 지 정보는 무인 자율 장치 주변 실제 공간에 현실감 있는 가상의 객체가 행동하는 이미지를 나타낼 수 있다. 일 실시예에 따라, 전자 장치는 제2이미지 정보 내에서 가상 표식이 제거된 부분 중 적어도 일부를 제1이미지 정보가 나타내는 주변 공간과 유사한 색상과 패턴으로 변환할 수 있다. 예를 들어, 도 7a에서 전자 장치는 제2 이미지 정보 내 가상 표식을 제거하고 해당 부분을 제1이미지 정보의 실제 표식이 위치하는 부분의 배경과 유사한 색상과 패턴으로 채워 넣을 수 있다. 이러한 과정을 통해, 제1이미지 정보와 제2이미지 정보의 혼합 시에 현실감이 향상되므로 무인 자율 장치 성능 테스트 결과의 신뢰도가 향상될 수 있다. 도 8은 또 다른 실시예에 따른 전자 장치의 동작 방법을 나타낸다. 도 8의 동작 방법의 각 단계는 도 1 내지 도 7b의 전자 장치에 의해 수행될 수 있으므로, 도 1 내지 도 7b와 중 복되는 내용에 대해서는 설명을 생략한다. 단계 S800에서, 전자 장치는 적어도 하나의 표식에 대한 소정의 센서가 장착된 무인 자율 장치의 접근을 감지할 수 있다. 단계 S810에서, 전자 장치는 상기 감지에 기초하여 무인 자율 장치에 장착된 센서로부터 측정된 무인 자율 장치 주변 공간에 관한 제1센싱 정보를 획득할 수 있다. 여기서 제1센싱 정보는 무인 자율 장치 주변 공간에 관한 제 1이미지 정보 및 무인 자율 장치 주변 공간에 관한 제1포인트 클라우드 정보를 포함할 수 있다. 단계 S820에서, 전자 장치는 소정의 3D 시뮬레이터를 이용하여 적어도 하나의 객체, 무인 자율 장치에 대응하는 가상 장치, 및 적어도 하나의 표식에 대응하는 적어도 하나의 가상 표식을 포함하는 가상 공간을 생성할 수 있 다. 단계 S830에서, 전자 장치는 제1센싱 정보, 및 무인 자율 장치에 장착된 센서에 대응하는 가상 센서로부터 측정 된 가상 공간에 관한 제2센싱 정보를 혼합할 수 있다. 여기서 제2센싱 정보는 가상 공간에 관한 제2이미지 정보 및 가상 공간에 관한 제2포인트 클라우드 정보를 포함할 수 있다. 결과적으로, 전자 장치는 제1이미지 정보 및 제2이미지 정보를 혼합하고, 제1포인트 클라우드 정보 및 제2포인트 클라우드 정보를 혼합할 수 있다. 단계 S840에서, 전자 장치는 혼합된 정보를 무인 자율 장치로 제공할 수 있다. 일 실시예에 따라, 사용자는 혼 합된 정보를 이용하여 소정의 시나리오에 따라 행동하는 적어도 하나의 객체에 대한 무인 자율 장치의 대응을 테스트할 수 있다. 도 9는 본 개시에 따른 전자 장치를 나타낸다. 전자 장치는 메모리 및 프로세서를 포함한다. 도 9에 도시된 전자 장치에는 본 실시예들과 관련된 구성요소들만이 도시되어 있다. 따라서, 전자 장치에는 도 9에 도시된 구성요소들 외에 다른 범용"}
{"patent_id": "10-2023-0042994", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "적인 구성요소들이 더 포함될 수 있음은 당해 기술분야의 통상의 기술자에게 자명하다. 메모리는 전자 장치 내에서 처리되는 각종 데이터들을 저장하는 하드웨어로서, 메모리는 전자 장치에서 처리된 데이터들 및 처리될 데이터들을 저장할 수 있다. 또한, 메모리는 전자 장치에 의해 구동될 애플리케이션들, 드라이버들 등을 저장할 수 있다. 메모리는 DRAM(dynamic random access memory), SRAM(static random access memory) 등과 같은 RAM(random access memory), ROM(read-only memory), EEPROM(electrically erasable programmable read-only memory), CD-ROM, 블루레이 또는 다른 광학 디스크 스 토리지, HDD(hard disk drive), SSD(sold state drive), 또는 플래시 메모리를 포함할 수 있다. 프로세서는 전자 장치의 전반적인 기능들을 제어하는 역할을 한다. 예를 들어, 프로세서는 전자 장치 내의 메모리에 저장된 프로그램들을 실행함으로써, 전자 장치를 전반적으로 제어한다. 프 로세서는 전자 장치 내에 구비된 CPU(central processing unit), GPU(graphics processing unit), AP(application processor) 등으로 구현될 수 있으나, 이에 제한되지 않는다. 일 실시예에 따라, 프로세서는 적어도 하나의 표식에 대한 소정의 센서가 장착된 무인 자율 장치의 접근을 감지할 수 있다. 일 실시예에 따라, 적어도 하나의 표식은 기 설정된 위치에 배치될 수 있고, 프로세서는 무인 자율 장치로부터 수신한 gps 신호에 기초하여 무인 자율 장치가 적어도 하나의 표식으로부터 기 설정된 거리 내에 접근하는지를 감지할 수 있다. 일 실시예에 따라, 프로세서는 무인 자율 장치의 접근에 기초하여 무인 자율 장치에 장착된 센서로부터 측 정된 무인 자율 장치 주변 공간에 관한 제1센싱 정보를 획득할 수 있다. 일 실시예에 따라, 프로세서는 무 인 자율 장치가 적어도 하나의 표식으로부터 기 설정된 거리 내에 접근한 경우, 제1센싱 정보를 무인 자율 장치 로부터 획득할 수 있다. 일 실시예에 따라, 무인 자율 장치의 센서는 라이더 센서 및 카메라 센서를 포함할 수 있다. 일 실시예에 따라, 제1센싱 정보는 무인 자율 장치의 라이더 센서로부터 측정된 무인 자율 장치의 주변 공간에 관한 제1포인트 클라우드 정보 및 무인 자율 장치의 카메라 센서로부터 측정된 무인 자율 장치의 주변 공간에 관한 제1이미지 정보를 포함할 수 있다. 일 실시예에 따라, 무인 자율 장치의 센서는 gps 센서, IMU 센 서, 및 주행기록계 센서를 더 포함할 수 있다. 프로세서는 gps 센서, IMU 센서, 및 주행기록계 센서를 이 용하여 무인 자율 장치의 위치에 관한 정보, 및 무인 자율 장치의 회전에 관한 정보를 획득할 수 있다. 일 실시예에 따라, 프로세서는 소정의 3D 시뮬레이터를 이용하여 적어도 하나의 객체, 무인 자율 장치에 대응하는 가상 장치, 및 적어도 하나의 표식에 대응하는 가상 표식을 포함하는 가상 공간을 생성할 수 있다. 일 실시예에 따라, 적어도 하나의 객체는 소정의 시나리오에 따라 행동하도록 생성될 수 있다. 일 실시예에 따라, 3D 시뮬레이터는 가제보 시뮬레이터, 언리얼 시뮬레이터, 및 이노 시뮬레이터 중 하나를 포함할 수 있다. 일 실시예에 따라, 프로세서는 제1센싱 정보 및 가상 장치에 장착된 가상 센서로부터 측정된 가상 공간에 관한 제2센싱 정보를 혼합할 수 있다. 일 실시예에 따라, 제2센싱 정보는 무인 자율 장치의 라이더 센서에 대응 하는 가상 라이더 센서로부터 측정된 가상 공간에 관한 제2포인트 클라우드 정보, 및 무인 자율 장치의 카메라 센서에 대응하는 가상 카메라 센서로부터 측정된 가상 공간에 관한 제2이미지 정보를 포함할 수 있다. 일 실시 예에 따라, 프로세서는 제1포인트 클라우드 정보 및 제2포인트 클라우드 정보를 혼합하고, 제1이미지 정보 및 제2이미지 정보를 혼합할 수 있다. 일 실시예에 따라, 프로세서는 3D 시뮬레이터를 이용하여 무인 자율 장치의 위치에 관한 정보 및 무인 자율 장치의 회전에 관한 정보를 기초로 표식의 중심의 좌표를 기준으로 가상 공간을 생성할 수 있다. 일 실시예에 따라, 프로세서는 가상 공간을 무인 자율 장치의 주변 공간과 동기화 하기 위해, 제1이미지 정보 내의 표식과 제2이미지 정보 내의 가상 표식을 비교하여 가상 장치에 장착된 가상 센서를 조절할 수 있다. 일 실시예에 따라, 프로세서는 제1이미지 정보 내 표식의 중심의 위치와 제2이미지 정보 내 가상 표식의 중심의 위치가 일치하도록 가상 장치의 요 및 피치 중 적어도 하나를 조정함으로써 가상 센서를 조절할 수 있다. 다른 실시예에 따라, 프로세서는 제1이미지 정보 내 표식의 각각의 축과 제2이미지 정보 내 가상 표 식의 각각의 축이 일치되도록 가상 장치의 롤, 요, 및 피치 중 적어도 하나를 조정함으로써 가상 센서를 조절할 수 있다. 일 실시예에 따라, 프로세서는 가상 센서의 조절이 완료된 경우, 제2이미지 정보 내에서 가상 표 식을 제거할 수 있다. 일 실시예에 따라, 프로세서는 통신부를 통해 혼합된 정보를 무인 자율 장치로 제공할 수 있다. 일 실시예 에 따라, 혼합된 정보는 적어도 하나의 객체가 행동하는 시나리오에 대한 무인 자율 장치의 대응을 테스트하기 위해 사용될 수 있다. 일 실시예에 따라, 프로세서는 무인 자율 장치가 적어도 하나의 표식으로부터 기 설 정된 거리를 벗어난 경우, 혼합된 정보를 무인 자율 장치에게 제공하는 것을 중단할 수 있다. 전술한 실시예들에 따른 전자 장치는, 프로세서, 프로그램 데이터를 저장하고 실행하는 메모리, 디스크 드라이 브와 같은 영구 저장부(permanent storage), 외부 장치와 통신하는 통신 포트, 터치 패널, 키(key), 버튼 등과 같은 사용자 인터페이스 장치 등을 포함할 수 있다. 소프트웨어 모듈 또는 알고리즘으로 구현되는 방법들은 상 기 프로세서상에서 실행 가능한 컴퓨터가 읽을 수 있는 코드들 또는 프로그램 명령들로서 컴퓨터가 읽을 수 있 는 기록 매체 상에 저장될 수 있다. 여기서 컴퓨터가 읽을 수 있는 기록 매체로 마그네틱 저장 매체(예컨대, ROM(read-only memory), RAM(random-access memory), 플로피 디스크, 하드 디스크 등) 및 광학적 판독 매체(예 컨대, 시디롬(CD-ROM), 디브이디(DVD: Digital Versatile Disc)) 등이 있다. 컴퓨터가 읽을 수 있는 기록 매 체는 네트워크로 연결된 컴퓨터 시스템들에 분산되어, 분산 방식으로 컴퓨터가 판독 가능한 코드가 저장되고 실 행될 수 있다. 매체는 컴퓨터에 의해 판독가능하며, 메모리에 저장되고, 프로세서에서 실행될 수 있다. 본 실시예는 기능적인 블록 구성들 및 다양한 처리 단계들로 나타내어질 수 있다. 이러한 기능 블록들은 특정 기능들을 실행하는 다양한 개수의 하드웨어 또는/및 소프트웨어 구성들로 구현될 수 있다. 예를 들어, 실시예는 하나 이상의 마이크로프로세서들의 제어 또는 다른 제어 장치들에 의해서 다양한 기능들을 실행할 수 있는, 메 모리, 프로세싱, 로직(logic), 룩 업 테이블(look-up table) 등과 같은 직접 회로 구성들을 채용할 수 있다. 구 성 요소들이 소프트웨어 프로그래밍 또는 소프트웨어 요소들로 실행될 수 있는 것과 유사하게, 본 실시 예는 데이터 구조, 프로세스들, 루틴들 또는 다른 프로그래밍 구성들의 조합으로 구현되는 다양한 알고리즘을 포함하여, C, C++, 자바(Java), 어셈블러(assembler) 등과 같은 프로그래밍 또는 스크립팅 언어로 구현될 수 있 다. 기능적인 측면들은 하나 이상의 프로세서들에서 실행되는 알고리즘으로 구현될 수 있다. 또한, 본 실시 예 는 전자적인 환경 설정, 신호 처리, 및/또는 데이터 처리 등을 위하여 종래 기술을 채용할 수 있다. “매커니즘 ”, “요소”, “수단”, “구성”과 같은 용어는 넓게 사용될 수 있으며, 기계적이고 물리적인 구성들로서 한 정되는 것은 아니다. 상기 용어는 프로세서 등과 연계하여 소프트웨어의 일련의 처리들(routines)의 의미를 포 함할 수 있다. 전술한 실시예들은 일 예시일 뿐 후술하는 청구항들의 범위 내에서 다른 실시예들이 구현될 수 있다."}
{"patent_id": "10-2023-0042994", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 전자 장치와 무인 자율 장치의 동작 방법을 설명하기 위한 도면이다. 도 2는 무인 자율 장치가 소정의 표식에 접근한 경우 전자 장치와 무인 자율 장치의 동작 방법을 설명하기 위한 도면이다. 도 3은 일 실시예에 따른 무인 자율 장치가 표식에 접근한 경우 전자 장치의 동작 방법을 설명하기 위한 도면이 다. 도 4는 일 실시예에 따른 전자 장치의 동작 방법을 나타낸다. 도 5는 다른 실시예에 따른 전자 장치의 동작 방법을 나타낸다. 도 6은 3D 시뮬레이터를 이용하여 가상 공간을 생성하는 일 실시예를 나타낸다. 도 7a 및 도 7b는 실시예들에 따른 표식을 이용하여 가상 공간과 주변 공간을 동기화하는 과정을 설명하기 위한 도면이다. 도 8은 또 다른 실시예에 따른 전자 장치의 동작 방법을 나타낸다. 도 9는 본 개시에 따른 전자 장치를 나타낸다."}
