{"patent_id": "10-2021-7029221", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0123399", "출원번호": "10-2021-7029221", "발명의 명칭": "인공 지능에 기초한 애니메이션 이미지 구동 방법, 및 관련 디바이스", "출원인": "텐센트 테크놀로지", "발명자": "바오, 린차오"}}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "오디오 및 비디오 처리 디바이스에 의해 수행되는 애니메이션 캐릭터 구동 방법으로서,화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터를 획득하는 단계;상기 얼굴 표정에 따라 상기 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하는 단계- 상기제1 표정 베이스는 상기 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용됨 -;타깃 텍스트 정보, 상기 미디어 데이터, 및 상기 제1 표정 베이스에 따라, 상기 타깃 텍스트 정보에 대응하는음향 특징 및 타깃 표정 파라미터를 결정하는 단계- 상기 음향 특징은 상기 화자가 상기 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 사운드를 식별하기 위해 사용되고, 상기 타깃 표정 파라미터는 상기 제1 표정 베이스에대해 상기 화자가 상기 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 및상기 음향 특징 및 상기 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하는단계를 포함하는 애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 애니메이션 캐릭터 및 상기 제2 애니메이션 캐릭터는 동일한 애니메이션 캐릭터이고, 상기 제1 표정베이스는 상기 제2 표정 베이스와 동일하고, 상기 얼굴 표정에 따라 상기 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하는 것은:상기 얼굴 표정에 따라 상기 제1 애니메이션 캐릭터의 상기 제1 표정 베이스 및 상기 제1 애니메이션 캐릭터의얼굴-대-파라미터 변환 파라미터를 결정하는 것- 상기 얼굴-대-파라미터 변환 파라미터는 상기 제1 애니메이션캐릭터에 대응하는 얼굴-대-파라미터 변환 베이스에 대한 상기 제1 애니메이션 캐릭터의 얼굴 형상의 변화 정도를 식별하기 위해 사용됨 -을 포함하고;상기 음향 특징 및 상기 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하는것은:상기 음향 특징, 상기 타깃 표정 파라미터, 및 상기 얼굴-대-파라미터 변환 파라미터에 따라 상기 제2 애니메이션 캐릭터를 구동하는 것을 포함하는, 애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제1 애니메이션 캐릭터 및 상기 제2 애니메이션 캐릭터는 상이한 애니메이션 캐릭터들이고, 상기 제1 표정베이스는 상기 제2 표정 베이스와 상이하고, 상기 음향 특징 및 상기 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하는 것은:상기 제1 표정 베이스에 대응하는 표정 파라미터와 상기 제2 표정 베이스에 대응하는 표정 파라미터 사이의 매핑 관계를 결정하는 것; 및상기 음향 특징, 상기 타깃 표정 파라미터, 및 상기 매핑 관계에 따라 상기 제2 애니메이션 캐릭터를 구동하는것을 포함하는, 애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 제2 표정 베이스는 상기 제2 표정 베이스와 음소 사이의 미리 설정된 관계에 따라 생성되고, 상기 제1 표공개특허 10-2021-0123399-3-정 베이스에 대응하는 표정 파라미터와 상기 제2 표정 베이스에 대응하는 표정 파라미터 사이의 매핑 관계를 결정하는 것은:상기 미디어 데이터에 따라, 상기 음성에 의해 식별된 음소, 상기 음소에 대응하는 시간 간격, 및 상기 시간 간격에서의 상기 미디어 데이터의 비디오 프레임들을 결정하는 것;상기 비디오 프레임들에 따라 상기 음소에 대응하는 제1 표정 파라미터를 결정하는 것- 상기 제1 표정 파라미터는 상기 제1 표정 베이스에 대해 상기 음소를 제공할 때 상기 화자의 얼굴 표정의 변화 정도를 식별하기 위해사용됨 -;상기 미리 설정된 관계 및 상기 제2 표정 베이스에 따라 상기 음소에 대응하는 제2 표정 파라미터를 결정하는것; 및상기 제1 표정 파라미터 및 상기 제2 표정 파라미터에 따라 상기 매핑 관계를 결정하는 것을 포함하는, 애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 타깃 텍스트 정보, 상기 미디어 데이터, 및 상기 제1 표정 베이스에 따라, 상기 타깃 텍스트 정보에 대응하는 음향 특징 및 타깃 표정 파라미터를 결정하는 것은:상기 타깃 텍스트 정보 및 상기 미디어 데이터에 따라, 상기 타깃 텍스트 정보에 대응하는 상기 음향 특징 및표정 특징을 결정하는 것- 상기 표정 특징은 상기 화자가 상기 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정을 식별하기 위해 사용됨 -; 및상기 제1 표정 베이스 및 상기 표정 특징에 따라 상기 타깃 표정 파라미터를 결정하는 것을 포함하는, 애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "오디오 및 비디오 처리 디바이스 상에 배치된 애니메이션 캐릭터 구동 장치로서,획득 유닛, 제1 결정 유닛, 제2 결정 유닛 및 구동 유닛을 포함하고,상기 획득 유닛은 화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터를 획득하도록 구성되고;상기 제1 결정 유닛은 상기 얼굴 표정에 따라 상기 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하도록 구성되고- 상기 제1 표정 베이스는 상기 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용됨 -;상기 제2 결정 유닛은 타깃 텍스트 정보, 상기 미디어 데이터, 및 상기 제1 표정 베이스에 따라, 상기 타깃 텍스트 정보에 대응하는 음향 특징 및 타깃 표정 파라미터를 결정하도록 구성되고- 상기 음향 특징은 상기 화자가상기 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 사운드를 식별하기 위해 사용되고, 상기 타깃 표정 파라미터는 상기 제1 표정 베이스에 대해 상기 화자가 상기 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 및상기 구동 유닛은 상기 음향 특징 및 상기 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션캐릭터를 구동하도록 구성되는 애니메이션 캐릭터 구동 장치."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 제1 애니메이션 캐릭터 및 상기 제2 애니메이션 캐릭터는 동일한 애니메이션 캐릭터이고, 상기 제1 표정베이스는 상기 제2 표정 베이스와 동일하고, 상기 제1 결정 유닛은:상기 얼굴 표정에 따라 상기 제1 애니메이션 캐릭터의 상기 제1 표정 베이스 및 상기 제1 애니메이션 캐릭터의얼굴-대-파라미터 변환 파라미터를 결정하고- 상기 얼굴-대-파라미터 변환 파라미터는 상기 제1 애니메이션 캐릭터에 대응하는 얼굴-대-파라미터 변환 베이스에 대한 상기 제1 애니메이션 캐릭터의 얼굴 형상의 변화 정도를공개특허 10-2021-0123399-4-식별하기 위해 사용됨 -; 및상기 구동 유닛은,상기 음향 특징, 상기 타깃 표정 파라미터, 및 상기 얼굴-대-파라미터 변환 파라미터에 따라 상기 제2 애니메이션 캐릭터를 구동하도록 구성되는, 애니메이션 캐릭터 구동 장치."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 제1 애니메이션 캐릭터 및 상기 제2 애니메이션 캐릭터는 상이한 애니메이션 캐릭터들이고, 상기 제1 표정베이스는 상기 제2 표정 베이스와 상이하고, 상기 구동 유닛은:상기 제1 표정 베이스에 대응하는 표정 파라미터와 상기 제2 표정 베이스에 대응하는 표정 파라미터 사이의 매핑 관계를 결정하고;상기 음향 특징, 상기 타깃 표정 파라미터, 및 상기 매핑 관계에 따라 상기 제2 애니메이션 캐릭터를 구동하도록 구성되는, 애니메이션 캐릭터 구동 장치."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 제2 표정 베이스는 상기 제2 표정 베이스와 음소 사이의 미리 설정된 관계에 따라 생성되고, 상기 구동 유닛은:상기 미디어 데이터에 따라, 상기 음성에 의해 식별된 음소, 상기 음소에 대응하는 시간 간격, 및 상기 시간 간격에서의 상기 미디어 데이터의 비디오 프레임들을 결정하고;상기 비디오 프레임들에 따라 상기 음소에 대응하는 제1 표정 파라미터를 결정하고- 상기 제1 표정 파라미터는상기 제1 표정 베이스에 대해 상기 음소를 제공할 때 상기 화자의 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -;상기 미리 설정된 관계 및 상기 제2 표정 베이스에 따라 상기 음소에 대응하는 제2 표정 파라미터를 결정하고;상기 제1 표정 파라미터 및 상기 제2 표정 파라미터에 따라 상기 매핑 관계를 결정하도록 추가로 구성되는, 애니메이션 캐릭터 구동 장치."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제6항에 있어서,상기 제2 결정 유닛은,상기 타깃 텍스트 정보 및 상기 미디어 데이터에 따라, 상기 타깃 텍스트 정보에 대응하는 상기 음향 특징 및표정 특징을 결정하고- 상기 표정 특징은 상기 화자가 상기 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴표정을 식별하기 위해 사용됨 -; 및상기 제1 표정 베이스 및 상기 표정 특징에 따라 상기 타깃 표정 파라미터를 결정하도록 구성되는, 애니메이션캐릭터 구동 장치."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "오디오 및 비디오 처리 디바이스에 의해 수행되는 애니메이션 캐릭터 구동 방법으로서,화자의 얼굴 표정 및 대응하는 음성을 포함하는 제1 미디어 데이터를 획득하는 단계;상기 얼굴 표정에 따라 상기 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하는 단계- 상기제1 표정 베이스는 상기 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용되고, 상기 제1 표정 베이스의차원 수량은 제1 차원 수량이고, 상기 제1 표정 베이스의 정점 토폴로지는 제1 정점 토폴로지임 -;상기 제1 표정 베이스 및 구동될 제2 애니메이션 캐릭터의 제2 표정 베이스에 따라 타깃 표정 베이스를 결정하는 단계- 상기 제2 표정 베이스의 차원 수량은 제2 차원 수량이고, 상기 제2 표정 베이스의 정점 토폴로지는 제공개특허 10-2021-0123399-5-2 정점 토폴로지이고, 상기 타깃 표정 베이스는 상기 제2 정점 토폴로지를 갖는 제1 애니메이션 캐릭터에 대응하는 표정 베이스이고, 상기 타깃 표정 베이스의 차원 수량은 상기 제2 차원 수량임 -;상기 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제2 미디어 데이터 및 상기 타깃 표정 베이스에 따라 타깃표정 파라미터 및 음향 특징을 결정하는 단계- 상기 타깃 표정 파라미터는 상기 타깃 표정 베이스에 대해 상기화자가 상기 음성을 말할 때 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 및상기 타깃 표정 파라미터 및 상기 음향 특징에 따라 상기 제2 표정 베이스를 갖는 상기 제2 애니메이션 캐릭터를 구동하는 단계를 포함하는 애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 제1 표정 베이스 및 구동될 제2 애니메이션 캐릭터의 제2 표정 베이스에 따라 타깃 표정 베이스를 결정하는 것은:상기 제1 표정 베이스로부터, 무표정인 상기 제1 애니메이션 캐릭터에 대응하는 무표정 그리드를 결정하고, 상기 제2 표정 베이스로부터, 무표정인 상기 제2 애니메이션 캐릭터에 대응하는 무표정 그리드를 결정하는 것;상기 제1 애니메이션 캐릭터에 대응하는 무표정 그리드 및 상기 제2 애니메이션 캐릭터에 대응하는 무표정 그리드에 따라 조정 그리드를 결정하는 것- 상기 조정 그리드는 무표정인 상기 제1 애니메이션 캐릭터를 식별하기위해 상기 제2 정점 토폴로지를 가짐 -; 및상기 조정 그리드 및 상기 제2 표정 베이스에서의 그리드 변형 관계에 따라 상기 타깃 표정 베이스를 생성하는것을 포함하는, 애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "애니메이션 캐릭터 구동 장치로서,상기 장치는 오디오 및 비디오 처리 디바이스 상에 배치되고, 상기 장치는 획득 유닛, 제1 결정 유닛, 제2 결정유닛, 제3 결정 유닛, 및 구동 유닛을 포함하고,상기 획득 유닛은 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제1 미디어 데이터를 획득하도록 구성되고;상기 제1 결정 유닛은 상기 얼굴 표정에 따라 상기 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하도록 구성되고- 상기 제1 표정 베이스는 상기 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용되고, 상기 제1 표정 베이스의 차원 수량은 제1 차원 수량이고, 상기 제1 표정 베이스의 정점 토폴로지는 제1정점 토폴로지임 -;상기 제2 결정 유닛은 상기 제1 표정 베이스 및 구동될 제2 애니메이션 캐릭터의 제2 표정 베이스에 따라 타깃표정 베이스를 결정하도록 구성되고- 상기 제2 표정 베이스의 차원 수량은 제2 차원 수량이고, 상기 제2 표정베이스의 정점 토폴로지는 제2 정점 토폴로지이고, 상기 타깃 표정 베이스는 상기 제2 정점 토폴로지를 갖는 제1 애니메이션 캐릭터에 대응하는 표정 베이스이고, 상기 타깃 표정 베이스의 차원 수량은 상기 제2 차원 수량임-;상기 제3 결정 유닛은 상기 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제2 미디어 데이터 및 상기 타깃 표정 베이스에 따라 타깃 표정 파라미터 및 음향 특징을 결정하도록 구성되고- 상기 타깃 표정 파라미터는 상기타깃 표정 베이스에 대해 상기 화자가 상기 음성을 말할 때 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -;및상기 구동 유닛은 상기 타깃 표정 파라미터 및 상기 음향 특징에 따라 상기 제2 표정 베이스를 갖는 상기 제2애니메이션 캐릭터를 구동하도록 구성되는, 애니메이션 캐릭터 구동 장치."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "애니메이션 캐릭터 구동 디바이스로서,프로세서 및 메모리를 포함하고,공개특허 10-2021-0123399-6-상기 메모리는 프로그램 코드를 저장하고 상기 프로그램 코드를 상기 프로세서에 송신하도록 구성되고;상기 프로세서는 상기 프로그램 코드 내의 명령어들에 따라 제1항 내지 제5항 또는 제11항 및 제12항 중 어느한 항에 따른 방법을 수행하도록 구성되는, 애니메이션 캐릭터 구동 디바이스."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "프로그램 코드를 저장하도록 구성된 컴퓨터 판독가능 저장 매체로서,상기 프로그램 코드는 제1항 내지 제5항 또는 제11항 및 제12항 중 어느 한 항에 따른 방법을 수행하기 위해 사용되는 컴퓨터 판독가능 저장 매체."}
{"patent_id": "10-2021-7029221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "컴퓨터 프로그램 제품으로서,실행될 때, 제1항 내지 제5항 또는 제11항 및 제12항 중 어느 한 항에 따른 방법을 수행하도록 구성되는 컴퓨터프로그램 제품."}
{"patent_id": "10-2021-7029221", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공 지능에 기초한 애니메이션 이미지 구동 방법 및 관련 디바이스. 방법은: 화자가 음성을 말할 때 얼굴 표정 변화들의 미디어 데이터를 수집하고, 화자에 대응하는 제1 애니메이션 이미지의 제1 표정 베이스를 결정하는 단 계- 제1 표정 베이스는 제1 애니메이션 이미지의 상이한 표정들을 반영할 수 있음 -; 제2 애니메이션 이미지를 (뒷면에 계속)"}
{"patent_id": "10-2021-7029221", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "관련 출원 본 출원은 2019년 9월 2일자로 중국 지적 재산권 관리국(China National Intellectual Property Administration)에 출원된 \"ARTIFICIAL INTELLIGENCE-BASED ANIMATION CHARACTER DRIVE METHOD AND RELATED APPARATUS\"라는 명칭의 중국 특허 출원 제201910824770.0호에 대한 우선권을 주장하며, 그 전체가 본 명세서에 참고로 포함된다. 기술 분야 본 출원은 데이터 처리 분야에 관한 것으로, 특히 애니메이션 캐릭터의 구동에 관한 것이다."}
{"patent_id": "10-2021-7029221", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "컴퓨터 기술들의 개발로, 인간-컴퓨터 상호작용들이 더 일반화되었지만, 그 대부분은 순수한 음성 상호작용들이 다. 예를 들어, 상호작용 디바이스는 사용자에 의해 입력된 텍스트 또는 음성에 따라 응답 콘텐츠를 결정하고, 응답 콘텐츠에 따라 합성된 가상 사운드를 재생할 수 있다. 이러한 타입의 인간-컴퓨터 상호작용들에 의해 야기되는 사용자의 몰입감은 사용자에 의한 현재의 상호작용 요 구를 충족시키기 어렵다. 사용자의 몰입감을 개선하기 위해, 사용자와 상호작용하는 상호작용 객체로서, 예를 들어, 입 형상을 변경할 수 있는 표정 변경 능력을 갖는 애니메이션 캐릭터의 사용이 현재 연구 및 개발 방향이 다. 그러나, 현재 정교한 애니메이션 캐릭터 구동 방법은 없다. 전술한 기술적 문제들을 해결하기 위해, 본 출원은 인공 지능 기반(AI 기반) 애니메이션 캐릭터 구동 방법 및 장치를 제공하여, 사용자에게 현실적인 존재감 및 몰입감을 야기함으로써, 사용자와 애니메이션 캐릭터 사이의 상호작용의 경험을 개선한다. 다음의 기술적 해결책들이 본 출원의 실시예들에 개시된다: 제1 양태에 따르면, 본 출원의 실시예는 오디오 및 비디오 처리 디바이스에 의해 수행되는 애니메이션 캐릭터 구동 방법을 제공하고, 이 방법은 다음을 포함한다: 화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터를 획득하는 단계; 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하는 단계- 제1 표정 베이 스는 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용됨 -;타깃 텍스트 정보, 미디어 데이터, 및 제1 표정 베이스에 따라, 타깃 텍스트 정보에 대응하는 음향 특징 및 타 깃 표정 파라미터를 결정하는 단계- 음향 특징은 화자가 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 사운드를 식별하기 위해 사용되고, 타깃 표정 파라미터는 제1 표정 베이스에 대해 화자가 타깃 텍스트 정보를 말할 때 시 뮬레이팅되는 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 및 음향 특징 및 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하는 단계. 제2 양태에 따르면, 본 출원의 실시예는 애니메이션 캐릭터 구동 장치를 제공하고, 이 장치는 오디오 및 비디오 처리 디바이스 상에 배치되고, 이 장치는 획득 유닛, 제1 결정 유닛, 제2 결정 유닛, 및 구동 유닛을 포함하고; 획득 유닛은 화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터를 획득하도록 구성되고; 제1 결정 유닛은 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하도록 구 성되고- 제1 표정 베이스는 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용됨 -; 제2 결정 유닛은 타깃 텍스트 정보, 미디어 데이터, 및 제1 표정 베이스에 따라, 타깃 텍스트 정보에 대응하는 음향 특징 및 타깃 표정 파라미터를 결정하도록 구성되고- 음향 특징은 화자가 타깃 텍스트 정보를 말할 때 시 뮬레이팅되는 사운드를 식별하기 위해 사용되고, 타깃 표정 파라미터는 제1 표정 베이스에 대해 화자가 타깃 텍 스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 및 구동 유닛은 음향 특징 및 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하 도록 구성된다. 제3 양태에 따르면, 본 출원의 실시예는 오디오 및 비디오 처리 디바이스에 의해 수행되는 애니메이션 캐릭터 구동 방법을 제공하고, 이 방법은 다음을 포함한다: 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제1 미디어 데이터를 획득하는 단계; 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하는 단계- 제1 표정 베이 스는 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용되고, 제1 표정 베이스의 차원 수량은 제1 차원 수 량이고, 제1 표정 베이스의 정점 토폴로지는 제1 정점 토폴로지임 -; 제1 표정 베이스 및 구동될 제2 애니메이션 캐릭터의 제2 표정 베이스에 따라 타깃 표정 베이스를 결정하는 단 계- 제2 표정 베이스의 차원 수량은 제2 차원 수량이고, 제2 표정 베이스의 정점 토폴로지는 제2 정점 토폴로지 이고, 타깃 표정 베이스는 제2 정점 토폴로지를 갖는 제1 애니메이션 캐릭터에 대응하는 표정 베이스이고, 타깃 표정 베이스의 차원 수량은 제2 차원 수량임 -; 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제2 미디어 데이터 및 타깃 표정 베이스에 따라 타깃 표정 파라 미터 및 음향 특징을 결정하는 단계- 타깃 표정 파라미터는 타깃 표정 베이스에 대해 화자가 음성을 말할 때 얼 굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 및 타깃 표정 파라미터 및 음향 특징에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하는 단계. 제4 양태에 따르면, 본 출원의 실시예는 애니메이션 캐릭터 구동 장치를 제공하고, 이 장치는 오디오 및 비디오 처리 디바이스 상에 배치되고, 이 장치는 획득 유닛, 제1 결정 유닛, 제2 결정 유닛, 제3 결정 유닛, 및 구동 유닛을 포함하고; 획득 유닛은 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제1 미디어 데이터를 획득하도록 구성되고; 제1 결정 유닛은 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하도록 구 성되고- 제1 표정 베이스는 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용되고, 제1 표정 베이스의 차 원 수량은 제1 차원 수량이고, 제1 표정 베이스의 정점 토폴로지는 제1 정점 토폴로지임 -; 제2 결정 유닛은 제1 표정 베이스 및 구동될 제2 애니메이션 캐릭터의 제2 표정 베이스에 따라 타깃 표정 베이 스를 결정하도록 구성되고- 제2 표정 베이스의 차원 수량은 제2 차원 수량이고, 제2 표정 베이스의 정점 토폴로 지는 제2 정점 토폴로지이고, 타깃 표정 베이스는 제2 정점 토폴로지를 갖는 제1 애니메이션 캐릭터에 대응하는 표정 베이스이고, 타깃 표정 베이스의 차원 수량은 제2 차원 수량임 -; 제3 결정 유닛은 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제2 미디어 데이터 및 타깃 표정 베이스에 따 라 타깃 표정 파라미터 및 음향 특징을 결정하도록 구성되고- 타깃 표정 파라미터는 타깃 표정 베이스에 대해화자가 음성을 말할 때 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 및 구동 유닛은 타깃 표정 파라미터 및 음향 특징에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하 도록 구성된다. 제5 양태에 따르면, 본 출원의 실시예는 애니메이션 캐릭터 구동 디바이스를 제공하고, 이 디바이스는 프로세서 및 메모리를 포함하고, 메모리는 프로그램 코드를 저장하고 프로그램 코드를 프로세서에 송신하도록 구성되고; 및 프로세서는 프로그램 코드 내의 명령어들에 따라 제1 양태 또는 제3 양태에 따른 방법을 수행하도록 구성된다. 제6 양태에 따르면, 본 출원의 실시예는 프로그램 코드를 저장하도록 구성된 컴퓨터 판독가능 저장 매체를 제공 하고, 프로그램 코드는 제1 양태 또는 제3 양태에 따른 방법을 수행하기 위해 사용된다. 제7 양태에 따르면, 본 출원의 실시예는 컴퓨터 프로그램 제품을 제공하고, 컴퓨터 프로그램 제품은, 실행될 때, 제1 양태 또는 제3 양태에 따른 방법을 수행하기 위해 사용된다. 전술한 기술적 해결책들로부터, 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스는 화자가 음성을 말 할 때 얼굴 표정 변화를 포함하는 미디어 데이터를 취득함으로써 결정될 수 있고, 제1 표정 베이스는 제1 애니 메이션 캐릭터의 상이한 표정들을 반영할 수 있다는 것을 알 수 있다. 제2 애니메이션 캐릭터를 구동하기 위해 사용되는 타깃 텍스트 정보가 결정된 후에, 타깃 텍스트 정보, 전술한 미디어 데이터, 및 제1 표정 베이스에 따 라 타깃 텍스트 정보에 대응하는 음향 특징 및 타깃 표정 파라미터가 결정될 수 있다. 음향 특징은 화자가 타 깃 텍스트 정보를 말할 때 시뮬레이팅되는 사운드를 식별하기 위해 사용될 수 있고, 타깃 표정 파라미터는 제1 표정 베이스에 대해 화자가 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정의 변화 정도를 식별하기 위 해 사용될 수 있다. 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터는 음향 특징 및 타깃 표정 파라미터에 따 라 구동될 수 있어, 제2 애니메이션 캐릭터는 음향 특징에 따라, 화자가 타깃 텍스트 정보를 말할 때 생성되는 사운드를 시뮬레이팅하고, 발성 동안 화자의 표정에 부합하는 얼굴 표정을 행하여, 사용자에게 현실적인 존재감 및 몰입감을 야기함으로써, 사용자와 애니메이션 캐릭터 사이의 상호작용의 경험을 개선할 수 있다."}
{"patent_id": "10-2021-7029221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부 도면들을 참조하여 본 출원의 실시예들을 설명한다. 현재, 사용자와 상호작용하는 상호작용 타깃으로서 표정 변화 능력을 갖는 애니메이션 캐릭터를 사용하는 것은 인간-컴퓨터 상호작용의 주요 연구 방향이다. 예를 들어, 게임 장면에서, 사용자와 동일한 얼굴 형상을 갖는 게임 캐릭터(애니메이션 캐릭터)가 구성될 수 있 고, 사용자가 텍스트 또는 음성을 입력할 때, 게임 캐릭터는 음성을 생성하고 대응하는 표정(예컨대, 입 형상) 을 행할 수 있거나; 또는 게임 장면에서, 사용자와 동일한 얼굴 형상을 갖는 게임 캐릭터가 구성되고, 반대측이 텍스트 또는 음성을 입력할 때, 게임 캐릭터는 반대측의 입력에 따라 음성을 응답하고 대응하는 표정을 행할 수 있다. 애니메이션 캐릭터를 더 잘 구동하기 위해, 예를 들어, 애니메이션 캐릭터를 구동하여 음성을 생성하고 대응하 는 표정을 행하기 위해, 본 출원의 실시예는 인공 지능 기반(AI 기반) 애니메이션 캐릭터 구동 방법을 제공한다. 이 방법에서, 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스는 화자가 음성을 말할 때 얼굴 표정 변화를 포함하는 미디어 데이터를 취득함으로써 결정될 수 있다. 제2 애니메이션 캐릭터를 구동하기 위해 사용되는 타깃 텍스트 정보가 결정된 후에, 타깃 텍스트 정보, 전술한 취득된 미디어 데이터, 및 제1 표정 베이스에 따라 타깃 텍스트 정보에 대응하는 음향 특징 및 타깃 표정 파라미터가 결정될 수 있어, 음향 특징 및 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하여, 제2 애니메이션 캐릭터 가 음향 특징에 따라, 화자가 타깃 텍스트 정보를 말할 때 생성되는 사운드를 시뮬레이팅하고, 발성 동안 화자 의 표정에 부합하는 얼굴 표정을 행하여, 텍스트 정보에 기초하여 제2 애니메이션 캐릭터를 구동한다. 본 출원의 실시예들에서 제공되는 애니메이션 캐릭터 구동 방법은 AI에 기초하여 구현된다는 점이 주목되어야 한다. AI는 디지털 컴퓨터 또는 디지털 컴퓨터에 의해 제어되는 머신을 사용하여, 인간 지능을 시뮬레이팅, 연 장, 및 확장하고, 환경을 인지하고, 지식을 습득하고, 지식을 사용하여 최적의 결과를 획득하는 이론, 방법, 기 술, 및 응용 시스템이다. 즉, AI는 지능의 본질을 이해하려고 시도하고, 인간 지능과 유사한 방식으로 반응할 수 있는 새로운 지능형 머신을 생산하는, 컴퓨터 과학의 포괄적인 기술이다. AI는 다양한 지능형 머신들의 설 계 원리들 및 구현 방법들을 연구하여, 머신들이 인식, 추론, 및 의사 결정의 기능을 가질 수 있게 한다. AI 기술은 포괄적인 분야이며, 하드웨어-레벨 기술들 및 소프트웨어-레벨 기술들 둘 다를 포함하는 광범위한 분 야에 관한 것이다. AI 기초 기술들은 일반적으로 센서, 전용 AI 칩, 클라우드 컴퓨팅, 분산 스토리지, 빅 데이 터 처리 기술, 운영/상호작용 체제, 및 전기기계적 통합과 같은 기술들을 포함한다. AI 소프트웨어 기술들은 주로 컴퓨터 비전 기술, 음성 처리 기술, 자연 언어 처리 기술, 및 머신 러닝(ML)/딥 러닝(DL)과 같은 여러 주 요 방향들을 포함한다. 본 출원의 실시예들에서, 주로 관련된 AI 기술들은 음성 처리 기술, ML, 및 컴퓨터 비전(이미지)과 같은 방향들 을 포함한다. 예를 들어, 음성 인식(Automatic Speech Recognition, ASR) 기술, 음성 합성(Text-To-Speech, TTS) 기술, 및 성문(voiceprint) 인식 기술이 음성 기술에 수반될 수 있다. 음성 인식 기술은 음성 신호 전처리, 음성 신호 주파수 분석, 음성 신호 특징 추출, 음성 신호 특징 매칭/인식, 음성 훈련 등을 포함한다. 음성 합성은 텍스트 분석, 음성 생성 등을 포함한다. 예를 들어, 머신 러닝(ML)이 수반될 수 있다. ML은 다-분야 간-학문(multi-field inter-discipline)이고, 확 률 이론, 통계, 근사 이론, 볼록 분석(convex analysis), 및 알고리즘 복잡도 이론과 같은 복수의 학문에 관련 된다. ML은 컴퓨터가 새로운 지식 또는 스킬들을 획득하기 위해 인간 학습 거동을 어떻게 시뮬레이팅 또는 구 현하는지를 연구하는 것을 전문화하고, 기존의 지식 구조를 재조직하여, 그 성능을 계속 개선시킨다. ML은 AI 의 핵심이고, 컴퓨터를 지능적으로 만드는 기본적인 방식이며, AI의 다양한 분야들에 적용된다. ML은 일반적으 로 딥 러닝(DL)과 같은 기술들을 포함하고, DL은 합성곱 신경망(CNN), 순환 신경망(RNN) 및 딥 신경망(DNN)과 같은 인공 신경망들을 포함한다.예를 들어, 컴퓨터 비전에서의 비디오 처리, 비디오 시맨틱 이해(VSU), 얼굴 인식 등이 수반될 수 있다. VSU는, 타깃 인식, 타깃 검출/국지화 등을 포함한다. 얼굴 인식은 얼굴 3D 재구성, 얼굴 검출, 얼굴 추적 등을 포함한다. 본 출원의 실시예들에서 제공되는 AI 기반 애니메이션 캐릭터 구동 방법은 애니메이션 캐릭터를 구동하는 능력 을 갖는 오디오 및 비디오 처리 디바이스에 적용가능하다. 오디오 및 비디오 처리 디바이스는 단말 디바이스일 수 있거나, 서버일 수 있다. 오디오 및 비디오 처리 디바이스는 음성 기술을 구현하기 위한 능력을 가질 수 있다. 오디오 및 비디오 처리 디바이스가 듣고, 보고, 느낄 수 있게 하는 것은 인간-컴퓨터 상호작용의 미래의 개발 방향이고, 음성은 미래에 가장 유망한 인간-컴퓨터 상호작용 방법들 중 하나가 되었다. 본 출원의 실시예들에서, 오디오 및 비디오 처리 디바이스는 전술한 컴퓨터 비전 기술을 구현함으로써 미디어 데이터에서 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정할 수 있고, 타깃 텍스트 정보 및 미디어 데이터에 따라 음성 기술 및 ML을 통해, 타깃 텍스트 정보에 대응하는 음향 특징 및 타깃 표정 파라미터 를 결정할 수 있고, 음향 특징 및 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 추가로 구동할 수 있다. 오디오 및 비디오 처리 디바이스가 단말 디바이스인 경우, 단말 디바이스는 스마트 단말, 컴퓨터, PDA(personal digital assistant), 태블릿 컴퓨터 등일 수 있다. 오디오 및 비디오 처리 디바이스가 서버인 경우, 서버는 독립적인 서버일 수 있거나, 클러스터 서버일 수 있다. 서버가 방법을 구현할 때, 단말 디바이스는 화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터를 서 버에 업로드할 수 있고, 서버는 음향 특징 및 타깃 표정 파라미터를 결정하고, 음향 특징 및 타깃 표정 파라미 터에 따라 단말 디바이스 상에서 제2 애니메이션 캐릭터를 구동한다. 본 출원의 실시예들에서 제공되는 AI 기반 애니메이션 캐릭터 구동 방법은 뉴스 방송, 날씨 예보, 게임 코멘터 리, 및 사용자와 동일한 얼굴 형상을 갖는 게임 캐릭터가 구성되는 것이 허용되는 게임 장면과 같은, 애니메이 션 캐릭터들에 적합한 다양한 응용 시나리오들에 적용가능하고, 또한 애니메이션 캐릭터들이 개인 서비스들을 착수하기 위해 사용되는 시나리오들, 예를 들어, 심리학자 또는 가상 비서와 같은 개인들에 관한 일대일 서비스 에 적용가능하다는 것을 이해할 수 있다. 이러한 시나리오들에서, 애니메이션 캐릭터는 본 출원의 실시예들에 서 제공되는 방법을 사용하여 구동될 수 있다. 본 출원의 기술적 해결책들의 이해의 용이함을 위해, 본 출원의 실시예들에서 제공되는 AI 기반 애니메이션 캐 릭터 구동 방법이 실제 응용 시나리오를 참조하여 아래에 설명된다. 도 1은 본 출원의 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 응용 시나리오의 개략도이다. 오디오 및 비디오 처리 디바이스가 단말 디바이스인 응용 시나리오를 예로서 사용하여 설명이 이루어진다. 응용 시나 리오는 단말 디바이스를 포함한다. 단말 디바이스는 화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터를 획득할 수 있다. 하나 이상의 미디어 데이터가 있을 수 있다. 미디어 데이터는 비디오일 수 있거나, 비디오 및 오디오일 수 있다. 미디어 데이터 내의 음성에 포함된 캐릭터에 대응하는 언어는 중국어, 영어, 한국어 또는 다른 언어일 수 있다. 얼굴 표정은 화자가 음성을 말할 때 얼굴에 의해 행해진 액션, 예를 들어, 입 형상, 눈 액션 또는 눈썹 액션일 수 있고, 비디오 시청자는 화자의 얼굴 표정을 통해 미디어 데이터 내의 음성이 화자에 의해 말해지는 것을 인 지할 수 있다. 단말 디바이스는 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정할 수 있고, 제1 표정 베이스는 제1 애니메이션 캐릭터의 상이한 표정들을 식별하기 위해 사용된다. 제2 애니메이션 캐릭터를 구동하기 위해 사용되는 타깃 텍스트 정보를 결정한 후에, 단말 디바이스는 타깃 텍스트 정보, 전술한 취득된 미디어 데이터, 및 제1 표정 베이스에 따라 타깃 텍스트 정보에 대응하는 음향 특 징 및 타깃 표정 파라미터를 결정할 수 있다. 후속하여 수반될 수 있는 얼굴-대-파라미터 변환 파라미터 및 표 정 파라미터의 표정 형태는 계수, 예를 들어, 특정 차원을 갖는 벡터일 수 있다. 미디어 데이터 내의 음성 및 얼굴 표정이 동기화되고, 음향 특징 및 타깃 표정 파라미터 둘 다가 미디어 데이터 에 기초하여 획득되고 동일한 시간 축에 대응하기 때문에, 음향 특징에 의해 식별된 사운드 및 타깃 표정 파라 미터에 의해 식별된 표정은 동일한 시간 축 상에서 동기적으로 변한다. 생성된 음향 특징은 시간 축과 관련된시퀀스이고, 타깃 표정 파라미터는 동일한 시간 축과 관련된 시퀀스이고, 둘 다는 텍스트 정보가 변함에 따라 적절히 조정될 수 있다. 그러나, 조정에 관계없이, 음향 특징은 전술한 미디어 데이터에서의, 화자가 타깃 텍 스트 정보를 말할 때 시뮬레이팅되는 사운드를 식별하기 위해 사용되고, 타깃 표정 파라미터는 제1 표정 베이스 에 대해 전술한 미디어 데이터에서의, 화자가 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정의 변화 정 도를 식별하기 위해 사용된다. 그 후, 단말 디바이스는 음향 특징 및 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동할 수 있어서, 제2 애니메이션 캐릭터는 음향 특징에 따라, 화자가 타깃 텍스트 정보를 말할 때 생성되는 사운드를 시뮬레이팅하고, 발성 동안 화자의 표정에 부합하는 얼굴 표정을 행할 수 있다. 제2 애니메 이션 캐릭터는 제1 애니메이션 캐릭터와 동일한 애니메이션 캐릭터일 수 있거나, 제1 애니메이션 캐릭터와 상이 한 애니메이션 캐릭터일 수 있으며, 이는 본 출원의 이 실시예에서 제한되지 않는다. 다음으로, 본 출원의 실시예들에서 제공되는 AI 기반 애니메이션 캐릭터 구동 방법이 첨부 도면들을 참조하여 상세히 설명된다. 도 2를 참조하면, 방법은 아래의 단계들을 포함한다: S201. 화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터를 획득한다. 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터는 카메라를 이용한 기록 환경에서, 카메라를 사용하는 것에 의해 화자가 말한 음성 및 화자에 대응하는 얼굴 표정을 기록함으로써 획득될 수 있다. 카메라를 사용하여 취득된 비디오가 화자의 얼굴 표정 및 대응하는 음성 둘 다를 포함하는 경우, 미디어 데이터 는 비디오이다; 카메라를 사용하여 취득된 비디오가 화자의 얼굴 표정을 포함하고 음성이 다른 디바이스, 예를 들어, 음성 기록 디바이스를 사용하여 취득되는 경우, 미디어 데이터는 비디오 및 오디오를 포함한다. 이 경우, 비디오 및 오디오는 동기적으로 취득되고, 비디오는 화자의 얼굴 표정을 포함하고, 오디오는 화자의 음성 을 포함한다. 본 출원의 실시예들에서 제공되는 방법은 애니메이션 캐릭터 구동 시스템을 사용하여 구현될 수 있다는 점에 유 의해야 한다. 도 3을 참조하면, 시스템은 주로 4개의 부분: 데이터 취득 모듈, 얼굴 모델링 모듈, 음향 특징 및 표정 파라미터 결정 모듈, 및 애니메이션 구동 모듈을 포함한다. 데이터 취득 모듈은 S201을 수행하도록 구 성되고, 얼굴 모델링 모듈은 S202를 수행하도록 구성되고, 음향 특징 및 표정 파라미터 결정 모듈은 S203을 수 행하도록 구성되고, 애니메이션 구동 모듈은 S204를 수행하도록 구성된다. 화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터는 데이터 취득 모듈을 사용하여 획득될 수 있다. 데이터 취득 모듈에 대한 많은 선택들이 있을 수 있다. 데이터 취득 모듈은, 특수 디바이스를 사용하여, 화자 의 음성 및 얼굴 표정을 포함하는 미디어 데이터를 취득할 수 있는데, 예를 들어, 액션 캡처링 시스템 또는 얼 굴 표정 캡처링 시스템과 같은 특수 디바이스를 사용하여 화자의 얼굴 표정을 캡처한다. 얼굴 표정은 예를 들 어, 얼굴 액션, 표정 또는 입 형상일 수 있다. 화자의 음성은 특수 음성 기록 디바이스를 사용하여 기록되고, 음성 및 얼굴 표정의 데이터 동기화는 동기화 신호 트리거링에 의해 상이한 디바이스들 사이에서 구현된다. 실제로, 특수 디바이스는 사용되는 고가의 캡처링 시스템으로 제한되지 않고, 멀티-뷰 초고화질(multi-view ultra-high-definition) 디바이스일 수 있다. 화자의 음성 및 얼굴 표정을 포함하는 비디오는 멀티-뷰 초고화 질 디바이스를 사용하여 취득된다. 데이터 취득 모듈은, 복수의 주변 카메라에 의해, 화자의 음성 및 얼굴 표정을 포함하는 미디어 데이터를 추가 로 취득할 수 있다. 가능한 구현에서, 3개, 5개, 또는 훨씬 더 많은 초고화질 카메라가 전방에서의 화자를 둘 러쌈으로써 촬영을 위해 선택될 수 있다. 취득 환경은 안정적인 주변 조명을 가질 필요가 있고, 화자가 특정 옷을 착용할 필요가 없다. 도 4를 참조하면, 예를 들어, 3개의 초고화질 카메라가 사용된다. 상부 파선 화살 표들은 안정적인 조명을 나타내고, 좌측 상의 3개의 화살표는 초고화질 카메라들과 화자의 시야각들 사이의 관 계들을 나타내어, 화자의 음성 및 얼굴 표정을 포함하는 미디어 데이터를 취득한다. 이 경우, 초고화질 카메라 를 사용하여 취득된 비디오는 음성 및 얼굴 표정 둘 다를 포함할 수 있다. 즉, 미디어 데이터는 비디오이다. 미디어 데이터가 취득될 때, 취득된 미디어 데이터의 표정 형태들은 얼굴 표정을 취득하기 위해 사용되는 상이 한 센서들에 따라 상이할 수 있다는 점에 유의해야 한다. 일부 경우들에서, 화자는 얼굴 모델을 확립하기 위해 RGBD(red-green-blue deep) 센서를 사용함으로써 촬영될 수 있다. RGBD 센서가 깊이 정보를 취득하여, 화자의 3차원(3D) 재구성 결과를 획득할 수 있기 때문에, 미디어 데이터는 화자에 대응하는 얼굴의 정적 모델링, 즉 3D 데이터를 포함한다. 일부 다른 경우들에서, RGBD 센서가 없을 수 있고 화자는 대신에 2차원(2D) 센서를 사용하 여 촬영된다. 이 경우, 화자의 3D 재구성 결과는 없다. 미디어 데이터는 화자에 대응하는 비디오 프레임들,즉 2D 데이터를 포함한다. S202. 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정한다. 전술한 미디어 데이터가 획득된 후에, 도 3의 얼굴 모델링 모듈을 사용하여 화자에 대해 얼굴 모델링을 수행하 여, 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 획득할 수 있다. 제1 표정 베이스는 제1 애 니메이션 캐릭터의 표정들을 식별하기 위해 사용된다. 얼굴 모델링의 목적은 위에서 언급된 화자와 같은 취득된 객체가 컴퓨터에 의해 이해되고 저장될 수 있는 것을 가능하게 하는 것이고, 이는 취득된 객체의 형상 및 텍스처를 포함한다. 얼굴 모델링은 복수의 방식으로 수행 될 수 있고, 주로 하드웨어, 수동 작업, 및 소프트웨어의 3가지 관점으로부터 구현된다. 하드웨어 관점으로부 터의 구현은 특수 디바이스, 예를 들어, 3D 스캐닝 기기를 사용하여 화자 상에서 고정밀 스캐닝을 수행하는 것 일 수 있고, 획득된 얼굴 모델에 대해 데이터를 수동/자동 클리닝하는 것이 선택될 수 있다; 수동 작업 관점으 로부터의 구현은 아트 디자이너에 의해 수동으로 데이터를 설계하거나, 데이터를 클리닝하거나, 또는 데이터를 조정하는 것일 수 있다; 소프트웨어 관점으로부터의 구현은 파라미터화된 얼굴-대-파라미터 변환 알고리즘을 사 용하여 화자의 얼굴 모델을 자동으로 생성하는 것일 수 있다. 표정 파라미터화 동안, 얼굴 모델링은 또한 하드웨어, 수동 작업 및 소프트웨어의 3개의 관점으로부터 구현될 수 있다. 예를 들어, 표정을 갖는 화자가 특수 얼굴 스캐닝 디바이스를 사용하여 스캐닝된 후에, 현재 표정의 파라미터화된 설명이 자동으로 주어진다. 이러한 설명은 스캐닝 디바이스에서의 맞춤형 표정 설명과 관련된다. 그러나, 아트 디자이너에 의해 수동으로 조정된 표정 파라미터에 대해, 표정 타입 및 대응하는 얼굴 파라미터화, 예컨대, 입 열기 및 닫기의 정도 또는 얼굴 근육의 움직임 진폭이 일반적으로 미리 정의될 필요가 있다. 소프트웨어를 통해 구현되는 표정 파라미터화의 경우, 상이한 표정들에서의 얼굴의 수학적 설명들이 일 반적으로 정의될 필요가 있다. 예를 들어, 대량의 실제 얼굴 데이터가 PCA(principal component analysis) 방 법을 사용하여 분해된 후에, 평균 얼굴에 대한 각각의 표정의 변화의 정도를 가장 잘 반영하는 수치적 설명이 획득된다. 이 실시예에서, 주로 소프트웨어에 기초한 얼굴 모델링 및 표정 파라미터화가 설명된다. 이 경우, 상이한 표정 들에서의 얼굴의 수학적 설명들은 모델 라이브러리를 사용하여 정의될 수 있다. 본 출원의 이 실시예에서의 애 니메이션 캐릭터(예를 들어, 제1 애니메이션 캐릭터 및 후속 제2 애니메이션 캐릭터)는 모델 라이브러리 내의 모델일 수 있거나, 모델 라이브러리 내의 모델들의 선형 조합을 통해 획득될 수 있다. 모델 라이브러리는 얼굴 3D 모핑가능 모델(3DMM) 라이브러리, 또는 다른 모델 라이브러리일 수 있으며, 이는 이 실시예에서 제한되지 않 는다. 애니메이션 캐릭터는 3D 그리드일 수 있다. 3DMM 라이브러리가 예로서 사용된다. 3DMM 라이브러리는 PCA 방법을 사용하여 대량의 고정밀 얼굴 데이터에 기 초하여 획득되거나, 평균 얼굴에 대한 고차원 얼굴 형상 및 표정의 주요 변화를 설명하거나, 텍스처 정보를 설 명할 수 있다. 일반적으로, 3DMM 라이브러리가 무표정 얼굴 형상을 설명할 때, 3DMM 라이브러리는"}
{"patent_id": "10-2021-7029221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "를 통해 획득될 수 있다. mu는 자연 표정을 갖는 평균 얼굴이고, 는 제i 얼굴 형상 주성분이고, αi는 각각의 얼굴 형상 주성분의 가중치, 즉 얼굴-대-파라미터 변환 파라미터이다. 3DMM 라이브러리 내의 애니메이션 캐릭터에 대응하는 그리드가 M에 의해 표현될 수 있다고 가정하면, 즉, 3DMM 라이브러리 내의 얼굴 형상, 수학식, 및 정점 간의 관계가 M에 의해 표현되고, M은 [m × n × d] 3D 행렬이고, 여기서 차원들은 각각 그리드의 정점 좌표 (m), 얼굴 형상 주성분 (n), 및 표정 주성분 (d)이다. 3DMM 라이브 러리 내의 M의 각각의 차원의 분포 및 의미가 도 5에 도시되고, 좌표 축들은 각각 정점 좌표 (m), 얼굴 형상 주 성분 (n), 및 표정 주성분 (d)을 나타낸다. m은 x, y 및 z의 3개의 좌표의 값들을 나타내기 때문에, 그리드의 정점들의 수량은 m/3이고, 이는 v로 표시된다. 애니메이션 캐릭터의 얼굴 형상 또는 표정이 결정되는 경우, M 은 2D 행렬일 수 있다. 본 출원의 이 실시예에서, 3DMM 라이브러리에서의 텍스처 차원을 고려하지 않고, 애니메이션 캐릭터의 구동이 F 라고 가정하면, 여기서, M은 애니메이션 캐릭터의 그리드이고, α는 얼굴-대-파라미터 변환 파라미터이고, β는 표정 파라미터 이고; 여기서, n은 얼굴-대-파라미터 변환 베이스에서의 얼굴-대-파라미터 변환 그리드들의 수량이고, d는 표정 베이스에서의 표정 그리드들의 수량이고, 는 제i 표정 그리드 및 제j 얼굴-대-파라미터 변환 그리드를 포함 하는 제k 그리드이고, αj는 얼굴-대-파라미터 변환 파라미터들의 세트 내의 제j 차원이고, 제j 얼굴 형상 주성 분의 가중치를 나타내고, βi는 표정 파라미터들의 세트 내의 제i 차원이고, 제i 표정 주성분의 가중치를 나타낸 다. 얼굴-대-파라미터 변환 파라미터를 결정하는 프로세스는 얼굴-대-파라미터 변환 알고리즘이고, 표정 파라미터를 결정하는 프로세스는 표정-대-파라미터 변환 알고리즘이다. 얼굴-대-파라미터 변환 파라미터는 얼굴-대-파라미 터 변환 베이스와 선형 조합하여 대응하는 얼굴 형상을 획득하기 위해 사용된다. 예를 들어, 50개의 얼굴-대- 파라미터 변환 그리드들(변형가능한 그리드들, 예를 들어, 블렌드셰이프(blendshape)들에 관련됨)을 포함하는 얼굴-대-파라미터 변환 베이스가 있고, 얼굴-대-파라미터 변환 베이스에 대응하는 얼굴-대-파라미터 변환 파라 미터는 50개의 차원을 갖는 벡터이고, 각각의 차원은 얼굴-대-파라미터 변환 파라미터에 대응하는 얼굴 형상과 하나의 얼굴-대-파라미터 변환 그리드 사이의 상관의 정도를 식별할 수 있다. 얼굴-대-파라미터 변환 베이스에 포함된 얼굴-대-파라미터 변환 그리드들은 각각 상이한 얼굴 형상을 나타내고, 각각의 얼굴-대-파라미터 변환 그리드는 평균 얼굴에 대해 크게 변하는 얼굴 외관이고, 많은 수량의 얼굴을 PCA가 분해함으로써 획득된 상이한 차원의 얼굴 형상 주성분이고, 동일한 얼굴-대-파라미터 변환 베이스 내의 상이한 얼굴-대-파라미터 변환 그리 드들에 대응하는 정점 일련 번호들은 서로 일치한다. 표정 파라미터는 표정 베이스와 선형 조합하여 대응하는 표정을 획득하기 위해 사용된다. 예를 들어, 50개(50 개의 차원 수량과 동등함)의 표정 그리드들(변형가능한 그리드들, 예를 들어, 블렌드셰이프(blendshape)들에 관 련됨)을 포함하는 표정 베이스가 있고, 표정 베이스에 대응하는 표정 파라미터는 50개의 차원을 갖는 벡터이고, 각각의 차원은 표정 파라미터에 대응하는 표정과 하나의 표정 그리드 사이의 상관의 정도를 식별할 수 있다. 표정 베이스에 포함된 표정 그리드들 각각은 상이한 표정을 나타내고, 각각의 표정 그리드는 상이한 표정들 하 에서 동일한 3D 모델을 변경함으로써 형성되고, 동일한 표정 베이스 내의 상이한 표정 그리드들에 대응하는 정 점 일련 번호들은 서로 일치한다. 전술한 변형가능한 그리드들에 대해, 단일 그리드가 미리 정의된 형상에 따라 변형되어, 임의의 수량의 그리드 들을 획득할 수 있다. 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스는 전술한 수학식 과 조합하여 획득되어, 후속하 여 제2 애니메이션 캐릭터를 구동할 수 있다. S203. 타깃 텍스트 정보, 미디어 데이터, 및 제1 표정 베이스에 따라, 타깃 텍스트 정보에 대응하는 음향 특징 및 타깃 표정 파라미터를 결정한다. 타깃 텍스트 정보에 대응하는 음향 특징 및 타깃 표정 파라미터는 도 3의 음향 특징 및 표정 파라미터 결정 모 듈을 사용하여 결정될 수 있다. 음향 특징은 화자가 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 사운드를 식 별하기 위해 사용되고, 타깃 표정 파라미터는 제1 표정 베이스에 대해 화자가 타깃 텍스트 정보를 말할 때 시뮬 레이팅되는 얼굴 표정의 변화 정도를 식별하기 위해 사용된다. 타깃 텍스트 정보를 획득하는 복수의 방식이 존재할 수 있다는 점이 이해될 수 있다. 예를 들어, 타깃 텍스트 정보는 단말 디바이스를 통해 사용자에 의해 입력될 수 있거나, 단말 디바이스에 입력된 음성에 따른 변환을 통 해 획득될 수 있다. S204. 음향 특징 및 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동한다. 도 3의 애니메이션 구동 모듈을 사용함으로써, 음향 특징에 의해 식별된 음성과 조합하여 타깃 표정 파라미터에 의해 식별된 표정은 사람이 직관적으로 이해할 수 있는 방식으로 복수의 감각을 사용하여 제시된다. 실현가능 한 방식은 타깃 표정 파라미터가 제2 표정 베이스 내의 표정 그리드들의 가중치들을 나타낸다고 가정하면, 대응 하는 표정이 제2 표정 베이스의 선형 가중 조합을 통해 획득될 수 있다는 것이다. 음성 생성 동안, 음성에 대 응하는 표정을 행하는 제2 애니메이션 캐릭터가 렌더링 방법을 사용하여 렌더링되어, 제2 애니메이션 캐릭터를 구동한다. 전술한 기술적 해결책들로부터, 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스는 화자가 음성을 말 할 때 얼굴 표정 변화를 포함하는 비디오를 취득함으로써 결정될 수 있고, 제1 표정 베이스는 제1 애니메이션 캐릭터의 상이한 표정들을 반영할 수 있다는 것을 알 수 있다. 제2 애니메이션 캐릭터를 구동하기 위해 사용되 는 타깃 텍스트 정보가 결정된 후에, 타깃 텍스트 정보, 전술한 미디어 데이터, 및 제1 표정 베이스에 따라 타 깃 텍스트 정보에 대응하는 음향 특징 및 타깃 표정 파라미터가 결정될 수 있다. 음향 특징은 화자가 타깃 텍 스트 정보를 말할 때 시뮬레이팅되는 사운드를 식별하기 위해 사용될 수 있고, 타깃 표정 파라미터는 제1 표정 베이스에 대해 화자가 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정의 변화 정도를 식별하기 위해 사 용될 수 있다. 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터는 음향 특징 및 타깃 표정 파라미터에 따라 구 동될 수 있어, 제2 애니메이션 캐릭터는 음향 특징에 따라, 화자가 타깃 텍스트 정보를 말할 때 생성되는 사운 드를 시뮬레이팅하고, 발성 동안 화자의 표정에 부합하는 얼굴 표정을 행하여, 사용자에게 현실적인 존재감 및 몰입감을 야기함으로써, 사용자와 애니메이션 캐릭터 사이의 상호작용의 경험을 개선할 수 있다. S203의 복수의 구현이 있을 수 있다는 점에 유의해야 한다. 일 구현은 본 출원의 이 실시예에서 상세히 설명된 다. 가능한 구현에서, S203의 구현은 타깃 텍스트 정보 및 미디어 데이터에 따라 타깃 텍스트 정보에 대응하는 음향 특징 및 표정 특징을 결정하는 것을 포함할 수 있다. 음향 특징은 화자가 타깃 텍스트 정보를 말할 때 시뮬레 이팅되는 사운드를 식별하기 위해 사용되고, 표정 특징은 화자가 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정을 식별하기 위해 사용된다. 그 후, 제1 표정 베이스 및 표정 특징에 따라 타깃 표정 파라미터가 결 정된다. 화자의 얼굴 표정 및 음성은 미디어 데이터에 동기적으로 기록되었다. 즉, 미디어 데이터에서의 화자의 얼굴 표정 및 음성은 동일한 시간 축에 대응한다. 따라서, 많은 수량의 미디어 데이터가 훈련 데이터로서 오프라인 으로 미리 취득될 수 있다. 텍스트 특징, 음향 특징, 및 표정 특징은 미디어 데이터로부터 추출되고, 지속기간 모델, 음향 모델, 및 표정 모델은 특징들로 훈련함으로써 획득된다. 화자의 얼굴 표정 및 대응하는 음성을 포 함하는 미디어 데이터가 온라인으로 획득될 때, 타깃 텍스트 정보에 대응하는 지속기간은 지속기간 모델을 사용 하여 결정될 수 있고, 그 후 대응하는 음향 특징 및 표정 특징은 지속기간을 타깃 텍스트 정보에 대응하는 텍스 트 특징과 조합함으로써 음향 모델 및 표정 모델을 각각 사용하여 결정된다. 음향 특징 및 표정 특징 둘 다가 동일한 지속기간 모델에 기초하여 획득된 지속기간에 대응하기 때문에, 음성은 표정과 용이하게 동기화되어, 제 2 애니메이션 캐릭터는 화자를 시뮬레이팅하여 대응하는 표정을 행하는 한편 화자를 시뮬레이팅하여 타깃 텍스 트 정보에 대응하는 음성을 말한다. 다음은 S204의 가능한 구현을 설명한다. 이 실시예에서, 제2 애니메이션 캐릭터는 제1 애니메이션 캐릭터와 동 일한 애니메이션 캐릭터일 수 있거나, 제1 애니메이션 캐릭터와 상이한 애니메이션 캐릭터일 수 있다는 것을 이 해해야 한다. 2개의 경우에, S204의 구현들은 상이할 수 있다. 제1 경우에, 제1 애니메이션 캐릭터 및 제2 애니메이션 캐릭터는 동일한 애니메이션 캐릭터이다. 이 경우, 구동될 필요가 있는 애니메이션 캐릭터가 제1 애니메이션 캐릭터이다. 이러한 방식으로, 제1 애니메 이션 캐릭터를 구동하기 위해, 제1 표정 베이스를 결정하는 것에 더하여 제1 애니메이션 캐릭터의 얼굴-대-파라 미터 변환 파라미터가 추가로 결정되어, 제1 애니메이션 캐릭터의 얼굴 형상을 획득할 필요가 있다. 따라서, 단계 S202에서, 제1 애니메이션 캐릭터의 제1 표정 베이스 및 제1 애니메이션 캐릭터의 얼굴-대-파라미터 변환 파라미터가 얼굴 표정에 따라 결정될 수 있고, 얼굴-대-파라미터 변환 파라미터는 제1 애니메이션 캐릭터에 대 응하는 얼굴-대-파라미터 변환 베이스에 대한 제1 애니메이션 캐릭터의 얼굴 형상의 변화 정도를 식별하기 위해 사용된다. 제1 애니메이션 캐릭터의 제1 표정 베이스 및 제1 애니메이션 캐릭터의 얼굴-대-파라미터 변환 파라미터를 결정 하는 복수의 방식이 있다. 일부 경우들에서, 얼굴-대-파라미터 변환 파라미터가 얼굴 모델을 확립하기 위해 미 디어 데이터에 기초하여 결정될 때, 취득된 미디어 데이터는 일반적으로 낮은 정밀도 및 비교적 큰 노이즈를 갖 는다. 따라서, 확립된 얼굴 모델은 열악한 품질 및 많은 불확실성들을 갖고, 구성될 객체의 실제 외관을 정확 하게 반영하는 것은 어렵다. 예를 들어, 모델링 품질은 비표준 취득으로 인해 낮다; 재구성 프로세스는 주변 조명, 사용자 메이크업 등에 의해 용이하게 영향을 받는다; 재확립된 얼굴 모델은 자연 상태에 있지 않은 표정 을 포함한다; 확립된 얼굴 모델은 표정 파라미터가 추출될 비디오에 적응할 수 없다. 문제를 해결하기 위해, 본 출원의 실시예는 도 6을 참조하여 얼굴-대-파라미터 변환 파라미터를 결정하기 위한 방법을 제공한다. 도 6에서, 획득된 미디어 데이터가 복수의 얼굴 정점 데이터 세트를 포함할 수 있는 경우, 초기 얼굴-대-파라미 터 변환 파라미터는 타깃 얼굴 모델을 식별하기 위해 사용되는 3DMM 라이브러리 내의 타깃 정점 데이터 및 복수 의 얼굴 정점 데이터 세트 내의 제1 정점 데이터에 기초하여 결정될 수 있다. 초기 얼굴-대-파라미터 변환 파 라미터를 결정하는 것에 기초하여, 미디어 데이터 내의 제2 정점 데이터를 획득함으로써 초기 얼굴-대-파라미터 변환 파라미터 및 타깃 정점 데이터에 기초하여 표정 파라미터가 결정되고, 그 후 표정 파라미터가 고정되어, 얼굴-대-파라미터 변환 파라미터를 역으로 추론하거나, 얼굴 형상을 어떻게 변경할지를 역으로 추론하여 표정 파라미터 하에서 화자의 얼굴 외관을 획득한다. 즉, 얼굴 형상을 역으로 추론하고 표정을 고정함으로써 초기 얼굴-대-파라미터 변환 파라미터가 정정되어, 타깃 얼굴-대-파라미터 변환 파라미터를 획득하여, 타깃 얼굴-대- 파라미터 변환 파라미터를 제1 애니메이션 캐릭터의 얼굴-대-파라미터 변환 파라미터로서 사용한다. 제2 정점 데이터 및 제1 정점 데이터는 각각 구성될 객체의 상이한 얼굴 외관들을 식별하기 때문에, 제2 정점 데이터 및 제1 정점 데이터가 완전히 동일한 불확실성에 의해 영향을 받을 확률은 비교적 작다. 제1 정점 데이 터에 기초하여 초기 얼굴-대-파라미터 변환 파라미터를 결정하는 것에 기초하여, 제2 정점 데이터에 기초하여 정정된 타깃 얼굴-대-파라미터 변환 파라미터는 제1 정점 데이터 내의 노이즈를 어느 정도 오프셋할 수 있어, 타깃 얼굴-대-파라미터 변환 파라미터를 사용하여 결정되는 화자에 대응하는 얼굴 모델은 비교적 높은 정확도를 갖는다. 제1 표정 베이스와 제2 표정 베이스가 동일하기 때문에, 즉 제1 표정 베이스와 제2 표정 베이스의 차원 수량들 및 각각의 차원의 시맨틱 정보가 동일하기 때문에, 결정된 타깃 표정 파라미터는 제2 애니메이션 캐릭터를 직접 구동할 수 있다. 따라서, S204에서 제2 애니메이션 캐릭터를 구동하는 방식은 음향 특징, 타깃 표정 파라미터, 및 얼굴-대-파라미터 변환 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하는 것일 수 있다. 제2 경우에, 제1 애니메이션 캐릭터 및 제2 애니메이션 캐릭터는 상이한 애니메이션 캐릭터들이다. 이 경우, 제1 표정 베이스는 제2 표정 베이스와 상이하기 때문에, 즉, 제1 표정 베이스 및 제2 표정 베이스의 차원 수량들 및 각각의 차원의 시맨틱 정보에 차이들이 있기 때문에, 타깃 표정 파라미터를 직접 사용함으로써 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하는 것이 어렵다. 제1 애니메이션 캐릭터에 대응하는 표정 파라미터와 제2 애니메이션 캐릭터에 대응하는 표정 파라미터 사이에 매핑 관계가 있고, 제1 애니메이션 캐릭터에 대응하는 표정 파라미터와 제2 애니메이션 캐릭터에 대응하는 표정 파라미터 사이의 매핑 관계는 함수 f()를 사용하여 표현될 수 있다. 이러한 방식으로, 제1 애니메이션 캐릭터에 대응하는 표정 파라미터에 따라 제2 애니메이션 캐릭터에 대응하는 표정 파라미터를 계산하기 위한 수학식은 다음과 같다:"}
{"patent_id": "10-2021-7029221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, βb는 제2 애니메이션 캐릭터에 대응하는 표정 파라미터이고, βa는 제1 애니메이션 캐릭터에 대응하는 표정 파라미터이고, f()는 제1 애니메이션 캐릭터에 대응하는 표정 파라미터와 제2 애니메이션 캐릭터에 대응하 는 표정 파라미터 사이의 매핑 관계를 나타낸다. 따라서, 매핑 관계가 결정되는 경우, 제2 애니메이션 캐릭터(예를 들어, 애니메이션 캐릭터 b)는 제1 애니메이 션 캐릭터(예를 들어, 애니메이션 캐릭터 a)에 대응하는 표정 파라미터를 사용하여 직접 구동될 수 있다. 매핑 관계는 선형 매핑 관계 또는 비선형 매핑 관계일 수 있다. 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하기 위해, 매핑 관계가 결정 될 필요가 있다. 매핑 관계를 결정하는 복수의 방식이 있을 수 있고, 그 중 2개가 이 실시예에서 주로 기술된 다. 제1 결정 방식은 제1 애니메이션 캐릭터에 대응하는 제1 표정 베이스 및 제2 애니메이션 캐릭터에 대응하는 제2 표정 베이스에 기초하여 표정 파라미터들 간의 매핑 관계를 결정하는 것을 포함할 수 있다. 도 7을 참조하면, 제1 애니메이션 캐릭터는 제1 애니메이션 캐릭터에 대응하는 실제 표정 파라미터에 따라 구동되어 실제 표정을 만들 수 있고, 실제 표정 파라미터는 상이한 차원들 하에서의 실제 표정과 표정 베이스 사이의 상관도를 반영할 수 있는데, 즉, 제2 애니메이션 캐릭터에 대응하는 실제 표정 파라미터는 또한 상이한 차원들 하에서의 제2 애 니메이션 캐릭터의 실제 표정과 표정 베이스 사이의 상관도를 반영할 수 있다. 따라서, 표정 파라미터들 간의 매핑 관계는 제1 애니메이션 캐릭터에 대응하는 제1 표정 베이스 및 제2 애니메이션 캐릭터에 대응하는 제2 표 정 베이스에 따라 전술한 표정 파라미터와 표정 베이스 사이의 연관 관계에 기초하여 결정될 수 있다. 그 후, 음향 특징, 타깃 표정 파라미터, 및 매핑 관계에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터가 구동된다. 제2 결정 방식은 음소와 제2 표정 베이스 사이의 미리 설정된 관계에 기초하여 표정 파라미터들 간의 매핑 관계 를 결정하는 것을 포함할 수 있다. 음소는 음성의 자연 속성에 따라 분할을 통해 획득되는 최소 음성 단위이고, 음절에서의 발음 액션들에 대한 분 석에 따라, 액션(예를 들어, 입 형상)이 음소를 형성한다. 즉, 음소는 화자와 독립적이다. 입 형상들과 같은 대응하는 표정들은, 화자가 누구인지, 음성의 언어가 영어인지 중국어인지, 및 생성된 음소들에 대응하는 텍스 트들이 동일한지에 관계없이, 음성에서의 시간 간격 내의 음소들이 동일한 한 일관성을 갖는다. 도 8을 참조하 면, 도 8은 시간 간격과 음소 사이의 대응을 도시하고, 어느 시간 간격이 음성에서의 어느 음소에 대응하는지를 설명한다. 예를 들어, 제2 행의 \"5650000\" 및 \"6300000\"은 타임스탬프들을 나타내고, 5.65초와 6.3초 사이의 시간 간격을 나타내며, 시간 간격에서 화자에 의해 생성된 음소는 \"u\"이다. 음소들에 대한 통계를 수집하기 위 한 다양한 방법이 있으며, 본 실시예에서는 33개의 중국어 음소가 예로서 사용된다. 미디어 데이터에서의 얼굴 표정 및 음성은 동기적으로 취득되고, 대응하는 비디오 프레임들은 음성의 분할을 통 해 편리하게 획득될 수 있다. 즉, 음성에 의해 식별된 음소, 음소에 대응하는 시간 간격, 및 시간 간격 내의 미디어 데이터의 비디오 프레임들이 미디어 데이터에 따라 결정된다. 이어서, 비디오 프레임들에 따라 음소에 대응하는 제1 표정 파라미터가 결정되고, 제1 표정 파라미터는 제1 표정 베이스에 대해 음소를 제공할 때 화자 의 얼굴 표정의 변화 정도를 식별하기 위해 사용된다. 예를 들어, 도 8의 제2 행에서, 음소 \"u\"에 대해, 음소에 대응하는 시간 간격은 5.65초와 6.3초 사이이고, 5.65 초와 6.3초 사이의 시간 간격에서의 비디오 프레임들이 결정되고, 음소 \"u\"에 대응하는 제1 표정 파라미터가 비 디오 프레임들에 따라 추출된다. 제1 애니메이션 캐릭터가 애니메이션 캐릭터 a인 경우, 제1 표정 파라미터는 βa로 표현될 수 있다. 제1 표정 베이스의 차원 수량이 na인 경우, 획득된 제1 표정 파라미터 βa는 na의 길이 를 갖는 벡터들의 세트이다. 매핑 관계를 결정하는 방식의 전제는 다른 애니메이션 캐릭터의 표정 베이스, 예를 들어, 제2 애니메이션 캐릭 터에 대응하는 제2 표정 베이스가 음소와의 미리 설정된 관계에 따라 생성되는 것이다. 미리 설정된 관계는 하 나의 음소가 하나의 표정 그리드에 대응하는 것을 나타낸다. 예를 들어, 제2 애니메이션 캐릭터 b에 대해, 미 리 설정된 관계에서의 음소 \"u\"는 제1 표정 그리드에 대응하고, 음소 \"i\"는 제2 표정 그리드에 대응하는 등등이 다. nb개의 음소가 있는 경우, nb개의 표정 그리드를 포함하는 제2 표정 베이스가 미리 설정된 관계에 따라 결 정될 수 있다. 이 경우, 음성에 의해 식별된 음소가 결정된 후, 음소에 대응하는 제2 표정 파라미터는 미리 설 정된 관계 및 제2 표정 베이스에 따라 결정될 수 있다. 이어서, 제1 표정 파라미터 및 제2 표정 파라미터에 따 라 매핑 관계가 결정된다. 예를 들어, 음성에 의해 식별된 음소가 \"u\"이고, 제2 표정 베이스 및 미리 설정된 관계에 따라 음소 \"u\"가 제1 표정 그리드에 대응한다는 것을 알 수 있는 경우, 제2 표정 파라미터가 이고, βb는 nb개의 요 소를 포함하고, 나머지 (nb-1)개의 요소는 제1 요소가 1인 것을 제외하고는 모두 0인 것으로 결정될 수 있다. 이러한 방식으로, βb 와 βa 사이의 매핑 관계들이 확립된다. 많은 수량의 제1 표정 파라미터 βa가 획득될 때, 많은 수량의 대응하는 제2 표정 파라미터 βb가 생성될 수 있다. L개의 제1 표정 파라미터 βa 및 L개의 제 2 표정 파라미터 βb가 있다고 가정하면, L개의 제1 표정 파라미터 βa는 제1 행렬을 형성하고, L개의 제2 표정 파라미터 βb는 제2 행렬을 형성하며, 이들은 각각 βA 및 βB로 표시되고, 여기서 . 이 해결책에서, 예를 들어, 제1 표정 파라미터와 제2 표정 파라미터 사이에 선형 매핑 관계가 있다; 따라서, 전 술한 수학식 는 다음으로 변환될 수 있다: 수학식 및 수학식 에 따르면, 매핑 관계를 결정하기 위한 수학식은 다음과 같을 수 있다:"}
{"patent_id": "10-2021-7029221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, f는 매핑 관계이고, βA는 제1 행렬이고, βB는 제2 행렬이고, inv는 행렬 역전 연산(matrix inversion operation)이다. 매핑 관계 f가 획득된 후에, 임의의 제1 표정 파라미터 βa에 대해, 대응하는 가 획득되어, 제1 표정 파라미터에 따라 제2 표정 파라미터를 획득하여, 제2 애니메이션 캐릭터, 예를 들어, 애니메이션 캐릭터 b를 구 동할 수 있다. 텍스트 정보에 기초하여 애니메이션 캐릭터를 구동하는 방법은 전술한 실시예들에서 주로 설명된다. 일부 경우 들에서, 애니메이션 캐릭터는 미디어 데이터에 기초하여 직접 구동될 수 있다. 예를 들어, 미디어 데이터 내의 화자에 대응하는 제1 애니메이션 캐릭터는 제1 표정 베이스를 갖고, 제1 표정 베이스의 차원 수량은 제1 차원 수량이고, 제1 표정 베이스의 정점 토폴로지는 제1 정점 토폴로지이고, 제1 표정 베이스는 Ea로 표현될 수 있고, 제1 차원 수량은 Na로 표현될 수 있고, 제1 정점 토폴로지는 Ta로 표현될 수 있고, 제1 표정 베이스의 외 관 Ea는 Fa이다; 구동될 제2 애니메이션 캐릭터는 제2 표정 베이스를 갖고, 제2 표정 베이스의 차원 수량은 제2 차원 수량이고, 제2 표정 베이스의 정점 토폴로지는 제2 정점 토폴로지이고, 제2 표정 베이스는 Eb로 표현될 수 있고, 제2 차원 수량은 Nb로 표현될 수 있고, 제2 정점 토폴로지는 Tb로 표현될 수 있고, 제2 표정 베이스의 외 관 Eb는 Fb이다. 제2 애니메이션 캐릭터는 화자의 얼굴 표정 및 음성을 포함하는 미디어 데이터를 사용하여 구 동될 것으로 예상된다. 이를 위해, 본 출원의 실시예는 AI 기반 애니메이션 캐릭터 구동 방법을 추가로 제공한다. 도 9를 참조하면, 방법은 다음의 단계들을 포함한다: S901. 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제1 미디어 데이터를 획득한다. S902. 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정한다. S903. 제1 표정 베이스 및 구동될 제2 애니메이션 캐릭터의 제2 표정 베이스에 따라 타깃 표정 베이스를 결정 한다. 이 실시예에서, 제1 표정 베이스의 차원 수량은 제2 표정 베이스의 차원 수량과 상이하고, 미디어 데이터 내의 화자의 얼굴 표정 및 음성에 따라 제2 애니메이션 캐릭터를 구동하기 위해, 새로운 표정 베이스, 예를 들어, 타 깃 표정 베이스가 구성될 수 있어, 타깃 표정 베이스는 제1 표정 베이스 및 제2 표정 베이스 둘 다의 특징들을 갖는다. 일 구현에서, S903의 구현은 다음을 포함할 수 있다: 제1 표정 베이스로부터, 무표정인 제1 애니메이션 캐릭터 에 대응하는 무표정 그리드를 결정하고, 제2 표정 베이스로부터, 무표정인 제2 애니메이션 캐릭터에 대응하는 무표정 그리드를 결정하는 것; 제1 캐릭터에 대응하는 무표정 그리드 및 제2 캐릭터에 대응하는 무표정 그리드 에 따라 조정 그리드를 결정하는 것- 조정 그리드는 무표정인 제1 애니메이션 캐릭터를 식별하기 위해 제2 정점 토폴로지를 가짐 -; 및 조정 그리드 및 제2 표정 베이스에서의 그리드 변형 관계에 따라 타깃 표정 베이스를 생 성하는 것. 제1 표정 베이스가 Ea이고, 제1 차원 수량이 Na이고, 제1 정점 토폴로지가 Ta이고, 제1 표정 베이스 Ea의 외관 이 Fa이고, 제2 표정 베이스가 Eb이고, 제2 차원 수량이 Nb이고, 제2 정점 토폴로지가 Tb이고, 제2 표정 베이스 Eb의 외관이 Fb인 경우, 방법의 흐름도에 대해서는 도 10a를 참조할 수 있다. 타깃 표정 베이스 Eb'는 제1 표 정 베이스 Ea 및 제2 표정 베이스 Eb에 기초하여 결정된다. 타깃 표정 베이스 Eb'를 결정하는 방식은 제2 표정 베이스 Eb의 무표정 그리드 및 제1 표정 베이스 Ea의 무표정 그리드를 추출하는 것을 포함할 수 있다. Eb의 무 표정 그리드는 얼굴-대-파라미터 변환 알고리즘, 예를 들어, nricp 알고리즘을 사용하여 Ea의 무표정 그리드에 첨부되어, Eb의 무표정 그리드의 외관은 정점 토폴로지 Fb를 유지하면서 Ea의 외관으로 변경되어, 조정 그리드 를 획득한다. 조정 그리드는 Newb에 의해 표현될 수 있다. 후속하여, Newb 및 제2 표정 베이스 Eb 내의 각각 의 차원에서의 표정과 자연 표정(표정 없음) 사이의 그리드 변형 관계가 이미 알려져 있기 때문에, 타깃 표정 베이스 Eb'는 Newb 및 제2 표정 베이스 Eb 내의 그리드 변형 관계에 따라 Newb로부터 변형될 수 있다. 타깃 표 정 베이스 Eb'의 외관은 Fa이고, 타깃 표정 베이스 Eb'의 차원들의 수량은 Nb이고, 타깃 표정 베이스 Eb'의 정점 토폴로지는 Tb이다. S904. 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제2 미디어 데이터 및 타깃 표정 베이스에 따라 타깃 표 정 파라미터 및 음향 특징을 결정한다. 타깃 표정 베이스가 획득된 후에, 도 10a를 참조하면, 음향 특징은 화자의 얼굴 표정 및 대응하는 음성을 포함 하는 제2 미디어 데이터 및 타깃 표정 베이스 Eb'에 따라 추출되고, 타깃 표정 파라미터 Bb는 표정-대-파라미터 변환 알고리즘을 사용하여 획득된다. 타깃 표정 파라미터는 타깃 표정 베이스에 대해 화자가 음성을 말할 때 얼굴 표정의 변화 정도를 식별하기 위해 사용된다. 이 방법을 사용하여 획득된 타깃 표정 파라미터 및 음향 특징은 전술한 음향 모델 및 표정 모델을 재훈련시키기 위해 사용될 수 있다는 것이 이해될 수 있다. S905. 타깃 표정 파라미터 및 음향 특징에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동한다. S901, S902, 및 S905의 특정 구현들에 대해, S201, S202, 및 S204의 전술한 구현들이 각각 참조될 수 있고, 세 부 사항들은 본 명세서에서 다시 설명되지 않는다. 다음으로, 본 출원의 실시예들에서 제공되는 AI 기반 애니메이션 캐릭터 구동 방법이 실제 응용 시나리오를 참 조하여 설명된다. 이 응용 시나리오에서, 제1 애니메이션 캐릭터는 화자의 캐릭터를 모방함으로써 구성되고, 제2 애니메이션 캐릭 터는 게임에서 사용자와 상호작용하는 게임 역할의 캐릭터인 것으로 가정된다. 게임 역할이 입력된 타깃 텍스 트 정보에 따라 사용자와 통신할 때, 타깃 텍스트 정보에 따라, 화자를 시뮬레이팅하여 타깃 텍스트 정보에 대 응하는 음성을 생성하고 대응하는 표정을 행하도록 게임 역할을 구동할 것으로 예상된다. 따라서, 단말 디바이 스는 화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터를 획득하고, 얼굴 표정에 따라 화자에 대응 하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정할 수 있다. 그 후, 단말 디바이스는, 타깃 텍스트 정보, 미디어 데이터, 및 제1 표정 베이스에 따라, 타깃 텍스트 정보에 대응하는 음향 특징 및 타깃 표정 파라 미터를 결정하여, 음향 특징 및 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하여, 제2 애니메이션 캐릭터가 타깃 텍스트 정보에 대응하는 음성을 생성하고, 대응하는 표정을 행하게 한 다. 이러한 방식으로, 사용자는 게임 역할이 화자를 시뮬레이팅하여 음성을 생성하고 대응하는 표정을 행하는 것을 알 수 있으며, 이는 현실적인 존재감 및 몰입감을 사용자에게 가져오며, 따라서 사용자와 애니메이션 캐릭 터 사이의 상호작용의 경험을 개선한다. 전술한 실시예들에서 제공되는 방법에 기초하여, 실시예는 애니메이션 캐릭터 구동 장치를 추가로 제공한 다. 장치는 오디오 및 비디오 처리 디바이스 상에 배치된다. 도 10b를 참조하면, 장치는 획득 유 닛, 제1 결정 유닛, 제2 결정 유닛, 및 구동 유닛을 포함한다: 획득 유닛은 화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터를 획득하도록 구성되고; 제1 결정 유닛은 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하 도록 구성되고, 제1 표정 베이스는 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용되고; 제2 결정 유닛은 타깃 텍스트 정보, 미디어 데이터, 및 제1 표정 베이스에 따라, 타깃 텍스트 정보에 대 응하는 음향 특징 및 타깃 표정 파라미터를 결정하도록 구성되고, 음향 특징은 화자가 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 사운드를 식별하기 위해 사용되고, 타깃 표정 파라미터는 제1 표정 베이스에 대해 화자가 타 깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정의 변화 정도를 식별하기 위해 사용되고; 및 구동 유닛은 음향 특징 및 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하도록 구성된다. 가능한 구현에서, 제1 애니메이션 캐릭터 및 제2 애니메이션 캐릭터는 동일한 애니메이션 캐릭터이고, 제1 표정 베이스는 제2 표정 베이스와 동일하고, 제1 결정 유닛은: 얼굴 표정에 따라 제1 애니메이션 캐릭터의 제1 표정 베이스 및 제1 애니메이션 캐릭터의 얼굴-대-파라미터 변 환 파라미터를 결정하도록 구성되고, 얼굴-대-파라미터 변환 파라미터는 제1 애니메이션 캐릭터에 대응하는 얼 굴-대-파라미터 변환 베이스에 대한 제1 애니메이션 캐릭터의 얼굴 형상의 변화 정도를 식별하기 위해 사용된다.구동 유닛은: 음향 특징, 타깃 표정 파라미터, 및 얼굴-대-파라미터 변환 파라미터에 따라 제2 애니메이션 캐릭터를 구동하도 록 구성된다. 가능한 구현에서, 제1 애니메이션 캐릭터 및 제2 애니메이션 캐릭터는 상이한 애니메이션 캐릭터들이고, 제1 표 정 베이스는 제2 표정 베이스와 상이하고, 구동 유닛은: 제1 표정 베이스에 대응하는 표정 파라미터와 제2 표정 베이스에 대응하는 표정 파라미터 사이의 매핑 관계를 결정하고; 및 음향 특징, 타깃 표정 파라미터, 및 매핑 관계에 따라 제2 애니메이션 캐릭터를 구동하도록 구성된다. 가능한 구현에서, 제2 표정 베이스는 제2 표정 베이스와 음소 사이의 미리 설정된 관계에 따라 생성되고, 구동 유닛은: 미디어 데이터에 따라, 음성에 의해 식별된 음소, 음소에 대응하는 시간 간격, 및 시간 간격에서의 미디어 데이 터의 비디오 프레임들을 결정하고; 비디오 프레임들에 따라 음소에 대응하는 제1 표정 파라미터를 결정하고- 제1 표정 파라미터는 제1 표정 베이스 에 대해 음소를 제공할 때 화자의 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 미리 설정된 관계 및 제2 표정 베이스에 따라 음소에 대응하는 제2 표정 파라미터를 결정하고; 및 제1 표정 파라미터 및 제2 표정 파라미터에 따라 매핑 관계를 결정하도록 추가로 구성된다. 가능한 구현에서, 제2 결정 유닛은: 타깃 텍스트 정보 및 미디어 데이터에 따라, 타깃 텍스트 정보에 대응하는 음향 특징 및 표정 특징을 결정하고- 표정 특징은 화자가 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정을 식별하기 위해 사용됨 -; 및 제1 표정 베이스 및 표정 특징에 따라 타깃 표정 파라미터를 결정하도록 구성된다. 실시예는 애니메이션 캐릭터 구동 장치를 추가로 제공한다. 장치는 오디오 및 비디오 처리 디바이 스 상에 배치된다. 도 11을 참조하면, 장치는 획득 유닛, 제1 결정 유닛, 제2 결정 유닛 , 제3 결정 유닛, 및 구동 유닛을 포함한다: 획득 유닛은 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제1 미디어 데이터를 획득하도록 구성되고; 제1 결정 유닛은 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하 도록 구성되고- 제1 표정 베이스는 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용되고, 제1 표정 베이 스의 차원 수량은 제1 차원 수량이고, 제1 표정 베이스의 정점 토폴로지는 제1 정점 토폴로지임 -; 제2 결정 유닛은 구동될 제2 애니메이션 캐릭터의 제1 표정 베이스 및 제2 표정 베이스에 따라 타깃 표정 베이스를 결정하도록 구성되고- 제2 표정 베이스의 차원 수량은 제2 차원 수량이고, 제2 표정 베이스의 정점 토 폴로지는 제2 정점 토폴로지이고, 타깃 표정 베이스는 제2 정점 토폴로지를 갖는 제1 애니메이션 캐릭터에 대응 하는 표정 베이스이고, 타깃 표정 베이스의 차원 수량은 제2 차원 수량임 -; 제3 결정 유닛은 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제2 미디어 데이터 및 타깃 표정 베이스 에 따라 타깃 표정 파라미터 및 음향 특징을 결정하도록 구성되고- 타깃 표정 파라미터는 타깃 표정 베이스에 대해 화자가 음성을 말할 때 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 및 구동 유닛은 타깃 표정 파라미터 및 음향 특징에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하도록 구성된다. 가능한 구현에서, 제2 결정 유닛은: 제1 표정 베이스로부터, 무표정인 제1 애니메이션 캐릭터에 대응하는 무표정 그리드를 결정하고, 제2 표정 베이스로부터, 무표정인 제2 애니메이션 캐릭터에 대응하는 무표정 그리드 를 결정하고; 제1 애니메이션 캐릭터에 대응하는 무표정 그리드 및 제2 애니메이션 캐릭터에 대응하는 무표정 그리드에 따라 조정 그리드를 결정하고- 조정 그리드는 무표정인 제1 애니메이션 캐릭터를 식별하기 위해 제2 정점 토폴로지를 가짐 -; 및조정 그리드 및 제2 표정 베이스에서의 그리드 변형 관계에 따라 타깃 표정 베이스를 생성하도록 구성된다. 본 출원의 실시예는 애니메이션 캐릭터 구동 디바이스를 추가로 제공한다. 디바이스는 음성을 사용하여 애니메 이션을 구동할 수 있고, 디바이스는 오디오 및 비디오 처리 디바이스일 수 있다. 이하, 첨부 도면을 참조하여 디바이스가 설명된다. 도 12를 참조하면, 본 출원의 실시예는 애니메이션 캐릭터 구동 디바이스를 제공하고, 디바이스는 대안적으로 단말 디바이스일 수 있다. 단말 디바이스는 모바일 폰, 태블릿 컴퓨터, PDA, POS(point of sales), 또는 온-보드 컴퓨터를 포함하는 임의의 스마트 단말일 수 있고, 모바일 폰인 단말 디바이스가 예로 서 사용된다. 도 12는 본 출원의 실시예에 따른 단말 디바이스와 관련된 모바일 폰의 일부의 구조의 블록도이다. 도 12를 참 조하면, 모바일 폰은 무선 주파수(RF) 회로, 메모리, 입력 유닛, 디스플레이 유닛, 센 서, 오디오 회로, Wi-Fi(wireless fidelity) 모듈, 프로세서, 및 전원과 같은"}
{"patent_id": "10-2021-7029221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "컴포넌트들을 포함한다. 본 기술분야의 통상의 기술자는 도 12에 도시된 모바일 폰의 구조가 모바일 폰에 대한 제한을 구성하지 않으며, 모바일 폰은 도면에 도시된 것들보다 많은 컴포넌트들 또는 적은 컴포넌트들을 포함할 수 있거나, 일부 컴포넌트들이 결합될 수 있거나, 상이한 컴포넌트 배치가 사용될 수 있다는 것을 이해할 수 있 다. 다음은 도 12를 참조하여 모바일 폰의 컴포넌트들을 상세히 설명한다. RF 회로는 정보 수신 및 전송 프로세스 또는 호출 프로세스 동안 신호들을 전송 및 수신하도록 구성될 수 있다. 특히, RF 회로는 기지국으로부터 다운링크 정보를 수신하고, 그 후 처리하기 위한 프로세서에 다 운링크 정보를 전달하고, 설계된 업링크 데이터를 기지국에 전송한다. 일반적으로, RF 회로는 안테나, 적어도 하나의 증폭기, 송수신기, 커플러, 저노이즈 증폭기(low noise amplifier, LNA), 및 듀플렉서를 포함하 지만, 이에 제한되지 않는다. 덧붙여서, RF 회로는 네트워크 및 다른 디바이스와는 무선 통신에 의해 또 한 통신할 수 있다. 무선 통신은 GSM(Global system for mobile communications), GPRS(general packet radio service), CDMA(Code Division Multiple Access), WCDMA(Wideband Code Division Multiple Access), LTE(Long Term Evolution), 이메일, SMS(Short Messaging Service) 등을 포함하지만 이에 제한되지 않는 임의의 통신 표 준 또는 프로토콜을 사용할 수 있다. 메모리는 소프트웨어 프로그램 및 모듈을 저장하도록 구성될 수 있다. 프로세서는 메모리에 저장되는 소프트웨어 프로그램 및 모듈을 실행하여, 모바일 폰의 다양한 기능 애플리케이션들 및 데이터 처리를 수행한다. 메모리는 프로그램 저장 영역 및 데이터 저장 영역을 주로 포함할 수 있다. 프로그램 저장 영역은 운영 체제, 적어도 하나의 기능(예컨대 사운드 재생 기능 및 이미지 디스플레이 기능)에 의해 요구되는 애플리케이션 프로그램 등을 저장할 수 있다. 데이터 저장 영역은 모바일 폰의 사용, 기타 등등에 따라 생성된 데이터(예를 들어 오디오 데이터 및 주소록)를 저장할 수 있다. 게다가, 메모리는 고속 랜덤 액세스 메 모리를 포함할 수 있고, 비휘발성 메모리, 예를 들어, 적어도 하나의 자기 디스크 저장 디바이스, 플래시 메모 리 디바이스, 또는 다른 휘발성 고체 상태 저장 디바이스를 또한 포함할 수 있다. 입력 유닛은 입력된 숫자 또는 캐릭터 정보를 수신하고, 모바일 폰의 사용자 설정 및 기능 제어에 관련된 키보드 신호 입력을 생성하도록 구성될 수 있다. 구체적으로, 입력 유닛은 터치 패널 및 다른 입 력 디바이스를 포함할 수 있다. 터치 스크린이라고도 지칭될 수 있는 터치 패널은 터치 패널 상의 또는 그 부근에서의 사용자의 터치 동작(예컨대 임의의 적합한 물체 또는 손가락 또는 스타일러스와 같은 액세 서리를 사용한 터치패널 상의 또는 그 부근에서의 사용자의 동작)을 수집하고, 미리 설정된 프로그램에 따라 대응하는 접속 장치를 구동할 수 있다. 선택적으로, 터치 패널은 2개의 부분: 터치 검출 장치 및 터치 제어기를 포함할 수 있다. 터치 검출 장치는 사용자의 터치 위치를 검출하고, 터치 동작에 의해 발생된 신호를 검출하고, 신호를 터치 제어기에 전달한다. 터치 제어기는 터치 검출 장치로부터 터치 정보를 수신하고, 터치 정보를 터치 포인트 좌표로 변환하고, 터치 포인트 좌표를 프로세서에 송신한다. 또한, 터치 제어기는 프로세서로부터 송신된 커맨드를 수신하고 실행할 수 있다. 게다가, 터치 패널은 저항성 타입, 용량성 타입, 적외선 타입, 및 표면 탄성파 타입과 같은 다수의 타입을 사용하여 구현될 수 있다. 터치 패널에 더하여, 입력 유닛은 다른 입력 디바이스를 추가로 포함할 수 있다. 구체적으 로, 다른 입력 디바이스는 물리적 키보드, 기능 키(예컨대 볼륨 제어 키 또는 스위치 키), 트랙 볼, 마우 스, 및 조이스틱 중 하나 이상을 비제한적으로 포함할 수 있다. 디스플레이 유닛은 사용자에 의해 입력된 정보 또는 사용자에 대해 제공된 정보, 및 모바일 폰의 다양한 메뉴들을 디스플레이하도록 구성될 수 있다. 디스플레이 유닛은 디스플레이 패널을 포함할 수 있다. 선택적으로, 디스플레이 패널은 액정 디스플레이(LCD), 유기 발광 다이오드(OLED) 등의 형태로 구성 될 수 있다. 또한, 터치 패널은 디스플레이 패널을 커버할 수 있다. 터치 패널 상의 또는 그 근 처의 터치 동작을 검출한 후에, 터치 패널은 터치 동작을 프로세서에 전달하여, 터치 이벤트의 타 입을 결정한다. 그 후, 프로세서는 터치 이벤트의 타입에 따라 디스플레이 패널 상에 대응하는 시 각적 출력을 제공한다. 도 12에서는 터치 패널 및 디스플레이 패널이 모바일 폰의 입력 및 출력 기능들을 구현하기 위해 2개의 별개의 부분으로서 사용되지만, 일부 실시예들에서는, 터치 패널 및 디스 플레이 패널이 통합되어 모바일 폰의 입력 및 출력 기능들을 구현할 수 있다. 모바일 폰은 광학 센서, 모션 센서, 및 다른 센서들과 같은 적어도 하나의 센서를 추가로 포함할 수 있다. 구체적으로, 광학 센서는 주변 광 센서 및 근접 센서를 포함할 수 있다. 주변 광 센서는 주변 광의 밝 기에 따라 디스플레이 패널의 휘도를 조정할 수 있다. 근접 센서는 모바일 폰이 귀로 이동될 때 디스플 레이 패널 및/또는 백라이트를 스위치 오프(switch off)할 수 있다. 모션 센서의 일 타입으로서, 가속도 센서가 다양한 방향들에서의(일반적으로 3개의 축 상의) 가속도들의 크기를 검출할 수 있고, 정적일 때 중력의 크기 및 방향을 검출할 수 있으며, 모바일 폰의 자세를 인지하는 애플리케이션(예를 들어, 가로 배향과 세로 배 향 사이의 전환, 관련된 게임, 및 자력계 자세 교정), 진동 인지와 관련된 기능(예컨대 만보계 및 노크), 기타 등등에 적용될 수 있다. 모바일 폰에서 구성될 수 있는 자이로스코프, 기압계, 습도계, 온도계, 및 적외선 센 서와 같은 다른 센서들은 본 명세서에서 추가로 설명되지 않는다. 오디오 회로, 확성기, 및 마이크로폰은 사용자와 모바일 폰 사이에 오디오 인터페이스들을 제공할 수 있다. 오디오 회로는 수신되는 오디오 데이터를 전기 신호로 변환할 수 있고, 전기 신호를 확 성기에 전송할 수 있다. 확성기는 전기 신호를 출력을 위한 사운드 신호로 변환한다. 한편, 마이 크로폰은 수집된 사운드 신호를 전기 신호로 변환한다. 오디오 회로는 전기 신호를 수신하고 전기 신호를 오디오 데이터로 변환하고, 처리를 위해 프로세서에 오디오 데이터를 출력한다. 그 후, 프로세서 는 오디오 데이터를 RF 회로를 사용하여 예를 들어, 다른 모바일 폰에 전송하거나, 오디오 데이터를 추가 처리를 위해 메모리에 출력한다. Wi-Fi는 단거리 무선 송신 기술이다. 모바일 폰은, Wi-Fi 모듈을 사용하여, 사용자가 이메일을 수신 및 송신하고, 웹 페이지를 브라우징하고, 스트림 미디어에 액세스하는 것 등을 도울 수 있다. 이는 사용자에게 무 선 광대역 인터넷 액세스를 제공한다. 도 12는 Wi-Fi 모듈을 도시하지만, Wi-Fi 모듈은 모바일 폰의 필 수 컴포넌트가 아니고, Wi-Fi 모듈은 본 발명의 본질의 범위가 변경되지 않는 한 필요에 따라 생략될 수 있다는 것을 이해할 수 있다. 프로세서는 모바일 폰의 제어 센터이고, 다양한 인터페이스들 및 라인들을 사용하여 모바일 폰의 다양한 부분들에 접속된다. 메모리에 저장된 소프트웨어 프로그램 및/또는 모듈을 가동 또는 실행하고, 메모리 에 저장된 데이터를 호출하는 것에 의해, 프로세서는 모바일 폰의 다양한 기능들 및 데이터 처리를 수행 하고, 이로써 모바일 폰에 대한 전반적인 모니터링을 수행한다. 선택적으로, 프로세서는 하나 이상의 처 리 유닛을 포함할 수 있다. 바람직하게는, 프로세서는 애플리케이션 프로세서와 모뎀을 통합할 수 있다. 애플리케이션 프로세서는 운영 체제, 사용자 인터페이스, 애플리케이션 프로그램 등을 주로 처리한다. 모뎀은 주로 무선 통신을 처리한다. 모뎀이 프로세서에 통합되지 않을 수도 있다는 것이 이해될 수 있다. 모바일 폰은 컴포넌트들에 전력을 공급하기 위한 전원(예컨대 배터리)을 추가로 포함한다. 바람직하게는, 전원은 전력 관리 시스템을 사용하는 것에 의해 프로세서에 논리적으로 접속됨으로써, 전 력 관리 시스템을 사용하는 것에 의한 충전, 방전, 및 소비 전력 관리와 같은 기능들을 구현할 수 있다. 도면에 도시되지는 않았지만, 모바일 폰은 카메라, 블루투스 모듈 등을 추가로 포함할 수 있다. 세부 사항들은 본 명세서에서 설명되지 않는다. 이 실시예에서, 단말 디바이스에 포함된 프로세서는 다음의 기능들을 추가로 갖는다: 화자의 얼굴 표정 및 대응하는 음성을 포함하는 미디어 데이터를 획득하는 기능; 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하는 기능- 제1 표정 베이 스는 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용됨 -; 타깃 텍스트 정보, 미디어 데이터, 및 제1 표정 베이스에 따라, 타깃 텍스트 정보에 대응하는 음향 특징 및 타 깃 표정 파라미터를 결정하는 기능- 음향 특징은 화자가 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 사운드를 식별하기 위해 사용되고, 타깃 표정 파라미터는 제1 표정 베이스에 대해 화자가 타깃 텍스트 정보를 말할 때 시뮬레이팅되는 얼굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 및 음향 특징 및 타깃 표정 파라미터에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하는 기능; 또는, 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제1 미디어 데이터를 획득하는 기능; 얼굴 표정에 따라 화자에 대응하는 제1 애니메이션 캐릭터의 제1 표정 베이스를 결정하는 기능- 제1 표정 베이 스는 제1 애니메이션 캐릭터의 표정들을 식별하기 위해 사용되고, 제1 표정 베이스의 차원 수량은 제1 차원 수 량이고, 제1 표정 베이스의 정점 토폴로지는 제1 정점 토폴로지임 -; 제1 표정 베이스 및 구동될 제2 애니메이션 캐릭터의 제2 표정 베이스에 따라 타깃 표정 베이스를 결정하는 기 능- 제2 표정 베이스의 차원 수량은 제2 차원 수량이고, 제2 표정 베이스의 정점 토폴로지는 제2 정점 토폴로지 이고, 타깃 표정 베이스는 제2 정점 토폴로지를 갖는 제1 애니메이션 캐릭터에 대응하는 표정 베이스이고, 타깃 표정 베이스의 차원 수량은 제2 차원 수량임 -; 화자의 얼굴 표정 및 대응하는 음성을 포함하는 제2 미디어 데이터 및 타깃 표정 베이스에 따라 타깃 표정 파라 미터 및 음향 특징을 결정하는 기능- 타깃 표정 파라미터는 타깃 표정 베이스에 대해 화자가 음성을 말할 때 얼 굴 표정의 변화 정도를 식별하기 위해 사용됨 -; 및 타깃 표정 파라미터 및 음향 특징에 따라 제2 표정 베이스를 갖는 제2 애니메이션 캐릭터를 구동하는 기능. 본 출원의 실시예는 서버를 추가로 제공한다. 도 13은 본 출원의 실시예에 따른 서버의 구조도이다. 서 버는 상이한 구성들 또는 성능으로 인해 크게 변할 수 있고, 하나 이상의 중앙 처리 유닛(CPU)(예 를 들어, 하나 이상의 프로세서) 및 메모리, 및 애플리케이션 프로그램들 또는 데이터를 저 장하는 하나 이상의 저장 매체(예를 들어, 하나 이상의 대용량 저장 디바이스)를 포함할 수 있다. 메모 리 및 저장 매체는 일시적 저장 또는 영구적 저장을 구현할 수 있다. 저장 매체에 저장되는 프로그램은 하나 이상의 모듈(도면에 나타나지 않음)을 포함할 수 있고, 각각의 모듈은 서버 상에 일련의 명령 동작들을 포함할 수 있다. 게다가, CPU는 저장 매체와 통신하고, 서버 상에서, 저장 매체 에서의 일련의 명령 동작들을 수행하도록 구성될 수 있다. 서버는 하나 이상의 전원, 하나 이상의 유선 또는 무선 네트워크 인터페이스, 하나 이상의 입력/출력 인터페이스, 및/또는 Windows ServerTM, Mac OS XTM, UnixTM, LinuxTM, 및 FreeBSDTM와 같은 하나 이상의 운영 체제를 추가로 포함할 수 있다. 전술한 실시예들에서 서버에 의해 수행되는 단계들은 도 13에 도시된 서버 구조에 기초할 수 있다. 본 출원의 실시예는 프로그램 코드를 저장하도록 구성되는 컴퓨터 판독가능 저장 매체를 추가로 제공하며, 프로 그램 코드는 전술한 실시예들에 따른 애니메이션 캐릭터 구동 방법을 수행하기 위해 사용된다. 본 출원의 실시예는 명령어들을 포함하는 컴퓨터 프로그램 제품을 추가로 제공하고, 이러한 명령어들은, 컴퓨터 상에서 실행될 때, 컴퓨터로 하여금 전술한 실시예들에 따른 애니메이션 캐릭터 구동 방법을 수행하게 한다. 본 출원의 명세서 및 첨부 도면들에서, 용어들 \"제1\", \"제2\", \"제3\", \"제4\" 등(존재하는 경우)은 특정 시퀀스 또는 우선 순위를 설명하기보다는 유사한 객체들을 구별하도록 의도된다. 이러한 방식으로 사용되는 데이터는 본 명세서에 설명된 본 출원의 실시예들이 본 명세서에 예시되거나 설명된 시퀀스 이외의 시퀀스로 구현될 수 있도록, 적절할 때 교환될 수 있다는 것을 이해해야 한다. 더욱이, \"포함하다(include)\", \"포함하다(contai n)\"이라는 용어들 및 임의의 다른 변형들은 비-배타적인 포함을 커버하는 것을 의미하고, 예를 들어, 단계들 또 는 유닛들의 리스트를 포함하는 프로세스, 방법, 시스템, 제품, 또는 디바이스가 반드시 이러한 명시적으로 열 거된 단계들 또는 유닛들로 제한되는 것은 아니고, 명시적으로 열거되지 않은 또는 이러한 프로세스, 방법, 시 스템, 제품, 또는 디바이스에 고유한 다른 단계들 또는 유닛들을 포함할 수 있다. 본 출원에서, \"적어도 하나\"는 하나 이상을 지칭하고, \"복수의\"는 2개 이상을 지칭한다는 것을 이해해야 한다. 용어 \"및/또는\"은 연관된 객체들 사이의 연관을 설명하고 3개의 연관이 존재할 수 있음을 나타내기 위해 사용된 다. 예를 들어, \"A 및/또는 B\"는 A만 존재하고, B만 존재하고, A와 B 둘 다가 존재함을 표시할 수 있으며, 여 기서 A와 B는 단수 또는 복수일 수 있다. 문자 \"/\"는 일반적으로 연관된 객체들 사이의 \"또는\" 관계를 표시한 다. \"다음 중 적어도 하나\" 또는 그의 유사한 표현은 하나의 항목 또는 복수의 항목의 임의의 조합을 포함하는 이들 항목의 임의의 조합을 지칭한다. 예를 들어, a, b, 또는 c 중 적어도 하나는 a, b, c, \"a 및 b\", \"a 및c\", \"b 및 c\", 또는 \"a, b, 및 c\"를 표시할 수 있고, 여기서 a, b, 및 c는 단수 또는 복수일 수 있다. 본 출원에서 제공되는 몇몇 실시예들에서, 개시된 시스템, 장치, 및 방법은 다른 방식들로 구현될 수 있다. 예 를 들어, 설명된 장치 실시예는 단지 예시적이다. 예를 들어, 유닛 분할은 단지 논리 기능 분할이고 실제 구현 동안 다른 분할일 수 있다. 예를 들어, 복수의 유닛 또는 컴포넌트가 조합되거나 다른 시스템에 통합되거나, 일부 특징들이 무시되거나 수행되지 않을 수 있다. 또한, 표시되는 또는 논의되는 상호 결합들 또는 직접 결합 들 또는 통신 접속들은 일부 인터페이스들을 사용하여 구현될 수 있다. 장치들 또는 유닛들 사이의 간접적인 결합들 또는 통신 접속들은 전자적, 기계적 또는 기타의 형태들로 구현될 수 있다. 별개의 부분들로서 설명된 유닛들은 물리적으로 분리되거나 분리되지 않을 수도 있고, 유닛들로서 표시된 컴포 넌트들은 물리적 유닛들이거나 아닐 수도 있는데, 즉, 한 위치에 위치하거나, 복수의 네트워크 유닛에 분산될 수도 있다. 일부 또는 모든 유닛들은 실제 요건들에 따라 선택되어 실시예들에서 해결책들의 목적들을 달성할 수 있다. 또한, 본 출원의 실시예들에서의 기능 유닛들은 하나의 처리 유닛으로 통합될 수 있거나, 또는 이러한 유닛들 각각은 물리적으로 분리될 수 있거나, 또는 2개 이상의 유닛이 하나의 유닛으로 통합될 수 있다. 통합된 유닛 은 하드웨어의 형태로 구현될 수 있거나, 소프트웨어 기능 유닛의 형태로 구현될 수 있다. 통합된 유닛이 소프트웨어 기능 유닛의 형태로 구현되고 독립적인 제품으로서 판매되거나 또는 사용될 때, 통합 된 유닛은 컴퓨터 판독가능 저장 매체에 저장될 수 있다. 이러한 이해에 기초하여, 본 출원의 기술적 해결책들 은 본질적으로, 또는 관련 기술에 기여하는 부분, 또는 기술적 해결책들의 전부 또는 일부는, 소프트웨어 제품 의 형태로 구현될 수 있다. 컴퓨터 소프트웨어 제품은 저장 매체에 저장되고 컴퓨터 디바이스(개인용 컴퓨터, 서버, 네트워크 디바이스 등일 수 있음)에게 본 출원의 실시예들에 설명되는 방법들의 단계들의 전부 또는 일부 를 수행하라고 명령하는 수개의 명령어를 포함한다. 전술한 저장 매체는: USB 플래시 드라이브, 이동식 하드 디스크, 판독 전용 메모리(ROM), 랜덤 액세스 메모리(RAM), 자기 디스크, 또는 광 디스크와 같이, 컴퓨터 프로 그램을 저장할 수 있는 임의의 매체를 포함한다. 전술한 실시예들은, 본 출원을 제한하기 위해서가 아니라, 단지 본 출원의 기술적 해결책들을 설명하기 위해 의"}
{"patent_id": "10-2021-7029221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "도된다. 본 출원이 전술한 실시예들을 참조하여 상세히 설명되더라도, 본 기술분야에서의 통상의 기술자들은, 본 출원의 실시예들의 기술적 해결책들의 사상 및 범위로부터 벗어나지 않고, 여전히 전술한 실시예들에서 설명 되는 기술적 해결책들에 수정들을 행할 수 있거나 또는 그 일부 기술적 특징들에 대해 동등한 대체들을 행할 수 있다는 점을 이해한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10a 도면10b 도면11 도면12 도면13"}
{"patent_id": "10-2021-7029221", "section": "도면", "subsection": "도면설명", "item": 1, "content": "이 출원의 실시예들에서 또는 관련된 기술에서 기술적 해결책들을 더 명확하게 설명하기 위하여, 다음은 실시예 들 또는 관련된 기술을 설명하기 위하여 요구된 첨부 도면들을 간략하게 설명한다. 명확하게, 다음 설명에서의"}
{"patent_id": "10-2021-7029221", "section": "도면", "subsection": "도면설명", "item": 2, "content": "첨부 도면들은 이 출원의 일부 실시예를 단지 도시하고, 본 기술분야의 통상의 기술자는 창조적인 노력들없이 이 첨부 도면들로부터 다른 도면들을 여전히 유도할 수 있다. 도 1은 본 출원의 실시예에 따른 인공 지능 기반(AI 기반) 애니메이션 캐릭터 구동 방법의 응용 시나리오의 개 략도이다. 도 2는 본 출원의 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 3은 본 출원의 실시예에 따른 애니메이션 캐릭터 구동 시스템의 구조적 흐름도이다. 도 4는 본 출원의 실시예에 따른 미디어 데이터를 취득하는 시나리오의 예시적인 도면이다. 도 5는 본 출원의 실시예에 따른 3DMM 라이브러리 내의 M의 각각의 차원의 분포 및 의미의 예시적인 도면이다. 도 6은 본 출원의 실시예에 따른 결정된 얼굴-대-파라미터 변환 파라미터에 기초한 애니메이션 캐릭터 구동 방 법의 응용 시나리오의 개략도이다. 도 7은 본 출원의 실시예에 따른 결정된 매핑 관계에 기초한 애니메이션 캐릭터 구동 방법의 응용 시나리오의 개략도이다. 도 8은 본 출원의 실시예에 따른 시간 간격과 음소 사이의 대응의 예시적인 도면이다. 도 9는 본 출원의 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 10a는 본 출원의 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 10b는 본 출원의 실시예에 따른 애니메이션 캐릭터 구동 장치의 구조도이다.도 11은 본 출원의 실시예에 따른 애니메이션 캐릭터 구동 장치의 구조도이다. 도 12는 본 출원의 실시예에 따른 애니메이션 캐릭터 구동 디바이스의 구조도이다. 도 13은 본 출원의 실시예에 따른 서버의 구조도이다."}
