{"patent_id": "10-2021-0117980", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0124104", "출원번호": "10-2021-0117980", "발명의 명칭": "음성 합성 방법 및 대응하는 모델의 훈련 방법, 장치, 전자 기기, 저장 매체 및 컴퓨터 프로", "출원인": "베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니", "발명자": "왕 웬푸"}}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 합성 방법으로서,합성될 음성의 스타일 정보와 음색 정보, 및 처리될 텍스트의 콘텐츠 정보를 획득하는 단계,사전 훈련된 음성 합성 모델을 사용하여, 상기 스타일 정보, 상기 음색 정보 및 상기 처리될 텍스트의 콘텐츠정보에 기반하여, 상기 처리될 텍스트의 음향학 특징 정보를 생성하는 단계, 및상기 처리될 텍스트의 음향학 특징 정보에 기반하여, 상기 처리될 텍스트의 음성을 합성하는 단계를 포함하는, 음성 합성 방법."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,사전 훈련된 음성 합성 모델을 사용하여, 상기 스타일 정보, 상기 음색 정보 및 상기 처리될 텍스트의 콘텐츠정보에 기반하여, 상기 처리될 텍스트의 음향학 특징 정보를 생성하는 단계는,상기 음성 합성 모델 중의 콘텐츠 인코더를 사용하여 상기 처리될 텍스트의 콘텐츠 정보에 대해 인코딩하여, 콘텐츠 인코딩 특징을 획득하는 것,상기 음성 합성 모델 중의 스타일 인코더를 사용하여 처리될 텍스트의 콘텐츠 정보 및 상기 스타일 정보에 대해인코딩하여, 스타일 인코딩 특징을 획득하는 것,상기 음성 합성 모델 중의 음색 인코더를 사용하여 상기 음색 정보에 대해 인코딩하여, 음색 인코딩 특징을 획득하는 것,상기 음성 합성 모델 중의 디코더를 사용하여, 상기 콘텐츠 인코딩 특징, 상기 스타일 인코딩 특징 및 상기 음색 인코딩 특징에 기반하여 디코딩하여, 상기 처리될 텍스트의 음향학 특징 정보를 생성하는 것을 포함하는, 음성 합성 방법."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 또는 제2항에 있어서,합성될 음성의 스타일 정보를 획득하는 것은,사용자의 입력 스타일에 대한 표현 정보를 획득하고, 상기 입력 스타일에 대한 표현 정보에 따라, 사전 설정된스타일 테이블로부터 상기 입력 스타일에 대응하는 스타일 식별자를 획득하여, 상기 합성될 음성의 스타일 정보로 하는 것을 포함하거나, 또는,입력 스타일에 의해 표현된 오디오 정보를 획득하며, 상기 오디오 정보에서 상기 입력 스타일의 음색 정보를 추출하여, 상기 합성될 음성의 스타일 정보로 하는 것을 포함하는, 음성 합성 방법."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "음성 합성 모델의 훈련 방법으로서,합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 상기 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보와, 상기 훈련 스타일 정보에 대응하는 훈련 스타일 및 상기 훈련 음색 정보에 대응하는 훈련 음색을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 각각 포함하는 복수의 훈련 데이터를 수집하는 단계 및공개특허 10-2021-0124104-3-상기 복수의 훈련 데이터를 사용하여, 상기 음성 합성 모델을 훈련하는 단계를 포함하는, 음성 합성 모델의 훈련 방법."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 복수의 훈련 데이터를 사용하여, 상기 음성 합성 모델을 훈련하는 단계는,각 상기 훈련 데이터에 대해, 상기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더 및 음색 인코더를 사용하여, 상기 훈련 데이터 중의 상기 훈련 텍스트의 콘텐츠 정보, 상기 훈련 스타일 정보 및 상기 훈련 음색 정보에 대해 각각 인코딩하여, 훈련 콘텐츠 인코딩 특징, 훈련 스타일 인코딩 특징 및 훈련 음색 인코딩 특징을 순차적으로 획득하는 것,상기 음성 합성 모델 중의 스타일 추출기를 사용하여, 상기 훈련 텍스트의 콘텐츠 정보 및 상기 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보에 기반하여, 목표 훈련 스타일 인코딩 특징을 추출하는 것,상기 음성 합성 모델 중의 디코더를 사용하여, 상기 훈련 콘텐츠 인코딩 특징, 상기 목표 훈련 스타일 인코딩특징 및 상기 훈련 음색 인코딩 특징에 기반하여, 디코딩하여 상기 훈련 텍스트의 예측 음향학 특징 정보를 생성하는 것,상기 훈련 스타일 인코딩 특징, 상기 목표 훈련 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표음향학 특징 정보에 기반하여, 종합 손실 함수를 구성하는 것, 및상기 종합 손실 함수가 수렴되지 않는 경우, 상기 종합 손실 함수가 수렴으로 향하도록, 상기 콘텐츠 인코더,상기 스타일 인코더, 상기 음색 인코더, 상기 스타일 추출기 및 상기 디코더의 파라미터를 조정하는 것을 포함하는, 음성 합성 모델의 훈련 방법."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 훈련 스타일 인코딩 특징, 상기 목표 훈련 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표음향학 특징 정보에 기반하여, 종합 손실 함수를 구성하는 것은,상기 훈련 스타일 인코딩 특징 및 상기 목표 훈련 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구성하는 것,상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재 구성 손실 함수를 구성하는 것, 및상기 스타일 손실 함수 및 상기 재 구성 손실 함수에 기반하여, 상기 종합 손실 함수를 생성하는 것을 포함하는, 음성 합성 모델의 훈련 방법."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "음성 합성 장치로서,합성될 음성의 스타일 정보와 음색 정보, 및 처리될 텍스트의 콘텐츠 정보를 획득하기 위한 획득 모듈,사전 훈련된 음성 합성 모델을 사용하여, 상기 스타일 정보, 상기 음색 정보 및 상기 처리될 텍스트의 콘텐츠정보에 기반하여, 상기 처리될 텍스트의 음향학 특징 정보를 생성하기 위한 생성 모듈, 및상기 처리될 텍스트의 음향학 특징 정보에 기반하여, 상기 처리될 텍스트의 음성을 합성하기 위한 합성 모듈을 포함하는, 음성 합성 장치."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,공개특허 10-2021-0124104-4-상기 생성 모듈은,상기 음성 합성 모델 중의 콘텐츠 인코더를 사용하여 상기 처리될 텍스트의 콘텐츠 정보에 대해 인코딩하여, 콘텐츠 인코딩 특징을 획득하기 위한 콘텐츠 인코딩부,상기 음성 합성 모델 중의 스타일 인코더를 사용하여 처리될 텍스트의 콘텐츠 정보 및 상기 스타일 정보에 대해인코딩하여, 스타일 인코딩 특징을 획득하기 위한 스타일 인코딩부,상기 음성 합성 모델 중의 음색 인코더를 사용하여 상기 음색 정보에 대해 인코딩하여, 음색 인코딩 특징을 획득하기 위한 음색 인코딩부,상기 음성 합성 모델 중의 디코더를 사용하여, 상기 콘텐츠 인코딩 특징, 상기 스타일 인코딩 특징 및 상기 음색 인코딩 특징에 기반하여, 디코딩하여 상기 처리될 텍스트의 음향학 특징 정보를 생성하기 위한 디코딩부를 포함하는, 음성 합성 장치."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항 또는 제8항에 있어서,상기 획득 모듈은,사용자의 입력 스타일에 대한 표현 정보를 획득하고, 상기 입력 스타일에 대한 표현 정보에 따라, 사전 설정된스타일 테이블로부터 상기 입력 스타일에 대응하는 스타일 식별자를 획득하여, 상기 합성될 음성의 스타일 정보로 하거나, 또는,입력 스타일에 의해 표현된 오디오 정보를 획득하며, 상기 오디오 정보에서 상기 입력 스타일의 음색 정보를 추출하여, 상기 합성될 음성의 스타일 정보로 하기 위한 것인, 음성 합성 장치."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "음성 합성 모델의 훈련 장치로서,합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 상기 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보와, 상기 훈련 스타일 정보에 대응하는 훈련 스타일 및 상기 훈련 음색 정보에 대응하는 훈련 음색을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 가각 포함하는 복수의 훈련 데이터를 수집하기 위한 수집모듈, 및상기 복수의 훈련 데이터를 사용하여, 상기 음성 합성 모델을 훈련하기 위한 훈련 모듈을 포함하는, 음성 합성 모델의 훈련 장치."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 훈련 모듈은,각 상기 훈련 데이터에 대해, 상기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더 및 음색 인코더를 사용하여, 상기 훈련 데이터 중의 상기 훈련 텍스트의 콘텐츠 정보, 상기 훈련 스타일 정보 및 상기 훈련 음색 정보에 대해 각각 인코딩하여, 훈련 콘텐츠 인코딩 특징, 훈련 스타일 인코딩 특징 및 훈련 음색 인코딩 특징을 순차적으로 획득하기 위한 인코딩부,상기 음성 합성 모델 중의 스타일 추출기를 사용하여, 상기 훈련 텍스트의 콘텐츠 정보 및 상기 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보에 기반하여, 목표 훈련 스타일 인코딩 특징을 추출하기 위한 추출부,상기 음성 합성 모델 중의 디코더를 사용하여, 상기 훈련 콘텐츠 인코딩 특징, 상기 목표 훈련 스타일 인코딩특징 및 상기 훈련 음색 인코딩 특징에 기반하여, 디코딩하여 상기 훈련 텍스트에 대한 예측 음향학 특징 정보를 생성하기 위한 디코딩부,상기 훈련 스타일 인코딩 특징, 상기 목표 훈련 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표공개특허 10-2021-0124104-5-음향학 특징 정보에 기반하여, 종합 손실 함수를 구성하기 위한 구성부,상기 종합 손실 함수가 수렴되지 않는 경우, 상기 종합 손실 함수가 수렴으로 향하도록, 상기 콘텐츠 인코더,상기 스타일 인코더, 상기 음색 인코더, 상기 스타일 추출기 및 상기 디코더의 파라미터를 조정하기 위한 조정부를 포함하는, 음성 합성 모델의 훈련 장치."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 구성부는,상기 훈련 스타일 인코딩 특징 및 상기 목표 훈련 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구성하고,상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재 구성 손실 함수를 구성하며,상기 스타일 손실 함수 및 상기 재 구성 손실 함수에 기반하여, 상기 종합 손실 함수를 생성하기 위한 것인, 음성 합성 모델의 훈련 장치."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "적어도 하나의 프로세서, 및상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하는 전자 기기로서,상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령어가 저장되어 있고, 상기 명령어가 상기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제2항, 또는제4항 내지 제6향 중 어느 한 항의 방법을 실행하게 하는, 전자 기기."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "컴퓨터 명령어가 저장되어 있는 비휘발성 컴퓨터 판독 가능 저장 매체로서,상기 컴퓨터 명령어가 프로세서로 하여금 제1항 내지 제2항 또는 제4항 내지 제6항 중 어느 한 항의 방법을 구현하게 하기 위한 것인, 비휘발성 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2021-0117980", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "프로세서에 의해 실행될 시에 제1항 내지 제2항 또는 제4항 내지 제6항 어느 한 항의 방법을 구현하는 컴퓨터프로그램을 포함하는 컴퓨터 프로그램 제품."}
{"patent_id": "10-2021-0117980", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 스마트 음성 및 딥 러닝 등 인공 지능"}
{"patent_id": "10-2021-0117980", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것으로, 음성 합성 방법 및 대응하는 모델의 훈련 방법, 장치, 기기 및 매체를 개시한다. 구체적인 구현 방안은, 합성될 음성의 스타일 정보, 음색 정보 및 처리될 텍스트의 콘텐츠 정보를 획득하는 것, 사전 훈련된 음성 합성 모델을 사용하여, 상기 스타일 정보, 상기 음색 정보 및 상기 처리될 텍스트의 콘텐츠 정보에 기반하여, 상기 처리될 텍스트의 음향학 특징 정보를 생성하 는 것, 상기 처리될 텍스트의 음향학 특징 정보에 기반하여, 상기 처리될 텍스트의 음성을 합성하는 것을 포함한 다. 본 발명에 따르면 크로스-언어, 크로스-스타일, 크로스-음색의 음성 합성을 구현할 수 있어, 음성 합성의 다 양성을 최대적으로 풍부화시킬 수 있고, 사용자의 사용 체험도를 향상할 수 있다. 대 표 도 - 도1"}
{"patent_id": "10-2021-0117980", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2021-0124104 CPC특허분류 G10L 25/30 (2013.01) 발명자 왕 시레이 중국 베이징 100085 하이디엔 디스트릭트 샹디 10 번가 넘버 10 바이두 캠퍼스 2층 장 쥔텅 중국 베이징 100085 하이디엔 디스트릭트 샹디 10 번가 넘버 10 바이두 캠퍼스 2층가오 정쿤 중국 베이징 100085 하이디엔 디스트릭트 샹디 10 번가 넘버 10 바이두 캠퍼스 2층 지아 레이 중국 베이징 100085 하이디엔 디스트릭트 샹디 10 번가 넘버 10 바이두 캠퍼스 2층명 세 서 청구범위 청구항 1 음성 합성 방법으로서, 합성될 음성의 스타일 정보와 음색 정보, 및 처리될 텍스트의 콘텐츠 정보를 획득하는 단계, 사전 훈련된 음성 합성 모델을 사용하여, 상기 스타일 정보, 상기 음색 정보 및 상기 처리될 텍스트의 콘텐츠 정보에 기반하여, 상기 처리될 텍스트의 음향학 특징 정보를 생성하는 단계, 및 상기 처리될 텍스트의 음향학 특징 정보에 기반하여, 상기 처리될 텍스트의 음성을 합성하는 단계 를 포함하는, 음성 합성 방법. 청구항 2 제1항에 있어서, 사전 훈련된 음성 합성 모델을 사용하여, 상기 스타일 정보, 상기 음색 정보 및 상기 처리될 텍스트의 콘텐츠 정보에 기반하여, 상기 처리될 텍스트의 음향학 특징 정보를 생성하는 단계는, 상기 음성 합성 모델 중의 콘텐츠 인코더를 사용하여 상기 처리될 텍스트의 콘텐츠 정보에 대해 인코딩하여, 콘 텐츠 인코딩 특징을 획득하는 것, 상기 음성 합성 모델 중의 스타일 인코더를 사용하여 처리될 텍스트의 콘텐츠 정보 및 상기 스타일 정보에 대해 인코딩하여, 스타일 인코딩 특징을 획득하는 것, 상기 음성 합성 모델 중의 음색 인코더를 사용하여 상기 음색 정보에 대해 인코딩하여, 음색 인코딩 특징을 획 득하는 것, 상기 음성 합성 모델 중의 디코더를 사용하여, 상기 콘텐츠 인코딩 특징, 상기 스타일 인코딩 특징 및 상기 음 색 인코딩 특징에 기반하여 디코딩하여, 상기 처리될 텍스트의 음향학 특징 정보를 생성하는 것 을 포함하는, 음성 합성 방법. 청구항 3 제1항 또는 제2항에 있어서, 합성될 음성의 스타일 정보를 획득하는 것은, 사용자의 입력 스타일에 대한 표현 정보를 획득하고, 상기 입력 스타일에 대한 표현 정보에 따라, 사전 설정된 스타일 테이블로부터 상기 입력 스타일에 대응하는 스타일 식별자를 획득하여, 상기 합성될 음성의 스타일 정보 로 하는 것을 포함하거나, 또는, 입력 스타일에 의해 표현된 오디오 정보를 획득하며, 상기 오디오 정보에서 상기 입력 스타일의 음색 정보를 추 출하여, 상기 합성될 음성의 스타일 정보로 하는 것 을 포함하는, 음성 합성 방법. 청구항 4 음성 합성 모델의 훈련 방법으로서, 합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 상기 훈련 스타일 정보에 대 응하는 훈련 스타일을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보와, 상기 훈련 스 타일 정보에 대응하는 훈련 스타일 및 상기 훈련 음색 정보에 대응하는 훈련 음색을 사용하여 상기 훈련 텍스트 의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 각각 포함하는 복수의 훈련 데이터를 수집하는 단계 및상기 복수의 훈련 데이터를 사용하여, 상기 음성 합성 모델을 훈련하는 단계 를 포함하는, 음성 합성 모델의 훈련 방법. 청구항 5 제4항에 있어서, 상기 복수의 훈련 데이터를 사용하여, 상기 음성 합성 모델을 훈련하는 단계는, 각 상기 훈련 데이터에 대해, 상기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더 및 음색 인코더를 사용 하여, 상기 훈련 데이터 중의 상기 훈련 텍스트의 콘텐츠 정보, 상기 훈련 스타일 정보 및 상기 훈련 음색 정보 에 대해 각각 인코딩하여, 훈련 콘텐츠 인코딩 특징, 훈련 스타일 인코딩 특징 및 훈련 음색 인코딩 특징을 순 차적으로 획득하는 것, 상기 음성 합성 모델 중의 스타일 추출기를 사용하여, 상기 훈련 텍스트의 콘텐츠 정보 및 상기 훈련 스타일 정 보에 대응하는 훈련 스타일을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보에 기반하 여, 목표 훈련 스타일 인코딩 특징을 추출하는 것, 상기 음성 합성 모델 중의 디코더를 사용하여, 상기 훈련 콘텐츠 인코딩 특징, 상기 목표 훈련 스타일 인코딩 특징 및 상기 훈련 음색 인코딩 특징에 기반하여, 디코딩하여 상기 훈련 텍스트의 예측 음향학 특징 정보를 생 성하는 것, 상기 훈련 스타일 인코딩 특징, 상기 목표 훈련 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 종합 손실 함수를 구성하는 것, 및 상기 종합 손실 함수가 수렴되지 않는 경우, 상기 종합 손실 함수가 수렴으로 향하도록, 상기 콘텐츠 인코더, 상기 스타일 인코더, 상기 음색 인코더, 상기 스타일 추출기 및 상기 디코더의 파라미터를 조정하는 것 을 포함하는, 음성 합성 모델의 훈련 방법. 청구항 6 제5항에 있어서, 상기 훈련 스타일 인코딩 특징, 상기 목표 훈련 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 종합 손실 함수를 구성하는 것은, 상기 훈련 스타일 인코딩 특징 및 상기 목표 훈련 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구성하 는 것, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재 구성 손실 함수를 구성하는 것, 및 상기 스타일 손실 함수 및 상기 재 구성 손실 함수에 기반하여, 상기 종합 손실 함수를 생성하는 것 을 포함하는, 음성 합성 모델의 훈련 방법. 청구항 7 음성 합성 장치로서, 합성될 음성의 스타일 정보와 음색 정보, 및 처리될 텍스트의 콘텐츠 정보를 획득하기 위한 획득 모듈, 사전 훈련된 음성 합성 모델을 사용하여, 상기 스타일 정보, 상기 음색 정보 및 상기 처리될 텍스트의 콘텐츠 정보에 기반하여, 상기 처리될 텍스트의 음향학 특징 정보를 생성하기 위한 생성 모듈, 및 상기 처리될 텍스트의 음향학 특징 정보에 기반하여, 상기 처리될 텍스트의 음성을 합성하기 위한 합성 모듈 을 포함하는, 음성 합성 장치. 청구항 8 제7항에 있어서,상기 생성 모듈은, 상기 음성 합성 모델 중의 콘텐츠 인코더를 사용하여 상기 처리될 텍스트의 콘텐츠 정보에 대해 인코딩하여, 콘 텐츠 인코딩 특징을 획득하기 위한 콘텐츠 인코딩부, 상기 음성 합성 모델 중의 스타일 인코더를 사용하여 처리될 텍스트의 콘텐츠 정보 및 상기 스타일 정보에 대해 인코딩하여, 스타일 인코딩 특징을 획득하기 위한 스타일 인코딩부, 상기 음성 합성 모델 중의 음색 인코더를 사용하여 상기 음색 정보에 대해 인코딩하여, 음색 인코딩 특징을 획 득하기 위한 음색 인코딩부, 상기 음성 합성 모델 중의 디코더를 사용하여, 상기 콘텐츠 인코딩 특징, 상기 스타일 인코딩 특징 및 상기 음 색 인코딩 특징에 기반하여, 디코딩하여 상기 처리될 텍스트의 음향학 특징 정보를 생성하기 위한 디코딩부 를 포함하는, 음성 합성 장치. 청구항 9 제7항 또는 제8항에 있어서, 상기 획득 모듈은, 사용자의 입력 스타일에 대한 표현 정보를 획득하고, 상기 입력 스타일에 대한 표현 정보에 따라, 사전 설정된 스타일 테이블로부터 상기 입력 스타일에 대응하는 스타일 식별자를 획득하여, 상기 합성될 음성의 스타일 정보 로 하거나, 또는, 입력 스타일에 의해 표현된 오디오 정보를 획득하며, 상기 오디오 정보에서 상기 입력 스타일의 음색 정보를 추 출하여, 상기 합성될 음성의 스타일 정보로 하기 위한 것인, 음성 합성 장치. 청구항 10 음성 합성 모델의 훈련 장치로서, 합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 상기 훈련 스타일 정보에 대 응하는 훈련 스타일을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보와, 상기 훈련 스 타일 정보에 대응하는 훈련 스타일 및 상기 훈련 음색 정보에 대응하는 훈련 음색을 사용하여 상기 훈련 텍스트 의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 가각 포함하는 복수의 훈련 데이터를 수집하기 위한 수집 모듈, 및 상기 복수의 훈련 데이터를 사용하여, 상기 음성 합성 모델을 훈련하기 위한 훈련 모듈 을 포함하는, 음성 합성 모델의 훈련 장치. 청구항 11 제10항에 있어서, 상기 훈련 모듈은, 각 상기 훈련 데이터에 대해, 상기 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더 및 음색 인코더를 사용 하여, 상기 훈련 데이터 중의 상기 훈련 텍스트의 콘텐츠 정보, 상기 훈련 스타일 정보 및 상기 훈련 음색 정보 에 대해 각각 인코딩하여, 훈련 콘텐츠 인코딩 특징, 훈련 스타일 인코딩 특징 및 훈련 음색 인코딩 특징을 순 차적으로 획득하기 위한 인코딩부, 상기 음성 합성 모델 중의 스타일 추출기를 사용하여, 상기 훈련 텍스트의 콘텐츠 정보 및 상기 훈련 스타일 정 보에 대응하는 훈련 스타일을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보에 기반하 여, 목표 훈련 스타일 인코딩 특징을 추출하기 위한 추출부, 상기 음성 합성 모델 중의 디코더를 사용하여, 상기 훈련 콘텐츠 인코딩 특징, 상기 목표 훈련 스타일 인코딩 특징 및 상기 훈련 음색 인코딩 특징에 기반하여, 디코딩하여 상기 훈련 텍스트에 대한 예측 음향학 특징 정보 를 생성하기 위한 디코딩부, 상기 훈련 스타일 인코딩 특징, 상기 목표 훈련 스타일 인코딩 특징, 상기 예측 음향학 특징 정보 및 상기 목표음향학 특징 정보에 기반하여, 종합 손실 함수를 구성하기 위한 구성부, 상기 종합 손실 함수가 수렴되지 않는 경우, 상기 종합 손실 함수가 수렴으로 향하도록, 상기 콘텐츠 인코더, 상기 스타일 인코더, 상기 음색 인코더, 상기 스타일 추출기 및 상기 디코더의 파라미터를 조정하기 위한 조정 부 를 포함하는, 음성 합성 모델의 훈련 장치. 청구항 12 제11항에 있어서, 상기 구성부는, 상기 훈련 스타일 인코딩 특징 및 상기 목표 훈련 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구성하 고, 상기 예측 음향학 특징 정보 및 상기 목표 음향학 특징 정보에 기반하여, 재 구성 손실 함수를 구성하며, 상기 스타일 손실 함수 및 상기 재 구성 손실 함수에 기반하여, 상기 종합 손실 함수를 생성하기 위한 것인, 음 성 합성 모델의 훈련 장치. 청구항 13 적어도 하나의 프로세서, 및 상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하는 전자 기기로서, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령어가 저장되어 있고, 상기 명령어가 상 기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제2항, 또는 제4항 내지 제6향 중 어느 한 항의 방법을 실행하게 하는, 전자 기기. 청구항 14 컴퓨터 명령어가 저장되어 있는 비휘발성 컴퓨터 판독 가능 저장 매체로서, 상기 컴퓨터 명령어가 프로세서로 하여금 제1항 내지 제2항 또는 제4항 내지 제6항 중 어느 한 항의 방법을 구 현하게 하기 위한 것인, 비휘발성 컴퓨터 판독 가능 저장 매체. 청구항 15 프로세서에 의해 실행될 시에 제1항 내지 제2항 또는 제4항 내지 제6항 어느 한 항의 방법을 구현하는 컴퓨터 프로그램을 포함하는 컴퓨터 프로그램 제품. 발명의 설명 기 술 분 야 본 발명은 컴퓨터 기술 분야에 관한 것으로서, 특히, 스마트 음성 및 딥 러닝 등 인공 지능 기술 분야에 관한 것이며, 특히 음성 합성 방법 및 대응하는 모델의 훈련 방법, 장치, 기기 및 매체에 관한 것이다."}
{"patent_id": "10-2021-0117980", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "문자 언어 전환(Text-to-Speech: TTS)라고도 하는 음성 합성은, 컴퓨터를 통하여 텍스트 정보를 음질이 좋고, 자연 유창도가 높은 음성 정보로 전환시키는 과정을 의미하며, 스마트 음성 인터랙션 기술의 핵심 기술 중 하나 이다. 최근 몇 년 간, 딥 러닝 기술의 발전 및 음성 합성 분야에서의 광범위한 응용과 함께, 음성 합성의 음질 및 자 연 유창도는 전례없는 발전을 거듭하였다. 현재 주류적 음성 합성 모델은 주로 단일 발음인(즉, 단일 음색), 단 일 스타일의 음성 합성을 구현하는데 사용되고 있다. 멀티 스타일, 멀티 음색 합성을 구현하려면, 각 발음인이"}
{"patent_id": "10-2021-0117980", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "녹음한 복수의 스타일의 훈련 데이터를 수집하여, 음성 합성 모델을 훈련할 수 있다.발명의 내용 본 발명은 음성 합성 방법 및 대응하는 모델의 훈련 방법, 장치, 기기 및 매체를 개시한다. 본 발명의 일 측면에 의하면, 음성 합성 방법을 제공하며, 상기 방법은, 합성될 음성의 스타일 정보와 음색 정보, 및 처리될 텍스트의 콘텐츠 정보를 획득하는 것, 사전 훈련된 음성 합성 모델을 사용하여, 상기 스타일 정보, 상기 음색 정보 및 상기 처리될 텍스트의 콘텐츠 정보에 기반하여, 처리될 텍스트의 음향학 특징 정보를 생성하는 것, 및 상기 처리될 텍스트의 음향학 특징 정보에 기반하여, 상기 처리될 텍스트의 음성을 합성하는 것을 포함한다. 본 발명의 다른 측면에 의하면, 음성 합성 모델의 훈련 방법을 제공하며, 상기 방법은, 합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 상기 훈련 스타일 정보에 대 응하는 훈련 스타일을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보와, 상기 훈련 스 타일 정보에 대응하는 훈련 스타일 및 상기 훈련 음색 정보에 대응하는 훈련 음색을 사용하여 상기 훈련 텍스트 의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 포함하는 복수의 훈련 데이터를 수집하는 것, 및 상기 복수의 훈련 데이터를 사용하여, 상기 음성 합성 모델을 훈련하는 것을 포함한다. 본 발명의 또 다른 측면에 의하면, 음성 합성 장치를 제공하며, 상기 장치는, 합성될 음성의 스타일 정보와 음색 정보, 및 처리될 텍스트의 콘텐츠 정보를 획득하기 위한 획득 모듈, 사전 훈련된 음성 합성 모델을 사용하여, 상기 스타일 정보, 상기 음색 정보 및 상기 처리될 텍스트의 콘텐츠 정보에 기반하여, 상기 처리될 텍스트의 음향학 특징 정보를 생성하기 위한 생성 모듈, 및 상기 처리될 텍스트의 음향학 특징 정보에 기반하여, 상기 처리될 텍스트의 음성을 합성하기 위한 합성 모듈을 포함한다. 본 발명의 또 다른 측면에 의하면, 음성 합성 모델의 훈련 장치에 있어서, 합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 상기 훈련 스타일 정보에 대 응하는 훈련 스타일을 사용하여 상기 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보, 및 상기 훈련 스타일 정보에 대응하는 훈련 스타일 및 상기 훈련 음색 정보에 대응하는 훈련 음색을 사용하여 상기 훈련 텍스 트의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 포함하는 복수의 훈련 데이터를 수집하기 위한 수집 모 듈, 및 상기 복수의 훈련 데이터를 사용하여, 상기 음성 합성 모델을 훈련하기 위한 훈련 모듈을 포함한다. 본 발명의 또 다른 측면에 의하면, 전자 기기를 제공하며, 상기 전자기기는, 적어도 하나의 프로세서, 및 상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하며, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령어가 저장되어 있고, 상기 명령어가 상 기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서로 하여금 상기 방법을 실행하게 한 다. 본 발명의 또 다른 측면에 의하면, 컴퓨터 프로그램이 저장되어 있는 비휘발성 컴퓨터 판독 가능 저장 매체를 제공하며, 상기 컴퓨터 프로그램은 프로세서로 하여금 상기 방법을 구현하게 하기 위한 것이다. 본 발명에 따른 기술에 의하면, 합성될 음성의 스타일 정보와 음색 정보, 및 처리될 텍스트의 콘텐츠 정보를 획 득하고, 스타일 정보, 음색 정보 및 처리될 텍스트의 콘텐츠 정보에 기반하여, 사전 훈련된 음성 합성 모델을 사용하여, 처리될 텍스트의 음향학 특징 정보를 생성하며, 처리될 텍스트의 음향학 특징 정보에 기반하여, 처리 될 텍스트의 음성을 합성함으로써, 크로스-언어, 크로스-스타일, 크로스-음색의 음성 합성을 구현할 수 있어, 음성 합성의 다양성을 최대적으로 풍부화시킬 수 있고, 사용자의 사용 체험도를 향상할 수 있다 본 명세서에 기술된 내용은 그 목적이 본 발명의 실시예의 핵심 또는 중요한 특징을 지정하기 위한 것이 아니고, 또한, 본 발명의 범위는 이에 한정되지 않음을 이해하여야 한다. 본 발명의 다른 특징들은 하기 설명으로부터 용이하게 이해할 수 있을 것이다."}
{"patent_id": "10-2021-0117980", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 결합하여 본 발명의 예시적인 실시예들을 설명한다. 쉽게 이해할 수 있도록, 본 명세서에서 설명 한 각 실시예의 세부사항을 포함하게 되는데, 이들은 단지 예시적인 것에 불과하다. 따라서, 당업자라면 본 발 명의 범위 및 취지를 어기지 않으면서 본 발명의 실시예에 대해 여러 가지 변경 및 수정이 이루어질 수 있음을 이해할 것이다. 또한, 명확성과 간결성을 위해 하기의 설명에 있어서, 공지된 기능 및 구성에 대한 설명은 생략 한다. 도 1은 본 발명에 따른 제1 실시예에 대한 개략도이다. 도 1에 도시된 바와 같이, 본 실시예는 음성 합성 방법 을 제공하며, 음성 합성 방법은 구체적으로 하기 단계를 포함할 수 있다. 단계S101에서, 합성될 음성의 스타일 정보와 음색 정보, 및 처리될 텍스트의 콘텐츠 정보를 획득한다. 단계S102에서, 사전 훈련된 음성 합성 모델을 사용하여, 스타일 정보, 음색 정보 및 처리될 텍스트의 콘텐츠 정 보에 기반하여, 처리될 텍스트의 음향학 특징 정보를 생성한다. 단계S103에서, 처리될 텍스트의 음향학 특징 정보에 기반하여, 처리될 텍스트의 음성을 합성한다. 본 발명의 실시예에 따른 음성 합성 방법의 실행 주체는 음성 합성 장치이다. 상기 음성 합성 장치는 전자 엔티 티일 수도 있고, 또는 소프트웨어로 집적된 애플리케이션일 수도 있고, 사용할 때, 합성될 음성의 스타일 정보 와 음색 정보, 및 처리될 텍스트의 콘텐츠 정보에 기반하여, 처리될 텍스트의 음성을 합성할 수 있다. 본 실시예에서, 합성될 음성의 스타일 정보 및 합성될 음성의 음색 정보는 반드시 음성 합성 모델을 훈련하는데 사용된 훈련 데이터 세트 중의 스타일 정보 및 음색 정보이어야 하고, 그렇지 않으면 구현될 수 없다. 본 실시예에서, 합성될 음성의 스타일 정보는 훈련 데이터 세트 중 훈련된 스타일 ID와 같은 합성될 음성의 스 타일 식별자일 수 있다. 또는, 스타일 정보는 상기 스타일에 의해 표현되는 음성으로부터 추출된 스타일의 다른 정보일 수도 있다. 그러나, 실제 응용에서 사용되는 경우, 스타일에 의해 표현되는 음성은 멜(mel) 스펙트럼 시 퀀스의 형대로 표현될 수 있다. 본 실시예에 따른 음색 정보는, 상기 음색에 의해 표현되는 음성에 기반하여 추 출될 수 있고, 상기 음색 정보는 멜(mel) 스펙트럼 시퀀스의 형태로 표현될 수 있다. 본 실시예에 따른 스타일 정보는 유머, 기쁨, 슬픔, 전통 등과 같은 음성에 의해 표현된 스타일을 한정하는데 사용된다. 본 실시예에 따른 음성 정보는 음성을 표현하는 소리의 음색을 한정하는데 사용된다. 예를 들어, 스 타A, 방송인B, 카툰 동물C 등일 수 있다. 본 실시예에 따른 처리될 텍스트의 콘텐츠 정보는 문자 형태이다. 선택적으로, 단계S101을 실행하기 전에, 또한, 처리될 텍스트에 대해 전처리하여, 음자 시퀸스와 같은 처리될 텍스트의 콘텐츠 정보를 획득할 수도 있다. 예를 들어, 처리될 텍스트가 중국어인 경우, 상기 처리될 텍스트의 콘텐츠 정보는 처리될 텍스트의 음조 를 가진 음자 시퀸스일 수 있고, 중국어 문자의 발음에는 음조가 포함되기 때문에, 중국어에 대해, 전처리 후 음조를 가진 음자 시퀸스를 획득하는 것이 필요하다. 다른 언어에 대해, 전처리하여 대응되는 처리될 텍스트의 음자 시퀸스를 획득하면 된다. 예를 들어, 처리될 텍스트가 중국어인 경우, 음자는 자음 또는 모음과 같은 중국 어 병음 중의 음절일 수 있다. 본 실시예에 따르면, 스타일 정보, 음색 정보 및 처리될 텍스트의 콘텐츠 정보를 함께 음성 합성 모델에 입력하 고, 상기 음성 합성 모델은 스타일 정보, 음색 정보 및 처리될 텍스트의 콘텐츠 정보에 기반하여, 처리될 텍스 트의 음향학 특징 정보를 생성할 수 있다. 본 실시예에 따른 음성 합성 모델은 Tacotron구조를 사용하여 구현할 수 있다. 마지막으로, 신경 보코더(WaveRNN) 모델을 사용하여 처리될 텍스트의 음향학 특징 정보에 기반하여, 처리될 텍스트의 음성을 합성할 수 있다. 종래 기술에서는 단일 음색이거나 단일 스타일의 음성만을 구현할 수 있다. 그러나 본 실시예에 따른 기술 방안 은, 스타일 정보, 음색 정보 및 처리될 텍스트의 콘텐츠 정보에 기반하여 음성을 합성하는 경우, 수요에 따라 스타일 및 음색을 입력할 수 있고, 처리될 텍스트는 또한 임의의 언어일 수 있으므로, 본 실시예에 따른 기술 방안은 단일 음색 또는 단일 스타일의 음성 합성에 한정되지 않고, 크로스-언어, 크로스-스타일, 크로스-음색의 음성 합성을 구현할 수 있다. 본 실시예에 따른 음성 합성 방법은, 합성될 음성의 스타일 정보, 음색 정보 및 처리될 텍스트의 콘텐츠 정보를 획득하고, 사전 훈련된 음성 합성 모델을 사용하여, 스타일 정보, 음색 정보 및 처리될 텍스트의 콘텐츠 정보에 기반하여, 처리될 텍스트의 음향학 특징 정보를 생성하며, 처리될 텍스트의 음향학 특징 정보에 기반하여, 처리 될 텍스트의 음성을 합성함으로써, 크로스-언어, 크로스-스타일, 크로스-음색의 음성 합성을 구현할 수 있어, 음성 합성의 다양성을 최대적으로 풍부화시킬 수 있고, 사용자의 사용 체험도를 향상할 수 있다 도 2는 본 발명에 따른 제2 실시예에 대한 개략도이다. 도 2에 도시된 바와 같이, 본 실시예의 음성 합성 방법 에 따르면, 상기 도 1에 도시된 실시예의 기술 방안의 기초에, 본 발명의 기술 방안은 보다 더 구체적으로 설명 한다. 도 2에 도시된 바와 같이, 본 실시예에 따른 음성 합성 방법은 구체적으로 하기 단계들을 포함할 수 있다. 단계S201에서, 합성될 음성의 스타일 정보와 음색 정보, 및처리될 텍스트의 콘텐츠 정보를 획득한다. 상기 도 1에 도시된 실시예의 관련 설명을 참조하면, 상기 합성될 음성의 음색 정보는 해당 음색에 의해 표현된 처리될 텍스트의 멜(mel) 스펙트럼 시퀸스일 수 있다. 처리될 텍스트의 콘텐츠 정보는 처리될 텍스트에 대해 전 처리하여 획득한 처리될 텍스트의 음자 시퀸스일 수 있다. 예를 들어, 본 실시예에서 스타일 정보를 획득하는 과정은 하기와 같은 임의의 방식을 포함할 수 있다. 사용자의 입력 스타일에 대한 표현 정보를 획득하고, 입력 스타일에 대한 표현 정보에 따라, 미리 설정된 스 타일 테이블로부터 입력 스타일에 대응하는 스타일 식별자를 획득하여, 합성될 음성의 스타일 정보로 한다. 예를 들어, 입력 스타일에 대한 표현 정보는 유머, 코미디, 슬픔, 전통 등일 수 있다. 본 실시예에 따르면, 다 양한 스타일 표현 정보에 대응하는 스타일 식별자를 기록하는 스타일 테이블을 미리 설정할 수 있다. 또한, 이 러한 스타일 식별자는 사전에 훈련 데이터 세트로 음성 합성 모델을 훈련할 때 훈련된 것이며, 이때 상기 스타 일 식별자를 합성될 음성의 스타일 정보로 사용할 수 있다. 입력 스타일에 의해 표현된 오디오 정보를 획득하고, 오디오 정보에서 입력 스타일의 정보를 추출하여 합성 될 음성의 스타일 정보로 한다. 상기 구현 방식 중, 스타일 정보는 입력 스타일에 의해 표현된 오디오 정보로부터 추출할 수 있고, 여기서 상기 오디오 정보는 멜(mel) 스펙트럼 시퀀스의 형태일 수 있다. 나아가 선택적으로, 상기 구현 방식 중, 또한 사전 에 하나의 스타일 추출 모델을 훈련하여, 사용할 때 일부 스타일에 기반하여 표현된 오디오 정보로부터 추출한 멜(mel) 스펙트럼 시퀀스를 입력하고, 오디오 정보에 대응하는 스타일을 출력할 수 있다. 훈련 과정에서, 상기 스타일 추출 모델은 수많은 훈련 데이터를 사용할 수 있으며, 각 훈련 데이터는 훈련 스타일 및 훈련 스타일을 가진 훈련 멜(mel) 스펙트럼 시퀀스를 포함한다. 수많은 훈련 데이터를 사용하여, 지도 훈련 방식으로 상기 스 타일 추출 모델을 훈련한다.또한, 설명해야 할 것은, 본 실시예에 따른 음색 정보는 음색 정보에 대응하는 음색에 의해 표현된 오디오 정보 로부터 추출하여 획득할 수도 있다. 상기 음색 정보는 멜(mel) 스펙트럼 시퀀스 형태를 사용할 수 있고, 음색 멜(mel) 스펙트럼 시퀀스라고도 한다. 예를 들어, 음성 합성 시, 음성 합성의 편리성을 위하여, 훈련 데이터 세 트로부터 직접 하나의 음색 멜(mel) 시퀀스를 획득할 수 있다. 설명해야 할 것은, 상기 구현 방식 중, 입력 스타일에 의해 표현된 오디오 정보는 입력 스타일만 가지면 되고, 음성 정보는 처리될 텍스트의 콘텐츠 정보와 관련되거나, 처리될 텍스트의 콘텐츠 정보와 무관할 수도 있다. 마 찬가지로, 음색 정보에 대응하는 음색에 의해 표현된 오디오 정보는 처리될 텍스트의 콘텐츠 정보일 수도 있고, 처리될 텍스트의 콘텐츠 정보와 무관할 수도 있다. 단계S202에서, 음성 합성 모델 중의 콘텐츠 인코더를 사용하여, 처리될 텍스트의 콘텐츠 정보에 대해 인코딩하 여, 콘텐츠 인코딩 특징을 획득한다. 예를 들어, 상기 콘텐츠 인코더는 처리될 텍스트의 콘텐츠 정보에 대해 인코딩하여, 대응하는 콘텐츠 인코딩 특 징을 생성한다. 처리될 텍스트의 콘텐츠 정보는 음자 시퀸스의 형태를 사용하므로, 대응하게 획득된 콘텐츠 인 코딩 특징도 대응하게 시퀸스의 형태를 사용할 수 있으며, 콘텐츠 인코딩 시퀸스라고 할 수 있다. 여기서, 각 음자는 각각 하나의 인코딩 벡터에 대응한다. 상기 콘텐츠 인코더는 각각의 음자가 어떻게 발음되는지를 결정한 다. 단계S203에서, 음성 합성 모델 중의 스타일 인코더를 사용하여, 처리될 텍스트의 콘텐츠 정보 및 스타일 정보에 대해 인코딩하여, 스타일 인코딩 특징을 획득한다. 상기 스타일 인코더는 처리될 텍스트의 콘텐츠 정보에 대해 인코딩하고, 동시에 스타일 정보를 사용하여 인코딩 되는 스타일을 제어하여, 대응하는 스타일 인코딩 행렬을 생성하며, 마찬가지로, 스타일 인코딩 시퀸스라고도 할 수 있다. 각 음자는 각각 하나의 인코딩 벡터에 대응한다. 상기 스타일 인코더는 각각의 음자가 어떻게 발음 되는지, 즉 스타일을 결정한다. 단계S204에서, 음성 합성 모델 중의 음색 인코더를 사용하여, 음색 정보에 대해 인코딩하여, 음색 인코딩 특징 을 획득한다. 상기 음색 인코더는 음색 정보에 대해 인코딩하고, 음색 정보는 멜(mel) 스펙트럼 시퀀스를 사용할 수도 있다. 즉, 음색 인코더는 멜(mel) 스펙트럼 시퀀스에 대해 인코딩하여, 대응하는 음색 벡터를 생성할 수 있다. 상기 음색 인코더는 음색A, 음색B 또는 음색C 등과 같은 합성될 음성의 음색을 결정한다. 단계S205에서, 음성 합성 모델 중의 디코더를 사용하여, 콘텐츠 인코딩 특징, 스타일 인코딩 특징 및 음색 인코 딩 특징에 기반하여, 디코딩하여 처리될 텍스트의 음향학 특징 정보를 생성한다. 상기 디코더는 콘텐츠 인코더, 스타일 인코더 및 음색 인코더 각각에 의해 출력되어 모아 이어진 특징을 입력으 로 하여, 대응하는 콘텐츠 정보, 스타일 정보 및 음색 정보의 조합에 따라, 대응하는 처리될 텍스트의 음향학 특징 정보를 생성하고, 처리될 텍스트의 음성 특징 시퀸스라고도 할 수 있으며, 또한, 멜(mel) 스펙트럼 시퀀스 의 형태를 사용할 수 있다. 상기 단계S202 내지 단계S205는 상기 도 1에 도시된 실시예 중의 단계S102의 하나의 구현 방식이다. 도 3은 본 실시예에 따른 음성 합성 모델의 응용 아키텍처 개략도이다. 도 3에 도시된 바와 같이, 본 실시예에 따른 음성 합성 모델은 콘텐츠 인코더, 스타일 인코더, 음색 인코더 및 디코더 등 몇 부분으로 구성된다. 여기서, 콘텐츠 인코더는 잔여 연결이 있는 복수 층의 합성곱 신경망Convolutional Neural Networks: CNN） 및 한 층의 양방향 장단기 메모리(Long Short-Term Memory: LSTM)로 구성된다. 음색 인코더는 복수 층의 CNN 및 한 층의 게이트 순환 유닛(Gated Recurrent Unit: GRU)으로 구성된다. 디코더는 어텐션 메커니즘(Attention Mechanism)에 기반한 자동 회귀 구조이다. 스타일 인코더는 복수 층의 CNN 및 복수 층의 양방향 GRU로 구성된다. 예를 들어, 도 4는 본 실시예에 따른 음성 합성 모델 중의 스타일 인코더의 개략도이다. 도 4에 도시 된 바와 같이, N층의 CNN 및 N층의 GRU를 포함한 스타일 인코더를 예로 들어 설명한다. 스타일 인코더가 인코딩 할 경우, 처리될 텍스트의 콘텐츠 정보에 대해, 예를 들어 처리될 텍스트가 중국어인 경우, 상기 콘텐츠 정보는 음조를 가진 음자 시퀸스일 수 있고, 직접 CNN에 입력될 수 있으며, 스타일 ID와 같은 스타일 정보는 직접 GRU 에 입력되며, 스타일 인코더의 인코딩을 거쳐 최종적으로 스타일 인코딩 특징을 출력할 수 있다. 입력된 음조를 가진 음자 시퀸스에 대응함으로써, 스타일 인코딩 시퀸스라고도 할 수 있다.도 3에 도시된 바와 같이, 종래의 음성 합성 모델 Tacotron에 비해, 본 발명의 실시예에 따른 음성 합성 모델 중, 콘텐츠 인코더 및 스타일 인코더, 음색 인코더는 3개의 독립적인 유닛이고, 3개의 유닛은 디커플링된 상태 에서 각자 서로 다른 역할을 실행하며, 각각 대응하는 기능을 담당하며, 크로스-스타일, 크로스-음색, 크로스- 언어 합성의 핵심 부분이다. 따라서, 본 실시예는 더 이상 단일 음색이거나 단일 스타일의 음성을 합성하는 것 에 한정되지 않고, 크로스-언어, 크로스-스타일, 크로스-음색의 음성 합성을 구현할 수 있다. 예를 들어, 가수A 가 유머스러운 스타일로 영어 세그먼트X를 방송하는 것을 구현할 수 있고, 카툰 동물C가 슬픈 스타일로 중국어 세그먼트Y를 방송하는 등을 구현할 수 있다. 단계S206에서, 처리될 텍스트의 음향학 특징 정보에 기반하여, 처리될 텍스트의 음성을 합성한다. 본 실시예에서는, 음성 합성 모델의 내부 구조를 보다 명확하게 설명하기 위해, 음성 합성 모델의 내부 구조에 대해 분석하였지만, 실제 응용에서, 상기 음성 합성 모델은 종단간(end-to-end) 모델이고, 여전히 상기 원리에 기반하여 스타일, 음색 및 언어의 디커플링을 구현할 수 있어, 나아가 크로스-스타일, 크로스-음색, 크로스-언 어의 음성 합성을 구현할 수 있다. 실제 응용에서, 도 3과 도 4에 도시된 바와 같이, 합성될 텍스트, 스타일 ID 및 음색 멜(mel) 스펙트럼 시퀀스 가 주어지면, 사전에 텍스트 전처리 모듈을 사용하여, 상기 텍스트를 대응하는 음조를 가진 음자 시퀸스로 변환 하여, 획득된 음자 시퀸스는 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더의 입력으로 각각 사용될 수 있 다. 한편, 스타일 인코더는 또한 스타일 ID를 입력으로 사용하여, 이와 같이 콘텐츠 인코딩 시퀸스X1 및 스타일 인코딩 시퀸스X2를 각각 획득한다. 다음, 합성될 음색에 따라, 훈련 데이터 세트에서 상기 음색에 대응하는 멜 (mel) 스펙트럼 시퀀스를 선택하여 음색 인코더의 입력으로 사용하여, 음색 인코딩 벡터X3를 획득한다. 다음, X1, X2 및 X3에 대해 차원상에 모아 이어서 시퀸스Z를 획득하여 디코더의 입력으로 사용한다. 디코더는 입력된 시퀸스Z에 따라 대응하는 스타일, 대응하는 음색을 생성하여 상기 텍스트를 표현하는 멜(mel) 스펙트럼 시퀀스 를 생성하고, 최종적으로, 신경 보코더(WaveRNN)에 의해 대응하는 오디오를 합성한다. 소정의 합성될 텍스트는 중국어, 영어, 중-영 혼합 등과 같은 크로스-언어 텍스트일 수 있다는 점을 유의하여야 한다. 본 실시예의 음성 합성 방법에 따르면, 상기 기술 방안을 사용함으로써, 크로스-언어, 크로스-스타일, 크로스- 음색의 음색 합성을 구현할 수 있으므로, 음성 합성의 다양성을 최대적으로 풍부화시킬 수 있고, 장시간 방송의 지루함을 감소시킬 수 있으며, 사용자의 체험을 크게 향상시킬 수 있다. 본 실시예의 기술 방안은 다양한 음성 인터랙션 장면에 적응될 수 있으며, 보편적인 보급성을 갖고 있다. 도 5는 본 발명에 따른 제3 실시예에 대한 개략도이다. 도 5에 도시된 바와 같이, 본 실시예는 음성 합성 모델 의 훈련 방법을 제공하며, 구체적으로, 하기 단계를 포함할 수 있다. 단계S501에서, 복수의 훈련 데이터를 수집하고, 각 훈련 데이터는 합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트의 콘 텐츠 정보를 표현하는 스타일 특징 정보와, 훈련 스타일 정보에 대응하는 훈련 스타일 및 훈련 음색 정보에 대 응하는 훈련 음색을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 포함한다. 단계S502에서, 복수의 훈련 데이터를 사용하여, 음성 합성 모델을 훈련한다. 본 실시예에 따른 음성 합성 모델의 훈련 방법의 실행 주체는 음성 합성 모델의 훈련 장치이다. 상기 장치는 전 자 엔티티일 수도 있고, 또는 소프트웨어로 집적된 애플리케이션일 수 있고, 사용할 때, 컴퓨터 디바이스에서 실행되어 음성 합성 모델을 훈련한다. 본 실시예에 따른 훈련에서, 수집된 훈련 데이터의 수량은 백만 자릿수에 달할 수 있어, 음성 합성 모델을 보다 정확하게 훈련하도록 한다. 각 훈련 데이터에는 합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트 의 콘텐츠 정보를 포함할 수 있고, 각각 상기 실시예 중의 스타일 정보, 음색 정보 및 콘텐츠 정보에 대응하며, 구체적인 과정은 상기 실시예의 관련 설명을 참조할 수 있으며, 여기서는 설명을 생략한다. 또한, 각 훈련 데이터에는 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보와, 훈련 스타일 정보에 대응하는 훈련 스타일 및 훈련 음색 정보에 대응하는 훈련 음색을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 포함할 수 있고, 이 두 가지 정보는 지도 훈련을 위한 참조로 사용됨으로써, 음성 합성 모델이 보다 효율적으로 학습할 수 있도록 한다. 본 실시예의 음성 합성 모델의 훈련 방법에 따르면, 상술한 방법을 사용함으로써, 음성 합성 모델로 하여금 훈 련 데이터에 기반하여, 콘텐츠, 스타일 및 음색에 따라 음성 합성하는 과정을 학습하게 하도록, 음성 합성 모델을 효율적으로 훈련할 수 있어, 나아가 학습된 음성 합성 모델은 음성 합성의 다양성을 풍부화시킬 수 있다. 도 6은 본 발명에 따른 제4 실시예에 대한 개략도이다. 도 6에 도시된 바와 같이, 본 실시예의 음성 합성 모델 의 훈련 방법에 따르면, 상기 도 5에 도시된 실시예의 기술 방안의 기초에, 본 발명의 기술 방안은 보다 더 구 체적으로 설명한다. 도 2에 도시된 바와 같이, 본 실시예에 따른 음성 합성 방법은 구체적으로 하기 단계를 포 함할 수 있다. 단계S601에서, 복수의 훈련 데이터를 수집하고, 각 훈련 데이터는 합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트의 콘 텐츠 정보를 표현하는 스타일 특징 정보와, 훈련 스타일 정보에 대응하는 훈련 스타일 및 훈련 음색 정보에 대 응하는 훈련 음색을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 목표 음향학 특징 정보를 포함한다. 실제 응용에서, 우선 훈련 스타일 및 훈련 음색을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하여 대응하는 음 색을 획득한 후, 획득된 음색에 대해 멜(mel) 스펙트럼 추출을 하여, 대응되는 목표 음향학 특징 정보를 획득한 다. 즉, 상기 목표 음향학 특징 정보도 멜(mel) 스펙트럼 시퀸스 형태를 사용한다. 단계S602에서, 각 훈련 데이터에 대해, 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더 및 음색 인코더를 사용하여, 훈련 데이터 중의 훈련 텍스트의 콘텐츠 정보, 훈련 스타일 정보 및 훈련 음색 정보에 대해 각각 인 코딩하여, 훈련 콘텐츠 인코딩 특징, 훈련 스타일 인코딩 특징 및 훈련 음색 인코딩 특징을 순차적으로 획득한 다. 구체적으로, 음성 합성 모델 중의 콘텐츠 인코더를 사용하여, 훈련 데이터 중의 훈련 텍스트의 콘텐츠 정보에 대해 인코딩하여 훈련 콘텐츠 인코딩 특징을 획득한다. 음성 합성 모델 중의 스타일 인코더를 사용하여, 훈련 데이터 중의 훈련 스타일 정보 및 훈련 텍스트의 콘텐츠 정보에 대해 인코딩하여 훈련 스타일 인코딩 특징을 획 득한다. 음성 합성 모델 중의 음색 인코더를 사용하여, 훈련 데이터 중의 훈련 음색 정보에 대해 인코딩하여 훈 련 음색 인코딩 특징을 획득한다. 구체적인 구현 과정은 상기 도 2에 도시된 실시예의 단계S202 내지 단계S204 에 대한 관련 설명을 참조할 수 있으며, 여기서는 설명을 생략한다. 단계S603에서, 음성 합성 모델 중의 스타일 추출기를 사용하여, 훈련 텍스트의 콘텐츠 정보 및 훈련 스타일 정 보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현한 스타일 특징 정보에 기반하여, 목 표 훈련 스타일 인코딩 특징을 추출한다. 설명해야 할 것은, 상기 훈련 텍스트의 콘텐츠 정보는 상기 스타일 인코더가 훈련 중에 입력된 훈련 텍스트의 콘텐츠 정보와 동일하다. 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보는 멜(mel) 스펙트럼 시퀀스의 형태일 수 있다. 도 7은 본 실시예에 따른 음성 합성 모델의 훈련 아키텍처 개략도이다. 도 7에 도시된 바와 같이, 상기 도 3에 도시된 상기 음성 합성 모델의 응용 아키텍처 개략도에 비해, 이 음성 합성 모델은 훈련의 효과를 향상시키기 위해, 훈련 과정에서 스타일 추출기가 더 추가된다. 그러나, 사용 과정에는 상기 스타일 추출기가 필요하지 않 고, 도 3에 도시된 아키텍처를 직접 사용한다. 도 7에 도시된 바와 같이, 이 스타일 추출기는 스타일 벡터가 텍 스트 측면으로 압축될 수 있도록, 참조 스타일 인코더, 참조 콘텐츠 인코더 및 어텐션 메커니즘 모듈을 포함할 수 있고, 획득된 목표 훈련 스타일 인코딩 특징은 스타일 인코더의 학습 목표이다. 구체적으로, 훈련 단계에서, 스타일 추출기는 무 지도 방식으로 스타일 표현을 학습하고, 상기 스타일 표현은 동시에 스타일 인코더에 대한 목표 구동 스타일 인코더로도 학습된다. 음성 합성 모델에 대한 훈련이 끝나면, 스타일 인코더는 스타일 추출기와 동일한 기능을 갖게 된다. 응용 단계에서, 스타일 인코더는 스타일 추출기를 대체할 것이다. 이로 인해, 스타일 추출기는 훈련 단계에서만 존재한다. 주의해야 할 것은, 스타일 추출기의 강 력한 효과에 기인하여, 전체 음성 합성 모델은 양호한 디커플링 기능을 가지게 되어, 즉, 콘텐츠 인코더, 스타 일 인코더, 음색 인코더가 각자 맡은바 소임을 실행하게 하고, 업무 분할이 명확한다. 여기서, 콘텐츠 인코더는 어떻게 발음하는지를 담당하고, 스타일 인코더는 발음의 스타일 방식에 대해 담당하며, 음색 인코더는 누구의 음색으로 발음할 것인지에 대해 담당한다. 단계S604에서, 음성 합성 모델 중의 디코더를 사용하여, 훈련 콘텐츠 인코딩 특징, 목표 훈련 스타일 인코딩 특 징 및 훈련 음색 인코딩 특징에 기반하여, 디코딩하여 훈련 텍스트의 예측 음향학 특징 정보를 생성한다. 단계S605에서, 훈련 스타일 인코딩 특징, 목표 훈련 스타일 인코딩 특징, 예측 음향학 특징 정보 및 목표 음향 학 특징 정보에 기반하여, 종합 손실 함수를 구성한다.예를 들어, 상기 단계의 구현 과정은 구체적으로 하기 단계들을 포함한다. (a) 훈련 스타일 인코딩 특징 및 목표 훈련 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구성한다. (b) 예측 음향학 특징 정보 및 목표 음향학 특징 정보에 기반하여, 재 구성 손실 함수를 구성한다. (c) 손실 함수 및 재 구성 손실 함수에 기반하여, 종합 손실 함수를 구성한다. 구체적으로, 스타일 손실 함수와 재 구성 손실 함수에 대한 특정 가중치를 설정할 수 있고, 스타일 손실 함수와 재 구성 손실 함수의 가중치의 합계를 최종 종합 손실 함수로 결정할 수 있다. 구체적인 가중치 비율은 실제 수 요에 따라 설정될 수 있다. 예를 들어, 스타일을 강조하려는 경우, 상대적으로 큰 가중치를 설정할 수 있다. 예 를 들어, 재 구성 손실 함수의 가중치를 1로 설정하는 경우, 스타일 손실 함수에 대한 가중치를 1-10 사이의 임 의의 값으로 설정할 수 있으며, 값이 클수록 스타일 손실 함수의 비중이 더 크고, 훈련 중에 스타일이 전체에 대한 영향이 더 크다. 단계S606에서, 종합 손실 함수가 수렴되는지를 판단하고, 수렴되지 않은 경우, 단계S607을 실행하고, 수렴되면, 단계S608을 실행한다. 단계S607에서, 종합 손실 함수가 수렴으로 향하도록, 콘텐츠 인코더, 스타일 인코더, 음색 인코더, 스타일 추출 기 및 디코더의 파라미터를 조정하고, 단계S602로 돌아가서 다음 훈련 데이터를 획득하고 훈련을 계속한다. 단계S608에서, 사전 설정된 차례수의 연속적인 훈련 중, 종합 손실 함수가 항상 수렴되는지를 판단하고, 항상 수렴되지 않은 경우, 단계S602로 돌아가서 다음 훈련 데이터를 획득하고 훈련을 계속하며, 반면에 항상 수렴되 는 경우, 음성 합성 모델의 파라미터를 결정하고, 나아가 음성 합성 모델을 결정하며, 훈련을 종료한다. 상기 단계는 훈련 종료의 조건으로 사용될 수 있고, 여기서, 연속적인 사전에 설정된 차례수는 실제 수요에 따 라 설정될 수 있어, 예를 들어, 연속적인 100회, 200회 또는 다른 횟수와 같은 횟수이다. 사전 설정된 차례수의 연속적인 훈련 중, 종합 손실 함수가 항상 수렴되면, 상기 음성 합성 모델이 이미 완벽하게 훈련되었음을 의미 하고, 훈련을 종료해도 된다. 또한, 선택적으로, 실제 훈련에서, 음성 합성 모델은 수렴에 무한히 접근하지만, 사전 설정된 차례수의 연속적인 훈련에서 절대적으로 수렴되지 않을 수 있다. 이때, 사전 차례수 임계치가 설정 된 훈련을 훈련 종료 조건으로 설정할 수 있으며, 훈련 차례수가 사전 설정된 차례수 임계치에 달하면, 훈련은 종료되고, 훈련 종료 시의 음성 합성 모델의 파라미터를 획득하여, 상기 음성 합성 모델의 최종 파라미터로 결 정할 수 있으며, 또한 상기 음성 합성 모델은 결정된 최종 파라미터에 기반하여 사용될 수 있다. 반면에, 훈련 차례수가 사전 설정된 차례수 임계치에 달할 때까지 훈련은 계속된다. 상기 단계S602 내지 단계S607은 상기 도 5에 도시한 실시예의 단계S502의 하나의 구현 방식이다. 본 실시예는 훈련 과정에서 음성 합성 모델 중의 각 유닛에 대해 설명하였지만, 전체 음성 합성 모델의 훈련 과 정은 종단간(end-to-end) 훈련이다. 상기 음성 합성 모델에 대한 훈련에는 총 두 부분의 손실 함수를 포함하 되, 하나는 디코더의 출력에 기반하여 구성된 재 구성 손실 함수이고, 다른 하나는 스타일 인코더의 출력과 스 타일 추출기의 출력에 기반하여 구성된 스타일 손실 함수이다. 두 부분의 손실 함수는 모두 L2 노름(norm)의 손 실 함수를 사용할 수 있다. 본 실시예의 음성 합성 모델의 훈련 방법에 따르면, 상기 방안을 사용함으로써, 훈련 과정 중에, 콘텐츠, 스타 일 및 음색의 완전한 디커플링을 효과적으로 확보할 수 있고, 이에 따라, 훈련된 음성 합성 모델로 하여금 크로 스-스타일, 크로스-음색, 크로스-언어에 대한 음성 합성을 구현 가능하게 할 수 있으며, 음성 합성의 다양성을 최대적으로 풍부화시킬 수 있고, 장시간 방송의 지루함을 감소시킬 수 있으며, 사용자의 체험을 크게 향상시킬 수 있다. 도 8은 본 발명에 따른 제5 실시예에 대한 개략도이다. 도 8에 도시된 바와 같이, 본 발명의 실시예가 음성 합 성 장치를 제공하며, 획득 모듈 및 생성 모듈, 합성 모듈을 포함한다. 획득 모듈은 합성될 음성의 스타일 정보 및 음색 정보, 처리될 텍스트의 콘텐츠 정보를 획득하기 위한 것 이다. 생성 모듈은 사전 훈련된 음성 합성 모델을 사용하여, 스타일 정보, 음색 정보 및 처리될 텍스트의 콘텐츠 정보에 기반하여, 처리될 텍스트의 음향학 특징 정보를 생성하기 위한 것이다. 합성 모듈은 처리될 텍스트의 음향학 특징 정보에 기반하여, 처리될 텍스트의 음성을 합성하기 위한 것이다. 본 실시예에 따른 음성 합성 장치는, 상기 모듈들을 사용하여 음성 합성 처리에 대한 구현 원리 및 기술 효과를 구현하고, 상기 관련 방법 실시예의 구현 메커니즘과 동일하며, 이에 대한 상세한 설명은 상기 관련 방 법 실시예를 참조할 수 있으며, 여기서는 설명을 생략한다. 도 9는 본 발명에 따른 제6 실시예에 대한 개략도이다. 도 9에 도시된 바와 같이, 본 실시예에 따르면 음성 합 성 모델을 제공한다. 본 발명의 실시예에 따른 음성 합성 모델은 상기 도 8에 도시된 실시예의 기초 에, 본 발명의 기술 방안을 보다 더 구체적으로 설명한다. 도 9에 도시된 바와 같이, 본 실시예에 따른 음성 합성 장치 중, 생성 모듈은, 콘텐츠 인코딩부 , 스타일 인코딩부, 음색 인코딩부, 및 디코딩부를 포함한다 콘텐츠 인코딩부는 음성 합성 모델 중의 콘텐츠 인코더를 사용하여, 처리될 텍스트의 콘텐츠 정보에 대해 인코딩하여, 콘텐츠 인코딩 특징을 획득하기 위한 것이다. 스타일 인코딩부는 음성 합성 모델 중의 스타일 인코더를 사용하여, 처리될 텍스트의 콘텐츠 정보 및 스 타일 정보에 대해 인코딩하여, 스타일 인코딩 특징을 획득하기 위한 것이다. 음색 인코딩부는 음성 합성 모델 중의 음색 인코더를 사용하여, 음색 정보에 대해 인코딩하여, 음색 인코 딩 특징을 획득하기 위한 것이다. 디코딩부는 음성 합성 모델 중의 디코더를 사용하여, 콘텐츠 인코딩 특징, 스타일 인코딩 특징 및 음색 인코딩 특징에 기반하여, 디코딩하여 처리될 텍스트의 음향학 특징 정보를 생성하기 위한 것이다. 나아가 선택적으로, 본 발명의 실시예에 따른 음성 합성 장치 중, 획득 모듈은, 사용자의 입력 스타일에 대한 표현 정보를 획득하고, 입력 스타일에 대한 표현 정보에 따라, 사전 설정된 스타 일 테이블로부터 입력 스타일에 대응하는 스타일 식별자를 획득하여, 합성될 음성의 스타일 정보로 하거나, 또 는, 입력 스타일에 의해 표현된 오디오 정보를 획득하고, 상기 오디오 정보에서 상기 입력 스타일의 음색 정보를 추 출하여, 상기 합성될 음성의 스타일 정보로 하기 위한 것이다. 본 실시예에 따른 음성 합성 장치는, 상기 모듈들을 사용하여 음성 합성 처리에 대한 구현 원리 및 기술 효과를 구현하고, 상기 관련 방법 실시예의 구현 메커니즘과 동일하며, 이에 대한 상세한 설명은 상기 관련 방 법 실시예를 참조할 수 있으며, 여기서는 설명을 생략한다. 도 10은 본 발명에 따른 제7 실시예에 대한 개략도이다. 도 10에 도시된 바와 같이, 본 발명의 실시예는 음성 합성 모델의 훈련 장치를 제공하며, 수집 모듈 및 훈련 모듈을 포함한다. 수집 모듈은 복수의 훈련 데이터를 수집하기 위한 것이고, 각 훈련 데이터는 합성될 음성의 훈련 스타일 정보, 훈련 음색 정보, 훈련 텍스트의 콘텐츠 정보, 및 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보와, 및 훈련 스타일 정보에 대응하는 훈련 스타일 및 훈련 음색 정보에 대응하는 훈련 음색을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 목표 음향학 특징 정 보를 포함한다. 훈련 모듈은 복수의 훈련 데이터를 사용하여, 음성 합성 모델을 훈련하기 위한 것이다. 본 실시예에 따른 음성 합성 모델의 훈련 장치는, 상기 모듈들을 사용하여 음성 합성 모델의 훈련에 대한 구현 원리 및 기술 효과를 구현하고, 상기 관련 방법 실시예의 구현 메커니즘과 동일하며, 이에 대한 상세한 설 명은 상기 관련 방법 실시예를 참조할 수 있으며, 여기서는 설명을 생략한다. 도 11은 본 발명에 따른 제8 실시예에 대한 개략도이다. 도 11에 도시된 바와 같이, 본 실시예는, 음성 합성 모 델의 훈련 장치를 제공한다. 본 실시예에 따른 음성 합성 모델의 훈련 장치는 상기 도 10에 도시된 실시예의 기초에, 본 발명의 기술 방안을 보다 더 구체적으로 설명한다. 도 11에 도시된 바와 같이, 본 발명의 실시예에 따른 음성 합성 모델의 훈련 장치 중, 훈련 모듈은, 인코딩부, 추출부, 디코딩부, 구성부 및 조정부를 포함한 다.인코딩부는 각 훈련 데이터에 대해, 음성 합성 모델 중의 콘텐츠 인코더, 스타일 인코더 및 음색 인코더 를 사용하여, 훈련 데이터 중의 훈련 텍스트의 콘텐츠 정보, 훈련 스타일 정보 및 훈련 음색 정보에 대해 각각 인코딩하여, 훈련 콘텐츠 인코딩 특징, 훈련 스타일 인코딩 특징 및 훈련 음색 인코딩 특징을 순차적으로 획득 하기 위한 것이다. 추출부는 음성 합성 모델 중의 스타일 추출기를 사용하여, 훈련 텍스트의 콘텐츠 정보 및 훈련 스타일 정보에 대응하는 훈련 스타일을 사용하여 훈련 텍스트의 콘텐츠 정보를 표현하는 스타일 특징 정보에 기반하여, 목표 훈련 스타일 인코딩 특징을 추출하기 위한 것이다. 디코딩부는 음성 합성 모델 중의 디코더를 사용하여, 훈련 콘텐츠 인코딩 특징, 목표 훈련 스타일 인코 딩 특징 및 훈련 음색 인코딩 특징에 기반하여, 디코딩하여 훈련 텍스트에 대한 예측 음향학 특징 정보를 생성 하기 위한 것이다. 구성부는 훈련 스타일 인코딩 특징, 목표 훈련 스타일 인코딩 특징, 예측 음향학 특징 정보 및 목표 음 향학 특징 정보에 기반하여, 종합 손실 함수를 구성하기 위한 것이다. 조정부는 종합 손실 함수가 수렴되지 않은 경우, 종합 손실 함수가 수렴으로 향하도록, 콘텐츠 인코더, 스타일 인코더, 음색 인코더, 스타일 추출기 및 디코더의 파라미터를 조정하기 위한 것이다. 나아가 선택적으로, 구성부는, 훈련 스타일 인코딩 특징 및 목표 훈련 스타일 인코딩 특징에 기반하여, 스타일 손실 함수를 구성하고, 예측 음향학 특징 정보 및 목표 음향학 특징 정보에 기반하여, 재 구성 손실 함수를 구성하며, 스타일 손실 함수 및 재 구성 손실 함수에 기반하여, 종합 손실 함수를 생성하기 위한 것이다. 본 발명의 실시예에 따른 음성 합성 모델의 훈련 장치는, 상기 모듈을 사용하여 음성 합성 모델의 훈련에 대한 구현 원리 및 기술 효과를 구현하고, 상기 관련 방법 실시예의 구현 메커니즘과 동일하며, 이에 대한 상세 한 설명은 상기 관련 방법 실시예를 참조할 수 있으며, 여기서는 설명을 생략한다. 본 발명의 실시예에 따르면, 본 발명은 전자 기기 및 판독 가능 저장 매체를 더 제공한다. 도 12에 도시된 바와 같이, 본 실시예에 따른 상기 방법을 구현하기 위한 전자 기기의 블록도이다. 전자 기기는 예를 들어, 랩톱 컴퓨터, 데스크톱 컴퓨터, 워크스테이션, 개인 디지털 비서(Personal Digital Assistants: PDA), 서버, 블레이드 서버, 메인프레임 컴퓨터, 및 기타 적절한 컴퓨터와 같은 다양한 형태의 디지털 컴퓨터를 의미한다. 전자 기기는 예를 들어, 개인 디지털 비서(Personal Digital Assistants: PDA), 셀룰러 폰, 스마트 폰, 웨어러블 기기, 및 기타 유사한 컴퓨팅 기기와 같은 다양한 형태의 모바일 기기도 의미할 수 있다. 본 명세 서에 기재된 부품, 이들의 연결 및 관계, 그리고 이들의 기능은 단지 예시적인 것에 불과하며, 본 명세서에서 설명 및/또는 요구하는 본 발명의 범위를 한정하기 위한 것이 아니다. 도 12에 도시된 바와 같이, 상기 전자 기기는, 하나 또는 복수의 프로세서, 메모리, 및 각 부품을 연결하기 위한 인터페이스를 포함하고, 상기 인터페이스에는 고속 인터페이스 및 저속 인터페이스가 포함된다. 각 부품들은 서로 다른 버스를 통해 서로 연결되고, 공공 메인보드에 장착되거나 또는 수요에 따라 기타 방식으 로 장착될 수 있다. 프로세서는 전자 기기에서 실행되는 명령어들을 실행할 수 있고, 상기 명령어는 메모리에 저장되어 외부 입력/출력 장치(예를 들어, 인터페이스에 접속된 표시 장치)에 GUI의 그래픽 정보를 표시하기 위 한 명령어를 포함할 수 있다. 다른 실시예에서는, 수요에 따라 복수의 프로세서 및/또는 복수의 버스를 복수의 메모리와 함께 사용할 수 있다. 마찬가지로, 복수의 전자 기기를 연결할 수 있고, 각 전자 기기에 의해 일부 필 요한 동작을 제공할 수 있다(예를 들어, 서버 어레이, 한 세트의 블레이드 서버, 또는 멀티 프로세서 시스템으 로 함). 도 12에서는, 하나의 프로세서의 경우를 예로 한다. 메모리는 본 발명에 의해 제공되는 비휘발성 컴퓨터 판독 가능 저장 매체이다. 여기서, 상기 메모리에는 적어도 하나의 프로세서에 의해 실행될 수 있는 명령어가 저장되어 있고, 상기 명령어가 상기 적어도 하나의 프 로세서에 의해 실행될 경우로 하여금, 본 발명에 의해 제공되는 음성 합성 방법 및 음성 합성 모델의 훈련 방법 을 실행하게 한다. 본 발명의 비휘발성 컴퓨터 판독 가능 저장 매체에는 컴퓨터로 하여금 본 발명에 의해 제공 되는 음성 합성 방법 및 음성 합성 모델의 훈련 방법을 실행하도록 기 위한 컴퓨터 명령어가 저장되어 있다. 메모리는 비휘발성 컴퓨터 판독가능 저장 매체로서, 예를 들어, 본 발명의 실시예에 따른 음성 합성 방법 및 음성 합성 모델의 훈련 방법에 대응하는 프로그램 명령어/모듈(예를 들어, 도 8, 도 9, 도 10 및 도 11에 도시된 관련 모듈)과 같은 비휘발성 소프트웨어 프로그램, 비휘발성 컴퓨터 실행가능 프로그램 및 모듈을 저장하 기 위한 것일 수 있다. 프로세서는 메모리에 저장된 비휘발성 소프트웨어 프로그램, 명령어 및 모 듈을 실행함으로써, 서버의 다양한 기능 애플리케이션 및 데이터 처리를 실행한다. 즉, 상기 방법 실시예에 따 른 음성 합성 방법 및 음성 합성 모델의 훈련 방법을 구현한다. 메모리는 프로그램 저장 영역 및 데이터 저장 영역을 포함할 수 있다. 여기서, 프로그램 저장 영역은 OS 시스템 및 적어도 하나의 기능에 필요한 앱을 저장할 수 있고, 데이터 저장 영역은 음성 합성 방법 및 음성 합 성 모델의 훈련 방법을 구현하는 전자 기기의 사용에 따라 생성된 데이터 등을 저장할 수 있다. 또한, 메모리 는 고속 RAM(Random Access Memory)를 포함할 수도 있고, 또한 예를 들어 적어도 하나의 디스크 저장 디 바이스, 플래시 메모리 디바이스, 또는 기타 비휘발성 고체 저장 디바이스와 같은 비휘발성 메모리를 포함할 수 도 있다. 일부 실시예에 따르면, 메모리는 프로세서에 대해 원격으로 설치된 메모리를 포함할 수 있고, 이러한 원격 메모리는 네트워크를 통해 음성 합성 방법 및 음성 합성 모델의 훈련 방법을 구현하는 전자 기기에 연결될 수 있다. 상기 네트워크의 실예로는 인터넷, 인트라넷, 근거리 통신망(LAN), 이동 통신망 및 이 들의 조합을 포함할 수 있는데, 이에 한정되지는 않는다. 음성 합성 방법 및 음성 합성 모델의 훈련 방법을 구현하는 전자 기기는, 입력 장치 및 출력 장치 를 더 포함할 수 있다. 프로세서, 메모리, 입력 장치 및 출력 장치는 버스 또는 기타 방식으로 연결될 수 있는데, 도 12에서는 버스를 통해 연결되는 경우를 예로 한다. 입력 장치는 입력된 숫자 또는 문자 부호 정보를 수신할 수 있고, 또한 음성 합성 방법 및 음성 합성 모 델의 훈련 방법을 구현하는 전자 기기의 사용자 설정 및 기능 제어와 연관된 키 신호 입력을 생성할 수 있으며, 예를 들어, 터치 스크린, 키 패드, 마우스, 트랙 패드, 터치 패드, 인디케이터 로드, 하나 또는 복수의 마우스 버튼, 트랙 볼, 콘트롤러 로드 등과 같은 입력 장치를 포함할 수 있다. 출력 장치는 표시 장치, 보조 조 명 장치(예를 들어, LED) 및 촉각 피드백 장치(예를 들어, 진동모터) 등을 포함할 수 있다. 상기 표시 장치는 액정 디스플레이(LCD), 발광 다이오드(LED) 디스플레이 및 플라스마 디스플레이를 포함할 수 있는데, 이에 한정 되지는 않는다. 일부 실시 형태에 따르면, 표시 장치는 터치 스크린일 수 있다. 여기서 설명한 시스템 및 기술의 다양한 실시 형태는 디지털 전자 회로 시스템, 집적 회로 시스템, 전용 ASIC (전용 집적 회로), 컴퓨터 하드웨어, 펌웨어, 소프트웨어, 및/또는 이들의 조합에서 구현될 수 있다. 이러한 다 양한 실시 형태는 하나 또는 복수의 컴퓨터 프로그램을 통해 구현되는 것을 포함할 수 있고, 상기 하나 또는 복 수의 컴퓨터 프로그램은 적어도 하나의 프로그램 가능 프로세서를 포함하는 프로그램 가능 시스템에서 실행 및/ 또는 해석될 수 있으며, 상기 프로그램 가능 프로세서는 전용 또는 범용 프로그램 가능 프로세서일 수 있고, 저 장 시스템, 적어도 하나의 입력 장치, 및 적어도 하나의 출력 장치로부터 데이터 및 명령어를 수신하고, 데이터 및 명령어를 저장 시스템, 적어도 하나의 입력 장치, 및 적어도 하나의 출력 장치로 송신할 수 있다. 이러한 컴퓨터 프로그램(프로그램, 소프트웨어, 소프트웨어 애플리케이션, 또는 코드라고도 함)은 프로그램 가 능 프로세서의 기계 명령어를 포함하고, 하이 라벨 프로세스 및/또는 객체 지향 프로그래밍 언어, 및/또는 어셈 블러/기계언어를 사용하여 이러한 컴퓨터 프로그램을 실시할 수 있다. 본 명세서에서 사용되는 \"기계 판독가능 매체\" 및 \"컴퓨터 판독가능 매체\" 등과 같은 용어는, 기계 명령어 및/또는 데이터를 프로그램 가능 프로세서의 임의의 컴퓨터 프로그램 제품, 기기, 및/또는 장치(예를 들어, 디스크, CD-ROM, 메모리, 프로그램 가능 논리 장 치(PLD))에 제공하기 위한 것이고, 기계 판독 가능 신호로서의 기계 명령어를 수신하는 기계 판독가능 매체를 포함한다. \"기계 판독가능 신호\"라는 용어는 기계 명령어 및/또는 데이터를 프로그램 가능 프로세서에 제공하기 위한 임의의 신호를 의미한다. 사용자와의 인터랙션을 제공하기 위해서는, 컴퓨터를 통해 본 명세서에서 설명한 시스템 및 기술을 구현할 수 있는데, 상기 컴퓨터는, 사용자에게 정보를 표시하기 위한 표시 장치(예를 들어, CRT(음극선관) 또는 LCD(액정 디스플레이) 모니터), 및 사용자가 상기 컴퓨터에 입력을 제공할 수 있는 키보드 및 포인팅 디바이스(예를 들어, 마우스 또는 트랙 볼)를 포함한다. 기타 유형의 디바이스도 사용자와의 인터랙션을 제공하기 위한 것일 수 있다. 예를 들어, 사용자에게 제공되는 피드백은 임의의 형태의 센싱 피드백(예를 들어, 시각 피드백, 청각 피드백, 또는 촉각 피드백)일 수 있고, 임의의 형태(소리 입력, 음성 입력, 또는 촉각 입력을 포함)로 사용자로 부터의 입력을 수신할 수 있다. 본 명세서에서 설명한 시스템 및 기술은, 백 그라운드 부품을 포함하는 컴퓨팅 시스템(예를 들어, 데이터 서 버), 또는 미들웨어 부품을 포함하는 컴퓨팅 시스템(예를 들어, 애플리케이션 서버), 또는 프론트 앤드 부품을 포함하는 컴퓨팅 시스템(예를 들어, GUI 또는 웹 브라우저를 갖는 사용자 컴퓨터이며, 사용자는 상기 GUI 또는상기 웹 브라우저를 통하여 본 명세서에서 설명한 상기 시스템 및 기술의 실시 형태와 인터랙션을 할 수 있음), 또는 이러한 백 그라운드 부품, 미들웨어 부품, 또는 프론트 앤드 부품의 임의의 조합을 포함하는 컴퓨팅 시스 템에서 구현될 수 있다. 시스템의 부품은 임의의 형태 또는 매체의 디지털 데이터 통신(예를 들어, 통신 네트워 크)을 통해 서로 연결될 수 있다. 통신 네트워크는 예를 들어 근거리 통신망(LAN), 광역 통신망(WAN), 인터넷 및 블록 체인 네트워크를 포함할 수 있다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트 및 서버는 일반적으로 서로 멀리 떨어져 있 고, 통상적으로 통신 네트워크를 통해 인터랙션을 진행한다. 클라이언트와 서버의 관계는 대응하는 컴퓨터에서 실행되고 서로 클라이언트-서버의 관계를 갖는 컴퓨터 프로그램에 의해 생성된다. 서버는 클라우드 컴퓨팅 서버 또는 클라우드 호스트라고도 하는 클라우드 서버일 수 있고, 클라우드 컴퓨팅 서비스 체계 중 하나의 호스트 제 품으로써, 전통적인 물리적 호스트 및 VPS 서비스 (\"Virtual Private Server\"，또는 \"VPS\"로 약칭함)에서 관리 가 어렵고 업무 확장성이 약한 단점을 해결한다. 본 발명의 실시예에 따른 기술 방안은, 합성될 음성의 스타일 정보 및 음색 정보, 처리될 텍스트의 콘텐츠 정보 를 획득하고, 사전 훈련된 음성 합성 모델을 사용하여, 스타일 정보, 음색 정보 및 처리될 텍스트의 콘텐츠 정 보에 기반하여, 처리될 텍스트의 음향학 특징 정보를 생성하며, 처리될 텍스트의 음향학 특징 정보에 기반하여, 처리될 텍스트의 음성을 합성함으로써, 크로스-언어, 크로스-스타일, 크로스-음색의 음성 합성을 구현할 수 있 으며, 음성 합성의 다양성을 최대적으로 풍부할 수 있고, 사용자의 사용 체험도를 향상할 수 있다. 본 실시예의 기술 방안에 따르면, 상기 기술 방안을 사용함으로써, 크로스-언어, 크로스-스타일, 크로스-음색의 음색 합성을 구현할 수 있으며, 음성 합성의 다양성을 최대적으로 풍부화시킬 수 있고, 장시간 방송의 지루함을 감소시킬 수 있으며, 사용자의 체험을 크게 향상시킬 수 있다. 본 발명의 실시예에 따른 기술 방안은 다양한 음 성 인터랙션 장면에 적응될 수 있으며, 보편적인 보급성을 갖고 있다. 본 실시예의 기술 방안에 따르면, 상기 방안을 사용함으로써, 음성 합성 모델을 효과적으로 훈련할 수 있어, 음 성 합성 모델로 하여금 훈련 데이터에 기반하여, 콘텐츠, 스타일 및 음색에 따라 음성 합성하는 과정을 학습하 게 하여, 나아가 학습된 음성 합성 모델은 음성 합성의 다양성을 풍부화시킬 수 있다. 본 발명의 실시예에 따른 기술 방안에 따르면, 상기 방안을 사용함으로써, 훈련 과정 중에, 콘텐츠, 스타일 및 음색의 완전 디커플링을 효과적으로 확보할 수 있고, 이에 따라, 훈련된 음성 합성 모델로 하여금 크로스-스타 일, 크로스-음색, 크로스-언어에 대한 음성 합성을 구현 가능하게 할 수 있으며, 음성 합성의 다양성을 최대적 으로 풍부화시킬 수 있고, 장시간 방송의 지루함을 감소시킬 수 있으며, 사용자의 체험을 크게 향상시킬 수 있 다. 상기에서 설명한 다양한 흐름을 사용하여 각 단계에 대해 다시 순서 배열, 추가 또는 삭제할 수 있다는 점을 이 해하여야 한다. 예를 들어, 본 발명이 개시된 기술 방안이 원하는 결과를 구현할 수 있는 한, 본 발명에 기재된 다양한 단계는 병렬적으로 또는 순차적으로, 또는 서로 다른 순서로 실행될 수 있고, 본 발명은 이에 대해 특별 히 한정하지 않는다. 본 발명의 보호범위는 상기 다양한 실시 형태에 의해 제한되지 않는다. 당업자라면, 설계 요구 및 기타 요인에 의해, 다양한 수정, 조합, 서브 조합 및 교체가 이루어질 수 있음을 이해할 것이다. 본 발명의 취지 및 원칙 내 에서 이루어진 임의의 수정, 등가 교체 및 개선 등은 모두 본 발명의 보호범위에 속한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12"}
{"patent_id": "10-2021-0117980", "section": "도면", "subsection": "도면설명", "item": 1, "content": "첨부된 도면은 본 발명을 보다 쉽게 이해하도록 하기 위한 것이고, 본 발명은 이에 한정되지 않는다. 도 1은 본 발명에 따른 제1 실시예에 대한 개략도이다. 도 2는 본 발명에 따른 제2 실시예에 대한 개략도이다. 도 3은 본 발명의 실시예에 따른 음성 합성 모델의 응용 아키텍처 개략도이다. 도 4는 본 발명의 실시예에 따른 음성 합성 모델 중의 스타일 인코더의 개략도이다. 도 5는 본 발명에 따른 제3 실시예에 대한 개략도이다. 도 6은 본 발명에 따른 제4 실시예에 대한 개략도이다. 도 7은 본 발명의 실시예에 따른 음성 합성 모델의 훈련 아키텍처 개략도이다. 도 8은 본 발명에 따른 제5 실시예에 대한 개략도이다. 도 9는 본 발명에 따른 제6 실시예에 대한 개략도이다. 도 10은 본 발명에 따른 제7 실시예에 대한 개략도이다. 도 11은 본 발명에 따른 제8 실시예에 대한 개략도이다. 도 12는 본 발명의 실시예에 따른 상기 방법을 구현하기 위한 전자 기기의 블록도이다."}
