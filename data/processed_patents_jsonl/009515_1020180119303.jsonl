{"patent_id": "10-2018-0119303", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0044182", "출원번호": "10-2018-0119303", "발명의 명칭": "자율 주행 장치의 객체 인식 방법 및 자율 주행 장치", "출원인": "삼성전자주식회사", "발명자": "박규태"}}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "자율 주행 장치가 객체를 인식하는 방법에 있어서,상기 자율 주행 장치에 배치된 카메라를 이용하여 제 1 RGB 이미지를 획득하는 단계; 상기 제 1 RGB 이미지의 명도(brightness) 정보에 기초하여, 상기 제 1 RGB 이미지 내에서 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예측하는 단계;상기 자율 주행 장치에 배치된 동적 비전 센서(DVS: Dynamic Vision Sensor)를 통해 획득되는 객체 정보에 기초하여, 상기 적어도 하나의 제 1 영역 중에서 객체가 존재하는 적어도 하나의 제 2 영역을 결정하는 단계; 상기 적어도 하나의 제 2 영역과 관련하여 상기 카메라의 촬영 설정 정보(photographic configurationinformation)를 제어함으로써, 개선된(enhanced) 제 2 RGB 이미지를 획득하는 단계; 및상기 제 2 RGB 이미지 내에서 상기 객체를 인식하는 단계를 포함하는, 방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예측하는 단계는, 상기 제 1 RGB 이미지 내에서 명도 값(brightness value)이 임계 범위를 벗어나는 영역을 상기 적어도 하나의제 1 영역으로 예측하는 단계를 포함하는, 방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서, 상기 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예측하는 단계는,상기 제 1 RGB 이미지를 분석하여, 상기 제 1 RGB 이미지의 히스토그램을 획득하는 단계; 상기 제 1 RGB 이미지의 히스토그램을 이용하여, 상기 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재하는지 판단하는 단계; 및상기 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재한다고 판단되는 경우, 상기 제 1 RGB 이미지의 명도 정보에 기초하여, 상기 제 1 RGB 이미지 내에서 상기 적어도 하나의 제 1 영역을 예측하는 단계를 포함하는,방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서, 상기 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예측하는 단계는,복수의 RGB 이미지를 학습한 제 1 인공 지능 모델을 이용하여, 상기 제 1 RGB 이미지 내에서 객체 인식 불가능영역이 존재하는지 판단하는 단계; 및상기 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재한다고 판단되는 경우, 상기 제 1 인공 지능 모델을이용하여 상기 제 1 RGB 이미지 내에서 상기 적어도 하나의 제 1 영역을 예측하는 단계를 포함하는, 방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서, 상기 객체 정보는, 상기 동적 비전 센서(DVS)를 통해 획득되는 DVS 이미지 및 상기 DVS 이미지에서 검출되는 적어도 하나의 객체에대한 위치 정보 중 적어도 하나를 포함하는, 방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1 항에 있어서, 상기 적어도 하나의 제 1 영역 중에서 객체가 존재하는 적어도 하나의 제 2 영역을 결정하는공개특허 10-2020-0044182-3-단계는,상기 제 1 RGB 이미지 및 상기 동적 비전 센서를 통해 획득되는 DVS 이미지를 제 2 인공 지능 모델에 적용하여,상기 적어도 하나의 제 2 영역을 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항에 있어서, 상기 제 2 RGB 이미지를 획득하는 단계는,상기 적어도 하나의 제 2 영역에 대한 노출, 포커스 및 화이트 밸런스 중 적어도 하나를 제어하는 단계를 포함하는, 방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서, 상기 제 2 RGB 이미지를 획득하는 단계는,상기 카메라의 센서 감도(GAIN), 조리개 및 노출 시간 중 적어도 하나를 조절하는 단계를 포함하는, 방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항에 있어서, 상기 방법은, 상기 제 2 RGB 이미지가 복수의 프레임으로 구성되는 경우, 상기 복수의 프레임 각각에서 인식되는 상기 객체에포함된 특징(feature)을 추적 (tracking)하여, 상기 자율 주행 장치의 위치 정보를 획득하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1 항에 있어서, 상기 방법은,상기 인식된 객체에 관한 정보에 기초하여, 상기 자율 주행 장치의 경로를 결정하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 1 항에 있어서, 상기 방법은, 상기 카메라를 이용하여, 상기 인식된 객체를 추적하는 단계; 상기 동적 비전 센서를 통해, 상기 자율 주행 장치 주변에 나타나는 새로운 객체를 감지하는 단계; 상기 새로운 객체가 감지됨에 따라, 상기 카메라를 통해 획득되는 제 3 RGB 이미지 내에서 상기 새로운 객체가인식될 가능성이 임계 값보다 큰 후보 영역을 결정하는 단계; 및상기 후보 영역에 대한 영상 처리를 통해 상기 제 3 RGB 이미지에서 상기 새로운 객체를 인식하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "카메라; 동적 비전 센서(DVS: Dynamic Vision Sensor); 및적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는,상기 카메라를 통해 제 1 RGB 이미지를 획득하는 동작; 상기 제 1 RGB 이미지의 명도 정보에 기초하여, 상기 제 1 RGB 이미지 내에서 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예측하는 동작;상기 동적 비전 센서를 통해 획득되는 객체 정보에 기초하여, 상기 적어도 하나의 제 1 영역 중에서 객체가 존재하는 적어도 하나의 제 2 영역을 결정하는 동작; 상기 적어도 하나의 제 2 영역과 관련하여 상기 카메라의 촬영 설정 정보(photographic configuration공개특허 10-2020-0044182-4-information)를 제어함으로써, 개선된(enhanced) 제 2 RGB 이미지를 획득하는 동작; 및상기 제 2 RGB 이미지 내에서 상기 객체를 인식하는 동작을 수행하는, 자율 주행 장치."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12 항에 있어서, 상기 적어도 하나의 프로세서는, 상기 제 1 RGB 이미지 내에서 명도 값(brightness value)이 임계 범위를 벗어나는 영역을 상기 적어도 하나의제 1 영역으로 예측하는, 자율 주행 장치."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 12 항에 있어서, 상기 적어도 하나의 프로세서는,상기 제 1 RGB 이미지를 분석하여, 상기 제 1 RGB 이미지의 히스토그램을 획득하는 동작;상기 제 1 RGB 이미지의 히스토그램을 이용하여, 상기 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재하는지 판단하는 동작; 및상기 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재한다고 판단되는 경우, 상기 제 1 RGB 이미지의 명도 정보에 기초하여, 상기 제 1 RGB 이미지 내에서 상기 적어도 하나의 제 1 영역을 예측하는 동작을 수행하는,자율 주행 장치."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 12 항에 있어서, 상기 적어도 하나의 프로세서는 인공 지능 프로세서를 포함하고, 상기 인공 지능 프로세서는, 복수의 RGB 이미지를 학습한 제 1 인공 지능 모델을 이용하여, 상기 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재하는지 판단하고, 상기 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재한다고 판단되는 경우, 상기 제 1 인공 지능 모델을 이용하여 상기 제 1 RGB 이미지 내에서 상기 적어도하나의 제 1 영역을 예측하는, 자율 주행 장치."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 12 항에 있어서, 상기 적어도 하나의 프로세서는,상기 카메라의 센서 감도(GAIN), 조리개 및 노출 시간 중 적어도 하나를 조절함으로써, 상기 카메라의 촬영 설정 정보를 제어하는, 자율 주행 장치."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 12 항에 있어서, 상기 적어도 하나의 프로세서는, 상기 카메라를 이용하여, 상기 인식된 객체를 추적하는 동작;상기 동적 비전 센서를 통해, 상기 자율 주행 장치 주변에 나타나는 새로운 객체를 감지하는 동작; 상기 새로운 객체가 감지됨에 따라, 상기 카메라를 통해 획득되는 제 3 RGB 이미지 내에서 상기 새로운 객체가인식될 가능성이 임계 값보다 큰 후보 영역을 결정하는 동작; 및상기 후보 영역에 대한 영상 처리를 통해 상기 제 3 RGB 이미지에서 상기 새로운 객체를 인식하는 동작을 더 수행하는, 자율 주행 장치."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 12 항에 있어서, 상기 적어도 하나의 프로세서는, 상기 동적 비전 센서의 프레임 레이트를 상기 카메라의 프레임 레이트와 동일하게 설정하는, 자율 주행 장치."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "공개특허 10-2020-0044182-5-제 12 항에 있어서, 상기 자율 주행 장치는, 자율 주행 차량, 자율 비행 장치 및 자율 주행 로봇 중 적어도 하나를 포함하는, 자율 주행 장치."}
{"patent_id": "10-2018-0119303", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "카메라를 이용하여 제 1 RGB 이미지를 획득하는 동작; 상기 제 1 RGB 이미지의 명도 정보에 기초하여, 상기 제 1 RGB 이미지 내에서 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예측하는 동작;동적 비전 센서(DVS: Dynamic Vision Sensor)를 통해 획득되는 객체 정보에 기초하여, 상기 적어도 하나의 제 1영역 중에서 객체가 존재하는 적어도 하나의 제 2 영역을 결정하는 동작; 상기 적어도 하나의 제 2 영역과 관련하여 상기 카메라의 촬영 설정 정보(photographic configurationinformation)를 제어함으로써, 개선된(enhanced) 제 2 RGB 이미지를 획득하는 동작; 및상기 제 2 RGB 이미지 내에서 상기 객체를 인식하는 동작을 수행하도록 하는 프로그램이 저장된 기록매체를 포함하는 컴퓨터 프로그램 제품."}
{"patent_id": "10-2018-0119303", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "카메라를 이용하여 제 1 RGB 이미지를 획득하는 단계; 제 1 RGB 이미지의 명도 정보에 기초하여, 제 1 RGB 이미 지 내에서 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예측하는 단계; 동적 비전 센서를 통해 획득되는 객 체 정보에 기초하여, 적어도 하나의 제 1 영역 중에서 객체가 존재하는 적어도 하나의 제 2 영역을 결정하는 단 계; 적어도 하나의 제 2 영역과 관련하여 카메라의 촬영 설정 정보를 제어함으로써, 개선된 제 2 RGB 이미지를 획득하는 단계; 제 2 RGB 이미지 내에서 객체를 인식하는 단계를 포함하는 객체 인식 방법을 개시한다."}
{"patent_id": "10-2018-0119303", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 자율 주행 중에 주변의 객체를 인식하는 방법 및 이를 위한 자율 주행 장치에 관한 것이다."}
{"patent_id": "10-2018-0119303", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "자율주행자동차에 대한 관심이 커지면서 자율주행을 가능하게 해주는 기술들이 주목을 받고 있다. 운전자가 조 작하지 않고 자동차가 스스로 움직이기 위해서는 ① 자동차 외부 환경을 인식하는 기술, ② 인식된 정보를 종합 해 가속, 정지, 선회 등의 동작을 결정하고 주행 경로를 판단하는 기술, ③ 판단된 정보를 이용해 자동차의 움 직임을 제어하는 기술 등이 필요하다. 자율주행이 되기 위해서는 모든 기술들이 유기적으로 결합돼야겠지만, 자 동차의 외부 환경을 인식하는 기술이 점점 중요해지고 있다. 외부 환경을 인식하는 것이 자율주행의 첫 번째 요 소일 뿐만 아니라, 외부 환경을 인식하기 위해서는 전기전자, IT기술과의 융합이 필요하기 때문이다. 외부 환경을 인식하는 기술은 크게 센서기반 인식기술과 연결기반 인식기술 두 가지로 분류될 수 있다. 자율주 행을 위해 차량에 장착되는 센서는 초음파(Ultrasonic), 카메라(Camera), 레이다(Radar), 라이다(Lidar) 등이 있는데, 이들 센서들은 자동차에 장착되어 단독으로 또는 다른 센서와 같이 자동차의 외부 환경 및 지형을 인식 하여 운전자와 차량에 정보를 제공한다. 자율주행을 위한 연결기반 인식기술은 V2X와 정밀 측위 기술이 있다. V2X는 Vehicle to Something의 약자로, 자 동차간 통신하는 V2V(Vehicle to Vehicle), 인프라와 통신하는 V2I(Vehicle to Infrastructure), 보행자와 통 신하는 V2P(Vehicle to Pedestrian) 등이 있다. V2X는 운행중인 자동차와 주위의 자동차, 교통 인프라, 보행자 들을 연결해주는 무선 통신 기술이다. 연결된 통신망을 통해 자동차간 위치, 거리, 속도 등의 정보를 주고 받을 수 있고, 주변의 교통 정보 및 보행자의 위치 등의 정보를 자동차에게 제공할 수 있다. 일 실시예는, 동적 비전 센서에서 감지되는 객체 정보에 기반하여, 카메라의 촬영 설정 정보를 조절함으로써, 카메라를 통한 외부 객체의 인식률을 높이는 자율 주행 장치의 객체 인식 방법에 관한 것이다. 일 실시예에 따른 자율 주행 장치가 객체를 인식하는 방법은, 자율 주행 장치에 배치된 카메라를 이용하여 제 1 RGB 이미지를 획득하는 단계; 제 1 RGB 이미지의 명도 정보에 기초하여, 제 1 RGB 이미지 내에서 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예측하는 단계; 자율 주행 장치에 배치된 동적 비전 센서(DVS: Dynamic Vision Sensor)를 통해 획득되는 객체 정보에 기초하여, 적어도 하나의 제 1 영역 중에서 객체가 존재하는 적어 도 하나의 제 2 영역을 결정하는 단계; 적어도 하나의 제 2 영역과 관련하여 카메라의 촬영 설정 정보 (photographic configuration information)를 제어함으로써, 개선된(enhanced) 제 2 RGB 이미지를 획득하는 단 계; 및 제 2 RGB 이미지 내에서 객체를 인식하는 단계를 포함할 수 있다.일 실시예에 따른 자율 주행 장치는, 카메라; 동적 비전 센서(DVS: Dynamic Vision Sensor); 및 적어도 하나의 프로세서를 포함하고, 적어도 하나의 프로세서는, 카메라를 통해 제 1 RGB 이미지를 획득하는 동작; 제 1 RGB 이미지의 명도 정보에 기초하여, 제 1 RGB 이미지 내에서 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예 측하는 동작; 동적 비전 센서를 통해 획득되는 객체 정보에 기초하여, 적어도 하나의 제 1 영역 중에서 객체가 존재하는 적어도 하나의 제 2 영역을 결정하는 동작; 적어도 하나의 제 2 영역과 관련하여 카메라의 촬영 설정 정보(photographic configuration information)를 제어함으로써, 개선된(enhanced) 제 2 RGB 이미지를 획득하 는 동작; 및 제 2 RGB 이미지 내에서 상기 객체를 인식하는 동작을 수행할 수 있다. 일 실시예에 따른 컴퓨터 프로그램 제품은 카메라를 이용하여 제 1 RGB 이미지를 획득하는 동작; 제 1 RGB 이미 지의 명도 정보에 기초하여, 제 1 RGB 이미지 내에서 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예측하 는 동작; 동적 비전 센서(DVS: Dynamic Vision Sensor)를 통해 획득되는 객체 정보에 기초하여, 적어도 하나의 제 1 영역 중에서 객체가 존재하는 적어도 하나의 제 2 영역을 결정하는 동작; 적어도 하나의 제 2 영역과 관련 하여 카메라의 촬영 설정 정보(photographic configuration information)를 제어함으로써, 개선된(enhanced) 제 2 RGB 이미지를 획득하는 동작; 및 제 2 RGB 이미지 내에서 상기 객체를 인식하는 동작을 수행하도록 하는 프로그램을 저장할 수 있다."}
{"patent_id": "10-2018-0119303", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 발명에 대해 구체적으로 설명하기로 한다. 본 발명에서 사용되는 용어는 본 발명에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상세 히 그 의미를 기재할 것이다. 따라서 본 발명에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지 는 의미와 본 발명의 전반에 걸친 내용을 토대로 정의되어야 한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"...부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 아래에서는 첨부한 도면을 참고하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설 명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 도 1은 일 실시예에 따른 자율 주행 장치를 설명하기 위한 도면이다. 일 실시예에 따른 자율 주행 장치는 외부로부터 입력되는 제어 명령에 의존하지 않고 자율 주행할 수 있는 장치로서, 예를 들어, 자율 주행 차량, 자율 비행 장치(예컨대, 드론, 무인 항공기), 자율 주행 로봇(예컨대, 청소 로봇, 재난 구조 로봇 등) 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 이하에서는 설명의 편의상 자율 주행 장치가 자율 주행 차량인 경우를 예로 들어 설명하기로 한다. 일 실시예에 의하면, 자율 주행 장치는, 카메라, 동적 비전 센서, 프로세서를 포함할 수 있으나, 이에 한정되는 것은 아니다. 예를 들어, 자율 주행 장치는 라이다 센서, 레이더 센서, 관성 센서 (IMU), 초음파 센서, 적외선 센서, 위치 센서(예컨대, GPS 모듈), 지자기 센서, 가속도 센서, 자이로스코프 센 서 등을 더 포함할 수 있다. 다른 실시예에 의하면, 자율 주행 장치는 통신부(예컨대, 블루투스 통신부, BLE 통신부, NFC 통신부, 지그비 통신부, UWB 통신부, 이동 통신부), 구동부(전원공급부, 추진부, 주행부, 주변 장치부), 출력부, 저장부 등을 더 포함할 수 있다. 자율 주행 장치의 구성에 대해서는 도 17을 참조하여 후에 자세히 살펴보기로 한다. 일 실시예에 따른 카메라는, 자율 주행 장치로부터 소정 거리 내에 존재하는 적어도 하나의 객체를 인식할 수 있다. 이때, 객체를 인식하기 위한 카메라는 한 개일 수도 있고 복수 개일 수도 있다. 예를 들 어, 카메라는 전방 카메라, 후방 카메라 및 측면 카메라 중 적어도 하나일 수 있으며, 카메라는 스테 레오 카메라 또는 어라운드 뷰 카메라일 수도 있다. 한편, 카메라를 통해 인식되는 객체는 정적(靜的) 환경 요소(예컨대, 차선, 운전가능도로, 교통표지판, 신 호등, 터널, 다리, 가로수 등)와 동적(動的) 환경 요소(예컨대, 차량, 보행자, 이륜차 등)를 포함할 수 있으나, 이에 한정되는 것은 아니다. 예를 들어, 카메라를 통해 인식되는 객체는, 위치 인식 기술 (예컨대, SLAM(Simultaneous localization and mapping) 또는 VIO(Visual Inertial Odometry))에 적용되는 특징 (feature)(예컨대, 특징 점, 특징 선)을 포함할 수 있다. 하지만, 일반적인 카메라의 다이나믹 레인지는 높지 않으므로, 카메라는 아주 어두운 곳이나 아주 밝 은 곳에서의 객체를 인식하기 어렵다. 예를 들어, 자율 주행 장치에 배치된 카메라는 터널 진입 시 (100-1), 터널 안의 어두운 영역에 존재하는 객체를 감지하기 어렵다. 또한, 자율 주행 장치에 배치된 카메라는 터널 진출 시(100-2) 터널 밖의 밝은 영역에 존재하는 객체를 감지하기 어렵고, 역광(100- 3)에 의해 밝게 빛나는 영역에 존재하는 객체를 감지하기도 어렵다. 한편, 자율 주행 장치가 조명 변 화가 심한 지역을 지나거나, 음영지역을 통과하거나, 야간에 고속으로 이동하거나, 배경색과 객체 색이 비슷한 경우에도, 카메라는 객체를 정확히 인지하기 어렵다. 따라서, 자율 주행 장치의 안전한 주행을 위해, 저조도 환경 또는 역광에서 카메라의 객체 인식률을 높일 필요가 있다. 예를 들어, 일 실시예에 의하면, 자율 주행 장치는 동적 비전 센서에서 검출되는 정보를 이용하여, 카메라의 촬영 설정 정보를 제어함으로써, 카메라의 객체 인식률을 높일 수 있다. 동적 비전 센서는, 고속으로 비전 변화를 캡처하는 이벤트 기반 카메라로, 움직이는 객체에 대한 이미지 데이터를 획득할 수 있는 센서이다. 예를 들어, 동적 비전 센서는 픽셀 단위에서 움직임에 의해 국소적인 변화가 있을 때에만 이미지 데이터를 프로세서로 전송하게 된다. 즉, 동적 비전 센서는 움직이는 이 벤트가 발생할 때 이미지 데이터를 프로세서에 전송할 수 있다. 동적 비전 센서는 일반적인 시각 인식 시스템이 빠른 움직임에 취약하다는 문제점을 해결할 수 있다. 동적 비전 센서는 프레임 단위로 데이터를 받는 것이 아니라 낱낱의 픽셀 기준(per-pixel basis)으로 데이터를 받기 때문에 블러(blur) 현상을 극복할 수 있다. 또한, 동적 비전 센서는 마이크로 초 단위의 해상도를 가질 수 있다. 다시 말해 동적 비전 센서는 1 초당 수천 프레임을 찍는 초고속 카메라보다 더 뛰어난 시간 분해능을 가질 수 있다(예컨대, 초고속 프레임 > 1K FPS). 뿐만 아니라 동적 비전 센서는 전력 소모 및 데이터 저장 요구 조건 또한 매우 줄어들었기 때문 에 다이나믹 레인지(센서가 구분할 수 있는 밝기의 범위) 또한 획기적으로 늘게 되었다. 따라서, 동적 비전 센 서는 어두운 곳에서도 약간의 빛만 있으면 객체의 움직임을 감지할 수 있다. 일 실시예에 의하면, 동적 비전 센서는 카메라와 근접한 위치에 배치될 수 있다. 또한, 동적 비전 센 서가 카메라와 유사한 영역의 이미지를 획득하도록 동적 비전 센서의 FOV(Field of view) 또는 동적 비전 센서의 포즈가 조절될 수 있다. 한편, 일 실시예에 의하면, 동적 비전 센서의 프레임 레이 트는 카메라의 프레임 레이트와 동일하거나 유사하게 설정될 수 있으나, 이에 한정되는 것은 아니다. 일 실시예에 의하면, 자율 주행 장치에 배치된 동적 비전 센서는 자율 주행 장치가 주행하는 동 안 픽셀 단위에서의 국소적인 변화를 감지하여 프로세서로 감지된 정보를 전달해 줄 수 있다. 이 경우, 동 적 비전 센서는 카메라보다 다이나믹 레인지가 넓으므로, 프로세서는 카메라에서 검출되지 않은 객체에 대한 정보를 동적 비전 센서로부터 수신할 수 있다. 이 경우, 프로세서는 카메라에 서는 검출되지 않았지만 동적 비전 센서에서 검출된 객체를 카메라에서도 검출할 수 있도록 카메라 의 촬영 설정 정보를 제어할 수 있다. 일 실시예에 의하면, 프로세서는 일반적인 이미지 신호 프로세 서(ISP) 또는 인공지능 프로세서(AI 프로세서)를 포함할 수 있다. 이하에서는, 자율 주행 장치의 프로세서가 동적 비전 센서에서 검출되는 정보를 이용하여, 카메 라의 촬영 설정 정보를 제어함으로써, 카메라의 객체 인식률을 높이는 방법에 대해서 도 2를 참조하 여, 자세히 살펴보기로 한다. 도 2는 일 실시예에 따른 자율 주행 장치의 객체 인식 방법을 설명하기 위한 순서도이다. 단계 S210에서, 자율 주행 장치는, 카메라를 이용하여 제 1 RGB 이미지를 획득할 수 있다. 일 실시예에 의하면, 제 1 RGB 이미지는 자율 주행 장치 주변에 있는 적어도 하나의 객체를 인식하기 위한 이미지로서, 적어도 하나의 프레임으로 구성될 수 있다. 예를 들어, 제 1 RGB 이미지가 정지 영상인 경우 제 1 RGB 이미지는 하나의 프레임으로 구성될 수 있고, 제 1 RGB 이미지가 동영상인 경우, 제 1 RGB 이미지는 복수의 프레임으로 구성될 수 있다. 일 실시예에 의하면, 자율 주행 장치는 주행 중에 카메라를 이용하여 제 1 RGB 이미지를 획득할 수도 있고, 주차 또는 정차 중에 카메라를 이용하여 제 1 RGB 이미지를 획득할 수도 있다. 단계 S220에서, 자율 주행 장치는, 제 1 RGB 이미지의 명도 정보에 기초하여, 제 1 RGB 이미지 내에서 객 체 인식이 불가능한 적어도 하나의 제 1 영역을 예측할 수 있다. 여기서, 명도 정보는 제 1 RGB 이미지 내 각 픽셀의 밝기 정도를 나타내는 정보일 수 있다. 명도 정보는 각 픽셀의 밝기 값을 포함할 수도 있고, 제 1 기준 밝기 값보다 어두운 영역에 관한 정보, 제 2 기준 밝기 값보다 밝은 영역에 관한 정보 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 일 실시예에 의하면, 자율 주행 장치는 제 1 RGB 이미지에 객체 인식 불가능 영역이 존재할 확률이 임계 값을 초과하는지 판단할 수 있다. 객체 인식 불가능 영역은 명도 값이 임계 범위(예컨대, 50~200)를 벗어나는 영역(예컨대, 너무 어둡거나 너무 밝은 영역)을 의미할 수 있다. 예를 들어, 자율 주행 장치는 제 1 RGB 이미지의 히스토그램을 이용하여, 제 1 RGB 이미지에 객체 인식 불가능 영역이 존재할 확률을 결정할 수 있다.제 1 RGB 이미지의 히스토그램 분포가 0 또는 255에 치우치는 경우, 자율 주행 장치는 제 1 RGB 이미지에 객체 인식 불가능 영역이 존재할 확률이 높다고 판단할 수 있다. 자율 주행 장치가 히스토그램을 이용하는 동작에 대해서는 도 5를 참조하여 후에 좀 더 자세히 살펴보기로 한다. 일 실시예에 의하면, 자율 주행 장치는, 제 1 RGB 이미지와 동적 비전 센서를 통해 획득되는 DVS 이 미지의 차이에 기초하여, 제 1 RGB 이미지에 객체 인식 불가능 영역이 존재할 확률을 결정할 수도 있다. 예를 들어, DVS 이미지에서 검출되는 에지 정보(또는 강도(intensity) 정보)와 제 1 RGB 이미지에서 검출되는 에지 정보(또는 강도 정보)의 차이가 클수록, 자율 주행 장치는 제 1 RGB 이미지에 객체 인식 불가능 영역이 존 재할 확률이 높다고 판단할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, 복수의 RGB 이미지를 학습한 인공 지능 모델을 이용하여, 제 1 RGB 이미지에 객체 인식 불가능 영역이 존재할 확률을 결정할 수도 있다. 자율 주행 장치가 인공 지능 모 델을 이용하는 동작에 대해서는 도 8을 참조하여 후에 자세히 살펴보기로 한다. 자율 주행 장치는, 제 1 RGB 이미지에 객체 인식 불가능 영역이 존재할 확률이 임계 값보다 크다고 판단된 경우, 제 1 RGB 이미지의 명도 정보를 이용하여, 제 1 RGB 이미지 내에서 객체 인식이 불가능한 적어도 하나의 제 1 영역을 예측할 수 있다. 여기서, 객체 인식이 불가능한 적어도 하나의 제 1 영역은, 객체 인식이 불가능한 정도가 임계 값을 초과하는 영역일 수 있다. 예를 들어, 자율 주행 장치는, 제 1 RGB 이미지 내에서 명도 값(brightness value)이 임계 범위를 벗어나는 영역(예컨대, 너무 어두운 영역 또는 너무 밝은 영역)을 객체 인 식이 불가능한 적어도 하나의 제 1 영역으로 예측할 수 있다. 따라서, 객체 인식이 불가능한 적어도 하나의 제 1 영역에서는 특징(feature)이 거의 검출되지 않을 수 있다. 이하에서는, 설명의 편의상 객체 인식이 불가능한 적어도 하나의 제 1 영역을 객체 인식 불가능 영역이라 표현할 수도 있다. 단계 S230에서, 자율 주행 장치는, 자율 주행 장치에 배치된 동적 비전 센서를 통해 획득되는 객체 정보에 기초하여, 적어도 하나의 제 1 영역 중에서 객체가 존재하는 적어도 하나의 제 2 영역을 결정할 수 있다. 이하에서는 설명의 편의상, 적어도 하나의 제 2 영역을 관심 영역으로 표현할 수도 있다. 일 실시예에 의하면, 자율 주행 장치는 동적 비전 센서에서 통해 객체 정보를 획득할 수 있다. 여기 서, 객체 정보는 동적 비전 센서에서 감지되는 객체에 관한 정보로서, DVS 이미지 및 DVS 이미지에서 검출 되는 적어도 하나의 객체에 대한 위치 정보 중 적어도 하나를 포함할 수 있으나, 이에 한정되는 것은 아니다. 일 실시예에 의하면, 자율 주행 장치는, DVS 이미지와 제 1 RGB 이미지를 비교하여, 제 1 RGB 이미지의 적 어도 하나의 제 1 영역 중에서 객체가 존재할 확률이 임계 값(예컨대, 98%)보다 큰 영역을 관심 영역으로 결정 할 수 있다. 예를 들어, 관심 영역은 DVS 이미지 상에서는 특징 정보가 많이 나타나지만 제 1 RGB 이미지에서는 특징 정보가 많이 나타나지 않는 영역일 수 있다. 예를 들어, 도 3을 참조하여, 객체 인식 불가능 영역 및 관심 영역에 대해서 살펴보기로 한다. 도 3의 300-1을 참조하면, 자율 주행 장치는, 터널을 통과하면서 RGB 이미지를 획득할 수 있다. 이때, RGB 이미지 상에서 터널 출구 쪽은 밝게 나타나고 터널 안 쪽은 어둡게 나타날 수 있다. 자율 주행 장치는, RGB 이미지의 히스토그램 또는 인공 지능 모델을 이용하여, RGB 이미지를 분석할 수 있다. 자율 주행 장 치는, 분석 결과, 명도 값이 임계 범위를 벗어나는 제 1-1 영역, 제 1-2 영역, 제 1-3 영역 , 제 1-4 영역, 제 1-5 영역(터널 출구 영역), 제 1-6 영역, 제 1-7 영역을 객체 인 식 불가능 영역들로 결정할 수 있다. 이때, 제 1-1 영역, 제 1-2 영역, 제 1-3 영역, 제 1-6 영역, 제 1-7 영역은 터널 안 영역으로 매우 어두운 영역일 수 있으며, 제 1-4 영역, 제 1-5 영 역은 터널 출구에서 들어오는 빛 때문에 매우 밝게 나타나는 영역일 수 있다. 도 3의 300-2를 참조하면, 자율 주행 장치는 RGB 이미지의 제 1-1 영역 내지 제 1-7 영역(31 7)을 DVS 이미지상의 대응 영역과 각각 비교할 수 있다. 이때, 터널 출구 영역은 RGB 이미지 상 에서는 밝아서 객체가 검출되지 않았는데, DVS 이미지의 대응 영역에서는 객체가 검출될 수 있다. 동적 비 전 센서가 카메라보다 다이나믹 레인지가 넓으므로, 동적 비전 센서는 밝은 영역에서의 객체도 감지할 수 있다. 따라서, 자율 주행 장치는 현재 카메라를 통해서는 객체 인식이 불가능하지만 동적 비전 센서를 통해 객체가 감지된 영역(예컨대, 터널 출구 영역)을 관심 영역으로 결정할 수 있다. 단계 S240에서, 자율 주행 장치는, 적어도 하나의 제 2 영역(설명의 편의상, 관심 영역이라 함)과 관련하 여, 카메라의 촬영 설정 정보(photographic configuration information)를 제어함으로써, 개선된(enhanced) 제 2 RGB 이미지를 획득할 수 있다. 여기서, 카메라의 촬영 설정 정보는, 노출 정보, 포커스 정보, 화이트 밸러스 정보, 모드 정보 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 또한, 개선된 제 2 RGB 이미지는, 제 1 RGB 이미지의 제 2 영역에 대응하는 영역에서 객체 검출이 가능하도록 명도가 변경된 이미 지를 의미할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, 카메라의 현재 촬영 설정 정보를 확인하고, 관심 영역의 명 도가 변경될 수 있도록 촬영 설정 정보를 제어할 수 있다. 예를 들어, 자율 주행 장치는, 관심 영역에 대 한 노출, 포커스 및 화이트 밸런스 중 적어도 하나를 제어할 수 있다. 특히, 자율 주행 장치는, 카메라의 센서 감도(GAIN), 조리개 및 노출 시간 중 적어도 하나를 조절함으로써, 관심 영역에 대한 노출 값을 제어할 수 있다. 예컨대, 관심 영역이 어두운 영역인 경우, 자율 주행 장치는, 센서 감도(Gain), 조리개, 노출 시간 을 적절히 조절하여, 개선된 제 2 RGB 이미지에서는 관심 영역이 밝게 나타나도록 제어할 수 있다. 반대로 관심 영역이 밝은 영역인 경우, 자율 주행 장치는, 센서 감도(Gain), 조리개, 노출 시간을 적절히 조절하여, 개 선된 제 2 RGB 이미지에서는 관심 영역이 어둡게 나타나도록 제어할 수 있다. 도 4를 참조하면, 조리개는 빛이 렌즈를 통과하는 구멍의 크기를 의미한다. 조리개는 조일수록(오른 쪽) 심도가 깊어져서 근경부터 원경까지 초첨이 맞는 이미지가 나오고, 개방할수록(왼쪽) 심도가 얕아져서 일명 아웃포커싱이라 부르는, 피사체와 배경이 분리되는 이미지가 나온다. 셔터 속도(노출 시간)는 빠를수록(왼 쪽) 빨리 움직이는 대상을 정지시키는 이미지가 나오고, 느릴수록(오른쪽) 블러 처리된 이미지가 나온다. ISO 감도(센서 감도)는 낮을수록(왼쪽) 노이즈가 적은 이미지가 나온다. ISO 감도는 높을수록 (오른쪽) 노이즈가 많아지며, 어두운 환경에서도 흔들림 없는 이미지를 찍을 수 있게 해준다. ISO 감도는 낮을수록(왼쪽) 명암대비가 크게 표현된다. 반대로 ISO 감도는 높을수록 명암대비 차를 포용하여 부드러운 이미지를 만든다. 그리고 낮은 ISO 감도의 필름입자는 가늘어서 선명한 이미 지를 만들고, 높은 ISO 감도의 필름입자는 굵어서 거친 이미지가 나온다. 따라서, 일 실시예에 의하면, 자율 주행 장치는 관심 영역이 어두운 경우, 카메라의 감도를 높 이거나, 셔터 속도를 느리게 제어할 수 있다. 반면에, 자율 주행 장치는 관심 영역이 밝은 경우, 카 메라의 감도를 낮출 수 있다. 한편, 일 실시예에 의하면, 관심 영역이 역광에 의해 밝게 빛나는 영역인 경우, 자율 주행 장치는, 측광 모드를 변경(예컨대, 평가 측광(Evaluative metering), 부분측광(partial metering), 중앙부 중점 평균 측광 (Center weighted average metering, 스팟 측광(spot metering) 중 하나로 변경)하거나, 측광을 위한 측거점 (AF Point)을 변경할 수 있다. 예를 들어, 자율 주행 장치는 관심 영역이 밝은 영역인 경우, 측거점을 관 심 영역으로 이동시켜, 전체적으로 어두운 제 2 RGB 이미지를 획득할 수 있다. 또한, 일 실시예에 의하면, 자율 주행 장치는, 광역 역광 보정(WDR: wide dynamic range) 기능을 선택할 수도 있다. 광역 역광 보정(WDR: wide dynamic range)은 밝은 곳과 어두운 곳의 이미지를 동시에 잘 볼 수 있게 구현해 주는 기술이다. 밝은 영역에서 고속 셔터 영상신호와 어두운 영역에서의 저속 셔터 영상 신호를 하나로 모아 영상화함으로써 역광의 문제점을 해결하여 선명한 영상을 구현하는 기능이다. 일 실시예에 의하면, 자율 주행 장치의 AI 프로세서가 촬영 설정 정보를 제어하기 위해 학습된 인공 지능 모델을 이용하여, 카메라의 촬영 설정 정보를 제어할 수도 있다. 자율 주행 장치가 인공 지능 모델을 이용하여, 카메라의 촬영 설정 정보를 제어하는 동작에 대해서는 도 8을 참조하여 후에 자세히 살펴보기로 한다. 단계 S250에서, 자율 주행 장치는, 제 2 RGB 이미지 내에서 객체를 인식할 수 있다. 일 실시예에 의하면, 자율 주행 장치는 제 2 RGB 이미지의 관심 영역에서 객체를 구성하는 적어도 하나의 특징(feature)를 추출할 수 있다. 제 2 RGB 이미지의 관심 영역은 제 1 RGB 이미지의 관심 영역에 대응하는 영 역일 수 있다. 자율 주행 장치는 추출된 적어도 하나의 특징을 이용하여, 제 2 RGB 이미지의 관심 영역에 서 객체를 인식할 수 있다. 일 실시예에 의하면, 객체를 인식하는 것은 객체의 종류를 결정하는 것을 포함할 수 있다. 일 실시예에 의하면, 자율 주행 장치는 템플릿 정보 또는 인공 지능 모델을 이용하여, 제 1 RGB 이미지의 관심 영역에 대응하는 제 2 RGB 이미지의 관심 영역에서 객체를 인식할 수 있다. 예를 들어, 자율 주행 장치 는, 카메라를 통해 획득되는 제 2 RGB 이미지를 분석하여, 객체의 종류를 결정할 수 있다. 예컨대, 객체가 외부 차량인 경우, 자율 주행 장치는, 제 2 RGB 이미지에 포함된 외부 차량의 외곽선을 특징(feature)으로 검출할 수 있다. 자율 주행 장치는, 검출된 외부 차량의 외곽선을 기 정의된 템플릿 (template)과 비교하여, 외부 차량의 종류, 외부 차량의 명칭 등을 검출할 수 있다. 예를 들어, 자율 주행 장치 는, 외부 차량의 외곽선이 버스의 템플릿과 유사한 경우, 외부 차량을 버스로 인식할 수 있다. 또한, 자율 주행 장치는 버스의 경우 크기가 크고, 무게가 많이 나가므로, 외부 차량의 종류를 대형 차량으로 결정할 수도 있다. 일 실시예에 의하면, 자율 주행 장치는, 정밀 지도를 이용하여, 제 2 RGB 이미지의 관심 영역에서 객체를 인식할 수도 있다. 여기서, 정밀 지도는 차량이 주행하는데 필요한 도로 정보를 포함할 뿐 아니라 기존 지도보 다 10배 이상 정밀해 실제 도로와 10~20cm 이하의 오차를 갖는 지도를 의미할 수 있다. 예를 들어, 자율 주행 장치는 주변의 정밀 지도를 호출할 수 있다. 그리고 자율 주행 장치는 제 2 RGB 이미지를 호출된 정 밀 지도와 비교하여, 제 2 RGB 이미지의 관심 영역에서 정적인 객체를 인식할 수 있다. 예를 들어, 자율 주행 장치는 제 2 RGB 이미지에서 추출된 특징들(feature)을 정밀 지도와 비교하여, 객체가 차선, 정지선, 도로 표식, 도로 구조물 등이라는 것을 인식할 수 있다. 한편, 자율 주행 장치는, 정밀 지도를 이용하여, 인식된 객체의 현재 위치(예컨대, 절대적 위치), 인식된 객체가 외부 차량인 경우 외부 차량이 주행 중인 주행 차선(예컨대, 1차선) 등의 식별할 수도 있다. 일 실시예에 의하면, 자율 주행 장치는, 인식된 객체가 동적인 객체(예컨대, 외부 차량)인 경우, 카메라 를 통해 인식된 객체를 추적할 수 있다. 객체 추적(object tracking)은 일련의 영상 프레임 내 객체의 크 기, 색, 모양, 윤곽선 등 특징적인 정보 간의 유사도를 이용하여 객체의 변화를 추적하는 것을 의미한다. 일 실시예에 의하면, 동적 비전 센서는 자율 주행 장치 주변에 나타나는 새로운 객체를 카메라 보다 먼저 감지할 수 있다. 따라서, 일 실시예에 의하면, 자율 주행 장치는 동적 비전 센서를 통해 새로운 객체가 감지된 경우, 새로운 객체가 감지된 위치에 기초하여, 카메라의 RGB 이미지 상에서 새로운 객체가 인식될 가능성이 임계 값보다 큰 후보 영역을 결정할 수 있다. 그리고 자율 주행 장치는 후보 영역 에 대한 영상 처리를 통해 RGB 이미지 상에서 새로운 객체를 인식하고, 추적할 수 있다. 이 경우, 자율 주행 장 치는 카메라를 통해 새로운 객체를 인식하기 위해 RGB 이미지의 전 영역에 대한 영상 처리를 하지 않 고, 후보 영역에 대해서만 영상 처리를 하므로, 빠르게 새로운 객체를 인식할 수 있다. 자율 주행 장치가 객체를 인식하고 추적하는 동작에 대해서는 도 15를 참조하여 후에 좀 더 자세히 살펴보기로 한다. 일 실시예에 의하면, 자율 주행 장치는 제 2 RGB 이미지가 복수의 프레임으로 구성되는 경우, 복수의 프레 임 각각에서 인식되는 객체에 포함된 특징(feature)을 추적(tracking)하여, 자율 주행 장치의 위치 정보를 획득할 수도 있다. 예를 들어, 자율 주행 장치는, 제 2 RGB 이미지에서 인식된 객체 포함된 특징(featur e)을 비주얼 오도메트리(예컨대, VIO 또는 스테레오 카메라를 이용한 비주얼 오도메트리 등)에 적용하기 위한 특징(feature)으로 이용할 수 있다. 여기서, 비주얼 오도메트리는 이전 프레임과 현재 프레임의 차이를 이용하 여 이동 장치의 위치변화를 예측하는 기술이다. 일 실시예에 의하면, 모든 픽셀에 대해서 이전 프레임과 현재 프레임의 변화를 계산하는 것은 매우 높은 연산량 이 필요하기 때문에, 자율 주행 장치는 화면 변화를 대표할 수 있는 선이나 코너와 같은 특징을 각 프레임 에서 추출하고, 추출된 특징을 정합할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, 이전 프레임에서 추출된 특징점을 현재 프레임에서 정합시킴으로 써, 화면에서의 특징점의 위치 변화를 예측할 수 있는 모션벡터를 생성할 수 있다. 이 모션벡터는 2차원 공간 (x, y)에서의 영상 변화이므로, 자율 주행 장치는, 스테레오 카메라의 거리정보(depth) 또는 관성 센서 (IMU)의 거리 정보를 첨가하여, 모션 벡터를 3차원 공간(x, y, z)의 좌표로 변환할 수 있다. 자율 주행 장치 는, 정합된 특징점 집합에서 이전 프레임의 특징점에 해당하는 3차원 좌표와 현재 프레임에서 대응하는 특 징점의 3차원 좌표를 이용하여, 실제 공간의 변화량을 의미하는 3차원 모션벡터를 계산할 수 있다. 자율 주행 장치는, 3차원 모션벡터를 이용하여, 자율 주행 장치의 현재 위치를 인식할 수 있다. 실외환경의 경우, 바닥의 재질이 균일하지 않을 뿐만 아니라, 평평하지도 않기 때문에 엔코더를 이용한 위치인 식은 사용하기 어렵고, GPS(Global Positioning System)의 경우 터널과 같은 인공구조물이나 건물에 둘러싸인 환경에서는 신호를 받지 못하는 단점이 있고, 6자유도 INS(Inertial Navigation System)는 매우 고가라서 실용 적으로 사용하기에는 어려움이 있다. 따라서, 일 실시예에 의하면, 제 2 RGB 이미지에서 추출되는 특징 (feature)를 이용하여, 자율 주행 장치의 위치를 인식함으로써, GPS와 INS의 단점을 보완할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, 비주얼 오도메트리를 통해 인식된 위치 정보에 기초하여 지도를 생성할 수도 있다. 일 실시예에 의하면, 자율 주행 장치는, 제 2 RGB 이미지에서 인식된 객체에 관한 정보에 기초하여, 자율 주행 장치의 경로를 결정할 수 있다. 예를 들어, 자율 주행 장치는, 제 2 RGB 이미지에서 인식된 객 체가 장애물인 경우, 장애물을 피하도록 움직임을 계획할 수 있다. 예컨대, 자율 주행 장치는 차선을 변경 하거나, 속도를 줄일 수 있다. 또한, 제 2 RGB 이미지에서 인식된 객체가 정차 신호등인 경우, 자율 주행 장치 는, 정지선 앞에서 정차할 수 있도록 경로를 계획할 수 있다. 따라서, 일 실시예에 의하면, 자율 주행 장치는, 조명 변화가 심한 환경에서도 동적 비전 센서에서 감지되 는 정보에 기초하여 카메라의 촬영 설정 정보를 제어함으로써, 카메라의 객체 인식률을 높일 수 있다. 또한, 카메라의 객체 인식률이 높아짐에 따라, 현재 위치 인식률, 경로 계획의 정밀성, 객체 추적률 도 향상될 수 있다. 이하에서는, 자율 주행 장치가 히스토그램을 이용하는 동작에 대해서 도 5를 참조하여 자세히 살펴보기로 한다. 도 5는 일 실시예에 따른 히스토그램을 이용하여 객체를 인식하는 방법을 설명하기 위한 순서도이다. 단계 S500에서, 자율 주행 장치는, 카메라를 이용하여 제 1 RGB 이미지를 획득할 수 있다. 단계 S500은 도 2의 단계 S210에 대응되므로, 구체적인 설명은 생략하기로 한다. 단계 S510에서, 자율 주행 장치는, 제 1 RGB 이미지를 분석하여, 제 1 RGB 이미지의 히스토그램을 획득할 수 있다. 히스토그램은 이미지의 밝기의 분포를 그래프로 표현한 방식이다. 예를 들어, 히스토그램은 0~255의 명암 값의 범위를 가지고 있으며, 각 명암 값의 빈도 수(픽셀 수)를 높이로 나타낼 수 있다. 즉, 히스토그램의 수평축은 명도 값으로 나타나고, 히스토그램의 수직 축은 픽셀의 수로 나타날 수 있다. 따라서, 제 1 RGB 이미 지가 전체적으로 어두운 경우, 0 쪽에 치우친 히스토그램이 획득될 수 있으며, 제 1 RGB 이미지가 전체적으로 밝은 경우 255에 치우친 히스토그램이 획득될 수 있다. 단계 S520에서, 자율 주행 장치는, 제 1 RGB 이미지의 히스토그램을 이용하여, 제 1 RGB 이미지 내에서 객 체 인식 불가능 영역이 존재하는지 판단할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, 히스토그램의 분포가 고르지 않고, 좌측으로 치우치거나 우측으로 치우치는 경우, 객체 인식 불가능 영역이 존재한다고 판단할 수 있다. 예를 들어, 도 6을 참조하면, 제 1 히스토그램의 경우, 명도 분포가 고르지 않고, 좌측과 우측에 치우친 것을 알 수 있다. 이 경우, 자율 주행 장치는, 제 1 RGB 이미지는 아주 밝게 나타나는 영역과 아주 어둡게 나타나는 영역으로 대부분 구성될 수 있으므로, 제 1 RGB 이미지에는 객체 인식 불가능 영역이 존재할 확률이 높다고 판단할 수 있다. 반면에 제 2 히스토그램의 경우, 명도 분포가 0부터 255까지 고르게 나타나므로, 자율 주행 장치는 제 1 RGB 이미지에는 객체 인식 불가능 영역이 존재할 확률이 낮다고 판단할 수 있다. 단계 S530에서, 자율 주행 장치는, 객체 인식 불가능 영역이 존재하지 않는 경우, 카메라의 촬영 설 정 정보를 변경하지 않을 수 있다. 즉, 자율 주행 장치는 카메라의 현재 촬영 설정 정보에 기초하여 계속 RGB 이미지를 획득할 수 있다. 단계 S535에서, 자율 주행 장치는 제 1 RGB 이미지에 객체 인식 불가능 영 역이 존재하지 않으므로, 제 1 RGB 이미지에서 객체를 인식할 수 있다. 단계 S540에서, 자율 주행 장치는, 객체 인식 불가능 영역이 존재하는 경우, 제 1 RGB 이미지 내에서 객체 인식이 불가능한 적어도 하나의 제 1 영역(객체 인식 불가능 영역)을 예측할 수 있다. 여기서 객체 인식이 불가 능한 적어도 하나의 제 1 영역은 명도 값이 임계 범위를 벗어나는 영역일 수 있다. 제 1 RGB 이미지가 히스토그램으로 변환될 때 제1 RGB 이미지의 모든 공간적인 정보는 사라진다. 즉, 히스토그 램은 각 명도 값을 가지고 있는 픽셀들의 개수를 나타내지만, 그 픽셀들이 어디에 위치하는지에 대해서는 전혀 정보를 주지 않는다. 따라서, 자율 주행 장치는, 제 1 RGB 이미지 상에서 객체 인식 불가능 영역을 구분하 기 위해, 히스토그램을 이용하여, 임계 값을 결정할 수 있다. 예를 들어, 자율 주행 장치는, 히스토그램을 분석하여, 너무 어두워서 객체 인식이 불가능한 영역을 검출하기 위한 제 1 기준 값을 결정하거나, 너무 밝아서 객체 인식이 불가능한 영역을 검출하기 위한 제 2 기준 값을 결정할 수 있다. 예를 들어, 도 7을 참조하면, 제 1 RGB 이미지의 히스토그램은 0~70사이의 명도 값을 갖는 픽셀들과 240~250 사이의 명도 값을 갖는 픽셀들로 구분될 수 있다. 따라서, 자율 주행 장치는 제 1 RGB 이미지 상에서 어두워서 객체 인식이 불가능한 영역을 결정하기 위한 제 1 기준 값으로 '80'을 결정할 수 있다. 이때, 자율 주행 장치는 명도 값이 80보다 작은 값을 갖는 픽셀은 '1'로 표현하고, 명도 값이 80이 상인 픽셀은 '0'으로 표현함으로써, 어두워서 객체 인식이 불가능한 영역을 결정할 수 있다. 또한, 자율 주행 장치는 제 1 RGB 이미지 상에서 밝아서 객체 인식이 불가능한 영역을 결정하기 위한 제 2 기준 값으로 '230'을 결정할 수 있다. 이때, 자율 주행 장치는 명도 값이 230보다 큰 값을 갖는 픽셀은 '1'로 표현하고, 명도 값이 230이하인 픽셀은 '0'으로 표현함으로써, 밝아서 객체 인식이 불가능한 영역을 결정할 수 있다. 한편, 도 7에서는 제 1 기준 값과 제 2 기준 값이 다른 경우를 예로 들어 설명하였으나, 이에 한정되 는 것은 아니다. 일 실시예에 의하면, 제 1 기준 값과 제 2 기준 값은 같은 값일 수 있다. 예컨대, 자율 주행 장치는 제 1 기준 값과 제 2 기준 값을 모두 '150'으로 결정할 수 있다. 이 경우, 자 율 주행 장치는, 어두운 영역을 검출하기 위해서는 150보다 작은 명도 값을 가지는 픽셀을 1로 표현 하고, 밝은 영역을 검출하기 위해서는 150보다 큰 명도 값을 가지는 픽셀을 1로 표현할 수 있다. 단계 S550에서, 자율 주행 장치는, 동적 비전 센서를 통해 획득되는 DVS 이미지의 객체 정보와 제 1 RGB 이미지의 객체 정보를 비교하여, 적어도 하나의 제 1 영역 중에서 객체가 존재하는 적어도 하나의 제 2 영 역(관심 영역)을 결정할 수 있다. 예를 들어, 자율 주행 장치는, DVS 이미지 상에서는 특징 정보가 많이 나타나지만 제 1 RGB 이미지에서는 특징 정보가 많이 나타나지 않는 영역을 관심 영역으로 결정할 수 있다. 단계 S550은 도 2의 단계 S230에 대응하므로, 구체적인 설명은 생략하기로 한다. 단계 S560에서, 자율 주행 장치는, 적어도 하나의 제 2 영역(관심 영역)과 관련하여 카메라의 촬영 설정 정보를 제어할 수 있다. 단계 S570에서, 자율 주행 장치는, 변경된 촬영 설정 정보에 기초하여, 카메 라로부터 개선된(enhanced) 제 2 RGB 이미지를 획득할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, 카메라의 현재 촬영 설정 정보를 확인하고, 관심 영역의 명 도가 변경될 수 있도록 촬영 설정 정보를 제어할 수 있다. 예를 들어, 자율 주행 장치는, 관심 영역에 대 한 노출, 포커스 및 화이트 밸런스 중 적어도 하나를 제어할 수 있다. 특히, 자율 주행 장치는, 카메라의 센서 감도(GAIN), 조리개 및 노출 시간 중 적어도 하나를 조절함으로써, 관심 영역에 대한 노출 값을 제어할 수 있다. 예컨대, 관심 영역이 어두운 영역인 경우, 자율 주행 장치는, 센서 감도(Gain), 조리개, 노출 시간 을 적절히 조절하여, 개선된 제 2 RGB 이미지에서는 관심 영역이 밝게 나타나도록 제어할 수 있다. 반대로 관심 영역이 밝은 영역인 경우, 자율 주행 장치는, 센서 감도(Gain), 조리개, 노출 시간을 적절히 조절하여, 개 선된 제 2 RGB 이미지에서는 관심 영역이 어둡게 나타나도록 제어할 수 있다. 단계 S560 및 단계 S570은 도 2의 단계 S240에 대응하므로, 구체적인 설명은 생략하기로 한다. 단계 S580에서, 자율 주행 장치는, 제 2 RGB 이미지의 관심 영역(제 1 RGB 이미지의 적어도 하나의 제 2 영역에 대응하는 영역)에서 객체가 인식되지 않는 경우, 적어도 하나의 제 2 영역(관심 영역)과 관련하여 카메 라의 촬영 설정 정보를 다시 제어할 수 있다. 예를 들어, 관심 영역이 어두운 영역인 경우, 자율 주행 장 치는, 카메라의 센서 감도를 더 높여, 관심 영역이 밝게 표현되도록 할 수 있다. 단계 S590에서, 자율 주행 장치는, 인식된 객체를 이용하여 자율 주행 장치의 위치 정보를 획득할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, 인식된 객체에 포함된 특징(feature)을 VIO 기술에 적용하기 위한 특징으로 이용할 수 있다. 예를 들어, 자율 주행 장치는, 제 2 RGB 이미지의 현재 프레임의 관심 영역 및 이전 프레임의 관심 영역에서 각각 특징을 추출하고, 추출된 특징을 정합하여, 2차원 공간 상의 위치 변화를 예 측할 수 있다. 자율 주행 장치는, 스테레오 카메라 또는 관성 센서를 이용하여 실제 이동된 거리 정보를 획득할 수 있다. 자율 주행 장치는, 예측된 위치 변화 및 스테레오 카메라 또는 관성 센서를 통해 획득된 거리 정보를 이용하여, 3차원의 위치 변화량을 추정할 수 있다. 자율 주행 장치는 3차원의 위치 변화량을 이용하여, 주변의 3차원 지도를 생성할 수도 있다. 이하에서는 자율 주행 장치가 히스토그램 대신에 인공 지능 모델을 이용하는 동작에 대해서 도 8을 참조하 여 자세히 살펴보기로 한다. 도 8은 일 실시예에 따른 인공 지능 모델을 이용하여 객체를 인식하는 방법을 설명하기 위한 순서도이다. 단계 S810에서, 자율 주행 장치는, 카메라를 이용하여 제 1 RGB 이미지를 획득할 수 있다. 단계 S810은 도 2의 단계 S210에 대응되므로, 구체적인 설명은 생략하기로 한다. 단계 S820에서, 자율 주행 장치는, 제 1 인공 지능 모델을 이용하여, 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재하는지 판단할 수 있다. 일 실시예에 의하면, 제 1 인공 지능 모델은, RGB 이미지들을 학습 하는 신경망 모델로서, RGB 이미지들에서 객체 인식 불가능 영역을 판단하도록 학습된 모델일 수 있다. 일 실시 예에 의하면, 제 1 인공 지능 모델은 자율 주행 장치가 자주 주행하는 경로 상에서 획득되는 RGB 이미지들 을 기초로 학습될 수 있다. 일 실시예에 의하면, 제 1 인공 지능 모델은, 제 1 RGB 이미지가 입력되는 경우, 제 1 RGB 이미지 상에서 어두 운 영역과 밝은 영역을 구별할 수 있다. 이 경우, 제 1 인공 지능 모델은, 어두운 영역과 밝은 영역의 분포를 고려하여, 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재하는지 판단할 수 있다. 예를 들어, 제 1 인공 지능 모델은, 제 1 RGB 이미지 상의 명도가 고르지 않고, 너무 어둡거나 너무 밝은 경우, 제 1 RGB 이미지 내에 객체 인식 불가능 영역이 존재한다고 판단할 수 있다. 또한, 일 실시예에 의하면, 제 1 인공 지능 모델은, 주변 상황 정보(컨텍스트 정보)를 입력 받는 경우, 주변 상 황 정보를 고려하여 제 1 RGB 이미지 상에서 객체 인식 불가능 영역이 존재하는지 판단할 수도 있다. 예를 들어, 제 1 인공 지능 모델은, 자율 주행 장치가 터널에 진입하거나 터널을 통과하고 있는 상황인 경우, 제 1 RGB 이미지 상에서 객체 인식 불가능 영역이 존재할 확률이 높다고 판단할 수 있다. 또한, 제 1 인공 지능 모델은 현재 주행 경로 상의 현재 위치가 역광이 발생하는 위치인 경우, 제 1 RGB 이미지 상에 객체 인식 불가 능 영역이 존재할 확률이 높다고 판단할 수 있다. 단계 S830에서, 자율 주행 장치는, 객체 인식 불가능 영역이 존재하지 않는 경우, 카메라의 촬영 설 정 정보를 변경하지 않을 수 있다. 즉, 자율 주행 장치는 카메라의 현재 촬영 설정 정보에 기초하여 계속 RGB 이미지를 획득할 수 있다. 단계 S535에서, 자율 주행 장치는, 제 1 RGB 이미지에 객체 인식 불가능 영 역이 존재하지 않으므로, 제 1 RGB 이미지에서 객체를 인식할 수 있다. 단계 S840에서, 자율 주행 장치는, 객체 인식 불가능 영역이 존재하는 경우, 제 1 인공 지능 모델을 이용 하여, 제 1 RGB 이미지 내에서 객체 인식이 불가능한 적어도 하나의 제 1 영역(객체 인식 불가능 영역)을 예측 할 수 있다. 예를 들어, 자율 주행 장치가 제 1 인공 지능 모델에 제 1 RGB 이미지를 적용하는 경우, 제 1 인공 지능 모델은 제 1 RGB 이미지에서 제 1 기준 값(예컨대, 100)보다 낮은 명도 값을 가지는 영역을 객체 인식 불가능 영역으로 예측하거나, 제 2 기준 값(예컨대, 150)보다 높은 명도 값을 가지는 영역을 객체 인식 불가능 영역으 로 예측할 수 있다. 단계 S850에서, 자율 주행 장치는, 제 2 인공 지능 모델에 DVS 이미지와 제 1 RGB 이미지를 적용하여, 적 어도 하나의 제 1 영역 중에서 객체가 존재하는 적어도 하나의 제 2 영역을 결정할 수 있다. 일 실시예에 의하면, 제 2 인공 지능 모델은, 동일한 공간에 대한 DVS 이미지들과 RGB 이미지들을 학습하는 모 델일 수 있다. 제 2 인공 지능 모델은, DVS 이미지와 RGB 이미지를 비교하여, RGB 이미지의 객체 인식 불가능 영역 중에서 객체가 존재하는 영역을 예측하는 신경망 모델일 수 있다. 일 실시예에 의하면, 제 2 인공 지능 모델은 RGB 이미지에서 객체 인식 불가능 영역을 결정하기 위한 제 1 인공 지능 모델과 분리된 별도의 모델일 수도 있고, 동일한 하나의 모델일 수도 있다. 도 9를 참조하면, 자율 주행 장치는, 터널을 통과하는 중에 카메라를 통해 RGB 이미지를 획득하 고, 동적 비전 센서를 통해 DVS 이미지를 획득할 수 있다. 이때, RGB 이미지와 DVS 이미지 는 AI 프로세서로 전달될 수 있다. AI 프로세서는 RGB 이미지와 DVS 이미지를 제 2 인공 지능 모델에 입력할 수 있다. 이 경우, 제 2 인공 지능 모델은, RGB 이미지와 DVS 이미지를 비 교하여, RGB 이미지의 터널 출구 영역에서는 특징(feature) 또는 엣지가 거의 추출되지 않지만, DVS 이미지의 터널 출구 영역에서는 특징(feature) 또는 엣지가 검출된다고 판단할 수 있다. 이때, 제 2 인공 지능 모델은 RGB 이미지의 터널 출구 영역을 객체가 존재하지만 객체가 인식이 되지 않는 관심 영역으로 결정할 수 있다. 제 2 인공 지능 모델은 관심 영역에 관한 정보를 AI 프로세서로 전달할 수 있다. 단계 S860에서, 자율 주행 장치는, 제 3 인공 지능 모델을 이용하여, 카메라의 촬영 설정 정보를 제 어할 수 있다. 단계 S870에서, 자율 주행 장치는, 변경된 촬영 설정 정보에 기초하여, 개선된 제 2 RGB 이 미지를 획득할 수 있다. 일 실시예에 의하면, 제 3 인공 지능 모델은 카메라의 촬영 설정 정보(예컨대, 노출, 화이트 밸런스, 포커 스)와 RGB 이미지들을 학습하는 신경망 모델일 수 있다. 제 3 인공 지능 모델은 적절한 촬영 설정 정보를 추천 하는 모델일 수 있다. 제 3 인공 지능 모델은 제 1 인공 지능 모델 및 제 2 인공 지능 모델과 분리된 별도의 모 델일 수 있다. 또한, 제 3 인공 지능 모델은 제 1 인공 지능 모델 및 제 2 인공 지능 모델과 하나의 모델을 구 성할 수도 있다. 도 10을 참조하면, 자율 주행 장치는 RGB 이미지 및 관심 영역에 관한 정보를 제 3 인공 지능 모델에 적용할 수 있다. 이 경우, 제 3 인공 지능 모델은 RGB 이미지의 관심 영역의 밝기가 변경되도 록 하는 촬영 설정 값을 결정할 수 있다. 일 실시예에 의하면, 제 3 인공 지능 모델은, 관심 영역에 대한 노출, 포커스 및 화이트 밸런스 중 적어도 하나를 변경할 수 있다. 특히, 제 3 인공 지능 모델은, 카메라 의 센서 감도(GAIN), 조리개 및 노출 시간 중 적어도 하나를 조절함으로써, 관심 영역에 대한 노출 값을 제어할 수 있다. 예를 들어, 관심 영역이 터널 출구의 아주 밝은 영역이므로, 제 3 인공 지능 모델은 관심 영역이 어두워지도록 하는 촬영 설정 값(예컨대, 높은 센서 감도 값)을 결정할 수 있다. 자율 주행 장치가 제 3 인공 지능 모델에 의해 결정된 촬영 설정 값으로 카메라의 현재 촬영 설정 값 을 변경하는 경우, 변경된 촬영 설정 값에 기초하여, 카메라는 개선된 RGB 이미지를 획득할 수 있다. 예를 들어, 개선된 RGB 이미지는 RGB 이미지에 비해 전체적으로 어두운 이미지일 수 있으며, 개선된 RGB 이미지의 관심 영역에서 특징(feature) 또는 엣지가 나타날 수 있다. 따라서, 자율 주 행 장치는, 개선된 RGB 이미지의 관심 영역에서 객체를 인식할 수 있다. 한편, 도 11을 참조하면, 자율 주행 장치는, 터널에 진입할 때 카메라를 통해 RGB 이미지를 획 득하고, 동적 비전 센서를 통해 DVS 이미지를 획득할 수 있다. 이때, RGB 이미지와 DVS 이미 지는 AI 프로세서로 전달될 수 있다. AI 프로세서는 RGB 이미지와 DVS 이미지를 제 2 인공 지능 모델에 입력할 수 있다. 이 경우, 제 2 인공 지능 모델은, RGB 이미지와 DVS 이미지 를 비교하여, RGB 이미지의 터널 입구 영역에서는 특징(feature) 또는 엣지가 거의 추출되지 않지만, DVS 이미지의 터널 입구 영역에서는 특징(feature) 또는 엣지가 검출된다고 판단할 수 있 다. 이때, 제 2 인공 지능 모델은 RGB 이미지의 터널 입구 영역을 객체가 존재하지만 객체가 인식 이 되지 않는 관심 영역으로 결정할 수 있다. 제 2 인공 지능 모델은 관심 영역에 관한 정보를 AI 프로세서 로 전달할 수 있다. AI 프로세서는 RGB 이미지 및 관심 영역에 관한 정보를 제 3 인공 지능 모델에 적용할 수 있다. 이 경우, 제 3 인공 지능 모델은 RGB 이미지의 관심 영역(예컨대, 터널 입구 영역)의 밝기가 변경되도 록 하는 촬영 설정 값을 결정할 수 있다. 예를 들어, 관심 영역이 터널 입구의 아주 어두운 영역이므로, 제 3 인공 지능 모델은 관심 영역이 밝아지도록 하는 촬영 설정 값(예컨대, 낮은 센서 감도 값)을 결정할 수 있다. 자율 주행 장치가 제 3 인공 지능 모델에 의해 결정된 촬영 설정 값으로 카메라의 현재 촬영 설정 값 을 변경하는 경우, 변경된 촬영 설정 값에 기초하여, 카메라는 개선된 RGB 이미지를 획득할 수 있다. 예를 들어, 개선된 RGB 이미지는 RGB 이미지에 비해 전체적으로 밝은 이미지일 수 있으며, 개선된 RGB 이미지의 관심 영역에서 특징(feature) 또는 엣지가 나타날 수 있다. 따라서, 자율 주 행 장치는, 개선된 RGB 이미지의 관심 영역에서 객체를 인식할 수 있다. 도 12를 참조하면, 자율 주행 장치는, 역광이 발생하는 환경에서 카메라를 통해 RGB 이미지를 획득하고, 동적 비전 센서를 통해 DVS 이미지를 획득할 수 있다. 이때, RGB 이미지와 DVS 이 미지는 AI 프로세서로 전달될 수 있다. AI 프로세서는 RGB 이미지와 DVS 이미지 를 제 2 인공 지능 모델에 입력할 수 있다. 이 경우, 제 2 인공 지능 모델은, RGB 이미지와 DVS 이미지 를 비교하여, RGB 이미지의 좌측 상단 영역에서는 특징(feature) 또는 엣지가 거의 추출되지 않지만, DVS 이미지의 좌측 상단 영역에서는 특징(feature) 또는 엣지가 검출된다고 판단할 수 있 다. 이때, 제 2 인공 지능 모델은 RGB 이미지의 좌측 상단 영역을 객체가 존재하지만 객체가 인식 이 되지 않는 관심 영역으로 결정할 수 있다. 제 2 인공 지능 모델은 관심 영역에 관한 정보를 AI 프로세서로 전달할 수 있다. AI 프로세서는 RGB 이미지 및 관심 영역에 관한 정보를 제 3 인공 지능 모델에 적용할 수 있다. 이 경우, 제 3 인공 지능 모델은 RGB 이미지의 관심 영역(예컨대, 역광에 의해 밝게 나타나는 좌측 상단 영 역)의 밝기가 변경되도록 하는 촬영 설정 값을 결정할 수 있다. 예를 들어, 관심 영역이 아주 밝은 영역 이므로, 제 3 인공 지능 모델은 관심 영역이 어두워지도록 하는 촬영 설정 값(예컨대, 높은 센서 감도 값)을 결 정할 수 있다. 또는, 제 3 인공 지능 모델은 측거점의 위치를 조정하거나, 측광 모드를 변경하여 노출을 제어할 수도 있다. 자율 주행 장치가 제 3 인공 지능 모델에 의해 결정된 촬영 설정 값으로 카메라의 현재 촬영 설정 값 을 변경하는 경우, 변경된 촬영 설정 값에 기초하여, 카메라는 개선된 RGB 이미지를 획득할 수 있다. 예를 들어, 개선된 RGB 이미지는 RGB 이미지에 비해 전체적으로 어두운 이미지일 수 있으며, 개선된 RGB 이미지의 관심 영역에서 특징(feature) 또는 엣지가 나타날 수 있다. 따라서, 자율 주 행 장치는, 개선된 RGB 이미지의 관심 영역에서 객체(예컨대, 표지판)를 인식할 수 있다. 단계 S880에서, 자율 주행 장치는, 자율 주행 장치는, 제 2 RGB 이미지의 관심 영역(제 1 RGB 이미지 의 적어도 하나의 제 2 영역에 대응하는 영역)에서 객체가 인식되지 않는 경우, 적어도 하나의 제 2 영역(관심 영역)과 관련하여 카메라의 촬영 설정 정보를 다시 제어할 수 있다. 예를 들어, 관심 영역이 어두운 영역 인 경우, 자율 주행 장치는, 카메라의 센서 감도를 더 높여, 관심 영역이 밝게 표현되도록 할 수 있 다. 단계 S890에서, 자율 주행 장치는, 인식된 객체를 이용하여 자율 주행 장치의 위치 정보를 획득할 수 있다. 단계 S890은 도 5의 단계 S590에 대응되므로, 구체적인 설명은 생략하기로 한다. 이하에서는, 자율 주행 장치가 복수의 관심 영역을 결정하는 경우, 복수의 관심 영역의 우선 순위에 따라 서, 카메라의 촬영 설정 정보를 제어하는 동작에 대해서 도 13 및 도 14를 참조하여 자세히 살펴보기로 한 다. 도 13은 일 실시예에 따른 복수의 관심 영역의 우선 순위에 따라, 카메라의 촬영 설정 정보를 제어하는 방법을 설명하기 위한 순서도이다. 단계 S1310에서, 자율 주행 장치는, 카메라를 이용하여 제 1 RGB 이미지를 획득할 수 있다. 단계 S1310은 도 2의 단계 S210에 대응되므로, 구체적인 설명은 생략하기로 한다. 단계 S1320에서, 자율 주행 장치는, 제 1 RGB 이미지의 명도 정보에 기초하여, 제 1 RGB 이미지 내에서 복 수의 객체 인식 불가능 영역을 예측할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, 제 1 RGB 이미지의 히스토그램 또는 인공 지능 모델을 이용하여, 제 1 RGB 이미지 내에서 복수의 객체 인식 불가능 영역을 예측할 수 있다. 예를 들어, 자율 주행 장치는, 제 1 RGB 이미지 내에서 명도 값(brightness value)이 임계 범위를 벗어나 는 영역(예컨대, 너무 어두운 영역 또는 너무 밝은 영역)을 객체 인식 불가능 영역으로 예측할 수 있다. 이때, 제 1 RGB 이미지 내에서 명도 값(brightness value)이 임계 범위를 벗어나는 영역이 복수 개 존재하는 경우, 자 율 주행 장치는 복수의 객체 인식 불가능 영역을 검출할 수 있다. 단계 S1320은 도 2의 단계 S220에 대응되므로, 구체적인 설명은 생략하기로 한다. 단계 S1330에서, 자율 주행 장치는, 동적 비전 센서(DVS)를 통해 획득되는 객체 정보에 기초하여, 복수의 객체 인식 불가능 영역 중에서 객체가 존재하는 복수의 관심 영역을 결정할 수 있다. 예를 들어, 자율 주행 장치는, 카메라를 통해서는 현재 객체 인식이 불가능하지만 동적 비전 센서 를 통해 객체가 감지된 복수의 영역을 복수의 관심 영역으로 결정할 수 있다. 이 때, 복수의 관심 영역의 명도 값은 상이할 수 있다. 예를 들어, 제 1 관심 영역은 어두운 영역이고, 제 2 관심 영역은 밝은 영역일 수 있다. 따라서, 카메라의 촬영 설정 정보를 한번 조절해서 복수의 관심 영역 각각에서 객체를 모두 검출하 기 어려울 수 있다. 따라서, 단계 S1340에서, 자율 주행 장치는, 소정 기준에 따라, 복수의 관심 영역의 우선 순위를 결정할 수 있다. 일 실시예에 의하면, 자율 주행 장치는 명도가 낮은 영역의 우선 순위를 높게 결정할 수 있다. 예를 들어, 어두운 영역이 밝은 영역보다 우선 순위가 높을 수 있다. 또는, 자율 주행 장치는, 명도가 높은 영역의 우 선 순위를 높게 결정할 수도 있다. 예를 들어, 밝은 영역이 어두운 영역보다 우선 순위가 높을 수 있다. 일 실시예에 의하면, 자율 주행 장치는 관심 영역의 면적에 기초하여 우선 순위를 결정할 수도 있다. 예를 들어, 넓은 영역의 우선 순위가 좁은 영역의 우선 순위보다 높을 수 있다. 한편, 일 실시예에 의하면, 자율 주행 장치는, 주변 환경 정보(예컨대, 컨텍스트 정보)를 이용하여, 관심 영역의 우선 순위를 결정할 수도 있다. 예를 들어, 자율 주행 장치가 터널 입구 쪽에 위치하는 경우, 명도 가 낮은 영역의 우선 순위를 높게 결정하고, 자율 주행 장치가 터널 출구 쪽에 위치하는 경우 명도가 높은 영역의 우선 순위를 높게 결정할 수 있으나, 이에 한정되는 것은 아니다. 단계 S1350 내지 단계 S1380에서, 자율 주행 장치는, 복수의 관심 영역의 우선 순위를 고려하여, 카메라 의 촬영 설정 정보를 제어함으로써, 개선된 RGB 이미지를 획득할 수 있다. 즉, 단계 S1350에서, 자율 주행 장치는 제 n 순위의 관심 영역과 관련하여 카메라의 촬영 설정 정보를 제어함으로써, 개선된 제 n+1 RGB 이미지를 획득할 수 있다. 단계 S1360에서, 자율 주행 장치는, 제 n+1 RGB 이미지 내에서 제 n 객체를 인식할 수 있다. 단계 S1370에서, 자율 주행 장치는 제 n 순위의 관심 영역이 마지막 관심 영역인지 판단 할 수 있다. 단계 S1380에서, 자율 주행 장치는, 제 n 순위의 관심 영역이 마지막 관심 영역이 아닌 경우, 제 n+1 순위의 관심 영역을 선택하고 단계 S1350으로 돌아갈 수 있다. 예를 들어, 자율 주행 장치는, 제 1 순위의 관심 영역과 관련하여, 카메라의 촬영 설정 정보를 제어 함으로써, 개선된 제 2 RGB 이미지를 획득할 수 있다. 이때, 자율 주행 장치는, 제 2 RGB 이미지의 관심 영역(제 1 순위 관심 영역에 대응하는 영역)에서 제 1 객체를 인식할 수 있다. 그리고 자율 주행 장치는, 제 2 순위 관심 영역과 관련하여, 카메라의 촬영 설정 정보를 제어함으로써, 개선된 제 3 RGB 이미지를 획 득할 수 있다. 자율 주행 장치는, 제 3 RGB 이미지의 관심 영역(제 2 순위 관심 영역에 대응하는 영역)에 서 제 2 객체를 인식할 수 있다. 이때, 제 1 객체와 제 2 객체는 상이할 수 있다. 자율 주행 장치는, 제 2 순위의 관심 영역이 마지막 관심 영역인지 판단할 수 있다. 자율 주행 장치는, 제 2 순위 관심 영역이 마 지막 관심 영역이 아닌 경우, 제 3 순위 관심 영역과 관련하여, 카메라의 촬영 설정 정보를 제어함으로써, 개선된 제 4 RGB 이미지를 획득하고, 개선된 제 4 RGB 이미지의 관심 영역(제 3 순위 관심 영역에 대응하는 영 역)에서 제 3 객체를 인식할 수 있다. 즉, 자율 주행 장치는 우선 순위에 따라 순차적으로 관심 영역들에 서 객체가 검출될 수 있도록 카메라의 촬영 설정 정보를 제어할 수 있다. 도 14를 참조하여, 자율 주행 장치가 복수의 관심 영역의 우선 순위를 고려하여, 카메라의 촬영 설정 정보를 제어하는 동작에 대해서 좀 더 자세히 살펴보기로 한다. 도 14는 일 실시예에 따른 복수의 관심 영역의 우선 순위를 설명하기 위한 도면이다. 자율 주행 장치는, 터널을 통과하는 중에 카메라를 통해 RGB 이미지를 획득하고, 동적 비전 센 서를 통해 DVS 이미지를 획득할 수 있다. 이때, 자율 주행 장치는, RGB 이미지와 DVS 이미지를 비교하여, RGB 이미지의 좌측 하단 영역(① 영역)에서는 특징(feature) 또는 엣지가 거의 추출되지 않지만, DVS 이미지의 좌측 하단 영역(① 영역)에서는 특징(feature) 또는 엣지가 검출된다고 판단할 수 있다. 또한, 자율 주행 장치는, RGB 이미지의 터널 출구 영역(② 영역)에서는 특징 (feature) 또는 엣지가 거의 추출되지 않지만, DVS 이미지의 터널 출구 영역(② 영역)에서는 특징 (feature) 또는 엣지가 검출된다고 판단할 수 있다. 이 경우, 자율 주행 장치는, RGB 이미지의 좌측 하단 영역(① 영역)과 터널 출구 영역(② 영역)을 관심 영역으로 결정할 수 있다. 만일, 명도가 낮은 관심 영역의 우선 순위가 높게 결정되도록 설정되어 있는 경우, 자율 주행 장치는 터널 출구 영역(② 영역)보다 좌측 하단 영역(① 영역)의 우선 순위를 높게 결정할 수 있다. 이 경우, 자율 주행 장치는, 먼저 좌측 하단 영역(① 영역)을 기준으로 카메라의 촬영 설정 정보를 제어할 수 있다. 예를 들어, 자율 주행 장치는, 좌측 하단 영역(① 영역)은 어두운 영역이므로, 센서 감도 를 높게 조절하여, 좌측 하단 영역(① 영역)이 밝게 나타나도록 할 수 있다. 좌측 하단 영역(① 영역)이 밝아졌 으므로, 자율 주행 장치는 좌측 하단 영역(① 영역)에서 객체(예컨대, 외부 차량)를 인식할 수 있다. 한편, 센서 감도가 높아지면 터널 출구 영역(② 영역)은 더 밝아지므로, 터널 출구 영역(② 영역)에서는 여전히객체가 검출되지 않을 수 있다. 그 다음, 자율 주행 장치는, 터널 출구 영역(② 영역)을 기준으로, 카메라의 촬영 설정 정보를 제어 할 수 있다. 예를 들어, 자율 주행 장치는, 터널 출구 영역(② 영역)은 밝은 영역이므로, 센서 감도를 낮 게 조절하여, 터널 출구 영역(② 영역)이 어둡게 나타나도록 할 수 있다. 터널 출구 영역(② 영역)이 어두워졌 으므로, 자율 주행 장치는 터널 출구 영역(② 영역)에서 객체(예컨대, 차선, 육교, 가로수 등)를 인식할 수 있다. 도 15는 일 실시예에 따른 자율 주행 장치가 객체를 추적하는 방법을 설명하기 위한 순서도이다. 단계 S1510에서, 자율 주행 장치는, 객체 검출을 위한 동작 모드를 선택할 수 있다. 객체 추적을 위한 동 작 모드는 고속 검출 모드와 전 영역 검출 모드를 포함할 수 있으나, 이에 한정되는 것은 아니다. 고속 검출 모드는, RGB 이미지 내에서 객체가 검출될 가능성이 높은 후보 영역에 대해서 영상 처리를 수행함으 로써, 고속으로 객체를 검출하는 모드를 의미한다. 이때, 후보 영역은 동적 비전 센서에서 감지되는 정보 에 기초하여 결정될 수 있다. 전 영역 검출 모드는 RGB 이미지의 전 영역에 대한 영상 처리를 수행함으로써, 객체를 검출하는 모드를 의미한 다. 단계 S1520 및 단계 S1530에서, 자율 주행 장치는, 고속 검출 모드가 선택된 경우, 고속 검출 모드로 동작 할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, 동적 비전 센서에서 새로운 객체가 감지되지 않는 경우, RGB 이미지에 대한 새로운 객체 인식 프로세스를 수행하지 않을 수 있다. 단계 S1540에서, 자율 주행 장치는, 동적 비전 센서를 통해, 자율 주행 장치 주변에 나타나는 새로운 객체를 감지할 수 있다. 여기서, 새로운 객체는 동적 객체(예컨대, 차량, 이륜차, 보행자 등)를 포함할 수 있다. 동적 비전 센서는 프레임 단위로 데이터를 획득하는 것이 아니라 낱낱의 픽셀 기준(per-pixel basis)으로 데이터를 획득하므로, 카메라 보다 먼저 새로운 객체를 감지할 수 있다. 일 실시예에 의하면, DVS 이미지의 현재 프레임과 이전 프레임을 비교하여, 자율 주행 장치 주변에 나타나 는 새로운 객체를 감지할 수 있다. 예를 들어, 자율 주행 장치가 주행 중인 제 1 차선의 오른쪽에 위치한 제 2 차선에 외부 차량이 나타나는 경우, 동적 비전 센서는 카메라보다 먼저 외부 차량을 감지할 수 있다. 이때, DVS 이미지의 오른쪽 영역에 외부 차량의 외곽선이 나타날 수 있다. 단계 S1550에서, 자율 주행 장치는, RGB 이미지 내에서 새로운 객체가 인식될 가능성이 임계 값보다 큰 후 보 영역을 결정할 수 있다. 일 실시예에 의하면, 자율 주행 장치는 DVS 이미지 상에 새로운 객체가 나타난 위치에 관한 정보에 기초하 여, RGB 이미지 내에서 후보 영역을 결정할 수 있다. 예를 들어, DVS 이미지를 분석한 결과, 자율 주행 장치 의 오른쪽 영역에서 새로운 객체가 나타난 경우, 자율 주행 장치는 RGB 이미지 내에서 오른쪽 영역을 후보 영역으로 결정할 수 있다. 단계 S1560에서, 자율 주행 장치는, 후보 영역에 대한 영상 처리를 통해 RGB 이미지의 후보 영역에서 새로 운 객체를 인식할 수 있다. 일 실시예에 의하면, 자율 주행 장치는 후보 영역에서 객체를 구성하는 적어도 하나의 특징(feature)를 추 출할 수 있다. 자율 주행 장치는 추출된 적어도 하나의 특징을 이용하여, 후보 영역에서 객체를 인식할 수 있다. 예를 들어, 자율 주행 장치는 RGB 이미지의 후보 영역에서 오른쪽 차선으로 주행하는 외부 차량을 인식할 수 있다. 이 경우, 자율 주행 장치는 새로운 객체를 인식하기 위해 RGB 이미지의 전 영역에 대해서 영상 처리를 수행하지 않아도 됨으로, 객체 인식 속도 및 정확도가 향상될 수 있다. 단계 S1570에서, 자율 주행 장치는, 고속 검출 모드가 선택되지 않은 경우, 전 영역 검출 모드로 동작할 수 있다. 일 시예에 의하면, 영상 처리 능력이 여유가 있거나, 중요 시점인 경우에, 자율 주행 장치는 전 영역 검출 모드 선택할 수 있다. 또는, 자율 주행 장치는, 주기적으로 전 영역 검출 모드로 동작할 수도 있다. 단계 S1590에서, 자율 주행 장치는, 전 영역 검출 모드로 동작하는 경우, RGB 이미지의 전체 영역에 대한 영상 처리를 통해서, 새로운 객체를 인식할 수 있다. 단계 S1590에서, 자율 주행 장치는, 카메라를 이용하여 새로운 객체를 추적할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, RGB 이미지의 일련의 프레임에서 추출되는 새로운 객체의 특징적 인 정보에 기초하여, 새로운 객체의 변화를 추적할 수 있다. 예를 들어, 자율 주행 장치는 새로운 객체의 위치 변화를 추적할 수 있다. 일 실시예에 의하면, 자율 주행 장치는 추적의 타겟이 되는 객체 주변에 식 별 이미지를 표시할 수도 있다. 일 실시예에 의하면, 단계 S1510 내지 S1590 중 일부 단계가 생략될 수 있으며, 일부 단계들의 순서가 변경될 수도 있다. 도 16은 자율 주행 장치가 동적 비전 센서에서 감지된 새로운 객체를 카메라로 인식하고 추적하는 동작을 설명 하기 위한 도면이다. 도 16의 제 1 RGB 이미지를 참조하면, 자율 주행 장치는, 전방의 카메라를 이용하여, 자율 주 행 장치 앞에서 이동하는 객체들을 인식하고 추적할 수 있다. 예를 들어, 자율 주행 장치는 제 1 차 량, 제 2 차량, 제 3 차량을 인식하고 추적할 수 있다. 도 16의 DVS 이미지를 참조하면, 자율 주행 장치는 동적 비전 센서를 이용하여, 자율 주행 장 치로 접근하는 새로운 객체를 검출할 수 있다. 예를 들어, 자율 주행 장치는, 자율 주행 장치 의 좌측으로 접근하는 제 4 차량의 윤곽선을 DVS 이미지에서 검출할 수 있다. 도 16의 제 2 RGB 이미지을 참조하면, 자율 주행 장치는, 동적 비전 센서를 통해서 새로운 객 체가 감지된 경우, 이후 카메라통해 획득되는 제 2 RGB 이미지 내에서 새로운 객체가 인식될 가능성 이 임계 값보다 큰 후보 영역을 결정할 수 있다. 예를 들어, 자율 주행 장치는, DVS 이미지를 통해 새로운 객체가 자율 주행 장치의 좌측으로 접근하고 있는 것을 알았으므로, 제 2 RGB 이미지의 좌측 영역을 후보 영역으로 결정할 수 있다. 자율 주행 장치는 제 2 RGB 이미지의 좌측 영역에 대한 영상 처리를 통해 제 4 차량을 인식할 수 있다. 그리고 자율 주행 장치는, 카메라를 이용하여, 제 1 차량 내지 제 3 차량 과 함께 제 4 차량도 추적할 수 있다. 일 실시예에 의하면, 자율 주행 장치는, 동적 비전 센서를 통해 새로운 객체의 존재 및 위치를 예상 할 수 있으므로, RGB 이미지 상에서 새로운 객체를 빠르게 인식하고 추적할 수 있다. 도 17은 일 실시예에 따른 자율 주행 장치의 구성을 설명하기 위한 블록 구성도이다. 도 17을 참조하면, 자율 주행 장치는, 센싱부, 프로세서, 통신부, 구동부, 출력부 , 저장부, 입력부를 포함할 수 있다. 그러나, 도 17에 도시된 구성 요소 모두가 자율 주행 장치 의 필수 구성 요소인 것은 아니다. 도 17에 도시된 구성 요소보다 많은 구성 요소에 의해 자율 주행 장치 가 구현될 수도 있고, 도 17에 도시된 구성 요소보다 적은 구성 요소에 의해 자율 주행 장치가 구현 될 수도 있다. 예를 들어, 도 1 에 도시된 바와 같이, 자율 주행 장치는, 카메라, 동적 비전 센서 및 프로세서로 구성될 수도 있다. 각 구성에 대해 차례로 살펴보기로 한다. 센싱부는, 자율 주행 장치 주변 환경에 관한 정보를 감지하도록 구성되는 다수의 센서들을 포함할 수 있다. 예를 들어, 센싱부는, 카메라(예컨대, 스테레오 카메라, 모노 카메라, 와이드 앵글 카메라, 어 라운드 뷰 카메라 또는 3D 비전 센서 등), 동적 비전 센서, 라이다 센서, 레이더 센서, IMU(관 성 센서), 초음파 센서, 적외선 센서, 거리 센서, 온/습도 센서, 위치 센서(예 컨대, GPS(Global Positioning System), DGPS(Differential GPS), 관성항법장치(INS: Inertial Navigation System)), 움직임 센싱부를 포함할 수 있으나, 이에 한정되는 것은 아니다. 움직임 센싱부는, 자율 주행 장치의 움직임을 감지하기 위한 것으로, 예를 들어, 지자기 센서, 가속도 센서, 자이로스코프 센서를 포함할 수 있으나, 이에 한정되는 것은 아니다. 일 실시예에 의하면, 카메라는 다수의 카메라들을 포함할 수 있고, 다수의 카메라들은 자율 주행 장치 의 내부 또는 외부 상의 다수의 위치들에 배치될 수 있다. 예를 들어, 자율 주행 장치의 전면부에 3 대의 카메라가 배치되고, 후면부에 1대의 카메라 배치되고, 좌측면부에 2대의 카메라가 배치되고, 우측면부에 2대의 카메라 배치될 수 있으나, 이에 한정되는 것은 아니다. 각 센서들의 기능은 그 명칭으로부터 당업자가 직 관적으로 추론할 수 있으므로, 구체적인 설명은 생략하기로 한다. 프로세서는, 통상적으로 자율 주행 장치의 전반적인 동작을 제어할 수 있다. 프로세서는 저장부 에 저장된 프로그램들을 실행함으로써, 센싱부, 통신부, 구동부, 출력부, 저장부 , 입력부를 제어할 수 있다. 프로세서는, 카메라를 통해 제 1 RGB 이미지를 획득할 수 있다. 프로세서는, 제 1 RGB 이미지를 분석하여, 제 1 RGB 이미지의 히스토그램을 획득하고, 제 1 RGB 이미지의 히스토그램을 이용하여, 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재하는지 판단할 수 있다. 프로세서는, 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재한다고 판단되는 경우, 제 1 RGB 이미 지의 명도 정보에 기초하여, 제 1 RGB 이미지 내에서 적어도 하나의 제 1 영역을 예측할 수 있다. 예를 들어, 프로세서는, 제 1 RGB 이미지 내에서 명도 값(brightness value)이 임계 범위를 벗어나는 영역을 적어도 하나의 제 1 영역으로 예측할 수 있다. 프로세서는 동적 비전 센서를 통해 획득되는 객체 정보에 기초하여, 적어도 하나의 제 1 영역 중에서 객체가 존재하는 적어도 하나의 제 2 영역을 결정할 수 있다. 프로세서는, 적어도 하나의 제 2 영역과 관 련하여 카메라의 촬영 설정 정보를 제어함으로써, 개선된(enhanced) 제 2 RGB 이미지를 획득할 수 있다. 예를 들어, 프로세서는, 카메라의 센서 감도(GAIN), 조리개 및 노출 시간 중 적어도 하나를 조절함으 로써, 카메라의 촬영 설정 정보를 제어할 수 있다. 프로세서는 제 2 RGB 이미지 내에서 객체를 인식할 수 있다. 프로세서는, 카메라를 이용하여, 제 2 RGB 이미지에서 인식된 객체를 추적할 수 있다. 프로세서는, 동적 비전 센서를 통해, 자율 주행 장치 주변에 나타나는 새로운 객체를 감지하고, 카메라를 통해 획득되는 제 3 RGB 이미지 내에서 새 로운 객체가 인식될 가능성이 임계 값보다 큰 후보 영역을 결정할 수 있다. 프로세서는, 후보 영역에 대한 영상 처리를 통해 제 3 RGB 이미지에서 동적 비전 센서를 통해 감지된 새로운 객체를 인식할 수 있다. 프로세서는, 동적 비전 센서의 프레임 레이트를 카메라의 프레임 레이트와 동일하게 설정할 수 도 있다. 일 실시예에 의하면, 프로세서는, 인공 지능(AI; artificial intelligence) 프로세서를 포함할 수 있다. 이 경우, AI 프로세서는, 복수의 RGB 이미지를 학습한 제 1 인공 지능 모델을 이용하여, 제 1 RGB 이미지 내에 서 객체 인식 불가능 영역이 존재하는지 판단하고, 제 1 RGB 이미지 내에서 객체 인식 불가능 영역이 존재한다 고 판단되는 경우, 제 1 인공 지능 모델을 이용하여 제 1 RGB 이미지 내에서 적어도 하나의 제 1 영역(객체 인 식 불가능 영역)을 예측할 수 있다. 또한, 프로세서는, 인공지능(AI) 시스템의 학습 네트워크 모델을 이 용하여, 자율 주행 장치의 모션을 계획할 수도 있다. AI 프로세서는, 인공 지능(AI)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또는 기존의 범용 프로세서 (예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 자율 주행 장 치에 탑재될 수도 있다. 통신부는, 다른 장치(예컨대, 외부 차량 또는 외부 서버)와 무선으로 통신하기 위한 적어도 하나의 안테나 를 포함할 수 있다. 예를 들어, 통신부는, 자율 주행 장치와 외부 차량 또는 자율 주행 장치과 서버 간의 통신을 하게 하는 하나 이상의 구성요소를 포함할 수 있다. 예를 들어, 통신부는, 근거리 통신 부(short-range wireless communication unit), 이동 통신부 및 방송 수신부 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 근거리 통신부(short-range wireless communication unit)는, 블루투스 통신부, BLE(Bluetooth Low Energy) 통신부, 근거리 무선 통신부(Near Field Communication unit), WLAN(와이파이) 통신부, 지그비 (Zigbee) 통신부, 적외선(IrDA, infrared Data Association) 통신부, WFD(Wi-Fi Direct) 통신부, UWB(ultra wideband) 통신부, Ant+ 통신부, 마이크로 웨이브(uWave) 통신부 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 이동 통신부는, 이동 통신망 상에서 기지국, 외부의 단말, 서버 중 적어도 하나와 무선 신호를 송수신한다. 여기에서, 무선 신호는, 음성 호 신호, 화상 통화 호 신호 또는 문자/멀티미디어 메시지 송수신에따른 다양한 형태의 데이터를 포함할 수 있다. 방송 수신부는, 방송 채널을 통하여 외부로부터 방송 신호 및/또는 방송 관련된 정보를 수신할 수 있다. 방송 채널은 위성 채널, 지상파 채널을 포함할 수 있다. 구현 예에 따라서 자율 주행 장치가 방송 수신부 를 포함하지 않을 수도 있다. 일 실시예에 의하면, 통신부는 자율 주행 장치로부터 소정의 거리 내에 위치한 외부 차량과 차량간 통신(예컨대, Vehicle to Vehicle) 또는 자율 주행 장치으로부터 소정의 거리 내에 위치한 인프라와 통신 (예컨대, V2I: Vehicle to Infrastructure)을 수행할 수 있다. 예를 들어, 통신부는, 자율 주행 장치 의 식별 정보, 위치, 속도 등을 포함하는 패킷을 방송 또는 광고할 수 있다. 또한, 통신부는, 외부 차량이 방송 또는 광고하는 패킷을 수신할 수도 있다. 구동부는 자율 주행 장치의 구동(운행) 및 자율 주행 장치 내부의 장치들의 동작에 이용되는 구 성들을 포함할 수 있다. 구동부는 전원 공급부, 추진부, 주행부 및 주변 장치부 중 적어도 하나를 포함할 수 있으나, 이에 한정되지 않는다. 주변 장치부는 네비게이션, 라이트, 방향 지시등, 와이퍼, 내부 조명, 히터 및 에어컨을 포함할 수 있다. 네비게이션은 자율 주행 장치에 대한 운행 경로를 결정하도록 구성되는 시스템일 수 있다. 네비게이션은 자율 주행 장치이 주행하고 있는 동안 동적으로 운행 경로를 갱신하도록 구성될 수 있다. 예를 들어, 네비 게이션은 자율 주행 장치에 대한 운행 경로를 결정하기 위해, GPS 모듈에 의하여 수집된 데이터를 이용할 수 있다. 출력부는, 오디오 신호 또는 비디오 신호 또는 진동 신호의 출력을 위한 것으로, 디스플레이와 음향 출력부, 진동부 등이 포함될 수 있다. 디스플레이는 자율 주행 장치에서 처리되는 정보를 표시 출력할 수 있다. 예를 들어, 디스플레이 는 주행 경로를 포함하는 지도를 표시하거나, 외부 차량들의 위치를 표시하거나, 외부 차량들을 운전하는 운전자의 사각 지대를 표시하거나, 현재 속도, 잔여 연료량, 자율 주행 장치의 주행 경로를 가이드하기 위 한 정보 등을 표시할 수 있으나, 이에 한정되는 것은 아니다. 디스플레이는, 통화 모드인 경우 통화와 관 련된 UI(User Interface) 또는 GUI(Graphic User Interface)를 표시할 수도 있다. 한편, 디스플레이와 터치패드가 레이어 구조를 이루어 터치 스크린으로 구성되는 경우, 디스플레이는 출력 장치 이외에 입력 장치로도 사용될 수 있다. 디스플레이는 액정 디스플레이(liquid crystal display), 박막 트랜지스터 액정 디스플레이(thin film transistor-liquid crystal display), 유기 발광 다이 오드(organic light-emitting diode), 플렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display), 전기영동 디스플레이(electrophoretic display) 중에서 적어도 하나를 포함할 수 있다. 그리고 장치 의 구현 형태에 따라 장치는 디스플레이를 2개 이상 포함할 수도 있다. 일 실시예에 의하면, 디스플레이는 투명 디스플레이를 포함할 수 있다. 투명 디스플레이는 투명 LCD(Liquid Crystal Display) 형, 투명 TFEL(Thin-Film Electroluminescent Panel) 형, 투명 OLED 형 이외에 투사형으로도 구현될 수 있다. 투사형이란 HUD(Head Up Display)와 같이 투명한 스크린에 영상을 투사하여 디스 플레이하는 방식을 의미한다. 음향 출력부는 통신부로부터 수신되거나 저장부에 저장된 오디오 데이터를 출력할 수 있다. 또 한, 음향 출력부는 자율 주행 장치에서 수행되는 기능과 관련된 음향 신호를 출력할 수 있다. 예를 들어, 음향 출력부는 자율 주행 장치의 주행 경로를 가이드하기 위한 음성 메시지를 출력할 수 있다. 이러한 음향 출력부에는 스피커(speaker), 버저(Buzzer) 등이 포함될 수 있다. 진동부는 진동 신호를 출력할 수 있다. 예를 들어, 진동부는 오디오 데이터 또는 비디오 데이터(예컨 대, 경고 메시지 등)의 출력에 대응하는 진동 신호를 출력할 수 있다. 저장부는, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수도 있고, 입/출력되는 데이터들(예 컨대, RGB 이미지, DVS 이미지, 도로 상황 정보, 정밀 지도, 히스토그램 등)을 저장할 수도 있다. 저장부 는 인공지능 모델을 저장할 수도 있다. 저장부는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory),EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 또한, 장치는 인터 넷(internet)상에서 저장 기능을 수행하는 웹 스토리지(web storage) 또는 클라우드 서버를 운영할 수도 있다. 입력부는, 사용자가 자율 주행 장치를 제어하기 위한 데이터를 입력하는 수단을 의미한다. 예를 들어, 입력부에는 키 패드(key pad), 돔 스위치 (dome switch), 터치 패드(접촉식 정전 용량 방식, 압력식 저항막 방식, 적외선 감지 방식, 표면 초음파 전도 방식, 적분식 장력 측정 방식, 피에조 효과 방식 등), 조그 휠, 조그 스위치 등이 있을 수 있으나, 이에 한정되는 것은 아니다. 도 18은 일 실시예에 따른 프로세서의 블록 구성도이다. 도 18을 참조하면, 일부 실시예에 따른 프로세서는 데이터 학습부 및 데이터 인식부를 포함할 수 있다. 데이터 학습부는 객체 인식 상황을 판단하기 위한 기준을 학습할 수 있다. 예를 들어, 데이터 학습부 는 카메라를 통해 객체 인식이 어려운 상황 (예컨대, 터널 진입, 터널 진출, 저녁 또는 새벽의 역광, 야간, 조명 변화가 심한 지역 통과, 음영 지역 통과)을 판단하기 위한 기준을 학습할 수 있다. 또한, 데 이터 학습부는 RGB 이미지에서 객체 인식 불가능 영역을 식별하기 위한 기준 또는 동적 비전 센서의 객체 정보에 기반하여, RGB 이미지에서 관심 영역을 결정하기 위한 기준을 학습할 수 있다. 데이터 학습부 는 카메라의 촬영 설정 정보를 결정하기 위하여 어떤 데이터를 이용할 지, 데이터를 이용하여 촬영 설정 정보를 어떻게 결정할 지에 관한 기준을 학습할 수도 있다. 데이터 학습부는 학습에 이용될 데이터 (예컨대, 이미지)를 획득하고, 획득된 데이터를 후술할 데이터 인식 모델에 적용함으로써, 카메라를 통해 객체를 인식하기 위한 기준을 학습할 수 있다. 일 실시예에 의하면, 데이터 학습부는 개인화 데이터를 학습할 수도 있다. 예를 들어, 데이터 학습부 는 자율 주행 장치가 자주 주행하는 경로에서 얻어지는 RGB 이미지들, 컨텍스트 정보 등을 학습할 수 있다. 일 실시예에 의하면, 데이터 학습부는 자율 주행 장치의 모션을 계획하기 위한 기준을 학 습할 수도 있고, 자율 주행 장치의 위치를 인식하기 위한 기준을 학습할 수도 있다. 데이터 인식부는 데이터에 기초한 객체 인식 상황을 판단할 수 있다. 데이터 인식부는 학습된 데이 터 인식 모델을 이용하여, 검출된 데이터로부터 객체 인식 상황을 판단할 수 있다. 데이터 인식부는 학습 에 의한 기 설정된 기준에 따라 이미지 데이터를 획득하고(예컨대, RGB 이미지 또는 DVS 이미지), 획득된 이미 지 데이터를 입력 값으로 하여 데이터 인식 모델을 이용함으로써, 이미지 데이터에 기초한 객체 인식을 수행할 수 있다. 또한, 획득된 이미지 데이터를 입력 값으로 하여 데이터 인식 모델에 의해 출력된 결과 값은, 데이터 인식 모델을 갱신(refine)하는데 이용될 수 있다. 데이터 학습부 및 데이터 인식부 중 적어도 하나는, 적어도 하나의 하드웨어 칩 형태로 제작되어 자율 주행 장치에 탑재될 수 있다. 예를 들어, 데이터 학습부 및 데이터 인식부 중 적어도 하 나는 인공 지능(AI; artificial intelligence)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또는 기존의 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 자율 주행 장치에 탑재될 수도 있다. 이 경우, 데이터 학습부 및 데이터 인식부는 하나의 자율 주행 장치에 탑재될 수도 있으며, 또는 별개의 전자 장치들에 각각 탑재될 수도 있다. 예를 들어, 데이터 학습부 및 데이터 인식부 중 하나는 자율 주행 장치에 포함되고, 나머지 하나는 서버에 포함될 수 있다. 또한, 데이터 학습부 및 데이터 인식부는 유선 또는 무선으로 통하여, 데이터 학습부가 구축한 모델 정보를 데이 터 인식부로 제공할 수도 있고, 데이터 인식부로 입력된 데이터가 추가 학습 데이터로서 데이터 학 습부로 제공될 수도 있다. 한편, 데이터 학습부 및 데이터 인식부 중 적어도 하나는 소프트웨어 모듈로 구현될 수 있다. 데이 터 학습부 및 데이터 인식부 중 적어도 하나가 소프트웨어 모듈(또는, 인스터력션(instruction) 포 함하는 프로그램 모듈)로 구현되는 경우, 소프트웨어 모듈은 컴퓨터로 읽을 수 있는 판독 가능한 비일시적 판독 가능 기록매체(non-transitory computer readable media)에 저장될 수 있다. 또한, 이 경우, 적어도 하나의 소 프트웨어 모듈은 OS(Operating System)에 의해 제공되거나, 소정의 애플리케이션에 의해 제공될 수 있다. 또는, 적어도 하나의 소프트웨어 모듈 중 일부는 OS(Operating System)에 의해 제공되고, 나머지 일부는 소정의 애플 리케이션에 의해 제공될 수 있다. 도 19는 일 실시예에 따른 데이터 학습부의 블록 구성도이다. 도 19를 참조하면, 일 실시예에 따른 데이터 학습부는 데이터 획득부(1310-1), 전처리부(1310-2), 학습 데이터 선택부(1310-3), 모델 학습부(1310-4) 및 모델 평가부(1310-5)를 포함할 수 있다. 데이터 획득부(1310-1)는 객체 인식 상황 판단에 필요한 데이터를 획득할 수 있다. 데이터 획득부(1310-1)는 객 체 인식 상황 판단을 위한 학습을 위하여 필요한 데이터(예컨대, RGB 이미지, DVS 이미지)를 획득할 수 있다. 일 실시예에 의하면, 데이터 획득부(1310-1)는 객체 인식 상황 판단에 필요한 데이터를 직접 생성할 수도 있고, 외부 장치 또는 서버로부터 객체 인식 상황 판단에 필요한 데이터를 수신할 수도 있다. 일 실시예에 의하면, 객체 인식 상황 판단에 필요한 데이터는, RGB 이미지, 동적 비전 센서 객체 정보, 자 율 주행 장치의 주변 환경 정보, 개인화 학습 데이터 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 전처리부(1310-2)는 객체 인식 상황 판단을 위한 학습에 획득된 데이터가 이용될 수 있도록, 획득된 데이터를 전처리할 수 있다. 전처리부(1310-2)는 후술할 모델 학습부(1310-4)가 객체 인식 상황 판단을 위한 학습을 위하 여 획득된 데이터를 이용할 수 있도록, 획득된 데이터를 기 설정된 포맷으로 가공할 수 있다. 학습 데이터 선택부(1310-3)는 전처리된 데이터 중에서 학습에 필요한 데이터를 선택할 수 있다. 선택된 데이터 는 모델 학습부(1310-4)에 제공될 수 있다. 학습 데이터 선택부(1310-3)는 객체 인식 상황 판단을 위한 기 설정 된 기준에 따라, 전처리된 데이터 중에서 학습에 필요한 데이터를 선택할 수 있다. 또한, 학습 데이터 선택부 (1310-3)는 후술할 모델 학습부(1310-4)에 의한 학습에 의해 기 설정된 기준에 따라 데이터를 선택할 수도 있다. 모델 학습부(1310-4)는 학습 데이터에 기초하여 객체 인식 상황을 어떻게 판단할 지에 관한 기준을 학습할 수 있다. 또한, 모델 학습부(1310-4)는 객체 인식 상황 판단을 위하여 어떤 학습 데이터를 이용해야 하는 지에 대 한 기준을 학습할 수도 있다. 또한, 모델 학습부(1310-4)는 객체 인식 상황 판단에 이용되는 데이터 인식 모델을 학습 데이터를 이용하여 학 습시킬 수 있다. 이 경우, 데이터 인식 모델은 미리 구축된 모델일 수 있다. 예를 들어, 데이터 인식 모델은 기 본 학습 데이터(예를 들어, 샘플 이미지 등)을 입력 받아 미리 구축된 모델일 수 있다. 데이터 인식 모델은, 인식 모델의 적용 분야, 학습의 목적 또는 장치의 컴퓨터 성능 등을 고려하여 구축될 수 있다. 데이터 인식 모델은, 예를 들어, 신경망(Neural Network)을 기반으로 하는 모델일 수 있다. 예컨대, DNN(Deep Neural Network), RNN(Recurrent Neural Network), BRDNN(Bidirectional Recurrent Deep Neural Network)과 같은 모델이 데이터 인식 모델로서 사용될 수 있으나, 이에 한정되지 않는다. 다양한 실시예에 따르면, 모델 학습부(1310-4)는 미리 구축된 데이터 인식 모델이 복수 개가 존재하는 경우, 입 력된 학습 데이터와 기본 학습 데이터의 관련성이 큰 데이터 인식 모델을 학습할 데이터 인식 모델로 결정할 수 있다. 이 경우, 기본 학습 데이터는 데이터의 타입 별로 기 분류되어 있을 수 있으며, 데이터 인식 모델은 데이 터의 타입 별로 미리 구축되어 있을 수 있다. 예를 들어, 기본 학습 데이터는 학습 데이터가 생성된 지역, 학습 데이터가 생성된 시간, 학습 데이터의 크기, 학습 데이터의 장르, 학습 데이터의 생성자, 학습 데이터 내의 오 브젝트의 종류 등과 같은 다양한 기준으로 기 분류되어 있을 수 있다. 또한, 모델 학습부(1310-4)는, 예를 들어, 오류 역전파법(error back-propagation) 또는 경사 하강법(gradient descent)을 포함하는 학습 알고리즘 등을 이용하여 데이터 인식 모델을 학습시킬 수 있다. 또한, 모델 학습부(1310-4)는, 예를 들어, 학습 데이터를 입력 값으로 하는 지도 학습(supervised learning) 을 통하여, 데이터 인식 모델을 학습시킬 수 있다. 또한, 모델 학습부(1310-4)는, 예를 들어, 별다른 지도없이 상 황 판단을 위해 필요한 데이터의 종류를 스스로 학습함으로써, 객체 인식 상황 판단을 위한 기준을 발견하는 비 지도 학습(unsupervised learning)을 통하여, 데이터 인식 모델을 학습시킬 수 있다. 또한, 모델 학습부(1310- 4)는, 예를 들어, 학습에 따른 객체 인식 상황 판단의 결과가 올바른 지에 대한 피드백을 이용하는 강화 학습 (reinforcement learning)을 통하여, 데이터 인식 모델을 학습시킬 수 있다. 또한, 데이터 인식 모델이 학습되면, 모델 학습부(1310-4)는 학습된 데이터 인식 모델을 저장할 수 있다. 이 경 우, 모델 학습부(1310-4)는 학습된 데이터 인식 모델을 데이터 인식부를 포함하는 자율 주행 장치의 저장부에 저장할 수 있다. 또는, 모델 학습부(1310-4)는 학습된 데이터 인식 모델을 후술할 데이터 인식부 를 포함하는 자율 주행 장치의 저장부에 저장할 수 있다. 또는, 모델 학습부(1310-4)는 학습된 데이터 인식 모델을 자율 주행 장치와 유선 또는 무선 네트워크로 연결되는 서버의 메모리에 저장할수도 있다. 이 경우, 학습된 데이터 인식 모델이 저장되는 저장부는, 예를 들면, 자율 주행 장치의 적어도 하나 의 다른 구성요소에 관계된 명령 또는 데이터를 함께 저장할 수도 있다. 또한, 저장부는 소프트웨어 및/또 는 프로그램을 저장할 수도 있다. 프로그램은, 예를 들면, 커널, 미들웨어, 어플리케이션 프로그래밍 인터페이 스(API) 및/또는 어플리케이션 프로그램(또는 \"어플리케이션\") 등을 포함할 수 있다. 모델 평가부(1310-5)는 데이터 인식 모델에 평가 데이터를 입력하고, 평가 데이터로부터 출력되는 인식 결과가 소정 기준을 만족하지 못하는 경우, 모델 학습부(1310-4)로 하여금 다시 학습하도록 할 수 있다. 이 경우, 평가 데이터는 데이터 인식 모델을 평가하기 위한 기 설정된 데이터일 수 있다. 예를 들어, 모델 평가부(1310-5)는 평가 데이터에 대한 학습된 데이터 인식 모델의 인식 결과 중에서, 인식 결 과가 정확하지 않은 평가 데이터의 개수 또는 비율이 미리 설정된 임계치를 초과하는 경우 소정 기준을 만족하 지 못한 것으로 평가할 수 있다. 예컨대, 소정 기준이 비율 2%로 정의되는 경우, 학습된 데이터 인식 모델이 총 1000개의 평가 데이터 중의 20개를 초과하는 평가 데이터에 대하여 잘못된 인식 결과를 출력하는 경우, 모델 평 가부(1310-5)는 학습된 데이터 인식 모델이 적합하지 않은 것으로 평가할 수 있다. 한편, 학습된 데이터 인식 모델이 복수 개가 존재하는 경우, 모델 평가부(1310-5)는 각각의 학습된 동영상 인식 모델에 대하여 소정 기준을 만족하는지를 평가하고, 소정 기준을 만족하는 모델을 최종 데이터 인식 모델로서 결정할 수 있다. 이 경우, 소정 기준을 만족하는 모델이 복수 개인 경우, 모델 평가부(1310-5)는 평가 점수가 높은 순으로 미리 설정된 어느 하나 또는 소정 개수의 모델을 최종 데이터 인식 모델로서 결정할 수 있다. 한편, 데이터 학습부 내의 데이터 획득부(1310-1), 전처리부(1310-2), 학습 데이터 선택부(1310-3), 모델 학습부(1310-4) 및 모델 평가부(1310-5) 중 적어도 하나는, 적어도 하나의 하드웨어 칩 형태로 제작되어 자율 주행 장치에 탑재될 수 있다. 예를 들어, 데이터 획득부(1310-1), 전처리부(1310-2), 학습 데이터 선택부 (1310-3), 모델 학습부(1310-4) 및 모델 평가부(1310-5) 중 적어도 하나는 인공 지능(AI; artificial intelligence)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또는 기존의 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 자율 주행 장치에 탑재될 수도 있다. 또한, 데이터 획득부(1310-1), 전처리부(1310-2), 학습 데이터 선택부(1310-3), 모델 학습부(1310-4) 및 모델 평가부(1310-5)는 하나의 자율 주행 장치에 탑재될 수도 있으며, 또는 별개의 전자 장치들에 각각 탑재될 수도 있다. 예를 들어, 데이터 획득부(1310-1), 전처리부(1310-2), 학습 데이터 선택부(1310-3), 모델 학습부 (1310-4) 및 모델 평가부(1310-5) 중 일부는 자율 주행 장치에 포함되고, 나머지 일부는 서버에 포함 될 수 있다. 또한, 데이터 획득부(1310-1), 전처리부(1310-2), 학습 데이터 선택부(1310-3), 모델 학습부(1310-4) 및 모델 평가부(1310-5) 중 적어도 하나는 소프트웨어 모듈로 구현될 수 있다. 데이터 획득부(1310-1), 전처리부(1310- 2), 학습 데이터 선택부(1310-3), 모델 학습부(1310-4) 및 모델 평가부(1310-5) 중 적어도 하나가 소프트웨어 모듈(또는, 인스터력션(instruction) 포함하는 프로그램 모듈)로 구현되는 경우, 소프트웨어 모듈은 컴퓨터로 읽을 수 있는 판독 가능한 비일시적 판독 가능 기록매체(non-transitory computer readable media)에 저장될 수 있다. 또한, 이 경우, 적어도 하나의 소프트웨어 모듈은 OS(Operating System)에 의해 제공되거나, 소정의 애플리케이션에 의해 제공될 수 있다. 또는, 적어도 하나의 소프트웨어 모듈 중 일부는 OS(Operating System)에 의해 제공되고, 나머지 일부는 소정의 애플리케이션에 의해 제공될 수 있다. 도 20은 일 실시예에 따른 데이터 인식부의 블록 구성도이다. 도 20을 참조하면, 일 실시예에 따른 데이터 인식부는 데이터 획득부(1320-1), 전처리부(1320-2), 인식 데이터 선택부(1320-3), 인식 결과 제공부(1320-4) 및 모델 갱신부(1320-5)를 포함할 수 있다. 데이터 획득부(1320-1)는 객체 인식 상황 판단에 필요한 데이터를 획득할 수 있으며, 전처리부(1320-2)는 객체 인식 상황 판단을 위해 획득된 데이터가 이용될 수 있도록, 획득된 데이터를 전처리할 수 있다. 전처리부(1320- 2)는 후술할 인식 결과 제공부(1320-4)가 객체 인식 상황 판단을 위하여 획득된 데이터를 이용할 수 있도록, 획 득된 데이터를 기 설정된 포맷으로 가공할 수 있다. 인식 데이터 선택부(1320-3)는 전처리된 데이터 중에서 객체 인식 상황 판단에 필요한 데이터를 선택할 수 있다. 선택된 데이터는 인식 결과 제공부(1320-4)에 제공될 수 있다. 인식 데이터 선택부(1320-3)는 객체 인식상황 판단을 위한 기 설정된 기준에 따라, 전처리된 데이터 중에서 일부 또는 전부를 선택할 수 있다. 또한, 인 식 데이터 선택부(1320-3)는 후술할 모델 학습부(1310-4)에 의한 학습에 의해 기 설정된 기준에 따라 데이터를 선택할 수도 있다. 인식 결과 제공부(1320-4)는 선택된 데이터를 데이터 인식 모델에 적용하여 객체 인식 상황을 판단할 수 있다. 인식 결과 제공부(1320-4)는 데이터의 인식 목적에 따른 인식 결과를 제공할 수 있다. 인식 결과 제공부(1320- 4)는 인식 데이터 선택부(1320-3)에 의해 선택된 데이터를 입력 값으로 이용함으로써, 선택된 데이터를 데이터 인식 모델에 적용할 수 있다. 또한, 인식 결과는 데이터 인식 모델에 의해 결정될 수 있다. 예를 들어, 적어도 하나의 이미지의 인식 결과는 텍스트, 음성, 동영상, 이미지 또는 명령어(예로, 어플리케이 션 실행 명령어, 모듈 기능 실행 명령어 등) 등으로 제공될 수 있다. 일 예로, 인식 결과 제공부(1320-4)는 적 어도 하나의 이미지에 포함된 객체의 인식 결과를 제공할 수 있다. 인식 결과는, 예로, 적어도 하나의 이미지에 포함된 객체의 자세 정보, 객체의 주변 상태 정보, 동영상에 포함된 객체의 움직임 변화 정보 등이 될 수 있다. 모델 갱신부(1320-5)는 인식 결과 제공부(1320-4)에 의해 제공되는 인식 결과에 대한 평가에 기초하여, 데이터 인식 모델이 갱신되도록 할 수 있다. 예를 들어, 모델 갱신부(1320-5)는 인식 결과 제공부(1320-4)에 의해 제공 되는 인식 결과를 모델 학습부(1310-4)에게 제공함으로써, 모델 학습부(1310-4)가 데이터 인식 모델을 갱신하도 록 할 수 있다. 한편, 데이터 인식부 내의 데이터 획득부(1320-1), 전처리부(1320-2), 인식 데이터 선택부(1320-3), 인식 결과 제공부(1320-4) 및 모델 갱신부(1320-5) 중 적어도 하나는, 적어도 하나의 하드웨어 칩 형태로 제작되어 자율 주행 장치에 탑재될 수 있다. 예를 들어, 데이터 획득부(1320-1), 전처리부(1320-2), 인식 데이터 선 택부(1320-3), 인식 결과 제공부(1320-4) 및 모델 갱신부(1320-5) 중 적어도 하나는 인공 지능(AI; artificial intelligence)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또는 기존의 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 자율 주행 장치에 탑재될 수도 있다. 또한, 데이터 획득부(1320-1), 전처리부(1320-2), 인식 데이터 선택부(1320-3), 인식 결과 제공부(1320-4) 및 모델 갱신부(1320-5)는 하나의 자율 주행 장치에 탑재될 수도 있으며, 또는 별개의 전자 장치들에 각각 탑 재될 수도 있다. 예를 들어, 데이터 획득부(1320-1), 전처리부(1320-2), 인식 데이터 선택부(1320-3), 인식 결 과 제공부(1320-4) 및 모델 갱신부(1320-5) 중 일부는 자율 주행 장치에 포함되고, 나머지 일부는 서버 에 포함될 수 있다. 또한, 데이터 획득부(1320-1), 전처리부(1320-2), 인식 데이터 선택부(1320-3), 인식 결과 제공부(1320-4) 및 모델 갱신부(1320-5) 중 적어도 하나는 소프트웨어 모듈로 구현될 수 있다. 데이터 획득부(1320-1), 전처리부 (1320-2), 인식 데이터 선택부(1320-3), 인식 결과 제공부(1320-4) 및 모델 갱신부(1320-5) 중 적어도 하나가 소프트웨어 모듈(또는, 인스터력션(instruction) 포함하는 프로그램 모듈)로 구현되는 경우, 소프트웨어 모듈은 컴퓨터로 읽을 수 있는 판독 가능한 비일시적 판독 가능 기록매체(non-transitory computer readable media)에 저장될 수 있다. 또한, 이 경우, 적어도 하나의 소프트웨어 모듈은 OS(Operating System)에 의해 제공되거나, 소정의 애플리케이션에 의해 제공될 수 있다. 또는, 적어도 하나의 소프트웨어 모듈 중 일부는 OS(Operating System)에 의해 제공되고, 나머지 일부는 소정의 애플리케이션에 의해 제공될 수 있다. 도 21은 일 실시예에 따른 자율 주행 장치 및 서버가 서로 연동함으로써 데이터를 학습하고 인식하는 예시를 나타내는 도면이다. 도 21을 참조하면, 서버는 객체 인식 상황 판단을 위한 기준을 학습할 수 있으며, 자율 주행 장치는 서버에 의한 학습 결과에 기초하여 객체 인식 상황을 판단할 수 있다. 이 경우, 서버의 모델 학습부는 도 19에 도시된 데이터 학습부의 기능을 수행할 수 있다. 서 버의 모델 학습부는 객체 인식 상황을 판단하기 위하여 어떤 데이터를 이용할 지, 데이터를 이용하 여 객체 인식 상황을 어떻게 판단할 지에 관한 기준을 학습할 수 있다. 모델 학습부는 학습에 이용될 데 이터를 획득하고, 획득된 데이터를 후술할 데이터 인식 모델에 적용함으로써, 객체 인식 상황 판단을 위한 기준 을 학습할 수 있다. 또한, 자율 주행 장치의 인식 결과 제공부(1320-4)는 인식 데이터 선택부(1320-3)에 의해 선택된 데이터를 서버에 의해 생성된 데이터 인식 모델에 적용하여 객체 인식 상황을 판단할 수 있다. 예를 들어, 인식 결 과 제공부(1320-4)는 인식 데이터 선택부(1320-3)에 의해 선택된 데이터를 서버에 전송하고, 서버가인식 데이터 선택부(1320-3)에 의해 선택된 데이터를 인식 모델에 적용하여 객체 인식 상황을 판단할 것을 요청 할 수 있다. 또한, 인식 결과 제공부(1320-4)는 서버에 의해 판단된 객체 인식 상황에 관한 정보를 서버 로부터 수신할 수 있다. 또는, 자율 주행 장치의 인식 결과 제공부(1320-4)는 서버에 의해 생성된 인식 모델을 서버로부 터 수신하고, 수신된 인식 모델을 이용하여 객체 인식 상황을 판단할 수 있다. 이 경우, 자율 주행 장치의 인식 결과 제공부(1320-4)는 인식 데이터 선택부(1320-3)에 의해 선택된 데이터를 서버로부터 수신된 데이 터 인식 모델에 적용하여 객체 인식 상황을 판단할 수 있다. 일 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD- ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 일부 실시예는 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행가능한 명령어를 포함하는 기 록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매 체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독가능 매 체는 컴퓨터 저장 매체 및 통신 매체를 모두 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데 이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구현된 휘발 성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈, 또는 반송파와 같은 변조된 데이터 신호의 기타 데이터, 또는 기타 전송 메커니즘 을 포함하며, 임의의 정보 전달 매체를 포함한다. 또한, 일부 실시예는 컴퓨터에 의해 실행되는 컴퓨터 프로 그램과 같은 컴퓨터에 의해 실행가능한 명령어를 포함하는 컴퓨터 프로그램 또는 컴퓨터 프로그램 제품 (computer program product)으로도 구현될 수 있다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17 도면18 도면19 도면20 도면21"}
{"patent_id": "10-2018-0119303", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 자율 주행 장치를 설명하기 위한 도면이다. 도 2는 일 실시예에 따른 자율 주행 장치의 객체 인식 방법을 설명하기 위한 순서도이다. 도 3은 일 실시예에 따른 RGB 이미지 내에서 결정되는 객체 인식 불가능 영역 및 관심 영역을 설명하기 위한 도 면이다. 도 4는 일 실시예에 따른 카메라의 촬영 설정 정보를 설명하기 위한 도면이다. 도 5는 일 실시예에 따른 히스토그램을 이용하여 객체를 인식하는 방법을 설명하기 위한 순서도이다. 도 6은 일 실시예에 따른 히스토그램을 이용하여 RGB 이미지 내에 객체 인식 불가능 영역이 존재하는지 판단하 는 동작을 설명하기 위한 도면이다. 도 7은 일 실시예에 따른 RGB 이미지에서 객체 인식 불가능 영역을 결정하는 동작을 설명하기 위한 도면이다. 도 8은 일 실시예에 따른 인공 지능 모델을 이용하여 객체를 인식하는 방법을 설명하기 위한 순서도이다. 도 9는 일 실시예에 따른 RGB 이미지와 DVS 이미지를 AI 프로세서에 적용하는 동작을 설명하기 위한 도면이다. 도 10은 일 실시예에 따른 자율 주행 장치가 개선된 RGB 이미지를 획득하는 동작을 설명하기 위한 도면이다. 도 11은 자율 주행 장치가 터널에 진입할 때 개선된 RGB 이미지를 획득하는 동작을 설명하기 위한 도면이다. 도 12는 역광에 의해 RGB 이미지 내에 객체 인식 불가능 영역이 존재하는 경우, 카메라의 촬영 설정 정보를 제 어하는 동작을 설명하기 위한 도면이다. 도 13은 일 실시예에 따른 복수의 관심 영역의 우선 순위에 따라, 카메라의 촬영 설정 정보를 제어하는 방법을 설명하기 위한 순서도이다. 도 14는 일 실시예에 따른 복수의 관심 영역의 우선 순위를 설명하기 위한 도면이다. 도 15는 일 실시예에 따른 자율 주행 장치가 객체를 추적하는 방법을 설명하기 위한 순서도이다. 도 16은 자율 주행 장치가 동적 비전 센서에서 감지된 새로운 객체를 카메라로 인식하고 추적하는 동작을 설명 하기 위한 도면이다. 도 17은 일 실시예에 따른 자율 주행 장치의 구성을 설명하기 위한 블록 구성도이다. 도 18은 일 실시예에 따른 프로세서의 블록 구성도이다. 도 19는 일 실시예에 따른 데이터 학습부의 블록 구성도이다. 도 20은 일 실시예에 따른 데이터 인식부의 블록 구성도이다 도 21는 일 실시예에 따른 자율 주행 장치 및 서버가 서로 연동함으로써 데이터를 학습하고 인식하는 예시를 나타내는 도면이다."}
