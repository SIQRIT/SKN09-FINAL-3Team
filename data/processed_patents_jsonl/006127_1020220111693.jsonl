{"patent_id": "10-2022-0111693", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0032556", "출원번호": "10-2022-0111693", "발명의 명칭": "트레이닝 방법 및 트레이닝 장치", "출원인": "주식회사 케이티", "발명자": "백성복"}}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "분류 모델부가, 입력 데이터에 기반하여 분류 결과를 출력하는 단계;강화 학습부가, 분류 모델의 분류 결과에 대한 검증 결과를 생성하는 단계;상기 강화 학습부가, 상기 분류 모델의 분류 결과 및 상기 강화 학습부의 검증 결과를 포함하는 튜플을 재현 메모리에 저장하는 단계;액티브 러닝부가, 상기 재현 메모리에 저장된 복수의 튜플 중 일부 튜플을 선별하는 단계;상기 액티브 러닝부가, 상기 선별된 일부 튜플에 대한 전문가의 판단 결과를 수신하는 단계;상기 액티브 러닝부가, 상기 판단 결과에 기초하여 상기 재현 메모리를 갱신하는 단계; 및모델 제어부가, 상기 갱신된 재현 메모리 내 튜플들을 이용하여 인공지능 모델을 트레이닝 하는 단계;를 포함하는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 분류 결과에 대한 검증 결과를 생성하는 단계는,상기 강화 학습부의 강화학습 에이전트가, 상기 분류 모델에 제공된 입력 데이터 및 상기 분류 모델의 분류 결과를 포함하는 현재 상태에 기반하여, 상기 분류 결과에 대한 유지 또는 변경을 포함하는 현재 행동을 출력하는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서,상기 입력 데이터는, 디지털 광고와 관련하여 발생하는 트래픽을 포함하고,상기 분류 결과는, 정상 트래픽 또는 허위 트래픽을 포함하는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 2항에 있어서,상기 분류 결과에 대한 검증 결과를 생성하는 단계는,상기 강화학습 에이전트에 대하여 환경(Environment)으로써 주어지는 환경 모델이, 상기 현재 상태 및 상기 현재 행동에 기반하여 평가값을 생성하는 단계;를 포함하는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4항에 있어서,상기 현재 상태 및 상기 현재 행동에 기반하여 평가값을 생성하는 단계는,상기 환경 모델이, 상기 현재 상태의 가치 및 상기 현재 행동에 따른 다음 상태의 가치에 기반하여 상기 평가값을 생성하는 단계;를 포함하는공개특허 10-2024-0032556-3-트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5항에 있어서,상기 현재 상태의 가치 및 상기 현재 행동에 따른 다음 상태의 가치에 기반하여 상기 평가값을 생성하는단계는,상기 환경 모델이, 차분을 상기 평가값으로써 생성하는 단계;를 포함하고,상기 차분은, ‘상기 현재 행동에 대한 보상 및 상기 다음 상태의 가치의 합’과 상기 현재 상태의 가치 간의차이를 이용하여 산출되는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6항에 있어서,상기 튜플은,상기 현재 상태, 상기 현재 행동, 상기 현재 행동에 대한 보상, 상기 다음 상태 및 상기 차분을 포함하는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서,상기 재현 메모리에 저장된 복수의 튜플 중 일부 튜플을 선별하는 단계는,상기 재현 메모리에 저장된 복수의 튜플 중 차분이 가장 큰 소정의 개수의 튜플을 선별하는 단계;를 포함하는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1항에 있어서,상기 판단 결과에 기초하여 상기 재현 메모리를 갱신하는 단계는,상기 전문가에 의해 부당한 것으로 판단된 튜플을 상기 재현 메모리로부터 삭제하는 단계;를 포함하는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1항에 있어서,상기 판단 결과에 기초하여 상기 재현 메모리를 갱신하는 단계는,상기 전문가에 의해 타당한 것으로 판단된 튜플에 대한 보상을 높이는 단계;를 포함하는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 1항에 있어서,상기 갱신된 재현 메모리 내 튜플들을 이용하여 인공지능 모델을 트레이닝 하는 단계는,상기 모델 제어부가, 상기 갱신된 재현 메모리 내 튜플을 이용하여 상기 분류 모델 및 상기 강화 학습부의 환경모델을 트레이닝하는 단계;를 포함하는트레이닝 방법.공개특허 10-2024-0032556-4-청구항 12 제 5항에 있어서,상기 분류 모델의 분류 결과 및 상기 강화 학습부의 검증 결과를 포함하는 튜플을 재현 메모리에 저장하는 단계는,상기 모델 제어부가, 상기 평가값을 이용하여 상기 강화학습부의 강화학습 에이전트를 트레이닝 하는 단계;를포함하는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 1항에 있어서,상기 분류 모델은, 기본 모델을 복사한 복사 모델로, 복수의 타입의 복사 모델들 중 하나이고,상기 기본 모델은, 전문가에 의해 생성된 레이블을 이용하여 트레이닝되는트레이닝 방법."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "입력 데이터에 기반하여 분류 결과를 출력하는 분류 모델부;분류 모델의 분류 결과에 대한 검증 결과를 생성하고, 상기 분류 모델의 분류 결과 및 강화 학습부의 검증 결과를 포함하는 튜플을 재현 메모리에 저장하는 강화 학습부;상기 재현 메모리에 저장된 복수의 튜플 중 일부 튜플을 선별하고, 상기 선별된 일부 튜플에 대한 전문가의 판단 결과를 수신하고, 상기 판단 결과에 기초하여 상기 재현 메모리를 갱신하는 액티브 러닝부; 및상기 갱신된 재현 메모리 내 튜플들을 이용하여 인공지능 모델을 트레이닝 하는 모델 제어부;를 포함하는트레이닝 장치."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14항에 있어서,상기 강화 학습부는, 강화학습 에이전트를 포함하고,상기 강화학습 에이전트는, 상기 분류 모델에 제공된 입력 데이터 및 상기 분류 모델의 분류 결과를 포함하는 현재 상태에 기반하여, 상기분류 결과에 대한 유지 또는 변경을 포함하는 현재 행동을 출력하는트레이닝 장치."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15항에 있어서,상기 입력 데이터는, 디지털 광고와 관련하여 발생하는 트래픽을 포함하고,상기 분류 결과는, 정상 트래픽 또는 허위 트래픽을 포함하는트레이닝 장치."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 15항에 있어서,상기 강화 학습부는, 상기 강화학습 에이전트에 대하여 환경(Environment)으로써 주어지는 환경 모델을 포함하고,공개특허 10-2024-0032556-5-상기 환경 모델은, 상기 현재 상태 및 상기 현재 행동에 기반하여 평가값을 생성하는트레이닝 장치."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 17항에 있어서,상기 환경 모델은, 상기 현재 상태의 가치 및 상기 현재 행동에 따른 다음 상태의 가치에 기반하여 상기 평가값을 생성하는트레이닝 장치."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 18항에 있어서,상기 환경 모델은, 차분을 상기 평가값으로써 생성하고,상기 차분은, ‘상기 현재 행동에 대한 보상 및 상기 다음 상태의 가치의 합’과 상기 현재 상태의 가치 간의차이를 이용하여 산출되는트레이닝 장치."}
{"patent_id": "10-2022-0111693", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 19항에 있어서,상기 튜플은,상기 현재 상태, 상기 현재 행동, 상기 현재 행동에 대한 보상, 상기 다음 상태 및 상기 차분을 포함하는트레이닝 장치."}
{"patent_id": "10-2022-0111693", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "트레이닝 방법이 개시된다. 본 발명에 따른 트레이닝 방법은, 분류 모델부가, 입력 데이터에 기반하여 분류 결과 를 출력하는 단계, 강화 학습부가, 분류 모델의 분류 결과에 대한 검증 결과를 생성하는 단계, 상기 강화 학습부 가, 상기 분류 모델의 분류 결과 및 상기 강화 학습부의 검증 결과를 포함하는 튜플을 재현 메모리에 저장하는 단계, 액티브 러닝부가, 상기 재현 메모리에 저장된 복수의 튜플 중 일부 튜플을 선별하는 단계, 상기 액티브 러 닝부가, 상기 선별된 일부 튜플에 대한 전문가의 판단 결과를 수신하는 단계, 상기 액티브 러닝부가, 상기 판단 결과에 기초하여 상기 재현 메모리를 갱신하는 단계, 및, 모델 제어부가, 상기 갱신된 재현 메모리 내 튜플들을 이용하여 인공지능 모델을 트레이닝 하는 단계를 포함한다."}
{"patent_id": "10-2022-0111693", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은, 분류 모델의 분류 결과에 대하여 강화 학습 모델이 검증하고, 주요 검증 결과에 대하여 전문가의 판 단을 받음으로써, 인공지능 모델이 전문가 이상의 성능을 발휘하면서도 안정적으로 훈련될 수 있도록 하는, 트 레이닝 방법 및 트레이닝 장치에 관한 것이다."}
{"patent_id": "10-2022-0111693", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "디지털 광고 시장에서 고객의 정상적인 활동에 의한 트래픽과는 달리, 트래픽을 허위로 생성 또는 조작하여 광 고주와 개발사로부터 부당한 이득을 취하는 사기행위가 큰 문제가 되고 있다. 그러나 디지털 광고 및 디지털 광 고에 대한 사용자의 응답 방식(클릭 등으로 트래픽 발생)을 고려할 때, 발생한 트래픽이 고객의 정상적인 활동 (해당 웹페이지 방문, 구매 버튼 클릭 등)에 의한 정상 트래픽인지, 또는 위조에 의한 허위 트래픽인지를 구분 하는 일은 매우 까다로운 작업에 해당한다. 기존의 디지털 광고 사기 탐지 기술은, 전문가의 판별 로직을 직접 프로그램화 하는 방식과, 머신러닝/딥러닝 기술을 활용하는 방식으로 나눌 수 있다. 전문가의 지식으로부터 로직을 추출하는 방식은 비용이 많이 들고 유연성이 떨어져서 빠르게 변화하는 사기 기 법을 따라잡기 어렵다는 단점이 있다. 따라서 최근에는 전문가의 판별 이력을 데이터로 축적하여 머신러닝/딥러 닝 기반의 시스템을 구축하는 방식이 많이 사용되고 있다. 다만 종래의 전문가의 판별 이력을 데이터로 축적하는 방식은 인력이 매우 많이 소모되는 작업이며, 대량의 인 력을 동원하더라도 결국 전문가의 능력을 넘어서는 정확도를 확보할 수 없기 때문에 빠르게 변화하는 사기 기법 을 따라잡기 어렵다는 단점이 여전히 존재한다. 또한 변화하는 사기 패턴을 반영하기 위해 주기적으로 머신러닝 /딥러닝 모델을 신규 데이터로 전면 재학습 시키거나 시스템 전체를 다시 구축해야 하는 등 탐지 체제 관리 및"}
{"patent_id": "10-2022-0111693", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "운용에 큰 비용이 소요된다.발명의 내용"}
{"patent_id": "10-2022-0111693", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상술한 문제점을 해결하기 위한 것으로, 본 발명의 목적은, 분류 모델의 분류 결과에 대하여 강화 학 습 모델이 검증하고, 주요 검증 결과에 대하여 전문가의 판단을 받음으로써, 인공지능 모델이 전문가 이상의 성 능을 발휘하면서도 안정적으로 훈련될 수 있도록 하는, 트레이닝 방법 및 트레이닝 장치에 관한 것이다."}
{"patent_id": "10-2022-0111693", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명에 따른 트레이닝 방법은, 분류 모델부가, 입력 데이터에 기반하여 분류 결과를 출력하는 단계, 강화 학 습부가, 분류 모델의 분류 결과에 대한 검증 결과를 생성하는 단계, 상기 강화 학습부가, 상기 분류 모델의 분 류 결과 및 상기 강화 학습부의 검증 결과를 포함하는 튜플을 재현 메모리에 저장하는 단계, 액티브 러닝부가, 상기 재현 메모리에 저장된 복수의 튜플 중 일부 튜플을 선별하는 단계, 상기 액티브 러닝부가, 상기 선별된 일 부 튜플에 대한 전문가의 판단 결과를 수신하는 단계, 상기 액티브 러닝부가, 상기 판단 결과에 기초하여 상기 재현 메모리를 갱신하는 단계, 및, 모델 제어부가, 상기 갱신된 재현 메모리 내 튜플들을 이용하여 인공지능 모 델을 트레이닝 하는 단계를 포함한다. 이 경우 상기 분류 결과에 대한 검증 결과를 생성하는 단계는, 상기 강화 학습부의 강화학습 에이전트가, 상기 분류 모델에 제공된 입력 데이터 및 상기 분류 모델의 분류 결과를 포함하는 현재 상태에 기반하여, 상기 분류 결과에 대한 유지 또는 변경을 포함하는 현재 행동을 출력할 수 있다. 이 경우 상기 입력 데이터는, 디지털 광고와 관련하여 발생하는 트래픽을 포함하고, 상기 분류 결과는, 정상 트 래픽 또는 허위 트래픽을 포함할 수 있다. 한편 상기 분류 결과에 대한 검증 결과를 생성하는 단계는, 상기 강화학습 에이전트에 대하여 환경 (Environment)으로써 주어지는 환경 모델이, 상기 현재 상태 및 상기 현재 행동에 기반하여 평가값을 생성하는 단계;를 포함할 수 있다. 이 경우 상기 현재 상태 및 상기 현재 행동에 기반하여 평가값을 생성하는 단계는, 상기 환경 모델이, 상기 현 재 상태의 가치 및 상기 현재 행동에 따른 다음 상태의 가치에 기반하여 상기 평가값을 생성하는 단계를 포함할 수 있다. 이 경우 상기 현재 상태의 가치 및 상기 현재 행동에 따른 다음 상태의 가치에 기반하여 상기 평가값을 생성하 는 단계는, 상기 환경 모델이, 차분을 상기 평가값으로써 생성하는 단계를 포함하고, 상기 차분은, ‘상기 현재 행동에 대한 보상 및 상기 다음 상태의 가치의 합’과 상기 현재 상태의 가치 간의 차이를 이용하여 산출될 수 있다. 이 경우 상기 튜플은, 상기 현재 상태, 상기 현재 행동, 상기 현재 행동에 대한 보상, 상기 다음 상태 및 상기 차분을 포함할 수 있다. 한편 상기 재현 메모리에 저장된 복수의 튜플 중 일부 튜플을 선별하는 단계는, 상기 재현 메모리에 저장된 복 수의 튜플 중 차분이 가장 큰 소정의 개수의 튜플을 선별하는 단계를 포함할 수 있다. 한편 상기 판단 결과에 기초하여 상기 재현 메모리를 갱신하는 단계는, 상기 전문가에 의해 부당한 것으로 판단 된 튜플을 상기 재현 메모리로부터 삭제하는 단계를 포함할 수 있다. 한편 상기 판단 결과에 기초하여 상기 재현 메모리를 갱신하는 단계는, 상기 전문가에 의해 타당한 것으로 판단 된 튜플에 대한 보상을 높이는 단계를 포함할 수 있다. 한편 상기 갱신된 재현 메모리 내 튜플들을 이용하여 인공지능 모델을 트레이닝 하는 단계는, 상기 모델 제어부 가, 상기 갱신된 재현 메모리 내 튜플을 이용하여 상기 분류 모델 및 상기 강화 학습부의 환경 모델을 트레이닝 하는 단계를 포함할 수 있다. 한편 상기 분류 모델의 분류 결과 및 상기 강화 학습부의 검증 결과를 포함하는 튜플을 재현 메모리에 저장하는 단계는, 상기 모델 제어부가, 상기 평가값을 이용하여 상기 강화학습부의 강화학습 에이전트를 트레이닝 하는 단계를 포함할 수 있다. 한편 상기 분류 모델은, 기본 모델을 복사한 복사 모델로, 복수의 타입의 복사 모델들 중 하나이고, 상기 기본 모델은, 전문가에 의해 생성된 레이블을 이용하여 트레이닝될 수 있다. 한편 본 발명에 따른 트레이닝 장치는, 입력 데이터에 기반하여 분류 결과를 출력하는 분류 모델부, 분류 모델 의 분류 결과에 대한 검증 결과를 생성하고, 상기 분류 모델의 분류 결과 및 강화 학습부의 검증 결과를 포함하 는 튜플을 재현 메모리에 저장하는 강화 학습부, 상기 재현 메모리에 저장된 복수의 튜플 중 일부 튜플을 선별 하고, 상기 선별된 일부 튜플에 대한 전문가의 판단 결과를 수신하고, 상기 판단 결과에 기초하여 상기 재현 메 모리를 갱신하는 액티브 러닝부, 및, 상기 갱신된 재현 메모리 내 튜플들을 이용하여 인공지능 모델을 트레이닝 하는 모델 제어부를 포함한다. 이 경우 상기 강화 학습부는, 강화학습 에이전트를 포함하고, 상기 강화학습 에이전트는, 상기 분류 모델에 제 공된 입력 데이터 및 상기 분류 모델의 분류 결과를 포함하는 현재 상태에 기반하여, 상기 분류 결과에 대한 유 지 또는 변경을 포함하는 현재 행동을 출력할 수 있다. 이 경우 상기 입력 데이터는, 디지털 광고와 관련하여 발생하는 트래픽을 포함하고, 상기 분류 결과는, 정상 트 래픽 또는 허위 트래픽을 포함할 수 있다. 한편 상기 강화 학습부는, 상기 강화학습 에이전트에 대하여 환경(Environment)으로써 주어지는 환경 모델을 포 함하고, 상기 환경 모델은, 상기 현재 상태 및 상기 현재 행동에 기반하여 평가값을 생성할 수 있다. 이 경우 상기 환경 모델은, 상기 현재 상태의 가치 및 상기 현재 행동에 따른 다음 상태의 가치에 기반하여 상 기 평가값을 생성할 수 있다. 이 경우 상기 환경 모델은, 차분을 상기 평가값으로써 생성하고, 상기 차분은, ‘상기 현재 행동에 대한 보상 및 상기 다음 상태의 가치의 합’과 상기 현재 상태의 가치 간의 차이를 이용하여 산출될 수 있다. 이 경우 상기 튜플은, 상기 현재 상태, 상기 현재 행동, 상기 현재 행동에 대한 보상, 상기 다음 상태 및 상기 차분을 포함할 수 있다."}
{"patent_id": "10-2022-0111693", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명 에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용 되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이 해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다.단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함한다\" 또 는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이 들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 발명을 구현함에 있어서 설명의 편의를 위하여 구성요소를 세분화하여 설명할 수 있으나, 이들 구성요소가 하나의 장치 또는 모듈 내에 구현될 수도 있고, 혹은 하나의 구성요소가 다수의 장치 또는 모듈들에 나뉘어져서 구현될 수도 있다. 이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명 에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용 되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이 해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 도 1은 본 발명에 따른 트레이닝 장치를 설명하기 위한 블록도이다. 도 2는 본 발명에 따른, 분류 모델부, 강화 학습부 및 액티브 러닝부를 설명하기 위한 블록도이다. 본 발명에 따른 트레이닝 장치는, 통신 인터페이스부, 제어부, 메모리, 입력부 및 출 력부를 포함할 수 있다. 트레이닝 장치는 도 1의 구성요소 중 일부만을 포함할 수 있으며, 도 1의 구 성요소 외 다른 구성요소를 추가적으로 포함할 수도 있다. 통신 인터페이스부는 외부 장치와 통신하기 위한 통신 모듈 또는 사용자로부터 데이터를 제공받기 위한 입 력 모듈을 포함하고, 데이터를 수신할 수 있다. 한편 통신 인터페이스부는 분류 모델의 분류 대상이 되는 데이터를 수집할 수 있다. 여기서 분류 모델의 분류 대상이 되는 데이터는, 디지털 광고 및 디지털 광고와 관련하여 발생하는 트래픽 중 적어도 하나를 포함할 수 있다. 입력부는 사용자 입력을 수신할 수 있다. 이를 위해 입력부는 터치 패드, 키보드 등의 입력 모듈을 포함하거나, 외부 장치로부터 사용자 입력을 수신하는 통신 모듈을 포함할 수 있다. 출력부는 트레이닝 장치에서 처리되는 데이터를 출력할 수 있다. 구체적으로 출력부 트레이닝 장치에서 처리되는 데이터를 표시하는 디스플레이 모듈 또는 트레이닝 장치에서 처리되는 데이터를 외부 장치로 전송하는 통신 모듈을 포함할 수 있다. 또한 출력부는 재현 메모리 내 복수의 튜플 중 일부 튜플을 출력할 수 있다. 제어부는 트레이닝 장치의 전반적인 동작을 제어할 수 있다. 제어부는 하나 이상의 프로세서, 하나 이상의 마이크로 프로세서 등으로 구현될 수 있다. 메모리는 트레이닝 장치의 구동을 위한 프로그램 또는 기타 명령어를 저장할 수 있다. 또한 메모리 는 인공지능 모델을 저장할 수 있다. 여기서 인공지능 모델은, 분류 모델, 강화학습 에이전트 및 환경 모델을 포함할 수 있다. 제어부는 메모리 내 인공지능 모델을 독출하여 실행할 수 있다. 따라서 본 명세서에서 설명하는 분류 모델, 강화학습 에이전트 및 환경 모델의 동작은, 제어부의 동작일 수 있다. 분류 모델부는 분류 모델을 포함할 수 있다. 분류 모델은 머신 러닝 또는 딥 러닝 모델로써, 지 도 학습 알고리즘에 의해 트레이닝될 수 있다. 본 명세서에서 “트레이닝”이란, 해당 모델의 정책이나 파라미 터(가중치, 편향 등)를 갱신하는 과정을 의미할 수 있다. 분류 모델은, 디지털 광고와 관련하여 발생하는 트래픽이, 디지털 광고를 본 고객의 정상적인 활동(해당 웹페이지 방문, 구매 버튼 클릭 등)에 의한 것인지 또는 트래픽을 허위로 생성 또는 조작하기 위한 것인지를 분 류하는 것을 목적으로 한다. 따라서 분류 모델에는 디지털 광고와 관련하여 발생하는 트래픽이 입력 데이 터로써 제공될 수 있다. 이 경우 분류 모델은 입력 데이터를 복수의 클래스 중 어느 하나로 분류하도록 트 레이닝될 수 있다. 여기서 복수의 클래스는 정상 트래픽 또는 허위 트래픽일 수 있다. 한편 분류 모델에 제공되는 입력 데이터는, 디지털 광고와 관련하여 발생하는 트래픽과 함께 디지털 광고 (영상)을 포함할 수 있다. 분류 모델에는, CNN, DNN 등 공지된 다양한 형태의 뉴럴 네트워크가 사용될 수 있다. 한편 분류 모델은 복사 모델일 수 있다. 구체적으로 분류 모델은 기본 모델 또는 복사 모델일 수 있으며, 기본 모델은 다양한 타입(다양한 타입의 디지털 광고, 광고주/개발사의 다양한 타입의 요구 사항 등)에 범용적 으로 적용될 수 있는 모델일 수 있다. 반면에 복사 모델은 특정 타입에 커스터마이징되는 모델일 수 있다. 기본 모델은, 신뢰할 수 있는 레이블, 예를 들어 전문가(인간)에 의해 입력 데이터에 매칭된 레이블을 이용하여 생성될 수 있다. 또한 기본 모델은 복수의 타입에 범용적으로 적용될 수 있는 레이블을 이용하여 생성될 수 있 다. 트레이닝 장치는 하나의 기본 모델을 복사하여 복사 모델을 생성하고, 복사 모델을 특정 타입에 적합하게 추가 트레이닝할 수 있다. 이에 따라 복수의 타입에 각각 대응하는 복사 모델이 생성될 수 있으며, 아래에서 설 명하는 분류 모델은 복수의 타입의 복사 모델들 중 하나일 수 있다. 한편 동일한 입력 데이터에 대해서도, 복사 모델이 속하는 타입에 따라 레이블이 달라질 수 있다. 따라서 전문 가(인간)은 해당 타입에 맞는 레이블을 다시 생성하고, 트레이닝 장치는 생성된 레이블을 이용하여 복사 모델을 지도 학습 알고리즘을 이용하여 트레이닝할 수 있다. 다만 아래에서 설명하는 강화 학습부 및 액티 브 러닝부가 활용됨으로써, 복사 모델의 트레이닝이 자동으로 수행될 수 있다. 분류 모델은 입력 데이터를 제공받고, 자신의 파라미터에 기초하여 분류 결과를 출력할 수 있다. 이 경우 분류 모델부는 입력 데이터 및 분류 결과를 강화학습부에 전달할 수 있다. 강화 학습부는 강화학습 에이전트 및 환경 모델을 포함할 수 있다. 강화 학습 에이전트는 분류 모델부로부터 제공되는 입력 데이터 및 분류 결과를 이용하여 상태 (state)를 구성하고, 상태(state)에 기반하여 행동(action)을 출력할 수 있다. 또한 강화 학습부는 상태 (state) 및 행동(action)을 환경 모델에 전송할 수 있다. 환경 모델은 상태(state) 및 행동(action)을 이용하여 목표값을 생성할 수 있다. 여기서 목표값은 다음 상 태 및 평가값을 포함할 수 있다. 또한 평가값은 보상, Q값을 포함할 수 있다. 또한 평가값은 보상 및 Q값 중 적 어도 하나를 이용하여 생성되는 값(예를 들어, 차분)을 포함할 수도 있다. 환경 모델은 Q함수 모델 및 보상 모델을 포함할 수 있다. Q 함수 모델은 강화 학습 에이전트로부터 전달받은 상태 및 행동에 기초하여, 다음 상태의 가치, 현재 상태의 가치 등을 산출할 수 있다. 또한 보상 모델 은 강화 학습 에이전트로부터 전달받은 상태 및 행동에 기초하여, 강화 학습 에이전트의 행동에 대한 보상을 산출할 수 있다. Q함수 모델 및 보상 모델 역시 러신 러닝 모델 또는 딥 러닝 모델일 수 있다. 강화 학습부는 튜플을 생성하여 재현 메모리에 저장할 수 있다. 여기서 튜플은 강화학습 에이전트 에 제공된 상태, 강화학습 에이전트가 출력한 행동, 환경 모델이 출력한 보상, 환경 모델 이 출력한 다음 상태 및 환경 모델이 출력한 차분을 포함할 수 있다. 강화 학습부는, 재현 메모리(Replay Memory)를 사용하는 DQN이나 SAC(Soft Actor-Critic)등으로 구현될 수 있다. 액티브 러닝부는 액티브 에이전트 및 재현 메모리를 포함할 수 있다. 재현 메모리에는 강화 학습 에이전트로부터 출력되는 튜플들이 저장될 수 있다. 재현 메모리 내 튜플들은 인공지능 모델을 트레이닝하는데 사용될 수 있다. 구체적으로 분류 모델 및 환경 모델은 재 현 메모리 내 튜플을 훈련 데이터로 이용하여 트레이닝될 수 있다. 액티브 에이전트는 재현 메모리 내 튜플들 중 일부 튜플을 선별하고, 선별된 튜플을 출력부를 통하여 출력할 수 있다. 또한 액티브 에이전트는 출력된 튜플에 대한 전문가의 판단 결과를 입력부를 통하여 수신할 수 있다. 또한 액티브 에이전트는 튜플에 전문가의 판단 결과에 기초하여 재현 메모리를 갱신할 수 있다. 구체적으 로 액티브 에이전트는 전문가에 의해 부당한 것으로 판단된 튜플을 재현 메모리로부터 삭제하거나, 전문가 에 의해 타당한 것으로 판단된 튜플에 대하여 보상을 최대화할 수 있다. 모델 제어부는 인공지능 모델을 트레이닝할 수 있다. 구체적으로 모델 제어부는 환경 모델에서 출력되는 목표값을 이용하여 강화학습 에이전트를 트 레이닝할 수 있다. 이 경우 목표값(차분)에 따라, 강화학습 에이전트의 정책(또는 파라미터)가 변경될 수 있다. 또한 모델 제어부는, 통상적인 강화 학습 알고리즘에서와 같이 강화학습 에이전트가 보상을 최대화 하는 방향으로 정책(또는 파라미터)를 수립하도록 강화학습 에이전트를 트레이닝하며, 여기에는 다양한 공지된 강화 학습 알고리즘이 사용될 수 있다. 또한 모델 제어부는 재현 메모리 내 튜플을 훈련 데이터로 이용하여 분류 모델을 트레이닝할 수 있다. 일 예로 모델 제어부는, 튜플 내 다음 상태에 포함되는 클래스(즉, 강화 학습부에 의해 검증된 클래스)를 분류 모델에 정답값으로 제공하고, 튜플 내 상태에 포함되는 입력 데이터를 분류 모델에 입력값으로 제공하여, 지도 학습 방식으로 분류 모델을 트레이닝할 수 있다. 다만 이에 한정되지 않으며, 모델 제어부는 재현 메모리에서 일부 튜플들을 랜덤 샘플링 하고 미니배치 기반의 역전파 방식으로 분류 모델을 트레이닝하는 것과 같이, 공지된 다양한 알고리즘에 따라 분류 모델을 트레이닝할 수 있 다. 또한 모델 제어부는 재현 메모리 내 튜플을 훈련 데이터로 이용하여 Q 함수 모델 및 보상 모델을 트 레이닝할 수 있다. 이 경우에도 모델 제어부는 재현 메모리에서 일부 튜플들을 랜덤 샘플링 하고 미 니배치 기반의 역전파 방식으로 분류 모델을 트레이닝하는 등, 공지된 다양한 알고리즘에 따라 Q 함수 모 델 및 보상 모델을 트레이닝할 수 있다. 도 3은 본 발명에 따른, 트레이닝 방법을 설명하기 위한 순서도이다. 본 발명에 따른 트레이닝 방법은, 분류 모델부가 입력 데이터에 기반하여 분류 결과를 출력하는 단계(S310), 강 화 학습부가 분류 모델의 분류 결과에 대한 검증 결과를 생성하는 단계(S320), 상기 강화 학습부가, 상기 분류 모델의 분류 결과 및 상기 강화 학습부의 검증 결과를 포함하는 튜플을 재현 메모리에 저장하는 단계(S330), 액 티브 러닝부가, 상기 재현 메모리에 저장된 복수의 튜플 중 일부 튜플을 선별하는 단계(S340), 상기 액티브 러 닝부가 상기 선별된 일부 튜플에 대한 전문가의 판단 결과를 수신하는 단계(S350), 상기 액티브 러닝부가 상기 판단 결과에 기초하여 상기 재현 메모리를 갱신하는 단계(S360), 모델 제어부가 상기 갱신된 재현 메모리 내 튜 플들을 이용하여 인공지능 모델을 트레이닝 하는 단계(S370)를 포함할 수 있다. 먼저 S310에 대하여 도 2를 함께 참고하여 설명한다. 분류 모델부는 입력 데이터에 기반하여 분류 결과를 출력할 수 있다(S310). 여기서 입력 데이터는, 디지털 광고와 관련하여 발생하는 트래픽을 포함할 수 있다. 구체적으로 디지털 광고를 본 사용자는, 디지털 광고를 클릭, 디지털 광고로부터 링크되는 다른 사이트의 접속, 디지털 광고에서 광고되는 제품 또는 서비스의 구매 요청/결제 등의 행위를 수행할 수 있다. 그리고 디지털 광 고와 관련하여 발생하는 트래픽은, 이와 같은 일련의 행위에 따라 온라인 상에서 발생하는 트래픽을 의미할 수 있다. 또한 디지털 광고와 관련하여 발생하는 트래픽은, 트래픽이 발생한 시간, 트래픽을 발생시킨 주체(IP 등), 트래 픽의 횟수 등에 대한 정보를 더 포함할 수 있다. 또한 분류 모델에 제공되는 입력 데이터는 디지털 광고(영상)을 더 포함할 수 있다. 입력 데이터가 제공되면, 분류 모델은 자신의 파라미터(가중치 등)에 기반하여 분류 결과를 출력할 수 있 다. 분류 결과는 복수의 클래스 중 어느 하나일 수 있으며, 복수의 클래스는 정상 트래픽(디지털 광고를 본 고객의 정상적인 활동에 의한 트래픽) 또는 허위 트래픽(허위로 생성 또는 조작하기 위한 트래픽)일 수 있다. 다음으로, 강화 학습부는 분류 모델의 분류 결과에 대한 검증 결과를 생성하고(S320), 분류 모델의 분류 결과 및 강화 학습부의 검증 결과를 포함하는 튜플을 재현 메모리에 저장할 수 있다(S330). 이와 관련해서는 도 2와 함께 도 4를 참고하여 설명한다. 도 4는 강화학습 에이전트에 주어지는 상태(state)를 도시한 도면이다. 강화 학습(Reinforcement learning)은, 강화학습 에이전트가 매순간 어떤 행동을 해야 좋을지 판단할 수 있는 환경이 주어진다면, 데이터 없이 경험으로 가장 좋을 길을 찾을 수 있다는 이론이다. 강화 학습(Reinforcement Learning)은 주로 마르코프 결정 과정(Markov Decision Process, MDP)에 의하여 수행 될 수 있다. 마르코프 결정 과정(Markov Decision Process, MDP)을 설명하면, 첫번째로 강화 학습 에이전트가 다음 행동을 하기 위해 필요한 정보들이 구성된 환경(environment) 이 주어지며, 두번째로 그 환경에서 에이전트가 상태 (state) 를 기반으로 어떻게 행동(action)할지 정의하고, 세번째로 에이전트가 무엇을 잘하면 보상(reward)를 주고 무엇을 못하면 벌점(penalty)을 줄지 정의하며, 네번째로 미래의 보상이 최고점에 이를 때까지 반복 경험 하여 최적의 정책(policy)을 도출하게 된다. 한편 본 발명에서 강화 학습 에이전트는 분류 모델의 분류 결과를 검증하는 역할을 수행한다. 따라서 상태 (state)는 분류 모델에 제공된 입력 데이터 및 분류 모델이 출력한 분류 결과를 포함할 수 있다. 도 4의 F1 내지 FN은 입력 데이터 내 각각의 피쳐(디지털 광고를 클릭, 디지털 광고로부터 링크되는 다른 사이 트의 접속, 디지털 광고에서 광고되는 제품 또는 서비스의 구매 요청/결제, 트래픽 발생 시간, 트래픽의 발생 주체, 트래픽의 횟수 등)를 의미할 수 있다. 또한 분류 결과는 두개의 클래스 중 어느 하나, 즉 정상 트래픽(N) 또는 허위 트래픽(F)를 포함할 수 있다. 한편 강화 학습 에이전트는 분류 모델에 제공된 입력 데이터 및 분류 모델의 분류 결과를 포함하는 상태 (state)에 기반하여 행동(action)을 출력할 수 있다. 구체적으로 강화 학습 에이전트는, 자신의 정책 또는 파라미터에 기반하여, 제공받은 상태(state)를 연산하여 행동(action)을 출력할 수 있다. 한편 강화 학습을 적용하기 위해서는 데이터가 시퀀스한 구조를 가져야 하며, 분류 모델의 분류 결과에 대 하여 시퀀스한 구조를 적용하기 위해 강화 학습 에이전트의 행동은 유지(K) 또는 변경(C)으로 정의될 수 있다. 여기서, “유지”는 분류 모델의 분류 결과를 그대로 유지하는 것이다. 예를 들어 분류 모델의 분류 결과가 정상 트래픽(N)이고 강화 학습 에이전트의 행동이 유지인 경우, 강화 학습 에이전트 역시 해 당 입력 데이터를 정상 트래픽(N)으로 판단했다는 것을 의미한다. 또한 “변경”은 분류 모델의 분류 결과를 변경하는 것이다. 예를 들어 분류 모델의 분류 결과가 허 위 트래픽(F)이고 강화 학습 에이전트의 행동이 변경인 경우, 강화 학습 에이전트가 해당 입력 데이 터를 정상 트래픽(N)으로 판단했다는 것을 의미한다. 강화 학습 에이전트는 상태(atate) 및 행동(action)을 환경 모델에 출력할 수 있다. 상태는 현재 상 태로도 명칭될 수 있으며, 행동은 현재 행동이라고도 명칭될 수 있다. 한편 환경 모델은 강화 학습 에이전트에 환경(environment)으로 주어질 수 있다. 이 경우 환경 모델 은 현재 상태(state) 및 현재 행동(action)에 기반하여, 목표값을 생성할 수 있다. 여기서 목표값은 다음 상태를 포함할 수 있으며, 다음 상태는 현재 상태와 동일한 형식을 가질 수 있다. 따라서 다음 상태는 입력 데이터 및 클래스(정상 트래픽(N) 또는 허위 트래픽(F))를 포함할 수 있다. 구체적으로 환경 모델은 강화 학습 에이전트에 제공된 현재 상태(state)에 강화 학습 에이전트 의 현재 행동(action)을 적용하여 다음 상태를 결정할 수 있다. 예를 들어 강화 학습 에이전트의 현재 행 동이 유지(K)인 경우, 분류 모델의 분류 결과가 그대로 다음 상태로써 설정될 수 있다. 반면에 강화 학습 에이전트의 현재 행동이 변경(C)인 경우, 분류 모델의 분류 결과의 반대 결과가 다음 상 태로써 설정될 수 있다. 이 경우 다음 상태는, 입력 데이터 및 분류 모델의 분류 결과의 반대 결과(분류 모델이 출력한 클래스와 다른 클래스)를 포함할 수 있다. 예를 들어 분류 모델의 분류 결과가 허위 트래픽(F)이고, 현재 상태가 입력 데이터 및 허위 트래픽(F)을 포함하며, 강화 학습 에이전트의 현재 행동이 변경(C)인 경 우, 다음 상태는 입력 데이터 및 정상 트래픽(N)을 포함할 수 있다. 한편 목표값은 평가값을 더 포함할 수 있으며, 평가값은 주어진 현재 상태에서 강화학습 에이전트가 출력 한 행동에 대한 환경 모델의 평가를 의미할 수 있다. 여기서 평가값은 보상 모델이 산출하는 보상 및 Q 함 수 모델이 산출하는 Q값 중 적어도 하나를 포함할 수 있다. 또한 평가값은, 다음 상태, 보상 및 Q값 중 적어도 하나를 이용하여 생성되는 값을 포함할 수도 있다. 환경 모델은, 강화학습 에이전트에 대하여 환경(Environment)으로써 주어지는 것으로, 현재 상태 및 현재 행동에 기반하여 평가값을 생성할 수 있다. 이 경우 환경 모델은 TD(Temporal Difference) 러닝 기법으로 상태에 대한 가치 및 보상을 산출할 수 있다. 구체적으로 환경 모델은, 현재 상태의 가치 및 강화학습 에이전트의 현재 행동에 따른 다음 상태의 가치에 기반하여 평가값을 생성할 수 있다. 일 예로 평가값은 차분일 수 있으며, 차분은 아래와 같이 정의될 수 있다. 수학식 1"}
{"patent_id": "10-2022-0111693", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "(s`: 다음 상태, a`: 다음 행동, s: 현재 상태, a: 현재 행동, : 다음 상태의 가치, : 현재 상태의 가치, r: 현재 행동에 대한 보상, : 감가율) 구체적으로 환경 모델은 다음 상태(s`) 및 다음 행동(a`)을 Q 함수 모델에 제공하고, 딥 러닝 모델인 Q 함 수 모델은 자신의 파라미터에 기반하여 현재 행동에 따른 다음 상태의 가치를 산출할 수 있다. 여기서 Q 함수 모델은 다음 가치의 최대치( )를 산출할 수 있으나 이에 한정되지는 않는다. 또한 환경 모델은 현재 상태(s) 및 현재 행동(a)을 보상 모델에 제공하고, 보상 모델은 자신의 파라미터에 기반하여 강화학습 에이전트의 행동에 대한 보상(r)을 산출할 수 있다. 또한 환경 모델은 현재 상태(s) 및 현재 행동(a)을 Q 함수 모델에 제공하고, 딥 러닝 모델인 Q 함수 모델 은 자신의 파라미터에 기반하여 현재 상태의 가치( )를 산출할 수 있다. 이 경우 환경 모델은 ‘현재 행동에 대한 보상 및 다음 상태의 가치의 합’과 현재 상태의 가치 간의 차이를 이 용하여 차분을 산출할 수 있다. 구체적으로 환경 모델은 다음 상태의 가치( )에 감가율( )을 곱한 후 보상(r)과 더함으로 써, 현재 행동에 대한 보상 및 다음 상태의 가치의 합( )을 산출할 수 있다. 은 참값이라고 명칭될 수 있다. 또한 현재 상태의 가치( )는 예측값이라 명칭될 수 있다. 이 경우 환경 모델은 참값과 예측 값의 차이를 이용하여 차분을 산출할 수 있다. 구체적으로 환경 모델 은 현재 행동에 대한 보상 및 다음 상태의 가치의 합( )과 현재 상태의 가치( )의 차이를 제곱함으로써 차분을 산출할 수 있다. 그리고 나서 환경 모델은, 다음 상태 및 평가 값을 포함하는 목표값을 출력할 수 있다. 환경 모델이 출력하는 평가값은 강화학습 에이전트의 행동에 대한 환경의 평가이다. 따라서 모델 제 어부는 평가값을 이용하여 강화학습 에이전트를 트레이닝할 수 있다. 구체적으로 강화 학습 에이전트 가 평가값에 기반하여 최적의 정책(ploicy)을 수립하도록, 모델 제어부는 평가값을 이용하여 강화학 습 에이전트의 파라미터를 업데이트할 수 있다. 이에 따라 강화학습 에이전트는 보상을 최대화 하는 방향으로 트레이닝될 수 있다. 또한 모델 제어부는 차분을 이용하여 강화학습 에이전트를 트레이닝할 수 있다. 한편 강화 학습 에이전트는 분류 모델의 분류 결과 및 강화 학습부의 검증 결과를 포함하는 튜 플을 재현 메모리에 저장할 수 있다(S330). 이와 관련해서는 도 2와 함께 도 5를 참고하여 설명한다. 도 5는 본 발명에 따른, 재현 메모리에 저장되는 튜플을 도시한 도면이다. 강화 학습 에이전트는 튜플을 재현 메모리에 저장할 수 있다. 여기서 튜플은 분류 모델의 분류 결과 및 강 화 학습부의 검증 결과를 포함할 수 있다. 구체적으로 튜플은, 상태(s), 행동(a), 행동에 대한 보상(r), 다음 상태(S`) 및 차분( )을 포함할 수 있다. 더욱 구체적으로, 앞서 설명한 바와 같이 상태(S)는 분류 모델에 제공된 입력데이터 및 분류 모델의 분류 결과를 포함한다. 또한 강화 학습부의 검증 결과는, 강화학습 에이전트의 검증 결과 및 환경 모델의 검증 결과를 포함할 수 있다. 구체적으로 행동(a)은 분류 결과에 대한 강화학습 에이전트의 검증 결과이며, 보상(r), 다음 상태(S`) 및 차분( )은 환경 모델의 검증 결과이다. 한편 분류 모델부 및 강화학습부는 상술한 동작을 반복하여 수행할 수 있다. 이에 따라 재현 메모리 에는 강화 학습부가 출력한 튜플이 지속적으로 업데이트되며, 강화학습 에이전트는 환경 모델 의 평가값에 기반하여 지속적으로 트레이닝될 수 있다. 다시 도 3으로 돌아가서, 액티브 러닝부는 재현 메모리에 저장된 복수의 튜플 중 일부 튜플을 선별할 수 있다(S340). 구체적으로 액티브 러닝부는 재현 메모리에 저장된 복수의 튜플 중 차분이 가장 큰 소정의 개수의 튜플을 선별할 수 있다. 여기서 “차분이 가장 큰 소정의 개수”란, 재현 메모리에 저장된 복수의 튜플 중 차분이 더 큰 일정 비율의 튜플을 의미할 수 있으나 이에 한정되지 않는다. 예를 들어 “차분이 가장 큰 소정의 개수”는 재현 메모리에 저장된 복수의 튜플 중 차분이 더 큰 일정 개수의 튜플을 의미할 수 있다. 다른 예를 들어 “차분이 가장 큰 소정의 개수”는 재현 메모리에 저장된 복수의 튜플 중 차분이 특정값(절대값) 이상인 튜플들을 의미할 수 있다. 차분이 크다는 것은 아래의 두가지를 의미한다. 구체적으로 차분이 크다는 것은, 입력 데이터에 대한 분류 모델의 판단과, 입력 데이터에 대한 강화학습 에이전트의 판단이 다르다는 것을 의미한다. 환경 모델이 완벽한 정답을 제공할 수 있는 경우 누가 틀렸는지가 식별이 가능하나, TD(Temporal Difference) 러닝 기법에서 환경 모델이 출력한 참값 ( ) 역시, 진정한 의미의 참값 아닌, 환경 모델이 추정한 추정치에 불과하다. 환경 모델 이 출력한 참값( )은 대부분의 경우 진정한 의미의 참값과 일치하나, 가끔은 환경 모델 이 틀리는 경우도 발생할 수 있다. 두번째로, 강화학습 에이전트는 평가값(차분)에 기반하여 트레이닝되며, 차분이 크다는 것은 트레이닝에 큰 효과를 거둘 수 있는 반면, 강화학습 에이전트의 파라미터에 대한 급격한 변화를 야기하기 때문에 발산 (divergence) 등의 문제가 생길 가능성도 높다. 따라서 큰 차분을 이용하여 트레이닝할 때에는 신중함이 요구된 다. 따라서 본 발명에서는 차분이 큰 튜플을 전문가 검증의 대상으로 삼고, 액티브 에이전트는 재현 메모리 에 저장된 복수의 튜플 중 차분이 가장 큰 소정의 개수의 튜플을 선별할 수 있다. 한편 TD(Temporal Difference) 러닝 기법을 가정하여, 차분이 큰 경우에 액티브 에이전트에 의해 선별되는 것으로 설명하였으나 이에 한정되지 않는다. 일 예로, 강화학습 에이전트의 정책 또는 파라미터의 변경량 이 큰 튜플을 선별하는 방식으로 구현될 수 있다. 다시 도 2 및 도 3을 참고하면, 액티브 러닝부는 선별된 일부 튜플에 대한 전문가의 판단 결과를 수신할 수 있다(S350). 구체적으로 액티브 러닝부는 선별된 일부 튜플을 인간 전문가에게 출력할 수 있다. 이 경우 인간 전문가는 선별된 튜플이 타당한지 또는 부당한지를 판단할 수 있다. 구체적으로 인간 전문가는 튜플 내의, 입력 데이터, 분류 모델의 분류 결과, 강화학습 모델의 검증 결과를 검토하고, 해당 튜플을 사용할 것인지 또는 사용하지 않 을 것인지를 판단할 수 있다. 예를 들어, 상태는 분류 모델의 분류 결과인 “허위 트래픽”을 포함하고 강화학습 에이전트의 행동은 “변경” 이며 다음 상태는 “정상 트래픽”을 포함하는 경우, 인간 전문가는 입력 데이터를 검토한 후 분류 모델의 분류 결과가 옳고, 강화학습 에이전트의 행동(변경)이 틀렸다는 것을 확인할 수 있다. 이 경우 인간 전문가는 해당 튜플이 부당하다는 판단 결과를 액티브 에이전트에 제공할 수 있다. 다른 예를 들어, 상태는 분류 모델의 분류 결과인 “허위 트래픽”을 포함하고 강화학습 에이전트의 행동은 “ 변경”이며 다음 상태는 “정상 트래픽”을 포함하는 경우, 인간 전문가는 입력 데이터를 검토한 후 분류 모델 의 분류 결과가 틀렸고, 강화학습 에이전트의 행동(변경)이 맞았다는 것을 확인할 수 있다. 이 경우 인간 전문 가는 해당 튜플이 타당하다는 판단 결과를 액티브 에이전트에 제공할 수 있다. 선별된 일부 튜플에 대한 전문가의 판단 결과를 수신되면, 액티브 러닝부는 전문가의 판단 결과에 기초하 여 재현 메모리를 갱신할 수 있다(S360). 구체적으로 전문가에 의해 부당하다고 판단된 튜플은, 분류 모델 및 환경 모델의 학습에서 악영향을 미칠 수 있다. 따라서 액티브 러닝부는 전문가에 의해 부당하다고 판단된 튜플을 재현 메모리로부터 삭제 할 수 있다. 또한 전문가에 의해 타당하다고 판단된 튜플은, 트레이닝에 큰 효과를 거둘 수 있는 튜플(차분이 큰 튜플)이며, 게다가 인간 전문가의 검증까지 받은 튜플이다. 따라서 액티브 러닝부는 전문가에 의해 타당하다고 판단된 튜플에 대한 보상을 높힐 수 있다. 예를 들어 액티브 러닝부는 해당 튜플에 대한 보상을 최대치로 설정할 수 있다. 또한 여기서 설명하는 보상은, 강화학습 에이전트에 제공되는 보상과는 다른 것으로, 모델 제어 부는 최대치로 설정된 보상을, 분류 모델, Q 함수 모델 또는 보상 모델에 역전파 되는 파라미터 갱신 값에 반영할 수 있다. 한편 재현 메모리가 갱신되면, 재현 메모리에는 전문가가 타당한 것으로 판단한 튜플 및 전문가의 판단을 받지 않은 튜플이 포함되게 된다. 이 경우 모델 제어부는 갱신된 재현 메모리 내 튜플들을 이용하여 인공 지능 모델을 트레이닝 할 수 있다. 구체적으로 모델 제어부는 재현 메모리 내 튜플을 이용하여 분류 모델 및 환경 모델(Q 함수 모 델 및 보상 모델)을 트레이닝할 수 있다. 이 과정은 통상의 역전파 알고리즘을 통해 수행되며, 모델 제어부 는 분류 모델, Q 함수 모델 및 보상 모델 각각에 적합한 방식을 이용하여 해당 모델들의 파라미터를 업데이트할 수 있다. 한편 이와 같은 과정을 거쳐 분류 모델의 트레이닝이 완료되면, 분류 모델은 허위 트래픽 판단 모델 의 역할을 수행할 수 있다. 즉 트레이닝 장치는 디지털 광고와 관련하여 발생하는 트래픽을 수집하고, 수 집된 트래픽을 허위 트래픽 판단 모델에게 제공하여 해당 트래픽이 정상 트래픽인지 또는 허위 트래픽인지 판단 할 수 있다. 그리고 허위 트래픽 발생 시, 트레이닝 장치는 고객(광고주, 개발사 등)에게 통보할 수 있다. 본 발명에 따르면, 강화학습 에이전트는 환경 모델의 평가에 기반하여 트레이닝되고, 분류 모델 및 환경 모델은 재현 메모리 내 튜플을 이용하여 트레이닝된다. 즉, 강화학습 에이전트, 분류 모델 및 환경 모델이 유기적으로 트레이닝됨으로써, 인간의 개입을 최소화 하면서도, 강화학습 에이 전트, 분류 모델 및 환경 모델이 서로 상호작용하며 성능을 향상시킬 수 있다. 또한 본 발명에 따르면, 분류 모델은 재현 메모리 내 튜플들을 이용하여 트레이닝되며, 재현 메모리 내에는 인간 전문가의 검증까지 거친 튜플들 뿐만 아니라, 강화학습 에이전트의 검증까지만 거친 튜 플들도 다수 포함된다. 즉 강화학습 에이전트는 인간이 발견할 수 없는 규칙이나 패턴 등을 발견할 수 있 기 때문에, 분류 모델 역시 인간 전문가가 판단하기 힘든 데이터를 정확히 분류해낼 수 있다. 즉 분류 모 델은 인간 전문가가 생성한 레이블을 이용하여 트레이닝 될 때 보다, 훨씬 더 높은 성능을 나타낼 수 있다. 특히 디지털 광고와 관련하여 발생하는 트래픽이 정상 트래픽인지 또는 허위 트래픽인지를 정확히 판단하는 것 은, 인간 전문가에게도 매우 어려운 일이다. 다만 본 발명에 따르면 강화 학습부와 분류 모델이 유기적으로 상 호작용 함으로써, 정확도 높은 분류 모델을 생성할 수 있는 장점이 있다. 또한 강화 학습부도 진정한 의미의 정답을 알고 있지 않다는 점에서, 분류 모델과 강화 학습부만을 연계시키는 경우 트레이닝에 실패할 가능성이 높아진다. 다만 본 발명에서는, 모델에 크리티컬한 영향을 미칠 수 있는 튜플 만을 선별하여 인간 전문가의 검증을 거치도록 구성함으로써, 안정적인 트레이닝을 가능하게 하는 장점이 있다. 또한 본 발명에 따르면, 입력 데이터와 분류 결과를 합쳐서 상태를 구성하고, 강화학습 에이전트의 행동을 변경 또는 유지로 설정하여 행동에 따라 상태가 변화하도록 구성하는 등, 단순히 강화 학습 알고리즘으로 분류 결과 를 검증한다는 개념을 제시함을 넘어, 수학적으로 마코프 의사결정 프로세스(MDP)의 개념을 그대로 따르고 있다. 즉, 이미 검증된 강화 학습 알고리즘을 따름으로써, 안정적이고 신뢰성 있는 트레이닝이 가능한 장점이 있다. 디지털 광고 사기의 방법은 지속적으로 변화하기 때문에 계속해서 신규 공격 패턴에 대한 대비가 필요하다. 또 한 여러 이해 당사자가 개입되는 광고 측정 업무의 특성 상 학습 데이터를 구축할 때에도 당사자 간의 복잡한 이해관계로 인해 레이블링의 정확도가 저하될 가능성이 있다. 무엇보다 광고 의뢰자의 광고 컨셉에 따라 레이블 링 기준이 달라질 수 있기 때문에 하나의 기본 모델만으로 모든 티입의 디지털 광고 사기를 판별하는 시스템을 구축하는 것은 불가능하다. 그리고 본 발명은 기본 모델을 복사하여 복사 모델을 생성하고, 복사 모델을 사용자 의 필요에 맞게 개선 시킴으로써, 여러 타입의 디지털 광고 사기를 유연하게 탐지하는 것이 가능하게 해주는 효 과가 있다. 복사 모델의 개선에 강화학습 기반의 개선 기술을 적용했기 때문에 충분한 반복 학습을 통해 모델이 최적으로 수렴한 경우 인간 전문가의 판단 능력 이상의 성능을 갖도록 구현하는 것이 가능하다. 예를 들어 트레이닝 장치는, 지도 학습 알고리즘에 기반, 훈련 데이터 및 레이블(전문가에 의해 생성된 레 이블)을 이용하여 뉴럴 네트워크를 트레이닝함으로써 기본 모델을 생성할 수 있다. 또한 트레이닝 장치는 기본 모델을 복사하여 복사 모델을 생성하고, 생성된 복사 모델을 강화 학습부 및 액티브 러닝부와 연계하여 추 가 트레이닝할 수 있다. 이에 따라 확실한 정답을 이용하여 트레이닝된 기본 모델을 다양하게 응용하여 사용할 수 있으며, 이 과정에서 인간의 비용 및 노력을 최소화할 수 있는 장점이 있다. 전술한 본 발명은, 프로그램이 기록된 매체에 컴퓨터가 읽을 수 있는 코드로서 구현하는 것이 가능하다. 컴퓨터 가 읽을 수 있는 매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 컴퓨터가 읽을 수 있는 매체의 예로는, HDD(Hard Disk Drive), SSD(Solid State Disk), SDD(Silicon Disk Drive), ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있다. 또한, 상기 컴 퓨터는 프로세서를 포함할 수도 있다. 따라서, 상기의 상세한 설명은 모든 면에서 제한적으로 해석되어서는 아 니 되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적 해석에 의해 결정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다.도면 도면1 도면2 도면3 도면4 도면5"}
{"patent_id": "10-2022-0111693", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 트레이닝 장치를 설명하기 위한 블록도이다. 도 2는 본 발명에 따른, 분류 모델부, 강화 학습부 및 액티브 러닝부를 설명하기 위한 블록도이다. 도 3은 본 발명에 따른, 트레이닝 방법을 설명하기 위한 순서도이다. 도 4는 강화학습 에이전트에 주어지는 상태(state)를 도시한 도면이다. 도 5는 본 발명에 따른, 재현 메모리에 저장되는 튜플을 도시한 도면이다."}
