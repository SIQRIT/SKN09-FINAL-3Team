{"patent_id": "10-2021-0110729", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0028887", "출원번호": "10-2021-0110729", "발명의 명칭": "시계열 예측을 위한 신경 아키텍처 및 하이퍼 파라미터 검색", "출원인": "한양대학교 산학협력단", "발명자": "조인휘"}}
{"patent_id": "10-2021-0110729", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "신경망 및 하이퍼 파라미터 검색 시스템에 의해 수행되는 시계열 예측을 위한 신경망 및 하이퍼 파라미터 검색방법에 있어서,데이터 세트 중 트레이닝 세트를 이용하여 컨트롤러에 의해 샘플링된 하이퍼 파라미터에 따라 신경망 모델을 훈련하는 단계;강화학습에 기초하여 상기 훈련을 통해 업데이트된 신경망 모델의 매개변수의 임시 가중치를 이용하여 계산된데이터 세트 중 검증 세트의 손실값을 보상으로서 컨트롤러의 매개변수를 업데이트하는 단계; 및상기 업데이트를 통해 신경망 모델의 매개변수와 컨트롤러의 매개변수가 수렴될 때의 신경망 모델의 셀 아키텍처, 신경망 모델의 아키텍처 및 하이퍼 파라미터의 조합을 출력하는 단계를 포함하는 시계열 예측을 위한 신경망 및 하이퍼 파라미터 검색 방법."}
{"patent_id": "10-2021-0110729", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 훈련하는 단계는, 입력 데이터를 이용하여 복수 개의 노드를 포함하는 비순환 방향 그래프를 통해 신경망 모델의 셀을 출력하는단계를 포함하고,상기 신경망 모델의 셀은, RNN 셀이고, 상기 입력 데이터는, 현재 타임 스텝의 데이터와 상기 현재 타임 스텝을 기준으로 이전 타임 스텝의 셀 출력이고, 상기 복수 개의 노드는 Tanh, ReLU, Sigmoid, Identity, Sum 및 Dot product를 포함하는 계산 연산 후보, 복수개의 노드의 사이에 연결된 엣지를 통해 정보 흐름을 나타내는, 것을 특징으로 하는 시계열 예측을 위한 신경망및 하이퍼 파라미터 검색 방법."}
{"patent_id": "10-2021-0110729", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 훈련하는 단계는, 타임 스텝 길이(time step length)와 히든 스테이트 사이즈(hidden state size)에 기초하여 신경망 모델의 탐색 공간을 생성하고, 상기 생성된 신경망 모델의 탐색 공간에 포함된 하이퍼 파라미터의 이산 공간과 연속 공간에 기초하여 하이퍼 파라미터를 표현하는 단계 를 포함하는 시계열 예측을 위한 신경망 및 하이퍼 파라미터 검색 방법."}
{"patent_id": "10-2021-0110729", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 훈련하는 단계는, 슈퍼 모델과 컨트롤러의 가중치를 초기화하고, 상기 가중치가 초기화된 컨트롤러를 사용하여 셀 아키텍처, 모델아키텍처 및 하이퍼 파라미터를 샘플링하고, 상기 샘플링된 하이퍼 파라미터에 따라 훈련을 통해 신경망 모델의매개변수를 업데이트하여 임시 가중치를 획득하는 단계 공개특허 10-2023-0028887-3-를 포함하고,상기 슈퍼 모델은, 계산 연산 후보에 기초하여 타임 스텝 길이와 히든 스테이트 사이즈를 이용하여 조합 가능한모델 아키텍처인, 것을 특징으로 하는 시계열 예측을 위한 신경망 및 하이퍼 파라미터 검색 방법."}
{"patent_id": "10-2021-0110729", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 업데이트하는 단계는, 원 스텝(one step) 확률적 경사 하강을 이용하여 상기 신경망 모델의 가중치를 업데이트하는 단계 를 포함하는 시계열 예측을 위한 신경망 및 하이퍼 파라미터 검색 방법."}
{"patent_id": "10-2021-0110729", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 출력하는 단계는, 상기 업데이트된 신경망 모델의 매개변수와 상기 업데이트된 컨트롤러의 매개변수가 수렴할 때까지 업데이트를통해 신경망 모델의 셀 아키텍처와 하이퍼 파라미터의 조합을 출력하는 단계 를 포함하는 시계열 예측을 위한 신경망 및 하이퍼 파라미터 검색 방법."}
{"patent_id": "10-2021-0110729", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "신경망 및 하이퍼 파라미터 검색 시스템에 있어서,데이터 세트 중 트레이닝 세트를 이용하여 컨트롤러에 의해 샘플링된 하이퍼 파라미터에 따라 신경망 모델을 훈련하는 훈련부; 강화학습에 기초하여 상기 훈련을 통해 업데이트된 신경망 모델의 매개변수의 임시 가중치를 이용하여 계산된데이터 세트 중 검증 세트의 손실값을 보상으로서 컨트롤러의 매개변수를 업데이트하는 컨트롤러 업데이트부;및상기 업데이트를 통해 신경망 모델의 매개변수와 컨트롤러의 매개변수가 수렴될 때의 신경망 모델의 셀 아키텍처, 신경망 모델의 아키텍처 및 하이퍼 파라미터의 조합을 출력하는 조합 출력부를 포함하는 신경망 및 하이퍼 파라미터 검색 시스템."}
{"patent_id": "10-2021-0110729", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "시계열 예측을 위한 신경 아키텍처 및 하이퍼 파라미터 검색 기술이 개시된다. 일 실시예에 따른 신경망 및 하 이퍼 파라미터 검색 시스템에 의해 수행되는 시계열 예측을 위한 신경망 및 하이퍼 파라미터 검색 방법은, 데이 터 세트 중 트레이닝 세트를 이용하여 컨트롤러에 의해 샘플링된 하이퍼 파라미터에 따라 신경망 모델을 훈련하 는 단계; 강화학습에 기초하여 상기 훈련을 통해 업데이트된 신경망 모델의 매개변수의 임시 가중치를 이용하여 계산된 데이터 세트 중 검증 세트의 손실값을 보상으로서 컨트롤러의 매개변수를 업데이트하는 단계; 및 상기 업 데이트를 통해 신경망 모델의 매개변수와 컨트롤러의 매개변수가 수렴될 때의 신경망 모델의 셀 아키텍처, 신경 망 모델의 아키텍처 및 하이퍼 파라미터의 조합을 출력하는 단계를 포함할 수 있다."}
{"patent_id": "10-2021-0110729", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "아래의 설명은 신경 아키텍쳐를 자동화하고 하이퍼 파라미터를 최적화하는 기술에 관한 것이다."}
{"patent_id": "10-2021-0110729", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일반적으로 사용되는 3가지의 NAS(Neural Architecture Search) 방법으로는 진화 알고리즘(evolutionary algorithm) 기반 NAS, 강화학습 기반 NAS, 경사 하강(gradient descent) 기반 NAS 방법이 있다. 진화 알고리 즘(evolutionary algorithm) 기반 NAS에 대하여 설명하기로 한다. 진화 알고리즘은 생물학적 진화에서 영감을 얻은 일반 인구 기반(population-based) 메타휴리스틱 최적화 알고리즘이다. 기존 최적화 알고리즘과 비교할 때 진화 알고리즘 기반의 NAS는 높은 견고성과 광범위한 적용성을 갖춘 성숙한 글로벌 최적화 방법이다. 이에, 기존 최적화 알고리즘이 해결하기 어려운 복잡한 문제를 효과적으로 해결할 수 있다. 진화 알고리즘 기반의 NAS는 먼저 네트워크 표현을 위해 인코딩 체계를 사용하고, 선택, 교차, 변형 및 업데이트 단계로 구성된다. 선택 단계는 크로스 오버를 위해 생성된 모든 네트워크에서 네트워크의 일부를 선택하는 것을 포함하며, 이는 약한 것을 제거하면서 우수한 성능의 신경 아키텍처를 유지하는 것을 목표로 한다. 선택 후, 두 개의 네트워크가 선택되어 각 부모의 유전 정보의 절반을 상속하는 새로운 자손 네트워크를 생성한다. 부모의 유전 정보가 다음 세대에 복제되어 유전됨에 따라 유전자 변이도 일어난다. 위의 단계를 완료하여 많은 새로운 네트워크가 생성되고 계산 리소스의 한계를 고려하여 이들 중 일부를 제거한다. 강화학습 기반 NAS에 대하여 설명하기로 한다. 컨트롤러는 일반적으로 검색 공간에서 새 아키텍처를 샘플링하 기 위해 각 단계에서 작업을 실행하고 컨트롤러의 샘플링 전략을 업데이트하기 위해 환경에서 보상 스칼라와 함 께 상태 관찰을 수신하는 RNN 이다. 환경은 표준 신경망 훈련 절차를 사용하여 컨트롤러가 생성한 네트워크를 훈련하고 평가한 후 해당 결과(예: 정확도)가 반환되는 것이다. ENAS(Effective Neural Architecture Searc h)는 모든 하위 아키텍처가 슈퍼 넷의 하위 그래프로 간주되는 매개변수 공유 전략을 채택한다. 이를 통해 이 러한 아키텍처는 매개변수를 공유할 수 있으므로 각 하위 모델을 처음부터 훈련할 필요가 없다. NAS, 경사 하강(gradient descent) 기반 NAS에 대하여 설명하기로 한다. DARTS는 Softmax 기능을 사용하여 이 산 공간을 완화하여 연속적이고 차별화 가능한 검색 공간에서 신경 아키텍처를 검색하는 최초의 경사 하강 기반 방법 중 하나이다. 아키텍처 검색 작업은 신경 아키텍처와 신경 아키텍처의 가중치의 공동 최적화로 변환된다. 이 두 가지 유형의 매개변수는 번갈아 최적화되어 이중 수준 최적화를 하는 것이다. AutoHAS는 아키텍처와 하이퍼 파라미터를 모두 효율적으로 검색한다. AutoHAS는 공유 네트워크 가중치와 강화 학습 컨트롤러를 교대로 업데이트하는 방법을 학습하여 아키텍처 후보 및 하이퍼 파라미터 후보에 대한 확률 분 포를 학습한다. 세 가지 유형의 NAS 방법은 최상의 셀 구조만 찾는다. 최적의 셀 구조를 찾는 과정에서 훈련 과정의 하이퍼 파 라미터 뉴런 수와 유사한 하이퍼 파라미터가 모두 고정된다. 그러나 이 방법은 최상의 구조를 찾은 후 다양한 하이퍼 파라미터를 수동으로 결정해야 한다. 또한, AutoHAS는 어느 정도 NAS와 HPO의 조합이지만 AutoHAS에 의 해 최적화된 하이퍼 파라미터는 훈련 중에 필요한 하이퍼 파라미터로 제한된다. 뉴런 수와 같은 하이퍼 파라미 터 검색은 해결되지 않는다."}
{"patent_id": "10-2021-0110729", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "신경 아키텍처와 하이퍼 파라미터를 최적화하는 방법 및 시스템을 제공할 수 있다. 강화학습을 이용하여 입력 데이터를 기반으로 신경망 모델의 셀을 자동으로 설계하고 하이퍼 파라미터의 조합을 탐색하는 방법 및 시스템을 제공할 수 있다."}
{"patent_id": "10-2021-0110729", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "신경망 및 하이퍼 파라미터 검색 시스템에 의해 수행되는 시계열 예측을 위한 신경망 및 하이퍼 파라미터 검색 방법은, 데이터 세트 중 트레이닝 세트를 이용하여 컨트롤러에 의해 샘플링된 하이퍼 파라미터에 따라 신경망 모델을 훈련하는 단계; 강화학습에 기초하여 상기 훈련을 통해 업데이트된 신경망 모델의 매개변수의 임시 가중 치를 이용하여 계산된 데이터 세트 중 검증 세트의 손실값을 보상으로서 컨트롤러의 매개변수를 업데이트하는 단계; 및 상기 업데이트를 통해 신경망 모델의 매개변수와 컨트롤러의 매개변수가 수렴될 때의 신경망 모델의 셀 아키텍처, 신경망 모델의 아키텍처 및 하이퍼 파라미터의 조합을 출력하는 단계를 포함할 수 있다. 상기 훈련하는 단계는, 입력 데이터를 이용하여 복수 개의 노드를 포함하는 비순환 방향 그래프를 통해 신경망 모델의 셀을 출력하는 단계를 포함하고, 상기 신경망 모델의 셀은, RNN 셀이고, 상기 입력 데이터는, 현재 타임 스텝의 데이터와 상기 현재 타임 스텝을 기준으로 이전 타임 스텝의 셀 출력이고, 상기 복수 개의 노드는 Tanh, ReLU, Sigmoid, Identity, Sum 및 Dot product를 포함하는 계산 연산 후보, 복수 개의 노드의 사이에 연결된 엣지를 통해 정보 흐름을 나타낼 수 있다. 상기 훈련하는 단계는, 타임 스텝 길이(time step length)와 히든 스테이트 사이즈(hidden state size)에 기초 하여 신경망 모델의 탐색 공간을 생성하고, 상기 생성된 신경망 모델의 탐색 공간에 포함된 하이퍼 파라미터의 이산 공간과 연속 공간에 기초하여 하이퍼 파라미터를 표현하는 단계를 포함할 수 있다. 상기 훈련하는 단계는, 슈퍼 모델과 컨트롤러의 가중치를 초기화하고, 상기 가중치가 초기화된 컨트롤러를 사용 하여 셀 아키텍처, 모델 아키텍처 및 하이퍼 파라미터를 샘플링하고, 상기 샘플링된 하이퍼 파라미터에 따라 훈 련을 통해 신경망 모델의 매개변수를 업데이트하여 임시 가중치를 획득하는 단계를 포함하고, 상기 슈퍼 모델은, 계산 연산 후보에 기초하여 타임 스텝 길이와 히든 스테이트 사이즈를 이용하여 조합 가능한 모델 아키 텍처일 수 잇다. 상기 업데이트하는 단계는, 원 스텝(one step) 확률적 경사 하강을 이용하여 상기 신경망 모델의 가중치를 업데 이트하는 단계를 포함할 수 있다. 상기 출력하는 단계는, 상기 업데이트된 신경망 모델의 매개변수와 상기 업데이트된 컨트롤러의 매개변수가 수 렴할 때까지 업데이트를 통해 신경망 모델의 셀 아키텍처와 하이퍼 파라미터의 조합을 출력하는 단계를 포함할 수 있다. 신경망 및 하이퍼 파라미터 검색 시스템은, 데이터 세트 중 트레이닝 세트를 이용하여 컨트롤러에 의해 샘플링 된 하이퍼 파라미터에 따라 신경망 모델을 훈련하는 훈련부; 강화학습에 기초하여 상기 훈련을 통해 업데이트된 신경망 모델의 매개변수의 임시 가중치를 이용하여 계산된 데이터 세트 중 검증 세트의 손실값을 보상으로서 컨 트롤러의 매개변수를 업데이트하는 컨트롤러 업데이트부; 및 상기 업데이트를 통해 신경망 모델의 매개변수와 컨트롤러의 매개변수가 수렴될 때의 신경망 모델의 셀 아키텍처, 신경망 모델의 아키텍처 및 하이퍼 파라미터의 조합을 출력하는 조합 출력부를 포함할 수 있다."}
{"patent_id": "10-2021-0110729", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "시계열 데이터를 처리할 때 데이터에 가장 적합한 RNN 셀 아키텍처를 찾을 수 있을 뿐만 아니라 가장 적합한 아 키텍처와 관련된 하이퍼 파라미터와 훈련 과정에 필요한 하이퍼 파라미터를 찾을 수 있는 강화학습 방법을 사용 하여 자동으로 예측을 수행할 수 있다. 자동 검색된 RNN의 셀 아키텍처가 다양하기 때문에 LSTM 또는 GRU를 직접 사용하는 것보다 정확도를 더욱 향상 시킬 수 있다."}
{"patent_id": "10-2021-0110729", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 실시예를 첨부한 도면을 참조하여 상세히 설명한다. 실시예에서는 인공지능 자동화와 하이퍼 파라미터 최적화를 위한 동작에 대하여 설명하기로 한다. 강화학습 방 법을 사용하여 입력 데이터를 기반으로 신경망 모델(예를 들면, RNN)의 셀을 자동으로 설계하고 하이퍼 파라미 터 조합을 탐색할 수 있다. 이때, 훈련 중에 트레이닝 세트의 데이터는 신경망 모델의 매개변수를 업데이트하 는데 사용하고, 검증 세트의 데이터를 강화학습을 이용하여 컨트롤러의 매개변수를 업데이트하는데 사용될 수 있다. 두 매개변수는 수렴될 때까지 교대로 업데이트될 수 있으며, 최종적으로 신경망 모델(예를 들면, RNN)의셀 아키텍처와 최적화된 하이퍼 파라미터의 조합을 출력할 수 있다. 도 1은 일 실시예에 따른 신경 아키텍처 및 하이퍼 파라미터 검색 시스템에서 비순환 방향 그래프를 생성하는 동작을 설명하기 위한 도면이다. 신경 아키텍처 및 하이퍼 파라미터 검색 시스템(이하 '검색 시스템'으로 기재하기로 함)은 데이터 세트를 트레 이닝 세트(training set), 검증 세트(validation set) 및 테스트 세트(test set)로 구분할 수 있다. 데이터 세트를 구분할 때 타임 스텝 길이(time step length)에 따라 다르게 트레이닝 세트, 검증 세트, 세트로 구분될 수 있다. 그런 다음, 검색 시스템은 복수 개의 노드를 포함하는 비순환 방향 그래프를 생성할 수 있다. 이때, 각 노드는 계산 연산 후보를 나타내고, 각 엣지는 정보 흐름을 나타낸다. 실시예에서는 강화학습을 사용하고 컨트롤러로 LSTM을 사용하는 것을 예를 들어 설명하기로 한다. 검색 시스템 은 먼저 슈퍼 모델과 컨트롤러의 가중치를 초기화하고, 초기화된 슈퍼 모델과 컨트롤러를 반복적으로 훈련시킬 수 있다. 이때, 검색 시스템은 훈련 반복 과정에서, 먼저 컨트롤러를 사용하여 학습에 필요한 셀 아키텍처, 모 델 아키텍처 및 하이퍼 파라미터를 샘플링할 수 있고, 샘플링된 하이퍼 파라미터에 따라 구성된 신경망 모델을 훈련시킬 수 있다. 강화학습을 통해 보상으로 도출된 검증 세트의 결과로 컨트롤러를 업데이트할 수 있다. 마 지막으로 검색 시스템은 확률적 경사 하강(Stochastic Gradient Descent; SGD)과 트레이닝 세트를 사용하여 하 위 모델의 매개변수를 업데이트할 수 있다. 최종 수렴 후 학습에 필요한 최상의 셀 아키텍처, 모델 아키텍처 및 하이퍼 파라미터가 생성될 수 있다. 더욱 상세하게는, 데이터 세트를 트레이닝 세트, 검증 세트 및 테스트 세트로 구분될 수 있다. 먼저, 타임 스 텝 길이(time step length) 후보를 10, 20, 30으로 설정하기로 한다. 데이터를 다른 타임 스텝 길이에 따라 입 력 데이터 및 라벨을 및 형식으로 처리한 다음, 다른 타임 스텝 길이에 따라 데이터 세트가 분할될 수 있다. 도 1을 참고하면, 복수 개의 노드를 포함하는 비순환 방향 그래프가 생성할 수 있다. 예를 들면, 5개의 노드로 구성된 비순환 방향 그래프가 생성될 수 있다. 이때, 각 노드는 계산 연산 후보를 나타내고, 엣지는 정보 흐름 을 나타낸다. 두 노드 간 계산 연산 후보에는 Tanh, ReLU, Sigmoid, Identity, Sum 및 Dot product 등이 포함될 수 있다. 두 노드가 highway connections 형식으로 연결될 수 있다. 각 셀의 입력 데이터는 와 이다. 입력 데이터를 이용하여 복수 개의 노드를 포함하는 신경망 모델의 셀에 대한 비순환 방향 그래프가 생성될 수 있다. 이때, 입력 데이터는 현재 타임 스텝의 입력과 현재 시간을 기준으로 이전 타임 스텝의 셀 출력을 의미 할 수 있다. 타임 스텝 길이와 히든 스테이트 사이즈(신경망 모델의 뉴런 개수)에 기초하여 신경망 모델의 탐색 공간이 생성 될 수 있다. 예를 들면, 타임 스텝 길이는 {10, 20, 30}이고, 히든 스테이트 사이즈는 {16, 32, 64, 128}로 설정될 수 있다. 하이퍼 파라미터의 탐색 공간에는 이산 공간과 연속 공간이 있다. 이에 따라, 하이퍼 파라미 터는 공식으로 표현될 수 있다. 만약, 연속 공간이라면 , 이산 공간이 라면 이다. 이산 공간은 최적화될 수 있으며, 최적화 프로그램은 Adam, SGD, RMSprop 등이 사 용될 수 있다. 배치 사이즈(batch size)의 후보는 {32, 64, 128}이다. 학습률(Learning rate)은 연속적인 공 간이므로 이산적 표현으로 변환하기 위해 앞서 설명된 방법을 사용하여 학습률의 basis가 1e-1, 1e-2, 1e-3 및 1e-4로 선택될 수 있다. 훈련을 시작할 때, 먼저 슈퍼 모델과 컨트롤러의 가중치가 초기화될 수 있다. 도 2를 참고하면, 슈퍼 모델과 후보 셀 아키텍처의 관계를 설명하기 위한 도면이다. 도 2는 슈퍼 모델과 후보 셀 아키텍처의 관계를 나타낸 것이다. 슈퍼 모델에는 모든 계산 연산 후보가 포함되 어 있다. 후보 셀 아키텍처는 슈퍼 모델에서 샘플링된 하위 모델로 간주될 수 있다. 이때, 하위 모델은 슈퍼모델의 매개변수와 공유하게 된다. 여기서 타임 스텝 길이의 옵션은 10, 20, 30이고, 히든 스테이트 사이즈의 옵션은 16, 32, 64, 128이기 때문에 서로 다른 모델 아키텍처의 각 조합에 대한 12개의 수퍼 모델이 생성될 수 있다. 먼저 컨트롤러를 사용하여 셀 아키텍처 , 모델 아키텍처 및 하이퍼 파라미터 가 샘플링될 수 있다. 그런 다음 트레이닝 세트의 데 이터를 사용하여 하이퍼 파라미터 에 따라 반복적 훈련을 통해 자체 신경망 모델의 매개변수 가 업데 이트됨에 따라 임시 가중치 가 획득될 수 있다. 여기서 임시 가중치는 검증 세트의 손실 값을 얻기 위하 여 사용되며 컨트롤러를 업데이트하기 위한 보상으로 사용될 수 있다. 이때, 컨트롤러를 업데이트하기 위해 강 화학습 알고리즘이 사용될 수 있다. 컨트롤러가 업데이트된 후, 임시 가중치를 포기해도 하위 모델의 가중치에 는 영향을 주지 않는다. 보상을 받고 컨트롤러를 업데이트 한 후 원 스텝(one step) 확률적 경사 하강을 사용 하여 신경망 모델의 가중치가 업데이트될 수 있다. 업데이트를 통해 최종 수렴 후, 최종 셀 아키텍처 , 모델 아키텍처 및 하이퍼 파라미터 가 출력될 수 있다. 도 3은 일 실시예에 따른 신경 아키텍처 및 하이퍼 파라미터 검색 시스템의 구성을 설명하기 위한 블록도이고, 도 4는 신경 아키텍처 및 하이퍼 파라미터 검색 시스템에서 신경 아키텍처 및 하이퍼 파라미터 검색 방법을 설 명하기 위한 흐름도이다. 신경 아키텍처 및 하이퍼 파라미터 검색 시스템의 프로세서는 훈련부, 컨트롤러 업데이트부 및 조합 출력부를 포함할 수 있다. 이러한 프로세서의 구성요소들은 신경 아키텍처 및 하이퍼 파라미터 검색 시스템에 저장된 프로그램 코드가 제공하는 제어 명령에 따라 프로세서에 의해 수행되는 서로 다른 기능들 (different functions)의 표현들일 수 있다. 프로세서 및 프로세서의 구성요소들은 도 4의 신경 아키텍처 및 하이퍼 파라미터 검색 방법이 포함하는 단계들(410 내지 430)을 수행하도록 신경 아키텍처 및 하이퍼 파라미터 검색 시스템을 제어할 수 있다. 이때, 프로세서 및 프로세서의 구성요소들은 메모리가 포함하는 운영체제의 코 드와 적어도 하나의 프로그램의 코드에 따른 명령(instruction)을 실행하도록 구현될 수 있다. 프로세서는 신경 아키텍처 및 하이퍼 파라미터 검색 방법을 위한 프로그램의 파일에 저장된 프로그램 코드를 메 모리에 로딩할 수 있다. 예를 들면, 신경 아키텍처 및 하이퍼 파라미터 검색 시스템에서 프로그램이 실행되면, 프로세서는 운영체제의 제어에 따라 프로그램의 파일로부터 프로그램 코드를 메모리에 로딩하도록 신경 아키텍 처 및 하이퍼 파라미터 검색 시스템을 제어할 수 있다. 이때, 훈련부, 컨트롤러 업데이트부 및 조합 출력부 각각은 메모리에 로딩된 프로그램 코드 중 대응하는 부분의 명령을 실행하여 이후 단계들(410 내지 430)을 실행하기 위한 프로세서의 서로 다른 기능적 표현들일 수 있다. 단계에서 훈련부는 데이터 세트 중 트레이닝 세트를 이용하여 컨트롤러에 의해 샘플링된 하이퍼 파라 미터에 따라 신경망 모델을 훈련할 수 있다. 훈련부는 입력 데이터를 이용하여 복수 개의 노드를 포함하 는 비순환 방향 그래프를 통해 신경망 모델의 셀을 출력할 수 있다. 훈련부는 타임 스텝 길이(time step length)와 히든 스테이트 사이즈(hidden state size)에 기초하여 신경망 모델의 탐색 공간을 생성하고, 생성된 신경망 모델의 탐색 공간에 포함된 하이퍼 파라미터의 이산 공간과 연속 공간에 기초하여 하이퍼 파라미터를 표 현할 수 있다. 훈련부는 슈퍼 모델과 컨트롤러의 가중치를 초기화하고, 가중치가 초기화된 컨트롤러를 사 용하여 셀 아키텍처, 모델 아키텍처 및 하이퍼 파라미터를 샘플링할 수 있다. 훈련부는 컨트롤러에 의해 샘플링된 하이퍼 파라미터에 따라 훈련을 통해 신경망 모델의 매개변수를 업데이트하여 임시 가중치를 획득할 수 있다. 단계에서 컨트롤러 업데이트부는 강화학습에 기초하여 훈련을 통해 업데이트된 신경망 모델의 매개변 수의 임시 가중치를 이용하여 계산된 데이터 세트 중 검증 세트의 손실값을 보상으로서 컨트롤러의 매개변수를 업데이트할 수 있다. 컨트롤러 업데이트부는 원 스텝(one step) 확률적 경사 하강을 이용하여 신경망 모 델의 가중치를 업데이트할 수 있다. 단계에서 조합 출력부는 업데이트를 통해 신경망 모델의 매개변수와 컨트롤러의 매개변수가 수렴될 때의 신경망 모델의 셀 아키텍처, 신경망 모델의 아키텍처 및 하이퍼 파라미터의 조합을 출력할 수 있다. 조합 출력부는 업데이트된 신경망 모델의 매개변수와 업데이트된 컨트롤러의 매개변수가 수렴할 때까지 업데이트를 통해 신경망 모델의 셀 아키텍처와 하이퍼 파라미터의 조합을 출력할 수 있다. 도 5는 일 실시예에 따른 신경 아키텍처 및 하이퍼 파라미터 검색 시스템의 상세 동작을 설명하기 위한 흐름도 이다. 단계에서 검색 시스템은 슈퍼 모델의 가중치와 컨트롤러의 가중치를 랜덤하게 초기화할 수 있다. 단계에서 검색 시스템은 수렴하는지 여부를 판단할 수 있다. 검색 시스템은 신경망 모델의 매개변수와 컨 트롤러의 매개변수를 수렴할 때까지 교대로 업데이트할 수 있다. 이때, 단계에서 검색 시스템은 수렴하는 것으로 판단될 경우, 최종 셀 아키텍처, 모델 아키텍처 및 하이퍼 파라미터를 도출할 수 있다. 다시 말해서, 검색 시스템은 업데이트를 통해 신경망 모델의 셀 아키텍처, 신경망 모델의 아키텍처 및 하이퍼 파라미터의 조 합을 출력할 수 있다. 검색 시스템은 수렴하지 않는 것으로 판단됨에 따라 단계을 수행할 수 있다. 단계에서 검색 시스템은 컨트롤러로부터 셀 아키텍처, 모델 아키텍처 및 하이퍼 파라미터를 샘플링할 수 있다. 검색 시스템은 가중치가 초기화된 컨트롤러를 사용하여 셀 아키텍처, 모델 아키텍처 및 하이퍼 파라미터 를 샘플링할 수 있다. 단계에서 검색 시스템은 임시 가중치를 계산할 수 있다. 검색 시스템은 데이터 세트 중 트레이닝 세트를 이용하여 컨트롤러에 의해 샘플링된 하이퍼 파라미터에 기초하여 훈련을 통해 신경망 모델의 매개변수를 업데이 트하여 임시 가중치를 획득할 수 있다. 단계에서 검색 시스템은 컨트롤러 업데이트에 대한 보상으로서 검증 세트의 손실을 계산할 수 있다. 검색 시스템은 강화학습을 이용하여 획득된 임시 가중치를 통해 계산된 데이터 세트 중 검증 세트의 손실값을 보상으 로서 컨트롤러를 업데이트할 수 있다. 단계에서 검색 시스템은 원 스텝 확률적 경사 하강(SGD)을 이용하여 모델의 가중치를 최적화할 수 있다. 검색 시스템은 확률적 경사 하강과 트레이닝 세트를 사용하여 하위 모델의 매개변수를 업데이트할 수 있다. 이 후, 다시 520 단계를 수행하게 된다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 예를 들어, 프로 세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴 퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상 의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설"}
{"patent_id": "10-2021-0110729", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치 는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치에 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터 는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2021-0110729", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2021-0110729", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 신경 아키텍처 및 하이퍼 파라미터 검색 시스템에서 비순환 방향 그래프를 생성하는 동작을 설명하기 위한 도면이다. 도 2는 일 실시예에 따른 신경 아키텍처 및 하이퍼 파라미터 검색 시스템에서 슈퍼 모델과 후보 셀 아키텍처의 관계를 설명하기 위한 도면이다. 도 3은 일 실시예에 따른 신경 아키텍처 및 하이퍼 파라미터 검색 시스템의 구성을 설명하기 위한 블록도이다. 도 4는 신경 아키텍처 및 하이퍼 파라미터 검색 시스템에서 신경 아키텍처 및 하이퍼 파라미터 검색 방법을 설 명하기 위한 흐름도이다. 도 5는 일 실시예에 따른 신경 아키텍처 및 하이퍼 파라미터 검색 시스템의 상세 동작을 설명하기 위한 흐름도 이다."}
