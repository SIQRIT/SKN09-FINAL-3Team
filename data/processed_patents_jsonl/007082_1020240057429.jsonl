{"patent_id": "10-2024-0057429", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0160517", "출원번호": "10-2024-0057429", "발명의 명칭": "가상 환경에서의 멀티 모달 데이터 스트림 기반 인공 지능 개입 시스템 및 방법", "출원인": "티엠알더블유 파운데이션 아이피 에스에이알엘", "발명자": "블랙 로버트 해리"}}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "방법에 있어서, 하나 이상의 가상 환경 시스템에서 인터랙티브 가상 회의 플랫폼(interactive virtual conference platform)에의해, 복수의 가상 환경을 저장하는 단계; 상기 하나 이상의 가상 환경 시스템에서 상기 인터랙티브 가상 회의 플랫폼에 의해, 복수의 정황 시나리오(contextual scenario)를 저장하는 단계; 상기 하나 이상의 가상 환경 시스템에서 상기 인터랙티브 가상 회의 플랫폼에 의해, 복수의 감정적 단서를 저장하는 단계; 상기 인터랙티브 가상 회의 플랫폼에서, 가상 회의에 참여하기 위한 복수의 요청을 수신하는 단계; 상기 인터랙티브 가상 회의 플랫폼에 의해, 상기 복수의 요청에 대응하는 복수의 세션을 연결하는 단계 - 각각의 세션은, 복수의 비디오 또는 오디오 스트림을 집합적으로 형성하는 적어도 하나 이상의 비디오 또는 오디오스트림을 포함하는 입력 데이터를 포함함 - ; 상기 인터랙티브 가상 회의 플랫폼의 서버에 의해, 상기 복수의 비디오 또는 오디오 스트림을 분석하는 단계; 상기 인터랙티브 가상 회의 플랫폼에 의해, 상기 입력 데이터로부터 상기 복수의 정황 시나리오 중의 정황 시나리오 또는 하나 이상의 감정적 단서를 자동으로 감지하는 단계; 상기 정황 시나리오 또는 감정적 단서를 감지하는 것에 응답하여 상기 인터랙티브 가상 회의 플랫폼에 의해, 상기 분석된 입력 데이터, 상기 감지된 정황 시나리오 또는 상기 감지된 하나 이상의 감정적 단서에 기초하여 개입(intervention) 데이터베이스로부터 개입을 선택하는 단계; 상기 정황 시나리오를 감지하는 것에 응답하여 상기 인터랙티브 가상 회의 플랫폼에 의해, 상기 개입 데이터베이스로부터 상기 개입을 읽어들이는 단계; 및상기 인터랙티브 가상 회의 플랫폼에 의해, 상기 개입 데이터베이스로부터 읽어들인 상기 개입에 기초하여 상기가상 회의에 개입하는 단계 - 상기 개입은 출력 오디오 신호 또는 출력 비디오 신호에 대한 적어도 하나의 변경을 포함함 - 를 포함하는, 방법."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서, 상기 인터랙티브 가상 회의 플랫폼에 의해, 상기 복수의 정황 시나리오에 대응하는 하나 이상의 데이터 세트를하나 이상의 신경망에 입력하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서, 상기 하나 이상의 신경망은 컨볼루션 신경망(CNN; convolutional neural network)과 순환 신경망(RNN;recurrent neural network) 중 하나 이상을 포함하는 것인, 방법."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 2에 있어서, 상기 개입에 대한 피드백을 수신하고, 향후 개입에 적용하기 위해 상기 피드백을 상기 하나 이상의 신경망에 적공개특허 10-2024-0160517-3-용하여 상기 하나 이상의 신경망을 트레이닝하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서, 상기 피드백은 적어도 하나의 사용자로부터의 랭킹을 포함하는 것인, 방법."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 5에 있어서, 상기 피드백은 상기 비디오 또는 오디오 스트림을 통해 감지된 하나 이상의 사용자로부터의 물리적 반응을 포함하는 것인, 방법."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 1에 있어서, 상기 개입은 가상 카메라 각도에 대한 변경, 샷 크기, 및 카메라 모션 중, 적어도 하나를 포함하는 것인, 방법."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 1에 있어서, 상기 개입은 상기 출력 비디오 신호의 밝기를 변경하는 것, 상기 출력 오디오 신호의 톤을 변경하는 것, 및 상기 출력 비디오 신호의 색조를 변경하는 것, 중 하나 이상을 포함하는 것인, 방법."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 1에 있어서, 상기 정황 시나리오를 감지하는 것에 응답하여 상기 인터랙티브 가상 회의 플랫폼에 의해, 상기 개입 데이터베이스로부터 상기 개입을 선택하는 단계는 하나 이상의 사용자 프로필에 기초하여 상기 개입을 선택하는 단계를더 포함하는 것인, 방법."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 9에 있어서, 상기 하나 이상의 사용자 프로필은 정황 시나리오를 미리 선택된 개입 기준과 상관시키는 하나 이상의 사용자설정을 포함하는 것인, 방법."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "에 있어서, 상기 복수의 개입은 하나 이상의 카메라 뷰에 대한 변경 및 환경 변경을 포함하는 것인, 컴퓨터 시스템."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "청구항 12에 있어서, 상기 인터랙티브 화상 회의 플랫폼은 또한, 상기 개입에 대한 피드백을 수신하고, 향후 개입에 적용하기 위해상기 피드백을 하나 이상의 신경망에 적용하여 상기 하나 이상의 신경망을 트레이닝하도록 구성되는 것인, 컴퓨터 시스템."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "청구항 11에 있어서, 타이핑 속도, 타이핑 볼륨(소거), 손 제스처, 말하기 시간의 양, 얼굴 (미세) 표정, 마우스/스와이프 속도, 지리적 위치, 브라우저, 로딩 시간, FPS/탭 포커스, 회의 제목, 참가자 수, 머리 포지션, 언어 독성, 디바이스,또는 발언 리듬 중, 하나 이상을 포함하는 입력을 수신하도록 구성된 네트워크 인터페이스를 더 포함하고, 상기인터랙티브 화상 회의 플랫폼은 상기 입력 중 하나 이상에 기초하여 상기 개입을 선택하도록 구성되는 것인, 컴퓨터 시스템."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "청구항 11에 있어서, 상기 개입은 상기 출력 비디오 신호의 밝기를 변경하는 것, 상기 출력 오디오 신호의 톤을 변경하는 것, 및 상기 출력 비디오 신호의 색조를 변경하는 것, 중 하나 이상을 포함하는 것인, 컴퓨터 시스템."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "청구항 11에 있어서, 상기 정황 시나리오에 대응하는 상기 개입은 또한 하나 이상의 사용자 프로필에 대응하는 것인, 컴퓨터 시스템."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "청구항 16에 있어서, 상기 하나 이상의 사용자 프로필은 정황 시나리오를 미리 선택된 개입 기준과 상관시키는 하나 이상의 사용자설정을 포함하는 것인, 컴퓨터 시스템."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "명령어를 포함하는 비일시적 컴퓨터 판독가능 매체에 있어서, 상기 명령어는 프로세서 상에서 수행될 수있으며, 상기 명령어는: 가상 회의에 참여하기 위한 복수의 요청에 기초하여 복수의 사용자를 상기 가상 회의에 연결하고 - 각각의 세션은, 복수의 비디오 또는 오디오 스트림을 집합적으로 형성하는 하나 이상의 비디오 또는 오디오 스트림을 포함함 - ; 시나리오 데이터베이스에 저장된 복수의 정황 시나리오로부터의 정황 시나리오를 감지하기 위해 상기 복수의 비디오 또는 오디오 스트림을 분석하고; 상기 시나리오 데이터베이스에 저장된 상기 복수의 정황 시나리오 중의 상기 정황 시나리오를 자동으로 감지하고; 상기 정황 시나리오에 기초하여 개입 데이터베이스로부터 개입을 선택하고; 상기 개입 데이터베이스로부터 상기 정황 시나리오에 기초하여 상기 개입을 읽어들이고; 공개특허 10-2024-0160517-5-상기 개입 데이터베이스로부터 읽어들인 상기 개입에 기초하여 상기 가상 회의에 개입하는 것 - 상기 개입은 출력 오디오 신호 또는 출력 비디오 신호에 대한 적어도 하나의 변경을 포함함 - 을 포함하는 것인, 비일시적 컴퓨터 판독가능 매체."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "청구항 18에 있어서, 상기 정황 시나리오를 감지하기 위해 컨볼루션 신경망(CNN)과 순환 신경망(RNN) 중 하나 이상을 포함하는 하나이상의 신경망을 분석하기 위한 명령어를 더 포함하는, 비일시적 컴퓨터 판독가능 매체."}
{"patent_id": "10-2024-0057429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "청구항 18에 있어서, 상기 정황 시나리오에 대응하는 상기 개입이 또한 하나 이상의 사용자 프로필에 대응하기 위한 명령어를 더 포함하는, 비일시적 컴퓨터 판독가능 매체."}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 실시예는 가상 환경에서의 개선된 화상 회의 시스템을 위한 시스템 및 방법을 포함한다. 실시예는 가 상 회의에서의 참가자 그룹 내의 공감 및 감정을 감지하기 위해 인공 지능, 대규모 언어 모델 및 머신 러닝을 사 용할 수 있다. 오디오/시각적 스트림, 기타 센서 데이터 및 트레이닝 세트와 같은 다양한 입력이 지루함, 행복 및 슬픔과 같은 다양한 감정적 시나리오를 감지하는 데 사용될 수 있다. 실시예는 이러한 시나리오를 감지하고 더 많은 참여를 독려하거나 사람들을 깨우는 등 참가자의 바람직한 행동 변화에 기초하여 개입을 취할 수 있다. 개입은 카메라 관점의 변경 또는 가상 환경에 대한 환경 변경일 수 있다. 개입은 또한, 심각한 비즈니스 미팅 대 친구들과의 모임과 같은, 정황 시나리오에 따라 달라질 수 있다."}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 컴퓨팅 환경에서 콘텐츠를 생성하는 것, 보다 구체적으로, 인공 지능(AI; artificial intelligence) 을 사용하여 화상 회의(video conferencing) 기술을 향상시키기 위한 시스템 및 방법에 관한 것이다. AI는 전 형적인 화자(speaker) 그리드나 화자에 대한 집중을 넘어 보다 인터랙티브한 시청 경험을 생성함으로써 더욱 매 력적인 경험을 개발하는 데 사용될 수 있다."}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "화상 회의는 오늘날의 세계에서 커뮤니케이션 및 협업을 위한 필수적인 도구가 되었다. 그러나, 종래의 화상 회의 포맷은 종종 참여도가 낮고 비생산적인 회의를 초래하여 참가자들로부터의 관심 및 주의 부족으로 이어진 다. 지루한 화상 회의는 팀 구성원들 간의 협업 및 커뮤니케이션을 감소시킬 뿐만 아니라 회의 목표를 달성하 는 데 있어서의 생산성 및 효율성을 감소시킬 수 있다. 따라서, 참가자들의 흥미와 참여도를 더 높이도록 화상 회의를 개선할 필요가 있다. 이는 사용자 경험을 향상시키고 적극적인 참여를 장려하는 혁신적인 기술 및 특징 의 개발을 통해 달성될 수 있으며, 그 결과 보다 생산적이고 성공적인 회의가 될 수 있다. 원격 근무와 가상 협업이 점점 더 보편화됨에 따라 종래의 화상 회의 포맷의 한계는 더욱 명백해졌다. 참가자 들은 종종 장기간의 또는 구조화되지 않은 회의 동안 집중과 참여를 유지하는 데 어려움을 겪으며, 이로 인해 생산성이 감소되고 협업 및 문제 해결을 위한 기회를 놓치게 된다. 더욱이, 대면 상호작용 및 비언어적(nonverbal) 감정적 단서(emotional cue)의 결여로 참가자들이 친밀감을 쌓 고 신뢰를 구축하는 것을 어렵게 할 수 있으며, 잠재적인 오해와 갈등으로 이어질 수 있다. 이는 참가자들 간 의 보다 역동적이고 상호작용적인 커뮤니케이션을 촉진하는 화상 회의에 대한 새로운 접근법에 대한 필요성을 강조한다. 이러한 과제에 응답하여, 전반적인 회의 경험을 개선할 수 있는 혁신적인 화상 회의 기술에 대한 요구가 증가하 고 있다. 이러한 기술 중 일부는 가상 배경, 증강 현실 오버레이, 인터랙티브 화이트보드 및 실시간 협업 도구 를 포함한다. 이러한 특징은 참가자들을 참여시키고 보다 의미있는 대화와 협업을 용이하게 하며 궁극적으로 생산성 및 의사 결정을 향상시키는 데 도움이 될 수 있다. 이와 같이, 화상 회의 기술을 보다 매력적이고 상호작용적이며 생산적으로 만들기 위해 화상 회의 기술에 대한 지속적인 투자와 개발이 명확히 필요하다. 그리 함으로써, 가상 회의 경험을 개선하고 원격 또는 하이브리드 작업 환경에서 참가자들 간의 보다 나은 커뮤니케이션, 협업 및 문제 해결을 촉진할 수 있다. 빈번한 온라인 회의는 지루해지고 반복적으로 될 수 있으며, 그러한 회의에 참여하려는 동기를 감소시킨다. 인 간은 본래 감정적인 존재이고 다양성을 즐기는 경향이 있기 때문이며, 이를 달성할 수 있는 많은 유형의 자극이있다. 그러나, 가상 환경에서의 커뮤니케이션을 가능하게 하는 현행 기술은 대부분 적합한 FPS, 음질 또는 그 래픽 품질을 허용하는 것과 같은 원활한 사용자 경험을 가능하게 하기에 충분한 기술적 능력을 제공하는 것에 초점을 맞추고 있다. 따라서, 사용자가 가상 환경에서 상호작용할 때 인간의 다양한 감정을 자극하여 사용자의 에너지, 집중력, 동기 부여 및 생산성까지도 개선할 수 있게 하는 기술적 대안을 개발할 필요성이 있다. 가상 환경은 사무실이나 레스토랑과 같은 실내 환경 및 공원이나 농장과 같은 실외 환경을 포함할 수 있다. 종래 기술 솔루션은 매력적인 화상 회의 또는 가상 회의를 생성하기 위한 일관된 솔루션을 제공하지 않는 몇 가 지 기본 빌딩 블록을 제공한다. 화상 회의는 Zoom 및 Teams와 같은 애플리케이션으로 당업계에 잘 알려져 있다. 그러나, 가상 회의는 전통적인 화상 회의 뿐만 아니라, 레스토랑이나 회의실과 같은 실세계 시나리오를 에뮬레이트하는 더 정교한 가상 환경도 포함할 수 있다. 본 개시의 일부 실시예는 입력을 원하는 출력으로 프 로세싱함으로써 이벤트의 분위기에 영향을 미치도록 맞춤화된 동적 정보에 기초하여 개선된 자동화된 응답형 AI 로직을 제공한다. 실시예는 복수의 입력을 출력으로 프로세싱하는 멀티 모달 데이터 스트림 기반 AI 카메라 시 스템 및 방법을 포함할 수 있다. 입력은 사용자가 가상 이벤트에 참여할 때 그의 대응하는 카메라 및 마이크에 의해 제공되는 사용자의 사운드 또는 비디오 피드일 수 있다. 가상 환경에서의 하나 이상의 가상 방송 카메라 에 의해 추가적인 입력이 캡처될 수 있다. 각각의 사용자는 가상 환경에서 제시되는 자신의 카메라 피드의 형 태로 사용자 그래픽 표현 또는 아바타를 갖는다. 사용자와의 카메라 피드의 배경은 그 배경이 제거되게 할 수 있으며, 그리하여 아바타는 가상 환경으로 실시간으로 보내지는 사용자의 컷아웃일 뿐이다. 시스템은 가상 환 경을 저장하는 서버를 포함하고, 복수의 참가자들로부터의 수신된 카메라 피드를 프로세싱 및 배포하도록 구성 된다. 시스템은 미국 특허 번호 제11,218,522호에 설명된 바와 같이, P2P, 클라이언트-서버, 또는 이들의 조합 과 같은 방식을 사용하는 WebRTC 아키텍처를 사용할 수 있다. 가상 환경은 사용자들이 원격으로 만나고 상호작용할 수 있는 기회를 제공한다. 이러한 가상 환경에서 상호작 용할 때, 사용자들은 가상 환경에서의 액션을 따를 수 있도록 자신의 카메라 뷰를 능동적으로 조정할 필요가 있 을 수 있다. 예를 들어, 많은 사용자들이 있는 원형 테이블에서, 한 화자(speaker)로부터 다른 화자로 차례를 바꿀 때, 사용자들은 새로운 화자를 볼 수 있도록 자신의 카메라 뷰를 조정하기 위해 한 번 이상의 움직임을 행 할 필요가 있을 수 있다. 그 전체가 참조에 의해 여기에 포함되어 있는, Yerli에 의한 특허 출원 번호 제 17/060,516호에서는, 사용자가 직접 볼 수 있는 가상 환경에서의 액션을 캡처하는 데 사용될 수 있는 가상 방송 카메라를 제안한다. 그러나, 이 경우, 가상 방송 카메라를 관리하기 위해 클라이언트 디바이스가 필요하며, 또 한 사람의 수동적인 노력도 필요로 한다. 사용자에게 제공되는 라이브 액션을 캡처하는 것 외에도, 가상 방송 카메라와 같은 가상 카메라에 의해 촬영된 영상은 실제 또는 가상 이벤트의 리플레이를 생성하기 위해 더 사용될 수 있다. 통상적인 방법은 비디오 스트 림이 서버 내의 저장 디바이스로 전송되어 컨트롤러가 청중이 관심가질 만한 샷을 검토하고 수동으로 선택 및/ 또는 편집해야 하며, 이는 많은 노력을 수반한다. 마지막으로, 통상적인 가상 이벤트는 회의를 지루하고 단조 롭게 만들 수 있는 미리 정의된 특정 순서를 따르며, 이들 전부 이러한 이벤트의 원격 특성에 의해 악화되어 피 로와 지루함을 초래한다. 본 개시의 특정 양상에 따르면, 복수의 입력을 출력으로 프로세싱하는 멀티 모달 데이터 스트림 기반 AI 카메라 시스템을 위한 시스템 및 방법이 제공된다. 입력은 가상 환경과 사용자에 의해 공유되는 카메라 및 오디오 피 드일 수 있다. 시스템은 가상 환경을 저장하는 서버를 포함하고, 복수의 참가자로부터의 수신된 카메라 피드를 프로세싱 및 배포하도록 구성된다. 시스템은 수신된 입력에 따라 다양한 가능한 출력의 구성을 가능하게 한다. 시스템은 사전설정(preset) 또는 정황 시나리오(contextual scenario)를 더 사용하여, 출력의 프로그래밍적 (programmatic) 선택을 가능하게 하고 그리고/또는 출력을 수정한다. 출력은 가상 환경에서 그의 하나 이상의 표면에, 예컨대 이벤트가 방송되고 있는 가상 환경의 가상 스크린에 제시될 수 있고, 더욱 매력적인 가상 환경 을 생성하도록 구성될 수 있다. 카메라 출력은, 수정되지 않은 라이브 카메라 피드; 실시간 카메라 조정의 피 드; 또는 예컨대 이벤트의 가장 관련있는 장면을 포함하는 \"베스트(best)\"를 생성하기 위한, 장면의 리플레이 중, 하나 이상의 형태를 취할 수 있다. 예를 들어, 시스템이 참가자가 열정과 흥분을 표현하고 있다는 감정적 단서를 감지하는 경우, 시스템은 이벤트 의 분위기를 이 감정에 맞게 조정할 수 있다. 이는 조명 및 사운드 효과를 조정하고 화자 톤(tone) 및 신체 언 어를 변경하고 대화의 속도를 조정함으로써 행해질 수 있다. 마찬가지로, 시스템이 참가자가 지루함을 느끼거 나 무관심하다는 것을 감지하는 경우, 시스템은 대화에 더 많은 에너지와 흥미를 불어넣도록 분위기를 조정할 수 있다.사용자의 감정적 단서는 신체 언어, 얼굴 표정, 음성(voice) 톤 및 단어 선택과 같은 여러 요인에 기초할 수 있 다. 아래는 감정적 단서의 예이다: 신체 언어: ● 팔이나 다리를 꼬는 것은 방어적이거나 불편함을 나타낼 수 있다. ● 앞으로 기대는 것은 관심이나 참여를 나타낼 수 있다. ● 안절부절 못하거나 두드리는 것은 긴장이나 조급함을 나타낼 수 있다. ● 구부정하거나 눈 마주침을 피하는 것은 무관심이나 불편함을 나타낼 수 있다. ● 개방적이고 이완된 신체 자세는 자신감과 편안함을 나타낼 수 있다. 얼굴 표정: ● 미소는 행복감이나 친근감을 나타낼 수 있다. ● 찡그리거나 눈썹을 찌푸리는 것은 슬픔이나 화를 나타낼 수 있다. ● 올라간 눈썹은 놀람이나 관심을 나타낼 수 있다. ● 턱을 괴거나 입술을 다문 것은 화나 긴장을 나타낼 수 있다. ● 눈을 찡그리거나 좁아지는 것은 의심이나 불신을 나타낼 수 있다. 음성 톤: ● 고음이나 떨리는 목소리는 긴장이나 두려움을 나타낼 수 있다. ● 단조로운 목소리는 지루함이나 무관심을 나타낼 수 있다. ● 크거나 높아진 목소리는 화나 좌절을 나타낼 수 있다. ● 부드럽고 느린 목소리는 슬픔이나 우려를 나타낼 수 있다. ● 빠르고 활기찬 목소리는 흥분이나 열정을 나타낼 수 있다. 단어 선택: ● \"굉장하다\", \"환상적이다\" 및 \"대단하다\"와 같은 긍정적인 단어는 행복감이나 열정을 나타낼 수 있다. ● \"형편없다\", \"끔찍하다\" 및 \"실망스럽다\"와 같은 부정적인 단어는 화나 실망을 나타낼 수 있다. ● \"흥미롭다\", \"괜찮다\" 또는 \"무엇이든지\"와 같은 중립적인 단어 또는 어구는 무관심이나 무신경을 나타낼 수 있다. ● \"나는 느낀다\" 또는 \"내 생각에\"와 같은 개인적인 경험을 반영하는 단어 또는 어구는 개인의 감정적 반응을 나타낼 수 있다. 데이터 세트는 신경망이 이러한 감정적 단서를 인식하게끔 학습하도록 트레이닝될 수 있다. 감정을 감지하기 위한 머신 러닝의 사용은 또한 가상 이벤트의 자연스러움과 자발성을 향상시킬 수 있다. 대면 대화에서, 참가자는 다른 참가자의 감정에 기초하여 자연스럽게 자신의 톤, 신체 언어 및 발언(speech) 패턴을 조정한다. 머신 러닝을 사용하여 감정을 감지함으로써, 시스템은 가상 이벤트에서 이 자연스러운 행동을 복제 할 수 있다. 시스템은 참가자의 감정을 분석하고, 이러한 감정을 반영하도록 대화를 조정하며, 보다 자연스럽 고 자발적인 대화를 생성할 수 있다. 실시예는, 사람들이 말하고 있는지 여부, 사람들이 손 제스처를 사용하고 있는지 여부와 이러한 제스처의 유형, 신체 언어, 또는 촉각, 압력, 온도, 습도, 광, 심장 박동 등과 같은 키보 드나 다른 센서로부터 수신된 다른 입력과 같은 추가 입력을 사용할 수 있다. 실시예가 프로세싱할 수 있는 추 가 정보는, 사용자가 가상 환경과 상호작용하거나 가상 환경에서 이동하는 방식이다. 또 추가의 입력은, 배경 오디오 및 컷아웃 프로세스에 의해 잘라내어진 비디오 정보와 같은, 소위 \"데드(dead)\" 정보를 포함할 수 있다. 특정 옵트인 요건 또는 기타 법적 수단에 의해 해결될 수 있는 보안 또는 프라이버시 문제가 있을 수 있다. 포어그라운드 또는 핵심 음성을 추출하거나 향상시키고 다른 사운드를 분류 및 분할하기위해 노이즈 제거 프로세스가 사용될 수 있다. 이들 입력 각각은 가상 회의의 가상 환경에서의 행동을 유도하도록 의도된 개입(intervention)을 선택하는 것을 돕기 위해 상이한 방식으로 그리고 상이한 간격으로 분석, 모델링 및 프로세싱될 수 있다. 예를 들어, 누군가 의 배경에서 개 짖는 소리가 있는 경우, 시스템의 추출 알고리즘이 이를 인식하고 짧은 기준 리스트를 가질 것 이다. 이어서, 시스템은 창 밖의 장면에 개의 비디오/애니메이션 3D 모델을 추가할 수 있다. 또는 주전자가 켜져 있는 경우, 방에 있는 주전자에 김이 나기 시작할 수 있다. 또는 기도 시간인 경우, 하늘에 황금빛 조명 이 켜질 수 있다. 원인과 결과가 직접적이거나 문자 그대로일 필요는 없다. 이들은 누적적이거나 조합적일 수 있다. 예를 들어, 2명 또는 3명의 사용자에게 동시에 Teams 알림이 울리는 경우, 사운드를 나타내는 3D 객체의 스케일이 변경될 수 있다. 또는 진공청소기 노이즈가 증가하는 경우, 로봇 청소기가 사운드를 생성하는 사용자 에게 더 가까이 이동하기 시작할 수 있다. 방 안의 물체가 구의 물결, 외부 불꽃놀이 또는 비품/가구의 미묘한 흔들림과 같은 새롭고 예술적인 방식으로 방 안의 누적 배경 노이즈 레벨을 나타낼 수 있다. 비디오 배경 정보에 대하여, 유사한 프로세스가 있을 수 있다. 누군가가 책상 위에 컵을 가지고 있는 경우, 이 는 실제 장면으로 시뮬레이션될 수 있다. 뒤에 녹색 커튼이 있는 경우, 가상 환경의 강조 디테일 중 하나가 변 경될 수 있다. 창문 밖이 화창한 경우, 3D 방 외부의 날씨가 변경될 수 있다. 다시 이는 조합적일 수 있다. 그래서 위치 A에서는 화창하고 위치 B에서는 흐린 경우, 구름 사이로 햇살이 비추거나 무지개가 있을 수 있다. 또는, 가상 환경에는 그 안에 있는 모든 사람들의 독서 내용 및 배경 사진 취향을 통합하는 공유 책장 또는 아 트 월이 있을 수 있다. 또한, 누군가가 해변의 배경 그림을 가지고 있다면, 가상 환경의 배경은 해변 장면이 될 수 있다. 스타트렉 포스터를 갖는 경우, 배경은 우주 등이 될 수 있다. 보다 일반적으로, 이들의 조명이 두 임계값 사이에 변하는 경우, 가상 환경은 이 행동과 매칭될 수 있다. 배경 에 예술 작품이 있는 경우, 이것이 식별되어 그들의 개인 가상 환경에 적용될 수 있다. 또는 브랜드를 착용하 고 있거나 브랜드가 있는 기술을 사용하는 경우, 브랜드 제거/블러링될 수 있다. 책장에 책을 갖고 있다면, 채 팅에 URL을 붙여 넣는 대신, 이를 다른 사람이 볼 수 있도록 불러오고 링크시킬 수 있다. 카메라에 대고 물체 를 잡는 경우, 이 인식 프로세스는 예를 들어 OCR에서 AI로의 세그먼트화를 사용하여 아날로그 정보를 디지털화 하는 데 따른 마찰을 없애는 것을 도울 수 있다. 실시예는 또한, 예를 들어, 하나 이상의 2D 이미지를 사용하여 3D 공간을 재생성하기 위해 신경 방사 필드 (neural radiance field)를 사용할 수 있다. 예를 들어, 누군가가 자신의 폰을 체스보드로 향하게 할 수 있으 며, 일치하는 3D 플레이 가능한 체스보드가 가상 환경에서 테이블 상에 나타날 수 있다. 가상 회의 또는 웨비나와 같은 가상 이벤트에서는 대면 대화의 자연스러운 흐름과 분위기를 복제하는 것이 어려 울 수 있다. 그러나, 머신 러닝을 사용하여 감정을 감지함으로써, 시스템은 실제와 같은 자연스러운 대화를 수 월하게 쉬운 방식으로 시뮬레이션할 수 있다. 전반적으로, 머신 러닝을 사용하여 감정을 감지하는 것은 가상 이벤트의 역동성, 자연성, 자발성, 흥분 및 에너지를 증가시키는 데 상당한 이점을 가질 수 있다. 시스템은 참 가자들의 감정을 실시간으로 분석하고 그에 따라 이벤트의 톤과 분위기를 조정하며, 보다 매력적이고 역동적인 경험을 생성할 수 있다. 대면 대화의 자연스러운 행동을 복제함으로써, 시스템은 또한 가상 이벤트의 자연스러 움과 자발성을 향상시킬 수 있다. 실시예는 하나 이상의 가상 환경 시스템에 복수의 가상 환경을 저장하는 인터랙티브 가상 회의 플랫폼 (interactive virtual conference platform)을 실행하기 위한 방법 및 시스템을 포함한다. 플랫폼은 또한 비 즈니스 회의 또는 파티와 같은 복수의 정황 시나리오를 저장할 수 있다. 감정적 단서도 또한 플랫폼에 의해 저 장될 수 있다. 플랫폼은 가상 회의에 참여하기 위한 복수의 요청을 수신할 수 있다. 참가자들은 비디오 및 오 디오 스트림을 포함하는 세션을 함께 연결함으로써 가상 회의에 연결될 수 있다. 플랫폼은 하나 이상의 신경망 을 사용하여 복수의 비디오 또는 오디오 스트림을 분석하기 위한 서버를 포함할 수 있다. 분석은 입력 데이터 로부터 복수의 정황 시나리오 중의 정황 시나리오 또는 하나 이상의 감정적 단서를 자동으로 감지할 수 있다. 분석된 입력 데이터에 기초하여, 시스템 및 방법은 분석된 입력 데이터, 감지된 정황 시나리오 또는 감지된 감 정적 단서에 기초하여 개입 데이터베이스로부터 개입을 선택할 수 있다. 다음으로, 플랫폼은 개입 데이터베이 스로부터 개입을 읽어들일 수 있다. 개입을 읽어들인 후, 플랫폼은 개입 데이터베이스로부터 읽어들인 개입에 기초하여 가상 회의에 개입할 수 있다. 개입은 출력 오디오 신호 또는 출력 비디오 신호에 대한 적어도 하나의 변경을 포함할 수 있다. 일부 실시예에서, 입력 데이터는 타이핑 속도, 타이핑 볼륨(소거), 손 제스처, 말하기 시간의 양, 얼굴 (미세) 표정, 마우스/스와이프 속도, 지리적 위치, 브라우저, 로딩 시간, FPS/탭 포커스, 회의 제목, 참가자 수, 머리포지션, 언어 독성(language toxicity), 디바이스, 또는 발언 리듬(rhythms of speech) 중, 하나 이상을 더 포 함한다. 일부 실시예에서, 플랫폼은 플랫폼의 특정 옵션을 셋업할 수 있게 하는 구성 애플리케이션을 더 포함한다. 예 를 들어, 구성 애플리케이션은 참가자들이 상호작용하는 정황 시나리오와 독립적으로 개입의 선택을 허용하도록 구성될 수 있다. 다른 예에서, 구성 애플리케이션은 정황 시나리오와 관련하여 입력 데이터에 기초하여 개입의 선택을 허용하도록 구성될 수 있다. 이 예에서, 특정 정황 시나리오에서의 입력 데이터는 정황 시나리오에 따 라 상이한 개입을 초래할 수 있다. 예를 들어, 설문 리얼리티 쇼 유형의 정황 시나리오에서 참가자가 손을 드 는 경우, 플랫폼은 참가자를 스포트라이팅하고 특정 음악을 재생하는 개입을 선택할 수 있지만, 비즈니스 정황 시나리오에서 손을 드는 참가자의 경우, 플랫폼은 음악을 재생하거나 참가자를 스포트라이팅하지 않고서 가상 카메라 각도만을 참가자에게 리디렉션하고 다른 모든 참가자들의 마이크를 낮추는 개입을 선택할 수 있다. 플랫폼은 또한 복수의 정황 시나리오에 대응하는 하나 이상의 데이터 세트를 포함하는 입력을 하나 이상의 신경 망으로 수신할 수 있다. 하나 이상의 신경망은 컨볼루션 신경망(CNN; convolutional neural network)과 순환 신경망(RNN; recurrent neural network) 중 하나 이상을 포함할 수 있다. 이들 시스템은 개입에 대한 피드백을 수신하고 향후 개입에 적용하기 위해 피드백을 하나 이상의 신경망에 적용 하여 하나 이상의 신경망을 트레이닝하도록 구성될 수 있다. 피드백은 적어도 하나의 사용자로부터의 랭킹 (ranking) 또는 비디오 또는 오디오 스트림을 통해 감지된 하나 이상의 사용자로부터의 물리적 반응을 포함할 수 있다. 개입은 가상 카메라 각도, 샷 크기 또는 가상 카메라 모션에 대한 변경을 포함할 수 있다. 개입은 또한, 출력 비디오 신호의 밝기를 변경하는 것, 출력 오디오 신호의 톤을 변경하는 것, 또는 출력 비디오 신호의 색조 (tint)를 변경하는 것을 포함할 수 있다."}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "개입은 또한, 이벤트의 요약을 생성하기 위해 이벤트의 가장 흥미로운 장면을 선택함으로써 가상 환경에서 발생"}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "하는 이벤트를 요약하는 것을 포함할 수 있다. 이 실시예에서, 광범위한 데이터를 캡처하는 이벤트에서 하나 이상의 가상 방송 카메라를 통해 레코딩이 이루어질 수 있다. 이는 가상 환경의 스크린 상에서와 같이 가상 환 경의 하나 이상의 표면 상에 제시되거나 그의 벽 중 하나 이상의 벽에 투사될 수 있다. 실시예는 또한 하나 이상의 사용자 프로필에 기초하여 개입을 선택하는 판독을 포함할 수 있다. 사용자 프로필 은 미리 선택된 개입 기준과 정황 시나리오를 상관시키는 하나 이상의 사용자 설정을 포함할 수 있다. 추가적인 실시예에서, 사용자 프로필은 사용자의 학습 스타일과 관련된 데이터를 포함하며, 개입은 학습을 용이 하게 하기 위해 사용자 학습 스타일에 기초하여 사용자에의 데이터의 프리젠테이션을 조정하는 것을 포함한다. 또 추가의 실시예에서, 플랫폼은 사용자 피드백을 읽어들이고 사용자 피드백에 기초하여 데이터 프리젠테이션을 조정하도록 구성된다. 다른 실시예에서, 플랫폼은 유사한 학습 스타일로 사용자를 분류하고, 이에 따라 그룹 학습을 용이하게 하기 위해 동일한 분류 하의 사용자에의 프레젠테이션을 위한 데이터를 클러스터링하도록 구성 된다. 추가적인 실시예는 복수의 클라이언트측 화상 회의 애플리케이션 및 복수의 클라이언트측 화상 회의 애플리케이 션으로부터 비디오 스트림 또는 오디오 스트림 중 하나 이상을 수신하도록 구성된 인터랙티브 화상 회의 플랫폼 을 포함할 수 있다. 인터랙티브 화상 회의 플랫폼은 하나 이상의 비디오 스트림 또는 오디오 스트림을 분석하 고 그 분석에 기초하여 정황 시나리오를 감지할 수 있다. 이들 실시예는 또한, 복수의 개입을 포함하는 개입 데이터베이스를 포함할 수 있고, 정황 시나리오를 감지하는 것에 기초하여, 실시예는 정황 시나리오에 대응하는 개입을 읽어들이고 구현할 수 있다. 이들 실시예는 또한 출력 오디오 신호 또는 출력 비디오 신호를 포함하는 출력 신호를 포함할 수 있으며, 인터랙티브 화상 회의 플랫폼은 정황 시나리오에 대응하는 개입에 기초하여 출 력 신호를 수정하도록 구성된다. 개입은 하나 이상의 카메라 뷰에 대한 변경 및 환경 변경을 포함할 수 있다. 실시예는 또한, 프로세서 상에서 수행될 수 있는 명령어를 포함하는 다양한 비일시적 컴퓨터 판독가능 매체를 포함할 수 있다. 하나의 명령어는 가상 회의에 참여하기 위한 복수의 요청에 기초하여 복수의 사용자를 가상 회의에 연결하는 것일 수 있으며, 각각의 세션은 복수의 비디오 또는 오디오 스트림을 집합적으로 형성하는 하 나 이상의 비디오 또는 오디오 스트림을 포함한다. 추가 명령어는 시나리오 데이터베이스에 저장된 복수의 정 황 시나리오로부터 정황 시나리오를 감지하기 위해 복수의 비디오 또는 오디오 스트림을 분석하는 것일 수 있다. 명령어는 또한, 시나리오 데이터베이스에 저장된 복수의 정황 시나리오 중의 정황 시나리오를 자동으로 감지하기 위한 명령어를 포함할 수 있다. 감지 후, 매체는 정황 시나리오에 기초하여 개입 데이터베이스로부터개입을 선택할 수 있다. 선택 후, 개입 데이터베이스로부터의 정황 시나리오에 기초하여 개입을 읽어들이기 위 한 명령어가 있을 수 있다. 그러면, 개입을 읽어들인 후, 매체는 개입 데이터베이스로부터 읽어들인 개입에 기 초하여 가상 회의에 개입하기 위한 명령어를 포함할 수 있으며, 개입은 출력 오디오 신호 또는 출력 비디오 신 호에 대한 적어도 하나의 변경을 포함한다. 전술한 일반적인 설명 및 다음의 상세한 설명은 단지 예시이고 설명을 위한 것일 뿐이며 청구하는 본 발명을 제 한하지 않는다."}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "다음의 실시예는 보다 매력적인 가상 회의 프레젠테이션을 생성하기 위한 시스템 및 방법을 설명한다. 현재의 가상 회의는 화자의 단일 비디오 피드 또는 여러 참석자들이 한꺼번에 있는 박스를 제공하기 때문에 비교적 정 적이고 지루하다. 본 개시의 실시예는 더욱 매력적인 프리젠테이션을 생성하기 위해 상이한 카메라 효과 또는 다른 개입을 허용할 수 있다. 본 명세서의 주제는 이제, 특정 예시적인 실시예의 일부를 형성하며 예시로써 이를 도시하는 첨부 도면을 참조 하여 이하에서 보다 상세하게 기재될 것이다. 본원에서 \"예시적인\"으로서 기재되는 실시예 또는 구현은, 예를 들어 다른 실시예 또는 구현에 비해 바람직하거나 유리한 것으로 해석되어서는 안 되며, 오히려, 실시예(들)가 \"예시적인\" 실시예(들)임을 반영하거나 나타내도록 의도된다. 주제는 다양한 상이한 형태로 구현될 수 있고, 따라서 커버되거나 청구되는 주제는 본원에서 서술되는 임의의 예시적인 실시예로 한정되지 않는 것으로 해석되 어야 하며, 예시적인 실시예는 단지 예시를 위한 것으로 제공된다. 마찬가지로, 청구되거나 커버되는 주제에 대해 합리적으로 넓은 범위가 의도된다. 무엇보다도, 예를 들어, 주제는 방법, 디바이스, 컴포넌트 또는 시스 템으로서 구현될 수 있다. 따라서, 실시예는, 예를 들어, 하드웨어, 소프트웨어, 펌웨어, 또는 이들의 임의의 조합의 형태를 취할 수 있다(소프트웨어 그 자체가 아님). 따라서, 다음의 상세한 설명은 한정하는 의미로 받 아들여져서는 안 된다. 본 명세서 및 청구항 전반에 걸쳐, 용어들은 명시적으로 언급된 의미를 넘어 문맥에서 제안되거나 암시되는 미 묘한 의미를 가질 수 있다. 마찬가지로, 본원에서 사용되는 \"하나의 실시예에서\"라는 문구는 반드시 동일한 실 시예를 지칭하는 것은 아니며, 본원에서 사용되는 \"다른 실시예에서\"라는 문구는 반드시 상이한 실시예를 지칭 하는 것은 아니다. 예를 들어, 청구된 주제는 예시적인 실시예의 조합을 전체적으로 또는 부분적으로 포함하는것으로 의도된다. 아래에서 사용되는 용어는 본 개시의 특정 구체적 예의 상세한 설명과 함께 사용되고 있더라도, 가장 광범위하 게 합리적인 방식으로 해석될 수 있다. 실제로, 특정 용어가 아래에서 강조되었을 수도 있지만, 임의의 제한된 방식으로 해석되도록 의도된 임의의 용어는 이 상세한 설명 섹션에서 명시적으로 그리고 구체적으로 정의될 것 이다. 전술한 일반적인 설명 및 다음의 상세한 설명 둘 다 단지 예시적이고 설명을 위한 것일 뿐이며, 청구하 는 특징을 제한하는 것이 아니다. 본 개시에서, 용어 \"에 기초하여\"는 \"에 적어도 부분적으로 기초하여\"를 의미한다. 단수 형태의 표현은 문맥상 달리 지시되지 않는 한 복수 대상을 포함한다. 용어 \"예시적인\"은 \"이상적인\"보다는 \"예시\"의 의미로 사용된다. 용어 \"또는\"은 포괄적인 의미이며, 나열된 항목들 중 어느 하나의 항목, 임의의 항목, 여러 항목 또 는 모든 항목을 의미한다. 용어 \"포함한다\", \"포함하는\", \"함유한다\", \"함유하는\", 또는 이들의 다른 변형은, 요소들의 리스트를 포함하는 프로세스, 방법 또는 제품이 반드시 이들 요소만을 포함하는 것이 아니라, 그러한 프로세스, 방법, 물품 또는 장치에 명시적으로 나열되지 않거나 내재되지 않은 다른 요소도 포함할 수 있도록, 배타적이지 않은 포괄을 커버하도록 의도된다. \"실질적으로\" 및 \"일반적으로\"와 같은 상대적인 용어는, 명시되 거나 이해되는 값의 ±10%의 가능한 변동을 나타내기 위해 사용된다. 이제 첨부 도면을 참조하면, 도 1은 본 개시의 하나 이상의 실시예에 따른 예시적인 환경(또는 시스템 (들))의 개요를 도시한다. 환경은 예를 들어, 네트워크 시스템(들) 및 협업 인터랙티브 화상 회의 플랫폼과 통신하도록 구성되는, 제1 사용자 디바이스(들) 및 제2 사용자 디바이스(들)를 포함할 수 있다. 환경에는 2개의 사용자 디바이스(들)(110 및 120)가 도시되어 있지만, 본 개시에 따라, 네트워크 시스템(들) 및/또는 협업 인터랙티브 화상 회의 플랫폼과 동기적으로 또는 비동기적으로 통 신하도록 그리고 다수의 사용자들의 협업 콘텐츠 생성에 참여하도록 추가적인 사용자 디바이스가 환경에 제공될 수 있다. 본 개시의 양상에 따르면, 네트워크 시스템(들)은 유선 또는 무선 네트워크를 포함하는 하나 이상의 네트 워크를 정의할 수 있다. 네트워크 시스템(들)은 예를 들어 인터넷 및/또는 하나 이상의 클라우드 네트워 크를 포함할 수 있다. 또한, 네트워크 시스템(들)은 인터넷과 같은 공중 네트워크, 인트라넷과 같은 사설 네트워크, 또는 이들의 조합을 포함할 수 있고, TCP/IP 기반 네트워킹 프로토콜을 포함하지만 이에 한정되는 것 은 아닌, 현재 이용가능하거나 나중에 개발되는 다양한 네트워킹 프로토콜을 이용할 수 있다. 네트워크 시스템 (들)은 사용자 디바이스(들)(110 및 120)와 협업 인터랙티브 화상 회의 플랫폼 사이의 데이터의 통신 을 가능하게 하기 위해 사용자 디바이스(들)(110 및 120)를 협업 인터랙티브 화상 회의 플랫폼에 통신가능 하게 커플링하도록 구성될 수 있다. 네트워크 시스템(들)은 일반적으로, 하나의 디바이스로부터 다른 디 바이스로 정보를 전달하기 위해 임의의 형태의 컴퓨터 판독가능 또는 기계 판독가능 매체를 채용하는 것이 가능 할 수 있다. 네트워크 시스템(들)은 컴퓨팅 디바이스들 사이에 정보가 이동할 수 있는 통신 방법을 포함 할 수 있다. 네트워크 시스템(들)은 공중 또는 사설 네트워크 연결로서 간주될 수 있고, 예를 들어 가상 사설 네트워크 또는 공중 인터넷을 통해 채용되는 암호화 또는 다른 보안 메커니즘 등을 포함할 수 있다. 하나의 실시예에서, 사용자 디바이스(들)(110 및 120)는 협업 인터랙티브 화상 회의 플랫폼과 직접 통신할 수 있거나 또는 네트워크 시스템(들) 또는 다른 이용가능한 통신 채널을 통해 간접적으로 통신할 수 있다. 사용자 디바이스(들)(110 및 120)가 협업 인터랙티브 화상 회의 플랫폼과 직접 통신하는 경우, 협업 인터 랙티브 화상 회의 플랫폼은, 예를 들어 위에 네트워크 시스템(들)에 관련하여 기재된 하나 이상의 통 신 방법을 통해, 통신을 용이하게 하도록 구현 및 구성될 수 있다. 본 개시의 양상에 따르면, 협업 인터랙티브 화상 회의 플랫폼은 서버 시스템(들), 비디오 저장 시스 템(들), 신경망, 가상 환경 시스템(들) 및 개입 데이터베이스를 포함할 수 있다. 일부 실 시예에서, 인터랙티브 화상 회의 플랫폼은, 본 개시의 양상에 따라, 서버 시스템(들), 비디오 저장 시스템(들), 신경망, 가상 환경 시스템(들) 및 개입 데이터베이스의 기능의 일부 또는 전 부를 수행하도록 구성될 수 있는 하나 이상의 서버일 수 있다. 본 개시에서, 시스템(들)은 다양한 전자 및 컴 퓨터 시스템을 포함할 수 있는 다양한 구현을 포함할 수 있다. 본원에 기재되는 하나 이상의 구현은 모듈들 사 이에 그리고 모듈들을 통해 또는 애플리케이션 특정 집적 회로의 일부로서 통신될 수 있는 관련 제어 및 데이터 신호로 둘 이상의 특정 상호연결된 하드웨어 모듈 또는 디바이스를 사용하여 기능을 구현할 수 있다. 따라서, 시스템(들)은 소프트웨어, 펌웨어 및 하드웨어 구현을 포함한다. 본 개시의 양상에 따르면, 서버 시스템(들)은, 네트워크 시스템(들)에서 클라우드 서버/네트워크, 에 지 서버/네트워크 상에, 그리고/또는 서버 시스템(들)이 협업 인터랙티브 가상 회의 플랫폼과 직접적 으로 또는 간접적으로 통합될 수 있는 위치 상에 위치될 수 있는 하나 이상의 데이터 서버 또는 데이터베이스를 포함할 수 있다. 본 개시의 실시예에 따라, 서버 시스템(들)은 매력적인 가상 회의를 생성하기 위해 사용 자 디바이스(들)(110 및 120) 및/또는 네트워크 시스템(들)으로부터 수신된 데이터를 저장 및 프로세싱할 수 있다. 추가적으로, 서버 시스템(들)은 매력적인 가상 회의의 실행을 용이하게 하기 위해 비디오 저장 시스템, 신경망, 가상 환경 시스템(들) 및 개입 데이터베이스에 대해 데이터 또는 커맨드 신호를 수신하고 송신할 수 있다. 본 개시의 양상에 따르면, 비디오 저장 시스템(들)은 제1 사용자 및 제2 사용자로부터 그의 대 응하는 사용자 디바이스(110/120)를 통해 하나 이상의 입력 또는 커맨드를 수신할 수 있다. 위에 기재된 바와 같이, 환경은 둘보다 많은 사용자 또는 사용자 디바이스(110 및 120)를 포함할 수 있다. 따라서, 비디오 저장 시스템(들)은 2개보다 많은 사용자 또는 사용자 디바이스로부터 입력 및 커맨드를 수신할 수 있다. 대안적으로, 비디오 저장 시스템(들)은 협업 세션의 유형에 따라 제1 사용자로부터만 또는 제2 사용 자로부터만 하나 이상의 입력 또는 커맨드를 수신할 수 있다. 즉, 협업 인터랙티브 화상 회의 플랫폼 은 하나 이상의 가상 회의를 용이하게 할 수 있다. 하나의 실시예에서, 제1 사용자 및/또는 제2 사용자는 각각 제1 사용자 디바이스(들) 및 제2 사 용자 디바이스(들)에 동기적으로 또는 비동기적으로 커맨드를 입력할 수 있다. 비디오 저장 시스템 (들)은 사용자 디바이스(들)(110 및 120)로부터 입력 커맨드를 직접, 또는 네트워크 시스템(들) 및/ 또는 서버 시스템(들)을 통해 간접적으로, 수신할 수 있다. 이어서, 비디오 저장 시스템(들)은 입력 커맨드를 텍스트 데이터로 전사하거나 변환할 수 있다. 텍스트 데이터의 형태로 수신될 수 있는 입력 커맨드의 경우, 비디오 저장 시스템(들)에 의한 텍스트 데이터로의 변환은 필요하지 않을 수 있다. 그 다음, 비디 오 저장 시스템(들)은, 텍스트 데이터의 시맨틱(semantic)을 캡처하는 것(예컨대, 대화의 컨텍스트를 이해 함), 중복 및/또는 리던던시(예컨대, 불필요한 정보)를 검출하는 것, 텍스트 데이터를 라벨링하는 것 및/또는 프롬프트를 검출하는 것에 의해, 입력 커맨드의 유형이나 형태에 따라, 변환된 텍스트 데이터 또는 수신된 텍스 트 데이터를 분석할 수 있다. 이어서, 비디오 저장 시스템(들)은, 예를 들어, 검출된 중복 및/또는 리던 던시를 제거함으로써, 필요한 경우, 분석된 텍스트 데이터를 클리닝 또는 수정할 수 있다. 그 다음, 비디오 저 장 시스템(들)은, 분석 및 클리닝된 텍스트 데이터로부터의 프롬프트(만약 있다면)를 추출하도록 진행할 수 있다. 일부 실시예에서, 비디오 저장 시스템(들)은 트레이닝된 머신 러닝 모델을 이용할 수 있다. 이 어서, 비디오 저장 시스템(들)은 추출된 클린 프롬프트를 가상 환경 시스템(들)에 보낼 수 있다. 일 부 실시예에서, 데이터 변환 또는 전사는 사용자 디바이스(들)(110, 120)에서 발생할 수 있다. 따라서, 사용자 디바이스(들)(110, 120)는, 오디오 또는 다른 유형의 데이터의 텍스트로의 변환이 가능하며 전사된 텍스트를 협 업 인터랙티브 화상 회의 플랫폼에 보내는 컴퓨터 코드를 구현할 수 있다. 대안적으로, 위에 기재된 바와 같이, 데이터 전사 또는 변환은 서버 시스템(들)에 의해 협업 인터랙티브 화상 회의 플랫폼에서 발생 할 수 있으며, 그리하여 입력 디바이스(들)(110, 120)는 어떠한 데이터 전사 또는 변환도 수행할 필요가 없을 수 있다. 대안적으로, 서버 시스템(들)의 모든 기능은 텍스트 전사 또는 변환, 입력, 또는 커맨드 데이터 분석, 및/또는 입력 또는 커맨드 데이터 클리닝을 포함하여 신경망에 의해 수행될 수 있다. 하나의 실시예에서, 이미지 데이터, 비디오 데이터 및/또는 다른 유형의 오디오 데이터와 같은 임의의 유형의 입력 또는 커맨드 데이터에 대해 라벨링 프로세스가 비디오 저장 시스템(들)에 의해 수행될 수 있다. 텍 스트 데이터를 포함하지 않는 데이터의 경우, 라벨링 프로세스는 커맨드를 텍스트로 전사하는 유형으로서 간주 될 수 있다. 예를 들어, 사용자(112, 122)의 톤이 비디오 저장 시스템(들)에 의해 특정 감정적 단서와 연 관되는 경우, 비디오 저장 시스템(들)은, 하나 이상의 신경망에 의해 결정될 수 있는, 예를 들어 화, 행복, 슬픔, 우려, 격노함 등을 포함하는 라벨을 생성할 수 있다. 다수의 참가자들은 유사하거나 상이하거나 심지어 상반된 감정을 드러내고 있을 수 있다. 비디오 저장 시스템(들)은 트레이닝 데이터 세트에 기초하여 각 참가자에 대한 감정을 실시간으로 결정할 수 있다. 본 개시의 양상에 따르면, 가상 환경 시스템(들)은 참가자의 음성 톤, 단어 및 모션과 같은 특정 요인을 분석하고, 가상 회의를 계속해서 원활하게 진행하거나 하나 이상의 사용자에 의해 정의되는 일부 다른 목적을 달성하기 위해 가상 회의에 특정 개입을 행할 수 있다. 본 개시의 양상에 따르면, 가상 환경 시스템(들)은 환경 내의 사용자(112, 122)가 콘텐츠를 생성하기 위해 협업하는 데 이용할 수 있는 하나 이상의 컴퓨팅 환경의 생성을 용이하게 할 수 있다. 컴퓨팅 환경은, 예 를 들어, 증강 현실 환경, 가상 현실 환경, 및 2차원(2D) 또는 3차원(3D) 시뮬레이션된 환경을 포함할 수 있지만, 이에 한정되는 것은 아니다. 컴퓨팅 실시예에서 컴퓨팅 환경 및 사용자 협업의 예는 또한, 그 전체가 참조 로서 본원에 포함되는 공동 계류 중인 미국 특허 출원 제17/006,327호에서 개시된다. 또한, 공동 계류 중인 출 원에서 개시된 임의의 머신 러닝 알고리즘이, 비디오 저장 시스템(들) 및/또는 가상 환경 시스템(들)(14 8)에 의해 사용되는 머신 러닝 모델과 조합하여 또는 개별적으로 사용되도록 통합될 수 있다. 시스템은 수신된 입력에 따라 사용자가 다양한 가능한 출력을 구성할 수 있게 할 수 있다. 시스템은 사전설정 또는 정황 시나리오를 더 사용하여, 출력의 프로그래밍적 선택을 가능하게 하고 그리고/또는 출력을 수정할 수 있다. 상이한 정황 시나리오는 소규모 사무실이나 대형 회의실에서의 비즈니스 회의, 하우스 생일 파티, 의사 사무실 또는 나이트 클럽을 포함할 수 있다. 따라서, 시스템은 사용자 및 비사용자(non-user) 데이터 소스를 포함하는 많은 데이터 소스로부터 이용가능한 복수의 데이터 포인트를 이용한다. 사용자 데이터 소스는 사용자 아바타에 의해 생성된 데이터, 예컨대 그들의 그래픽 데이터, 그들의 아바타의 위치와 배향, 그들의 음성, 움직임, 사용자 아바타에 의해 수행되는 활동 등을 포함한다. 비사용자 데이터는 가상 환경 내의 물체, 주변 소음, 주변 음악, 조명, 밝기, 색상 등을 포함한다. 데이터는 사용자가 감정과 행동을 생성하기 위한 추가 자극을 나타내는 카메라 출력을 생성하도록 프로세싱될 수 있다. 데이터의 프로세싱은 시스템에 의해 구현되는 신경망에 의한 입력의 논리적 해석을 포함하며, 이는 대응하는 카메라 출력을 생성하기 위해 사용될 수 있다. 추가 자극의 생성은 가상 환경에서 상호작용하는 사용 자의 더 넓은 범위의 인간 감정에 접근하는 데 도움이 될 수 있으며, 이는 사용자의 동기 부여, 공감, 학습, 연 결 및 전반적인 에너지를 향상시키며, 더 흥미롭고 재미있는 가상 이벤트를 생성하고, 잠재적으로 집중력, 학습 능력 및 생산성을 높일 수 있다. 본 개시의 시스템 및 방법은 가상 작업 회의, 비디오 게임, 영업 회의, 가상 회의, 세미나 등과 같은 가상 환경에서 호스팅되는 임의의 유형의 가상 이벤트에 대하여 사용될 수 있다. 시스템은 하나 이상의 카메라 뷰에 대한 변경 또는 환경 변경과 같은 개입을 생성하기 위해 스마트하고 공감적 인 가상 환경을 가능하게 하는 기계 학습된 공감을 구현할 수 있다. 본 개시에서, 공감이라는 용어는, 사용자 의 정신적 관점, 감정 및 행동을 논리적으로 이해하고 그 논리적 이해를 사용하여 사용자에게 영향을 미치는 출 력을 생성하기 위해 인공 지능 알고리즘을 구현하는 컴퓨터 시스템의 능력을 지칭한다. 본 개시에 채용된 공감은 3개의 카테고리로 나누어질 수 있다: - 인지적 공감(Cognitive empathy): 하나 이상의 개입을 통해 사용자의 행동에 영향을 미치기 위해 사용자의 생 각, 의도 및 동기를 이해할 수 있는 능력. - 감정적 공감(Emotional empathy): 감정과 기분을 이해할 수 있는 능력. 이는 손 제스처, 음성 톤, 말하기 또 는 타이핑 속도, 단어 및/또는 문장 사이의 일시 중지, 얼굴 표정, 손 제스처 등과 같은 미묘한 비언어적 메시 지에 기초하여 행해질 수 있다. - 사회적 공감(Social empathy): 하나 이상의 개입을 통해 그들의 행동에 영향을 미치기 위해 그룹을 인지적으 로 및/또는 정서적으로 이해할 수 있는 능력. 시스템은 여러 유형의 입력으로부터 상이한 인지적 및 감정적 단 서를 읽어내고, 원하는 출력을 생성할 수 있는 개입을 정의하기 위해 그룹의 지배적인 관점, 감정 및 행동을 결 정할 수 있다. 시스템은, 예컨대, 주로 같은 방식으로 느끼는 하나의 그룹에 특정 유형의 카메라 출력을 제공 하고 다른 그룹에 다른 카메라 출력을 제공하는 것에 의해, 그들의 사회적으로 결정된 인지적 및 감정적 우위에 기초하여 개입을 더 분류하고 분리할 수 있다. 실시예는 긍정적인 가상 회의 경험을 조성하기 위한 개입을 생성하기 위해 공감 감지를 사용할 수 있다. 예시적인 개입은 예를 들어, 사용자의 볼륨을 높이고 다른 사용자의 볼륨을 낮추는 것, 사용자를 스포트라이팅 하는 것, 가상 환경의 날씨를 수정하는 것, 가상 환경의 조명 및/또는 색상을 변경하는 것, 사용자에게 액션을 취하도록 프롬프트하는 텍스트가 있는 팝업을 생성하는 것, 사용자에게 액션을 취하도록 프롬프트하는 오디오 또는 햅틱 데이터를 생성하는 것, 가상 카메라의 시점을 수정하는 것, 하나 이상의 사용자 아바타 등일 수 있다. 사용자의 결과적인 행동은 시스템에 의해 추가로 캡처되고, 대응하는 새로운 출력을 생성하기 위한 새로 운 입력으로서 사용될 수 있다. 카메라 출력은 다음 중 하나 이상의 형태를 취할 수 있다: - 간접적 개입: 이는 색상, 사운드, 분위기 등의 변경이며, 그로부터 어떤 유형의 행동 변경이 예상되는지 직접 적으로 사용자에게 알려주지 않는다. 예를 들어, 배경색을 변경하는 것, 누군가를 스포트라이팅하는 것, 음악 이나 날씨를 변경하는 것은 전부 간접적인 개입이다.- 직접적 개입: 사용자에게 직접 무언가를 하도록 유도하는 텍스트, 시각적 또는 오디오 출력이며, 예컨대 질문"}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "요청, 추가 데이터 제공, 정보의 요약 제공, 정보 분석 등. 이는 사용자가 해석할 필요가 있는 것이 아닌 명확 한 피드백과 관련되며, 예컨대, 한동안 말하지 않은 사용자가 있는 경우, 해당 사용자에게 질문하라는 텍스트 프롬프트가 있을 수 있는 것과 같이 명확한 규칙과 연결된다. 다른 예로, 영업사원이 영업 회의에서 고객에게 영업을 하고 있는데 고객이 오랫동안 말하지 않는 경우를 들 수 있다. 고객의 긴 침묵을 측정할 때, 시스템이 영업사원에게 질문하도록 프롬프트하기 위해 영업사원에의 직접적 개입을 프롬프트할 수 있도록, 시간 규칙이 시스템에서 구성된다. 이러한 시간 규칙은 시스템이 미리 정의된 수 초 또는 분 후에 프롬프트를 보내도록 구 성될 수 있고, 시간 규칙은 대화의 시맨틱 컨텍스트 및/또는 다른 요인에 의해 더 영향을 받을 수 있다. 예를 들어, 이전 대화에서 사용자가 다른 사용자에게 문서 검토를 위해 5분을 달라는 지시가 포함되었다면, 시간 규 칙은 그 시간을 고려하고 그 동안 어떠한 중단도 일으키지 않도록 할 것이다. 프롬프트는 텍스트, 햅틱 또는 오디오 프롬프트의 형태일 수 있다. 텍스트가 디폴트이지만, 다른 수단도 따라서 가능하다; 또는 - 대규모 행동 개입: 이 개입은 그룹 상호작용을 요구하는 비디오 게임 또는 기타 애플리케이션에 적용될 수 있 다. 비디오 게임의 경우, 대규모 행동 개입을 통해 플레이어에 시나리오를 변경하도록 프롬프트하거나, 플레이 어가 다른 곳으로 이동하도록 월드를 운석에 충돌하게 하거나, 플레이어가 다른 곳으로 이동하도록 몬스터가 나 타나게 할 수 있다. 이 개입은 예컨대 회의에 적용될 수 있는데, 회의 동안 분위기가 뜨거워지면 휴식을 취하 도록 대규모 개입이 모든 사람에게 프롬프트할 수 있다. 이러한 개입은 프롬프트되는 행동을 수행하는 것 외에 는 선택의 여지가 없다. 도 2는 현행 시스템 및 방법의 프로세스 흐름의 일반적인 예를 예시한다. 시스템은 시스템 입력(290- 299)에 대응하는 복수의 입력을 가질 수 있다. 시스템 입력(290-299)은 예를 들어, 비디오 또는 오디오 스트림, 타이핑 속도, 타이핑 볼륨(소거), 손 제스처, 말하기 시간의 양, 얼굴 (미세) 표정, 마우스/스와이프 속도, 지리적 위치, 브라우저, 로딩 시간, FPS/탭 포커스, 회의 제목, 참가자 수, 머리 포지션, 언어 독성, 디 바이스, 또는 발언 리듬, 유입 신호의 대역폭, 처리량 및 강도일 수 있다. 시스템은 신경망(270-285)(예컨대, 안면 인식, 사운드 분석 등)에 대응하는 복수의 신경망 케이던스 (cadences)를 사용할 수 있으며, 이는 시스템의 구성 및 수신된 시스템 입력(290-299)에 따라, 하나 이상 의 입력 카테고리(예컨대, 사운드, 비디오, 시스템 입력 등)를 프로세싱하고 네트워크 출력(265 및 260)에 대응하는 상이한 형태의 네트워크 데이터를 생성한다. 신경망(270-285)은 상이한 신경망 케이 던스에서 동작할 수 있다. 예를 들어, 병렬 입력 스트림은 캡처하고 있는 대상에 따라 상이한 간격 또는 틱 레이트로 프로세싱될 수 있다. 감정은 5 내지 10초마다 프로세싱될 수 있다. 움직임은 초당 여러 번 프로 세싱될 수 있다. 이는 또한 대화의 초점이 누구인지에 따라 달라질 수 있다. 예를 들어, 화자는 능동 참가자 가 아닌 사람보다 더 빈번하게 프로세싱될 수 있다. 각각의 병렬 입력 스트림은 입력(들)을 생성하는 사람, 그 들이 하고 있는 것 및 기타 요인에 따라 상이하게 프로세싱될 수 있다. 이 네트워크 출력(260 및 265)은 하나 이상의 대응하는 카메라 출력을 생성하기 위해 논리적 테스트 형태의 저장된 규칙 기반 동작 또는 엔진 코 드를 수행함으로써 더 프로세싱된다. 카메라 출력은 가상 환경에서 그의 하나 이상의 표면에, 예컨대 이 벤트가 방송되고 있는 가상 환경의 가상 스크린에, 제시될 수 있다. 카메라 출력은 환경 변경(235 및 240)에 대응하는 환경 변경 중 하나 이상의 형태를 취할 수 있다: - 수정되지 않은 라이브 카메라 피드; - 실시간에 가까운 카메라 뷰 조정; 또는 - 장면의 리플레이, 예컨대 이벤트의 가장 관련있는 장면을 포함하는 \"베스트\"를 생성하기 위해. 그 다음, 시스템은, 요소(250 및 245)에 각각 대응하는 상태 변경을 행함으로써 환경 변경(235 및 240)을 구현할 수 있다. 실시예는 카메라, 마이크, 센서 및 기타 적합한 입력 디바이스와 같은 복수의 소스로부터 데이터를 수신하도록 구성된 프로세서를 포함할 수 있다. 이어서, 프로세서는 필터를 적용하고 데이터를 분석을 위한 적합한 포맷으 로 변환함으로써 데이터를 사전 프로세싱할 수 있다. 일부 실시예에서, 데이터는 분석의 정확도를 개선하기 위 해 더 작은 부분들 또는 프레임들로 분할될 수 있다. 그 다음, 사전 프로세싱된 데이터는 이미지 분석에 특히 적합한 딥 러닝 알고리즘의 유형인 컨볼루션 신경망 (CNN)을 통과할 수 있다. CNN은 다양한 감정 상태에 대응하는 라벨링된 이미지의 대규모 데이터세트를 사용하 여 트레이닝될 수 있다. 트레이닝 프로세스 동안, CNN은 특정 감정과 연관되는 데이터 내의 패턴을 인식하도록학습한다. 그러면, 트레이닝된 CNN은 학습한 패턴에 기초하여 새로운 데이터를 분류하는 데 사용될 수 있다. 일부 실시예에서, 시스템은 특정 유형의 입력에 대해 각각이 트레이닝된 다수의 CNN들을 사용할 수 있다. 예를 들어, 하나의 CNN은 얼굴 표정에 대해 트레이닝될 수 있는 반면, 다른 CNN은 음성 억양에 대해 트레이닝될 수 있다. 이어서, 이들 CNN의 출력은 감정 상태의 보다 정확한 예측을 생성하기 위해 융합 알고리즘을 사용하여 조합될 수 있다. CNN에 추가하여, 입력 유형 및 원하는 출력에 따라 순환 신경망(RNN: recurrent neural network) 또는 지원 벡 터 머신(SVM; support vector machine)과 같은 다른 AI 알고리즘이 사용될 수 있다. 예를 들어, RNN은 음성 억양과 같은 시계열 데이터에 사용될 수 있는 반면에, SVM은 손 제스처 인식에 사용될 수 있다. 전반적으로, 본 개시는 다양한 입력을 사용하여 감정을 감지하기 위해 머신 러닝을 사용하기 위한 방법 및 시스 템을 제공한다. CNN 및 다른 적합한 AI 알고리즘의 사용은 감정 상태의 정확하고 견고한 감지를 가능하게 하며, 이는 시장 조사, 정신 건강 진단 및 인간-컴퓨터 상호작용과 같은 다양한 응용에서 유용할 수 있다. 카메라 리플레이에서 보이는 카메라 출력은 이벤트의 분위기 및 그 역동성(dynamic)에 영향을 미치기 위해 가상 환경에서 하나 이상의 사용자의 하나 이상의 행동 수정을 프롬프트하도록 구성될 수 있으며, 이는 보다 활기있 고 자발적이며 현실적인 사용자 경험을 생성할 수 있다. 카메라 조정 출력은, 예를 들어, 사용자의 줌인 또는 줌아웃, 카메라의 패닝, 카메라의 각도 변경, 카메라 효과 추가, 카메라의 시야 수정 등을 포함할 수있다. 사 용자의 결과적인 행동은 가상 카메라에 의해 추가로 캡처되고, 대응하는 새로운 출력을 생성하기 위한 새로운 입력으로서 사용될 수 있다. 오디오 입력은 사용자의 마이크에 의해 캡처될 수 있고, 사용자의 음성 데이터, 가상 환경의 음악, 타이핑 사운 드, 톤, 및 음성의 속도, 말하기 리듬, 운율 등을 포함할 수 있다. 비디오 입력은 실제 및 가상 카메라에 의해 캡처될 수 있고, 그들의 포지션 및 배향과 함께 가상 환경에서의 다양한 사용자 및 요소의 가상 컷아웃, 각 사 용자의 가상 카메라의 현재 시점, 사용자의 미세 얼굴 표정, 손 제스처, 머리 움직임을 포함할 수 있고, 시스템 입력은 가상 환경의 프로세싱 시스템에 의해 캡처될 수 있고, 타이핑 속도, 로딩 속도, 초당 프레임(FPS; frames per second), 마우스/스와이프 속도, 현재 시스템 부하, 이용가능한 대역폭, 이용가능한 컴퓨팅 전력, 이용가능한 메모리를 포함한 시스템 용량 등을 포함할 수 있다. 신경망은 시각적 데이터와 같은 데이터를 취하여 가중치가 적용되는 논리적 그래프를 통해 이를 전달하는 적합 한 AI 알고리즘을 사용할 수 있고, 일부 속성에 대해 수치 응답이 반환된다. 입력 유형 및 원하는 출력에 따라 사용될 수 있는 AI 알고리즘의 몇몇 순열이 있다. 이러한 AI 알고리즘의 하나의 예로는 컨볼루션 신경망이 있 다. 네트워크 출력은, 하나 이상의 대응하는 카메라 출력을 생성하기 위해 논리적 테스트의 형태로 저장된 규칙 기 반 동작을 수행함으로써 추가로 프로세싱되는, 하나 이상의 카테고리의 프로세싱된 입력일 수 있다. 본원에서 개입으로도 지칭되는 카메라 출력은 이벤트의 분위기 및 그의 역동성에 영향을 미치기 위해 가상 환경 에서 하나 이상의 사용자의 하나 이상의 행동 수정을 프롬프트하도록 구성될 수 있으며, 이는 보다 활기있고 자 발적이며 현실적인 사용자 경험을 생성할 수 있다. 카메라 출력은 사용자 프로필에 기초하여 사용자마다 달라지는 방식으로 개별화될 수 있다. 사용자 프로필은 사용자 배경, 선호도(예컨대, 관심 주제, 음악, 음식 등), 문화, 개인 이력, 연령대, 사회적 계층, 성격, 학습 유형 등을 포함한 데이터를 포함할 수 있다. 선호도는 또한, 사용자가 큰 개입을 선호하는지 작은 개입을 선호 하는지, 다양한 정황 시나리오 또는 감정적 단서에 기초한 특정 개입 간의 선택과 같은 미리 선택된 개입 기준 을 포함할 수 있다. 시스템은 사용자 프로필을 가상 환경에서 그들이 갖는 상호작용에 기초하여 연속적으로 업 데이트하기 위해 사용되는 추가적인 사용자 프로필 데이터를 생성할 수 있다. 따라서 시스템은 각 사용자로부 터 연속적으로 학습하고 있으며 데이터를 사용하여 자체를 더 개선하고 각 사용자를 더 잘 이해하며 그의 공감 레벨을 증가시킨다. 따라서, 사용자 프로필에 기초하여 복수의 \"베스트\" 리플레이가 생성될 수 있으며, 적어도 하나의 가능한 리플레이가 각 사용자에게 제공된다. 리플레이에 추가하여, 실시예는 감지된 감정적 단서, 신체 언어 등과 같은 입력에 기초하여 가상 회의의 자동화"}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "된 요약을 생성할 수 있다. 요약은 회의가 행복했는지, 슬펐는지, 화났는지, 나빴는지, 또는 판매로 이어졌는"}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "지와 같은 하나 이상의 회의 결과를 나타내는 스마트 분석일 수 있다. 자동화된 요약은 또한, 많은 액션 포인 트가 있었는지 여부, 또는 웃음의 양, 질문의 수, 또는 가상 회의의 다양한 사람들의 참가와 같은 통계를 포함"}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "할 수 있다. 실시예는 자동화된 요약을 소셜 미디어 플랫폼 상에 자동으로 보고하기 위해 소셜 미디어와 직접연결될 수 있다. 간접적인 환경적 개입은 개인 또는 그룹 레벨로 사용자에게 제시될 수 있다. 따라서, 원하는 결과에 따라, 시 스템은 특정 개입이 특정 사용자에게만 보이고 다른 사용자에게는 보이지 않음을 결정할 수 있다. 간접 개입의 예에서, 사용자가 회의에서 한동안 말하지 않은 경우, 개입은 사용자의 선호하는 노래인 것으로 알 려진 개별화된 곡을 재생하는 것일 수 있으며, 이는 해당 사용자에게만 재생되어 그 또는 그녀가 활력을 되찾고 대화에 다시 참여하도록 프롬프트할 수 있다. 다른 예에서, 예컨대 대부분의 사용자가 짜증의 징후(예컨대, 얼 굴 표정, 음성 톤, 손 제스처, 사용된 단어 등)를 표시하고 있기 때문에, 시스템이 가상 회의의 에너지가 짜증 에 해당한다고 결정하는 경우, 이어서, 시스템은 가상 환경 내에 차가운 색, 예컨대 파란색, 녹색 또는 보라색 을 더 추가하고/하거나 가상 세션의 에너지를 전환시키는 부드러운 배경 음악을 가상 환경으로 재생하도록 결정 할 수 있다. 재생되는 노래 및/또는 색상은 사용자 프로필에 기초하여 개별적으로 선택될 수 있고, 그리하여 유사한 개입이 선택되었더라도(예컨대, 색 수정 및 음악), 각 사용자는 상이한 것을 보고 듣게 될 수 있다. 시 스템은 또한, 사용자의 현재 가상 카메라의 POV를 수정하고 그리고/또는 가상 카메라 앞에 그 또는 그녀의 감정 에 영향을 미칠 수 있는 무언가를 디스플레이하도록 결정할 수 있다. 마찬가지로, 직접 개입이 복수의 사용자에게 동시에 보내질 수 있으며, 각각은 사용자 프로필에 기초한 차이를 포함한다. 동일한 논리가 대규모 행동 개입에 적용된다. 예를 들어, 게임 레벨의 변경이 발생할 경우, 시스템은 특정 영 역으로 이동하도록 진행하기 위해 모든 사용자에게 메시지를 디스플레이할 수 있으며, 여기서 메시지의 텍스트 는 사용자 프로필에 기초하여 사용자마다 상이한 색을 갖는다. 또한, 재생되는 음악은 프로필에 기초하여 각 사용자마다 상이할 수 있다. 하나의 실시예에서, 가상 환경은 3D 보드 게임일 수 있다. 보드가 폭발하는 것과 같이 개입이 대규모일 수 있 다. 행동, 액션, 감정 등과 같은 감지된 입력에 기초하여 게임에 대해 여러 상이한 개입이 이루어질 수 있다. 이는 제품의 상호작용, 유지 및 이탈에 막대한 영향을 미칠 수 있다. 트레이닝, 영업 또는 교육 환경과 같은 또다른 실시예에서, 3D 환경의 파워를 화상 회의 서비스와 결합하는 것 은 매우 유용할 수 있다. 종종, 흥미진진한 그래픽과 조명 쇼는 제작하는데 비용이 매우 많이 든다. 이는 가 상 환경에서 매우 저렴하며 유사한 효과를 이끌어낼 수 있다. 도 3에서, 여러 유형의 입력을 취하여 기존의 행동을 결정하고, 데이터를 프로세싱하며(신경망을 통 해), 논리적 테스트를 통해 네트워크 출력을 분석 및 평가함으로써, 시스템은 가상 환경에서 상이한 행동 을 프롬프트하도록 의도된 카메라 출력의 형태로 개입을 설계할 수 있다. 본 명세서 전반에 걸쳐 기 재된 바와 같이, 개입은 학습된 공감에 기초하여 결정될 수 있다. 도 4는 마이크 및 웹캠에 의해 캡처된 시스템 입력이 4개의 신경망에 의한 프로세싱을 위 해 보내지는 예시적인 흐름도를 예시하며, 각각의 신경망은 가상 환경에서의 그들의 포지션 및 배향 을 포함하여 상이한 요소를, 예컨대 머리 움직임, 손 움직임, 감정 및 제스처를 프로세싱 하도록 구성된다. 신경망(455, 460, 465 및 470)은 입력 데이터를 정기적으로(예컨대, 매 초마다) 폴링할 수 있고, 머리 포지션 및 음성 톤과 같은 네트워크 데이터를 생성할 수 있다. 폴링은 누가 말하고 있는지, 빈번한 샘플링에 대한 필요성, 대역폭 및 기타 컴퓨팅 리소스와 같은 여러 요인에 기초하여 상이한 신 경망 케이던스에서 발생할 수 있다. 케이던스의 변경은 실시간으로 일어날 수 있다. 선택된 네트워크 데 이터는 화와 같은 감정을 결론짓기 위해 논리적 테스트에서 사용될 수 있으며, 이는 FOV의 증가, 카메라 각도의 변경, 카메라를 사용자로부터 멀리 또는 사용자 쪽으로 이동, 날씨, 사운드, 색상의 변경 등의 형태로 대응하는 출력을 프롬프트할 수 있다. 예를 들어, 음성 톤이 높아지고 머리 포지션 (그리고 아마도 얼굴 제스처)이 시스템에 의해 어떻게든 \"공격적\"인 것으로 특징지어지면, 논리적 테스트 는 신경망의 트레이닝에 기초하여 사용자가 화를 표시하고 있다고 결론을 내릴 수 있다. 출력은, 예를 들어, 가상 환경 외부의 날씨 변경, 예컨대 뇌우 시작, 천둥 소리 재생, 사용자의 음성 증폭, 배 경색 변경(예컨대, 빨간 색으로), 또는 빨간색에서 보라색이나 파란색으로 색상 페이딩의 형태로 이루어질 수 있다. 따라서, 환경적 개입은 행동 또는 감정을 프롬프트할 수 있다. 행동은 필요에 따라 추가 출력을 생성하 기 위해 가상 환경에 의해 추가로 감지된다. 일부 실시예에서, 시스템은 가상 환경에서의 이벤트의 레코딩이 원하는 때에 사용자에게 제시될 수 있게 한다."}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "다른 실시예에서, 시스템은 또한, 이벤트를 요약하는 것을 가능하게 하는 AI 알고리즘을 적용하여 이벤트의 가장 흥미로운 장면을 선택함으로써 이벤트의 요약을 생성할 수 있게 한다. 레코딩은 사용자 및 비사용자 데이터 를 포함하는 광범위한 데이터를 캡처하는 이벤트에서 하나 이상의 가상 방송 카메라를 통해 이루어질 수 있다. 이벤트 레코딩은 가상 환경의 하나 이상의 표면 상에 제시될 수 있거나, 또는 사용자가 나중에 재생할 수 있도 록 사용자 디바이스에 추후에 보내기 위해 메모리에 저장될 수 있다. 이벤트 레코딩은 필요한 경우 사용자에게 보내기 전에 더 프로세싱될 수 있다. 이벤트 레코딩은, 예를 들어, 참가자로부터의 강한 반응을 식별함으로써 감지될 수 있다. 반응은 한 번에 여러 개의 카메라 각도를 사용하여 저장될 수 있고, 나중에 가상 회의가 끝난 후, 종료 시, 또는 사용자가 재생을 요청할 때 등 언제든지 재생을 위해 메모리에 저장될 수 있다. 재생은 전 체 사용자 스크린 상에서 또는 가상 화이트보드와 같은 가상 환경의 표면 상에서 이루어질 수 있다. 재생을 위한 가장 중요한 부분은 또한 정황 시나리오에 기초할 수 있다. 예를 들어, 비즈니스 회의는 음성 인 식과 공감 트레이닝된 신경망의 조합을 통해 감지될 수 있는 주요 포인트의 선택이 필요할 수 있다. 대안적으 로, 오락 자료의 회의에서는 상이한 맥락을 가질 것이며, 가장 강한 반응 또는 웃음을 유발한 활동을 식별하는 것일 것이다. 도 5는 가상 환경에서 사운드를 생성하는 사람들의 위치 입력을 포함하는 사운드 입력과 같은 시스템 입력을 시스템이 취하는 예시적인 모델을 예시한다. 사운드 및 위치는 상이한 사운드 파라미터(51 5)를 추출하기 위해, 예컨대, 누가 말하고 있는지, 말투(tone)가 어떤지, 발언의 길이 및 메시 지의 수신자를 결정하기 위해, 프로세싱되고 집계된다. 각 사운드 파라미터에 대하여 해 당 응답이 있을 수 있으며, 이 예에서는 다음과 같다: Alex가 말하고 있고, 그의 말투는 차분하고, 10초동안 말하였고, Bill에게 말하고 있다. 이러한 사운드 분석을 위해, 필요한 응답을 추출하기 위해 프로세 싱될 수 있는 가치있는 데이터를 제공하기 위해 사운드 스펙트로그램(sound spectrogram)이 사용될 수 있다. 스펙트로그램 분석은 누군가의 음성의 에너지, 진폭 또는 주파수에 초점을 맞춘 다음 분석하고 해당 곡선의 크 기 또는 카메라 변경(540 및 545)의 속도를 추정하여 그에 따라 카메라를 동원시키기 위한 패턴을 예측하려고 시도할 수 있다. 이 데이터는 규칙 기반 논리적 테스트와 같은 엔진 코드를 거쳐 하나 이상의 카메 라 변경, 예컨대 카메라를 이동시키거나 카메라 테이크의 컷팅 속도 및 지속시간을 수정하는 것 을 결정할 수 있다. 예를 들어, 규칙은, 동일한 사람이 10초 말한 후에, 카메라 컷과 같은 카메라 변경이 나, 또는 화자의 변경을 프롬프트하기 위해 다른 화자를 향한 느린 패닝과 같은, 예컨대 Bill에게 Alex를 중단 시키고 질문하도록 프롬프트하는, 또다른 카메라 변경이 있어야 하는 것일 수 있다. 시스템은 사용자 경험 기반 머신 러닝을 개선하기 위해 자신의 피드백 데이터 또는 사용자 선호도 또는 입력을 사용하도록 트레이닝될 수 있다. 예를 들어, 시스템이 한 번 이상의 라운드 후에 10초의 말하기 후에 컷을 수 행하는 것이 너무 빠르다고 고려하는 경우, 시스템은 다음 라운드에 더 많은 초를 추가할 수 있다. 시스템은 또한, 이벤트에서 말해지고 있는 것을 결정하고 이에 따라 카메라 출력을 생성하기 위해 시맨틱 분석 및 프로세 싱을 수행할 수 있으며, 예컨대, 사용자가 \"이 회의를 마치려고 한다\"라고 말하는 경우, 카메라 출력의 프레이 밍, 조명, 컷 레이트 등은 서사적인 음악을 추가하고/하거나 가상 환경의 조명을 디밍하는 것과 같이 보다 극적 인 효과 또는 환경적 변경을 생성하도록 수정될 수 있다. 시스템은 자체를 개선하기 위해 직접적인 사용자 피 드백을 더 사용할 수 있다. Blinkist 및 Newsleo를 포함하는 프로그램에 의해 사용되는 것과 같이 데이터를 요 약하도록 트레이닝될 수 있는 상이한 알고리즘들이 있으며, 이는 시스템에 통합될 수 있다. 실시예는 가상 회의 전, 도중, 및 후에 다양한 유형의 피드백 데이터를 요청할 수 있다. 그러면, 이 데이터는, 사람들이 즐긴 것 또는 즐기지 않은 것을 감지하고 그 정보에 기초하여 행동을 변경하기 위해 신경망을 트레이 닝하는데 사용될 수 있다. 피드백은, 예를 들어, 사용자들이 가상 회의를 즐기고 있다는 것을 표시하는 사용자 인터페이스(UI) 상에 포함된 슬라이더에 기초할 수 있다. 시스템은 개입이 취해질 때와 같은 다양한 때에, 또 는 매 5분마다와 같은 주기적으로, 또는 이 둘의 조합으로, 슬라이더를 움직이도록 사용자에게 프롬프트할 수 있다. 이 방식으로, 시스템은 각각의 개별 사용자에 대한 슬라이더의 위치에 대응하는 사용자 랭킹에 기초하여 성공적인 개입과 실패한 개입 간의 차이를 결정할 수 있다. 대안적으로, 사용자는 숫자 1 내지 5 또는 \"좋음\", \"OK\", 또는 \"나쁨\"과 같은 값을 제공하도록 요청받을 수 있다. 또한, 시스템은 다양한 개입의 미묘함 (subtlety)을 결정할 수 있다. 임시 중단과 같은 가벼운 개입을 요구할 때가 있다. 논쟁이나 싸움과 같이 더 큰 개입을 요구할 때가 있을 수 있으며, 회의는 진정될 필요가 있다. 사용할 개입의 선택은 아래에 더 설명된 다. 시스템은 누군가가 있는 곳, 예컨대 다음으로 입을 움직이고 있는 곳을 예측하기 위해, 스펙트로그램 데이터에 대한 선제적 평활화(preemptive smoothing)와 같은 추가적인 프로세싱을 더 적용할 수 있다. 이에 관련하여, 예를 들어 누군가가 예컨대 5초 지속되는 발표를 하려고 하는 경우, 이는 카메라에 셋업되고 그에 따라 움직일수 있으며, 또는 가상 환경에 환경적 변경이 행해질 수 있다. 이 방식으로, 시스템은 회의 또는 이벤트의 감정 적 에너지를 캡처하고 이에 따라 카메라를 조정하거나 이 데이터에 기초하여 가상 환경을 수정할 수 있다. 선제적 평활화는 퍼즐 조각을 선제적으로 움직이는 것과 같은 2D 모션 정보에 대해 사용될 수 있지만, 이를 이 용해 수행될 수 있는 것보다 훨씬 더 많은 것이 있을 수 있으며, 예를 들어 누군가 손가락을 추적되게 하고 있 는 경우, 이는 많은 데이터이므로 보간하고 평활화할 수 있다면 FPS를 증가시킬 수 있다. 실시예는 더 높은 처 리량과 더 적은 대역폭 소비를 가능하게 하기 위해 손가락과 같은 사소한 특징의 세분성을 감소시킴으로써 FPS 를 증가시킬 수 있다. 이는 특히 대규모의 경우 유리할 수 있는데, 수백 또는 수천 개의 가상 회의가 병렬로 발생하고 있을 수 있기 때문이다. 이러한 활성 가상 회의들 각각에 대해 데이터를 조금만 절약해도 전체 컴퓨 터 및 네트워크 리소스의 많은 양을 절약할 수 있다. 도 6은, 장면으로부터 더 많은 수의 디테일을 캡처하기 위해 사용될 수 있는 다양한 각도를 캡처하기 위해 가상 환경에 복수의 가상 방송 카메라가 설치되고 장면 내로부터 사운드를 캡처하는 복수의 마이크가 있는 전체 시스템을 예시하며, 이는 대응하는 프로세싱을 프롬프트하기 위한 입력 및 출력일 수 있다. 다양한 카메라 피드는 또한, 행동 변경을 프롬프트하기 위해 더 많은 가능한 리플레이를 생성하거나 라이브 카메라 조 정을 정의하거나 환경 변경을 행하도록 프로세싱을 위한 더 많은 가능한 데이터를 생성하는 데 사용될 수 있다. 시스템은 예를 들어, 머리, 손, 감정 및 제스처에 초점을 맞추는 것과 같이, 가상 회의의 가상 환경에서 객체 및 아바타 각각의 공간 포지션 또는 위치, 배향, 발언 내용 및 상태를 더 캡처하며, 이들 전부는 대응하는 리플레이를 생성하기 위해 임의의 각도로부터 재구성될 수 있다. 공간 포지션, 배향 및 상태는 입력 데이터의 메타데이터일 수 있고, 그리하여 상이한 유형의 프로세싱 및 출력 을 도출하기 위해 입력 데이터를 더 특성화하는 데 사용될 수 있다. 결과적인 카메라 출력은, 예컨대, 다양한 유형의 카메라 프레이밍, 카메라 모션, 컷팅, 환경적 변경 등일 수 있다. 시스템은 결과 적인 출력을 생성하기 위한 추가 피드백을 제공할 수 있는 시맨틱 분석에 사용될 수 있는 발언 내용을 더 캡처한다. 본 개시는 시각적 스토리텔링 경험을 향상시키기 위해 상이한 카메라 각도를 사용하여 비디오를 캡처하기 위한 시스템 및 방법을 제공한다. 특히, 본 개시는 눈높이, 더치(Dutch), 로우 앵글, 하이 앵글, 탑 앵글 및 오버 숄더를 포함한 다양한 카메라 각도를 기재하며, 이는 독창적이고 강렬한 방식으로 비디오를 캡처하는 데 사용될 수 있다. 눈높이 카메라 각도는 피사체의 눈과 동일한 레벨에 카메라를 배치한 것을 지칭한다. 이 각도는 피사체와 시청 자 사이의 친밀감 및 유대감을 조성하기 위해 인터뷰 및 대화 장면에 종종 사용된다. 눈높이 카메라 각도를 사 용함으로써, 시청자는 자신이 피사체와 동일한 가상 환경에 있으며 피사체와 대화를 나누고 있는 것처럼 느낄 수 있다. 캔트(canted) 각도로도 알려진, 더치 카메라 각도는 경사 또는 대각 구도를 만들기 위해 카메라를 기울이는 것 을 지칭한다. 이 각도는 시청자에게 긴장감, 불안감 또는 방향 감각 상실을 일으키기 위해 사용될 수 있다. 더치 앵글은 종종 공포 또는 스릴러 영화에서 시청자에게 불안감이나 방향 감각 상실을 조성하기 위해 사용된다. 로우 앵글 카메라 각도는 카메라를 피사체의 눈 레벨 아래에 배치하여 올려다 보는 것을 지칭한다. 이 각도는 피사체의 힘, 지배력 또는 영웅적 느낌을 일으키기 위해 사용될 수 있다. 로우 앵글 샷은 종종 액션 영화에서 히어로를 실물보다 크게 보이게 하기 위해 사용된다. 하이 앵글 카메라 각도는 카메라를 피사체의 눈 레벨 위에 배치하여 내려다 보는 것을 지칭한다. 이 각도는 피 사체의 취약함 또는 무력감의 느낌을 일으키기 위해 사용될 수 있다. 하이 앵글 샷은 종종 공포 영화에서 희생 자의 취약한 느낌을 주기 위해 사용된다. 탑 앵글 카메라 각도는 카메라를 피사체의 바로 위에 배치하여 똑바로 내려다 보는 것을 지칭한다. 이 각도는 피사체에 대한 독특하고 흥미로운 관점을 제공하기 위해 사용될 수 있다. 탑 앵글 샷은 종종 다큐멘터리 또는 자연 영화에서 피사체를 상이한 관점에서 보여주기 위해 사용된다. 오버 숄더 카메라 각도는 카메라를 한 캐릭터 뒤에 배치하여 어깨 너머로 다른 캐릭터를 바라보는 것을 지칭한 다. 이 각도는 캐릭터들 사이의 친밀감 또는 긴장의 느낌을 일으키기 위해 사용될 수 있다. 오버 숄더 샷은 종종 대화 장면에서 두 캐릭터의 반응을 보여주기 위해 사용된다. 전반적으로, 본 개시는 시각적 스토리텔링 경험을 향상시키기 위해 상이한 카메라 각도를 사용하여 비디오를 캡 처하기 위한 시스템 및 방법을 제공한다. 다양한 카메라 각도를 사용함으로써, 영화 제작자는 시청자의 관심을 사로잡는 독특하고 강렬한 비주얼을 만들 수 있고 더 매력적이고 효과적인 방식으로 스토리를 전달하는 것을 도 울 수 있다. 업 샷 카메라 샷은 카메라를 피사체 아래에 배치하여 올려다 보는 것을 지칭한다. 이 샷은 피사체의 힘이나 지 배적인 느낌을 일으키기 위해 사용될 수 있다. 업 샷은 종종 액션 영화에서 히어로를 실물보다 크게 보이게 하 기 위해 사용된다. 다운 샷 카메라 샷은 카메라를 피사체 위에 배치하여 내려다 보는 것을 지칭한다. 이 샷은 피사체의 취약함이 나 무력감의 느낌을 일으키기 위해 사용될 수 있다. 다운 샷은 종종 공포 영화에서 희생자의 취약한 느낌을 주 기 위해 사용된다. 오버 숄더 카메라 샷은 카메라를 한 캐릭터 뒤에 배치하여 어깨 너머로 다른 캐릭터를 바라보는 것을 지칭한다. 이 샷은 캐릭터들 사이의 친밀감이나 긴장의 느낌을 일으키기 위해 사용될 수 있다. 오버 숄더 샷은 종종 대화 장면에서 두 캐릭터의 반응을 보여주기 위해 사용된다. 클로즈업 카메라 샷은 카메라를 피사체에 매우 근접하게 배치하여 피사체의 얼굴이나 신체를 자세히 캡처하는 것을 지칭한다. 이 샷은 친밀감의 느낌을 일으키거나 피사체의 감정을 강조하기 위해 사용될 수 있다. 익스트림 클로즈업 카메라 샷은 카메라를 피사체에 극도로 근접하게 배치하여 눈이나 입술과 같은 작은 디테일 을 캡처하는 것을 지칭한다. 이 샷은 피사체의 특정 감정이나 특징을 강조하기 위해 사용될 수 있다. 미디엄 샷 카메라 샷은 카메라를 피사체로부터 적당한 거리에 배치하여 상체 또는 허리 위쪽을 캡처하는 것을 지칭한다. 이 샷은 종종 피사체의 신체 언어나 물리적 상호작용을 보여주기 위해 사용된다. 롱 샷 카메라 샷은 카메라를 피사체로부터 먼 거리에 배치하여 전신과 주변 환경을 캡처하는 것을 지칭한다. 이 샷은 종종 장소나 셋팅을 설정하기 위해 사용된다. 싱글 샷 카메라 샷은 하나의 피사체만을 캡처하는 카메라의 배치를 지칭한다. 투샷 카메라 샷은 단일 샷에서 2개의 피사체를 캡처하는 카메라의 배치를 지칭한다. 쓰리샷 카메라 샷은 단일 샷에서 3개의 피사체를 캡처하는 카메라의 배치를 지칭한다. POV(Point of view) 카메라 샷은 캐릭터의 관점으로부터 장면을 캡처하는 카메라의 배치를 지칭한다. 이 샷은 시청자에게 마치 장면을 직접 경험하고 있는 것처럼 독특하고 몰입적인 경험을 제공하기 위해 사용될 수 있다. 본 개시는 시각적 스토리텔링 경험을 향상시키기 위해 상이한 카메라 샷을 사용하여 비디오를 캡처하기 위한 시 스템 및 방법을 제공한다. 특히, 본 개시는 업 샷, 다운 샷, 오버 더 숄더, 클로즈업, 익스트림, 미디엄 샷, 롱 샷, 익스트림, 싱글, 투, 쓰리 샷, 및 POV를 포함한 다양한 카메라 샷을 기재하며, 이는 독창적이고 강렬하 며 매력적인 방식으로 비디오를 캡처하는 데 사용될 수 있다. 도 7a 및 도 7b는 그에 따라 조합될 수 있는 입력, 사전설정 및 출력(715, 720)을 포함하는 상이한 다양한 파라미터를 예시한다. 사전설정은, 가상 회의가 이루어지는 회의의 유형과 같이, 입력이 캡 처되고 출력(715, 720)이 생성되는 정황 시나리오이며, 입력에 대해 수행되는 논리적 테스트의 유형에 영향을 미칠 수 있다. 입력은 타이핑 속도, 타이핑 볼륨(소거), 손 제스처, 말하기 시간의 양, 얼굴 (미세) 표정, 마우스/스와이프 속도, 지리적 위치, 브라우저, 로딩 시간, FPS/탭 포커스, 회의 제목, 참가자 수, 머리 포지션, 언어 독성, 디바이스, 또는 발언 리듬 등을 포함할 수 있다. 사전설정은 회의 관리자에 의해 미 리 선택될 수 있고, 이벤트 카테고리를 더 포함할 수 있으며, 예컨대, 비즈니스 회의, 영업 회의, 교실, 채팅 쇼, Alfred Hitchcock, 토크 쇼, YouTube, 슬로우 팬, 점프 컷, 스노리캠, 예측 뷰, Stanley Kubrick, 프리핸 드, TikTok, 고속 컷 레이트, 페이드 투 블랙, J/.L 컷, 셀피캠, 워크숍, 또는 스탠드업인지 등, 더 포함할 수 있다. 예를 들어, 행복의 표현인 것으로 결정되는 입력은 친구들 간의 대화와 비교하여 워크숍에서는 상이한 출력(715, 720)을 내놓을 수 있으며, 워크숍에서는 리플레이, 음악 또는 환경이 친근한 대화에서보다 격식있게 설정될 수 있다. 출력(715, 720)은 조명/라이트닝, DOF(depth of field), 블룸(bloom)(셰이더 효과), 컷팅 스 타일, 상이한 색상 설정, 카메라 각도, 파티클, 음영, 컷팅 속도, 3D 룩업 테이블, 카메라 움직임 및 카메라 샷 크기를 포함할 수 있다. 예를 들어, 회의실은 전시실과 상이하며, 판매 매장과도 상이하다. 모든 가상 환경은 상이하며, 입력은 각각의 컨텍스트에서 상이한 것을 의미하고 다른 출력(715, 720)을 생성할 수 있으므로, 프레임워크와 비슷하며, 입력과 출력 둘 다에 영향을 미친다. 사전설정은 회의 전에 선택될 수 있고 호스트에 의해 수정될 수 있다. 특정 시간대에서, 이는 사전 정의된 설정이고, 그러면 이벤트 동적인 변경이 있을 수 있거나, 또는 호스트가 개입하 여 설정을 변경하는 일이 있을 수 있거나, 또는 설정을 너무 많이 변경하여 새로운 컨텍스트(예컨대, 출력과 같 이)가 되는 일이 발생한다. 도 8은 특정 실시예의 복수의 잠재적 AI 카메라 시나리오를 예시한다. 이들은 상황을 포함할 수 있 는데, 예컨대, 누군가 이야기하는 것, 사람들 사이의 대화, 누군가 장시간 말하지 않는 것, 누군가 너무 오래 말하는 것, 회의가 곧 끝나려고 하는 것, 및 누군가 화를 표시하는 것 등을 포함할 수 있다. 데이터 입력(81 0)은, 1) 발언, 리듬, 포지션; 2) 다수의 사람들의 발언, 리듬, 포지션; 3) 마지막 발언과 지금 사이의 시간 길 이; 4) 발언 지속시간; 5) 회의 지속시간, 더 이상 남아있는 화자가 없음; 6) 발언, 리듬, 제스처 및 톤 등을 포함할 수 있다. 출력은, 화자로의 카메라 패닝 또는 줌, 다수의 카메라가 그들 사이를 전환하고 줌인 및 줌아웃, 방송 카메라를 참가자에게 포커싱하고 줌, 화자로부터 멀어져 다른 화자를 향해 카메라 이동, 화자로부 터 멀어져 다른 화자를 향해 카메라 이동, 줌인 및 극적인 카메라 효과 생성을 포함할 수 있다. 행동은, 말하 는 사람에게 포커싱하는 것, 말하는 사람을 계속 주목하는 것, 사용자가 말하기 시작하는 것, 사용자가 말을 끝 내는 것, 또는 사용자가 반응을 알게 되고 행동을 조정하는 것 등을 포함할 수 있다. 도 9는 하나의 실시예에 따른 흐름도를 예시한다. 이 실시예는 하나 이상의 가상 환경 시스템에서 인터랙 티브 가상 회의 플랫폼에 의해 수행될 수 있고, 메모리에 저장된 복수의 가상 환경을 포함할 수 있다. 플랫폼 은 또한, 단계 902에 예시된 바와 같이, 복수의 정황 시나리오 및 감정적 단서를 저장할 수 있다. 이 실시예에 서, 단계 904에서 가상 회의가 시작된다. 이는 하나 이상의 사용자 디바이스로부터 가상 회의에 참여하기 위한 하나 이상의 요청을 수신하는 것을 포함할 수 있다. 이어서, 시스템은 고유 식별자 또는 미리 프로그래밍된 값 세트에 기초하여 하나 이상의 요청을 함께 연관시킬 수 있다. 요청은 가상 회의를 형성하기 위해 연결될 수 있 다. 단계 906에서, 시스템은 위에서 더 설명된 바와 같이, 미리 프로그래밍된 정황 시나리오 또는 감정적 단서 에 대하여 하나 이상의 사용자 디바이스로부터의 비디오 및 오디오 피드를 포함하는 수신된 입력 데이터를 분석 할 수 있다. 단계 908에서, 시스템이 입력 데이터를 분석함으로써 이러한 정황 시나리오 또는 감정적 단서를 감지할 때, 시스템은 정황 시나리오 및 입력 데이터와 상관된 개입 데이터베이스로부터의 하나 이상의 개입을 선택할 수 있다. 이러한 선택은 트레이닝 데이터 및 사용자 피드백에 기초하여 시스템이 학습한 공감 레벨에 의해 영향을 받을 수 있다. 위에서 설명된 바와 같이, 여러 입력이 개입을 선택하는 데 사용될 수 있다. 예를 들어, 음성 톤, 사용자 선호도 및 음성의 사운드 레벨을 포함할 수 있다. 이 모든 정보는, 예를 들어, 대화가 험악해지고 있다는 것을 결정하는 데 사용될 수 있고, 대화를 톤 다운시키기 위해 개입이 필요하다. 시스템은 개입 데이터베이스 또는 메모리로부터 개입을 선택할 수 있으며, 이는 단계 910에서 읽어들이고 적용될 수 있다. 예를 들어, 광대가 스크린을 가로질러 진행할 수 있거나, 또는 즐거운 음악이 재생되기 시작할 수 있다. 단계 912에서, 시스템은 개입의 성공을 결정하기 위해 피드백을 수집할 수 있다. 성공은 다양한 방식으로 결정 될 수 있다. 예를 들어, 목표가 대화를 톤 다운시키는 것이라면, 시스템은 대화가 톤 다운되었는지 여부를 결 정하는 데 사용되는 톤 또는 단어를 모니터링할 수 있다. 대안적으로, 예를 들어, 시스템이 오락 목적으로 논 쟁을 장려하도록 설정되는 경우, 대화에 화가 더 많은지 여부에 기초하여 성공이 결정될 수 있다. 추가 피드백 이 하나 이상의 사용자 입력을 통해 참가자로부터 요청될 수 있다. 사용자 입력은 1 내지 10과 같은 척도로 이 루어질 수 있고, 개입을 즐겼는지 여부, 개입이 주어진 목표에 대해 얼마나 효과적이었는지, 그리고 상이한 개 입이 바람직했었는지 여부와 같은 항목을 포함할 수 있다. 그러면, 시스템은 이러한 피드백을 수집하여 향후 가상 회의에 통합하여 더 성공적이게 할 수 있다. 마지막으로, 단계 914에서, 시스템은 가상 회의를 종료하거 나, 가상 회의가 끝날 때까지 단계 906에서 시작하는 프로세스를 반복할 수 있다. 도 10은 예시적인 환경적 개입 흐름을 예시한다. 예를 들어, 가상 환경에서 사람들이 정체 상태에 있는 경우, 가상 환경은 언어적 입력의 부족 또는 밋밋한 신체 언어에 기초하여 사람들이 정체 상태에 있 다고 평가할 수 있다. 가상 환경은 그룹 행동의 변화를 일으키는 노이즈/객체/캐릭터와 같은 개입을 생 성한다. 행동은 더 측정될 수 있고, 향후 개입을 개선하기 위해 또는 개입을 변경하기 위해 개입이 성공이었는 지 실패였는지 결정하기 위한 입력으로서 사용될 수 있다. 예를 들어, 볼륨, 색상, 톤, 형상 등은 전부 개입(들)에 대한 물리적 및 언어적 응답에 기초하여 신경망이 평가할 수 있을 다양한 행동 시스템을 이끌어낼 수 있다. 도 11은 컴퓨터 시스템의 컴퓨팅 디바이스의 예를 예시한다. 컴퓨팅 디바이스는 프로세서(들)(예 컨대, CPU, GPU 또는 다른 프로세싱 유닛), 메모리 및 통신 인터페이스(들)(예컨대, 네트워크 인터 페이스)를 포함하여, 다른 디바이스와 통신하고 이들 디바이스로부터 입력, 예컨대 타이핑 속도, 타이핑 볼륨(소거), 손 제스처, 말하는 시간의 양, 얼굴 (미세) 표정, 마우스/스와이프 속도, 지리적 위치, 브라우저, 로딩 시간, FPS/탭 포커스, 회의 제목, 참가자 수, 머리 포지션, 언어 독성, 디바이스, 또는 발언 리듬을 수신할 수 있으며, 인터랙티브 화상 회의 플랫폼은 입력 중 하나 이상에 기초하여 개입을 선택하도록 구성된다. 메모리 는 RAM과 같은 휘발성 메모리 및/또는 ROM 및 저장 매체와 같은 비휘발성 메모리를 포함할 수 있다. 저 장 매체의 예는 솔리드 스테이트 저장 매체(예컨대, 솔리드 스테이트 드라이브 및/또는 착탈식 플래시 메모리), 광학 저장 매체(예컨대, 광학 디스크) 및/또는 자기 저장 매체(예컨대, 하드 디스크 드라이브)를 포함한다. 전 술한 명령어(예컨대, 소프트웨어 또는 컴퓨터 판독가능 코드)는 메모리의 임의의 휘발성 및/또는 비휘발 성 메모리 컴포넌트에 저장될 수 있다. 컴퓨팅 디바이스는, 일부 실시예에서, 입력 디바이스(들) (예컨대, 키보드, 마우스, 조이스틱, 컨트롤러 또는 터치스크린) 및 출력 디바이스(들)(예컨대, 디스플레 이, 헤드업 디스플레이, AR 디스플레이, VR 디스플레이, 프린터)를 더 포함할 수 있다. 예를 들어, 사용자 디 바이스(들)(110, 120)가 태블릿 컴퓨터로서 구현될 수 있는 경우, 사용자 디바이스(들)(110, 120)는 터치스크린 및 디스플레이를 가질 수 있다. 컴퓨팅 디바이스의 전술한 요소들은 하나 이상의 버스를 나타내는 버스 를 통해 서로 연결될 수 있다. 일부 실시예에서, 컴퓨팅 디바이스의 프로세서(들)는 CPU와 GPU 둘 다를 포함한다. 하나의 예로서, 시스템은 너무 많은 사람들이 말하고 있는 동안 한 사람이 조용하다는 것을 감지할 수 있다. 시스템은 다수의 입력들로부터의 노이즈를 캡처하고, 많은 것이 능동적이고 하나가 수동적(또는 침묵)이라고 결 론을 내릴 수 있으며, 모든 능동 사용자들의 볼륨 감소와 침묵 사용자에 대한 스포트라이트를 프롬프트할 수 있 다. 이러한 행동 개입의 결과로서, 다른 사용자들은 덜 말할 수 있고, 침묵한 사람은 더 많이 말할 수 있으며, 회의의 균형을 회복시킨다. 사용자는 자신의 프로필에서 시스템이 자신이나 다른 사람들을 위해 이 개입을 얼 마나 적극적으로 사용하기를 원하는지 설정할 수 있다. 얼굴 지오메트리 좌표를 생성하기 위해 얼굴 감지기 AI 알고리즘이 사용될 수 있다. 얼굴 지오메트리 좌표는 사용자의 머리의 배향에 대한 스티어링 벡터를 생성할 수 있다. 사용자가 머리를 돌릴 때에, 두 가지 표현이 가능하다. 첫 번째로, 위치 벡터는 3D 공간에서 사용자를 향하는 가상 카메라에 의해 감지될 수 있으며, 이는 가상 환경에 있을 수 있는 가상 회의의 3D 지오메트리 내에서 2D 카메라 피드가 올바르게 배향되어 있다는 착각을 유지하기 위해 역방향으로 움직일 수 있다. 이 기술은 매치 무빙(match-moving)이라고 불릴 수 있다. 컨트롤러로서 사 용자의 머리는 또한 조명과 같은 가상 환경 내의 엔티티에 적용될 수 있고, 사용자가 움직임에 따라 동적인 그 림자를 드리울 수 있다. 두 번째로, 위치 벡터는 3D 공간에서 사용자의 1인칭 또는 2인칭 시점 중 어느 하나를 나타내는 카메라에 의해 감지될 수 있다. 사용자가 자신의 머리를 좌우로, 상하로 또는 앞뒤로 움직일 때에, 스케일링/제한 및 최소 펜 스 로직이 있을 수 있다. 이는 3D 엔진이 문자 그대로가 아닌(non-literal) 지오메트리를 그릴 수 있게 할 수 있다. 가상 현실 3D 공간에서, 사용자의 움직임은 1:1로 캡처될 수 있는 반면, 2D 스크린 상에서 사용자의 움 직임은 그에 따라 스케일링된다. 손 포즈는 3D 공간에서 사용자의 손가락 좌표를 맵핑하는 데 사용될 수 있다. 각 너클/숫자의 3D 좌표는 본질 적으로 실시간으로 캡처될 수 있다. 그 다음, 이 정보는 리깅된 3D 손 모델로 전달될 수 있다. 신경 속도 감 지 네트워크에 의해 식별되는 바와 같이, 사용자의 피부 색상이 이마의 대표 패치로부터 추가적으로 캡처될 수 있다. 그 다음, 이 색상/텍스처는 사용자 손의 리깅된 3D 모델의 텍스처의 방출 특성에 매핑될 수 있다. 그러 면 이는 가상 환경이 개인화되고 현실적인 3D 손을 디스플레이할 수 있게 할 수 있다. 손은 또한 제스처/잡기/ 꼬집기 또는 기타 움직임에 대해 리깅될 수 있다. 손의 더 많은 총체적 움직임이 가상 환경의 임의의 시각적 특성, 예컨대 날씨, 조명, 외부 바람 등에 매핑될 수 있다. 특정 실시예가 해결할 수 있는 추가적인 문제로는, 대부분의 사람들이 상이한 학습 스타일, 개성, 능력 등을 갖 기 때문에, 많은 학생의 학습을 늦출 수 있는 표준화된 교육 환경을 포함한다. 사용자가 당면한 콘텐츠에 감정 적으로 연결되지 않을 때, 그들은 지루해지고, 이는 학습에 대한 저항과 학생의 스트레스를 증가시킨다. 각각 의 사용자는 사용자 입력 선호도 또는 상이한 개입에 대한 사용자의 반응(들)을 갖는 과거 경험에 기초할 수 있 는 미리 정의된 프로필을 가질 수 있다. 따라서, 각각의 프리젠테이션은 각 사람에 대해 맞춤화될 수 있다. 한 사람은 보다 공식적인 학교와 같은 가상 환경에서 더 잘 학습할 수 있는 반면, 다른 사람은 가상 공원과 같 은 가상 환경에서 더 잘 학습할 수 있다. 하나의 실시예는 스마트 가상 환경을 생성함으로써 이 문제를 해결할 수 있다. 이 실시예에서, 시스템은 교실 또는 세미나 컨텍스트에서 학습을 향상시키기 위해 사용될 수 있다. 사용자 프로필은 선호도, 예컨대 관심 주 제, 음악, 음식, 배경 이미지, 문화, 개인 이력, 연령대, 사회적 계층, 성격, 학습 유형 등을 포함할 수 있다. 시스템은 가상 환경에서의 상호작용 및 개입에 대한 응답에 기초하여 사용자 프로필을 연속적으로 업데이트하는 데 사용될 수 있는 추가적인 사용자 프로필 데이터를 생성할 수 있다. 따라서 시스템은 각 사용자로부터 연속 적으로 학습하고 있으며 데이터를 사용하여 자체를 더 개선하고 각 사용자를 더 잘 이해하며 그의 공감 레벨을 증가시킨다. 사용자 학습 스타일은 또한, 각 사용자가 자신의 학습 효율을 증가시키기 위해 바람직하게 학습하는 방식을 결 정하기 때문에 유용하다. 관리자(예컨대, 교사)는 클래스에 전달될 콘텐츠를 정의할 수 있다. 시스템은 각 학생의 콘텐츠 및 사용자 프 로필을 분석하고, 각 학생의 사용자 프로필과 가장 잘 맞는 복수의 프레젠테이션 미디어를 출력할 수 있다. 따 라서, 각각의 학생은 유사한 정보의 상이한 프리젠테이션을 수신할 수 있다. 각 학생과 가장 잘 맞는 프레젠테 이션 미디어를 찾음으로써, 학생은 자신의 특정 학습 스타일에 연결하여 콘텐츠에 대한 감정적 연결을 증가시키 며 학습을 촉진하고 향상시킬 수 있다."}
{"patent_id": "10-2024-0057429", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "사용자 프로필을 고려한 콘텐츠의 프로세싱은, 학생마다 상대적으로 상이한 관점에서 콘텐츠를 요약하는 것을 포함할 수 있으며, 특정 학생의 경우, 시스템은 더 감정적으로 채워지거나 그의 특정 프로필에 더 공감할 수 있 는 단어를 사용하는 반면, 다른 학생의 경우 더 기술적이고 더 자세한 수준의 단어를 사용할 수 있다. 시스템 은 또한 특정 상황을 예시할 수 있는 예를 생성하거나 선택할 수 있으며, 이러한 방식으로 학생의 공감을 더 잘 이끌어낼 수 있다. 또한, 선택될 시각적 매체의 유형은 사용자마다 다를 수 있다. 예를 들어, 학생이 시각적 학습자인 것으로 정의되는 경우, 시스템은 짧은 동영상을 만들고/만들거나 상이한 소스로부터의 동영상을 선택 하거나 혼합하여 학생을 위한 보다 효율적인 학습 프레젠테이션을 구성할 수 있다. 보다 분석적이고 독서를 선 호하는 다른 사용자의 경우, 더 분석적인 예시가 있는 텍스트 또는 비디오 클립이 그에게 제공될 수 있다. 색 상, 배경, 사운드, 분위기 등의 변경을 포함하는 가상 환경에서의 수정은 또한 개별적으로 발생할 수 있으며, 각 학생의 학습을 향상시키기 위해 사용될 수 있다. 일부 실시예에서, 시스템은 실질적으로 실시간으로 사용자 입력의 판독에 기초하여 전달될 콘텐츠의 특정 양상 을 수정할 수 있다. 예를 들어, 사용자가 혼란스러워 보이거나 의심을 표현하는 경우, 시스템은 이를 읽어들이 고, 그의 경험을 개선하기 위해 무엇을 할 수 있는지 사용자에게 물어보고 옵션을 제공하거나 사용자가 자신의 상황을 설명할 수 있는 공간을 남겨둘 수 있다. 시스템은 또한, 시스템이 사용자의 행동의 변화를 감지함에 따 라 실시간으로 콘텐츠, 비주얼, 오디오 등의 특정 부분을 적응시킬 수 있다. 프리젠테이션은 비디오, 오디오, 햅틱 경험, 전달되고 있는 수업을 표현하기 위한 가상 환경의 총 수정의 형태로 이루어질 수 있고, AR 및/또는 VR 등에서 행해질 수 있다. 시스템은 또한 반드시 가상 환경일 필요는 없고, 이들 상이한 유형의 콘텐츠 프리 젠테이션을 제공하는 프로그램일 수 있으며, 이 중 하나는 3D 가상 환경에서의 프리젠테이션을 위한 것일 수 있 지만, 이에 한정되는 것은 아니다. 시스템은 또한, 아이가 자료를 이해함을 보장하기 위해 추가적인 개입을 취 하도록 교사나 부모에게 이 정보를 제공할 수 있다. 시스템은 또한 유사한 학습 스타일을 갖는 학생들을 분류하고 유사하거나 동일한 매체로 수업을 제공할 수 있다. 이는 또한, 동일한 학습 스타일을 가진 사용자들이 서로의 해석과 수업의 프로세싱으로부터 이익을 얻을 수 있기 때문에, 그룹 학습 및 상호작용을 촉진하는 데 도움이 될 수 있다. 사용자들은 하나 이상의 학습 스타 일(예컨대, 시각적 또는 오디오)로 분류될 수 있고, 동일한 분류 하의 사용자와 유사한 가상 환경 및 개입을 수 신할 수 있다. 하나 이상의 프로세서에 의해 실행가능한 명령어는 비일시적 컴퓨터 판독가능 매체 상에 저장될 수 있다. 따라 서, 본 개시에서 컴퓨터 구현 방법이 기재될 때마다, 본 개시는 또한, 하나 이상의 프로세서에 의해 실행될 때, 하나 이상의 프로세서로 하여금 컴퓨터 구현 방법을 수행하게 하는 명령어를 저장하는 비일시적 컴퓨터 판독가 능 매체를 기재하는 것으로 이해되어야 한다. 비일시적 컴퓨터 판독가능 매체의 예는, RAM, ROM, 솔리드 스테 이트 저장 매체(예컨대, 솔리드 스테이트 드라이브), 광학 저장 매체(예컨대, 광학 디스크) 및 자기 저장 매체 (예컨대, 하드 디스크 드라이브)를 포함한다. 비일시적 컴퓨터 판독가능 매체는 컴퓨터 시스템의 메모리의 일 부일 수 있거나 또는 임의의 컴퓨터 시스템으로부터 분리될 수 있다. 예시적인 실시예의 위의 기재에서, 본 개시를 간소화하고 다양한 발명의 양상 중 하나 이상의 이해를 돕기 위해, 다양한 특징들이 때때로 단일 실시예, 도면 또는 그 설명에서 함께 그룹화된다는 것을 이해해야 한다. 그러나, 본 개시의 이러한 방법은 청구된 발명이 각 청구항에서 명시적으로 언급된 것보다 더 많은 특징을 필요로 한다는 의도를 반영하는 것으로 해석되어서는 안 된다. 오히려, 다음의 청구항이 반영하는 바와 같이, 발명 의 양상은 앞서 개시된 단일 실시예의 모든 특징들보다 더 적은 특징에 있다. 본 명세서에서 단어 \"또는\"의 사 용은 비배타적인 것으로 의미되며, 각각의 개별 요소를 단독으로 또는 이들의 임의의 조합을 포함하도록 해석되 어야 한다. 따라서, 상세한 설명 다음의 청구항들은 본 상세한 설명에 명시적으로 통합되며, 각각의 청구항은 본 개시의 별도의 실시예로서 독립적으로 존재한다. 또한, 본원에 기재되는 일부 실시예는 일부 특징을 포함하고 다른 실시예에 포함된 다른 특징을 포함하지는 않 지만, 당업자에 의해 이해되는 바와 같이 상이한 실시예의 특징의 조합은 본 개시의 범위 내에 있으며 상이한 실시예를 형성하는 것을 의미한다. 예를 들어, 다음의 청구항에서, 청구된 실시예 중 임의의 실시예가 임의의 조합으로 사용될 수 있다. 따라서, 특정 실시예가 기재되었지만, 당업자는 본 개시의 사상을 벗어나지 않고서 이에 다른 그리고 추가 수정 이 행해질 수 있다는 것을 인식할 것이며, 그러한 모든 변경 및 수정이 본 개시내용의 범위 내에 속하는 것으로 청구하도록 의도된다. 예를 들어, 블록도로부터 기능이 추가되거나 삭제될 수 있고, 기능 블록 간에 동작이 상 호교환될 수 있다. 본 개시의 범위 내에서 기재된 방법에 단계가 추가되거나 삭제될 수 있다. 위에 개시된 주제는 제한적인 것이 아니라 예시적인 것으로 간주되어야 하며, 첨부된 청구항은 본 개시의 진정 한 사상 및 범위 내에 속하는 모든 그러한 수정, 향상 및 다른 구현을 커버하도록 의도된다. 본 명세서에서 \" 또는\"이라는 용어의 사용은 임의의 이용가능한 옵션들 중 하나 이상을 의미하며, 상호 배타적인 리스트일 필요 는 없다. 따라서, 법에 의해 허용되는 최대 범위까지, 본 개시의 범위는 다음의 청구항 및 그 등가물의 가장 광범위한 허용가능한 해석에 의해 결정되어야 하며, 전술한 상세한 설명에 의해 제한되거나 한정되어서는 안 된 다. 본 개시의 다양한 구현이 기재되었지만, 본 개시의 범위 내에서 더 많은 실시예 및 구현이 가능하다는 것 이 당업자에게 명백할 것이다. 따라서, 본 개시는 제한되어서는 안 된다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7a 도면7b 도면8 도면9 도면10 도면11"}
{"patent_id": "10-2024-0057429", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 명세서에 통합되고 이의 일부를 구성하는 첨부 도면은 본 개시의 예시적인 양상을 예시하며 기재와 함께 본 개시의 원리를 설명한다. 도 1은 본 개시의 시스템, 방법 및 다른 양상이 구현될 수 있는 예시적인 환경의 개요를 예시한다. 도 2는 현행 시스템 및 방법의 프로세스 흐름의 일반적인 예를 예시한다. 도 3은 일부 실시예에 따라 개입을 행하기 위한 예시적인 프로세스를 예시한다. 도 4는 본 개시의 양상에 따라, 컴퓨팅 환경에서 인공 지능을 사용하여 개입을 행하는 예시적인 방법의 예시적 인 흐름도를 예시한다. 도 5는 본 개시의 양상에 따라, 컴퓨팅 환경에서 인공 지능을 사용하는 예시적인 모델을 예시한다. 도 6은 본 개시의 양상에 따른 예시적인 가상 방송 환경을 예시한다. 도 7a는 본 개시의 양상에 따른 예시적인 입력 및 출력을 예시한다. 도 7b는 본 개시의 양상에 따른 추가의 예시적인 입력 및 출력을 예시한다. 도 8은 본 개시의 특정 실시예의 복수의 잠재적인 AI 카메라 시나리오를 예시한다. 도 9는 본 개시의 특정 실시예에 따른 예시적인 흐름도를 예시한다. 도 10은 본 개시의 특정 실시예에 따른 예시적인 환경적 개입 흐름을 예시한다. 도 11은 본 개시의 다양한 실시예의 방법에 대한 명령어를 수행할 수 있는 컴포넌트의 예시적인 시스템을 예시 한다."}
