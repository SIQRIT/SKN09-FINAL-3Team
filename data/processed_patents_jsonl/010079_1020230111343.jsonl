{"patent_id": "10-2023-0111343", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0073747", "출원번호": "10-2023-0111343", "발명의 명칭": "영상 처리 방법 및 그 방법을 수행하는 전자 장치", "출원인": "삼성전자주식회사", "발명자": "이 저우"}}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "프로세서에 의해 수행되는, 파노라마 분할 모델을 통한 영상 처리 방법에 있어서,복수의 영상 프레임을 포함하는 영상의 영상 특징을 얻는 단계;신경망을 사용하여 상기 영상 특징을 기반으로 상기 영상의 타겟 객체 표현을 결정하는 단계; 및상기 타겟 객체 표현을 기반으로 상기 영상의 파노라마 분할 결과를 결정하는 단계를 포함하는, 영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 신경망을 사용하여 상기 영상 특징을 기반으로 상기 영상의 타겟 객체 표현을 결정하는 단계는,상기 신경망을 사용하여 상기 영상 특징에 대해 복수의 반복 처리를 수행하여 상기 영상의 타겟 객체 표현을 결정하는 단계를 포함하는,영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 신경망을 사용하여 상기 영상 특징에 대해 상기 복수의 반복 처리를 수행하여 상기 영상의 상기 타겟 객체표현을 결정하는 단계는,상기 신경망을 사용하여, 상기 영상 특징 및 상기 영상의 이전 반복 처리에 의한 객체 표현에 기초하여 반복 처리를 수행하여 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계를 포함하는,영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 복수의 반복 처리 중 첫 번째 반복 처리의 경우, 상기 이전 반복 처리에 의한 객체 표현은 미리 구성된 초기 객체 표현인,영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서,상기 영상 특징 및 상기 영상의 이전 반복 처리에 의한 객체 표현에 기초하여 상기 반복 처리를 수행하여 상기영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계는,상기 영상의 이전 반복 처리에 의한 객체 표현에 대해 변환 처리하여 마스크를 얻는 단계;공개특허 10-2024-0073747-3-상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크를 처리하여 제1 객체 표현을 얻는단계; 및상기 제1 객체 표현을 기반으로 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계를 포함하는,영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크를 처리하여 상기 제1 객체 표현을 얻는단계는,상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크에 대해 어텐션 처리를 수행하여 마스크와 관련된 객체 표현을 얻는 단계; 및상기 마스크와 관련된 객체 표현과 상기 이전 반복 처리에 의한 객체 표현을 기반으로 셀프 어텐션 처리 및 분류 처리를 수행하여 제1 객체 표현을 얻는 단계를 포함하는,영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크에 대해 상기 어텐션 처리를 수행하여상기 마스크와 관련된 객체 표현을 얻는 단계는,상기 영상 특징에 대응하는 키 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크를 기반으로 제2 객체 표현을 얻는 단계;상기 제2 객체 표현에 기초하여, 상기 영상에 포함된 객체 카테고리를 나타내는 제1 확률을 결정하는 단계; 및상기 제1 확률, 상기 영상 특징에 대응하는 값 특징 및 상기 영상 특징을 기반으로 상기 마스크와 관련된 객체표현을 얻는 단계를 포함하는,영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제5항에 있어서, 상기 제1 객체 표현을 기반으로 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계는,상기 영상 특징 및 상기 제1 객체 표현에 기초하여 적어도 하나의 영상 프레임 중 각각의 영상 프레임에 대응하는 객체 표현을 결정하는 단계; 및상기 제1 객체 표현과 상기 결정된 각각의 영상 프레임에 대응하는 객체 표현을 기반으로 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계를 포함하는,공개특허 10-2024-0073747-4-영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 영상 특징 및 상기 제1 객체 표현에 기초하여 상기 적어도 하나의 영상 프레임 중 각각의 영상 프레임에대응하는 객체 표현을 결정하는 단계는,상기 영상 특징에 대응하는 키 특징 및 상기 제1 객체 표현에 기초하여 제4 객체 표현을 결정하는 단계;상기 제4 객체 표현에 기초하여 상기 영상에 포함된 상기 객체 카테고리를 나타내는 제2 확률을 결정하는 단계;및상기 제2 확률 및 상기 영상 특징에 대응하는 값 특징에 기초하여 상기 적어도 하나의 영상 프레임 중 각각의영상 프레임에 대응하는 객체 표현을 결정하는 단계를 포함하는,영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서, 상기 제1 객체 표현과 상기 결정된 각각의 영상 프레임에 대응하는 객체 표현을 기반으로 상기 영상의 현재 반복 처리에 의한의 객체 표현을 결정하는 단계는,상기 결정된 각각의 영상 프레임에 대응하는 객체 표현에 대해 분류 처리 및 셀프 어텐션 처리를 수행하여 상기영상에 대응하는 제3 객체 표현을 획득하는 단계; 및상기 제1 객체 표현과 상기 제3 객체 표현을 기반으로 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계를 포함하는,영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,상기 타겟 객체 표현을 기반으로 상기 영상의 파노라마 분할 결과를 결정하는 단계는,상기 타겟 객체 표현에 대해 선형 변환 처리를 수행하는 단계; 및선형 변환 처리된 상기 타겟 객체 표현과 상기 영상 특징을 기반으로 상기 영상의 마스크 정보를 결정하고, 선형 변환 처리된 상기 타겟 객체 표현을 기반으로 상기 영상의 카테고리 정보를 결정하는 단계를 포함하는,영상 처리 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "프로세서에 의해 수행되는, 파노라마 분할 모델의 훈련 방법에 있어서,상기 파노라마 분할 모델은 제1 모듈 및 제2 모듈을 포함하고,공개특허 10-2024-0073747-5-훈련 데이터를 얻는 단계 - 상기 훈련 데이터는 훈련 영상, 훈련 영상의 제1 영상 특징 및 훈련 영상에 대응하는 샘플 파노라마 분할 결과를 포함함 -;상기 제1 영상 특징의 프레임 순서를 바꿈으로써 제2 영상 특징을 얻는 단계;상기 제1 모듈을 통해, 상기 제1 영상 특징 및 상기 제2 영상 특징에 각각 기초하여 상기 훈련 영상의 제1 예측객체 표현 및 제2 예측 객체 표현을 결정하는 단계;상기 제2 모듈을 통해, 상기 제1 예측 객체 표현 및 상기 제2 예측 객체 표현에 각각 기초하여 상기 훈련 영상의 제1 예측 결과 및 제2 예측 결과를 결정하는 단계; 및상기 샘플 파노라마 분할 결과, 상기 제1 예측 객체 표현, 상기 제2 예측 객체 표현, 상기 제1 예측 결과 및 상기 제2 예측 결과를 기반으로 타겟 손실 함수를 사용하여 상기 파노라마 분할 모델을 훈련하는 단계를 포함하는,훈련 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 샘플 파노라마 분할 결과, 상기 제1 예측 객체 표현, 상기 제2 예측 객체 표현, 상기 제1 예측 결과 및 상기 제2 예측 결과를 기반으로 타겟 손실 함수를 사용하여 상기 파노라마 분할 모델을 훈련하는 단계는,상기 제1 예측 객체 표현 및 상기 제2 예측 객체 표현에 기초하여 제1 유사 행렬을 결정하는 단계;상기 샘플 파노라마 분할 결과, 상기 제1 예측 결과 및 상기 제2 예측 결과에 기초하여 제2 유사 행렬을 결정하는 단계; 및상기 제1 유사 행렬 및 상기 제2 유사 행렬을 기반으로 타겟 손실 함수가 최소로 결정되는 경우, 훈련된 파노라마 분할 모델을 출력하는 단계를 포함하는,훈련 방법."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "전자 장치에 있어서,메모리; 프로세서; 및메모리에 저장된 컴퓨터 프로그램;을 포함하고,상기 프로세서는 상기 컴퓨터 프로그램을 실행하여 제1항 내지 제13항 중 어느 한 항에 따른 방법의 단계를 구현하는 것인, 전자 장치."}
{"patent_id": "10-2023-0111343", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "컴퓨터 프로그램이 저장된 컴퓨터 판독 가능 저장 매체에 있어서, 상기 컴퓨터 프로그램이 프로세서에 의해 실행될 때 제1항 내지 제13항 중 어느 한 항에 따른 방법의 단계를 구현하는 것인, 컴퓨터 판독 가능 저장 매체.공개특허 10-2024-0073747-6-"}
{"patent_id": "10-2023-0111343", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 출원의 실시예는 영상 처리 방법, 전자 장치, 및 저장 매체를 제공한다. 상기 영상 처리 방법은, 복수의 영상 프레임을 포함하는 영상의 영상 특징을 얻는 단계, 신경망을 사용하여 영상 특징을 기반으로 영상의 타겟 객체 표현을 결정하는 단계, 타겟 객체 표현을 기반으로 영상의 파노라마 분할 결과를 결정하는 단계를 포함한다. 본 출원의 실시예는 클립 레벨의 타겟 객체 표현을 통해 영상의 파노라마 분할 결과를 예측하고, 네트워크 구조를 효과적으로 단순화하여 분할 정확도와 견고성을 향상시킬 수 있다. 전자 장치에서 수행되는 상기 영상 처리 방법 은 인공지능 모델을 이용하여 수행될 수 있다."}
{"patent_id": "10-2023-0111343", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 영상 처리 방법, 전자 장치, 저장 매체 및 프로그램 제품에 관한 것으로, 더욱 상세하게는 인공지능 기술을 이용한 영상 처리 분야에 관한 것이다."}
{"patent_id": "10-2023-0111343", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "파노라마 분할은 2차원 이미지의 각 픽셀에 레이블 정보를 할당하는 과정이다. 동영상(video)의 파노라마 분할 은 시간 영역에서의 파노라마 분할의 확장으로, 각 이미지에 대한 파노라마 분할 외에도 객체 추적 작업, 즉 다 른 이미지에서 동일한 인스턴스에 속하는 픽셀에 동일한 레이블을 할당하는 작업 또한 결합한다. 기존 영상 파노라마 분할 기술에서는 단일 프레임 이미지에 대한 파노라마 객체의 표현을 결정할 때 파노라마 분할의 정확도가 낮다. 영상의 각 영상 프레임 간의 대응 정보를 얻기 위해서는 추가적인 추적 모듈이 필요하므 로 네트워크 구조가 복잡해지게 된다."}
{"patent_id": "10-2023-0111343", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 출원의 실시예는 네트워크 구조를 단순화하고 파노라마 분할의 정확성 및 견고성을 개선하는 것을 목표로 하 는 영상 처리 방법, 전자 장치, 및 저장 매체를 제공한다. 해당 기술 방안은 다음과 같다. 일 실시 예에 따른, 파노라마 분할 모델을 통한 영상 처리 방법은, 복수의 영상 프레임을 포함하는 영상의 영상 특징을 얻는 단계, 신경망을 사용하여 상기 영상 특징을 기반으로 상기 영상의 타겟 객체 표현을 결정하는 단계, 및 상기 타겟 객체 표현을 기반으로 상기 영상의 파노라마 분할 결과를 결정하는 단계를 포함한다. 상기 신경망을 사용하여 상기 영상 특징을 기반으로 상기 영상의 타겟 객체 표현을 결정하는 단계는, 상기 신경망을 사용하여 상기 영상 특징에 대해 복수의 반복 처리를 수행하여 상기 영상의 타겟 객체 표현을 결정하는 단계를 포함할 수 있다. 상기 신경망을 사용하여 상기 영상 특징에 대해 상기 복수의 반복 처리를 수행하여 상기 영상의 상기 타겟 객체 표현을 결정하는 단계는, 상기 신경망을 사용하여, 상기 영상 특징 및 상기 영상의 이전 반복 처리에 의한 객체 표현에 기초하여 반복 처리를 수행하여 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계를 포함 할 수 있다. 상기 복수의 반복 처리 중 첫 번째 반복 처리의 경우, 상기 이전 반복 처리에 의한 객체 표현은 미리 구성된 초 기 객체 표현일 수 있다. 상기 영상 특징 및 상기 영상의 이전 반복 처리에 의한 객체 표현에 기초하여 상기 반복 처리를 수행하여 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계는, 상기 영상의 이전 반복 처리에 의한 객체 표현에 대해 변환 처리하여 마스크를 얻는 단계, 상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스 크를 처리하여 제1 객체 표현을 얻는 단계, 및 상기 제1 객체 표현을 기반으로 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계를 포함할 수 있다. 상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크를 처리하여 상기 제1 객체 표현을 얻는 단계는, 상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크에 대해 어텐션 처리를 수행하 여 마스크와 관련된 객체 표현을 얻는 단계, 및 상기 마스크와 관련된 객체 표현과 상기 이전 반복 처리에 의한 객체 표현을 기반으로 셀프 어텐션 처리 및 분류 처리를 수행하여 제1 객체 표현을 얻는 단계를 포함할 수 있다. 상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크에 대해 상기 어텐션 처리를 수행하여 상기 마스크와 관련된 객체 표현을 얻는 단계는, 상기 영상 특징에 대응하는 키 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크를 기반으로 제2 객체 표현을 얻는 단계, 상기 제2 객체 표현에 기초하여, 상기영상에 포함된 객체 카테고리를 나타내는 제1 확률을 결정하는 단계, 및 상기 제1 확률, 상기 영상 특징에 대응 하는 값 특징 및 상기 영상 특징을 기반으로 상기 마스크와 관련된 객체 표현을 얻는 단계를 포함할 수 있다. 상기 제1 객체 표현을 기반으로 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계는, 상기 영상 특징 및 상기 제1 객체 표현에 기초하여 적어도 하나의 영상 프레임 중 각각의 영상 프레임에 대응하는 객체 표 현을 결정하는 단계, 및 상기 제1 객체 표현과 상기 결정된 각각의 영상 프레임에 대응하는 객체 표현을 기반으 로 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계를 포함할 수 있다. 상기 영상 특징 및 상기 제1 객체 표현에 기초하여 상기 적어도 하나의 영상 프레임 중 각각의 영상 프레임에 대응하는 객체 표현을 결정하는 단계는, 상기 영상 특징에 대응하는 키 특징 및 상기 제1 객체 표현에 기초하여 제4 객체 표현을 결정하는 단계, 상기 제4 객체 표현에 기초하여 상기 영상에 포함된 상기 객체 카테고리를 나 타내는 제2 확률을 결정하는 단계, 및 상기 제2 확률 및 상기 영상 특징에 대응하는 값 특징에 기초하여 상기 적어도 하나의 영상 프레임 중 각각의 영상 프레임에 대응하는 객체 표현을 결정하는 단계를 포함할 수 있다. 상기 제1 객체 표현과 상기 결정된 각각의 영상 프레임에 대응하는 객체 표현을 기반으로 상기 영상의 현재 반 복 처리에 의한의 객체 표현을 결정하는 단계는, 상기 결정된 각각의 영상 프레임에 대응하는 객체 표현에 대해 분류 처리 및 셀프 어텐션 처리를 수행하여 상기 영상에 대응하는 제3 객체 표현을 획득하는 단계, 및 상기 제1 객체 표현과 상기 제3 객체 표현을 기반으로 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계를 포함할 수 있다. 상기 타겟 객체 표현을 기반으로 상기 영상의 파노라마 분할 결과를 결정하는 단계는, 상기 타겟 객체 표현에 대해 선형 변환 처리를 수행하는 단계, 및 선형 변환 처리된 상기 타겟 객체 표현과 상기 영상 특징을 기반으로 상기 영상의 마스크 정보를 결정하고, 선형 변환 처리된 상기 타겟 객체 표현을 기반으로 상기 영상의 카테고리 정보를 결정하는 단계를 포함할 수 있다. 일 실시 예에 따른, 프로세서에 의해 수행되는, 파노라마 분할 모델의 훈련 방법에 있어서, 상기 파노라마 분할 모델은 제1 모듈 및 제2 모듈을 포함하고, 훈련 데이터를 얻는 단계 - 상기 훈련 데이터는 훈련 영상, 훈련 영 상의 제1 영상 특징 및 훈련 영상에 대응하는 샘플 파노라마 분할 결과를 포함함 -, 상기 제1 영상 특징의 프레 임 순서를 바꿈으로써 제2 영상 특징을 얻는 단계, 상기 제1 모듈을 통해, 상기 제1 영상 특징 및 상기 제2 영 상 특징에 각각 기초하여 상기 훈련 영상의 제1 예측 객체 표현 및 제2 예측 객체 표현을 결정하는 단계, 상기 제2 모듈을 통해, 상기 제1 예측 객체 표현 및 상기 제2 예측 객체 표현에 각각 기초하여 상기 훈련 영상의 제1 예측 결과 및 제2 예측 결과를 결정하는 단계, 및 상기 샘플 파노라마 분할 결과, 상기 제1 예측 객체 표현, 상 기 제2 예측 객체 표현, 상기 제1 예측 결과 및 상기 제2 예측 결과를 기반으로 타겟 손실 함수를 사용하여 상 기 파노라마 분할 모델을 훈련하는 단계를 포함한다. 상기 샘플 파노라마 분할 결과, 상기 제1 예측 객체 표현, 상기 제2 예측 객체 표현, 상기 제1 예측 결과 및 상 기 제2 예측 결과를 기반으로 타겟 손실 함수를 사용하여 상기 파노라마 분할 모델을 훈련하는 단계는, 상기 제 1 예측 객체 표현 및 상기 제2 예측 객체 표현에 기초하여 제1 유사 행렬을 결정하는 단계, 상기 샘플 파노라마 분할 결과, 상기 제1 예측 결과 및 상기 제2 예측 결과에 기초하여 제2 유사 행렬을 결정하는 단계, 및 상기 제 1 유사 행렬 및 상기 제2 유사 행렬을 기반으로 타겟 손실 함수가 최소로 결정되는 경우, 훈련된 파노라마 분할 모델을 출력하는 단계를 포함할 수 있다. 본 출원의 실시예에서 제공하는 기술 방안의 유익한 효과는 다음과 같다: 본 출원은 영상 처리 방법, 전자 장치, 저장 매체 및 프로그램 제품을 제공하고, 구체적으로, 획득된 영상 특징 은 적어도 2개의 영상 프레임을 포함하는 영상에 대응하고, 즉, 영상 특징은 적어도 2개의 영상 프레임의 특징 정보에 대응하고; 그후 신경망을 사용하여 영상 특징을 기반으로 영상의 타겟 객체 표현을 결정하고 타겟 객체 표현을 기반으로 영상의 파노라마 분할 결과를 예측할 수 있다. 본 출원은 클립(clip) 레벨의 대상(예를 들어 적어도 두 개 영상 프레임에 대응하는 영상 특징, 객체 표현)에 대해 처리하는 것으로, 네트워크에 추가 객체 추적 모듈을 구축할 필요 없이 영상의 각 영상 프레임 간의 대응 정보를 얻을 수 있고, 네트워크 구조를 효과적 으로 단순화할 수 있다. 또한, 클립 레벨의 대상 자체는 영상에서 시간 영역 정보를 전달하므로 본 출원 방안의 구현은 영상 정보를 보다 완전하게 활용할 수 있으며, 이는 파노라마 분할의 정확성과 견고성을 향상시키는데 도움이 된다."}
{"patent_id": "10-2023-0111343", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "실시예들에 대한 특정한 구조적 또는 기능적 설명들은 단지 예시를 위한 목적으로 개시된 것으로서, 다양한 형 태로 변경되어 구현될 수 있다. 따라서, 실제 구현되는 형태는 개시된 특정 실시예로만 한정되는 것이 아니며, 본 개시의 범위는 실시예들로 설명한 기술적 사상에 포함되는 변경, 균등물, 또는 대체물을 포함한다. \"제1\" 또는 \"제2\" 등의 용어를 다양한 구성요소들을 설명하는데 사용될 수 있지만, 이런 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 해석되어야 한다. 예를 들어, \"제1 구성요소\"는 \"제2 구성요 소\"로 명명될 수 있고, 유사하게 \"제2 구성요소\"는 \"제1 구성요소\"로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"포함하다\" 또 는 \"가지다\" 등의 용어는 설명된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함으 로 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 명세서에서 사용된 단수형 \"일\", \"하나\", 및 \"상기\" 등은 특별히 언급되지 않는 한 복수형도 포함할 수 있음"}
{"patent_id": "10-2023-0111343", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "은 당업자에게 있어 자명하다. 본 명세서의 실시예에서 사용된 \"포함\" 및 \"함유\"라는 용어는 상응하는 특징이제시된 특징, 정보, 데이터, 단계, 동작, 요소 및/또는 구성요소로서 구현될 수 있음을 의미하고, 본 기술분야 에서 지원하는 다른 특징, 정보, 데이터, 단계, 동작, 요소, 구성 요소 및/또는 이들의 조합 등을 배제하지 않 는다. 한 요소가 다른 요소에 \"연결\"되거나 \"결합\"되었다고 말할 때, 하나의 요소는 다른 요소에 직접 연결되거 나 결합될 수 있고, 하나의 요소 및 다른 요소가 중간 요소를 통해 연결 관계가 구성될 수도 있다. 또한, 본 명 세서에서 \"연결\" 또는 \"결합\"은 무선 연결 또는 무선 결합을 포함할 수 있다. 본 명세서에서 \"및/또는\"의 용어 는 해당 용어가 정의한 항목 중 적어도 하나를 나타내며, 예를 들어 \"A 및/또는 B\"는 \"A\"로 구현, \"B\"로 구현 또는 \"A 및 B\"로 구현됨을 나타낸다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 해당 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적으로 사용되 는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미를 갖는 것으로 해석되어야 하며, 본 개시에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 인공지능(AI)은 디지털 컴퓨터 또는 디지털 컴퓨터로 제어되는 기계를 사용하여 인간 지능을 시뮬레이션, 연장 및 확장하고, 환경을 인식하고, 지식을 획득하고, 지식을 사용하여 최상의 결과를 얻는 이론, 방법, 기술 및 응 용 시스템이다. 즉, 인공지능은 지능의 본질을 이해하고 인간의 지능과 유사하게 반응할 수 있는 새로운 지능 기계를 생산하려는 컴퓨터 과학의 종합 기술이다. 인공지능은 다양한 지능형 기계의 설계 원리와 구현 방법을 연구하여 기계가 인식, 추론 및 의사 결정 기능을 갖도록 하는 것이다. 인공지능 기술은 하드웨어 측면의 기술 과 소프트웨어 측면의 기술을 모두 포함하는 광범위한 분야를 포함하는 포괄적인 학문이다. 인공지능의 기본 기 술에는 일반적으로 센서, 특수 인공지능 칩, 클라우드 컴퓨팅, 분산 스토리지, 빅 데이터 처리 기술, 운영/상호 작용 시스템, 전자 기계 통합 등과 같은 기술이 포함된다. 인공지능 소프트웨어 기술은 주로 컴퓨터 비전 기술, 음성 처리 기술, 자연어 처리 기술 및 기계 학습/딥 러닝, 자율 주행, 스마트 교통 등 주요 방향을 포함한다. 구체적으로, 본 출원은 확률 이론, 통계학, 근사 이론, 볼록 분석, 알고리즘 복잡성 이론 등 다양한 학과와 관 련된 여러 분야의 교차 학과인 머신 러닝(Machine Learning, ML) 및 컴퓨터 비전 기술(Computer Vision, CV)과 관련이 있다. 컴퓨터가 인간의 학습 행동을 시뮬레이션하거나 구현하여 새로운 지식 또는 기술을 습득하고 기존 지식 구조를 재구성하여 자신의 성능을 지속적으로 향상시키는 방법을 전문적으로 연구한다. 머신 러닝은 인공 지능의 핵심이자 컴퓨터를 지능화하는 근본적인 방법으로 인공지능의 다양한 분야에 적용된다. 머신 러닝 및 딥 러닝은 일반적으로 인공 신경망, 신뢰 네트워크, 강화 학습, 전이 학습, 귀납 학습, 형식 학습 등 기술을 포함 한다. 컴퓨터 비전은 기계가 \"보는\" 방법을 연구하는 과학으로, 보다 구체적으로, 사람의 눈 대신 카메라와 컴 퓨터를 이용해 대상을 식별하고 측정하는 머신 비전을 말하며, 더 나아가 그래픽 처리를 통해 사람의 눈 관찰이 나 이미지의 기기 감지에 더 적합하도록 컴퓨터 처리를 수행하는 것을 말한다. 과학 분야로서 컴퓨터 비전은 관 련 이론 및 기술을 연구하여 이미지 또는 다차원 데이터에서 정보를 얻을 수 있는 인공지능 시스템을 구축하려 고 시도한다. 컴퓨터 비전 기술에는 일반적으로 이미지 처리, 이미지 인식, 이미지 의미론적 이해, 이미지 검색, OCR, 영상 처리, 영상 의미론적 이해, 영상 콘텐츠/행동 인식, 3D 객체 재구성, 3D 기술, 가상 현실, 증 강 현실, 동기 위치 지정 및 지도 구축, 자율 주행, 스마트 교통 등 기술이 포함되며, 얼굴인식, 지문인식 등 일반적인 생체인식 기술도 포함된다. 본 개시는 영상 처리 방법, 전자 장치, 저장 매체 및 프로그램 제품을 제안하고, 구체적으로 강력한 클립-객체 중심 표현 학습을 통해 영상 판옵틱 분할 알고리즘(Robust Clip-object-centric Representation Learning for Video Panoptic Segmentation)을 구현할 수 있으며, 해당 방안의 구현에는 객체 추적 모듈이 필요하지 않고 알 고리즘 구조가 단순화되며 동시에 분할 정확도 및 견고성이 일정 수준 향상된다. 이하, 몇 가지 예시적 실시예에 대해 설명한다. 구현 방법은 상호 참고, 참조 또는 결합될 수 있으며, 상이한 구현 방법 중 동일한 용어, 유사한 기능 및 유사한 구현 단계 등에 대해서는 반복 설명하지 않는다. 도 1은 일 실시 예에 따른 영상 처리 방법의 흐름도이며, 해당 방법은 단말기 또는 서버와 같은 임의의 전자 장 치에 의해 실행될 수 있다. 단말기는 스마트폰, 태블릿, 노트북, 데스크톱 컴퓨터, 스마트 스피커, 스마트 워치, 자동차 탑재 장치 등일 수 있다. 서버는 독립적인 물리적 서버이거나 여러 물리적 서버로 구성된 서버 클 러스터 또는 분산 시스템일 수 있으며, 클라우드 서비스, 클라우드 데이터베이스, 클라우드 컴퓨팅, 클라우드 기능, 클라우드 스토리지, 네트워크 서비스, 클라우드 커뮤니케이션, 미들웨어 서비스, 도메인 네임 서비스, 보 안 서비스, CDN, 빅데이터 및 인공지능 플랫폼 등 기본적인 클라우드 컴퓨팅 서비스를 제공하는 클라우드 서버 가 될 수도 있으며, 이에 국한되지 않는다.실시예를 더 잘 설명하기 위해, 이하 도 2 및 도 3을 결합하여 파노라마 분할 장면에 대해 설명한다. 이미지 파노라마 분할은 2차원 이미지의 각 픽셀에 레이블 정보(예, 시맨틱 레이블 및 인스턴스 레이블의 정 보)를 할당하는 프로세스이다. 이미지 콘텐츠는 두 가지 범주로 나눌 수 있으며, 하나의 유형은 'stuff'으로, 잔디, 하늘, 건축물 등 서로 다른 객체를 구별할 필요가 없는 콘텐츠를 말하며, 도 2의 의미론적 분할 결과와 같이 시맨틱 레이블을 예측한다. 또 다른 유형은 'thing'으로, 사람, 자동차 등 서로 다른 객체를 구분할 필요 가 있는 콘텐츠로 도 2의 인스턴스 분할 결과와 같이 인스턴스 레이블을 예측한다. 파노라마 분할 작업은 도 2 의 파노라마 분할 결과와 같이 시맨틱 분할과 인스턴스 분할의 복합 작업으로 볼 수 있다. 영상 파노라마 분할은 시간 영역에서 이미지 파노라마 분할의 확장이다. 구체적으로, 이미지의 각 프레임에 대 해 파노라마 분할을 수행하는 것 외에도 영상 파노라마 분할은 객체 추적 작업도 결합하며, 즉 서로 다른 이미 지에서 동일한 인스턴스에 속하는 픽셀에 동일한 레이블을 할당한다. 일 실시예에서, 영상 파노라마 분할의 장면에 대해, 임의의 영상 클립에서 파노라마 객체를 나타내기 위해 클립 레벨의 객체 표현이 제안된다. 구체적으로, 'stuff'와 'thing'이라는 두 가지 내용이 파노라마 객체로 일률적으 로 표현된다. 'stuff' 콘텐츠(예, 하늘, 잔디 등)의 경우 이미지에서 동일한 유형의 모든 픽셀은 파노라마 객체 를 형성한다(예. 모든 하늘 범주의 픽셀은 하늘 파노라마 객체를 형성함). 'thing' 콘텐츠(예, 보행자, 자동차 등)의 경우 각 개인이 파노라마 객체를 구성한다. 단일 영상 프레임의 파노라마 객체 표현은 프레임 쿼리, 즉 단일 이미지에 대한 객체 표현이라고 할 수 있다. 영상 클립을 처리할 때, 단일 프레임 상의 파노라마 객체는 영상 클립 내의 파노라마 객체, 즉 영상의 객체 표현(도 5와 같은 클립-객체 표현(clip-object representation))으로 처리할 수 있으며, 본 출원의 실시예에서는 클립 쿼리(Clip Query)라고도 하며, 클립 쿼 리는 하나의 벡터(벡터의 길이는 C이고, C는 하나의 하이퍼파라미터일 수 있음)로 나타낼 수 있다. 영상 클립 상에 L개의 클립 쿼리가 있고 L이 0보다 크거나 같은 정수일 수 있다고 가정하면, 해당 영상 클립 상의 모든 클 립 파노라마 객체 표현은 LxC 차원의 매트릭스를 형성하며, 즉 클립 객체 중심의 표현(clip-object-centric representation)이다. 예를 들어 영상 클립에 포함된 클립 쿼리는 아래 [수학식 1]과 같이 나타낼 수 있다. 수학식 1"}
{"patent_id": "10-2023-0111343", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "[수학식 1]에서, 영상 클립의 모든 클립 쿼리는 L개의 벡터(벡터 길이는 C)를 나타내고, 각 벡터는 하나의 클립 레벨의 대상을 나타내며; C는 벡터 차원으로, 클립 레벨의 대상의 복잡도를 제어하는데 사용할 수 있는 하이퍼 파라미터이다. 선택적으로, 클립 쿼리는 네트워크 훈련 단계에서 무작위로 초기화할 수 있고 시간 영역 및 공간 정보와 같은 시공간 정보와의 상호 작용을 통해 점진적으로 최적화할 수 있는 일련의 학습 가능한 매개변수이다. 구체적으로, 도 3에 도시된 바와 같이, 영상 프레임이 시간에 따라 진행함에 따라 4개의 클립 쿼리의 예시가 제 공되며, 각 클립 쿼리는 각 영상 프레임의 특징맵에 일대일로 대응한다. 이하, 일 실시예의 첨부된 도면에 포함될 수 있는 벡터 및 텐서를 나타내는 일부 상수의 차원, 연산자 등에 대 해 설명한다. T: 영상 클립의 길이를 나타내며, 즉 영상 클립의 프레임 수이다. L: 클립 쿼리의 최대 수를 나타내며 클립의 파노라마 객체 수이다. C: 특징맵 및 클립 쿼리의 채널 차원(channel dimension)을 나타낸다. H 및 W: 이미지(즉, 영상 프레임)의 해상도를 나타내며, H는 높이이고 W는 너비이다. 또한 2차원 이미지의 길이 와 너비를 나타낼 수도 있다. nc: 총 범주 수를 나타낸다. : 요소별 가산 연산을 나타낸다. : 행렬 곱셈 연산을 나타낸다. 이하, 일 실시 예에 따른 영상 처리 방법에 대해 설명한다. 구체적으로, 도 1에 도시된 바와 같이, 일 실시 예에 따른 영상 처리 방법은 다음 단계 S101-S103를 포함한다. 단계 S101: 영상의 영상 특징을 획득하고; 상기 영상은 적어도 두 개의 영상 프레임을 포함한다. 단계 S102: 신경망을 사용하여 상기 영상 특징에 기초하여 상기 영상의 타겟 객체 표현을 결정한다. 단계 S103: 상기 타겟 객체 표현에 기초하여 상기 영상의 파노라마 분할 결과를 결정한다. 구체적으로, 본 출원은 파노라마 분할 모델을 통해 상기 영상 처리 방법을 구현할 수 있다. 도 4에 도시된 바와 같이, 파노라마 분할 모델은 마스크 디코더와 분할 헤드 모듈을 포함할 수 있다. 선택적으로, 도 5에 도시된 바 와 같이, 파노라마 분할 모델은 특징 추출 모듈을 더 포함할 수 있다. 즉, 파노라마 분할 모델은 다른 네트워크 에서 영상(영상 클립이라고도 함, 적어도 두 개의 영상 프레임을 포함하는 데이터를 나타냄)에 대해 추출하여 얻은 특징맵을 처리할 수 있고, 또한 획득한 영상 자체에서 특징 추출을 수행한 다음 추출된 영상 특징을 처리 할 수도 있다. 특징 추출 모듈(Clip feature extractor)은 백본 네트워크(Res50-backbone) 및 픽셀용 픽셀 디코더(pixel decoder)와 같은 범용 특징 추출 네트워크 구조를 채택할 수 있고, 본 개시는 이에 대해 제한하지 않는다. 선택 적으로, 도 5에 도시된 바와 같이, 특징 추출 모듈은 입력된 영상 클립에 대해 특징 추출을 수행하여 클립 레벨 멀티 스케일 특징(clip-level multi-scale features)을 추출할 수 있다. 입력된 영상 클립의 모든 프레임(t-T+1, t-T+2, 쪋, t; 여기서 T는 영상 클립의 길이, 즉 해당 영상 클립에 포 함된 영상 프레임 수를 나타냄)에 대해, 특징 추출 모듈을 통해 영상 특징(클립 특징이라고도 함, clip feature)을 추출할 수 있다. 특징 추출 모듈은 여러 프레임 또는 단일 프레임을 입력할 수 있다. 여러 영상 프 레임을 포함하는 영상의 경우 여러 영상 프레임을 특징 추출 모듈에 함께 입력하거나 각 영상 프레임을 프레임 별로 특징 추출 모듈에 입력할 수 있다. 선택적으로, 특징 추출 작업을 단순화하고 특징 추출 속도를 향상시키 기 위해 단일 프레임 입력 방법을 통해 영상 특징을 추출할 수 있다. 마스크 디코더(Masked decoder)는 N개의 계층적 상호작용 모듈 HIM(Hierarchical Interaction Modules)로 구성 될 수 있고, 여기서 N은 1보다 크거나 같은 정수이므로 마스크 디코더는 복수의 캐스케이드된 HIM으로 구성될 수 있다. 예를 들어 각 레벨의 계층적 상호작용 모듈은 구조는 같지만 매개변수는 다르다. 구체적으로, 마스크 디코더는 입력된 영상 특징에 기초하여 영상의 타겟 객체 표현을 결정할 수 있다. 분할 헤드(Segmentation Head) 모듈은 카테고리, 마스크, 객체 ID와 같은 타겟 객체 표현을 기반으로 영상 클립 의 파노라마 객체의 분할 결과를 출력할 수 있다. 구체적으로, 획득된 마스크는 여러 프레임에 정의된 클립 파 노라마 객체의 마스크이며, 서로 다른 프레임에서 동일한 마스크에 속하는 픽셀은 서로 다른 프레임 상의 객체 의 대응 관계를 나타낸다. 즉 이는 서로 다른 영상 프레임 간의 일치 또는 추적 작업 없이 객체 ID를 자동으로 얻을 수 있음을 의미한다. 마스크는 이미지 필터의 템플릿으로 이해할 수 있다. 특징맵에서 타겟 객체를 추출할 때, n*n(n의 값은 수용 필드, 정확도 등 요소에 기반하여 고려할 수 있고, 예를 들어 3*3, 5*5, 7*7 등으로 설 정할 수 있음)의 행렬을 통해 이미지를 필터링한 후, 타겟 객체를 강조 표시할 수 있다. 이하, 본 출원의 실시예에서의 상호작용 과정에 대해 설명한다. 일 실행 가능한 실시예에서, 단계 S102에서, 신경망을 사용하여 상기 영상 특징을 기반으로 상기 영상의 타겟 객체 표현을 결정하는 단계는, 다음 단계 A1을 포함한다. 단계 A1: 신경망을 사용하여 상기 영상 특징에 대해 복수의 반복 처리를 수행하여 상기 영상의 타겟 객체 표현 을 결정한다. 각각의 반복 처리는, 상기 영상 특징 및 영상의 이전 반복 처리에 의한 객체 표현에 기초하여 반복 처리를 수행 하여 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 것을 포함한다.선택적으로, 복수의 반복 처리 중 첫 번째 반복인 경우 상기 이전 반복 처리에 의한 객체 표현은 미리 구성된 초기 객체 표현이다. 선택적으로, 마스크 디코더는 하나 이상의 계층적 상호작용 모듈을 포함할 수 있다. 두 개 이상의 계층적 상호 작용 모듈을 포함할 때, 각 계층적 상호작용 모듈은 캐스케이드되고 정렬되며, 이전 레벨 모듈의 출력은 다음 레벨 모듈의 입력으로 작용할 수 있다. 해당 네트워크 구조를 기반으로 마스크 디코더는 영상 특징의 복수의 반 복을 구현할 수 있다. 반복 횟수는 마스크 디코더에 포함된 계층적 상호작용 모듈의 수와 관련된다. 레벨의 계층적 상호작용 모듈은 해당 영상의 영상 특징과 이전 반복 처리에 의한 객체 표현(즉 이전 레벨의 계 층적 상호작용 모듈의 출력)에 대해 처리할 수 있고, 현재 반복 처리에 의한 객체 표현을 출력할 수 있다. 1차 적으로 정렬된 계층적 상호작용 모듈로서 그 입력은 영상의 영상 특징과 사전 구성된 초기 객체 표현을 포함한 다. 두 번째 및 후속 레벨에서 정렬된 계층적 상호작용 모듈로서 그 입력은 해당 영상 특징과 이전 레벨의 계층 적 상호작용 모듈에 의해 출력된 객체 표현(즉 이전 반복 처리에 의한 객체 표현)을 포함한다. 선택적으로, 파노라마 분할 처리가 필요한 영상과 비교할 때 초기 객체 표현은 동일하다. 도 4 및 도 5에 도시된 바와 같이, 마스크 디코더에서 1차적 계층적 상호작용 모듈의 입력은 특징 추출 모듈에 의해 추출된 영상 특징뿐 아니라 초기 클립 쿼리(초기 객체 표현이라고도 함, 즉 네트워크 훈련에서 얻은 매개 변수)을 더 포함하며, 후속 레벨의 계층적 상호작용 모듈에 의한 클립 쿼리 입력은 이전 레벨의 계층적 상호작 용 모듈에 의해 출력된 클립 쿼리다. 구체적으로, 마스크 디코더는 클립 쿼리와 영상 특징 사이의 관계 연산을 수행하여 해당 영상 클립의 특정 쿼리(즉, 타겟 객체 표현, 즉 도 5에 도시된 클립 쿼리 출력)를 얻을 수 있다. 본 출원 실시예의 알고리즘 프로세스는 클립 쿼리와 영상 클립에서 추출된 영상 특징 간의 관계 연산을 수행하 고, 최종적으로 클립 파노라마 객체의 분할 결과를 얻을 때까지 각 클립 쿼리를 영상 클립 상의 특정 클립 파노 라마 객체와 정렬한다. 일 실행 가능한 실시예에서, 도 6a에 도시된 바와 같이, 마스크 디코더는 N개의 동일한 계층적 상호작용 모듈을 포함할 수 있다. 선택적으로, 도 6b에 도시된 바와 같이, 계층적 상호작용 모듈은 제1 계층적 상호작용 모듈과 제2 계층적 상호작용 모듈로 구별될 수 있다. 즉, 마스크 디코더는 M개의 제1 계층적 상호작용 모듈과 N-M개의 제2 계층적 상호작용 모듈을 포함할 수 있고, M은 N보다 작거나 같다. 도 7a에 도시된 바와 같이, 제1 계층적 상호작용 모듈의 네트워크 구조는 클립 특징 쿼리 상호작용 모듈을 포함할 수 있고, 도 7b에 도시된 바와 같이, 제2 계층적 상호작용 모듈은 클립 특징 쿼리 상호작용 모듈 및 클립 프레임 쿼리 상호작용 모듈을 포함할 수 있다. 즉, 제1 계층적 상호작용 모듈은 제2 계층적 상호 작용 모듈의 단순화된 네트워크일 수 있다. 선택적으로, 마스크 디코더는 제1 계층적 상호작용 모듈과 제2 계층적 상호작용 모듈의 임의의 조합으로 구성될 수 있다. 선택적으로, 마스크 디코더에 의해 채택된 계층적 상호작용 모듈이 크립 특징 쿼리 상호작용 모듈을 포함 할 때, 단계 A1에서, 상기 영상 특징과 상기 영상의 이전 반복 처리에 의한 객체 표현에 기초하여 반복 처리를 수행하여 상기 영상의 현재 반복 처리에 의한 객체 표현을 결정하는 단계는 다음 단계 A11-A13을 포함한다. 단계 A11: 이전 반복 처리에 의한 객체 표현에 대해 변환 처리하여 마스크를 얻는다. 단계 A12: 상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크를 처리하여 제1 객체 표현을 얻는다. 단계 A13: 상기 제1 객체 표현을 기반으로 현재 반복에 대한 객체 표현을 결정한다. 단계 A11에서 이전 반복에 의한 객체 표현의 변환에 대해 도 13과 같은 네트워크 구조의 마스크 분기를 통해 처 리될 수 있다. 즉, 이전 반복 처리에 의한 객체 표현에 대해 복수의 선형 변환 처리를 수행한 후, 영상 특징과 행렬 곱셈을 수행하여 최종적으로 변환된 마스크를 얻는다. 일 실행 가능한 실시예에서, 도 7a 및 도 7b에 도시된 바와 같이, 현재의 반복 처리에서, 클립 특징 쿼리 상호 작용 모듈에 입력되는 데이터는 단계 S101에서 획득된 영상 특징(클립 특징 X)과 이전 반복 처리에 의한 객체 표현(현재 반복이 처음이면 클립 쿼리 S는 도 5와 같은 초기 클립 쿼리가 될 수 있고, 현재 반복이 처음이 아니면 클립 쿼리 S는 이전 레벨의 계층적 상호 작용 모듈이 출력한 클립 쿼리가 될 수 있음) 및 이전 반복 처 리에 의한 객체 표현의 변환에서 얻은 이진 마스크(클립 마스크 MA)를 포함할 수 있다. 클립 쿼리 및 특징맵을처리한 후, 제1 객체 표현이라고도 하는 클립 쿼리 출력을 얻을 수 있다. 선택적으로, 제1 객체 표현을 현재 반 복 처리에 의한 객체 표현으로 직접 사용할 수 있다. 클립 특징 쿼리 상호작용 모듈에 의해 구현된 처리는 클립 특징의 모든 픽셀로부터 객체의 위치 및 외관 정보를 얻을 수 있다. 마스크를 사용하여 관련 없는 영역의 영향을 제거하고 학습 프로세스를 가속화할 수 있다. 일 실행 가능한 실시예에서, 단계 A12에서, 상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마 스크를 처리하여 제1 객체 표현을 얻는 단계는, 다음 단계 A121-A122를 포함한다. 단계 A121: 상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크에 대해 어텐션 처리를 수행 하여 마스크와 관련된 객체 표현을 얻는다. 단계 A122: 상기 마스크와 관련된 객체 표현과 상기 이전 반복 처리에 의한 객체 표현을 기반으로 셀프 어텐션 처리 및 분류 처리를 수행하여 제1 객체 표현을 얻는다. 일 실행 가능한 실시예에서, 도 8에 도시된 바와 같이, 입력된 클립 특징 X(영상 특징), 클립 쿼리 S(이전 반복 처리에 의한 객체 표현) 및 클립 마스크 MA(이전 반복 처리에 의한 객체 표현으로부터 변환됨)에 대해, 마스크 어텐션 모듈을 사용하여 처리를 수행하여 세분화된 클립 쿼리를 얻을 수 있다. 이를 바탕으로, 단계 A121 의 처리 결과를 요소별로 클립 쿼리 S에 추가할 수 있고, 추가된 합 결과는 정규화될 수 있다(도 8에 도시된 합 산 및 정규화 모듈을 통해 구현됨). 이후, 셀프 어텐션 모듈을 통해 정규화 처리된 결과 A에 대해 셀 프 어텐션 처리하고, 출력된 결과는 요소별로 정규화 처리된 결과 A에 가산하고, 가산된 합 결과는 정규화 처리 한다(도 8에 도시된 합산 및 정규화 모듈을 통해 구현됨). 그런 다음, FFN(Feed Forward Network) 모듈 을 통해, 정규화 처리된 결과 B에 대해 피드포워드 연산(분류 처리)을 수행하고, 출력된 결과를 요소별로 정규화 처리된 결과 B에 가산하고, 가산된 합 결과는 정규화 처리하여(도 8에 도시된 합산 및 정규화 모듈(50 6)을 통해 구현됨), 클립 쿼리 출력(제1 객체 표현)을 얻는다. 도 8에 도시된 바와 같이, 셀프 어텐션 모듈에서, 입력된 Q, K 및 V는 클립 쿼리(정규화 처리된 결과 A)의 차원 L, C에 대응한다. 선택적으로, 도 8에 도시된 네트워크 구조에서, FFN 모듈 및 어텐션 메커니즘을 포함하는 다른 모듈의 처 리 순서는 교환될 수 있다. 도 8에 도시된 네트워크 구조에서 셀프 어텐션 모듈 및 FFN 모듈의 위치 가 교환될 수 있는 경우, 교환 후 합산 및 정규화 모듈의 출력은 FFN 모듈의 입력으로 사용될 수 있 다. 선택적으로, 단계 A121에서, 상기 영상 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크에 대해 어 텐션 처리를 수행하여 마스크와 관련된 객체 표현을 얻는 단계는, 다음 단계 A121a-A121c를 포함한다. 단계 A121a: 상기 영상 특징에 대응하는 키 특징, 상기 이전 반복 처리에 의한 객체 표현 및 상기 마스크를 기 반으로 제2 객체 표현을 얻는다. 단계 A121b: 상기 제2 객체 표현에 기초하여, 상기 영상에 포함된 객체 카테고리를 나타내는 제1 확률을 결정한 다. 단계 A121c: 상기 제1 확률, 상기 영상 특징에 대응하는 값 특징 및 상기 영상 특징을 기반으로 상기 마스크와 관련된 객체 표현을 얻는다. 일 실행 가능한 실시예에서, 상기 단계 A121a 내지 A121c의 입력 및 출력은 텐서일 수 있고, 텐서의 차원은 도 10을 참조할 수 있다. 먼저, 입력 Q(클립 쿼리, 이전 반복 처리에 의한 객체 표현, 차원 L, C에 해당), K(영상 특징을 키 특징으로 사 용, 차원 THW, C에 해당) 및 V(영상 특징을 값 특징으로 사용, 차원 THW, C에 해당)에 대해 각각 선형 작업을 수행한다. 도 10에 도시된 선형 연산 모듈(701, 702, 703)은 하나 이상의 선형 연산 레이어(Linear Layer)를 포 함할 수 있다. 이어서, 선형 연산 모듈의 출력과 선형 연산 모듈의 출력은 모듈을 통해 행렬 곱 셈 연산을 수행할 수 있고, 모듈의 출력 결과를 모듈을 통해 클립 마스크 MA에 요소별로 추가하여 모 듈의 출력(제2 객체 표현)을 모듈을 통해 softmax 연산(THW 차원에서 수행 가능)을 수행하여 카테고 리에 해당하는 제1 확률로 변환할 수 있다. 해당 제1 확률은 영상에 포함된 모든 객체 카테고리를 나타낼 수 있 고, 예를 들어 카테고리가 자동차인 경우 확률은 x이고, 카테고리가 가로등 기둥인 경우 확률은 y이다. 이를 바탕으로, 모듈의 출력(변환된 제1 확률)은 모듈을 통해 선형 변환 처리된 후 값 특징에 의해 행렬 곱 될 수 있고, 모듈에 의해 출력된 결과는 모듈을 통해 선형 변환 처리된 후의 영상 특징에 요소별로 중첩되어 최종 클립 쿼리 출력(마스크 관련 객체 표현)을 얻을 수 있다. 도 10에 도시된 위치 정보 (P_q) 및 (P_k)는 Q 및 K에 대응하는 위치 정보로서 범용 위치 정보 부호화 모듈에 의해 생성될 수 있고, 위치 정보는 마스크 어텐션 모듈에 대해 선택적이다. 선택적으로, 마스크 디코더에 의해 채택된 계층적 상호작용 모듈이 클립 특징 쿼리 상호작용 모듈 및 클립 프레임 쿼리 상호작용 모듈을 포함할 때, 단계 A13에서, 상기 제1 객체 표현을 기반으로 현재 반복 처리에 의한 객체 표현을 결정하는 단계는, 다음 단계 A131-A132를 포함한다. 단계 A131: 상기 영상 특징 및 상기 제1 객체 표현에 기초하여 적어도 하나의 영상 프레임 중 각각의 영상 프레 임에 대응하는 객체 표현을 결정한다. 단계 A132: 상기 제1 객체 표현과 결정된 영상 프레임에 대응하는 객체 표현을 기반으로 현재 반복 처리에 의한 객체 표현을 결정한다. 구체적으로, 도 9에 도시된 바와 같이, 입력된 클립 특징 X(영상 특징) 및 클립 쿼리 S(클립 특징 쿼리 상호작 용 모듈의 출력, 제1 객체 표현)에 대해, 프레임 쿼리 생성 모듈을 통해 처리하여 [1, T] 프레임 상 의 프레임 쿼리(여기서 T는 영상에 포함된 영상 프레임의 수를 나타냄)를 얻는다. 각 C차원의 클립 쿼리 벡터에 대해, T 프레임의 각 프레임에 대한 C차원 프레임 쿼리 벡터를 얻을 수 있으므로 전체 차원은 L, C에서 TL, C로 변경된다. 도 7b에 도시된 바와 같이, 현재의 반복 처리에서, 클립 특징 쿼리 상호작용 모듈에 의해 출력된 클립 쿼 리를 획득한 후, 해당 클립 쿼리와 프레임 쿼리(각 영상 프레임에 대응하는 객체 표현) 사이에서도 처리를 수행 하여 현재 레벨의 계층적 상호작용 모듈의 최종 출력(현재 반복 처리에 의한 객체 표현)을 얻을 수 있다. 선택적으로, 제1 객체 표현은 하나 이상의 프레임 쿼리 또는 영상에 포함된 모든 영상 프레임에 각각 1:1 대응 하는 프레임 쿼리로 처리될 수 있다. 선택적으로, 단계 A131에서, 상기 영상 특징 및 상기 제1 객체 표현에 기초하여 적어도 하나의 영상 프레임 중 각각의 영상 프레임에 대응하는 객체 표현을 결정하는 단계는, 다음 단계 A131a-A131c를 포함한다. 단계 A131a: 상기 영상 특징에 대응하는 키 특징 및 상기 제1 객체 표현에 기초하여 제4 객체 표현을 결정한다. 단계 A131b: 상기 제4 객체 표현에 기초하여 상기 영상에 포함된 객체 카테고리를 나타내는 제2 확률을 결정한 다. 단계 A131c: 상기 제2 확률 및 상기 영상 특징에 대응하는 값 특징에 기초하여 적어도 하나의 영상 프레임 중 각 영상 프레임에 대응하는 객체 표현을 결정한다. 구체적으로, 상술한 단계 A131a-A131c의 입력과 출력은 텐서가 될 수 있으며, 텐서의 차원은 도 11을 참조할 수 있다. 먼저, 입력 Q(클립 쿼리, 제1 객체 표현, 즉 상기 단계 A12의 출력, 차원 L, C에 해당), K(영상 특징을 키 특징 으로 사용, 차원 THW, C에 해당) 및 V(영상 특징을 값 특징으로 사용, 차원 THW, C에 해당)에 대해 각각 선형 작업을 수행한다. 도 11에 도시된 선형 연산 모듈(801, 802, 803)은 하나 이상의 선형 연산 레이어(Linear Layer)를 포함할 수 있다. 이어서, 선형 연산 모듈의 출력과 선형 연산 모듈의 출력에 대해 행렬 곱 셈 연산을 수행하여(모듈를 통해 실행), 모듈의 출력 결과(제4 객체 표현)를 softmax 연산하여 카테 고리에 해당하는 제2 확률(모듈를 통해 실행, 해당 제2 확률은 영상에 포함된 객체 카테고리를 나타낼 수 있음)로 변한한다. 이를 바탕으로, 모듈을 통해 선형 변환 처리된 후의 값 특징과 모듈의 출력(제2 확률)에 대해 행렬 곱셈 연산을 수행하여 최종 프레임 쿼리 출력(적어도 하나의 영상 프레임 중 각 영상 프레임 에 대응하는 객체 표현)을 얻을 수 있다. 선택적으로, 모듈의 입력에 대해 변형(reshape) 작업을 수행할 수 있으며 텐서의 차원 변경은 도 11을 참 조할 수 있다. 도 11에 도시된 위치 정보 (P_q) 및 (P_k)는 Q 및 K에 대응하는 위치 정보로서 범용 위치 정보 부호화 모듈에 의해 생성될 수 있고, 위치 정보는 프레임 쿼리 생성 모듈에 대해 선택적이다.일 실시예에서, 단계 A132에서, 상기 제1 객체 표현과 결정된 영상 프레임에 대응하는 객체 표현을 기반으로 현 재 반복 처리에 의한 객체 표현을 결정하는 단계는, 다음 단계 B1-B2를 포함한다. 단계 B1: 결정된 영상 프레임에 대응하는 객체 표현에 대해 분류 처리 및 셀프 어텐션 처리를 수행하여 상기 영 상에 대응하는 제3 객체 표현을 획득한다. 단계 B2: 상기 제1 객체 표현과 상기 제3 객체 표현을 기반으로 현재 반복 처리에 의한 객체 표현을 결정한다. 선택적으로, 클립 프레임 쿼리 상호작용 모듈의 동작에서, 그 입력 및 출력은 모두 텐서일 수 있고, 텐서 의 차원은 도 9에 도시되어 있다. 일 실행 가능한 실시예에서, 도 9에 도시된 바와 같이, 모듈 출력의 결과(결정된 영상 프레임에 대응하는 객체 표현)에 대해 FFN 모듈을 통해 피드포워드 연산(분류 처리)을 수행하고, 모듈 출력의 결과에 대 해 셀프 어텐션 모듈을 통해 셀프 어텐션 처리한다. 그런 다음, 셀프 어텐션 작업 후의 결과를 요소별로 모듈 출력의 결과 상에 중첩하고, 가산된 합 결과에 대해 정규화 처리(도 9에 도시된 합산 및 정규화 모듈 을 통해 구현됨)하여 정규화 처리로 얻은 결과 C(제3 객체 표현)를 얻는다. 이를 바탕으로, 상호 어텐션 모듈을 통해 정규화 처리로 얻은 결과 C와 입력된 클립 쿼리 S에 대해 상호 어텐션 계산을 수행하고, 이때 모듈 출력의 차원은 L, C가 되고, 이는 입력된 클립 쿼리S와 동일하다. 마지막으로, 모듈 출력의 결 과를 클립 쿼리S에 요소별로 가산하고, 가산된 합 결과에 대해 정규화 처리하여 정규화 처리로 얻은 결과 D를 얻는다. d는 즉 클립 프레임 쿼리 상호작용 모듈의 클립 쿼리 출력이다(현재 반복 처리에 의한 타겟 객체 표현). 도 9에 도시된 바와 같이, 셀프 어텐션 모듈에서, 입력된 Q, K 및 V는 모듈의 출력 결과의 차원 TL, C에 대응한다. 선택적으로, 도 9에 도시된 네트워크 구조에서, FFN 모듈 및 어텐션 메커니즘을 포함하는 다른 모듈의 처 리 순서는 교환될 수 있다. 도 9에 도시된 네트워크 구조에서 셀프 어텐션 모듈 및 FFN 모듈의 위치 가 교환될 수 있는 경우, 교환 후 프레임 쿼리 생성 모듈의 출력은 셀프 어텐션 모듈의 입력으로 사 용될 수 있고, 셀프 어텐션 모듈의 출력은 FFN 모듈의 입력으로 사용될 수 있다. 선택적으로, 도 9에 도시된 상호 어텐션 모듈의 특정 네트워크 구조는 도 12에 도시된 구조를 참조할 수 있다. 먼저, 입력 Q(클립 쿼리, 제1 객체 표현, 즉 클립 특징 쿼리 상호작용 모듈의 출력, 차원 L, C에 해당), K(모듈의 출력의 결과를 키 특징으로 사용, 차원 TL, C에 해당) 및 V(모듈의 출력의 결과를 값 특징 으로 사용, 차원 TL, C에 해당)에 대해 각각 선형 작업을 수행한다. 도 12에 도시된 선형 연산 모듈(1201, 1202, 1203)은 하나 이상의 선형 연산 레이어(Linear Layer)를 포함할 수 있다. 이어서, 선형 연산 모듈(120 1)의 출력과 선형 연산 모듈의 출력의 결과에 대해 행렬 곱셈 연산을 수행하여(모듈를 통해 실행), 모듈의 출력 결과를 softmax 연산하여 카테고리에 해당하는 제3 확률(모듈를 통해 실행)로 변환한 다. 이를 바탕으로, 모듈을 통해 선형 변환 처리된 후의 영상 특징의 값 특징과 모듈의 출력(제3 확률)에 대해 행렬 곱셈 연산을 수행하고, 모듈의 출력을 선형 변환 처리된 후의 입력Q에 중첩하여 모듈 의 상호 어텐션 출력을 얻는다. 도 12에 도시된 위치 정보 (P_q) 및 (P_k)는 Q 및 K에 대응하는 위치 정보로서 범용 위치 정보 부호화 모듈에 의해 생성될 수 있고, 위치 정보는 상호 어텐션 모듈에 대해 선택적이다. 이하, 본 출원의 실시예의 영상 파노라마 분할 프로세스에 대해 설명한다. 구체적으로, 단계 S103에서, 상기 타겟 객체 표현을 기반으로 상기 영상의 파노라마 분할 결과를 결정하는 단계 는, 다음 단계 S103a-S103b를 포함한다. 단계 S103a: 상기 타겟 객체 표현에 대해 선형 변환 처리를 수행한다. 단계 S103b: 선형 변환 처리된 후의 타겟 객체 표현과 상기 영상 특징을 기반으로 상기 영상의 마스크 정보를 결정하고, 선형 변환 처리된 후의 타겟 객체 표현을 기반으로 상기 영상의 카테고리 정보를 결정한다. 일 실행 가능한 실시예에서, 도 13에 도시된 바와 같이, 분할 헤드 모듈의 입력은 클립 쿼리 S(마스크 디코더의 출력) 및 클립 특징 X를 포함하며, 그 출력은 예측된 마스크 정보(모듈의 출력) 및 카테고리 정보(모듈 의 출력)이다. 분할 헤드 모듈은 구체적으로 두 분기로 나눌 수 있으며, 하나는 모듈(901, 902 및 903)로구성된 마스크 분기이고 하나는 모듈(904 및 905)로 구성된 카테고리 분기이다. 마스크 분기에서, 먼저 클립 쿼리 S에 대해 선형 변환을 수행하여 모듈의 출력을 얻은 다음, 모듈의 출력에 대해 선형 변환을 수행하여 모듈의 출력을 얻는다(1개, 2개 또는 3개와 같이 여러 개의 선형 연산 모듈을 마스크 분기에 설정할 수 있으며, 본 출원은 마스크 분기에 포함된 선형 연산 모듈의 수를 제한하지 않 음). 이를 기반으로 모듈의 출력과 클립 특징 X에 대해 행렬 곱셈 연산을 수행하여 마스크 출력을 얻는다. 도 13에 도시된 마스크 출력(마스크 정보)은 클립 레벨에 속하며, 서로 다른 영상 프레임 간의 매칭이나 추적 작업 없이 객체 ID를 자동으로 얻을 수 있다. 즉, 본 출원의 실시예에서 마스크 분기에 의해 출력되는 마스크 정보는 마스크 및 객체 ID를 포함할 수 있다. 카테고리 분기에서, 먼저 클립 쿼리 S에 대해 선형 변환을 수행하여 모듈의 출력을 얻고, 그런 다음 모듈 의 출력에 대해 선형 변환을 수행하여 모듈의 출력을 얻는다(1개, 2개 또는 3개와 같이 여러 개의 선 형 연산 모듈을 카테고리 분기에 설정할 수 있으며, 본 출원은 카테고리 분기에 포함된 선형 연산 모듈의 수를 제한하지 않음). 이는 즉 도 13에 도시된 카테고리 출력(카테고리 정보)이다. 선택적으로, 영상 클립의 프레임 수가 1보다 크면 마스크 분기에서 출력한 마스크 정보는 클립 레벨에 속하므로 영상의 모든 프레임의 객체 ID를 자동으로 얻을 수 있다. 따라서 카테고리 분기에서 출력되는 카테고리 정보는 객체의 의미 정보이다(예, 자동차, 사람 등). 이하, 본 출원의 실시예의 파노라마 분할 모델의 훈련 과정에 대해 설명한다. 구체적으로, 네트워크를 훈련할 때 예측 결과(마스크 및 카테고리)를 ground-truth(GT, 레이블)의 마스크 및 카 테고리와 유사하게 제한해야 하며 이는 손실 함수를 최소화하여 달성할 수 있다. 도 14에 도시된 바와 같이, 이 진 일치 알고리즘을 사용하여 예측 마스크와 GT 마스크 간의 대응 관계를 설정할 수 있다. 여기에서 사용되는 손실 함수는 픽셀 레벨의 교차 엔트로피 손실, 파노라마 객체 레벨의 파노라마 품질 손실, 마스크 ID 손실 및 마스크 유사성 손실(Dice 기반 계산) 등과 같은 여러 항목을 포함한다. 일 실행 가능한 실시예에서, 파노라마 분할 모델의 훈련 단계는 다음 단계 B1-B2를 포함한다. 단계 B1: 훈련 데이터를 얻는다. 상기 훈련 데이터는 훈련 영상, 훈련 영상의 제1 영상 특징 및 훈련 영상에 대 응하는 샘플 파노라마 분할 결과를 포함한다. 단계 B2: 상기 훈련 데이터를 기반으로 상기 파노라마 분할 모델을 훈련하여 훈련된 파노라마 분할 모델을 얻는 다. 훈련 시 다음 단계 B21-B24를 실행한다. 단계 B21: 상기 제1 영상 특징의 프레임 순서를 바꿔 제2 영상 특징을 얻는다. 단계 B22: 상기 계층적 상호작용 모듈(즉 제1 모듈)을 통해 각각 상기 제1 영상 특징 및 상기 제2 영상 특징에 기초하여 상기 훈련 영상의 제1 예측 객체 표현 및 제2 예측 객체 표현을 결정한다. 단계 B23: 상기 분할 헤드 모듈(즉 제2 모듈)을 통해 각각 상기 제1 예측 객체 표현 및 상기 제2 예측 객체 표 현에 기초하여 상기 훈련 영상의 제1 예측 결과 및 제2 예측 결과를 결정한다. 단계 B24: 상기 샘플 파노라마 분할 결과, 상기 제1 예측 객체 표현, 상기 제2 예측 객체 표현, 상기 제1 예측 결과 및 상기 제2 예측 결과를 기반으로 타겟 손실 함수를 사용하여 상기 파노라마 분할 모델을 훈련한다. 선택적으로, 본 출원에서, 클립 특징맵 X(도 14에 도시된 제1 영상 특징)의 프레임 순서를 교체하여 교체된 프 레임 특징맵 X'(도 14에 도시된 제2 영상 특징)를 얻을 수 있다. X 및 X'는 각각 계층적 상호작용 모듈을 통해 두 세트의 클립 쿼리 출력 out_S(제1 예측 객체 표현) 및 out_S'(제2 예측 객체 표현)를 얻는다. 네트워크 훈련 에서, out_S와 out_S'의 동일한 클립 파노라마 객체에 해당하는 클립 쿼리 벡터는 유사해야 하고, 다른 클립 파 노라마 객체에 해당하는 클립 쿼리 벡터는 유사하지 않아야 한다. out_S(제1 예측 객체 표현) 및 out_S'(제2 예 측 객체 표현)는 분할 헤드 모듈에 의해 각각 처리되어 제1 예측 결과 및 제2 예측 결과를 얻을 수 있다. 선택적으로, 단계 B24에서, 상기 샘플 파노라마 분할 결과, 상기 제1 예측 객체 표현, 상기 제2 예측 객체 표현, 상기 제1 예측 결과 및 상기 제2 예측 결과를 기반으로 타겟 손실 함수를 사용하여 상기 파노라마 분할 모델을 훈련하는 단계는, 다음 단계 B242-B243을 포함한다. 단계 B241: 상기 제1 예측 객체 표현 및 상기 제2 예측 객체 표현에 기초하여 객체 표현 간의 제1 유사 행렬을 결정한다.단계 B242: 상기 샘플 파노라마 분할 결과, 상기 제1 예측 결과 및 상기 제2 예측 결과를 기반으로 분할 결과 간의 제2 유사 행렬을 결정한다. 단계 B243: 상기 제1 유사 행렬과 상기 제2 유사 행렬을 기반으로 타겟 손실 함수가 최소로 결정되면, 훈련된 파노라마 분할 결과를 출력한다. 구체적으로, 타겟 손실 함수(클립 대비 손실 함수)는 아래 [수학식 2]와 같이 표현될 수 있다. 수학식 2"}
{"patent_id": "10-2023-0111343", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "X는 out_S와 out_S' 사이에서 계산된 유사성 행렬(벡터 간의 유사성은 일반 코사인 유사도(cosine similarity) 와 같은 기존 방법을 사용하여 계산할 수 있음)이고, Y는 ground-truth의 유사성 행렬로, GT 행렬(도 14에 도시 된 바와 같이, 두 개의 클립 쿼리 벡터가 동일한 ground-truth의 클립 파노라마 객체에 대응하는 경우 Y의 해당 위치값은 1이고 그렇지 않은 경우 0임)로 칭할 수 있다. W는 가중치 행렬이며, 동일한 카테고리의 위치에 대해 값이 1이고 그렇지 않으면 0이다(본 출원에서는 다른 카테고리의 객체가 명백하다는 점을 고려하여 감독할 필요 가 없음). 또한 [수학식 2]에서 곱셈 기호 \"*\"는 요소별 곱셈을 나타내는 반면 Ave()는 모든 클립의 파노라마 객체의 평균이다. 본 출원의 실시예에서, 네트워크 구조는 딥 러닝 프레임워크인 Python을 사용하여 구현할 수 있다. 기존 기술과 비교하여 해당 네트워크는 네트워크 구조를 단순화하면서 계산 복잡성을 효과적으로 줄일 수 있는 동시에 영상 정보를 최대한 활용하면서 분할 정확도를 효과적으로 향상시킬 수 있다. 여기서 정확도는 영상 파노라마 품질 (VPQ, video panoptic quality)을 사용하여 측정할 수 있다. 본 출원에서 달성할 수 있는 기술적 효과를 보다 더 잘 설명하기 위해, 이하 도 15 내지 도 17에 도시된 분할 결과에 대해 설명한다. 도 15에 도시된 바와 같이, 일반적인 기술을 통해 얻은 파노라마 분할 결과나 네트워크 훈련에 사용할 수 있는 샘플 파노라마 분할 결과 모두 두 원이 도시하는 자동차 사이에 존재하는 자동차를 인식하지 못한다(일부 인스 턴스 누락). 본 개시에 따른 영상 처리 방법을 이용하여 얻은 파노라마 분할 결과에서는 차량(후방 차량) 사이 에 위치한 차량(전방 차량)을 식별할 수 있다. 도 16에 도시된 바와 같이, 일반적인 기술의 파노라마 분할 결과에서 한편으로는 자전거의 마스크가 불완전하여 자전거에 속한 ID를 부여할 수 없고; 또한 제3 영상 프레임에서의 더 작은 보행자를 식별할 수 없다. 또한, 제1 프레임 영상 프레임에서의 자동차(후방 차량) 사이에 존재하는 자동차(전방 차량)를 식별할 수 없다. 네트워크 훈련에 사용되는 샘플 파노라마 분할 결과에서, 제1 프레임 영상 프레임에서 자동차 사이에 존재하는 자동차(원 으로 표시됨)를 인식할 수 없다. 그러나 본 개시에 따른 실시예에서 제공하는 영상 처리 방법을 이용하여 얻은 파노라마 분할 결과에서는 기존 기술과 샘플 파노라마 분할 결과 중 존재하는 단점을 효과적으로 보완할 수 있 다. 도 17에 도시된 바와 같이, 본 개시는 보다 명확한 자전거 윤곽 표현을 얻을 수 있으며, 이는 본 출원이 더 나 은 분할 정확도를 달성할 수 있음을 의미한다. 네트워크 훈련에 사용되는 샘플 파노라마 분할 결과와 비교할 때, 본 출원에서 얻은 자전거에 속하는 분할 결과는 각 영상 프레임에서 일치하는 반면, 샘플 파노라마 분할 결 과의 각 영상 프레임에서 동일한 자전거에 대한 분할 결과는 서로 다르며, 이는 본 출원이 더 나은 견고성을 달 성할 수 있음을 의미한다. 선택 가능한 실시예에서, 전자 장치를 제공한다. 도 18에 도시된 바와 같이, 도 18에 도시된 전자 장치는 프로세서 및 메모리를 포함한다. 프로세서는, 예를 들어 버스를 통해 메모리에 연결된다. 선택적으로, 전자 장치는 트랜시버를 더 포함할 수 있으며, 트랜시버는 데이터 송 신 및/또는 데이터 수신과 같은 전자 장치와 다른 전자 장치 간의 데이터 상호작용을 위해 사용될 수 있다. 실 제 응용에서 트랜시버는 하나로 제한되지 않으며, 해당 전자 장치의 구조는 본 출원 실시예에 대한제한을 구성하지 않는다는 점에 유의해야 한다. 프로세서는 CPU, 범용 프로세서, DSP, 주문형 집적 회로(ASIC), 필드 프로그램 가능 게이트 어레이(FPGA) 또는 기타 프로그램 가능 논리 장치, 트랜지스터 논리 장치, 하드웨어 구성 요소, 또는 이들의 임의의 조합일 수 있다. 이는 본 출원에서 설명된 다양한 예시적 논리 블록, 모듈 및 회로를 구현하거나 실행할 수 있다. 프로 세서는 또한, 예를 들어, 하나 이상의 마이크로프로세서 조합, DSP와 마이크로프로세서의 조합 등을 포함 하는 컴퓨팅 기능을 실현하는 조합일 수 있다. 버스는 상기 구성요소들 사이에서 정보를 전달하기 위한 경로를 포함할 수 있다. 버스는 PCI(Peripheral Component Interconnect) 버스 또는 EISA(Extended Industry Standard Architecture) 버스일 수 있다. 버스는 어드레스 버스, 데이터 버스, 제어 버스 등으로 구분될 수 있다. 예시의 편의를 위해, 도 18에는 굵은 선 하나만 도시하였으나, 버스가 하나 또는 한 종류만 있는 것은 아니다. 메모리는 읽기 전용 메모리(ROM) 또는 정적 정보 및 명령을 저장할 수 있는 다른 유형의 정적 저장 장치, 랜덤 액세스 메모리(RAM) 또는 정보 및 명령을 저장할 수 있는 다른 유형의 동적 저장 장치일 수 있고, EEPROM, CD-ROM 또는 기타 광 디스크 스토리지, 광 디스크 스토리지(압축 광 디스크, 레이저 디스크, 광 디스크, 디지털 다목적 디스크, 블루 레이 디스크 등 포함), 디스크 저장 매체, 기타 자기 저장 장치 또는 컴퓨터 프로그램을 운반하거나 저장하는데 사용할 수 있고 컴퓨터에서 읽을 수 있는 기타 모든 매체일 수도 있으며, 여기서 이에 대해 제한하지는 않는다. 메모리는 본 출원의 실시예를 실행하기 위한 컴퓨터 프로그램을 저장하는데 사용되며 프로세서에 의해 제어된다. 프로세서는 메모리에 저장된 컴퓨터 프로그램을 실행하여 전술한 방법 실시예에 도 시된 단계들을 실현하도록 구성된다. 일 실시예에서 제공하는 방법에서, AI 모델을 통해 복수의 모듈 중 적어도 하나의 모듈을 구현할 수 있다. AI 와 관련된 기능은 비휘발성 메모리, 휘발성 메모리 및 프로세서에 의해 수행될 수 있다. 해당 프로세서는 하나 이상의 프로세서를 포함할 수 있다. 이때, 해당 하나 이상의 프로세서는 범용 프로세서 (예, 중앙 처리 장치(CPU), 응용 프로세서(AP) 등) 또는 순수 그래픽 처리 장치(예, 그래픽 처리 장치(GPU), 시 각 처리 장치(VPU)), 및/또는 AI 전용 프로세서(예, 신경 처리 장치(NPU))일 수 있다. 하나 이상의 프로세서는 비휘발성 메모리 및 휘발성 메모리에 저장된 사전 정의된 동작 규칙 또는 인공 지능 (AI) 모델에 따라 입력 데이터의 처리를 제어한다. 훈련 또는 학습을 통해 사전 정의된 동작 규칙 또는 인공 지 능 모델을 제공한다. 여기서, 학습에 의한 제공은 복수의 학습 데이터에 학습 알고리즘을 적용하여 사전 정의된 동작 규칙 또는 원하 는 특성을 갖는 AI 모델을 얻는 것을 의미한다. 이러한 학습은 실시예에 따른 AI가 수행되는 장치 자체에서 수 행될 수 있고, 및/또는 별도의 서버/시스템에 의해 구현될 수 있다. AI 모델은 복수의 신경망 레이어로 구성될 수 있다. 각 레이어는 복수의 가중치 값을 가지며, 하나의 레이어의 계산은 이전 레이어의 계산 결과와 현재 레이어의 복수의 가중치에 의해 수행된다. 신경망의 예시로, 컨볼루션 신경망(CNN), 심층 신경망(DNN), 순환 신경망(RNN), 제한된 볼츠만 머신(RBM), 심층 신뢰망(DBN), 양방향 순환 심층 신경망(BRDNN), 생성 대응 네트워크(GAN) 및 심층 Q 네트워크를 포함하나 이에 제한되지 않는다. 학습 알고리즘은 복수의 학습 데이터를 이용하여 소정의 타겟 장치(예, 로봇)를 훈련시켜 타겟 장치를 결정 또 는 예측하도록 유도, 허용 또는 제어하는 방법이다. 학습 알고리즘의 예시는 지도 학습(supervised learning), 비지도 학습, 반 지도 학습 또는 강화 학습을 포함하나 이에 국한되지는 않는다. 본 출원 실시예는 컴퓨터 프로그램이 저장된 컴퓨터 판독 가능 저장 매체를 제공한다. 컴퓨터 프로그램이 프로 세서에 의해 실행될 때, 전술한 방법 실시예의 단계 및 상응하는 내용이 구현될 수 있다. 본 출원 실시예는 또한 프로세서에 의해 실행될 때 전술한 방법 실시예의 단계 및 상응하는 내용을 구현할 수 있는 컴퓨터 프로그램을 포함하는 컴퓨터 프로그램 제품을 제공한다. 첨부된 도면 뿐만 아니라 본 출원의 명세서 및 특허청구범위에서 사용되는 용어 \"제1\", \"제2\", \"제3\", \"제4\", \"1\", \"2\" 등(있는 경우)은 특정 순서 또는 앞뒤 순서를 설명하지 않고 유사한 대상을 구별하기 위해 사용된다. 이러한 방식으로 사용된 데이터는 적절한 경우에 교환될 수 있으므로, 여기에 설명된 본 출원의 실시예는 도면 예시 또는 텍스트 설명 이외의 순서로 구현될 수 있음을 이해해야 한다.본 출원의 실시예에서 설명된 모듈은 소프트웨어 또는 신경망을 통해 구현될 수 있다. 경우에 따라 모듈 명칭이 해당 모듈 자체의 제한을 구성하지 않으며, 예를 들어, 셀프 어텐션 모듈은 \"셀프 어텐션 처리를 위한 모듈\", \"제1 모듈\", 셀프 어텐션 네트워크, 셀프 어텐션 신경망 등으로도 설명될 수 있다. 이상에서 설명된 실시예들은 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨 어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치, 방법 및 구성요소는, 예를 들 어, 프로세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마 이크로컴퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령(instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 소프트웨어 애플리 케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처 리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설명된 경우도 있지만,"}
{"patent_id": "10-2023-0111343", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소(processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치는 복수 개의 프로세서 또는 하 나의 프로세서 및 하나의 컨트롤러를 포함할 수 있다. 또한, 병렬 프로세서(parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(signal wave)에 영구적으로, 또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단 독으로 또는 조합하여 저장할 수 있으며 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구 성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 위에서 설명한 하드웨어 장치는 실시예의 동작을 수행하기 위해 하나 또는 복수의 소프트웨어 모듈로서 작동하 도록 구성될 수 있으며, 그 역도 마찬가지이다."}
{"patent_id": "10-2023-0111343", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "이상과 같이 실시예들이 비록 한정된 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가진 자라면 이를 기초로 다양한 기술적 수정 및 변형을 적용할 수 있다. 예를 들어, 설명된 기술들이 설명된 방법과 다른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다.도면 도면1 도면2 도면3 도면4 도면5 도면6a 도면6b 도면7a 도면7b 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17 도면18"}
{"patent_id": "10-2023-0111343", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 출원의 실시예의 기술방안에 대한 보다 명확한 설명을 제공하기 위해, 이하 본 출원의 실시예의 설명 시 필 요한 첨부 도면에 대해 간략히 소개한다. 도 1은 일 실시 예에 따른 영상 처리 방법의 흐름도이다. 도 2는 일 실시 예에 따른 파노라마 분할 예시의 개략도이다. 도 3은 일 실시 예에 따른 클립 쿼리(clip query) 시각화의 결정 과정의 예시도이다. 도 4는 일 실시 예에 따른 파노라마 분할 모델의 네트워크 아키텍처 도면이다. 도 5는 일 실시 예에 따른 파노라마 분할 알고리즘의 흐름도이다. 도 6a는 일 실시 예에 따른 마스크 디코더의 프레임워크의 개략도이다. 도 6b는 일 실시 예에 따른 다른 마스크 디코더의 프레임워크의 개략도이다. 도 7a는 일 실시 예에 따른 계층적 상호작용(Hierarchical interaction) 모듈의 프레임워크의 개략도이다. 도 7b는 일 실시 예에 따른 다른 계층적 상호작용 모듈의 프레임워크의 개략도이다. 도 8은 일 실시 예에 따른, 도 7a 및 도 7b에 도시된 클립 특징 쿼리 상호작용의 프레임워크의 개략도이다. 도 9는 일 실시 예에 따른, 도 7b에 도시된 클립 특징 쿼리 상호작용의 프레임워크의 개략도이다. 도 10은 일 실시 예에 따른, 도 8에 도시된 마스크 어텐션 모듈의 구조의 개략도이다. 도 11은 일 실시 예에 따른, 도 9에 도시된 프레임 쿼리 생성 모듈의 구조의 개략도이다. 도 12는 일 실시 예에 따른, 도 9에 도시된 상호 어텐션 모듈의 구조의 개략도이다. 도 13은 일 실시 예에 따른 분할 헤드 모듈의 구조의 개략도이다. 도 14는 일 실시 예에 따른 파노라마 분할 모델의 훈련 과정의 개략도이다. 도 15는 일 실시 예에 따른 효과 대비도이다. 도 16은 일 실시 예에 따른 다른 효과 대비도이다. 도 17은 일 실시 예에 따른 또 다른 효과 대비도이다. 도 18은 일 실시 예에 따른 전자 장치의 구조의 개략도이다."}
