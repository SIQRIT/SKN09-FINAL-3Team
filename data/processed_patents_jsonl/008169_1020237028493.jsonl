{"patent_id": "10-2023-7028493", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0152670", "출원번호": "10-2023-7028493", "발명의 명칭": "시스템 온 칩", "출원인": "퀄컴 인코포레이티드", "발명자": "박, 희 준"}}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공 뉴럴 네트워크(artificial neural network)를 위한 방법으로서,복수의 컴퓨팅 유닛들을 통해 복수의 커널(kernel)들과 컨볼빙될(convolved) 한 세트의 입력 값들을 수신하는단계;상기 복수의 컴퓨팅 유닛들 중 적어도 하나의 열적 스트레스를 받는(thermally-stressed) 컴퓨팅 유닛을 결정하는 단계;상기 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 SOC(system-on-chip)의 상기 복수의 컴퓨팅유닛들에 상기 복수의 커널들을 맵핑하는 단계; 및가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 상기 복수의 커널들 중 가장 희소한 커널(most sparsekernel)과 상기 한 세트의 입력 값들의 컨볼루션을 수행하는 단계를 포함하는, 인공 뉴럴 네트워크를 위한 방법."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,열적 스트레스의 양에 기초하여 상기 컴퓨팅 유닛들의 제1 순서를 그리고 희소성 메트릭(sparsity metric)에 기초하여 상기 커널들의 제2 순서를 결정하는 단계; 및상기 제1 순서 및 상기 제2 순서에 기초하여 상기 복수의 커널들 각각을 상기 복수의 컴퓨팅 유닛들 중 하나에할당하는 단계를 더 포함하는, 인공 뉴럴 네트워크를 위한 방법."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 제1 순서는 계층 내 상기 복수의 커널들이 희소성 감소에 따라 배열되는 것을 포함하고, 상기 제2 순서는상기 복수의 컴퓨팅 유닛들이 열적 스트레스 증가에 따라 배열되는 것을 포함하고; 그리고상기 방법은, 최소 희소한 커널이 가장 적게 열적 스트레스를 받는 컴퓨팅 유닛에 할당되고 그리고 가장 희소한커널이 상기 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛에 할당되도록 하는 순서로, 상기 복수의 컴퓨팅 유닛들 중 하나에 상기 커널들 각각을 할당하는 단계를 더 포함하는, 인공 뉴럴 네트워크를 위한 방법."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 SOC의 상기 복수의 컴퓨팅 유닛들 각각의 온도 또는 전류 소모 중 적어도 하나를 검출하는 단계를 더 포함하고, 상기 맵핑하는 단계는 상기 온도 또는 상기 전류 소모에 기초하는, 인공 뉴럴 네트워크를 위한 방법."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서,상기 맵핑하는 단계는 상기 복수의 컴퓨팅 유닛들 중 임계치를 초과하는 온도 또는 전류를 갖는 컴퓨팅 유닛들에 대해 수행되는, 인공 뉴럴 네트워크를 위한 방법."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2023-0152670-3-제1 항에 있어서, 상기 복수의 커널들 각각의 커널의 통계 정보(statistical information)를 컴퓨팅하는 단계를 더 포함하고, 상기 맵핑하는 단계는 상기 통계 정보에 추가로 기초하는, 인공 뉴럴 네트워크를 위한 방법."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 맵핑하는 단계는 런타임 동안 상기 SOC의 상기 복수의 컴퓨팅 유닛들에 상기 복수의 커널들을 동적으로 할당하는 단계를 포함하는, 인공 뉴럴 네트워크를 위한 방법."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "인공 뉴럴 네트워크를 위한 장치로서,메모리; 및상기 메모리에 커플링된 적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는,복수의 컴퓨팅 유닛들을 통해 복수의 커널들과 컨볼빙될 한 세트의 입력 값들을 수신하고; 상기 복수의 컴퓨팅 유닛들 중 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛을 결정하고;상기 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 SOC(system-on-chip)의 상기 복수의 컴퓨팅유닛들에 상기 복수의 커널들을 맵핑하고; 그리고가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 상기 복수의 커널들 중 가장 희소한 커널과 상기 한 세트의 입력 값들의 컨볼루션을 수행하도록구성되는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 적어도 하나의 프로세서는,열적 스트레스의 양에 기초하여 상기 컴퓨팅 유닛들의 제1 순서를 그리고 희소성 메트릭에 기초하여 상기 커널들의 제2 순서를 결정하고; 그리고상기 제1 순서 및 상기 제2 순서에 기초하여 상기 복수의 커널들 각각을 상기 복수의 컴퓨팅 유닛들 중 하나에할당하도록추가로 구성되는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9 항에 있어서,상기 제1 순서는 계층 내 상기 복수의 커널들이 희소성 감소에 따라 배열되는 것을 포함하고, 상기 제2 순서는상기 복수의 컴퓨팅 유닛들이 열적 스트레스 증가에 따라 배열되는 것을 포함하고; 그리고상기 적어도 하나의 프로세서는, 최소 희소한 커널이 가장 적게 열적 스트레스를 받는 컴퓨팅 유닛에 할당되고그리고 가장 희소한 커널이 상기 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛에 할당되도록 하는 순서로, 상기복수의 컴퓨팅 유닛들 중 하나에 상기 커널들 각각을 할당하도록 추가로 구성되는, 인공 뉴럴 네트워크를 위한장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8 항에 있어서,상기 적어도 하나의 프로세서는, 공개특허 10-2023-0152670-4-상기 SOC의 상기 복수의 컴퓨팅 유닛들 각각의 온도 또는 전류 소모 중 적어도 하나를 검출하고; 그리고상기 온도 또는 상기 전류 소모에 기초하여 상기 복수의 커널들을 상기 복수의 컴퓨팅 유닛들에 맵핑하도록추가로 구성되는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 적어도 하나의 프로세서는, 상기 복수의 컴퓨팅 유닛들 중 임계치를 초과하는 온도 또는 전류를 갖는 컴퓨팅 유닛들에 상기 복수의 커널들의 커널들을 할당하도록 추가로 구성되는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8 항에 있어서,상기 적어도 하나의 프로세서는,상기 복수의 커널들 각각의 커널의 통계 정보를 컴퓨팅하고; 그리고상기 통계 정보에 기초하여 상기 복수의 커널들을 상기 복수의 컴퓨팅 유닛들에 맵핑하도록추가로 구성되는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8 항에 있어서, 상기 적어도 하나의 프로세서는, 런타임 동안 상기 SOC의 상기 복수의 컴퓨팅 유닛들에 상기 복수의 커널들을동적으로 할당하도록 추가로 구성되는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "인공 뉴럴 네트워크를 위한 장치로서,복수의 컴퓨팅 유닛들을 통해 복수의 커널들과 컨볼빙될 한 세트의 입력 값들을 수신하기 위한 수단;상기 복수의 컴퓨팅 유닛들 중 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛을 결정하기 위한 수단;상기 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 SOC(system-on-chip)의 상기 복수의 컴퓨팅유닛들에 상기 복수의 커널들을 맵핑하기 위한 수단; 및가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 상기 복수의 커널들 중 가장 희소한 커널과 상기 한 세트의 입력 값들의 컨볼루션을 수행하기 위한 수단을 포함하는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15 항에 있어서,열적 스트레스의 양에 기초하여 상기 컴퓨팅 유닛들의 제1 순서를 그리고 희소성 메트릭에 기초하여 상기 커널들의 제2 순서를 결정하기 위한 수단; 및상기 제1 순서 및 상기 제2 순서에 기초하여 상기 복수의 커널들 각각을 상기 복수의 컴퓨팅 유닛들 중 하나에할당하기 위한 수단을 더 포함하는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16 항에 있어서,상기 제1 순서는 계층 내 상기 복수의 커널들이 희소성 감소에 따라 배열되는 것을 포함하고, 상기 제2 순서는공개특허 10-2023-0152670-5-상기 복수의 컴퓨팅 유닛들이 열적 스트레스 증가에 따라 배열되는 것을 포함하고; 그리고상기 장치는, 최소 희소한 커널이 가장 적게 열적 스트레스를 받는 컴퓨팅 유닛에 할당되고 그리고 가장 희소한커널이 상기 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛에 할당되도록 하는 순서로, 상기 복수의 컴퓨팅 유닛들 중 하나에 상기 커널들 각각을 할당하기 위한 수단을 더 포함하는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제15 항에 있어서,상기 SOC의 상기 복수의 컴퓨팅 유닛들 각각의 온도 또는 전류 소모 중 적어도 하나를 검출하기 위한 수단; 및상기 온도 또는 상기 전류 소모에 기초하여 상기 복수의 커널들을 상기 복수의 컴퓨팅 유닛들에 할당하기 위한수단을 더 포함하는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제15 항에 있어서,상기 복수의 컴퓨팅 유닛들 중 임계치를 초과하는 온도 또는 전류를 갖는 컴퓨팅 유닛들에 상기 복수의 커널들의 커널들을 할당하기 위한 수단을 더 포함하는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제15 항에 있어서,상기 복수의 커널들 각각의 커널의 통계 정보를 컴퓨팅하기 위한 수단, 및 상기 통계 정보에 기초하여 상기 복수의 컴퓨팅 유닛들의 컴퓨팅 유닛들에 상기 복수의 커널들의 커널들을 할당하기 위한 수단을 더 포함하는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제15 항에 있어서,런타임 동안 상기 SOC의 상기 복수의 컴퓨팅 유닛들에 상기 복수의 커널들을 동적으로 할당하기 위한 수단을 더포함하는, 인공 뉴럴 네트워크를 위한 장치."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "인공 뉴럴 네트워크를 위한 프로그램 코드가 인코딩되어 있는 비일시적 컴퓨터 판독 가능 저장 매체로서, 상기 프로그램 코드는, 프로세서에 의해 실행되고, 그리고복수의 컴퓨팅 유닛들을 통해 복수의 커널들과 컨볼빙될 한 세트의 입력 값들을 수신하기 위한 프로그램 코드;상기 복수의 컴퓨팅 유닛들 중 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛을 결정하기 위한 프로그램 코드;상기 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 SOC(system-on-chip)의 상기 복수의 컴퓨팅유닛들에 상기 복수의 커널들을 맵핑하기 위한 프로그램 코드; 및가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 상기 복수의 커널들 중 가장 희소한 커널과 상기 한 세트의 입력 값들의 컨볼루션을 수행하기 위한 프로그램 코드를 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제22 항에 있어서,열적 스트레스의 양에 기초하여 상기 컴퓨팅 유닛들의 제1 순서를 그리고 희소성 메트릭에 기초하여 상기 커널들의 제2 순서를 결정하기 위한 프로그램 코드; 및공개특허 10-2023-0152670-6-상기 제1 순서 및 상기 제2 순서에 기초하여 상기 복수의 커널들 각각을 상기 복수의 컴퓨팅 유닛들 중 하나에할당하기 위한 프로그램 코드를 더 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제23 항에 있어서,상기 제1 순서는 계층 내 상기 복수의 커널들이 희소성 감소에 따라 배열되는 것을 포함하고, 상기 제2 순서는상기 복수의 컴퓨팅 유닛들이 열적 스트레스 증가에 따라 배열되는 것을 포함하고; 그리고상기 비일시적 컴퓨터 판독 가능 저장 매체는, 최소 희소한 커널이 가장 적게 열적 스트레스를 받는 컴퓨팅 유닛에 할당되고 그리고 가장 희소한 커널이 상기 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛에 할당되도록 하는 순서로, 상기 복수의 컴퓨팅 유닛들 중 하나에 상기 커널들 각각을 할당하기 위한 프로그램 코드를 더 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제22 항에 있어서,상기 SOC의 상기 복수의 컴퓨팅 유닛들 각각의 온도 또는 전류 소모 중 적어도 하나를 검출하기 위한 프로그램코드; 및상기 온도 또는 상기 전류 소모에 기초하여 상기 복수의 커널들을 상기 복수의 컴퓨팅 유닛들에 맵핑하기 위한프로그램 코드를 더 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제25 항에 있어서,상기 복수의 컴퓨팅 유닛들 중 임계치를 초과하는 온도 또는 전류를 갖는 컴퓨팅 유닛들에 상기 복수의 커널들의 커널들을 할당하기 위한 프로그램 코드를 더 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제22 항에 있어서,상기 복수의 커널들 각각의 커널의 통계 정보를 컴퓨팅하기 위한 프로그램 코드; 및 상기 통계 정보에 기초하여 상기 복수의 커널들을 상기 복수의 컴퓨팅 유닛들에 맵핑하기 위한 프로그램 코드를 더 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2023-7028493", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "제22 항에 있어서,런타임 동안 상기 SOC의 상기 복수의 컴퓨팅 유닛들에 상기 복수의 커널들을 동적으로 할당하기 위한 프로그램코드를 더 포함하는, 비일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2023-7028493", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공 뉴럴 네트워크(artificial neural network)를 위한 방법은 다수의 컴퓨팅 유닛들을 통해 다수의 커널 (kernel)들과 컨볼빙될(convolved) 한 세트의 입력 값들을 수신하는 단계를 포함한다. 다수의 컴퓨팅 유닛들 중 하나 이상의 열적 스트레스를 받는(thermally-stressed) 컴퓨팅 유닛이 결정된다. 다수의 커널들은 하나 이상의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 SOC(system-on-chip)의 다수의 컴퓨팅 유닛들에 맵핑된다. 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 다수의 커널들 중 가장 희소한 커널(most sparse kernel)과 한 세트의 입력 값들에 대해 컨볼루션이 수행된다."}
{"patent_id": "10-2023-7028493", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 2021년 3월 5일자로 출원되고 발명의 명칭이 \"SPARSITY-BASED NEURAL NETWORK MAPPING TO COMPUTING UNITS IN A SYSTEM-ON-CHIP\"인 미국 특허출원 제17/194,202호에 대한 우선권을 주장하며, 이 특허출 원의 개시내용은 전체가 인용에 의해 본원에 명백히 포함된다. 본 개시내용의 양상들은 일반적으로 인공 뉴럴 네트워크(artificial neural network)들에 관한 것이며, 보다 구체적으로는 개선된 프로세싱 및 맵핑에 관한 것이다."}
{"patent_id": "10-2023-7028493", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 뉴럴 네트워크들은 상호 연결된 인공 뉴런들의 그룹들(예를 들어, 뉴런 모델들)을 포함할 수 있다. 인공 뉴럴 네트워크는 컴퓨테이셔널(computational) 디바이스일 수 있거나, 또는 컴퓨테이셔널 디바이스에 의해 수행되는 방법으로 표현될 수 있다. 딥 컨볼루셔널 뉴럴 네트워크들과 같은 컨볼루셔널 뉴럴 네트워크들은 피 드 포워드(feed-forward) 인공 뉴럴 네트워크의 일 타입이다. 컨볼루셔널 뉴럴 네트워크들은 타일링된 수용 필 드에 구성될 수 있는 뉴런들의 계층들을 포함할 수 있다. 딥 컨볼루셔널 뉴럴 네트워크(deep convolutional neural network, DCN)들은 이미지 인식, 음성 인식, 자율 주행 및 사물 인터넷(IoT) 디바이스들과 같은 다양한 기술들에서 사용된다. 임베디드 IoT 디바이스들은 한정된 온칩 메모리와 같은 제한된 리소스들을 가질 수 있다. 따라서, 이러한 디바이스들에서 DCN들의 사용이 제한될 수 있다. 한정된 리소스들을 가진 디바이스들에서 DCN들의 사용을 개선하는 것이 바람직하다. 본 개시내용의 일 양상에서, 인공 뉴럴 네트워크를 위한 방법이 제시된다. 이 방법은 다수의 컴퓨팅 유 닛들을 통해 다수의 커널(kernel)들과 컨볼빙될(convolved) 한 세트의 입력 값들을 수신하는 단계를 포함한다. 이 방법은 또한 다수의 컴퓨팅 유닛들 중 하나 이상의 열적 스트레스를 받는(thermally-stressed) 컴퓨팅 유닛 들을 결정하는 단계를 포함한다. 이 방법은 하나 이상의 열적 스트레스를 받는 컴퓨팅 유닛들에 기초하여 SOC(system-on-chip)의 다수의 컴퓨팅 유닛들에 다수의 커널들을 맵핑하는 단계를 추가적으로 포함한다. 추가 로, 이 방법은 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 다수의 커널들 중 가장 희소한 커널(most sparse kernel)과 한 세트의 입력 값들의 컨볼루션을 수행하는 단계를 포함한다. 본 개시내용의 일 양상에서, 인공 뉴럴 네트워크를 위한 장치가 제공된다. 이 장치는 메모리 및 메모리 에 커플링된 하나 이상의 프로세서들을 포함한다. 프로세서(들)는 다수의 컴퓨팅 유닛들을 통해 다수의 커널들 과 컨볼빙될 한 세트의 입력 값들을 수신하도록 구성된다. 프로세서(들)는 또한 다수의 컴퓨팅 유닛들 중 하나 이상의 열적 스트레스를 받는 컴퓨팅 유닛을 결정하도록 구성된다. 또한, 프로세서(들)는 하나 이상의 열적 스 트레스를 받는 컴퓨팅 유닛들에 기초하여 SOC(System-on-Chip)의 다수의 컴퓨팅 유닛들에 다수의 커널들을 맵핑 하도록 구성된다. 추가로, 프로세서(들)는 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 다수의 커널들 중 가장 희소한 커널과 한 세트의 입력 값들의 컨볼루션을 수행하도록 구성된다. 본 개시내용의 일 양상에서, 인공 뉴럴 네트워크를 위한 장치가 제공된다. 이 장치는 다수의 컴퓨팅 유 닛들을 통해 다수의 커널들과 컨볼빙될 한 세트의 입력 값들을 수신하기 위한 수단을 포함한다. 이 장치는 또 한 다수의 컴퓨팅 유닛들 중 하나 이상의 열적 스트레스를 받는 컴퓨팅 유닛들을 결정하기 위한 수단을 포함한 다. 추가적으로, 이 장치는 하나 이상의 열적 스트레스를 받는 컴퓨팅 유닛들에 기초하여 SOC(system-on- chip)의 다수의 컴퓨팅 유닛들에 다수의 커널들을 맵핑하기 위한 수단을 포함한다. 추가로, 이 장치는 가장 많 이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 복수의 커널들 중 가장 희소한 커널과 한 세트의 입력 값들의 컨 볼루션을 수행하기 위한 수단을 포함한다. 본 개시내용의 일 양상에서, 비일시적 컴퓨터 판독 가능 매체가 제공된다. 이 컴퓨터 판독 가능 매체에 는 인공 뉴럴 네트워크를 위한 프로그램 코드가 인코딩되어 있다. 프로그램 코드는 프로세서에 의해 실행되며, 다수의 컴퓨팅 유닛들을 통해 다수의 커널들과 컨볼빙될 한 세트의 입력 값들을 수신하기 위한 코드를 포함한다. 프로그램 코드는 또한 다수의 컴퓨팅 유닛들 중 하나 이상의 열적 스트레스를 받는 컴퓨팅 유닛들을 결정하기 위한 코드를 포함한다. 또한, 프로그램 코드는 하나 이상의 열적 스트레스를 받는 컴퓨팅 유닛들에 기초하여 SOC(System-on-Chip)의 다수의 컴퓨팅 유닛들에 다수의 커널들을 맵핑하기 위한 코드를 포함한다. 또 한, 프로그램 코드는 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 복수의 커널들 중 가장 희소한 커널 과 한 세트의 입력 값들의 컨볼루션을 수행하기 위한 코드를 포함한다. 본 개시내용의 추가적인 특징들 및 이점들이 아래에서 설명될 것이다. 본 개시내용이 본 개시내용의 동 일한 목적들을 수행하기 위해 다른 구조들을 수정하거나 또는 설계하기 위한 기초로서 용이하게 활용될 수 있다 는 것이 당업자들에 의해 인식되어야 한다. 또한, 그러한 등가 구성들이 첨부된 청구항들에서 기술되는 바와 같은 본 개시내용의 교시들을 벗어나지 않는다는 것이 당업자들에 의해 인식되어야 한다. 본 개시내용의 구조 및 동작 방법 둘 모두에 대해, 본 개시내용의 특성인 것으로 여겨지는 신규한 특징들은 추가의 목적들 및 이점 들과 함께, 첨부 도면들과 관련하여 고려될 때 다음의 설명으로부터 더 잘 이해될 것이다. 그러나, 도면들 각 각이 단지 예시 및 설명만을 위해 제공되며, 본 개시내용의 제한들의 정의로서 의도되지 않는다는 것이 명백하게 이해되어야 한다."}
{"patent_id": "10-2023-7028493", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "첨부 도면들과 관련하여 아래에 기술되는 상세한 설명은 다양한 구성들의 설명으로 의도되며, 본 명세서 에서 설명된 개념들이 실시될 수 있는 유일한 구성들을 표현하도록 의도되는 것은 아니다. 상세한 설명은 다양 한 개념들의 철저한 이해를 제공할 목적으로 특정 세부사항들을 포함한다. 그러나, 이러한 특정 세부사항들 없 이도 이러한 개념들이 실시될 수 있음은 당업자들에게 자명할 것이다. 일부 예들에서, 이러한 개념들을 불명료 하게 하는 것을 피하기 위해, 잘 알려진 구조들 및 컴포넌트들은 블록도 형태로 도시된다. 본원의 교시들에 기초하여, 당업자는, 본 개시내용의 범위가, 본 개시내용의 임의의 다른 양상과 독립적 으로 구현되든 또는 임의의 다른 양상과 결합되어 구현되든, 본 개시내용에 설명된 개시의 임의의 양상을 커버 하도록 의도됨을 인식해야 한다. 예를 들어, 제시된 양상들 중 임의의 수의 양상을 사용하여 장치가 구현될 수 있거나 또는 방법이 실시될 수 있다. 또한, 본 개시내용의 범위는 제시된 본 개시내용의 다양한 양상들에 추가 하여 또는 그 외에 다른 구조, 기능 또는 구조와 기능을 사용하여 실시되는 그러한 장치 또는 방법을 커버하는 것으로 의도된다. 개시된 본 개시내용의 임의의 양상은 청구항의 하나 이상의 엘리먼트들에 의해 구현될 수 있 다는 것이 이해되어야 한다. \"예시적인\" 이라는 단어는, \"예, 경우, 또는 예시로서 기능하는\" 것을 의미하도록 본 명세서에서 사용된 다. 본 명세서에서 \"예시적인\" 것으로 설명되는 임의의 양상은 반드시 다른 양상들에 비해 선호되거나 유리한 것으로 해석될 필요는 없다. 특정 양상들이 본 명세서에서 설명되지만, 이 양상들의 많은 변형들 및 치환들은 본 개시내용의 범위 내 에 속한다. 바람직한 양상들의 일부 이득들 및 이점들이 언급되지만, 본 개시내용의 범위는 특정 이득들, 용도들 또는 목적들로 제한되는 것으로 의도되지 않는다. 오히려, 본 개시내용의 양상들은, 상이한 기술들, 시스템 구성들, 네트워크들 및 프로토콜들에 광범위하게 적용 가능한 것으로 의도되며, 이들 중 일부는, 바람직한 양상 들의 하기 설명 및 도면들에서 예로서 예시된다. 상세한 설명 및 도면들은 본 개시내용을 제한하는 것이 아니 라 단지 예시할 뿐이며, 본 개시내용의 범위는 첨부된 청구항들 및 이들의 균등물들에 의해 정의된다. 뉴럴 네트워크는 프로세싱의 하나의 라운드(round)로부터 여러 서브파트들로 분할될 수 있다. 이러한 서브파트들 또는 부분들은 타일(tile)들로 지칭될 수 있다. 이 부분들은, 예를 들어, 인공지능(AI) 가속기, 텐 서 프로세싱 유닛, NSP(neural signal processor), NPU(neural processing unit) 등과 같은 컴퓨팅 유닛들에 맵핑될 수 있다. SOC(System-on-Chip)는 컴퓨팅 시스템의 컴포넌트들(예를 들면, 프로세싱 유닛들, 메모리, 입 /출력 포트들 및 보조 스토리지)을 통합하는 집적 회로(IC)이다. SOC는 향상된 프로세싱 성능을 위해 단일 IC 또는 칩 상에 이러한 컴퓨팅 유닛들 중 하나 이상을 포함할 수 있다. 뉴럴 네트워크가 프로세싱될 시에, 전력 밀도 또는 온도가 SOC 전체에 걸쳐 균일하지 않을 수 있다. 즉, 동일한 크기의 텐서들(매트릭스들, 타일들)이 컴퓨팅 유닛들 각각에 맵핑되더라도, SOC의 일부 부분들이 SOC의 다른 부분들보다 더 높은 온도를 가질 수 있다. 이것은 통상적으로 커널 컨볼루션들이 미리 설정된 순서로 컴퓨팅 유닛들에 그대로 할당되기 때문이다. 현실 세계 조건들에서, 동일한 SOC의 컴퓨팅 유닛들은 동일한 동작 주파수, 전압 및 사용률(utilization percentage)에서 온도 및 전력 소모가 다를 수 있다. 이러한 온도 차이는, 예를 들어, 열 전도 경로의 차이, 온-다이(on-die) 누설 편차, 다른 인접한 기능 블록들로부터의 열 커플링, 또는 각각의 컴퓨팅 유닛들에 의해 마주치게 되는 워크로드(workload)의 차이들에 기인할 수 있다. 불균일한 온도들(예를 들면, 핫스팟들)은 성능 -스로틀링(throttling)의 비효율성(동작 주파수 및 실행 파이프라인들의 감소에 의한 저하)을 유발할 수 있다. 일부 경우들에서, 핫스팟으로 인해 열 폭주(thermal runaway)(제어 불능의 급격한 온도 상승)가 발생할 수 있어, SOC 셧다운이 트리거될 수 있다. 본 개시내용의 양상들은 인공 뉴럴 네트워크의 각각의 계층에 대한 커널들에 대한 통계 정보 (statistical information)(예를 들면, 희소성, 희소성 퍼센티지, 또는 가중치 값들의 평균 유효 비트(average significant bit)들)에 기초하여 SOC의 컴퓨팅 유닛들에 네트워크 파티션들 또는 타일들을 맵핑하는 것에 의한 그리고 일부 양상들에서 동적으로 네트워크 파티션들 또는 타일들을 동적으로 맵핑하는 것에 의한 인공 뉴럴 네 트워크들의 개선된 프로세싱에 관한 것이다. 예를 들어, 재맵핑은 런타임 동안 주기적으로(예를 들면, 30초마 다) 발생할 수 있다. 일부 양상들에서, 각각의 컴퓨팅 유닛의 온도 또는 전류 소모가 검출되고 모니터링될 수 있다. 따라서, 일부 양상들에서는, 검출된 온도 또는 전류 소모가 통계 정보와 함께 사용됨으로써, SOC의 하나 이상의 컴퓨팅 유닛들의 검출된 온도 또는 전류 소모를 감소시키도록 맵핑이 수행될 수 있다. 도 1은 본 개시내용의 특정 양상들에 따라, 뉴럴 네트워크 파티션들을 프로세싱 유닛들에 맵핑하기 위해 구성된 중앙 처리 장치(CPU) 또는 멀티-코어 CPU를 포함할 수 있는 SOC(system-on-chip)의 예시적인 구현을 예시한다. 변수들(예컨대, 뉴럴 신호들 및 시냅스 가중치들), 컴퓨테이셔널 디바이스(예컨대, 가중치들 을 갖는 뉴럴 네트워크)와 연관된 시스템 파라미터들, 지연들, 주파수 빈(frequency bin) 정보, 및 태스크 정보 가, NPU(neural processing unit)와 연관된 메모리 블록, CPU와 연관된 메모리 블록, GPU(graphics processing unit)와 연관된 메모리 블록, DSP(digital signal processor)와 연관된 메모리 블록, 메 모리 블록에 저장될 수 있거나, 또는 다수의 블록들에 걸쳐 분산될 수 있다. CPU에서 실행되는 명령 어들은 CPU와 연관된 프로그램 메모리로부터 로딩될 수 있거나 또는 메모리 블록으로부터 로딩될 수 있다. SOC는 또한 특정 기능들에 맞춤화된 추가적인 프로세싱 블록들, 이를테면 GPU, DSP, 연 결 블록(이것은 5G(fifth generation) 연결, 4G LTE(fourth generation long term evolution) 연결, Wi- Fi 연결, USB 연결, 블루투스 연결 등을 포함할 수 있음), 및 예컨대 제스처들을 검출하고 인식할 수 있는 멀티 미디어 프로세서를 포함할 수 있다. 일 구현에서, NPU는 CPU, DSP, 및/또는 GPU에서 구현된다. SOC는 또한 센서 프로세서, ISP(image signal processor)들, 및/또는 글로벌 포지셔닝 시스템 을 포함할 수 있는 내비게이션 모듈을 포함할 수 있다. SOC는 ARM 명령어 세트에 기반할 수 있다. 본 개시내용의 일 양상에서, 범용 프로세서에 로 딩된 명령어들은 다수의 컴퓨팅 유닛들을 통해 다수의 커널들과 컨볼빙되는 한 세트의 입력 값들을 수신하기 위 한 코드를 포함할 수 있다. 범용 프로세서는 또한 다수의 컴퓨팅 유닛들 중 하나 이상의 열적 스트레스를 받는 컴퓨팅 유닛들을 결정하기 위한 코드를 포함할 수 있다. 추가적으로, 범용 프로세서는 하나 이상의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 SOC(System-on-Chip)의 다수의 컴퓨팅 유닛들에 다수의 커널들을맵핑하기 위한 코드를 포함할 수 있다. 범용 프로세서는 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상 에서 다수의 커널들 중 가장 희소한 커널과 한 세트의 입력 값들의 컨볼루션을 수행하기 위한 코드를 더 포함할 수 있다. 딥 러닝 아키텍처들은 각각의 계층에서 연속적으로 더 높은 추상화 레벨들에서 입력들을 표현하도록 학 습하는 것에 의해 객체 인식 태스크를 수행함으로써, 입력 데이터의 유용한 특징 표현을 구축할 수 있다. 이러 한 방식으로, 딥 러닝은 전통적인 기계 학습의 주요 병목을 해결한다. 딥 러닝의 출현 전에, 객체 인식 문제에 대한 기계 학습 접근법은, 아마도 얕은 분류기(shallow classifier)와 결합한, 인간 공학적 특징들에 크게 의존 했을 수 있다. 얕은 분류기는, 예를 들어, 특징 벡터 성분들의 가중 합을 임계치과 비교하여 입력이 속하는 클 래스를 예측할 수 있는 2 클래스 선형 분류기일 수 있다. 인간 공학적 특징(human engineered feature)들은 도 메인 전문 지식을 가진 엔지니어들에 의해서 특정 문제 도메인에 맞추어진 템플릿들 또는 커널들일 수 있다. 대조적으로, 딥 러닝 아키텍처들은 인간 엔지니어가 설계할 수 있는 것과 유사한 특징들을 표현하도록, 그러나 트레이닝을 통해, 학습할 수 있다. 또한, 딥 네트워크는 인간이 고려하지 않았을 수 있는 새로운 타입들의 특 징들을 표현하고 인식하도록 학습할 수 있다. 딥 러닝 아키텍처는 특징들의 계층 구조(hierarchy)를 학습할 수 있다. 예를 들어, 시각적 데이터가 제 시되면, 제1 계층은 입력 스트림에서 에지들과 같은 비교적 단순한 특징들을 인식하도록 학습할 수 있다. 다른 예에서, 청각 데이터가 제시되면, 제1 계층은 특정 주파수들에서 스펙트럼 전력을 인식하도록 학습할 수 있다. 제2 계층은, 제1 계층의 출력을 입력으로 취하여, 특징들의 조합들, 이를테면 시각 데이터에 대한 단순한 형상 들 또는 청각 데이터에 대한 사운드들의 조합들을 인식하도록 학습할 수 있다. 예를 들어, 상위 계층들은 시각 데이터로 복잡한 형상들을 또는 청각 데이터로 단어들을 표현하도록 학습할 수 있다. 더욱 상위 계층들은 일반 적인 시각 객체들 또는 구어(spoken phrase)들을 인식하도록 학습할 수 있다. 딥 러닝 아키텍처들은 자연적 계층 구조를 갖는 문제들에 적용될 때 특히 잘 수행될 수 있다. 예를 들 어, 자동차들의 분류는 바퀴들, 앞 유리들 및 다른 특징들을 인식하도록 먼저 학습하는 것으로부터 이익을 얻을 수 있다. 이러한 특징들은 승용차들, 트럭들 및 비행기들을 인식하기 위해 다양한 방식들로 상위 계층들에서 결합될 수 있다. 뉴럴 네트워크들은 다양한 연결 패턴들로 설계될 수 있다. 피드 포워드 네트워크들에서, 정보는 하위 계층에서 상위 계층으로 전달되며, 주어진 계층의 각각의 뉴런은 상위 계층들의 뉴런들과 통신한다. 계층적 표 현은 전술한 바와 같이, 피드 포워드 네트워크의 연속적인 계층들에 구축될 수 있다. 뉴럴 네트워크들은 또한 순환(recurrent) 또는 피드백(하향식이라고도 함) 연결들을 가질 수 있다. 순환 연결(recurrent connection) 에서, 주어진 계층의 뉴런으로부터의 출력은 동일한 계층의 다른 뉴런으로 통신될 수 있다. 순환 아키텍처는, 시퀀스로 뉴럴 네트워크에 전달되는 입력 데이터 청크(chunk)들 중 하나 초과의 입력 데이터 청크에 걸쳐 있는 패턴들을 인식하는데 도움이 될 수 있다. 주어진 계층의 뉴런으로부터 하위 계층의 뉴런으로의 연결은 피드백(또는 하향식) 연결로 지칭된다. 많 은 피드백 연결들을 갖는 네트워크는, 상위-레벨 개념의 인식이 입력의 특정 하위-레벨 특징들을 구별하는 것을 보조할 수 있을 때 도움이 될 수 있다. 뉴럴 네트워크의 계층들 사이의 연결들은 완전히 연결되거나 로컬로 연 결될 수 있다. 도 2a는 완전 연결 뉴럴 네트워크의 예를 예시한다. 완전 연결 뉴럴 네트워크에서, 제1 계층의 뉴런은 제2 계층의 각각의 뉴런에 자신의 출력을 통신할 수 있으며, 이에 따라 제2 계층의 각각의 뉴런은 제1 계층의 모든 뉴런으로부터 입력을 수신하게 된다. 도 2b는 로컬 연결 뉴럴 네트워크의 예를 예시한다. 로컬 연결 뉴럴 네트워크에서, 제1 계층의 뉴런은 제2 계층의 제한된 수의 뉴런들에 연결될 수 있다. 더 일반적으로, 로컬 연결 뉴럴 네트워크의 로컬 연결 계층은, 계층의 각각의 뉴런이 동일하거나 유사한 연결 패턴을 갖지만, 상이한 값들(예컨대, 210, 212, 214, 및 216)을 가질 수 있는 연결 강도들을 갖도 록 구성될 수 있다. 로컬 연결 연결성 패턴은 상위 계층에서 공간적으로 별개인 수용 필드들을 발생시킬 수 있 으며, 그 이유는 주어진 영역의 상위 계층 뉴런들이 네트워크에 대한 총 입력의 제한된 부분의 특성들로 트레이 닝을 통해 튜닝되는 입력들을 수신할 수 있기 때문이다. 로컬 연결 뉴럴 네트워크의 일 예는 컨볼루셔널 뉴럴 네트워크이다. 도 2c는 컨볼루셔널 뉴럴 네트워크 의 예를 예시한다. 컨볼루셔널 뉴럴 네트워크는 제2 계층의 각각의 뉴런에 대한 입력들과 연관된 연 결 강도들(예컨대, 208)이 공유되도록 구성될 수 있다. 컨볼루셔널 뉴럴 네트워크들은 입력들의 공간적 위치가 의미있는 문제들에 대해 매우 적합할 수 있다. 컨볼루셔널 뉴럴 네트워크의 일 타입은 DCN(deep convolutional network)이다. 도 2d는 자동차 장착 카메라와 같은 이미지 캡처 디바이스로부터 입력된 이미지로부터 시각적 특징들을 인식하도록 설계된 DCN의 상세한 예를 예시한다. 본 예의 DCN은 교통 표지판들 및 교통 표지판에 제공된 넘버를 식별하 도록 트레이닝될 수 있다. 물론, DCN은 차선 표시(lane marking)들을 식별하거나 신호등들을 식별하는 것 과 같은 다른 태스크들을 위해 트레이닝될 수 있다. DCN은 지도 학습(supervised learning)으로 트레이닝될 수 있다. 트레이닝 동안, DCN에는 속도 제한 표지판의 이미지와 같은 이미지가 제시될 수 있고, 이어서 출력을 생성하기 위해 포워드 패스(forward pass)가 컴퓨팅될 수 있다. DCN은 특징 추출 섹션 및 분류 섹션을 포함할 수 있다. 이미지 의 수신 시에, 컨볼루셔널 계층은 이미지에 컨볼루셔널 커널들(미도시)을 적용하여 제1 세트의 특징 맵들을 생성할 수 있다. 일 예로서, 컨볼루셔널 계층에 대한 컨볼루셔널 커널은 28x28 특징 맵 들을 생성하는 5x5 커널일 수 있다. 본 예에서, 4개의 상이한 특징 맵들이 제1 세트의 특징 맵들에서 생 성되기 때문에, 4개의 상이한 컨볼루셔널 커널들이 컨볼루셔널 계층에서 이미지에 적용되었다. 컨볼 루셔널 커널들은 또한 필터들 또는 컨볼루셔널 필터들로 지칭될 수 있다. 제1 세트의 특징 맵들은 제2 세트의 특징 맵들을 생성하기 위해 최대 풀링 계층(max pooling layer)(미도시)에 의해 서브샘플링될 수 있다. 최대 풀링 계층은 제1 세트의 특징 맵들의 크기를 감소시 킨다. 즉, 14x14와 같은 제2 세트의 특징 맵들의 크기는 28x28과 같은 제1 세트의 특징 맵들의 크기 보다 더 작다. 감소된 크기는 메모리 소비를 줄이면서 후속 계층에 유사한 정보를 제공한다. 제2 세트의 특징 맵들은 하나 이상의 후속 컨볼루셔널 계층들(미도시)을 통해 추가로 컨볼빙되어 하나 이상의 후속 세트들 의 특징 맵들(미도시)을 생성할 수 있다. 도 2d의 예에서, 제2 세트의 특징 맵들이 컨볼빙되어 제1 특징 벡터를 생성한다. 또한, 제1 특징 벡터가 추가로 컨볼빙되어 제2 특징 벡터를 생성한다. 제2 특징 벡터의 각각의 특징은 \" 표지판\", \"60\" 및 \"100\"과 같은, 이미지의 가능한 특징에 대응하는 넘버를 포함할 수 있다. 소프트맥스 함수(softmax function)(미도시)는 제2 특징 벡터의 넘버들을 확률로 변환할 수 있다. 따라서, DCN(20 0)의 출력은 하나 이상의 특징들을 포함하는 이미지의 확률이다. 본 예에서, 출력에서 \"표지판\" 및 \"60\"에 대한 확률들은 출력의 다른 것들, 이를테면 \"30\", \"40\", \"50\", \"70\", \"80\", \"90\", 및 \"100\"의 확률들보다 더 높다. 트레이닝 전에는, DCN에 의해 생성되는 출력이 부정확할 가능성이 있다. 따라서, 출력과 타겟 출력 사이의 오차가 계산될 수 있다. 타겟 출력은 이미지의 지상 실측 정보(ground truth)(예컨대, \"표지판\" 및 \"60\")이다. 그 다음, DCN의 가중치들이, DCN의 출력이 타겟 출력과 더 근접하게 정렬되도록 조정될 수 있다. 가중치들을 조정하기 위해, 학습 알고리즘은 가중치들에 대한 기울기 벡터를 컴퓨팅할 수 있다. 기울기 는 가중치가 조정된 경우 오차가 증가 또는 감소할 양을 표시할 수 있다. 최상위 계층에서, 기울기는 끝에서 두 번째(penultimate) 계층의 활성화된 뉴런과 출력 계층의 뉴런을 연결하는 가중치의 값에 직접 대응할 수 있 다. 하위 계층들에서, 기울기는 가중치들의 값 및 상위 계층들의 컴퓨팅된 오차 기울기들에 의존할 수 있다. 그 다음, 오차를 줄이기 위해 가중치들이 조정될 수 있다. 가중치들을 조정하는 이러한 방식은 뉴럴 네트워크 를 통한 \"백워드 패스(backward pass)\"를 수반하므로 \"역전파(back propagation)\"로 지칭될 수 있다. 실제에 있어서, 가중치들의 오차 기울기는, 계산된 기울기가 실제 오차 기울기에 근사하도록, 적은 수의 예들에 걸쳐 계산될 수 있다. 이러한 근사화 방법은 확률적 기울기 하강으로 지칭될 수 있다. 확률적 기울기 하강은, 전체 시스템의 달성 가능한 오차 레이트가 감소하는 것을 중단할 때까지 또는 오차 레이트가 타겟 레벨 에 도달할 때까지, 반복될 수 있다. 학습 이후에, DCN에는 새로운 이미지들(예를 들어, 이미지의 속도 제 한 표지판)이 제시될 수 있으며, 네트워크를 통한 포워드 패스는 DCN의 추론 또는 예측으로 간주될 수 있는 출 력을 산출할 수 있다. DBN(deep belief network)들은 은닉 노드들의 다수의 계층들을 포함하는 확률 모델들이다. DBN들은 트 레이닝 데이터 세트들의 계층적 표현을 추출하기 위해 사용될 수 있다. DBN은 RBM(Restricted Boltzmann Machine)들의 계층들을 적층함으로써 획득될 수 있다. RBM은 한 세트의 입력들에 걸쳐 확률 분포를 학습할 수 있는 인공 뉴럴 네트워크의 타입이다. RBM들이 각각의 입력이 카테고리화되어야 하는 클래스에 관한 정보가 없 을 경우에 확률 분포를 학습할 수 있기 때문에, RBM들은 종종 비지도 학습(unsupervised learning)에서 사용된 다. 하이브리드 비지도 및 지도 패러다임을 사용하여, DBN의 최하위 RBM들은 비지도 방식으로 트레이닝될 수 있고 특징 추출기들로서의 역할을 할 수 있으며, 최상위 RBM은 지도 방식으로 (이전 계층 및 타겟 클래스들로부 터의 입력들의 공동 분포(joint distribution)에 대해) 트레이닝될 수 있고 분류기로서의 역할을 할 수 있다. DCN(deep convolutional network)들은 추가적인 풀링 및 정규화 계층들로 구성되는 컨볼루셔널 네트워 크들의 네트워크들이다. DCN들은 많은 태스크들에 대해 최신 성능(state-of-the-art performance)을 달성하였 다. DCN들은 지도 학습을 사용하여 트레이닝될 수 있으며, 여기서 입력 및 출력 타겟들 둘 모두는 많은 예시들 에 대해 알려져 있고, 기울기 하강 방법들의 사용에 의해 네트워크의 가중치들을 수정하기 위해 사용된다. DCN들은 피드 포워드 네트워크들일 수 있다. 또한, 위에서 설명된 바와 같이, DCN의 제1 계층의 뉴런으 로부터 다음 상위 계층의 뉴런들의 그룹으로의 연결들은 제1 계층의 뉴런들에 걸쳐 공유된다. DCN들의 피드 포 워드 및 공유된 연결들은 고속 프로세싱을 위해 이용될 수 있다. DCN의 컴퓨테이셔널 부담은, 예컨대, 순환 또 는 피드백 연결들을 포함하는 유사한 크기의 뉴럴 네트워크의 컴퓨테이셔널 부담보다 훨씬 더 적을 수 있다. 컨볼루셔널 네트워크의 각각의 계층의 프로세싱은 공간적으로 불변인 템플릿 또는 베이시스 프로젝션으 로 고려될 수 있다. 입력이 컬러 이미지의 다수의 채널들, 이를테면 적색, 녹색 및 파란색 채널들로 먼저 분해 되면, 해당 입력에 대해 트레이닝된 컨볼루셔널 네트워크는, 2개의 공간 차원들이 이미지의 축들을 따르고 제3 차원이 컬러 정보를 캡처하는, 3차원으로 간주될 수 있다. 컨볼루셔널 연결들의 출력들은 후속 계층에서 특징 맵을 형성하는 것으로 고려될 수 있으며, 특징 맵(예를 들어, 220)의 각각의 엘리먼트는 이전 계층의 뉴런들의 범위(예를 들어, 특징 맵들)로부터 그리고 다수의 채널들 각각으로부터 입력을 수신한다. 특징 맵의 값들 은 추가로, 정류(rectification) max(0, x)와 같이 비-선형성으로 프로세싱될 수 있다. 인접한 뉴런들로부터의 값들은 추가로 풀링될 수 있으며(이는 다운 샘플링에 대응함), 또한 추가적인 로컬 불변성 및 차원 감소를 제공 할 수 있다. 화이트닝(whitening)에 대응하는 정규화가 또한, 특징 맵에서 뉴런들 사이의 측면 억제(lateral inhibition)를 통해서 적용될 수 있다. 딥 러닝 아키텍처들의 성능은 더 많은 라벨링된 데이터 포인트들이 이용 가능해짐에 따라 또는 컴퓨테이 셔널 파워가 증가함에 따라 증가할 수 있다. 현대의 딥 뉴럴 네트워크들은 불과 15년 전만 해도 일반 연구자가 이용 가능했던 것보다 수천 배 더 많은 컴퓨팅 리소스들로 일상적으로 트레이닝된다. 새로운 아키텍처들과 학 습 패러다임들은 딥 러닝의 성능을 더욱 향상(boost)시킬 수 있다. 정류된 선형 유닛들은 기울기 소실 (vanishing gradient)로 알려진 트레이닝 문제를 줄일 수 있다. 새로운 트레이닝 기술들은 오버-피팅(over- fitting)을 감소시킬 수 있으며, 따라서 더 큰 모델들이 더 양호한 일반화를 달성할 수 있도록 한다. 캡슐화 기술들은 주어진 수용 필드에서 데이터를 추상화하고 전체 성능을 추가로 향상시킬 수 있다. 도 3은 딥 컨볼루셔널 네트워크를 예시하는 블록도이다. 딥 컨볼루셔널 네트워크는 연결 및 가중치 공유에 기반하여 다수의 상이한 타입들의 계층들을 포함할 수 있다. 도 3에 도시된 바와 같이, 딥 컨볼 루셔널 네트워크는 컨볼루션 블록들(354A, 354B)을 포함한다. 컨볼루션 블록들(354A, 354B) 각각은 컨볼 루셔널 계층(CONV), 정규화 계층(LNorm) 및 최대 풀링 계층(MAX POOL)으로 구성될 수 있다. 컨볼루션 계층들은 특징 맵을 생성하기 위해 입력 데이터에 적용될 수 있는 하나 이상의 컨볼루셔 널 필터들을 포함할 수 있다. 컨볼루션 블록들(354A, 354B) 중 단지 2개만이 도시되어 있지만, 본 개시내용은 이것에 제한되지 않으며, 대신에, 임의의 수의 컨볼루션 블록들(354A, 354B)이 설계 선호도에 따라 딥 컨볼루셔 널 네트워크에 포함될 수 있다. 정규화 계층은 컨볼루션 필터들의 출력을 정규화할 수 있다. 예컨 대, 정규화 계층은 화이트닝 또는 측면 억제를 제공할 수 있다. 최대 풀링 계층은 로컬 불변성 및 차원 감소를 위해 공간에 걸쳐 다운 샘플링 어그리게이션(down sampling aggregation)을 제공할 수 있다. 예를 들어, 딥 컨볼루셔널 네트워크의 병렬 필터 뱅크들은 SOC의 CPU 또는 GPU 상에 로 딩되어 고성능 및 저전력 소모를 달성할 수 있다. 대안적인 실시예들에서, 병렬 필터 뱅크들은 SOC의 DSP 또는 ISP 상에 로딩될 수 있다. 또한, 딥 컨볼루셔널 네트워크는 SOC 상에 존재할 수 있는 다른 프로세싱 블록들, 이를테면, 센서들 및 내비게이션에 대해 각각 전용되는 센서 프로세서 및 내비게이션 모듈에 액세스할 수 있다. 딥 컨볼루셔널 네트워크는 또한 하나 이상의 완전 연결 계층들(FC1 및 FC2)을 포함할 수 있다. 딥 컨볼루셔널 네트워크는 LR(logistic regression) 계층을 더 포함할 수 있다. 딥 컨볼루 셔널 네트워크의 각각의 계층(356, 358, 360, 362, 364) 사이에는 업데이트될 가중치들(미도시)이 있다. 계층들(예컨대, 356, 358, 360, 362, 364) 각각의 출력은 컨볼루션 블록들 중 제1 컨볼루션 블록들(354A)에서 공급되는 입력 데이터(예컨대, 이미지들, 오디오, 비디오, 센서 데이터 및/또는 다른 입력 데이터)로부터 계층적 특징 표현들을 학습하기 위해 딥 컨볼루셔널 네트워크의 계층들(예컨대, 356, 358, 360, 362, 364) 중 후속 계층의 입력으로서의 역할을 할 수 있다. 딥 컨볼루셔널 네트워크의 출력은 입력 데이터에 대한 분류 스코어이다. 분류 스코어는 한 세트의 확률들일 수 있으며, 각각의 확률은 한 세트의 특징들로부터의 특징을 포함하는 입력 데이터의 확률이다. 도 4는 본 개시내용의 양상들에 따른 SOC(system-on-chip)을 예시하는 블록도이다. SOC는 하나 이상의 컴퓨팅 유닛들(402a-z)을 포함할 수 있다. 컴퓨팅 유닛들(402a-z) 각각은, 예를 들면, NPU(neural processing unit), NSP(neural signal processor), TPU(tensor processing unit), 인공지능 가속기 (artificial intelligence accelerator) 또는 다른 프로세싱 유닛일 수 있다. SOC의 컴퓨팅 유닛들(예를 들어, 402a-z)은 버스를 통해 상호 연결될 수 있으며 이에 따라 컴퓨팅 유닛들(예를 들어, 402a-z)은 도 3 의 컨볼루셔널 뉴럴 네트워크와 같은 인공 뉴럴 네트워크를 실행할 수 있다. 그러나, 본 개시내용이 이에 제한되는 것은 아니며, 패브릭, NOC(network on a chip), 또는 임의의 적절한 인터커넥트가 컴퓨팅 유닛들을 상 호 연결할 수 있다. SOC 전체에 걸친 컴퓨팅 유닛들(예를 들어, 402a-z)의 온도 및 전력 소모는 달라질 수 있다. 현실 세계 조건에서, 동일한 SOC 상의 컴퓨팅 유닛들(402a-z)은 동일한 동작 주파수, 전압 및 사용률에서 상이 한 온도 및 전력 소모를 가질 수 있다. 이러한 편차들은, 예를 들어, SOC 패키지(예를 들어, SOC 플로어 플랜(floorplan))에 대한 컴퓨팅 유닛들(402a-z)의 상이한 열 저항 및 전도 경로들, 인접한 기능 블록들(예를 들어, 인접한 컴퓨팅 유닛들)로부터의 열 차이들, 실리콘 누설에서의 온-다이 편차들(예를 들어, 전류(Idd) 정 지 상태(IDDQ)에서의 전류 소모), 및 컴퓨팅 유닛들(402a-z)에 맵핑된 타일들에서의 가중치의 희소성 차이에 기 인할 수 있다. 컴퓨팅 유닛들(402a-z) 사이의 온도 편차들(예를 들어, 핫스팟들)은 성능-스로틀링의 비효율성(동작 주 파수 및 실행 파이프라인들의 감소에 의한 저하)을 유발할 수 있다. 일부 경우들에서, 핫스팟으로 인해 열 폭 주(제어 불능의 급격한 온도 상승)가 발생할 수 있어, SOC 셧다운이 트리거될 수 있다. 도 5a 내지 도 b는 본 개시내용의 양상들에 따른 뉴럴 네트워크 파티션들을 SOC의 컴퓨팅 유닛들에 재맵 핑하는 예들을 예시한다. 도 5a는 본 개시내용의 양상들에 따른 컴퓨팅 유닛들에 대한 뉴럴 네트워크의 맵핑을 예시하는 블록도이다. 도 5a에 도시된 바와 같이, 한 세트의 입력 값들(502a-n)이 프로세싱을 위해 입력 을 통해 수신될 수 있다. 한 세트의 입력 값들(502a-n)은, 예를 들어, 이미지를 나타낼 수 있다. 한 세트의 입력 값들(502a-n)이 뉴럴 네트워크를 통해 프로세싱됨으로써 출력(예를 들어, 이미지 분류)을 도출할 수 있다. 일부 양상들에서, 입력은 다른 입력 블록들과 상이하거나 동일할 수 있다. 한 세트의 입력 값들(502a- n)이 한 세트의 가중치 매트릭스들 또는 커널들(예를 들어, 커널 #0-7)과 컨볼빙됨으로써 출력을 통해 한 세트의 출력 값들(504a-m)을 생성할 수 있다. 일부 양상들에서, 출력은 다른 출력 블록들과 상이하거나 동일할 수 있다. 출력 값들(504a-m)이 뉴럴 네트워크를 동작시키도록 뉴럴 네트워크의 후속 계층들에 대한 입 력 값들로서 제공됨으로써 원하는 태스크(예를 들어, 입력 이미지의 분류)를 수행할 수 있다. 도 5a에 예시된 바와 같이, 인공 뉴럴 네트워크의 동작은 병렬 프로세싱을 통해 수행될 수 있다. 한 세 트의 입력 값들(502a-n)은 컨볼루션 연산을 수행하기 위해 개별 컴퓨팅 유닛들(예를 들어, 컴퓨팅 유닛들(402a- z))에 맵핑될 수 있는 타일로 간주될 수 있다. 예를 들어, 입력 값들(502a-n)은 8개의 커널들(예를 들어, 커널 #0 내지 커널 #7) 각각과 개별적으로, 병렬로 컨볼빙될 수 있다. 각각의 이러한 타일-커널 컨볼루션은 출력 값 들(504a-m)의 계층을 생성하기 위해 개별 컴퓨팅 유닛(예를 들어, 520a)에 대해 수행될 수 있다. 예를 들어, 한 세트의 입력 값들(502a-n)를 커널(예를 들어, 커널 #0)과 컨볼빙하는 연산이 컴퓨팅 유닛(520a)에 할당될 수 있으며 이에 따라 출력 값들(예를 들어, 504a-m)의 출력 계층 #0을 생성할 수 있다. 다른 예에서, 한 세트의 입력 값들(502a-n)를 커널 #1과 컨볼빙하는 연산이 컴퓨팅 유닛(520b)에 할당될 수 있으며 이에 따라 출력 값들 (예를 들어, 504a-m)의 출력 계층 #1을 생성할 수 있다. 커널들 #0-7이 도시되어 있지만, 커널들의 수가 이에 제한되는 것은 아니며 임의의 수의 커널들이 사용될 수 있다는 것이 주목되어야 한다. 도 5b는 본 개시내용의 양상들에 따른 컴퓨팅 유닛들에 대한 동적 맵핑(또는 재맵핑)을 갖는 병렬 프로 세싱을 통해 수행되는 인공 네트워크의 동작을 예시한다. 도 5b에 도시된 바와 같이, 커널(예를 들어, 커널들 #0-7) 각각의 희소성 파라미터(예를 들어, 0 값들의 수)가 결정될 수 있다. 일부 양상들에서, 희소성 파라미터 는 각각의 커널에 대한 이진 라벨(예를 들어, 희소함/희소하지 않음)이거나, 또는 희소성의 순서에 따른 커널의 랭킹(특정 희소성 레벨이 계산되었는지 여부와 상관없이) 또는 다른 희소성 척도일 수 있다. 각각의 커널(예를 들어, 커널들 #0-7)의 희소성을 결정하면, 커널들은 결정된 희소성에 따라(예를 들어, 희소성이 감소하는 순서 로) 소팅 및 배열되거나 정렬될 수 있다. 도 5b의 예에서, 커널 #3은 커널들 중에서 가장 큰 희소성을 가지며 첫 번째 순서로 배치된다. 반면에, 커널 #5는 커널들의 그룹 중에서 가장 작은 희소성을 가지며 마지막 순서로 배치된다. 따라서, 입력 부분들 또는 볼륨들(예를 들어, 552a-n)과 커널들(커널들 #0-7)을 컨볼빙하는 타일들또는 연산들이, 결정된 희소성 순서에 기초하여 컴퓨팅 유닛(예를 들어, 컴퓨팅 유닛(402a-z))에 맵핑될 수 있 다. 일부 양상들에서, 컴퓨팅 유닛들(예를 들어, 컴퓨팅 유닛(402a-z)) 각각에서의 또는 그 근처의 온도는 또한, 온도 센서들(예를 들어, 도 1의 센서들)을 통해 검출될 수 있다. 추가적으로, 컴퓨팅 유닛들에서의 또는 그 근처의 온도는 뉴럴 네트워크의 동작 전반에 걸쳐 디바이스를 보호하기 위해 연속적으로 모니터링될 수 있다. 엘리먼트는 SOC의 예시적인 컴퓨팅 유닛들(NSP #0-7)의 온도를 그래픽으로 예시한 차트이다. 엘리 먼트에 도시된 바와 같이, 컴퓨팅 유닛(NSP #1)은 가장 높은 온도를 가지며, SOC 상의 핫스팟이다. 반면 에, 온도 센서들(예를 들어, 센서들)은 컴퓨팅 유닛(NSP #7)이 가장 낮은 온도를 갖는다는 것을 검출하였 다. 본 개시내용의 양상들에 따르면, 컴퓨팅 유닛들에 대한 뉴럴 네트워크의 맵핑은 컴퓨팅 유닛들에서 또는 그 근처에서 검출된 온도에 기초하여 결정될 수 있다. 제한이 아닌 단지 예로서, 도 5b에 도시된 바와 같이, 가장 큰 희소성을 가진 커널(예를 들어, 커널 # 3)과 대응하는 입력 부분 또는 볼륨을 컨볼빙하는 연산을 위한 타일(예를 들어, 570a)이, 가장 높은 검출 온도 를 갖는 컴퓨팅 유닛(예를 들어, NSP #1)에 할당되거나 맵핑될 수 있다. 그렇게 함으로써, 수행될 컨볼루션 연 산들의 수가 감소되거나 커널 #3이 커널들 중 가장 작을 수 있기 때문에 NSP #1의 워크로드가 감소될 수 있다. 그 이유는 컨볼루션 연산이 0의 가중치 값을 포함하는 경우, 대응하는 곱셈 및 누산 연산들이 스킵될 수 있기 때문이다. 따라서, 이러한 컴퓨팅 유닛(예를 들어, NSP #1)에서 또는 그 근처에서 연속적으로 검출되는 온도가 감소될 수 있다. 유사하게, 컴퓨팅 유닛(NSP #2)이 그 다음으로 가장 높은 검출 온도를 갖는 것으로 결정되는 경우, 다음으로 가장 큰 희소성을 가진 커널(예를 들어, 커널 #7)과 대응하는 입력 부분 또는 볼륨(예를 들어, 552a-n)을 컨볼빙하는 연산을 위한 타일(예를 들어, 570b)이, 그 다음으로 가장 높은 검출 온도를 갖는 컴퓨팅 유닛(예를 들어, NSP #2)에 할당되거나 맵핑될 수 있다. 모든 뉴럴 네트워크 파티션들이 프로세싱될 때까지 이 러한 방식으로 재매핑이 수행될 수 있다. 예를 들어, 컴퓨팅 유닛(NSP #7)이 가장 낮은 검출 온도를 갖는 것으 로 결정되며, 따라서, 가장 작은 희소성을 가진 커널(예를 들어, 커널 #5)과 대응하는 입력(예를 들어, 552a- n)을 컨볼빙하는 연산을 위한 타일(예를 들어, 570n)이 가장 낮은 검출 온도를 가진 컴퓨팅 유닛(예를 들어, NSP #7)에 할당되거나 맵핑될 수 있다. 따라서, 컴퓨팅 유닛(NSP #7)은 커널 #5와 대응하는 입력(예를 들어, 552a-n)의 컨볼루션을 수행하여 출력#5를 생성할 수 있다. 출력 부분들 각각이, 차례로, 뉴럴 네트워크의 후속 계층들에 공급될 수 있으며 이에 따라 원하는 태스크를 수행할 수 있다. 이러한 방식으로, SOC의 컴퓨팅 유닛들(예를 들어, NSP #0-7) 전체에 걸친 온도가 밸런싱될 수 있으며, 일부 양상들에서, 핫스팟들이 감소될 수 있다. 그 이유는 온도가 전력 소모와 상관 관계가 있고, (백퍼센트 (100%)-희소성 퍼센티지)와는 역상관 관계가 있기 때문이다. 즉, 컴퓨팅 유닛에 맵핑된 커널들의 희소성이 증 가함에 따라, 이러한 컴퓨팅 유닛에 의해 수행될 계산들이 감소하고 전력 소모 및 온도가 감소한다. 또한, 온 도 밸런싱 및 핫스팟 감소는 동작 주파수를 감소시키거나 커맨드들 또는 스레드들의 처리를 지연시킴 없이 유리 하게 수행될 수 있다. 도 6은 본 개시내용의 양상들에 따른 온도 밸런싱 및 핫스팟 감소를 보여주는 한 쌍의 예시적인 히트맵 들을 예시한다. 도 6을 참조하면, 히트맵은 SOC의 불균일한 열 분포를 예시한다. 예를 들어, 하나 이상의 온도 센서들(예를 들어, 센서들)을 통해 검출된 온도들이 다양한 음영도들로 히트맵에 반영될 수 있다. 더 높은 음영도(예를 들어, 더 어두운 음영)는 더 낮은 음영도(예를 들어, 더 밝은 음영)를 가진 영역들 보다 지시된 영역에서 더 높은 온도를 나타낸다. 예를 들어, 영역들(604a-f)은 SOC의 다른 영역들보다 더 높은 음영도를 갖는 것으로 도시되어 있으며, 핫스팟들로 간주될 수 있다. 히트맵은 희소성 및 온도에 기 초하여 뉴럴 네트워크 파티션들의 프로세싱을 재맵핑한 이후에 뉴럴 네트워크를 프로세싱하는 동안 SOC의 예시적인 열 분포를 예시한다. 더 높은 희소성(예를 들어, 더 많은 수의 0 값들)을 가진 커널들과 관련된 컨볼 루션 연산들이 히트맵에서 더 높은 온도의 영역들(604a-f)에서의 또는 그 근처의 컴퓨팅 유닛들(예를 들어, NSP들)에 맵핑되고, 더 낮은 희소성(예를 들어, 더 낮은 수의 0 값들)을 가진 커널들과 관련된 컨볼루션 연산들이 더 밝은 음영의 영역들에서 또는 그 근처의 컴퓨팅 유닛들(NSP들)에 맵핑되도록 뉴럴 네트워크의 처리 를 재맵핑하는 것은 히트맵의 대응하는 영역들(654a-f)이 더 낮은 음영도를 갖는 것으로 도시된다. 더 낮 은 음영도는 이러한 대응하는 영역(654a-f)에 대해 검출된 온도가 영역(604a-f)보다 더 낮다는 것을 나타낸다. 도 7은 본 개시내용의 양상들에 따른 뉴럴 네트워크의 재맵핑 프로세싱을 위한 아키텍처를 예시하 는 블록도이다. 도 7을 참조하면, 아키텍처는 온도 센서들, 전류 센서들, 커널 통계 유닛 , 메타데이터 유닛, 동적 맵핑 유닛 및 컴퓨팅 유닛들을 포함한다. 뉴럴 네트워크의 동작 중 SOC(예를 들면, SOC)와 관련된 조건들이 변경됨에 따라, 하나 이상의 온도 센서들이 각각의 컴퓨팅 유닛(예를 들면, 402a-z) 및 주변 영역 및/또는 컴포넌트들의 온도를 검출할 수 있다. 마찬가지로, 하나 이 상의 전류 센서들이 각각의 컴퓨팅 유닛(예를 들어, 402a-z)의 전류 소모를 검출할 수 있다. 온도 센서들 및 전류 센서들은 각각 뉴럴 네트워크의 런타임 동작 동안에 컴퓨팅 유닛들의 온도 및 전류 소모를 연속적으로 모니터링하기 위해 사용될 수 있다. 커널 통계 유닛은 뉴럴 네트워크의 각 계층에서 각 커널에 대한 통계를 계산할 수 있다. 계산되는 커널 통계는, 예를 들어, 커널에서 가중치 값들의 희소성, 각 커널에서 가중치 값들의 희소성 퍼센티지, 및/또 는 각 커널에서 가중치의 유효 비트들의 평균을 포함할 수 있다. 계산되는 커널 통계는 메타데이터 유닛 에 공급될 수 있다. 데이터 프로세싱 유닛은 뉴럴 네트워크의 커널들(가중치 값들)을 수신하여 오차 보정, 채널 코딩, 채널 또는 특징 맵 프루닝, 커널 내 레벨 프루닝(intra-kernel level pruning), 가중치 희소성을 도출하기 위한 트레이닝, 양자화, 정규화(예를 들어, L1 정규화)를 수행할 수 있으며, 또는 그렇지 않으면 데이터 또는 커널 (예를 들어, 가중치 값들)을 조정할 수 있다. 데이터 프로세싱은 뉴럴 네트워크를 프로세싱하기 전에 수행될 수 있다. 동작 시에, 데이터 프로세싱 유닛은 커널 통계(예를 들어, 희소성, 희소성 퍼센티지, 평균 유효 비트들, 또는 다른 커널 통계)의 계산을 위해 조정된 커널 정보를 커널 통계 유닛에 제공할 수 있다. 메 타데이터 유닛은, 차례로, 커널 통계 정보를 수신할 수 있으며, 동적 맵핑 유닛에 공급될 수 있는 메 타데이터를 생성할 수 있다. 동적 맵핑 유닛은 메타데이터에 포함된 커널 통계 정보(예를 들어, 희소성, 희소성 퍼센티지, 평균 유효 비트들 등)를 수신하며, 커널 통계 정보에 기초하여 뉴럴 네트워크의 프로세싱을 할당할 수 있다. 온도 센서 및 전류 센서는 검출된 온도 정보 및 검출된 전류 정보를 동적 맵핑 유닛에 각각 공급할 수 있다. 차례로, 동적 맵핑 유닛은 온도 정보, 전류 정보, 및 커널 통계 정보를 포함하는 메타데이터 중 하나 이상에 기초하여 뉴럴 네트워크를 프로세싱하기 위한 컴퓨팅 유닛을 할당할 수 있다. 일부 양상들에 서, 컴퓨팅 유닛 정보(예를 들어, 구성 정보, 프로세싱 능력, 및 프로세싱 워크로드)는, 뉴럴 네트워크의 프로 세싱을 할당하기 위해 동적 맵핑 유닛에 공급될 수 있다. 도 8은 본 개시내용의 양상들에 따른 뉴럴 네트워크 컨테이너 파일을 예시하는 블록도이다. 뉴럴 네트워크 컨테이너 파일은 네트워크 아키텍처 정보(예를 들어, 네트워크 아키텍처에 대한 설명), 네 트워크 파라미터들 및 뉴럴 네트워크의 계층들에 대한 텐서 정보를 포함할 수 있다. 또한, 뉴럴 네 트워크 컨테이너 파일은 커널 통계 정보(예를 들어, 희소성, 계층당 각 커널들의 희소성 퍼센티지, 또는 평균 유효 비트들)와 같은 메타데이터를 포함할 수 있다. 일부 양상들에서, 뉴럴 네트워크 컨테이너 파일 은 컴파일 타임에 생성될 수 있다. 따라서, SOC(예를 들어, SOC)가 데이터 또는 뉴럴 네트워크 컨테 이너 파일을 메모리에 로딩할 때, 뉴럴 네트워크 컨테이너 파일(예를 들어, 메타데이터)에 포함된 정 보가, 컴퓨팅 유닛들(예를 들어, 컴퓨팅 유닛들(402a-z))에 대한 맵핑을 동적으로 결정하는데 사용될 수 있다. 도 9a 내지 도 9c는 본 개시내용의 양상들에 따른 희소성 기반 재맵핑을 예시하는 도면들의 세트이다. 도 9a를 참조하면, 한 세트의 커널들이, 예를 들어, 한 세트의 출력 값들(예를 들어, 출력 특징 맵)을 생 성하기 위해 입력 이미지에 적용될 수 있다. 출력 값들은 원하는 태스크(예를 들어, 이미지 분류)를 수행하기 위해 뉴럴 네트워크의 후속 계층에 제공될 수 있다. 커널들(0-7) 각각은 희소성을 도출하기 위해 프루닝을 거 칠 수 있다. 즉, 커널들이 더 많은 0 값들을 갖도록 독려될 수 있다. 프루닝은 모든 커널들에 적용되거나 커 널 내 기반으로 적용될 수 있다. 추가적으로, 프루닝은 예를 들어, 트레이닝 알고리즘을 사용하여 가중치 희소 성을 도출하는 것(예를 들어, 낮은 값 가중치를 0으로 반복적으로 설정하고 네트워크를 계속 트레이닝하는 것), 양자화(예를 들어, 낮은 값 가중치를 0으로 반올림), 또는 트레이닝 동안 정규화(예를 들어, L1 정규화)를 통해 수행될 수 있다. 낮은 가중치 값들은 한 세트의 커널들(902a)에서 더 어두운 음영으로 표현된다. 예를 들어, 커널 0에서 엘리먼트들 a, b, f, g 및 h는 낮은 값을 나타내는 더 어두운 음영을 갖는다. 마찬가지로, 커널 2에서, 엘리먼 트들 a, b, d, e, f, g 및 h는 더 낮은 값을 갖는다. 도 9b에 도시된 바와 같이, 커널들(도 9a에 도시됨)의 더 낮은 가중치 값들은 검은색 음영 엘리먼 트들에 의해 표현된 바와 같이 0으로 설정되었다. 예를 들어, 커널 0에서, 엘리먼트들 a, b, f, g 및 h는 검은 색 음영으로 표현된 바와 같이 0으로 설정되었다. 마찬가지로, 커널 2에서, 엘리먼트들 a, b, d, e, f, g 및 h 는 검은색 음영으로 표현된 바와 같이 0으로 설정되었다. 도 9c에 도시된 바와 같이, 한 세트의 커널들은 희소성에 따라 더 소팅될 수 있다. 예를 들어, 커 널 2(도 9b에서)는 커널들 중에서 가장 많은 희소성(검은색 음영으로 표시된 0 값들)을 가지며, 커널 0(도 9c에서)에 맵핑된다. 커널 0(도 9b에서)은 커널들 중에서 두 번째로 많은 희소성(검은색 음영으로 표시된 0 값들)을 가지며, 커널 0에 맵핑된다(도 9c에서). 반면에, 커널 3(도 9b에서)은 가장 적은 희소성을 가지며, 커널 7(도 9c에서)에 맵핑될 수 있다. 도 10은 본 개시내용의 양상들에 따른 온도 문제들을 완화하기 위해 적용된 예시적인 희소성 기반 재맵 핑을 예시하는 블록도이다. 한 세트의 타일들이 초기에 한 세트의 컴퓨팅 유닛들에 대한 가상 ID들에 맵핑된다 (1002 참조). 맵핑된 타일들 각각의 희소성 퍼센티지가 또한 제공된다. 컴퓨팅 유닛들에 대한 가상 ID들은 맵 핑된 타일의 희소성 퍼센티지에 기초하여 소팅된다. 비록, 이 예에서는, 타일들이 가상 ID들 또는 가상 컴퓨팅 유닛들에 맵핑되지만, 본 개시내용이 이것에 제한되지 않으며 본 개시내용의 양상들은 가상 컴퓨팅 유닛 들의 사용 없이 구현될 수 있다. 또한, 뉴럴 네트워크는 물리적 컴퓨팅 유닛들의 상태 및 동작에 관한 메트릭들을 결정하고 모니터링할 수 있다. 이 예시적인 양상에서는, 표에 도시된 바와 같이, 각각의 컴퓨팅 유닛의 온도가 모니터링된다. 물론, 이전에 개시된 바와 같이, 다른 메트릭들 또는 통계 정보가 결정되고 및/또는 모니터링될 수 있다. 도 10에 도시된 바와 같이, 물리적 컴퓨팅 유닛들 간의 온도 분포는 불균일하다. 특히, 물리적 컴퓨팅 유닛 2는 섭씨 103도의 온도에서 동작하고, 물리적 컴퓨팅 유닛 7은 섭씨 91도의 온도에서 동작한다. 컴퓨팅 유닛들 사 이의 온도 분포를 개선하기 위해, 온도들이 가장 높은 것에서 가장 낮은 것으로 소팅될 수 있으며, 타일들은 희 소성 퍼센티지에 기초하여 물리적 컴퓨팅 유닛들에 맵핑될 수 있다(표 참조). 이와 같이, 가장 높은 온 도를 갖는 컴퓨팅 유닛들(예를 들어, 섭씨 103도에서의 컴퓨팅 유닛 2)이 가장 높은 희소성 퍼센티지를 갖는 타 일들(예를 들어, 88% 희소성에서의 가상 ID 2)에 맵핑되고, 가장 낮은 온도를 갖는 컴퓨팅 유닛들(예를 들어, 섭씨 91도에서 컴퓨팅 유닛 7)이 가장 낮은 희소성 퍼센티지를 갖는 타일들(예를 들어, 0%의 가상 ID 3)에 맵핑 된다. 이에 따라, SOC의 컴퓨팅 유닛들의 온도들이 감소될 수 있다. 도 11은 본 개시내용의 일 양상에 따른 방법에 대한 흐름도를 예시한다. 블록에서, 방법 는 복수의 컴퓨팅 유닛들을 통해 복수의 커널들과 컨볼빙될 한 세트의 입력 값들을 수신한다. 도 5a에 도시된 바와 같이, 한 세트의 입력 값들(502a-n)은 프로세싱을 위해 입력을 통해 수신될 수 있다. 한 세 트의 입력 값들(502a-n)은, 예를 들어, 이미지를 나타낼 수 있다. 한 세트의 입력 값들(502a-n)은 뉴럴 네트워 크를 통해 프로세싱되어 출력(예를 들어, 이미지 분류)을 도출할 수 있다. 블록에서, 방법은 복수의 컴퓨팅 유닛들 중 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유 닛을 결정한다. 도 5b를 참조하여 논의된 바와 같이, 컴퓨팅 유닛들(예를 들어, 컴퓨팅 유닛(402a-z)) 각각에 서의 또는 그 근처의 온도는 또한, 온도 센서들(예를 들어, 도 1의 센서들)을 통해 검출될 수 있다. 추가 적으로, 뉴럴 네트워크의 동작 전반에 걸쳐 디바이스를 보호하기 위해 컴퓨팅 유닛들에서의 또는 그 근처의 온 도가 연속적으로 모니터링될 수 있다. 엘리먼트는 SOC의 예시적인 컴퓨팅 유닛들(NSP #0-7)의 온도를 그 래픽으로 예시한 차트이다. 엘리먼트에 도시된 바와 같이, 컴퓨팅 유닛(NSP #1)은 가장 높은 온도를 가지 며, SOC 상의 핫스팟이다. 반면에, 온도 센서들(예를 들어, 센서들)은 컴퓨팅 유닛(NSP #7)이 가장 낮은 온도를 갖는 것을 검출하였다. 본 개시내용의 양상들에 따르면, 컴퓨팅 유닛들에 대한 뉴럴 네트워크의 맵핑은 컴퓨팅 유닛들에서 또는 그 근처에서 검출된 온도에 기초하여 결정될 수 있다. 블록에서, 방법은 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 복수의 커 널들을 SOC(system-on-chip)의 복수의 컴퓨팅 유닛들에 맵핑한다. 예를 들어, 도 5b에 도시된 바와 같이, 가장 큰 희소성을 가진 커널(예를 들어, 커널 #3)과 대응하는 입력 부분 또는 볼륨을 컨볼빙하는 연산이, 가장 높은 검출 온도를 갖는 컴퓨팅 유닛(예를 들어, NSP #1)에 할당되거나 맵핑될 수 있다. 그렇게 함으로써, 수행될 컨 볼루션 연산들의 수가 감소되거나 커널 #3이 커널들 중 가장 작을 수 있기 때문에, NSP #1의 워크로드가 감소될 수 있다. 그 이유는 컨볼루션 연산이 0의 가중치 값을 포함하는 경우, 해당 곱셈 및 누산 연산들이 스킵될 수 있기 때문이다. 블록에서, 방법은 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 복수의 커널들 중 가장 희소한 커널과 한 세트의 입력 값들의 컨볼루션을 수행한다. 예를 들어, 도 5b를 참조하여 논의된 바와 같이, 컴퓨팅 유닛(NSP #7)이 가장 낮은 검출 온도를 갖는 것으로 결정되며, 따라서, 가장 작은 희소성을 갖는 커널(예를 들어, 커널 #5)과 대응하는 입력(예를 들어, 552a-n)을 컨볼빙하는 연산을 위한 타일(예를 들어, 570n)이, 가장 낮은 검출 온도를 갖는 컴퓨팅 유닛(예를 들어, NSP #7)에 맵핑되거나 할당될 수 있다. 따라서,컴퓨팅 유닛(NSP #7)은 커널 #5과 대응하는 입력(예를 들어, 552a-n)의 컨볼루션을 수행하여 출력 #5를 생성할 수 있다. 출력 부분들 각각은, 차례로, 원하는 태스크를 수행하기 위해 뉴럴 네트워크의 후속 계층들에 공급될 수 있다. 일 양상에서, 수신 수단, 결정 수단, 맵핑 수단 및/또는 컨볼루션을 수행하는 수단은 CPU, CPU와 연관된 프로그램 메모리, 전용 메모리 블록, 완전 연결 계층들, 및/또는 언급된 기능들을 수행하도록 구성된 라우팅 연결 프로세싱 유닛일 수 있다. 다른 구성에서, 전술한 수단들은, 전술한 수단 들에 의해 언급된 기능들을 수행하도록 구성되는 임의의 모듈 또는 임의의 장치일 수 있다. 위에서 설명된 방법들의 다양한 동작들은 대응하는 기능들을 수행할 수 있는 임의의 적절한 수단에 의해 수행될 수 있다. 이 수단은, 회로, ASIC(application specific integrated circuit) 또는 프로세서를 포함하 는(그러나, 이에 제한되지는 않는) 다양한 하드웨어 및/또는 소프트웨어 컴포넌트(들) 및/또는 모듈(들)을 포함 할 수 있다. 일반적으로, 도면들에 예시된 동작들이 존재하는 경우, 이 동작들은 유사한 넘버링을 가진 상응하 는 대응 수단-및-기능 컴포넌트들을 가질 수 있다. 구현 예들이 다음의 넘버링된 조항들에서 제공된다: 1. 인공 뉴럴 네트워크(artificial neural network)를 위한 방법은, 복수의 컴퓨팅 유닛들을 통해 복수의 커널들과 컨볼빙될 한 세트의 입력 값들을 수신하는 단계; 복수의 컴퓨팅 유닛들 중 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛을 결정하는 단계; 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 SOC의 복수의 컴퓨팅 유닛에 복수의 커널을 맵핑 하는 단계; 및 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 복수의 커널들 중 가장 희소한 커널과 한 세트의 입력 값 들의 컨볼루션을 수행하는 단계를 포함한다. 2. 조항 1의 방법에 있어서, 열적 스트레스의 양에 기초하여 컴퓨팅 유닛들의 제1 순서를 그리고 희소성 메트릭에 기초하여 커널들의 제2 순서를 결정하는 단계; 및 제1 순서 및 제2 순서에 기초하여 복수의 커널들 각각을 복수의 컴퓨팅 유닛들 중 하나에 할당하는 단계를 더 포함한다. 3. 조항 2의 방법에 있어서, 제1 순서는 계층 내 복수의 커널들이 희소성 감소에 따라 배열되는 것을 포 함하고, 제2 순서는 복수의 컴퓨팅 유닛들이 열적 스트레스 증가에 따라 배열되는 것을 포함하고; 그리고 방법은, 최소 희소한 커널(least sparse kernel)이 가장 적게 열적 스트레스를 받는 컴퓨팅 유닛에 할당되고 그 리고 가장 희소한 커널이 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛에 할당되도록 하는 순서로, 복수의 컴퓨 팅 유닛들 중 하나에 커널들 각각을 할당하는 단계를 더 포함한다. 4. 조항 1의 방법에 있어서, SOC의 복수의 컴퓨팅 유닛들 각각의 온도 또는 전류 소모 중 적어도 하나를 검출하는 단계를 더 포함하고, 맵핑하는 단계는 온도 또는 전류 소모에 기초한다. 5. 조항 4의 방법에 있어서, 맵핑하는 단계는 복수의 컴퓨팅 유닛들 중 임계치를 초과하는 온도 또는 전 류를 갖는 컴퓨팅 유닛들에 대해 수행된다. 6. 조항 1의 방법에 있어서, 복수의 커널들 각각의 커널의 통계 정보를 컴퓨팅하는 단계를 더 포함하고, 맵핑하는 단계는 통계 정보에 추가로 기초한다. 7. 조항 1 내지 조항 6 중 어느 한 조항의 방법에 있어서, 맵핑하는 단계는 런타임 동안 SOC의 복수의 컴퓨팅 유닛들에 복수의 커널들을 동적으로 할당하는 단계를 포함한다. 8. 인공 뉴럴 네트워크를 위한 장치는, 메모리; 및 메모리에 커플링된 적어도 하나의 프로세서를 포함하고, 적어도 하나의 프로세서는, 복수의 컴퓨팅 유닛들을 통해 복수의 커널들과 컨볼빙될 한 세트의 입력 값들을 수신하고; 복수의 컴퓨팅 유닛들 중 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛을 결정하고; 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 SOC(system-on-chip)의 복수의 컴퓨팅 유닛들에 복수의 커널들을 맵핑하며; 그리고 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 복수의 커널들 중 가장 희소한 커널과 한 세트의 입력 값 들의 컨볼루션을 수행하도록 구성된다. 9. 조항 8의 장치에 있어서, 적어도 하나의 프로세서는, 열적 스트레스의 양에 기초하여 컴퓨팅 유닛들의 제1 순서를 그리고 희소성 메트릭에 기초하여 커널들의 제2 순 서를 결정하고; 그리고 제1 순서 및 제2 순서에 기초하여 복수의 커널들 각각을 복수의 컴퓨팅 유닛들 중 하나에 할당하도록 추가로 구 성된다. 10. 조항 9의 장치에 있어서, 제1 순서는 계층 내 복수의 커널들이 희소성 감소에 따라 배열되는 것을 포함하고, 제2 순서는 복수의 컴퓨팅 유닛들이 열적 스트레스 증가에 따라 배열되는 것을 포함하고, 그리고 적어도 하나의 프로세서는, 최소 희소한 커널이 가장 적게 열적 스트레스를 받는 컴퓨팅 유닛에 할당되고 그리 고 가장 희소한 커널이 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛에 할당되도록 하는 순서로, 복수의 컴퓨팅 유닛들 중 하나에 커널들 각각을 할당하도록 추가로 구성된다. 11. 조항 8의 장치에 있어서, 적어도 하나의 프로세서는, SOC의 복수의 컴퓨팅 유닛들 각각의 온도 또는 전류 소모 중 적어도 하나를 검출하고, 그리고 온도 또는 전류 소모에 기초하여 복수의 커널들을 복수의 컴퓨팅 유닛들에 맵핑하도록 추가로 구성된다. 12. 조항 11의 장치에 있어서, 적어도 하나의 프로세서는, 복수의 컴퓨팅 유닛들 중 임계치를 초과하는 온도 또는 전류를 갖는 컴퓨팅 유닛들에 복수의 커널들의 커널들을 할당하도록 추가로 구성된다. 13. 조항 8의 장치에 있어서, 적어도 하나의 프로세서는, 복수의 커널들 각각의 커널의 통계 정보를 컴퓨팅하고; 그리고 통계 정보에 기초하여 복수의 커널들을 복수의 컴퓨팅 유닛들에 맵핑하도록 추가로 구성된다. 14. 조항 8 내지 조항 13 중 어느 한 조항의 장치에 있어서, 적어도 하나의 프로세서는, 런타임 동안 SOC의 복수의 컴퓨팅 유닛들에 복수의 커널들을 동적으로 할당하도록 추가로 구성된다. 15. 인공 뉴럴 네트워크를 위한 장치는, 복수의 컴퓨팅 유닛들을 통해 복수의 커널들과 컨볼빙될 한 세트의 입력 값들을 수신하기 위한 수단; 복수의 컴퓨팅 유닛들 중 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛을 결정하기 위한 수단; 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 SOC(system-on-chip)의 복수의 컴퓨팅 유닛들에 복수의 커널들을 맵핑하기 위한 수단; 및 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 복수의 커널들 중 가장 희소한 커널과 한 세트의 입력 값 들의 컨볼루션을 수행하기 위한 수단을 포함한다. 16. 조항 15의 장치에 있어서, 열적 스트레스의 양에 기초하여 컴퓨팅 유닛들의 제1 순서를 그리고 희소성 메트릭에 기초하여 커널들의 제2 순 서를 결정하기 위한 수단; 및 제1 순서 및 제2 순서에 기초하여 복수의 커널들 각각을 복수의 컴퓨팅 유닛들 중 하나에 할당하기 위한 수단을 더 포함한다. 17. 조항 16의 장치에 있어서, 제1 순서는 계층 내 복수의 커널들이 희소성 감소에 따라 배열되는 것을 포함하고, 제2 순서는 복수의 컴퓨팅 유닛들이 열적 스트레스 증가에 따라 배열되는 것을 포함하고; 그리고 장 치는, 최소 희소한 커널이 가장 적게 열적 스트레스를 받는 컴퓨팅 유닛에 할당되고 그리고 가장 희소한 커널이 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛에 할당되도록 하는 순서로, 복수의 컴퓨팅 유닛들 중 하나에 커널 들 각각을 할당하기 위한 수단을 더 포함한다. 18. 조항 15의 장치에 있어서, SOC의 복수의 컴퓨팅 유닛들 각각의 온도 또는 전류 소모 중 적어도 하나를 검출하기 위한 수단; 및 온도 또는 전류 소모에 기초하여 복수의 커널들을 복수의 컴퓨팅 유닛들에 할당하기 위한 수단을 더 포함한다. 19. 조항 15의 장치에 있어서, 복수의 컴퓨팅 유닛들 중 임계치를 초과하는 온도 또는 전류를 갖는 컴퓨 팅 유닛들에 복수의 커널들의 커널들을 할당하기 위한 수단을 더 포함한다. 20. 조항 15의 장치에 있어서, 복수의 커널들 각각의 커널의 통계 정보를 컴퓨팅하기 위한 수단, 및 통 계 정보에 기초하여 복수의 컴퓨팅 유닛들의 컴퓨팅 유닛들에 복수의 커널들의 커널들을 할당하기 위한 수단을 더 포함한다. 21. 조항 15 내지 조항 20 중 어느 한 조항의 장치에 있어서, 런타임 동안 SOC의 복수의 컴퓨팅 유닛들 에 복수의 커널들을 동적으로 할당하기 위한 수단을 더 포함한다. 22. 비일시적 컴퓨터 판독 가능 매체에는 인공 뉴럴 네트워크를 위한 프로그램 코드가 인코딩되어 있고, 프로그램 코드는 프로세서에 의해 실행되고, 그리고 복수의 컴퓨팅 유닛들을 통해 복수의 커널들과 컨볼빙될 한 세트의 입력 값들을 수신하기 위한 프로그램 코드; 복수의 컴퓨팅 유닛들 중 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛을 결정하기 위한 프로그램 코드; 적어도 하나의 열적 스트레스를 받는 컴퓨팅 유닛에 기초하여 SOC(system-on-chip)의 복수의 컴퓨팅 유닛들에 복수의 커널들을 맵핑하기 위한 프로그램 코드; 및 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛 상에서 복수의 커널들 중 가장 희소한 커널과 한 세트의 입력 값 들의 컨볼루션을 수행하기 위한 프로그램 코드를 포함한다. 23. 조항 22의 비일시적 컴퓨터 판독 가능 매체에 있어서, 열적 스트레스의 양에 기초하여 컴퓨팅 유닛들의 제1 순서를 그리고 희소성 메트릭에 기초하여 커널들의 제2 순 서를 결정하기 위한 프로그램 코드; 및 제1 순서 및 제2 순서에 기초하여 복수의 커널들 각각을 복수의 컴퓨팅 유닛들 중 하나에 할당하기 위한 프로그 램 코드를 더 포함한다. 24. 조항 23의 비일시적 컴퓨터 판독 가능 매체에 있어서, 제1 순서는 계층 내 복수의 커널들이 희소성 감소에 따라 배열되는 것을 포함하고, 제2 순서는 복수의 컴퓨팅 유닛들이 열적 스트레스 증가에 따라 배열되는 것을 포함하고; 그리고 비일시적 컴퓨터 판독 가능 저장 매체는, 최소 희소한 커널이 가장 적게 열적 스트레스를 받는 컴퓨팅 유닛에 할당되고 그리고 가장 희소한 커널이 가장 많이 열적 스트레스를 받는 컴퓨팅 유닛에 할당되도록 하는 순서로, 복수의 컴퓨팅 유닛들 중 하나에 커널들 각각을 할당하기 위한 프로그램 코드를 더 포함한다. 25. 조항 22의 비일시적 컴퓨터 판독 가능 매체에 있어서, SOC의 복수의 컴퓨팅 유닛들 각각의 온도 또는 전류 소모 중 적어도 하나를 검출하기 위한 프로그램 코드; 및 온도 또는 전류 소모에 기초하여 복수의 커널들을 복수의 컴퓨팅 유닛들에 맵핑하기 위한 프로그램 코드를 더 포함한다. 26. 조항 25의 비일시적 컴퓨터 판독 가능 매체에 있어서, 복수의 컴퓨팅 유닛들 중 임계치를 초과하는 온도 또는 전류를 갖는 컴퓨팅 유닛들에 복수의 커널들의 커널들을 할당하기 위한 프로그램 코드를 더 포함한다. 27. 조항 22의 비일시적 컴퓨터 판독 가능 매체에 있어서, 복수의 커널들 각각의 커널의 통계 정보를 컴퓨팅하기 위한 프로그램 코드; 및 통계 정보에 기초하여 복수의 커널들을 복수의 컴퓨팅 유닛들에 맵핑하기 위한 프로그램 코드를 더 포함한다. 28. 조항 22 내지 조항 27 중 어느 한 조항의 비일시적 컴퓨터 판독 가능 매체에 있어서, 런타임 동안 SOC의 복수의 컴퓨팅 유닛들에 복수의 커널들을 동적으로 할당하기 위한 프로그램 코드를 더 포함한다. 본 명세서에서 사용되는, \"결정하는\"이라는 용어는 광범위하게 다양한 액션들을 포함한다. 예컨대, \"결 정하는\"은 계산, 컴퓨팅, 프로세싱, 도출, 조사, 룩업(예컨대, 표, 데이터베이스 또는 다른 데이터 구조에서의 룩업), 확인 등을 포함할 수 있다. 추가적으로, \"결정하는\" 것은 수신하는 것(예컨대, 정보를 수신하는 것), 액세스하는 것(예컨대, 메모리의 데이터에 액세스하는 것) 등을 포함할 수 있다. 또한, \"결정하는\" 것은 해결 하는 것, 선택하는 것, 선정하는 것, 확립하는 것 등을 포함할 수 있다. 본 명세서에서 사용되는, 아이템들의 리스트 \"중 적어도 하나\"를 지칭하는 문구는 단일 멤버들을 포함하 여, 이러한 아이템들의 임의의 조합을 지칭한다. 일 예로서, \"a, b, 또는 c 중 적어도 하나\"는 a, b, c, a-b, a-c, b-c, 및 a-b-c를 커버하도록 의도된다. 본 개시내용과 관련하여 설명된 다양한 예시적인 로직 블록들, 모듈들 및 회로들은 범용 프로세서, DSP(digital signal processor), ASIC(application specific integrated circuit), FPGA(field programmable gate array signal) 또는 다른 PLD(programmable logic device), 이산 게이트 또는 트랜지스터 로직, 이산 하 드웨어 컴포넌트들, 또는 설명된 기능들을 수행하도록 설계된 이들의 임의의 조합으로 구현되거나 이들에 의해 수행될 수 있다. 범용 프로세서는 마이크로프로세서일 수 있지만, 대안적으로 프로세서는 임의의 상업적으로 입수가능한 프로세서, 제어기, 마이크로제어기, 또는 상태 머신일 수 있다. 프로세서는 또한, 컴퓨팅 디바이스 들의 조합, 예컨대, DSP와 마이크로프로세서의 조합, 복수의 마이크로프로세서들, DSP 코어와 결합된 하나 이상 의 마이크로프로세서들, 또는 임의의 다른 그러한 구성으로서 구현될 수 있다. 본 개시내용과 관련하여 설명된 알고리즘 또는 방법의 단계들은 직접 하드웨어로, 프로세서에 의해 실행"}
{"patent_id": "10-2023-7028493", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "되는 소프트웨어 모듈로, 또는 이 둘의 조합으로 구현될 수 있다. 소프트웨어 모듈은 당해 기술분야에 알려진 임의의 형태의 저장 매체에 상주할 수 있다. 사용될 수 있는 저장 매체들의 일부 예들은 RAM(random access memory), ROM(read only memory), 플래시 메모리, EPROM(erasable programmable read-only memory), EEPROM(electrically erasable programmable read-only memory), 레지스터들, 하드 디스크, 착탈형 디스크, CD-ROM 등을 포함한다. 소프트웨어 모듈은 단일 명령 또는 많은 명령들을 포함할 수 있으며, 여러 상이한 코드 세그먼트들에 걸쳐, 상이한 프로그램들 사이에, 그리고 다수의 저장 매체들에 걸쳐 분산될 수 있다. 저장 매체 는, 프로세서가 저장 매체로부터 정보를 판독하고 저장 매체에 정보를 기록할 수 있도록 프로세서에 커플링될 수 있다. 대안적으로, 저장 매체는 프로세서에 통합될 수 있다. 본 명세서에 개시된 방법들은 설명된 방법을 달성하기 위한 하나 이상의 단계들 또는 액션들을 포함한다. 방법 단계들 및/또는 액션들은 청구항들의 범위를 벗어나지 않으면서 서로 상호 교환될 수 있다. 다시 말해서, 단계들 또는 액션들의 특정 순서가 특정되지 않으면, 특정 단계들 및/또는 액션들의 순서 및/또는 사용은 청구항들의 범위를 벗어나지 않으면서 수정될 수 있다. 설명된 기능들은 하드웨어, 소프트웨어, 펌웨어, 또는 이들의 임의의 조합으로 구현될 수 있다. 하드웨 어로 구현되는 경우, 예시적인 하드웨어 구성은 디바이스의 프로세싱 시스템을 포함할 수 있다. 프로세싱 시스 템은 버스 아키텍처로 구현될 수 있다. 버스는 프로세싱 시스템의 특정 애플리케이션 및 전체 설계 제약들에 따라 임의의 수의 상호연결 버스들 및 브리지들을 포함할 수 있다. 버스는 프로세서, 기계 판독 가능 매체들, 및 버스 인터페이스를 포함하는 다양한 회로들을 함께 링크시킬 수 있다. 버스 인터페이스는 다른 것들 중에서 도, 네트워크 어댑터를 버스를 통해 프로세싱 시스템에 연결하는 데 사용될 수 있다. 네트워크 어댑터는 신호 프로세싱 기능들을 구현하기 위해 사용될 수 있다. 특정 양상들의 경우, 사용자 인터페이스(예컨대, 키패드, 디스플레이, 마우스, 조이스틱 등)가 또한 버스에 연결될 수 있다. 버스는 또한, 타이밍 소스들, 주변장치들,"}
{"patent_id": "10-2023-7028493", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "전압 조절기들, 전력 관리 회로들 등과 같은 다양한 다른 회로들을 링크시킬 수 있으며, 이들은 당해 기술분야 에 잘 알려져 있고 따라서 더 이상 설명되지 않을 것이다. 프로세서는 기계 판독 가능 매체들에 저장된 소프트웨어의 실행을 포함하여, 버스의 관리 및 일반적인 프로세싱을 담당할 수 있다. 프로세서는 하나 이상의 범용 및/또는 특수 목적용 프로세서들로 구현될 수 있다. 예들은 마이크로프로세서들, 마이크로제어기들, DSP 프로세서들, 및 소프트웨어를 실행할 수 있는 다른 회로부 를 포함한다. 소프트웨어는 소프트웨어, 펌웨어, 미들웨어, 마이크로코드, 하드웨어 기술 언어, 또는 다른 식 으로 지칭되든지 간에, 명령들, 데이터, 또는 이들의 임의의 조합을 의미하는 것으로 광범위하게 해석될 것이다. 기계 판독 가능 매체들은, RAM(random access memory), 플래시 메모리, ROM(read only memory), PROM(programmable read-only memory), EPROM(erasable programmable read-only memory), EEPROM(electrically erasable programmable Read-only memory), 레지스터들, 자기 디스크들, 광학 디스크들, 하드 드라이브들, 또는 임의의 다른 적합한 저장 매체, 또는 이들의 임의의 조합을 예로서 포함할 수 있다. 기계 판독 가능 매체들은 컴퓨터 프로그램 제품으로 구현될 수 있다. 컴퓨터 프로그램 제품은 패키징 재료들을 포함할 수 있다. 하드웨어 구현에서, 기계 판독 가능 매체들은 프로세서와 별개인 프로세싱 시스템의 일부일 수 있다. 그러나, 당업자들이 용이하게 인식할 바와 같이, 기계 판독 가능 매체들 또는 이들의 임의의 부분은 프로세싱 시스템 외부에 있을 수 있다. 일 예로서, 기계 판독 가능 매체들은 송신 라인, 데이터에 의해 변조된 캐리어 파, 및/또는 디바이스와 별개인 컴퓨터 제품을 포함할 수 있으며, 이들 전부는 버스 인터페이스를 통해 프로세 서에 의해 액세스될 수 있다. 대안적으로 또는 추가하여, 기계 판독 가능 매체들 또는 이들의 임의의 부분은 캐시 및/또는 일반 레지스터 파일들에서 흔히 있듯이, 프로세서에 통합될 수 있다. 논의된 다양한 컴포넌트들 이 로컬 컴포넌트와 같이 특정 위치를 갖는 것으로 설명될 수 있지만, 이들은 또한, 특정 컴포넌트들이 분산 컴 퓨팅 시스템의 일부로서 구성되는 것과 같이 다양한 방식들로 구성될 수 있다. 프로세싱 시스템은, 프로세서 기능성을 제공하는 하나 이상의 마이크로프로세서들 및 기계 판독 가능 매 체들의 적어도 일부를 제공하는 외부 메모리를 갖는 범용 프로세싱 시스템으로서 구성될 수 있으며, 이들 전부 는 외부 버스 아키텍처를 통해 다른 지원 회로부와 함께 링크된다. 대안적으로, 프로세싱 시스템은 본 명세서 에서 설명된 뉴론 모델들 및 뉴럴 시스템들의 모델들을 구현하기 위한 하나 이상의 뉴로모픽(neuromorphic) 프 로세서들을 포함할 수 있다. 다른 대안으로서, 프로세싱 시스템은 프로세서, 버스 인터페이스, 사용자 인터페 이스, 지원 회로부, 및 단일 칩에 통합된 기계 판독 가능 매체들의 적어도 일부를 갖는 ASIC(application specific integrated circuit)로 구현될 수 있거나, 또는 하나 이상의 FPGA(field programmable gate array)들, PLD(programmable logic device)들, 프로세서들, 상태 머신들, 게이팅된 로직, 이산 하드웨어 컴포 넌트들, 또는 임의의 다른 적합한 회로부, 또는 본 개시내용 전반에 걸쳐 설명된 다양한 기능성을 수행할 수 있 는 회로들의 임의의 조합으로 구현될 수 있다. 당업자들은 특정 애플리케이션 및 전체 시스템에 부과되는 전체 설계 제약들에 따라 프로세싱 시스템에 대해 설명된 기능성을 어떻게 최상으로 구현할지를 인지할 것이다. 기계 판독 가능 매체들은 다수의 소프트웨어 모듈들을 포함할 수 있다. 소프트웨어 모듈들은 프로세서 에 의해 실행될 때, 프로세싱 시스템으로 하여금 다양한 기능들을 수행하게 하는 명령들을 포함한다. 소프트웨 어 모듈들은 송신 모듈 및 수신 모듈을 포함할 수 있다. 각각의 소프트웨어 모듈은 단일 저장 디바이스에 상주 할 수 있거나, 다수의 저장 디바이스들에 걸쳐 분산될 수 있다. 일 예로서, 소프트웨어 모듈은 트리거링 이벤 트가 발생하는 경우 하드 드라이브로부터 RAM으로 로딩될 수 있다. 소프트웨어 모듈의 실행 동안, 프로세서는 명령들 중 일부를 캐시로 로딩하여 액세스 속도를 높일 수 있다. 그런 다음, 하나 이상의 캐시 라인들이 프로 세서에 의한 실행을 위해 일반적인 레지스터 파일로 로딩될 수 있다. 아래의 소프트웨어 모듈의 기능성을 참조 하면, 그러한 기능성은 그 소프트웨어 모듈로부터의 명령들을 실행할 때 프로세서에 의해 구현된다는 것이 이해 될 것이다. 또한, 본 개시내용의 양상들은 프로세서, 컴퓨터, 기계, 또는 그러한 양상들을 구현하는 다른 시스 템의 기능에 대한 개선들을 야기한다는 것이 인식되어야 한다. 소프트웨어로 구현되는 경우, 기능들은 컴퓨터 판독 가능 매체 상에 하나 이상의 명령들 또는 코드로서 저장되거나 이를 통해 송신될 수 있다. 컴퓨터 판독 가능 매체들은 한 장소에서 다른 장소로 컴퓨터 프로그램 의 전달을 가능하게 하는 임의의 매체를 포함하는 통신 매체들과 컴퓨터 저장 매체들 둘 모두를 포함한다. 저 장 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 이용가능한 매체일 수 있다. 제한이 아닌 예로서, 그러한 컴퓨터 판독 가능 매체들은, RAM, ROM, EEPROM, CD-ROM 또는 다른 광학 디스크 저장소, 자기 디스크 저장소 또 는 다른 자기 저장 디바이스들, 또는 원하는 프로그램 코드를 명령들 또는 데이터 구조들의 형태로 반송 또는 저장하는 데 사용될 수 있고 컴퓨터에 의해 액세스될 수 있는 임의의 다른 매체를 포함할 수 있다. 추가적으로, 임의의 연결수단(connection)이 컴퓨터 판독 가능 매체로 적절히 지칭된다. 예컨대, 소프트웨어가 동축 케이블, 광섬유 케이블, 꼬임 쌍선(twisted pair), DSL(digital subscriber line), 또는 IR(infrared), 라디오, 및 마이크로파와 같은 무선 기술들을 사용하여 웹사이트, 서버, 또는 다른 원격 소스로부터 송신된다면, 동축 케이블, 광섬유 케이블, 꼬임 쌍선, DSL, 또는 적외선, 라디오, 및 마이크로파와 같은 무선 기술들이 매체의 정의에 포함된다. 본 명세서에서 사용되는 디스크(disk 및 disc)는 CD(compact disc), 레이저 디스크(laser disc), 광 디스크(optical disc), DVD(digital versatile disc), 플로피 디스크(floppy disk), 및 블루레이 디스크(Blu-ray® disc)를 포함하고, 여기서 디스크(disk)들은 일반적으로 데이터를 자기적으로 재 생하는 한편, 디스크(disc)들은 레이저들을 이용하여 데이터를 광학적으로 재생한다. 따라서, 일부 양상들에서, 컴퓨터 판독 가능 매체들은 비-일시적 컴퓨터 판독 가능 매체들(예컨대, 유형적(tangible) 매체 들)을 포함할 수 있다. 게다가, 다른 양상들의 경우, 컴퓨터 판독 가능 매체들은 일시적 컴퓨터 판독 가능 매 체들(예컨대, 신호)을 포함할 수 있다. 상기의 것들의 조합들이 또한 컴퓨터 판독 가능 매체들의 범위 내에 포함되어야 한다. 따라서, 특정 양상들은 제시된 동작들을 수행하기 위한 컴퓨터 프로그램 제품을 포함할 수 있다. 예컨 대, 그러한 컴퓨터 프로그램 제품은 명령들이 저장(및/또는 인코딩)된 컴퓨터 판독 가능 매체를 포함할 수 있고, 명령들은 설명된 동작들을 수행하도록 하나 이상의 프로세서들에 의해 실행가능하다. 특정 양상들의 경 우, 컴퓨터 프로그램 제품은 패키징 재료를 포함할 수 있다. 추가로, 본 명세서에서 설명된 방법들 및 기법들을 수행하기 위한 모듈들 및/또는 다른 적합한 수단은 적용가능한 경우에 사용자 단말 및/또는 기지국에 의해 다운로드될 수 있고 그리고/또는 다른 방식으로 획득될 수 있다는 것이 인식되어야 한다. 예컨대, 그러한 디바이스는 서버에 커플링되어, 본 명세서에서 설명된 방법 들을 수행하기 위한 수단의 전달을 가능하게 할 수 있다. 대안적으로, 본 명세서에서 설명된 다양한 방법들은, 사용자 단말 및/또는 기지국이 저장 수단(예컨대, RAM, ROM, 물리적 저장 매체, 이를테면, CD(compact disc) 또 는 플로피 디스크 등)을 디바이스에 커플링 또는 제공할 때 다양한 방법들을 획득할 수 있도록, 이러한 저장 수 단을 통해 제공될 수 있다. 또한, 본 명세서에서 설명된 방법들 및 기법들을 디바이스에 제공하기 위한 임의의 다른 적합한 기법이 활용될 수 있다. 청구항들이 위에서 예시된 정확한 구성 및 컴포넌트들로 제한되지 않는다는 것이 이해되어야 한다. 청 구항들의 범위를 벗어나지 않으면서, 위에서 설명된 방법들 및 장치의 구성, 동작, 및 세부사항들에서 다양한 수정들, 변경들, 및 변형들이 이루어질 수 있다."}
{"patent_id": "10-2023-7028493", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 개시내용의 특징들, 본질 및 이점들은 도면들과 함께 취해질 때 아래에서 제시되는 상세한 설명으로 부터 더 자명해질 수 있으며, 도면들에서 유사한 참조 부호들은 전반에 걸쳐 대응하게 식별한다. 도 1은 본 개시내용의 특정 양상들에 따른 범용 프로세서를 포함하는 SOC(system-on-chip)를 사용하여 뉴럴 네트워크를 설계하는 예시적인 구현을 예시한다. 도 2a, 도 2b 및 도 2c는 본 개시내용의 양상들에 따른 뉴럴 네트워크를 예시하는 도면들이다. 도 2d는 본 개시내용의 양상들에 따른 예시적인 DCN(deep convolutional network)를 예시하는 도면이다. 도 3은 본 개시내용의 양상들에 따른 예시적인 DCN(deep convolutional network)를 예시하는 블록도이 다. 도 4는 본 개시내용의 양상들에 따른 예시적인 SOC(system-on-chip)을 예시하는 블록도이다. 도 5a 및 도 5b는 본 개시내용의 양상들에 따른 SOC(system-on-chip)의 컴퓨팅 유닛들에 뉴럴 네트워크 파티션들을 재맵핑하는 예들을 예시한다. 도 6은 본 개시내용의 양상들에 따른 온도 밸런싱 및 핫스팟 감소를 나타내는 한 쌍의 예시적인 히트맵 (heat map)들을 예시한다. 도 7은 본 개시내용의 양상들에 따른 뉴럴 네트워크의 맵핑 프로세싱을 위한 아키텍처를 예시하는 블록 도이다. 도 8은 본 개시내용의 양상들에 따른 뉴럴 네트워크 컨테이너 파일을 예시하는 블록도이다. 도 9a 내지 도 9c는 본 개시내용의 양상들에 따른 희소성 기반 재맵핑(sparsity based remapping)을 예 시하는 도면들의 세트이다. 도 10은 본 개시내용의 양상들에 따른 온도 문제들을 완화하기 위해 적용되는 예시적인 희소성 기반 재 맵핑을 예시하는 블록도이다. 도 11은 본 개시내용의 일 양상에 따른 방법에 대한 흐름도를 예시한다."}
