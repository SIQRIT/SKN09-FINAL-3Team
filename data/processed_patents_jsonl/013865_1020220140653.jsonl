{"patent_id": "10-2022-0140653", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0126631", "출원번호": "10-2022-0140653", "발명의 명칭": "임베디드 장치에서 실행될 신경망 모델 최적화 방법, 신경망 모델 최적화 장치, 및 신경망 모", "출원인": "주식회사 에너자이", "발명자": "이현재"}}
{"patent_id": "10-2022-0140653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "학습이 완료된 신경망 모델의 실행 데이터에 기초하여 신경망 모델에 대한 최적화를 수행하는 신경망 모델 최적화 장치가, 신경망 모델이 구동될 임베디드 장치의 컴퓨팅 환경을 고려하여 신경망 모델을 최적화하는 방법에있어서, 학습이 완료된 신경망 모델의 실행 데이터를 획득하는 단계-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망 모델을 구성하는 연산 데이터(operation data), 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-; 상기 신경망 모델의 실행 데이터에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계; 및상기 인스트럭션 정보에 기초하여 상기 신경망 모델이 구동될 임베디드 장치에 대한 최적화를 수행하고 최적 코드 정보를 획득하는 단계;를 포함하되,상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계는, 상기 실행 데이터에 기초하여 방향성 비사이클 그래프(DAG)를 생성하는 단계; 상기 방향성 비사이클 그래프의 구조를 고려하여 연산들의 실행 순서를 결정하는 단계; 상기 결정된 실행 순서에 기초하여 제1 메모리 공간 맵을 획득하고, 상기 제1 메모리 공간 맵에 기초하여 메모리 할당과 관련된 최적화를 수행하는 단계; 및최적화 수행 결과에 기초하여 메모리 주소와 관련된 인스트럭션을 생성하는 단계;를 더 포함하되, 상기 연산들의 실행 순서를 결정하는 단계는, 상기 방향성 비사이클 그래프의 제1 브랜치에 포함된 연산들을 순차적으로 실행한 이후에 상기 방향성 비사이클그래프의 제2 브랜치에 포함된 연산들을 실행하도록 실행 순서를 결정하는 단계;를 더 포함하는, 신경망 모델 최적화 방법."}
{"patent_id": "10-2022-0140653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계는, 미리 정해진 기준 연산 패턴에 기초하여 상기 기준 연산 패턴에 대응되는 상기 방향성 비사이클 그래프의 대상연산 패턴을 검출하고, 상기 대상 연산 패턴에 포함된 제1 대상 연산과 제2 대상 연산을 병합하는 단계;를 더포함하는, 신경망 모델 최적화 방법."}
{"patent_id": "10-2022-0140653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서, 상기 제1 대상 연산과 상기 제2 대상 연산을 병합하는 단계는, 미리 정해진 기준 연산 패턴 정보를 획득하는 단계-상기 기준 연산 패턴 정보는 제1 연산 및 상기 제1 연산과연계된 제2 연산을 포함함-; 공개특허 10-2023-0126631-3-상기 기준 연산 패턴 정보에 기초하여 상기 방향성 비사이클 그래프로부터 상기 제1 연산에 대응되는 상기 제1대상 연산 및 상기 제2 연산에 대응되는 상기 제2 대상 연산을 검출하는 단계; 및상기 제1 대상 연산과 상기 제2 대상 연산을 병합하고, 상기 병합 결과에 기초하여 커널을 변환하는 단계;를 더포함하는, 신경망 모델 최적화 방법."}
{"patent_id": "10-2022-0140653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서, 상기 메모리 할당과 관련된 최적화를 수행하는 단계는, 상기 결정된 실행 순서 및 상기 연산을 통하여 출력되는 데이터의 크기에 기초하여 상기 제1 메모리 공간 맵을생성하는 단계; 제3 대상 연산으로 입력된 값이 저장되는 제1 메모리 텐서를 상기 제3 대상 연산을 통하여 출력된 값이 저장되는 제2 메모리 텐서로 변경하는 단계; 및 상기 변경 결과에 기초하여 상기 제1 메모리 공간 맵으로부터 제2 메모리 공간 맵을 생성하는 단계;를 더 포함하는, 신경망 모델 최적화 방법."}
{"patent_id": "10-2022-0140653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서, 상기 연산들의 실행 순서를 결정하는 단계는, 상기 방향성 비사이클 그래프의 제1 브랜치(branch)에 포함된 제4 대상 연산에 요구되는 제1 메모리 공간과 상기 방향성 비사이클 그래프의 제2 브랜치에 포함된 제5 대상 연산에 요구되는 제2 메모리 공간을 연산하는단계; 상기 제1 메모리 공간과 상기 제2 메모리 공간을 비교하는 단계; 및 상기 비교 결과에 따라 상기 제4 대상 연산과 상기 제5 대상 연산의 실행 순서를 결정하는 단계;를 더포함하는, 신경망 모델 최적화 방법."}
{"patent_id": "10-2022-0140653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서, 상기 제1 메모리 공간이 상기 제2 메모리 공간보다 큰 경우, 상기 제4 대상 연산의 실행 순서는 상기 제5 대상연산의 실행 순서보다 후순위로 할당하되, 상기 제1 메모리 공간이 상기 제2 메모리 공간보다 작은 경우, 상기 제4 대상 연산의 실행 순서는 상기 제5 대상 연산의 실행 순서보다 선순위로 할당하는,신경망 모델 최적화 방법."}
{"patent_id": "10-2022-0140653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,공개특허 10-2023-0126631-4-상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계는, 상기 신경망 모델의 연산(operation)과 관련된 입력 데이터 및 출력 데이터를 획득하는 단계; 및상기 입력 데이터 및 상기 출력 데이터를 미리 결정된 정수 범위에 해당하는 값으로 조정하는 단계;를 더 포함하는,신경망 모델 최적화 방법."}
{"patent_id": "10-2022-0140653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "컴퓨터에 제1 항 내지 제7 항 중 어느 하나의 항에 따른 방법을 실행시키기 위한 프로그램을 기록한 컴퓨터로읽을 수 있는 기록 매체."}
{"patent_id": "10-2022-0140653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "학습이 완료된 신경망 모델의 실행 데이터 및 신경망 모델이 구동될 임베디드 장치의 컴퓨팅 환경에 기초하여신경망 모델에 대한 최적화를 수행하는 신경망 모델 최적화 장치에 있어서, 학습이 완료된 신경망 모델의 실행 데이터 및 신경망 모델이 구동될 임베디드 장치의 컴퓨팅 환경 정보를 획득하는 송수신부; 및상기 실행 데이터 및 상기 임베디드 장치의 컴퓨팅 환경 정보에 기초하여 상기 신경망 모델에 대한 최적화를 수행하는 프로세서;를 포함하되, 상기 프로세서는, 학습이 완료된 신경망 모델의 실행 데이터-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망모델을 구성하는 연산 데이터(operation data), 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-를획득하고, 상기 신경망 모델의 실행 데이터에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하고, 상기 인스트럭션 정보에 기초하여 상기 신경망 모델이 구동될 임베디드 장치에 대한최적화를 수행하고 최적 코드 정보를 획득하도록 구성되되, 상기 프로세서는, 상기 실행 데이터에 기초하여 방향성 비사이클 그래프(DAG)를 생성하고, 상기 방향성 비사이클 그래프의 구조를고려하여 연산들의 실행 순서를 결정하고, 상기 결정된 실행 순서에 기초하여 제1 메모리 공간 맵을 획득하고,상기 제1 메모리 공간 맵에 기초하여 메모리 할당과 관련된 최적화를 수행하고, 최적화 수행 결과에 기초하여실행 순서 및 메모리 주소와 관련된 인스트럭션을 생성함으로써, 상기 인스트럭션 정보를 획득하도록 구성되되, 상기 프로세서는, 상기 방향성 비사이클 그래프의 제1 브랜치에 포함된 연산들을 순차적으로 실행한 이후에 상기 방향성 비사이클그래프의 제2 브랜치에 포함된 연산들을 실행하도록 실행 순서를 결정하도록 구성된, 신경망 모델 최적화 장치."}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 출원의 일 실시예에 따른 신경망 모델 최적화 방법은, 학습이 완료된 신경망 모델의 실행 데이터를 획득하는 단계-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망 모델을 구성하는 연산 데이터(operation data), 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-; 상기 신경망 모델의 실행 데이터에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계; 및 상기 인스트럭션 정보 에 기초하여 상기 신경망 모델이 구동될 임베디드 장치에 대한 최적화를 수행하고 최적 코드 정보를 획득하는 단 계;를 포함한다."}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 신경망 모델 최적화 방법, 신경망 모델 최적화 장치, 및 신경망 모델 최적화 시스템에 관한 것이다. 구체적으로 본 출원은 임베디드 장치에서 실행될 신경망 모델을 최적화하는 방법, 장치, 및 시스템에 관한 것이다."}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 기술이 발전하면서 다양한 산업 분야에서 활용되는 임베디드 시스템이 내재된 임베디드 장치에 인공지 능 기술이 접목되는 것이 요구되고 있다. 이에 따라 경량화 기술들이 개발되었고 저성능, 저사양인 임베디드 장 치들에 인공지능 기술이 접목될 수 있게 되었다. 특히, 미리 학습이 완료된 인공지능 모델을 임베디드 장치에 최대한 효율적으로 실행시키도록 개발된 소프트웨어인 실행 엔진(Inference Engine) 기술을 통하여 임베디드 장 치에 인공지능 기술이 접목될 수 있게 되었다. 종래의 임베디드용 인공지능 실행 엔진은 임베디드 장치 자체에서 모델의 실행에 대한 정보를 읽고, 모델 실행 순서를 설정하고, 모델 실행에 필요한 메모리를 할당하여 모델을 실행하는 방식을 채택하고 있었다. 다만, 메모 리 공간에 제약이 존재하는 임베디드 장치 자체에서 전술한 모델 실행을 위한 준비 과정들을 실행하는 것은 임 베디드 장치의 하드웨어 환경에 상당한 부담을 야기하였다. 또한, 종래의 임베디드용 인공지능 실행 엔진은 비표준화된 하드웨어 요구사항에 대한 최적화를 위하여는 추가 적인 수작업이 요구된다는 문제가 존재하였다. 구체적으로 종래의 임베디드용 인공지능 실행 엔진은, 각 하드웨 어의 특수 명령어를 이용하거나, For Loop Unrolling 등의 최적화 기술을 이용하기 위하여 엔진 코드를 일일이 수작업으로 수정해야 하는 제약이 존재였다. 즉 종래의 임베디드용 인공지능 실행 엔진은 특정 하드웨어에만 최 적화될 수 있었으며, 특정 하드웨어 이외의 하드웨어에서 실제로 사용하기 위하여는 수작업으로 실행 함수들을 최적화하는 과정이 필수적으로 요구되었다. 이에, 인공지능 모델 및 임베디드 장치의 하드웨어 정보(컴퓨팅 사양)에 기초하여 신경망 모델을 임베디드 장치 에서 최적으로 실행하기 위한 신경망 모델 최적화 방법, 장치 및 시스템의 개발이 요구된다."}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 일 과제는, 임베디드 장치의 하드웨어 정보를 고려하여 신경망 모델을 임베디드 장 치에서 최적으로 실행하기 위한 신경망 모델 최적화 방법, 신경망 모델 최적화 장치 및 신경망 모델 최적화 시 스템을 제공하는 것이다. 본 발명이 해결하고자 하는 과제가 상술한 과제로 제한되는 것은 아니며, 언급되지 아니한 과제들은 본 명세서"}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "및 첨부된 도면으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 출원의 일 실시예에 따른 신경망 모델 최적화 방법은, 학습이 완료된 신경망 모델의 실행 데이터를 획득하는 단계-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망 모델을 구성하는 연산 데이터 (operation data), 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-; 상기 신경망 모델의 실행 데이 터에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계; 상기 인 스트럭션 정보에 기초하여 상기 신경망 모델이 구동될 임베디드 장치에 대한 최적화를 수행하고 최적 코드 정보 를 획득하는 단계; 및 상기 최적 코드 정보를 송신하는 단계;를 포함하되, 상기 신경망 모델의 구조에 대한 최 적화를 수행하고 인스트럭션 정보를 획득하는 단계는, 상기 실행 데이터에 기초하여 방향성 비사이클 그래프 (DAG)를 생성하는 단계; 상기 방향성 비사이클 그래프에 기초하여 연산들의 실행 순서를 결정하는 단계; 미리 정해진 기준 연산 패턴에 기초하여 상기 기준 연산 패턴에 대응되는 상기 방향성 비사이클 그래프의 대상 연산 패턴을 검출하고, 상기 대상 연산 패턴에 포함된 제1 대상 연산과 제2 대상 연산을 병합하는 단계; 상기 결정된 실행 순서에 기초하여 제1 메모리 공간 맵을 획득하고, 상기 제1 메모리 공간 맵에 기초하여 메모리 할당과 관 련된 최적화를 수행하는 단계; 및 최적화 수행 결과에 기초하여 메모리 주소와 관련된 인스트럭션을 생성하는 단계;를 더 포함할 수 있다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 장치는, 학습이 완료된 신경망 모델의 실행 데이터 및 신경망 모델이 구동될 임베디드 장치의 컴퓨팅 환경 정보를 획득하는 송수신부; 및 상기 실행 데이터 및 상기 임베디드 장치의 컴퓨팅 환경 정보에 기초하여 상기 신경망 모델에 대한 최적화를 수행하는 프로세서;를 포함하되, 상기 프로세서는, 학습이 완료된 신경망 모델의 실행 데이터-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망 모델을 구성하는 연산 데이터(operation data), 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-를 획득하고, 상기 신경망 모델의 실행 데이터에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수 행하고 인스트럭션 정보를 획득하고, 상기 인스트럭션 정보에 기초하여 상기 신경망 모델이 구동될 임베디드 장 치에 대한 최적화를 수행하고 최적 코드 정보를 획득하고, 상기 최적 코드 정보를 송신하도록 구성되되, 상기프로세서는, 상기 실행 데이터에 기초하여 방향성 비사이클 그래프(DAG)를 생성하고, 상기 방향성 비사이클 그 래프에 기초하여 연산들의 실행 순서를 결정하고, 미리 정해진 기준 연산 패턴에 기초하여 상기 기준 연산 패턴 에 대응되는 상기 방향성 비사이클 그래프의 대상 연산 패턴을 검출하고, 상기 대상 연산 패턴에 포함된 제1 대 상 연산과 제2 대상 연산을 병합하고, 상기 결정된 실행 순서에 기초하여 제1 메모리 공간 맵을 획득하고, 상기 제1 메모리 공간 맵에 기초하여 메모리 할당과 관련된 최적화를 수행하고, 최적화 수행 결과에 기초하여 메모리 주소와 관련된 인스트럭션을 생성함으로써, 상기 인스트럭션 정보를 획득하도록 구성될 수 있다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 방법은, 학습이 완료된 신경망 모델의 실행 데이터를 획득하는 단계-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망 모델을 구성하는 연산 데이터 (operation data) 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-; 상기 신경망 모델의 실행 데이터 에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계-상기 인스트 럭션 정보는 연산의 유형, 연산의 및 메모리 주소 중 적어도 하나와 관련된 정보를 포함함-; 상기 인스트럭션 정보에 기초하여 상기 신경망 모델이 구동될 임베디드 장치에 대한 최적화를 수행하고 최적 코드 정보를 획득하 는 단계; 및 상기 최적 코드 정보를 송신하는 단계;를 포함하되, 상기 최적 코드 정보를 획득하는 단계는, 상기 임베디드 장치의 컴퓨팅 환경 정보를 획득하는 단계; 강화 학습으로 훈련된 에이전트를 통하여, 인스트럭션 정 보로부터 최적화 파라미터를 획득하는 단계; 및 상기 최적화 파라미터에 기초하여 상기 임베디드 장치에서 이용 될 코드 정보를 생성하는 단계;를 더 포함할 수 있다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 장치는, 학습이 완료된 신경망 모델의 실행 데이터 및 신경망 모델이 구동될 임베디드 장치의 컴퓨팅 환경 정보를 획득하는 송수신부; 및 상기 실행 데이터 및 상기 임베디드 장치의 컴퓨팅 환경 정보에 기초하여 상기 신경망 모델에 대한 최적화를 수행하는 프로세서;를 포함하되, 상기 프로세서는, 학습이 완료된 신경망 모델의 실행 데이터-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망 모델을 구성하는 연산 데이터(operation data), 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-를 획득하고, 상기 신경망 모델의 실행 데이터에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수 행하고 인스트럭션 정보-상기 인스트럭션 정보는 연산의 유형, 및 메모리 주소 중 적어도 하나와 관련된 정보를 포함함-를 획득하고, 상기 인스트럭션 정보에 기초하여 상기 신경망 모델이 구동될 임베디드 장치에 대한 최적 화를 수행하고 최적 코드 정보를 획득하고, 상기 최적 코드 정보를 송신하도록 구성되되, 상기 프로세서는, 상 기 임베디드 장치의 컴퓨팅 환경 정보를 획득하고, 강화 학습으로 훈련된 에이전트를 통하여, 인스트럭션 정보 로부터 최적화 파라미터를 획득하고, 상기 최적화 파라미터에 기초하여 상기 임베디드 장치에서 이용될 코드 정 보를 생성함으로써 상기 최적 코드 정보를 획득하도록 구성될 수 있다."}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "본 발명의 과제의 해결 수단이 상술한 해결 수단들로 제한되는 것은 아니며, 언급되지 아니한 해결 수단들은 본"}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "명세서 및 첨부된 도면으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 출원의 실시예에 따른 신경망 모델 최적화 방법, 신경망 모델 최적화 장치, 및 신경망 모델 최적화 시스템에 의하면, 임베디드 장치의 하드웨어의 비표준화 문제와 기존 인공지능 실행 엔진의 구조적인 제약을 해결하고, 신경망 모델을 임베디드 장치의 하드웨어 플랫폼에 대하여 최적화시킬 수 있다. 본 출원의 실시예에 따른 신경망 모델 최적화 방법, 신경망 모델 최적화 장치, 및 신경망 모델 최적화 시스템에 의하면, 임베디드 장치에서의 신경망 모델의 실행 능력이 향상될 수 있다. 본 출원의 실시예에 따른 신경망 모델 최적화 방법, 신경망 모델 최적화 장치, 및 신경망 모델 최적화 시스템에 의하면, 임베디드 장치에서 신경망 모델을 실행시키기 위하여 필요한 전력 사용량을 감소시킬 수 있다."}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과가 상술한 효과들로 제한되는 것은 아니며, 언급되지 아니한 효과들은 본 명세서 및 첨부된 도면"}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확히 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 출원의 상술한 목적, 특징들 및 장점은 첨부된 도면과 관련된 다음의 상세한 설명을 통해 보다 분명해질 것 이다. 다만, 본 출원은 다양한 변경을 가할 수 있고 여러 가지 실시예들을 가질 수 있는 바, 이하에서는 특정 실시예들을 도면에 예시하고 이를 상세히 설명하고자 한다. 명세서 전체에 걸쳐서 동일한 참조번호들은 원칙적으로 동일한 구성요소들을 나타낸다. 또한, 각 실시예의 도면 에 나타나는 동일한 사상의 범위 내의 기능이 동일한 구성요소는 동일한 참조부호를 사용하여 설명하며, 이에 대한 중복되는 설명은 생략하기로 한다. 본 출원과 관련된 공지 기능 혹은 구성에 대한 구체적인 설명이 본 출원의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 본 명세서의 설명 과정에서 이용되는 숫자(예를 들어, 제1, 제2 등)는 하나의 구성요소를 다른 구성요소와 구분하기 위한 식별기호에 불과하다. 또한, 이하의 실시예에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되 어 부여되거나 혼용되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 이하의 실시예에서, 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 이하의 실시예에서, 포함하다 또는 가지다 등의 용어는 명세서상에 기재된 특징, 또는 구성요소가 존재함을 의 미하는 것이고, 하나 이상의 다른 특징들 또는 구성요소가 부가될 가능성을 미리 배제하는 것은 아니다. 도면에서는 설명의 편의를 위하여 구성 요소들이 그 크기가 과장 또는 축소될 수 있다. 예컨대, 도면에서 나타 난 각 구성의 크기 및 두께는 설명의 편의를 위해 임의로 나타낸 것으로, 본 발명이 반드시 도시된 바에 한정되 지 않는다. 어떤 실시예가 달리 구현 가능한 경우에 특정한 프로세스의 순서는 설명되는 순서와 다르게 수행될 수도 있다. 예를 들어, 연속하여 설명되는 두 프로세스가 실질적으로 동시에 수행될 수도 있고, 설명되는 순서와 반대의 순 서로 진행될 수 있다. 이하의 실시예에서, 구성 요소 등이 연결되었다고 할 때, 구성 요소들이 직접적으로 연결된 경우뿐만 아니라 구 성요소들 중간에 구성 요소들이 개재되어 간접적으로 연결된 경우도 포함한다. 예컨대, 본 명세서에서 구성 요소 등이 전기적으로 연결되었다고 할 때, 구성 요소 등이 직접 전기적으로 연결 된 경우뿐만 아니라, 그 중간에 구성 요소 등이 개재되어 간접적으로 전기적 연결된 경우도 포함한다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 방법은, 학습이 완료된 신경망 모델의 실행 데이터를 획득하는 단계-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망 모델을 구성하는 연산 데이터 (operation data), 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-; 상기 신경망 모델의 실행 데이 터에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계; 상기 인 스트럭션 정보에 기초하여 상기 신경망 모델이 구동될 임베디드 장치에 대한 최적화를 수행하고 최적 코드 정보 를 획득하는 단계; 및 상기 최적 코드 정보를 송신하는 단계;를 포함하되, 상기 신경망 모델의 구조에 대한 최 적화를 수행하고 인스트럭션 정보를 획득하는 단계는, 상기 실행 데이터에 기초하여 방향성 비사이클 그래프 (DAG)를 생성하는 단계; 상기 방향성 비사이클 그래프에 기초하여 연산들의 실행 순서를 결정하는 단계; 미리 정해진 기준 연산 패턴에 기초하여 상기 기준 연산 패턴에 대응되는 상기 방향성 비사이클 그래프의 대상 연산패턴을 검출하고, 상기 대상 연산 패턴에 포함된 제1 대상 연산과 제2 대상 연산을 병합하는 단계; 상기 결정된 실행 순서에 기초하여 제1 메모리 공간 맵을 획득하고, 상기 제1 메모리 공간 맵에 기초하여 메모리 할당과 관 련된 최적화를 수행하는 단계; 및 최적화 수행 결과에 기초하여 메모리 주소와 관련된 인스트럭션을 생성하는 단계;를 더 포함할 수 있다. 본 출원의 일 실시예에 따르면, 상기 제1 대상 연산과 상기 제2 대상 연산을 병합하는 단계는, 미리 정해진 기 준 연산 패턴 정보를 획득하는 단계-상기 기준 연산 패턴 정보는 제1 연산 및 상기 제1 연산과 연계된 제2 연산 을 포함함-; 상기 기준 연산 패턴 정보에 기초하여 상기 방향성 비사이클 그래프로부터 상기 제1 연산에 대응되 는 상기 제1 대상 연산 및 상기 제2 연산에 대응되는 상기 제2 대상 연산을 검출하는 단계; 및 상기 제1 대상 연산과 상기 제2 대상 연산을 병합하고, 상기 병합 결과에 기초하여 커널을 변환하는 단계;를 더 포함할 수 있 다. 본 출원의 일 실시예에 따르면, 상기 메모리 할당과 관련된 최적화를 수행하는 단계는, 상기 결정된 실행 순서 및 상기 연산을 통하여 출력되는 데이터의 크기에 기초하여 상기 제1 메모리 공간 맵을 생성하는 단계; 제3 대 상 연산으로 입력된 값이 저장되는 제1 메모리 텐서를 상기 제3 대상 연산을 통하여 출력된 값이 저장되는 제2 메모리 텐서로 변경하는 단계; 및 상기 변경 결과에 기초하여 상기 제1 메모리 공간 맵으로부터 제2 메모리 공 간 맵을 생성하는 단계;를 더 포함할 수 있다. 본 출원의 일 실시예에 따르면, 상기 연산들의 실행 순서를 결정하는 단계는, 상기 방향성 비사이클 그래프의 제1 브랜치(branch)에 포함된 제4 대상 연산에 요구되는 제1 메모리 공간과 상기 방향성 비사이클 그래프의 제2 브랜치에 포함된 제5 대상 연산에 요구되는 제2 메모리 공간을 연산하는 단계; 상기 제1 메모리 공간과 상기 제 2 메모리 공간을 비교하는 단계; 및 상기 비교 결과에 따라 상기 제4 대상 연산과 상기 제5 대상 연산의 실행 순서를 결정하는 단계;를 더 포함할 수 있다. 본 출원의 일 실시예에 따르면, 상기 제1 메모리 공간이 상기 제2 메모리 공간보다 큰 경우, 상기 제4 대상 연 산의 실행 순서는 상기 제5 대상 연산의 실행 순서보다 후순위로 할당하되, 상기 제1 메모리 공간이 상기 제2 메모리 공간보다 작은 경우, 상기 제4 대상 연산의 실행 순서는 상기 제5 대상 연산의 실행 순서보다 선순위로 할당할 수 있다. 본 출원의 일 실시예에 따르면, 상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하 는 단계는, 상기 신경망 모델의 연산(operation)과 관련된 입력 데이터 및 출력 데이터를 획득하는 단계; 및 상 기 입력 데이터 및 상기 출력 데이터를 미리 결정된 정수 범위에 해당하는 값으로 조정하는 단계;를 더 포함할 수 있다. 본 출원의 일 실시예에 따르면, 상기 신경망 모델 최적화 방법을 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체가 제공될 수 있다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 장치는, 학습이 완료된 신경망 모델의 실행 데이터 및 신경망 모델이 구동될 임베디드 장치의 컴퓨팅 환경 정보를 획득하는 송수신부; 및 상기 실행 데이터 및 상기 임베디드 장치의 컴퓨팅 환경 정보에 기초하여 상기 신경망 모델에 대한 최적화를 수행하는 프로세서;를 포함하되, 상기 프로세서는, 학습이 완료된 신경망 모델의 실행 데이터-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망 모델을 구성하는 연산 데이터(operation data), 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-를 획득하고, 상기 신경망 모델의 실행 데이터에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수 행하고 인스트럭션 정보를 획득하고, 상기 인스트럭션 정보에 기초하여 상기 신경망 모델이 구동될 임베디드 장 치에 대한 최적화를 수행하고 최적 코드 정보를 획득하고, 상기 최적 코드 정보를 송신하도록 구성되되, 상기 프로세서는, 상기 실행 데이터에 기초하여 방향성 비사이클 그래프(DAG)를 생성하고, 상기 방향성 비사이클 그 래프에 기초하여 연산들의 실행 순서를 결정하고, 미리 정해진 기준 연산 패턴에 기초하여 상기 기준 연산 패턴 에 대응되는 상기 방향성 비사이클 그래프의 대상 연산 패턴을 검출하고, 상기 대상 연산 패턴에 포함된 제1 대 상 연산과 제2 대상 연산을 병합하고, 상기 결정된 실행 순서에 기초하여 제1 메모리 공간 맵을 획득하고, 상기 제1 메모리 공간 맵에 기초하여 메모리 할당과 관련된 최적화를 수행하고, 최적화 수행 결과에 기초하여 메모리 주소와 관련된 인스트럭션을 생성함으로써, 상기 인스트럭션 정보를 획득하도록 구성될 수 있다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 방법은, 학습이 완료된 신경망 모델의 실행 데이터를 획득하는 단계-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망 모델을 구성하는 연산 데이터 (operation data) 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-; 상기 신경망 모델의 실행 데이터에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계-상기 인스트 럭션 정보는 연산의 유형, 연산의 및 메모리 주소 중 적어도 하나와 관련된 정보를 포함함-; 상기 인스트럭션 정보에 기초하여 상기 신경망 모델이 구동될 임베디드 장치에 대한 최적화를 수행하고 최적 코드 정보를 획득하 는 단계; 및 상기 최적 코드 정보를 송신하는 단계;를 포함하되, 상기 최적 코드 정보를 획득하는 단계는, 상기 임베디드 장치의 컴퓨팅 환경 정보를 획득하는 단계; 강화 학습으로 훈련된 에이전트를 통하여, 인스트럭션 정 보로부터 최적화 파라미터를 획득하는 단계; 및 상기 최적화 파라미터에 기초하여 상기 임베디드 장치에서 이용 될 코드 정보를 생성하는 단계;를 더 포함할 수 있다. 본 출원의 일 실시예에 따르면, 상기 최적화 파라미터를 획득하는 단계는, 상기 연산에 대응되는 적어도 하나 이상의 연산 유형 정보, 메모리 상태 정보, 및 상기 임베디드 장치의 컴퓨팅 환경 정보 중 적어도 하나를 상기 에이전트에 입력하는 단계; 및 상기 에이전트를 통하여 출력되는 최적화 파라미터를 획득하는 단계;를 더 포함 할 수 있다. 본 출원의 일 실시예에 따르면, 상기 최적화 파라미터는, 상기 연산에 대하여 수행될 알고리즘 유형을 선택하는 파라미터, 상기 연산의 블록 사이즈와 관련된 파라미터, 및 코드의 길이와 관련된 파라미터 중 적어도 하나와 관련될 수 있다. 본 출원의 일 실시예에 따르면, 상기 에이전트는, 초기 규칙에 따라, 타겟 임베디드 장치의 컴퓨팅 환경과 관련 된 타겟 장치 정보, 메모리 상태 정보, 및 연산(operation)에 대응되는 적어도 하나 이상의 알고리즘 유형 정보 에 기초하여 파라미터와 관련된 예측값을 출력하도록 구성되되, 상기 에이전트는, 예측값을 통하여 생성된 코드 의 성능에 대한 평가값이 최대화되도록 상기 초기 규칙이 갱신됨으로써 훈련될 수 있다. 본 출원의 일 실시예에 따르면, 상기 임베디드 장치에서 이용될 코드 정보를 생성하는 단계는, 상기 최적화 파 라미터에 기초하여 상기 인스트럭션 정보에 대응되도록 코드를 생성하는 단계; 및 상기 생성된 코드를 컴파일하 여 바이너리 파일 형태로 변환하는 단계;를 더 포함할 수 있다. 본 출원의 일 실시예에 따르면, 상기 신경망 모델 최적화 방법을 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체가 제공될 수 있다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 장치는, 학습이 완료된 신경망 모델의 실행 데이터 및 신경망 모델이 구동될 임베디드 장치의 컴퓨팅 환경 정보를 획득하는 송수신부; 및 상기 실행 데이터 및 상기 임베디드 장치의 컴퓨팅 환경 정보에 기초하여 상기 신경망 모델에 대한 최적화를 수행하는 프로세서;를 포함하되, 상기 프로세서는, 학습이 완료된 신경망 모델의 실행 데이터-상기 실행 데이터는 상기 신경망 모델의 계층 데이터, 상기 신경망 모델을 구성하는 연산 데이터(operation data), 및 상기 신경망 모델의 파라미터 중 적어도 하나를 포함함-를 획득하고, 상기 신경망 모델의 실행 데이터에 기초하여 상기 신경망 모델의 구조에 대한 최적화를 수 행하고 인스트럭션 정보-상기 인스트럭션 정보는 연산의 유형, 및 메모리 주소 중 적어도 하나와 관련된 정보를 포함함-를 획득하고, 상기 인스트럭션 정보에 기초하여 상기 신경망 모델이 구동될 임베디드 장치에 대한 최적 화를 수행하고 최적 코드 정보를 획득하고, 상기 최적 코드 정보를 송신하도록 구성되되, 상기 프로세서는, 상 기 임베디드 장치의 컴퓨팅 환경 정보를 획득하고, 강화 학습으로 훈련된 에이전트를 통하여, 인스트럭션 정보 로부터 최적화 파라미터를 획득하고, 상기 최적화 파라미터에 기초하여 상기 임베디드 장치에서 이용될 코드 정 보를 생성함으로써 상기 최적 코드 정보를 획득하도록 구성될 수 있다. 이하에서는 도 1 내지 도 9를 참고하여 본 출원의 신경망 모델 최적화 방법, 신경망 모델 최적화 장치, 및 신경 망 모델 최적화 시스템에 관하여 설명한다. 도 1은 본 출원의 일 실시예에 따른 신경망 모델 최적화 시스템의 개략도이다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 시스템은 임베디드 장치 및 신경망 모델 최적화 장치 (1000, 혹은 서버)를 포함할 수 있다. 임베디드 장치는 특정 목적(혹은 특정 기능)을 가지고 만들어진 프로그래밍이 가능한 임의의 임베디드 시 스템(Embedded system)이 내재된 장치를 포괄하는 의미일 수 있다. 임베디드 장치는 프로세서 및/또는 메 모리를 포함하는 하드웨어를 포함할 수 있다. 또한, 임베디드 장치는 하드웨어를 제어하기 위한 펌웨어 (Firmware)를 포함할 수 있다. 또한, 임베디드 장치는 인공지능 실행 엔진을 포함하여 임의의 소프트웨어 를 펌웨어에 입력하여 임의의 인공지능 모델을 실행하도록 구성될 수 있다. 여기서, 인공지능 실행 엔진(Inference Engine)은 미리 학습된 신경망 모델을 임베디드 장치에 최대한 효율적으로 실행시키기 위한 소 프트웨어로서, 인공지능 실사용에 목적을 둔 기술이며 탑재되는 장치의 환경에 효율성을 높이는 기능을 수행한 다. 예컨대, 모바일 기기의 경우, 모바일 기기의 컴퓨팅 환경인 느린 연산속도 및 저전력 사양에 맞춰 실행엔진 이 구현될 수 있다. 다른 예로, 컴퓨팅 성능이 상대적으로 높은 PC 서버의 경우에는 고성능 병렬처리 능력을 극 대화하도록 실행 엔진이 구현될 수 있다. 본 출원의 일 실시예에 따른 임베디드 장치는 신경망 모델 최적화 장치로부터 임베디드 장치의 컴퓨팅 환경에 최적화된 최적 코드 정보를 획득하고, 최적 코드 정보를 펌웨어에 추가(혹은 입력)할 수 있다. 후술할 바와 같이, 최적 코드 정보는 신경망 모델의 내부 구조를 분석하여 생성될 수 있다. 또한, 최적 코드 정 보는 임베디드 장치의 메모리 사양 및/또는 프로세서 사양 등을 포함한 컴퓨팅 환경을 고려하여 생성될 수 있다. 또한, 임베디드 장치는 신경망 모델 최적화 장치로부터 생성된 최적 코드 정보를 펌웨어에 추 가하고, 신경망 모델의 실행을 수행할 수 있다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 장치는 임베디드 장치 이외의 임의의 장치(혹은 서 버)에서 훈련된 신경망 모델이 임베디드 장치의 컴퓨팅 환경에서 최적으로 실행될 수 있도록, 신경망 모델 의 연산 구조 및/또는 메모리 할당에 대한 최적화를 수행할 수 있다. 또한, 본 출원의 일 실시예에 따른 신경망 모델 최적화 장치는 신경망 모델이 임베디드 장치의 컴퓨팅 환경에서 최적으로 실행될 수 있는 최적 코드를 자동적으로 생성할 수 있다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 장치는 송수신부, 메모리, 및 프로세서 를 포함할 수 있다. 신경망 모델 최적화 장치의 송수신부는 임베디드 장치를 포함하여 임의의 외부 기기와 통신을 수행할 수 있다. 예컨대, 신경망 모델 최적화 장치는, 송수신부를 통해, 최적화를 수행함으로써 획 득한 최적 코드 정보를 임베디드 장치로 송신할 수 있다. 또한, 신경망 모델 최적화 장치는, 송수신 부를 통해, 임베디드 장치 혹은 임의의 외부 장치로부터 임베디드 장치의 컴퓨팅 환경 정보를 수신할 수 있다. 또한 신경망 모델 최적화 장치는, 송수신부를 통해, 학습이 완료된 신경망 모델 및/또는 신경망 모델을 실행시키기 위한 실행데이터를 수신할 수 있다. 신경망 모델 최적화 장치는, 송수신부를 통해, 네트워크에 접속하여 각종 데이터를 송수신할 수 있 다. 송수신부는 크게 유선 타입과 무선 타입을 포함할 수 있다. 유선 타입과 무선 타입은 각각의 장단점을 가지 므로, 경우에 따라서 신경망 모델 최적화 장치에는 유선 타입과 무선 타입이 동시에 마련될 수도 있다. 여기서, 무선 타입의 경우에는 주로 와이파이(Wi-Fi) 같은 WLAN(Wireless Local Area Network) 계열의 통신 방 식을 이용할 수 있다. 또는, 무선 타입의 경우에는 셀룰러 통신, 예컨대, LTE, 5G 계열의 통신 방식을 이용할 수 있다. 다만, 무선 통신 프로토콜이 상술한 예시에 제한되는 것은 아니며, 임의의 적절한 무선 타입의 통신 방식을 이용하는 것도 가능하다. 유선 타입의 경우에는 LAN(Local Area Network)이나 USB(Universal Serial Bus) 통신이 대표적인 예이며 그 외의 다른 방식도 가능하다. 신경망 모델 최적화 장치의 메모리는 각종 정보를 저장할 수 있다. 메모리에는 각종 데이터 가 임시적으로 또는 반영구적으로 저장될 수 있다. 메모리의 예로는 하드 디스크(HDD: Hard Disk Drive), SSD(Solid State Drive), 플래쉬 메모리(flash memory), 롬(ROM: Read-Only Memory), 램(RAM: Random Access Memory) 등이 있을 수 있다. 메모리는 신경망 모델 최적화 장치에 내장되는 형태나 탈부착 가능한 형태로 제공될 수 있다. 메모리에는 신경망 모델 최적화 장치를 구동하기 위한 운용 프로그램(OS: Operating System)이나 신경망 모델 최적화 장치의 각 구성을 동작시키기 위한 프로그램을 비롯해 신경망 모델 최적화 장치의 동작에 필요한 각종 데이터가 저장될 수 있다. 프로세서는 신경망 모델 최적화 장치의 전반적인 동작을 제어할 수 있다. 예컨대, 프로세서 는 후술할 학습이 완료된 신경망 모델의 실행 데이터를 획득하는 동작, 신경망 모델의 구조에 대한 최적화를 수 행하는 동작, 임베디드 장치에 대한 최적화를 수행하는 동작, 최적화 결과에 따라 생성된 최적 코드 정보를 획 득하는 동작, 및/또는 최적 코드 정보를 송신하는 동작 등 신경망 모델 최적화 장치의 전반적인 동작을 제어할 수 있다. 구체적으로 프로세서는 메모리로부터 신경망 모델 최적화 장치의 전반적인 동작을 위한 프로그램을 로딩하여 실행할 수 있다. 프로세서는 하드웨어나 소프트웨어 또는 이들의 조합 에 따라 AP(Application Processor), CPU(Central Processing Unit), MCU(Microcontroller Unit)나 이와 유사 한 장치로 구현될 수 있다. 이때, 하드웨어적으로는 전기적 신호를 처리하여 제어 기능을 수행하는 전자 회로 형태로 제공될 수 있으며, 소프트웨어적으로는 하드웨어적 회로를 구동시키는 프로그램이나 코드 형태로 제공될수 있다. 이하에서는 도 2 내지 도 9를 참고하여, 본 출원의 일 실시예에 따른 신경망 모델 최적화 장치의 동작 및 신경망 모델 최적화 방법을 구체적으로 서술한다. 도 2는 본 출원의 일 실시예에 따른 신경망 모델 최적화 방법을 나타낸 순서도이다. 본 출원의 일 실시예에 따른 신경망 모델 최적화 방법은, 학습이 완료된 신경망 모델의 실행 데이터를 획득하는 단계(S1000), 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계(S2000), 임베디 드 장치에 대한 최적화를 수행하고 최적 코드 정보를 획득하는 단계(S3000), 및 최적 코드 정보를 송신하는 단 계(S4000)를 더 포함할 수 있다. 학습이 완료된 신경망 모델의 실행 데이터를 획득하는 단계(S1000)에서는, 신경망 모델 최적화 장치는, 송수신부를 통하여 학습이 완료된 신경망 모델의 실행 데이터를 획득할 수 있다. 여기서, 실행 데이터란, 신경망 모델의 계층 데이터, 신경망 모델을 구성하는 연산 데이터(operation data), 및/또는 신경망 모델과 관 련된 임의의 가중치(혹은 파라미터)를 포함하여, 신경망 모델을 실행시키기 위하여 필요한 임의의 적절한 데이 터를 포괄하는 의미일 수 있다. 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계(S2000)에서는, 신경망 모델 최 적화 장치는 신경망 모델의 실행데이터에 기초하여 신경망 모델의 구조에 대한 최적화, 예컨대 신경망 모 델의 구조에 대한 경량화를 수행할 수 있다. 일 예로, 신경망 모델 최적화 장치는 신경망 모델의 연산 구 조에 포함된 연산 패턴을 검출하고, 연산 패턴에 포함된 대상 연산들을 병합하는 동작을 수행하도록 구성될 수 있다. 다른 예로, 신경망 모델 최적화 장치는 신경망 모델의 연산들의 실행 순서를 결정하고, 결정된 실 행 순서에 기초하여 메모리 할당과 관련된 최적화를 수행할 수 있다. 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계(S2000)에서는, 신경망 모델 최 적화 장치는 최적화 수행 결과에 따라 인스트럭션 정보를 획득할 수 있다. 여기서, 인스트럭션 정보는, 신경망 모델의 연산들의 유형에 대한 인스트럭션, 및/또는 신경망 모델의 각 연산과 관련된 메모리 주소에 대한 인스트럭션을 포함할 수 있다. S2000 단계에 대하여는 도 3 내지 도 7에서 보다 구체적으로 서술하기로 한다. 임베디드 장치에 대한 최적화를 수행하고 최적 코드 정보를 획득하는 단계(S3000)에서는, 신경망 모델 최적화 장치는, 훈련이 완료된 신경망 모델을 임베디드 장치의 컴퓨팅 환경에 최적으로 실행시키기 위한 코 드를 생성할 수 있다. 구체적으로 신경망 모델 최적화 장치는 강화 학습 기법으로 훈련된 에이전트를 통 하여, S2000 단계에서 획득한 인스트럭션 정보 및 임베디드 장치 정보에 기초하여 최적화 파라미터를 획득하고, 최적화 파라미터에 기초하여 임베디드 장치에서 이용될 최적 코드 정보를 생성하도록 구현될 수 있다. S3000 단계에 대하여는 도 8 내지 도 9에서 보다 구체적으로 서술하기로 한다. 최적 코드 정보를 송신하는 단계(S4000)에서는, 신경망 모델 최적화 장치는, 송수신부를 통하여, 획득된 최적 코드 정보를 임베디드 장치를 포함하여 임의의 외부 장치(혹은 외부 서버)로 송신하도록 구현 될 수 있다. 이하에서는 도 3 내지 7을 참고하여 본 출원의 일 실시예에 따른 신경망 모델의 구조에 대한 최적화에 관한 내 용을 보다 구체적으로 서술하도록 한다. 도 3은 본 출원의 일 실시예에 따른 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계를 구체화한 순서도이다. 본 출원의 일 실시예에 따른 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계 (S2000)는, 실행 데이터에 기초하여 방향성 비사이클 그래프(Directed Acyclic Graph, DAG)를 생성하는 단계 (S2100), DAG에 기초하여 연산들의 실행 순서를 결정하는 단계(S2200), 미리 정해진 기준 연산 패턴에 대응되는 DAG의 대상 연산 패턴을 검출하고, 대상 연산 패턴에 포함된 제1 대상 연산과 제2 대상 연산을 병합하는 단계 (S2300), 결정된 실행 순서에 기초하여 제1 메모리 공간 맵을 획득하고, 제1 메모리 공간 맵에 기초하여 메모리 할당과 관련된 최적화를 수행하는 단계(S2400), 및 최적화 수행 결과에 기초하여 메모리 주소와 관련된 인스트 럭션을 생성하는 단계(S2500)를 더 포함할 수 있다. 실행 데이터에 기초하여 방향성 비사이클 그래프(Directed Acyclic Graph, 이하 DAG)를 생성하는 단계(S2100)에 서는, 신경망 모델 최적화 장치는 신경망 모델의 실행 데이터(예컨대, 신경망 모델의 계층 데이터, 신경 망 모델을 구성하는 연산 데이터(operation data), 및/또는 신경망 모델의 파라미터 등)에 기초하여 DAG를 생성할 수 있다. DAG는 방향 순환이 없는 임의의 무한 유향 그래프를 지칭하는 의미일 수 있다. 도 4는 본 출원의 일 실시예에 따른 DAG의 일 양상을 도시한 도면이다. DAG는 신경망 모델을 구성하는 각 함수들의 데이터 의존 관계, 예컨대, 각 함수들의 연결관계와 관련된 정보를 포함할 수 있다. 예컨대, DAG는 제1 연산(예컨대, A)과 연결된 제2 연산(예컨대, B) 및 제2 연산과 연결된 제3 연산(예컨대, C)을 포함하는 제1 브랜치 구조와 관련된 연결관계와 관련된 정보를 포함할 수 있다. 예컨대, DAG 는 제1 연산(예컨대, A)과 연결된 제4 연산(예컨대, D) 및 제4 연산과 연결된 제5 연산(예컨대, E)을 포함하는 제2 브랜치 구조와 관련된 연결관계와 관련된 정보를 포함할 수 있다. 또한, DAG는 제3 연산과 제5 연산과 연결 된 제6 연산(예컨대, F)과 관련된 연결관계와 관련된 정보를 포함할 수 있다. 다만, 도 4는 DAG의 설명의 편의 를 위한 예시에 불과하며, 이에 제한적으로 해석되지 않는다. DAG에 기초하여 연산들의 실행 순서를 결정하는 단계(S2200)에서는, 신경망 모델 최적화 장치는 DAG를 이 용하여 신경망 모델을 구성하는 적어도 하나 이상의 연산들의 실행 순서를 결정할 수 있다. 실행 순서를 결정하는 데는 임의의 적절한 규칙이 이용될 수 있다. 일 예로, 신경망 모델 최적화 장치는 신경망 모델을 구성하는 연산에 요구되는 메모리 공간에 기초하여 DAG의 연산들의 실행 순서를 결정할 수 있다. 예컨대, 신경망 모델 최적화 장치는 DAG의 제1 브랜치에 포 함된 연산(예컨대, 도 4의 B)에 요구되는 제1 메모리 공간과 DAG의 제2 브랜치에 포함된 연산(예컨대, 도 4의 D)에 요구되는 제2 메모리 공간을 연산하고, 제1 메모리 공간과 제2 메모리 공간을 비교하고, 비교 결과에 기초 하여 제1 브랜치에 포함된 연산(예컨대, 도 4의 B)과 제2 브랜치에 포함된 연산(예컨대, 도 4의 D) 간의 실행 순서를 결정할 수 있다. 구체적으로 제1 메모리 공간이 제2 메모리 공간보다 큰 경우, 신경망 모델 최적화 장치 는 제1 브랜치에 포함된 연산(예컨대, 도 4의 B)의 실행 순서를 제2 브랜치에 포함된 연산(예컨대, 도 4 의 D)의 실행 순서보다 후순위로 할당할 수 있다. 반면 제1 메모리 공간이 제2 메모리 공간보다 작은 경우, 신 경망 모델 최적화 장치는 제1 브랜치에 포함된 연산(예컨대, 도 4의 B)의 실행 순서를 제2 브랜치에 포함 된 연산(예컨대, 도 4의 D)의 실행 순서보다 선순위로 할당할 수 있다. 다른 예로, 신경망 모델 최적화 장치는 DAG의 브랜치 구조를 고려하여 신경망 모델을 구성하는 연산들의 실행 순서를 결정할 수 있다. 예컨대, 같은 브랜치에 포함된 연산들을 순차적으로 실행하는 것이, 제1 브랜치에 포함된 연산을 실행하고 제2 브랜치에 포함된 연산을 실행하는 것보다는 메모리 공간 측면에 유리할 수 있다. 따라서, 신경망 모델 최적화 장치는 제1 브랜치에 포함된 연산들(예컨대, 도 4의 B, C)을 순차적으로 실 행한 이후에, 제2 브랜치에 포함된 연산들(예컨대, 도 4의 D, E)를 실행하도록 실행 순서를 결정할 수 있다. 혹 은 신경망 모델 최적화 장치는 제2 브랜치에 포함된 연산들(예컨대, 도 4의 D, E)을 실행하고, 제1 브랜 치에 포함된 연산들(예컨대, 도 4의 B, C)를 실행하도록 실행 순서를 결정할 수 있다. 미리 정해진 기준 연산 패턴에 대응되는 DAG의 대상 연산 패턴을 검출하고, 대상 연산 패턴에 포함된 제1 대상 연산과 제2 대상 연산을 병합하는 단계(S2300)에서는, 신경망 모델 최적화 장치는, 미리 정해진 기준 연 산 패턴을 획득하고, 기준 연산 패턴에 대응되는 DAG의 대상 연산 패턴을 검출할 수 있다. 또한, 미리 정해진 기준 연산 패턴에 대응되는 DAG의 대상 연산 패턴을 검출하고, 대상 연산 패턴에 포함된 제1 대상 연산과 제2 대상 연산을 병합하는 단계(S2300)에서는, 신경망 모델 최적화 장치는 검출된 대상 연산 패턴에 포함된 연산들을 병합하는 동작을 수행하도록 구현될 수 있다. 보다 구체적으로 본 출원의 일 실시예에 따른 제1 대상 연산과 제2 대상 연산을 병합하는 단계는, 미리 정해진 기준 연산 패턴 정보-기준 연산 패턴 정보는 제1 연산 및 제1 연산과 연계된 제2 연산을 포함함-를 획득하는 단 계, 기준 연산 패턴 정보에 기초하여 DAG로부터 제1 연산에 대응되는 제1 대상 연산 및 제2 연산에 대응되는 제 2 대상 연산을 검출하는 단계, 및 제1 대상 연산과 제2 대상 연산을 병합하고, 병합 결과에 기초하여 커널을 변 환하는 단계를 더 포함할 수 있다. 도 5는 본 출원의 일 실시예에 따른 제1 대상 연산과 제2 대상 연산을 병합하는 일 양상을 도시한 도면이다. 미리 정해진 기준 연산 패턴 정보-기준 연산 패턴 정보는 제1 연산 및 제1 연산과 연계된 제2 연산을 포함함-를 획득하는 단계에서는, 신경망 모델 최적화 장치는 미리 정해진 기준 연산 패턴 정보를 획득할 수 있다. 이때, 기준 연산 패턴 정보는 일반적으로 많이 이용되는 연산 패턴과 관련된 정보로, 미리 설정될 수 있다. 예 컨대, 기준 연산 패턴 정보는 제1 연산(예컨대, 콘볼루션(Convolution)) 및 제1 연산과 연결된 제2 연산(예컨대, Rectified Linear Unit (ReLu))을 포함하는 연산 패턴과 관련될 수 있다. 다만 이는 설명의 편의 를 위한 예시에 불과하며, 임의의 적절한 연산 패턴이 미리 설정될 수 있다. 예컨대, 기준 연산 패턴 정보는 컨볼루션(Convolution) 연산을 수행하고, 뎁스와이스 컨볼루션(Depthwise convolution) 연산과 액티베이션 (Activation) 연산을 순차적으로 수행하는 연산 패턴과 관련될 수 있다. 다른 예로, 기준 연산 패턴 정보는 이 미지의 색상과 관련된 채널별로 필터를 적용하여 채널별로 데이터를 압축하는 뎁스와이스 컨볼루션(Depthwise convolution) 연산을 수행하여 중간 결과값을 획득하고, 중간결과 값에 기초하여 포인트와이스(Pointwise) 연산 을 수행하는 연산 패턴을 포함할 수 있다. 기준 연산 패턴 정보에 기초하여 DAG로부터 제1 연산에 대응되는 제1 대상 연산 및 제2 연산에 대응되는 제2 대 상 연산을 검출하는 단계에서는, 신경망 모델 최적화 장치는 기준 연산 패턴 정보를 이용하여 DAG에 포함 된 제1 연산(예컨대, Convolution)에 대응되는 제1 대상 연산(예컨대, 도 5의 제1 대상 연산), 및 제2 연산(예 컨대, ReLu)에 대응되는 제2 대상 연산(예컨대, 도 5의 제2 대상 연산)을 포함하는 대상 연산 패턴을 검출할 수 있다. 이때, 대상 연산 패턴을 검출하기 위하여 임의의 패턴 매칭 알고리즘이 이용될 수 있다. 제1 대상 연산과 제2 대상 연산을 병합하고, 병합 결과에 기초하여 커널을 변환하는 단계에서는, 신경망 모델 최적화 장치는 제1 대상 연산(예컨대, 도 5의 Convolution)과 제2 대상 연산(예컨대, 도 5의 ReLu)를 병 합하고, 병합 결과에 기초하여 대상 연산 패턴에 포함된 커널들을 단일 커널(예컨대, Convolution + ReLu 커 널)로 변환하도록 구현될 수 있다. 본 실시예에 따르면, 제1 대상 연산과 제2 대상 연산을 병합하여 일체로 연 산을 수행함으로써, 연산에 요구되는 메모리 공간을 줄이고, 실행 속도를 높일 수 있다는 유리한 효과가 제공될 수 있다. 다시 도 2를 참고하면, 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계(S200 0)에서는, 신경망 모델 최적화 장치는 신경망 모델을 구성하는 연산의 입력 데이터 및/또는 출력 데이터 에 대한 정수화를 수행할 수 있다. 구체적으로 신경망 모델 최적화 장치는 양자화 기법을 활용하여 소수 형태의 연산의 입력 데이터 및/또는 출력 데이터를 특정 범위의 정수로 변환할 수 있다. 여기서 양자화 (Quantizaiton)는 32 비트 소수점(float) 값을 8비트 정수(int)로 변환하는 기법이다. 신경망 모델 최적화 장치 는 신경망 모델을 구성하는 연산에 대한 데이터 텐서별로 스케일(Scale)와 제로 포인트(ZeroPoint)를 연 산하여, 32 비트 소수점 값을 용량이 상대적으로 작은 8비트 정수 값(int8Value)으로 변환할 수 있다. 예컨대, 신경망 모델 최적화 장치는 하기의 수학식을 통하여 32 비트 소수점 값을 8비트의 정수 값으로 변환하거 나 조정하도록 구성될 수 있다."}
{"patent_id": "10-2022-0140653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "다만, 전술한 8비트 정수 양자화는 하나의 예시에 불과하며, 신경망 모델 최적화 장치는 8비트 정수 외에 도, 임의의 소수점 값을 0 혹은 8 비트 자연수 값으로 변환하거나 조정하여 신경망 모델의 구조에 대한 최적화 를 수행하도록 구현될 수 있다. 다시 도 3을 참고하면, 본 출원의 일 실시예에 따른 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는 단계(S2000)는, 결정된 실행 순서에 기초하여 제1 메모리 공간 맵을 획득하고, 제1 메모리 공 간 맵에 기초하여 메모리 할당과 관련된 최적화를 수행하는 단계(S2400)를 더 포함할 수 있다. 도 6은 본 출원의 일 실시예에 따른 제1 메모리 공간 맵의 일 양상을 도시한 도면이다. 결정된 실행 순서에 기초하여 제1 메모리 공간 맵을 획득하고, 제1 메모리 공간 맵에 기초하여 메모리 할당과 관련된 최적화를 수행하는 단계(S2400)에서는, 신경망 모델 최적화 장치는, S2200 단계에서 결정된 실행 순서 및 각 연산에 요구되는 메모리 공간에 기초하여 제1 메모리 공간 맵을 생성할 수 있다. 구체적으로 도 4와 관련하여 전술한 규칙에 따라 신경망 모델을 구성하는 연산들의 실행 순서(예컨대, 도 6의 A, B, C, D, E, F 순)가 결정될 수 있다. 이때, 신경망 모델 최적화 장치는 각 연산을 통하여 출력되는 데이터의 크기와 관 련된 메모리 공간에 기초하여 제1 메모리 공간 맵을 생성할 수 있다. 예컨대, 신경망 모델 최적화 장치는 연산 A를 통하여 출력되는 데이터에 필요한 메모리 공간과 연산 A를 통하여 출력되는 데이터가 필요한 연산들 (예컨대, 연산 B, 연산 D)의 실행 순서를 고려하여 연산 A와 관련된 메모리 텐서(예컨대, T1)를 배치할 수 있다. 또한, 신경망 모델 최적화 장치는 연산 B를 통하여 출력되는 데이터에 필요한 메모리 공간과 연산 B를 통하여 출력되는 데이터가 필요한 연산들(예컨대, 연산 C)의 실행 순서를 고려하여 연산 B와 관련된 메모리 텐서(예컨대, T2)를 배치할 수 있다. 이때, 연산 B를 통하여 출력되는 데이터가 저장되는 T2 메모리 텐서는 연 산 B에 필요한 데이터를 저장하고 있는 연산 A와 관련된 T1 메모리 텐서의 인접 위치에 배치될 수 있다. 유사한방식으로 신경망 모델 최적화 장치는 신경망 모델을 구성하는 연산들(예컨대, 연산 C, D, E 등)을 통하여 출력되는 데이터 크기와 관련된 메모리 공간과 연산들의 실행 순서를 고려하여 제1 메모리 공간 맵을 생성할 수 있다. 이상에서는 도 6에 도시된 제1 메모리 공간 맵을 중심으로 신경망 모델 최적화 장치의 메모리 텐서를 배 치하는 동작을 설명하였다. 다만 도 6에 도시된 제1 메모리 공간 맵은 설명의 편의를 위한 예시일 뿐이며, 이에 제한적으로 해석되어서는 아니된다. 도 7은 본 출원의 일 실시예에 따른 메모리 할당과 관련된 최적화의 일 양상을 도시한 도면이다. 결정된 실행 순서에 기초하여 제1 메모리 공간 맵을 획득하고, 제1 메모리 공간 맵에 기초하여 메모리 할당과 관련된 최적화를 수행하는 단계(S2400)에서는, 신경망 모델 최적화 장치는 신경망 모델을 구성하는 연산 들의 실행 순서 및 연산들을 통하여 출력되는 데이터 크기와 관련된 메모리 공간에 기초하여 생성된 제1 메모리 공간 맵을 획득할 수 있다. 또한, 신경망 모델 최적화 장치는 제1 메모리 공간 맵에 기초하여 메모리 할당과 관련된 최적화를 수행할 수 있다. 일 예로, 신경망 모델 최적화 장치는 메모리 인-플레이싱(Memory In-placing) 기법을 활용하여, 특정 연산(예컨대, ReLu 연산, Add 연산, 및/또는 Sigmoid 연산 등)의 출력 공간을 해당 연산의 입력 공간에 덮 어쓰는 동작을 수행할 수 있다. 구체적으로 신경망 모델 최적화 장치는 메모리 인-플레이싱 기법을 활용 하여, 특정 연산(예컨대, 도 7의 제3 대상 연산(연산 C))으로 입력된 값이 저장되는 메모리 텐서(예컨대, 도 7 의 T2 메모리 텐서)를 특정 연산(예컨대, 도 7의 제3 대상 연산(연산 C))를 통하여 출력된 값이 저장되는 메모 리 텐서(예컨대, 도 7의 T3 메모리 텐서)로 변경하도록 구현될 수 있다. 또한, 신경망 모델 최적화 장치 는 메모리 텐서의 변경 결과에 기초하여 제1 메모리 공간 맵으로부터 제2 메모리 공간 맵을 생성할 수 있다. 제 2 메모리 공간 맵은 제1 메모리 공간 맵보다는 상대적으로 적은 메모리 공간을 차지하게 된다. 따라서 본 동작 을 통하여 본 출원의 일 실시예에 따른 신경망 모델 최적화 방법에 의하면, 필요한 전체 메모리 공간을 줄이고, 연산들의 실행 속도를 높일 수 있다. 최적화 수행 결과에 기초하여 메모리 주소와 관련된 인스트럭션을 생성하는 단계(S2500)에서는, 신경망 모델 최 적화 장치는 전술한 대상 연산 패턴에 포함된 대상 연산들의 병합 혹은 메모리 할당과 관련된 최적화 수 행 결과에 기초하여, 신경망 모델을 구성하는 각 연산에 대응되는 메모리 텐서의 메모리 주소와 관련된 인스트 럭션, 및/또는 신경망 모델을 구성하는 각 연산의 유형과 관련된 인스트럭션을 생성할 수 있다. 다시 도 2를 참고하면, 본 출원의 일 실시예에 따른 신경망 모델의 최적화 방법은 임베디드 장치에 대한 최적화 를 수행하고 최적 코드 정보를 획득하는 단계(S3000)를 포함할 수 있다. 이하에서는 도 8 내지 9를 참고하여 본 출원의 일 실시예에 따른 임베디드 장치에 대한 신경망 모델의 최적화에 관한 내용을 보다 구체적으로 서술하도록 한다. 본 출원의 일 실시예에 따르면, 강화 학습 기법을 활용하여 임 베디드 장치에 대한 신경망 모델의 최적화를 수행하도록 구현될 수 있다. 도 8은 본 출원의 일 실시예에 따른 임베디드 장치에 대한 최적화를 수행하고 최적 코드 정보를 획득하는 단계 를 구체화한 순서도이다. 본 출원의 일 실시예에 따른 임베디드 장치에 대한 최적화를 수행하고 최적 코드 정보를 획득하는 단계(S3000) 는, 임베디드 장치의 컴퓨팅 환경 정보를 획득하는 단계(S3100), 강화 학습으로 훈련된 에이전트를 통하여 인스 트럭션 정보에 기초하여 최적화 파라미터를 획득하는 단계(S3200), 및 최적화 파라미터에 기초하여 임베디드 장 치에서 이용될 최적 코드 정보를 생성하는 단계(S3300)를 더 포함할 수 있다. 임베디드 장치의 컴퓨팅 환경 정보를 획득하는 단계(S3100)에서는, 신경망 모델 최적화 장치는, 신경망 모델 최적화 장치는, 송수신부를 통하여, 임베디드 장치 혹은 임의의 외부 장치로부터 신경망 모델이 실행될 타겟 장치인 임베디드 장치의 컴퓨팅 환경 정보(예컨대, 임베디드 장치의 메모리 정보 및 프로세서 정보)를 획득할 수 있다. 한편, 도 8에서는 도시하지는 않았지만, 임베디드 장치의 컴퓨팅 환경 정 보를 획득하는 단계(S3100)에서는, 신경망 모델 최적화 장치는, 임베디드 장치의 장치 유형 정보나 타겟 기능 정보 등 임베디드 장치에서 신경망 모델을 실행시키기는 영향을 주는 변수에 대한 임의의 정보 를 획득할 수 있다. 강화 학습으로 훈련된 에이전트를 통하여 인스트럭션 정보에 기초하여 최적화 파라미터를 획득하는 단계(S320 0)에서는, 신경망 모델 최적화 장치는 강화 학습 기법을 활용하여 훈련된 에이전트를 이용하여, 최적 코 드를 생성하기 위한 최적화 파라미터를 획득할 수 있다. 강화 학습으로 훈련된 에이전트를 통하여 인스트럭션 정보에 기초하여 최적화 파라미터를 획득하는 단계(S320 0)에서는, 신경망 모델 최적화 장치는, 메모리 상태 정보, 인스트럭션 정보에 포함된 연산 유형 정보, 및 임베디드 장치의 컴퓨팅 환경 정보를 포함하는 임베디드 장치 정보 중 적어도 하나를 에이전트에 입력하고, 에이전트를 통하여 출력되는 최적화 파라미터를 획득할 수 있다. 여기서 최적화 파라미터란, 연산에 대하여 수행될 알고리즘 유형을 선택하는 파라미터, 연산의 블록 사이즈와 관련된 파라미터, 및/또는 코드의 길 이와 관련된 파라미터를 포함하여, 신경망 모델을 실행시키기 위하여 필요한 코드와 관련된 임의의 변수와 관련 될 수 있다. 일 예로, 에이전트에 입력된 연산 유형을 콘볼루션(Convolution)이라고 가정한다. 이때, 훈련이 완료된 에이전 트는, 입력값에 기초하여 콘볼루션과 관련된 알고리즘, 예컨대, Im2Col 알고리즘, Default 알고리즘, 및 FFT 알 고리즘 중에서 적어도 하나의 알고리즘을 선택하는 파라미터를 출력할 수 있다. 또한, 에이전트는 입력값에 기 초하여 연산의 블록 사이즈와 관련된 파라미터, 및/또는 코드의 길이와 관련된 파라미터를 출력할 수 있다. 이 때, 신경망 모델 최적화 장치는 에이전트를 통하여 출력된 최적화 파라미터를 획득할 수 있다. 도 9는 본 출원의 일 실시예에 따른 강화 학습 방식을 통하여 에이전트를 훈련시키는 일 양상을 도시한 도면이 다. 전술한 바와 같이, 본 출원의 일 실시예에 따른 신경망 모델 최적화 장치는 '강화 학습'을 통하여 학습된 에이전트를 통하여 최적화 파라미터를 획득할 수 있다. 구체적으로 에이전트는, 초기 규칙(policy)에 기초하여, 연산 유형 정보, 메모리 상태 정보, 및 임베디드 장치 정보(혹은 타겟 장치 정보)에 수신하여 최적화 파라미터 (예컨대, 알고리즘 유형을 선택하는 파라미터, 연산의 블록 사이즈와 관련된 파라미터, 및/또는 코드의 길이와 관련된 파라미터 등)와 관련된 예측값을 출력하도록 구성될 수 있다. 한편, 코드 생성기는 최적화 파라미터를 획득하고, 최적화 파라미터에 기초하여 코드를 생성할 수 있다. 이때, 생성된 코드의 성능에 대한 실행 및 평가 가 수행되고, 평가 결과에 기초하여 코드의 성능에 대한 평가값을 최대화하도록(즉, 코드의 성능이 최대화되도 록) 에이전트의 초기 규칙이 갱신될 수 있다. 구체적으로 코드의 성능에 대한 평가값이 최대화되는 최적화 파라 미터를 출력하도록, 에이전트의 초기 규칙이 갱신되도록 학습될 수 있다. 훈련이 완료된 에이전트는 연산 유형 정보, 메모리 상태 정보, 및/또는 임베디드 장치 정보를 수신하여 최대화된 성능을 가지는 코드를 생성할 수 있 는 최적화 파라미터를 출력할 수 있다. 최적화 파라미터에 기초하여 임베디드 장치에서 이용될 최적 코드 정보를 생성하는 단계(S3300)에서는, 신경망 모델 최적화 장치는, S3200 단계에서 생성된 최적화 파라미터(예컨대, 연산에 대하여 수행될 알고리즘 유 형을 선택하는 파라미터, 연산의 블록 사이즈와 관련된 파라미터, 및/또는 코드의 길이와 관련된 파라미터 등) 에 기초하여 임베디드 장치에서 이용될 최적 코드 정보를 생성할 수 있다. 구체적으로 신경망 모델 최적화 장치는, 코드 생성기를 통하여, 최적화 파라미터 및 해당 연산의 메모리 주소에 기초하여 최적 코드를 생 성할 수 있다. 예컨대, 최적화 파라미터가 Im2Col 알고리즘, Default 알고리즘, 및 FFT 알고리즘 중에서 적어도 하나의 알고리즘(예컨대, Im2Col 알고리즘)을 선택하는 파라미터, 연산의 블록 사이즈 값과 관련된 파라미터, 및/또는 코드의 길이 값과 관련된 파라미터 중 적어도 하나를 포함하는 경우에는, 신경망 모델 최적화 장치 는, 코드 생성기를 통하여, 최적화 파라미터 및 인스트럭션 정보에 포함된 연산의 메모리 주소에 기초하 여, 최적 코드를 생성할 수 있다. 한편, 도 8에서는 도시하지는 않았지만, 본 출원의 일 실시예에 따른 최적화 파라미터에 기초하여 임베디드 장 치에서 이용될 최적 코드 정보를 생성하는 단계(S3300)는, 최적화 파라미터에 기초하여 인스트럭션 정보에 대응 되도록 코드를 생성하는 단계, 생성된 코드를 컴파일하여 바이너리 파일 형태로 변환하는 단계를 더 포함할 수 있다. 구체적으로 신경망 모델 최적화 장치는 최적화 파라미터에 기초하여 코드(예컨대, C 언어 코드)를 생성하고, 인스트럭션 정보에 포함된 메모리 주소에 생성된 코드를 저장할 수 있다. 또한, 신경망 모델 최적화 장치는 생성된 코드를 컴파일하여 바이너리 파일(예, API 파일)로 변환할 수 있다. 또한 신경망 모델 최 적화 장치는, 송수신부를 통하여, 생성된 코드의 바이너리 파일을 임베디드 장치 혹은 임의의 외부 장치로 송신할 수 있다. 본 실시예에 따르면, 최적 코드가 바이너리 파일(예컨대, API 파일) 형태로 임베 디드 장치로 송신됨으로써, 임베디드 장치의 사용자가 최적 코드를 시각적으로 확인할 수 있다. 본 출원의 실시예에 따른 신경망 모델 최적화 방법, 신경망 모델 최적화 장치, 및 신경망 모델 최적화 시스템에 의하면, 임베디드 장치의 하드웨어의 비표준화 문제와 기존 인공지능 실행 엔진의 구조적인 제약을 해결하고,신경망 모델을 임베디드 장치의 하드웨어 플랫폼에 대하여 최적화시킬 수 있다. 본 출원의 실시예에 따른 신경망 모델 최적화 방법, 신경망 모델 최적화 장치, 및 신경망 모델 최적화 시스템에 의하면, 임베디드 장치에서의 신경망 모델의 실행 능력이 향상될 수 있다. 본 출원의 실시예에 따른 신경망 모델 최적화 방법, 신경망 모델 최적화 장치, 및 신경망 모델 최적화 시스템에 의하면, 임베디드 장치에서 신경망 모델을 실행시키기 위하여 필요한 전력 사용량을 감소시킬 수 있다. 상술한 신경망 모델 최적화 장치의 다양한 동작들은 신경망 모델 최적화 장치의 메모리에 저 장될 수 있으며, 신경망 모델 최적화 장치의 프로세서는 메모리에 저장된 동작들을 수행하도 록 제공될 수 있다. 본 출원에 개시된 신경망 모델 최적화 방법, 신경망 모델 최적화 장치 및 신경망 모델 최적화 시스템은 가전 제 품, 차량용 센서, 유아 혹은 노인의 안전을 위한 제품 및 스마트 워치 등을 포함하여 다양한 임베디드 시스템에 서의 인공지능 모델의 효율적인 실행을 위하여 이용될 수 있다. 이상에서 실시 형태들에 설명된 특징, 구조, 효과 등은 본 발명의 적어도 하나의 실시 형태에 포함되며, 반드시 하나의 실시 형태에만 한정되는 것은 아니다. 나아가, 각 실시 형태에서 예시된 특징, 구조, 효과 등은 실시 형 태들이 속하는 분야의 통상의 지식을 가지는 자에 의해 다른 실시 형태들에 대해서도 조합 또는 변형되어 실시 가능하다. 따라서 이러한 조합과 변형에 관계된 내용들은 본 발명의 범위에 포함되는 것으로 해석되어야 할 것 이다. 또한, 이상에서 실시 형태를 중심으로 설명하였으나 이는 단지 예시일 뿐 본 발명을 한정하는 것이 아니며, 본 발명이 속하는 분야의 통상의 지식을 가진 자라면 본 실시 형태의 본질적인 특성을 벗어나지 않는 범위에서 이 상에 예시되지 않은 여러 가지의 변형과 응용이 가능함을 알 수 있을 것이다. 즉, 실시 형태에 구체적으로 나타 난 각 구성 요소는 변형하여 실시할 수 있는 것이다. 그리고 이러한 변형과 응용에 관계된 차이점들은 첨부된 청구 범위에서 규정하는 본 발명의 범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2022-0140653", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 출원의 일 실시예에 따른 신경망 모델 최적화 시스템의 개략도이다. 도 2는 본 출원의 일 실시예에 따른 신경망 모델 최적화 방법을 나타낸 순서도이다. 도 3은 본 출원의 일 실시예에 따른 신경망 모델의 구조에 대한 최적화를 수행하고 인스트럭션 정보를 획득하는단계를 구체화한 순서도이다. 도 4는 본 출원의 일 실시예에 따른 방향성 비사이클 그래프의 일 양상을 도시한 도면이다. 도 5는 본 출원의 일 실시예에 따른 제1 대상 연산과 제2 대상 연산을 병합하는 일 양상을 도시한 도면이다. 도 6은 본 출원의 일 실시예에 따른 제1 메모리 공간 맵의 일 양상을 도시한 도면이다. 도 7은 본 출원의 일 실시예에 따른 메모리 할당과 관련된 최적화의 일 양상을 도시한 도면이다. 도 8은 본 출원의 일 실시예에 따른 임베디드 장치에 대한 최적화를 수행하고 최적 코드 정보를 획득하는 단계 를 구체화한 순서도이다. 도 9는 본 출원의 일 실시예에 따른 강화 학습 방식을 통하여 에이전트를 훈련시키는 일 양상을 도시한 도면이 다."}
