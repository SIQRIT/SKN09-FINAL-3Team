{"patent_id": "10-2023-0125012", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0041962", "출원번호": "10-2023-0125012", "발명의 명칭": "동시적 위치 추정 및 맵 작성 방법 및 이를 수행하는 전자 장치", "출원인": "삼성전자주식회사", "발명자": "김윤태"}}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 인스트럭션을 저장하는 메모리(230); 및상기 적어도 하나의 인스트럭션을 실행하는 적어도 하나의 프로세서를 포함하되, 상기 적어도 하나의 프로세서(280)는:제1 사용 환경에서 기-작성된(pre-mapped) 제1 맵 데이터를 이용하여 학습된 인공지능 모델의 적어도 하나의 파라미터를 획득하고,제2 사용 환경에 대응하는 사용 환경 데이터를 획득하고,상기 사용 환경 데이터에 기초하여 적어도 하나의 키 프레임에 대응하는 카메라 포즈(camera pose) 및 이미지평면(image plane)을 결정하고,상기 카메라 포즈 및 상기 이미지 평면에 기초하여, 카메라 위치에서 상기 이미지 평면 상의 픽셀을 향하는 광선(ray)의 방향 및 상기 광선 상의 지점의 3차원 좌표 값을 결정하고,상기 적어도 하나의 파라미터를 갖는 상기 학습된 인공지능 모델에 상기 광선의 방향 및 상기 3차원 좌표 값을입력하여, 상기 픽셀에 대응하는 이미지 픽셀 정보를 획득하고,상기 획득된 이미지 픽셀 정보에 기초하여 상기 적어도 하나의 키 프레임을 획득하고,상기 획득된 적어도 하나의 키 프레임에 기초하여 제2 맵 데이터를 획득하는, 전자 장치."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 맵 데이터는 적어도 하나의 원시(raw) 키 프레임을 포함하고,상기 학습된 인공지능 모델은, 상기 적어도 하나의 원시 키 프레임에 기초하여 미리 학습된, 전자 장치."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 및 제2항 중 어느 한 항에 있어서,상기 제2 사용 환경에서 공간을 촬영하여 이미지를 획득하는 카메라(240); 및사용자 인터페이스(270)를 포함하고,상기 사용 환경 데이터는 사용자의 키 및 사용자의 사용 패턴 중 적어도 하나에 대응하는 정보를 포함하는 기정의된 사용자 정보를 포함하고,상기 사용자의 키에 대한 정보는 상기 사용자 인터페이스(270)를 통해 기 획득되어 상기 메모리(230)에 저장되고,상기 적어도 하나의 프로세서(280)는:상기 메모리로부터 상기 사용자의 키에 대한 정보를 획득하고,상기 이미지에 기초하여 상기 사용자의 사용 패턴에 대한 정보를 획득하고,상기 기-정의된 사용자 정보에 기초하여 상기 카메라 포즈 및 상기 이미지 평면을 결정하는, 전자 장치.공개특허 10-2025-0041962-3-청구항 4 제1항 내지 제3항 중 어느 한 항에 있어서,상기 사용 환경 데이터는 조명 정보를 포함하고,상기 적어도 하나의 프로세서(280)는:상기 적어도 하나의 파라미터를 갖는 상기 학습된 인공지능 모델에 상기 조명 정보, 상기 광선의 방향 및 상기3차원 좌표 값을 입력하여, 상기 이미지 픽셀 정보를 획득하는, 전자 장치."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 제2 사용 환경에 대응하는 복수의 이미지 프레임들을 획득하는 카메라(240)를 포함하고,상기 적어도 하나의 프로세서(280)는:조명 추정 모델에 상기 복수의 이미지 프레임들 중 적어도 하나를 입력하여, 상기 조명 정보를 획득하는, 전자장치."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,공간의 조명 값을 측정하는 조명 센서를 포함하고,상기 적어도 하나의 프로세서(280)는:상기 조명 값에 기초하여 상기 조명 정보를 획득하는, 전자 장치."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 내지 제6항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(280)는:상기 제2 사용 환경에 대응하는 복수의 이미지 프레임들을 획득하고,상기 복수의 이미지 프레임들간의 시간 간격, 상기 복수의 이미지 프레임들간의 카메라의 위치 관계 변화량, 상기 복수의 이미지 프레임들간의 깊이 정보 차이, 및 상기 복수의 이미지 프레임들 간의 특징점 분포의 차이 중적어도 하나가 기 정의된 임계 값을 초과하는지 여부에 기초하여 적어도 하나의 추가 키 프레임을 결정하고,상기 결정된 적어도 하나의 추가 키 프레임에 기초하여 상기 학습된 인공지능 모델의 상기 적어도 하나의 파라미터를 업데이트하는, 전자 장치."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 적어도 하나의 프로세서(280)는:상기 결정된 적어도 하나의 추가 키 프레임에 대해 객체 세그멘테이션을 수행하고,상기 제2 맵 데이터와 상기 객체 세그멘테이션 결과에 기초하여, 상기 제2 맵 데이터에 대응하는 공간에 속하는적어도 하나의 객체의 추가, 변경 또는 삭제 여부를 결정하고,상기 적어도 하나의 객체가 추가, 변경, 또는 삭제된 것으로 결정한 것에 기초하여, 상기 결정된 적어도 하나의공개특허 10-2025-0041962-4-추가 키 프레임을 학습 데이터로 하여, 상기 학습된 인공지능 모델의 상기 적어도 하나의 파라미터를 업데이트하고,상기 적어도 하나의 업데이트된 파라미터를 갖는 상기 학습된 인공지능 모델을 이용하여 제3 맵 데이터를 획득하는, 전자 장치."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 사용 환경 데이터는, 외부 장치의 하드웨어 성능 정보를 포함하고,상기 적어도 하나의 프로세서(280)는:상기 하드웨어 성능 정보에 기초하여 기-결정된 수의 상기 적어도 하나의 키 프레임을 획득하고,상기 기-결정된 수의 상기 적어도 하나의 키 프레임을 포함하는 상기 제2 맵 데이터를 상기 외부 장치에 전달하는, 전자 장치."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제9항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(280)는:현재 카메라 포즈를 획득하고,상기 제2 맵 데이터에 기초하여 상기 현재 카메라 포즈의 오차를 보정하는, 전자 장치."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "동시적 위치 추정 및 맵 작성 방법에 있어서,제1 사용 환경에서 기-작성된(pre-mapped) 제1 맵 데이터를 이용하여 학습된 인공지능 모델의 적어도 하나의 파라미터를 획득하는 단계 (S310);제2 사용 환경에 대응하는 사용 환경 데이터를 획득하는 단계 (S320);상기 사용 환경 데이터에 기초하여 적어도 하나의 키 프레임에 대응하는 카메라 포즈 및 이미지 평면을 결정하는 단계 (S330);상기 카메라 포즈 및 상기 이미지 평면에 기초하여, 카메라 위치에서 상기 이미지 평면 상의 픽셀을 향하는 광선의 방향 및 상기 광선 상의 지점의 3차원 좌표 값을 결정하는 단계 (S340);상기 적어도 하나의 파라미터를 갖는 상기 학습된 인공지능 모델에 상기 광선의 방향 및 상기 3차원 좌표 값을입력하여, 이미지 픽셀 정보를 획득하는 단계 (S350);상기 획득된 이미지 픽셀 정보에 기초하여 상기 적어도 하나의 키 프레임을 획득하는 단계 (S360); 및상기 획득된 적어도 하나의 키 프레임에 기초하여 제2 맵 데이터를 획득하는 단계 (S370)를 포함하는, 방법."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 사용 환경 데이터는 사용자의 키 및 사용자의 사용 패턴 중 적어도 하나에 대응하는 정보를 포함하는 기정의된 사용자 정보를 포함하고,공개특허 10-2025-0041962-5-상기 사용자의 키에 대한 정보는 사용자 인터페이스를 통해 기 획득되어 메모리에 저장되고,상기 사용 환경 데이터에 기초하여 카메라 포즈 및 3차원 좌표 값을 결정하는 단계는:상기 메모리로부터 상기 사용자의 키에 대한 정보를 획득하는 단계;상기 제2 사용 환경에서 공간을 촬영하여 이미지를 획득하는 단계;상기 이미지에 기초하여 상기 사용자의 사용 패턴에 대한 정보를 획득하는 단계;상기 기 정의된 사용자 정보에 기초하여 상기 카메라 포즈 및 상기 이미지 평면을 결정하는 단계를 포함하는,방법."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항 및 제12항 중 어느 한 항에 있어서,상기 사용 환경 데이터는 조명 정보를 포함하고,상기 적어도 하나의 파라미터를 갖는 상기 학습된 인공지능 모델에 상기 광선의 방향 및 상기 3차원 좌표 값을입력하여, 이미지 픽셀 정보를 획득하는 단계는:상기 적어도 하나의 파라미터를 갖는 상기 학습된 인공지능 모델에 상기 조명 정보, 상기 광선의 방향 및 상기3차원 좌표 값을 입력하여, 상기 이미지 픽셀 정보를 획득하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,제2 사용 환경에 대응하는 사용 환경 데이터를 획득하는 단계는,상기 제2 사용 환경에 대응하는 복수의 이미지 프레임들을 획득하는 단계; 및조명 추정 모델에 상기 복수의 이미지 프레임들 중 적어도 하나를 입력하여, 상기 조명 정보를 획득하는 단계를포함하는, 방법."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13항에 있어서,상기 방법은:조명 센서를 이용하여 공간의 조명 값을 측정하는 단계; 및상기 조명 값에 기초하여 상기 조명 정보를 획득하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항 내지 제15항 중 어느 한 항에 있어서,상기 방법은:상기 제2 사용 환경에 대응하는 복수의 이미지 프레임들을 획득하는 단계;상기 복수의 이미지 프레임들간의 시간 간격, 상기 복수의 이미지 프레임들간의 카메라의 위치 관계 변화량, 상기 복수의 이미지 프레임들간의 깊이 정보 차이, 및 상기 복수의 이미지 프레임들 간의 특징점 분포의 차이 중적어도 하나가 기 정의된 임계 값을 초과하는지 여부에 기초하여 적어도 하나의 추가 키 프레임을 결정하는 단계; 및공개특허 10-2025-0041962-6-상기 결정된 적어도 하나의 추가 키 프레임에 기초하여 상기 학습된 인공지능 모델의 상기 적어도 하나의 파라미터를 업데이트하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 결정된 적어도 하나의 추가 키 프레임에 기초하여 상기 학습된 인공지능 모델의 상기 적어도 하나의 파라미터를 업데이트하는 단계는:상기 결정된 적어도 하나의 추가 키 프레임에 대해 객체 세그멘테이션을 수행하는 단계;상기 제2 맵 데이터와 상기 객체 세그멘테이션 결과에 기초하여, 상기 제2 맵 데이터에 대응하는 공간에 속하는적어도 하나의 객체의 추가, 변경 또는 삭제 여부를 결정하는 단계;상기 적어도 하나의 객체가 추가, 변경, 또는 삭제된 것으로 결정한 것에 기초하여, 상기 결정된 적어도 하나의추가 키 프레임을 학습 데이터로 하여, 상기 학습된 인공지능 모델의 상기 적어도 하나의 파라미터를 업데이트하는 단계; 및상기 적어도 하나의 업데이트된 파라미터를 갖는 상기 학습된 인공지능 모델을 이용하여 제3 맵 데이터를 획득하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항에 있어서,상기 사용 환경 데이터는, 외부 장치의 하드웨어 성능 정보를 포함하고,상기 방법은:상기 하드웨어 성능 정보에 기초하여 기-결정된 수의 상기 적어도 하나의 키 프레임을 획득하는 단계; 및상기 기-결정된 수의 상기 적어도 하나의 키 프레임을 포함하는 상기 제2 맵 데이터를 상기 외부 장치에 전달하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항 내지 제18항 중 어느 한 항에 있어서,상기 방법은:현재 카메라 포즈를 획득하는 단계; 및상기 제2 맵 데이터에 기초하여 상기 현재 카메라 포즈의 오차를 보정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0125012", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제11항 내지 제19항 중 어느 한 항의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2023-0125012", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 동시적 위치 추정 및 맵 작성 방법 및 이를 수행하는 전자 장치를 제공한다. 전자 장치는 적어도 하나 의 인스트럭션을 저장하는 메모리, 및 적어도 하나의 인스트럭션을 실행하는 적어도 하나의 프로세서를 포함할 수 있다. 적어도 하나의 프로세서는, 제1 사용 환경에서 기-작성된 제1 맵 데이터를 이용하여 학습된 인공지능 (뒷면에 계속)"}
{"patent_id": "10-2023-0125012", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 동시적 위치 추정 및 맵 작성 방법 및 이를 수행하는 전자 장치에 관한 것으로, 좀 더 상세하게는, 인공지능 모델을 이용하여 다양한 사용 환경에 강건한 동시적 위치 추정 및 맵 작성을 수행하는 방법 및 이를수행하는 전자 장치에 관한 것이다."}
{"patent_id": "10-2023-0125012", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "동시적 위치 추정 및 맵 작성(SLAM; Simultaneous Localization and Mapping)은 로봇과 같은 전자 장치가 주변 환경을 인식하고 위치를 파악함과 동시에 지도를 작성하는 기술이다. 동시적 위치 추정 및 맵 작성 기술을 통해, 전자 장치 또는 전자 장치를 사용하는 사용자의 위치를 정확하게 추정함으로써, 공간 상의 객체와 인터랙 션할 수 있는 기술들이 최근 활발하게 연구되고 있다. 특히, 증강 현실(Augmented Reality) 디바이스가 착용한 상황에서 증강 현실 디바이스를 착용한 사용자의 공간 상의 위치를 정확하게 추정하고, 작성되어 저장된 맵을 강건하게 활용하는 기술들이 대두되고 있다. 이에 따라, 동시적 위치 추정 및 맵 작성 알고리즘의 정확성, 실시간성, 일관성, 환경 변화에 대한 적응성 등을 향상시키기 위한 다양한 기술과 알고리즘에 개발되고 있다."}
{"patent_id": "10-2023-0125012", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 일 실시예에 있어서, 전자 장치는 적어도 하나의 인스트럭션을 저장하는 메모리, 및 상기 적어도 하 나의 인스트럭션을 실행하는 적어도 하나의 프로세서를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 제1 사용 환경에서 기-작성된(pre-mapped) 제1 맵 데이터를 이용하여 학습된 인공지능 모델의 적어도 하나의 파라미 터를 획득하고, 제2 사용 환경에 대응하는 사용 환경 데이터를 획득하고, 상기 사용 환경 데이터에 기초하여 적 어도 하나의 키 프레임에 대응하는 카메라 포즈(camera pose) 및 이미지 평면(image plane)을 결정하고, 상기 카메라 포즈 및 이미지 평면에 기초하여, 카메라 위치에서 상기 이미지 평면 상의 픽셀을 향하는 광선(ray)의 방향 및 상기 광선 상의 지점의 3차원 좌표 값을 결정하고, 상기 적어도 하나의 파라미터를 갖는 상기 학습된 인공지능 모델에 상기 광선의 방향 및 상기 3차원 좌표 값을 입력하여, 상기 픽셀에 대응하는 이미지 픽셀 정보 를 획득하고, 상기 획득된 이미지 픽셀 정보에 기초하여 상기 적어도 하나의 키 프레임을 획득하고, 상기 획득 된 적어도 하나의 키 프레임에 기초하여 제2 맵 데이터를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 동시적 위치 추정 및 맵 작성 방법은 제1 사용 환경에서 기-작성된(pre-mapped) 제1 맵 데이터를 이용하여 학습된 인공지능 모델의 적어도 하나의 파라미터를 획득하는 단계, 제2 사용 환경에 대응하는 사용 환경 데이터를 획득하는 단계, 상기 사용 환경 데이터에 기초하여 적어도 하나의 키 프레임에 대 응하는 카메라 포즈 및 이미지 평면을 결정하는 단계, 상기 카메라 포즈 및 상기 이미지 평면에 기초하여, 카메 라 위치에서 상기 이미지 평면 상의 픽셀을 향하는 광선의 방향 및 상기 광선 상의 지점의 3차원 좌표 값을 결 정하는 단계, 상기 적어도 하나의 파라미터를 갖는 상기 학습된 인공지능 모델에 상기 광선의 방향 및 상기 3차 원 좌표 값을 입력하여, 이미지 픽셀 정보를 획득하는 단계, 상기 획득된 이미지 픽셀 정보에 기초하여 상기 적 어도 하나의 키 프레임을 획득하는 단계, 및 상기 획득된 적어도 하나의 키 프레임에 기초하여 제2 맵 데이터를 획득하는 단계를 포함할 수 있다. 본 개시의 일 실시예에 있어서, 동시적 위치 추정 및 맵 작성 방법을 컴퓨터에서 수행하기 위한 프로그램이 기 록된 컴퓨터로 읽을 수 있는 기록매체가 제공될 수 있다."}
{"patent_id": "10-2023-0125012", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서의 실시예들에서 사용되는 용어는 본 개시의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 실시예의 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 명세서에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 개시 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 본 명세서에 기재 된 \"...부\", \"...모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 본 개시에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for)\", \"~하는 능력을 가지는(having the capacity to)\", \"~하도록 설계된(designed to)\", \"~하도 록 변경된(adapted to)\", \"~하도록 만들어진(made to)\", 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 시스템\"이라는 표현은, 그 시 스템이 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수 행하도록 구성된(또는 설정된) 프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메모리에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 또한, 본 개시에서 일 구성요소가 다른 구성요소와 \"연결된다\" 거나 \"접속된다\" 등으로 언급된 때에는, 상기 일 구성요소가 상기 다른 구성요소와 직접 연결되거나 또는 직접 접속될 수도 있지만, 특별히 반대되는 기재가 존 재하지 않는 이상, 중간에 또 다른 구성요소를 매개하여 연결되거나 또는 접속될 수도 있다고 이해되어야 할 것 이다. 본 개시에서, '증강 현실(Augmented Reality)'은 현실 세계(Real world)의 물리적 환경 공간 내에 가상 이미지 를 함께 보여주거나 현실 객체와 가상 이미지를 함께 보여주는 것을 의미한다. 본 개시에서, '증강 현실 디바이스'는 증강 현실을 표현할 수 있는 장치로서, 일반적으로 사용자가 안면부(顔面 部)에 착용하는 안경 형상의 증강 현실 안경 장치(Augmented Reality Glasses) 뿐만 아니라, 두부(頭部)에 착용 하는 헤드 마운트 디스플레이 장치 (HMD: Head Mounted Display Apparatus)나, 증강 현실 헬멧(AugmentedReality Helmet) 등을 포괄한다. 그러나, 이에 한정되는 것은 아니고, 증강 현실 디바이스는 모바일 디바이스, 스마트 폰(smart phone), 노트북 컴퓨터(laptop computer), 데스크 탑, 태블릿 PC, 전자책 단말기, 디지털 방 송용 단말기, PDA(Personal Digital Assistants), PMP(Portable Multimedia Player), 네비게이션, MP3 플레이 어, 캠코더, IPTV(Internet Protocol Television), DTV(Digital Television), 웨어러블 디바이스(wearable device) 등과 같은 다양한 전자 장치로 구현될 수 있다. 본 개시에서, '인공지능(Artificial Intelligence)'과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로 세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저 장된 기 정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미 한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도 형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 본 개시에서, '인공지능 모델'은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망 모델은 심층 신경 망(DNN: Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), RBM(Restricted Boltzmann Machine), DBN(Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크(Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 예를 들어, 인공지능 모델은 뉴럴 렌더링 모델을 포함할 수 있다. 뉴럴 렌더링 모델은 입력 데이터로부터 시각 적 데이터를 출력하는 모델일 수 있다. 뉴럴 렌더링 모델은 임의의 조건의 2차원 이미지 및 이에 대응하는 정보 를 포함하는 데이터셋을 이용하여 학습될 수 있다. 학습된 뉴럴 렌더링 모델을 이용하여 학습되지 않은 조건의 2차원 이미지 및 이에 대응하는 정보가 추론될 수 있다. 본 개시에서, '맵 데이터'는 현실 공간의 구조와 특징을 나타내는 데이터를 의미할 수 있다. 맵 데이터는 전자 장치가 현실 공간 내의 자신의 위치를 추정할 때 기준이 되는 데이터일 수 있다. 전자 장치는 센서를 이용하여 주변 환경을 탐색하고, 주변 환경에 대한 정보를 맵 데이터로 작성할 수 있다. 맵 데이터는 전자 장치가 자신의 위치를 정확하게 파악하고, 동시에 현실 공간 내에서의 경로를 계획하는 데 이용될 수 있다. 본 개시에서, '키 프레임'은 맵 데이터를 구성하는 이미지 프레임들을 나타낼 수 있다. 키 프레임은 공간에 대 응하는 복수의 이미지 프레임들 중 특이성 및 대표성을 갖는 이미지 프레임을 나타낼 수 있다. 예를 들어, 키 프레임은 복수의 이미지 프레임들 중 기-결정된 시간적 또는 공간적 간격을 갖는 일부 이미지 프레임들을 나타 낼 수 있다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 도 1은 본 개시의 일 실시예에 따른 전자 장치의 동작을 설명하기 위한 개념도이다. 도 1을 참조하면, 전자 장치는 주변 환경을 감지하고 특정 위치 또는 공간에서 소정의 서비스를 제공하는 기기일 수 있다. 전자 장치는 공간을 스캔하고 공간 내의 객체를 검출하기 위한 다양한 종류의 센서와 인 공지능 모델을 탑재할 수 있다. 예를 들어, 전자 장치는 카메라와 같은 이미지 센서, 깊이 센서, 라이다(LiDAR, Light Detection And Ranging) 센서, ToF(Time of Flight) 센서, 및 IMU(Inertial Measurement Unit) 센서 중 적어도 하나를 포함할 수 있다. 전자 장치는 센서를 통해 획득한 이미지 프레임을 이용하여 전자 장치 또는 이미지 프레임에 대응하는 센서의 위치를 추정하고, 이미지 프레임에 대응하는 공간에 대한 맵 작성(mapping)을 수행할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 사용자의 안면부(顔面部)에 착용하는 안경 형상의 증강 현실 안경 장치(Augmented Reality Glasses)로 구현될 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 사용자의 두부(頭部)에 착용하는 헤드 마운트 디스플레이 장치 (HMD; Head Mounted Display Apparatus) 또는 증강 현실 헬멧 등의 장치로 구현될 수 있다. 그러나 본 개시가 이에 한정되는 것은 아니며, 전자 장치는 모바일 디바이스, 스마트 폰(smart phone), 노트북 컴퓨터(laptop computer), 데스크 탑, 태블릿 PC, 전자책 단말기, 디지털 방송용 단말기, PDA(Personal Digital Assistants), PMP(Portable Multimedia Player), 네비게 이션, MP3 플레이어, 캠코더, IPTV(Internet Protocol Television), DTV(Digital Television), 웨어러블 디바 이스(wearable device), 가상 현실 디바이스 등과 같은 다양한 전자 장치로 구현될 수 있다. 이하에서, 설명의 편의를 위해, 전자 장치가 증강 현실 디바이스인 것을 전제하여 설명한다. 본 개시의 일 실시예에 있어서, 전자 장치는 메모리 및 프로세서를 포함할 수 있다. 그러나, 도 1에 도시된 구성요소가 필수 구성요소인 것은 아니며, 전자 장치는 추가 구성요소를 더 포함할 수 있다. 전자 장치의 구성 요소에 대해서는 도 2에서 상세하게 설명하기로 한다. 프로세서는 사용 환경 데이터를 이용하여 맵 데이터를 변환할 수 있다. 프로세서는 제1 사용 환경에 서 기-작성된 제1 맵 데이터를 획득할 수 있다. 예를 들어, 프로세서는 제1 사용 환경에서 수행된 동시적 위치 추정 및 맵 작성을 통해 작성된 제1 맵 데이터를 획득할 수 있다. 예를 들어, 제1 맵 데이터는 적어도 하 나의 키 프레임을 포함할 수 있다. 예를 들어, 도 1에 도시된 바와 같이, 제1 사용자가 전자 장치(10 0)를 이용하는 상황을 전제한다. 제1 사용자의 사용 패턴(예컨대, 공간의 바닥 방향을 향하여 전자 장치 를 사용) 및 키 중 적어도 하나에 따라, 전자 장치의 카메라의 시야각이 제한될 수 있다. 전자 장치 는 제한된 시야각에서 공간을 촬영한 복수의 이미지 프레임들을 획득할 수 있다. 전자 장치는 복수의 이미지 프레임들에 기초하여 위치 추정에 필요한 적어도 하나의 키 프레임을 결정할 수 있다. 적어도 하나 의 키 프레임은 공간의 바닥 방향을 촬영한 이미지 프레임일 수 있다. 전자 장치는 적어도 하나의 키 프레임을 이용하여 전자 장치의 위치를 추정할 수 있다. 예를 들어, 전자 장치는 현재 이미지 프 레임을 획득할 수 있다. 전자 장치는 현재 이미지 프레임과 적어도 하나의 키 프레임을 비교할 수 있 다. 전자 장치는 비교 결과에 기초하여 전자 장치의 현재 위치를 추정할 수 있다. 프로세서는 사용 환경 데이터를 획득할 수 있다. 예를 들어, 사용 환경 데이터는 전자 장치 또는 맵 데이터를 이용하는 외부 장치의 사용 환경에 대한 데이터를 포함할 수 있다. 사용 환경 데이터는 제1 맵 데이터 에 대응하는 제1 사용 환경과 다른 제2 사용 환경에 대한 데이터일 수 있다. 예를 들어, 사용 환경 데이터는 전 자 장치 또는 외부 장치의 사용자에 대한 정보(예컨대, 사용자의 키, 사용자의 사용 패턴 등), 전자 장치 또는 외부 장치가 동시적 위치 추정 및 맵 작성(SLAM; Simultaneous Localization and Mapping)을 수행 하는 공간에 대한 정보(예컨대, 공간의 조명 정보, 공간 상의 객체 정보 등), 및 전자 장치 또는 외부 장 치의 하드웨어 성능 정보(예컨대, 메모리 크기, 프로세서 처리 성능 등)를 포함할 수 있다. 프로세서는 사 용 환경 데이터를 이용하여, 제1 맵 데이터를 제2 맵 데이터로 변환할 수 있다. 본 개시의 일 실시예에 있어서, 도 1에 도시된 바와 달리, 사용 환경 데이터는 메모리이 미리 저장되어 있을 수 있다. 프로세서는 메 모리로부터 사용 환경 데이터를 로드할 수 있다. 프로세서는 제2 맵 데이터를 생성할 수 있다. 예를 들어, 제2 맵 데이터는 적어도 하나의 키 프레임을 포함할 수 있다. 예를 들어, 도 1에 도시된 바와 같이, 제2 사용자가 전자 장치를 이용하는 상황을 전 제한다. 제2 사용자의 사용 패턴(예컨대, 공간의 천장 방향을 향하여 전자 장치를 사용) 및 키 중 적 어도 하나에 따라, 전자 장치의 카메라의 시야각이 제한될 수 있다. 전자 장치는 제한된 시야각에서 공간을 촬영한 복수의 이미지 프레임들을 획득할 수 있다. 전자 장치는 복수의 이미지 프레임들에 기초하 여 위치 추정에 필요한 적어도 하나의 키 프레임을 결정할 수 있다. 적어도 하나의 키 프레임은 공간의 천장 방향을 촬영한 이미지 프레임일 수 있다. 전자 장치는 적어도 하나의 키 프레임을 이용하여 전자 장치의 위치를 추정할 수 있다. 예를 들어, 전자 장치는 현재 이미지 프레임을 획득할 수 있다. 전자 장치는 현재 이미지 프레임과 적어도 하나의 키 프레임을 비교할 수 있다. 전자 장치는 비교 결 과에 기초하여 전자 장치의 현재 위치를 추정할 수 있다.본 개시의 일 실시예에 따르면, 전자 장치가 사용 환경을 고려하여 맵 데이터를 변환함으로써, 사용 환경 에 변화가 생기더라도 동시적 위치 추정 및 맵 작성을 효율적으로 수행할 수 있다. 예를 들어, 제1 사용자 와 제2 사용자가 동일한 전자 장치를 공유하는 경우를 전제한다. 제1 사용자가 전자 장치를 이용할 때 생성된 제1 맵 데이터를 활용하여 제2 사용자가 전자 장치를 이용할 때 사용되는 제2 맵 데 이터를 생성할 수 있다. 결과적으로, 전자 장치의 사용자가 변경되더라도 동시적 위치 추정 및 맵 작성의 정확도가 유지될 수 있다. 본 개시의 일 실시예에 있어서, 프로세서는 인공지능 모델을 이용하여 맵 데이터를 생성할 수 있다. 예를 들어, 인공지능 모델은 주어진 조건의 이미지 프레임을 데이터셋으로 학습함으로써, 주어지지 않은 조건의 이미 지 프레임을 추론하는 뉴럴 렌더링 모델일 수 있다. 프로세서는 제1 사용 환경에서 기-작성된 제1 맵 데이 터를 이용하여 인공지능 모델을 사전에 학습시킬 수 있다. 그러나, 본 개시는 이에 한정되지 않으며, 인공지능 모델은 외부 서버에서 사전에 학습될 수 있다. 본 개시의 일 실시예에 있어서, 인공지능 모델은 제1 맵 데이터 중 적어도 하나의 키 프레임을 입력으로 하여 공간에 대응하는 이미지를 출력하도록 학습될 수 있다. 본 개시의 일 실시예에 있어서, 인공지능 모델은 제1 맵 데이터 중 포인트 클라우드 및 적어도 하나의 키 프레임을 입력으 로 하여 공간에 대응하는 이미지를 출력하도록 학습될 수 있다. 본 개시의 일 실시예에 있어서, 프로세서는 사전에 학습된 인공지능 모델에 대응하는 적어도 하나의 파라 미터를 메모리에 저장할 수 있다. 프로세서는 적어도 하나의 파라미터를 메모리로부터 로드할 수 있다. 프로세서는 적어도 하나의 파라미터를 갖는 학습된 인공지능 모델에 사용 환경 데이터 또는 사용 환경 데이터를 전처리한 데이터를 입력으로 하여, 제2 맵 데이터 또는 제2 맵 데이터에 대응하는 원시(raw) 데 이터를 추론할 수 있다. 프로세서는 인공지능 모델을 이용하여 제2 맵 데이터를 생성할 수 있다. 본 개시의 일 실시예에 있어서, 프로세서의 적어도 일부의 기능은 외부 장치에 의해 수행될 수 있다. 본 개시의 일 실시예에 있어서, 도 1에서 파선으로 표시된 바와 같이, 프로세서는 인공지능 모델을 학습시 키고 학습된 인공지능 모델에 대응하는 적어도 하나의 파라미터를 메모리에 저장할 수 있다. 예를 들어, 프로세서는 제1 사용 환경의 이미지 프레임을 획득할 수 있다. 프로세서는 제1 사용 환경의 이미지 프레임에 기초하여, 카메라 포즈를 획득할 수 있다. 프로세서는 카메라 포즈 및 이미지 프레임에 기초하여 제1 맵 데이터를 생성할 수 있다. 예를 들어, 제1 맵 데이터는 적어도 하나의 키 프레임을 포함할 수 있다. 프 로세서는 작성된 제1 맵 데이터를 이용하여 인공지능 모델을 학습시킬 수 있다. 학습된 인공지능 모델에 대응하는 적어도 하나의 파라미터는 메모리에 저장될 수 있다. 본 개시의 일 실시예에 따르면, 맵 데이터가 메모리에 저장되는 대신, 학습된 인공지능 모델에 대응하는 적어도 하나의 파라미터가 메모리에 저장됨으로써, 전자 장치의 메모리 사용량을 줄일 수 있다. 본 개시의 일 실시예에 있어서, 도 1에서 실선으로 표시된 바와 같이, 전자 장치는 학습된 인공지능 모델 을 이용하여 동시적 위치 추정 및 맵 작성을 수행할 수 있다. 예를 들어, 프로세서는 기-저장된 적어도 하 나의 파라미터를 메모리로부터 획득할 수 있다. 프로세서는 제2 사용 환경에 대응하는 사용 환경 데 이터를 획득할 수 있다. 프로세서는 적어도 하나의 파라미터를 갖는 학습된 인공지능 모델에 사용 환경 데 이터 (또는 사용 환경 데이터에 기초하여 전처리된 데이터)를 입력하여, 제2 사용 환경에 대응하는 제2 맵 데이 터(또는 제2 맵 데이터에 대응하는 원시 데이터)를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 프로세서 는 학습된 인공지능 모델을 이용하여 제2 사용 환경에 대응하는 적어도 하나의 키 프레임을 획득할 수 있 다. 프로세서는 제2 사용 환경의 이미지 프레임을 획득할 수 있다. 프로세서는 제2 사용 환경의 이미 지 프레임에 기초하여, 카메라 포즈(camera pose)를 획득할 수 있다. 프로세서는 제2 맵 데이터에 기초하 여 카메라 포즈의 오차를 보정할 수 있다. 도 2는 본 개시의 일 실시예에 따른 전자 장치의 구성 요소를 보여주는 블록이다. 전자 장치의 구성, 동작, 및 기능은 도 1의 전자 장치의 구성, 동작, 및 기능에 대응할 수 있다. 설명의 편의를 위해, 도 1에 서 설명한 내용과 중복되는 내용은 생략한다. 도 2를 참조하면, 본 개시의 일 실시예에 따른 전자 장치는 카메라, 센서, 통신 인터페이스 , 사용자 인터페이스, 프로세서, 및 메모리를 포함할 수 있다. 그러나, 도 2에 도시된 구 성요소가 필수 구성요소인 것은 아니며, 전자 장치는 구성요소를 생략하거나, 추가 구성요소를 더 포함할 수 있다. 예를 들어, 본 개시의 일 실시예에 따른 전자 장치에, 도 2에 도시된 바와 달리, 전자 장치(20 0)는 센서가 생략될 수 있다.본 개시의 일 실시예에 있어서, 전자 장치는 휴대용 장치로 구현되고, 이 경우 전자 장치는 카메라 , 센서, 통신 인터페이스, 사용자 인터페이스, 프로세서, 및 메모리에 전원을 공급하는 배터리를 더 포함할 수 있다. 카메라는 공간을 촬영함으로써, 공간에 관한 적어도 하나의 이미지 또는 동영상을 획득할 수 있다. 예를 들어, 공간은 밀폐된 실내 공간일 수 있으나, 본 개시는 이에 제한되지 않으며, 실외 공간일 수 있다. 카메라 는 RGB 신호를 획득하고, RGB 신호로부터 공간에 대한 이미지를 생성할 수 있다. 본 개시의 일 실시예에 있어서, 카메라는 단일 카메라로 구성되거나, 스테레오 카메라 쌍으로 구성될 수 있으나, 본 개시는 이에 제한되지 않으며, 카메라는 적어도 하나의 카메라로 구성될 수 있다. 예를 들어, 카메라는 제1 카메라 및 제2 카메라를 포함할 수 있다. 예를 들어, 제1 카메라는 좌안에 대응하고, 제2 카 메라는 우안에 대응할 수 있으나, 본 개시는 이에 제한되지 않는다. 제1 카메라 및 제2 카메라는 시야(Field of View)가 겹치는 영역에서 획득된 2차원 이미지와 카메라 간의 위치 관계에 기초하여 삼각측량법(triangulatio n)을 통해 객체의 3차원 위치 좌표값을 획득하는 스테레오 카메라를 구성할 수 있다. 본 개시의 일 실시예에 있어서, 카메라는 가상 현실 디바이스 또는 증강 현실 디바이스에 장착될 수 있도 록 소형 폼 팩터(form factor)로 구현되고, 저전력을 소비하는 경량 RGB 카메라일 수 있다. 그러나, 이에 한정 되는 것은 아니며, 본 개시의 일 실시예에 있어서, 카메라는 깊이 추정 기능을 포함하는 RGB-D 카메라, 스 테레오 어안 카메라, 그레이스케일 카메라, 또는 적외선 카메라 등 공지의 모든 종류의 카메라로 구현될 수 있 다. 본 개시의 일 실시예에 있어서, 카메라는 렌즈 모듈, 이미지 센서, 및 이미지 프로세싱 모듈을 포함할 수 있다. 카메라는 이미지 센서(예를 들어, CMOS 또는 CCD)에 의해 현실 객체 및 사용자의 손에 관한 정지 이 미지(still image) 또는 동영상(video)을 획득할 수 있다. 동영상은 카메라를 통해 공간을 촬영함으로써 실시간으로 획득되는 복수의 이미지 프레임을 포함할 수 있다. 이미지 프로세싱 모듈은 이미지 센서를 통해 획 득된 단일 이미지 프레임으로 구성된 정지 이미지 또는 복수의 이미지 프레임으로 구성된 동영상 데이터를 인코 딩할 수 있다. 센서는 물리량을 계량하거나 감지함으로써, 계량 또는 감지된 정보를 전기 신호로 변환할 수 있다. 센서 는 변환된 전기 신호를 프로세서에 전달할 수 있다. 프로세서는 전기 신호를 프로세싱하여 센서 데이터를 생성할 수 있다. 예를 들어, 센서는 IMU 센서, 라이다 센서, 깊이 센서, ToF 센서, 터치 입력을 위한 적어도 하나의 버튼, 마이크 센서, 제스처 센서, 자이로스코프, 자이로 센서, 기압 센서, 자기 센서, 자력 계, 가속도 센서, 가속도계, 그립 센서, 근접 센서, 이미지 센서, 생체물리 센서, 온도 센서, 습도 센서, 조도 센서, 자외선 센서, 근전도 센서, 뇌파 센서, 심전도 센서, 적외선 센서, 초음파 센서, 홍채 센서, 또는 지문 센서 중 적어도 하나를 포함할 수 있으나, 본 개시는 이에 한정되지 않는다. 본 개시의 일 실시예에 있어서, 센서가 IMU 센서를 포함하는 경우, IMU 센서는 전자 장치의 운동에 대한 정보를 측정할 수 있다. IMU 센서는 가속도계(Accelerometer)와 자이로스코프(Gyroscope)를 포함할 수 있 다. 가속도계는 특정 축에 대한 가속도를 측정할 수 있다. 가속도계는 전자 장치의 운동의 방향 및 크기를 측정할 수 있다. 자이로스코프는 특정 축에 대한 회전 속도를 측정할 수 있다. 자이로스코프는 전자 장치 의 회전 운동의 방향과 각도의 변화를 측정할 수 있다. IMU 센서는 가속도계에 의해 측정된 데이터와 자이로스 코프에 의해 측정된 데이터를 활용하여, 전자 장치의 위치 관계 변화, 속도, 가속도 등을 추정할 수 있다. 본 개시의 일 실시예에 있어서, 센서가 라이다 센서를 포함하는 경우, 라이다 센서는 공간을 감지하여 3차 원 포인트 클라우드 형태의 데이터를 획득할 수 있다. 통신 인터페이스는 전자 장치와 외부의 다른 전자 장치(미도시) 또는 서버(미도시) 사이의 유선 또는 무선 통신 채널의 수립 및 수립된 통신 채널을 통한 통신 수행을 지원할 수 있다. 일 실시 예에 있어서, 통신 인터페이스는 유선 또는 무선 통신을 통해 외부의 다른 전자 장치(미도시) 또는 서버(미도시)로부터 데이 터를 수신하거나 외부의 다른 전자 장치(미도시) 또는 서버(미도시)로 데이터를 송신할 수 있다. 본 개시의 일 실시예에 있어서, 통신 인터페이스는 무선 통신 모듈(예컨대, 셀룰러 통신 모듈, 근거리 무 선 통신 모듈, 또는 GNSS(global navigation satellite system) 통신 모듈) 또는 유선 통신 모듈(예컨대, LAN(local area network) 통신 모듈, 또는 전력선 통신 모듈)을 포함할 수 있고, 그 중 어느 하나의 통신 모듈 을 이용하여 적어도 하나의 네트워크(예컨대, 근거리 통신 네트워크(예컨대, 블루투스, WiFi direct 또는 IrDA(infrared data association)) 또는 원거리 통신 네트워크(예컨대, 셀룰러 네트워크, 인터넷, 또는 컴퓨터네트워크(예컨대, LAN 또는 WAN)))를 통하여 외부의 다른 전자 장치(미도시) 또는 서버(미도시)와 통신할 수 있 다. 본 개시의 일 실시예에 있어서, 전자 장치는 통신 인터페이스를 통해 외부 서버로부터 맵 데이터 를 수신할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 통신 인터페이스를 통해 외부 서버 로부터 학습된 인공지능 모델의 적어도 하나의 파라미터를 수신할 수 있다. 사용자 인터페이스는 입력 인터페이스 및 출력 인터페이스를 포함할 수 있다. 입력 인터페이스는, 사용자로부터의 입력(이하에서, 사용자 입력)을 수신하기 위한 것이다. 입력 인터페이스는 키 패드(key pad), 돔 스위치 (dome switch), 터치 패드(접촉식 정전 용량 방식, 압력식 저항막 방식, 적외선 감지 방식, 표면 초음파 전도 방식, 적분식 장력 측정 방식, 피에조 효과 방식 등), 조그 휠, 조그 스위치 중 적어도 하나일 수 있으나, 이에 한정되는 것은 아니다. 입력 인터페이스는 음성 인식 모듈을 포함할 수 있다. 예를 들어, 전자 장치는 마이크로폰(또는 마이크 센서로도 지칭될 수 있음)을 통해 아날로그 신호인 음성 신호를 수신하고, ASR(Automatic Speech Recognition) 모델을 이용하여 음성 부분을 컴퓨터로 판독 가능한 텍스 트로 변환할 수 있다. 전자 장치는 자연어 이해(Natural Language Understanding, NLU) 모델을 이용하여 변환된 텍스트를 해석하여, 사용자의 발화(utterance) 의도를 획득할 수 있다. 여기서 ASR 모델 또는 NLU 모델 은 인공지능 모델일 수 있다. 인공지능 모델은 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계된 인공지 능 전용 프로세서에 의해 처리될 수 있다. 인공지능 모델은 학습을 통해 만들어 질 수 있다. 여기서, 학습을 통 해 만들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨 으로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미한다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행한다. 언어적 이해는 인간의 언어/문자를 인식하고 응용/처리하는 기술로서, 자연어 처리(Natural Language Processing), 기계 번역(Machine Translation), 대화 시스템(Dialog System), 질의 응답(Question Answering), 음성 인식/합성(Speech Recognition/Synthesis) 등을 포함한다. 출력 인터페이스는 오디오 신호 또는 비디오 신호의 출력을 위한 것으로, 예컨대 디스플레이 또는 스피커 등을 포함할 수 있다. 예를 들어, 디스플레이는 액정 디스플레이(liquid crystal display), 박막 트랜지스터 액정 디 스플레이(thin film transistor-liquid crystal display), 발광 다이오드(LED, light-emitting diode), 유기 발광 다이오드(organic light-emitting diode), 플렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display), 전기영동 디스플레이(electrophoretic display) 중에서 적어도 하나를 포함할 수 있다. 그리고 전자 장치의 구현 형태에 따라 디스플레이를 2개 이상 포함할 수도 있다. 예를 들어, 스피커는 통신 인터페이스 로부터 수신되거나 메모리에 저장된 오디오 신호를 출력할 수 있다. 본 개시의 일 실시예에 있어서, 디스플레이 와 터치패드가 레이어 구조를 이루어 터치 스크린으로 구성되는 경우, 디스플레이는 출력 인터페이스 이외에 입 력 인터페이스로도 사용될 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 디스플레이를 통해 사용자 정보를 입력하기 위한 화면을 표시 할 수 있다. 전자 장치는 스피커를 통해 사용자 정보를 입력하라는 오디오 신호를 출력할 수 있다. 전자 장치는 입력 인터페이스를 통해 사용자 정보를 획득할 수 있다. 전자 장치는 사용자 정보를 메모리 에 저장할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 디스플레이를 통해 사용자의 키를 입력하기 위한 화면을 표시 할 수 있다. 전자 장치는 사용자의 키에 대응하는 사용자 입력을 입력 인터페이스를 통해 획득할 수 있다. 프로세서는 AP(application processor), CPU(central processing unit) 또는 GPU(graphic processing unit)와 같은 범용 프로세서와 소프트웨어의 조합을 통해 구현될 수도 있다. 전용 프로세서의 경우, 본 개시의 실시예를 구현하기 위한 메모리를 포함하거나, 외부 메모리를 이용하기 위한 메모리 처리부를 포함할 수 있다. 프로세서는 복수의 프로세서로 구성될 수도 있다. 이 경우, 전용 프로세서들의 조합으로 구현될 수도 있고, AP, CPU 또는 GPU와 같은 다수의 범용 프로세서들과 소프트웨어의 조합을 통해 구현될 수도 있다. 본 개시의 일 실시예에 있어서, 프로세서는, 인공지능(AI) 프로세서를 탑재할 수도 있다. 인공지능(AI) 프 로세서는, 인공지능(AI)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 기존의 범용 프로세서(예: CPU 또 는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 전자 장치에 탑재될 수도 있다. 예를 들어, 인공지능 프로세서는 적어도 하나의 인공지능 모델과 관련된 학습 및/또는 추론에 필요한 데이터 처리를 수행할 수 있다. 본 개시에 따른 인공지능과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하 나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인공지능 모델(예컨대, 심층 신경망 모델)에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델 의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 본 개시의 일 실시예에 있어서, 맵 변환 모듈은 인공 지능 모델을 포함할 수 있다. 인공지능 모델을 학습하고 추론하는 데 인공지능 전용 프로세서가 사용될 수 있다. 메모리는, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수도 있고, 입/출력되는 데이터들을 저장할 수도 있다. 메모리는 적어도 하나의 인공지능 모델을 저장할 수도 있다. 예를 들어, 메모리는 학습된 인공지능 모델에 대응하는 적어도 하나의 파라미터를 저장할 수 있다. 메모리는 맵 데이터를 저장 할 수도 있다. 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 또한, 전자 장치는 인터넷(internet)상에서 저장 기능을 수행하는 웹 스토리지(web storage) 또는 클라우드 서버를 운영할 수도 있 다. 본 개시의 일 실시예에 있어서, 메모리에는 프로세서에 의하여 처리되거나 처리될 예정인 데이터, 펌 웨어, 소프트웨어, 및 프로세스 코드 등이 저장될 수 있다. 본 개시의 일 실시예에 있어서, 메모리에는 트래커 모듈, 매퍼 모듈, 모델 파라미터 중 적 어도 하나에 대응되는 데이터 및 프로그램 코드들이 저장될 수 있다. 본 개시의 일 실시예에 있어서, 트래커 모 듈 및 매퍼 모듈이 특정 기능을 수행하는 것으로 설명되나, 트래커 모듈 및 매퍼 모듈의 적어도 하나의 기능은 프로세서에 의해 처리될 수 있다. 본 개시의 일 실시예에 있어서, 트래커 모듈 및 매퍼 모듈이 메모리에 포함된 프로세스 코드인 것으로 설명되나, 본 개시는 이제 제한되지 않으며, 도 2에 도시된 바와 달리, 트래커 모듈 및 매퍼 모듈 각각이 하드웨어로 구성될 수 있다. 트래커 모듈는 카메라를 이용하여 공간에 대응하는 이미지 프레임을 획득할 수 있다. 트래커 모듈 은 센서를 이용하여 공간에 대응하는 센서 데이터를 획득할 수 있다. 트래커 모듈은 이미지 프 레임 및 센서 데이터 중 적어도 하나를 이용하여 공간 상의 전자 장치의 위치를 추정할 수 있다. 트래커 모듈은 이미지 프레임을 이용하여 전자 장치 또는 전자 장치의 카메라(또는 센서 )의 위치를 추정할 수 있다. 예를 들어, 이미지 프레임은 단일 카메라 또는 스테레오 카메라 쌍으로부터 획득한 이미지 프레임을 나타낼 수 있다. 예를 들어, 단일 카메라 또는 스테레오 카메라 쌍은 RGB 카메라 또는 RGB-D 카메라를 포함할 수 있다. 트래커 모듈은 카메라 외의 다른 센서로부터 획득한 데이터를 이용하여 전자 장치 또는 전자 장치의 센서의 위치를 추정할 수 있다. 예를 들어, 트래커 모듈 은, 이미지 프레임과 함께, 깊이 센서, 라이다 센서, ToF 센서, 및 IMU 센서 중 적어도 하나를 이용하여 획득한 센서 데이터를 이용하여 전자 장치 또는 전자 장치의 카메라(또는 센서)의 위치를 추정할 수 있다. 본 개시의 일 실시예에 있어서, 트래커 모듈은 6DoF(6 Degrees of Freedom) 값을 추정할 수 있 다. 6DoF 값은 전자 장치 또는 전자 장치의 카메라(또는 센서)에 대한 3차원 공간 상의 위 치 및 운동 방향에 대한 정보를 나타낼 수 있다. 트래커 모듈은 이미지 프레임에 대응하는 카메라 포즈를 획득할 수 있다. 예를 들어, 카메라 포즈는 3차원 공간 상에서의 카메라의 위치 및 카메라의 촬영 방향을 포함할 수 있다. 트래커 모듈은 카메라 포즈를 이용하여 전자 장치의 위치를 추정할 수 있다. 트래커 모듈은 현 재 이미지 프레임의 카메라 포즈와 이전 이미지 프레임의 카메라 포즈를 비교할 수 있다. 트래커 모듈은 현재 이미지 프레임과 이전 이미지 프레임 사이의 카메라 포즈의 변화량을 계산할 수 있다.본 개시의 일 실시예에 있어서, 트래커 모듈은 현재 이미지 프레임의 특징점을 검출할 수 있다. 예를 들어, 특징점은 이미지 프레임의 고유 지점을 나타낼 수 있다. 트래커 모듈은 현재 이미지 프레임의 특징 점과 이전 이미지 프레임의 특징점을 매칭시킬 수 있다. 트래커 모듈은 이미지 프레임의 특징점을 검출하 고 매칭시키는 다양한 알고리즘을 사용할 수 있다. 트래커 모듈은 매칭된 특징점 쌍을 이용하여 현재 이미 지 프레임에 대응하는 카메라 포즈 또는 카메라 포즈의 변화량(또는 카메라의 움직임)을 추정할 수 있다. 설명 의 편의를 위해, 이하에서, 현재 이미지 프레임에 대응하는 카메라 포즈 또는 카메라 포즈의 변화량은 카메라 포즈로 지칭될 수 있다. 트래커 모듈은 추정된 카메라 포즈 또는 카메라 포즈의 변화량을 매퍼 모듈 에 전달할 수 있다. 매퍼 모듈은 맵 데이터를 이용하여 트래커 모듈의 위치 추정 결과의 오차를 보정할 수 있다. 매퍼 모 듈은 모델 파라미터(적어도 하나의 파라미터로도 지칭될 수 있음)를 갖는 학습된 인공지능 모델을 이 용하여, 사용 환경에 맞는 맵 데이터를 생성할 수 있다. 본 개시의 일 실시예에 있어서, 매퍼 모듈은 기존 맵 데이터를 활용하거나, 및/또는 학습된 인공지능 모델을 이용하여 생성된 맵 데이터를 활용하여 최종 맵 데이 터를 생성할 수 있다. 매퍼 모듈은 트래커 모듈로부터 카메라 포즈를 수신할 수 있다. 매퍼 모듈은 맵 데이터를 이용 하여 카메라 포즈의 오차를 보정할 수 있다. 매퍼 모듈은 사용 환경 데이터를 이용하여 맵 데이터를 생성 할 수 있다. 예를 들어, 매퍼 모듈은 적어도 하나의 파라미터(또는 모델 파라미터로 지칭될 수도 있음)를 갖는 인공지능 모델을 이용하여 사용 환경을 고려한 맵 데이터를 생성할 수 있다. 본 개시의 일 실시예에 있어서, 매퍼 모듈은 복수의 이미지 프레임들 중 키 프레임을 결정할 수 있다. 예 를 들어, 매퍼 모듈은 복수의 이미지 프레임들간의 시간 간격, 복수의 이미지 프레임들간의 카메라 포즈 변화량, 복수의 이미지 프레임들간의 깊이 정보 차이, 및 복수의 이미지 프레임들 간의 특징점 분포의 차이 중 적어도 하나가 기 정의된 임계 값을 초과하는지 여부에 기초하여 적어도 하나의 키 프레임을 결정할 수 있다. 도시되지 않았지만, 매퍼 모듈은 현재 이미지 프레임을 수신할 수 있다. 매퍼 모듈은 기존의 키 프레 임들과 현재 이미지 프레임을 비교하여, 비교 결과에 따라 현재 이미지 프레임을 키 프레임으로 결정할 수 있다. 매퍼 모듈은 키 프레임을 포함하는 맵 데이터를 생성할 수 있다. 본 개시의 일 실시예에 있어서, 매퍼 모듈은 맵 변환 모듈 및 보정 모듈을 포함할 수 있다. 맵 변환 모듈은 인공지능 모델을 포함할 수 있다. 인공지능 모델은 제1 사용 환경에서 생성된, 특정 공간에 대응하는 제1 맵 데이터를 데이터셋으로 하여, 3차원 공간의 렌더링에 대응하는 데이터를 추론하도록 학습될 수 있다. 예를 들어, 인공지능 모델은 카메라 위치에서 이미지 평면 상의 픽셀을 향하는 방향과, 3차원 공간 상의 좌표 값을 입력으로 하여, 좌표 값에 대응하는 색상 값 및 밀도 값을 추론할 수 있다. 도 2와 함께, 도 3b를 참조하면, 본 개시의 일 실시예에 있어서, 인공지능 모델의 입력은 5차원 데이터일 수 있 다. 전자 장치는 카메라 위치(예컨대, O)에서 이미지 평면 상의 픽셀을 바라보는 광선(ray)의 방향(예컨대, ()) 및 이미지 픽셀의 좌표 값을 전처리하여 인공지능 모델로 입력할 수 있다. 예를 들어, 광 선 방향(또는 광선의 방향으로도 지칭될 수 있음)은 구면좌표계에서 카메라 위치(예컨대, O)에서 이미지 평면 상의 픽셀을 잇는 광선의 방향을 나타낼 수 있다. 는 방위각을 나타낼 수 있다. 예를 들어, 는 z 축의 양의 방향으로부터 카메라 위치(예컨대, O)와 픽셀이 이루는 직선까지의 각도를 나타낼 수 있다. 는 고도각을 나타 낼 수 있다. 예를 들어, 는 x 축의 양의 방향으로부터 카메라 위치(예컨대, O)와 픽셀이 이루는 직선을 xy 평 면 상에 투영시킨 직선까지의 각도를 나타낼 수 있다. 예를 들어, 전자 장치는 카메라 위치(예컨대, O)에 서 이미지 픽셀(또는 이미지 평면 상의 픽셀로도 지칭될 수 있음)의 좌표 값으로 향하는 임의의 광선(ray)을 설 정할 수 있다. 전자 장치는 임의의 광선 상의 복수의 지점들을 샘플링할 수 있다. 전자 장치는 기 결 정된 샘플링 간격을 갖도록 복수의 지점들을 샘플링할 수 있다. 전자 장치는 샘플링된 지점들에 대응하는 3차원 좌표 값(예컨대, (x, y, z))을 계산할 수 있다. 전자 장치는 광선의 방향(예컨대, ())과 광선 상의 지점의 3차원 좌표 값(예컨대, (x, y, z))으로 구성되는 5차원 데이터(예컨대, (x, y, z, ))를 인공 지능 모델에 입력할 수 있다.본 개시의 일 실시예에 있어서, 인공지능 모델의 출력은 4차원 데이터일 수 있다. 전자 장치는 인공지능 모델을 이용하여, 3차원 좌표 값(예컨대, (x, y, z))에 대응하는 색상 값(예컨대, (R, G, B)) 및 밀도 값(예컨 대, σ)을 추론할 수 있다. 전자 장치는 색상 값(예컨대, (R, G, B)) 및 밀도 값(예컨대, σ)을 처리하여 이미지 픽셀의 좌표 값에 대응하는 색상 값(예컨대, (R, G, B))을 생성할 수 있다. 예를 들어, 전자 장치 는, 3차원 좌표 값(예컨대, (x, y, z))에 대하여, 색상 값에 밀도 값을 곱할 수 있다. 전자 장치는 이미지 픽셀의 좌표 값이 같은 복수의 지점들에 대응하는 곱셈 결과를 모두 합하여, 이미지 픽셀의 좌표 값에 대응하는 색상 값(예컨대, (R, G, B))을 생성할 수 있다. 맵 변환 모듈은 좌표 값과 대응하는 색상 값에 기초하여 3차원 공간에 대응하는 이미지 프레임들을 생성할 수 있다. 맵 변환 모듈은 생성된 이미지 프레임들에 기초하여 공간을 렌더링할 수 있다. 매퍼 모듈은 학습된 인공지능 모델에 대응하는 적어도 하나의 파라미터를 메모리에 저장할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 제1 맵 데이터의 포인트 클라우드에 가우시안(Gaussian) 필터 를 적용함으로써 3차원 가우시안 값을 획득할 수 있다. 가우시안 필터는 가우시안 분포 함수를 근사하여 생성한 필터 마스크를 나타낼 수 있다. 전자 장치는 제1 맵 데이터의 적어도 하나의 키 프레임 및 3차원 가우시안 값을 학습 데이터로 하여, 인공지능 모델을 학습시킬 수 있다. 전자 장치는 학습된 인공지능 모델의 적어 도 하나의 파라미터를 메모리에 저장할 수 있다. 예를 들어, 적어도 하나의 파라미터는 3차원 가우시안 값 을 포함할 수 있다. 전자 장치는 제2 사용 환경에 대응하는 사용 환경 데이터를 획득할 수 있다. 전자 장 치는 사용 환경 데이터에 기초하여 적어도 하나의 키 프레임에 대응하는 카메라 포즈를 획득할 수 있다. 전자 장치는 카메라 포즈를 입력으로 하는 인공지능 모델을 이용하여 적어도 하나의 키 프레임에 대응하는 이미지 정보를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 매퍼 모듈은 사용 환경 데이터를 획득할 수 있다. 예를 들어, 사용 환경 데이터는 전자 장치의 센서를 이용하여 획득될 수 있다. 예를 들어, 사용 환경 데이터는 사용자 인터 페이스를 통해 획득될 수 있다. 예를 들어, 사용 환경 데이터는 전자 장치의 메모리에 미리 저장되어 있을 수 있다. 맵 변환 모듈은 사용 환경 데이터에 기초하여 제2 사용 환경에 대응하는 카메라 포즈를 결 정(또는 계산)할 수 있다. 예를 들어, 사용 환경 데이터가 사용자 정보인 경우, 맵 변환 모듈은 사용자 정 보에 대응하는 사용자의 사용 패턴 또는 키 등의 정보를 이용하여 특정 공간 좌표를 향하는 카메라 포즈를 결정 할 수 있다. 맵 변환 모듈은 제1 사용 환경에 대응하는 제1 맵 데이터를 이용하여 학습된 인공지능 모델의 적어도 하나 의 파라미터를 메모리로부터 수신할 수 있다. 맵 변환 모듈은 적어도 하나의 파라미터를 갖는 학습된 인공지능 모델을 이용하여, 제2 사용 환경에 대응하는 제2 맵 데이터를 생성할 수 있다. 예를 들어, 맵 변환 모 듈은 사용 환경 데이터를 이용하여, 제2 사용 환경에 대응하는 카메라 포즈 및 이미지 평면을 결정할 수 있다. 카메라 포즈는 가상 카메라에 대응하는 카메라 위치를 포함할 수 있다. 이미지 평면은 기-정의된 적어도 하나의 키 프레임에 대응하는 3차원 공간 상의 평면을 나타낼 수 있다. 적어도 하나의 키 프레임은, 특정 카메 라 포즈를 갖도록 설정된 카메라로 촬영되는 이미지 프레임을 나타낼 수 있다. 이미지 평면은 기 정의된 카메라 내부 파라미터를 이용하여 정규화된 이미지 평면을 나타낼 수 있다. 본 개시의 일 실시예에 있어서, 맵 변환 모 듈은 카메라 포즈 및 이미지 평면에 기초하여, 카메라 위치에서 이미지 평면 상의 픽셀을 향하는 광선의 방향 및 광선 상의 지점의 3차원 지점을 결정할 수 있다. 맵 변환 모듈은 결정된 광선의 방향과 3차원 지 점에 대응하는 3차원 공간 좌표 값을 입력으로 하는 인공지능 모델을 이용하여, 이미지 픽셀의 색상 값을 추론 할 수 있다. 맵 변환 모듈은 추론 결과에 기초하여, 제2 사용 환경에 대응하는 적어도 하나의 키 프레임을 생성할 수 있다. 맵 변환 모듈은 생성된 적어도 하나의 키 프레임을 포함하는 제2 맵 데이터를 생성할 수 있다. 예를 들어, 제2 맵 데이터는 3차원 포인트 클라우드를 포함할 수 있다. 맵 변환 모듈은 인공지능 모 델을 이용하여 렌더링된 3차원 공간에 기초하여 3차원 포인트 클라우드를 생성할 수 있다. 대안적으로, 맵 변환 모듈은 제1 맵 데이터의 3차원 포인트 클라우드를 제2 맵 데이터의 3차원 포인트 클라우드로 이용할 수 있 다. 보정 모듈은 트래커 모듈로부터 수신된 카메라 포즈의 오차를 보정할 수 있다. 보정 모듈은 제2 맵 데이터 또는 제2 맵 데이터의 키 프레임을 이용하여 트래커 모듈이 추정한 카메라 포즈의 오차를 보정 할 수 있다. 예를 들어, 보정 모듈은 루프 폐쇄(loop closure) 및 번들 조정(bundle adjustment)과 같은 알고리즘을 이용하여 카메라 포즈의 오차를 보정할 수 있다. 본 개시의 일 실시예에 있어서, 보정 모듈은 제2 맵 데이터의 키 프레임과 현재 이미지 프레임을 비교할 수 있다. 보정 모듈은 제2 맵 데이터의 키 프레임과 현재 이미지 프레임이 매칭되는지를 결정할 수 있다. 예를 들어, 보정 모듈은 키 프레임의 특징점과 현재 이미지 프레임의 특징점을 매칭시킬 수 있다. 보정 모 듈은 매칭 결과에 기초하여 카메라 포즈의 오차를 보정할 수 있다. 본 개시의 일 실시예에 있어서, 매칭되는 특징점 쌍의 개수가 기-정의된 임계 값을 초과하는 경우, 보정 모듈 은 키 프레임과 현재 이미지 프레임이 매칭되는 것으로 결정할 수 있다. 보정 모듈은 결정 결과에 기 초하여, 카메라 포즈의 오차를 보정할 수 있다. 도 3a는 본 개시의 일 실시예에 따른 동시적 위치 추정 및 맵 작성 방법을 보여주는 흐름도이다. 도 1 및 2에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2를 참조하여, 도 3a를 설명한다. 도 3a를 참조하면, 동시적 위치 추정 및 맵 작성 방법은 단계 S310 내지 S370을 포함할 수 있다. 본 개시의 일 실시 예에 있어서, 단계 S310 내지 S370은 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 현실 객체를 입력 도구로 이용하는 방법은 도 3a에 도 시된 바에 한정되지 않으며, 도 3a에 도시된 단계 중 어느 하나를 생략할 수도 있고, 도 3a에 도시되지 않은 단 계를 더 포함할 수도 있다. 단계 S310에서, 전자 장치는 제1 사용 환경에서 기-작성된(pre-mapped) 제1 맵 데이터를 이용하여 학습된 인공지능 모델의 적어도 하나의 파라미터를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 제1 맵 데이터는 적어도 하나의 원시(raw) 키 프레임을 포함할 수 있다. 본 개시의 일 실시예에 있어서, 인공지능 모델은 적어도 하나의 원시 키 프레임에 기초하여 미리 학습될 수 잇다. 예를 들어, 인공지능 모델은 적어도 하나의 원시 키 프레임에 대응하는 카메라 포즈, 및 적어도 하나의 원시 키 프레임에 대응하는 이미지 픽셀의 2차원 좌표 값과 기-정의된 간격으로 샘플링된 깊이 값에 기초한 3차원 좌표 값(예컨대, (x, y, z))을 입력으로 하여 색상 값(예 컨대, (R, G, B)) 및 밀도 값(예컨대, σ)을 출력하도록 미리 학습될 수 있다. 단계 S320에서, 전자 장치는 제2 사용 환경에 대응하는 사용 환경 데이터를 획득할 수 있다. 예를 들어, 제2 사용 환경은 제1 사용 환경과 상이할 수 있다. 본 개시의 일 실시예에 있어서, 제1 사용 환경은 제1 사용자가 전자 장치를 사용하는 환경을 나타내고, 제 2 사용 환경은 제2 사용자가 전자 장치를 사용하는 환경을 나타낼 수 있다. 본 개시의 일 실시예에 있어서, , 사용 환경 데이터는 사용자의 키 및 사용자의 사용 패턴 중 적어도 하나에 대 응하는 정보를 포함하는 기 정의된 사용자 정보를 포함할 수 있다. 본 개시의 일 실시예에 있어서, 사용자의 키에 대한 정보는 전자 장치의 메모리에 미리 저장되어 있을 수 있다. 전자 장치는 사용자 인터페이스를 통해 사용자의 키에 대한 정보에 대응하는 사용자 입력을 수신할 수 있다. 전자 장치는 사용자 입력에 기초하여 사용자의 키에 대한 정보를 획득할 수 있다. 전자 장치 는 사용자의 키에 대한 정보를 메모리에 저장할 수 있다. 본 개시의 일 실시예에 있어서, 사용자의 사용 패턴에 대한 정보는 전자 장치의 메모리에 미리 저장되어 있을 수 있다. 예를 들어, 전자 장치는 센서(예컨대, IMU 센서, 카메라 등)를 이용하여 공간에 대응하는 센서 데이터(또는 센싱 값으로도 지칭될 수 있음)를 획득할 수 있다. 전자 장치는 센서 데이터에 기초하여 사용자의 사용 패턴을 결정할 수 있다. 예를 들어, 사용자의 사용 패턴은 사용자가 공간을 바라보는 방향의 경 향성, 또는 사용자가 전방을 바라보는 시선의 방향의 경향성을 나타낼 수 있다. 예를 들어, 전자 장치는 미리 정해진 방향들(예컨대, 위 방향, 중간 방향, 아래 방향) 중 하나의 방향을 사용자의 사용 패턴으로 결정할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 카메라를 이용하여 공간을 촬영한 이미지를 획득할 수 있다. 전자 장치는 이미지에 기초하여 사용자의 사용 패턴에 대한 정보를 획득할 수 있다. 예를 들어, 전자 장치 는 이미지를 미리 학습된 인공지능 모델에 입력하여, 미리 정해진 방향들 중 하나에 대한 정보를 획득할 수 있다. 예를 들어, 인공지능 모델은 입력 이미지에 기초하여 공간을 바라보는 방향을 분류하도록 미리 학습될 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 IMU 센서를 이용하여 전자 장치의 기울기 및/또는 회전 각에 대응하는 센싱 값을 획득할 수 있다. 전자 장치는 미리 정의된 임계 값과 센싱 값을 비교할 수 있다. 전자 장치는 비교 결과에 기초하여 미리 정해진 방향들 중 하나의 방향을 사용자의 사용 패턴으로 결정할 수 있다. 예를 들어, 센싱 값이 제1 임계 값 미만인 경우, 전자 장치는 사용자의 사용 패턴이 제1 방향인것으로 결정할 수 있다. 예를 들어, 센싱 값이 제1 임계 값 이상 및 제2 임계 값 미만인 경우, 전자 장치 는 사용자의 사용 패턴이 제2 방향인 것으로 결정할 수 있다. 예를 들어, 센싱 값이 제2 임계 값 이상인 경우, 전자 장치는 사용자의 사용 패턴이 제3 방향인 것으로 결정할 수 있다. 본 개시의 일 실시예에 있어서, 제1 사용 환경은 공간을 비추는 조명이 제1 특성(예컨대, 조도, 색온도 등)을 갖는 환경을 나타내고, 제2 사용 환경은 공간을 비추는 조명이 제2 특성을 갖는 환경을 나타낼 수 있다. 예를 들어, 사용 환경 데이터는 공간에 대응하는 조명 정보를 포함할 수 있다. 예를 들어, 조명 정보는 공간의 조도 또는 색온도에 대한 정보를 포함할 수 있으나, 본 개시는 이에 한정되지 않으며 조명 정보는 조명에 대한 특성 을 나타내는 임의의 정보를 포함할 수 있다. 본 개시의 일 실시예에 있어서, 제1 사용 환경은 제1 전자 장치(예컨대, 전자 장치)를 사용하는 환경을 나 타낼 수 있고, 제2 사용 환경은 제2 전자 장치(예컨대, 외부 장치)를 사용하는 환경을 나타낼 수 있다. 예를 들 어, 사용 환경 데이터는 제2 전자 장치(예컨대, 외부 장치)의 하드웨어 성능 정보를 포함할 수 있다. 단계 S330에서, 전자 장치는 사용 환경 데이터에 기초하여 적어도 하나의 키 프레임에 대응하는 카메라 포 즈 및 이미지 평면을 결정할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 사용 환경 데이터에 기 초하여 공간에 대응하는 적어도 하나의 키 프레임을 결정할 수 있다. 예를 들어, 사용 환경에 따라서 키 프레임 에 대응하는 카메라 포즈 및/또는 키 프레임의 개수가 미리 정의될 수 있다. 예를 들어, 적어도 하나의 키 프레 임이 복수인 경우, 복수의 키 프레임들 간 공간적 간격은 균등할 수 있다. 여기에서, 균등한 간격은 사전에 결 정될 수 있다. 본 개시의 일 실시예에 있어서, 사용 환경 데이터는 기 정의된 사용자 정보를 포함할 수 있다. 예를 들어, 기 정의된 사용자 정보는 사용자의 키 및 사용자의 사용 패턴 중 적어도 하나에 대한 정보를 포함할 수 있다. 전자 장치는 기 정의된 사용자 정보에 기초하여 전자 장치를 사용하는 사용자가 누구인지를 식별할 수 있 다. 전자 장치는 기 정의된 사용자 정보(또는 식별된 사용자에 대한 사용자 정보)에 기초하여 적어도 하나 의 키 프레임에 대응하는 카메라 포즈 및 2차원 좌표 값을 결정할 수 있다. 예를 들어, 카메라 포즈는 적어도 하나의 키 프레임을 촬영하기 위한 카메라의 촬영 방향 및 위치를 나타낼 수 있다. 전자 장치는 사용자의 키 및 사용자의 사용 패턴 중 적어도 하나에 기초하여 적어도 하나의 키 프레임을 결정할 수 있다. 전자 장치 는 적어도 하나의 키 프레임에 대응하는 카메라 포즈 및 적어도 하나의 키 프레임에 대응하는 이미지 평면 을 결정할 수 있다. 본 개시의 일 실시예에 있어서, 카메라 포즈는 사용 환경 데이터에 기초하여 추출될 수 있 다. 예를 들어, 사용 환경 데이터가 사용자 정보인 경우, 전자 장치는 사용자의 사용 패턴에 매핑된 카메 라 포즈(예컨대, 카메라 각도)를 결정하거나, 사용자의 키에 매핑된 카메라 포즈(예컨대, 카메라 높이)를 결정 할 수 있다. 예를 들어, 전자 장치는 메모리에 저장된 매핑 테이블로부터, 사용자의 사용 패턴 또는 사용 자의 키와, 카메라 포즈 간의 대응 관계를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 사용 환경 데이터에 사용자 정보가 포함되지 않은 것을 식별 할 수 있다. 전자 장치는 기 정의된 키 프레임에 대응하는 카메라 포즈 및 이미지 평면을 결정할 수 있다. 단계 S340에서, 전자 장치는 카메라 포즈 및 이미지 평면에 기초하여, 카메라 위치에서 이미지 평면 상의 픽셀을 향하는 광선의 방향 및 광선 상의 지점의 3차원 좌표 값을 결정할 수 있다. 전자 장치는 카메라 포 즈에 기초하여 카메라 위치를 획득할 수 있다. 전자 장치는 이미지 평면 상의 픽셀의 2차원 좌표 값을 획 득할 수 있다. 예를 들어, 2차원 좌표 값은 적어도 하나의 키 프레임의 픽셀의 좌표 값(이미지 픽셀의 좌표 값 으로도 지칭될 수 있음)을 나타낼 수 있다. 전자 장치는 카메라 포즈 및 이미지 픽셀의 2차원 좌표 값에 기초하여 3차원 좌표 값을 획득할 수 있다. 전자 장치가 3차원 좌표 값을 획득하는 동작은 도 4a에서 상세 하게 설명한다. 단계 S350에서, 전자 장치는 적어도 하나의 파라미터를 갖는 학습된 인공지능 모델에 광선의 방향 및 3차 원 좌표 값을 입력하여, 이미지 픽셀 정보를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 학습된 인공지능 모델은 카메라 포즈 및 3차원 좌표 값을 수신할 수 있다. 학습 된 인공지능 모델은 카메라 포즈 및 3차원 좌표 값에 기초하여 이미지 픽셀 정보를 획득할 수 있다. 이미지 픽 셀 정보는 키 프레임의 픽셀에 대응하는 색상 값을 포함할 수 있다. 본 개시의 일 실시예에 있어서, 사용 환경 데이터는 조명 정보를 더 포함할 수 있다. 학습된 인공지능 모델은 조명 정보, 카메라 포즈, 및 3차원 좌표 값을 수신할 수 있다. 학습된 인공지능 모델은 카메라 포즈 및 3차원 좌표 값에 기초하여 이미지 픽셀 정보를 획득할 수 있다.본 개시의 일 실시예에 있어서, 학습된 인공지능 모델은 카메라 포즈, 및 3차원 좌표 값을 입력하여, 3차원 좌 표 값에 대응하는 색상 값 및 밀도 값을 획득할 수 있다. 전자 장치는 3차원 좌표 값, 색상 값, 및 밀도 값에 기초하여 이미지 픽셀 정보를 획득할 수 있다. 예를 들어, 이미지 픽셀 정보는 이미지 프레임(예컨대, 키 프레임)에 대응하는 픽셀들 중 하나에 대한 정보를 나타낼 수 있다. 예를 들어, 이미지 픽셀 정보는 이미지 픽 셀의 좌표 값 및 색상 값을 포함할 수 있다. 본 개시의 일 실시예에 있어서, 이미지 픽셀의 색상 값은, 전자 장 치(또는 카메라 또는 센서)의 위치와 임의의 이미지 픽셀의 위치를 지나가는 광선 상의 복수의 3차원 지점들에 대응하는, 색상 값 및 밀도 값으로 계산되는 가중 합으로 정의될 수 있다. 전자 장치가 이 미지 픽셀 정보를 획득하는 동작은 도 4b에서 상세하게 설명한다. 단계 S360에서, 전자 장치는 이미지 픽셀 정보에 기초하여 적어도 하나의 키 프레임을 획득할 수 있다. 전 자 장치는 키 프레임에 대응하는 이미지 픽셀들에 대응하는 이미지 픽셀 정보를 이용하여 키 프레임을 렌 더링할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 키 프레임의 모든 픽셀 정보를 획득하기 위 해, 단계 S340 및 단계 S350을 반복할 수 있다. 예를 들어, 단계 S340에서 키 프레임의 다른 픽셀에 대응하는 2 차원 좌표 값을 결정하고, 2차원 좌표 값에 대응하는 광선 상의 임의의 3차원 좌표 값들을 샘플링할 수 있다. 단계 S350에서, 샘플링된 3차원 좌표 값들에 기초하여 해당 픽셀에 대응하는 이미지 픽셀 정보를 획득할 수 있 다. 단계 S370에서, 전자 장치는 적어도 하나의 키 프레임에 기초하여 제2 맵 데이터를 획득할 수 있다. 본 개 시의 일 실시예에 있어서, 전자 장치는 제2 맵 데이터에 대응하는 모든 키 프레임을 획득하기 위해, 단계 S340 내지 S360을 반복할 수 있다. 본 개시의 일 실시예에 있어서, 사용 환경 데이터(예컨대, 하드웨어 성능 정 보)에 기초하여 키 프레임의 개수가 정해진 경우, 전자 장치는 정해진 개수만큼 키 프레임을 획득할 수 있 다. 본 개시의 일 실시예에 있어서, 전자 장치는 제2 맵 데이터를 이용하여 동시적 위치 추정 및 맵 작성 을 수행할 수 있다. 예를 들어, 전자 장치는 제2 맵 데이터를 이용하여 제2 사용 환경에서 추정한 전자 장 치의 위치의 오차를 보정할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 이미지 픽셀에 대응하는 정보(예컨대, 3차원 좌표 값, 색상 값, 밀도 값) 및 카메라 포즈에 기초하여 공간에 대응하는 복수의 이미지 프레임들을 생성할 수 있다. 전자 장 치는 복수의 이미지 프레임들을 이용하여 공간을 3차원 렌더링할 수 있다. 전자 장치는 3차원 렌더링 된 공간에 기초하여 3차원 포인트 클라우드를 생성할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 3차원 포인트 클라우드 및 적어도 하나의 키 프레임을 포함하 는 제2 맵 데이터를 맵 데이터 DB에 저장할 수 있다. 맵 데이터 DB는 전자 장치 또는 외부 서버의 메모리 혹은 스토리지에 저장될 수 있다. 도 4a는 본 개시의 일 실시예에 따른 동시적 위치 추정 및 맵 작성 방법을 보여주는 흐름도이다. 도 1 내지 3b 에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2 및 3a을 참조하여, 도 4a를 설명한다. 도 4a를 참조하면, 단계 S340은 단계 S441 내지 S443을 포함할 수 있다. 본 개시의 일 실시예에 있어서, 단계 S441 내지 S443은 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따 른 단계 S340의 세부 단계들은 도 4a에 도시된 바에 한정되지 않으며, 도 4a에 도시된 단계 중 어느 하나를 생 략할 수도 있고, 도 4a에 도시되지 않은 단계를 더 포함할 수도 있다. 단계 S441에서, 전자 장치는 카메라 포즈(예컨대, 카메라 위치)에 대응하는 제1 위치와 2차원 좌표 값에 대응하는 제2 위치를 지나는 광선을 결정(또는 정의)할 수 있다. 예를 들어, 2차원 좌표 값은 이미지 픽셀의 이 미지 평면 상의 좌표 값을 나타낼 수 있다. 예를 들어, 카메라 포즈는 전자 장치(또는 카메라 또는 센서)의 위치를 포함할 수 있다. 전자 장치는 전자 장치(또는 카메라 또는 센서)의 위치(즉, 제1 위치)에서 이미지 픽셀들 중 하나의 좌표 값에 대응하는 위치(즉, 제2 위치)를 잇는 직선(또는 광 선(ray)로 지칭될 수 있음)을 결정할 수 있다. 전자 장치는 결정 결과에 기초하여 광선 방향을 결정할 수 있다. 단계 S442에서, 전자 장치는 광선에 포함되는 지점들을 기 결정된 간격으로 샘플링할 수 있다. 본 개시의 일 실시예에 있어서, 샘플링 간격은 사용자 또는 제조사의 설정에 따라 미리 결정될 수 있다. 단계 S443에서, 전자 장치는 샘플링된 지점들 중 하나에 대응하는 3차원 좌표 값을 획득할 수 있다. 단계 S443 이후에, 절차는 S350으로 이동한다. 단계 S350에서, 전자 장치는 광선 방향 및 3차원 좌표 값으로 구성되는 5차원 데이터를 인공지능에 입력하여, 3차원 좌표 값에 대응하는 색상 값 및 3차원 좌표 값에 대응하는 밀도 값을 획득할 수 있다. 본 개시에서, 색상 값은 빨간색, 녹색, 파란색으로 구성되는 세 가지 기본 색상들 각각에 대한 값들로 구성될 수 있다. 예를 들어, 각 색상에 대응하는 값은 0 부터 255까지의 정수 값으로 표현 될 수 있다. 본 개시에서, 밀도 값은 광선이 3차원 좌표 값에 대응하는 지점을 통과할 때의 해당 지점의 밀도 값을 나타낼 수 있다. 밀도 값은 해당 지점에 광선에 의해 얼마나 많은 빛이 축적되는지를 나타낼 수 있다. 밀 도 값은 해당 지점의 투명도의 역수로 표현될 수도 있다. 예를 들어, 밀도 값이 상대적으로 낮은 경우, 해당 지 점이 투명하다는 것을 의미할 수 있고, 밀도 값이 상대적으로 높은 경우, 해당 지점이 불투명하다는 것을 의미 할 수 있다. 도 4b는 본 개시의 일 실시예에 따른 동시적 위치 추정 및 맵 작성 방법을 보여주는 흐름도이다. 도 1 내지 4a 에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2 및 4a을 참조하여, 도 4b를 설명한다. 도 4b를 참조하면, 단계 S350은 단계 S451 내지 S455을 포함할 수 있다. 본 개시의 일 실시예에 있어서, 단계 S451 내지 S455은 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따 른 단계 S350의 세부 단계들은 도 4b에 도시된 바에 한정되지 않으며, 도 4b에 도시된 단계 중 어느 하나를 생 략할 수도 있고, 도 4b에 도시되지 않은 단계를 더 포함할 수도 있다. 단계 S451에서, 전자 장치는 학습된 인공지능 모델에 광선의 방향 및 3차원 좌표 값을 입력하여, 3차원 좌 표 값에 대응하는 색상 값 및 밀도 값을 획득할 수 있다. 단계 S452에서, 전자 장치는 도 4a의 단계 S442에서 샘플링된 모든 지점들에 대해서 인공지능 모델이 추론 했는지를 결정할 수 있다. 샘플링된 모든 지점들에 대해서 인공지능 모델이 추론하지 않은 것으로 결정한 것에 기초하여, 절차는 도 4a의 단계 S443로 이동한다. 샘플링된 모든 지점들에 대해서 인공지능 모델이 추론한 것으 로 결정한 것에 기초하여, 절차는 단계 S453으로 이동한다. 단계 S453에서, 전자 장치는 색상 값 및 3차원 좌표 값에 대응하는 밀도 값에 기초하여 이미지 픽셀의 좌 표 값에 대응하는 픽셀 색상 값을 획득할 수 있다. 전자 장치는 이미지 픽셀에 대응하는 광선 상의 샘플링 된 모든 지점들 각각에 대응하는 색상 값 및 밀도 값을 획득할 수 있다. 전자 장치는 색상 값 및 밀도 값 에 기초하여 픽셀 색상 값을 획득할 수 있다. 예를 들어, 픽셀 색상 값은 이미지 픽셀의 색상 값을 나타낼 수 있다. 본 개시의 일 실시예에 있어서, 픽셀 색상 값은, 하나의 광선 상의 샘플링된 모든 지점들에 대하여, 색상 값 및 밀도 값을 피연산자로 하여 계산된 값을 적분(또는 가중 합)하여 결정될 수 있다. 단계 S454에서, 전자 장치는 이미지 픽셀의 좌표 값 및 픽셀 색상 값에 기초하여 이미지 픽셀 정보를 획득 할 수 있다. 이미지 픽셀 정보는 키 프레임에 대응하는 이미지 픽셀을 구성하는 값들을 포함할 수 있다. 단계 S455에서, 전자 장치는 키 프레임의 모든 이미지 픽셀들에 대한 이미지 픽셀 정보가 획득되었는지를 결정할 수 있다. 키 프레임의 모든 이미지 픽셀들에 대한 이미지 픽셀 정보가 획득되지 않은 것으로 결정한 것 에 기초하여, 절차는 단계 S340으로 이동한다. 예를 들어, 제1 픽셀에 대한 이미지 픽셀 정보가 획득되었으나, 제2 픽셀에 대한 이미지 픽셀 정보가 획득되지 않은 경우, 절차는 단계 S340으로 이동하여, 제2 픽셀에 대응하 는 광선의 방향과 3차원 좌표 값을 획득할 수 있다. 키 프레임의 모든 이미지 픽셀들에 대한 이미지 픽셀 정보 가 획득된 것으로 결정한 것에 기초하여, 절차는 단계 S360으로 이동한다. 도 5a 및 5b는 본 개시의 일 실시예에 따른 사용자 정보를 이용하는 동시적 위치 추정 및 맵 작성 방법을 설명 하기 위한 개념도이다. 도 1 내지 4에서 설명한 내용과 중복되는 내용은 생략한다. 전자 장치의 구성, 동 작, 및 기능은 도 1의 전자 장치 및 도 2의 전자 장치의 구성, 동작, 및 기능에 대응할 수 있다. 도 5a 및 5b를 참조하면, 전자 장치는 메모리 및 프로세서을 포함할 수 있다. 메모리 및 프로세서의 구성, 동작, 및 기능은 도 1의 메모리 및 프로세서 또는 도 2의 메모리 및 프 로세서의 구성, 동작, 및 기능에 대응할 수 있다. 예를 들어, 전자 장치가 증강 현실 디바이스인 것 을 가정하여 이하 설명한다. 본 개시의 일 실시예에 있어서, 전자 장치의 위치를 3차원 좌표계의 원점으로 하여, 3차원 좌표 축들을 설 정할 수 있다. 예를 들어, 제1 방향(D1)은 전자 장치를 착용한 사용자의 시선이 향하는 방향일 수 있다. 예를 들어, 제2 방향(D2)은 전자 장치를 착용한 사용자의 아래쪽 방향일 수 있다. 예를 들어, 제3 방향 (D3)은 전자 장치 전자 장치를 착용한 사용자의 오른쪽 방향일 수 있다. 그러나, 본 개시는 이에 한정되지않으며, 제1 내지 제3 방향들(D1, D2, D3)은 서로가 직각인 방향을 향할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치를 착용한 사용자의 사용 패턴은 사용자의 시선의 방향을 포함할 수 있다. 사용자의 시선의 방향은 전자 장치 또는 전자 장치의 카메라(또는 센서)가 공간을 바라보는 적어도 하나의 방향을 포함할 수 있다. 예를 들어, 전자 장치를 착용한 사용자의 시선의 방향은 공간의 바 닥을 보는 방향(DL), 공간의 중간을 보는 방향(DM), 및 공간의 천장을 보는 방향(DH) 중 하나일 수 있다. 그러 나, 본 개시는 이에 제한되지 않으며, 사용자의 시선의 방향은 제1 내지 제3 방향들(D1, D2, D3)로 표현할 수 있는 단위 벡터일 수 있다. 도 5a를 참조하면, 제1 사용자가 공간 내에서 전자 장치를 사용하는 환경(예컨대, 제1 사용 환 경)을 전제한다. 예시적으로, 제1 사용자는 키가 상대적으로 작고 시선이 아래를 향하는 사람일 수 있다. 예를 들어, 제1 사용자의 키는 제1 높이(H1)일 수 있다. 예를 들어, 제1 사용자의 시선은 공간의 바닥을 보는 방향(DL)을 향할 수 있다. 공간의 바닥을 보는 방향(DL)은 양의 제1 방향(D1)과 음의 제2 방향 (D2)으로 표현될 수 있다.. 프로세서은 제1 사용 환경에서의 이미지 프레임을 획득할 수 있다. 프로세서은 제1 사용 환경에서의 이미지 프레임에 기초하여 전자 장치의 공간 상의 위치를 추정할 수 있다. 프로세서은 제1 사용 환경에서의 이미지 프레임 및 카메라 포즈에 기초하여 공간에 대응하는 제1 맵 데이터를 생성할 수 있다. 프로세서는 제1 맵 데이터를 이용하여 트래커 모듈이 추정한 위치를 보정할 수 있다. 프로세서 는 제1 맵 데이터를 학습 데이터로 하여 공간을 렌더링하는 인공지능 모델을 학습시킬 수 있다. 본 개시의 일 실시예에 있어서, 인공지능 모델은 공간을 렌더링하거나, 공간을 렌더링하기 위한 원시 데이터(예컨 대, 이미지 픽셀의 색상 값 및 밀도 값)를 추론하도록 학습될 수 있다. 프로세서는 학습된 인공지능 모델 에 대응하는 적어도 하나의 파라미터를 메모리에 저장할 수 있다. 도 5b를 참조하면, 제2 사용자가 공간 내에서 전자 장치를 사용하는 환경(예컨대, 제2 사용 환 경)을 전제한다. 예시적으로, 제2 사용자는 키가 (제1 사용자 대비) 상대적으로 크고 시선이 위를 향하 는 사람일 수 있다. 예를 들어, 제2 사용자의 키는 제2 높이(H2)일 수 있다. 예를 들어, 제2 사용자의 시선은 공간의 천장을 보는 방향(DH)을 향할 수 있다. 제2 높이(H2)는 제1 높이(H1)보다 클 수 있다. 공간 의 천장을 보는 방향(DH)은 양의 제1 방향(D1)과 양의 제2 방향(D2)으로 표현될 수 있다. 프로세서는 제2 사용자에 대한 사용자 정보를 획득할 수 있다. 예를 들어, 제2 사용자에 대한 사 용자 정보는, 제2 사용자의 키(예컨대, 제2 높이(H2))와, 제2 사용자의 사용 패턴 중 하나인, 제2 사용 자의 시선 방향(예컨대, 공간의 천장을 보는 방향(DH))을 포함할 수 있다. 프로세서는 메모리 에 저장된 적어도 하나의 파라미터를 획득할 수 있다. 프로세서는 사용자 정보와 인공지능 모델을 이 용하여 제2 사용 환경에 대응하는 제2 맵 데이터를 생성할 수 있다. 본 개시의 일 실시예에 있어서, 프로세서는 사용자 정보에 기초하여 제2 사용 환경에 대응하는 카메라 포 즈를 결정할 수 있다. 프로세서는 카메라 포즈를 인공지능 모델에 입력하여, 적어도 하나의 키 프레임에 대응하는 이미지 정보를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 프로세서는 사용자 정보에 기초하여 제2 사용 환경에 대응하는 카메라 포 즈 및 이미지 평면을 결정할 수 있다. 프로세서는 카메라 포즈 및 이미지 평면에 기초하여 광선의 방향 및 광선 상의 3차원 지점에 대응하는 3차원 좌표 값을 획득할 수 있다. 프로세서는 광선의 방향 및 3차원 좌 표 값을 인공지능 모델에 입력하여, 이미지 픽셀 정보를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 프로 세서는 광선의 방향 및 3차원 좌표 값을 인공지능 모델에 입력하여, 3차원 좌표 값에 대응하는 색상 값 및 밀도 값을 추론할 수 있다. 프로세서는 카메라 포즈, 3차원 좌표 값, 색상 값, 및 밀도 값에 기초하여 이미지 픽셀 정보를 획득할 수 있다. 프로세서는 이미지 픽셀 정보에 기초하여 제2 사용 환경에 대응하는 적어도 하나의 키 프레임을 생성할 수 있다. 따라서, 생성된 적어도 하나의 키 프레임은 제2 사용자의 키 및 시선 방향에 대응할 수 있다. 프로세서는 적어도 하나의 키 프레임에 기초하여 제2 맵 데이터를 구성할 수 있다. 프로세서는 제2 사용 환경에서의 이미지 프레임을 획득할 수 있다. 프로세서는 제2 사용 환경에서의 이미지 프레임에 기초하여 전자 장치의 공간 상의 위치를 추정할 수 있다. 프로세서는 제2 맵 데 이터를 이용하여, 추정된 위치를 보정할 수 있다. 도 6a 및 6b는 본 개시의 일 실시예에 따른 조명 정보를 이용하는 동시적 위치 추정 및 맵 작성 방법을 설명하 기 위한 개념도이다. 도 1 내지 5b에서 설명한 내용과 중복되는 내용은 생략한다. 전자 장치의 구성, 동작, 및 기능은 도 1의 전자 장치 및 도 2의 전자 장치의 구성, 동작, 및 기능에 대응할 수 있다. 도 6a 및 6b를 참조하면, 전자 장치는 메모리 및 프로세서을 포함할 수 있다. 메모리 및 프로세서의 구성, 동작, 및 기능은 도 1의 메모리 및 프로세서 또는 도 2의 메모리 및 프 로세서의 구성, 동작, 및 기능에 대응할 수 있다. 도 6a를 참조하면, 사용자가 제1 조명 특성(IL1)을 갖는 공간 내에서 전자 장치를 사용하는 환경 (예컨대, 제1 사용 환경)을 전제한다. 예시적으로, 제1 조명 특성(IL1)은 제1 색 온도 및/또는 제1 밝기를 갖는 조명의 특성을 나타낼 수 있다. 프로세서는 공간의 제1 조명 특성에 대응하는 제1 조명 정보를 획득할 수 있다. 본 개시의 일 실시예 에 있어서, 전자 장치는 조명 센서(미도시)를 포함할 수 있다. 조명 센서(미도시)는 공간의 조명 특성 을 측정할 수 있다. 예를 들어, 조명 센서(미도시)는 공간의 조명 밝기 및/또는 조명 색 온도를 측정할 수 있다. 프로세서는 조명 센서(미도시)를 이용하여 제1 조명 정보를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 프로세서는 조명 추정 모델을 이용하여 조명 정보를 획득할 수 있다. 예를 들어, 조명 추정 모델 은 제1 사용 환경에 대응하는 복수의 이미지 프레임들을 입력으로 하여 조명 정보를 추론하는 인공지능 모델일 수 있다. 프로세서는 제1 사용 환경에서의 이미지 프레임을 획득할 수 있다. 프로세서는 제1 사용 환경에서의 이미지 프레임에 기초하여 전자 장치의 공간 상의 위치를 추정할 수 있다. 프로세서는 제1 사용 환경에서의 이미지 프레임 및 카메라 포즈에 기초하여 공간에 대응하는 제1 맵 데이터를 생성할 수 있다. 프로세서는 제1 맵 데이터를 이용하여, 추정된 위치를 보정할 수 있다. 프로세서는 제1 맵 데이터 및 제1 조명 정보를 학습 데이터로 하여 공간을 렌더링하는 인공지능 모델을 학습시킬 수 있다. 본 개시의 일 실시예에 있어서, 인공지능 모델은 광선의 방향, 3차원 좌표 값, 및 조명 정보를 입력으로 하여, 이미지 픽셀 정보를 추론하도록 학습될 수 있다. 본 개시의 일 실시예에 있어서, 인공지능 모델은 공간을 렌더링하거나, 공간을 렌더링하기 위한 원시 데이터(예컨대, 이미지 픽셀의 색상 값 및 밀도 값)를 추론하도록 학습될 수 있다. (예컨대, 이미지 픽셀의 색상 값 및 밀도 값) 학습된 인공지능 모델에 대응하는 적어도 하나의 파라미터 를 메모리에 저장할 수 있다. 도 6b를 참조하면, 사용자가 제2 조명 특성(IL2)을 갖는 공간 내에서 전자 장치를 사용하는 환경 (예컨대, 제2 사용 환경)을 전제한다. 예시적으로, 제2 조명 특성(IL2)은 제2 색 온도 및/또는 제2 밝기를 갖는 조명의 특성을 나타낼 수 있다. 프로세서는 제2 조명 정보를 획득할 수 있다. 예를 들어, 제2 조명 정보는 제2 색 온도 및/또는 제2 밝기 를 갖는 조명의 특성에 대한 정보를 포함할 수 있다. 프로세서는 메모리에 저장된 적어도 하나의 파 라미터를 획득할 수 있다. 프로세서는 제2 조명 정보와 인공지능 모델을 이용하여 제2 사용 환경에 대응하 는 제2 맵 데이터를 생성할 수 있다. 본 개시의 일 실시예에 있어서, 프로세서는 카메라 포즈 및 제2 조명 정보를 인공지능 모델에 입력하여, 적어도 하나의 키 프레임에 대응하는 이미지 정보를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 프로세서는 사용 환경 데이터에 기초하여 카메라 포즈, 이미지 평면, 및 제2 조명 정보를 획득할 수 있다. 프로세서는 카메라 포즈 및 이미지 평면에 기초하여 광선의 방향 및 3차 원 좌표 값을 획득할 수 있다. 프로세서는 제2 조명 정보, 광선의 방향 및 3차원 좌표 값을 인공지능 모델 에 입력하여, 이미지 픽셀 정보를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 프로세서는 제2 조명 정보, 광선의 방향 및 3차원 좌표 값을 인공지능 모델에 입력하여, 3차원 좌표 값에 대응하는 색상 값 및 밀도 값을 추론할 수 있다. 프로세서는 색상 값 및 밀도 값에 기초하여 이미지 픽셀 정보를 획득할 수 있다. 프 로세서는 광선의 방향, 3차원 좌표 값, 색상 값, 및 밀도 값에 기초하여 제2 사용 환경에 대응하는 적어도 하나의 키 프레임을 생성할 수 있다. 생성된 적어도 하나의 키 프레임은 공간에 대응하는 제2 조명 정보를 반영할 수 있다. 프로세서는 적어도 하나의 키 프레임에 기초하여 제2 맵 데이터를 구성할 수 있다. 프로세서는 제2 사용 환경에서의 이미지 프레임을 획득할 수 있다. 프로세서는 제2 사용 환경에서의 이미지 프레임에 기초하여 전자 장치의 공간 상의 위치를 추정할 수 있다. 프로세서는 제2 맵 데 이터를 이용하여, 추정된 위치를 보정할 수 있다. 본 개시의 일 실시예에 따르면, 공간 상의 조명의 특성이 달라지더라도, 맵 데이터를 구성하는 키 프레임에 현 재 공간의 조명의 특성을 반영함으로써, 조명 환경 변화에 강건한 맵 데이터를 생성할 수 있다. 도 7a는 본 개시의 일 실시예에 따른 인공지능 모델의 파라미터를 업데이트하는 방법을 보여주는 흐름도이다. 도 1 내지 6b에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2 및 3a를 참조하여, 도 7a를 설명한다. 도 7a를 참조하면, 동시적 위치 추정 및 맵 작성 방법은 인공지능 모델의 파라미터를 업데이트하는 방법을 더 포함할 수 있다. 동시적 위치 추정 및 맵 작성 방법은 단계 S710 내지 S730을 포함할 수 있다. 본 개시의 일 실 시예에 있어서, 단계 S710 내지 S730은 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따른 단계 S710 내지 S730은 도 7a에 도시된 바에 한정되지 않으며, 도 7a에 도시된 단계 중 어느 하나를 생략할 수도 있고, 도 7a에 도시되지 않은 단계를 더 포함할 수도 있다. 본 개시의 일 실시예에 있 어서, 단계 S370 이후에, 절차는 단계 S710로 이동할 수 있다. 단계 S710에서, 전자 장치는 제2 사용 환경에 대응하는 복수의 이미지 프레임들을 획득할 수 있다. 예를 들어, 전자 장치는 카메라를 이용하여 공간을 촬영한 복수의 이미지 프레임들을 획득할 수 있다. 단계 S720에서, 전자 장치는 복수의 이미지 프레임들 중에서 적어도 하나의 추가 키 프레임을 결정할 수 있다. 예를 들어, 전자 장치는 복수의 이미지 프레임들간의 시간 간격이 기-정의된 임계 값을 초과하는지 에 기초하여 적어도 하나의 키 프레임을 결정할 수 있다. 예를 들어, 전자 장치는 카메라의 위치 관계 변 화량이 기-정의된 임계 값을 초과하는지에 기초하여 적어도 하나의 키 프레임을 결정할 수 있다. 예를 들어, 전 자 장치는 깊이 정보 차이가 기-정의된 임계 값을 초과하는지에 기초하여 적어도 하나의 키 프레임을 결정 할 수 있다. 예를 들어, 전자 장치는 특징점 분포의 차이가 기-정의된 임계 값을 초과하는지에 기초하여 적어도 하나의 추가 키 프레임을 결정할 수 있다. 단계 S730에서, 전자 장치는 적어도 하나의 추가 키 프레임에 기초하여 학습된 인공지능 모델의 적어도 하 나의 파라미터를 업데이트할 수 있다. 예를 들어, 전자 장치는 적어도 하나의 키 프레임에 대응하는 카메 라 포즈, 및 이미지 픽셀 정보를 학습 데이터로 하여, 학습된 인공지능 모델의 적어도 하나의 파라미터를 업데 이트할 수 있다. 예를 들어, 기존의 맵 데이터(예컨대, 제2 맵 데이터)에 포함되는 키 프레임과 다른 새로운 키 프레임이 결정된 경우, 전자 장치는 새로운 키 프레임을 이용하여, 인공지능 모델에 대한 온라인 학습을 수행할 수 있다. 전자 장치는 학습 결과에 따른 업데이트된 적어도 하나의 파라미터를 메모리에 저장 할 수 있다. 도 7b는 본 개시의 일 실시예에 따른 공간에 속하는 객체의 변화를 반영하는 동시적 위치 추정 및 맵 작성 방법 을 보여주는 흐름도이다. 도 1 내지 7a에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2, 3, 및 7a을 참조하여, 도 7b을 설명한다. 도 7b를 참조하면, 도 7a의 단계 S730은 단계 S731 내지 S734을 포함할 수 있다. 본 개시의 일 실시예에 있어서, 단계 S731 내지 S734은 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따른 단계 S730의 세부 단계들은 도 7b에 도시된 바에 한정되지 않으며, 도 7b에 도시된 단계 중 어 느 하나를 생략할 수도 있고, 도 7b에 도시되지 않은 단계를 더 포함할 수도 있다. 단계 S731에서, 전자 장치는 결정된 적어도 하나의 추가 키 프레임에 대해 객체 세그멘테이션(object segmentation)을 수행할 수 있다. 본 개시에서, '객체 세그멘테이션'은, 인공지능 모델을 이용하여 이미지나 동 영상에서 복수의 객체들을 픽셀 수준으로 구분하고, 각 객체의 경계를 추출하는 작업을 의미한다. 전자 장치 은 객체 세그멘테이션을 통해 공간 상의 객체를 검출할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장 치는 현재 이미지 프레임에 대해 객체 세그멘테이션을 수행할 수 있다. 단계 S732에서, 전자 장치는 제2 맵 데이터와 객체 세그멘테이션 결과에 기초하여, 제2 맵 데이터에 대응 하는 공간에 속하는 적어도 하나의 객체의 추가, 변경, 또는 삭제 여부를 결정할 수 있다. 예를 들어, 전자 장 치는 제2 맵 데이터에 대응하는 공간 속에서 객체가 존재하거나 존재하지 않는 부분을 식별할 수 있다. 전 자 장치는 제2 맵 데이터와, 결정된 적어도 하나의 키 프레임 또는 현재 이미지 프레임에 대한 객체 세그 멘테이션 결과를 비교하여, 객체의 추가, 변경, 또는 삭제 여부를 결정할 수 있다. 예를 들어, 제2 맵 데이터에 서는 존재하는 객체가 객체 세그멘테이션 결과에 검출되지 않은 경우, 전자 장치는 객체가 삭제된 것으로 결정할 수 있다. 제2 맵 데이터에서는 존재하지 않는 객체가 객체 세그멘테이션 결과에 검출된 경우, 전자 장치 는 객체가 추가된 것으로 결정할 수 있다. 본 개시의 일 실시예에 있어서, 객체가 추가, 변경, 또는 삭제된 것으로 결정되면(예), 절차는 단계 S733으로 이동하고, 객체가 추가, 변경, 또는 삭제되지 않은 것으로 결정되면(아니오), 절차는 종료되고, 인공지능 모델 의 적어도 하나의 파라미터는 업데이트되지 않는다. 단계 S733에서, 전자 장치는 적어도 하나의 추가 키 프레임에 대응하는 카메라 포즈 및 이미지 픽셀 정보 를 학습 데이터로 하여, 학습된 인공지능 모델의 적어도 하나의 파라미터를 업데이트할 수 있다. 단계 S734에서, 전자 장치는 적어도 하나의 업데이트된 파라미터를 갖는 학습된 인공지능 모델을 이용하여 제3 맵 데이터를 획득할 수 있다. 제3 맵 데이터에는 객체의 추가, 변경, 또는 삭제 여부가 반영될 수 있다. 도 8은 본 개시의 일 실시예에 따른 공간에 속하는 객체의 변화를 반영하는 동시적 위치 추정 및 맵 작성 방법 을 설명하기 위한 개념도이다. 도 1 내지 7b에서 설명한 내용과 중복되는 내용은 생략한다. 전자 장치의 구성, 동작, 및 기능은 도 1의 전자 장치 및 도 2의 전자 장치의 구성, 동작, 및 기능에 대응할 수 있다. 도 8을 참조하면, 전자 장치는 메모리 및 프로세서을 포함할 수 있다. 메모리 및 프로세서 의 구성, 동작, 및 기능은 도 1의 메모리 및 프로세서 또는 도 2의 메모리 및 프로세서 의 구성, 동작, 및 기능에 대응할 수 있다. 예를 들어, 사용자가 공간 내에서 전자 장치를 사용하는 환경(예컨대, 제2 사용 환경)을 전제한다. 프로세서는 메모리에 저장된 적어도 하나의 파라미터를 획득할 수 있다. 프로세서는 적어도 하 나의 파라미터를 갖는 인공지능 모델을 이용하여 제2 사용 환경에 대응하는 제2 맵 데이터를 생성할 수 있다. 제2 맵 데이터에 대응하는 공간에는 특정 영역에 특정 객체가 존재할 수 있다. 프로세서는 제2 사용 환경에서의 이미지 프레임을 획득할 수 있다. 프로세서는 제2 사용 환경에서의 이미지 프레임에 기초하여 전자 장치의 공간 상의 위치를 추정할 수 있다. 프로세서는 제2 맵 데 이터를 이용하여, 추정된 위치를 보정할 수 있다. 프로세서는 제2 사용 환경에 대응하는 복수의 이미지 프레임들 중에서 적어도 하나의 추가 키 프레임을 결 정할 수 있다. 프로세서는 적어도 하나의 추가 키 프레임에 대해 객체 세그멘테이션을 수행할 수 있다. 예 를 들어, 객체 세그멘테이션 결과에 따르면, 공간의 특정 영역에 객체가 존재하지 않을 수 있다. 공간 , 공간, 및 공간은 같은 공간이며, 특정 영역, 특정 영역, 및 특정 영역은 서로 대응 하는 영역일 수 있다. 프로세서는 제2 맵 데이터와 객체 세그멘테이션 결과를 비교하여, 공간의 특정 영역에 존재하던 특정 객체가 현재 공간에선 삭제된 것을 결정할 수 있다. 프로세서는 객체가 삭제된 키 프레임에 기초하여, 인공지능 모델의 적어도 하나의 파라미터를 업데이트할 수 있다. 본 개시의 일 실시예에 있어서, 프로세서는 3차원 포인트 클라우드에서 특정 객체에 대응하는 포인트 그룹을 삭제할 수 있다. 본 개시의 일 실시예에 있어서, 프로세서는 적어도 하나의 업데이트된 파라미터를 갖는 인공지능 모델을 이용하여, 공간을 렌더링할 수 있다. 프로세서는 공간에 대응하는 제3 맵 데이터를 생성할 수 있 다. 공간의 특정 영역에는 객체가 존재하지 않을 수 있다. 도 9는 본 개시의 일 실시예에 따른 하드웨어 성능 값을 이용하는 동시적 위치 추정 및 맵 작성 방법을 설명하 기 위한 개념도이다. 도 1 내지 8에서 설명한 내용과 중복되는 내용은 생략한다. 제1 전자 장치(900a)의 구성, 동작, 및 기능은 도 1의 전자 장치 및 도 2의 전자 장치의 구성, 동작, 및 기능에 대응할 수 있다. 도 9를 참조하면, 제1 전자 장치(900a)는 메모리(930a), 프로세서(980a), 및 통신 인터페이스(960a)를 포함할 수 있다. 메모리(930a), 프로세서(980a), 및 통신 인터페이스(960a)의 구성, 동작, 및 기능은 도 2의 메모리 , 프로세서, 및 통신 인터페이스의 구성, 동작, 및 기능에 대응할 수 있다. 제2 전자 장치 (900b)는 프로세서(980b) 및 통신 인터페이스(960b)를 포함할 수 있다. 프로세서(980b) 및 통신 인터페이스 (960b)의 구성, 동작, 및 기능은 도 2의 프로세서, 및 통신 인터페이스의 구성, 동작, 및 기능에 대 응할 수 있다. 제1 전자 장치(900a)는 제2 전자 장치(900b)와 하드웨어 성능이 상이할 수 있다. 예를 들어, 제1 전자 장치 (900a)의 하드웨어 성능이 제2 전자 장치(900b)의 하드웨어 성능보다 높은 경우를 전제하여 이하 설명한다. 그 러나, 본 개시는 이에 제한되지 않으며, 제1 전자 장치(900a)의 하드웨어 성능이 제2 전자 장치(900b)의 하드웨 어 성능보다 낮을 수 있다. 예를 들어, 하드웨어 성능은, 프로세서 속도, 프로세서의 코어의 개수, 메모리의 용 량, 메모리 속도, 그래픽 처리 속도, 통신 속도, 발열, 에너지 효율성, 확장성, 또는 이들의 조합 등을 포함할수 있으나, 본 개시는 이에 한정되지 않으며, 전자 장치의 포함되는 적어도 하나의 하드웨어의 성능을 표현하는 임의의 지표를 나타낼 수 있다. 예를 들어, 제1 사용 환경은 제1 전자 장치(900a)를 이용하여 공간에서의 위치 추정 및 맵 작성을 수행하는 환경을 나타내고, 제2 사용 환경은 제2 전자 장치(900b)를 이용하여 공간(1 0)에서의 위치 추정 및 맵 작성을 수행하는 환경을 나타낼 수 있다. 프로세서(980a)는 제1 사용 환경에서의 이미지 프레임을 획득할 수 있다. 프로세서(980a)는 제1 사용 환경에서 의 이미지 프레임에 기초하여 전자 장치의 공간 상의 위치를 추정할 수 있다. 프로세서(980a)는 제1 사용 환경에서의 이미지 프레임 및 카메라 포즈에 기초하여 공간에 대응하는 제1 맵 데이터를 생성할 수 있 다. 프로세서(980a)는 제1 맵 데이터를 이용하여, 추정된 위치를 보정할 수 있다. 프로세서(980a)는 제1 맵 데이터를 학습 데이터로 하여 공간을 렌더링하는 인공지능 모델을 학습시킬 수 있 다. 본 개시의 일 실시예에 있어서, 제1 맵 데이터는 제1 개수의 키 프레임을 포함할 수 있다. 본 개시의 일 실 시예에 있어서, 인공지능 모델은 공간을 렌더링하거나, 공간을 렌더링하기 위한 원시 데이터를 추론하 도록 학습될 수 있다. 프로세서(980a)는 학습된 인공지능 모델에 대응하는 적어도 하나의 파라미터를 메모리 에 저장할 수 있다. 프로세서(980a)는 제2 전자 장치(900b)에 대한 하드웨어 성능 정보를 획득할 수 있다. 프로세서(980a)는 메모리 에 저장된 적어도 하나의 파라미터를 획득할 수 있다. 프로세서(980a)는 제2 전자 장치(900b)에 대한 하드 웨어 성능 정보와 인공지능 모델을 이용하여 제2 사용 환경에 대응하는 제2 맵 데이터를 생성할 수 있다. 예를 들어, 제2 전자 장치(900b)의 하드웨어 성능 정보에 제2 개수의 키 프레임이 사전에 매핑될 수 있다. 프로세서 (980a)는 인공지능 모델을 이용하여 제2 개수의 키 프레임을 포함하는 제2 맵 데이터를 생성할 수 있다. 본 개 시의 일 실시예에 있어서, 제2 개수는 제1 개수보다 적을 수 있다. 프로세서(980a)는 통신 인터페이스(960a)를 통해 기 결정된 수(예컨대, 제2 개수)의 키 프레임을 포함하는 제2 맵 데이터를 제2 전자 장치(900b)에 전달할 수 있다. 프로세서(980b)는 제2 전자 장치(900b)의 통신 인터페이스(960b)를 통해 제2 개수의 키 프레임을 포함하는 제2 맵 데이터를 획득할 수 있다. 프로세서(980b)는 제2 사용 환경에서의 이미지 프레임을 획득할 수 있다. 프로세 서(980b)는 제2 사용 환경에서의 이미지 프레임에 기초하여 제2 전자 장치(900b)의 공간 상의 위치를 추정 할 수 있다. 프로세서(980b)는 제2 맵 데이터를 이용하여, 추정된 위치를 보정할 수 있다. 본 개시의 일 실시예에 따르면, 전자 장치의 하드웨어 성능에 따라 적절한 키 프레임의 수를 갖도록 조정함으로 써, 하드웨어 성능에 강건한 맵 데이터를 생성할 수 있다. 도 10은 본 개시의 일 실시예에 따른 전자 장치의 구성 요소 및 사용자 단말의 구성 요소를 보여주는 블록도이 다. 도 1 내지 9에서 설명한 내용과 중복되는 내용은 생략한다. 도 10을 참조하면, 전자 장치는 통신 인터페이스, 프로세서, 및 메모리을 포함할 수 있다. 통신 인터페이스, 프로세서, 및 메모리의 구성, 동작, 및 기능은 도 2의 통신 인터페 이스, 프로세서, 및 메모리의 구성, 동작, 및 기능에 대응할 수 있다. 메모리는 매퍼 모 듈 및 모델 파라미터를 포함할 수 있다. 매퍼 모듈 및 모델 파라미터의 구성, 동작, 및 기능은 도 2의 매퍼 모듈 및 모델 파라미터의 구성, 동작, 및 기능에 대응할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 도 2의 트래커 모듈에 대응하는 기능을 수행하지 않을 수 있다. 예를 들어, 전자 장치는 서버 장치 또는 스마트 폰일 수 있으나, 본 개시는 이에 한정되지 않는다. 사용자 단말은 카메라, 센서, 통신 인터페이스, 프로세서, 및 메모리을 포함할 수 있다. 카메라, 센서, 통신 인터페이스, 프로세서, 및 메모리의 구성, 동작, 및 기능은 도 2의 카메라, 센서, 통신 인터페이스, 프로세서, 및 메모리(23 0)의 구성, 동작, 및 기능에 대응할 수 있다. 메모리는 트래커 모듈을 포함할 수 있다. 트래커 모 듈의 구성, 동작, 및 기능은 도 2의 트래커 모듈의 구성, 동작, 및 기능에 대응할 수 있다. 본 개시 의 일 실시예에 있어서, 사용자 단말은 도 2의 매퍼 모듈에 대응하는 기능을 수행하지 않을 수 있다. 예를 들어, 전자 장치는 증강 현실 디바이스일 수 있으나, 본 개시는 이에 한정되지 않는다. 본 개시의 일 실시예에 있어서, 사용자 단말은 카메라를 이용하여 복수의 이미지 프레임들을 획득 할 수 있다. 사용자 단말은 센서를 이용하여 공간에 대응하는 센서 데이터를 획득할 수 있다. 사용 자 단말은 복수의 이미지 프레임들 및 센서 데이터 중 적어도 하나에 기초하여 사용자 단말의 위치 를 추정할 수 있다. 사용자 단말은 통신 인터페이스를 통해, 복수의 이미지 프레임들, 센서데이터, 및 카메라 포즈 중 적어도 하나를 전자 장치에 전달할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치는 모델 파라미터를 갖는 인공지능 모델을 이용하여 공간 에 대응하는 맵 데이터를 생성할 수 있다. 전자 장치는 통신 인터페이스를 통해, 복수의 이미지 프 레임들, 센서 데이터, 및 카메라 포즈 중 적어도 하나를 사용자 단말로부터 수신할 수 있다. 전자 장치 는 맵 데이터를 이용하여, 카메라 포즈의 오차를 보정할 수 있다. 전자 장치는 통신 인터페이스 를 통해, 보정 결과를 사용자 단말에 전달할 수 있다. 사용자 단말은 보정 결과를 수신할 수 있다. 사용자 단말은 보정 결과에 기초하여 사용자 단말의 위치를 추정할 수 있다. 본 개시의 일 실시예에 따르면, 복잡한 연산이 필요한 매퍼 모듈의 기능을 외부 장치(또는 외부 장치의 프로세 서)에서 수행함으로써, 동시적 위치 추정 및 맵 작성을 효율적으로 수행할 수 있다. 본 개시의 일 실시예에 있어서, 전자 장치가 제공될 수 있다. 전자 장치는 적어도 하나의 인스트럭션을 저장하 는 메모리를 포함할 수 있다. 전자 장치는 상기 적어도 하나의 인스트럭션을 실행하는 적어도 하나의 프로세서 를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 제1 사용 환경에서 기-작성된(pre-mapped) 제1 맵 데이터 를 이용하여 학습된 인공지능 모델의 적어도 하나의 파라미터를 획득할 수 있다. 상기 적어도 하나의 프로세서 는, 제2 사용 환경에 대응하는 사용 환경 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 사용 환경 데이터에 기초하여 적어도 하나의 키 프레임에 대응하는 카메라 포즈(camera pose) 및 이미지 평면(image plane)을 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 카메라 포즈 및 이미지 평면에 기초하여, 카메 라 위치에서 상기 이미지 평면 상의 픽셀을 향하는 광선(ray)의 방향 및 상기 광선 상의 지점의 3차원 좌표 값 을 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 적어도 하나의 파라미터를 갖는 상기 학습된 인공지 능 모델에 상기 광선의 방향 및 상기 3차원 좌표 값을 입력하여, 상기 픽셀에 대응하는 이미지 픽셀 정보를 획 득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 획득된 이미지 픽셀 정보에 기초하여 상기 적어도 하나의 키 프레임을 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 획득된 적어도 하나의 키 프레임에 기초하 여 제2 맵 데이터를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 상기 제1 맵 데이터는 적어도 하나의 원시(raw) 키 프레임을 포함할 수 있다. 상기 학습된 인공지능 모델은, 상기 적어도 하나의 원시 키 프레임에 기초하여 미리 학습될 수 있다. 본 개시의 일 실시예에 있어서, 상기 전자 장치는, 상기 제2 사용 환경에서 공간을 촬영하여 이미지를 획득하는 카메라를 포함할 수 있다. 상기 전자 장치는, 사용자 인터페이스를 포함할 수 있다. 상기 사용 환경 데이터는 사용자의 키 및 사용자의 사용 패턴 중 적어도 하나에 대응하는 정보를 포함하는 기 정의된 사용자 정보를 포함 할 수 있다. 상기 사용자의 키에 대한 정보는 상기 사용자 인터페이스를 통해 기 획득되어 상기 메모리에 저장 될 수 있다. 상기 적어도 하나의 프로세서는, 상기 메모리로부터 상기 사용자의 키에 대한 정보를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 이미지에 기초하여 상기 사용자의 사용 패턴에 대한 정보를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 기-정의된 사용자 정보에 기초하여 상기 카메라 포즈 및 상기 이미지 평면을 결정할 수 있다. 본 개시의 일 실시예에 있어서, 상기 사용 환경 데이터는 조명 정보를 포함하고, 상기 적어도 하나의 프로세서 는, 상기 적어도 하나의 파라미터를 갖는 상기 학습된 인공지능 모델에 상기 조명 정보, 상기 광선의 방향 및 상기 3차원 좌표 값을 입력하여, 상기 이미지 픽셀 정보를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 상기 전자 장치는, 상기 제2 사용 환경에 대응하는 복수의 이미지 프레임들을 획득하는 카메라를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 조명 추정 모델에 상기 복수의 이미지 프 레임들 중 적어도 하나를 입력하여, 상기 조명 정보를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 상기 전자 장치는, 공간의 조명 값을 측정하는 조명 센서를 포함할 수 있다. 상 기 적어도 하나의 프로세서는, 상기 조명 정보를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 상기 적어도 하나의 프로세서는, 상기 제2 사용 환경에 대응하는 복수의 이미지 프레임들을 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 복수의 이미지 프레임들간의 시간 간격, 상 기 복수의 이미지 프레임들간의 카메라의 위치 관계 변화량, 상기 복수의 이미지 프레임들간의 깊이 정보 차이, 및 상기 복수의 이미지 프레임들 간의 특징점 분포의 차이 중 적어도 하나가 기 정의된 임계 값을 초과하는지 여부에 기초하여 적어도 하나의 추가 키 프레임을 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 결정 된 적어도 하나의 키 프레임에 기초하여 상기 학습된 인공지능 모델의 상기 적어도 하나의 파라미터를 업데이트 할 수 있다.본 개시의 일 실시예에 있어서, 상기 적어도 하나의 프로세서는, 상기 결정된 적어도 하나의 추가 키 프레임에 대해 객체 세그멘테이션을 수행할 수 있다. 상기 적어도 하나의 프로세서는, 상기 적어도 하나의 업데이트된 파 라미터를 갖는 상기 학습된 인공지능 모델을 이용하여 제3 맵 데이터를 획득상기 제2 맵 데이터와 상기 객체 세 그멘테이션 결과에 기초하여, 상기 제2 맵 데이터에 대응하는 공간에 속하는 적어도 하나의 객체의 추가, 변경 또는 삭제 여부를 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 적어도 하나의 객체가 추가, 변경, 또 는 삭제된 것으로 결정한 것에 기초하여, 상기 결정된 적어도 하나의 추가 키 프레임을 학습 데이터로 하여, 상 기 학습된 인공지능 모델의 상기 적어도 하나의 파라미터를 업데이트할 수 있다. 상기 적어도 하나의 프로세서 는, 상기 적어도 하나의 업데이트된 파라미터를 갖는 상기 학습된 인공지능 모델을 이용하여 제3 맵 데이터를 획득할 수 있다. 본 개시의 일 실시예에 있어서, 상기 사용 환경 데이터는, 외부 장치의 하드웨어 성능 정보를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하드웨어 성능 정보에 기초하여 기-결정된 수의 상기 적어도 하나의 키 프레임을 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 기-결정된 수의 상기 적어도 하나의 키 프레임 을 포함하는 상기 제2 맵 데이터를 상기 외부 장치에 전달할 수 있다. 본 개시의 일 실시예에 있어서, 상기 적어도 하나의 프로세서는, 현재 카메라 포즈를 획득할 수 있다. 상기 적 어도 하나의 프로세서는, 상기 제2 맵 데이터에 기초하여 상기 현재 카메라 포즈의 오차를 보정할 수 있다. 본 개시의 일 실시예에 있어서, 동시적 위치 추정 및 맵 작성 방법이 제공될 수 있다. 방법은, 제1 사용 환경에 서 기-작성된(pre-mapped) 제1 맵 데이터를 이용하여 학습된 인공지능 모델의 적어도 하나의 파라미터를 획득하 는 단계를 포함할 수 있다. 방법은, 제2 사용 환경에 대응하는 사용 환경 데이터를 획득하는 단계를 포함할 수 있다. 상기 사용 환경 데이터에 기초하여 적어도 하나의 키 프레임에 대응하는 카메라 포즈 및 이미지 평면을 결정하는 단계를 포함할 수 있다. 방법은, 상기 카메라 포즈 및 상기 이미지 평면에 기초하여, 카메라 위치에서 상기 이미지 평면 상의 픽셀을 향하는 광선의 방향 및 상기 광선 상의 지점의 3차원 좌표 값을 결정하는 단계를 포함할 수 있다. 방법은,상기 적어도 하나의 파라미터를 갖는 상기 학습된 인공지능 모델에 상기 광선의 방향 및 상기 3차원 좌표 값을 입력하여, 이미지 픽셀 정보를 획득하는 단계를 포함할 수 있다. 방법은,상기 획득된 이미지 픽셀 정보에 기초하여 상기 적어도 하나의 키 프레임을 획득하는 단계를 포함할 수 있다., 방법은,상기 획득된 적어도 하나의 키 프레임에 기초하여 제2 맵 데이터를 획득하는 단계를 포함할 수 있다. 본 개시의 일 실시예에 있어서, 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, ‘비일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다는 것을 의미할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적 으로 저장되는 경우를 구분하지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포 함할 수 있다. 본 개시의 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품 (computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두개의 사용자 장치들(예: 스마 트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프 로그램 제품(예: 다운로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서 버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생 성될 수 있다.도면 도면1 도면2 도면3a 도면3b 도면4a 도면4b 도면5a 도면5b 도면6a 도면6b 도면7a 도면7b 도면8 도면9 도면10"}
{"patent_id": "10-2023-0125012", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 개시는, 다음의 자세한 설명과 그에 수반되는 도면들의 결합으로 쉽게 이해될 수 있으며, 참조 번호 (reference numerals)들은 구조적 구성요소(structural elements)를 의미한다. 도 1은 본 개시의 일 실시예에 따른 전자 장치의 동작을 설명하기 위한 개념도이다. 도 2는 본 개시의 일 실시예에 따른 전자 장치의 구성 요소를 보여주는 블록도이다. 도 3a는 본 개시의 일 실시예에 따른 동시적 위치 추정 및 맵 작성 방법을 보여주는 흐름도이다. 도 3b는 본 개시의 일 실시예에 따른 광선의 방향과 3차원 지점 샘플링을 보여주는 개념도이다. 도 4a는 본 개시의 일 실시예에 따른 동시적 위치 추정 및 맵 작성 방법을 보여주는 흐름도이다. 도 4b는 본 개시의 일 실시예에 따른 동시적 위치 추정 및 맵 작성 방법을 보여주는 흐름도이다.도 5a 및 5b는 본 개시의 일 실시예에 따른 사용자 정보를 이용하는 동시적 위치 추정 및 맵 작성 방법을 설명 하기 위한 개념도이다. 도 6a 및 6b는 본 개시의 일 실시예에 따른 조명 값을 이용하는 동시적 위치 추정 및 맵 작성 방법을 설명하기 위한 개념도이다. 도 7a는 본 개시의 일 실시예에 따른 인공지능 모델의 파라미터를 업데이트하는 방법을 보여주는 흐름도이다. 도 7b는 본 개시의 일 실시예에 따른 공간에 속하는 객체의 변화를 반영하는 동시적 위치 추정 및 맵 작성 방법 을 보여주는 흐름도이다. 도 8은 본 개시의 일 실시예에 따른 공간에 속하는 객체의 변화를 반영하는 동시적 위치 추정 및 맵 작성 방법 을 설명하기 위한 개념도이다. 도 9는 본 개시의 일 실시예에 따른 하드웨어 성능 값을 이용하는 동시적 위치 추정 및 맵 작성 방법을 설명하 기 위한 개념도이다. 도 10은 본 개시의 일 실시예에 따른 전자 장치의 구성 요소 및 사용자 단말의 구성 요소를 보여주는 블록도이 다."}
