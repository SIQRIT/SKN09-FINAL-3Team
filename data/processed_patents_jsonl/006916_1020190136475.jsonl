{"patent_id": "10-2019-0136475", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0050409", "출원번호": "10-2019-0136475", "발명의 명칭": "저장 매체 액세스의 인공 지능 가능 관리", "출원인": "마벨 월드 트레이드 리미티드", "발명자": "세레느 크리스토프"}}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "저장 매체 액세스의 인공 지능 기반 관리를 위한 방법으로서, 호스트 시스템으로부터 그리고 저장 시스템의 호스트 인터페이스를 통하여, 상기 저장 시스템의 저장 매체에 액세스하기 위한 호스트 입/출력(I/O)을 수신하는 단계;상기 호스트 시스템으로부터 수신된 호스트 I/O를 설명하는 정보를 상기 저장 시스템과 관련된 인공 지능 엔진에 제공하는 단계;상기 호스트 시스템에 의한 상기 저장 매체에 대한 후속 액세스에 관련된 호스트 시스템 행동의 예측을 상기 인공 지능 엔진으로부터 수신하는 단계; 및 상기 호스트 시스템 행동의 예측에 기초하여, 상기 저장 시스템의 저장 매체에 액세스하기 위한 호스트 I/O를스케줄링하는 단계를 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 저장 시스템이 보류중인(pending) 내부 I/O를 가지고 있는지를 결정하는 단계를 더 포함하고, 상기 스케줄링하는 단계는, 상기 호스트 시스템 행동의 예측에 기초하여, 상기 저장 시스템의 저장 매체에 액세스하기 위해 상기 호스트 시스템의 호스트 I/O 및 상기 저장 시스템의 내부 I/O를 스케줄링하는 단계를 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 저장 시스템의 내부 I/O를 스케줄링하는 단계는, 상기 내부 I/O 및 호스트 I/O 또는 상기 저장 매체에 액세스하기 위한 후속 호스트 I/O 사이의 경합을 완화시키도록, 상기 호스트 시스템 행동의 예측에 기초하여, 상기 저장 시스템의 내부 I/O를 속행(advancing) 또는 지연시키는 단계를 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서, 상기 내부 I/O는, 가비지 콜렉션, 데이터 마이그레이션, 또는 웨어 레벨링(wear leveling) 중 하나를 포함하는저장 시스템의 플래시 변환 계층(Flash translation layer) 중 하나 이상의 작업들에 대응하는 것을 특징으로하는 방법."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 인공 지능 엔진으로부터 수신된 호스트 시스템 행동의 예측은, 호스트 시스템이 유휴 상태가 될 때까지의 시간 기간에 대한 표시;호스트 시스템이 유휴 상태로 유지되는 시간 기간에 대한 표시; 또는호스트 시스템에 의해 발행된 다음 호스트 I/O에 관한 파라미터들에 대한 표시를 포함하는 것을 특징으로 하는공개특허 10-2020-0050409-3-방법."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 호스트 I/O를 설명하는 정보는, 적어도 하나의 호스트 I/O에 대한, 호스트 I/O의 이벤트 유형에 대한 표시;호스트 I/O의 이벤트 기간에 대한 표시; 또는호스트 I/O와 관련된 데이터의 이벤트 크기에 대한 표시를 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 호스트 I/O를 스케줄링하는 단계는, 상기 저장 매체의 온도 관리를 위한 저장 매체의 디바이스 레벨 파라미터들에 기초하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 인공 지능 엔진은 다수의 인공 지능 모델들을 실행하고;다수의 인공 지능 모델들 중 적어도 2 개는 상기 저장 시스템의 플래시 변환 계층 또는 디바이스 레벨 관리자에의해 구현되는 각각의 내부 작업들과 관련되며; 그리고 상기 방법은 상기 정보를 인공 지능 엔진에 제공하기 전에, 호스트 시스템 행동의 예측이 가능하도록 상기 다수의 인공 지능 모델들 중 적어도 하나를 인공 지능 엔진에 로딩하는 단계를 더 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 저장 시스템의 적어도 2개의 인공 지능 보조 내부 작업들(artificial intelligence-assisted internaltasks)을 구현하도록, 상기 인공 지능 엔진을 통해 상기 다수의 인공 지능 모델들 중 적어도 2개를 동시에 실행하는 단계를 더 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서, 상기 인공 지능 모델의 온라인 재-트레이닝(online re-training) 또는 개량(refinement)이 가능하도록, 상기 인공 지능 엔진을 통해 상기 다수의 인공 지능 모델들 중 한 모델의 2개의 인스턴스들을 병렬로 실행하는 단계를더 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "장치로서, 호스트 시스템과 통신하도록 구성된 호스트 인터페이스;호스트 시스템의 데이터를 저장하는 저장 매체;상기 저장 매체에 대한 액세스를 가능하게하는 매체 인터페이스;인공 지능 엔진; 및매체 액세스 관리자를 포함하고, 상기 매체 액세스 관리자는,공개특허 10-2020-0050409-4-상기 호스트 인터페이스를 통해, 상기 장치의 저장 매체에 액세스하기 위한 호스트 입/출력(I/O)을 상기 호스트시스템으로부터 수신하고;상기 호스트 시스템으로부터 수신된 호스트 I/O를 설명하는 정보를 상기 인공 지능 엔진에 제공하고;상기 호스트 시스템에 의한 상기 저장 매체에 대한 후속 액세스에 관련된 호스트 시스템 행동의 예측을 상기 인공 지능 엔진으로부터 수신하고; 및 상기 호스트 시스템 행동의 예측에 적어도 일부 기초하여, 상기 장치의 저장 매체에 액세스하기 위한 호스트I/O를 스케줄링하는 것을 특징으로 장치."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 매체 액세스 관리자는 또한, 상기 장치가 상기 저장 매체에 액세스하기 위한 보류중인(pending) 내부 I/O를 가지고 있는지를 결정하고; 상기 호스트 시스템 행동의 예측에 기초하여, 상기 장치의 저장 매체에 액세스하기 위해 상기 호스트 시스템의호스트 I/O 및 상기 장치의 내부 I/O를 스케줄링하는 것을 특징으로 하는 장치."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서, 상기 인공 지능 엔진으로부터 수신된 호스트 시스템 행동의 예측은, 상기 호스트 시스템이 유휴 상태가 될 때까지의 시간 기간, 상기 호스트 시스템이 유휴 상태로 유지되는 시간기간, 또는 상기 호스트 시스템에 의해 발행된 다음 호스트 I/O에 관한 파라미터들 중 적어도 하나에 대한 표시를 포함하고, 상기 호스트 I/O를 설명하는 정보는, 적어도 하나의 호스트 I/O에 대한, 호스트 I/O의 이벤트 유형에 대한표시, 호스트 I/O의 이벤트 기간에 대한 표시, 또는 호스트 I/O와 관련된 데이터의 이벤트 크기에 대한 표시를포함하는 것을 특징으로 하는 장치."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서, 상기 인공 지능 엔진은 다수의 인공 지능 모델들을 실행하고, 상기 다수의 인공 지능 모델들 중 적어도 2 개는상기 장치의 플래시 변환 계층 또는 디바이스 레벨 관리자에 의해 구현되는 각각의 내부 작업들과 관련되며; 그리고 상기 매체 액세스 관리자는 또한, 상기 정보를 인공 지능 엔진에 제공하기 전에, 호스트 시스템 행동의 예측이가능하도록 상기 다수의 인공 지능 모델들 중 적어도 하나를 상기 인공 지능 엔진에 로딩하는 것을 특징으로 하는 장치."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서, 상기 매체 액세스 관리자는 또한, 상기 인공 지능 엔진으로 하여금, 상기 장치의 적어도 2개의 인공 지능 보조 내부 작업들을 구현하도록, 상기다수의 인공 지능 모델들 중 적어도 2개를 동시에 실행하게 하거나; 또는 상기 인공 지능 엔진으로 하여금, 상기 인공 지능 모델의 온라인 재-트레이닝 또는 개량이 가능하도록, 상기 다수의 인공 지능 모델들 중 한 모델의 2개의 인스턴스들을 병렬로 실행하게 하는 것을 특징으로 하는 장치."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "시스템 온 칩(SoC)으로서, 공개특허 10-2020-0050409-5-저장 시스템의 저장 매체에 액세스하기 위한 매체 인터페이스;호스트 시스템과 통신하는 호스트 인터페이스;인공 지능 엔진;하드웨어 기반 프로세서;프로세서 실행가능 명령들을 저장하는 메모리를 포함하고, 상기 명령들은 상기 하드웨어 기반 프로세서에 의한실행에 응답하여, 매체 액세스 관리자를 구현하고, 상기 매체 액세스 관리자는, 상기 호스트 인터페이스를 통해, 상기 저장 시스템의 저장 매체에 액세스하기 위한 호스트 입/출력(I/O)을 상기호스트 시스템으로부터 수신하고;상기 호스트 시스템으로부터 수신된 호스트 I/O를 설명하는 정보를 상기 인공 지능 엔진에 제공하고;상기 호스트 시스템에 의한 상기 저장 매체에 대한 후속 액세스에 관련된 호스트 시스템 행동의 예측을 상기 인공 지능 엔진으로부터 수신하고; 및 상기 호스트 시스템 행동의 예측에 적어도 일부 기초하여, 상기 저장 시스템의 저장 매체에 액세스하기 위한 호스트 I/O를 스케줄링하는 것을 특징으로 하는 시스템 온 칩."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서, 상기 매체 액세스 관리자는 또한, 상기 SoC가 상기 저장 매체에 액세스하기 위한 보류중인(pending) 내부 I/O를 가지고 있는지를 결정하고; 상기 호스트 시스템 행동의 예측에 기초하여, 상기 SoC의 저장 매체에 액세스하기 위해 상기 호스트 시스템의호스트 I/O 및 상기 SoC의 내부 I/O를 스케줄링하며, 상기 내부 I/O를 스케줄링하는 것은, 상기 내부 I/O와 호스트 I/O 사이의 경합을 완화시키도록 상기 SoC의 내부I/O를 속행(advancing) 또는 지연시키는 것을 특징으로 하는 시스템 온 칩."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항에 있어서, 상기 인공 지능 엔진으로부터 수신된 호스트 시스템 행동의 예측은, 상기 호스트 시스템이 유휴 상태가 될 때까지의 시간 기간, 상기 호스트 시스템이 유휴 상태로 유지되는 시간기간, 또는 상기 호스트 시스템에 의해 발행된 다음 호스트 I/O에 관한 파라미터들 중 적어도 하나에 대한 표시를 포함하고, 상기 호스트 I/O를 설명하는 정보는, 적어도 하나의 호스트 I/O에 대한, 호스트 I/O의 이벤트 유형에 대한표시, 호스트 I/O의 이벤트 기간에 대한 표시, 또는 호스트 I/O와 관련된 데이터의 이벤트 크기에 대한 표시를포함하는 것을 특징으로 하는 시스템 온 칩."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제16항에 있어서, 상기 매체 액세스 관리자는 또한, 상기 호스트 시스템 행동의 예측 및 상기 저장 매체의 온도 관리를 위한 저장 매체의 디바이스 레벨 파라미터들에 기초하여 호스트 I/O를 스케줄링하는 것을 특징으로 하는 시스템 온 칩."}
{"patent_id": "10-2019-0136475", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제16항에 있어서, 상기 인공 지능 엔진은 다수의 인공 지능 모델들을 실행하고, 상기 다수의 인공 지능 모델들 중 적어도 2 개는상기 SoC의 플래시 변환 계층 또는 디바이스 레벨 관리자에 의해 구현되는 각각의 내부 작업들과 관련되며; 그공개특허 10-2020-0050409-6-리고 상기 매체 액세스 관리자는 또한, 상기 인공 지능 엔진으로 하여금, 상기 SoC의 적어도 2개의 인공 지능 보조 내부 작업들을 구현하도록, 상기 다수의 인공 지능 모델들 중 적어도 2개를 동시에 실행하게 하거나; 또는 상기 인공 지능 엔진으로 하여금, 상기 인공 지능 모델의 온라인 재-트레이닝 또는 개량이 가능하도록, 상기 다수의 인공 지능 모델들 중 한 모델의 2개의 인스턴스들을 병렬로 실행하게 하는 것을 특징으로 하는 시스템 온칩."}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 인공 지능 가능 저장 매체의 관리를 위한 장치 및 방법을 설명한다. 일부 양상들에서, 저장 매체 시스 템의 매체 액세스 관리자는 호스트 시스템으로부터 저장 매체 시스템의 저장 매체에 대한 액세스를 위한 호스트 입/출력 명령 (I/O)을 수신한다. 미디어 액세스 관리자는 호스트 I/O를 기술하는 정보를 인공 지능 엔진에 제공 하고 인공 지능 엔진으로부터 저장 매체의 후속 액세스에 대한 호스트 시스템 거동의 예측을 수신한다. 그런 다 음 매체 액세스 관리자는 호스트 시스템 동작의 예측에 따라 저장 시스템의 저장 매체에 액세스하기 위한 호스트 I/O를 예약한다. 그렇게 함으로써, 호스트 I/O는 저장 시스템의 내부 I/O와의 충돌을 피하거나 다가오는 유휴 시 간에 기초하여 다양한 임계값을 선점하기 위해 저장 매체의 호스트 시스템 액세스를 최적화하도록 스케줄링될 수 있다."}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "관련 출원들에 대한 상호 참조 본 출원은 2019년 10월 25일자 미국특허출원(16/664,528)의 우선권을 주장하며, 상기 미국 출원은 2018년 10월 30일자 미국특허출원(62/752,876)의 우선권을 주장한다. 이들 미국 출원들은 본 발명에 대한 참조로서, 그 전체 내용이 본 명세서에 통합된다."}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "다수의 컴퓨팅 디바이스 및 전자 디바이스는 소프트웨어, 어플리케이션, 또는 상기 디바이스의 데이터를 저장하 기 위한 비휘발성 메모리를 포함한다. 또한, 대부분의 사용자는 다양한 위치들 또는 이동중에 데이터 네트워크 를 통해 그들의 디바이스로 멀티미디어 컨텐츠 또는 소셜 미디어 어플리케이션과 같은 데이터를 스트리밍하거 나 서비스에 액세스한다. 데이터 및 서비스에 대한 사용자의 증가하는 수요로 인해, 스토리지 제공자는 사용자 및 기타 데이터 저장 클라이언트의 이러한 활동과 관련된 데이터 액세스를 지원하기 위해 저장 드라이브의 용량 및 성능을 향상시켜 왔다. 전형적으로, 디바이스의 저장 드라이브는 디바이스의 데이터가 기입되고 판독되는 저 장 매체를 포함한다. 그렇게하기 위해, 디바이스는 데이터 액세스 요청을 저장 드라이브로 발행할 수 있으며, 저장 드라이브는 각 요청에 의해 지정된 바와 같이 저장 매체에 데이터를 기입하거나 저장 매체로부터 데이터를 판독한다. 따라서, 저장 드라이브 성능은 일반적으로 저장 드라이브가 디바이스 또는 저장 클라이언트의 데이터 액세스 요청을 완료할 수 있는 속도에 달려있다."}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "저장 드라이브의 저장 매체는 디바이스로부터 수신된 데이터 액세스 요청들에만 전적으로 기초하여 액세스되는 것은 아니다. 저장 드라이브 자체는 저장 매체의 건강 상태 또는 유지 관리와 관련된 다양한 내부 동작들을 구 현할 수 있다. 종래의 저장 드라이브에서, 이러한 내부 드라이브 동작들과 관련된 저장 매체에 대한 액세스는 계획되지 않았으며, 디바이스의 데이터 요청을 서비스하기 위한 저장 매체로의 액세스와 충돌할 수 있다. 따라 서, 저장 드라이브의 내부 동작이 디바이스의 데이터 기입 동작 또는 데이터 판독 동작과 간섭하는 액세스 충돌 을 야기하는 경우, 저장 드라이브 전체적인 성능이 저하될 수 있는바, 왜냐하면 데이터 요청 레이턴시가 증가하 고 저장 드라이브의 데이터 처리량이 감소하기 때문이다."}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 요약 부분은 본 발명의 주제를 소개하기 위해 제공되었으며, 본 발명의 주제는 상세한 설명 및 도면에서 보"}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "다 상세히 설명된다. 따라서, 본 요약 부분은 본질적인 특징들을 설명하는 것으로 간주되지 않야아할 뿐만 아니 라, 청구된 본 발명의 주제의 범위를 제한하기 위해 사용되지 않아야 한다. 일부 양상들에서, 저장 매체 시스템의 매체 액세스 관리자는 방법을 구현하며, 상기 방법은 호스트 시스템으로 부터 그리고 저장 시스템의 호스트 인터페이스를 통하여, 저장 시스템의 저장 매체에 액세스하기 위한 호스트 입/출력(I/O)을 수신하는 단계를 포함한다. 매체 액세스 관리자는 호스트 시스템으로부터 수신된 호스트 I/O를 설명하는 정보를 상기 저장 시스템과 관련된 인공 지능 엔진에 제공한다. 매체 액세스 관리자는 상기 호스트 시 스템에 의한 상기 저장 매체에 대한 후속 액세스에 관련된 호스트 시스템 행동의 예측을 상기 인공 지능 엔진으로부터 수신한다. 매체 액세스 관리자는 상기 호스트 시스템 행동의 예측에 기초하여, 상기 저장 시스템의 저장 매체에 액세스하기 위한 호스트 I/O를 스케줄링한다. 다른 양상들에서, 장치가 제공되며, 상기 장치는 호스트 시스템과 통신하도록 구성된 호스트 인터페이스, 호스 트 시스템의 데이터를 저장하는 저장 매체, 및 상기 저장 매체에 대한 액세스를 가능하게하는 매체 인터페이스 를 포함한다. 상기 장치는 또한, 인공 지능 엔진 및 매체 액세스 관리자를 포함하고, 매체 액세스 관리자는, 호 스트 인터페이스를 통해, 상기 장치의 저장 매체에 액세스하기 위한 호스트 입/출력(I/O)을 호스트 시스템으로 부터 수신한다. 매체 액세스 관리자는 호스트 시스템으로부터 수신된 호스트 I/O를 설명하는 정보를 인공 지능 엔진에 제공한다. 다음으로, 매체 액세스 관리자는 호스트 시스템에 의한 상기 저장 매체에 대한 후속 액세스에 관련된 호스트 시스템 행동의 예측을 상기 인공 지능 엔진으로부터 수신한다. 상기 호스트 시스템 행동의 예측 에 적어도 일부 기초하여, 매체 액세스 관리자는 상기 장치의 저장 매체에 액세스하기 위한 호스트 I/O를 스케 줄링한다. 또 다른 양상에서, 시스템-온-칩(SoC)이 제공되며, SoC는 저장 시스템의 저장 매체에 액세스하기 위한 매체 인 터페이스, 호스트 시스템과 통신하는 호스트 인터페이스, 및 인공 지능 엔진을 포함한다. SoC는 또한, 하드웨어 기반 프로세서 및 프로세서 실행가능 명령들을 저장하는 메모리를 포함하고, 상기 명령들은 하드웨어 기반 프로 세서에 의한 실행에 응답하여, 매체 액세스 관리자를 구현하고, 매체 액세스 관리자는, 호스트 인터페이스를 통 해, 저장 시스템의 저장 매체에 액세스하기 위한 호스트 입/출력(I/O)을 호스트 시스템으로부터 수신한다. 매체 액세스 관리자는 호스트 시스템으로부터 수신된 호스트 I/O를 설명하는 정보를 인공 지능 엔진에 제공한다. 다 음으로, 매체 액세스 관리자는 호스트 시스템에 의한 저장 매체에 대한 후속 액세스에 관련된 호스트 시스템 행 동의 예측을 인공 지능 엔진으로부터 수신한다. 호스트 시스템 행동의 예측에 적어도 일부 기초하여, 매체 액세 스 관리자는 저장 시스템의 저장 매체에 액세스하기 위한 호스트 I/O를 스케줄링한다. 하나 이상의 구현예들의 세부 내용은 첨부 도면 및 이하의 설명에서 개시될 것이다. 다른 특징 및 장점은 상세 한 설명 및 도면 및 청구 범위로부터 명백할 것이다. 저장 매체 액세스의 인공 지능 기반(AI 기반) 관리의 하나 이상의 구현예들의 세부 내용이 첨부 도면 및 아래의 상세한 설명 부분에서 설명된다. 도면들에서, 참조 번호의 가장 왼쪽 자리는 참조 번호가 처음 나타나는 도면을 나타낸다. 설명 및 도면들에서 상이한 경우에 동일한 참조 번호를 사용하는 것은 유사한 요소를 나타낸다."}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "저장 매체에 대한 액세스를 관리하기 위한 종래의 기법은 종종 충돌되거나 또는 비효율적인 저장 매체 액세스를 야기하며, 이는 저장 드라이브 성능을 저하시킨다. 일반적으로, 저장 매체의 데이터에 대한 액세스를 요청하는 디바이스로부터 수신된 데이터 커맨드를 번역하는 것과 같이, 엔드-투-엔드 방식으로 저장 드라이브의 데이터 경로를 관리하는데 저장 드라이브 펌웨어가 사용된다. 동작 중에 저장 매체의 건강 상태 또는 유지 관리 문제가 발생하면, 저장 드라이브의 펌웨어는 디바이스의 데이터 커맨드와 관련된 저장 매체 액세스에 대해, 이러한 내 부 작업들을 용이하게 하기 위해 저장 매체 액세스를 스케줄링하는 것이 일반적이다. 저장 매체의 자원을 이용 하는, 디바이스의 데이터 커맨드는 또한 저장 매체의 지속적인 건강 상태 또는 유지 관리 문제에 영향을 미칠 수 있다. 다시 말해서, 종래의 기술은 종종 저장 매체의 건강 상태 또는 저장 매체의 유지 보수의 현재 상태에 기초하여, 저장 매체에 대한 액세스를 스케쥴하는데, 이는 저장 드라이브 요구 및 성능에 대한 단기적인 관점 또는 협소한 관점이다. 이와 같이, 액세스 관리를 위한 종래의 기술은 종종 충돌되거나 또는 비효율적인 저장 매체에 대한 액세스를 야기하며, 이는 저장 드라이브의 성능을 저하시키거나 감소시킨다. 본 발명은 저장 매체 액세스의 AI 기반의 관리를 위한 장치 및 방법을 설명한다. 저장 매체 액세스에 대한 종래 의 기술과는 대조적으로, 본 발명의 장치 및 기술은, 최적화된 저장 드라이브 성능이 획득되도록 호스트 입력/ 출력(I/O) 또는 내부 I/O의 효율적이면서 조정된 스케쥴링을 위해, 저장 매체 액세스에 대한 AI-기반 관리를 구 현할 수 있다. 일부 양상들에서, 저장 컨트롤러(매체 액세스 관리자)의 펌웨어는 기본 또는 전용 AI 하드웨어 및/또는 펌웨어에서 구동되는 인공 지능(AI) 모델을 사용할 수 있는바, 이는 저장 매체 액세스를 관리하고 저장 드라이브 성능를 향상시키도록 상기 펌웨어에 의해 유용한 활동을 예측 또는 예상하기 위한 것이다. 예를 들어, AI를 사용하는 AI 엔진은 저장 컨트롤러의 내부 펌웨어/소프트웨어에 대한 관련 외부 이벤트들(예컨대, 호스트 시스템 활동)을 예측할 수 있다. AI-기반의 저장 컨트롤러는 현재 디바이스 상태 또는 과거 이벤트들의 이력만 에 기초하는 것이 아니라, 예측된 외부 이벤트들에 기초하여, 내부 저장 컨트롤러-관련 작업들을 언제 수행할지 를 결정할 수 있다. 일반적으로, 저장 매체 액세스에 대한 AI 기반 관리의 소정 양상들은, 저장 시스템(예를 들어, 저장 드라이브) 에 대한 예측된 호스트 시스템 행동을 통해, 호스트 I/O 및/또는 내부 I/O에 대한 지능형 예측 및 적응형 스케 줄링을 구현할 수 있다. 경우에 따라 AI 지원 이벤트 예측을 통해 최적화된 내부 I/O 스케줄링이 가능할 수 있 다. 다른 경우에, 매체 액세스 관리자는 다가오는 유휴 시간 또는 호스트 시스템 활동의 예상된 감소에 기초하 여 호스트 I/O의 성능을 허용하기 위해 임계값(예를 들어, 가비지 컬렉션 또는 열 제한)을 선점(preempt), 중지 (suspend) 또는 무시(disregard)할수 있다. 또한, 매체 액세스 관리자 및 AI 엔진은 저장 매체 디바이스의 열 관리 또는 전력 관리를 포함하는 디바이스-레벨 작업들 뿐만 아니라, 다양한 플래시 변환 계층(FTL) 관리 동작 들(예컨대, 캐싱, 마이그레이션(이주) 또는 가비지 컬렉션)을 최적화할 수 있다. 다양한 양상들에서, 저장 컨트롤러의 AI 엔진 및 AI 모델은, 호스트 시스템 I/O 행동(예를 들어, 기입 밀도) 또 는 저장 매체 액세스의 다른 파라미터들 뿐만 아니라, 호스트 시스템의 특정한 유휴(idle) 관련 이벤트들을 예 측할 수 있다(예를 들면, 다음의 유휴까지의 시간, 다음 유휴의 지속 기간). 일반적으로, 저장 컨트롤러의 AI엔진은 다수의 AI 모델들을 구현하거나 관리할 수 있으며, AI 지원의 혜택을 받을 수 있는 저장 시스템의 다양 한 내부 작업들에 기초하여 AI 하드웨어 또는 AI 펌웨어에 로딩될 수 있다. 경우에 따라 이러한 다중 또는 다른 AI 모델들은 기본 또는 AI 하드웨어에서 동시에 실행 또는 구동되어 여러 내부 작업들에 대한 동시 AI 지원을 제공한다. 대안적으로 또는 추가적으로, AI 엔진은 AI 모델의 온라인(런-타임) 재트레이닝 또는 개량 (refinement)을 수행하여, 사용자-특정 또는 호스트 시스템-특정 I/O 워크로드에 동적으로 적응할 수 있다. 저장 매체 액세스의 AI 기반 관리의 다양한 양상들에서, 저장 매체 시스템의 매체 액세스 관리자는 호스트 시스 템으로부터 저장 매체 시스템의 저장 매체에 액세스하기 위한 호스트 I/O를 수신한다. 매체 액세스 관리자는 호 스트 시스템으로부터 수신한 호스트 I/O를 설명하는 정보를 인공 지능 엔진에 제공한다. 다음으로, 매체 액세스 관리자는 인공 지능 엔진으로부터 저장 매체의 후속 액세스와 관련하여 호스트 시스템 행동의 예측을 수신한다. 호스트 시스템 행동의 예측에 기초하여, 매체 액세스 관리자는 저장 시스템의 저장 매체에 대한 액세스를 위해 호스트 I/O를 스케줄링한다. 그렇게함으로써, 저장 시스템의 내부 I/O와의 충돌을 회피하거나 또는 다가오는 유 휴 시간에 기초하여 다양한 임계값들 또는 파라미터들을 선점하는 것과 같이, 저장 매체에 대한 호스트 시스템 액세스를 최적화하도록 호스트 I/O가 스케줄링될 수 있다. 다음 논의는 동작 환경, 동작 환경에서 구현될 수 있는 기술들, 및 동작 환경의 컴포넌트들이 구체화될 수 있는 시스템 온칩(SoC)을 설명한다. 본 개시의 맥락에서, 단지 예시로서 동작 환경이 참조된다. 동작 환경 도 1은 다양한 형태의 데이터 또는 정보를 저장 또는 액세스할 수 있는 호스트 시스템을 갖는 예시적인 동 작 환경을 도시한다. 호스트 시스템의 예는 랩톱 컴퓨터, 데스크탑 컴퓨터 및 서버를 포함할 수 있으며, 이들 중 임의의 것은 사용자 디바이스, 컴퓨팅 디바이스, 또는 스토리지 네트워크 또는 클라 우드 스토리지의 일부로서 구성될 수 있다. 호스트 시스템(도시되지 않음)의 다른 예는 태블릿 컴퓨터, 셋 톱 박스, 데이터 저장 디바이스, 웨어러블 스마트 디바이스, 텔레비전, 콘텐츠 스트리밍 디바이스, 고화질 멀티 미디어 인터페이스(HDMI) 미디어 스틱, 스마트 기기, 홈 오토메이션 컨트롤러, 스마트 서모스탯, 사물 인터넷 (IoT: Internet-of-Things) 디바이스, 모바일 인터넷 디바이스(MID), 네트워크-부착 스토리지(NAS: Network- Attached Storage) 드라이브, 통합 저장 시스템, 게임 콘솔, 자동차 엔터테인먼트 디바이스, 자동차 컴퓨팅 시 스템, 자동차 제어 모듈(예를 들어, 엔진 또는 파워 트레인 제어 모듈) 등을 포함한다. 일반적으로, 호스트 시 스템은 특정 유형의 디바이스의 기능을 가능하게하고, 사용자 인터페이스를 제공하며, 네트워크 액세스를 가능하게하고, 게임 어플리케이션을 구현하고, 미디어를 재생하고, 네비게이션을 제공하고, 컨텐츠를 편집하고, 데이터 저장소를 제공하는 것 등과 같은 임의의 적합한 목적을 위해 데이터를 통신하거나 저장할 수 있다. 호스트 시스템은 프로세서 및 컴퓨터 판독가능 매체를 포함한다. 프로세서는 호스트 시스 템의 운영 시스템 또는 다른 어플리케이션들의 명령들 또는 커맨드들을 실행하기 위해 딘일 코어 또는 멀 티 코어인 임의의 적절한 유형 또는 개수의 프로세서들로 구현될 수 있다. 컴퓨터 판독가능 매체(CRM: 112)는 호스트 시스템의 메모리(도시되지 않음) 및 저장 시스템을 포함한다. 호스트 시스템의 메모리는 임의의 적합한 유형 또는 휘발성 메모리 또는 비휘발성 메모리의 조합을 포함할 수 있다. 예를 들어, 호스트 시스템의 휘발성 메모리는 다양한 유형들의 랜덤 액세스 메모리(RAM), 동적 RAM(DRAM), 정적 RAM(SRAM) 등을 포함할 수 있다. 비휘발성 메모리는 ROM(Read-Only Memory), EEPROM(Electronically Erasable Programmable ROM) 또는 플래시 메모리(예를 들어, NAND Flash)를 포함할 수 있다. 이들 메모리는 개별적으로 또는 조합하여 사용자, 어플리케이션 및/또는 호스트 시스템의 운영 체제와 관련된 데이터를 저장할 수 있 다. 호스트 시스템의 저장 디바이스는 저장 디바이스, 저장 드라이브, 저장 어레이, 저장 볼륨 등과 같은 데이터 저장 시스템의 임의의 적절한 형태로 구성될 수 있다. 호스트 시스템을 참조하여 설명되었지만, 저 장 시스템은 독립형 디바이스로서 또는 네트워크 부착 저장 디바이스, 외부 저장 드라이브, 데이터 센터, 서버 팜 또는 가상화된 저장 시스템(예: 클라우드 기반 스토리지 또는 서비스)과 같은 더 큰 저장용량 집단의 일부로서 개별적으로 구현될 수 있다. 저장 시스템의 예는 비휘발성 메모리 익스프레스(NVMe) 솔리드 스테 이트 드라이브, PCIe(Peripheral Component Interconnect Express) 솔리드 스테이트 드라이브, 솔리 드 스테이트 드라이브(SSD 120) 및 저장 어레이를 포함하며, 이는 저장 디바이스들 또는 저장 드라이 브들의 임의의 조합으로 구현될 수 있다. 저장 시스템은 저장 매체 및 저장 시스템의 다양한 동작 또는 기능을 관리하기 위한 저장 매체 컨트롤러(저장 컨트롤러 126)를 포함한다. 저장 매체는 호스트 시스템의 정보 또는 데이터가 저장되는 비휘발성 메모리 디바이스들을 포함하거나 이로부터 형성될 수 있다. 저장 매체는 플래시, NAND 플래시, RAM, DRAM(예를 들어, 캐싱을 위해), SRAM 등과 같은 솔리드-스테이트 메모리 매체의 임의의 유형 또는 조합으로 구현될 수 있다. 일부 경우에, 저장 매체에 저장되는 데이터는 저장 시스템에 저 장되고 호스트 시스템에 의해 액세스되는 데이터 파일들(예를 들어, 컨텐츠) 또는 데이터 객체로 구성된다. 데이터의 파일 유형, 사이즈 또는 포맷은 파일과 관련된 각각의 소스, 사용 또는 어플리케이션 에 따라 달라질 수 있다. 예를 들어, 저장 시스템에 저장된 데이터는 오디오 파일, 비디오 파일, 텍 스트 파일, 이미지 파일, 멀티미디어 파일, 스프레드 시트 등을 포함할 수 있다. 솔리드 스테이트 메모리를 참 조하여 설명되었지만, 저장 매체 액세스의 AI 기반 관리의 양상들은 또한 자기 기반 또는 광학 기반 매체 유형 으로 구현될 수 있다. 일반적으로, 저장 컨트롤러는 저장 시스템의 동작을 관리하며, 호스트 시스템이 데이터 저장을 위해 저장 매체에 액세스할 수 있게 한다. 저장 컨트롤러는 저장 시스템의 다양한 기능을 제공 하기 위해 하드웨어, 펌웨어 또는 소프트웨어의 임의의 적절한 조합을 통해 구현될 수 있다. 저장 컨트롤러 는 또한 데이터 캐싱, 데이터 마이그레이션('이주' 라고도 함), 가비지 컬렉션, 열 관리(예: 스로틀 링), 전원 관리 등과 같은 저장 매체와 관련된 내부 작업 또는 동작을 관리 또는 감독할 수 있다. 이와 같이, 저장 컨트롤러는 데이터 액세스를 위해 호스트 시스템으로부터 호스트 I/O를 수신할 수 있고, 저장 매체에 대한 내부 동작과 관련된 내부 I/O를 큐잉(또는 생성)할 수 있다. 일반적으로, 저장 컨트롤러(12 6)는 데이터 액세스를 위해 스케줄된 호스트 I/O에 대응하는 저장 매체의 액세스를 위한 매체(media) I/O 및/또는 저장 매체와 관련된 내부 동작 또는 작업을 위한 내부 I/O를 수행할 수 있다. 본 실시예에서, 저장 컨트롤러는 또한 저장 매체 액세스 관리자(매체 액세스 관리자 130), 인공 지능 엔진(AI 엔진 132) 및 인공 지능 모델(AI 모델 134)를 포함한다. 다른 구성에서, 저장 컨트롤러(12 6)는 저장 컨트롤러와 별도로 구현되는 AI 엔진 또는 AI 모델에 액세스할 수 있다. 다양한 양상 에서, 저장 매체 액세스를 관리하고, 내부 작업들을 수행하고, 그리고 저장 드라이브 성능을 향상시키는데 유용 한 활동(예컨대, 호스트 활동 또는 비활동)에 대한 예측들 또는 예상들을 획득하기 위해 매체 액세스 관리자 는 AI 엔진 및 AI 모델을 이용한다. 일반적으로, 매체 액세스 관리자는 저장 시스템(예를 들어, 저장 드라이브)에 대한 예측된 호스트 시스템 행동을 통해, 호스트 I/O 및/또는 내부 I/O의 지능형 예측 및 적응형 스케줄링(예를 들어, 속행 또는 지연)을 구현할 수 있다. 경우에 따라, 이것은 AI 지원 이벤트 예측 을 통해 최적화된 내부 I/O 스케줄링을 가능케할 수 있다. 예를 들면, 매체 액세스 관리자는 저장 매체 액세스를 위한 호스트 I/O에 대한 표시를 AI 엔진 에 제공할 수 있다. AI 엔진은 호스트 I/O 활동의 표시들을 AI 모델에 대한 입력으로서 사용하여, 후 속 호스트 시스템 행동을 예측하거나 예상할 수 있다. 호스트 시스템 행동의 이러한 예측에 기초하여, 매체 액 세스 관리자는 호스트 I/O 및/또는 저장 시스템의 내부 I/O를 스케줄링할 수 있을 뿐만 아니라 저장 시스 템의 성능을 향상시키기 위해 사용자-특정 워크로드에 적응시킬 수 있다. AI 모델은 리커런트 뉴럴 네트워 크(RNN) 아키텍처에 기초한 AI 모델과 같은 임의의 적합한 유형의 모델을 포함할 수 있다. RNN 유형 아키텍처를 가진 AI 모델은 입력들의 히스토리를 프로세싱하기 위한 메모리를 구비할 수 있으며, 이들 모델들은 호스트 시 스템 동작 또는 향후 활동을 예측하는데 적합하다. AI 모델은 유휴 관련 파라미터(예를 들어, 다음 유휴까 지의 시간 또는 다음 유휴의 지속 시간) 또는 기입 밀도 파라미터(예를 들어, 소정 시간 동안 호스트 시스템이 얼마나 많은 데이터를 기입할 것으로 예상되는지)와 같은 임의의 적합한 파라미터를 통해 호스트 행동을 예측할 수 있다. 매체 액세스 관리자, AI 엔진 및 AI 모델이 구현되고 사용되는 방법은 다양하며 본 개 시에 걸쳐 설명된다. 호스트 시스템은 또한 I/O 포트, 그래픽 프로세싱 유닛(GPU 138), 및 데이터 인터페이스를 포함할 수 있다, 일반적으로, I/O 포트는 호스트 시스템이 다른 디바이스들, 주변 장치들 또는 사용 자들과 상호작용할 수 있게한다. 예를 들어, I/O 포트는 범용 직렬 버스, 휴먼 인터페이스 디바이스, 오디 오 입력, 오디오 출력 등을 포함하거나 이와 연결될 수 있다. GPU는 운영 체제, 어플리케이션 등의 사용자 인터페이스 요소와 같은 호스트 시스템에 대한 그래픽 관련 데이터를 처리하고 렌더링한다. 일부 경우에, GPU는 그래픽을 렌더링하기 위해 로컬 메모리의 일부에 액세스하거나 또는 호스트 시스템의 그래픽 (예를 들어, 비디오 RAM)을 렌더링하기 위한 전용 메모리를 포함할 수 있다. 호스트 시스템의 데이터 인터페이스는 하나 이상의 네트워크들 및 이들 네트워크에 연결된 다른 디바 이스에 대한 연결성을 제공한다. 데이터 인터페이스는 로컬 네트워크, 인트라넷 또는 인터넷을 통해 통신 하기 위한 이더넷 또는 광섬유 인터페이스와 같은 유선 인터페이스를 포함할 수 있다. 대안적으로 또는 추가적으로, 데이터 인터페이스는 무선 LAN, 광역 무선 네트워크(예를 들어, 셀룰러 네트워크) 및/또는 무선 개 인 영역 네트워크(WPAN)와 같은 무선 네트워크를 통한 통신을 용이하게 하는 무선 인터페이스를 포함할 수 있다. I/O 포트 또는 데이터 인터페이스를 통해 통신되는 임의의 데이터는 저장 매체 액세스의 AI 기 반 관리의 하나 이상의 양상들에 따라 호스트 시스템의 저장 시스템에 기입되거나 이로부터 판독될 수 있다. 도 2는 저장 매체 액세스의 AI 기반 관리의 하나 이상의 양상에 따라 구현되는 매체 액세스 관리자 및 AI 엔진(이들은 일반적으로 200)의 구성예를 도시한다. 이 예에서, 매체 액세스 관리자 및 AI 엔진(13 2)은 SSD(Solid-State Storage Drive)로서 구현되는 저장 시스템과 관련하여 예시된다. SSD는 임의의 적합한 호스트 시스템에 연결될 수 있으며 다수의 NAND 플래시 디바이스들(204-1 내지 204-n)을 포 함하는 저장 매체를 구비하며, 여기서 n은 임의의 적절한 정수이다. 경우에 따라, NAND Flash 디바이스 는 채널-레벨(디바이스들의 그룹) 또는 디바이스-레벨(개별 디바이스들)에서 액세스가능하거나 관리될 수 있는 메모리 디바이스들, 다이들, 또는 칩들의 다수의 플래시 채널을 포함한다. SSD의 컴포넌트로서 도시 되어 있지만, 매체 액세스 관리자 및/또는 AI 엔진은 저장 시스템과 별도로 또는 외부에 구현될 수 있다. 일부 경우에, 매체 액세스 관리자 또는 AI 엔진은 호스트 시스템과 하나 이상의 저장 시스템들 사이에 연결된 저장 매체 가속기 또는 집합 저장 컨트롤러의 일부로서 구현될 수도 있다. 일반적으로, SSD의 동작들은 저장 컨트롤러의 인스턴스에 의해 가능 혹은 관리되고, 이는 본 일례에 서, 호스트 시스템과의 통신을 가능케하는 호스트 인터페이스 및 저장 매체에 대한 액세스를 가 능케하는 매체 인터페이스를 포함한다. 호스트 인터페이스는 임의의 적합한 유형의 저장 인터페이스 또는 프로토콜 가령, SATA(serial advanced technology attachment), 범용 직렬 버스(USB), PCIe, 고급 호스 트 컨트롤러 인터페이스(AHCI: advanced host controller interface), NVMe, NVM-OF(NVM-over Fabric), NVMHCIS(NVM Host Controller Interface Specification), SCSI(Small Computer System Interface), SAS(Serial Attached SCSI), SDIO(Secure Digital I/O), 파이버 채널, 이들의 조합(예: M.2 또는 차세대 폼 팩 터(NGFF) 결합 인터페이스) 등을 구현하도록 구성될 수 있다. 대안적으로 또는 추가적으로, 매체 인터페이스 는 임의의 적합한 유형의 저장 매체 인터페이스 가령, 플래시 인터페이스, 플래시 버스 채널 인터페이스, NAND 채널 인터페이스, 물리 페이지 어드레싱(PPA) 인터페이스 등을 구현할 수 있다. 다양한 양상들에서, SSD 또는 저장 컨트롤러의 컴포넌트들은 호스트 시스템으로의 호스트 인터 페이스와 저장 매체로의 매체 인터페이스 사이에 데이터 경로를 제공한다. 본 일례에서, 저장 컨트롤러는 커널, 펌웨어를 실행하기 위한 프로세서 코어 또는 저장 컨트롤러의 기능을 구현하 기 위한 드라이버를 포함한다. 일부의 경우에서, 프로세서 코어는 또한, 프로세서 실행가능 명령들을 실행 하여 저장 컨트롤러의 매체 액세스 관리자 또는 AI 엔진을 구현할 수 있다. 대안적으로 또는 부 가적으로, 매체 액세스 관리자 또는 AI 엔진은 AI 전용 하드웨어 혹은 프로세서 코어에서 실행 또는 구동될 수 있다. 도 2에 도시된 바와 같이, 콘트롤들 및 데이터 버스들을 포함할 수 있는 저장 컨트롤러의 패브릭은 저장 컨트롤러의 컴포넌트들 사이에 동작가능하게 연결되어 통신을 가능케할 수 있다. 예를 들어, 매체 액 세스 관리자 또는 AI 엔진은 호스트 인터페이스, 프로세서 코어(예를 들어, 펌웨어) 또는 미디어 인터페이스와 통신하여, 저장 컨트롤러 내에서 데이터, 정보 또는 I/O를 교환할 수 있다. 저 장 컨트롤러의 정적 랜덤 액세스 메모리는 저장 컨트롤러의 펌웨어 혹은 드라이버들을 위한 프로세서 실행 가능한 명령들 또는 코드들을 저장할 수 있으며, 이는 프로세서 코어에 의해 실행될 수 있다. 또한, 저장 컨트롤러는 저장 컨트롤러가 호스트 시스템, 저장 매체, 또는 저장 컨트롤러의 다른 컴포 넌트들 사이에서 데이터를 이동시킴에 따라 다양한 데이터를 저장 또는 캐싱하기 위한 DRAM 제어기 및 관 련 DRAM을 포함할 수 있다. 도 3은 저장 매체 컨트롤러의 AI 엔진을 구현하기 위한 다양한 하드웨어 및 펌웨어 컴포넌트들의 예시적인 구성 을 도시한다. 본 일례에서, 저장 컨트롤러의 컴포넌트들은 저장 컨트롤러의 구성 요소는 펌웨어 또는 하드웨어에서 구현될 수 있다 추상적인 엔티티들로 도시된다. 하지만, 이것은 다양한 컴포넌트의 하 나의 예시적인 구현일 뿐이며, 이들 중 임의의 것은 본 명세서에 설명된 다른 컴포넌트와 별도로 또는 조합하여 구현될 수 있다. 대안적으로 또는 부가적으로, 도 2 또는 도 3을 참조하여 설명된 임의의 컴포넌트는 구성 요소 의 다양한 설명된 기능들을 제공하는 로직, 셀 및/또는 집적회로(IC) 단위로 구성된 지능형 속성 블록(IP 블록) 또는 IP 코어로서 구현될 수 있다. 예를 들어, 저장 컨트롤러의 컴포넌트(예를 들어, AI 엔진 132)는 컴포 넌트의 각각의 기능을 제공하거나 컴포넌트의 각각의 동작을 구현하기 위해 하드웨어, 펌웨어 또는 소프트웨어의 조합을 포함하는 IP 코어 또는 IP 블록으로서 이식될 수 있다. 본 실시예에서, 저장 컨트롤러의 하드웨어는 NAND 플래시 디바이스, 호스트 인터페이스, 매체 인터페이스 및 프로세서들을 포함할 수 있으며, 이들은 도 2를 참조하려 서술될 바와 같이 구현 될 수 있다. 일부 양상에서, AI 엔진은 AI 엔진이 실행 또는 구동되는 프로세서 코어를 포함하 는 AI 블록(예를 들어, AI IP 블록)의 일부로서 구현된다. AI 엔진은 또한 하나 이상의 AI 모델(13 4)을 실행할 수 있으며 및 저장 컨트롤러의 펌웨어와 AI 엔진이 상호작용할 수 있게하는 AI 엔 진 드라이버를 제공할 수 있다. 선택적으로 또는 부가적으로, AI 엔진(예를 들어, 경량 AI 엔진 132 및 모델 134)은 AI 엔진 드라이버를 제공하기 위해 저장 컨트롤러의 프로세서 코어 상에서 실행 될 수 있다. 일반적으로, 저장 컨트롤러의 펌웨어는 하드웨어를 보조하여, 호스트 시스템과 저장 매체 는 사이의 데이터 경로를 관리하는데 도움을 줄 수 있다. 즉, 펌웨어는 호스트 시스템으로부터 수신된 커맨드 또는 데이터 요청을 번역하여 저장 매체의 액세스를 가능하게 할 수 있다. 도 3에 도시된 바와 같이, 펌웨어는 호스트 커맨드 핸들러를 구현하는 호스트 인터페이스 드라이버 및 매체 커 맨드 관리자를 구현하는 미디어 인터페이스 드라이버를 포함한다. 도 3에 도시된 바와 같이, 호스트 입/출력 커맨드(호스트 I/O )는 호스트 커맨드 핸들러에 의해 수신되고, 저장 컨트롤러의 플래시 변환 계층(FTL 324)의 I/O 스케줄러로 전송된다. FTL 및/또는 I/O 스케줄러는 호 스트 I/O를 프로세싱 및 스케줄링할 수 있고, 이것은 이후 매체 커맨드 관리자를 통한 저장 매체 액 세스를 위한 대응하는 미디어 입력/출력 커맨드(매체 I/O)로서 수행될 수 있다. 다양한 양상들에서, FTL는, 저장 시스템 내에서의 호스트 시스템 데이터의 이동을 용이하게 하기위해 및/또는 저장 컨트롤러를 통해 가령, 저장 매체로의 이동을 용이하게 하기위해 커맨드 프로세싱(예컨 대, 호스트 I/O 번역 및 스케줄링)을 관리할 수 있다. 또한, FTL은 저장 매체 또는 저장 매체 디바이스의 리소스나 건강 상태를 모니터링할 수 있다. 예를 들어, FTL은 여유 공간의 양, 용량, 여유 블 록(free block), 불량(bad) 블록, 기입/프로그래밍 사이클 카운트, 디바이스/블록 마모 패턴, 전력 소비, 온도, 등에 대하여 저장 매체 또는 캐시 DRAM을 모니터링할 수 있다. 일부 경우에, FTL은 저장 매체의 건강 및 리소스를 관리하기 위한 내부 FTL 작업(내부 작업 328)을 포함한다. FTL 또는 저장 컨트롤러 의 이러한 내부 작업들은 데이터 마이그레이션, 가비지 콜렉션, 웨어 레벨링 등과 같은 저장 매체에 액세스하는 작업들 또는 동작들을 포함할 수 있다. 내부 작업들을 구현하기 위해, FTL은 저장 매체 액세스를 위한 내부 I/O를 생성할 수 있고, 이는 이어서 매체 커맨드 관리자를 통한 저장 매체 액세스를 위한 대응하는 매체 I/O로서 수행될 수 있다. 본 실시예에서, 저장 컨트롤러의 펌웨어는 또한 저장 매체의 디바이스-레벨 양상들을 관리하기 위한 디바이스-레벨 관리 컴포넌트를 포함할 수 있다. 예를 들어, 디바이스-레벨 관리 컴포넌트는 NAND 플래시 디바이스들, NAND 채널들, 메모리 칩들, 메모리 다이들, 물리적 메모리 블록들 등의 각각의 것들의 파라미터들을 모니터링 또는 관리할 수 있다. 일부 경우에, 디바이스-레벨 관리 컴포넌트는 저장 매체의 온도 조건(예를 들어, 다이 또는 디바이스 온도)을 모니터링하고, 미리 정의되거나 적응적인 온도 임계값에 기초하여 열 제어(예를 들어, 스로틀링)를 구현한다. 예를 들어, 디바이스-레벨 관리 컴포넌트는 특정 NAND 디바이스 또는 NAND 채널의 온도가 온도 임계값을 초과한 것에 응답하여, 상기 특정 NAND 디바이스 또는 NAND 채널에 대한 액세스를 지연시키거나 제한할 수 있다. 대안적으로 또는 부가적으로, 디바이스 레벨 관 리 컴포넌트는 유사한 형태의 전력 관리(예를 들어, 소비 기반) 또는 다른 디바이스 레벨 모니터링 및 저 장 매체의 제어를 구현할 수 있다. 저장 매체 액세스의 AI 기반 관리의 양상들에서, 매체 액세스 관리자는 AI 엔진과 상호작용하여, 저 장 매체에 액세스하기 위한 다양한 I/O들의 프로세싱 및 스케줄링을 최적화할 수 있다. 일부 예에서, 매체 액세스 관리자는 호스트 I/O를 입력으로서 AI 엔진의 AI 모델에 제공 또는 전달한다. 입력은 가령, 호스트 I/O의 이벤트 유형, 호스트 I/O의 이벤트 기간, 호스트 I/O와 관련된 데이터의 이벤 트 크기 등과 같은, 호스트 I/O를 설명하는 임의의 적합한 유형의 데이터 또는 정보(호스트 시스템 활동) 를 포함할 수 있다. 입력에 기초하여, AI 모델은 호스트 시스템 행동의 예측을 출력으로서 생성 하거나 도출한다. 출력들 또는 호스트에 대한 예측은, 호스트 시스템이 유휴 상태가 될 때까지의 시간 기 간의 표시, 호스트 시스템이 유휴 상태를 유지하는 시간 기간, 호스트 시스템에 의해 발행된 다음 호스트 I/O에 관한 파라미터들 등과 같은 임의의 적합한 유형의 데이터 또는 정보를 포함할 수 있다. 출력들은 매체 액 세스 관리자에 의해 직접 수신되거나 또는 저장 컨트롤러의 I/O 스케줄러 또는 FTL과 같은다른 엔티티를 통해 수신될 수 있다. 예측된 호스트 이벤트(예컨대, 기입 밀도 또는 유휴 시간들) 또는 예측된 호스트 행동 정보와 같은 호스트 행동의 예측을 사용하여, 매체 액세스 관리자는 호스트 I/O 및 내부 I/O가 저장 매체 액세스를 위한 매체 I/O로서 프로세싱, 스케줄링 또는 수행되는 방식을 변경 또는 수정할 수 있다. 예를 들어, 내부 작업들과 관련하여, 매체 액세스 관리자는 호스트 I/O를 설명하는 정보를 AI 모델 에 제공할 수 있으며 그리고 AI 모델로부터 저장 매체의 후속 액세스와 관련된 호스트 시스템 행동의 예측을 수신할 수 있다. 다음으로, 매체 액세스 관리자 및 I/O 스케줄러는 저장 매체의 건강 상태 및 리소스에 대한 표시 뿐만 아니라 호스트 시스템 행동의 예측을 사용하여, 호스트 I/O(예컨대, 현 재 및 후속적으로 수신된 호스트 I/O)와 내부 작업들에 대한 내부 I/O를 프로세싱 및 스케줄링함으로써, 저장 매체에 대한 액세스를 최적화할 수 있다. 이렇게 함으로써, 동일한 저장 매체 리소스에 액세스하기 위해 경쟁하는 호스트 I/O와 내부 I/O 사이의 충돌을 피하거나 감소시킴으로써 I/O 스케줄링이 개선될 수 있다. 또 다른 일례로서, 저장 매체 액세스의 열-기반 스로틀링(thermal-based throttling)과 같은, 디바이스-레벨 관 리 동작들의 관점에서 저장 매체 액세스의 AI 기반 관리를 고려하자. 열 스로틀링의 정도 또는 양을 결정하기 위해 현재의 I/O 워크로드 또는 메모리 디바이스 상태를 사용하는 것 외에도, 매체 액세스 관리자는 AI 엔 진의 AI 모델에 의해 제공되는 호스트 시스템 행동의 예측을 사용할 수 있다. 일부 경우들에서, 매체 액세스 관리자는 비활동 동안 메모리 디바이스의 온도가 냉각될 시간인 예측된 다가오는 유휴 시간에 기초 하여 메모리 디바이스의 온도 임계값을 변경 또는 선점(예를 들어, 일시적으로 중단)할 수 있다. 대안적으로 또 는 추가적으로, 매체 액세스 관리자는 호스트 시스템 행동의 예측에 기초하여 저장 매체 액세스가 스로틀 링되는 정도를 감소시킬 수 있다. 따라서, 이러한 호스트 시스템 동작 예측을 사용할 수 있는 능력이 있으면 저 장 매체 액세스의 스로틀링을 감소시킬 수 있으므로, 호스트 I/O 성능이 향상될 수 있다. 이러한 것들은 저장 매체 액세스의 AI 기반 관리의 몇 가지 일례일 뿐이며, 다른 일례들이 본 명세서 전체에 걸쳐 가능하고 설명되 어 있다. 도 4는 다수의 AI 모델들을 구현하기 위한 AI 엔진 및 영구 메모리의 구성예를 도시한다. 몇몇 양상들에서, AI 엔진은 저장 매체 컨트롤러의 서로 다른 각각의 동작들 또는 작업들을 보조하거나 최적화 시키도록 구성된 또는 트레이닝된 다수의 AI 모델들을 포함하거나 이에 액세스할 수 있다. AI 모델들(13 4)은 저장 매체 또는 저장 컨트롤러의 내부 메모리(미도시)와 같은 저장 시스템의 영구 저장 매체에 저장될 수 있다. 본 일례에서, AI 모델들은 도 2의 SSD에 의해 구현되는 것과 같은 NAND 플래시 디바 이스에 저장된다. 여기서, AI 모델은 다수의 AI 모델들을 포함하며, 이는 AI 모델 A(134-1) 내지 AI 모델 N(134-n)으로 도시되며, 여기서 n은 임의의 적절한 정수이다. AI 모델들 각각은 데이터 마이그레이션, 가비지 컬렉션, 웨어 레벨링, 열 제어 등과 같은 저장 컨트롤러의 각각의 내부 작업 또는 디바이스-레벨 동작을 위해 구성되거나 트레이닝될 수 있다. 대안적으로 또는 추가적으로, AI 모델은 저장 컨트롤러의 FTL의 다수의 내부 작업을 지원하도록 구성되거나 훈련될 수 있다. 402로 도시된 바와 같이, 매체 액세스 관리자 또는 AI 엔진은 저장 시스템의 내부 작업들 을 지원하거나 및/또는 최적화시키기 위해 여러 AI 모델을 로드할 수 있다. 일부 경우에서, 저장 시스템 의 부팅 또는 전원 공급에 응답하여, 다수의 AI 모델들이 로딩된다. 다른 경우에, 하나 이상의 AI 모 델이 주문형(on-demand)으로 또는 AI 엔진에 의해 요청된대로 로딩될 수 있다. 일반적으로, AI 모델 은 저장 시스템 또는 저장 컨트롤러의 대응하는 내부 작업을 지원하거나 최적화시키기 위 해 로딩될 수 있다. 본 실시예에서, 매체 액세스 관리자 또는 FTL이, 제 1 내부 작업 A(328-1)(예컨 대, 데이터 마이그레션) 및 제 2 내부 작업 B(328-2)(예컨대, 가비지 컬렉션)을 실행하고 있다라고 가정하자. 여기서, AI 엔진은 저장 시스템이 부팅될 때 매체 인터페이스를 통해 AI 모델 A(134-1) 및 AI 모델 B(134-2) 둘다를 로딩한다라고 가정하자. 본 일례의 맥락에서, AI 모델 A(134-1) 및 AI 모델 B(134-2)는 저장 컨트롤러의 내부 작업 A 및/또는 내부 작업 B를 보조하기 위해 순차적으로 또는 동시에 실행될 수 있다. 도 4에 도시된 바와 같이, AI 엔진 드라이버 는 제 1 입력 세트(332-1)를 AI 모델 A(134-1)에 제공하고, 제 2 입력 세트(332-2)를 AI 모델 B(134-2)에 제공 할 수 있다. 제 1 입력 세트(332-1)는 각각의 내부 작업에 대해 다르게 구성되는 것과 같이 제 2 입력 세 트(332-2)와 동일하거나 유사하거나 상이할 수 있다. 내부 작업을 지원하기 위해 AI 모델 A는 I/O 스케줄러 또는 매체 액세스 관리자에게 제 1 출력 세트(334-1)를 제공하고, AI 모델 B(134-2)는 제 2 출력 세트(334-2)를 I/O 스케줄러 또는 매체 액세스 관리자에게 제공한다. 제 1 출력 세트(334-1)는 각각 의 내부 작업에 대해 다르게 구성되는 것과 같이 제 2 출력 세트(334-2)와 동일하거나 유사하거나 상이할수 있다. 도 5는 저장 매체 액세스의 AI 기반 관리의 양상들이 구현될 수 있는 캐시 메모리 및 저장 메모리의 구성예 를 도시한다. 비록 본 일례에서 캐시 메모리는 싱글 레벨 셀(SLC) 플래시 캐시로 도시되어 있지만, 멀티 레벨 셀(MLC) 플래시 스토리지에 기입하기 전에 데이터를 캐시하는데 다른 플래시 또는 메모리 유형 이 사용될 수 있다. 상기 MLC 스토리지는 또한 트리플 레벨 셀(TLC) 플래시, 쿼드 레벨 셀 플래시(QLC), XLC 플래시, NOR 플래시 등과 같은 다른 플래시 또는 메모리 타입으로 구현될 수 있다. 일반적으로, 저장 시스 템은 개선된 버스트 기입 성능을 위해 또는 MLC 저장 매체에서의 마모를 감소시키기 위해 SLC 캐시 로 구현될 수 있다. 그러나, SLC 플래시 또는 다른 유형의 캐시 메모리는 SLC 캐시가 MLC 플래시 메 모리의 주 저장 영역보다 적은 용량으로 구현되도록 저장 매체보다 비쌀 수 있다. 도 5에 도시된 바와 같이, 호스트 시스템의 호스트 I/O는 데이터가 SLC 캐시의 빈 공간에 기입되게 한다(시각적 간결함을 위해 호스트 I/O 대 매체 I/O 변환은 여기서는 생략됨). 호스트 I/O의 데 이터는 SLC 캐시의 인접하지 않은(non-contiguous) 영역들(예를 들어, 부분적 페이지들 또는 블록들)을 유 효 데이터 또는 통합 데이터로서 채우거나 점유할 수 있다. 일부 경우에, 유효 데이터가 SLC에 캐시 내에서 조직화되고 이동되어 통합 데이터를 형성한다. SLC 캐시가 데이터로 채워지면, 저 장 컨트롤러는 유효 데이터 또는 통합 데이터를 512로 도시된 바와 같이, MLC 스토리지로 이주시킨다(migrate). 대안적으로 또는 추가적으로, 호스트 I/O는 또한 SLC 캐시를 경유함이 없이, MLC 스토리지에 데이터가 직접 기입되게할 수 있다. MLC 스토리지가 데이터로 채워지면, 저장 컨트롤러는 부분적으로 유효한 MLC 블록들에 대한 가비지 컬렉션(garbage collection)을 구현하여, 514로 도시된 바와 같이 재사용을 위해 이들 MLC 블록들 비운다(예를 들어, 빈 MLC 블록으로서 데이터 기입이 가능해짐). 이와 같이, 저장 매체에 데이터를 기입하기 위한 호스트 I/O는 저장 컨트롤러 또는 FTL에 의해 수행되는 데이터 마이그레이션 및 가비지 컬렉션 에 대응하는 내부 I/O를 유발할 수 있다. 일부 경우, 데이터 마이그레이션 및 가비지 컬렉션을 위한 내부 I/O들은, 호스트 시스템으로부터 수신된 은 호스트 I/O와 동시에 발생할 수도 있다. 저장 매체에 액세스하기 위해(예컨대, 매체 I/O에 대응) 내부 I/O들이 경쟁하는 것이 허용되는 경우, 호스트 I/O 성능, 따라서 전체 호스트 시스템 성능은, 심각하게 손상되거나 저하될 수 있다. 저장 매체 액세스의 AI 기반 관리의 양상들에서, 매체 액세스 관리자는 호스트 I/O 및 내부 I/O를 스 케줄링함에 있어서 AI 엔진을 사용하여, 호스트 I/O 성능을 최적화할 수 있으며 그리고 저장 매체에 액세스하기 위한 호스트 I/O와 내부 I/O 사이의 경쟁을 감소시키거나 제거할 수 있다. 일반적으로, 매체 액세스 관리자는 호스트 관련 이벤트들 및 행동의 예측들을 이용하여, 호스트 I/O와 내부 I/O의 적응형 스케줄링 또는 최적화된 스케줄링을 구현할 수 있다. 즉, 미래의 호스트 I/O 이벤트나 활동들이 예측된다면, 과거의 호스 트 시스템 활동에 기초하여, 가능한한 많은 동시 호스트 I/O 및 내부 I/O를 회피 또는 감소시킬 수 있는 기회들 에 대하여 내부 FTL 동작들의 스케줄링이 최적화될 수 있다. 다양한 양상들에서, 저장 컨트롤러의 매체 액세스 관리자 및/또는 AI 엔진은 저장 시스템에 대한 예측형 또는 적응형 내부 작업들 또는 디바이스-레벨 관리를 구현할 수 있다. 몇몇 양상들에서, 매체 액세스 관리자는 판독 또는 기입 커맨드들(예를 들면, 논리 블록 어드레스(LBA), 크기, 타임 스탬프) 또는 유휴 시간(예를 들면, 간격, 듀티 사이클, 주기, 빈도, 지속시간, 등)에 대한 정보 또 는 설명 데이터와 같은, 호스트 시스템의 현재 또는 과거의 활동에 관한 정보를 AI 엔진에 제공한다. 현재 또는 과거의 호스트 활동에 기초하여, AI 엔진은 AI 모델을 사용하여, 저장 매체 액세스 또는 유휴 시간과 같은 후속 또는 미래의 호스트 활동에 대한 예측을 제공할 수 있다. 저장 매체 액세스에 대한 예측은, 액세스 유형, 호스트 시스템이 기입 또는 판독할 데이터의 양, 기입 또는 판독이 완료되는데 걸리는 시간, 또는 미래에 어떤 데이터가 무효화되거나 덮어쓰기될 수 있는지, 기타 등등을 포함할 수 있다. 미래 호스트 시스템 행동의 예측에 기초하여, 매체 액세스 관리자는 저장 매체에 액세스하기 위한 경쟁을 회피 또는 감소시키 도록, 다양한 I/O들을 속행(advancing) 또는 지연(delaying)시킴으로써, 호스트 I/O 또는 내부 I/O의 스케쥴링 을 변경할 수 있다. 이렇게함으로써, 내부 I/O의 실행을 효율적으로 마스킹 또는 은폐시키고 그리고 호스트 시 스템(예를 들면, 데스크탑 또는 랩탑 컴퓨팅 디바이스)으로부터 인지된 호스트 시스템의 유휴 시간의 장점을 취 함으로써, 호스트 I/O 성능이 개선될 수 있다. 도 6은 저장 시스템 컨트롤러의 매체 액세스 관리자 및 AI 엔진에 의해 구현된 예측형 가비지 콜렉션 의 일례를 도시한다. 600에서 볼 수 있는 바와 같이, 종래의 가비지 컬렉션에서 자주 발생하는 스로틀링(throttling) 또는 스톨링(stalling)으로 인해 호스트 성능이 저하된다. AI-기반의 관리가 없다면, 호스트(60 2)의 기입 버스트가 저장 매체의 여유 공간의 레벨을 초과할 때마다, 가비지 컬렉션이 실행되는 것이 일반적이다. 기입 버스트와 관련된 인입 데이터는 여유 공간의 레벨이 가비지 콜렉션 임계값 아래로 떨어지게 하며, 이는 가비지 콜렉션을 트리거링하여 호스트 I/O의 성능을 감소시키는데, 왜냐하면 가비지 콜렉션을 위한 내부 I/O가 저장 매체 액세스를 위해 기입 버스트의 호스트 I/O와 경쟁하기 때문이다. 종래 기술과는 대조적으로, AI 엔진 및 모델에 의해 구현된 예측형 가비지 콜렉션의 일례가 610으로 도시된다. 일부 양상들에서, 저장 매체의 볼륨은 예측형 가비지 콜렉션을 지원하는 각각의 임계값들과 여유 공 간을 구비하도록 구성되는바, 이는 호스트 I/O 성능의 감소를 완화시키거나 회피할 수 있는(예를 들어, 호스트 I/O 스로틀링을 방지함) 적응형 가비지 콜렉션 동작을 가능하게 한다. AI 엔진을 사용하여, 매체 액세스 관리자는 호스트 시스템의 예측된 기입 버스트와 관련된 정보를 수신할 수 있다. 매체 액세스 관리자 는 예측된 기입 버스트가 여유 공간의 레벨을 초과한다고 판단할 수 있으며 그리고 예측된 기입 버스트가 발생하기 전에, 예측형 가비지 콜렉션 내부 I/O를 구현할 수 있다. 그렇게함으로써, 예측형 가비지 콜렉션은 새로운 레벨의 여유 공간을 제공하며, 이러한 새로운 레벨의 여유 공간은 기입 버스트의 호스트 I/O와 간섭할 수 있는 정규 가비지 컬렉션 내부 I/O를 트리거링함이 없이, 예측된 기입 버스트 또는 다른 기입 버스트의 데이터를 수신(또는 흡수)할 수 있다. 도 7은 하나 이상의 양상들에 따라 AI 엔진에 의해 스케줄링된 내부 I/O 동작들의 일례를 도시한다. 본 일례에 서, 매체 액세스 관리자는 가비지 컬렉션 및/또는 데이터 마이그레이션을 지연시켜 호스트 I/O 성능을 향 상시킬 수 있다. 700으로 도시된 바와 같이, AI 기반의 관리가 없는 경우, 호스트 기입들은 각각에 대한 각각의 임계값들에 도달했기 때문에(예를 들어, 사용 가능한 공간이 감소함에 따라), 데이터 마이그레이션 및 가비지 컬렉션을 트리거링할 수 있다. 저장 매체의 블록들을 확보하기 위해 데이터 마이그레이션 및 가 비지 콜렉션이 수행될 때, 내부 I/O는 저장 매체 인터페이스에 액세스하기 위해 호스트 I/O와 경쟁하며, 708 로 도시된 바와 같이, 호스트 액세스를 저하시킨다. 종래 기술과 달리, AI 엔진 및 AI 모델에 구현되는 지연된 가비지 컬렉션 및/또는 데이터 마이그레이 션의 일례가 710에 도시된다. 일부 양상들에서, 매체 액세스 관리자는 호스트 시스템의 호스트 I/O를 모니 터링하며, 이는 저장 매체로의 호스트 기입을 위한 호스트 I/O를 포함한다. 호스트 기입에 기초하여, 매체 액세스 관리자는 이벤트 유형 또는 지속 기간의 표시를 AI 엔진 드라이버의 AI 모델 에 제공할 수 있다. 호스트 활동의 표시에 기초하여, AI 모델은 다음의 호스트 시스템 행동의 예측을 제공한다. 호스트 시스템 행동의 예측은, 다음 유휴까지의 시간, 다음 유휴의 지속기간, 호스트 시스템의 다음 유휴까지의 호스트 기입들의 밀도, 기타 등등을 설명하는 정보를 포함할 수 있다. 다가오는 유휴 시간 의 예측에 기초하여, 매체 액세스 관리자는 720에 도시된 바와 같이, 저장 컨트롤러의 내부 작업을 지연시킬 수 있다. 종래의 저장 컨트롤러의 내부 작업들이 호스트 I/O 성능을 저하시키는 경우에 비하여, FTL의 내부 I/O와 경쟁함이 없이 722에서 전체 호스트 성능이 가능해진다. 호스트 시스템의 유휴 시간의 예측을 이용하여, 매체 액세스 관리자는 호스트 시스템의 호스트 I/O와 충돌함이 없이, 지연된 데이터 마 이그레이션 및 지연된 가비지 컬렉션을 구현할 수 있다. 대안적으로 또는 부가적으로는, 매체 액세스 관리자는 하나 이상의 양상들에 따라, 적응형 캐시 관리 및 동적 우회를 구현할 수 있다. 적응형 가비지 컬렉션 및 데이터 마이그레이션과 유사하게, 매체 액세스 관리자 는 \"기입 밀도\"에 대한 호스트 시스템 행동의 예측을 사용하여, 저장 매체(예컨대, xLC 플래시 스토리지) 로의 데이터 마이그레이션의 선점(예를 들어, 일찍) 또는 캐시의 동적 우회(dynamic bypass)를 통해 캐시(예를 들어, SLC 플래시 캐시)의 사용을 관리할 수 있다. 예를 들어, 매우 큰 기입 버스트에 대한 예측에 기초하여, 매체 액세스 관리자는 상기 기입 버스트를 흡수할 수 있을 만큼 충분한 공간을 확보하기 위하여 캐쉬 메모 리로부터 저장 매체로의 조기(early) 데이터 마이그레이션을 수행할 수 있다. 다른 경우에, 매체 액세스 관리자 는 현재 유휴 시간은 데이터 마이그레이션에 불충분하다고 결정할 수 있으며, 대신에 큰 기입 버스트가 캐 시 메모리를 우회하여, 저장 매체에 직접 기입되게 할 수 있다. 이러한 우회 데이터 기입은 충분한 사이즈의 캐 시 메모리를 사용하는 것보다 느릴 수 있지만, 만일, 캐시 메모리가 전체 기입 버스트를 흡수할 수 없는 경우, 동시 데이터 마이그레이션이 트리거링될 때 호스트 I/O 성능을 크게 저하(예컨대, 우회 데이터 기입보다 훨씬 느리다)시킬 수 있는 캐시 메모리로부터의 데이터 마이그레이션을 방지할 수 있다. 본 개시 전반에 걸쳐 설명되는 다양한 양상들은, 저장 시스템과 연관된 AI 엔진, AI 모델 또는 AI 드라이버와 상호작용하는 매체 액세스 관리자 또는 FTL에 의해 구현될 수 있다. 저장 시스템의 다양한 정보를 처 리하는 것과 관련하여, AI 엔진 및/또는 AI 모델은 호스트 시스템 활동 또는 행동 예측을 위한 하나이상의 뉴럴 네트워크에 기초한 머신-러닝으로 구현될 수 있다. 각각의 AI 모델, AI 알고리즘 또는 AI 엔진 의 뉴럴 네트워크는 하나 이상의 계층들로 조직화된 뉴런들 또는 퍼셉트론들과 같은, 연결된 노드들의 그 룹을 포함할 수 있다. 예를 들어, AI 엔진의 AI 모델(예컨대, 머신-러닝 모델)은 딥 뉴럴 네트워크(deep neural network)로 구 현될 수 있으며, 딥 뉴럴 네트워크는 입력 계층, 출력 계층, 및 입력 계층과 출력 계층 사이에 위치된 하나 이 상의 숨겨진 중간 계층을 포함한다. 다음으로, 딥 뉴럴 네트워크의 각 노드는 신경망의 계층들 사이에서 완전히 연결되거나 부분적으로 연결될 수 있다. AI 모델 또는 AI 알고리즘은 AlexNet, ResNet, GoogleNet, MobileNet 등 중 하나를 포함하는 콘볼루션 뉴럴 네트워크(Convolutional Neural Network: CNN)과 같은 임의의 DNN(deep neural network)일 수 있다. 대안적으로 또는 추가적으로, AI 모델은 임의의 적절한 리커런트 뉴럴 네트워크 (recurrent neural network: RNN) 또는 이의 임의의 변형을 포함할 수 있다. 일반적으로, AI 엔진에 의해 사용되는 AI 모델 또는 AI 알고리즘은 또한 임의의 다른 지도 학습(supervised learning), 비지도 학습 (unsupervised learning), 강화 학습 알고리즘(reinforcement learning algorithm) 등을 포함할 수 있다. 다양한 양상들에서, AI 엔진의 AI 모델은 입력 데이터 시퀀스(예컨대, 호스트 I/O 또는 이벤트 설 명)의 후속 부분에 대해 입력 데이터 시퀀스의 이전 부분으로부터의 정보를 유지하는 사이클을 형성하는 노드들 사이가 연결된 리커런트 뉴럴 네트워크로서 구현될 수 있다. 대안적으로, AI 모델은 입력 데이터 시퀀스들 사이 의 사이클을 형성하지 않는 노드들 사이의 연결들을 갖는 피드-포워드 뉴럴 네트워크로서 구현될 수 있다. 또 다른 경우에, AI 엔진의 AI 모델은 주어진 층의 각각의 뉴런이 인접한 층의 모든 뉴런과 연결된 다중 층 퍼셉트론(multilayer perceptron)을 갖는 컨볼루션 뉴럴 네트워크(convolutional neural network)(CNN)를 포함할 수 있다. 일부 양상들에서, 컨벌루션 뉴럴 네트워크에 기초한 AI 모델은, 소정 형태의 후속 또는 미래 호스트 시스템 행동 또는 활동을 예측 또는 예상하기 위해 이전 호스트 시스템 활동에 적용될 수 있다. 대 안적으로 또는 추가적으로, AI 엔진은 다중 선형 회귀 모델, 단일 선형 회귀 모델, 로지스틱 회귀 모델, 단계적 회귀 모델, 다변량 적응형 회귀 모델, 국소 추정 산포도 모델(locally estimated scatterplot models) 등과 같은 다양한 회귀 모델을 포함하거나 이용할 수 있다. 도 8은 하나 이상의 양상들에 따라 호스트 시스템 행동을 예측함에 있어서 AI 모델에 유용한 호스트 I/O 이벤트 유형들의 일례를 도시한다. 본 일례에서, 플래시 변환 계층은 저장 매체에 액세스하기 위해 대 응하는 매체 I/O를 생성하도록 호스트 I/O를 프로세싱하는 것으로 도시된다. 이들 호스트 I/O들 은 각각의 저장 매체 액세스 작업들에 대한 호스트 I/O 이벤트들(802-0 내지 802-t)을 포함하거나 이에 대응할 수 있다. 호스트 I/O에 기초하여, 매체 액세스 관리자는 AI 엔진의 하나 이상의 AI 모델에 입력을 제공한다. 도 8에 도시된 바와 같이, 입력은 이벤트 유형, 이벤트 크기, 이벤트 기간, 이벤트 타임 스탬프 등을 포함하는 이벤트 서술자(descriptor) 또는 설명들을 포함할 수 있다. 입력(예를 들어, 호스트 I/O 이벤트)에 기초하여, AI 모델은 예측된 호스트 행동에 대한 출력을 매체 액세스 관리자 에 생성하거나 제공할 수 있다. 도 8에 도시된 바와 같이, 예측된 호스트 행동에 대한 출력은 다음 유휴까지의 시간, 다음 유휴의 지속 기간, (후속 호스트 시스템 액세스의) 기입 밀도 등을 포함할 수 있다. 몇몇 양상에서, 매체 액세스 관리자 또는 FTL는 이들 I/O 이벤트들의 설명 혹은 서술자를 AI 모델 로 포워딩하여, 미래 호스트 행동이나 활동을 예측하도록 구성된 뉴럴 네트워크로 프로세싱하게 할 수 있 다. 본 일례의 맥락에서, 호스트 I/O 이벤트는 수학식 1에 도시된 바와 같이 서술되거나 분류될 수 있으며, 여 기서 xt 는 이벤트 유형이고 dt 는 이벤트 기간이다."}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1: 호스트 I/O 이벤트 서술자 구조 일반적으로, 이벤트 유형 또는 I/O 커맨드는 액세스의 유형(예컨대, 기입/판독) 및 액세스의 사이즈(예컨대, 4KB, 8KB 또는 16KB)를 참조하여 서술될 수 있다. 이벤트 기간(dt)은 이벤트 또는 I/O 커맨드의 도착으로부터 다음 또는 후속 I/O 커맨드의 도착까지의 시간으로 정의될 수 있다. 유휴 이벤트 및 유휴 시간과 관련하여, 이 벤트와 연관된 지속 시간이 미리정의된 유휴 지속 시간 임계값(D)보다 긴 경우, 이러한 이벤트는 \"유휴(idle)\" 로 분류되고, 연관된 지속 시간은 \"유휴 시간\"으로 분류될 수 있다. 804에 도시된 바와 같이, AI 모델에 대한 입력 시퀀스는 다수의 이벤트 서술자들로부터 형성될 수 있으며, 임의의 단계(t)에서 호스트 I/O 이벤트들의 히스토리는 수학식 2와 같이 정의될 수 있다."}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 2: 호스트 I/O 이벤트들의 히스토리 호스트 I/O 이벤트들의 입력 시퀀스와 함께, 트레이닝되는 AI 모델에는 유휴 기간 임계값이 제공될 수 있 으며, 이를 이용하여 다음 유휴까지의 시간(ut), 다음 유휴의 기간(vt), 및 기입 밀도(st)를 예측할 수 있다. 일 례로서, 호스트 I/O 행동을 예측하도록 구성된 AI 모델의 예시적인 구현예를 예시하는 도 9를 고려하자. 다양한 양상들에서, AI 모델은 호스트 시스템의 I/O 활동과 같은 입력들의 이력을 메모리로 처리하는데 유 리할 수 있는 리커런트 뉴럴 네트워크(RNN) 아키텍처에 기초할 수 있다. 도 9에 도시된 바와 같이, AI 모델은 매립(embedding) 계층, 리커런트 계층과 \"다음 유휴까지 의 시간\", \"다음 유휴 기간\" 및 \"다음 유휴까지의 기입 분량”에 대한 각각의 예측 계층들 및 분류 계층들(906- 910)을 포함한다. 본 일례에서, AI 모델의 입력 가중치는 지속 기간 입력 및 이벤트 입력을 포 함하며, 이는 여기에 설명된 바와 같이 매체 액세스 관리자에 의해 생성 및 포맷될 수 있다. 일부 경우에, 매립 가중치는 리커런트 계층 이전에 매립 계층에서 이벤트 입력에 적용된다. 리커런트 계 층은 또한 AI 모델의 인스턴스를 AI 모델의 이전 단계 또는 실행과 연결하기 위해 적용될 수 있 는 숨겨진 가중치를 포함할 수 있다. 도 9에 도시된 바와 같이, AI 모델은 \"다음 유휴까지의 시간\", \"다음 유휴 기간\" , \"다음 유휴까지 기입 분량\" 을 예측하기 위한 각각의 가중치(922-926)를 포함한다. 이와 유사하게, 예측 계층 또는 분류 계층(906 내 지 910)의 일부 또는 전부는 \"다음 유휴까지의 시간\", \"다음 유휴 기간\" , \"다음 유휴까지 기입 분량\"의 호스트 I/O 활동량들의 해당 예측들(928-932)을 제공할 수 있다. 일반적으로, 분류 손실 함수 뿐만 아니라 회귀 손실 함수(934 및 936)는 역 전파(back propagation)를 사용하여 모델 가중치들을 훈련시키기 위해 사용된다. 대안적으로 또는 추가적으로, AI 모델은 AI 모델이 이전의 숨겨진 상태의 이력에 기초하여 어떤 가중치 또 는 계층을 주목할지를 학습할 수 있게 하는 주목 계층(attention layer)을 포함할 수 있다. 예측된 호스트 행동의 다른 예로서 도 10을 고려하면, 도 10은 다양한 I/O들 또는 유휴 시간을 포함하는 예측된 호스트 행동의 일례들을 도시한다. 일부 양상들에서, 멀티-스테이지 접근법이 AI 모델에 의해 구현되어, 다음 유휴까지의 시간, 다음 유휴 기간, 및/또는 호스트 I/O 활동의 기입 밀도를 예측할 수 있다. 1000으로 도 시된 바와 같이, AI 모델은 w 이벤트들의 이력을 입력 시퀀스로서 취하여 중간 결과로서 작용하는 다음 이 벤트(Et+1)를 예측한다. 여기서, AI 모델이 95 % 확률로 4KB 판독 동작을 예측하고(다음 이벤트 1002 로 도시됨) 그리고 5 % 확률로 8KB 기입 동작을 예측한다라고 가정하자. 이러한 입력 시퀀스에 기초하여, AI 모 델은 또한 다음 이벤트(예를 들어, 4KB 판독 동작에 대해)에 대한 지속 시간(dt+1)을 예측할 수 있다. 이러한 중간 결과인 이벤트는, 다음 이벤트(Et+2)를 예측하기 위하여, 상기 입력 시퀀스의 다음 이 벤트로서 1004에서 다시 AI 모델에 제공될 수 있다. 여기서, 다음 이벤트는 확률을 통해 지속 시간 (dt+2)의 8KB 기입이 될 것으로 예측될 수 있다라고 가정하자. AI 모델의 이러한 재귀적 접근법은 유휴 시 간 이벤트가 예측될 때까지 수행 및/또는 반복될 수 있다. 예를 들어, 재귀적 접근법은 1008에 도시된 바와 같 이 예측된 지속 시간(dt+T)이 미리정의된 유휴 지속 시간 임계값 D 보다 긴, 단계(t + T)까지 반복되고, 이 시점에서 유휴 이벤트가 예측되며, 또는 단계(t + T)가 기정의된 룩어헤드(lookahead) 단계 S에 도달할 때까 지 반복된다. 재귀적 접근법이 유휴 이벤트의 예측을 초래하는 경우, 후속 예측 스테이지는 아래의 수학식 3- 5에 도시된 바와 같이 호스트 시스템 활동에 대한 다른 I/O 관련 분량들을 예측하는데 사용될 수 있다."}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "[3] 예측된 \"다음 유휴 이벤트까지의 시간\" = dt+1 + ... + dt+T-1"}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "[4] 예측된 \"다음 유휴 기간\" = dt+T"}
{"patent_id": "10-2019-0136475", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "[5] 예측된 \"기입 밀도\" = 여기서 j = t + 1,… t + T이고, 는 기입 이벤트이다. 수학식 3-5: 예측된 유휴 이벤트 관련 호스트 시스템 동작에 대한 분량들 일례로서, 호스트 I/O 행동의 멀티-스테이지 예측을 수행하도록 구성된 AI 모델의 예시적인 구현예를 도 시한 도 11을 고려하자. 도 11에 도시된 바와 같이, AI 모델은 매립 계층, 리커런트 계층, 예측 계층 및 분류 계층을 포함한다. 본 일례에서, AI 모델의 입력 가중치는 지속 시 간 입력 및 이벤트 입력을 포함하며, 이들은 본 명세서에 기술된 바와 같이 매체 액세스 관리자 에 의해 생성 및 포맷될 수 있다. 일부 경우에, 매립 가중치는 리커런트 계층 이전에 매립 계 층에서 이벤트 입력에 적용된다. 리커런트 계층은 또한 숨겨진 가중치를 포함할 수 있 는바, 숨겨진 가중치는 AI 모델의 인스턴스를 AI 모델의 이전 단계 또는 실행과 연결하기 위 해 적용될 수 있다. 도 11에 도시된 바와 같이, AI 모델는 또한 다양한 이벤트 지속 시간을 예측하는 \"이벤트 지속 시간\" 가 중치 및 이벤트들을 분류하는 \"이벤트 유형\" 가중치를 포함하며, 이는 예측 계층에 제공될 수 있다. 예측 계층 및 분류 계층은 다음 이벤트 또는 중간 이벤트에 대한 각각의 이벤트 예측들 및 이벤트 분류들을 제공할 수 있다. 일반적으로, 회귀 손실 함수 및 분류 손실 함수(113 0)는 역 전파를 사용하여 모델 가중치를 트레이닝하는데 이용된다. 대안적으로 또는 부가적으로, AI 모델(110 0)은 AI 모델이 이전의 숨겨진 상태의 이력에 기초하여 어떤 가중치 또는 계층을 주목할지를 학습할 수 있게 하 는 주목 계층(attention layer)을 포함할 수 있다. 도 12는 이벤트 확률에 기초하여 예측된 호스트 행동의 경로를 결정하는데 유용한 예시적인 빔 검색(beam search)을 나타낸다. 일부 양상들에서, 빔 검색은 호스트 시스템 행동의 예측들을 통한 에러 전파를 감소 시키거나 완화시키기 위해 사용될 수 있다. 일반적으로, 각각의 시간 단계에서, 매체 액세스 관리자 또는 AI 엔진에 의해 구현된 빔 검색은 K 개의 이벤트를 가장 높은 조인트 또는 조합 확률로 유지할 수 있다. 그렇게함으로써, 빔 검색은 AI 엔진이 시간 단계 t + 1 에서 시간 단계 t + T 까지 최상의 이벤트 경로를 선택할 확률을 증가시킬 수 있다. 도 12에 도시된 바와 같이, K = 3 개의 이벤트인 경우, 빔 검색은 각각의 다 음 이벤트에 할당된 확률들에 기초하여, 이벤트(Et+1)로부터 이벤트(Et+2)까지의 경로들을 따라 진행 할 수 있다. 유휴 이벤트(Et+T)까지의 빔 검색으로부터 선택된 경로에 기초하여, AI 엔진은 본 명세 서에 설명된 바와 같이 AI 모델을 사용하여 \"다음 유휴 이벤트까지의 시간\", \"다음 유휴 지속 시간\"또는 \"기입 밀도\"를 예측할 수 있다. 도 13은 사용자-특정 또는 디바이스-특정 I/O 워크로드에 AI 모델이 적응될 수 있게 하는, AI 모델의 온라인 재 -트레이닝 또는 온라인 개량을 위한 동작들의 예시적인 타임라인을 도시한다. 일반적으로, 호스트 시스템 I/O 이벤트 또는 행동을 예측하는데 사용되는 AI 모델은, 저장 시스템을 사용자에게 배포하기 전에 소정 유형의 디바이스 또는 어플리케이션에 포괄적인(generic) 대형의 데이터 세트를 사용하여 오프라인으로 트레이닝될 수 있다. 디바이스가 사용자에 의해 사용되면, 다양한 어플리케이션들 및 일상적인 사용 패턴은 사용자마다 다른 것이 일반적이다. 이와 같이, AI 모델의 유효성 또는 정확성은 최종 사용자의 실제 사용에 기초하는 온라인(예 를 들어, 런-타임) 개량(refinement) 또는 재-트레이닝으로 향상될 수 있다. AI 모델의 재트레이닝 또는 개량(refining)과 관련하여, 다음의 논의가 둘 중 하나에 적용될 수 있는바, 이들 동작들은 동일하거나 유사한 동작들로 구현될 수 있기 때문이다. 예를 들어, AI 모델이 강건하거나 또는 해당 사용자에게 의미있는 데이터에 대하여 충분한 시간 동안 이미 트레이닝된 경우, AI 모델이 개량(refine)될 수 있다. 다른 경우에, AI 모델이 디바이스의 특정 사용자에 대한 최상의 파라미터들에 대하여 정확하지 않는 경우, AI 모델이 재트레이닝될 수 있다. AI 모델의 개량은, 모델 내의 계층들의 개수 또는 DNN의 소정 계층에서 의 파라미터들을 수정하는 것과 같은, 모델 파라미터들의 부분적 업데이트를 포함할 수 있다. 재트레이닝과 관 련하여, DNN의 다양한 파라미터들, 가중치들 또는 계층들의 도입, 확장 또는 제거, 등에 의해서, 대부분의 또는 모든 AI 모델들이 조정될 수 있다. 다양한 양상들에서, 저장 컨트롤러의 펌웨어, AI 엔진, 또는 AI 엔진 드라이버는 하나 이상의 AI 모델을 재트레이닝하거나 개량할 수 있다. 몇몇 양상에서, AI 모델에 대한 추론 프로세스 및 온라인 트레이닝은 동시에 또는 병렬로 수행된다. 추론 프로 세스에 의해 제공되는 결과는 호스트 시스템 행동을 예측하는 것 및 온라인 트레이닝 목적 둘다를 위해 이용될 수 있다. 그렇게하기 위해, 저장 컨트롤러의 펌웨어 또는 AI 엔진은 동시 온라인 트레이닝 프로세스 동안정확하거나 최적의 모델 가중치 세트가 사용됨을 보장하도록 추론 프로세스 및 온라인 트레이닝의 단계들을 동 기화 또는 조정할 수 있다. 일반적으로 온라인 트레이닝(또는 개량)에는 포워드 스텝과 백워드 스텝을 포함하며, 이는 필요에 따라 수행되거나 반복될 수 있다. 포워드 스텝은 추론 단계와 유사하거나 동일할 수 있 다. 백워드 스텝은 역 전파를 통해 재트레이닝되는 AI 모델의 가중치들 또는 파라미터들을 업데이트한다. 일부 경우에, 역 전파는 하나 이상의 모델 가중치가 얼마나 또는 어느 정도까지 수정되는지를 계산하는 것을 포함한 다. 예를 들어, AI 엔진은 각각의 모델 가중치에 대한 가중치 수정자(weight modifier)(Δw)를 계산할 수 있으며, 이후 그 가중치 수정자를 해당 가중치로부터 가산/감산할 수 있다. 도 13에 도시된 바와 같이, AI 모델이 트레이닝 또는 개량될 때, 2개의 모델들 혹은 AI 모델의 2개의 인스턴스 들이 병렬로 동작할 수 있다. 본 일례에서, 추론 프로세스는 AI 모델의 온라인 트레이닝과 동시에 또는 병렬로 실행된다. 일반적으로, 하나의 AI 모델은 \"현재 모델\"로 지정될 수 있으며, 다른 하나의 AI 모델은 \"업데이트 모델\"로 지정될 수 있는바, 후자는 재트레이닝 또는 개량된 모델 가중치를 제공한다. AI 모델 파라미 터들의 각각의 버전들을 저장하는 다중 버퍼들의 사용을 통해, AI 모델의 인스턴스들이 동시에 또는 병렬로 구 현될 수 있다. 도 13을 참조하면, 이들 버퍼들은 버퍼 A 및 버퍼 B로 표시되어 있다. 도 13에 도시 된 바와 같이, 추론 프로세스는 연속적으로 실행될 수 있으며, 온라인 트레이닝 프로세스도 가중치 가 업데이트되는 경우를 제외하고는 연속적으로 실행될 수 있다. 일부 양상들에서, 추론 단계들 및 온라인 트레이닝 프로세스들을 실행하도록, 추론 프로세스 및 온라인 트레이닝은 동일한 하드웨어(미도시)에서 실행된다. 다시 말해서, 온라인 트레이닝의 추론 부분(또 는 포워드 부분) 및 추론 프로세스는, 도 1 내지 4에 도시된 하드웨어 및/또는 다른 시스템(예를 들어, 도 19) 또는 콘트롤러들(예를 들어, 도 20)의 동일한 인스턴스에서 전술한 바와 같이 실행될 수 있다. 예를 들 어, 도 13의 추론 프로세스 및 온라인 트레이닝 둘다를 수행하도록 추론 하드웨어의 단일 인스턴스 가 구현될 수 있다. 도 13에 도시된 바와 같이, 추론 프로세스는 버퍼 A의 가중치가 업데이트됨에 따라 버퍼 B의 가중치를 사용하여 계속 실행될 수 있다. 유사하게, 추론 프로세스는 버퍼 B 의 가중치가 업데이트됨에 따라 버퍼 A의 가중치를 사용하여 계속 실행될 수 있다. 본 실시예의 문맥에서, 버퍼 A에 현재의 AI 모델에 기초하여 온라인 트레이닝이 실행될 수 있 다. AI 엔진은 하나의 배치(batch) (예를 들어, 배치 1(1310-1))로서 현재 AI 모델 또는 버퍼 A의 모델 가중치들을 이용하여 하나 또는 다수의 인스턴스들(단계들)에 대해 추론을 실행한다. 배치 가 완료되면, AI 모델이 역 전파을 통해 업데이트되며, 이는 AI 엔진의 프로세서 코어 또는 저장 컨트롤러에 의해 구현될 수 있다. AI 모델 또는 버퍼 B의 모델 가중치들을 업데이트 하는 것은 시간을 소비하며, 이 시간 동안 추론 프로세스는 AI 모델의 다른 인스턴스를 통해 계속 실행될 수 있다. 배치 1(1310-1)의 가중치 수정자들(1314-1)이 버퍼 B에서 업데이트됨에 따라, 온라인 트레이닝 프로세스는 잠겨질 수 있으며(모델 파라미터 조정 일시 정지), 반면에 추론 프로세스는 버퍼 A에서 계속 실행된다. 이렇게 함으로써, AI 모델의 빠르고 효과적인 트레이닝을 가능하게 하면서도, 추론 프로세스 의 성능을 저하시킴이 없이, 온라인 트레이닝 프로세스가 구현될 수 있다. 역 전파가 완료되고, AI 모델 또는 버퍼 B의 모델 가중치가 업데이트된 후, 이 버퍼는 온라인 트레 이닝의 다음 배치에 대한 및 추론 프로세스에 대한, \"현재\" 버퍼가 된다. 달리 말하면, 배치 2(1310-2)는 AI 모델 또는 버퍼 B에 저장된 모델 가중치로 실행되고, 버퍼 A가 \"업데이트\" 버퍼가 된다. 도 13에 도시된 바와 같이, 교번하는 방식에서, 버퍼 A는 \"현재\" 모델 또는 모델 가중치를 보유하는 반면에, 버퍼 B가 \"업데이트\" 모델 또는 모델 가중치로 사용된다. 모델 업데이트가 완료되면, 버퍼 B가 \"현재\" 모델 또는 모델 가중치를 보유하는 반면에, 버퍼 A가 \"업데이트\" 모델 또는 모델 가중치로 사용된다는 점에서, 버퍼는 역할을 스위치한다. 다시 말해서, 각 온라인 배치(예를 들어, 배치 3, 배 치 4 등)를 갖는 버퍼들 사이에서 온라인 트레이닝이 \"핑-퐁(ping-pongs)\"하여, 지속적인 온라인 트레이닝 프로 세스를 구현한다. 온라인 또는 런타임 트레이닝 동안, 추론 프로세스는 \"현재\" 버퍼의 모델 또는 모델 가 중치를 사용하여, 호스트 시스템 행동의 적응형 예측을 제공할 수 있다. 다른 양상들에서, 제 1 세트의 시간 간 격들 동안 AI 모델 트레이닝이 구현되고 제 2 세트의 상이한 시간 간격들에서 추론이 수행되도록, AI 모델 개량 또는 재트레이닝이 수행될 수 있다. 저장 매체 액세스의 AI 기반의 관리를 위한 기법들 다음의 논의들은 저장 드라이브의 성능을 최적화하기 위해 호스트 I/O 또는 내부 I/O를 스케줄링할 수 있는, 저 장 매체 액세스의 AI 기반 관리를 위한 기법들을 설명한다. 이들 기법들은 매체 액세스 관리자, AI 엔진 및/또는 AI 모델과 같은 본 명세서에 기술된 임의의 환경 및 엔티티를 사용하여 구현될 수 있다. 이 들 기법들은 도 14-18에 도시된 다양한 방법을 포함하며, 도 14 내지 도 18 각각은 하나 이상의 엔티티에 의해 수행될 수 있는 동작들의 세트로서 도시된다. 이들 방법들은 반드시 관련 도면에 도시된 동작들의 순서대로 한정되지 않는다. 오히려, 본 명세서에 설명된 다 양한 양상들을 구현하기 위해 임의의 동작들이 반복, 스킵, 대체 또는 재정렬될 수 있다. 또한, 이들 방법들은 동일한 엔티티, 별개의 엔티티, 또는 이들의 임의의 조합에 의해 수행되는지에 상관없이, 전체적으로 또는 부분 적으로 서로와 함께 사용될 수 있다. 예를 들어, 상기 방법들은 서로 조합되어, 저장 매체 액세스(예컨대, 대기 시간 또는 처리량)에 대하여 호스트 시스템 성능을 최적화하기 위한 호스트 시스템 동작의 예측에 기초하여, 저 장 매체 시스템의 호스트 I/O 및/또는 내부 I/O를 스케줄링할 수 있는 저장 매체의 AI 기반의 관리를 구현할 수 있다. 다음의 논의의 일부에서, 도 1의 동작 환경 및 도 2-13의 다양한 엔티티 또는 구성이 일례로서 참조 될 것이다. 이러한 참조는 설명된 양상들을 운영 환경, 엔티티 또는 구성으로 제한하고자 의도되는 것이 아니라, 다양한 일례들 중 하나로서 예시하기 위한 것이다. 대안적으로 또는 부가적으로, 상기 방법들의 동작들 은 도 19의 시스템-온-칩 및/또는 도 20의 저장 시스템 컨트롤러를 참조하여 서술된 엔티티들로 구현될 수 있다. 도 14는 매체 액세스 관리자 또는 매체 저장 컨트롤러의 AI 엔진에 의해 수행되는 동작을 포함하는, 저장 매체 액세스의 AI 기반 관리를 위한 예시적인 방법을 도시한다. 단계 1402에서, 저장 매체에 액세스하기 위한 호스트 I/O가 호스트 시스템으로부터 수신된다. 호스트 I/O는 저 장 시스템 또는 저장 컨트롤러의 호스트 시스템 인터페이스를 통해 호스트 시스템으로부터 수신될 수 있다. 일 부 경우에, 호스트 I/O는 각각의 논리 블록 어드레스, 사이즈 또는 타임 스탬프를 포함하는 호스트 시스템의 기 입 명령 또는 판독 명령을 포함하거나 이에 대응한다. 단계 1404에서, 호스트 시스템의 호스트 I/O를 서술하는 정보가 AI 엔진에 제공된다. AI 엔진은 저장 컨트롤러 의 일부로서 저장 시스템과 연관되거나 또는 저장 시스템의 저장 컨트롤러에 의해 액세스될 수 있다. 일부 경우 에, 호스트 I/O를 서술하는 정보 또는 서술자(descriptor)는 호스트 I/O의 이벤트 유형, 호스트 I/O의 이벤트 지속 기간, 또는 호스트 I/O와 관련된 데이터의 이벤트 크기를 포함한다. 단계 1406에서, 호스트 시스템 행동에 대한 예측이 AI 엔진으로부터 수신된다. 호스트 시스템 행동의 예측은 호 스트 시스템에 의한 저장 매체의 후속 액세스에 대한 호스트 시스템 활동을 설명하는 정보를 포함할 수 있다. 경우에 따라 호스트 시스템 동작 예측은, 호스트 시스템이 유휴 상태가 될 때까지의 시간 기간, 호스트 시스템 이 유휴 상태로 유지되는 시간 기간 또는 호스트 시스템에 의해 발행된 다음 호스트 I/O에 관한 파라미터들을 포함한다. 단계 1408에서, 저장 매체에 액세스하기 위한 호스트 I/O는 호스트 시스템 행동의 예측에 근거하여 스케줄링된 다. 호스트 I/O는 저장 시스템의 내부 작업들의 다른 I/O를 선점하도록 스케줄링될 수 있다. 경우에 따라, 호스 트 I/O는 가비지 컬렉션, 캐싱, 데이터 마이그레이션 또는 열 스로틀링을 위한 수정된 파라미터들 또는 임계값 들을 이용하여 스케줄링되거나 수행된다. 예를 들어, 매체 액세스 관리자는 호스트 I/O를 완료할 수 있도록 내 부 저장 시스템 동작들이 트리거링하는 것을 방지하기 위하여, 하나 이상의 임계값을 일시 중단하거나 선점할 수 있다. 선택적으로는, 단계 1410에서, 호스트 시스템 동작의 예측에 기초하여 저장 시스템의 내부 I/O가 스케줄링될 수 있다. 일부 경우, 매체 액세스 관리자는 저장 시스템이 보류중인(pending) 내부 I/O를 갖고 있다라고 결정한다. 내부 I/O의 스케줄링은 가령, 내부 I/O와 호스트 I/O 또는 저장 매체에 액세스하기 위한 후속 호스트 I/O 사이 의 경합을 완화하기 위하여, 호스트 시스템 행동의 예측에 기초하여, 저장 시스템의 내부 I/O를 속행 (advancing) 또는 지연(delaying)시키는 것을 포함할 수 있다. 단계 1412에서, 스케줄링된 호스트 I/O 및/또는 저장 시스템의 내부 I/O에 대응하는 매체(media) I/O가 수행된 다. 저장 컨트롤러의 FTL은 스케줄링된 호스트 I/O 및 저장 시스템의 내부 I/O에 대응하는 각각의 매체 I/O를 생성할 수 있다. 매체 I/O에 응답하여, 저장 시스템은 판독된 데이터를 호스트 시스템으로 리턴하거나, 호스트 시스템의 데이터를 저장 시스템에 저장할 수 있다. 도 15는 호스트 시스템 행동의 예측에 기초하여 저장 시스템의 내부 동작을 지연시키기 위한 예시적인 방법 을 나타낸다. 일부 양상들에서, 매체 액세스 관리자 또는 매체 저장 컨트롤러의 AI 엔진이 상 기 방법의 동작들을 수행한다. 단계 1502에서, 호스트 I/O 이벤트들의 각각의 유형들 및 지속 시간을 나타내는 정보가 AI 엔진의 AI 모델에 제 공된다. 호스트 I/O 이벤트는 AI 엔진과 연관된 저장 시스템의 저장 매체에 대한 액세스를 지정하거나 요청할 수 있다. 일부 경우, 호스트 I/O 이벤트를 설명하는 정보(또는 서술자)는 호스트 I/O의 이벤트 유형, 호스트 I/O의 이벤트 지속 기간 또는 호스트 I/O와 연관된 데이터의 이벤트 크기를 포함한다. 단계 1504에서, 호스트 시스템의 다가오는 유휴 시간에 대한 예측이 AI 엔진의 AI 모델로부터 수신된다. 다가오 는 유휴 시간에 대한 예측은 호스트 시스템이 유휴 상태가 될 때까지의 시간 기간, 호스트 시스템이 유휴 상태 를 유지하는 시간 기간, 또는 호스트 시스템에 의해 발행된 다음 호스트 I/O에 관한 파라미터들을 포함할 수 있 다. 일부 경우, 이러한 예측은, 호스트 시스템의 다음 유휴 시간 이전에 발생할 것으로 예상되는 호스트 I/O 활 동에 대한 표시를 포함한다. 단계 1506에서, 호스트 시스템의 다가오는 유휴 시간의 예측에 기초하여 저장 시스템의 내부 동작 또는 내부 작 업이 지연된다. 내부 동작은 가비지 컬렉션, 데이터 마이그레이션 또는 저장 매체의 액세스를 수반하는 기타 작 업들을 포함할 수 있다. 일부 경우, 내부 동작들은 각각의 임계값들에 의해 트리거링되며, 이들 임계값들은 내 부 동작이 지연되는 동안 일시중지되거나 선점될 수 있다. 단계 1508에서, 저장 시스템의 내부 동작이 지연되는 동안, 호스트 시스템의 호스트 I/O에 대응하는 매체 I/O가 수행된다. 수행되는 매체 I/O는 호스트 시스템의 기입 커맨드 또는 판독 커맨드에 해당할 수 있다. 저장 시스템 의 내부 동작이 지연되는 동안, 저장 시스템의 내부 I/O와 충돌 혹은 경쟁함이 없이 상기 매체 I/O가 수행될 수 있다. 이와 같이, 호스트 I/O 성능은 저장 매체에 대한 호스트 시스템의 완전한 액세스를 허용하도록(예를 들 어, 스로틀링 또는 스톨링 없이) 최적화될 수 있다. 단계 1510에서, 지연된 내부 동작의 내부 I/O에 대응하는 다른 매체 I/O가 예정된 유휴 시간 동안 수행된다.내 부 I/O는 호스트 시스템의 유휴 시간 동안 수행되는, 지연된 데이터 마이그레이션 또는 지연된 가비지 컬렉션에 대응할 수 있다. 일부 경우, 이러한 지연은 데이터 마이그레이션 또는 가비지 콜렉션이 호스트 I/O의 성능을 저 하시키지 않게 하는데 효과적이며, 따라서 저장 매체에 대한 호스트 시스템 액세스가 최적화된다. 도 16는 호스트 시스템 행동의 예측에 기초하여 저장 시스템의 내부 동작을 속행시키기 위한 예시적인 방법 을 나타낸다. 일부 양상들에서, 매체 액세스 관리자 또는 매체 저장 컨트롤러의 AI 엔진이 상 기 방법의 동작들을 수행한다. 단계 1602에서, 호스트 시스템의 호스트 I/O를 서술하는 정보가 AI 엔진의 AI 모델에 제공된다. 호스트 I/O는 AI 엔진과 연관된 저장 시스템의 저장 매체에 대한 액세스하기 위한 I/O 커맨드 또는 I/O 동작을 포함할 수 있 다. 일부 경우, 호스트 I/O 이벤트를 설명하는 정보(또는 서술자)는 호스트 I/O의 이벤트 유형, 호스트 I/O의 이벤트 지속 기간 또는 호스트 I/O와 연관된 데이터의 이벤트 크기를 포함한다. 단계 1604에서, 호스트 시스템의 액세스 활동에 대한 예측이 AI 엔진의 AI 모델로부터 수신된다. 액세스 활동의 예측은 호스트 시스템의 기입 밀도, 저장 매체에 기입될 데이터의 양, 또는 기입 활동의 예상 지속 시간을 포함 할 수 있다. 일부 경우에, 예측은 또한 기입 활동이 발생할 때까지의 시간 분량을 포함하며, 따라서 매체 액세 스 관리자는 호스트 시스템의 예측된 활동이 시작되기 전에 다른 내부 작업이 수행될 수 있는지 여부를 결정할 수 있다. 단계 1606에서, 저장 시스템의 내부 동작은 호스트 시스템의 액세스 활동의 예측에 기초하여 속행(advance)된다 . 내부 동작은 캐시 메모리의 여유 공간을 확보하거나 캐시 메모리 또는 저장 매체의 여유 공간에 대한 공칭 임 계값을 넘어 여유 공간의 양을 증가시키는 것과 같은 가비지 컬렉션 또는 데이터 마이그레이션을 포함할 수 있 다. 예를 들어, 내부 동작은 호스트 시스템의 예측된 활동이, 상기 활동이 발생하는 동안 내부 동작들을 트리거 링하지 않도록 충분한 캐시 공간을 확보하기 위해 속행 및 구성될 수 있다. 단계 1608에서, 속행된 내부 동작의 내부 I/O에 대응하는 매체 I/0가 수행된다. 매체 I/O는 캐시 메모리 또는 저장 매체에 대한 속행된 또는 예상된 가비지 컬렉션 또는 데이터 마이그레이션에 대응할 수 있다. 적어도 일부 경우에, 속행된 또는 예측형 내부 동작은, 호스트 시스템 활동을 가능케하기 위해 캐시 메모리 또는 저장 매체 에 충분한 여유 공간을 생성한다(상기 활동이 추가적인 내부 동작들에 대한 임계값들을 트리거링함이 없이). 단계 1610에서, 호스트 시스템의 예측된 액세스 활동의 호스트 I/O에 대응하는 다른 매체 I/O가 수행된다. 내부 동작들의 매체 I/O들 중 적어도 일부가 수행된 이후에, 호스트 I/O에 대한 매체 I/O가 저장 매체 액세스를 위해 수행된다. 일부 경우, 속행된 내부 동작에 의해 제공되는 여유 공간은, 호스트 I/O에 대응하는 매체 I/O가 호스트 시스템 액세스의 전체 레벨에서 수행될 수 있게 한다. 이와 같이, 저장 시스템의 내부 동작들을 속행시킴으 로써, 저장 매체로의 액세스를 위해 내부 I/O가 호스트 I/O와 경쟁하는 것이 방지될 수 있으며, 따라서 저장 매 체에 대한 호스트 시스템 액세스가 최적화될 수 있다. 도 17은 저장 매체의 디바이스-레벨 관리를 위한 파라미터들을, 호스트 시스템 행동의 예측에 기초하여 변경하 는 예시적인 방법을 도시한다. 일부 양상들에서, 매체 액세스 관리자 또는 매체 저장 컨트롤러의 AI 엔진이 상기 방법의 동작들을 수행한다. 단계 1702에서, 저장 매체에 액세스하기 위한 호스트 I/O가 호스트 시스템으로부터 수신된다. 호스트 I/O는 저 장 시스템 또는 저장 컨트롤러의 호스트 시스템 인터페이스를 통해 호스트 시스템으로부터 수신될 수 있다. 일 부 경우에, 호스트 I/O는 각각의 논리 블록 주소, 크기 또는 타임 스탬프를 포함하는 호스트 시스템의 기입 커 맨드 또는 판독 커맨드를 포함하거나 이에 상응한다. 단계 1704에서, 호스트 I/O를 서술하는 정보가 인공 지능 엔진에 제공된다. AI 엔진은 저장 컨트롤러의 일부로 서 저장 시스템과 연관되거나 또는 저장 시스템의 저장 컨트롤러에 의해 액세스될 수 있다. 일부 경우에, 호스 트 I/O를 서술하는 정보 또는 서술자는 호스트 I/O의 이벤트 유형, 호스트 I/O의 이벤트 지속 기간, 또는 호스 트 I/O와 관련된 데이터의 이벤트 크기를 포함한다. 단계 1706에서, 호스트 동작의 예측이 AI 엔진으로부터 수신된다. 호스트 시스템 행동에 대한 예측은 호스트 시 스템에 의한 저장 매체의 후속 액세스에 대한 호스트 시스템 활동을 설명하는 정보를 포함할 수 있다. 일부 경 우, 호스트 시스템 행동의 예측은, 호스트 시스템이 유휴 상태가 될 때까지의 시간 기간, 호스트 시스템이 유휴 상태를 유지하는 시간 기간 또는 호스트 시스템에 의해 발행된 다음 호스트 I/O에 관련된 파라미터들(예컨대, 기입 강도)을 포함한다. 단계 1708에서, 저장 매체의 디바이스-레벨 관리를 위한 파라미터는, 호스트 시스템 행동의 예측에 기초하여 변 경된다. 상기 파라미터는 저장 매체 디바이스의 열 관리를 위한 임계값을 포함할 수 있다. 예를 들어, 예정된 유휴 시간에 기초하여, 매체 액세스 관리자는 저장 매체에 대한 액세스를 열 스로틀링함이 없이, 데이터 기입를 위해 호스트 I/O가 최대 성능으로 완료될 수 있도록 열 제한을 선점하거나 정지할 수 있다. 대안적으로 또는 부 가적으로, 액세스 동작에 대한 완전한 호스트 I/O 성능을 허용하기 위해, 가비지 컬렉션 또는 데이터 마이그레 이션에 대한 임계값이 선점되거나 정지될 수 있다. 예정된 유휴 시간(호스트 시스템 활동 없음)의 예측에 기초하여, 호스트 시스템은 전체 성능 액세스를 제공받을 수 있는바, 이는 동작의 완료 후에, 저장 매체 디바이스가 냉각에 충분한 시간을 갖게될 것임을 알고 있기 때문 이다. 다른 경우에, 매체 액세스 관리자는 가비지 컬렉션 또는 데이터 마이그레이션이 기입 동작이 끝날 때까지 지연되는 경우에도(예를 들어, 임계값을 변경 또는 일시 중단함), 여유 공간이 부족하지 않은 상태에서 기입 동 작이 완료될 것으로 판단할 수 있다. 단계 1710에서, 디바이스-레벨 관리를 위한 변경된 파라미터들에 따라, 호스트 시스템의 호스트 I/O에 대응하는 매체 I/O가 수행된다. 저장 컨트롤러의 FTL은 스케줄링된 호스트 I/O 및 저장 시스템의 내부 I/O에 대응하는 각 각의 매체 I/O를 생성할 수 있다. 매체 I/O는 저장 매체 또는 저장 매체 디바이스에 대한 변경되거나 정지된 열 제한에 따라 수행될 수 있다. 대안적으로 또는 부가적으로, 매체 I/O는 데이터 마이그레이션 또는 가비지 컬렉 션을 위해 변경되거나 정지된 임계값에 따라 수행될 수 있다. 도 18은 저장 시스템의 매체 액세스 관리자에 의해 수행되는 동작들을 포함하여, 온라인 재트레이닝 또는 개량이 가능하도록 AI 모델의 다중 인스턴스 추론을 실행하는 예시적인 방법을 도시한다. 단계 1802에서, AI 엔진의 하드웨어를 통해, AI 모델의 제 1 인스턴스(예컨대, 각각의 가중치들 또는 파라미터 들)의 제 1 파라미터들로 추론이 실행된다. 제 1 파라미터들은 제 1 버퍼 또는 메모리 영역에 저장될 수 있다. 일부 경우, AI 모델의 제 1 인스턴스는 추론을 실행하는데 사용되는 AI 모델의 \"현재\" 인스턴스이다. 추론은 AI 엔진의 동일한 하드웨어에서 실행되는 온라인 트레이닝 프로세스와 병행하여 실행될 수 있다. 단계 1804에서, AI 모델의 제 1 인스턴스의 제 1 파라미터들에 기초하여 AI 모델의 제 2 인스턴스의 제 2 파라 미터들이 업데이트된다. 제 2 파라미터들은 제 2 버퍼 또는 메모리 영역에 저장될 수 있다. 일부 경우, AI 모델 의 제 2 인스턴스는 하드웨어에서 실행중인 \"현재\" 인스턴스로부터 업데이트된 AI 모델의 \"업데이트\" 인스턴스 이다. AI 모델의 제 2 인스턴스의 제 2 파라미터들이 업데이트 동안, 온라인 재트레이닝 프로세스가 일시 정지 될 수 있다. 단계 1806에서, 제 2 인스턴스의 제 2 파라미터들이 업데이트되는 동안, AI 모델의 제 1 인스턴스의 제 1 파라미터들로 추론이 계속 실행된다. 단계 1808에서, AI 엔진의 하드웨어를 통해, AI 모델의 제 2 인스턴스의 제 2 파라미터들로 추론이 실행된다. AI 모델의 제 2 인스턴스는 추론을 실행하는데 사용되는 AI 모델의 \"현재\" 인스턴스가 될 수 있다. AI 모델의 제 1 인스턴스는 하드웨어에서 실행중인 \"현재\" 인스턴스로부터 업데이트된 AI 모델의 \"업데이트\" 인스턴스가 될 수 있다. 추론은 AI 엔진의 동일한 하드웨어에서 실행되는 온라인 트레이닝 프로세스와 병행하여 실행될 수 있다. 단계 1810에서, AI 모델의 제 2 인스턴스의 제 2 파라미터에 기초하여 AI 모델의 제 1 인스턴스의 제 1 파라미 터들이 업데이트된다. AI 모델의 제 1 인스턴스의 제 1 파라미터들이 업데이트 동안, 온라인 재트레이닝 프로세 스가 일시 정지될 수 있다. 단계 1812에서, 제 1 인스턴스의 제 1 파라미터들이 업데이트되는 동안, AI 모델의 제 2 인스턴스의 제 2 파라미터들로 추론이 계속 실행된다. 단계 1812로부터, 상기 방법은 단계 1802로 복귀하여 온라인 트레이닝 프로세스의 다른 이터레이션을 시작할 수 있다. 시스템-온-칩 도 19는 저장 매체 액세스의 AI 기반 관리의 다양한 양상들을 구현할 수 있는 예시적인 시스템 온 칩(SoC: System-on-Chip)을 도시한다. SoC은 스마트폰, 넷북, 태블릿 컴퓨터, 액세스 포인트, 네트워크-부 착 스토리지, 카메라, 스마트 기기, 프린터, 셋톱 박스, 서버, 데이터 센터, 솔리드 스테이트 드라이브(SSD), 하드 디스크 드라이브(HDD), 스토리지 드라이브 어레이, 메모리 모듈, 자동차 컴퓨팅 시스템 또는 임의의 다른 적합한 유형의 디바이스(예를 들어, 여기에 기술된 다른 디바이스)와 같은 임의의 적절한 시스템 또는 디바이스 에서 구현될 수 있다. SoC를 참조하여 설명되었지만, 도 19의 엔티티들은 또한 주문형 집적회로(ASIC: ajpplication-specific integrated-circuit), 메모리 컨트롤러, 저장 컨트롤러, 통신 컨트롤러, 어플리케이션 특정 표준 제품(ASSP), 디지털 신호 프로세서(DSP), 프로그램가능 SoC(PSoC), 시스템-인-패키지(SiP) 또는 필드 프로그램가능 게이트 어레이(FPGA)와 같은 다른 유형의 집적 회로 또는 내장 시스템으로서 구현될 수 있다. SOC는 본원에 기술된 임의의 디바이스 또는 구성 요소(예를 들어, 저장 드라이브 또는 저장 어레이)와 같 은, 전자 회로, 마이크로 프로세서, 메모리, 입출력(I/O) 제어 로직, 통신 인터페이스, 컴퓨팅 디바이스의 기능 을 제공하는데 유용한 펌웨어 및/또는 소프트웨어, 호스트 시스템 또는 저장 시스템과 통합될 수 있다. SoC는 또한 제어 시그널링, 데이터 통신 및/또는 컴포넌트들 간의 라우팅을 위해 SoC의 다양한 컴포넌트 들을 결합하는 통합 데이터 버스 또는 상호연결 패브릭(도시되지 않음)을 포함할 수 있다. SoC 1900의 통합 데 이터 버스, 상호연결 패브릭 또는 기타 구성 요소는 외부 포트, 병렬 데이터 인터페이스, 직렬 데이터 인터페이 스, 패브릭 기반 인터페이스, 주변 컴포넌트 인터페이스 또는 기타 적합한 데이터 인터페이스를 통해 노출되거 나 액세스될 수 있다. 예를 들어, SoC의 컴포넌트들은 외부 인터페이스 또는 오프-칩 데이터 인터페이스 를 통해 외부 저장 매체, AI 엔진, AI 모델 또는 AI 네트워크에 액세스하거나 제어할 수 있다. 본 실시예에서, SoC는 입출력(I/O) 제어 로직 및 하드웨어 기반 프로세서(프로세서 1904), 가령, 마이크로 프로세서, 프로세서 코어, 어플리케이션 프로세서, DSP 등과 같은 하드웨어 기반 프로세서(프로세서 1904) 등의 다양한 구성 요소를 포함한다. SoC는 또한 메모리를 포함하며, 메모리는 RAM, SRAM, DRAM, 비휘발성 메모리, ROM, 1회용 프로그램가능(OTP) 메모리, MTP(다중 프로그래밍 가능) 메모리, 플래시 메 모리, 및/또는 다른 적절한 전자 데이터 저장소와 같은 임의의 유형 및/또는 조합을 포함할 수 있다. 일부 양상 들에서, 프로세서 및 메모리에 저장된 코드는 저장 매체 액세스의 AI 기반 관리와 관련된 다양한 기능을 제공하기 위해 저장 시스템 컨트롤러 또는 스토리지 어그리게이터(aggregator)로서 구현된다. 본 개시의 맥락에서, 메모리는 비일시적 신호를 통해 데이터, 코드, 명령 또는 다른 정보를 저장하고, 반송파 또는 일시적 신호는 포함하지 않는다. 대안적으로 또는 부가적으로, SoC는 솔리드 스테이트 메모리(예를 들어, 플래시 또는 NAND 메모리), 자기 기반 메모리 매체, 또는 광학-기반 메모리 매체와 같은 추가적인 또는 확장가 능한 오프-칩 저장 매체에 액세스하기 위한 데이터 인터페이스(도시되지 않음)를 포함할 수 있다. SoC는 또한, 펌웨어, 어플리케이션, 프로그램, 소프트웨어, 및/또는 운영 체제를 포함할 수 있는바, 이는 SoC의 기능들을 구현하기 위해 프로세서에 의해 실행되도록 메모리에 유지되는 프로세서 실행가능한 명령들로 구현될 수 있다. SoC는 또한, 로컬 온-칩(미도시) 또는 오프-칩 통신 트랜 시버의 구성요소들을 제어하거나 통신하기 위한 트랜시버 인터페이스 등과 같은 다른 통신 인터페이스들을 포함 할 수 있다. 대안적으로 또는 부가적으로, 트랜시버 인터페이스는 트랜시버, 물리 계층 트랜시버(PHY), 또는 시 스템 온칩에 결합된 매체 액세스 컨트롤러(MAC)를 통한 유선 또는 무선 통신을 용이하게 하기 위해 칩 외 부의 무선 주파수(RF), 중간 주파수(IF) 또는 기저 대역 주파수 신호를 통신하기 위한 신호 인터페이스를 포함하거나 구현할 수 있다. 예를 들어, SoC는 저장 매체 액세스의 AI 기반 관리에 네트워크 부착 스토리지 (NAS) 볼륨을 제공하는 것과 같이, 유무선 네트워크를 통해 저장이 가능하도록 구성된 트랜시버 인터페이스를 포함할 수 있다. 또한, SoC는 매체 액세스 관리자, AI 엔진, 및 AI 모델을 포함하며, 이들은 도시된 바와 같이 개별적으로 구현될 수도 있으며 또는 저장 컴포넌트, 데이터 인터페이스와 결합되거나, 또는 오프-칩 인터 페이스를 통해 액세스될 수도 있다(예컨대, 외부 메모리에 저장된 AI 모델). 저장 매체 액세스의 AI 기반 관리 의 다양한 양상에 따르면, 매체 액세스 관리자는 AI 엔진과 상호작용하여 미래의 저장 매체 액세스에 대한 호스트 행동의 예측을 생성할 수 있고, 저장 매체 성능을 최적화하도록 상기 예측에 기초하여 내부 동작들 (예컨대, 내부 작업 I/O들)을 스케줄링할 수 있다. 대안적으로 또는 추가적으로, AI 엔진은 호스트 행동을 예측하기 위해 다수의 AI 모델들을 구현할 수 있으며, 이들은 호스트 특정 행동의 개선된 예측을 위해 동 시에 실행되거나 및/또는 온라인 개량 또는 재트레이닝과 함께 실행될 수 있다. 이들 엔티티 중 임의의 것은 본 명세서에 제시된 다양한 양상을 참조하여 설명된 바와 같이 개별 컴포넌트 또는 조합된 컴포넌트로서 구현될 수 있다. 이들 컴포넌트 및/또는 엔티티, 또는 대응하는 기능의 일례는 도 1의 환경의 각각의 컴포넌트 또는 엔티티 또는 도 2 내지 도 13에 도시된 각각의 구성을 참조하여 설명된다. 매체 액세스 관리자 또는 AI 엔 진은 전체적으로 또는 부분적으로, 메모리에 의해 유지되는 프로세서 실행가능 명령들로서 구현될 수 있고, 저장 매체 액세스의 AI 기반 관리의 다양한 양상 및/또는 특징을 구현하기 위해 프로세서에 의 해 실행될 수 있다. 매체 액세스 관리자는 독립적으로 구현되거나 또는 임의의 적당한 컴포넌트 또는 회로와 조합으로 구현되 어, 본 명세서에 기재된 양상들을 구현할 수 있다. 예를 들어, 매체 액세스 관리자 또는 AI 엔진은 DSP, 프로세서/스토리지 브리지, I/O 브리지, 그래픽 프로세싱 유닛, 메모리 컨트롤러, 저장 컨트롤러, 산술 논 리 유닛(ALU), 등의 일부로서 구현될 수 있다. 또한, 매체 액세스 관리자는 SoC의 다른 엔티티들과 일체로 제공될 수 있는바 가령, SoC의 프로세서, 메모리, 저장 매체 인터페이스 또는 펌웨어 와 통합되어 제공될 수 있다. 대안적으로 또는 추가적으로, 매체 액세스 관리자, AI 엔진 및/ 또는 SoC의 다른 구성요소들은 하드웨어, 펌웨어, 고정 논리 회로 또는 이들의 임의의 조합으로 구현될 수 있다. 또 다른 일례로서, 저장 매체 액세스의 AI 기반 관리의 하나 이상의 양상들에 따른 예시적인 저장 시스템 컨트 롤러를 도시한 도 20을 고려하자. 다양한 양상들에서, 저장 시스템 컨트롤러 또는 이들의 컴포넌트 들의 임의의 조합은 스토리지 드라이브 컨트롤러, 저장 매체 컨트롤러, NAS 컨트롤러, 패브릭 인터페이스, NVMe 타겟, 또는 고체 저장 매체를 위한 스토리지 집합 컨트롤러로서 구현될 수 있다. 일부 경우에, 저장 시스템 컨 트롤러는 도 19를 참조하여 설명된 바와 같은 SoC의 구성요소들과 유사하게 또는 이와 함께 구현된 다. 다시 말해서, SoC의 인스턴스는, AI의 도움으로 솔리드 스테이트(예컨대, NAND 플래시 기반의) 매체 를 관리하는 저장 시스템 컨트롤러와 같은, 저장 시스템 컨트롤러로서 구성될 수 있다. 본 실시예에서, 저장 시스템 컨트롤러는 마이크로 프로세서, 프로세서 코어, 어플리케이션 프로세서, DSP 등과 같은 프로세서와 입력-출력(I/O) 제어 로직을 포함할 수 있다. 일부 경우, 저장 시스템 컨트 롤러의 프로세서 및 펌웨어는 방법 내지 방법을 참조하여 설명된 것과 같은 저장 매체 액세스의 AI 기반 관리와 관련된 다양한 기능들을 제공하도록 구현될 수 있다. 저장 시스템 컨트롤러는 또한 호스트 인터페이스(예를 들어, SATA, PCIe, NVMe 또는 패브릭 인터페이스) 및 저장 매체 인터페이스 (예를 들어, NAND 인터페이스)를 포함하며, 이들을 통해 각각 호스트 시스템 및 저장 매체에 액세스할 수 있다. 저장 시스템 컨트롤러는 또한 플래시 변환 계층(FTL), 디바이스-레벨 관리자 및 I/O 스케줄러를 포함한다. 저장 매체 액세스의 AI 기반 관리의 일부 양상에서, FTL 및/또는 디바이스-레 벨 관리자는 저장 시스템 컨트롤러의 내부 I/O를 생성하거나, 또는 저장 매체 인터페이스을 통해 저장 매체 디바이스를 관리한다. 또한, 저장 시스템 컨트롤러는 매체 액세스 관리자, AI 엔진 및 AI 모델의 인스턴스들을 포함한다. 이들 컴포넌트들 중 전부 또는 일부는 도시된 바와 같이 별도로 구현되거나 또는 프로세서, 호 스트 인터페이스, 저장 매체 인터페이스, 플래시 변환 계층, 디바이스-레벨 관리자 및 /또는 I/O 스케줄러와 조합되어 구현될 수 있다. 이들 컴포넌트들 및/또는 엔티티들, 또는 대응하는 기능 의 일례는 도 1의 환경의 각각의 컴포넌트들 또는 엔티티들을 참조하여 설명되거나 또는 도 2 내지 도 13 에 도시된 각각의 구성을 참조하여 설명된다. 저장 매체 액세스의 AI 기반 관리의 다양한 양상들에 따르면, 매 체 액세스 관리자는 저장 시스템 컨트롤러의 AI 엔진과 상호작용하여, 미래의 저장 매체 액세스와 관련된 호스트 행동의 예측을 생성하고 그리고 저장 매체 성능을 최적화하도록 상기 예측에 기초하여 내부 동작들(예컨대, 내부 작업 I/O)를 스케줄링한다. 대안적으로 또는 추가적으로, AI 엔진은 호스트 행동을 예측하기 위해 다수의 AI 모델들을 구현할 수 있으며, 이들은 호스트 특정 행동의 개선된 예측을 위해 동 시에 실행되거나 및/또는 온라인 개량 또는 재트레이닝과 함께 실행될 수 있다. 매체 액세스 관리자는 전 체적으로 또는 부분적으로, 컨트롤러의 메모리에 의해 유지되는 프로세서 실행가능 명령들로서 구현될 수 있고, 저장 매체 액세스의 AI 기반 관리의 다양한 양상 및/또는 특징을 구현하기 위해 프로세서에 의해 실행될 수 있다. 비록, 구조적 특징들 및/또는 방법론적 동작들에 특정한 언어로서 본 발명의 주제가 설명되었지만, 청구범위에 정의된 본 발명의 주제는 이들이 수행되는 순서를 포함하여 본원에 서술된 특정 일례들, 특징들, 또는 동작들로 한정될 필요는 없다. 이하에서, 몇몇 실시예들이 설명된다: 실시예 1: 저장 매체 액세스의 인공 지능 기반 관리를 위한 방법으로서: 호스트 시스템으로부터 그리고 저장 시스템의 호스트 인터페이스를 통하여, 상기 저장 시스템의 저장 매체에 액 세스하기 위한 호스트 입/출력(I/O)을 수신하는 단계; 상기 호스트 시스템으로부터 수신된 호스트 I/O를 설명하는 정보를 상기 저장 시스템과 관련된 인공 지능 엔진 에 제공하는 단계; 상기 호스트 시스템에 의한 상기 저장 매체에 대한 후속 액세스에 관련된 호스트 시스템 행동의 예측을 상기 인 공 지능 엔진으로부터 수신하는 단계; 및 상기 호스트 시스템 행동의 예측에 기초하여, 상기 저장 시스템의 저장 매체에 액세스하기 위한 호스트 I/O를 스케줄링하는 단계를 포함한다. 실시예 2: 제1항에 있어서, 상기 저장 시스템이 보류중인(pending) 내부 I/O를 가지고 있는지를 결정하는 단계를 더 포함하고, 상기 스케줄 링하는 단계는, 상기 호스트 시스템 행동의 예측에 기초하여, 상기 저장 시스템의 저장 매체에 액세스하기 위해 상기 호스트 시 스템의 호스트 I/O 및 상기 저장 시스템의 내부 I/O를 스케줄링하는 단계를 포함한다. 실시예 3: 제2항에 있어서, 상기 저장 시스템의 내부 I/O를 스케줄링하는 단계는, 상기 내부 I/O 및 호스트 I/O 또는 상기 저장 매체에 액세스하기 위한 후속 호스트 I/O 사이의 경합을 완화시 키도록, 상기 호스트 시스템 행동의 예측에 기초하여, 상기 저장 시스템의 내부 I/O를 속행(advancing) 또는 지 연시키는 단계를 포함한다. 실시예 4: 제2항에 있어서, 상기 내부 I/O는, 가비지 콜렉션, 데이터 마이그레이션, 또는 웨어 레벨링(wear leveling) 중 하나를 포함하는 저장 시스템의 플래시 변환 계층(Flash translation layer) 중 하나 이상의 작업들에 대응한다. 실시예 5: 제1항에 있어서, 상기 인공 지능 엔진으로부터 수신된 호스트 시스템 행동의 예측은, 호스트 시스템이 유휴 상태가 될 때까지의 시간 기간에 대한 표시; 호스트 시스템이 유휴 상태로 유지되는 시간 기간에 대한 표시; 또는 호스트 시스템에 의해 발행된 다음 호스트 I/O에 관한 파라미터들에 대한 표시를 포함한다. 실시예 6: 제1항에 있어서, 상기 호스트 I/O를 설명하는 정보는, 적어도 하나의 호스트 I/O에 대한, 호스트 I/O의 이벤트 유형에 대한 표시; 호스트 I/O의 이벤트 기간에 대한 표시; 또는 호스트 I/O와 관련된 데이터의 이벤트 크기에 대한 표시를 포함한다. 실시예 7: 제1항에 있어서, 상기 호스트 I/O를 스케줄링하는 단계는, 상기 저장 매체의 온도 관리를 위한 저장 매체의 디바이스 레벨 파라미터들에 기초한다. 실시예 8: 제1항에 있어서, 상기 인공 지능 엔진은 다수의 인공 지능 모델들을 실행하고; 다수의 인공 지능 모델들 중 적어도 2 개는 상기 저장 시스템의 플래시 변환 계층 또는 디바이스 레벨 관리자에 의해 구현되는 각각의 내부 작업들과 관련되며; 그리고 상기 방법은 상기 정보를 인공 지능 엔진에 제공하기 전에, 호스트 시스템 행동의 예측이 가능하도록 상기 다수 의 인공 지능 모델들 중 적어도 하나를 인공 지능 엔진에 로딩하는 단계를 더 포함한다. 실시예 9: 제8항에 있어서, 상기 저장 시스템의 적어도 2개의 인공 지능 보조 내부 작업들(artificial intelligence-assisted internal tasks)을 구현하도록, 상기 인공 지능 엔진을 통해 상기 다수의 인공 지능 모델들 중 적어도 2개를 동시에 실행 하는 단계를 더 포함한다. 실시예 10: 제8항에 있어서, 상기 인공 지능 모델의 온라인 재-트레이닝(online re-training) 또는 개량(refinement)이 가능하도록, 상기 인 공 지능 엔진을 통해 상기 다수의 인공 지능 모델들 중 한 모델의 2개의 인스턴스들을 병렬로 실행하는 단계를 더 포함한다. 실시예 11: 장치로서, 호스트 시스템과 통신하도록 구성된 호스트 인터페이스; 호스트 시스템의 데이터를 저장하는 저장 매체; 상기 저장 매체에 대한 액세스를 가능하게하는 매체 인터페이스; 인공 지능 엔진; 및 매체 액세스 관리자를 포함하고, 상기 매체 액세스 관리자는, 상기 호스트 인터페이스를 통해, 상기 장치의 저장 매체에 액세스하기 위한 호스트 입/출력(I/O)을 상기 호스트 시스템으로부터 수신하고; 상기 호스트 시스템으로부터 수신된 호스트 I/O를 설명하는 정보를 상기 인공 지능 엔진에 제공하고; 상기 호스트 시스템에 의한 상기 저장 매체에 대한 후속 액세스에 관련된 호스트 시스템 행동의 예측을 상기 인 공 지능 엔진으로부터 수신하고; 및 상기 호스트 시스템 행동의 예측에 적어도 일부 기초하여, 상기 장치의 저장 매체에 액세스하기 위한 호스트 I/O를 스케줄링한다. 실시예 12: 제11항에 있어서, 상기 매체 액세스 관리자는 또한, 상기 장치가 상기 저장 매체에 액세스하기 위한 보류중인(pending) 내부 I/O를 가지고 있는지를 결정하고; 상기 호스트 시스템 행동의 예측에 기초하여, 상기 장치의 저장 매체에 액세스하기 위해 상기 호스트 시스템의 호스트 I/O 및 상기 장치의 내부 I/O를 스케줄링한다. 실시예 13: 제11항에 있어서, 상기 인공 지능 엔진으로부터 수신된 호스트 시스템 행동의 예측은, 상기 호스트 시스템이 유휴 상태가 될 때까지의 시간 기간, 상기 호스트 시스템이 유휴 상태로 유지되는 시간 기간, 또는 상기 호스트 시스템에 의해 발행된 다음 호스트 I/O에 관한 파라미터들 중 적어도 하나에 대한 표시 를 포함하고, 상기 호스트 I/O를 설명하는 정보는, 적어도 하나의 호스트 I/O에 대한, 호스트 I/O의 이벤트 유형에 대한 표시, 호스트 I/O의 이벤트 기간에 대한 표시, 또는 호스트 I/O와 관련된 데이터의 이벤트 크기에 대한 표시를 포함한다. 실시예 14: 제11항에 있어서, 상기 인공 지능 엔진은 다수의 인공 지능 모델들을 실행하고, 상기 다수의 인공 지능 모델들 중 적어도 2 개는 상기 장치의 플래시 변환 계층 또는 디바이스 레벨 관리자에 의해 구현되는 각각의 내부 작업들과 관련되며; 그 리고 상기 매체 액세스 관리자는 또한, 상기 정보를 인공 지능 엔진에 제공하기 전에, 호스트 시스템 행동의 예측이 가능하도록 상기 다수의 인공 지능 모델들 중 적어도 하나를 상기 인공 지능 엔진에 로딩한다. 실시예 15: 제11항에 있어서, 상기 매체 액세스 관리자는 또한, 상기 인공 지능 엔진으로 하여금, 상기 장치의 적어도 2개의 인공 지능 보조 내부 작업들을 구현하도록, 상기 다수의 인공 지능 모델들 중 적어도 2개를 동시에 실행하게 하거나; 또는 상기 인공 지능 엔진으로 하여금, 상기 인공 지능 모델의 온라인 재-트레이닝 또는 개량이 가능하도록, 상기 다 수의 인공 지능 모델들 중 한 모델의 2개의 인스턴스들을 병렬로 실행하게 한다. 실시예 16: 시스템 온 칩(SoC)으로서, 저장 시스템의 저장 매체에 액세스하기 위한 매체 인터페이스; 호스트 시스템과 통신하는 호스트 인터페이스; 인공 지능 엔진; 하드웨어 기반 프로세서; 프로세서 실행가능 명령들을 저장하는 메모리를 포함하고, 상기 명령들은 상기 하드웨어 기반 프로세서에 의한 실행에 응답하여, 매체 액세스 관리자를 구현하고, 상기 매체 액세스 관리자는, 상기 호스트 인터페이스를 통해, 상기 저장 시스템의 저장 매체에 액세스하기 위한 호스트 입/출력(I/O)을 상기 호스트 시스템으로부터 수신하고; 상기 호스트 시스템으로부터 수신된 호스트 I/O를 설명하는 정보를 상기 인공 지능 엔진에 제공하고; 상기 호스트 시스템에 의한 상기 저장 매체에 대한 후속 액세스에 관련된 호스트 시스템 행동의 예측을 상기 인 공 지능 엔진으로부터 수신하고; 및 상기 호스트 시스템 행동의 예측에 적어도 일부 기초하여, 상기 저장 시스템의 저장 매체에 액세스하기 위한 호 스트 I/O를 스케줄링한다. 실시예 17: 제16항에 있어서, 상기 매체 액세스 관리자는 또한, 상기 SoC가 상기 저장 매체에 액세스하기 위한 보류중인(pending) 내부 I/O를 가지고 있는지를 결정하고; 상기 호스트 시스템 행동의 예측에 기초하여, 상기 SoC의 저장 매체에 액세스하기 위해 상기 호스트 시스템의 호스트 I/O 및 상기 SoC의 내부 I/O를 스케줄링하며, 상기 내부 I/O를 스케줄링하는 것은, 상기 내부 I/O와 호스트 I/O 사이의 경합을 완화시키도록 상기 SoC의 내부 I/O를 속행(advancing) 또는 지연시킨다. 실시예 18: 제16항에 있어서, 상기 인공 지능 엔진으로부터 수신된 호스트 시스템 행동의 예측은, 상기 호스트 시스템이 유휴 상태가 될 때까지의 시간 기간, 상기 호스트 시스템이 유휴 상태로 유지되는 시간 기간, 또는 상기 호스트 시스템에 의해 발행된 다음 호스트 I/O에 관한 파라미터들 중 적어도 하나에 대한 표시 를 포함하고, 상기 호스트 I/O를 설명하는 정보는, 적어도 하나의 호스트 I/O에 대한, 호스트 I/O의 이벤트 유형에 대한 표시, 호스트 I/O의 이벤트 기간에 대한 표시, 또는 호스트 I/O와 관련된 데이터의 이벤트 크기에 대한 표시를 포함한다. 실시예 19: 제16항에 있어서, 상기 매체 액세스 관리자는 또한, 상기 호스트 시스템 행동의 예측 및 상기 저장 매체의 온도 관리를 위한 저장 매체의 디바이스 레벨 파라미터들 에 기초하여 호스트 I/O를 스케줄링한다. 실시예 20: 제16항에 있어서, 상기 인공 지능 엔진은 다수의 인공 지능 모델들을 실행하고, 상기 다수의 인공 지능 모델들 중 적어도 2 개는 상기 SoC의 플래시 변환 계층 또는 디바이스 레벨 관리자에 의해 구현되는 각각의 내부 작업들과 관련되며; 그 리고 상기 매체 액세스 관리자는 또한, 상기 인공 지능 엔진으로 하여금, 상기 SoC의 적어도 2개의 인공 지능 보조 내부 작업들을 구현하도록, 상기 다 수의 인공 지능 모델들 중 적어도 2개를 동시에 실행하게 하거나; 또는 상기 인공 지능 엔진으로 하여금, 상기 인공 지능 모델의 온라인 재-트레이닝 또는 개량이 가능하도록, 상기 다 수의 인공 지능 모델들 중 한 모델의 2개의 인스턴스들을 병렬로 실행하게 한다. 도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17 도면18 도면19 도면20"}
{"patent_id": "10-2019-0136475", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 하나 이상의 양상들에 따라 AI 기반의 저장 매체 컨트롤러가 구현되는 디바이스들을 갖는 예 시적인 동작 환경을 도시한다. 도 2는 도 1에 도시된 저장 컨트롤러의 매체 액세스 관리자 및 AI 엔진의 예시적인 구성을 도시한다. 도 3은 저장 매체 컨트롤러의 AI 엔진을 구현하기 위한 다양한 하드웨어 및 펌웨어 구성요소의 예시적인 구성을 도시한다. 도 4는 다수의 AI 모델들을 구현하기 위한 AI 엔진 및 영구 메모리의 예시적인 구성을 도시한다. 도 5는 저장 매체 액세스의 AI 기반 관리의 양상들이 구현될 수 있는 캐시 메모리 및 저장 메모리의 예시적인 구성을 도시한다. 도 6은 저장 시스템 컨트롤러의 AI 엔진으로 구현된 예측형 가비지 콜렉션의 예를 도시한다. 도 7은 본 개시의 하나 이상의 양상에 따라 AI 엔진에 의해 스케쥴링된 내부 I/O 동작의 예를 도시한다. 도 8은 본 개시의 하나 이상의 양상에 따라 호스트 시스템 행동을 예측하는 AI 모델에 유용한 예시적인 호스트 I/O 이벤트 유형을 도시한다. 도 9는 호스트 I/O 행동의 다양한 측면을 예측하도록 구성된 AI 모델의 예시적인 구현을 도시한다. 도 10은 다양한 I/O 또는 유휴 시간을 포함하는 예측된 호스트 행동의 예를 나타낸다. 도 11은 호스트 행동의 멀티-스테이지 예측을 지원하는 AI 모델의 예시적인 구현을 도시한다. 도 12는 이벤트 확률에 기초하여 예측된 호스트 행동의 경로를 결정하는데 유용한 빔 검색의 예를 도시한다. 도 13은 AI 모델의 온라인 재트레이닝 또는 온라인 개량을 위한 예시적인 동작들의 타임라인을 도시한다.도 14는 본 개시의 하나 이상의 양상에 따라 저장 매체 액세스의 AI 가능 관리를 위한 예시적인 방법을 도시한 다. 도 15는 호스트 시스템 행동의 예측에 기초하여 저장 시스템의 내부 동작을 지연시키기위한 예시적인 방법을 도 시한다. 도 16은 호스트 시스템 행동의 예측에 기초하여 저장 시스템의 내부 동작을 속행시키기위한 예시적인 방법을 도 시한다. 도 17은 호스트 시스템 행동의 예측에 기초하여 디바이스 레벨 관리를 위한 임계값을 변경하는 예시적인 방법을 도시한다. 도 18은 온라인 재트레이닝 또는 개량을 가능하게하기 위해 다수의 AI 모델 인스턴스들로 추론을 실행하는 예시 적인 방법을 도시한다. 도 19 는 저장 매체 액세스의 AI 가능 관리의 양상들의 구현될 수 있는 예시적인 SoC(System-on-Chip) 환경을 도시한다. 도 20 은 본 개시의 하나 이상의 양상에 따라 AI 엔진이 구현되는 예시적인 저장 시스템 컨트롤러를 도시한다."}
