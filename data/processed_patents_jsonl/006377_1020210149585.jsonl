{"patent_id": "10-2021-0149585", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0064659", "출원번호": "10-2021-0149585", "발명의 명칭": "반려동물 안면인식을 이용한 AR 이미지 합성 방법 및 장치", "출원인": "주식회사 루씨드드림", "발명자": "김창동"}}
{"patent_id": "10-2021-0149585", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "카메라로부터 영상 데이터를 획득하는 영상 획득부;상기 영상 데이터 내에서 객체를 인지하여 상기 객체가 사람인지 반려동물인지의 여부를 구분하는 객체 판별부;상기 객체가 상기 반려동물로 확인되면, 상기 반려동물의 얼굴 영역에 반려동물 얼굴 바운딩 박스를 생성하고,상기 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴 특징값을 추출하는 특징값 추출부;선택받은 AR 스티커 데이터를 로딩하는 스티커 로딩부;상기 AR 스티커 데이터에 대응하는 스티커 메타데이터를 추출하고, 상기 스티커 메타데이터를 기준으로 상기 반려동물 얼굴 특징값에 대응하는 위치값과 회전값을 산출하는 스티커 매칭부; 및상기 위치값과 상기 회전값을 기반으로 상기 AR 스티커를 상기 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴레이어(Layer)로 합성하는 스티커 합성부를 포함하는 것을 특징으로 하는 AR 이미지 합성 장치."}
{"patent_id": "10-2021-0149585", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 객체 판별부는상기 객체의 눈 위치, 코 위치, 입 위치를 확인하고, 상기 눈 위치, 상기 코 위치, 상기 입 위치를 수치화한 값을 산출하고, CNN(Convolution Neural Network)을 이용하여 기 학습된 인공지능 모델을 이용하여 상기 눈 위치,상기 코 위치, 상기 입 위치를 수치화한 값을 기반으로 상기 객체가 상기 사람인지 상기 반려동물인지를 판별하는 것을 특징으로 하는 AR 이미지 합성 장치."}
{"patent_id": "10-2021-0149585", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 객체 판별부는상기 영상 데이터에 SSD(Single Shot Detection)을 이용하여 상기 객체의 얼굴 영역을 인식하는 것을 특징으로하는 AR 이미지 합성 장치."}
{"patent_id": "10-2021-0149585", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 특징값 추출부;상기 객체가 상기 사람으로 확인되면, 상기 사람의 얼굴 영역에 사람 얼굴 바운딩 박스를 생성하고, 기구축된라이브러리를 이용하여 상기 사람 얼굴 바운딩 박스 내의 사람 얼굴 특징값을 추출하는 것을 특징으로 하는 AR이미지 합성 장치."}
{"patent_id": "10-2021-0149585", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 스티커 합성부는,상기 AR 스티커 데이터가 애니메이션 스티커인 경우, 상기 영상 데이터에 대응하는 프레임별로 상기 AR 스티커데이터를 각각 결합하여 인코딩하는 것을 특징으로 하는 AR 이미지 합성 장치.공개특허 10-2023-0064659-3-"}
{"patent_id": "10-2021-0149585", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "반려동물 안면인식을 이용한 AR 이미지 합성 방법 및 장치를 개시한다. 본 실시예는 촬영된 영상에서 반려동물의 얼굴의 위치만 확인한 후 영역을 구분하고, 해당 영역 내의 반려동물의 얼굴로부터 특징을 추출하고, 추출된 특징을 이미지 내의 포인트 데이터로 처리한 후 특징 추출된 위치를 전체 이미지 내에서의 위치로 재조정하여 스티커를 합성하도록 하는 반려동물 안면인식을 이용한 AR 이미지 합성 방법 및 장치를 제공한다."}
{"patent_id": "10-2021-0149585", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명의 일 실시예는 반려동물 안면인식을 이용한 AR 이미지 합성 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2021-0149585", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이하에 기술되는 내용은 단순히 본 실시예와 관련되는 배경 정보만을 제공할 뿐 종래기술을 구성하는 것이 아니 다. 인간은 신분을 검증하고 확인하는 방법으로 생체인증(Biometrics) 분야를 사용하고 있고, 이는 이미 오랜 기간 동안 발전해왔다. 생체인증 방식은 지문인식, 홍채인식, 정맥인식(Vein Recognition) 등이 존재한다. 인간의 지문은 사람마다 모두 다르듯이, 개과에 속하는 동물들의 비문(코 무늬)은 동물마다 각각 다르다. 동물 들의 비문을 이용하여 동물의 종 확인 또는 주인이 찾고자 하는 동물을 확인할 수 있는 방법은 동물의 체계적인 관리를 가능하게 할 수 있다. 전술한 동물의 비문을 채취하기 위해서 사람의 지문을 채취한 것과 동일한 방식을 이용하였다. 전술한 동물의 비문은 동물 코 부위를 닦아서 이물질을 제거하고, 그 부위를 잉크를 묻혀서 종이로 탁본을 뜨는 단계를 거쳐서 획득했다. 다만, 동물의 비문은 사용자의 숙련도에 따라 잉크가 번지거나 코 부위를 누르는 압력이 달라져 탁본 이미지가 달라졌을 뿐만 아니라, 동일한 사람에 의해 수행되더라도 탁본을 뜨는 방향에 따라 매번 다른 탁본 이미지가 발 생되는 문제점이 있었다. 따라서, 동물의 얼굴을 빠르게 인식하고 영상을 합성하는 기술을 필요로 한다."}
{"patent_id": "10-2021-0149585", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 실시예는 촬영된 영상에서 반려동물의 얼굴의 위치만 확인한 후 영역을 구분하고, 해당 영역 내의 반려동물 의 얼굴로부터 특징을 추출하고, 추출된 특징을 이미지 내의 포인트 데이터로 처리한 후 특징 추출된 위치를 전 체 이미지 내에서의 위치로 재조정하여 스티커를 합성하도록 하는 반려동물 안면인식을 이용한 AR 이미지 합성 방법 및 장치를 제공하는 데 목적이 있다."}
{"patent_id": "10-2021-0149585", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 실시예의 일 측면에 의하면, 카메라로부터 영상 데이터를 획득하는 영상 획득부; 상기 영상 데이터 내에서 객체를 인지하여 상기 객체가 사람인지 반려동물인지의 여부를 구분하는 객체 판별부; 상기 객체가 상기 반려동 물로 확인되면, 상기 반려동물의 얼굴 영역에 반려동물 얼굴 바운딩 박스를 생성하고, 상기 반려동물 얼굴 바운 딩 박스 내의 반려동물 얼굴 특징값을 추출하는 특징값 추출부; 선택받은 AR 스티커 데이터를 로딩하는 스티커 로딩부; 상기 AR 스티커 데이터에 대응하는 스티커 메타데이터를 추출하고, 상기 스티커 메타데이터를 기준으로 상기 반려동물 얼굴 특징값에 대응하는 위치값과 회전값을 산출하는 스티커 매칭부; 및 상기 위치값과 상기 회 전값을 기반으로 상기 AR 스티커를 상기 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴 레이어(Layer)로 합성 하는 스티커 합성부를 포함하는 것을 특징으로 하는 AR 이미지 합성 장치를 제공한다."}
{"patent_id": "10-2021-0149585", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상에서 설명한 바와 같이 본 실시예에 의하면, 촬영된 영상에서 반려동물의 얼굴의 위치만 확인한 후 영역을 구분하고, 해당 영역 내의 반려동물의 얼굴로부터 특징을 추출하고, 추출된 특징을 이미지 내의 포인트 데이터 로 처리한 후 특징 추출된 위치를 전체 이미지 내에서의 위치로 재조정하여 스티커를 합성하여 출력하는 효과가 있다."}
{"patent_id": "10-2021-0149585", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 실시예를 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 본 실시예에 따른 반려동물 안면인식을 이용한 AR 이미지 합성 시스템을 나타낸 도면이다. 본 실시예에 따른 AR 이미지 합성 시스템은 단말기, 반려동물 AR 합성 애플리케이션를 포함한다. AR 이미지 합성 시스템에 포함된 구성요소는 반드시 이에 한정되는 것은 아니다. 단말기는 사용자의 키 조작에 따라 네트워크를 경유하여 데이터 통신을 수행하는 전자 기기를 의미한다. 단말기는 스마트폰(Smart Phone), 태블릿(Tablet), 랩톱(Laptop), 개인용 컴퓨터(PC: Personal Computer), 개인 휴대 단말기(PDA: Personal Digital Assistant), 휴대형 멀티미디어 플레이어(PMP: Portable Multimedia Player), 무선 통신 단말기(Wireless Communication Terminal), 미디어 플레이어 등과 같은 전자 기기일 수 있다. 단말기는 각종 기기 또는 유무선 네트워크와 통신을 수행하기 위한 통신 모뎀 등의 통신 장치, 각종 프로 그램과 데이터를 저장하기 위한 메모리, 프로그램을 실행하여 연산 및 제어하기 위한 마이크로프로세서 등을 구 비하는 다양한 장치이다. 적어도 일 실시예에 따르면, 메모리는 램(Random Access Memory: RAM), 롬(Read Only Memory: ROM), 플래시 메모리, 광 디스크, 자기 디스크, 솔리드 스테이트 디스크(Solid State Disk: SSD) 등의 컴퓨터로 판독 가능한 기록/저장매체일 수 있다. 적어도 일 실시예에 따르면, 마이크로프로세서는 명세서상에 기재된 동작과 기능을 하나 이상 선택적으로 수행하도록 프로그램될 수 있다. 적어도 일 실시예에 따르면, 마이 크로프로세서는 전체 또는 부분적으로 특정한 구성의 주문형반도체(Application Specific Integrated Circuit: ASIC) 등의 하드웨어로써 구현될 수 있다. 본 실시예에 따른 반려동물 AR 합성 애플리케이션은 단말기에 구비된 카메라로부터 영상 데이터를 획 득한다. 반려동물 AR 합성 애플리케이션은 획득한 영상 데이터 내에서 객체를 인지하여 객체가 사람인지 반려동물인지의 여부를 구분한다. 반려동물 AR 합성 애플리케이션은 객체가 반려동물로 확인되면, 반려동 물의 얼굴 영역에 반려동물 얼굴 바운딩 박스를 생성하고, 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴 특징값을 추출한다. 반려동물 AR 합성 애플리케이션은 사용자의 조작 또는 명령에 의해 선택받은 AR 스티커 데이터를 로딩한다. 반려동물 AR 합성 애플리케이션은 AR 스티커 데이터에 대응하는 스티커 메타데이터를 추출하고, 스티커 메타데이터를 기준으로 반려동물 얼굴 특징값에 대응하는 위치값과 회전값을 산출한다. 반려동물 AR 합 성 애플리케이션은 위치값과 회전값을 기반으로 AR 스티커를 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴 레이어(Layer)로 합성한다. 도 2는 본 실시예에 따른 반려동물 안면인식을 이용한 AR 이미지 합성 장치를 나타낸 도면이다. 본 실시예에 따른 반려동물 AR 합성 애플리케이션은 하드웨어와 결합하거나 연동하여 AR 이미지 합성 장치 로 구현 가능하다. 본 실시예에 따른 AR 이미지 합성 장치는 영상 획득부, 객체 판별부, 특징값 추출부, 반려 동물 상태 확인부, 스티커 로딩부, 스티커 매칭부, 스티커 합성부, 영역 분할부, 움 직임 인지부, 이동성 판별부, 운동량 판별부를 포함한다. AR 이미지 합성 장치에 포함된 구성요소는 반드시 이에 한정되는 것은 아니다. AR 이미지 합성 장치에 포함된 각 구성요소는 장치 내부의 소프트웨어적인 모듈 또는 하드웨어적인 모듈을 연결하는 통신 경로에 연결되어 상호 간에 유기적으로 동작할 수 있다. 이러한 구성요소는 하나 이상의 통신 버 스 또는 신호선을 이용하여 통신한다. 도 2에 도시된 AR 이미지 합성 장치의 각 구성요소는 적어도 하나의 기능이나 동작을 처리하는 단위를 의 미하며, 소프트웨어적인 모듈, 하드웨어적인 모듈 또는 소프트웨어와 하드웨어의 결합으로 구현될 수 있다. 영상 획득부는 단말기에 탑재된 카메라로부터 영상 데이터를 획득한다. 객체 판별부는 영상 데이터 내에서 객체를 인지하여 객체가 사람인지 반려동물인지의 여부를 구분한다. 객체 판별부는 영상 데이터 내에서 얼굴 영역을 식별한 후 사람 또는 동물인지 판단한다. 객체 판별부 는 얼굴 영역에서 사람 또는 동물인지를 판단할 때 CNN(Convolution Neural Network)을 이용하여 분류 (Classifiaction)를 수행한다. 객체 판별부는 얼굴 영역의 특징(눈,코,입의 위치)을 콘벌루션 (Convolution)하여 수치화 한 후 학습된 인공지능 모델을 이용하여 수치화값을 기반으로 사람 또는 동물을 판단 한다. 객체 판별부는 동물로 판단된 경우도 콘벌루션을 이용한 수치 데이터를 기준으로 학습된 데이터를 기반으로 학습을 시켜서 3가지 점(눈,코,입)의 위치를 추출한다. 객체 판별부는 객체의 눈 위치, 코 위치, 입 위치를 확인하고, 눈 위치, 코 위치, 입 위치를 수치화한 값 을 산출한다. 객체 판별부는 CNN(Convolution Neural Network)을 이용하여 기 학습된 인공지능 모델을 이 용하여 눈 위치, 코 위치, 입 위치를 수치화한 값을 기반으로 객체가 사람인지 반려동물인지를 판별한다. 객체 판별부는 2차적으로 영상 데이터에 SSD(Single Shot Detection)을 이용하여 객체의 얼굴 영역을 인식 한다. 객체 판별부는 객체 윤곽을 기반으로 기 학습된 객체와 비교하여 객체가 반려동물인지를 확인한다. 객체 판별부는 객체 윤곽 및 객체의 눈과 코를 기반으로 기 학습된 객체와 비교하여 반려동물 종류, 반려동물 암수 정보, 반려동물 사이즈, 반려동물 몸무게를 예측한다. 객체 판별부는 반려동물 종류를 기반으로 반려 동물이 소형, 중형 또는 대형인지를 판단한 1차 판단 결과를 생성한다. 객체 판별부는 1차 판단 결과에 반 려동물 사이즈와 반려동물 몸무게를 추가로 반영하여 반려동물이 소형, 중형 또는 대형인지를 2차적으로 판단한 2차 판단 결과를 생성한다. 객체 판별부는 2차 판단 결과를 기반으로 반려동물이 소형으로 판단되면 스티 커 로딩부로 하여금 소형견 스티커가 로딩되도록 한다. 객체 판별부는 2차 판단 결과를 기반으로 반려동물 이 중형으로 판단되면 스티커 로딩부로 하여금 중형견 스티커가 로딩되도록 한다. 객체 판별부는 2차 판단 결과를 기반으로 반려동물이 대형으로 판단되면 스티커 로딩부로 하여금 대형견 스티커가 로딩되도록 한다. 객체 판별부는 2차 판단 결과를 기반으로 레퍼런스 반려동물 종류 대비 마른 정도, 비만 정도, 영양상태정 보를 예측하고, 스티커 로딩부로 하여금 레퍼런스 반려동물 종류 대비 마른 정도, 비만 정도, 영양상태정 보 중 어느 하나 이상을 포함하는 스티커가 후보 스티커로 로딩되도록 한다. 객체 판별부는 반려동물의 머리 부분, 다리 부분, 꼬리 부분을 구분하여 인지한다. 객체 판별부는 영상 데이터가 동영상 데이터인 경우, 동영상 데이터의 전처리(preprocessing)를 수행한 전 처리 데이터를 생성한다. 객체 판별부는 전처리 데이터 내의 각 픽셀의 결함(Defect)을 보정하고, 렌즈의 가장자리에서의 왜곡을 보정하기 위한 렌즈 세이딩(lens shading)을 수행한 보정 데이터를 생성한다. 객체 판별 부는 보정 데이터의 소정 비율로 이미지 크기를 줄이는 다운 스케일링을 수행한 다운 스케일링 데이터를 생성한다. 객체 판별부는 다운 스케일링 데이터를 각 픽셀을 주변 픽셀값을 이용하여 보간(interpolatio n)을 수생한 보간 데이터를 생성한다. 객체 판별부는 보간 데이터에 대해서 RGB 감마 보정(gammacorrection)을 수행한 RGB 감마 데이터를 생성한다. 객체 판별부는 RGB 감마 데이터에 대한 휘도(Y)-색차 (Chroma) 영역의 데이터로 칼라 영역 변환(Color space conversion)을 수행한 YC 변환 데이터를 생성한다. 객 체 판별부는 YC 변환 데이터의 잡음을 줄이기 위한 필터링을 수행한 노이즈 필터 데이터를 생성한다. 객체 판별부는 노이즈 필터 데이터로부터 반려동물을 검출한다. 객체 판별부는 반려동물의 머리 부분, 다 리 부분, 꼬리 부분을 구분하여 인지한다. 특징값 추출부는 객체가 반려동물로 확인되면, 반려동물의 얼굴 영역에 반려동물 얼굴 바운딩 박스를 생성한다. 특징값 추출부는 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴 특징값을 추출한다. 특징값 추출부는 객체가 사람으로 확인되면, 사람의 얼굴 영역에 사람 얼굴 바운딩 박스를 생성하고, 기구축된 라이브러리를 이용하여 사람 얼굴 바운딩 박스 내의 사람 얼굴 특징값을 추출한다. 특징값 추출부는 눈 위치와 코 위치의 거리차와 얼굴 크기에 매칭하여 반려동물의 닉네임을 등록하여 저장 하고 있다가, 새로운 영상 데이터 내에서 객체를 인지하여 객체가 눈 위치와 코 위치의 거리차와 얼굴 크기가 동일한 경우 스티커 로딩부로 하여금 닉네임 스티커가 로딩되도록 한다. 반려동물 상태 확인부는 영상 데이터로부터 눈부분 영상, 눈주변부분 영상, 코 영상, 코 주변 영상을 추출 한다. 반려동물 상태 확인부는 눈부분 영상, 눈주변부분 영상, 코 영상, 코 주변 영상을 각각 라벨링한 눈 부분 라벨링 영상, 눈주변부분 라벨링 영상, 코 라벨링 영상, 코 주변 라벨링 영상을 생성한다. 반려동물 상태 확인부는 눈부분 라벨링 영상, 눈주변부분 라벨링 영상, 코 라벨링 영상, 코 주변 라벨링 영상을 딥러닝 기반의 신경망 모델에 입력하여 눈부분 라벨링 영상에 대한 홍채 영상을 생성하고, 눈주변부분 라벨링 영상에 대한 눈주변 특징 부분 영상을 생성하고, 코 라벨링 영상에 대한 코 특징 영상을 생성하고, 코 주변부분 라벨링 영상에 대한 코 주변 특징 부분 영상을 생성한다. 반려동물 상태 확인부는 홍채 영상, 눈주변 특징 부분 영상, 코 특징 영상, 코 주변 특징 부분 영상 각각 의 크기를 정규화시킨 후 부호화한다. 반려동물 상태 확인부는 부호화한 홍채 영상, 눈주변 특징 부분 영 상, 코 특징 영상, 코주변 특징 부분 영상 각각을 기저장된 영상들과 비교하여 특정 동물로 인식하거나 반려동 물의 질병을 확인한다. 반려동물 상태 확인부는 홍채 영상, 눈주변 특징 부분 영상을 기저장된 레퍼런스 영상과 비교하여 눈물 분 비량을 확인한다. 반려동물 상태 확인부는 눈물 분비량이 기 설정된 임계치를 초과하는 경우, 결막염으로 판정하여 스티커 로딩부로 하여금 결막염 감염 상태 스티커가 로딩되도록 한다. 반려동물 상태 확인부는 코 특징 영상, 코주변 특징 부분을 기저장된 레퍼런스 영상과 비교하여 코의 콧물 량을 확인한다. 반려동물 상태 확인부는 콧물량이 기 설정된 임계치를 초과하는 경우, 비염으로 판정하며, 콧물량이 기 설정된 임계치 이하인 경우 코가 건조한 것으로 판정하여 스티커 로딩부로 하여금 비염 스티 커 또는 코 건조 상태 스티커가 로딩되도록 한다. 반려동물 상태 확인부는 눈부분 영상, 눈주변부분 영상, 코 영상, 코 주변 영상 각각을 기 학습된 감정 상 태와 비교하여 반려동물의 감정상태를 편안(Relaxed), 긴장(Nervous), 관심(Interested), 행복(Happy), 스트레 스(Stressed) 중 어느 하나로 예측한다. 반려동물 상태 확인부는 스티커 로딩부로 하여금 반려동물의 감정상태를 나타내는 스티커가 로딩되도록 하며, 반려동물의 감정상태를 나타내는 스티커에 감정 상태에 대응하 는 색깔이 표현되도록 한다. 반려동물 상태 확인부는 영상 데이터에서 관심 영역(ROI, Region Of Interest)을 설정하고, 관심 영역 (ROI)에 포함된 노이즈 및 반사광을 제거한다. 반려동물 상태 확인부는 관심 영역(ROI) 상에서 노이즈를 제거하고, 관심 영역(ROI) 상에서 반사광 영역에 위치하는 픽셀들(pixels)의 명도(intensity) 값을 확인하여 최 대 명도 값으로 수렴되는 적어도 하나의 픽셀을 추출한다. 반려동물 상태 확인부는 최대 명도 값으로 수렴 되는 적어도 하나의 픽셀에 의해 추출되는 개체 특징점을 제거하여 관심 영역에 대한 개체 특징점을 추출한 후 코 특징 영상, 코주변 특징 부분 영상에서 조직편을 분석하여 코의 건조 상태를 확인한다. 반려동물 상태 확인부는 코 특징 영상, 코주변 특징 부분 영상으로부터 골곡 부분과 솟아오른 부분 을 추출한다. 반려동물 상태 확인부는 골곡 부분과 솟아오른 부분에서 조직편을 분석한 결과를 기 저장된 영상과 비교하여 코의 건조 여부를 확인한다. 스티커 로딩부는 선택받은 AR 스티커 데이터를 로딩한다. 스티커 매칭부는 AR 스티커 데이터에 대응하는 스티커 메타데이터를 추출한다. 스티커 매칭부는 스티 커 메타데이터를 기준으로 반려동물 얼굴 특징값에 대응하는 위치값과 회전값을 산출한다. 스티커 매칭부는 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴 내의 양안의 중심점을 확인한다. 스 티커 매칭부는 양안의 중심점을 통과하는 직선을 기반으로 반려동물 얼굴 내의 코의 중심점을 확인한다. 스티커 매칭부는 양안의 중심점과 양안 거리를 이용한 얼굴 크기를 계산한다. 스티커 매칭부는 얼굴 크기 내에서 양안의 중심점을 기반으로 양안의 위치값을 산출한다. 스티커 매칭부는 얼굴 크기 내에서 코 의 중심점을 기반으로 코의 위치값을 산출한다. 스티커 매칭부는 가상의 수평선을 기준으로 양안의 위치값까지의 각도를 확인한 후 양안의 위치값까지의 각도를 기반으로 눈의 회전 각도를 계산한다. 스티커 매칭부는 가상의 수직선을 기준으로 코의 위치값까지의 각도를 확인한 후 코의 위치값으로부터 양 안의 중심점을 통과하는 직선이 중심 코의 위치값까지의 각도까지의 코의 회전 각도를 계산한다. 스티커 매칭부는 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴에 AR 스티커를 눈의 회전 각도까지 회전 시킨 후 양안의 위치값에 매칭한다. 스티커 매칭부는 AR 스티커를 코의 회전 각도까지 회전시킨 후 코의 위치값에 매칭한다. 스티커 매칭부는 움직임 감지 영역에 따라 반려동물이 움직이는 것이 아닌 꼬리만 움직이는 것으로 인지하면, 반려동물의 기분이 좋은 것으로 판별하여 기분 좋음을 나타내는 스티커를 선별하여 매칭한다. 스티커 합성부는 위치값과 회전값을 기반으로 AR 스티커를 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴 레이어(Layer)로 합성한다. 스티커 합성부는 AR 스티커 데이터가 애니메이션 스티커인 경우, 영상 데이터 에 대응하는 프레임별로 AR 스티커 데이터를 각각 결합하여 인코딩한다. 영역 분할부는 영상 데이터를 복수의 분할 영역으로 분할한다. 움직임 인지부는 분할 영역마다 반려동물의 신체 부위를 감지한다. 움직임 인지부는 반려동물의 신체 부위가 존재하는 영역을 움직임 감지 영역과 직전까지 움직임 감지 영역으로 구분하여 인지한다. 움직임 인지부는 반려동물의 머리 부분, 다리 부분, 꼬리 부분을 구분하여 각 부위의 움직임을 인지한다. 이동성 판별부는 반려동물의 이동성이 확인되면, 이동성을 기반으로 움직임 방향을 산출한다. 이동성 판별 부는 움직임 감지 영역이 판별되면, 한 개 이상의 직전까지 움직임 감지 영역을 한 개 이상의 움직임 감지 영역으로 매칭시켜서, 최초 움직임을 감지하고, 다시 감지되는 움직임 감지 영역에 따 라 움직임 방향을 산출하며, 스티커 로딩부로 하여금 움직임 방향 스티커가 로딩되도록 한다. 운동량 판별부 반려동물의 이동성을 운동량으로 환산한다. 운동량 판별부는 움직임 감지 영역이 직전까지 움직임 감지 영역으로 전환된 후 직전까지 움 직임 감지 영역의 유지 시간에 따라 운동량을 낮은 상태로 산출하며, 스티커 로딩부로 하여금 운동 량 낮은 스티커가 로딩되도록 한다. 운동량 판별부는 직전까지 움직임 감지 영역이 움직임 감지 영 역으로 전환된 후 움직임 감지 영역의 유지 시간에 따라 운동량을 높은 상태로 산출하며, 스티커 로딩부로 하여금 운동량 높은 스티커가 로딩되도록 한다. 이동성 판별부는 움직임 감지 영역이 머리 부분 또는 다리 부분으로 판별되면, 머리 부분 또는 다리 부분에 해당하는 직전까지 움직임 감지 영역을 한 개 이상의 움직임 감지 영역으로 매칭시켜서, 최 초 움직임을 감지하고, 다시 감지되는 머리 부분 또는 다리 부분에 해당하는 움직임 감지 영역에 따라 움 직임 방향을 산출한다. 운동량 판별부는 움직임 감지 영역이 꼬리 부분으로 판별되면, 꼬리 부분에 해당하는 직전까지 움직 임 감지 영역을 한 개 이상의 움직임 감지 영역으로 매칭시켜서, 최초 움직임을 감지하고, 다시 감 지되는 꼬리 부분에 해당하는 움직임 감지 영역에 따라 반려동물이 움직이는 것이 아닌 꼬리만 움직이는 것으로 인지(예컨대, 기분 좋음)하여 운동량을 낮은 상태로 환산하며, 스티커 로딩부로 하여금 기분 좋음 스티커가 로딩되도록 한다. 도 3은 본 실시예에 따른 반려동물 안면인식을 이용한 AR 이미지 합성 방법을 설명하기 위한 순서도이다. 반려동물 AR 합성 애플리케이션은 단말기에 탑재된 카메라를 이용하여 객체를 촬영한 영상 데이터를 생성한다(S310). 반려동물 AR 합성 애플리케이션은 영상 데이터 내에서 객체를 인식하여, 해당 객체가 사람인지, 반려동물 인지를 구분한다(S320). 반려동물 AR 합성 애플리케이션은 각 객체의 눈코입의 위치를 확인하고, 눈코입의 거리차와 위치에 따라 사람 또는 반려동물인지의 여부를 확인한다. 반려동물 AR 합성 애플리케이션은 영상 데이터 내에서 객체가 반려동물로 인식되면, 반려동물의 안면을 인식하여 특징점을 추출한다(S330). 반려동물 AR 합성 애플리케이션은 반려동물의 안면의 특징점에 스티커를 AR 방식으로 매칭하여 출력한다 (S340). 반려동물 AR 합성 애플리케이션은 반려동물의 안면의 특징점에 AR 방식으로 매칭된 스티커를 사진 또는 동영상 형태로 저장한다(S350). 도 3에서는 단계 S310 내지 단계 S350을 순차적으로 실행하는 것으로 기재하고 있으나, 반드시 이에 한정되는 것은 아니다. 다시 말해, 도 3에 기재된 단계를 변경하여 실행하거나 하나 이상의 단계를 병렬적으로 실행하는 것으로 적용 가능할 것이므로, 도 3은 시계열적인 순서로 한정되는 것은 아니다. 전술한 바와 같이 도 3에 기재된 본 실시예에 따른 반려동물 안면인식을 이용한 AR 이미지 합성 방법은 프로그 램으로 구현되고 컴퓨터로 읽을 수 있는 기록매체에 기록될 수 있다. 본 실시예에 따른 반려동물 안면인식을 이 용한 AR 이미지 합성 방법을 구현하기 위한 프로그램이 기록되고 컴퓨터가 읽을 수 있는 기록매체는 컴퓨터 시 스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 도 4는 본 실시예에 따른 영상 내에서 사람과 반려동물을 구분하여 인식하는 방법을 설명하기 위한 순서도이다. 반려동물 AR 합성 애플리케이션은 영상 데이터에 SSD(Single Shot Detection)을 이용하여 사람, 반려동물 의 얼굴인지를 구분하고, 각 얼굴의 위치를 확인한다(S410). 단계 S410에서, 반려동물 AR 합성 애플리케이션 은 반려동물의 얼굴의 위치만 확인하여 영역을 구분한다. 반려동물 AR 합성 애플리케이션은 반려동물 의 얼굴을 인식할 때, 중복 특징 추출 방지를 위하여 사람의 얼굴을 함께 인식한다. 반려동물 AR 합성 애플리케이션은 기구축된 인공지능 모델을 이용하여 반려동물의 얼굴을 인식한다(S420). 단계 S420에서, 반려동물 AR 합성 애플리케이션은 기구축된 인공지능 모델으로 반려동물의 얼굴을 인식할 수 있다. 반려동물 AR 합성 애플리케이션은 반려동물 얼굴로 구분된 영역의 화상을 재가공한 후 반려동물 얼굴특징 추출 모듈을 이용하여 특징을 추출한다. 반려동물 AR 합성 애플리케이션은 재가공된 반려동물 얼 굴 영상으로부터 양쪽 눈의 위치, 코의 위치를 특징으로 추출한다. 반려동물 AR 합성 애플리케이션은 외부 구축 데이터 베이스를 이용하여 사람 얼굴의 특징을 인식한다 (S430). 반려동물 AR 합성 애플리케이션은 기구축된 라이브러리를 이용하여 사람 얼굴 영상으로부터 사람 의 얼굴 특징을 별도로 추출한다. 도 4에서는 단계 S410 내지 단계 S430을 순차적으로 실행하는 것으로 기재하고 있으나, 반드시 이에 한정되는 것은 아니다. 다시 말해, 도 4에 기재된 단계를 변경하여 실행하거나 하나 이상의 단계를 병렬적으로 실행하는 것으로 적용 가능할 것이므로, 도 4는 시계열적인 순서로 한정되는 것은 아니다. 전술한 바와 같이 도 4에 기재된 본 실시예에 따른 영상 내에서 사람과 반려동물을 구분하여 인식하는 방법은 프로그램으로 구현되고 컴퓨터로 읽을 수 있는 기록매체에 기록될 수 있다. 본 실시예에 따른 영상 내에서 사람 과 반려동물을 구분하여 인식하는 방법을 구현하기 위한 프로그램이 기록되고 컴퓨터가 읽을 수 있는 기록매체 는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 도 5는 본 실시예에 따른 반려동물의 얼굴에 AR 스티커를 매칭하는 방법을 설명하기 위한 순서도이다. 반려동물 AR 합성 애플리케이션은 촬영된 영상 데이터에 SSD(Single Shot Detection)을 이용하여 객체 얼 굴 영역을 인식하고, 객체 얼굴 영역을 기반으로 기 학습된 객체와 비교하여 해당 객체가 반려동물의 안면 인지 의 여부를 인식한다(S510). 반려동물 AR 합성 애플리케이션은 반려동물의 얼굴 영역 내에서 눈, 코, 입의 위치 및 각도를 인식한다 (S520). 반려동물 AR 합성 애플리케이션은 반려동물의 얼굴 영역 내에서 눈, 코, 입의 위치 및 각도를 기 반으로 선택된 AR 스티커를 프레임별로 매칭한다(S530). 도 5에서는 단계 S510 내지 단계 S540을 순차적으로 실행하는 것으로 기재하고 있으나, 반드시 이에 한정되는 것은 아니다. 다시 말해, 도 5에 기재된 단계를 변경하여 실행하거나 하나 이상의 단계를 병렬적으로 실행하는 것으로 적용 가능할 것이므로, 도 5는 시계열적인 순서로 한정되는 것은 아니다.전술한 바와 같이 도 5에 기재된 본 실시예에 따른 반려동물의 얼굴에 AR 스티커를 매칭은 프로그램으로 구현되 고 컴퓨터로 읽을 수 있는 기록매체에 기록될 수 있다. 본 실시예에 따른 반려동물의 얼굴에 AR 스티커를 매칭 을 구현하기 위한 프로그램이 기록되고 컴퓨터가 읽을 수 있는 기록매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 도 6은 본 실시예에 따른 사람과 반려동물의 얼굴을 동시에 인식하는 방법을 설명하기 위한 도면이다. 도 6의 (a)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션은 단말기에 탑재된 카메라를 이용하 여 객체를 촬영한 영상 데이터로부터 사람과 반려동물의 얼굴을 인식한다. 반려동물 AR 합성 애플리케이션은 SSD(Single Shot Detection)를 이용하여 3가지 객체(고양이(Cat), 개 (Dog), 사람(Person))의 얼굴을 인식한 후, 얼굴 위치를 박스(Box) 형태로 인식한다. 반려동물 AR 합성 애플리케이션은 촬영된 영상 데이터에 객체 윤곽을 인식하고, 객체 윤곽을 기반으로 기 학습된 객체(고양이, 개, 사람)와 비교하여 해당 객체가 사람인지 반려동물인지의 여부를 인식한다. 반려동물 AR 합성 애플리케이션은 영상 데이터의 객체가 사람 얼굴로 인지하면 사람 얼굴에 대응하는 위치 에 사람 얼굴 바운딩 박스를 생성한다. 반려동물 AR 합성 애플리케이션은 영상 데이터의 객체가 반려 동물 얼굴로 인지하면 반려동물 얼굴에 대응하는 위치에 반려동물 얼굴 바운딩 박스를 생성한다. 반려동물 AR 합성 애플리케이션은 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴(고양이, 개) 특징 값을 추출한다. 반려동물 AR 합성 애플리케이션은 사람 얼굴 바운딩 박스 내의 사람 얼굴에 대 해서는 별도의 처리를 수행하지 않는다. 반려동물 AR 합성 애플리케이션은 사람 얼굴 바운딩 박스 내 의 사람 얼굴에 대해서 별도의 상용인식 엔진을 이용하여 처리한다. 도 6의 (b),(c),(d),(e)에 도시된 바와 같이 반려동물 얼굴 특징값을 추출한다. 도 6의 (b)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션은 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴로 인식된 부분을 잘라(Crop)낸다. 도 6의 (c)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이 션은 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴로부터 특징값을 추출할 수 있도록 기설정된 사 이즈로 업스케일링한다. 도 6의 (d)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션은 기설정된 사이즈로 업스케일링된 이미 지로부터 반려동물 얼굴 특징값(눈, 코의 위치 및 각도)을 추출한다. 도 6의 (e)에 도시된 바와 같이, 반 려동물 AR 합성 애플리케이션은 기설정된 사이즈로 업스케일링된 이미지를 원본 이미지의 사이즈로 다시 다운스케일링한다. 반려동물 AR 합성 애플리케이션은 다운스케일링한 영상에 맞게 반려동물 얼굴 특징값 에 해당하는 눈, 코의 위치 및 각도를 재계산한다. 도 6의 (f)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션은 영상 데이터 상에 재계산한 눈, 코의 위치 및 각도를 기반으로 포인트 데이터로 처리한다. 반려동물 AR 합성 애플리케이션은 포인트 데이터를 기반으로 반려동물 얼굴 특징값이 추출된 위치를 전체 이미지 내에서의 위치로 재조정하여 AR 스티커를 합 성시 이용한다. 추후 반려동물 AR 합성 애플리케이션은 사람 얼굴 바운딩 박스 내의 사람 얼굴을 별 도의 외부엔진으로 전달하여 사람얼굴 특징값을 획득한다. 도 7은 본 실시예에 따른 머신러닝을 이용한 반려동물 인식 모델의 데이터 전처리 방법을 나타낸 도면이다. 반려동물 AR 합성 애플리케이션은 학습 전 모델 학습에 사용될 이미지 데이터를 수집하고 수집한 이미지별 얼굴영역, 눈과 코에 대한 좌표 데이터(정답)를 제작한다. 반려동물 AR 합성 애플리케이션은 이미지와 정 답 좌표 데이터를 모델이 학습할 수 있도록 데이터 전처리과정을 통해 데이터변환을 수행한다. 도 7의 (a)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션은 학습을 위한 이미지 데이터를 수집한다. 도 7의 (b)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션은 학습데이터 세트를 제작한다. 반려동물 AR 합성 애플리케이션은 학습데이터 세트를 제작하기 위해 얼굴 바운딩 박스(Facial Bounding Box) 제작 툴을 이용하여, 영상 데이터 내에서 얼굴영역 4점에 대한 좌표, 분류 데이터 생성(Rect)한다. 반려동물 AR 합성 애플리케이션은 학습데이터 세트를 제작하기 위해 얼굴 랜드마크(Facial Landmark) 제작 툴을 이용하여, 영상 데이터 내에서 눈,코,입 3점에 대한 좌표 데이터를 생성한다.도 7의 (c)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션은 데이터 증강을 수행한다. 반려동물 AR 합성 애플리케이션은 기존 데이터를 -30˚ ~ 30˚까지 5˚단위로 회전시켜 데이터를 증강시킨다. 반려동물 AR 합성 애플리케이션은 회전 각도에 따른 바운딩 박스(Bounding Box)와 랜드마크(Landmark) 데이터세트에 대한 좌표 변환을 수행한다. 반려동물 AR 합성 애플리케이션은 인공지능 모델이 학습 가능한 데이터 형태 로 변환한다. 도 8은 본 실시예에 따른 머신러닝을 이용한 반려동물 인식 모델의 모델 학습 방법을 나타낸 도면이다. 반려동물 AR 합성 애플리케이션에서 사전 학습한 인공지능 모델은 반려동물별 학습데이터를 집중 학습시키 는 전이학습 과정을 수행하여 구축된 모델이다. 반려동물 AR 합성 애플리케이션은 인공지능 모델을 학습할 때, 인식률 향상을 위하여 반려동물 종료, 묘종, 색상별로 집중 학습을 수행한다. 도 8의 (a)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션에서 인공지능 모델을 사전 학습하기 위해 입력 레이어와 출력 레이어를 재정의한다. 반려동물 AR 합성 애플리케이션은 이미지 기반 학습 및 공개된 사전 학습(Pre-trained) 모델의 입력과 출력 레이어를 학습 목적에 맞게 재정의한다 도 8의 (b)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션에서 인공지능 모델을 사전 학습하기 위해 반려동물 데이터 세트를 인공지능 모델에 입력한다. 반려동물 AR 합성 애플리케이션은 학습 및 검증 세트 로 나눈 반려동물 데이터 세트를 인공지능 모델에 입력하여 학습을 수행한다. 반려동물 AR 합성 애플리케이션 은 학습(75%), 검증(25%) 비율로 학습을 수행한다. 도 8의 (c)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션에서 인공지능 모델을 사전 학습하기 위해 미세 조정(Fine-tuning)을 수행한다. 반려동물 AR 합성 애플리케이션은 모델의 전체 레이어 중 일부만 동 결을 해제를 해주면서 학습률(Learning Rate)을 조정 및 반복 학습을 수행하며 정확도와 손실률을 개선한다. 예 컨대, 반려동물 AR 합성 애플리케이션은 모델 학습 종료 기준으로 정확도 97% 이상, 손실률 1.5% 이하로 학습을 수행할 수 있다. 도 9는 본 실시예에 따른 반려동물 얼굴특징을 이용하여 AR 스티커 합성 방법을 설명하기 위한 도면이다. 반려동물 AR 합성 애플리케이션은 단말기에 구비된 카메라를 이용하여 영상 데이터를 획득한다 (S910). 반려동물 AR 합성 애플리케이션은 영상 데이터 내에서 객체를 인식하여, 해당 객체가 사람인지, 반려동물인지를 구분한다. 반려동물 AR 합성 애플리케이션은 촬영된 영상 데이터에 SSD(Single Shot Detection)을 이용하여 객체 윤 곽을 인식하고, 객체 윤곽을 기반으로 기 학습된 객체와 비교하여 해당 객체가 반려동물의 안면 인지의 여부를 구분하고, 하고, 각 얼굴의 위치를 확인한다(S920). 반려동물 AR 합성 애플리케이션은 영상 데이터의 객체 가 사람 얼굴로 인지하면 사람 얼굴에 대응하는 위치에 사람 얼굴 바운딩 박스를 생성한다. 반려동물 AR 합성 애플리케이션은 영상 데이터의 객체가 반려동물 얼굴로 인지하면 반려동물 얼굴에 대응하는 위치에 반려동물 얼굴 바운딩 박스를 생성한다. 반려동물 AR 합성 애플리케이션은 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴 특징값을 추 출한다(S930). 반려동물 AR 합성 애플리케이션은 선택받은 AR 스티커 데이터를 로딩한다(S940). 반려동물 AR 합성 애플리케이션은 로딩된 AR 스티커 데이터에 대응하는 스티커 메타데이터를 추출한다. 반려동물 AR 합성 애플리케이션은 스티커 메타데이터를 반려동물 얼굴 특징값을 기반으로 AR 스티커 의 위치값과 회전값을 계산한다(S950). 반려동물 AR 합성 애플리케이션은 계산된 위치값과 회전값을 기반 으로 AR 스티커를 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴 레이어(Layer)로 합성한다(S960). 도 9에서는 단계 S910 내지 단계 S960을 순차적으로 실행하는 것으로 기재하고 있으나, 반드시 이에 한정되는 것은 아니다. 다시 말해, 도 8에 기재된 단계를 변경하여 실행하거나 하나 이상의 단계를 병렬적으로 실행하는 것으로 적용 가능할 것이므로, 도 9는은 시계열적인 순서로 한정되는 것은 아니다. 전술한 바와 같이 도 9에 기재된 본 실시예에 따른 반려동물 얼굴특징을 이용하여 AR 스티커 합성 방법은 프로 그램으로 구현되고 컴퓨터로 읽을 수 있는 기록매체에 기록될 수 있다. 본 실시예에 따른 반려동물 얼굴특징을 이용하여 AR 스티커 합성 방법을 구현하기 위한 프로그램이 기록되고 컴퓨터가 읽을 수 있는 기록매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 도 10a,b는 본 실시예에 따른 반려동물 얼굴 특징 확인하는 방법을 설명하기 위한 도면이다. 반려동물 AR 합성 애플리케이션은 촬영된 영상 데이터에 객체 얼굴 영역을 인식한다. 반려동물 AR 합성 애 플리케이션은 객체 얼굴 영역을 기반으로 기 학습된 객체와 비교하여 해당 객체가 반려동물의 안면으로 확 인한다. 반려동물 AR 합성 애플리케이션은 반려동물 얼굴에 대응하는 위치에 반려동물 얼굴 바운딩 박스 를 생성하고, 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴 내의 양안의 중심점 ‘A’을 획득한다 (S1010). 반려동물 AR 합성 애플리케이션은 양안의 중심점 ‘A’과 양안 거리를 이용한 얼굴 크기 ‘B’를 계산한다 (S1020). 반려동물 AR 합성 애플리케이션은 양안 각각에 해당하는 눈 두 점의 각도로 Z축 회전 각도 ‘C’ 를 계산한다(S1030). 반려동물 AR 합성 애플리케이션은 양쪽 눈의 회전 각도 ‘C’를 기반으로 코의 회전 각도 ‘D’를 계산한다(S1040). 추후, 반려동물 AR 합성 애플리케이션은 반려동물 얼굴 바운딩 박스 내의 반려동물 얼굴의 양안과 코 의 위치값과 양안의 회전 각도 ‘C’, 코의 회전 각도 ‘D’을 기반으로 AR 스티커를 반려동물 얼굴 바운딩 박 스 내의 반려동물 얼굴 레이어(Layer)로 합성한다. 도 10a에서는 단계 S1010 내지 단계 S1040을 순차적으로 실행하는 것으로 기재하고 있으나, 반드시 이에 한정되 는 것은 아니다. 다시 말해, 도 10a에 기재된 단계를 변경하여 실행하거나 하나 이상의 단계를 병렬적으로 실행 하는 것으로 적용 가능할 것이므로, 도 10a는 시계열적인 순서로 한정되는 것은 아니다. 전술한 바와 같이 도 10a에 기재된 본 실시예에 따른 반려동물 얼굴 특징 확인하는 방법은 프로그램으로 구현되 고 컴퓨터로 읽을 수 있는 기록매체에 기록될 수 있다. 본 실시예에 따른 반려동물 얼굴 특징 확인하는 방법을 구현하기 위한 프로그램이 기록되고 컴퓨터가 읽을 수 있는 기록매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 도 11은 본 실시예에 따른 사진과 애니메이션 스티커를 활용한 영상제작을 설명하기 위한 도면이다. 반려동물 AR 합성 애플리케이션은 카메라로 촬영한 영상 데이터를 불러온 후 반려동물의 얼굴 특징값(53 0)을 추출하여 자동으로 스티커 위치를 결정하고, 애니메이션 스티커인 경우 프레임 별로 합성한 영상으로 다시 인코딩한다. 도 11의 (a)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션은 단말기에 구비된 카메라로 촬영 한 영상 데이터를 로딩한다. 도 11의 (b)에 도시된 바와 같이, 반려동물 AR 합성 애플리케이션은 애니메이 션 스티커의 프레임별로 영상 데이터를 결합한 이미지를 생성한다. 도 11의 (c)에 도시된 바와 같이, 애니메이 션 스티커의 프레임별로 영상 데이터를 결합한 이미지를 영상으로 인코딩한다. 도 12a,b는 본 실시예에 따른 눈상태 확인 과정을 나타낸 도면이다. 반려동물 AR 합성 애플리케이션은 눈의 동공, 안구 위치정보, 눈 주변 영상, 코 영상, 코 주변 영상이 포 함된 영상 데이터를 촬영한다. 반려동물 AR 합성 애플리케이션은 촬영된 영상 데이터를 다중층 인식망 (Multilayer Perceptron) 모델에 입력하여, 눈부분 영상, 눈주변부분 영상, 코 영상, 코 주변 영상을 추출한다. 반려동물 AR 합성 애플리케이션은 눈부분 영상, 눈주변부분 영상, 코 영상, 코 주변 영상을 각각 라벨링한 다. 반려동물 AR 합성 애플리케이션은 라벨링된 눈부분 영상, 눈주변부분 영상, 코 영상, 코 주변 영상을 딥러닝 기반의 나선 신경망 모델(CNN: Convolutional Neural Network)에 입력한다. 반려동물 AR 합성 애플리케이션은 CNN에 입력된 눈부분 영상에 대한 홍채 영상을 생성하고, CNN에 입력된 눈주변부분 영상에 대한 눈주변 특징 부분 영상을 생성한다. 반려동물 AR 합성 애플리케이션은 CNN에 입력 된 코 영상에 대한 코 특징 영상을 생성하고, CNN에 입력된 코 주변부분 영상에 대한 코 주변 특징 부분 영상을 생성한다. 반려동물 AR 합성 애플리케이션은 생성한 홍채 영상과 눈주변 특징 부분 영상의 크기를 정규화시킨 후 부 호화한다. 반려동물 AR 합성 애플리케이션은 입력된 영상의 크기를 일정하게 하고, 중복 정보 포함을 최소 화하는 정규화를 수행한다. 반려동물 AR 합성 애플리케이션은 생성한 코 특징 영상과 코주변 특징 부분 영상의 크기를 정규화시킨 후 부호화한다. 반려동물 AR 합성 애플리케이션은 부호화한 홍채 영상, 눈주변 특징 부분 영상, 코 특징 영상, 코주변 특징 부분 영상을 기저장된 영상들과 비교하여 특정 동물을 인식한다. 반려동물 AR 합성 애플리케이션은 부호화시킨 홍채 영상, 눈주변 특징 부분 영상, 코 특징 영상, 코주변 특징 부분 영상을 기저장된 영상들과 비교하여 특정 동물을 인식하거나 동물의 질병(안구 질병, 코의 건조 여부)확인한다. 반려동물 AR 합성 애플리케이션은 인식된 결과와 함께 촬영한 사진, 동물의 이름, 개체번호, 홍채 코드, 품종, 생년월일, 등록 지역 및 양육인 정보를 출력한다. 인공 신경망은 컨볼루션 신경망(convolution neural network: CNN), 오토 인코더(auto encoder), 피드포워드 신경망(feedforward neural network), 방사 신경망(radial basis function network), 코헨 자기조직 신경망 (kohonen self-organizing network), 순환 신경망(RNN: recurrent neural network), 다중층 인식망(Multilayer Perceptron) 등을 포함한다. 반려동물 AR 합성 애플리케이션은 특정 동물임을 인식한 경우, 기저장된 영상들과 비교대상이 되었던 추출 된 영상들을 기저장된 영상들에 더하여 추가로 저장한다. 반려동물 AR 합성 애플리케이션은 기저장된 동물 정보를 최신의 상태로 두기 위해서 매 인증시마다 비교대상이 된 영상을 더하여 추가로 저장한다. 반려동물 AR 합성 애플리케이션은 동물의 홍채 패턴이 변화를 확인하여 안구 질환을 확인할 수 있다. 반려 동물 AR 합성 애플리케이션은 동물의 홍채 패턴이 변화된 경우, 홍채 패턴 및 눈 주변부분의 특징을 기저 장된 영상과 비교하여 홍채 패턴이 소실여부, 홍채 패턴을 제외한 눈주변 특징을 기반으로 안구 질환을 확인한 다. 홍채 영상은 홍채의 모양 또는 홍채의 색 중 하나 이상을 포함한다. 눈주변 특징 부분 영상은 눈꺼풀의 모양, 눈주변 털의 모양 또는 눈주변 털의 색 중 하나를 포함한다. 반려동물 AR 합성 애플리케이션은 동물의 코의 건조 변화를 확인하여 질환을 확인할 수 있다. 반려동물 AR 합성 애플리케이션은 동물의 코 건조 패턴이 변화된 경우, 코 특징 영상 및 코 주변부분의 특징을 기저장 된 영상과 비교하여 코의 건조 정도가 임계치를 초과한 경우 질환을 확인한다. 도 13은 본 실시예에 따른 코 건조 상태 확인 과정을 나타낸 도면이다. 반려동물 AR 합성 애플리케이션은 코 특징 영상, 코주변 특징 부분 영상을 획득하여 분석한다. 반려동물 AR 합성 애플리케이션은 촬영된 영상 데이터에서 관심 영역을 설정한다. 반려동물 AR 합성 애플리케이션 은 관심 영역에 포함된 노이즈 및 반사광을 제거한다. 반려동물 AR 합성 애플리케이션은 노이즈 및 반사광이 제거된 관심 영역 내에서 조직편(Phyton)을 분석한다. 반려동물 AR 합성 애플리케이션은 입력된 비문 이미지에서 관심 영역(ROI, Region Of Interest)을 설정하 는 과정과, 관심 영역에 대한 전처리를 수행하여 관심 영역에 포함된 잡음(noise)을 제거한다. 반려동물 AR 합 성 애플리케이션은 잡음이 제거된 관심 영역에서 반사광이 차지하는 반사광 영역을 산출한다. 반려동물 AR 합성 애플리케이션은 반사광 영역에 위치하는 픽셀들(pixels)의 명도(intensity) 값을 확인하 여 최대 명도 값으로 수렴되는 적어도 하나의 픽셀을 추출한다. 반려동물 AR 합성 애플리케이션은 관심 영역에 대한 개체 특징점을 추출함에 있어 최대 명도 값으로 수렴 되는 적어도 하나의 픽셀에 의해 추출되는 개체 특징점을 제거한다. 반려동물 AR 합성 애플리케이션은 관 심 영역에 대한 개체 특징점을 추출한 후 코 특징 영상, 코주변 특징 부분 영상에서 조직편을 분석한다. 반려동 물 AR 합성 애플리케이션은 코 특징 영상, 코주변 특징 부분 영상을 기반으로 코의 건조 상태를 확인한다. 반려동물 AR 합성 애플리케이션은 관심 영역에서 코의 골곡 부분과 솟아오른 부분을 추출한다. 반려동물 AR 합성 애플리케이션은 코의 골곡 부분과 솟아오른 부분을 기 저장된 영 상과 비교하여 코의 건조 여부를 확인한다. 도 14a,b는 본 실시예에 따른 반려동물의 움직임 판별 과정을 나타낸 도면이다. 반려동물 AR 합성 애플리케이션은 단말기에 탑재된 카메라로부터 획득한 영상 데이터를 실시간으로 처리하여 반려동물의 움직임 방향을 인식한다. 반려동물 AR 합성 애플리케이션은 획득한 영상 데이터를 처 리하여 반려동물의 머리, 몸, 다리, 꼬리 및 기타 객체의 움직임을 감지한다. 반려동물 AR 합성 애플리케이션 은 반려동물의 얼굴 인식, 표정 인식으로 반려동물의 상태를 예측할 수 있다. 반려동물 AR 합성 애플리케이션은 획득한 영사 데이터로부터 반려동물을 인지하고, 반려동물의 이동방향을 검출할 수 있다. 반려동물 AR 합성 애플리케이션은 획득한 영상 데이터를 복수의 영역으로 분할한다. 반려동물 AR 합성 애플리케이션은 분할 영역 내에서 움직임 감지 영역과 직전까지 움직임 감지 영역 으로 구분한다. 반려동물 AR 합성 애플리케이션은 복수의 분할 영역마다 반려 동물의 신체 부위를 확인한다. 반려동물 AR 합성 애플리케이션은 분할 영역마다 움직임 감지 영역과 직전까지 움직임 감지 영역으로 구분 한다. 반려동물 AR 합성 애플리케이션은 촬상한 화상에 기초하여 복수의 분할 영역마다 반려동물의 신체 부위를 검지한다. 반려동물 AR 합성 애플리케이션은 분할 영역 중 반려동물의 신체부위 존재 영역마다, 움직임 감 지 영역과 직전까지 움직임 감지 영역으로 구분하여 반려동물의 이동량과 운동량을 확인한다. 반려동물 AR 합성 애플리케이션은 복수의 분할 영역 각각에서 직전까지 움직임 감지 영역의 유지 시 간과 움직임 감지 영역의 유지 시간을 기반으로 운동량을 산출한다. 반려동물 AR 합성 애플리케이션은 운동량을 기반으로 CO2 농도를 산출한다. 반려동물 AR 합성 애플리케이 션은 직전까지 움직임 감지 영역에서 움직임 감지 영역으로 변경 여부에 따라 반려동물의 이 동량을 산출한다. 반려동물 AR 합성 애플리케이션은 복수의 분할 영역에 포함되는 한 개 이상의 움직임 감지 영역과 한 개 이상의 직전까지 움직임 감지 영역을 매칭시키거나 한 개 이상의 직전까지 움직임 감지 영역(142 0)을 한 개 이상의 움직임 감지 영역으로 매칭시킨다. 반려동물 AR 합성 애플리케이션은 움직임 감지 영역에서 직전까지 움직임 감지 영역으로 전환 되는 전환 영역을 추출한다. 반려동물 AR 합성 애플리케이션은 전환 영역을 기반으로 반려동물의 이동량과 운동량을 확인한다. 반려동물 AR 합성 애플리케이션은 전환 영역을 사이즈 또는 영역 개수를 기반으로 반 려동물의 이동량과 운동량을 확인한다. 반려동물 AR 합성 애플리케이션은 복수의 분할 영역 중 움직임 미 감지 영역을 오프 영역으로 설정한다. 반려동물 AR 합성 애플리케이션은 탐지 영역을 분할한 복수의 분할 영역마다, 반려 동물의 존재 유무를 검 지한다. 반려동물 AR 합성 애플리케이션은 탐지 영역의 복수의 분할 영역마다, 반려 동물의 존재 유무를 검지하는 검지 기능의 온오프를 전환할 수 있다. 반려동물 AR 합성 애플리케이션은 분할 영역 상의 탐지 영역을 직사각형으로 형태로 인지한다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2021-0149585", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 실시예에 따른 반려동물 안면인식을 이용한 AR 이미지 합성 시스템을 나타낸 도면이다. 도 2는 본 실시예에 따른 반려동물 안면인식을 이용한 AR 이미지 합성 장치를 나타낸 도면이다. 도 3은 본 실시예에 따른 반려동물 안면인식을 이용한 AR 이미지 합성 방법을 설명하기 위한 순서도이다. 도 4는 본 실시예에 따른 영상 내에서 사람과 반려동물을 구분하여 인식하는 방법을 설명하기 위한 순서도이다. 도 5는 본 실시예에 따른 반려동물의 얼굴에 AR 스티커를 매칭하는 방법을 설명하기 위한 순서도이다. 도 6은 본 실시예에 따른 사람과 반려동물의 얼굴을 동시에 인식하는 방법을 설명하기 위한 도면이다. 도 7은 본 실시예에 따른 머신러닝을 이용한 반려동물 인식 모델의 데이터 전처리 방법을 나타낸 도면이다. 도 8은 본 실시예에 따른 머신러닝을 이용한 반려동물 인식 모델의 모델 학습 방법을 나타낸 도면이다. 도 9는 본 실시예에 따른 반려동물 얼굴특징을 이용하여 AR 스티커 합성 방법을 설명하기 위한 도면이다. 도 10a,b는 본 실시예에 따른 반려동물 얼굴 특징 확인하는 방법을 설명하기 위한 도면이다. 도 11은 본 실시예에 따른 사진과 애니메이션 스티커를 활용한 영상제작을 설명하기 위한 도면이다. 도 12a,b는 본 실시예에 따른 눈상태 확인 과정을 나타낸 도면이다. 도 13은 본 실시예에 따른 코 건조 상태 확인 과정을 나타낸 도면이다. 도 14a,b는 본 실시예에 따른 반려동물의 움직임 판별 과정을 나타낸 도면이다."}
