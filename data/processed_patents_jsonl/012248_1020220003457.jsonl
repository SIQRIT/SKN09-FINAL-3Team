{"patent_id": "10-2022-0003457", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0108092", "출원번호": "10-2022-0003457", "발명의 명칭": "복수의 카메라를 이용하여 3차원 이미지를 생성하는 방법 및 장치", "출원인": "삼성전자주식회사", "발명자": "유병욱"}}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치가 복수의 카메라를 이용하여 획득된 이미지로부터 3차원 이미지를 생성하는 방법에 있어서,제1 카메라를 통해 제1 이미지를 획득하고 제2 카메라를 통해 제2 이미지를 획득하는 단계;상기 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하는 단계;상기 깊이 정보에 기초하여 상기 제1 이미지로부터 제1 레이어 이미지 및 제2 레이어 이미지를 식별하는 단계;상기 제1 이미지 및 제2 이미지에 기초하여 상기 제1 레이어 이미지의 적어도 일부를 인페인팅(inpainting)하는단계; 및상기 제2 레이어 이미지 및 인페인팅된 상기 제1 레이어 이미지에 기초하여 복수의 레이어를 포함하는 3차원 이미지를 생성하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제2 카메라의 화각은 상기 제1 카메라의 화각에 비해 크고,상기 제1 이미지에 대응되는 영역은 상기 제2 이미지에 대응되는 영역에 포함되는, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 제2 이미지를 전처리(preprocess)하는 단계를 더 포함하고,상기 제2 이미지를 전처리하는 단계는, 상기 제2 이미지를 자르거나 상기 제2 이미지의 축을 조절하는 단계를포함하는, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하는 단계는,상기 제1 이미지에 포함된 제1 픽셀, 및 상기 제2 이미지에 포함되고 상기 제1 픽셀에 대응되는 제2 픽셀 간의시차에 기초하여, 상기 제1 픽셀의 깊이 정보를 획득하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하는 단계는, 상기 제1 이미지를 입력 값으로 하여 상기 제1이미지에 포함된 픽셀들에 대한 깊이 정보를 포함하는 깊이 맵을 출력하도록 훈련된, 인공지능 모델을 이용하는것인, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 깊이 정보에 기초하여 상기 제1 이미지로부터 제1 레이어 이미지 및 제2 레이어 이미지를 식별하는단계는,관심 객체(target object)에 대응되는 이미지 영역을 상기 제2 레이어 이미지로 식별하고, 상기 제1 이미지에서공개특허 10-2023-0108092-3-상기 제2 레이어 이미지를 제외한 영역을 상기 제1 레이어 이미지로 식별하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 제1 이미지 및 제2 이미지에 기초하여 제1 레이어 이미지의 적어도 일부를 인페인팅하는 단계는;상기 제1 레이어 이미지에서 인페인팅 영역을 결정하는 단계;상기 인페인팅 영역 내의 특정 픽셀에 대해, 제1 이미지 및 제2 이미지 중 상기 특정 픽셀의 픽셀 값(pixelvalue)을 결정하기 위해 이용할 이미지를 결정하는 단계; 및상기 결정된 이미지에 기초하여 상기 특정 픽셀에 대응되는 픽셀 값을 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 인페인팅 영역은, 상기 제2 레이어 이미지에 대응되는 영역의 적어도 일부를 포함하고, 상기 제1 이미지에대응되는 깊이 맵에 기초하여 결정되는 것인, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 인페인팅 영역 내의 특정 픽셀에 대해, 제1 이미지 및 제2 이미지 중 상기 특정 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지를 결정하는 단계는,상기 특정 픽셀의 상기 인페인팅 영역 내의 상대적인 위치에 기초하여 결정되는, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서,상기 인페인팅 영역 내의 특정 픽셀에 대해, 제1 이미지 및 제2 이미지 중 상기 특정 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지를 결정하는 단계는,상기 특정 픽셀에 대응되는 객체 이미지를 식별하는 단계; 및상기 대응되는 객체 이미지의 전체가 상기 제1 이미지에 포함된 경우, 상기 제1 이미지를 상기 특정 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지로 결정하고, 상기 대응되는 객체 이미지의 전체가 상기 제1 이미지에 포함되지 않은 경우, 상기 제2 이미지를 상기 특정 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지로 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,상기 3차원 이미지는 제1 레이어 및 제2 레이어를 포함하고,상기 3차원 이미지를 생성하는 단계는, 인페인팅된 상기 제1 레이어 이미지를 상기 제1 레이어에 렌더링하고,상기 제2 레이어 이미지를 상기 제2 레이어에 렌더링하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "전자 장치에 있어서,제1 카메라;제2 카메라;적어도 하나의 명령어(instruction)를 저장하는 저장부; 및상기 저장부에 저장된 적어도 하나의 명령어를 실행하는 적어도 하나의 프로세서를 포함하고,공개특허 10-2023-0108092-4-상기 프로세서는 상기 적어도 하나의 명령어를 실행함으로써,상기 제1 카메라를 통해 제1 이미지를 획득하고 상기 제2 카메라를 통해 제2 이미지를 획득하고,상기 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하고,상기 깊이 정보에 기초하여 상기 제1 이미지로부터 제1 레이어 이미지 및 제2 레이어 이미지를 식별하고,상기 제1 이미지 및 상기 제2 이미지에 기초하여 상기 제1 레이어 이미지의 적어도 일부를 인페인팅하고,상기 제2 레이어 이미지 및 인페인팅된 상기 제1 레이어 이미지에 기초하여 복수의 레이어를 포함하는 3차원 이미지를 생성하는, 전자 장치."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 제2 카메라의 화각은 상기 제1 카메라의 화각에 비해 크고,상기 제1 이미지에 대응되는 영역은 상기 제2 이미지에 대응되는 영역에 포함되는, 전자 장치."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 프로세서는 상기 적어도 하나의 명령어들을 실행하여, 상기 제2 이미지를 자르거나 상기 제2 이미지의 축을 조절하여 상기 제2 이미지를 전처리하는, 전자 장치."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12항에 있어서,상기 프로세서는 상기 적어도 하나의 명령어들을 실행하여, 상기 제1 이미지에 포함된 제1 픽셀, 및 상기 제2이미지에 포함되고 상기 제1 픽셀에 대응되는 제2 픽셀 간의 시차에 기초하여, 상기 제1 픽셀의 깊이 정보를 획득하는, 전자 장치."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제12항에 있어서,상기 프로세서는 상기 적어도 하나의 명령어들을 실행하여, 관심 객체에 대응되는 이미지 영역을 상기 제2 레이어 이미지로 식별하고, 상기 제1 이미지에서 상기 제2 레이어 이미지를 제외한 영역을 상기 제1 레이어 이미지로 식별하는, 전자 장치."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제12항에 있어서,상기 프로세서는 상기 적어도 하나의 명령어들을 실행하여,상기 제1 레이어 이미지에서 인페인팅 영역을 결정하고,상기 인페인팅 영역 내의 특정 픽셀에 대해, 제1 이미지 및 제2 이미지 중 상기 특정 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지를 결정하고,상기 결정된 이미지에 기초하여 상기 특정 픽셀에 대응되는 픽셀 값을 결정하는, 전자 장치."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 프로세서는 상기 적어도 하나의 명령어들을 실행하여,상기 특정 픽셀에 대응되는 객체 이미지를 식별하고,상기 대응되는 객체 이미지의 전체가 상기 제1 이미지에 포함된 경우, 상기 제1 이미지를 상기 특정 픽셀의 픽공개특허 10-2023-0108092-5-셀 값을 결정하기 위해 이용할 이미지로 결정하고, 상기 대응되는 객체 이미지의 전체가 상기 제1 이미지에 포함되지 않은 경우, 상기 제2 이미지를 상기 특정 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지로 결정하는,전자 장치."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제12항에 있어서,상기 3차원 이미지는 제1 레이어 및 제2 레이어를 포함하고,상기 프로세서는 상기 적어도 하나의 명령어들을 실행하여,인페인팅된 상기 제1 레이어 이미지를 상기 제1 레이어에 렌더링하고, 상기 제2 레이어 이미지를 상기 제2 레이어에 렌더링하는, 전자 장치."}
{"patent_id": "10-2022-0003457", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제1항 내지 제11항 중 어느 한 항의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2022-0003457", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 복수의 카메라를 이용하여 획득된 이미지로부터 3차원 이미지를 생성하는 방법이 제공된다. 방법은, 제1 카메라를 통해 제1 이미지를 획득하고 제2 카메라를 통해 제2 이미지를 획득하는 단계, 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하는 단계, 깊이 정보에 기초하여 제1 이미지로부터 제1 레이어 이미지 및 제2 레이어 이미지를 식별하는 단계, 제1 이미지 및 제2 이미지에 기초하여 제1 레이어 이미지의 적어도 일부를 인페인팅 (inpainting)하는 단계, 및 제2 레이어 이미지 및 인페인팅된 제1 레이어 이미지에 기초하여 복수의 레이어를 포 함하는 3차원 이미지를 생성하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0003457", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 서로 다른 화각(Field of view, FOV)을 갖는 복수의 카메라를 이용하여 획득된 이미지로부터 3차원 이미지를 생성하는 방법 및 전자 장치에 관한 것이다."}
{"patent_id": "10-2022-0003457", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근, 다양한 디지털 카메라 제품에 대한 수요가 증가하고 있으며, 3차원 영상을 촬영하는 3차원 카메라에 대해 서도 관심이 집중되고 있다. 일반적인 2차원 카메라는 하나의 촬영 렌즈와 하나의 이미지 센서를 이용하여 하나 의 장면을 캡쳐(capture)한다. 3차원 영상을 캡처하기 위해서는 사람의 눈과 같이, 적어도 2개의 촬영 렌즈와 2 개의 이미지 센서가 필요하다. 사람이 두 눈으로 물체(object)를 바라볼 때, 두 눈은 서로 다른 광 축을 갖는다. 따라서, 왼쪽 눈으로 본 2차원 영상과 오른쪽 눈으로 본 2차원 영상은 서로 다른 기하학적 배치를 가지 며, 이 차이로부터 깊이 감이 인지될 수 있다. 최근에는 기술의 발달로, 단일한 2차원 영상을 3차원 영상으로 변환하기도 하지만, 그 성능에는 한계가 있으며, 이에 따라 적어도 2 이상의 서로 다른 시점(viewpoint)의 영상을 캡처하기 위해 2개 이상의 카메라를 포함하는 3차원 카메라에 대한 다양한 설계가 제시되고 있다. 다양한 전자 제품들은 카메라를 포함할 수 있다. 예를 들어, 휴대폰은 하나 이상의 카메라를 포함할 수 있고, 사용자는 휴대폰에 설치된 카메라를 이용하여 사진 또는 동영상을 촬영할 수 있다. 카메라의 시야는 제한되어 있으므로, 하나의 카메라를 이용하여 3차원 피사체를 3차원 공간에서 정확히 표현하는 것은 어렵다. 3차원의 피 사체를 3차원 공간에서 정확히 표현하기 위해 복수의 카메라들을 이용할 수 있다. 최근 기술의 발달로 가상 현실(virtual reality, VR) 또는 증강 현실(augmented reality, AR)을 제공하는 컨텐 츠(content)의 보급 또한 증가하고 있다. 증강 현실 기술은 현실의 환경에 가상 사물이나 정보를 합성하여, 가 상 사물이나 정보가 현실의 물리적 환경에 존재하는 사물처럼 보이도록 하는 기술이다. 현대의 컴퓨팅 및 디스 플레이 기술은 증강 현실 경험을 위한 시스템의 개발을 가능하게 하였는데, 증강 현실 경험에서는, 디지털적으 로 재생성된 3차원 이미지 또는 그 일부가, 현실인 것처럼 생각되거나 또는 현실로서 인식될 수 있는 방식으로 사용자에게 제시될 수 있다. 보다 실감나는 증강 현실 기술의 구현을 위해서는 현실감 있는 3차원 이미지를 생 성하는 기술이 요구된다."}
{"patent_id": "10-2022-0003457", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 일 실시예는, 서로 다른 화각을 갖는 복수의 카메라를 이용하여 획득된 복수의 이미지로부터 3차원 이미지를 생성함으로써, 이미지의 인페인팅(inpainting) 동작에 보다 많은 픽셀 정보를 이용할 수 있고, 전경이미지와 배경 이미지의 경계 부분에서 발생하는 아티팩트(artifact)를 줄일 수 있어 현실감 있는 3차원 이미지 를 생성할 수 있는, 전자 장치 및 방법을 제공할 수 있다. 본 개시의 일 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제로 한정되지 않으며, 이하의 실시예들로부터 또 다른 기술적 과제들이 유추될 수 있다."}
{"patent_id": "10-2022-0003457", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 기술적 수단으로서 개시된 전자 장치가 복수의 카메라를 이용하여 획득된 이미지로부터 3차원 이미지를 생성하는 방법은, 제1 카메라를 통해 제1 이미지를 획득하고 제2 카메라를 통해 제2 이미지를 획득하는 단계, 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하는 단계, 깊이 정보에 기초하여 제1 이미지로부터 제1 레이어 이미지 및 제2 레이어 이미지를 식별하는 단계, 제1 이미지 및 제2 이미지에 기초 하여 제1 레이어 이미지의 적어도 일부를 인페인팅(inpainting)하는 단계, 및 제2 레이어 이미지 및 인페인팅된 제1 레이어 이미지에 기초하여 복수의 레이어를 포함하는 3차원 이미지를 생성하는 단계를 포함할 수 있다. 상술한 기술적 과제를 달성하기 위한 기술적 수단으로서 개시된 복수의 카메라를 이용하여 획득된 이미지로부터 3차원 이미지를 생성하는 전자 장치는, 제1 카메라, 제2 카메라, 적어도 하나의 명령어(instruction)를 저장하 는 저장부, 및 저장부에 저장된 적어도 하나의 명령어를 실행하는 적어도 하나의 프로세서를 포함할 수 있다. 프로세서는 적어도 하나의 명령어를 실행함으로써, 제1 카메라를 통해 제1 이미지를 획득하고 제2 카메라를 통 해 제2 이미지를 획득하고, 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하고, 깊이 정보에 기초하여 제1 이미 지로부터 제1 레이어 이미지 및 제2 레이어 이미지를 식별하고, 제1 이미지 및 제2 이미지에 기초하여 제1 레이 어 이미지의 적어도 일부를 인페인팅하고, 제2 레이어 이미지 및 인페인팅된 제1 레이어 이미지에 기초하여 복 수의 레이어를 포함하는 3차원 이미지를 생성할 수 있다. 상술한 기술적 과제를 달성하기 위한 기술적 수단으로서 개시된, 컴퓨터로 읽을 수 있는 기록 매체는, 개시된 방법의 실시예들 중에서 적어도 하나를 컴퓨터에서 실행시키기 위한 프로그램이 저장된 것일 수 있다."}
{"patent_id": "10-2022-0003457", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 개시의 실시예를 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해서 설명과 관 계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 개시의 실시예들에서 사용되는 용어는 본 개시의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용 어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라 질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 실시예의 설명 부 분에서 상세히 그 의미를 기재할 것이다. 따라서 본 명세서에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 개시 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 본 명세서에 기재 된 \"~부\", \"~모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\"되어 있는 경우도 포함한다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아 니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 본 명세서에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적 합한(suitable for)\", \"~하는 능력을 가지는(having the capacity to)\", \"~하도록 설계된(designed to)\", \"~하 도록 변경된(adapted to)\", \"~하도록 만들어진(made to)\", 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용 될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 시스템\"이라는 표현은, 그 시스템이 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C 를 수행하도록 구성된(또는 설정된) 프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로 세서), 또는 메모리에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있 는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 본 개시에 따른 인공지능과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등 과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인 공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인 공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 하나 또는 복수의 프로세서가 인공지능 전용 프로세 서인 경우, 인공지능 전용 프로세서는 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델(또는, 딥러닝 모델)이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모 델이 만들어짐을 의미한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습 (supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또 는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다.인공지능 모델(또는, 딥러닝 모델)은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각 은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간 의 연산을 통해 신경망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모 델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경 망(DNN:Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q- Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 본 개시에서 ‘화각(Field of View, FOV)’은 카메라를 통해 촬영되는 이미지 또는 영상의 영역을 나타낸다. 예 를 들어, '카메라의 화각'은 렌즈를 통해서 카메라가 촬영할 수 있는 각도를 나타내고, '이미지의 화각'은 해당 이미지를 촬상면으로 하는 카메라의 화각을 나타낼 수 있다. 화각은 시야 또는 시야각(FOV degree)으로 지칭될 수도 있다. 카메라를 통해 촬영된 이미지 또는 영상의 영역은, 디스플레이 화면 범위(screen area) 내에 표시되 는 이미지 또는 영상의 영역을 나타낼 수도 있다. 본 개시에서 '전경 이미지(foreground image)'는 이미지 내에 포함된 관심 객체(target object)의 이미지를 나 타낸다. 특정 이미지에는 적어도 하나의 전경 이미지가 포함될 수 있다. 일 실시예에서 하나의 전경 이미지는 단일한 관심 객체에 대응될 수도 있고, 복수의 관심 객체들에 대응될 수도 있다. 전경 이미지에서 배경 이미지 에 대응되는 영역의 픽셀들은 픽셀 값을 가지지 않거나, 대응되는 배경 이미지의 픽셀 값을 갖거나, 인페인팅된 픽셀 값을 가질 수 있다. 본 개시에서 '배경 이미지(background image)'는 특정 이미지에서 적어도 하나의 전경 이미지를 제외한 영역을 나타낸다. 배경 이미지에서 전경 이미지에 대응되는 영역의 픽셀들은 픽셀 값을 가지지 않거나, 대응되는 전경 이미지의 픽셀의 픽셀 값을 갖거나, 인페인팅된 픽셀 값을 가질 수 있다. 본 개시에서 3차원 이미지는 복수의 레이어(layer)를 포함할 수 있다. '레이어(layer)'는 이미지가 그래픽으로 렌더링(rendering)되는 층을 나타낸다. 3차원 이미지는 복수의 투명 레이어에 각각 렌더링된 이미지들이 겹쳐짐 으로써 생성될 수 있다. 이하 첨부된 도면을 참고하여 본 개시를 상세히 설명하기로 한다. 도 1은 본 개시의 일 실시예에 따른 전자 장치가 복수의 카메라를 이용하여 3차원 이미지를 생성하는 방법의 개 요도이다. 단계 110에서, 전자 장치는 내장된 복수의 카메라를 통해 입력 이미지(IM1, IM2)를 획득할 수 있다. 도 1을 참 조하면, 전자 장치는 서로 다른 화각을 갖는 제1 카메라 및 제2 카메라를 포함할 수 있다. 전자 장치는, 제1 카 메라를 통해 제1 이미지(IM1)를 획득하고, 제2 카메라를 통해 제2 이미지(IM2)를 획득할 수 있다. 제1 이미지 (IM1) 및 제2 이미지(IM2)는 2차원 이미지일 수 있다. 일 실시예에서, 제2 카메라가 제1 카메라에 비해 넓은 화 각을 가질 수 있다. 이 경우, 제2 카메라를 통해 획득한 제2 이미지(IM2)가 제1 카메라를 통해 획득한 제1 이미 지(IM1)에 비해 넓은 영역에 대응되는 정보를 포함할 수 있다. 예를 들어, 제1 카메라를 통해 획득한 제1 이미 지(IM1)에 대응되는 영역은 제2 카메라를 통해 획득한 제2 이미지(IM2)에 대응되는 영역에 포함될 수 있다. 예 를 들어, 제1 이미지(IM1)는 제2 이미지(IM2)의 일부분을 확대한 이미지에 대응될 수 있다. 본 개시의 일 실시 예에 따른 제1 이미지(IM1)의 화각 및 제2 이미지(IM2)의 화각의 관계에 대해서는 후술할 도 5를 참조하여 보다 자세히 설명하도록 한다. 본 개시의 일 실시예에 따른 전자 장치는 제1 이미지(IM1) 및 제2 이미지(IM2)에 기초하여 3차원 이미지(IM3)를 생성할 수 있다. 일 실시예에서, 전자 장치가 생성하는 3차원 이미지(IM3)의 화각은 제1 이미지(IM1)의 화각보다 작거나 같을 수 있다. 예를 들어, 제1 이미지(IM1)의 화각이 a1이고, 제2 이미지(IM2)의 화각이 a2일 때, 제1 이미지(IM1) 및 제2 이미지(IM2)에 기초하여 생성된 3차원 이미지(IM3)의 화각 a3은, a3≤a1≤a2를 만족시킬 수 있다. 일 실시예에서, 전자 장치가 생성하는 3차원 이미지(IM3)의 화각은 제1 이미지(IM1)의 화각보다 크거나 같고 제 2 이미지(IM2)의 화각보다는 작거나 같을 수도 있다. 예를 들어, 제1 이미지(IM1)의 화각이 a1이고, 제2 이미지 (IM2)의 화각이 a2일 때, 제1 이미지(IM1) 및 제2 이미지(IM2)에 기초하여 생성된 3차원 이미지(IM3)의 화각 a3은, a1≤a3≤a2를 만족시킬 수도 있다. 단계 115에서, 전자 장치는 제2 이미지(IM2)를, 생성하고자 하는 3차원 이미지(IM3)의 화각 a3에 기초하여 전처 리(preprocess)할 수 있다. 제2 이미지(IM2)를 전처리하는 동작은, 제2 이미지(IM2)를 화각 a3에 맞추어 자르는 동작(crop) 및 제2 이미지(IM2)의 이미지 축을 조절하는(warp) 동작을 포함할 수 있다. 일 실시예에서, 전자 장치는 제1 이미지(IM1)에 대해서도 전처리(preprocess) 동작을 수행할 수 있다. 제1 이미 지(IM1)를 전처리 하는 동작 또한, 생성하고자 하는 3차원 이미지(IM3)의 화각 a3에 기초하여 제1 이미지(IM1) 를 자르거나 제1 이미지(IM1)의 이미지 축을 조절하는 동작을 포함할 수 있다. 전처리된 제2 이미지(IM2)는 전 처리된 제1 이미지(IM1)와 동일한 이미지 축을 갖고, 동일한 영역에 대한 이미지에 대응될 수 있다. 예를 들어, 전처리된 제2 이미지(IM2)의 모든 픽셀들은 전처리된 제1 이미지(IM1)에 포함된 모든 픽셀들과 1대1 대응될 수 있다. 예를 들어, 전처리된 제1 이미지(IM1) 및 전처리된 제2 이미지(IM2)는 동일한 객체에 대응되는 이미지를 포함할 수 있고, 전처리된 제1 이미지(IM1) 및 전처리된 제2 이미지(IM2)에 포함되는 객체 이미지는 동일하거나 비슷한 크기를 가질 수 있다. 일 실시예에서, 전자 장치가 생성하고자 하는 3차원 이미지(IM3)의 화각 a3이 제1 이미지(IM1)의 화각인 a1으로 설정될 수 있다 (a1=a3). 이 경우, 전자 장치는 제1 이미지(IM1)에 대응되도록 제2 이미지(IM2)의 이미지 축을 조절하고, 제2 이미지(IM2)를 자르고, 확대하여 제2 이미지(IM2)를 전처리할 수 있다. 이후 동작들에서는, 전자 장치가 생성하고자 하는 3차원 이미지(IM3)의 화각 a3이 제1 이미지(IM1)의 화각 a1인 경우(a1=a3)에 대해 예시적으로 설명한다. 일 실시예에서, 전자 장치가 생성하고자 하는 3차원 이미지(IM3)의 화 각 a3이 제1 이미지(IM1)의 화각 a1이 아닌 경우에는, 제1 이미지(IM1)를 생성하고자 하는 3차원 이미지(IM3)의 화각 a3에 기초하여 전처리한 후, (예를 들어, 제1 이미지(IM1)를 자르거나 제1 이미지(IM1)의 이미지 축을 조 절한 후) 전처리된 제1 이미지에 대해 이후 동작들을 적용할 수 있다. 단계 120에서, 전자 장치는 제1 이미지(IM1)에 포함된 픽셀(pixel)의 깊이(depth) 정보를 획득할 수 있다. 일 실시예에서, 제1 이미지(IM1)에 포함된 픽셀의 깊이 정보를 획득하는 동작은, 제1 이미지(IM1)를 입력 값으 로 하여 제1 이미지(IM1)에 포함된 픽셀들에 대한 깊이 정보를 포함하는 깊이 맵(depth map, DM)을 출력하도록 훈련된 인공지능 모델을 이용하는 것일 수 있다. 일 실시예에서, 전자 장치는 단계 120에서 제1 이미지(IM1) 및 제2 이미지(IM2) 간의 시차에 기초하여, 제1 이 미지(IM1)에 포함된 픽셀의 깊이 정보를 획득할 수도 있다. 예를 들어, 전자 장치는 제1 이미지(IM1)에 포함된 제1 픽셀, 및 제2 이미지(IM2)에 포함되고 제1 픽셀에 대응되는 제2 픽셀 간의 시차에 기초하여, 제1 픽셀의 깊 이 정보를 획득할 수 있다. 일 실시예에서, 제1 이미지(IM1) 및 제2 이미지(IM2) 간의 시차에 기초하여 계산된 깊이 맵(DM)은, 제1 이미지 (IM)에 인공지능 모델을 적용하여 추정된 깊이 맵(DM)에 비해 정확성이 높은 깊이 값을 포함할 수 있다. 따라서, 본 개시의 일 실시예에 따른 복수의 카메라(서로 다른 위치에서 촬영된 복수의 이미지)를 이용하여 3차 원 이미지(IM3)를 생성하는 방법에서는, 단일한 카메라(단일한 이미지)를 이용하여 3차원 이미지(IM3)를 생성하 는 경우에 비해, 정확도가 높은 깊이 맵(DM)을 획득할 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 제1 이미지(IM1) 및 제2 이미지(IM2)에 기초하여, 제1 이미지(IM1)에 포함된 픽셀의 깊이 정보를 포함하는 깊이 맵(DM)을 획득하는 동작에 대해서는 후술할 도 6을 참조하여 보다 자 세히 설명하도록 한다. 단계 130에서, 전자 장치는 깊이 맵(DM)에 포함된 깊이 정보에 기초하여 제1 이미지(IM1)로부터 전경 이미지 (foreground image, FG) 및 배경 이미지(background image, BG)를 식별할 수 있다. 일 실시예에서, 배경 이미 지(BG)는 제1 레이어 이미지에 대응될 수 있고, 전경 이미지(FG)는 제2 레이어 이미지에 대응될 수 있다. 예를 들어, 제1 레이어 이미지는 배경 이미지(BG)를 포함할 수 있고, 제2 레이어 이미지는 전경 이미지(FG)를 포함할 수 있다. 전자 장치는, 제1 이미지(IM1) 및 제2 이미지(IM2)에 기초하여 배경 이미지(BG)가 포함된 제1 레이어 이미지의 적어도 일부를 인페인팅(inpainting)할 수 있다. 일 실시예에서, 전자 장치가 깊이 정보에 기초하여 제1 이미지(IM1)로부터 전경 이미지(FG) 및 배경 이미지(B G)를 구분하는 동작은, 관심 객체(target object)에 대응되는 이미지 영역을 전경 이미지(FG)로 식별하고, 제1 이미지(IM1)에서 전경 이미지(FG)를 제외한 영역을 배경 이미지(BG)로 식별하는 동작을 포함할 수 있다. 예를 들어, 관심 객체는 3차원 이미지(IM3) 생성시 전면 레이어(layer)에 렌더링(rendering)할 객체를 나타낼 수 있 다. 도 1을 참조하면, 관심 객체는 오리일 수 있고, 전경 이미지(FG)는 제1 이미지(IM1) 중 오리에 해당하는 이 미지 영역에 대응될 수 있고, 배경 이미지(BG)는 제1 이미지(IM1)에서 오리 이미지를 제외한 나머지 영역(예를 들어, 땅, 물 등)에 대응될 수 있다. 깊이 맵(DM) 상에서, 서로 다른 객체에 대응되는 이미지의 경계 부분에서는 깊이 값이 급격하게 변화하거나 불 연속적인 깊이 값이 나타날 수 있다. 따라서, 제1 이미지(IM1)에 대응되는 깊이 맵(DM)을 참조하면, 제1 이미지 (IM1)에 포함된 객체들의 에지(edge)에 관련된 정보를 획득할 수 있다. 일 실시예에서, 전자 장치는 제1 이미지 (IM1)에 대응되는 깊이 맵(DM)에 기초하여, 제1 이미지(IM1)로부터 전경 이미지(FG) 및 배경 이미지(BG)를 구분 할 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 제1 이미지(IM1)로부터 전경 이미지(foreground image, FG) 및 배경 이미지(background image, BG)를 식별하는 동작에 대해서는 후술할 도 7을 참조하여 보다 자세히 설명하도록 한 다. 일 실시예에서, 제1 이미지(IM1) 및 제2 이미지(IM2)에 기초하여 배경 이미지(BG)가 포함된 제1 레이어 이미지 의 적어도 일부를 인페인팅하는 동작은, 제1 레이어 이미지에서 인페인팅 영역을 결정하는 단계, 인페인팅 영역 내의 특정 픽셀에 대해, 제1 이미지(IM1) 및 제2 이미지(IM2) 중 특정 픽셀의 픽셀 값(pixel value)을 결정하기 위해 이용할 이미지를 결정하는 단계, 및 결정된 이미지에 기초하여 특정 픽셀에 대응되는 픽셀 값을 결정하는 단계를 포함할 수 있다. 일 실시예에서, 인페인팅 영역은 제1 레이어 이미지 상에서, 제2 레이어 이미지에 대응되는 영역의 적어도 일부 를 포함할 수 있다. 일 실시예에서, 인페인팅 영역은 제1 이미지(IM1)에 대응되는 깊이 맵(DM)에 기초하여 결정 될 수 있다. 예를 들어, 깊이 맵(DM) 상에서 제1 레이어 이미지와 제2 레이어 이미지의 깊이 값의 차이가 큰 경 우 인페인팅 영역이 넓어질 수 있고, 제1 레이어 이미지와 제2 레이어 이미지의 깊이 값의 차이가 작은 경우 인 페인팅 영역이 좁아질 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 깊이 맵(DM)에 기초하여 인페인팅 영 역을 결정하는 동작에 대해서는 후술할 도 8을 참조하여 보다 자세히 설명하도록 한다. 일 실시예에서, 전자 장치는 특정 픽셀의 인페인팅 영역 내의 상대적인 위치에 기초하여, 제1 이미지(IM1) 및 제2 이미지(IM2) 중 해당 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지를 결정할 수 있다. 예를 들어, 인페인 팅 영역은 배경 이미지(BG)측 경계선(외측 경계선)과, 인페인팅을 수행하지 않는 영역측 경계선(내측 경계선)을 갖는다고 볼 수 있다. 이 때, 픽셀 값을 결정하고자 하는 특정 픽셀이, 외측 경계선에 더 가까운 경우 제2 이미 지(IM2)를 픽셀 값을 결정하기 위해 이용할 이미지로 결정할 수 있고, 내측 경계선에 더 가까운 경우 제1 이미 지(IM1)를 픽셀 값을 결정하기 위해 이용할 이미지로 결정할 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 특정 픽셀의 인페인팅 영역 내의 상대적인 위치에 기초하여 제1 이미지(IM1) 및 제2 이미지(IM2) 중 해당 픽셀 의 픽셀 값을 결정하기 위해 이용할 이미지를 결정하는 동작에 대해서는 후술할 도 9를 참조하여 보다 자세히 설명하도록 한다. 일 실시예에서, 전자 장치가 인페인팅 영역 내의 특정 픽셀에 대해, 제1 이미지(IM1) 및 제2 이미지(IM2) 중 해 당 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지를 결정하는 동작은, 해당 픽셀에 대응되는 객체(object) 이 미지를 식별하고, 대응되는 객체 이미지의 전체가 제1 이미지(IM1)에 포함된 경우, 제1 이미지(IM1)를 해당 픽 셀의 픽셀 값을 결정하기 위해 이용할 이미지로 결정하고, 대응되는 객체 이미지의 전체가 제1 이미지(IM1)에 포함되지 않은 경우, 제2 이미지(IM2)를 해당 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지로 결정하는 동작 을 포함할 수도 있다. 본 개시의 일 실시예에 따른 전자 장치가 제1 이미지(IM1) 및 제2 이미지(IM2)에 기초하여 제1 레이어 이미지를 인페인팅하는 동작에 대해서는 후술할 도 8 및 도 9를 참조하여 보다 자세히 설명하도록 한다. 단계 140에서, 전자 장치는 제2 레이어 이미지 및 인페인팅된 제1 레이어 이미지에 기초하여 복수의 레이어 (layer)를 포함하는 3차원 이미지(IM3)를 생성할 수 있다. 일 실시예에서, 3차원 이미지(IM3)는 복수의 레이어 를 포함할 수 있다. 예를 들어, 3차원 이미지(IM3)는 제1 레이어 및 제2 레이어를 포함할 수 있다. 전자 장치가 3차원 이미지(IM3)를 생성하는 동작은, 인페인팅된 제1 레이어 이미지를 제1 레이어에 렌더링하고, 제2 레이어 이미지를 제2 레이어에 렌더링하는 동작을 포함할 수 있다. 제1 레이어는 제2 레이어에 비해 전자 장치로부터의깊이 값이 클 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 인페인팅된 제1 레이어 이미지 및 제2 레이어 이미지에 기초하여 복수의 레이어를 포함하는 3차원 이미지(IM3)를 생성하는 동작에 대해서는 후술할 도 10을 참조하여 보다 자세히 설명하도록 한다. 본 개시의 일 실시예서는, 서로 다른 화각을 갖는 복수의 카메라를 이용하여 획득된 복수의 이미지로부터 3차원 이미지(IM3)를 생성함으로써, 이미지의 인페인팅(inpainting) 동작에 보다 많은 픽셀 정보를 이용할 수 있고, 전경 이미지(FG)와 배경 이미지(BG)의 경계 부분에서 발생하는 아티팩트(artifact)를 줄일 수 있어, 현실감 있 는 3차원 이미지(IM3)를 생성할 수 있다. 도 2는 본 개시의 일 실시예에 따른 전자 장치가 복수의 카메라를 이용하여 3차원 이미지를 생성하는 방법의 흐 름도이다. 단계 S210에서, 전자 장치는 제1 이미지 및 제2 이미지를 획득한다. 단계 S210은 도 1의 단계 110에 대응될 수 있다. 일 실시예에서, 전자 장치는 서로 다른 화각을 갖는 제1 카메라 및 제2 카메라를 포함할 수 있다. 전자 장치는 제1 카메라를 통해 제1 이미지를 획득하고, 제2 카메라를 통해 제2 이미지를 획득할 수 있다. 제1 이미 지 및 제2 이미지는 2차원 이미지일 수 있다. 일 실시예에서, 제2 카메라가 제1 카메라에 비해 넓은 화각을 가 질 수 있다. 본 개시의 일 실시예에 따른 제1 이미지의 화각 및 제2 이미지의 화각의 관계에 대해서는 후술할 도 5를 참조하여 보다 자세히 설명하도록 한다. 일 실시예에서, 전자 장치가 생성하고자하는 3차원 이미지의 화각이 제1 이미지의 화각과 동일하게 설정될 수 있다. 이 경우, 전자 장치는 제1 이미지에 대응되도록 제2 이미지의 이미지 축을 조절하고 제2 이미지를 자르고, 확대하여 제2 이미지를 전처리할 수 있다. 전처리된 제2 이미지는 제1 이미지와 동일한 이미지 축을 갖 고, 동일한 영역에 대한 이미지에 대응될 수 있다. 예를 들어, 전처리된 제2 이미지 내의 모든 픽셀들은 제1 이 미지에 포함된 모든 픽셀들과 1대1 대응될 수 있다. 이후 동작들에서는, 전자 장치가 생성하고자 하는 3차원 이미지의 화각이 제1 이미지의 화각과 동일한 경우에 대해 예시적으로 설명한다. 일 실시예에서, 전자 장치가 생성하고자 하는 3차원 이미지의 화각이 제1 이미지의 화각이 아닌 경우에는, 제1 이미지를 생성하고자 하는 3차원 이미지의 화각에 기초하여 전처리한 후, 전처리된 제1 이미지에 대해 이후 동작들을 적용할 수 있다. 단계 S220에서, 전자 장치는 제1 이미지에 포함된 픽셀의 깊이 정보를 획득한다. 단계 S220은 도 1의 단계 120 에 대응될 수 있다. 일 실시예에서, 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하는 동작에서는, 제1 이미지 를 입력 값으로 하여 제1 이미지에 포함된 픽셀들에 대한 깊이 정보를 포함하는 깊이 맵을 출력하도록 훈련된 인공지능 모델이 이용될 수 있다. 일 실시예에서, 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하는 동작에서 는, 제1 이미지 및 제2 이미지 간의 시차가 이용될 수도 있다. 예를 들어, 전자 장치는 제1 이미지에 포함된 제 1 픽셀, 및 제2 이미지에 포함되고 제1 픽셀에 대응되는 제2 픽셀 간의 시차에 기초하여, 제1 픽셀의 깊이 정보 를 획득할 수 있다. 제1 이미지 및 제2 이미지 간의 시차를 이용하여 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하는 동작에서는, 제1 카메라와 제2 카메라가 배치된 상대적인 위치 정보에 기초한 삼각측량법이 이용될 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 제1 이미지 및 제2 이미지에 기초하여, 제1 이미지에 포함된 픽셀의 깊이 정보를 포함하는 깊이 맵을 획득하는 동작에 대해서는 후술할 도 6을 참조하여 보다 자세히 설명하도록 한 다. 단계 S230 및 S240은 도 1의 단계 130에 대응될 수 있다. 단계 S230에서, 전자 장치는 제1 이미지에 포함된 픽 셀의 깊이 정보에 기초하여 제1 이미지로부터 제1 레이어 이미지 및 제2 레이어 이미지를 식별한다. 일 실시예 에서, 제1 레이어 이미지는 배경 이미지를 포함할 수 있고, 제2 레이어 이미지는 전경 이미지를 포함할 수 있다. 예를 들어, 전자 장치는 제1 이미지로부터 전경 이미지 및 배경 이미지를 구분하고, 구분된 전경 이미지 및 배경 이미지에 기초하여 제1 레이어 이미지 및 제2 레이어 이미지를 식별할 수 있다. 일 실시예에서, 전자 장치가 제1 이미지로부터 전경 이미지 및 배경 이미지를 구분하는 동작은, 관심 객체(target object)에 대응되 는 이미지 영역을 전경 이미지로 식별하고, 제1 이미지에서 전경 이미지를 제외한 영역을 배경 이미지로 식별하 는 동작을 포함할 수 있다. 예를 들어, 관심 객체는 3차원 이미지 생성시 전면 레이어에 렌더링할 객체를 나타 낼 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 제1 이미지로부터 전경 이미지 및 배경 이미지를 식별하 는 동작에 대해서는 후술할 도 7을 참조하여 보다 자세히 설명하도록 한다. 단계 S240에서, 전자 장치는 제1 이미지 및 제2 이미지에 기초하여 제1 레이어 이미지의 적어도 일부를 인페인 팅한다. 일 실시예에서, 제1 이미지 및 제2 이미지에 기초하여 제1 레이어 이미지의 적어도 일부를 인페인팅하 는 동작은 제1 레이어 이미지에서 인페인팅 영역을 결정하는 단계, 인페인팅 영역 내의 특정 픽셀에 대해, 제1 이미지 및 제2 이미지 중 특정 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지를 결정하는 단계, 및 결정된 이 미지에 기초하여 특정 픽셀에 대응되는 픽셀 값을 결정하는 단계를 포함할 수 있다. 일 실시예에서, 전자 장치는 제1 이미지 및 제2 이미지에 기초하여 제1 레이어 이미지 및 제2 레이어 이미지 중 적어도 일부를 인페인팅할 수도 있다. 예를 들어, 전자 장치는 제1 레이어 이미지의 적어도 일부를 인페인팅하 거나, 제2 레이어 이미지의 적어도 일부를 인페인팅하거나, 제1 레이어 이미지의 적어도 일부 및 제2 레이어 이 미지의 적어도 일부에 대해 인페인팅 동작을 수행할 수도 있다. 제2 레이어 이미지의 적어도 일부를 인페인팅하 는 동작은 제1 레이어 이미지의 적어도 일부를 인페인팅하는 동작과 유사하게 수행될 수 있다. 예를 들어, 제2 레이어 이미지의 적어도 일부를 인페인팅하는 동작은 제2 레이어 이미지에서 인페인팅 영역을 결정하는 단계, 인페인팅 영역 내의 특정 픽셀에 대해, 제1 이미지 및 제2 이미지 중 특정 픽셀의 픽셀 값을 결정하기 위해 이 용할 이미지를 결정하는 단계, 및 결정된 이미지에 기초하여 특정 픽셀에 대응되는 픽셀 값을 결정하는 단계를 포함할 수 있다. 이하, 전자 장치가 제1 이미지 및 제2 이미지에 기초하여 제1 레이어 이미지의 적어도 일부를 인페인팅하는 경우를 예시적으로 설명하도록 한다. 일 실시예에서, 인페인팅 영역은 제1 레이어 이미지 상에서 전경 이미지에 대응되는 영역의 적어도 일부를 포함 할 수 있다. 인페인팅 영역은 제1 이미지에 대응되는 깊이 맵에 기초하여 결정될 수 있다. 예를 들어, 깊이 맵 상에서 전경 이미지와 배경 이미지의 깊이 값의 차이가 큰 경우 인페인팅 영역이 넓어질 수 있고, 전경 이미지 와 배경 이미지의 깊이 값의 차이가 작은 경우 인페인팅 영역이 좁아질 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 깊이 맵에 기초하여 인페인팅 영역을 결정하는 동작에 대해서는 후술할 도 8을 참조하여 보다 자세 히 설명하도록 한다. 일 실시예에서, 전자 장치는 특정 픽셀의 인페인팅 영역 내의 상대적인 위치에 기초하여, 제1 이미지 및 제2 이 미지 중 해당 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지를 결정할 수 있다. 예를 들어, 인페인팅 영역은 배경 이미지측 경계선(외측 경계선)과, 인페인팅을 수행하지 않는 영역측 경계선(내측 경계선)을 가질 수 있다. 이 때, 픽셀 값을 결정하고자 하는 특정 픽셀이, 외측 경계선에 더 가까운 경우 제2 이미지를 이용해 해당 픽셀 의 픽셀 값을 결정할 수 있고, 내측 경계선에 더 가까운 경우 제1 이미지를 이용해 해당 픽셀의 픽셀 값을 결정 할 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 특정 픽셀의 인페인팅 영역 내의 상대적인 위치에 기초하 여 제1 이미지 및 제2 이미지 중 해당 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지를 결정하는 동작에 대해 서는 후술할 도 9를 참조하여 보다 자세히 설명하도록 한다. 일 실시예에서, 전자 장치는, 인페인팅 영역 내의 특정 픽셀에 대응되는 객체 이미지를 식별하고, 대응되는 객 체 이미지의 전체가 제1 이미지에 포함된 경우, 제1 이미지를 이용하여 해당 픽셀의 픽셀 값을 결정할 수 있다. 또한, 대응되는 객체 이미지의 전체가 제1 이미지에 포함되지 않은 경우, 보다 많은 정보를 포함하는 제2 이미 지를 이용하여 해당 픽셀의 픽셀 값을 결정할 수 있다. 본 개시의 일 실시예에 따른 전자 장치가 제1 이미지 및 제2 이미지에 기초하여 제1 레이어 이미지를 인페인팅하는 동작에 대해서는 후술할 도 8 및 도 9를 참조하여 보 다 자세히 설명하도록 한다. 단계 S250에서, 전자 장치는 인페인팅된 제1 레이어 이미지 및 제2 레이어 이미지에 기초하여 복수의 레이어를 포함하는 3차원 이미지를 생성한다. 단계 S250은 도 1의 단계 140에 대응될 수 있다. 일 실시예에서, 생성된 3 차원 이미지는 제1 레이어 및 제2 레이어를 포함할 수 있다. 제1 레이어는 제2 레이어에 비해 전자 장치로부터 의 깊이 값이 클 수 있다. 전자 장치가 3차원 이미지를 생성하는 동작은 인페인팅된 제1 레이어 이미지를 제1 레이어에 렌더링하고, 제2 레이어 이미지를 제2 레이어에 렌더링하는 동작을 포함할 수 있다. 본 개시의 일 실 시예에 따른 전자 장치가 인페인팅된 제1 레이어 이미지 및 제2 레이어 이미지에 기초하여 복수의 레이어를 포 함하는 3차원 이미지를 생성하는 동작에 대해서는 후술할 도 10을 참조하여 보다 자세히 설명하도록 한다. 도 3은 본 개시의 일 실시예에 따른 전자 장치가 적어도 하나의 2차원 이미지로부터 3차원 이미지를 생성하는 동작을 설명하기 위한 도면이다. 본 개시의 일 실시예에 따른 3차원 이미지는, 깊이 추정(estimation)을 통한 깊이 맵(depth map) 생성 단계, 생 성된 깊이 맵을 이용해 3차원 이미지의 레이어(layer)를 생성하는 단계, 이미지의 적어도 일부분에 대해 픽셀 값을 인페인팅(inpainting)하는 단계, 및 메싱(meshing) 단계를 통해 생성될 수 있다. 단계 310에서, 전자 장치는 적어도 하나의 2차원 이미지를 획득할 수 있다. 전자 장치는, 카메라를 통해 카메라 화각 내의 현실 영역에 대한 2차원 이미지를 촬영할 수 있다. 단계 320에서, 전자 장치는 생성하고자 하는 3차원 이미지에 대응되는 영역에 대해 깊이 맵을 생성할 수 있다. 깊이 맵은 각각의 픽셀 단위에 대응되는 깊이 값을 포함할 수 있다. 일 실시예에서, 전자 장치는 서로 다른 위 치에 배치된 두 개의 카메라를 이용함으로써, 단일한 카메라를 이용하는 경우에 비해 정확한 깊이 맵을 생성할 수 있다. 예를 들어, 전자 장치는 제1 위치에 배치된 제1 카메라와 제2 위치에 배치된 제2 카메라를 포함할 수 있다. 제1 카메라를 통해 촬영된 제1 이미지와 제2 카메라를 통해 촬영된 제2 이미지는 각각 2차원 이미지일 수 있다. 일 실시예에서, 전자 장치는 2차원의 제1 이미지를 3차원 이미지로 변환하고자 할 수 있다. 이 경우, 전자 장치 는 제1 이미지를 입력 값으로 하여 제1 이미지에 포함된 픽셀들에 대한 깊이 값을 출력하도록 훈련된 인공지능 모델을 이용하여, 제1 이미지 만으로부터 깊이 맵을 생성할 수 있다. 또한, 전자 장치는, 제1 이미지 및 제2 이 미지 간의 시차에 기초하여 제1 이미지에 대응되는 깊이 맵을 획득할 수도 있다. 예를 들어, 전자 장치는 제1 이미지의 특정 제1 픽셀의 좌표, 제2 이미지에서 해당 제1 픽셀과 대응되는 제2 픽셀의 좌표, 제1 카메라의 화 각, 제2 카메라의 화각, 및 제1 카메라와 제2 카메라가 배치된 상대적인 위치 정보에 기초하여, 제1 픽셀의 3차 원적 깊이 정보를 획득할 수도 있다. 서로 다른 위치에 배치된 두 개의 카메라를 통해 계산된 깊이 맵은 단일한 카메라를 통해 추정된 깊이 맵에 비해 정확성이 높은 깊이 값을 포함할 수 있다. 단계 330에서, 전자 장치는 3차원 이미지의 레이어를 생성할 수 있다. 3차원 이미지는 두 개 이상의 레이어를 포함할 수 있다. 두 개 이상의 레이어는 상호 평행한 평면으로 구성될 수 있다. 3차원 이미지의 레이어는, 제1 이미지의 각각의 픽셀들에 대한 깊이 정보에 기초하여 생성될 수 있다. 레이어의 개수는 2차원 이미지를 3차원 이미지로 변환하는 연산 시간에 영향을 줄 수 있다. 본 개시의 일 실시예에서, 3차원 이미지는 두 개의 레이어 로 구성될 수 있고, 두 개의 레이어 중 전방 레이어에는 전경 이미지를 포함하는 제2 레이어 이미지가 렌더링되 고, 후방 레이어에는 배경 이미지를 포함하는 제1 레이어 이미지가 렌더링될 수 있다. 단계 340에서, 전자 장치는 인페인팅 동작을 수행할 수 있다. 인페인팅 동작은 값이 지정되지 않은 픽셀에 대해 픽셀 값을 결정하는 동작을 나타낸다. 하나의 레이어에 표시된 2차원 이미지를 복수의 레이어를 갖는 3차원 이 미지로 재구성하는 경우, 전방 레이어로 분리된 후방 레이어 부분은 픽셀 값이 누락되게 된다. 이 경우, 누락된 픽셀의 픽셀 값을 결정하기 위해, 해당 픽셀 주위의 픽셀들의 정보가 이용될 수 있다. 일 실시예에서, 인페인팅 동작은 후방 레이어에 렌더링되는 제1 레이어 이미지에 대해 수행될 수 있다. 한편, 일 실시예에서, 전방 레이 어에 렌더링되는 제2 레이어 이미지의 적어도 일부에 대해서도 인페인팅 동작이 수행될 수도 있다. 일 실시예에서, 전자 장치는 서로 다른 화각을 갖는 두 개의 카메라를 이용함으로써, 정확한 인페인팅 동작을 수행할 수 있다. 예를 들어, 생성하고자 하는 3차원 이미지의 영역을 포함하는 넓은 화각을 갖는 카메라(예를 들어, 광각 카메라 또는 초광각 카메라)를 이용함으로써, 더 넓은 배경 영역의 이미지를 획득할 수 있다. 넓은 화각의 이미지를 이용해 이미지의 적어도 일부를 인페인팅할 경우, 실제와 더욱 일치하는 픽셀 값을 획득할 수 있다. 예를 들어, 전자 장치는 제1 화각을 갖는 제1 카메라와 제2 화각을 갖는 제2 카메라를 포함할 수 있다. 제2 화 각은 제1 화각에 비해 클 수 있다. 제2 카메라가 촬영하는 영역은 제1 카메라가 촬영하는 영역을 포함할 수 있 다. 전자 장치는 제1 카메라를 통해 촬영된 제1 이미지를 3차원 이미지로 변환할 수 있다. 이 경우, 전자 장치 는 제1 이미지보다 넓은 영역이 촬영된 제2 이미지를 이용해 제1 이미지를 인페인팅함으로써, 제1 이미지에서 전경 이미지와 중첩되어 촬영되지 않은 배경 이미지 부분을 보다 정확하게 인페인팅 할 수 있다. 단계 350에서, 메싱 동작을 통해 3차원 이미지가 생성될 수 있다. 예를 들어, 전경 이미지를 포함하는 제2 레이 어 이미지는 제2 레이어에 그대로 렌더링될 수 있다. 일 실시예에서, 제2 레이어 이미지는 3차원 표현으로 재구 성되어 제2 레이어에 렌더링될 수도 있다. 일 실시예에서, 제2 레이어 이미지는 두 개 이상의 레이어에 렌더링 될 수도 있다. 인페인팅된 제1 레이어 이미지는 제1 레이어에 렌더링될 수 있다. 제1 레이어 및 제2 레이어를 수직으로 바라볼 때, 전경 이미지와 제1 레이어 이미지 중 인페인팅된 배경 영역은 중첩(overlap)될 수 있다. 도 4는 본 개시의 일 실시예에 따른 전자 장치에 포함된 복수의 카메라를 설명하기 위한 도면이다. 본 개시의 일 실시예에 따른 전자 장치는 증강 현실 디바이스를 포함할 수 있다. 증강 현실 디바이스는 사 용자가 안면부에 착용하는 안경 형상의 AR 안경(AR glasses), 두부에 착용하는 헤드 마운트 디스플레이(head mounted display, HMD), 가상 현실 헤드셋(virtual reality headset, VRH), 또는 AR 헬멧(AR helmet) 등을 포 함할 수 있다.일 실시예에서, 전자 장치는 주변의 이미지 및 비디오를 캡쳐할 수 있는 하나 이상의 카메라를 포함할 수 있다. 하나 이상의 카메라는 각각 해당 카메라의 화각에 대응되는 영상을 촬영하거나 화각 내에 포함된 객체까 지의 거리를 측정할 수 있다. 일 실시예에서, 카메라는 헤드 트래킹(head tracking) 또는 공간 인식을 위해 사 용될 수도 있다. 또한, 카메라는 전자 장치를 착용한 사용자의 움직임을 인식할 수도 있다. 일 실시예에서, 전자 장치에 포함된 카메라는 사용자의 시야에 대응되는 영상, 예를 들어 3차원 이미지를 획득하기 위해 사용될 수 있다. 전자 장치는 외향 카메라를 통해 획득한 실제 공간과 관련된 영상 정보 중 사용자의 시야로 판단되는 영역에 대응하는 적어도 일부에 포함되는 외부 객체를 확인할 수 있다. 전자 장치 는 적어도 일부에서 확인한 외부 객체와 관련된 3차원 이미지를 전자 장치의 표시 영역(디스플레이) 을 통해 출력할 수 있다. 일 실시예에서, 전자 장치는 복수의 카메라를 포함할 수 있다. 전자 장치에 포함된 카메라들을 각각 2차원 이미지를 획득할 수 있다. 전자 장치는 복수의 카메라를 이용하여 전방 영역 또는 물체의 3차원 이 미지를 획득할 수 있다. 예를 들어, 전자 장치에 포함된 복수의 카메라들 중 두 개 이상의 카메라를 통해 3차원 이미지가 획득될 수 있다. 도 4를 참조하면, 일 실시예에서 헤드 마운트 디바이스 타입의 전자 장치는 사용자의 두 눈 사이에 배치된 두 개의 카메라와 사용자의 두 눈 바깥쪽에 배치된 두 개의 카메라를 포함할 수 있다. 실시예 1에서, 전자 장치 는 사용자의 두 눈 사이에 배치된 두 개의 카메라를 이용하여 전방 영역의 3차원 이미지를 획득할 수 있다. 실시예 2에서, 전자 장치는 사용자의 두 눈 사이에 배치된 카메라 중 하나와 사용자의 두 눈 바깥쪽 에 배치된 카메라 중 하나를 이용하여 전방 영역의 3차원 이미지를 획득할 수 있다. 실시예 3에서, 전자 장치 는 사용자의 두 눈 바깥쪽에 배치된 두 개의 카메라를 이용하여 전방 영역의 3차원 이미지를 획득할 수 있 다. 실시예 4에서, 전자 장치는 사용자의 두 눈 사이에 배치된 카메라 중 하나와 사용자의 두 눈 바깥쪽에 배치된 두 개의 카메라를 이용하여 전방 영역의 3차원 이미지를 획득할 수 있다. 전자 장치가 3차원 이미 지를 생성하기 위해 이용하는 다양한 카메라들은 서로 다른 화각을 가지거나, 적어도 일부 영역에서 중첩되는 촬영 영역을 가질 수 있다. 3차원 이미지를 획득하기 위해 3개의 카메라를 이용하는 실시예 4의 경우, 두 개의 카메라를 이용하는 실시예 1 내지 3에 비해 생성된 3차원 이미지에 포함되는 아티팩트(artifact)가 적을 수 있 다. 본 개시의 일 실시예에 따른 전자 장치는 전술한 구성으로 제한되는 것은 아니며, 다양한 화각의 카메라를 다양한 위치에 다양한 개수로 포함할 수 있다. 예를 들어, 망원 카메라, 광각 카메라, 초광각 카메라를 포함하 는 휴대폰에도 본 개시의 일 실시예에 따른 복수의 카메라를 이용하여 3차원 이미지를 생성하는 방법이 적용될 수 있을 것이다. 또한, 가상 현실 글라스(AR glass), HMD(head mount display), 스마트폰(smart phone) 등 복 수의 카메라가 실장된 다양한 전자 장치에서 복수의 카메라들의 다양한 조합을 통해 3차원 이미지가 생성될 수 있다. 도 5a 및 도 5b는 본 개시의 일 실시예에 따른 전자 장치에 포함된 복수의 카메라의 화각(Field of view, FOV) 을 설명하기 위한 도면이다. 전자 장치가 3차원 이미지를 생성하기 위해 이용하는 복수의 카메라들은 서로 다른 화각을 가지거나, 적어도 일 부 영역에서 중첩되는 촬영 영역을 가질 수 있다. 일 실시예에서 전자 장치는 3차원 이미지를 생성하기 위해 2개의 카메라(제1 카메라 및 제2 카메라)를 이용할 수 있다. 제1 카메라를 통해 제1 이미지(IM1)가 획득될 수 있고, 제2 카메라를 통해 제2 이미지(IM2)가 획득될 수 있다. 하나의 카메라(제1 카메라 또는 제2 카메라)를 통해 획득된 이미지는 2차원 이미지일 수 있고, 따라서 제1 이미지(IM1) 및 제2 이미지(IM2)는 2차원의 평면 이미지일 수 있다. 카메라의 화각(FOV)은 '카메라 렌즈의 시야가 닿는 영역'을 나타낼 수 있다. 화각(FOV)은 시야각(degree of FOV, DFOV)으로 표현될 수도 있다. 시야각(DFOV)은 수평 시야각(HFOV) 및 수직 시야각(VFOV)으로 구성될 수 있 다. 수평 시야각(HFOV)은 카메라가 고정된 상태로 촬영할 수 있는 화면의 좌측 끝에서 우측 끝까지의 각도일 수 있다. 수직 시야각(VFOV)은 카메라가 고정된 상태로 촬영할 수 있는 화면의 위 끝에서 아래 끝까지의 각도일 수 있다. 시야각(DFOV)은 카메라가 고정된 상태로 촬영할 수 있는 화면의 상단 좌측 끝에서, 하단 우측 끝까지의 각도를 의미할 수 있다. 예를 들어, 시야각(DFOV)이 90도인 카메라는 전방 90도 영역을 이미지로 촬영할 수 있 다. 카메라의 시야각(DFOV)은 일정한 수치를 가질 수 있다. 따라서, 카메라가 촬영 가능한 면적은, 카메라로부터 거 리가 멀수록 증가할 수 있고, 카메라로부터 거리가 가까울수록 감소할 수 있다. 카메라로부터 가까이 위치하는 물체는 촬영된 이미지 상 크게 표시될 수 있고, 카메라로부터 멀리 위치하는 물체는 촬영된 이미지 상 작게 표 시될 수 있다. 한편, 카메라는 줌 인(zoom in) 또는 줌 아웃(zoom out) 동작을 수행할 수 있다. 줌 인 동작의 경우 카메라의 시야각(DFOV)을 좁히는 동작에 해당하고, 줌 아웃 동작의 경우 카메라의 시야각(DFOV)을 넓히는 동작에 해당할 수 있다. 카메라의 줌 인 및 줌 아웃은 최소 시야각(DFOV) 내지 최대 시야각(DFOV)의 범위 내에서 이루어질 수 있다. 카메라의 화각이 넓은 경우, 더 넓은 영역의 촬영이 가능하다. 일 실시예에서, 제2 카메라가 제1 카메라에 비해 넓은 화각을 가질 수 있고, 제2 카메라를 통해 촬영된 제2 이미지(IM2)가 제1 카메라를 통해 촬영된 제1 이미지 (IM1)에 비해 넓은 영역을 포함할 수 있다. 도 5a를 참조하면, 넓은 화각의 카메라로 촬영된 제2 이미지(IM2)는 좁은 화각의 카메라로 촬영된 제1 이미지 (IM1)를 포함할 수 있다. 일 실시예에서, 하나의 이미지가 다른 이미지를 '포함'한다는 것은, 두 이미지의 축척 (scale)이 동일할 때, 하나의 이미지에 다른 이미지와 대응되는 부분이 포함되는 관계를 나타낸다. 도 5a를 참 조하면, 제2 카메라를 통해 획득된 제2 이미지(IM2)를 제1 이미지(IM1)와 동일한 축척을 갖도록 확대할 경우, 제1 이미지(IM1)에 대응되는 영역이 확대된 제2 이미지(IM2)에 포함될 수 있다. 도 5b를 참조하면, 제1 이미지(IM1)와 제2 이미지(IM2)는 적어도 일부 영역에서 중첩될 수 있다. 일 실시예에서, 카메라를 통해 촬영되는 영역은 해당 카메라가 배치된 위치 및 해당 카메라의 화각에 기초하여 결 정될 수 있다. 제1 카메라와 제2 카메라는 각각의 카메라가 전자 장치 상에 배치된 위치 및 각각의 카메라의 화 각에 기초하여 촬영 영역을 가질 수 있다. 일 실시예에서, 서로 다른 위치에 배치된 두 개의 카메라는 일부 영 역에서 중첩되는 촬영 영역을 가질 수 있고, 그러한 두 개의 카메라를 통해 획득된 제1 이미지(IM1) 및 제2 이 미지(IM2)는 적어도 일부 영역에서 중첩될 수 있다. 도 5b에서는 제2 이미지(IM2)의 화각이 제1 이미지(IM1)의 화각에 비해 넓은 경우를 도시하고 있으나, 본 개시의 일 실시예에 따른 복수의 카메라는 동일한 크기의 화각을 가질 수도 있다. 도 6은 본 개시의 일 실시예에 따른 전자 장치가 복수의 2차원 이미지로부터 깊이 맵(DM)을 획득하는 동작을 설 명하기 위한 도면이다. 본 개시의 일 실시예에 따른 전자 장치는 2차원의 제2 이미지(IM2)에 기초하여, 2차원의 제1 이미지(IM1)를 3차 원 이미지로 재구성할 수 있다. 이러한 실시예는 전술한 도 1에서 전자 장치가 생성하고자 하는 3차원 이미지의 화각 a3이 제1 이미지(IM1)의 화각 a1인 경우(a1=a3)에 대응될 수 있다. 도 6을 참조하면, 전자 장치는 내장된 제1 카메라를 통해 제1 이미지(IM1)를 획득하고, 내장된 제2 카메라를 통 해, 제1 이미지(IM1)를 포함하며 제1 이미지(IM1)보다 넓은 화각을 갖는 제2 이미지(IM2)를 획득할 수 있다. 단계 615에서, 전자 장치는 제2 이미지(IM2)를 제1 이미지(IM1)의 화각 a1에 기초하여 전처리할 수 있다. 제2 이미지(IM2)를 전처리 하는 동작은, 제2 이미지(IM2)의 이미지 축을 제1 이미지(IM1)의 이미지 축과 동일하도록 조절하는 동작, 제2 이미지(IM2)를 화각 a1에 맞추어 자르는 동작, 및 a1의 화각을 갖도록 잘린 제2 이미지(IM 2)가 제1 이미지(IM1)와 동일한 크기가 되도록 확대하는 동작을 포함할 수 있다. 전처리된 제2 이미지(IM2-P)는 제1 이미지(IM1)와 동일한 이미지 축을 갖고, 동일한 영역에 대한 이미지에 대응 될 수 있다. 예를 들어, 전처리된 제2 이미지(IM2-P)의 모든 픽셀들은 제1 이미지(IM1)에 포함된 모든 픽셀들과 1대1로 대응될 수 있다. 예를 들어, 제1 이미지(IM1) 및 전처리된 제2 이미지(IM2-P)는 동일한 객체에 대응되는 이미지를 포함할 수 있고, 제1 이미지(IM1) 및 전처리된 제2 이미지(IM2-P)에 포함되는 객체 이미지는 동일하거 나 비슷한 크기를 가질 수 있다. 전자 장치는, 제1 이미지(IM1) 및 전처리된 제2 이미지(IM2-P) 간의 시차에 기초하여, 제1 이미지(IM1)에 포함 된 각각의 픽셀에 대한 깊이 값을 계산할 수 있다. 예를 들어, 깊이 값을 계산하는 동작에는 스테레오(stereo) 매칭, 삼각측량법, 닮음꼴 삼각형에 대한 비례 공식, 또는 딥 러닝(deep learning) 기법이 이용될 수 있다. 전자 장치는, 2차원의 제1 이미지(IM1)에 포함된 제1 픽셀들에 대해 2차원 위치 좌표를 획득하고, 2차원의 전처 리된 제2 이미지(IM2-P)에 포함된 제2 픽셀들에 대해 2차원 위치 좌표를 획득할 수 있다. 이후, 전자 장치는 특 정 제1 픽셀의 2차원 위치 좌표, 제1 픽셀에 대응되는 제2 픽셀의 2차원 위치 좌표, 제1 카메라의 화각, 제2 카메라의 화각, 및 제1 카메라와 제2 카메라가 배치된 상대적인 위치 정보(예를 들어, 베이스라인(baseline) 길이)에 기초하여 제1 픽셀들의 깊이 값을 획득할 수 있다. 일 실시예에서, 제1 이미지(IM1) 및 전처리된 제2 이미지(IM2-P) 간의 시차에 기초하여 계산된 깊이 맵(DM)은, 단일한 제1 이미지(IM)에 인공지능 모델을 적용하여 추정된 깊이 맵(DM)에 비해 정확성이 높은 깊이 값을 포함 할 수 있다. 도 7a는 본 개시의 일 실시예에 따른 전자 장치가 전경 이미지(foreground image, FG) 및 배경 이미지 (background image, BG)를 식별하는 동작을 설명하기 위한 도면이고, 도 7b는 본 개시의 일 실시예에 따른 전자 장치가 제1 레이어 이미지(LI1) 및 제2 레이어 이미지(LI2)를 식별하는 동작을 설명하기 위한 도면이다. 본 개시의 일 실시예에 따른 전자 장치는 2차원의 제2 이미지에 기초하여, 2차원의 제1 이미지를 3차원 이미지 로 재구성할 수 있다. 이러한 실시예는 전술한 도 1에서 전자 장치가 생성하고자 하는 3차원 이미지의 화각 a3이 제1 이미지의 화각 a1인 경우(a1=a3)에 대응될 수 있다. 도 7a를 참조하면, 전자 장치는 깊이 맵에 포함된 제1 이미지의 픽셀 별 깊이 정보에 기초하여 제1 이미지로부 터 전경 이미지(FG) 및 배경 이미지(BG)를 식별할 수 있다. 일 실시예에서, 전자 장치는 관심 객체(target object)에 대응되는 이미지 영역을 전경 이미지(FG)로 식별하고, 제1 이미지에서 전경 이미지(FG)를 제외한 영 역을 배경 이미지(BG)로 식별할 수 있다. 예를 들어, 관심 객체는 3차원 이미지 생성시 전면 레이어(layer)에 이미지를 렌더링(rendering)할 객체를 나타낼 수 있다. 도 7a를 참조하면 관심 객체는 오리일 수 있고, 이 때, 전경 이미지(FG)는 제1 이미지 중 오리 이미지에 해당하는 픽셀들을 포함하고, 배경 이미지(BG)는 제1 이미지에 서 오리 이미지 부분을 제외한 나머지 영역(예를 들어, 땅, 물 등)에 대응될 수 있다. 제1 이미지 상에서, 서로 다른 객체에 대응되는 이미지의 경계 부분에서는 인접한 픽셀들 간의 깊이 값이 급격 하게 변화하거나, 불연속적인 깊이 값이 나타날 수 있다. 따라서, 제1 이미지에 대응되는 깊이 맵을 참조하면, 제1 이미지에 포함된 객체들의 에지(edge)에 관련된 정보를 획득할 수 있다. 일 실시예에서, 전자 장치는 깊이 맵에 포함된 제1 이미지의 픽셀 별 깊이 정보에 기초하여, 제1 이미지로부터 전경 이미지(FG) 및 배경 이미지 (BG)를 구분할 수 있다. 도 7b를 참조하면, 전자 장치는 제1 이미지로부터 전경 이미지(FG)를 포함하는 제2 레이어 이미지(LI2) 및 배경 이미지(BG)를 포함하는 제1 레이어 이미지(LI1)를 식별할 수 있다. 제1 레이어 이미지(LI1) 및 제2 레이어 이미 지(LI2)는 각각 제1 이미지와 동일한 크기를 가질 수 있다. 도 7b를 참조하면, 제2 레이어 이미지(LI2)는 전경 이미지(FG)에 대응되는 영역에 포함된 픽셀들에 대해서만 픽셀 값을 가질 수 있다. 제1 레이어 이미지(LI1)는 배경 이미지(BG)에 대응되는 영역에 포함된 픽셀들, 즉, 관심 객체에 대응되는 영역을 제외한 영역에 포함된 픽 셀들에 대해서만 픽셀 값을 가질 수 있다. 제1 레이어 이미지(LI1) 또는 제2 레이어 이미지(LI2)에서 값을 가지 지 않는 픽셀은 이미지 렌더링 시 투명하게 표현될 수 있다. 제1 레이어 이미지(LI1)와 제2 레이어 이미지(LI 2)의 결합은 제1 이미지를 구성할 수 있다. 제1 이미지에 포함되는 픽셀은 제1 레이어 이미지(LI1) 및 제2 레이 어 이미지(LI2) 중 적어도 하나에 포함될 수 있다. 도 8은 본 개시의 일 실시예에 따른 전자 장치가 이미지의 적어도 일부를 인페인팅하는 동작을 설명하기 위한 도면이다. 본 개시의 일 실시예에 따른 전자 장치는 2차원의 제2 이미지에 기초하여, 2차원의 제1 이미지를 3차원 이미지 로 재구성할 수 있다. 이러한 실시예는 전술한 도 1에서 전자 장치가 생성하고자 하는 3차원 이미지의 화각 a3이 제1 이미지의 화각 a1인 경우(a1=a3)에 대응될 수 있다. 도 8을 참조하면, 전자 장치는 제1 이미지 및 제2 이미지에 기초하여 이미지의 적어도 일부를 인페인팅할 수 있 다. 일 실시예에서, 전자 장치는 제1 레이어 이미지(LI1)의 적어도 일부를 인페인팅 영역으로 결정하고, 결정된 인페인팅 영역 내의 특정 픽셀에 대해, 제1 이미지나 제2 이미지 중 해당 픽셀의 픽셀 값을 결정하 기 위해 이용할 이미지를 결정할 수 있다. 인페인팅 영역은 제1 레이어 이미지(LI1) 상에서, 배경 이미지(BG)를 포함하고, 전경 이미지(FG)에 대응되 는 영역의 적어도 일부를 포함할 수 있다. 도 8을 참조하면, 인페인팅 영역은 외측 경계선 및 내측 경계선에 기초하여 식별될 수 있다. 예를 들어, 인페인팅 영역은 외측 경계선 및 내측 경계선 사이에 배치된 픽셀들을 포함하는 프레임 형태를 가질 수 있다. 인페인팅 영역의 외측 경계선 은, 제1 이미지에서 배경 이미지(BG)와 전경 이미지(FG) 사이의 경계선으로 나타낼 수 있다. 예를 들어, 외측경계선은 전경 이미지(FG)에 대응되는 관심 객체 이미지의 에지(edge)에 대응될 수 있다. 인페인팅 영역 의 내측 경계선을 기준으로 인페인팅 동작이 수행되는 픽셀과 인페인팅 동작이 수행되지 않는 픽셀이 구별될 수 있다. 일 실시예에서, 인페인팅 영역은 제1 이미지에 대응되는 깊이 맵에 기초하여 결정될 수 있다. 예를 들어, 깊이 맵 상에서 전경 이미지(FG)와 배경 이미지(BG)의 깊이 값의 차이가 큰 경우 인페인팅 영역이 넓어질 수 있고, 전경 이미지(FG)와 배경 이미지(BG)의 깊이 값의 차이가 작은 경우 인페인팅 영역이 좁아질 수 있다. 전경 이미지(FG)와 배경 이미지(BG) 간의 깊이 값 차이가 클 경우, 사용자가 3차원 이미지를 바라보는 각도가 조금 변경된 경우에도 배경 이미지(BG) 상에서 전경 이미지(FG)가 상대적으로 크게 움직일 수 있다. 따라서, 이 러한 경우 보다 실감나는 3차원 이미지를 생성하기 위해 이미지의 넓은 부분을 인페인팅할 수 있다. 예를 들어, 인페인팅 영역의 외측 경계선 및 내측 경계선 사이의 수직 선상 또는 수평 선상 포함되는 픽셀 은 각각 N개일 수 있다. 전경 이미지(FG)와 배경 이미지(BG) 간의 깊이 값 차이가 작을 경우, 사용자가 3차원 이미지를 바라보는 각도가 크게 변경되더라도 배경 이미지(BG) 상에서 전경 이미지(FG)가 움직이는 폭이 상대적으로 작을 수 있다. 따라서, 이러한 경우 이미지의 비교적 좁은 부분만을 인페인팅하더라도, 실감나는 3차원 이미지를 생성할 수 있 다. 예를 들어, 인페인팅 영역의 외측 경계선 및 내측 경계선 사이의 수직 선상 또는 수평 선상 포함되는 픽셀은 각각 n개이며, n은 N보다 작을 수 있다. (N≥n) 본 개시의 일 실시예에 따른 3차원 이미지의 사용자 시선 변화에 대한 임계 각도는 약 10도(degree) 일 수 있다. 이 경우, 제1 레이어 이미지(LI1) 중 전경 이미지(FG)에 가려 촬영되지 않은 배경 이미지(BG) 부분 (예를 들어, 제1 레이어 이미지(LI1) 상 픽셀 값이 존재하지 않는 영역 또는 외측 경계선 내부의 전체 영역) 전 부가 인페인팅 영역이 될 필요는 없을 것이다. 도 9는 본 개시의 일 실시예에 따른 전자 장치가 복수의 2차원 이미지를 이용하여 이미지를 인페인팅하는 동작 을 설명하기 위한 도면이다. 도 9를 참조하면, 3차원 이미지는 전경 이미지(FG)를 포함하는 제2 레이어 이미지가 렌더링된 제2 레이어 및 배 경 이미지(BG)를 포함하는 인페인팅된 제1 레이어 이미지가 렌더링된 제1 레이어를 포함할 수 있다. 사용자가 3 차원 이미지를 바라보는 각도는 임계 각도 이상으로 변화할 수 있다. 사용자가 3차원 이미지를 바라보는 각도가 이미지 평면의 수직 방향으로부터 변화하는 경우, 사용자의 눈으로부터 서로 다른 깊이를 갖는 레이어에 렌더링 되는 제1 레이어 이미지와 제2 레이어 이미지는 이미지 평면의 수평 방향으로 어긋나게 인식될 수 있다. 예를 들어, 도 9는 사용자가 이미지를 정면에 비해 상부 우측에서 바라본 경우를 도시한다. 일 실시예에서, 사용자가 3차원 이미지를 정면에서 바라본 경우 전경 이미지(FG)의 외측 경계선이 인페인팅 영 역의 외측 경계선 상 중첩되어 나타날 수 있다. 즉, 사용자가 3차원 이미지를 정면에서 바라보는 경우, 인 페인팅 영역이 전혀 보이지 않을 수 있다. 사용자의 시선 방향이 정면으로부터 임계 각도까지 변하는 경우, 전경 이미지(FG)의 외측 경계선이 인페인팅 영역의 외측 경계선에서 내측 경계선까지 이동할 수 있 다. 이 경우, 3차원 이미지는 인페인팅 영역의 외측 경계선으로부터 전경 이미지의 외측 경계선까지의 부 분에 인페인팅 영역을 표시할 수 있다. 사용자의 시선 변화가 임계 각도 이상으로 변하는 경우, 3차원 이 미지는 인페인팅 영역의 내측 경계선이 전경 이미지(FG)의 외측 경계선에 고정된 2차원 이미지와 같이 표 시될 수 있다. 일 실시예에서, 전자 장치는 특정 픽셀의 인페인팅 영역 내의 상대적인 위치에 기초하여, 제1 이미지 및 제2 이미지 중 해당 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지를 결정할 수 있다. 도 9의 인페인팅 영역 은 전술한 도 8의 인페인팅 영역의 일부를 나타낼 수 있다. 도 9에 도시된 일 실시예는 사용자가 3차 원 이미지를 바라보는 시선 각도가 임계 각도 이상인 경우를 나타낸다. 인페인팅 영역의 배경 이미지(BG)측 경계선을 외측 경계선으로 나타내고, 인페인팅을 수행하지 않는 영역 측 경계선을 내측 경계선으로 나타낼 수 있다. 예를 들어, 픽셀 값을 결정하고자 하는 제1 픽셀(P1)이, 내측 경 계선보다 외측 경계선에 더 가까운 경우, 제1 픽셀(P1)은 '배경' 영역에 대응된다고 볼 수 있다. 따라서, 이 경 우, 화각이 작은 제1 이미지에 비해 '배경' 영역에 대한 많은 정보를 포함하는 제2 이미지를 제1 픽셀(P1)의 픽 셀 값을 결정하기 위해 이용할 이미지로 결정할 수 있다. 또한, 예를 들어, 픽셀 값을 결정하고자 하는 제2 픽 셀(P2)이, 외측 경계선보다 내측 경계선에 더 가까운 경우, 제2 픽셀(P2)은 '전경' 영역 (관심 객체)에 대응된다고 볼 수 있다. 따라서, 이 경우, 화각이 작아 비교적 높은 해상도의 관심 객체 이미지를 포함하는 제1 이미 지를 제2 픽셀(P2)의 픽셀 값을 결정하기 위해 이용할 이미지로 결정할 수 있다. 한편, 제1 이미지 및 제2 이미지 중 인페인팅 영역 내의 특정 픽셀의 픽셀 값을 결정하기 위해 이용할 이 미지를 결정하는 동작은 전술한 방법에 한정되지 않으며, 다양한 방법에 의해 결정될 수 있다. 일 실시예에서, 전자 장치는 특정 픽셀에 대응되는 객체(object) 이미지를 식별하고, 대응되는 객체 이미지의 전체가 제1 이미지에 포함된 경우, 제1 이미지를 해당 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지로 결정하 고, 대응되는 객체 이미지의 전체가 제1 이미지에 포함되지 않은 경우, 제2 이미지를 해당 픽셀의 픽셀 값을 결 정하기 위해 이용할 이미지로 결정할 수도 있다. 예를 들어, 특정 픽셀에 대응되는 객체가 제1 이미지 내에 모두 포함된 경우, 해당 픽셀의 픽셀 값을 결정하기 위한 정보는 제1 이미지에 충분히 포함될 수 있다. 따라서 이 경우, 제1 이미지를 해당 픽셀의 픽셀 값을 결정 하기 위해 이용할 이미지로 결정하여도 정확도 높은 픽셀 값을 획득할 수 있다. 특정 픽셀에 대응되는 객체가 제1 이미지 내에 모두 포함되지 않는 경우, 해당 객체의 이미지는 화각이 제1 이미지보다 넓은 제2 이미지에서 더 많은 픽셀에 걸쳐 포함될 수 있다. 즉, 해당 픽셀의 픽셀 값을 결정하기 위한 정보는 제1 이미지보다 제2 이 미지에 더 많이 포함될 수 있다. 따라서 이 경우, 제2 이미지를 해당 픽셀의 픽셀 값을 결정하기 위해 이용할 이미지로 결정할 수 있다. 도 10은 본 개시의 일 실시예에 따른 전자 장치가 복수의 레이어(L1, L2)를 포함하는 3차원 이미지를 생성하는 동작을 설명하기 위한 도면이다. 본 개시의 일 실시예에 따른 전자 장치는 두 개의 레이어(L1, L2)로 구성된 3차원 이미지를 생성할 수 있다. 두 개의 레이어 중 제2 레이어(L2)에는 전경 이미지(FG)를 포함하는 제2 레이어 이미지(LI2)가 렌더링될 수 있고, 제1 레이어(L1)에는 배경 이미지(BG)를 포함하는 인페인팅된 제1 레이어 이미지(LI1)가 렌더링될 수 있다. 제1 레이어(L1)의 평면과 제2 레이어(L2)의 평면은 평행 관계를 가질 수 있다. 레이어의 깊이는 사용자의 눈으로부터 해당 레이어까지의 수직 거리를 나타낸다. 따라서, 두 개의 레이어 중 깊 이가 작은 레이어가 전방에 배치된다. 도 10을 참조하면, 제2 레이어(L2)의 깊이가 제1 레이어(L1)의 깊이에 비 해 작을 수 있다. 전방에 배치된 제2 레이어(L2)의 픽셀 값이 후방에 배치된 제1 레이어(L1)의 픽셀 값에 우선 할 수 있다. 예를 들어, 사용자의 시선 방향에서 중첩되는 제1 레이어(L1)의 제1 픽셀과 제2 레이어(L2)의 제2 픽셀 중 사용자는 제2 픽셀에 대한 픽셀 값을 인식할 수 있다. 일 실시예에서, 전자 장치는 3개 이상의 레이어들을 포함하는 3차원 이미지를 생성할 수도 있다. 3차원 이미지 에 포함된 복수의 레이어들 중 적어도 하나의 레이어에 전경 이미지(FG)를 포함하는 제2 레이어 이미지(LI2)가 렌더링될 수 있고, 적어도 하나의 레이어에 배경 이미지(BG)를 포함하는 제1 레이어 이미지(LI1)가 렌더링될 수 있다. 도 11은 본 개시의 일 실시예에 따른 전자 장치의 블록도이다. 전자 장치는 2차원의 입력 영상 또는 입력 이미지를 획득하고, 3차원의 출력 영상 또는 출력 이미지를 출 력하는 장치일 수 있다. 전자 장치는 예를 들어, 스마트 폰(smartphone), 태블릿 PC(tablet personal computer), 이동 전화기(mobile phone), 영상 전화기, 전자책 리더기(e-book reader), 데스크탑 PC(desktop personal computer), 랩탑 PC(laptop personal computer), 넷북 컴퓨터(netbook computer), 워크스테이션 (workstation), 서버, PDA(personal digital assistant), PMP(portable multimedia player), MP3 플레이어, 모바일 의료기기, 카메라(camera), 웨어러블 장치(wearable device), 가전기기 또는 다양한 컴퓨팅 장치로 구성 될 수 있다. 한편, 본 개시의 일 실시예에 따른 전자 장치가 전술한 예시로 한정되는 것은 아니며, 전자 장치는 복수의 카메라를 포함하고, 영상 또는 이미지를 획득 및 처리하여 출력하는 다양한 종류의 기기를 포함할 수 있다. 도 11을 참조하면, 전자 장치는 제1 카메라, 제2 카메라, 프로세서, 및 저장부(113 0)를 포함할 수 있다. 도 11에 도시된 구성 요소 모두가 전자 장치의 필수 구성 요소인 것은 아니다. 도 11에 도시된 구성 요소보다 많은 구성 요소들에 의해 전자 장치가 구현될 수도 있고, 도 11에 도시된 구 성 요소보다 적은 구성 요소에 의해 전자 장치가 구현될 수도 있다. 제1 카메라 및 제2 카메라는 디지털 촬영 장치를 포함할 수 있다. 일 실시예에서, 제1 카메라 및 제2 카메라는 각각 입력 영상 또는 입력 이미지를 획득할 수 있다. 일 실시예에서, 제1 카메라 및 제2 카메라는 각각의 화각에 대응되는 영상을 촬영할 수 있다. 일 실시예에서, 제1 카메라 및 제2 카메라는 각각 2차원 이미지를 획득할 수 있다. 전자 장치 는 제1 카메라 및 제2 카메라를 이용하여 전방 영역 또는 물체의 3차원 이미지를 획득할 수 있다. 제1 카메라 및 제2 카메라는 각각 망원 카메라, 광각 카메라, 또는 초광각 카메라로 구성될 수 있다. 일 실시예에서, 제1 카메라 및 제2 카메라는 서로 다른 화각을 가질 수 있다. 예를 들어, 제1 카메라의 화각이 a-1이고, 제2 카메라의 화각이 a2일 때, a-1은 a2보다 작거나 같을 수 있다. (a1≤a2) 이 경우, 제2 카메라를 통해 획득한 제2 이미지가 제1 카메라를 통해 획득한 제1 이미지 에 비해 넓은 영역에 대응되는 정보를 포함할 수 있다. 예를 들어, 제1 카메라를 통해 획득한 제1 이미지 에 대응되는 현실 영역은 제2 카메라를 통해 획득한 제2 이미지에 대응되는 현실 영역에 포함될 수 있다. 저장부는 전자 장치의 동작을 제어하기 위해 후술할 프로세서에 의해 실행될 프로그램을 저 장할 수 있다. 저장부는 전자 장치의 동작을 제어하기 위한 적어도 하나의 명령어들(instruction s)을 포함하는 프로그램을 저장할 수 있다. 저장부에는 프로세서가 판독할 수 있는 명령어들 및 프 로그램 코드(program code)가 저장될 수 있다. 일 실시예에서, 프로세서는 저장부에 저장된 프로그 램의 명령어들 또는 코드들을 실행하도록 구현될 수 있다. 저장부는 전자 장치로 입력되거나 전자 장치로부터 출력되는 데이터를 저장할 수 있다. 저장부는 예를 들어, 플래시 저장부(flash memory), 하드디스크(hard disk), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 저장부(예를 들어, SD 또는 XD 저장부 등), 램(RAM, Random Access Memory), SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 저장부, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장 매체를 포함할 수 있다. 다만 저장부는 전술한 예시로 한정되는 것은 아니며, 데이터가 저장될 수 있는 모든 종류의 저장 매체를 포함할 수 있다. 프로세서는, 전자 장치의 전반적인 동작을 제어할 수 있다. 예를 들어, 프로세서는 저장부 에 저장된 프로그램들을 실행함으로써, 제1 카메라, 제2 카메라, 및 저장부 등을 전반 적으로 제어할 수 있다. 프로세서는 산술, 로직 및 입출력 연산과 시그널 프로세싱을 수행하는 하드웨어 구성 요소로 구성될 수 있다. 프로세서는 예를 들어, 중앙 처리 장치(Central Processing Unit), 마이크로 프로세서 (microprocessor), 그래픽 프로세서(Graphic Processing Unit), ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), 및 FPGAs(Field Programmable Gate Arrays) 중 적어도 하나로 구성될 수 있으나, 이에 제한되는 것은 아니다. 프로세서는, 저장부에 저장된 적어도 하나의 명령어들을 실행함으로써, 제2 카메라를 통해 획득된 2차원의 제2 이미지에 기초하여, 제1 카메라를 통해 획득된 2차원의 제1 이미지를 3차원 이미지로 재구성할 수 있다. 예를 들어 프로세서는, 저장부에 저장된 적어도 하나의 명령어들을 실행함으로 써, 제1 카메라를 통해 제1 이미지를 획득하고 제2 카메라를 통해 제2 이미지를 획득하고, 제1 이 미지에 포함된 픽셀의 깊이 정보를 획득하고, 깊이 정보에 기초하여 제1 이미지로부터 제1 레이어 이미지 및 제 2 레이어 이미지를 식별하고, 제1 이미지 및 제2 이미지에 기초하여 제1 레이어 이미지의 적어도 일부를 인페인 팅하고, 제2 레이어 이미지 및 인페인팅된 제1 레이어 이미지에 기초하여 복수의 레이어를 포함하는 3차원 이미 지를 생성할 수 있다. 프로세서가 제1 카메라를 통해 제1 이미지를 획득하고 제2 카메라를 통해 제2 이미지를 획득 하는 동작은 전술한 도 2의 단계 S210에 대응될 수 있다. 프로세서가 제1 이미지에 포함된 픽셀의 깊이 정보를 획득하는 동작은 전술한 도 2의 단계 S220에 대응될 수 있다. 프로세서가 깊이 정보에 기초하여 제1 이미지로부터 제1 레이어 이미지 및 제2 레이어 이미지를 식별하는 동작은 전술한 도 2의 단계 S230에 대응 될 수 있다. 프로세서가 제1 이미지 및 제2 이미지에 기초하여 제1 레이어 이미지의 적어도 일부를 인페 인팅하는 동작은 전술한 도 2의 단계 S240에 대응될 수 있다. 프로세서가 제2 레이어 이미지 및 인페인팅 된 제1 레이어 이미지에 기초하여 복수의 레이어를 포함하는 3차원 이미지를 생성하는 동작은 전술한 도 2의 단 계 S250에 대응될 수 있다. 이와 같이, 본 개시의 일 실시예에 따르면, 서로 다른 화각을 갖는 복수의 카메라를 이용하여 획득된 복수의 이 미지로부터 3차원 이미지를 생성함으로써, 이미지의 인페인팅 동작에 보다 많은 픽셀 정보를 이용할 수 있고, 전경 이미지와 배경 이미지의 경계 부분에서 발생하는 아티팩트를 줄일 수 있어 현실감 있는 3차원 이미지를 생 성할 수 있다. 본 개시의 다양한 실시예들은 하나 이상의 컴퓨터 프로그램들에 의해 구현 또는 지원될 수 있고, 컴퓨터 프로그 램들은 컴퓨터 판독 가능한 프로그램 코드(code)로부터 형성되고, 컴퓨터로 판독 가능한 매체에 수록될 수 있다. 본 개시에서, \"애플리케이션(application)\" 및 \"프로그램(program)\"은 컴퓨터 판독 가능한 프로그램 코드 에서의 구현에 적합한 하나 이상의 컴퓨터 프로그램, 소프트웨어 컴포넌트, 명령어 세트, 프로시저(procedure), 함수, 개체(object), 클래스, 인스턴스, 관련 데이터, 또는 그것의 일부를 나타낼 수 있다. \"컴퓨터 판독 가능 한 프로그램 코드\"는, 소스 코드, 목적 코드, 및 실행 가능한 코드를 포함하는 다양한 유형의 컴퓨터 코드를 포 함할 수 있다. \"컴퓨터 판독 가능한 매체\"는, ROM(read only memory), RAM(random access memory), 하드 디스 크 드라이브(HDD), CD(compact disc), DVD(digital video disc), 또는 다양한 유형의 메모리와 같이, 컴퓨터에 의해 액세스될 수 있는 다양한 유형의 매체를 포함할 수 있다. 또한, 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장 매체의 형태로 제공될 수 있다. 여기서, ‘비일시적 저장 매체'는 실재(tangible)하는 장치이고, 일시적인 전기적 또는 다른 신호들을 전송하는 유선, 무선, 광학적, 또는 다른 통신 링크들을 배제할 수 있다. 한편, 이 '비일시적 저장 매체'는 데이터가 저 장 매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구분하지 않는다. 예를 들어, '비일시적 저장 매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 컴퓨터 판독 가능한 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포 함할 수 있다. 컴퓨터 판독 가능한 매체는, 데이터가 영구적으로 저장될 수 있는 매체와 데이터가 저장되고 나 중에 덮어쓰기 될 수 있는 매체, 이를테면 재기입 가능한 광 디스크 또는 소거 가능한 메모리 디바이스를 포함 한다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예를 들어, compact disc read only memory (CD- ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두 개의 사용자 장치들(예를 들어, 스마트 폰) 간에 직접, 온라인으로 배포(예를 들어, 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예를 들어, 다운로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스 토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시 적으로 생성될 수 있다."}
{"patent_id": "10-2022-0003457", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본 개시의 설명은 예시를 위한 것이며, 본 개시가 속하는 기술분야의 통상의 지식을 가진 자는 본 개시 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 개시의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 개시의 범위에 포함되는 것으 로 해석되어야 한다.도면 도면1 도면2 도면3 도면4 도면5a 도면5b 도면6 도면7a 도면7b 도면8 도면9 도면10 도면11"}
{"patent_id": "10-2022-0003457", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 전자 장치가 복수의 카메라를 이용하여 3차원 이미지를 생성하는 방법의 개 요도이다. 도 2는 본 개시의 일 실시예에 따른 전자 장치가 복수의 카메라를 이용하여 3차원 이미지를 생성하는 방법의 흐 름도이다. 도 3은 본 개시의 일 실시예에 따른 전자 장치가 적어도 하나의 2차원 이미지로부터 3차원 이미지를 생성하는 동작을 설명하기 위한 도면이다. 도 4는 본 개시의 일 실시예에 따른 전자 장치에 포함된 복수의 카메라를 설명하기 위한 도면이다. 도 5a는 본 개시의 일 실시예에 따른 전자 장치에 포함된 복수의 카메라의 화각(Field of view, FOV)을 설명하 기 위한 도면이다. 도 5b는 본 개시의 일 실시예에 따른 전자 장치에 포함된 복수의 카메라의 화각을 설명하기 위한 도면이다. 도 6은 본 개시의 일 실시예에 따른 전자 장치가 복수의 2차원 이미지로부터 깊이 맵을 획득하는 동작을 설명하 기 위한 도면이다. 도 7a는 본 개시의 일 실시예에 따른 전자 장치가 전경 이미지 및 배경 이미지를 식별하는 동작을 설명하기 위 한 도면이다. 도 7b는 본 개시의 일 실시예에 따른 전자 장치가 제1 레이어 이미지 및 제2 레이어 이미지를 식별하는 동작을 설명하기 위한 도면이다. 도 8은 본 개시의 일 실시예에 따른 전자 장치가 이미지의 적어도 일부를 인페인팅하는 동작을 설명하기 위한 도면이다. 도 9는 본 개시의 일 실시예에 따른 전자 장치가 복수의 2차원 이미지를 이용하여 이미지를 인페인팅하는 동작 을 설명하기 위한 도면이다.도 10은 본 개시의 일 실시예에 따른 전자 장치가 복수의 레이어를 포함하는 3차원 이미지를 생성하는 동작을 설명하기 위한 도면이다. 도 11은 본 개시의 일 실시예에 따른 전자 장치의 블록도이다."}
