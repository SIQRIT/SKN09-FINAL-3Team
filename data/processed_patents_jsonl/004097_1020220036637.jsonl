{"patent_id": "10-2022-0036637", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0138703", "출원번호": "10-2022-0036637", "발명의 명칭": "인공지능 기반으로 텍스트 및 비-텍스트 이미지를 포함하는 화면 정보를 인지하여 화면 상의", "출원인": "(주)인포플라", "발명자": "최인묵"}}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "AI기반으로 텍스트 및 비-텍스트를 포함하는 이미지를 포함하는 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법으로서,PC에서 웹기반 IT운영관리시스템 플랫폼에 접속하여 스케줄러에 스케줄을 등록하는 단계; 스케줄러에 스케줄이 등록되면 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓에 스케줄의 등록을 알리는 단계;상기 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓으로부터 정해진 시간에 PC의 AI스크린 에이전트(Agent)의 AI웹소켓으로 통신을 통해 스케줄러의 시작을 알리는 데이터를 전송하는 단계;AI스크린 에이전트가 웹기반 IT운영관리시스템 플랫폼의 AI스크린으로 텍스트 및 비-지텍스트를 포함하는 이미지를 포함하는 PC의 화면 이미지를 전송하고, 텍스트 및 비-지텍스트를 포함하는 이미지를 포함하는 화면 이미지로부터 오브젝트 위치를 학습한 AI 모델을 포함하는 AI스크린으로부터 화면 상의 오브젝트의 위치를 추론하는정보데이터를 요청하는 단계;AI스크린이 전송받은 텍스트 및 비-지텍스트를 포함하는 이미지를 포함하는 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면 상의 오브젝트의 위치를 추론하는 단계; 추론된 오브젝트의 위치에 대한 정보데이터를 AI스크린 에이전트의 AI웹소켓으로 통신을 통해 전송하는 단계;및AI스크린 에이전트가 전송된 데이터를 토대로 PC의 화면에서 오브젝트에 대한 이벤트를 발생시키는 단계;를 포함하고,AI스크린의 AI 모델은 전체 화면의 이미지들 및 상기 전체 화면의 텍스트 및 비-텍스트를 포함하는 이미지들에레이블링된 오브젝트들의 위치를 학습 데이터로 하여, 전체 화면에서 오브젝트의 이벤트를 발생시킬 오브젝트위치를 추론한 결과 데이터를 출력하고,상기 AI스크린이 전송받은 텍스트 및 비-지텍스트를 포함하는 이미지를 포함하는 화면 이미지로부터 AI스크린의학습된 AI 모델을 통해 화면의 오브젝트의 위치를 추론하는 단계는 AI스크린의 AI 모델은 상기 텍스트 및 비-텍스트를 포함하는 이미지를 탐지하기 위해, 텍스트 및 비-텍스트를 포함하는 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하는 단계 및 텍스트를 포함하는 이미지의 부분으로부터 텍스트를 인식하는 단계를 포함하는, AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법."}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 AI스크린의 학습된 AI 모델을 통해 화면의 오브젝트의 위치를 추론하는 단계에서, AI 모델은 텍스트 및 비-텍스트를 포함하는 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하기 위해 한 화면 내에 어느 위치에(localization) 어떤 종류의 오브젝트가 있는지(classification)에 대한 정보를 주는 오브젝트 탐지기(object detector)의 기능을 수행하도록 학습되고, 오브젝트 탐지기는 위치찾기(localization stage) 및 분류(classification stage)를 동시에 수행하는 1 스테이지 탐지기(one stage detector)인,AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법.공개특허 10-2023-0138703-3-청구항 3 제 2 항에 있어서, 1-스테이지 탐지기는 SSD(Single Shot MultiBox Detector), 또는 YOLO, 또는 DSSD(Deconvolutional SingleShot Detector)인,AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법."}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서, AI스크린의 학습된 AI 모델을 통해 화면의 오브젝트의 위치를 추론하는 단계는 상기 텍스트를 포함하는 이미지의 부분으로부터 텍스트를 인식하기 위해, AI 모델이 C-RNN 모델을 활용하는,AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법."}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "AI스크린 에이전트를 포함하는 PC에서 AI기반으로 텍스트 및 비-지텍스트를 포함하는 이미지를 포함하는 화면정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법으로서,PC 내의 AI스크린 에이전트는 PC의 텍스트 및 비-지텍스트를 포함하는 이미지를 포함하는 화면 이미지로부터 오브젝트 위치를 학습한 AI 모델을 포함하는 AI스크린을 포함하고, AI스크린이 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면 상의 오브젝트의 위치를 추론하는 단계; 및AI스크린 에이전트가 AI스크린 에이전트 내의 AI스크린에서 추론한 오브젝트의 위치를 토대로 PC의 화면 상에서오브젝트에 대한 이벤트를 발생시키는 단계;를 포함하고,AI스크린의 AI 모델은 전체 화면의 이미지들 및 상기 전체 화면의 텍스트 및 비-텍스트를 포함하는 이미지들에레이블링된 오브젝트의 위치를 학습 데이터로 하여, 전체 화면에서 오브젝트의 이벤트를 발생시킬 오브젝트 위치를 추론한 결과 데이터를 출력하고,상기 AI스크린이 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면 상의 오브젝트의 위치를 추론하는단계는 AI스크린의 AI 모델이 상기 텍스트 및 비-텍스트를 포함하는 이미지를 탐지하기 위해, 텍스트 및 비-텍스트를 포함하는 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하는 단계 및 텍스트를 포함하는 이미지의 부분으로부터 텍스트를 인식하는 단계를 포함하는, AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법."}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서,PC에서 웹기반 IT운영관리시스템 플랫폼에 접속하여 스케줄러에 스케줄을 등록하는 단계; 스케줄러에 스케줄이 등록되면 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓에 스케줄의 등록을 알리는 단계;상기 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓으로부터 정해진 시간에 PC의 AI스크린 에이전트(Agent)의 AI웹소켓으로 통신을 통해 스케줄러의 시작을 알리는 데이터를 전송하는 단계;를 더 포함하는, AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법."}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "공개특허 10-2023-0138703-4-제 5 항에 있어서, 상기 AI스크린이 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면 상의 오브젝트의 위치를 추론하는단계에서, AI 모델은 텍스트 및 비-텍스트를 포함하는 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하기 위해 한 화면 내에 어느 위치에(localization) 어떤 종류의 오브젝트가 있는지(classification)에대한 정보를 주는 오브젝트 탐지기(object detector)의 기능을 수행하도록 학습되고, 오브젝트 탐지기는 오브젝트 자체가 존재하는 위치를 찾아내는 위치찾기(localization) stage, 및 찾아진 위치(local)에 존재하는 오브젝트가 무엇인지 확인하는 분류(classification)의 stage를 순차적으로 수행하는 2 스테이지 탐지기(2 stage detector)이거나, 또는 위치찾기(localization stage) 및 분류(classification stage)를 동시에 수행하는 1 스테이지 탐지기(onestage detector)인,AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법."}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 5 항에 있어서, 상기 AI스크린이 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면 상의 오브젝트의 위치를 추론하는단계에서, AI 모델은 상기 텍스트를 포함하는 이미지의 부분으로부터 텍스트를 인식하기 위해, C-RNN 모델을 활용하는,AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법."}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "컴퓨터를 이용하여 제 1 항 내지 제 8 항 중 어느 한 항에 따른 화면 상의 오브젝트에 이벤트를 발생시키는 방법을 수행하도록 프로그래밍된 프로그램을 저장한 컴퓨터 판독 가능한 기록매체."}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 시스템으로서,상기 시스템은 AI스크린 에이전트를 포함하는 PC; 및 웹기반 IT운영관리시스템 플랫폼을 포함하는 서버;를 포함하고, 상기 AI스크린 에이전트는 웹기반 IT운영관리시스템 플랫폼에 접속하여 스케줄러에 스케줄을 등록하고, 상기 서버는 스케줄러에 스케줄이 등록되면 상기 서버 내의 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓에 스케줄의 등록을 알리고, 상기 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓으로부터 정해진 시간에 PC의 AI스크린에이전트(Agent)의 AI웹소켓으로 통신을 통해 스케줄러의 시작을 알리는 데이터를 전송하고, 상기 PC의 AI스크린 에이전트는 웹기반 IT운영관리시스템 플랫폼의 AI스크린으로 텍스트 및 비-지텍스트를 포함하는 이미지를 포함하는 PC의 화면 이미지를 전송하고 텍스트 및 비-지텍스트를 포함하는 이미지를 포함하는 화면 이미지로부터 오브젝트 위치를 학습한 AI 모델을 포함하는 AI스크린으로부터 화면 상의 하나 이상의 오브젝트의 위치를 추론한 정보데이터를 요청하며,상기 AI 스크린은 전송받은 텍스트 및 비-지텍스트를 포함하는 이미지를 포함하는 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면의 오브젝트의 위치를 추론하며, 추론된 오브젝트의 위치에 대한 정보데이터를AI스크린 에이전트의 AI웹소켓으로 통신을 통해 전송하고, 그리고상기 AI스크린 에이전트는 전송된 데이터를 토대로 PC의 화면에서 하나 이상의 오브젝트에 대한 이벤트를 발생시키고,공개특허 10-2023-0138703-5-상기 학습된 AI 모델은 전체 화면의 이미지들 및 상기 전체 화면의 텍스트 및 비-텍스트를 포함하는 이미지들에레이블링된 오브젝트들의 위치를 학습 데이터로 하여, 전체 화면에서 오브젝트의 이벤트를 발생시킬 오브젝트위치를 추론한 결과 데이터를 출력하고,상기 AI스크린의 AI 모델은 상기 텍스트 및 비-텍스트를 포함하는 이미지를 탐지하기 위해, 텍스트 및 비-텍스트를 포함하는 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하는 것 및 텍스트를 포함하는이미지의 부분으로부터 텍스트를 인식하는 것을 수행하는, AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 시스템."}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "컴퓨터 내에서 AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 화면 오브젝트 제어 장치로서,AI스크린 에이전트를 포함하고, AI스크린 에이전트는 텍스트 및 비-지텍스트를 포함하는 이미지를 포함하는 컴퓨터 화면 상에 표시된 오브젝트의 위치를 학습시키고오브젝트에 이벤트를 발생시키기 위해, 컴퓨터의 디스플레이 장치로부터 전체 화면에 관한 데이터 및 텍스트 및비-지텍스트를 포함하는 이미지를 포함하는 화면 상에 표시된 오브젝트의 위치 데이터를 수집하는 데이터 수집부, 수집된 데이터를 기초로 심층신경망을 통해 학습시키는 인공지능 모델 학습부, 인공지능 모델 학습부에서 학습된 결과를 기초로, 화면 내 오브젝트를 탐지하는 화면 오브젝트 탐지부, 및오브젝트 탐지부에서 탐지하고 분류한 전체 화면 상의 오브젝트 위치를 기초로 오브젝트에 이벤트를 발생시키는화면 오브젝트 제어부를 포함하고,상기 인공지능 모델 학습부로부터 학습된 AI 모델은 전체 화면의 이미지들 및 상기 전체 화면의 텍스트 및 비-텍스트를 포함하는 이미지들에 레이블링된 오브젝트의 위치들을 학습 데이터로 하여, 전체 화면에서 오브젝트의이벤트를 발생시킬 오브젝트위치를 추론한 결과 데이터를 출력하고,상기 AI 모델은 상기 텍스트 및 비-텍스트를 포함하는 이미지를 탐지하기 위해, 텍스트 및 비-텍스트를 포함하는 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하는 것 및 텍스트를 포함하는 이미지의 부분으로부터 텍스트를 인식하는 것을 수행하는, 화면 오브젝트 제어 장치."}
{"patent_id": "10-2022-0036637", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서, AI 모델은 텍스트 및 비-텍스트를 포함하는 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하기 위해, 한 화면 내에 어느 위치에(localization) 어떤 종류의 오브젝트가 있는지(classification)에 대한 정보를 주는 오브젝트 탐지기(object detector)의 기능을 수행하도록 학습되고, AI 모델은 상기 텍스트를 포함하는 이미지의 부분으로부터 텍스트를 인식하기 위해, C-RNN 모델을 활용하는,화면 오브젝트 제어 장치."}
{"patent_id": "10-2022-0036637", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법은 PC에서 웹기반 IT운영관리 시스템 플랫폼에 접속하여 스케줄러를 등록하는 것, 스케줄러가 등록되면 웹기반 IT운영관리시스템 플랫폼의 AI 웹소켓에 스케줄러의 등록을 알리는 것, 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓으로부터 정해진 시간에 PC (뒷면에 계속)"}
{"patent_id": "10-2022-0036637", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공기능 기반 화면 정보 인지 방법을 이용하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법 및 시스템에 관한 것으로, 더욱 상세하게는 인공지능 기반의 화면의 내용 추론 방법을 이용하여 디스플레이 화면 상에서 오브젝트의 이벤트를 발생시키는 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2022-0036637", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "RPA(로봇 프로세스 자동화)는 이전에는 사람이 하던 반복적인 태스크를 소프트웨어 로봇이 대신하는 것이다. 선행기술인 한국특허공개공보 10-2020-0127695호는 챗봇을 통해 RPA에 업무가 전달되면 RPA가 PC화면에서 웹브 라우저를 구동하여 정보를 찾아 이를 다시 챗봇에 전달할 수 있다. 이때 RPA가 웹브라우저의 검색창 또는 검색 버튼 등을 인식하는 방법은 웹 스크립트 언어인 HTML 및 JAVASCRIPT의 소스에서 미리 학습되어 있는 해당 검색 창 또는 검색버튼의 Class Id 등을 찾아서 화면에 존재하는지 찾고, 만약 있다면 해당 검색창 Class Id에 검색 어 등의 텍스트를 입력하고, 검색버튼의 클래스 아이디(Class Id)에 마우스 클릭 이벤트를 입력하여 웹브라우저 를 동작시킨다."}
{"patent_id": "10-2022-0036637", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "최근 보안 및 RPA 자동화에 대항하기 위해 HTML의 Class Id를 매번 바꾸는 방식으로 웹페이지를 구성하는 사례 가 점점 늘어나고 있어, 이 경우 RPA가 학습된 Class Id를 찾을 수 없어 인식 및 입력이 불가능한 경우를 인공 지능 학습 모델을 통해 해결하려는 연구가 진행되고 있다. 그런데 인공지능 학습 모델을 통해 화면 UI 인식을 통한 이벤트 클릭이 가능하더라도, 텍스트 이미지를 포함하 는 이미지에서 텍스트를 인식하지 못하여 UI 인식의 어려움이 있었다. 화면상의 아이콘, 웹 브라우저, 검색버튼 등 화면 UI에 대해서 인식율을 높이기 위해 여러 연구가 진행되고 있 는데, 개발되고 있는 화면 인식 방법에서는 화면상의 아이콘, 웹 브라우저, 검색버튼 등 비-텍스트의 화면 UI에 대해서는 인식이 가능하지만, 아이콘 텍스트, 어플리케이션 제목 텍스트 등 이미지 속의 텍스트에 대한 인식이 불가능하다는 단점이 있었다."}
{"patent_id": "10-2022-0036637", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 과제를 해결하기 위한 본 발명의 일 실시 예에 따른 화면을 조정하는 방법 및 조정 장치는 AI 기술을 기 반으로 디스플레이 상의 UI 화면의 오브젝트 탐지 및 텍스트 인식을 수행될 수 있다. 구체적으로, AI기반으로 텍스트 및 비-텍스트 이미지를 포함하는 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법은 PC에서 웹기반 IT운영관리시스템 플랫폼에 접속하여 스케줄러에 스케줄을 등록하는 것, 스케줄러에 스케줄이 등록되면 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓에 스케줄의 등록을 알리는 것, 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓으로부터 정해진 시간에 PC의 AI스크린 에이전트(Agent)의 AI웹소켓 으로 통신을 통해 스케줄러의 시작을 알리는 데이터를 전송하는 것, AI스크린 에이전트가 웹기반 IT운영관리시 스템 플랫폼의 AI스크린으로 텍스트 및 비-텍스트 이미지를 포함하는 PC의 화면 이미지를 전송하고, 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 오브젝트 위치를 학습한 AI 모델을 포함하는 AI스크린으로부터 화면 상의 오브젝트의 위치를 추론하는 정보데이터를 요청하는 것, AI스크린이 전송받은 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면 상의 오브젝트의 위치를 추론하 는 것, 추론된 오브젝트의 위치에 대한 정보데이터를 AI스크린 에이전트의 AI웹소켓으로 통신을 통해 전송하는 것, 및 AI스크린 에이전트가 전송된 데이터를 토대로 PC의 화면에서 오브젝트에 대한 이벤트를 발생시키는 것을 포함하고, AI스크린의 AI 모델은 전체 화면의 이미지들 및 전체 화면의 텍스트 및 비-텍스트를 포함하는 UI 이 미지들에 레이블링된 오브젝트들의 위치를 학습 데이터로 하여, 전체 화면에서 오브젝트의 이벤트를 발생시킬 오브젝트 위치를 추론한 결과 데이터를 출력하고, AI스크린이 전송받은 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면의 오브젝트의 위치를 추론하는 단계는 AI스크린의 AI 모델은 텍스트 및 비-텍스트를 포함하는 UI 이미지를 탐지하기 위해, 텍스트 및 비-텍스트를 포함하는 UI 이 미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하는 단계 및 텍스트를 포함하는 UI 이미지의 부분으로부터 텍스트를 인식하는 것을 포함할 수 있다. 본 발명의 다른 실시 예에서, AI스크린의 학습된 AI 모델을 통해 화면의 오브젝트의 위치를 추론하는 것에서, AI 모델은 텍스트 및 비-텍스트를 포함하는 UI 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분 하기 위해 한 화면 내에 어느 위치에(localization) 어떤 종류의 오브젝트가 있는지(classification)에 대한 정 보를 주는 오브젝트 탐지기(object detector)의 기능을 수행하도록 학습되고, 오브젝트 탐지기는 위치찾기 (localization stage) 및 분류(classification stage)를 동시에 수행하는 1 스테이지 탐지기(one stage detector)일 수 있다. 본 발명의 다른 실시예에서, 1-스테이지 탐지기는 SSD(Single Shot MultiBox Detector), 또는 YOLO, 또는 DSSD(Deconvolutional Single Shot Detector)일 수 있다. 본 발명의 다른 실시예에서, AI스크린의 학습된 AI 모델을 통해 화면의 오브젝트의 위치를 추론하는 것은 텍스 트를 포함하는 UI 이미지의 부분으로부터 텍스트를 인식하기 위해, AI 모델이 C-RNN 모델을 활용할 수 있다. 본 발명의 다른 실시 예에서, AI스크린 에이전트를 포함하는 PC에서 AI기반으로 텍스트 및 비-텍스트 이미지를 포함하는 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법은 PC 내의 AI스크린 에이전트 는 PC의 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 오브젝트 위치를 학습한 AI 모델을 포함하 는 AI스크린을 포함하고, AI스크린이 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면 상의 오브젝트 의 위치를 추론하는 것, 및 AI스크린 에이전트가 AI스크린 에이전트 내의 AI스크린에서 추론한 오브젝트의 위치 를 토대로 PC의 화면 상에서 오브젝트에 대한 이벤트를 발생시키는 것을 포함하고, AI스크린의 AI 모델은 전체 화면의 이미지들 및 전체 화면의 텍스트 및 비-텍스트를 포함하는 UI 이미지들에 레이블링된 오브젝트의 위치를 학습 데이터로 하여, 전체 화면에서 오브젝트의 이벤트를 발생시킬 오브젝트 위치를 추론한 결과 데이터를 출력 하고, AI스크린이 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면 상의 오브젝트의 위치를 추론하는 것은 AI스크린의 AI 모델이 텍스트 및 비-텍스트를 포함하는 UI 이미지를 탐지하기 위해, 텍스트 및 비-텍스트 를 포함하는 UI 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하는 단계 및 텍스트를 포함하 는 UI 이미지의 부분으로부터 텍스트를 인식하는 것을 포함할 수 있다. 본 발명의 다른 실시예에서, AI스크린 에이전트를 포함하는 PC에서 AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법은 PC에서 웹기반 IT운영관리시스템 플랫폼에 접속하여 스케줄러에 스케줄 을 등록하는 것, 스케줄러에 스케줄이 등록되면 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓에 스케줄의 등록을 알리는 것, 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓으로부터 정해진 시간에 PC의 AI스크린 에이전트(Agen t)의 AI웹소켓으로 통신을 통해 스케줄러의 시작을 알리는 데이터를 전송하는 것을 더 포함할 수 있다. 본 발명의 다른 실시예에서, 오브젝트는 선택될 수 있는 컴퓨터 화면 상의 콘솔창, 윈도우창, 대화창, 선택될 수 있는 링크, 선택될 수 있는 버튼, 정보의 입력이 가능한 커서위치, 아이디 입력위치, 패스워드 입력위치, 검 색바 입력위치 중 하나 이상일 수 있다. 본 발명의 다른 실시 예에서, 오브젝트는 패스워드 입력부일 수 있다. 본 발명의 다른 실시 예에서, 웹기반 IT운영관리시스템 플랫폼은 클라우드 서버에 설치될 수 있다. 본 발명의 다른 실시 예에서, 컴퓨터를 이용하여 I기반으로 텍스트 및 비-텍스트 이미지를 포함하는 화면 정보 를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법을 수행하도록 프로그래밍된 프로그램이 컴퓨터 판 독 가능한 기록매체에 저장될 수 있다. 본 발명의 다른 실시 예에서, AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 시 스템은 시스템은 AI스크린 에이전트를 포함하는 PC, 및 웹기반 IT운영관리시스템 플랫폼을 포함하는 서버;를 포 함하고, AI스크린 에이전트는 웹기반 IT운영관리시스템 플랫폼에 접속하여 스케줄러에 스케줄을 등록하고, 서버 는 스케줄러에 스케줄이 등록되면 서버 내의 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓에 스케줄의 등록을 알 리고, 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓으로부터 정해진 시간에 PC의 AI스크린 에이전트(Agent)의 AI 웹소켓으로 통신을 통해 스케줄러의 시작을 알리는 데이터를 전송하고, PC의 AI스크린 에이전트는 웹기반 IT운 영관리시스템 플랫폼의 AI스크린으로 텍스트 및 비-텍스트 이미지를 포함하는 PC의 화면 이미지를 전송하고 텍 스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 오브젝트 위치를 학습한 AI 모델을 포함하는 AI스크린 으로부터 화면 상의 하나 이상의 오브젝트의 위치를 추론한 정보데이터를 요청하며, AI 스크린은 전송받은 텍스 트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면의 오브젝트의 위치를 추론하며, 추론된 오브젝트의 위치에 대한 정보데이터를 AI스크린 에이전트의 AI웹소켓으로 통신을 통해전송하고, 그리고 AI스크린 에이전트는 전송된 데이터를 토대로 PC의 화면에서 하나 이상의 오브젝트에 대한 이 벤트를 발생시키고, 학습된 AI 모델은 전체 화면의 이미지들 및 전체 화면의 텍스트 및 비-텍스트를 포함하는 UI 이미지들에 레이블링된 오브젝트들의 위치를 학습 데이터로 하여, 전체 화면에서 오브젝트의 이벤트를 발생 시킬 오브젝트 위치를 추론한 결과 데이터를 출력하고, AI스크린의 AI 모델은 텍스트 및 비-텍스트를 포함하는 UI 이미지를 탐지하기 위해, 텍스트 및 비-텍스트를 포함하는 UI 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하는 것 및 텍스트를 포함하는 UI 이미지의 부분으로부터 텍스트를 인식하는 것을 수행할 수 있다. 본 발명의 다른 실시 예에서, 컴퓨터 내에서 AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 화면 오브젝트 제어 장치는 AI스크린 에이전트를 포함하고, AI스크린 에이전트는 텍스트 및 비-텍스 트 이미지를 포함하는 컴퓨터 화면 상에 표시된 오브젝트의 위치를 학습시키고 오브젝트에 이벤트를 발생시키기 위해, 컴퓨터의 디스플레이 장치로부터 전체 화면에 관한 데이터 및 텍스트 및 비-텍스트 이미지를 포함하는 화 면 상에 표시된 오브젝트의 위치 데이터를 수집하는 데이터 수집부, 수집된 데이터를 기초로 심층신경망을 통해 학습시키는 인공지능 모델 학습부, 인공지능 모델 학습부에서 학습된 결과를 기초로, 화면 내 오브젝트를 탐지 하는 화면 오브젝트 탐지부, 및 오브젝트 탐지부에서 탐지하고 분류한 전체 화면 상의 오브젝트 위치를 기초로 오브젝트에 이벤트를 발생시키는 화면 오브젝트 제어부를 포함하고, 인공지능 모델 학습부로부터 학습된 AI 모 델은 전체 화면의 이미지들 및 전체 화면의 텍스트 및 비-텍스트를 포함하는 UI 이미지들에 레이블링된 오브젝 트의 위치들을 학습 데이터로 하여, 전체 화면에서 오브젝트의 이벤트를 발생시킬 오브젝트위치를 추론한 결과 데이터를 출력하고, AI 모델은 텍스트 및 비-텍스트를 포함하는 UI 이미지를 탐지하기 위해, 텍스트 및 비-텍스 트를 포함하는 UI 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하는 것 및 텍스트를 포함하 는 UI 이미지의 부분으로부터 텍스트를 인식하는 것을 수행할 수 있다. 본 발명의 다른 실시 예에서, AI 모델은 텍스트 및 비-텍스트를 포함하는 UI 이미지 중 텍스트를 포함하는 부분 과 텍스트가 아닌 부분을 구분하기 위해, 한 화면 내에 어느 위치에(localization) 어떤 종류의 오브젝트가 있 는지(classification)에 대한 정보를 주는 오브젝트 탐지기(object detector)의 기능을 수행하도록 학습되고, AI 모델은 텍스트를 포함하는 UI 이미지의 부분으로부터 텍스트를 인식하기 위해, C-RNN 모델을 활용할 수 있다. 이 외에도, 본 발명을 구현하기 위한 다른 방법, 다른 시스템 및 방법을 실행하기 위한 컴퓨터 프로그램이 더 제공될 수 있다. 전술한 것 외의 다른 측면, 특징, 이점이 이하의 도면, 특허청구범위 및 발명의 상세한 설명으로부터 명확해질 것이다."}
{"patent_id": "10-2022-0036637", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 기존의 RPA 문제점을 해결하기 위해, 데이터 학습부에서는 PC 등 다양한 디바이스의 화면 관련 데이 터 즉, 브라우저, 검색창, 검색버튼 등 화면상에 나타날 수 있는 다양한 오브젝트 데이터를 학습하여 인식할 수 있는 AI 스크린 모델을 생성할 수 있다. 서버에서 스케쥴러가 일정시간에 동작하여, 사용자 단말, 노트북, 데스크탑 컴퓨터에 프로그램 또는 앱 형태로 실행되는 인공지능 에이전트에 웹소켓 등의 TCP/IP 소켓통신으로 실행을 지시할 수 있고, 인공지능 에이전트 자 신의 화면 사진을 서버 또는 자신의 PC에 위치한 AI 스크린 모델에 전송하여, 학습된 모델을 통하여 원하는 오 브젝트를 예측할 수 있다. 예측된 데이터값을 인공지능 에이전트에 소켓통신으로 전송하여 사용자 PC 화면 좌표상에서 텍스트데이터 입력 또는 마우스 버튼 클릭 등을 입력제어하여 처리할 수 있고, 화면인식과 화면좌표 입력제어를 반복하여 사람이 사용자 PC 등의 화면에서 수행하는 작업을 인공지능이 자동으로 수행할 수 있다. 본 발명을 이용하면, 기대하는 브라우저, 이미지, 입력창 등의 오브젝트가 화면에 있는지 화면사진에서 판단함 으로써, 웹, Command Line, RDP(Remote Desktop Protocol) 등의 환경을 모두 지원할 수 있으며, 화면의 좌표를 이용하여 직접 텍스트 데이터 입력, 버튼 클릭 등이 가능하므로 대부분의 환경에서 입력이 가능하다. 따라서 PC, IoT, Connected Car 단말기, 키오스크 등 네트워크에 연결된 화면을 이용하는 대부분의 장비에서 화면을 인 식하고, 입력제어할 수 있다.본 발명은 화면인지 인공지능기술이 화면속의 다양한 프로그램의 오브젝트를 학습시킬 수 있는 장점이 있다. RPA가 제품별 특징에 의해 지원하는 환경(웹, CLI, RDP 등)에 제약이 있는데 반하여 화면인지 인공지능기술은 화면에 나타나는 모든 오브젝트를 인지가능하다. 또한, RPA가 브라우저안의 인풋박스나 버튼 등의 오브젝트를 찾기 위해서는 앵커라고 불리우는 기준값이 필요하나, 화면인지 인공지능기술은 앵커없이 오브젝트를 직접 인 식하여 접근할 수 있다. 기존 RPA는 PC에서 업무자동화라는 특성상 웹을 주로 사용하게 되고, 웹을 빠르고 더 잘 이해하기 위해 html내 에서 텍스트 검색을 주로 한다. 그러나, 보안 html처럼 html이 바뀌면 기존 RPA는 작동하는 문제점이 있었다. 본 발명의 화면인지 인공지능기술을 사용하면 보안 html처럼 html이 바뀌더라도 보안 html을 검색하지 않고 화 면에서 오브젝트 인식이 가능할 수 있다. 또한, 웹, 윈도우, Mac OS, 리눅스와 같이 운영체계에 상관없이 OS들 이 제공하는 화면을 보고 인식하므로, 본 발명의 인공지능을 이용한 화면 오브젝트 인식 기술은 작동 가능하다. 또한, RDP의 경우 RPA는 특정 RDP 제품들의 API를 이용하여 화면속의 오브젝트 정보를 얻는데 비하여, 화면인지 인공지능기술은 어떠한 RDP 제품들의 API 필요없이 화면속의 오브젝트 인식할 수 있다. 본 발명을 이용하면 화면 오브젝트의 연속적인 인식 및 화면좌표에 글자/버튼 입력을 통해 사람이 작업하는 일 련의 행위를 자동화할 수 있다."}
{"patent_id": "10-2022-0036637", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 설명되는 실시 예들 을 참조하면 명확해질 것이다. 그러나 본 발명은 아래에서 제시되는 실시 예들로 한정되는 것이 아니라, 서로 다른 다양한 형태로 구현될 수 있고, 본 발명의 사상 및 기술 범위에 포함되는 모든 변환, 균등물 내지 대체물 을 포함하는 것으로 이해되어야 한다. 아래에 제시되는 실시 예들은 본 발명의 개시가 완전하도록 하며, 본 발"}
{"patent_id": "10-2022-0036637", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "명이 속하는 기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이다. 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되 는 경우 그 상세한 설명을 생략한다. 본 출원에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조 합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 구성요소들은 상기 용어들에 의해 한정되어서 는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 이하, 본 발명에 따른 실시 예들을 첨부된 도면을 참조하여 상세히 설명하기로 하며, 첨부 도면을 참조하여 설 명함에 있어, 동일하거나 대응하는 구성 요소는 동일한 도면번호를 부여하고 이에 대한 중복되는 설명은 생략하 기로 한다. 도 1은 본 발명의 일 실시 예에 따른 화면 오브젝트 제어 시스템의 예시도이다. 화면 오브젝트 제어 시스템은 사용자 PC 및 서버로 구성될 수 있다. 사용자 PC는 디스플레이에 표시되는 사용자 PC 화면 및 AI스크린 에이전트(Agent)를 포함할 수 있다. AI스크린 에이전트(Agent)는 AI웹소켓를 포함할 수 있다. 웹기반 IT운영관리시스템 플랫폼은 웹기반 IT운영관리시스템 플랫폼의 홈페이지, AI웹소켓 , AI스크린을 포함할 수 있다. AI스크린은 학습된 AI 모델을 포함할 수 있다. 본 발명의 다른 실시예에서, 사용자 PC의 컴퓨팅파워가 충분한 경우 AI스크린은 사용자 PC에 포 함될 수 있다. 본 발명에서 '오브젝트'는 화면에서 마우스나, 키보드 등의 입력 장치로 활성하시킬 수 있는 화면상의 모든 대 상을 의미한다. 이러한 화면상의 오브젝트는 인공지능 모델로 학습시킬 대상이 될 수 있다. 예를 들어, 사용자 가 PC 화면상에서 사용하는 프로그램 창, 대화창의 입력창, 브라우저의 검색창 창이거나, 로그인버튼 및 가입버 튼 등의 각종 버튼이거나, 또는 로고, 아이디, 패스워드, 회사명 등의 특정 문자나 기호일 수 있다. 본 발명에 서 '오브젝트'의 '제어'이라고 하면 프로그램 창의 활성화, 대화창에 입력 사항 입력, 브라우저 창의 검색바 입 력, 아이디 입력, 패스워드 입력, 회사명 입력하여 오브젝트의 이벤트를 발생시키는 모든 행위들을 가리킨다. 서버는 클라우드 서버일 수 있으며, 일반적인 독립 서버일 수 있다. 아이톰즈(ITOMS)는 (주)인포플라의 웹기반 IT운영관리시스템 플랫폼이다. 사용자 PC는 자동으로 또는 사용자의 스케줄러 버튼의 클릭으로 서버의 웹기반 IT운영관리시스템 플 랫폼에 접속하여 스케줄러를 등록할 수 있다(S302). 사용자 PC는 자동으로 또는 사용자의 스케줄러 버튼의 클릭으로 서버의 웹기반 IT운영관리시스템 플 랫폼에 접속하여 스케줄러를 등록할 수 있다(S202). 스케줄러가 등록되면 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓에 스케줄러의 등록을 알릴 수 있다 (S304). 상기 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓으로부터 정해진 시간에 사용자 PC의 AI스크린 에이전트(Agent)에 있는 AI웹소켓으로 통신을 통해 스케줄러의 시작을 알리는 데이터를 전송할 수 있 다(S306). AI스크린 에이전트는 웹기반 IT운영관리시스템 플랫폼의 AI스크린으로 사용자 PC의 화면 이미지를 전송하고 학습된 AI 모델을 포함하는 AI스크린으로부터 화면 상의 오브젝트의 위치를 추론 한 정보데이터를 요청할 수 있다(S308). 학습된 AI 모델은 전체 화면의 이미지들 및 상기 전체 화면의 이미지들 에 레이블링된 오브젝트의 위치를 학습 데이터로 하여, 전체 화면에서 오브젝트의 이벤트를 발생시키는 오브젝 트위치를 추론하는 오브젝트 위치탐색모델일 수 있다. 일반적으로 AI 학습데이터의 구축을 위해서는 학습데이터 의 수집이 필요하다. 이러한 학습데이터의 수집은 예를들어, PC화면 이미지를 모은 후 어노테이션 툴에서 주요 오브젝트에 바운딩 박스를 치고, 레이블링 작업을 하여 수집할 수 있다. 예를 들면 구글 검색 사이트 웹 화면에 서 구글 검색창에 박스를 치고 구글검색창이라고 레이블링하여, 구글 검색 사이트 전체 화면 데이터 및 구글 검 색창의 오브젝트에 대한 레이블 데이터를 수집할 수 있다. 전송받은 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면의 오브젝트의 위치를 추론할 수 있다(S310 및 S312). 웹기반 IT운영관리시스템 플랫폼은 추론된 오브젝트의 위치에 대한 정보데이터를 AI스크린 에이전트 의 AI웹소켓으로 통신을 통해 전송할 수 있다(S314). 전송된 데이터를 토대로 예를 들어 AI스크린 에이전트를 통해 사용자 pc의 화면에서 오브젝트에 대한 이벤트를 발생시킬 수 있다(S316). 본 발명의 다른 실시예에서, 사용자 PC에 AI스크린이 포함될 수 있다. 이 경우, 웹기반 IT운영관리시 스템 플랫폼에 데이터들을 전송하지 않고, 자체적으로 AI스크린 학습모델을 생성할 수 있다. 사용자 PC에 AI스크린이 포함된 경우에서, AI스크린 에이전트가 웹기반 IT운영관리시스템 플랫폼 의 AI스크린으로 사용자 PC의 화면 이미지를 전송하고 학습된 AI 모델을 포함하는 AI스크린 으로부터 화면 상의 오브젝트의 위치를 추론한 정보데이터를 요청하는 단계(S308) 및 웹기반 IT운영관리시 스템 플랫폼가 추론된 오브젝트의 위치에 대한 정보데이터를 AI스크린 에이전트의 AI웹소켓으로 통신을 통해 전송하는 단계(S314)는 그 대상이 클라우드 서버내의 아이톰즈 AI 스크린에서 사용자 PC내의 아이톰즈 AI 스크린으로 변경되며, AI스크린 에이전트의 아이톰즈 AI 스크린은 하기 도 2의 데이터 수집부, 인공지능모델 학습부, 및 오브젝트 탐지부가 아이톰즈 AI 스크린의 기능과 동일한 기능을 수행한다. 사용자 PC에 AI스크린이 포함된 경우에서, AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법은 사용자 PC에서 웹기반 IT운영관리시스템 플랫폼에 접속하여 스케줄러에 스케줄을 등록하는 단계; 스케줄러에 스케줄이 등록되면 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓에 스케줄의 등록을 알리는 단계; 상기 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓으로부터 정해진 시간에 사용자 PC의 AI스크린 에이전트(Agent)의 AI웹소켓으로 통신을 통해 스케줄러의 시작을 알리는 데이터를 전송하는 단계; AI스크린 에 이전트가 AI스크린 에이전트 내의 AI스크린에서 사용자 PC의 화면 이미지로부터 오브젝트 위치를 학습한 AI 모 델을 포함하는 AI스크린으로부터 화면 상의 하나 이상의 오브젝트의 위치를 추론하는 정보데이터를 요청하는 단 계; AI스크린이 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면의 하나 이상의 오브젝트의 위치를 추론하는 단계; 및 AI스크린 에이전트가 AI스크린 에이전트 내의 AI스크린에서 추론한 하나 이상의 오브젝트의 위치를 토대로 사용자 pc의 화면에서 하나 이상의 오브젝트에 대한 이벤트를 발생시키는 단계;를 포함할 수 있 고, AI스크린의 AI 모델은 전체 화면의 이미지들 및 상기 전체 화면의 하나 이상의 이미지들에 레이블링된 오브 젝트의 위치를 학습 데이터로 하여, 전체 화면에서 하나 이상의 오브젝트의 이벤트를 발생시킬 오브젝트위치를 추론한 결과 데이터를 출력할 수 있다."}
{"patent_id": "10-2022-0036637", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 3, "content": "도 2는 본 발명의 일 실시 예에 따른 AI스크린 에이전트의 블록도이다. 화면 오브젝트 제어 시스템은 웹기반 IT운영관리시스템 플랫폼 없이 사용자 PC 내에 화면 오브젝트 제어 장치로 구축될 수 있다. 화면 오브젝트 제어 장치는 스케줄러 등록부(미도시) 및 AI스크린 에이전트를 포함하고, AI스크린 에이전 트는 화면 상에 표시된 오브젝트의 위치를 학습시키고 오브젝트에 이벤트를 발생시키는 기능을 포함할 수 있다. AI스크린 에이전트는 자체적으로 오브젝트 위치를 학습시키기 위해, 디스플레이 장치로부터 전체 화면에 관한 데이터를 수집하는 데이터 수집부, 수집된 데이터를 기초로 심층신경망을 통해 학습시키는 인공 지능 모델 학습부, 화면 오브젝트 탐지부를 포함할 수 있다. AI스크린 에이전트는 화면 오브젝 트 제어부, 영상 화면 관련 데이터, 학습 데이터 등 각종 데이터를 저장하는 메모리, 서버 또는 외부 장치와 통신하는 통신부, 및 입력/출력 조정부를 포함할 수 있다. 스케줄을 등록하는 스케줄러 등록부는 AI스크린 에이전트에 스케줄러의 등록을 알리고, 정해진 시간에 사 용자 PC에서 스케줄러의 시작을 알리는 기능을 한다. 스케줄러 등록부의 알림에 따라, AI스크린 에이전트의 데이터 수집부는 디스플레이 상의 PC 화면 상의 전체 화면과 관련된 데이터를 수집할 수 있다. 오브젝트 탐지부는 학습된 인공지능 학습 모델 을 통해 수집된 데이터에 대해, 상기 전체 화면 상에서 오브젝트들의 위치를 탐지(detect)할 수 있다. 인공지능 모델 학습부는 PC 화면의 이미지들, 및 PC 화면의 이미지들에 레이블링된 오브젝트들의 특정 위 치들을 학습용 데이터(또는 학습 데이터 세트)로 하여, 전체 화면 상에서 오브젝트의 위치를 추론하도록 학습시 킨다. 인공지능 모델 학습부는는 NPU와 같은 병렬처리에 특화된 프로세서를 포함할 수 있다. 인공지능 모 델 학습부는는 오브젝트 위치 학습을 위해 메모리에 학습용 데이터를 저장한 후 NPU가 메모리와 협업하여 오브젝트 위치를 학습시켜, 오브젝트 탐지부에 학습된 AI 모델을 생성하고, 새로운 학습용 데이 터가 수집되면 특정시기에 또는 주기적으로 학습시켜서 인공지능 학습 모델을 지속적으로 개선시킬 수 있다. 본 발명의 일 실시예에서, 인공지능 모델 학습부는 일단 오브젝트 탐지부에 학습된 인공지능 모델이 생성되면, 데이터 수집부에서 새로운 학습용 데이터가 수집되기 전까지 기능을 정지할 수 있다. 이 경우 데이터 수집부 및 수집된 인공지능 모델 학습부는 기능을 정지하고, 사용자 PC 화면으로부터 수신된 화면 이미지를 바로 오브젝트 탐지부에 전달할 수 있다. 새로운 인공지능 모델 학습부는 지도학습을 이용하여 인공지능 모델을 생성하지만, 비지도학습, 또는 강화학습을 이용하여 하나 이상의 오브젝트를 학습시 킬 수 있다. 오브젝트 탐지부는 인공지능 모델 학습부에서 학습된 인공지능 모델을 통해, 화면 상에 원하는 오브 젝트가 있는지 여부 및 하나 오브젝트의 위치를 탐지하고 복수개의 오브젝트 위치들을 탐지할 수 있다. 학습된 AI 모델은 전체 화면의 이미지들 및 상기 전체 화면의 하나 이상의 이미지들에 레이블링된 오브젝트의 위치를 학습 데이터로 하여, 전체 화면에서 하나 이상의 오브젝트의 이벤트를 발생시킬 오브젝트위치를 추론한 결과 데 이터를 출력한다. 본 발명의 다른 실시 예에서, 전술한 바와 같이, 화면 탐지부는 서버에서 전송받은 학습 된 인공지능 모델을 통해 사용자 PC의 화면 상의 오브젝트 위치를 탐지 및 분류하도록 구성될 수 있다. 오브젝트 제어부는 오브젝트 탐지부에서 탐지하고 분류한 전체 화면 상의 오브젝트 위치를 기초로 오 브젝트에 이벤트를 발생시킬 수 있다. 오브젝트 제어부는 화면 오브젝트의 연속적인 인식 및 화면좌표에 글자/버튼 입력을 통해 사람이 작업하는 일련의 행위를 자동화하도록 제어할 수 있다. 예를 들어, 오브젝트 제 어부는 도 5에서와 같이 브라우저 상의 검색바을 탐지하고, 원하는 검색쿼리를 검색하는 이벤트를 발 생시킬 수 있다. 또한, 오브젝트 제어부는 도 6에서와 같이 PC 바탕화면 상의 여러개의 프로그램 창에서 로그인 대화창을 탐지하고, 아이디 및 패스워드 입력위치, 및 검색창 브라우저 상의 검색바위치, 각 종버튼 등을 탐지하고, 원하는 회사명, 아이디, 및 패스워드를 입력하거나, 또는 검색쿼리를 검 색하는 이벤트를 발생시킬 수 있다. AI스크린 에이전트가 사용자 단말, 노트북, 데스크탑 컴퓨터에 프로그램 또는 앱 형태로 실행되는 방법으 로 포함되어 있다면, AI스크린 에이전트는 통신부를 통해 사용자 단말, 노트북, 데스크탑 컴퓨터의 통신부를 이용하여 서버와 같은 외부 기기와 통신할 수 있다. 다른 실시예에서, AI스크린 에이전트는 사용자 PC의 외부에 있는 웹기반 IT운영관리시스템 플랫폼에 접속 하여 웹기반 IT운영관리시스템 플랫폼에서 학습된 오브젝트 위치 정보데이터를 수신하여 화면상의 오브젝트에 대한 이벤트를 발생시킬 수 있다. 이 경우 데이터 수집부, 인공지능모델 학습부, 및 오브젝트탐지부 를 사용하지 않고, 웹기반 IT운영관리시스템 플랫폼가 데이터 수집부, 인공지능모델 학습부 , 및 오브젝트탐지부를 포함하여, AI 스크린 모델 학습을 진행하며, AI스크린 에이전트는 통신 부를 통해 웹기반 IT운영관리시스템 플랫폼에 사용자 PC 화면 이미지를 전송하고 오브젝트 위치 정보 데이터를 수신하여 오브젝트에 대한 이벤트를 발생시킬 수 있다. 도 3은 본 발명의 일 실시 예에 따른 화면 오브젝트 제어 과정의 흐름도이다. 사용자 PC와 같은 등 화면인식을 원하는 단말기에서 AI 스크린의 오브젝트 제어를 시작하면(S200), 웹기반 IT운영관리시스템 플랫폼을 자동으로 또는 사용자의 스케줄러 버튼의 클릭으로 서버의 웹기반 IT운영 관리시스템 플랫폼에 접속하여 스케줄러를 등록할 수 있다(S202). 스케줄러가 등록되면 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓에 스케줄러의 등록을 알릴 수 있다. 스케줄러의 등록에 따라 웹기반 IT운영관리시스템 플랫폼은 정해진 시간에 동작하고(S204), 정해진 스케줄러 기능을 실행하고(S206), 웹기반 IT운영관리시스템 플랫폼의 AI웹소켓으로부터 정해진 시간 에 사용자 PC의 AI스크린 에이전트(Agent)의 AI웹소켓으로 통신을 통해 스케줄러의 시작을 알리 는 데이터를 전송할 수 있다. AI스크린 에이전트는 웹기반 IT운영관리시스템 플랫폼의 AI스크린으로 사용자 PC의 화면 이미지를 전송하고 학습된 AI 모델을 포함하는 AI스크린으로부터 화면 상의 오브젝트의 위치를 추론 한 정보데이터를 요청할 수 있다. PC로부터 이미지 인식 데이터 요청이 있는지 판단하고(S208), PC로부터 이미지 인식 데이터 요청이 있으면 데이터 요청이 완료될 때까지(S212) 전송받은 화면 이미지로부터 AI스크린의 학습된 AI 모델 을 통해 화면의 오브젝트의 위치를 추론할 수 있고, 웹기반 IT운영관리시스템 플랫폼은 추론된 오브젝트의 위치에 대한 정보데이터를 AI스크린 에이전트의 AI웹소켓으로 통신을 통해 전송할 수 있고, PC 의 AI스크린 에이전트는 전송된 데이터를 토대로 사용자 pc의 화면에서 오브젝트에 대한 이벤트를 발 생시켜서, 텍스트 또는 마우스 입력 이벤트를 처리한다(S214). PC로부터 이미지 인식 데이터 요청이 없다면, 주어진 과정 모든 처리 완료시 또는 에러시 로그를 작성하고 (S216), AI 스크린의 오브젝트 제어를 종료한다. 도 4는 도 1의 화면의 오브젝트의 위치를 추론하는 인공지능 스크린 학습 모델을 학습시키기 위한 흐름도이다. 도 4를 참조하면, AI스크린 에이전트 또는 AI 스크린에서 화면 상의 오브젝트 위치를 추론하기 위한 인공지능 모델 학습이 시작되어 진행된다(S100). 인공지능 모델의 학습은 지도학습, 비지도학습 및 강화학습 중 에서 어느 하나의 형태로 수행될 수 있다. 사용자 PC 화면 상의 화면 이미지와 관련된 데이터 및 상기 데이터에 오브젝트 위치를 레이블링한 데이터 를 포함하는 인공지능 모델 학습용 데이터로 인공지능 모델 학습이 진행되고(S110). 학습이 완료되면(S110), AI 스크린 학습 모델을 생성한다. AI스크린 에이전트 또는 AI 스크린의 데이터 수집부는 일정 주기 로 화면 이미지 데이터 값 및 상기 화면 이미지 데이터 값에 대해 레이블된 오브젝트 위치들을 인공지능 학습용 데이터 및 테스트용 데이터로 생성할 수 있다. 학습용 데이터 및 테스트용 데이터의 비율은 데이터 양에 따라 다를 수 있으며, 일반적으로 7:3의 비율로 정할 수 있다. 학습용 데이터의 수집 및 저장은 오브젝트 별로 수집 하여 저장할 수 있으며, 실제 사용 화면을 캡처앱을 통해 수집할 수 있다. 이러한 학습데이터의 수집 및 저장은 서버에서 화면 이미지를 취합하여 저장할 수 있다. 인공지능 모델 학습용 데이터는 정확한 학습결과를 얻 기 위해 데이터 전처리 및 데이터 증강 과정을 거칠 수 있다. 도 5와 같은 결과를 얻기 위해, 인공지능 모델 학 습은 브라우저의 사이트에 표시된 사용자 PC 화면 상의 화면 이미지 데이터 값들을 입력 데이터로 하고, 검색 창 및 클릭가능한 아이콘들 등 오브젝트들의 위치를 레이블링한 데이터를 출력 데이터로 학습 데이터 세트 를 구성하여 진행될 수 있다. 인공지능 모델, 예를 들어 마스크-RCNN이나 SSD와 같은 인공 신경망은 지도학습을 통해 수집된 학습 데이터를 이용하여 전체화면 상에서 오브젝트의 위치들이 학습된다(S100). 본 발명의 일 실시 예에서, 딥러닝 기반의 화 면 분석기가 사용될 수 있고, 예를 들어 인공지능 프로그래밍에 사용되는 인공지능 언어 라이브러리인 TensorFlow 또는 Keras의 MobileNetV1/MobileNetV2 기반으로 인공지능 학습 모델을 튜닝하여 사용할 수 있다. CNN(Convolutional Neural Network)은 심층신경망의 가장 대표적인 방법으로, 이미지를 작은 특징에서 복잡한 특징화한다. CNN은 하나 또는 여러 개의 컨볼루션 레이어와 그 위에 올려진 일반적인 인공 신경망 레이어들로 이루어져 컨볼루션 레이어에서 전처리를 수행하는 구조를 가진 인공신경망이다. 예를 들어, 사람 얼굴의 이미지 를 CNN을 통해 학습시키기 위해, 제일 먼저 필터를 사용하여 간단한 특징들을 뽑아내어 하나의 컨볼루션 레이어 를 만들고, 이 특징들에서 좀 더 복잡한 특징을 추출하는 새로운 레이어, 예를 들어 폴링 레이어를 추가한다.볼루션 레이어는 컨볼루션 연산을 통해 특징들을 추출하는 레이어로서, 규칙적인 패턴을 가진 곱셈을 수행한다. 폴링레이어는 입력 공간을 추상화하는 레이어로 서브샘플링을 통해 이미지의 차원을 축소시킨다. 예를들어 28x28 사이즈의 얼굴 이미지를 스크라이드가 1인 4개의 필터를 사용하여 각각 24x24의 피쳐맵을 만들고 서브샘 플링(또는 풀링)으로 12x12로 압축할 수 있다. 그 다음 레이어에서 8x8 사이즈로 12개의 피처맵을 만들고 다시 4x4로 서브샘플링을 하여, 최종적으로 12x4x4 = 192의 입력을 가진 신경망으로 학습을 하여 이미지를 탐지할 수 있다. 이렇게 여러 개의 컨볼류션 레이어를 연결하여 이미지의 특징을 뽑아내고 최종적으로 기존과 같은 오류역 전파 신경망을 사용하여 학습을 시킬 수 있다. CNN의 장점은 인공신경망 학습을 통해 이미지의 특징을 특징화하 는 필터를 스스로 만든다는 것이다. 오브젝트 탐지(Object Detection, 객체 탐지)란 컴퓨터 비전의 하위 분야 중 하나로 전체 디지털 이미지 및 비 디오 내에서 유의미한 특정 객체를 감지하는 작업을 한다. 이러한 object detection은 Image retrieval(이미지 검색), Image annotation(이미지 주석), Face detection(얼굴 인식), Video Tracking(비디오 추적) 등 다양한 분야의 문제를 해결하기 위해 사용될 수 있다. 본 발명에서 object detection는 한 화면(또는 이미지) 내에 object로 분류한 오브젝트(객체)들에 대해, 어느 위치에(localization) 어떤 종류의 object가 있는지 (classification)에 대한 정보를 주는 것이다. object detection은 두 가지로 구성된다. 첫 번째는 object 자체가 존재하는 위치를 찾아내는 localization이 며, 두 번째는 해당 local에 존재하는 object가 무엇인지 확인하는 classification이다. 일반적으로 object detection의 딥러닝 네트워크는 2-Stage Detector 와 1-Stage Detector 로 구분하고 있다. 간단히 말하면, localization과 classification이 따로 이루어지면 2-Stage Detector, 동시에 이루어지는 것이 1-Stage Detector이다. 2-Stage에서는 먼저 object가 있을거라고 생각하는 영역을 선택하고 그 영역 각각에 대해 classification을 한다. 1-Stage에서는 이 과정이 동시에 이루어지기 때문에 더욱 속도가 빠른 장점이 있다. 원 래 2-Stage와 1-Stage 사이에서 2-Stage는 정확도가 높지만 느리고 1-Stage는 빠르지만 2-Stage보다는 정확도 가 낮다는 것으로 구분이 되었지만, 최근에는 1-Stage 방법들이 2-Stage의 정확도를 따라잡으면서 1-Stage 방법 들이 각광받고 있다. R-CNN은 CNN에 Region Proposal 을 추가하여 물체가 있을 법한 곳을 제안하고, 그 구역에 서 object detection을 하는 2-스테이지 디텍터(2 Stage Detector) 계열의 알고리즘이다. R-CNN 계열 모델은 R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN 까지 총 4가지 종류가 있다. R-CNN, Fast R-CNN, Faster R-CNN 모두 Object Detection을 위한 모델이다. Mask R-CNN은 Faster R-CNN을 확장하여 Instance Segmentation에 적 용하고자 하는 모델이다. Mask R-CNN은 Faster R-CNN에 각 픽셀이 객체인지 아닌지를 masking하는 CNN을 추가한 것이다. Mask R-CNN은 COCO challenges의 모든 task에서 이전 모델보다 우수한 성능을 보이는 것으로 알려져 있 다. 도 7d는 도 7a의 학습시킬 화면으로부터 마스크-RCNN을 적용하여 학습시키는 과정을 예시한다. SSD(Single Shot MultiBox Detector), YOLO, DSSD(Deconvolutional Single Shot Detector) 등은 1-스테이지 디텍터(1 Stage Detector) 계열의 알고리즘이다. 1-스테이지 디텍터(1 Stage Detector) 계열의 알고리즘은 물 체가 있을 법한 구역제시와 object detection을 나누지 않고 동시에 실행하므로 실행속도가 빠른 장점이 있으므 로, 본 발명의 실시예들에서는 적용대상에 따라 1-스테이지 디텍터 또는 2-스테이지 디텍터를 사용할 수 있다. YOLO는 2-Stage object detection 모델들의 느리다는 단점을 해결한 최초의 real-time object detector이다. YOLO에서는 convolution layer들을 통해 feature map을 추출하고, fully connected layer를 거쳐 바로 bounding box와 class probability를 예측할 수 있다. 또한, YOLO에서는 input 이미지를 SxS grid로 나누고 각 grid 영역에 해당하는 bounding box와 confidence, class probability map을 구할 수 있다. YOLO에서는 이미지를 grid로 나누어서 각 영역에 대해 bounding box를 예측했다면, SSD 는 CNN pyramidal feature hierarchy 를 이용해 예측할 수 있다. SSD에서는 image feature를 다양한 위치의 layer들에서 추출하 여 detector와 classifier를 적용할 수 있다. SSD는 YOLO 보다 학습 속도 및 인식속도나 정확도 측면에서 더 높 은 성능을 보였다. AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키기 위한 학습모델 에 적용된 마스크RCNN, YOLO, 및 SSD의 성능을 비교하면, 마스크 RCNN은 분류 및 위치 찾기 정확도가 상대적으 로 높지만 학습속도 및 오브젝트 인식속도가 상대적으로 느리고, YOLO는 분류 및 위치 찾기 정확도가 상대적으 로 낮지만 학습속도 및 오브젝트 인식속도가 빨랐고, SSD는 분류 및 위치 찾기 정확도가 상대적으로 빠르고, 학 습속도 및 오브젝트 인식속도도 빨랐다. DSSD는 기존의 SSD(Single Shot MultiBox Detecotr)에서 성능 향상을 위해, Context 특징들을 더하기 위해 Deconvolution 연산을 추가하였다. 기존의 SSD에 Deconvolution 연산을 추가함으로써, 속도를 상대적으로 유지 하면서 탐지 성능을 높이고자 하였다. 특히 작은 객체들(small object)에 대하여 SSD의ㄷ앞ㄷㄷㄷㄷㄷ부분에서 사용되었던 VGG network를 Resnet기반의 Residual-101로 교체하였고, 네트워크에서 테스트 할때, 배치 정규 화 과정을 제거함으로써 테스트 시간을 1.2 ~ 1.5배 줄였다. 학습된 인공지능 모델의 평가를 통해 인공지능 모델이 생성된다. 학습된 인공지능 모델의 평가는 테스트용 데이 터를 사용하여 수행된다. 본 발명 전체에서 '학습된 인공지능 모델'은 학습용 데이터를 학습시키고 생성된 특별 한 언급이 없어도 테스트용 데이터를 통해 테스트한 후 학습된 모델을 결정한 것을 의미한다. 인공신경망은 생물학적 뉴런의 동작원리와 뉴런간의 연결 관계를 모델링한 것으로 노드(node) 또는 처리 요소 (processing element)라고 하는 다수의 뉴런들이 레이어(layer) 구조의 형태로 연결된 정보처리 시스템이다. 인공 신경망은 기계 학습에서 사용되는 모델로써, 기계학습과 인지과학에서 생물학의 신경망(동물의 중추신경계 중 특히 뇌)에서 영감을 얻은 통계학적 학습 알고리즘이다. 구체적으로 인공신경망은 시냅스(synapse)의 결합으로 네트워크를 형성한 인공 뉴런(노드)이 학습을 통해 시냅 스의 결합 세기를 변화시켜, 문제 해결 능력을 가지는 모델 전반을 의미할 수 있다. 용어 인공신경망은 용어 뉴럴 네트워크(Neural Network)와 혼용되어 사용될 수 있다. 인공신경망은 복수의 레이어(layer)를 포함할 수 있고, 레이어들 각각은 복수의 뉴런(neuron)을 포함할 수 있다. 또한 인공신경망은 뉴런과 뉴런을 연결하는 시냅스를 포함할 수 있다. 인공 신경망은 일반적으로 다음의 세가지 인자, 즉 다른 레이어의 뉴런들 사이의 연결 패턴 연결의 가 중치를 갱신하는 학습 과정 이전 레이어로부터 수신되는 입력에 대한 가중 합으로부터 출력값을 생성하는 활성화 함수에 의해 정의될 수 있다. 인공 신경망은, DNN(Deep Neural Network), RNN(Recurrent Neural Network), BRDNN(Bidirectional Recurrent Deep Neural Network), MLP(Multilayer Perceptron), CNN(Convolutional Neural Network), C-RNN, R-CNN, Fast R-CNN, Faster R-CNN, 및 마스크-RCNN와 같은 방식의 네트워크 모델들을 포함할 수 있으나, 이에 한정되지 않는다. 본 명세서에서 용어 '레이어'는 용어 '계층'과 혼용되어 사용될 수 있다. 인공신경망은 계층 수에 따라 단층 신경망(Single-Layer Neural Networks)과 다층 신경망(Multi-Layer Neural Networks)으로 구분된다. 일반적인 단층 신경망은, 입력층과 출력층으로 구성된다. 또한 일반적인 다층 신경망은 입력층(Input Layer)과 하나 이상의 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성된다. 입력층은 외부의 자료들을 받아들이는 층으로서, 입력층의 뉴런 수는 입력되는 변수의 수와 동일하며, 은닉층은 입력층과 출력층 사이에 위치하며 입력층으로부터 신호를 받아 특성을 추출하여 출력층으로 전달한다. 출력층은 은닉층으로부터 신호를 받고, 수신한 신호에 기반한 출력 값을 출력한다. 뉴런간의 입력신호는 각각의 연결강도 (가중치)와 곱해진 후 합산되며 이 합이 뉴런의 임계치보다 크면 뉴런이 활성화되어 활성화 함수를 통하여 수신 한 출력값을 출력한다. 한편 입력층과 출력 층 사이에 복수의 은닉층을 포함하는 심층 신경망은, 기계 학습 기술의 한 종류인 딥 러닝 을 구현하는 대표적인 인공 신경망일 수 있다. 한편 용어 '딥 러닝'은 용어 '심층 학습'과 혼용되어 사용될 수 있다. 인공 신경망은 훈련 데이터(training data)를 이용하여 학습(training)될 수 있다. 여기서 학습이란, 입력 데이 터를 분류(classification)하거나 회귀분석(regression)하거나 군집화(clustering)하는 등의 목적을 달성하기 위하여, 학습 데이터를 이용하여 인공 신경망의 파라미터(parameter)를 결정하는 과정을 의미할 수 있다. 인공 신경망의 파라미터의 대표적인 예시로써, 시냅스에 부여되는 가중치(weight)나 뉴런에 적용되는 편향(bias)을 들 수 있다. 훈련 데이터에 의하여 학습된 인공 신경망은, 입력 데이터를 입력 데이터가 가지는 패턴에 따라 분류하거나 군 집화 할 수 있다.한편 훈련 데이터를 이용하여 학습된 인공 신경망을, 본 명세서에서는 학습 모델(a trained model)이라 명칭 할 수 있다. 다음은 인공 신경망의 학습 방식에 대하여 설명한다. 인공 신경망의 학습 방식은 크게, 지도 학습, 비지도 학습, 준지도 학습(Semi-Supervised Learning), 강화 학습 (Reinforcement Learning)으로 분류될 수 있다. 지도 학습은 훈련 데이터로부터 하나의 함수를 유추해내기 위한 기계 학습의 한 방법이다. 그리고 이렇게 유추되는 함수 중, 연속 적인 값을 출력하는 것을 회귀분석(Regression)이라 하고, 입력 벡터의 클래스(class)를 추론하여 출력하는 것을 분류(Classification)라고 할 수 있다. 지도 학습에서는, 훈련 데이터에 대한 레이블(label)이 주어진 상태에서 인공 신경망을 학습시킨다. 여기서 레이블이란, 훈련 데이터가 인공 신경망에 입력되는 경우 인공 신경망이 추론해 내야 하는 정답(또는 결 과 값)을 의미할 수 있다. 본 명세서에서는 훈련 데이터가 입력되는 경우 인공 신경망이 추론해 내야 하는 정답(또는 결과값)을 레이블 또 는 레이블링 데이터(labeling data)이라 명칭 한다. 또한 본 명세서에서는, 인공 신경망의 학습을 위하여 훈련 데이터에 레이블을 설정하는 것을, 훈련 데이터에 레 이블링 데이터를 레이블링(labeling) 한다고 명칭 한다. 이 경우 훈련 데이터와 훈련 데이터에 대응하는 레이블은 하나의 트레이닝 셋(training set)을 구성하고, 인공 신경망에는 트레이닝 셋의 형태로 입력될 수 있다. 한편 훈련 데이터는 복수의 특징(feature)을 나타내고, 훈련 데이터에 레이블이 레이블링 된다는 것은 훈련 데 이터가 나타내는 특징에 레이블이 달린다는 것을 의미할 수 있다. 이 경우 훈련 데이터는 입력 객체의 특징을 벡터 형태로 나타낼 수 있다. 인공 신경망은 훈련 데이터와 레이블링 데이터를 이용하여, 훈련 데이터와 레이블링 데이터의 연관 관계에 대한 함수를 유추할 수 있다. 그리고, 인공 신경망에서 유추된 함수에 대한 평가를 통해 인공 신경망의 파라미터가 결정(조정)될 수 있다. 인공 신경망은 모델의 구성, 활성 함수(Activation Function), 손실 함수(Loss Function) 또는 비용 함수(Cost Function), 학습 알고리즘, 조정 알고리즘 등에 의해 그 구조가 특정되며, 학습 전에 하이퍼파라미터 (Hyperparameter)가 미리 설정되고, 이후에 학습을 통해 모델 파라미터(Model Parameter)가 설정되어 내용이 특 정될 수 있다. 예컨대, 인공 신경망의 구조를 결정하는 요소에는 은닉층의 개수, 각 은닉층에 포함된 은닉 노드의 개수, 입력 특징 벡터(Input Feature Vector), 대상 특징 벡터(Target Feature Vector) 등이 포함될 수 있다. 하이퍼파라미터는 모델 파라미터의 초기값 등과 같이 학습을 위하여 초기에 설정하여야 하는 여러 파라미터들을 포함한다. 그리고, 모델 파라미터는 학습을 통하여 결정하고자 하는 여러 파라미터들을 포함한다. 예컨대, 하이퍼파라미터에는 노드 간 가중치 초기값, 노드 간 편향 초기값, 미니 배치(Mini-batch) 크기, 학습 반복 횟수, 학습률(Learning Rate) 등이 포함될 수 있다. 그리고, 모델 파라미터에는 노드 간 가중치, 노드 간 편향 등이 포함될 수 있다. 손실 함수는 인공 신경망의 학습 과정에서 최적의 모델 파라미터를 결정하기 위한 지표(기준)로 이용될 수 있다. 인공 신경망에서 학습은 손실 함수를 줄이기 위하여 모델 파라미터들을 조작하는 과정을 의미하며, 학습 의 목적은 손실 함수를 최소화하는 모델 파라미터를 결정하는 것으로 볼 수 있다. 손실 함수는 주로 평균 제곱 오차(MSE: Mean Squared Error) 또는 교차 엔트로피 오차(CEE, Cross Entropy Error)를 사용할 수 있으며, 본 발명이 이에 한정되지는 않는다. 교차 엔트로피 오차는 정답 레이블이 원 핫 인코딩(one-hot encoding)된 경우에 사용될 수 있다. 원 핫 인코딩 은 정답에 해당하는 뉴런에 대하여만 정답 레이블 값을 1로, 정답이 아닌 뉴런은 정답 레이블 값이 0으로 설정 하는 인코딩 방법이다. 기계 학습 또는 딥 러닝에서는 손실 함수를 최소화하기 위하여 학습 조정 알고리즘을 이용할 수 있으며, 학습 조정 알고리즘에는 경사 하강법(GD: Gradient Descent), 확률적 경사 하강법(SGD: Stochastic Gradient Descent), 모멘텀(Momentum), NAG(Nesterov Accelerate Gradient), Adagrad, AdaDelta, RMSProp, Adam, Nadam 등이 있다. 경사 하강법은 현재 상태에서 손실 함수의 기울기를 고려하여 손실 함수값을 줄이는 방향으로 모델 파라미터를 조정하는 기법이다. 모델 파라미터를 조정하는 방향은 스텝(step) 방향, 조정하는 크기는 스텝 사이즈(size)라고 칭한다. 이때, 스텝 사이즈는 학습률을 의미할 수 있다. 경사 하강법은 손실 함수를 각 모델 파라미터들로 편미분하여 기울기를 획득하고, 모델 파라미터들을 획득한 기 울기 방향으로 학습률만큼 변경하여 갱신할 수 있다. 확률적 경사 하강법은 학습 데이터를 미니 배치로 나누고, 각 미니 배치마다 경사 하강법을 수행하여 경사 하강 의 빈도를 높인 기법이다. Adagrad, AdaDelta 및 RMSProp는 SGD에서 스텝 사이즈를 조절하여 조정 정확도를 높이는 기법이다. SGD에서 모 멘텀 및 NAG는 스텝 방향을 조절하여 조정 정확도를 높이는 기법이다. Adam은 모멘텀과 RMSProp를 조합하여 스 텝 사이즈와 스텝 방향을 조절하여 조정 정확도를 높이는 기법이다. Nadam은 NAG와 RMSProp를 조합하여 스텝 사 이즈와 스텝 방향을 조절하여 조정 정확도를 높이는 기법이다. 인공 신경망의 학습 속도와 정확도는 인공 신경망의 구조와 학습 조정 알고리즘의 종류뿐만 아니라, 하이퍼파라 미터에 크게 좌우되는 특징이 있다. 따라서, 좋은 학습 모델을 획득하기 위하여는 적당한 인공 신경망의 구조와 학습 알고리즘을 결정하는 것뿐만 아니라, 적당한 하이퍼파라미터를 설정하는 것이 중요하다. 통상적으로 하이퍼파라미터는 실험적으로 다양한 값으로 설정해가며 인공 신경망을 학습시켜보고, 학습 결과 안 정적인 학습 속도와 정확도를 제공하는 최적의 값으로 설정한다. 도 5는 브라우저 화면에서 학습된 인공지능 모델을 통해 오브젝트의 위치를 추론한 결과를 도시한 예시도이다. 도 5의 화면 이미지로부터 도 4의 AI 스크린 학습 모델의 학습 결과로 브라우저의 검색 바(search bar)의 위치 가 특정된다. 검색 바의 입력창인 오브젝트의 위치를 특정하는 이벤트이외에도, 브라우저의 해당 사 이트에서 다른 아이콘들을 클릭하는 이벤트를 발생시키기 위해, 클릭할 아이콘들을 오브젝트의 데이터 및 오브 젝트들의 위치를 특정한 데이터를 학습 데이터 세트로 하여, 학습시킨 AI 스크린 학습 모델의 학습 결과로 아이 콘들의 위치를 특정할 수 있다. 도 6은 PC 바탕화면에서 학습된 인공지능 모델을 통해 오브젝트의 위치를 추론한 결과를 도시한 예시도이다. 복수의 검색창 및 대화창이 있는 경우에도 원하는 검색바, 오브젝트들인 로그인, 회사명, 아이 디, 및 패스워드의 위치를 특정할 수 있다. 도 7a는 도 4에 따른 화면의 오브젝트의 위치를 추론하는 인공지능 모델을 학습시킬 화면을 도시한 예시도이다. 사용자 PC 화면은 학습시킬 화면 이미지가 된다. AI스크린 에이전트는 웹기반 IT운영관리시스템 플랫 폼의 AI스크린으로 사용자 PC의 화면 이미지를 전송하고 학습된 AI 모델을 포함하는 AI스 크린으로부터 화면 상의 오브젝트의 위치를 추론한 정보데이터를 요청할 수 있다(S308). 도 7b는 도 4에 따른 화면의 오브젝트의 위치를 추론하는 인공지능 모델을 학습시킬 화면에서 오브젝트에 라벨 링을 한 예시도이다. 데이터처리부는 사용자 PC로부터 화면 이미지를 수신하여 오브젝트들인 로그인, 회사명, 아이디, 및 패스워드의 라벨링을 수행한다. 다른 실시예에서, 다른 데이터 베이스로부터 화면 이미지 데이터 및 화면 이미지에 대한 각 오브젝트 들의 위치가 라벨링된 데이터 세트를 제공받을 수 있다. 도 7c는 도 4에 따른 화면의 오브젝트의 위치를 추론하는 인공지능 모델을 학습시킨 후 오브젝트를 실제 인식한 결과의 예시도이다. 학습된 AI 스크린 학습모델을 통해 AI 스크린은 오브젝트의 위치를 전송한다. 도 7d는 도 7a의 학습시킬 화면으로부터 마스크-RCNN을 적용하여 학습시키는 과정을 도시한 예시도이다. 도 7d의 화면 이미지에서 기존의 Faster RCNN과정을 실행하여 물체를 검출한다. 기존의 Faster RCNN 에서 RoI pooling은 오브젝트 탐지(object detection)을 위한 모델이였기에 정확한 위치정보를 담는 것은 중요하지 않고 따라서 RoI가 소수점 좌표를 가지면 좌표를 반올림하는 식으로 이동시킨후 풀링(pooling)을 진행하였다. 마스크(mask)를 씌울 때는(segmentation) 소수점을 반올림하면 위치정보가 왜곡되기 때문에 위치정보가 중요하 다. 따라서 양선형 보간법(bilinear interpolation)을 이용해서 위치정보를 담는 RoI align을 이용한다. RoI align으로 conv를 이용해 특징 지도(feature map)을 뽑아내고, 그 특징 지도에서 RoI를 뽑아내 class별로 분류 하고 마스킹(masking)을 병렬로 수행하여 오브젝트를 검출해낸다. 도 8은 텍스트 및 비-텍스트 이미지, 비-텍스트 이미지, 및 텍스트 이미지를 포함하는 화면 정 보를 도시한 예시도이다. 도 1 내지 도 7d의 인공지능 모델을 통한 화면의 오브젝트 인식은 비-텍스트 이미지 인식에 의해 화면 UI 인식을 통한 이벤트를 클릭하는 것은 가능하나, UI 이미지 내에 텍스트 이미지(83 0)이 포함된 경우, 텍스트 이미지의 내용을 인식하지 못하고, 텍스트 내용이 다른 유사한 텍스트를 동일한 텍스트 이미지로 인식하는 한계가 있었다. 이 경우 반쪽짜리 화면인식 기능만 제공하는게 되므로 개선이 필요하 게 되었다. 이러한 문제점은 텍스트 및 비-텍스트를 포함하는 UI 이미지(\"텍스트 및 비-텍스트 이미지\"라 고도 함)를 인식하기 위해, 형태 이미지 및 텍스트를 복합 인식하기 위한 인공지능 모델구성 통해 해결할 수 있 다. 도 9a는 비-텍스트 이미지 및 텍스트 이미지를 포함하는 특정 포털 사이트의 화면 정보를 도시한 예 시도이다. 화면 UI는 오브젝트 탐지(object detection)로 미리 학습하여 등록된 클래스(네이버모자 아이콘, 검 색 돋보기 등)만 인식이 가능할 수 있다. 네이버모자 아이콘 및 검색 돋보기는 비-텍스트 이미지의 예이고, 텍스트를 포함하는 배너광고, 포털 로그인창, 및 메뉴 버튼은 텍스트 이미지의 예이다. 배너광고 안의 텍스트 및 메뉴 내의 테스트는 오브젝트 탐지로는 미리 학습 불가하므로, 이를 해결하기 위해, 1단계로 오 브젝트 탐지기를 통해 텍스트 박스 오브젝트(클래스라고도 부름)로 인식한 후 2단계에서 C-RNN(Convolutional Recurrent Neural Network) 모델로 텍스트 인식을 하도록 할 수 있다. 도 9b는 본 발명의 일 실시예에 따라 화면 UI 이미지들을 오브젝트 탐지에 따라 구별하는 경우를 도시한 예시도 이다. 만약 비-텍스트 이미지(모자 이미지)와 텍스트 이미지(\"NAVER\" 텍스트)로 UI 이미지를 구성한다면, 기존 의 오브젝트 탐지를 활용한 화면 UI 인식모델은 비-텍스트 이미지 및 텍스트 이미지를 서로 다른 2개 의 이미지로 구분하는 것은 가능하다. 그러나 기존의 오브젝트 탐지를 활용한 화면 UI 인식모델은 텍스트의 내 용을 모르는채 텍스트 이미지(NAVER 이미지)를 단지 텍스트 박스(text class)로만 인식할 수 있다. 도 9c는 본 발명의 일 실시예에 따라 화면 UI 이미지들을 오브젝트 탐지만으로 UI 이미지를 구별하지 못하는 텍 스트 이미지들을 도시한 예시도이다. 만약 텍스트 이미지 및 이와 유사한 텍스트 이미지가 같이 들어 온다면 화면 UI 인식모델은 두개의 서로 다른 이미지(NAVER, NAVEG)를 올바르게 구분할 수 없다. 오브젝트 탐지 (Object detection) 모델에서는 모양이나 형태가 비슷하다면 동일한 오브젝트로 인식하기 때문이다. 따라서, 화 면 UI 뿐만 아니라 텍스트 이미지의 내용까지 인식이 가능해야 화면내의 모든 object를 올바르게 인식할 수 있 다. 도 10은 본 발명의 일 실시예에 따라 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 텍스트 이미지 를 인식하는 과정을 도시한 예시도이다. 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델은 비-텍스트 UI 및 텍스트 이미지를 포함하는 화면 이미지를 한 장 입력 받으면 크게 1) 오브젝트 탐지(SSD or YOLO or Mask R-CNN 등)를 활용하여 비-텍스트 이미지 부분의 모 자 이미지와 나머지 두개의 텍스트 이미지를 구분하는 단계(S2000) 및 2) C-RNN(Convolutional Recurrent Neural Network) 모델 등 텍스트 인식 인공지능 모델을 활용하여 각 텍스트 이미지에서 텍스트를 인식하여 내용 (830-NAVER, 840-NAVEG)을 파악하는 단계(S5000)의 2 단계로 이미지 인식을 수행할 수 있다. 본 발명의 다른 실 시예에서, 텍스트 인식 인공지능 모델은 C-RNN에 국한되지 않으며 텍스트를 인식하는 다른 인공지능 모델이 사 용될 수 있다. 본발명의 일 실시예에 따른 비-텍스트 UI 및 텍스트 이미지를 복합적으로 인지하여 화면 상의 오 브젝트에 이벤트를 발생시키는 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델의 플로우는 다음과 같다. 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델은 비-텍스트 및 텍스트 이미지를 한장 입력받는다(S1000). 만약 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델이 비-텍스트 UI 이미지만을 입력받는다면 화면 UI를 인식 하는 오브젝트 탐지기로부터 인식이 가능할 것이다. 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델은 오브젝트 탐지(SSD or YOLO or Mask R-CNN 등)를 활용하여 비 -텍스트 이미지 부분의 모자 이미지와 나머지 두개의 텍스트 이미지를 구분할 수 있다(S2000). 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델은 오브젝트 탐지를 활용하여 비-텍스트 UI이미지를 인식 처리를 완료할 수 있다(S3000). 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델의 오브젝트 탐지는 비-텍스트 UI 이미지에 사각형(Rectangle) 좌표 및 비-텍스트 화면 UI 클래스(네이버 모자아이콘)를 출력할 수 있다. 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델은 2개의 텍스트 이미지들(830 및 840)을 텍스트 박스로 인식하 고 추출하여 다음단계로 넘긴다(S4000). 오브젝트 탐지는 텍스트 이미지 2개(830 및 840)의 사각형 좌표 및 화 면UI클래스(텍스트박스)를 출력할 수 있다. 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델은 C-RNN(Convolutional Recurrent Neural Network) 모델을 활 용하여 각 텍스트 이미지에서 텍스트를 인식하여 내용(830-NAVER, 840-NAVEG)을 파악한다(S5000). C-RNN 모델은 텍스트 박스내의 텍스트를 정확하게 인식할 수 있다. C-RNN은 CNN을 연산을 먼저 한 뒤에 각 채널을 나눠서 RNN 에 입력하는 구조이다. CNN을 통해 Feature를 추출하고, 이를 RNN으로 분류하는 흐름이라 볼 수 있다. 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델은 텍스트 이미지들의 텍스트를 인식하여 텍스트이미지 처리를 완료한다(S6000). 도 3의 \"AI기반으로 화면 정보를 인지하여 화면 상의 오브젝트에 이벤트를 발생시키는 방법\"에 도 10의 \"비-텍 스트 UI 및 텍스트 이미지 복합 인지 모델\"을 도입하면, AI스크린의 AI 모델은 전체 화면의 이미지들 및 상기 전체 화면의 텍스트 및 비-텍스트를 포함하는 UI 이미지들에 레이블링된 오브젝트들의 위치를 학습 데이터로 하 여, 전체 화면에서 오브젝트의 이벤트를 발생시킬 오브젝트 위치를 추론한 결과 데이터를 출력하고, 상기 AI스 크린이 전송받은 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 AI스크린의 학습된 AI 모델을 통해 화면의 오브젝트의 위치를 추론하는 단계에서 AI스크린의 AI 모델은 상기 텍스트 및 비-텍스트를 포함하는 UI 이미지를 탐지하기 위해, 텍스트 및 비-텍스트를 포함하는 UI 이미지 중 텍스트를 포함하는 부분과 텍스트가 아 닌 부분을 구분하는 단계(1 단계) 및 텍스트를 포함하는 UI 이미지의 부분으로부터 텍스트를 인식하는 단계(2 단계)를 포함할 수 있다. 본 발명의 다른 실시예에서, 화면 내 오브젝트를 탐지하는 화면 오브젝트 탐지부는 상기 인공지능 모델 학습부 로부터 학습된 AI 모델을 통해 전체 화면의 이미지들 및 상기 전체 화면의 텍스트 및 비-텍스트를 포함하는 UI 이미지들에 레이블링된 오브젝트의 위치들을 학습 데이터로 하여, 전체 화면에서 오브젝트의 이벤트를 발생시킬 오브젝트위치를 추론한 결과 데이터를 출력하고, 상기 AI 모델은 상기 텍스트 및 비-텍스트를 포함하는 UI 이미 지를 탐지하기 위해, 텍스트 및 비-텍스트를 포함하는 UI 이미지 중 텍스트를 포함하는 부분과 텍스트가 아닌 부분을 구분하는 것 및 텍스트를 포함하는 UI 이미지의 부분으로부터 텍스트를 인식하는 것을 수행할 수 있다. 도 11a는 본 발명의 일 실시예에 따라 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 텍스트 이미 지를 인식하여 오인식이 줄어드는 경우를 도시한 예시도이다. 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델은 비-텍스트 UI 및 텍스트 이미지를 모두 포함하는 UI에 에러가 있어서 비-텍스트 부분 또는 텍스트 부분의 일부 가 인식불가하더라도, 인식가능한 비-텍스트 부분 또는 텍스트 부분으로부터 올바른 UI를 인식할 수 있다. 도 11b는 본 발명의 일 실시예에 따라 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 텍스트 이미 지를 인식하여 에러정보로 인식 가능한 경우를 도시한 예시도이다. 도 11b에서와 같이, 비-텍스트 UI 이미지부 분에 대응하는 텍스트 부분에 에러가 있는 경우 또는 텍스트 부분에 대응하는 비-텍스트 UI 부분에 에러가 있는 경우, 비-텍스트 UI 및 텍스트 이미지 복합 인지 모델은 이를 에러 정보를 인식할 수 있다. 이상 설명된 본 발명에 따른 실시 예는 컴퓨터 상에서 다양한 구성요소를 통하여 실행될 수 있는 컴퓨터 프로그 램의 형태로 구현될 수 있으며, 이와 같은 컴퓨터 프로그램은 컴퓨터로 판독 가능한 매체에 기록될 수 있다. 이 때, 매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등과 같은, 프로그램 명령어를 저장하고 실행하도록 특별히 구성된 하드웨어 장치를 포함할 수 있다. 한편, 상기 컴퓨터 프로그램은 본 발명을 위하여 특별히 설계되고 구성된 것이거나 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수 있다. 컴퓨터 프로그램의 예에는, 컴파일러에 의하여 만들어지는 것 과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용하여 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함될 수 있다. 본 발명의 명세서(특히 특허청구범위에서)에서 \"상기\"의 용어 및 이와 유사한 지시 용어의 사용은 단수 및 복수 모두에 해당하는 것일 수 있다. 또한, 본 발명에서 범위(range)를 기재한 경우 상기 범위에 속하는 개별적인 값 을 적용한 발명을 포함하는 것으로서(이에 반하는 기재가 없다면), 발명의 상세한 설명에 상기 범위를 구성하는 각 개별적인 값을 기재한 것과 같다. 본 발명에 따른 방법을 구성하는 단계들에 대하여 명백하게 순서를 기재하거나 반하는 기재가 없다면, 상기 단 계들은 적당한 순서로 행해질 수 있다. 반드시 상기 단계들의 기재 순서에 따라 본 발명이 한정되는 것은 아니 다. 본 발명에서 모든 예들 또는 예시적인 용어(예들 들어, 등등)의 사용은 단순히 본 발명을 상세히 설명하기 위한 것으로서 특허청구범위에 의해 한정되지 않는 이상 상기 예들 또는 예시적인 용어로 인해 본 발명의 범위 가 한정되는 것은 아니다. 또한, 당업자는 다양한 수정, 조합 및 변경이 부가된 특허청구범위 또는 그 균등물의 범주 내에서 설계 조건 및 팩터에 따라 구성될 수 있음을 알 수 있다. 따라서, 본 발명의 사상은 상기 설명된 실시 예에 국한되어 정해져서는 아니 되며, 후술하는 특허청구범위뿐만 아니라 이 특허청구범위와 균등한 또는 이로부터 등가적으로 변경된 모든 범위는 본 발명의 사상의 범주에 속한 다고 할 것이다. 100: 사용자 PC 102: 메모리 103: 통신부 104: 입력/출력 인터페이스 110: AI 스크린 에이전트 112: AI 웹소켓 120: 사용자 PC 화면 131: 데이터 수집부 132: 인공지능모델 학습부 133: 오브젝트 분류부 134: 오브젝트 제어부 200: IT운영관리시스템 플랫폼 210: IT운영관리시스템 홈페이지 212: 스케줄러 버튼 222: AI 웹소켓 230: IT운영관리시스템 AI 스크린 232: AI 스크린 학습모델 234: 데이터처리부 810: 텍스트 및 비-텍스트 포함 이미지 820: 비-텍스트 이미지 830: 텍스트 이미지 840: 유사한 텍스트 이미지"}
{"patent_id": "10-2022-0036637", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시 예에 따른 화면 오브젝트 제어 시스템의 예시도이다. 도 2는 본 발명의 일 실시 예에 따른 AI스크린 에이전트의 블록도이다. 도 3은 본 발명의 일 실시 예에 따른 화면 오브젝트 제어 과정의 흐름도이다. 도 4는 도 1의 화면의 오브젝트의 위치를 추론하는 인공지능 스크린 학습 모델을 학습시키기 위한 흐름도이다. 도 5는 브라우저 화면에서 학습된 인공지능 모델을 통해 오브젝트의 위치를 추론한 결과를 도시한 예시도이다. 도 6은 PC 바탕화면에서 학습된 인공지능 모델을 통해 오브젝트의 위치를 추론한 결과를 도시한 예시도이다. 도 7a는 도 4에 따른 화면의 오브젝트의 위치를 추론하는 인공지능 모델을 학습시킬 화면을 도시한 예시도이다. 도 7b는 도 4에 따른 화면의 오브젝트의 위치를 추론하는 인공지능 모델을 학습시킬 화면에서 오브젝트에 라벨 링을 한 예시도이다. 도 7c는 도 4에 따른 화면의 오브젝트의 위치를 추론하는 인공지능 모델을 학습시킨 후 오브젝트를 실제 인식한 결과의 예시도이다. 도 7d는 도 7a의 학습시킬 화면으로부터 마스크-RCNN을 적용하여 학습시키는 과정을 도시한 예시도이다. 도 8은 텍스트 및 비-텍스트 이미지를 포함하는 화면 정보를 도시한 예시도이다. 도 9a는 텍스트 및 비-텍스트 이미지 및 비-텍스트 이미지를 포함하는 특정 포털 사이트의 화면 정보를 도시한 예시도이다. 도 9b는 본 발명의 일 실시예에 따라 화면 UI 이미지들을 오브젝트 탐지에 따라 구별하는 경우를 도시한 예시도 이다. 도 9c는 본 발명의 일 실시예에 따라 화면 UI 이미지들을 오브젝트 탐지만으로 UI 이미지를 구별하지 못하는 텍 스트 이미지들을을 도시한 예시도이다. 도 10은 본 발명의 일 실시예에 따라 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 텍스트 이미지 를 인식하는 과정을 도시한 예시도이다. 도 11a는 본 발명의 일 실시예에 따라 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 텍스트 이미 지를 인식하여 오인식이 줄어드는 경우를 도시한 예시도이다. 도 11b는 본 발명의 일 실시예에 따라 텍스트 및 비-텍스트 이미지를 포함하는 화면 이미지로부터 텍스트 이미 지를 인식하여 에러정보로 인식 가능한 경우를 도시한 예시도이다."}
