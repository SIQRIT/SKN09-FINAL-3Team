{"patent_id": "10-2023-7041967", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0016975", "출원번호": "10-2023-7041967", "발명의 명칭": "오디오 및 비디오 트렌스레이터", "출원인": "딥 미디어 인크.", "발명자": "굽타 리줄"}}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "미디어 파일 내의 스피치를 번역하는 방법으로서, 상기 방법은,입력 미디어 파일을 획득하는 단계, 여기서 상기 입력 미디어 파일은 제1 입력 언어로 된 입력 오디오를포함함;제1 출력 언어를 획득하는 단계, 여기서 상기 제1 출력 언어는 상기 제1 입력 언어와 상이함;상기 입력 오디오를 복수의 음성 세그먼트로 세그먼트화하는 단계, 여기서 상기 복수의 음성 세그먼트의 각 음성 세그먼트는 각 음성 세그먼트의 화자를 식별하기 위한 화자 식별을 포함함;상기 복수의 음성 세그먼트에서 각 음성 세그먼트에 대해:각 음성 세그먼트의 각 단어 또는 음소에 대한 속도(pacing) 정보를 식별하는 단계;입력 전사본(transcription)을 획득하는 단계, 여기서 상기 입력 전사본은 각 음성 세그먼트에서 상기 발화된단어에 해당하는 텍스트를 포함함;입력 메타 정보를 획득하는 단계, 상기 메타 정보는 감정 데이터 및 어조 데이터를 포함하고, 여기서 감정 데이터는 소정의 감정 목록으로부터 검출 가능한 하나 이상의 감정에 해당함;상기 입력 전사본 및 입력 메타 정보를 적어도 타이밍 정보 및 상기 감정 데이터에 기초하여 상기 제1 출력 언어로 번역하는 단계, 이로 인해 상기 번역된 전사본 및 메타 정보는 상기 입력 전사본 및 입력 메타 정보와 비교하여 유사한 감정 및 속도를 포함하게 됨; 및번역된 입력 전사본 및 메타 정보를 사용하여, 번역된 오디오를 생성하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 입력 미디어 파일은 컴퓨터 판독 가능한 형식인, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,하나의 음성 스트림을 다른 음성 스트림으로부터 분할하거나, 배경 잡음을 감소시키거나, 또는 상기 음성 스트림의 품질을 향상시키기 위해 입력 오디오를 프리프로세싱(preprocessing)하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,입술 움직임 추적 데이터를 캡처하기 위해 상기 입력 비디오를 프리프로세싱하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 입력 오디오를 상기 복수의 음성 세그먼트로 세그먼트화하고 속도 정보를 식별하는 단계는 상기 입력 미디어 파일을 입력으로서 수신하도록 구성된 화자 구분(speaker diarization) 프로세서에 의해 수행되는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,공개특허 10-2024-0016975-3-상기 텍스트 전사본은 국제 음성학 알파벳에 따라 형식화되는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 입력 전사본은 각 음성 세그먼트에 대한 화자의 해부학적 랜드마크에 해당하는 정서(sentiment) 분석 및추적 데이터를 더 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 입력 오디오의 상기 입력 전사본을 획득하는 단계는 상기 입력 오디오를 텍스트로 변환하도록 구성된 인공지능(artificial intelligence(AI)) 생성기에 상기 입력 오디오를 제공하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,입력 메타 정보를 획득하는 단계는 메타 정보를 식별하도록 구성된 AI 메타 정보 프로세서에 상기 입력 오디오및 상기 입력 전사본을 제공하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,상기 입력 전사본 및 입력 메타 정보를 번역하는 단계는 상기 번역된 전사본 및 메타 정보를 생성하도록 구성된AI 전사본 및 메타 번역 생성기에 상기 입력 전사본 및 입력 메타 정보를 제공하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,유사한 속도는 20% 이하의 차이를 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항에 있어서,번역된 오디오를 생성하는 단계는 상기 번역된 오디오를 생성하도록 구성된 AI 오디오 번역 생성기에 상기 번역된 전사본 및 메타 정보를 제공하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항에 있어서,상기 번역된 오디오를 각 음성 세그먼트에 대해 단일 오디오 파일로 다시 스티칭(stitching)하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1항에 있어서,상기 입력 미디어 파일은 입력 비디오를 포함하는, 방법."}
{"patent_id": "10-2023-7041967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제15항에 있어서,상기 번역된 오디오 및 상기 입력 비디오를 비디오 동기화 생성기에 제공하는 단계, 및 상기 비디오 동기화 생성기에 의해, 상기 번역된 오디오가 상기 입력 비디오와 동기화되는 동기화 비디오를 생성하는 단계를 더 포함하는, 방법.공개특허 10-2024-0016975-4-"}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "원하는 경우, 오디오 및 비디오를 번역하는 시스템 및 방법. 번역은 AI 시스템을 사용하여 생성된 합성 미디어 및 데이터를 포함한다. 일련의 단계를 실행하는 고유한 프로세서 및 생성기를 통해, 상기 시스템 및 방법은 다 양한 스피치 특성 (예를 들면, 감정, 속도, 관용구, 풍자, 농담, 어조, 음소 등)을 고려할 수 있는 보다 정확한 번역을 생성한다. 이러한 스피치 특성은 입력 미디어에서 식별되고 번역된 출력에 합성적으로 통합되어 입력 미 디어의 특성을 반영한다. 일부 실시예는 화자의 얼굴 및/또는 입술이, 생성된 오디오를 원어민처럼 발화하는 것 처럼 보이도록 입력 비디오를 조작하는 시스템 및 방법을 더 포함한다."}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "관련 출원과의 상호 참조 본 정규 출원은 동일한 발명자(들)에 의해 2021년 5월 5일에 출원된 '이미지 번역기'라는 명칭의 가출원 제 63/184,746에 대한 우선권을 주장한다."}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "1. 발명의 분야 본 발명은 일반적으로 비디오 및 오디오 조작에 관한 것이다. 보다 상세하게는, 본 발명은 오디오 번역 및 입 술 재현(reanimation)에 관한 것이다. 2. 선행 기술에 대한 간략한 설명 기존의 오디오 번역 기술은 매우 지루하고 시간이 많이 소요된다. 오디오를 듣고, 녹음하고, 기록하고, 번역하 기 위해서는 한 명 이상이 필요한 경우가 많다. 기존 비디오에 번역된 오디오를 더빙하는 것은 훨씬 더 어려울 수 있으며, 종종 상당한 인적 투자와 개입을 필요로 한다. 또한, 번역된 오디오는 해당 비디오에서 화자의 입 술 움직임과 거의 동기화되지 않는다. 따라서, 보다 효율적이고 효과적으로 오디오를 번역하고 화자의 입술을 비디오에 재현할 수 있는 시스템 및 방 법이 필요하다. 그러나, 본 발명이 이루어질 당시의 기술을 전체적으로 고려할 때, 본 발명 분야의 통상의 기 술자에게는 선행 기술의 단점을 어떻게 극복할 수 있을지 분명하지 않았다. 참조된 모든 간행물은 그 전체가 참조로서 본원에 포함된다. 또한, 본원에 참조로서 포함된 참고 문헌의 용어 의 정의 또는 사용이 본원에 제공된 해당 용어의 정의와 일치하지 않거나 또는 상반되는 경우, 본원에 제공된 해당 용어의 정의가 적용되며 참고 문헌의 해당 용어의 정의는 적용되지 않는다. 본 발명의 개시를 용이하게 하기 위해 종래 기술의 특정 양상이 논의되었지만, 출원인은 이러한 기술적 양상의 권리를 포기하지 않으며, 청구된 발명이 본원에서 논의된 종래 기술적 양상 중 하나 이상을 포함할 수 있다고 생각된다. 본 발명은 위에서 논의한 선행 기술의 문제점 및 결함 중 하나 이상을 해결할 수 있다. 그러나, 본 발명은 다 수의 기술적 영역에서 다른 문제 및 결함을 해결하는 데 유용할 수 있다고 생각된다. 따라서, 청구된 발명이 반드시 본원에서 논의된 특정 문제 또는 결함 중 어느 하나를 해결하는 것으로 제한되는 것으로 해석되어서는 안 된다. 본 명세서에서 문서, 행위 또는 지식 항목이 언급되거나 또는 논의되는 경우, 이러한 언급 또는 논의는 해당 문 서, 행위 또는 지식의 항목 또는 이들의 임의의 조합이 해당 법률 조항에 따라 우선일에 공개적으로 이용 가능 하거나, 대중에게 알려져 있거나, 보통의 일반 지식의 일부이거나, 또는 선행 기술을 구성하는 것임을 인정하는 것이 아니며; 본 명세서와 관련된 모든 문제를 해결하려는 시도와 관련된 것으로 알려져 있다. 향상된 오디오 및 비디오 번역기에 대한 오래된 숙원이었지만 지금까지 충족되지 않았던 요구가 이제 새롭고, 유용하며, 비직관적인 발명에 의해 충족된다. 본 발명은 미디어 파일 내의 스피치를 번역하는 시스템 및 방법을 포함한다. 본 방법의 일 실시예는 먼저 입력 미디어 파일을 획득하는 단계를 포함한다. 일부 실시예에서, 입력 미디어 파일은 컴퓨터 판독 가능한 형식이다. 입력 미디어 파일은 제1 입력 언어로 된 입력 오디오를 포함하며, 일부 실시예에서는 입력 비디오를 포함한다. 본 방법은 제1 출력 언어를 획득하는 단계를 더 포함하며, 제1 출력 언어는 제1 입력 언어와 상이하 다. 일부 실시예는, 하나의 음성 스트림을 다른 음성 스트림으로부터 분할하거나, 배경 잡음을 감소시키거나, 또는 음성 스트림의 품질을 향상시키기 위해 입력 오디오를 프리프로세싱(preprocessing)하는 단계를 더 포함한다. 일부 실시예는 또한 입술 움직임 추적 데이터를 캡처하기 위해 입력 비디오를 프리프로세싱하는 단계를 포함한다. 입력이 획득되면, 입력 오디오는 복수의 음성 세그먼트로 세그먼트화된다. 복수의 음성 세그먼트의 각 음성 세 그먼트는 각 음성 세그먼트의 화자를 식별하기 위한 화자 식별을 포함한다. 복수의 음성 세그먼트의 각 음성 세그먼트에 대해 각 음성 세그먼트의 각 단어 또는 음소에 대해 속도(pacing) 정보가 식별된다. 일부 실시예에 서, 입력 오디오를 복수의 음성 세그먼트로 세그먼트화하고 타이밍 정보를 식별하는 단계는 입력 미디어 파일을 입력으로서 수신하도록 구성된 화자 구분(speaker diarization) 프로세서에 의해 수행된다. 이 신규한 방법은 입력 전사본을 획득하는 단계를 더 포함한다. 입력 전사본은 각 음성 세그먼트에서 말한 단 어에 해당하는 텍스트를 포함한다. 텍스트 전사본은 국제 음성학 알파벳에 따라 형식화될 수 있다. 또한, 입 력 전사본은 각 음성 세그먼트에 대한 화자의 해부학적 랜드마크에 해당하는 정서 분석 및 추적 데이터를 더 포 함할 수 있다. 일부 실시예에서, 입력 오디오의 입력 전사본을 획득하는 단계는 입력 오디오를 텍스트로 변환 하도록 구성된 인공 지능(artificial intelligence (AI)) 생성기에 입력 오디오를 제공하는 단계를 포함한다. 이어서, 입력 메타 정보가 획득된다. 메타 정보는 감정 데이터 및 어조 데이터를 포함한다. 감정 데이터는 소 정의 감정 목록으로부터 검출 가능한 하나 이상의 감정에 해당한다. 어조 데이터도 마찬가지로 소정의 감정 목 록 또는 어조 스펙트럼으로부터 검출 가능한 하나 이상의 어조에 해당할 수 있다. 일부 실시예에서, 입력 메타 정보를 획득하는 단계는 메타 정보를 식별하도록 구성된 AI 메타 정보 프로세서에 입력 오디오 및 입력 전사본 을 제공하는 단계를 포함한다. 메타 데이터가 획득되면, 입력 전사본 및 입력 메타 정보는 적어도 타이밍 정보 및 감정 데이터에 기초하여 제1 출력 언어로 번역되므로, 번역된 전사본 및 메타 정보는 입력 전사본 및 입력 메타 정보와 비교하여 유사한 감 정 및 속도를 포함하게 된다. 일부 실시예에서, 유사한 속도는 음성 문자 사이의 해밍 거리(hamming distanc e)의 차이가 20% 이하이고 적절한 위치에 일시 정지, 호흡, 및 필러 사운드를 포함하는 것을 포함한다. 일부 실시예에서, 입력 전사본 및 입력 메타 정보를 번역하는 단계는 번역된 전사본 및 메타 정보를 생성하도록 구성 된 AI 전사본 및 메타 번역 생성기에 입력 전사본 및 입력 메타 정보를 제공하는 단계를 포함한다. 마지막으로, 번역된 입력 전사본 및 메타 정보를 사용하여, 번역된 오디오가 생성된다. 일부 실시예에서, 번역 된 오디오를 생성하는 단계는 번역된 오디오를 생성하도록 구성된 AI 오디오 번역 생성기에 번역된 전사본 및 메타 정보를 제공하는 단계를 포함한다. 본 방법의 일부 실시예는 번역된 오디오를 각 음성 세그먼트에 대해 단일 오디오 파일로 다시 스티칭 (stitching)하는 단계를 더 포함한다. 일부 실시예는 번역된 오디오 및 입력 비디오를 비디오 동기화 생성기에 제공하는 단계, 및 비디오 동기화 생성기에 의해, 번역된 오디오가 입력 비디오와 동기화되는 동기화 비디오를 생성하는 단계를 더 포함한다. 본 발명의 이러한 목적과 기타 중요한 목적, 이점, 및 특징은 본 개시가 진행됨에 따라 명확해질 것이다. 따라서 본 발명은 이하에 설명되는 개시에서 예시될 구성, 구성 요소들의 조합, 및 부품의 배열의 특징을 포함 하며, 본 발명의 범위는 청구범위에 기재될 것이다."}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "바람직한 실시예에 대한 하기 상세한 설명에서는, 그 일부를 구성하는 첨부된 도면을 참조하고, 그 안에서 본 발명이 실시될 수 있는 구체적인 실시예를 예시적으로 도시한다. 본 발명의 범위를 벗어나지 않는 범위 내에서 다른 실시예가 활용될 수 있으며, 구조적 변경이 이루어질 수 있음을 이해해야 한다. 본 명세서 및 첨부된 청구범위에서 사용된 바와 같이, 단수 형태(\"a\", \"an\", 및 \"the\")는 내용이 명확하게 달리 지시하지 않는 한 복수 형태를 포함한다. 본 명세서 및 첨부된 청구범위에서 사용된 바와 같이, 용어 \"또는\"은 문맥이 명확하게 달리 지시하지 않는 한 일반적으로 \"및/또는\"을 포함하는 의미로 사용된다. \"일부 실시예들에서\", \"일부 실시예들에 따라\", \"도시된 실시예들에서\", \"다른 실시예들에서\" 등의 문구는 일반 적으로 문구 뒤에 오는 특정 특징, 구조, 또는 특성이 적어도 하나의 구현에 포함된다는 것을 의미한다. 또한, 이러한 문구가 반드시 동일한 실시예 또는 다른 실시예를 지칭하는 것은 아니다. 하기 설명에는, 설명의 목적을 위해, 본 기술의 실시예에 대한 완전한 이해를 제공하기 위해 수많은 구체적인 세부 사항이 기재되어 있다. 그러나 해당 기술 분야의 통상의 기술자에게는 이러한 일부 구체적인 세부 사항 없이 본 기술의 실시예가 실행될 수 있음이 명백할 것이다. 본원에 소개된 기술은 특수-목적 하드웨어 (예를 들면, 회로), 소프트웨어 및/또는 펌웨어로 적절히 프로그래밍된 프로그램 가능한 회로로서, 또는 특수-목적 회 로 및 프로그램 가능한 회로의 조합으로서 구현될 수 있다. 따라서, 실시예는 프로세스를 수행하기 위해 컴퓨 터 (또는 기타 전자 기기)를 프로그래밍하는 데 사용될 수 있는 명령어를 그 안에 저장한 기계 판독 가능한 매 체를 포함할 수 있다. 기계 판독 가능한 매체에는 플로피 디스켓, 광 디스크, 컴팩트 디스크 읽기 전용 메모리 (compacts disc read-only memory(CD-ROM)), 자기 광학 디스크, ROM(read-only memory), 랜덤 액세스 메모리 (random access memory(RAM)), 지울 수 있는 프로그램 가능한 읽기 전용 메모리(erasable programmable read- only memory(EPROM)), 전기적으로 지울 수 있는 프로그램 가능한 읽기 전용 메모리(electrically erasable programmable read-only memory(EEPROM)), 자기 또는 광학 카드, 플래시 메모리, 또는 전자 명령어를 저장하기 에 적합한 기타 유형의 미디어/기계 판독 가능한 매체가 포함되지만, 이에 제한되지 않는다. 이제 본 발명의 구체적인 사항을 참조하면, 일부 실시예는, 메모리, 시각적 디스플레이를 갖는 사용자 인터페이 스(또한 \"그래픽 사용자 인터페이스\" 또는 \"GUI(graphic user interface)\"라고 지칭함), 및 적어도 본원에 설명 된 단계를 수행하는 프로그램을 실행하기 위한 프로세서를 갖는 하나 이상의 컴퓨터 시스템을 포함한다. 일부 실시예에서, 본 발명은 컴퓨터 실행 가능한 방법 또는 본원에 설명된 단계를 실행하기 위한 소프트웨어에 구현 된 방법이다. 하드웨어 및 소프트웨어에 대한 추가적 설명은 이하의 하드웨어 및 소프트웨어 인프라 예시 섹션 에서 확인할 수 있다."}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "미디어 파일은 비디오 파일 및/또는 오디오 파일을 지칭한다. 이러한 미디어 파일은 해당 기술분야의 통상의 기술자에게 공지된 임의의 파일 형식을 가질 수 있다. 미디어 파일의 대상체는 미디어 파일 내에서 오디오의 출처가 되는 식별 가능한 객체, 화자, 사람, 또는 동물이다. 각 미디어 파일에 두 개 이상의 대상체가 있을 수 있다. 입력 미디어 파일은 번역을 위해 번역기에 제공되거나 또는 획득된 미디어 파일이다. 출력 미디어 파일은 번역 이 발생한 입력 미디어 파일의 합성 또는 조작된 버전이다. 출력 미디어 파일은 입력 미디어 파일에서 발화 언 어와 상이한 언어로 발화하는 것처럼 보이는 하나 이상의 대상체를 묘사한다. 일부 실시예에서, 출력 미디어 파일은 새로운 언어에 따라 움직이는 대상체의 얼굴 랜드마크(예를 들면, 대상체의 입술)의 적어도 일부를 더 포함한다. 현재의 번역 작업은 텍스트 대 텍스트 솔루션을 기반으로 하므로, 전화 통화, 비디오 통화, 오디오북 번역, 자 막 생성, 원어민 출현 비디오 생성 등을 포함하지만 이에 제한되지 않는 오디오 및/또는 비디오를 포함하는 커 뮤니케이션에 유용하지 않다. 본 발명은 합성 미디어 (즉, 생성 AI, 생성 적대적 네트워크, 딥페이크 시스템,및 미디어를 그것의 원본 형태로부터 조작하도록 구성된 기타 시스템 및 방법을 사용하여 생성된 미디어)를 생 성 및 사용하여 학습 및 추론 (추론은 AI 기반 시스템을 사용하여 출력을 생성하는 것을 지칭하는 용어이며, 이 경우 합성 미디어)하는 동안 텍스트, 오디오, 및 비디오 정보를 결합하여 엔드투엔드(end-to-end) 번역 시스템 을 가능하게 한다. 본 발명의 시스템 및 방법은 고유한 일련의 단계를 실행하는 고유한 프로세서 및 생성기를 통해, 다양한 스피치 특성 (예를 들면, 관용구, 풍자, 농담, 어조, 음소 등)을 고려할 수 있는 보다 정확한 번역을 생성한다. 또한, 오디오 번역은 해당 오디오 신호 (예를 들면, 멜스펙토그램(MelSpectogram) 및/또는 원시 오디오 파형)의 디지 털 표현을 생성 AI와 통합함으로써 화자의 음성 신원, 어조, 억양, 감정 등을 일치시킨다. 또한, 시스템은 화 자의 얼굴 (적어도 화자의 입술)이, 생성된 오디오를 원어민처럼 발화하는 것처럼 보이도록 프로그레시브 GAN을 포함하는 고해상도 생성 AI를 통해 비디오를 생성/조작할 수 있다. 도 1에 도시된 바와 같이, 본 발명은 일반적으로 입력 미디어 파일을 포함하며, 이는 입력 비디오, 입력 오디오를 포함할 수 있고, 선택적으로 입력 전사본을 포함할 수 있다. 입력 언어 및 출력 언어도 번역기에 제공된다. 번역기는 입력 정보를 사용하여 출력 미디어를 생성한다. 출 력 미디어는 출력 비디오, 출력 오디오, 및/또는 출력 언어로 번역된 언어를 갖는 출력 전사본 을 포함할 수 있다. 입력 미디어 파일은 입력 비디오, 입력 오디오, 및/또는 입력 전사본 을 포함할 수 있기 때문에, 일부 실시예는 어떤 정보가 번역 프로세스에 포함될지 및 번역 프로세스가 오 디오 및 비디오 번역/조작을 모두 포함할지 판단하기 위해 입력 미디어 파일로부터 오디오, 비디오, 및/또 는 전사본 정보를 검출하도록 구성된 식별 시스템을 포함한다. 입력 미디어는 본원에 설명된 바와 같이 번역기가 구성될 수 있는 다양한 프리프로세서 또는 생성기 중 하나 이상에 제공되는 경우 컴퓨터 판독 가능한 형식으로 변환되거나 및/또는 제공될 수 있다. 다양한 입력 에 대해 컴퓨터 판독 가능한 형식의 비제한적인 예로는 이진 벡터 및 문자열 벡터가 포함된다. 이진 벡터는 1-"}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "핫(1-hot) 벡터 및 다중 클래스 벡터를 포함하지만 이에 제한되지 않는 해당 기술분야에 공지된 임의의 벡터일"}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수 있다. 마찬가지로, 문자열 벡터는 해당 기술분야에 공지된 임의의 벡터일 수 있다. 일부 실시예는 하나 이 상의 입력을 국제 음성학 알파벳(international phonetics alphabet(IPA))의 문자열로 변환한다. 후속 단락에 서 설명하겠지만, IPA 문자열을 사용하면 서로 다른 언어의 동일한 단어 사이의 음성학적인 차이와 연관된 오류 를 감소시킬 수 있다. 일부 실시예에서, 사용자는 입력 언어 및/또는 출력 언어를 식별한다. 일부 실시예에서 입력 언어 및 출력 언어는 본원에 설명된 것과 유사한 컴퓨터 판독 가능한 형식으로 표시되고 제공된다. 예를 들면, 입력 언어 및 출력 언어는 단일 1-상태가 각 언어에 해당하는 모든 가능한 언어 크기의 이진 벡터의 형태로 제공될 수 있다. 일부 실시예에서, 입력 언어는 스피치 인식 소프트웨어를 통해 자동으로 식별된다. 마찬가지로, 일부 실시예는 입력 전사본을 자동으로 생성하기 위한 스피치-텍스트 변환 (speech-to-text(STT)) 시스템/소프트웨어를 포함한다. 아래에서 더 자세히 설명하는 바와 같이, 본 발명의 일부 실시예는 번역 전에 입력 정보를 식별, 추출, 및/또는 조작하기 위한 하나 이상의 프로세서(\"프리프로세서\"라고도 지칭함)를 포함한다. 또한, 일부 실시예는 다양한 정보의 합성 생성을 통해 번역을 개선하도록 구성된 복수의 생성기로 구성된 번역기를 포함한다. 본 발명의 일 부 실시예는 번역의 품질 및/또는 출력 미디어의 품질을 향상시키도록 구성된 포스트 프로세서를 더 포함한다. 이러한 다양한 구성 요소는 후속 섹션에서 더 상세히 논의된다. 도 2는 번역 프로세스의 대략적인 개요를 제공하며, 이는 적어도 부분적으로 도 3 내지 도 7의 도면들에 해당한다. 도시된 바와 같이, 번역 프로세스의 예시적인 실시예는, 먼저 단계에서 디지털 입력 미디"}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "어 파일을 수신 또는 획득하는 단계를 포함한다. 입력 미디어 파일은 해당 기술분야에 공지된 임의의 시 스템 및 방법을 통해 획득되거나 또는 제공될 수 있다. 앞서 언급한 바와 같이, 입력 미디어 파일은 비디오, 오디오, 및/또는 전사 정보를 포함할 수 있다. 간결 하고 명확하게 하기 위해, 하기 설명은 입력 비디오와 입력 오디오를 갖는 입력 미디어 파일을 참조할 것이다. 입력 미디어 파일이 단순한 오디오 파일인 경우 비디오 번역에 해당하는 단계는 수행되지 않을 것이다. 또한, 입력 미디어 파일은 복수의 또는 단일 디지털 입력 파일로서 제공될 수 있지만, 예시적인 도면에서는 입력 비디오 및 입력 오디오를 별도의 입력으로서 도시한다. 이제 도 3을 참조하면, 일부 실시예는 아래에 설명된 바와 같이 입력 미디어를 프리프로세싱하는 단계를 포함한다. 입력 미디어는 비디오 프리프로세서 및/또는 오디오 프리프로세서에 제공될 수 있다. 일부 실시예에서, 이러한 프리프로세서는 화자 구분 프로세서가 입력 오디오를 올바르게 분할하고, 입력 전사본 생성기가 보다 정확한 입력 전사본을 생성하는 능력을 향상시키도록 구성된다. 오디오 프리프로세서는, 각 화자에 대한 오디오 콘텐츠를 별도의 오디오 트랙으로 분할하고, 배경 잡음을 제거하거나 또는 정리하고, 음성 품질 데이터를 향상시키는 프로세스를 포함할 수 있다. 이러한 프로세스는 본 원에 열거된 프로세스를 수행할 수 있는 공지된 임의의 시스템 및 방법을 사용하여 수행될 수 있다. 일부 실시"}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "예에서, 오디오 프리프로세서는 또한 해당 기술분야에 공지된 것들과 같은 음성 인식 소프트웨어를 사용하 여 입력 언어를 자동으로 식별하도록 구성된다. 비디오 프리프로세서는 비디오 내의 대상체를 식별 및 추적하는 프로세스를 포함할 수 있다. 예를 들면, 비디오 프리프로세서는 얼굴 검출 소프트웨어를 사용하여, 예를 들면, 8픽셀 2D 랜드마크, 68픽셀 2D 랜드 마크, 기타 2D 얼굴 랜드마크, 기타 3D 얼굴 랜드마크를 사용하여 비디오에 묘사된 각 대상체를 추적하는 얼굴 경계 박스를 생성할 수 있다. 일부 실시예에서, 비디오 프리프로세서는, 예를 들면, 13픽셀 2D 랜드마크, 기타 2D 신체 랜드마크, 기타 3D 신체 랜드마크 등을 사용하는 신체 추적 소프트웨어를 사용하여, 각 대상체를 추적하기 위한 신체 경계 박스 를 생성할 수 있다. 모든 유형의 경계 박스 또는 신원 추적 소프트웨어를 사용하여 비디오의 프레임 전체에서 대상체를 식별 및 추적할 수 있다. 일부 실시예에서, 비디오 프리프로세서는 입술 움직임을 식별 및 추적 하도록 구성되며, 이는 비디오에서 특정 음성 세그먼트 동안 어떤 화자가 말하고 있는지 판단하는 데 사용된다. 일부 실시예에서, 비디오 프리프로세서의 출력은 오디오 프리프로세서에 공급된다. 오디오 프리프로 세서에 비디오 정보를 공급하면, 오디오 프리프로세서는 오디오만으로는 구별하기 어려운 단어/음소 (예를 들면, \"B\" 대 \"V\"의 발음)를 더 잘 이해할 수 있다. 이제 도 2 내지 도 3을 참조하면, 입력 미디어 파일이 획득된 후, 단계에서 화자 구분이 실행된다. 일부 실시예에서, 입력 미디어 및 입력 언어는 단계에서 화자 구분 프로세서에 제공된다. 일부 실시예에서, 입력 오디오는 입력 비디오 없이 화자 구분 프로세서에 제공된다. 일부 실시 예는 프리프로세싱된 오디오 출력과 함께 원본 입력 오디오를 오디오 프리프로세서로부터 화자 구분 프로세서로 제공한다. 화자 구분 프로세서는 식별 가능한 화자에 따라 입력 오디오를 균질한 음성 세그먼트로 분할하도록 구성된다. 궁극적으로, 화자 구분 프로세서는 입력 미디어에서 하나 이상의 화자를 식별하고 각 음 성 문자열(또한 음성 세그먼트라고 칭함)을 적절한 화자와 연관시키기 위해 일련의 단계를 수행한다. 일부 실 시예에서, 화자 구분 프로세서의 출력은 입력 오디오에 해당하는 일련의 음성 세그먼트를 포함하며, 각 세그먼트는 화자 식별자 또는 화자의 신원에 대한 참조를 포함한다. 일부 실시예에서, 화자 구분 프로세서 는 오디오의 각 단어/음절/음소에 대한 시간 코드 (예를 들면, 각 단어의 시작 및 종료 시간)를 캡처하고, 누가 말하는지 식별하고, 말한 단어를 식별하고, 및 화자의 연관된 특성을 식별하도록 더 구성된다. 화자 구분 프로세서의 출력은 기침, 재채기, 스피치의 일시 정지 및 기타 비언어적 오디오 세그먼트 또는 화자에 의 해 생성된 비언어적 잡음에 대한 식별 및 연관된 시간 코드를 더 포함할 수 있다. 다른 화자 구분 정보와 마찬 가지로, 이러한 데이터는 전체 시스템을 통해 제공된다. 화자 구분 프로세서는, 화자를 식별하고 화자를 특정 음성 세그먼트와 연관시키거나 및/또는 상술한 임의의 다른 정보를 캡처하도록 구성될 수 있는, 해당 기술 분야에 공지된 임의의 화자 구분 시스템일 수 있다. 화자 구분 프로세서의 일부 실시예는 입력 비디오에 기초하여 특정 음성 세그먼트를 화자와 연관시키 도록 더 구성된다. 이는 각 화자의 얼굴, 식별 가능한 특성 및/또는 얼굴 움직임을 추적함으로써 이루어진다. 예를 들면, 일부 실시예는 얼굴 궤적(trajectory) 분석을 사용하여 특정 음성 세그먼트에 대한 화자의 특성을 추적, 식별, 및 캡처한다. 이러한 실시예에서, 화자 구분 프로세서의 출력은 일련의 음성 세그먼트와 연 관된 얼굴 궤적 데이터를 더 포함한다. 화자 구분의 출력은 반드시 비디오 자체는 아니고, 그 안에 포함되거나 또는 그것과 연관된 컴퓨터 판독 가능한 데이터이다. 얼굴 궤적 분석과 연관된 데이터는 얼굴이 묘사되는 시작 및 종료 시간, 다른 사람과 비교한 개별 주체의 신원, 성별, 화면에 표시된 시간, 오디오 기반의 말하기 시간, 및 말하는 사람을 식별하기 위한 립싱크 분석을 포함할 수 있다. 이 모든 정보는 누가 말하고 있는지 및 식별 가능한 특성이 음성 특성에 어떻게 영향을 미칠 수 있는 지 판단하기 위해 사용될 수 있다. 예를 들면, 비디오가 남성과 여성 대상체가 동시에 말하는 것을 묘사하는경우, 남성적인 어조를 인식하면 화자를 남성 대상체로서 식별하는 데 도움이 될 수 있다. 각 음성 세그먼트와 얼굴 정보를 연관시키면, 후속 섹션에 설명되는 바와 같이 동기화된 비디오를 생성하 는 데 더 도움이 된다. 그러나, 일부 입력 미디어는 입력 비디오를 포함하지 않는다. 또한, 본 발 명의 일부 실시예는 번역된 오디오를 입력 비디오에 추가로 동기화하지 않고 번역된 오디오를 출력한다. 이러한 경우, 화자 구분 프로세서는 음성 세그먼트를 얼굴 추적 데이터와 연관시킬 필요가 없다. 화자 구분 후, 단계에서 출력이 입력 전사본 생성기에 제공되어 입력 전사본을 생성한다. 도 3을 참 조하면, 화자 구분에 따른 오디오의 각 화자 식별 세그먼트는 입력 전사본 생성기에 제공되어 음성 세그먼 트를 세그먼트화된 입력 전사본으로 변환한다. 입력 전사본은 하기에 설명된 데이터를 포함하는 입 력 전사본을 생성하도록 구성된 임의의 공지된 시스템 또는 방법을 사용하여 생성될 수 있다. 입력 전사본은 단지 발화 단어부터 입 움직임, 음소, 타임스탬프, 및 기타 그러한 설명에 대한 매우 상세 한 데이터까지 모든 것을 포함할 수 있다. 종종, 입력 전사본는 발화되는 언어, 이름/적절한 명사의 식별, 정서 분석, 단어 및/또는 음절의 타임스탬프/타임 인덱스, 및/또는 오디오에서 발화하는 각 개별 사람에 대한 타임스탬프를 갖는 음소를 포함할 것이다. 일부 실시예에서, 프로세싱되지 않은 원본 입력 비디오 및/또는 입력 오디오는 또한 입력 전사본 생 성기에 제공된다. 일부 실시예에서, 비디오 프리프로세서 및/또는 오디오 프리프로세서의 출력 은 또한 입력 전사본 생성기에 제공된다. 일부 실시예는 입력 전사본 생성기에 입력 언어를 더 제공한다. 일부 실시예에서, 입력 전사본은 사용자에 의해 제공되거나 또는 준비된다. 이러한 상황에서, 본 발명은 입력 전사본 생성기를 포함하지 않거나 또는 입력 전사본의 생성 단계를 단순히 우회한다. 일부 실 시예는 검토를 위해 사용자에게 입력 전사본을 제시하고 사용자에게 전사본을 수정할 수 있는 기능을 제공 한다. 사용자는 입력을 수정한 다음 수정된 입력을 입력 전사본 생성기로 전송하여 향상된 출력을 생성할 수 있다. 이제 도 2 및 도 4를 참조하면, 일부 실시예는 단계에서 입력 오디오 및 입력 전사본로부터 메 타 정보를 식별하는 단계를 더 포함한다. 도 4에 도시된 바와 같이, 일부 실시예는 입력 오디오, 입력 전 사본, 및 입력 언어를 메타 정보 프로세서로 전송한다. 일부 실시예에서, 입력 오디오 및 /또는 입력 비디오는 프리프로세서(124 및/또는 126)으로부터 메타 정보 프로세서로의 출력과 함께 제공된다. 마찬가지로, 일부 실시예는 원본 입력 전사본 및/또는 텍스트 프리프로세서를 통해 프로 세싱된 후의 입력 전사본을 제공한다. 일부 실시예에서, 텍스트 프리프로세서는 텍스트를 음소 분석으로 변환하거나 및/또는 감정/정서 분석을 수행하도록 구성된다. 이러한 분석은 음성 세그먼트에 해당하는 데이터 및 연관된 화자 구분 데이터를 포함하 는 입력 전사본로부터 그러한 데이터를 추출하도록 구성된 임의의 공지된 시스템 및 방법을 사용하여 수행 될 수 있다. 메타 정보 프로세서는 다양한 메타 정보를 식별 및 각 음성 세그먼트와 연관시키도록 구성될 수 있다. 메 타 정보의 비제한적인 예로는 감정, 강세, 속도/운율/리듬, 음소 분석, 어조, 연령, 성별, 인종이 포함된다. 일부 실시예에서, 메타 정보 프로세서는 적어도 감정 데이터 및 속도 데이터를 식별 및 각 음성 세그먼트 와 연관시킨다. 감정 데이터는 검출 가능한 모든 감정을 포함한다. 감정의 비제한적인 예로는 행복, 슬픔, 분노, 무서움, 혼란, 흥분, 피곤, 풍자, 혐오, 두려움, 및 놀라움이 포함된다. 감정 데이터는 소정의 감정 목록으로 더 컴파 일될 수 있으며, 감정은 1-핫 또는 다중 클래스 벡터, 신경망의 마지막에서 두번째 계층 또는 유사성을 판단하 는 샴 네트워크의 출력과 같은, 컴퓨터 판독 가능한 형식을 사용하여 하나 이상의 프로세서 및 생성기에 전달될 수 있다. 기타 다양한 유형의 메타 정보를 식별 및 전달하기 위해서 동일한 접근 방식이 사용될 수 있다. 속도/운율/리듬(이하 '속도'라고 칭함)은 각 음절, 단어, 기타 음소, 비언어적 스피치(예를 들면, 기침, 웃음, 헐떡임) 또는 스피치의 일시정지와 연관된 측정 가능한 시간으로, 0.05초의 시간 분해능으로 측정된다. 속도 정보가 알려져 있고 데이터를 통해 흐르는 경우, 생성기는 동일한 속도와 일치하거나 또는 거의 일치하는 출력 을 생성할 수 있다. 그 결과, 번역된 텍스트, 오디오, 및/또는 비디오는 입력된 오디오, 비디오, 및/또는 텍스 트와 유사하거나 또는 일치하는 속도로 생성된다.다양한 메타 정보는 상술한 하나 이상의 메타 정보를 식별 및 생성하도록 구성된 임의의 공지된 시스템 및 방법 을 사용하여 식별 및 생성될 수 있다. 일부 실시예에서, 다양한 메타 정보는 각 음성 세그먼트에 대한 감정 데 이터 및 속도 데이터를 식별 및 생성하도록 구성된 임의의 공지된 시스템 및 방법을 사용하여 식별 및 생성될 수 있다. 메타 정보 프로세서는 각 음성 세그먼트에 대한 메타 정보를 식별 및 캡처하고, 메타 정보를 각 음성 세그 먼트와 연관시킨다. 이러한 정보의 조합이 입력 메타 정보로 캡처된다. 이러한 정보를 캡처 및 제공하면 스피치 중의 고유성을 캡처할 수 있다. 이러한 정보를 사용하면, AI 생성기는 감정이 스피치에 어떤 영향을 미 치는지 파악하기 위해 특성에 대해 학습될 수 있다. 학습 후, AI 생성기는 문장이 감정을 포함하는 경우를 파 악하여 식별된 감정을 포함하는 합성 오디오를 생성할 수 있다. 적절히 학습되면, 메타 정보 프로세서는 다양 한 수준의 분노 또는 상이한 악센트, 감정, 속도 등이 포함된 오디오 등 다중-라벨링된 출력을 생성할 수 있다. 일부 실시예에서, 메타 정보 프로세서는 메타 정보 및 화자 구분 데이터가 각 음성 세그먼트와 연관되는 입력 메타 정보를 생성한다. 따라서, 입력 메타 정보는 전사본 및 메타 번역 생성기에 의해 사 용 가능한 형식의 화자 구분 데이터에 대한 속도 및 타임코드를 포함한다. 일부 실시예에서, 입력 메타 정보 는 음소로 변환된 화자 구분 데이터를 포함하며, 이에 따라 시스템은 음소 유사성에 기초하여 입력과 일치 하도록 번역된 출력을 조정할 수 있다. 일부 실시예에서, 입력 메타 정보는 사용자에 의해 제공되거나 또는 준비된다. 이러한 상황에서, 본 발명 은 메타 정보 프로세서를 포함하지 않거나 또는 입력 메타 정보를 생성하는 단계를 단순히 우회한다. 일부 실시예는 검토를 위해 사용자에게 입력 메타 정보를 제시하고, 사용자에게 입력 메타 정보를 수 정할 수 있는 기능을 제공한다. 일부 실시예는 수정을 위해 사용자에게 입력을 제시하는 단계를 포함하며, 사 용자는 입력을 수정한 다음 수정된 입력을 AI 메타 정보 프로세서로 전송하여 향상된 출력을 생성할 수 있 다. 단계에서, 본 발명은 입력 전사본을 입력 언어에서 출력 언어로 번역한다. 도 5에 예시된 바와 같이, 다양한 입력이 전사본 및 메타 번역 생성기에 제공되며, 이는 입력된 전사본을 번역된 전사본 및 번역된 메타 정보의 형태로 출력 언어로 번역한다. 번역된 전사본 및 번역된 메타 정 보는 단일 데이터 세트로서 제공될 수 있지만, 도면에서는 명확성을 위해 정보를 분리하여 나타낸다. 전 사본 및 메타 번역 생성기의 출력은 속도, 감정, 억양, 어조 등을 포함할 수 있다. 일부 실시예에서, 입력은 입력 전사본, 텍스트 프리프로세서의 출력, 및 입력 메타 정보만을 포 함한다. 일부 실시예에서, 입력은 또한 입력 비디오, 오디오 입력, 비디오 프리프로세서의 출 력 및/또는 오디오 프리프로세서의 출력을 포함한다. 또한, 입력 언어 및 출력 언어는 전사본 및 메타 번역 생성기에 제공된다. 일부 실시예는 입력 전사본(원시 또는 프리프로세싱됨) 과 입력 언어 및 출력 언어만을 전사본 및 메타 번역 생성기로 전송하여 번역된 전사본을 생성한다. 일부 실시예는 적어도 입력 전사본(원시 또는 프리프로세싱됨)과 입력 언어 및 출력 언어 를 전사본 및 메타 번역 생성기로 전송하여 번역된 전사본을 생성한다. 입력 메타 정보를 포함하면, 전사본 및 메타 번역 생성기는 입력 메타 정보를 통해 식별된 다양 한 스피치 특성을 갖는 번역된 전사본 및 번역된 메타 정보를 생성할 수 있다. 이러한 특성에는 풍 자, 유머, 음소, 음소를 일치시키기 위한 속도 등이 포함되지만 이에 제한되지 않는다. 입력 오디오 및/ 또는 오디오 프리프로세서의 출력을 전사본 및 메타 번역 생성기에 공급하면, 오디오 내에 포함된 풍 자, 유머, 관용어, 및 기타 정보에 변함이 없는 전사본을 또한 만들 수 있다. 입력 비디오 및/또는 비디 오 프리프로세서의 비디오 정보는 또한 전사본 및 메타 번역 생성기에 대한 입력으로서 제공될 수 있 으며, 이는 다른 감정 정보를 포함할 수 있고 번역된 전사본 및 번역된 메타 정보를 더욱 향상시킬 수 있다. 일부 입력 미디어에서는, 입력 오디오에서 두 개 이상의 언어 (예를 들면, 영어 및 스페인어)가 발화 될 수 있다. 이러한 정보는 종종 입력 전사본 내에 있을 것이다. 두 개 이상의 입력 언어를 번역하 는 경우, 전사본 및 메타 번역 생성기에는 각 입력 언어 (예를 들면, 영어를 독일어로, 스페인어를 독일어 로, 또는 영어를 독일어로, 및 스페인어를 프랑스어로)에 대한 특정 출력 언어가 제공된다. 일부 실시예에서, 번역된 전사본 및/또는 번역된 메타 정보는 사용자에 의해 제공되거나 또는 준비된 다. 이러한 상황에서, 본 발명은 전사본 및 메타 번역 생성기를 포함하지 않거나 또는 번역된 전사본 및/또는 번역된 메타 정보를 생성하는 단계를 단순히 우회한다. 일부 실시예는 검토를 위해 사용자 에게 번역된 전사본 및/또는 번역된 메타 정보를 제시하고, 사용자에게 번역된 전사본 및/또는 번역된 메타 정보를 수정할 수 있는 기능을 제공한다. 일부 실시예는 수정을 위해 사용자에게 입력을 제 시하는 단계를 포함하며, 사용자는 입력을 수정한 다음 수정된 입력을 전사본 및 메타 번역 생성기로 전송 하여 향상된 출력을 생성할 수 있다. 도 2 및 도 6에 도시된 바와 같이, 번역된 전사본 및 번역된 메타 정보가 획득되면, 본 발명은 오디 오 번역 생성기를 사용하여 입력 오디오를 입력 언어에서 출력 언어로 번역함으로써, 번역 된 오디오를 단계에서 생성할 수 있다. 일부 실시예에서, 오디오 번역 생성기에 대한 입력은 출력 언어 및 번역된 전사본 및/또는 번역된 텍스트 프리프로세서의 출력을 포함한다. 번역된 텍스트 프리프로세서는 텍스트를 음소 분석으로 변환하거나 및/또는 감정 또는 정서 분석을 수행하 도록 구성된다. 이러한 분석은 음성 세그먼트에 해당하는 번역된 데이터 및 연관된 화자 구분 데이터를 포함하 는 번역된 전사본로부터 그러한 데이터를 추출하도록 구성된 임의의 공지된 시스템 및 방법을 사용하여 수 행될 수 있다. 따라서, 번역된 텍스트 프리프로세서의 출력은 컴퓨터 판독 가능한 형식의 이러한 분석 데 이터를 포함하며, 출력 데이터는 오디오 번역 생성기에 제공될 수 있다. 일부 실시예는 오디오 번역 생성기에 대한 입력으로서 입력 메타 정보 및/또는 번역된 메타 정보 를 더 포함한다. 일부 실시예에서, 오디오 번역 생성기의 입력은 출력 언어 및/또는 입력 오디 오 및/또는 오디오 프리프로세서의 출력을 포함한다. 도 6에 더 예시된 바와 같이, 오디오 번역 생성기에 대한 입력은 입력 언어, 출력 언어, 입력 미디어, 비디오 프리프로세서 및 오디오 프리프로세서의 출력, 입력 전사본, 텍스트 프리 프로세서의 출력, 입력 메타 정보, 번역된 전사본, 번역된 텍스트 프리프로세서의 출력, 및/또는 번역된 메타 정보를 포함할 수 있다. 일부 실시예는 번역된 오디오를 생성하기 위해, 번역된 전사본, 번역된 메타 정보, 및 출력 언어 만을 오디오 번역 생성기로 전송한다. 일부 실시예에서, 출력 언어는 번역된 전사본 내에 포함되거나/번역된 전사본으로부터 판단될 수 있다. 일부 실시예는 번역된 오디오를 생성하기 위해, 적어도 번역된 전사본, 번역된 메타 정보, 및 출력 언어를 오디오 번역 생성기로 전송한다. 앞서 언급한 바와 같이, 일부 실시예는 또한 비디오 프리프로세서 및/또는 오디오 프리프로세서로부 터 출력을 전송하는 단계를 포함한다. 비디오 및/또는 오디오 정보를 추가하면 음성 특성, 감정, 화자 신원 등 을 통합하여 번역 결과가 향상된다. 일부 실시예는 번역된 오디오를 생성하기 위해 입력 오디오(프리프로세싱되거나 및/또는 원시) 및 출 력 언어만 오디오 번역 생성기로 전송한다. 일부 실시예는 번역된 오디오를 생성하기 위해 적 어도 입력 오디오(프리프로세싱되거나 및/또는 원시) 및 출력 언어를 오디오 번역 생성기로 전 송한다. 입력 오디오는 입력 시 청크되어 오디오를 관리 가능한 청크 (예를 들면, 15초 미만 또는 30초 미만)로 감소시키거나 및/또는 자동 정렬을 통해 최종 결과를 향상시킨다. 번역된 전사본, 번역된 메타 정보, 및 출력 언어가 오디오 번역 생성기로 전송되는 최초의 또는 유일한 입력인 경우, 오디오 번역 생성기는 텍스트-스피치 변환(text-to-speech(TTS)) 생성기를 포함 할 수 있으며, 이는 일반적인 제3자 클라우드 TTS 시스템, 맞춤형 클라우드 TTS 시스템, 제3자 온-디바이스 TTS 시스템, 또는 맞춤형 온-디바이스 TTS 시스템을 포함하지만 이에 제한되지 않는다. 오디오 번역 생성기는 프리프로세싱 오디오로부터 얻은 성별, 연령, 감정 특성 등과 같은 음성 특성을 식별 및 통합하도록 더 구성될 수 있다. 그 결과 번역된 오디오는 일반적으로 TTS에서 제공되는 것보다 훨씬 더 많은 정보를 포함한다. 예를 들면, 번역된 오디오는 발화 단어, 감정, 속도, 일시 정지, 어조, 운율, 세기/어조, 강세, 음성 정체 성 등을 일치시킨다. 그 결과, 번역된 오디오는 원래의 사람의 음성으로 전달되지 않지만, 생성기는 사람의 음 성과 거의 일치시킨다. 오디오가 입력 미디어에서와 동일한 화자에 해당하는 학습 데이터를 사용하여 생 성기가 학습되는 경우, 생성기는 약 99%의 음성 일치도를 갖는 합성 번역된 오디오를 생성할 수 있다. 동일한 화자의 오디오에 대해 학습되지 않은 경우, 생성기는 약 80% 이상의 음성 일치도를 갖는 합성 번역된 오디오를 생성할 수 있다. 일부 실시예는 검토를 위해 사용자에게 번역된 오디오 출력을 제시하고, 사용자에게 번역된 오디오 출력 을 수정할 수 있는 기능을 제공한다. 사용자는 입력을 수정한 다음 수정된 입력을 오디오 번역 생성기 로 전송하여 향상된 출력을 생성할 수 있다. 일부 실시예는 최종 번역된 오디오 출력을 향상시키도록 구성된 포스트 프로세서를 더 포함한다. 포스트 프로세서는 번역된 오디오를 원본 오디오 배경 사운드, 사운드 효과 등과 함께 단일 오디오 스트림으로 다 시 스티칭하도록 구성된다. 일부 실시예에서, 포스트 프로세서는 (예를 들면, 전문 오디오 믹싱으로부터의) 원 본 오디오 사운드 파라미터와 원본 오디오 입력의 비감독된 암묵적 특성을 자동으로 일치시킨다. 일부 실 시예에서, 포스트 프로세서는 화자 구분과 같은 오디오 프리프로세싱 정보를 직접 재통합하도록 구성된다. 번역 프로세스의 이 시점에서, 번역된 오디오는 최종 사용자에게 제시되거나 또는 제공될 수 있다. 그러 나, 일부 실시예는 번역된 오디오와 일치하도록 화자의 얼굴 움직임을 동기화하는 단계를 더 포함한다. 도 2 및 도 7의 단계 216은 이러한 추가 프로세스에 해당하는 세부 사항을 제공한다. 일부 실시예에서, 입력 비디오, 비디오 프리프로세서의 출력, 및 번역된 오디오는 동기화된 비디오를 출력하는 비 디오 동기화 생성기로 제공된다. 일부 실시예는 번역된 오디오, 비디오 프리프로세서의 출력, 및 입력 비디오만 비디오 동기화 생성기로 전송한다. 일부 실시예는 적어도 번역된 오디오, 비디오 프리프로세서의 출력, 및 입력 비디오를 비디오 동기화 생성기로 전송한다. 도 7에 예시된 바와 같이, 일부 실시예는 입력 언어, 비디오 프리프로세 서의 출력, 입력 오디오, 오디오 프리프로세서의 출력, 입력 전사본, 텍스트 프리프로세서 의 출력, 입력 메타 정보, 번역된 전사본, 번역된 텍스트 프리프로세서의 출력, 번역된 메 타 정보, 및/또는 번역된 오디오 프리프로세서의 출력을 비디오 동기화 생성기에 더 제공한다. 제공된 정보를 사용하여, 비디오 동기화 생성기는 번역된 오디오가 입력 비디오에 더빙되고 화 자의 얼굴 움직임이 번역된 오디오와 일치하는 동기화된 비디오를 생성한다. 보다 구체적으로, 비디 오 동기화 생성기는 번역된 비디오를 생성하고, 화자의 얼굴 움직임이 번역된 오디오와 일치하도록 하기 위해 경계 박스 및/또는 얼굴 랜드마크, 입/입술 랜드마크 등에 기초하여, 번역된 비디오를 원본 비디오로 다시 재결합한다. 일부 실시예는 검토를 위해 사용자에게 동기화된 비디오를 제시하고, 사용자에게 동기화된 비디오를 수정할 수 있는 기능을 제공한다. 일부 실시예에서 사용자는 입력을 수정한 다음 수정된 입력을 전송하여 개선 된 출력을 생성할 수 있다. 일부 실시예에서, 비디오 동기화 생성기는 대상체의 입술을 재현하기 위한 광학 흐름 네트워크/광학 흐름 손실을 포함한다. 일부 실시예에서, 비디오는 별도의 얼굴, 장면 컷 등을 고려하기 위해 입력 시 청크(chunk) 될 수 있다. 도 8에 예시된 바와 같이, 일부 실시예는 다양한 데이터를 동기화된 비디오에 재통합하도록 구성된 포스트 프로세싱 단계를 더 포함한다. 일부 실시예는 이러한 단계를 수행하도록 구성된 비디오 포스트 프로세서 를 포함한다. 포스트프로세싱 후, 출력 비디오가 사용자에게 제공될 수 있다. 일부 실시예에서, 비디오 포스트 프로세서는 입력 비디오, 번역 전사본, 번역된 오디오, 및 동기화된 비디오를 입력으로서 수신한다. 비디오 포스트 프로세서는 이러한 입력을 사용하여 원 본 비디오 광학 파라미터(예를 들면, 전문 비디오 믹싱, 비디오 컬러링 등)와 원본 비디오 입력의 비감독된 암 묵적 특성을 자동으로 일치시킨다. 본 발명의 일부 실시예는 위에서 설명한 번역 프로세스의 출력 및 효율성을 개선하기 위해 GAN/ML/AI (통칭하여 \"AI\"라고 함)를 사용한다. 다양한 AI는 감독, 비감독, 및/또는 반감독 방식으로 학습될 수 있다. 그 결과 학 습된 AI 프로세서 및 생성기는 보다 효율적인 방식으로 상당히 향상된 번역을 생성하는 데 사용될 수 있다. 일반적으로, AI를 사용하는 실시예는 AI 시스템의 의도된 기능에 기초하여 두 가지 유형의 AI를 갖는다. 이러 한 AI 시스템은 일반적으로 프리프로세싱 AI 및 생성 AI로 구분할 수 있다. AI 프로세서/프리프로세서는 정보 를 변환, 추출, 식별 또는 컴파일하는 등의 작업을 보다 효과적이고 효율적으로 수행하도록 설계된 시스템이다. 반면, AI 생성기는 조작되거나 또는 변형된 미디어와 같은 합성 정보를 생성하도록 구성된 시스템이다. 비디오 프리프로세서, 오디오 프리프로세서, 화자 구분 프로세서, 텍스트 프리프로세서, 번역 텍스트 프리프로세서, 번역된 오디오 프리프로세서, 및 메타 정보 프로세서 등의 시스템은 프리프로세싱 AI로 대체될 수 있다. 마찬가지로, 입력 전사본 생성기, 전사본 및 메타 번역 생성기,번역된 텍스트 프리프로세서, 오디오 번역 생성기, 번역된 오디오 프리프로세서, 및 비디오 동 기화 생성기 등의 생성기는 생성 AI로 대체될 수 있다. 다양한 프리프로세싱 AI 및 생성 AI는 각각 하기 에 개별적으로 상세히 설명되어 있다. 비디오 프리프로세서 본 발명의 일부 실시예에서, 비디오 프리프로세서는 프리프로세싱 AI이다. 비디오 프리프로세서는 식별 및 추적 시스템 및 방법을 사용하여 비디오 내의 대상체를 식별 및 추적하기 위한 프로세스 (예를 들면,"}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "이전 섹션에서 식별된 것들)를 포함할 수 있다. 이러한 시스템 및 방법은 해당 기술분야에 공지된 임의의 AI 프로세싱 시스템일 수 있다. 예를 들면, 비디오 프리프로세서는 얼굴 랜드마크 분석, 얼굴 추적 알고리즘, 얼굴 자르기 및 정렬 알고리즘, 장면 식별, 및 복원 및 초고해상도를 포함할 수 있다. 일부 실시예에서, 비디오 프리프로세서는 입술 움직임을 식별 및 추적하도록 구성된 AI를 포함한다. 입술 움직임을 추적함으로써, AI는 비디오의 특정 음성 세그먼트에서 어떤 화자가 발화하고 있는지 판단할 수 있다. 입술 움직임을 추적하는 데 사용되는 시스템 및 방법은 얼굴 랜드마크 분석, 얼굴 추적 알고리즘, 얼굴 자르기 및 정렬 알고리즘, 분류, 세그먼트화, 및 입술-텍스트 알고리즘을 포함하지만 이에 제한되지 않는 해당 기술분 야에 공지된 임의의 AI 프로세싱 시스템일 수 있다. 일부 실시예에서, 비디오 프리프로세서는 입력 비디오 및/또는 입력 비디오의 컴퓨터 판독 가능 한 표현을 수신하도록 구성된다. 마찬가지로, 비디오 프리프로세서는 컴퓨터 판독 가능한 데이터를 출력 한다. 일부 실시예에서, 컴퓨터 판독 가능한 데이터는 이진 벡터 및/또는 문자열의 벡터로 제공된다. 이진 벡"}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "터는 1-핫 벡터 및 다중 클래스 벡터를 포함하지만 이에 제한되지 않는 해당 기술분야에 공지된 임의의 벡터일"}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "수 있다. 마찬가지로, 문자열 벡터는 해당 기술분야에 공지된 임의의 것일 수 있다. 일부 실시예는 IPA 기반 의 문자열을 사용한다. IPA 문자열을 사용하면 서로 다른 언어의 동일한 단어 사이의 음성학적인 차이와 연관 된 오류를 감소시킬 수 있다. 일부 실시예는 검토 및 잠재적 수정을 위해 비디오 프리프로세서 AI의 출력을 사용자에게 제시한다. 비디오 프 리프로세서 AI가 학습된 경우에도, 사용자에 의해 출력이 향상될 수 있는 경우가 있을 수 있다. 따라서, 일부 실시예는 수정을 위해 사용자에게 출력을 제시하는 단계를 포함한다. 오디오 프리프로세서 본 발명의 일부 실시예에서, 오디오 프리프로세서는 프리프로세싱 AI이다. AI 오디오 프리프로세서 는 각 화자에 대해 오디오 콘텐츠를 분할하고, 배경 잡음을 제거하거나 또는 정리하고, 음성 품질 데이터를 향 상시키는 프로세스를 포함할 수 있다. 이러한 프로세스는 본원에 열거된 프로세스를 수행할 수 있는 공지된 임 의의 AI 프리프로세서를 사용하여 수행될 수 있다. 예를 들면, AI 오디오 프리프로세서는 음성 소스 분리, 잡음 감소, 오디오 복원 및 초고해상도를 포함할 수 있다. AI 비디오 프리프로세서와 마찬가지로, AI 오디오 프리프로세서는 입력 오디오 및/또는 입력 오 디오의 컴퓨터 판독 가능한 표현을 수신하도록 구성된다. 마찬가지로, AI 오디오 프리프로세서는 본 원에 설명된 것과 같은 컴퓨터 판독 가능한 데이터를 출력한다. 일부 실시예는 검토 및 잠재적 수정을 위해 AI 오디오 프리프로세서의 출력을 사용자에게 제시한다. AI 오디오 프리프로세서가 학습된 경우에도, 사용자에 의해 출력이 향상될 수 있는 경우가 있을 수 있다. 따 라서, 일부 실시예는 수정을 위해 사용자에게 출력을 제시하는 단계를 포함한다. 화자 구분 프로세서 본 발명의 일부 실시예에서, 화자 구분(SD) 프로세서는 프리프로세싱 AI이다. AI SD 프로세서는 식 별 가능한 화자에 따라 입력 오디오를 균질한 음성 세그먼트로 분할하는 프로세스를 포함할 수 있다. AI"}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "SD 프로세서는 화자 구분을 실행하기 위한 해당 기술분야의 통상의 기술자에게 공지된 임의의 시스템 및 방법일 수 있다. 궁극적으로, AI SD 프로세서는 입력 미디어에서 하나 이상의 화자를 식별하고 각 문자열 또는 음성 세그먼트를 적절한 화자와 연관시키기 위해 일련의 단계를 수행한다. 일부 실시예에서, AI SD 프로세서의 출력은 입력 오디오에 해당하는 일련의 음성 세그먼트를 포함하며, 각 세그먼트는 화 자 식별자 또는 화자의 신원에 대한 참조를 포함한다. 일부 실시예에서, AI SD 프로세서는 오디오의 각 단어에 대한 시간 코드, 말하는 사람, 화자가 말하는 내용, 각 화자가 말하는 시기, 화자 신원, 말한 단어, 및 화자의 연관된 특성을 캡처하도록 더 구성된다. AI SD 프로세서는 기침, 재채기, 스피치의 일시 정지 및기타 비언어적 오디오 세그먼트 또는 화자에 의해 생성된 비언어적 잡음을 더 식별할 수 있다. 다른 SD 정보와 마찬가지로, 이러한 데이터는 전체 시스템을 통해 공급된다. AI SD 프로세서의 일부 실시예는 입력 비디오에 기초하여 특정 음성 세그먼트를 화자와 연관시키도록 더 구성된다. 이는 각 화자의 얼굴, 식별 가능한 특성 및/또는 얼굴 움직임을 추적함으로써 이루어진다. 예를 들면, 일부 실시예는 얼굴 궤적 분석을 사용하여 특정 음성 세그먼트에 대한 화자의 특성을 추적, 식별, 및 캡 처한다. 이러한 실시예에서, AI SD 프로세서의 출력은 일련의 음성 세그먼트와 연관된 얼굴 궤적 데이터 를 더 포함한다. 화자 구분의 출력은 반드시 비디오 자체는 아니고, 그 안에 포함되거나 또는 그것과 연관된 컴퓨터 판독 가능한 데이터이다. 얼굴 궤적 분석과 연관된 데이터는 얼굴이 묘사되는 시작 및 종료 시간, 다른 사람과 비교한 개별 주체의 신원, 성별, 화면에 표시된 시간, 오디오 기반의 말하기 시간, 및 말하는 사람을 식별하기 위한 립싱크 분석을 포함할 수 있다. 이 모든 정보는 누가 말하고 있는지 및 식별 가능한 특성이 음성 특성에 어떻게 영향을 미칠 수 있는 지 판단하기 위해 사용될 수 있다."}
{"patent_id": "10-2023-7041967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "AI SD 프로세서는 화자를 특정 음성 세그먼트와 식별하고 연관시키도록 구성된, 해당 기술분야에 공지된 임의의 AI 화자 구분 시스템일 수 있다. 예를 들면, AI SD 프리프로세서는 AWS, 구글, IBM 등에 의해 제 공되는 제3자 SD 툴일 수 있고, 또는 CNN, RNN, LSTM, GNN, 트랜스포머, GAN, 또는 기타 ML 아키텍처에 기초한 음성 활동 검출, 음성 세그먼트화, 화자 임베딩, 세그먼트 클러스터링, 선호도 매트릭스, MAP-인코딩을 활용하 는 맞춤형 구현일 수 있다. AI SD 프리프로세서는 입력 미디어 및 입력 언어를 본원에 설명된 것과 같은 컴퓨터 판독 가능 한 형식으로 수신하도록 구성된다. 일부 실시예에서, 입력 오디오는 입력 비디오 없이 AI SD 프로세 서에 제공된다. 일부 실시예에서, 입력 비디오 및/또는 입력 오디오는 AI SD 프로세서에 제공된다. 일부 실시예는 프리프로세싱된 오디오 출력과 함께 원본 입력 오디오를 오디오 프리프로세서 로부터 AI SD 프로세서로 제공한다. 앞서 설명한 AI 프리프로세서와 마찬가지로, AI SD 프리프로세서는 본원에 설명된 것과 같은 컴퓨터 판독 가능한 데이터를 출력한다. 보다 구체적으로, AI SD 프리프로세서는 각 음성 세그먼트에 화자 신원이 포 함된 데이터를 출력한다. 일부 실시예는 검토 및 잠재적 수정을 위해 AI SD 프리프로세서의 출력을 사용자에게 제시한다. AI SD 프 리프로세서가 학습된 경우에도, 사용자에 의해 출력이 향상될 수 있는 경우가 있을 수 있다. 따라서, 일 부 실시예는 수정을 위해 사용자에게 출력을 제시하는 단계를 포함한다. 텍스트 프리프로세서 본 발명의 일부 실시예에서, 텍스트 프리프로세서는 프리프로세싱 AI이다. AI 텍스트 프리프로세서 는 입력 전사본과 같은 텍스트 내의 음소를 검출 및 분석하는 프로세스를 포함할 수 있다. AI 텍스트 프 리프로세서는 텍스트, 품사, 고유 명사, 및 관용구 내에서 감정/정서를 검출 및 분석하는 프로세스를 더 포함할 수 있다. 이러한 프로세스는 본원에 열거된 프로세스를 수행할 수 있는 공지된 임의의 AI 프리프로세서 를 사용하여 수행될 수 있다. 예를 들면, AI 텍스트 프리프로세서는 사전 검색 또는 트랜스포머 모델 또 는 GAN 모델을 통해 생성된 IPA 또는 이와 유사한 시스템에 기반한 음성학 분석, 정서 분석, 품사 분석, 고유 명사 분석, 및 관용구 검출 알고리즘을 포함할 수 있다. AI 텍스트 프리프로세서는 입력 전사본 및/또는 입력 전사본의 컴퓨터 판독 가능한 표현 또는 이와 연관된 데이터를 수신하도록 구성된다. 일부 실시예에서, 이러한 입력은 SD 프로세서 및 입력 전사 본 생성기로 인해 각 음성 세그먼트에 해당하는 SD 데이터를 포함한다. AI 텍스트 프리프로세서는 음소 및/또는 감정 데이터를 본원에 설명된 유형들과 같은 컴퓨터 판독 가능한 데이터로서 출력한다. 또한, 이 러한 데이터는 각 음성 세그먼트에 해당하는 SD 데이터와 연관되어 출력된다. 일부 실시예는 검토 및 잠재적 수정을 위해 AI 텍스트 프리프로세서의 출력을 사용자에게 제시한다. AI 텍스트 프리프로세서가 학습된 경우에도, 사용자에 의해 출력이 향상될 수 있는 경우가 있을 수 있다. 따 라서, 일부 실시예는 수정을 위해 사용자에게 출력을 제시하는 단계를 포함한다. 메타 정보 프로세서 본 발명의 일부 실시예에서, 메타 정보 프로세서는 AI 생성기이다. AI 메타 정보 프로세서는 각 음 성 세그먼트와 연관된 다양한 메타 정보를 식별하고 생성하도록 구성된다. 메타 정보의 비제한적인 예로는 감 정, 강세, 속도/운율/리듬, 음소 분석, 연령, 성별, 인종이 포함된다. 일부 실시예에서, AI 메타 정보 프로세 서는 각 음성 세그먼트의 단어에 대해 적어도 감정 데이터를 식별 및 생성한다. AI 메타 정보 프로세서는 상술한 메타 정보 중 하나 이상을 식별 및 생성하도록 구성된 임의의 AI 프로세 서일 수 있다. AI 프로세서의 비제한적인 예로는, 얼굴 감정 검출, 얼굴 연령 검출, 얼굴 성별 검출, 얼굴 유 사도 벡터 생성, 입술-운율 분석, 음성 감정 검출, 음성 연령 검출, 음성 성별 검출, 음성 운율 분석, 음성 강 도 분석, 음성 피치 검출, 음성 활동 검출, 텍스트 감정 검출, 및 텍스트 의미 검출을 수행하도록 구성된 CNN, RNN, LSTM이 포함된다. 일부 실시예에서, AI 메타 정보 프로세서는 입력 오디오 및 입력 전사본을 수신한다. 일부 실 시예는 입력 언어, 입력 비디오, 출력 프리프로세서, 프리프로세서의 출력, 및/또는 텍스 트 프리프로세서의 출력을 AI 메타 정보 프로세서에 대한 입력으로서 더 포함한다. 일부 실시예에서, AI 메타 정보 프로세서는 메타 정보 및 SD 데이터가 각 음성 세그먼트와 연관되는 합성 입력 메타 정보를 생성한다. 따라서, 입력 메타 정보는 전사본 및 메타 번역 생성기에 의해 사 용 가능한 형식의 SD 데이터에 대한 속도 및 타임코드를 포함한다. 일부 실시예에서, 입력 메타 정보는 음소로 변환된 SD 데이터를 포함하며, 이에 따라 시스템은 음소 유사성에 기초하여 입력과 일치하도록 번역된 출력을 조정할 수 있다. 일부 실시예는 SD 데이터와 연관된 오디오/비디오 분석의 감정 데이터를 더 포함한다. AI 메타 정보 프로세서의 출력 메타 정보는 다양한 기타 생성기를 직접적으로 또는 간접적으로 통과한다. 그 결과, 생성된 번역된 텍스트, 오디오, 및/또는 비디오는 입력된 오디오, 비디오, 및/또는 텍스트와 유사하거 나 또는 일치하는 속도로 생성된다. AI 메타 정보 프로세서의 일부 실시예는 각 음성 세그먼트에 대한 메타 정보를 식별 및 캡처하고 각 음성 세그먼트와 연관된 메타 정보를 생성하도록 학습된다. AI 메타 정보 프로세서는 복수의 네트워크 계층으 로 구성될 수 있으며, 각 계층은 특정 유형의 메타 정보에 해당한다. AI 메타 정보 프로세서가 감정 데이터를 인식 및 생성하도록 학습시키면, 다양한 정서가 캡처되어 번역에 삽입될 수 있으므로 전반적인 시스템이 더욱 향상된다. 직역은 다양한 감정을 인식하거나 또는 전달하지 못하 므로, 오디오의 해석에 큰 영향을 미칠 수 있다. 또한, 감정 데이터를 캡처하지 않으면, 오디오 번역이 비디오 에 나타난 임의의 가시적인 감정과 동기화되지 않는다. 반면, 학습된 AI 메타 정보 프로세서는 감정 데이 터를 인식 및 생성할 수 있으며, 이는 후속 프리프로세서 및 생성기를 통해 전달된다. AI 메타 정보 프로세서는 학습되기 때문에, 스피치 중 고유성에 해당하는 메타 정보를 생성할 수 있다. 예를 들면, AI 메타 정보 프로세서는 감정이 스피치에 어떤 영향을 미치는지 파악하기 위해 특성에 대해 학습될 수 있다. 학습 후, AI 메타 정보 프로세서는 문장에 감정이 포함되어 있는지를 파악하여 해당 메 타 데이터를 생성할 수 있다. 그러면 후속 AI 생성기가 식별된 감정을 포함하는 합성 오디오를 생성할 수 있다. 적절히 학습되면, 메타 정보 프로세서는 다양한 수준의 분노 또는 상이한 악센트, 감정, 속도 등이 포함 된 오디오 등 다중-라벨링된(multi-labeled) 출력을 생성할 수 있다. 일부 실시예에서, AI 메타 정보 프로세서는 결과를 개선하기 위해 오디오 및/또는 프리프로세싱된 비디오 정보(예를 들면, 잘린 얼굴, 입 움직임 검출 등)에 대해 학습된다. 오디오 정보에는 억양 및 의미가 담겨 있다. 따라서, 학습된 AI 메타 정보 프로세서는 (비감독 또는 반감독 방식으로) 문자 그대로의 의미를 넘 어서는 전사본 결과를 향상시킬 것이다. 오디오를 공급하면 오디오 내에 포함된 풍자, 유머, 관용구, 및 기타 정보에 변함이 없는 전사본을 만들 수 있다. 번역된 텍스트 프리프로세서 본 발명의 일부 실시예에서, 번역된 텍스트 프리프로세서는 프리프로세싱 AI이다. AI 번역된 텍스트 프리 프로세서는 번역된 전사본과 같은 텍스트 내의 음소를 검출 및 분석하는 프로세스를 포함할 수 있다. AI 번역된 텍스트 프리프로세서는 텍스트, 품사, 고유 명사, 및 관용구 내의 감정/정서를 검출 및 분석하 는 프로세스를 더 포함할 수 있다. 이러한 프로세스는 본원에 열거된 프로세스를 수행할 수 있는 공지된 임의 의 AI 프리프로세서를 사용하여 수행될 수 있다. 예를 들면, AI 번역된 텍스트 프리프로세서는 사전 검색 (dictionary lookup) 또는 트랜스포머 모델 또는 GAN 모델을 통해 생성된 IPA 또는 이와 유사한 시스템에 기반 한 음성학 분석, 정서 분석, 품사 분석, 고유 명사 분석, 및 관용구 검출 알고리즘을 포함할 수 있다.AI 번역된 텍스트 프리프로세서는 번역된 전사본 및/또는 입력 번역된 전사본의 컴퓨터 판독 가 능한 표현 또는 이와 연관된 데이터를 수신하도록 구성된다. 일부 실시예에서, 이러한 입력은 SD 프로세서 및 입력 전사본 생성기로 인해 각 음성 세그먼트에 해당하는 SD 데이터를 포함한다. 일부 실시예에 서, AI 번역된 텍스트 프리프로세서에 대한 입력은 입력 및/또는 번역된 메타 정보를 더 포함한다. AI 번역된 텍스트 프리프로세서는 음소 및/또는 감정 데이터를 본원에 설명된 유형들과 같은 컴퓨터 판독 가능한 데이터로서 출력한다. 또한, 이러한 데이터는 각 음성 세그먼트에 해당하는 SD 데이터 및/또는 메타 정 보와 연관되어 출력된다. 일부 실시예는 검토 및 잠재적 수정을 위해 AI 번역된 텍스트 프리프로세서의 출력을 사용자에게 제시한다. AI 번역된 텍스트 프리프로세서가 학습된 경우에도, 사용자에 의해 출력이 향상될 수 있는 경 우가 있을 수 있다. 따라서, 일부 실시예는 수정을 위해 사용자에게 출력을 제시하는 단계를 포함한다. 번역된 오디오 프리프로세서 본 발명의 일부 실시예에서, 번역된 오디오 프리프로세서는 프리프로세싱 AI이다. AI 번역된 오디오 프리 프로세서는, 각 화자에 대해 분할된 오디오 콘텐츠를 재조합하고, 배경 잡음을 제거하거나 또는 정리하고, 음성 품질 데이터를 향상시키는 프로세스를 포함할 수 있다. 예를 들면, AI 번역된 오디오 프리프로세서 는 음성 소스 식별, 잡음 감소, 오디오 복원 및 초고해상도를 포함할 수 있다. AI 번역된 오디오 프리프로세서는 번역된 오디오 및/또는 번역된 오디오의 컴퓨터 판독 가능한 표현 또는 이와 연관된 데이터를 수신하도록 구성된다. 일부 실시예에서, 이러한 출력은 각 음성 세그먼트에 해당하는 SD 데이터 및 메타 정보를 포함한다. 마찬가지로, 출력은 또한 각 음성 세그먼트에 해당하는 SD 데이 터 및 메타 정보를 포함할 수 있다. 또한, 입력 및 출력 데이터는 본원에 설명된 유형들과 같은, 컴퓨터 판독 가능한 모든 형식일 수 있다. 일부 실시예는 검토 및 잠재적 수정을 위해 AI 번역된 오디오 프리프로세서의 출력을 사용자에게 제시한다. AI 번역된 오디오 프리프로세서가 학습된 경우에도, 사용자에 의해 출력이 향상될 수 있는 경 우가 있을 수 있다. 따라서, 일부 실시예는 수정을 위해 사용자에게 출력을 제시하는 단계를 포함한다. 상술한 다양한 프리프로세싱 AI의 출력에는, 궤적 분석(예를 들면, 얼굴, 잘림, 정렬됨, 별도의 신원, 타임코드, 위치), 신원 특성(예를 들면, 연령, 인종, 성별 등), 음성 분석(예를 들면, 음성, 잘린 타임코드, 정 규화된 볼륨, 잡음 감소, 별도의 신원), 음성 특성(예를 들면, 감정, 어조, 속도 등), 화자 구분(예를 들면, 정 렬된 텍스트 - \"누가 언제 무엇을 말하는지\" 및 음소 분석), 텍스트 특성(예를 들면, 화자 구분 결과와 일치하 는 감정 분석)이 포함된다. 이러한 출력은 아래에 설명된 AI 생성기에 직접 제공된다. 그런 다음 AI 생성기가 새로운(즉, 번역된) 텍스트, 오디오, 및 비디오를 생성한다. 음성은 원래의 화자처럼 들리고 비디오는 화자의 입술이 오디오와 일치하도록 조작된다. 입력 전사본 생성기 본 발명의 일부 실시예에서, 입력 전사본 생성기는 AI 생성기이다. AI 입력 전사본 생성기는 SD 데 이터 출력을 수신하여 입력 전사본을 합성적으로 생성하도록 구성된다. 일부 실시예에서, 프로세싱되지 않은 원본 입력 비디오 및/또는 입력 오디오는 또한 입력 전사본 생성기에 제공된다. 일부 실시예에 서, 비디오 프리프로세서 및/또는 오디오 프리프로세서의 출력은 또한 입력 전사본 생성기에 제 공된다. 일부 실시예는 입력 전사본 생성기에 입력 언어를 더 제공한다. 앞서 설명한 바와 같이, SD 데이터의 일부 실시예는 화자 식별 정보를 갖는 세그먼트화된 오디오(\"음성 세그먼 트\")를 포함한다. 따라서, AI 입력 전사본 생성기의 실시예는 오디오 음성 세그먼트를 입력 전사본으로 변환한다. 보다 구체적으로, AI 입력 전사본 생성기는 단지 발화된 단어부터 입 움직임, 음소, 타임스탬 프, 및 기타 그러한 설명에 대한 매우 상세한 데이터까지 모든 것을 포함하여 합성적으로 전사본을 생성한다. 종종, 입력 전사본은 발화되는 언어, 이름/적절한 명사의 식별, 정서 분석, 단어 및/또는 음절의 타임스탬 프/타임 인덱스, 및/또는 오디오에서 발화하는 각 개별 대상체에 대한 타임스탬프를 갖는 음소를 포함할 것이다. 일부 실시예에서, AI 입력 전사본 생성기는 입력을 수신하고 본원에 설명된 것과 같은 컴퓨터 판 독 가능한 형식으로 출력을 생성하도록 구성된다. AI 입력 전사본 생성기는 시스템의 나머지 구성 요소에 의해 사용 가능한 형식을 내보내기 위해 SD의 결과 를 해석 및 통합하는 비AI 기반 알고리즘을 포함할 수 있다. 일부 실시예에서, AI 입력 전사본 생성기는학습된 AI 생성기이다. 일부 실시예에서, AI 입력 전사본 생성기는 결과를 개선하기 위해 오디오 및/또는 프리프로세싱된 비디오 정보(예를 들면, 잘린 얼굴, 입 움직임 검출 등)에 대해 학습된다. 오디오 정보에는 억 양 및 의미가 담겨 있다. 따라서, 학습된 AI 입력 전사본 생성기는 (비감독 또는 반감독 방식으로) 문자 그대로의 의미를 넘어서는 전사본 결과를 향상시킬 것이다. 오디오를 공급하면 오디오 내에 포함된 풍자, 유머, 관용구, 및 기타 정보에 변함이 없는 전사본을 만들 수 있다. 비디오 정보는 기타 감정적인 정보를 포함할 수 있다. 따라서, 유사한 비감독 또는 반감독 방식으로 비디오 정 보에 대해 AI 입력 전사본 생성기를 학습시키면 전사본 번역 결과가 더욱 향상된다. AI 입력 전사본 생성기의 일부 실시예는 오디오 및/또는 비디오 입력에서 서로 다른 언어와 속도의 IPA 문 자를 식별 및 생성하는 것에 대해 더 학습된다. AI 입력 전사본 생성기를 IPA 문자와 속도를 식별 및 생 성하도록 학습시킴으로써, AI 입력 전사본 생성기는 하나의 언어의 입력을 입력 오디오의 속도와 일치하는 IPA의 전사본으로 변환하는 기능을 개발한다. IPA를 사용할 때, 시스템은 다양한 단어에 대한 대체 번역을 생 성하여 번역이 속도 관점에서 동기화될 수 있도록 할 수 있다. 반면, 하나의 언어에서 다른 언어로 직역하면 종종 간격이 일정하지 않고 최종 번역된 오디오가 입력 오디오의 속도와 일치하지 않게 된다. 더 아래쪽으로 이동하면 번역된 오디오가 입술 움직임의 속도와 일치하지 않기 때문에 시스템이 화자의 입술을 동기화할 수 없 게 된다. 전사본 및 메타 번역 생성기 본 발명의 일부 실시예에서, 전사본 및 메타 번역(TMT) 생성기는 AI 생성기이다. AI TMT 생성기는 번역된 전사본 및 번역된 메타 정보를 하나 이상의 입력으로부터 생성하도록 구성된다. AI TMT 생성기는 번역된 전사본 및 번역된 메타 정보를 하나 이상의 입력으로부터 생성하도록 구성된 임의의 AI 생성기일 수 있 다. 비제한적인 예로는, 속도, 음소, 메타 및 기타 정보를 통합하도록 수정된 BERT/GPT3와 같은 트랜스포머 기 반 모델, GAN 기반 모델, 및 속도, 음소, 메타 및 기타 정보를 통합하는 다른 AI 기반 번역 모델이 포함된다. 일부 실시예에서, 입력은 입력 전사본(원시 또는 텍스트 프리프로세서를 사용하여 프리프로세싱됨), 입력 언어, 출력 언어, 및 입력 메타 정보만을 포함한다. 일부 실시예에서, 이러한 입력은 IPA 음성 문자에 대한 속도 정보를 포함한다. AI TMT 생성기를 사용하면, IPA 음성학과 일치할 뿐만 아니라 IPA 음성학과 연관된 속도 및 시간 코드와 일치하는 번역된 단어를 합성 생성할 수 있다. 엄격한 번역은 속도 오류가 발생할 수 있지만, 합성 생성 번역은 이러한 오류를 방지할 수 있다. 일부 실시예에서, 입력은 또한 입력 비디오, 오디오 입력, 비디오 프리프로세서의 출력 및/또는 오디오 프리프로세서의 출력을 포함한다. 일부 실시예는 입력 전사본(원시 또는 프리프로세싱됨) 과 입력 언어 및 출력 언어만을 AI TMT 생성기로 전송하여 번역된 전사본을 생성한다. 일부 실시예는 적어도 입력 전사본(원시 또는 프리프로세싱됨)과 입력 언어 및 출력 언어를 AI TMT 생성기로 전송하여 번역된 전사본을 생성한다. 입력 메타 정보를 포함하면, AI TMT 생성기는 입력 메타 정보를 통해 식별된 다양한 스피치 특 성을 갖는 번역된 전사본 및 번역된 메타 정보를 생성할 수 있다. 이러한 특성에는 풍자, 유머, 음 소, 음소를 일치시키기 위한 속도 등이 포함되지만 이에 제한되지 않는다. 입력 오디오 및/또는 오디오 프리프로세서의 출력을 AI TMT 생성기에 공급하면, 오디오 내에 포함된 풍자, 유머, 관용어, 및 기타 정보에 변함이 없는 전사본을 또한 만들 수 있다. 입력 비디오 및/또는 비디오 프리프로세서의 비디 오 정보는 또한 AI TMT 생성기에 대한 입력으로서 제공될 수 있으며, 이는 다른 감정 정보를 포함할 수 있 고 번역된 전사본 및 번역된 메타 정보를 더욱 향상시킬 수 있다. 일부 입력 미디어에서는, 입력 오디오에서 두 개 이상의 언어(예를 들면, 영어 및 스페인어)가 발화 될 수 있다. 이 정보는 종종 입력 전사본 내에 있을 것이다. 둘 이상의 입력 언어를 번역하는 경우, AI TMT 생성기에는 각 입력 언어 (예를 들면, 영어를 독일어로, 스페인어를 독일어로, 또는 영어를 독일어로, 및 스페인어를 프랑스어로)에 대한 특정 출력 언어가 제공된다. AI TMT 생성기의 일부 실시예는 상술한 하나 이상의 입력을 갖는 데이터에 대해 학습된다. 일부 실시예에 서, AI TMT 생성기는 결과를 개선하기 위해 오디오 및/또는 프리프로세싱된 비디오 정보 (예를 들면, 잘린 얼굴, 입 움직임 검출 등)에 대해 학습된다. 오디오 정보에는 억양 및 의미가 담겨 있다. 따라서, 학습된 AI TMT 생성기는 (비감독 또는 반감독 방식으로) 문자 그대로의 의미를 넘어서는 전사본 결과를 향상시킬 것 이다. 오디오를 공급하면 오디오 내에 포함된 풍자, 유머, 관용구, 및 기타 정보에 변함이 없는 전사본을 만들수 있다. 비디오 정보는 유사한 비감독 또는 반감독 방식으로 학습하는 동안, AI TMT 생성기에 공급되는 기타 감정 정보를 포함할 수 있으며, 이는 전사본 번역 결과를 더욱 향상시킨다. AI TMT 생성기는 또한 오디오 프리 프로세서로 제공되는 비디오 프리프로세서 출력을 사용하여 학습될 수 있다. 일부 실시예에서, AI TMT 생성기는 사용자에 의해 직접 업데이트될 수 있다. 예를 들면, 사용자는 문자 그대로 번역을 수정함으로써 텍스트 번역 자체를 편집할 수 있다. 그런 다음 이러한 번역은 AI를 사용하여 음 소로 변환된다. 오디오 번역 생성기 본 발명의 일부 실시예에서, 오디오 번역 생성기는 AI 생성기이다. AI 오디오 번역 생성기는 번역된 오디오를 하나 이상의 입력으로부터 생성하도록 구성된다. AI 오디오 번역 생성기는 번역된 오디오를 본 원에 설명된 하나 이상의 입력으로부터 생성하도록 구성된 임의의 AI 생성기일 수 있다. 비제한적인 예로서, 클라우드 TTS 시스템, 맞춤형 클라우드 TTS 시스템, 제3자 온-디바이스 TTS 시스템, 맞춤형 온-디바이스 TTS 시 스템, TacoTron2 기반 방법, MelGAN, Seq2Seq 또는 Wav2Wav 기반 방법, 음성-클로닝 기반 방법, FastSpeech2와 같은 비자동 회귀(non-autoregressive) 기반 방법 등이 포함된다. 일부 실시예에서, AI 오디오 번역 생성기에 대한 입력은 출력 언어 및 번역된 전사본 및/또는 번역된 텍스트 프리프로세서의 출력을 포함한다. 일부 실시예는 입력 메타 정보 및/또는 번역된 메 타 정보를 AI 오디오 번역 생성기에 대한 입력으로서 더 포함한다. 오디오 번역 생성기에 대한 입력은 입력 언어, 입력 미디어, 비디오 프리프로세서 및 오디 오 프리프로세서의 출력, 입력 전사본, 및/또는 텍스트 프리프로세서의 출력을 더 포함할 수 있 다. AI 오디오 번역 생성기의 일부 실시예는 번역된 오디오를 생성하기 위해 번역된 전사본, 번역된 메타 정보, 및 출력 언어만을 필요로 한다. 일부 실시예는 번역된 오디오를 생성하기 위해 입력 오 디오(프리프로세싱되거나 및/또는 원시) 및 출력 언어만을 필요로 한다. AI 오디오 번역 생성기의 일부 실시예는 위에서 설명한 하나 이상의 입력을 갖는 데이터에 대해 학습된다. 일부 실시예에서, AI 오디오 번역 생성기는 일반적으로 이전 생성기와 동일한 유형의 정보에 대해 학습되 어 출력을 향상시킨다. 예를 들면, 비디오 및/또는 오디오 정보를 추가하면 음성 특성, 감정, 화자 신원, 음성 특성, 성별, 연령 등을 통합하여 번역 결과가 향상된다. 따라서 학습으로 인해, 결과적으로 번역된 오디오 는 일반적으로 TTS에서 제공되는 것보다 훨씬 더 많은 정보를 포함한다. 예를 들면, 번역된 오디오 는 발화 단어, 감정, 속도, 일시 정지, 어조, 운율, 세기, 강세, 음성 정체성 등과 일치한다. AI 오디오 번역 생성기의 일부 실시예는 2단계 GAN을 기반으로 한다. 제1 단계는 감정 및 기타 메타 정보 를 학습 및 추론에 통합하기 위한 고유한 인코더 및 디코더 구조를 갖춘 종래 GAN이다. 이러한 다수의 추가 인 코더 및 디코더를 제공하면, 감정과 메타 특성을 인식 및 생성하는 방법을 학습할 수 있다. 따라서 이 AI 오디 오 번역 생성기를 훈련시키는 단계는, 생성된 감정과 메타 특성 및 훈련 데이터 사이의 손실 또는 오류를 검출하도록 구성된 추가 고유 손실 기능을 더 포함한다. 제2 단계 GAN은 유사하게 설계되었지만 제1 단계 생성기의 출력을 제2 단계 생성기에 대한 입력으로서 받아들인 다. 이러한 방식으로 GAN을 레이어링하면, 생성된 출력의 사실감이 향상되고, 결과적으로 생성기가 사실적인 합성 번역을 생성하는 능력이 향상된다. 일부 실시예에서, AI 오디오 번역 생성기는 글로벌 스타일 토큰; 프리프로세싱 오디오에서 획득한 성별, 연령, 감정 특성 등과 같은 음성 특성에 대하여; \"원샷(one-shot)\" 접근 방식을 사용하여; 및/또는 인스턴스 정 규화의 유무에 관계없이 화자, 콘텐츠, 및/또는 감정 표현을 풀어서 학습/추론을 수행한다. 비디오 동기화 생성기 본 발명의 일부 실시예에서, 비디오 동기화 생성기는 AI 생성기이다. AI 비디오 동기화 생성기는 번 역된 오디오를 하나 이상의 입력으로부터 생성하도록 구성된다. AI 비디오 동기화 생성기는 번역된 오디 오를 본원에 설명된 하나 이상의 입력으로부터의 입력 비디오와 동기화하도록 구성된 임의의 AI 생성기일 수 있 다. 비제한적인 예로는 Wav2Lip, PC-AVS, NPFAP, HeadNeRF, FaceFormer, 및 LipSync3D가 포함된다.일부 실시예에서, AI 비디오 동기화 생성기는 입력 비디오, 비디오 프리프로세서의 출력, 및 번 역된 오디오로부터, 동기화된 비디오를 생성하도록 구성된다. 일부 실시예는 동기화된 비디오를 생 성하기 위해, 번역된 오디오, 비디오 프리프로세서의 출력, 및 입력 비디오만을 필요로 한다. 비디오 동기화 생성기의 일부 실시예는 입력 언어, 비디오 프리프로세서의 출력, 입력 오디오 , 오디오 프리프로세서의 출력, 입력 전사본, 텍스트 프리프로세서의 출력, 입력 메타 정 보, 번역된 전사본, 번역된 텍스트 프리프로세서의 출력, 번역된 메타 정보, 및/또는 번역 된 오디오 프리프로세서의 출력을 수신하도록 구성된다. 생성기 아키텍처, 학습 및 추론, 및 트레이닝을 위한 GAN의 아키텍처 측면에서, AI 비디오 동기화 생성기 는 AI 오디오 번역 생성기와 실질적으로 동일하다. 그러나, AI 비디오 동기화 생성기는 동기화된 비 디오를 상술한 입력의 하나 이상의 조합으로부터 생성하도록 학습되고 구성된다. 또한, AI 비디오 동기화 생성기는 재학습된 \"Wav2Lip\" GAN을 기반으로 할 수 있고; 복수의 프로그레시브 GAN을 포함할 수 있으며, 및/또는 광학 흐름 네트워크/광학 흐름 손실 고려사항을 포함할 수 있다. AI 비디오 동기화 생성기는 또한 경계 박스 또는 얼굴 랜드마크, 입/입술 랜드마크에 기초하여, 번역된 비 디오를 원본 비디오로 다시 재결합하기 위한 AI를 포함할 수 있다. AI 비디오 동기화 생성기는 원본 비디 오 광학 파라미터(예를 들면, 전문 비디오 믹싱, 비디오 컬러링 등)와 원본 비디오 입력의 비감독된 암묵적 특 성을 자동으로 일치시킬 수 있다. 일부 경우에서는, 멜스펙토그램을 원시 오디오 파형으로 변환하기 위해 맞춤형 GAN에 의해 학습된 추가적인 제2 단계 생성기 (MelGan, WaveGAN, WaveGlow, VoiceFixer 등)가 AI 비디오 동기화 생성기 (제1 단계 GAN 역 할을 함)의 아티팩트 및 저해상도를 개선하는 데 사용된다. 이 제2 단계 생성기는 아티팩트 및 저해상도를 개 선하기 위해 2단계 방식으로 사이클-재구성 데이터에 대해 학습될 수 있다. AI 비디오 동기화 생성기의 일부 실시예는 동기화된 비디오의 품질을 향상시키기 위한 제2 단계 생성기를 포함한다. 제2 단계 생성기는 향상된 비디오를 생성하기 위해 입력 비디오 입력 및 동기화된 비디오(14 6)만을 필요로 한다. 개선 사항에는 최대 비디오 크기 증가, 아티팩트 감소 (예를 들면, GAN의 클래식 아티팩 트 및 AI 비디오 동기화 생성기에 특정한 기타 아티팩트), 및 사실감 향상 등이 포함되지만 이에 제한되지 않는다. 예를 들면, 제2 단계 생성기는 비디오의 크기를 (예를 들면, 96, 256, 512)에서 더 큰 크기 (예를 들 면, 각각 256, 512, 1024 - 최대 2048)로 증가시킬 수 있으며, 비디오 품질 생성기의 출력이 원본 비디오에 재 삽입되어 4K 품질의 비디오를 효과적으로 생성할 수 있다. 원본 비디오는 3840 x 2160 이상일 수 있으며, 얼굴 궤적 비디오의 크기는 512 내지 2048일 수 있다. 제2 단계 생성기는 감독, 비감독, 또는 반감독으로 학습된 GAN-기반 네트워크에 의해 이루어질 수 있다. 제2 단계 생성기는 글로벌 스타일 토큰을 포함할 수 있고; 맞춤형 독점 데이터로 재학습된 \"FewshotVid2Vid\", \"Pix2PixHD\", \"GFPGAN\", \"Pix2Style2Pix\" 또는 \"Vid2VidHD\" 모델을 기반으로 할 수 있으며; 프로그레시브 GAN 을 포함하거나; 및/또는 광학 흐름 네트워크/광학 흐름 손실을 포함할 수 있다. 예시적인 구현 구현 1.1은 도 9에 제공된다. 도 9에 도시된 바와 같이, 기존 미디어 파일 (즉, 오디오/비디오 콘텐츠)이 컴퓨 터 디바이스를 통해 제출되고, 따라서 입력 미디어가 된다. 미디어 파일은 오디오 채널을 포함해야 한다. 그러나, 미디어 파일은 디바이스 (예를 들면, 스마트폰 앱, 데스크톱 앱, 웹 앱 등)에서 녹화되어 업로드되거나; 디바이스 (예를 들면, 스마트폰 앱, 데스크톱 앱, 웹 앱 등)로부터 업로드되거나; 또는 공유 클 라우드 데이터 링크 (예를 들면, Google 드라이브, Dropbox, AWS 등)를 통해 제출될 수 있다. 전사본은 본원에 설명된 바와 같이 입력 오디오로부터 획득된다. 전사본을 획득하기 위해서는 입력 오디오 및 입력 언어만이 필요하다. 일부 실시예는 제3자 클라우드 기반 서비스 (예를 들면, 구글, AWS 등)를 사용하여 전사본을 획득하고; 맞춤형 클라우드 기반 기술을 사용하여 머신 러닝 라이브러리 (예를 들면, Pytorch, Tensorflow, Caffe 등)로 작성된 전사본을 획득하고; 내장된 온-디바이스 서비스 (예를 들면, Siri)를 사용하여 전사본을 획득하고; 맞춤형 온-디바이스 서비스 (예를 들면, 코어ML, TF라이트 등)를 사용하여 에지 언어로 작성된 전사본을 획득한다. 앞서 설명한 바와 같이, 전사본은 종종 각 객체 (단어/ 음절/음소)에 대한 타임스탬프를 갖는 단어 및/또는 음절 및/또는 음소의 사전을 포함하며, 각 사람의 발화에 의해 지정된다.사용자는 원본 또는 번역된 구독에 대한 전사본을 업데이트할 수 있다. 사용자는 원본 언어의 전사본을 수정하거나 및/또는 속어, 고유명사 등에 대한 상세한 정보를 추가하여 원본 언어 및/또는 번역된 언어의 결과 를 향상시킬 수 있다. 텍스트 프리프로세서는 번역된 텍스트의 타임스탬프를 정렬하여 오디오 번역 생성기가 번역된 오디오의 타이밍을 원본 오디오와 동기화할 수 있도록 돕는다. 비디오 프리프로세서는 얼굴 인식 및 모든 얼굴의 정렬을 실행하여 입력 비디오에서 얼굴의 \"궤적\"을 찾아서 자른다. 이 작업은 클라우드 또는 온-디바이스에서 수행될 수 있다. 그런 다음, 오디오 번역 생성기는 번역된 오디오를 생성하기 위해 번역된 전사본(출력 언어 를 포함함)만을 입력으로서 취한다. 오디오 번역 생성기는 타임스탬프 정보를 사용하여 오디오 생성 입력을 적절한 크기의 세그먼트 (예를 들면, ~1.0초 내지 30.0초)로 분할하고 번역된 오디오를 입력 오디 오와 동기화할 수 있다. 오디오 번역 생성기는 또한 긴 형식의 콘텐츠(> 120.0초)에 대한 오디오 번 역 생성을 처리하기 위해 정보를 세그먼트 (예를 들면, ~1.0초 내지 30.0초)로 분할할 수 있다. 오디오 생성은 Google, AWS, 및 Apple과 같은 제3자 TTS 제공업체를 통해, 또는 예를 들면 TacoTron2, MelloTron, FlowTron 등에서 영감을 얻은 맞춤형 TTS 구현 (클라우드 또는 온-디바이스)을 통해 이루어질 수 있 다. 오디오 번역 생성기의 출력은 원본 오디오와 동일한 길이의 번역된 오디오 파일이다. 번역된 오디오 파일은 원본 오디오의 배경/주변 잡음 데이터, 원본 오디오의 음성 데이터/사운드가 제거된 번역된 음성 데이터/사운드를 포함할 수 있으며, 번역된 음성 데이터/사운드는 원본 오디오의 발화 요소와 시간적으로 밀접 하게 일치한다. 비디오 동기화 생성기는 번역된 비디오를 생성하기 위해, 번역된 오디오, 프리프로세싱된 비디 오, 및 입력 비디오를 입력으로서 취한다. 비디오 동기화 생성기는 맞춤형 데이터 세트에 대해 학습 된 Wav2Lip 모델 또는 Wav2Lip에서 영감을 얻은 모델을 사용할 수 있지만, 학습 중에 추가 데이터 증강 및 \"블 랙아웃\" 섹션의 변경을 갖는 맞춤형 데이터 세트에 대해 학습된다. 비디오 동기화 생성기는 생성된 출력 을 원본 비디오에 삽입하기 위한 포스트 프로세서(비 \"2단계\")를 포함할 수 있으며, 이는 원본 얼굴 랜드마크/ 입 랜드마크에 기반한 마스킹을 포함할 수 있다. 구현 1.2는 도 10에 도시된 바와 같이 엔드투엔드 번역 디바이스이다. 이 구현은 출력을 향상시키는 2단계 비 디오 품질 생성기를 더 포함한다. 이 생성기는 Wav2Lip에서 영감을 얻은 맞춤형 모델의 사이클 재구성 데이터 세트에서 페어링된 데이터에 대해 학습되거나 또는 FewShotVid2Vid 네트워크를 기반으로 할 수 있다. 구현 1.3은 도 11에 도시된 바와 같이 엔드투엔드 번역 디바이스이다. 이 구현은 원래의 화자의 음성 특성, 신 원, 감정 등과 일치하는 오디오 번역을 생성하기 위해 입력 오디오를 오디오 번역 생성기에 입력하는 단계를 더 포함한다. 오디오 번역 생성기는 맞춤형 데이터에 대해 학습된 적응형 음성 변환 네트워크에서 영감을 얻은 맞춤형 모델을 사용하여 비감독 방식으로 구현될 수 있다. 또한, 오디오 번역 생성기는 2단 계 품질 향상 포스트 프로세서로서 사이클 재구성 데이터에 대해 학습된 맞춤형 학습된 WaveGlow 네트워크를 포 함할 수 있다. 오디오 번역 생성기는 입력 오디오의 음성 특성, 신원, 감정 등을 구현 예 1.1의 오 디오 번역 출력에 적용할 수 있다. 구현 1.4는 도 12에 도시된 바와 같이 엔드투엔드 번역 디바이스이다. 구현 1.4는 오디오 프리프로세서를 포함하며, 이는 배경 잡음 분리, 화자 구분, 및/또는 의미 분할을 포함할 수 있다. 오디오 프리프로세서 의 출력은 전사본 결과의 품질 및 정확성을 향상시키고 오디오 번역의 품질 및 정확성을 향상시키는 데 사용될 수 있다. 구현 1.5는 도 13에 도시된 바와 같이 엔드투엔드 번역 디바이스이다. 구현 1.5는 전사본 및 메타 번역 생성기 에 프리프로세싱된 오디오 입력을 제공하는 단계를 포함한다. 이 접근 방식은 전사본 번역의 품질 및 정 확도가 향상된 맞춤형 GAN 네트워크 또는 트랜스포머 네트워크를 학습하는 데 사용될 수 있고; 원본 오디오 입 력에 기초하여 풍자, 유머, 관용구 등을 번역 결과가 포착할 수 있도록 하고; 및 전사본 번역 생성기의 고급 비 감독 및 반감독 학습을 통해 품질 및 정확도를 향상시키고 학습 중에 자주 또는 전혀 볼 수 없는 언어에 대한 전사본 결과를 허용할 수 있다 (예를 들면, Few-Shot 및 One-Shot 네트워크). 일부 실시예에서, 본 발명은 실시간 결과 또는 실시간에 가까운 결과를 제공하는 증강 현실(augmented reality(AR)) 번역기이다. AR 번역기는 사전 녹음된 콘텐츠와 라이브 오디오 또는 오디오/비디오 채팅에서 모 든 언어로 원활한 커뮤니케이션을 가능하게 한다. 하드웨어 및 소프트웨어 인프라 예시 본 발명은 소프트웨어 기반 명령어에 반응하여 동작을 수행하는 다양한 컴퓨팅 시스템 및/또는 플랫폼에 구현될 수 있다. 다음은 본 발명을 구현하기 위해 활용될 수 있는 정보 기술에 대한 선행 기술을 제공한다. 하기 청구범위에 설명된 컴퓨터 판독 가능한 매체는 컴퓨터 판독 가능한 신호 매체 또는 컴퓨터 판독 가능한 저 장 매체일 수 있다. 컴퓨터 판독 가능한 저장 매체는 예를 들어 전자, 자기, 광학, 전자기, 적외선, 또는 반도 체 시스템, 장치, 또는 디바이스, 또는 상술한 것들의 임의의 적절한 조합일 수 있지만 이에 제한되지 않는다. 컴퓨터 판독 가능한 저장 매체의 보다 구체적인 예시 (비배타적인 목록)로는, 하나 이상의 배선을 갖는 전기적 연결, 휴대용 컴퓨터 디스켓, 하드 디스크, 랜덤 액세스 메모리(RAM), 읽기 전용 메모리(ROM), 지울 수 있는 프 로그램 가능한 읽기 전용 메모리(EPROM 또는 플래시 메모리), 광섬유, 휴대용 컴팩트 디스크 읽기 전용 메모리 (CD-ROM), 광 저장 장치, 자기 저장 장치, 또는 상술한 것들의 적절한 조합이 포함될 수 있다. 본 문서의 맥락 에서, 컴퓨터 판독 가능한 저장 매체는 명령 실행 시스템, 장치, 또는 디바이스에 의해 또는 이와 관련하여 사 용하기 위한 프로그램을 포함하거나, 또는 저장할 수 있는 임의의 비일시적 유형(tangible) 매체일 수 있다. 컴퓨터 판독 가능한 신호 매체는 예를 들면, 기저대역 또는 반송파의 일부로 컴퓨터 판독 가능한 프로그램 코드 가 구현된 전파된 데이터 신호를 포함할 수 있다. 이러한 전파된 신호는 전자기, 광학, 또는 이들의 임의의 적 절한 조합을 포함하지만 이에 제한되지 않는 임의의 다양한 형태를 취할 수 있다. 컴퓨터 판독 가능한 신호 매 체는 컴퓨터 판독 가능한 저장 매체가 아니며 명령 실행 시스템, 장치, 또는 디바이스에 의해 또는 이와 관련하 여 사용하기 위한 프로그램을 통신, 전파, 또는 전송할 수 있는 임의의 컴퓨터 판독 가능한 매체일 수 있다. 컴퓨터 판독 가능한 매체에 구현된 프로그램 코드는 무선, 유선, 광섬유 케이블, 무선 주파수 등을 포함하지만 이에 제한되지 않는 임의의 적절한 매체, 또는 상술한 것들의 임의의 적절한 조합을 사용하여 전송될 수 있다. 본 발명의 양상에 대한 작동을 수행하기 위한 컴퓨터 프로그램 코드는, 자바, C#, C++, 비주얼 베이직 등과 같 은 객체 지향 프로그래밍 언어 및 \"C\" 프로그래밍 언어 또는 이와 유사한 프로그래밍 언어와 같은 종래 절차적 프로그래밍 언어를 포함하는 하나 이상의 프로그래밍 언어의 임의의 조합으로 작성될 수 있다. 본 발명의 양상은 본 발명의 실시예에 따른 방법, 장치(시스템) 및 컴퓨터 프로그램 제품의 플로우 차트 도면 및/또는 블록도를 참조하여 설명할 수 있다. 플로우 차트 도면 및/또는 블록도의 각 블록, 및 플로우 차트 도 면 및/또는 블록도의 블록의 조합은, 컴퓨터 프로그램 명령어에 의해 구현될 수 있다는 것을 이해할 것이다. 이러한 컴퓨터 프로그램 명령어는 범용 컴퓨터, 특수 목적 컴퓨터, 또는 기타 프로그램 가능한 데이터 처리 장 치의 프로세서에 제공되어, 컴퓨터 또는 기타 프로그램 가능한 데이터 처리 장치의 프로세서를 통해 실행되는 명령어가 플로우 차트 및/또는 블록도 블록 또는 블록에 명시된 기능/동작을 구현하는 수단을 생성하도록 기계 를 생성할 수 있다. 이러한 컴퓨터 프로그램 명령어는 또한 컴퓨터, 기타 프로그래밍 가능한 데이터 처리 장치, 또는 기타 장치가 특정 방식으로 작동하도록 지시할 수 있는 컴퓨터 판독 가능한 매체에 저장될 수 있으므로, 컴퓨터 판독 가능한 매체에 저장된 명령어는 플로우 차트 및/또는 블록도 블록 또는 블록에 명시된 기능/동작을 구현하는 명령어를 포함하는 제조물을 생성할 수 있다. 컴퓨터 프로그램 명령어는 또한 컴퓨터, 기타 프로그램 가능한 데이터 프로세싱 장치, 또는 기타 디바이스에 로 딩되어 컴퓨터, 기타 프로그램 가능한 장치 또는 기타 디바이스에서 일련의 동작 단계가 수행되도록 하여 컴퓨 터 또는 기타 프로그램 가능한 장치에서 실행되는 명령어가 플로우 차트 및/또는 블록도 또는 블록에 명시된 기 능/동작을 구현하기 위한 프로세스를 제공하도록 컴퓨터 구현 프로세스를 생성할 수 있다. 상술한 이점들, 및 상술한 설명으로부터 명백해진 이점들은 효율적으로 달성된다. 본 발명의 범위를 벗어나지 않는 범위 내에서 상기 구성에 특정 변경이 이루어질 수 있으므로, 상술한 설명에 포함되거나 또는 첨부된 도면 에 도시된 모든 사항은 제한적인 의미가 아니라 예시적인 의미로 해석되어야 한다. 또한, 하기 청구범위는 본원에 설명된 본 발명의 모든 일반적이고 구체적인 특징과, 언어의 문제로서 그 사이에 속한다고 할 수 있는 본 발명의 범위에 대한 모든 기재를 포함하도록 의도되었음을 이해해야 한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13"}
{"patent_id": "10-2023-7041967", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 발명의 보다 완전한 이해를 위해, 첨부된 도면과 관련된 하기의 상세한 설명을 참조해야 한다. 도 1은 본 발명의 일 실시예의 블록도이다. 도 2는 본 발명의 일 실시예의 플로우 차트이다. 도 3은 입력 전사본을 생성하는 프로세스의 일 실시예의 블록도이다. 도 4는 입력 메타 정보를 생성하는 프로세스의 일 실시예의 블록도이다. 도 5는 번역된 전사본 및/또는 번역된 메타 정보를 생성하는 프로세스의 일 실시예의 블록도이다. 도 6은 번역된 오디오를 생성하는 프로세스의 일 실시예의 블록도이다. 도 7은 번역된 오디오와 동기화된 비디오를 생성하는 프로세스의 일 실시예의 블록도이다. 도 8은 번역된 오디오와 동기화된 비디오를 생성하고 고품질의 출력 비디오를 생성하기 위해 포스트프로세싱 (postprocessing) 프로세스를 실행하는 프로세스의 일 실시예의 블록도이다.도 9는 본 발명의 일 실시예의 구현의 블록도이다. 도 10은 본 발명의 일 실시예의 구현의 블록도이다. 도 11은 본 발명의 일 실시예의 구현의 블록도이다. 도 12는 본 발명의 일 실시예의 구현의 블록도이다. 도 13은 본 발명의 일 실시예의 구현의 블록도이다."}
