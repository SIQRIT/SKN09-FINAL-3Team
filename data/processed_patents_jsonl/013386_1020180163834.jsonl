{"patent_id": "10-2018-0163834", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0080389", "출원번호": "10-2018-0163834", "발명의 명칭": "전자 장치 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "유태환"}}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치의 제어 방법에 있어서,컨텐츠를 획득하여 메모리에 저장하는 단계;상기 저장된 컨텐츠를 분석하여 제1 행동과 관련된 제1 구간을 판단하는 단계;상기 판단된 제1 구간의 컨텐츠를 출력하는 단계;상기 제1 구간의 컨텐츠가 출력되는 동안 촬영된 사용자 영상을 획득하는 단계;상기 사용자 영상을 분석하여 사용자가 상기 제1 행동을 완료하였는지 여부를 판단하는 단계; 및상기 사용자가 상기 제1 행동을 완료한 것으로 판단되면, 상기 제1 구간 다음의 제2 구간 컨텐츠를 출력하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 제1 구간을 판단하는 단계는,학습된 인공지능 모델에 상기 컨텐츠에 포함된 영상을 구성하는 영상 프레임을 입력하여 상기 입력된 영상 프레임의 특징값을 획득하는 단계;상기 획득된 특징값을 바탕으로 상기 제1 행동과 관련된 장면 이해를 수행하는 단계; 및상기 수행된 장면 이해를 바탕으로 상기 제1 행동과 관련된 제1 구간을 판단하는 단계;를 포함하는 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 사용자 영상을 획득하는 단계는,상기 학습된 인공지능 모델에 상기 사용자 영상을 구성하는 영상 프레임을 입력하여 상기 입력된 영상 프레임의특징값을 획득하는 단계; 및상기 획득된 특징값을 바탕으로 상기 사용자 영상에 포함된 상기 사용자의 행동에 대한 장면 이해를 수행하는단계;를 포함하는 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 제1 행동을 완료하였는지 여부를 판단하는 단계는,상기 영상 중 상기 제1 구간의 영상을 구성하는 영상 프레임의 특징값 및 상기 사용자 영상을 구성하는 영상 프레임의 특징값을 비교하여 상기 제1 행동을 완료하였는지 여부를 판단하는 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 3 항에 있어서,공개특허 10-2020-0080389-3-상기 제1 행동을 완료하였는지 여부를 판단하는 단계는,상기 제1 행동과 관련된 장면 이해와 상기 사용자의 행동에 대한 장면 이해를 바탕으로 상기 제1 행동을 완료하였는지 여부를 판단하는 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 3 항에 있어서,상기 제1 구간의 영상 및 상기 사용자 영상 각각과 연관된 문장을 획득하여 표시하는 단계;를 더 포함하며,상기 표시하는 단계는,상기 제1 구간의 영상에 포함된 상기 제1 행동에 대한 장면 이해를 수행하여 제1 문장을 획득하고,상기 사용자 영상에 포함된 상기 사용자의 행동에 대한 장면 이해를 수행하여 제2 문장을 획득하며,상기 획득된 제1 및 제2 문장을 상기 제1 구간의 영상이 디스플레이되는 일 영역에 표시하는 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 제1 행동을 완료하였는지 여부를 판단하는 단계는,상기 제1 및 제2 문장의 유사도 정도에 기초하여 상기 제1 행동을 수행하는지 여부를 판단하는 것을 특징으로하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서,상기 출력하는 단계는,상기 제1 행동을 수행하지 않는 것으로 판단되면, 상기 제1 행동과 상이한 행동을 수행하고 있음을 알리는 메시지를 영상 및 오디오 중 적어도 하나를 통해 출력하는 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 5 항에 있어서,상기 출력하는 단계는,상기 제1 행동을 완료하지 않은 것으로 판단되면, 상기 영상의 재생을 중지하거나 혹은 상기 제1 구간의 영상을반복 재생하는 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서,상기 출력하는 단계는,상기 영상의 재생이 중지되거나 또는 상기 제1 구간의 영상이 반복 재생되는 동안 상기 제1 행동과 상이한 행동이 기설정된 임계 시간 이상 수행되면, 상기 제2 구간의 영상을 출력하는 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 1 항에 있어서,상기 메모리는,버퍼 및 메인 메모리를 포함하며,상기 저장하는 단계는,공개특허 10-2020-0080389-4-상기 컨텐츠를 상기 버퍼에 저장하며, 상기 버퍼의 저장 용량이 부족하면, 상기 버퍼에 저장되지 않은 나머지컨텐츠를 상기 메인 메모리에 저장하는 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "전자 장치에 있어서,촬영부;컨텐츠를 획득하여 저장하는 메모리;상기 컨텐츠를 출력하는 출력부; 및상기 저장된 컨텐츠를 분석하여 제1 행동과 관련된 제1 구간을 판단하여, 상기 판단된 제1 구간의 컨텐츠를 출력하도록 상기 출력부를 제어하며,상기 제1 구간의 컨텐츠가 출력되는 동안 상기 촬영부를 통해 촬영된 사용자 영상을 획득하고, 상기 획득한 사용자 영상을 분석하여 사용자가 상기 제1 행동을 완료하였는지 여부를 판단하여 상기 사용자가상기 제1 행동을 완료한 것으로 판단되면, 상기 제1 구간 다음의 제2 구간 컨텐츠를 출력하도록 상기 출력부를제어하는 프로세서;를 포함하는 전자 장치."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12 항에 있어서,상기 프로세서는,학습된 인공지능 모델에 상기 컨텐츠에 포함된 영상을 구성하는 영상 프레임을 입력하여 상기 입력된 영상 프레임의 특징값을 획득하고,상기 획득된 특징값을 바탕으로 상기 제1 행동과 관련된 장면 이해를 수행하며, 상기 수행된 장면 이해를 바탕으로 상기 제1 행동과 관련된 제1 구간을 판단하는 것을 특징으로 하는 전자장치."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13 항에 있어서,상기 프로세서는,상기 학습된 인공지능 모델에 상기 사용자 영상을 구성하는 영상 프레임을 입력하여 상기 입력된 영상 프레임의특징값을 획득하고, 상기 획득된 특징값을 바탕으로 상기 사용자 영상에 포함된 상기 사용자의 행동에 대한 장면 이해를 수행하는것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서,상기 프로세서는,상기 영상 중 상기 제1 구간의 영상을 구성하는 영상 프레임의 특징값 및 상기 사용자 영상을 구성하는 영상 프레임의 특징값을 비교하여 상기 제1 행동을 완료하였는지 여부를 판단하는 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 14 항에 있어서,상기 프로세서는,공개특허 10-2020-0080389-5-상기 제1 행동과 관련된 장면 이해와 상기 사용자의 행동에 대한 장면 이해를 바탕으로 상기 제1 행동을 완료하였는지 여부를 판단하는 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 14 항에 있어서,상기 프로세서는,상기 제1 구간에 포함된 상기 제1 행동에 대한 장면 이해를 수행하여 제1 문장을 획득하고,상기 사용자 영상에 포함된 상기 사용자의 행동에 대한 장면 이해를 수행하여 제2 문장을 획득하며,상기 획득된 제1 및 제2 문장을 상기 제1 구간의 영상이 디스플레이되는 일 영역에 표시하도록 상기 출력부를제어하는 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 17 항에 있어서,상기 프로세서는,상기 제1 및 제2 문장의 유사도 정도에 기초하여 상기 제1 행동을 수행하는지 여부를 판단하는 것을 특징으로하는 전자 장치."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 18 항에 있어서,상기 프로세서는,상기 제1 행동을 수행하지 않는 것으로 판단되면, 상기 제1 행동과 상이한 행동을 수행하고 있음을 알리는 메시지를 영상 및 오디오 중 적어도 하나를 통해 출력하도록 상기 출력부를 제어하는 것을 특징으로 하는 전자장치."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 16 항에 있어서,상기 프로세서는,상기 제1 행동을 완료하지 않은 것으로 판단되면, 상기 영상의 재생을 중지하거나 혹은 상기 제1 구간의 영상을반복 재생하도록 상기 출력부를 제어하는 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제 20 항에 있어서,상기 프로세서는,상기 영상의 재생이 중지되거나 또는 상기 제1 구간의 영상이 반복 재생되는 동안 상기 제1 행동과 상이한 행동이 기설정된 임계 시간 이상 수행되면, 상기 제2 구간의 영상을 재생하도록 상기 출력부를 제어하는 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2018-0163834", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제 12 항에 있어서,상기 메모리는,버퍼; 및메인 메모리;를 포함하며,상기 프로세서는,공개특허 10-2020-0080389-6-상기 컨텐츠를 상기 버퍼에 저장하며, 상기 버퍼의 저장 용량이 부족하면, 상기 버퍼에 저장되지 않은 나머지컨텐츠를 상기 메인 메모리에 저장하는 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2018-0163834", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치 및 그 제어 방법이 개시된다. 본 개시에 따른 전자 장치의 제어 방법은, 컨텐츠를 획득하여 메모리에 저장하는 단계, 저장된 컨텐츠를 분석하여 제1 행동과 관련된 제1 구간을 판단하는 단계, 판단된 제1 구간의 컨 텐츠를 출력하는 단계, 제1 구간의 컨텐츠가 출력되는 동안 촬영된 사용자 영상을 획득하는 단계, 사용자 영상을 분석하여 사용자가 제1 행동을 완료하였는지 여부를 판단하는 단계 및 사용자가 제1 행동을 완료한 것으로 판단 되면, 제1 구간 다음의 제2 구간 컨텐츠를 출력하는 단계를 포함한다."}
{"patent_id": "10-2018-0163834", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 그 제어 방법에 관한 것으로써, 보다 상세하게는 사용자의 행동에 기초하여 컨텐츠를 자동 출력하는 전자 장치 및 그 제어 방법에 관한 것이다."}
{"patent_id": "10-2018-0163834", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "전자 장치의 통신 기술 및 사용자 인터페이스가 발전함에 따라, 사용자는 장소 및 시간에 제약 없이 필요한 정 보를 쉽게 전자 장치를 통하여 제공받을 수 있다. 예를 들어, 스마트 TV와 같은 전자 장치는 사용자가 요청한 영상을 재생할 뿐만 아니라, 사용자가 요청한 영상 에서 사용자 의도에 적합한 구간의 영상만을 선별하여 제공하는 동작을 수행한다. 이 같은 다양한 서비스 관련 기술이 개발되고 있으며, 최근에는 전자 장치에서 사용자가 직접 참여할 수 있는 영상(예를 들어, 운동, 교육, 게임, 요리 등)을 제공하는 서비스를 제공하고 있다. 그러나, 이 같은 사용자 참여 관련 영상 서비스를 이용하는 사용자는 자신이 요청한 영상의 진행 속도에 맞춰 해당 영상과 관련된 동작을 수행하지 못하거나 혹은 정보를 습득하지 못하게 되어, 사용자가 전자 장치를 통해 해당 영상의 재생 관련 제어 동작을 수행해야 하는 문제가 있다. 본 개시는 상술한 문제점을 해결하기 위해 안출된 것으로서, 본 개시의 목적은 전자 장치에서 사용자의 행동을 고려하여 컨텐츠의 출력을 자동으로 제어하도록 함을 목적으로 한다. 이상과 같은 목적을 달성하기 위한 전자 장치의 제어 방법은, 컨텐츠를 획득하여 메모리에 저장하는 단계, 상기 저장된 컨텐츠를 분석하여 제1 행동과 관련된 제1 구간을 판단하는 단계, 상기 판단된 제1 구간의 컨텐츠를 출 력하는 단계, 상기 제1 구간의 컨텐츠가 출력되는 동안 촬영된 사용자 영상을 획득하는 단계, 상기 사용자 영상 을 분석하여 사용자가 상기 제1 행동을 완료하였는지 여부를 판단하는 단계 및 상기 사용자가 상기 제1 행동을 완료한 것으로 판단되면, 상기 제1 구간 다음의 제2 구간 컨텐츠를 출력하는 단계를 포함한다. 그리고, 상기 제1 구간을 판단하는 단계는, 학습된 인공지능 모델에 상기 컨텐츠에 포함된 영상을 구성하는 영 상 프레임을 입력하여 상기 입력된 영상 프레임의 특징값을 획득하는 단계, 상기 획득된 특징값을 바탕으로 상 기 제1 행동과 관련된 장면 이해를 수행하는 단계 및 상기 수행된 장면 이해를 바탕으로 상기 제1 행동과 관련 된 제1 구간을 판단하는 단계를 포함할 수 있다. 또한, 상기 사용자 영상을 획득하는 단계는, 상기 학습된 인공지능 모델에 상기 사용자 영상을 구성하는 영상 프레임을 입력하여 상기 입력된 영상 프레임의 특징값을 획득하는 단계 및 상기 획득된 특징값을 바탕으로 상기 사용자 영상에 포함된 상기 사용자의 행동에 대한 장면 이해를 수행하는 단계를 포함할 수 있다. 그리고, 상기 제1 행동을 완료하였는지 여부를 판단하는 단계는, 상기 영상 중 상기 제1 구간의 영상을 구성하 는 영상 프레임의 특징값 및 상기 사용자 영상을 구성하는 영상 프레임의 특징값을 비교하여 상기 제1 행동을 완료하였는지 여부를 판단할 수 있다. 또한, 상기 제1 행동을 완료하였는지 여부를 판단하는 단계는, 상기 제1 행동과 관련된 장면 이해와 상기 사용 자의 행동에 대한 장면 이해를 바탕으로 상기 제1 행동을 완료하였는지 여부를 판단할 수 있다. 그리고, 상기 제1 구간의 영상 및 상기 사용자 영상 각각과 연관된 문장을 획득하여 표시하는 단계를 더 포함하 며, 상기 표시하는 단계는, 상기 제1 구간의 영상에 포함된 상기 제1 행동에 대한 장면 이해를 수행하여 제1 문 장을 획득하고, 상기 사용자 영상에 포함된 상기 사용자의 행동에 대한 장면 이해를 수행하여 제2 문장을 획득 하며, 상기 획득된 제1 및 제2 문장을 상기 제1 구간의 영상이 디스플레이되는 일 영역에 표시할 수 있다. 또한, 상기 제1 행동을 완료하였는지 여부를 판단하는 단계는, 상기 제1 및 제2 문장의 유사도 정도에 기초하여 상기 제1 행동을 수행하는지 여부를 판단할 수 있다.그리고, 상기 출력하는 단계는, 상기 제1 행동을 수행하지 않는 것으로 판단되면, 상기 제1 행동과 상이한 행동 을 수행하고 있음을 알리는 메시지를 영상 및 오디오 중 적어도 하나를 통해 출력할 수 있다. 또한, 상기 출력하는 단계는, 상기 제1 행동을 완료하지 않은 것으로 판단되면, 상기 영상의 재생을 중지하거나 혹은 상기 제1 구간의 영상을 반복 재생할 수 있다. 그리고, 상기 출력하는 단계는, 상기 영상의 재생이 중지되거나 또는 상기 제1 구간의 영상이 반복 재생되는 동 안 상기 제1 행동과 상이한 행동이 기설정된 임계 시간 이상 수행되면, 상기 제2 구간의 영상을 출력할 수 있다. 또한, 상기 메모리는, 버퍼 및 메인 메모리를 포함하며, 상기 저장하는 단계는, 상기 컨텐츠를 상기 버퍼에 저 장하며, 상기 버퍼의 저장 용량이 부족하면, 상기 버퍼에 저장되지 않은 나머지 영상을 상기 메인 메모리에 저 장할 수 있다. 한편, 본 개시의 또다른 실시 예에 따르면, 전자 장치는 촬영부, 컨텐츠를 획득하여 저장하는 메모리, 상기 컨 텐츠를 출력하는 출력부 및 상기 저장된 컨텐츠를 분석하여 제1 행동과 관련된 제1 구간을 판단하여, 상기 판단 된 제1 구간의 컨텐츠를 출력하도록 상기 출력부를 제어하며, 상기 제1 구간의 컨텐츠가 출력되는 동안 상기 촬 영부를 통해 촬영된 사용자 영상을 획득하고, 상기 획득한 사용자 영상을 분석하여 사용자가 상기 제1 행동을 완료하였는지 여부를 판단하여 상기 사용자가 상기 제1 행동을 완료한 것으로 판단되면, 상기 제1 구간 다음의 제2 구간의 컨텐츠를 출력하도록 상기 출력부를 제어하는 프로세서를 포함한다. 그리고, 상기 프로세서는, 학습된 인공지능 모델에 상기 컨텐츠에 포함된 영상을 구성하는 영상 프레임을 입력 하여 상기 입력된 영상 프레임의 특징값을 획득하고, 상기 획득된 특징값을 바탕으로 상기 제1 행동과 관련된 장면 이해를 수행하며, 상기 수행된 장면 이해를 바탕으로 상기 제1 행동과 관련된 제1 구간을 판단할 수 있다. 또한, 상기 프로세서는, 상기 학습된 인공지능 모델에 상기 사용자 영상을 구성하는 영상 프레임을 입력하여 상 기 입력된 영상 프레임의 특징값을 획득하고, 상기 획득된 특징값을 바탕으로 상기 사용자 영상에 포함된 상기 사용자의 행동에 대한 장면 이해를 수행할 수 있다. 그리고, 상기 프로세서는, 상기 영상 중 상기 제1 구간의 영상을 구성하는 영상 프레임의 특징값 및 상기 사용 자 영상을 구성하는 영상 프레임의 특징값을 비교하여 상기 제1 행동을 완료하였는지 여부를 판단할 수 있다. 또한, 상기 프로세서는, 상기 제1 행동과 관련된 장면 이해와 상기 사용자의 행동에 대한 장면 이해를 바탕으로 상기 제1 행동을 완료하였는지 여부를 판단할 수 있다. 그리고, 상기 프로세서는, 상기 제1 구간에 포함된 상기 제1 행동에 대한 장면 이해를 수행하여 제1 문장을 획 득하고, 상기 사용자 영상에 포함된 상기 사용자의 행동에 대한 장면 이해를 수행하여 제2 문장을 획득하며, 상 기 획득된 제1 및 제2 문장을 상기 제1 구간의 영상이 디스플레이되는 일 영역에 표시하도록 상기 출력부를 제 어할 수 있다. 또한, 상기 프로세서는, 상기 제1 및 제2 문장의 유사도 정도에 기초하여 상기 제1 행동을 수행하는지 여부를 판단할 수 있다. 그리고, 상기 프로세서는, 상기 제1 행동을 수행하지 않는 것으로 판단되면, 상기 제1 행동과 상이한 행동을 수 행하고 있음을 알리는 메시지를 영상 및 오디오 중 적어도 하나를 통해 출력하도록 상기 출력부를 제어할 수 있 다. 또한, 상기 프로세서는, 상기 제1 행동을 완료하지 않은 것으로 판단되면, 상기 영상의 재생을 중지하거나 혹은 상기 제1 구간의 영상을 반복 재생하도록 상기 출력부를 제어할 수 있다. 그리고, 상기 프로세서는, 상기 영상의 재생이 중지되거나 또는 상기 제1 구간의 영상이 반복 재생되는 동안 상 기 제1 행동과 상이한 행동이 기설정된 임계 시간 이상 수행되면, 상기 제2 구간의 영상을 재생하도록 상기 출 력부를 제어할 수 있다. 또한, 상기 메모리는, 버퍼 및 메인 메모리를 더 포함하며, 상기 프로세서는, 상기 컨텐츠를 상기 버퍼에 저장 하며, 상기 버퍼의 저장 용량이 부족하면, 상기 버퍼에 저장되지 않은 나머지 컨텐츠를 상기 메인 메모리에 저 장할 수 있다. 이상과 같이, 본 개시에 따르면, 전자 장치는 사용자의 행동을 고려하여 컨텐츠의 출력을 자동으로 제어할 수 있다."}
{"patent_id": "10-2018-0163834", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 문서의 다양한 실시 예가 첨부된 도면을 참조하여 기재된다. 그러나, 이는 본 문서에 기재된 기술을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 문서의 실시 예의 다양한 변경(modifications), 균등물 (equivalents), 및/또는 대체물(alternatives)을 포함하는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조 부호가 사용될 수 있다. 본 문서에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. 본 문서에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\"등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 문서에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중 요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요 소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제1 구성요소)가 다른 구성 요소(예: 제2 구성요소)에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 상기 어떤 구성요 소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제 3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 문서에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for),\" \"~하는 능력을 가지는(having the capacity to),\" \"~하도록 설계된(designed to),\" \"~하도 록 변경된(adapted to),\" \"~하도록 만들어진(made to),\" 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장치가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행하도록 구성된(또는 설정된) 부프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메모리 장치에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 본 문서의 다양한 실시예들에 따른 전자 장치는, 예를 들면, 스마트폰, 태블릿 PC, 이동 전화기, 영상 전화기, 전자책 리더기, 데스크탑 PC, 랩탑 PC, 넷북 컴퓨터, 워크스테이션, 서버, PDA, PMP(portable multimedia player), MP3 플레이어, 의료기기, 카메라, 또는 웨어러블 장치 중 적어도 하나를 포함할 수 있다. 웨어러블 장 치는 액세서리형(예: 시계, 반지, 팔찌, 발찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head- mounted-device(HMD)), 직물 또는 의류 일체형(예: 전자 의복), 신체 부착형(예: 스킨 패드 또는 문신), 또는 생체 이식형 회로 중 적어도 하나를 포함할 수 있다. 어떤 실시예들에서, 전자 장치는, 예를 들면, 텔레비전, DVD(digital video disk) 플레이어, 오디오, 냉장고, 에어컨, 청소기, 오븐, 전자레인지, 세탁기, 공기 청정기, 셋톱 박스, 홈 오토매이션 컨트롤 패널, 보안 컨트롤 패널, 미디어 박스(예: 삼성 HomeSyncTM, 애플TVTM, 또는 구글 TVTM), 게임 콘솔(예: XboxTM, PlayStationTM), 전자 사전, 전자 키, 캠코더, 또는 전자 액자 중 적어도 하나를 포함할 수 있다. 다른 실시예에서, 전자 장치는, 각종 의료기기(예: 각종 휴대용 의료측정기기(혈당 측정기, 심박 측정기, 혈압 측정기, 또는 체온 측정기 등), MRA(magnetic resonance angiography), MRI(magnetic resonance imaging), CT(computed tomography), 촬영기, 또는 초음파기 등), 네비게이션 장치, 위성 항법 시스템(GNSS(global navigation satellite system)), EDR(event data recorder), FDR(flight data recorder), 자동차 인포테인먼 트 장치, 선박용 전자 장비(예: 선박용 항법 장치, 자이로 콤파스 등), 항공 전자기기(avionics), 보안 기기, 차량용 헤드 유닛(head unit), 산업용 또는 가정용 로봇, 드론(drone), 금융 기관의 ATM, 상점의 POS(point of sales), 또는 사물 인터넷 장치 (예: 전구, 각종 센서, 스프링클러 장치, 화재 경보기, 온도조절기, 가로등, 토 스터, 운동기구, 온수탱크, 히터, 보일러 등) 중 적어도 하나를 포함할 수 있다. 본 문서에서, 사용자라는 용어는 전자 장치를 사용하는 사람 또는 전자 장치를 사용하는 장치(예: 인공지능 전 자 장치)를 지칭할 수 있다. 도 1은 본 개시의 일 실시예에 따른 전자 장치의 블록도이다. 도 1에 도시된 바와 같이, 전자 장치는 촬영부, 메모리, 출력부 및 프로세서를 포함 한다. 촬영부는 사용자 및 전자 장치의 주변 환경을 촬영한다. 이 같은 촬영부는 카메라로 구현될 수 있다. 카메라로 구현될 경우, 촬영부는 영상이 투과되는 렌즈(미도시) 및 렌즈를 통해 투과된 영상을 감 지하는 이미지 센서(미도시)를 포함할 수 있다. 이미지 센서(이미지)는 CCD이미지 센서 또는 CMOS 이미지 센서 로 구현될 수 있다. 촬영부를 통해 획득된 영상 데이터는 영상 처리부(미도시)에서 처리될 수 있다. 메모리는 외부로부터 수신된 컨텐츠 및 전자 장치의 또다른 메모리 또는 전자 장치와 물리적으 로 연결된 외부 메모리에 저장된 컨텐츠 중 적어도 하나를 임시 저장한다. 여기서, 컨텐츠는 영상 데이터, 오디오 데이터, 텍스트 중 적어도 하나를 포함할 수 있다. 출력부는 컨텐츠를 출력한다. 구체적으로, 출력부는 컨텐츠에 포함된 영상 데이터, 오디오 데이터 텍스트 중 적어도 하나를 출력할 수 있다.프로세서는 메모리에 임시 저장된 컨텐츠를 분석하여 제1 행동과 관련된 제1 구간을 판단하고, 판단 된 제1 구간의 컨텐츠를 출력하도록 출력부를 제어한다. 또한, 프로세서는 출력부를 통해 제1 구간의 컨텐츠가 출력되는 동안 촬영부가 촬영 모드로 동작하도록 제어한다. 이에 따라, 촬영부는 촬영 모드로 전환하여 영상을 촬영하며, 프로세서는 제1 구간의 컨텐츠가 출력되는 동안, 촬영부를 통해 촬영된 사용자 영상을 획득할 수 있다. 이후, 프로세서는 획득된 사용자 영상을 분석하여 사용자가 제1 구간의 컨텐츠에서 수행되는 제1 행동을 완료하였는지 여부를 판단하여, 제1 행동을 완료한 것으로 판단되면, 제1 구간 다음의 제2 구간의 컨텐츠를 출 력하도록 출력부를 제어한다. 이에 따라, 출력부는 제2 구간의 컨텐츠를 출력하며, 프로세서는 제2 구간의 컨텐츠가 출력되면, 전 술한 바와 같은 일련의 동작을 수행하여 사용자가 제2 구간의 컨텐츠에서 수행되는 제2 행동을 완료하였는지 여 부를 판단할 수 있다. 한편, 프로세서는 다음과 같은 실시예를 통해 각 행동과 관련된 구간을 판단할 수 있다. 실시예에 따라, 프로세서는 학습된 인공지능 모델에 컨텐츠에 포함된 영상을 구성하는 영상 프레임을 입 력하여 입력된 영상 프레임의 특징값을 획득하고, 획득된 특징값을 바탕으로 제1 행동과 관련된 장면 이해를 수 행한다. 이후, 프로세서는 수행된 장면 이해를 바탕으로 기획득된 영상으로부터 제1 행동과 관련된 제1 구간을 판단할 수 있다. 여기서, 입력된 영상 프레임으로부터 특징값을 추출하는 인공지능 모델에 대한 구체적인 설명은 하기에서 상세 히 설명하도록 한다. 한편, 컨텐츠는 오디오 데이터를 포함할 수 있다. 이 경우, 프로세서는 컨텐츠에 포함된 오디오 데이터로 부터 문장을 획득하고, 획득한 문장을 구성하는 구성 요소를 분석하여 객체와 행동 정보를 판단하고, 판단된 결 과에 기초하여 제1 행동과 관련된 제1 구간을 판단할 수 있다. 이 같은 실시예를 통해, 프로세서는 컨텐츠로부터 제1 행동과 관련된 제1 구간을 판단할 수 있다. 한편, 프로세서는 학습된 인공지능 모델에 촬영부를 통해 촬영된 사용자 영상을 구성하는 영상 프레 임을 입력하여, 입력된 영상 프레임의 특징값을 획득한다. 이후, 프로세서는 획득된 특징값을 바탕으로 사용자 영상에 포함된 사용자의 행동에 대한 장면 이해를 수행한다. 이후, 프로세서는 사용자가 제1 구간의 컨텐츠와 관련된 제1 행동을 완료하였는지 여부를 판단할 수 있다. 컨텐츠에 영상 데이터를 포함하는 경우, 프로세서는 다음과 같은 실시예를 통해 제1 구간의 영상을 이용하 여 제1 행동을 완료하였는지 여부를 판단할 수 있다. 일 실시예에 따라, 프로세서는 기획득한 제1 구간의 영상을 구성하는 영상 프레임의 특징값 및 사용자 영 상을 구성하는 영상 프레임의 특징값을 비교하여 사용자가 제1 행동을 완료하였는지 여부를 판단할 수 있다. 또다른 실시예에 따라, 프로세서는 제1 구간의 영상을 구성하는 영상 프레임의 특징값으로부터 수행된 제1 구간의 영상에 대한 장면 이해와, 사용자 영상을 구성하는 영상 프레임의 특징값으로부터 수행된 사용자의 행동 에 대한 장면 이해를 바탕으로 사용자가 제1 행동을 완료하였는지 여부를 판단할 수 있다. 본 개시의 추가적인 양상에 따라, 프로세서는 제1 구간의 영상에 포함된 제1 행동에 대한 장면 이해를 수 행하여 제1 문장을 획득하고, 사용자 영상에 포함된 사용자의 행동에 대한 장면 이행을 수행하여 제2 문장을 획 득한다. 구체적으로, 프로세서는 제1 구간의 영상을 구성하는 연속된 복수의 영상 프레임의 특징값에 기초하여 영 상 프레임 내 포함된 오브젝트를 판단하고, 판단된 오브젝트 간의 연관 관계로부터 제1 구간에 포함된 제1 행동 에 대한 장면 이해를 수행하여 제1 행동을 나타내는 제1 문장을 획득할 수 있다. 마찬가지로, 프로세서는 제1 구간의 영상이 재생되는 동안 촬영된 사용자 영상을 구성하는 연속된 복수의 영상 프레임의 특징값에 기초하여 영상 프레임 내 포함된 오브젝트를 판단하고, 판단된 오브젝트 간의 연관 관 계로부터 사용자의 행동에 대한 장면 이행을 수행하여 사용자의 행동을 나타내는 제2 문장을 획득할 수 있다. 이 같이 제1 구간에 포함된 제1 행동을 나타내는 제1 문장과, 사용자의 행동을 나타내는 제2 문장이 획득되면, 프로세서는 획득된 제1 및 제2 문장을 제1 구간의 영상이 디스플레이되는 일 영역에 표시되도록 출력부를 제어한다. 이에 따라, 출력부는 제1 구간의 영상이 디스플레이되는 화면의 일 영역에 제1 및 제2 문장을 표시한다. 따라서, 사용자는 전자 장치의 화면의 일 영역에 표시된 제1 및 제2 문장을 통해 자신이 제1 구간의 영상 에 포함된 제1 행동과 연관된 행동을 수행하는지 여부를 판단할 수 있다. 한편, 전술한 바와 같이, 프로세서에서 제1 구간의 영상에 포함된 제1 행동에 대한 장면 이해를 수행하여 제1 문장을 획득하고, 사용자 영상에 포함된 사용자의 행동에 대한 장면 이해를 수행하여 제2 문장을 수행하는 방법은 하기에서 상세히 설명하도록 한다. 본 개시의 추가적인 양상에 따라, 프로세서는 기획득한 제1 및 제2 문장 간의 유사도 정도에 기초하여 사 용자가 제1 행동을 수행하는지 여부를 판단할 수 있다. 구체적으로, 프로세서는 기획득한 제1 및 제2 문장 간의 유사도 값을 측정하고, 측정된 유사도 값과 기설 정된 임계값을 비교하여 기설정된 임계값 이상이면, 사용자가 제1 구간의 영상에 대한 제1 행동을 수행하는 것으로 판단할 수 있다. 한편, 프로세서는 측정된 유사도 값이 기설정된 임계값 미만이면, 사용자가 제1 구간의 영상에 대한 제1 행동을 수행하지 않는 것으로 판단할 수 있다. 이때, 프로세서는 유사도 값과 기설정된 임계값 간의 비교를 지속적으로 수행하여 기설정된 임계 시간 동 안 측정된 유사도 값이 기설정된 임계값 미만인 것으로 판단되면, 사용자가 제1 구간의 영상에 대한 제1 행동을 수행하지 않는 것으로 판단할 수 있다. 한편, 컨텐츠는 오디오 데이터를 포함할 수 있다. 이 경우, 프로세서는 전술한 바와 같이, 컨텐츠에 포함된 오디오 데이터와 관련된 문장으로부터 제1 행동 과 관련된 제1 구간의 문장을 획득할 수 있다. 이후, 프로세서는 제1 구간의 문장과, 제1 구간의 영상이 재생되는 동안 촬영된 사용자 영상으로부터 획득 한 사용자의 행동을 나타내는 제2 문장 간의 유사도 값과 기설정된 임계값을 비교하여 사용자가 제1 구간에 대 한 제1 행동을 수행하였는지 여부를 판단할 수 있다. 이 같은 실시예를 통해 제1 행동을 수행하지 않는 것으로 판단되면, 프로세서는 제1 행동과 상이한 행동을 수행하고 있음을 알리는 메시지를 생성하고, 생성된 메시지를 영상 및 오디오 중 적어도 하나를 통해 출력하도 록 출력부를 제어한다. 이에 따라, 사용자는 전자 장치를 통해 출력되는 메시지를 통해 자신이 제1 구간에 대한 제1 행동을 올바 르게 수행하지 않고 있음을 확인할 수 있다. 본 개시의 추가적인 양상에 따라, 프로세서는 사용자가 제1 구간에 대한 제1 행동을 완료하지 않은 것으로 판단되면, 기획득된 영상의 재생을 중지하거나 혹은 제1 구간의 영상을 반복 재생한다. 전술한 바와 같이, 프로세서는 제1 구간의 영상을 구성하는 영상 프레임의 특징값 및 사용자 영상을 구성 하는 영상 프레임의 특징값을 비교하여 사용자가 제1 구간에 대한 제1 행동을 완료했는지 여부를 판단할 수 있 다. 뿐만 아니라, 프로세서는 제1 구간에 대한 제1 행동과 관련된 장면 이해와 사용자의 행동에 대한 장면 이 해를 바탕으로 사용자가 제1 구간에 대한 제1 행동을 완료했는지 여부를 판단할 수 있다. 이 같은 실시예를 통해 사용자가 제1 구간에 대한 제1 행동을 완료하지 못한 것으로 판단되면, 프로세서는 영상을 재생을 중지하거나 혹은 제1 구간의 영상을 반복 재생하도록 출력부를 제어한다. 이에 따라, 출력 부는 영상의 재생을 중지하거나 혹은 제1 구간의 영상을 반복 재생할 수 있다. 영상의 재생을 중지하거나 혹은 제1 구간의 영상의 반복 재생하는 동안, 프로세서는 제1 행동과 상이한 행 동이 기설정된 임계 시간 이상 수행되면, 제2 구간의 영상을 재생하도록 출력부를 제어한다. 구체적으로, 프로세서는 영상의 재생을 중지하거나 혹은 제1 구간의 영상의 반복 재생하는 동안 획득된 사 용자 영상을 분석하여 사용자의 행동에 대한 장면 이해를 수행한다. 이후, 프로세서는 수행된 장면 이해 를 바탕으로 사용자가 제1 구간에 대한 제1 행동과 상이한 행동을 기설정된 임계 시간 이상 연속으로 수행하는 지 여부를 판단한다. 판단 결과, 사용자가 제1 구간에 대한 제1 행동과 상이한 행동을 기설정된 임계 시간 이상 연속으로 수행한 것으로 판단되면, 프로세서는 제2 구간의 영상을 재생하도록 출력부를 제어한다. 이에 따라, 출력부는 제1 구간의 영상 이후의 연속된 제2 구간의 영상을 재생할 수 있다. 한편, 전술한 바와 같이, 컨텐츠를 오디오 데이터를 포함할 수 있다. 이 경우, 프로세서는 사용자가 제1 구간에 대한 제1 행동을 완료하지 않은 것으로 판단되면, 컨텐츠에 포 함된 오디오 데이터와 관련된 문장의 재생을 중지하거나 혹은 제1 구간의 문장을 연속적으로 출력하도록 출력부 를 제어할 수 있다. 이후, 프로세서는 컨텐츠에 포함된 오디오 데이터와 관련된 문장의 재생을 중지하거나 혹은 제1 구간의 문 장을 연속적으로 출력하는 동안 전술한 바와 같은 실시예를 통해 제1 구간에 대한 제1 행동을 완료한 것으로 판 단되면, 제2 구간의 문장을 출력하도록 출력부를 제어할 수 있다. 한편, 전술한 메모리는 버퍼 및 메인 메모리를 포함할 수 있다. 따라서, 프로세서는 획득 한 컨텐츠를 버퍼에 저장하며, 버퍼의 저장 용량이 부족하면, 버퍼에 저장되지 않은 나머지 영 상을 메인 메모리에 저장한다. 구체적으로, 버퍼는 데이터를 임시 저장하며, 프로세서에서 버퍼에 임시 저장된 데이터를 이용 하는 휘발성 메모리가 될 수 있다. 이 같은 버퍼의 저장 용량은 전자 장치 내 구비된 메인 메모리의 저장 용량 사이즈 보다 작은 사이즈로 구현될 수 있다. 이 같은버퍼는 외부로부터 수신된 컨텐츠에 포함된 영상 데이터(이하 영상이라 함)를 구성하는 연속된 복수의 영상 프레임에 대해서 일정 단위로 임시 저장한다. 이후, 버퍼는 일정 단 위의 영상 프레임이 재생되면, 해당 영상 프레임이 재생되는 동안, 이후 연속되는 일정 단위의 영상 프레임을 임시 저장한다. 한편, 전술한 바와 같이, 사용자가 제1 구간에 대한 제1 행동을 완료하지 못하여 영상의 재생을 중지하거나 제1 구간의 영상을 반복 재생하는 이벤트가 발행하는 경우, 프로세서는 해당 이벤트가 발생하는 동안 영상을 저장하는 버퍼의 저장 가능한 용량을 체크한다. 체크 결과, 저장 가능한 용량이 없는 것으로 판단되면, 프로세서는 버퍼에 저장되지 않은 나머지 영상을 메인 메모리에 저장한다. 이후, 사용자가 제1 구간에 대한 제1 행동을 완료하여 제2 구간의 영상이 재생되면, 버퍼는 메인 메모리 에 저장된 나머지 영상을 임시 저장할 수 있다. 도 2는 본 개시의 일 실시예에 따른 전자 장치의 세부 블록도이다. 전술한 바와 같이, 전자 장치는 스마트 TV, 스마트 폰, 테블릿 PC와 같은 다양한 스마트 기기가 될 수 있 다. 이 같은 전자 장치는 전술한 촬영부, 메모리 및 출력부 구성 외에 입력부, 통신 부, 감지부를 더 포함할 수 있다. 메모리는 전술한 바와 같이, 버퍼 및 메인 메모리를 포함할 수 있다. 버퍼는 컨텐츠를 임시 저장한다. 그리고, 메인 메모리는 다양한 컨텐츠 및 전자 장치의 동작을 제어하기 위한 운용 프로그램을 저장할 수 있다. 이 같은 버퍼 및 메인 메모리를 포함하는 메모리 는 후술할 롬(ROM), 램(RAM) 또는 전자 장치에 탈착/장착 가능한 메모리 카드(예, SD 카 드, 메모리 스틱), 비휘발성 메모리, 휘발성 메모리, 하드 디스크 드라이브(HDD) 또는 솔리드 스테이트 드라이 브(SSD) 중 적어도 하나로 구현될 수 있다. 입력부는 사용자 명령을 입력받는다. 이를 위해, 입력부는 마이크, 조작부, 터치 입력부 및 사용자 입력부를 포함할 수 있다. 마이크는 사용자의 음성 명령을 입력받으며, 조작부는 각종 기능키, 숫자키, 특수키, 문자키 등을 구 비한 키패드(Key Pad)로 구현될 수 있다. 그리고, 터치 입력부는 후술할 디스플레이부가 터치 스크린 형태로 구현될 경우, 디스플레이부 와 상호 레어어 구조를 이루는 터치 패드로 구현될 수 있다. 이 경우, 터치 입력부는 디스플레이부를 통해 디스플레이된 다양한 어플리케이션 관련 아이콘에 대한 선택 명령을 입력받을 수 있다. 사용자 입력부는 원격 제어 장치와 같은 적어도 하나의 주변 기기(미도시)로부터 전자 장치의 동작을 제어하기 위한 IR 신호 혹은 RF 신호를 입력받을 수 있다. 통신부는 스마트 TV, 스마트 폰, 태블릿 PC 등의 주변 기기(미도시), 컨텐츠 서버(미도시) 등과 데이터 통 신을 수행한다. 특히, 통신부는 인공지능 모델이 별도의 인공지능 서버(미도시)에 저장된 경우, 전술한 바와 같은 영상 및 사용자 영상 각각에 대한 특징값을 인공지능 서버(미도시)로부터 수신할 수 있다. 이 같은 통신부는 근거리 통신 모듈, 무선 랜 모듈 등의 무선 통신 모듈과, HDMI(High- Definition Multimedia Interface), USB(Universal Serial Bus), IEEE(Institute of Electrical and Eletronics Engineers) 1394 등의 유선 통신 모듈 중 적어도 하나를 포함하는 커넥터를 포함할 수 있다. 근거리 통신 모듈은 전자 장치와 근거리에 위치한 주변 기기, 인공지능 서버(미도시) 등과 무선으로 근거리 통신을 수행하는 구성이다. 이 같은 근거리 통신 모듈은 블루투스(bluetooth)모듈, 적외선 통신 (IrDA, infrared data association)모듈, NFC(Near Field Communication)모듈, 와이파이(WIFI)모듈, 지그비 (Zigbee) 모듈 중 적어도 하나를 포함할 수 있다. 무선 통신 모듈은 IEEE 등과 같은 무선 통신 프로토콜에 따라 외부 네트워크에 연결되어 통신을 수행하는 모듈이다. 이 밖에 무선 통신 모듈은 3G(3rd Generation), 3GPP(3rd Generation Partnership Project), LTE(Long Term Evoloution) 등과 같은 다양한 이동 통신 규격에 따라 이동 통신 망에 접속하여 통신을 수행하는 이동 통신 모듈을 더 포함할 수도 있다. 이처럼 통신부는 상술한 다양한 근거리 통신 방식에 의해 구현될 수 있고, 필요에 따라 본 명세서에 언급 되지 않은 다른 통신 기술을 채용할 수 있다. 한편, 커넥터는 USB 2.0, USB 3.0, HDMI, IEEE 1394 등 다양한 소스 장치와의 인터페이스를 제공하는 구 성이다. 이 같은 커넥터는 프로세서의 제어 명령에 따라 커넥터에 연결된 유선 케이블을 통해 컨텐츠 서버(미도시)로부터 전송된 영상 관련 데이터를 수신하거나, 기저장된 영상을 외부 기록 매체로 전송할 수 있다. 또한, 커넥터는 커넥터와 물리적으로 연결된 유선 케이블을 통해 전원 소스로부터 전원을 입력받을 수 있다. 감지부는 전자 장치의 모션을 감지한다. 이 같은 감지부는 가속도 센서, 지자기 센서 및 자이로 센서 등을 포함할 수 있으며, 이 같은 다양한 센서를 이용하여 전자 장치의 모션을 감지할 수 있다. 가속도 센서(Accelerometer Sensor)는 이동하는 전자 장치의 가속도나 충격의 세기를 측정하는 센서로써, 스마트 폰, 테블릿 PC와 같은 전자 장치 뿐만 아니라, 자동차, 기차, 비행기 등과 같은 각종 운송 수단 및 로봇 등의 제어 시스템에 이용되는 필수적인 센서이다. 지자기 센서(Magnetic Sensor)는 지구 자기장을 이용하여 방위각을 탐지할 수 있는 전자 나침판으로써, 위치 추 적, 3D 영상 게임 등에 사용되거나, 스마트 폰, 무전기, GPS, PDA, 네비게이션 항법 장치 등에 사용되는 센서이 다. 자이로 센서(Gyroscope Sensor)는 기존의 가속도 센서에 각각 회전을 넣어 6축 방향을 인식하여 하여 좀더 세밀 하고 정밀한 동작을 인식할 수 있도록 도와주는 센서이다. 한편, 전술한 출력부는 디스플레이부 및 오디오 출력부를 포함할 수 있다. 디스플레이부는 영상 처리부(미도시)에서 신호 처리된 영상 데이터를 출력한다. 또한, 디스플레이부 는 메모리에 저장된 복수의 어플리케이션 각각을 실행하기 위한 아이콘을 포함하는 실행 화면을 디스플레 이하거나 혹은 전자 장치의 동작을 제어하기 위한 다양한 UI 화면을 디스플레이할 수 있다. 이 같은 디스플레이부는 액정 표시 장치(Liquid Crystal Display, LCD), 유기 전기 발광 다이오드 (Organic Light Emitting Display, OLED) 등으로 구현될 수 있다. 또한, 디스플레이부는 플렉서블 디스플레이(flexible display)의 형태로 전자 장치의 전면 영역 및, 측면 영역 및 후면 영역 중 적어도 하나에 결합될 수도 있다. 플렉서블 디스플레이는 종이처럼 얇고 유연한 기판을 통해 손상 없이 휘거나 구부리거나 말 수 있는 것을 특징 으로 할 수 있다. 이러한 플렉서블 디스플레이는 일반적으로 사용되는 유리 기판뿐 아니라 플라스틱 기판을 사 용하여 제조될 수도 있다. 플라스틱 기판을 사용하는 경우, 기판의 손상을 방지하기 위해서 기존의 제조 프로세 서를 사용하지 않고 저온 제조 프로세서를 사용하여 형성될 수 있다. 또한, 플렉서블 액정을 싸고 있는 유리 기판을 플라스틱 필름으로 대체하여, 접고 펼 수 있는 유연성을 부여할 수 있다. 이러한 플렉서블 디스플레이는 얇고 가벼울 뿐만 아니라 충격에도 강하며, 또한 휘거나 굽힐 수 있고 다양한 형태로 제작이 가능하다는 장점을 갖고 있다. 오디오 출력부는 영상 관련 오디오 데이터를 출력한다. 구체적으로, 오디오 출력부는 오디오 처리부 (미도시)에 의해 디코딩이나 증폭, 노이즈 필터링과 같은 다양한 처리 작업이 수행된 각종 오디오 데이터뿐만 아니라 각종 알림 음이나 음성 메시지를 출력하는 구성이다. 특히, 오디오 출력부는 스피커로 구현될 수 있으나, 이는 일 실시 예에 불과할 뿐, 오디오 데이터를 출력할 수 있는 출력 단자로 구현될 수 있다. 한편, 전술한 프로세서는 전자 장치의 동작을 전반적으로 제어하거나, 혹은 전자 장치의 전반적 인 동작을 제어할 수 있도록 하는 처리 장치가 될 수 있다. 이 같은 프로세서는 CPU, ROM, RAM 및 GPU를 포함할 수 있으며, CPU, ROM, RAM 및 GPU는 버스를 통해 서로 연결될 수 있다. CPU는 메모리를 액세스하여, 메모리에 저장된 OS를 이용하여 부팅을 수행한다. 또한 CPU는 메모리에 저장된 각종 프로그램, 컨텐츠, 데이터 등을 이용하여 다양한 동작을 수행한다. GPU는 아이콘, 이미지, 텍스트 등과 같은 다양한 객체를 포함하는 디스플레이 화면을 생성한다. 구체적으 로, GPU는 수신된 제어 명령에 기초하여 화면의 레이아웃에 따라 각 객체들이 표시될 좌표값, 형태, 크기, 컬러 등과 같은 속성값을 연산하고, 연상된 속성값에 기초하여 객체를 포함하는 다양한 레이아웃의 디스플레이 화면을 생성한다. ROM은 시스템 부팅을 위한 명령어 세트 등이 저장된다. 턴 온 명령이 입력되어 전원이 공급되면, CPU(14 1)는 ROM에 저장된 명령어에 따라 메모리에 저장된 OS를 RAM에 복사하고, OS를 실행시켜 시스템 을 부팅시킨다. 부팅이 완료되면, CPU는 메모리에 저장된 각종 프로그램을 RAM에 복사하고, RAM에 복사된 프로그램을 실행시켜 각종 동작을 수행한다. 이 같은 프로세서는 전술한 각 구성들과 결합되어 단일칩 시스템(System-on-a-chip 또는 System on chip, SOC, SoC)으로 구현될 수 있다. 도 3은 본 개시의 일 실시예에 따른 사용자 행동에 따라 컨텐츠에 포함된 영상을 자동 재생하기 위한 각종 모듈 을 저장하는 메모리의 블록도이다. 도 3에 도시된 바와 같이, 메모리는 특징값 획득 모듈, 장면 이해 모듈, 구간 분석 모듈, 문장 생성 모듈 및 음성 인식 모듈를 포함할 수 있다. 특징값 획득 모듈은 학습된 인공지능 모델에 영상을 구성하는 복수의 영상 프레임 중 메모리에 임시 저장된 영상 프레임을 입력하여 입력된 영상 프레임의 특징값을 획득하는 모듈이다. 이 같은 특징값 획득 모듈을 통해 영상 프레임별 특징값이 획득되면, 획득된 특징값에 기초하여 인공지능 모듈에 입력된 영상 프레임 내 포함된 오브젝트를 판단할 수 있다. 장면 이해 모듈은 획득된 특징값에 기초하여 각 행동과 관련된 장면 이해를 수행한다. 구체적으로, 장면 이해 모듈는 획득된 특징값에 기초하여 각각의 영상 프레임에 대한 이미지 정보를 획득한다. 여기서, 이 미지 정보는 오브젝트 정보, 색상 정보, 오브젝트에 대한 위치 정보 등을 포함할 수 있다. 또한, 장면 이해 모 듈은 획득된 특징값에 기초하여 각각의 영상 프레임에 대한 이미지 정보에 포함된 오브젝트 정보 중 기정 의된 특정 오브젝트에 대한 움직임 정보를 획득한다. 여기서, 기정의된 특정 오브젝트는 각 영상 프레임 내 포 함된 오브젝트 중 대표하는 오브젝트가 될 수 있다. 이후, 장면 이해 모듈은 특징값에 기초하여 획득된 이미지 정보 및 특정 오브젝트에 대한 움직임 정보를 바탕으로 각 행동과 관련된 장면 이해를 수행한다. 이후, 장면 이해 모듈은 각 행동과 관련하여 수행된 장면 이해를 바탕으로 각 행동과 관련된 문장을 생성 한다. 실시예에 따라, 장면 이해 모듈은 LSTM(Long Short-term Memory) 방식의 RNN(Recurrent Neural Network) 모델을 이용하여 각 행동과 관련된 문장을 생성할 수 있다. 여기서, LSTM(Long Short-term Memory) 방식의 RNN(Recurrent Neural Network) 모델은 장기 저장된 데이터와 단기 저장된 데이터 각각을 입력하여 그에 따른 결과값을 출력하는 모델이다. 따라서, 장면 이해 모듈은 LSTM(Long Short-term Memory) 방식의 RNN(Recurrent Neural Network) 모델에 복수의 영상 프레임 각각으로부터 획득된 이미지 정보 및 특정 오브젝트에 대한 움직임 정보를 입력하여 각 행 동과 관련된 문장을 생성할 수 있다. 구간 분석 모듈은 장면 이해 모듈을 통해 수행된 장면 이해를 바탕으로 각 행동과 관련된 구간을 판 단한다. 예를 들어, A 요리를 하는 영상과 관련하여 해당 영상을 구성하는 복수의 영상 프레임으로부터 특징값이 획득되 면, 장면 이해 모듈은 획득된 특징값에 기초하여 A 요리를 위한 요리 단계(행동)별 장면 이해를 수행한다. 이후, 구간 분석 모듈은 장면 이해 모듈을 통해 수행된 A 요리를 위한 요리 단계(행동)별 장면 이해 를 바탕으로 요리 단계별 연관된 영상 프레임을 동일 구간의 영상으로 분류한다. 음성 인식 모듈은 입력부를 통해 사용자 음성이 입력되면, 입력된 사용자 음성을 전자 장치에서 인식할 수 있는 언어로 변환한다. 따라서, 음성 인식 모듈은 사용자의 음성이 입력되면, STT(Speech to Text) 알고리즘을 이용하여 입력된 사용자 음성을 텍스트로 변환하고, 텍스트로 변환된 사용자 음성을 분석하여 사용자의 발화 의도를 파악할 수 있다. 이 같은 음성 인식 모듈을 통해 사용자 음성에 대한 텍스트가 획득되면, 장면 이해 모듈은 전술한 바 와 같이, 복수의 영상 프레임 각각으로부터 획득된 이미지 정보 및 특정 오브젝트에 대한 움직임 정보와, 음성 인식 모듈을 통해 사용자 음성에 대한 텍스트를 입력하여 각 행동과 관련된 문장을 생성할 수 있다. 이하에서는, 본 개시에 따른 사용자 행동에 기초하여 영상을 자동 재생하기 위한 전자 장치의 제어 방법에 대해서 상세히 설명하도록 한다. 도 4는 본 개시의 일 실시예에 따른 전자 장치에서 사용자 행동에 기초하여 컨텐츠를 자동 출력하기 위한 방법 의 흐름도이다. 도 4에 도시된 바와 같이, 전자 장치는 외부로부터 수신된 컨텐츠가 입력되면, 입력된 컨텐츠를 획득하여 메모리에 임시 저장한다(S410). 여기서, 컨텐츠는 영상 데이터, 오디오 데이터 및 텍스트 중 적어도 하나를 포함할 수 있다. 이후, 전자 장치는 저장된 컨텐츠를 분석하여 제1 행동과 관련된 제1 구간을 판단하고, 판단된 제1 구간의 컨텐츠를 출력한다(S420,S430). 이후, 전자 장치는 제1 구간의 컨텐츠가 출력되는 동안 촬영된 사용자 영 상을 획득한다(S440). 그러나, 본 개시는 이에 한정되지 않으며, 전자 장치는 제1 구간의 컨텐츠가 출력되는 동안 입력부를 통해 사용자 음성을 입력받을 수 있으며, 전술한 실시예를 통해 입력된 사용자 음성으로부터 변환된 텍스트를 획득할 수 있다. 한편, 제1 구간의 컨텐츠는 최초 시작된 구간의 컨텐츠가 될 수 있으며, 이 같은 제1 구간의 컨텐츠가 출력되면, 전자 장치는 다음과 같은 실시예를 통해 촬영된 사용자 영상을 획득할 수 있다. 일 실시예에 따라, 전자 장치는 제1 구간의 컨텐츠의 출력이 개시되면, 사용자를 포함하는 주변 환경을 촬 영하도록 카메라(미도시)를 활성화시키고, 활성화된 카메라(미도시)를 통해 사용자를 포함하는 사용자 영상을 획득할 수 있다. 또다른 실시예에 따라, 전자 장치는 제1 구간의 컨텐츠 출력이 개시되고, 감지부를 통해 기정의된 사 용자 모션이 감지되면, 사용자를 포함하는 주변 환경을 촬영하도록 카메라(미도시)를 활성화시키고, 활성화된 카메라(미도시)를 통해 사용자를 포함하는 사용자 영상을 획득할 수 있다. 이 같은 실시예를 통해 제1 구간의 컨텐츠가 출력되는 동안 촬영된 사용자 영상이 획득되면, 전자 장치는 획득된 사용자 영상을 분석하여 사용자가 제1 구간의 컨텐츠와 관련된 제1 행동을 완료하였는지 여부를 판단한 다(S450). 판단 결과, 사용자가 제1 구간의 컨텐츠와 관련된 제1 행동을 완료하지 못한 것으로 판단되면, 전자 장치 는 컨텐츠의 출력을 중지하거나 혹은 사용자가 제1 행동을 완료할 때까지 제1 구간의 컨텐츠를 반복 출력한다(S460). 한편, 사용자가 제1 구간의 컨텐츠와 관련된 제1 행동을 완료한 것으로 판단되면, 전자 장치는 제1 구간 다음의 제2 구간의 컨텐츠를 출력한다(S470). 이후, 전자 장치는 전술한 단계 S440 내지 S450 단계를 수행하여 제2 구간의 컨텐츠가 출력되는 동안 촬영 된 사용자 영상을 분석하여 사용자가 제2 구간의 컨텐츠와 관련된 제2 행동을 완료하였는지 여부를 판단한다. 판단 결과, 사용자가 제2 구간의 컨텐츠와 관련된 제2 행동을 완료하지 못한 것으로 판단되면, 전자 장치 는 전술한 단계 S460을 통해 컨텐츠의 재생을 중지하거나 혹은 사용자가 제1 행동을 완료할 때까지 제2 구간의 컨텐츠를 반복 출력한다. 한편, 사용자가 제2 구간의 컨텐츠와 관련된 제2 행동을 완료한 것으로 판단되면, 전자 장치는 제2 구간 이후의 구간 컨텐츠가 존재하는지 여부를 판단하여 제2 구간 이후의 구간 컨텐츠가 존재하면, 전술한 단계 S440 내지 S470의 동작을 반복하여 수행하고, 제2 구간 이후의 구간 컨텐츠가 존재하지 않으면, 사용자 행동에 따른 컨텐츠 자동 출력에 대한 동작을 종료한다. 이하에서는, 전자 장치에서 컨텐츠에 포함된 영상 데이터로부터 행동별 구간 영상을 판단하는 방법 및 촬 영된 사용자 영상을 획득하는 방법에 대해서 상세히 설명하도록 한다. 도 5는 본 개시의 일 실시예에 따른 전자 장치에서 컨텐츠에 포함된 영상으로부터 행동별 구간 영상을 판단하는 흐름도이다. 외부로부터 수신된 컨텐츠는 영상 데이터를 포함할 수 있다. 이 경우, 전자 장치는 외부로부터 수신된 컨 텐츠에 포함된 영상 데이터(이하 영상이라 함)를 구성하는 복수의 영상 프레임을 메모리에 임시 저장한다. 여기서, 메모리에 임시 저장된 복수의 영상 프레임은 영상을 구성하는 전체 영상 프레임 중 시간적으로 연 속된 일부 영상 프레임이 될 수 있다. 이 같은 연속된 영상 프레임이 메모리에 저장되면, 전자 장치는 도 5에 도시된 바와 같이, 메모리 에 임시 저장된 복수의 영상 프레임을 인공지능 모델에 입력하여, 복수의 영상 프레임 각각에 대한 특징값 을 획득한다(S421). 여기서, 복수의 영상 프레임 각각에 대한 특징값은 복수의 영상 프레임 각각에 포함된 오브젝트를 판단하기 위 한 정보가 될 수 있다. 이 같은 복수의 영상 프레임 각각에 대한 특징값이 획득되면, 전자 장치는 획득된 각각의 특징값에 기초하 여 제1 행동과 관련된 장면 이해를 수행한다(S422). 이후, 전자 장치는 제1 행동과 관련하여 수행된 장면 이해를 바탕으로 제1 행동과 관련된 제1 구간을 판단 한다(S423). 구체적으로, 전자 장치는 전술한 바와 같은 장면 이해 모듈을 이용하여 복수의 영상 프레임 각각에 대한 특징값에 기초하여 획득된 복수의 영상 프레임 각각에 대한 이미지 정보 및 특정 오브젝트에 대한 움직임 정보를 바탕으로 복수의 영상 프레임으로부터 제1 행동과 관련된 장면 이해를 수행할 수 있다. 이후, 전자 장치는 제1 행동과 관련하여 수행된 장면 이해를 바탕으로 복수의 영상 프레임 중 제1 행동과 관련된 영상 프레임을 제1 구간의 영상 프레임으로 판단할 수 있다. 도 6은 본 개시의 일 실시예에 따른 전자 장치에서 촬영된 사용자 영상을 획득하는 방법의 흐름도이다. 도 6에 도시된 바와 같이, 전자 장치는 제1 구간의 영상이 재생되는 동안 카메라(미도시)를 통해 촬영된 사용자 영상이 획득되면, 획득된 사용자 영상을 구성하는 복수의 영상 프레임을 학습된 인공지능 모델에 입력하 여, 인공지능 모델에 입력된 복수의 영상 프레임의 특징값을 획득한다(S441). 여기서, 복수의 영상 프레임 각각에 대한 특징값은 복수의 영상 프레임 각각에 포함된 오브젝트를 판단하기 위 한 정보가 될 수 있다. 이 같은 복수의 영상 프레임의 특징값이 획득되면, 전자 장치는 획득된 각각의 특징값에 기초하여 기획득 된 사용자 영상으로부터 사용자의 행동에 대한 장면 이해를 수행한다(S422). 전술한 바와 같이, 전자 장치는 제1 행동과 관련된 제1 구간의 영상이 재생되는 동안 카메라(미도시)를 활 성화시키고, 활성화된 카메라(미도시)를 통해 촬영된 사용자 영상을 획득할 수 있다. 그러나, 본 개시는 이에 한정되지 않으며, 전자 장치는 감지부를 통해 기정의된 사용자 모션이 감지 되면, 카메라(미도시)를 활성화시키고, 활성화된 카메라(미도시)를 통해 촬영된 사용자 영상을 획득할 수 있다. 예를 들어, 제1 행동과 관련된 제1 구간의 영상이 전체 영상으로부터 구분된 복수의 구간 영상 중 시작 구간의 영상이면, 전자 장치는 감지부를 통해 기정의된 사용자 모션이 감지되면, 카메라(미도시)를 활성화시 키고, 활성화된 카메라(미도시)를 통해 촬영된 사용자 영상을 획득할 수 있다. 또다른 예를 들어, 전자 장치는 감지부를 통해 기정의된 사용자 모션이 감지되면, 각 구간의 영상을 재생한다. 즉, 전자 장치는 기정의된 사용자 모션이 감지되면, 복수의 구간 중 시작 구간인 제1 구간의 영상을 재생하고, 제1 구간의 영상의 재생이 종료되면, 기정의된 사용자 모션이 감지되는지 여부에 따라 제1 구 간 다음의 제2 구간의 영상을 재생할 수 있다. 한편, 전자 장치는 각 구간의 영상이 재생되는 동안, 카메라(미도시)를 활성화시키고, 활성화된 카메라(미 도시)를 통해 촬영된 사용자 영상을 획득할 수 있다. 이 같은 실시예를 통해 제1 구간의 영상이 재생되는 동안 카메라(미도시)를 통해 촬영된 사용자 영상이 획득되 면, 전자 장치는 획득된 사용자 영상을 구성하는 복수의 영상 프레임을 인공지능 모델에 입력하여 복수의 영상 프레임을 구성하는 오브젝트에 대한 특징값을 획득할 수 있다. 사용자 영상과 관련하여 복수의 영상 프레임을 구성하는 오브젝트에 대한 특징값을 획득되면, 전자 장치는 전술한 바와 같은 장면 이해 모듈을 이용하여 복수의 영상 프레임에 대한 특징값에 기초하여 획득된 복수 의 영상 프레임 각각에 대한 이미지 정보 및 특정 오브젝트에 대한 움직임 정보를 바탕으로 사용자 영상으로부 터 사용자의 행동에 대한 장면 이해를 수행할 수 있다. 한편, 전자 장치는 제1 행동과 관련된 제1 구간의 영상의 재생이 종료되면, 제1 구간의 영상의 재생이 재 생되는 동안 촬영된 사용자 영상을 분석하여 사용자가 제1 구간에 대한 제1 행동을 완료했는지 여부를 판단할 수 있다. 일 실시예에 따라, 전자 장치는 재생 중인 제1 행동과 관련된 제1 구간의 영상이 종료 시점의 영상인 것으 로 판단되면, 제1 구간의 영상을 구성하는 복수의 영상 프레임 중 종료 시점의 영상을 구성하는 영상 프레임의 특징값(이하 제1 특징값이라 함)과, 재생 중인 제1 구간의 영상이 종료되는 시점 전후에 촬영된 사용자 영상을 구성하는 영상 프레임의 특징값(이하 제2 특징값)을 획득한다. 이후, 전자 장치는 획득된 제1 및 제2 특 징값을 비교하여 사용자가 제1 구간의 영상과 관련된 제1 행동을 완료하였는지 여부를 판단할 수 있다. 또다른 실시예에 따라, 전자 장치는 재생 중인 제1 행동과 관련된 제1 구간의 영상이 종료 시점의 영상인 것으로 판단되면, 제1 구간의 영상을 구성하는 복수의 영상 프레임 중 종료 시점의 영상을 구성하는 영상 프레 임의 특징값을 이용하여 수행된 장면 이해와, 재생 중인 제1 구간의 영상이 종료되는 시점 전후에 촬영된 사용 자 영상을 구성하는 영상 프레임의 특징값을 이용하여 수행된 장면 이해를 바탕으로 사용자가 제1 구간의 영상 과 관련된 제1 행동을 완료하였는지 여부를 판단할 수 있다. 이 같은 실시예를 통해 사용자가 제1 구간의 영상과 관련된 제1 행동을 완료한 것으로 판단되면, 전자 장치 는 제1 구간 다음의 제2 구간의 영상을 재생할 수 있다. 제2 구간의 영상이 재생되면, 전자 장치는 전술한 바와 같은 동작을 수행하여 제2 간의 영상이 재생되는 동안 촬영된 사용자 영상으로부터 사용자가 제2 구간의 영상과 관련된 제2 행동을 수행하였는지 여부를 판단할 수 있다. 한편, 전자 장치는 제1 구간의 영상과 관련된 제1 행동을 완료하지 않은 것으로 판단되면, 영상의 재생을 중지하거나 혹은 사용자가 제1 행동을 완료할 때까지 제1 구간의 영상을 반복 재생할 수 있다. 한편, 전자 장치는 제1 행동과 관련된 제1 구간의 영상이 재생되는 동안, 제1 구간의 영상이 재생된 동안 촬영된 사용자 영상을 분석하여 사용자가 제1 구간에 대한 제1 행동을 정상적으로 수행하고 있는지 여부를 판단 할 수 있다. 실시예에 따라, 전자 장치는 제1 행동과 관련된 제1 구간의 영상이 재생되는 동안, 제1 구간의 영상을 구 성하는 복수의 영상 프레임으로부터 획득된 특징값을 이용하여 수행된 장면 이해를 바탕으로 제1 행동과 관련된 문장을 생성한다.그리고, 전자 장치는 제1 구간의 영상이 재생되는 동안 촬영된 사용자 영상을 구성하는 복수의 영상 프레 임으로부터 획득된 특징값을 이용하여 수행된 장면 이해를 바탕으로 사용자의 행동과 관련된 문장을 생성한다. 이후, 전자 장치는 생성된 제1 구간의 영상에 대한 제1 행동과 관련된 문장과 사용자의 행동과 관련된 문 장의 유사도를 비교한다. 두 문장의 유사도가 기설정된 임계값 이상이면, 전자 장치는 사용자가 제1 구간 과 관련된 제1 행동을 수행하고 있는 것으로 판단한다. 한편, 두 문장의 유사도가 기설정된 임계값 미만이면, 전자 장치는 사용자가 제1 구간과 관련된 제1 행동 을 수행하지 않는 것으로 판단한다. 제1 행동을 수행하지 않는 것으로 판단되면, 전자 장치는 제1 행동과 상이한 행동을 수행하고 있음을 알리 는 메시지를 영상 및 오디오 중 적어도 하나를 통해 출력할 수 있다. 지금까지, 본 개시에 따른 전자 장치에서 사용자 행동에 따라 영상을 자동 재생하는 방법에 대해서 상세히 설명하였다. 이하에서는, 본 개시에 따른 전자 장치에서 사용자가 영상으로부터 구분된 구간 영상의 행동 을 수행하는지 여부를 판단하는 동작에 대해서 보다 구체적으로 설명하도록 한다. 도 7은 본 개시의 일 실시예에 따른 전자 장치에서 사용자 행동이 구간 영상의 행동인지 여부를 판단하는 예시 도이다. 도 7에 도시된 바와 같이, 전자 장치는 영상을 제공하는 컨텐츠 서버(미도시)를 통해 사용자가 요청한 스 트리밍 영상을 수신할 수 있다. 예를 들어, 전자 장치는 사용자 요청에 따라, 중국 요리 관련 스트 리밍 영상을 수신할 수 있다. 이 같은 영상이 수신되면, 전자 장치는 수신된 영상을 구성하는 복수의 영상 프레임을 순차적으 로 메모리에 임시 저장할 수 있다. 영상을 구성하는 복수의 영상 프레임이 메모리에 임시 저장되면, 전자 장치는 메모리에 임시 저 장된 복수의 영상 프레임을 분석하여 사용자가 요청한 중국 요리의 조리 단계(행동)별 구간을 판단한다. 구체적으로, 전자 장치는 메모리에 임시 저장된 복수의 영상 프레임을 학습된 인공지능 모델에 입력 하여, 입력된 복수의 영상 프레임의 특징값을 획득한다. 이후, 전자 장치100)는 복수의 영상 프레임의 특 징값을 바탕으로 사용자가 요청한 중국 요리의 조리 단계(행동)별 대한 장면 이해를 수행한다. 이후, 전자 장치는 사용자가 요청한 중국 요리의 조리 단계(행동)별 수행된 장면 이해를 바탕으로 각 조리 단계(행동)별 영상 재생을 위한 구간을 판단한다. 조리 단계(행동)별 영상 재생을 위한 구간이 판단되면, 전자 장치는 사용자가 요청한 중국 요리의 시 작 구간인 제1 구간(722-1)을 선택하고, 선택된 제1 구간의 영상(722-1')을 재생한다. 한편, 전자 장치는 제1 구간의 영상(722-1')이 재생이 개시되면, 카메라(미도시)를 통해 카메라(미도시) 주위에 있는 사용자를 촬영하여 사용자 영상을 획득한다. 제1 구간의 영상(722-1')이 재생되는 동안 카메 라(미도시)를 통해 촬영된 사용자 영상이 획득되면, 전자 장치는 획득된 사용자 영상을 구성하 는 영상 프레임을 학습된 인공지능 모델에 입력하여, 입력된 영상 프레임의 특징값의 특징값을 획득한다. 이후, 전자 장치는 제1 구간의 영상(722-1')과 관련하여 획득된 특징값(721-1)과, 제1 구간의 영상(722- 1')이 재생되는 동안 촬영된 사용자 영상과 관련하여 획득된 특징값을 비교한다. 이후, 전자 장치는 두 특징값(721-1,731)의 유사도 정도에 따라 사용자가 제1 구간의 영상(722-1')과 관련 된 재료 손질 단계(행동)을 수행했는지 여부를 판단한다. 구체적으로, 제1 구간의 영상(722-1')은 요리 재료를 준비 및 손질하는 재료 손질 단계(행동)와 관련된 영 상일 수 있다. 따라서, 전자 장치는 두 특징값(721-1,731)을 비교하여 사용자가 요리 재료를 준비 및 손질하는 재료 손질 단계(행동)를 수행했는지 여부를 판단할 수 있다. 판단 결과, 두 특징값(721-1,731)이 유사하면, 전자 장치는 사용자가 요리 재료를 준비 및 손질하는 재료 손질 단계(행동)를 수행한 것으로 판단하고, 제1 구간의 영상(722-1') 다음의 제2 구간의 영상을 재생할 수 있다.여기서, 제2 구간의 영상은 요리 재료를 준비 및 손질하는 재료 손질 단계(행동) 다음의 조리 단계(행동)와 관 련된 영상이다. 한편, 두 특징값(721-1,731)이 유사하지 않으면, 전자 장치는 사용자가 요리 재료를 준비하는 손질 단계 (행동)를 완료하지 않은 것으로 판단하고, 사용자가 요리 재료를 준비 및 손질하는 재료 손질 단계(행 동)를 완료할 때까지 중국 요리 관련 영상의 재생을 중지하거나 혹은 제1 구간의 영상(722-1')을 반복 재 생할 수 있다. 도 8a은 본 개시의 일 실시예에 따른 전자 장치에서 사용자의 행동에 따라 영상을 자동 재생하는 제1 예시도, 도 8b는 본 개시의 일 실시예에 따른 전자 장치에서 사용자의 행동에 따라 영상을 자동 재생하는 제2 예시도, 도 8c는 본 개시의 일 실시예에 따른 전자 장치에서 사용자의 행동에 따라 영상을 자동 재생하는 제3 예시도, 도 8d는 본 개시의 일 실시예에 따른 전자 장치에서 사용자의 행동에 따라 영상을 자동 재생하는 제4 예시도이 다. 전자 장치는 컨텐츠 서버(미도시)로부터 사용자가 요청한 요리 관련 영상이 수신되면, 수신된 요리 관련 영상을 분석하여 각 행동별 관련 구간을 판단한다. 이후, 전자 장치는 도 8a에 도시된 바와 같이, 기판단된 행동별 구간 중 최초 시작 구간인 제1 행동과 관 련된 제1 구간의 영상을 재생한다. 예를 들어, 제1 행동이 사용자가 요청한 요리를 위한 재료 손질 관련 행동이면, 전자 장치는 영상 중 재료 손질 관련 행동과 관련된 제1 구간의 영상를 화면상에 디스플레이할 수 있다. 한편, 전자 장치는 제1 구간의 영상의 재생이 개시되면, 카메라(미도시)를 통해 카메라(미도시) 주변 에 위치한 사용자를 포함하는 사용자 영상을 촬영한다. 이후, 전자 장치는 제1 구간의 영상을 화면의 제1 영역에 디스플레이하고, 제1 구간의 영상이 디스플레이되는 동안 촬영된 사용자 영상을 제1 구간의 영상이 디스플레이된 제1 영역과 상이한 제2 영역에 디스플레이한다. 이에 따라, 사용자는 화면의 제2 영역에 디스플레이된 사용자 영상을 모니터링하여 자신이 제1 구간의 영 상과 관련된 재료 손질 관련 행동을 올바르게 수행하고 있는지 여부를 판단할 수 있다. 또한, 전자 장치는 전술한 바와 같이, 제1 구간의 영상을 화면의 제1 영역에 디스플레이하고, 제1 구 간의 영상이 디스플레이되는 동안 촬영된 사용자 영상을 제1 구간의 영상이 디스플레이된 제1 영역과 상이한 제2 영역에 디스플레이한다. 이때, 전자 장치는 제1 구간의 영상 및 사용자 영상 각각에 대응하여 생성된 문장을 제1 구간의 영상 및 사용자 영상 각각이 디스플레이된 제1 및 제2 영역과 상이한 제3 영역에 디스플레이 할 수 있다. 구체적으로, 전자 장치는 전술한 바와 같이, 제1 구간의 영상과 관련된 재료 손질 관련 행동에 대한 장면 이해를 수행한다. 이후, 전자 장치는 수행된 장면 이해를 바탕으로 재료 손질 관련 행동과 관련된 제1 문장을 생성하고, 생성된 제1 문장을 화면의 제3 영역에 디스플레이한다. 또한, 전자 장치는 전술한 바와 같이, 제1 구간의 영상이 디스플레이되는 동안 촬영된 사용자 영상 과 관련된 사용자의 행동에 대한 장면 이해를 수행한다. 이후, 전자 장치는 수행된 장면 이해를 바 탕으로 사용자의 행동과 관련된 제2 문장를 생성하고, 생성된 제2 문장을 화면의 제3 영역에 디 스플레이한다. 이에 따라, 사용자는 화면의 제2 영역에 디스플레이된 사용자 영상을 모니터링하거나, 화면의 제3 영역에 디스플레이된 문장을 통해 자신이 제1 구간의 영상과 관련된 재료 손질 관련 행동을 올바르게 수행하 고 있는지 여부를 판단할 수 있다. 한편, 전자 장치는 제1 구간의 영상이 종료되면, 제1 구간의 영상이 종료되는 시점에 촬영된 사 용자 영상을 분석하여 사용자가 재료 손질 관련 행동을 완료했는지 여부를 판단한다. 판단 결과, 사용자가 재료 손질 관련 행동을 완료하지 못한 것으로 판단되면, 전자 장치는 도 8b의 (a)에 도시된 바와 같이, 사용자가 요청한 요리 관련 영상의 재생을 중지한다.즉, 전자 장치는 사용자가 재료 손질 관련 행동을 완료하지 못한 것으로 판단되면, 제1 구간의 영상 이 종료되는 시점의 영상(810-1)을 화면의 제1 영역에 디스플레이하고, 제1 구간의 영상이 종료되는 시점 의 영상(810-1)이 디스플레이되는 동안 촬영된 사용자 영상(820-1)을 화면의 제2 영역에 디스플레이할 수 있다. 또한, 사용자가 재료 손질 관련 행동을 완료하지 못한 것으로 판단되면, 전자 장치는 도 8b의 (b)에 도시 된 바와 같이, 제1 구간의 영상을 반복 재생한다. 즉, 전자 장치는 사용자가 재료 손질 관련 행동을 완료하지 못한 것으로 판단되면, 제1 구간의 영상 을 제1 영역 상에서 반복해서 디스플레이하고, 제1 구간의 영상이 제1 영역 상에서 반복해서 디스플레이되 는 동안 촬영된 사용자 영상(820-1)을 화면의 제2 영역에 디스플레이할 수 있다. 한편, 도 8d의 (a)에 도시된 바와 같이, 전자 장치는 화면의 제1 영역에 제1 구간의 영상이 종료되는 시점의 영상(810-1)을 디스플레이하고, 화면의 제2 영역에 제1 구간의 영상이 종료되는 시점의 영상(810- 1)이 디스플레이되는 동안 촬영된 사용자 영상(820-2)을 디스플레이할 수 있다. 여기서, 제2 영역에 디스플레이된 사용자 영상(820-2)은 제1 구간의 영상이 종료되는 시점(810-1)의 영상 과 유사한 특징값을 가지는 영상이 될 수 있다. 이 경우, 전자 장치는 사용자의 행동이 재료 손질 관련 행동을 완료한 것으로 판단한다. 이후, 전자 장치 는 도 8d의 (b)에 도시된 바와 같이, 제1 구간의 영상 다음에 재생될 제2 구간의 영상을 화면의 제1 영역에 디스플레이하고, 제2 구간의 영상이 재생되는 동안 촬영된 사용자 영상을 제2 영역에 디 스플레이한다. 이때, 전자 장치는 제2 구간의 영상 및 사용자 영상 각각에 대응하여 생성된 문장을 제2 구간의 영상 및 사용자 영상 각각이 디스플레이된 제1 및 제2 영역과 상이한 제3 영역에 디스플레이 할 수 있다. 구체적으로, 전자 장치는 전술한 바와 같이, 제2 구간의 영상과 관련된 재료 조리 관련 행동에 대한 장면 이해를 수행한다. 이후, 전자 장치는 수행된 장면 이해를 바탕으로 재료 조리 관련 행동과 관련된 제1 문장을 생성하고, 생성된 제1 문장을 화면의 제3 영역에 디스플레이한다. 또한, 전자 장치는 전술한 바와 같이, 제2 구간의 영상이 디스플레이되는 동안 촬영된 사용자 영상 과 관련된 사용자의 행동에 대한 장면 이해를 수행한다. 이후, 전자 장치는 수행된 장면 이해를 바 탕으로 사용자의 행동과 관련된 제2 문장를 생성하고, 생성된 제2 문장을 화면의 제3 영역에 디스플 레이한다. 이하에서는, 본 개시에 따른 전자 장치에서 획득된 영상 또는 촬영된 사용자 영상을 분석하여 장면 이해를 수행하는 동작에 대해서 상세히 설명하도록 한다. 도 9는 본 개시의 일 실시예에 따른 전자 장치에서 획득된 영상에 대한 장면 이해를 수행하는 예시도이다. 도 9에 도시된 바와 같이, 전자 장치는 영상이 획득되면, 획득된 영상을 구성하는 복수의 영상 프레임을 학습된 인공지능 모델에 입력하여, 입력된 복수의 영상 프레임에 대한 특징값을 획득한다. 여기서, 영상은 컨텐츠 서버(미도시)와 같은 외부 장치로부터 수신된 영상이거나, 카메라(미도시)를 통해 촬영된 영상이 될 수 있다. 이 같은 영상을 구성하는 복수의 영상 프레임에 대한 특징값이 획득되면, 전자 장치는 복수의 영상 프레임으로부터 획득된 특징값에 기초하여 영상 프레임 각각에 대한 이미지 정보를 획득한다. 여기서, 이미지 정보는 영상 프레임에 포함된 오브젝트에 대한 정보, 색상 정보, 오브젝트에 대한 위치 정 보 등을 포함할 수 있다. 이후, 전자 장치는 복수의 영상 프레임으로부터 획득된 특징값에 기초하여 각각의 영상 프레임에 대한 이 미지 정보에 포함된 오브젝트 중 기정의된 특정 오브젝트에 대한 움직임 정보를 획득한다. 여기서, 기정 의된 특정 오브젝트는 각 영상 프레임 내 포함된 오브젝트 중 대표하는 오브젝트가 될 수 있다. 추가적으로, 전자 장치는 영상과 관련된 사용자 음성 신호가 수신되면, MFCC(Mel Frequency Cepstral Coefficient) 알고리즘을 이용하여 수신된 사용자 음성 신호에 대한 음성 인식을 수행하여 사용자 음 성 신호에 대한 음성 정보를 획득한다.추가적으로, 전자 장치는 영상과 관련하여 사용자로부터 입력된 카테고리 정보를 더 획득할 수 있다. 여기서, 카테고리 정보는 영상에 대한 장면 이해가 해당 카테고리 정보 내에서 수행되도 록 하기 위한 것으로써, 이 같은 카테고리 정보를 통해 영상에 대한 장면 이해 수행 속도가 개선될 수 있다. 이 같은 이미지 정보, 특정 오브젝트에 대한 움직임 정보, 음성 정보 및 카테고리 정보 적 어도 하나가 획득되면, 전자 장치는 획득된 정보를 이용하여 영상 내 포함된 행동에 대한 장면 이해 를 수행할 수 있다. 이후, 전자 장치는 영상에 대한 장면 이해를 바탕으로 영상 내 포함된 행동과 관련된 문장을 생 성한다. 구체적으로, 전자 장치는 LSTM(Long Short-term Memory) 방식의 RNN(Recurrent Neural Network) 모델을 이용하여 영상 내 포함된 행동과 관련된 문장을 생성할 수 있다. 전술한 바와 같이, LSTM(Long Short-term Memory) 방식의 RNN(Recurrent Neural Network) 모델은 장기 저장된 데이터와 단기 저장된 데이터 각각을 입력하여 그에 따른 결과값을 출력하는 모델이다. 따라서, 전자 장치는 LSTM(Long Short-term Memory) 방식의 RNN(Recurrent Neural Network) 모델에 영상 으로부터 획득된 다양한 정보를 입력하여 영상 내 포함된 행동과 관련된 문장을 생성할 수 있다. 지금까지, 본 개시에 따른 전자 장치에서 사용자의 행동에 따라 영상을 자동 재생하는 동작에 대해서 상세 히 설명하였다. 이하에서는, 전술한 인공지능 학습모델을 업데이트하는 동작에 대해서 상세히 설명하도록 한다. 도 10은 본 개시의 일 실시예에 따른 인공지능 학습모델을 업데이트하고 이용하는 전자 장치의 프로세서의 세부 블록도이다. 도 10에 도시된 바와 같이, 프로세서는 학습부 및 획득부 중 적어도 하나를 더 포함할 수 있 다. 이 같은 프로세서는 도 1 및 도 2의 전자 장치의 프로세서에 대응될 수 있다. 학습부는 학습 데이터를 이용하여 전자 장치에 입력된 영상을 구성하는 복수의 영상 프레임에 포함 된 오브젝트를 인식하기 위한 모델(이하 제1 모델이라 함)을 생성 또는 학습시킬 수 있다. 뿐만 아니라, 학습부는 사용자 음성에 대한 키워드를 획득하기 위한 모델(이하 제2 모델이라 함)을 생성 또는 학습시킬 수 있다. 이 같은 학습부는 수집된 학습 데이터를 이용하여 인식 기준을 갖는 학습된 모델 을 생성할 수 있다. 일 예로, 학습부는 전자 장치에 입력된 영상을 구성하는 복수의 영상 프레임을 입력 데이터로 사용 하여 해당 영상 프레임에 포함된 복수의 오브젝트에 대한 정보를 획득하기 위한 제1 모델을 생성, 학습 또는 갱 신시킬 수 있다. 또한, 학습부는 영상으로부터 획득된 이미지 정보, 특정 오브젝트에 대한 움직임 정보, 해당 영상과 관련 된 음성 정보, 카테고리 정보 중 적어도 하나를 입력 데이터로 사용하여 해당 영상에 대한 장면 이해를 수행하 는데 이용되는 키워드를 획득하기 위한 제2 모델을 생성, 학습 또는 갱신시킬 수 있다. 획득부는 소정의 데이터를 학습된 모델의 입력 데이터로 사용하여, 다양한 정보를 획득할 수 있다. 일 예로, 획득부는 입력된 영상을 구성하는 복수의 영상 프레임을 학습된 제1 모델의 입력 데이터로 사용 하여 해당 영상을 구성하는 복수의 영상 프레임에 포함된 복수의 오브젝트에 대한 정보를 획득(또는, 인식, 추 정)할 수 있다. 또한, 획득부는 영상으로부터 획득된 이미지 정보, 특정 오브젝트에 대한 움직임 정보, 해당 영상과 관련 된 음성 정보, 카테고리 정보 중 적어도 하나를 학습된 제2 모델의 입력 데이터로 사용하여 영상의 장면 이해를 수행하는데 이용되는 키워드를 획득(또는 추정, 추론, 인식)할 수 있다. 학습부의 적어도 일부 및 획득부의 적어도 일부는, 소프트웨어 모듈로 구현되거나 적어도 하나의 하드웨어 칩 형태로 제작되어 전자 장치에 탑재될 수 있다. 예를 들어, 학습부 및 획득부 중적어도 하나는 인공 지능(AI; artificial intelligence)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또 는 기존의 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 전술한 각종 전자 장치에 탑재될 수도 있다. 이때, 인공 지능을 위한 전용 하드웨어 칩은 확률 연산 에 특화된 전용 프로세서로서, 기존의 범용 프로세서보다 병렬처리 성능이 높아 기계 학습과 같은 인공 지능 분 야의 연산 작업을 빠르게 처리할 수 있다. 학습부 및 획득부가 소프트웨어 모듈(또는, 인스트럭션(instruction) 포함하는 프로그램 모듈)로 구현되는 경우, 소프트웨어 모듈은 컴퓨터로 읽을 수 있는 판독 가능한 비일시적 판독 가능 기록매체(non- transitory computer readable media)에 저장될 수 있다. 이 경우, 소프트웨어 모듈은 OS(Operating System)에 의해 제공되거나, 소정의 애플리케이션에 의해 제공될 수 있다. 또는, 소프트웨어 모듈 중 일부는 OS(Operating System)에 의해 제공되고, 나머지 일부는 소정의 애플리케이션에 의해 제공될 수 있다. 이 경우, 학습부 및 획득부는 하나의 전자 장치에 탑재될 수도 있으며, 또는 별개의 전자 장 치들에 각각 탑재될 수도 있다. 예를 들어, 학습부 및 획득부 중 하나는 전자 장치에 포함되 고, 나머지 하나는 외부의 서버(미도시)에 포함될 수 있다. 또한, 학습부 및 획득부는 유선 또는 무선으로 통하여, 학습부가 구축한 모델 정보를 획득부로 제공할 수도 있고, 학습부로 입력 된 데이터가 추가 학습 데이터로서 학습부로 제공될 수도 있다. 도 11a는 본 개시의 일 실시예에 따른 학습부 및 획득부의 세부 블록도이다. 도 11a의 (a)에 도시된 바와 같이, 학습부는 학습 데이터 획득부(1110-1) 및 모델 학습부(1110-4)를 포함 할 수 있다. 또한, 학습부는 학습 데이터 전처리부(1110-2), 학습 데이터 선택부(1110-3) 및 모델 평가 부(1110-5) 중 적어도 하나를 선택적으로 더 포함할 수 있다. 학습 데이터 획득부(1110-1)는 제1 모델 및 제2 모델에 필요한 학습 데이터를 획득할 수 있다. 실시예에 따라, 학습 데이터 획득부(1110-1)는 영상 데이터, 복수의 오브젝트에 대한 정보, 사용자 정보, 사용자 음성 등을 학 습 데이터로서 획득할 수 있다. 학습 데이터는 학습부 또는 학습부의 제조사가 수집 또는 테스트 한 데이터가 될 수도 있다. 모델 학습부(1110-4)는 학습 데이터를 이용하여, 영상을 구성하는 복수의 영상 프레임에 포함된 오브젝트를 어 떻게 인식할지에 관한 기준을 갖도록 학습시킬 수 있다. 예로, 모델 학습부(1110-4)는 학습 데이터 중 적어도 일부를 판단 기준으로 이용하는 지도 학습(supervised learning)을 통하여, 인공지능 학습 모델을 학습시킬 수 있다. 또는, 모델 학습부(1110-4)는, 예를 들어, 별다른 지도 없이 학습 데이터를 이용하여 스스로 학습함으로 써, 상황의 판단을 위한 판단 기준을 발견하는 비지도 학습(unsupervised learning)을 통하여, 인공지능 모델을 학습시킬 수 있다. 또한, 모델 학습부(1110-4)는 예를 들어, 학습에 따른 상황 판단의 결과가 올바른 지에 대한 피드백을 이용하는 강화 학습(reinforcement learning)을 통하여, 인공지능 학습 모델을 학습시킬 수 있다. 또한, 모델 학습부 (1110-4)는, 예를 들어, 오류 역전파법(error back-propagation) 또는 경사 하강법(gradient descent)을 포함 하는 학습 알고리즘 등을 이용하여 인공지능 학습 모델을 학습시킬 수 있다 모델 학습부(1110-4)는 미리 구축된 인공지능 모델이 복수 개가 존재하는 경우, 입력된 학습 데이터와 기본 학 습 데이터의 관련성이 큰 인공지능 학습 모델을 학습할 인공지능 학습 모델로 결정할 수 있다. 이 경우, 기본 학습 데이터는 데이터의 타입별로 기분류되어 있을 수 있으며, 인공지능 모델은 데이터의 타입별로 미리 구축되 어 있을 수 있다. 예를 들어, 기본 학습 데이터는 학습 데이터가 생성된 지역, 학습 데이터가 생성된 시간, 학습 데이터의 크기, 학습 데이터의 장르, 학습 데이터의 생성자, 학습 데이터 내의 오브젝트의 종류 등과 같은 다양한 기준으로 기 분류되어 있을 수 있다. 인공지능 학습 모델이 학습되면, 모델 학습부(1110-4)는 학습된 인공지능 학습 모델을 저장할 수 있다. 이 경 우, 모델 학습부(1110-4)는 학습된 인공지능 학습 모델을 전자 장치의 메모리에 저장할 수 있다. 또 는, 모델 학습부(1110-4)는 학습된 인공지능 학습 모델을 전자 장치와 유선 또는 무선 네트워크로 연결되 는 인공지능 서버(미도시)의 메모리에 저장할 수도 있다. 학습부는 인공지능 학습 모델의 인식 결과를 향상시키거나, 인공지능 학습 모델의 생성에 필요한 자원 또 는 시간을 절약하기 위하여, 학습 데이터 전처리부(1110-2) 및 학습 데이터 선택부(1110-3)를 더 포함할 수도있다. 학습 데이터 전처리부(1110-2)는 오브젝트에 대한 정보 획득 및 키워드 생성을 위한 학습에 획득된 데이터가 이 용될 수 있도록, 획득된 데이터를 전처리할 수 있다. 학습 데이터 전처리부(1110-2)는 모델 학습부(1110-4)가 오브젝트에 대한 정보를 획득하기 위하여 획득된 데이터를 이용할 수 있도록, 해당 데이터를 기설정된 포맷으로 가공할 수 있다. 학습 데이터 선택부(1110-3)는 학습 데이터 획득부(1110-1)에서 획득된 데이터 또는 학습 데이터 전처리부 (1110-2)에서 전처리된 데이터 중에서 학습에 필요한 데이터를 선택할 수 있다. 선택된 학습 데이터는 모델 학 습부(1110-4)에 제공될 수 있다. 학습 데이터 선택부(1110-3)는 기설정된 선별 기준에 따라, 획득되거나 전처리된 데이터 중에서 학습에 필요한 학습 데이터를 선택할 수 있다. 또한, 학습 데이터 선택부(1110-3)는 모델 학습부(1110-4)에 의한 학습에 의해 기설정된 선별 기준에 따라 학습 데이터를 선택할 수도 있다. 학습부는 인공지능 학습 모델의 인식 결과를 향상시키기 위하여, 모델 평가부(1110-5)를 더 포함할 수도 있다. 모델 평가부(1110-5)는 인공지능 학습 모델에 평가 데이터를 입력하고, 평가 데이터로부터 출력되는 인식 결과 가 소정 기준을 만족하지 못하는 경우, 모델 학습부(1110-4)로 하여금 다시 학습하도록 할 수 있다. 이 경우, 평가 데이터는 인공지능 모델을 평가하기 위한 기정의된 데이터일 수 있다. 예를 들어, 모델 평가부(1110-5)는 평가 데이터에 대한 학습된 인공지능 학습 모델의 인식 결과 중에서, 인식 결과가 정확하지 않은 평가 데이터의 개수 또는 비율이 미리 설정된 임계치를 초과하는 경우 소정 기준을 만족 하지 못한 것으로 평가할 수 있다. 한편, 학습된 인공지능 학습 모델이 복수 개가 존재하는 경우, 모델 평가부(1110-5)는 각각의 학습된 인공지능 학습 모델에 대하여 소정 기준을 만족하는지를 평가하고, 소정 기준을 만족하는 모델을 최종 인공지능 학습 모 델로서 결정할 수 있다. 소정 기준을 만족하는 학습 모델이 복수 개인 경우, 모델 평가부(1110-5)는 평가 점수 가 높은 순으로 미리 설정된 어느 하나 또는 소정 개수의 학습 모델을 최종 인공지능 학습 모델로서 결정할 수 있다. 한편, 획득부는 도 11a의 (b)에 도시된 바와 같이, 입력 데이터 획득부(1120-1) 및 제공부(1120-4)를 포 함할 수 있다. 또한, 획득부는 입력 데이터 전처리부(1120-2), 입력 데이터 선택부(1120-3) 및 모델 갱신부(1120-5) 중 적어도 하나를 선택적으로 더 포함할 수 있다. 입력 데이터 획득부(1120-1)는 영상을 구성하는 복수의 영상 프레임에 포함된 오브젝트에 대한 정보를 획득하기 위해 필요한 데이터를 획득할 수 있다. 제공부(1120-4)는 입력 데이터 획득부(1120-1)에서 획득된 입력 데이터를 입력 값으로 학습된 인공지능 학습 모 델에 적용하여 영상을 구성하는 복수의 영상 프레임에 포함된 오브젝트에 대한 정보를 획득할 수 있다. 이 같은 제공부(1120-4)는 후술할 입력 데이터 전처리부(1120-2) 또는 입력 데이터 선택부(1120-3)에 의해 선택 된 데이터를 입력 값으로 인공지능 학습 모델에 적용하여 인식 결과를 획득할 수 있다. 인식 결과는 인공지능 학습 모델에 의해 결정될 수 있다. 일 실시예로, 제공부(1120-4)는 입력 데이터 획득부(1120-1)에서 획득한 영상 관련 데이터를 학습된 제1 모델에 적용하여 영상을 구성하는 복수의 영상 프레임에 포함된 오브젝트에 대한 정보를 획득(또는, 추정)할 수 있다. 또 다른 예로, 제공부(1120-4)는 입력 데이터 획득부(1120-1)에서 획득한 오브젝트에 대한 정보, 사용자 정보 및 사용자 음성 등을 학습된 제2 모델에 적용하여 사용자 음성에 대응되는 오브젝트에 대한 키워드를 획득(또는, 추정)할 수 있다. 획득부는 인공지능 학습 모델의 인식 결과를 향상시키거나, 인식 결과의 제공을 위한 자원 또는 시간을 절약하기 위하여, 입력 데이터 전처리부(1120-2) 및 입력 데이터 선택부(1120-3)를 더 포함할 수도 있다. 입력 데이터 전처리부(1120-2)는 제1 및 제2 모델에 입력되기 위해 획득된 데이터가 이용될 수 있도록, 획득된 데이터를 전처리할 수 있다. 입력 데이터 전처리부(1120-2)는 제공부(1120-4)가 오브젝트에 대한 정보 획득 및키워드 생성을 위하여 획득된 데이터를 이용할 수 있도록, 획득된 데이터를 기정의된 포맷으로 가공할 수 있다. 입력 데이터 선택부(1120-3)는 입력 데이터 획득부(1120-1)에서 획득된 데이터 또는 입력 데이터 전처리부 (1120-2)에서 전처리된 데이터 중에서 상황 판단에 필요한 데이터를 선택할 수 있다. 선택된 데이터는 제공부 (1120-4)에게 제공될 수 있다. 입력 데이터 선택부(1120-3)는 상황 판단을 위한 기설정된 선별 기준에 따라, 획득되거나 전처리된 데이터 중에서 일부 또는 전부를 선택할 수 있다. 또한, 입력 데이터 선택부(1120-3)는 모델 학습부(1120-4)에 의한 학습에 의해 기설정된 선별 기준에 따라 데이터를 선택할 수도 있다. 모델 갱신부(1120-5)는 제공부(1120-4)에 의해 제공되는 인식 결과에 대한 평가에 기초하여, 인공지능 모델이 갱신되도록 제어할 수 있다. 예를 들어, 모델 갱신부(1120-5)는 제공부(1120-4)에 의해 제공되는 인식 결과를 모델 학습부(1120-4)에게 제공 함으로써, 모델 학습부(1120-4)가 인공지능 학습 모델을 추가 학습 또는 갱신하도록 요청할 수 있다. 도 11b는 본 개시의 일 실시예에 따른 전자 장치 및 외부 서버가 서로 연동하여 데이터를 학습하고 판단하는 예 시도이다. 도 11b에 도시된 바와 같이, 외부의 서버(S)는 영상을 구성하는 복수의 프레임에 포함된 복수의 오브젝트에 대 한 정보를 획득한다. 뿐만 아니라, 외부의 서버(S)는 사용자 음성에 대응되는 오브젝트에 대한 키워드를 획득 하기 위한 기준을 학습할 수 있다. 전자 장치(A)는 서버(S)에 의한 학습 결과에 기초하여 생성된 모델들을 이용하여 영상을 구성하는 복수의 영상 프레임에 포함된 복수의 오브젝트에 대한 정보를 획득할 뿐만 아니라, 오브젝트에 대한 다양한 키워드를 획득할 수 있다. 이 경우, 서버(S)의 모델 학습부(1110-4)는 도 10에 도시된 학습부의 기능을 수행할 수 있다. 서버(S)의 모델 학습부(1110-4)는 제1 및 제2 모델에 대한 판단 기준(혹은, 인식 기준)을 학습할 수 있다. 또한, 전자 장치(A)의 제공부(1120-4)는 입력 데이터 선택부(1120-3)에 의해 선택된 데이터를 서버(S)에 의해 생성된 인공지능 학습 모델에 적용하여 영상을 구성하는 복수의 영상 프레임에 포함된 오브젝트에 대한 정보를 획득할 뿐만 아니라, 오브젝트에 대한 다양한 키워드를 획득할 수 있다. 또한, 전자 장치(A)의 제공부(1120-4)는 서버(S)에 의해 생성된 인공지능 학습 모델을 서버(S)로부터 수신하고, 수신된 인공지능 학습 모델을 이용하여 영상을 구성하는 복수의 영상 프레임에 포함된 오브젝트에 대한 정보를 획득할 뿐만 아니라, 오브젝트에 대한 다양한 키워드를 획득할 수 있다. 지금까지, 본 개시에 따른 전자 장치에서 인공지능 학습 모델을 이용하여 영상을 구성하는 복수의 영상 프 레임에 포함된 오브젝트를 판단하는 동작에 대해서 상세히 설명하였다. 이하에서는, 전자 장치에 입력된 사용자 음성을 인식하는 동작에 대해서 설명하도록 한다. 도 12는 본 개시의 일 실시예에 따른 전자 장치에 입력된 사용자 발화 음성 명령을 인식하는 지능형 서버에 대 한 블록도이다. 도 12에 도시된 바와 같이, 지능형 서버는 자동 음성 인식(automatic speech recognition)(ASR) 모듈 , 자연어 이해(natural language understanding)(NLU) 모듈, 패스 플래너(path planner) 모듈 , 대화 매니저(dialogue manager)(DM) 모듈, 자연어 생성(natural language generator)(NLG) 모듈 또는 텍스트 음성 변환(text to speech)(TTS) 모듈을 포함할 수 있다. 지능형 서버의 자연어 이해 모듈 또는 패스 플래너 모듈은 패스 룰(path rule)을 생성할 수 있다. 일 실시 예에 따르면, 자동 음성 인식(automatic speech recognition)(ASR) 모듈은 전자 장치로부 터 수신된 사용자 음성 명령을 텍스트 데이터로 변환할 수 있다. 일 실시 예에 따르면, 자동 음성 인식 모듈은 전자 장치로부터 수신된 사용자 음성 명령을 텍스트 데이터로 변환할 수 있다. 예를 들어, 자동 음성 인식 모듈은 발화 인식 모듈을 포함할 수 있다. 여기 서, 발화 인식 모듈은 음향(acoustic) 모델 및 언어(language) 모델을 포함할 수 있다. 예를 들어, 음향 모델 은 발성에 관련된 정보를 포함할 수 있고, 언어 모델은 단위 음소 정보 및 단위 음소 정보의 조합에 대한 정보 를 포함할 수 있다. 발화 인식 모듈은 발성에 관련된 정보 및 단위 음소 정보에 대한 정보를 이용하여 사용자음성을 텍스트 데이터로 변환할 수 있다. 음향 모델 및 언어 모델에 대한 정보는, 예를 들어, 자동 음성 인식 데이터베이스(automatic speech recognition database)(ASR DB)에 저장될 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 문법적 분석(syntactic analyze) 또는 의미적 분석(semantic analyze)을 수행하여 사용자 발화 의도를 파악할 수 있다. 문법적 분석은 사용자 발화 음성 명령을 문법적 단 위(예: 단어, 구, 형태소 등)로 나누고, 나누어진 단위가 어떤 문법적인 요소를 갖는지 파악할 수 있다. 의미 적 분석은 의미(semantic) 매칭, 룰(rule) 매칭, 포뮬러(formula) 매칭 등을 이용하여 수행할 수 있다. 자연어 이해 모듈은 사용자 음성 명령이 어느 도메인(domain), 의도(intent) 또는 의도를 표현하는데 필요한 파 라미터(parameter)(또는, 슬롯(slot))를 얻을 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 도메인(domain), 의도(intend) 및 의도를 파악하는데 필요한 파라미터(parameter)(또는, 슬롯(slot))로 나누어진 매칭 규칙을 이용하여 사용자의 발화 의도 및 파라미터를 결정할 수 있다. 예를 들어, 하나의 도메인(예: 알람)은 복수의 의도(예: 알람 설정, 알람 해제 등)를 포함할 수 있고, 하나의 의도는 복수의 파라미터(예: 시간, 반복 횟수, 알람음 등)을 포함할 수 있다. 복수의 룰은, 예를 들어, 하나 이상의 필수 요소 파라미터를 포함할 수 있다. 매칭 규칙은 자연어 인식 데이터베이스 (natural language understanding database)(NLU DB)에 저장될 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 형태소, 구 등의 언어적 특징(예: 문법적 요소)을 이용하여 사 용자 음성 명령으로부터 추출된 단어의 의미를 파악하고, 파악된 단어의 의미를 도메인 및 의도에 매칭시켜 사 용자의 발화 의도를 결정할 수 있다. 예를 들어, 자연어 이해 모듈은 각각의 도메인 및 의도에 사용자 음성 명령에서 추출된 단어가 얼마나 포함되어 있는 지를 계산하여 사용자 발화 의도를 결정할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 의도를 파악하는데 기초가 된 단어를 이용하여 사용자 음성 명 령의 파라미터를 결정할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 사용자 발화 의도를 파악하 기 위한 언어적 특징이 저장된 자연어 인식 데이터베이스를 이용하여 사용자의 발화 의도를 결정할 수 있 다. 또다른 실시 예에 따르면, 자연어 이해 모듈은 개인화 언어 모델(personal language model)(PLM)을 이용 하여 사용자의 발화 의도를 결정할 수 있다. 예를 들어, 자연어 이해 모듈은 개인화된 정보(예: 연락처 리스트, 음악 리스트)를 이용하여 사용자의 발화 의도를 결정할 수 있다. 개인화 언어 모델은, 예를 들어, 자 연어 인식 데이터베이스에 저장될 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈뿐만 아니라 자동 음성 인식 모듈도 자연어 인식 데이터베 이스에 저장된 개인화 언어 모델을 참고하여 사용자의 음성 명령을 인식할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 사용자 발화 의도 및 파라미터에 기초하여 패스 룰을 생성할 수 있다. 예를 들어, 자연어 이해 모듈은 사용자 발화 의도에 기초하여 실행될 앱을 선택하고, 선택된 앱에서 수행될 동작을 결정할 수 있다. 자연어 이해 모듈은 결정된 동작에 대응되는 파라미터를 결정하 여 패스 룰을 생성할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈에 의해 생성된 패스 룰은 실행될 앱, 앱에서 실행될 동작 및 동 작을 실행하는데 필요한 파라미터에 대한 정보를 포함할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 사용자 발화 의도 및 파라미터를 기반으로 하나의 패스 룰, 또 는 복수의 패스 룰을 생성할 수 있다. 예를 들어, 자연어 이해 모듈은 패스 플래너 모듈로부터 전 자 장치에 대응되는 패스 룰 셋을 수신하고, 사용자 발화 의도 및 파라미터를 수신된 패스 룰 셋에 맵핑하 여 패스 룰을 결정할 수 있다. 또다른 실시 예에 따르면, 자연어 이해 모듈은 사용자 발화 의도 및 파라미터에 기초하여 실행될 앱, 앱 에서 실행될 동작 및 동작을 실행하는데 필요한 파라미터를 결정하여 하나의 패스 룰, 또는 복수의 패스 룰을 생성할 수 있다. 예를 들어, 자연어 이해 모듈은 전자 장치의 정보를 이용하여 상기 실행될 앱 및 상기 앱에서 실행될 동작을 사용자 발화 의도에 따라 온톨로지(ontology) 또는 그래프 모델(graph model) 형태 로 배열하여 패스 룰을 생성할 수 있다. 생성된 패스 룰은, 예를 들어, 패스 플래너 모듈를 통해 패스 룰 데이터베이스(path rule database)(PR DB)에 저장될 수 있다. 그리고, 생성된 패스 룰은 데이터베이 스의 패스 룰 셋에 추가될 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 생성된 복수의 패스 룰 중 적어도 하나의 패스 룰을 선택할 수 있다. 예를 들어, 자연어 이해 모듈은 복수의 패스 룰 최적의 패스 룰을 선택할 수 있다. 또다른 예를 들어, 자연어 이해 모듈은 사용자 발화 음성 명령에 기초하여 일부 동작만이 특정된 경우 복수의 패스 룰 을 선택할 수 있다. 자연어 이해 모듈은 사용자의 추가 음성 명령에 의해 상기 복수의 패스 룰 중 하나 의 패스 룰을 결정할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 사용자 음성 명령에 대한 요청으로 패스 룰을 전자 장치 로 송신할 수 있다. 예를 들어, 자연어 이해 모듈은 사용자 음성 명령에 대응되는 하나의 패스 룰을 전 자 장치로 송신할 수 있다. 또다른 예를 들어, 자연어 이해 모듈은 사용자 음성 명령에 대응되는 복수의 패스 룰을 전자 장치로 송신할 수 있다. 여기서, 복수의 패스 룰은, 예를 들어, 사용자 음성 명령에 기초하여 일부 동작만이 특정된 경우 자연어 이해 모듈에 의해 생성될 수 있다. 일 실시 예에 따르면, 패스 플래너 모듈은 복수의 패스 룰 중 적어도 하나의 패스 룰을 선택할 수 있다. 일 실시 예에 따르면, 패스 플래너 모듈은 자연어 이해 모듈로 복수의 패스 룰을 포함하는 패스 룰 셋을 전달할 수 있다. 상기 패스 룰 셋의 복수의 패스 룰은 패스 플래너 모듈에 연결된 패스 룰 데이터 베이스에 테이블 형태로 저장될 수 있다. 예를 들어, 패스 플래너 모듈은 전자 장치의 정보 (예: OS 정보, 앱 정보)에 대응되는 패스 룰 셋을 자연어 이해 모듈로 전달할 수 있다. 여기서, 패스 룰 데이터베이스에 저장된 테이블은, 예를 들어, 도메인 또는 도메인의 버전 별로 저장될 수 있다. 일 실시 예에 따르면, 패스 플래너 모듈은 패스 룰 셋에서 하나의 패스 룰, 또는 복수의 패스 룰을 선택 하여 자연어 이해 모듈로 전달할 수 있다. 예를 들어, 패스 플래너 모듈은 사용자의 발화 의도 및 파라미터를 전자 장치에 대응되는 패스 룰 셋에 매칭하여 하나의 패스 룰, 또는 복수의 패스 룰을 선택하 여 자연어 이해 모듈로 전달할 수 있다. 일 실시 예에 따르면, 패스 플래너 모듈은 사용자 발화 의도 및 파라미터를 이용하여 하나의 패스 룰, 또 는 복수의 패스 룰을 생성할 수 있다. 예를 들어, 패스 플래너 모듈은 사용자 발화 의도 및 파라미터에 기초하여 실행될 앱 및 해당 앱에서 실행될 동작을 결정하여 하나의 패스 룰, 또는 복수의 패스 룰을 생성할 수 있다. 일 실시 예에 따르면, 패스 플래너 모듈은 기생성된 패스 룰을 패스 룰 데이터베이스에 저장할 수 있다. 일 실시 예에 따르면, 패스 플래너 모듈은 자연어 이해 모듈에서 생성된 패스 룰을 패스 룰 데이터 베이스에 저장할 수 있다. 이 같이, 생성된 패스 룰은 패스 룰 데이터베이스에 저장된 패스 룰 셋 에 추가될 수 있다. 일 실시 예에 따르면, 패스 룰 데이터베이스에 저장된 테이블에는 복수의 패스 룰 또는 복수의 패스 룰 셋을 포함할 수 있다. 복수의 패스 룰 또는 복수의 패스 룰 셋은 각 패스 룰을 수행하는 장치의 종류, 버전, 타입, 또는 특성을 반영할 수 있다. 일 실시 예에 따르면, 대화 매니저 모듈은 자연어 이해 모듈에 의해 파악된 사용자의 발화 의도가 명확한지 여부를 판단할 수 있다. 예를 들어, 대화 매니저 모듈은 파라미터의 정보가 충분하지 여부에 기초하여 사용자의 발화 의도가 명확한지 여부를 판단할 수 있다. 대화 매니저 모듈은 자연어 이해 모듈 에서 파악된 파라미터가 태스크를 수행하는데 충분한지 여부를 판단할 수 있다. 일 실시 예에 따르면, 대화 매니저 모듈는 사용자의 발화 의도가 명확하지 않은 경우 사용자에게 필요한 정보를 요청하는 피드백을 수행할 수 있다. 예를 들어, 대화 매니저 모듈는 사용자의 발화 의도를 파악 하기 위한 파라미터에 대한 정보를 요청하는 피드백을 수행할 수 있다. 일 실시 예에 따르면, 대화 매니저 모듈은 멀티미디어 컨텐츠 제공(content provider) 모듈을 포함할 수 있다. 여기서, 멀티미디어 컨텐츠 제공 모듈은 자연어 이해 모듈에서 파악된 의도 및 파라미터에 기초하 여 동작을 수행할 수 있는 경우, 사용자 음성 명령에 대응되는 태스크를 수행한 결과를 생성할 수 있다. 일 실시 예에 따르면, 대화 매니저 모듈은 사용자 음성 명령에 대한 응답으로 멀티미디어 컨텐츠 제공 모 듈에서 생성된 결과를 전자 장치로 송신할 수 있다. 일 실시 예에 따르면, 자연어 생성 모듈(NLG 모듈)은 지정된 정보를 텍스트 형태로 변경할 수 있다. 여 기서, 텍스트 형태로 변경된 정보는 자연어 발화의 형태일 수 있다. 그리고, 지정된 정보는, 예를 들어, 추가 입력에 대한 정보, 사용자 음성 명령에 대응되는 동작의 완료를 안내하는 정보 또는 사용자의 추가 음성 명령을 안내하는 정보(예: 사용자 입력에 대한 피드백 정보)일 수 있다. 그리고, 텍스트 형태로 변경된 정보는 전자 장 치로 송신되어 디스플레이에 표시되거나, 텍스트 음성 변환 모듈(TTS 모듈)로 송신되어 음성 형태로 변경될 수 있다. 일 실시 예에 따르면, 텍스트 음성 변환 모듈(TTS 모듈)은 텍스트 형태의 정보를 음성 형태의 정보로 변 경할 수 있다. 텍스트 음성 변환 모듈은 자연어 생성 모듈로부터 텍스트 형태의 정보를 수신하고, 수신된 텍스트 형태의 정보를 음성 형태의 정보로 변경하여 전자 장치로 송신할 수 있다. 전자 장치(10 0)는 송신된 음성 형태의 정보를 스피커로 출력할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈, 패스 플래너 모듈 및 대화 매니저 모듈은 하나의 모듈로 구현될 수 있다. 예를 들어, 자연어 이해 모듈, 패스 플래너 모듈 및 대화 매니저 모듈 은 하나의 모듈로 구현되어 사용자의 발화 의도 및 파라미터를 결정하고, 결정된 사용자의 발화 의도 및 파라미터에 대응되는 응답(예: 패스 룰)을 생성할 수 있다. 이에 따라, 생성된 응답은 전자 장치로 송신 될 수 있다. 본 개시의 다양한 실시예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호 출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실시예들에 따른 전자 장치(예: 전자 장치 )를 포함할 수 있다. 상기 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 상기 프로세서의 제어하에 다른 구성요소들을 이용하여 상기 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또는 인 터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장매체는, 비일시적(non- transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장매체가 신호(signal)를 포함하지 않 으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임시적으로 저장됨을 구분하지 않는다. 일시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있다. 온라인 배포의 경 우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 다양한 실시예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요소가 다 양한 실시예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작들은 순차 적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다."}
{"patent_id": "10-2018-0163834", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 전자 장치의 블록도, 도 2는 본 개시의 일 실시예에 따른 전자 장치의 세부 블록도, 도 3은 본 개시의 일 실시예에 따른 사용자 행동에 따라 컨텐츠에 포함된 영상을 자동 재생하기 위한 각종 모듈 을 저장하는 메모리의 블록도, 도 4는 본 개시의 일 실시예에 따른 전자 장치에서 사용자 행동에 기초하여 컨텐츠를 자동 출력하기 위한 방법 의 흐름도, 도 5는 본 개시의 일 실시예에 따른 전자 장치에서 컨텐츠에 포함된 영상으로부터 행동별 구간 영상을 판단하는 흐름도, 도 6은 본 개시의 일 실시예에 따른 전자 장치에서 촬영된 사용자 영상을 획득하는 방법의 흐름도, 도 7은 본 개시의 일 실시예에 따른 전자 장치에서 사용자 행동이 구간 영상의 행동인지 여부를 판단하는 예시 도, 도 8a은 본 개시의 일 실시예에 따른 전자 장치에서 사용자의 행동에 따라 영상을 자동 재생하는 제1 예시도, 도 8b는 본 개시의 일 실시예에 따른 전자 장치에서 사용자의 행동에 따라 영상을 자동 재생하는 제2 예시도, 도 8c는 본 개시의 일 실시예에 따른 전자 장치에서 사용자의 행동에 따라 영상을 자동 재생하는 제3 예시도, 도 8d는 본 개시의 일 실시예에 따른 전자 장치에서 사용자의 행동에 따라 영상을 자동 재생하는 제4 예시도, 도 9는 본 개시의 일 실시예에 따른 전자 장치에서 획득된 영상에 대한 장면 이해를 수행하는 예시도, 도 10은 본 개시의 일 실시예에 따른 인공지능 학습모델을 업데이트하고 이용하는 전자 장치의 프로세서의 세부 블록도, 도 11a는 본 개시의 일 실시예에 따른 학습부 및 획득부의 세부 블록도, 도 11b는 본 개시의 일 실시예에 따른 전자 장치 및 외부 서버가 서로 연동하여 데이터를 학습하고 판단하는 예 시도, 도 12는 본 개시의 일 실시예에 따른 전자 장치에 입력된 사용자 발화 음성 명령을 인식하는 지능형 서버에 대 한 블록도이다."}
