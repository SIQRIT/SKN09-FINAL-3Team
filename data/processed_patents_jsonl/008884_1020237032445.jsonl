{"patent_id": "10-2023-7032445", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0148239", "출원번호": "10-2023-7032445", "발명의 명칭": "신경망을 사용하는 비디오로부터의 로버스트 얼굴 애니메이션", "출원인": "로브록스 코포레이션", "발명자": "나바로, 이냐키"}}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터 구현 방법으로서,완전 컨벌루션 네트워크(fully convolutional network)를 사용하여, 비디오의 제1 프레임으로부터 바운딩 박스후보의 세트를 식별하는 단계 - 각각의 바운딩 박스 후보는 얼굴을 포함함 -;컨벌루션 신경망을 사용하여, 상기 바운딩 박스 후보의 세트를 바운딩 박스로 정제하는 단계;오버로드된 출력 컨볼루션 신경망을 사용하여, 상기 바운딩 박스 및 상기 제1 프레임에 기초하여 미리 정의된얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트를 획득하는 단계;상기 미리 정의된 얼굴 표정 가중치, 상기 머리 포즈 및 상기 얼굴 랜드마크 중 하나 이상의 상기 제1 세트에기초하여, 3차원(3D) 아바타의 애니메이션의 제1 애니메이션 프레임을 생성하는 단계 - 상기 아바타의 머리 포즈는 상기 제1 세트의 머리 포즈와 매칭하고, 상기 아바타의 얼굴 랜드마크는 상기 제1 세트의 얼굴 랜드마크와매칭함 -; 및상기 제1 프레임에 후속하는 상기 비디오의 각 추가 프레임에 대해, 상기 추가 프레임에 적용된 상기 바운딩 박스가 상기 얼굴을 포함하는지 여부를 검출하고; 상기 바운딩 박스가 상기 얼굴을 포함하는 것으로 검출되면, 상기 오버로드된 출력 컨볼루션 신경망을 사용하여, 상기 추가 프레임 및 상기 바운딩 박스에 기초하여 상기 하나이상의 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크의 추가 세트를 획득하고; 그리고 상기 추가세트를 사용하여 상기 3D 아바타의 애니메이션의 추가 애니메이션 프레임을 생성하는 단계를 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 추가 프레임에 대한 바운딩 박스가 상기 얼굴을 포함하지 않는 것으로 검출되면, 상기 추가 프레임을 상기제1 프레임으로 설정하는 것인, 컴퓨터 구현 방법."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 오버로드된 출력 컨볼루션 신경망에 의해, 상기 얼굴에서 혀가 검출되는 혀 내밀기 조건(tongue outcondition)을 식별하는 단계를 더 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 혀 내밀기 조건을 식별하는 단계는 상기 오버로드된 출력 컨벌루션 신경망의 서브모델에 기초하는 것인,컴퓨터 구현 방법."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 오버로드된 출력 컨벌루션 네트워크는 제1 신경망이고, 상기 방법은, 추가 오버로드된 출력 컨볼루션 신경망을 사용하여, 상기 제1 프레임 및 상기 바운딩 박스에 기초하여 상기 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 상기 제1 세트를 정제하는 단계를 더 포함하는 컴퓨터 구현 방법.공개특허 10-2023-0148239-3-청구항 6 제5항에 있어서,상기 추가 오버로드된 출력 컨벌루션 신경망은 상기 제1 신경망과 비교할 때 더 높은 LOD(level of detail)를제공하는 것인, 컴퓨터 구현 방법."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서,상기 추가 오버로드된 출력 컨볼루션 신경망에 의해, 혀 내밀기 조건을 식별하는 단계를 더 포함하고, 혀는 상기 얼굴에서 검출되는 것인, 컴퓨터 구현 방법."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제5항에 있어서,상기 제1 신경망의 입력 프레임 해상도는 상기 추가 오버로드된 출력 컨볼루션 신경망의 입력 프레임 해상도보다 작은 것인, 컴퓨터 구현 방법."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 오버로드된 출력 컨볼루션 신경망은 인공 비디오의 인공적으로 생성된 입력 프레임의 세트에 대해 트레이닝되는 것인, 컴퓨터 구현 방법."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 오버로드된 출력 컨벌루션 신경망은 손으로 라벨링되는 실제 이미지의 세트로 추가로 트레이닝되는 것인,컴퓨터 구현 방법."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "시스템으로서,명령어가 저장된 메모리; 및상기 메모리에 결합된 프로세싱 장치를 포함하고, 상기 프로세싱 장치는 상기 메모리에 액세스하고 상기 명령어를 실행하도록 구성되며, 상기 명령어는 상기 프로세싱 장치로 하여금 동작을 수행하게 하고, 상기 동작은:완전 컨벌루션 네트워크를 사용하여, 비디오의 제1 프레임으로부터 바운딩 박스 후보의 세트를 식별하는 것 -각각의 바운딩 박스 후보는 얼굴을 포함함 -;컨벌루션 신경망을 사용하여, 상기 바운딩 박스 후보의 세트를 바운딩 박스로 정제하는 것;오버로드된 출력 컨볼루션 신경망을 사용하여, 상기 제1 프레임 및 상기 바운딩 박스에 기초하여 미리 정의된얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트를 획득하는 것;상기 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 상기 제1 세트에 기초하여, 3차원(3D) 아바타의 애니메이션의 제1 애니메이션 프레임을 생성하는 것 - 상기 아바타의 머리 포즈는 상기 제1세트의 머리 포즈와 매칭하고, 상기 아바타의 얼굴 랜드마크는 상기 제1 세트의 얼굴 랜드마크와 매칭함 -; 및상기 제1 프레임에 후속하는 상기 비디오의 각 추가 프레임에 대해, 상기 추가 프레임에 적용된 바운딩 박스가상기 얼굴을 포함하는지 여부를 검출하고; 상기 바운딩 박스가 상기 얼굴을 포함하는 것으로 검출되면, 상기 오버로드된 출력 컨볼루션 신경망을 사용하여, 상기 추가 프레임 및 상기 바운딩 박스에 기초하여 상기 하나 이상의 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크의 추가 세트를 획득하고; 그리고 상기 추가 세트를 사용하여 상기 3D 아바타의 애니메이션의 추가 애니메이션 프레임을 생성하는 것공개특허 10-2023-0148239-4-을 포함하는 컴퓨터 구현 방법."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 추가 프레임에 대한 상기 바운딩 박스가 상기 얼굴을 포함하지 않는 것으로 검출되면, 상기 추가 프레임을상기 제1 프레임으로 설정하는 것인, 시스템."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 동작은 상기 오버로드된 출력 컨볼루션 신경망에 의해, 혀 내밀기 조건을 식별하는 것을 더 포함하고, 혀는 상기 얼굴에서 검출되는, 시스템."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 오버로드된 출력 컨벌루션 네트워크는 제1 신경망이고, 상기 동작은, 추가 오버로드된 출력 컨볼루션 신경망을 사용하여, 상기 제1 프레임 및 상기 바운딩 박스에 기초하여 상기 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 상기 제1 세트를 추가로정제하는 것을 더 포함하는, 시스템."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,추가 오버로드된 출력 컨벌루션 신경망은 상기 제1 신경망과 비교할 때 더 높은 LOD(level of detail)를 제공하는 것인, 시스템."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "프로세싱 장치에 의한 실행에 응답하여, 상기 프로세싱 장치로 하여금 동작을 수행하게 하는 저장된 명령어를갖는 비일시적 컴퓨터 판독 가능 매체로서, 상기 동작은:완전 컨벌루션 네트워크를 사용하여, 비디오의 제1 프레임으로부터 바운딩 박스 후보의 세트를 식별하는 것 -각각의 바운딩 박스 후보는 얼굴을 포함함 -;컨벌루션 신경망을 사용하여, 상기 바운딩 박스 후보의 세트를 바운딩 박스로 정제하는 것;오버로드된 출력 컨볼루션 신경망을 사용하여, 상기 제1 프레임 및 상기 바운딩 박스에 기초하여 미리 정의된얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트를 획득하는 것;상기 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 상기 제1 세트에 기초하여, 3차원(3D) 아바타의 애니메이션의 제1 애니메이션 프레임을 생성하는 것 - 상기 아바타의 머리 포즈는 상기 제1세트의 머리 포즈와 매칭하고, 상기 아바타의 얼굴 랜드마크는 상기 제1 세트의 얼굴 랜드마크와 매칭함; 및상기 제1 프레임에 후속하는 상기 비디오의 각 추가 프레임에 대해, 상기 추가 프레임에 적용된 바운딩 박스가상기 얼굴을 포함하는지 여부를 검출하고; 상기 바운딩 박스가 상기 얼굴을 포함하는 것으로 검출되면, 상기 오버로드된 출력 컨볼루션 신경망을 사용하여, 상기 추가 프레임 및 상기 바운딩 박스에 기초하여 상기 하나 이상의 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크의 추가 세트를 획득하고; 그리고 상기 추가 세트를 사용하여 상기 3D 아바타의 애니메이션의 추가 애니메이션 프레임을 생성하는 것을 포함하는, 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 추가 프레임에 대한 바운딩 박스가 상기 얼굴을 포함하지 않는 것으로 검출되면, 상기 추가 프레임을 상기공개특허 10-2023-0148239-5-제1 프레임으로 설정하는 것인, 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항에 있어서,상기 동작은 상기 오버로드된 출력 컨벌루션 신경망에 의해, 혀 내밀기 조건을 식별하는 것을 더 포함하고, 혀는 상기 얼굴에서 검출되는, 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제16항에 있어서,상기 오버로드된 출력 컨벌루션 네트워크는 제1 신경망이고, 상기 동작은, 추가 오버로드된 출력 컨볼루션 신경망을 사용하여, 상기 제1 프레임 및 상기 바운딩 박스에 기초하여 상기 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 상기 제1 세트를 추가로정제하는 것을 더 포함하는, 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7032445", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서,상기 추가 오버로드된 출력 컨벌루션 신경망은 상기 제1 신경망과 비교할 때 더 높은 LOD(level of detail)를제공하는 것인, 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-7032445", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "여기에서 설명된 구현은 클라이언트 장치에서 캡처된 입력 비디오로부터 3D 아바타에 대한 애니메이션을 생성하 기 위한 방법, 시스템 및 컴퓨터 판독 가능 매체에 관한 것이다. 트레이닝된 얼굴 검출 모델과 트레이닝된 회귀 모델이 3D 아바타의 애니메이션으로 변환될 FACS 가중치, 머리 포즈 및 얼굴 랜드마크의 세트를 출력하는 동안 카메라는 얼굴의 비디오를 캡처할 수 있다. 추가적으로, 클라이언트 장치에서의 사용자 선호 및/또는 컴퓨팅 조 건에 기초하여 더 높은 LOD(level-of-detail)가 지능적으로 선택될 수 있다."}
{"patent_id": "10-2023-7032445", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 2021년 2월 22일에 출원된 \"비디오로부터 실시간 로버스트 얼굴 애니메이션(Real Time Robust Facial Animation From Video)\"라는 제목의 미국 특허 가출원 번호 63/152,327 및 2021년 2월 23일에 출원된 \"비디오 로부터 실시간 로버스트 얼굴 애니메이션(Real Time Robust Facial Animation From Video)\"라는 제목의 미국 특허 가출원 번호 63/152,819에 대한 우선권의 이익을 주장하며, 각각의 전체 내용은 여기에서 참조로 포함된다. 실시예는 일반적으로 컴퓨터 기반 가상 체험에 관한 것으로, 보다 구체적으로 실시간으로 비디오로부터 로버스 트 얼굴 애니메이션을 위한 방법, 시스템 및 컴퓨터 판독 가능 매체에 관한 것이다."}
{"patent_id": "10-2023-7032445", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일부 온라인 플랫폼(예컨대, 게이밍 플랫폼, 미디어 교환 플랫폼 등)은 사용자가 서로 연결하고, 서로 인터랙팅 하고 (예컨대, 게임 내에서), 게임을 제작하고, 그리고 인터넷을 통해 서로 정보를 공유할 수 있도록 허용한다. 온라인 플랫폼의 사용자는 멀티 플레이어 게이밍 환경 또는 가상 환경(예컨대, 3차원 환경)에 참여하고, 커스텀 게이밍 환경을 디자인하고, 캐릭터 및 아바타를 디자인하고, 아바타를 장식하고, 다른 사용자와 가상 아이템/객 체를 교환하고, 오디오 또는 텍스트 메시지를 사용하여 다른 사용자와 통신하는 것 등을 할 수 있다. 메타버스 또는 멀티버스 환경과 같은 환경은 또한 참여하는 사용자가 자신이 만든 객체를 다른 사용자와 공유, 판매 또는 거래할 수 있도록 한다. 서로 인터랙팅하는 사용자는 사용자의 아바타의 표현(presentation)을 포함하는 인터랙티브 인터페이스를 사용 할 수 있다. 아바타를 애니메이팅하는 것은 통례적으로 제스처, 움직임 및 다른 유사한 미리 구성된 애니메이션 디테일이 요청된 사용자 입력을 갖는 것 및 사용자의 입력에 기초하여 애니메이션을 표시하는 것을 포함할 수 있다. 이러한 종래의 솔루션은 결점을 갖고 있으며, 일부 구현은 위의 관점에서 고안되었다."}
{"patent_id": "10-2023-7032445", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "여기에서 제공된 배경기술 설명은 본 개시의 맥락을 제시하는 목적을 위한 것이다. 현재 명명된 발명자의 작업"}
{"patent_id": "10-2023-7032445", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "은, 이 배경기술 섹션에 설명된 범위 내에서 뿐만 아니라, 출원 당시 선행 기술로 달리 자격이 없을 수 있는 설 명의 양태까지, 명시적으로나 묵시적으로 본 개시에 대한 선행 기술로 인정되지 않는다."}
{"patent_id": "10-2023-7032445", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 4, "content": "발명의 내용이 적용의 구현은 실시간으로 비디오로부터 로버스트 얼굴 애니메이션을 자동으로 생성하는 것과 관련된다. 일 양태에 따르면, 컴퓨터 구현 방법은: 완전 컨벌루션 네트워크를 사용하여, 비디오의 제1 프레임으로부터 바운딩 박스 후보의 세트를 식별하는 단계 - 각각의 바운딩 박스 후보는 얼굴을 포함함 -; 컨벌루션 신경망을 사용하여 바운딩 박스 후보의 세트를 바운딩 박스로 정제하는 단계; 오버로드된 출력 컨볼루션 신경망을 사용하여 제1 프 레임 및 바운딩 박스에 기초하여 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트를 획득하는 단계; 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세 트에 기초하여 3차원(3D) 아바타의 애니메이션의 제1 애니메이션 프레임을 생성하는 단계 - 아바타의 머리 포즈 는 제1 세트의 머리 포즈와 매칭하고, 아바타의 얼굴 랜드마크는 제2 세트의 얼굴 랜드마크와 매칭함 -. 및 제1 프레임에 후속하는 비디오의 각각의 추가 프레임에 대해, 추가 프레임에 적용된 바운딩 박스가 얼굴을 포함하는 지 여부를 검출하는 단계; 바운딩 박스가 얼굴을 포함하는 것으로 검출되면, 오버로드된 출력 컨볼루션 신경망 을 사용하여 추가 프레임 및 바운딩 박스에 기초하여 하나 이상의 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크의 추가 세트를 획득하는 단계; 및 추가 세트를 사용하여 3D 아바타의 애니메이션의 추가 애니메 이션 프레임을 생성하는 단계를 포함한다. 컴퓨터 구현 방법의 다양한 구현 및 변형이 개시된다. 일부 구현에서, 추가 프레임에 대한 바운딩 박스가 얼굴을 포함하지 않은 것으로 검출되면, 추가 프레임을 제1 프레임으로 설정한다. 일부 구현에서, 컴퓨터 구현 방법은 오버로드된 출력 컨볼루션 신경망에 의해, 혀 내밀기 조건을 식별하는 단계 를 더 포함하고, 혀는 얼굴에서 검출된다. 일부 구현에서, 혀 내밀기 조건을 식별하는 단계는 오버로드된 출력 컨벌루션 신경망의 서브모델에 기초한다. 일부 구현에서, 오버로드된 출력 컨벌루션 네트워크는 제1 신경망이고, 방법은 추가 오버로드된 출력 컨볼루션 신경망을 사용하여 제1 프레임 및 바운딩 박스에 기초하여, 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트를 정제하는 단계를 더 포함한다. 일부 구현에서, 추가 오버로드된 출력 컨벌루션 신경망은 제1 신경망과 비교할 때 더 높은 LOD(level of detail)를 제공한다. 일부 구현에서, 컴퓨터 구현 방법은 추가 오버로드된 출력 컨볼루션 신경망에 의해, 혀 내밀기 조건을 식별하는 단계를 더 포함하고, 혀는 얼굴에서 검출된다. 일부 구현에서, 제1 신경망의 입력 프레임 해상도는 추가 오버로드된 출력 컨볼루션 신경망의 입력 프레임 해상 도보다 작다. 일부 구현에서, 오버로드된 출력 컨볼루션 신경망은 인공 비디오의 인공적으로 생성된 입력 프레임의 세트에 대 해 트레이닝된다. 일부 구현에서, 오버로드된 출력 컨벌루션 신경망은 손으로 라벨링되는 실제 이미지의 세트로 추가로 트레이닝 된다. 다른 양태에 따르면, 시스템이 제공된다. 시스템은: 명령어가 저장된 메모리; 및 메모리에 결합된 프로세싱 장 치를 포함하고, 프로세싱 장치는 메모리에 액세스하고 명령어를 실행하도록 구성되며, 명령어는 프로세싱 장치 로 하여금 동작을 수행하게 하고, 동작은: 완전 컨벌루션 네트워크를 사용하여, 비디오의 제1 프레임으로부터 바운딩 박스 후보의 세트를 식별하는 것 - 각각의 바운딩 박스 후보는 얼굴을 포함함 -; 컨벌루션 신경망을 사 용하여, 바운딩 박스 후보의 세트를 바운딩 박스로 정제하는 것; 오버로드된 출력 컨볼루션 신경망을 사용하여 제1 프레임 및 바운딩 박스에 기초하여 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이 상의 제1 세트를 획득하는 것; 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트에 기초하여, 3차원(3D) 아바타의 애니메이션의 제1 애니메이션 프레임을 생성하는 것 - 아바타의 머리 포 즈는 제1 세트의 머리 포즈와 매칭하고, 아바타의 얼굴 랜드마크는 제1 세트의 얼굴 랜드마크와 매칭함 -; 및 제1 프레임에 후속하는 비디오의 각 추가 프레임에 대해, 추가 프레임에 적용된 바운딩 박스가 얼굴을 포함하는 지 여부를 검출하는 것; 바운딩 박스가 얼굴을 포함하는 것으로 검출되면, 오버로드된 출력 컨볼루션 신경망을 사용하여, 추가 프레임 및 바운딩 박스에 기초하여 하나 이상의 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크의 추가 세트를 획득하는 것; 및 추가 세트를 사용하여 3D 아바타의 애니메이션의 추가 애니메이 션 프레임을 생성하는 것을 포함한다.시스템의 다양한 구현 및 변형이 개시된다. 일부 구현에서, 추가 프레임에 대한 바운딩 박스가 얼굴을 포함하지 않는 것으로 검출되면, 추가 프레임을 제1 프레임으로 설정한다. 일부 구현에서, 동작은 오버로드된 출력 컨볼루션 신경망에 의해, 혀 내밀기 조건을 식별하는 것을 더 포함하고, 혀는 얼굴에서 검출된다. 일부 구현에서, 오버로드된 출력 컨벌루션 네트워크는 제1 신경망이고, 동작은 추가 오버로드된 출력 컨볼루션 신경망을 사용하여 제1 프레임 및 바운딩 박스에 기초하여 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트를 추가로 정제하는 것을 더 포함한다. 일부 구현에서, 추가 오버로드된 출력 컨벌루션 신경망은 제1 신경망과 비교할 때 더 높은 LOD(level of detail)를 제공한다. 다른 양태에 따르면, 비일시적 컴퓨터 판독 가능 매체가 제공된다. 저장된 명령어를 갖는 비일시적 컴퓨터 판독 가능 매체는 프로세싱 장치에 의한 실행에 응답하여, 프로세싱 장치로 하여금 동작을 수행하게 하고, 동작은: 완전 컨벌루션 네트워크를 사용하여, 비디오의 제1 프레임으로부터 바운딩 박스 후보의 세트를 식별하는 것 - 각각의 바운딩 박스 후보는 얼굴을 포함함 -; 컨벌루션 신경망을 사용하여, 바운딩 박스 후보의 세트를 바운딩 박스로 정제하는 것; 오버로드된 출력 컨볼루션 신경망을 사용하여 제1 프레임 및 바운딩 박스에 기초하여 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트를 획득하는 것; 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트에 기초하여, 3차원(3D) 아바타의 애니 메이션의 제1 애니메이션 프레임을 생성하는 것 - 아바타의 머리 포즈는 제1 세트의 머리 포즈와 매칭하고, 아 바타의 얼굴 랜드마크는 제1 세트의 얼굴 랜드마크와 매칭함; 및 제1 프레임에 후속하는 비디오의 각 추가 프레 임에 대해, 추가 프레임에 적용된 바운딩 박스가 얼굴을 포함하는지 여부를 검출하는 것; 바운딩 박스가 얼굴을 포함하는 것으로 검출되면, 오버로드된 출력 컨볼루션 신경망을 사용하여, 추가 프레임 및 바운딩 박스에 기초 하여 하나 이상의 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크의 추가 세트를 획득하는 것; 및 추가 세트를 사용하여 3D 아바타의 애니메이션의 추가 애니메이션 프레임을 생성하는 것을 포함한다. 비일시적 컴퓨터 판독 가능 매체의 다양한 구현 및 변형이 개시된다. 일부 구현에서, 추가 프레임에 대한 바운딩 박스가 얼굴을 포함하지 않는 것으로 검출되면, 추가 프레임을 제1 프레임으로 설정한다. 일부 구현에서, 동작은 오버로드된 출력 컨벌루션 신경망에 의해, 혀 내밀기 조건을 식별하는 것을 더 포함하고, 혀는 얼굴에서 검출된다. 일부 구현에서, 오버로드된 출력 컨벌루션 네트워크는 제1 신경망이고, 동작은 추가 오버로드된 출력 컨볼루션 신경망을 사용하여 제1 프레임 및 바운딩 박스에 기초하여 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트를 추가로 정제하는 것을 더 포함한다. 일부 구현에서, 추가 오버로드된 출력 컨벌루션 신경망은 제1 신경망과 비교할 때 더 높은 LOD(level of detail)를 제공한다."}
{"patent_id": "10-2023-7032445", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "여기에서 설명된 하나 이상의 구현은 비디오로부터의 실시간 로버스트 애니메이션에 관한 것이다. 특징은 클라 이언트 장치로부터 수신된 입력 비디오에 기초하여, 3차원(3D) 아바타의 애니메이션을 자동으로 생성하는 것을 포함할 수 있다. 여기에 설명된 특징은 비디오에서 얼굴의 자동 검출, 검출된 얼굴에서 3D 아바타를 애니메이팅하는 데 사용되는 파라미터의 회귀, 파라미터에 기초하는 3D 아바타의 애니메이션 생성을 제공한다. 얼굴 검출 모델은 바운딩 상 자를 정확하게 식별하고, 예측된 파라미터를 제공하도록 트레이닝된다. 유사하게, 회귀 모델은 예측된 파라미터 에 기초하여 사용자의 얼굴을 나타내는 파라미터를 정확하게 생성하도록 트레이닝된다. 그 후, 트레이닝된 모델 은 입력 비디오에서 얼굴을 식별하고, 비디오에 기초하여 3D 아바타의 로버스트 애니메이션을 생성하는 데 사용 될 수 있다. 트레이닝된 모델은 그 연관된 아바타에 대해 자동으로 생성된 애니메이션을 갖기를 원하는 사용자의에 의해, 사 용을 위해 클라이언트 장치에 배치될 수 있다. 클라이언트 장치는 가상 체험 플랫폼과 같은, 온라인 플랫폼과 동작 가능하게 통신하도록 추가로 구성될 수 있으며, 이로써 그 연관된 아바타는 통신 인터페이스에서(예컨대, 비디오 채팅), 가상 체험 내에서(예컨대, 대표적인 가상 신체의 충분히 애니메이팅된 얼굴), 다른 사용자에게 송신된 애니메이팅된 비디오 내에서(예컨대, 채팅 기능 또는 다른 기능을 통해 애니메이팅된 아바타의 기록을 전송하여), 그리고 온라인 플랫폼의 다른 부분 내에서, 표현을 위해 충분히 애니메이팅될 수 있다. 온라인 가상 체험 플랫폼(\"사용자 생성 콘텐츠 플랫폼\" 또는 \"사용자 생성 콘텐츠 시스템\"이라고도 지칭됨)은 사용자가 서로 인터랙팅하는 다양한 방식을 제공한다. 예컨대, 온라인 가상 체험 플랫폼의 사용자는 플랫폼 내 에서 체험, 게임 또는 기타 콘텐츠 또는 리소스(예컨대, 캐릭터, 그래픽, 가상 세계 내에서의 게임 플레이를 위 한 아이템 등)를 생성할 수 있다. 온라인 가상 체험 플랫폼의 사용자는 게임이나 게임 제작에서 공통 목표를 향해 함께 작업하고, 다양한 가상 아 이템을 공유하고, 서로에게 전자 메시지를 전송하는 것 등을 할 수 있다. 온라인 가상 체험 플랫폼의 사용자는 환경과 인터랙팅하고, 예컨대, 캐릭터(아바타) 또는 기타 게임 객체 및 메커니즘을 포함하는, 게임을 플레이할 수 있다. 온라인 가상 체험 플랫폼은 또한 플랫폼의 사용자들이 서로 소통할 수 있도록 허용할 수 있다. 예컨대, 온라인 가상 체험 플랫폼의 사용자는 음성 메시지(예컨대, 음성 채팅을 통해), 텍스트 메시지, 비디오 메시지 또는 상기 조합을 사용하여 서로 통신할 수 있다. 일부 온라인 가상 체험 플랫폼은 사용자가 아바타 또 는 자신의 가상 표현을 사용하여 자신을 표현할 수 있는 가상 3차원 환경을 제공할 수 있다. 온라인 가상 체험 플랫폼의 엔터테인먼트 가치를 향상시키도록 돕기 위해, 플랫폼은 아바타를 자동으로 애니메 이팅하는 것을 용이하게 하기 위한 애니메이션 엔진을 제공할 수 있다. 애니메이션 엔진은 사용자가 예컨대, 클 라이언트 장치로부터 송신된 라이브 비디오 피드에 기초하여, 아바타의 얼굴 또는 신체의 애니메이션을 포함하 는, 애니메이션에 대한 옵션을 요청하거나 선택할 수 있도록 허용할 수 있다. 예컨대, 사용자는 온라인 가상 체험 플랫폼과 연관된 사용자 장치의 애플리케이션에 의한 카메라 액세스를 허용 할 수 있다. 카메라에서 생성된 비디오는 제스처 또는 추출된 제스처에 기초하여 아바타의 애니메이션을 용이하 게 하는 다른 정보를 추출하도록 해석될 수 있다. 유사하게, 사용자는 지시된 컨트롤의 입력을 통해, 다른 신체 부위를 이동시키거나, 얼굴 제스처를 과장함으로써, 얼굴 애니메이션을 증강할 수 있다. 그러나, 종래의 솔루션에서는 많은 모바일 클라이언트 장치에서 충분한 컴퓨팅 리소스의 부족으로 인해, 비디오 프로세싱이 제한된다. 예컨대, 많은 사용자는 애니메이션을 정확하게 생성하기 위해 제스처를 신속하게 해석하 는 데 충분한 계산 능력이 부족한 휴대용 컴퓨팅 장치(예컨대, 초경량 휴대용 기기, 태블릿, 휴대폰 등)를 사용 할 수 있다. 이러한 상황에서, 많은 자동 애니메이션은 얼굴 지터(facial jitter), 응답성 부족, 시각적 큐의 잘못한 번역 및 기타 단점과 같은, 바람직하지 않은 아티팩트를 도입할 수 있다. 이러한 시나리오에서, 사용자 는 그들을 위해 생성된 자동 애니메이션을 갖지 못할 수 있으며, 대신 그들의 아바타에 상대적으로 고정된 얼굴 특징을 갖거나, 다른 수단을 통해 제어 제스처를 입력하여 애니메이션을 생성하도록 요구될 수 있다. 따라서, 일부 사용자는 종래의 솔루션을 통해 로버스트 애니메이션을 처리하기에 충분한 프로세싱 능력을 갖춘 모바일 장치 또는 컴퓨팅 장치를 획득할 수 있고, 다른 사용자는 애니메이션 제스처를 직접 입력할 수 있는 반 면, 많은 사용자는 복잡한 컴퓨터 비전 프로세싱을 위한 충분한 계산 리소스가 부족한 달리 적절한 장치의 활용 으로 인해, 이러한 경험이 부족할 수 있다. 도 1: 시스템 아키텍처 도 1은 본 개시의 일부 구현에 따른, 예시적인 네트워크 환경을 예시한다. 네트워크 환경(여기에서 \"시스템\"이라고도 지칭됨)은 온라인 가상 체험 플랫폼, 제1 클라이언트 장치, 제2 클라이언트 장치 (일반적으로 여기에서 \"클라이언트 장치(110/116)\"으로 지칭됨)를 포함하며, 모두 네트워크를 통해 연결된다. 온라인 가상 체험 플랫폼은 그 중에서도, 가상 체험(VE) 엔진, 하나 이상의 가상 체험 , 통신 엔진, 얼굴 애니메이션 엔진, 및 데이터 저장소를 포함할 수 있다. 클라이언트 장 치는 가상 체험 애플리케이션을 포함할 수 있다. 사용자(114 및 120)는 온라인 가상 체험 플랫폼 과 인터랙팅하기 위해 각각 클라이언트 장치(110 및 116)를 사용할 수 있다. 네트워크 환경은 예시를 위해 제공된다. 일부 구현에서, 네트워크 환경은 도 1에 도시된 것과 동일하 거나 상이한 방식으로 구성된 동일하거나, 더 적거나, 더 많거나, 상이한 요소를 포함할 수 있다. 일부 구현에서, 네트워크는 공중 네트워크(예컨대, 인터넷), 사설 네트워크(예컨대, LAN(local area network) 또는 WAN(wide area network)), 유선 네트워크(예컨대, 이더넷 네트워크), 무선 네트워크(예컨대, 802.11 네트워크, Wi-Fi ® 네트워크 또는 WLAN(wireless LAN)), 셀룰러 네트워크(예컨대, LTE(Long Term Evolution) 네트워크), 라우터, 허브, 스위치, 서버 컴퓨터 또는 이들의 조합을 포함할 수 있다. 일부 구현에서, 데이터 저장소는 비일시적 컴퓨터 판독 가능 메모리(예컨대, 랜덤 액세스 메모리), 캐시, 드라이브(예컨대, 하드 드라이브), 플래시 드라이브, 데이터베이스 시스템, 또는 데이터를 저장할 수 있는 다른 유형의 컴포넌트 또는 장치일 수 있다. 데이터 저장소는 또한 다중 컴퓨팅 장치(예컨대, 다중 서버 컴퓨터)에 걸쳐 있을 수도 있는 다중 저장 컴포넌트(예컨대, 다중 드라이브 또는 다중 데이터베이스)를 포함할 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼은 하나 이상의 컴퓨팅 장치(예컨대, 클라우드 컴퓨팅 시스템, 랙 마운트 서버, 서버 컴퓨터, 물리적 서버의 클러스터, 가상 서버 등)를 갖는 서버를 포함할 수 있다. 일부 구현 에서, 서버는 온라인 가상 체험 플랫폼에 포함될 수 있거나, 독립 시스템일 수 있거나, 다른 시스템 또는 플랫폼의 일부일 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼은 하나 이상의 컴퓨팅 장치(예컨대, 랙마운트 서버, 라우터 컴퓨 터, 서버 컴퓨터, 개인용 컴퓨터, 메인프레임 컴퓨터, 랩톱 컴퓨터, 태블릿 컴퓨터, 데스크톱 컴퓨터 등), 데이 터 저장소(예컨대, 하드 디스크, 메모리, 데이터베이스), 네트워크, 소프트웨어 컴포넌트 및/또는 온라인 가상 체험 플랫폼에서 동작을 수행하고 사용자에게 온라인 가상 체험 플랫폼에 대한 액세스를 제공하는 데 사용될 수 있는 하드웨어 컴포넌트를 포함할 수 있다. 온라인 가상 체험 플랫폼은 또한 온라인 가상 체험 플랫폼에 의해 제공되는 콘텐츠에 대한 액세스를 사용자에게 제공하는 데 사용될 수 있는 웹사이트(예컨대, 하나 이상의 웹페이지) 또는 애플리케이션 백엔드 소프트웨어를 포함할 수 있다. 예컨대, 사 용자는 각각 클라이언트 장치(110/116) 상의 가상 체험 애플리케이션(112/118)을 사용하여, 온라인 가상 체험 플랫폼에 액세스할 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼은 사용자 사이의 연결을 제공하는 소셜 네트워크의 유형 또는 사 용자(예컨대, 최종 사용자 또는 소비자)가 온라인 가상 체험 플랫폼을 통해 다른 사용자와 통신할 수 있도 록 허용하는 사용자 생성 콘텐츠 시스템의 유형을 포함할 수 있고, 여기에서 통신은 음성 채팅(예컨대, 동기 및 /또는 비동기 음성 통신), 비디오 채팅(예컨대, 동기 및/또는 비동기 비디오 통신), 또는 텍스트 채팅(예컨대, 동기 및/또는 비동기 텍스트 기반 통신)을 포함할 수 있다. 본 개시의 일부 구현에서, \"사용자\"는 단일 개인으 로 표현될 수 있다. 그러나, 본 개시의 다른 구현은 사용자의 세트 또는 자동화된 소스에 의해 제어되는 엔티티 인 \"사용자\"(예컨대, 사용자 생성)를 포함한다. 예컨대, 사용자 생성 콘텐츠 시스템에서 커뮤니티 또는 그룹으 로 연합된 개별 사용자의 세트는 \"사용자\"로 간주될 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼은 가상 게이밍 플랫폼일 수 있다. 예컨대, 게이밍 플랫폼은 네트 워크를 통해 클라이언트 장치(110/116)를 사용하여 게임(예컨대, 사용자 생성 게임 또는 다른 게임)에 액 세스하거나 인터랙팅할 수 있는 사용자의 커뮤니티에 싱글 플레이어 또는 멀티 플레이어 게임을 제공할 수 있다. 일부 구현에서, 게임(여기에서 “비디오 게임”, “온라인 게임\" 또는 \"가상 게임\"이라고도 지칭됨)은 예컨대, 2차원(2D) 게임, 3차원(3D) 게임(예컨대, 3D 사용자 생성 게임), 가상 현실(VR) 게임 또는 증강 현실(AR) 게임일 수 있다. 일부 구현에서, 사용자는 게임 및 게임 아이템을 검색하고, 하나 이상의 게임에서 다른 사용자 와 게임플레이에 참여할 수 있다. 일부 구현에서, 게임은 게임의 다른 사용자와 실시간으로 플레이될 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼 대신에 또는 추가로 여기에서 설명된 로버스트 애니메이션 특징 과 함께 다른 협업 플랫폼이 사용될 수 있다. 예컨대, 소셜 네트워킹 플랫폼, 비디오 채팅 플랫폼, 메시징 플랫 폼, 사용자 콘텐츠 생성 플랫폼, 가상 회의 플랫폼 등은 사용자의 얼굴 움직임을 가상 아바타에 신속하고, 로버 스트하며, 정확하게 표현하는 것을 용이하게 하기 위해 여기에서 설명된 로버스트 애니메이션 특징과 함께 사용 될 수 있다. 일부 구현에서, \"게임플레이\"는 게임 또는 체험(예컨대, VE) 내에서 클라이언트 장치(예컨대, 110 및/또는 116)를 사용하는 하나 이상의 플레이어의 인터랙션 또는 클라이언트 장치(110 또는 116)의 디스플레이 또는 다 른 출력 장치 상의 인터랙션의 표시를 지칭할 수 있다. 하나 이상의 가상 체험은 온라인 가상 체험 플랫폼에 의해 제공된다. 일부 구현에서, 가상 체험은 가 상 콘텐츠(예컨대, 디지털 미디어 아이템)를 엔티티에 제시하도록 구성된 소프트웨어, 펌웨어 또는 하드웨어를 사용하여 실행되거나 로드될 수 있는 전자 파일을 포함할 수 있다. 일부 구현에서, 가상 체험 애플리케이션 (112/118)은 실행될 수 있고, 가상 체험은 가상 체험 엔진과 관련하여 렌더링될 수 있다. 일부 구현 에서, 가상 체험은 규칙의 공통 세트 또는 공통 목표를 가질 수 있고, 가상 체험의 환경은 규칙의 공 통 세트 또는 공통 목표를 공유한다. 일부 구현에서, 상이한 가상 체험은 서로 상이한 규칙 또는 목표를 가질 수 있다. 유사하게 또는 대안적으로, 일부 가상 체험은 임의의 사회적 방식으로 사용자 간의 인터랙션을 의도하 는 목표가 전혀 없을 수 있다. 일부 구현에서, 가상 체험은 다중 환경이 링크될 수 있는 하나 이상의 환경(여기에서, \"게이밍 환경\" 또는 \"가 상 환경\"이라고도 지칭됨)을 가질 수 있다. 환경의 예시는 3차원(3D) 환경일 수 있다. 가상 체험의 하나 이상의 환경은 여기에서 \"세계\" 또는 \"가상 세계\" 또는 \"가상 유니버스\" 또는 \"메타버스\"로 집합적으로 지칭될 수 있다. 세계의 예시는 가상 체험의 3D 세계일 수 있다. 예컨대, 사용자는 다른 사용자에 의해 생성된 다 른 가상 환경에 링크되는 가상 환경을 구축할 수 있다. 가상 체험의 캐릭터는 가상 경계를 넘어 인접한 가상 환 경으로 들어갈 수 있다. 3D 환경 또는 3D 세계는 가상 콘텐츠를 나타내는 기하학적 데이터의 3차원 표현을 사용하는 그래픽을 사용한다 는 점에 유의할 수 있다 (또는 기하학적 데이터의 3D 표현이 사용되는지 여부에 관계없이 적어도 현재 콘텐츠가 3D 콘텐츠로 나타남). 2D 환경 또는 2D 세계는 가상 콘텐츠를 나타내는 기하학적 데이터의 2차원 표현을 사용하 는 그래픽을 사용한다. 일부 구현에서, 온라인 가상 체험 플랫폼은 하나 이상의 가상 체험을 호스팅할 수 있고, 사용자가 클 라이언트 장치(110/116)의 가상 체험 애플리케이션(112/118)을 사용하여 가상 체험과 인터랙팅하는 것(예 컨대, 게임, VE 관련 콘텐츠 또는 다른 콘텐츠를 검색)을 허가할 수 있다. 온라인 가상 체험 플랫폼의 사 용자(예컨대, 114 및/또는 120)는 가상 체험을 플레이, 생성, 인터랙팅 또는 구축하고, 가상 체험을 검색하고, 다른 사용자와 통신하고, 가상 체험의 객체(예컨대, 여기에서 “아이템(들)” 또는 “게임 객체 ” 또는 “가상 게임 아이템(들)이라고도 지칭됨)를 생성 및 구축하고, 그리고/또는 객체를 검색할 수 있다. 예 컨대, 사용자 생성 가상 아이템을 생성하는 데 있어, 사용자는 그 중에서도 캐릭터, 캐릭터를 위한 장식, 인터 랙티브 체험을 위한 하나 이상의 가상 환경을 생성하거나, 가상 체험에서 사용된 구조를 구축할 수 있다. 일부 구현에서, 사용자는 플랫폼 내 화폐(예컨대, 가상 화폐)와 같은, 가상 객체를 온라인 가상 체험 플랫폼 의 다른 사용자와 구매, 판매 또는 거래할 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼은 가상 콘텐츠를 가상 체험 애플리케이션(예컨대, 112, 118)으로 전송할 수 있다. 일부 구현에서, 가상 콘텐츠(여기에 서 \"콘텐츠\"라고도 지칭됨)는 온라인 가상 체험 플랫폼 또는 가상 체험 애플리케이션과 연관된 임의의 데 이터 또는 소프트웨어 명령어(예컨대, 가상 객체, 체험, 사용자 정보, 비디오, 이미지, 명령, 미디어 아이템 등)를 지칭할 수 있다. 일부 구현에서, 가상 객체(예컨대, 여기에서 \"아이템(들)\" 또는 \"객체\" 또는 \"가상 게임 아이템(들)\"이라고도 지칭됨)는 가상 체험 플랫폼의 가상 체험 애플리케이션 또는 클라이언트 장치(110/116)의 가상 체험 애플리케이션(112 또는 118)에 사용되거나, 생성되거나, 공유되거나, 그 외 도시되는 객체를 지칭할 수 있다. 예컨대, 가상 객체는 부품, 모델, 캐릭터, 도구, 무기, 의복, 건물, 차량, 화폐, 식물군, 동물군, 전술한 것의컴포넌트(예컨대, 건물의 창) 등을 포함할 수 있다. 가상 체험을 호스팅하는 온라인 가상 체험 플랫폼은 제한이 아니라 예시의 목적을 위해 제공된다는 점에 유의할 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼은 한 사용자로부터 하나 이상의 다른 사용 자로의 통신 메시지를 포함할 수 있는 하나 이상의 미디어 아이템을 호스팅할 수 있다. 미디어 아이템은 디지털 비디오, 디지털 영화, 디지털 사진, 디지털 음악, 오디오 콘텐츠, 멜로디, 웹사이트 콘텐츠, 소셜 미디어 업데 이트, 전자 서적, 전자 잡지, 디지털 신문, 디지털 오디오 북, 전자 저널, 웹 블로그, RSS(real simple syndication) 피드, 전자 만화책, 소프트웨어 애플리케이션 등을 포함할 수 있지만, 이에 제한되지는 않는다. 일부 구현에서, 미디어 아이템은 엔티티에 디지털 미디어 아이템을 표시하도록 구성된 소프트웨어, 펌웨어 또는 하드웨어를 사용하여 실행되거나 로드될 수 있는 전자 파일일 수 있다. 일부 구현에서, 가상 체험은 특정 사용자 또는 사용자의 특정 그룹과 연관되거나 (예컨대, 비공개 체험), 온라인 가상 체험 플랫폼의 사용자에게 널리 이용 가능하게 만들어질 수 있다 (예컨대, 공개 체험). 온라 인 가상 체험 플랫폼이 하나 이상의 가상 체험을 특정 사용자 또는 사용자 그룹과 연관시키는 일부 구현에서, 온라인 가상 체험 플랫폼은 사용자 계정 정보(예컨대, 사용자 이름 및 비밀번호와 같은 사용자 계정 식별자)를 사용하여 특정 사용자(들)를 가상 체험과 연관시킬 수 있다. 유사하게, 일부 구현에서, 온 라인 가상 체험 플랫폼은 개발자 계정 정보(예컨대, 사용자 이름 및 비밀번호와 같은 개발자 계정 식별 자)를 사용하여 특정 개발자 또는 개발자 그룹을 가상 체험과 연관시킬 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼 또는 클라이언트 장치(110/116)는 가상 체험 엔진 또는 가 상 체험 애플리케이션(112/118)을 포함할 수 있다. 가상 체험 엔진은 가상 체험 애플리케이션(112/118)과 유사한 가상 체험 애플리케이션을 포함할 수 있다. 일부 구현에서, 가상 체험 엔진은 가상 체험의 개 발 또는 실행을 위해 사용될 수 있다. 예컨대, 가상 체험 엔진은 다른 특징 중에서, 2D, 3D, VR 또는 AR 그래픽을 위한 렌더링 엔진(\"렌더러(renderer)\"), 물리 엔진, 충돌 검출 엔진(및 충돌 응답), 사운드 엔진, 스 크립팅 기능, 애니메이션 엔진, 인공 지능 엔진, 네트워킹 기능, 스트리밍 기능, 메모리 관리 기능, 스레딩 기 능(threading functionality), 장면 그래프 기능 또는 시네마틱에 대한 비디오 지원을 포함할 수 있다. 가상 체 험 엔진의 컴포넌트는 가상 체험을 컴퓨팅하고 렌더링하는 것을 돕는 명령(예컨대, 렌더링 명령, 충돌 명 령, 물리 명령 등)을 생성할 수 있다. 일부 구현에서, 클라이언트 장치(110/116)의 가상 체험 애플리케이션 (112/118)은 각각 독립적으로, 온라인 가상 체험 플랫폼의 가상 체험 엔진과 협력하여, 또는 이 둘의 조합으로 작동할 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼 및 클라이언트 장치(110/116) 모두는 가상 체험 엔진(각각, 104, 112 및 118)을 실행할 수 있다. 가상 체험 엔진을 사용하는 온라인 가상 체험 플랫폼은 일부 또는 모 든 가상 체험 엔진 기능(예컨대, 물리 명령, 렌더링 명령 생성 등)을 수행하거나, 일부 또는 모든 가상 체험 엔 진 기능을 클라이언트 장치의 가상 체험 엔진에 오프로드할 수 있다. 일부 구현에서, 각각의 가상 체 험은 온라인 가상 체험 플랫폼에서 수행되는 가상 체험 엔진 기능과 클라이언트 장치(110 및 116)에 서 수행되는 가상 체험 엔진 기능 사이에 상이한 비율을 가질 수 있다. 예컨대, 온라인 가상 체험 플랫폼의 가상 체험 엔진은 적어도 2개의 게임 객체 사이에 충돌이 있는 경우 물리 명령을 생성하는 데 사용될 수 있는 반면, 추가 가상 체험 엔진 기능(예컨대, 렌더링 명령 생성)은 클라이언트 장치로 오프로드될 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼 및 클라이언트 장 치 상에서 수행되는 가상 체험 엔진 기능의 비율은 인터랙팅성 조건에 기초하여 (예컨대, 동적으로) 변경 될 수 있다. 예컨대, 가상 체험에 참여하는 사용자의 수가 임계 수를 초과하는 경우, 온라인 가상 체험 플 랫폼은 클라이언트 장치(110 또는 116)에 의해 이전에 수행되었던 하나 이상의 가상 체험 엔진 기능을 수 행할 수 있다. 예컨대, 사용자는 클라이언트 장치(110 및 116)에서 가상 체험과 인터랙팅할 수 있고, 제어 명령어(예컨대, 오른쪽, 왼쪽, 위, 아래, 사용자 선택, 또는 캐릭터 위치 및 속도 정보 등과 같은 사용자 입 력)를 온라인 가상 체험 플랫폼에 전송할 수 있다. 클라이언트 장치(110 및 116)로부터 제어 명령어를 수 신한 후, 온라인 가상 체험 플랫폼은 인터랙션 명령어(예컨대, 가상 체험에 참여하는 캐릭터의 위치 및 속 도 정보 또는 렌더링 명령, 충돌 명령과 같은 명령 등)를 제어 명령어에 기초하여 클라이언트 장치(110 및 11 6)에 전송할 수 있다. 예컨대, 온라인 가상 체험 플랫폼은 클라이언트 장치(110 및 116)에 대한 인터랙션 명령어를 생성하기 위해 제어 명령어에 대한 (예컨대, 가상 체험 엔진을 사용하는) 하나 이상의 논리 연산 을 수행할 수 있다. 다른 경우에서, 온라인 가상 체험 플랫폼은 하나의 클라이언트 장치로부터 가상체험에 참여하는 다른 클라이언트 장치(예컨대, 116)로 제어 명령어 중 하나 이상을 전달할 수 있다. 클라 이언트 장치(110 및 116)는 명령어를 사용하고, 클라이언트 장치(110 및 116)의 디스플레이 상에서의 표시를 위 해 체험을 렌더링할 수 있다. 일부 구현에서, 제어 명령어는 사용자의 캐릭터 또는 아바타의 체험 내 활동을 나타내는 명령어를 지칭할 수 있 다. 예컨대, 제어 명령어는 오른쪽, 왼쪽, 위, 아래, 사용자 선택, 자이로스코프 위치 및 방향 데이터, 힘 센서 데이터 등과 같은 체험 내 활동을 제어하기 위한 사용자 입력을 포함할 수 있다. 제어 명령어는 캐릭터 위치 및 속도 정보를 포함할 수 있다. 일부 구현에서, 제어 명령어는 온라인 가상 체험 플랫폼으로 직접 전송된다. 다른 구현에서, 제어 명령어는 클라이언트 장치로부터 다른 클라이언트 장치(예컨대, 116)로 전송될 수 있 으며, 여기에서 다른 클라이언트 장치는 로컬 가상 체험 엔진을 사용하여 플레이 명령어를 생성한다. 제어 명령어는 음성 통신 메시지 또는 오디오 장치(예컨대, 스피커, 헤드폰 등)에서 다른 사용자로부터의 다른 사운 드를 플레이하고, 캐릭터 또는 아바타를 이동하기 위한 명령어, 및 다른 명령어를 포함할 수 있다. 일부 구현에서, 인터랙션 또는 플레이 명령어는 클라이언트 장치(또는 116)가 멀티 플레이어 게임과 같은, 가상 체험의 요소의 이동을 렌더링할 수 있도록 허용하는 명령어를 지칭할 수 있다. 명령어는 사용자 입력(예컨 대, 제어 명령어), 캐릭터 위치 및 속도 정보, 또는 명령(예컨대, 물리 명령, 렌더링 명령, 충돌 명령 등) 중 하나 이상을 포함할 수 있다. 여기에서 보다 상세히 설명되는 바와 같이, 다른 명령어는 가상 아바타의 대표 가 상 얼굴의 애니메이션을 실시간으로 지시하기 위해, 사용자의 얼굴의 입력 비디오 분석을 통해 추출된 얼굴 애 니메이션 명령어를 포함할 수 있다. 따라서, 인터랙션 명령어는 캐릭터의 일부 신체 동작을 직접 제어하기 위해 사용자에 의한 입력을 포함할 수 있지만, 인터랙션 명령어는 사용자의 비디오에서 추출한 제스처 또한 포함할 수 있다. 일부 구현에서, 캐릭터(또는 일반적으로 가상 객체)는 컴포넌트로부터 구성되고, 그 중 하나 이상이 사용자에 의해 선택될 수 있으며, 이는 편집에서 사용자를 돕기 위해 자동으로 함께 결합된다. 하나 이상의 캐릭터(여기 에서, \"아바타\" 또는 \"모델\"이라고도 지칭됨)는 가상 체험과의 사용자 인터랙션을 용이하게 하기 위해 사 용자가 캐릭터를 제어할 수 있는 사용자와 연관될 수 있다. 일부 구현에서, 캐릭터는 신체 일부(예컨대, 머리카 락, 팔, 다리 등) 및 액세서리(예컨대, 티셔츠, 안경, 장식 이미지, 도구 등)와 같은 컴포넌트를 포함할 수 있 다. 일부 구현에서, 커스터마이징 가능한 캐릭터의 신체 부분은 그 중에서도, 머리 유형, 신체 부분 유형(팔, 다리, 몸통 및 손), 얼굴 유형, 머리카락 유형 및 피부 유형을 포함한다. 일부 구현에서, 커스터마이징 가능한 액세서리는 의복(예컨대, 셔츠, 바지, 모자, 신발, 안경 등), 무기 또는 기타 도구를 포함한다. 일부 구현에서, 사용자는 또한 캐릭터의 크기(예컨대, 높이, 너비 또는 깊이) 또는 캐릭터 컴포넌트의 크기를 제어할 수 있다. 일부 구현에서, 사용자는 캐릭터의 비율(예컨대, 뭉툭한(blocky), 해부학적 등)을 제어할 수 있다. 일부 구현에서, 캐릭터는 캐릭터 객체(예컨대, 신체 부위 등)를 포함하지 않을 수 있지만, 사용자는 게임 (예컨대, 렌더링된 캐릭터 게임 객체가 없지만, 사용자가 게임 내 활동을 제어하기 위해 여전히 캐릭터를 제어 하는 퍼즐 게임)과 사용자의 인터랙션을 용이하게 하기 위해, (캐릭터 객체 없이) 캐릭터를 제어할 수 있다. . 일부 구현에서, 신체 부분과 같은 컴포넌트는 블록, 실린더, 구 등과 같은 기본 기하학적 형상 또는 쐐기 (wedge), 원환체(torus), 튜브, 채널 등 일부 다른 기본 형상일 수 있다. 일부 구현에서, 생성자 모듈은 온라인 가상 체험 플랫폼의 다른 사용자가 보거나 사용할 수 있도록 사용자의 캐릭터를 공개할 수 있다. 일부 구 현에서, 캐릭터, 다른 가상 객체, 가상 체험 또는 가상 환경을 생성하고, 수정하고, 커스터마이징하는 것 은 사용자 인터페이스(예컨대, 개발자 인터페이스)를 사용하고, 스크립팅이 있거나 없는 (또는 API(application programming interface)가 있거나 없는), 사용자에 의해 수행될 수 있다. 제한보다는 예시의 목적으로 캐릭터가 휴머노이드 형태를 갖는 것으로 설명된다는 점에 유의할 수 있다. 캐릭터는 차량, 동물, 무생물 객체 또는 기타 창의적인 형태와 같은 임의의 형태를 가질 수 있음을 더 유의할 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼은 사용자에 의해 생성된 캐릭터를 데이터 저장소에 저장할 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼은 가상 체험 엔진, 가상 체험 및/또는 클라 이언트 장치(110/116)를 통해 사용자에게 제시될 수 있는 캐릭터 카탈로그 및 체험 카탈로그를 유지한다. 일부 구현에서, 체험 카탈로그는 온라인 가상 체험 플랫폼에 저장된 상이한 체험의 이미지를 포함한다. 또한, 사용자는 선택된 체험에 참여하기 위해 캐릭터 카탈로그로부터 캐릭터(예컨대, 사용자 또는 다른 사용자에 의해 생성된 캐릭터)를 선택할 수 있다. 캐릭터 카탈로그는 온라인 가상 체험 플랫폼에 저장된 캐릭터의 이미지 를 포함한다. 일부 구현에서, 캐릭터 카탈로그의 캐릭터 중 하나 이상은 사용자에 의해 생성되거나 커스터마이 징될 수 있다. 일부 구현에서, 선택된 캐릭터는 캐릭터의 컴포넌트 중 하나 이상을 정의하는 캐릭터 설정을 가질 수 있다. 일부 구현에서, 사용자의 캐릭터는 컴포넌트의 구성을 포함할 수 있으며, 여기에서 컴포넌트의 구성 및 외관, 보다 일반적으로 캐릭터의 외관은 캐릭터 설정에 의해 정의될 수 있다. 일부 구현에서, 사용자 캐릭터의 캐릭터 설정은 적어도 부분적으로 사용자에 의해 선택될 수 있다. 다른 구현에서, 사용자는 디폴트 캐릭터 설정 또는 다른 사용자에 의해 선택된 캐릭터 설정으로 캐릭터를 선택할 수 있다. 예컨대, 사용자는 미리 정의된 캐릭터 설정을 갖는 캐릭터 카탈로그에서 디폴트 캐릭터를 선택할 수 있으며, 캐릭터 설정 중 일부를 변경하여 (예컨대, 커스터마이징된 로고가 있는 셔츠를 추가함), 디폴트 캐릭터를 추가로 커스터마이징할 수 있다. 캐릭 터 설정은 온라인 가상 체험 플랫폼에 의해 특정 캐릭터와 연관될 수 있다. 일부 구현에서, 클라이언트 장치(들)(110 또는 116)는 각각 개인용 컴퓨터(PC), 모바일 장치(예컨대, 랩톱, 휴 대폰, 스마트폰, 태블릿 컴퓨터 또는 넷북 컴퓨터), 네트워크 연결된 텔레비전, 게이밍 콘솔 등과 같은 컴퓨팅 장치를 포함할 수 있다. 일부 구현에서, 클라이언트 장치(110 또는 116)는 또한 \"사용자 장치\"로 지칭될 수 있 다. 일부 구현에서, 하나 이상의 클라이언트 장치(110 또는 116)는 임의의 주어진 순간에 온라인 가상 체험 플 랫폼에 접속할 수 있다. 클라이언트 장치(110 또는 116)의 수는 제한보다는 예시로서 제공된다는 점에 유 의할 수 있다. 일부 구현에서, 임의의 수의 클라이언트 장치(110 또는 116)가 사용될 수 있다. 일부 구현에서, 각각의 클라이언트 장치(110 또는 116)는 가상 체험 애플리케이션(112 또는 118)의 인스턴스를 각각 포함할 수 있다. 일 구현에서, 가상 체험 애플리케이션(112 또는 118)은 사용자가 특정 체험 또는 기타 콘 텐츠를 검색하거나,온라인 가상 체험 플랫폼에 의해 호스팅되는 가상 게임에서 가상 캐릭터를 제어하거나, 가상 체험, 이미지, 비디오 아이템, 웹 페이지, 문서 등과 같은 콘텐츠를 보거나 업로드하는 것과 같이, 온라인 가상 체험 플랫폼을 사용하고 인터랙팅하는 것을 허가할 수 있다. 일 예시에서, 가상 체험 애플리 케이션은 웹 서버에 의해 제공된 콘텐츠(예컨대, 가상 환경의 가상 캐릭터 등)에 액세스, 검색, 제시 또는 탐색 할 수 있는 웹 애플리케이션(예컨대, 웹 브라우저와 함께 동작하는 애플리케이션)일 수 있다. 다른 예시에서, 가상 체험 애플리케이션은 클라이언트 장치(110 또는 116)에 설치 및 로컬로 실행되고, 사용자가 온라인 가상 체험 플랫폼과 인터랙팅할 수 있도록 허용하는 네이티브 애플리케이션(예컨대, 모바일 애플리케이션, 앱, 또는 프로그램)일 수 있다. 가상 체험 애플리케이션은 콘텐츠(예컨대, 웹 페이지, 사용자 인터페이스, 미디어 뷰어)를 사용자에게 렌더링, 디스플레이 또는 제시할 수 있다. 구현에서, 가상 체험 애플리케이션은 또한 웹 페 이지에 임베디드된 임베디드 미디어 플레이어(예컨대, Flash ® 플레이어)를 포함할 수 있다. 본 개시의 양태에 따르면, 가상 체험 애플리케이션(112/118)은 사용자가 온라인 가상 체험 플랫폼과 인터 랙팅할 뿐만 아니라(예컨대, 온라인 가상 체험 플랫폼에 의해 호스팅되는 가상 체험을 플레이하고 인 터랙팅함), 콘텐츠를 온라인 가상 체험 플랫폼에 구축하고, 생성하고, 편집하고, 업로드하는 온라인 가상 체험 플랫폼 애플리케이션일 수 있다. 이와 같이, 가상 체험 애플리케이션(112/118)은 온라인 가상 체험 플랫폼 에 의해 클라이언트 장치(110 또는 116)에 제공될 수 있다. 다른 예시에서, 가상 체험 애플리케이션 (112/118)은 서버로부터 다운로드되는 애플리케이션일 수 있다. 일부 구현에서, 사용자는 가상 체험 애플리케이션을 통해 온라인 가상 체험 플랫폼에 로그인할 수 있다. 사용자는 사용자 계정이 온라인 가상 체험 플랫폼의 하나 이상의 가상 체험에 참여할 수 있는 하나 이상의 캐릭터와 연관되는 사용자 계정 정보(예컨대, 사용자 이름 및 비밀번호)를 제공함으로써 사용자 계정에 액세스할 수 있다. 일반적으로, 온라인 가상 체험 플랫폼에 의해 수행되는 것으로 설명된 기능은 또한 적절한 경우 다른 구현 에서 클라이언트 장치(들)(110 또는 116) 또는 서버에 의해 수행될 수 있다. 또한, 특정 컴포넌트에 부여된 기 능은 함께 동작하는 상이한 또는 다수의 컴포넌트에 의해 수행될 수 있다. 온라인 가상 체험 플랫폼은 또 한 적절한 API(application programming interface)를 통해 다른 시스템 또는 장치에 제공되는 서비스로서 액 세스될 수 있으며, 따라서 웹사이트에서의 사용에 제한되지 않는다. 일부 구현에서, 온라인 가상 체험 플랫폼은 통신 엔진을 포함할 수 있다. 일부 구현에서, 통신 엔진 은 비디오 통신 기능을 사용자에게 제공하기 위한 온라인 가상 체험 플랫폼을 허용하는 시스템, 애플 리케이션 또는 모듈일 수 있으며, 기능은 사용자가 온라인 가상 체험 플랫폼 및 자신의 연관된 가상 표현 을 사용하여 가상 채팅 또는 가상 비디오 회의를 체험할 수 있게 허용한다. 예컨대, 사용자는 가상 아바타를 디 자인 및 구축하고, 채팅 기능을 통해 가상 아바타를 사용할 수 있다. 일부 구현에서, 온라인 가상 체험 플랫폼은 얼굴 애니메이션 엔진을 포함할 수 있다. 일부 구현에서, 얼굴 애니메이션 엔진은 사용자의 아바타 또는 캐릭터 얼굴의 로버스트 실시간 애니메이션을 생성하기 위 해 얼굴 검출 및 회귀 모델을 구현하는 시스템, 애플리케이션 또는 모듈일 수 있다. 애니메이션은 사용자의 실 제 얼굴에 기초하여 할 수 있으며, 이와 같이 사용자 얼굴의 입력 비디오에서 추출된 미소, 깜박임, 윙크, 찌푸 림, 머리 포즈 및 다른 제스처를 포함할 수 있다. 온라인 가상 체험 플랫폼에서 직접 실행되는 것으로 예 시되어 있지만, 얼굴 검출 및 회귀 모델은 예컨대, 각 클라이언트 장치(110, 116)에서 구현될 수 있음을 이해해 야 한다. 이하, 얼굴 애니메이션 엔진의 회귀 모델의 컴포넌트 및 트레이닝이 도 2를 참조하여 보다 상세히 설명된 다. 도 2: 회귀 모델 도 2는 일부 구현에 따른, 회귀 모델을 위한 트레이닝 환경의 도식이다. 예시된 바와 같이, 회귀 모델은 입력으로서 하이브리드 라벨링된 데이터 세트를 수신할 수 있고, 얼굴 애니메이션 코딩 시스템(facial animation coding system: FACS) 가중치, 머리 포즈 및 얼굴 랜드마크의 세트를 출력할 수 있다. 이 예시에서, 출력은 회귀 모델을 트레이닝하기 위해 사용될 수 있다; 그러나, 출력은 초기 트레이닝 이후 아바타의 애 니메이션에도 사용될 수 있음을 이해해야 한다. 일반적으로, 회귀 아키텍처는 얼굴 특징 추출기로서 공유된 백본(shared backbone)(예컨대, 인코더) 을 사용하여 얼굴 랜드마크 및 FACS 가중치를 공동 트레이닝하는 멀티태스크 셋업을 사용한다. 이 배열은 얼굴 표정의 미묘함(subtlety)을 캡처하는 실제 이미지로 합성 애니메이션 시퀀스로부터 학습한 FACS 가중치를 증강 한다. FACS 회귀 서브네트워크는 랜드마크 회귀 모델과 함께 트레이닝된다. FACS 회귀 서브네트워크 는 인과적 컨볼루션(causal convolution)을 구현한다. 인과적 컨볼루션은 인코더에서 발 견되는 공간적 특징에 대해서만 작용하는 컨볼루션과 반대로 시간에 따라 특징에 작용한다. 도시된 바와 같이, 도식의 입력 부분은 라벨링된 트레이닝 데이터세트의 일부인 하이브리드 입력 비디오 프레임을 포함하는 트레이닝 세트를 활용한다. 하이브리드 입력 비디오 프레임은 살아있는 예시 인물을 캡처한 실제 비디오 프레임과 알려진 FACS 가중치 및 알려진 머리 포즈(예컨대, 미리 구성된 FACS 가중치, 포즈 등을 사용하여 생성된 예시 아바타 얼굴)를 사용하여 생성된 합성 프레임이 모두 포함된다. 트레이닝 세트는 트 레이닝 이후 실제 비디오로 대체될 수 있다. 트레이닝 세트는 트레이닝 목적을 위해 회귀 모델에 입 력될 수 있다. 회귀 모델은 인코더 및 FACS 회귀 서브네트워크를 포함한다. 인코더는 일반적으로 컨볼루 션 신경망으로 배열된 하나 이상의 서브네트워크를 포함할 수 있다. 하나 이상의 서브네트워크는 적어도, 2차원 (2D) 컨볼루션 서브네트워크(또는 층) 및 완전 연결(FC) 컨볼루션 서브네트워크(또는 층)를 포함할 수 있다. 인 코더에 대한 다른 배열이 또한 적용 가능할 수 있다. FACS 회귀 서브네트워크는 인과적 컨볼루션, 완전 연결(fully connected: FC) 컨볼루션 서브네트워크 및 순환 신경 서브네트워크(RNN)를 포함할 수 있다. 인과적 컨볼루션은 시간이 지남에 따라 축적된 하이 레벨 특징에 대해 동작할 수 있다. 이 아키텍처는 실시간 적용에 적합하므로, 출력 예측은 입력이 도착하 는 동일한 기간에 컴퓨팅된다 (즉, 각 입력 프레임에 대해, 다음 프레임이 도착하기 전이나 그 무렵에 출력을 예측할 필요가 있음). 이는 미래 시간 단계로부터의 정보를 사용할 수 없음을 의미한다 (즉, 정상적인 대칭 컨 볼루션은 작동하지 않는다). 따라서, 인과적 컨볼루션의 각 컨볼루션은 과거 정보만 고려하고, 실시간 시 나리오에서 작동할 수 있는 비대칭 커널(2x1의 예시적 커널 크기)로 동작한다. 인과적 컨볼루션 층은 정상 컨볼루션 층처럼 쌓일 수 있다. 커널의 크기를 증가시키거나 더 많은 층을 쌓아서 시야(field of view)를 증가 시킬 수 있다. 예시된 층 수는 3개이지만, 동일한 것은 임의의 층 수로 증가될 수 있다. 추가적으로 예시된 바와 같이, 트레이닝 동안, FACS 손실 및 랜드마크 회귀 분석은 출력의 정확성을 강화 하는 데 사용될 수 있다. 예컨대, 회귀 모델은 실제 이미지와 합성 이미지 모두를 사용하여 초기에 트레이 닝될 수 있다. 특정 수의 단계 이후, 합성 시퀀스는 시간적 FACS 회귀 서브네트워크에 대한 가중치를 학습 하는 데 사용될 수 있다. 합성 애니메이션 트레이닝 시퀀스는 상이한 신원(얼굴 메시)에 사용되는 정규화된 리 그로 생성되고, 미리 결정된 FACS 가중치를 포함하는 애니메이션 파일을 사용하여 자동으로 렌더링될 수 있다. 이러한 애니메이션 파일은 전형적인 마커 기반 접근법으로 캡처된 시퀀스를 사용하여 생성되거나, 마커 기반 데 이터에서 누락된 임의의 표현을 채우기 위해 아티스트에 의해 직접 제작될 수 있다. 또한, 블록(208 및 209)에 도시된 바와 같이, 손실은 랜드마크 및 FACS 가중치를 회귀하기 위해 결합된다.예컨대, 여러 상이한 손실 항(loss term)은 얼굴 랜드마크와 FACS 가중치를 회귀하기 위해 선형으로 결합될 수 있다. 얼굴 랜드마크의 경우, 회귀된 위치의 평균 제곱근 오차(root mean square error: RMSE)는 트레이닝을 강 화하기 위해 랜드마크 회귀 모델에 의해 사용될 수 있다. 추가적으로, FACS 가중치의 경우, 평균 제곱 오 차(mean squared error: MSE)가 FACS 손실 회귀 모델에 의해 활용된다. 예시된 바와 같이, 회귀 모델 에서 Lpos로 표시된 FACS 손실은 타겟과 예측된 속도 사이의 MSE로 정의되는 속도 손실(Lv)을 활용한다. 이 는 동적 표현의 전반적인 평활성(smoothness)을 촉진한다. 또한, FACS 가중치 지터를 감소시키기 위해 가속도에 대한 정규화 항(Lacc)이 추가되었다 (그리고 그 가중치는 응답성을 유지하기 위해 상대적으로 낮게 유지된다). 비지도 일관성 손실(Lc)은 트레이닝 이미지의 서브세트에 대한 랜드마크 레이블을 요구하지 않고, 랜드마크 예 측이 상이한 변환에서 동일하도록 장려하는 데에도 활용될 수 있다. 일단 트레이닝되면, 회귀 모델은 아바타를 애니메이팅하는 데 사용하기 위해 입력 비디오(예컨대, 사용자 의 라이브 비디오)로부터 FACS 가중치, 머리 포즈 및 얼굴 랜드마크를 추출하는 데 사용될 수 있다. 예컨대, 클 라이언트 장치(110, 116)와 연관된 얼굴 애니메이션 엔진은 출력 FACS 가중치, 머리 포즈 및 얼굴 랜드마 크를 취하고, 동일한 것에 기초하여 개별 애니메이션 프레임을 생성할 수 있다. 개별 애니메이션 프레임은 입력 비디오에 기초하여 애니메이팅된 얼굴과 아바타를 제시하기 위해 순차적으로(예컨대 실시간으로) 배열될 수 있 다. 전술한 바와 같이, 회귀 모델은 입력 비디오로부터 애니메이션을 생성하도록 클라이언트 장치(110, 116)에 서 구현될 수 있다. 얼굴 검출 모델은 후술하는 바와 같이, 회귀 모델에 대한 바운딩 박스에 기초하여 FACS 가중치, 머리 포즈 및 얼굴 랜드마크의 식별을 위해 사용될 수 있다. 도 3: 얼굴 검출 모델 도 3은 일부 구현에 따른, 배치된 얼굴 검출 모델을 갖는 예시적인 얼굴 추적 시스템의 도식이다. 얼 굴 추적 시스템은 4개의 네트워크 - P-Net, R-Net, A-Net 및 B-Net를 포함하고, 회 귀를 위한 예측된 FACS 가중치, 머리 포즈 및 얼굴 랜드마크를 포함하는 출력을 생성하도록 구성된다. 일반적으로, P-Net은 상이한 해상도(resolution)에서 전체 입력 프레임을 수신하고, 얼굴 제안 및/또 는 바운딩 박스 후보를 생성한다. P-Net은 완전 컨벌루션 네트워크(fully convolutional network)라고도 지칭될 수 있다. R-Net은 P-Net로부터 입력 제안/바운딩 박스 후보를 취한다. R-Net은 정제된 바운딩 박스를 출 력한다. R-Net은 컨벌루션 신경망(convolutional neural network)이라고도 지칭될 수 있다. A-Net은 회귀를 위한 바운딩 박스, FACS 가중치, 머리 포즈 및 얼굴 랜드마크 뿐만 아니라 정제된 제안 및 리턴 얼굴 확률을 수신한다. 일 구현에서, 회귀는 회귀 모델에 의해 수행될 수 있다. B-Net은 A- Net과 유사할 수 있지만, 증가된 LOD(level-of-detail)을 제공한다. 이러한 방식으로, A-Net은 B- Net과 상이한, 낮은 해상도에서 입력 프레임으로 작동할 수 있다. 도 3에 예시된 바와 같이, A-Net에 의해 출력된 바운딩 박스 및/또는 얼굴 확률이 원래 바운딩 박스 내에 서 얼굴이 검출되었음을 나타내는 경우, 얼굴 검출 모델이 P-Net 및 R-Net을 우회 및/또는 바이 패스하도록 허용하는, 제1 입력 프레임 이후에 고급 추론 결정이 이루어질 수 있다. 이러한 방식으로, 여 러 계산 단계가 생략될 수 있으므로, 이로써 계산 시간 감소, 지연(lag) 감소, 전력 사용 감소(배터리 구동 장 치의 배터리를 보존하는 데 도움을 줄 수 있음) 및 계산 효율성 향상을 포함하는 기술적 이점을 제공한다. 그와 같이, 얼굴 검출 모델은 스마트폰, 태블릿 또는 웨어러블 장치와 같은 상대적으로 낮은 계산 용량 장치에 서 사용하기에 적합하게 만드는 계산 효율성을 제공할 수 있다. P-Net, R-Net 및 A-Net을 포함 하는 얼굴 검출 모델은 상대적으로 낮은 계산 능력의 모바일 장치에 의해 사용 가능할 수 있다. 추가적으로, 동작 중에, 더 높은 LOD(level-of-detail)가 특정 클라이언트 장치(110, 116)에 적합한지 결정하기 위해, A-Net으로부터의 출력 이후 고급 LOD 결정(advanced level-of-detail decision)이 이루어질 수 있다. 예컨대, 클라이언트 장치가 이용 가능한 컴퓨팅 리소스의 부족, 충분한 배터리 전원의 부족, B- Net에 의해 제공된 증가된 LOD를 구현하는 데 적합하지 않은 환경을 나타내는 경우, B-Net 프로세싱 전체가 즉석에서(on-the-fly) 생략될 수 있다. 이러한 방식으로, B-Net에 의해 요구되는 추가적인 계산 단 계가 생략될 수 있으며, 이로써 계산 시간 감소, 지연 감소, 효율성 증가를 포함하는 기술적 이점을 제공한다. 그와 같이, 얼굴 검출 모델은 과도한 컴퓨터 비전 분석의 단점을 극복하는 계산 효율성을 제공할 수 있으 며, P-Net, R-Net 및 A-Net을 포함하는 얼굴 검출 모델은 상대적으로 낮은 계산 능력 모바일장치 또는 향상된 효율을 요구하는 동작 조건 하에 있는 장치에 의해 실행될 수 있다. B-Net은 하나 이상의 조건이 충족되면 바이패스될 수 있다. 예컨대, 그러한 조건은 낮은 배터리 예비량 (battery reserve), 낮은 전력 가용성, 높은 열 조건, 네트워크 대역폭 또는 메모리 제한 등을 포함할 수 있다. 적절한 임계값이 각 조건에 대해 사용될 수 있다. 또한, 장치 동작에 대한 영향이 적거나 전혀 없이, 예컨대 가 상 환경 내에서, 아바타 애니메이션을 제공하도록 사용자가 얼굴 검출 모델이 더 낮은 LOD와 동작하도록 지시하게 허용하는, 사용자 선택 가능 옵션이 제공될 수 있다. A-Net과 B-Net은 모두 오버로드된 출력 컨벌루션 신경망이다. 이와 관련하여, 각 신경망은 전형적인 출력 신경망에 비해 더 많은 예측된 FACS 가중치와 얼굴 랜드마크를 제공한다. 예컨대, MTCNN의 전형적인 출력 네트워크(예컨대, O-Net)는 약 5 얼굴 랜드마크와 FACS 가중치의 작은 세트를 출력으로 제공할 수 있다. 이에 비해, A-Net과 B-Net은 모두 예컨대, 175까지 또는 이를 초과하는 얼굴 랜드마크와 여러 FACS 가중치 를, 실질적으로 더 많이 제공할 수 있다. 동작 중에, 예측된 FACS 가중치, 머리 포즈 및 얼굴 랜드마크는 아바 타의 (얼굴의) 애니메이션 및 회귀를 위해 회귀 모델(A-Net 또는 B-Net 중 하나)에 제공된다. 회귀 모델 은 A-Net 및 B-Net 각각에 포함될 수 있어서, FACS 가중치, 머리 포즈 및 얼굴 랜드마크의 회 귀가 어느 구현된 출력 네트워크 내에서 발생할 수 있다는 점에 유의한다. A-Net 및 B-Net 각각에 관 련된 추가 설명 및 세부사항은 각각 도 5a 및 도 5b를 참조하여 제공된다. 이하, 도 4를 참조하여 얼굴 검출 모델 및 회귀 모델의 동작과 관련된 추가 세부사항이 제공한다. 도 4: 트레이닝된 모델을 사용하여 애니메이션 생성 도 4는 일부 구현에 따른, 3D 아바타의 애니메이션을 생성하도록 구성된 회귀 모델 및 얼굴 검출 모델 의 예시적인 프로세스 흐름도이다. 프로세스 흐름에 도시된 바와 같이, 초기 입력 프레임(402a)(타임 스탬프 t=0에서)은 얼굴 검출 모델에 대한 입력으로서 제공된다. 이 예시에서, A-Net 및 선택적으로 B-Net은 출력 LOD를 결정할 때, 액티브 결정 및/또는 정렬 패치에 의해 동적으로 선택된다. 따 라서, 이 출력 네트워크는 결정과 A-Net 및 B-Net 아키텍처의 조합일 수 있음을 쉽게 이해해야 한다. 일부 구현에서, 회귀 모델은 예컨대, 이용 가능한 컴퓨팅 용량, 얼굴 애니메이션의 품질에 대한 사용자 설정 등에 따 라, A-Net 또는 B-Net 중 하나만 구현할 수 있다. 일부 구현에서, 결정은 B-Net의 선택적으로 활용으로 런 타임에 이루어질 수 있다. 정제된 바운딩 박스 또는 얼굴 랜드마크를 획득하면, 입력 프레임이 정렬되고 식별된 얼굴의 윤곽선을 나타내도 록 축소된다. 그 후, 회귀 모델(A-net 및 B-Net의 일부)은 t=0에서 정렬된 입력 프레임을 입력 으로 취하고, 애니메이션에 대한 실제 FACS 가중치, 머리 포즈 및 얼굴 랜드마크를 출력한다. 후속 프레임(402b)(예컨대, 타임스탬프 t=+1 및 이후 타임스탬프에서)에 대해, 정렬 패치(예컨대, 고급 바 이패스 결정에 기초함)는 얼굴이 여전히 초기 바운딩 박스 내에 있는지 여부를 결정하는 A-Net에 직접 입 력된다. 얼굴이 여전히 초기 바운딩 박스 내에 있는 경우, 회귀 모델은 정렬된 입력 프레임을 입력으로 취 하고, 얼굴이 원래의 바운딩 박스 내에서 검출되는 각 후속 프레임에 대한 애니메이션을 위한 실제 FACS 가중치, 머리 포즈 및 얼굴 랜드마크를 출력한다. 바운딩 박스 내에서 얼굴이 검출되지 않는 상황에서, 얼굴 검출 모델은 얼굴을 포함하는 새로운 바운딩 박 스를 제공하기 위해 P-Net 및 R-Net을 활용할 수 있다. 오버로드된 컨볼루션 신경망은 A-Net과 B-Net 둘 다에 사용된다. 이하, A-Net의 간략한 설명이 도 5a를 참조하여 제공되고, B-Net의 간략한 설명이 도 5b를 참조하여 제공된다. 도 5: 오버로드된 출력 네트워크 도 5a 및 도 5b는 일부 구현에 따른, 얼굴 검출 모델에 대한 예시적인 출력 네트워크(312 및 316)의 개략 도이다. 도 5a에 도시된 바와 같이, A-Net은 머리 각도 및/또는 머리 포즈를 회귀시키도록 구성된 오버로드된 컨볼 루션 신경망이다. A-Net은 얼굴의 입 밖에 있는 혀를 검출하도록 구성된 혀 서브모델, FACS 가중치를 예측하기 위한 FACS서브모델, 머리 포즈 또는 머리 각도를 예측하기 위한 포즈 서브모델, 제공된 바운딩 박스 내의 얼굴 을 검출하기 위한 얼굴 확률 층(또는 층들)를 포함한다. A-Net은 또한 랜드마크와 이러한 랜드마크에 대한 오클 루전 정보(예컨대, 입력 비디오에 존재하는 물리적 오클루전)를 검출하도록 구성된다.A-Net은 전형적인 출력 네트워크에 비해 여러 장점을 제공한다. 첫째, 엔드-투-엔드 머리 포즈를 직접 회 귀하는 것을 허용한다. 이를 위해, 이미지를 입력하는 데 사용된 정렬은 임의의 회전을 적용하지 않는다(입력이 R-Net 예측된 바운딩 박스 또는 A-Net 랜드마크에서 계산되는 경우 모두). 추가적으로, FACS 가중치와 혀 신호 를 예측한다. 또한, 이 경우에서 175 이상의 개별 윤곽 또는 랜드마크를 포함하여 임의의 수의 얼굴 랜드마크의 예측을 허용한다. A-Net은 단계적으로 트레이닝될 수 있다. 초기에, A-Net은 얼굴 확률을 회귀하는 분기(branch)와 함 께 랜드마크 및 오클루전을 회귀하는 인코더와 공동 트레이닝될 수 있다. 이 트레이닝에 대해, 부정적인 예시(얼굴이 존재하지 않는 이미지 또는 존재하는 경우, 비정상적인 규모를 갖는 이미지, 예컨대, 이미지의 극 히 크거나 작은 부분, 이미지 내 얼굴의 오직 일부) 뿐만 아니라 주석이 달린 랜드마크(annotated landmarks) (실제 및 합성 모두)를 갖는 얼굴의 이미지가 사용된다. 네트워크의 이 부분과 데이터에는 시간적 정보가 없다. 후속 단계는 FACS 제어 및 머리 포즈 각도를 회귀하고 혀 내밀기 검출(tongue out detection)을 수행하는 서브 모델을 트레이닝한다. 인코더는 이들 트레이닝 단계 동안 수정되지 않기 때문에, 서브모델 트레이닝은 임 의의 순서로 수행될 수 있다. FACS 가중치 및 머리 포즈 서브모델은 이를 강제하는 손실뿐만 아니라 시간적 필 터링과 시간적 일관성을 허용하는 시간적 아키텍처를 사용하여, 다양한 표현과 포즈를 갖는 합성 시퀀스를 사용 하여 트레이닝될 수 있다. 혀 내밀기 서브모델은 혀 내밀기 상태를 검출하기 위해 실제 이미지로 트레이닝된 간 단한 분류기(classifier)일 수 있다. 도 5b를 참조하면, B-Net은 더 높은 LOD를 제공하도록 구현되어, 더 높은 품질의 얼굴 애니메이션을 가능하게 한다. 일반적으로 B-Net A-Net보다 더 나은 품질의 FACS 가중치 및 혀 예측을 회귀한다. 입력 이미지 는 A-Net에 의해 동일한 프레임에 제공되는 랜드마크를 사용하여 정렬된다. 예컨대, 정렬은 프로크루스테 스 분석(procrustes analysis) 및 /또는 다른 적합한 형상 정렬 및/또는 윤곽 정렬 방법론을 사용하여 수행될 수 있다. 이 예시에서, B-Net은 A-Net과 유사한 구조를 따르지만, 얼굴 확률과 머리 포즈를 제공하지 않으며, 더 큰 용량을 갖는다. A-Net과 동일한 방식으로 트레이닝된다: 랜드마크 및 오클루전 정보에 대한 첫 번째 트레 이닝, FACS 가중치 트레이닝 및 혀 내밀기 트레이닝 이어진다. B-Net은 또한 랜드마크 및 해당 랜드마크에 대한 오클루전 정보(예컨대, 입력 비디오에 존재하는 물리적 오클루전)를 검출하도록 구성된다. LOD(level-of-detail) 및 B-Net 프로세싱의 구현과 관련하여, 여러 요인이 도 3의 결정에서 B-Net이 선택 되는지 여부에 영향을 미칠 수 있다. LOD의 관리는 실행 중인 장치 유형, 장치의 현재 상태 및 얼굴 검출 모델 의 현재 성능에 기초한다. 충분한 컴퓨팅 성능을 갖는 장치는 예컨대 A-Net과 B-Net을 모두 실행하여, 가장 높은 LOD 수준에서 실행할 수 있다. 얼굴 인식 모델의 성능은 모니터링될 수 있으며, 초당 프레임 수(frames per second: FPS)가 일정 수준 이상 저하되면 B-Net이 바이패스될 수 있다. 에너지 보존을 위해, 장치의 배터리가 특정 임계값 아래로 떨어지 면 LOD를 낮출 수도 있다. 또한, CPU 온도와 같은 하드웨어 활용에 대한 2차 효과를 측정하는 신호를 고려하여 LOD 수준을 결정하고, 상응하여 B-Net 활용 여부를 결정할 수 있다. 주어진 컴퓨팅 예산이 A-Net의 구현으로만 제한되는 특정 장치가 있을 수도 있다. 이는 얼굴 추적기 성능의 온 라인 추정 및/또는 미리 정의된 장치 리스트 중 하나 이상을 통해 수행될 수 있다. A-Net만으로 실행하는 경우, 이미 B-Net을 바이패스하는 동안 예측의 품질이 일정 값 이하로 떨어지면, 회귀된 FACS 제어의 품질이 충분하지 않고 대신 머리 포즈만 고정되거나 미리 결정된 FACS 가중치로 제공될 수 있다고 결정될 수 있다. A-Net 및/또는 B-Net(회귀 모델도 포함)을 사용하여, 아바타의 애니메이션을 위한 출력 프레임 의 시퀀스가 모델에 제공된 입력 비디오 프레임에 기초하여 생성된다. 도 6은 일부 구현에 따른, 로버스트 애니 메이션을 생성하도록 구성된 회귀 모델 및 얼굴 검출 모델의 단순화된 프로세스 흐름도의 예시이다. 도시된 바 와 같이, 입력 프레임은 분석되고, 얼굴 랜드마크가 추출되어, 출력 프레임은 입력 프레임에 존 재하는 얼굴의 움직임, 제스터 및 다른 특징에 기초하여 트랜스포즈 모듈을 통해 생성될 수 있다. 트랜스 포즈 모듈은 얼굴 애니메이션 엔진 및/또는 VE 애플리케이션(112/118)과 연관될 수 있다. FACS 가중치 및 얼굴 랜드마크에 기초하여 조작될 수 있는 임의의 아바타는 이러한 기법을 사용하여 애니메이팅될 수 있다는 점 에 유의한다. 따라서, 휴머노이드 출력 프레임이 예시되지만, 출력의 어떠한 변형도 가능하며,본 개시의 예시적 인 실시예의 범위 내에 있다.이하, 전술한 모델들을 사용하여 애니메이션을 생성하는 것에 대한 보다 상세한 논의가 아래에 제시된다. 도 7: 트레이닝된 모델로 아바타를 애니메이팅 도 7은 일부 구현에 따른, 비디오로부터의 실시간 로버스트 얼굴 애니메이션의 예시적인 방법의 흐름도이 다. 일부 구현에서, 방법은 예컨대, 서버 시스템, 예컨대, 도 1에 도시된 바와 같은 온라인 가상 체험 플 랫폼에서 구현될 수 있다. 일부 구현에서, 방법의 일부 또는 전부는 도 1에 도시된 바와 같은 하나 이상의 클라이언트 장치(110 및 116)와 같은 시스템, 및/또는 서버 시스템 및 하나 이상의 클라이언트 시스템 둘 다에서 구현될 수 있다. 설명된 예시에서, 구현 시스템은 하나 이상의 프로세서 또는 프로세싱 회로, 및 데 이터베이스 또는 다른 액세스 가능한 스토리지와 같은 하나 이상의 저장 장치를 포함한다. 일부 구현에서, 하나 이상의 서버 및/또는 클라이언트의 상이한 컴포넌트는 방법의 상이한 블록 또는 다른 부분을 수행할 수 있 다. 아바타 애니메이션을 제공하기 위해, 입력 비디오로부터 얼굴은 검출될 수 있고, 얼굴 랜드마크, 머리 포즈, 혀 상태 등이 결정되고, 대응하는 아바타의 얼굴을 애니메이팅하는 데 활용될 수 있다. 얼굴 검출 또는 분석을 수 행하기 전에, 사용자에게는 그러한 기법이 아바타 애니메이션을 위해 활용된다는 표시(indication)가 제공된다. 사용자가 허가를 거부하면, 비디오에 기초하는 얼굴 애니메이션이 꺼진다 (예컨대, 디폴트 애니메이션이 사용될 수 있거나, 애니메이션이 사용자에 의해 제공된 오디오 및/또는 텍스트 입력과 같은, 다른 사용자 허가 입력에 기초할 수 있음). 사용자 제공된 비디오는 아바타 애니메이션을 위해 특별히 활용되며 저장되지 않는다. 사용자 는 언제든지 비디오 분석 및 애니메이션 생성을 끌 수 있다. 또한, 비디오 내에서 얼굴의 위치를 검출하기 위해 얼굴 검출이 수행된다; 얼굴 인식은 수행되지 않는다. 사용자가 아바타 애니메이션을 위한 비디오 분석의 사용 을 허가하면, 방법은 블록에서 시작한다. 블록에서, 완전 컨벌루션 네트워크는 비디오의 제1 프레임으로부터 바운딩 박스 후보의 세트를 식별하는 데 사용된다. 각 바운딩 박스 후보는 비디오의 제1 프레임으로부터의 얼굴을 포함한다. 예컨대, 비디오는 온라 인 가상 체험 플랫폼과 동작 가능하게 통신하는 사용자 장치(예컨대, 110, 116)에 의해 캡처될 수 있다. 클라이언트 장치와 연관된 사용자는 카메라 컴포넌트를 통해 캡처를 위해 자신의 얼굴을 제시할 수 있고, 자신 의 기기에서 가상 체험 애플리케이션(또는 다른 얼굴 애니메이션 애플리케이션)에 의한 비디오에 대한 액세스를 허용할 수 있다. 가상 체험 애플리케이션(또는 다른 얼굴 애니메이션 애플리케이션)은 전술한 바와 같이, 얼굴 검출 모델과 회귀 모델(A-Net 또는 B-Net으로 구현될 수 있음)을 모두 구현할 수 있다. 블록 다음에는 블록이 이어진다. 블록에서, 바운딩 박스 후보의 세트는 컨벌루션 신경망을 사용하여 바운딩 박스로 정제된다. 예컨대, 도 3 을 참조하여 전술한 바와 같이, P-Net은 바운딩 박스 후보를 R-Net에 제공할 수 있으며, 여기에서 후 보는 얼굴 검출 모델에 의한 사용을 위해 바운딩 박스로 정제된다. 블록 다음에는 블록이 이어 진다. 블록에서, 바운딩 박스와 제1 프레임에 기초하는 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드 마크 중 하나 이상의 제1 세트가 오버로드된 출력 컨벌루션 신경망을 사용하여 획득된다. 예컨대, 일부 구현에 서, 오버로드된 출력 컨벌루션 네트워크는 A-Net만 포함할 수 있다. 다른 구현에서, 오버로드된 출력 컨벌 루션 네트워크는 A-Net 및 B-Net 모두를 포함할 수 있다. A-Net과 조합하여 B-Net을 사용할지 여부에 대한 결정은, 사용자 선호도, 시스템 관리자 선호도, 이용 가능한 컴퓨팅 리소스, 이용 가능한 메모리, 예상된 또는 실현된 프레임 속도, 네트워크 조건 및 기타 요인을 포함하는 복수의 요인에 기초할 수 있지만, 이에 제한 되지 않는다. 블록 다음에는 블록이 이어진다. 블록에서, 미리 정의된 얼굴 표정 가중치, 머리 포즈 및 얼굴 랜드마크 중 하나 이상의 제1 세트에 기초하 여, 3차원(3D) 아바타 애니메이션의 제1 애니메이션 프레임이 생성된다. 일반적으로, 아바타의 머리 포즈는 제1 세트의 머리 포즈와 매칭하고, 아바타의 얼굴 랜드마크는 제1 세트의 얼굴 랜드마크와 매칭한다. 또한, 애니메 이션 프레임은 오버로드된 출력 컨벌루션 네트워크에 의해 출력된 예측 값을 수신하고 프로세싱하는 트레이닝된 회귀 모델에 기초하여 생성된다. 따라서, 회귀 모델은 일부 구현에서 아바타의 애니메이션에 대한 최종 가 중치를 제공할 수 있다. 블록 다음에는 블록이 이어진다. 블록에서, 제1 프레임에 후속하는 비디오의 각 추가 프레임에 대해 얼굴이 검출되는지 여부에 대한 결정이 이루어진다. 바운딩 박스가 얼굴을 포함하는 것으로 검출되면, 블록은 오버로드된 출력 컨벌루션 신경망을 사용하여, 바운딩 박스 및 추가 프레임에 기초하는 하나 이상의 미리 정의된 얼굴 표정 가중치, 머리 포즈 및얼굴 랜드마크의 추가 세트를 획득하는 단계를 포함한다. 또한, 블록은 추가 세트를 사용하여, 3D 아바타 의 애니메이션의 추가 애니메이션 프레임을 생성하는 단계를 포함한다. 그러나, 블록에서, 그리고 제1 프레임에 후속하는 비디오의 각각의 추가 프레임에서, 바운딩 박스가 얼굴 을 포함하지 않는 것으로 검출되면, 방법은 식별하는 단계, 정제하는 단계 및 얼굴을 포함하는 새로운 바운딩 박스에 기초하여 획득하는 단계를 반복하는 것을 포함한다. 이와 관련하여, 얼굴이 검출되 지 않으면, P-Net과 R-Net이 다시 사용된다. 따라서, 방법은 추가 프레임에 대한 바운딩 박스가 얼굴을 포함하지 않는 것으로 검출되면, 추가 프레임을 제1 프레임으로 설정하고, 전술한 단계를 반복하는 것을 포함한다. 블록(702-714)은 전술한 것과 상이한 순서로 수행(또는 반복)될 수 있고, 그리고/또는 하나 이상의 블록이 생략 될 수 있다. 방법은 서버(예컨대, 102) 및/또는 클라이언트 장치(예컨대, 110 또는 116)에서 수행될 수 있 다. 또한, 방법의 일부는 임의의 원하는 구현에 따라, 결합되고 순차적으로 또는 병렬로 수행될 수 있다. 전술한 바와 같이, 로버스트 얼굴 애니메이션의 기법은 클라이언트 장치에서 트레이닝된 얼굴 검출 모델과 트레 이닝된 회귀 모델의 구현을 포함한다. 얼굴 검출 모델은 제1 프레임 이후에 얼굴이 검출되는 프레임에 대한 바 운딩 박스 생성을 바이패싱하고, 사용자 선호 및/또는 장치 조건에 따라 더 높은 LOD(level of detail) 출력 네 트워크를 바이패싱(예컨대, B-Net 바이패싱)함으로써 사용되는 계산 리소스를 자동으로 줄이는 둘 이상의 지능 형 결정 컴포넌트를 포함할 수 있다. 이하, 도 1-6에 예시된 상이한 장치 및 컴포넌트를 구현하는 데 사용될 수 있는 다양한 컴퓨팅 장치의 보다 상 세한 설명이 도 8을 참조하여 제공된다. 도 8은 일부 구현에 따른, 여기에 설명된 하나 이상의 특징을 구현하는 데 사용될 수 있는 예시적인 컴퓨팅 장 치의 블록도이다. 일 예시에서, 장치는 컴퓨터 장치(예컨대, 도 1의 102, 110 및/또는 116)를 구현하 고 여기에 설명된 적절한 방법 구현을 수행하는 데 사용될 수 있다. 컴퓨팅 장치는 임의의 적절한 컴퓨터 시스템, 서버, 또는 기타 전자 또는 하드웨어 장치일 수 있다. 예컨대, 컴퓨팅 장치는 메인프레임 컴퓨터, 데스크탑 컴퓨터, 워크스테이션, 휴대용 컴퓨터, 또는 전자 장치(휴대용 장치, 모바일 장치, 휴대 전화, 스마트 폰, 태블릿 컴퓨터, 텔레비전, TV 셋톱 박스, PDA(personal digital assistant), 미디어 플레이어, 게임 장치, 웨어러블 장치 등)일 수 있다. 일부 구현에서, 장치는 프로세서, 메모리, 입력/출력(I/O) 인터 페이스, 및 오디오/비디오 입력/출력 장치(예컨대, 디스플레이 스크린, 터치스크린, 디스플레이 고글 또는 안경, 오디오 스피커, 마이크 등)를 포함한다. 프로세서는 프로그램 코드를 실행하고 장치의 기본 동작을 제어하기 위한 하나 이상의 프로세서 및/ 또는 프로세싱 회로일 수 있다. \"프로세서\"는 데이터, 신호 또는 기타 정보를 프로세싱하는 임의의 적절한 하드 웨어 및/또는 소프트웨어 시스템, 메커니즘 또는 컴포넌트를 포함한다. 프로세서는 범용 CPU(general-purpose central processing unit), 다중 프로세싱 장치, 기능을 달성하기 위한 전용 회로 또는 기타 시스템을 갖는 시 스템을 포함할 수 있다. 프로세싱은 특정 지리적 위치에 제한되거나, 시간적 제한이 있을 필요는 없다. 예컨대, 프로세서는 \"실시간\", \"오프라인\", \"배치 모드\" 등으로 기능을 수행할 수 있다. 프로세싱의 일부는 상이한(또는 동일한) 프로세싱 시스템에 의해 상이한 시간 및 상이한 위치에서 수행될 수 있다. 컴퓨터는 메모리와 통신하는 임의의 프로세서일 수 있다. 메모리는 전형적으로 프로세서에 의한 액세스를 위해 장치에 제공되며, 프로세서에 의한 실행을 위한 명령어를 저장하기에 적합하고, 프로세서와 분리되어 위치하고, 그리고/또는 그와 통합된 임의의 적 절한 프로세서 판독 가능 저장 매체, 예컨대, RAM(random access memory), ROM(read-only memory), 전기적 소 거 가능 판독 전용 메모리(EEPROM), 플래시 메모리 등일 수 있다. 메모리는 운영 체제, 애플리케이션 , 및 관련 데이터를 포함하는 프로세서에 의해 서버 장치 상에서 동작하는 소프트웨어를 저장할 수 있다. 일부 구현에서, 애플리케이션은 프로세서가 여기에서 설명된 기능, 예컨대, 도 7의 방법의 일부 또는 전부를 수행할 수 있게 하는 명령어를 포함할 수 있다. 일부 구현에서, 애플리케이션은 또한 여기에 설명된 바와 같이, 입력 비디오에 기초하여 로버스트 실시간 애니메이션을 생성하기 위한 하나 이 상의 트레이닝된 모델을 포함할 수 있다. 예컨대, 메모리는 온라인 가상 체험 플랫폼(예컨대, 102) 내에서, 카메라에 포착된 사용자의 얼굴 움직임 에 기초하여 애니메이팅된 아바타를 제공할 수 있는 애플리케이션을 위한 소프트웨어 명령어를 포함할 수 있다. 메모리에서 소프트웨어 중 어느 것은 대안적으로 임의의 다른 적절한 저장 위치 또는 컴퓨터 판독가능 매체에 저장될 수 있다. 또한, 메모리(및/또는 다른 연결된 저장 장치(들))는 여기에서 설명된 특징에 서 사용되는 명령어 및 데이터를 저장할 수 있다. 메모리 및 임의의 다른 유형의 스토리지(자기 디스크, 광 디스크, 자기 테이프 또는 기타 유형의 매체(tangible media))는 \"스토리지\" 또는 \"저장 장치\"로 간주될 수 있다. I/O 인터페이스는 서버 장치를 다른 시스템 및 장치와 인터페이싱할 수 있게 하는 기능을 제공할 수 있다. 예컨대, 네트워크 통신 장치, 저장 장치(예컨대, 메모리 및/또는 데이터 저장소) 및 입력/출력 장치 는 인터페이스를 통해 통신할 수 있다. 일부 구현에서, I/O 인터페이스는 입력 장치(키보드, 포인팅 장치, 터치스크린, 마이크, 카메라, 스캐너 등) 및/또는 출력 장치(디스플레이 장치, 스피커 장치, 프린터, 모터 등) 를 포함하는 인터페이스 장치에 연결할 수 있다. 설명의 편의를 위해, 도 8은 프로세서, 메모리, I/O 인터페이스, 소프트웨어 블록(808 및 810), 및 데이터베이스 각각에 대한 하나의 블록을 도시한다. 이들 블록은 하나 이상의 프로세서 또는 프로세싱 회로, 운영 체제, 메모리, I/O 인터페이스, 애플리케이션 및/또는 소프트웨어 모듈을 나타낼 수 있다. 다른 구 현에서, 장치는 도시된 컴포넌트들 모두를 갖지 않을 수 있고, 그리고/또는 여기에서 도시된 것들 대신에 또는 이에 추가하여 다른 유형의 요소를 포함하는 다른 요소들을 가질 수 있다. 온라인 가상 체험 플랫폼 이 여기에서 일부 구현에 설명된 바와 같이 동작을 수행하는 것으로 설명되지만, 온라인 가상 체험 플랫폼 또는 유사한 시스템의 임의의 적절한 컴포넌트 또는 컴포넌트의 조합, 또는 이러한 시스템과 연관된 임의의 적 절한 프로세서 또는 프로세서들이 설명된 동작을 수행할 수 있다. 사용자 장치는 또한 여기에서 설명된 특징을 구현 및/또는 사용할 수 있다. 예시적인 사용자 장치는 장치 와 같은 일부 유사한 컴포넌트, 예컨대, 프로세서(들), 메모리 및 I/O 인터페이스를 포함하는 컴퓨터 장치일 수 있다. 클라이언트 장치에 적합한 운영 체제, 소프트웨어 및 애플리케이션은 메모리에 제공되 고, 프로세서에 의해 사용될 수 있다. 클라이언트 장치를 위한 I/O 인터페이스는 네트워크 통신 장치뿐만 아니 라, 입력 및 출력 장치, 예컨대, 사운드 캡처를 위한 마이크, 이미지 또는 비디오 캡처를 위한 카메라, 사운드 출력을 위한 오디오 스피커 장치, 이미지 또는 비디오를 출력하기 위한 디스플레이 장치 또는 기타 출력 장치에 연결될 수 있다. 예컨대, 오디오/비디오 입력/출력 장치 내의 디스플레이 장치는 장치에 연결되어(또 는 포함되어), 여기에서 설명된 바와 같은 전처리 및 후처리 이미지를 디스플레이할 수 있으며, 여기에서 이러 한 디스플레이 장치는 임의의 적절한 디스플레이 장치, 예컨대, LCD, LED 또는 플라즈마 디스플레이 스크린, CRT, 텔레비전, 모니터, 터치스크린, 3-D 디스플레이 스크린, 프로젝터 또는 기타 시각적 디스플레이 장치를 포 함할 수 있다. 일부 구현은 오디오 출력 장치, 예컨대, 텍스트를 말하는 음성 출력 또는 합성을 제공할 수 있다. 여기에서 설명된 방법, 블록 및/또는 동작은 도시되거나 설명된 것과 상이한 순서로 수행될 수 있고, 그리고/또 는 적절한 경우, 다른 블록 또는 동작과 동시에(부분적으로 또는 완전히) 수행될 수 있다. 일부 블록 또는 동작 은 데이터의 한 부분에 대해 수행되고, 나중에 예컨대, 데이터의 다른 부분에 대해 다시 수행될 수 있다. 설명 된 모든 블록 및 동작이 다양한 구현에서 수행될 필요는 없다. 일부 구현에서, 블록 및 동작은 방법에서 상이한 순서로 및/또는 상이한 시간에 여러 번 수행될 수 있다. 일부 구현에서, 방법의 일부 또는 전부는 하나 이상의 클라이언트 장치와 같은 시스템에서 구현될 수 있다. 일 부 구현에서, 여기에 설명된 하나 이상의 방법은 예컨대, 서버 시스템에서 그리고/또는 서버 시스템과 클라이언 트 시스템 모두에서 구현될 수 있다. 일부 구현에서, 하나 이상의 서버 및/또는 클라이언트의 상이한 컴포넌트 는 방법의 상이한 블록, 동작 또는 다른 부분을 수행할 수 있다. 여기에서 설명된 하나 이상의 방법(예컨대, 방법(600 및/또는 700))은 컴퓨터에서 실행될 수 있는 컴퓨터 프로 그램 명령어 또는 코드에 의해 구현될 수 있다. 예컨대, 코드는 하나 이상의 디지털 프로세서(예컨대, 마이크로 프로세서 또는 다른 프로세싱 회로)에 의해 구현될 수 있고, 비일시적 컴퓨터 판독 가능 매체(예컨대, 저장 매 체), 예컨대, 반도체 또는 솔리드 스테이트 메모리를 포함하는, 자기, 광학, 전자기 또는 반도체 저장 매체, 자 기 테이프, 이동식 컴퓨터 디스켓(removable computer diskette), RAM(random access memory), ROM(read-only memory), 플래시 메모리, 강자성 디스크(rigid magnetic disk), 광 디스크, 솔리드 스테이트 메모리 드라이브 등을 포함하는, 컴퓨터 프로그램 제품에 저장될 수 있다. 프로그램 명령어는 예컨대, 서버(예컨대, 분산 시스템 및/또는 클라우드 컴퓨팅 시스템)로부터 전달되는 SaaS(software as a service)의 형태에서, 전자 신호에 포함 되고, 제공될 수도 있다. 대안적으로, 하나 이상의 방법은 하드웨어(논리 게이트 등), 또는 하드웨어와 소프트 웨어의 조합으로 구현될 수 있다. 예시적인 하드웨어는 프로그래밍 가능한 프로세서(예컨대, FPGA(field-programmable gate array), 복합 프로그래밍 가능 논리 소자(complex programmable logic device)), 범용 프로 세서, 그래픽 프로세서, ASIC(application specific integrated circuit) 등일 수 있다. 하나 이상의 방법은 시스템에서 실행되는 애플리케이션의 일부 또는 컴포넌트, 또는 다른 애플리케이션 및 운영 체제와 함께 실행되 는 애플리케이션 또는 소프트웨어로 수행될 수 있다. 여기에서 설명된 하나 이상의 방법은 임의의 유형의 컴퓨팅 장치에서 실행될 수 있는 자립형 프로그램 (standalone program), 웹 브라우저에서 실행되는 프로그램, 모바일 컴퓨팅 장치(예컨대, 휴대폰, 스마트폰, 태 블릿 컴퓨터, 웨어러블 장치(손목시계, 암 밴드, 보석, 모자, 고글, 안경 등), 랩톱 컴퓨터 등)에서 실행하는 모바일 애플리케이션(\"앱\")에서 실행될 수 있다. 일 예시에서, 클라이언트/서버 아키텍처가 사용될 수 있으며, 예컨대, (클라이언트 장치로서) 모바일 컴퓨팅 장치는 사용자 입력 데이터를 서버 장치로 전송하고, 서버로부터 출력을 위한(예컨대, 디스플레이를 위한) 최종 출력 데이터를 수신한다. 다른 예시에서, 모든 계산은 모바일 컴 퓨팅 장치의 모바일 앱(및/또는 다른 앱) 내에서 수행될 수 있다. 다른 예시에서, 계산은 모바일 컴퓨팅 장치와 하나 이상의 서버 장치 사이에서 분할될 수 있다. 설명이 그 특정 구현과 관련하여 설명되었지만, 이러한 특정 구현은 단지 예시적이며 제한적이지 않는다. 예시 에 설명된 개념은 다른 예시 및 구현에 적용될 수 있다. 여기에서 논의된 특정 구현이 사용자 데이터(예컨대, 사용자 이미지, 사용자 인구 통계, 플랫폼에서의 사용자 행동 데이터, 사용자 검색 기록, 구매 및/또는 본 아이템, 플랫폼에서의 사용자 우정 등)를 획득하거나 사용할 수 있는 상황에서, 사용자는 그러한 정보가 수집, 저장 또는 사용되는지 여부와 방법을 제어하는 옵션을 제공받 는다. 즉, 여기에서 논의된 구현은 명시적인 사용자 승인(user authorization)를 수신하고, 적용 가능한 규정에 따라 사용자 정보를 수집, 저장 및/또는 사용한다. 사용자는 프로그램 또는 특징이 특정 사용자 또는 프로그램 또는 특징과 관련된 다른 사용자에 대한 사용자 정 보를 수집하는지 여부에 대한 제어가 제공된다. 정보를 수집할 각 사용자는 사용자가 해당 사용자와 관련된 정 보 수집에 대한 제어를 가할 수 있도록 허용하고, 정보가 수집되는지 여부 및 정보의 어느 부분이 수집되는지에 대한 허가 또는 승인을 제공하기 위한 옵션이 제시된다 (예컨대, 사용자 인터페이스를 통해). 또한, 특정 데이 터는 개인 식별 정보가 제거되도록, 저장 또는 사용 전에 하나 이상의 방식으로 수정될 수 있다. 일 예시로서, 개인 식별 정보가 결정될 수 없도록 사용자의 신원이 수정될 수 있다 (예컨대, 가명(pseudonym), 숫자 값 등을 사용한 대체에 의해). 다른 예시에서, 사용자의 지리적 위치는 더 큰 지역(예컨대, 도시, 우편 번호, 주, 국가 등)으로 일반화될 수 있다. 본 개시에 설명된 기능 블록, 동작, 특징, 방법, 장치 및 시스템은 당업자에게 알려진 바와 같이, 시스템, 장치 및 기능 블록의 상이한 조합으로 통합되거나 분할될 수 있다는 점에 유의한다. 임의의 적합한 프로그래밍 언어 및 프로그래밍 기법이 특정 구현의 루틴을 구현하는 데 사용될 수 있다. 상이한 프로그래밍 기법, 예컨대, 절차 적 또는 객체 지향(procedural or object-oriented)이 사용될 수 있다. 루틴은 단일 프로세싱 장치 또는 다중 프로세서에서 실행할 수 있다. 단계, 동작 또는 계산이 특정 순서로 제시될 수 있지만, 순서는 상이한 특정 구 현에서 변경될 수 있다. 일부 구현에서, 본 명세서에서 순차적으로 도시된 다수의 단계 또는 동작은 동시에 수 행될 수 있다.도면 도면1 도면2 도면3 도면4 도면5a 도면5b 도면6 도면7 도면8"}
{"patent_id": "10-2023-7032445", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일부 구현에 따른, 예시적인 네트워크 환경의 도식이다. 도 2는 일부 구현에 따른, 회귀 모델을 위한 트레이닝 환경의 도식이다. 도 3은 일부 구현에 따른, 얼굴 추적 시스템의 도식이다. 도 4는 일부 구현에 따른, 로버스트 애니메이션을 생성하도록 구성된 회귀 모델 및 얼굴 검출 모델의 프로세스 흐름도이다. 도 5a 및 5b는 일부 구현에 따른, 얼굴 검출 모델에 대한 예시적인 출력 네트워크의 개략도이다. 도 6은 일부 구현에 따른, 로버스트 애니메이션을 생성하도록 구성된 회귀 모델 및 얼굴 검출 모델의 단순화된 프로세스 흐름도이다.도 7은 일부 구현에 따른, 비디오로부터의 실시간 로버스트 얼굴 애니메이션의 예시적인 방법의 흐름도이다. 도 8은 일부 구현에 따른, 여기에서 설명된 하나 이상의 특징을 구현하는 데 사용될 수 있는 예시적인 컴퓨팅 장치를 예시하는 블록도이다."}
