{"patent_id": "10-2014-0122959", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2016-0032608", "출원번호": "10-2014-0122959", "발명의 명칭": "지능형 휴머노이드 로봇", "출원인": "국민대학교산학협력단", "발명자": "문대영"}}
{"patent_id": "10-2014-0122959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "지능형 휴머노이드 로봇 시스템으로서,센서 데이터에 기초하여 로봇의 미션 수행 계획을 생성하는 플래닝 모듈(Planning module); 상기 플래닝 모듈의지시에 따라 로봇의 동작을 제어하는 액션 모듈(Action module); 맵 리딩(Map reading)을 통한 데이터에 기초하여 로봇 간 협업을 처리하는 로봇 통신 모듈(Robot communication module); 및 카메라의 영상 데이터에 기초하여 로봇이 미션을 수행하기 위한 판단을 내리는 기준 데이터를 생성하는 비젼 인식 모듈(Vision recognitionmodule)을 포함하는 지능형 휴머노이드 로봇 시스템."}
{"patent_id": "10-2014-0122959", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "센서 데이터에 기초하여 로봇의 미션 수행 계획을 생성하는 플래닝 모듈(Planning module); 상기 플래닝 모듈의 지시에 따라 로봇의 동작을 제어하는 액션 모듈(Action module); 맵 리딩(Map reading)을 통한 데이터에 기초하 여 로봇 간 협업을 처리하는 로봇 통신 모듈(Robot communication module); 및 카메라의 영상 데이터에 기초하 여 로봇이 미션을 수행하기 위한 판단을 내리는 기준 데이터를 생성하는 비젼 인식 모듈(Vision recognition module)을 포함하는 지능형 휴머노이드 로봇 시스템이 제공된다."}
{"patent_id": "10-2014-0122959", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 미션을 수행하는 지능형 휴머노이드 로봇에 관한 것이다."}
{"patent_id": "10-2014-0122959", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 TV 등 매체를 통해 휴머노이드를 포함한 여러 종류의 로봇들이 인간을 대신하여 다양한 업무를 수행하는 모습을 쉽게 접할 수 있다. 하지만 2011년도 일본에서 발생한 후쿠시마 원전사고 당시, 사고로 인해 방사능에 유출된 위험 지역에 로봇을 투입하는 것에는 한계가 있었다. 결국 인간이 직접 들어가 문제를 해결해야만 했다. 이때의 사건 이후, 미국과 일본을 필두로 한 세계 각국의 나라 및 단체에서 인간의 힘으로 해결할 수 없는 다양 한 일들을 휴머노이드가 수행하게 함으로써 인간이 위험에 노출 될 상황을 최소화하자는 움직임을 보이고 있다. 즉, 주어진 환경에서 여러 미션들을 해결할 수 있는 휴머노이드 및 그 알고리즘의 개발의 필요성이 대두되고 있 다."}
{"patent_id": "10-2014-0122959", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 외부 환경을 인식하고 스스로 상황을 판단하여 자율적으로 동작하는 휴머노이드 로봇으로서, 위치 파 악, 임무(mission) 파악, 임무 수행의 정확성 및 안정성을 높인 지능형 휴머노이드 로봇을 제공하기 위한 것이다."}
{"patent_id": "10-2014-0122959", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 측면에 따르면, 지능형 휴머노이드 로봇 시스템으로서, 센서 데이터에 기초하여 로봇의 미션 수행 계획을 생성하는 플래닝 모듈(Planning module); 상기 플래닝 모듈의 지시에 따라 로봇의 동작을 제어하는 액션 모듈(Action module); 맵 리딩(Map reading)을 통한 데이터에 기초하 여 로봇 간 협업을 처리하는 로봇 통신 모듈(Robot communication module); 및 카메라의 영상 데이터에 기초하 여 로봇이 미션을 수행하기 위한 판단을 내리는 기준 데이터를 생성하는 비젼 인식 모듈(Vision recognition module)을 포함하는 지능형 휴머노이드 로봇 시스템이 제공된다."}
{"patent_id": "10-2014-0122959", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예에 의하면, 외부 환경을 인식하고 스스로 상황을 판단하여 자율적으로 동작하는 휴머노이드 로 봇으로서, 위치 파악, 임무(mission) 파악, 임무 수행의 정확성 및 안정성을 높인 지능형 휴머노이드 로봇을 구 현할 수 있는 효과가 있다."}
{"patent_id": "10-2014-0122959", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변환을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변환, 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 본 발명을 설명함에 있어서, 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 본 명세서의 설명 과정에서 이용되는 숫자(예를 들어, 제1, 제2 등)는 하나의 구성요소를 다른 구성요소와 구분하기 위한 식별기호에 불과하다. 본 발명의 실시예에서 지능형 휴머노이드 로봇이 수행하는 미션은 크게 두 가지이다. 첫 번째는 라인이 주어진 경기장에서의 릴레이 경주(라인 트레이싱)를 펼치는 미션이고, 두 번째는 센서 인식과 영상처리 알고리즘을 통 해 사물을 인식하고 특정 지점을 기준으로 지정된 장소에 위치하는 미션이다. 이를 위해, 카메라 및 센서가 부착된 완성된 휴머노이드 로봇이 사용된다. 그리고 위와 같은 미션을 수행하기 위해, 본 발명의 실시예에 따른 지능형 휴머노이드 로봇에 적용된 핵심 기술들은 영상처리 기술, 신경망 처리 기술, 인공지능 기술, OS 관련 기술, 무선 통신 기술, HRI 기술, 프로그래밍 기술이다. 영상처리 기술은 임베디드 보드에 연결된 카메라를 통해 영상 데이터를 받으며, 카메라 버퍼로부터 프레임 데이 터를 읽어와 목적에 맞게 처리하는 기술이다. 구체적으로는 color extraction(특정 색상 추출), panorama(시야 각 확장), stereo vision(입체공간인식)이 있다. 신경망 처리 기술은 영상을 통해 물체의 패턴을 발견하고 잡음 을 제거하여 물체를 인식하기 위한 기술이다. 구체적으로는 labeling(일정 영역을 객체로 인식), size filtering(영상의 불필요한 부분 제거), noise filtering(영상 내 노이즈 제거)이 있다. 인공지능 기술은 A* 등 최단 거리 알고리즘을 활용한 알고리즘 기술, 임무수행을 위한 planning 기술, rule-based system, vectorize (영상을 통해 로봇의 방향 및 동작 제어), SLAM(공간 탐색 및 공간 위치 인식)이다. OS 관련 기술은 임베디드 보드를 위한 커널 설치 및 포팅을 위한 리눅스 관련 기술이다. 무선 통신 기술은 적외선 센서, zigbee 센서, 블 루투스 등을 이용한 기술이다. 무선 통신 기술을 통해 서버 로봇과 클라이언트 로봇을 구분하며 각 로봇들이 상 호 작용하도록 할 수 있다. HRI 기술은 휴머노이드 분야의 연구를 위해 로봇과 서버간의 상호작용을 제어하는 기술이다. 프로그래밍 기술은 동아리 팀원들이 사용할 수 있는 프로그래밍 언어에 대한 기술이다. 동아리 구성원이 사용할 수 있는 프로그래밍 언어는 ROBOBASIC(휴머노이드 로봇의 동작 제어), C, C++, JAVA, PYTHON이다. 미션 수행을 위한 소프트웨어 알고리즘과 그 구현 방식에 관해서는 이하 첨부된 도면들을 참조하여 차례로 설명 하기로 한다. 도 1은 본 발명의 실시예에 따른 지능형 휴머노이드 로봇에 관한 전체 시스템의 4가지 모듈의 알고리즘 구성도 이다. 도 1을 참조하면, 시스템은 다음과 같이 4개의 모듈로 구성된다. 액션 모듈(Action module)은 플래닝 모듈(Planning module)의 지시에 따라 로봇의 동작을 제어한다. 플래닝 모 듈은 다른 모듈 및 센서의 데이터에 기초하여 로봇의 미션 수행 계획을 생성한다. 로봇 통신 모듈(Robot communication module)은 맵 리딩(Map reading)을 통한 데이터에 기초하여 로봇 간 협업을 처리한다. 그리고 비젼 인식 모듈(Vision recognition module)은 카메라의 영상 데이터에 기초하여 로봇이 미션을 수행하기 위한 판단을 내리는 기준 데이터를 생성한다. 지능형 휴머노이드 로봇의 알고리즘은 위의 4개의 모듈에 따라 크게 위 치 파악, 임무 파악, 임무 수행으로 구분될 수 있다. 위치 파악 알고리즘 임무 수행에 필요한 물체의 위치를 파악하고 그를 통한 전체적인 map reading이 필요한데, 정확한 위치를 파악 하기 위해서 구현해야 할 알고리즘으로서 파노라마(Panorama) 기능이 적용된다. 파노라마(Panorama)는 전체 이미지 중에서도 360ㅀ방향의 모든 이미지를 담아내는 기법이나 장치, 또는 그렇게 담아낸 사진이나 그림이다. 일반적으로 카메라를 사용하여 360ㅀ방향의 모든 이미지를 사진 한 장에 담을 수는 없다. 대신 몇 장으로 나누어 촬영한 뒤 옆으로 길게 이어 붙여 한눈에 볼 수 있게 만든다(도 2 참조). 파노라 마 촬영을 할 때는 보통 카메라를 제자리에서 돌리며 찍는다. 본 발명의 실시예에서는 파노라마를 이용하여 로 봇의 가로 시야각 문제를 해결할 것이다. 로봇의 가로 시야각은 좁기 때문에 Map 전체를 인식하기 어렵다. 따라 서 로봇의 머리를 회전시키기 전의 영상과 로봇의 머리를 회전 시킨 후의 영상을 합쳐 90ㅀ이상의 시야각을 확 보하여 Map 전체를 인식한다. 임무 파악(물체 인식) 알고리즘 임무 파악 시 물체 인식이 필요한데, 물체를 인식한 영상데이터를 OpenCV(Open Computer Vision) 라이브러리를 사용하여 처리한다. 그리고 이 라이브러리를 사용해서 구현해야 할 알고리즘은 다음과 같다. 첫 째로, 컬러 추출(Color extraction) 기법이 적용된다. Color extraction은 이미지에서 특정한 색만 남기고 지우는 기술이다. 이와 연관된 기술에는 RGB 기술 Image color-difference signal 기술이 있다. 전자의 경우에 는 RGB signal과 추출된 영역에서의 색의 집합을 range로 정하여 처리하는 기술이고 후자의 경우에는 image signal의 vector 영역에 기반을 두어 상한과 하한 제한을 밝기 signal Y, color signal R-Y, B-Y값을 식으로 계산하여 range를 정하는 방법이다(도 3 참조). 이를 기반으로 로봇에서는 pixel의 RGB값의 크기와 비율을 확인 하여 조건에 맞으면 표시하고 아닌 경우에는 지운다. 둘 째로, 라벨링(Labeling) 기법이 적용된다. Labeling은 heuristic 알고리즘에 기반을 두어 연관요소의 부분집 합을 labeling하는 그래프 이론을 알고리즘적으로 적용한 것이다. 이는 이미지에서의 연관 영역을 탐지하기 위 해 사용한다. Labeling은 pixel에 라벨을 붙여 근처 pixel과 속성이 비슷하면 해당 pixel과 라벨을 일치시킴으 로써 이루어진다. 이로 인해 묶인 pixel들을 하나의 개체로서 인식한다. 그리고 labeling 과정을 통해 얻어진 결과에 대하여 size filtering을 진행한다. 이 과정에서는 인접한 pixel들의 수가 적으면 그 개체는 삭제하게 된다(도 4 참조). 셋 째로, 노이즈 필터링(Noise Filtering) 기법이 적용된다. Noise Filtering은 촬영된 이미지 등에 존재하는 고르지 않은 데이터를 적절히 처리하기 위해 사용되는 기법이다. Noise를 제거하는 방법에는 대표적으로 blocking이 있다. Blocking을 이용한 noise filtering에서는 이미지를 구간으로 나누어 나눈 구간 안에 특정 범 위의 색깔을 가진 pixel이 일정 개수 이상 있으면 그 구간을 전부 특정 색으로 통일한다(도 5 참조).도 6은 본 발명의 실시예에 따른 지능형 휴머노이드 로봇에 관한 전체 시스템 구성도이다. 그리고 도 7은 도 6 의 시스템에서의 시스템 흐름도이다. 도 6을 참조할 때, 시스템의 구성은 카메라의 영상처리를 위한 모듈, 로봇의 움직임을 제어하는 모터 제어모듈, 그리고 주어진 임무를 지능적으로 수행하는 AI모듈로 구성하였다. 각각의 모듈은 독립적으로 실행되며, 세부적 기능은 다음과 같다. CAM 모듈은 영상을 처리하여 진행 방향을 결정하며, 모터 제어 모듈은 모터 제어 보드로 명령을 전달하고, AI 모듈은 영상 및 센서 데이터를 사용하여 임무 수행하며, 센서 모듈은 센서를 통해 데이터를 수집한다. 시스템의 전반적인 흐름은 도 7과 같이 진행된다. 시스템의 주요한 흐름은 AI module이 이끌어 간다. 일에 따라 plan을 생성하고 사용될 명령들을 결정한다. Cam module로부터 영상을 분석한 data를 얻고, 그 데이터에 따라 Action module에 필요한 동작의 수행을 요청하는 방식으로 Module 간 통신을 한다. 임무 수행 알고리즘 도 8은 릴레이 경주(라인 트레이싱)을 위한 플래닝 플로우 차트를 예시한 도면이다. 로봇 1이 라인 트레이싱을 하다가 로봇 2를 만나면 로봇 2를 작동시키면서 멈춘다. 그 후 로봇 2가 라인 트레이싱을 하다가 멈춘 로봇 1을 만나면 릴레이가 끝난다. 그리고 도 9 및 도 10은 미션 수행을 위한 플래닝 플로우 차트를 예시한 도면이다. 도 9를 참조할 때, 임무 수행의 제1 플래닝 플로우 차트에서는, 로봇 1이 공을 우선 밀고 우유팩을 잡아 이동시 키는 미션을 수행하고, 로봇 2는 시작하자마자 바로 우유팩을 잡아 이동시키는 미션을 수행한다. 그리고 도 10 을 참조할 때, 임무 수행의 제2 플래닝은 제 1의 Planning에 블로킹 작업이 수행되었다. 한 로봇이 우유팩을 이 동시키는 미션을 수행중이면, 다른 로봇이 블로킹을 하는 형태를 취한다. 두 대의 로봇이 서로 통신하면서 번갈 아가며 우유팩을 넘기고 블로킹을 한다. 이상에서는 본 발명의 실시예를 참조하여 설명하였지만, 해당 기술 분야에서 통상의 지식을 가진 자라면 하기의 특허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수 정 및 변경시킬 수 있음을 쉽게 이해할 수 있을 것이다."}
{"patent_id": "10-2014-0122959", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 지능형 휴머노이드 로봇에 관한 전체 시스템의 4가지 모듈의 알고리즘 구성도. 도 2는 지능형 휴머노이드 로봇에서 넓은 가로 시야각을 구현할 수 있는 파노라마 기법을 예시한 도면. 도 3은 지능형 휴머노이드 로봇의 RGB 색 인식 범위를 나타낸 도면. 도 4는 지능형 휴머노이드 로봇에서 라벨링을 통한 이미지 변경을 예시한 도면. 도 5는 지능형 휴머노이드 로봇에서 블록킹을 통한 이미지 변경을 예시한 도면. 도 6은 본 발명의 실시예에 따른 지능형 휴머노이드 로봇에 관한 전체 시스템 구성도. 도 7은 도 6의 시스템에서의 시스템 흐름도. 도 8은 라인트레이싱을 위한 플래닝 플로우 차트를 예시한 도면. 도 9 및 도 10은 미션 수행을 위한 플래닝 플로우 차트를 예시한 도면."}
