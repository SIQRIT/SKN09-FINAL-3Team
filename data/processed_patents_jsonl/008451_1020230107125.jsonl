{"patent_id": "10-2023-0107125", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0026006", "출원번호": "10-2023-0107125", "발명의 명칭": "선택적 정규화를 통한 클래스 증분 학습", "출원인": "아주대학교산학협력단", "발명자": "황원준"}}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "학습할 신규 클래스의 학습 데이터에 포함된 학습 이미지의 각 채널별로, 현재 모델의 각 레이어별 분류 손실(classification loss) 기울기 및 지식 증류 손실(knowledge distillation loss) 기울기를 산출하는 단계;산출된 분류 손실 기울기 및 지식 증류 손실 기울기에 기초하여, 상기 채널별 및 레이어별로 특징 증류를 수행하는 단계;기 학습된 기존 클래스에 대한 이전 모델의 제1 신뢰도와 상기 현재 모델의 제2 신뢰도를 산출하는 단계; 및상기 제1 신뢰도와 상기 제2 신뢰도의 차이에 기초하여, 상기 이전 모델의 파라미터를 선택적으로 적용하여 상기 현재 모델의 파라미터를 업데이트하는 단계를 포함하는,학습 방법."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 분류 손실 기울기 및 지식 증류 손실 기울기를 산출하는 단계는,상기 신규 클래스의 학습 이미지를 상기 현재 모델로 입력하는 단계;상기 학습 이미지에 대한 상기 현재 모델의 예측 결과와, 상기 학습 데이터에 포함된 레이블에 기초하여 분류손실을 산출하는 단계; 및산출된 분류 손실에 기초하여 상기 분류 손실 기울기를 산출하는 단계를 포함하는,학습 방법."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 산출된 분류 손실에 기초하여 상기 분류 손실 기울기를 산출하는 단계는,역전파(backpropagation) 알고리즘을 이용하여 상기 산출된 분류 손실로부터 상기 분류 손실 기울기를 산출하는단계를 포함하는,학습 방법."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 분류 손실 기울기 및 지식 증류 손실 기울기를 산출하는 단계는,상기 신규 클래스의 학습 이미지를 상기 이전 모델 및 상기 현재 모델로 입력하는 단계;상기 학습 이미지에 대한 상기 이전 모델의 제1 예측 확률 분포와 상기 현재 모델의 제2 예측 확률 분포를 산출하는 단계; 상기 제1 예측 확률 분포와 상기 제2 예측 확률 분포의 차이에 기초하여 지식 증류 손실을 산출하는 단계; 및공개특허 10-2025-0026006-3-산출된 지식 증류 손실에 기초하여 상기 지식 증류 손실 기울기를 산출하는 단계를 포함하는,학습 방법."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 특징 증류를 수행하는 단계는,상기 분류 손실 기울기 및 상기 지식 증류 손실 기울기 간의 유사도를 확인하는 단계; 및확인된 유사도가 기 설정된 조건을 만족하는 채널 및 레이어에 대해 특징 증류를 수행하는 단계를 포함하는,학습 방법."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 유사도를 확인하는 단계는,상기 분류 손실 기울기와 상기 지식 증류 손실 기울기 간의 코사인 유사도(cosine similarity)를 산출함으로써상기 유사도를 확인하는 단계를 포함하고,상기 기 설정된 조건은 상기 코사인 유사도가 0 이상인 조건에 해당하는,학습 방법."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 현재 모델의 파라미터를 업데이트하는 단계는,상기 제1 신뢰도와 상기 제2 신뢰도의 차이가 임계치 이상인 경우, 상기 이전 모델의 파라미터를 기 설정된 비율만큼 반영하여 상기 현재 모델의 파라미터를 보간하는 단계; 및상기 제1 신뢰도와 상기 제2 신뢰도의 차이가 임계치 미만인 경우, 상기 이전 모델의 파라미터를 반영하지 않고상기 현재 모델의 파라미터를 업데이트하는 단계를 포함하는,학습 방법."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 임계치는 학습된 클래스의 수에 따라 가변 설정되는,학습 시스템."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서에 전기적으로 연결되는 메모리를 포함하고,공개특허 10-2025-0026006-4-상기 메모리는, 상기 적어도 하나의 프로세서가 실행 시에,학습할 신규 클래스의 학습 데이터에 포함된 학습 이미지의 각 채널별로, 현재 모델의 각 레이어별 분류 손실기울기 및 지식 증류 손실 기울기를 산출하고,산출된 분류 손실 기울기 및 지식 증류 손실 기울기에 기초하여, 상기 채널별 및 레이어별로 특징 증류를 수행하고,기 학습된 기존 클래스에 대한 이전 모델의 제1 신뢰도와 상기 현재 모델의 제2 신뢰도를 산출하고,상기 제1 신뢰도와 상기 제2 신뢰도의 차이에 기초하여, 상기 이전 모델의 파라미터를 선택적으로 적용하여 상기 현재 모델의 파라미터를 업데이트하는 인스트럭션들을 저장하는,학습 디바이스."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 메모리는, 상기 적어도 하나의 프로세서가 실행 시에,상기 신규 클래스의 학습 이미지를 상기 현재 모델로 입력하고,상기 학습 이미지에 대한 상기 현재 모델의 예측 결과와, 상기 학습 데이터에 포함된 레이블에 기초하여 상기분류 손실 기울기를 산출하는 인스트럭션들을 저장하는,학습 디바이스."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 메모리는, 상기 적어도 하나의 프로세서가 실행 시에,상기 신규 클래스의 학습 이미지를 상기 이전 모델 및 상기 현재 모델로 입력하고,상기 학습 이미지에 대한 상기 이전 모델의 제1 예측 확률 분포와 상기 현재 모델의 제2 예측 확률 분포를 산출하고,상기 제1 예측 확률 분포와 상기 제2 예측 확률 분포의 차이에 기초하여 상기 지식 증류 손실 기울기를 산출하는 인스트럭션들을 저장하는,학습 디바이스."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,상기 메모리는, 상기 적어도 하나의 프로세서가 실행 시에,상기 분류 손실 기울기 및 상기 지식 증류 손실 기울기 간의 유사도를 확인하고,확인된 유사도가 기 설정된 조건을 만족하는 채널 및 레이어에 대해 특징 증류를 수행하는 인스트럭션들을 저장하는,학습 디바이스."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "공개특허 10-2025-0026006-5-제12항에 있어서,상기 유사도는 상기 분류 손실 기울기와 상기 지식 증류 손실 기울기 간의 코사인 유사도를 포함하고,상기 기 설정된 조건은 상기 코사인 유사도가 0 이상인 조건에 해당하는,학습 디바이스."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제9항에 있어서,상기 메모리는, 상기 적어도 하나의 프로세서가 실행 시에,상기 제1 신뢰도와 상기 제2 신뢰도의 차이가 임계치 이상인 경우, 상기 이전 모델의 파라미터를 기 설정된 비율만큼 반영하여 상기 현재 모델의 파라미터를 보간하고,상기 제1 신뢰도와 상기 제2 신뢰도의 차이가 임계치 미만인 경우, 상기 이전 모델의 파라미터를 반영하지 않고상기 현재 모델의 파라미터를 업데이트하는 인스트럭션들을 저장하는,학습 디바이스."}
{"patent_id": "10-2023-0107125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1항의 학습 방법을 수행하는 인스트럭션들을 포함하고 적어도 하나의 컴퓨팅 장치에 의해 실행가능한 컴퓨터프로그램."}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 기술적 사상에 의한 일 양태에 따르면, 학습할 신규 클래스의 학습 데이터에 포함된 학습 이미지의 각 채널별로, 현재 모델의 각 레이어별 분류 손실 기울기 및 지식 증류 손실 기울기를 산출하는 단계; 산출된 분류 손실 기울기 및 지식 증류 손실 기울기에 기초하여, 상기 채널별 및 레이어별로 특징 증류를 수행하는 단계; 기 학습된 기존 클래스에 대한 이전 모델의 제1 신뢰도와 상기 현재 모델의 제2 신뢰도를 산출하는 단계; 및 상기 제1 신뢰도와 상기 제2 신뢰도의 차이에 기초하여, 상기 이전 모델의 파라미터를 선택적으로 적용하여 상기 현재 모델의 파라미터를 업데이트하는 단계를 포함하는 학습 방법이 제공된다."}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시(disclosure)의 기술적 사상은 클래스 증분 학습 방법에 대한 것으로서, 보다 상세하게는 이전 모델로부 터 지식을 전달받는 것이 이로운지 여부를 판단하여 현재 모델을 선택적으로 정규화하여 현재 모델에 대한 클래 스 증분 학습을 수행하는 방법에 관한 것이다."}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "딥러닝(deep learning)의 성공적인 발전은 현실 세계에서 다양한 비즈니스와 실용적인 응용 프로그램의 등장을 야기시켰다. 그러나 종래의 딥러닝에 대한 연구는 다소 제한된 조건에 국한되어 왔다. 인간의 능력과 비교 가능 한 수준의 목표를 달성하기 위해서는 현실 세계 환경의 변화를 고려하는 것이 중요할 수 있다. 이에 따라 최근 인공지능 분야에서는 지속 학습(continual learning)에 대한 관심이 증가하고 있다. 지속 학습 은 로봇 비전 및 자율 주행 시스템 등과 같이 시각적 특성이 변화하는 실제 상황에 지속적으로 적응할 수 있는 방법론이다. 특히 클래스 증분 학습은 점진적으로 새로운 물체의 종류를 학습하면서, 이전에 학습하였던 물체에 대한 성능을 유지 또는 향상하는 것을 목표로 한다. 최근 클래스 증분 학습에 관한 기술이 발전하고 있으나, 여 전히 새로운 정보를 학습할 때 이전에 습득한 지식을 잊어버리는 현상(치명적 망각(catastrophic forgetting)) 으로 인해 딥러닝 모델의 성능이 저하되는 문제가 발생한다."}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 일 과제는, 클래스 증분 학습 시 이전에 학습하였던 물체에 대한 인식 성능을 유지 할 수 있는 모델의 학습 위한 방법을 제공하는 것이다."}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기와 같은 목적을 달성하기 위하여, 본 개시의 기술적 사상에 의한 일 양태(aspect)에 따른 학습 방법은, 학 습할 신규 클래스의 학습 데이터에 포함된 학습 이미지의 각 채널별로, 현재 모델의 각 레이어별 분류 손실 기 울기 및 지식 증류 손실 기울기를 산출하는 단계; 산출된 분류 손실 기울기 및 지식 증류 손실 기울기에 기초하 여, 상기 채널별 및 레이어별로 특징 증류를 수행하는 단계; 기 학습된 기존 클래스에 대한 이전 모델의 제1 신 뢰도와 상기 현재 모델의 제2 신뢰도를 산출하는 단계; 및 상기 제1 신뢰도와 상기 제2 신뢰도의 차이에 기초하 여, 상기 이전 모델의 파라미터를 선택적으로 적용하여 상기 현재 모델의 파라미터를 업데이트하는 단계를 포함 한다. 일 실시 예에 따라, 상기 분류 손실 기울기 및 지식 증류 손실 기울기를 산출하는 단계는, 상기 신규 클래스의 학습 이미지를 상기 현재 모델로 입력하는 단계; 상기 학습 이미지에 대한 상기 현재 모델의 예측 결과와, 상기 학습 데이터에 포함된 레이블에 기초하여 분류 손실을 산출하는 단계; 및 산출된 분류 손실에 기초하여 상기 분 류 손실 기울기를 산출하는 단계를 포함할 수 있다. 일 실시 예에 따라, 상기 산출된 분류 손실에 기초하여 상기 분류 손실 기울기를 산출하는 단계는, 역전파 (backpropagation) 알고리즘을 이용하여 상기 산출된 분류 손실로부터 상기 분류 손실 기울기를 산출하는 단계 를 포함할 수 있다. 일 실시 예에 따라, 상기 분류 손실 기울기 및 지식 증류 손실 기울기를 산출하는 단계는, 상기 신규 클래스의 학습 이미지를 상기 이전 모델 및 상기 현재 모델로 입력하는 단계; 상기 학습 이미지에 대한 상기 이전 모델의 제1 예측 확률 분포와 상기 현재 모델의 제2 예측 확률 분포를 산출하는 단계; 상기 제1 예측 확률 분포와 상기 제2 예측 확률 분포의 차이에 기초하여 지식 증류 손실을 산출하는 단계; 및 산출된 지식 증류 손실에 기초하여 상기 지식 증류 손실 기울기를 산출하는 단계를 포함할 수 있다. 일 실시 예에 따라, 상기 특징 증류를 수행하는 단계는, 상기 분류 손실 기울기 및 상기 지식 증류 손실 기울기 간의 유사도를 확인하는 단계; 및 확인된 유사도가 기 설정된 조건을 만족하는 채널 및 레이어에 대해 특징 증 류를 수행하는 단계를 포함할 수 있다. 일 실시 예에 따라, 상기 유사도를 확인하는 단계는, 상기 분류 손실 기울기와 상기 지식 증류 손실 기울기 간 의 코사인 유사도(cosine similarity)를 산출함으로써 상기 유사도를 확인하는 단계를 포함하고, 상기 기 설정 된 조건은 상기 코사인 유사도가 0 이상인 조건에 해당할 수 있다. 일 실시 예에 따라, 상기 현재 모델의 파라미터를 업데이트하는 단계는, 상기 제1 신뢰도와 상기 제2 신뢰도의 차이가 임계치 이상인 경우, 상기 이전 모델의 파라미터를 기 설정된 비율만큼 반영하여 상기 현재 모델의 파라 미터를 보간하는 단계; 및 상기 제1 신뢰도와 상기 제2 신뢰도의 차이가 임계치 미만인 경우, 상기 이전 모델의 파라미터를 반영하지 않고 상기 현재 모델의 파라미터를 업데이트하는 단계를 포함할 수 있다. 일 실시 예에 따라, 상기 임계치는 학습된 클래스의 수에 따라 가변 설정될 수 있다. 본 개시의 기술적 사상에 의한 일 양태에 따른 학습 디바이스는, 적어도 하나의 프로세서; 및 상기 적어도 하나 의 프로세서에 전기적으로 연결되는 메모리를 포함하고, 상기 메모리는, 상기 적어도 하나의 프로세서가 실행 시에, 학습할 신규 클래스의 학습 데이터에 포함된 학습 이미지의 각 채널별로, 현재 모델의 각 레이어별 분류 손실 기울기 및 지식 증류 손실 기울기를 산출하고, 산출된 분류 손실 기울기 및 지식 증류 손실 기울기에 기초 하여, 상기 채널별 및 레이어별로 특징 증류를 수행하고, 기 학습된 기존 클래스에 대한 이전 모델의 제1 신뢰 도와 상기 현재 모델의 제2 신뢰도를 산출하고, 상기 제1 신뢰도와 상기 제2 신뢰도의 차이에 기초하여, 상기 이전 모델의 파라미터를 선택적으로 적용하여 상기 현재 모델의 파라미터를 업데이트하는 인스트럭션들을 저장 한다."}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 기술적 사상에 따른 클래스 증분 학습 방법은, 기울기 기반 특징 증류와 신뢰도 기반 파라미터 업데 이트를 통해, 이전 모델의 지식을 유지하면서 신규 클래스에 대한 효과적인 학습을 수행하도록 한다. 따라서, 본 학습 방법에 따르면, 종래의 클래스 증분 학습의 문제점인 안정성-가변성(stability-plasticity) 문제를 개 선한 고성능의 모델을 구축할 수 있다. 본 개시의 기술적 사상을 통해 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은"}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "또 다른 효과들은 아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해 될 수 있을 것이다."}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시의 기술적 사상에 따른 예시적인 실시예들은 당해 기술 분야에서 통상의 지식을 가진 자에게 본 개시의 기술적 사상을 더욱 완전하게 설명하기 위하여 제공되는 것으로, 아래의 실시예들은 여러 가지 다른 형태로 변 형될 수 있으며, 본 개시의 기술적 사상의 범위가 아래의 실시예들로 한정되는 것은 아니다. 오히려, 이들 실시 예들은 본 개시를 더욱 충실하고 완전하게 하며 당업자에게 본 발명의 기술적 사상을 완전하게 전달하기 위하여 제공되는 것이다. 본 개시에서 제1, 제2 등의 용어가 다양한 부재, 영역, 층들, 부위 및/또는 구성 요소들을 설명하기 위하여 사 용되지만, 이들 부재, 부품, 영역, 층들, 부위 및/또는 구성 요소들은 이들 용어에 의해 한정되어서는 안 됨은 자명하다. 이들 용어는 특정 순서나 상하, 또는 우열을 의미하지 않으며, 하나의 부재, 영역, 부위, 또는 구성 요소를 다른 부재, 영역, 부위 또는 구성 요소와 구별하기 위하여만 사용된다. 따라서, 이하 상술할 제1 부재, 영역, 부위 또는 구성 요소는 본 개시의 기술적 사상의 가르침으로부터 벗어나지 않고서도 제2 부재, 영역, 부 위 또는 구성 요소를 지칭할 수 있다. 예를 들면, 본 개시의 권리 범위로부터 이탈되지 않은 채 제1 구성 요소 는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소도 제1 구성 요소로 명명될 수 있다. 달리 정의되지 않는 한, 여기에 사용되는 모든 용어들은 기술 용어와 과학 용어를 포함하여 본 개시의 개념이 속하는 기술 분야에서 통상의 지식을 가진 자가 공통적으로 이해하고 있는 바와 동일한 의미를 지닌다. 또한, 통상적으로 사용되는, 사전에 정의된 바와 같은 용어들은 관련되는 기술의 맥락에서 이들이 의미하는 바와 일관 되는 의미를 갖는 것으로 해석되어야 하며, 여기에 명시적으로 정의하지 않는 한 과도하게 형식적인 의미로 해 석되어서는 아니 될 것이다. 어떤 실시예가 달리 구현 가능한 경우에 특정한 공정 순서는 설명되는 순서와 다르게 수행될 수도 있다. 예를 들면, 연속하여 설명되는 두 공정이 실질적으로 동시에 수행될 수도 있고, 설명되는 순서와 반대의 순서로 수행 될 수도 있다. 또한, 본 명세서에 기재된 \"~부\", \"~기\", \"~자\", \"~모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 프로세서(Processor), 마이크로 프로세서(Micro Processer), 마이크로 컨트롤러(Micro Controller), CPU(Central Processing Unit), GPU(Graphics Processing Unit), APU(Accelerate Processor Unit), DSP(Drive Signal Processor), ASIC(Application Specific Integrated Circuit), FPGA(FieldProgrammable Gate Array) 등과 같은 하드웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있으며, 적어도 하나의 기능이나 동작의 처리에 필요한 데이터를 저장하는 메모리(memory)와 결합되는 형태 로 구현될 수도 있다. 그리고, 본 명세서에서의 구성부들에 대한 구분은 각 구성부가 담당하는 주기능별로 구분한 것에 불과함을 명확 히 하고자 한다. 즉, 이하에서 설명할 2개 이상의 구성부가 하나의 구성부로 합쳐지거나 또는 하나의 구성부가 보다 세분화된 기능별로 2개 이상으로 분화되어 구비될 수도 있다. 그리고 이하에서 설명할 구성부 각각은 자신 이 담당하는 주기능 이외에도 다른 구성부가 담당하는 기능 중 일부 또는 전부의 기능을 추가적으로 수행할 수 도 있으며, 구성부 각각이 담당하는 주기능 중 일부 기능이 다른 구성부에 의해 전담되어 수행될 수도 있음은 물론이다. 여기에서 사용된 '및/또는' 용어는 언급된 부재들의 각각 및 하나 이상의 모든 조합을 포함한다. 이하에서는 첨부한 도면들을 참조하여 본 개시의 기술적 사상에 의한 실시예들에 대해 상세히 설명한다. 도 1은 본 개시의 예시적 실시예에 따른 선택적 정규화를 통한 클래스 증분 학습 방법의 개략적인 플로우차트이 다. 도 2는 본 개시의 예시적 실시예에 따른 선택적 정규화를 통한 클래스 증분 학습 동작을 나타낸 개념도이다. 도 1을 참조하면, 본 개시의 실시예에 따른 클래스 증분 학습 방법은, 신규 클래스의 학습 데이터에 대한 분류 손실(classification loss) 기울기, 및 지식 증류 손실(knowledge distillation (KD) loss) 기울기에 기초하여 특징 증류(feature distillation)를 수행하는 단계(S100)를 포함할 수 있다. 본 개시에 따른 특징 증류 방법은 인지 과학(cognitive science)의 패턴 통합(pattern completion)과 패턴 분 리(pattern separation)에 기초하여, 이전 학습된 클래스와 새롭게 학습하는 클래스 간에 비대칭 학습을 적용할 수 있다. 상기 패턴 통합은 외부 자극에 의해 기억이 조정될 때, 기억된 정보의 일부분이 부족하거나 손실되더 라도 완전한 기억을 복원하려는 과정이다. 패턴 통합은 새로운 정보를 받아들이는 데 있어서 기존의 기억을 활 용하여 빈틈을 채우는 과정이다. 상기 패턴 분리는 새로운 정보가 기존 기억과 구별되어 저장되는 과정이다. 패 턴 분리는 기억들이 서로 겹치거나 혼동되는 것을 방지하여 기억의 정확성과 신뢰성을 유지한다. 이러한 패턴 통합과 패턴 분리의 개념을 본 개시에 반영하자면, 패턴 통합은 새로운 클래스의 특징이 이전 학습 된 클래스와 유사함을 의미하고, 패턴 분리는 새로운 클래스의 특징이 이전 학습된 클래스와 상이함을 의미할 수 있다. 상술한 개념에 기초하여, 본 개시의 실시예에 따른 특징 증류 방법은 상기 분류 손실 기울기와 지식 증류 손실 기울기를 이용한 각 채널의 선택적 특징 증류를 통해 비대칭 학습을 수행할 수 있다. 본 명세서에서는 상기 특 징 증류 방법을 기울기 기반 특징 증류(gradient-based feature distillation)으로 정의한다. 상기 기울기 기반 특징 증류 방법에 대해서는 추후 도 2, 도 4, 및 도 5를 통해 보다 상세히 설명하기로 한다. 또한, 본 개시의 실시예에 따른 클래스 증분 학습 방법은, 기존 클래스의 데이터에 대한 이전 모델의 신뢰도와 현재 모델의 신뢰도에 기초하여, 현재 모델의 파라미터(가중치)를 업데이트하는 단계(S200)를 포함할 수 있다. 지속 학습 또는 클래스 증분 학습에서, 새로운 지식(새로운 클래스)을 학습하는 동안 이전 지식을 망각하는 문 제(이전 클래스에 대한 인식 정확도 저하 문제)가 존재하며, 이는 모델의 안정성을 저하시키는 원인이다. 이를 해소하기 위해, 본 개시에서는 현재 모델의 신뢰도를 통해 모델이 이전 지식을 유지하고 있는지 여부를 판단하 여, 현재 모델의 파라미터 업데이트 시 이전 모델의 파라미터를 현재 모델에 보간할 것인지 여부를 결정할 수 있다. 한편, 상기 이전 모델은 상기 새로운 클래스에 대한 학습이 수행되지 않는 모델에 해당하고, 현재 모델은 클래 스 증분 학습에 따라 새로운 클래스에 대한 학습을 수행하는 모델에 해당한다. S200 단계에 대해서는 추후 도 3 및 도 6을 통해 보다 상세히 설명하기로 한다. 도 2 내지 도 3은 도 1의 클래스 증분 학습 방법에 대한 구체적인 실시예들을 설명하기 위한 플로우차트이다. 도 4 내지 도 5는 본 개시의 예시적 실시예에 따른 기울기 기반의 특징 증류 방법을 설명하기 위한 예시도이다.도 6은 본 개시의 예시적 실시예에 따른 신뢰도 기반 파라미터 업데이트 방법을 설명하기 위한 예시도이다. 이하 본 명세서에서는 설명의 편의를 위해, 본 개시의 실시예에 따른 클래스 증분 학습 방법이 적어도 하나의 컴퓨팅 장치를 포함하는 학습 시스템에 의해 수행되는 것으로 설명한다. 도 2를 참조하면, S100 단계에서 상술한 기울기 기반 특징 증류 방법은 신규 클래스의 학습 데이터를 이전 모델 및 현재 모델에 입력하는 단계(S110), 및 채널 각각의 분류 손실 기울기 및 지식 증류 손실 기울기를 산출하는 단계(S120)를 포함할 수 있다. 예컨대, 상기 학습 데이터는 학습 이미지, 및 상기 학습 이미지에 포함된 객체에 대한 레이블(클래스 등)을 포 함할 수 있다. 상기 학습 시스템은 신규 클래스의 학습 데이터 중 학습 이미지를 현재 모델에 입력하고, 상기 현재 모델로부터 상기 학습 이미지에 포함된 객체의 클래스에 대한 예측 결과를 획득할 수 있다. 상기 학습 시스템은, 획득된 예측 결과와 상기 학습 데이터에 포함된 레이블에 기초하여, 상기 현재 모델의 분 류 손실을 산출할 수 있다. 예컨대 상기 분류 손실은 교차 엔트로피(cross-entropy) 손실 함수를 이용하여 산출 될 수 있으나, 반드시 그러한 것은 아니다. 상기 학습 시스템은, 산출된 분류 손실에 기초하여 분류 손실 기울기를 산출할 수 있다. 예컨대, 상기 학습 시 스템은 역전파(backpropagation) 알고리즘을 이용하여 상기 분류 손실 기울기를 산출할 수 있으나, 이에 한정되 는 것은 아니다. 한편, 상기 분류 손실 및 상기 분류 손실 기울기는 현재 모델의 레이어 각각에 대해 상기 학습 이미지의 채널별 로 산출될 수 있다. 즉, 본 개시의 기울기 기반 특징 증류는 모델의 레이어 및 이미지의 채널 각각에 대한 중간 특징(intermediate feature)에 대해 수행될 수 있다. 또한, 상기 학습 시스템은 상기 학습 데이터에 대해 이전 모델의 예측 확률 분포와 현재 모델의 예측 확률 분포 를 각각 계산하고, 계산된 예측 확률 분포의 차이를 측정함으로써 지식 증류 손실을 산출할 수 있다. 예컨대 상 기 예측 확률 분포는 소프트맥스(softmax) 함수 또는 시그모이드(sigmoid) 함수를 활용하여 계산할 수 있고, 상 기 지식 증류 손실은 크로스 엔트로피 손실 함수를 이용하여 산출될 수 있으나, 반드시 그러한 것은 아니다. 상기 학습 시스템은 산출된 지식 증류 손실에 기초하여 지식 증류 손실 기울기를 산출할 수 있다. 예컨대, 상기 학습 시스템은 역전파 알고리즘을 이용하여 상기 지식 증류 손실 기울기를 산출할 수 있으나, 이에 한정되는 것 은 아니다. 상기 기울기 기반 특징 증류 방법은, 분류 손실 기울기와 지식 증류 손실 기울기 간의 유사도를 확인하는 단계 (S130), 및 확인된 유사도가 기 설정된 조건을 만족하는 채널 및 레이어에 대해 특징 증류를 수행하는 단계 (S140)를 포함할 수 있다. 상기 학습 시스템은 상기 분류 손실 기울기와 상기 지식 증류 손실 기울기 간의 유사도를 확인함으로써, 상기 학습 데이터에 대해 이전 모델로부터 지식(특징)을 전달받는 것이 이로운지(패턴 통합) 또는 해로운지(패턴 분 리) 여부를 판단할 수 있다. 예컨대, 상기 학습 시스템은 각 채널별 및 레이어별로, 상기 분류 손실 기울기와 상기 지식 증류 손실 기울기 간의 코사인 유사도(cosine similarity)를 산출함으로써, 분류 손실 기울기와 지식 증류 손실 기울기 간의 유사 도를 확인할 수 있다. 상기 학습 시스템은 확인 결과에 기초하여 채널별-레이어별 마스크를 생성하고, 생성된 마스크를 이용하여 특징 증류를 적용할 수 있다. 구체적으로, 상기 코사인 유사도가 0 이상인 경우, 즉, 도 4에 도시된 바와 같이 지식 증류 손실 기울기와 분류 손실 기울기가 소정 평면을 기준으로 동일한 방향에 해당할 경우, 이전 모델로부터 지식을 전달받는 것이 이로운 것으로 판단할 수 있다. 따라서, 학습 시스템은 분류 손실 기울기와 지식 증류 손실 기울기 간의 코사인 유사도가 0 이상인 레이어 및 채널에 대해 지식 증류를 적용함으로써 패턴 통합을 달성할 수 있다. 반면, 상기 학습 시스템은 상기 분류 손실 기울기와 상기 지식 증류 손실 기울기 간의 코사인 유사도가 0 미만 인 경우, 즉, 도 4에 도시된 바와 같이 지식 증류 손실 기울기와 분류 손실 기울기가 소정 평면을 기 준으로 서로 다른 방향에 해당할 경우, 이전 모델로부터 지식을 전달받는 것이 해로운 것으로 판단할 수 있다. 따라서, 학습 시스템은 분류 손실 기울기와 지식 증류 손실 기울기 간의 코사인 유사도가 0 미만인 레이어 및 채널에 대해 지식 증류를 배제하여 패턴 분리를 달성할 수 있다. 상기 학습 시스템은 확인 결과에 기초하여, 아래의 수학식 1에 따라 레이어 및 채널별 마스크를 생성할 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 는 l번째 레이어의 채널 c에 대한 마스크(이진 마스크)를 의미하고, 는 l번째 레이 어의 채널 c의 특징에 대한 지식 증류 손실 기울기를 의미하며, 는 l번째 레이어의 채널 c의 특징 에 대한 분류 손실 기울기를 의미한다. 상기 이진 마스크가 활성화될 경우 특징 증류가 적용될 수 있다. 다만, 특정 채널에 대해 전혀 정규화를 적용하지 않을 경우, 모델의 안정성이 저하될 수 있다. 또한 패턴 분리 과정에서 기존 지식과의 차별화를 추구하는 관점에서 기존 패턴을 그대로 유지할 경우 완전한 차별화가 이루어 지지 않을 수 있다. 따라서, 학습 시스템은 기존 클래스의 데이터에 대해서는 패턴 분리를 위해 상기 이진 마스 크와 반대되는 마스크를 적용하고, 신규 클래스의 데이터에 대해서는 상기 이진 마스크를 적용함으로써, 현재 모델이 신규 클래스의 패턴을 잘 구별하면서도 기존의 지식을 유지할 수 있도록 할 수 있다. 한편, 본 개시에 따른 기울기 기반 특징 증류의 목적 함수는 아래의 수학식 2에 따라 정의되고, 특징 증류 손실 은 아래의 수학식 3에 따라 정의될 수 있다. [수학식 2]"}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "[수학식 3]"}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서 와 는 신규 클래스 데이터 및 기존 클래스 데이터를 의미하고, L은 모델의 레이어 수, C는 이미지의 채널 수를 의미한다. 는 프로베니우스 놈(Frobenius norm)으로서, 행렬 또는 벡터의 크기를 측 정하는 방법에 해당한다. 상기 목적 함수를 통해, 각각의 특징은 안정성 있는 학습을 위해 정규화될 수 있으며, 신규 클래스와 기존 클래스 간의 패턴을 효과적으로 구별하면서 모델의 학습이 수행될 수 있다. 도 5의 히스토그램은 신규 클래스에 대한 추가 학습 시 중간 특징의 채널별 변화를 보여준다. 도 5의 (a)와 (b)를 참조하면, 새로운 클래스의 학습 시 일부 채널(601, 602)의 상대적 거리(relative distance)가 급격히 증 가함을 알 수 있다. 해당 채널들(601, 602)은 패턴 분리를 위한 지배적인 채널들에 해당할 수 있다. 본 개시의 실시예에 따르면, 학습 시스템은 해당 채널들(601, 602)에 대해서는 패턴 분리를 수행하고, 다른 채널들에 대해 서는 패턴 통합을 수행함으로써, 새로운 클래스에 대한 지식을 효과적으로 학습하면서도 이전 클래스에 대한 지 식을 유지할 수 있다.도 3을 참조하면, S200 단계에서 상술한 신뢰도 기반 파라미터 업데이트 방법은, 기존 클래스의 데이터에 대해, 이전 모델과 현재 모델 각각의 신뢰도(confidence)를 산출하는 단계(S210), 산출된 신뢰도 간의 차이를 확인하 는 단계(S220), 및 확인된 차이에 기초하여 이전 모델의 파라미터를 현재 모델의 파라미터를 이전 모델의 파라 미터를 통해 선택적으로 보간함으로써 현재 모델의 파라미터를 업데이트하는 단계(S230)를 포함할 수 있다. 클래스 증분 학습에 따른 안정성(stability) 및 가변성(plasticity)의 개선을 위해, 현재 모델의 파라미터 업데 이트 시 이전 모델의 파라미터를 통해 보간하는 방법을 고려할 수 있다. 이 경우, 업데이트된 현재 모델의 파라 미터가 이전 모델의 파라미터와 지나치게 큰 차이를 가지는 것이 방지되어, 안정성의 개선이 가능할 수 있다. 상기 보간 방법은 아래의 수학식 4에 따라 표현될 수 있다. [수학식 4]"}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "는 현재 모델의 파라미터를 의미하고, 는 이전 모델의 파라미터를 의미한다. 그리고 β는 보간 매개 변수에 해당한다. 다만, 이전 모델의 파라미터를 통해 현재 모델의 파라미터를 보간하는 것이 지속될 경우, 가변성 측면에서 한계 를 유발할 수 있다. 이를 해소하기 위해, 학습 시스템은 기존 클래스의 데이터에 대한 모델의 신뢰도를 이용하 여 현재 모델이 기존 클래스에 대한 충분한 지식을 가지고 있는지 여부를 판단할 수 있다. 구체적으로, 학습 시스템은 기존 클래스의 데이터를 이전 모델과 현재 모델 각각으로 입력함으로써, 이전 모델 의 신뢰도 및 현재 모델의 신뢰도 를 산출할 수 있다. 학습 시스 템은 이전 모델의 신뢰도와 현재 모델의 신뢰도 간의 차이가 임계치 이상인 경우, 현재 모델이 기존 클래스에 대한 충분한 지식을 가지고 있지 못한 것으로 판단하고, 임계치 미만인 경우 현재 모델이 기존 클래스에 대한 충분한 지식을 가지고 있는 것으로 판단할 수 있다. 학습 시스템은 판단 결과에 기초하여 상기 보간 매개 변수 의 값을 설정할 수 있으며, 이는 아래의 수학식 5에 따라 표현될 수 있다. [수학식 5]"}
{"patent_id": "10-2023-0107125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서 신뢰도는 로 나타내며, α는 0과 1 사이의 하이퍼파라미터에 해당 할 수 있다. α의 값이 조정됨에 따라 파라미터 업데이트 시 현재 모델의 파라미터와 이전 모델의 파라미터 중 어느 파라미터를 강조할 것인지를 결정할 수 있다. 는 기존 클래스 데이터에 대한 ground truth y 에 대한 예측 확률을 나타내며, δ는 임계값에 해당한다. 한편, 모델이 학습한 클래스 수가 증가함에 따라 예측 분포가 변할 수 있으므로, 임계치는 로 가변 설정될 수 있다. 여기서 는 하이퍼파라미터이 며, 적응적 파라미터는 이고, 는 작업 t에서의 클래스 수에 해당할 수 있다. 즉, 학습 시스템은 이전 모델의 신뢰도와 현재 모델의 신뢰도 차이가 임계치 이상인 경우 보간 매개 변수를 α 로 설정함으로써, 현재 모델의 파라미터 업데이트 시 이전 모델의 파라미터를 보간 매개 변수에 대응하는 비율 로 반영하여 현재 모델의 파라미터를 보간할 수 있다. 반면 이전 모델의 신뢰도와 현재 모델의 신뢰도 차이가 임계치 미만인 경우 보간 매개 변수를 1로 설정함으로써, 현재 모델의 파라미터 업데이트 시 이전 모델의 파라 미터를 활용하지 않을 수 있다. 도 7은 본 개시의 예시적 실시예에 따른 클래스 증분 학습 방법에 따라 구현되는 모델과 종래의 모델 간의 성능 차이를 설명하기 위한 데이터이다. 도 7은 공지된 학습 데이터셋(예컨대 CIFAR-100 등)으로 학습된 0번째 작업 모델과 50번째 작업 모델 간의 중간 특징(intermediate feature)에 대한 중심 커널 얼라인먼트(centered kernel alignment (CKA)) 유사도를 나타낸 다. 도 7의 (a)와 (b)는 종래 모델에 대한 CKA 유사도를 나타내고, (c)는 본 개시의 실시예에 따른 클래스 증분 학습 방법에 따라 구현되는 모델에 대한 CKA 유사도를 나타낸다. 양 축은 중간 레이어의 출력을 나타내며, 해당 출력에는 0번째 작업 모델(이전 모델)과 50번째 작업 모델(현재 모델)의 특징 증류가 적용되어 있다. 히트맵의 대각선은 동일한 레이어 간의 CKA 유사도를 나타낸다. 즉, 대각 선 성분의 값이 1에 가까울수록, 0번째 작업 모델과 50번째 작업 모델 간의 표현 유사도가 높음을 알 수 있다. 도 7에 도시된 바와 같이, 본 개시의 실시 예에 따른 클래스 증분 학습 방법에 따라 구현된 모델의 CKA 유사도 는, 종래 모델의 CKA 유사도에 비해 높음을 알 수 있다. 즉 본 개시의 실시 예에 따라 구현된 모델은 이전 클래 스에 대한 성능을 유지하고 높은 안정성을 가질 수 있다. 본 개시의 실시예에 따른 클래스 증분 학습 방법은, 기울기 기반 특징 증류와 신뢰도 기반 파라미터 업데이트를 통해, 이전 모델의 지식을 유지하면서 신규 클래스에 대한 효과적인 학습을 수행하도록 한다. 따라서, 본 학습 방법에 따르면, 종래의 클래스 증분 학습의 문제점인 안정성-가변성(stability-plasticity) 문제를 개선한 고성 능의 모델을 구축할 수 있다. 도 8은 본 개시의 실시예에 따른 학습 방법을 수행하는 디바이스의 개략적인 블록도이다. 도 8을 참조하면, 본 개시의 실시 예에 따른 디바이스는 도 1 내지 도 6에서 상술한 선택적 정규화를 통한 클래스 증분 학습 방법을 수행하는 적어도 하나의 컴퓨팅 장치 중 어느 하나에 대응할 수 있다. 이러한 디바이스는 프로세서 및 메모리를 포함할 수 있다. 다만, 디바이스의 구성 요소가 전술한 예에 한정되는 것은 아니다. 예를 들어, 디바이스는 전술한 구성 요소들보다 더 많은 구성 요소를 포함하거나 더 적은 구성 요소를 포함할 수 있다. 또한, 프로세서는 적어도 하나일 수 있으며, 메모리 또한 적어도 하나일 수 있다. 또한, 프로세서 및 메모리 중 둘 이상이 하나의 칩으로 결합된 형태일 수도 있다. 프로세서는 도 1 내지 도 6에서 상술한 클래스 증분 학습 방법을 전반적으로 제어할 수 있다. 예컨대 프로 세서는 CPU, AP(application processor), 집적 회로, 마이크로컴퓨터, ASIC(application specific integrated circuit), FPGA(field programmable gate array), 및/또는 NPU(neural processing unit) 등의 하 드웨어를 포함할 수 있다. 본 개시의 일 실시 예에 따르면, 메모리는 디바이스의 동작에 필요한 프로그램 및 데이터를 저장할 수 있다. 특히, 메모리는 도 1 내지 도 6에서 상술한 클래스 증분 학습 방법을 수행하는 컴퓨터 프로그램 을 저장할 수 있다. 상기 컴퓨터 프로그램은 상기 클래스 증분 학습 방법을 수행하는 인스트럭션들을 포함할 수 있다. 프로세서는 상기 저장된 컴퓨터 프로그램을 실행 및 제어함으로써 클래스 증분 학습 방법을 수행할 수 있다. 또한, 메모리는 프로세서를 통해 생성되거나 획득된 데이터 중 적어도 하나를 저장할 수 있 다. 메모리는 롬(ROM), 램(RAM), 플래시 메모리, SSD, HDD 등의 저장 매체 또는 저장 매체들의 조합으로 구성 될 수 있다. 상기한 실시예들의 설명은 본 개시의 더욱 철저한 이해를 위하여 도면을 참조로 예를 든 것들에 불과하므로, 본 개시의 기술적 사상을 한정하는 의미로 해석되어서는 안될 것이다. 또한, 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 있어 본 개시의 기본적 원리를 벗어나지 않 는 범위 내에서 다양한 변화와 변경이 가능함은 명백하다 할 것이다."}
{"patent_id": "10-2023-0107125", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 개시에서 인용되는 도면을 보다 충분히 이해하기 위하여 각 도면의 간단한 설명이 제공된다. 도 1은 본 개시의 예시적 실시예에 따른 선택적 정규화를 통한 클래스 증분 학습 방법의 개략적인 플로우차트이 다. 도 2 내지 도 3은 도 1의 클래스 증분 학습 방법에 대한 구체적인 실시예들을 설명하기 위한 플로우차트이다. 도 4 내지 도 5는 본 개시의 예시적 실시예에 따른 기울기 기반의 비대칭 특징 증류 방법을 설명하기 위한 예시 도이다. 도 6은 본 개시의 예시적 실시예에 따른 신뢰도 기반 파라미터 업데이트 방법을 설명하기 위한 예시도이다. 도 7은 본 개시의 예시적 실시예에 따른 클래스 증분 학습 방법에 따라 구현되는 모델과 종래의 모델 간의 성능 차이를 설명하기 위한 데이터이다. 도 8은 본 개시의 실시예에 따른 학습 방법을 수행하는 디바이스의 개략적인 블록도이다."}
