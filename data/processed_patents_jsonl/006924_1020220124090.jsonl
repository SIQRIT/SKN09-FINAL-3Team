{"patent_id": "10-2022-0124090", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0044719", "출원번호": "10-2022-0124090", "발명의 명칭": "RNN과 AM을 이용한 청크 기반 음성 감정인식 방법 및 시스템", "출원인": "배재대학교 산학협력단", "발명자": "신현삼"}}
{"patent_id": "10-2022-0124090", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "RNN과 AM을 이용한 청크 기반 음성 감정 인식 시스템에 있어서,청크 기반 음성 감정인식 모델을 훈련시켜 음성 감정인식을 하는 감정인식서버; 및훈련된 음성 감정인식 모델을 바탕으로 성능을 분석하는 성능분석서버를 포함하는 것을 특징으로 하는 RNN과 AM을 이용한 청크 기반 음성 감정인식 시스템."}
{"patent_id": "10-2022-0124090", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 감정인식서버는,음성 감정인식을 위한 음성 신호를 수신하는 음성신호수신부;음성신호수신부에서 수신한 음성신호를 분할하여 청크를 생성하는 청크생성부;청크생성부에서 생성한 각각의 청크에서 감정인식에 필요한 특징들을 추출하는 특징추출부;RNN(Recurrent Neural Network) 모델을 통해 특징추출부에서 추출한 특징들을 바탕으로 감정을 인식하는 감정인식부; 및AM(Attention Mechanisms)을 이용하여 음성 신호의 중요도를 계산한 후 감정인식부가 인식한 감정에 추가적으로음성 신호도의 중요도를 반영하는 중요도계산부를 포함하는 것을 특징으로 하는 RNN과 AM을 이용한 청크 기반음성 감정인식 시스템."}
{"patent_id": "10-2022-0124090", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,인식된 감정들 중 최종 감정을 인식하기 위해 하드 투표 및 소프트 투표를 진행하는 투표부; 및투표부에서 진행한 투표 결과를 바탕으로 감정을 판단하는 감정판단부를 더 포함하는 것을 특징으로 하는 RNN과AM을 이용한 청크 기반 음성 감정인식 시스템."}
{"patent_id": "10-2022-0124090", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "RNN과 AM을 이용한 청크 기반 음성 감정인식 방법에 있어서,음성신호수신부가 감정인식 할 음성 신호를 수신하는 단계;청크생성부가 음성신호수신부에서 수신한 음성신호를 분할하여 청크를 생성하는 단계;특징추출부가 청크생성부에서 생성한 각각의 청크에서 감정인식에 필요한 특징들을 추출하는 단계;감정인식부가 특징추출부에서 추출한 특징들을 바탕으로 RNN 모델을 통해 감정을 인식하는 단계; 및중요도계산부가 AM(Attention Mechanisms)을 이용하여 음성 신호의 중요도를 계산한 후 감정인식부가 인식한 감정에 음성 신호의 중요도를 추가적으로 반영하는 단계를 포함하는 것을 특징으로 하는 RNN과 AM을 이용한 청크기반 음성 감정인식 방법.공개특허 10-2024-0044719-3-청구항 5 제4항에 있어서,투표부가 인식된 감정들 중 최종 감정을 인식하기 위해 하드 및 소프트 투표를 진행하는 단계; 및성능분석 서버가 투표부에서 진행한 투표 결과를 바탕으로 정확도를 계산하고, 시뮬레이션 시간을 측정하는 단계를 더 포함하는 것을 특징으로 하는 RNN과 AM을 이용한 청크 기반 음성 감정인식 방법."}
{"patent_id": "10-2022-0124090", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 RNN과 AM을 이용한 청크 기반 음성 감정 인식 시스템에 관한 것이다. 본 발명은 청크 기반 음성 감정 인식 모델을 훈련시켜 음성 감정인식을 하는 감정인식서버 및 훈련된 음성 감정인식 모델을 바탕으로 성능을 분 석하는 성능분석서버를 포함한다. 감정인식서버는, 음성 감정인식을 위한 음성 신호를 수신하는 음성신호수신부 와, 음성신호수신부에서 수신한 음성신호를 분할하여 청크를 생성하는 청크생성부와, 청크생성부에서 생성한 각 각의 청크에서 감정인식에 필요한 특징들을 추출하는 특징추출부와, RNN(Recurrent Neural Network) 모델을 통해 특징추출부에서 추출한 특징들을 바탕으로 감정을 인식하는 감정인식부와, AM(Attention Mechanisms)을 이용하여 음성 신호의 중요도를 계산한 후 감정인식부가 인식한 감정에 추가적으로 음성 신호도의 중요도를 반영하는 중요 도계산부를 포함한다."}
{"patent_id": "10-2022-0124090", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 RNN(Recurrent neural network)과 AM(Attention Mechanisms)을 이용한 청크 기반 음성 감정인식 방 법 및 시스템에 관한 것으로, 특히 음성 신호를 청크로 분할하여 인식하고, 인식한 음성 신호에서 감정인식을 한 후 최종 투표를 거쳐 정확한 감정을 판단할 수 있는 RNN과 AM을 이용한 청크 기반 음성 감정인식 방법 및 시 스템에 관한 것이다."}
{"patent_id": "10-2022-0124090", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근에는 상담원 대신 인공지능 기반의 자동 고객 응대 시스템이 널리 사용되고 있어 자동 고객 서비스는 고객 의 목소리에 담긴 감정을 신속하게 인식하여 그에 맞는 서비스를 제공하는 것이 중요하다. 또한, 음성 인식 기반의 인공 지능 스피커는 고객 응대 서비스에 널리 사용되고, 로봇 및 기타 장치는 대부분의 산업 분야에서 간단한 음성 인식 및 디스플레이 인터페이스로 구성된다. 더 나아가, 고객의 정확한 의도를 파악 하기 위해 음성 신호에서 감정 인식(ER)률을 향상시키기 위한 다양한 연구가 진행되고 있다. ER 기술은 인간의 음성이나 제스처에서 정보를 수집하고 분석하여 인간의 감정 상태를 분석하지만, 제스처로 감 정을 표현하는 것은 문화에 따라 다를 수 있기 때문에 음성 신호로 결정되는 감정 상태는 제스처에서 결정되는 것보다 더 정확하다. 선행특허로는 공개특허 제10-2020-0109958(음성 신호를 이용한 감정 분류 방법, 이를 수행하기 위한 기록 매체 및 장치)가 있으나, 복수의 감정 별 음성 신호를 학습한 복수의 감정 분류 모델을 구축하는 단계 및 상기 복수 의 감정 분류 모델을 이용하여 상기 사용자의 음성 신호로부터 계산되는 복수의 감정 별 감정 확률을 획득하고, 이를 조합하여 사용자의 음성 신호에 나타나는 감정을 결정하는 단계를 포함하고 있을 뿐이다."}
{"patent_id": "10-2022-0124090", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는 상기와 같은 종래 기술의 문제점을 해결하기 위해 안출된 것으로, 음성 감정 인식 모델을 사용하여 청크를 기반으로 음성신호에서 감정을 인식할 수 있고, 더 나아가 AM을 이용하여 음성 신 호의 맥락에 따라 음성 신호의 중요도를 계산하여 반영할 수 있는 데 있다."}
{"patent_id": "10-2022-0124090", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 RNN과 AM을 이용한 청크 기반 음성 감정 인식 시스템은, 청크 기반 음성 감정인식 모델을 훈련시켜 음성 감정인식을 하는 감정인식서버 및 훈련된 음성 감정인식 모델을 바탕으로 성능을 분석하는 성능분석서버를 포함한다. 상기 감정인식서버는, 음성 감정인식을 위한 음성 신호를 수신하는 음성신호수신부와, 음성신호수신부에서 수신 한 음성신호를 분할하여 청크를 생성하는 청크생성부와, 청크생성부에서 생성한 각각의 청크에서 감정인식에 필 요한 특징들을 추출하는 특징추출부와, RNN(Recurrent Neural Network) 모델을 통해 특징추출부에서 추출한 특 징들을 바탕으로 감정을 인식하는 감정인식부와, AM(Attention Mechanisms)을 이용하여 음성 신호의 중요도를 계산한 후 감정인식부가 인식한 감정에 추가적으로 음성 신호도의 중요도를 반영하는 중요도계산부를 포함한다. 본 발명의 RNN과 AM을 이용한 청크 기반 음성 감정인식 방법은, 음성신호수신부가 감정인식 할 음성 신호를 수 신하는 단계와, 청크생성부가 음성신호수신부에서 수신한 음성신호를 분할하여 청크를 생성하는 단계와, 특징추 출부가 청크생성부에서 생성한 각각의 청크에서 감정인식에 필요한 특징들을 추출하는 단계와, 감정인식부가 특 징추출부에서 추출한 특징들을 바탕으로 RNN 모델을 통해 감정을 인식하는 단계와, 중요도계산부가 AM(Attention Mechanisms)을 이용하여 음성 신호의 중요도를 계산한 후 감정인식부가 인식한 감정에 음성 신호 의 중요도를 추가적으로 반영하는 단계를 포함한다."}
{"patent_id": "10-2022-0124090", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면 RNN과 AM을 사용하여 음성신호에서 감정을 인식할 수 있다. 본 발명의 일 실시예에 따르면 음성 감정인식 모델을 사용하여 감정인식(ER)의 정확도 성능을 분석할 수 있다."}
{"patent_id": "10-2022-0124090", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에 개시되어 있는 본 발명의 개념에 따른 실시 예들에 대해서 특정한 구조적 또는 기능적 설명은 단지 본 발명의 개념에 따른 실시 예들을 설명하기 위한 목적으로 예시된 것으로서, 본 발명의 개념에 따른 실시 예 들은 다양한 형태들로 실시될 수 있으며 본 명세서에 설명된 실시 예들에 한정되지 않는다. 본 발명의 개념에 따른 실시 예들은 다양한 변경들을 가할 수 있고 여러 가지 형태들을 가질 수 있으므로 실시 예들을 도면에 예시하고 본 명세서에서 상세하게 설명하고자 한다. 그러나 이는 본 발명의 개념에 따른 실시 예 들을 특정한 개시 형태들에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물, 또는 대체물을 포함한다. 본 명세서에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로서, 본 발명을 한정하려는 의 도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 본 명세서에 기재된 특징, 숫자, 단계, 동작, 구성 요소, 부분품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구 성 요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 이하, 본 명세서에 첨부된 도면들을 참조하여 본 발명의 실시 예들을 상세히 설명한다. 도 1은 본 발명의 실시예에 따른 RNN과 AM을 이용한 청크 기반 음성 감정인식 시스템을 설명하기 위한 구성도이 다.도 1을 참조하면, RNN과 AM을 이용한 청크 기반 음성 감정인식 시스템은 청크 기반 음성 감정인식 모델을 훈련시켜 음성 감정인식을 하는 감정인식서버와 훈련된 음성 감정인식 모델을 바탕으로 성능을 분석하는 성능분석서버를 포함한다. 상기 감정인식서버는 음성신호수신부, 청크생성부, 특징추출부, 감정인식부, 중요도 계산부, 투표부, 감정판단부로 구성된다. 음성신호수신부는 감정인식 할 음성신호를 수신한다. 이 때 음성신호수신부가 수신하는 음성신호는 인공지능 기반의 자동 고객 응대 시스템, 인공지능 스피커, 산업 분야의 로봇 및 기타 장치 중 적어도 하나일 수 있으나 반드시 이에 한정하는 것은 아니다. 청크생성부는 상기 음성신호수신부가 수신한 음성신호를 전부 정확하게 인식하기 위해 3초 길이의 청 크로 분할할 수 있다. 이 때 전체 음성신호 대비 청크의 최소 개수(n)는 [식 1]에 의해 계산될 수 있다. [식 1] 이 때, l은 전체 음성신호의 길이이며, c는 청크 하나의 크기를 의미한다. 청크생성부에서 생성되는 청크는 일정한 간격으로 겹쳐져야 하며, 청크가 겹쳐지는 오버랩 크기(h)는 [식 2]에 의해 계산될 수 있다. [식 2] 특징추출부는 상기 청크생성부에서 생성한 각각의 청크에서 감정인식에 필요한 특징들을 추출할 수 있다. 이 때 특징추출부에서는 ZCR(Zero-crossing rate), RMS(root mean square), Mel vector, CSFT(Chroma short-time Fourier Transform), MFCC(Mel-frequency cepstral coefficient) 및 스펙트럼 특징들 을 추출할 수 있다. ZCR은 각 청크의 진폭의 부호 변화율을 계산한 값을 의미하고 신호값이 0을 통과하는 비율을 부호변화율로 나타 낸 것으로, 가장 원시적인 음정 검출을 위한 알고리즘이다. ZCR( )은 [식 3]에 의해 계산될 수 있다. [식 3] 이 때, 은 [식 4]와 같은 함수이다. [식 4] 단, x는 청크 프레임의 진폭, i는 청크 프레임의 인덱스, N은 청크 프레임의 길이를 의미한다. RMS는 각 청크 프레임의 에너지, 즉 각 프레임의 소리 세기를 계산한 값을 의미하며, 가장 기본적인 감정의 척 도이다. RMS는 [식 5]에 의해 계산될 수 있다. [식 5] 단, 는 i번째 청크의 신호 진폭을 의미한다. Mel-scale 스펙트로그램(Mel-scale spectrogram)은 시간과 주파수에 따른 각 청크의 에너지(dB) 특성 벡터로, 감정인식(ER)에서 기본 특성으로 많이 사용된다. CSFT(Chroma short-time Fourier Transform)은 12개의 고유한 음정 등급의 변화를 나타내는 특징 벡터로, 시간에 따른 청크에서 각 스케일의 에너지를 추출하여 스케일로 특 징지을 수 있다. MFCC(Mel-frequency cepstral coefficient)는 오디오 신호에서 추출되는 소리의 고유한 특성 을 나타내는 특징이다. 스펙트럼 특징은 주파수 영역의 통계적 특징 값이며, 주파수 대역 스펙트럼은 다른 특징 과 함께 감정인식(ER)에 대한 통계 값으로 사용된다. 감정인식부는 상기 특징추출부에서 추출한 특징들을 바탕으로 RNN(Recurrent Neural Network) 기법 중 CSER모델을 사용하여 감정을 인식한다. 이 때 사용되는 CSER모델에는 LSTM(Long Short-Term Memory), Bi-LSTM(Bidirectional-LSTM), GRU(Gated Recurrent Unit), Bi-GRU(Bidirectional Gated Recurrent Unit) 기법을 사용한다. LSTM(Long Short-Term Memory)은 기존 RNN에서 장기간 학습이 지속되면 초기 학습을 잊는 단점을 보완한 기술이 다. 게이트라는 셀을 RNN의 입력, 망각, 출력 레이어에 연결하여 값을 조정한다. 입력 게이트는 새로운 정보의 저장 여부를 결정하고, 망각 게이트는 이전 상태 정보의 저장 여부를 결정하며, 출력 게이트는 업데이트된 셀의 출력 값을 제어한다. 도 2는 LSTM의 구조를 나타내며, 도 2를 참조하면, 와 는 각각 t 시간일 때의 입력과 숨겨진 상태를 나타낸 다. 또한, i는 입력 게이트, f는 망각 게이트, o는 출력 게이트를 나타낸다. 첫번째 단계로, LSTM은 제거할 정보를 결정하기 위해 시그모이드 함수를 사용한다. 다음 단계로, 새로운 정보를 셀 상태에 저장해야 하는지 여부를 결정하기 위해 다른 시그모이드 함수와 tanh 함수를 사용한다. 세번째 단계 에서 셀 상태가 업데이트되고, 출력 값은 셀 상태가 전달됨에 따라 최종 시그모이드 함수와 tanh 함수를 사용하 여 결정된다. LSTM은 좌우 방향으로 연속적으로 연결된 유닛들로 구성되어 있다. 각 단계에서 LSTM은 이전 시간 단계의 숨겨 진 상태와 셀 상태를 수신하고 현재 단계의 입력 값을 받고 게이트를 통해 계산을 수행한 후, 숨겨진 상태와 셀 상태를 업데이트하여 다음 단계로 전송한다. 망각 게이트 는 LSTM 메모리에서 제거해야 할 정보를 결정하며, 망각 게이트( )는 [식 6]에 의해 계산될 수 있다. [식 6] 이 때 W는 가중치 행렬, b는 편향 벡터를 의미하며, 입력 레이어, 메모리 블록 및 출력 레이어를 연결하는 데 사용된다. 망각 게이트는 이전 숨겨진 상태 와 현재 입력 값 에 시그모이드 함수를 적용한다. 출력 값이 0이 면 값이 완전히 삭제됨을 나타내고, 값이 1이면 값이 완전히 유지됨을 나타낸다. 입력 게이트 는 새로운 정보를 LSTM 메모리에 추가할지 여부를 결정한다. 이 게이트는 시그모이드와 탄젠트 레 이어로 구성된다. 시그모이드 레이어는 업데이트해야 할 값을 결정하고, tanh 레이어는 셀 상태에 추가할 새 후 보 값의 벡터를 생성한다. 이를 수식으로 정리하면 [식 7], [식 8]과 같다. [식 7] [식 8] 이 때 는 값의 업데이트 필요 여부를 결정하고, 는 셀 상태에 추가될 새 후보 값의 벡터를 나타낸다. 그 후, 이전 셀 상태 는 새로운 셀 상태 로 업데이트되며, [식 9]와 같이 표현될 수 있다. [식 9] 이 때 는 0과 1사이에 있는 망각 게이트의 결과 값을 나타낸다. 마지막으로 결과에 시그모이드 레이어의 출력 을 곱하면 출력 게이트 는 과 의 시그모이드 레이어의 결과를 사용하여 계산되고, [식 10], [식 11]과 같이 계산된다. [식 10] [식 11] 이 때 는 -1과 1 사이의 값이고, 는 입력 청크 음성 신호 데이터를 의미한다. 입력 시계열 음성 신호 데이터 는 X=[ , , ..., ]으로 표현될 수 있으며, 메모리 셀의 숨겨진 상태는 H=[ , , ..., ]으로 표시될수 있다. 단, N은 청크된 음성 데이터 값의 수를 의미한다. Bi-LSTM(Bidirectional-LSTM )은 수정된 LSTM으로, 두 개의 독립적인 히든 레이어를 가지고 있다. Bi-LSTM은 순방향 히든 시퀀스를 먼저 계산한 다음 역 히든 시퀀스를 계산하며, 두 레이어를 결합하여 출력을 얻는다. LSTM에 비해 Bi-LSTM은 네트워크에서 사용 가능한 정보의 양을 효과적으로 증가시켜 알고리즘에서 사용 가능한 내용을 향상시킬 수 있다. GRU(Gated Recurrent Unit)은 LSTM의 확장된 접근 방식으로, 매개변수가 더 적은 LSTM과 유사하다. 매개변수는 게이팅 메커니즘을 통해 학습되며, 내부 구조가 더 단순하여 숨겨진 상태에 대한 업데이트가 더 적은 계산을 필 요로 하기 때문에 훈련이 더 쉽다. 또한, LSTM의 소멸 기울기 문제를 해결할 수 있다. 도 3은 GRU의 구조를 보여주며, 도 3을 참조하면, LSTM에는 3개의 게이트(출력, 입력, 망각)가 있는 반면, GUR 에는 업데이트 및 재설정 게이트의 2개 게이트만 있다. 업데이트 게이트( )는 다음 상태에 어떤 정보를 유지할 지 결정하고, 재설정 게이트( )는 이전 상태 정보가 GUR의 새 입력 정보와 결합되는 방식을 결정한다. GRU의 공 식은 [식 12], [식 13]과 같다. [식 12] [식 13] 후보 숨겨진 상태( )와 현재 숨겨진 상태( )는 [식 14]와 [식 15]에 의해 계산될 수 있다. [식 14] [식 15] 이 때, , x, h는 LSTM과 동일한 것을 나타내며, 와 는 현재와 이전 상태의 결과값을 각각 나타낸다."}
{"patent_id": "10-2022-0124090", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "는 청크된 음성 신호의 음성 데이터 신호를 나타낸다. 입력 시계열 음성 신호 데이터는 X=[ , , ..., ]으 로 표현될 수 있으며, 메모리 셀의 숨겨진 상태는 H=[ , , ..., ]으로 표시될 수 있다. 단, N은 청크된 음성 데이터 값의 수를 의미한다. Bi-GRU(Bidirectional Gated Recurrent Unit)은 양방향 RNN과 GRU를 결합한 개선된 모델이다. BI-GRU의 구조는 순환 단위를 제외하고 BI-LSTM과 유사하다. 이 두 양방향 네트워크는 순방향 및 역방향 정보를 동시에 사용할 수 있다. GRU가 LSTM보다 간단하기 때문에 Bi-GRU는 Bi-LSTM보다 간단하다. 중요도계산부는 상기 감정인식부에서 CSER모델을 사용하여 감정을 인식할 때, self-AM(Attention Mechanisms)을 이용하여 음성 신호의 중요도를 계산한 후 감정인식부가 인식한 감정에 음성 신호의 중요도 를 추가적으로 반영한다. AM은 상기 LSTM의 마지막 숨겨진 계층의 상태 또는 LSTM의 결과의 암시적 상태를 사용 하여 현재 모먼트 입력의 숨겨진 상태를 맞출 수 있으나, SER 분야에서는 현재 입력되는 음성 신호에 가중치를 부여하는 self-AM이 더 적합하여 self-AM을 사용한다. 또한, self-AM은 외부 정보에 대한 의존도를 줄이고 데이 터 또는 기능의 내부 상관 관계를 더 잘 포착하여 AM의 개선된 버전이다. self-AM에서 사용하는 인간의 감정을 나타내는 오디오 시퀀스의 경우, 인접한 프레임은 유사한 음향 특성을 나 타내며, i번째 음성 데이터 시퀀스(X)의 쿼리(Q), 키(K), 값(V)는 [식 16], [식 17], [식 18]과 같이 표현될 수 있다. [식 16] [식 17] [식 18] 이 때, 는 X의 i번째 요소이며, 는 각각 쿼리, 값, 키의 i번째 요소를 대응하는 선형 투영을 의미 한다. Q, V, K의 치수는 1 X hp 이며, hp는 하이퍼파라미터를 의미한다. 마지막으로 AM은 일련의 시퀀스에 대한 행렬 곱셈을 사용하여 계산되며, [식 19]와 같이 계산될 수 있다. [식 19] 이 때, Q는 쿼리, K는 키, V는 값을 의미하며, 는 스케일링 요소이다. 투표부는 상기 감정인식부가 인식한 감정에 중요도계산부가 음성 신호의 중요도를 추가적으로 반영한 감정을 바탕으로 최종 감정을 인식하기 위한 투표를 진행할 수 있다. 투표부은 하드투표모듈 과 소프트투표모듈로 구성된다. 하드투표모듈은 하드 투표를 진행한다. 상기 하드 투표는 청크에서 인식한 감정 중, 가장 많은 표를 얻은 감정을 예측할 수 있다. 소프트투표모듈는 소프트 투표를 진행한다. 상기 소프트 투표는 청크에서 예측한 감정의 합산 확률이 가장 큰 감정을 예측할 수 있다. 감정판단부는 상기 투표부에서 하드투표모듈과 소프트투표모듈에 의해 예측된 감정을 바탕 으로 최종적인 감정을 판단한다. 성능분석서버는 상기 감정인식서버에서 최종적으로 예측한 감정이 시뮬레이션시간 대비 어느 정도의 정확도를 가지고 있는지 성능을 분석할 수 있으며, 정확도계산모듈과 시뮬레이션시간 측정모듈로 구 성된다. 정확도계산모듈은 감정인식서버의 감정인식부에서 사용한 RNN기법 중 사용된 LSTM, Bi-LSTM, GRU 및 Bi-GRU 네 가지의 CSER모델을 사용했을 때 각각 최종적으로 예측한 감정과 실제 음성 파일의 감정에 대 한 정확도를 계산할 수 있다. 시뮬레이션시간 측정모듈은 감정인식서버의 감정인식부에서 사용한 RNN기법 중 사용된 LSTM, Bi-LSTM, GRU 및 Bi-GRU 네 가지의 CSER모델을 사용했을 때 각각 최종적으로 감정을 예측할 때까지 걸리는 시뮬 레이션 시간을 측정할 수 있다. 도 4는 본 발명의 실시예에 따른 RNN과 AM을 이용한 청크 기반 음성 감정인식 방법을 설명하기 위한 흐름도이다. 도 4를 참조하면, 먼저 음성신호수신부가 감정인식 할 음성 신호를 수신한다(S201). 이 후, 청크생성부 가 음성신호수신부에서 수신한 음성신호를 분할하여 청크를 생성하고(S203), 특징추출부가 청크 생성부에서 생성한 각각의 청크에서 감정인식에 필요한 특징들을 추출한다(S205). 감정인식부가 특징 추출부에서 추출한 특징들을 바탕으로 RNN 기법을 사용하여 감정을 인식하고(S207), 중요도계산부가 AM을 이용하여 음성 신호의 중요도를 계산한 후, 감정인식부가 인식한 감정에 음성 신호의 중요도를 추가 적으로 반영한다(S209). 마지막으로 투표부가 인식된 감정들 중 최종 감정을 인식하기 위해 하드 및 소프 트 투표를 진행한다(S211). 도 5 내지 도 11은 본 발명의 실시예에 따른 성능분석에 관한 실험에 대한 결과값을 나타내는 그래프와 표이다. 도 5를 참조하면, 실험에 사용된 데이터는 IEMOCAP 데이터베이스이며, 10명의 배우가 혼성 쌍의 대본에 따라 즉 흥 또는 감정 연기를 하여 준비한 5개의 세션으로 구성되어 평가자에 의해 분석된다. 도 5와 같이, 데이터베이 스의 감정 데이터는 분노, 행복, 슬픔, 평온, 흥분, 두려움, 놀람, 혐오 등 11가지 감정 유형과 관련된 10,039 개의 음성 파일로 구성된다. 이 때, 분노, 행복, 중립, 슬픔과 관련된 음성파일의 감정데이터를 이용하여 CSER 모델의 성능을 분석한다. 도 6 내지 도 8은 최종 감정인식에 하드 투표를 사용한 실험에 대한 그래프와 표를 나타낸다. 도 6을 참조하면, CSER 모델에 하드 투표가 적용된 혼동 행렬을 나타낸다. CSER 모델에 LSTM, Bi-LSTM, GRU, Bi-GRU를 적용했을 때, 음성 신호에서 네 가지의 감정을 높은 확률로 인식한다. 또한, 분노와 같은 뚜렷한 감정 에 대한 예측 정확도가 높다. 그러나 네 가지 RNN 기법 모두 모든 감정에 대한 혼동 행렬에서 대각선 행렬의 확 률이 가장 높다. 도 7(a)와 도 7(b)를 참조하면, 최종 감정 인식에 하드 투표를 사용하여 평가한 CSER 모델에 적용된 네 가지 RNN 기법에 대한 정확도와 시뮬레이션 시간을 각각 보여준다. 도 8(a)를 참조하면, LSTM과 GRU 방법 모두 양방향 기술을 적용하면 시뮬레이션 시간이 대략 2.5에서 1.8배 증 가하며, 정확도는 대략 3.92%에서 0.49% 증가한다. 일반적으로 하드 투표를 사용하여 감정을 평가할 때, GRU의 구조가 LSTM보다 간단하기 때문에 시뮬레이션 시간에 대한 시간 효율성이 GRU에 가장 적합하다. 도 8(b)에서는 하드 투표를 이용한 음성 감정 인식 시, GRU 기법의 시뮬레이션 시간이 가장 짧은 것을 알 수 있 다. 도 8(b)는 GRU와 나머지 세 가지 RNN 기법을 비교하였을 때 GRU의 정확도 및 시간 효율성을 보여준다. 또한, 하드 투표를 사용한 GRU와 비교하여 정확도와 시간 효율성의 상대적인 성능을 보여준다. 도 8(b)는 GRU와 비교한 세 가지 RNN 기법의 정확도 차이 및 시간 효율성을 나타낸다. GRU에서 음성 감정을 인식하면 정확도가 LSTM에 비해 2.45% 증가하나, Bi-LSTM 및 Bi-GRU에 비해 음성 인식 정확도가 각각 1.45%, 0.49% 감소한다. 그 러나, 도 8(a)에서 알 수 있듯이, GRU를 적용하면 네 가지의 RNN 기법 중 51.02초로 가장 시뮬레이션 시간이 짧 다. 시간 효율은 LSTM, Bi-LSTM 및 Vi-GRU 기술에 비해 15.29%, 66.63%, 47.75%가 증가하는 것을 확인할 수 있 기 때문에 CSER 모델에 GRU를 적용하는 것이 정확도 대비 시간 효율이 가장 높다. 도 9 내지 도 11은 최종 감정인식에 소프트 투표를 사용한 실험에 대한 그래프와 표를 나타낸다. 도 9를 참조하면, CSER 모델에 LSTM, Bi-LSTM, GRU, Bi-GRU 기법을 적용하고, 소프트 투표를 통해 감정 인식 결과의 정확도와 시뮬레이션 시간을 분석한다. 도 9는 CSER에 4가지 RNN 기법을 적용하고 소프트 투표를 적용하 여 얻은 감정 인식 결과의 혼동 행렬을 보여준다. 도 9에서는 하드 투표 결과와 유사하게, 소프트 투표 결과가 높은 확률로 음성 신호에서 감정을 인식하는 것을 알 수 있다. 도 10(a)와 도 10(b)는 CSER모델에 적용된 4가지 RNN 기법에 대한 정확도와 시뮬레이션 시간을 각각 보여주고, 최종 감정인식은 소프트 투표를 통해 평가된다. 4가지 RNN 기법 모두 유사한 시뮬레이션 정확도를 보였으며, Bi-LSTM과 Bi-GRU는 모두 LSTM과 GRU에 비해 시뮬레이션 시간이 약 1.8배 증가했다. 도 11(a)를 참조하면, 양방향 기술을 LSTM과 GRU에 적용한 경우, LSTM의 감정인식 정확도는 약 3.4% 증가했지만 GRU의 정확도는 증가하지 않았다. 따라서 GRU는 소프트 투표와 하드 투표를 사용할 때 모두 CSER 모델에 대한 시뮬레이션 시간 측면에서 가장 효율적이다. 도 11(b)는 CSER 모델에서 소프트 투표를 이용하여 감정을 인식할 때 GRU와 다른 세 가지 RNN 기법의 정확도와 시간 효율성을 비교한 결과를 보여준다. GRU를 적용했을 때의 정확도는 LSTM 대비 2.21% 증가하고 Bi-LSTM 대비 1.22% 감소하며, Bi-GRU와는 동일한 성능을 보인다. CSER에 GRU를 적용했을 때 감정인식의 시간 효율은 LSTM, Bi-LSTM, Bi-GRU에 비해 각각 29.54%, 60.74%, 45.37% 증가하였으므로, GRU는 하드 투표와 소프트 투표 모두에 서 시뮬레이션 시간 측면에서 가장 높은 성능을 보인다. 발명은 도면에 도시된 실시 예를 참고로 설명되었으나 이는 예시적인 것에 불과하며, 본 기술 분야의 통상의 지 식을 가진 자라면 이로부터 다양한 변형 및 균등한 타 실시 예가 가능하다는 점을 이해할 것이다. 따라서, 본 발명의 진정한 기술적 보호 범위는 첨부된 등록청구범위의 기술적 사상에 의해 정해져야 할 것이다."}
{"patent_id": "10-2022-0124090", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 RNN과 AM을 이용한 청크 기반 음성 감정인식 시스템을 설명하기 위한 구성도이 다. 도 2는 본 발명의 실시예에 따른 LSTM의 구조를 설명하기 위한 예시도이다. 도 3은 본 발명의 실시예에 따른 GUR의 구조를 설명하기 위한 예시도이다. 도 4는 본 발명의 실시예에 따른 RNN과 AM을 이용한 청크 기반 음성 감정인식 방법을 설명하기 위한 흐름도이다. 도 5 내지 도 11는 본 발명의 실시예에 따른 성능분석에 관한 실험에 대한 결과값을 나타내는 그래프와 표이다."}
