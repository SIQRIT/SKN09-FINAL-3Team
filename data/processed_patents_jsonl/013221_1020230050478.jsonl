{"patent_id": "10-2023-0050478", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0154203", "출원번호": "10-2023-0050478", "발명의 명칭": "심층 신경망 훈련용 전하 저장형 시냅스 장치 및 이의 구동 방법", "출원인": "서울대학교산학협력단", "발명자": "김상범"}}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "제 1 단자와 제 2 단자를 가지며 가중치에 대응하는 전압을 저장하는 가중치 캐패시터;상기 캐패시터의 전압 또는 가중치를 변화시키는 4개의 제어 트랜지스터들;상기 캐패시터의 제 1 단자와 결합되는 제 1 게이트 단자를 포함하며, 상기 제 1 게이트 단자에 걸린 전압에 따라 제 1 드레인 전류를 출력하는 제 1 출력 트랜지스터; 및상기 캐패시터의 제 2 단자와 결합되는 제 2 게이트 단자를 포함하며, 상기 제 2 게이트 단자에 걸린 전압에 따라 제 2 드레인 전류를 출력하는 제 2 출력 트랜지스터를 포함하며,상기 가중치는 상기 제 1 드레인 전류와 상기 제 2 드레인 전류의 차이로 읽히는 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 4개의 제어 트랜지스터들 중 제 1 제어 트랜지스터와 제 3 트랜지스터는 각각 제 1 소스 단자를 가지며,상기 제 1 소스 단자에 연결된 제 1 전압을 상기 가중치 캐패시터에 전달하는 통과 트랜지스터(passtransistor)를 포함하고,상기 4개의 제어 트랜지스터들 중 제 2 제어 트랜지스터와 제 4 트랜지스터는 각각 제 2 소스 단자를 가지며,상기 제 2 소스 단자에 연결된 접지를 기반으로 전류원으로 기능하는 트랜지스터를 포함하며,상기 제 1 제어 트랜지스터와 상기 제 4 트랜지스터는 각각 제 1 드레인 단자를 포함하며, 상기 제 1 드레인 단자는 서로 연결되고 상기 가중치 캐패시터의 상기 제 1 단자와 연결되고,상기 제 2 제어 트랜지스터와 상기 제 3 트랜지스터는 각각 제 2 드레인 단자를 포함하며, 상기 제 2 드레인 단자는 서로 연결되고 상기 가중치 캐패시터의 상기 제 2 단자와 연결되는 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 제 1 제어 트랜지스터는 상기 제 1 전압과 상기 가중치 캐패시터의 상기 제 1 단자 사이에 접속되며, 제 1제어 신호에 응답하여 상기 제 1 전압과 상기 가중치 캐패시터의 상기 제 1 단자를 연결하고,상기 제 2 제어 트랜지스터는 상기 접지와 상기 가중치 캐패시터의 상기 제 2 단자 사이에 접속되며, 제 2 제어신호에 응답하여 상기 접지와 상기 제 2 단자를 연결하고,상기 제 3 제어 트랜지스터는 상기 제 1 전압과 상기 가중치 캐패시터의 상기 제 2 단자사이에 접속되며, 제 3제어 신호에 응답하여 상기 제 1 전압과 상기 가중치 캐패시터의 상기 제 2 단자를 연결하고,상기 제 4 제어 트랜지스터는 상기 접지와 상기 가중치 캐패시터의 상기 제 1 단자 사이에 접속되며, 제 4 제어신호에 응답하여 상기 접지와 상기 제 1 단자를 연결하고,상기 제 1 출력 트랜지스터는 제 1 입력 라인과 연결되는 제 3 드레인 단자 및 제 1 출력 라인과 연결되는 제 3소스 단자를 갖고,상기 제 2 출력 트랜지스터는 제 2 입력 라인과 연결되는 제 4 드레인 단자 및 제 2 출력 라인과 연결되는 제 4소스 단자를 가지며,상기 제 1 출력 트랜지스터의 상기 제 1 드레인 전류는 상기 가중치 캐패시터의 상기 제 1 단자의 전압, 상기제 1 입력 라인의 전압 및 상기 제 1 출력 라인의 전압에 의해 결정되며, 상기 제 1 출력 라인으로 공급되고,상기 제 2 출력 트랜지스터의 상기 제 2 드레인 전류는 상기 가중치 캐패시터의 상기 제 2 단자의 전압, 상기공개특허 10-2024-0154203-3-제 2 입력 라인의 전압 및 상기 제 2 출력 라인의 전압에 의해 결정되며, 상기 제 2 출력 라인으로 공급되는 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 2 항에 있어서,상기 제 2 제어 트랜지스터와 상기 제 4 제어 트랜지스터에 상기 제 1 전압보다 상대적으로 낮은 과구동전압(overdrive voltage)이 인가되는 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 2 항에 있어서,상기 제 1 제어 트랜지스터와 상기 제 2 제어 트랜지스터를 온(on)시켜 동작하는 강화(potentiation) 업데이트를 통해 상기 가중치가 양의 방향으로 증가하고,상기 제 3 제어 트랜지스터와 상기 제 4 제어 트랜지스터를 온(on)시켜 동작하는 약화(depression) 업데이트를통해 상기 가중치가 음의 방향으로 증가하는 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서,상기 가중치는 상기 제 2 제어 트랜지스터 또는 상기 제 4 제어 트랜지스터의 방전 전류와 상기 제 2 제어 트랜지스터 또는 상기 제 4 제어 트랜지스터에 인가되는 펄스 폭 및 상기 가중치 캐패시터의 정전 용량에 의해 결정되는 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 2 항에 있어서,상기 제 3 제어 트랜지스터가 온(on)된 읽기(read) 동작을 통해, 상기 제 1 출력 트랜지스터의 제 1 드레인 전류를 독출하고,상기 제 1 제어 트랜지스터가 온(on)된 읽기(read) 동작을 통해, 상기 제 2 출력 트랜지스터의 제 2 드레인 전류를 독출하고,상기 제 1 드레인 전류와 상기 제 2 드레인 전류의 차이를 상기 가중치로 결정하는 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 2 항에 있어서,상기 제 1 제어 트랜지스터가 온(on)된 참조 읽기(reference read) 동작을 통해, 상기 제 1 출력 트랜지스터의제 3 드레인 전류를 독출하고,상기 제 3 제어 트랜지스터가 온(on)된 참조 읽기(reference read) 동작을 통해, 상기 제 2 출력 트랜지스터의제 4 드레인 전류를 독출하고,상기 제 3 드레인 전류와 상기 제 4 드레인 전류의 차이를 상기 참조 컨덕턴스로 결정하며상기 가중치 캐패시터가 상기 참조 컨덕턴스를 갖는 경우 상기 가중치 캐패시터의 전압(Vcap)은 0인 시냅스장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 5 항, 제 7항 또는 제 8 항에 있어서,상기 강화(potentiation) 업데이트와 상기 약화(depression) 업데이트, 상기 읽기(read) 동작 및 상기 참조 읽기(reference read) 동작 중 적어도 하나를 반전(invert) 동작시키는 시냅스 장치.공개특허 10-2024-0154203-4-청구항 10 제 2 항에 있어서, 상기 제 2 제어 트랜지스터와 상기 제 4 제어 트랜지스터를 온(on)시켜 상기 가중치 캐패시터의 전압(Vcap)을 0으로 리셋(reset) 동작하는 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10 항에 있어서, 상기 리셋 동작은 반전 동작 전에 수행되는 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 1 항에 있어서,상기 4개의 제어 트랜지스터들과 상기 제 1 및 제 2 출력 트랜지스터는 비정질 InGaZnO FET(Field EffectTransistor), 다결정질 InGaZnO FET, 단결정질 InGaZnO FET 또는 C-축 성장결정 InGaZnO(C-axis alignedInGaZnO) FET인 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 1 항에 있어서,상기 4개의 제어 트랜지스터들에 인가되는 펄스 조건에 따라 상기 시냅스 장치의 비선형이 조절되는 시냅스 장치."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "개시된 청구항 제 1 항의 시냅스 장치의 구동 방법으로서,선형 학습의 경우, 참조 읽기(reference read) 동작을 수행하며,비선형 학습의 경우, 상기 참조 읽기(reference read) 동작, 리셋(reset) 동작 및 반전(invert) 동작을 수행하는 시냅스 장치의 구동 방법."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "M×N 제 1 시냅스 장치들을 포함하는 코어 장치 배열(core device array)와 개시된 청구항 제 1 항의 M×N 제 2시냅스 장치들을 포함하는 보조 장치 배열(auxiliary device array)로 구성된 신경망(neural network)의 학습방법으로서,상기 코어 장치 배열에서 순전파(forward propagation)와 역전파(back propagation) 방식을 적용하여 출력 값을 생성하는 제 1 단계;상기 보조 장치 배열에 상기 코어 장치 배열로부터의 출력 값에 대응하는 펄스를 인가하여 상기 보조 장치 배열을 제 1 업데이트하는 제 2 단계;상기 제 1 업데이트 후에 상기 보조 장치 배열의 특정 컬럼(column)를 선택하여 상기 특정 컬럼(column)으로부터 출력 전류를 생성하는 제 3 단계; 상기 특정 컬럼(column)의 각각의 개시된 청구항 제 1 항의 시냅스 장치를 통해 읽기 동작 및 참조 읽기 동작을 수행하여, 각각의 출력 전류를 상기 코어 장치 배열의 입력 값으로 변환하는 제 4 단계; 및상기 입력 값을 통해 상기 코어 장치 배열의 특정 컬럼(column)을 제 2 업데이트하는 제 5 단계를 포함하는 신경망(neural network)의 학습 방법."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15 항에 있어서공개특허 10-2024-0154203-5-상기 보조 장치 배열의 특정 컬럼(column)의 각각의 개시된 청구항 제 1 항의 시냅스 장치를 이용하여 확률변수에 따라 리셋 또는 반전 동작을 수행하는 단계를 더 포함하는 신경망(neural network)의 학습 방법."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16 항에 있어서상기 리셋 또는 상기 반전 동작은 선형 학습인 경우 수행되지 않고 비선형 학습인 경우에 수행되는 신경망(neural network)의 학습 방법."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 16 항에 있어서상기 리셋 또는 상기 반전 동작은 초기 학습 구간 동안 수행되지 않고 상기 초기 학습 구간 이후에 수행되는 신경망(neural network)의 학습 방법."}
{"patent_id": "10-2023-0050478", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 16 항에 있어서상기 제 5 단계 이후에 상기 코어 장치 배열 및 상기 보조 장치 배열의 나머지 컬럼(column)에 대해서 제 1 단계 내지 제 5 단계를 반복 수행하는 신경망(neural network)의 학습 방법."}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "심층 신경망 훈련용 전하 저장형 시냅스 장치 및 이의 구동 방법에 관해 개시되어 있다. 개시된 시냅스 장치는 제 1 단자와 제 2 단자를 가지며 가중치에 대응하는 전압을 저장하는 가중치 캐패시터; 상기 캐패시터의 전압 또 는 가중치를 변화시키는 4개의 제어 트랜지스터들; 상기 캐패시터의 제 1 단자와 결합되는 제 1 게이트 단자를 포함하며, 상기 제 1 게이트 단자에 걸린 전압에 따라 제 1 드레인 전류를 출력하는 제 1 출력 트랜지스터; 및 상기 캐패시터의 제 2 단자와 결합되는 제 2 게이트 단자를 포함하며, 상기 제 2 게이트 단자에 걸린 전압에 따 라 제 2 드레인 전류를 출력하는 제 2 출력 트랜지스터를 포함하며, 상기 가중치는 상기 제 1 드레인 전류와 상 기 제 2 드레인 전류의 차이로 결정될 수 있다."}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 시냅스 장치 및 이의 구동 방법에 관한 것으로서, 더욱 상세하게는 심층 신경망 훈련용 전하 저장형 시냅스 장치 및 이의 구동 방법에 관한 것이다."}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 인공 신경망에 기반한 인공지능을 활용한 산업의 발전에 따라 인공 신경망 연산에 최적화된 컴퓨팅 구조가 널리 제안되고 있다. 특히, 대규모로 병렬화 곱셈 및 누산(Multiply-and-accumulation, MAC) 연산 효율을 극대 화할 수 있는 하드웨어 구조를 통해서 저전력 및 고효율화의 심층 신경망 컴퓨팅이 기능하다고 알려져 있다. 이 러한 구조로 크로스바 배열(crossbar array) 구조의 아날로그 RPU(Resistive Processing Unit) 아키텍처가 제 안되어 알려져 있다. 상기 RPU 아키텍처의 단위 셀(Unit Cell)은 입력 펄스 신호에 따라 전기전도도나 저장된 전하량을 변화시켜 인공 신경망 시냅스 가중치 연결 강도를 조정할 수 잇다. 이것은 인공 신경망 시냅스가 곱셈 및 누산 연산과 가중치 저장을 동시에 수행할 수 있게 해준다. 또한, 인공 신경망 시냅스는 학습 동작과 추론 동작에서 각각 그 요구사항이 다르다. 학습을 위한 소자는 비 휘 발성 보다 연산 정확도 구현이 보다 중요하다. 반면 추론을 위한 소자는 학습된 가중치를 잘 보존하는 것이 보 다 중요하다. 이런 연산 기능 수행과의 관계에 따른 소자 요구사항의 변화는 현재 널리 활용되는 폰노이만 기반 컴퓨팅 체계의 메모리 소자에서도 유사하게 나타난다. 연산 장치와 가장 가까운 소자로 사용되는 SRAM과 더불어 프로그램을 보조 기억장치에서 가져와 주 기억장치로 사용되는 DRAM 등은 휘발성이지만, 빠른 속도와 넓은 대역 폭을 특징으로 하고 있다. 반면 보조 기억장치는 연산이 완료된 데이터를 잘 보존하는 것이 중요하고, 대용량의 데이터를 저장하는 것이 중요하다. 이와 같이 현재의 메모리 소자도 연산장치와 상호작용하기 위해 그 요구사항 을 달리하며 계층구조를 가진다. 한편, 차세대 IoT 네트워크에 연결된 기기의 수는 2017년 80억개에서 2025년에 700억개로 크게 증가할 전망이나, 모바일 기기와 IoT 기기에서 심층 신경망 연산 능력 부족하여 클라우드 또는 데이터서버 연산에 의존 하고 있다. 초연결 정보통신 사회에 데이터 연산과 통신에 필요한 전력소모 요구를 획기적으로 감소시킬 필요가 있다. IoT와 빅데이터 기술 기반의 초연결성으로 대변되는 제4차 산업혁명 시대에 소비전력을 극소화하고 기존 방식과 혁신적으로 차별화되는 신개념 컴퓨팅 기술이 요구된다. 방대한 양의 비정형 데이터를 입력 받아 인간의 두뇌와 같이 높은 에너지 효율을 지닌 뇌신경 모사 연산 방식은 인공지능, 빅데이터, 센서네트워크, 패턴/사물인식 등에 필요한 차세대 컴퓨팅 솔루션이다. 현재까지의 뇌신경 모사 컴퓨팅 소자 연구는 저항변화메모리, 상변화메모리, 강유전메모리 등을 이용해 다중전 도 레벨의 대칭적, 선형적 전도도 증가 및 감소 특성을 확보하는 것에 초점이 있었다. 하지만 실험적 메모리 소 자의 비이상적 특성으로 인하여 하드웨어로 구현되는 신경망은 소프트웨어 기반 심층 신경망의 인식 및 분류 정 확도에 비하여 크게 뒤떨어질 수 있다. 아날로그 메모리를 사용한 하드웨어 구현은 에너지 효율이 높은 장점이 있지만, 소프트웨어 수준의 훈련 정확도가 달성되어야 신경망 연산 효율의 장점이 극대화될 수 있다. 종래 기존 연구의 비휘발성 메모리 소자들은 시냅스 소자의 가중치를 갱신하는 훈련(training)과 이미지, 사물, 음성 등을 인식하고 분류하는 추론(inference) 중에 추론에만 적합하였다. 효율적 훈련을 위해 요구되는 전도레 벨 수가 210개(약 1000개) 이상 필요하고 시냅스 가중치의 증감이 대칭적, 선형적이어야 하기 때문에 기존 메모 리 소자로는 물리적 기술적 한계가 있었기 때문이다. 이러한 한계를 극복하기 위해 종래 연구는 훈련을 소프트웨어에서 수행하고 시냅스 가중치 값을 아날로그 메모 리 배열에 이동시켜 추론 기능만 하드웨어에서 수행하였다."}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 이루고자 하는 기술적 과제는 선형 및 대칭 가중치 업데이트뿐만 아니라 충분한 유지 시간과 병렬 온 -칩(on-chip) 훈련 동작을 제공하는 심층 신경망 훈련용 전하 저장형 시냅스 장치를 제공하는 것이다. 또한, 드리프팅 참조(drifting reference) 및 장기 유지 손실(long-term retention loss)과 같은 나머지 디바 이스 비이상성을 보상하기 위해 효율적이면서도 현실적인 훈련 알고리즘을 사용하여 장치-알고리즘의 공동 최적 화를 제공하는 심층 신경망 훈련용 전하 저장형 시냅스 장치를 제공하는 것이다. 또한, 기존의 아날로그 메모리 소자를 이용하지 않고 대칭적이고 선형적 훈련 특성을 구현하여 소프트웨어 레벨 의 온-칩(on-chip) 학습 능력과 소프트웨어 수준의 정확도를 갖는 심층 신경망 훈련용 전하 저장형 시냅스 장치 를 제공하는 것이다. 본 발명이 해결하고자 하는 과제는 이상에서 언급한 과제에 제한되지 않으며, 언급되지 않은 또 다른 과제들은 아래의 기재로부터 당업자에게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 제 1 단자와 제 2 단자를 가지며 가중치에 대응하는 전압을 저장하는 가중치 캐 패시터; 상기 캐패시터의 전압 또는 가중치를 변화시키는 4개의 제어 트랜지스터들; 상기 캐패시터의 제 1 단자 와 결합되는 제 1 게이트 단자를 포함하며, 상기 제 1 게이트 단자에 걸린 전압에 따라 제 1 드레인 전류를 출 력하는 제 1 출력 트랜지스터; 및 상기 캐패시터의 제 2 단자와 결합되는 제 2 게이트 단자를 포함하며, 상기 제 2 게이트 단자에 걸린 전압에 따라 제 2 드레인 전류를 출력하는 제 2 출력 트랜지스터를 포함하며, 상기 가 중치는 상기 제 1 드레인 전류와 상기 제 2 드레인 전류의 차이로 읽힐 수 있다. 상기 4개의 제어 트랜지스터들 중 제 1 제어 트랜지스터와 제 3 트랜지스터는 각각 제 1 소스 단자를 가지며, 상기 제 1 소스 단자에 연결된 제 1 전압을 상기 가중치 캐패시터에 전달하는 통과 트랜지스터(pass transistor)를 포함하고, 상기 4개의 제어 트랜지스터들 중 제 2 제어 트랜지스터와 제 4 트랜지스터는 각각 제 2 소스 단자를 가지며, 상기 제 2 소스 단 자에 연결된 접지를 기반으로 전류원으로 기능하는 트랜지스터를 포함하며, 상기 제 1 제어 트랜지스터와 상기 제 4 트랜지스터는 각각 제 1 드레인 단자를 포함하며, 상기 제 1 드레인 단자는 서로 연결되고 상기 가중치 캐 패시터의 상기 제 1 단자와 연결되고, 상기 제 2 제어 트랜지스터와 상기 제 3 트랜지스터는 각각 제 2 드레인 단자를 포함하며, 상기 제 2 드레인 단자는 서로 연결되고 상기 가중치 캐패시터의 상기 제 2 단자와 연결될 수 있다. 상기 제 1 제어 트랜지스터는 상기 제 1 전압과 상기 가중치 캐패시터의 상기 제 1 단자 사이에 접속되며, 제 1 제어 신호에 응답하여 상기 제 1 전압과 상기 가중치 캐패시터의 상기 제 1 단자를 연결하고, 상기 제 2 제어 트랜지스터는 상기 접지와 상기 가중치 캐패시터의 상기 제 2 단자 사이에 접속되며, 제 2 제어 신호에 응답하 여 상기 접지와 상기 제 2 단자를 연결하고, 상기 제 3 제어 트랜지스터는 상기 제 1 전압과 상기 가중치 캐패 시터의 상기 제 2 단자사이에 접속되며, 제 3 제어 신호에 응답하여 상기 제 1 전압과 상기 가중치 캐패시터의상기 제 2 단자를 연결하고, 상기 제 4 제어 트랜지스터는 상기 접지와 상기 가중치 캐패시터의 상기 제 1 단자 사이에 접속되며, 제 4 제어 신호에 응답하여 상기 접지와 상기 제 1 단자를 연결하고, 상기 제 1 출력 트랜지 스터는 제 1 입력 라인과 연결되는 제 3 드레인 단자 및 제 1 출력 라인과 연결되는 제 3 소스 단자를 갖고, 상 기 제 2 출력 트랜지스터는 제 2 입력 라인과 연결되는 제 4 드레인 단자 및 제 2 출력 라인과 연결되는 제 4 소스 단자를 가지며, 상기 제 1 출력 트랜지스터의 상기 제 1 드레인 전류는 상기 가중치 캐패시터의 상기 제 1 단자의 전압, 상기 제 1 입력 라인의 전압 및 상기 제 1 출력 라인의 전압에 의해 결정되며, 상기 제 1 출력 라 인으로 공급되고, 상기 제 2 출력 트랜지스터의 상기 제 2 드레인 전류는 상기 가중치 캐패시터의 상기 제 2 단 자의 전압, 상기 제 2 입력 라인의 전압 및 상기 제 2 출력 라인의 전압에 의해 결정되며, 상기 제 2 출력 라인 으로 공급될 수 있다. 상기 제 2 제어 트랜지스터와 상기 제 4 제어 트랜지스터에 상기 제 1 전압보다 상대적으 로 낮은 과구동전압(overdrive voltage)이 인가될 수 있다. 일 실시예에서, 상기 제 1 제어 트랜지스터와 상기 제 2 제어 트랜지스터를 온(on)시켜 동작하는 강화 (potentiation) 업데이트를 통해 상기 가중치가 양의 방향으로 증가하고, 상기 제 3 제어 트랜지스터와 상기 제 4 제어 트랜지스터를 온(on)시켜 동작하는 약화(depression) 업데이트를 통해 상기 가중치가 음의 방향으로 증 가할 수 있다. 상기 가중치는 상기 제 2 제어 트랜지스터 또는 상기 제 4 제어 트랜지스터의 방전 전류와 상기 제 2 제어 트랜지스터 또는 상기 제 4 제어 트랜지스터에 인가되는 펄스 폭 및 상기 가중치 캐패시터의 정전 용 량에 의해 결정될 수 있다. 상기 제 3 제어 트랜지스터가 온(on)된 읽기(read) 동작을 통해, 상기 제 1 출력 트 랜지스터의 제 1 드레인 전류를 독출하고, 상기 제 1 제어 트랜지스터가 온(on)된 읽기(read) 동작을 통해, 상 기 제 2 출력 트랜지스터의 제 2 드레인 전류를 독출하고, 상기 제 1 드레인 전류와 상기 제 2 드레인 전류의 차이를 상기 가중치로 결정할 수 있다. 상기 제 1 제어 트랜지스터가 온(on)된 참조 읽기(reference read) 동작 을 통해, 상기 제 1 출력 트랜지스터의 제 3 드레인 전류를 독출하고, 상기 제 3 제어 트랜지스터가 온(on)된 참조 읽기(reference read) 동작을 통해, 상기 제 2 출력 트랜지스터의 제 4 드레인 전류를 독출하고, 상기 제 3 드레인 전류와 상기 제 4 드레인 전류의 차이를 상기 참조 컨덕턴스로 결정하며, 상기 가중치 캐패시터가 상 기 참조 컨덕턴스를 갖는 경우 상기 가중치 캐패시터의 전압(Vcap)은 0일 수 있다. 상기 강화(potentiation) 업 데이트와 상기 약화(depression) 업데이트, 상기 읽기(read) 동작 및 상기 참조 읽기(reference read) 동작 중 적어도 하나를 반전(invert) 동작 시킬 수 있다. 상기 제 2 제어 트랜지스터와 상기 제 4 제어 트랜지스터를 온 (on)시켜 상기 가중치 캐패시터의 전압(Vcap)을 0으로 리셋(reset) 동작할 수 있다. 상기 리셋 동작은 반전 동작 전에 수행될 수 있다. 상기 4개의 제어 트랜지스터들과 상기 제 1 및 제 2 출력 트랜지스터는 비정질 InGaZnO FET(Field Effect Transistor), 다결정질 InGaZnO FET, 단결정질 InGaZnO FET 또는 C-축 성장결정 InGaZnO(C- axis aligned InGaZnO) FET일 수 있다. 상기 4개의 제어 트랜지스터들에 인가되는 펄스 조건에 따라 상기 시냅 스 장치의 비선형이 조절될 수 있다. 본 발명의 다른 실시예에 따르면, 개시된 청구항 제 1 항의 시냅스 장치의 구동 방법은 선형 학습의 경우, 참조 읽기(reference read) 동작을 수행하며, 비선형 학습의 경우, 상기 참조 읽기(reference read) 동작, 리셋 (reset) 동작 및 반전(invert) 동작을 수행할 수 있다. 본 발명의 또 다른 실시예에 따르면, M×N 제 1 시냅스 장치들을 포함하는 코어 장치 배열(core device array) 와 개시된 청구항 제 1 항의 M×N 제 2 시냅스 장치들을 포함하는 보조 장치 배열(auxiliary device array)로 구성된 신경망(neural network)의 학습 방법은 상기 코어 장치 배열에서 순전파(forward propagation)와 역전 파(back propagation) 방식을 적용하여 출력 값을 생성하는 제 1 단계; 상기 보조 장치 배열에 상기 코어 장치 배열로부터의 출력 값에 대응하는 펄스를 인가하여 상기 보조 장치 배열을 제 1 업데이트하는 제 2 단계; 상기 제 1 업데이트 후에 상기 보조 장치 배열의 특정 컬럼(column)를 선택하여 상기 특정 컬럼(column)으로부터 출 력 전류를 생성하는 제 3 단계; 상기 특정 컬럼(column)의 각각의 개시된 청구항 제 1 항의 시냅스 장치를 통 해 읽기 동작 및 참조 읽기 동작을 수행하여, 각각의 출력 전류를 상기 코어 장치 배열의 입력 값으로 변환하는 제 4 단계; 및 상기 입력 값을 통해 상기 코어 장치 배열의 특정 컬럼(column)을 제 2 업데이트하는 제 5 단계 를 포함할 수 있다. 상기 보조 장치 배열의 특정 컬럼(column)의 각각의 개시된 청구항 제 1 항의 시냅스 장치 를 이용하여 확률변수에 따라 리셋 또는 반전 동작을 수행하는 단계가 더 포함될 수 있다. 상기 리셋 또는 상기 반전 동작은 선형 학습인 경우 수행되지 않고 비선형 학습인 경우에 수행될 수 있다. 또는 상기 리셋 또는 상기 반전 동작은 초기 학습 구간 동안 수행되지 않고 상기 초기 학습 구간 이후에 수행될 수 있다. 상기 제 5 단계 이후에 상기 코어 장치 배열 및 상기 보조 장치 배열의 나머지 컬럼(column)에 대해서 제 1 단계 내지 제 5 단 계가 반복 수행될 수 있다."}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들에 따르면, 가중치 캐패시터; 상기 캐패시터의 전압 또는 가중치를 변화시키는 4개의 제어 트랜지스터들; 상기 캐패시터의 제 1 단자와 결합되는 제 1 게이트 단자를 포함하며, 상기 제 1 게이트 단자에 걸린 전압에 따라 제 1 드레인 전류를 출력하는 제 1 출력 트랜지스터; 및 상기 캐패시터의 제 2 단자와 결합되 는 제 2 게이트 단자를 포함하며, 상기 제 2 게이트 단자에 걸린 전압에 따라 제 2 드레인 전류를 출력하는 제 2 출력 트랜지스터를 포함하는 6T1C 구조를 가짐으로써, 선형 및 대칭 가중치 업데이트뿐만 아니라 충분한 유지 시간과 병렬 온-칩(on-chip) 훈련 동작을 제공할 수 있다. 또한, 드리프팅 참조(drifting reference) 및 장기 유지 손실(long-term retention loss)과 같은 나머지 디바 이스 비이상성을 보상하기 위해 효율적이면서도 현실적인 훈련 알고리즘을 사용하여 장치-알고리즘의 공동 최적 화를 제공할 수 있다. 또한, 기존의 아날로그 메모리 소자를 이용하지 않고 대칭적이고 선형적 훈련 특성을 구현하여 소프트웨어 레벨 의 온-칩(on-chip) 학습 능력과 소프트웨어 수준의 정확도를 높일 수 있다. 더하여, 본 발명의 시냅스 장치를 선형 학습 알고리즘 및 비선형 학습 알고리즘에 이용하는 경우, 누설 (leakage)이 존재하는 캐패시터 기반 장치에 쌓인 가중치(weight)를 비휘발성 메모리(non-volatile memory)로 효율적으로 전달할 수 있고, 6T1C 장치의 장점을 활용하여 auxiliary device array에 필수적인 reference device array 없이도 누설 수렴 컨덕턴스(leakage converging conductance)를 읽을 수 있어서 집적도 향상을 기대할 수 있다. reference로서 leakage converging conductance를 읽기 때문에 보유 시간에 대해 강인한 특성 (robust)을 가지며, 누설에 의한 정확도 열화를 개선시킬 수 있다. 또한, 선형 학습의 경우에 비대칭성(asymmetry)에 의한 정확도 열화가 거의 발생하지 않는다. 더하여, 비선형 학습의 경우, 비대칭성(asymmetry)으로 인한 정확도 열화를 개선시켜주도록 확률적으로 반전 (invert)을 진행할 수 있으며, 추가적으로 6T1C 구조를 갖는 시냅스 장치 내의 트랜지스터의 비대칭성도 보완할 수 있다. 또한, non-linear device의 자체 weight regularization 이용 가능하다."}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "그러나, 본 발명의 효과는 상기 효과들로 한정되는 것은 아니며, 본 발명의 기술적 사상 및 영역으로부터 벗어 나지 않는 범위에서 다양하게 확장될 수 있다."}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면들을 참조하여 본 발명의 실시예들을 상세히 설명하기로 한다. 이하에서 설명할 본 발명의 실시예들은 당해 기술 분야에서 통상의 지식을 가진 자에게 본 발명을 더욱 명확하 게 설명하기 위하여 제공되는 것이고, 본 발명의 범위가 하기 실시예에 의해 한정되는 것은 아니며, 하기 실시 예는 여러 가지 다른 형태로 변형될 수 있다. 본 명세서에서 사용된 용어는 특정 실시예를 설명하기 위하여 사용되며, 본 발명을 제한하기 위한 것이 아니다. 본 명세서에서 사용되는 단수 형태의 용어는 문맥상 다른 경우를 분명히 지적하는 것이 아니라면, 복수의 형태 를 포함할 수 있다. 또한, 본 명세서에서 사용되는 \"포함한다(comprise)\" 및/또는 \"포함하는(comprising)\"이라 는 용어는 언급한 형상, 단계, 숫자, 동작, 부재, 요소 및/또는 이들 그룹의 존재를 특정하는 것이며, 하나 이 상의 다른 형상, 단계, 숫자, 동작, 부재, 요소 및/또는 이들 그룹의 존재 또는 부가를 배제하는 것이 아니다. 또한, 본 명세서에서 사용된 \"연결\"이라는 용어는 어떤 부재들이 직접적으로 연결된 것을 의미할 뿐만 아니라, 부재들 사이에 다른 부재가 더 개재되어 간접적으로 연결된 것까지 포함하는 개념이다. 제 1 또는 제 2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만, 예컨대 본 발명의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제 1 구성요소는 제 2 구성요소로 명 명될 수 있고, 유사하게 제 2 구성요소는 제 1 구성요소로도 명명될 수 있다. 아울러, 본원 명세서에서 어떤 부재가 다른 부재 \"상에\" 위치하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재가 존재하는 경우도 포함한다. 본 명세서에서 사용된 용 어 \"및/또는\"은 해당 열거된 항목 중 어느 하나 및 하나 이상의 모든 조합을 포함한다. 또한, 본원 명세서에서 사용되는 \"약\", \"실질적으로\" 등의 정도의 용어는 고유한 제조 및 물질 허용 오차를 감안하여, 그 수치나 정도의 범주 또는 이에 근접한 의미로 사용되고, 본원의 이해를 돕기 위해 제공된 정확하거나 절대적인 수치가 언급 된 개시 내용을 침해자가 부당하게 이용하는 것을 방지하기 위해 사용된다. 이하 첨부된 도면들을 참조하여 본 발명의 실시예들에 대해 상세히 설명한다. 첨부된 도면에 도시된 영역이나 파트들의 사이즈나 두께는 명세서의 명확성 및 설명의 편의성을 위해 다소 과장되어 있을 수 있다. 상세한 설명 전체에 걸쳐 동일한 참조번호는 동일한 구성요소를 나타낸다. 이하, 첨부한 도면을 참조하여 본 발명의 바람직한 실시 예를 설명함으로써, 본 발명을 상세히 설명한다. 도 1a는 본 발명의 실시 예에 따른 뇌신경 모사 연산 시스템의 개념도를 나타낸다. 뇌신경 모사 연산을 위해서 필요한 뉴런과 시냅스의 구성도로서 프리시냅틱 뉴런들에서 입력 데이터(전압)를 크로스바 형태로 이루어진 시 냅스 셀 어레이로 보내고, 포스트시냅틱 뉴런들에서 상기 시냅스 셀 어레이에서 나온 전류 값 (I=GV, G는 각 시 냅스의 전도도값을 배열의 형태로 저장)을 읽어들인다. 각 시냅스의 전도도 값의 업데이트 방식은 신경망 연산 방식에 따라 순전파/역전파에 의해 업데이트 값을 결정짓거나, 로컬하게 spike-timing dependent plasticity 학 습 규칙 등을 사용할 수 있다. 도 1a를 참조하면, 뇌신경 모사 연산 시스템은 프리시냅틱 뉴런들(Presynaptic Neurons, Npre1 내지 Nprem), 포스트시냅틱 뉴런들(Postsynaptic Neurons, Npost1 내지 Npostn) 및 복수의 시냅스 장치들을 포함한다. 도 1a에서는 편의상 프리시냅틱 뉴런들(Npre1 내지 Nprem)에 접속되는 라인들을 입력 라인(IL1 내지 ILm)으로 표현 하고 포스트시냅틱 뉴런들(Npost1 내지 Npostn)에 접속되는 라인들을 출력 라인(OL1 내지 OLn)으로 표현하였으나, 이 는 프리시냅틱 뉴런들(Npre1 내지 Nprem)에 접속되는 라인들과 포스트시냅틱 뉴런들(Npost1 내지 Npostn)에 접 속되는 라인들을 구분하여 설명하기 위함일 뿐이다. 예를 들어, 입력 값이 프리시냅틱 뉴런들(Npre1 내지 Nprem)로부터 시냅스 장치들을 통해 포스트 포스트시냅 틱 뉴런들(Npost1 내지 Npostn)로 전달되는 순전파(forward propagation) 동작 동안에는 입력 라인들(IL1 내지 ILm) 이 입력 라인들로 동작하고 출력 라인들(OL1 내지 OLn)이 출력 라인들로 동작할 수 있다. 반대로, 입력 값이 포스트시냅틱 뉴런들(Npost1 내지 Npostn)로부터 시냅스 장치들을 통해 프리시냅틱 뉴런들 (Npre1 내지 Nprem)로 전달되는 역전파(backward propagation) 동작 동안에는 입력 라인들(IL1 내지 ILm)이 출력 라 인들로 동작하고 출력 라인들(OL1 내지 OLn)이 입력 라인들로 동작할 수 있다. 이하에서는 특별히 언급하지 않는 이상 순전파 동작을 가정하여 본 발명의 개념을 설명하는데, 본 발명의 개념 은 이에 한정되지 않는다. 시냅스 장치는 프리시냅틱 뉴런들(Npre1 내지 Nprem) 중 어느 하나와 포스트시냅틱 뉴런들(Npost1 내지 Npostn) 중 어느 하나 사이에 구비된다. 도 1a에서는 생략되었으나 시냅스 장치는 외부 회로(peripheral circuit, 미도시)로부터 제어 신호들을 공 급받는다. 외부 회로(미도시)는 훈련(training)하는 동안 제어 신호들을 시냅스 장치로 반복적으로 출력하여 시냅스 장치의 가중치를 갱신한다. 다시 말해, 외부 회로(미도시)는 소정의 파형을 갖는 제어 신호들을 시냅스 장 치로 출력함으로써 시냅스 장치를 강화(potentiation) 또는 약화(depression)시킬 수 있다. 외부 회로(미도시)는, 뇌신경 모사 연산 시스템을 훈련할 때, 후술할 시냅스 장치에 포함된 캐패시터 (도 1b의 C1)의 양단 전압(Vcap)를 목표 전압으로 설정하는데, 이때 강화 동작 또는 약화 동작을 충전되어 있는 전압과 목표 전압의 차이에 대응하는 횟수만큼 반복적으로 수행될 수 있다. 도 1b는 도 1a에 도시된 시냅스 장치의 회로도를 나타낸다. 비제한적으로, 시냅스 장치는 뇌신경 모 사 연산 시스템을 구성하는, 프리시냅틱 뉴런들 중 어느 하나와 포스트시냅틱 뉴런들 중 어느 하나 사이에 연결되어 시냅스 가중치를 저장(store)/업데이트(update)/읽기(read) 할 수 있다. 도 1b를 참조하면, 단일 시냅스 장치는 6개의 트랜지스터들(N1 내지 N6)과 1개의 캐패시터로 구성(이하, '6T1C' 구조라 칭함)될 수 있다. 가중치 캐패시터(C1)는 셀에서 메모리 역할을 하며, 가중치를 전하 형태로 저 장할 수 있다. 6개의 트랜지스터들(N1 내지 N6) 중 2개의 출력 트랜지스터들(N5, N6)는 캐패시터(C1)에 저장된가중치를 후술할 읽기 동작(read operating) 및 참조 읽기 동작(reference read operating)을 수행하기 위한 read FET로서 동작하고, 다른 4개의 제어 트랜지스터들(N1 내지 N4)은 후술할 강화 업데이트(potentiation update)및 약화 업데이트(depression update)을 통해 캐패시터 전압(Vcap)을 변화시키는 역할을 수행한다. 가중 치 캐패시터(C1)의 양단(T1, T2)의 전압(Vcap) 변화시키는 과정에서 제 1 제어 트랜지스터(N1)와 제 3 트랜지스 터(N3)는 각각 드레인(D) 단자에 제 1 전압(Vdd/2)를 가중치 캐패시터(C1)의 제 1 단자(T1)와 제 2 단자(T2)로 전달하는 Pass Gate FET로 동작하고, 제 2 제어 트랜지스터(N2)와 제 4 제어 트랜지스터(N4)는 정전류를 방전 (discharge)하는 전류원 FET 역할을 수행할 수 있다. 구체적으로, 가중치 캐패시터(C1)는 제 1 단자(T1)와 제 2 단자(T2)을 포함하며, 가중치에 대응하는 전압을 저 장할 수 있다. 상기 가중치는 제 1 단자(T1)와 제 2 단자(T2) 사이의 전압차에 해당할 수 있다. 상기 전압차는 제 1 단자(T1)에 걸린 전압(VCP)과 제 2 단자(T2)에 걸린 전압(VCN)의 차이를 지칭한다. 4개의 제어 트랜지스터들(N1 내지 N4)는 가중치 캐패시터(C1)의 전압(Vcap) 또는 가중치를 변화시키며, 제 1 출 력 트랜지스터 (N5) 는 N1, N2, N4가 off 되고 N3만 on되어있는 상태에서 가중치 캐패시터의 가중치에 대응하는 제 1 읽기 전류를 출력하고, 제 2 출력 트랜지스터(N6)의 경우에도 N1만 on, N2 내지 N4가 off된 경우에 가중치 캐패시터의 가중치에 대응하는 저 2 읽기 전류를 출력할 수 있다. 또한, 제 1 출력 트랜지스터(N1)는 캐패시터 (C1)의 제 1 단자(T1)와 결합되는 제 1 게이트 단자(G1)를 포함하고, 제 2 출력 트랜지스터(N2)는 캐패시터(C 1)의 제 2 단자(T2)와 결합되는 제 2 게이트 단자(G2)를 포함한다. 4개의 제어 트랜지스터들(N1 내지 N4)는 각 각 게이트 단자를 통해 제어 신호(S1, S2, S3, S4)가 인가될 수 있다. 제 1 제어 트랜지스터(N1)와 제 3 트랜지스터(N3)는 제 1 소스 단자(S1)를 가지며, 소스 단자(S1)에 연결된 제 1 전압(Vdd/2)을 캐패시터(C1)에 전달하는 통과 트랜지스터(Pass Transistor)를 포함하고, 제 2 제어 트랜지스 터(N2)와 제 4 트랜지스터(N4)는 제 2 소스 단자(S2)를 가지며, 제 2 소스 단자(S2)에 연결된 접지(GND)를 기반 으로 전류원으로 기능하는 트랜지스터를 포함할 수 있다. 제 1 제어 트랜지스터(N1)와 제 4 트랜지스터(N4)는 제 1 드레인 단자(D1)를 포함하며, 제 1 드레인 단자(D1)는 캐패시터(C1)의 제 1 단자(T1)와 연결되고, 제 2 제 어 트랜지스터(N2)와 제 3 트랜지스터(N3)는 제 2 드레인 단자(D2)를 포함하며, 제 2 드레인 단자(D2)는 캐패시 터(C1)의 제 2 단자(T2)와 연결될 수 있다. 또한, 제 1 제어 트랜지스터(N1)는 제 1 전압(Vdd/2)과 캐패시터(C1)의 제 1 단자(T1) 사이에 접속되며, 제 1 제어 신호(S1)에 응답하여 제 1 전압과 상기 캐패시터의 제 1 단자(T1)를 연결할 수 있다. 제 2 제어 트랜지스 터(N2)는 접지(GND)와 캐패시터(C1)의 제 2 단자(T2) 사이에 접속되며, 제 2 제어 신호(S2)에 응답하여 접지와 제 2 단자(T2)를 연결할 수 있다. 제 3 제어 트랜지스터는 제 1 전압(Vdd/2)과 캐패시터(C1)의 제 2 단자(T2) 사이에 접속되며, 제 3 제어 신호(S3)에 응답하여 제 1 전압(Vdd/2)과 캐패시터(C1)의 제 2 단자(T2)를 연결할 수 있다. 제 4 제어 트랜지스터(N4)는 접지(GND)와 캐패시터(C1)의 제 1 단자(T1) 사이에 접속되며, 제 4 제어 신호(S4)에 응답하여 접지(GND)와 제 1 단자(T1)를 연결할 수 있다. 제 1 출력 트랜지스터(N5)는 어느 하나의 프리시냅틱 뉴런이 접속되는 제 1 입력 라인(WLU)과 연결되는 제 3 드 레인 단자(D3) 및 상기 어느 하나의 포스트시냅틱 뉴런이 접속되는 제 1 출력 라인(BLU)과 연결되는 제 3 소스 단자(S3)를 갖고, 제 2 출력 트랜지스터(N6)는 어느 하나의 프리시냅틱 뉴런이 접속되는 제 2 입력 라인(WLD)과 연결되는 제 4 드레인 단자(D4) 및 상기 어느 하나의 포스트시냅틱 뉴런이 접속되는 제 2 출력 라인(BLD)과 연 결되는 제 4 소스 단자(D4)를 갖는다. 제 1 출력 트랜지스터(N5)의 상기 제 1 읽기 전류는 캐패시터(C1)의 제 1 단자(T1)의 전압(VCP), 제 1 입력 라 인(WLU)의 전압 및 제 1 출력 라인(BLU)의 전압에 의해 결정되며, 제 1 출력 라인(BLU)으로 공급될 수 있다. 제 2 출력 트랜지스터(N6)의 상기 제 2 읽기 전류는 캐패시터의 제 2 단자(T2)의 전압(VCN), 제 2 입력 라인(WLD) 의 전압 및 제 2 출력 라인(BLD)의 전압에 의해 결정되며, 제 2 출력 라인(BLD)으로 공급될 수 있다. 학습 정확도 손실이 없는 보유(retention) 요구사항을 만족하는 전하저장형 소자를 합리적인 소자 크기로 구현 하기 위해서는 충분히 작은 트랜지스터 누설전류를 바탕으로 캐패시터의 면적 축소가 필요하다. 이를 위해서, 본 발명에서는 충분한 이동성(mobility)와 매우 낮은 누설전류를 특성을 갖는 In-Ga-Zn-Oxide Thin Film Transistor(IGZO TFT)를 사용한다. 밴드갭이 실리콘 대비 큰 IGZO TFT는 축적 모드 (accumulation mode)로 작 동하며, 홀 터널링 게이트 누설 전류(hole tunneling gate leakage current)가 거의 없고 홀 유효질량이 커intrinsic NMOS로 활용이 적합하다. 비제한적으로, IGZO TFT는 도펀트를 변경하여 PMOS로 활용할 수도 있다. 또 한, 가중치 캐패시터(C1)는 Metal-Insulator-Metal (MIM) 캐패시터를 포함하며, Atomic Layer Deposition (ALD) 공정을 통해 High-k dielectric의 insulator를 증착하여 제조될 수 있다. 전극 역할을 하는 금속 물질과 절연체 역할을 하는 High-k dielectric의 재료적 특성 및 제조된 캐패시터의 구조, 캐패시터 양단에 가해진 전 압 조건 등에 따라 캐패시터가 저장가능한 전하량과 누설전류가 결정될 수 있다. 이때 캐패시터에 저장한 전하 를 잘 보존하는 보유(retention) 특성을 개선하기 위해서는 절연체의 유전율(dielectric constant)을 증가시키 면서도 다양한 누설전류 원인 메커니즘을 제어할 수 있는 높은 에너지 갭(energy gap)을 갖는 물질을 사용할 수 있다. 또한, 시냅스 장치에 대한 훈련이 종료된 후 시냅스 장치에 설정된 가중치가 유지되어야 학습 결과에 따른 추론(inference)이 가능할 수 있다. 상기 가중치는 캐패시터(C1)의 양단 전압(Vcap = VCP- VCN)으로 저장되 므로 제1 내지 제4 제어 트랜지스터들(N1 내지 N4)은 오프전류(off-current)가 낮은 것, 즉, 오프 상태에서의 누설전류가 낮은 것이 좋다. 바람직하게, 제1 내지 제4 제어 트랜지스터들(N1 내지 N4) 및 제 1 내지 제 2 출력 트랜지스터들(N5, N6)은 비정질 InGaZnO FET, 다결정질 InGaZnO FET 또는 단결정질 InGaZnO FET으로 구현되거 나 In, Ga, Zn, Sn, Al, Hf, Zr, Si 및 O 중에서 적어도 하나의 원소를 포함하는 금속 산화물 트랜지스터로 구 현될 수 있다. 특히, C-축 성장결정 InGaZnO FET은 오프전류(off-current)가 약 10-24 [A/㎛]로 발표되었으며, 이는 금속 산화물 트랜지스터의 다수 캐리어 accumulation mode 동작 소자, 높은 band gap, valence band 부근 높은 sub-gap state, 높은 hole effective mass에 기인한 누설전류 성분들이 원천봉쇄되기 때문이다. 이하 시냅스 장치의 update/read/reference read/invert/reset 동작에 대한 설명은 후술할 도 3a 내지 도 3g를 통해 상세히 설명하기로 한다. 인공 신경망의 시냅스 가중치 연결 강도를 강하게 하는 방향으로 시냅스 장 치에 양전하를 저장하여 캐패시터 전압을 positive update 하는 것을 potentiation update라 한다. 인공 신경 망의 시냅스 가중치 연결 강도를 약하게 하는 방향으로 시냅스 소자에 음전하를 저장하여 캐패시터 전압을 negative update하는 것을 depression update라 한다. 도 2a는 본 발명의 실시 예에 따른 IGZO(In-Ga-Zn-O) 박막 트랜지스터(Thin Film Transistor: TFT) 구조 및 제 조 방법을 설명하기 위한 도면이다. 도 2a를 참조하면, 우수한 낮은 누설전류 특성을 갖는 IGZO TFT(N1 내지 N6)를 활용하여 가중치 업데이트 동작 을 수행하는 전하 저장형 시냅스 장치를 제조할 수 있다. 비제한적으로, IGZO TFT는 Top-Gate Staggered 구조를 갖는다. 그리고 Amorphous IGZO 증착을 위해 In:Ga:Zn = 1:1:1로 구성된 타겟을 장착한 스퍼터링(sputtering) 장치를 활용해 1Pa Ar 100sccm 분위기에서 2.44W/㎠ RF Bias로 스퍼터링하였다. 구체적으로, IGZO TFT 소자 제조 방법은 습식 산화를 이용한 실리콘 기판 산화 공정(대략 5000Å 두께의 산화층), 금속 증착(대략 200Å 두께의 텅스텐(W) 금속층) 공정, 소스 및 드레인 패터닝 및 건식 에칭 공정, 스 퍼터링을 이용한 채널 증착 공정(예: 대략 10㎚ 두께의 Amorphous IGZO), ALD를 이용한 게이트 절연층 증착 공 정(예; 대략 10㎚ 두께의 산화하프늄) 및 게이트 단자 형성 공정(예: 대략 300Å 두께의 텅스텐(W) 금속층)을 포함할 있다. 이때, IGZO의 수소 농도를 효과적으로 제어하기 위해 IGZO TFT는 후술할 캐패시터 소자를 먼저 제조한 후에 제 조할 수 있다. 이는 TMA(trimethylaluminum) 기반 ALD(Atomic Layer Deposition) 공정 과정에서 발생하는 수소 에 의해 IGZO 채널 특성이 영향을 받아 IGZO TFT의 특성이 열화 되는 것을 방지하기 위함이다. IGZO 채널에 과 다한 수소가 주입되는 경우 전자 운반체(electron carrier)가 증가하여 전도성 특성을 나타내게 되므로, 이를 효과적으로 조절하는 공정 조건 확보가 중요하다. 도 2b는 본 발명의 실시 예에 따른 캐패시터 소자 구조 및 제조 방법을 설명하기 위한 도면이다. 도 2b를 참조하면, 전하 저장형 시냅스 소자의 가중치는 IGZO TFT를 통해 캐패시터(C1)에 저장될 수 있다. 이번 연구에서는 IGZO TFT를 활용한 새로운 구조의 시냅스의 동작 검증에 문제가 없는 수준의 High Capacitance 확보 를 위해 High- dielectric을 활용한 Metal-Insulator-Metal(MIM) 캐패시터를 활용한다. High- dielectric은 TMA기반 소스(source)와 O3 반응물질(reactant)를 활용해 대략 250에서 증착하였다. 구체적인 캐패시터 소자 제조 방법은 습식 산화를 이용한 실리콘 기판 산화 공정(대략 5000Å 두께의 산화층), 하부 전극층 증착(대략 200Å 두께의 텅스텐(W) 금속층) 공정, 하부 전극 패터닝 및 건식 에칭 공정, ALD를 이용한 게이트 절연층 증착 공정(예; 대략 10㎚ 두께의 산화하프늄), 상부 전극층 증착(대략 300Å 두께의 텅스텐 (W) 금속층) 공정, 상부 전극 패터닝 및 건식 에칭 공정을 포함할 수 있다. 이때, 캐패시터 절연체로 사용할 고 유전율 산화물은 ALD를 통해 필요한 두께로 증착되며, 적절한 고유전율 재료를 사용하는 캐패시터는 시냅스 캐 패시터의 용량 및 누설 전류 요구 사항을 고려하여 제조될 수 있다. 도 3a는 본 발명의 시냅스 소자의 가중치를 감소시키기 위한 강화(potentiation) 업데이트 동작을 설명하 기 위한 동작 설명이다. 도 3b는 본 발명의 시냅스 소자의 가중치를 감소시키기 위한 약화(depression) 업 데이트 동작을 설명하기 위한 동작 설명이다. 도 3a를 참조하면, 강화(potentiation) 업데이트 동안, 제 1 제어 트랜지스터(N1)의 게이트 단자(G)에 펄스를 인가하여 가중치 캐패시터(C1)의 제 1 단자(T1)에 전압(VCP)(upper terminal voltage)를 Vdd/2로 되게 하고 다음 제 2 제어 트랜지스터(N2)의 게이트 단자(G)에 펄스를 인가하여 가중치 캐패시터(C1)의 양단 전위차(Vcap)(VCP- VCN)를 양의 방향으로 높일 수 있다. 6T1C 구조는 PMOS 역할이 가능한 트랜지스터 없이 NMOS 역할만 가능한 트랜지스터로 시냅스 소자 구성이 필요하 다. 선형적이고 점진적인 업데이트 특성 확보를 위해 NMOS 트랜지스터의 소스(source)가 고정 전압일 필요가 있 고, 6T1C 구조에서 트랜지스터(N1)는 캐패시터 하단 노드 전압(VCN)을 접지 보다 높거나 같아지도록 하는 역할을 수행하여, 제 2 트랜지스터(N2)가 가중치 업데이트를 Gate Pulse 전압에 따라 일정하게 수행할 수 있도록 돕는 다. 캐패시터(C1) 하단 노드(T2)와 접지 노드(GND) 사이에 연결된 제 2 제어 트랜지스터(N2)를 활용하여 펄스 전류를 통해 업데이트를 진행하며, 제 1 제어 트랜지스터(N1)를 활용한 potentiation Enable 작동이 캐패시터 상단 노드 전압(VCP)을 Vdd/2로 부스트 업(boost up)하기 때문에 캐패시터 전압 변화는 + Vdd/2까지 potentiation 이 가능할 수 있다. 도 3b를 참조하면, 약화(depression) 업데이트 동안, 제 3 제어 트랜지스터 (N3)의 게이트 단자(G)에 펄스를 인 가하여 가중치 캐패시터(C1)의 제 2 단자(T2)에 전압(VCN)(lower terminal voltage)을 Vdd/2로 되게 하고 다음 제 4 제어 트랜지스터(N4)에 펄스를 인가하여 가중치 캐패시터(C1)의 양단 전위차(Vcap)(VCP-VCN)를 음의 방향으 로 낮출 수 있다. 다시 말해, 제 3 제어 트랜지스터(N3)를 켜서 캐패시터(C1)의 하단 노드 전압(VCN)을 Vdd/2로 부스트 업(boost up)시켜 캐패시터(C1)의 상단 노드 전압(VCP)이 접지(Ground) 보다 낮아지지 않도록 한다(Depression Enable). 캐패시터(C1) 상단 노드(T1)와 접지 노드(GND) 사이에 연결된 제 4 제어 트랜지스터(N4)를 활용하여 펄스 전류 를 통해 업데이트를 진행하며, 제 3 제어 트랜지스터(N3)를 활용한 Depression Enable 작동이 캐패시터 하단 노 드 전압(VCN)을 Vdd/2로 부스트 업(boost up)하기 때문에 캐패시터 전압 변화는 - Vdd/2까지 Depression이 가능하 다. 강화(potentiation) 업데이트 및 약화(depression) 업데이트 동작 동안에, 제어 트랜지스터 N1(N3)과 N2(N4)의 확률적 펄스가 동시에 생성되면 가중치 업데이트를 수행하고 업데이트당 전압 변화는 하기 [수학식 1] 같이 산 출될 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, 은 각각 제어 트랜지스터(N2) 및 제어 트랜지스터(N4)로부터의 방전 전류이고, 는 제어 트랜지스터 N2(N4)에 인가된 펄스 폭이고 C는 캐패시터(C1)의 정전 용량이다. 도 3c과 도 3d는 본 발명의 시냅스 소자의 읽기 동작을 설명하기 위한 동작 설명이다. 읽기 동작(read operation)은 제 1 출력 트랜지스터(N5)와 제 2 출력 트랜지스터(N6)에 흐르는 전류를 읽는 과정으로 구분할 수 있다. 도 3c를 참조하면, 제 1 출력 트랜지스터(N5)에서의 읽기 동작을 위해 제 3 제어 트랜지스터(N3)에 펄스를 인가 하여 캐패시터(C1)의 상단 노드(T1) 및 하단 노드(T2)의 전압을 각각 Vdd/2+Vcap 및 Vdd/2로 되게 한다. 제 1 출력 트랜지스터(N5)의 게이트 단자(G)에 Vdd/2+Vcap 전압이 걸린 경우 제 1 출력 트랜지스터(N5)의 소스(S)와 드레 인(D) 사이에 작은 바이어스를 적용하여 제 1 가중치 전류(IN5)를 측정할 수 있다. 즉, 캐패시터(C1) 상단 노드 (T1)와 제 1 출력 트랜지스터(N5)의 게이트 단자(G)가 연결되어 있고, 제 1 출력 트랜지스터(N5)의 드레인 단자 (D)에 0.1V 정도의 낮은 전압을 가하여 캐패시터 양단 전위차에 따라 달라지는 제 1 출력 트랜지스터(N5)의 드 레인 전류를 생성할 수 있다. 이때 제 3 제어 트랜지스터(N3)의 게이트 단자(G)에 펄스 전압을 인가해 Vdd/2 전 압을 캐패시터(C1) 하단 노드(T2)에 인가해주어 제 1 출력 트랜지스터(N5)가 선형 영역(triode region) 구간에 서 작동하여 제 1 전류(IN5)가 게이트 전압에 따라 선형적으로 거동할 수 있다. 도 3d를 참조하면, 전술한 제 1 가중치 전류(IN5_ref)를 읽은 다음, 제 2 출력 트랜지스터(N6)에서의 읽기 동작을 수행할 수 있다. 제 2 출력 트랜지스터(N6)에서의 읽기 동작을 위해 제 1 제어 트랜지스터(N1)에 펄스를 인가하 여 캐패시터(C1)의 상단 노드(T1) 및 하단 노드(T2)의 전압을 각각 Vdd/2 및 Vdd/2-Vcap로 되게 한다. 제 2 출력 트랜지스터(N6)의 게이트 단자(G)에 Vdd/2-Vcap 전압이 걸린 경우 제 2 출력 트랜지스터(N6)의 소스(S)와 드레인 (D) 사이에 작은 바이어스를 적용하여 제 2 전류(IN6)를 측정할 수 있다. 즉, 캐패시터(C1) 하단 노드(T2)와 제 2 출력 트랜지스터(N6)의 게이트 단자(G)가 연결되어 있고, 제 2 출력 트랜지스터(N6)의 드레인 단자(D)에 0.1V 정도의 낮은 전압을 가하여 캐패시터 양단 전위차에 따라 달라지는 제 2 출력 트랜지스터(N6)의 드레인 전류를 생성할 수 있다. 이때 제 1 제어 트랜지스터(N1)의 게이트 단자(G)에 펄스 전압을 인가해 Vdd/2 전압을 캐패시터 (C1) 상단 노드(T1)에 인가해주어 제 2 출력 트랜지스터(N6)가 선형 영역 구간에서 작동하여 제 2 가중치 전류 (IN6)가 게이트 전압에 따라 선형적으로 거동할 수 있다. 이때, 양수 가중치와 음수 가중치를 모두 나타내기 위해 가중치 Wij는 제 1 전류(IN5)와 제 2 전류(IN6)를 빼서 하기 수학식 2와 같이 정의된다. [수학식 2]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, i,j는 배열의 행과 열을 나타내고, IijN5는 해당 배열에서 제 1 가중치 전류(IN5_ref)을 나타내고, IijN6는 해당 배열에서 제 2 가중치 전류(IN6_ref)을 나타낸다. 도 3e와 도 3f는 본 발명의 시냅스 소자의 참조 읽기 동작(reference read operation)을 설명하기 위한 동작 설명이다. Vcap_ij=0 일 때 매칭되는 컨덕턴스를 참조(reference)라 지칭한다. 상기 참조 읽기 동작은 Vcap_ij=0 일 때 매칭되는 컨덕턴스를 읽기 하는 동작을 지칭할 수 있다. 도 3e와 도 3f는 본 발명의 시냅스 소자의 참조 읽기 동작을 설명하기 위한 동작 설명이다. 도 3e를 참조하면, 제 1 출력 트랜지스터(N5)에서의 참조 읽기 동작을 위해 제 1 제어 트랜지스터(N1)에 펄스를 인가하여 캐패시터(C1)의 상단 노드(T1)의 전압을 Vdd/2로 되게 한다. 제 1 출력 트랜지스터(N5)의 게이트 단자 (G)에 Vdd/2 전압이 걸린 경우 제 1 출력 트랜지스터(N5)의 소스(S)와 드레인(D) 사이에 작은 바이어스를 적용하 여 제 1 참조 전류(IN5_ref)를 측정할 수 있다. 즉, 캐패시터(C1) 상단 노드(T1)와 제 1 출력 트랜지스터(N5)의 게이트 단자(G)가 연결되어 있고, 제 1 출력 트랜지스터(N5)의 드레인 단자(D)에 0.1V 정도의 낮은 전압을 가하 여 제 1 출력 트랜지스터(N5)의 게이트 단자(G)에 걸린 Vdd/2 전압에 의해 흐르는 제 1 출력 트랜지스터(N5)의 드레인 전류를 생성할 수 있다. 도 3f를 참조하면, 전술한 제 1 참조 전류(IN5_ref)를 읽은 다음, 제 2 출력 트랜지스터(N6)에서의 참조 읽기 동 작을 수행할 수 있다. 제 2 출력 트랜지스터(N6)에서의 참조 읽기 동작을 위해 제 3 제어 트랜지스터(N3)에 펄 스를 인가하여 캐패시터(C1)의 하단 노드(T2)의 전압을 Vdd/2로 되게 한다. 제 2 출력 트랜지스터(N6)의 게이트 단자(G)에 Vdd/2 전압이 걸린 경우 제 2 출력 트랜지스터(N6)의 소스(S)와 드레인(D) 사이에 작은 바이어스를 적 용하여 제 2 참조 전류(IN6_ref)를 측정할 수 있다. 즉, 캐패시터(C1) 하단 노드(T2)와 제 2 출력 트랜지스터(N 6)의 게이트 단자(G)가 연결되어 있고, 제 2 출력 트랜지스터(N6)의 드레인 단자(D)에 0.1V 정도의 낮은 전압을가하여 제 2 출력 트랜지스터(N5)의 게이트 단자(G)에 걸린 Vdd/2 전압에 의해 흐르는 제 2 출력 트랜지스터(N 6)의 드레인 전류를 생성할 수 있다. 여기서, 제 1 참조 전류(IN5_ref)와 제 2 참조 전류(IN6_ref)의 차이를 산출함으로써, 캐패시터(C1)에 전하가 하나 도 축적되지 않은 상황, 즉, Vcap=0인 상황에서의 컨덕턴스(이하 'reference conductance' 또는 'leakage converging conductance'라 지칭함)를 추가적인 reference array 없이도 참조(reference)를 읽을 수 있다. 본 발명은 6T1C 구조의 장치 내에서 leakage converging conductance를 이용하여 'reference conductance'를 안정 적으로 읽을 수 있다. 더하여, 본 발명의 6T1C 구조의 장치는 반전(invert) 동작을 수행할 수 있다. 구체적으로, 도 3a의 강화 (potentiation) 업데이트 동작은 제 1과 제 2 트랜지스터(N1, N2)를 on 시킴으로써 수행되고, 도 3b의 약화 (depression) 업데이트 동작은 제 3과 제 4 트랜지스터(N3, N4)를 on 시킴으로써 수행된다. 여기서, 반전 동작 을 통해서 강화(potentiation) 업데이트 동작은 제 3과 제 4 트랜지스터(N3, N4)를 on 시킴으로써 수행되고, 도 3b의 약화(depression) 업데이트 동작은 제 1과 제 2 트랜지스터(N1, N2)를 on 시킴으로써 수행될 수 있다. 유사하게, 도 3c와 도 3d의 읽기 동작(read operation)은 제 3 트랜지스터를 on 시켜 제 1 가중치 전류(IN5)를 읽은 다음 제 1 트랜지스터를 on 시켜 제 2 가중치 전류(IN6)를 읽음으로써 수행된다. 여기서, 반전 동작을 통해 서 읽기 동작(read operation)은 제 1 트랜지스터를 on 시켜 제 2 가중치 전류(IN6)를 읽은 다음 제 3 트랜지스 터를 on 시켜 제 1 가중치 전류(IN5)를 읽음으로써 수행될 수 있다. 마찬가지로, 도 3e와 도 3f의 참조 읽기 동작(reference read operation)은 제 1 트랜지스터를 on 시켜 제 1 참조 전류(IN5_ref)를 읽은 다음 제 3 트랜지스터를 on 시켜 제 2 참조 전류(IN6_ref)를 읽음으로써 수행될 수 있다. 여기서, 반전 동작을 통해서 참조 읽기 동작은 제 3 트랜지스터를 on 시켜 제 2 참조 전류(IN6_ref)를 읽은 다음 제 1 트랜지스터를 on 시켜 제 1 참조 전류(IN5_ref)를 읽음으로써 수행될 수 있다. 전술한 업데이트 동작, 읽기 동작(read operation), 참조 읽기 동작(reference read operation) 및 반전(invert) 동작을 위한 제어 트 랜지스터(N1 내지 N4)와 출력 트랜지스터(N5, N6)의 스위칭 on/off 동작은 하기 [표 1]과 같다. [표 1]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "반전(invert) 동작을 수행함으로써, symmetry conductance와 leakage converging conductance의 값이 서로 달 라 상기 참조 읽기 동작(reference read operation)을 수행했을 시 발생할 수 있는 asymmetry로 인한 정확도 열화를 개선시킬 수 있다. 또한, 장치 내 트랜지스터의 편차(variation)를 상쇄시킬 수 있다. 더하여, 추가적인 reference array 필요 없이 집적도 향상 가능하다. 도 3g는 본 발명의 시냅스 소자의 리셋 동작(reset method)을 설명하기 위한 동작 설명이다. 도 3g를 참조하면, 제 2 제어 트랜지스터(N2)와 제 4 제어 트랜지스터(N4)만 온(on)시킴으로써 가중치 캐패시터 (C1)에 쌓인 전하가 모두 소실되어 Vcap = 0V으로 로 리셋(reset) 시킬 수 있다. 바람직하게 상기 리셋 동작은 상기 반전 동작 전에 수행되어야 한다. 만약 상기 리셋 동작이 상기 반전 동작 후에 수행되면 이전에 쌓인 전하 에 대응하는 가중치가 부호가 반전되어 읽히는 것을 방지하기 위함이다. 도 4a 내지 도 4e는 본 발명의 선형 특성을 갖는 시냅스 소자(6T1C)를 이용한 코어 소자의 학습 방식을 설명하 기 위한 도면이다. 선형 학습 및 비선형 학습은 core device array(또는 inference cell array)와 auxiliary device array를 통해 수행될 수 있으며, core device array와 auxiliary device array는 각각 복수의 시냅스장치들이 column(N)× row(M) 배열로 구성될 수 있다. 여기서, M, N은 자연수이다. 또한, core device array와 auxiliary device array의 시냅스 장치는 서로 동일하거나 서로 다를 수 있다. 바람직하게, core device array 의 시냅스 장치는 종래 시냅스 장치를 사용하고, auxiliary device array의 시냅스 장치는 전술한 6T1C 장치를 사용할 수 있다. 또는 core device array와 auxiliary device array의 시냅스 장치는 모두 전술한 6T1C 장치를 사용할 수 있다. 도 4a를 참조하면, 먼저 core device array에서 순전파(forward propagation)와 역전파(backpropagation) 방식 을 적용하여 출력 값을 획득할 수 있다. 구체적으로, core device array에서 제 1 입력 값(input(x))를 입력하 여 포워드(forward) 출력 값(fo)을 출력하고, 제 2 입력 값(Delta(d))를 입력하여 백워드(backward) 출력 값 (bo)을 출력할 수 있다. 상기 백워드(backward) 출력 값(bo)은 back propagate된 오차(error)일 수 있다. 순전 파(forward propagation)와 역전파(backpropagation) 방식에 의해 업데이트 양을 결정하는 알고리즘은 종래 공 지된 기술을 참조할 수 있다. 도 4b를 참조하면, 도 4a 이후에 core device array에서 순전파와 역전파를 기반으로 얻은 입력 값(x, d)를 기 반으로 펄스를 생성하여 auxiliary device array(6T1C array)에서 업데이트가 진행될 수 있다. 즉, x, d 값에 비례한 상기 펄스가 auxiliary device array에 입력되어, pulse overlap 방식으로 fully-parallel update가 진 행될 수 있다. 일반적인 RPU(resistive processing units) update 기술을 참조할 수 있다. 도 4c를 참조하면, 도 4b에 설명한 업데이트 이후에 입력(input)를 통해 auxiliary device array의 특정 column를 선택하여 특정 column의 각각의 소자로부터 출력 전류를 획득할 수 있다. 예컨대, 도 4c에서는 첫번째 column에 소정의 펄스 전압(V)을 공급하고 나머지 column들에는 OV 펄스 전압을 공급하여, 첫번째 column에 배 치된 장치들(G11, G21,쪋,GM1)로부터 각각 출력 전류(I1, I2,쪋,IM)를 획득할 수 있다. 여기서, p = Ouput(I1, I2,...,IM) × transfer_learining_rate 으로 정의한다. transfer_learining_rate는 전이 학습률이다. 상기 출 력 전류(I1, I2,...,IM)는 후술할 도 4d에 개시된 읽기 동작 및 참조 읽기 동작을 통해 p로 변환될 수 있다. 도 4d를 참조하면, 1-(a)와 2-(a)는 전술한 도 3c와 도 3d의 읽기 동작이고, 2-(b)와 2-(b)는 전술한 도 3e와 도 3f의 참조 읽기 동작이다. 상기 읽기 동작 및 상기 참조 읽기 동작의 상세한 설명은 도 3c 내지 도 3f의 상 세한 동작을 참조할 수 있다. 전술한 p 변환을 위한 후처리 연산은 (Output1-(a) - Output2-(a)) - (Output1-(b) - Output2-(b))(이하 제 1 연산이라 칭함) 또는 (Output1-(a) + Output1-(b)) - (Output2-(a) + Output2-(b))(이하 제 2 연산이라 칭함)일 수 있다. 여기서, Output1-(a)는 1-(a)의 회로 동작으로부터 출력된 전류(IN5)이고, Output1-(b)는 1-(b)의 회로 동작으로부 터 출력된 전류(IN6_ref)이고, Output2-(a)는 2-(a)의 회로 동작으로부터 출력된 전류(IN6)이고, Output2-(b)는 2- (b)의 회로 동작으로부터 출력된 전류(IN5_ref)이다. 바람직하게, 상기 제 1 연산의 결과 값과 상기 제 2 연산의 결과 값은 동일할 수 있다. 상기 제 2 연산에서, (Output1-(a) + Output1-(b))항의 의미는 1-(a)의 회로 동작과 1- (b)의 회로 동작이 동시에 수행되는 것을 의미하고 (Output2-(a) + Output2-(b)) 항의 의미는 2-(a)의 회로 동작과 2-(b)의 회로 동작이 동시에 수행되는 것을 의미한다. 상기 제 2 연산의 경우, read 시 때마다 동시에 수행되므 로 read에 소요되는 시간을 감소시킬 수 있지만, 매 read시 reference를 읽을 필요가 없는 경우도 있으며, 회로 동작상의 문제를 고려하여 동시에 읽는 것을 제한할 필요가 있다. 따라서, 상기 제 1 연산에서 (Output1-(a) - Output2-(a)) 항 및 (Output1-(b) - Output2-(b))항처럼 read 동작을 따로 수행할 수 있다. 예컨대, 1-(a)의 회로 동 작 다음에 2-(a)의 회로 동작이 수행되거나 1-(b)의 회로 동작 다음에 2-(b)의 회로 동작이 수행될 수 있다. 도 4e를 참조하면, auxiliary device array에서 선택된 column과 동일한 위치의 core cell array의 column에 p 에 비례한 만큼 업데이트를 진행한다. p>0이면 강화(potentiation)이고, p<0이면 약화(depression)이다. 예컨대, 도 4d에서는 core cell array의 첫번째 column의 소자들(G11, G21,...,GM1)의 가중치를 p에 비례한 만큼 업데이트시킬 수 있다. 상기 p는 도 4d에 개시된 회로 동작을 통해 얻어진 값일 수 있다. 첫번째 column에 대해 서 core device array와 auxiliary device array가 업데이트가 끝나면, 두번째 column 내지 N column에 대해서 전술한 도 4a 내지 도 4e를 반복 수행할 수 있다. 도 5a 내지 도 5f는 본 발명의 비선형 특성을 갖는 시냅스 소자(6T1C)를 이용한 코어 소자의 학습 방식을 설명 하기 위한 도면이다. 도 5a 내지 도 5c는 도 4a 내지 도 4c와 동일하므로, 모순되지 않는 한 도 4a 내지 도 4c의 설명을 참조할 수 있다. 전술한 참조 읽기의 경우에 누설 수렴 컨덕턴스(leakage converging conductance)를 읽는 방식이어서, 소자가 비선형일 경우 업데이트(update)가 진행됨에 따라 대칭 컨덕턴스(symmetry conductance)로 컨덕턴스가 변화하려 는 의도치 않은 힘이 발생할 수 있다. 이러한 소자 내부적인 의도치 않은 힘 때문에 원하는 컨덕턴스로 소자를 설정하지 못하고 그로 인해 정확도가 열화될 수 있다. 따라서 상기 누설 수렴 컨덕턴스와 상기 대칭 컨덕턴스가 동일하지 않기 때문에 정확도 열화가 발생하는 것이며, 상기 누설 수렴 컨덕턴스와 상기 대칭 컨덕턴스를 0으로 수렴시키는 것이 아니라 symmetry conductance로 conductance가 변화하는 방향성을 확률적인 반전(invert) 동 작을 통해 상기 누설 수렴 컨덕턴스와 상기 대칭 컨덕턴스의 방향성 차이를 상쇄시켜줄 수 있다. 구체적으로, 소자 자체적으로 대칭 컨덕턴스(weight > 0) > 누설 수렴 컨덕턴스 (weight = 0을 대변하는 컨덕턴 스) 일 때, 업데이트가 진행됨에 따라서 가중치(weight)가 양의 방향으로 커지는 의도치 않은 경향성이 나타날 수 있다. 그러나, 본 발명의 반전(invert) 과정을 통해서 상기 대칭 컨덕턴스가 weight <0을 대변할 수 있게 하 여 주기적으로 의도치 않은 가중치 변화 방향성을 상쇄시킬 수 있다. 일 실시예에서, 난수를 발생시켜 발생된 난수를 이용하여 확률을 계산할 수 있으며, 계산된 확률에 의해 리셋 또는 반전 동작이 수행된다. 도 5d는 확률에 기반하여 선택적으로 수행되거나 수행되지 않을 수 있다. 도 5d의 리셋 동작은 전술한 도 3g의 리셋 동작과 동일하므로, 모순되지 않는 한 도 3g의 리셋 동작에 대한 설 명을 참조할 수 있다. 또한, 도 5d의 반전 동작은 전술한 표 1의 반전 동작과 동일하므로, 모순되지 않는 한 상 기 [표 1]의 리셋 동작에 대한 설명을 참조할 수 있다. 이후 도 5e는 읽기 동작 및 참조 읽기 동작을 통해 출력 전류를 p로 변환시키는 도 4d와 동일하므로, 모순되지 않는 한 도 4d의 리셋 동작에 대한 설명을 참조할 수 있다. 이후 도 5f는 읽기 동작 및 참조 읽기 동작을 통해 출력 전류를 p로 변환시키는 도 4e와 동일하므로, 모순되지 않는 한 도 4e의 core device array 업데이트에 대한 설명을 참조할 수 있다. 또 다른 실시예에서, 도 5d의 리셋 동작 또는 반전 동작은 도 5f 이후에 수행되거나 core device array를 업데 이트하는 동안(도 5f) 수행될 수 있다. 종래에는 1개의 신경망(neural network)은 동일한 차원(dimension)을 갖는 코어 장치 배열(core device arra y)와 보조 장치 배열(auxiliary device array)로 구성된다. 여기서, 보조 장치(auxiliary device)의 선형 컨덕 턴스(symmetry conductance)로 참조 장치(reference device)의 컨덕턴스를 초기화(initialization)시켜야 하며, 상기 보조 장치 자체도 대칭성 컨덕턴스로 초기화되어야 한다. 따라서, 추가적인 참조 장치 배열 (reference device array)가 필수적이다. 또한, 종래에는 대칭성 컨덕턴스를 초기화 하기 위해서 모든 보조 장 치에 컨덕턴스를 증가시키는 조건의 펄스와 감소시키는 펄스를 교대로 인가해야 하는 불편함이 있다. 그러나, 본 발명의 제안된 6T1C 구조를 갖는 장치 및 상기 장치의 가중치 업데이트 동작(potentiation, depression), 읽기 동작(read operating), 참조 읽기 동작(reference read operating), 반전 동작(invert operating), 리셋 동작(reset operating)을 포함하는 구동 알고리즘을 이용함으로써, 다음과 같은 특징을 가질 수 있으며 이로 인해 종래의 문제점을 해결할 수 있다. 먼저, 본 발명의 시냅스 장치를 선형 학습 알고리즘 및 비선형 학습 알고리즘에 이용하는 경우에, 누설 (leakage)이 존재하는 캐패시터 기반 장치에 쌓인 가중치(weight)를 비휘발성 메모리(non-volatile memory)로 효율적으로 전달할 수 있고, 6T1C 장치의 장점을 활용하여 auxiliary device array에 필수적인 reference device array 없이도 누설 수렴 컨덕턴스(leakage converging conductance)를 읽을 수 있어서 집적도 향상을 기대할 수 있다. 또한, 소자가 비선형적일수록 업데이트를 반복함에 따라서 symmetry conductance로의 힘을 받게 되기 때문에, 종래에는 reference conductance를 symmetry conductance로 하여 해당 힘이 weight=0으로 되도록 한다(weight =conductance -ref conductance(symmetry conductance)). 그러나, 본 발명의 6T1C 장치의 경우 비대칭성 뿐만 이 아니라 보유(retention) 문제로 인해 conductance를 변화시키는 요인이 존재할 수 있다. 만약 6T1C 장치가 매우 선형적일 경우 비대칭성으로 인한 symmetry conductance로의 힘이 약하기 때문에 보유(retention)으로 인 한 conductance 변화를 weight=0이 되게 reference로 leakage converging conductance를 읽을 수 있다. 뿐만 아니라 leakage converging conductance 즉, Vcap=0일 때의 경우 이상적으로 (이상적: N1, N3 트랜지스터 특성 이 같고, N2, N4의 트랜지스터 특성이 같은 경우) potentiation으로 인한 Vcap 변화량과 depression으로 인한Vcap 변화량이 같아 leakage converging conductance가 symmetry conductance가 될 수 있다. 어느 정도 기대 값 이 같기 때문에 본 발명에서 제안한 방식으로 reference conductance를 설정하는 것이며, 장치의 비대칭성이 큰 경우 symmetry conductance와 leakage converging conductance의 차이로 인한 코어 장치의 잘못된 학습을 보완 하기 위해서 반전(invert) 과정을 진행할 수 있다. 이로 인해, 보유 시간에 대해서 강인한 특성(robust)을 가지 며 누설로 인한 정확도 열화를 개선시킬 수 있다. 또한, 선형 학습의 경우에 비대칭성(asymmetry)에 의한 정확도 열화가 거의 발생하지 않는다. 더하여, 비선형 학습의 경우, 비대칭성(asymmetry)으로 인한 정확도 열화를 개선시켜주도록 확률적으로 반전 (invert)을 진행할 수 있으며, 추가적으로 6T1C 구조를 갖는 시냅스 장치 내의 트랜지스터의 비대칭성도 보완할 수 있다. 또한, 소자가 비선형적일수록 해당 소자가 표현하는 가중치(weight) 값이 너무 커지거나 작아 지는 현상을 방지할 수 있으며, 비선형적일수록 업데이트를 반복함에 따라 대칭 컨덕턴스(symmetry conductance)로 영향을 받을 수 있다. 실험 측정 셋업 및 단일 시냅스 디바이스의 다양한 속성 도 6은 본 발명의 실시예에서 시냅스 장치 측정을 위한 인쇄회로기판(Printed Circuit Board, PCB) 이미지이다. 도 7a는 본 발명의 실시예에 따른 6T1C 구조를 갖는 단일 시냅스 장치의 가중치 업데이트 결과 그래프이다. 도 7b는 본 발명의 실시예에 따른 가중치 업데이트 곡선을 나타내는 도면이다. 도 7c는 본 발명의 실시예에 따른 6T1C 구조를 갖는 단일 시냅스 장치의 보유(retention) 특성 결과를 보여주는 그래프이다. 도 7d는 본 발명의 실시예에 따른 6T1C 구조를 갖는 단일 시냅스 장치의 사이클링 내구성 측정을 위한 초기 사이클에서의 측정 결 과를 보여주는 그래프이다. 도 7e는 본 발명의 실시예에 따른 6T1C 구조를 갖는 단일 시냅스 장치에 109개의 업 데이트 펄스를 인가하여 얻은 측정 결과 그래프이다. 도 6을 참조하면, 단일 시냅스 디바이스의 다양한 특성을 측정하기 위해, 마이크로컨트롤러 유닛 (microcontroller unit: MCU)와 개별 집적 회로 부품(예: shifter, Integrator, Noise Filter, I/O PIN, power 모듈)을 결합하여 시냅스 셀 어레이와 상호 작용하는 인쇄 회로 기판(PCB)을 사용하였다. shifter, integrator, Noise Filter는 종래에 공개된 IC 칩이나 부품으로 구성될 수 있으며, 상기 PCB 회로에 대한 상세 한 설명은 생략하기로 한다. 상기 시냅스 셀 전류는 PCB 상에 전류 적분기(integrator)와 MCU(ADC)에 의해 측정 될 수 있다. 도 7a를 참조하면, 단일 시냅스 장치의 제 1 전압(=Vdd/2)은 2.0V로 공급되고, 과구동전압(= Vgs-Vth으로 정의되 는 Vov)은 0.25V 크기와 펄스 폭이 300 ns인 업/다운 펄스 1,000 개를 제 2 제어 트랜지스터(N2), 제 4 제어 트 랜지스터(N4)에 인가하여 결과를 얻었다. 도 7b를 참조하면, 제 1 전압(=Vdd/2)이 1.5로 공급되고, 과구동전압 (Vov = Vgs-Vth)이 1.4V의 1,000 up/down 펄스를 제 2 제어 트랜지스터(N2), 제 4 제어 트랜지스터(N4)에 인가하 여 결과를 얻었다. 도 7a와 도 7b는 도 6의 개시된 PCB 회로를 이용하여 측정한 6T1C 구조를 갖는 단일 시냅스 장치의 가중치 업데 이트 특성을 보여준다. 도 7a는 1,000회 양(positive)의 업데이트와 1,000회의 음(negative)의 업데이트의 4 주기를 적용하여 단일 셀의 측정된 ADC 변화를 보여준다. 시냅스 장치의 향상된 선형성과 대칭성을 유지하기 위 해서 제 2 제어 트랜지스터(N2), 제 4 제어 트랜지스터(N4)가 포화영역에 도달하도록 하여야 한다. 이를 위해서, 제 2 제어 트랜지스터(N2), 제 4 제어 트랜지스터(N4)에 제 1 전압(=Vdd/2) 보다 낮은 오버드라이브 전 압(Vov = Vgs-Vth)을 인가하고 제 1 전압(=Vdd/2)에 높은 전압을 인가할 수 있다. 도 7b를 참조하면, 각각 100ns, 200ns, 400ns 전압 펄스에 따른 컨덕턴스 변조를 보여준다. 도 7a와 도 7b를 참조하면, 6T1C 구조를 갖는 시냅 스 장치가 1,000개의 컨덕턴스 상태에서 매우 선형적이며 대칭적인 가중치 업데이트 특성을 나타내며 장치의 컨 덕턴스 변조가 동일한 장치에서 측정 조건을 변경하여 정확하게 제어될 수 있음을 알 수 있다. 도 7c를 참조하면, 각 제어 트랜지스터(N1-N4)에 -2V의 오프 전압(off voltage)을 인가하여 보유 특성을 측정하 였다. 먼저 장치의 보유 특성을 평가하기 위해 시냅스 셀에 강화 펄스를 반복적으로 인가하여 캐패시터(C1)를 의도적으로 충전한 다음 모든 제어 트랜지스터(N1-N4)에 오프 전압을 인가하고 정해진 시간 간격(60분)마다 읽 기 동작을 수행하였다. 측정된 ADC를 캐패시터 전압으로 환산한 후 지수 조정(exponential fitting)하여 시정수 를 추출한 결과 약 775 분을 얻었다. 이러한 결과는 6T1C 구조의 시냅스 장치가 기존의 실리콘 및 캐패시터 기반 시냅스 장치에 비해 매우 우수한 유지 특성을 나타냄을 보여준다. 도 7d와 도 7e를 참조하면, 109개의 업데이트 펄스(업데이트 펄스의 높이는 0.5V/-2V, 길이는 1μs)를 인가한 후의 측정 결과이다. 업데이트 펄스 109는 시냅스 장치에 1,000 up/1,000 down 펄스를 500,000회 반복하여 인가 하였다. 도 7d, 도 7e에 나타낸 초기 사이클과 마지막 사이클의 출력 ADC를 각각 비교하면 109개의 펄스가 인가 된 후에도 디바이스가 여전히 동작하고 있고 ADC의 출력 범위가 거의 변하지 않음을 확인하였다. 확인을 통해 6T1C 구조의 시냅스 장치는 오래 견뎌낼 뿐만 아니라 많은 사이클에 걸쳐 안정적인 아날로그 특성을 보이는 것 을 확인할 수 있다. 5Х1 6T1C 배열에서 확률적 업데이트 방식에 기반한 선형 회귀의 실험적 결과 도 8a는 본 발명의 실시예에 따른 선형 회귀 훈련을 위한 흐름도이고, 도 8b는 본 발명의 실시예에 따른 훈련 전반에 걸친 손실 및 오류의 평가 결과 그래프이다. 도 8a를 참조하면, 훈련은 피드포워드(feedforward) 및 가중치 업데이트(potentiation, depression)의 2단계로 구성되며, 손실은 평균제곱오차(mean square error MSE) 함수를 사용하여 로 정의된다. 도 8b를 참조하면, 훈련이 진행됨에 따라 손실과 오류가 모두 0으로 수렴될 수 있다. 25개의 입력 데이터 세트 가 각 epoch에서 훈련된다(input data per 1 epoch). total epochs은 20이고, 데모 동안 비트 길이(bit length)는 10으로 설정되었고 학습률은 0.05이다. 도 8b의 오차는 로 정의된다. 크로스바 배열에서 선형 회귀 구현 6T1C 5Х1 크로스바 어레이를 사용하여 선형 회귀를 실험적으로 수행하여 시냅스 장치의 온-칩 훈련 성능을 평 가했다. 훈련 과정은 도 8a의 흐름도를 사용하였고, 피드포워드 과정에서의 입력 데이터와 가중치 업데이트 과 정에서의 확률적 업데이트 펄스는 PCB에 위치한 MCU를 통해 실시간으로 생성되었다. 먼저 피드포워드 과정을 위 한 입력 데이터셋을 생성하고 이를 5개의 시냅스 장치의 워드라인(WL)에 각각 펄스 폭 형태로 적용하였다. 입력 데이터 행렬은 임의로 생성된 4개의 데이터와 의 형태로 y절편 역할을 하는 1개의 고정 값 으로 구성된다. 피드포워드 프로세스가 수행되면 하기 [수학식 3]의 y 값이 생성된다. [수학식 3]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, xi는 입력 데이터 행렬을 나타내고 는 가중치 행렬을 나타낸다. 다음 Y는 [수학식 4a]의 대상 값 t와 비교하여 [수학식 4b]에서 오차 벡터 를 생성할 수 있다. [수학식 4a]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "[수학식 4b]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "여기서, 는 목표 가중치 매트릭스를 나타낸다. 출력 ADC 값을 가중 합계로 변환하는 프로세스 및 손실 계산 프로세스는 PCB에 위치한 MCU를 통해 수행된다. 다음 확률론적 업데이트 기법을 통해 가중치 업데 이트가 수행된다. 각 시냅스 셀의 가중치 업데이트 양은 하기 [수학식 5a] 및 [수학식 5b]를 따른다. [수학식 5a] [수학식 5b]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, xn과 a는 각 시냅스 셀에 적용된 입력 데이터, 는 피드포워드를 통해 추출된 오류, η는 학습률을 나 타낸다. 가중치 갱신 과정에서 xn과 a는 제 1 제어 트랜지스터(N1) 또는 제 3 제어 트랜지스터(N3)의 펄스 발생 확률로 변환되었고, 는 제 2 제어 트랜지스터(N2) 또는 제 4 제어 트랜지스터(N4)의 펄스 발생 확률로 변환되 었다. 도 8a에 도시된 바와 같이, (y-t)의 부호는 강화 또는 약화를 위한 펄스를 생성할지 여부를 결정할 것이다. 이 절차를 반복하면 6T1C 5Х1 크로스바 배열을 훈련하고 선형 회귀 문제를 해결할 수 있다. 도 8b는 학습 이 진행됨에 따라 손실과 오류가 0으로 수렴됨을 보여준다. 이러한 선형 회귀 결과는 6T1C 어레이의 자동 및 병 렬 온-칩 훈련 기능을 보여준다. 6T1C 장치를 이용한 패턴인식 시뮬레이션 도 9a는 본 발명의 실시예에 따른 다양한 측정 조건에서 얻은 컨덕턴스-컨덕턴스 변화 형태의 결과 그래프이다. 도 7b는 본 발명의 실시예에 따른 6T1C 구조를 갖는 시냅스 장치의 NL 범위 내에서 다양한 값을 반영하여 종래 Tiki-taka algorithm를 적용한 보유 특성에 따른 장치의 정확도를 나타내는 그래프이다. 도 7c는 본 발명의 실 시예에 따른 NL=0.2, NL=2.0에서 제안 발명을 적용한 보유 특성에 따른 장치의 정확도를 나타내는 그래프이다. 이하 설명에서, 'NL'는 선형성과 비선형성을 나타내는 파라미터이고, 'NLp'는 컨덕턴스가 증가시키는 방향, 즉, 강화 동작에서 선형과 비선형성을 나타내고, 'NLd'는 컨덕턴스를 감소시키는 방향, 즉, 약화 동작에서 선형과 비 선형성을 나타낸다. 구체적으로, 'NL' 값이 0에 가까울수록 선형성을 갖고, 'NL' 값이 2에 가까울수록 비선형성 을 갖는다. LENET5은 1998년 Yann LeCun의 논문 'Gradient-Based Learning Applied to Document Recognitio n'에 담겨있는 CNN 신경망의 구조를 지칭하고, 다층 퍼셉트론(MLP: multi-layer perceptron)는 여러 개의 퍼셉 트론 뉴런을 여러 층으로 쌓은 다층신경망 구조 모델을 지칭한다. 도 9a를 참조하면, NL = 0.2, NL = 2.0의 그래프는 각각 도 7a와 도 7b의 빨간선과 녹색선을 변환하여 얻은 결 과이다. NL = 2.0인 경우, 학습에 사용되는 일부 영역에 대해서만 선형 회귀 분석을 사용하였다. 도 9b를 참조하면, 종래 Tiki-taka algorithm를 사용한 경우에, 보유 특성적용한 보유 시간(retention time: t/RC)이 작아질수록 정확도가 떨어진다. 이는 종래 캐패시터 기반 소자의 경우 누설 수렴 컨덕턴스(leakage converging conductance)로 인해 누설(leakage)가 발생되기 때문이다. 또한, 종래의 경우 학습에 필요한 신경망이 커질 경우, 보조 학습을 위한 참조 배열(reference array)를 필요 로 하며, 이러한 참조 배열이 차지하는 면적이 커져 집적도가 저하되는 문제점이 있다. 또한, 학습이 진행됨에 따라 소자의 특성이 변경되어 대칭 컨덕턴스(symmetry conductance)가 변경될 수 있다. 상기 대칭 컨덕턴스는 소자 마다 인가되는 펄스 1회당 컨덕턴스로서, 증가량과 감소량이 같은 컨덕턴스 값이다. 도 9c를 참조하면, 본 발명의 따른 6T1C 구조를 갖는 시냅스 장치의 경우 도 7b의 종래의 경우와 다르게 선형성 환경(NL=0.2)에서 leakage converging conductance로 reference로 설정하여 신경망 훈련 결과, 즉 학습의 정확 도 결과가 작은 보유 시간(retention time: t/RC)에서도 대략 97% 이상임을 확인할 수 있다. 즉, LENET5 모델 의 경우 5e-2 및 MLP 모델의 5e-3에서 학습에 최적화된 학습률(선형 학습)을 사용했을 때 약 97%의 정확도를 달 성했다. 그러나, 비대칭성이 큰 경우 대칭 컨덕턴스로의 힘이 강하기 때문에 leakage converging conductance로 reference를 설정하는 것만으로는 최적의 정확도를 얻을 수 없음을 의미한다. 도 10은 본 발명의 실시예에 따른 참조 컨덕턴스(reference conductance)를 설정하는 방법을 설명하기 위한 도 면이다. 상기 참조 컨덕턴스는 전술한 Vcap = 0일 때의 컨덕턴스를 지칭한다. 도 10을 참조하면, 상기 참조 컨덕턴스는 제 1 제어 트랜지스터(N1)에 펄스가 인가(①)된 제 1 출력 트랜지스터 (N5)의 전류(IN5)(②)와 제 3 제어 트랜지스터(N3)에 펄스가 인가(③)된 제 2 출력 트랜지스터(N6)의 전류(IN6) (④)의 차이로 읽힌다(⑤). 구체적으로, 제 1 제어 트랜지스터(N1)에 펄스(VDD/2)를 인가한 후(①), 제 1 출력트랜지스터(N5)에 흐르는 전류(IN5)를 측정한다(②). 다음 제 3 제어 트랜지스터(N3)에 펄스를 인가한 후(③), 제 2 출력 트랜지스터(N6)에 흐르는 전류(IN6)(④)를 측정한다. 이후 전류(IN5)와 전류(IN6)의 차이를 산출한다 (⑤). 도 11a와 도 11b는 LENET5 모델 및 MLP 모델에서 각각 다양한 보유 레벨에서 본 발명의 선형 학습 방법과 종래 방법에 따른 학습 결과를 비교하는 결과 그래프이다. 본 발명의 선형 학습 방법(제안 발명1(Modified TTv1)이라 칭함)은 전술한 참조 읽기(reference read) 동작을 통해 수행되는 학습을 의미하며, 종래 방법은 선형 또는 비 선형 학습에 TTv1 알고리즘을 통해 수행되는 학습을 의미하다. 도 11a를 참조하면, LENET5 모델의 경우 5e-4 이하에서 제안 발명(빨간색)을 적용하면 에러율이 종래 Tiki-taka algorithm(검정색)를 적용했을 때보다 낮지만, 5e-5 이상에서는 제안 발명을 적용하면 에러율이 종래 Tiki-taka algorithm를 적용했을 때보다 높다. 도 11b를 참조하면, 도 9a와 비슷하게, MLP 모델의 경우 5e-5 이하에서 제안 발명(빨간색)을 적용하면 에러율이 종래 Tiki-taka algorithm(검정색)를 적용했을 때보다 낮지만, 5e-6 이상에서는 제안 발명을 적용하면 에러율이 종래 Tiki-taka algorithm를 적용했을 때보다 높다. 전술한 바와 같이, 선형 학습의 경우, 특정 임계치를 기준으로 제안 발명 1의 알고리즘(Modified TTv1)을 수행 하고, 상기 특정 임계치 이후에는 종래 방식의 TTv1 알고리즘을 수행함으로써 에러율을 개선시킬 수 있다. 또 다른 실시예에서, 선형 또는 비선형 학습에 대해서 전술한 상기 참조 읽기(reference read) 동작, 리셋 (reset) 동작 및 반전(invert) 동작을 수행하는 알고리즘(예컨대, 제안발명 2라 지칭함)을 이용하여, 전체 구간 에 대해서 에러율을 개선하는 효과를 얻을 수 있다. 도 12a와 도 12b는 LENET5 모델에서 반전 및 리셋 동작이 수행되는 경우와 수행되지 않은 경우의 비선형 학습 결과를 비교하는 결과 그래프이다. 도 12a는 retention 값을 5e8로 설정하여 실험한 결과이고, 도 12b는 retention 값을 무한대(infinite)로 설정하여 실험한 결과이다. x 축(epoch)은 학습 반복 횟수이다. 빨간색 선 은 전술한 반전(invert) 및 리셋(reset) 동작이 적용된 제안 발명 2의 알고리즘을 적용한 경우이고, 검정색은 전술한 반전(invert) 및 리셋(reset) 동작이 적용되지 않은 제안 발명 1의 알고리즘을 적용한 경우이다. 도 12a와 도 12b을 참조하면, 제안 발명 1(검정색)은 비선형 학습이 반복될수록 학습도에 열화가 증가되어 에러 율이 증가하지만, 제안 발명 2(빨간색)은 비선형 학습이 반복될수록 학습도 열화가 개선되어 에러율이 감소하는 것을 알 수 있다. 이는 앞서 언급한 바와 같이, 소자 자체적으로 대칭 컨덕턴스(weight > 0) > 누설 수렴 컨덕 턴스 (weight = 0을 대변하는 컨덕턴스) 일 때, 업데이트가 진행됨에 따라서 가중치(weight)가 양의 방향으로 커지는 의도치 않은 경향성이 나타나 정확도가 열화될 수 있지만, 본 발명의 반전(invert) 과정을 통해서 상기 대칭 컨덕턴스가 weight <0을 대변할 수 있게 하여 주기적으로 의도치 않은 가중치 변화 방향성을 상쇄시켜서 열화가 개선되는 것으로 사료된다. 다른 실시예에서, 비선형 학습 초기(대략 0 < epoch < 18 범위)에는 상대적으로 제안 발명 2(빨간색)보다 제안 발명 1(검정색)의 에러율이 낮으므로, 학습 초기에는 제안 발명 1로 학습을 진행하다가 임계치 이후에는 제안 발명 2으로 진행하면 제안 발명 2로만 비선형 학습을 진행하는 경우보다 에러율을 낮출 수 있다. 6T1C 구조를 갖는 시냅스 장치에 최적화된 알고리즘 AiMC 시스템의 성능 개선은 알고리즘 레벨과 하드웨어 레벨 접근 방식 모두를 고려할 필요가 있다. 본 발명은 하드웨어-알고리즘 공동 디자인을 통한 최적화 향상을 목표로 한다. 종래 SGD 알고리즘 및 TTv1(Tiki-Taka 알고 리즘)를 본 발명의장치를 적용하여 시뮬레이션을 수행했다. 본 발명의 시뮬레이션 결과는 본 발명이 SGD 알고리 즘뿐만 아니라 TTv1에 적합하다는 것을 보여준다. 추가로, 본 발명은 TTv1의 성능을 뛰어 넘는 장치인 6T1C 구 조의 시냅스 장치의 구동 알고리즘을 제안한다. 본 발명의 강력한 구동 알고리즘은 1) 학습에 필요한 보유 (retention) 레벨이 증가하는 경우에도 학습 정확도가 감소하지 않으며, 2) 별도의 참조 셀 어레이없이 장치 자 체에서 참조 컨덕턴스를 쉽게 설정할 수 있다.먼저, 본 발명은 도 7b 도 7c에 표시된 측정 결과로부터 가중치 업데이트 특성과 보유 시간에 기반하여 SGD 알 고리즘으로 사용하여 신경망 훈련을 시뮬레이션 했다. 또한, 6T1C 구조의 장치 편차(device variations)는 장치 비대칭을 나타내는 매개 변수인 NL에 15%, Gmax와 Gmin에 7%, 에 6%, 보유 시간(retention time)에 15%, Gleak에서 15%를 적용했다. Gmax와 Gmin은 각각 장치의 최대 및 최소 컨덕턴스를 나타낸다. Gleak는 완전한 보 유 실패(retention failure) 후 휘발성 디바이스가 수렴하는 컨덕턴스를 나타낸다. 사이클 표준 편차의 경우, 쓰기 잡음(write noise)에 5% 표준 편차, 전류 합(current sum)에서 6%, 에 30%를 적용했다. SGD 알고리 즘 이외의 모든 시뮬레이션에 다음 변형이 적용되었다. 레이어 당 훈련주기 길이(Forward + Backward + Updat e)가 200ns이라고 가정하면 ~ 98.5% 정확도가 얻어졌다. 이 결과는 6T1C 시냅스 장치의 대칭 거동 및 우수한 보 유 특성에 기인한 것으로 보인다. 둘째, 본 발명은 종래 비대칭 아날로그 시냅스 디바이스를 위해 설계된 훈련 알고리즘(TTv1)을 사용하여 신경망 훈련을 수행했다. 종래 TTv1은 완전히 병렬로 작동하며 보조 장치(auxiliary device)에서 수행된 업데이트 정보 를 통해 코어 장치(core device)를 훈련한다. 이 경우 업데이트 특성은 우수하지만 누설(leakage)이 있는 6T1C 구조의 시냅스 장치를 보조 장치로 사용하고 보통의 비휘발성 메모리(non-volatile memory, NVM)를 코어 장치로 주기적으로 6T1C의 가중치를 NVM에 전달하여, 추론 과정에서 가중치 손실 없이 읽을 수 있도록 한다. 훈련을 위 한 TTv1을 이용하기 위해, 6T1C의 가중치 업데이트 측정 결과를 도 7a의 컨덕턴스-컨덕턴스 변화 형태로 변환하 였다. 이후 도 7a를 선형 회귀를 통해 모델링하여 하기 [수학식 6a] 및 [수학식 6b]에서 시뮬레이션 파라미터를 추출하였다. [수학식 6a]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "[수학식 6b]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "여기서, ΔGp, ΔGd는 각각 하나의 강화, 약화 펄스의 컨덕턴스 변화를 나타낸다. Gsym은 ΔGp=ΔGd를 만족하는 대 칭 컨덕턴스를 나타낸다. 도 7a에서 볼 수 있듯이 동일한 장치에서 측정 조건을 변경하여 선형 업데이트 결과 (NL 0.2)와 의도적으로 비대칭 업데이트 결과(NL 2.0)를 모두 추출할 수 있다. 종래에서는 TTv1를 사용 하여 최적의 학습 정확도를 달성하기 위해서는 코어와 보조 장치의 NL 조합이 중요하다는 것을 보여준다. 즉, 최적의 조합을 찾기 위해서는 목표 NL 값을 얻는 것이 중요하다. 그러나 일반적인 저항성 스위칭 장치에서는 컨 덕턴스 변조 메커니즘이 원자 수준의 랜덤 프로세스에 의존하기 때문에 목표 NL 값을 얻는 것이 불가능하다. 반 면 도 7a에서와 같이 본 발명의 6T1C 구조를 갖는 장치는 동일한 장치에서 측정 조건을 변경하여 NL=0.2~2 사이 의 넓은 범위를 가질 수 있으며 명확한 컨덕턴스 변조를 기반으로 목표 NL 값을 쉽게 얻을 수 있다. 도 7b는 6T1C 구조의 장치의 NL 범위 내에서 다양한 값을 반영하여 동일한 코어 장치 조건에서 TTv1을 적용한 결과를 보 여준다. 이전 종래 결과와 마찬가지로 6T1C 구조의 장치의 NL에 따라 학습 정확도가 변화하였다. 그러나 본 발 명의 장치는 측정 조건을 변경하여 쉽게 목표 NL 값을 얻을 수 있으므로 코어 장치의 종류에 관계없이 TTv1을 통해 최적의 학습 정확도를 달성할 수 있다. 또한, 6T1C 장치용 TTv1을 사용하면, 가중치 전달(weight transfer)도 수행할 수 있다. 6T1C 구조를 갖는 장치 가 좋은 보유 특성에도 불구하고 장기 보유 손실(long-term retention loss) 때문에 저장된 가중치를 NVM으로 전달하는 가중치 전달 과정이 필수적이다. 그러나 가중치 전송 기법은 교차점 요소에 하나씩 또는 행 별로 직렬 액세스가 필요하다. 순수한 가중치 전달은 시간 소모적인 직렬 작업과 반복적인 프로그래밍 및 가중치 확인을 포함하기 때문에 대규모 네트워크에 엄청난 오버헤드를 유발할 수 있다. 반면에 TTv1을 사용하면 학습 (learning)과 가중치 전달을 동시에 완전히 병렬로 수행할 수 있다. 6T1C 구조를 갖는 장치가 종래 TTv1에 적합한 장치임을 입증했지만 도 7b와 같이 학습에 필요한 시간이 증가함 에 따라 학습 정확도가 저하될 수 있다. 따라서 더 긴 보존(retention)이 필요한 경우에도 학습 정확도를 복구 하기 위해 본 발명의 구동 알고리즘을 이용한다. 먼저 종래 TTv1에서 보유 레벨에 따라 정확도가 저하되는 현상 을 분석하였다.휘발성 장치를 보조 장치로 사용하고 비휘발성 장치를 코어 장치로 사용하여 TTv1을 수행하는 경우 보조 장치 및 코어 장치의 가중치 업데이트 측면은 각각 [수학식 7a] 및 [수학식 7b]를 따른다. [수학식 7a]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "[수학식 7b]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "여기서, tupdate는 보조 배열의 레이어당 훈련 주기 길이를 나타내고 ε(t)는 업데이트 중에 발생하는 확률적 효 과를 나타낸다. ηA 및 ηC는 각각 보조 및 코어 장치의 학습 속도이다. E는 비용 함수를 나타낸다. 그러면 보조 장치와 코어 장치의 가중치가 정상 상태에 도달하려면 각각 [수학식 8a] 및 [수학식 8b]를 만족해 야 한다. NVM을 사용하는 TTv1에서는 Aref를 Asym으로 설정하여 충분히 학습한 후에 | A - Asym = 0 | 조건이 만족 할 수 있기 때문에, 글로벌 최소 값에 도달할 수 있다. 그러나 휘발성 장치를 보조 장치로 사용하는 경우 [수학 식 8a]의 우변이 0으로 수렴하지 않아 전역 최소값에 도달할 수 없다. 그 결과 훈련에 필요한 유지 시간이 증가 할수록 도 7b와 같이 학습 정확도가 감소하였다. [수학식 8a]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "[수학식 8b]"}
{"patent_id": "10-2023-0050478", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "그러나, 보유(retention)로 인한 정확도 감소는 Aref를 Aleak로 취하는 본원 발명의 장치의 구동 알고리즘 (modified TTv1)으로 해결할 수 있다. 6T1C 구조를 갖는 장치로 인해 Asym 및 Aleak의 예상 값은 Vcap=0일 때 동일 하며 6T1C 구조를 갖는 장치는 도 7a에 나타난 고도의 선형 가중치 업데이트가 가능하다. 따라서 [수학식 8a]의 우변의 첫 번째 항인 비대칭과 관련된 항의 영향은 거의 무시할 수 있으며 Aref를 Aleak로 설정하면 전역 최소값 (global minimum)에 도달할 것으로 예상할 수 있다. 전역 최소값에 도달하면, [수학식 8a]의 크기가 작아짐에 따라 비대칭 효과가 더욱 감소한다. 도 7c는 modified TTv1을 NL=0.2, 2.0에 적용한 신경망 훈련 결과 를 보여준다. NL=0.2인 고도의 선형 사례에서는 필요한 보유 시간이 증가하더라도 ~97.5%의 정확도를 달성할 수 있다. 또한 상기 [수학식 7a]에서 분석한 바와 같이 장치의 비대칭성이 커질수록 정확도가 떨어지는 것을 확인 하였다. 본 발명의 modified TTv1도 별도의 참조 셀 어레이 없이도 6T1C 구조 장치에서 자체적으로 빠르고 쉽게 참조 컨 덕턴스를 설정할 수 있다는 장점이 있다. 종래 TTv1에서 대칭 컨덕턴스로 정확하고 안정적인 참조 컨덕턴스를 설정하는 것이 중요하다고 알려져 있으나, 종래 대칭 컨덕턴스 설정 방법은 상대적으로 복잡하고 추가 배열이 필요하다. 그러나, 본 발명의 6T1C 구조를 갖는 장치는 한 번 더 읽기 작업을 수행하여 참조 컨덕턴스를 빠르고 정확하게 가져올 수 있다(도 7d 참조). 또한, 훈련 중에 장치의 특성이 변경되면 장치 자체의 대칭 컨덕턴스나 참조 장치의 컨덕턴스가 바뀌어 정확도가 떨어질 수 있다. 그러나 본 발명의 modified TTv1의 경우 훈련 중에 장치의 특성이 변경되어도 참조 컨덕턴스를 안정적으로 읽을 수 있다. 도 9a와 도 9b를 다시 참조하면, 다양한 보유(retention) 레벨에서 종래 TTv1과 modified TTv1을 적용한 학습 결과를 보여준다. [수학식 8a]에서 확인할 수 있듯이 장치의 보유 특성과 비대칭성은 학습 정확도에 복합적인 영향을 미치며, 최적의 학습에 필요한 알고리즘은 보유 레벨에 따라 다르게 나타날 수 있다. 그러나 6T1C 구조 를 갖는 장치를 사용하면 보유 레벨에 따라 유연하게 알고리즘을 선택할 수 있다. 만일 복잡한 데이터셋이나 신경망으로 인해 학습에 필요한 보유 특성이 증가하면, 고도의 선형 업데이트 조건을 사용하는 수정된 TTv1을 적 용하여 최적의 정확도를 얻을 수 있다. 만일 장치의 보유 특성이 학습에 충분하다면 코어 장치의 비대칭성에 적 합한 업데이트 조건을 사용하는 TTv1을 적용하여 최적의 정확도를 얻을 수 있다. 또한 6T1C 구조를 갖는 장치와 최적화된 알고리즘을 통해 캐패시터 기반 시냅스의 단점인 확장성도 개선할 수 있다. 캐패시터의 크기가 작아질수록 학습을 위한 충분한 보유 시간을 확보하기 어려워 캐패시터 기반 시냅스 장치의 확장성에 한계가 있다. 예를 들어, CNN(Convolutional Neural Network)의 경우 100fF의 커패시턴스/셀 의 큰 커패시턴스가 필요한 것으로 알려져 있다. 크기를 줄일 수 있어 장치 확장성을 높일 수 있다. 예를 들어, 현재 가장 낮은 전류 레벨의 IGZO TFT 및 캐패시터를 적용하면 10fF 커패시턴스/셀의 시냅스로 큰 입력 데이터 를 훈련함으로써 높은 학습 정확도를 달성할 수 있다. 따라서, 6T1C 구조를 갖는 장치는 대용량 입력 데이터와 복잡한 신경망에 적용할 수 있는 다재 다능하고 실용적인 시냅스 장치로 활용될 수 있다. 전술한 바와 같이, 캐패시터 기반 전하 저장 시냅스의 유지 문제를 해결하기 위해, 본 발명은 누설 전류가 낮은 IGZO TFT와 1개의 캐패시터와 6개의 트랜지스터들로 구성된 시냅스 장치를 제안한다. 단일 시냅스 장치와 5Х5 크로스바 어레이를 제조함으로써 본 발명의 디바이스가 선형 및 대칭 가중치 업데이트뿐만 아니라 충분한 유지 시간과 병렬 온-칩 훈련 동작을 제공할 수 있음을 입증하였다. 또한 드리프팅 참조(drifting reference) 및 장 기 유지 손실(long-term retention loss)과 같은 나머지 디바이스 비이상성을 보상하기 위해 효율적이면서도 현 실적인 훈련 알고리즘을 개발함으로써 디바이스-알고리즘의 공동 최적화의 중요성을 입증했다. 또한, 본 발명의 알고리즘은 별도의 참조 셀 어레이가 필요하지 않으며 훈련에 필요한 유지 시간이 증가하더라 도 ~97%의 높은 학습 정확도에 도달할 수 있어 더 작은 캐패시터로 더 작은 시냅스 어레이 크기를 가능하게 한 다. 종래 초저 누설(ultralow-leakage) 및 나노 크기의 IGZO TFT 및 캐패시터로 6T1C 장치의 크기를 더 줄일 수 있을 것으로 기대한다. M3D(Monolithic 3D) 통합 및 IGZO ALD(원자층 증착) 기반의 수직 채널 박막 트랜지스 터(vertical channel thin-film transistor, VTFT)를 통해 디바이스 풋프린트(device footprint)를 더욱 개선 할 수 있다. 따라서 본 발명의6T1C 장치가 뉴로모픽 컴퓨팅을 위한 실용적인 시냅스 장치라 할 수 있다. 본 명세서에서는 본 발명의 바람직한 실시예에 대하여 개시하였으며, 비록 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사용된 것이지, 본 발 명의 범위를 한정하고자 하는 것은 아니다. 여기에 개시된 실시예 외에도 본 발명의 기술적 사상에 바탕을 둔 다른 변형예들이 실시 가능하다는 것은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것 이다. 해당 기술 분야에서 통상의 지식을 가진 자라면, 도 1a 내지 도 12b를 참조하여 설명한 실시예에 따른 심 층 신경망 훈련용 전하 저장형 시냅스 장치 및 이의 구동 방법이, 본 발명의 기술적 사상이 벗어나지 않는 범위 내에서, 다양하게 치환, 변경 및 변형될 수 있음을 알 수 있을 것이다. 때문에 발명의 범위는 설명된 실시예에 의하여 정하여 질 것이 아니고 특허 청구범위에 기재된 기술적 사상에 의해 정하여져야 한다. 도면 도면1a 도면1b 도면2a 도면2b 도면3a 도면3b 도면3c 도면3d 도면3e 도면3f 도면3g 도면4a 도면4b 도면4c 도면4d 도면4e 도면5a 도면5b 도면5c 도면5d 도면5e 도면5f 도면6 도면7a 도면7b 도면7c 도면7d 도면7e 도면8a 도면8b 도면9a 도면9b 도면9c 도면10 도면11a 도면11b 도면12a 도면12b"}
{"patent_id": "10-2023-0050478", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1a는 본 발명의 실시 예에 따른 뇌신경 모사 연산 시스템의 개념도를 나타낸다. 도 1b는 도 1a에 도시된 시냅스 장치의 회로도를 나타낸다. 도 2a는 본 발명의 실시 예에 따른 IGZO(In-Ga-Zn-O) 박막 트랜지스터(Thin Film Transistor: TFT) 구조 및 제 조 방법을 설명하기 위한 도면이다. 도 2b는 본 발명의 실시 예에 따른 캐패시터 소자 구조 및 제조 방법을 설명하기 위한 도면이다. 도 3a는 본 발명의 시냅스 소자의 가중치를 증가시키기 위한 강화(potentiation) 업데이트 동작을 설명하 기 위한 동작 설명이다. 도 3b는 본 발명의 시냅스 소자의 가중치를 감소시키기 위한 약화(depression) 업데이트 동작을 설명하기 위한 동작 설명이다. 도 3c과 도 3d는 본 발명의 시냅스 소자의 읽기 동작을 설명하기 위한 동작 설명이다. 도 3e와 도 3f는 본 발명의 시냅스 소자의 참조 읽기 동작(reference read operation)을 설명하기 위한 동작 설명이다. 도 3g는 본 발명의 시냅스 소자의 리셋 동작(reset method)을 설명하기 위한 동작 설명이다. 도 4a 내지 도 4e는 본 발명의 선형 특성을 갖는 시냅스 소자(6T1C)를 이용한 코어 소자의 학습 방식을 설명하 기 위한 도면이다. 도 5a 내지 도 5f는 본 발명의 비선형 특성을 갖는 시냅스 소자(6T1C)를 이용한 코어 소자의 학습 방식을 설명 하기 위한 도면이다.도 6은 본 발명의 실시예에서 시냅스 장치 측정을 위한 인쇄회로기판(Printed Circuit Board, PCB) 이미지이다. 도 7a는 본 발명의 실시예에 따른 6T1C 구조를 갖는 단일 시냅스 장치의 가중치 업데이트 결과 그래프이다. 도 7b는 본 발명의 실시예에 따른 가중치 업데이트 곡선을 나타내는 도면이다. 도 7c는 본 발명의 실시예에 따른 6T1C 구조를 갖는 단일 시냅스 장치의 보유(retention) 특성 결과를 보여주는 그래프이다. 도 7d는 본 발명의 실시예에 따른 6T1C 구조를 갖는 단일 시냅스 장치의 사이클링 내구성 측정을 위한 초기 사 이클에서의 측정 결과를 보여주는 그래프이다. 도 7e는 본 발명의 실시예에 따른 6T1C 구조를 갖는 단일 시냅스 장치에 109개의 업데이트 펄스를 인가하여 얻 은 측정 결과 그래프이다. 도 8a는 본 발명의 실시예에 따른 선형 회귀 훈련을 위한 흐름도이고, 도 8b는 본 발명의 실시예에 따른 훈련 전반에 걸친 손실 및 오류의 평가 결과 그래프이다. 도 9a는 본 발명의 실시예에 따른 다양한 측정 조건에서 얻은 컨덕턴스-컨덕턴스 변화 형태의 결과 그래프이다. 도 9b는 본 발명의 실시예에 따른 6T1C 구조를 갖는 시냅스 장치의 NL 범위 내에서 다양한 값을 반영하여 종래 Tiki-taka algorithm를 적용한 보유 특성에 따른 장치의 정확도를 나타내는 그래프이다. 도 9c는 본 발명의 실시예에 따른 NL=0.2, NL=2.0에서 제안 발명을 적용한 보유 특성에 따른 장치의 정확도를 나타내는 그래프이다. 도 10은 본 발명의 실시예에 따른 참조 컨덕턴스(reference conductance)를 설정하는 방법을 설명하기 위한 도 면이다. 도 11a와 도 11b는 LENET5 모델 및 MLP 모델에서 본 발명과 종래 방법에 따른 선형 학습 결과를 비교하는 결과 그래프이다. 도 12a와 도 12b는 LENET5 모델에서 반전 및 리셋 동작이 수행되는 경우와 수행되지 않은 경우의 비선형 학습 결과를 비교하는 결과 그래프이다."}
