{"patent_id": "10-2021-7039976", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0016859", "출원번호": "10-2021-7039976", "발명의 명칭": "디지털 처리 시스템에서 매트릭스 작업을 스케줄링하기 위한 방법 및 장치", "출원인": "엑스페데라, 아이엔씨.", "발명자": "추앙 샹-체"}}
{"patent_id": "10-2021-7039976", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "프로세싱용 매트릭스 작동을 스케줄링하는 방법으로서, 상기 방법은,복수의 작업 큐를 생성하고, 상기 작업 큐 각각은 순서가 정해진 컴퓨터 작동 세트를 포함하는 단계;복수의 작업 큐에서 각각의 컴퓨터 작동에 대한 우선순위 값을 설정하는 단계;활성 큐의 수를 설정하고, 상기 활성 큐는, 동시에 활성할 수 있는 상기 복수의 작업 큐의 수를 결정하는 단계;선점을 사용할 것인지 여부를 설정하는 단계; 및상기 작업 큐로부터 상기 컴퓨터 작동을 실행하는 단계를 포함하는, 방법."}
{"patent_id": "10-2021-7039976", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공 지능은 컴퓨터 산업에서 점점 더 중요한 분야이다. 그러나, 인공 지능은 매우 계산 집약적인 분야여서, 인 공 지능 계산을 수행하는 것이 비용이 많이 들고, 시간 소모적이고, 에너지 소모적일 수 있다. 다행히도, 인공지 능 애플리케이션에 필요한 많은 계산이 병렬로 수행될 수 있어서, 특수 선형 대수 매트릭스 프로세서가 계산 성 능을 크게 증가시킬 수 있다. 그러나 선형 대수 매트릭스 프로세서를 사용해도, 복잡한 데이터 의존성으로 인해 성능이 제한될 수 있다. 적절한 조정이 없으면 선형 대수 매트릭스 프로세서가 유휴 상태로 끝나거나 데이터를 이동하는 데 많은 시간을 소비할 수 있다. 따라서, 본 출원은 선형 대수 매트릭스 프로세서를 효율적으로 스케줄 링하는 방법을 개시한다."}
{"patent_id": "10-2021-7039976", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 PCT 특허 출원은, 2019년 5월 7일에 출원되고 발명의 명칭이 \"Method and Apparatus for Scheduling Matrix Operations in Digital Processing Systems\"인 일련 번호 62/844,499의 이전 미국 가특허 출원의 이익을 주장 한다. 본 발명은 컴퓨터 프로세싱 분야에 관한 것이다. 특히, 본 발명은, 디지털 프로세싱 회로 내에서 매트릭스 작동 을 스케줄링하기 위한 디지털 회로 설계, 방법 및 제어 시스템을 제한 없이 개시한다."}
{"patent_id": "10-2021-7039976", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "초기 컴퓨터 시스템은 한 번에 하나의 명령어를 컴퓨터 명령어 처리하였으며, 원래는 한 번에 하나의 컴퓨터 프 로그램을 실행하는 것으로 제한되었다. 다수의 상이한 컴퓨터 프로그램 사이에서 컴퓨터 리소스를 공유하기 위 해, 멀티태스킹 컴퓨터 운영 체제가 개발되었다. 멀티태스킹 컴퓨터 운영 체제는, 실행 중인 컴퓨터 프로그램을 중단시키고, 그 컴퓨터 프로그램의 현재 상태를 저장한 다음, 다른 컴퓨터 프로그램의 작동을 시작하거나 재개 하여 컴퓨터 시스템이 한 번에 하나 이상의 컴퓨터 프로그램을 실행할 수 있도록 한다. 컴퓨터가 더욱 개발됨에 따라, 컴퓨터 시스템은 다수의 독립적인 프로세싱 코어를 가져서, 컴퓨터 시스템은 다 수의 명령어 세트를 병렬로 실행할 수 있었다. 컴퓨터 운영 시스템은, 다수의 독립적인 컴퓨터 프로그램을 독립 적으로 그리고 병렬로 실행시킴으로써, 이를 활용하였다. 또한, 병렬로 실행될 수 있는 코드의 상이한 섹션, 또 는 병렬로 복제되고 실행될 수 있는 코드의 단일 섹션을 포함하는 컴퓨터 프로그램을 개발하였다. 이는 멀티스 레딩으로 알려져 있다. 멀티태스킹 및 멀티스레딩은, 종래의 컴퓨터 시스템의 프로세싱 처리량을 최대화하기 위해, 다수의 프로세싱 코 어를 갖는 컴퓨터 시스템에서 사용된다. 이는 대량 병렬화를 갖는 프로세싱 작업을 취급하기 위해 매우 많은 양 의 독립적인 컴퓨터 프로세서 또는 컴퓨터 시스템을 사용할 수 있는, 대규모 병렬 프로세싱(MPP) 컴퓨터 시스템 으로 더욱 확장되었다. 최근 몇 년에 인공 지능(AI) 분야는 매우 중요한 분야로 성장했다. 인공 지능은 이미지 인식, 고성능 컴퓨팅 (HPC), 과학 컴퓨팅, 머신 러닝, 데이터 마이닝, 음성 인식 및 자율 주행 차량과 같은 다양하고 넓은 작업에 점 점 더 많이 사용되고 있다. 인공 지능 애플리케이션은, 선형 대수 매트릭스 연산에 매우 크게 의존하는 경향이 있다. 구체적으로, 매트릭스 작동은, 훈련 데이터의 세트로부터 학습한 다음에 그 학습을 새로운 입력 데이터에 이후 적용하는, 인공 신경망(ANN)을 구현하는 데 요구된다. 인공 지능(AI) 애플리케이션은, 전통적으로 종래의 컴퓨터 시스템으로 구현되어 왔다. 인공 지능 애플리케이션 에 상당한 양의 고유 병렬화가 있기 때문에, 멀티코어 프로세서 및 대규모 병렬 처리(MPP) 컴퓨터 시스템과 같 은 다양한 병렬 컴퓨터 시스템이 사용되어 왔다. 그러나, 인공 지능 애플리케이션은 선형 대수 매트릭스 연산에 특히 매우 의존한다. 전통적인 컴퓨터 CPU는 선형 대수 매트릭스 연산을 쉽게 취급할 수 있지만, 선형 대수 매트릭스 연산에 최적화되지는 않는다. 따라서, 효율을 개선하고 복잡한 선형 대수 매트릭스 연산을 수행하는 데 필요한 시간을 단축시키도록, 많은 전문화된 프로세서가 개발되어 인공 지능(AI) 내에서 사용되는 전문 선형 대 수 매트릭스 연산을 다루고 있다. 인공 지능 기반 애플리케이션의 사용이 증가함에 따라, 디지털 회로 설계자는, 최근에 인공 신경망을 구현하는 데 필요한 선형 대수 매트릭스 작동을 수행하기 위해 전문화된 매트릭스 프로세싱 회로를 개발하기 시작했다. 그래픽 프로세싱 유닛(GPU)은 3차원 그래픽 렌더링을 위한 선형 대수 작동을 수행하는 데 오랫동안 사용되어 왔 다. 따라서, 그래픽 프로세싱 유닛(GPU)은 인공 신경망에 대한 선형 대수 작동을 수행하도록 변형되었다. 변형된 그래픽 프로세싱 유닛(GPU)은, 인공 신경망에 사용되는 선형 대수 매트릭스 작동을 효율적이고 신속하게 수행하는 데 매우 효과적이었다. 그러나, 변형된 그래픽 프로세싱 유닛(GPU)은, 일반적으로 3차원 그래픽 렌더 링을 위한 선형 대수 작동을 수행하기 위해 원래 개발된 긴 파이프라인 아키텍처를 사용하였다. 따라서, 변형된 그래픽 프로세싱 유닛(GPU)은, 인공 신경망에 대한 선형 대수 작동의 대규모 배치 작동을 수행할 경우에 가장 잘 작용한다. 인공 신경망 내에서 사용되는 선형 대수 작동을 특이적으로 수행하기 위해, 보다 새로운 전문화 디지털 프로세 싱 회로가 개발되었다. 그러나, 이들 새로운 인공지능(AI) 프로세서는 여전히 다양한 이유로 활용도가 낮다. 예 를 들어, 메모리 제한, 데이터 의존성, 벡터 데이터의 이동, 가중치 매트릭스 재로드, 및 기타 작업은, 전문화 된 AI 프로세서의 처리량을 상당히 감소시킬 수 있다. 따라서, 적절한 조정이 없으면, 전문화된 AI 프로세서 회 로는 유휴 상태가 될 수 있다. 따라서, 연산 효율 전문화된 AI 프로세서를 최적화하기 위해, 신규 스케줄링 방 법을 개발하는 것이 바람직하다."}
{"patent_id": "10-2021-7039976", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "다음의 상세한 설명은 첨부 도면에 대한 참조를 포함하며, 이는 상세한 설명의 일부를 형성한다. 도면은, 예시 적인 구현예에 따른 예시를 나타낸다. 본원에서 \"실시예\"로도 지칭되는 이들 구현예는, 당업자가 본 발명을 실 시할 수 있도록 충분히 상세하게 설명된다. 본 발명을 실시하기 위해, 예시적인 구현예에서 특정 세부 사항을 요구하지 않음이 당업자에게는 명백할 것이다. 예를 들어, 일부 구현 예시가 특정하게 추상화된 매트릭스 프로 세서를 참조하여 개시되지만, 본 기술은 다른 구현예의 인공 지능 디지털 프로세싱 회로와 함께 사용될 수 있다. 예시적인 구현예는 조합될 수 있고, 다른 구현예가 사용될 수 있거나, 청구된 것의 범주를 벗어나지 않는 다면, 구조적, 논리적 및 전기적 변화가 이루어질 수 있다. 따라서, 다음의 상세한 설명은 제한적인 의미로 취 해지지 않으며, 그 범주는 첨부된 청구범위 및 이들의 균등물에 의해 정의된다. 본 문서에서, 특허 문헌에서 공통인 바와 같이, 용어 \"일\" 또는 \"하나\"는 하나 이상의 것을 포함하도록 사용된 다. 본 문서에서, 용어 \"또는\"은 비배타적인 것을 지칭하는 데 사용되거나, 달리 명시되지 않는 한, \"A 또는 B\"는 \"A이지만 B는 아님\", \"B이지만 A는 아님\", 및 \"A 및 B\"를 포함한다. 또한, 본 문헌에서 언급된 모든 간행물, 특허 및 특허 문서는, 참조에 의해 개별적으로 포함되지만, 그 전체가 참조로서 본원에 포함된다. 본 문서 와, 참조로 포함된 문서 간에 일관성이 없는 사용이 발생하는 경우, 포함된 참조(들)에서의 사용은 본 문서의 참조(들)에 대한 보충으로 간주되어야 한다; 조정 불가능한 불일치의 경우, 이 문서에서의 사용은 통제한다. 신경망 개요 대부분의 인공 지능(AI) 작업에서 핵심 기술 중 하나는 인공 신경망(ANN)의 사용이다. 인공 신경망은 원래 동물 뇌 내에 사용된 뉴런 세포의 생물학적 네트워크에 기초하여 설계되었다. 그러나, 인공 신경망(ANN) 내에서 사용 되는 기술은 수년 간의 연구에 걸쳐 개선되었다. 생물학적 뇌와 마찬가지로, 인공 신경망은 주변 세계의 입력 데이터 경험을 통해 학습한다. 인공 신경망의 경우, 훈련 데이터 세트가 인공 신경망에 제시되고 인공 신경망은 추론을 시도한다. 결과를 원하는 응답과 비교 하여 오류를 결정하고, 그 오류를 인공 신경망 내의 가중치 세트로서 조절하여 성능을 개선하는 데 사용한다. 이 기법은 알려진 지도 학습이다. 도 1a는 단일-계층 네 개-입력 인공 신경망(ANN) 100의 개념도를 나타낸다. 도 1a의 인공 신경망(ANN)을 참조하 면, 입력 데이터 값 101 내지 104는, 훈련 세션 동안에 훈련 데이터 벡터가 제공된 다음 인공 신경망이 나중에 추론을 위해 사용될 때 새로운 입력 데이터 벡터가 제공되는, 입력 데이터 벡터 105를 형성한다. 입력 데이터 벡터 105는 가중 매트릭스 120으로 프로세싱되어 출력 데이터 벡터 147(데이터 값 141 내지 144)을 생성한다. 상이하게 많은 유형의 데이터 프로세싱은 가중 매트릭스 120(예컨대, Hadamard 곱, Frobenius 내적, 매트릭스 합 등)를 사용하여 수행될 수 있지만, 본 문서는 잘 알려진 매트릭스 곱에 초점을 맞출 것이다. (이 문서에 설 명된 기술은 이들 다른 데이터 프로세싱 작동 어느 것과 함께 사용될 수 있음.) 출력 데이터 벡터 147(출력 데이터 값 141 내지 144)을 생성하기 위해 가중 매트릭스 120으로 입력 데이터 벡터 107(데이터 값 101 내지 104)를 프로세싱한 후, 출력 데이터 벡터 147은 출력 함수 170과 조합되어 인공 신경망 100에 대한 최종 출력 191을 생성할 수 있다. 출력 함수 170는 활성화 함수로서 지칭될 수 있다. 도 1a의 4개 입력 인공 신경망은 매우 작은 인공 신경망의 일례만을 예시한다는 것을 유의한다. 인공 신경망은 단지 네 개의 입력보다 훨씬 더 넓게 구성될 수 있다. 다수의 독립적인 인공 신경망이 병렬로 사용될 수 있고, 독립적인 인공 신경망의 출력이 조합될 수 있다. 인공 신경망은, 입력 데이터의 매우 복잡한 분석이 수행될 수 있도록, 많은 계층의 가중치 매트릭스를 포함할 수 있다. 예를 들어, 도 1b는 2-계층 인공 신경망을 나타내며, 여기서 입력 데이터(101 내지 104)는 제1 가중 매트릭스 121로 프로세싱되어 중간 출력 데이터(141 내지 144)를 생성한다. 다음으로, 중간 출력 데이터(141 내 지 144)가 제2 가중 매트릭스 122로 처리되어 출력 데이터(151 내지 154)를 생성한다. 출력 데이터(151 내지 154)는 최종 출력을 생성하기 위해 출력 함수 170에 의해 프로세싱될 수 있다. 대안적으로 (또는 이에 추가하여), 출력 데이터(151 내지 154)는 또한 매우 복잡한 계층적 인공 신경망이 생성될수 있도록, 추가적인 인공 신경망 계층(미도시)에 공급되는 중간 데이터로서 사용될 수 있다. 추상화된 매트릭스 프로세서"}
{"patent_id": "10-2021-7039976", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "배경기술에서 설명된 바와 같이, 인공지능 분야는 점점 더 인기를 끌고 있다. 따라서, 이제 인공 신경망 애플리 케이션에서 집중적으로 수행되는 선형 대수 매트릭스 작동을 수행하는 작업을 가속화하도록 설계된, 전용 인공 지능 디지털 프로세싱 회로가 현재 많이 존재한다. 도 2는 인공 신경망 애플리케이션에 대한 선형 대수 매트릭스 작동을 수행하도록 설계된 추상화된 매트릭스 프 로세서 201의 블록도를 나타낸다. 매트릭스 프로세서는 상이하게 많은 크기 및 상이하게 많은 방식으로 구현될 수 있다. 본 문서는 주로, 이러한 매트릭스 프로세서에 의한 프로세싱용 선형 대수 매트릭스 작동을 스케줄링하 는 것에 관한 것으로서, 매트릭스 프로세서 하드웨어에 대해 상세하게 논의하지 않을 것이다. 그러나, 매트릭스 프로세서가 수반할 수 있는 예시를 제공하기 위해, 추상화된 매트릭스 프로세서의 예시가 설명될 것이다. 도 2를 참조하면, 추상화된 매트릭스 프로세서 201은 하나 이상의 피연산자 버스에서 입력 데이터를 수신한다. 도 2의 특정 매트릭스 프로세서 구현예에서, 두 개의 피연산자 버스가 있다: 상단 221T로부터의 피연산자 버스 및 좌측으로부터 피연산자 버스 221L. 피연산자 버스에서 수신된 데이터는, 프로세싱 로직 267에 의해 직접 사 용될 수 있거나, 나중에 사용하기 위해 로컬 메모리 시스템 230에 저장될 수 있다. 수신된 데이터는, 전체 가중 치 매트릭스 및 입력 데이터 피연산자 벡터를 포함할 수 있다. 메모리 시스템 230은, 또한 프로세싱 로직 267에 밀접하게 결합된 레지스터 파일을 포함할 수있다.매트릭스 프로세서 201은, 또한 명령 버스 207에서 명령을 수신한다. 매트릭스 프로세서 201 내의 제어 시스템 205는 수신된 명령을 파싱하고 수신된 명령을 사용하여, 프로세싱 로직 267이 데이터를 프로세싱하는 데 사용되 어야 하는 방법을 결정한다. 매트릭스 프로세서 201이 원하는 선형 대수 매트릭스 작동을 수행하고 적절한 선형 대수 매트릭스 작동 결과를 출력하는 한, 프로세싱 로직 267은 많은 상이한 방식으로 구현될 수 있다. 예를 들 어, 프로세싱 로직 267은 단일-명령 다중-데이터(SIMD) 프로세서, 디지털 신호 프로세서(DSP), 종래의 중앙 처 리 유닛(CPU) 코어, 고도로 병렬화된 맞춤형 매트릭스 프로세서로 구현될 수 있거나, 원하는 선형 대수 매트릭 스 작동을 수행하는 임의의 다른 방식으로 구현될 수 있다. 매트릭스 프로세서 201은, 상이하게 많은 유형의 데이터 포맷 및 데이터 정밀도 레벨을 사용하여 작동하도록 설 계될 수 있다. 예를 들어, 매트릭스 프로세서 201은 정수, 16 비트 부동 소수점 수, 32비트 부동 소수점 수, 또 는 임의의 다른 데이터 포맷을 프로세싱할 수 있다. 상이하게 많은 매트릭스 작동이 추상화된 매트릭스 프로세서 201에서 구현될 수 있다. 포함될 수 있는 두 개의 잘 알려진 매트릭스 작동은, 매트릭스 내적 및 매트릭스 외적이다. 매트릭스 프로세서 201의 제어 시스템 205는, 하나 이상의 결과 버스 291 상에서 요청된 매트릭스 작동의 결과 를 출력하도록, 프로세싱 로직 267에 명령한다. 일부 구현예에서, 매트릭스 프로세서 205는, 감소 버스 295 상 에서 결과의 감소된 형태를 출력하는 감소 로직을 포함할 것이다. 피연산자 버스는, 전체 입력 데이터 벡터가 단일 작동 사이클 동안에 매트릭스 프로세서 201 내로 로딩될 수 있 도록, 넓은 병렬 버스일 수 있다. 유사하게, 신경망 가중치 매트릭스로부터의 전체 가중치 매트릭스 행은, 단일 작동 사이클 동안에 매트릭스 프로세서 201 내로 판독될 수 있다. 유사하게, 결과 버스 291은, 또한 전체 출력 데이터 벡터가 단일 작동 사이클 동안에 출력될 수 있도록, 넓은 병렬 버스이다. 메모리 시스템 230은 일반적으로 추상화된 매트릭스 프로세서 201의 매우 중요한 구성 요소이다. 성능을 최적화 하기 위해, 매트릭스 프로세서 201의 메모리 시스템 230은 넓고 깊게 구성될 수 있다. 메모리 시스템 230은 매 트릭스 프로세서의 중요한 리소스이며, 작동을 최적화하기 위해 신중하게 사용되어야 한다. 따라서, 스케줄링 시스템은, 오버플로우 없이 효율적으로 사용되는 것을 보장하기 위해 매트릭스 프로세서 내의 메모리 시스템 230의 한계를 신중하게 고려해야 한다. 메모리 시스템 230은, 단일 작동 사이클 동안에 전체 데이터 벡터가 메모리 시스템 230에 쓰이거나 메모리 시스 템으로부터 판독될 수 있다는 점에서, 넓다. 예를 들어, 16 x 16 요소 매트릭스를 취급하는 매트릭스 프로세서 에서, 각각의 요소는 16 비트 부동 소수점 값이고, 메모리 시스템은, 각각 16 비트 데이터 값을 포함한 16개의 요소 데이터 벡터 전체가 단일 작동 사이클 동안에 메모리 시스템 230으로부터 판독될 수 있도록, 256 비트 값 을 판독할 수 있다. 하나의 특정 매트릭스 프로세서에서, 메모리 시스템 230은 다수의 상이한 가중치 매트릭스 세트를 저장할 만큼 충분히 크게 구성된다는 점에서 심층적이다. 이러한 방식으로, 매트릭스 프로세서 201은 다수의 상이한 인공 신 경망 계층에서 매트릭스 작동을 수행하는 데 사용될 수 있다. 예를 들어, 요구되는 입력 데이터 벡터가 아직 이 용 가능하지 않기 때문에 매트릭스 프로세서 201이 하나의 특정 신경망 계층에 대한 작동을 수행할 수 없는 경 우, 그 매트릭스 프로세서는 대신에 다른 신경망 계층 또는 다른 신경망에 대한 매트릭스 작동을 수행하는 데 사용될 수 있다. 심층 메모리 230은 매트릭스 프로세서 201을 매우 효율적으로 사용할 수 있게 하는데, 그 이유 는 매트릭스 프로세싱을 위해 가장 시간 소모적인 (그리고 에너지 소모적인) 작업 중 하나인, 가중치 매트릭스 데이터를 로딩할 필요 없이, 다수의 상이한 신경망에 대해 요청된 매트릭스 작동의 일정한 스트림을 취급할 수 있기 때문이다. 다수의 가중치 매트릭스를 저장하는 것 이외에, 메모리 230은 입력 데이터 벡터, 출력 데이터 벡터, 오류 벡터 등과 같이 필요할 수 있는 다른 정보를 저장하는 데 사용될 수 있다. 정방향 계산 작동으로부터의 중간 결과 데 이터 벡터는, 메모리 시스템 230에 저장될 수 있고, 그 후에 관련 역전파 작동을 수행할 경우에 이후 액세스될 수 있다. 저장될 수 있는 다른 매우 중요한 유형의 데이터는, 매트릭스 가중치 경사이다. 매트릭스 가중치 경사 는, 가중치 매트릭스를 업데이트하기 위해 주기적으로 사용될 수 있는 가중치 매트릭스에 대한 조절 매트릭스를 포함한다. 매트릭스 프로세서 어레이 도 2에 나타낸 추상화된 매트릭스 프로세서 201은, 간단한 선형 매트릭스 작동을 매우 신속하게 수행하기 위해 단독으로 사용될 수 있다. 예를 들어, 매트릭스 프로세서 201은 도 1a에 나타낸 매우 작은 인공 신경망 100을 구현하는 데 사용될 수 있다. 이는, 두 인공 신경망 계층의 필수 매트릭스 작동을 수행하기 위해 직렬로 사용함 으로써, 도 1b에 나타낸 작은 2-계층 인공 신경망을 구현하는 데에 또한 사용될 수 있다. 그러나, 대부분의 인공 신경망은 도 1a 및 도 1b에 나타낸 매우 작은 인공 신경망 예시보다 더 많은 입력 및 출 력을 취급해야 한다. 따라서, 보다 넓은 인공 신경망 및 다층 인공 신경망을 프로세싱하기 위해, 상이하게 많은 매트릭스 프로세서의 연산 능력을 함께 조합하는 것이 바람직하다. 이러한 방식으로, 유용한 인공 지능 작업을 수행하는 데 사용되는 훨씬 더 큰 다중 계층 인공 신경망을 매우 효율적으로 취급할 수 있다. 도 3a는 넓은 다중 계층 인공 신경망을 구현하기 위해 조정된 방식으로 다수의 매트릭스 프로세서 회로를 사용 하는, 아키텍처의 블록 다이어그램을 나타낸다. 도 3a에서, 각각의 개별 매트릭스 프로세서는, 매트릭스 프로세 서에 대해 \"MP\"로 표지된다. 도 3a에 나타낸 바와 같이, 매트릭스 프로세서는 그리드 어레이 포맷으로 배열된다. 결과 데이터 벡터를 수신하고 이들 결과 데이터 벡터를 추가로 프로세싱하는 입력 데이터 및 벡터 프 로세싱 유닛(VPU)을 제공하는 버퍼에 모든 매트릭스 프로세서를 결합하는 버스 배선과 조합 로직 399가 매트릭 스 프로세서 어레이의 개별 매트릭스 프로세서 사이에 있다. 버스 배선 및 조합 로직 399는, 상이한 목표를 달 성하기 위해 상이한 방식으로 구현될 수 있다. 일 구현예에서 매트릭스 프로세서의 어레이에 데이터 벡터를 제공하기 위해, 좌측의 버퍼 1 및 상단의 버퍼 2는, 어레이 버스 배선 399에서 모든 개별 매트릭스 프로세서의 피연산자 버스에 결합된다. 이는 도 3b에 나타 낸 바와 같이, 피연산자 버스를 버퍼 1에 결합하고 피연산자 버스를 버퍼 2에 결합함으로써, 달성될 수있다. 이 러한 방식으로, 버퍼 1 또는 버퍼 2로부터의 데이터 벡터가, 어레이 내의 매트릭스 프로세서에 로딩될 수 있다. 데이터 벡터는, 가중치 매트릭스 행, 입력 데이터 벡터, 또는 임의의 다른 필수 데이터를 포함할 수 있다. 버스 가 여러 개 있기 때문에, 피연산자 로딩 작동은 병렬로 수행될 수 있다. 유사하게, 어레이 내의 모든 매트릭스 프로세서의 결과 버스는, 버스 배선 및 조합 로직 399를 사용하여 우측의 벡터 프로세싱 유닛 1(VPU1) 및 어레이의 하단의 벡터 프로세싱 유닛 2(VPU2)에 결합된다. 이는 도 3b에 나타낸 바와 같이 우측의 벡터 프로세싱 유닛 1(VPU1)에 결과 버스를 결합하고 하단의 벡터 프로세싱 유닛 2(VPU2)에 결과 버스를 결합함으로써 달성될 수 있다. 벡터 프로세싱 유닛은, 결과 데이터 벡터를 저장하기 위한 저장 장 치, 및 수신된 결과 데이터 벡터에 대한 다양한 벡터 프로세싱 작동을 수행하기 위한 프로세싱 로직을 포함한다. 예를 들어, 벡터 프로세싱 유닛(VPU)은, 다수의 상이한 매트릭스 프로세서로부터의 부분적인 결과 데 이터 벡터를, 완전한 단일 출력 데이터 벡터 결과로 조합할 수 있다. 어레이 내의 모든 개별 매트릭스 프로세서는. 개별 명령 버스에서 명령을 수신한다(도 3b에는 미도시). 이러한 방식으로, 어레이 내의 각각의 개별 매트릭스 프로세서는, 개별적으로 제어될 수 있다. 예를 들어, 개별 매트릭 스 프로세서는, 피연산자 버스에서 데이터가 사용 가능한 시간과 수행할 작동이 무엇인지에 대해 정보를 받을 수 있다. 조정된 방식으로 어레이의 각각의 개별 매트릭스 프로세서를 신중하게 제어함으로써, 매트릭스 프로세 서 어레이는, 인공 지능 애플리케이션에 필요한 매트릭스 작동을 효율적으로 프로세싱하기 위한 매우 강력한 시 스템이 된다. 인공 신경망 처리 인공 신경망(ANN)은, 일반적으로 세 단계 프로세스에서 훈련을 수행한다: 정방향 계산 추론, 역전파 손실 오류 감지, 및 가중치 매트릭스 업데이트. 도 4a 내지 도 4c는 이들 공통 세 개의 인공 신경망 프로세싱 단계를 나타 낸다. 도 4a는 4 계층 인공 신경망(ANN) 422를 통한 정방향 계산 추론 작동 451을 개념적으로 나타낸다. 샘플 데이터 벡터 배치 411은 입력 데이터 벡터 421을 4 계층 ANN 422에 제공하는 데 사용된다. 데이터 벡터는, ANN 422의 네 개의 계층을 통해 프로세싱되어 각각의 계층 이후에 중간 결과를 생성한다. 훈련 중에, 이들 중간 결과는 나 중에 사용하기 위해 저장될 필요가 있지만, 추론 전용 작동 중에는 폐기될 수 있다. 4 계층 ANN 422의 말단에서, 최종 결과는, 추론으로서 사용될 수 있는 최종 출력 값을 생성하기 위해 활성화 함수 420과 조합될 수 있다. 지도 훈련 모드에 있을 경우에, 그 최종 출력 값은 손실 값 485를 계산하기 위해 비교 480에서 목표 값 481과 비교된다. 이러한 손실 값은, 원하는 결과와 4 계층 ANN 422에 의해 이루어진 추론 사이의 차이를 나타낸다. 지도 훈련 중에, 4 계층 ANN 422의 학습을 개선하는 데 사용되는 두 개 이상의 계산 세트가 있다: 역전파 및 가 중치 업데이트. 도 4b는 역전파 작동 453을 개념적으로 나타낸다. 손실 값 485를 사용하여, 4 계층 ANN 422의모든 계층에 대한 에러 벡터를 계산하기 위해 저장된 중간 결과를 사용하여 계층을 통해 연속적으로 다시 돌아 간다. 그 다음, 이 오차 벡터를 저장하거나 즉시 사용하여 다음에 설명되는 바와 같이 가중치 업데이트를 수행 한다. 역전파 후, 가중치 업데이트 작동 457은 개념도 도 4c에 나타낸 바와 같이 수행될 수 있다. 가중치 업데이트 작 동은, 정방향 계산 작동 451으로부터의 중간 데이터, 및 역전파 453 동안에 계산된 오차 벡터를 사용하여, 4-계 층 ANN 457에서의 가중치 매트릭스를 업데이트하는데 사용될 가중치 경사를 계산한다. 경사 값은 조합되고 저장 될 수 있으며, 가중치 매트릭스 내의 실제 가중치를 업데이트하기 위해 주기적으로만 사용될 수 있음을 유의한 다. 인공 신경망 배치 프로세싱 많은 양의 훈련 데이터를 효율적으로 프로세싱하기 위해, 훈련 샘플 데이터 벡터는, 인공 신경망(ANN)을 통해 프로세싱하기 위한 배치로 구성된다. 예를 들어, 도 5a는 도 4a 내지 도 4c의 동일한 4 계층 인공 신경망(ANN) 522를 나타낸다. 도 5b는, 샘플 데이터 벡터 527(제1 열)의 추론 작동을 위해 수행되어야 하는 작동 배치 560을 나타낸다. 작동 배치 560은 특정 데이터 의존성이 있다. 추론 작동만을 위해, 데이터 의존성은 비교적 간단하다: 각각의 데이터 샘플은 4 계층의 ANN 522의 모든 계층을 통해 프로세싱되어야 한다. 이러한 의존성은, 네 개의 계층 모 두를 통해 데이터 샘플의 연속적 작동을 연결하는 화살표에 의해 예시된다. 상이한 샘플 데이터 벡터 각각은 서 로 독립적이어서, 상이한 샘플 벡터 사이에 데이터 의존성이 없다(따라서 이들 사이에 화살표가 없음). 인공 신경망 프로세싱 단계의 세 가지 세트(정방향 계산, 역전파 및 경사 업데이트) 모두에 대한 전체 데이터 의존성은 훨씬 더 복잡하다. 도 6a 및 6b는 세 개의 공통 인공 신경망 프로세싱 단계 모두에 대한 전체 데이터 의존성을 나타낸다. 도 6a를 참조하면, 샘플 데이터 602의 입력 벡터가 좌측 상부에 입력된다. 그 다음, 이러한 샘플 데이터를 사용해 정방향 프로세싱(FP) 단계 611, 612, 613, 및 614를 수행하여 출력 671을 생성한다. 이들 각 단계는 나중에 사용할 수 있도록 저장된 중간 데이터를 생성하는 것에 유의한다. 출력 값 671은 목표 값 691과 비교되어, 추론이 원하는 목표 값으로부터 얼마나 멀리 있는지를 나타내는 손실 값 672를 계산한다. 그 다음, 이러한 손실 값 672가 일련의 역전파 작동에 사용된다. 구체적으로, 손실 값 672 는 계층 4에 대한 역전파(BP) 654에서 계층 4 정방향 계산으로부터의 중간 데이터와 조합된다. 역전파(BP) 654 로부터의 출력은, 계층 3에 대한 역전파(BP) 작동 653에서 계층 3 정방향 계산 613으로부터의 중간 데이터와 조 합된다. 이렇게 하면 계층 1 역전파 작동 651로 되돌아간다. 그 다음, 손실 값 672 및 연속하는 역전파 작동(654, 653, 및 652)으로부터의 출력이, 경사 업데이트(GU) 작동 을 위해 사용될 수 있다. 경사 업데이트(GU) 작동은 계층에 대한 정방향 계산 작동과 역전파 작동 모두로부터 계산된 데이터를 필요로 한다. 도 6a의 데이터 의존성 다이어그램에서, 프로세싱 작동은, 그 프로세싱 작동이 수행될 수 있기 전에, 화살표로 부터 그 프로세싱 작동으로 가는 모든 데이터를 이용 가능한 것을 요구한다. 따라서, 모든 정방향 계산 작동 611, 612, 613, 및 614는 임의의 다른 작동 이전에 그 순서로 수행되어야 한다. 하지만 그 이후에는 일부 작동 을 다른 순서로 수행할 수 있다. 예를 들어, 역전파 작동 654이 수행된 후에, 다음 작동은 경사 업데이트 633 또는 다음 역전파 작동 653일 수 있다. 도 6b는 데이터 의존성의 대안적인 예시를 나타낸다. 구체적으로, 도 6b는 4-계층 인공 신경망을 통해 프로세싱 될 세 개의 데이터 샘플을 위한 세 개의 조립된 작업 큐를 나타낸다. 도 6b에서, 각각의 작업 큐는, 순서대로 수행될 필요가 있는 컴퓨터 작동 세트를 포함한다. 또한, 각각의 컴퓨터 작동으로 들어가는 화살표로부터의 데 이터는, 컴퓨터 작동이 수행될 수 있기 전에, 이용 가능할 필요가 있다. 단순 배치 스케줄링 하나의 인공 신경망 모델, 하나의 훈련 벡터 배치, 및 상기 훈련 벡터 배치를 프로세싱하기 위한 하나의 매트릭 스 프로세서를 갖는 비교적 간단한 환경에서; 하나의 훈련 벡터 배치의 프로세싱을 스케줄링하는 것이 비교적 단순해 보일 수 있다. 그러나, 이러한 단순한 환경에서도, 작업은 보이는 것만큼 단순하지 않다. 도 5b를 다시 참조하면, 도 5a의 4 계층 인공 신경망으로 프로세싱될 샘플 데이터 벡터 527의 배치가 있다. 데 이터 벡터는 도 6a 및 6b의 데이터 의존성이 적절하게 취급되는 한, 임의의 순서로 네 개의 계층 522를 통해 프로세싱될 수 있다. 도 7a 및 7b는, 도 5b의 다양한 샘플 데이터 벡터 527이 도 5a의 4 계층 인공 신경망을 통해 프로세싱되도록 스 케줄링될 수 있는 방법에 대한 두 가지 상이한 극단을 나타낸다. 도 7a는 \"계층 우선\" 스케줄링 시스템을 나타 내고, 도 7b는 \"샘플 우선\" 스케줄링 시스템을 나타낸다. 도 7a의 \"계층 우선\" 스케줄링 시스템을 참조하면, 모든 데이터 벡터 샘플은 먼저 도 5a의 인공 신경망의 제1 계층을 통해 프로세싱된다. 그 제1 계층 프로세싱의 결과는 그 다음 먼저 도 5a의 인공 신경망의 제2 계층을 통 해 프로세싱된다. 모든 데이터 벡터가 도 5a의 전체 인공 신경망을 통해 정방향 계산 프로세싱될 때까지 계속된 다. 그 다음, 시스템은 도 5a의 모든 계층 인공 신경망을 통해 다시 하향으로 역전파 작동을 순차적으로 수행할 수 있다. 계층 우선 스케줄링 시스템은, 추론 작동(정방향 계산 작동)에 대해 상대적으로 낮은 지연 시간을 얻기 위한 하 나의 방법을 제공할 수 있고, 그런 다음에 역전파 및 가중치 업데이트가 이후 수행된다. 그러나, 계층 우선 스 케줄링은, 장시간 동안 저장되어야 하는 많은 양의 중간 결과를 생성할 것이다. 이는 일반적으로 오프-칩 저장 장치가 필요하므로, 오프-칩 메모리 대역폭이 요구된다. 칩에서 데이터를 꺼내 옮기는 데 소요되는 시간은, 활 용도를 낮추고 지연 시간을 증가시킨다. 또한, 모든 데이터 이동은 프로세싱의 전력 효율을 감소시킬 것이고 이 러한 에너지는 모든 데이터를 칩으로 이동시키고 칩에서 꺼내는 데 소비되어야 한다. 도 7b는, 다른 극단적인 스케줄링인, \"샘플 우선\" 스케줄링 시스템을 나타낸다. 샘플 우선 스케줄링 시스템에서, 각각의 개별 샘플은 도 5a의 모든 계층의 인공 신경망을 통해 프로세싱된다. 예를 들어, 제1 샘플 데이터 벡터는, 추론 결과를 얻기 위해 도 5a의 인공 신경망의 4 계층 모두를 통해 정방향 계산 프로세싱된다. 이것이 훈련인 경우, 시스템은 그 다음 인공 신경망의 모든 층을 통해 다시 하향으로 역전파를 수행할 수 있고, 그 다음 가중치 경사 업데이트를 계산할 수 있다. (이것이 단지 추론을 위한 것인 경우, 시스템은 제1 데이터 벡터의 정방향 계산을 완료한 후 제2 데이터 벡터를 프로세싱하기 위해 즉시 이동할 수 있음.) 하나의 데이터 벡터로부터의 중간 결과만이 저장될 필요가 있기 때문에, 프로세싱으로부터의 결과는 로컬로 저 장될 수 있다. 이는 프로세싱의 전력 소비를 감소시킨다. 그러나, 샘플 우선 스케줄링 시스템은 몇몇 단점이 있 다. 예를 들어, 마지막 데이터 샘플을 취급할 때까지 상당한 지연이 있다. 또한, 데이터 의존성, 하드웨어 지연 시간 및 데이터플로우 지연으로 인해 활용도가 낮다. 도 7a의 \"계층 우선\" 스케줄링 시스템과 도 7b의 \"샘플 우선\" 스케줄링 시스템의 두 극단 사이에는, 상이하게 많은 스케줄링 시스템이 있다. 예를 들어, 세 개의 데이터 샘플 벡터의 작은 서브-배치는, 전체 4 계층 인공 신 경망을 통해 시간 프로세싱될 수 있다. 대안적으로, 샘플 데이터 벡터 배치는, 메모리가 채워지기 시작할 때까 지 정방향 계산 작동을 통해 순차적으로 프로세싱될 수 있고, 그 다음 역전파 및 가중치 업데이트가 메모리로부 터 제거되기 시작할 수 있다. 따라서, 단일 인공 네트워크를 통해, 샘플 데이터 벡터 단일 배치를 스케줄링하는 작업은 상당히 복잡할 수 있다. 도 5a, 5b, 7a 및 7b를 참조하여 설명된 데이터 벡터 스케줄링은, 프로세싱이 디스패치될 수 있는 다수의 상이 한 매트릭스 프로세서를 가짐으로써 훨씬 더 복잡해질 수 있다. 인공 신경망의 사본은, 데이터 샘플이 병렬로 처리될 수 있도록 다수의 매트릭스 프로세서에 제공될 수 있다. 상이한 매트릭스 프로세서에 저장될 중간 데이 터는 데이터 의존성을 복잡하게 만들어, 중간 데이터를 필요로 하는 작동이 중간 데이터를 필요로 하는 매트릭 스 프로세서로 디스패치될 수만 있게 하도록 하는 것에 유의한다. 복잡한 환경에서의 스케줄링 도 5a 및 도 5b에 도시된 바와 같이 인공 신경망에 의해 실행되어야 하는 단일 인공 신경망(ANN) 및 단일 작동 배치 560의 단순한 환경은, 많은 인공 신경망 프로세서에 대한 현재의 상황을 나타낸다. 그러나, 프로세싱될 다 수의 상이한 인공 신경망과 다수의 상이한 데이터 배치 세트를 처리할 경우에, 훨씬 더 복잡한 상황이 발생할 수 있다. 도 8은 본 개시의 교시가 사용될 수 있는 다양하게 상이한 실행 패러다임을 설명하는 개념도를 나타낸다. 상이 한 패러다임은 제1 축 890을 따라 하나의 인공 신경망 또는 다수의 상이한 인공 신경망만을 취급할 수 있다. 또 한, 상이한 실행 패러다임은, 데이터 샘플의 단일 배치만을 동시에 취급하거나 제2 축 880을 따라 데이터 샘플 의 다수의 상이한 배치를 동시에 취급할 수 있다. 각각의 상이한 실행 패러다임을 개별적으로 설명할 것이다. 제1 실행 패러다임은, 현재의 패러다임 810 좌측 상부 사분면이다. 이러한 패러다임은, 단일 인공 신경망 모델 로 프로세싱된 단일 데이터 샘플 배치(단일 데이터 = SD)를 취급하는 단일 인공 신경망 모델(단일 모델 = SM)의 현재 패러다임이다. 이는, 본질적으로 도 5a의 단일 인공 신경망 및 도 5b의 데이터 샘플 527의 단일 배치를 참 조하여 전술한 예시이다. 이것은, 기존 그래픽 프로세서 유닛(GPU) 및 텐서 프로세싱 유닛(TPU) 기반 인공 지능 프로세서가 작동하는 방식이다. 도 7a 및 도 7b를 참조하여 설명된 바와 같이, 도 8의 가장 간단한 경우에 대한 데이터 샘플 스케줄링조차 상당히 복잡할 수 있다. 단일 모델 단일 데이터(SMSD) 패러다임 810 이외에, 도 8은 본 개시의 스케줄링 기술이 또한 취급하기 위해 사용될 수 있는 세 가지 다른 작동 패러다임을 나타낸다. 우측 상부 사분면에, 단일 인공 신경망(ANN) 모델 그러나 데이터 샘플의 다수의 상이한 배치를 취급하는 단일 모델 다중 데이터(SMMD) 패러다임 850가 있다. 하나의 인공 신경망(ANN) 모델만을 다루는 것으로써, ANN 가중치 매트릭스 한 세트만이 취급될 필요가 있고, 따라서 메모리 요구 사항을 최소화한다. 그리고, 다수의 상이한 데 이터 샘플 배치를 다루는 것으로써, 데이터 의존성이 실행 속도를 늦추는 경우가 거의 없고 프로세싱할 데이터 없는 시간이 거의 없도록 실행될 수 있는, 다수의 상이한 데이터 샘플 세트가 있다. 이러한 방식으로, 이러한 SMMD 실행 패러다임은 높은 활용도를 달성할 수 있다. 데이터 샘플 한 세트에 대해 일부 데이터를 칩 으로 이동 하거나 칩으로부터 이동시킬 필요가 있는 경우, 데이터 샘플의 다른 세트 상에서 연산이 수행될 수 있다. 좌측 하부 사분면은, 다중 모델 단일 데이터(MMSD) 작동 패러다임 870을 포함한다. MMSD 작동 패러다임 870은 다수의 상이한 인공 신경망(ANN) 모델을 취급할 수 있지만, 데이터 샘플의 단일 배치로 실행을 제한한다. 데이 터 샘플의 단일 배치만을 프로세싱함으로써, 시스템은, 임의의 다른 데이터 샘플로부터의 중단 없이 가능하면 빠르게 데이터 샘플의 단일 배치에 대한 연산을 신속하게 완료함으로써, 낮은 지연 시간 응답 시간을 달성할 수 있다. MMSD 작동 패러다임 870은 가능하면 빠르게 결과를 반환하기 위해 실시간 추론을 수행하는 데 적합하다. 마지막으로, 우측 하부 사분면은 다중 모델 다중 데이터(MMMD) 작동 패러다임 860을 지정한다. MMMD 작동 패러 다임 860은, 다수의 상이한 인공 신경망(ANN) 모델 및 다수의 상이한 데이터 샘플 배치를 동시에 모두 취급한다. 이러한 MMMD 작동 패러다임 860은 매우 많은 양의 인공 지능 프로세싱 작업을 취급해야 하는 데이터 센터에서 마주칠 수 있다. 이러한 MMMD 실행 패러다임 860은 지연 시간이 낮은 작업을 취급할 수 있어야 하므로 중요한 데이터 작동을 플래깅하기 위한 프로비저닝이 이루어져야 한다. 동시에 취급될 필요가 있는 다수의 상이 한 ANN 모델 및 다수의 상이한 데이터 샘플 배치 모두로 인해, 리소스 할당 및 작업 스케줄링을 처리하는 데에 는 엄청난 수의 상이한 방법이 있다. 인공 신경망 프로세싱 개요 인공 신경망(ANN) 모델에 필요한 프로세싱을 가장 효율적으로 수행하기 위해, 본 발명의 시스템은 각각의 ANN 모델을 분석하고, 각각의 모델에 대한 리소스를 할당하고, 각각의 모델에 대한 작업 큐의 스케줄링을 생성한 다 음, 매트릭스 프로세서 상에서 작업 스케줄을 실행한다. 본 섹션은 도 9를 참조하여 전체 프로세싱 시스템의 개 요를 제공한다. 도 9의 상단에 있는 소스 정보는 기존의 신경망 프레임워크 910이다. 신경망 프레임워크 910은, TensorFlow, Keras, PyTorch, Caffe2, Deeplearning4j, 및 인공 신경망을 구축하기 위한 다른 적절한 프레임워크와 같이, 여러 가지 상이한 신경망 프레임워크 중 하나일 수 있다. 다양한 신경망 프레임워크 910은, 개발자로 하여금 심 층 인공 신경망 모델을 신속하고 쉽게 구축할 수 있게 한다. 신경망 프레임워크 910은, 미리 구축되고 최적화된 구성 요소의 집합을 사용하여 인공 신경망 모델을 정의하기 위해, 명확하고 간결한 방법을 개발자에게 제공한다. 상이한 개발자가 사용하기로 선택할 수 있는 몇몇 상이한 신경망 프레임워크 910이 있기 때문에, 이들 몇몇 신 경망 프레임워크 910으로부터의 정보는, 보다 통합된 중간 신경망 표현 920으로 프로세싱될 수있다. 공통 사용 되는 두 가지 중간 표현은, 개방형 신경망 교환(ONNX) 및 가속 선형 대수(XLA)를 포함한다. 이러한 방식으로, 상이하게 많은 신경망 프레임워크 910을 보다 쉽게 지원할 수 있다. 중간 신경망 표현 920은 지향성 비순환 그래프(DAG) 형태의 연산 데이터플로우 그래프를 포함한다. 중간 신경망 표현 920의 연산 데이터플로우 그래프는, 특정 인공 신경망 모델에 대해 수행될 컴퓨터 작동 모두를 설명한다. 그 다음, 중간 신경망 표현 920이 신경망 컴퓨터 시스템에 제공될 수 있고, 이는 그 다음 인공 신경망 모델을 실행할 것이다. 본 개시의 시스템에서, 중간 신경망 표현 920은 신경망 작업 구성 시스템 940에 제공된다. 신경망 작업 구성 시 스템 940은 중간 신경망 표현을 분석하고, 그 다음 신경망 표현을 파티셔닝하고, 리소스를 할당하고, 성능 분석 을 수행하여 신경망 표현이 하드웨어에 할당되는 방법을 결정한다. 이 할당 시스템은 다음 섹션에서 더 자세히설명될 것이다. 마지막으로, 리소스 할당 후, 실행을 위해 신경망 하드웨어 950에 신경망이 제공된다. 신경망 하드웨어 950의 주요 구성 요소는, 하드웨어 동적 스케줄러 951이다. 하드웨어 동적 스케줄러 951은, 인공 신경망을 실행하는 데 사용될 모든 실행 하드웨어를 신중하게 제어하는 역할을 한다. 구체적으로, 하드웨어 동적 스케줄러 951은 연산을 수행하는 매트릭스 프로세서 엔진 957, 다양한 유닛 사이의 데이터 인터페이스 958, 그리고 버퍼 및 메 모리 시스템 959를 제어한다. 하드웨어 동적 스케줄러 951은 여러 기능을 수행한다. 하드웨어 동적 스케줄러 951은 데이터 의존성을 해결하고 프로세싱용 작업 큐를 생성한다. 하드웨어 동적 스케줄러 951은 메모리 관리를 동적으로 취급해, 각 작업이 메 모리 리소스를 필요로 하고 메모리 오버플로우가 없는 것을 보장한다. 그리고 하드웨어 동적 스케줄러 951은 작 업 우선순위와 동기화를 취급한다. 신경망 파티셔닝 및 리소스 할당 도 9를 다시 참조하면, 신경망 작업 구성 시스템 940은, 하드웨어에 대한 신경망 표현을 파티셔닝하기 위해 중 간 신경망 표시를 분석하고 필요한 리소스를 할당한다. 이는, 할당이 매우 다양한 방식으로 수행될 수 있고 최 적의 할당을 찾기가 어렵기 때문에, 잘 하기 매우 어려운 작업이다. 잠재적 할당 공간을 탐색하기 위해 반복적 접근법을 사용한다. 도 10은 신경망 작업 구성 시스템이 작동하는 방법을 설명한 흐름도를 나타낸다. 도 10의 상단에서, 중간 신경망 표현 1005이 입력 데이터로서 제공된다. 제1 프로세싱 단계는 신경망 파티셔닝 단계 1010이다. 파티셔닝의 목표는, 매트릭스 프로세서 엔진의 활용도를 최대화하기 위해 이용 가능한 매트릭스 프로세서 엔진에 걸쳐 컴퓨터 작업을 균등하게 분산시키는 것이다. 따라서, 신경망 파티셔닝 단계 1010은 연산 데이터플로우를 분석한 다음, 이용 가능한 매트릭스 프로세서 엔진 사이에서 연산 데이터플로우의 상이한 연산 단계를 균등하게 파티셔닝하려고 시도한다. 신경망 파티셔닝 후, 다음 단계는 리소스 할당 단계 1020이다. 매트릭스 프로세서 엔진 이외에, 신경망 하드웨 어는 메모리 시스템, 동기화 플래그, 메모리 대역폭, 오프-칩 인터페이스 대역폭 등과 같은 다른 리소스를 갖는 다. 리소스 할당 단계 1020은 이들 리소스를 연산 데이터플로우의 다양하게 상이한 계산 단계에 할당한다. 리소 스 할당 단계 1020 이후, 연산 데이터플로우의 제안된 파티셔닝 및 리소스 할당이 생성되었다. 다음으로, 성능 분석 단계 1030은 제안된 파티셔닝 및 리소스 할당을 주의 깊게 분석한다. 구체적으로, 연산 데 이터플로우는 제안된 파티셔닝 및 리소스 할당으로 엔드-투-엔드로 분석되어 성능의 추정치를 결정한다. 연산 데이터플로우의 각 연산 단계의 성능 추정치가 생성된다. 그 다음, 1040 단계에서 성능 추정치를 검사한다. 추정된 성능이 충분하지 않은 것으로 간주되는 경우, 시스템 은 힌트 생성 단계 1050으로 진행한다. 힌트 생성 단계 1050은 발견법을 사용하여, 신경망 파티셔닝 단계 1010 및 리소스 할당 단계 1020으로부터의 출력을 변경시키는 힌트 세트를, 이들 단계를 통해 다음 실행 후 생성한다. 예를 들어, 다양한 연산 단계의 하위 추정치를 조사하고, 성능 추정치가 낮은 것에는 성능을 개선하 기 위한 추가 리소스를 할당한다. 예를 들어, 매트릭스 프로세서 엔진 간에 균형이 불량하거나, 메모리 리소스 가 부족할 경우, 이들 불충분성은 리소스의 파티셔닝 및 할당을 변경하는 데 사용된다. 그 다음, 시스템은 신경 망 파티셔닝 단계 1010 및 리소스 할당 단계 1020을 반복하여 신규 제안된 파티셔닝 및 리소스 할당을 생성할 수 있다. 시스템은, 양호한 파티셔닝 및 리소스 할당을 결정하기 위해, 단계 1010, 1020, 1030, 1040, 및 1050의 반복을 수행할 수 있다. 단계 1040을 다시 참조하면, 충분한 파티션 및 리소스 할당이 생성된 후에, 시스템은 실행을 위해 파티션 및 리소스 할당 계획을 신경망 하드웨어로 출력 진행한다. 신경망 작업 스케줄링 정책 목표 신경망 프로세싱 작업을 스케줄링하는 것은 여러 가지 상이한 목표를 포함한다. 문제를 복잡하게 만들기 위해, 이들 상이한 목표는 종종 서로 충돌한다. 상충하는 목표는, 다른 작업의 긴급성을 조사하거나 활용도를 극대화 하여 해결될 수 있다. 이 섹션에서는 다양한 스케줄링 정책 목표를 설명하고, 이후 섹션에서는 스케줄링 시스템 이 이들 목표를 달성하는 방법에 대해 설명한다. 우선순위 - 첫 번째 목표는 단순히 프로세싱 작업 우선순위를 준수하는 것이다. 우선순위가 더 높은 프로세싱 작업은 우선순위가 더 낮은 작업 이전에 일반적으로 프로세싱되어야 한다. 본 문서에 개시된 시스템에서, 우선 순위 번호는, 가장 낮게 할당된 우선순위 번호가 실제로 가장 높은 우선순위 작업이 되도록 반전된다. 따라서,동적 스케줄러는 가장 낮은 우선순위를 선택한다. 우선순위 동점은, 일반적으로 라운드-로빈 또는 선입선출 (FIFO) 시스템으로 깨진다. 이전 샘플이 우선순위가 더 높다 - 일반적으로, 이전 샘플 벡터는 이후 샘플 데이터 벡터보다 더 높은 우선순위 가 부여된다. 이전 샘플에 더 높은 우선순위를 제공함으로써, 이는 이전 작업의 프로세싱을 완료시키고, 이에 따라 가능한 빠르게 메모리와 같은 리소스를 확보시킨다. 또한, 컴퓨터 작업이 여러 개의 매트릭스 프로세서 엔 진 사이에서 분할될 경우에, 이후 연산을 취급하기 위해 할당된 매트릭스 프로세싱 엔진은, 작업이 이용 가능할 때까지 유휴 상태로 둘 수 있다. 따라서, 이전 샘플을 우선순위화 하면, 이후 연산을 취급하기 위해 할당된 매 트릭스 프로세싱 엔진이 가능한 빠르게 작업되도록 보장할 것이다. 작업을 보다 빠르게 생성 - 데이터 의존성은, 실행을 위해 선택될 수 있는 컴퓨터 작동의 가능한 양을 제한한다. 예를 들어, 이전 정방향 연산이 먼저 수행될 때까지, 역전파 및 경사 업데이트 연산을 수행할 수 없 다. 따라서, 일반적으로 정방향 계산 작동은, 역전파 작동보다 높은 우선순위로 할당되어야 한다. 또한 역전파 작동은, 일반적으로 경사 업데이트 작동보다 우선순위가 높게 부여된다. 이 정책 목표는 위에서 \"이전 샘플이 우선순위가 더 높다\"는 정책과 어느 정도 모순되는데, 그 이유는 경사 업데이트 작동을 완료하는 것이 일부 메 모리 리소스를 확보할 것이지만 반면에 정방향 계산 작동 또는 역전파 작동이 작업을 더 빠르게 생성할 것이라 는 것을 유의한다. 정책 목표로 무엇을 선택하는가 하는 것은, 메모리 리소스가 낮은지 또는 활용도가 가장 중 요한지 여부에 대한 현재 상황에 따라 달라질 수 있다. 파이프라인 중요 경로에 없는 작업 연기 - 경사 업데이트는 배치를 완료하거나 신규 작업을 생성하는 중요 경로 에 없다. 따라서, 경사 업데이트는 가장 낮은 우선순위가 부여될 수 있다. 다시, 이는 다른 목표와 충돌할 수 있어서, 경사 업데이트를 연기하는 것이 메모리 압력을 생성하고 이에 의해 경사 업데이트의 우선순위를 높일 수 있도록 한다. 컨텍스트를 보다 중요한 작동으로 전환 - 수신된 프로세싱 작동에는 중요한 등급이 할당되거나 지연 시간이 짧 아야 할 수 있다. 따라서, 컨텍스트 전환은 리소스를 보다 중요한 작업으로 전환하는 데 사용될 수 있다. 동적 메모리 관리 - 이전에 언급한 바와 같이, 메모리 리소스는 제한되므로 스케줄링 시스템은 메모리 리소스를 주의 깊게 모니터링하여 메모리가 부족하지 않도록 보장해야 한다. 메모리는, 이후에 계산하기 위한 중간 결과 로 채워질 수 있다. 메모리 제약을 취급하기 위해, 시스템은 매트릭스 프로세서 엔진 칩에서 더 큰 메모리 시스 템으로 데이터를 이동시킬 수 있지만, 이는, 메모리 대역폭을 요구하고 연산을 느리게 한다. 작업 간 공정성 보장 - 위의 정책은 지연 시간을 줄이고 활용도를 극대화하기 위해 사용된다. 그러나 이들 정책 을 엄격하게 따르면 소정의 작업이 무시될 수 있다. 따라서, 스케줄링 정책은, 효율성을 극대화하기 위해 어떤 작업도 무시되지 않도록, 공정성을 보장해야 한다. 스케줄링 절차 도 9를 다시 참조하면, 하드웨어 동적 스케줄러 951은 이전 섹션의 정책 목표를 취하고, 이들 정책 목표를 사용 하여 프로세싱 작동 스케줄링을 안내한다. 하드웨어 동적 스케줄러 951은, 순서가 지정된 컴퓨터 작동으로 채워 진 작업 큐 세트를 생성하고 컴퓨터 작동에 우선순위를 할당한 다음에 큐 순서와 우선순위를 사용하여 실행을 위한 컴퓨터 작동을 디스패칭함으로써, 달성한다. 이러한 스케줄링 방법은 도 11을 참조하여 설명될 것이다. 도 11을 참조하면, 특정 신경망 모델에 대한 데이터 샘플 배치 1105는, 스케줄링 시스템에 대한 입력이다. 제1 및 가장 큰 작업은, 1110 단계에서 데이터 샘플 배치 1105에 대한 일련의 작업 큐를 먼저 생성하는 것이다. 작업 큐 각각은, 특정 작업을 완료하기 위해 수행될 필요가 있는 컴퓨터 작동의 순서가 정해진 세트이다. 다음 목록은, 작업 큐에 배치될 수 있지만 추가적인 컴퓨터 작동이 추가될 수 있고 신호 플래그가 작업 큐에 배치될 수도 있는, 공통적으로 사용되는 컴퓨터 작동 세트를 설명한다. 표 1 - 컴퓨터 작업 정방향 계산(FP) - 계층에 대한 정방향 계산를 계산. 역전파(BP) - 계층에 대한 역전파를 계산. 손실(Loss) - 추론 손실 계산 경사 업데이트(GU) - 데이터 샘플에 대한 경사 업데이트를 계산. 재연산(RC) - 떨어진 FP 계산을 재연산 가중치 업데이트(WU) - 가중치 매트릭스를 경사로 업데이트 데이터 병렬 병합(PM) - 병렬 경사 업데이트를 조합 작업 큐가 생성되는 방법을 설명하기 위해 일부 예시가 본원에 제공된다. 제1 예시의 경우, L1과 L2로 지칭되는 2 계층을 갖는 도 1b의 작은 2 계층 인공 신경망(ANN)을 고려한다. S1 및 S2로 지칭되는 두 개의 데이터 샘플의 작은 배치가, 전체 훈련 프로세싱(정방향 계산, 역전파, 및 경사 업데이트)를 위해 도 1b의 2-계층 ANN에 제공 될 수 있다. 이전 표의 컴퓨터 작동 약어를 사용, 두 개의 데이터 샘플(S1 및 S2)에 대한 두 개의 작업 큐(WQ1 및 WQ2) 표 2 - 작업 큐 예시 [WQ1] [WQ2] S1 L1 FP S2 L1 FP S1 L2 FP S2 L2 FP S1 L2 BP S2 L2 BP S1 L2 GU S2 L2 GU S1 L1 BP S2 L1 BP S1 L1 GU S2 L1 GU 도 5a의 4-계층 인공 신경망(ANN)에 대해 제2 작업 큐 예시가 제공될 수있다. 도 5a의 4-계층 ANN을 통한 전체 훈련 프로세싱용 세 개의 데이터 샘플의 배치를 고려한다. 이들 세 개의 데이터 샘플에 대한 세 개의 작업 큐가 도 6b에 나타나 있다. 순서가 지정된 작업 큐는, 컴퓨터 작동의 데이터 의존성을 준수하는 것을 보장하는 보조하기 위해 사용된다. 이 러한 방식으로, 스케줄링 시스템은, 작업 큐의 상단에 액세스할 경우에 필수 데이터가 이용 가능하도록 보장할 수 있다. 도 11 을 다시 참조하면, 1110 단계에서 작업 큐를 생성한 후, 실행을 위해 작업 큐를 디스패칭하기 전에 몇 가 지 단계가 더 있다. 다음 단계는 1120 단계에서 허용되는 활성 큐의 수를 결정하는 것이다. 대형 샘플 배치의 경우, 실행을 위한 많은 작업 큐 세트가 있을 것이다. 많은 수의 큐를 병렬로 실행할 수 있도 록 허용하면, 컴퓨터 리소스의 활용도를 높게 제공한다. 그러나, 많은 수의 작업 큐가 병렬로 실행됨에 따라, 메모리 리소스가 제한될 수 있으며 작업 큐 완료에 대한 지연 시간이 더 길어질 수 있다. 따라서, 스케줄링 시 스템은 동시에 능동적으로 프로세싱될 수 있는 작업 큐의 수를 결정할 것이다. 예를 들어, 메모리 소비를 줄이기 위해, 많은 수의 작업 큐가 생성되었음에도 오직 두 개의 활성 큐만 허용될 수 있다. 그 다음, 시스템은 두 개의 작업 큐에서 작동을 시작하지만 다른 모든 작업 큐는 대기할 것이다. 구체 적으로, 다른 작업 큐는, 이전에 디스패치된 작업 큐 중 하나가 실행을 시작하기 전에 작동을 완료될 때까지 기 다릴 필요가 있다. 스케줄링 시스템은, 활성 큐의 수를 결정하는 데 도움을 주기 위해 도 9의 신경망 작업 구성 단계 940으로부터 메모리 할당 정보를 사용할 수 있다. 도 11을 다시 참조하면, 단계 1120에서 허용된 활성 큐의 수를 결정한 후, 시스템은, 단계 1110에서 생성된 작 업 큐 내의 각 컴퓨터 작동에 대한 우선순위 수준을 결정하는 단계 1130으로 진행한다. 시스템은, 이전 섹션의 신경망 스케줄링 정책을 사용하여 각 컴퓨터 작동에 대한 우선순위 수준을 결정한다. 모든 유입 데이터 배치에는 중요도 척도가 표시되어 있다. 예를 들어, 스마트 장치로부터의 실시간 자연 언어 프로세싱 작업이, 지연 시간을 최소화하기 위해, 매우 중요한 척도 값으로 수신될 수 있다. 디지털 이미지를 조 사하고 디지털 이미지에 태그를 자동으로 추가하려고 시도하는 것과 같은 다른 작업은, 실행할 다른 중요한 작 업이 없을 경우에 작업이 실행되도록 낮은 중요도 척도 값으로 수신될 수 있다. 이 모든 정보는, 적절한 프로세 싱을 보장하는 방식으로 우선순위를 적절히 할당하는 데 사용된다. 마지막으로, 1140 단계에서, 스케줄링 시스템은 프로세싱 중에 선점이 허용되는지 여부를 결정할 것이다. 선점 은, 다른 프로세싱 작업이 실행을 시작할 수 있도록, 프로세싱 작업을 일시 중지시킬 수 있다. 스케줄링 사례 1 - 최소 지연 시간 도 12a는 제1 스케줄링 작동 예시용 4-계층 인공 신경망을 나타낸다. 제1 스케줄링 예시는, 추론 작동으로 프로 세싱될 필요가 있는 네 개의 데이터 샘플을 포함한 샘플 데이터 배치 1211을 갖는다. 이는, 각각의 데이터 샘플 이 네 개의 네트워크 계층 모두를 통해 정방향 계산(FP) 컴퓨터 작동으로 프로세싱될 필요가 있음을 의미한다. 또한, 이 특정 예시에서 네 개의 데이터 샘플 배치는 최소 지연 시간으로 프로세싱되어야 한다. 이러한 특정 작 업은, \"서버 0\"으로 지칭될 단일 선형 대수 매트릭스 프로세서로 수행되는 중이다. 첫 번째 단계는 프로세싱될 각 데이터 샘플에 대해 하나씩 네 개의 작업 큐 세트를 생성하는 것이다. 도 12b는 이전 단락에서 설명된 프로세싱 작업을 수행하기 위한 네 개의 작업 큐 세트(1251, 1252, 1253, 및 1254)를 나 타낸다. 이는 단지 추론 작동이기 때문에, 네 개의 작업 큐 각각이 단지 네 개의 정방향 계산(FP) 컴퓨터 작동 을, 도 12a의 4-계층 ANN의 각 계층에 대해 하나씩 요구한다. 이 경우, 목적은 지연 시간을 최소화해서, 네 개의 데이터 샘플이 가장 신속하게 모든 데이터 샘플을 완성하는 방식으로 우선순위화되도록 한다. 따라서, 컴퓨터 작동은 데이터 샘플과 동일한 순서로 우선순위화 된다. 따라 서, 작업 큐 1251 내의 제1 데이터 샘플에 대한 모든 컴퓨터 작동은 가장 높은 우선순위, 우선순위 0이 부여된 다(본 개시는 가장 낮은 우선순위 번호에 가장 높은 우선순위를 부여함을 상기하기 바람). 작업 큐 1252 내의 제2 데이터 샘플에 대한 모든 컴퓨터 작동에는 우선순위 1이 주어지고, 작업 큐 1253 및 1254에 대해서도 마찬 가지로 순위가 부여된다. 이러한 우선순위를 통해, 네 가지 컴퓨터 작동은 동일한 순서로 가능한 빠르게 완료되 어야 한다. 또한, 이 경우에, 시스템은 선점을 활성할 수 있다. 선점을 활성함으로써, 시스템은 우선순위가 낮은 작업으로 부터 컨텍스트 전환에 더 높은 우선순위 작업을 허용한다. 이는, 처리량이 저하될 수 있지만, 이 작업에 대한 지연 시간을 최소화한다. 스케줄링 사례 2 - 처리량 최대화 도 13a는 제2 스케줄링 작동 예시를 위한 동일한 4-계층 인공 신경망을 나타낸다. 다시, 본 스케줄링 예시는, 추론 작동으로만 프로세싱될 필요가 있는 네 개의 데이터 샘플을 포함한 샘플 데이터 배치 1311을 갖는다. 그러 나, 이 두 번째 예시에서, 네 개의 데이터 샘플 배치는 최대 처리량으로 프로세싱되어야 한다. 다시 한 번, 첫 번째 단계는 네 개의 작업 큐 세트(1351, 1352, 1353, 및 1354)를, 도 13b에 나타낸 바와 같이 프로세싱될 각각의 데이터 샘플에 대해 하나씩 생성한다. 다시 말해, 이는 단지 추론 작동이기 때문에, 네 개의 작업 큐 각각은 정방향 계산 FP 컴퓨터 작동만을 필요로 한다. 이 두 번째 예시에서, 목적은 처리량을 최대화하여 컴퓨터 작동이 최대 활용도를 보장하는 방식으로 우선순위화 되도록 하는 것이다. 따라서, 컴퓨터 작동은 최대 병렬 연산을 달성하는 방식으로 우선순위화 된다. 따라서, 제 1 네트워크 계층에 대한 모든 컴퓨터 작동은, 가장 높은 우선순위, 우선순위 0이 부여된다. 제2 네트워크 계층 에 대한 모든 컴퓨터 작동은 우선순위 1이 부여되고, 네트워크 계층 3 및 4에 대해서도 마찬가지로 순위가 부여 된다. 따라서, 네 개의 작업 큐 모두(1351, 1352, 1353, 및 1354)는 네 개의 컴퓨터 작동을 0, 1, 2 및 3으로 순서가 정해진다. 이러한 우선순위화 체계를 통해, 컴퓨터 작동은 가능한 많이 병렬화된 연산으로 완료되어야 한다. 실행할 준비가 된 컴퓨터 작동을 갖는 작업 큐 사이에 동점이 있을 때마다 라운드 로빈 시스템은, 어느 큐가 컴퓨터 작동을 디스패치할 것인지 선택하는 데 사용될 수 있음을 유의해야 한다. 메모리 제약 또는 기타 리소스 제약이 있는 경우에, 스케줄링 시스템은 리소스 사용량을 줄이기 위해 활성 큐 수를 제한할 수 있음을 유의한다. 이 예시에서, 시스템은 처리량을 최대화하기 위해 선점을 비활성할 수 있다. 선점은, 데이터를 주위로 이동하는 시간을 낭비할 수 있고, 이에 따라 시스템의 처리량을 감소시킨다. 스케줄링 사례 3 - 다중 서버 추론 도 14a는 추론 작동으로 프로세싱될 필요가 있는 네 개의 데이터 샘플을 포함한 샘플 데이터 배치 1411을 이용 한 제3 스케줄링 작동 예시용 4-계층 인공 신경망을 나타낸다. 이러한 제3 스케줄링 예시에서, 4-계층 인공 신 경망은 두 개의 절반부로 분할되었고, 두 개의 절반부는 각각 상이한 선형 대수 매트릭스 프로세서에 의해 취급 된다. 구체적으로, 도 14a에 나타낸 바와 같이, 처음 두 계층은 \"서버 0\" 1431로 표지된 제1 매트릭스 프로세서 에 의해 취급될 것이고, 두 번째 두 계층은 \"서버 1\" 1432로 표지된 제2 매트릭스 프로세서에 의해 취급될 것이다. 서버 0 1431 및 서버 1 1432에 걸쳐 균등하게 ANN을 분할하는 것은, 최대 활용이 달성될 수 있도록 균형을이룬다. 또한, 이 특정 예시에서 네 개의 데이터 샘플 배치는 최소 지연 시간으로 프로세싱되어야 한다. 다시 한번, 첫 번째 단계는 여덟 개의 작업 큐 세트를 생성하는 것이며, 프로세싱될 네 개의 데이터 샘플 각각 에 대해 서버 0 1431에 의해 하나씩 프로세싱되고, 프로세싱될 네 개의 데이터 샘플 각각에 대해 서버 1 1432에 의해 하나씩 프로세싱된다. 도 14b는 열 1451, 1452, 1453 및 1454에 대해 서로 상단에 두 개의 작업 큐가 있는 여덟 개의 작업 큐를 나타낸다. 이는 단지 추론 작동이기 때문에, 여덟 개의 작업 큐는 각 작업 큐에서 두 개의 정방향 계산(FP) 컴퓨터 작동만을 필요로 한다. 서버 0 1431과 서버 1 1432 사이에 분할된 ANN에 대한 네 개의 정방향 계산(FP) 컴퓨터 작동이 있다. 구체적으로, ANN의 계층 0 및 1에 대한 두 개의 정방향 계산(FP) 컴퓨터 작동은 각 큐에 대해 서버 0 1431에 할당되고, ANN의 계층 2 및 3에 대한 두 개의 정방향 계산(FP) 컴퓨터 작동 은 각 작업 큐에 대해 서버 1 1432에 할당된다. 이 세 번째 예시에 있어, 목적은 두 개의 상이한 서버인 서버 0 1431 및 서버 1 1432의 활용도를 최대화하는 것 이다. 이를 달성하기 위해, 서버 0 1431은, 출력 데이터가 서버 1 1432로 전달되어 서버 1 1432가 프로세싱을 시작할 수 있도록, 그 정방향 계산(FP) 컴퓨터 작동을 완료하려고 시도해야 한다. 따라서, 서버 0 1431에서의 FP 컴퓨터 작동은, 모든 데이터 샘플을 가장 신속하게 완성하는 방식으로 우선순위화 된다. 이것이 이 예제에서 가장 중요한데, 그 이유는 제2 서버(서버 1 1432)가 서버 0 1431의 하위 두 계층으로부터 출력 데이터를 수신할 때까지 유휴 상태가 되기 때문이다. 이러한 목표를 달성하기 위해, 컴퓨터 작동은 제1 예시에 제시된 바와 같은 데이터 샘플의 동일한 순서로 우선 순위화 된다. 따라서, 열 1451의 두 개의 작업 큐에서 제1 데이터 샘플에 대한 모든 컴퓨터 작동에는 우선순위 0이 할당되고; 열 1452의 두 개의 작업 큐에서 제2 데이터 샘플에 대한 모든 컴퓨터 작동에는 우선순위 1이 할 당되고, 열 1453 및 열 1454의 작업 큐에 대해서도 마찬가지로 순위가 부여된다. 이러한 우선순위화를 이용해, 컴퓨터 작동은 동일한 순서로 가능한 빠르게 완료되어야 한다. 다시, 제1 사례에서와 같이, 시스템은 이 사례에서 선점을 활성할 수 있다. 선점을 활성함으로써, 시스템은 우 선순위가 낮은 작업으로부터 컨텍스트 전환에 더 높은 우선순위 작업을 허용한다. 또한, 선점을 활성함으로써, (이전 서버의 데이터에 종속된) 나중 단계 서버는, 나중 단계가 데이터를 기다릴 때에 다른 작업을 프로세싱하 여 더 높은 활용도를 가질 수 있게 한다. 스케줄링 사례 4 - 단일 서버 훈련 - 메모리 최소화 도 15a는 제4 스케줄링 작동 예시용 4-계층 인공 신경망을 나타낸다. 제4 스케줄링 예시는, 전체 훈련 사이클로 프로세싱될 필요가 있는 네 개의 데이터 샘플을 포함한 샘플 데이터 배치 1511을 갖는다. 이는, 배치 1511의 모 든 네 개의 데이터 샘플이, 정방향 계산(FP) 1551, 역전파 1553, 및 경사 업데이트 1557 컴퓨터 작동으로 프로 세싱될 필요가 있음을 의미한다. 또한, 이러한 특정 스케줄링 예시에서, 네 개의 데이터 샘플은, 메모리와 같은 리소스의 사용을 최소화하면서 프로세싱되어야 한다. 첫 번째 단계는, 프로세싱될 네 개의 데이터 샘플 각각에 대한 작업 큐를 생성하는 것이다. 도 15b는 각각의 데 이터 샘플에 대해 하나씩, 네 개의 작업 큐 세트 (1551, 1552, 1553, 및 1554)를 나타낸다. 이는 전체 훈련 작 동이기 때문에, 네 개의 작업 큐 각각은 네 개의 정방향 계산(FP), 네 개의 역전파(BP) 및 네 개의 경사 업데이 트(GU) 컴퓨터 작동을 필요로 한다. 또한, 오름차순으로 정방향 계산(FP) 작동의 네 계층 다음에 내림차순으로 역방향 전파(BP) 및 경사 업데이트(GU) 컴퓨터 작동의 4-계층의 순서로 컴퓨터 작동이 배치된다. 작업 큐에서 이러한 컴퓨터 작동 순서는, 데이터 의존성을 유지하고 스케줄링 효율성을 최적화한다. 예를 들어, 각 계층에 대한 대응하는 역전파(BP) 작동 직후에 각 경사 업데이트(GU) 작동을 배치함으로써, 각 계층에 사용 되는 메모리 리소스는, 경사 업데이트(GU) 작동이 완료되는 즉시 확보될 수 있다. 컴퓨터 작동 순서 외에도, 지연 시간을 최소화하기 위해 각 작업 큐에 대한 우선순위를 설정해야 한다. 구체적 으로, 우선순위는 지연 시간을 최소화하기 위해 모든 데이터 샘플을 가장 빠르게 완료하는 방식으로 설정되어야 한다. 따라서, 컴퓨터 작동은 데이터 샘플과 동일한 순서로 우선순위화 된다. 따라서, 도 15b에 나타낸 바와 같 이, 제1 작업 큐 1551에 대한 모든 컴퓨터 작동은 최고 우선순위 0이 부여된다. 작업 큐 1552 내의 제2 데이터 샘플에 대한 모든 컴퓨터 작동에는 우선 순위 1이 주어지고, 작업 큐 1553 및 1554에 대해서도 마찬가지로 순위 가 부여가 된다. 이 우선순위화 체계를 이용해, 네 개의 작업 큐는, 각 작업 큐가 완료된 후에 해당 작업 큐에 사용된 모든 리소스가 확보되도록, 데이터 샘플과 일반적으로 동일한 순서로 가능한 빠르게 완료되어야 한다. 이 경우에 대한 리소스 사용을 더욱 최소화하기 위해, 시스템은 선점을 활성한다. 선점을 활성함으로써, 시스템 은, 우선순위가 낮은 작업으로부터 우선순위가 높은 작업으로 컨텍스트 전환하여, 우선순위가 높은 작업을 가능한 빠르게 완료한다. 우선순위가 높은 작업 큐가 완료되면 해당 작업 큐에 사용되는 모든 리소스를 확보할 수 있다. 스케줄링 사례 5 - 단일 서버 훈련 - 처리량 최대화 도 16a는 제5 스케줄링 작동 예시용 4-계층 인공 신경망(ANN)을 나타낸다. 이 제5 스케줄링 예시는, 인공 신경 망(ANN)을 통해 전체 훈련 사이클로 프로세싱될 필요가 있는 네 개의 데이터 샘플을 포함한 샘플 데이터 배치 1611을 갖는다. 또한, 이러한 제5 스케줄링 예시 사례에서, 네 개의 데이터 샘플은 ANN 프로세싱 시스템의 처리 량을 최대화하면서 프로세싱되어야 한다. 첫 번째 단계는, 프로세싱될 배치 1611의 네 개의 데이터 샘플 각각에 대한 작업 큐를 생성하는 것이다. 도 16b 는 각각의 데이터 샘플에 대해 하나씩, 네 개의 작업 큐 세트 (1651, 1652, 1653, 및 1654)를 나타낸다. 이는 이전 예시에서처럼 전체 훈련 작동이기 때문에, 네 개의 작업 큐 각각은 네 개의 정방향 계산(FP) 컴퓨터 작동, 네 개의 역전파(BP) 컴퓨터 작동 및 네 개의 경사 업데이트(GU) 컴퓨터 작동을 필요로 한다. 이 제5 스케줄링 예시에서, 목표는 프로세싱 시스템의 처리량을 최대화하는 것이다. 따라서, 스케줄링 시스템은 가장 많은 양의 병렬 프로세싱을 가능하게 하는 방식으로 작업 큐에서 컴퓨터 작동의 우선순위를 정해야 한다. 따라서, 도 13a 및 도 13b를 참조하여 설명된 제2 예시 사례에서와 같이, 컴퓨터 작동의 우선순위는 가장 빠른 컴퓨터 작동에 대해 가장 높은 우선순위로 그리고 최종 컴퓨터 작동에 대해 가장 낮은 우선순위로 설정되어야 한다. 따라서, 각각의 작업 큐 내의 모든 제1 컴퓨터 작동에는 우선순위 0이 부여된다. 각 작업 큐의 모든 제2 컴퓨터 작동에는 우선순위 2가 부여되고, 따라서 작업 큐의 나머지 컴퓨터 작동 모두에 대해서도 마찬가지로 순 위가 부여된다. 따라서, 네 개의 작업 큐 모두(1651, 1652, 1653, 및 1654)는 순차적으로 0, 1, 2,...10, 11로 정렬된 12개의 컴퓨터 작동을 갖는다. 이는 본질적으로 우선순위 시스템의 계층 우선 유형이고, 이는, 가능한 많은 컴퓨터 작동을 병렬로 프로세싱하려고 시도함으로써 최대 처리량을 달성한다. 메모리 제약 또는 기타 리소스 제약이 있는 경우에, 스케줄링 시스템은 리소스 사용량을 줄이기 위해 활성 큐 수를 제한할 수 있음을 유의한다. 또한, 처리량을 최대화하도록 설계된 이러한 예시의 경우, 시스템은 선점을 비활성할 수 있다. 선점은, 데이터를 주위로 이동하는 시간을 낭비할 수 있고, 이에 따라 시스템의 처리량을 감 소시킨다. 스케줄링 사례 6- 다중 서버 훈련 도 17a는 제6 스케줄링 작동 예시용 4-계층 인공 신경망(ANN)을 나타낸다. 제6 스케줄링 예시는, ANN을 통해 전 체 훈련 사이클로 프로세싱될 필요가 있는 네 개의 데이터 샘플을 포함한 샘플 데이터 배치 1711을 갖는다. 이 러한 제6 스케줄링 예시에서, 4-계층 인공 신경망은 두 개의 절반부로 분할되었고, 두 개의 절반부는 각각 상이 한 선형 대수 매트릭스 프로세서에 의해 취급된다. 구체적으로, 도 17a에 나타낸 바와 같이, 처음 두 계층은 \" 서버 0\" 1731로 표지된 제1 매트릭스 프로세서에 의해 취급될 것이고, 두 번째 두 계층은 \"서버 1\" 1732로 표지 된 제2 매트릭스 프로세서에 의해 취급될 것이다. 두 서버에 걸쳐 균등하게 ANN을 분할하는 것은, 최대 활용이 달성될 수 있도록 균형을 이룬다. 첫 번째 단계는 여덟 개의 작업 큐 세트, 즉 서버 0 1731에 의해 취급되는 네 개의 데이터 샘플 각각에 대한 네 개의 작업 큐와 서버 1 1732에 의해 취급되는 네 개의 데이터 샘플 각각에 대한 네 개의 작업 큐를 생성하는 것 이다. 도 17b는, 프로세싱 작업을 수행하기 위해 열 1751, 1752, 1753 및 1754 각각에 두 개의 작업 큐(서버 0 1731에 대해 한 개 그리고 서버 0 1732에 대해 한 개)가 있는 여덟 개의 작업 큐 세트를 나타낸다. 이는 전체 훈련 세션이기 때문에, 시스템은 서버 0 1731에 대한 작업 큐에 의해 취급되는 처음 두 개의 계층에 대한 두 개 의 정방향 계산(FP), 두 개의 역전파(BP) 및 두 개의 경사 업데이트(GU) 컴퓨터 작동, 그리고 서버 1 1732에 대 한 두 번째 두 개의 계층에 대한 동일한 여섯 개의 컴퓨터 작동을 갖는다. 다음으로, 컴퓨터 작동의 우선순위를 할당해야 한다. 제2 서버(서버 1 1732)가 신속하게 작동을 시작하도록 하 기 위해; 두 개의 정방향 계산(FP) 컴퓨터 작동은 높은 우선순위 설정이 부여된다. 이렇게 하면, 서버 0 1731이 작동을 신속하게 완료하고 데이터를 서버 1 1732로 전달하는 것을 보장한다. 유사하게, 서버 1 1732에서의 역전 파(BP) 컴퓨터 작동은, 완전히 신속하게 데이터를 서버 0 1731에 반환하여 역전파(BP) 컴퓨터 작동을 완료할 수 있도록, 높은 우선순위가 할당된다. 그러나, 경사 업데이트 작동은, 올바른 활용도를 보장하기 위해 필요한 중요 실행 경로에 있지 않기 때문에, 훨 씬 낮은 우선순위 값이 부여된다. 경사 업데이트(GU) 컴퓨터 작동은, 더 높은 우선순위 컴퓨터 작동이 없을 때 취급될 수 있는데, 그 이유는 어떤 추가 작동도 경사 업데이트(GU) 컴퓨터 작동으로부터의 정보에 의존하지 않기 때문이다. 이 우선순위화 체계를 이용해, 네 개의 작업 큐 모두에 대한 정방향 계산(FP) 및 역전파(BP) 컴퓨 터 작동이 데이터 샘플과 일반적으로 동일한 순서로 가능한 빠르게 완료되어야 한다. 이는 높은 활용도를 보장 한다. 낮은 우선순위의 경사 업데이트 컴퓨터 작동은 나중에 완료될 것이다. 중요 경로 작동이 먼저 완료되도록 보장하기 위해, 시스템은 이 예시에 대해 선점을 활성한다. 선점을 활성함으 로써, 시스템은, 우선순위가 낮은 작업으로부터 우선순위가 높은 작업으로 컨텍스트 전환하여, 중요 경로를 따 라 놓이는 작업을 완료한다. 메모리 제약 또는 다른 리소스 제약이 있는 경우, 리소스 사용량을 줄이기 위해 스 케줄링 시스템이 활성 큐 수를 제한할 수 있다. 스케줄링 사례 7 - 다중 서버 훈련 - 재연산 도 18a는 재연산 작동의 사용을 설명할 제7 스케줄링 작동 예시에 대한 4-계층 인공 신경망(ANN)을 나타낸다. 이 제7 스케줄링 예시는, 도 18a의 ANN을 통해 전체 훈련 사이클로 프로세싱될 필요가 있는 네 개의 데이터 샘 플을 포함한 샘플 데이터 배치 1811을 갖는다. 이러한 제7 스케줄링 예시에서, 4-계층 인공 신경망은 두 개의 절반부로 또한 분할되었고, 두 개의 절반부는 각각 상이한 선형 대수 매트릭스 프로세서에 의해 취급된다. 구체 적으로, 도 18a에 나타낸 바와 같이, 처음 두 계층은 \"서버 0\" 1831로 표지된 제1 매트릭스 프로세서에 의해 취 급될 것이고, 두 번째 두 계층은 \"서버 1\" 1832로 표지된 제2 매트릭스 프로세서에 의해 취급될 것이다. 두 서 버에 걸쳐 균등하게 ANN을 분할하는 것은, 최대 활용이 달성될 수 있도록 균형을 이룬다. 첫 번째 단계는 여덟 개의 작업 큐 세트, 즉 서버 0 1831 및 서버 1 1832 모두에서 프로세싱될 네 개의 데이터 샘플 각각에 대해 하나의 작업 큐를 생성하는 것이다. 도 18b는, 각 열 1851, 1852, 1853, 및 1854에 두 개의 작업 큐가 있는, 여덟 개의 작업 큐 세트를 나타낸다; 상부 작업 큐는 서버 0 1831용이고 하부 작업 큐는 서버 1 1832용이다. 이는 전체 훈련 세션이기 때문에, 시스템은, 서버 0 1831에서 처음 두 개의 계층을 취급하는 작 업 큐에 대한 두 개의 정방향 계산(FP), 두 개의 역전파(BP) 및 두 개의 경사 업데이트(GU) 컴퓨터 작동, 그리 고 서버 1 1832에서 두 번째 두 개의 계층을 취급하는 작업 큐에 대한 동일한 여섯 개의 컴퓨터 작동을 갖는다. 이 특정 스케줄링 예시에서, 시스템은 메모리 리소스를 저장하기 위해 재연산으로 알려진 기술을 사용한다. 구 체적으로, 처음 두 개의 ANN 계층에 대한 서버 0 1831에서의 초기 두 개의 정방향 프로세싱(FP) 컴퓨터 작동으 로부터의 중간 데이터가 폐기된다. 이후 역전파(BP) 컴퓨터 작동을 완료하기 위해, 이들 두 개의 정방향 프로세 싱(FP) 컴퓨터 작동을 재연산할 필요가 있다. 이는, 서버 0 1831에 대한 작업 큐에서 재연산(RC) 작동으로 수행 된다. 또한, 서버 0 1831에 대한 작업 큐는 \"대기\"로 표시된 플래그 항목을 포함한다. 작업 큐가 서버 1 1832의 해당 작업 큐로부터 이제 데이터가 작동을 재개할 수 있음을 표시한 \"통지\" 메시지를 수신할 때까지, \"대기\" 플 래그는, 특정 작업 큐가 작동을 일시 중지해야 한다는 것을 표시한다. \"대기\" 플래그는, 작업 큐에서 두 개의 정방향 프로세싱(FP) 컴퓨터 작동 직후에 바로 배치된다. 이 \"대기\" 플래그는, 작동을 재개해야 하는 데이터를 사용할 수 있을 때까지 서버 1 1832가 리소스를 소비하는 것을 방지함을 유의해야 한다. 서버 1 1832를 위한 작업 큐를 참조하면, 작업 큐는, ANN의 상위 두 개의 계층을 완료하기 위해 두 개의 정방향 계산(FP) 컴퓨터 작동과 그 다음 역전파 상태로 두 개의 역전파(BP) 작동을 포함한다. 두 개의 역전파(BP) 컴퓨 터 작동 후, \"통지\" 플래그가 사용되어, 서버 0 1831의 해당 작업 큐에게, 역 전파(BP) 작동으로부터 필요한 데 이터를 이제 사용할 수 있기 때문에 작업 큐가 작동을 재개할 수 있음을 통지한다. 그 다음, 서버 1 1832의 나 머지 두 개의 경사 업데이트(GU) 작동은 서버 1 1832에 대한 작업 큐를 완료한다. 서버 0 1831용 작업 큐를 다시 참조하면, \"통지\" 메시지가 수신된 이후, 서버 0 1831의 작업 큐는 작동을 재개 한다. 먼저, 두 개의 재연산(RC) 작동은 이전의 정방향 계산(FP) 컴퓨터 작동으로부터 폐기된 중간 데이터를 재 생성한다. 다음으로, 역전파(BP) 컴퓨터 작동이 수행될 수 있다. 마지막으로, 마지막 두 개의 경사 업데이트 (GU) 컴퓨터 작동이 수행된다. 모든 작업 큐에서 컴퓨터 작동의 우선순위를 설정해야 한다. 이 예시에서, \"샘플 우선\" 우선순위 시스템은, 메 모리 리소스가 확보될 수 있도록 가능한 빠르게 각 샘플을 완료하기 위해 사용된다. 중요 경로 작동이 먼저 완료되도록 보장하기 위해, 시스템은 이 예시에 대해 선점을 활성한다. 선점을 활성함으 로써, 시스템은, 우선순위가 낮은 작업으로부터 우선순위가 높은 작업으로 컨텍스트 전환하여, 리소스를 확보하 기 위한 작업을 완료한다. 메모리 제약 또는 다른 리소스 제약이 있는 경우, 리소스 사용량을 줄이기 위해 스케 줄링 시스템이 활성 큐 수를 제한할 수 있다. 스케줄링 사례 8 - 다수 작업이 있는 단일 서버 도 19a는 제8 스케줄링 작동 예시용 4-계층 인공 신경망을 나타낸다. 이 제8 스케줄링 예시는, 두 개의 다른 작 업: 작업1 1971 및 작업2 1972와 연관된 두 개의 상이한 샘플 데이터 배치 1911 및 1912를 갖는다. 각 작업은, 추론 작동으로 프로세싱되어야 할 필요가 있는 두 개의 데이터 샘플을 갖는다. 따라서, 각각의 데이터 샘플은 네트워크 계층 1922 네 개와 연관된 네 개의 정방향 계산(FP) 컴퓨터 작동으로 프로세싱될 필요가 있다. 이 예 시에서, 두 작업 모두 실행 리소스를 받고 둘 다 중단되지 않도록, 작업 간에 공정성을 유지하는 것이 목표이다. 첫 번째 단계는 네 개의 작업 큐 세트를 생성하는 것이며, 작업1에 대해 프로세싱될 두 개의 데이터 샘플 각각 에 대해 하나, 그리고 작업2에 대해 프로세싱될 두 개의 데이터 샘플 각각에 대해 하나를 생성하는 것이다. 도 19b는 작업 1 1971에 대한 두 개의 작업 큐(작업 큐 1951 및 1952) 및 작업 2 1972에 대한 두 개의 작업 큐(작 업 큐 1953 및 1954)를 나타낸다. 이는 단지 추론 작동이기 때문에, 네 개의 작업 큐 각각이 단지 네 개의 정방 향 계산(FP) 컴퓨터 작동을, 도 19a의 4-계층 ANN의 각 계층에 대해 하나씩 요구한다. 이 경우, 목적은, 두 작업 간의 공정성을 보장하는 것이다. 이 목표를 달성하기 위해, 두 작업의 우선순위 값을 동일한 우선순위로 설정할 수 있다. 실제로, 시스템은 DWRR(결함 가중 라운드 로빈)과 같은 알고리즘을 사용하 여 작업 간 리소스의 우선순위 및 공정한 공유를 보장할 수 있다. 더 높은 수준에서, 작업 간에 적절한 공정성이 시행되고 있는지 여부를 결정하기 위해 각 작업이 수신한 프로세 싱의 양이 모니터링될 수 있다. 프로세싱의 양이 정의된 작업 우선순위 수준과 일치하지 않는 경우, 시스템은, 특정 작업으로부터의 데이터 샘플이 시스템에 제공되는 속도를 증가시키거나 감소시킬 수 있다. 추가 고려 사항. 이전 섹션은 몇 가지 스케줄링 시나리오와 이들 시나리오가 어떻게 취급되는지에 대해 설명했다. 이들 시나리오 각각에 설명된 기술을 결합하여, 어려운 스케줄링 문제에 대해 복잡한 해결책을 만들 수 있다. 또한, 시스템의 작동은, 원하는 결과가 달성되지 않는 경우에 다양한 파라미터가 조절될 수 있도록, 연속적으로 모니터링될 수 있다. 예를 들어, 데이터 샘플이 제공되는 속도가 조절될 수 있거나, 활성 큐의 수가 증가되거나 감소될 수 있다. 작동 환경은 상당히 복잡해질 수 있다. 다양한 작업을 대규모로 지원하려면, 백그라운드에서 또는 백그라운드에 서 작업을 전환할 수 있는 능력이 필요하다. 이는, 또한 작업 간 버퍼와 대역폭을 공유하는 것과 작업 유사군을 갖는 것을 포함한다. 병렬 훈련이 발생할 수 있으며, 여기서 동일한 인공 신경망 모델이 복제되고 다수의 훈련 세트가 병렬로 실행된 다. 병렬 훈련 후, 병렬 모델로부터의 경사 업데이트를 함께 병합하여 병렬 모델로부터 단일 모델을 생성할 필 요가 있다. 따라서, 이는, 파라미터 서버에 대한 다양한 가중치를 병합하고 이를 다시 브로드캐스팅하는 것을 필요로 한다. 가중치 업데이트가 적절히 스케줄링된 경우에, 백그라운드에서 이를 수행할 수 있다. 전술한 기술 개시는 예시적인 것으로 의도되며, 제한적이지 않다. 예를 들어, 전술한 구현예 (또는 이의 하나 이상의 양태)는 서로 조합하여 사용될 수 있다. 상기 설명을 검토할 시, 다른 구현예가 당업자에게 명백해질 것 이다. 따라서, 청구범위의 범주는 이러한 청구범위가 부여되는 균등물의 전체 범주와 함께 첨부된 청구범위를 참조하여 결정되어야 한다. 첨부된 청구범위에서, 용어 \"포함하는\" 및 \"그 안에서\"는 각각의 용어 \"구성하는\" 및 \"여기서\"의 쉬운 영어 등가물로서 사용된다. 또한, 다음의 청구범위에서, 용어 \"포함하는\" 및 \"구성하는\"은 개방형, 즉, 청구범위에서 이러한 용어 이후에 열거된 것에 추가하여 요소를 포함한 시스템, 장치, 물품, 또는 프로세스가, 여전히 그 청구범위의 범주 내에 있는 것으로 간주된다. 또한, 다음의 청구범위에서, 용어 \"제1\", \"제2\", 및 \"제3\" 등은 단지 라벨로만 사용되며, 그 객체에 수치 요건을 부과하기 위한 것이 아니다."}
{"patent_id": "10-2021-7039976", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "요약은 37 C.F.R. §1.72(b)를 준수하기 위해 제공되며, 이는 독자가 기술 개시의 성질을 신속하게 확인시킬 수"}
{"patent_id": "10-2021-7039976", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "있게 요구된다. 요약은, 그것이 청구범위의 범주 또는 의미를 해석하거나 제한하기 위해 사용되지 않을 것이라 는 것을 이해를 가지고, 제출된다. 또한, 상기 상세한 설명에서, 다양한 특징부가 함께 그룹화되어 본 개시를 간소화할 수 있다. 이는, 청구되지 않고 개시된 특징부가 임의의 청구범위에 필수적인 것으로 의도되는 것으로 해석되어서는 안 된다. 오히려, 본 발명의 주제는, 특정 개시된 구현예의 모든 특징부보다 더 적게 존재할 수 있다. 따라서, 다음의 청구범위는 상세한 설명에 통합되며, 각각의 청구범위는 그 자체로서 별도의 구현예로서 유지된다.도면 도면1a 도면1b 도면2 도면3a 도면3b 도면4a 도면4b 도면4c 도면5a 도면5b 도면6a 도면6b 도면7a 도면7b 도면8 도면9 도면10 도면11 도면12a 도면12b 도면13a 도면13b 도면14a 도면14b 도면15a 도면15b 도면16a 도면16b 도면17a 도면17b 도면18a 도면18b 도면19a 도면19b"}
{"patent_id": "10-2021-7039976", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도면에서, 도면은 반드시 축척에 맞게 도시되지는 않으며, 유사한 숫자는 여러 도면 전체에 걸쳐 실질적으로 유 사한 구성 요소를 설명한다. 상이한 문자 접미사를 갖는 유사한 숫자는, 실질적으로 유사한 구성 요소의 상이한 인스턴스를 나타낸다. 도면은, 일반적으로 본 문서에서 논의된 다양한 구현예를 예시로서 도시하지만, 제한적인 것은 아니다. 도 1a는 단일 계층 인공 신경망의 개념도를 나타낸다. 도 1b는 이중 계층 인공 신경망의 개념도를 나타낸다. 도 2는 매트릭스 연산을 수행하는 데 사용될 수 있는 추상화된 매트릭스 프로세서의 블록도를 나타낸다. 도 3a는 두 측면 상의 버퍼 및 두 측면 상의 벡터 프로세서로 둘러싸인 매트릭스 프로세서 어레이의 블록도를 나타낸다. 도 3b는 도 3a의 매트릭스 프로세서 어레이의 일 구현예를 나타낸다. 도 4a는 4 계층 인공 신경망을 통한 정방향 계산 추론 작동을 개념적으로 나타낸다. 도 4b는 4 계층 인공 신경망을 통한 역전파 작동을 개념적으로 나타낸다. 도 4c는 4 계층 인공 신경망을 통한 가중치 업데이트 작동을 개념적으로 나타낸다. 도 5a는 4 계층 인공 신경망을 나타낸다. 도 5b는 샘플 데이터 벡터의 배치, 및 샘플 데이터 벡터의 추론 작동을 위해 수행되어야 하는 작동을 나타낸다. 도 6a는 4-계층 인공 신경망용 공통 인공 신경망 프로세싱의 세 단계 모두에 대한 전체 데이터 의존성을 나타낸 다. 도 6b는 4-계층 인공 신경망용 공통 인공 신경망 프로세싱의 세 단계 모두에 대한 데이터 의존성에 따라 순서화 된 세 개의 작업 큐를 나타낸다. 도 7a는 도 5a의 4-계층 인공 신경망을 통해 데이터 샘플을 프로세싱하기 위한 \"계층-우선\" 스케줄링 시스템을 나타낸다. 도 7b는 도 5a의 4-계층 인공 신경망을 통해 데이터 샘플을 프로세싱하기 위한 \"샘플-우선\" 스케줄링 시스템을 나타낸다.도 8은 본 개시의 교시가 사용될 수 있는 다양하게 상이한 실행 패러다임을 설명하는 개념도를 나타낸다. 도 9는 실행을 위해 신경망을 준비하기 위한 신경망 프로세싱 시스템의 개요를 나타낸다. 도 10은 신경망 작업 구성 시스템이 작동하는 방법을 도시한 흐름도를 나타낸다. 도 11은 신경망 동적 스케줄러가 인공 신경망을 통해 프로세싱하기 위한 배치식 데이터 샘플을 준비하는 방법을 도시한 흐름도를 나타낸다. 도 12a는 제1 스케줄링 작동 예시용 4-계층 인공 신경망을 나타낸다. 도 12b는, 지연 시간이 적은 도 12a의 인공 신경망을 통해 네 개의 데이터 샘플을 프로세싱하기 위한 네 개의 작업 큐 세트를 나타낸다. 도 13a는 제2 스케줄링 작동 예시용 4-계층 인공 신경망을 나타낸다. 도 13b는, 최대 처리량을 갖는 도 13a의 인공 신경망을 통해 네 개의 데이터 샘플을 프로세싱하기 위한 네 개의 작업 큐 세트를 나타낸다. 도 14a는 제3 스케줄링 작동 예시를 위해 두 개의 서버를 교차해 분할된 4-계층 인공 신경망을 나타낸다. 도 14b는 최소 지연 시간을 갖는 두 개의 서버를 갖는 도 14a의 인공 신경망을 통해 네 개의 데이터 샘플을 프 로세싱하기 위한 여덟 개의 작업 큐 세트를 나타낸다. 도 15a는 전체 훈련 세션을 갖는 제4 스케줄링 작동 예시용 4-계층 인공 신경망을 나타낸다. 도 15b는 도 15a의 인공 신경망 상의 전체 훈련 세션을 통해 네 개의 데이터 샘플을 프로세싱하기 위한 네 개의 작업 큐 세트를 나타낸다. 도 16a는 전체 훈련 세션을 갖는 제5 스케줄링 작동 예시용 4-계층 인공 신경망을 나타낸다. 도 16b는 도 16a의 인공 신경망 상의 전체 훈련 세션을 통해 네 개의 데이터 샘플을 프로세싱하기 위한 네 개의 작업 큐 세트를 나타낸다. 도 17a는 제6 스케줄링 작동 예시를 위해 두 개의 서버를 교차해 분할된 4-계층 인공 신경망을 나타낸다. 도 17b는 두 개의 서버를 갖는 도 17a의 인공 신경망을 통해 네 개의 데이터 샘플을 프로세싱하기 위한 여덟 개 의 작업 큐 세트를 나타낸다. 도 18a는 제7 스케줄링 작동 예시를 위해 두 개의 서버를 교차해 분할된 4-계층 인공 신경망을 나타낸다. 도 18b는 두 개의 서버와 재연산을 갖는 도 18a의 인공 신경망을 통해 네 개의 데이터 샘플을 프로세싱하기 위 한 여덟 개의 작업 큐 세트를 나타낸다. 도 19a는 두 개의 작업을 갖는 추론 작동을 갖는 제8 스케줄링 작동 예시용 4-계층 인공 신경망을 나타낸다. 도 19b는 도 19a의 인공 신경망 상의 추론 작동을 통해 두 개의 상이한 작업으로부터두 개의 데이터 샘플을 프 로세싱하기 위한 네 개의 작업 큐 세트를 나타낸다."}
