{"patent_id": "10-2022-0122436", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0078502", "출원번호": "10-2022-0122436", "발명의 명칭": "이미지 처리 장치 및 방법", "출원인": "삼성전자주식회사", "발명자": "바오 허"}}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "제1 이미지의 특징맵을 획득하고, 상기 특징맵을 기반으로 상기 제1 이미지에서 타겟 영역을 검출하는 단계;검출된 타겟 영역을 보정하는 단계; 및보정된 타겟 영역을 기반으로 상기 타겟 영역에 대응되는 객체를 처리하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 이미지의 특징맵을 획득하고, 상기 특징맵을 기반으로 상기 제1 이미지에서 타겟 영역을 검출하는 단계는,적어도 하나의 스케일에서 상기 제1 이미지의 특징을 추출하여 상기 제1 이미지의 적어도 하나의 특징맵을 획득하는 단계; 및상기 적어도 하나의 특징맵을 기반으로 상기 제1 이미지에서 타겟 영역을 검출하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 적어도 하나의 스케일에서 상기 제1 이미지의 특징을 추출하여 상기 제1 이미지의 적어도 하나의 특징맵을획득하는 단계는,상기 적어도 하나의 스케일 각각에서, 컨볼루션 신경망으로 상기 제1 이미지에 대해 컨볼루션 동작을 수행하여상기 적어도 하나의 스케일 각각의 특징맵을 획득하는 단계를 포함하고,상기 컨볼루션 신경망은,상기 제1 이미지 상의 적어도 하나의 위치 각각에 대하여, 상기 적어도 하나의 위치 각각에 대응되는 컨볼루션커널 함수를 이용하여 상기 컨볼루션 동작을 수행하는이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 컨볼루션 신경망으로 상기 제1 이미지에 대해 상기 컨볼루션 동작을 수행하여 상기 적어도 하나의 스케일각각의 특징맵을 획득하는 단계는,상기 제1 이미지 상의 상기 적어도 하나의 위치 각각에 대응되는 상기 컨볼루션 커널 함수의 샘플링 위치를 획득하는 단계; 및상기 적어도 하나의 위치 각각에 대응되는 상기 컨볼루션 커널 함수의 샘플링 위치에 따라 상기 컨볼루션 동작공개특허 10-2023-0078502-3-을 수행하여 상기 적어도 하나의 스케일 각각의 특징맵을 획득하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 제1 이미지 상의 상기 적어도 하나의 위치 각각에 대응되는 상기 컨볼루션 커널 함수의 샘플링 위치를 획득하는 단계는,상기 제1 이미지의 이미징 모델에 따라, 3차원 공간에서 상기 적어도 하나의 위치 각각에 대응되는 상기 컨볼루션 커널 함수의 샘플링 위치를 결정하는 단계; 및상기 3차원 공간에서의 상기 컨볼루션 커널 함수의 샘플링 위치 및 상기 이미징 모델에 따라, 상기 제1 이미지에서 상기 적어도 하나의 위치 각각에 대응되는 상기 컨볼루션 커널 함수의 샘플링 위치를 결정하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제2항에 있어서,상기 적어도 하나의 특징맵은,복수의 특징맵이고, 상기 적어도 하나의 특징맵을 기반으로 상기 제1 이미지에서 타겟 영역을 검출하는 단계는,상기 복수의 특징맵에서 인접한 스케일의 특징맵을 융합하고, 적어도 하나의 융합된 특징맵에 기초하여 상기 제1 이미지에서 타겟 영역을 검출하는이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 검출된 타겟 영역을 보정하는 단계는,상기 제1 이미지의 특징맵에서 상기 검출된 타겟 영역에 대응되는 제1 특징 영역을 제1 타겟 영역 특징맵으로결정하는 단계; 및상기 제1 타겟 영역 특징맵을 공간적으로 변환하여 변환된 제1 타겟 영역 특징맵을 생성하는 단계를 포함하고,상기 보정된 타겟 영역을 기반으로 상기 타겟 영역에 대응되는 객체를 처리하는 단계는,상기 변환된 제1 타겟 영역 특징맵에 기초하여 상기 타겟 영역에 대응되는 객체를 처리하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 제1 타겟 영역 특징맵을 공간적으로 변환하여 상기 변환된 제1 타겟 영역 특징맵을 생성하는 단계는,상기 제1 이미지의 이미징 모델 및 상기 검출된 타겟 영역에 따라 상기 타겟 영역에 대응되는 가상 카메라를 생공개특허 10-2023-0078502-4-성하는 단계; 및상기 제1 타겟 영역 특징맵을 상기 가상 카메라로 공간적으로 변환하여 상기 변환된 제1 타겟 영역 특징맵을 생성하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 변환된 제1 타겟 영역 특징맵에 기초하여 상기 타겟 영역에 대응되는 객체를 처리하는 단계는,상기 변환된 제1 타겟 영역 특징맵에 기초하여 상기 타겟 영역에 대응하는 객체의 제1 속성 정보를 획득하는 단계; 및상기 제1 속성 정보에 따라 상기 타겟 영역에 대응되는 객체를 처리하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 제1 이미지에 연관된 제2 이미지를 획득하는 단계; 및상기 제2 이미지에 기초하여 상기 객체의 제2 속성 정보를 획득하는 단계를 더 포함하고,상기 제1 속성 정보에 따라 상기 타겟 영역에 대응되는 객체를 처리하는 단계는,상기 제1 속성 정보 및 상기 제2 속성 정보에 따라 상기 타겟 영역에 대응되는 객체를 처리하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 타겟 영역에 대응되는 객체를 처리하는 단계는,상기 객체에 대한 객체 인식, 객체 분할 및 객체 자세 추정 중 적어도 하나를 수행하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,상기 제1 속성 정보는,상기 객체의 카테고리 정보, 상기 객체의 마스크 정보, 상기 객체의 키 포인트 정보 및 상기 객체의 자세 정보중 적어도 하나를 포함하는이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "공개특허 10-2023-0078502-5-제10항에 있어서,상기 제1 속성 정보는,상기 객체의 제1 키 포인트 정보 및 초기 자세 정보를 포함하고,상기 제2 속성 정보는,상기 객체의 제2 키 포인트 정보를 포함하고,상기 제1 속성 정보 및 상기 제2 속성 정보에 따라 상기 타겟 영역에 대응되는 객체를 처리하는 단계는,상기 초기 자세 정보, 상기 제1 키 포인트 정보 및 상기 제2 키 포인트 정보에 기초하여 상기 객체의 최종 자세정보를 추정하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 제2 이미지에 기초하여 상기 객체의 상기 제2 속성 정보를 획득하는 단계는,상기 초기 자세 정보 및 상기 제1 이미지를 생성하는 제1 카메라의 파라미터 및 상기 제2 이미지를 생성하는 제2 카메라의 파라미터에 기초하여 상기 제2 이미지에서 상기 객체에 대응되는 타겟 영역을 결정하는 단계; 및상기 제2 이미지 상의 상기 객체에 대응되는 타겟 영역에 기초하여 상기 객체의 상기 제2 키 포인트 정보를 획득하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 초기 자세 정보 및 상기 제1 이미지를 생성하는 상기 제1 카메라의 파라미터 및 상기 제2 이미지를 생성하는 상기 제2 카메라의 파라미터에 기초하여 상기 제2 이미지에서 상기 객체에 대응되는 타겟 영역을 결정하는단계는, 상기 초기 자세 정보 및 상기 제1 카메라의 파라미터에 기초하여 상기 제1 카메라의 좌표계에서 상기 객체의 초기 자세 정보를 결정하는 단계;상기 제1 카메라의 좌표계에서의 상기 객체의 초기 자세 정보 및 상기 제2 카메라의 파라미터에 기초하여 상기제2 카메라의 좌표계에서 상기 객체의 초기 자세 정보를 결정하는 단계; 및상기 제2 카메라의 좌표계에서의 상기 객체의 초기 자세 정보에 따라서 상기 제2 이미지에서 상기 객체에 대응되는 타겟 영역을 결정하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서,상기 제2 이미지 상의 상기 객체에 대응되는 타겟 영역에 기초하여 상기 객체의 상기 제2 키 포인트 정보를 획득하는 단계는,공개특허 10-2023-0078502-6-상기 제2 이미지 상의 상기 객체에 대응되는 타겟 영역을 보정하는 단계; 및상기 제2 이미지 상의 보정된 타겟 영역에 기초하여 상기 객체의 상기 제2 키 포인트 정보를 획득하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 제2 이미지 상의 상기 객체에 대응되는 타겟 영역을 보정하는 단계는, 상기 제2 이미지의 특징맵을 획득하는 단계;상기 제2 이미지의 특징맵에서 상기 제2 이미지 상의 타겟 영역에 대응되는 제2 특징 영역을 제2 타겟 영역 특징맵으로 결정하는 단계; 및상기 제2 타겟 영역 특징맵을 공간적으로 변환하여 변환된 제2 타겟 영역 특징맵을 생성하는 단계를 포함하고,상기 제2 이미지 상의 보정된 타겟 영역에 기초하여 상기 객체의 상기 제2 키 포인트 정보를 획득하는 단계는,상기 변환된 제2 타겟 영역 특징맵을 기반으로 상기 객체의 제2 키 포인트 정보를 획득하는 단계를 포함하는 이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "컨볼루션 신경망을 이용하여 제1 이미지에 대해 컨볼루션 동작을 수행하여 상기 제1 이미지의 특징맵을 획득하는 단계; 및상기 제1 이미지의 특징맵에 기초하여 상기 제1 이미지 중의 객체를 처리하는 단계를 포함하고,상기 컨볼루션 신경망은,상기 제1 이미지 상의 적어도 하나의 위치 각각에 대응되는 컨볼루션 커널 함수를 이용하여 상기 컨볼루션 동작을 수행하는이미지 처리 방법."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제1항 내지 제18항 중 어느 한 항의 방법을 실행하기 위한 프로그램이 기록되어 있는 것을 특징으로 하는 컴퓨터에서 판독 가능한 기록 매체."}
{"patent_id": "10-2022-0122436", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제1 이미지의 특징맵을 획득하고, 상기 특징맵을 기반으로 상기 제1 이미지에서 타겟 영역을 검출하는 검출부;검출된 타겟 영역을 보정하는 보정부; 및보정된 타겟 영역을 기반으로 상기 타겟 영역에 대응되는 객체를 처리하는 이미지 처리부를 포함하는 이미지 처리 장치."}
{"patent_id": "10-2022-0122436", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 이미지 처리 장치 및 방법에 관한 것으로, 이미지 처리 방법은 제1 이미지의 특징맵을 획득하고, 상기 특징맵을 기반으로 상기 제1 이미지에서 타겟 영역을 검출하는 단계, 검출된 타겟 영역을 보정하는 단계 및 보정 된 타겟 영역을 기반으로 상기 타겟 영역에 대응되는 객체를 처리하는 단계를 포함할 수 있다. 또한, 이미지 처 리 방법은 인공 지능 모델을 이용하여 수행될 수 있다."}
{"patent_id": "10-2022-0122436", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공 지능 분야에 관한 것으로, 특히 이미지 처리 방법, 장치, 전자 기기 및 저장 매체에 관한 것이다."}
{"patent_id": "10-2022-0122436", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이미지를 처리하기 전에, 객체 처리 효과를 보장하기 위해 전체 이미지를 보정한 다음 보정된 전체 이미지를 기 준으로 객체를 처리한다. 예를 들어, 어안 카메라로 촬영한 어안 이미지는 뚜렷한 왜곡을 포함하는데, 종래의 기술에서는 어안 이미지 중의 객체를 처리하기 전에, 전체 어안 이미지를 보정하고, 보정된 전체 이미지를 기반 으로 객체를 처리한다(예를 들어, 객체 인식, 분할 및 자세 추정). 그러나 이와 같은 이미지 처리 방식은 전체 이미지를 보정할 때 객체가 큰 폭으로 늘어나 후속되는 객체의 처리 효과가 좋지 않을 뿐만 아니라, 이미지 처 리 속도가 느리다. 상기 관점에서, 객체 처리 효과를 향상시키거나 이미지 처리 속도를 향상시킬 수 있는 보다 나은 이미지 처리 기술이 필요하다. 본 개시는 이미지 처리 장치 및 방법을 제공한다. 일 실시 예에 따른 이미지 처리 방법은, 제1 이미지의 특징맵을 획득하고, 상기 특징맵을 기반으로 상기 제1 이 미지에서 타겟 영역을 검출하는 단계; 검출된 타겟 영역을 보정하는 단계; 및 보정된 타겟 영역을 기반으로 상 기 타겟 영역에 대응되는 객체를 처리하는 단계를 포함할 수 있다. 이때, 상기 제1 이미지의 특징맵을 획득하고, 상기 특징맵을 기반으로 상기 제1 이미지에서 타겟 영역을 검출하 는 단계는, 적어도 하나의 스케일에서 상기 제1 이미지의 특징을 추출하여 상기 제1 이미지의 적어도 하나의 특 징맵을 획득하는 단계; 및 상기 적어도 하나의 특징맵을 기반으로 상기 제1 이미지에서 타겟 영역을 검출하는 단계를 포함할 수 있다. 이때, 상기 적어도 하나의 스케일에서 상기 제1 이미지의 특징을 추출하여 상기 제1 이미지의 적어도 하나의 특 징맵을 획득하는 단계는, 상기 적어도 하나의 스케일 각각에서, 컨볼루션 신경망으로 상기 제1 이미지에 대해 컨볼루션 동작을 수행하여 상기 적어도 하나의 스케일 각각의 특징맵을 획득하는 단계를 포함하고, 상기 컨볼루 션 신경망은, 상기 제1 이미지 상의 적어도 하나의 위치 각각에 대하여, 상기 적어도 하나의 위치 각각에 대응 되는 컨볼루션 커널 함수를 이용하여 상기 컨볼루션 동작을 수행할 수 있다. 이때, 상기 컨볼루션 신경망으로 상기 제1 이미지에 대해 상기 컨볼루션 동작을 수행하여 상기 적어도 하나의 스케일 각각의 특징맵을 획득하는 단계는, 상기 제1 이미지 상의 상기 적어도 하나의 위치 각각에 대응되는 상 기 컨볼루션 커널 함수의 샘플링 위치를 획득하는 단계; 및 상기 적어도 하나의 위치 각각에 대응되는 상기 컨 볼루션 커널 함수의 샘플링 위치에 따라 상기 컨볼루션 동작을 수행하여 상기 적어도 하나의 스케일 각각의 특 징맵을 획득하는 단계를 포함할 수 있다. 이때, 상기 제1 이미지 상의 상기 적어도 하나의 위치 각각에 대응되는 상기 컨볼루션 커널 함수의 샘플링 위치 를 획득하는 단계는, 상기 제1 이미지의 이미징 모델에 따라, 3차원 공간에서 상기 적어도 하나의 위치 각각에 대응되는 상기 컨볼루션 커널 함수의 샘플링 위치를 결정하는 단계; 및 상기 3차원 공간에서의 상기 컨볼루션 커널 함수의 샘플링 위치 및 상기 이미징 모델에 따라, 상기 제1 이미지에서 상기 적어도 하나의 위치 각각에 대응되는 상기 컨볼루션 커널 함수의 샘플링 위치를 결정하는 단계를 포함할 수 있다. 이때, 상기 적어도 하나의 특징맵은, 복수의 특징맵이고, 상기 적어도 하나의 특징맵을 기반으로 상기 제1 이미 지에서 타겟 영역을 검출하는 단계는, 상기 복수의 특징맵에서 인접한 스케일의 특징맵을 융합하고, 적어도 하 나의 융합된 특징맵에 기초하여 상기 제1 이미지에서 타겟 영역을 검출할 수 있다. 이때, 상기 검출된 타겟 영역을 보정하는 단계는, 상기 제1 이미지의 특징맵에서 상기 검출된 타겟 영역에 대응 되는 제1 특징 영역을 제1 타겟 영역 특징맵으로 결정하는 단계; 및 상기 제1 타겟 영역 특징맵을 공간적으로 변환하여 변환된 제1 타겟 영역 특징맵을 생성하는 단계를 포함하고, 상기 보정된 타겟 영역을 기반으로 상기 타겟 영역에 대응되는 객체를 처리하는 단계는, 상기 변환된 제1 타겟 영역 특징맵에 기초하여 상기 타겟 영역에 대응되는 객체를 처리하는 단계를 포함할 수 있다. 이때, 상기 제1 타겟 영역 특징맵을 공간적으로 변환하여 상기 변환된 제1 타겟 영역 특징맵을 생성하는 단계는, 상기 제1 이미지의 이미징 모델 및 상기 검출된 타겟 영역에 따라 상기 타겟 영역에 대응되는 가상 카 메라를 생성하는 단계; 및 상기 제1 타겟 영역 특징맵을 상기 가상 카메라로 공간적으로 변환하여 상기 변환된 제1 타겟 영역 특징맵을 생성하는 단계를 포함할 수 있다. 이때, 상기 변환된 제1 타겟 영역 특징맵에 기초하여 상기 타겟 영역에 대응되는 객체를 처리하는 단계는, 상기 변환된 제1 타겟 영역 특징맵에 기초하여 상기 타겟 영역에 대응하는 객체의 제1 속성 정보를 획득하는 단계; 및 상기 제1 속성 정보에 따라 상기 타겟 영역에 대응되는 객체를 처리하는 단계를 포함할 수 있다. 이때, 이미지 처리 방법은, 상기 제1 이미지에 연관된 제2 이미지를 획득하는 단계; 및 상기 제2 이미지에 기초 하여 상기 객체의 제2 속성 정보를 획득하는 단계를 더 포함하고, 상기 제1 속성 정보에 따라 상기 타겟 영역에 대응되는 객체를 처리하는 단계는, 상기 제1 속성 정보 및 상기 제2 속성 정보에 따라 상기 타겟 영역에 대응되 는 객체를 처리하는 단계를 포함할 수 있다. 이때, 상기 타겟 영역에 대응되는 객체를 처리하는 단계는, 상기 객체에 대한 객체 인식, 객체 분할 및 객체 자 세 추정 중 적어도 하나를 수행하는 단계를 포함할 수 있다. 이때, 상기 제1 속성 정보는, 상기 객체의 카테고리 정보, 상기 객체의 마스크 정보, 상기 객체의 키 포인트 정 보 및 상기 객체의 자세 정보 중 적어도 하나를 포함할 수 있다. 상기 제1 속성 정보는, 상기 객체의 제1 키 포인트 정보 및 초기 자세 정보를 포함하고, 상기 제2 속성 정보는, 상기 객체의 제2 키 포인트 정보를 포함하고, 상기 제1 속성 정보 및 상기 제2 속성 정보에 따라 상기 타겟 영 역에 대응되는 객체를 처리하는 단계는, 상기 초기 자세 정보, 상기 제1 키 포인트 정보 및 상기 제2 키 포인트 정보에 기초하여 상기 객체의 최종 자세 정보를 추정하는 단계를 포함할 수 있다. 이때, 상기 제2 이미지에 기초하여 상기 객체의 상기 제2 속성 정보를 획득하는 단계는, 상기 초기 자세 정보 및 상기 제1 이미지를 생성하는 제1 카메라의 파라미터 및 상기 제2 이미지를 생성하는 제2 카메라의 파라미터 에 기초하여 상기 제2 이미지에서 상기 객체에 대응되는 타겟 영역을 결정하는 단계; 및 상기 제2 이미지 상의 상기 객체에 대응되는 타겟 영역에 기초하여 상기 객체의 상기 제2 키 포인트 정보를 획득하는 단계를 포함할 수 있다. 이때, 상기 초기 자세 정보 및 상기 제1 이미지를 생성하는 상기 제1 카메라의 파라미터 및 상기 제2 이미지를 생성하는 상기 제2 카메라의 파라미터에 기초하여 상기 제2 이미지에서 상기 객체에 대응되는 타겟 영역을 결정 하는 단계는, 상기 초기 자세 정보 및 상기 제1 카메라의 파라미터에 기초하여 상기 제1 카메라의 좌표계에서 상기 객체의 초기 자세 정보를 결정하는 단계; 상기 제1 카메라의 좌표계에서의 상기 객체의 초기 자세 정보 및 상기 제2 카메라의 파라미터에 기초하여 상기 제2 카메라의 좌표계에서 상기 객체의 초기 자세 정보를 결정하는 단계; 및 상기 제2 카메라의 좌표계에서의 상기 객체의 초기 자세 정보에 따라서 상기 제2 이미지에서 상기 객 체에 대응되는 타겟 영역을 결정하는 단계를 포함할 수 있다. 이때, 상기 제2 이미지 상의 상기 객체에 대응되는 타겟 영역에 기초하여 상기 객체의 상기 제2 키 포인트 정보 를 획득하는 단계는, 상기 제2 이미지 상의 상기 객체에 대응되는 타겟 영역을 보정하는 단계; 및 상기 제2 이 미지 상의 보정된 타겟 영역에 기초하여 상기 객체의 상기 제2 키 포인트 정보를 획득하는 단계를 포함할 수 있 다. 이때, 상기 제2 이미지 상의 상기 객체에 대응되는 타겟 영역을 보정하는 단계는, 상기 제2 이미지의 특징맵을 획득하는 단계; 상기 제2 이미지의 특징맵에서 상기 제2 이미지 상의 타겟 영역에 대응되는 제2 특징 영역을 제 2 타겟 영역 특징맵으로 결정하는 단계; 및 상기 제2 타겟 영역 특징맵을 공간적으로 변환하여 변환된 제2 타겟 영역 특징맵을 생성하는 단계를 포함하고, 상기 제2 이미지 상의 보정된 타겟 영역에 기초하여 상기 객체의 상 기 제2 키 포인트 정보를 획득하는 단계는, 상기 변환된 제2 타겟 영역 특징맵을 기반으로 상기 객체의 제2 키 포인트 정보를 획득하는 단계를 포함할 수 있다. 다른 실시 예에 따른 이미지 처리 방법은, 컨볼루션 신경망을 이용하여 제1 이미지에 대해 컨볼루션 동작을 수 행하여 상기 제1 이미지의 특징맵을 획득하는 단계; 및 상기 제1 이미지의 특징맵에 기초하여 상기 제1 이미지 중의 객체를 처리하는 단계를 포함하고, 상기 컨볼루션 신경망은 상기 제1 이미지 상의 적어도 하나의 위치 각 각에 대응되는 컨볼루션 커널 함수를 이용하여 상기 컨볼루션 동작을 수행할 수 있다.일 실시 예에 따른 이미지 처리 장치는, 제1 이미지의 특징맵을 획득하고, 상기 특징맵을 기반으로 상기 제1 이 미지에서 타겟 영역을 검출하는 검출부; 검출된 타겟 영역을 보정하는 보정부; 및 보정된 타겟 영역을 기반으로 상기 타겟 영역에 대응되는 객체를 처리하는 이미지 처리부를 포함할 수 있다."}
{"patent_id": "10-2022-0122436", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서, 첨부된 도면을 참조하여 실시예들을 상세하게 설명한다. 그러나, 실시예들에는 다양한 변경이 가해 질 수 있어서 특허출원의 권리 범위가 이러한 실시예들에 의해 제한되거나 한정되는 것은 아니다. 실시예들에 대한 모든 변경, 균등물 내지 대체물이 권리 범위에 포함되는 것으로 이해되어야 한다. 실시예에서 사용한 용어는 단지 설명을 목적으로 사용된 것으로, 한정하려는 의도로 해석되어서는 안 된다. 단 수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또 는 \"가지다\" 등의 용어는 명세서 상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 실시예가 속 하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의 미를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다. 또한, 첨부 도면을 참조하여 설명함에 있어, 도면 부호에 관계없이 동일한 구성 요소는 동일한 참조부호를 부여 하고 이에 대한 중복되는 설명은 생략하기로 한다. 실시예를 설명함에 있어서 관련된 공지 기술에 대한 구체적 인 설명이 실시예의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 실시 예의 구성 요소를 설명하는 데 있어서, 제1, 제2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이 러한 용어는 그 구성 요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성 요소의 본질 이나 차례 또는 순서 등이 한정되지 않는다. 어떤 구성 요소가 다른 구성요소에 \"연결\", \"결합\" 또는 \"접속\"된 다고 기재된 경우, 그 구성 요소는 그 다른 구성요소에 직접적으로 연결되거나 접속될 수 있지만, 각 구성 요소사이에 또 다른 구성 요소가 \"연결\", \"결합\" 또는 \"접속\"될 수도 있다고 이해되어야 할 것이다. 어느 하나의 실시 예에 포함된 구성요소와, 공통적인 기능을 포함하는 구성요소는, 다른 실시 예에서 동일한 명 칭을 사용하여 설명하기로 한다. 반대되는 기재가 없는 이상, 어느 하나의 실시 예에 기재한 설명은 다른 실시 예에도 적용될 수 있으며, 중복되는 범위에서 구체적인 설명은 생략하기로 한다."}
{"patent_id": "10-2022-0122436", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "본 개시의 배경기술에서 언급한 바와 같이, 종래의 이미지 처리 방법에 있어서, 전체 이미지를 보정하는 과정에 서 객체가 큰 폭으로 늘어나, 후속되는 객체 처리 효과가 좋지 않을 뿐만 아니라, 처리 속도도 느리다. 반대로, 본 개시에서는 먼저 타겟 영역을 검출한 후, 검출된 타겟 영역만 보정함으로써, 객체가 큰 폭으로 늘어 나는 것을 방지하고, 후속되는 객체 처리 효과를 향상시키며, 무관한 영역을 보정하지 않으므로, 이미지 처리 속도를 향상시킬 수 있다. 이하에서는, 본 개시의 일 실시 예에 따른 이미지 처리 장치 및 방법을 첨부된 도 1 내지 도 13을 참조하여 상 세히 설명한다. 도 1은 일 실시 예에 따른 이미지 처리 방법의 흐름도이다. 도 1을 참조하면, 이미지 처리 방법은 110단계에서 제1 이미지의 특징맵을 획득하고, 상기 특징맵을 기반으로 제1 이미지 중의 타겟 영역을 검출할 수 있다. 여기서, 타겟 영역은 관심 영역(ROI, Region of Interest)(예를 들어, 후보 객체 영역)일 수 있다. 여기서, 제1 이미지는 어안 이미지일 수 있는데, 예를 들어, 어안 스테레오 카메라로 촬영한 왼쪽 어안 이미지 및 오른쪽 어안 이미지 중 하나일 수 있으나, 이에 한정되지 않는다. 실제로, 제1 이미지는 객체의 변형이 존재하는 임의의 이미지일 수 있다. 예를 들어, 이미지 처리 방법은 적어도 하나의 스케일에서 제1 이미지의 특징을 추출하여, 제1 이미지의 적어도 하나의 특징맵을 획득하고, 상기 적어도 하나의 특징맵을 기반으로 제1 이미지 중의 타겟 영역을 검출할 수 있 다. 일 실시 예에 따르면, 이미지 처리 방법은 제1 이미지 전체를 보정한 후, 보정된 이미지에서 특징을 추출할 필 요가 없다. 이미지 처리 방법은 각 스케일에서 제1 이미지의 특징을 직접 추출할 수 있다. 따라서, 이미지 처리 방법은 전체 제1 이미지를 보정하는 시간 소모를 피할 수 있을 뿐만 아니라, 추출된 특징의 정확도도 향상시킬 수 있다. 바람직하게는, 이미지 처리 방법은 컨볼루션 신경망으로 제1 이미지에 대해 컨볼루션 동작을 수행하여 각 스케 일의 특징맵을 획득할 수 있다. 이때, 상기 컨볼루션 신경망은 제1 이미지 상의 적어도 하나의 위치 중 각 위치 에 대하여, 상기 각 위치에 대응되는 컨볼루션 커널 함수를 이용하여 컨볼루션 동작을 수행할 수 있다. 아래에 서, 일 실시예에 따른 컨볼루션 신경망을 \"변형 가능한 컨볼루션 신경망(CNN, Convolutional Neural Networ k)\"으로 기재할 수 있다. 앞서 언급한 바와 같이, 어안 이미지와 같은 제1 이미지는 객체 왜곡이나 변형을 포함한다. 기존의 컨볼루션 신 경망(컨볼루션 커널 함수는 항상 고정됨)을 사용하는 이미지 처리 방법은 샘플링 왜곡이 발생하여 특징 추출이 어렵거나 정확하지 않을 수 있다. 하지만, 본 개시의 일 실시 예에 따른 변형 가능한 CNN을 사용하는 이미지 처 리 방법의 CNN은 제1 이미지 상의 적어도 하나의 위치 중 각 위치에 대하여, 상기 각 위치에 대응되는 컨볼루션 커널 함수를 이용하여 컨볼루션 동작을 수행할 수 있다. 따라서, 본 개시의 이미지 처리 방법은 객체 변형이 있 는 이미지에서 기존 CNN을 사용할 때 발생하는 샘플링 왜곡을 방지하여, 보다 정확하게 특징을 추출할 수 있으 므로, 후속 단계에서 이러한 특징을 사용하여 객체 처리 효과를 향상시킬 수 있다. 예를 들어, 변형 가능한 CNN은 어안 이미지의 중심 부분과 주변 부분의 이미지 해상도 변화에 적응할 수 있기에, 보다 정확하게 특징을 추출할 수 있고, 어안 이미지에서 객체의 처리 효과를 향상시킬 수 있다. 도 2는 일 실시 예에 따른 이미지 처리 방법에서 변형 가능한 CNN을 사용하여 다중 스케일에서 제1 이미지의 특 징을 추출하는 개략적인 과정을 도시한 도면이다. 도 2의 예시에서, 이미지 처리 방법은 3개의 스케일에서 제1 이미지의 특징을 추출하고, 제1 이미지가 어안 이 미지인 것을 가정할 수 있다. 이때, 스케일의 개수는 3개에 한정되지 않고, 1 이상의 임의의 양의 정수 일 수 있다. 또한, 제1 이미지는 어안 이미지에 한정되지 않고, 객체 변형이 존재하는 임의의 이미지일 수 있다. 도 2와 같이, 이미지 처리 방법은 3개의 스케일 각각에서 변형 가능한 CNN(210, 212, 214)을 각각 사용하여, 어 안 이미지 특징을 추출할 수 있다. 대응되게, 어안 이미지는 각 스케일에 따라 축소되는데, 예를 들어 제 1 이미지의 크기의 1/2, 1/4 및 1/8로 조정될 수 있다. 전술된 바와 같이, 변형 가능한 CNN(210, 212, 214)은 제1 이미지 상의 적어도 하나의 위치 중 각 위치에 대하여, 상기 각 위치에 대응되는 컨볼루션 커널 함수를 이용하여 컨볼루션 동작을 수행할 수 있다. 즉, 제1 이 미지 상의 적어도 하나의 위치 중 각 위치는 하나의 자체의 컨볼루션 커널 함수에 대응된다. 즉, 각 위치 의 컨볼루션 커널 함수는 항상 고정되어 있는 것이 아니라 변할 수 있다. 여기의 위치는 픽셀 포인트가 될 수 있다. 따라서, 일 실시 예에 따르면, 이미지 처리 방법은 컨볼루션 동작을 수행할 때, 먼저 제1 이미지 상의 상기 적어도 하나의 위치 중 각 위치에 대응되는 컨볼루션 커널 함수의 샘플링 위치를 획득할 수 있다. 그 리고, 이미지 처리 방법은 각 위치에 대응되는 컨볼루션 커널 함수의 샘플링 위치에 따라 컨볼루션 동작을 수행 하여, 각 스케일의 특징맵을 얻을 수 있다. 여기서, 컨볼루션 커널 함수의 샘플링 위치는 제1 이미지의 이미징 모델에 따라 결정된다. 예를 들어, 각 스케일에서, 제1 이미지의 이미징 모델에 따라 제1 이미지 상의 적어도 하나의 위치 중 각 위치에 대 응되는 컨볼루션 커널 함수의 샘플링 위치를 미리 계산하여, 계산된 샘플링 위치를 예를 들어 룩업 테이블(LUT; Look Up Table)에 저장할 수 있다. LUT는 미리 저장될 수 있으며, 각 스케일에서 컨볼루션 동작을 수행하는데 사용되어, 각 스케일에서 특징맵을 얻을 수 있다. 도 2에 나타낸 여러 변형 가능한 CNN(210, 212, 214)으로 구 성된 피라미드의 각 스케일에 대해, 해당 스케일의 제1 이미지 상의 적어도 하나의 위치 중 각 위치에 대 응되는 컨볼루션 커널 함수의 샘플링 위치가 모두 미리 계산되고 저장되었기에, 전술된 제1 이미지 상의 적어도 하나의 위치 중 각 위치에 대응되는 컨볼루션 커널 함수의 샘플링 위치를 획득하는 것은, 예를 들어, 미 리 저장된 LCU로부터 각 위치에 대응되는 컨볼루션 커널 함수의 샘플링 위치를 획득하는 것일 수 있다. 도 3은 일 실시 예에 따른 이미지 처리 방법에서 변형 가능한 CNN 컨볼루션 커널 함수의 샘플링 위치를 결정하 는 도면이다. 도 3을 참조하여, 변형 가능한 CNN의 컨볼루션 커널 함수의 샘플링 위치를 결정하는 방식을 간략하게 설명한다. 일 실시 예에 따르면, 이미지 처리 방법은 이미징 모델에 따라 3차원 공간에서 각 위치의 컨볼루션 커널 함수의 샘플링 위치를 결정할 수 있다. 그리고, 이미지 처리 방법은 3차원 공간에서의 컨볼루션 커널 함수의 샘플링 위치 및 이미징 모델에 따라, 제1 이미지에서 각 위치에 대응되는 컨볼루션 커널 함수의 샘플링 위치를 결정할 수 있다. 도 3의 예시에서, 이미지 처리 방법은 제1 이미지를 어안 이미지로 가정하였기 때문에 이미징 모델은 어안 이미 지의 이미징 모델일 수 있다. 아래에서 어안 이미지의 이미징 모델은 \"어안 카메라 모델\"이라고도 할 수 있다. 예를 들어, 어안 카메라 모델은 Kannala-Brandt 모델일 수 있다. 도 3에 도시된 바와 같이, 구체적으로, 이미지 처리 방법은 먼저 어안 이미지 상의 각 위치와 어안 카메라 모델 의 광학 중심(도 3의 점(Oc))을 연결하여 하나의 광선(도 3에서 Oc와 픽셀 포인트(A)를 연결한 직선)을 결정할 수 있다. 다음으로, 이미지 처리 방법은 어안 카메라 모델의 파라미터(\"어안 카메라의 내부 파라미터\"라고도 함)에 따라 해당 광선이 광학 중심을 통과하는 입사광선이 편향되어 발생하는지 결정할 수 있다. 예를 들어, 어안 카메라 모델이 Kannala-Brandt 모델인 경우, 입사광선은 아래 <수학식 1>에 따라 결정될 수 있다: [수학식 1]"}
{"patent_id": "10-2022-0122436", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, 는 픽셀 위치와 광학 중심을 연결하는 선과 어안 카메라 모델의 광학 축(도 3에서 OcZc가 위치한 선) 사이의 각도이고, 는 입사광선과 어안 카메라의 광학 축이 이루는 각도이며. k1 내지 k4는 다항식 계수이 다. 입사광선이 결정된 후, 이미지 처리 방법은 해당 입사광선(도 3의 광선(OcP))과 어안 카메라 모델의 교차점(도 3의 점(B))을 결정할 수 있다. 마지막으로, 이미지 처리 방법은 해당 교차점을 지나 어안 카메라 모델의 구면과 접하는 3차원 로컬 평면 격자에서 3차원 공간에서의 컨볼루션 커널 함수의 샘플링 위치를 선택할 수 있다. 예를 들어, 이미지 처리 방법은 해당 3차원 로컬 평면 격자에서 등간격 균일 샘플링으로 샘플링 포인트 세트를 선택할 수 있다. 도 3과 같이, 이미지 처리 방법은 해당 교차점 주변에서 9개 샘플링 포인트를 등간격으로 선택하는 \"3차원 공간에서의 컨볼루션 커널 함수 샘플링 포인트\"를 선택할 수 있다. 이미지 처리 방법은 3차원 공간에서 각 위치의 컨볼루션 커널 함수의 샘플링 위치를 결정한 후, 어안 카메라 모 델에 따른 광선 투영을 통해 3차원 공간에서의 컨볼루션 커널 함수의 샘플링 위치를 어안 이미지에 매핑하 여, 어안 이미지 상에서 어안 이미지 상의 각 위치에 대응되는 컨볼루션 커널 함수의 샘플링 위치를 결정 할 수 있다. 예를 들어, 도 3과 같이, 이미지 처리 방법은 Kannala-Brandt 모델에 따라, 광선 투영을 통해 3차 원 공간 중의 9개 샘플링 포인트를 어안 이미지 평면에 각각 매핑하여, 어안 이미지에서도 대응되게 9개의 샘플링 포인트에 해당하는 \"어안 이미지 평면의 컨볼루션 커널 함수의 샘플링 포인트\"를 획득할 수 있다. 이와 같은 9개 샘플링 포인트는 향후 컨볼루션 동작을 수행할 때 픽셀(A)에 대응되는 컨볼루션 커널 함수 의 샘플링 위치이다. 이상과 같이, 도 3과 결합하여 제1 이미지 상의 적어도 하나의 위치 중 각 위치에 대응되는 컨볼루션 커널 함수 의 샘플링 위치를 결정하는 방식을 설명하였다. 일 실시 예에 따르면, 각 스케일에서 모두 변형 가능한 CNN을 이용하여, 각 위치에 대응되는 컨볼루션 커널 함수의 샘플링 위치에 따라 컨볼루션 동작을 수행하여, 각 스케일 의 특징맵을 얻을 수 있다. 이때, 적어도 하나의 특징맵은 복수의 특징맵일 수 있다. 이 경우, 일 실시 예에 따르면, 적어도 하나의 특징맵 을 기반으로 제1 이미지에서 타겟 영역을 검출하는 것은 복수의 특징맵에서 인접한 스케일의 특징맵을 융합하고, 적어도 하나의 융합된 특징맵에 기초하여 상기 제1 이미지에서 타겟 영역을 검출하는 것을 포함할 수 있다. 예를 들어, 특징맵을 타겟 영역 제안 네트워크에 입력하여, 제1 이미지 중의 타겟 영역을 검출할 수 있 다. 여기서, 타겟 영역 제안 네트워크는 미리 학습된 컨볼루션 신경망일 수 있으나, 이에 한정되지 않는다. 타겟 영역 제안 네트워크는 입력한 특징맵에 대하여, 제1 이미지 중의 타겟 영역을 검출할 수 있도록 미리 학습 될 수 있다. 도 4는 일 실시 예에 따른 이미지 처리 방법에서 다중 스케일 특징을 융합하는 예를 도시한 도면이다. 도 4를 참조하면, 이미지 처리 방법은 피라미드 모양의 변형 가능한 CNN에 의해 추출된 다중 스케일 특징에 대 해, 상이한 스케일 간의 특징 융합을 더 수행할 수 있다. 구체적으로, 이미지 처리 방법은 도 4와 같이, 저해 상도 특징맵을 업 샘플링한 후, 인접한 스케일의 상위 레이어 특징맵과 융합(예를 들어, 픽셀 단위로 가산)함으 로써, 융합된 특징맵이 저해상도 특징맵 중의 의미적 정보를 포함할 뿐만 아니라, 고해상도 특징맵 중의 이미지 세부 정보도 포함하도록 할 수 있다. 이러한 경우, 적어도 하나의 융합된 특징맵에 기초하여 제1 이미지 중의 타겟 영역을 검출하는 것이 보다 정확할 수 있다. 특징을 융합한 후, 저해상도 특징은 예를 들어 객체 영역의 제안, 포지셔닝 및 분류에 사용되어 계산 비용을 절 약할 수 있다. 그리고, 고해상도 특징은 객체의 세부 속성(예를 들어, 키 포인트, 객체 마스크 맵, 6DoF(자유도) 자세)을 추정하는 정확성을 보장할 수 있다. 예를 들어, 이미지 처리 방법은 융합된 특징맵 중의 상대적으로 낮은 해상도의 특징맵(예를 들어, 도 4의 특징 맵 1 및 특징맵 2만을 사용하여, 타겟 영역 제안 네트워크을 통해서 제1 이미지 중의 타겟 영역을 검출할 수 있다. 이러한 방법은 계산 비용을 더 절약할 수 있다. 융합된 특징맵에서 상대적으로 높은 해 상도의 특징맵(예를 들어, 도 4의 특징맵 3)은 후속되는 검출된 타겟 영역에 대한 보정(\"변형을 방지하는 타겟 영역을 풀링\")에 사용될 수 있다. 이때, 타겟 영역이 ROI인 경우, \"변형을 방지하는 타겟 영역을 풀 링\"은 \"변형을 방지하는 ROI 풀링\"이라고도 할 수도 있다. 변형을 방지하는 타겟 영역을 풀링은 객체 자세 추정 등에 사용될 수 있다(예를 들어, 아래에서 설명할 특징 추출 및 객체 키 포인트 예측, 객체 마스크 맵 획득 및 객체 자세 추정 등). 이미지 처리 방법은 110단계에서 제1 이미지 중의 타겟 영역을 검출한 후, 120단계에서, 검출된 타겟 영역을 보 정할 수 있다. 구체적으로, 먼저, 이미지 처리 방법은 제1 이미지의 특징맵에서, 검출된 타겟 영역에 대응되는 제1 특징 영역을 결정하여 제1 타겟 영역 특징맵으로 사용할 수 있다. 다음으로, 이미지 처리 방법은 제1 타겟 영역 특징맵을 공간적으로 변환하면, 변환된 제1 타겟 영역 특징맵을 생성할 수 있다. 제1 타겟 영역 특징맵을 공간적으로 변환하면, 타겟 영역에 대한 보정이 실현된다. 예를 들어, 상술된 바와 같이, 다중 스케일에서 복 수의 특징맵을 획득하고 특징맵을 융합하면, 융합된 특징맵 중 상대적으로 고해상도의 특징맵(예를 들어, 융합 된 특징맵 중 가장 높은 해상도의 특징맵, 즉 융합된 최대 스케일의 특징맵)에서 검출된 타겟 영역에 대응되는제1 특징 영역을 결정할 수 있다. 도 4에 도시된 바와 같이, 특징맵 3에서 검출된 타겟 영역에 대응되는 제1 특징 영역을 결정하여 제1 타겟 영역 특징맵으로 사용할 수 있다. 이미지 처리 방법은 복수의 타겟 영역이 검출되면, 각각의 타겟 영역에 대응되는 제1 타겟 영역 특징맵을 결정할 수 있다. 그 후, 이미지 처리 방법은 각각의 제1 타겟 영역 특징맵에 대해 공간적으로 변환하여, 각각의 타겟 영역을 보정할 수 있다. 일 실시 예에 따르면, 이미지 처리 방법은 제1 이미지의 이미징 모델 및 검출된 타겟 영역에 따라, 타겟 영역에 대응되는 가상 카메라를 생성하고, 제1 타겟 영역 특징맵을 가상 카메라로 공간적으로 변환하여, 변환된 제1 타 겟 영역 특징맵을 생성할 수 있다. 본 개시에서, 이미지 처리 방법은 전체 이미지 또는 모든 타겟 영역에 대해 동일한 가상 카메라를 사용하는 것이 아니라, 각각의 검출된 타겟 영역에 대해 대응되는 가상 카메라를 생성함 으로써, 보정 시 객체의 모양이 늘어나는 것을 방지할 수 있다. 예를 들어, 본 개시의 이러한 이미지 처리 방 법은 어안 렌즈의 시야각 가장자리에서 흔히 발생하는 모양의 늘어짐을 방지할 수 있다. 또한, 변형을 방지하 는 타겟 영역을 풀링한 후, 제1 타겟 영역 특징맵은 기존 카메라와 동일한 기하학적 형태로 변환되어, 후속 객 체 처리 모델의 훈련이나 예측을 수행하기에 특징맵의 사용이 유리할 수 있다. 일 실시예에 따르면, 가상 카메라의 광학 축에 대응되는 광선은, 이미징 모델을 지나 굴절되어, 검출된 타겟 영 역의 중심을 통과한다. 또한, 가상 카메라의 광학 축은 이미징 모델의 광학 중심을 가리킬 수 있다. 도 5는 일 실시 예에 따른 이미지 처리 방법에서 변형을 방지하는 관심 영역의 풀링의 예를 도시한 도면이다. 도 5의 예시에서, 이미지 처리 방법은 여전히 제1 이미지를 어안 이미지로, 대응되게, 제1 이미지의 이미징 모 델을 어안 카메라 모델로 가정한다. 도 5와 같이, 타겟 영역에 대해 생성한 가상 카메라의 광학 축은 점(Oc)과 점(F)을 연결하여 결정된 직선이 될 수 있다. 해당 직선에 대응되는 광선은 어안 카메라 모델(도 5의 구형)을 지나 굴절된 후, 타겟 영역의 픽셀 중심을 통과한다(도 5의 점(E)). 또한, 타겟 영역 가상 카메라의 이미지 평 면은 어안 카메라 모델의 구면에 접하며, 이미지의 y축은 Zc-Oc-P에 의해 정의된 평면에 있다. 구체적으로, 이미지 처리 방법은 가상 카메라를 생성할 때, 먼저 타겟 영역의 픽셀 중심인 점(E)과 어안 카메라 모델의 광학 중심(Oc)을 연결하여 직선을 결정수 있다. 다음으로, 이미지 처리 방법은 어안 카메라 모델의 파라 미터에 따라서 해당 직선이 대응하는 광선이 광학 중심을 통과한 입사광선 중 어느 광선의 편향으로 인한 것인 지 결정할 수 있다. 예를 들어, 어안 카메라 모델이 Kannala-Brandt 모델인 경우, 도 3에서 언급한 <수학식 1>로 입사광선을 결정할 수 있다. 따라서, 중복된 설명은 생략한다. 해당 입사광선에 대응되는 직선이 바로 가 상 카메라의 광학 축이다. 이미지 처리 방법은 광학 축을 결정한 후, 광학 축에 수직인 평면을 가상 카메라의 평면으로 결정할 수 있다. 이때, 가상 카메라의 평면은 어안 카메라 모델의 구면에 접하는 평면일 수 있지만, 이에 한정되지 않는다. 도 5에 도시된 바와 같이, 가상 카메라의 초점 거리(f)(즉, 도 5의 광학 중심(Oc)과 가 상 카메라 평면의 중심(F) 사이의 거리)이다. 가상 카메라의 평면 타겟 영역의 가상 카메라의 초점 거리(f)는 타겟 영역의 크기에 따라 동적으로 계산되어, 변형을 방지하는 타겟 영역 특징 이미지의 이미지 높이(H)와 이미 지 너비(W)가 고정된 크기를 갖도록 할 수 있다. 이미지 처리 방법은 각각의 타겟 영역에 대해 대응되는 가상 카메라를 생성한 후, 각 타겟 영역의 제1 타겟 영 역 특징맵을 생성된 가상 카메라로 공간적으로 변형시켜, 각각의 변환된 제1 타겟 영역 특징맵을 생성할 수 있 다. 구체적으로, 이미지 처리 방법은 이미징 모델에 따라 제1 타겟 영역 특징맵 중의 각 특징 포인트를 대응되 는 가상 카메라 평면에 매핑하여, 변환된 제1 타겟 영역 특징맵을 획득할 수 있다. 예를 들어, 각 특징 포인트 를 이미징 모델의 광학 중심과 연결하여 하나의 광선을 결정하고, 이미징 모델의 파라미터에 따라 해당 광선에 대응되는 입사 광선과 가상 카메라 평면의 교차점을 결정하여, 이러한 교차점에 따라 변환된 제1 타겟 영역 특 징맵을 얻을 수 있다. 이상과 같이, 이미지 처리 방법은 제1 타겟 영역 특징맵을 변환함으로써, 타겟 영역의 보정을 실현할 수 있다. 마지막으로 이미지 처리 방법은 130단계에서, 보정된 타겟 영역을 기반으로, 타겟 영역에 대응되는 객체를 처리 할 수 있다. 구체적으로, 변환된 제1 타겟 영역 특징맵에 기초하여, 타겟 영역에 대응되는 객체를 처리할 수 있다. 예를 들어, 이미지 처리 방법은 변환된 제1 타겟 영역 특징맵에 기초하여, 타겟 영역에 대응되는 객체의 제1 속성 정보를 획득하고, 제1 속성 정보에 따라, 타겟 영역에 대응되는 객체를 처리할 수 있다. 예를 들어, 이미지 처리 방법은 변환된 제1 타겟 영역 특징맵에 기초하여, 적어도 하나의 컨볼루션 신경망을 이용하여 타겟 영역에 대응되는 객체의 제1 속성 정보를 획득할 수 있다. 예를 들어, 제1 속성 정보는 객체 카테고리 정보,마스크 정보, 키 포인트 정보, 및 자세 정보 중 적어도 하나를 포함할 수 있으나, 이에 한정되지 않는다. 대응 되게, 이미지 처리 방법은 서로 다른 속성 정보에 따라, 타겟 영역에 대응되는 객체에 대해 서로 다른 처리를 수행할 수 있다. 예를 들어, 이미지 처리 방법은 객체에 대해, 객체 인식, 객체 분할, 객체 자세 추정 중 적어 도 하나를 수행할 수 있다. 이미지 처리 방법은 변환된 제1 타겟 영역 특징맵에 기초하여 적어도 하나의 컨볼 루션 신경망을 사용하여 자세 정보를 얻을 수 있지만, 객체의 키 포인트 정보를 얻은 후, n점 투시(PnP, Perspective-n-Point) 알고리즘 등을 이용하여 객체의 자세 정보를 결정할 수도 있다. 상술한 바와 같이, 제1 이미지에 객체 변형이 존재하므로, 제1 이미지만을 이용하여 전술된 객체 처리를 수행하 는 경우, 효과가 여전히 정확하지 않을 수 있다. 본 개시의 일 실시 예에 따르면, 이미지 처리 방법은 제1 이미 지에 연관된 제2 이미지를 획득하고, 제2 이미지에 기초하여 객체의 제2 속성 정보를 획득하는 것을 더 포함할 수 있다. 이 경우, 제1 속성 정보에 따라 타겟 영역에 대응되는 객체를 처리하는 것은, 제1 속성 정보 및 제2 속성 정보에 따라 타겟 영역에 대응되는 객체를 처리하는 것을 포함할 수 있다. 이러한 방식으로 객체 처리 효 과를 보다 더 향상시킬 수 있다. 이때, 제2 이미지도 변형된 이미지일 수 있다. 상술한 바와 같이, 제1 이미 지는 왼쪽 어안 이미지 및 오른쪽 어안 이미지 중 어느 하나일 수 있고, 이때, 제2 이미지는 왼쪽 어안 이미지 및 오른쪽 어안 이미지 중 다른 하나일 수 있다. 상기 방법에 따라, 왼쪽 어안 이미지 및 오른쪽 어안 이미지 모두에 기초하여 객체를 보다 정확하게 처리할 수 있는데, 예를 들어 객체의 자세를 보다 정확하게 추정할 수 있다. 도 6은 일 실시 예에 따른 이미지 처리 방법에서 객체의 자세를 추정하는 개략적인 과정을 도시한 도면이다. 구체적으로, 본 개시의 일 실시 예에 따르면, 예를 들어, 객체의 자세 추정을 보다 정확하게 수행하기 위해, 제 1 속성 정보는 객체의 제1 키 포인트 정보(도면 중의 객체의 2차원 키 포인트에 대응됨(좌측도)) 및 초기 자세 정보(도면의 초기 객체 자세에 대응됨)를 포함할 수 있다. 그리고, 제2 속성 정보는 객체의 제2 키 포인트 정보(도면 중의 객체의 2차원 키 포인트에 대응됨(우측도))를 포함할 수 있다. 제1 속성 정보 및 제2 속성 정보에 따라 타겟 영역에 대응되는 객체를 처리하는 것은 제1 키 포인트 정보, 초기 자세 정보 및 제2 키 포인트 정보에 기초하여, 객체의 최종 자세 정보를 추정하는 것을 포함 할 수 있다(도면의 입체 6Dof 자세 최적화에 대응됨). 구체적으로, 예를 들어, 이미지 처리 방법은 초기 자세 정보, 제1 이미지를 생성하는 제1 카메라의 파라미터 및 제2 이미지를 생성하는 제2 카메라의 파라미터에 기초 하여, 제2 이미지에서 상기 객체에 대응되는 타겟 영역을 결정할 수 있다. 그리고, 이미지 처리 방법은 제2 이미지 상의 객체에 대응되는 타겟 영역에 기초하여, 객체의 제2 키 포인트 정 보를 획득하는 동작을 수행하여, 객체의 제2 키 포인트 정보를 획득할 수 있다. 예를 들어, 도 6에 도시된 바 와 같이, 제1 이미지 및 제2 이미지가 각각 왼쪽 어안 이미지 및 오른쪽 어안 이미지인 경우, 이미지 처리 방법 은 왼쪽 어안 이미지에 대하여 이미지 특징 추출 및 객체 속성 정보 예측을 수행하여 객체의 2차원 키 포인트와 초기 객체 자세를 획득할 수 있다. 이후, 이미지 처리 방법은 초기 객체 자세와 스 테레오 어안 카메라 파라미터에 따라 오른쪽 어안 이미지에서 객체에 대응되는 타겟 영역을 결정할 수 있 다(도 6에서 \"객체 영역 투영\"이라고 함). 다음으로, 이미지 처리 방법은 오른쪽 어안 이미지 상의 대응되는 타겟 영역에 기초하여, 객체의 제2 키 포인트 정보를 획득할 수 있다. 마지막으로, 이미지 처리 방법은 초기 객체 자세, 왼쪽 어안 이미지에 따라 획득한 2차원 키 포인트 및 오른쪽 어안 이미지에 따라 획득한 2차원 키 포인트에 기초하여 객체의 최종 자세 정보(즉, 초기 자세 정보에 대해 최적화)를 추 정할 수 있다. 예를 들어, 640 단계에서 이미지 처리 방법은 객체의 6Dof 자세를 추정할 수 있다. 객체 자세 추정 작업의 목표는 객체 좌표계에서 카메라 좌표계로의 회전 및 병진을 추정하는 것이다. 따라서, 자세 정보는 카메라 좌표계의 선택과 밀접하게 관련된다. 객체 자세 추정을 수행할 때, 타겟 영역에 대해 생성 된 가상 카메라로 생성한 변환된 제1 타겟 영역 특징맵을 사용하는 경우, 획득한 초기 자세 정보는 여전히 가상 카메라 좌표계의 정보이므로, 이를 실제 카메라(예를 들어, 어안 카메라 좌표계)로 다시 전환하여 추정된 자세 정보를 출력해야 한다. 또는, 후속 단계에서, 보다 정확한 자세 추정을 위해 초기 자세 정보를 전술된 제2 키 포인트 정보와 결합하여 사용하고자 하는 경우에도, 초기 자세 정보를 실제 카메라 좌표계에 전환할 필요가 있 다. 따라서, 초기 자세 정보, 제1 이미지를 생성하는 제1 카메라의 파라미터 및 제2 이미지를 생성하는 제2 카메라 의 파라미터에 기초하여 제2 이미지에서 객체에 대응되는 타겟 영역을 결정하는 것은, 초기 자세 정보 및 제1카메라 파라미터에 기초하여 제1 카메라의 좌표계에서 상기 객체의 초기 자세 정보를 결정하고, 제1 카메라의 좌표계에서의 객체의 초기 자세 정보 및 제2 카메라의 파라미터에 기초하여 제2 카메라의 좌표계에서 객체의 초 기 자세 정보를 결정하고, 제2 카메라의 좌표계에서의 객체의 초기 자세 정보에 따라서 제2 이미지에서 객체에 대응되는 타겟 영역을 결정하는 것을 포함할 수 있다. 즉, 이미지 처리 방법은 가상 카메라 좌표계에서의 초기 자세 정보를 실제 카메라 좌표계에서의 초기 자세 정보로 전환한 후, 실제 카메라 좌표계에서의 초기 자세 정보 를 이용하여, 제2 이미지에서 객체에 대응되는 타겟 영역을 결정할 수 있다. 예를 들어, 제1 이미지 및 제2 이미지가 각각 왼쪽 어안 이미지 및 오른쪽 어안 이미지인 경우, 이미지 처리 방 법은 왼쪽 어안 이미지를 기반으로 추정한 객체의 초기 자세를 오른쪽 어안 이미지에 투영하여, 오른쪽 어안 이 미지에서 대응되는 후보 객체 영역을 결정할 수 있다. 구체적으로, 예를 들어, 이미지 처리 방법은 타겟 영역 가상 카메라의 파라미터 행렬을 Kv, 가상 카메라의 카메라 좌표계를 Ov-XvYvZv로 가정할 수 있다. 이미지 처리 방법은 어안 이미지 원근 보정 이미지의 카메라 내부 파라미터를 Kc라 하고, 왼쪽 어안 카메라의 카메라 좌표계 를 Oc-XcYcZc라고 할 수 있다. 타겟 영역에 있는 가상 카메라의 좌표계에서 추정한 자세 정보는 회전 행렬(R v)과 병진 벡터(Tv)로 표현될 수 있으며, 이들은 아래 <수학식 2>를 통하여, Oc-XcYcZc 좌표계에서의 회전 행렬 (Rc)과 병진 벡터(Tc)로 전환될 수 있다. [수학식 2] Rc = inv(Kc)*Kv*Rv Tc = inv(Kc)*Kv*Tv 여기서, inv( )는 행렬의 역수를 구하는 함수이다. 그리고, 이미지 처리 방법은 결정된 좌우 두 개의 어안 카메라의 외부 파라미터를 통해, 왼쪽 어안 카메라 좌표 계와 오른쪽 어안 카메라 좌표계 사이의 회전 및 병진 변환을 얻을 수 있으며, 이로부터 왼쪽 어안 카메라의 좌 표계에서의 객체를 오른쪽 어안 카메라 좌표계로 회전 및 병진 변환할 수 있다. 다음으로, 이미지 처리 방법은 오른쪽 어안 카메라의 내부 파라미터(어안 이미지 이미징 모델의 파라미터)를 사용하여, 오른쪽 어안 이미지의 이미지 평면에 객체를 투영하여, 오른쪽 어안 이미지에서 객체에 대응되는 타겟 영역을 결정할 수 있다. 상술한 바와 같이, 이미지 처리 방법은 제2 이미지에서 객체에 대응되는 타겟 영역을 결정한 후, 제2 이미지 상 의 객체에 대응되는 타겟 영역에 기초하여, 객체의 제2 키 포인트 정보를 획득할 수 있다. 예를 들어, 이미지 처리 방법은 제2 이미지 상의 객체에 대응되는 타겟 영역을 보정한 후, 보정된 타겟 영역에 기초하여 객체의 제 2 키 포인트 정보를 획득할 수 있다. 선택적으로, 이미지 처리 방법은 제1 이미지 상의 타겟 영역을 보정하는 동일한 방식(즉, 상술한 변형을 방지하는 타겟 영역의 풀링)으로, 제2 이미지 상의 대응되는 타겟 영역을 보정 할 수 있다. 도 6을 참조하면, 이미지 처리 방법은 오른쪽 어안 이미지에서 제2 이미지의 특징맵을 획득한 다음, 제2 이미지 상의 타겟 영역에 대응되는 제2 특징 영역을, 제2 타겟 영역 특징맵으로서 제2 이미지의 특징맵에서 결정한다. 다음으로, 제2 타겟 영역 특징맵을 공간적으로 변환하여, 변환된 제2 타겟 영역 특징맵을 생성(62 6)할 수 있다. 마지막으로 이미지 처리 방법은 변환된 제2 타겟 영역 특징맵을 기반으로, 객체의 제2 키 포인트 정보를 획득 할 수 있다. 예를 들어, 이미지 처리 방법은 제1 이미지의 특징맵을 획득하는 동일한 방식으로 제2 이미지의 특징맵을 획득 할 수 있다. 즉, 이미지 처리 방법은 변형 가능한 CNN을 사용하여 제2 이미지의 특징맵을 얻을 수 있다. 중복된 설명은 생략한다. 또한 선택적으로, 계산량을 줄이고 2차원 키 포인트 정보 추출의 정확성을 보장하기 위해, 제2 이미지의 특징맵은 단지 제2 이미지의 보다 높은 해상도의 특징맵일 수 있다. 예를 들어, 도 6과 같이, 이 미지 처리 방법은 계산 비용을 줄이고, 2차원 키 포인트 특징을 정확하게 추출하기 위하여, 오른쪽 어안 이미지 에서 고해상도 특징만 추출하여 고해상도 특징맵을 얻을 수 있다. 또한, 이미지 처리 방법은 제1 타겟 영역 특징맵을 공간적으로 변환하여, 변환된 제1 타겟 영역 특징맵을 생성 하는 동일한 방식으로, 제2 타겟 영역 특징맵을 공간적으로 변환하여, 변환된 제2 타겟 영역 특징맵을 생성할 수 있다. 이때, 중복된 설명은 생략한다. 유사하게, 이미지 처리 방법은 변환된 제2 타겟 영역 특징맵을 획득한 후, 도 6에 도시된 바와 같이, 변환 된 제2 타겟 영역 특징맵에 기초하여, 적어도 하나의 컨볼루션 신경망(628, 630)을 사용하여 객체의 제2 키 포인트 정보를 획득할 수 있다. 제2 키 포인트 정보를 획득한 후, 이미지 처리 방법은 초기 자세 정보, 제1 키 포인트 정보 및 제2 키 포인트 정보에 기초하여, 객체의 최종 자세 정보를 추정할 수 있다. 구체적으로, 예를 들어, 이 미지 처리 방법은 두 개의 이미지(제1 이미지와 제2 이미지)에서 객체의 2차원 키 포인트 재투영 오차의 합을 최소화함으로써, 객체의 최종 자세 정보를 결정할 수 있다. 즉, 640단계는 초기 자세를 최적화하는 것이다. 예를 들어, 오차를 최소화하는 최적화는 Lev-Mar 알고리즘과 같은 비선형 최적화 알고리즘을 사용할 수 있다. 구체적으로, 제1 이미지 및 제2 이미지가 왼쪽 어안 이미지 및 오른쪽 어안 이미지인 경우, 최종 자세를 추정하 는 것은 아래 <수학식 3>으로 표현될 수 있다: [수학식 3]"}
{"patent_id": "10-2022-0122436", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, 초기 자세 정보를 나타내는 회전 행렬(R)과 병진 벡터(T)가 왼쪽 어안 좌표계에 정의되어 있고, 왼쪽 어안 카메라의 타겟 영역의 가상 카메라 좌표계에서 왼쪽 어안 카메라 좌표계로의 변환을 [ , ]로 가 정하면, 왼쪽 어안 카메라의 좌표계에서의 초기 자세 정보의 회전 행렬 및 변환 벡터 는 아래 <수학식 4>와 같이 표현될 수 있다. [수학식 4]"}
{"patent_id": "10-2022-0122436", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, 오른쪽 어안 카메라의 타겟 영역 의가상 카메라 좌표계에서 오른쪽 어안 카메라 좌표계로의 변환은 [ , ]이고, 오른쪽 어안 카메라 좌표계에서의 초기 자세 정보의 회전 행렬 및 변환 벡터 는 아래 <수학식 5>와 같이 표현될 수 있다. [수학식 5]"}
{"patent_id": "10-2022-0122436", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서, [ ]는 초기 자세 정보와 오른쪽 어안 카메라 사이의 회전 및 병진 파라미터를 기반으로 얻은 초 기 자세 정보를 오른쪽 어안 좌표계에서 나타낸 것이다. Pi는 객체의 3차원 모델에 정의된 키 포인트이며, i=1, ..., N이다. 여기서, N은 객체의 키 포인트 수이다. 는 왼쪽 어안 이미지의 타겟 영역 가상 카메라 에서 추출한 i번째 키 포인트의 위치이고, 는 오른쪽 어안 이미지에서 추출한 i번째 키 포인트 위치이다. 또한, 및 는 각각 왼쪽 어안 카메라와 오른쪽 어안 카메라의 파라미터 행렬이다. 이상과 같이, 도 6을 참조하여 본 개시의 일 실시 예에 따른 구체적인 이미지 처리 방법, 즉 객체의 자세를 추 정하는 방법에 대해 설명하였다. 도 6에 도시된 객체 자세 추정 방법으로 객체의 자세를 더 정확하게 추정할 수 있다. 이상과 같이, 도 1 내지 도 6에 결합하여 본 개시의 일 실시 예에 따른 이미지 처리 방법 및 그 예시를 설명하 였다. 이미지 처리 방법은 이미지 처리 속도와 객체 처리 효과를 향상시킬 수 있다. 상술한 이미지 처리 방법에 대한 보다 명확한 이해를 돕기 위해, 도 7을 참조하여 전술된 이미지 처리 방법에 대해 간략하게 설명한다. 도 7은 일 실시 예에 따른 이미지 처리 방법에서 이미지를 처리하는 과정을 개략적으로 도시한 도면이다. 도 8 은 도 7의 예를 보다 구체화한 도면이다. 도 7의 예시에서, 제1 이미지는 왼쪽 어안 이미지이고, 제2 이미지는 오른쪽 어안 이미지이다. 이미 지 처리 방법은 왼쪽 어안 이미지를 기반으로, 앞서 설명한 변형 가능한 CNN으로 구성된 피라미드(도면 중 의 변형 가능 피라미드 네트워크에 대응됨)를 사용하여, 복수의 특징맵을 얻을 수 있다. 예를 들어, 도 8 과 같이 왼쪽 어안 이미지는 각 스케일에 따라 제1 이미지의 크기의 1/2 이미지, 1/4 이미지 및 1/8 이미지로 축소될 수 있다. 이미지 처리 방법은 상술한 변형 가능한 CNN을 기반으로 복수의 스케일된 이미지(811, 812, 813)에서 특징을 추 출 융합해서 3개의 특징맵(821, 822, 23)을 생성할 수 있다. 이미지 처리 방법은 3개의 특징맵(821, 822, 23) 중에서 두 개의 저해상도 특징맵(821, 823)에 기초하여 타겟 영역 제안 네트워크(예를 들어, 컨볼루션 신경망 등)를 사용하여, 왼쪽 어안 이미지 중의 타겟 영역 을 검출할 수 있다. 그후, 이미지 처리 방법은 검출된 객체 영역을 보정(도 7에서 \"변형을 방지하는 타겟 영역의 풀링\"이라고 함)할 수 있다. 예를 들어, 융합된 고해상도 특징맵에서 검출된 타겟 영역에 대응되는 타겟 영역 특징맵을 결정한 후, 어안 렌즈의 내부 파라미터(즉, 어안 이미지 이미징 모델의 파라미터)에 따라서 결정된 타겟 영역 특징맵을 공간적으로 변환하여 변환된 타겟 영역 특징맵을 생성할 수 있다. 그 다음, 이미지 처리 방법은 적어도 하나의 CNN(743, 744)을 사용하여, 객체 카테고리, 객체 마스크 맵, 2 차원 키 포인트 좌측도 , 초기화 자세와 같은 객체의 속성 정보를 얻을 수 있다. 이미지 처리 방법은 추정된 객체의 자세를 보다 정확하게 하기 위해, 도 7의 예시에서, 오른쪽 어안 이미지 를 기반으로 키 포인트 정보(도 7의 \"2차원 키 포인트(우측도)\")를 획득한 다음, 왼쪽 어안 이미지 에 따라 얻은 초기화 자세 및 2차원 키 포인트(좌측도)와 결합하여, 6Dof 자세를 추정할 수 있다. 구체적으로, 이미지 처리 방법은 도 7과 같이, 변형 가능한 CNN을 이용하여 오른쪽 어안 이미지 의 특징맵을 얻을 수도 있다. 예를 들어, 이미지 처리 방법은 도 8과 같이, 변형 가능한 CNN을 이용하여 오른쪽 어안 이미지의 고해상도 특징맵을 획득할 수 있다. 그리고, 이미지 처리 방법은 초기 자세 정보와 스 테레오 어안 렌즈의 내부 및 외부 파라미터를 기반으로 객체 영역의 투영을 수행하여 오른쪽 어안 이 미지 상의 객체에 대응되는 타겟 영역을 결정할 수 있다. 그리고, 이미지 처리 방법은 변형을 방지하는 타겟 영역의 풀링을 사용하여 해당 타겟 영역을 보정할 수 있다. 구체적으로, 이미지 처리 방법은 오른쪽 어안 이미지의 고해상도 특징맵에서 해당 타겟 영역에 대응되는 타겟 영역 특징맵을 결정하고, 이를 공간 적으로 변형하여, 변환된 타겟 영역 특징맵을 생성할 수 있다. 변환된 타겟 영역 특징맵은 적어도 하나의 CNN(772, 7736)에 입력되어, 최종적으로 2차원 키 포인트 우측도를 획득할 수 있다. 마지막으로, 이미지 처리 방법은 초기화 자세, 왼쪽 어안 이미지에서 획득한 2차원 키 포인트 좌측도 , 및 오른쪽 어안 이미지에서 획득한 2차원 키 포인트 우측도를 기반으로, 객체 자세를 최적화 할 수 있다. 도 7 및 도 8의 실시 예에서, 이미지 처리 방법은 왼쪽 어안 이미지에 대해 피라미드를 구성하였는데, 여기서 저해상도 특징맵(821, 822)은 타겟 영역 제안 네트워크(즉, 관심 영역 예측)에 사용 되고, 고해상도 특징맵은 2차원 키 포인트의 정확한 추출에 사용된다. 이미지 처리 방법은 왼쪽 어 안 이미지에서 이미 타겟 영역을 예측하였기 때문에, 오른쪽 어안 이미지에 대하여 고해상도의 특징 만을 계산한다. 이와 같이, 이미지 처리 방법은 계산량을 효과적으로 줄이고, 동시에 왼쪽 어안 이미지에 따라 획득한 2차원 키 포인트 정보와 결합하여 자세를 추정하기에, 자세 추정이 보다 정확하다. 키 포인 트 특징은 객체 6DoF 자세를 결정하는데 효과적이다. 도 7의 예시에서는 왼쪽 및 오른쪽 어안 이미지에서 키 포 인트를 추출하고, 키 포인트 재투영 오차를 최소화하여 자세 추정을 최적화한다. 도 7 및 도 8의 실시 예에 따르면, 이미지 처리 방법은 스테레오 어안 이미지를 이용하여 3차원 객체 분할 및 자세 추정을 빠르게 실현할 수 있다. 해당 기술은, 증강 현실 등과 같이, 환경에서 3차원 객체의 자세를 인식 하고, 상호 작용해야 하는 작업 시나리오에 사용될 수 있다. 예를 들어, 증강 현실 기술은 사용자 앞의 실제장면에 가상 콘텐츠를 추가하여 사용자에게 실감나는 정보 경험을 제공할 수 있다. 3차원 공간에서 사용자에게 고품질의 가상 현실 융합 효과를 제공하기 위하여, 증강 현실 시스템은 주변 사물의 3차원 상태에 대해, 높은 정밀도의 실시간 처리와 이해 능력을 구비하여야 할 수 있다. 다른 한편, 자율주행과 같은 시나리오에서, 환경 중 차량과 같은 물체에 대한 분할 및 자세 추정도 필요할 수 있다. 도 9는 일 실시 예에 따른 이미지 처리 방법이 적용되는 시나리오의 예를 도시한 도면이다. 도 9에 도시된 바와 같이, 사용자가 스테레오 어안 카메라가 장착된 증강 현실 안경을 착용한 상태에서, 일 실시 예에 따른 이미지 처리 방법은 스테레오 어안 이미지(왼쪽 어안 이미지 및 오른쪽 어안 이미지)를 기반 으로 스테레오 어안 이미지 중의 실제 객체(예를 들어, 테이블)의 3차원 자세를 추정할 수 있다. 증강 현실 안 경은 객체의 3차원 자세를 추정한 후에, 실제 객체의 3차원 자세에 따라, 가상의 3차원 그래픽 콘텐츠(예 를 들어, 엔진)를 실제 객체의 표면에 중첩 표시함으로써 사용자의 증강 현실 경험을 향상시킬 수 있다. 설명이 필요한 것은, 위의 설명에서, 도 1의 이미지 처리 방법을 설명함에 있어서, 변형 가능한 CNN을 사용하여 제1 이미지의 특징을 추출하여 제1 이미지의 특징맵을 획득하고, 획득한 특징맵에 기초하여, 제1 이미지 중의 타겟 영역을 검출하고, 타겟 영역을 보정한 후, 보정된 타겟 영역을 기반으로 객체를 처리하는 것을 설명하였다. 그러나, 이미지 처리 방법은 변형 가능한 CNN을 사용하여 객체의 변형이 존재하는 객체의 특징을 추출하여 특징맵을 획득한 후, 획득한 특징맵을 기반으로 객체를 직접 처리할 수도 있다. 즉, 이미지 처리 방 법은 앞서 설명한 변형 가능한 CNN을 단독으로 사용하여 객체 처리를 수행하는데, 이로부터 종래의 기술에서 객 체 변형이 있는 전체 이미지를 보정하는 시간 소모를 방지하고, 변형 가능한 CNN을 사용함으로써, 종래의 CNN으 로 객체 변형이 있는 이미지를 추출할 때 발생하는 샘플링 왜곡을 방지하여, 이미지 특징을 보다 정확하게 추출 하고, 후속 객체 처리 효과를 개선할 수 있다. 따라서, 본 개시의 다른 실시 예에 따르면, 도 10에 도시된 이미지 처리 방법도 제공될 수 있다. 도 10은 다른 실시 예에 따른 이미지 처리 방법의 흐름도이다. 도 10을 참조하면, 이미지 처리 방법은 1010단계에서, 컨볼루션 신경망을 이용하여 제1 이미지에 대해 컨볼루션 동작을 수행하여 제1 이미지의 특징맵을 획득할 수 있다. 이때, 컨볼루션 신경망은 제1 이미지 상의 적어도 하 나의 위치 중 각 위치에 대하여, 각 위치에 대응되는 컨볼루션 커널 함수를 이용하여 컨볼루션 동작을 수행할 수 있다. 예를 들어, 제1 이미지는 객체의 변형이 존재하는 이미지일 수 있다. 구체적으로, 1010단계에서, 이미지 처리 방법은 제1 이미지 상의 적어도 하나의 위치 중 각 위치에 대응되는 컨 볼루션 커널 함수의 샘플링 위치를 획득할 수 있다. 여기서, 컨볼루션 커널 함수의 샘플링 위치는 제1 이미지의 이미징 모델에 따라 결정될 수 있다. 다음으로, 이미지 처리 방법은 각 위치에 대응되는 컨볼루션 커널 함수의 샘플링 위치에 따라 컨볼루션 동작을 수행하여, 특징맵을 얻을 수 있다. 예를 들어, 컨볼루션 커널 함수의 샘 플링 위치는 이미징 모델에 따라, 3차원 공간에서 각 위치의 컨볼루션 커널 함수의 샘플링 위치를 결정하고, 3 차원 공간에서의 컨볼루션 커널 함수의 샘플링 위치 및 이미징 모델에 따라, 제1 이미지에서 각 위치에 대응되 는 컨볼루션 커널 함수의 샘플링 위치를 결정하는 방식을 통하여 결정될 수 있다. 변형 가능한 CNN이 수행하는 동작은 위의 설명에서 자세히 설명하였으므로 중복된 설명은 생략한다. 이미지 처리 방법은 1020단계에서, 특징맵에 기초하여 제1 이미지 중의 객체를 처리할 수 있다. 상술한 바와 같 이, 이미지 처리 방법은 특징맵을 획득한 후, 특징맵을 기반으로 제1 이미지의 타겟 영역을 검출하고, 검출된 타겟 영역을 보정하고, 보정된 타겟 영역을 기반으로 타겟 영역에 대응되는 객체를 처리할 수 있다. 동작의 자 세한 내용은 앞서 도 1의 이미지 처리 방법에서 언급되었기에, 중복된 설명은 생략한다. 도 10에 도시된 이미지 처리 방법은 전체 이미지를 보정하는 시간 소모를 줄이고, 종래의 CNN으로 이미지를 추 출할 때 발생하는 샘플링 왜곡을 피할 수 있으므로, 이미지를 보다 정확하게 추출하고, 후속된 이미지 처리 효 과를 개선할 수 있다. 도 11은 일 실시 예에 따른 이미지 처리 장치의 블록도이다. 도 11을 참조하면, 이미지 처리 장치는 검출부, 보정부 및 이미지 처리부을 포함할 수 있다. 구체적으로, 검출부는 제1 이미지의 특징맵을 획득하고, 특징맵을 기반으로 제1 이미지 중의 타겟영역을 검출할 수 있다. 보정부는 검출된 타겟 영역을 보정할 수 있다. 이미지 처리부는 보정된 타겟 영역에 기초하여 타겟 영역에 대응되는 객체를 처리할 수 있다. 도 1에 도시된 이미지 처리 방법은 도 11에 도시된 이미지 처리 장치에 의해 실행되고, 검출부에서 110단계를 수행하고, 보정부에서 120단계를 수행하고, 이미지 처리부에서 130단계를 수행할 수 있 다. 도 11의 검출부, 보정부 및 이미지 처리부이 수행하는 동작과 관련된 임의의 세부 사항 은 도 1의 설명을 참조할 수 있으므로, 중복된 설명은 생략한다. 도 12는 다른 실시 예에 따른 이미지 처리 장치의 블록도이다. 도 12를 참조하면, 이미지 처리 장치는 획득부 및 이미지 처리부를 포함할 수 있다. 구체적으로, 획득부는 컨볼루션 신경망을 이용하여 제1 이미지에 대해 컨볼루션 동작을 수행하여, 제1 이 미지의 특징맵을 획득할 수 있다. 여기서, 컨볼루션 신경망은 제1 이미지 상의 적어도 하나의 위치 중 각 위치 에 대하여, 각 위치에 대응되는 컨볼루션 커널 함수를 이용하여 컨볼루션 동작을 수행할 수 있다. 이미지 처리부는 상기 특징맵에 기초하여 제1 이미지 중의 객체를 처리할 수 있다. 도 10에 도시된 이미지 처리 방법은 도 12에 도시된 이미지 처리 장치에 의해 수행될 수 있다. 획득부 는 1010단계를 수행하고, 이미지 처리부는 1020 단계를 수행할 수 있다. 도 12의 획득부 및 이미지 처리부가 수행하는 동작과 관련된 임의의 세부 사항은 도 10의 설명을 참조할 수 있으므로, 중복된 설명은 생략한다. 또한, 이미지 처리 장치 및 이미지 처리 장치에서 대응되는 처리를 각각 수행하는 구성으로 구분하 여 설명하였지만, 이미지 처리 장치(1100, 1200)가 특정 구성으로 분할되지 않거나, 구성 사이에 명확한 경계가 없는 경우에도, 각 구성에서 수행하는 처리를 수행할 수 있는 것이 자명할 것이다. 또한, 이미지 처리 장치 및 이미지 처리 장치는 다른 구성, 예를 들어, 저장부 등을 더 포함할 수 있다. 도 13은 일 실시 예에 따른 전자 기기의 블록도이다. 도 13을 참조하면, 전자 기기는 적어도 하나의 메모리 및 적어도 하나의 프로세서를 포함할 수 있다. 적어도 하나의 메모리는 컴퓨터 실행가능 명령어를 저장하고, 적어도 하나의 프로세서에 의해 컴퓨터 실행가능 명령어가 실행됨으로써, 적어도 하나의 프로세서로 하여금 본 개시의 실시예의 이 미지 처리 방법을 실행하도록 할 수 있다. 상술한 이미지 처리 방법은 인공 지능 모델을 이용하여 수행될 수 있 다. 전술된 여러 구성들 중에서 적어도 하나는 AI(Artificial Intelligence) 모델을 통해 구현될 수 있다. AI와 관 련된 기능은 비 휘발성 메모리, 휘발성 메모리 및 프로세서를 통해 수행될 수 있다. 적어도 하나의 프로세서는 중앙 처리 유닛(CPU), 애플리케이션 프로세서(AP) 등과 같은 범용 프로세서이 거나, 그래픽 처리 유닛(GPU), 시각 처리 유닛(VPU) 및 /또는 신경망 처리 유닛(NPU)와 같은 AI 전용 프로세서 와 같은 순수 그래픽 처리 유닛일 수 있다. 적어도 하나의 프로세서는 비 휘발성 메모리 및 휘발성 메모리에 저장된 사전 정의된 동작 규칙 또는 인 공 지능(AI) 모델에 따라 입력 데이터의 처리를 제어할 수 있다. 적어도 하나의 프로세서는 훈련 또는 학습을 통해 미리 정해진 동작 규칙 또는 인공 지능 모델을 제공한다. 여기서, 학습을 통한 제공은 학습 알고 리즘을 여러 학습 데이터에 적용하여 미리 정의된 동작 규칙 또는 원하는 특성을 가진 AI 모델을 획득하는 것을 의미한다. 해당 학습은 실시예에 따른 AI가 수행되는 장치 자체에서 수행될 수 있고, 및/또는 별도의 서버/시 스템을 통해 구현될 수 있다. 학습 알고리즘은 다중 학습 데이터를 사용하여 미리 정해진 목표 장치(예, 로봇)를 훈련하여 목표 장치를 결정 하거나 예측하도록 만들거나 허용하거나 제어하는 방법이다. 해당 학습 알고리즘은 예를 들어, 지도 학습 (supervised learning), 비지도 학습, 반 지도 학습 또는 강화 학습을 포함할 수 있으나, 이에 한정되지않는다. 본 개시의 전자 기기에서 수행되는 이미지 처리 방법에서, 인공 지능 모델의 입력 데이터로서 입력 이미지를 이 용하여 타겟 영역을 처리한 후의 출력 이미지를 얻을 수 있다. AI 모델은 훈련을 통해 획득될 수 있다. 여기서, \"훈련을 통한 획득\"이란 훈련 알고리즘을 통해 다수의 훈련 데이터로 기본 AI 모델을 훈련시켜 원하는 특징(또는 목적)을 수행하도록 구성된 미리 정의된 동작 규칙 또는 AI 모델을 획득하는 것을 의미할 수 있다. 예를 들어, AI 모델에는 복수의 신경망 레이어가 포함될 수 있다. 복수의 신경망 레이어 각각은 복수의 가중치 값을 포함하고, 신경망 계산은 이전 레이어의 계산 결과와 복수의 가중치 값 사이의 계산에 의해 수행될 수 있 다. 신경망은 예를 들어, 컨볼루션 신경망(CNN), 심층 신경망(DNN), 순환 신경망(RNN), 제한된 볼츠만 머신 (RBM), 심층 신념 네트워크(DBN), 양방향 순환 심층 네트워크(BRDNN), 생성적 대립쌍 네트워크(GAN) 및 심층 Q 네트워크를 포함할 수 있으나, 이에 한정되지 않는다. 예를 들어, 전자 기기는 PC 컴퓨터, 태블릿 장치, 개인 휴대 정보 단말기, 스마트 폰, 또는 위의 명령어 세트를 실행할 수 있는 다른 장치일 수 있다. 여기서, 전자 기기는 단일 전자 기기일 필요는 없으며, 개별적으로 또는 공동으로 전술된 명령어(또는 명령어 세트)를 실행할 수 있는 임의의 장치 또는 회로의 집합일 수 있다. 전자 기기는 또한 통합 제어 시스템 또는 시스템 관리기의 일부일 수 있거나, 로컬로 또는 원격으로(예를 들어, 무선 전송을 통해) 인터페이스로 연결되는 휴대용 전자 기기로 구성될 수 있다. 전자 기기에서 프로세서는 중앙 처리 장치(CPU), 그래픽 처리 장치(GPU), 프로그램 가능 논리 장치, 특수 목적 프로세서 시스템, 마이크로컨트롤러 또는 마이크로프로세서를 포함할 수 있다. 프로세서는 또한 아날로그 프로 세서, 디지털 프로세서, 마이크로프로세서, 멀티코어 프로세서, 프로세서 어레이, 네트워크 프로세서 등을 포함 하되, 이에 한정되지 않는다. 프로세서는 메모리에 저장된 명령어 또는 코드를 실행할 수 있다. 여기서, 메모리는 데이터를 저장할 수 있다. 명령어 및 데이터는 또한 네트워크 인터페이스 장치를 통해 네트워크를 통해 송수신될 수 있으며, 여기서 네트 워크 인터페이스 장치는 임의의 알려진 전송 프로토콜을 채택할 수 있다. 메모리는 RAM이나 플래시 메모리를 집적회로 마이크로프로세서 내에 배치하는 등 방식으로 프로세서와 통합될 수 있다. 추가로, 메모리는 외부 디스크 드라이브, 저장 어레이, 또는 데이터베이스 시스템에 의해 사용될 수 있는 임의의 다른 저장 장치와 같은 별도의 장치를 포함할 수 있다. 메모리와 프로세서의 동작은 결합될 수 있 고, 또는 예를 들어, I/O 포트, 네트워크 접속 등을 통해 서로 통신하여, 프로세서가 메모리에 저장된 파일을 읽을 수 있도록 할 수 있다. 또한, 전자 기기는 비디오 디스플레이(예를 들어, 액정 디스플레이) 및 사용자 상호 작용 인터페이스(예를 들어, 키보드, 마우스, 터치 입력 장치 등)를 포함할 수도 있다. 전자 기기의 모든 구성요소는 버스 및/또는 네트워크를 통해 서로 연결될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 저장할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 실시예의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도 록 구성될 수 있으며, 그 역도 마찬가지이다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(signal wave)에 영구적으로,또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 하나 이상의 컴퓨터 판독 가능 기록 매 체에 저장될 수 있다."}
{"patent_id": "10-2022-0122436", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "이상과 같이 실시예들이 비록 한정된 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가진 자라면 상기를 기초로 다양한 기술적 수정 및 변형을 적용할 수 있다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 청구범위의 범위에 속한다."}
{"patent_id": "10-2022-0122436", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시 예에 따른 이미지 처리 방법의 흐름도이다. 도 2는 일 실시 예에 따른 이미지 처리 방법에서 변형 가능한 CNN을 사용하여 다중 스케일에서 제1 이미지의 특 징을 추출하는 개략적인 과정을 도시한 도면이다. 도 3은 일 실시 예에 따른 이미지 처리 방법에서 변형 가능한 CNN 컨볼루션 커널 함수의 샘플링 위치를 결정하 는 도면이다. 도 4는 일 실시 예에 따른 이미지 처리 방법에서 다중 스케일 특징을 융합하는 예를 도시한 도면이다. 도 5는 일 실시 예에 따른 이미지 처리 방법에서 변형을 방지하는 관심 영역의 풀링의 예를 도시한 도면이다. 도 6은 일 실시 예에 따른 이미지 처리 방법에서 객체의 자세를 추정하는 개략적인 과정을 도시한 도면이다. 도 7은 일 실시 예에 따른 이미지 처리 방법에서 이미지를 처리하는 과정을 개략적으로 도시한 도면이다. 도 8은 도 7의 예를 보다 구체화한 도면이다. 도 9는 일 실시 예에 따른 이미지 처리 방법이 적용되는 시나리오의 예를 도시한 도면이다. 도 10은 다른 실시 예에 따른 이미지 처리 방법의 흐름도이다. 도 11은 일 실시 예에 따른 이미지 처리 장치의 블록도이다. 도 12는 다른 실시 예에 따른 이미지 처리 장치의 블록도이다. 도 13은 일 실시 예에 따른 전자 기기의 블록도이다."}
