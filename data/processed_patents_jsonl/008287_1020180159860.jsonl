{"patent_id": "10-2018-0159860", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0075123", "출원번호": "10-2018-0159860", "발명의 명칭": "음성 기반 감정 인식 장치 및 방법", "출원인": "건국대학교 산학협력단", "발명자": "박능수"}}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 기반 감정 인식 방법으로서,(a) 음성 데이터로부터 추출된 복수의 음성 특징 벡터 각각을 인공신경망에 입력하는 단계; 및(b) 상기 복수의 음성 특징 벡터 각각에 대한 인공신경망의 출력값을 평균하고, 평균값에 대한 전연결 레이어의출력값을 기반으로 하여 상기 음성 데이터에 대한 감정을 인식하는 단계,를 포함하는 음성 기반 감정 인식 방법."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 (a) 단계는,(a1) 상기 음성 데이터를 복수의 구간으로 분할하는 단계;(a2) 상기 분할된 복수의 구간 각각에 대하여 음성 특징 벡터를 추출하는 단계; 및(a3) 상기 분할된 복수의 구간 각각에 대하여 추출된 음성 특징 벡터를 상기 인공신경망에 입력하는 단계,를 포함하는 음성 기반 감정 인식 방법."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 (a1) 단계에서, 상기 복수의 구간은 각각 동일한 길이 구간을 갖는 것인, 음성 기반 감정 인식 방법."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 (a3) 단계는,상기 분할된 복수의 구간에서 추출된 복수의 음성 특징 벡터 중 어느 하나의 음성 특징 벡터를 복수 개로 나누어 상기 인공신경망에 입력하는 것인, 음성 기반 감정 인식 방법."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 (a) 단계는,상기 복수의 음성 특징 벡터 각각을 상기 복수의 음성 특징 벡터 각각에 대응하는 인공신경망에 입력하는 것인,음성 기반 감정 인식 방법."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 (a) 단계에서, 상기 음성 데이터로부터 추출된 음성 특징 벡터는 MFCC, 전파(spread), 중심(centroid), 편평함(flatness) 및 에너지(energy) 중 적어도 하나의 유형으로 이루어진 것인, 음성 기반 감정 인식 방법."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,공개특허 10-2020-0075123-3-상기 (a) 단계에서, 상기 인공신경망은 단일 양방향 순환신경망(bi-directional RNN)인 것인, 음성 기반 감정인식 방법."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 (b) 단계는,상기 전연결 레이어의 출력값을 소프트맥스 레이어의 입력으로 하여 확률 값으로 변환한 이후, 가장 높은 확률값에 대응하는 감정을 상기 음성 데이터에 대한 감정으로 인식하는 것인, 음성 기반 감정 인식 방법."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "음성 기반 감정 인식 장치로서,음성 데이터로부터 추출된 복수의 음성 특징 벡터 각각을 인공신경망에 입력하는 전처리부; 및상기 복수의 음성 특징 벡터 각각에 대한 인공신경망의 출력값을 평균하고, 평균값에 대한 전연결 레이어의 출력값을 기반으로 하여 상기 음성 데이터에 대한 감정을 인식하는 인식부,를 포함하는 음성 기반 감정 인식 장치."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 전처리부는,상기 음성 데이터를 복수의 구간으로 분할하고, 상기 분할된 복수의 구간 각각에 대하여 음성 특징 벡터를 추출하고, 상기 분할된 복수의 구간 각각에 대하여 추출된 음성 특징 벡터를 상기 인공신경망에 입력하는 것인, 음성 기반 감정 인식 장치."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 복수의 구간은 각각 동일한 길이 구간을 갖는 것인, 음성 기반 감정 인식 장치."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,상기 인식부는,상기 전연결 레이어의 출력값을 소프트맥스 레이어의 입력으로 하여 확률 값으로 변환한 이후, 가장 높은 확률값에 대응하는 감정을 상기 음성 데이터에 대한 감정으로 인식하는 것인, 음성 기반 감정 인식 장치."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,상기 전처리부는 상기 복수의 음성 특징 벡터 각각을 상기 복수의 음성 특징 벡터 각각에 대응하는 인공신경망에 입력하는 것인, 음성 기반 감정 인식 장치."}
{"patent_id": "10-2018-0159860", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1항 내지 제8항 중 어느 한 항의 방법을 컴퓨터에서 실행하기 위한 프로그램을 기록한 컴퓨터에서 판독 가능한 기록매체."}
{"patent_id": "10-2018-0159860", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "음성 기반 감정 인식 방법에 관한 것이며, 음성 기반 감정 인식 방법은 음성 데이터로부터 추출된 복수의 음성 특징 벡터 각각을 인공신경망에 입력하는 단계; 및 (b) 상기 복수의 음성 특징 벡터 각각에 대한 인공신경망의 출력값을 평균하고, 평균값에 대한 전연결 레이어의 출력값을 기반으로 하여 상기 음성 데이터에 대한 감정을 인 식하는 단계를 포함할 수 있다."}
{"patent_id": "10-2018-0159860", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본원은 음성 기반 감정 인식 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2018-0159860", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "스마트폰이 보급되면서 상황 정보, 감정 정보 등의 사용자 정보를 활용한 다양한 개인화 서비스 연구가 활발히 진행 중이다. 특히, 감정 정보는 사용자의 현재 감정 상태를 나타내는 정보로서, 감정 상태에 따라 달라지는 음 악 추천과 같은 문화 콘텐츠 서비스, 콜센터나 메디컬 센터에서의 고객 감정 모니터링 등에 매우 유용하다. 음성 기반 감정 인식이란 사용자의 음성신호를 분석하여 사용자의 감정을 자동으로 인식하는 기술을 의미한다. 최근 마이크로폰 센서가 탑재된 스마트폰에서 사용자의 통화 음성 데이터 수집 및 처리가 용이해짐에 따라 감정 인식 기술 연구가 활발히 수행되고 있다. 스마트폰에서의 감정 기반 개인화 서비스를 제공하기 위해서는 통화 종료 후 사용자 감정을 통화 단위로 도출해 야 한다. 그러나, 종래의 음성 기반 감정 인식 기술은 수 초 정도의 작은 크기의 타임 윈도우에 대하여 수집한 음성 데이터로부터 사용자의 감정을 주기적으로 인식한다. 이와 같은 종래의 감정 인식 기술은 하나의 통화 이 벤트 전체에 대한 감정의 인식이 아닌 통화 중 특정 시간 동안의 감정을 인식하는 기술로서, 전체 통화 기간 동 안의 감정을 인식하기 어렵다. 또한, 하나의 감정으로 표현된 음성 파일을 짧은 음성 구간으로 나누어 인공지능을 통해 학습을 할 때 일부 짧 은 음성 구간이 다른 감정으로 표현된 파일의 짧은 음성 구간과 서로 유사한 경우를 가질 수 있다. 이를 이용해 학습을 할 경우 감정 판단이 서로 모호하여 인식률이 낮아지게 되는 문제가 있으며, 또한 이를 기반으로 한 파 일에서 다수로 인식되는 감정을 대표 감정으로 분류하여도 오차가 큰 문제가 있었다. 예를 들어, 사용자가 통화의 대부분을 화를 내며 대화를 하다가 통화를 종료하였다고 가정하자. 이러한 경우, 통화의 전체적인 감정은 '화남'이 될 수 있다. 그러나 통화를 짧은 구간으로 나누어 비교하면 일부 구간의 경우 \"평범\"한 통화의 짧은 구간과 유사한 경우가 있다. 따라서, 짧은 구간으로 나누어 인공지능 학습을 할 경우 이 와 같이 유사한 짧은 구간이 \"화남\" 통화에서는 \"화남\"으로 \"평범\" 통화에서는 \"평범\"으로 학습됨에 따라 학습 후 감정인식이 모호해지고 이로 인하여 인식의 오류를 범할 수 있다. 하나의 감정으로 표현되는 통화에도 다른 감정과 유사한 짧은 구간이 다수 개가 존재할 수 있어서 감정이 변화하는 것과 같은 현상을 나타낸다. 이처럼, 종래의 음성 기반 감정 인식 기술은 전체 통화 내에 짧은 음성 구간들의 감정 기복에 따른 학습 오류로 인하여 해당 음성데이터를 대표하는 적합한 감정을 인식하는 데에 한계가 있다. 따라서, 여러 가지 감정이 혼재되어 있는 통화 등의 음성 데이터의 전체적인 감정 상태를 고려하여 음성 데이터 에 대한 감정을 효과적으로 인식할 수 있는 기술이 요구된다. 또한, 종래에 감정 인식 기술은 음성과 영상을 결합하거나 영상만을 사용한 경우가 대부분이다. 이러한 영상을 이용한 감정 인식 기술은 영상의 크기가 큼으로 인해 휴대용 기기와 같은 제한된 용량을 가진 디바이스에서 사 용하기에 적합하지 않아 사용성이 떨어지는 문제가 있다. 이에 따라, 최근 음성 기반 인공지능 비서 서비스와 같이 음성을 이용한 서비스가 증가하고 있는바, 음성만을 이용한 감정 인식 기술의 개발이 요구된다. 본원의 배경이 되는 기술은 한국공개특허공보 제10-2012-0107033호에 개시되어 있다."}
{"patent_id": "10-2018-0159860", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본원은 전술한 종래 기술의 문제점을 해결하기 위한 것으로서, 여러 가지 감정이 혼재되어 있는 통화 등의 음성 데이터에서 시간 흐름에 따른 감정 상태를 고려하여 음성 데이터 전체에 대한 감정(즉, 음성 데이터 전체를 대 표하는 적합한 감정)을 효과적으로 인식할 수 있는 음성 기반 감정 인식 장치 및 방법을 제공하려는 것을 목적 으로 한다. 본원은 전술한 종래 기술의 문제점을 해결하기 위한 것으로서, 휴대용 기기와 같은 제한된 용량을 가진 디바이 스에서 실시간으로 사용자의 감정에 따른 감정 기반 서비스 제공을 가능하게 하는 음성 기반 감정 인식 장치 및 방법을 제공하려는 것을 목적으로 한다. 다만, 본원의 실시 예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2018-0159860", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본원의 일 실시예에 따른 음성 기반 감정 인식 방법은, (a) 음성 데이터로부터 추출된 복수의 음성 특징 벡터 각각을 인공신경망에 입력하는 단계; 및 (b) 상 기 복수의 음성 특징 벡터 각각에 대한 인공신경망의 출력값을 평균하고, 평균값에 대한 전연결 레이어의 출력 값을 기반으로 하여 상기 음성 데이터에 대한 감정을 인식하는 단계를 포함할 수 있다. 또한, 상기 (a) 단계는, (a1) 상기 음성 데이터를 복수의 구간으로 분할하는 단계; (a2) 상기 분할된 복수의 구 간 각각에 대하여 음성 특징 벡터를 추출하는 단계; 및 (a3) 상기 분할된 복수의 구간 각각에 대하여 추출된 음 성 특징 벡터를 상기 인공신경망에 입력하는 단계를 포함할 수 있다. 또한, 상기 (a1) 단계에서, 상기 복수의 구간은 각각 동일한 길이 구간을 가질 수 있다. 또한, 상기 (a3) 단계는, 상기 분할된 복수의 구간에서 추출된 복수의 음성 특징 벡터 중 어느 하나의 음성 특 징 벡터를 복수 개로 나누어 상기 인공신경망에 입력할 수 있다. 또한, 상기 (a) 단계는, 상기 복수의 음성 특징 벡터 각각을 상기 복수의 음성 특징 벡터 각각에 대응하는 인공 신경망에 입력할 수 있다. 또한, 상기 (a) 단계에서, 상기 음성 데이터로부터 추출된 음성 특징 벡터는 MFCC, 전파(spread), 중심 (centroid), 편평함(flatness) 및 에너지(energy) 중 적어도 하나의 유형으로 이루어질 수 있다. 또한, 상기 (a) 단계에서, 상기 인공신경망은 단일 양방향 순환신경망(bi-directional RNN)일 수 있다. 또한, 상기 (b) 단계는, 상기 전연결 레이어의 출력값을 소프트맥스 레이어의 입력으로 하여 확률 값으로 변환 한 이후, 가장 높은 확률 값에 대응하는 감정을 상기 음성 데이터에 대한 감정으로 인식할 수 있다. 한편, 본원의 일 실시예에 따른 음성 기반 감정 인식 장치는, 음성 데이터로부터 추출된 복수의 음성 특징 벡터 각각을 인공신경망에 입력하는 전처리부; 및 상기 복수의 음성 특징 벡터 각각에 대한 인공신경망의 출력값을 평균하고, 평균값에 대한 전연결 레이어의 출력값을 기반으로 하여 상기 음성 데이터에 대한 감정을 인식하는 인식부를 포함할 수 있다. 또한, 상기 전처리부는, 상기 음성 데이터를 복수의 구간으로 분할하고, 상기 분할된 복수의 구간 각각에 대하 여 음성 특징 벡터를 추출하고, 상기 분할된 복수의 구간 각각에 대하여 추출된 음성 특징 벡터를 상기 인공신 경망에 입력할 수 있다. 또한, 상기 복수의 구간은 각각 동일한 길이 구간을 가질 수 있다. 또한, 상기 인식부는, 상기 전연결 레이어의 출력값을 소프트맥스 레이어의 입력으로 하여 확률 값으로 변환한 이후, 가장 높은 확률 값에 대응하는 감정을 상기 음성 데이터에 대한 감정으로 인식할 수 있다. 상술한 과제 해결 수단은 단지 예시적인 것으로서, 본원을 제한하려는 의도로 해석되지 않아야 한다. 상술한 예 시적인 실시예 외에도, 도면 및 발명의 상세한 설명에 추가적인 실시예가 존재할 수 있다."}
{"patent_id": "10-2018-0159860", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본원의 과제 해결 수단에 의하면, 복수의 음성 특징 벡터 각각에 대한 인공신경망의 출력값을 평균하고, 평균값에 대한 전연결 레이어의 출력값을 기반으로 음성 데이터에 대한 감정 인식을 수행함으로써, 여러 가지 감정이 혼재되어 있는 음성 데이터에서 시간 흐름에 따른 감정 상태를 고려하여 음성 데이터 전체에 대한 감정 (즉, 음성 데이터 전체를 대표하는 적합한 감정)을 효과적으로 인식할 수 있다. 전술한 본원의 과제 해결 수단에 의하면, 휴대용 기기와 같은 제한된 용량을 가진 디바이스(휴대 단말)에서 실 시간으로 사용자의 감정을 효과적으로 인식할 수 있다. 다만, 본원에서 얻을 수 있는 효과는 상기된 바와 같은 효과들로 한정되지 않으며, 또 다른 효과들이 존재할 수 있다."}
{"patent_id": "10-2018-0159860", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본원이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본원의 실시예를 상세히 설명한다. 그러나 본원은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본원을 명확하게 설명하기 위해서 설명과 관계없는 부분 은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본원 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\" 또는 \"간접적으로 연결\"되어 있는 경우 도 포함한다. 본원 명세서 전체에서, 어떤 부재가 다른 부재 \"상에\", \"상부에\", \"상단에\", \"하에\", \"하부에\", \"하단에\" 위치 하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재가 존 재하는 경우도 포함한다. 본원 명세서 전체에서, 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 도 1은 본원의 일 실시예에 따른 음성 기반 감정 인식 장치의 개략적인 구성을 나타낸 도면이고, 도 2는 본원의 일 실시예에 따른 음성 기반 감정 인식 장치에 의한 음성 기반 감정 인식 과정을 설명하기 위한 도 면이다. 이하에서는 본원의 일 실시예에 따른 음성 기반 감정 인식 장치를 설명의 편의상 본 장치라 하기로 한다. 도 1 및 도 2를 참조하면, 본 장치는 음성 데이터 획득부, 전처리부 및 인식부를 포함할 수 있다. 음성 데이터 획득부는 음성 데이터를 획득할 수 있다. 여기서, 음성 데이터는 일예로 사용자 단말의 마이크를 통해 입력되어 기저장된 음성 데이터, 사용자 단말 의 마이크를 통해 실시간으로 입력되는 음성 데이터 등을 의미할 수 있다. 다만, 이에만 한정되는 것은 아니고, 음성 데이터는 다양한 음성 인식 수단(예를 들어, 마이크 등)을 통해 획득된 사용자의 음성 데이터(음성 정 보)를 의미할 수 있다. 사용자 단말은 예시적으로 휴대성과 이동성이 보장되는 이동 통신 장치로서, 예를 들면, PCS(Personal Communication System), GSM(Global System for Mobile communication), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(WCode Division Multiple Access), Wibro(Wireless Broadband Internet) 단말, 스마트폰(Smartphone), 스마트패드(SmartPad), 태블릿 PC, 노트북, 웨어러블 디바이스 등과 같은 모든 종류의 무선 통신 장치를 포함할 수 있으며, 이에만 한정되는 것은 아니다. 전처리부는 음성 데이터로부터 추출된 복수의 음성 특징 벡터(21, 22, 23, …, 2n) 각각을 인공신 경망에 입력할 수 있다. 전처리부는 음성 특징 벡터 추출부라 달리 표현될 수 있다. 전처리부는 음성 데이터를 복수의 구간(11, 12, 13, …, 1n)으로 분할할 수 있다. 이때, 분할된 복수 의 구간(11, 12, 13, …, 1n)은 각각 동일한 길이 구간을 가질 수 있다. 즉, 전처리부는 음성 데이터 를 동일한 길이를 갖는 복수의 구간(11, 12, 13, …, 1n)으로 분할할 수 있다. 이후, 전처리부는 분할된 복수의 구간(11, 12, 13, …, 1n) 각각에 대하여 음성 특징 벡터(21, 22, 23, …, 2n)를 추출할 수 있다. 구체적으로, 전처리부는 복수의 구간 중 제1 구간에 대하여(대응하여) 제1 음성 특징 벡터(21, FV x 0)를 추출할 수 있다. 또한, 전처리부는 제2 구간에 대하여 제2 음성 특징 벡터(22, FV x1)를 추출하 고, 제3 구간에 대하여 제3 음성 특징 벡터(23, FV x2)를 추출할 수 있다. 또한, 전처리부는 복수의 구간 중 제n 구간(1n)에 대하여(대응하여) 제n 음성 특징 벡터(2n, FV xf-1)를 추출할 수 있다.이처럼, 전처리부는 분할된 복수의 구간(11, 12, 13, …, 1n) 각각에 대하여 음성 특징 벡터를 추출함으로 써, 음성 데이터로부터 복수의 음성 특징 벡터(21, 22, 23, …, 2n)를 추출(획득)할 수 있다. 이때, 전처리부에 의해 음성 데이터로부터 추출된 음성 특징 벡터는 MFCC(Mel Frequent Coefficient Cepstral), 전파(spread), 중심(centroid), 편평함(flatness) 및 에너지(energy) 중 적어도 하나의 유형으로 이루어질 수 있다. 즉, 전처리부에 의해 추출된 복수의 음성 특징 벡터(21, 22, 23, …, 2n) 각각은 MFCC(Mel Frequent Coefficient Cepstral), 전파(spread), 중심(centroid), 편평함(flatness) 및 에너지 (energy) 중 적어도 하나의 유형으로 이루어질 수 있다. 여기서, 본원에서는 일예로 음성 특징 벡터의 유형으로서 MFCC(Mel Frequent Coefficient Cepstral), 전파 (spread), 중심(centroid), 편평함(flatness) 및 에너지(energy)가 고려되는 것으로 예시하였으나, 이에만 한 정되는 것은 아니고, 다양한 음성 특징 유형(다양한 음성 특징요소)가 고려될 수 있다. 이후, 전처리부는 분할된 복수의 구간(11, 12, 13, …, 1n) 각각에 대하여 추출된 음성 특징 벡터(즉, 복 수의 음성 특징 벡터)를 인공신경망에 입력할 수 있다. 여기서, 본원에서 고려되는 인공신경망은 신경망, 딥러닝 등으로 달리 표현될 수 있다. 또한, 본원에서 고 려되는 인공신경망은 일예로 단일 양방향 순환신경망(bi-directional Recurrent Neural Network, bi- directional RNN, BiRNN)일 수 있다. 일반적인 순환신경망(RNN, Recurrent Neural Network)은 음성 데이터의 한 시점에서 이전 데이터만 참조하여 학 습을 수행한다. 반면, 본원에서 고려되는 인공신경망인 단일 양방향 순환신경망(bi-directional RNN)은 음성 데 이터의 한 시점에서 이전 데이터와 다음 데이터를 모두 참조하여 학습을 수행하므로, 일반적인 순환신경망(RNN) 대비 앞뒤 발화에 모두 의존성이 존재하는 음성의 특징을 더 잘 반영할 수 있다. 따라서, 본원에서는 인공신경 망으로서 일반적인 순환신경망(RNN)이 아닌 단일 양방향 순환신경망(bi-directional RNN)을 고려할 수 있다. 다만, 이에만 한정되는 것은 아니고, 본원에서 고려되는 인공신경망은 합성곱 신경망(Convolution Neural Network, CNN), 순환신경망(RNN, Recurrent Neural Network), 딥 신경망(Deep Neural Network) 등 종래에 이미 공지되었거나 향후 개발되는 다양한 신경망이 적용될 수 있다. 전처리부는 복수의 음성 특징 벡터(21, 22, 23, …, 2n) 각각을 복수의 음성 특징 벡터 각각에 대응하는 인공신경망에 입력할 수 있다. 구체적으로, 인공신경망은 1개의 단일한 층(layer)로 이루어지고 복수의 인공신경망(31, 32, 33, …, 3n)을 포함할 수 있다. 여기서, 복수의 인공신경망(31, 32, 33, …, 3n) 중 제1 인공신경망은 제1 음성 특징 벡터(21, FV x0)에 대 응하는 인공신경망일 수 있다. 또한, 제2 인공신경망은 제2 음성 특징 벡터(22, FV x1)에 대응하는 인공신 경망일 수 있다. 제3 인공신경망은 제3 음성 특징 벡터(23, FV x2)에 대응하는 인공신경망일 수 있다. 제n 인공신경망(3n)은 제n 음성 특징 벡터(2n, FV xf-1)에 대응하는 인공신경망일 수 있다. 따라서, 전처리부는 제1 음성 특징 벡터를 제1 인공신경망에 입력하고, 제2 음성 특징 벡터를 제2 인공신경망에 입력할 수 있다. 또한, 전처리부는 제3 음성 특징 벡터를 제3 인공신경망에 입력하고, 제n 음성 특징 벡터(2n)를 제n 인공신경망(3n)에 입력할 수 있다. 이때, 전처리부는 인공신경망에 대한 음성 특징 벡터의 입력시, 분할된 복수의 구간에서 추출된 복수의 음 성 특징 벡터(21, 22, 23, …, 2n) 중 어느 하나의 음성 특징 벡터를 복수 개로 나누어 인공신경망에 입력할 수 있다. 여기서, 전처리부는 어느 하나의 음성 특징 벡터를 동일 길이로 복수 개로 나누어 인공신경망에 입 력할 수 있다. 여기서, 동일 길이는 일예로 0.3초에 대응하는 길이(간격)을 의미할 수 있으나, 이에 한정되는 것은 아니다. 구체적인 예로, 전처리부는 제1 구간에 대하여 제1 음성 특징 벡터를 추출한 이후, 추출된 제1 음 성 특징 벡터를 그에 대응하는 인공신경망인 제1 인공신경망에 입력할 수 있다. 이때, 전처리부는 제1 음성 특징 벡터를 추출한 이후, 추출된 제1 음성 특징 벡터를 포함하는 파일을 생성할 수 있다. 이 후, 전처리부는 생성된 제1 음성 특징 벡터를 포함하는 파일을 복수 개로 나누어 제1 인공신경망 에 입력할 수 있다. 일예로, 제1 음성 특징 벡터를 포함하는 파일이 3초에 대응하는 파일인 경우, 전처리부는 제1 음성 특징 벡터를 포함하는 파일을 예시적으로 0.3초 간격으로 복수 개로 나눌 수 있다. 이에 따라, 전처리부는 제1 인공신경망에 대한 입력값으로서, 제1 음성 특징 벡터를 포함하는 파일을 복수 개로 나눈 10개의 파일(즉, 0.3초에 대응하는 파일 10개)를 제공할 수 있다. 즉, 전처리부는 제1 음 성 특징 벡터를 포함하는 파일이 복수 개로 분할된 10개의 파일을 제1 인공신경망에 입력할 수 있다. 여기서, 이하 생략된 내용이라 하더라도 제1 음성 특징 벡터에 대하여 설명된 내용은 다른 음성 특징 벡터 (22, 23, …, 2n)에 대한 설명에도 동일하게 적용될 수 있다. 전처리부에 의해 복수의 음성 특징 벡터(21, 22, 23, …, 2n) 각각이 인공신경망에 입력된 이후, 인식부 는 복수의 음성 특징 벡터(21, 22, 23, …, 2n) 각각에 대한 인공신경망의 출력값(31', 32', 33', …, 3n')을 평균하고, 평균한 평균값에 대한 전연결 레이어(Fully Connected Layer, FC Layer, 50)의 출력값을 기 반으로 하여 음성 데이터에 대한 감정을 인식할 수 있다. 구체적으로, 전처리부는 복수의 음성 특징 벡터(21, 22, 23, …, 2n) 각각을 그에 대응하는 복수의 인공신 경망(31, 32, 33, …, 3n) 각각에 입력할 수 있다. 이에 따르면, 복수의 인공신경망(31, 32, 33, …, 3n)을 통 해 복수의 음성 특징 벡터(21, 22, 23, …, 2n) 각각에 대한 인공신경망의 출력값(31', 32', 33', …, 3n')이 획득될 수 있다. 즉, 인공신경망을 통하여 복수의 음성 특징 벡터의 수에 대응하는 수로 인공신경망의 출력 값이 획득될 수 있다. 인식부는 복수의 인공신경망(31, 32, 33, …, 3n) 각각으로부터 출력된 복수의 출력값(즉, 복수의 음성 특 징 벡터 각각에 대응하는 복수의 인공신경망 출력값)(31', 32', 33', …, 3n')을 AoT 풀링 레이어(Average Over Time Pooling Layer, 40)의 입력값으로 제공할 수 있다. AoT 풀링 레이어는 입력된 복수의 음성 특징 벡터 각각에 대한 인공신경망의 출력값(즉, 복수의 인공신경망 의 출력값)(31', 32', 33', …, 3n')을 더한 후 복수의 음성 특징 벡터의 수로 나눔으로써, 복수의 인공신경망 의 출력값(31', 32', 33', …, 3n')에 대한 평균값을 출력할 수 있다. 인식부는 AoT 풀링 레이어를 통해 출력된 평균값(즉, 복수의 음성 특징 벡터 각각에 대한 인공신경망 의 출력값에 대한 평균값)을 전연결 레이어의 입력값으로 제공할 수 있다. 이후, 인식부는 전연결 레 이어의 출력값을 소프트맥스 레이어(Softmax Layer, 60)의 입력값으로 제공할 수 있다. 이를 통해, 인식부는 전연결 레이어의 출력값을 소프트맥스 레이어의 입력으로 하여 확률 값으로 변환한 이후, 가장 높은 확률 값에 대응하는 감정(즉, 가장 높은 확률 값을 갖는 음성 특징 벡터에 대응하는 감 정)을 음성 데이터에 대한 감정으로 인식할 수 있다. 달리 말해, 인식부는 전연결 레이어의 출력값을 소프트맥스 레이어에 입력하여 확률 값으로 바꿀 수 있으며, 그 결과 가장 높은 확률 값에 대응하는 감정으로 음성 데이터를 분류할 수 있다. 인식부는 복수의 음성 특징 벡터에 대한 인공신경망의 출력값을 평균함으로써, 종래의 감정 인식 기술과 같이 특정 시간 동안의 감정만 인식하는 것이 아니라, 여러 가지 감정이 혼재되어 있는 음성 데이터에서 시간 흐름에 따른 감정 상태의 변화를 고려하여 음성 데이터 전체에 대한 감정(즉, 음성 데이터 전체를 대표하는 적합한 감정)을 효과적으로 인식할 수 있다. 다시 말해, 전처리부는 음성 데이터에 대한 전처리를 수행할 수 있다. 전처리시, 전처리부는 음 성 데이터를 동일한 길이 구간을 갖는 복수의 구간(11, 12, 13, …, 1n)으로 분할할 수 있다. 이후, 전처리 부는 복수의 구간(11, 12, 13, …, 1n) 각각에 음성 특징요소를 추출하는 함수를 적용함으로써, 복수의 구 간(11, 12, 13, …, 1n) 각각에 대한 음성 특징 벡터를 추출할 수 있다. 달리 표현하여, 전처리부는 복수 의 구간(11, 12, 13, …, 1n) 각각에 음성 특징요소를 추출하는 함수를 적용함으로써, 복수의 구간(11, 12, 13, …, 1n) 각각을 음성 특징 벡터로 변환할 수 있다. 이러한 전처리 과정에 의하면 전처리부는 복수의 구간 각각에 대하여 음성 특징 벡터를 추출할 수 있으며, 이를 통해 음성 데이터로부터 복수의 음성 특징 벡터 (21, 22, 23, …, 2n)를 추출(획득)할 수 있다. 전처리부는 복수의 음성 특징 벡터(21, 22, 23, …, 2n)를 추출한 이후, 일예로 추출된 복수의 음성 특징 벡터(21, 22, 23, …, 2n) 각각을 파일로 생성할 수 있다. 즉, 전처리부는 복수의 음성 특징 벡터(21, 22, 23, …, 2n)로부터 각 음성 특징 벡터를 포함하는 복수의 파일을 생성할 수 있다.이때, 생성된 복수의 파일에는 제1 음성 특징 벡터를 포함하는 제1 파일, 제2 음성 특징 벡터를 포함하는 제2 파일, 제n 음성 특징 벡 터(2n)를 포함하는 제n 파일 등이 포함될 수 있다. 전처리부는 복수의 파일을 인공신경망에 입력할 때, 신경망 학습을 위해 복수의 파일을 각각 동일 길이(일정 길이)로 복수 개로 나누어 인공신경망에 입력할 수있다. 달리 표현하여, 전처리부는 복수의 구간(11, 12, 13, …, 1n) 각각의 음성 데이터를 MFCC, 전파(spread), 중심(centroid), 편평함(flatness) 및 에너지(energy) 중 적어도 하나로 이루어진 음성 특징 벡터를 포함하는 파일(특징 값 파일)로 변환할 수 있다. 이에 따르면, 제1 구간의 음성 데이터에 대응하여 생성되는 제1 음 성 특징 벡터를 포함하는 제1 파일은 제1 특징 값 파일이라 달리 표현될 수 있다. 이때, 제1 파일(즉, 제1 특징 값 파일)은 동일 길이(예를 들어, 0.3초에 대응하는 길이)로 나누어 일예로 제1 인공신경망에 입력될 수 있다. 이후, 인식부는 복수의 음성 특징 벡터(21, 22, 23, …, 2n) 각각에 대한 인공신경망의 출력값(31', 32', 33', …, 3n')을 평균하고, 평균값에 대한 전연결 레이어의 출력값을 기반으로 하여 음성 데이터 에 대한 감정을 인식할 수 있다. 이때, 일반적인 RNN 학습에서는 RNN에서 나온 출력값을 그대로 사용하여 하나씩 전연결 레이어에 넣고 나온 값 을 이용해 손실(loss) 값을 계산한다. 즉, 일반적인 RNN 학습에서는 RNN에서 나온 출력값을 그대로 하나씩(개별 적으로) 전연결 레이어에 입력시킴에 따라, RNN의 출력값 각각에 대응하는 전연결 레이어의 출력값을 이용해 손 실 값을 계산한다. 이러한 일반적인 RNN 학습을 기반으로 한 종래의 감정 인식(분류) 기술은 음성 데이터의 각 구간마다 감정을 인식(판별)하기 때문에, 일부 구간에서 강하게 나타나는 감정에만 치우쳐서 감정을 분류하게 되는 문제가 있다. 이처럼, 일부 구간에서 강하게 나타나는 감정에 치우쳐 감정을 인식(즉, 특정 시간 동안의 감정만 인식)하는 종 래의 감정 인식 기술의 문제를 해소하고자, 본 장치의 인식부는 인공신경망의 출력값을 한 파일 단위마다 모아서 평균값을 계산한 다음, 계산된 평균값을 전연결 레이어의 입력값으로 제공할 수 있다. 즉, 본 장치의 인식부는 복수의 인공신경망(31, 32, 33, …, 3n) 각각의 출력값(31', 32', 33', …, 3n')을 개별적으로 전연결 레이어에 입력하는 것이 아니라, 복수의 인공신경망(31, 32, 33, …, 3n) 각각의 출력값(31', 32', 33', …, 3n')을 평균한 평균값을 산출하고, 산출된 평균값을 전연결 레이어의 입력값으 로 제공할 수 있다. 이처럼, 복수의 인공신경망(31, 32, 33, …, 3n) 각각의 출력값(31', 32', 33', …, 3n')에 대한 평균값을 전 연결 레이어의 입력으로 함으로써, 본 장치는 음성 데이터 전체의 시간 흐름을 고려하여, 특정 구 간에 대한 감정 인식이 아닌 음성 데이터 전체에 대한 감정 인식을 수행할 수 있다. 다시 말해, 본 장치는 음성 데이터를 복수의 구간으로 분할한 후 각 분할 구간마다 음성 특징 벡터를 추출할 수 있다. 이후 본 장치는 추출된 복수의 음성 특징 벡터(21, 22, 23, …, 2n)를 신경망 학습(심층 학습)을 위한 인공신경망에 입력할 수 있다. 이후, 본 장치는 인공신경망의 출력값(31', 32', 33', …, 3n')(달리 표현하여, 인공신경망을 통해 추출된 특징점)을 통합하여 평균값을 산출하고, 산출된 평균 값을 전연결 레이어에 입력함으로써 음성 데이터에 대응하는 사용자의 감정을 인식할 수 있다. 종래 감정 인식 기술은 음성의 짧은 구간 단위(짧은 음성 구간 단위)로 감정을 판단(인식)하거나, 구간 단위로 인식된 감정들 중 다수로 인식된 감정을 대표 감정으로 판단하였다. 그런데, 인공신경망을 통한 음성 감정의 학습시에는 음성 파일이 하나의 감정으로 분류(구분)되도록 학습이 이루어지게 되는데, 이때, 짧은 구간 단위 (짧은 음성 구간 단위)로 나누어 학습을 수행하면, 일부 짧은 음성 구간들이 서로 다른 감정임에도 불구하고 서 로 유사한 감정으로 인식(분류)되는 경우가 발생하게 된다. 즉, 종래의 감정 인식 기술의 경우, 짧은 구간 단위 에 대한 감정 판단이 서로 모호해져 감정 인식률이 낮아지게 되는 문제가 있다. 또한, 종래 감정 인식 기술의 경우에는 이를 기반으로 한 파일에서 다수로 인식되는 감정을 대표 감정으로 인식(판단, 분류)함에 따라, 오차 가 크고 정확성이 떨어지는 문제가 있다. 이에 반해, 본 장치는 음성 데이터에 대하여 시간 흐름에 따라 나타나는 복수의 음성 특징 벡터를 추 출하여 인공신경망의 입력으로 제공하고, 인공신경망의 출력값(인공신경망의 출력 특징점)을 통합해 평균하여 인공신경망의 출력값들에 대한 평균값을 산출하고, 산출된 평균값을 이용한 감정 인식을 수행함으로써, 음성 데 이터 전체에 대한 감정 인식이 가능하다. 즉, 본 장치는 시간 상에서 음성의 짧은 구간별(복수의 구간별) 추출된 음성 특징 벡터에 대한 인공신경망 의 출력값들을 평균하고, 평균값을 기반으로 음성 데이터에 대한 감정 인식을 수행함으로써, 시간 길이에 의존하지 않고(독립적으로) 음성 데이터의 전체 구간을 고려한 감정 인식이 가능하며, 종래 대비 인식 정확도를 향상시킬 수 있다. 본 장치는 사용자(발화자)의 음성에서 나타나는 감정을 효과적으로 인식할 수 있다. 이러한 본 장치 는 제한된 사용 환경(예를 들어, 제한된 용량을 가진 디바이스, 사용자 단말)에서도 실시간으로 사용자 개인의 감정을 인식할 수 있음에 따라, 실시간 개인 감정 기반의 서비스(미디어 서비스) 제공 기술 등에 효과적으로 적 용될 수 있다. 즉, 본 장치는 인공신경망을 이용하여 사용자의 음성(음성 데이터) 기반의 감정 인식(분 류)을 수행할 수 있다. 본 장치는 종래에 음성을 인식하여 의미 기반으로 분석을 수행하는 기술과는 달리, 음성의 특징만을 이용 하여 감정 인식(분류)를 수행하므로 언어의 유형과 상관없이 모든 언어에 대하여 효과적으로 적용될 수 있다. 본 장치는 음성이나 사용자(고객)의 감정 상태 파악이 중요하고 그것을 활용할 수 있는 서비스 분야(예를 들어, 음성 비서, 인공지능 기기, 텔레마케팅 분야 등), 의료계 분야(예를 들어, 정신과적 분석을 활용하는 분 야) 등에 적용될 수 있다. 이하에서는 상기에 자세히 설명된 내용을 기반으로, 본원의 동작 흐름을 간단히 살펴보기로 한다. 도 3은 본원의 일 실시예에 따른 음성 기반 감정 인식 방법에 대한 동작 흐름도이다. 도 3에 도시된 음성 기반 감정 인식 방법은 앞서 설명된 본 장치에 의하여 수행될 수 있다. 따라서, 이하 생략된 내용이라고 하더라도 본 장치에 대하여 설명된 내용은 음성 기반 감정 인식 방법에 대한 설명에도 동일하게 적용될 수 있다. 도 3을 참조하면, 단계S11에서는 음성 데이터를 획득할 수 있다. 다음으로, 단계S12에서는 음성 데이터로부터 추출된 복수의 음성 특징 벡터 각각을 인공신경망에 입력할 수 있 다. 또한, 단계S12은 음성 데이터를 복수의 구간으로 분할하는 단계(S12-1); 분할된 복수의 구간 각각에 대하여 음 성 특징 벡터를 추출하는 단계(S12-2); 및 분할된 복수의 구간 각각에 대하여 추출된 음성 특징 벡터를 인공신 경망에 입력하는 단계(S12-3)를 포함할 수 있다. 이때, 단계 S12-1에서, 복수의 구간은 각각 동일한 길이 구간을 가질 수 있다. 또한, 단계 S12-3에서는 분할된 복수의 구간에서 추출된 복수의 음성 특징 벡터 중 어느 하나의 음성 특징 벡터 를 복수 개로 나누어 인공신경망에 입력할 수 있다. 또한, 단계S12에서는 복수의 음성 특징 벡터 각각을 복수의 음성 특징 벡터 각각에 대응하는 인공신경망에 입력 할 수 있다. 또한, 단계S12에서, 음성 데이터로부터 추출된 음성 특징 벡터는 MFCC, 전파(spread), 중심(centroid), 편평함 (flatness) 및 에너지(energy) 중 적어도 하나의 유형으로 이루어질 수 있다. 또한, 단계S12에서, 인공신경망은 단일 양방향 순환신경망(bi-directional RNN)일 수 있다. 다음으로, 단계S13에서는 복수의 음성 특징 벡터 각각에 대한 인공신경망의 출력값을 평균하고, 평균값에 대한 전연결 레이어의 출력값을 기반으로 하여 음성 데이터에 대한 감정을 인식할 수 있다. 이때, 단계S13에서는 전연결 레이어의 출력값을 소프트맥스 레이어의 입력으로 하여 확률 값으로 변환한 이후, 가장 높은 확률 값에 대응하는 감정을 음성 데이터에 대한 감정으로 인식할 수 있다. 상술한 설명에서, 단계S11 내지 단계S13는 본원의 구현예에 따라서, 추가적인 단계들로 더 분할되거나, 더 적은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있 다. 본원의 일 실시 예에 따른 음성 기반 감정 인식 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령 은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자 기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명 령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈 로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 또한, 전술한 음성 기반 감정 인식 방법은 기록 매체에 저장되는 컴퓨터에 의해 실행되는 컴퓨터 프로그램 또는 애플리케이션의 형태로도 구현될 수 있다."}
{"patent_id": "10-2018-0159860", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본원의 설명은 예시를 위한 것이며, 본원이 속하는 기술분야의 통상의 지식을 가진 자는 본원의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본원의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본원의 범위에 포함되는 것으로 해 석되어야 한다."}
{"patent_id": "10-2018-0159860", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본원의 일 실시예에 따른 음성 기반 감정 인식 장치의 개략적인 구성을 나타낸 도면이다. 도 2는 본원의 일 실시예에 따른 음성 기반 감정 인식 장치에 의한 음성 기반 감정 인식 과정을 설명하기 위한 도면이다. 도 3은 본원의 일 실시예에 따른 음성 기반 감정 인식 방법에 대한 동작 흐름도이다."}
