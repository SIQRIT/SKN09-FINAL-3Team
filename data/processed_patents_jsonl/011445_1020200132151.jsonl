{"patent_id": "10-2020-0132151", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0048832", "출원번호": "10-2020-0132151", "발명의 명칭": "인공 신경망 프루닝 방법 및 장치", "출원인": "삼성전자주식회사", "발명자": "이원조"}}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "미리 학습된 인공 신경망에 의해 처리된 추론 작업에 대한 제1 작업 정확도를 획득하는 단계;상기 미리 학습된 인공 신경망을 구성하는 복수의 레이어들의 각각의 채널에 대응하는 채널 별 프루닝 파라미터(pruning parameter)에 기초하여, 미리 정해진 학습 비중에 따라 상기 채널에 속한 노드들 간의 웨이트들을 조정하여 상기 인공 신경망을 채널 단위로 프루닝하는 단계;상기 제1 작업 정확도 및 상기 프루닝된 인공 신경망의 작업 정확도에 기초하여, 상기 프루닝된 인공 신경망에대한 상기 학습 비중을 갱신하는 단계;상기 갱신된 학습 비중 및 상기 프루닝된 인공 신경망의 작업 정확도에 기초하여, 상기 채널 별 프루닝 파라미터를 갱신하는 단계; 및상기 갱신된 채널 별 프루닝 파라미터에 기초하여, 상기 갱신된 학습 비중에 따라 상기 프루닝된 인공 신경망을채널 단위로 재 프루닝하는 단계를 포함하는 인공 신경망 프루닝 방법."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 채널 별 프루닝 파라미터는프루닝의 임계값을 결정하는 제1 파라미터 및 상기 임계값을 전후로 가중치 변환 함수가 변화하는 기울기를 결정하는 제2 파라미터 중 적어도 하나를 포함하는, 인공 신경망 프루닝 방법."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 각각의 채널 중에서, 채널에 포함된 채널 요소들 중 0이 차지하는 비율이 임계값 이상인 채널을 프루닝하는 단계를 더 포함하는, 인공 신경망 프루닝 방법."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 학습 비중을 갱신하는 단계는상기 프루닝된 인공 신경망의 작업 정확도가 상기 제1 작업 정확도보다 작은 경우 상기 작업 정확도가 증가되도록 상기 학습 비중을 갱신하는 단계를 포함하는, 인공 신경망 프루닝 방법."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2022-0048832-2-제1항에 있어서,상기 재 프루닝된 인공 신경망의 작업 정확도 및 상기 재 프루닝된 인공 신경망에 대한 학습 비중을 결정하는프루닝-평가 동작을 반복적으로 수행하는 단계를 더 포함하는,"}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,미리 정의된 에폭(epoch) 및 상기 재 프루닝된 인공 신경망의 작업 정확도에 기초하여, 상기 프루닝-평가 동작을 추가적으로 수행할 것인지 여부를 결정하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서,상기 프루닝-평가 동작이 반복적으로 수행됨에 따라, 상기 결정된 학습 비중을 학습 비중의 하한 임계 값과 비교하는 단계; 및상기 비교 결과에 기초하여, 현재 프루닝 세션을 종료하고 상기 학습 비중이 초기 기준 값으로 설정된 다음 프루닝 세션을 개시할 것인지 여부를 판단하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항 내지 제7항 중 어느 한 항의 방법을 수행하기 위한 명령어들을 포함하는 하나 이상의 프로그램을 저장한컴퓨터 판독 가능 저장매체."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "인공 신경망을 프루닝하는 장치에 있어서,적어도 하나의 프로그램이 저장된 메모리; 및상기 적어도 하나의 프로그램을 실행함으로써 상기 인공 신경망을 프루닝하는 동작을 처리하는 제어부를 포함하고,상기 제어부는,미리 학습된 인공 신경망에 의해 처리된 추론 작업에 대한 제1 작업 정확도를 획득하고, 상기 미리 학습된 인공신경망을 구성하는 복수의 레이어들의 각각의 채널에 대응하는 채널 별 프루닝 파라미터(pruning parameter)에기초하여, 미리 정해진 학습 비중에 따라 상기 채널에 속한 노드들 간의 웨이트들을 조정하여 상기 인공 신경망을 채널 단위로 프루닝하고, 상기 제1 작업 정확도 및 상기 프루닝된 인공 신경망의 작업 정확도에 기초하여,상기 프루닝된 인공 신경망에 대한 상기 학습 비중을 갱신하고, 상기 갱신된 학습 비중 및 상기 프루닝된 인공신경망의 작업 정확도에 기초하여, 상기 채널 별 프루닝 파라미터를 갱신하고, 상기 갱신된 채널 별 프루닝 파라미터에 기초하여, 상기 갱신된 학습 비중에 따라 상기 프루닝된 인공 신경망을 채널 단위로 재 프루닝하는 인공 신경망 프루닝 장치."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 채널 별 프루닝 파라미터는공개특허 10-2022-0048832-3-프루닝의 임계값을 결정하는 제1 파라미터를 포함하는, 인공 신경망 프루닝 장치."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 제어부는상기 각각의 채널 중에서, 채널에 포함된 채널 요소들 중 0이 차지하는 비율이 임계값 이상인 채널을 프루닝하는, 인공 신경망 프루닝 장치."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,상기 제어부는상기 프루닝된 인공 신경망의 작업 정확도가 상기 제1 작업 정확도보다 작은 경우 상기 작업 정확도가 증가되도록 상기 학습 비중을 갱신하는, 인공 신경망 프루닝 장치."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,상기 제어부는상기 재 프루닝된 인공 신경망의 작업 정확도 및 상기 재 프루닝된 인공 신경망에 대한 학습 비중을 결정하는프루닝-평가 동작을 반복적으로 수행하는, 인공 신경망 프루닝 장치."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 제어부는미리 정의된 에폭(epoch) 및 상기 재 프루닝된 인공 신경망의 작업 정확도에 기초하여, 상기 프루닝-평가 동작을 추가적으로 수행할 것인지 여부를 결정하는, 인공 신경망 프루닝 장치."}
{"patent_id": "10-2020-0132151", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13항에 있어서,상기 제어부는상기 프루닝-평가 동작이 반복적으로 수행됨에 따라, 상기 결정된 학습 비중을 학습 비중의 하한 임계 값과 비교하고, 상기 비교 결과에 기초하여, 현재 압축 세션을 종료하고 상기 학습 비중이 초기 기준 값으로 설정된 다음 프루닝 세션을 개시할 것인지 여부를 판단하는, 인공 신경망 프루닝 장치."}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공 신경망 프루닝 방법 및 장치가 개시된다. 일 실시예에 따른 인공 신경망 프루닝 방법은 미리 학습된 인공 신경망에 의해 처리된 추론 작업에 대한 제1 작업 정확도를 획득하는 단계, 미리 학습된 인공 신경망을 구성하는 복수의 레이어들의 각각의 채널에 대응하는 채널 별 프루닝 파라미터(pruning parameter)에 기초하여, 미리 정해 진 학습 비중에 따라 채널들에 속한 노드들 간의 웨이트들을 조정하여 인공 신경망을 채널 단위로 프루닝하는 단 계, 제1 작업 정확도 및 프루닝된 인공 신경망의 작업 정확도에 기초하여, 프루닝된 인공 신경망에 대한 학습 비 중을 갱신하는 단계, 갱신된 학습 비중 및 프루닝된 인공 신경망의 작업 정확도에 기초하여, 채널 별 프루닝 파 라미터를 갱신하는 단계 및 갱신된 채널 별 프루닝 파라미터에 기초하여, 갱신된 학습 비중에 따라 프루닝된 인 공 신경망을 채널 단위로 재 프루닝하는 단계를 포함한다."}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "아래 실시예들은 인공 신경망 프루닝 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 신경망(neural network)은 생물학적 뇌를 모델링한 컴퓨터 과학적 아키텍쳐(computational architectur e)를 참조한다. 최근 인공 신경망 기술이 발전함에 따라, 다양한 종류의 전자 시스템에서 인공 신경망 장치를 사용하여 입력 데이터를 분석하고 유효한 정보를 추출하는 연구가 활발히 진행되고 있다. 인공 신경망 장치는 복잡한 입력 데이터에 대한 많은 양의 연산을 필요로 한다. 한편, 인공 신경망의 학습 양이 증가함에 따라, 인공 신경망을 구성하는 연결성이 복잡해지고, 과거의 학습 데이터에 대하여 정확도가 증가하나 새로운 데이터에 대한 예측 값의 신뢰성이 저하되는 과적합(Over Fitting) 문제가 발생한다. 또한, 인공 신경망 의 복잡도가 증가하고, 이로 인하여, 메모리 할당량이 과도하게 증가함에 따라, 소형화 및 상용화에 있어서 문 제가 발생한다. 따라서, 인공 신경망의 성능을 유지하면서 인공 신경망의 구현에 있어서 시스템 비용을 감소시키기 위한 압축 방법이 요구된다."}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 인공 신경망 프루닝 방법은 미리 학습된 인공 신경망에 의해 처리된 추론 작업에 대한 제1 작 업 정확도를 획득하는 단계; 상기 미리 학습된 인공 신경망을 구성하는 복수의 레이어들의 각각의 채널에 대응 하는 채널 별 프루닝 파라미터(pruning parameter)에 기초하여, 미리 정해진 학습 비중 따라 상기 채널에 속한 노드들 간의 웨이트들을 조정하여 상기 인공 신경망을 채널 단위로 프루닝하는 단계; 상기 제1 작업 정확도 및 상기 프루닝된 인공 신경망의 작업 정확도에 기초하여, 상기 프루닝된 인공 신경망에 대한 상기 학습 비중을 갱 신하는 단계; 상기 갱신된 학습 비중 및 상기 프루닝된 인공 신경망의 작업 정확도에 기초하여, 상기 채널 별 프루닝 파라미터를 갱신하는 단계; 및 상기 갱신된 채널 별 프루닝 파라미터에 기초하여, 상기 갱신된 학습 비 중에 따라 상기 프루닝된 인공 신경망을 채널 단위로 재 프루닝하는 단계를 포함한다. 상기 채널 별 프루닝 파라미터는 프루닝의 임계값을 결정하는 제1 파라미터 및 상기 임계값을 전후로 가중치 변 환 함수가 변화하는 기울기를 결정하는 제2 파라미터 중 적어도 하나를 포함할 수 있다. 일 실시예에 따른 인공 신경망 프루닝 방법은 상기 각각의 채널 중에서, 채널에 포함된 채널 요소들 중 0이 차 지하는 비율이 임계값 이상인 채널을 프루닝하는 단계를 더 포할 수 있다. 상기 학습 비중을 갱신하는 단계는 상기 프루닝된 인공 신경망의 작업 정확도가 상기 제1 작업 정확도보다 작은 경우 상기 작업 정확도가 증가되도록 상기 학습 비중을 갱신하는 단계를 포함할 수 있다. 일 실시예에 따른 인공 신경망 프루닝 방법은 상기 재 프루닝된 인공 신경망의 작업 정확도 및 상기 재 프루닝 된 인공 신경망에 대한 학습 비중을 결정하는 프루닝-평가 동작을 반복적으로 수행하는 단계를 더 포함할 수 있 다. 일 실시예에 따른 인공 신경망 프루닝 방법은 미리 정의된 에폭(ecoch) 및 상기 재 프루닝된 인공 신경망의 작 업 정확도에 기초하여, 상기 프루닝-평가 동작을 추가적으로 수행할 것인지 여부를 결정하는 단계를 더 포함할 수 있다. 일 실시예에 따른 인공 신경망 프루닝 방법은 상기 프루닝-평가 동작이 반복적으로 수행됨에 따라, 상기 결정된 학습 비중을 학습 비중의 하한 임계 값과 비교하는 단계; 및 상기 비교 결과에 기초하여, 현재 프루닝 세션을 종료하고 상기 학습 비중이 초기 기준 값으로 설정된 다음 프루닝 세션을 개시할 것인지 여부를 판단하는 단계 를 더 포함할 수 있다. 일 실시예에 따른 인공 신경망을 압축하는 장치는 적어도 하나의 프로그램이 저장된 메모리; 및 상기 적어도 하나의 프로그램을 실행함으로써 상기 인공 신경망을 압축하는 동작을 처리하는 제어부를 포함하고, 상기 제어 부는, 미리 학습된 인공 신경망에 의해 처리된 추론 작업에 대한 제1 작업 정확도를 획득하고, 상기 미리 학습 된 인공 신경망을 구성하는 복수의 레이어들의 각각의 채널에 대응하는 채널 별 프루닝 파라미터(pruning parameter)에 기초하여, 미리 정해진 학습 비중에 따라 상기 채널에 속한 노드들 간의 웨이트들을 조정하여 상 기 인공 신경망을 채널 단위로 프루닝하고, 상기 제1 작업 정확도 및 상기 프루닝된 인공 신경망의 작업 정확도에 기초하여, 상기 프루닝된 인공 신경망에 대한 상기 학습 비중을 갱신하고, 상기 갱신된 학습 비중 및 상기 프루닝된 인공 신경망의 작업 정확도에 기초하여, 상기 채널 별 프루닝 파라미터를 갱신하고, 상기 갱신된 채널 별 프루닝 파라미터에 기초하여, 상기 갱신된 학습 비중에 따라 상기 프루닝된 인공 신경망을 채널 단위로 재 프루닝한다. 상기 채널 별 프루닝 파라미터는 프루닝의 임계값을 결정하는 제1 파라미터 및 상기 임계값을 전후로 가중치 변 환 함수가 변화하는 기울기를 결정하는 제2 파라미터 중 적어도 하나를 포함할 수 있다. 상기 제어부는 상기 각각의 채널 중에서, 채널에 포함된 채널 요소들 중 0이 차지하는 비율이 임계값 이상인 채 널을 프루닝할 수 있다. 상기 제어부는 상기 프루닝된 인공 신경망의 작업 정확도가 상기 제1 작업 정확도보다 작은 경우 상기 작업 정 확도가 증가되도록 상기 학습 비중을 갱신할 수 있다. 상기 제어부는 상기 재 프루닝된 인공 신경망의 작업 정확도 및 상기 재 프루닝된 인공 신경망에 대한 학습 비 중을 결정하는 프루닝-평가 동작을 반복적으로 수행할 수 있다. 상기 제어부는 미리 정의된 에폭 및 상기 재 프루닝된 인공 신경망의 작업 정확도에 기초하여, 상기 프루닝-평 가 동작을 추가적으로 수행할 것인지 여부를 결정할 수 있다. 상기 제어부는 상기 프루닝-평가 동작이 반복적으로 수행됨에 따라, 상기 결정된 학습 비중을 학습 비중의 하한 임계 값과 비교하고, 상기 비교 결과에 기초하여, 현재 프루닝 세션을 종료하고 상기 학습 비중이 초기 기준 값 으로 설정된 다음 프루닝 세션을 개시할 것인지 여부를 판단할 수 있다."}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서 개시되어 있는 특정한 구조적 또는 기능적 내용들은 단지 기술적 개념에 따른 실시예들을 설명하 기 위한 목적으로 예시된 것으로서, 다양한 다른 형태로 실시될 수 있으며 설명된 구조 또는 기능에 한정되지 않는다. 제1 또는 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 이런 용어들은 하나의 구성요소 를 다른 구성요소로부터 구별하는 목적으로만 이해되어야 한다. 예를 들어 제1 구성요소는 제2 구성요소로 명명 될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관계를 설명하는 표현들, 예를 들어 \"~간의에\"와 \"바로~간의에\" 또는 \"~에 이웃하는\"과 \"~에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 설시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함 을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 해당 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적으로 사용되 는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미를 갖는 것으로 해석되어야 하며, 본 명세서에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 실시예들은 데이터 센터, 서버, 퍼스널 컴퓨터, 랩톱 컴퓨터, 태블릿 컴퓨터, 스마트 폰, 텔레비전, 스마트 가 전 기기, 지능형 자동차, 키오스크, 웨어러블 장치 등 다양한 형태의 제품으로 구현될 수 있다. 이하, 실시예 들을 첨부된 도면을 참조하여 상세하게 설명한다. 각 도면에 제시된 동일한 참조 부호는 동일한 부재를 나타낸 다. 도 1는 일 실시예에 따른 인공 신경망에서 수행되는 연산을 설명하기 위한 도면이다. 인공 신경망이란 사람의 두뇌가 정보를 처리하는 방식을 모방한 연산 시스템이다. 심층 신경망(Deep Neural Network)은 인공 신경망을 구현하는 하나의 방식으로서, 복수의 레이어(layer)를 포함 할 수 있다. 예를 들어, 심층 신경망은 입력 데이터가 인가되는 입력 레이어(Input Layer), 학습을 바탕으로 입력 데이터에 기반한 예측을 통해 도출된 결과 값을 출력하는 출력 레이어(Output Layer) 및 입력 레이어와 출 력 레이어 사이에 다중의 은닉 레이어(Hidden Layer)을 포함한다. 심층 신경망은 정보를 처리하기 위해 이용되는 알고리즘에 따라, 컨볼루션 신경망(Convolutional Neural Network), 리커런트 신경망(Recurrent Neural Network) 등으로 분류된다. 인공 신경망을 학습하는 방식을 딥러닝(Deep Learning)이라 하며, 상술한 바와 같이 딥러닝에는 컨볼루션 신경 망, 리커런트 신경망 방식과 같이 다양한 알고리즘이 이용될 수 있다. 이 때, 인공 신경망을 학습한다는 것은 계층간 가중치 및 바이어스 또는 인접한 계층 중 서로 다른 레이어에 속 하는 복수의 뉴런들간의 가중치 및 바이어스를 결정하고 갱신하는 것을 나타낼 수 있다. 예를 들어, 복수의 계층적 구조 및 복수의 레이어들, 또는, 뉴런들 간의 가중치 및 바이어스를 총칭하여 인공 신경망의 연결성(connectivity)이라 할 수 있다. 따라서, 인공 신경망을 학습한다는 것은 연결성을 구축하고 학습하는 것을 나타낼 수 있다. 도 1를 참조하면, 인공 신경망은 입력 레이어, 히든 레이어들 및 출력 레이어를 포함하는 구조를 가지며, 수신되는 입력 데이터(예를 들어, I1 및 I1)를 기초로 연산을 수행하고, 수행 결과를 기초로 출력 데이터(예를 들어, O1 및 O1)를 생성할 수 있다. 인공 신경망은 앞서 설명된 바와 같이, 1개 이상의 히든 레이어들을 포함하는 DNN 또는 n-계층 인공 신경 망일 수 있다. 예를 들어, 도 1에 도시된 바와 같이, 인공 신경망은 입력 레이어(Layer 1), 1개의 히든 레이어들(Layer 1 및 Layer 3) 및 출력 레이어(Layer 4)를 포함하는 DNN일 수 있다. DNN은 Convolutional Neural Networks(CNN), Recurrent Neural Networks(RNN), Deep Belief Networks, Restricted Boltzman Machines 등을 포함할 수 있으나, 이에 제한되지 않는다. 인공 신경망이 DNN 아키텍처로 구현된 경우 유효한 정보를 처리할 수 있는 보다 많은 레이어들을 포함하므 로, 인공 신경망은 싱글 레이어를 갖는 인공 신경망보다 복잡한 데이터 집합들을 처리할 수 있다. 한편, 인공 신경망은 4개의 레이어들을 포함하는 것으로 도시되어 있으나, 이는 예시에 불과할 뿐 인공 신경망 은 더 적거나 많은 레이어들을 포함하거나, 더 적거나 많은 채널들을 포함할 수 있다. 즉, 인공 신경망 은 도 1에 도시된 것과는 다른, 다양한 구조의 레이어들을 포함할 수 있다. 인공 신경망에 포함된 레이어들 각각은 복수의 채널들을 포함할 수 있다. 채널은 뉴런(neuron), 프로세싱 엘리먼트(Processing element, PE), 유닛(unit) 또는 이와 유사한 용어들로 알려진, 복수의 인공 노드(artificial node)들에 해당될 수 있다. 예를 들어, 도 1에 도시된 바와 같이, Layer 1은 1개의 채널들(노드들), Layer 1 및 Layer 3 각각은 3개의 채널들을 포함할 수 있다. 다만, 이는 예시에 불과할 뿐 인 공 신경망에 포함된 레이어들 각각은 다양한 개수의 채널들(노드들)을 포함할 수 있다. 인공 신경망의 레이어들 각각에 포함된 채널들은 서로 연결되어 데이터를 처리할 수 있다. 예를 들어, 하 나의 채널은 다른 채널들로부터 데이터를 수신하여 연산할 수 있고, 연산 결과를 또 다른 채널들로 출력할 수 있다. 채널들 각각의 입력 및 출력 각각은 입력 액티베이션 및 출력 액티베이션이라고 지칭될 수 있다. 즉, 액티베이 션은 한 채널의 출력임과 동시에, 다음 레이어에 포함된 채널들의 입력에 해당되는 파라미터일 수 있다. 한편, 채널들 각각은 이전 레이어에 포함된 채널들로부터 수신된 액티베이션들 및 웨이트 및 바이어스에 기초하여 자 신의 액티베이션을 결정할 수 있다. 웨이트는 각 채널에서의 출력 액티베이션을 계산하기 위해 이용되는 파라 미터로서, 채널들 간의 연결관계에 할당되는 값일 수 있다. 채널들 각각은 입력을 수신하여 출력 액티베이션을 출력하는 연산 유닛(computational unit) 또는 프로세싱 엘 리먼트(processing element)에 의해 처리될 수 있고, 채널들 각각의 입력-출력은 매핑될 수 있다. 예를 들어, σ는 활성화 함수(activation function)이고, 는 (i-1) 번째 레이어에 포함된 k 번째 노드로부터 i 번째 레이어에 포함된 j번째 노드로의 웨이트이며, 는 i 번째 레이어에 포함된 j 번째 노드의 바이어스(bias) 값이 고, 는 i 번째 레이어의 j 번째 노드의 액티베이션이라고 할 때, 액티베이션 는 다음과 같은 수학식 1을 따 를 수 있다. 수학식 1"}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 2, "content": "도 1에 도시된 바와 같이, 1번째 레이어(Layer 1)의 첫 번째 채널(CH 1)의 액티베이션은 로 표현될 수 있 다. 또한, 은 수학식 1에 따라 의 값을 가질 수 있다. 다만, 앞서 설명한 수학식 1은 인공 신경망에서 데이터를 처리하기 위해 이용되는 액티베이션 및 웨이트 및 바이 어스를 설명하기 위한 예시일 뿐, 이에 제한되지 않는다. 액티베이션은 이전 레이어로부터 수신된 액티베이션 들의 가중치 합(weighted sum)을 sigmoid 함수나 Rectified Linear Unit (ReLU) 함수 등의 액티베이션 함수에 통과시킴으로써 획득된 값일 수 있다. 도 2는 일 실시예에 따른 프루닝(pruning)을 설명하기 위한 도면이다. 도 2에서, 인공 신경망은 프루닝이 수행되기 전 미리 학습된 인공 신경망의 일부 레이어를 도시한 도면이 다. 또한, 인공 신경망은 프루닝이 수행된 인공 신경망의 대응하는 일부 레이어를 도시한 도면이다. 도 2를 참조하면 인공 신경망에서 인접한 두 개의 서로 다른 레이어의 3개의 채널들에 포함된 모든 2 개의 뉴런 조합들간에 연결 관계가 형성되어 있다. 도 2에서 미리 학습된 인공 신경망이 완전히 연결(fully-connected)되어, 인공 신경망에 포함된 인접한 서 로 다른 레이어에 속한 임의의 두 개의 뉴런들 간의 연결 강도를 나타내는 가중치가 0보다 큰 값일 수 있다. 모든 인접 레이어의 뉴런들 간의 연결성이 있는 경우, 인공 신경망 전체의 복잡도가 증가한다. 또한, 인공 신 경망의 예측 결과가 과적합(over fitting)으로 인하여 정확성 및 신뢰성이 감소할 수 있다. 이를 고려하여, 인공 신경망에 대한 프루닝이 수행될 수 있다. 예를 들어, 도 2에서, 프루닝 되기 전 인공 신 경망에서, 복수의 가중치들 중에서 가중치가 소정의 임계 값 이하인 경우, 해당 가중치를 약화 또는 제거 하는 프루닝이 수행될 수 있다. 프루닝될 수 있는 인공 신경망의 일부를 결정하기 위해, 인공 신경망이 탐색될 수 있다. 이 때, 인공 신경망의 정확도를 실질적으로 손상시키지 않는 인공 신경망 파라미터들의 부분들 또는 레이어의 각 채널들을, 제거하거 나 감소시킴으로써 인공 신경망은 프루닝될 수 있다. 프루닝은 인공 신경망의 출력에 실질적으로 영향을 주지 않는 인공 신경망의 레이어의 각 채널에 대해 수행될 수 있다. 예를 들어, 레이어의 각 채널에 의해 생성된 출력 특징 맵(output feature map)에 실질적으로 영향을 주지 않는, 레이어의 각 채널의 하나 이상의 입력 특징 맵들에 대해 프루닝이 수행될 수 있다. 가중치들이 특정된 임계 값보다 작은 값을 갖는 뉴런들간의 연결이 검색될 수 있다. 임계 값보다 작은 값을 갖 는 것으로 식별된 모든 가중치와 대응하는 연결 관계는 제거되거나, 완전히 0에 맞춰지거나 그렇지 않으면 무시 될 수 있다. 가중치들이 작을 때, 예를 들어, 가중치가 특정된 하한 임계 값보다 작을 때, 인공 신경망의 채널이 감지될 수 있다. 이 경우, 감지된 채널은 인공 신경망에서 제거하기 위한 후보로서 선택될 수 있다. 프루닝은 인공 신경망의 하나 이상의 수치 값들의 수치 형태의 정밀도를 저하시키는 것을 포함할 수 있다. 예 를 들어, 가중치들을 위해 사용된 수치 형태의 정밀도가 감소될 수 있는지 여부는, 인공 신경망의 다수의 레이 어들 각각의 채널에서 하나 이상의 가중치에 대한 분석을 통해 결정될 수 있다. 사용된 수치 형태들의 정밀도 가 감소됨으로써, 낮은 정밀도 연산 하드웨어(lower precision arithmetic hardware)가 차례대로 사용될 수 있 다. 낮은 정밀도 연산 하드웨어는 높은 정밀도 연산 하드웨어보다 전력 효율이 좋고, 조밀하게 내장될 수 있다. 파라미터들의 정밀도와 범위를 표시하기 위해 필요한 최소 수의 비트들을 사용하는 인공 신경망은, 필요 한 비트보다 더 많은 비트를 사용하는 인공 신경망보다 높은 성능, 예를 들어, 빠른 작동 시간 및/또는 낮은 소 비 전력을 달성할 수 있다. 나아가, 약간의 정확도 손실을 감수하고 복수의 채널들 중에서 특정 채널 자체를 프루닝할 수도 있다. 일 실시 예에 따른 복수의 채널들 중에서 채널에 포함된 채널 요소들 중 0이 차지하는 비율이 임계 값 이상인 채널 전체 가 프루닝될 수도 있다. 예를 들어, 2번 레이어의 5번 채널의 값이 대부분 0에 수렴하고 데이터 숫자가 작은 경우, 해당 채널 자체를 프루닝할 수도 있다. 인공 신경망의 가중치는 아래 수학식 2와 같이 프루닝 되기 전 인공 신경망의 가중치에 가중치 변환 함수를 적용하여 변환될 수 있다. 수학식 2"}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 2에서, w'n,c,m는 프루닝 전 인공 신경망(예를 들어, 인공 신경망)의 가중치, wn,c,m는 프루닝 후 인 공 신경망(예를 들어, 인공 신경망)의 가중치, n은 해당 레이어 인덱스, c는 해당 채널 인덱스, m은 해당 가중치 인덱스, g는 가중치 변환 함수를 나타낸다. 나아가, 가중치 변환 함수 g는 수학식 3과 같이 나타낼 수 있다. 수학식 3"}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수학식 3에서 는 임계 값 결정 변수, αn,c는 임계 값을 전후로 가중치 변환 함수가 변화하는 기울기를 결 정하는 변수이다. αn,c 및 는 프루닝 파라미터로 지칭할 수 있고, 프루닝 파라미터는 해당 레이어의 각 채널 별로 결정될 수 있다. 예를 들어, αn,c 및 는 n번째 레이어의 c번째 채널에 대응하는 프루닝 파라미터일 수 있다. α가 커질수록 임계 값을 기준으로 가중치 변환 함수 값이 완만하게 변할 수 있고, 가 커질수록 0에 가까운 값으로 변환되는 가중치의 영역이 감소할 수 있다. 가중치 변환 함수의 값이 소정의 값 이하인 경우, 가중치 값은 0으로 변환될 수 있다. 예를 들어, 의 값이 0.5보다 작은 가중치 영역 에서, 가중치 변환 함수 =0으로 결정될 수 있다. 가중치 변환 함수 g는 다양한 형태를 가질 수 있으며, 다른 가능한 함수들이 본 개시의 권리 범위에 속함을 통 상의 기술자라면 이해할 수 있을 것이다. 채널 별 프루닝 파라미터는 학습을 통해 결정될 수 있다. 인공 신경망의 학습의 목적은 손실 함수를 최소화하 는 모델 파라미터를 결정하는 것으로 볼 수 있다. 손실 함수는 인공 신경망의 학습 과정에서 최적의 모델 파라 미터를 결정하기 위한 지표로 이용될 수 있다. 일 실시예에 따른 인공 신경망은 아래 수학식 4의 손실 함수 (loss function)에 기초하여 학습될 수 있다. 수학식 4"}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수학식 4에서, L'은 손실 함수, L은 작업 정확도, λ는 학습 비중, N은 레이어 수, C는 채널 수이다. 수학식 4를 참조하면, 학습 비중(λ)를 통해 작업 정확도와 프루닝 양 사이의 비율이 결정될 수 있다. 예를 들 어, 학습 비중(λ)가 1에 가까울수록 프루닝 양이 증가하여 작업 속도는 빨라지지만 작업 정확도가 떨어질 수 있고, 학습 비중(λ)가 0에 가까울수록 작업 정확도는 증가하지만 프루닝 양이 줄어들어 작업 속도가 느려질 수 있다. 아래에서, 도 3 내지 도 6을 참조하여 학습 비중 및 프루닝 파라미터를 결정하고, 결정된 학습 비중 및 프루닝 파라미터로 인공 신경망을 프루닝하는 방법을 상세히 설명한다. 도 3은 일 실시 예에 따른 인공 신경망을 프루닝하는 장치의 블록도를 도시한 도면이다. 도 3과 같이, 장치는 메모리 및 제어부를 포함할 수 있다. 장치는 메모리 및 시스템 버스 또는 다른 적절한 회로를 통해 메모리와 연결된 제어부를 포함할 수 있다. 장치는 메모리에 프로그램 코드를 저장할 수 있다. 제어부는 시스템 버스를 통해 메모리 에서 불러들인 프로그램 코드를 실행함으로써 인공 신경망을 프루닝하는 동작을 처리할 수 있다. 메모리는 로컬 메모리 또는 하나 이상의 대용량 저장 장치들(bulk storage devices)과 같은 하나 이상의 물리적 메모리 장치들을 포함할 수 있다. 이 때, 로컬 메모리는 RAM(Random Access Memory) 또는 프로그램 코 드를 실제로 실행하는 동안 일반적으로 사용되는 다른 휘발성 메모리 장치를 포함할 수 있다. 대용량 저장 장 치는 HDD(Hard Disk Drive), SSD(Solid State Drive) 또는 다른 비휘발성 메모리 장치로 구현될 수 있다. 또 한, 장치는 프루닝 동작을 수행하는 동안에 대용량 저장 장치들에서 프로그램 코드를 검색하는 횟수를 줄 이기 위해, 적어도 일부 프로그램 코드의 임시 저장 공간을 제공하는, 하나 이상의 캐시 메모리들(미도시)을 포 함할 수 있다. 메모리에 저장된 실행 가능한 프로그램 코드가 장치에 의해 실행됨에 따라, 제어부에 의해 본 개시에 기재된 다양한 동작들을 수행할 수 있다. 예를 들어, 메모리는 제어부가 도 1, 2 및 4 내지 7에 기재된 하나 이상의 동작을 수행하도록 하기 위한 프로그램 코드를 저장할 수 있다. 구현되는 장치의 특정 유형에 따라, 장치는 도시된 구성 요소보다 적은 구성 요소들 또는 도 3에 도시되지 않은 추가적인 구성 요소들을 포함할 수 있다. 또한, 하나 이상의 구성 요소들은 다른 구성 요소에 포함될 수있고, 그렇지 않으면 다른 구성 요소의 일부를 형성할 수 있다. 제어부는 미리 학습된 인공 신경망에 의해 처리된 추론 작업에 대한 작업 정확도의 초기 값을 획득할 수 있다. 이하에서, 미리 학습된 인공 신경망에 의해 처리된 추론 작업에 대한 작업 정확도의 초기 값은 제1 작업 정확도로 지칭될 수 있다. 인공 신경망은 목적하는 작업에 대한 학습을 수행하고, 추론 모델을 구축할 수 있다. 또한, 인공 신경망은 구 축된 추론 모델을 바탕으로 외부 입력 값에 대한 추론 결과를 출력할 수 있다. 인공 신경망을 이용하여 수행되는 작업과 관련하여, 인공 신경망은 스마트폰을 위한 안면 인식 모듈이나 소프트 웨어, 사물 인식, 음성 인식, 이미지 분류 등과 같은 인식/분류 동작, 의료 및 진단 기기, 무인 시스템 등에 적 용될 수 있으며, 영상 데이터들을 처리하여 유의미한 정보들을 추출하는 전용 처리장치로서도 구현될 수 있다. 제어부는 미리 학습된 인공 신경망을 평가하여, 작업 정확도의 초기 값을 획득할 수 있다. 작업 정확도는 인공 신경망에 의한 추론 결과 값과 기대되는 결과 값 간의 오차를 나타내는 MSE(Mean Squared Error)를 포함할 수 있다. 이 때, 작업 정확도에 대응하는 값인 MSE가 작을수록 인공 신경망의 성능이 우수함을 나타낸다. 예를 들어, 제어부는 작업 정확도를 측정하기 위해 인공 신경망에 성능을 측정하기 위한 데이터 세트(data set)를 인공 신경망에 입력할 수 있다. 제어부는 입력된 데이터 세트에 기초하여 인공 신경망으로부터 추 론된 결과 값과 기대되는 결과 값 간의 MSE를 산출하여 작업 정확도를 결정할 수 있다. 이 때, 데이터 세트는 인공 신경망이 목표로 하는 작업의 분야에 따라 달리 결정될 수 있다. 예를 들어, 이미지 분류(image classification) 분야에서, cifar-10, cifar-100 등의 데이터 세트가 이용될 수 있다. 제어부는 프루닝을 수행하기 전 미리 학습된 인공 신경망의 작업 정확도를 작업 정확도의 초기 값으로 획 득할 수 있다. 예를 들어, 작업 정확도는 훈련 데이터 세트를 입력 받아 인공 신경망이 예측한 결과에 대한 평 가를 통해 산출될 수 있다. 작업 정확도는 인공 신경망의 추론 결과가 정확할수록 작은 값을 갖는 예측 손실 (Prediction Loss)일 수 있다. 작업 정확도는 이 외에도 다양한 방식으로 측정될 수 있다. 한편, 제어부(32 0)는 복수 횟수에 걸쳐 인공 신경망을 평가하고, 복수의 평가 결과의 평균에 기초하여 작업 정확도를 획득할 수 있다. 제어부는 미리 학습된 인공 신경망을 구성하는 복수의 레이어들의 각각의 채널에 대응하는 채널 별 프루닝 파라미터(pruning parameter)에 기초하여, 미리 정해진 학습 비중에 따라 채널들에 속한 노드들 간의 웨이트들 을 조정하여 인공 신경망을 채널 단위로 프루닝할 수 있다. 예를 들어, 제어부는 미리 학습된 인공 신경 망의 복수의 레이어들의 각각의 채널 정보를 송수신하기 위해 이용되는 채널들에 속한 노드들 사이의 연결들의 적어도 일부를 조정하여 인공 신경망을 프루닝할 수 있다. 이 때, 제어부는 미리 학습된 인공 신경망을 구성하는 복수의 레이어들 각각에 포함된 채널들의 연결들에 관한 정보를 획득할 수 있다. 즉, 제어부는 다레이어 인공 신경망의 복수 레이어들 각각에 포함된 채널들 의 정보 및 복수의 레이어들 각각에 포함된 채널들 간의 연결들에 관한 정보를 획득할 수 있다. 또한, 연결들 에 관한 정보는 복수의 레이어들 중 인접 레이어들 간의 연결들의 가중치들에 대한 정보를 포함할 수 있다. 제어부는 인공 신경망의 레이어들 각각에 포함된 채널들의 연결들의 적어도 일부를 조정하여 인공 신경망 을 프루닝할 수 있다. 예를 들어, 제어부는 인공 신경망의 연결들의 가중치들을 조정하는 프루닝을 통해 인공 신경망을 압축할 수 있다. 프루닝과 관련하여 도 2를 참조하여 상술한 내용이 적용될 수 있다. 제어부는 작업 정확도의 초기 값 및 프루닝된 인공 신경망의 작업 정확도에 기초하여, 인공 신경망에 대한 학습 비중을 결정할 수 있다. 이 때, 제어부는 프루닝된 인공 신경망의 작업 정확도가 초기 값보다 작은 경우 작업 정확도가 증가되도록 학습 비중을 결정할 수 있다. 즉, 제어부는 프루닝된 인공 신경망의 작업 정확도에 기초하여 판단된 추론 작업 정확도가 작업 정확도의 초기 값에 기초하여 판단된 프루닝 전 인공 신경 망의 추론 작업 정확도보다 낮은 경우, 프루닝으로 인한 추론 작업의 성능 저하를 막기 위하여 학습 비중을 감 소시킬 수 있다. 예를 들어, 작업 정확도의 값이 추론 작업 성능에 비례하는 경우, 제어부는 프루닝된 인 공 신경망의 작업 정확도의 값이 프루닝되기 전 작업 정확도의 값보다 작다고 판단함에 따라, 학습 비중을 감소 시킬 수 있다. 반대로, 작업 정확도의 값이 추론 작업 성능에 반비례하는 경우, 제어부는 프루닝된 인공 신경망의 작업 정확도의 값이 프루닝되기 전 작업 정확도의 값보다 작다고 판단함에 따라, 학습 비중을 증가시 킬 수 있다. 학습 비중은 1 회의 프루닝이 수행되는 시간 대비 프루닝이 수행되는 정도를 나타낸다. 프루닝을 수행하는 장 치마다 1 회의 프루닝을 수행하는데 소모되는 시간은 동일하므로, 학습 비중이 증가할수록 각각의 프루닝 단계 에서 프루닝되는 정보량이 증가할 수 있다. 제어부는 결정된 학습 비중 및 프루닝된 인공 신경망의 작업 정확도에 기초하여, 채널 별 프루닝 파라미터 를 갱신할 수 있다. 예를 들어, 제어부는 프루닝의 기준이 되는 가중치의 임계값을 결정하는 변수, 결정 된 프루닝 비중 및 프루닝된 인공 신경망의 작업 정확도에 기초하여 손실 함수를 판단하고, 손실 함수를 감소시 키는 방향으로 임계값 결정 변수를 갱신할 수 있다. 제어부는 갱신된 채널 별 프루닝 파라미터에 기초하여, 갱신된 학습 비중에 따라 프루닝된 인공 신경망을 채널 단위로 재 프루닝할 수 있다. 제어부는 인공 신경망을 재 프루닝하고, 재 프루닝된 인공 신경망의 작업 정확도, 및 재 프루닝된 인공 신경망에 대한 학습 비중을 결정하는 프루닝-평가 동작을 반복적으로 수행할 수 있다. 이 때, 제어부는 미리 정의된 에폭(epoch) 및 상기 재 프루닝된 인공 신경망의 작업 정확도에 기초하여, 상기 프루닝-평가 동작을 추가적으로 수행할 것인지 여부를 결정할 수 있다. 프루닝-평가 동작은 1 회의 프루닝 및 작업 정확도를 측정하는 단위일 수 있다. 제어부는 프루닝-평가 동작을 반복적으로 수행함에 따라, 결정된 학습 비중을 학습 비중의 하한 임계 값과 비교할 수 있다. 제어부는 비교 결과에 기초하여, 현재 프루닝 세션을 종료하고 학습 비중이 초기 기준 값으로 설정된 다음 프루닝 세션을 개시할 것인지 여부를 판단할 수 있다. 예를 들어, 제어부는 결정된 학습 비중이 학습 비중의 하한 임계 값보다 작거나 같다고 판단한 경우, 현재 프루닝 세션을 종료하며 학습 비 중을 초기 기준 값으로 설정하고 다음 프루닝 세션을 개시할 수 있다. 이 때, 제어부는 현재 프루닝 세션 을 종료함에 따라, 현재 프루닝 세션 동안 프루닝된 인공 신경망에 관한 정보를 저장할 수 있다. 제어부 는 복수의 프루닝 세션들 각각을 종료함에 따라, 저장된 인공신경망들의 성능을 비교함으로써, 최적의 프루닝 방식을 결정할 수 있다. 세션은 복수의 프루닝-평가 동작에 따라 프루닝 비중을 초기 값으로 갱신하기 전까지 의 단위일 수 있다. 제어부는 프루닝-평가 동작을 수행함에 따라, 프루닝된 또는 재 프루닝된 인공 신경망의 작업 정확도를 획 득할 수 있다. 예를 들어, 제어부는 개별 세션 동안 수행된 복수의 프루닝-평가 동작들 각각을 수행할 때 마다, 작업 정확도를 획득 또는 저장하고, 개별 세션마다 작업 정확도의 평균을 산출할 수 있다. 일 세션의 평 균 작업 정확도는 이후 세션에서 학습 비중을 결정하기 위한 비교 기준이 될 수 있다. 도 4는 일부 실시예에 따른 전자 시스템의 구성을 나타내는 블록도이다. 도 4를 참고하면, 전자 시스템은 인공 신경망을 기초로 입력 데이터를 실시간으로 분석하여 유효한 정보를 추출하고, 추출된 정보를 기초로 상황 판단을 하거나 또는 전자 시스템이 탑재되는 전자 디바이스의 구성 들을 제어할 수 있다. 예컨대 전자 시스템은 드론(drone), 첨단 운전자 보조 시스템(Advanced Drivers Assistance System; ADAS) 등과 같은 로봇 장치, 스마트 TV, 스마트폰, 의료 디바이스, 모바일 디바이스, 영상 표시 디바이스, 계측 디바이스, IoT 디바이스 등에 적용될 수 있으며, 이 외에도 다양한 종류의 전자 디바이스 들 중 적어도 하나에 탑재될 수 있다. 전자 시스템은 제어부, RAM, 뉴로모픽 장치, 메모리 및 통신 모듈을 포함할 수 있다. 전자 시스템의 하드웨어 구성들 중 일부는 적어도 하나의 반도체 칩에 탑재될 수 있다. 제어부는 전자 시스템의 전반적인 동작을 제어한다. 제어부는 싱글 코어(Single Core)를 포함 하거나, 멀티 코어들(Multi-Core)을 포함할 수 있다. 제어부는 메모리에 저장된 프로그램들 및/또는 데이터를 처리 또는 실행할 수 있다. 일부 실시예에 있어서, 제어부는 메모리에 저장된 프로그램들 을 실행함으로써, 뉴로모픽 장치의 기능을 제어할 수 있다. 또한, 제어부는 뉴로모픽 장치에 의해 이용되는 가중치 정보 량을 감소시키는 프루닝을 수행할 수 있다. 제어부는 CPU, GPU, AP 등으로 구 현될 수 있다. 도 4의 제어부 및 메모리는 각각 도 3의 제어부 및 메모리에 대응할 수 있 다. RAM은 프로그램들, 데이터, 또는 명령들(instructions)을 일시적으로 저장할 수 있다. 예컨대 메모리 에 저장된 프로그램들 및/또는 데이터는 제어부의 제어 또는 부팅 코드에 따라 RAM에 일시적으 로 저장될 수 있다. RAM은 DRAM(Dynamic RAM) 또는 SRAM(Static RAM) 등의 메모리로 구현될 수 있다. 뉴로모픽 장치는 수신되는 입력 데이터를 기초로 연산을 수행하고, 수행 결과를 기초로 정보 신호를 생성 할 수 있다. 뉴로모픽 장치는 인공 신경망 전용 하드웨어 가속기 자체 또는 이를 포함하는 장치에 해당할수 있다. 정보 신호는 음성 인식 신호, 사물 인식 신호, 영상 인식 신호, 생체 정보 인식 신호 등과 같은 다양한 종류의 인식 신호 중 하나를 포함할 수 있다. 예를 들어, 뉴로모픽 장치는 비디오 스트림에 포함되는 프레임 데 이터를 입력 데이터로서 수신하고, 프레임 데이터로부터 프레임 데이터가 나타내는 이미지에 포함된 사물에 대 한 인식 신호를 생성할 수 있다. 그러나, 이에 제한되는 것은 아니며, 전자 시스템이 탑재된 전자 장치의 종류 또는 기능에 따라 뉴로모픽 장치는 다양한 종류의 입력 데이터를 수신할 수 있고, 입력 데이터에 따 른 인식 신호를 생성할 수 있다. 메모리는 데이터를 저장하기 위한 저장 장소로서, OS(Operating System), 각종 프로그램들, 및 각종 데이 터를 저장할 수 있다. 실시예에 있어서, 메모리는 뉴로모픽 장치의 연산 수행 과정에서 생성되는 중 간 결과들 또는 연산 수행 과정에 이용되는 가중치들을 저장할 수 있다. 메모리는 DRAM일 수 있으나, 이에 한정되는 것은 아니다. 메모리는 휘발성 메모리 또는 불휘발성 메 모리 중 적어도 하나를 포함할 수 있다. 불휘발성 메모리는 ROM, PROM, EPROM, EEPROM, 플래시 메모리, PRAM, MRAM, RRAM, FRAM 등을 포함한다. 휘발성 메모리는 DRAM, SRAM, SDRAM, PRAM, MRAM, RRAM, FeRAM 등을 포함한 다. 실시예에 있어서, 메모리는 HDD, SSD, CF, SD, Micro-SD, Mini-SD, xD 또는 Memory Stick 중 적어도 하나를 포함할 수 있다. 통신 모듈은 외부 디바이스와 통신할 수 있는 다양한 유선 또는 무선 인터페이스를 구비할 수 있다. 예컨 대 통신 모듈은 유선 근거리통신망(Local Area Network; LAN), Wi-fi(Wireless Fidelity)와 같은 무선 근 거리 통신망 (Wireless Local Area Network; WLAN), 블루투스(Bluetooth)와 같은 무선 개인 통신망(Wireless Personal Area Network; WPAN), 무선 USB (Wireless Universal Serial Bus), Zigbee, NFC (Near Field Communication), RFID (Radio-frequency identification), PLC(Power Line communication), 또는 3G (3rd Generation), 4G (4th Generation), LTE (Long Term Evolution) 등 이동 통신망(mobile cellular network)에 접속 가능한 통신 인터페이스 등을 포함할 수 있다. 도 5는 일 실시 예에 따른 인공 신경망을 프루닝하는 장치에 의해 수행되는 프루닝 알고리즘을 도시한 흐름도이 다. 도 5의 동작은 도 3의 인공 신경망을 프루닝하는 장치에 의해 수행될 수 있다. 이하 도 5 및 도 6에서 사용되는 수치, 변수의 범위 및 수식은 당업자가 용이하게 도출할 수 있는 범위에서 변 형이 가능하며, 변형된 수치 등은 본 개시의 권리범위에 속한다. 단계에서, 장치는 미리 학습된 인공 신경망 관련 정보를 획득할 수 있다. 상술한 바와 같이 인공 신경망 에 관련된 정보는 다계층의 인접 레이어들에 포함된 채널들 각각에 속하는 복수의 뉴런들간의 가중치들일 수 있 다. 단계에서, 장치는 최초 세션을 개시할 수 있다. 이 때, 세션을 식별하는 인덱스 n은 1로 설정될 수 있다. 단계에서 장치는 프루닝-평가 동작 변수의 초기 값을 설정할 수 있다. 예를 들어, 장치는 각각의 세션에 서 반복되는 프루닝-평가 동작의 횟수를 나타내는 인덱스인 s를 0으로 설정할 수 있다. 또한, 장치는 학습 비중을 나타내는 λs를 초기 기준 비중 값인 λinit으로 설정할 수 있다. 예를 들어, λinit은 학습 비중의 가능한 범위에서 최댓값을 나타낼 수 있다. 다른 일 예로서, λinit은 학습 비 중의 가능한 범위 내에 임의의 값일 수 있다. 예를 들어, 학습 비중 λs 는 0부터 1까지의 값을 가질 수 있고 이 때, λinit 은 1일 수 있으나 이에 제한되지 아니한다. 또한, 장치는 세션 별 평균 작업 정확도 를 설정할 수 있다. 아래 첨자가 n이 아닌 n-1인 것은 현재 세션 의 작업 정확도와의 비교 기준으로서 이전 세션의 평균 작업 정확도가 이용됨을 나타낸다. 이는 예시적인 것으 로서 다양한 방식으로 비교 기준이 되는 작업 정확도가 결정될 수 있다. 예를 들어, 장치는 프루닝이 수행되기 전 미리 학습된 인공 신경망에 대하여 미리 결정된 훈련 데이터 세트에 기초하여 반복적으로 평가를 수행한 후 도출된 평가 결과의 평균 값을 Tinit으로 결정할 수 있다. 학습 정확도는 장치에서 결정될 수 있다. 다른 일 예로서, 인공 신경망이 구현된 외부 장치 또는 그 이외의 외부 장치에서 결정된 후 장치가 외부 장치로부터 획득할 수 있다. 또한, 장치는 각각의 세션의 종료 기준이 되는 학습 비중의 하한 임계 값 λmin 을 획득할 수 있다. 예를 들어, 하한 임계 값 λmin은 학습 비중의 가능한 범위 및 하한 값을 고려하여 결정되는 하한 값에 가까운 값일 수 있다. 예를 들어, 학습 비중 λ는 0부터 1까지의 값을 가질 수 있고 이 때, λmin 은 10-20일 수 있으나 이에 제 한되지 아니한다. 또한, 장치는 프루닝의 일 예로서 프루닝을 수행하기 위한 기준이 되는 가중치의 임계 값을 결정하기 위한 임계 값 결정 변수 을 초기 값으로 설정할 수 있다. 는 가중치기를 통하여 기존 가중치를 변환하는 가중치 변 환 함수의 변수로서, 가중치의 임계 값의 크기를 결정할 수 있다. 예를 들어, 가 클수록 임계 값이 증가하 고 확률적으로 더 많은 가중치가 프루닝 되어 0값으로 변환되거나 작은 가중치 값으로 변환된다. 단계에서, 장치는 프루닝-평가 동작 변수에 기초하여 인공 신경망에 대한 프루닝 동작을 수행할 수 있다. 예를 들어, 장치는 인공 신경망의 인접 계층별 모든 가중치 또는 적어도 일부의 가중치 값들에 관한 정보를 획 득하고, 미리 결정된 학습 비중 λs 및 프루닝 파라미터에 기초하여 프루닝을 수행할 수 있다. 장치는 미리 학습된 인공 신경망 또는 앞선 프루닝-평가 동작에서 프루닝된 인공 신경망의 가중치 값이 프루닝 의 기준이 되는 임계 값보다 작은 경우 해당 가중치 값을 0으로 변환하거나 보다 작은 값으로 감소시킬 수 있다. 또한, 장치는 미리 학습된 인공 신경망 또는 앞선 프루닝-평가 동작에서 프루닝된 인공 신경망의 가중치 값이 프루닝의 기준이 되는 임계 값보다 큰 경우 해당 가중치 값을 1로 변환하거나 보다 큰 값으로 증가시킬 수 있다. 단계에서, 장치는 프루닝-평가 동작 변수를 갱신할 수 있다. 프루닝-평가 동작 변수를 갱신하는 구체적인 방식은 도 6을 참조하여 하기에서 설명하도록 한다. 단계에서, 장치는 갱신된 학습 비중 λs가 학습 비중의 하한 임계 값 λmin보다 작은지 판단할 수 있다. 단계에서, 장치는 학습 비중 λs가 학습 비중의 하한 임계 값 λmin 보다 작지 않다고 판단한 경우 단계 로 되돌아가 이후의 절차를 수행할 수 있다. 단계에서, 장치는 학습 비중 λs가 학습 비중의 하한 임계 값 λmin 보다 작다고 판단한 경우 세션 종료 절 차를 수행한다. 프루닝된 인공 신경망의 작업 정확도가 저하되어 프루닝이 진행 속도가 감소함에 따라, 현재 세션 n을 종료시키고 학습 비중 λs를 증가시켜 다음 세션 n+1을 진행한다. 단계에서, 장치는 다음 세션 n+1에서 프루닝된 인공 신경망의 작업 정확도 평가의 비교 기준이 되는 현재 세션의 평균 추론 작업 정확도를 갱신한다. 예를 들어, 장치는 세션 n에서의 추론 작업 정확도를, 현재 세션 n 에서 소정의 횟수의 프루닝-평가 동작을 수행할 때마다 획득한 작업 정확도의 평균 값으로 갱신할 수 있다. 예 를 들어, 장치는 하기 수학식 5에 따라 추론 작업 정확도를 갱신할 수 있다. 수학식 5"}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "이 때 Tn은 세션 n에서의 추론 작업 정확도를 나타내며, AVG는 평균을 산출하는 연산자이며, Ts는 세션 n에서 획 득된, 프루닝된 인공 신경망의 작업 정확도를 합한 값을 나타낸다. 단순 평균 외에도 다양한 방식으로, 다음 세션 n+1에서 작업 정확도의 비교 기준이 되는 추론 작업 정확도를 산출하는 방식이 본 개시의 권리범위에 속함 을 통상의 기술자라면 이해할 수 있을 것이다. 단계에서, 장치는 미리 정의된 에폭(epoch) 및 재 프루닝된 인공 신경망의 작업 정확도에 기초하여, 프루 닝-평가 동작을 추가적으로 수행할 것인지 여부를 결정할 수 있다. 예를 들어, 장치는 전체 인공 신경망 중 미리 정해진 비율(예를 들어, 70%) 이상 프루닝 되었거나, 미리 정의된 에폭만큼 인공 신경망을 학습한 경우 프루닝 작업을 종료할 수 있다. 여기서, 프루닝된 정도는 재 프루닝된 인 공 신경망의 작업 정확도에 기초하여 판단될 수 있다. 재 프루닝된 인공 신경망의 작업 정확도가 높을수록 프 루닝된 정도가 낮고, 재 프루닝된 인공 신경망의 작업 정확도가 낮을수록 프루닝된 정도가 높을 수 있다. 단계에서, 장치는 세션을 갱신하기 전, 세션 n에서 프루닝된 인공 신경망에 관한 정보를 저장할 수 있다. 또한, 장치는 저하된 학습 비중 λs를 증가시킬 수 있다. 예를 들어, 장치는 λs를 초기 기준 비중 값인 λinit 으로 설정할 수 있다. 도 6은 일 실시 예에 따른 인공 신경망을 프루닝하는 장치에 의해 수행되는 프루닝-평가 동작 변수의 갱신 동작 의 흐름도를 도시한 도면이다. 도 6의 동작은 도 3의 인공 신경망을 프루닝하는 장치에 의해 수행될 수 있다. 단계에서 장치는 프루닝-평가 동작 변수의 갱신 동작을 개시한다. 단계에서 장치는 세션 n내에 프루닝-평가 동작 반복 수 s를 s+1로 갱신한다. 이하에서 상술하는 s는 갱신 된 값을 나타낸다. 단계에서, 장치는 작업 정확도 Ls-1을 결정할 수 있다. 예를 들어, Ls-1은 프루닝된 인공 신경망에 대하여 수행된 평가에 기초하여 결정될 수 있다. 상술한 바와 같이 작업 정확도 Ls-1의 값은 추론이 정확할수록 작을 수 있다. 작업 정확도 Ls-1는 예를 들어 예측 손실(Prediction loss)일 수 있으나, 이에 제한되지 아니한다. 단계에서 장치는 현재 세션 n의 작업 정확도 이력을 추가할 수 있다. 예를 들어, 장치는 도 5에서 상술한 세션 별 평균 추론 작업 정확도 을 결정하기 위해 이용되는 Ls-1을 저장할 수 있다. 단계에서, 장치는 작업 정확도 Ls-1의 값이 이전 세션 n-1에서 평균 추론 작업 정확도 의 값보다 큰지 여부를 판단할 수 있다. 장치는 프루닝된 인공 신경망의 작업 정확도가 초기 값 또는 이전 세션의 추론 작업 정확도 보다 작은 경우 작업 정확도가 증가되도록 학습 비중을 결정할 수 있다. 단계에서, 장치는 작업 정확도 Ls-1의 값이 세션 n-1에서 평균 추론 작업 정확도 의 값보다 크다고 판 단한 경우, 단계에서 학습 비중 λs를 하기 수학식 6에 따라 갱신할 수 있다. 수학식 6"}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "단계에서 장치는 작업 정확도 Ls-1가 세션 n-1에서 평균 추론 작업 정확도 보다 크지 않다고 판단한 경 우, 단계에서 학습 비중 λs를 하기 수학식 7에 따라 갱신할 수 있다. 수학식 7"}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "이 때, 학습 비중 λs 및 작업 정확도에 기초하여 임계 값 결정 변수 및 임계 값을 전후로 가중치 변환 함수가 변화하는 기울기를 결정하는 변수 αn,c를 결정할 수 있다. 단계에서, 장치는 프루닝-평가 동작 변수 갱신을 종료할 수 있다. 일반적으로 인공 신경망의 프루닝률이 증가함에 따라 인공 신경망의 정확도가 감소될 수 있다. 상술한 도 4 내 지 6를 참고하여 개시된 실시 예에 따라, 프루닝을 수행함에 있어서 필요한 변수들이 장치에 의해 획득된 데이 터 및 관계식을 통해 결정될 수 있다. 즉, 프루닝 비용을 최소화하는 알고리즘에 따라, 최적의 프루닝을 확정 하기 위해 미세 조정(fine tuning)이나 민감도가 높은 주요 파라미터를 수동으로 설정하는 방식이 요구되지 않 고, 이에 따라 프루닝에 소모되는 시간과 비용을 줄이며, 프루닝의 효율성이 증진될 수 있다. 도 7은 인공 신경망 및 인공 신경망을 프루닝하는 장치를 포함하는 시스템을 도시한 도면이다. 예를 들어 인공 신경망은 장치의 외부 서버 및 데이터 베이스 등에 포함된 장치일 수 있다. 장치는 메모리, 프로세서제어부, 및 통신부를 포함할 수 있다. 도 7에서 메모리 및 프로세서제어부는 도 7의 실시예와 모순되지 않는 범위에서 도 3의 메모리 및 제어부( 320)에 대한 내용이 적용될 수 있다. 장치는 통신부를 통해 인공 신경망과 통신 네트워크를 형성할 수 있다. 장치는 인공 신경망에 대한 정보를 통신부를 통하여 인공 신경망으로부터 획득할 수 있다. 장치가 프루닝 동작을 수행함에 있어서 장치는 통신부를 통하여 인공 신경망에 대한 정보에 접근할 수 있으므로 인공 신경망의 모든 정보를 메모리에 저장할 필요가 없다. 이 외에도, 다양한 방식으로 장치가 구현될 수 있다. 예를 들어, 사용자 단말기에 장치가 구현되고 외부 인공 신경망에 접근하여 프루닝된 인공 신경망을 획득하는 방식으로 장치가 구현될 수 있다. 다른 일 예로서 인공 신경망 및 장치가 일체로서 사용자 단말기에 구현될 수 있다. 또한, 다른 일 예로서, 사 용자 단말기와 별도로 장치 및 인공 신경망이 구현되고, 사용자 단말기는 장치에 의해 프루닝된 인공 신경망만을 획득할 수 있다. 이상에서 설명된 실시예들은 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨 어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치, 방법 및 구성요소는, 프로세 서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴퓨 터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 운영 체제 상에서 수행되는 하나 이상의 소 프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설명된"}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소(processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서(parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(signal wave)에 영구적으로, 또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 하나 이상의 컴퓨터 판독 가능 기록 매 체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단 독으로 또는 조합하여 포함할 수 있다. 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구 성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2020-0132151", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "이상과 같이 실시예들이 비록 한정된 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가진 자라면 상기를 기초로 다양한 기술적 수정 및 변형을 적용할 수 있다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7"}
{"patent_id": "10-2020-0132151", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1는 일 실시예에 따른 인공 신경망에서 수행되는 연산을 설명하기 위한 도면이다. 도 2는 일 실시예에 따른 프루닝(pruning)을 설명하기 위한 도면이다. 도 3은 일 실시 예에 따른 인공 신경망을 프루닝하는 장치의 블록도를 도시한 도면이다. 도 4는 일부 실시예에 따른 전자 시스템의 구성을 나타내는 블록도이다. 도 5는 일 실시 예에 따른 인공 신경망을 프루닝하는 장치에 의해 수행되는 프루닝 알고리즘을 도시한 흐름도이 다. 도 6은 일 실시 예에 따른 인공 신경망을 프루닝하는 장치에 의해 수행되는 프루닝-평가 동작 변수의 갱신 동작 의 흐름도를 도시한 도면이다. 도 7은 인공 신경망 및 인공 신경망을 프루닝하는 장치를 포함하는 시스템을 도시한 도면이다."}
