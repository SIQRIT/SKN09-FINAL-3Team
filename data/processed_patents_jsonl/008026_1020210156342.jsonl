{"patent_id": "10-2021-0156342", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0070601", "출원번호": "10-2021-0156342", "발명의 명칭": "사운드 기반의 지능형 위급상황 분석 시스템 및 그 방법", "출원인": "한국광기술원", "발명자": "김선만"}}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "각 우범 지역에 설치되어, 해당 우범 지역 내에서 발생되는 음향 또는 음성을 포함한 오디오 데이터를 수집하여수집된 오디오 데이터에서 위급 상황에 대한 이벤트를 감지하여 비상벨 작동 신호를 발생하는 비상벨 장치; 및 상기 비상벨 작동 신호가 수신되면 상기 비상벨 장치로부터 오디오 데이터를 수신하고, 상기 오디오 데이터에서음성 데이터를 추출한 후 음성 데이터를 분석하여 대화음성 상황을 분류하고, 상기 오디오 데이터에서 음성 데이터를 제외한 나머지 음향 데이터를 분석하여 음향 상황을 분류하며, 상기 분류된 대화음성 상황과 음향 상황을 통합하여 기 설정된 분류 기준에 따라 단계별 보안 레벨로 구분되는 범죄 또는 위급 상황을 판별하여 상황분류 결과를 제공하는 분석 서버를 포함하되,상기 분석 서버는, 상기 범죄 또는 위급 상황에 해당하는 대화음성 관련 텍스트들을 학습 데이터로 수집하고, 상기 학습 데이터를이용하여 인공 지능 기반의 음성 분석 모델을 사전에 학습한 후 학습된 음성 분석 모델에 기반하여 음성 데이터추출 및 대화음성 상황을 분류하며,상기 범죄 또는 위급 상황에 해당하는 음향 관련 소리들을 학습 데이터로 수집하고, 상기 학습 데이터를 이용하여 인공 지능 기반의 음향 분석 모델을 학습한 후 학습된 음향 분석 모델에 기반하여 음향 상황을 분류하는 것인, 사운드 기반의 지능형 위급상황 분석 시스템."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 상황 분류 결과가 수신되면 상기 비상벨 작동 신호가 발생된 우범 지역을 관할하는 보안 단말에 상기 상황분류 결과에 근거한 보안 레벨에 따라 현장 출동 정보 또는 상황 대처 정보를 제공하는 관제 서버를 더 포함하는 것인, 사운드 기반의 지능형 위급상황 분석 시스템."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 분석 서버는,상기 비상벨 장치로부터 현장의 오디오 데이터를 수신하고, 외부 장치와의 송수신을 수행하는 통신부;상기 오디오 데이터에서 음성 데이터를 추출하는 음성 추출부;추출된 음성 데이터를 한국어 기반의 대화음성 텍스트로 변환하고, 대화음성텍스트에 기반하여 대화음성 상황을분석하는 대화음성 상황 분석부;상기 오디오 데이터에서 음성 데이터를 제외한 나머지 음향 데이터를 분석하여 음향 상황을 분석하는 음향 상황분석부; 및 상기 대화음성 상황 분석부에서 분석한 대화음성 상황과 음향 상황 분석부에서 분석한 음향 상황을 통합하여 기설정된 분류 기준에 따라 단계별 범죄 코드로 구분되는 범죄 또는 위급 상황을 판별하여 상황 분류 결과를 제공하는 상황 판단부를 포함하는 것인, 사운드 기반의 지능형 위급상황 분석 시스템."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 음성 추출부는 상기 오디오 데이터에서 언어종류별 음성 발생 여부를 판별하여 언어종류 정보를 상기 대화음성 상황 분석부로 제공하고, 공개특허 10-2023-0070601-3-상기 대화음성 상황 분석부는 상기 언어종류 정보에 기초하여 추출된 음성 데이터를 문자데이터로 변환하고, 변환된 문자데이터를 한국어 기반의 대화음성 텍스트로 번역하는 것인, 사운드 기반의 지능형 위급상황 분석 시스템."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서,상기 음성 분석 모델은 음성 추출부, 대화음성 상황 분석부, 음향 상황 분석부, 상황 판단부를 위해 각각의 심층신경망(DNN, deep neural networks)을 포함하는 다층 신경망 구조로 형성되는 것인, 사운드 기반의 지능형 위급상황 분석 시스템."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "음향 기반의 비상벨 장치와 연동하여 위급 상황을 분석하는 분석 서버에 의해 수행되는 사운드 기반의 지능형위급상황 분석 방법에 있어서,a) 기 설정된 우범 지역에 설치된 비상벨 장치로부터 비상벨 작동 신호가 감지되면, 해당 우범 지역 내에서 발생되는 현장의 오디오 데이터를 수신하는 단계;b) 상기 오디오 데이터에서 음성 데이터를 추출하고, 추출된 음성 데이터에 기반하여 대화음성 상황을 분석하는단계; c) 상기 오디오 데이터에서 음향 데이터를 추출하고, 추출된 음향 데이터에 기반하여 음향 상황을 분석하는 단계; 및 d) 상기 대화음성 상황과 음향 상황을 통합하여 기 설정된 분류 기준에 따라 단계별 보안 레벨로 구분되는 범죄또는 위급 상황을 판별하여 상황 분류 결과를 제공하는 단계를 포함하는 것인, 사운드 기반의 지능형 위급상황분석 방법."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,e) 상기 비상벨 작동 신호가 발생된 우범 지역을 관할하는 보안 단말에 상기 상황 분류 결과에 근거하여 현장출동 정보 또는 상황 대처 정보를 제공하는 단계를 더 포함하는 것인, 사운드 기반의 지능형 위급상황 분석 방법."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 b) 단계는, 상기 범죄 또는 위급 상황에 해당하는 대화음성 관련 텍스트들을 학습 데이터로 수집하고, 상기 학습 데이터를이용하여 인공 지능 기반의 음성 분석 모델을 사전에 학습한 후 학습된 음성 분석 모델에 기반하여 음성 데이터추출 및 대화음성 상황을 분류하는 것인, 사운드 기반의 지능형 위급상황 분석 방법."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 음성 분석 모델은, 상기 오디오 데이터에서 언어종류별 음성 발생 여부를 판별하고, 판별된 언어종류 정보에 기초하여 추출된 음성 데이터를 문자데이터로 변환하고, 변환된 문자데이터를 한국어 기반의 대화음성 텍스트로 번역하여 대화음성 상황을 분류하는 것인, 사운드 기반의 지능형 위급상황 분석 방법."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 음성 분석 모델은 언어종류별 음생 발생 여부 판별, 음성 데이터 추출, 음성 데이터의 문자데이터 변환,한국어 기반의 대화음성 텍스트 번역, 대화음성 상황 분류를 위해 각각의 심층신경망(DNN, deep neural공개특허 10-2023-0070601-4-networks)을 포함하는 다층 신경망 구조로 형성되는 것인, 사운드 기반의 지능형 위급상황 분석 방법."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제6항에 있어서,상기 c) 단계는, 상기 범죄 또는 위급 상황에 해당하는 음향 관련 소리들을 학습 데이터로 수집하고, 상기 학습 데이터를 이용하여 인공 지능 기반의 음향 분석 모델을 학습한 후 학습된 음향 분석 모델에 기반하여 음향 상황을 분류하는 것인, 사운드 기반의 지능형 위급상황 분석 방법."}
{"patent_id": "10-2021-0156342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "비상벨 장치와 연동하여 음향 기반의 비상벨 장치와 연동하여 위급 상황을 분석하는 분석 서버에 있어서,상기 비상벨 장치로부터 오디오 데이터를 수신하고, 인공 지능 기반의 음성 분석 모델을 이용하여 상기 오디오데이터에서 음성 데이터를 추출한 후 음성 데이터를 분석하여 대화음성 상황을 분류하고, 인공 지능 기반의 음향 분석 모델을 이용하여 상기 오디오 데이터에서 음성 데이터를 제외한 나머지 음향 데이터를 분석하여 음향상황을 분류하며, 상기 분류된 대화음성 상황과 음향 상황을 통합하여 기 설정된 분류 기준에 따라 단계별 보안레벨로 구분되는 범죄 또는 위급 상황을 판별하여 상황 분류 결과를 제공하하되,인공 지능 기반의 음성 분석 모델은 상기 범죄 또는 위급 상황에 해당하는 대화음성 관련 텍스트들을 수집한 학습 데이터를 이용하여 사전 학습된 것이고,인공 지능 기반의 음향 분석 모델은 상기 범죄 또는 위급 상황에 해당하는 음향 관련 소리들을 수집한 학습 데이터를 이용하여 사전 학습된 것을 특징으로 하는 분석 서버."}
{"patent_id": "10-2021-0156342", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 사운드 기반의 지능형 위급상황 분석 시스템 및 그 방법에 관한 것으로서, 각 우범 지역에 설치되어, 해당 우범 지역 내에서 발생되는 음향 또는 음성을 포함한 오디오 데이터를 수집하여 수집된 오디오 데이터에서 위급 상황에 대한 이벤트를 감지하여 비상벨 작동 신호를 발생하는 비상벨 장치; 및 상기 비상벨 작동 신호가 수 (뒷면에 계속)"}
{"patent_id": "10-2021-0156342", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 음성 및 음향 신호를 종합적으로 분석하여 현재 폭력 또는 위급 상황을 분류할 수 있는 사운드 기반 의 지능형 위급상황 분석 시스템에 관한 것이다."}
{"patent_id": "10-2021-0156342", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이 부분에 기술된 내용은 단순히 본 발명의 일 실시예에 대한 배경 정보를 제공할 뿐 종래기술을 구성하는 것은 아니다. 일반적으로, 범죄 예방 시스템은 폭력, 응급 상황 등의 비상 상황 발생 시 신고 및 대응을 할 수 있도록 보안이 취약한 지역에 설치된다. 범죄 예방 시스템 중 방범용 비상벨은 우범 지역과 같이 특정 지역에 설치되어 현장에 서 위험상황 발생시 사용자의 조작에 따라 도움을 요청할 수 있는 특정 서버로 신호를 전송하여 관리자가 위험 상황을 감지할 수 있도록 하는 장치이다. 이러한 방범용 비상벨과 같이 설치되는 감시 카메라는 해당 우범 지역의 일측 상부 영역에 설치되어 위험 상황 이 발생하는 경우에 관리자가 촬영된 영상을 확인하여 도움을 주거나, 이후 범죄자를 색출하는데 이용되도록 범 행 영상을 녹화하는 기능을 수행한다. 여기서, 감시 카메라는 일반적으로 폐쇄회로 텔레비전(CCTV: Closed Circuit Television)이 사용되고 있으나, 고성능의 카메라가 사용되기도 한다. 최근, 화장실 등과 같이 공중의 이용이 가능하면서 외부와의 노출이 차단되는 공간(예를 들어, 실내 공공 장소 등)에서, 폭행, 강도, 성추행, 살인 등의 범죄사고가 빈번하게 발생하고 있고, 이에 따라 실내공공 장소를 이용 하는 이용자의 불안감이 점차 증가하고 있다. 특히, 여성의 경우 남성에 비교하여 신체적 능력이 낮기 때문에 실내 공공 장소 이용에 대하여 더욱 큰 불안감 및 부담을 가지게 된다. 이에 따라, 실내 공공 장소에서의 위급상황을 미연에 방지함과 동시에 대처하기 위한 비상경보장치에 대한 다양 한 연구가 진행되고 있다. 방범용 비상벨은 설치가 간단하며, 조작이 편리한 장점으로 인해 실제 현장에 설치되 고 있으나, 비상벨을 구동시키기 위해서는 위급상황에 처한 당사자가 직접 비상벨이 설치된 위치로 이동하여야만 하고, 물리적인 접촉을 통해서만 비상벨을 누를 수 있기 때문에 실제 위급상황에 처한 당사자가 범죄자의 시 야에서 비상벨을 누르기가 어렵고, 강제적으로 비상벨의 동작이 정지될 수 있어 위급 상황에 신속한 대응을 할 수 없다는 문제점이 있다. 이러한 문제점으로 인해, 마이크로폰을 통해 수집된 음향신호의 데시벨 크기를 임계치에 비교하여 위급상황을 감지하도록 하는 음향 기반의 보안 기술이 연구되었으나, 이러한 방식은 위급상황과 무관한 소리에도 반응하기 때문에 오동작 및 에러가 높아 신뢰도가 떨어지는 문제점이 있다. 최근, 실내 공공 장소에 설치되는 비상벨 장치는 버튼식 비상벨과 음향 인식 모듈이 적용된 비상벨을 함께 사용 하고 있으나, 실제로 대화음성과 주변소리를 구분할 수 없어 현재 상황을 인지하기 어렵고, 오동작으로 인해 매 일 2~3회 정도 방범 담당자(관할 경찰 등)가 비상벨 장치가 설치된 장소로 출동하고 있어 인력 낭비가 발생하고 있다. 실제로, 비상벨 장치로 인한 출동 건수의 85.6%가 취객이나 소음으로 인한 상황이며, 출동 건수의 13.7%가 장난 이나 실수로 인한 상황으로서, 99.3%가 실제 범죄 상황이 아닌 비범죄 상황에서의 불필요한 출동이 되고 있다. 이로 인해, 비상벨 장치가 동작하는 장소에 출동한 방범 당당자는 주로 범죄 상황 대처보다는 범죄 발생 여부에 대한 진위 파악을 하고 있는 실정이다. 이와 같이, 종래에는 비상벨 장치의 동작시 먼저 초동 인력이 출동하여 범죄 발생에 대한 진위 여부를 파악하고, 실제 범죄 상황 발생시 대응 인력이 재출동하여 범죄 상황에 대처하고 있으므로 범죄 대처에 필요한 출동 지연이 발생할 뿐만 아니라 신속한 범죄 대처가 어렵게 되는 문제점이 있다."}
{"patent_id": "10-2021-0156342", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 전술한 문제점을 해결하기 위하여, 본 발명의 일 실시예에 따라 현장에서 발생되는 음성 및 음향 신 호를 종합적으로 분석하여 현재 폭력 또는 윕급 상황을 구체적으로 분류할 수 있는 사운드 기반의 지능형 위급 상황 분석 시스템 및 그 방법을 제공하는 것에 목적이 있다. 다만, 본 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제로 한정되지 않으며, 또 다른 기 술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2021-0156342", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 기술적 과제를 달성하기 위한 기술적 수단으로서 본 발명의 일 실시예에 따른 사운드 기반의 지능형 위 급상황 분석 시스템은, 각 우범 지역에 설치되어, 해당 우범 지역 내에서 발생되는 음향 또는 음성을 포함한 오 디오 데이터를 수집하여 수집된 오디오 데이터에서 위급 상황에 대한 이벤트를 감지하여 비상벨 작동 신호를 발 생하는 비상벨 장치; 및 상기 비상벨 작동 신호가 수신되면 상기 비상벨 장치로부터 오디오 데이터를 수신하고, 상기 오디오 데이터에서 음성 데이터를 추출한 후 음성 데이터를 분석하여 대화음성 상황을 분류하고, 상기 오 디오 데이터에서 음성 데이터를 제외한 나머지 음향 데이터를 분석하여 음향 상황을 분류하며, 상기 분류된 대 화음성 상황과 음향 상황을 통합하여 기 설정된 분류 기준에 따라 단계별 보안 레벨로 구분되는 범죄 또는 위급 상황을 판별하여 상황 분류 결과를 제공하는 분석 서버를 포함하되, 상기 분석 서버는, 상기 범죄 또는 위급 상 황에 해당하는 대화음성 관련 텍스트들을 학습 데이터로 수집하고, 상기 학습 데이터를 이용하여 인공 지능 기 반의 음성 분석 모델을 사전에 학습한 후 학습된 음성 분석 모델에 기반하여 음성 데이터 추출 및 대화음성 상 황을 분류하며, 상기 범죄 또는 위급 상황에 해당하는 음향 관련 소리들을 학습 데이터로 수집하고, 상기 학습 데이터를 이용하여 인공 지능 기반의 음향 분석 모델을 학습한 후 학습된 음향 분석 모델에 기반하여 음향 상황 을 분류하는 것이다. 본 발명에 일측면에 따르면, 사운드 기반의 지능형 위급상황 분석 시스템은, 상기 상황 분류 결과가 수신되면 상기 비상벨 작동 신호가 발생된 우범 지역을 관할하는 보안 단말에 상기 상황 분류 결과에 근거한 보안 레벨에 따라 현장 출동 정보 또는 상황 대처 정보를 제공하는 관제 서버를 더 포함하는 것이다. 상기 분석 서버는, 상기 비상벨 장치로부터 현장의 오디오 데이터를 수신하고, 외부 장치와의 송수신을 수행하 는 통신부; 상기 오디오 데이터에서 음성 데이터를 추출하는 음성 추출부; 추출된 음성 데이터를 한국어 기반의 대화음성 텍스트로 변환하고, 대화음성텍스트에 기반하여 대화음성 상황을 분석하는 대화음성 상황 분석부; 상 기 오디오 데이터에서 음성 데이터를 제외한 나머지 음향 데이터를 분석하여 음향 상황을 분석하는 음향 상황 분석부; 및 상기 대화음성 상황 분석부에서 분석한 대화음성 상황과 음향 상황 분석부에서 분석한 음향 상황을 통합하여 기 설정된 분류 기준에 따라 단계별 범죄 코드로 구분되는 범죄 또는 위급 상황을 판별하여 상황 분류 결과를 제공하는 상황 판단부를 포함하는 것이다. 상기 음성 추출부는 상기 오디오 데이터에서 언어종류별 음성 발생 여부를 판별하여 언어종류 정보를 상기 대화 음성 상황 분석부로 제공하고, 상기 대화음성 상황 분석부는 상기 언어종류 정보에 기초하여 추출된 음성 데이 터를 문자데이터로 변환하고, 변환된 문자데이터를 한국어 기반의 대화음성 텍스트로 번역하는 것이다. 상기 음성 분석 모델은 음성 추출부, 대화음성 상황 분석부, 음향 상황 분석부, 상황 판단부를 위해 각각의 심 층신경망(DNN, deep neural networks)을 포함하는 다층 신경망 구조로 형성되는 것이다. 본 발명의 일 실시예에 따른 사운드 기반의 지능형 위급상황 분석 방법에 따르면, 음향 기반의 비상벨 장치와 연동하여 위급 상황을 분석하는 분석 서버에 의해 수행되는 사운드 기반의 지능형 위급상황 분석 방법에 있어서, a) 기 설정된 우범 지역에 설치된 비상벨 장치로부터 비상벨 작동 신호가 감지되면, 해당 우범 지역 내 에서 발생되는 현장의 오디오 데이터를 수신하는 단계; b) 상기 오디오 데이터에서 음성 데이터를 추출하고, 추 출된 음성 데이터에 기반하여 대화음성 상황을 분석하는 단계; c) 상기 오디오 데이터에서 음향 데이터를 추출 하고, 추출된 음향 데이터에 기반하여 음향 상황을 분석하는 단계; 및 d) 상기 대화음성 상황과 음향 상황을 통 합하여 기 설정된 분류 기준에 따라 단계별 보안 레벨로 구분되는 범죄 또는 위급 상황을 판별하여 상황 분류 결과를 제공하는 단계를 포함하는 것이다. 사운드 기반의 지능형 위급상황 분석 방법은, e) 상기 비상벨 작동 신호가 발생된 우범 지역을 관할하는 보안 단말에 상기 상황 분류 결과에 근거하여 현장 출동 정보 또는 상황 대처 정보를 제공하는 단계를 더 포함하는 것이다. 상기 b) 단계는, 상기 범죄 또는 위급 상황에 해당하는 대화음성 관련 텍스트들을 학습 데이터로 수집하고, 상 기 학습 데이터를 이용하여 인공 지능 기반의 음성 분석 모델을 사전에 학습한 후 학습된 음성 분석 모델에 기 반하여 음성 데이터 추출 및 대화음성 상황을 분류하는 것이다. 상기 음성 분석 모델은, 상기 오디오 데이터에서 언어종류별 음성 발생 여부를 판별하고, 판별된 언어종류 정보 에 기초하여 추출된 음성 데이터를 문자데이터로 변환하고, 변환된 문자데이터를 한국어 기반의 대화음성 텍스 트로 번역하여 대화음성 상황을 분류하는 것이다. 상기 음성 분석 모델은 언어종류별 음생 발생 여부 판별, 음성 데이터 추출, 음성 데이터의 문자데이터 변환, 한국어 기반의 대화음성 텍스트 번역, 대화음성 상황 분류를 위해 각각의 심층신경망(DNN, deep neural networks)을 포함하는 다층 신경망 구조로 형성되는 것이다. 상기 c) 단계는, 상기 범죄 또는 위급 상황에 해당하는 음향 관련 소리들을 학습 데이터로 수집하고, 상기 학습 데이터를 이용하여 인공 지능 기반의 음향 분석 모델을 학습한 후 학습된 음향 분석 모델에 기반하여 음향 상황 을 분류하는 것이다."}
{"patent_id": "10-2021-0156342", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본 발명의 과제 해결 수단에 의하면, 본 발명은 현장의 오디오 데이터에서 음성 데이터를 검출하여 대화 음성 상황을 인지하여 폭력 또는 위급상황에 대한 초기 인지가 가능할 뿐만 아니라 현장의 오디오 데이터에서 검출된 음향 데이터를 분석하여 음향 상황을 분류한 후 대화음성 상황과 음향 상황을 융합하여 폭력 또는 위급 상황에 대한 범죄레벨을 단계별로 분류할 수 있고, 그로 인해 비상벨 서비스의 실효성에 대한 사회적 불신과 활 용 저하를 방지할 수 있고, 양질의 범죄 안전 관련 서비스를 제공할 수 있는 효과가 있다."}
{"patent_id": "10-2021-0156342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 발명의 실시예를 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관 계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\"되어 있는 경우도 포함한다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아 니라 다른 구성요소를 더 포함할 수 있는 것을 의미하며, 하나 또는 그 이상의 다른 특징이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해 되어야 한다. 본 명세서에서 ‘단말’은 휴대성 및 이동성이 보장된 무선 통신 장치일 수 있으며, 예를 들어 스마트 폰, 태블 릿 PC 또는 노트북 등과 같은 모든 종류의 핸드헬드(Handheld) 기반의 무선 통신 장치일 수 있다. 또한, ‘단 말’은 네트워크를 통해 다른 단말 또는 서버 등에 접속할 수 있는 PC 등의 유선 통신 장치인 것도 가능하다. 또한, 네트워크는 단말들 및 서버들과 같은 각각의 노드 상호 간에 정보 교환이 가능한 연결 구조를 의미하는 것으로, 근거리 통신망(LAN: Local Area Network), 광역 통신망(WAN: Wide Area Network), 인터넷 (WWW: World Wide Web), 유무선 데이터 통신망, 전화망, 유무선 텔레비전 통신망 등을 포함한다. 무선 데이터 통신망의 일례에는 3G, 4G, 5G, 3GPP(3rd Generation Partnership Project), LTE(Long Term Evolution), WIMAX(World Interoperability for Microwave Access), 와이파이(Wi-Fi), 블루투스 통신, 적외선 통신, 초음파 통신, 가시광 통신(VLC: Visible Light Communication), 라이파이(LiFi) 등이 포함되나 이에 한 정되지는 않는다. 이하의 실시예는 본 발명의 이해를 돕기 위한 상세한 설명이며, 본 발명의 권리 범위를 제한하는 것이 아니다. 따라서 본 발명과 동일한 기능을 수행하는 동일 범위의 발명 역시 본 발명의 권리 범위에 속할 것이다. 또한, 본 발명의 각 실시예에 포함된 각 구성, 과정, 공정 또는 방법 등은 기술적으로 상호간 모순되지 않는 범 위 내에서 공유될 수 있다. 이하 첨부된 도면을 참고하여 본 발명의 일 실시예를 상세히 설명하기로 한다. 도 1은 본 발명의 일 실시예에 따른 사운드 기반의 지능형 위급상황 분석 시스템의 구성을 설명하는 도면이다. 도 1을 참조하면, 본 발명의 일 실시예에 따른 사운드 기반의 지능형 위급상황 분석 시스템은, 적어도 하나 이 상의 비상벨 장치, 분석 서버 및 관제 서버를 포함하지만 이에 한정되지는 않는다. 비상벨 장치는 각 우범 지역에 설치되어, 해당 우범 지역 내에서 발생되는 음향 또는 음성을 포함한 오디 오 데이터를 수집하여 수집된 오디오 데이터에서 위급 상황에 대한 이벤트를 감지하여 비상벨 작동 신호를 발생 한다. 이러한 비상벨 장치는 버튼식 비상벨과 소리인식모듈을 포함하는 소리인식 비상벨을 모두 포함할 수 있다. 비상벨 장치는 음향 수집을 위한 마이크(미도시), 비상벨 작동 신호와 오디오 데이터를 분석 서버로 전송하기 위한 통신 모듈(미도시), 메모리(미도시), 파손이나 강제 전원 꺼짐시 발생되는 경고장치(미도시), 제 어모듈(미도시) 등을 포함할 수 있다. 비상벨 장치는 일정 시간 단위(대략 10초)로 우범 지역(공중 화장실, 버스 정류장, 골목, 건물 사각지대 등)에서 발생되는 모든 음향 및 음성을 포함한 오디오 데이터를 버퍼(미도시)에 저장하고, 위급 상황에 대한 이 벤트가 감지되면 비상벨 작동신호를 발생시키며, 비상벨 작동신호가 발생되기 이전의 일정 시간 동안 녹음된 오 디오 데이터를 버퍼에서 불러와 비상벨 작동 신호와 함께 분석 서버로 전송한다. 이때, 비상벨 장치 는 버퍼에 저장된 오디오 데이터를 선입선출 방식으로 삭제하여 기설정된 용량 이상의 저장 용량을 확보할 수 있다. 분석 서버는 비상벨 장치로부터 비상벨 작동 신호가 수신되면 해당 비상벨 장치로부터 오디오 데이터를 수신하고, 오디오 데이터에서 음성 데이터를 추출한 후 음성 데이터를 분석하여 대화음성 상황을 분류 하고, 오디오 데이터에서 음성 데이터를 제외한 나머지 음향 데이터를 분석하여 음향 상황을 분류하며, 분류된 대화음성 상황과 음향 상황을 통합하여 기 설정된 분류 기준에 따라 단계별 코드로 구분되는 범죄 또는 위급 상 황을 판별하여 상황 분류 결과를 제공한다. 이때, 분석 서버는 비상벨 작동 신호가 발생되기 이전에 일정 시간동안 녹음된 음향 정보도 함께 수신하여 분석할 수 있어, 더욱 정확하게 현재 상황을 파악할 수 있다. 관제 서버는 분석 서버로부터 상황 분류 결과가 수신되면 비상벨 작동 신호가 발생된 우범 지역을 관 할하는 보안 단말에 상황 분류 결과에 근거한 범죄 코드에 따라 현장 출동 정보 또는 상황 대처 정보를 제 공한다. 이때, 분석 서버 및 관제 서버는 일반적인 의미의 서버용 컴퓨터 본체일 수 있고, 그 외에 서버 역할 을 수행할 수 있는 다양한 형태의 장치로 구현될 수 있다. 구체적으로, 분석 서버 및 관제 서버는 각 각 통신 모듈(미도시), 메모리(미도시), 프로세서(미도시) 및 데이터베이스(미도시)를 포함하는 컴퓨팅 장치에 구현될 수 있는데, 일례로 휴대폰이나 TV, PDA, 태블릿 PC, PC, 노트북 PC 및 기타 사용자 단말 장치 등으로 구 현될 수 있다. 또한, 보안 단말은 경찰서 또는 타기관과 연계하여 보안 요원의 출동 여부, 범죄 상황 알림 등을 수행하기 위해 무선 통신이 가능한 단말로서, 스마트폰, 태블릿 PC, PC, 노트북 PC 등으로 구현될 수 있다. 비상벨 장치는 관제 서버에 의해 지정된 고유한 식별 정보를 가지고, 비상벨 작동 신호와 상황 분류 결과는 해당 비상벨 장치의 식별 정보를 포함한다. 따라서, 분석 서버 및 관제 서버는 비상벨 장치의 식별 정보를 이용하여 해당 비상벨 장치가 설치된 장소, 즉 우범 지역의 위치 정보를 확인할 수 있고, 해당 우범 지역을 관할하는 보안 단말로 신속히 정보를 전송할 수 있다. 따라서, 분석 서버와 관제 서버는 데이터베이스에 각 비상벨 장치의 식별 정보, 각 우범 지역을 관할하는 보안 단말의 정보를 저장한다. 한편, 비상벨 장치는 우범 지역에 대한 현장 영상을 촬영하는 적어도 하나 이상의 카메라 장치를 더 포함할 수 있다. 예를 들어, 우범 지역이 버스정류장, 지하인도, 건물 옥상이나 건물 계단 등의 건물 사각지대 인 경우에 CCTV 등의 카메라 장치를 지하인도, 건물 옥상이나 계단 등 해당 우범 지역의 일측 상부에 설치 하고, 카메라 장치를 통해 현장 상황을 촬영할 수 있다. 관제 서버는 상황 분류 결과가 수신되면, 해당 우범 지역의 카메라 장치를 통해 현장 영상을 실시간 수신하고, 상황 분류 결과를 기초로 하여 현장 영상을 확인하면서 현재 상황을 기설정된 보안 레벨로 구분하고, 구분된 보안 레벨에 따라 현장 출동 정보 또는 상황 대처 정보를 발생시킬 수 있다. 이때, 관제 서버는 실시간 수신되는 현장 영상에 따라 보안 레벨을 수시로 변경할 수 있다. 도 2는 본 발명의 일 실시예에 따른 분석 서버의 구성을 설명하는 도면이고, 도 3은 본 발명의 일 실시예에 따 른 범죄 상황별 분류된 범죄 코드를 설명하는 도면이다. 도 2 및 도 3을 참조하면, 분석 서버는 통신부, 음성 추출부, 대화음성 상황 분석부, 음향 상황 분석부, 상황 판단부를 포함하지만 이에 한정되지는 않는다. 통신부는 비상벨 장치로부터 오디오 데이터를 수신하는데, 통신망과 연동하여 비상벨 장치 뿐만 아니라 관제 서버, 사용자 단말 간의 송수신 신호를 패킷 데이터 형태로 제공하는데 필요한 통신 인터페이 스를 제공한다. 여기서, 통신부는 다른 네트워크 장치와 유무선 연결을 통해 제어 신호 또는 데이터 신호 와 같은 신호를 송수신하기 위해 필요한 하드웨어 및 소프트웨어를 포함하는 장치일 수 있다. 음성 추출부는 오디오 데이터에서 사람의 음성 데이터를 추출하고, 대화음성 상황 분석부는 추출된 음성 데이터를 텍스트로 변환한 후 텍스트에 기반하여 대화음성 상황(협박 상황, 폭행 상황, 금전갈취 상황 등)을 분석한다. 음향 상황 분석부는 오디오 데이터에서 음성 데이터를 제외한 나머지 음향 데이터를 분석하여 음향 상황 (동물학대, 몰카설치, 기물파손, 주취자, 아이울음, 성인울음, 맹견짖음, 벌떼, 화재경보, 폭행 등)을 분석한다. 상황 판단부는 대화음성 상황 분석부에서 분석한 대화음성 상황과 음향상황 분석부에서 분석한 음향 상황을 통합하여 기 설정된 분류 기준에 따라 단계별 범죄 코드로 구분되는 범죄 또는 위급 상황을 판별하 여 상황 분류 결과를 제공한다. 도 3에 도시된 바와 같이, 범죄 코드는 5개의 보안 레벨(코드0~코드4)로 구분되고, 코드4에서 코드 0으로 갈수 록 출동 시간, 출동인원, 상황 대처의 심각성들이 높아짐을 알 수 있다. 예를 들어, 공중 화장실에 비상벨 장치 가 설치된 경우에, 분석 서버는 비상벨 작동 신호가 감지되고 공중 화장실 내에서 여자 비명이 감지 되면 범죄 코드를 코드0로 분류하고, 관제 서버로 범죄 코드와 범죄 상황(여자 화장실에 남자 출입 상황, 협박에 피해자가 흐느끼는 상황, 폭행하는 상황 등)에 대한 상황 분석 결과를 전송한다. 그러면, 관제 서버 는 상황 분석 결과를 통해 범죄 코드가 코드 0임을 확인하고, 최단 시간내에 경찰 등의 방범 요원이 출동 하고, 피해자의 안전과 가해자의 신속한 검거 등을 위해 구급차, 여성 경찰, 인접 지역의 경찰인력 등의 출동 요소와의 공조 출동 등의 현장 출동 정보 또는 상황 대처 정보를 지시할 수 있다. 이와 같이, 상황 판단부는 폭력 또는 위급 상황은 벌집, 화재, 유기견 발생 등의 상황 발생시 코드 4로 판 단하고, 아이울음, 성인울음, 맹견짖음 등의 상황 발생시 코드 3로 판단하며, 2인상의 괴롭힘 상황 발생시 코드 2로 판단하고, 동물학대, 기물파손, 몰카설치 등의 상황 발생시 코드 1로 판단하며, 금전갈취, 협박, 괴롭힘, 폭력 등의 상황 발생시 코드 0로 판단할 수 있다. 도 4는 본 발명의 일 실시예에 따른 사운드 기반의 지능형 위급상황 분석 방법을 설명하는 순서도이다. 도 5는 종래 기술의 일 실시예에 따른 Wave U-Net 구조를 설명하는 도면이고, 도 6은 본 발명의 일 실시예에 따른 Nested Wave U-Net 구조의 음성 분석 모델을 설명하는 도면이다. 도 4를 참조하면, 사운드 기반의 지능형 위급상황 분석 방법은, 분석 서버가 비상벨 장치로부터 비상 벨 작동 신호를 감지하면(S11), 실시간 현장 오디오 데이터를 수신한다(S12). 분석 서버는 현장 오디오 데이터에서 음성 데이터를 추출한 후(S13), 추출한 음성 데이터에 기반하여 대화 음성 상황을 분석한다(S14). 분석 서버는 범죄 또는 위급 상황에 해당하는 대화음성 관련 텍스트들을 학습 데이터로 수집하고, 수집된 학습 데이터를 이용하여 인공 지능 기반의 음성 분석 모델을 사전에 학습한 후 학습 된 음성 분석 모델에 기반하여 음성 데이터 추출 및 대화음성 상황을 분류할 수 있다. 이때, 음성 분석 모델은 End-to-End 방식의 Fully-Convolutional Network 기반 모델인 U-Net의 딥러닝 구조를 사용하여 알고리즘을 구성하고 있지만, 은닉층의 연결 구조를 달리함에 따라 VGGnet, GoogLeNet, ResNet, DenseNet, fully convolutional network, AlexNet 등 다양한 구조의 딥러닝을 사용할 수 있다. 특히, 음성 분석 모델은 음성 데이터 추출을 위해 Nested wave U-Net을 사용하여 음향과 음성이 혼합된 오디오 데이터에 2개의 음향 데이터와 음성 데이터를 나누게 된다. 도 5에 도시된 바와 같이, 기존의 Wave U-Net은 1D Convolution 모듈과 다운샘플링(downsampling) 모듈이 연속 으로 구성되어 앞쪽 특징 맵 크기가 줄어드는 수축 단계(Contracting Path)을 수행하고, 업샘플링(upsampling) 모듈의 연속 과정으로 뒤쪽 특징 맵 크기를 다시 늘려주는 팽창 단계(Expanding Path)을 수행한다. 이때, 1D Convolution 모듈을 통해서 시간 도메인(Time domain)에서 많은 하이레벨 특징 맵(High-level features map)을 추출해 내고, 다운 샘플링을 하며 시간 단계에 대해 특정한 패턴을 따르며 시간 샘플(time sample)들을 무시하 여 시간 분해능(Time resolution)을 절반으로 줄이게 된다. 그러나, 도 6에 도시된 바와 같이, 본 발명의 음성 분석 모델에 적용되는 Nested wave U-Net 구조는 아래로 향 하는 경로인 수축 단계와 위로 향하는 경로인 팽창 단계로 이루어져 수축 단계에서 입력 데이터에 대한 고차원 정보를 포착하고, 팽창 단계에서 세밀한 지역화(localization)를 진행한다. 이때, Nested wave U-Net 구조는 기 존의 Wave U-Net 구조와 다르게, 업샘플링 단계마다 각 단계에 해당하는 수축 단계(contracting path)의 특징 맵을 가져와 뒤로 이어 붙이게 되는데(concate), 특징 맵의 크기를 맞추기 위해 팽창 단계의 특징 맵의 크기에 맞춰 수축 단계의 특징맵을 적당한 크기로 crop(잘라냄)한 후 concat(붙임)하는 방식으로 오디오 데이터의 정보 를 유지하고 있다. 이와 같이, 음성 분석 모델은 Nested wave U-Net 구조를 이용하여 현장 오디오 데이터에서 잡음을 제거하고 깨 긋한 음질의 음성 데이터를 추출할 수 있다. 분석 서버는 오디오 데이터에서 음성 데이터를 제외한 음향 데이터를 추출하고, 추출한 음향 데이터에 기 반하여 음향 상황을 분석한다(S15). 분석 서버는 범죄 또는 위급 상황에 해당하는 음향 관련 소리들을 학 습 데이터로 수집하고, 수집된 학습 데이터를 이용하여 인공 지능 기반의 음향 분석 모델을 학습한 후 학습된 음향 분석 모델에 기반하여 음향 상황을 분류할 수 있다. 분석 서버는 분석된 대화음성 상황과 음향 상황을 통합하여 현장 오디오 데이터에 따른 상황이 범죄/위급 상황별 범죄 코드에 해당하는지를 분류한다(S16). 한편. 도 4의 각 단계들은 본 발명의 구현예에 따라서 추가적인 단계들로 분할되거나, 더 적은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계간의 순서가 변경될 수도 있다. 도 7은 본 발명의 일 실시예에 따른 분석 서버에서 대화음성 상황 및 음향 상황을 분석하는 과정을 설명하는 도 면이고, 도 8은 본 발명의 일 실시예에 따른 음성종류별 음성 발생 여부를 판별하는 과정을 설명하는 순서도이 며, 도 9는 본 발명의 일 실시예에 따른 음성 데이터 추출 과정을 설명하는 순서도이다. 도 7에 도시된 바와 같이, 분석 서버는 현장의 오디오 데이터(예를 들어, 10초간 현장 소리)를 수신하고 (S21), 음성종류별 음성 발생 여부를 판별한 후 음성 데이터를 추출한다(S22, S23). 도 8에 도시된 바와 같이, 분석 서버는 비명, 발자국, 맹견짖음 등의 50종 이상의 음향 데이터가 저장된 음향 DB와, 영어, 한국어, 중국어, 조선족어 등의 언어종류별 음성 데이터가 저장된 음성 DB를 구축한다. 따라서, 분석 서버는 음향 DB에서 1~3종의 음향 샘플 데이터를 랜덤으로 선택하고(S31), 음성 DB에서 1 종 류의 음성 샘플 데이터를 랜덤으로 선택한 후(S32), 음향 샘플 데이터와 음성 샘플 데이터를 서로 믹싱한 후 언 어종류를 라벨로 하여 언어 종류를 분류하기 위해 인공지능 기반의 음성 분석 모델을 학습한다(S33, S34). 따라 서, 분석 서버는 학습된 음성 분석 모델을 이용하여 현장 오디오 데이터에서 언어 종류를 확인할 수 있다. 도 9에 도시된 바와 같이, 분석 서버는 1~3종의 음향 샘플 데이터를 랜덤으로 선택하고(S41), 음성 DB에서 1 종류의 음성 샘플 데이터를 랜덤으로 선택한 후(S42), 음향 샘플 데이터와 음성 샘플 데이터를 서로 믹싱한 후 음성 DB에서 선택한 본래의 음성 데이터를 라벨로 하여 음성 데이터 추출을 위해 인공 지능 기반의 음성 분 석 모델을 학습한다(S43, S44). 따라서, 분석 서버는 학습된 음성 분석 모델을 이용하여 현장 오디오 데이 터에서 1종 이상의 언어 종류를 사용한 음성 신호를 추출할 수 있다. 다시 도 7을 참조하면, 분석 서버는 음성 종류 정보를 한국어 특성에 기반한 STT(Speech To Text)의 음성 처리 엔진에 제공하여, 음성 처리 엔진에서 인간의 언어를 텍스트로 변환한다(S24). 여기서, STT(Speech To Text)는 음성인식의 한 분야로서 사람의 음성언어를 컴퓨터의 해석으로 문자데이터로 변환하는 처리를 의미한다. 최근 STT를 위한 음성인식 엔진은 딥러닝(Deep Learning)알고리즘을 통해 음향과 언어 모델을 이용해 정확도를 높이고 있다. 전통적인 음성인식 알고리즘인 HMM(Hidden Markov Model) 이외에 딥러닝 기반 알고리즘 으로 널리 사용되는 DNN(Deep Neural Network)과 RNN(Recurrent Neural Network)기법을 적용함으로써 과거에 비해 높은 정확도를 보이고 있다. 따라서, 분석 서버는 한국어, 영어, 중국어, 조선족어 중 적어도 하나 이상의 언어종류를 포함한 음성 데 이터가 텍스트로 변환되고, 텍스트 기반의 대화음성 상황을 분류하며(S25), 음성방해신호에 강건한 음향 상황을 분류한 후 이 대화음성 상황과 음향 상황을 통합하여 폭력/위급상황별 범죄코드를 분류할 수 있다(S26, S27). 도 10은 본 발명에 일 실시예에 따른 대화음성 상황 분류를 위해 음성 분석 모델을 학습하는 과정을 설명하는 도면이고, 도 11은 본 발명에 일 실시예에 따른 학습된 음성 분석 모델을 이용해 대화음성 상황 분류하는 과정 을 설명하는 도면이다. 도 10을 참조하면, 분석 서버는 일반 대화, 금전갈취, 협박, 괴롭힘 등의 여러가지 대화 상황별 한국어 텍 스트 DB를 구축하고(S51), 하나 이상의 텍스트를 포함한 대화 상황에서 단어 순서를 변경하고, 단어(한국어, 중 국어, 영어, 유사어, 비속어 등)를 변경하는 방식으로 학습 데이터를 1차적으로 증강(Data Augmentation)한다 (S52). 분석 서버는 한국어 텍스트로 1차 증강된 DB를 영어 번역하여 영어 텍스트 증강 DB를 구축하고, 한국어 텍 스트로 1차 증강된 DB를 중국어 번역하여 중국어 텍스트 증강 DB를 구축한다(S53). 분석 서버는 한국어 텍스트로 1차 증강된 DB와 영어 텍스트 증강 DB, 중국어 텍스트 증강 DB를 통합하여 2 차 증강된 DB를 구축한 후(S54), 대화음성 상황 분류를 위해 2차 증강된 DB를 학습 데이터로 하여 음성 분석 모 델을 학습한다(S55). 도 11에 도시된 바와 같이, 분석 서버는 현장 오디오 데이터가 수신되면 현장 오디오 데이터에서 언어종류 별 음성 발생 여부를 판별한 후 음성 데이터를 추출한 후(S61, S62, S63), 추출한 음성 데이터를 STT를 통해 해 당 언어 종류에 따라 문자데이터로 변환한다(S64). 분석 서버는 변환된 문자 데이터가 한국어가 아닌 경우에 한국어 텍스트로 번역하고, 한국어로 번역된 대 화음성 텍스트를 학습된 음성 분석 모델에 입력하고, 음성 분석 모델은 입력된 대화음성 텍스트가 어떠한 대화 음성 상황에 해당하는지를 분류하여 출력한다(S65~S67). 대화음성 상황을 분석한다(S14). 분석 서버는 범죄 또는 위급 상황에 해당하는 대화음성 관련 텍스트들을 학습 데이터로 수집하고, 수집된 학습 데이터를 이용하여 인공 지능 기반의 음성 분석 모델을 사전에 학습한 후 학습된 음성 분석 모델에 기반하여 음성 데이터 추출 및 대화음성 상황을 분류할 수 있다. 이와 같이, 음성 분석 모델은 딥러닝 기반 알고리즘으로 구현될 수 있는데, 언어종류 분류, 음성 신호 추출, STT, 대화음성 상황 분류를 위해 각각의 심층신경망(DNN, deep neural networks)을 포함하는 다층 신경망 구조 로 형성될 수 있다. 도 12는 본 발명에 일 실시예에 따른 학습된 음향 분석 모델을 이용해 음향 상황 분류하는 과정을 설명하는 도 면이다. 도 12에 도시된 바와 같이, 분석 서버는 폭력 상황, 동물학대, 기물파손, 아이울음, 성인울음, 맹견, 벌떼, 화재경보, 몰카설치 등의 여러 상황별 음향 데이터를 저장한 음향 DB를 구축하고, 음향 DB에서 1종 상황 의 음향 샘플 데이터를 랜덤으로 선택하고, 음성 DB에서 1종의 음성 샘플 데이터를 랜덤으로 선택한다(S71, S72). 분석 서버는 음향 샘플 데이터와 음성 샘플 데이터를 서로 혼합하여 음향 상황 분류를 위해 음향 상황 종 류를 라벨로 하는 음향 분석 모델을 학습한다. 따라서, 분석 서버는 현장 오디오 데이터에서 추출한 음향 데이터를 학습된 음향 분석 모델에 입력하고, 학습된 음향 분석 모델을 통해 현장 오디오 데이터에 해당되는 음 향 상황을 분류할 수 있다. 이상에서 설명한 본 발명의 실시예는 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행 가능한 명령어를 포함하는 기록 매체의 형태로도 구현될 수 있다. 이러한 기록 매체는 컴퓨터 판독 가능 매체를 포함 하며, 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘 발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독가능 매체는 컴퓨터 저장 매체를 포 함하며, 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정 보의 저장을 위한 임의의 방법 또는 기술로 구현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함"}
{"patent_id": "10-2021-0156342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한다.전술한 본 발명의 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자는 본 발명 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 발명의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으 로 해석되어야 한다."}
{"patent_id": "10-2021-0156342", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 사운드 기반의 지능형 위급상황 분석 시스템의 구성을 설명하는 도면이다. 도 2는 본 발명의 일 실시예에 따른 분석 서버의 구성을 설명하는 도면이다. 도 3은 본 발명의 일 실시예에 따른 범죄 상황별 분류된 범죄 코드를 설명하는 도면이다. 도 4는 본 발명의 일 실시예에 따른 사운드 기반의 지능형 위급상황 분석 방법을 설명하는 순서도이다. 도 5는 종래 기술의 일 실시예에 따른 Wave U-Net 구조를 설명하는 도면이고, 도 6은 본 발명의 일 실시예에 따 른 Nested Wave U-Net 구조의 음성 분석 모델을 설명하는 도면이다. 도 7은 본 발명의 일 실시예에 따른 분석 서버에서 대화음성 상황 및 음향 상황을 분석하는 과정을 설명하는 도 면이다. 도 8은 본 발명의 일 실시예에 따른 음성종류별 음성 발생 여부를 판별하는 과정을 설명하는 순서도이다. 도 9는 본 발명의 일 실시예에 따른 음성 데이터 추출 과정을 설명하는 순서도이다. 도 10은 본 발명에 일 실시예에 따른 대화음성 상황 분류를 위해 음성 분석 모델을 학습하는 과정을 설명하는 도면이다. 도 11은 본 발명에 일 실시예에 따른 학습된 음성 분석 모델을 이용해 대화음성 상황 분류하는 과정을 설명하는 도면이다. 도 12는 본 발명에 일 실시예에 따른 학습된 음향 분석 모델을 이용해 음향 상황 분류하는 과정을 설명하는 도 면이다."}
