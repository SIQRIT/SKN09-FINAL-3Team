{"patent_id": "10-2024-0107707", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0024503", "출원번호": "10-2024-0107707", "발명의 명칭": "거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치 및 방법", "출원인": "(주)나라지식정보", "발명자": "이규민"}}
{"patent_id": "10-2024-0107707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치에 있어서,외부와 통신하도록 구성된 통신 회로;메모리; 및상기 통신 회로 및 상기 메모리와 전기적으로 연결된 프로세서를 포함하고,상기 프로세서는,상기 통신 회로를 이용하여 민원인의 질의를 포함하는 쿼리를 수신하고,상기 메모리에 저장된 제1 임베더 및 제2 임베더를 이용하여 상기 쿼리에 대응하는 밀집 벡터(dense vector) 및희소 벡터(sparse vector)를 생성하고,복수의 참조 문서 각각에 포함된 복수의 유닛 각각에 대응하는 밀집 벡터들과 상기 쿼리에 대응하는 밀집 벡터사이의 제1 관련성 벡터(relevance vector)를 산출하고,상기 복수의 참조 문서 각각에 포함된 상기 복수의 유닛 각각에 대응하는 희소 벡터들과 상기 쿼리에 대응하는희소 벡터 사이의 제2 관련성 벡터를 산출하고,상기 제1 관련성 벡터 및 상기 제2 관련성 벡터에 기초하여 상기 쿼리에 대응하는 참조 문서 및 상기 참조 문서에 포함된 유닛을 검색하고,상기 통신 회로를 이용하여 상기 참조 문서 및 상기 참조 문서에 포함된 유닛에 기초하여 상기 쿼리에 대응하는답변을 제공하는 것을 특징으로 하는, 장치."}
{"patent_id": "10-2024-0107707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 프로세서는,상기 제1 관련성 벡터 및 상기 제2 관련성 벡터를 스케일링(scaling)하고,상기 제1 관련성 벡터는 스케일링 대상 성분, 상기 제1 관련성 벡터의 성분들 중 최솟값, 상기 성분들과 상기최솟값 사이의 편차 및 제1 스케일링 파라미터에 기초하여 스케일링되고,상기 제2 관련성 벡터는 스케일링 대상 성분, 상기 제2 관련성 벡터의 성분들 중 최솟값, 상기 성분들과 상기최솟값 사이의 편차 및 제2 스케일링 파라미터에 기초하여 스케일링되는 것을 특징으로 하는, 장치."}
{"patent_id": "10-2024-0107707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 프로세서는,상기 복수의 참조 문서 각각의 상기 복수의 유닛 각각에 대한 키워드 및 상기 키워드에 대응하는 질문을 자동적으로 생성하고,상기 자동적으로 생성된 질문과 상기 복수의 유닛 사이의 제1 관련성 벡터들에 기초하여 상기 제1 임베더에 대한 상기 제1 스케일링 파라미터를 산출하고,상기 자동적으로 생성된 질문과 상기 복수의 유닛 사이의 제2 관련성 벡터들에 기초하여 상기 제2 임베더에 대한 상기 제2 스케일링 파라미터를 산출하는 것을 특징으로 하는, 장치.공개특허 10-2025-0024503-3-청구항 4 제 1 항에 있어서,상기 프로세서는,머신 러닝 모델을 이용하여 상기 질의에 대응하는 요약문을 생성하고,상기 복수의 유닛과 상기 요약문 사이의 제1 관련성 벡터 및 제2 관련성 벡터에 기초하여 상기 쿼리에 대응하는상기 참조 문서 및 상기 참조 문서에 포함된 유닛을 검색하고,인과적 언어 모델(causal language models)을 이용하여 상기 답변에 포함되는 컨텐츠를 생성하는 것을 특징으로하는, 장치."}
{"patent_id": "10-2024-0107707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "거대 언어 모델에 기반한 공공기관의 민원 업무 처리 방법에 있어서,민원인의 질의를 포함하는 쿼리를 수신하는 단계;제1 임베더 및 제2 임베더를 이용하여 상기 쿼리에 대응하는 밀집 벡터(dense vector) 및 희소 벡터(sparsevector)를 생성하는 단계;복수의 참조 문서 각각에 포함된 복수의 유닛 각각에 대응하는 밀집 벡터들과 상기 쿼리에 대응하는 밀집 벡터사이의 제1 관련성 벡터(relevance vector)를 산출하는 단계;상기 복수의 참조 문서 각각에 포함된 상기 복수의 유닛 각각에 대응하는 희소 벡터들과 상기 쿼리에 대응하는희소 벡터 사이의 제2 관련성 벡터를 산출하는 단계;상기 제1 관련성 벡터 및 상기 제2 관련성 벡터에 기초하여 상기 쿼리에 대응하는 참조 문서 및 상기 참조 문서에 포함된 유닛을 검색하는 단계; 및상기 참조 문서 및 상기 참조 문서에 포함된 유닛에 기초하여 상기 쿼리에 대응하는 답변을 제공하는 단계를 포함하는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 문서에 개시되는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치는 외부와 통신 하도록 구성된 통신 회로, 메모리, 및 통신 회로 및 메모리와 전기적으로 연결된 프로세서를 포함하고, 프로세서 는 통신 회로를 이용하여 민원인의 질의를 포함하는 쿼리를 수신하고, 메모리에 저장된 제1 임베더 및 제2 임베 (뒷면에 계속)"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 문서에서 개시되는 실시 예들은 거대 언어 모델을 이용하여 공공기관의 민원 업무를 처리하기 위한 장치 및 방법과 관련된다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "현재 대부분의 공공 기관은 온라인을 통한 민원 접수 시스템을 운영하고 있다. 온라인 민원 접수는 민원인에 대 한 편의성을 제공할 수 있고, 행정의 효율성이 향상될 수 있다. 온라인 시스템의 높은 접근성으로 인해 민원인 은 편리하고 신속하게 공공 기관과 의사 소통을 할 수 있다. 다만, 온라인 시스템의 도입에 따라 민원 건수가 지속적으로 증가하는 경우 민원을 처리하는 담당 공무원의 업 무 부담이 가중될 수 있고, 이는 공공 기관의 인력 관리에 어려움을 줄 수 있다. 예를 들어, 정부와 지방자치체 를 대상으로 한 정보공개 청구는 매년 증가하여 2017년 86만여건에서 2022년 182만건으로 5년간 2배 이상 증가 했다. 따라서, 온라인 민원에 대한 대응이 가능한 인공지능 시스템의 개발이 요구될 수 있고, 이러한 인공지능 시스템은 언어 모델에 의해 구현될 수 있다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "언어 모델을 이용하여 민원을 처리하기 위해서는 민원에 대한 요약문을 제공하고, 민원과 관련된 참조 문서를 검색하고, 민원에 대한 최종 답변을 작성하는 기능이 요구된다. 통상적인 거대 언어 모델(large language model)을 이용하여 질의에 답변하기 위한 포괄적인 시스템을 구축하기 위해서는 오픈 웹에 접근을 허용하거나매우 높은 수준의 리소스를 보유할 필요성이 있다. 그러나, 민원 업무를 처리하는 공공 기관의 특성으로 인해 오픈 웹에 대한 접근을 제한할 필요성이 있고, 거대 언어 모델을 이용한 시스템을 구축할 수 있는 높은 수준의 로컬 리소스를 보유하는 것은 현실적으로 어려울 수 있다. 따라서, 공공 기관의 특성을 고려하여 민원에 대한 답변 도출에 특화된 모델을 개발할 필요성이 있다. 본 발명의 실시 예들은, 언어 모델을 이용하여 민원 업무를 처리하기 위해 제한된 리소스를 갖는 시스템에서 동 작할 수 있는 온프레미스(on-premise) 솔루션을 제공하기 위한 것이다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 문서에 개시되는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치는 외부와 통 신하도록 구성된 통신 회로, 메모리, 및 통신 회로 및 메모리와 전기적으로 연결된 프로세서를 포함하고, 프로 세서는 통신 회로를 이용하여 민원인의 질의를 포함하는 쿼리를 수신하고, 메모리에 저장된 제1 임베더 및 제2 임베더를 이용하여 쿼리에 대응하는 밀집 벡터(dense vector) 및 희소 벡터(sparse vector)를 생성하고, 복수의 참조 문서 각각에 포함된 복수의 유닛 각각에 대응하는 밀집 벡터들과 쿼리에 대응하는 밀집 벡터 사이의 제1 관련성 벡터(relevance vector)를 산출하고, 복수의 참조 문서 각각에 포함된 복수의 유닛 각각에 대응하는 희 소 벡터들과 쿼리에 대응하는 희소 벡터 사이의 제2 관련성 벡터를 산출하고, 제1 관련성 벡터 및 제2 관련성 벡터에 기초하여 쿼리에 대응하는 참조 문서 및 참조 문서에 포함된 유닛을 검색하고, 통신 회로를 이용하여 참 조 문서 및 참조 문서에 포함된 유닛에 기초하여 쿼리에 대응하는 답변을 제공할 수 있다. 일 실시 예에 따르면, 프로세서는 제1 관련성 벡터 및 제2 관련성 벡터를 스케일링(scaling)하고, 제1 관련성 벡터는 스케일링 대상 성분, 제1 관련성 벡터의 성분들 중 최솟값, 성분들과 최솟값 사이의 편차 및 제1 스케일 링 파라미터에 기초하여 스케일링되고, 제2 관련성 벡터는 스케일링 대상 성분, 제2 관련성 벡터의 성분들 중 최솟값, 성분들과 최솟값 사이의 편차 및 제2 스케일링 파라미터에 기초하여 스케일링될 수 있다. 일 실시 예에 따르면, 프로세서는 복수의 참조 문서 각각의 복수의 유닛 각각에 대한 키워드 및 키워드에 대응 하는 질문을 자동적으로 생성하고, 자동적으로 생성된 질문과 복수의 유닛 사이의 제1 관련성 벡터들에 기초하 여 제1 임베더에 대한 제1 스케일링 파라미터를 산출하고, 자동적으로 생성된 질문과 복수의 유닛 사이의 제2 관련성 벡터들에 기초하여 제2 임베더에 대한 제2 스케일링 파라미터를 산출할 수 있다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "일 실시 예에 따르면, 프로세서는 머신 러닝 모델을 이용하여 질의에 대응하는 요약문을 생성하고, 복수의 유닛"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "과 요약문 사이의 제1 관련성 벡터 및 제2 관련성 벡터에 기초하여 쿼리에 대응하는 참조 문서 및 참조 문서에 포함된 유닛을 검색하고, 인과적 언어 모델(causal language models)을 이용하여 답변에 포함되는 컨텐츠를 생 성할 수 있다. 본 문서에 개시되는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 방법은 민원인의 질의를 포함하는 쿼리를 수신하는 단계, 제1 임베더 및 제2 임베더를 이용하여 쿼리에 대응하는 밀집 벡터 및 희소 벡터를 생성하는 단계, 복수의 참조 문서 각각에 포함된 복수의 유닛 각각에 대응하는 밀집 벡터들과 쿼리 에 대응하는 밀집 벡터 사이의 제1 관련성 벡터를 산출하는 단계, 복수의 참조 문서 각각에 포함된 복수의 유닛 각각에 대응하는 희소 벡터들과 쿼리에 대응하는 희소 벡터 사이의 제2 관련성 벡터를 산출하는 단계, 제1 관련 성 벡터 및 제2 관련성 벡터에 기초하여 쿼리에 대응하는 참조 문서 및 참조 문서에 포함된 유닛을 검색하는 단 계, 및 참조 문서 및 참조 문서에 포함된 유닛에 기초하여 쿼리에 대응하는 답변을 제공하는 단계를 포함할 수 있다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 문서에 개시되는 실시 예들에 따르면, 로컬 장치에서 실행될 수 있는 답변 생성을 위한 인과적 언어 모델을 구축함으로써, 내부 정보의 보안이 요구되는 공공 기관에서 사용 가능한 민원 처리 시스템을 제공할 수 있다. 또한, 참조 문서에 대한 밀집 벡터 및 희소 벡터와 질의에 대한 밀집 벡터 및 희소 벡터 사이의 관련성 벡터에 기초하여 민원에 대응하는 참조 문서를 검색함으로써, 민원의 해결을 위한 검색의 정확성과 효율성을 향상시킬 수 있다. 또한, 2개의 관련성 벡터를 스케일링하기 위한 파라미터를 자동적으로 산출함으로써, 2개의 관련성 벡터를 균형 있게 고려하여 참조 문서를 검색할 수 있다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "또한, 민원인의 질의뿐만 아니라 질의에 대한 요약문을 고려하여 참조 문서를 검색함으로써, 생성된 답변의 정 확도를 향상시킬 수 있다. 이 외에, 본 문서를 통해 직접적 또는 간접적으로 파악되는 다양한 효과들이 제공될 수 있다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 일부 실시 예들을 예시적인 도면을 통해서 상세하게 설명한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 실시 예의 다양한 변경, 균등물 또는 대체물을 포함하는 것으로 이해되어야 한다. 각 도면의 구성요소들에 참조부호를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 발 명의 실시 예를 설명함에 있어 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 발명의 실시 예에 대한 이해를 방해한다고 판단되는 경우에는 그 상세한 설명은 생략한다. 도 1은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 동작을 설명하기 위한 블록도이다. 도 1을 참조하면, 일 실시 예에 따른 민원 업무 처리 장치는 민원인으로부터 입력된 질의에 대응하는 답변을 생 성하기 위한 모델을 포함할 수 있다. 민원 업무 처리 장치는 기관 인트라넷의 모델 구동 서버로 구현될 수 있다. 민원 업무 처리 장치는 민원인이 어플리케이션을 통해 입력한 신규 민원을 수신할 수 있다. 민원 업무 처리 장"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "치는 신규 민원에 대한 답변을 생성할 수 있고, 답변은 요약문, 답변 내용 및 참조 문서 정보 등을 포함할 수 있다. 민원 업무 처리 장치는, 예를 들어, E2E(end-to-end) 모델을 이용하여 신규 민원에 포함된 질의에 대한"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "요약문을 생성할 수 있다. 민원 업무 처리 장치는, 예를 들어, 인과적 언어 모델을 이용하여 답변 내용을 생성할 수 있다. 민원 업무 처리 장치는, 예를 들어, 질의를 임베딩한 벡터와 참조 문서를 임베딩한 벡터 사이의 유 사도에 기초하여 참조 문서를 획득할 수 있다. 획득된 참조 문서는 답변 내용 생성에 활용될 수도 있다. 민원"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "업무 처리 장치는 요약문, 답변 내용 및 참조 문서 정보를 포함하는 답변을 생성할 수 있고, 답변의 구조는 미 리 지정된 형식으로 이루어질 수 있다. 민원 업무 처리 장치는 생성된 답변을 민원 담당자에게 출력할 수 있다. 민원 업무 처리 장치는 참조 문서에 대한 CRUD(create, read, update, delete) 요청을 수신할 수 있다. 민원 업무 처리 장치는 처리에 대한 유효성을 판단하고, 유효한 경우 CRUD를 수행하고, 유효하지 않은 경우 CRUD를 거절할 수 있다. 민원 업무 처리 장치는 결과를 출력할 수 있다. 민원 업무 처리 장치는 CRUD를 수행한 후 참조 문서 및 참조 문서에 대한 데이터베이스를 업데이트할 수 있다. 참조 문서에 대한 데이터베이스는, 예를 들어, 참조 문서를 임베딩한 벡터를 저장할 수 있다. 도 2는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 구성을 도시하는 블록도 이다. 도 2를 참조하면, 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치는 다양한 종류의 컴퓨팅 디바이스 중 하나로 구현될 수 있고, 물리적으로는 온프레미스에 구현될 수 있다. 일 실시 예에 따른 민원 업무 처리 장치는 통신 회로, 메모리 및 프로세서를 포함할 수 있다. 통신 회로는 외부(예: 외부 저장 장치 또는 외부 컴퓨팅 디바이스)와 무선 또는 유선으로 통신하는 인터페 이스일 수 있다. 통신 회로는 외부 장치와 데이터를 송수신할 수 있다. 메모리는 휘발성 메모리 및/또는 비휘발성 메모리를 포함할 수 있다. 메모리는 민원 업무 처리 장치 에서 취급되는 다양한 데이터를 저장할 수 있고, 언어 모델 및 임베더 등을 저장할 수 있다. 프로세서는 통신 회로 및 메모리와 전기적으로 연결될 수 있다. 프로세서는 통신 회로 및 메모리를 제어할 수 있고, 다양한 데이터 처리 및 연산을 수행할 수 있다. 프로세서는 메모 리에 저장된 소프트웨어 내지 인스트럭션을 실행함으로써, 이하와 같은 동작을 수행할 수 있다. 일 실시 예에 따르면, 프로세서는 통신 회로를 이용하여 민원인의 질의를 포함하는 쿼리를 수신할 수 있다. 민원인의 질의는 텍스트를 포함하고, 온라인 민원을 접수하는 웹 페이지를 통해 입력될 수 있다. 프로세 서는 웹 페이지를 통해 질의를 포함하는 쿼리를 수신할 수 있다. 일 실시 예에 따르면, 프로세서는 메모리에 저장된 제1 임베더 및 제2 임베더를 이용하여 쿼리에 대 응하는 밀집 벡터(dense vector) 및 희소 벡터(sparse vector)를 생성할 수 있다. 프로세서는 데이터 처 리를 위해 쿼리에 대응하는 벡터를 생성할 수 있다. 예를 들어, 제1 임베더는 사전 학습된 DPR(dense passage retrieval)일 수 있다. 제2 임베더는 BM25일 수 있다. 프로세서는 제1 임베더를 이용하여 쿼리에 대응하는 밀집 벡터를 생성하고, 제2 임베더를 이용하여 쿼리에 대응하는 희소 벡터를 생성할 수 있다. 생성된 밀집 벡터 및 희소 벡터는 메모리에 저장될 수 있다. 일 실시 예에 따르면, 프로세서는 복수의 참조 문서 각각에 포함된 복수의 유닛 각각에 대응하는 밀집 벡 터들과 쿼리에 대응하는 밀집 벡터 사이의 제1 관련성 벡터(relevance vector)를 산출할 수 있다. 프로세서 는 참조 문서를 복수의 유닛으로 분할할 수 있다. 프로세서는 참조 문서를 문장 단위 또는 의미 단위 로 분할할 수 있다. 프로세서는 미리 저장된 모든 참조 문서 각각에 포함된 모든 유닛 각각에 대응하는 밀 집 벡터들을 제1 임베더를 이용하여 생성할 수 있고, 유닛에 대응하는 밀집 벡터들은 미리 저장될 수 있다. DPR 은 구절 검색을 위해 사용되는 딥러닝 알고리즘 중 하나로, 구절 및 쿼리를 각각 처리하기 위한 두 개의 독립적 인 인코더를 포함할 수 있다. 2개의 하위 모듈은 구절 및 쿼리의 표현을 양방향으로 학습할 수 있다. 학습되거 나 활용되는 표현은 CLS(special classification token) 토큰의 임베딩으로 모델링될 수 있다. 제1 관련성 벡 터는 쿼리에 대응하는 밀집 벡터와 복수의 유닛 각각에 대응하는 밀집 벡터들 각각 사이의 관련성을 나타내는 벡터일 수 있다. 제1 관련성 벡터를 산출하기 위한 예시적인 수학식은 아래와 같다. [수학식 1] 여기서, q는 쿼리이고, Pi는 i번째 유닛이고, hCLS는 CLS 토큰의 임베딩이고, sDPR(q; P)i는 제1 관련성 벡터의 i 번째 성분을 의미한다. 즉, 제1 관련성 벡터의 i번째 성분은 쿼리 벡터와 i번째 유닛 벡터의 코사인 유사도로 정의될 수 있다. 제1 관련성 벡터의 각 성분의 범위는 -1 내지 +1일 수 있다. 일 실시 예에 따르면, 프로세서는 복수의 참조 문서 각각에 포함된 복수의 유닛 각각에 대응하는 희소 벡 터들과 쿼리에 대응하는 희소 벡터 사이의 제2 관련성 벡터를 산출할 수 있다. 프로세서는 미리 저장된 모 든 참조 문서 각각에 포함된 모든 유닛 각각에 대응하는 희소 벡터들을 제2 임베더를 이용하여 생성할 수 있고, 유닛에 대응하는 희소 벡터들은 미리 저장될 수 있다. BM25는 주어진 쿼리에 대해 문서와의 연관성을 평가하는 랭킹 함수로 사용되는 알고리즘으로, TF-IDF의 성능을 개선한 알고리즘이다. 제2 관련성 벡터는 쿼리에 대응하 는 희소 벡터와 복수의 유닛 각각에 대응하는 희소 벡터들 각각 사이의 관련성을 나타내는 벡터일 수 있다. 제2 관련성 벡터를 산출하기 위한 예시적인 수학식은 아래와 같다. [수학식 2]"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, f(qi, p)는 쿼리의 토큰 qi가 유닛 p에 나타난 횟수이고, |p|은 토큰 단위의 유닛 길이이고, avgl은 모 든 유닛의 평균 길이이고, n(qi)는 토큰 qi를 포함하는 문서의 개수이고, N은 가정된 문서의 개수이고, k1 및 b 는 하이퍼 파라미터이고, sBM25(q, P)j는 제2 관련성 벡터의 j번째 성분을 의미한다. 일 실시 예에 따르면, 프로세서는 제1 관련성 벡터 및 제2 관련성 벡터를 스케일링(scaling)할 수 있다. 프로세서는 제1 관련성 벡터와 제2 관련성 벡터의 앙상블에 의해 민원을 해결하기 위한 참조 문서를 검색 할 수 있다. 그러나, 제1 관련성 벡터와 제2 관련성 벡터의 각 성분의 범위가 상이하므로, 제1 관련성 벡터와 제2 관련성 벡터를 합산하였을 때 (가중치를 적용하더라도) 2개의 벡터 중 하나가 결과에 우세한 영향을 미치고, 나머지 하나는 무시될 수도 있다. 따라서, 제1 관련성 벡터와 제2 관련성 벡터에 대한 리스케일링 (rescaling)이 요구될 수 있다. 제1 관련성 벡터는 스케일링 대상 성분, 제1 관련성 벡터의 성분들 중 최솟값, 성분들과 최솟값 사이의 편차 및 제1 스케일링 파라미터에 기초하여 스케일링될 수 있고, 제2 관련성 벡터는 스 케일링 대상 성분, 제2 관련성 벡터의 성분들 중 최솟값, 성분들과 최솟값 사이의 편차 및 제2 스케일링 파라미 터에 기초하여 스케일링될 수 있다. 관련성 벡터에 대한 스케일링을 위한 예시적인 수학식은 아래와 같다. [수학식 3]"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서, model은 임베딩 모델을 의미하고, τ는 스케일링 파라미터(기본 값은 1로 설정될 수 있음)이고, 될 수 있고, s'model(q; P)j는 스케일링된 관련성 벡터의 j번째 성분일 수 있다. 스케일링된 관련성 벡터 s'model(q; P)의 각 성분의 합은 1일 수 있다. 이로써, 2개의 관련성 벡터가 모두 검색 결과에 동일한 영향을 미칠 수 있다. τ는 스케일링 파라미터로서 하이퍼 파라미터일 수 있다. τ를 조작함으로써, 관련성 벡터의 각 성분의 분포 모 양을 조정할 수 있다. τ가 높아지면 큰 값들 사이의 차이를 강조하고, τ가 작아지면 작은 값들 사이의 차이를 강조할 수 있다. 그러나, 하이퍼 파라미터의 스위핑에 요구되는 시간은 매우 길기 때문에, 효율을 위해 프로세 서는 관련성 벡터의 각 성분의 분포를 고려하여 τ를 자동적으로 조절할 수도 있다. 예를 들어, 프로세서는 복수의 참조 문서 각각의 복수의 유닛 각각에 대한 키워드 및 키워드에 대응하는 질문을 자동적으로 생성하고, 자동적으로 생성된 질문과 복수의 유닛 사이의 제1 관련성 벡터들에 기초하여 제1 임베더에 대한 제1 스케일링 파라미터를 산출하고, 자동적으로 생성된 질문과 복수의 유닛 사이의 제2 관련성 벡터들에 기초하여 제2 임베더에 대한 제2 스케일링 파라미터를 산출할 수 있다. 스케일링 파라미터의 산출에 대해서는 이하에서 도 9를 참조하여 상세히 설명한다. 스케일링 파라미터를 산출하기 위한 예시적인 수학식은아래와 같다. [수학식 4]"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "일 실시 예에 따르면, 프로세서는 제1 관련성 벡터 및 제2 관련성 벡터에 기초하여 쿼리에 대응하는 참조 문서 및 참조 문서에 포함된 유닛을 검색할 수 있다. 예를 들어, 프로세서는 스케일링된 제1 관련성 벡터 와 제2 관련성 벡터의 가중 합에 기초하여 관련성이 높은 참조 문서 및 유닛을 검색할 수 있다. 프로세서 는 관련성 벡터를 이용하여 관련성이 높은 순서대로 미리 지정된 개수의 유닛을 검색할 수도 있다. 관련성 벡터 의 가중 합을 산출하기 위한 예시적인 수학식은 아래와 같다. [수학식 5]"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, α는 제1 관련성 벡터에 대한 가중치로서, 하이퍼 파라미터일 수 있다. 일 실시 예에 따르면, 프로세서는 통신 회로를 이용하여 참조 문서 및 참조 문서에 포함된 유닛에 기"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "초하여 쿼리에 대응하는 답변을 제공할 수 있다. 답변은 인사말, 요약문, 답변 내용 및 참조 문서 정보 등을 포 함할 수 있다. 프로세서는 미리 설정된 구조에 따라 답변을 생성하고, 생성된 답변을 민원 담당자의 단말"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "또는 온라인 민원 시스템으로 제공할 수 있다. 참조 문서는 상술한 관련성 벡터에 의해 검색될 수 있고, 요약문 및 답변 내용은 이하와 같은 방식으로 생성될 수 있다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "일 실시 예에 따르면, 프로세서는 머신 러닝 모델을 이용하여 질의에 대응하는 요약문을 생성할 수 있다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "요약문 생성을 위한 머신 러닝 모델은 E2E 모델일 수 있고, 예를 들어, BART 또는 T5 등일 수 있다. 요약문의 생성은 질의를 짧게 만드는 것뿐만 아니라, 민원에 답신하는 공무원의 언어적 행동을 모방하고, 적절한 답변을 하기 위한 정보를 추출하는 것이다. 머신 러닝 모델은 한국어 데이터셋을 사용하여 사전 학습된 언어 모델(예: KoBART)일 수 있고, 언어 모델은 민원 로그 데이터로부터 획득된 데이터셋을 이용하여 파인 튜닝될 수 있다. 일 실시 예에 따르면, 프로세서는 인과적 언어 모델(causal language models)을 이용하여 답변에 포함되는 컨텐츠(답변 내용)를 생성할 수 있다. 인과적 언어 모델은 ChatGPT 등과 같은 통상적인 거대 언어 모델에 비해 서 상대적으로 작은 모델로서, 모델은 개인 정보가 제거된 민원을 포함하는 문서를 이용하여 사전 훈련될 수 있 다. 사전 훈련된 모델은 낮은 시스템 요구 사항을 보장하기 위해 매개변수의 수가 제한된 언어 모델로 선택될 수 있다. 인과적 언어 모델은, 예를 들어, Polyglot-ko-3.8b 또는 llama-3-8b 등일 수 있다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "요약문이 생성된 경우, 요약문을 이용하여 참조 문서가 검색될 수도 있다, 요약문은 적절한 답변을 하기 위한"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "정보를 추출하고 불필요한 정보를 제거한 것이므로, 요약문을 활용하는 경우 검색의 정확도가 향상될 수 있다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "예를 들어, 프로세서는 복수의 유닛과 요약문 사이의 제1 관련성 벡터 및 제2 관련성 벡터에 기초하여 쿼"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "리에 대응하는 참조 문서 및 참조 문서에 포함된 유닛을 검색할 수도 있다. 즉, 프로세서는 요약문에 대응"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "하는 밀집 벡터 및 희소 벡터를 생성하고, 요약문에 대응하는 밀집 벡터 및 희소 벡터와 참조 문서의 유닛들에 대응하는 밀집 벡터 및 희소 벡터 사이의 제1 관련성 벡터 및 제2 관련성 벡터를 산출할 수 있다. 산출된 제1 관련성 벡터 및 제2 관련성 벡터는 쿼리에 대한 그것과 유사한 방식으로 참조 문서 및 유닛의 검색에 사용될 수"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "있다. 또한, 요약문은 답변 내용의 생성에 활용될 수도 있다. 도 3은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 동작을 설명하기 위한 블록도이다. 도 3을 참조하면, 일 실시 예에 따른 민원 업무 처리 장치는 민원인이 어플리케이션을 통해 입력한 신규 민원을"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "수신할 수 있다. 민원 업무 처리 장치는 신규 민원에 대한 답변을 생성할 수 있고, 답변은 요약문, 답변 내용 및 참조 문서 정보 등을 포함할 수 있다. 설명의 편의를 위해 도 1을 참조하여 설명된 동작과 유사한 동작에 대 한 설명은 생략한다."}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "민원 업무 처리 장치는 생성된 요약문을 참조 문서의 검색에 활용할 수도 있다. 민원 업무 처리 장치는 (질의를 임베딩한 벡터를 대신하여 또는 질의를 임베딩한 벡터와 함께) 요약문을 임베딩한 벡터를 이용하여 참조 문서를"}
{"patent_id": "10-2024-0107707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "획득할 수 있다. 민원 업무 처리 장치는 요약문을 임베딩한 벡터와 참조 문서를 임베딩한 벡터 사이의 유사도에 기초하여 참조 문서를 획득할 수 있다. 민원 업무 처리 장치는 획득된 참조 문서를 답변 내용의 생성에 활용할 수도 있다. 민원 업무 처리 장치는 인과 적 언어 모델에 질의와 함께 참조 문서를 입력함으로써 답변 내용을 생성할 수 있다. 도 4는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 예시적인 참조 문서 검 색 동작을 설명하기 위한 도면이다. 도 4를 참조하면, 일 실시 예에 따른 민원 업무 처리 장치는 민원 처리를 위한 다수의 참조 문서를 확보할 수 있다. 민원 업무 처리 장치는 참조 문서를 구성하는 각각의 유닛에 대한 벡터 데이터베이스를 생성할 수 있다. 벡터 데이터베이스는 각각의 유닛에 대한 밀집 벡터, 희소 벡터 및 메타데이터를 포함할 수 있다. 민원 업무 처리 장치는 민원인의 질의를 밀집 벡터 및 희소 벡터를 포함하는 한 쌍의 벡터로 매핑할 수 있다. 민원 업무 처리 장치는 벡터 데이터베이스 및 질의에 매핑된 벡터에 대해 서치 루틴(search routine)을 실행함 으로써, 관련도가 높은 미리 지정된 개수(예: k개)의 참조 문서(또는 유닛)를 검색할 수 있다. 도 5는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 예시적인 참조 문서에 대한 임베딩 동작을 설명하기 위한 도면이다. 도 5를 참조하면, 일 실시 예에 따른 민원 업무 처리 장치는 참조 문서에 포함된 유닛들에 대한 밀집 벡터, 희 소 벡터 및 메타데이터를 생성할 수 있다. 민원 업무 처리 장치는 참조 문서를 KSS(Korean sentence splitter) 및/또는 DPR의 토크나이저를 이용하여 유닛으로 분할할 수 있다. 민원 업무 처리 장치는 유닛을 제1 임베더 DPR 및 제2 임베더 BM25에 적합하도록 피팅할 수 있다. 민원 업무 처리 장치는 DPR을 이용하여 각각의 유닛을 밀집 벡터로 임베딩할 수 있고, BM25를 이용하여 각각의 유닛을 희소 벡터로 임베딩할 수 있다. 민원 업무 데이터는 각각의 유닛에 대한 메타데이터를 생성할 수 있다. 민원 업무 처리 장치는 생성된 밀집 벡터, 희소 벡터 및 메타데이터를 데이터베이스에 저장할 수 있다. 도 6은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 예시적인 메타데이터 생 성 동작을 설명하기 위한 도면이다. 도 6을 참조하면, 참조 문서의 각 유닛에 대한 메타데이터는 참조 문서의 경로, 참조 문서의 ID, 유닛의 ID 및 유닛의 컨텐츠를 포함할 수 있다. 참조 문서의 ID는 각각의 참조 문서를 식별하기 위해 부여된 정보이고, 유닛 의 ID는 특정 참조 문서에 포함된 각각의 유닛을 식별하기 위해 부여된 정보일 수 있다. 유닛의 컨텐츠는 유닛 에 포함된 텍스트로 이루어진 스트링일 수 있다. 메타데이터의 ID는 참조 문서 ID 및 유닛 ID를 조합하여 생성 될 수 있다. 예를 들어, 참조 문서 ID가 a이고 유닛 ID가 b인 경우, 해당 유닛에 대한 메타데이터 ID는 a-b일 수 있다. 유닛에 대한 메타데이터는 블록-와이즈 리딩(Block-wise reading)을 지원하고, 벡터는 블록으로 그룹 화될 수 있다. 도 7은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 예시적인 관련도 벡터 산출 동작을 설명하기 위한 도면이다. 도 7을 참조하면, 일 실시 예에 따른 민원 업무 처리 장치는 쿼리에 대한 밀집 벡터 및 희소 벡터를 생성할 수 있다. 설명의 편의를 위해 밀집 벡터 및 희소 벡터는 8차원인 것으로 가정한다. 벡터 데이터베이스에는 참조 문서의 각 유닛에 대한 밀집 벡터가 미리 저장될 수 있다. 설명의 편의를 위해 유 닛의 총 개수는 6개인 것으로 가정한다. 이 경우, 벡터 데이터베이스에는 8차원의 밀집 벡터 6개가 저장될 수 있다. 민원 업무 처리 장치는 유닛들에 대한 6개의 밀집 벡터와 쿼리에 대한 밀집 벡터 사이의 유사도를 나타내 는 제1 관련성 벡터를 산출할 수 있다. 유닛의 총 개수가 6개인 경우, 6개의 유닛 밀집 벡터와 쿼리 밀집 벡터 사이의 유사도를 나타내는 제1 관련성 벡터는 6차원일 수 있다.벡터 데이터베이스에는 참조 문서의 각 유닛에 대한 희소 벡터가 미리 저장될 수 있다. 이 경우, 벡터 데이터베 이스에는 8차원의 희소 벡터 6개가 저장될 수 있다. 민원 업무 처리 장치는 유닛들에 대한 6개의 희소 벡터와 쿼리에 대한 희소 벡터 사이의 유사도를 나타내는 제2 관련성 벡터를 산출할 수 있다. 유닛의 총 개수가 6개인 경우, 6개의 유닛 희소 벡터와 쿼리 희소 벡터 사이의 유사도를 나타내는 제2 관련성 벡터는 6차원일 수 있다. 민원 업무 처리 장치는 제1 관련성 벡터와 제2 관련성 벡터의 앙상블에 의해 관련도가 높은 2개의 문서(검색되 는 문서의 개수는 다양하게 설정될 수 있다)를 검색할 수 있다. 도 8은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치에서 산출되는 스케일링 파 라미터의 특성을 도시하는 그래프이다. 도 8을 참조하면, 그래프의 가로 축은 관련성 벡터의 각 성분을 정규화한 값이고, 세로 축은 스케일링된 확률 분포를 의미할 수 있다. τ는 하이퍼 파라미터일 수도 있고, 도 9를 참조하여 설명될 것과 같이 민원 업무 처리 장치에 의해 최적화될 수도 있다. τ를 작게 설정하는 경우, 가장 위에 표시된 그래프(τ=0.125)와 같이 작은 성분 값 사이의 차이를 강조할 수 있다. τ를 크게 설정하는 경우, 가장 아래에 표시된 그래프(τ=8)와 같이 큰 성분 값 사이의 차이를 강조할 수 있다. τ는 관련성 벡터의 각 성분의 분포에 따라 적절히 설정될 필요성이 있 다. 도 9는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 예시적인 스케일링 파라 미터 산출 동작을 설명하기 위한 도면이다. 도 9를 참조하면, 일 실시 예에 따른 민원 업무 처리 장치는 관련성 벡터의 스케일링에 사용되는 파라미터를 산 출할 수 있다. 민원 업무 처리 장치는 참조 문서의 각 유닛으로부터 키워드를 추출할 수 있다. 키워드는 TF-IDF 를 이용하여 추출될 수 있다. 여기서, 키워드는 복수의 단어를 포함할 수도 있고, 이 경우 키 프레이즈(key phrase)로 언급될 수도 있다. 민원 업무 처리 장치는 템플릿에 기반하여 질문을 자동적으로 생성할 수 있다. 질문의 형식은 미리 정의될 수 있고, 그 형식과 키워드를 결합함으로써 질문이 생성될 수 있다. 민원 업무 처리 장치는 2개의 모델(예: DPR 및 BM25)에 의해 산출되는 관련성 벡터에 대한 엔트로피(본 발명에 서 벡터의 성분의 불확실성(uncertainty)의 정도를 나타내는 값으로 정의됨)를 산출할 수 있다. 통상적으로, 불 확실성은 DPR이 높고, BM25이 낮을 수 있다. 모델 자체의 불확실성을 예측하기 위해 τ로 설정하고, 수학식 4에 따라 2개의 모델에 대한 엔트로피를 각각 산출할 수 있다. 민원 업무 처리 장치는 산출된 엔트로피( )에 제곱근을 하여 τ를 산출하고, 산출된 τ를 수학식 3에 적 용할 수 있다. 엔트로피에 제곱근을 취하는 것은 경험적으로(empirically) 결정되었으며, 필요에 따라 엔트로피 에는 다양한 연산이 적용될 수 있다. 도 10은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 방법을 설명하기 위한 흐름도 이다. 이하에서는 도 2의 민원 업무 처리 장치가 도 10의 프로세스를 수행하는 것을 가정한다. 또한, 도 10의 설명에 서, 민원 업무 처리 장치에 의해 수행되는 것으로 기술된 동작은 프로세서에 의해 제어되는 것으로 이해될 수 있다. 도 10을 참조하면, 단계 1010에서, 민원 업무 처리 장치는 민원인의 질의를 포함하는 쿼리를 수신할 수 있다. 단계 1020에서, 민원 업무 처리 장치는 제1 임베더 및 제2 임베더를 이용하여 쿼리에 대응하는 밀집 벡터 및 희 소 벡터를 생성할 수 있다. 단계 1030에서, 민원 업무 처리 장치는 복수의 참조 문서 각각에 포함된 복수의 유닛 각각에 대응하는 밀집 벡 터들과 쿼리에 대응하는 밀집 벡터 사이의 제1 관련성 벡터를 산출할 수 있다. 단계 1040에서, 민원 업무 처리 장치는 복수의 참조 문서 각각에 포함된 복수의 유닛 각각에 대응하는 희소 벡 터들과 쿼리에 대응하는 희소 벡터 사이의 제2 관련성 벡터를 산출할 수 있다. 단계 1050에서, 민원 업무 처리 장치는 제1 관련성 벡터 및 제2 관련성 벡터에 기초하여 쿼리에 대응하는 참조 문서 및 참조 문서에 포함된 유닛을 검색할 수 있다. 단계 1060에서, 민원 업무 처리 장치는 참조 문서 및 참조 문서에 포함된 유닛에 기초하여 쿼리에 대응하는 답 변을 제공할 수 있다. 본 문서의 실시 예들 및 이에 사용된 용어들은 본 문서에 기재된 기술을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 해당 실시 예의 다양한 변경, 균등물, 및/또는 대체물을 포함하는 것으로 이해되어야 한다. 도면 의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조 부호가 사용될 수 있다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 본 문서에서, \"A 또는 B\", \"A 및/또는 B 중 적어도 하나\", \"A, B 또는 C\" 또는 \"A, B 및/또는 C 중 적어도 하나\" 등의 표현은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\" 등의 표현들은 해당 구성요소들을, 순서 또는 중요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요 소들을 한정하지 않는다. 어떤 구성요소가 다른 구성요소에 \"(기능적으로 또는 통신적으로) 연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소를 통하여 연결될 수 있다. 본 문서에서, \"~하도록 설정된(adapted to or configured to)\"은 상황에 따라, 예를 들면, 하드웨어적 또는 소 프트웨어적으로 \"~에 적합한,\" \"~하는 능력을 가지는,\" \"~하도록 변경된,\" \"~하도록 만들어진,\" \"~를 할 수 있 는,\" 또는 \"~하도록 설계된\"과 상호 호환적으로(interchangeably) 사용될 수 있다. 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장치가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행하도록 설정된 (또는 구성된) 프로세서\"는 해당 동작들을 수행하기 위한 전용 프 로세서(예: 임베디드 프로세서), 또는 메모리 장치에 저장된 하나 이상의 프로그램들을 실행함으로써, 해당 동 작들을 수행할 수 있는 범용 프로세서(예: CPU)를 의미할 수 있다. 본 문서에서 사용된 용어 \"모듈\"은 하드웨어, 소프트웨어 또는 펌웨어(firmware)로 구성된 유닛(unit)을 포함하 며, 예를 들면, 로직, 논리 블록, 부품, 또는 회로 등의 용어와 상호 호환적으로 사용될 수 있다. \"모듈\"은, 일 체로 구성된 부품 또는 하나 또는 그 이상의 기능을 수행하는 최소 단위 또는 그 일부가 될 수 있다. \"모듈\"은 기계적으로 또는 전자적으로 구현될 수 있으며, 예를 들면, 어떤 동작들을 수행하는, 알려졌거나 앞으로 개발될, ASIC(application-specific integrated circuit) 칩, FPGAs(field-programmable gate arrays), 또는 프로그램 가능 논리 장치를 포함할 수 있다. 일 실시 예에 따른 장치(예: 모듈들 또는 그 기능들) 또는 방법(예: 동작들)의 적어도 일부는 프로그램 모듈의 형태로 컴퓨터로 판독 가능한 저장 매체에 저장된 명령어로 구현될 수 있다. 상기 명령어가 프로세서에 의해 실 행될 경우, 프로세서가 상기 명령어에 해당하는 기능을 수행할 수 있다. 일 실시 예에 따른 구성 요소(예: 모듈 또는 프로그램 모듈) 각각은 단수 또는 복수의 개체로 구성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요소를 더 포함할 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로그램 모듈)은 하나의 개체 로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 일 실시 예에 따른 모듈, 프로그램 모듈 또는 다른 구성 요소에 의해 수행되는 동작들은 순차적, 병렬적, 반복적 또는 휴리스틱(heuristic)하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10"}
{"patent_id": "10-2024-0107707", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 동작을 설명하기 위한 블록도이다. 도 2는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 구성을 도시하는 블록도 이다. 도 3은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 동작을 설명하기 위한 블록도이다. 도 4는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 예시적인 참조 문서 검 색 동작을 설명하기 위한 도면이다. 도 5는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 예시적인 참조 문서에 대한 임베딩 동작을 설명하기 위한 도면이다. 도 6은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 예시적인 메타데이터 생 성 동작을 설명하기 위한 도면이다. 도 7은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 예시적인 관련도 벡터 산출 동작을 설명하기 위한 도면이다. 도 8은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치에서 산출되는 스케일링 파 라미터의 특성을 도시하는 그래프이다. 도 9는 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 장치의 예시적인 스케일링 파라 미터 산출 동작을 설명하기 위한 도면이다. 도 10은 일 실시 예에 따른 거대 언어 모델에 기반한 공공기관의 민원 업무 처리 방법을 설명하기 위한 흐름도 이다. 도면의 설명과 관련하여, 동일 또는 유사한 구성요소에 대해서는 동일 또는 유사한 참조 부호가 사용될 수 있다."}
