{"patent_id": "10-2024-0141348", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0034899", "출원번호": "10-2024-0141348", "발명의 명칭": "멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치 및 그 동작 방법", "출원인": "주식회사 파일러", "발명자": "박동찬"}}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "저장부; 및 프로세서;를 포함하고,상기 프로세서는,동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하고,텍스트로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하고,서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 상기 복수의 제1 시각적 특징과 상기 복수의 텍스트 특징으로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적융합 특징을 획득하고,상기 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하며,상기 프로세서는,상기 복수의 제1 지역적 융합 특징을 획득하기 위해,상기 제1 시각적 특징과 상기 텍스트 특징으로부터 제1 중간 지역적 융합 특징을 획득하고,컨벌루션을 이용해 상기 제1 중간 지역적 융합 특징으로부터 제2 중간 지역적 융합 특징을 획득하고,상기 제1 시각적 특징과 상기 제2 중간 지역적 융합 특징으로부터 제1 지역적 융합 특징을 획득하는, 멀티-모달시간 축 융합 인공지능 모델을 위한 전자 장치."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 프로세서는,상기 복수의 텍스트 특징을 획득하기 위해, 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 맵핑 층을 이용해 상기 텍스트로부터 상기 복수의 시점또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는, 멀티-모달 시간 축 융합 인공지능 모델을 위한전자 장치."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서, 상기 프로세서는,상기 복수의 제1 시각적 특징을 획득하기 위해,상기 동영상으로부터 복수의 객체 특징을 획득하고,상기 복수의 객체 특징으로부터 복수의 객체 그래프를 획득하고,상기 복수의 객체 그래프로부터 상기 복수의 제1 시각적 특징을 획득하는, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치.공개특허 10-2025-0034899-3-청구항 4 제1 항에 있어서, 상기 프로세서는,상기 동영상으로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 시각적 특징을 획득하고,서로 같은 시점 또는 시구간에 대응하는 제2 시각적 특징과 텍스트 특징을 융합함으로써 상기 복수의 제2 시각적 특징과 상기 복수의 텍스트 특징으로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 지역적융합 특징을 획득하고,상기 복수의 제1 지역적 융합 특징 및 상기 복수의 제2 지역적 융합 특징으로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 제3 지역적 융합 특징을 획득하고,상기 복수의 제3 지역적 융합 특징으로부터 상기 적어도 하나의 전역적 융합 특징을 획득하는, 멀티-모달 시간축 융합 인공지능 모델을 위한 전자 장치."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서, 상기 프로세서는,상기 적어도 하나의 전역적 융합 특징을 획득하기 위해,셀프-어텐션을 이용해 상기 복수의 제1 지역적 융합 특징으로부터 상기 적어도 하나의 전역적 융합 특징을 획득하는, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 텍스트는 질문을 포함하고,상기 프로세서는,상기 적어도 하나의 전역적 융합 특징을 이용해 상기 질문에 대한 답을 획득하는, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 적어도 하나의 전역적 융합 특징은 복수의 전역적 융합 특징을 포함하고,상기 프로세서는,상기 질문에 대한 답을 획득하기 위해,상기 복수의 전역적 융합 특징으로부터 요약 벡터를 획득하고,복수의 선택지로부터 각각 복수의 선택지 특징을 획득하고,상기 요약 벡터와 각각의 선택지 특징으로부터 상기 질문에 대한 답을 획득하는, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치.공개특허 10-2025-0034899-4-청구항 8 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계;텍스트로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계;서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 상기 복수의 제1 시각적 특징과 상기 복수의 텍스트 특징으로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적융합 특징을 획득하는 단계; 및상기 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단계;를 포함하며,상기 복수의 제1 지역적 융합 특징을 획득하는 단계는,상기 제1 시각적 특징과 상기 텍스트 특징으로부터 제1 중간 지역적 융합 특징을 획득하는 단계;컨벌루션을 이용해 상기 제1 중간 지역적 융합 특징으로부터 제2 중간 지역적 융합 특징을 획득하는 단계; 및상기 제1 시각적 특징과 상기 제2 중간 지역적 융합 특징으로부터 제1 지역적 융합 특징을 획득하는 단계;를 포함하는, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치의 동작 방법."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 복수의 텍스트 특징을 획득하는 단계는, 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 맵핑 층을 이용해 상기 텍스트로부터 상기 복수의 시점또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계를 포함하는, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치의 동작 방법."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8 항에 있어서, 상기 복수의 제1 시각적 특징을 획득하는 단계는,상기 동영상으로부터 복수의 객체 특징을 획득하는 단계; 상기 복수의 객체 특징으로부터 복수의 객체 그래프를 획득하는 단계; 및상기 복수의 객체 그래프로부터 상기 복수의 제1 시각적 특징을 획득하는 단계를 포함하는, 멀티-모달 시간 축융합 인공지능 모델을 위한 전자 장치의 동작 방법."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8 항에 있어서, 상기 동영상으로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 시각적 특징을 획득하는 단계;서로 같은 시점 또는 시구간에 대응하는 제2 시각적 특징과 텍스트 특징을 융합함으로써 상기 복수의 제2 시각적 특징과 상기 복수의 텍스트 특징으로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 지역적융합 특징을 획득하는 단계; 및상기 복수의 제1 지역적 융합 특징 및 상기 복수의 제2 지역적 융합 특징으로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 제3 지역적 융합 특징을 획득하는 단계;를 더 포함하고,상기 적어도 하나의 전역적 융합 특징을 획득하는 단계는, 상기 복수의 제3 지역적 융합 특징으로부터 상기 적어도 하나의 전역적 융합 특징을 획득하는 단계를 포함하는, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전공개특허 10-2025-0034899-5-자 장치의 동작 방법."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8 항에 있어서, 상기 적어도 하나의 전역적 융합 특징을 획득하는 단계는,셀프-어텐션을 이용해 상기 복수의 제1 지역적 융합 특징으로부터 상기 적어도 하나의 전역적 융합 특징을 획득하는 단계를 포함하는, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치의 동작 방법."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8 항에 있어서,상기 텍스트는 질문을 포함하고,상기 적어도 하나의 전역적 융합 특징을 이용해 상기 질문에 대한 답을 획득하는 단계를 더 포함하는, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치의 동작 방법."}
{"patent_id": "10-2024-0141348", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13 항에 있어서,상기 적어도 하나의 전역적 융합 특징은 복수의 전역적 융합 특징을 포함하고,상기 질문에 대한 답을 획득하는 단계는,상기 복수의 전역적 융합 특징으로부터 요약 벡터를 획득하는 단계;복수의 선택지로부터 각각 복수의 선택지 특징을 획득하는 단계; 및상기 요약 벡터와 각각의 선택지 특징으로부터 상기 질문에 대한 답을 획득하는 단계;를 포함하는, 멀티-모달시간 축 융합 인공지능 모델을 위한 전자 장치의 동작 방법."}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치가 개시된다. 본 전자 장치는 저장부, 및 프로세서를 포 함하고, 프로세서는, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특 징을 획득하고, 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하고, 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제1 시각적 특징과 복 수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적 융합 특징을 획득하고, 복수의 제1 지역적 융합 특징으로부터 복수의 전역적 융합 특징을 획득할 수 있다."}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 전자 장치 및 그 동작 방법에 관한 것이다. 보다 구체적으로 본 발명은 멀티-모달 시간 축 융합 인공 지능 모델을 위한 전자 장치 및 그 동작 방법에 관한 것이다."}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "서로 다른 형태의 입력을 종합적으로 이용할 수 있는 멀티-모달 인공지능 모델에 대한 연구가 활발히 진행되고 있다. 최근에는 정지 영상-텍스트 멀티-모달 인공지능 모델을 넘어 동영상-텍스트 멀티-모달 인공지능 모델에"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "대한 연구가 높은 관심을 받고 있다. 기존의 동영상-텍스트 멀티-모달 인공지능 모델은 동영상으로부터 요약된"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "시각적 벡터를 획득하고, 텍스트로부터 텍스트 벡터를 획득하고, 요약된 시각적 벡터와 텍스트 벡터를 융합하는 방법을 사용하였다. 이로 인해 시각적 특징과 텍스트 특징이 시간 축 상에서 세밀하게 융합되지 못해 기존의 동 영상-텍스트 멀티-모달 인공지능 모델의 성능에 한계가 있었다."}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 4, "content": "발명의 내용"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는 향상된 성능의 동영상-텍스트 멀티-모달 인공지능 모델을 위한 전자 장치 및 그 동작 방법을 제공하는 것이다."}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치는, 저장부, 및 프로세 서를 포함하고, 상기 프로세서는, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하고, 텍스트로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징 을 획득하고, 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 상기 복 수의 제1 시각적 특징과 상기 복수의 텍스트 특징으로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수 의 제1 지역적 융합 특징을 획득하고, 상기 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득할 수 있다. 본 발명의 일 실시예에 따르면, 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치의 동작 방법은, 동영 상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계, 텍 스트로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계, 서로 같은 시 점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 상기 복수의 제1 시각적 특징과 상 기 복수의 텍스트 특징으로부터 상기 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적 융합 특징을 획득하는 단계, 및 상기 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단 계;를 포함할 수 있다."}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 본 발명은 텍스트 특징을 시간 축으로 확장하고, 각 시점 또는 시구간에서 지역 적으로 시각적 특징과 텍스트 특징을 융합하고, 여러 시점 또는 시구간에 걸쳐 전역적으로 지역적 융합 특징을 시간 축으로 융합함으로써 즉, 멀티-레벨 시간 축 융합을 수행함으로써 시각적 특징과 텍스트 특징의 보다 미세 하고 완전한 융합을 가능하게 하여 동영상-텍스트 멀티-모달 인공지능 모델의 성능을 향상시킬 수 있다."}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명의 바람직한 실시예에 대한 동작 원리를 상세히 설명한다. 또한, 발명에 대한 실시예를 설명함에 있어 관련된 공지 기능 또는 구성에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명을 생략할 것이다. 그리고 하기에서 사용되는 용어들은 본 발명에서의기능을 고려하여 정의된 용어들로써, 이는 사용자, 운용자의 의도 또는 관례 등에 따라 달라질 수 있다. 그러므 로 사용된 용어들의 정의는 본 명세서 전반에 걸친 내용 및 이에 상응한 기능을 토대로 해석되어야 할 것이다. 도 1은 본 발명의 일 실시예에 따른 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치의 블록도이다. 도 1을 참조하면, 전자 장치는 저장부 및 프로세서를 포함할 수 있다. 저장부는 각종 데이터 및 프로그램을 저장할 수 있다. 예를 들어, 저장부는 멀티-모달 시간 축 융합 인공지능 모델(MD, MD' 각각 도 2 및 도 7 참조)을 저장할 수 있다. 일 실시예에서, 저장부는 인공지능 모 델에 입력되는 동영상 데이터 및 텍스트 데이터를 저장할 수 있다. 저장부는 휘발성 메모리 및 비휘발성 메모리 중 적어도 하나를 포함할 수 있다. 예를 들어, 휘발성 메모리는 DRAM, SRAM, SDRAM, DDR SDRAM, FeRAM, MRAM, PRAM, PoRAM, 또는 ReRAM을 포함할 수 있다. 예를 들어, 비휘발성 메모리는 플래시 메모리, mask ROM, PROM, OTPROM, EPROM, EEPROM, 하드디스크, 또는 광디스크를 포함할 수 있다. 프로세서는 전자 장치의 동작 전반을 제어할 수 있다. 프로세서는 저장부를 제어할 수 있 다. 예를 들어, 프로세서는 중앙 처리 장치(CPU) 및 그래픽 처리 장치(GPU) 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 프로세서는, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하고, 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획 득하고, 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제1 시 각적 특징과 복수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적(local) 융합 특징을 획득하고, 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적(global) 융합 특징을 획득 할 수 있다. 일 실시예에서, 프로세서는, 복수의 텍스트 특징을 획득하기 위해, 복수의 시점 또는 시구간에 각각 대응 하는 복수의 맵핑 층을 이용해 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득할 수 있다. 일 실시예에서, 프로세서는, 복수의 텍스트 특징을 획득하기 위해, 텍스트로부터 복수의 제1 중간 텍스트 특징을 획득하고, 복수의 시점 또는 시구간에 각각 대응하는 복수의 맵핑 층을 이용해 복수의 제1 중간 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 중간 텍스트 특징을 획득하고, 제2 중간 텍 스트 특징의 차원을 변경함으로써 복수의 제2 중간 텍스트 특징으로부터 복수의 텍스트 특징을 획득할 수 있다. 일 실시예에서, 복수의 제1 중간 텍스트 특징의 개수는 복수의 제2 중간 텍스트 특징의 개수와 상이할 수 있다. 일 실시예에서, 복수의 텍스트 특징 각각의 차원은 복수의 제1 시각적 특징 각각의 차원과 동일할 수 있다. 일 실시예에서, 프로세서는, 복수의 제1 지역적 융합 특징을 획득하기 위해, 컨벌루션(convolution)을 이 용해 제1 시각적 특징과 텍스트 특징으로부터 제1 지역적 융합 특징을 획득할 수 있다. 일 실시예에서, 프로세서는, 복수의 제1 지역적 융합 특징을 획득하기 위해, 제1 시각적 특징과 텍스트 특 징으로부터 제1 중간 지역적 융합 특징을 획득하고, 컨벌루션을 이용해 제1 중간 지역적 융합 특징으로부터 제2 중간 지역적 융합 특징을 획득하고, 제1 시각적 특징과 제2 중간 지역적 융합 특징으로부터 제1 지역적 융합 특 징을 획득할 수 있다. 일 실시예에서, 프로세서는, 복수의 제1 시각적 특징을 획득하기 위해, 동영상으로부터 복수의 객체 특징 을 획득하고, 복수의 객체 특징으로부터 복수의 객체 그래프를 획득하고, 복수의 객체 그래프로부터 복수의 제1 시각적 특징을 획득할 수 있다. 일 실시예에서, 프로세서는, 동영상으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 시각적 특징을 획득하고, 서로 같은 시점 또는 시구간에 대응하는 제2 시각적 특징과 텍스트 특징을 융합함으로써 복수 의 제2 시각적 특징과 복수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 지역 적 융합 특징을 획득하고, 복수의 제1 지역적 융합 특징 및 복수의 제2 지역적 융합 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제3 지역적 융합 특징을 획득하고, 복수의 제3 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득할 수 있다. 일 실시예에서, 프로세서는, 적어도 하나의 전역적 융합 특징을 획득하기 위해, 셀프-어텐션(self- attention)을 이용해 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득할 수 있다.일 실시예에서, 텍스트는 질문을 포함하고, 프로세서는 적어도 하나의 전역적 융합 특징을 이용해 질문에 대한 답을 획득할 수 있다. 일 실시예에서, 적어도 하나의 전역적 융합 특징은 복수의 전역적 융합 특징을 포함하고, 프로세서는, 질"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "문에 대한 답을 획득하기 위해, 복수의 전역적 융합 특징으로부터 요약 벡터를 획득하고, 복수의 선택지로부터"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "각각 복수의 선택지 특징을 획득하고, 요약 벡터와 각각의 선택지 특징으로부터 질문에 대한 답을 획득할 수 있 다. 일 실시예에서, 프로세서는, 적어도 하나의 전역적 융합 특징을 이용해 동영상을 분류, 검색, 예측, 또는 생성할 수 있다. 도 2는 본 발명의 일 실시예에 따른 멀티-모달 시간 축 융합 인공지능 모델의 개념도이다. 도 2를 참조하면, 멀 티-모달 시간 축 융합 인공지능 모델(MD)은 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하 는 복수의 제1 시각적 특징(Va1 내지 Van)을 획득하기 위한 제1 시각적 특징 추출 모듈(MD1a)을 포함할 수 있다. 일 실시예에서, 제1 시각적 특징 추출 모듈(MD1a)은 도 5에 도시된 바와 같은 제1 시각적 특징 추출 모듈(MD1 a)을 포함할 수 있다. 프로세서(120, 도 1 참조)는 제1 시각적 특징 추출 모듈(MD1a)을 이용해 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징(Va1 내지 Van)을 획득할 수 있다. 이하, 다른 설명이 없는 한 프로세서는 도 1 상의 프로세서를 의미한다. 일 실시예에서, 제1 시각적 특징 추출 모듈(MD1a)은 AlexNet(Krizhevsky et al., ImageNet Classification with Deep Convolutional Neural Networks, NIPS 2012), VGGNet(Simonyan et.al, Very Deep Convolutional Networks for Large-Scale Image Recognition, ICLR 2015), GoogleNet(Szegedy et al., Going Deeper with Convolutions, CVPR 2015), ResNet(He et al., Deep Residual Learning for Image Recognition, CVPR 2016), 및 ViT(Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers or Image Recognition at Scale, ICLR 2021) 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 동영상은 광고 동영상일 수 있다. 일 실시예에서, 동영상은 서로 다른 복수의 시점에 각각 대응 하는 복수의 프레임(frame)을 포함할 수 있고, 각각의 프레임은 정지 영상일 수 있다. 일 실시예에서, 각각의 제1 시각적 특징(Va1 내지 Van)은 벡터, 행렬, 또는 텐서일 수 있다. 일 실시예에서, 복 수의 제1 시각적 특징(Va1 내지 Van)은 서로 다른 복수의 시점 또는 시구간에 각각 대응할 수 있다. 일 실시예에 서, 각각의 제1 시각적 특징(Va1 내지 Van)은 각각의 프레임(시점)에 대응할 수 있다. 다른 실시예에서, 각각의 제1 시각적 특징(Va1 내지 Van)은 각각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 또 다른 실시예에서, 복수의 제1 시각적 특징(Va1 내지 Van) 중 적어도 하나 각각은 각각의 프레임(시점)에 대응할 수 있고, 복수의 제1 시각적 특징(Va1 내지 Van) 중 나머지 각각은 각각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 멀티-모달 시간 축 융합 인공지능 모델(MD)은 텍스트로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하 는 복수의 텍스트 특징(L1 내지 Ln)을 획득하기 위한 텍스트 특징 추출 모듈(MD2)을 더 포함할 수 있다. 일 실시 예에서, 텍스트 특징 추출 모듈(MD2)은 도 3에 도시된 바와 같은 텍스트 특징 추출 모듈(MD2)을 포함할 수 있다. 프로세서는 텍스트 특징 추출 모듈(MD2)을 이용해 텍스트로부터 서로 다른 복수의 시점 또는 시구간 에 각각 대응하는 복수의 텍스트 특징(L1 내지 Ln)을 획득할 수 있다. 일 실시예에서, 텍스트는 동영상의 자막, 동영상의 댓글, 동영상의 제목, 동영상에 대한 설명, 및 동영상 분류 기준 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 각각의 텍스트 특징(L1 내지 Ln)은 벡터, 행렬, 또는 텐서일 수 있다. 일 실시예에서, 각각의 텍 스트 특징(L1 내지 Ln)의 차원은 각각의 제1 시각적 특징(Va1 내지 Van)의 차원과 동일할 수 있다. 즉, 각각의 텍 스트 특징(L1 내지 Ln)을 구성하는 실수의 개수는 각각의 제1 시각적 특징(Va1 내지 Van)을 구성하는 실수의 개수 와 동일할 수 있다. 일 실시예에서, 복수의 텍스트 특징(L1 내지 Ln)은 서로 다른 복수의 시점 또는 시구간에 각각 대응할 수 있다. 일 실시예에서, 각각의 텍스트 특징(L1 내지 Ln)은 각각의 프레임(시점)에 대응할 수 있다. 다른 실시예에서, 각각의 텍스트 특징(L1 내지 Ln)은 각각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 또 다른 실시예에서, 복수의 텍스트 특징(L1 내지 Ln) 중 적어도 하나 각각은 각각의 프레임(시점)에 대응할 수 있고, 복수의 텍스트 특징(L1 내지 Ln) 중 나머지 각각은 각각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 멀티-모달 시간 축 융합 인공지능 모델(MD)은 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스 트 특징을 융합함으로써 복수의 제1 시각적 특징(Va1 내지 Van)과 복수의 텍스트 특징(L1 내지 Ln)으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적 융합 특징(Ta1 내지 Tan)을 획득하기 위한 제1 지역적 융합 모듈(MD3)을 더 포함할 수 있다. 일 실시예에서, 제1 지역적 융합 모듈(MD3)은 도 4에 도시된 바와 같은 제1 지역적 융합 모듈(MD3)을 포함할 수 있다. 프로세서는 제1 지역적 융합 모듈(MD3)을 이용해 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제1 시각적 특징(Va1 내지 Van)과 복수의 텍스트 특징(L1 내지 Ln)으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수 의 제1 지역적 융합 특징(Ta1 내지 Tan)을 획득할 수 있다. 예를 들어, 프로세서는 제1 시점에 대응하는 제 1 시각적 특징(Va1)과 제1 시점에 대응하는 텍스트 특징(L1)을 융합해 제1 시점에 대응하는 제1 지역적 융합 특 징(Ta1)을 획득할 수 있다. 일 실시예에서, 각각의 제1 지역적 융합 특징(Ta1 내지 Tan)은 벡터, 행렬, 또는 텐서일 수 있다. 일 실시예에서, 복수의 지역적 융합 특징(Ta1 내지 Tan)은 서로 다른 복수의 시점 또는 시구간에 각각 대응할 수 있 다. 일 실시예에서, 각각의 제1 지역적 융합 특징(Ta1 내지 Tan)은 각각의 프레임(시점)에 대응할 수 있다. 다른 실시예에서, 각각의 제1 지역적 융합 특징(Ta1 내지 Tan)은 각각의 복수의 연속된 프레임(시구간)에 대응할 수 있 다. 또 다른 실시예에서, 복수의 제1 지역적 융합 특징(Ta1 내지 Tan) 중 적어도 하나 각각은 각각의 프레임(시 점)에 대응할 수 있고, 복수의 제1 지역적 융합 특징(Ta1 내지 Tan) 중 나머지 각각은 각각의 복수의 연속된 프 레임(시구간)에 대응할 수 있다. 멀티-모달 시간 축 융합 인공지능 모델(MD)은 복수의 제1 지역적 융합 특징(Ta1 내지 Tan)으로부터 적어도 하나 의 전역적 융합 특징, 예를 들어, 복수의 전역적 융합 특징(G1 내지 Gn)을 획득하기 위한 전역적 융합 모듈(MD 4)을 더 포함할 수 있다. 프로세서는 전역적 융합 모듈(MD4)을 이용해 복수의 제1 지역적 융합 특징(Ta1 내 지 Tan)으로부터 적어도 하나의 전역적 융합 특징, 예를 들어, 복수의 전역적 융합 특징(G1 내지 Gn)을 획득할 수 있다. 일 실시예에서, 전역적 융합 모듈(MD4)은 셀프-어텐션 층을 포함할 수 있다. 프로세서는 셀프-어텐션을 이 용해 복수의 제1 지역적 융합 특징(Ta1 내지 Tan)으로부터 적어도 하나의 전역적 융합 특징, 예를 들어, 복수의 전역적 융합 특징(G1 내지 Gn)을 획득할 수 있다. 셀프-어텐션은 좁은 의미의 셀프-어텐션(Vaswani et al., Attention is All you Need, NIPS 2017) 및 변형된 셀프-어텐션 중 적어도 하나일 수 있다. 좁은 의미의 셀프- 어텐션은 멀티-헤드 셀프-어텐션을 포함할 수 있다. 예를 들어, 전역적 융합 모듈(MD4)은 좁은 의미의 트랜스포 머(Vaswani et al., Attention is All you Need, NIPS 2017 또는 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021) 및 변형된 트랜스포머 중 적어도 하나를 포함할 수 있다. 예를 들어, 변형된 트랜스포머는 Swin transformer(Liu et al., Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, ICCV 2021), Reformer(Kitaev et al., Reformer: The efficient transformer, ICLR 2020), Linformer(Wang et al., Linformer: Self-attention with linear complexity, arXiv 2020), Performer(Choromanski et al., Rethinking attention with performers, ICLR 2020), 및 Longformer(Beltagy et al., Longformer: The long-document transformer, arXiv 2020) 중 적어도 하나를 포 함할 수 있다. 일 실시예에서, 각각의 전역적 융합 특징(G1 내지 Gn)은 벡터, 행렬, 또는 텐서일 수 있다. 일 실시예에서, 복수 의 전역적 융합 특징(G1 내지 Gn)은 서로 다른 복수의 시점 또는 시구간에 각각 대응할 수 있다. 일 실시예에서, 각각의 전역적 융합 특징(G1 내지 Gn)은 각각의 프레임(시점)에 대응할 수 있다. 다른 실시예에서, 각각의 전역 적 융합 특징(G1 내지 Gn)은 각각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 또 다른 실시예에서, 복수의 전역적 융합 특징(G1 내지 Gn) 중 적어도 하나 각각은 각각의 프레임(시점)에 대응할 수 있고, 복수의 전역적 융합 특징(G1 내지 Gn) 중 나머지 각각은 각각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 도 3은 본 발명의 일 실시예에 따른 본 전자 장치에 사용되는 텍스트 특징 추출 모듈의 개념도이다. 도 3을 참 조하면, 텍스트 특징 추출 모듈(MD2)은 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 맵핑 층(W1 내지 Wn)을 포함할 수 있다. 프로세서는 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 맵핑 층(W1 내지 Wn)을 이용해 텍스트로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징 (L1 내지 Ln)을 획득할 수 있다. 일 실시예에서, 복수의 맵핑 층은 서로 다른 복수의 행렬을 포함할 수 있다. 프 로세서는 각 시점 또는 시구간 마다 다른 맵핑 층(W1 내지 Wn)을 이용함으로써 각 시점 또는 시구간 마다 다른 텍스트 특징(L1 내지 Ln)을 획득할 수 있다. 일 실시예에서, 텍스트 특징 추출 모듈(MD2)은 텍스트로부터 복수의 제1 중간 텍스트 특징(L'1 내지 L'm)을 획득 하기 위한 언어 모델(MD2a)을 포함할 수 있다. 프로세서는 언어 모델(MD2a)을 이용해 텍스트로부터 복수의 제1 중간 텍스트 특징(L'1 내지 L'm)을 획득할 수 있다. 일 실시예에서, 복수의 제1 중간 텍스트 특징(L'1 내지 L'm)은 각각 텍스트의 토큰에 대응할 수 있다. 일 실시예에서, 각각의 제1 중간 텍스트 특징(L'1 내지 L'm)은 벡 터, 행렬, 또는 텐서일 수 있다. 일 실시예에서, 언어 모델(MD2a)은 순환 신경망(Recurrent Neural Network, RNN) 기반 언어 모델 및 트랜스포 머 기반 언어 모델 중 적어도 하나를 포함할 수 있다. 순환 신경망 기반 언어 모델은 좁은 의미의 RNN, LSTM(Long Short Term Memory), GRU(Gated Recurrent Unit), 및 Bi-RNN(Bidirectional-RNN) 중 적어도 하나를 포함할 수 있다. 트랜스포머 기반 언어 모델은 예를 들어, BERT(Devlin et al., BERT: Pre-training of Deep Bidirectional Transformer for Language Understanding, ACL 2019), 및 RoBERTa(Liu et al., RoBERTa: A Robustly Optimized BERT Pretraining Approach, ICLR 2020) 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 텍스트 특징 추출 모듈(MD2)은 복수의 제1 중간 텍스트 특징(L'1 내지 L'm)으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 중간 텍스트 특징(L''1 내지 L''n)을 획득하기 위한 복수 의 맵핑 층(W1 내지 Wn)을 포함할 수 있다. 각각의 맵핑 층(W1 내지 Wn)은 각 시점 또는 시구간에 대응할 수 있 다. 프로세서는 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 맵핑 층(W1 내지 Wn)을 이용해 복수의 제1 중간 텍스트 특징(L'1 내지 L'm)으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수 의 제2 중간 텍스트 특징(L''1 내지 L''n)을 획득할 수 있다. 예를 들어, 프로세서는 제1 시점에 대응하는 맵핑 층(W1)을 이용해 텍스트의 복수의 토큰에 각각 대응하는 복수의 제1 중간 텍스트 특징(L'1 내지 L'm)으로부 터 제1 시점에 대응하는 제2 중간 텍스트 특징(L''1)을 획득할 수 있다. 일 실시예에서, 제1 중간 텍스트 특징 (L'1 내지 L'm)의 수(m)는 제2 중간 텍스트 특징(L''1 내지 L''n)의 수(n)와 상이할 수 있다. 일 실시예에서, 각각의 제2 중간 텍스트 특징(L''1 내지 L''n)은 벡터, 행렬, 또는 텐서일 수 있다. 일 실시예에 서, 복수의 제2 중간 텍스트 특징(L''1 내지 L''n)은 서로 다른 복수의 시점 또는 시구간에 각각 대응할 수 있다. 일 실시예에서, 각각의 제2 중간 텍스트 특징(L''1 내지 L''n)은 각각의 프레임(시점)에 대응할 수 있다. 다른 실시예에서, 각각의 제2 중간 텍스트 특징(L''1 내지 L''n)은 각각의 복수의 연속된 프레임(시구간)에 대응 할 수 있다. 또 다른 실시예에서, 복수의 제2 중간 텍스트 특징(L''1 내지 L''n) 중 적어도 하나 각각은 각각의 프레임(시점)에 대응할 수 있고, 복수의 제2 중간 텍스트 특징(L''1 내지 L''n) 중 나머지 각각은 각각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 일 실시예에서, 텍스트 특징 추출 모듈(MD2)은 제2 중간 텍스트 특징(L''1 내지 L''n)의 차원을 변경함으로써 복 수의 제2 중간 텍스트 특징(L''1 내지 L''n)으로부터 복수의 텍스트 특징(L1 내지 Ln)을 획득하기 위한 차원 변 경 모듈(MD2b)을 포함할 수 있다. 프로세서는 차원 변경 모듈(MD2b)을 이용해 제2 중간 텍스트 특징(L''1 내지 L''n)의 차원을 변경함으로써 복수의 제2 중간 텍스트 특징(L''1 내지 L''n)으로부터 복수의 텍스트 특징(L1 내지 Ln)을 획득할 수 있다. 예를 들어, 프로세서는 차원 변경 모듈(MD2b)을 이용해 제1 시점에 대응하 는 제2 중간 텍스트 특징(L''1)으로부터 제1 시점에 대응하는 텍스트 특징(L1)을 획득할 수 있다. 각각의 제2 중 간 텍스트 특징(L''1 내지 L''n)을 구성하는 실수의 개수와 각각의 텍스트 특징(L1 내지 Ln)을 구성하는 실수의 개수는 상이할 수 있다. 일 실시예에서, 각각의 텍스트 특징(L1 내지 Ln)의 차원은 각각의 제1 시각적 특징(Va1 내지 Van, 도 2 참조)의 차원과 동일할 수 있다. 즉, 각각의 텍스트 특징(L1 내지 Ln)을 구성하는 실수의 개수와 각각의 제1 시각적 특징(Va1 내지 Van, 도 2 참조)을 구성하는 실수의 개수는 동일할 수 있다. 도 4는 본 발명의 일 실시예에 따른 본 전자 장치에 사용되는 제1 지역적 융합 모듈의 개념도이다. 도 4를 참조 하면, 제1 지역적 융합 모듈(MD3)은 컨벌루션 네트워크(MD3a)를 포함할 수 있다. 일 실시예에서, 컨벌루션 네트 워크(MD3a)는 풀링 층, 1x1 컨벌루션 층, 및 활성(activation) 층을 포함할 수 있다. 예를 들어, 풀링 층은 채 널(channel-wise) 평균(average) 층, 채널 최대(max) 층, 및 채널 최소(min) 층 중 적어도 하나를 포함할 수 있다. 예를 들어, 활성 층은 시그모이드 층, ReLU 층, 또는 tanh 층을 포함할 수 있다. 프로세서는 컨벌루 션을 이용해 제1 시각적 특징(Va)과 텍스트 특징(L)으로부터 제1 지역적 융합 특징(Ta)을 획득할 수 있다. 일 실시예에서, 프로세서는 제1 시각적 특징(Va)과 텍스트 특징(L)으로부터 제1 중간 지역적 융합 특징 (T'a)을 획득할 수 있다. 일 실시예에서, 제1 중간 지역적 융합 특징(T'a)은 벡터, 행렬, 또는 텐서일 수 있다. 예를 들어, 프로세서는 제1 시각적 특징(Va)과 텍스트 특징(L)의 합을 획득함으로써 제1 중간 지역적 융합 특징(T'a)을 획득할 수 있다. 일 실시예에서, 프로세서는 컨벌루션 네트워크(MD3a)를 이용해 제1 중간 지역적 융합 특징(T'a)으로부터 제2 중간 지역적 융합 특징(T''a)을 획득할 수 있다. 일 실시예에서, 제2 중간 지역적 융합 특징(T''a)은 벡터, 행렬, 또는 텐서일 수 있다. 일 실시예에서, 프로세서는 제1 시각적 특징(Va)과 제2 중간 지역적 융합 특징(T''a)으로부터 제1 지역적 융합 특징(Ta)을 획득할 수 있다. 예를 들어, 프로세서는 제1 시각적 특징(Va)과 제2 중간 지역적 융합 특 징(T''a)의 내적(dot product)을 획득함으로써 제1 지역적 융합 특징(Ta)을 획득할 수 있다. 도 5는 본 발명의 일 실시예에 따른 본 전자 장치에 사용되는 제1 시각적 특징 추출 모듈의 개념도이다. 도 5를 참조하면, 제1 시각적 특징 추출 모듈(MD1a)은 동영상으로부터 복수의 객체 특징(O1 내지 On)을 획득하기 위한 객체 특징 추출 모듈(MD1a-1)을 포함할 수 있다. 프로세서는 객체 특징 추출 모듈(MD1a-1)을 이용해 동영 상으로부터 복수의 객체 특징(O1 내지 On)을 획득할 수 있다. 예를 들어, 객체 특징 추출 모듈(MD1a-1)은 Faster-RCNN(Ren et al., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, NIPS 2015) 및 YOLO(Redmon et al., You Only Look Once: Unified, Real-Time Object Detection, CVPR 2016) 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 복수의 객체 특징(O1 내지 On)은 벡터, 행렬, 또는 텐서일 수 있다. 일 실시예에서, 복수의 객체 특징(O1 내지 On)은 서로 다른 복수의 시점 또는 시구간에 각각 대응할 수 있다. 일 실시예에서, 각각의 객체 특 징(O1 내지 On)은 각각의 프레임(시점)에 대응할 수 있다. 다른 실시예에서, 각각의 객체 특징(O1 내지 On)은 각 각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 또 다른 실시예에서, 복수의 객체 특징(O1 내지 On) 중 적어도 하나 각각은 각각의 프레임(시점)에 대응할 수 있고, 복수의 객체 특징(O1 내지 On) 중 나머지 각각은 각 각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 일 실시예에서, 제1 시각적 특징 추출 모듈(MD1a)은 복수의 객체 특징(O1 내지 On)으로부터 복수의 객체 그래프 (E1 내지 En)를 획득하기 위한 객체 그래프 추출 모듈(MD1a-2)을 포함할 수 있다. 예를 들어, 객체 그래프 추출 모듈(MD1a-2)은 도 6에 도시된 객체 그래프 추출 모듈(MD1a-2)을 포함할 수 있다. 프로세서는 객체 그래프 추출 모듈(MD1a-2)을 이용해 복수의 객체 특징(O1 내지 On)으로부터 복수의 객체 그래프(E1 내지 En)를 획득할 수 있다. 일 실시예에서, 프로세서는 객체 그래프 추출 모듈(MD1a-2)을 이용해 각 시점 또는 시구간에 대 응하는 객체 특징(O1 내지 On)으로부터 각 시점 또는 시구간에 대응하는 복수의 객체 그래프(E1 내지 En)를 획득할 수 있다. 예를 들어, 프로세서는 객체 그래프 추출 모듈(MD1a-2)을 이용해 제1 시점에 대응하는 객체 특징(O1)으로부터 제1 시점에 대응하는 객체 그래프(E1)를 획득할 수 있다. 일 실시예에서, 복수의 객체 그래프(E1 내지 En)는 서로 다른 복수의 시점 또는 시구간에 각각 대응할 수 있다. 일 실시예에서, 각각의 객체 그래프(E1 내지 En)는 각각의 프레임(시점)에 대응할 수 있다. 다른 실시예에서, 각 각의 객체 그래프(E1 내지 En)는 각각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 또 다른 실시예에서, 복수의 객체 그래프(E1 내지 En) 중 적어도 하나 각각은 각각의 프레임(시점)에 대응할 수 있고, 복수의 객체 그 래프(E1 내지 En) 중 나머지 각각은 각각의 복수의 연속된 프레임(시구간)에 대응할 수 있다. 일 실시예에서, 제1 시각적 특징 추출 모듈(MD1a)은 복수의 객체 그래프(E1 내지 En)로부터 복수의 제1 시각적 특징(Va1 내지 Van)을 획득하기 위한 객체 그래프 특징 추출 모듈(MD1a-3)을 포함할 수 있다. 프로세서는 객체 그래프 특징 추출 모듈(MD1a-3)을 이용해 복수의 객체 그래프(E1 내지 En)로부터 복수의 제1 시각적 특징 (Va1 내지 Van)을 획득할 수 있다. 예를 들어, 객체 그래프 특징 추출 모듈(MD1a-2)은 그래프 컨벌루션 층을 포 함할 수 있다. 일 실시예에서, 프로세서는 객체 그래프 특징 추출 모듈(MD1a-3)을 이용해 각 시점 또는 시 구간에 대응하는 객체 그래프(E1 내지 En)로부터 각 시점 또는 시구간에 대응하는 복수의 제1 시각적 특징(Va1 내 지 Van)을 획득할 수 있다. 예를 들어, 프로세서는 객체 그래프 특징 추출 모듈(MD1a-3)을 이용해 제1 시점 에 대응하는 객체 그래프(E1)로부터 제1 시점에 대응하는 제1 시각적 특징(Va1)을 획득할 수 있다. 도 6은 본 발명의 일 실시예에 따른 본 전자 장치에 사용되는 객체 그래프 추출 모듈의 개념도이다. 도 6을 참 조하면, 객체 그래프 추출 모듈(MD1a-2)은 복수의 객체 특징(O1 내지 On)으로부터 복수의 객체 그래프 노드 집합 (Z1 내지 Zn)을 획득하기 위한 노드 추출 모듈(MD1a-2N) 및 복수의 객체 그래프 노드 집합(Z1 내지 Zn)으로부터 각각의 객체 그래프 노드 집합(Z1 내지 Zn) 내의 노드들 사이의 관계를 획득함으로써 복수의 객체 그래프(E1 내 지 En)를 획득하기 위한 관계 추출 모듈(MD1a-2R)을 포함할 수 있다. 프로세서는 노드 추출 모듈(MD1a- 2N)을 이용해 복수의 객체 특징(O1 내지 On)으로부터 복수의 객체 그래프 노드 집합(Z1 내지 Zn)을 획득할 수 있 고, 관계 추출 모듈(MD1a-2R)을 이용해 복수의 객체 그래프 노드 집합(Z1 내지 Zn)으로부터 각각의 객체 그래프 노드 집합(Z1 내지 Zn) 내의 노드들 사이의 관계를 획득함으로써 복수의 객체 그래프(E1 내지 En)를 획득할 수 있다. 일 실시예에서, 노드 추출 모듈(MD1a-2N)은 트랜스포머 기반 모델, 및 컨벌루션 네트워크 기반 모델 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 관계 추출 모듈(MD1a-2R)은 트랜스포머 기반 모델, 및 컨벌루션 네트워 크 기반 모델 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 프로세서는 노드 추출 모듈(MD1a-2N)을 이용해 각 시점 또는 시구간에 대응하는 객체 특징 (O1 내지 On)으로부터 각 시점 또는 시구간에 대응하는 객체 그래프 노드 집합(Z1 내지 Zn)을 획득할 수 있다. 예를 들어, 프로세서는 노드 추출 모듈(MD1a-2N)을 이용해 제1 시점에 대응하는 객체 특징(O1)으로부터 제 1 시점에 대응하는 객체 그래프 노드 집합(Z1)을 획득할 수 있다. 일 실시예에서, 프로세서는 관계 추출 모듈(MD1a-2R)을 이용해 각 시점 또는 시구간에 대응하는 객체 그래 프 노드 집합(Z1 내지 Zn)으로부터 각 시점 또는 시구간에 대응하는 객체 그래프 노드 집합(Z1 내지 Zn) 내의 노 드들 사이의 관계를 획득함으로써 각 시점 또는 시구간에 대응하는 객체 그래프(E1 내지 En)를 획득할 수 있다. 예를 들어, 프로세서는 관계 추출 모듈(MD1a-2R)을 이용해 제1 시점에 대응하는 객체 그래프 노드 집합 (Z1)으로부터 제1 시점에 대응하는 객체 그래프 노드 집합(Z1) 내의 노드들 사이의 관계를 획득함으로써 제1 시 점에 대응하는 객체 그래프(E1)를 획득할 수 있다. 도 7은 본 발명의 일 실시예에 따른 멀티-모달 시간 축 융합 인공지능 모델의 개념도이다. 이하, 도 2에 도시된 멀티-모달 시간 축 융합 인공지능 모델(MD)과 도 7에 도시된 멀티-모달 시간 축 융합 인공지능 모델(MD') 사이 의 차이점을 중심으로 도 7에 도시된 멀티-모달 시간 축 융합 인공지능 모델(MD')이 설명된다.도 7을 참조하면, 멀티-모달 시간 축 융합 인공지능 모델(MD')은 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 시각적 특징(Vb1 내지 Vbn)을 획득하기 위한 제2 시각적 특징 추출 모듈 (MD1b) 및 서로 같은 시점 또는 시구간에 대응하는 제2 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제2 시각적 특징(Vb1 내지 Vbn)과 복수의 텍스트 특징(L1 내지 Ln)으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 지역적 융합 특징(Tb1 내지 Tbn)을 획득하기 위한 제2 지역적 융합 모듈(MD6)을 더 포함할 수 있다. 프로세서는 제2 시각적 특징 추출 모듈(MD1b)을 이용해 동영상으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 시각적 특징(Vb1 내지 Vbn)을 획득하고, 제2 지역적 융합 모듈(MD6)을 이용해 서로 같은 시 점 또는 시구간에 대응하는 제2 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제2 시각적 특징(Vb1 내지 Vbn)과 복수의 텍스트 특징(L1 내지 Ln)으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 지역적 융 합 특징(Tb1 내지 Tbn)을 획득할 수 있다. 복수의 제1 시각적 특징(Va1 내지 Van, 도 2 참조)과 복수의 제2 시각적 특징(Vb1 내지 Vbn)은 상이할 수 있다. 예 를 들어, 복수의 제1 시각적 특징(Va1 내지 Van)은 도 5를 참조하여 설명된 바와 같이 객체 그래프에 기초한 시 각적 특징일 수 있고, 복수의 제2 시각적 특징(Vb1 내지 Vbn)은 외관(appearance)에 기초한 시각적 특징일 수 있 다. 예를 들어, 제1 시각적 특징 추출 모듈(MD1a)은 Faster-RCNN을 포함할 수 있고, 제2 시각적 특징 추출 모듈 (MD1b)은 ResNet을 포함할 수 있다. 일 실시예에서, 제2 지역적 융합 특징 모듈(MD6)은 제1 지역적 융합 모듈 (MD3)과 동일한 구조를 가질 수 있으나 상이한 파라미터를 가질 수 있다. 다른 실시예에서, 제2 지역적 융합 특 징 모듈(MD6)은 제1 지역적 융합 모듈(MD3)과 상이한 구조를 가질 수 있다. 프로세서는 복수의 제1 지역적 융합 특징(Ta1 내지 Tan) 및 복수의 제2 지역적 융합 특징(Tb1 내지 Tbn)으로 부터 복수의 제3 지역적 융합 특징(Tc1 내지 Tcn)을 획득할 수 있다. 각각의 제3 지역적 융합 특징(Tc1 내지 Tcn) 은 각 시점 또는 시구간에 대응할 수 있다. 예를 들어, 프로세서는 서로 같은 시점 또는 시구간에 대응하 는 제1 지역적 융합 특징과 제2 지역적 융합 특징을 합치거나(concatenate), 더하거나, 곱함으로써 서로 같은 시점 또는 시구간에 대응하는 제3 지역적 융합 특징을 획득할 수 있다. 예를 들어, 프로세서는 제1 시점에 대응하는 제1 지역적 융합 특징(Ta1)과 제2 지역적 융합 특징(Tb1)을 합치거나(concatenate), 더하거나, 곱함으 로써 제1 시점에 대응하는 제3 지역적 융합 특징(Tc1)을 획득할 수 있다. 전역적 융합 모듈(MD4')은 복수의 제3 지역적 융합 특징(Tc1 내지 Tcn)으로부터 적어도 하나의 전역적 융합 특징, 예를 들어, 복수의 전역적 융합 특징(G1 내지 Gn)을 획득할 수 있다. 프로세서는 전역적 융합 모듈 (MD4')을 이용해 복수의 제3 지역적 융합 특징(Tc1 내지 Tcn)으로부터 적어도 하나의 전역적 융합 특징, 예를 들 어, 복수의 전역적 융합 특징(G1 내지 Gn)을 획득할 수 있다. 도 2를 참조하여 설명한 바와 같이, 전역적 융합 모듈(MD4')은 셀프-어텐션 층을 포함할 수 있다. 도 8은 본 발명의 일 실시예에 따른 멀티-모달 시간 축 융합 인공지능 모델을 설명하기 위한 개념도이다. 도 8 을 참조하면, 멀티-모달 시간 축 융합 인공지능 모델(MD, MD')은 적어도 하나의 전역적 융합 특징, 예를 들어, 복수의 전역적 융합 특징(G1 내지 Gn)을 이용해 질문에 대한 답을 획득할 수 있다. 텍스트는 질문을 포함할 수 있다. 멀티-모달 시간 축 융합 인공지능 모델(MD, MD')은 적어도 하나의 전역적 융합 특징, 예를 들어, 복수의 전역적"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "융합 특징(G1 내지 Gn)으로부터 요약 벡터(G)를 획득하는 요약 벡터 추출 모듈(MD7)을 포함할 수 있다. 프로세서"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "는 요약 벡터 추출 모듈(MD7)을 이용해 적어도 하나의 전역적 융합 특징, 예를 들어, 복수의 전역적 융합"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "특징(G1 내지 Gn)으로부터 요약 벡터(G)를 획득할 수 있다. 멀티-모달 시간 축 융합 인공지능 모델(MD, MD')은 복수의 선택지로부터 각각 복수의 선택지 특징을 획득하는 선택지 특징 추출 모듈(MD8)을 포함할 수 있다. 프로세서는 선택지 특징 추출 모듈(MD8)을 이용해 복수의 선택지로부터 각각 복수의 선택지 특징을 획득할 수 있다. 일 실시예에서, 선택지 특징 추출 모듈(MD8)은 BERT, 및 RoBERTa 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 각각의 선택지 특징은 벡터, 행렬, 또는 텐서일"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "수 있다.프로세서는 요약 벡터(G)와 각각의 선택지 특징으로부터 텍스트로 표현된 질문에 대한 답을 획득할 수 있"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "다. 예를 들어, 프로세서는 요약 벡터(G)와 각각의 선택지 특징 사이의 유사도를 획득하고, 요약 벡터(G)"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "와 가장 유사도가 높은 선택지를 답으로서 선택할 수 있다. 예를 들어, 프로세서는 요약 벡터(G)와 각각의"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "선택지 특징 사이의 유사도를 획득하기 위해 요약 벡터(G)와 각각의 선택지 특징 사이의 코사인 유사도 또는 요 약 벡터(G)와 각각의 선택지 특징 사이의 거리를 획득할 수 있다. 도 9는 본 발명의 일 실시예에 따른 멀티-모달 시간 축 융합 인공지능 모델을 설명하기 위한 개념도이다. 도 9 를 참조하면, 멀티-모달 시간 축 융합 인공지능 모델(MD, 및 MD')은 적어도 하나의 전역적 융합 특징, 예를 들 어, 복수의 전역적 융합 특징(G1 내지 Gn)을 이용해 동영상을 분류하는 분류 모듈(MD9)을 더 포함할 수 있다. 프 로세서는 적어도 하나의 전역적 융합 특징, 예를 들어, 복수의 전역적 융합 특징(G1 내지 Gn)과 분류 모듈 (MD9)을 이용해 동영상을 분류할 수 있다. 일 실시예에서, 프로세서는 동영상을 종교별 또는 정치 성향별 로 분류할 수 있다. 일 실시예에서, 프로세서는 동영상을 성인물 해당 여부, 혐오물 해당 여부, 특정 이슈 관련 여부, 특정 브랜드 관련 여부 등에 따라 분류할 수 있다. 도 10은 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 10을 참조하면, 본 전자 장치 의 동작 방법은, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징 을 획득하는 단계(S1010), 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득 하는 단계(S1020), 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복 수의 제1 시각적 특징과 복수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지 역적 융합 특징을 획득하는 단계(S1030), 및 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단계(S1040)를 포함할 수 있다. 도 11은 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 11을 참조하면, 복수의 텍스 트 특징을 획득하는 단계(S1020, 도 10 참조)는, 복수의 시점 또는 시구간에 각각 대응하는 복수의 맵핑 층을 이용해 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계(S1120)를 포함할 수 있다. 구체적으로, 본 전자 장치의 동작 방법은, 동영상으로부터 서로 다른 복수의 시점 또는 시구간 에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계(S1110), 복수의 시점 또는 시구간에 각각 대응하는 복수의 맵핑 층을 이용해 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하 는 단계(S1120), 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수 의 제1 시각적 특징과 복수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역 적 융합 특징을 획득하는 단계(S1130), 및 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특 징을 획득하는 단계(S1140)를 포함할 수 있다. 도 12는 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 12를 참조하면, 복수의 텍스 트 특징을 획득하는 단계(S1020, 도 10 참조)는, 텍스트로부터 복수의 제1 중간 텍스트 특징을 획득하는 단계 (S1221), 복수의 시점 또는 시구간에 각각 대응하는 복수의 맵핑 층을 이용해 복수의 제1 중간 텍스트 특징으로 부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 중간 텍스트 특징을 획득하는 단계(S1222), 및 제2 중간 텍스트 특징의 차원을 변경함으로써 복수의 제2 중간 텍스트 특징으로부터 복수의 텍스트 특징을 획득하는 단계(S1223)를 포함할 수 있다. 일 실시예에서, 복수의 제1 중간 텍스트 특징의 개수는 복수의 제2 중간 텍스트 특징의 개수와 상이할 수 있다. 일 실시예에서, 복수의 텍스트 특징 각각의 차원은 복수의 제1 시각적 특징 각 각의 차원과 동일할 수 있다. 구체적으로, 본 전자 장치의 동작 방법은, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계(S1210), 텍스트로부터 복수의 제1 중간 텍스트 특징을 획득하는 단계 (S1221), 복수의 시점 또는 시구간에 각각 대응하는 복수의 맵핑 층을 이용해 복수의 제1 중간 텍스트 특징으로 부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 중간 텍스트 특징을 획득하는 단계(S1222), 제2 중 간 텍스트 특징의 차원을 변경함으로써 복수의 제2 중간 텍스트 특징으로부터 복수의 텍스트 특징을 획득하는 단계(S1223), 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제1 시각적 특징과 복수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적 융합 특징을 획득하는 단계(S1230), 및 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징 을 획득하는 단계(S1240)를 포함할 수 있다. 도 13은 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 13을 참조하면, 복수의 제1 지역적 융합 특징을 획득하는 단계(S1030, 도 10 참조)는 컨벌루션을 이용해 제1 시각적 특징과 텍스트 특징으로부터 제1 지역적 융합 특징을 획득하는 단계(S1330)를 포함할 수 있다. 구체적으로, 본 전자 장치의 동작 방 법은, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계(S1310), 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계 (S1320), 컨벌루션을 이용해 제1 시각적 특징과 텍스트 특징으로부터 제1 지역적 융합 특징을 획득하는 단계 (S1330), 및 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단계(S1340)를 포함할 수 있다. 도 14는 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 14를 참조하면, 복수의 제1 지역적 융합 특징을 획득하는 단계(S1030, 도 10 참조)는, 제1 시각적 특징과 텍스트 특징으로부터 제1 중간 지 역적 융합 특징을 획득하는 단계(S1431), 컨벌루션을 이용해 제1 중간 지역적 융합 특징으로부터 제2 중간 지역 적 융합 특징을 획득하는 단계(S1432), 및 제1 시각적 특징과 제2 중간 지역적 융합 특징으로부터 제1 지역적 융합 특징을 획득하는 단계(S1433)를 포함할 수 있다. 구체적으로, 본 전자 장치의 동작 방법은, 동영상으로부 터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계(S1410), 텍스 트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계(S1420), 제1 시각적 특징과 텍스트 특징으로부터 제1 중간 지역적 융합 특징을 획득하는 단계(S1431), 컨벌루션을 이용해 제1 중간 지역적 융합 특징으로부터 제2 중간 지역적 융합 특징을 획득하는 단계(S1432), 제1 시각적 특징과 제2 중간 지 역적 융합 특징으로부터 제1 지역적 융합 특징을 획득하는 단계(S1433), 및 복수의 제1 지역적 융합 특징으로부 터 적어도 하나의 전역적 융합 특징을 획득하는 단계(S1440)를 포함할 수 있다. 도 15는 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 15를 참조하면, 복수의 제1 시각적 특징을 획득하는 단계(S1010, 도 10 참조)는, 동영상으로부터 복수의 객체 특징을 획득하는 단계 (S1511), 복수의 객체 특징으로부터 복수의 객체 그래프를 획득하는 단계(S1512), 및 복수의 객체 그래프로부터 복수의 제1 시각적 특징을 획득하는 단계(S1513)를 포함할 수 있다. 구체적으로, 본 전자 장치의 동작 방법은, 동영상으로부터 복수의 객체 특징을 획득하는 단계(S1511), 복수의 객체 특징으로부터 복수의 객체 그래프를 획 득하는 단계(S1512), 복수의 객체 그래프로부터 복수의 제1 시각적 특징을 획득하는 단계(S1513), 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계(S1520), 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제1 시각적 특징과 복수의 텍스트 특 징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적 융합 특징을 획득하는 단계(S1530), 및 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단계(S1540)를 포함할 수 있다. 도 16은 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 16을 참조하면, 본 전자 장치 의 동작 방법은, 동영상으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 시각적 특징을 획득하는 단계(S1610B), 서로 같은 시점 또는 시구간에 대응하는 제2 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제2 시각적 특징과 복수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 지역적 융합 특징을 획득하는 단계(S1630B), 및 복수의 제1 지역적 융합 특징 및 복수의 제2 지역적 융합 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제3 지역적 융합 특징을 획득하는 단계(S1650)를 더 포함하고, 적어도 하나의 전역적 융합 특징을 획득하는 단계(S1040, 도 10 참조)는 복수의 제3 지역적 융합 특 징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단계(S1640)를 포함할 수 있다. 구체적으로, 본 전자 장치의 동작 방법은, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계(S1610A), 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복 수의 텍스트 특징을 획득하는 단계(S1620), 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제1 시각적 특징과 복수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적 융합 특징을 획득하는 단계(S1630A), 동영상으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 시각적 특징을 획득하는 단계(S1610B), 서로 같은 시점 또는 시구간에 대응하는 제2 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제2 시각적 특징과 복수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제2 지역적 융합 특징을 획득하는 단계(S1630B), 복수의 제1 지역적 융합 특징 및 복수의 제2 지역적 융합 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제3 지역적 융 합 특징을 획득하는 단계(S1650), 및 복수의 제3 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단계(S1640)를 포함할 수 있다. 도 17은 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 17을 참조하면, 적어도 하나 의 전역적 융합 특징을 획득하는 단계(S1040, 도 10 참조)는 셀프-어텐션을 이용해 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단계(S1740)를 포함할 수 있다. 구체적으로, 본 전자 장치의 동작 방법은, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계(S1710), 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계(S1720), 서로 같은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제1 시각적 특징과 복수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적 융합 특징을 획득하는 단계(S1730), 및 셀프-어텐션을 이용해 복수의 제1 지역적 융합 특징으로부터 적 어도 하나의 전역적 융합 특징을 획득하는 단계(S1740)를 포함할 수 있다. 도 18은 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 18 상의 텍스트는 질문을 포 함할 수 있다. 도 18을 참조하면, 본 전자 장치의 동작 방법은 적어도 하나의 전역적 융합 특징을 이용해 질문 에 대한 답을 획득하는 단계(S1850)를 더 포함할 수 있다. 구체적으로, 본 전자 장치의 동작 방법은, 동영상으 로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계(S1810), 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계(S1820), 서로 같 은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제1 시각적 특징과 복 수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적 융합 특징을 획득하는 단계(S1830), 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단계(S1840), 및 적어도 하나의 전역적 융합 특징을 이용해 질문에 대한 답을 획득하는 단계(S1850)를 포함할 수 있다. 도 19는 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 19 상의 적어도 하나의 전역 적 융합 특징은 복수의 전역적 융합 특징을 포함할 수 있다. 도 19를 참조하면, 질문에 대한 답을 획득하는 단"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "계(S1850, 도 18 참조)는 적어도 하나의 전역적 융합 특징으로부터 요약 벡터를 획득하는 단계(S1951), 복수의"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "선택지로부터 각각 복수의 선택지 특징을 획득하는 단계(S1952), 및 요약 벡터와 각각의 선택지 특징으로부터 질문에 대한 답을 획득하는 단계(S1953)를 포함할 수 있다. 구체적으로, 본 전자 장치의 동작 방법은, 동영상으 로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계(S1910), 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계(S1920), 서로 같 은 시점 또는 시구간에 대응하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제1 시각적 특징과 복 수의 텍스트 특징으로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적 융합 특징을 획득하는 단계(S1930), 복수의 제1 지역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단계(S1940),"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "적어도 하나의 전역적 융합 특징으로부터 요약 벡터를 획득하는 단계(S1951), 복수의 선택지로부터 각각 복수의"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "선택지 특징을 획득하는 단계(S1952), 및 요약 벡터와 각각의 선택지 특징으로부터 질문에 대한 답을 획득하는 단계(S1953)를 포함할 수 있다. 도 20은 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다. 도 20을 참조하면, 본 전자 장치 의 동작 방법은 적어도 하나의 전역적 융합 특징을 이용해 동영상을 분류, 검색, 예측, 또는 생성하는 단계 (S2050)를 더 포함할 수 있다. 구체적으로, 본 전자 장치의 동작 방법은, 동영상으로부터 서로 다른 복수의 시 점 또는 시구간에 각각 대응하는 복수의 제1 시각적 특징을 획득하는 단계(S2010), 텍스트로부터 복수의 시점 또는 시구간에 각각 대응하는 복수의 텍스트 특징을 획득하는 단계(S2020), 서로 같은 시점 또는 시구간에 대응 하는 제1 시각적 특징과 텍스트 특징을 융합함으로써 복수의 제1 시각적 특징과 복수의 텍스트 특징으로부터 복 수의 시점 또는 시구간에 각각 대응하는 복수의 제1 지역적 융합 특징을 획득하는 단계(S2030), 복수의 제1 지 역적 융합 특징으로부터 적어도 하나의 전역적 융합 특징을 획득하는 단계(S2040), 및 적어도 하나의 전역적 융 합 특징을 이용해 동영상을 분류, 검색, 예측, 또는 생성하는 단계(S2050)를 포함할 수 있다. [실시예들과 비교예들] 제1 실시예: 도 7에 도시된 바와 같이 객체 그래프 기반 제1 시각적 특징과 외관 기반 제2 시각적 특징을 사용. 제2 실시예: 도 2에 도시된 바와 같이 제1 시각적 특징으로서 객체 그래프 기반 시각적 특징을 사용하고, 외관 기반 시각적 특징을 사용하지 않음. 제3 실시예: 도 2에 도시된 바와 같이 제1 시각적 특징으로서 외관 기반 시각적 특징을 사용하고 객체 그래프 기반 시각적 특징을 사용하지 않음. 제4 실시예: 객체 그래프 기반 제1 시각적 특징과 외관 기반 제2 시각적 특징과 함께 추가적으로 움직임 (motion) 기반 제3 시각적 특징을 더 사용. 제5 실시예: VGT(Xiao et al., Video graph transformer for video question answering, ECCV 2022)의 방법을 이용해 획득한 제1 시각적 특징을 사용하고, 제2 시각적 특징을 사용하지 않음. 지역적 융합과 전역적 융합을 모두 수행. 제1 비교예: HME(Fan et al., Heterogeneous memory enhanced multimodal attention model for video question answering, CVPR 2019) 제2 비교예: CoMem(Gao et al., Motion-appearance co-memory networks for video question answering, CVPR 2018) 제3 비교예: HCRN(Le et al., Hierarchical conditional relation networks for video question answering, CVPR 2020) 제4 비교예: HGA(Jiang et al., Reasoning with heterogeneous graph alignment for video question answering, AAAI 2020) 제5 비교예: B2A(Park et al., Bridge to answer: Structure-aware graph interaction network for video question answering, CVPR 2021) 제6 비교예: IGV(Li et al., Invariant grounding for video question answering, CVPR 2022) 제7 비교예: MHN(Peng et al., Multilevel hierarchical network with multi-scale sampling for video question answering, arXiv 2022) 제8 비교예: VGT 제9 비교예: HQGA(Xiao et al., Video as conditional graph hierarchy for multi-granular question answering, AAAI 2022) 제10 비교예: ML(가장 출현 빈도 수가 높은 답을 선택) 제11 비교예: PSAC(Li et al., Beyond rnns: Positional self-attention with co-attention for video question answering, AAAI 2019) 제12 비교예: VGT를 이용해 획득한 시각적 특징 벡터 및 어텐션 모듈을 이용해 전역적 융합을 수행. 제13 비교예: VGT를 이용해 획득한 시각적 특징 벡터 및 어텐션 모듈을 이용해 지역적 융합과 전역적 융합을 모 두 수행. 제14 비교예: VGT의 방법을 이용해 획득한 제1 시각적 특징을 사용하고, 제2 시각적 특징을 사용하지 않음. 전 역적 융합을 수행. 제15 비교예: 외관 기반 제2 시각적 특징을 사용하고 객체 그래프 기반 제1 시각적 특징을 사용하지 않음. 복수 의 텍스트 특징(L1 내지 Ln)이 서로 동일. [평가 데이터셋] Causal-VidQA: Wang et al., Causal attention for unbiased visual recognition, ICCV 2021 NExT-QA: Xiao et al., Next-qa: Next phase of question-answering to explaining temporal actions, CVPR 2021 MSVD-QA: Xu et al., Video question answering via gradually refined attention over appearance and motion, ACM international conference on Multimedia 2017 AGQA-2.0: Grunde-McLaughlin et al., Agqa 2.0: An updated benchmark for compositional spatio-temporal reasoning, arXiv 2022 [선행 기술과의 비교] 아래 표 1을 참조하면, Causal-VidQA 데이터셋에 대해 제1 실시예가 제1 내지 제5 비교예보다 높은 성능을 보였 다. 아래 표 1에서, ACCD는 묘사적(descriptive) 문제 유형에 대한 답 정확도, ACCE는 설명적(explanatory) 문 제 유형에 대한 답 정확도, ACCP는 예측적(predictive) 문제 유형에 대한 답 정확도, ACCC는 반사실적(counterfactual) 문제 유형에 대한 답 정확도, ACCA는 데이터셋 전반에 대한 정확도를 나타낸다. 특히 제1 실 시예는 예측적 문제 유형 및 반사실적 문제 유형에 대해 높은 성능 향상을 보였다. 표 1"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "아래 표 2를 참조하면, Test-C는 NexT-QA 데이터셋의 인과적(causal) 질문 유형에 대한 답 정확도, NexT-QA 데 이터셋의 Test-T는 시간적(temporal) 질문 유형에 대한 답 정확도, NexT-QA 데이터셋의 Test-D는 묘사적 질문 유형에 대한 답 정확도, Test-A는 NexT-QA 데이터셋의 전반에 대한 정확도를 나타낸다. 제1 실시예의 전반적 정 확도가 제1 비교예 및 제6 내지 제9 비교예의 전반적 정확도보다 높았다. 또한, MSVD-QA 데이터셋에 대해 제1 실시예는 제1 비교예 및 제6 내지 제9 비교예 보다 높은 성능을 보였다. 표 2"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "아래 표 3을 참조하면, AGQA v2.0 데이터셋의 문제 유형별 제10 비교예, 제11 비교예, 제1 비교예, 제3 비교예, 및 제1 실시예의 정확도 성능이 평가되었다 (표 3에서 obj: object, rel: relationship, act: action, comp: comparison, recog: recognition). 제1 실시예의 모든 문제에 대한 성능이 제10 비교예, 제11 비교예, 제1 비 교예, 및 제3 비교예의 모든 문제에 대한 성능보다 높았다. 특히 제1 실시예는 제10 비교예, 제11 비교예, 제1 비교예, 및 제3 비교예보다 action recognition 질문 유형에 대해 상당한 성능 향상을 보였다. 표 3"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "[멀티-레벨 융합의 효과] 아래 표 4를 참조하면, 지역적 융합 및 전역적 융합을 포함하는 멀티-레벨 융합의 효과를 확인하기 위해 제12 내지 제14 비교예와 제5 실시예의 성능을 비교하였다. 아래 표 4에서, ACCD는 묘사적 문제 유형에 대한 답 정확 도, ACCE는 설명적 문제 유형에 대한 답 정확도, ACCP는 예측적 문제 유형에 대한 답 정확도, ACCC는 반사실적 문제 유형에 대한 답 정확도, ACCA는 데이터셋에 대한 전반적 정확도를 나타낸다. 제12 비교예와 제13 비교예는 동일한 시각적 특징 벡터 및 동일한 어텐션 모듈을 사용하나, 제12 비교예는 전역적 융합만을 수행하고, 제13 비교예는 지역적 융합과 전역적 융합을 모두 수행한다. 한편 제14 비교예 및 제5 실시예는 동일한 시각적 특징 벡터 및 동일한 인공지능 모델(MD, 도 2 참조)을 사용하나, 제14 비교예는 전역적 융합만을 수행하고, 제5 실시 예는 지역적 융합과 전역적 융합을 모두 수행한다. 제13 비교예의 전반적 성능이 제12 비교예보다 높고, 제5 실 시예의 전반적 성능이 제14 비교예보다 높은 점에 비추어, 전역적 융합만을 수행하는 것 대비 지역적 융합과 전 역적 융합을 결합한 멀티-레벨 융합이 성능 향상에 기여함을 알 수 있다. 특히 멀티-레벨 융합이 예측적 질문 유형에 대한 성능(ACCP) 향상에 중요하게 작용할 수 있음을 알 수 있다. 표 4"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "[시각적 특징의 영향] 아래 표 5를 참조하면, 제2 실시예는 객체 그래프 기반 시각적 특징을 사용하고, 제3 실시예는 외관 기반 시각 적 특징을 사용하고, 제1 실시예는 객체 그래프 기반 제1 시각적 특징과 외관 기반 제2 시각적 특징을 사용하고, 제4 실시예는 객체 그래프 기반 제1 시각적 특징, 외관 기반 제2 시각적 특징, 및 움직임 기반 제3 시각적 특징을 사용한다. Test-C는 NexT-QA 데이터셋의 인과적 질문 유형에 대한 답 정확도, NexT-QA 데이터셋 의 Test-T는 시간적 질문 유형에 대한 답 정확도, NexT-QA 데이터셋의 Test-D는 묘사적 질문 유형에 대한 답 정 확도, Test-A는 NexT-QA 데이터셋의 전반에 대한 정확도를 나타낸다. 표 5로부터 다양한 종류의 시각적 특징에 대해 제1 내지 제4 실시예들이 만족스러운 성능을 나타냄을 확인할 수 있었다. 따라서 본 발명이 시각적 특징 추출 모듈(MD1a 및 MD1b) 또는 시각적 특징의 종류에 의해 제한되지 않음을 알 수 있다. 표 5"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "[텍스트 특징의 시간 축 확장] 아래 표 6을 참조하면, 제15 비교예는 하나의 텍스트 특징을 복수개 복사하는 방식을 사용하여 모든 시점 또는 시구간에 동일한 텍스트 특징을 사용하는 반면, 제3 실시예는 복수의 시점 또는 시구간에 각각 대응하는 서로 다른 복수의 텍스트 특징을 사용한다. 아래 표 6에서, ACCD는 묘사적 문제 유형에 대한 답 정확도, ACCE는 설명 적 문제 유형에 대한 답 정확도, ACCP는 예측적 문제 유형에 대한 답 정확도, ACCC는 반사실적 문제 유형에 대한 답 정확도, ACCA는 Causal-VidQA 데이터셋 전반에 대한 정확도를 나타낸다. 전반적 성능(ACCA) 측면에서 제3 실시예는 제15 비교예보다 뛰어난 성능을 보였다. 따라서 각각의 시간마다 서 로 다른 텍스트 특징을 추출하는 본 발명의 텍스트 특징의 시간 축 확장 방법이 성능 향상에 기여함을 알 수 있 다. 특히 제3 실시예는 제15 비교예보다 반사실적 문제 유형에 대해 높은 성능(ACCC) 향상을 보였다. 표 6"}
{"patent_id": "10-2024-0141348", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "[정답과의 시간적 유사도] 본 발명의 실시예들이 시간적 문맥의 이해에 효과적인지 살펴보기 위해 정답 벡터와 각 시점 또는 시구간에 대 응하는 전역적 융합 벡터 사이의 유사도를 계산했다. 예를 들어,\"which object were they behind before standing up but after lying on a bed?\"라는 질문에 대해, 정답 벡터는 제1 실시예의 \"just before standing\"에 대응하는 시점에 대응하는 전역적 융합 특징에 가장 높은 유사도를 보였다. 즉, 제1 실시예는 정답과 관련 있는 전역적 융합 특징을 배울 수 있음을 알 수 있다. 또한, \"While putting on a shoe, which object did the person close?\" 라는 질문에 대해, 정답 벡터는 제1 실시예의 사람이 신발과 상호 작용하고 노트북을 닫는 시점에 대응하는 전역적 융합 특징에 가장 높은 유사도를 보였다. 즉, 제1 실시예는 정답과 관련 있는 전역적 융합 특징을 배울 수 있음을 알 수 있다. 또한, \"Was the person tidying up the object they were behind first before or after tidying something on the thing they stood on?\"이라는 질문에 대해, 정답 벡터는 제1 실시예의\"tidying the floor\"에 대응하는 시점과 \"tidying the object they were behind\"에 대응하는 시점 사이에서 사람이 행동을 변경하는 시점에 대 응하는 전역적 융합 특징에 가장 높은 유사도를 보였다. 즉, 제1 실시예는 정답과 관련 있는 전역적 융합 특징 을 배울 수 있음을 알 수 있다. 이상으로, 본 발명의 실시예들이 도시되고 설명되었지만, 당업자는 첨부된 청구항들 및 그에 동등한 것들에 의 해 정의되는 바와 같은 본 실시예의 사상 및 범위를 벗어나지 않고 형태 및 세부 사항들에 있어 다양한 변경이 이루어질 수 있음을 이해할 것이다."}
{"patent_id": "10-2024-0141348", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 멀티-모달 시간 축 융합 인공지능 모델을 위한 전자 장치의 블록도이다. 도 2는 본 발명의 일 실시예에 따른 멀티-모달 시간 축 융합 인공지능 모델의 개념도이다. 도 3은 본 발명의 일 실시예에 따른 본 전자 장치에 사용되는 텍스트 특징 추출 모듈의 개념도이다. 도 4는 본 발명의 일 실시예에 따른 본 전자 장치에 사용되는 제1 지역적 융합 모듈의 개념도이다. 도 5는 본 발명의 일 실시예에 따른 본 전자 장치에 사용되는 제1 시각적 특징 추출 모듈의 개념도이다. 도 6은 본 발명의 일 실시예에 따른 본 전자 장치에 사용되는 객체 그래프 추출 모듈의 개념도이다. 도 7은 본 발명의 일 실시예에 따른 멀티-모달 시간 축 융합 인공지능 모델의 개념도이다. 도 8 및 9는 본 발명의 일 실시예에 따른 멀티-모달 시간 축 융합 인공지능 모델을 설명하기 위한 개념도이다. 도 10 내지 20은 본 발명의 일 실시예에 따른 본 전자 장치의 동작 방법의 흐름도이다."}
