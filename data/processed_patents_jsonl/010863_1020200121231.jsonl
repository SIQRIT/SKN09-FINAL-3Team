{"patent_id": "10-2020-0121231", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0143093", "출원번호": "10-2020-0121231", "발명의 명칭": "전자 장치 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "오지훈"}}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,복수의 레이어로 구성된 인공 지능 모델이 저장된 메모리; 및프로세서;를 포함하며,상기 인공 지능 모델은, 상기 복수의 레이어 각각에 포함된 복수의 채널 별로 상이한 시프트 스케일링 팩터에기초하여 스케일링되고 상기 복수의 레이어 별로 양자화된 복수의 가중치 값을 포함하며,상기 프로세서는,입력 데이터가 수신되면, 상기 입력 데이터에 대한 신경망 연산 과정에서 각 채널 별 연산 결과를 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링된 합성 스케일 파라미터와 연산하는, 전자 장치."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 인공 지능 모델은,상기 양자화된 복수의 가중치 값, 상기 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케일링 팩터 및상기 복수의 레이어 각각에 대응되는 스케일 파라미터와 제로 포인트 파라미터를 포함하며,상기 스케일 파라미터는,상기 복수의 레이어 각각을 양자화하는 경우, 양자화 전의 값과 양자화 후 값 간의 기울기를 나타내고,상기 제로 포인트 파라미터는,상기 복수의 레이어 각각을 양자화하는 경우, 양자화 전 제로 값의 양자화 후의 값을 나타내는, 전자 장치."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 프로세서는,현재 레이어의 스케일 파라미터, 상기 현재 레이어 직전 레이어의 스케일 파라미터 및 상기 복수의 가중치 값의스케일 파라미터에 기초하여 획득된 값을, 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링하여 상기 합성 스케일 파라미터를 획득하는, 전자 장치."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 프로세서는,상기 획득된 값을 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 시프트함으로써 상기 합성 스케일파라미터를 획득하는, 전자 장치."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서,상기 획득된 값은,상기 현재 레이어의 스케일 파라미터에 반비례하고, 상기 현재 레이어 직전 레이어의 스케일 파라미터 및 상기공개특허 10-2021-0143093-3-복수의 가중치 값의 스케일 파라미터에 비례하는, 전자 장치."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제2항에 있어서,상기 프로세서는,상기 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케일링 팩터가 상기 인공 지능 모델에 포함되었다고 식별되면, 상기 역스케일링을 수행하는, 전자 장치."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 전자 장치는,신경망 처리 장치(Neural Processing Unit, NPU)로 구현된, 전자 장치."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 각 채널에 대응되는 시프트 스케일링 팩터는,상기 각 채널에 포함된 가중치 값 및 상기 각 채널을 포함하는 레이어에 포함된 가중치 값에 기초하여결정되는, 전자 장치."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 각 채널에 대응되는 시프트 스케일링 팩터는,상기 각 채널에서 가장 크기가 큰 가중치 값 및 상기 각 채널을 포함하는 레이어에서 가장 크기가 큰 가중치 값에 기초하여 결정되는, 전자 장치."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "전자 장치의 제어 방법에 있어서,입력 데이터를 수신하는 단계; 및인공 지능 모델을 이용하여 상기 입력 데이터에 대한 신경망 연산 과정에서 상기 인공 지능 모델을 구성하는 복수의 레이어 각각의 채널 별 연산 결과를 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링된 합성 스케일 파라미터와 연산하는 단계;를 포함하며,상기 인공 지능 모델은, 상기 복수의 레이어 각각에 포함된 복수의 채널 별로 상이한 시프트 스케일링 팩터에기초하여 스케일링되고 상기 복수의 레이어 별로 양자화된 복수의 가중치 값을 포함하는, 제어 방법."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 인공 지능 모델은,상기 양자화된 복수의 가중치 값, 상기 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케일링 팩터 및상기 복수의 레이어 각각에 대응되는 스케일 파라미터와 제로 포인트 파라미터를 포함하며,상기 스케일 파라미터는,상기 복수의 레이어 각각을 양자화하는 경우, 양자화 전의 값과 양자화 후 값 간의 기울기를 나타내고,상기 제로 포인트 파라미터는,상기 복수의 레이어 각각을 양자화하는 경우, 양자화 전 제로 값의 양자화 후의 값을 나타내는, 제어 방법.공개특허 10-2021-0143093-4-청구항 12 제11항에 있어서,상기 연산하는 단계는,현재 레이어의 스케일 파라미터, 상기 현재 레이어 직전 레이어의 스케일 파라미터 및 상기 복수의 가중치 값의스케일 파라미터에 기초하여 획득된 값을, 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링하여 상기 합성 스케일 파라미터를 획득하는, 제어 방법."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 연산하는 단계는,상기 획득된 값을 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 시프트함으로써 상기 합성 스케일파라미터를 획득하는, 제어 방법."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서,상기 획득된 값은,상기 현재 레이어의 스케일 파라미터에 반비례하고, 상기 현재 레이어 직전 레이어의 스케일 파라미터 및 상기복수의 가중치 값의 스케일 파라미터에 비례하는, 제어 방법."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 연산하는 단계는,상기 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케일링 팩터가 상기 인공 지능 모델에 포함되었다고 식별되면, 상기 연산을 수행하는, 제어 방법."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제10항에 있어서,상기 전자 장치는,신경망 처리 장치(Neural Processing Unit, NPU)로 구현된, 제어 방법."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제10항에 있어서,상기 각 채널에 대응되는 시프트 스케일링 팩터는,상기 각 채널에 포함된 가중치 값 및 상기 각 채널을 포함하는 레이어에 포함된 가중치 값에 기초하여결정되는, 제어 방법."}
{"patent_id": "10-2020-0121231", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 각 채널에 대응되는 시프트 스케일링 팩터는,상기 각 채널에서 가장 크기가 큰 가중치 값 및 상기 각 채널을 포함하는 레이어에서 가장 크기가 큰 가중치 값에 기초하여 결정되는, 제어 방법."}
{"patent_id": "10-2020-0121231", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 본 전자 장치는 복수의 레이어로 구성된 인공 지능 모델이 저장된 메모리 및 프로세서를 포함하며, 인공 지능 모델은 복수의 레이어 각각에 포함된 복수의 채널 별로 상이한 시프트 스케일링 팩터에 기 초하여 스케일링되고 복수의 레이어 별로 양자화된 복수의 가중치 값을 포함하며, 프로세서는 입력 데이터가 수 신되면, 입력 데이터에 대한 신경망 연산 과정에서 각 채널 별 연산 결과를 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링된 합성 스케일 파라미터와 연산할 수 있다."}
{"patent_id": "10-2020-0121231", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 그 제어 방법에 대한 것으로, 더욱 상세하게는 딥러닝 등의 기계 학습 알고리즘을 활용 하여 인간 두뇌의 인지, 판단 등의 기능을 모사하는 인공 지능(Artificial Intelligence, AI) 모델을 처리하는 전자 장치 및 그 제어 방법에 대한 것이다."}
{"patent_id": "10-2020-0121231", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 딥러닝 모델의 성능 저하를 최소화하면서 압축률을 높이기 위해 양자화(Quantization)가 이용되고 있다. 가중치 양자화(Weight Quantization) 방법에는 양자화를 하는 시점을 기준으로 학습 후 양자화(Post-training Quantization) 및 학습 중 양자화(Quantization-aware Training)로 나눌 수 있고, 양자화 방식을 기준으로 선 형 양자화(Linear quantization) 및 비선형 양자화(Non-linear quantization)로 나눌 수 있다. 학습 후 양자화는 이미 학습된 Float32 모델을 이용하여 재학습 없이 IntN Quantization을 수행하기 때문에 양 자화 속도가 빠르고 학습 데이터가 요구되지 않는다. 선형 양자화는 하드웨어에서 INT MULTIPLYER와 INT ADDER 로 쉽게 구현할 수 있어 Neural Processing Units(NPU)에서 보편적으로 지원된다. 이러한 장점에도 불구하고, 일반적으로 학습 후 양자화와 선형 양자화의 조합은 학습 중 양자화와 비선형 양자 화의 조합 대비 양자화 후에 작은 양자 비트(8비트 이하) 조건으로 갈수록 정확도 손실이 크다는 단점이 존재한 다. 이는 하나의 레이어에 포함된 복수의 채널이 서로 파라미터 분포가 상이하며, 특히 작은 범위의 파라미터 분포를 가지는 채널은 양자화 후에 하나의 양자값으로 수렴하여 양자 에러가 커지기 때문이다. 이러한 단점을 극복하기 위해 다양한 방법이 개발되고 있다. 먼저, 채널 별 양자화(Channel-wise quantization)는 기존 방식과 같이 신경망 레이어 단위로 양자화를 진행하 여 레이어 별로 파라미터의 최소값, 최대값 한 쌍을 구하고, 이로부터 하드웨어 Fixed computing에 필요한 양자 파라미터(QWeight, Scale, Zero point)를 계산하는 것과는 달리, 레이어에 포함된 채널 단위로 양자화를 진행하 여 채널 별로 파라미터의 최소값, 최대값 한 쌍을 구한다. 예를 들어, n채널인 경우 n개의 [min, max]가 획득될 수 있다. 이러한 동작을 통해 양자화 정밀도 손실은 작아지게 되나, 양자 파라미터 사이즈가 채널 개수에 비례해서 증가 하는 문제가 있다. 이는 메인 메모리에서 캐시 메모리로 양자 파라미터를 로딩하는 시간을 증가시켜 Latency 성 능이 저하된다. 레이어간 파라미터 동등화(Cross Layer Equalization, CLE)는 앞 컨볼루션 레이어의 출력과 뒤 컨볼루션 레이어 의 대응되는 입력에 (float) scaling, (float) rescaling를 적용하는 전처리를 수행한다. 신경망 첫 레이어부터 마지막 레이어까지 연속적으로 스케일을 조정하며, 이를 의 변화가 없게되는 때까지 전체를 다시 반복 수행하며, 전처리가 종료되면 일반적인 Layer-wise quantization를 수행한다. 이러한 전처리를 통해 채널 별 파라미터의 범위가 겹치도록 조정되어 양자화 정밀도 손실은 작아지나, 연속된 신경망 레이어 간에 scaling equivariance를 유지해야 하는 속성 때문에 레이어 사이의 ReLU6나 PReLU처럼 piecewise linear인 경우에는 문제가 발생할 수 있다. 그에 따라, ReLU6나 PReLU를 ReLU로 교체해주어야 하며, 그에 따라 activated feature map distribution이 달라지고, 오차가 커지고 정확도가 하락하는 문제가 발생한다. 그에 따라, 신경망 레이어의 구조적 변화를 요구하지 않으면서 하드웨어 연산에 유리하고, 양자화의 정확도를 유지하면서도 양자화 효율을 높이는 방법이 개발될 필요가 있다."}
{"patent_id": "10-2020-0121231", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 필요성에 따른 것으로, 본 개시의 목적은 채널 별로 시프트 스케일링되고 레이어 별로 양자화 된 인공 지능 모델을 이용하여 신경망 연산을 수행하는 전자 장치 및 그 제어 방법을 제공함에 있다."}
{"patent_id": "10-2020-0121231", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "이상과 같은 목적을 달성하기 위한 본 개시의 일 실시 예에 따른 전자 장치는 복수의 레이어로 구성된 인공 지 능 모델이 저장된 메모리 및 프로세서를 포함하며, 상기 인공 지능 모델은 상기 복수의 레이어 각각에 포함된 복수의 채널 별로 상이한 시프트 스케일링 팩터에 기초하여 스케일링되고 상기 복수의 레이어 별로 양자화된 복 수의 가중치 값을 포함하며, 상기 프로세서는 입력 데이터가 수신되면, 상기 입력 데이터에 대한 신경망 연산 과정에서 각 채널 별 연산 결과를 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링된 합 성 스케일 파라미터와 연산할 수 있다. 또한, 상기 인공 지능 모델은 상기 양자화된 복수의 가중치 값, 상기 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케일링 팩터 및 상기 복수의 레이어 각각에 대응되는 스케일 파라미터와 제로 포인트 파라미터를 포함하며, 상기 스케일 파라미터는 상기 복수의 레이어 각각을 양자화하는 경우, 양자화 전의 값과 양자화 후 값 간의 기울기를 나타내고, 상기 제로 포인트 파라미터는 상기 복수의 레이어 각각을 양자화하는 경우, 양자화 전 제로 값의 양자화 후의 값을 나타낼 수 있다. 그리고, 상기 프로세서는 현재 레이어의 스케일 파라미터, 상기 현재 레이어 직전 레이어의 스케일 파라미터 및 상기 복수의 가중치 값의 스케일 파라미터에 기초하여 획득된 값을, 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링하여 상기 합성 스케일 파라미터를 획득할 수 있다. 그리고, 상기 프로세서는 상기 획득된 값을 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 시프트 함으로써 상기 합성 스케일 파라미터를 획득할 수 있다. 여기서, 상기 획득된 값은 상기 현재 레이어의 스케일 파라미터에 반비례하고, 상기 현재 레이어 직전 레이어의 스케일 파라미터 및 상기 복수의 가중치 값의 스케일 파라미터에 비례할 수 있다. 또한, 상기 프로세서는 상기 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케일링 팩터가 상기 인공 지능 모델에 포함되었다고 식별되면, 상기 역스케일링을 수행할 수 있다. 또한, 상기 전자 장치는 신경망 처리 장치(Neural Processing Unit, NPU)로 구현될 수 있다. 그리고, 상기 각 채널에 대응되는 시프트 스케일링 팩터는 상기 각 채널에 포함된 가중치 값 및 상기 각 채널을 포함하는 레이어에 포함된 가중치 값에 기초하여 결정될 수 있다. 또한, 상기 각 채널에 대응되는 시프트 스케일링 팩터는 상기 각 채널에서 가장 크기가 큰 가중치 값 및 상기 각 채널을 포함하는 레이어에서 가장 크기가 큰 가중치 값에 기초하여 결정될 수 있다. 한편, 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법은 입력 데이터를 수신하는 단계 및 인공 지능 모델 을 이용하여 상기 입력 데이터에 대한 신경망 연산 과정에서 상기 인공 지능 모델을 구성하는 복수의 레이어 각 각의 채널 별 연산 결과를 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링된 합성 스케 일 파라미터와 연산하는 단계를 포함하며, 상기 인공 지능 모델은 상기 복수의 레이어 각각에 포함된 복수의 채 널 별로 상이한 시프트 스케일링 팩터에 기초하여 스케일링되고 상기 복수의 레이어 별로 양자화된 복수의 가중 치 값을 포함할 수 있다. 또한, 상기 인공 지능 모델은 상기 양자화된 복수의 가중치 값, 상기 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케일링 팩터 및 상기 복수의 레이어 각각에 대응되는 스케일 파라미터와 제로 포인트 파라미터를 포함하며, 상기 스케일 파라미터는 상기 복수의 레이어 각각을 양자화하는 경우, 양자화 전의 값과 양자화 후 값 간의 기울기를 나타내고, 상기 제로 포인트 파라미터는 상기 복수의 레이어 각각을 양자화하는 경우, 양자화 전 제로 값의 양자화 후의 값을 나타낼 수 있다. 그리고, 상기 연산하는 단계는 현재 레이어의 스케일 파라미터, 상기 현재 레이어 직전 레이어의 스케일 파라미 터 및 상기 복수의 가중치 값의 스케일 파라미터에 기초하여 획득된 값을, 상기 각 채널에 대응되는 시프트 스 케일링 팩터에 기초하여 역스케일링하여 상기 합성 스케일 파라미터를 획득할 수 있다. 그리고, 상기 연산하는 단계는 상기 획득된 값을 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 시 프트함으로써 상기 합성 스케일 파라미터를 획득할 수 있다. 여기서, 상기 획득된 값은 상기 현재 레이어의 스케일 파라미터에 반비례하고, 상기 현재 레이어 직전 레이어의 스케일 파라미터 및 상기 복수의 가중치 값의 스케일 파라미터에 비례할 수 있다. 또한, 상기 연산하는 단계는 상기 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케일링 팩터가 상기 인공 지능 모델에 포함되었다고 식별되면, 상기 연산을 수행할 수 있다. 또한, 상기 전자 장치는 신경망 처리 장치(Neural Processing Unit, NPU)로 구현될 수 있다. 그리고, 상기 각 채널에 대응되는 시프트 스케일링 팩터는 상기 각 채널에 포함된 가중치 값 및 상기 각 채널을 포함하는 레이어에 포함된 가중치 값에 기초하여 결정될 수 있다. 또한, 상기 각 채널에 대응되는 시프트 스케일링 팩터는 상기 각 채널에서 가장 크기가 큰 가중치 값 및 상기 각 채널을 포함하는 레이어에서 가장 크기가 큰 가중치 값에 기초하여 결정될 수 있다."}
{"patent_id": "10-2020-0121231", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상과 같은 본 개시의 다양한 실시 예에 따르면, 전자 장치는 입력 데이터에 대한 신경망 연산 과정에서 각 채 널 별 연산 결과를 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링된 합성 스케일 파라미터와 연산함으로써 상대적으로 작은 용량으로 구현된 인공 지능 모델을 이용하면서도 신경망 연산을 정확도를 향상시 킬 수 있다."}
{"patent_id": "10-2020-0121231", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부 도면을 참조하여 본 개시를 상세히 설명한다. 본 개시의 실시 예에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 개시의 설명 부 분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수 치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. A 또는/및 B 중 적어도 하나라는 표현은 \"A\" 또는 \"B\" 또는 \"A 및 B\" 중 어느 하나를 나타내는 것으로 이해되어 야 한다. 본 명세서에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들 을 한정하지 않는다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또 는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 명세서에서, 사용자라는 용어는 전자 장치를 사용하는 사람 또는 전자 장치를 사용하는 장치(예: 인공 지능 전자 장치)를 지칭할 수 있다. 이하 첨부된 도면들을 참조하여 본 개시의 다양한 실시 예를 보다 상세하게 설명한다. 도 1a는 본 개시의 일 실시 예에 따른 전자 장치의 구성을 나타내는 블록도이다. 전자 장치는 도 1a 에 도시된 바와 같이, 메모리 및 프로세서를 포함한다. 전자 장치는 인공 지능 모델에 기초하여 신경망 연산을 수행하는 장치일 수 있다. 예를 들어, 전자 장치 는 인공 지능 모델을 저장하고, 입력 데이터가 수신되면, 인공 지능 모델에 기초하여 입력 데이터에 대한 신경망 연산을 수행하는 장치로서, 데스크탑 PC, 노트북, TV 등으로 구현될 수 있다. 다만, 이에 한정되는 것은 아니며, 전자 장치는 인공 지능 모델에 기초하여 신경망 연산을 수행할 수 있는 장치라면 어떠한 장치라도 무방하다. 특히, 전자 장치는 스마트폰, 태블릿 PC, 웨어러블 기기 등과 같이 리소스가 한정된 장치로서, 양자화된 인공 지능 모델을 저장하고, 양자화된 인공 지능 모델에 기초하여 신경망 연산을 수행하는 장치일 수 있다. 양 자화란 연속적인 값을 복수의 레벨로 구분하고, 각 레벨 내의 값을 각 레벨을 대표하는 값으로 치환하는 것을 의미한다. 예를 들어, 0에서 1 사이의 값을 1로 치환하고, 1에서 2 사이의 값을 2로 치환하는 양자화를 통해 데이터 크기 를 줄일 수 있다. 즉, 인공 지능 모델을 양자화하여 리소스가 한정된 전자 장치에서도 온 디바이스(on- device) 형태로 신경망 연산이 수행될 수 있다. 인공 지능 모델의 양자화에 대한 구체적인 설명을 후술한다. 메모리는 프로세서 등이 접근할 수 있도록 데이터 등의 정보를 전기 또는 자기 형태로 저장하는 하드 웨어를 지칭할 수 있다. 이를 위해, 메모리는 비휘발성 메모리, 휘발성 메모리, 플래시 메모리(Flash Memory), 하드디스크 드라이브(HDD) 또는 솔리드 스테이트 드라이브(SSD), RAM, ROM 등 중에서 적어도 하나의 하드웨어로 구현될 수 있다. 메모리에는 전자 장치 또는 프로세서의 동작에 필요한 적어도 하나의 인스트럭션(instruction) 또는 모듈이 저장될 수 있다. 여기서, 인스트럭션은 전자 장치 또는 프로세서의 동작을 지시하는 부 호 단위로서, 컴퓨터가 이해할 수 있는 언어인 기계어로 작성된 것일 수 있다. 모듈은 작업 단위의 특정 작업을 수행하는 일련의 인스트럭션의 집합체(instruction set)일 수 있다. 메모리에는 문자, 수, 영상 등을 나타낼 수 있는 비트 또는 바이트 단위의 정보인 데이터가 저장될 수 있 다. 예를 들어, 메모리에는 복수의 문장을 포함하는 문서와 같은 데이터가 저장될 수 있다. 메모리에는 복수의 레이어로 구성된 인공 지능 모델이 저장될 수 있다. 여기서, 인공 지능 모델은 복수의 레이어 각각에 포함된 복수의 채널 별로 상이한 시프트 스케일링 팩터에 기초하여 스케일링되고 복수의 레이어 별로 양자화된 복수의 가중치 값을 포함할 수 있다. 예를 들어, 스케일링 및 양자화 전의 인공 지능 모델이 5개의 레이어로 구성되고, 5개의 레이어 각각은 32개의 채널을 포함한다고 가정하면, 스케일링 및 양자화 전의 인공 지능 모델은 총 160개의 채널을 포함할 수 있다. 먼저, 160개의 채널 각각은 상이한 시프트 스케일링 팩터에 기초하여 스케일링될 수 있다. 도 1c의 상단은 인공 지능 모델에 포함된 5개의 레이어 중 하나에 포함된 복수의 채널을 도시하였다. 도 1c의 하단은 복수의 채널 각 각이 상이한 시프트 스케일링 팩터를 통해 스케일링된 도면이다. 이러한 방식으로 시프트 스케일링 팩터는 채널 별로 상이할 수 있으며, 그에 따라 전체 시프트 스케일링 팩터는 총 160개일 수 있다. 여기서, 채널 별 시프트 스케일링 팩터는 하드웨어에서 shift 연산으로 수행될 수 있도록 Power-of-Two 형태로 결정될 수 있다. 가령, 제1 채널은 shift by 3에 기초하여 스케일링되고, 제2 채널은 shift by 5에 기초하여 스케일링될 수 있다. 이때, 각 채널에 적용되는 시프트 스케일링 팩터는 각 채널에서 가 장 크기가 큰 가중치 값 및 각 채널이 포함된 레이어에서 가장 크기가 큰 가중치 값에 기초하여 결정될 수 있다. 가령, 5개의 레이어 중 제1 레이어에서 크기가 가장 큰 가중치 값이 10이고, 제1 레이어에 포함된 제1 채 널에서 크기가 가장 큰 가중치 값이 6이면, 10과 6 간의 2지수 대수 비율(logarithmic ratio)에 기초하여 제1 레이어에 포함된 제1 채널의 시프트 스케일링 팩터의 초기값이 결정될 수 있다. 그리고, 양자-역양자화 에러값 이나 Top-1 테스트 정확도를 비용 함수로 정의하고, 비선형 최적화 방법(Nelder-Mead, Bayesian Optimization, etc.)을 통해 채널 별 시프트 스케일링 팩터의 최적 값이 획득될 수 있다. 이상과 같이 편차가 상대적으로 작은 채널은 시프트 스케일링 팩터가 상대적으로 크고, 편차가 상대적으로 큰 채널은 시프트 스케일링 팩터가 상대적 으로 작을 수 있으며, 이러한 동작을 통해 레이어 단위로 양자화를 수행하더라도 일정 수준의 정확도가 확보될수 있다. 스케일링이 완료되면, 각 레이어 별로 양자화가 수행된다. 상술한 예에서 가령, 제1 레이어의 가중치 값 중 최 소값 및 최대값이 각각 0 및 255로 매핑되면, 제1 레이어의 가중치 값은 0에서 255 사이의 정수로 치환될 수 있 다. 이상과 같이 채널 별로 시프트 스케일링되고 레이어 별로 양자화된 인공 지능 모델은 양자화된 복수의 가중치 값, 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케일링 팩터 및 복수의 레이어 각각에 대응되는 스 케일 파라미터와 제로 포인트 파라미터를 포함할 수 있다. 여기서, 스케일 파라미터는 복수의 레이어 각각을 양 자화하는 경우, 양자화 전의 값과 양자화 후 값 간의 기울기를 나타내고, 제로 포인트 파라미터는 복수의 레이 어 각각을 양자화하는 경우, 양자화 전 제로 값의 양자화 후의 값을 나타낼 수 있다. 이는 종래 채널 별 스케일링 없이 레이어 단위 양자화를 수행하는 경우보다 채널 별 시프트 스케일링 팩터만큼 의 추가 데이터가 필요하나, 종래 스케일링 없이 양자화를 수행하는 경우는 양자화에 따라 일부 채널의 정확도 가 낮아지는 문제가 있으며, 본 개시에 따르면 채널 별 시프트 스케일링을 통해 이러한 문제를 해소하는 장점이 있다. 또는, 종래 채널 별 양자화를 수행하는 경우 정확도가 확보되는 반면, 각 채널마다 양자화 파라미터(38- bit 부동 소수점 형태의 스케일과 8-bit 정수 형태의 제로 포인트)가 필요하여 상당한 데이터가 필요한 문제가 있으나, 본 개시에 따르면 채널 별 시프트 스케일링을 통해 일정 수준의 정확도를 확보하면서도 추가되는 데이 터는 4-bit 정수 형태의 채널 별 시프트 스케일링 팩터에 불과한 장점이 있다. 즉, 채널 별 시프트 스케일링 후 레이어 별로 양자화된 인공 지능 모델은 종래 스케일링 없이 레이어 별로 양자 화를 수행하는 경우보다 채널 별 시프트 스케일링 팩터만큼의 추가 저장 용량을 요구하나 가중치의 용량 대비 미미한 수준으로, 이는 온 디바이스 형태로 신경망 연산을 수행함에 있어 크게 문제되지 않는다. 그럼에도 채널 별 스케일링 후 레이어 별로 양자화된 인공 지능 모델을 이용하면 일정 수준의 정확도가 확보 가능하며, 이에 대하여는 프로세서의 동작과 함께 설명한다. 프로세서는 전자 장치의 동작을 전반적으로 제어한다. 구체적으로, 프로세서는 전자 장치 의 각 구성과 연결되어 전자 장치의 동작을 전반적으로 제어할 수 있다. 예를 들어, 프로세서는 메모 리, 통신 인터페이스(미도시) 등과 같은 구성과 연결되어 전자 장치의 동작을 제어할 수 있다. 일 실시 예에 따라 프로세서는 디지털 시그널 프로세서(digital signal processor(DSP), 마이크로 프로세 서(microprocessor), TCON(Time controller)으로 구현될 수 있다. 다만, 이에 한정되는 것은 아니며, 중앙처리 장치(central processing unit(CPU)), MCU(Micro Controller Unit), MPU(micro processing unit), 컨트롤러 (controller), 어플리케이션 프로세서(application processor(AP)), 또는 커뮤니케이션 프로세서 (communication processor(CP)), ARM 프로세서 중 하나 또는 그 이상을 포함하거나, 해당 용어로 정의될 수 있 다. 또한, 프로세서는 프로세싱 알고리즘이 내장된 SoC(System on Chip), LSI(large scale integration) 로 구현될 수도 있고, FPGA(Field Programmable gate array) 형태로 구현될 수도 있다. 프로세서는 입력 데이터가 수신되면, 입력 데이터에 대한 신경망 연산 과정에서 각 채널 별 연산 결과를 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링된 합성 스케일 파라미터와 연산할 수 있다. 이러한 동작은 이상에서 설명한 바와 같이, 양자 에러를 줄이기 위해 채널 별로 시프트 스케일링함에 따라 상이 해진 출력의 스케일을 원래 스케일로 복원하고자 함이다. 이러한 프로세서의 동작은 도 1b의 각종 모듈을 통해 좀더 구체적으로 설명한다. 도 1b는 본 개시의 일 실시 예에 따른 전자 장치의 소프트웨어 구성을 설명하기 위한 블록도이다. 도 1b에 서 프로세서 내부에 복수의 모듈이 위치하는 것은 복수의 모듈이 프로세서에 의해 로딩(또는 실행)되 어 프로세서에서 동작되는 상태를 나타내기 위한 것이며, 복수의 모듈은 메모리에 기저장된 상태일 수 있다. 도 1b를 참조하면, 메모리에는 채널 별로 시프트 스케일링되고 레이어 별로 양자화된 인공 지능 모델, 입 력 데이터가 저장될 수 있다. 여기서, 인공 지능 모델은 스케일 파라미터 및 제로 포인트 파라미터를 포함할 수 있다. 그리고, 프로세서는 메모리에 저장된 모듈 또는 인스트럭션을 실행함으로써 전자 장치의 전반적 인 동작을 제어할 수 있다. 구체적으로, 프로세서는 모듈 또는 인스트럭션을 읽고 해석하며 데이터 처리를 위한 시퀀스를 결정할 수 있으며, 그에 따라 메모리 등 다른 구성의 동작을 제어하는 제어 신호를 전송함으로써 다른 구성의 동작을 제어할 수 있다. 프로세서는 신경망 연산 모듈 및 채널 별 역스케일링 모듈을 실행함으로써 입력 데이터를 양자화된 인공 지능 모델에 적용할 수 있다. 이때, 프로세서는 입력 데이터에 대한 신경망 연산을 수행할 수 있고, 각 채 널 별 연산 결과에 대한 역스케일링을 위해 합성 스케일 파라미터를 획득할 수 있다. 여기서, 신경망 연산 모듈 및 채널 별 역스케일링 모듈은 물리적으로 하나의 모듈로 구현될 수도 있고, 구분된 형태로 구현될 수도 있다. 예를 들어, 프로세서는 입력 데이터 또는 피쳐 맵 데이터를 대응되는 채널의 가중치 값과 연산한 후, 연산 결과를 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링된 합성 스케일 파라미터와 연산할 수 있다. 구체 적으로, 프로세서는 입력 데이터를 제1 레이어에 포함된 복수의 제1 채널 각각의 가중치 값과 연산하고, 복수의 제1 채널 각각의 가중치 값과의 연산 결과를 복수의 제1 채널 각각에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링된 합성 스케일 파라미터와 연산할 수 있다. 그리고, 프로세서는 제1 레이터로부터 출 력되는 피쳐 맵 데이터를 제1 레이어 다음의 제2 레이어에 포함된 복수의 제2 채널 각각의 가중치 값과 연산하 고, 복수의 제2 채널 각각의 가중치 값과의 연산 결과를 복수의 제2 채널 각각에 대응되는 시프트 스케일링 팩 터에 기초하여 역스케일링된 합성 스케일 파라미터와 연산할 수 있다. 프로세서의 역스케일링 동작은 시프 트 동작으로써 구현되며, 시프트 동작 및 합성 스케일 파라미터에 대하여는 후술한다. 먼저, 인공 지능 모델에 포함된 가중치 값은 인공 지능 모델에 포함된 채널 별로 상이한 시프트 스케일링 팩터 에 기초하여 시프트 스케일링되고, 레이어 별로 양자화됨에 따라 획득된 값이다. 이를 설명하기 위해 인공 지능 모델의 데이터의 구조를 설명한다. 인공 지능 모델은 복수의 레이어 각각에 대응되는 스케일 파라미터와 제로 포인트 파라미터를 포함할 수 있다. 여기서, 스케일 파라미터는 복수의 레이어 각각을 양자화하는 경우, 양자화 전의 값과 양자화 후 값 간의 기울 기를 나타내고, 제로 포인트 파라미터는 복수의 레이어 각각을 양자화하는 경우, 양자화 전 제로 값의 양자화 후의 값을 나타낼 수 있다. 예를 들어, 제1 레이어의 가중치 값 중 최소값 및 최대값이 각각 0 및 255로 매핑되는 방식으로 양자화를 수행 하면, 가중치 값 중 최소값 및 최대값과 0 및 255의 상관 관계를 나타내는 스케일 파라미터, 제로 포인트 파라 미터 및 양자화된 가중치 값이 획득될 수 있다. 여기서, 스케일 파라미터는 양자화 전후 데이터의 상관 관계를 나타내는 기울기를 의미하고, 제로 포인트 파라미터는 실수 0.0을 나타내는 양자값 또는 상관 관계가 원점으로 부터 벗어난 정도를 의미한다. 이상과 같은 방법을 통해 각 레이어 별 가중치에 대한 스케일 파라미터 및 제로 포인트 파라미터가 획득될 수 있다. 또한, 동일한 방법으로 각 레이어 별 입력 및 출력에 대한 스케일 파라미터 및 제로 포인트 파라미터가 획득될 수 있다. 프로세서는 현재 레이어의 스케일 파라미터, 상기 현재 레이어 직전 레이어의 스케일 파 라미터 및 상기 복수의 가중치 값의 스케일 파라미터에 기초하여 획득된 값을, 상기 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링하여 상기 합성 스케일 파라미터를 획득할 수 있다. 예를 들어, 프로세서는 하기의 수학식 1로 양자화된 출력 값을 획득할 수 있다. 수학식 1의 획득 과정은 도면을 통해 후술한다. [수학식 1]"}
{"patent_id": "10-2020-0121231", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, 는 각각 이전 레이어의 출력 데이터로서 피쳐 맵 데이터(또는, 입력 데이터), 이전 레이 어의 스케일 파라미터, 이전 레이어의 제로 포인트 파라미터이고, 는 각각 양자화된 가중치 값, 가 중치 값의 스케일 파라미터, 가중치 값의 제로 포인트 파라미터이고, 는 각각 현재 레이어의 출 력 데이터, 현재 레이어의 스케일 파라미터, 현재 레이어의 제로 포인트 파라미터이다. 는 floating bias를 스케일로 symmetric quantization 한 후의 양자 bias 값이고, i, j는 각각 출력 채널 인덱스, 입 력 채널 인덱스이다. 프로세서는 신경망 연산 모듈을 실행함으로써 메모리로부터 가중치 값, 제로 포인트 파라미터(이전 레이어, 현재 레이어, 가중치 값), 입력 데이터를 획득하고, 채널 별 역스케일링 모듈을 실행함으로써 메모리 로부터 채널 별 시프트 스케일링 팩터, 스케일 파라미터(이전 레이어, 현재 레이어, 가중치 값)를 획득할 수 있다. 채널 별 역스케일링 모듈은 이전 레이어의 스케일 파라미터, 현재 레이어의 스케일 파라미터 및 가중 치 값의 스케일 파라미터에 기초하여 획득된 값을, 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스 케일링하여 합성 스케일 파라미터를 획득할 수 있다. 즉, 합성 스케일 파라미터의 획득 과정은 역스케일링을 포 함하며, 채널 별 역스케일링 모듈은 합성 스케일 파라미터를 신경망 연산 모듈로 제공할 수 있다. 여기서, 획득 된 값은 현재 레이어의 스케일 파라미터에 반비례하고, 현재 레이어 직전 레이어의 스케일 파라미터 및 복수의 가중치 값의 스케일 파라미터에 비례할 수 있다. 즉, 프로세서는 수학식 1의 스케일 파라미터 간의 연산 값인 를 의 형태로 변환하고, 각 채널에 대응되는 시프트 스케일링 팩터()를 와 같이 추가함에 따라 역스케일링된 합성 스케일 파라미터를 획득할 수 있다. 특히, 프로세서는 이진법으로 데이터를 처리하기 때문에, 시프트 스케일링 팩터의 추가는 시프트 동작으로 써 구현될 수 있다. 즉, 프로세서는 획득된 값을 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 시 프트함으로써 합성 스케일 파라미터를 획득할 수 있으며, 단순히 시프트 동작을 추가하는 것에 불과하여 하드웨 어 구현이 용이하다. 프로세서는 신경망 연산 모듈을 실행함으로써 수학식 1과 같은 연산을 수행할 수 있다, 구체적으로, 프로 세서는 가중치 값, 제로 포인트 파라미터(이전 레이어, 현재 레이어, 가중치 값), 입력 데이터, 합성 스케 일 파라미터에 기초하여 신경망 연산을 수행할 수 있다. 여기서, 이전 레이어 및 현재 레이어의 제로 포인트 파 라미터는 현재 레이어의 계산 동안에는 고정된 값이고, 가중치 값의 제로 포인트 파라미터는 가중치 값의 양자 화에 이용된 값으로 레이어가 변경되기 까지는 동일하다. 또한, 신경망 연산의 수행 과정에서 채널 별 역스케일 링 모듈을 통해 획득되는 합성 스케일 파라미터는 채널 별로 상이할 수 있다. 한편, 프로세서는 채널 별 역스케일링 모듈을 실행함으로써 양자화된 인공 지능 모델에 복수의 레이어 각 각에 포함된 복수의 채널 별 시프트 스케일링 팩터가 포함되었다고 식별되면, 역스케일링을 수행할 수 있다. 즉, 프로세서는 인공 지능 모델이 채널 별 시프트 스케일링 팩터를 포함하는지 여부에 기초하여 역스케일 링 동작의 수행 여부를 결정할 수 있다. 구체적으로, 프로세서는 인공 지능 모델이 채널 별 시프트 스케일 링 팩터를 포함하는 경우 시프트 동작을 통해 역스케일링을 수행하고, 인공 지능 모델이 채널 별 시프트 스케일 링 팩터를 포함하지 않는 경우 시프트 동작을 수행하지 않을 수 있다. 즉, 프로세서는 인공 지능 모델이 채널 별 시프트 스케일링 팩터를 포함하는 경우 를 신경망 연산에 이용하고, 인공 지능 모델이 채널 별 시프트 스케일링 팩터를 포함하지 않는 경우 를 신경망 연산에 이용하게 된다. 한편, 전자 장치는 신경망 처리 장치(Neural Processing Unit, NPU)로 구현될 수도 있다. 경우, 이 경우, 신경망 처리 장치에 포함된 캐시 메모리 등이 메모리로서 동작하고, 신경망 처리 장치에 포함된 복수의 연 산 소자(Processing Element) 등이 프로세서로서 동작할 수 있다. 이상과 같이 프로세서는 신경망 연산 과정에서 역스케일링 동작을 수행할 수 있으며, 일부 데이터의 시프 트 동작 만으로 역스케일링의 결과를 획득할 수 있어 온 디바이스 형태로의 구현이 용이하다. 또한, 인공 지능 모델의 양자화 과정에서 각 채널의 스케일링에 따라 각 채널의 데이터가 뭉개지지 않는 효과가 있으며, 그로 인해 일정 수준의 정확도의 확보가 가능하다. 한편, 이상에서 설명한 바와 같이, 역스케일링 동작은 시프터를 통해 구현될 수 있다. 예를 들어, 도 1d에 도시 된 바와 같이, 시프터는 프로세서 내부의 일 구성으로서 구현될 수 있다. 즉, 도 1b의 채널 별 역스케일링 모듈은 시프터로서 구현될 수 있다. 또는, 시프터는 도 1e에 도시된 바와 같이, 프로세서 외부의 구성으로서 구현될 수도 있다. 이 경우, 시프터는 메모리로부터 스케일 파라미터 및 채널 별 시프트 스케일링 팩터를 수신하고, 이전 레이어 의 스케일 파라미터, 현재 레이어의 스케일 파라미터 및 가중치 값의 스케일 파라미터에 기초하여 획득된 값을, 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링하여 합성 스케일 파라미터를 획득할 수 있다. 그리고, 시프터는 합성 스케일 파라미터를 프로세서로 제공할 수 있다. 도 1e와 같이 시프터가 구현되는 경우, 종래의 프로세서를 이용하더라도 본 개시와 같은 역스케일링이 가능한 효과가 있다. 이하에서는 도면을 통해 본 개시의 다양한 실시 예를 좀더 구체적으로 설명한다. 도 2는 본 개시의 일 실시 예에 따른 컴파일러 및 전자 장치의 동작을 설명하기 위한 도면이다. 컴파일러는 입력 모델(Float Model File)에 포함된 복수의 가중치 값을 복수의 레이어 각각에 포함된 복수 의 채널 별로 상이한 시프트 스케일링 팩터에 기초하여 스케일링하고 복수의 레이어 별로 양자화할 수 있다. 컴파일러는 입력 모델을 파싱하는 파싱 모듈, custom NPU에서 지원하는 op로 재구성하는 Instruction stream 모듈, WES(weight equalizing scaler) 모듈, Float 파라미터를 IntN으로 양자화하는 양자화(quantizer) 모듈, 메모리 분배, 연산을 최적화(타일화)하는 최적화(optimization) 모듈 및 Binary 파일로 만들어 주는 Binarization 모듈을 포함할 수 있다. 특히, WES 모듈은 Instruction Stream 모듈로부터 NPU HW Operation으로 구성된, 단방향으로 사슬처럼 연결된 그래프 파일과 Float32 포맷의 파라미터들을 수신할 수 있다. WES 모듈은 도 1c의 상단과 같이 채널 별로 오리지널 파라미터의 최소값, 최대값들을 획득하고, 각 채널 별 최 소값, 최대값을 이용하여 최소의 양자화 에러를 갖도록 기준 범위를 정하여 채널 별 시프트 스케일링 팩터 (channel-wise shift scale 값)를 획득할 수 있다. 구체적으로, WES 모듈은 채널 별 오리지널 파라미터의 범위를 구하고, 그 중에서 최대값(도 1c 상단의 채널 22 번)을 가지는 범위를 기준 범위로 정하고, 기준 범위에 기초하여 각 채널 별 시프트 스케일링 팩터를 획득할 수 있다. 예를 들어, WES 모듈은 하나의 레이어에 포함된 가중치 값 중 크기가 가장 큰 가중치 값을 식별하고, 식별된 가 중치 값에 기초하여 각 채널의 시프트 스케일링 팩터를 획득할 수 있다. 이때, WES 모듈은 각 채널에서 크기가 가장 큰 가중치 값 및 이상에서 식별된 가중치 값에 기초하여 각 채널의 시프트 스케일링 팩터를 획득할 수 있 다. 또는, WES 모듈은 전체 범위에 대해 각 채널을 스케일링(shift scale)한 후 변화된 범위 값의 비율을 합산하여 그 값이 커지도록 gradient-descent 방식을 적용하여 각 채널의 시프트 스케일링 팩터를 획득할 수도 있다. 또는, WES 모듈은 각 채널을 시프트 스케일링(shift scale)한 후 int로 변환하는 양자화를 진행하고 floating으 로 복구하는 역양자화 과정을 거치면 발생하는 양자 에러를 채널 별로 합산하여 그 값이 최소값을 가지도록 각 채널의 시프트 스케일링 팩터를 획득할 수도 있다. WES 모듈은 기준 범위를 각 채널의 범위로 나눈 다음 지수 2를 가지는 algorithm을 취하고 그것의 내림 값을 취 함으로써 Si를 획득할 수 있다. 2^Si는 이진수를 연산하는 하드웨어에서 비트 Shift(<< Si) 연산으로 간단히 처 리할 수 있어 하드웨어 구현이 용이하다. 채널 별로 2^Si 형태의 스케일링에 따라 업데이트된 파라미터들(wi)은 도 1c의 하단에 도시된 바와 같이, 레이 어 전체의 최소값, 최대값 범위와 최대한 매칭되어 layer-wise quantization하기에 최적화된 범위를 가질 수 있 다. 이때, bias(bi)에도 동일하게 2^Si 스케일이 적용될 수 있다. 이후, 양자화 모듈 파라미터 스케일이 조정된 컨볼루션 레이어에 레이어 별(layer-wise) 선형 양자화(Linear quantization)를 수행할 수 있다. 한편, 전자 장치(NPU HW, 100)는 Fixed computing을 담당하는 ALU 모듈, 각 싸이클에서 연산에 필요한 파라미터 와 입출력 피쳐맵을 저장하는 캐시 메모리 및 전체 파라미터와 피쳐맵을 공유하는 메모리를 포함할 수 있다. 여기서, 컴파일러의 WES 모듈의 동작에 따라 ALU 모듈은 구조적으로 Fixed Computing ALU w/ Ch-wise Shift Scaling로서 변경될 수 있으며, 이에 대하여는 이하의 도면을 통해 좀더 구체적으로 설명한다.도 3은 본 개시의 다양한 실시 예에 따른 시프트 스케일링 팩터를 획득하는 방법을 설명하기 위한 흐름도이다. 먼저, WES 모듈은 채널 별 오리지널 파라미터의 범위를 구할 수 있다(S310). 그리고, WES 모듈은 전체 범위를 선택하고(S320), 전체 범위 및 채널 별 범위에 기초하여 채널 별 시프트 스케일링 팩터를 획득하고, 획득된 채 널 별 시프트 스케일링 팩터에 기초하여 채널 별 시프트 스케일링을 수행할 수 있다(S330). WES 모듈은 채널 별 시프트 스케일링 후, 추가적으로 레이어 별 양자화를 수행하고(S331), 양자화 에러를 산출 하며(S332), 양자화 에러에 기초하여 채널 별 시프트 스케일링 팩터를 재획득하고, 재획득된 채널 별 시프트 스 케일링 팩터에 기초하여 채널 별 시프트 스케일링을 재수행할 수도 있다(S390). 또는, WES 모듈은 채널 별 시프트 스케일링 후, 추가적으로 전체 범위에 대한 비용 함수를 정의하고(S340), 채 널 별 시프트 스케일링 팩터에 기초하여 채널 별 시프트 스케일링을 수행하며(S350), 양자화(S360) 후 양자화 에러를 산출할 수 있다(S370). WES 모듈은 양자화 에러가 기설정된 값에 수렴하는 경우, 채널 별 시프트 스케일 링 팩터를 확정하고, 확정된 채널 별 시프트 스케일링 팩터에 기초하여 채널 별 스케일링을 수행할 수 있다 (S390). 또는, WES 모듈은 양자화 에러가 기설정된 값에 수렴하지 않는 경우, 스케일링 후 변화된 범위 값의 비 율을 합산하고, 그 값이 커지도록 gradient-descent 방식을 적용하여 채널 별 시프트 스케일링 팩터를 재조정할 수 있다. 이상과 같은 방식으로 WES 모듈는 채널 별 시프트 스케일링 팩터를 획득할 수 있다. 도 4a 및 도 4b는 본 개시의 일 실시 예에 따른 역스케일링 동작을 설명하기 위한 도면들이다. 도 4a에 도시된 바와 같이, 좌측의 컨볼루션 연산 및 스케일러는 우측과 같이 하나의 구성으로 구현될 수 있다. 먼저, 좌측의 구성을 설명한다. INT 연산을 수행하는 Fixed computing ALU는 INTN 양자화된 값의 입력 데이터, 가중치 값, 스케일 파라미터, 제 로 포인트 파라미터를 메모리로부터 로드할 수 있다. 여기서, 스케일 파라미터, 제로 포인트 파라미터는 현재 레이어의 스케일 파라미터, 현재 레이어 직전 레이어의 스케일 파라미터 및 복수의 가중치 값의 스케일 파라미 터를 포함할 수 있다. 스케일 파라미터는 양자화 전후 데이터의 상관 관계를 나타내는 기울기를 의미하고, 제로 포인트 파라미터는 실 수 0.0을 나타내는 양자값 또는 상관 관계가 원점으로부터 벗어난 정도를 의미한다. 예를 들어, 도 4b에 도시된 바와 같이, 실수축의 최대값(max)은 255로 양자화되고, 실수축의 최소값(min)은 0으로 양자화될 수 있으며, 이 때의 기울기가 스케일 파라미터이다. 그리고, 실수축의 0 값은 z로 양자화되는데, 이때 z가 제로 포인트 파라미 터이다. 이러한 방식으로 레이어 별 스케일 파라미터 및 제로 포인트 파라미터가 획득될 수 있다. Fixed computing ALU는 상술한 수학식 1과 같이 Fixed Computing Convolution 레이어에서 양자화된 입력값과 파라미터로부터 출력값을 획득할 수 있다. 여기서, , , 은 각각 이전 레이어, 현재 레이어 및 가중 치 값에 대한 int to float 변환하는 float 스케일 값이고, 레이어마다 상이할 수 있다. Fixed computing ALU 는 를 (M : mantissa or multiplier, : exponent or shiftamount)의 형태로 변환할 수 있다. Fixed computing ALU는 추가로 각 채널 별 2^Si 중의 4비트로 표현된 integer Si를 메모리로부터 로드할 수 있 다. 채널 별 integer Si는 컴파일러로부터 수신되어 메모리에 저장될 수 있다. 그리고, Fixed computing ALU는 각 채널에 대응되는 시프트 스케일링 팩터()를 와 같이 추가함에 따라 가 역스케일링된 합성 스케일링 파라미터를 획득할 수 있으며, 도 4a의 우측과 같이 역스케일링 동작을 컨볼루 션 연산 중에 부가적으로 수행할 수 있다. 특히, 역스케일링 동작은 채널 별 반대 방향으로의 시프트 형태로 처 리될 수 있어 하드웨어적인 구현이 용이하다. 한편, 이상에서는 컴파일러가 전자 장치로 채널 별 시프트 스케일링 팩터에 대한 정보를 2^Si 중의 4 비트로 표현된 integer Si로서 제공하는 것으로 설명하였다. 예를 들어, layer-wise quantization을 지원하는 하드웨어에서는 composite scale은 채널 공통으로 Multiplier(M, 32bit)와 Shift amount(S, 6bit)로 구성되며, 상술한 방법에 따르면 Multiplier(M), Shift amount(S)는 채널 공통으로 사용하되, Ch-wise shift scaling(Si)를 별도의 4bit 포맷의 추가 파라미터로 컴파일러로부터 수신하고, Shift amount(S)에서 Ch-wise shift scale(Si)를 뺀 만큼을 출력값에 스케일해주고 Multiplier를 곱하는 연산을 수행하는 방식으로 역스케일링을 수 행할 수 있다. 다른 방법으로는, 출력값을 Shift amount만큼 시프트한 후, Ch-wise shift scale만큼 또 시프트 한 후, Multiplier를 곱하는 이중 시프트 연산을 수행할 수도 있다. 다만, 이에 한정되는 것은 아니며, 컴파일러가 역스케일링이 적용된 채널 별 시프트 스케일링 팩터를 제공 할 수도 있다. 예를 들어, Multiplier(M)는 채널 공통으로 사용하되, 컴파일러는 Shift amount(S)는 채널 공통의 S에 Ch-wise shift scale(Si)을 채널 별로 뺀 6bit 정보를 전자 장치로 제공할 수도 있다. 한편, 이상에서는 컴파일러 및 전자 장치가 별도의 장치인 것으로 설명하였으나, 두 장치는 하나의 통합된 장치로 구현될 수도 있다. 도 5a 내지 도 5c는 본 개시에 따른 효과를 설명하기 위한 도면들이다. 도 5a에 도시된 바와 같이, WES가 이용된 경우의 정확도는 양자화가 없는 경우(baseline)의 정확도에 상당히 근 접하면서 종래보다 높은 정확도를 나타낸다. 반면, 도 5b에 도시된 바와 같이, WES가 이용된 경우의 파라미터의 크기는 채널 별로 양자화하는 경우보다 상당 히 작고, 레이어 별로 양자화하는 경우보다 약간 큰 정도이다. 도 5c는 채널의 개수에 따른 파라미너터의 크기를 비교하며, 도 5b와 유사한 결과를 나타낸다. 즉, WES가 이용 된 경우는 레이어 별로 양자화하는 경우보다 약간의 파라미터의 증가가 있으나, 그로 인해 확보되는 정확도는 양자화가 없는 경우에 근접한다. 도 6은 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도이다. 먼저, 입력 데이터를 수신한다(S610). 그리고, 인공 지능 모델을 이용하여 입력 데이터에 대한 신경망 연산 과 정에서 인공 지능 모델을 구성하는 복수의 레이어 각각의 채널 별 연산 결과를 각 채널에 대응되는 시프트 스케 일링 팩터에 기초하여 역스케일링된 합성 스케일 파라미터와 연산한다(S620). 여기서, 인공 지능 모델은 복수의 레이어 각각에 포함된 복수의 채널 별로 상이한 시프트 스케일링 팩터에 기초하여 스케일링되고 복수의 레이어 별로 양자화된 복수의 가중치 값을 포함할 수 있다. 또한, 인공 지능 모델은 양자화된 복수의 가중치 값, 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케 일링 팩터 및 복수의 레이어 각각에 대응되는 스케일 파라미터와 제로 포인트 파라미터를 포함할 수 있다. 여기 서, 스케일 파라미터는 복수의 레이어 각각을 양자화하는 경우, 양자화 전의 값과 양자화 후 값 간의 기울기를 나타내고, 제로 포인트 파라미터는 복수의 레이어 각각을 양자화하는 경우, 양자화 전 제로 값의 양자화 후의 값을 나타낼 수 있다. 그리고, 연산하는 단계(S620)는 현재 레이어의 스케일 파라미터, 현재 레이어 직전 레이어의 스케일 파라미터 및 복수의 가중치 값의 스케일 파라미터에 기초하여 획득된 값을, 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링하여 합성 스케일 파라미터를 획득할 수 있다. 또한, 연산하는 단계(S620)는 획득된 값을 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 시프트함으로 써 합성 스케일 파라미터를 획득할 수 있다. 여기서, 획득된 값은 현재 레이어의 스케일 파라미터에 반비례하고, 현재 레이어 직전 레이어의 스케일 파라미 터 및 복수의 가중치 값의 스케일 파라미터에 비례할 수 있다. 그리고, 연산하는 단계(S620)는 복수의 레이어 각각에 포함된 복수의 채널 별 시프트 스케일링 팩터가 인공 지 능 모델에 포함되었다고 식별되면, 연산을 수행할 수 있다. 한편, 전자 장치는 신경망 처리 장치(Neural Processing Unit, NPU)로 구현될 수 있다. 그리고, 각 채널에 대응되는 시프트 스케일링 팩터는 각 채널에 포함된 가중치 값 및 각 채널을 포함하는 레이 어에 포함된 가중치 값에 기초하여 결정될 수 있다. 여기서, 각 채널에 대응되는 시프트 스케일링 팩터는 각 채널에서 가장 크기가 큰 가중치 값 및 각 채널을 포함 하는 레이어에서 가장 크기가 큰 가중치 값에 기초하여 결정될 수 있다. 이상과 같은 본 개시의 다양한 실시 예에 따르면, 전자 장치는 입력 데이터에 대한 신경망 연산 과정에서 각 채 널 별 연산 결과를 각 채널에 대응되는 시프트 스케일링 팩터에 기초하여 역스케일링된 합성 스케일 파라미터와연산함으로써 상대적으로 작은 용량으로 구현된 인공 지능 모델을 이용하면서도 신경망 연산을 정확도를 향상시 킬 수 있다. 한편, 본 개시의 일시 예에 따르면, 이상에서 설명된 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media)에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실 시 예들에 따른 전자 장치(예: 전자 장치(A))를 포함할 수 있다. 명령이 프로세서에 의해 실행될 경우, 프로세 서가 직접, 또는 프로세서의 제어 하에 다른 구성요소들을 이용하여 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저 장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장매체가 신 호(signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임시 적으로 저장됨을 구분하지 않는다. 또한, 본 개시의 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품 (computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있 다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 또한, 본 개시의 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들은 소프트웨어(software), 하드웨어 (hardware) 또는 이들의 조합을 이용하여 컴퓨터(computer) 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 일부 경우에 있어 본 명세서에서 설명되는 실시 예들이 프로세서 자체로 구현될 수 있 다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들은 별도의 소프트웨 어 모듈들로 구현될 수 있다. 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 동작을 수행할 수 있다. 한편, 상술한 다양한 실시 예들에 따른 기기의 프로세싱 동작을 수행하기 위한 컴퓨터 명령어(computer instructions)는 비일시적 컴퓨터 판독 가능 매체(non-transitory computer-readable medium)에 저장될 수 있 다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령어는 특정 기기의 프로세서에 의해 실행되었 을 때 상술한 다양한 실시 예에 따른 기기에서의 처리 동작을 특정 기기가 수행하도록 한다. 비일시적 컴퓨터 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체가 아니라 반영구 적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 비일시적 컴퓨터 판독 가능 매체의 구체적인 예로는, CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등이 있을 수 있다. 또한, 상술한 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구 성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요 소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로 그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작 들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생 략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2020-0121231", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2020-0121231", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1a는 본 개시의 일 실시 예에 따른 전자 장치의 구성을 나타내는 블록도이다. 도 1b는 본 개시의 일 실시 예에 따른 전자 장치의 소프트웨어 구성을 설명하기 위한 블록도이다. 도 1c는 본 개시의 일 실시 예에 따른 채널 별 스케일링을 설명하기 위한 도면이다. 도 1d 및 도 1e는 본 개시의 다양한 실시 예에 따른 시프터의 구현 방법을 설명하기 위한 도면이다. 도 2는 본 개시의 일 실시 예에 따른 컴파일러 및 전자 장치의 동작을 설명하기 위한 도면이다. 도 3은 본 개시의 다양한 실시 예에 따른 시프트 스케일링 팩터를 획득하는 방법을 설명하기 위한 흐름도이다. 도 4a 및 도 4b는 본 개시의 일 실시 예에 따른 역스케일링 동작을 설명하기 위한 도면들이다. 도 5a 내지 도 5c는 본 개시에 따른 효과를 설명하기 위한 도면들이다. 도 6은 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도이다."}
