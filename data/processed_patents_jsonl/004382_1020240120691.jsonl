{"patent_id": "10-2024-0120691", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0052951", "출원번호": "10-2024-0120691", "발명의 명칭": "인공지능 기술에 기반한 감정 추적 및 관리 시스템 및 방법", "출원인": "주식회사 마고", "발명자": "고현웅"}}
{"patent_id": "10-2024-0120691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "복수의 사용자 각각에 대응하는 복수의 사용자 장치; 및상기 복수의 사용자 장치에 질문 콘텐츠를 제공하고, 상기 질문 콘텐츠에 대한 상기 복수의 사용자의 답변에 기초하여 추적된 상기 복수의 사용자의 감정을 복수의 사용자 사이에 서로 공유하는 사용자 환경을 제공하는 서버를 포함하되, 상기 사용자 환경은, 상기 질문 콘텐츠가 제공되는 질문 인터페이스;상기 질문 콘텐츠에 대한 답변을 녹음하여 상기 복수의 사용자의 음성 데이터를 생성하는 것을 지원하는 녹음인터페이스;상기 질문 콘텐츠, 상기 음성 데이터 및 추적된 감정 데이터를 매칭하여 기록된 트랙을 조회하고, 특정트랙을선택하는 것을 지원하는 조회 인터페이스; 선택된 트랙과 관련된 음성 데이터를 제공하는 음성 인터페이스;상기 음성 인터페이스에서 제공되는 음성 데이터에 대응한 감정 데이터를 표시하는 감정 인터페이스;상기 음성 인터페이스에서 제공되는 음성 데이터에 대응하는 키워드와 요약 정보를 제공하는 텍스트 인터페이스및상기 선택된 트랙을 다른 사용자에게 공유하는 것을 지원하는 공유 인터페이스를 포함하는, 사용자의 감정 추적 및 관리 시스템."}
{"patent_id": "10-2024-0120691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 질문 인터페이스는 상기 질문 콘텐츠의 유형 정보를 함께 제공하는, 상기 조회 인터페이스는 상기 질문 콘텐츠의 유형 정보를 함께 제공하는, 사용자의 감정 추적 및 관리 시스템."}
{"patent_id": "10-2024-0120691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서, 상기 사용자 환경은 제1 사용자가 관계된 제2 사용자에게 제1 질문 콘텐츠를 생성하여 전달하는 것을 지원하며,상기 제2 사용자가 상기 제1 질문 콘텐츠에 대한 응답을 통해 감정 데이터를 생성하여 제1 트랙을 기록하는 경우, 상기 사용자 환경은 상기 제1 트랙에 대한 조회하는 기능을 상기 제1 사용자에게 제공하고, 상기 제1 트랙은 관계된 상기 제1 사용자와 상기 제2 사용자에 대한 정보가 포함되는, 사용자의 감정 추적 및 관리 시스템."}
{"patent_id": "10-2024-0120691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "공개특허 10-2025-0052951-3-제1 항에 있어서,상기 사용자 환경은 사용자의 시간에 따른 감정 데이터 변화를 나타내는 감정 그래프를 더 제공하는, 사용자의 감정 추적 및 관리 시스템."}
{"patent_id": "10-2024-0120691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서,상기 감정 그래프는,제1 사용자의 제1 감정 데이터의 변화, 상기 제1 사용자와 관계된 제2 사용자의 제2 감정 데이터의 변화 및 상기 제1 감정 데이터와 상기 제2 감정 데이터에 기초하여 도출된 공유 감정 데이터의 변화를 함께 제공하며, 상기 공유 감정 데이터는 상기 사용자 환경을 통해 상기 제1 사용자와 상기 제2 사용자에게 모두 제공되며, 상기 제1 사용자와 상기 제2 사용자가 상호간의 공유하는 감정의 유사 정도에 대한 정보 확인을 지원하는,사용자의 감정 추적 및 관리 시스템."}
{"patent_id": "10-2024-0120691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "서버에서 수행되는 복수의 사용자의 감정 추적, 관리 및 공유하는 방법으로,제1 사용자의 제1 사용자 장치에 질문 콘텐츠를 제공하는 단계;상기 질문 콘텐츠에 대한 상기 제1 사용자의 답변인 제1 음성 데이터를 상기 제1 사용자 장치로부터 수집하는단계;상기 제1 음성 데이터에서 음성 이외의 소리를 제거하여 제1 음성 분리 데이터를 생성하는 단계;상기 제1 음성 분리 데이터에서 음성 구간과 묵음 구간을 식별하여 제1 전처리 음성 데이터를 생성하는 단계;상기 제1 전처리 음성 데이터에 기초하여 상기 제1 사용자의 음성에 대한 제1 텍스트 분석 데이터를 생성하는단계;상기 제1 전처리 음성 데이터에서 상기 묵음 구간을 제외하고 상기 음성 구간만을 취득하여 제1 음성 인식 입력데이터를 구성하는 단계;상기 제1 음성 인식 입력 데이터의 음성 특징을 특징 추출 모델을 통해 추출하는 단계;상기 추출된 음성 특징을 자소 단위로 음성 인식 데이터 생성 모델을 통해 인식하여 제1 음성 인식 데이터를 생성하는 단계;상기 제1 전처리 음성 데이터에서 바이오마커를 추출하고, 상기 바이오마커를 제1 감정 분석 모델에 입력하여복수의 감정 각각에 대한 점수를 계산하는 단계;상기 제1 음성 인식 데이터를 제2 감정 분석 모델에 입력하여 텍스트 기반 감정 분석을 수행하여 복수의 감정각각에 대한 점수를 계산하는 단계; 및상기 제1 감정 분석 모델에서 출력되는 결과와 상기 제2 감정 분석 모델에서 출력되는 결과에 기초하여 상기 제1 사용자의 제1 감정 데이터를 생성하는 것을 포함하는, 사용자의 감정 추적 및 관리 방법."}
{"patent_id": "10-2024-0120691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서, 상기 제1 음성 인식 데이터를 텍스트 분석 모델을 통해 분석하여 키워드 정보 및 요약 정보를 생성하여 제1 텍공개특허 10-2025-0052951-4-스트 분석 데이터를 구성하는 단계; 및상기 제1 감정 데이터와 상기 제1 텍스트 분석 데이터를 기록하는 기록 단계; 및상기 제1 감정 데이터와 상기 제1 텍스트 분석 데이터를 사용자 환경을 통해 상기 제1 사용자 장치에 제공하는단계를 더 포함하는, 사용자의 감정 추적 및 관리 방법."}
{"patent_id": "10-2024-0120691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서, 상기 특징 추출 모델은 스펙트로그램 변환 또는 필터 뱅크 특징 값을 이용하여 상기 음성 특징을 추출하며, 상기 음성 인식 데이터 생성 모델은 인공 신경망 기반의 딥러닝 모델에 해당하고, 상기 텍스트 분석 모델은 상기 제1 음성 인식 데이터에 대한 언어 처리를 수행하도록 구성된 모델인,사용자의 감정 추적 및 관리 방법."}
{"patent_id": "10-2024-0120691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7 항에 있어서, 상기 바이오 마커는 보이스 바이오 마커이며, 상기 제1 감정 분석 모델은 상기 제1 전처리 음성 데이터에서 상기 보이스 바이오 마커를 추출하도록 구성된 바이오마커 추출 모델과 상기 추출된 보이스 바이오 마커를 기초로 복수의 감정 각각에 대한 점수를 계산하는 감정 결정 모델을 포함하는, 사용자의 감정 추적 및 관리 방법."}
{"patent_id": "10-2024-0120691", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "하드웨어와 결합되어 제6 항 내지 제9 항 중 어느 한 항에 따른 사용자의 감정 추적 및 관리 방법을 실행시키기위해 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공지능 기술에 기반한 복수 사용자의 감정 추적 및 관리 방법 및 서버가 제공된다. 몇몇 실시예에 따른 감정 추적 및 관리 방법 및 서버는 복수의 사용자 각각의 음성 데이터에 기초하여 감정 데이터 및 텍스트 분석 데이터 를 생성하는 것을 지원하며, 서로 관계 사용자들 사이에서 생성된 감정을 공유하는 것을 지원할 수 있다. 몇몇 (뒷면에 계속)"}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 기술에 기반한 감정 추적 및 관리 시스템 및 방법에 관한 것이다. 본 발명은 음성 인식 인 공지능 기술과 언어 처리 인공지능 기술에 기반하여 사용자들의 일상을 효과적으로 기록하고, 이 과정에서 획득 된 사용자의 음성 및 언어를 분석하여 사용자의 감정을 추적하고, 관리하는 서비스를 제공하는 시스템 및 방법 에 관한 것이다."}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이 부분에 기술된 내용은 단순히 본 실시예에 대한 배경 정보를 제공할 뿐 종래기술을 구성하는 것은 아니다. COVID-19 팬데믹 시기를 거치면서 사람들은 주변 사람과 일상을 나누고 소통하는 것의 중요성을 더욱 인식하게 되었다. 특히, 평소 자신의 감정을 꾸준히 체크하고 케어하는 행동은 더욱 건강한 삶을 가능하게 하는 습관으로 강조되고 있다. 하지만, 일반인이 자신의 감정을 스스로 판단하는 것은 주관적인 요인이 작용할 수 있어 정확한 결과를 출력하 기 어려우며, 이러한 활동을 매일 혼자서 수행하는 것도 쉽지 않은 상황이다. 또한, 일반인이 전문가와 함께하 는 상담을 주기적으로 참여하는 것은 비용과 시간적인 부담이 발생하게 된다. 따라서, 일반 사용자가 꾸준히 정신 건강을 확인할 수 있는 활동을 지원하면서, 정확한 결과를 출력하는 것을 지원하는 시스템 및 방법이 요구되고 있다."}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 과제는, 사용자에게 질문 콘텐츠를 제공하여 사용자의 발화를 유도하며, 음성 인식을 통해 사용자의 발화를 용이하게 기록하고, 기록된 사용자의 발화 내용을 분석하여 사용자의 감정을 분석하여 사용자의 감정 상 태를 추적, 관리할 수 있는 시스템 및 방법을 제공하는 것이다. 본 발명의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 발명의 다른 목적 및 장점들 은 하기의 설명에 의해서 이해될 수 있고, 본 발명의 실시예에 의해 보다 분명하게 이해될 것이다. 또한, 본 발 명의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것 이다."}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 몇몇 실시예에 따른 사용자의 감정 추적 및 관리 방법은 제1 사용자의 음성에 기초하여 생성된 제1 음성 데이터를 제1 사용자 단말로부터 수집하는 음성 수집 단계; 상기 제1 음성 데이터에 대한 전처리를 수행하 여 제1 전처리 음성 데이터를 생성하는 전처리 단계; 상기 제1 전처리 음성 데이터에 기초하여 상기 제1 사용자 의 음성에 대한 제1 텍스트 분석 데이터를 생성하는 텍스트 분석 데이터 생성 단계; 상기 제1 전처리 음성 데이 터 및 상기 텍스트 분석 데이터에 기초하여 상기 제1 사용자의 제1 감정 데이터를 생성하는 감정 데이터 생성 단계; 상기 제1 감정 데이터와 상기 제1 텍스트 분석 데이터를 매칭하여 기록하는 기록 단계; 및 상기 제1 감정 데이터와 상기 제1 텍스트 분석 데이터를 사용자 환경을 통해 상기 제1 사용자 단말에 함께 제공하는 단계를 포 함한다. 또한, 상기 음성 수집 단계, 상기 전처리 단계, 상기 텍스트 분석 데이터 생성 단계, 상기 감정 데이터 생성 단 계 및 상기 기록 단계는 적어도 하루에 한번 수행되며, 상기 사용자 환경은 시간에 따른 상기 제1 사용자의 제1 감정 데이터의 변화를 나타내는 감정 그래프를 제공하는 것을 포함할 수 있다. 또한, 상기 제1 사용자의 음성은 질문 콘텐츠에 대한 상기 제1 사용자가 답변일 수 있다. 또한, 상기 제1 사용자의 명령에 대응하여 질문 콘텐츠를 생성하는 단계; 상기 질문 콘텐츠를 상기 제1 사용자 와 관계된 제2 사용자의 제2 사용자 단말로 제공하는 단계; 상기 질문 콘텐츠에 대응하여 생성된 상기 제2 사용 자의 제2 음성 데이터를 수신하는 단계; 및 상기 제2 음성 데이터를 분석하여 제2 감정 데이터와 제2 텍스트 분 석 데이터를 생성하는 단계를 포함할 수 있다. 상기 제1 감정 데이터와 상기 제2 감정 데이터에 기초하여 공유 감정 데이터를 생성하는 단계를 더 포함하고, 상기 사용자 환경은 상기 제2 감정 데이터, 상기 제2 텍스트 분석 데이터 및 상기 공유 감정 데이터를 더 제공 하도록 구성될 수 있다. 또한, 상기 사용자 환경은 상기 기록된 제1 감정 데이터 및 제1 텍스트 분석 데이터에 대한 통계 분석 자료를 더 제공하도록 구성될 수 있다. 또한, 상기 전처리 단계는 상기 제1 음성 데이터에서 음성 이외의 소리를 제거하여 제1 음성 분리 데이터를 생 성하는 음성 분리 단계; 및 상기 제1 음성 분리 데이터에서 음성 구간과 묵음 구간을 식별하여 상기 제1 전처리 음성 데이터를 생성하는 음성 구간 분리 단계를 포함할 수 있다. 또한, 상기 텍스트 분석 데이터 생성 단계는 상기 제1 전처리 음성 데이터에서 상기 묵음 구간을 제외하고 상기 음성 구간만을 취득하여 제1 음성 인식 입력 데이터를 구성하는 단계; 상기 제1 음성 인식 입력 데이터를 특징 추출 모델에 입력하는 단계; 상기 특징 추출 모델에서 추출된 특징에 기초하여 상기 제1 음성 인식 입력 데이터에 대응하는 제1 음성 인식 데이터를 생성하는 단계; 및 상기 제1 음성 인식 데이터에서 적어도 하나의 키워드"}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "를 판단하여 키워드 정보를 구성하고, 상기 제1 음성 인식 데이터를 요약한 요약 정보를 생성하여 상기 제1 텍 스트 분석 데이터를 생성하는 단계를 포함할 수 있다. 또한, 상기 감정 데이터 생성 단계는 상기 제1 전처리 음성 데이터에서 바이오마커를 추출하고, 상기 바이오마 커를 제1 감정 분석 모델에 입력하여 복수의 감정 각각에 대한 점수를 계산하는 단계; 상기 제1 음성 인식 데이 터를 제2 감정 분석 모델에 입력하여 텍스트 기반 감정 분석을 수행하여 복수의 감정 각각에 대한 점수를 계산 하는 단계; 및 상기 제1 감정 분석 모델에서 출력되는 결과와 상기 제2 감정 분석 모델에서 출력되는 결과에 기 초하여 상기 제1 감정 데이터를 생성하는 것을 포함할 수 있다. 본 발명의 몇몇 실시예에 따른 방법은 서버에서 수행되는 복수의 사용자의 감정 추적, 관리 및 공유하는 방법으 로, 제1 사용자의 제1 음성 데이터를 분석하여 제1 음성 데이터와 제1 텍스트 분석 데이터를 생성하는 단계; 상 기 복수의 사용자 중 상기 제1 사용자와 관계된 적어도 한명의 제2 사용자에게 질문 콘텐츠를 제공하는 단계; 상기 질문 콘텐츠에 대응하여 생성된 제2 사용자의 제2 음성 데이터를 수신하는 단계; 상기 제2 음성 데이터를 분석하여 제2 음성 데이터와 제2 텍스트 분석 데이터를 생성하는 단계; 및 상기 제1 사용자에게 상기 제2 음성 데이터와 상기 제2 텍스트 분석 데이터를 공유하는 단계를 포함한다. 본 발명의 몇몇 실시예에 따른 컴퓨터 프로그램은 하드웨어와 결합되어 상기 사용자의 감정 추적 및 관리 방법 을 실행시키기 위해 매체에 저장된다. 본 발명의 몇몇 실시예에 따른 서버는 제1 사용자의 음성에 기초하여 생성된 제1 음성 데이터를 제1 사용자 단 말로부터 수집하는 통신부; 상기 제1 음성 데이터에 대한 전처리를 수행하여 제1 전처리 음성 데이터를 생성하 고, 상기 제1 전처리 음성 데이터에 기초하여 상기 제1 사용자의 음성에 대한 제1 텍스트 분석 데이터를 생성하 며, 상기 제1 전처리 음성 데이터에 기초하여 상기 제1 사용자의 제1 감정 데이터를 생성하는 데이터 처리부; 및 상기 제1 감정 데이터와 상기 제1 텍스트 분석 데이터가 매칭되어 기록되는 데이터 저장부를 포함하되, 상기 데이터 처리부는 상기 제1 감정 데이터와 상기 제1 텍스트 분석 데이터를 함께 조회할 수 있는 사용자 환경을 상기 제1 사용자 단말에 제공하도록 더 구성된다."}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법 및 시스템은 사용자에게 질문 콘텐츠를 제공하여 사용자의 발화를 유도하며, 음성 인식을 통해 사용자의 발화를 용이하게 기록하고, 사용자가 발화한 음성과 기록된 사용자의 발화 내용을 기초로 사용자의 감정을 분석하여 사용자의 감정 상태를 추적, 관 리하는 것을 지원할 수 있다. 또한, 본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법 및 시스템은 사용자와 관 계된 다른 사용자의 감정 데이터를 상호 간에 교환하여 공유 감정 데이터를 도출하여 함께 제공함으로써, 관계 된 사용자들 사이의 감정 교환 및 감정 공유가 수행되는 것을 지원할 수 있다. 또한, 본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법 및 시스템은 수집된 음성 데이터, 이를 기초로 분석된 감정 데이터 및 텍스트 분석 데이터를 함께 제공하는 사용자 환경을 통해 사용자가 본인의 음성 데이터에 기반한 감정, 발화 내용의 키워드 및 핵심을 확인하는 것을 지원할 수 있다. 또한, 본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법 및 시스템은 수집된 복수 의 데이터들을 기간 또는 그동안의 이력을 통해 분석한 통계 정보를 제공하여, 사용자가 본인의 감정 변화 및 관계된 사용자의 감정 변화를 확인하는 것을 더욱 지원할 수 있다. 상술한 내용과 더불어 본 발명의 구체적인 효과는 이하"}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서 및 특허청구범위에서 사용된 용어나 단어는 일반적이거나 사전적인 의미로 한정하여 해석되어서는 아 니된다. 발명자가 그 자신의 발명을 최선의 방법으로 설명하기 위해 용어나 단어의 개념을 정의할 수 있다는 원 칙에 따라, 본 발명의 기술적 사상과 부합하는 의미와 개념으로 해석되어야 한다. 또한, 본 명세서에 기재된 실 시예와 도면에 도시된 구성은 본 발명이 실현되는 하나의 실시예에 불과하고, 본 발명의 기술적 사상을 전부 대 변하는 것이 아니므로, 본 출원시점에 있어서 이들을 대체할 수 있는 다양한 균등물과 변형 및 응용 가능한 예 들이 있을 수 있음을 이해하여야 한다. 본 명세서 및 특허청구범위에서 사용된 제1, 제2, A, B 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성 요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. '및/또는' 이라는 용어는 복수의 관련된 기재된 항목들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함 한다. 본 명세서 및 특허청구범위에서 사용된 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서 \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해서 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의 미를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다. 또한, 본 발명의 각 실시예에 포함된 각 구성, 과정, 공정 또는 방법 등은 기술적으로 상호 간 모순되지 않는 범위 내에서 공유될 수 있다. 이하, 도 1 내지 도 14를 참조하여, 본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 시스템 및 방법을 설명한다.도 1은 본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 시스템을 설명하기 위한 개념 도이다. 도 2는 본 발명의 실시예에 따른 사용자 환경이 제공하는 주요 서비스 기능을 설명하기 위한 예시도이 다. 도 1을 참조하면, 인공지능 기술에 기반한 감정 추적 및 관리 시스템은 서버, 제1 사용자 장치 및 제2 사용자 장치를 포함한다. 서버, 제1 사용자 장치 및 제2 사용자 장치은 네트워크를 통해 서로 데이터를 교환하도록 구성 된다. 여기서, 네트워크는 유선 인터넷 기술, 무선 인터넷 기술 및 근거리 통신 기술에 의한 네트워크를 포함할 수 있다. 유선 인터넷 기술은 예를 들어, 근거리 통신망(LAN, Local area network) 및 광역 통신망(WAN, wide area network) 중 적어도 하나를 포함할 수 있다. 무선 인터넷 기술은 예를 들어, 무선랜(Wireless LAN: WLAN), DLNA(Digital Living Network Alliance), 와이브 로(Wireless Broadband: Wibro), 와이맥스(World Interoperability for Microwave Access: Wimax), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), IEEE 802.16, 롱 텀 에볼루션(Long Term Evolution: LTE), LTE-A(Long Term Evolution-Advanced), 광대역 무선 이동 통신 서비스 (Wireless Mobile Broadband Service: WMBS) 및 5G NR(New Radio) 기술 중 적어도 하나를 포함할 수 있다. 단, 본 실시예가 이에 제한되는 것은 아니다. 근거리 통신 기술은 예를 들어, 블루투스(Bluetooth), RFID(Radio Frequency Identification), 적외선 통신 (Infrared Data Association: IrDA), UWB(Ultra-Wideband), 지그비(ZigBee), 인접 자장 통신(Near Field Communication: NFC), 초음파 통신(Ultra Sound Communication: USC), 가시광 통신(Visible Light Communication: VLC), 와이 파이(Wi-Fi), 와이 파이 다이렉트(Wi-Fi Direct), 5G NR (New Radio) 중 적어도 하 나를 포함할 수 있다. 단, 본 실시예가 이에 제한되는 것은 아니다. 서버는 복수의 사용자의 감정을 추적하고, 복수의 사용자들이 추적된 감정을 공유하며, 관리할 수 있는 사 용자 환경을 제공할 수 있다. 도 1의 예시에서, 제1 사용자, 제2 사용자만이 도시되어 있으나, 본 발명의 실시 예가 이에 한정되는 것은 아니다. 후술하는 설명에서는 제1 사용자를 기준으로 주요 단계가 수행되고, 제2 사용 자와의 상호 작용에 대해 서술하게 되며, 제1 사용자에 대한 설명 및 특징은 다른 사용자들에게도 동일하게 적 용될 수 있다. 서버는 사용자의 감정을 추적하고, 추적된 감정을 공유하며, 관리할 수 있는 사용자 환경을 제1 사용자 장 치과 제2 사용자 장치에 각각 제공할 수 있다. 제1 사용자 장치은 제1 사용자의 단말에 해당한 다. 제2 사용자 장치은 제2 사용자의 단말로, 제2 사용자는 제1 사용자과 관계된 사용자에 해당한다. 제1 사용자 및 제2 사용자는 각각 제1 사용자 장치과 제2 사용자 장치을 통해 서버가 제공하는 사용자 환경에 접속할 수 있다. 제1, 제2 사용자 장치(110, 120)은 휴대용 단말기의 일종인 스마트폰(smart phone)일 수 있으나, 본 발명이 이에 제한되는 것은 아니다. 제1, 제2 사용자 장치(110, 120)은 퍼스널 컴퓨터 (PC), 노트북, 태블릿, 휴대폰, 스마트폰, 웨어러블 디바이스(예를 들어, 워치형 단말기) 등의 다양한 형태의 전자 장치를 포함할 수 있다. 서버가 제공하는 상술한 서비스는 제1, 제2 사용자 장치(110, 120)에 설치되어 있는 어플리케이션을 통해 제공될 수 있다. 여기서, 어플리케이션(application)은 응용 프로그램을 의미하며, 예를 들어, 모바일 단말(스 마트 폰)에서 실행되는 앱(app)을 포함할 수 있다. 다만 이에 한정되는 것은 아니며, 서버는 웹 문서를 통 한 데이터 전송 방식으로 제1, 제2 사용자 장치(110, 120)과 데이터를 주고받을 수 있다. 이때 웹 문서란, 그 종류나 형식을 한정하지 않고 소정의 네트워크 주소를 갖는 웹 사이트(web site)를 통하여 전송됨으로써 열람 및/또는 수정이 가능하도록 제공되는 임의의 데이터를 지칭한다. 이 경우, 서버는 소정의 URL(Uniform Resource Locator)을 갖는 웹 페이지를 제공하는 웹 서버(web server)일 수 있으며, 제1, 제2 사용자 장치 (110, 120)에서는 웹 브라우저(web browser)를 이용하여 해당 웹 페이지에 접속함으로써 서버가 제공하는 서비스를 이용할 수 있다. 웹 문서의 형식은 HTML(Hyper Text Markup Language), XML(Extensible Markup Language), JSON(JavaScript Object Notation), 또는 다른 상이한 언어에 기반한 형식일 수 있으며 특정 형식 으로 한정되지 않는다. 제1, 제2 사용자는 각각 제1, 제2 사용자 장치(110, 120)을 통해 상기 어플리케이션(App) 또는 웹 환경에 접속 하여 서버가 제공하는 서비스를 이용할 수 있다. 제1 사용자 장치과 제2 사용자 장치은 각각 사용자 환경을 출력하는 출력부, 대응되는 사용자의 입력 을 수신하는 입력부, 상기 사용자의 음성을 녹음하는 녹음부, 서버와 네트워크를 통한 데이터 교환을 위한 통신부, 사용자 장치의 전반적인 동작을 제어하는 제어부 및 상기 제어부의 동작에 따라 생성되거나 동작에 필 요한 데이터를 저장하는 저장부를 포함할 수 있다. 여기서, 출력부는 사용자 환경과 관련된 비디오 신호를 출력하는 표시부와 오디오 신호를 출력하는 음향부를 포 함할 수 있다. 녹음부는 마이크로폰을 포함하고, 마이크로폰을 통해 획득되는 음성 데이터를 처리할 수 있다. 입력부는 사용자의 입력을 수신할 수 있다. 입력부는 사용자의 조작을 수신하는 장치로, 마우스, 키보드일 수 있으나, 이에 한정되는 것은 아니며 표시부 상에 배치되는 터치 패널 등일 수 있다. 도 2a를 참조하면, 제1 사용자 장치과 제2 사용자 장치을 통해 각각 제1 사용자 음성과 제2 사용자 음성이 수집될 수 있다. 이러한, 제1 사용자 음성은 서버에서 제1 사용자 장치로 제공된 제1 질문 콘 텐츠에 대응한 제1 사용자의 답변일 수 있다. 또한, 제2 사용자 음성은 서버에서 제2 사용자 장치로 제공된 제2 질문 콘텐츠에 대응한 제2 사용자의 답변일 수 있다. 이러한, 질문 콘텐츠는 사용자의 음성을 취득 하기 위한 수단으로, 서버에서 임의적인 방식으로 선택되고, 미리 설정된 시각에 대응하여 사용자 장치에 제공될 수 있으나, 이에 한정되는 것은 아니다. 서로 관계된 사용자들은 감정을 교류하고, 확인하기 위해 서로 에게 질문 콘텐츠를 서버를 통해 전달할 수도 있다. 서버는 제1 사용자의 음성에 기초하여 제1 감정 데이터와 제1 텍스트 분석 데이터를 각각 생성할 수 있다. 또한, 서버는 제2 사용자의 음성에 기초하여 제2 감정 데이터와 제2 텍스트 분석 데이터를 각각 생성할 수 있다. 제1 감정 데이터는 제1 사용자의 음성에 기초하여 제1 사용자의 감정을 분석한 데이터일 수 있으며, 제1 텍스트 분석 데이터는 제1 사용자의 음성을 인식하고, 인식된 음성에서 주요 정보를 추출하여 생성한 텍스트 분 석 데이터를 의미한다. 제2 감정 데이터는 제2 사용자의 음성에 기초하여 제2 사용자의 감정을 분석한 데이터일 수 있으며, 제2 텍스트 분석 데이터는 제2 사용자의 음성을 인식하고, 인식된 음성에서 주요 정보 또는 중요 키 워드를 추출하여 생성한 텍스트 분석 데이터를 의미한다. 도 2b를 참조하면, 서버는 제1 감정 데이터와 제1 텍스트 분석 데이터를 제1 사용자 장치에 제공하고, 제2 감정 데이터와 제2 텍스트 분석 데이터를 제2 사용자 장치에 제공할 수 있다. 즉, 서버 는 생성된 제1 감정 데이터, 제1 텍스트 분석 데이터, 제2 감정 데이터 및 제2 텍스트 분석 데이터를 사용 자 환경에 제공하여 제1 사용자와 제2 사용자가 자신의 감정과 자신이 발언한 내용을 확인하고 관리하는 것을 지원할 수 있다. 여기서, 서버는 특정 사용자에게 감정 데이터와 텍스트 분석 데이터를 제공할 때, 관계된 다른 사용자에게 도 특정 사용자의 감정 데이터와 텍스트 분석 데이터를 제공하여, 관계된 사용자들 사이의 감정 변화를 공유할 수 있는 기능을 제공할 수 있다. 즉, 관계된 사용자들 사이에 감정 변화의 공유, 상호간의 감정의 확인, 공감대 형성 등의 감정에 기반한 상호 작용이 형성되는 것을 지원할 수 있다. 도 2c를 참조하면, 제1 사용자와 제2 사용자가 미리 관계가 설정된 상태인 경우, 제1 사용자에게는 제1 사용자 의 제1 감정 데이터와 제2 사용자의 제2 감정 데이터가 제공되고, 제2 사용자에게도 제2 사용자의 제2 감정 데 이터와 제1 사용자의 제1 감정 데이터가 제공될 수 있다. 또한, 제1 사용자에게는 제1 사용자의 제1 텍스트 분 석 데이터와 제2 사용자의 제2 텍스트 분석 데이터가 제공되고, 제2 사용자에게도 제2 사용자의 제2 텍스트 분 석 데이터와 제1 사용자의 제1 텍스트 분석 데이터가 제공될 수 있다. 이하, 도 3 내지 도 14를 참조하여, 서버에서 수행되는 사용자의 감정 추적 및 관리 방법에 대해 더욱 상세히 설명하도록 한다. 도 3은 본 발명의 몇몇 실시예에 따른 사용자의 감정 추적 및 관리 방법의 순서도이다. 도 4는 제1 사용자의 제1 음성 데이터를 수집하는 과정을 예시적으로 도시한다. 도 5a 및 도 5b는 제1 사용자 장치를 통해 질문 콘텐츠가 제공되는 사용자 환경의 인터페이스를 나타낸다. 도 6은 제1 전처리 음성 데이터를 생성하는 과정을 설명하기 위한 예시도이다. 도 7은 제1 텍스트 분석 데이터를 생성하는 과정을 설명하기 위한 예시도이다. 도 8은 제1 감정 데이터를 생성하는 과정을 설명하기 위한 예시도이다. 도 9는 제3 감정 분석 모델에서 제1 감정 데이터를 생성하는 과정을 설명하기 위한 예시도이다. 도 10은 음성 데이터와 감정 데이터를 생성하여 기록하는 과정을 설명하기 위한 예시도이다. 도 11은 사용자 환경에서 음성 데이터와 감정 데이터가 함께 제공되는 예시적인 화면을 도시한다. 도 12는 사용자 환경에서 음성 데이터와 감정 데이터가 기록된 리스트를 확인할 수 있는 예시적인 화면을 도시 한다. 도 13 및 도 14는 사용자 환경에서 통계 정보를 제공하는 예시적인 화면을 도시한다. 본 발명의 몇몇 실시예에 따른 사용자의 감정 추적 및 관리 방법은 서버에서 수행될 수 있다. 서버는 하드웨어적으로는 통상적인 웹 서버와 동일한 구성을 가지며, 소프트웨어적으로는 C, C++, Java, Visual Basic, Visual C 등과 같은 다양한 형태의 언어를 통해 구현되어 여러 가지 기능을 하는 프로그램 모듈을 포함한다. 또 한, 일반적인 서버용 하드웨어에 도스(Dos), 윈도우(Window), 리눅스(Linux), 유닉스(Unix), 매킨토시 (Macintosh) 등의 운영 체제에 따라 다양하게 제공되고 있는 웹 서버 프로그램을 이용하여 구현될 수 있으며, 대표적인 것으로는 윈도우 환경에서 사용되는 웹사이트(website), IIS(Internet Information Server)와 유닉스 환경에서 사용되는 CERN, NCSA, APPACH 등이 이용될 수 있다. 도 3을 참조하면, 본 발명의 몇몇 실시예에 따른 사용자의 감정 추적 및 관리 방법은 제1 사용자의 음성에 기초 하여 생성된 제1 음성 데이터를 수집하는 음성 수집 단계(S100); 제1 음성 데이터에 대한 데이터 처리를 수행하 여 제1 전처리 음성 데이터를 생성하는 단계(S110); 상기 제1 전처리 음성 데이터에 기초하여 상기 제1 사용자 의 제1 텍스트 분석 데이터를 생성하는 단계(S120); 상기 제1 전처리 음성 데이터 및 상기 텍스트 분석 데이터 중 적어도 하나에 기초하여 상기 제1 사용자의 제1 감정 데이터를 생성하는 단계(S130); 상기 제1 감정 데이터 와 상기 제1 텍스트 분석 데이터를 기록하는 기록 단계(S140); 및 상기 제1 감정 데이터와 상기 제1 텍스트 분 석 데이터를 사용자 환경을 통해 함께 제공하는 단계(S150)를 포함한다. 먼저, 제1 사용자의 음성에 기초하여 생성된 음성 데이터가 수집된다(S100). 제1 사용자의 음성은 제1 사용자의 제1 사용자 장치를 통해 녹음될 수 있다. 제1 사용자 장치는 녹음 부를 통해 제1 사용자의 음성을 녹음할 수 있으며, 이에 대응하는 음성 데이터를 생성할 수 있다. 단계(S100)에 서, 서버는 제1 사용자 장치에서 생성된 음성 데이터를 네트워크를 통해 수신할 수 있다. 몇몇 실시예에서, 제1 사용자 장치는 제1 사용자가 발화하는 상태를 인식하여 제1 사용자의 음성을 녹음하 도록 제어될 수 있다. 즉, 제1 사용자 장치는 제1 사용자의 발화 여부를 센싱하고, 제1 사용자가 발화 상 태로 판단되는 경우, 녹음부를 활성화하여 제1 사용자의 음성을 녹음하여 음성 데이터를 생성할 수 있다. 제1 사용자는 제1 사용자 장치를 이용하여 자신의 음성을 녹음하여 음성 데이터를 생성할 수 있다. 이러한, 음성 데이터는 서버로 제공되어 사용자 계정별로 저장될 수 있다. 음성 데이터는 적어도 생성일이 기록될 수 있으며, 서버는 음성 데이터에 대한 제목을 입력할 수 있는 기능을 사용자에게 제공하여 음성 데이터에 대한 관리가 용이하게 수행되는 것을 지원할 수 있다. 즉, 제1 사용자는 일상 생활을 수행하는 과정에 서 기록하여야 하는 정보가 생기거나, 기록하고 싶은 감정 변화가 발생하는 경우, 제1 사용자 장치를 통해 이러한 사항을 음성 데이터로 저장할 수 있다. 즉, 음성 데이터는 음성 메모와 같은 기능을 사용자에게 제공할 수 있다. 단계(S100)에서, 사용자들은 아직 음성 인식 기술이나 자발적인 녹음 서비스에 대한 경험이 많지 않을 수 있다. 따라서, 서버는 음성 인식 서비스에 대한 친근감을 높이고, 음성 인식률을 높이기 위한 충분한 길이의 대 답을 유도하기 위한 수단으로 질문 콘텐츠를 제공할 수 있다. 즉, 이러한 질문 콘텐츠는 음성 인식을 유도하는 도구로서 제1 사용자의 솔직한 답변과 음성 인식과 감정 분석을 위한 충분한 길이의 답변을 유도할 수 있으며, 제1 음성 데이터에 따른 제1 사용자의 감정 분석에 더욱 정확한 결과가 도출되는 것을 지원할 수 있다. 몇몇 실시예에서, 서버는 제1 사용자의 발화를 유도하기 위한 질문 콘텐츠를 제1 사용자 장치에서 출 력되도록 제어할 수 있다. 도 4a를 참조하면, 서버는 제1 사용자 장치에 질문 콘텐츠를 전달하며, 제1 사용자 장치는 질문 콘텐츠를 출력부를 통해 출력할 수 있다. 사용자 장치는 출력부의 표시부를 통해 질문 콘텐츠의 내용, 컨 텍스트를 표시하고, 출력부의 음향부를 통해 질문 콘텐츠의 내용을 오디오 신호로 제1 사용자에게 제공할 수 있 다. 제1 사용자는 이러한 질문에 대응하여 답변을 할 수 있으며, 제1 사용자 장치는 제1 사용자의 답변을녹음하여 제1 음성 데이터를 생성할 수 있다. 몇몇 실시예에서, 서버는 복수의 질문 콘텐츠는 포함할 수 있으며, 랜덤 방식으로 복수의 질문 콘텐츠 중 어느 하나를 선택하여 제1 사용자 장치에 제공할 수 있다. 몇몇 실시예에서, 서버는 객관적 정보를 기록할 수 있는 복수의 제1 유형의 질문 콘텐츠와 주관적 정보를 기록할 수 있는 복수의 제2 유형의 질문 콘텐츠를 포함할 수 있다. 서버는 복수의 제1 유형의 질문 콘텐츠 중 어느 하나와 복수의 제2 유형의 질문 콘텐츠 중 어느 하나를 제1 사용자 장치에 제공할 수 있다. 복수의 제1 유형의 질문 콘텐츠는 소개(사용자를 소개할 수 있는 질문), 취미(영화, 음악, 독서, 운동 등 다양 한 취미에 대한 질문), 직업(학업, 하는 일에 대한 질문) 및 기억(과거 에피소드나 추억 등에 대한 질문)을 포 함할 수 있으며, 사용자는 객관적인 정보를 전달하기 위한 답변을 발화하게 된다. 복수의 제2 유형의 질문 콘텐츠는 감정(현재의 긍정적 감정, 부정적 감정에 대한 질문), 생각(개인의 선호도, 가치관 등 깊이 생각해 볼 수 있는 질문), 상상(상상하며 아이디어를 필요로 하는 질문), 사랑(연애관, 사랑 등 에 대한 질문)를 포함할 수 있다. 사용자는 주관적인 정보, 생각을 전달하기 위한 답변을 발화하게 된다. 서버는 이와 같이, 사용자의 객관적인 정보뿐만 아니라 주관적인 정보, 생각을 획득, 유도할 수 있는 질문 콘텐츠를 사용자에게 제공할 수 있으며, 더욱 다양한 정보에 기초하여 사용자의 감정을 분석하는 것이 지원될 수 있다. 서버는 미리 설정된 시각에 대응하여 질문 콘텐츠를 제공하도록 구성될 수 있다. 질문 콘텐츠가 제공되는 시각은 사용자에 의해 설정될 수 있으나, 이에 한정되는 것은 아니다. 예시적으로, 서버는 하루에 두 번 서로 상이한 종류의 질문 콘텐츠를 사용자에게 제공하도록 구성될 수 있다. 예시적으로, 질문 콘텐츠가 제공되 는 시각은 오전 시간대에 한 번, 오후 시간대에 한 번으로 각각 설정될 수 있다. 오전 시간대는 사용자 자신의 의견이나 객관적 정보를 기록할 수 있는 제1 유형의 질문 콘텐츠가 제공될 수 있으며, 오후 시간대는 사용자 자 신의 감정이나 주관적 정보를 정리할 수 있는 제2 유형의 질문 콘텐츠가 제공될 수 있다. 일반적으로, 사용자의 감정은 오전에서 오후로 갈수록 이성적인 감정에서 감성적인 감정으로 변화될 수 있다. 이러한, 감정 변화를 반 영하여 서버는 오전 시간대에 제1 유형의 질문 콘텐츠를 제공하고, 오후 시간대에 제2 유형의 질문 콘텐츠 를 제공하여 사용자의 감정을 더욱 정확하게 분석, 판단하는 것을 더욱 지원할 수 있다. 또한, 서버가 제공하는 사용자 환경은 제1 사용자가 질문 콘텐츠를 구성하는 것을 지원할 수 있다. 제1 사 용자는 사용자 장치에서 제공하는 사용자 환경의 인터페이스를 이용하여 질문 콘텐츠를 입력할 수 있다. 사용자는 텍스트 형태로 질문을 작성하거나, 목소리를 녹음하여 질문 콘텐츠를 생성할 수 있다. 제1 사용자는 녹음을 하려는 질문 콘텐츠의 카테고리를 설정(예를 들어, 소개, 취미, 직업, 기억, 감정, 상상, 사랑 중 어느 하나)할 수 있으며, 설정된 카테고리에 대응하는 질문을 녹음하여 질문 콘텐츠를 직접 생성할 수 있다. 제1 사 용자가 생성한 질문 콘텐츠는 서버에 등록될 수 있다. 이러한, 사용자가 생성한 질문 콘텐츠는 단순 기록 용도에 그치지 않고, 추후 다른 사용자에게 질문 콘텐츠로 제공되도록 활용될 수 있다. 또한, 사용자는 질문 콘 텐츠를 생성한 이후, 관계된 사용자에게 질문 콘텐츠를 제공하고, 이에 대한 답변을 수신하여 더욱 다양한 내용 을 함께 기록하는 것이 지원될 수 있다. 예시적으로, 도 4b를 참조하면, 제1 사용자는 질문 콘텐츠를 생성하여 관계된 제2 사용자에게 생성한 질문 콘텐 츠를 제공할 수 있다. 즉, 몇몇 실시예에에 따른 서버에서 수행되는 복수의 사용자의 감정 추적, 관리 및 공유 하는 방법은 제1 사용자의 제1 음성 데이터를 분석하여 제1 음성 데이터와 제1 텍스트 분석 데이터를 생성하는 단계; 상기 복수의 사용자 중 상기 제1 사용자와 관계된 적어도 한명의 제2 사용자에게 질문 콘텐츠를 제공하는 단계; 상기 질문 콘텐츠에 대응하여 생성된 제2 사용자의 제2 음성 데이터를 수신하는 단계; 상기 제2 음성 데 이터를 분석하여 제2 음성 데이터와 제2 텍스트 분석 데이터를 생성하는 단계; 및 상기 제1 사용자에게 상기 제 2 음성 데이터와 상기 제2 텍스트 분석 데이터를 공유하는 단계를 포함할 수 있다. 본 발명의 실시예에서, 관계된 제1 사용자와 제2 사용자는 상호 간에 질문 콘텐츠를 제공할 수 있으며, 상호 간 에 질문 콘텐츠에 대한 답변을 유도하여 감정 데이터 및 텍스트 분석 데이터가 생성되는 것을 유도할 수 있다. 또한, 자신이 제공한 질문에 대한 상대방의 답변을 확인하고, 이에 따른 감정 데이터까지 확인할 수 있으며, 상 호 간에 감정의 교환, 공유가 수행되는 것이 더욱 지원될 수 있다. 도 5a를 참조하면, 제1 사용자 장치를 통해 제공되는 사용자 환경의 인터페이스를 통해 질문 콘텐츠가 제공된다. 테이프(I1)는 질문 콘텐츠 또는 사용자가 직접 음성 데이터를 녹음하기 위한 기능을 제공한다. 사용 자 환경에서 음성 데이터는 테이프로 명칭될 수 있다. 리스트(I2)는 사용자가 녹음을 한 음성 데이터와 이와 관련된 정보를 확인할 수 있는 기능을 제공한다. 대시 보드(I3)은 사용자의 음성 데이터에 따라 분석된 정보들을 통계적으로 확인할 수 있는 기능을 제공한다. 내 정보(I4)는 사용자와 관련된 정보를 입력하거나 수정하는 기능 을 제공할 수 있다. 테이프(I1)는 질문 콘텐츠를 출력하고, 이에 대응한 제1 사용자의 답변을 녹음하여 음성 데이터를 생성하는 것 을 지원할 수 있다. 제1 사용자는 질문 콘텐츠를 사용자 환경의 테이프(I1)를 통해 확인할 수 있다. 제1 사용자 가 테이프(I1)를 선택하는 경우, 관련된 세부 인터페이스들이 제공되게 된다. 도 5a의 질문 인터페이스(I5)에는 \"내가 어른이 됐다고 느낄 때는?\"이라는 질문 콘텐츠가 제공되는 것을 알 수 있다. 또한, 질문 인터페이스(I5)는 해당 질문 콘텐츠가 어떠한 유형(예를 들어, 취미)에 해당하는 지에 대한 정보를 출력할 수 있다. 사용자는 테이프 녹음하기(I6)를 선택하여 질문 콘텐츠에 대한 답변을 녹음할 수 있다. 테이프 녹음하기(I6)를 선택하는 사용자의 입력에 대응하여 녹음부가 활성화되면서, 도 5b와 같은 녹음 인터페 이스(I7)가 출력되게 된다. 녹음 인터페이스(I7)는 질문 콘텐츠를 질문 인터페이스(I5)에 표시하며, 이에 대응 하는 녹음이 진행되는 상황을 카세트 테이프의 동작과 녹음 시간이 변해 가는 것으로 표시할 수 있다. 제1 사용 자는 녹음 인터페이스(I7)를 이용하여 질문 콘텐츠에 대한 답변을 녹음하여 제1 음성 데이터를 생성하게 되며, 이러한 제1 사용자의 제1 음성 데이터는 제1 사용자 장치을 통해 서버로 제공되게 된다. 다음으로, 제1 음성 데이터에 대한 데이터 처리를 수행하여 제1 전처리 음성 데이터를 생성한다(S110). 단계(S110)에서, 서버는 수집된 제1 음성 데이터에 대한 전처리를 수행하여, 후속 단계(S110)에서 제1 음 성 데이터를 활용하는 것이 용이해지도록 처리할 수 있다. 도 6을 참조하면, 단계(S110)는, 제1 음성 데이터에서 음성 이외의 소리(노이즈, 배경음, 배경음악 등)을 제거 하여 제1 음성 분리 데이터를 생성하는 음성 분리 단계(S112) 및 제1 음성 분리 데이터에서 음성 구간과 묵음 구간을 식별하는 음성 구간 분리 단계(S114)를 포함할 수 있다. 음성 분리 단계(S112)는 제1 음성 데이터에 대해 음성만을 분리하는(Voice separation)를 수행하는 단계에 해당 한다. 음성을 녹음할 때, 배경음(음악 등), 배경 잡음, 사람들이 웅성거리는 소리 등 다양한 소리들이 동시에 녹음이 될 수 있다. 서버는 제1 음성 데이터에서 배경음, 배경 잡음 등과 같은 음성 이외의 소리를 제거하 는 전처리를 수행할 수 있다. 몇몇 실시예에서, 서버는 딥러닝 기반의 음성 분리 모델(VM1)을 포함할 수 있다. 음성 분리 모델(VM1)을 통해 입력된 음원에서 음성만을 추출할 수 있다. 음성 분리 모델은 입력층, 출력층, 그 리고 상기 입력층과 출력층 사이에 적어도 하나 이상의 중간층(또는 은닉층, hidden layer)을 포함하는 계층 구 조로 구성된 딥러닝 알고리즘에 기반하여 구성될 수 있다. 딥러닝 알고리즘은, 이와 같은 다중 계층 구조에 기 반하여, 층간 활성화 함수(activation function)의 가중치를 최적화 (optimization)하는 학습을 통해 결과적으 로 신뢰성 높은 결과를 도출할 수 있다. 본 발명에 적용 가능한 딥러닝 알고리즘은, 트랜스포머(Transformer), 심층 신경망 (deep neural network; DNN), 합성곱 신경망 (convolutional neural network; CNN), 순환 신경망 (recurrent neural network; RNN), 장단기 메모리(Long Short Term Memory; LSTM), 양방향 장단기 메모리(Bidirectional Short Term Memory; BLSTM 등을 포함할 수 있다. 여기서, 음성 분리에 효과적인 End-to-End 방식의 Transformer를 기초로 음성 분리 모델을 구현할 수 있으나, 이에 한정되는 것은 아니다. 서버는 상술한 딥러닝 알고리즘을 통해 효과적으로 음성 데이터에서 배경음, 배경 잡음 등과 같은 음성 이외의 소리를 필터링할 수 있으며, 음성만을 분리 추출하 는 전처리를 수행할 수 있다. 음성 구간 분리 단계(S114)는 단계(S112)에서 전처리된 음성 데이터, 제1 음성 분리 데이터를 분석하여 음성 구 간과 묵음 구간을 구분하는 단계에 해당한다. 제1 사용자 장치를 통해 발화하는 제1 사용자는 계속해서 발 성하는 것이 아니라, 생각을 하거나 다른 일을 통해 잠시 발성이 중단될 수 있다. 즉, 음성 데이터에는 사용자 의 음성이 나타나지 않는 묵음 구간이 포함될 수 있다. 서버는 딥러닝 기반의 미리 학습된 음성 구간 검출 모델(VM2)을 포함할 수 있다. 음성 구간 검출 모델(VM2)은 음성 데이터에서 음성이 나타나지 않는 구간을 묵음 구간으로 판단할 수 있다. 즉, 음성 구간 검출 모델(VM2)은 사용자의 목소리가 나타나지 않는 구간을 묵음 구간 으로 판단하고, 사용자의 목소리가 나타나는 구간을 음성 구간으로 판단할 수 있으며, 입력된 음성 데이터에서 음성 구간과 묵음 구간을 구분할 수 있다. 즉, 음성 이외의 소리가 필터링된 제1 음성 분리 데이터는 단계(S114)를 수행함에 따라, 음성 구간과 묵음 구간이 구분되어 검출되게 된다. 단계(S112)와 단계(S114)를 순차적으로 수행되어 생성되는 제1 전처리 음성 데이터는 배경음, 배경 잡음 등이 필터링된 상태에, 음성 구간과 묵음 구간이 구분되도록 전처리된 상태에 해당한다. 다음으로, 제1 전처리 음성 데이터에 기초하여 제1 사용자의 제1 텍스트 분석 데이터를 생성한다(S120). 도 7을 참조하면, 단계(S120)는, 제1 전처리 음성 데이터에서 묵음 구간을 제외한 제1 음성 인식 입력 데이터를 생성하는 단계(S122), 제1 음성 인식 입력 데이터에 대한 제1 음성 인식 데이터를 생성하는 단계(S124) 및 제1 음성 인식 데이터를 분석하여 제1 텍스트 분석 데이터를 생성하는 단계(S126)를 포함한다. 단계(S122)에서, 제1 전처리 음성 데이터에 대응하는 제1 음성 인식 데이터가 생성되게 된다. 서버는 제1 전처리 음성 데이터에서 묵음 구간을 제외하고 음성 구간만을 취득, 활용하여 제1 음성 인식 입력 데이터를 구 성할 수 있다. 즉, 음성 인식을 위해서는 사용자의 목소리가 나타나는 음성 구간만이 필요하며, 묵음 구간은 제 1 음성 인식 데이터의 생성에 있어 불필요한 구간일 수 있다. 단계(S124)에서, 생성된 제1 음성 인식 입력 데이터는 음성 인식 모델(VM3)에 입력될 수 있으며, 음성 인식 모 델(VM3)은 제1 음성 인식 입력 데이터를 분석하여 제1 음성 인식 데이터를 생성할 수 있다. 실시예에서, 제1 음 성 인식 데이터는 음성 인식 모델(VM3)을 사용하여 획득된 결과물에 해당한다. 구체적으로, 제1 음성 인식 데이 터는 인식 가능한 모든 텍스트를 포함한 단어 격자(word lattice) 형태의 결과물일 수 있으며, 토큰, 단어별 시 간 정보의 신뢰도 값이 획득될 수 있으므로, 후술하는 텍스트 분석 데이터를 생성하는 분석 과정이 용이해질 수 있다. 몇몇 실시예에서, 음성 인식 모델(VM3)은 Transformer에 기반한 End-to-End, BLSTM(Bidirectional LSTM) 또는 합성곱 뉴럴 네트워크(convolutional neural network; CNN)로 구현될 수 있으나, 본 발명의 실시예가 이에 한 정되는 것은 아니다. 실시예에서, 음성 인식 모델(VM3)은 입력된 제1 음성 인식 입력 데이터의 음성으로부터 특징을 추출하는 특징 추출 모델(VM3_1)과, 추출된 특징에 기반하여 제1 음성 인식 데이터를 생성하는 음성 인식 데이터 생성 모델 (VM3_2)을 포함할 수 있다. 특징 추출 모델(VM3_1)은 음성으로부터 스펙트로그램(spectrogram)을 획득하고, 스펙트로그램으로부터 음성의 주파수 특징을 추출할 수 있다. 특징 추출 모델(VM3_1)은 음성을 푸리에 변환(Fourier transform)을 이용한 스 펙트럼 분석 결과를 시간-스펙트럼으로 표현하면 스펙트로그램을 획득할 수 있다. 특징 추출 모델(VM3_1)은 푸 리에 변환을 통해 음성의 기본 주파수(fundamental frequency)를 계산하고, 주파수 값을 x 축에, 진폭 값을 y 축에 나타내 스펙트럼을 생성할 수 있다. 스펙트로그램은 생성된 스펙트럼을 시간에 따라 스펙크럼을 연속적으 로 표시한 것이다. 특징 추출 모델(VM3_1)은 스펙트로그램에 나타난 주파수 특징 정보를 음성의 특징으로서 추출할 수 있다. 다만, 음성으로부터 특징을 추출하는 방법은 위 실시예에 한정되지 아니하며, 음성의 특징을 추출하기 위해 다 양한 방법이 이용될 수 있다. 예를 들어, 특징 추출 모델(VM3_1)은 스펙트로그램을 추출하여 바로 음성인식에 사용할 수 있으며, 필터 뱅크(Filter Bank) 특징 값을 이용할 수도 있다. 음성을 음성 프레임 단위로 분할하 고, FFT(Fast Fourier Transform)을 이용하여 음성 프레임을 주파수 영역으로 변환하며, 필터 뱅크(Filter bank)를 이용하여 음성 프레임을 여러 주파수 대역의 신호로 나누고, 각 주파수 대역의 신호의 에너지를 계산 을 통해 획득될 수 있다. 음성 인식 데이터 생성 모델(VM3_2)은 음성으로부터 추출된 특징에 기초하여 음성을 자소(grapheme) 단위로 인 식하여 제1 음성 인식 입력 데이터에 대응하는 제1 음성 인식 데이터를 생성할 수 있다. 음성 인식 데이터 생성 모델(VM3_2)은 자소 단위의 인식 결과에 기초하여 음성이 나타내는 단어들을 추정하도록 학습된 모델일 수 있다. 예시적으로, 음성 인식 데이터 생성 모델(VM3_2)은 특징 추출 모델(VM3_1)에서 추출된 특징을 인공 신경 망 기반의 딥러닝 모델을 통해 음성 인식 텍스트로 변환할 수 있다. 실시예에서, 음성 인식 데이터 생성 모델 (VM3_2)은 발음 사전을 포함하지 않아 하나의 언어로 음성 인식 텍스트를 생성할 수 있다. 여기서, 하나의 언어 는 한국어일 수 있으나 이에 한정되는 것은 아니다. 제1 음성 인식 데이터는 적어도 하나의 문장을 포함할 수 있다. 서버는 이러한 제1 음성 인식 데이터를 분 석하여 제1 텍스트 분석 데이터를 생성한다(S126)."}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "단계(S126)에서 생성되는 제1 텍스트 분석 데이터는 제1 사용자의 음성 인식 데이터에 대한 언어 처리 결과에 따라 생성된 키워드 정보 및 요약 정보 중 적어도 하나를 포함할 수 있다. 서버는 텍스트 분석 모델(VM4)"}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "을 포함할 수 있으며, 텍스트 분석 모델(VM4)을 통해 키워드 정보 및 요약 정보를 포함하는 텍스트 분석 데이터 를 생성할 수 있다. 구체적으로, 텍스트 분석 모델(VM4)은 제1 음성 인식 데이터에서 개체명 인식, 명사 추출, 자동 띄어쓰기 등의 언어 처리를 수행할 수 있다. 텍스트 분석 모델(VM4)은 이름, 지명, 용어 등 고유 명사를 인식할 수 있으며, 인 식된 명사를 통해 제1 음성 인식 데이터에 포함된 적어도 하나의 문장의 의미와 사용자의 의도를 파악하도록 구 성될 수 있다. 또한, 텍스트 분석 모델(VM4)은 제1 음성 인식 데이터에서 적어도 하나의 키워드를 판단하여 키 워드 정보를 구성할 수 있다. 키워드는 제1 음성 인식 데이터를 구성하는 적어도 하나의 문장에 포함된 핵심 단 어일 수 있으나, 이에 한정되는 것은 아니다. 텍스트 분석 모델(VM4)은 제1 음성 인식 데이터에 대한 전체적인 맥락, 제1 사용자의 의도를 분석하고, 이러한 맥락과 의도를 반영하여 키워드를 새로운 단어로 작성, 구성할 수"}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "도 있다. 또한, 텍스트 분석 모델(VM4)은 키워드 정보를 기초로 제1 음성 인식 데이터의 내용을 요약할 수 있으"}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "며, 이를 기초로 요약 정보를 생성할 수 있다. 예를 들어, 복수의 문장 또는 긴 문장으로 구성되어 요점이 흩어"}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "져 있는 제1 음성 인식 데이터를 간추려 줌으로써, 제1 사용자의 발화의 핵심, 요점을 요약된 문장으로 빠르게 이해하는 것이 지원될 수 있다."}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "단계(S120)에서, 생성되는 제1 텍스트 분석 데이터는 제1 키워드 정보와 제1 요약 정보 중 적어도 하나를 포함 하도록 구성될 수 있다. 다음으로, 제1 전처리 음성 데이터에 기초하여 제1 사용자의 제1 감정 데이터를 생성한다(S130). 단계(S130)에서, 서버는 제1 전처리 음성 데이터에 기초하여 제1 사용자의 제1 감정 데이터를 생성할 수 있다. 여기서, 제1 감정 데이터는 제1 사용자의 다양한 감정 상태 및 심리 상태를 나타내는 데이터일 수 있다. 즉, 발화 당시의 제1 사용자의 심리 내적 상태를 표현할 수 있는 데이터일 수 있다. 예시적으로, 제1 감정 데이 터는 보통(NEUTRAL), 기쁨(HAPPINESS), 슬픔(SADNESS) 및 분노(ANGER) 중 어느 하나로 결정한 상태일 수 있으 나, 본 발명의 실시예가 이에 한정되는 것은 아니다. 단계(S130)에서 제1 사용자의 발화 내용에 대한 분석이 수 행될 수 있으며, 제1 사용자의 감정 상태가 결정된 제1 감정 데이터가 결정될 수 있다. 도 8을 참조하면, 서버는 입력된 제1 전처리 음성 데이터에 기초하여 제1 사용자의 감정 상태를 예측하는 제1 감정 분석 모델(EM1)을 포함할 수 있다. 또한, 서버는 이전 단계(S120)에서 생성된 제1 음성 인식 데 이터를 분석하여 제1 사용자의 감정 상태를 예측하는 제2 감정 분석 모델(EM2)을 포함할 수 있다. 제3 감정 분 석 모델(EM3)은 제1 감정 분석 모델(EM1)에서 출력되는 결과와 제2 감정 분석 모델(EM2)에서 출력되는 결과 중 적어도 하나에 기초하여 제1 사용자의 제1 감정 데이터를 최종적으로 생성할 수 있다. 제1 감정 분석 모델(EM1)은 제1 전처리 음성 데이터에서 바이오마커를 추출하는 바이오마커 추출 모델(EM1_1)을 포함할 수 있다. 단계(S130)에서, 제1 전처리 음성 데이터는 이전 단계(S120)와 달리, 묵음 구간이 제외되지 않 고 묵음 구간과 음성 구간이 함께 고려되어 특징이 추출될 수 있다. 즉, 음성이 나타나는 사이에 존재하는 묵음 의 경우, 발화하는 사용자의 감정이 내포되어 있는 구간일 수 있으므로, 감정을 분석하는 과정에서는 이러한 묵 음 구간에 대한 특징도 함께 고려되게 된다. 여기서, 바이오마커는 보이스 바이오마커(voice biomarker)일 수 있다. 바이오마커는 음성으로부터 획득 가능한 사용자의 상태 관련 정보를 총칭하며, 사용자의 감정 상태에 따라 변화될 수 있는 정보를 내포한다. 예를 들어, 보이스 바이오마커는 주파수, 진폭, 음성 라이즈타임/폴타임, 발성 시간, 발화 속도, 음성진전(voice tremor), 고저 변화(피치, pitch), 음색 변화 뿐만 아니라 음성인식 데이터 산출 과정에서 얻을 수 있는 발화 속도, 발음 정확도, 발성 스타일, 간투사 사용 빈도 등일 수 있으나, 이에 한정되지 않으며, 음성으로부터 감지하고 추출할 수 있는, 비언어적 정보를 모두 포함할 수 있다. 이러한 정보들은 사용자의 감정 변화에 따라 변화될 수 있는 정보에 해당한다. 바이오마커 추출 모델(EM1_1)은 제1 전처리 음성 데이터에서 적어도 하나의 바이오마커를 추출하도록 구성될 수 있다. 예시적으로, 바이오마커 추출 모델(EM1_1)은 Transformer 모델을 사용하여 음성 데이터의 특징을 추출하 거나, 음성 신호처리 기술을 사용하여 음성 및 음성의 언어/비언어적 특징을 수집 및 처리할 수 있다. 제1 감정 분석 모델(EM1)은 바이오마커 추출 모델(EM1_1)에서 추출된 바이오 마커를 기초로 사용자의 감정을 결 정하는 감정 결정 모델(EM1_2)를 포함할 수 있다. 감정 결정 모델(EM1_2)은 바이오 마커에 기초하여 사용자의 복수의 감정에 대한 개별 점수를 책정할 수 있다. 복수의 감정은 보통(NEUTRAL), 기쁨(HAPPINESS), 슬픔 (SADNESS) 및 분노(ANGER)일 수 있으며, 보통, 기쁨, 슬픔 및 분노에 대한 개별 점수가 계산될 수 있다.몇몇 실시예에서, 제1 전처리 음성 데이터는 제1 감정 분석 모델(EM1)에 의해 분석되어 제1 사용자의 4가지 감 정을 예측한 개별 점수, 즉, 제1 감정 평가 점수가 출력될 수 있다. 제2 감정 분석 모델(EM2)은 Transformer, BERT(Bidirectional Encoder Representations from Transformers), RNTN(Recursive Neural Tensor Network) 또는 MV-RNN(Matrix-Vector Recursive Neural Network)과 같은 뉴럴 네트워크 기반의 분석 모델로 구성될 수 있으나, 이에 한정되는 것은 아니다. 제2 감정 분석 모델(EM2)은 제1 음성 인식 데이터에 포함되어 있는 제1 사용자의 의견이나 감성, 평가, 태도 등 의 주관적인 정보에 기초하여 제1 사용자의 복수의 감정에 대한 개별 점수를 예측할 수 있다. 복수의 감정은 보 통(NEUTRAL), 기쁨(HAPPINESS), 슬픔(SADNESS) 및 분노(ANGER)일 수 있으며, 보통, 기쁨, 슬픔 및 분노에 대한 개별 점수가 계산될 수 있다. 몇몇 실시예에서, 제1 음성 인식 데이터는 제2 감정 분석 모델(EM2)에 의해 분석되어 제1 사용자의 4가지 감정 을 예측한 개별 점수가 계산될 수 있다. 즉, 제2 감정 평가 점수가 출력될 수 있다. 제3 감정 분석 모델(EM3)은 제1 감정 분석 모델(EM1)에서 출력되는 결과와 제2 감정 분석 모델(EM2)에서 출력되 는 결과 중 적어도 하나에 기초하여 제1 감정 데이터를 결정할 수 있다. 제1 감정 데이터는 분석 모델을 통해 출력되는 결과에 기초하여 최종적으로 결정된 제1 사용자의 대표 감정일 수 있다. 제3 감정 분석 모델(EM3)은 제1 감정 분석 모델(EM1)의 결과와 제2 감정 분석 모델(EM2)의 결과를 기초로 가장 높은 점수를 가진 감정을 제 1 사용자의 대표 감정으로 판단할 수 있다. 예시적으로, 제1 감정 분석 모델(EM1)에서 출력되는 복수의 감정에 대한 개별 점수와 제2 감정 분석 모델(EM2)에서 출력되는 복수의 감정에 대한 개별 점수를 제3 감정 분석 모델 (EM2)은 일정 가중치에 따라 합산하여 복수의 감정에 대한 최종 점수를 계산할 수 있으며, 최종 점수가 가장 높 은 감정을 대표 감정으로 판단하여 제1 감정 데이터를 구성할 수 있다. 도 9의 예시에서, 제1 감정 평가 점수는 HAPPINESS: 34, SADNESS: 16, ANGER: 46, NEUTRAL: 50로 책정될 수 있 으며, 제2 감정 평가 점수는 HAPPINESS: 32, SADNESS: 14, ANGER: 44, NEUTRAL: 50로 책정될 수 있다. 제3 감 정 분석 모델(EM3)은 제1 감정 평가 점수와 제2 감정 평가 점수를 각각 입력받고 동일한 가중치를 적용하여 이 들의 평균값에 해당하는 제3 감정 평가 점수를 생성할 수 있다. 즉, 제3 감정 평가 점수는 HAPPINESS: 33, SADNESS: 15, ANGER: 45, NEUTRAL: 50으로 책정될 수 있으며, 가장 점수가 높은 NEUTRAL을 대표 감정으로 설정 된 제1 감정 데이터가 구성될 수 있다. 다음으로, 생성된 제1 감정 데이터와 제1 텍스트 분석 데이터를 매칭하여 기록한다(S140). 서버는 이전 단계들(S120, S130)에서 생성된 제1 감정 데이터와 제1 텍스트 분석 데이터를 수집된 제1 음 성 데이터에 대응하여 기록할 수 있다. 제1 음성 데이터의 수집에 대응하여 제1 감정 데이터와 제1 텍스트 분석 데이터가 생성되는 과정이 수행될 수 있으며, 생성이 완료된 제1 감정 데이터와 제1 텍스트 분석 데이터는 매칭 되어 기록될 수 있다. 여기서, 제1 질문 콘텐츠에 대응하여 제1 음성 데이터가 수집되고, 제1 감정 데이터, 제1 텍스트 분석 데이터가 생성된 상태인 경우, 제1 질문 콘텐츠, 제1 음성 데이터, 제1 감정 데이터 및 제1 텍스트 분석 데이터는 하나의 그룹, 하나의 트랙으로 설정되어 기록되게 된다. 본 발명에서, 트랙은 음성 데이터, 음성 데이터를 분석하여 생성된 감정 데이터 및 텍스트 분석 데이터가 함께 저장된 상태를 정의할 수 있다. 또한, 트 랙은 음성 데이터의 생성을 유도한 질문 콘텐츠도 더 포함하도록 구성될 수 있다. 도 10을 참조하면, 예시적으로 하루에 두 번(오전, 오후), 제1 음성 데이터와 제2 음성 데이터가 수집될 수 있 다. 즉, 오전에는 제1 질문 콘텐츠(제1 유형)에 대응하여 제1 음성 데이터가 수집되고, 이에 대응하여 제1 감정 데이터와 제1 텍스트 분석 데이터가 생성될 수 있으며, 관계된 데이터들은 제1 트랙으로 함께 기록될 수 있다. 또한, 오후에는 제2 질문 콘텐츠(제2 유형)에 대응하여 제2 음성 데이터가 수집되고, 이에 대응하여 제2 감정 데이터와 제2 텍스트 분석 데이터가 생성될 수 있으며, 관계된 데이터들은 제2 트랙으로 함께 기록될 수 있다. 다만, 이는 예시적인 것으로 다양한 방법(서버에서 제공되는 질문 콘텐츠, 관계된 사용자로부터 제공되는 질문 콘텐츠)을 통해 하루에 복수의 음성 데이터가 수집될 수 있다. 다음으로, 생성된 제1 감정 데이터와 제1 텍스트 분석 데이터를 사용자 환경을 통해 함께 제공한다(S150). 서버는 사용자 환경을 통해 생성된 제1 감정 데이터와 제1 텍스트 분석 데이터를 함께 조회할 수 있는 기 능을 제1 사용자에게 제공할 수 있다. 제1 사용자는 본인이 발화한 음성에 따라 분석된 감정을 확인할 수 있으"}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "며, 해당 발화 내용의 주요 키워드 및 요약 중 어느 하나를 함께 확인할 수 있다. 도 11과 같이, 사용자 환경은 생성된 트랙을 조회할 수 있는 조회 인터페이스를 제공할 수 있으며, 복수의 트랙 중에 사용자의 선택에 의해 하나의 트랙이 선택되는 것을 지원할 수 있다. 사용자 환경은 선택된 하나의 트랙에 포함된 정보를 한 번에 조회할 수 있는 기능을 제공할 수 있으며, 하나의 트랙을 구성하는 음성 데이터, 감정 데이터 및 텍스트 분석 데이터를 사용자는 하나의 화면에서 확인할 수 있다. 도 12를 참조하면, 사용자 환경은 제1 사용자가 생성한 음성 데이터를 확인할 수 있는 인터페이스(I11), 이에"}
{"patent_id": "10-2024-0120691", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "대응한 감정 데이터를 확인할 수 있는 감정 인터페이스(I8) 및 키워드와 요약 정보를 확인할 수 있는 텍스트 인 터페이스(I9)를 제공하여, 특정 트랙과 관련된 정보가 한 번에 출력되는 것을 지원할 수 있다. 또한, 사용자 환 경은 \"나의 테이프 보내기\"와 같은 공유 인터페이스(I10)를 더 제공하여, 생성된 트랙을 관계된 다른 사용자에 게 공유하는 것을 지원할 수 있다. 또한, 관계된 사용자들 사이에는 자동으로 감정 데이터 및 텍스트 분석 데이터의 공유가 수행될 수 있으며, 사 용자 환경은 관계된 다른 사용자의 트랙을 조회하는 기능을 더 제공할 수 있다. 또한, 몇몇 실시예에서, 서버는 관계된 사용자 사이에 생성된 복수의 감정 데이터를 기초로 공유 감정 데 이터를 생성할 수 있다. 즉, 본 발명의 몇몇 실시예에 따른 사용자의 감정 추적 및 관리 방법은 제1 사용자와 관계된 제2 사용자의 제2 감정 데이터와 제2 텍스트 분석 데이터를 수신하는 단계; 및 상기 제1 감정 데이터와 상기 제2 감정 데이터에 기초하여 공유 감정 데이터를 생성하는 단계를 더 포함하고, 상기 사용자 환경은 상기 제2 감정 데이터, 상기 제2 텍스트 분석 데이터 및 상기 공유 감정 데이터를 더 제공하도록 구성될 수 있다. 서버는 제1 사용자의 제1 감정 데이터와 제2 사용자자의 제2 감정 데이터를 기초로 공유 감정 데이터를 생 성할 수 있다. 공유 감정 데이터는 제1 감정 데이터에 포함되는 복수의 감정 각각에 대한 개별 점수와 제2 감정 데이터에 포함되는 복수의 감정 각각에 대한 개별 점수를 일정 가중치에 따라 합산하여 복수의 감정에 대한 최 종 점수를 계산하는 과정을 통해 생성될 수 있다. 예시적으로, 제1 사용자의 제1 감정 데이터가 HAPPINESS: 33, SADNESS: 15, ANGER: 45, NEUTRAL: 50이고, 제2 사용자의 제2 감정 데이터가 HAPPINESS: 13, SADNESS: 45, ANGER: 25, NEUTRAL: 10인 경우, 공유 감정 데이터 는 HAPPINESS: 23, SADNESS: 30, ANGER: 35, NEUTRAL: 30로 결정될 수 있다. 서버는 유사한 생성 시점을 가진 제1 감정 데이터와 제2 감정 데이터를 기초로 공유 감정 데이터를 생성할 수 있다. 예시적으로, 서버는 오전 시간대에 생성된 제1 감정 데이터와 제2 감정 데이터를 기초로 공유 감 정 데이터를 생성하고, 오후 시간대에서 생성된 제1 감정 데이터와 제2 감정 데이터를 기초로 공유 감정 데이터 를 생성할 수 있다. 또한, 서버는 동일한 질문 콘텐츠에 기초하여 생성된 제1 감정 데이터와 제2 감정 데 이터를 기초로 공유 감정 데이터를 생성할 수도 있다. 이러한, 공유 감정 데이터는 사용자 환경을 통해 제1 사 용자와 제2 사용자에게 모두 제공될 수 있으며, 제1 사용자와 제2 사용자는 상호간에 공유하는 감정의 유사 정 도와 어떠한 감정을 함께 공유하고 있는 지에 대한 정보를 확인할 수 있게 된다. 사용자 환경은 상기와 같이 수집된 정보를 제1 감정 데이터 및 제1 텍스트 분석 데이터에 대한 통계 분석 자료 를 더 제공하도록 구성될 수 있다. 여기서, 통계 분석 자료는 제1 감정 데이터 및 제1 텍스트 분석 데이터를 특정 기간에 따라 분석한 정보일 수 있다. 도 13은 제1 사용자에 대한 통계 분석 자료를 특정 기간(예를 들어, 이번주)를 기준으로 제공하는 사용자 환경 을 예시적으로 도시하며, 도 14는 제1 사용자가 현재까지의 사용 이력에 기초하여 통계 분석 자료를 제공하는 사용자 환경을 예시적으로 도시한다. 도 13을 참조하면, 감정 그래프(I12)는 시간에 따른 제1 사용자의 제1 감정 데이터의 변화를 표시할 수 있다. 여기서, 서버는 제1 사용자와 관련하여 하루에 복수의 제1 감정 데이터가 수집되었을 때, 가장 수치가 높 은 감정을 대표 감정으로 표시하여 감정 그래프를 구성할 수 있다. 또한, 감정 그래프(I12)는 제1 사용자과 관 계된 제2 사용자의 제2 감정 데이터의 변화도 함께 제공하는 것을 알 수 있다. 또한, 제1 그래프(I12)는 제1 감 정 데이터와 제2 감정 데이터를 기초로 생성된 공유 감정 데이터가 제공되는 것을 알 수 있다. 이러한, 감정 그 래프(I12)를 통해 제1 사용자 자신의 감정 변화와 제1 사용자와 관계된 제2 사용자의 감정 변화, 이들이 공유하는 감정의 변화를 확인할 수 있는 것이 지원되게 된다. 또한, 사용자 환경은 특정 기간에서 가장 높은 감정 수치를 가진 기록에 대한 정보를 감정 통계 인터페이스 (I13)를 통해 더 제공할 수 있다. 예를 들어, 감정 통계 인터페이스(I13)는 가장 기뻤던 기록, 가장 슬펐던 기 록, 가장 화났던 기록에 대한 정보를 제1 사용자 및 제1 사용자와 관계된 제2 사용자에 대해서 제공될 수 있다. 또한, 도 14를 참조하면, 사용자 환경은 제1 사용자가 현재까지 음성 데이터를 생성한 이력, 즉, 녹음을 수행한 이력과 관련된 정보를 제1 이력 인터페이스(I14)를 통해 제공할 수 있다. 또한, 사용자 환경은 제1 사용자가 현재까지 음성 데이터를 생성할 때, 활용한 질문 콘텐츠의 종류와 관련된 이 력을 제2 이력 인터페이스(I15)를 통해 제공할 수 있다. 또한, 사용자 환경은 제1 사용자가 현재까지 생성한 텍스트 분석 데이터와 관련하여 이의 분석을 통해 도출된 키워드와 관련된 이력을 제3 이력 인터페이스(I16)을 통해 제공할 수 있다. 본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법 및 시스템은 사용자에게 질문 콘텐츠를 제공하여 사용자의 발화를 유도하며, 음성 인식을 통해 사용자의 발화를 용이하게 기록하고, 기록된 사용자의 발화 내용을 분석하여 사용자의 감정을 분석하여 사용자의 감정 상태를 추적, 관리하는 것을 지원할 수 있다. 또한, 본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법 및 시스템은 사용자와 관 계된 다른 사용자의 감정 데이터를 상호 간에 교환하여 공유 감정 데이터를 도출하여 함께 제공함으로써, 관계 된 사용자들 사이의 감정 교환 및 감정 공유가 수행되는 것을 지원할 수 있다. 또한, 본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법 및 시스템은 수집된 음성 데이터, 이를 기초로 분석된 감정 데이터 및 텍스트 분석 데이터를 함께 제공하는 사용자 환경을 통해 사용자가 본인의 음성 데이터에 기반한 감정, 발화 내용의 키워드 및 핵심을 확인하는 것을 지원할 수 있다. 또한, 본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법 및 시스템은 수집된 복수 의 데이터들을 기간 또는 그동안의 이력을 통해 분석한 통계 정보를 제공하여, 사용자가 본인의 감정 변화 및 관계된 사용자의 감정 변화를 확인하는 것을 더욱 지원할 수 있다. 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법은 컴퓨터에 의해 실행 가능한 명령어 및 데이터 를 저장하는, 컴퓨터로 판독 가능한 매체의 형태로도 구현될 수 있다. 이때, 명령어 및 데이터는 프로그램 코드 의 형태로 저장될 수 있으며, 프로세서에 의해 실행되었을 때, 소정의 프로그램 모듈을 생성하여 소정의 동작을 수행할 수 있다. 또한, 컴퓨터로 판독 가능한 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터로 판독 가능한 매체 는 컴퓨터 기록 매체일 수 있는데, 컴퓨터 기록 매체는 컴퓨터 판독 가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함할 수 있다. 예를 들어, 컴퓨터 기록 매체는 HDD 및 SSD 등과 같은 마그네틱 저장 매 체, CD, DVD 및 블루레이 디스크 등과 같은 광학적 기록 매체, 또는 네트워크를 통해 접근 가능한 서버에 포함 되는 메모리일 수 있다. 또한 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법은 컴퓨터에 의해 실행 가능한 명령어를 포 함하는 컴퓨터 프로그램(또는 컴퓨터 프로그램 제품)으로 구현될 수도 있다. 컴퓨터 프로그램은 프로세서에 의 해 처리되는 프로그래밍 가능한 기계 명령어를 포함하고, 고레벨 프로그래밍 언어(High-level Programming Language), 객체 지향 프로그래밍 언어(Object-oriented Programming Language), 어셈블리 언어 또는 기계 언 어 등으로 구현될 수 있다. 또한 컴퓨터 프로그램은 유형의 컴퓨터 판독가능 기록매체(예를 들어, 메모리, 하드 디스크, 자기/광학 매체 또는 SSD(Solid-State Drive) 등)에 기록될 수 있다. 따라서 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 방법은 상술한 바와 같은 컴퓨터 프로그램이 컴퓨팅 장치에 의해 실행됨으로써 구현될 수 있다. 컴퓨팅 장치는 프로세서와, 메모리와, 저장 장치와, 메모리 및 고속 확장포트에 접속하고 있는 고속 인터페이스와, 저속 버스와 저장 장치에 접속하고 있는 저속 인터페이 스 중 적어도 일부를 포함할 수 있다. 이러한 성분들 각각은 다양한 버스를 이용하여 서로 접속되어 있으며, 공 통 머더보드에 탑재되거나 다른 적절한 방식으로 장착될 수 있다. 여기서 프로세서는 컴퓨팅 장치 내에서 명령어를 처리할 수 있는데, 이런 명령어로는, 예컨대 고속 인터페이스 에 접속된 디스플레이처럼 외부 입력, 출력 장치상에 GUI(Graphic User Interface)를 제공하기 위한 그래픽 정보를 표시하기 위해 메모리나 저장 장치에 저장된 명령어를 들 수 있다. 다른 실시예로서, 다수의 프로세서 및 (또는) 다수의 버스가 적절히 다수의 메모리 및 메모리 형태와 함께 이용될 수 있다. 또한 프로세서는 독립적인 다수의 아날로그 및(또는) 디지털 프로세서를 포함하는 칩들이 이루는 칩셋으로 구현될 수 있다. 또한 메모리는 컴퓨팅 장치 내에서 정보를 저장한다. 일례로, 메모리는 휘발성 메모리 유닛 또는 그들의 집합으 로 구성될 수 있다. 다른 예로, 메모리는 비휘발성 메모리 유닛 또는 그들의 집합으로 구성될 수 있다. 또한 메 모리는 예컨대, 자기 혹은 광 디스크와 같이 다른 형태의 컴퓨터 판독 가능한 매체일 수도 있다. 그리고 저장장치는 컴퓨팅 장치에게 대용량의 저장공간을 제공할 수 있다. 저장 장치는 컴퓨터 판독 가능한 매 체이거나 이런 매체를 포함하는 구성일 수 있으며, 예를 들어 SAN(Storage Area Network) 내의 장치들이나 다른 구성도 포함할 수 있고, 플로피 디스크 장치, 하드 디스크 장치, 광 디스크 장치, 혹은 테이프 장치, 플래시 메 모리, 그와 유사한 다른 반도체 메모리 장치 혹은 장치 어레이일 수 있다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2024-0120691", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 몇몇 실시예에 따른 인공지능 기술에 기반한 감정 추적 및 관리 시스템을 설명하기 위한 개념 도이다. 도 2는 본 발명의 실시예에 따른 사용자 환경이 제공하는 주요 서비스 기능을 설명하기 위한 예시도이다. 도 3은 본 발명의 몇몇 실시예에 따른 사용자의 감정 추적 및 관리 방법의 순서도이다. 도 4는 제1 사용자의 제1 음성 데이터를 수집하는 과정을 예시적으로 도시한다. 도 5a 및 도 5b는 제1 사용자 장치를 통해 질문 콘텐츠가 제공되는 사용자 환경의 인터페이스를 나타낸다. 도 6은 제1 전처리 음성 데이터를 생성하는 과정을 설명하기 위한 예시도이다. 도 7은 제1 텍스트 분석 데이터를 생성하는 과정을 설명하기 위한 예시도이다. 도 8은 제1 감정 데이터를 생성하는 과정을 설명하기 위한 예시도이다. 도 9는 제3 감정 분석 모델에서 제1 감정 데이터를 생성하는 과정을 설명하기 위한 예시도이다. 도 10은 음성 데이터와 감정 데이터를 생성하여 기록하는 과정을 설명하기 위한 예시도이다. 도 11은 사용자 환경에서 음성 데이터와 감정 데이터가 기록된 리스트를 확인할 수 있는 예시적인 화면을 도시 한다. 도 12는 사용자 환경에서 음성 데이터와 감정 데이터가 함께 제공되는 예시적인 화면을 도시한다. 도 13 및 도 14는 사용자 환경에서 통계 정보를 제공하는 예시적인 화면을 도시한다."}
