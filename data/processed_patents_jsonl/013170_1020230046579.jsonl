{"patent_id": "10-2023-0046579", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0150847", "출원번호": "10-2023-0046579", "발명의 명칭": "학습하는 사용자의 모습이 포함된 비디오를 분석하여 집중 시간을 산출하는 서버, 사용자 단", "출원인": "김지욱", "발명자": "김지욱"}}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "서버의 동작 방법에 있어서,상기 서버가, 사용자 단말을 통해 촬영된 비디오를 상기 사용자 단말로부터 수신하는 단계;상기 서버가, 상기 비디오를 구성하는 복수의 이미지 프레임 각각에서, 사용자 및 학습 물품을 식별하는 단계;상기 서버가, 상기 복수의 이미지 프레임 각각에서 상기 사용자가 바라보는 방향을 추출하는 단계; 및상기 서버가, 상기 사용자가 바라보는 방향과 상기 학습 물품의 위치를 바탕으로 상기 사용자의 집중 여부를 시간 별로 결정하는 단계;를 포함하는, 서버의 동작 방법."}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 사용자의 집중 여부를 시간 별로 결정하는 단계는,상기 사용자가 바라보는 방향이 상기 학습 물품의 위치에 매칭되는 경우, 상기 사용자가 집중하고 있는 것으로결정하고,상기 서버의 동작 방법은,시간 별로 결정된 상기 사용자의 집중 여부를 바탕으로, 상기 사용자의 집중 시간을 산출하는 단계;를 더 포함하는, 서버의 동작 방법."}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 서버의 동작 방법은,상기 서버가, 상기 사용자의 몸통이 향하는 방향을 추출하는 단계; 및상기 사용자가 바라보는 방향이 상기 학습 물품에 매칭되는 동안, 상기 사용자의 몸통이 향하는 방향을 바탕으로 상기 사용자의 집중도를 결정하는 단계;를 더 포함하는, 서버의 동작 방법."}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 사용자의 집중 여부를 시간 별로 결정하는 단계는,상기 사용자의 얼굴에 포함된 눈을 추출하는 단계;상기 복수의 프레임 각각에 대하여, 상기 추출된 눈이 감겨 있는지 여부를 식별하는 단계;상기 눈이 임계 시간 이상 연속적으로 감겨 있는 시간 구간에 대해서는, 상기 사용자가 집중하지 않는 것으로식별하는 단계; 및시간 별로 결정된 상기 사용자의 집중 여부를 바탕으로, 상기 사용자의 집중 시간을 산출하는 단계;를포함하는, 서버의 동작 방법."}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 서버의 동작 방법은,공개특허 10-2024-0150847-3-상기 비디오의 시간 길이를 바탕으로 상기 사용자의 학습 시간을 식별하는 단계; 및상기 사용자 단말로부터 수신된 비디오의 개수를 바탕으로 상기 사용자의 학습 횟수를 식별하는 단계;를 포함하는, 서버의 동작 방법."}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 복수의 이미지 프레임 중 상기 사용자가 식별되지 않은 이미지 프레임의 수를 바탕으로 상기 사용자의 학습 시간을 차감하는 단계;를 더 포함하는, 서버의 동작 방법."}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 사용자가 바라보는 방향을 획득하는 단계는,상기 사용자의 얼굴을 구성하는 복수의 지점 각각의 좌표를 식별하고,상기 식별된 좌표를 바탕으로 상기 사용자의 얼굴이 바라보는 방향을 획득하는, 서버의 동작 방법."}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 서버의 동작 방법은,제1 사용자의 제1 사용자 단말을 통해 촬영된 제1 비디오를 복수의 사용자 단말로 제공하는 단계;제2 사용자의 제2 사용자 단말로부터, 상기 제1 비디오에 대한 호감을 표시하는 제2 사용자의 사용자 입력을 수신하는 단계; 및상기 제1 비디오에 대한 호감을 표시하는 사용자 입력을 바탕으로, 상기 제1 사용자를 위한 쿠폰 정보를 상기제1 사용자 단말로 전송하는 단계;를 더 포함하는, 서버의 동작 방법."}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 서버의 동작 방법은,상기 사용자 단말을 통해 복수의 이미지 프레임이 순차적으로 촬영되는 동안, 상기 순차적으로 촬영되는 복수의이미지 프레임 각각을 실시간으로 표시하도록 상기 사용자 단말을 제어하는 단계; 및상기 사용자 단말에 상기 복수의 프레임이 실시간으로 표시되는 동안, 사용자 입력에 따라 선택된 다른 비디오를 함께 표시하도록 상기 사용자 단말을 제어하는 단계;를 더 포함하는, 서버의 제어 방법."}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,상기 서버의 동작 방법은,상기 복수의 이미지 프레임 중 일정 수 이상의 연속된 이미지 프레임 내에서 사용자는 식별되고 기설정된 학습물품은 식별되지 않는 경우, 상기 사용자 단말을 통해 상기 사용자로 하여금 학습 물품을 바라보도록 하는 요청을 출력하는 단계;상기 요청이 출력된 이후 촬영된 적어도 하나의 이미지 프레임 내에서 상기 사용자가 바라보는 목표 방향을 식별하는 단계; 및실시간으로 상기 사용자가 바라보는 방향과 상기 목표 방향을 비교하여, 상기 사용자의 집중 여부를 시간 별로결정하는 단계;를 포함하는, 서버의 동작 방법.공개특허 10-2024-0150847-4-청구항 11 적어도 하나의 인스트럭션이 저장된, 메모리;적어도 하나의 사용자 단말과 통신을 수행하는, 통신부; 및상기 인스트럭션을 실행하여 제1항의 동작 방법을 수행하는, 프로세서;를 포함하는, 전자 장치."}
{"patent_id": "10-2023-0046579", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "사용자 단말의 동작 방법에 있어서,상기 사용자 단말이, 사용자 입력에 따라 비디오를 촬영하는 단계;상기 사용자 단말이, 상기 비디오를 구성하는 복수의 이미지 프레임 각각에서, 사용자 및 학습 물품을 식별하는단계;상기 사용자 단말이, 상기 복수의 이미지 프레임 각각에서 상기 사용자가 바라보는 방향을 추출하는 단계; 및상기 사용자 단말이, 상기 사용자가 바라보는 방향과 상기 학습 물품의 위치를 바탕으로 상기 사용자의 집중 여부를 시간 별로 결정하는 단계;를 포함하는, 사용자 단말의 동작 방법."}
{"patent_id": "10-2023-0046579", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "서버의 동작 방법이 개시된다. 본 동작 방법은, 서버가, 사용자 단말을 통해 촬영된 비디오를 사용자 단말로부터 수신하는 단계, 서버가, 비디오를 구성하는 복수의 이미지 프레임 각각에서, 사용자 및 학습 물품을 식별하는 단 계, 서버가, 복수의 이미지 프레임 각각에서 사용자의 얼굴이 바라보는 방향을 추출하는 단계, 서버가, 사용자의 얼굴이 바라보는 방향과 학습 물품의 위치를 바탕으로 사용자의 집중 여부를 시간 별로 결정하는 단계를 포함한 다."}
{"patent_id": "10-2023-0046579", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 사용자의 학습 영상을 획득하여 분석하는 서버 내지 사용자 단말에 관한 것으로, 보다 상세하게는, 사용자가 바라보는 방향에 따라 사용자의 집중 여부를 식별하는 동작에 관한 것이다."}
{"patent_id": "10-2023-0046579", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 특정인이 공부하는 모습을 담은 브이로그 영상들의 인기가 높아지면서, 다양한 사람들이 스스로의 학습 영 상을 담은 비디오를 불특정 다수에게 공개하고 있다. 다만, 이러한 콘텐츠는 타인에게 본인의 모습을 보여주기 위한 목적으로 제공되는 경우가 많아, 실질적으로 학 습에 도움이 되지 않을 수 있으며, 유명인이 아닌 일반인의 경우 본인의 학습 영상을 촬영하여 기록할 필요성을 느끼지 못하는 경우가 대부분이다. 선행기술문헌 특허문헌 (특허문헌 0001) 공개특허공보 제10-2021-0012713호(공부시간 측정 시스템 및 방법)"}
{"patent_id": "10-2023-0046579", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는, 스스로의 공부 모습을 담은 비디오를 촬영하여 공부 시간을 자동으로 기록할 뿐 아니라, 이미지 분 석을 바탕으로 집중 시간을 추출하여 사용자에게 제공하는 소프트웨어 내지 플랫폼을 제공하는 서버 및 사용자 단말을 제공하고자 한다. 본 개시는, 공부하는 모습이 촬영된 비디오가 사용자 본인의 학습 과정에 효율적으로 활용될 수 있도록 지원하 는 다양한 부가 기능을 포함하는 서버 내지 사용자 단말을 제공한다. 본 개시의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 개시의 다른 목적 및 장점들 은 하기의 설명에 의해서 이해될 수 있고, 본 개시의 실시 예에 의해 보다 분명하게 이해될 것이다. 또한, 본 개시의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것이다."}
{"patent_id": "10-2023-0046579", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 서버의 동작 방법은, 상기 서버가, 사용자 단말을 통해 촬영된 비디오를 상기 사 용자 단말로부터 수신하는 단계, 상기 서버가, 상기 비디오를 구성하는 복수의 이미지 프레임 각각에서, 사용자 및 학습 물품을 식별하는 단계, 상기 서버가, 상기 복수의 이미지 프레임 각각에서 상기 사용자가 바라보는 방 향을 추출하는 단계, 상기 서버가, 상기 사용자가 바라보는 방향과 상기 학습 물품의 위치를 바탕으로 상기 사 용자의 집중 여부를 시간 별로 결정하는 단계를 포함한다. 상기 사용자의 집중 여부를 시간 별로 결정하는 단계는, 상기 사용자가 바라보는 방향이 상기 학습 물품의 위치 에 매칭되는 경우, 상기 사용자가 집중하고 있는 것으로 결정할 수 있다. 상기 서버의 동작 방법은, 시간 별로 결정된 상기 사용자의 집중 여부를 바탕으로, 상기 사용자의 집중 시간을 산출하는 단계를 더 포함할 수 있다. 이때, 상기 서버의 동작 방법은, 상기 서버가, 상기 사용자의 몸통이 향하는 방향을 추출하는 단계, 상기 사용 자가 바라보는 방향이 상기 학습 물품에 매칭되는 동안, 상기 사용자의 몸통이 향하는 방향을 바탕으로 상기 사 용자의 집중도를 결정하는 단계를 더 포함할 수 있다. 한편, 상기 사용자의 집중 여부를 시간 별로 결정하는 단계는, 상기 사용자의 얼굴에 포함된 눈을 추출하는 단 계, 상기 복수의 프레임 각각에 대하여, 상기 추출된 눈이 감겨 있는지 여부를 식별하는 단계, 상기 눈이 임계 시간 이상 연속적으로 감겨 있는 시간 구간에 대해서는, 상기 사용자가 집중하지 않는 것으로 식별하는 단계, 시간 별로 결정된 상기 사용자의 집중 여부를 바탕으로, 상기 사용자의 집중 시간을 산출하는 단계를 포함할 수 있다. 상기 서버의 동작 방법은, 상기 비디오의 시간 길이를 바탕으로 상기 사용자의 학습 시간을 식별하는 단계, 상 기 사용자 단말로부터 수신된 비디오의 개수를 바탕으로 상기 사용자의 학습 횟수를 식별하는 단계를 포함할 수 있다. 이 경우, 상기 서버의 동작 방법은, 상기 복수의 이미지 프레임 중 상기 사용자가 식별되지 않은 이미지 프레임 의 수를 바탕으로 상기 사용자의 학습 시간을 차감하는 단계를 더 포함할 수 있다. 상기 사용자가 바라보는 방향을 획득하는 단계는, 상기 사용자의 얼굴을 구성하는 복수의 지점 각각의 좌표를 식별하고, 상기 식별된 좌표를 바탕으로 상기 사용자의 얼굴이 바라보는 방향을 획득할 수 있다. 상기 서버의 동작 방법은, 제1 사용자의 제1 사용자 단말을 통해 촬영된 제1 비디오를 복수의 사용자 단말로 제 공하는 단계, 제2 사용자의 제2 사용자 단말로부터, 상기 제1 비디오에 대한 호감을 표시하는 제2 사용자의 사 용자 입력을 수신하는 단계, 상기 제1 비디오에 대한 호감을 표시하는 사용자 입력을 바탕으로, 상기 제1 사용 자를 위한 쿠폰 정보를 상기 제1 사용자 단말로 전송하는 단계를 더 포함할 수 있다. 상기 서버의 동작 방법은, 상기 사용자 단말을 통해 복수의 이미지 프레임이 순차적으로 촬영되는 동안, 상기 순차적으로 촬영되는 복수의 이미지 프레임 각각을 실시간으로 표시하도록 상기 사용자 단말을 제어하는 단계, 상기 사용자 단말에 상기 복수의 프레임이 실시간으로 표시되는 동안, 사용자 입력에 따라 선택된 다른 비디오 를 함께 표시하도록 상기 사용자 단말을 제어하는 단계를 더 포함할 수 있다. 상기 서버의 동작 방법은, 상기 복수의 이미지 프레임 중 일정 수 이상의 연속된 이미지 프레임 내에서 사용자 는 식별되고 기설정된 학습 물품은 식별되지 않는 경우, 상기 사용자 단말을 통해 상기 사용자로 하여금 학습 물품을 바라보도록 하는 요청을 출력하는 단계, 상기 요청이 출력된 이후 촬영된 적어도 하나의 이미지 프레임 내에서 상기 사용자가 바라보는 목표 방향을 식별하는 단계, 실시간으로 상기 사용자가 바라보는 방향과 상기 목표 방향을 비교하여, 상기 사용자의 집중 여부를 시간 별로 결정하는 단계를 포함할 수 있다. 본 개시의 일 실시 예에 따른 사용자 단말의 동작 방법은, 상기 사용자 단말이, 사용자 입력에 따라 비디오를 촬영하는 단계, 상기 사용자 단말이, 상기 비디오를 구성하는 복수의 이미지 프레임 각각에서, 사용자 및 학습 물품을 식별하는 단계, 상기 사용자 단말이, 상기 복수의 이미지 프레임 각각에서 상기 사용자가 바라보는 방향 을 추출하는 단계, 상기 사용자 단말이, 상기 사용자가 바라보는 방향과 상기 학습 물품의 위치를 바탕으로 상 기 사용자의 집중 여부를 시간 별로 결정하는 단계를 포함한다."}
{"patent_id": "10-2023-0046579", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따른 서버 및 사용자 단말은, 학습을 수행하는 사용자를 촬영하여 비디오를 기록하는 한편, 사용자의 공부 시간 및 집중 시간에 대한 데이터를 제공하는 효과가 있다."}
{"patent_id": "10-2023-0046579", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에 대하여 구체적으로 설명하기에 앞서, 본 명세서 및 도면의 기재 방법에 대하여 설명한다. 먼저, 본 명세서 및 청구범위에서 사용되는 용어는 본 개시의 다양한 실시 예들에서의 기능을 고려하여 일반적 인 용어들을 선택하였다. 하지만, 이러한 용어들은 당해 기술 분야에 종사하는 기술자의 의도나 법률적 또는 기 술적 해석 및 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 일부 용어는 출원인이 임의로 선정한 용어 도 있다. 이러한 용어에 대해서는 본 명세서에서 정의된 의미로 해석될 수 있으며, 구체적인 용어 정의가 없으 면 본 명세서의 전반적인 내용 및 당해 기술 분야의 통상적인 기술 상식을 토대로 해석될 수도 있다. 또한, 본 명세서에 첨부된 각 도면에 기재된 동일한 참조번호 또는 부호는 실질적으로 동일한 기능을 수행하는 부품 또는 구성요소를 나타낸다. 설명 및 이해의 편의를 위해서 서로 다른 실시 예들에서도 동일한 참조번호 또 는 부호를 사용하여 설명한다. 즉, 복수의 도면에서 동일한 참조 번호를 가지는 구성요소를 모두 도시되어 있다 고 하더라도, 복수의 도면들이 하나의 실시 예를 의미하는 것은 아니다. 또한, 본 명세서 및 청구범위에서는 구성요소들 간의 구별을 위하여 \"제1\", \"제2\" 등과 같이 서수를 포함하는 용어가 사용될 수 있다. 이러한 서수는 동일 또는 유사한 구성요소들을 서로 구별하기 위하여 사용하는 것이며 이러한 서수 사용으로 인하여 용어의 의미가 한정 해석되어서는 안 된다. 일 예로, 이러한 서수와 결합된 구성 요소는 그 숫자에 의해 사용 순서나 배치 순서 등이 제한되어서는 안 된다. 필요에 따라서는, 각 서수들은 서로 교체되어 사용될 수도 있다. 본 명세서에서 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이 들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시의 실시 예에서 \"모듈\", \"유닛\", \"부(part)\" 등과 같은 용어는 적어도 하나의 기능이나 동작을 수행하는 구성요소를 지칭하기 위한 용어이며, 이러한 구성요소는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\", \"유닛\", \"부(part)\" 등은 각각이 개별적인 특정 한 하드웨어로 구현될 필요가 있는 경우를 제외하고는, 적어도 하나의 모듈이나 칩으로 일체화되어 적어도 하나 의 프로세서로 구현될 수 있다. 또한, 본 개시의 실시 예에서, 어떤 부분이 다른 부분과 연결되어 있다고 할 때, 이는 직접적인 연결뿐 아니라, 다른 매체를 통한 간접적인 연결의 경우도 포함한다. 또한, 어떤 부분이 어떤 구성요소를 포함한다는 의미는, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것 을 의미한다. 도 1은 본 개시의 일 실시 예에 따라 다양한 사용자 단말과 통신을 수행하는 서버의 동작을 설명하기 위한 블록 도이다. 도 1을 참조하면, 본 개시에 따른 서버는 다양한 사용자 단말(200-1, 2, 3, …)과 통신을 수행하여 후술할 다양한 실시 예에 따른 동작을 수행할 수 있다. 서버는 적어도 하나의 컴퓨터를 포함하는 서버 장치 내지는 시스템으로 구현될 수 있다. 또는, 서버 는 데스크탑 PC, 노트북 PC, 스마트폰 등 단말 기기로 구현될 수도 있다. 서버는 사용자의 학습 관리를 위한 적어도 하나의 웹 페이지 내지는 애플리케이션을 제공할 수 있으며, 각 사용자 단말은 서버가 제공하는 웹 페이지 내지 애플리케이션을 실행하여 서버가 제공하는 이하 학습 관리 서비스를 이용할 수 있다. 또는, 각 사용자 단말은 서버로부터 학습 관리를 위한 소프트웨어를 다운 로드 받아 실행할 수도 있다. 사용자 단말(200-1, 2, 3, …)은 각 사용자의 단말 기기에 해당하며, 스마트폰, 태블릿 PC, 노트북 PC, 데스크 탑 PC, 웨어러블 장치 등에 해당할 수 있다. 또는, 사용자 단말(200-1, 2, 3, …)은 휴대 가능하거나 또는 특정 장소에 설치된 상태로 촬영을 위한 카메라를 포함하는 다양한 촬영 기기 내지 시스템에 해당할 수도 있다. 도 2는 본 개시의 일 실시 예에 따른 서버의 동작 방법을 설명하기 위한 흐름도이다. 도 2를 참조하면, 서버는 사용자 단말을 통해 촬영된 비디오를 수신할 수 있다(S210). 해당 비디오는, 사용자 입력에 따라 사용자가 공부하는 모습이 촬영된 비디오에 해당할 수 있으며, 사용자 단말 을 통해 촬영된 것일 수도 있고, 사용자 단말과 연결된 적어도 하나의 카메라 시스템을 통해 촬영된 것일 수도 있다. 대표적으로, 후술할 도 6a 내지 도 6b의 과정을 통해 촬영된 것일 수 있다. 사용자 단말을 통해 비디오가 촬영되는 동안, 서버는 실시간으로 촬영되는 이미지 프레임을 사용자 단말로 부터 수신할 수 있다. 사용자 단말을 통해 복수의 이미지 프레임이 순차적으로 촬영되는 동안, 순차적으로 촬영되는 복수의 이미지 프 레임은 사용자 단말을 통해서도 실시간으로 표시될 수 있다. 그 결과, 사용자는 본인이 공부하는 모습이 촬영된 결과물(비디오)을 사용자 단말을 통해 실시간으로 확인할 수 있다. 한편, 서버는 비디오를 구성하는 복수의 이미지 프레임 각각에서, 사용자 및 학습 물품을 식별할 수 있다 (S220). 이를 위해, 서버는 인간(ex. 사용자)을 인식하기 위한 적어도 하나의 객체 인식 모델(ex. CNN(Convolutional Neural Network) 모델)을 포함할 수 있다. 이때, 본 객체 인식 모델은 인간의 얼굴, 몸통, 팔, 다리 등을 각각 구분하여 인식하도록 훈련된 모델일 수도 있으나, 이에 한정되지 않는다. 또한, 서버는 다양한 종류의 학습 물품을 식별하도록 훈련된 적어도 하나의 객체 인식 모델을 포함할 수 있다. 학습 물품은, 사용자의 학습에 이용될 수 있는 다양한 물품에 해당할 수 있다. 예를 들어, 노트북 PC, 태블릿 PC, 책, 노트, 펜, 연필 등이 학습 물품으로 정의될 수 있다. 그리고, 상기 정의된 학습 물품 각각을 인식하도 록 훈련된 적어도 하나의 객체 인식 모델이 서버에 포함될 수 있다. 관련하여, 도 3a는 본 개시의 일 실시 예에 따른 서버가 실시간으로 촬영된 이미지 내에서 사용자 및 학습 물품 을 식별하는 동작을 설명하기 위한 도면이다. 도 3a를 참조하면, 촬영된 이미지 프레임 내에서 서버는 사용자 및 다양한 학습 물품(351, 352, 353)을 식별할 수 있다. 도 3a를 참조하면, 서버는 이미지 프레임 내에서 사용자의 얼굴을 식별할 수 있다. 또한, 서버는 이미지 프레임 내에서 노트북, 노트, 펜 등 다양한 학습 물품을 인식할 수 있 다. 그리고, 도 2를 참조하면, 서버는 복수의 이미지 프레임 각각에서 사용자가 바라보는 방향을 추출할 수 있 다(S230). 일 실시 예로, 서버는 사용자의 얼굴이 바라보는 방향을 사용자가 바라보는 방향으로 식별할 수 있다. 구체적으로, 서버는 이미지 프레임 내에서 얼굴을 구성하는 복수의 지점 각각의 좌표를 식별하고, 식별된 좌표를 바탕으로 사용자의 얼굴이 바라보는 방향을 획득할 수 있다. 또는, 서버는 얼굴에 포함된 코, 입술, 눈매 중 적어도 하나의 각도를 바탕으로 얼굴이 바라보는 방향을 식별할 수도 있다. 관련하여, 도 3b는 본 개시의 일 실시 예에 따른 서버가 실시간으로 촬영된 이미지 내에서 사용자의 얼굴이 바 라보는 방향을 식별하는 동작을 설명하기 위한 도면이다. 도 3b를 참조하면, 서버는 사용자의 얼굴 내에서 광대, 코 끝, 이마, 턱 등에 해당하는 각 지점의 좌 표를 식별하고, 좌표 간의 상대적 위치 관계를 바탕으로 얼굴이 바라보는 방향을 식별할 수 있다. 이를 위해, 얼굴 내 각 지점을 추출하도록 훈련된 적어도 하나의 인공지능 모델이 활용될 수 있다. 한편, 서버는 얼굴이 바라보는 방향 외에, 동공의 위치, 몸통이 향하는 방향 등을 바탕으로 사용자가 바라 보는 방향을 식별할 수도 있다. 상술한 실시 예들 중 적어도 하나에 따라 사용자가 바라보는 방향이 식별되면, 서버는 사용자가 바라보는 방향과 학습 물품의 위치를 바탕으로 사용자의 집중 여부를 시간 별로 결정할 수 있다(S240). 구체적으로, 사용자가 바라보는 방향이 학습 물품의 위치에 매칭되는 경우, 사용자가 집중하고 있는 것으로 결 정될 수 있다. 예를 들어, 사용자가 바라보는 방향이 학습 물품의 적어도 일부를 향하는 경우, 서버는 사용자가 해당 이 미지 프레임(ex. 310) 상에서 집중하고 있는 것으로 판단할 수 있다. 한편, 일 실시 예로, 서버는 학습 물품을 “보는 물품”(“thing to see”)과 “보지 않는 물품”(“thing not to see”)으로 구분하여 인식할 수 있다. 여기서, 보는 물품은, 학습 과정에서 사용자가 주로 보는 물품으 로, 시각적으로 확인될 수 있는 정보를 포함하거나 정보가 추가될 수 있는 노트북 PC, 태블릿 PC, 노트, 책 등 에 해당한다. 보지 않는 물품은, 학습 과정에서 사용자가 주로 보는 것은 아닌 물품으로, 펜, 연필, 지우개 등 에 해당한다. 이를 위해, 서버는 객체 인식 모델을 통해 인식된 각 객체를 “보는 물품”과 “보지 않는 물품”으로 구분할 수 있다. 예를 들어, 도 3a 내지 도 3b에서, 노트북 PC와 노트는 보는 물품이지만 펜은 보지 않는 물품에 해당한다. 여기서, 서버는, 사용자가 “보는 물품”의 적어도 일부를 바라보는 상황을 전제로, 사용자가 집중하고 있 는 것으로 결정할 수 있다. 또는, 서버는 “보는 물품”이든 “보지 않는 물품”이든 상관없이 사용자가 학습 물품을 바라보는 동안 사용자가 집중하고 있는 것으로 결정할 수도 있다. 다만, 이 경우에도, 사용자가 “보지 않는 물품”을 바라보 는 시간이 임계 시간 이상 지속된다면, 사용자가 집중하고 있지 않은 것으로 식별될 수 있다. 상술한 실시 예들 중 적어도 하나에 따라 시간 별 집중 여부가 결정되면, 서버는 시간 별(이미지 프레임 별)로 결정된 사용자의 집중 여부를 바탕으로 사용자의 집중 시간을 산출할 수 있다. 구체적인 예로, 서버(10 0)는 집중하고 있는 사용자를 포함하는 이미지 프레임의 수를 바탕으로 사용자의 집중시간을 산출할 수 있다. 또한, 일 실시 예로, 서버는 사용자의 얼굴이 포함된 눈이 감긴 시간을 바탕으로 집중 여부를 식별할 수 있다. 예를 들어, 설령 사용자의 얼굴이 학습 물품(ex. 노트북)을 향하더라도, 사용자의 눈이 연속적으로 감겨있다면 집중하고 있지 않은 것으로 식별될 수 있다. 구체적으로, 서버는 사용자의 얼굴에 포함된 눈을 추출하는 한편, 복수의 프레임 각각에 대하여, 추출된 눈이 감겨 있는지 여부를 식별할 수 있다. 여기서, 눈이 임계 시간 이상 연속적으로 감겨 있는 시간 구간에 대 해서는, (사용자의 얼굴이 학습 물품을 향하고 있더라도) 사용자가 집중하지 않는 것으로 식별될 수 있다. 이는, 집중 시간의 산출에 반영될 수 있다. 또한, 눈이 감겨 있는지 여부를 식별하는 과정에서, 서버는 졸음 감지도 가능하다. 구체적으로, 눈이 일정 시간 이상 연속적으로 감겨 있는 경우, 서버는 사용자의 졸음을 감지하고, 졸음을 깨우기 위한 알림 사운 드 또는 알림 메시지를 출력하도록 사용자 단말을 제어할 수 있다. 한편, 일 실시 예로, 서버는 사용자가 집중하고 있는 동안에도 집중도라는 추가적인 항목의 수치를 추출/ 저장할 수 있다. 집중도는, 사용자가 집중하고 있는 정도를 의미하는 개념이다. 일 실시 예로, 서버는 사용자의 몸통이 향하는 방향 및/또는 사용자의 자세를 바탕으로 집중도를 추출할 수 있다. 관련하여, 도 4는 본 개시의 일 실시 예에 따른 서버가 실시간으로 촬영된 이미지 내에서 사용자의 몸통이 바라 보는 방향을 식별하는 동작을 설명하기 위한 도면이다. 도 4를 참조하면, 서버는 몸통을 구성하는 각 신체부위(ex. 어깨 끝 지점, 옆구리 지점)에 해당하는 지점 을 추출하고, 또한 몸통과 관련된 각 신체부위(ex. 팔의 각 관절)에 해당하는 지점을 추출할 수 있다. 이때, 서 버는 각 지점 간의 상대적 위치 관계를 바탕으로 몸통이 바라보는 방향을 식별할 수 있다. 여기서, 서버는 몸통이 바라보는 방향을 현재 사용자가 바라보는 학습 물품의 위치와 비교할 수 있다. 여 기서, 몸통이 바라보는 방향이 학습 물품의 위치와 멀어질수록(ex. 각도 차이가 클수록), 집중도는 낮게 산출될 수 있다. 또한, 서버는 몸통이 바라보는 방향을 바탕으로 사용자의 자세를 식별할 수도 있다. 예를 들어, 얼굴이 바 라보는 방향과 몸통이 바라보는 방향이 수평으로 일정 각도 이상 차이를 가지는 경우, 서버는 사용자 단말 을 통해 '바른 자세'를 유발하는 요청 메시지를 제공할 수 있다. 해당 메시지를 확인한 사용자가 자세를 고쳐 앉는 상황이 기대될 수 있다. 또한, 서버는 이미지 프레임(ex. RGB 이미지) 내에서 몸통의 각 지점의 뎁스(depth)를 식별할 수 있다. 이 를 위해, RGB 이미지 내 객체의 각 표면 지점까지의 거리를 추측하도록 훈련된 적어도 하나의 인공지능 모델이 이용될 수 있다. 이 경우, 서버는 어깨에 해당하는 각 지점의 제1 뎁스와, 옆구리에 해당하는 각 지점의 제2 뎁스를 비교하여, 자세가 올바른지 여부를 식별할 수 있다. 구체적으로, 서버는 제1 뎁스와 제2 뎁스 간의 차이를 바탕으로 몸통이 세워진 각도를 산출할 수 있으며, 해당 각도가 수직 방향(ex. 지면으로부터 수 직)과 일정 각도 이상 차이를 가지는 경우, 서버는 사용자 단말을 통해 '바른 자세'를 유발하는 요청 메시 지를 제공할 수 있다. 한편, 촬영된 이미지 프레임 내에서 학습 물품이 식별되지 않는 경우도 발생할 수 있다. 구체적으로, 복수의 이 미지 프레임 중 일정 수 이상의 연속된 이미지 프레임 내에서 사용자는 식별되고 학습 물품은 식별되지 않는 경 우, 서버는 사용자 단말을 통해 사용자로 하여금 (촬영 범위를 벗어나지만 사용자 주변에 위치하고 있는) 학습 물품을 바라보도록 하는 요청을 출력할 수 있다. 또는, 복수의 이미지 프레임 중 일정 수 이상의 연속된 이미지 프레임 내에서 사용자는 식별되지만 '보는 물 품'이 식별되지 않는 경우, 서버는 사용자 단말을 통해 사용자로 하여금 학습 물품을 바라보도록 하 는 요청을 출력할 수 있다. 그리고, 서버는 요청이 출력된 이후 촬영된 적어도 하나의 이미지 프레임 내에서 사용자가 바라보는 목표 방향을 식별할 수 있다. 이후, 서버는 실시간으로 사용자가 바라보는 방향과 목표 방향을 비교하여, 사용 자의 집중 여부를 시간 별로 결정할 수 있다. 구체적으로, 사용자가 바라보는 방향이 목표 방향으로부터 일정 각도 이상 벗어나지 않음을 전제로, 사용자가 집중하고 있는 것으로 식별될 수 있다. 한편, 상술한 다양한 실시 예들과 같이 집중 여부를 식별하고 집중 시간을 산출하는 과정에서, 서버는 적 어도 하나의 웹 페이지 내지 애플리케이션을 통해 사용자 단말을 제어하여 다양한 UI(User Interface)를 제공할 수 있다. 그리고, 서버는 사용자 단말이 제공하는 UI를 통해 수신되는 사용자 입력을 바탕으로 각 기능을수행할 수 있다. 도 5는 본 개시의 일 실시 예에 따라 공부 영상의 촬영과 관련된 UI(User Interface) 항목들을 제공하는 사용자 단말의 화면을 도시한 도면이다. 도 5의 화면은, 서버가 제공하는 애플리케이션 및/또는 소프트웨어 프로 그램이 실행된 사용자 단말의 화면의 예시에 해당한다. 도 5를 참조하면, 사용자 단말은 UI를 통해 해당 애플리케이션에 접속한 사용자의 아이디(또는 닉네 임), 이메일 등을 표시할 수 있다. 또한, 사용자 단말은 UI를 통해 사용자의 공부 시간 또는 집중 시간을 표시(UI 내 시계 모양의 아이콘)할 수 있으며, 다른 사용자가 (로그인 된) 사용자의 비디오에 표현한 호감 정도를 표시(UI 내 커피 모양의 아이콘)할 수 있다. 또한, 사용자 단말은 사용자가 공부하는 모습이 촬영된 횟수, 사용자 입력에 따라 설정된 목표에 대한 달 성 정도 등을 각각 표시할 수 있다(UI(553, 554)). 또한, 사용자 단말은 달력 모양의 UI를 제공하는 한편, 일자 별로 저장된 사용자의 공부 기록(ex. 촬 영된 비디오, 공부 시간, 집중 시간, 학습 과목 등)을 제공할 수 있다. 또한, 사용자 단말은 UI를 통해 오늘의 최대 집중시간, 누적공부횟수, 누적 공부시간 등에 대한 정보 를 제공할 수 있다. 최대 집중시간은, 사용자가 연속하여 집중한 최대 시간에 해당하고, 누적 공부횟수는 사용 자가 공부한 횟수에 해당하며, 누적 공부시간은 하루 동안 공부한 시간이 모두 합산된 것이다. 관련하여, 서버는 촬영된 비디오의 시간 길이를 바탕으로 사용자의 학습 시간을 식별할 수 있다. 또한, 서 버는 사용자 단말로부터 수신된 비디오의 개수를 바탕으로 사용자의 학습 횟수를 식별할 수 있다. 즉, UI 내에서, 누적 공부횟수는 비디오가 촬영된 횟수에 해당하며, 누적 공부시간은 비디오들의 촬영 시간이 모두 합산된 결과일 수 있다. 다만, 일 실시 예로, 서버는 비디오 내에서 사용자가 존재하지 않는 시간에 대해서는 공부 시간에 반영하 지 않을 수 있다. 구체적으로, 서버는 복수의 이미지 프레임 중 사용자가 식별되지 않은 이미지 프레임의 수를 바탕으로 사용자의 학습 시간을 차감할 수 있다. 즉, 전체 비디오의 시간 길이에서 사용자가 등장하지 않 는 시간 구간만큼은 차감되어 공부 시간, 누적 공부시간 등이 산출될 수 있다. 도 5를 참조하면, 사용자 단말은 홈 메뉴, 캘린더 메뉴, 소셜 메뉴, 기능 활성화 메뉴 등을 기본 메뉴로 제공할 수 있다. 홈 메뉴가 선택됨에 따라 사용자 단말은 도 5와 같은 화면을 제공할 수 있다. 타임랩스(: 비디오 촬 영 및 기록)를 시작하기 위한 항목이 선택되면, 비디오의 촬영 기능을 실행하기 위한 화면이 제공될 수 있 으며, 학습과목을 추가하기 위한 항목이 선택되면, 촬영 대상이 되는 학습의 과목이 추가될 수 있다. 타임랩스 시작하기 항목이 선택된 이후 사용자 단말은 도 6a와 같은 화면을 제공할 수 있다. 도 6a를 참조하면, 사용자 단말의 카메라가 활성화되어 실시간으로 이미지 프레임(비디오)이 촬영될 수 있 다. 이때, 사용자 단말은 실시간으로 촬영된 이미지를 화면 상에 표시할 수 있다. 도 6a를 참조하면, UI를 통해 현재 학습 중인 과목(ex. 세무회계)이 표시될 수 있으며, 그 밖에 목표 학습 시간(ex. 1시간), 목표 학습시간에 대한 실제 공부 시간(ex. 촬영 시간)이 표시될 수 있다. 여기서, 학습 중인 과목은 사용자 입력에 따라 변경 설정될 수 있으며, 이후 촬영된 결과에 따라 과목 별 공부 시간/집중 시간 등 이 기록될 수 있다. '사진 찍기' 항목이 선택되면, 현재 표시되는 이미지에 대한 캡쳐 및 저장이 진행될 수 있다. '녹화 시작' 항목이 선택되면, 순차적으로 촬영된 이미지 프레임으로 구성된 비디오가 저장될 수 있으며, 실시간으로 촬영된 이미지 프레임이 서버 또는 사용자 단말을 통해 분석될 수 있다. '녹화 시 작' 항목이 선택되어 비디오가 표시 및 생성되는 동안, 해당 항목은 '녹화 종료'로 변경될 수 있으며, '녹 화 종료'가 선택되면 비디오 촬영이 종료될 수 있다. 촬영된 이미지 프레임들은 실시간으로 서버로 전송될 수 있으며, 상술한 S210 내지 S240의 과정에 따라 분석될 수 있다. 한편, 이러한 과정이 사용자 단말에서 자체적으로 모두 수행될 수도 있는 바, 도 10을 통해 후술한다. 한편, 도 6a를 참조하면, 다양한 부가 기능과 관련된 UI 항목들(611, 612, 613, 614)이 제공될 수 있다. '같이 공부'는 사용자 본인의 과거의 학습이 촬영된 비디오 또는 다른 사용자(ex. 친구 등록된 사용자)의 비디오를 제공하기 위한 기능이다. 예를 들어, 서버는, 사용자 단말에 (현재 촬영되고 있는) 복수의 프레 임이 실시간으로 표시되는 동안, 사용자 입력에 따라 선택된 다른 비디오를 함께 표시하도록 사용자 단말 을 제어할 수 있다. 그 결과, 도 6b와 같이 다른 비디오(611')가 함께 제공될 수 있다. 이 경우, 사용자는 본인 또는 다른 사용자가 공부한 모습(611')을 보면서 함께 공부하는 느낌을 받을 수 있다. '페이스 필터'는 실시간으로 촬영 및 표시되는 사용자의 얼굴의 적어도 일부를 가리기 위한 기능이다. 예 를 들어, 사용자의 얼굴의 일부를 가리는 마스크나 안경과 같은 가상 이미지가 입혀질 수 있다. 이 경우, 본인 의 얼굴이 온전히 촬영되어 비디오로 생성되는 상황을 피하고 싶은 사용자의 의도가 반영될 수 있다. 일 예로, 비디오를 구성하는 이미지 프레임을 분석하여 사용자의 집중 여부를 식별하는 과정은 서버 상에 서 수행되지만, '페이스 필터' 기능은 사용자 단말 상에서 수행될 수도 있다. 구체적으로, 사용자 단말 은 사용자의 얼굴을 식별하기 위한 적어도 하나의 객체 인식 모델을 포함할 수 있으며, 이미지 프레임 내 에서 사용자의 얼굴의 적어도 일부를 상술한 가상 이미지로 오버랩한 이후, 오버랩 된 이미지 프레임을 실시간 으로 사용자 단말의 디스플레이에 표시할 수 있다. 그 결과, 도 6a의 화면 속 이미지 내에서 사용자 의 얼굴의 적어도 일부가 가려지게 된다. '페이스 필터' 기능이 활성화된 채로 녹화가 진행된 경우, 서버 는 오버랩 된 이미지 프레임으로 구성된 비디오를 등록할 수 있다. 이후 사용자 또는 다른 사용자가 해당 비디 오를 재생하는 경우, '페이스 필터'에 따른 오버랩이 적용된 상태로 복수의 이미지 프레임이 출력될 수 있다. '배경 가리기'는 이미지 프레임 내에서 사용자를 제외한 나머지 배경 부분의 이미지 값을 변경하여 배경 부분이 식별되도록 하지 않는 기능이다. '화면 잠금'은, 실시간으로 비디오가 촬영되는 동안, 촬영된 순차적인 이미지 프레임이 사용자 단말 을 통해 표시되지는 않도록 비활성화하는 기능이다. 공부하는 본인이 촬영되고 있는 모습에 신경쓰기보다는, 온 전히 공부에만 집중했다가 비디오는 추후에 확인하고 싶은 사용자의 의도가 반영될 수 있다. 한편, 도 5에서 캘린더 메뉴는 일자 별 공부량과 관련된 다양한 데이터를 관리하기 위한 메뉴이다. 관련하 여, 도 7은, 캘린더 메뉴가 선택된 상황의 사용자 단말의 화면의 예시를 도시한 것이다. 도 7을 참조하면, 사용자 단말을 통해 캘린더가 제공될 수 있으며, 캘린더 내 적어도 하나의 일자가 선택 되는 경우, 해당 일자의 누적 공부시간, 과목 별 공부시간, 집중 시간 등이 제공될 수 있다. 한편, 도 5에서 소셜 메뉴는 사용자 간에 비디오를 공유하고 소통하기 위한 메뉴이다. 관련하여, 도 8은, 소셜 메뉴가 선택된 상황의 사용자 단말의 화면의 예시를 도시한 것이다. 도 8을 참조하면, UI를 통해 다양한 사용자의 비디오의 썸네일이 제공되는 한편 선택된 비디오가 재생될 수 있다. 썸네일은, 사용자의 비디오를 구성하는 복수의 이미지 프레임 중 상술한 집중도가 가장 높았던 시점에 해당하는 이미지 프레임으로 자동으로 채택될 수 있다. 도 8을 참조하면, 재생되는 비디오는 UI 상에서 제공될 수 있다. UI는 해당 비디오의 사용자에 대한 정보(ex. 아이디, 닉네임 등), 비디오 재생 화면, 호감 표시 항목, 비디오 시간(822': 공부 시간) 등 을 포함할 수 있다. 예를 들어, 도 8의 비디오 재생 화면을 통해 재생되는 비디오가 사용자 A의 사용자 단말을 통해 촬영된 비 디오인 경우를 가정한다. 여기서, 촬영된 비디오는 과거에 촬영된 것일 수 있으나, 실시간으로 촬영되고 있는 비디오에 해당할 수도 있다. 이 경우, 사용자 단말의 사용자인 사용자 B는, 호감 표시 항목을 선택하여 사용자 A 및/또는 사용자 A의 비디오에 대한 호감을 표시할 수 있다. 이때, 서버는 사용자 B의 호감 표시를 바탕으로 사용자 A를 위한 쿠폰 정보를 생성하여 사용자 A의 사용자 단말로 제공할 수 있다. 예를 들어, 사용자 A에 대하여 한 명 이상의 사용자가 호감을 표시할 때마다 호감도가 누적될 수 있고, 누적된 호감도가 임계치 이상이 됨에 따라 사용자 B를 위한 쿠폰 정보(ex. 커피, 스낵 등 다양 한 상품에 대한 쿠폰 등)가 생성될 수 있다. 그 결과, 사용자들은 서로 간에 공부하는 모습을 공유하며 호감을 표시하고, 간접적으로 선물을 제공할 수도 있게 된다. 또한, 일 실시 예로, 호감 표시의 대상이 되는 사용자의 실시간 공부 상태에 따라 호감도의 누적 수치가 달라지 는 것도 가능하다. 예를 들어, 사용자 A가 공부하는 모습을 촬영 중이지 않은 상태(ex. 서버가 제공하는 애플리케이션을 통해 사용자 단말의 카메라가 활성화되지 않음)에서 사용자 B의 호감 표시가 입력된 경우, 제1 호감도 가 누적되고, 사용자 A가 공부하는 모습을 촬영 중인 상태(ex. 서버가 제공하는 애플리케이션을 통해 사용자 단 말의 카메라가 활성화됨)에서 사용자 B의 호감 표시가 입력된 경우, 제1 호감도보다 큰 제2 호감도가 누적될 수 있다. 또한, 사용자가 공부하는 모습을 촬영 중인 상태 중에서도, 사용자가 집중하지 않는 것으로 식별된 시점에 호감 표시가 입력된 경우보다 사용자가 집중하고 있는 것으로 식별된 시점에 호감 표시가 입력된 경우에 더 큰 호감 도가 누적될 수 있다. 이렇듯, 본 개시에 따른 서버 내지 사용자 단말은 쿠폰 생성 등의 이벤트를 통 해 각 사용자의 공부 및 집중에 대한 의욕을 고취시킬 수 있다. 한편, 도 5에서 기능 활성화 메뉴는 비디오의 촬영 내지는 기타 서비스의 이용과 관련된 다양한 기능 각각 에 대하여 개별적으로 활성화 여부에 대한 설정을 하기 위한 메뉴이다. 예를 들어, 해당 메뉴가 선택된 이후 제 공되는 UI를 통해 공부 시간의 측정 기능, 집중 시간의 측정 기능, 학습 자세에 대한 트래킹 기능, 졸음 감지 트래킹 기능, 함께 공부 기능, 스터디 플래너 기능, 페이스필터 기능, 배경 가리기 기능 등 다양한 기능 각각에 대한 활성화/비활성화 여부가 사용자 입력에 따라 설정될 수 있다. 도 9a는 본 개시의 일 실시 예에 따른 서버의 구성을 설명하기 위한 블록도이다. 도 9a를 참조하면, 서버 는 메모리, 통신부, 프로세서 등을 포함할 수 있다. 메모리는 서버의 구성요소들의 전반적인 동작을 제어하기 위한 운영체제(OS: Operating System) 및 전자 장치의 구성요소와 관련된 적어도 하나의 인스트럭션 또는 데이터를 저장하기 위한 구성이다. 메모리는 ROM, 플래시 메모리 등의 비휘발성 메모리를 포함할 수 있으며, DRAM 등으로 구성된 휘발성 메모 리를 포함할 수 있다. 또한, 메모리는 하드 디스크, SSD(Solid state drive) 등을 포함할 수도 있다. 통신부는 다양한 유무선 통신방식으로 적어도 하나의 외부 장치와 통신을 수행하기 위한 회로, 모듈, 칩 등을 포함할 수 있다. 통신부는 다양한 유무선 통신방식으로 적어도 하나의 외부 장치와 통신을 수행하기 위한 회로, 모듈, 칩 등을 포함할 수 있다. 통신부는 다양한 네트워크를 통해 외부 장치와 연결될 수 있다. 네트워크는 영역 또는 규모에 따라 개인 통신망(PAN; Personal Area Network), 근거리 통신망(LAN; Local Area Network), 광역 통신망(WAN; Wide Area Network) 등일 수 있으며, 네트워크의 개방성에 따라 인트라넷 (Intranet), 엑스트라넷(Extranet), 또는 인터넷(Internet) 등일 수 있다. 통신부는 LTE(long-term evolution), LTE-A(LTE Advance), 5G(5th Generation) 이동통신, CDMA(code division multiple access), WCDMA(wideband CDMA), UMTS(universal mobile telecommunications system), WiBro(Wireless Broadband), GSM(Global System for Mobile Communications), DMA(Time Division Multiple Access), WiFi(Wi-Fi), WiFi Direct, Bluetooth, NFC(near field communication), Zigbee 등 다양한 무선 통 신 방식을 통해 외부 장치들과 연결될 수 있다. 또한, 통신부는 이더넷(Ethernet), 광 네트워크(optical network), USB(Universal Serial Bus), 선더볼트 (ThunderBolt) 등의 유선 통신 방식을 통해 외부 장치들과 연결될 수도 있다. 이 밖에도, 통신부는 향후 새롭게 고안되는 다양한 통신 방식/기술을 활용한 구성일 수 있다. 프로세서는 서버를 전반적으로 제어하기 위한 구성이다. 구체적으로, 프로세서는 메모리와 연결되는 한편 메모리에 저장된 적어도 하나의 인스트럭션을 실행함으로써 본 개시의 다양한 실시 예들에 따른 동작을 수행할 수 있다. 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit) 등과 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서 등을 포함할 수 있 다. 인공지능 전용 프로세서는, 특정 인공지능 모델의 훈련 내지는 이용에 특화된 하드웨어 구조로 설계될 수 있다. 한편, 서버는 다양한 사용자의 사용자 단말(200-1, 2, 3, …) 외에 독서실, 카페, 도서관 등 특정 장소에 설치된 하나 이상의 카메라 장치와 통신을 수행할 수도 있다. 예를 들어, 독서실의 자리 별로 설치되어 각 자리에서 학습하는 사람을 촬영하기 위한 카메라 장치가 상술한 서 버와 유/무선 통신을 수행할 수 있으며, 이 경우 각 자리 별로 촬영된 비디오가 서버로 전송될 수 있 다. 구체적인 예로, 일 사용자의 사용자 단말(200-1)은 서버의 웹 페이지 및/또는 애플리케이션에 접속하여 특 정 자리에 대한 사용 예약 또는 사용 요청을 입력할 수 있다. 이때, 사용 시간(ex. 시작 시간, 종료 시간)이 설 정될 수 있다. 이 경우, 서버는 해당 자리에 설치된 카메라 장치의 촬영 기능을 활성화할 수 있으며, 카메라 장치에 의해 실시간으로 촬영된 복수의 이미지 프레임을 수신하여 사용자의 공부 시간, 집중 시간 등을 분석할 수 있다. 그 리고, 서버는 촬영된 비디오(복수의 이미지 프레임), 공부 시간, 집중 시간 등에 대한 데이터를 사용자의 사용자 단말(200-1)로 제공할 수 있다. 이렇듯, 본 개시에 따른 서버의 동작 방법이 특정 장소(ex. 독서실, 카페, 도서관 등)와 연동되어 다양한 사용자의 학습을 모니터링하는 것도 가능하다. 한편, 도 9b는 본 개시의 일 실시 예에 따른 사용자 단말의 구성을 설명하기 위한 블록도이다. 도 9b를 참조하 면, 사용자 단말은 메모리, 통신부, 프로세서, 카메라, 출력부, 사용자 입력부 등을 포함할 수 있다. 메모리는 사용자 단말의 구성요소들의 전반적인 동작을 제어하기 위한 운영체제(OS: Operating System) 및 사용자 단말의 구성요소와 관련된 적어도 하나의 인스트럭션 또는 데이터를 저장하기 위한 구 성이다. 메모리는 ROM, 플래시 메모리 등의 비휘발성 메모리를 포함할 수 있으며, DRAM 등으로 구성된 휘 발성 메모리를 포함할 수 있다. 또한, 메모리는 하드 디스크, SSD(Solid state drive) 등을 포함할 수도 있다. 통신부는 다양한 유무선 통신방식으로 적어도 하나의 외부 장치와 통신을 수행하기 위한 회로, 모듈, 칩 등을 포함할 수 있다. 통신부는 다양한 유무선 통신방식으로 적어도 하나의 외부 장치와 통신을 수행하기 위한 회로, 모듈, 칩 등을 포함할 수 있다. 통신부는 다양한 네트워크를 통해 외부 장치와 연결될 수 있다. 네트워크는 영역 또는 규모에 따라 개인 통신망(PAN; Personal Area Network), 근거리 통신망(LAN; Local Area Network), 광역 통신망(WAN; Wide Area Network) 등일 수 있으며, 네트워크의 개방성에 따라 인트 라넷(Intranet), 엑스트라넷(Extranet), 또는 인터넷(Internet) 등일 수 있다. 통신부는 LTE(long-term evolution), LTE-A(LTE Advance), 5G(5th Generation) 이동통신, CDMA(code division multiple access), WCDMA(wideband CDMA), UMTS(universal mobile telecommunications system), WiBro(Wireless Broadband), GSM(Global System for Mobile Communications), DMA(Time Division Multiple Access), WiFi(Wi-Fi), WiFi Direct, Bluetooth, NFC(near field communication), Zigbee 등 다양한 무선 통신 방식을 통해 외부 장치들과 연결될 수 있다. 또한, 통신부는 이더넷(Ethernet), 광 네트워크(optical network), USB(Universal Serial Bus), 선더볼트(ThunderBolt) 등의 유선 통신 방식을 통해 외부 장치들과 연결될 수도 있다. 이 밖에도, 통신부는 향후 새롭게 고안되는 다양한 통신 방식/기술을 활용한 구성일 수 있다. 프로세서는 사용자 단말을 전반적으로 제어하기 위한 구성이다. 구체적으로, 프로세서는 메모리 와 연결되는 한편 메모리에 저장된 적어도 하나의 인스트럭션을 실행함으로써 본 개시의 다양한 실시 예들에 따른 동작을 수행할 수 있다. 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범 용 프로세서, GPU, VPU(Vision Processing Unit) 등과 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서 등을 포함할 수 있다. 인공지능 전용 프로세서는, 특정 인공지능 모델의 훈련 내지는 이용에 특 화된 하드웨어 구조로 설계될 수 있다. 카메라는 사용자 단말의 주변을 촬영하기 위한 구성으로, 하나 이상의 이미지 센서를 포함할 수 있다. 카메라는 RGB 카메라, 스테레오 카메라, 뎁스 카메라 중 적어도 하나를 포함할 수 있다. 일 실시 예 로, 사용자 단말은 카메라를 통해 사용자를 촬영하여 비디오를 생성할 수 있다. 출력부는 다양한 정보를 시각적/청각적으로 출력하기 위한 구성이다. 출력부는 디스플레이, 스피커, 오디오 단자(이어폰/헤드셋용) 등을 포함할 수 있다. 사용자 입력부는 다양한 사용자 명령 내지는 정보를 입력 받기 위한 구성으로, 터치 패널로 구성된 디스플 레이와 연동될 수 있으며, 다양한 버튼, 마이크 등을 포함할 수 있으나, 이에 한정되지 않는다. 한편, 상술한 실시 예들의 경우, 도 2의 S220 내지 S240으로 설명된 이미지의 분석 과정이 서버를 통해 진 행되는 경우를 주로 설명하였다. 이는, 사용자 단말 상에서 해당 분석 과정을 수행하는 경우 사용자 단말 에 너무 많은 로드가 생겨 문제가 생길 수 있기 때문이다. 다만, 사용자 단말의 성능이나 사양이 점차 좋아지는 추세를 고려하면 상술한 분석 과정이 사용자 단말 자체에서 진행(: 엣지 컴퓨팅)되더라도 로드 상 문제가 되지 않을 수 있다. 이 경우 서버로 실시간 으로 촬영된 이미지 프레임을 전송하지 않아도 된다는 점에서 통신 로드가 감소하고, 서버 자체의 로드가 감소되며, 개인 촬영물 내지 생활 기록과 관련된 보안이 강화된다는 장점이 있다. 따라서, 도 10을 통해, 본 개시의 일 실시 예에 따라 엣지 컴퓨팅을 수행하는 사용자 단말의 동작을 설명한다. 도 10을 참조하면, 사용자 단말은 상술한 도 6a 등의 화면을 제공하면서 사용자 입력(ex. 녹화 시작)에 따 라 비디오를 촬영할 수 있다(S1010). 이때, 사용자 단말은 실시간으로 촬영된 이미지 프레임을 표시할 수 있다. 또한, 사용자 단말은 비디오를 구성하는 복수의 이미지 프레임 각각에서, 사용자 및 학습 물품을 식별할 수 있다(S1020). 이때, 사용자 단말은 복수의 이미지 프레임 각각에서 사용자가 바라보는 방향을 추출할 수 있으며(S1030), 사용자가 바라보는 방향과 학습 물품의 위치를 바탕으로 사용자의 집중 여부를 시간 별로 결정할 수 있다 (S1040). 한편, 이상에서 설명된 다양한 실시 예들은 서로 저촉되거나 모순되지 않는 한 두 개 이상의 실시 예가 서로 결 합되어 구현될 수 있다. 한편, 이상에서 설명된 다양한 실시 예들은 소프트웨어(software), 하드웨어(hardware) 또는 이들의 조합된 것 을 이용하여 컴퓨터(computer) 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 하드웨어적인 구현에 의하면, 본 개시에서 설명되는 실시 예들은 ASICs(Application Specific Integrated Circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 프로세서(processors), 제어기 (controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서(microprocessors), 기타 기능 수행 을 위한 전기적인 유닛(unit) 중 적어도 하나를 이용하여 구현될 수 있다. 일부의 경우에 본 명세서에서 설명되는 실시 예들이 프로세서 자체로 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들은 별도의 소프트웨어 모듈들로 구현될 수 있다. 상술한 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 작동을 수행할 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 로봇, 서버 등 전자 장치에서의 처리동작을 수행하기 위한 컴 퓨터 명령어(computer instructions) 또는 컴퓨터 프로그램은 비일시적 컴퓨터 판독 가능 매체(non-transitory computer-readable medium)에 저장될 수 있다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령어 또는 컴퓨터 프로그램은 특정 기기의 프로세서에 의해 실행되었을 때 상술한 다양한 실시 예에 따른 서버, 사용 자 단말 등에서의 처리 동작을 상술한 특정 기기가 수행하도록 한다. 비일시적 컴퓨터 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체 가 아니라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 비일시적 컴퓨터 판독 가능 매체의 구체적인 예로는, CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등 이 있을 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2023-0046579", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2023-0046579", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시 예에 따라 다양한 사용자 단말과 통신을 수행하는 서버의 동작을 설명하기 위한 블록 도, 도 2는 본 개시의 일 실시 예에 따른 서버의 동작 방법을 설명하기 위한 흐름도, 도 3a는 본 개시의 일 실시 예에 따른 서버가 실시간으로 촬영된 이미지 내에서 사용자 및 학습 물품을 식별하 는 동작을 설명하기 위한 도면, 도 3b는 본 개시의 일 실시 예에 따른 서버가 실시간으로 촬영된 이미지 내에서 사용자의 얼굴이 바라보는 방향 을 식별하는 동작을 설명하기 위한 도면, 도 4는 본 개시의 일 실시 예에 따른 서버가 실시간으로 촬영된 이미지 내에서 사용자의 몸통이 바라보는 방향 을 식별하는 동작을 설명하기 위한 도면, 도 5는 본 개시의 일 실시 예에 따라 공부 영상의 촬영과 관련된 UI(User Interface) 항목들을 제공하는 사용자 단말의 화면을 도시한 도면, 도 6a 내지 도 6b는 본 개시의 일 실시 예에 따른 사용자 단말이 사용자가 공부하는 모습을 촬영하는 기능을 제 공하는 과정을 설명하기 위한 도면들, 도 7은 본 개시의 일 실시 예에 따라 공부 일정과 관련된 UI 항목들을 제공하는 사용자 단말의 화면을 도시한 도면, 도 8은 본 개시의 일 실시 예에 따라 다른 사용자들의 공부 영상을 제공하는 사용자 단말의 화면을 도시한 도면, 도 9a는 본 개시의 일 실시 예에 따른 서버의 구성을 설명하기 위한 블록도, 도 9b는 본 개시의 일 실시 예에 따른 사용자 단말의 구성을 설명하기 위한 블록도, 그리고 도 10은 본 개시의 일 실시 예에 따라 엣지 컴퓨팅을 수행하는 사용자 단말의 동작을 설명하기 위한 흐름도이다."}
