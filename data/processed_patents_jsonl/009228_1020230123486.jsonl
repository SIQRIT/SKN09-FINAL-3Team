{"patent_id": "10-2023-0123486", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0040434", "출원번호": "10-2023-0123486", "발명의 명칭": "전자 장치 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "최유나"}}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,센싱 데이터를 획득하는 센서부;메모리;프로젝션부; 및적어도 하나의 프로세서;를 포함하고,상기 적어도 하나의 프로세서는,상기 센싱 데이터에 기초하여 식별된 사용자의 얼굴 영역의 크기 정보를 획득하고,상기 얼굴 영역의 크기 정보에 기초하여 상기 전자 장치의 위치와 상기 사용자의 위치 사이의 거리에 대응하는상기 얼굴 영역의 거리 정보를 획득하고,상기 얼굴 영역의 거리 정보에 기초하여 컨텐츠 이미지가 출력되도록 상기 프로젝션부를 제어하는, 전자 장치."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 적어도 하나의 프로세서는,상기 얼굴 영역의 가로 길이, 상기 얼굴 영역의 세로 길이, 상기 얼굴 영역의 넓이 중 적어도 하나를 포함하는상기 크기 정보를 획득하는, 전자 장치."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 적어도 하나의 프로세서는,상기 센싱 데이터에 기초하여 사용자의 얼굴 영역이 식별되면, 상기 얼굴 영역의 좌표 정보를 획득하고,상기 좌표 정보에 기초하여 상기 크기 정보를 획득하는, 전자 장치."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 적어도 하나의 프로세서는,상기 거리 정보에 기초하여 상기 프로젝션부를 통해 출력되는 이미지의 크기를 나타내는 투사 해상도 정보를 획득하고,상기 투사 해상도 정보에 기초하여 상기 컨텐츠 이미지가 출력되도록 상기 프로젝션부를 제어하는, 전자 장치."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 거리 정보는 제1 거리 정보이고,상기 적어도 하나의 프로세서는,상기 전자 장치의 위치와 투사면 사이의 거리를 나타내는 제2 거리 정보를 획득하고,공개특허 10-2025-0040434-3-상기 제1 거리 정보 및 상기 제2 거리 정보에 기초하여 상기 투사 해상도 정보를 획득하는, 전자 장치."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 메모리는,투사 비율 범위를 저장하고,상기 적어도 하나의 프로세서는,상기 투사 비율 범위 및 상기 제2 거리 정보에 기초하여 해상도 범위를 획득하고,상기 제1 거리 정보에 기초하여 상기 해상도 범위 중 상기 투사 해상도 정보를 획득하는, 전자 장치."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4항에 있어서,상기 적어도 하나의 프로세서는, 컨텐츠 해상도 정보 및 상기 투사 해상도 정보에 기초하여 스케일링 계수를 획득하고,상기 스케일링 계수에 기초하여 상기 컨텐츠 이미지를 스케일링하여 투사 이미지를 획득하고,상기 투사 이미지를 출력하도록 상기 프로젝션부를 제어하는, 전자 장치."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 적어도 하나의 프로세서는,상기 센싱 데이터에 기초하여 제1 얼굴 영역 및 제2 얼굴 영역이 식별되면, 상기 제1 얼굴 영역의 제1 크기 정보 및 상기 제2 얼굴 영역의 제2 크기 정보를 획득하고,상기 제1 얼굴 영역의 제1 크기 정보에 기초하여 상기 제1 얼굴 영역의 제1 거리값을 획득하고,상기 제2 얼굴 영역의 제2 크기 정보에 기초하여 상기 제2 얼굴 영역의 제2 거리값을 획득하고,상기 제1 거리값 및 상기 제2 거리값에 기초하여 대표 거리값을 획득하고,상기 대표 거리값을 포함하는 상기 거리 정보를 획득하고,상기 거리 정보에 기초하여 상기 컨텐츠 이미지를 출력하도록 상기 프로젝션부를 제어하는, 전자 장치."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 메모리는,학습된 적어도 하나의 가중치를 포함하는 인공 지능 모델을 저장하고,상기 적어도 하나의 프로세서는,상기 크기 정보를 상기 인공 지능 모델에 입력하여 상기 거리 정보를 출력 데이터로써 획득하는, 전자 장치."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 적어도 하나의 가중치는,RGB 이미지를 포함하는 제1 학습 데이터 및 뎁스 이미지를 포함하는 제2 학습 데이터에 기초하여 학습된, 전자장치.공개특허 10-2025-0040434-4-청구항 11 센싱 데이터를 획득하는 전자 장치의 제어 방법에 있어서,상기 센싱 데이터에 기초하여 식별된 사용자의 얼굴 영역의 크기 정보를 획득하는 단계;상기 얼굴 영역의 크기 정보에 기초하여 상기 전자 장치의 위치와 상기 사용자의 위치 사이의 거리에 대응하는상기 얼굴 영역의 거리 정보를 획득하는 단계; 및상기 얼굴 영역의 거리 정보에 기초하여 컨텐츠 이미지를 출력하는 단계;를 포함하는, 제어 방법."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 크기 정보를 획득하는 단계는,상기 얼굴 영역의 가로 길이, 상기 얼굴 영역의 세로 길이, 상기 얼굴 영역의 넓이 중 적어도 하나를 포함하는상기 크기 정보를 획득하는, 제어 방법."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 크기 정보를 획득하는 단계는,상기 센싱 데이터에 기초하여 사용자의 얼굴 영역이 식별되면, 상기 얼굴 영역의 좌표 정보를 획득하고,상기 좌표 정보에 기초하여 상기 크기 정보를 획득하는, 제어 방법."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 제어 방법은,상기 거리 정보에 기초하여 상기 전자 장치를 통해 출력되는 이미지의 크기를 나타내는 투사 해상도 정보를 획득하는 단계;를 더 포함하고,상기 컨텐츠 이미지를 출력하는 단계는,상기 투사 해상도 정보에 기초하여 상기 컨텐츠 이미지를 출력하는, 제어 방법."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 거리 정보는 제1 거리 정보이고,상기 투사 해상도 정보를 획득하는 단계는,상기 전자 장치의 위치와 투사면 사이의 거리에 대응하는 제2 거리 정보를 획득하고,상기 제1 거리 정보 및 상기 제2 거리 정보에 기초하여 상기 투사 해상도 정보를 획득하는, 제어 방법."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 전자 장치는,투사 비율 범위를 저장하고,상기 투사 해상도 정보를 획득하는 단계는,상기 투사 비율 범위 및 상기 제2 거리 정보에 기초하여 해상도 범위를 획득하고,공개특허 10-2025-0040434-5-상기 제1 거리 정보에 기초하여 상기 해상도 범위 중 상기 투사 해상도 정보를 획득하는, 제어 방법."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제14항에 있어서,상기 컨텐츠 이미지를 출력하는 단계는,컨텐츠 해상도 정보 및 상기 투사 해상도 정보에 기초하여 스케일링 계수를 획득하고,상기 스케일링 계수에 기초하여 상기 컨텐츠 이미지를 스케일링하여 투사 이미지를 획득하고,상기 투사 이미지를 출력하는, 제어 방법."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항에 있어서,상기 크기 정보를 획득하는 단계는,상기 센싱 데이터에 기초하여 제1 얼굴 영역 및 제2 얼굴 영역이 식별되면, 상기 제1 얼굴 영역의 제1 크기 정보 및 상기 제2 얼굴 영역의 제2 크기 정보를 획득하고,상기 거리 정보를 획득하는 단계는,상기 제1 얼굴 영역의 제1 크기 정보에 기초하여 상기 제1 얼굴 영역의 제1 거리값을 획득하고,상기 제2 얼굴 영역의 제2 크기 정보에 기초하여 상기 제2 얼굴 영역의 제2 거리값을 획득하고,상기 제1 거리값 및 상기 제2 거리값에 기초하여 대표 거리값을 획득하고,상기 대표 거리값을 포함하는 상기 거리 정보를 획득하고,상기 컨텐츠 이미지를 출력하는 단계는,상기 거리 정보에 기초하여 상기 컨텐츠 이미지를 출력하는, 제어 방법."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항에 있어서,상기 전자 장치는,학습된 적어도 하나의 가중치를 포함하는 인공 지능 모델을 저장하고,상기 거리 정보를 획득하는 단계는,상기 크기 정보를 상기 인공 지능 모델에 입력하여 상기 거리 정보를 출력 데이터로써 획득하는, 제어 방법."}
{"patent_id": "10-2023-0123486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서,상기 적어도 하나의 가중치는,RGB 이미지를 포함하는 제1 학습 데이터 및 뎁스 이미지를 포함하는 제2 학습 데이터에 기초하여 학습된, 제어방법."}
{"patent_id": "10-2023-0123486", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치는 센싱 데이터를 획득하는 센서부, 메모리, 프로젝션부 및 적어도 하나의 프로세서를 포함하고, 적어 도 하나의 프로세서는 센싱 데이터에 기초하여 식별된 사용자의 얼굴 영역의 크기 정보를 획득하고, 얼굴 영역의 크기 정보에 기초하여 전자 장치의 위치와 사용자의 위치 사이의 거리에 대응하는 얼굴 영역의 거리 정보를 획득 하고, 얼굴 영역의 거리 정보에 기초하여 컨텐츠 이미지가 출력되도록 프로젝션부를 제어한다."}
{"patent_id": "10-2023-0123486", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 그 제어방법에 관한 것으로, 더욱 상세하게는 사용자와 전자 장치 사이의 거리를 이용 하여 투사 이미지의 크기를 조절하는 전자 장치 및 그 제어방법에 대한 것이다."}
{"patent_id": "10-2023-0123486", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "사용자와 전자 장치 사이의 거리 정보를 측정하기 위해 다양한 센서(예를 들어, 뎁스 카메라, 3D TOF, 라이다 등)가 이용될 수 있다. 다양한 제약 사항에서 거리 정보를 측정하는데 정확도가 높은 뎁스 센서가 이용되지 못 할 수 있다. 다양한 센서 중 RGB 이미지를 획득하는 RGB 카메라가 이용되는 경우, 거리 정보를 측정하는데 어려움이 있다. RGB 카메라 2개를 이용하는 경우 거리 측정의 정확도를 높일 수 있다. 하지만, 2개의 카메라를 전자 장치에 배 치하는 경우 공간 상의 어려움 또는 비용측면에서 어려움이 발생할 수 있다. 이미지를 투사하는 장치의 경우, 사용자와의 상대적인 거리에 따라 투사 이미지를 보정할 필요성이 있다. 장치 와 사용자 사이의 거리를 고려하지 않는 경우, 이미지의 크기가 너무 크거나 작아지게 되어 사용자에게 적합한 이미지를 제공하기 어려울 수 있다."}
{"patent_id": "10-2023-0123486", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시는 상술한 문제를 개선하기 위해 고안된 것으로, 본 개시의 목적은 전자 장치와 사용자 사이의 거리를 고려하여 투사 이미지를 출력하는 전자 장치 및 그의 제어 방법을 제공함에 있다. 일 실시 예에 따른 전자 장치는 센싱 데이터를 획득하는 센서부, 메모리, 프로젝션부 및 적어도 하나의 프로세 서를 포함하고, 상기 적어도 하나의 프로세서는 상기 센싱 데이터에 기초하여 식별된 사용자의 얼굴 영역의 크 기 정보를 획득하고, 상기 얼굴 영역의 크기 정보에 기초하여 상기 전자 장치의 위치와 상기 사용자의 위치 사 이의 거리에 대응하는 상기 얼굴 영역의 거리 정보를 획득하고, 상기 얼굴 영역의 거리 정보에 기초하여 컨텐츠 이미지가 출력되도록 상기 프로젝션부를 제어한다. 상기 적어도 하나의 프로세서는 상기 얼굴 영역의 가로 길이, 상기 얼굴 영역의 세로 길이, 상기 얼굴 영역의 넓이 중 적어도 하나를 포함하는 상기 크기 정보를 획득할 수 있다. 상기 적어도 하나의 프로세서는 상기 센싱 데이터에 기초하여 사용자의 얼굴 영역이 식별되면, 상기 얼굴 영역 의 좌표 정보를 획득하고, 상기 좌표 정보에 기초하여 상기 크기 정보를 획득할 수 있다. 상기 적어도 하나의 프로세서는 상기 거리 정보에 기초하여 상기 프로젝션부를 통해 출력되는 이미지의 크기를 나타내는 투사 해상도 정보를 획득하고, 상기 투사 해상도 정보에 기초하여 상기 컨텐츠 이미지를 출력하도록 상기 프로젝션부를 제어할 수 있다. 상기 거리 정보는 제1 거리 정보이고, 상기 적어도 하나의 프로세서는 상기 전자 장치의 위치와 투사면 사이의 거리에 대응하는 제2 거리 정보를 획득하고, 상기 제1 거리 정보 및 상기 제2 거리 정보에 기초하여 상기 투사 해상도 정보를 획득할 수 있다. 상기 메모리는 투사 비율 범위를 저장하고, 상기 적어도 하나의 프로세서는 상기 투사 비율 범위 및 상기 제2 거리 정보에 기초하여 해상도 범위를 획득하고, 상기 제1 거리 정보에 기초하여 상기 해상도 범위 중 상기 투사 해상도 정보를 획득할 수 있다. 상기 적어도 하나의 프로세서는 상기 컨텐츠 해상도 정보 및 상기 투사 해상도 정보에 기초하여 스케일링 계수 를 획득하고, 상기 스케일링 계수에 기초하여 상기 컨텐츠 이미지를 스케일링하여 투사 이미지를 획득하고, 상 기 투사 이미지를 출력하도록 상기 프로젝션부를 제어할 수 있다. 상기 적어도 하나의 프로세서는 상기 센싱 데이터에 기초하여 제1 얼굴 영역 및 제2 얼굴 영역이 식별되면, 상 기 제1 얼굴 영역의 제1 크기 정보 및 상기 제2 얼굴 영역의 제2 크기 정보를 획득하고, 상기 제1 얼굴 영역의 제1 크기 정보에 기초하여 상기 제1 얼굴 영역의 제1 거리값을 획득하고, 상기 제2 얼굴 영역의 제2 크기 정보 에 기초하여 상기 제2 얼굴 영역의 제2 거리값을 획득하고, 상기 제1 거리값 및 상기 제2 거리값에 기초하여 대 표 거리값을 획득하고, 상기 대표 거리값을 포함하는 상기 거리 정보를 획득하고, 상기 거리 정보에 기초하여 상기 컨텐츠 이미지를 출력하도록 상기 프로젝션부를 제어할 수 있다. 상기 메모리는 학습된 적어도 하나의 가중치를 포함하는 인공 지능 모델을 저장하고, 상기 적어도 하나의 프로 세서는 상기 크기 정보를 상기 인공 지능 모델에 입력하여 상기 거리 정보를 출력 데이터로써 획득할 수 있다.상기 적어도 하나의 가중치는 RGB 이미지를 포함하는 제1 학습 데이터 및 뎁스 이미지를 포함하는 제2 학습 데 이터에 기초하여 학습될 수 있다. 일 실시 예에 따른 센싱 데이터를 획득하는 전자 장치의 제어 방법은 상기 센싱 데이터에 기초하여 식별된 사용 자의 얼굴 영역의 크기 정보를 획득하는 단계, 상기 얼굴 영역의 크기 정보에 기초하여 상기 전자 장치의 위치 와 상기 사용자의 위치 사이의 거리에 대응하는 상기 얼굴 영역의 거리 정보를 획득하는 단계 및 상기 얼굴 영 역의 거리 정보에 기초하여 컨텐츠 이미지를 출력하는 단계를 포함한다. 상기 크기 정보를 획득하는 단계는 상기 얼굴 영역의 가로 길이, 상기 얼굴 영역의 세로 길이, 상기 얼굴 영역 의 넓이 중 적어도 하나를 포함하는 상기 크기 정보를 획득할 수 있다. 상기 크기 정보를 획득하는 단계는 상기 센싱 데이터에 기초하여 사용자의 얼굴 영역이 식별되면, 상기 얼굴 영 역의 좌표 정보를 획득하고, 상기 좌표 정보에 기초하여 상기 크기 정보를 획득할 수 있다. 상기 제어 방법은 상기 거리 정보에 기초하여 상기 전자 장치를 통해 출력되는 이미지의 크기를 나타내는 투사 해상도 정보를 획득하는 단계를 더 포함하고, 상기 컨텐츠 이미지를 출력하는 단계는 상기 투사 해상도 정보에 기초하여 상기 컨텐츠 이미지를 출력할 수 있다. 상기 거리 정보는 제1 거리 정보이고, 상기 투사 해상도 정보를 획득하는 단계는 상기 전자 장치의 위치와 투사 면 사이의 거리에 대응하는 제2 거리 정보를 획득하고, 상기 제1 거리 정보 및 상기 제2 거리 정보에 기초하여 상기 투사 해상도 정보를 획득할 수 있다. 상기 전자 장치는 투사 비율 범위를 저장하고, 상기 투사 해상도 정보를 획득하는 단계는 상기 투사 비율 범위 및 상기 제2 거리 정보에 기초하여 해상도 범위를 획득하고, 상기 제1 거리 정보에 기초하여 상기 해상도 범위 중 상기 투사 해상도 정보를 획득할 수 있다. 상기 컨텐츠 이미지를 출력하는 단계는 컨텐츠 해상도 정보 및 상기 투사 해상도 정보에 기초하여 스케일링 계 수를 획득하고, 상기 스케일링 계수에 기초하여 상기 컨텐츠 이미지를 스케일링하여 투사 이미지를 획득하고, 상기 투사 이미지를 출력할 수 있다. 상기 크기 정보를 획득하는 단계는 상기 센싱 데이터에 기초하여 제1 얼굴 영역 및 제2 얼굴 영역이 식별되면, 상기 제1 얼굴 영역의 제1 크기 정보 및 상기 제2 얼굴 영역의 제2 크기 정보를 획득하고, 상기 거리 정보를 획 득하는 단계는 상기 제1 얼굴 영역의 제1 크기 정보에 기초하여 상기 제1 얼굴 영역의 제1 거리값을 획득하고, 상기 제2 얼굴 영역의 제2 크기 정보에 기초하여 상기 제2 얼굴 영역의 제2 거리값을 획득하고, 상기 제1 거리 값 및 상기 제2 거리값에 기초하여 대표 거리값을 획득하고, 상기 대표 거리값을 포함하는 상기 거리 정보를 획 득하고, 상기 컨텐츠 이미지를 출력하는 단계는 상기 거리 정보에 기초하여 상기 컨텐츠 이미지를 출력할 수 있 다. 상기 전자 장치는 학습된 적어도 하나의 가중치를 포함하는 인공 지능 모델을 저장하고, 상기 거리 정보를 획득 하는 단계는 상기 크기 정보를 상기 인공 지능 모델에 입력하여 상기 거리 정보를 출력 데이터로써 획득할 수 있다. 상기 적어도 하나의 가중치는 RGB 이미지를 포함하는 제1 학습 데이터 및 뎁스 이미지를 포함하는 제2 학습 데 이터에 기초하여 학습될 수 있다."}
{"patent_id": "10-2023-0123486", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부 도면을 참조하여 본 개시를 상세히 설명한다. 본 개시의 실시 예에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 개시의 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수 치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. A 또는/및 B 중 적어도 하나라는 표현은 \"A\" 또는 \"B\" 또는 \"A 및 B\" 중 어느 하나를 나타내는 것으로 이해되어 야 한다. 본 명세서에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들 을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 어떤 구성요소가 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또 는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시에서 \"모듈\" 혹은 \"부\"는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨어로 구현되 거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 복수의 \"모듈\" 혹은 복수의 \"부\"는 특정한 하드웨어로 구현될 필요가 있는 \"모듈\" 혹은 \"부\"를 제외하고는 적어도 하나의 모듈로 일체화되어 적어도 하나의 프로세서(미도시)로 구현될 수 있다. 본 명세서에서, 사용자라는 용어는 전자 장치를 사용하는 사람 또는 전자 장치를 사용하는 장치(예: 인공지능 전자 장치)를 지칭할 수 있다. 이하 첨부된 도면들을 참조하여 본 개시의 일 실시 예를 보다 상세하게 설명한다. 도 1은 일 실시 예에 따라, 전자 장치를 통해 출력되는 이미지의 크기를 결정하는 동작을 설명하기 위한 도면이다. 전자 장치는 이미지(또는 투사 이미지)를 출력하는 기기일 수 있다. 전자 장치는 이미지를 투사면 에 출력할 수 있다. 전자 장치는 투사면과 사용자 사이의 거리를 이용하여 투사 이미지의 크 기를 획득(또는 결정 또는 식별)할 수 있다. 투사 이미지의 크기는 투상 이미지의 해상도 정보에 포함될 수 있 다. 전자 장치는 투사 이미지의 크기에 기초하여 투사 이미지를 출력할 수 있다. 실시 예에 따라, 투사면 및 사용자 사이의 거리가 제1 거리(d1) 일 수 있다. 전자 장치는 제1 거리(d1)에 기초하여 투사 이미지의 크기를 제1 크기(i1)로 결정할 수 있다. 실시 예에 따라, 투사면 및 사용자 사이의 거리가 제2 거리(d2) 일 수 있다. 전자 장치는 제2 거리(d2)에 기초하여 투사 이미지의 크기를 제2 크기(i2)로 결정할 수 있다. 제1 거리(d1)가 제2 거리(d2)보다 작으면, 제1 크기(i1)는 제2 크기(i2)보다 작을 수 있다. 사용자가 투사 면으로부터 멀리 떨어져 있을수록 투사 이미지의 크기가 커질 수 있다. 제1 크기(i1) 및 제2 크기(i2)가 투사 이미지의 세로 높이로 기재되었다. 구현 예에 따라, 제1 크기(i1) 및 제2 크기(i2)는 투사 이미지의 세로 길이, 가로 길이 또는 투사 이미지의 대각선 길이 중 적어도 하나를 포함할 수 있다. 전자 장치는 이동형 기기로 구현될 수 있다. 전자 장치는 이동형 프로젝터 또는 이동형 이미지 출력 기기로 구현될 수 있다. 전자 장치는 이동 부재를 포함할 수 있다. 이동 부재는 전자 장치가 배치된 공간에서 제1 위치에서 제2 위치로 이동하기 위한 부재를 의미할 수 있다. 전자 장치는 구동부에서 생성된 힘을 이용하여 전 자 장치가 이동되도록 이동 부재를 제어할 수 있다. 전자 장치는 구동부에 포함된 모터를 이용 하여 이동 부재에 전달되기 위한 힘을 생성할 수 있다. 이동 부재는 적어도 하나의 바퀴(예를 들어, 원형 바퀴)를 포함할 수 있다. 전자 장치는 이동 부재를 통해 타겟 위치(또는 목표 위치)로 이동할 수 있다. 사용자 입력 또는 제어 명령이 수신되면, 전자 장치는 모터 를 통해 발생된 힘을 이동 부재에 전달함으로써 이동 부재를 회전시킬 수 있다. 전자 장치는 회전 속도, 회전 방향 등을 조절하기 위해 이동 부재를 제어할 수 있다. 전자 장치는 타겟 위치 또는 진행 방향 등에 기초하여 이동 부재를 제어함으로써 이동 동작(또는 이동 기능)을 수행할 수 있다. 도 1에서 도시된 바와 같이 전자 장치는 사용자의 얼굴보다 낮은 위치에 존재할 가능성이 있다. 전자 장치가 사용자의 얼굴을 촬상하기 위해서는 아래에서 위 방향으로 촬상 동작을 수행할 필요성이 있다. 아래에서 위 방향으로 촬상 동작을 수행하는 경우, 촬영된 이미지의 품질(예를 들어, 밝기)이 좋지 않은 경우가 발생할 수 있다. 일 예로, 어두운 환경에서 사용자의 얼굴을 아래에서 위 방향으로 촬영하는 경우, 전자 장치는 밝기가 어두운 이미지를 이용하여 사용자의 얼굴을 식별(또는 분석)해야 한다. 밝기가 어두운 이미지를 이용하는 경우 사용자의 얼굴에 대한 분석이 정확하지 않을 수 있다. 전자 장치는 아래에서 위 방향으로 촬상된 이미지 또는 품질이 좋지 않은 이미지(예를 들어, 평균 픽셀 밝 기가 임계값 이하인 이미지)에 기초하여 사용자와 전자 장치 사이의 거리를 구하는데 효과적으로 적용 될 수 있다. 도 2는 일 실시 예에 따라, 전자 장치를 도시한 블록도이다. 도 2를 참조하면, 전자 장치는 적어도 하나의 프로세서, 프로젝션부, 메모리, 센서부 중 적어도 하나를 포함할 수 있다. 적어도 하나의 프로세서는 전자 장치의 전반적인 제어 동작을 수행할 수 있다. 구체적으로, 적어도 하나의 프로세서는 전자 장치의 전반적인 동작을 제어하는 기능을 한다. 적어도 하나의 프로세서 와 관련된 구체적인 설명은 도 3에서 기재한다. 프로젝션부는 이미지(투사 이미지, 컨텐츠 등)을 외부로 투사하는 구성이다. 프로젝션부와 관련된 구 체적인 설명은 도 3에서 기재한다. 메모리는 프로젝션부를 통해 투사되는 투사 이미지를 저장할 수 있다. 투사 이미지는 정지 이미지뿐 아니라 연속 이미지(또는 동영상)을 의미할 수 있다. 투사 이미지는 컨텐츠에 포함된 이미지를 의미할 수 있다. 센서부는 센싱 데이터를 획득할 수 있다. 메모리는 컨텐츠 이미지를 저장할 수 있다. 프로젝션부 는 이미지를 출력하는데 이용될 수 있다. 적어도 하나의 프로세서는 전자 장치에서 수행되는 동작을 제어할 수 있다. 적어도 하나의 프로세서는 센싱 데이터에 기초하여 사용자의 얼굴 영역이 검출되면, 얼굴 영역의 크기 정보를 획득하고, 얼굴 영역의 크기 정보에 기초하여 센싱 데이터를 획득한 전자 장치의 위치와 사용자 의 위치 사이의 거리에 대응하는 얼굴 영역의 거리 정보를 획득하고, 프로젝션부를 통해, 거리 정보에 기초하여 컨텐츠 이미지를 출력할 수 있다. 얼굴 영역의 거리 정보는 전자 장치의 위치와 사용자의 위치 사이의 거리를 포함할 수 있다. 적어도 하나의 프로세서는 센서부를 통해 센싱 데이터를 획득할 수 있다. 기 설정된 이벤트(예를 들 어, 사용자 입력)가 식별되면, 적어도 하나의 프로세서는 센싱 데이터를 획득하기 위한 제어 명령을 센서 부에 전송할 수 있다. 적어도 하나의 프로세서는 제어 명령에 응답하여 센싱 데이터를 센서부를 통해 수신할 수 있다. 센싱 데이터는 사용자와 관련된 정보가 포함될 수 있다. 사용자와 관련된 정보는 일 예로, 센싱 데이터는 RGB 이미지를 포함할 수 있다. 센서부는 RGB 카메라를 포함할 수 있다. 일 예로, 센싱 데이터가 적외선 이미지 를 포함할 수 있다. 센서부는 적외선 센서를 포함할 수 있다. 일 예로, 센싱 데이터는 뎁스 이미지 를 포함할 수 있다. 센서부는 뎁스 카메라를 포함할 수 있다. 일 예로, 센싱 데이터는 라이다 데이터를 포함할 수 있다. 센서부는 라이다 센서를 포함할 수 있다. 적어도 하나의 프로세서는 센싱 데이터에 기초하여 사용자가 검출(또는 식별)되는지 판단할 수 있다. 적어도 하나의 프로세서는 센싱 데이터에 사람의 얼굴을 검출할 수 있다. 적어도 하나의 프로세서는 사람의 얼굴을 나타내는 얼굴 영역을 검출할 수 있다. 적어도 하나의 프로세서는 얼굴 영역을 검출할 수 있다. 센싱 데이터에서 얼굴 영역이 검출되면, 적어도 하나의 프로세서는 얼굴 영역의 크기 정보를 획득할 수 있 다. 얼굴 영역의 크기 정보는 센싱 데이터 상에서 얼굴 영역의 크기를 나타내기 위해 필요한 적어도 하나의 요 소를 포함할 수 있다. 적어도 하나의 프로세서는 얼굴 영역의 가로 길이, 얼굴 영역의 세로 길이, 얼굴 영역의 넓이 중 적어도 하나를 포함하는 크기 정보를 획득할 수 있다. 적어도 하나의 프로세서는 센싱 데이터에 기초하여 얼굴 영 역의 가로 길이, 얼굴 영역의 세로 길이, 얼굴 영역의 넓이 중 적어도 하나를 계산할 수 있다. 적어도 하나의 프로세서는 얼굴 영역의 크기 정보에 기초하여 사용자와 전자 장치 사이의 거리에 대응하는 얼굴 영역의 거리 정보를 획득할 수 있다. 적어도 하나의 프로세서는 센싱 데이터를 센싱한 전자 장치의 위치와 사용자의 얼굴이 존재하는 위치 사이의 거리에 대응하는 얼굴 영역의 거리 정보를 획득 할 수 있다. 다양한 실시 예에 따라, 적어도 하나의 프로세서는 사용자의 머리 방향과 관련된 회전 축 정보(yaw, pitch, roll)를 추가로 획득할 수 있다. 적어도 하나의 프로세서는 사용자의 머리 방향과 관련된 회전 축 정보(yaw, pitch, roll)를 추가로 이용하여 얼굴 영역의 거리 정보를 획득할 수 있다. 다양한 실시 예에 따라, 어도 하나의 프로세서는 사용자의 얼굴 영역과 관련된 특징 오브젝트(또는 랜 드 마크 오브젝트)를 검출할 수 있다. 특징 오브젝트는 사람의 눈, 코, 입 중 적어도 하나를 포함할 수 있다. 적어도 하나의 프로세서는 특징 오브젝트를 이용하여 얼굴 영역의 거리 정보를 획득할 수 있다. 특징 오브젝트를 이용하는 동작은 특징 오브젝트의 위치를 이용하는 동작을 포함할 수 있다. 적어도 하나의 프로세서는 센싱 데이터에 기초하여 사용자의 얼굴 영역이 검출되면, 얼굴 영역의 좌표 정보를 획득하고, 좌표 정보에 기초하여 크기 정보를 획득할 수 있다. 좌표 정보는 센싱 데이터에서 특정 픽셀 내지 특정 오브젝트를 나타내는데 이용될 수 있다. 적어도 하나의 프로 세서는 좌표 정보를 이용하여 얼굴 영역이 어디에 위치하는지 분석할 수 있다. 적어도 하나의 프로세서는 센싱 데이터에 포함된 얼굴 영역이 센싱 데이터에서 어떠한 위치에 배치되는 것 인지를 나타내는 얼굴 영역의 좌표 정보를 획득할 수 있다. 좌표 정보는 복수의 좌표값을 포함할 수 있다. 얼굴 영역은 하나의 픽셀로 이루어지지 않을 수 있기 때문이다. 중심점 또는 기준점은 하나의 픽셀일 수 있으나, 얼굴 영역 자체는 복수의 좌표값을 포함할 수 있다. 얼굴 영역 의 좌표 정보는 사용자의 얼굴 위치를 나타내는 복수의 좌표값을 포함할 수 있다. 적어도 하나의 프로세서는 좌표 정보에 기초하여 얼굴 영역의 크기 정보를 획득할 수 있다. 일 예로, 적어도 하나의 프로세서는 얼굴 영역의 좌표 정보 중 최좌측 x값(제1값)을 획득할 수 있다. 적어 도 하나의 프로세서는 얼굴 영역의 좌표 정보 중 최우측 x값(제2값)을 획득할 수 있다. 적어도 하나의 프 로세서는 최좌측 x값(제1값)과 최우측 x값(제2값)의 차이값을 얼굴 영역의 가로 길이로 획득할 수 있다. 일 예로, 적어도 하나의 프로세서는 얼굴 영역의 좌표 정보 중 최상측 y값(제3값)을 획득할 수 있다. 적어 도 하나의 프로세서는 얼굴 영역의 좌표 정보 중 최하측 y값(제4값)을 획득할 수 있다. 적어도 하나의 프 로세서는 최상측 y값(제3값)과 최하측 y값(제4값)의 차이값을 얼굴 영역의 세로 길이로 획득할 수 있다. 일 예로, 적어도 하나의 프로세서는 얼굴 영역의 좌표 정보를 고려하여 얼굴 영역의 내부 방향의 넓이(또 는 면적)를 획득할 수 있다. 적어도 하나의 프로세서는 얼굴 영역의 크기 정보에 기초하여 얼굴 박스(또는 얼굴 GUI)를 생성할 수 있다. 적어도 하나의 프로세서는 얼굴 영역의 좌표 정보 및 크기 정보에 기초하여 사용자의 얼굴 부분 에 얼굴 박스를 적용한 촬상 이미지를 획득할 수 있다. 메모리는 컨텐츠 해상도 정보를 저장하고, 적어도 하나의 프로세서는 거리 정보에 기초하여 프로젝션 부를 통해 출력되는 이미지의 크기를 나타내는 투사 해상도 정보를 획득하고, 투사 해상도 정보에 기초하 여 컨텐츠 이미지를 출력할 수 있다. 컨텐츠 해상도 정보는 메모리에 저장된 컨텐츠 이미지와 관련된 원본 해상도를 포함할 수 있다. 컨텐츠 해 상도 정보는 컨텐츠 이미지의 크기를 나타내는 정보를 포함할 수 있다. 컨텐츠 해상도 정보는 컨텐츠 이미지의 픽셀 수를 나타내는 정보를 포함할 수 있다. 투사 해상도 정보는 프로젝션부를 통해 출력하는 투사 이미지와 관련된 보정된 해상도(또는 변경된 해상도)를 포함할 수 있다. 투사 해상도 정보는 투사 이미지의 크기를 나타내는 정보를 포함할 수 있다. 컨텐츠 이미지는 컨텐츠에 포함된 원본 이미지이고, 투사 이미지는 컨텐츠 이미지를 프로젝션부를 통해 출 력하도록 보정한 이미지일 수 있다. 적어도 하나의 프로세서는 컨텐츠 이미지를 투사 이미지로 변환할 수 있다. 컨텐츠 해상도와 프로젝션부가 제공 가능한 해상도가 상이한 경우, 투사 이미지의 품질이 떨어질 수 있다. 적어도 하나의 프로세서는 투사 이미지를 획득할 수 있다. 변환 동작은 스케일링 동작으로 기재될 수 있다. 적어도 하나의 프로세서는 컨텐츠 해상도 정보 및 투사 해상도 정보에 기초하여 스케일링 계수를 획득하고, 스케일링 계수에 기초하여 컨텐츠 이미지를 스케일링하여 투사 이미지를 획득하고, 프로젝션부 를 통해, 투사 이미지를 출력할 수 있다. 적어도 하나의 프로세서는 컨텐츠 해상도 정보 및 투사 해상도 정보에 기초하여 스케일링 계수를 획득할 수 있다. 적어도 하나의 프로세서는 스케일링 계수에 기초하여 컨텐츠 이미지를 스케일링 할 수 있다. 적 어도 하나의 프로세서는 스케일링 기능의 결과로써 투사 이미지를 획득할 수 있다. 이와 관련된 설명은 도 13에서 기재한다. 거리 정보는 제1 거리 정보이고, 적어도 하나의 프로세서는 전자 장치의 위치와 투사면 사이의 거리에 대 응하는 제2 거리 정보를 획득하고, 제1 거리 정보 및 제2 거리 정보에 기초하여 투사 해상도 정보를 획득할 수있다. 이와 관련된 설명은 도 14에서 기재한다. 제1 거리 정보는 전자 장치와 사용자 사이의 거리값을 포함할 수 있다. 제2 거리 정보는 전자 장치 와 투사면 사이의 거리값을 포함할 수 있다. 메모리는 투사 비율 범위를 저장하고, 적어도 하나의 프로세서는 투사 비율 범위 및 제2 거리 정보에 기초하여 해상도 범위를 획득하고, 제1 거리 정보에 기초하여 해상도 범위 중 투사 해상도 정보를 획득할 수 있 다. 투사 비율 범위는 전자 장치에서 제공 가능한 투사 비율의 범위를 포함할 수 있다. 투사 비율 범위는 프로 젝션부의 하드웨어적 특징에 기초하여 결정될 수 있다. 프로젝션부에 포함된 렌즈의 성능에 따라 투 사 비율 범위가 달라질 수 있다. 적어도 하나의 프로세서는 메모리에 저장된 전자 장치에 대응 되는 투사 비율 범위를 획득할 수 있다. 적어도 하나의 프로세서는 투사 비율 범위를 벗어나 투사 이미지를 출력하는 경우, 초점이 맞지 않는 이미 지를 제공할 수 있다. 예를 들어, 투사 비율 범위를 벗어나 투사 이미지를 출력하는 경우 사용자는 흐릿한 이미 지를 보게 될 수 있다. 적어도 하나의 프로세서는 투사 비율 범위 안에서 투사 이미지를 출력할 필요성이 있다. 투사 비율은 투사 거리/투사 이미지의 크기로 계산될 수 있다. 투사 이미지의 크기는 투사 이미지의 대각선 길 이를 포함할 수 있다. 투사 거리는 제2 거리 정보에 대응될 수 있다. 적어도 하나의 프로세서는 제2 거리 정보(투사 거리) 및 투 사 비율 범위에 기초하여 해상도 범위를 획득할 수 있다. 해상도 범위는 전자 장치의 현재 위치에서 제공 가능한 투사 이미지의 크기 범위를 포함할 수 있다. 해상도 범위 또는 크기 범위는 투사 이미지의 대각선 길이 를 포함할 수 있다. 적어도 하나의 프로세서는 해상도 범위에서 제1 거리 정보를 고려하여 투사 해상도 정보를 획득할 수 있다. 해상도 범위는 현재 전자 장치에서 제공 가능한 투사 이미지의 크기에 해당하므로, 적어도 하나의 프로세서는 제1 거리 정보를 이용하여 사용자를 위한 투사 해상도 정보를 획득할 수 있다. 적어도 하나의 프로세서는 해상도 범위 중 최대값(제1값)를 획득할 수 있다. 적어도 하나의 프로세서(11 1)는 해상도 범위 중 최소값(제2값)를 획득할 수 있다. 적어도 하나의 프로세서는 제1 거리 정보에 기초하여 임계 상수를 획득할 수 있다. 제1 거리 정보가 제1 임계 범위에 포함되면, 적어도 하나의 프로세서는 제1 임계 상수를 획득할 수 있다. 제1 거리 정보가 제2 임계 범위에 포함되면, 적어도 하나의 프로세서는 제2 임계 상수를 포함할 수 있다. 임계 범위 및 임계 상수는 각각 복수 개일 수 있다. 적어도 하나의 프로세서는 해상도 범위 중 최대값(제1값)에 임계 상수를 곱셈한 값(제3값)을 획득할 수 있 다. 적어도 하나의 프로세서는 해상도 범위 중 최소값(제2값) 및 곱셈한 값(제3값) 중 큰 값(제4값)을 투 사 해상도 정보로 획득할 수 있다. 예를 들어, 제2 거리 정보가 2m이고 투사 비율 범위가 1~2라고 가정한다. 해상도 범위(또는 크기 범위)는 1~2m 일 수 있다. 제1 거리 정보가 0.1m인 경우, 적어도 하나의 프로세서는 투사 해상도 정보를 1m로 결정할 수 있다. 제1 거리 정보가 10m인 경우, 적어도 하나의 프로세서는 투사 해상도 정보를 2m로 결정할 수 있다. 적어도 하나의 프로세서는 제1 거리 정보(0.1m)에 대응되는 임계 상수(c)를 획득할 수 있다. 적어도 하나 의 프로세서는 임계 상수(c)와 최대값(2m)을 곱셈한 값(2m*c)을 획득할 수 있다. 적어도 하나의 프로세서 는 곱셈한 값(2m*c)과 최소값(1m) 중 큰 값을 투사 해상도 정보로 결정할 수 있다. 사용자가 전자 장치에 가까이 있을수록, 적어도 하나의 프로세서는 투사 이미지의 크기를 해상도 범위 안에서 작게 제어할 수 있다. 사용자가 전자 장치에서 멀어질수록, 적어도 하나의 프로세서(11 1)는 투사 이미지의 크기를 해상도 범위 안에서 크게 제어할 수 있다. 적어도 하나의 프로세서는 센싱 데이터에 기초하여 제1 얼굴 영역 및 제2 얼굴 영역이 검출되면, 제1 얼굴 영역의 제1 크기 정보 및 제2 얼굴 영역의 제2 크기 정보를 획득할 수 있다. 적어도 하나의 프로세서는 제 1 얼굴 영역의 제1 크기 정보에 기초하여 제1 얼굴 영역의 제1 거리값을 획득할 수 있다. 적어도 하나의 프로세 서는 제2 얼굴 영역의 제2 크기 정보에 기초하여 제2 얼굴 영역의 제2 거리값을 획득할 수 있다. 적어도하나의 프로세서는 제1 거리값 및 제2 거리값에 기초하여 대표 거리값을 획득할 수 있다. 적어도 하나의 프로세서는 대표 거리값을 포함하는 거리 정보를 획득하고, 프로젝션부를 통해, 거리 정보에 기초하 여 컨텐츠 이미지를 출력할 수 있다. 이와 관련된 설명은 도 15 내지 도16에서 기재한다. 전자 장치 하나의 센서를 통해 획득한 촬상 이미지에 기초하여 사용자와 전자 장치 사이의 거리 를 정확하게 측정할 수 있다. 도 3을 참조하면, 전자 장치는 적어도 하나의 프로세서, 프로젝션부, 메모리, 통신 인터페 이스, 조작 인터페이스, 입출력 인터페이스, 스피커, 마이크, 전원부, 구동부 또는 센서부 중 적어도 하나를 포함할 수 있다. 도 3에 도시된 구성은 다양한 실시 예에 불과할 뿐, 일부 구성이 생략될 수 있으며, 새로운 구성이 추가될 수 있다. 도 2에서 이미 설명한 내용은 생략한다. 적어도 하나의 프로세서는 디지털 신호를 처리하는 디지털 시그널 프로세서(digital signal processor(DSP), 마이크로 프로세서(microprocessor), TCON(Time controller)으로 구현될 수 있다. 다만, 이에 한정되는 것은 아니며, 중앙처리장치(central processing unit(CPU)), MCU(Micro Controller Unit), MPU(micro processing unit), 컨트롤러(controller), 어플리케이션 프로세서(application processor(AP)), GPU(graphics- processing unit) 또는 커뮤니케이션 프로세서(communication processor(CP)), ARM(advanced reduced instruction set computer (RISC) machines) 프로세서 중 하나 또는 그 이상을 포함하거나, 해당 용어로 정의 될 수 있다. 적어도 하나의 프로세서는 프로세싱 알고리즘이 내장된 SoC(System on Chip), LSI(large scale integration)로 구현될 수도 있고, FPGA(Field Programmable gate array) 형태로 구현될 수도 있다. 적 어도 하나의 프로세서는 메모리에 저장된 컴퓨터 실행가능 명령어(computer executable instructions)를 실행함으로써 다양한 기능을 수행할 수 있다. 프로젝션부는 이미지를 외부로 투사하는 구성이다. 본 개시의 다양한 실시 예에 따른, 프로젝션부는 다양한 투사 방식(예를 들어, CRT(cathode-ray tube) 방식, LCD(Liquid Crystal Display) 방식, DLP(Digital Light Processing) 방식, 레이저 방식 등)으로 구현될 수 있다. 일 예로, CRT 방식은 기본적으로 CRT 모니터와 원리가 동일하다. CRT 방식은 브라운관(CRT) 앞의 렌즈로 상을 확대시켜서 스크린에 이미지를 표시한다. 브라운 관의 개수에 따라 1관식과 3관식으로 나뉘며, 3관식의 경우 Red, Green, Blue의 브라운관이 따로 분리되어 구현 될 수 있다. 다른 예로, LCD 방식은 광원에서 나온 빛을 액정에 투과시켜 이미지를 표시하는 방식이다. LCD 방식은 단판식과 3판식으로 나뉘며, 3판식의 경우 광원에서 나온 빛이 다이크로익 미러(특정 색의 빛만 반사하고 나머지는 통과 시키는 거울)에서 Red, Green, Blue로 분리된 뒤 액정을 투과한 후 다시 한 곳으로 빛이 모일 수 있다. 또 다른 예로, DLP 방식은 DMD(Digital Micromirror Device) 칩을 이용하여 이미지를 표시하는 방식이다. DLP 방식의 프로젝션부는 광원, 컬러 휠, DMD 칩, 프로젝션 렌즈 등을 포함할 수 있다. 광원에서 출력된 빛은 회전 하는 컬러 휠을 통과하면서 색을 띌 수 있다. 컬러 휠을 통화한 빛은 DMD 칩으로 입력된다. DMD 칩은 수많은 미 세 거울을 포함하고, DMD 칩에 입력된 빛을 반사 시킨다. 프로젝션 렌즈는 DMD 칩에서 반사된 빛을 이미지 크기 로 확대시키는 역할을 수행할 수 있다. 또 다른 예로, 레이저 방식은 DPSS(Diode Pumped Solid State) 레이저와 검류계를 포함한다. 다양한 색상을 출 력하는 레이저는 DPSS 레이저를 RGB 색상별로 3개를 설치한 후 특수 거울을 이용하여 광축을 중첩한 레이저를 이용한다. 검류계는 거울과 높은 출력의 모터를 포함하여 빠른 속도로 거울을 움직인다. 예를 들어, 검류계는 최대 40 KHz/sec로 거울을 회전시킬 수 있다. 검류계는 스캔 방향에 따라 마운트되는데 일반적으로 프로젝터는 평면 주사를 하므로 검류계도 x, y축으로 나뉘어 배치될 수 있다. 프로젝션부는 다양한 유형의 광원을 포함할 수 있다. 예를 들어, 프로젝션부는 램프, LED, 레이저 중 적어도 하나의 광원을 포함할 수 있다. 프로젝션부는 전자 장치의 용도 또는 사용자의 설정 등에 따라 4:3 화면비, 5:4 화면비, 16:9 와이드 화면비로 이미지를 출력할 수 있고, 화면비에 따라 WVGA(854*480), SVGA(800*600), XGA(1024*768), WXGA(1280*720), WXGA(1280*800), SXGA(1280*1024), UXGA(1600*1200), Full HD(1920*1080) 등의 다양한 해상 도로 이미지를 출력할 수 있다.프로젝션부는 적어도 하나의 프로세서의 제어에 의해 출력 이미지를 조절하기 위한 다양한 기능을 수 행할 수 있다. 예를 들어, 프로젝션부는 줌, 키스톤, 퀵코너(4코너)키스톤, 렌즈 시프트 등의 기능을 수행 할 수 있다. 구체적으로, 프로젝션부는 스크린과의 거리(투사거리)에 따라 이미지를 확대하거나 축소할 수 있다. 즉, 스크린과의 거리에 따라 줌 기능이 수행될 수 있다. 이때, 줌 기능은 렌즈를 이동시켜 화면의 크기를 조절하는 하드웨어 방식과 이미지를 크롭(crop) 등으로 화면의 크기를 조절하는 소프트웨어 방식을 포함할 수 있다. 줌 기능이 수행되면, 이미지의 초점의 조절이 필요하다. 예를 들어, 초점을 조절하는 방식은 수동 포커스 방식, 전 동 방식 등을 포함한다. 수동 포커스 방식은 수동으로 초점을 맞추는 방식을 의미하고, 전동 방식은 줌 기능이 수행되면 프로젝터가 내장된 모터를 이용하여 자동으로 초점을 맞추는 방식을 의미한다. 줌기능을 수행할 때, 프로젝션부는 소프트웨어를 통한 디지털 줌 기능을 제공할 수 있으며, 구동부를 통해 렌즈를 이동하 여 줌 기능을 수행하는 광학 줌 기능을 제공할 수 있다. 프로젝션부는 키스톤 보정 기능을 수행할 수 있다. 정면 투사에 높이가 안 맞으면 위 혹은 아래로 화면이 왜곡될 수 있다. 키스톤 보정 기능은 왜곡된 화면을 보정하는 기능을 의미한다. 예를 들어, 화면의 좌우 방향으 로 왜곡이 발생되면 수평 키스톤을 이용하여 보정할 수 있고, 상하 방향으로 왜곡이 발생되면 수직 키스톤을 이 용하여 보정할 수 있다. 퀵코너(4코너)키스톤 보정 기능은 화면의 중앙 영역은 정상이지만 모서리 영역의 균형 이 맞지 않은 경우 화면을 보정하는 기능이다. 렌즈 시프트 기능은 화면이 스크린을 벗어난 경우 화면을 그대로 옮겨주는 기능이다. 프로젝션부는 사용자 입력없이 자동으로 주변 환경 및 프로젝션 환경을 분석하여 줌/키스톤/포커스 기능을 제공할 수 있다. 구체적으로, 프로젝션부는 센서(뎁스 카메라, 거리 센서, 적외선 센서, 조도 센서 등)를 통해 감지된 전자 장치와 스크린과의 거리, 현재 전자 장치가 위치하는 공간에 대한 정보, 주변 광량 에 대한 정보 등을 바탕으로 줌/키스톤/포커스 기능을 자동으로 제공할 수 있다. 프로젝션부는 광원을 이용하여 조명 기능을 제공할 수 있다. 특히, 프로젝션부는 LED를 이용하여 광 원을 출력함으로써 조명 기능을 제공할 수 있다. 다양한 실시 예에 따라 프로젝션부는 하나의 LED를 포함 할 수 있으며, 다른 실시 예에 따라 전자 장치는 복수의 LED를 포함할 수 있다. 프로젝션부는 구현 예에 따라 면발광 LED를 이용하여 광원을 출력할 수 있다. 면발광 LED는 광원이 고르게 분산하여 출력되도록 LED의 상측에 광학 시트가 배치되는 구조를 갖는 LED를 의미할 수 있다. 구체적으로, LED를 통해 광원이 출력되 면 광원이 광학 시트를 거쳐 고르게 분산될 수 있고, 광학 시트를 통해 분산된 광원은 디스플레이 패널로 입사 될 수 있다. 프로젝션부는 광원의 세기를 조절하기 위한 디밍 기능을 사용자에게 제공할 수 있다. 구체적으로, 조작 인 터페이스(예를 들어, 터치 디스플레이 버튼 또는 다이얼)를 통해 사용자로부터 광원의 세기를 조절하기 위 한 사용자 입력이 수신되면, 프로젝션부는 수신된 사용자 입력에 대응되는 광원의 세기를 출력하도록 LED 를 제어할 수 있다. 프로젝션부는 사용자 입력 없이 적어도 하나의 프로세서에 의해 분석된 컨텐츠를 바탕으로 디밍 기능 을 제공할 수 있다. 구체적으로, 프로젝션부는 현재 제공되는 컨텐츠에 대한 정보(예를 들어, 컨텐츠 유형, 컨텐츠 밝기 등)를 바탕으로 광원의 세기를 출력하도록 LED를 제어할 수 있다. 프로젝션부는 적어도 하나의 프로세서의 제어에 의해 색온도를 제어할 수 있다. 적어도 하나의 프로 세서는 컨텐츠에 기초하여 색온도를 제어할 수 있다. 구체적으로, 컨텐츠가 출력되기로 식별되면, 적어도 하나의 프로세서는 출력이 결정된 컨텐츠의 프레임별 색상 정보를 획득할 수 있다. 그리고, 적어도 하나의 프로세서는 획득된 프레임별 색상 정보에 기초하여 색온도를 제어할 수 있다. 적어도 하나의 프로세서 는 프레임별 색상 정보에 기초하여 프레임의 주요 색상을 적어도 하나 이상 획득할 수 있다. 그리고, 적어 도 하나의 프로세서는 획득된 적어도 하나 이상의 주요 색상에 기초하여 색온도를 조절할 수 있다. 예를 들어, 적어도 하나의 프로세서가 조절할 수 있는 색온도는 웜 타입(warm type) 또는 콜드 타입(cold typ e)으로 구분될 수 있다. 출력될 프레임(이하 출력 프레임)이 화재가 일어난 장면을 포함하고 있다고 가정한다. 적어도 하나의 프로세서는 현재 출력 프레임에 포함된 색상 정보에 기초하여 주요 색상이 적색이라고 식별 (또는 획득)할 수 있다. 그리고, 적어도 하나의 프로세서는 식별된 주요 색상(적색)에 대응되는 색온도를 식별할 수 있다. 적색에 대응되는 색온도는 웜 타입일 수 있다. 적어도 하나의 프로세서는 프레임의 색상 정보 또는 주용 색상을 획득하기 위하여 인공 지능 모델을 이용할 수 있다. 다양한 실시 예에 따라, 인공 지능 모델은 전자 장치(예를 들어, 메모리)에 저장될 수 있다. 다른 실시 예에 따라, 인공 지능 모델은 전자 장치와 통신 가능한 외부 서버에 저장될 수 있다. 메모리는 적어도 하나의 프로세서에 포함된 롬(ROM)(예를 들어, EEPROM(electrically erasable programmable read-only memory)), 램(RAM) 등의 내부 메모리로 구현되거나, 적어도 하나의 프로세서와 별도의 메모리로 구현될 수도 있다. 이 경우, 메모리는 데이터 저장 용도에 따라 전자 장치에 임베디 드된 메모리 형태로 구현되거나, 전자 장치에 탈부착이 가능한 메모리 형태로 구현될 수도 있다. 예를 들 어, 전자 장치의 구동을 위한 데이터의 경우 전자 장치에 임베디드된 메모리에 저장되고, 전자 장치 의 확장 기능을 위한 데이터의 경우 전자 장치에 탈부착이 가능한 메모리에 저장될 수 있다. 전자 장치에 임베디드된 메모리의 경우 휘발성 메모리(예: DRAM(dynamic RAM), SRAM(static RAM), 또는 SDRAM(synchronous dynamic RAM) 등), 비휘발성 메모리(non-volatile Memory)(예: OTPROM(one time programmable ROM), PROM(programmable ROM), EPROM(erasable and programmable ROM), EEPROM(electrically erasable and programmable ROM), mask ROM, flash ROM, 플래시 메모리(예: NAND flash 또는 NOR flash 등), 하드 드라이브, 또는 솔리드 스테이트 드라이브(solid state drive(SSD)) 중 적어도 하나로 구현되고, 전자 장 치에 탈부착이 가능한 메모리의 경우 메모리 카드(예를 들어, CF(compact flash), SD(secure digital), Micro-SD(micro secure digital), Mini-SD(mini secure digital), xD(extreme digital), MMC(multi-media card) 등), USB 포트에 연결 가능한 외부 메모리(예를 들어, USB 메모리) 등과 같은 형태로 구현될 수 있다. 메모리는 전자 장치에 관한 적어도 하나의 명령이 저장될 수 있다. 그리고, 메모리에는 전자 장 치를 구동시키기 위한 O/S(Operating System)가 저장될 수 있다. 메모리에는 본 개시의 다양한 실시 예들에 따라 전자 장치가 동작하기 위한 각종 소프트웨어 프로그램이나 애플리케이션이 저장될 수도 있다. 그리고, 메모리는 플래시 메모리 (Flash Memory) 등과 같은 반도체 메모리나 하드디스크(Hard Disk) 등과 같은 자기 저장 매체 등을 포함할 수 있다. 구체적으로, 메모리에는 본 개시의 다양한 실시 예에 따라 전자 장치가 동작하기 위한 각종 소프트웨 어 모듈이 저장될 수 있으며, 적어도 하나의 프로세서는 메모리에 저장된 각종 소프트웨어 모듈을 실 행하여 전자 장치의 동작을 제어할 수 있다. 즉, 메모리는 적어도 하나의 프로세서에 의해 액세 스되며, 적어도 하나의 프로세서에 의한 데이터의 독취/기록/수정/삭제/갱신 등이 수행될 수 있다. 본 개시에서 메모리라는 용어는 저장부, 적어도 하나의 프로세서 내 롬(미도시), 램(미도시) 또는 전 자 장치에 장착되는 메모리 카드(미도시)(예를 들어, micro SD 카드, 메모리 스틱)를 포함하는 의미로 사 용될 수 있다. 통신 인터페이스는 다양한 유형의 통신 방식에 따라 다양한 유형의 외부 장치와 통신을 수행하는 구성이다. 통신 인터페이스는 무선 통신 모듈 또는 유선 통신 모듈을 포함할 수 있다. 각 통신 모듈은 적 어도 하나의 하드웨어 칩 형태로 구현될 수 있다. 무선 통신 모듈은 무선으로 외부 장치와 통신하는 모듈일 수 있다. 예를 들어, 무선 통신 모듈은 와이파이 모듈, 블루투스 모듈, 적외선 통신 모듈 또는 기타 통신 모듈 중 적어도 하나의 모듈을 포함할 수 있다. 와이파이 모듈, 블루투스 모듈은 각각 와이파이 방식, 블루투스 방식으로 통신을 수행할 수 있다. 와이파이 모 듈이나 블루투스 모듈을 이용하는 경우에는 SSID(service set identifier) 및 세션 키 등과 같은 각종 연결 정 보를 먼저 송수신하여, 이를 이용하여 통신 연결한 후 각종 정보들을 송수신할 수 있다. 적외선 통신 모듈은 가시 광선과 밀리미터파 사이에 있는 적외선을 이용하여 근거리에 무선으로 데이터를 전송 하는 적외선 통신(IrDA, infrared Data Association)기술에 따라 통신을 수행한다. 기타 통신 모듈은 상술한 통신 방식 이외에 지그비(zigbee), 3G(3rd Generation), 3GPP(3rd Generation Partnership Project), LTE(Long Term Evolution), LTE-A(LTE Advanced), 4G(4th Generation), 5G(5th Generation)등과 같은 다양한 무선 통신 규격에 따라 통신을 수행하는 적어도 하나의 통신 칩을 포함할 수 있다. 유선 통신 모듈은 유선으로 외부 장치와 통신하는 모듈일 수 있다. 예를 들어, 유선 통신 모듈은 LAN(Local Area Network) 모듈, 이더넷 모듈, 페어 케이블, 동축 케이블, 광섬유 케이블 또는 UWB(Ultra Wide-Band) 모듈 중 적어도 하나를 포함할 수 있다. 조작 인터페이스는 다양한 유형의 입력 장치를 포함할 수 있다. 예를 들어, 조작 인터페이스는 물리 적 버튼을 포함할 수 있다. 이때, 물리적 버튼은 기능키(function key), 방향키(예를 들어, 4방향 키) 또는 다이얼 버튼(dial button)을 포함할 수 있다. 다양한 실시 예에 따라, 물리적 버튼은 복수의 키로 구현될 수 있다. 다른 실시 예에 따라, 물리적 버튼은 하나의 키(one key)로 구현될 수 있다. 물리적 버튼이 하나의 키로 구현되는 경우, 전자 장치는 하나의 키가 임계 시간 이상 눌려지는 사용자 입력을 수신할 수 있다. 하나의 키가 임계 시간 이상 눌려지는 사용자 입력이 수신되면, 적어도 하나의 프로세서는 사용자 입력에 대응되 는 기능을 수행할 수 있다. 예를 들어, 적어도 하나의 프로세서는 사용자 입력에 기초하여 조명 기능을 제 공할 수 있다. 조작 인터페이스는 비접촉 방식을 이용하여 사용자 입력을 수신할 수 있다. 접촉 방식을 통해서 사용자 입 력을 수신하는 경우 물리적인 힘이 전자 장치에 전달되어야 한다. 따라서, 물리적인 힘에 관계없이 전자 장치를 제어하기 위한 방식이 필요할 수 있다. 구체적으로, 조작 인터페이스는 사용자 제스쳐를 수신 할 수 있고, 수신된 사용자 제스쳐에 대응되는 동작을 수행할 수 있다. 조작 인터페이스는 센서(예를 들어, 이미지 센서 또는 적외선 센서)를 통해 사용자의 제스쳐를 수신할 수 있다. 조작 인터페이스는 터치 방식을 이용하여 사용자 입력을 수신할 수 있다. 예를 들어, 조작 인터페이스 는 터치 센서를 통해 사용자 입력을 수신할 수 있다. 다양한 실시 예에 따라, 터치 방식은 비접촉 방식으 로 구현될 수 있다. 예를 들어, 터치 센서는 임계 거리 이내로 사용자 신체가 접근했는지 여부를 판단할 수 있 다. 터치 센서는 사용자가 터치 센서를 접촉하지 않는 경우에도 사용자 입력을 식별할 수 있다. 다른 구현 예에 따라, 터치 센서는 사용자가 터치 센서를 접촉하는 사용자 입력을 식별할 수 있다. 전자 장치는 상술한 조작 인터페이스 외에 다양한 방법으로 사용자 입력을 수신할 수 있다. 다양한 실시 예로, 전자 장치는 외부 원격 제어 장치를 통해 사용자 입력을 수신할 수 있다. 외부 원격 제어 장치 는 전자 장치에 대응되는 원격 제어 장치(예를 들어, 전자 장치의 전용 제어 기기) 또는 사용자의 휴 대용 통신 기기(예를 들어, 스마트폰 또는 웨어러블 디바이스)일 수 있다. 사용자의 휴대용 통신 기기는 전자 장치를 제어하기 위한 어플리케이션이 저장될 수 있다. 휴대용 통신 기기는 저장된 어플리케이션을 통해 사용자 입력을 획득하고, 획득된 사용자 입력을 전자 장치에 전송할 수 있다. 전자 장치는 휴대용 통 신 기기로부터 사용자 입력을 수신하여 사용자의 제어 명령에 대응되는 동작을 수행할 수 있다. 전자 장치는 음성 인식을 이용하여 사용자 입력을 수신할 수 있다. 다양한 실시 예에 따라, 전자 장치 는 전자 장치에 포함된 마이크를 통해 사용자 음성을 수신할 수 있다. 다른 실시 예에 따라, 전자 장 치는 마이크 또는 외부 장치로부터 사용자 음성을 수신할 수 있다. 구체적으로, 외부 장치는 외부 장치의 마이크를 통해 사용자 음성을 획득할 수 있고, 획득된 사용자 음성을 전자 장치에 전송할 수 있다. 외부 장치로부터 전송되는 사용자 음성은 오디오 데이터 또는 오디오 데이터가 변환된 디지털 데이터(예를 들어, 주 파수 도메인으로 변환된 오디오 데이터 등)일 수 있다. 전자 장치는 수신된 사용자 음성에 대응되는 동작 을 수행할 수 있다. 구체적으로, 전자 장치는 마이크를 통해 사용자 음성에 대응되는 오디오 데이터를 수 신할 수 있다. 그리고, 전자 장치는 수신된 오디오 데이터를 디지털 데이터로 변환할 수 있다. 그리고, 전 자 장치는 STT(Speech To Text) 기능을 이용하여 변환된 디지털 데이터를 텍스트 데이터로 변환할 수 있다. 다양한 실시 예에 따라, STT(Speech To Text) 기능은 전자 장치에서 직접 수행될 수 있다. 다른 실시 예에 따라, STT(Speech To Text) 기능은 외부 서버에서 수행될 수 있다. 전자 장치는 디지털 데 이터를 외부 서버로 전송할 수 있다. 외부 서버는 디지털 데이터를 텍스트 데이터로 변환하고, 변환된 텍스트 데이터를 바탕으로 제어 명령 데이터를 획득할 수 있다. 외부 서버는 제어 명령 데이터(이때, 텍스트 데이터도 포함될 수 있음.)를 전자 장치에 전송할 수 있다. 전자 장치는 획득된 제어 명령 데이터를 바탕으로 사용자 음성에 대응되는 동작을 수행할 수 있다. 전자 장치는 하나의 어시스턴스(또는 인공지능 비서, 예로, 빅스비TM 등)를 이용하여 음성 인식 기능을 제 공할 수 있으나, 이는 다양한 실시 예에 불과할 뿐 복수의 어시스턴스를 통해 음성 인식 기능을 제공할 수 있다. 이때, 전자 장치는 어시스턴스에 대응되는 트리거 워드 또는 리모컨에 존재하는 특정 키를 바탕으로 복수의 어시스턴스 중 하나를 선택하여 음성 인식 기능을 제공할 수 있다. 전자 장치는 스크린 인터렉션을 이용하여 사용자 입력을 수신할 수 있다. 스크린 인터렉션이란, 전자 장치 가 스크린(또는 투사면)에 투사한 이미지를 통해 기 결정된 이벤트가 발생하는지 식별하고, 기 결정된 이 벤트에 기초하여 사용자 입력을 획득하는 기능을 의미할 수 있다. 기 결정된 이벤트는 특정 위치(예를 들어, 사 용자 입력을 수신하기 위한 UI가 투사된 위치)에 특정 위치에 기 결정된 오브젝트가 식별되는 이벤트를 의미할 수 있다. 기 결정된 오브젝트는 사용자의 신체 일부(예를 들어, 손가락), 지시봉 또는 레이저 포인트 중 적어도 하나를 포함할 수 있다. 전자 장치는 투사된 UI에 대응되는 위치에 기 결정된 오브젝트가 식별되면, 투사된 UI를 선택하는 사용자 입력이 수신된 것으로 식별할 수 있다. 예를 들어, 전자 장치는 스크린에 UI를 표시하도록 가이드 이미지를 투사할 수 있다. 그리고, 전자 장치는 사용자가 투사된 UI를 선택하는지 여부 를 식별할 수 있다. 구체적으로, 전자 장치는 기 결정된 이벤트가 투사된 UI의 위치에서 식별되면, 사용자 가 투사된 UI를 선택한 것으로 식별할 수 있다. 투사되는 UI는 적어도 하나 이상의 항목(item)을 포함할 수 있 다. 전자 장치는 기 결정된 이벤트가 투사된 UI의 위치에 있는지 여부를 식별하기 위하여 공간 분석을 수 행할 수 있다. 전자 장치는 센서(예를 들어, 이미지 센서, 적외선 센서, 뎁스 카메라, 거리 센서 등)를 통 해 공간 분석을 수행할 수 있다. 전자 장치는 공간 분석을 수행함으로써 특정 위치(UI가 투사된 위치)에서 기 결정된 이벤트가 발생하는지 여부를 식별할 수 있다. 그리고, 특정 위치(UI가 투사된 위치)에서 기 결정된 이벤트가 발생되는 것으로 식별되면, 전자 장치는 특정 위치에 대응되는 UI를 선택하기 위한 사용자 입력 이 수신된 것으로 식별할 수 있다. 입출력 인터페이스는 오디오 신호 및 이미지 신호 중 적어도 하나를 입출력 하기 위한 구성이다. 입출력 인터페이스는 외부 장치로부터 오디오 및 이미지 신호 중 적어도 하나를 입력 받을 수 있으며, 외부 장치 로 제어 명령을 출력할 수 있다. 구현 예에 따라, 입출력 인터페이스는 오디오 신호만을 입출력하는 인터페이스와 이미지 신호만을 입출력 하는 인터페이스로 구현되거나, 오디오 신호 및 이미지 신호를 모두 입출력하는 하나의 인터페이스로 구현될 수 있다. 본 개시의 다양한 실시 예에 입출력 인터페이스는 HDMI(High Definition Multimedia Interface), MHL (Mobile High- Definition Link), USB (Universal Serial Bus), USB C-type, DP(Display Port), 썬더볼트 (Thunderbolt), VGA(Video Graphics Array)포트, RGB 포트, D-SUB(Dsubminiature) 및 DVI(Digital Visual Interface) 중 적어도 하나 이상의 유선 입출력 인터페이스로 구현될 수 있다. 다양한 실시 예에 따라, 유선 입 출력 인터페이스는 오디오 신호만을 입출력하는 인터페이스와 이미지 신호만을 입출력하는 인터페이스로 구현되 거나, 오디오 신호 및 이미지 신호를 모두 입출력하는 하나의 인터페이스로 구현될 수 있다. 전자 장치는 유선 입출력 인터페이스를 통해 데이터를 수신할 수 있으나, 이는 다양한 실시 예에 불과할 뿐, 유선 입출력 인터페이스를 통해 전력을 공급받을 수 있다. 예를 들어, 전자 장치는 USB C-type을 통해 외부 배터리에서 전력을 공급받거나 전원 어뎁터를 통해 콘센트에서 전력을 공급받을 수 있다. 또 다른 예로, 전자 장치는 DP를 통해 외부 장치(예를 들어, 노트북이나 모니터 등)로부터 전력을 공급받을 수 있다. 오디오 신호는 유선 입출력 인터페이스를 통해 입력 받고, 이미지 신호는 무선 입출력 인터페이스(또는 통신 인 터페이스)를 통해 입력 받도록 구현될 수 있다. 또는, 오디오 신호는 무선 입출력 인터페이스(또는 통신 인터페 이스)를 통해 입력 받고, 이미지 신호는 유선 입출력 인터페이스를 통해 입력 받도록 구현될 수 있다. 스피커는 오디오 신호를 출력하는 구성이다. 특히, 스피커는 오디오 출력 믹서, 오디오 신호 처리기, 음향 출력 모듈을 포함할 수 있다. 오디오 출력 믹서는 출력할 복수의 오디오 신호들을 적어도 하나의 오디오 신호로 합성할 수 있다. 예를 들면, 오디오 출력 믹서는 아날로그 오디오 신호 및 다른 아날로그 오디오 신호 (예: 외부로부터 수신한 아날로그 오디오 신호)를 적어도 하나의 아날로그 오디오 신호로 합성할 수 있다. 음향 출력 모듈은, 스피커 또는 출력 단자를 포함할 수 있다. 다양한 실시 예에 따르면 음향 출력 모듈은 복수의 스 피커들을 포함할 수 있고, 이 경우, 음향 출력 모듈은 본체 내부에 배치될 수 있고, 음향 출력 모듈의 진동판의 적어도 일부를 가리고 방사되는 음향은 음도관(waveguide)을 통과하여 본체 외부로 전달할 수 있다. 음향 출력 모듈은 복수의 음향 출력 유닛을 포함하고, 복수의 음향 출력 유닛이 본체의 외관에 대칭 배치됨으로써 모든 방 향으로, 즉 360도 전 방향으로 음향을 방사할 수 있다. 마이크는 사용자 음성이나 기타 소리를 입력 받아 오디오 데이터로 변환하기 위한 구성이다. 마이크 는 활성화 상태에서 사용자의 음성을 수신할 수 있다. 예를 들어, 마이크는 전자 장치의 상측이나 전 면 방향, 측면 방향 등에 일체형으로 형성될 수 있다. 마이크는 아날로그 형태의 사용자 음성을 수집하는 마이크, 수집된 사용자 음성을 증폭하는 앰프 회로, 증폭된 사용자 음성을 샘플링하여 디지털 신호로 변환하는 A/D 변환회로, 변환된 디지털 신호로부터 노이즈 성분을 제거하는 필터 회로 등과 같은 다양한 구성을 포함할 수 있다. 전원부는 외부로부터 전력을 공급받아 전자 장치의 다양한 구성에 전력을 공급할 수 있다. 본 개시의 다양한 실시 예에 따른 전원부는 다양한 방식을 통해 전력을 공급받을 수 있다. 다양한 실시 예로, 전원부 는 도 1에 도시된 바와 같은 커넥터를 이용하여 전력을 공급받을 수 있다. 전원부는 220V의 DC전원 코드를 이용하여 전력을 공급받을 수 있다. 다만 이에 한정되지 않고, 전자 장치는 USB 전원 코드를 이용하여 전력을 공급받거나 무선 충전 방식을 이용하여 전력을 공급받을 수 있다. 전원부는 내부 배터리 또는 외부 배터리를 이용하여 전력을 공급받을 수 있다. 본 개시의 다양한 실시 예 에 따른 전원부는 내부 배터리를 통해 전력을 공급받을 수 있다. 일 예로, 전원부는 220V의 DC 전원 코드, USB 전원 코드 및 USB C-Type 전원 코드 중 적어도 하나를 이용하여 내부 배터리의 전력을 충전하고, 충 전된 내부 배터리를 통해 전력을 공급받을 수 있다. 본 개시의 다양한 실시 예에 따른 전원부는 외부 배터 리를 통해 전력을 공급받을 수 있다. 일 예로, USB 전원 코드, USB C-Type 전원 코드, 소켓 홈 등 다양한 유선 통신 방식을 통하여 전자 장치와 외부 배터리의 연결이 수행되면, 전원부는 외부 배터리를 통해 전력 을 공급받을 수 있다. 즉, 전원부는 외부 배터리로부터 바로 전력을 공급받거나, 외부 배터리를 통해 내부 배터리를 충전하고 충전된 내부 배터리로부터 전력을 공급받을 수 있다. 본 개시에 따른 전원부는 상술한 복수의 전력 공급 방식 중 적어도 하나 이상을 이용하여 전력을 공급받을 수 있다. 소비 전력과 관련하여, 전자 장치는 소켓 형태 및 기타 표준 등을 이유로 기설정된 값(예로, 43W) 이하의 소비 전력을 가질 수 있다. 이때, 전자 장치는 배터리 이용 시에 소비 전력을 줄일 수 있도록 소비 전력을 가변시킬 수 있다. 즉, 전자 장치는 전원 공급 방법 및 전원 사용량 등을 바탕으로 소비 전력을 가변시킬 수 있다. 구동부는 전자 장치에 포함된 적어도 하나의 하드웨어 구성을 구동할 수 있다. 구동부는 물리적 인 힘을 생성하여 전자 장치에 포함된 적어도 하나의 하드웨어 구성에 전달할 수 있다. 구동부는 전자 장치에 포함된 하드웨어 구성의 이동(예를 들어, 전자 장치의 이동) 또는 구성의 회전(예를 들어, 프로젝션 렌즈의 회전) 동작을 위해 구동 전력을 발생시킬 수 있다. 구동부는 프로젝션부의 투사 방향(또는 투사 각도)을 조절할 수 있다. 구동부는 전자 장치(10 0)의 위치를 이동시킬 수 있다. 구동부는 전자 장치를 이동시키기 위해 이동 부재를 제어할 수 있다. 예를 들어, 구동부는 모터를 이용하여 이동 부재를 제어할 수 있다. 센서부는 적어도 하나의 센서를 포함할 수 있다. 구체적으로, 센서부는 전자 장치의 기울기를 센싱하는 기울기 센서, 이미지를 촬상하는 이미지 센서 중 적어도 하나를 포함할 수 있다. 기울기 센서는 가속 도 센서, 자이로 센서일 수 있고, 이미지 센서는 카메라 또는 뎁스 카메라를 의미할 수 있다. 기울기 센서는 움 직임 센서로 기재될 수 있다. 센서부는 기울기 센서 또는 이미지 센서 이외에 다양한 센서를 포함할 수 있 다. 예를 들어, 센서부는 조도 센서, 거리 센서를 포함할 수 있다. 거리 센서는 ToF(Time of Flight)일 수 있다. 센서부는 라이다 센서를 포함할 수 있다. 전자 장치는 외부 기기와 연동하여 조명 기능을 제어할 수 있다. 구체적으로, 전자 장치는 외부 기기 로부터 조명 정보를 수신할 수 있다. 조명 정보는 외부 기기에서 설정된 밝기 정보 또는 색온도 정보 중 적어도 하나를 포함할 수 있다. 외부 기기는 전자 장치와 동일한 네트워크에 연결된 기기(예를 들어, 동일한 홈/ 회사 네트워크에 포함된 IoT 기기) 또는 전자 장치와 동일한 네트워크는 아니지만 전자 장치와 통신 가능한 기기(예를 들어, 원격 제어 서버)를 의미할 수 있다. 예를 들어, 전자 장치와 동일한 네트워크에 포함된 외부 조명 기기(IoT 기기)가 붉은색 조명을 50의 밝기로 출력하고 있다고 가정한다. 외부 조명 기기(IoT 기기)는 조명 정보(예를 들어, 붉은색 조명을 50의 밝기로 출력하고 있음을 나타내는 정보)를 전자 장치에 직접적으로 또는 간접적으로 전송할 수 있다. 전자 장치는 외부 조명 기기로부터 수신된 조명 정보에 기초 하여 광원의 출력을 제어할 수 있다. 예를 들어, 외부 조명 기기로부터 수신된 조명 정보가 붉은색 조명을 50의 밝기로 출력하는 정보를 포함하면, 전자 장치는 붉은색 조명을 50의 밝기로 출력할 수 있다. 전자 장치는 생체 정보에 기초하여 조명 기능을 제어할 수 있다. 구체적으로, 적어도 하나의 프로세서 는 사용자의 생체 정보를 획득할 수 있다. 생체 정보는, 사용자의 체온, 심장 박동 수, 혈압, 호흡, 심전 도 중 적어도 하나를 포함할 수 있다. 생체 정보는 상술한 정보 이외에 다양한 정보가 포함될 수 있다. 일 예로, 전자 장치는 생체 정보를 측정하기 위한 센서를 포함할 수 있다. 적어도 하나의 프로세서는 센 서를 통해 사용자의 생체 정보를 획득할 수 있고, 획득된 생체 정보에 기초하여 광원의 출력을 제어할 수 있다. 다른 예로, 적어도 하나의 프로세서는 입출력 인터페이스를 통해 생체 정보를 외부 기기로부터 수신 할 수 있다. 외부 기기는 사용자의 휴대용 통신 기기(예를 들어, 스마트폰 또는 웨어러블 디바이스)를 의미할 수 있다. 적어도 하나의 프로세서는 외부 기기로부터 사용자의 생체 정보를 획득할 수 있고, 획득된 생체정보에 기초하여 광원의 출력을 제어할 수 있다. 구현 예에 따라, 전자 장치는 사용자가 수면하고 있는지 여부를 식별할 수 있고, 사용자가 수면 중(또는 수면 준비 중)인 것으로 식별되면 적어도 하나의 프로세서(11 1)는 사용자의 생체 정보에 기초하여 광원의 출력을 제어할 수 있다. 본 개시의 다양한 실시 예에 따른 전자 장치는 다양한 스마트 기능을 제공할 수 있다. 구체적으로, 전자 장치는 전자 장치를 제어하기 위한 휴대 단말 장치와 연결되어 휴대 단말 장치에서 입력되는 사용자 입력을 통해 전자 장치에서 출력되는 화면이 제어될 수 있다. 일 예로, 휴대 단말 장치는 터치 디스플레이를 포함하는 스마트폰으로 구현될 수 있으며, 전자 장치는 휴대 단말 장치에서 제공하는 화면 데이터를 휴대 단말 장치로부터 수신하여 출력하고, 휴대 단말 장치에서 입력되는 사용자 입력에 따라 전 자 장치에서 출력되는 화면이 제어될 수 있다. 전자 장치는 미라캐스트(Miracast), Airplay, 무선 DEX, Remote PC 방식 등 다양한 통신 방식을 통해 휴 대 단말 장치와 연결을 수행하여 휴대 단말 장치에서 제공하는 컨텐츠 또는 음악을 공유할 수 있다. 그리고, 휴대 단말 장치와 전자 장치는 다양한 연결 방식으로 연결이 수행될 수 있다. 다양한 실시 예로, 휴대 단말 장치에서 전자 장치를 검색하여 무선 연결을 수행하거나, 전자 장치에서 휴대 단말 장치를 검색하여 무선 연결을 수행할 수 있다. 그리고, 전자 장치는 휴대 단말 장치에서 제공하는 컨텐츠를 출력 할 수 있다. 다양한 실시 예로, 휴대 단말 장치에서 특정 컨텐츠 또는 음악이 출력 중인 상태에서 휴대 단말 장치를 전자 장 치 근처에 위치시킨 후 휴대 단말 장치의 디스플레이를 통해 기 설정된 제스처가 감지되면(예로, 모션 탭 뷰), 전자 장치는 휴대 단말 장치에서 출력 중인 컨텐츠 또는 음악을 출력할 수 있다. 다양한 실시 예로, 휴대 단말 장치에서 특정 컨텐츠 또는 음악이 출력 중인 상태에서 휴대 단말 장치가 전자 장 치와 기 설정 거리 이하로 가까워지거나(예로, 비접촉 탭뷰) 휴대 단말 장치가 전자 장치와 짧은 간 격으로 두 번 접촉되면(예로, 접촉 탭뷰), 전자 장치는 휴대 단말 장치에서 출력 중인 컨텐츠 또는 음악을 출력할 수 있다. 상술한 실시 예에서는 휴대 단말 장치에서 제공되고 있는 화면과 동일한 화면이 전자 장치에서 제공되는 것으로 설명하였으나, 본 개시는 이에 한정되지 않는다. 즉, 휴대 단말 장치와 전자 장치 간 연결이 구축 되면, 휴대 단말 장치에서는 휴대 단말 장치에서 제공되는 제1 화면이 출력되고, 전자 장치에서는 제1 화 면과 상이한 휴대 단말 장치에서 제공되는 제2 화면이 출력될 수 있다. 일 예로, 제1 화면은 휴대 단말 장치에 설치된 제1 어플리케이션이 제공하는 화면이며, 제2 화면은 휴대 단말 장치에 설치된 제2 어플리케이션이 제공 하는 화면일 수 있다. 일 예로, 제1 화면과 제2 화면은 휴대 단말 장치에 설치된 하나의 어플리케이션에서 제공 하는 서로 상이한 화면일 수 있다. 일 예로, 제1 화면은 제2 화면을 제어하기 위한 리모컨 형식의 UI를 포함하 는 화면일 수 있다. 본 개시에 따른 전자 장치는 대기 화면을 출력할 수 있다. 일 예로, 전자 장치가 외부 장치와 연결이 수행되지 않은 경우 또는 외부 장치로부터 기 설정된 시간 동안 수신되는 입력이 없는 경우 전자 장치는 대기 화면을 출력할 수 있다. 전자 장치가 대기 화면을 출력하기 위한 조건은 상술한 예에 한정되지 않고 다양한 조건들에 의해 대기 화면이 출력될 수 있다. 전자 장치는 블루 스크린 형태의 대기 화면을 출력할 수 있으나, 본 개시는 이에 한정되지 않는다. 일 예 로, 전자 장치는 외부 장치로부터 수신되는 데이터에서 특정 오브젝트의 형태만을 추출하여 비정형 오브젝 트를 획득하고, 획득된 비정형 오브젝트를 포함하는 대기 화면을 출력할 수 있다. 전자 장치는 디스플레이(미도시)를 더 포함할 수 있다. 디스플레이(미도시)는 LCD(Liquid Crystal Display), OLED(Organic Light Emitting Diodes) 디스플레이, PDP(Plasma Display Panel) 등과 같은 다양한 형태의 디스플레이로 구현될 수 있다. 디스플레이(미도시)내에는 a-si TFT(amorphous silicon thin film transistor), LTPS(low temperature poly silicon) TFT, OTFT(organic TFT) 등과 같은 형태로 구현될 수 있는 구동 회로, 백라이트 유닛 등도 함께 포함될 수 있다. 디스플레이(미도 시)는 터치 센서와 결합된 터치 스크린, 플렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display, three-dimensional display) 등으로 구현될 수 있다. 본 개시의 다양한 실시 예에 따른, 디스플레이 (미도시)는 이미지를 출력하는 디스플레이 패널뿐만 아니라, 디스플레이 패널을 하우징하는 베젤을 포함할 수 있다. 특히, 본 개시의 다양한 실시 예에 따른, 베젤은 사용자 인터렉션을 감지하기 위한 터치 센서(미도시)를포함할 수 있다. 전자 장치는 셔터부(미도시)를 더 포함할 수 있다. 셔터부(미도시)는 셔터, 고정 부재, 레일 또는 몸체 중 적어도 하나를 포함할 수 있다. 셔터는 프로젝션부에서 출력되는 광을 차단할 수 있다. 고정 부재는 셔터의 위치를 고정시킬 수 있다. 레 일은 셔터 및 고정 부재를 이동시키는 경로일 수 있다. 몸체는 셔터 및 고정 부재를 포함하는 구성일 수 있다. 도 3은 일 실시 예에 따라, 도 2의 전자 장치의 구체적인 구성을 설명하기 위한 블록도이다. 도 4는 일 실시 예에 따라, 얼굴 영역의 거리 정보를 획득하는 동작을 설명하기 위한 도면이다. 도 4를 참조하면, 전자 장치는 촬상 이미지 획득 모듈, 얼굴 영역 검출 모듈, 얼굴 영역의 크기 정보 획득 모듈, 인공 지능 모델, 얼굴 영역의 거리 정보 제공 모듈 중 적어도 하나를 포함할 수 있다. 촬상 이미지 획득 모듈은 촬상 이미지를 획득할 수 있다. 촬상 이미지는 사용자를 포함하는 이미지일 수 있다. 예를 들어, 촬상 이미지는 RGB 이미지를 포함할 수 있다. 일 예로, 전자 장치는 전자 장치에 포함된 카메라를 이용하여 촬상 이미지를 획득할 수 있다. 일 예로, 전자 장치는 외부 기기로부터 촬상 이미지를 수신할 수 있다. 촬상 이미지 획득 모듈은 획득된 촬상 이미지를 저장할 수 있다. 촬상 이미지 획득 모듈은 촬상 이미 지를 얼굴 영역 검출 모듈에 전송할 수 있다. 얼굴 영역 검출 모듈은 얼굴 영역 검출 모듈로부터 촬상 이미지를 수신할 수 있다. 얼굴 영역 검출 모듈은 촬상 이미지에서 얼굴 영역을 획득(또는 검출 또는 식별)할 수 있다. 촬상 이미지는 사용자의 얼굴을 포함할 수 있다. 전자 장치는 촬상 이미지에 포함된 사용자의 얼굴을 나타내는 얼굴 영역을 획 득할 수 있다. 얼굴 영역은 얼굴 영역을 나타내는 정보, 얼굴 영역에 대응되는 정보, 얼굴 영역과 관련된 정보 로 기재될 수 있다. 얼굴 영역 검출 모듈은 얼굴 영역의 좌표 정보를 획득할 수 있다. 촬상 이미지에 사용자의 얼굴 영역이 식별되면, 얼굴 영역 검출 모듈은 얼굴 영역의 좌표 정보를 획득 할 수 있다. 좌표 정보는 얼굴 영역이 표시된 위치를 나타내는 좌표값을 포함할 수 있다. 좌표 정보는 2차원 값 일 수 있다. 얼굴 영역 검출 모듈은 얼굴 영역의 좌표 정보를 얼굴 영역의 크기 정보 획득 모듈에 전송할 수 있다. 얼굴 영역의 크기 정보 획득 모듈은 얼굴 영역 검출 모듈로부터 얼굴 영역의 좌표 정보를 수신할 수 있다. 얼굴 영역의 크기 정보 획득 모듈는 얼굴 영역의 좌표 정보에 기초하여 얼굴 영역의 크기 정보를 획 득할 수 있다. 얼굴 영역의 크기 정보는 얼굴 영역의 가로 길이, 얼굴 영역의 세로 길이, 얼굴 영역의 넓이 중 적어도 하나를 포함할 수 있다. 얼굴 영역의 크기 정보는 얼굴 영역의 대각선 길이를 더 포함할 수 있다. 얼굴 영역의 크기 정보 획득 모듈은 얼굴 영역의 크기 정보를 인공 지능 모델에 전송할 수 있다. 인공 지능 모델은 얼굴 영역의 크기 정보 획득 모듈로부터 얼굴 영역의 크기 정보를 수신할 수 있다. 인공 지능 모델은 머신 러닝 모델 또는 딥 러닝 모델 등으로 기재될 수 있다. 인공 지능 모델은 얼굴 영역의 크기 정보 획득 모듈로부터 수신된 얼굴 영역의 크기 정보를 입력 데 이터로 이용하여 얼굴 영역의 거리 정보를 획득할 수 있다. 얼굴 영역의 거리 정보는 인공 지능 모델의 출 력 데이터일 수 있다. 전자 장치는 출력 데이터로써 얼굴 영역의 거리 정보를 획득할 수 있다. 인공 지능 모델은 얼굴 영역의 거리 정보를 얼굴 영역의 거리 정보 제공 모듈에 전송할 수 있다. 얼굴 영역의 거리 정보 제공 모듈은 인공 지능 모델로부터 얼굴 영역의 거리 정보를 획득할 수 있다. 얼굴 영역의 거리 정보 제공 모듈은 얼굴 영역의 크기 정보를 이용하여 다양한 기능을 수행할 수 있다. 다 양한 기능은 얼굴 영역의 거리 정보를 표시하는 기능, 얼굴 영역의 거리 정보를 이용하여 투사 이미지의 해상도를 결정하는 기능, 사용자의 위치를 판단하는 기능, 사용자와 일정 거리를 유지하는 기능, 사용자 를 회피하는 기능 중 적어도 하나를 포함할 수 있다. 도 5는 일 실시 예에 따라, 입력 이미지에서 얼굴 영역을 검출하는 동작을 설명하기 위한 도면이다. 도 5의 실시 예를 참조하면, 전자 장치는 촬상 이미지를 획득할 수 있다. 촬상 이미지에 사용자 가 포함되어 있음을 가정한다. 도 5의 실시 예를 참조하면, 전자 장치는 촬상 이미지에서 사용자의 얼굴 영역을 획득(또는 검출 또는 식별)할 수 있다. 전자 장치는 얼굴 영역의 좌표 정보를 획득할 수 있다. 좌표 정보는 사 용자의 얼굴을 나타내는 얼굴 영역에 대응되는 적어도 하나의 좌표값을 포함할 수 있다. 예를 들어, 좌표 정보는 2차원 좌표값을 포함할 수 있다. 도 5의 실시 예를 참조하면, 전자 장치는 얼굴 영역의 좌표 정보를 이용하여 얼굴 영역의 크기 정보를 획득할 수 있다. 크기 정보는 가로 길이, 세로 길이, 넓이 중 적어도 하나를 포함할 수 있다. 전자 장치는 얼굴 영역의 가로 길이, 얼굴 영역의 세로 길이 중 적어도 하나를 획득 할 수 있다. 일 예로, 전자 장치는 얼굴 영역의 가로 길이를 획득할 수 있다. 일 예로, 전자 장치는 얼굴 영역의 얼굴 영역의 세로 길이를 획득할 수 있다. 일 예로, 전자 장치는 얼굴 영역의 가로 길이 및 얼굴 영역의 세로 길이를 획득할 수 있다. 전자 장치는 얼굴 영역의 넓이를 획득할 수 있다. 전자 장치는 가로 길이, 세로 길이, 넓이 중 적어도 하나를 포함하는 얼굴 영역의 크기 정보를 획득 할 수 있다. 도 6은 일 실시 예에 따라, 얼굴 영역의 거리 정보를 표시하는 동작을 설명하기 위한 도면이다. 전자 장치는 얼굴 영역의 크기 정보에 기초하여 얼굴 영역의 거리 정보(예를 들어, 0.8m)를 획득할 수 있 다. 도 6의 실시 예를 참조하면, 전자 장치는 얼굴 영역의 거리 정보를 사용자에 대응되는 위치에 출 력할 수 있다. 전자 장치는 얼굴 영역의 거리 정보를 나타내는 UI를 출력할 수 있다. 전자 장치(10 0)는 사용자에 대응되는 위치에 UI를 출력할 수 있다. 사용자에 대응되는 위치는 사용자의 위 치에 기초하여 기 설정된 방향으로 기 설정된 거리만큼 떨어진 위치일 수 있다. 전자 장치는 기 설정된 위 치 및 기 설정된 표시 설정(크기, 색상, 폰트 등)에 기초하여 UI를 출력할 수 있다. 도 6의 실시 예를 참조하면, 전자 장치는 얼굴 영역의 거리 정보를 얼굴 영역의 거리 정보를 나타내 는 UI 및 사용자의 얼굴 영역을 나타내는 UI를 사용자에 대응되는 위치에 출력할 수 있다. 전자 장치는 사용자에 대응되는 위치에 UI(621,622)를 출력할 수 있다. 사용자에 대응되는 위치는 사용자의 위치에 기초하여 기 설정된 방향으로 기 설정된 거리만큼 떨어진 위치일 수 있다. 전자 장치(10 0)는 기 설정된 위치 및 기 설정된 표시 설정(크기, 색상, 폰트 등)에 기초하여 UI(621, 622)를 출력할 수 있다. 도 7은 일 실시 예에 따라, 인공 지능 모델을 이용하는 동작을 설명하기 위한 도면이다. 도 7을 참조하면, 전자 장치는 인공 지능 모델을 포함할 수 있다. 인공 지능 모델은 도 4의 인 공 지능 모델에 대응될 수 있다. 중복 설명을 생략한다. 인공 지능 모델은 입력 데이터를 수신할 수 있다. 입력 데이터는 얼굴 영역의 크기 정보를 수신 (또는 획득)할 수 있다. 입력 데이터는 얼굴 영역의 가로 길이, 얼굴 영역 세로 길이, 얼굴 영역 넓이 중 적어도 하나를 포함하는 크기 정보를 포함할 수 있다. 입력 데이터는 얼굴 영역의 대각선 길이, 얼굴 영역의 모양 중 적어도 하나를 추가적으로 포함할 수 있다. 인공 지능 모델은 입력 데이터를 수신하여 출력 데이터를 획득할 수 있다. 인공 지능 모델(70 0)은 학습된 가중치를 포함할 수 있다. 출력 데이터는 얼굴 영역의 거리 정보를 포함할 수 있다.인공 지능 모델은 얼굴 영역의 크기 정보(가로 길이, 세로 길이, 넓이)를 입력 데이터로써 수신하여 얼굴 영역의 거리 정보를 출력 데이터로써 획득할 수 있다. 인공 지능 모델은 미리 학습된 가중치를 이용하여 입력 데이터에 대응되는 출력 데이터를 획득할 수 있다. 다양한 실시 예에 따라, 입력 데이터는 사용자의 머리 방향과 관련된 회전 축 정보(yaw, pitch, roll)를 추 가로 포함할 수 있다. 전자 장치는 사용자의 머리 방향과 관련된 회전 축 정보(yaw, pitch, roll)를 추가로 이용하여 출력 데이터를 획득할 수 있다. 다양한 실시 예에 따라, 입력 데이터는 사용자의 얼굴 영역과 관련된 특징 오브젝트(또는 랜드 마크 오브젝 트)를 포함할 수 있다. 특징 오브젝트는 사람의 눈, 코, 입 중 적어도 하나를 포함할 수 있다. 전자 장치 는 특징 오브젝트를 이용하여 출력 데이터를 획득할 수 있다. 특징 오브젝트를 이용하는 동작은 특징 오브젝트 의 위치를 이용하는 동작을 포함할 수 있다. 도 8은 일 실시 예에 따라, 인공 지능 모델을 학습하는 동작을 설명하기 위한 도면이다. 전자 장치는 제1 타입의 제1 학습 데이터 획득 모듈, 제2 타입의 제2 학습 데이터 획득 모듈, 얼굴 영역 검출 모듈, 학습 데이터 정렬 모듈, 얼굴 영역의 속성 정보 획득 모듈, 학습 데이터 세트 생성 모듈, 학습 모듈, 인공 지능 모델 저장 모듈 중 적어도 하나를 포함할 수 있다. 제1 타입의 제1 학습 데이터 획득 모듈은 제1 학습 데이터를 획득 및 저장할 수 있다. 제1 학습 데이터는 제1 타입의 이미지를 포함할 수 있다. 제1 학습 데이터는 RGB 이미지를 포함할 수 있다. 제1 학습 데이터는 사 용자를 촬상한 RGB 이미지를 포함할 수 있다. 전자 장치는 RGB 카메라를 이용하여 제1 학습 데이터를 획득할 수 있다. 제1 타입의 제1 학습 데이터 획득 모듈은 제1 학습 데이터를 얼굴 영역 검출 모듈, 학습 데이터 정렬 모듈 중 적어도 하나에 전송할 수 있다. 제2 타입의 제2 학습 데이터 획득 모듈는 제2 학습 데이터를 획득 및 저장할 수 있다. 제2 학습 데이터는 제2 타입의 이미지를 포함할 수 있다. 제2 학습 데이터는 뎁스 이미지를 포함할 수 있다. 전자 장치는 뎁 스 카메라, 3D TOF(Time Of Flight), 라이다(Lidar) 중 하나를 이용하여 뎁스 이미지를 획득할 수 있다. 제2 학습 데이터는 사용자를 촬상한(또는 센싱한) 뎁스 이미지를 포함할 수 있다. 제2 학습 데이터는 뎁스 맵(depth map) 정보를 포함할 수 있다. 뎁스 맵은 이미지 좌표 각각에 대응되는 뎁스(또는 거리) 값을 포함할 수 있다. 예를 들어, 뎁스 맵은 제1 픽셀에 대응되는 제1 거리값, 제2 픽셀에 대응되는 제2 거리값을 포함할 수 있다. 제1 학습 데이터는 제1 학습 이미지 정보, 제1 학습 이미지로 기재될 수 있다. 제2 학습 데이터는 제2 학습 이 미지 정보, 제2 학습 이미지로 기재될 수 있다. 제2 타입의 제2 학습 데이터 획득 모듈은 제2 학습 데이터를 학습 데이터 정렬 모듈에 전송할 수 있 다. 제1 학습 데이터 및 제2 학습 데이터는 동일한 사용자를 촬상한 이미지(또는 데이터)를 나타낼 수 있다. 제 1 학습 데이터는 제1 센서(예를 들어, 카메라)로 사용자를 촬상한 이미지일 수 있다. 제2 학습 데이터는 제 2 센서(예를 들어, 뎁스 카메라)로 사용자를 촬상한 이미지일 수 있다. 제1 학습 데이터 및 제2 학습 데이 터는 동일한 시점에 사용자를 촬상한(또는 센싱된) 이미지일 수 있다. 제2 학습 데이터는 제1 학습 데이터 에 대응한 이미지일 수 있다. 제1 학습 데이터, 제2 학습 데이터는 좌표값을 포함할 수 있다. 다양한 실시 예에 따라, 복수의 학습 데이터는 조도가 낮은 환경에서 획득되는 데이터를 포함할 수 있다. 조도 가 낮은 환경에서 센싱된 학습 데이터를 이용하는 경우 조도 변화에 민감하지 않도록 학습이 이루어질 수 있다. 복수의 학습 데이터는 사용자의 얼굴 보다 아래에서 사용자를 바라보는 방향으로 촬상된 학습 데이터를 포함할 수 있다. 아래쪽에서 사용자를 바라보는 방향으로 촬상된 학습 데이터를 이용하는 경우 카메라 각도 변화에 민 감하지 않도록 학습이 이루어질 수 있다. 다양한 실시 예에 따라, 복수의 학습 데이터는 데이터 증강(augmentation)을 진행하여 생성된 데이터를 포함할 수 있다. 데이터 증강 동작은 좌우 플립(flip), 대비(contrast) 조절, 블러(blur) 처리, 잘라내기(crop) 등의 기능을 포함할 수 있다. 얼굴 영역 검출 모듈은 제1 타입의 제1 학습 데이터 획득 모듈로부터 제1 학습 데이터를 수신할 수 있다. 얼굴 영역 검출 모듈은 제1 학습 데이터에 기초하여 얼굴 영역의 좌표 정보를 획득할 수 있다. 얼굴 영역 검출 모듈은 얼굴 영역의 좌표 정보를 얼굴 영역의 속성 정보 획득 모듈에 전송할 수 있다. 학습 데이터 정렬 모듈은 제1 타입의 제1 학습 데이터 획득 모듈로부터 제1 학습 데이터를 수신할 수 있다. 학습 데이터 정렬 모듈는 제2 타입의 제2 학습 데이터 획득 모듈로부터 제2 학습 데이터를 수 신할 수 있다. 학습 데이터 정렬 모듈은 제1 학습 데이터 및 제2 학습 데이터를 이용하여 정렬 기능을 수 행할 수 있다. 학습 데이터 정렬 모듈은 제1 학습 데이터와 제2 학습 데이터를 동일 좌표로 표현하기 위해 정렬 기능을 수행할 수 있다. 전자 장치는 제1 학습 데이터 및 제2 학습 데이터에서 획득되는 공통 특징에 기초하여 정렬 기능을 수행할 수 있다. 공통 특징은 얼굴 영역과 관련된 기준선, 얼굴 영역과 관련된 기준점 중 적어도 하나일 수 있다. 제1 학습 데이터에서 나타내는 사용자의 위치와 제2 학습 데이터에서 나타내는 사용자의 위치가 일부 상이할 수 있다. 제1 학습 데이터를 획득하는데 이용되는 제1 센서의 배치 위치와 제2 학습 데이터를 획득하는 데 이용되는 제2 센서의 배치 위치가 상이하기 때문이다. 제1 센서와 제2 센서의 배치 위치를 완벽히 동일하게 구현하기 어렵다. 전자 장치는 제1 학습 데이터에 포함된 사용자의 좌표 정보와 제2 학습 데이터에 포함된 사용자의 좌표 정보가 맵핑되도록 정렬 기능을 수행할 수 있다. 정렬 기능과 관련된 설명을 도 9, 도 10에서 기재한다. 정렬 기능은 맵핑 기능으로 기재될 수 있다. 학습 데이터 정렬 모듈은 정렬 기능을 수행하여 정렬된 제2 학습 데이터를 획득할 수 있다. 정렬된 제2 학 습 데이터는 제1 학습 데이터의 좌표 정보에 맵핑된 좌표 정보를 갖는 이미지일 수 있다. 제1 학습 데이터에 포 함된 사용자의 좌표값과 제2 학습 데이터에 포함된 사용자의 좌표값이 일치할 수 있다. 학습 데이터 정 렬 모듈은 정렬된 제2 학습 데이터를 얼굴 영역의 속성 정보 획득 모듈에 전송할 수 있다. 정렬된 제 2 학습 데이터는 이미지가 변경되었다는 점에서 제3 학습 데이터로 기재될 수 있다. 얼굴 영역의 속성 정보 획득 모듈은 얼굴 영역 검출 모듈로부터 얼굴 영역의 좌표 정보를 수신할 수 있다. 얼굴 영역의 속성 정보 획득 모듈은 학습 데이터 정렬 모듈로부터 정렬된 제2 학습 데이터를 수신할 수 있다. 얼굴 영역의 속성 정보 획득 모듈은 얼굴 영역의 좌표 정보 및 정렬된 제2 학습 데이터에 기초하여 얼굴 영역의 속성 정보를 획득할 수 있다. 얼굴 영역의 속성 정보는 얼굴 영역의 크기 정보, 얼굴 영역의 거리 정보 중 적어도 하나를 포함할 수 있다. 얼 굴 영역의 크기 정보는 얼굴 영역의 가로 길이, 얼굴 영역의 세로 길이, 얼굴 영역의 넓이 중 적어도 하나를 포 함할 수 있다. 얼굴 영역의 거리 정보는 촬상 위치(또는 센싱 위치)부터 얼굴 영역의 기준점까지의 거리를 포함 할 수 있다. 기준점은 얼굴 영역을 대표하는 점일 수 있다. 기준점은 대표점으로 기재될 수 있다. 기준점은 얼 굴 영역의 중앙값 또는 평균값에 기초하여 획득될 수 있다. 얼굴 영역의 속성 정보 획득 모듈은 얼굴 영역의 속성 정보를 학습 데이터 세트 생성 모듈에 전송할 수 있다. 학습 데이터 세트 생성 모듈은 얼굴 영역의 속성 정보 획득 모듈로부터 얼굴 영역의 속성 정보를 수 신할 수 있다. 학습 데이터 세트 생성 모듈은 얼굴 영역의 속성 정보를 맵핑한 제1 학습 데이터 세트를 생 성(또는 획득)할 수 있다. 전자 장치는 제1 학습 데이터를 이용하여 사용자의 얼굴 영역이 나타내는 속성 정보(가로 길이, 세로 길이, 넓이, 촬상 위치-사용자 사이의 거리 등)를 추출(또는 획득)할 수 있다. 학습 데이터 세트 생성 모듈은 제1 학습 데이터 세트를 학습 모듈에 전송할 수 있다. 학습 모듈은 학습 데이터 세트 생성 모듈로부터 제1 학습 데이터 세트를 수신할 수 있다. 학습 모듈 은 제1 학습 데이터 세트 및 기 저장된 학습 데이터(예를 들어, 제2 학습 데이터 세트, 제3 학습 데이터 세트 등)에 기초하여 인공 지능 모델에 대한 학습을 수행할 수 있다. 학습 모듈은 적어도 하나의 가중치 (또는 파라미터)를 이용하여 입력 데이터에 대응되는 출력 데이터를 획득하도록 학습할 수 있다. 입력 데이터는 얼굴 영역의 크기 정보일 수 있다. 출력 데이터는 얼굴 영역의 거리 정보일 수 있다. 학습 모듈은 얼굴 영 역의 크기 정보 및 적어도 하나의 가중치에 기초하여 얼굴 영역의 거리 정보가 출력되도록 적어도 하나의 가중 치를 학습할 수 있다. 학습 모듈은 반복된 복수의 학습 동작에 기초하여 최종적으로 학습된 가중치를 획득할 수 있다. 학습 모듈 은 학습된 가중치를 인공 지능 모델 저장 모듈에 전송할 수 있다.인공 지능 모델 저장 모듈은 학습 모듈로부터 학습된 가중치를 수신할 수 있다. 인공 지능 모델 저장 모듈은 학습된 가중치를 포함하는 인공 지능 모델을 저장할 수 있다. 인공 지능 모델 저장 모듈은 기 설정된 이벤트에 따라 인공 지능 모델을 전송할 수 있다. 도 9는 일 실시 예에 따라, 이미지 정렬 동작을 설명하기 위한 도면이다. 도 9의 실시 예(900-1)는 정렬 기능을 수행하기 전의 제1 학습 데이터 및 제2 학습 데이터를 나타낸 다. 제1 학습 데이터는 RGB 이미지를 포함할 수 있다. 제2 학습 데이터는 뎁스 맵을 포함할 수 있다. 전자 장치는 제1 학습 데이터에서 얼굴 영역을 식별할 수 있다. 전자 장치는 얼굴 영역과 관련 된 제1 기준선 또는 제2 기준선 중 적어도 하나를 획득(또는 식별)할 수 있다. 전자 장치는 제1 학습 데이터에서 얼굴 영역의 최상측 가로축을 제1 기준선으로 결정할 수 있다. 전자 장치는 제2 학습 데이터에서 얼굴 영역의 최상측 가로축을 제2 기준선으로 결정할 수 있다. 전자 장치는 제2 학습 데이터에서 획득한 제2 기준선이 제1 학습 데이터에서 획득한 제1 기준선과 일치하는지 여부를 확인할 수 있다. 제2 기준선이 제1 기준선과 일치하지 않는 경우, 전자 장치는 제2 학습 데이터에 대하여 정렬 기능을 수행할 수 있다. 전자 장치는 제2 기준선이 나타내는 좌표값과 제1 기준선이 나타 내는 좌표값이 일치하는지 여부를 판단할 수 있다. 제2 기준선이 제1 기준선과 일치하지 않는 경우, 전자 장치는 제2 기준선이 나타내는 좌표 값과 제1 기준선이 나타내는 좌표값의 차이값을 획득할 수 있다. 전자 장치는 차이값에 기초하여 제2 학습 데이터의 좌표값을 보정할 수 있다. 예를 들어, 제2 기준선이 나타내는 좌표값과 제1 기준선이 나타내는 좌표값의 차이값이 3cm 이면, 전 자 장치는 제2 학습 데이터의 y축 좌표값을 모두 3cm 만큼 감축할 수 있다. 실시 예(900-2)는 정렬 기능(또는 보정 기능)을 수행한 이후의 제1 학습 데이터 및 정렬된 제2 학습 데이 터를 나타낸다. 전자 장치는 제2 학습 데이터를 보정하여 정렬된 제2 학습 데이터를 획득할 수 있다. 정렬된 제 2 학습 데이터는 제2 학습 데이터의 좌표값에서 제2 기준선이 나타내는 좌표값과 제1 기준선 이 나타내는 좌표값의 차이값만큼 보정한 데이터일 수 있다. 실시 예(900-2)에서, 전자 장치는 정렬된 제2 학습 데이터에서 얼굴 영역의 최상측 가로축을 제2 기 준선으로 결정할 수 있다. 실시 예(900-2)에서, 정렬 기능이 수행되었으므로, 제1 기준선 및 제2 기준선이 일치될 수 있다. 제1 학습 데이터 및 정렬된 제2 학습 데이터는 맵핑된 데이터일 수 있다. 제1 학습 데이터에 포함된 얼굴 영역의 위치와 정렬된 제2 학습 데이터에 포함된 얼굴 영역의 위치가 동일한 좌표값으로 표현될 수 있다. 다양한 실시 예에 따라, 전자 장치는 얼굴 영역의 최하측 가로축을 이용하여 정렬 기능을 수행할 수 있다. 전자 장치는 제1 학습 데이터에서 얼굴 영역의 최하측 가로축을 제3 기준선으로 결정할 수 있다. 전자 장치는 제2 학습 데이터에서 얼굴 영역의 최하측 가로축을 제4 기준선으로 결정할 수 있다. 전자 장치는 제2 학습 데이터에서 획득한 제4 기준선이 제1 학습 데이터에서 획득한 제3 기준선과 일치하는지 여부를 확인할 수 있다. 제4 기준선이 제3 기준선과 일치하지 않는 경우, 전자 장치는 제2 학습 데이터에 대하여 정렬 기능을 수행할 수 있다. 전자 장치는 제4 기준선이 나타내는 좌표값과 제3 기준선이 나타 내는 좌표값이 일치하는지 여부를 판단할 수 있다.제4 기준선이 제3 기준선과 일치하지 않는 경우, 전자 장치는 제4 기준선이 나타내는 좌표 값과 제3 기준선이 나타내는 좌표값의 차이값을 획득할 수 있다. 전자 장치는 차이값에 기초하여 제2 학습 데이터의 좌표값을 보정할 수 있다. 예를 들어, 제4 기준선이 나타내는 좌표값과 제3 기준선이 나타내는 좌표값의 차이값이 3cm 이면, 전 자 장치는 제2 학습 데이터의 y축 좌표값을 모두 3cm 만큼 감축할 수 있다. 실시 예(900-2)는 정렬 기능(또는 보정 기능)을 수행한 이후의 제1 학습 데이터 및 정렬된 제2 학습 데이 터를 나타낸다. 전자 장치는 제2 학습 데이터를 보정하여 정렬된 제2 학습 데이터를 획득할 수 있다. 정렬된 제 2 학습 데이터는 제2 학습 데이터의 좌표값에서 제4 기준선이 나타내는 좌표값과 제3 기준선 이 나타내는 좌표값의 차이값만큼 보정한 데이터일 수 있다. 실시 예(900-2)에서, 전자 장치는 정렬된 제2 학습 데이터에서 얼굴 영역의 최하측 가로축을 제4 기 준선으로 결정할 수 있다. 실시 예(900-2)에서, 정렬 기능이 수행되었으므로, 제3 기준선 및 제4 기준선이 일치될 수 있다. 제1 학습 데이터 및 정렬된 제2 학습 데이터는 맵핑된 데이터일 수 있다. 제1 학습 데이터에 포함된 얼굴 영역의 위치와 정렬된 제2 학습 데이터에 포함된 얼굴 영역의 위치가 동일한 좌표값으로 표현될 수 있다. 다양한 실시 예에 따라, 전자 장치는 얼굴 영역의 최상측 가로축 및 최하측 가로축을 모두 이용하여 정렬 기능을 수행할 수 있다. 전자 장치는 제1 기준선, 제2 기준선을 비교하고 제3 기준선, 제4 기준선을 비교할 수 있다. 도 10은 일 실시 예에 따라, 이미지 정렬 동작을 설명하기 위한 도면이다. 도 10의 실시 예(1000-1)는 정렬 기능을 수행하기 전의 제1 학습 데이터 및 제2 학습 데이터를 나 타낸다. 제1 학습 데이터는 RGB 이미지를 포함할 수 있다. 제2 학습 데이터는 뎁스 맵을 포함할 수 있다. 전자 장치는 제1 학습 데이터에서 얼굴 영역을 식별할 수 있다. 전자 장치는 얼굴 영역과 관련 된 제1 기준선 또는 제2 기준선 중 적어도 하나를 획득(또는 식별)할 수 있다. 전자 장치는 제1 학습 데이터에서 얼굴 영역의 최좌측 세로축을 제1 기준선으로 결정할 수 있 다. 전자 장치는 제2 학습 데이터에서 얼굴 영역의 최좌측 세로축을 제2 기준선으로 결정할 수 있 다. 전자 장치는 제2 학습 데이터에서 획득한 제2 기준선이 제1 학습 데이터에서 획득한 제 1 기준선과 일치하는지 여부를 확인할 수 있다. 제2 기준선이 제1 기준선과 일치하지 않는 경우, 전자 장치는 제2 학습 데이터에 대하 여 정렬 기능을 수행할 수 있다. 전자 장치는 제2 기준선이 나타내는 좌표값과 제1 기준선이 나타내는 좌표값이 일치하는지 여부를 판단할 수 있다. 제2 기준선이 제1 기준선과 일치하지 않는 경우, 전자 장치는 제2 기준선이 나타내는 좌표값과 제1 기준선이 나타내는 좌표값의 차이값을 획득할 수 있다. 전자 장치는 차이값에 기초하 여 제2 학습 데이터의 좌표값을 보정할 수 있다. 예를 들어, 제2 기준선이 나타내는 좌표값과 제1 기준선이 나타내는 좌표값의 차이값이 3cm 이면, 전자 장치는 제2 학습 데이터의 y축 좌표값을 모두 3cm 만큼 감축할 수 있다. 실시 예(1000-2)는 정렬 기능(또는 보정 기능)을 수행한 이후의 제1 학습 데이터 및 정렬된 제2 학습 데 이터를 나타낸다. 전자 장치는 제2 학습 데이터를 보정하여 정렬된 제2 학습 데이터를 획득할 수 있다. 정렬된 제2 학습 데이터는 제2 학습 데이터의 좌표값에서 제2 기준선이 나타내는 좌표값과 제1 기준선이 나타내는 좌표값의 차이값만큼 보정한 데이터일 수 있다. 실시 예(1000-2)에서, 전자 장치는 정렬된 제2 학습 데이터에서 얼굴 영역의 최좌측 세로축을 제2 기준선으로 결정할 수 있다. 실시 예(1000-2)에서, 정렬 기능이 수행되었으므로, 제1 기준선 및 제2 기준선이 일치될 수 있다. 제1 학습 데이터 및 정렬된 제2 학습 데이터는 맵핑된 데이터일 수 있다. 제1 학습 데이터에 포함된 얼굴 영역의 위치와 정렬된 제2 학습 데이터에 포함된 얼굴 영역의 위치가 동일한 좌표값으로 표 현될 수 있다. 다양한 실시 예에 따라, 전자 장치는 얼굴 영역의 최우측 세로축을 이용하여 정렬 기능을 수행할 수 있다. 전자 장치는 제1 학습 데이터에서 얼굴 영역의 최우측 세로축을 제3 기준선으로 결정할 수 있 다. 전자 장치는 제2 학습 데이터에서 얼굴 영역의 최우측 세로축을 제4 기준선으로 결정할 수 있 다. 전자 장치는 제2 학습 데이터에서 획득한 제4 기준선이 제1 학습 데이터에서 획득한 제 3 기준선과 일치하는지 여부를 확인할 수 있다. 제4 기준선이 제3 기준선과 일치하지 않는 경우, 전자 장치는 제2 학습 데이터에 대하 여 정렬 기능을 수행할 수 있다. 전자 장치는 제4 기준선이 나타내는 좌표값과 제3 기준선이 나타내는 좌표값이 일치하는지 여부를 판단할 수 있다. 제4 기준선이 제3 기준선과 일치하지 않는 경우, 전자 장치는 제4 기준선이 나타내는 좌표값과 제3 기준선이 나타내는 좌표값의 차이값을 획득할 수 있다. 전자 장치는 차이값에 기초하 여 제2 학습 데이터의 좌표값을 보정할 수 있다. 예를 들어, 제4 기준선이 나타내는 좌표값과 제3 기준선이 나타내는 좌표값의 차이값이 3cm 이면, 전자 장치는 제2 학습 데이터의 y축 좌표값을 모두 3cm 만큼 감축할 수 있다. 실시 예(1000-2)는 정렬 기능(또는 보정 기능)을 수행한 이후의 제1 학습 데이터 및 정렬된 제2 학습 데 이터를 나타낸다. 전자 장치는 제2 학습 데이터를 보정하여 정렬된 제2 학습 데이터를 획득할 수 있다. 정렬된 제2 학습 데이터는 제2 학습 데이터의 좌표값에서 제4 기준선이 나타내는 좌표값과 제3 기준 선이 나타내는 좌표값의 차이값만큼 보정한 데이터일 수 있다. 실시 예(1000-2)에서, 전자 장치는 정렬된 제2 학습 데이터에서 얼굴 영역의 최우측 세로축을 제4 기준선으로 결정할 수 있다. 실시 예(1000-2)에서, 정렬 기능이 수행되었으므로, 제3 기준선 및 제4 기준선이 일치될 수 있다. 제1 학습 데이터 및 정렬된 제2 학습 데이터는 맵핑된 데이터일 수 있다. 제1 학습 데이터에 포함된 얼굴 영역의 위치와 정렬된 제2 학습 데이터에 포함된 얼굴 영역의 위치가 동일한 좌표값으로 표 현될 수 있다. 다양한 실시 예에 따라, 전자 장치는 얼굴 영역의 최좌측 세로축 및 최우측 세로축을 모두 이용하여 정렬 기능을 수행할 수 있다. 전자 장치는 제1 기준선, 제2 기준선을 비교하고 제3 기준선, 제4 기준선을 비교할 수 있다. 도 9 및 도 10에서 나오는 기준선의 서수는 변경될 수 있다. 도 9 및 도 10에서 도시하는 x축 및 y축은 거리를 나타낼 수 있다. 도 11은 일 실시 예에 따라, 얼굴 영역과 관련된 거리값을 추출하는 동작을 설명하기 위한 도면이다. 도 11의 실시 예를 참조하면, 전자 장치는 제1 학습 데이터 및 정렬된 제2 학습 데이터(112 5)를 획득할 수 있다. 전자 장치는 제1 학습 데이터 및 정렬된 제2 학습 데이터를 이용하여 맵핑된 좌표값을 획득(또는 추출)할 수 있다. 전자 장치는 제1 학습 데이터에서 얼굴 영역의 좌표 정보를 획득할 수 있다. 전자 장치 는 얼굴 영역의 좌표 정보에 기초하여 얼굴 영역의 중심점의 좌표 정보를 획득할 수 있다. 전자 장치는 제1 학습 데이터에 포함된 얼굴 영역의 중심점의 좌표 정보에 기초하여 정 렬된 제2 학습 데이터에서 얼굴 영역의 중심점의 좌표 정보를 획득할 수 있다. 일 예로, 얼굴 영역의 중심점의 좌표 정보와 얼굴 영역의 중심점의 좌표 정보가 동일 할 수 있다. 정렬된 제2 학습 데이터는 뎁스 맵을 포함할 수 있다. 전자 장치는 중심점의 좌 표 정보에 기초하여 중심점에 대응되는 뎁스 정보를 획득할 수 있다. 뎁스 정보는 중심점에 대응되 는 거리값을 포함할 수 있다. 전자 장치는 중심점에 대응되는 거리값에 기초하여 제1 학습 데이터 에서 중심점의 거리값을 획득할 수 있다. 중심점 대신 기 설정된 지점을 이용할 수 있다. 기 설정된 지점은 대표점, 대표지점, 기준점 등으로 기재될 수 있다. 도 12는 일 실시 예에 따라, 얼굴 영역의 거리 정보를 획득하는 동작을 설명하기 위한 도면이다. 도 12를 참조하면, 전자 장치는 촬상 이미지를 획득할 수 있다 (S1210). 전자 장치는 촬상 이미지에 서 얼굴 영역이 검출되었는지 여부를 결정할 수 있다 (S1220). 얼굴 영역이 검출되지 않으면 (S1220-N), 전자 장치는 S1210, S1220 단계를 반복적으로 수행할 수 있다. 얼굴 영역이 검출되면 (S1220-Y), 전자 장치는 얼굴 영역의 좌표 정보를 획득할 수 있다 (S1220). 전자 장 치는 얼굴 영역의 좌표 정보에 기초하여 얼굴 영역의 크기 정보를 획득할 수 있다 (S1230). 얼굴 영역의 크기 정보는 가로 길이, 세로 길이, 넓이 중 적어도 하나를 포함할 수 있다. 전자 장치는 얼굴 영역의 크기 정보에 기초하여 얼굴 영역의 거리 정보를 획득할 수 있다 (S1250). 얼굴 영역의 거리 정보는 촬상 지점(촬상 이미지 기준)부터 얼굴 영역의 중심점 까지의 거리를 포함할 수 있다. 전자 장치는 얼굴 영역의 거리 정보를 제공할 수 있다 (S1260). 거리 정보를 제공하는 동작은 다양한 기능 에 얼굴 영역의 거리 정보를 이용하는 기능을 수행하는 동작을 포함할 수 있다. 다양한 기능은 얼굴 영역의 거리 정보를 표시하는 기능, 얼굴 영역의 거리 정보를 이용하여 투사 이미지의 해상 도를 결정하는 기능, 사용자의 위치를 판단하는 기능, 사용자와 일정 거리를 유지하는 기능, 사용자 를 회피하는 기능 중 적어도 하나를 포함할 수 있다. 도 13은 일 실시 예에 따라, 얼굴 영역의 거리 정보를 이용하여 투사 이미지를 출력하는 동작을 설명하기 위한 도면이다. 도 13의 S1310 내지 S1350 단계는 도 12의 S1210 내지 S1250 단계에 대응될 수 있다. 중복 설명을 생략한다. 전자 장치는 컨텐츠 출력을 위한 사용자 입력을 획득할 수 있다 (S1305). 전자 장치는 사용자 입력에 기초하여 촬상 이미지를 획득할 수 있다 (S1310). 전자 장치는 촬상 이미지에 포함된 사용자의 얼굴 영역을 식별할 수 있다. 전자 장치는 얼굴 영역의 거리 정보를 획득할 수 있다 (S1350). 얼굴 영역의 거리 정보는 제1 거리 정보로 기재될 수 있다. 전자 장치는 제1 거리 정보에 기초하여 투사 해상도 정보를 획득할 수 있다 (S1370). 투사 해상도 정보는 전자 장치가 투사면에 컨텐츠를 출력함에 있어 투사 크기를 나타내는 정보일 수 있다. 투사 거리가 동 일한 조건에서 투사 크기가 클수록 투사면에 출력되는 투사 이미지가 클 수 있다. 전자 장치는 컨텐츠 정보를 획득할 수 있다 (S1375). 컨텐츠 정보는 컨텐츠 이미지, 컨텐츠 해상도 정보 중 적어도 하나를 포함할 수 있다. 컨텐츠 이미지는 복수의 프레임을 포함하는 개념일 수 있다. 컨텐츠 이미지 는 컨텐츠 이미지 그룹, 컨텐츠 이미지 데이터 등으로 기재될 수 있다. 컨텐츠 해상도 정보는 컨텐츠의 크기를 나타낼 수 있다. 컨텐츠의 크기가 클수록 컨텐츠 이미지를 표현하는 픽셀 수가 많을 수 있다. 전자 장치는 컨텐츠 해상도 정보 및 투사 해상도 정보에 기초하여 스케일링 계수를 획득할 수 있다 (S1380). 컨텐츠 해상도 정보와 투사 해상도 정보가 상이한 경우, 사용자가 원하는 품질의 투사 이미지가 출력 되지 않을 수 있다. 전자 장치는 스케일링 기능을 수행하여 투사 이미지를 출력할 수 있다. 전자 장치 는 컨텐츠에 포함된 컨텐츠 이미지의 크기를 보정하여 출력할 수 있다. 전자 장치는 스케일링 계수에 기초하여 컨텐츠 이미지를 스케일링하여 투사 이미지를 획득할 수 있다 (S1385). 전자 장치는 스케일링 계수에 기초하여 컨텐츠의 이미지의 크기를 변경할 수 있다. 변경된 이미지는 투사 이미지로 기재할 수 있다. 전자 장치는 투사 이미지를 출력할 수 있다 (S1390). 전자 장치는 스케일링 기능을 수행한 후 투사 이미지를 투사면에 출력할 수 있다. 도 14는 일 실시 예에 따라, 투사면까지의 거리를 이용하여 투사 이미지를 출력하는 동작을 설명하기 위한 도면 이다. 도 14의 S1405, S1410 내지 S1450, S1475, S1480, S1485, S1490 단계는 도 13의 S1305, S1310 내지 S1350, S1375, S1380, S1385, S1390 단계에 대응될 수 있다. 중복 설명을 생략한다. 전자 장치는 투사면까지의 거리 정보를 획득할 수 있다 (S1471). 투사면까지의 거리 정보는 제2 거리 정보로 기재될 수 있다. 전자 장치는 제1 거리 정보 및 제2 거리 정보에 기초하여 투사 해상도 정보 를 획득할 수 있다 (S1472). 전자 장치는 전자 장치와 사용자 사이의 거리를 나타내는 제1 거리 정보 및 전자 장치와 투사면 사이의 거리를 나타내는 제2 거리 정보를 모두 이용하여 투사 해상도 정 보를 결정할 수 있다. 전자 장치와 투사면 사이의 거리는 투사 거리로 기재될 수 있다. 전자 장치는 기 설정된 투사 비 율 범위를 저장할 수 있다. 전자 장치가 제공할 수 있는 투사 비율이 기 설정된 범위로 제한될 수 있다. 투사 비율은 투사 거리/투사 이미지의 크기로 계산될 수 있다. 투사 이미지의 크기는 투사 이미지의 대각선 길 이를 포함할 수 있다. 전자 장치는 기 설정된 투사 비율 범위를 고려하여 투사 해상도 정보를 결정할 수 있다. 투사 해상도 정보는 투사 이미지의 크기를 포함할 수 있다. 도 15는 일 실시 예에 따라, 복수의 얼굴 영역을 식별하는 동작을 설명하기 위한 도면이다. 도 15의 실시 예를 참조하면, 전자 장치는 촬상 이미지에서 복수의 사용자를 식별할 수 있다. 전자 장치는 복수의 사용자 각각에 대한 얼굴 영역(1511, 1512)을 검출할 수 있다. 전자 장치는 제1 얼굴 영역의 크기 정보(x1, y1, x1*y1)를 획득할 수 있다. 전자 장치는 제2 얼굴 영역의 크기 정 보(x2, y2, x2*y2)를 획득할 수 있다. 도 15의 실시 예를 참조하면, 전자 장치는 촬상 이미지에 포함된 얼굴 영역 각각에 대한 거리 정보 를 제공할 수 있다. 전자 장치는 제1 얼굴 영역의 거리 정보(d1)를 획득할 수 있다. 전자 장치(10 0)는 제2 얼굴 영역의 거리 정보(d2)를 획득할 수 있다. 다양한 실시 예에 따라, 전자 장치는 거리 정보를 포함하는 촬상 이미지를 출력할 수 있다. 도 16은 일 실시 예에 따라, 복수의 얼굴 영역 각각의 거리 정보를 획득하는 동작을 설명하기 위한 도면이다. 도 16의 S1610, S1620, S1630, S1640, S1660 단계는 도 12의 S1210, S1220, S1230, S1240, S1260 단계에 대응 될 수 있다. 중복 설명을 생략한다. 얼굴 영역을 검출한 후, 전자 장치는 제1 얼굴 영역의 제1 좌표 정보 및 제2 얼굴 영역의 제2 좌표 정보를 획득할 수 있다 (S1630). 전자 장치는 제1 얼굴 영역의 제1 크기 정보 및 제2 얼굴 영역의 제2 크기 정보 를 획득할 수 있다 (S1640). 전자 장치는 제1 얼굴 영역의 제1 거리값 및 제2 얼굴 영역의 제2 거리값을 획득할 수 있다 (S1651). 전자 장치는 제1 거리값 및 제2 거리값에 기초하여 대표 거리값을 획득할 수 있다 (S1652). 일 예로, 대표 거리 값은 평균값, 최소값, 최대값 중 하나일 수 있다. 일 예로, 대표 거리값은 대표 사용자의 얼굴 영역을 나타내는 거리값일 수 있다. 전자 장치는 복수의 사용 자 중 대표 사용자를 식별하고, 대표 사용자의 얼굴 영역에 대응되는 거리값을 대표 거리값으로 결정할 수 있다. 전자 장치는 대표 거리값을 포함하는 얼굴 영역의 거리 정보를 획득할 수 있다 (S1653). 전자 장치는 얼굴 영역의 거리 정보를 제공할 수 있다 (S1660). 도 17은 일 실시 예에 따라, 사용자와 전자 장치 사이의 거리를 제어하는 동작을 설명하기 위한 도면 이다. 도 17의 실시 예를 참조하면, 전자 장치는 투사면에 투사 이미지를 출력할 수 있다. 전자 장치 는 사용자를 기준으로 기 설정된 범위 또는 임계 거리(r) 이내에 위치하도록 제어될 수 있다. 전자 장치는 사용자의 위치를 식별할 수 있다. 전자 장치는 사용자의 위치를 얼굴 영역의 거리 정보 에 기초하여 판단할 수 있다. 전자 장치는 사용자의 위치를 기준으로 임계 거리(r) 안에 위치하도록 이동할 수 있다. 사용자의 위치와 전자 장치의 위치 사이의 거리가 임계 거리(r) 이상이면, 전자 장치는 전자 장 치가 사용자의 위치로부터 임계 거리(r) 이내에 존재하도록 주행(또는 이동)할 수 있다. 도 18은 일 실시 예에 따라, 사용자를 회피하는 동작을 설명하기 위한 도면이다. 도 18의 실시 예를 참조하면, 전자 장치는 투사면의 위치를 획득할 수 있다. 전자 장치는 사용자의 위치를 획득할 수 있다. 전자 장치는 투사면의 위치와 사용자의 위치 사이로 이동할 수 있다. 투사 이미지를 출력함에 있 어 사용자가 방해되지 않도록 하기 위함이다. 사용자가 전자 장치와 투사면 사이에 존재하는 경우, 투사 이미지가 사용자에 의해 가려질 수 있다. 전자 장치는 투사면과 사용자 사이에 위 치하도록 주행할 수 있다. 전자 장치는 사용자의 위치에 기초하여 사용자를 회피하도록 주행할 수 있다. 전자 장치는 얼굴 영역의 거리 정보를 이용하여 사용자의 위치를 식별할 수 있다. 도 19는 일 실시 예에 따라, 사용자와 전자 장치 사이와 관련된 가이드 화면을 설명하기 위한 도면이 다. 도 19를 참조하면, 전자 장치는 투사 이미지를 출력할 수 있다. 투사 이미지는 얼굴 영역의 거리 정보와 관련된 정보를 포함할 수 있다. 투사 이미지는 촬상 이미지, 전자 장치와 사용자 사이의 거리 정보를 나타내는 UI, 전자 장치와 투사면 사이의 거리 정보를 나타내는 UI, 투사 이미지의 해상도 정보(또는 크기 정보)를 나타내는 UI, 해상도를 조절하기 위한 UI 중 적어도 하나를 포함할 수 있다. 촬상 이미지는 전자 장치가 촬상한 이미지를 포함할 수 있다. 촬상 이미지는 사용자의 얼굴 영역을 나타내는 GUI, 거리 정보를 나타내는 GUI 중 적어도 하나를 포함할 수 있다. 도 20은 일 실시 예에 따라, 전자 장치의 제어 동작을 설명하기 위한 도면이다. 도 20을 참조하면, 센싱 데이터를 획득하고, 컨텐츠 이미지를 저장하는 전자 장치의 제어 방법은 센싱 데 이터에 기초하여 사용자의 얼굴 영역이 검출되면, 얼굴 영역의 크기 정보를 획득하는 단계 (S2010), 얼굴 영역 의 크기 정보에 기초하여 센싱 데이터를 획득한 전자 장치의 위치와 사용자의 위치 사이의 거리를 나타내 는 얼굴 영역의 거리 정보를 획득하는 단계 (S2020) 및 거리 정보에 기초하여 컨텐츠 이미지를 출력하는 단계 (S2030)를 포함한다. 크기 정보를 획득하는 단계 (S2010)는 얼굴 영역의 가로 길이, 얼굴 영역의 세로 길이, 얼굴 영역의 넓이 중 적 어도 하나를 포함하는 크기 정보를 획득할 수 있다. 크기 정보를 획득하는 단계 (S2010)는 센싱 데이터에 기초하여 사용자의 얼굴 영역이 검출되면, 얼굴 영역의 좌 표 정보를 획득하고, 좌표 정보에 기초하여 크기 정보를 획득할 수 있다. 전자 장치는 컨텐츠 해상도 정보를 저장하고, 제어 방법은 거리 정보에 기초하여 전자 장치를 통해 출력되는 이미지의 크기를 나타내는 투사 해상도 정보를 획득하는 단계를 더 포함하고, 컨텐츠 이미지를 출력하 는 단계 (S2030)는 투사 해상도 정보에 기초하여 컨텐츠 이미지를 출력할 수 있다. 거리 정보는 제1 거리 정보이고, 투사 해상도 정보를 획득하는 단계는 전자 장치의 위치와 투사면 사이의 거리를 나타내는 제2 거리 정보를 획득하고, 제1 거리 정보 및 제2 거리 정보에 기초하여 투사 해상도 정보를 획득할 수 있다. 전자 장치는 투사 비율 범위를 저장하고, 투사 해상도 정보를 획득하는 단계는 투사 비율 범위 및 제2 거 리 정보에 기초하여 해상도 범위를 획득하고, 제1 거리 정보에 기초하여 해상도 범위 중 투사 해상도 정보를 획 득할 수 있다. 컨텐츠 이미지를 출력하는 단계 (S2030)는 컨텐츠 해상도 정보 및 투사 해상도 정보에 기초하여 스케일링 계수 를 획득하고, 스케일링 계수에 기초하여 컨텐츠 이미지를 스케일링하여 투사 이미지를 획득하고, 투사 이미지를출력할 수 있다. 크기 정보를 획득하는 단계 (S2010)는 센싱 데이터에 기초하여 제1 얼굴 영역 및 제2 얼굴 영역이 검출되면, 제 1 얼굴 영역의 제1 크기 정보 및 제2 얼굴 영역의 제2 크기 정보를 획득하고, 거리 정보를 획득하는 단계 (S2030)는 제1 얼굴 영역의 제1 크기 정보에 기초하여 제1 얼굴 영역의 제1 거리값을 획득하고, 제2 얼굴 영역 의 제2 크기 정보에 기초하여 제2 얼굴 영역의 제2 거리값을 획득하고, 제1 거리값 및 제2 거리값에 기초하여 대표 거리값을 획득하고, 대표 거리값을 포함하는 거리 정보를 획득하고, 컨텐츠 이미지를 출력하는 단계 (S2030)는 거리 정보에 기초하여 컨텐츠 이미지를 출력할 수 있다. 전자 장치는 학습된 적어도 하나의 가중치를 포함하는 인공 지능 모델을 저장하고, 거리 정보를 획득하는 단계 (S2030)는 크기 정보를 인공 지능 모델에 입력하여 거리 정보를 출력 데이터로써 획득할 수 있다. 적어도 하나의 가중치는 RGB 이미지를 포함하는 제1 학습 데이터 및 뎁스 이미지를 포함하는 제2 학습 데이터에 기초하여 학습될 수 있다. 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 설치 가능한 어플리케이션 형태로 구현 될 수 있다. 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 대한 소프트웨어 업그레이드, 또는 하 드웨어 업그레이드 만으로도 구현될 수 있다. 상술한 본 개시의 다양한 실시 예들은 전자 장치에 구비된 임베디드 서버, 또는 전자 장치 및 디스플레이 장치 중 적어도 하나의 외부 서버를 통해 수행되는 것도 가능하다. 본 개시의 일시 예에 따르면, 이상에서 설명된 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media)에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실 시 예들에 따른 전자 장치를 포함할 수 있다. 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 프 로세서의 제어 하에 다른 구성요소들을 이용하여 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또 는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장 매체는, 비일시적 (non-transitory) 저장 매체의 형태로 제공될 수 있다. '비일시적'은 저장 매체가 신호(signal)를 포함하지 않 으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장 매체에 반영구적 또는 임시적으로 저장됨을 구분하지 않는다. 본 개시의 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품 (computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있 다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 상술한 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로그 램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유 사하게 수행할 수 있다. 다양한 실시 예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작 들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생 략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2023-0123486", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형 실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2023-0123486", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시 예에 따라, 전자 장치를 통해 출력되는 이미지의 크기를 결정하는 동작을 설명하기 위한 도면이 다. 도 2는 일 실시 예에 따라, 전자 장치를 도시한 블록도이다. 도 3은 일 실시 예에 따라, 도 2의 전자 장치의 구체적인 구성을 설명하기 위한 블록도이다. 도 4는 일 실시 예에 따라, 얼굴 영역의 거리 정보를 획득하는 동작을 설명하기 위한 도면이다. 도 5는 일 실시 예에 따라, 입력 이미지에서 얼굴 영역을 검출하는 동작을 설명하기 위한 도면이다. 도 6은 일 실시 예에 따라, 얼굴 영역의 거리 정보를 표시하는 동작을 설명하기 위한 도면이다.도 7은 일 실시 예에 따라, 인공 지능 모델을 이용하는 동작을 설명하기 위한 도면이다. 도 8은 일 실시 예에 따라, 인공 지능 모델을 학습하는 동작을 설명하기 위한 도면이다. 도 9는 일 실시 예에 따라, 이미지 정렬 동작을 설명하기 위한 도면이다. 도 10은 일 실시 예에 따라, 이미지 정렬 동작을 설명하기 위한 도면이다. 도 11은 일 실시 예에 따라, 얼굴 영역과 관련된 거리값을 추출하는 동작을 설명하기 위한 도면이다. 도 12는 일 실시 예에 따라, 얼굴 영역의 거리 정보를 획득하는 동작을 설명하기 위한 도면이다. 도 13은 일 실시 예에 따라, 얼굴 영역의 거리 정보를 이용하여 투사 이미지를 출력하는 동작을 설명하기 위한 도면이다. 도 14는 일 실시 예에 따라, 투사면까지의 거리를 이용하여 투사 이미지를 출력하는 동작을 설명하기 위한 도면 이다. 도 15는 일 실시 예에 따라, 복수의 얼굴 영역을 식별하는 동작을 설명하기 위한 도면이다. 도 16은 일 실시 예에 따라, 복수의 얼굴 영역 각각의 거리 정보를 획득하는 동작을 설명하기 위한 도면이다. 도 17은 일 실시 예에 따라, 사용자와 전자 장치 사이의 거리를 제어하는 동작을 설명하기 위한 도면이다. 도 18은 일 실시 예에 따라, 사용자를 회피하는 동작을 설명하기 위한 도면이다. 도 19는 일 실시 예에 따라, 사용자와 전자 장치 사이와 관련된 가이드 화면을 설명하기 위한 도면이다. 도 20은 일 실시 예에 따라, 전자 장치의 제어 동작을 설명하기 위한 도면이다."}
