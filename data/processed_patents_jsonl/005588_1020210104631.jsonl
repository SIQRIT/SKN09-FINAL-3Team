{"patent_id": "10-2021-0104631", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0022629", "출원번호": "10-2021-0104631", "발명의 명칭": "에피소드 메모리 기반 인공 지능 학습 장치 및 방법", "출원인": "한국전자통신연구원", "발명자": "문예빈"}}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전체 메모리에 저장된 학습데이터의 특징벡터를 이용하여 에피소드 메모리를 구성하는 단계;쿼리 데이터를 인공지능 모델에 입력하여 출력 데이터를 획득하는 단계;상기 출력 데이터와 상기 구성된 에피소드 메모리 내의 특징 벡터 간의 유사도를 도출하는 단계;상기 유사도를 기반으로 에피소드 손실 함수를 도출하는 단계;를 포함하는 에피소드 메모리 기반 인공지능 학습방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 에피소드 메모리는상기 에피소드 메모리에 포함된 특징 벡터의 상기 전체 메모리 내 인덱스 및상기 특징벡터의 행렬 표현을 포함하는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 에피소드 손실 함수는,강한 어텐션 손실 함수와 약한 어텐션 손실 함수 중 적어도 어느 하나를 기반으로 도출되는, 에피소드 메모리기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,상기 강한 어텐션 손실 함수는,상기 에피소드 메모리 내 임의의 슬롯이 상기 쿼리 데이터에 해당할 확률을 기반으로 도출되는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3 항에 있어서,상기 약한 어텐션 손실 함수는,상기 에피소드 메모리 내 임의의 슬롯이 상기 쿼리 데이터에 해당하는 경우, 상기 에피소드 메모리 내 다른 임의의 슬롯이 상기 쿼리 데이터에 해당할 확률을 이용하여 상기 인공 지능 모델을 통과하여 얻은 상기 출력 데이터 간의 차이를 기반으로 도출되는, 에피소드 메모리 기반 인공지능 학습방법. 공개특허 10-2023-0022629-3-청구항 6 제1 항에 있어서,상기 인공지능 모델은제1 인공지능 모델과 제2 인공지능 모델을 포함하되,상기 제2 인공지능 모델은 지식 증류를 통해 내재적인 지식을 상기 제1 인공지능 모델에 전달하는 교사 모델인,에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 제2 인공지능 모델은 상기 제1 인공지능 모델과 동일한 유형의 태스크를 수행하는 기 학습된 인공 신경망을 포함하는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6 항에 있어서,상기 제2 인공지능 모델을 이용하여 지식 증류 손실 함수를 도출하는 단계;를 더 포함하는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 에피소드 손실 함수와 상기 지식 증류 손실 함수에 가중치를 적용하여 최종 손실 함수를 도출하는 단계;를더 포함하는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항에 있어서,상기 인공 지능 모델은 CNN(convolutional neural network) 혹은 오토인코더(autoencoder)를 기반으로 하는,에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1 항에 있어서,상기 전체 메모리는, 상기 특징 벡터와 대응되는 클래스 라벨을 저장하는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서, 상기 유사도가 가장 높은 특징 벡터의 클래스 라벨은공개특허 10-2023-0022629-4-상기 쿼리 데이터의 클래스 라벨로 할당되는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1 항에 있어서,상기 학습 데이터의 특징 벡터는상기 전체 메모리에 저장되고 일정 주기마다 업데이트되는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1 항에 있어서, 상기 학습 데이터는상기 쿼리 데이터와 랜덤 데이터를 포함하는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1 항에 있어서,상기 쿼리 데이터는 복수 개로서 미니 배치(mini batch)로 구성되는, 에피소드 메모리 기반 인공지능 학습방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제1 항에 있어서,상기 에피소드 메모리가 구성되기 전에,상기 인공지능 모델과 상기 전체 메모리를 초기화하는 단계;를 더 포함하는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제1 항에 있어서,상기 에피소드 손실 함수가 도출되고 나면, 상기 인공 지능 모델의 모수를 업데이트하여 역전파(back propagation)하는 단계;를 더 포함하는, 에피소드 메모리 기반 인공지능 학습 방법."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제1 항에 있어서,상기 에피소드 손실 함수가 도출되고 나면,상기 전체 메모리에 저장된 학습데이터의 특징벡터를 이용하여 상기 에피소드 메모리를 재구성하는 단계;를 더포함하는, 에피소드 메모리 기반 인공지능 학습 방법.공개특허 10-2023-0022629-5-청구항 19 에피소드 메모리 기반 인공지능 학습 장치에 있어서,학습 데이터의 특징 벡터를 저장하는 메모리; 및상기 메모리를 제어하는 프로세서;를 포함하되,상기 프로세서는전체 메모리에 저장된 학습데이터의 특징벡터를 이용하여 에피소드 메모리를 구성하고, 쿼리 데이터를 인공지능모델에 입력하여 출력 데이터를 획득하고, 상기 출력 데이터와 상기 구성된 에피소드 메모리 내의 특징 벡터 간의 유사도를 도출하고, 상기 유사도를 기반으로 에피소드 손실 함수를 도출하는, 에피소드 메모리 기반 인공지능 학습 장치."}
{"patent_id": "10-2021-0104631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "비 일시적 컴퓨터 판독가능한 매체에 저장된 컴퓨터 프로그램에 있어서, 컴퓨터에서 전체 메모리에 저장된 학습데이터의 특징벡터를 이용하여 에피소드 메모리를 구성하는 단계;쿼리 데이터를 인공지능 모델에 입력하여 출력 데이터를 획득하는 단계;상기 출력 데이터와 상기 구성된 에피소드 메모리 내의 특징 벡터 간의 유사도를 구하는 단계;상기 유사도를 기반으로 에피소드 손실 함수를 도출하는 단계;를 실행하는, 비 일시적 컴퓨터 판독 가능한 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 에피소드 메모리 기반 인공 지능 학습 장치 및 방법에 대한 것이다. 본 개시의 일 실시예에 따른 에피 소드 메모리 기반 인공 지능 학습 방법에 의하면, 전체 메모리에 저장된 학습데이터의 특징벡터를 이용하여 에피 소드 메모리를 구성하는 단계, 쿼리 데이터를 인공지능 모델에 입력하여 출력 데이터를 획득하는 단계, 상기 출 력 데이터와 상기 구성된 에피소드 메모리 내의 특징 벡터 간의 유사도를 도출하는 단계, 상기 유사도를 기반으 로 에피소드 손실 함수를 도출하는 단계를 포함할 수 있다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 에피소드 메모리 기반 인공 지능 학습 장치 및 방법에 대한 것으로서, 기존에 학습하지 않았던 새로 운 클래스(class)에 대해서도 인공지능 모델이 별도의 추가적인 학습 과정 없이 해당 태스크를 수행할 수 있도 록 하기 위한 기술에 대한 것이다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 기술이 최근 몇 년 사이 급부상하고 있다. 자율 주행 자동차의 인지/판단 시스템을 비롯한 교통, 언론, 물류, 안전, 환경 등 각종 분야에 인공지능 기술이 빠르게 접목되고 확산되면서 인간중시 가치 산업 및 지식정보 사회를 이끌어 갈 부가가치 창출의 새로운 원천으로 주목받고 있다. 인공 지능을 인간의 능력과 비견 되거나 더 좋은 성능을 보이는 수준에 이르게 훈련하기 위해 다양한 방식의 인공지능 분야의 심층 인공 신경망 을 이용하여 방대한 양의 데이터를 분석하는 기술이 제안된 바 있다. 하지만 현재의 심층 인공 신경망 기술의 경우 수백에서 수만 개의 예제데이터를 보고 학습해야 인간과 비슷한 수준의 추론 정확도를 얻을 수 있다. 그러기 위해서는 수많은 예제 데이터들 각각에 정확한 클래스 라벨(labe l)을 일일이 기술하는 작업이 필수적으로 동반되며, 이로 인해 매우 큰 시각적, 경제적 비용이 발생한다. 또, 학습 시에 사용하지 않았던 새로운 클래스(class)에 대해 동일한 태스크(task)를 수행하고자 할 때, 새로운 클 래스의 데이터를 다량으로 준비한 후 기존의 심층 인공 신경망을 다시 학습해야 한다는 번거로움이 존재하여 보 다 효율적인 인공 지능 학습 기술이 필요한 실정이다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "종래 기술의 문제점을 해결하기 위해, 본 개시의 목적은 에피소드 메모리 기반 인공 지능 학습 장치 및 방법을 제공하는 데 있다. 또한, 본 개시의 목적은 기존에 학습하지 않았던 새로운 클래스(class)에 대해서도 인공지능 모델이 별도의 추 가적인 학습 과정 없이 해당 태스크를 수행할 수 있는 인공 지능 학습 기술을 제공하는데 있다. 또한, 본 개시의 목적은 작은 수의 학습 데이터들을 이용하여 클래스 라벨(label)이 없이도 인공지능 모델을 학 습하는 기술을 제공하는 데 있다. 본 개시의 다른 목적 및 장점들은 하기의 설명에 의해서 이해될 수 있으며, 본 개시의 실시예에 의해 보다 분명 하게 알게 될 것이다. 또한, 본 개시의 목적 및 장점들은 특허청구범위에 나타낸 수단 및 그 조합에 의해 실현 될 수 있음을 쉽게 알 수 있을 것이다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따르면, 에피소드 메모리 기반 인공지능 학습 방법은 전체 메모리에 저장된 학습데이터 의 특징 벡터를 이용하여 에피소드 메모리를 구성하는 단계, 쿼리 데이터를 인공지능 모델에 입력하여 출력 데 이터를 획득하는 단계, 상기 출력 데이터와 상기 구성된 에피소드 메모리 내의 특징 벡터 간의 유사도를 도출하 는 단계, 상기 유사도를 기반으로 에피소드 손실 함수를 도출하는 단계를 포함할 수 있다. 한편, 상기 에피소드 메모리는 상기 에피소드 메모리에 포함된 특징 벡터의 상기 전체 메모리 내 인덱스 및 상 기 특징벡터의 행렬 표현을 포함할 수 있다. 한편, 상기 에피소드 손실 함수는, 강한 어텐션 손실 함수와 약한 어텐션 손실 함수 중 적어도 어느 하나를 기 반으로 도출될 수 있다. 한편, 상기 강한 어텐션 손실 함수는 상기 에피소드 메모리 내 임의의 슬롯이 상기 쿼리 데이터에 해당할 확률 을 기반으로 도출될 수 있다. 한편, 상기 약한 어텐션 손실 함수는 상기 에피소드 메모리 내 임의의 슬롯이 상기 쿼리 데이터에 해당하는 경 우, 상기 에피소드 메모리 내 다른 임의의 슬롯이 상기 쿼리 데이터에 해당할 확률을 이용하여 상기 인공 지능 모델을 통과하여 얻은 상기 출력 데이터 간의 차이를 기반으로 도출될 수 있다. 한편, 상기 인공지능 모델은 제1 인공지능 모델과 제2 인공지능 모델을 포함하되, 상기 제2 인공지능 모델은 지 식 증류를 통해 내재적인 지식을 상기 제1 인공지능 모델에 전달하는 교사 모델일 수 있다. 한편, 상기 제2 인공지능 모델은 기 학습된 인공 신경망을 포함할 수 있다. 한편, 상기 기 학습된 인공 신경망은 CNN(convolutional neural network) 기반일 수 있다. 한편, 상기 제2 인공지능 모델은 상기 제1 인공지능 모델과 동일한 유형의 태스크를 수행하는 기 학습된 인공 신경망을 포함할 수 있다. 한편, 상기 제2 인공지능 모델을 이용하여 지식 증류 손실 함수를 도출하는 단계가 더 포함될 수 있다. 한편, 상기 에피소드 손실 함수와 상기 지식 증류 손실 함수에 가중치를 적용하여 최종 손실 함수를 도출하는 단계를 더 포함할 수 있다. 한편, 상기 인공 지능 모델은 CNN(convolutional neural network) 혹은 오토인코더(autoencoder)를 기반으로 할 수 있다. 한편, 상기 전체 메모리는 상기 특징 벡터와 대응되는 클래스 라벨을 저장할 수 있다. 한편, 상기 유사도가 가장 높은 특징 벡터의 클래스 라벨은 상기 쿼리 데이터의 클래스 라벨로 할당될 수 있다. 한편, 상기 학습 데이터의 특징 벡터는 상기 전체 메모리에 저장되고 일정 주기마다 업데이트될 수 있다. 한편, 상기 학습 데이터는 상기 쿼리 데이터와 랜덤 데이터를 포함할 수 있다. 한편, 상기 쿼리 데이터는 복수 개로서 미니 배치(mini batch)로 구성될 수 있다. 본 개시의 일 실시예에 따르면, 에피소드 메모리 기반 인공지능 학습 장치는 학습 데이터의 특징 벡터를 저장하 는 메모리 및 상기 메모리를 제어하는 프로세서를 포함하되, 상기 프로세서는 전체 메모리에 저장된 학습데이터 의 특징벡터를 이용하여 에피소드 메모리를 구성하고, 쿼리 데이터를 인공지능 모델에 입력하여 출력 데이터를 획득하고, 상기 출력 데이터와 상기 구성된 에피소드 메모리 내의 특징 벡터 간의 유사도를 도출하고, 상기 유 사도를 기반으로 에피소드 손실 함수를 도출할 수 있다.한편, 상기 에피소드 메모리는 상기 에피소드 메모리에 포함된 특징 벡터의 상기 전체 메모리 내 인덱스 및 상 기 특징벡터의 행렬 표현을 포함할 수 있다. 한편, 상기 에피소드 손실 함수는 강한 어텐션 손실 함수와 약한 어텐션 손실 함수 중 적어도 어느 하나를 기반 으로 도출될 수 있다. 한편, 상기 인공지능 모델은 제1 인공지능 모델과 제2 인공지능 모델을 포함하되, 상기 제2 인공지능 모델은 지 식 증류를 통해 내재적인 지식을 상기 제1 인공지능 모델에 전달하는 교사 모델일 수 있다. 본 개시의 일 실시예에 따르면, 비 일시적 컴퓨터 판독 가능한 매체에 저장된 컴퓨터 프로그램은, 컴퓨터에서 전체 메모리에 저장된 학습데이터의 특징벡터를 이용하여 에피소드 메모리를 구성하는 단계, 쿼리 데이터를 인 공지능 모델에 입력하여 출력 데이터를 획득하는 단계, 상기 출력 데이터와 상기 구성된 에피소드 메모리 내의 특징 벡터 간의 유사도를 도출하는 단계, 상기 유사도를 기반으로 에피소드 손실 함수를 도출하는 단계를 실행 할 수 있다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 에피소드 메모리 기반 인공 지능 학습 장치 및 방법의 구현을 기대할 수 있다. 또한, 본 개시에 따르면, 작은 수의 학습 데이터들을 이용하여 라벨(label)이 없이도 인공지능 모델 학습을 수 행할 수 있다. 또한, 본 개시에 따르면, 기존에 학습하지 않았던 새로운 클래스(class)에 대해서도 인공지능 모델이 별도의 추 가적인 학습 과정 없이 해당 태스크를 수행할 수 있다. 또한, 본 개시에 따르면, 주어진 태스크에 대한 정보를 더 잘 반영할 수 있도록 하는 별도의 기술과도 잘 결합 하여 더 나은 태스크 수행 결과를 얻을 수 있다. 본 개시의 실시 예들에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은 이하의 본 개시의 실시 예들에 대한 기재로부터 본 개시의 기술 구성이 적용되는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 도출되고 이해될 수 있다. 즉, 본 개시에서 서술하는 구성을 실시함에 따 른 의도하지 않은 효과들 역시 본 개시의 실시 예들로부터 당해 기술 분야의 통상의 지식을 가진 자에 의해 도 출될 수 있다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참고로 하여 본 개시의 실시 예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나, 본 개시는 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 본 개시의 실시 예를 설명함에 있어서 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그에 대한 상세한 설명은 생략한다. 그리고, 도면에서 본 개시에 대한 설명과 관계없 는 부분은 생략하였으며, 유사한 부분에 대해서는 유사한 도면 부호를 붙였다.본 개시에 있어서, 서로 구별되는 구성요소들은 각각의 특징을 명확하게 설명하기 위함이며, 구성요소들이 반드 시 분리되는 것을 의미하지는 않는다. 즉, 복수의 구성요소가 통합되어 하나의 하드웨어 또는 소프트웨어 단위 로 이루어질 수도 있고, 하나의 구성요소가 분산되어 복수의 하드웨어 또는 소프트웨어 단위로 이루어질 수도 있다. 따라서, 별도로 언급하지 않더라도 이와 같이 통합된 또는 분산된 실시 예도 본 개시의 범위에 포함된다. 본 개시에 있어서, 다양한 실시 예에서 설명하는 구성요소들이 반드시 필수적인 구성요소들을 의미하는 것은 아 니며, 일부는 선택적인 구성요소일 수 있다. 따라서, 일 실시 예에서 설명하는 구성요소들의 부분집합으로 구성 되는 실시 예도 본 개시의 범위에 포함된다. 또한, 다양한 실시 예에서 설명하는 구성요소들에 추가적으로 다른 구성요소를 포함하는 실시 예도 본 개시의 범위에 포함된다. 본 개시의 실시예를 도면을 참조하여 상세히 설명하기에 앞서, 본 개시에 적용 가능한 인공지능 학습 기술에 대 하여 설명한다. 상기에서 언급한 종래 기술의 문제점을 극복하기 위해 작은 수의 예제만을 이용하여 학습하는 퓨샷 학습(few- shot learning) 기술을 제안한다. 퓨샷 학습 방법이란, 인공지능이 기존에 본 적 없는 새로운 클래스에 대해 매우 적은 수의 데이터만 가지고 태 스크를 수행할 수 있도록 인공신경망을 학습시키는 기술이다. 대표적인 퓨샷 학습 방법 중 하나인 에피소드 학 습(episodic learning) 기술은 각 클래스마다 작은 수의 예제가 주어졌을 때 입력으로 들어온 쿼리(query) 데이 터에 대해 태스크를 수행하는 시나리오를 하나의 에피소드(episode)로 가정하여 학습 상황에서도 에피소드 단위 로 인공신경망을 학습시키는 기술을 말한다. 예를 들어, 이미지 분류(image classification) 태스크의 경우, N 개의 객체 클래스와 각 클래스마다 k개의 이미지 예제가 주어졌을 때, 입력으로 들어온 쿼리 이미지가 N개의 클 래스 중 어느 클래스에 속하는 지 판단하는 태스크를 하나의 에피소드로 구성할 수 있다. 따라서 클래스 라벨이 존재하는 학습 데이터를 이용하여 랜덤하게 에피소드를 구성하고 이미지 분류 태스크의 성능을 향상시키는 방향 으로 인공신경망을 학습시킬 수 있다. 하지만 에피소드 학습 기술을 포함한 퓨샷 학습 기술 역시 클래스 라벨이 존재하는 데이터를 통해서만 학습 가 능하다는 문제점이 있다. 특히, 기존의 클래스보다 더 세부적인 클래스에 대한 태스크를 고려할수록 클래스 라 벨이 존재하는 데이터를 구하는 것은 더욱 어려워 구현 상 어려움이 발생할 수 있다. 한편, 클래스 라벨이 존재하지 않는 데이터들을 이용해서 인공신경망을 학습하는 비지도학습(unsupervised learning) 기술에 대해서도 연구가 활발히 진행되고 있으며, 그 중 한 가지 방법으로 객체 단위 분리(instance- level discrimination) 기술을 제안한다. 객체 단위 분리 방법은 학습 데이터의 각 객체(instance)들이 서로 구 별이 분명해지도록 특징 벡터를 생성하는 인공신경망을 학습하는 자가지도학습(self-supervised learning) 기술 이다. 이렇게 인공신경망이 객체 단위로 구별이 가능한 특징 벡터를 만들게 되면, 실제로 유사한 객체들은 특징 벡터 간에도 유사도가 높게 나타난다. 하지만 객체 단위 분리 방법으로 인공신경망을 학습하게 되면 굉장히 큰 계산상의 오버헤드가 발생할 수 있다. 본 개시는 퓨샷 학습 기술과 객체 단위 분리 기술의 장점만을 결합하는 새로운 기술을 제시한다. 본 개시에 의 하면, 작은 수의 학습 데이터들을 이용하여 라벨(label)이 없이도 인공지능 모델을 학습할 수 있고, 태스크 수 행 단계에서 새로운 클래스에 대해서도 작은 수의 데이터만을 이용해서 별도의 추가적인 학습 과정 없이 주어진 태스크를 원활하게 수행할 수 있다. 한편, 본 개시를 설명함에 있어서, 인공 지능 학습 장치는 인공 지능 장치일 수 있다. 이하, 도면을 참조하여 본 개시의 실시예에 대해 더욱 상세하게 설명할 것이다. 도 1은 본 개시의 일 실시예에 따른 인공 지능 모델 학습 방법의 구조도를 나타낸 도면이다. 보다 상세하게는, 주어진 태스크를 수행하는 인공 지능 모델을 훈련하기 위한 퓨샷(few-shot) 자기지도학습 방법의 구조도이다. 일 실시예로서, 퓨샷 자기지도 학습을 위한 인공지능 모델을 포함한 인공 지능 시스템은 학습 데이터(training dataset, 101)를 입력 데이터로 받는 인공 지능 모델(주 모델, main model)을 포함하고, 인공 지능 모델 은 입력 데이터에 따른 출력 데이터( )를 생성할 수 있다. 또한, 인공 지능 시스템에는 에피소드 메모리과 전체 메모리가 포함될 수 있다. 학습 데이터는 인공 지능 모델을 훈련시키기 위한 데이터로서, 이미지 데이터이거나, 텍스트 데이터이거나, 음성 데이터일 수 있으며, 이에 한정되는 것은 아니다. 즉, 데이터의 유형에 제한이 없으며, 복 수 개일 수 있다. 여기서 인공 지능 모델은 임의로 주어진 태스크를 수행하는데 적합하다고 가정한다. 예를 들어, 인공 지능 모델 은 CNN(convolutional neural network), 오토인코더(autoencoder) 등을 기반으로 하는 인공신경망을 포함 할 수 있다. 또다른 예로서, 인공지능 모델은 확률적 그래픽 모델(probabilistic graphical model) 등과 같은 통계 모델일 수 있다. 이하, 본 개시의 실시예를 설명함에 있어서는 내용 전달의 용이성을 위해 인공 지능 모델이 인공신경망을 포함한다고 가정하고 있으나, 본 개시가 이에 한정되는 것은 아니다. 일 실시예로서, 인공지능 모델을 학습시키기 위해 임의의 n개의 학습 데이터가 주어진다고 가정한다. 먼저, 인공지능 모델의 학습을 위해 별도의 인공지능 모델이 존재할 수 있다. 별도의 인공지능 모델은 각 각의 학습 데이터로부터 초기 특징 벡터(feature vector)들을 생성할 수 있다. 학습시키고자 하는 인공지능 모 델(주 모델)과는 별도의 인공지능 모델에 학습 데이터를 입력으로 넣어 나온 출력 데이터를 초기 특징 벡 터로 생성하여 전체 메모리를 초기화하는 데 이용될 수 있다. 한편, 일 예로서, 이 별도의 다른 인공지능 모델 은 보다 일반적인 태스크를 수행할 수 있도록 미리 학습된 것일 수 있으며, 임의의 n개일 수 있다. 즉, 각 학습 데이터는 별도의 인공지능 모델에 입력될 수 있는데, 별도의 인공 지능 모델은 출력 데이터( )로서 특징 벡터를 결과로 출력할 수 있다. 또 다른 예로, 각 학습 데이터마다 인공 지능 모델에 포함된 기 학습된 심층 인공 신경망의 입력으로 넣어 나오 는 초기 특징 벡터를 출력 데이터( )로 할 수 있는데, 여기서 인공 지능 모델이란 상기의 예처럼 별도의 인공 지능 모델이 아닌 학습시키고자 하는 인공 지능 모델(주 모델)일 수 있다. 여기서, 인공 지능 모델은 초기 화된 모델일 수 있다. 또 다른 예로, 임의로 n개의 초기 특징 벡터를 생성하여 사용하는 것도 가능하며, 본 개시가 이에 한정되는 것 은 아니다. 한편, 학습 데이터로부터 얻은 N개의 특징 벡터들은 전체 메모리(full memory)에 순차적으로 저장될 수 있 다. 이때, i를 전체 메모리의 임의의 인덱스라 하면, i∈{1,,N}이고, i번째 학습 데이터의 특징 벡터는 전체 메 모리의 i번째 슬롯(slot)에 해당할 수 있다. 한편, 인공지능 모델의 학습 과정 중 전체 메모리의 특징 벡터 역시 업데이트될 수 있다. 예를 들어, 인공 지능 모델의 학습 과정 중에 일정 주기마다 N개의 학습 데이터를 인공지능 모델의 입력데이터로 넣어 그에 대응 되는 출력 데이터로 얻은 특징 벡터를 전체 메모리에 저장함으로써 전체 메모리를 업데이트할 수 있다. 일 실시예로서, 인공지능 모델의 학습 과정은 하기와 같이 수행될 수 있다. 먼저, 우선 학습 데이터로부터 쿼리 데이터와 다수의 랜덤 데이터를 임의로 선택할 수 있다. 이때, 쿼리 데이터는 하나의 데이터일 수도 있고, 복수 개일 수도 있다. 복수 개의 쿼리 데이터가 선택되면 이를 묶어서 미니 배치(minibatch)로 구성할 수 있다. 하기의 실시예를 설명함에 있어서는 편의상 B개의 쿼리 데이터가 하나의 미니 배치를 구성한다고 가정한다. 일 예로서, 전체 메모리로부터 쿼리 데이터에 해당하는 인덱스와 특징 벡터, 랜덤 데이터의 인덱스와 특징 벡터를 찾아 하나의 에피소드 메모리(episodic memory, 103)로 구성할 수 있다. 보다 상세하게는, 에피소드 메 모리는 전체 메모리 내 N개의 특징 벡터 중 K(≤N)개를 가져와서 구성할 수 있으며, 이 중 B(≤K)개의 특 징 벡터는 쿼리 데이터의 특징 벡터에 해당할 수 있다. 여기서는 편의상 에피소드 메모리의 처음 B개의 슬롯이 쿼리 데이터의 특징 벡터에 해당한다고 가정한다. 에피소드 메모리는 함수 m과 행렬 M을 포함할 수 있는데, 슬 롯 순서를 가리키는 인덱스 k∈{1,,K}에 대해, m(k)는 에피소드 메모리 내 k번째 특징 벡터가 전체 메모리 내 몇 번째 슬롯에 해당하는 지를 나타낼 수 있고, M[k]는 에피소드 메모리 내 k번째 특징 벡터 자체를 가리킬 수 있다. 한편, 쿼리 데이터는 인공 지능 모델에 입력되면 출력 데이터( )가 도출된다고 가정한다. 이 출력데이터 ( )와 에피소드 메모리 안의 특징 벡터들과의 유사도(similarity) 계산이 수행될 수 있다. 인공지능 시 스템이 이상적인 상황이라면 출력 데이터( )와 에피소드 메모리 내, 쿼리 데이터에 해당하는 특징 벡터 의 유사도가 가장 높게 나타날 것이다. 유사도 계산 결과를 토대로 에피소드 손실(episode loss) 함수가 계산될 수 있으며, 이 손실 함수는 인공지능 모델을 학습시킬 때 활용될 수 있다. 이러한 과정을 반복하면서 인공지능 모델이 최적화될 수 있다. 일 실시예로서, 에피소드 손실 함수는 다음과 같이 계산될 수 있다. 인공 지능 모델이 심층 인공 신경망을 포함 하고, 이 심층인공신경망을 라 표시하고 i번째 학습 데이터를 , i번째 특징 벡터(또는 전체 메모리의 i번 째 슬롯의 특징 벡터)를 , 사용자 정의 온도(temperature) 파라미터를 라 하면, 객체 단위 분리 손실 함수 의 계산 방법에서 착안하여 다음과 같이 에피소드 손실 함수 EpiLoss를 계산할 수 있다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "일 예로서, 위 수학식 1의 손실 함수는 쿼리 데이터가 에피소드 메모리 내 j번째 슬롯에 해당한다고 할 때, 인 공 지능 모델을 통과 후 얻은 출력 데이터 를 이용하여 에피소드 메모리 내 j번째 슬롯 이 쿼리 데이터에 해당할 확률(P)를 기반으로 하여 계산한 것이다. 이를 여기서는 강한 어텐션 손실(hard- attention loss) 함수라고 일컬을 수 있다. 또 다른 예로, 만약 벡터 를 함수를 이용하여 의 각 요소(element)의 합을 1로 정규화 (normalization)하는 함수를 로 표시하면, 다음과 같이 에피소드 손실 함수 EpiLoss를 계산 할 수 있다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "위의 손실 함수는 쿼리 데이터가 에피소드 메모리 내 j번째 슬롯에 해당한다고 할 때, 에피소드 메모리 내 k번 째 슬롯이 쿼리 데이터에 해당할 확률을 각 요소로 갖는 확률 벡터 를 이용하여 주 모델을 통과 후 얻은 출 력 데이터 를 재생성(reconstruction)했을 때 얼마만큼 차이가 발생하는 지를 계 산한 것으로, 여기서는 약한 어텐션 손실(soft-attention loss) 함수라고 일컬을 수 있다.한편, 또 다른 예로서 강한 어텐션 손실 함수와 약한 어텐션 손실 함수를 결합하여 다음과 같이 하나의 에피소 드 손실 함수 EpiLoss로 계산할 수 있다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "다만, 위에서 설명한 손실함수 도출 과정은 일 실시예에 해당하므로, 본 개시의 손실 함수 계산이 이에 한정되 는 것은 아니다. 하기에서는 설명의 명료함을 위하여 주 모델(main model)을 포함한 인공 지능 모델이 이와 같은 방법으로 학습 될 수 있다고 가정하고 설명하나, 본 개시가 이에 한정되는 것은 아니다. 한편, 일 실시예로서, 도 1의 인공지능 모델 학습 방법에는 에피소드 메모리를 구성하기 전에 먼저 인공지능 모 델(주 모델)과 전체 메모리의 초기화 과정이나 에피소드 손실 함수를 도출한 다음, 인공지능 모델을 업데이트하 는 과정이 더 포함될 수 있다. 이에 대하여는, 도 4를 참조하여 더욱 상세하게 설명할 것이다. 도 2는 본 개시의 다른 일 실시예에 따른 지식 증류 기술 기반의 인공 지능 모델 학습 방법의 구조도를 나타낸 도면이다. 보다 상세하게는, 주어진 태스크를 수행하는 인공 지능 모델 (예를 들어, 주 모델)을 학습하기 위한 퓨샷 자기지도학습 기술을 확장하는 기술의 예로써, 지식 증류 기술을 적용한 퓨샷 자기지도학습 방법의 구조도 이다. 일 실시예로서, 퓨샷 자기지도 학습을 위한 인공지능 모델을 포함한 인공 지능 시스템은 학습 데이터(training dataset, 201)를 입력 데이터로 받는 인공 지능 모델을 주 모델(main model) 및 교사 모델(teacher model)을 포함하고, 인공 지능 모델인 주 모델은 입력 데이터에 따른 출력 데이터( )를 생성 할 수 있다. 또한, 인공 지능 시스템에는 에피소드 메모리와 전체 메모리가 포함될 수 있다. 일 실시예로서, 학습 데이터, 주 모델, 에피소드 메모리, 및 전체 메모리는 도 1에서 다른 도면 번호(예를 들어, 101, 102, 103, 104)로 설명한 것과 동일할 수 있다. 일 실시예로서, 도 2의 방법은 도 1과 달리 주 모델인 인공지능 모델 외에 별도의 인공지능 모델을 교사 모델을 더 기반으로 한다. 일 예로서, 별도의 인공지능 모델은 기 학습된 교사 모델(teacher model)이 존재하여 주 모 델을 학습할 때 지식 증류(knowledge distillation) 방법을 통해 교사 모델의 내재된 지식을 주 모델에 전달할 수 있다. 도 1의 실시예에 따르면, 학습 데이터의 클래스를 고려하지 않고 학습 데이터의 개별적인 특성만을 반 영하여 주 모델이 학습될 수 있다. 본 실시예에 의하면, 기 학습된 교사 모델을 통해 간접적으로 태스크 수행에 더 도움이 될 수 있는 의미 있는 정보들을 주 모델이 학습할 수 있다. 일 실시예로서, 교사 모델은 다음과 같은 방법으로 주 모델에 지식 증류를 수행할 수 있다. 주 모델에 심 층 인공 신경망이 포함되어 있다고 가정하고, 심층 인공 신경망의 마지막 레이어(layer)에서의 뉴런(neuron)의 개수를 라 가정한다. 이 경우, 주 모델을 통과하여 얻은 출력 데이터 는 차원의 벡터로 표현될 수 있다. 한편, 교사 모델을 통과하여 얻은 출력 데이터 가 차원의 벡터로 표현된다고 가정하면, 주 모델의 마지막 레이어에서의 뉴런의 개수를 개로 늘려, 앞의 개의 뉴런은 출력 데이터 을 생성하는데 사용될 수 있고, 뒤의 개의 뉴런은 출력 데이터 을 생성하는데 사용될 수있다. 주 모델의 마지막 레이어가 완전 연결 레이어(fully connected layer)인 경우, 단순히 뉴런의 숫자를 늘 림으로써 상기의 과정을 수행할 수 있다. 여기서, 출력 데이터 은 도 1에서 서술한 바와 같이, 에피소 드 손실 함수의 계산에 이용될 수 있으며, 특징 벡터로서의 사용도 가능할 것이다. 한편, 출력 데이터 은 지식 증류와 관련된 손실 함수를 계산하는 데 사용될 수 있다. 일 실시예로서, 손실 함수의 계산은 다음과 같이 수행될 수 있다. 먼저 에피소드 메모리를 이용하여 얻을 수 있는 에피소드 손실 함수는 상기에서 도 1을 참조하여 서술한 것과 동일한 방법을 사용할 수 있다. 그 이외 에, 지식 증류를 이용하여 얻을 수 있는 지식 증류 손실(knowledge-distillation loss) 함수는, 교사 모델이 인 공 신경망을 포함한다고 가정하고, 그 인공신경망을 라 표시하면, 다음과 같이 계산할 수 있다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "따라서 에피소드 손실 함수와 지식 증류 손실 함수를 결합하여 다음과 같이 하나의 손실 함수로 계산할 수 있다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "위 수학식 5에 의하면, 에피소드 손실 함수(EpiLoss)와 지식 증류 손실 함수(KDLoss)에 각각 가중치를 적용하여 하나의 손실함수를 생성할 수 있다. 이 경우, 각 가중치의 합은 1이 될 수 있다. 다만, 위에서 설명한 손실함수 도출 과정은 일 실시예에 해당하므로, 본 개시의 손실 함수 계산이 이에 한정되 는 것은 아니다. 한편, 일 실시예로서, 도 2의 인공지능 모델 학습 방법에는 에피소드 메모리를 구성하기 전에 먼저 인공지능 모 델(주 모델)과 전체 메모리의 초기화 과정이나 에피소드 손실 함수를 도출한 다음, 인공지능 모델을 업데이트하 는 과정이 더 포함될 수 있다. 이에 대하여는, 하기에서 도 4를 참조하여 더욱 상세하게 설명할 것이다. 도 3a는 본 개시의 다른 일 실시예에 따른 수직적 확장이 가능한 태스크를 위한 인공 지능 모델 학습 방법의 구 조도를 나타낸 도면이다. 보다 상세하게는, 일 실시 예로서 수직적 확장이 가능한 이미지 분류 태스크, 즉 이미 지 내 객체 클래스의 세분화된 단위의 분류 태스크를 수행하기 위한 학습 및 테스트 방법을 설명하기 위한 도면 이다. 일 실시예로서, 도 3a로서 도시된 인공지능 모델을 포함한 인공 지능 시스템은 학습 데이터(training dataset, 301)를 입력 데이터로 받는 인공 지능 모델을 주 모델(main model) 및 교사 모델(teacher model)을포함하고, 인공 지능 모델인 주 모델은 입력 데이터에 따른 출력 데이터( )를 생성할 수 있다. 또 한, 인공 지능 시스템에는 에피소드 메모리와 전체 메모리가 포함될 수 있다. 일 실시예로서, 학습 데이터, 교사 모델, 주 모델, 에피소드 메모리, 및 전체 메모리(30 5)는 도 1에서 다른 도면 번호(예를 들어, 101, 102, 103, 104)로 설명한 것 및/혹은 도 2에서 다른 도면 번호 (예를 들어, 201, 202, 203, 204)로 설명한 것과 동일할 수 있다. 일 실시예로서, 수직적 확장이 가능한 태스크(예를 들어, 이미지 분류 태스크)가 원활하게 수행되기 위해서는 각 세분화된 클래스의 특징 벡터가 다른 세분화된 클래스의 특징 벡터와 구별 가능하도록 모델을 학습하는 것이 필요할 수 있다. 또한, 세분화된 클래스들의 학습 데이터의 수가 상위 클래스들의 학습 데이터의 수보다 상당히 작을 수 있기 때문에 다소 작은 수의 데이터를 이용함에도 모델의 학습이 잘 이루어져야 한다. 이하, 위의 조건 들을 충족시키면서도 효율적이고 신뢰성 있는 수직적 확장이 가능한 이미지 분류 태스크를 가정하여 본 개시의 실시 예를 설명한다. 다만, 본 개시가 이에 한정되는 것은 아니다. 일 실시예로서, 도 3a의 실시예에 의하면, 수직적 확장이 가능한 태스크(예를 들어, 이미지 분류 태스크)를 수 행하도록 주 모델을 학습시킬 수 있다. 먼저 분류 대상이 되는 세분화된 클래스의 예제 데이터가 준비될 수 있 다. 이러한 예제 데이터는 학습 데이터로 표현될 수 있다. 예를 들어, 새(bird) 클래스를 세분화된 단위로 분류하고자 한다면 새의 세부 종에 해당하는 이미지(예를 들어, 비둘기, 부엉이, 참새, 뻐꾸기 등)들을 수집하 여 예제 데이터로 활용할 수 있다. 이때, 학습 데이터의 세부 클래스와 분류 대상이 되는 세부 클래스는 반드시 동일할 필요는 없다. 또한, 학습 데이터의 각 이미지들이 어떤 세부 클래스에 들어가는지에 대한 클래스 라벨이 반드시 필요한 것은 아니다. 한편, 인공지능 모델인 주 모델은 인공 신경망을 포함할 수 있으며, 특히 이미지 분류 태스크에 적합한 인 공신경망으로 선택될 수 있다. 예를 들어, 이미지 분류 태스크에 잘 적용되는 CNN(convolutional neural network)와 같은 심층 인공신경망을 주 모델로 활용할 수 있다. 다만, 본 개시가 이에 한정되는 것은 아니다. 인공지능 모델인 교사 모델 또한 인공 신경망을 포함할 수 있는데, 이미지 분류 태스크를 수행할 수 있는 기 학습된 인공 신경망으로 선택될 수 있다. 이때, 교사 모델은 주 모델과 반드시 동일한 이미지 분류 태스크를 수행하는 인공 신경망이어야만 하는 것은 아니다. 예를 들어, 교사 모델은 이미지넷(ImageNet) 분류 태스크 등 과 같이 일반적인 이미지 분류 태스크를 수행할 수 있는 학습된 모델을 선택해도 무방하다. 일 실시예로서, 이 와 같이 학습 데이터, 주 모델, 교사 모델이 준비되면 도 1, 도 2에서 서술한 것처럼 주 모델을 학습시키는 것 이 가능하다. 한편, 도 3a를 참조하여 본 개시의 실시예를 설명함에 있어 수직적 확장이 가능한 태스크로서 이미지 분류 태스 크를 들어 설명하였으나, 이는 설명의 명확함을 위한 것일 뿐이고 본 개시가 이에 한정되는 것은 아니다. 도 3b는 본 개시의 일 실시예에 따른 세분화된 클래스별 특징 벡터 등록 과정을 나타낸 도면이다. 일 실시예로서, 설명의 명확함을 위하여 도 3b의 실시예를 도 3a의 구조도를 기반으로 설명하나, 본 개시가 이 에 한정되는 것은 아니다. 도 3b의 클래스 별 특징 벡터 등록 과정이 기반으로 하는 인공 지능 시스템은 교사 모델을 포함하거나 포함하지 않을 수 있다. 일 실시예로서, 도 3b는 학습된 주 모델을 사용하여 수직적 확장이 가능한 태스크(예를 들어, 이미지 분류 태스 크)를 수행하기 위한 클래스별 특징 벡터 등록 단계를 나타낼 수 있다. 분류 대상이 되는 세분화된 클래스의 예 제들을 학습된 주 모델의 입력 데이터로 입력하여 획득한 출력 데이터를 특징 벡터로 간주하여, 이 특징 벡터와 클래스 라벨의 쌍을 데이터베이스에 저장할 수 있다. 여기서, 데이터베이스는 별도의 데이터베이스이 거나 전체 메모리일 수도 있다. 예를 들어, 새(bird) 클래스의 세분화된 클래스(북극흰갈매기, 청둥오리, 큰까 마귀, …들을 분류하기 위해 세분화된 클래스마다 하나 이상의 예제 이미지 데이터를 준비하고, 각 이미지 데이 터를 학습된 주 모델의 입력 데이터로 하여 획득한 각 입력 데이터에 해당하는 특징 벡터를 세분화된 클래스 라 벨과 함께 데이터베이스에 저장할 수 있다. 이때, 세분화된 클래스들은 반드시 학습 시에 사용한 클래스일 필요 가 없으며, 주 모델 역시 고정되어 추가적인 학습을 수행하지 않을 수 있다.도 3c는 본 개시의 일 실시예에 따른 입력 이미지 데이터의 세분화된 클래스 분류를 위한 구조도를 나타낸 도면 이다. 일 실시예로서, 설명의 명확함을 위하여 도 3c의 실시예를 도 3a 및 도 3b의 구조도를 기반으로 설명하나, 본 개시가 이에 한정되는 것은 아니다. 도 3c의 세분화된 클래스 분류가 기반으로 하는 인공 지능 시스템은 교사 모델을 포함하거나 포함하지 않을 수 있다. 일 실시예로서, 도 3c는 학습된 주 모델을 사용하여 수직적 확장이 가능한 태스크(예를 들어, 이미지 분류 태스 크)를 수행하기 위한 세분화된 이미지 분류 단계를 나타낼 수 있다. 이 단계에서는 세분화된 클래스 분류를 수 행하기 위한 쿼리 이미지(쿼리 데이터)를 입력 데이터로 하면, 먼저 쿼리 이미지 데이터가 지정된 상위 클래스 (예를 들어, 새 클래스)에 해당하는지를 판단할 수 있고, 상위 클래스에 해당할 경우, 쿼리 이미지 데이터를 학 습된 주 모델의 입력 데이터로 입력하여 특징 벡터 를 출력 데이터로 얻게 될 수 있다. 예를 들어, 새 클래스의 세분화된 클래스들을 분류하기 위해, 먼저 쿼리 이미지 데이터가 새 클래스에 해당하는지 상위 클래스 단계에서 분류할 수 있다. 상위 클래스 단계에서의 분류는 잘 알려진 인공지능 기술을 통해 구현할 수 있다. 예 를 들어, 쿼리 이미지 데이터가 새 클래스에 해당한다고 판단되면, 쿼리 이미지 데이터를 학습된 주 모델의 입 력 데이터로 입력하여 쿼리 이미지 데이터의 출력 데이터로서 특징 벡터 를 얻을 수 있다. 이후, 이 특 징 벡터 와 도 3b의 클래스별 특징 벡터 등록 단계에서 데이터베이스에 저장된 특징 벡터들과의 유사도 계산을 수행할 수 있다. 이 중 가장 유사도가 높은 특징 벡터의 클래스 라벨을 쿼리 이미지의 클래스 라벨로 할 당하여 출력 라벨을 획득할 수 있으며, 세분화된 이미지 분류를 수행할 수 있다. 하기에서는 도면을 참조하여 본 개시에 따른 에피소드 메모리 기반 인공지능 모델 학습 방법 및 장치의 실시예 에 대하여 설명한다. 한편, 에피소드 메모리 기반 인공지능 모델 학습 방법 및 장치에는 상기의 다른 도면을 참 조한 설명이 적용될 수 있으며, 위 설명에 나타난 기술, 인공 지능 시스템 등을 기반으로 할 수 있다. 다만, 본 개시가 이에 한정되는 것은 아니다. 도 4는 본 개시의 일 실시예에 따른 에피소드 메모리 기반 인공 지능 모델 학습 방법을 나타낸 도면이다. 일 실시예로서, 도 4의 에피소드 메모리 기반 인공지능 모델 학습 방법은 도 5의 에피소드 메모리 기반 인공지 능 모델 학습 장치를 포함한 인공지능 모델 학습 장치, 인공지능 시스템 및/혹은 인공 지능 장치에 의해 수행될 수 있다. 한편, 도 4에는 도시되지 않았으나, 에피소드 메모리 구성(S401) 이전에, 먼저 인공지능 모델(주 모델)과 전체 메모리의 초기화 과정이 수행될 수 있다. 일 실시예로서, 인공지능 모델의 초기화 과정은 인공지능 모델의 모수 (parameter)를 임의의 값으로 지정하거나, 기 학습된 모델의 모수를 가져오는 방식으로 수행될 수 있다. 또한, 전체 메모리의 초기화 과정은 상기 도 1을 참조한 설명과 같이 초기화된 인공지능 모델을 통과하여 얻은 출력 데이터이거나 다른 인공지능 모델로부터 얻은 출력 데이터로 지정하는 과정을 포함할 수 있다. 이러한 초기화 과정 이후, 에피소드 메모리 구성(S401)이 수행될 수 있다. 다만, 본 개시가 이에 한정되는 것은 아니다. 일 예로서, 전체 메모리에 저장된 학습데이터의 특징 벡터를 이용하여 에피소드 메모리를 구성(S401)할 수 있다. 여기서, 학습 데이터는 상기에서 언급한 바와 같이 이미지 데이터, 텍스트 데이터, 및/혹은 음성데이터일 수 있으며, 복수 개일 수 있다. 또한, 학습 데이터는 쿼리 데이터와 랜덤 데이터를 포함할 수 있다. 에피소드 메모리는 에피소드 메모리에 포함된 특징 벡터의 전체 메모리 내 인덱스 및 특징 벡터의 행렬 표현을 포함할 수 있다. 또한, 전체 메모리는 특징 벡터와 대응되는 클래스 라벨을 저장할 수 있다. 한편, 학습 데이터의 특징 벡 터는 전체 메모리에 저장되고 일정 주기마다 업데이트될 수 있다. 일 예로서, 전체 메모리는 데이터베이스를 더 포함할 수 있다. 에피소드 메모리가 구성되면, 쿼리 데이터를 인공지능 모델에 입력하여 출력 데이터를 획득(S402)할 수 있다. 일 예로서, 쿼리 데이터는 복수 개로서 미니 배치(mini batch)로 구성될 수 있다. 여기서 출력 데이터는 상기에 서 언급한 바와 같이 특징 벡터일 수 있으며, 인공지능 모델은 인공 신경망을 포함할 수 있으며, CNN(convolutional Neural Network) 혹은 오토인코더(autoencoder)를 기반으로 할 수 있다. 또한, 인공지능 모델은 제1 인공지능 모델과 제2 인공지능 모델을 포함할 수 있으며, 제2 인공지능 모델은 지식 증류를 통해 내재 적인 지식을 상기 제1 인공지능 모델에 전달하는 교사 모델이고, 제1 인공 지능 모델은 주 모델일 수 있다. 또 한, 제2 인공지능 모델은 이미지 분류 태스크를 포함한 태스크를 수행하기 위한 기 학습된 인공 신경망을 포함 할 수 있다. 여기서, 제2 인공지능 모델은 제1 인공지능 모델과 동일한 유형의 태스크(예를 들어, 이미지 분류 태스크)를 수행하는 기 학습된 인공 신경망을 포함할 수 있다. 또한, 기 학습된 인공 신경망은 CNN(Convolutional Neural Network) 기반의 인공 신경망을 포함할 수 있다. 이후, 출력 데이터와 상기 구성된 에피소드 메모리 내의 특징 벡터 간의 유사도를 도출(S403)할 수 있다. 일 예 로서, 전체 메모리에 특징 벡터와 대응되는 클래스 라벨이 저장된 경우, 유사도가 가장 높은 특징 벡터의 클래 스 라벨은 쿼리 데이터의 클래스 라벨로 할당될 수 있다. 또한, 유사도를 기반으로 에피소드 손실 함수를 도출(S404)할 수 있다. 일 예로서, 에피소드 손실 함수는 강한 어텐션 손실 함수와 약한 어텐션 손실 함수 중 적어도 어느 하나를 기반으로 도출될 수 있다. 다른 예로서, 에 피소드 손실함수가 강한 어텐션 손실 함수 및 약한 어텐션 손실 함수 모두를 기반으로 도출될 수도 있다. 한편, 상기에서 언급한 바와 같이 강한 어텐션 손실 함수는 에피소드 메모리 내 임의의 슬롯이 쿼리 데이터에 해당할 확률을 기반으로 도출될 수 있고, 약한 어텐션 손실 함수는 에피소드 메모리 내 임의의 슬롯이 쿼리 데이터에 해당하는 경우, 에피소드 메모리 내 다른 임의의 슬롯이 쿼리 데이터에 해당할 확률을 이용하여 인공 지능 모델 을 통과하여 얻은 출력 데이터 간의 차이를 기반으로 도출될 수 있다. 한편, 도 4에 명시되지는 않았으나, 에피소드 손실 함수를 도출(S404)한 다음, 인공지능 모델을 업데이트하는 과정이 수행될 수 있다. 인공지능 모델이 인공신경망이라고 가정했을 때, 에피소드 손실 함수의 미분값을 계산 하고 이를 기반으로 인공지능 모델의 모수를 업데이트 하는 역전파(back propagation) 과정이 수행될 수 있다. 일 실시예로서, 역전파 과정에는 다양한 알고리즘이 적용될 수 있으나, stochastic gradient descent (SGD) 등 과 같은 최적화 알고리즘을 이용하여 모델의 모수 업데이트 과정을 수행할 수 있다. 한편, 제2 인공지능 모델을 이용하여 지식 증류 손실 함수가 도출될 수 있는데, 이는 별도의 단계(예를 들어, 유사도를 기반으로 에피소드 손실 함수를 도출하는 단계(S404) 이후))로 표현될 수 있다. 또한, 에피소드 손실 함수와 도출된 지식 증류 손실 함수에 가중치를 적용하여 최종 손실 함수를 도출할 수 있으며, 이 또한 별도의 단계(예를 들어, 유사도를 기반으로 에피소드 손실 함수를 도출하는 단계(S404) 이후))로 표현될 수 있다. 이러 한 가중치의 합은 1일 수 있다. 도 4의 에피소드 메모리 기반 인공지능 학습 방법에 나타난 단계들의 순서는 일 실시예에 불과하므로, 순서가 변경되거나 다른 단계가 추가 혹은 일부 단계가 삭제될 수도 있다. 또한, 도 4는 본 개시의 일 실시예에 해당하므로, 또, 상기에서 언급한 초기화 과정을 제외하고 S401에서 S404 단계 및 상기에서 언급한 모델 업데이트 과정은 한 번에 진행될 수도 있으나, 위 과정을 반복하여 최적의 인공 지능 모델을 도출할 수 있다. 즉, S401에서 S404단계 및 모델 업데이트 과정은 일정 횟수만큼 반복될 수 있다. 즉, 에피소드 손실함수 도출 이후 에피소드 메모리가 재구성될 수 있다. 또한, 별도의 조건을 주고 조건이 충족 될 때까지 반복 수행될 수도 있다. 이 때, 조건은 기 설정된 것일 수도 있으나 유동적일 수도 있다. 도 5는 본 개시의 일 실시예에 따른 에피소드 메모리 기반 인공 지능 모델 학습 장치를 나타낸 도면이다. 일 실시예로서, 도 5의 에피소드 메모리 기반 인공지능 모델 학습 장치는 메모리 및 프로세서를 포함할 수 있으며, 도 1 내지 3a, 3b, 3c의 인공지능 모델 학습 방법과 도 4의 에피소드 메모리 기반 인공 지능 모델 학습 방법을 포함한 인공지능 모델 학습 방법을 수행할 수 있다. 즉, 도 1 내지 4를 참조한 설명은 도 5에 도 적용될 수 있다. 일 예로서, 메모리는 학습 데이터의 특징 벡터를 저장할 수 있으며, 전체 메모리 및 에피소드 메모리를 포 함할 수 있다. 또한 상기에서 언급한 바와 같이 데이터베이스를 포함할 수도 있다. 또한, 프로세서는 메모리를 제어할 수 있으며, 전체 메모리에 저장된 학습데이터의 특징벡터를 이용 하여 에피소드 메모리를 구성할 수 있고, 쿼리 데이터를 인공지능 모델에 입력하여 출력 데이터를 획득할 수 있 다. 또한, 출력 데이터와 구성된 에피소드 메모리 내의 특징 벡터 간의 유사도를 도출하고, 유사도를 기반으로 에피소드 손실 함수를 도출할 수 있다. 이는, 상기에서 언급한 바와 같다. 한편, 도 5에는 명시적으로 도시되지 않았으나 도 5의 장치는 외부의 인공 지능 장치, 인공 지능 학습 장 치 및 시스템을 포함한 외부의 장치 및 혹은 시스템과 통신할 수 있는 송수신부를 더 포함할 수 있다. 본 개시에 따르면 비교적 작은 수의 학습 데이터들을 이용하여 클래스 라벨이 없이도 인공 지능 모델의 학습 과 정에서 에피소드 메모리를 구성하여 인공지능 모델 학습을 수행하고, 태스크 수행 과정에서 새로운 클래스에 대 해 작은 수의 데이터만을 이용해서 주어진 태스크를 수행할 수 있는 기술이 제안된다. 특히, 본 개시의 일 실시 예에 따른 학습 기술을 이용하면, 기 학습된 인공지능 모델을 이용하여 별도의 추가적인 인공지능 모델의 학습 과정 없이 새로운 클래스에 대해서도 주어진 태스크를 원활하게 수행할 수 있다. 또한, 본 개시에 따른 학습 기 술은 지식 증류 방법 등과 같이 주어진 태스크에 대한 정보를 더 잘 반영할 수 있도록 하는 별도의 기술과도 잘 결합하여 더 나은 태스크 수행 결과를 얻을 수 있다. 본 개시의 다양한 실시 예는 모든 가능한 조합을 나열한 것이 아니고 본 개시의 대표적인 양상을 설명하기 위한 것이며, 다양한 실시 예에서 설명하는 사항들은 독립적으로 적용되거나 또는 둘 이상의 조합으로 적용될 수도 있다. 또한, 본 개시의 다양한 실시예를 설명함에 있어서 동일한 용어를 이용하여 설명된 구성요소 및/혹은 단계는 동 일한 것을 나타낼 수 있다. 또한, 각 도면을 참조한 실시예에 대한 설명들은 서로 배치되는 명확한 설명이 없는 한 상호 보완적으로 적용될 수도 있다. 또한, 본 개시의 다양한 실시 예는 하드웨어, 펌웨어(firmware), 소프트웨어, 또는 그들의 결합 등에 의해 구현 될 수 있다. 또한, 하나의 소프트웨어가 아닌 하나 이상의 소프트웨어의 결합에 의해 구현될 수 있으며, 일 주 체가 모든 과정을 수행하지 않을 수 있다. 하드웨어에 의한 구현의 경우, 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 범용 프로세서(general processor), 컨트롤러, 마이크로 컨트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 예를 들어, 상기 범용 프로세서를 포함한 다양한 형태 를 띨 수도 있다. 하나 혹은 그 이상의 결합으로 이루어진 하드웨어로 개시될 수도 있음은 자명하다. 본 개시의 범위는 다양한 실시 예의 방법에 따른 동작이 장치 또는 컴퓨터 상에서 실행되도록 하는 소프트웨어 또는 머신-실행 가능한 명령들(예를 들어, 운영체제, 애플리케이션, 펌웨어(firmware), 프로그램 등), 및 이러 한 소프트웨어 또는 명령 등이 저장되어 장치 또는 컴퓨터 상에서 실행 가능한 비-일시적 컴퓨터-판독가능 매체 (non-transitory computer-readable medium)를 포함한다. 일 실시예로서, 본 개시에 따른 비 일시적 컴퓨터 판독 가능한 매체에 저장된 컴퓨터 프로그램은, 컴퓨터에서 전체 메모리에 저장된 학습데이터의 특징벡터를 이용하여 에피소드 메모리를 구성하는 단계, 쿼리 데이터를 인 공지능 모델에 입력하여 출력 데이터를 획득하는 단계, 상기 출력 데이터와 상기 구성된 에피소드 메모리 내의 특징 벡터 간의 유사도를 구하는 단계, 상기 유사도를 기반으로 에피소드 손실 함수를 도출하는 단계를 실행할 수 있다."}
{"patent_id": "10-2021-0104631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "이상에서 설명한 본 개시는, 본 개시가 속하는 기술분야에서 통상의 지식을 가진 자에게 있어 본 개시의 기술적 사상을 벗어나지 않는 범위 내에서 여러 가지 치환, 변형 및 변경이 가능하므로, 본 개시의 범위는 전술한 실시 예 및 첨부된 도면에 의해 한정되는 것이 아니다."}
{"patent_id": "10-2021-0104631", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 인공 지능 모델 학습 방법의 구조도를 나타낸 도면이다. 도 2는 본 개시의 다른 일 실시예에 따른 지식 증류 기술 기반의 인공 지능 모델 학습 방법의 구조도를 나타낸 도면이다. 도 3a는 본 개시의 다른 일 실시예에 따른 수직적 확장이 가능한 테스크를 위한 인공 지능 모델 학습 방법의 구 조도를 나타낸 도면이다. 도 3b는 본 개시의 일 실시예에 따른 세분화된 클래스별 특징 벡터 등록 과정을 나타낸 도면이다. 도 3c는 본 개시의 일 실시예에 따른 이미지 데이터의 세분화된 클래스 분류를 위한 구조도를 나타낸 도면이다. 도 4는 본 개시의 일 실시예에 따른 에피소드 메모리 기반 인공 지능 모델 학습 방법을 나타낸 도면이다. 도 5는 본 개시의 일 실시예에 따른 에피소드 메모리 기반 인공 지능 모델 학습 장치를 나타낸 도면이다."}
