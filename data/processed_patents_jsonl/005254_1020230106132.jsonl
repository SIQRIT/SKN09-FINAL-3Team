{"patent_id": "10-2023-0106132", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0025107", "출원번호": "10-2023-0106132", "발명의 명칭": "음원 방향 추정 방법, 장치, 시스템 및 컴퓨터 프로그램", "출원인": "주식회사 케이티", "발명자": "박연석"}}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "장치에서 음원의 방향을 추정하는 방법으로서,N개의 마이크로폰 중에서 선택된 복수의 마이크로폰의 음향 신호를 조합하여 M개의 오디오 신호를 생성하는 것- 여기서, N은 3 이상인 자연수, M은 2 이상인 자연수; 상기 M개의 오디오 신호에 대하여 상기 음향 신호 사이의 비행 시간 차이를 추정하는 비행 시간 추정 알고리즘을 적용하여 복수의 비행 시간 추정 데이터를 산출하는 것; 및상기 복수의 비행 시간 추정 데이터를 미리 학습된 신경망에 입력하여 상기 오디오 신호의 음원 방향을 추정하는 것;을 포함하는, 방법."}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 비행 시간 추정 데이터는 시간의 흐름에 따른 비행 시간 차이 추정치에 대한 정보를 포함하는, 방법."}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서,상기 비행 시간 추정 데이터는 제1축을 시간 축으로 하고, 제2 축에서는 각 샘플링 주기에 따른 비행 시간 차이를 나타내는 2차원 이미지로 구성되는, 방법."}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 1에 있어서,상기 비행 시간 추정 알고리즘은 호모모픽 디콘볼루션(Homomorphic Deconvolution) 연산에 기초하여 상기 음향신호 사이의 비행 시간 차이를 추정하는, 방법."}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서,상기 비행 시간 추정 알고리즘에서는,상기 오디오 신호에 대하여 단시간 푸리에 변환(Short Time Fourier Transform)을 수행하는, 방법."}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 4에 있어서,상기 비행 시간 추정 알고리즘에서는,상기 미리 정해진 샘플 개수에 대한 윈도우 연산을 거쳐 상기 비행 시간 추정 데이터를 산출하는, 방법."}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서,상기 윈도우 연산에서의 상기 샘플 개수는 상기 마이크로폰 사이의 간격에 기초하여 산정되는, 방법."}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 7에 있어서,공개특허 10-2025-0025107-3-상기 N개의 마이크로폰은 서로 미리 정해진 간격으로 배열되는, 방법."}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 1에 있어서,상기 신경망은 상기 복수의 비행 시간 추정 데이터를 복수의 입력 채널로 제공받아 상기 음향 신호의 음원 방향에 대한 정보를 출력하는, 방법."}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "컴퓨터에서 제1 항 내지 제9항 중 어느 한 항에 기재된 방법의 각 단계를 실행시키기 위한 컴퓨터로 판독 가능한 기록 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0106132", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "프로세서; 및 메모리를 포함하는 장치에 있어서,상기 메모리는 상기 프로세서에 의해 수행될 때 상기 장치가 특정 동작을 구현하도록 구성된 명령어를포함하고, 상기 특정 동작은:N개의 마이크로폰 중에서 선택된 복수의 마이크로폰의 음향 신호를 조합하여 M개의 오디오 신호를 생성하는 것- 여기서, N은 3 이상인 자연수, M은 2 이상인 자연수;상기 M개의 오디오 신호에 대하여 상기 음향 신호 사이의 비행 시간 차이를 추정하는 비행 시간 추정 알고리즘을 적용하여 복수의 비행 시간 추정 데이터를 산출하는 것; 및상기 복수의 비행 시간 추정 데이터를 미리 학습된 신경망에 입력하여 상기 오디오 신호의 음원 방향을 추정하는 것;을 포함하는, 장치."}
{"patent_id": "10-2023-0106132", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 음원 방향 추정 방법, 장치, 시스템 및 컴퓨터 프로그램에 관한 것으로서, 보다 구체적으로는 본 발명 에서는, 장치에서 음원의 방향을 추정하는 방법으로서, N개의 마이크로폰 중에서 선택된 복수의 마이크로폰의 음 향 신호를 조합하여 M개의 오디오 신호를 생성하는 것 - 여기서, N은 3 이상인 자연수, M은 2 이상인 자연수; 상 기 M개의 오디오 신호에 대하여 상기 음향 신호 사이의 비행 시간 차이를 추정하는 비행 시간 추정 알고리즘을 적용하여 복수의 비행 시간 추정 데이터를 산출하는 것; 및 상기 복수의 비행 시간 추정 데이터를 미리 학습된 신경망에 입력하여 상기 오디오 신호의 음원 방향을 추정하는 것;을 포함하는 방법을 개시한다."}
{"patent_id": "10-2023-0106132", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 음원 방향 추정 방법, 장치, 시스템 및 컴퓨터 프로그램에 관한 것으로서, 보다 구체적으로는 복수의 마이크에서 측정되는 오디오 신호의 쌍을 합산하고 비행 시간 추정 알고리즘을 적용하여 산출되는 복수의 특징 데이터를 미리 학습된 신경망에 입력하여 음원의 방향을 보다 정확하게 추정할 수 있는 음원 방향 추정 방법, 장치, 시스템 및 컴퓨터 프로그램에 관한 것이다."}
{"patent_id": "10-2023-0106132", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "복수의 마이크 등을 사용하여 음원의 방향을 추정하기 위한 다양한 기술이 시도되고 있으며, 나아가 최근에는 인공지능(AI) 등을 기반으로 음원의 방향을 추정하기 위한 기술이 연구되고 있다. 예를 들어, 전처리 단계에서 다수의 마이크로부터 입력되는 신호와 마이크 사이의 상관 관계 등을 계산하여 특 징(feature) 데이터를 추출하고, 상기 추출된 특징값을 기초로 신경망을 학습하거나 학습된 신경망을 이용하여 음원의 방향을 추정할 수 있다. 이때, 상기 전처리를 위하여 대표적인 예를 들어 마이크 사이의 공분산 행렬(covariance matrix)의 고유 벡터 (eigenvector)를 이용한 SALSA feature나 정규화된 마이크 간 위상차(inter-channel phase difference, IPD)를 이용한 SALSA-IPD feature가 사용될 수 있으며, 이러한 특징(feature) 데이터는 각 마이크의 로그-스펙트럼 (log-spectrum)과 함께 중첩하여 인공지능 신경망 모델의 학습 데이터로 사용될 수 있다. 그런데, 이러한 종래 기술에서는 아직 충분한 정확도로 음원의 방향을 추정하지 못하고 있으며, 이에 따라 추정 된 음원의 방향을 기초로 수행되는 다양한 어플리케이션에서 오류가 발생하는 문제가 종종 발생하게 된다. 이에 따라, 음원의 방향을 보다 정확하게 추정하기 위한 방안이 지속적으로 요구되고 있으나, 아직 이에 대한 적절한 해법이 제시되지 못하고 있는 상황이다. 선행기술문헌특허문헌 (특허문헌 0001) 대한민국 공개특허공보 특2001-0083059호 (2001. 08. 31.)"}
{"patent_id": "10-2023-0106132", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기와 같은 종래 기술의 문제점을 해결하기 위해 창안된 것으로, 본 발명의 목적은 보다 높은 정확 도로 음원의 방향을 추정할 수 있는 음원 방향 추정 방법, 장치, 시스템 및 컴퓨터 프로그램을 제공하는 데 있 다. 본 발명에서 해결하고자 하는 기술적 과제는 이상에서 언급한 기술적 과제로 제한되지 않으며, 언급하지 않은"}
{"patent_id": "10-2023-0106132", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "또 다른 기술적 과제는 본 명세서에 기재된 내용으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0106132", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 과제를 해결하기 위한 본 발명의 한 측면에 따른 방법은, 장치에서 음원의 방향을 추정하는 방법으로서, N 개의 마이크로폰 중에서 선택된 복수의 마이크로폰의 음향 신호를 조합하여 M개의 오디오 신호를 생성하는 것 - 여기서, N은 3 이상인 자연수, M은 2 이상인 자연수; 상기 M개의 오디오 신호에 대하여 상기 음향 신호 사이의 비행 시간 차이를 추정하는 비행 시간 추정 알고리즘을 적용하여 복수의 비행 시간 추정 데이터를 산출하는 것; 및 상기 복수의 비행 시간 추정 데이터를 미리 학습된 신경망에 입력하여 상기 오디오 신호의 음원 방향을 추정 하는 것;을 포함할 수 있다. 여기서, 상기 비행 시간 추정 데이터는 시간의 흐름에 따른 비행 시간 차이 추정치에 대한 정보를 포함할 수 있 다. 이때, 상기 비행 시간 추정 데이터는 제1축을 시간 축으로 하고, 제2 축에서는 각 샘플링 주기에 따른 비행 시 간 차이를 나타내는 2차원 이미지로 구성될 수 있다. 또한, 상기 비행 시간 추정 알고리즘은 호모모픽 디콘볼루션(Homomorphic Deconvolution) 연산에 기초하여 상기 음향 신호 사이의 비행 시간 차이를 추정할 수 있다. 이때, 상기 비행 시간 추정 알고리즘에서는, 상기 오디오 신호에 대하여 단시간 푸리에 변환(Short Time Fourier Transform)을 수행할 수 있다. 또한, 상기 비행 시간 추정 알고리즘에서는, 상기 미리 정해진 샘플 개수에 대한 윈도우 연산을 거쳐 상기 비행 시간 추정 데이터를 산출할 수 있다. 나아가, 상기 윈도우 연산에서의 상기 샘플 개수는 상기 마이크로폰 사이의 간격에 기초하여 산정될 수 있다. 또한, 상기 3개 이상의 마이크로폰은 서로 미리 정해진 간격으로 배열될 수 있다. 또한, 상기 신경망은 상기 복수의 비행 시간 추정 데이터를 복수의 입력 채널로 제공받아 상기 음향 신호의 음 원 방향에 대한 정보를 출력할 수 있다. 또한, 본 발명의 다른 측면에 따른 컴퓨터 프로그램은, 컴퓨터에서 상기 기재된 방법의 각 단계를 실행시키기 위한 컴퓨터로 판독 가능한 기록 매체에 저장된 컴퓨터 프로그램일 수 있다. 또한, 본 발명의 다른 측면에 따른 장치는, 프로세서; 및 메모리를 포함하는 장치에 있어서, 상기 메모리는 상 기 프로세서에 의해 수행될 때 상기 장치가 특정 동작을 구현하도록 구성된 명령어를 포함하고, 상기 특정 동작 은: N개의 마이크로폰 중에서 선택된 복수의 마이크로폰의 음향 신호를 조합하여 M개의 오디오 신호를 생성하는 것 - 여기서, N은 3 이상인 자연수, M은 2 이상인 자연수; 상기 M개의 오디오 신호에 대하여 상기 음향 신호 사 이의 비행 시간 차이를 추정하는 비행 시간 추정 알고리즘을 적용하여 복수의 비행 시간 추정 데이터를 산출하 는 것; 및 상기 복수의 비행 시간 추정 데이터를 미리 학습된 신경망에 입력하여 상기 오디오 신호의 음원 방향 을 추정하는 것;을 포함할 수 있다."}
{"patent_id": "10-2023-0106132", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이에 따라, 본 발명의 일 실시예에 따른 음원 방향 추정 방법, 장치, 시스템 및 컴퓨터 프로그램에서는, 보다 높은 정확도로 음원의 방향을 추정할 수 있게 된다. 또한, 본 발명의 일 실시예에 따른 음원 방향 추정 방법, 장치, 시스템 및 컴퓨터 프로그램에서는, 음원 방향 추정을 위한 복수의 마이크로폰의 위치 또는 마이크로폰 사이의 거리에 대한 정보가 주어지지 않더라도 음원의 방향을 효과적으로 추정할 수 있게 된다. 나아가, 본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과로 제한되지 않으며, 언급하지 않은 또 다른 효"}
{"patent_id": "10-2023-0106132", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "과는 본 명세서에 기재된 내용으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이 해될 수 있을 것이다."}
{"patent_id": "10-2023-0106132", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변환을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 이하에서는 특정 실시예들을 첨부 된 도면을 기초로 상세히 설명하고자 한다. 이하의 실시예는 본 명세서에서 기술된 방법, 장치 및/또는 시스템에 대한 포괄적인 이해를 돕기 위해 제공된다. 그러나 이는 예시에 불과하며 본 발명은 이에 제한되지 않는다. 본 발명의 실시예들을 설명함에 있어서, 본 발명과 관련된 공지기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명을 생략하기로 한다. 그리고, 후술되는 용어들은 본 발명에서의 기능을 고려하여 정의된 용어들로서 이는 사용자, 운용자의 의도 또는 관례 등에 따라 달라질 수 있다. 그러므로 그 정의는 본 명세서 전반에 걸친 내용을 토대로 내려져야 할 것이다. 상세한 설명에서 사용되 는 용어는 단지 본 발명의 실시예들을 기술하기 위한 것이며, 결코 제한적이어서는 안 된다. 명확하게 달리 사 용되지 않는 한, 단수 형태의 표현은 복수 형태의 의미를 포함한다. 본 설명에서, \"포함\" 또는 \"구비\"와 같은 표현은 어떤 특성들, 숫자들, 단계들, 동작들, 요소들, 이들의 일부 또는 조합을 가리키기 위한 것이며, 기술된 것 이외에 하나 또는 그 이상의 다른 특성, 숫자, 단계, 동작, 요소, 이들의 일부 또는 조합의 존재 또는 가능 성을 배제하도록 해석되어서는 안 된다. 또한, 제1, 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되는 것은 아니며, 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만 사용된다. 먼저, 도 1에서는 본 발명의 일 실시예에 따른 음원 방향 추정 장치의 동작을 예시하고 있다. 도 1에서 볼 수 있는 바와 같이, 본 발명의 일 실시예에 따른 음원 방향 추정 장치는 음원에서 발생되는 소리를 수 신하여 상기 음원의 방향을 추정할 수 있다. 이때, 상기 음원 방향 추정 장치에는 복수의 마이크로폰이 구비되어 상기 소리를 수신하여 복수의 음향 신 호를 생성할 수 있다. 이에 따라, 상기 음원 방향 추정 장치에서는 상기 복수의 마이크로폰에서 생성된 복수의 음향 신호를 기초 로 상기 음원의 방향을 추정하게 된다.여기서, 상기 음원 방향 추정 장치는 복수의 마이크로폰과 중앙 처리 유닛(CPU) 등이 구비되는 전용 장치 등으로 구성될 수 있으나, 본 발명이 반드시 이에 한정되는 것은 아니며 자동차나 로봇 등 다양한 장치에 장착 되는 모듈 형태로 구현되거나 상기 장치와 일체형으로 구현하는 것도 가능하며, 나아가 스마트폰이나 태블릿 PC 등 다양한 단말 장치를 기반으로 구동되는 애플리케이션 등의 형태로 구현하는 것도 가능하다. 또한, 상기 음원 방향 추정 장치는 복수의 장치가 연동되어 동작하는 형태로 구현하는 것도 가능하다. 보다 구체적인 예를 들어, 복수의 마이크로폰이 구비되는 제1 장치와 상기 복수의 마이크로폰에서 생성되는 복 수의 음향 신호를 기초로 상기 음원의 방향을 추정하는 제2 장치가 통신 네트워크 등으로 연동되어 동작하 는 형태로 상기 음원 방향 추정 장치를 구현하는 것도 가능하다. 이때, 상기 제2 장치는 하나 혹은 둘 이상의 서버(server)를 이용하여 구현될 수 있으나, 본 발명이 반드시 이 에 한정되는 것은 아니며 이외에도 데스크톱 컴퓨터, 노트북, 태블릿, 스마트폰 등 개인용 전산 처리 장치를 이 용하여 구성되는 등 다양한 형태로 구현하는 것이 가능하다. 또한, 상기 제1 장치와 상기 제2 장치를 연결하는 통신 네트워크로서는 유선 네트워크와 무선 네트워크 등을 사 용할 수 있으며, 구체적으로 PSTN 등의 유선 전화망, 3G 내지 5G 등의 무선 통신망, 근거리 통신망 (LAN: Local Area Network), 도시권 통신망 (MAN: Metropolitan Area Network), 광역 통신망 (WAN: Wide Area Network) 등 의 데이터 통신망 등과 같은 다양한 통신망을 포함할 수 있고, 나아가 상기 통신 네트워크는 공지의 월드 와이 드 웹(WWW: World Wide Web) 등을 포함할 수도 있다. 또한, 도 2에서는 본 발명의 일 실시예에 따른 음원 방향 추정 방법의 순서도를 예시하고 있다. 도 2에서 볼 수 있는 바와 같이, 본 발명의 일 실시예에 따른 음원 방향 추정 방법은, 장치에서 음원 의 방향을 추정하는 방법으로서, N개의 마이크로폰 중에서 선택된 복수의 마이크로폰의 음향 신호를 조합하여 M 개의 오디오 신호를 생성하는 것 - 여기서, N은 3 이상인 자연수, M은 2 이상인 자연수(S110), 상기 M개의 오디 오 신호에 대하여 상기 음향 신호 사이의 비행 시간 차이를 추정하는 비행 시간 추정 알고리즘을 적용하여 복수 의 비행 시간 추정 데이터를 산출하는 것(S120) 및 상기 복수의 비행 시간 추정 데이터를 미리 학습된 신경망에 입력하여 상기 오디오 신호의 음원 방향을 추정하는 것(S130)을 포함할 수 있다. 여기서, 상기 비행 시간 추정 데이터는 시간의 흐름에 따른 비행 시간 차이 추정치에 대한 정보를 포함할 수 있 다. 이때, 상기 비행 시간 추정 데이터는 제1축을 시간 축으로 하고, 제2 축에서는 각 샘플링 주기에 따른 비행 시 간 차이를 나타내는 2차원 이미지로 구성될 수 있다. 또한, 상기 비행 시간 추정 알고리즘은 호모모픽 디콘볼루션(Homomorphic Deconvolution) 연산에 기초하여 상기 음향 신호 사이의 비행 시간 차이를 추정할 수 있다. 이때, 상기 비행 시간 추정 알고리즘에서는, 상기 합산 오디오 신호에 대하여 단시간 푸리에 변환(Short Time Fourier Transform)을 수행할 수 있다. 또한, 상기 비행 시간 추정 알고리즘에서는, 상기 미리 정해진 샘플 개수에 대한 윈도우 연산을 거쳐 상기 비행 시간 추정 데이터를 산출할 수 있다. 나아가, 상기 윈도우 연산에서의 상기 샘플 개수는 상기 마이크로폰 사이의 간격에 기초하여 산정될 수 있다. 또한, 상기 N개의 마이크로폰은 서로 미리 정해진 간격으로 배열될 수 있다. 또한, 상기 신경망은 상기 복수의 비행 시간 추정 데이터를 복수의 입력 채널로 제공받아 상기 음향 신호의 음 원 방향에 대한 정보를 출력할 수 있다. 이에 따라, 본 발명의 일 실시예에 따른 음원 방향 추정 방법, 장치, 시스템 및 컴퓨터 프로그램에서는 보다 높 은 정확도로 음원의 방향을 추정할 수 있으며, 나아가 음원 방향 추정을 위한 복수의 마이크로폰의 위치 또 는 마이크로폰 사이의 거리에 대한 정보가 주어지지 않더라도 음원의 방향을 효과적으로 추정할 수 있게 된 다. 이하, 도 1 내지 2를 참조하여 본 발명의 일 실시예에 따른 음원 방향 추정 방법을 각 단계별로 나누어 보다 자세하게 살핀다.먼저, 상기 S110 단계에서는, 상기 장치에서 N개의 마이크로폰 중에서 선택된 복수의 마이크로폰의 음향 신호를 조합하여 M개의 오디오 신호를 생성하게 되며, 이때 N은 3 이상인 자연수, M은 2 이상인 자연수일 수 있 다. 이때, 상기 마이크로폰은 음파를 수신하여 이에 대응하는 전기 신호 또는 데이터를 생성할 수 있는 일체의 장치 를 말한다. 여기서, 도 3에서 볼 수 있는 바와 같이, 상기 장치에는 3개 이상 복수의 마이크로폰이 구비될 수 있으며, 이때 상기 마이크로폰은 서로 미리 정해진 간격으로 배열될 수 있다. 이때, 도 3에서는 상기 장치에 4개의 마이크로폰(141, 142, 143, 144)이 정사각형 형상으로 배열되는 마이 크로폰 어레이가 구비되는 경우를 예시하고 있으며, 이하 본 발명의 일 실시예로서 도 3과 같이 상기 장치 에 4개의 마이크로폰(141, 142, 143, 144)이 구비되는 경우를 들어 설명하겠으나, 본 발명이 반드시 이에 한정되는 것은 아니다. 이에 따라, 도 3에서 볼 수 있는 바와 같이, 상기 장치에서는 각 마이크로폰(141, 142, 143, 144)에서 상 기 음원에서 발생되는 소리를 수신하여 각각 음향 신호로 생성하게 된다. 이때, 도 3에서 볼 수 있는 바와 같이, 상기 음원에서 발생되어 전달되는 소리는 상기 음원의 방향에 따라 상기 각 마이크로폰(141, 142, 143, 144)에 도달하는 비행 시간(즉, 거리)가 달라질 수 있다. 보다 구체적인 예를 들어, 도 3의 경우, 음원에서 발생된 소리는 제1 마이크로폰에 가장 먼저 도달할 수 있고, 이어서 T14의 비행 시간 차이를 두고 제4 마이크로폰에 도달할 수 있으며, 다시 T42의 비행 시간 차이를 두고 제2 마이크로폰에 도달할 수 있고, 마지막으로 T23의 비행 시간 차이를 두고 제3 마이크로폰 에 도달할 수 있다. 따라서, 가장 이격된 제1 마이크로폰과 제3 마이크로폰의 경우에는 T13의 비 행 시간 차이를 가질 수 있다. 이에 따라, 도 4에서 볼 수 있는 바와 같이, 상기 장치에서는 상기 각 마이크로폰(141, 142, 143, 144)에 서 생성된 음향 신호를 조합하면서 2개씩 묶어 각 음향 신호의 쌍에 대한 합산을 통해 복수의 오디오 신호를 생성할 수 있다. 보다 구체적인 예를 들어, 도 4에서 볼 수 있는 바와 같이, 4개의 마이크로폰(141, 142, 143, 144)에서 생성된 4개의 음향 신호 (M1, M2, M3, M4)를 2개씩 짝을 지으면 (M1, M2), (M1, M3), (M1, M4), (M2, M3), (M2, M4), (M3, M4)의 6개의 조합을 구성할 수 있으며, 상기 6개의 조합의 각 음향 신호를 합산하여 아래 [수학 식 1]과 같이 6개의 오디오 신호를 생성할 수 있다. [수학식 1] S12 = M1+M2 S13 = M1+M3 S14 = M1+M4 S23 = M2+M3 S24 = M2+M4 S34 = M3+M4 이어서, 상기 S120 단계에서는, 상기 장치에서 상기 M개의 오디오 신호에 대하여 상기 음향 신호 사이의 비행 시간 차이를 추정하는 비행 시간 추정 알고리즘을 적용하여 복수의 비행 시간 추정 데이터를 산출하게 된 다. 이때, 본 발명에서 상기 비행 시간 추정 알고리즘은 호모모픽 디콘볼루션(Homomorphic Deconvolution) 연산에 기초하여 상기 음향 신호 사이의 비행 시간 차이를 추정하는 알고리즘일 수 있다. 이때, 상기 도 5에서는 호모모픽 디콘볼루션(Homomorphic Deconvolution) 알고리즘의 연산 절차를 예시하고 있 다. 보다 구체적으로 음원에서 발생한 소리 x[n]는 시간 지연 d 차이를 두고 각각의 마이크로폰에 전달된다. 이때 시간 지연 정보가 포함된 해당 전달 함수 h[n]는 감쇠율 α를 가지는 δ[n] + αδ[n - d]로 표 현될 수 있다. 또한, 두 마이크로폰에 입력된 신호 y[n]는 x[n] * h[n]와 같이 콘볼루션(convolution)의 형태로 표현될 수 있 으며, 이때 h[n]은 비행 시간 정보를 포함하는 전달함수일 수 있다. 또한, 절대값의 로그 연산이 있는 첫번째 고속 푸리에 변환(FFT) 및 고속 푸리에 역변환(IFFT) 쌍으로 음원 신호와 신호 전파의 분포를 나누는 실 스펙트럼을 나타내고, 윈도우 함수 w[n]로 비행 시간의 전달 함수를 정확히 추출한다. 지수 함수가 포함된 다음 고속 푸리에 변환(FFT) 및 고속 푸리에 역변환(IFFT) 쌍으로 비매개변수 전파의 임펄스 응답 을 계산하고 추정 할 수 있다. 이에 따라, 상기 x[n] * h[n]에 대하여 디콘볼루션 연산을 이용해 h[n]의 추정치 를 구하여 비행 시간을 산출할 수 있게 된다. 보다 구체적으로, 본 발명에서 상기 비행 시간 추정 알고리즘은, 도 5에서 볼 수 있는 바와 같이, 푸리에 변환 (211, 221), 절대값 연산(212, 222), 로그 연산(213, 223), 역푸리에 변환(214, 224), 윈도우 연산(215, 225), 푸리에 변환(216, 226), 지수 연산(217, 227) 및 역푸리에 변환(218, 228)을 포함하여 구현될 수 있다. 또한, 도 5에서 (a)의 제1 알고리즘(이하, Homomorphic Deconvolution(HD) 알고리즘이라 칭함)과 (b)의 제2 알 고리즘(이하, Short Time Homomorphic Deconvolution(STHD) 알고리즘이라 칭함)을 대비하여 보면, 도 5 (a)의 제1 알고리즘에서는 푸리에 변환으로서 고속 푸리에 변환(FFT)을 적용하는데 반하여, 도 5 (b)의 제2 알고 리즘에서는 단시간 푸리에 변환(Short Time Fourier Transform)을 적용하는 것을 알 수 있으며, 이러한 경우 시 간의 흐름에 따른 주파수 성분의 변화를 산출할 수 있어 상기 제2 알고리즘에서는 시간의 흐름에 따른 비행 시 간 차이 추정치를 파악할 수 있다는 장점을 가질 수 있게 된다. 나아가, 도 5 (b)에서 볼 수 있는 바와 같이, 상기 제2 알고리즘에서는 미리 정해진 샘플 개수에 대한 윈도우 연산을 거쳐 상기 비행 시간 추정 데이터를 산출할 수 있고, 이때 상기 윈도우 연산에서의 샘플 개수 는 상기 마이크로폰 사이의 간격에 기초하여 정해질 수 있으며, 이에 따라 도 7에서 볼 수 있는 바와 같이 상기 마이크로폰 사이의 거리보다 짧은 샘플링 주기에서의 비행 시간 사이 추정치를 제거하여 줌으로써(예를 들어, 도 7에서 세로축(Time of Flight)의 1 - 3 번째 샘플을 제거), 상기 마이크로폰 사이의 거리보다 짧은 샘플링 주기에서의 노이즈 등에 의한 오류를 효과적으로 억제할 수 있게 된다. 이에 따라, 상기 도 5 (a)의 제1 알고리즘의 경우에는, 도 6에서 볼 수 있는 바와 같이 특정 시점에서의 비행 시간 차이 추정치를 보여주는 1차원(선형) 데이터를 산출하게 되는 반면, 상기 도 5 (b)의 제2 알고리즘의 경우 에는, 도 7에서 볼 수 있는 바와 같이 제1 축을 시간 축으로 하고, 제2 축에서는 비행 시간 차이 추정치를 보여 주는 2차원(평면) 데이터를 산출할 수 있게 된다. 나아가, 상기 음향 신호 또는 상기 오디오 신호는 소정의 샘플링 주기로 샘플링되어 처리될 수 있는 데, 이에 따라 상기 비행 시간 추정 데이터는, 도 7에서 볼 수 있는 바와 같이, 제1축을 시간 축으로 하고, 제2 축에서는 각 샘플링 주기에 따른 비행 시간 차이를 나타내는 2차원 이미지로 구성될 수 있다. 보다 구체적인 예를 들어, 도 6의 (a)에서는 제1 마이크로폰(M1)과 제2 마이크로폰(M2)에서 생성된 음향 신호 를 합산한 오디오 신호에 대한 제1 알고리즘 처리 결과 데이터를 예시하고 있는데, 가로축(Time of Flight)의 4번째 샘플에서 가장 높은 크기(Magnitude)를 나타내는 바, 상기 제1 마이크로폰(M1)과 제2 마이크로 폰(M2)의 비행 시간 차이 추정치는 4 샘플링 주기로 추정될 수 있다. 이에 대하여, 도 7의 (a)에서도 제1 마이크로폰(M1)과 제2 마이크로폰(M2)에서 생성된 음향 신호를 합산한 오디오 신호에 대한 제2 알고리즘 처리 결과 데이터를 예시하고 있는데, 세로축(Time of Flight)의 4번째 샘플에서 가장 높은 빈도로 뚜렷하게 적색을 띄게 되는 바, 상기 제1 마이크로폰(M1)과 제2 마이크로폰(M2)의 비행 시간 차이 추정치는 4 샘플링 주기로 추정될 수 있으며, 나아가 가로축(시간축) 0 - 3000 샘플의 시간 범 위에서 시간의 변화에 따른 상기 비행 시간 차이 추정치의 변동 여부 및 추세를 파악할 수 있음을 알 수 있다. 이에 따라, 상기 S130 단계에서는, 상기 장치가 상기 복수의 비행 시간 추정 데이터를 미리 학습된 신경망 에 입력하여 상기 오디오 신호의 음원 방향을 추정하게 된다. 보다 구체적인 예를 들어, 도 4에서 볼 수 있는 바와 같이, 상기 장치에서 상기 신경망은 상기 복수의 비 행 시간 추정 데이터를 복수의 입력 채널(예를 들어, 도 4에서는 6개 채널)로 제공받아 상기 음향 신호의 음원 방향에 대한 정보를 출력할 수 있다(예를 들어, 상기 음원의 방향을 0도 내지 360도의 범위에서 1도 단위로 나누어진 360개 방향으로 출력). 예를 들어, 본 발명의 일 실시예로서 상기 신경망으로 도 8에 예시된 바와 같이 콘볼루션 연산에 기반하는 ResNet을 사용할 수 있으나, 본 발명이 반드시 이에 한정되는 것은 아니다. 보다 구체적으로, 본 발명에서는 도 8의 (a)의 사전 학습된 ResNet을 기초로 하면서, 도 8의 (b)에서 볼 수 있 는 바와 같이, 입력 채널(input channel)을 입력되는 비행 시간 추정 데이터의 개수(예를 들어, 6개)에 맞추어 각 비행 시간 추정 데이터에 포함되는 정보를 최대한 전달할 수 있도록 하였다. 또한, 도 8의 (b)에서는 출력 사이즈(output size)를 4로 하여 각각 음원의 방향이 0도, 90도, 180도, 270 도에 대응하도록 하였으나, 본 발명이 반드시 이에 한정되는 것은 아니며 적용되는 어플리케이션에 따라 1도, 5 도, 10도 등 다양한 각도 단위로 출력을 조절하는 것도 가능하다. 보다 구체적으로, 본 발명의 일 실시예로서 도 8에서는 이미지 분류를 효율적으로 수행할 수 있으며 가벼운 ResNet18 모델을 이용해서 학습을 수행하였으며, 학습 결과를 신속히 확인할 수 있도록 사전 훈련된 (pretrained) 모델을 채택하였다. 이때, 상기 사전 훈련된 모델은 RGB 이미지인 3채널 데이터를 입력으로 받게 되며, 224 x 224 x 3의 크기의 입 력을 가지는 바, 상기 생성된 특징(feature) 데이터의 너비와 높이가 상이한 경우(예를 들어, 257 x 255) 이를 리사이즈(resize)할 수 있으며, 나아가 6개의 채널은 유지하여 224 x 224 x 6 의 크기를 가지는 데이터로 변형 하여 모델에 입력하였다. 이때 채널 개수를 6개로 유지한 이유는 상기 특징(feature) 데이터의 각 채널에 포함 되는 주요한 정보들을 최대한 손상시키지 않기 위한 것이었다. 이에 따라, 본 발명의 일 실시예로서, 상기 ResNet18 모델의 첫번째 콘볼루션 레이어(Convolution Layer)의 입 력 채널 크기를 6채널로 변경하여 특징 데이터를 사용할 수 있게 하였으며, 또한 사용한 AI 모델 학습은 첫번째 콘볼루션 레이어(Convolution Layer)와 마지막 선형 레이어(Linear Layer)를 제외하고 모든 파라미터는 사전 훈 련된 파라미터로 고정(freeze)하여 적용하여, 학습 과정에서 변경되는 파라미터의 개수를 줄이고 학습 속도를 높여 모델을 신속히 구현할 수 있도록 하였다. 또한, 도 9에서는 본 발명의 일 실시예에 따른 음원 방향 추정 방법의 구체적인 순서도를 예시하고 있다. 이하, 도 9를 참조하여 본 발명의 일 실시예에 따른 음원 방향 추정 방법을 수행하는 구체적인 과정을 설명한다. 먼저, S210 단계에서는, 상기 장치가 3개 이상의 다채널 마이크로폰이 음원에서 발생한 소리를 수신하 여 각 채널에 대한 음향 신호를 생성하게 된다. 이어서, S220 단계에서는, 상기 장치가 각 마이크로폰의 쌍에서의 음향 신호에 대하여 합산 연산을 통해 복수의 오디오 신호를 생성하게 된다. 또한, S230 단계에서는, 상기 장치가 상기 복수의 오디오 신호에 대하여 STHD 알고리즘을 적용하게 된다. 이에 따라, S240 단계에서는, 상기 장치가 복수의 비행 시간 추정 데이터를 특징(feature) 데이터로 산출 할 수 있게 된다. 이어서, S250 단계에서는, 상기 장치가 상기 복수의 비행 시간 추정 데이터를 특징(feature) 데이터로 하 여 신경망에 대한 학습을 수행하거나, 상기 복수의 비행 시간 추정 데이터를 미리 학습된 신경망에 입력할 수 있다. 이에 따라, S260 단계에서는, 상기 장치가 상기 신경망의 출력 데이터를 기초로 상기 오디오 신호의 음원 방향을 추정할 수 있게 된다. 또한, 도 10에서는 본 발명에 대한 성능을 검증하는 실험을 수행하기 위한 장치를 예시하고 있으며, 도 11에서 는 이에 따른 실험 결과를 예시하고 있다. 보다 구체적으로, 도 10에서 볼 수 있는 바와 같이, 실험에서는 4채널(141, 142, 143, 144) 마이크로폰 어레이 가 사용되었고, 상기 장치에서 사용된 샘플링 주파수는 44100 Hz이다. 이때, 상기 마이크로폰 사이의 거리는 최소 3.23 cm에서 최대 6.46 cm이며, 이를 샘플(sample) 단위로 전환하면 대략 4 ~ 8샘플에 해당한다 (즉, , 여기서 소리의 속도는 약34300 cm/s). 따라서, 상기 실험예에서 비행 시간 추정 알고리즘에서는 4 내지 8 샘플에서 유의미한 비행 시간 차이 결과 값 이 추정되어 산출될 가능성이 높아지게 된다. 이에 대하여, 본 발명의 실험예에서는 상기 STHD 알고리즘에서 윈도우 연산(도 5 (b)의 225, 229)에 대한 최적 의 값의 실험적으로 도출하여 적용하였다. 보다 구체적으로, 첫번째 윈도우 연산(도 5 (b)의 225)에서는 첫번째 샘플만 0이고 그 외에는 1을 갖도록 윈도 우 함수를 구성하였다. 상기 윈도우 연산은 마이크로폰 사이의 거리를 고려하여 불필요한 정보를 제거하는 용도로 사용되는데, 실험예에서는 마이크로폰 사이의 거리가 짧아서 첫번째 샘플만 0으로 설정하였다. 또한, 본 발명에서는 STHD 알고리즘의 최종 결과값에 대해서도 낮은 비행 시간 샘플에서 발생할 수 있는 높은 결과치를 제거할 수 있도록 윈도우 연산(도 5 (b)의 229)을 추가하였다. 이때, 상기 실험예에서는 마이크로폰 어레이에서 얻을 수 있는 최소 비행 시간이 약 4샘플(3.23cm)이므로, 1 내지 3샘플을 0으로 설정하는 윈도우 연산을 통해 불필요한 정보를 제거하였다. 이러한 윈도우 연산에 따라 도 7의 (a) 내지 (f) 그래프에서 (세로축의) 1 내지 3 샘플에 해당하는 데이터 가 모두 0으로 설정된 것을 알 수 있다. 이에 대하여, 도 6에서는 도 10의 마이크로폰 어레이에서 반시계 방향 45도(약 10시 방향)에 위치하는 음 원의 소리에 HD 알고리즘을 적용한 경우의 출력치를 예시하고 있고, 도 7에서는 STHD 알고리즘을 적용한 경 우의 출력치를 예시하고 있다. 이때, 상기 HD 알고리즘에서는 알고리즘 중간에 평균을 계산하여 비행 시간을 추정하면서 산출되는 최종 결과치 가 1차원 데이터로 산출될 수 있다. 반면, 상기 STHD 알고리즘에서는 시간에 따른 비행 시간을 추정하여 최종 결과치가 2차원 데이터로 산출될 수 있어 시간의 경과에 따른 추가적인 정보를 포함할 수 있다는 것을 알 수 있다. 또한, 본 발명의 실시예에서, 상기 STHD 알고리즘의 주파수 프레임 크기는 512, 윈도우 크기는 1024이고, 도 7 의 (b)에서는 분석을 용이하게 수행할 수 있도록 40 샘플 이하 데이터만 표시하였다. 이때, 상기 음원이 상기 반시계 방향 45도(약 10시 방향)에 위치하는 경우, 상기 마이크로폰 어레이에 서는 S24 오디오 신호 (즉, M2 + M4)의 비행 시간 차이가 6.46cm (약 8샘플)로 가장 길어지게 된다. 이에 따라, 도 6의 (c) 및 도 7의 (c)에서 모두 S24 (M2 + M4)의 비행 시간 차이가 약 8샘플에서 가장 높은 값 으로 추정되는 것을 확인할 수가 있다. 또한, 도 6 및 도 7에서 S12, S14, S23, S34의 경우를 살펴보더라도 비행 시간 차이가 마이크로폰 사이의 거리 (3.23cm)에 대응하는 약 4샘플에서 가장 높은 값으로 추정되는 것을 확인할 수 있다. 반면, S13은 상기 음원으로부터 진행되는 음향의 방향(예를 들어, 반시계 방향 45도)에 대하여 수직으로 마 이크로폰이 배열(마이크로폰(141, 143)이 각각 시계 방향 45도, 135도에 배열)되는 형태이므로 상기 마이크로폰 (141, 143)에서의 비행 시간 차이가 0이 되고, 따라서 비행 시간 추정 알고리즘이 정확하게 비행 시간 차이를 추정하지 못하면서 노이즈 형태의 쓰레기 데이터가 출력될 수 있다. 또한, 본 발명의 실험예로서, STHD 알고리즘에서는 6채널 데이터를 구성하고 이를 기초로 각 데이터 셋에 대해 데이터 표준화 또는 정규화를 진행하였다. 이때, 상기 표준화 또는 정규화는 채널별로 진행하였는데, 모든 데이 터에서 각 채널 별로 평균과 표준편차를 구하여 저장하고, 상기 평균과 표준편차로 데이터를 표준화 (Standardization) 또는 정규화(Normalization)를 수행하여 데이터의 분포가 정규 분포를 가지도록 하였으며, 보다 구체적으로 이미지 데이터에서 각 RGB 채널 별로 평균 및 표준 편차를 구하는 방식을 그대로 적용하였다. 이에 따라, 본 발명의 실험예에서는, 상기 표준화 또는 정규화를 수행한 특징(feature) 데이터를 딥러닝 모델에 적용하였으며, 앞서 설명한 바와 같이 상기 딥러닝 모델은 0, 45, 90, 135도를 분류하는 모델로 구성하였다. 이에 따라, 본 발명의 실험예에서는 상기 학습된 모델을 기초로 학습과 검증에 사용한 모든 데이터에 대하여 99.5% 이상일 때 정답이 맞는지 검증하였고, 이에 대한 결과를 기초로 도 11의 혼돈 매트릭스(confusion matrix)로 구성하였으며, 이에 따라 본 발명의 실험예에서 4개 방향에 대해 높은 정확도로 음원의 방향을 추정할 수 있음을 확인할 수 있다. 부다 구체적으로, 상기 음원이 0도 방향(예를 들어, 12시 방향)에 위치하는 경우에 대하여 정확하게 방향을 예측한 케이스는 1412건이고 잘못 예측한 경우는 11건으로 약 99.2%의 정확도를 보였다. 다만, 상기 음원이 90도 방향(예를 들어, 9시 방향)에 위치하는 경우에는 정확도가 86.0% 정도로 다소 정확 도가 떨어지는 결과를 보여주었는데, 이는 도 10의 마이크로폰(143, 144) 사이에 위치하는 컨넥터 등 음향의 진 행에 간섭을 일으킬 수 있는 구조물에 의한 영향으로 판단되는 바, 마이크로폰(141, 142, 143, 144) 사이에 위치하는 구조물 등을 재배치하는 등의 방식으로 구조를 개선하여 방향에 따른 정확도 편차를 더욱 개선하는 것도 가능하다. 또한, 본 발명의 또 다른 측면에 따른 컴퓨터 프로그램은 앞서 살핀 음원 방향 추정 방법을 실행할 수 있도록 컴퓨터로 판독 가능한 매체에 저장된 컴퓨터 프로그램인 것을 특징으로 한다. 상기 컴퓨터 프로그램은 컴파일러 에 의해 만들어지는 기계어 코드를 포함하는 컴퓨터 프로그램뿐만 아니라, 인터프리터 등을 사용해서 컴퓨터에 서 실행될 수 있는 고급 언어 코드를 포함하는 컴퓨터 프로그램일 수도 있다. 이때, 상기 컴퓨터로서는 퍼스널 컴퓨터(PC)나 노트북 컴퓨터 등에 한정되지 아니하며, 서버, 스마트폰, 태블릿 PC, PDA, 휴대전화 등 중앙처리 장치(CPU)를 구비하여 컴퓨터 프로그램을 실행할 수 있는 일체의 정보처리 장치를 포함한다. 또한, 컴퓨터가 읽을 수 있는 매체는, 컴퓨터로 실행 가능한 프로그램을 계속 저장하거나, 실행 또는 다운로드 를 위해 임시 저장하는 것일 수도 있다. 또한, 매체는 단일 또는 복수의 하드웨어가 결합된 형태의 다양한 기록 수단 또는 저장수단일 수 있는데, 어떤 컴퓨터 시스템에 직접 접속되는 매체에 한정되지 않고, 네트워크 상에 분산 존재하는 것일 수도 있다. 따라서, 상기의 상세한 설명은 모든 면에서 제한적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적 해석에 의해 결정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. 또한, 도 12에서는 본 발명의 일 실시예에 따른 음원 방향을 추정하는 장치의 구성도를 예시하고 있다. 도 12를 참조하면, 상기 장치는 본 발명의 제안 방법에 따라 음원의 방향을 추정하는 프로세스를 구현 하도록 구성될 수 있다. 예를 들어, 본 발명의 제안 방법이 적용될 수 있는 장치는 리피터, 허브, 브리지, 스위치, 라우터, 게이트 웨이 등과 같은 네트워크 장치, 데스크톱 컴퓨터, 워크스테이션 등과 같은 컴퓨터 장치, 스마트폰 등과 같은 이 동 단말, 랩톱 컴퓨터 등과 같은 휴대용 기기, 디지털 TV 등과 같은 가전 제품, 자동차 등과 같은 이동 수단 등 을 포함할 수 있다. 다른 예로, 본 발명이 적용될 수 있는 장치는 SoC(System On Chip) 형태로 구현된 ASIC(Application Specific Integrated Circuit)의 일부로 포함될 수 있다. 메모리는 프로세서와 동작 시 연결될 수 있고, 프로세서의 처리 및 제어를 위한 프로그램 및/또는 명령어들을 저장할 수 있고, 본 발명에서 사용되는 데이터와 정보, 본 발명에 따른 데이터 및 정보 처리를 위해 필요한 제어 정보, 데이터 및 정보 처리 과정에서 발생하는 임시 데이터 등을 저장할 수 있다. 메모리는 ROM(Read Only Memory), RAM(Random Access Memory), EPROM(Erasable Programmable Read Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), 플래쉬(flash) 메모리, SRAM(Static RAM), HDD(Hard Disk Drive), SSD(Solid State Drive) 등과 같은 저장 장치로서 구현될 수 있다. 프로세서는 메모리 및/또는 네트워크 인터페이스와 동작 시 연결(operatively connected)될 수 있 고, 장치 내 각 모듈의 동작을 제어한다. 특히, 프로세서는 본 발명의 제안 방법을 수행하기 위한 각종 제어 기능을 수행할 수 있다. 프로세서는 컨트롤러(controller), 마이크로 컨트롤러(microcontroller), 마 이크로 프로세서(microprocessor), 마이크로 컴퓨터(microcomputer) 등으로도 불릴 수 있다. 본 발명의 제안 방 법은 하드웨어(hardware) 또는 펌웨어(firmware), 소프트웨어, 또는 이들의 결합에 의해 구현될 수 있다. 하드 웨어를 이용하여 본 발명을 구현하는 경우에는, 본 발명을 수행하도록 구성된 ASIC(application specific integrated circuit) 또는 DSP(digital signal processor), DSPD(digital signal processing device), PLD(programmable logic device), FPGA(field programmable gate array) 등이 프로세서에 구비될 수 있다. 한편, 펌웨어나 소프트웨어를 이용하여 본 발명의 제안 방법을 구현하는 경우에는 펌웨어나 소프트웨어는 본 발 명의 제안 방법을 구현하는 데 필요한 기능 또는 동작들을 수행하는 모듈, 절차 또는 함수 등과 관련된 명령어 (instruction)들을 포함할 수 있으며, 명령어들은 메모리에 저장되거나 메모리와 별도로 컴퓨터 판독가 능한 기록 매체(미도시)에 저장되어 프로세서에 의해 실행될 때 장치가 본 발명의 제안 방법을 구현하 도록 구성될 수 있다. 또한, 장치는 네트워크 인터페이스 디바이스(network interface device)를 포함할 수 있다. 네트워크 인터페이스 디바이스는 프로세서와 동작 시 연결되며, 프로세서는 네트워크 인터페이스 디바이스 를 제어하여 무선/유선 네트워크를 통해 정보 및/또는 데이터, 신호, 메시지 등을 나르는 무선/유선 신호를 전송 또는 수신할 수 있다. 네트워크 인터페이스 디바이스는 예를 들어 IEEE 802 계열, 3GPP LTE(-A), 3GPP 5G 등과 같은 다양한 통신 규격을 지원하며, 해당 통신 규격에 따라 제어 정보 및/또는 데이터 신호를 송수신할 수 있다. 네트워크 인터페이스 디바이스는 필요에 따라 장치 밖에 구현될 수도 있다.이상에서 설명된 실시예들은 본 발명의 구성 요소들과 특징들이 소정 형태로 결합된 것들이다. 각 구성 요소 또 는 특징은 별도의 명시적 언급이 없는 한 선택적인 것으로 고려되어야 한다. 각 구성 요소 또는 특징은 다른 구 성 요소나 특징과 결합되지 않은 형태로 실시될 수 있다. 또한, 일부 구성 요소들 및/또는 특징들을 결합하여 본 발명의 실시예를 구성하는 것도 가능하다. 본 발명의 실시예들에서 설명되는 동작들의 순서는 변경될 수 있 다. 어느 실시예의 일부 구성이나 특징은 다른 실시예에 포함될 수 있고, 또는 다른 실시예의 대응하는 구성 또 는 특징과 교체될 수 있다. 특허청구범위에서 명시적인 인용 관계가 있지 않은 청구항들을 결합하여 실시예를 구성하거나 출원 후의 보정에 의해 새로운 청구항으로 포함시킬 수 있음은 자명하다."}
{"patent_id": "10-2023-0106132", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 발명에 관한 이해를 돕기 위해 상세한 설명의 일부로 포함되는, 첨부도면은 본 발명에 대한 실시예를 제공하 고, 상세한 설명과 함께 본 발명의 기술적 사상을 설명한다. 도 1은 본 발명의 일 실시예에 따른 음원 방향 추정 장치의 동작을 설명하는 도면이다. 도 2는 본 발명의 일 실시예에 따른 음원 방향 추정 방법의 순서도이다. 도 3 내지 도 9는 본 발명의 일 실시예에 따른 음원 방향 추정 방법의 구체적인 구성 및 동작을 설명하는 도면 이다. 도 10 내지 도 11은 본 발명의 일 실시예에 따른 음원 방향 추정 방법에 대한 실험예를 예시하는 도면이다. 도 12는 본 발명의 일 실시예에 따른 음원 방향 추정 장치의 구성을 예시하는 도면이다."}
