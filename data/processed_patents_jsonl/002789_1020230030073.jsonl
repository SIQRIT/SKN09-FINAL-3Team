{"patent_id": "10-2023-0030073", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0136731", "출원번호": "10-2023-0030073", "발명의 명칭": "디바이스 클러스터링을 이용한 연합 학습 방법 및 장치", "출원인": "아주대학교산학협력단", "발명자": "고영배"}}
{"patent_id": "10-2023-0030073", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "복수의 클라이언트에 대하여 클러스터링을 수행한 결과, 일부 클라이언트를 포함하는 임의의 클라이언트 그룹을획득하는 단계;상기 클러스터링과 연관된 센트로이드(centroid)를 기초로, 상기 일부 클라이언트 중 하나의 클라이언트를 리더클라이언트를 결정하는 단계 - 상기 리더 클라이언트는 상기 일부 클라이언트 각각으로부터 사전 학습된 모델의적어도 하나의 파라미터와 연관된 데이터를 수신함 -;상기 사전 학습된 모델의 컴퓨팅 리소스 양 및 상기 사전 학습된 모델의 트레이닝-로스(training loss)를 기초로, 상기 일부 클라이언트 중 적어도 하나의 클라이언트를 대상 클라이언트로 결정하는 단계; 및상기 리더 클라이언트로부터 상기 대상 클라이언트의 모델의 적어도 하나의 파라미터와 연관된 일부 데이터를수신하는 단계를 포함하고,상기 일부 데이터는 상기 데이터에 포함되는, 적어도 하나의 프로세서에 의해 수행되는 디바이스 클러스터링을이용한 연합 학습 방법."}
{"patent_id": "10-2023-0030073", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 대상 클라이언트와 연관된 제1 트레이닝 로스는 상기 일부 클라이언트 중 상기 대상 클라이언트가 아닌 임의의 클라이언트와 연관된 제2 트레이닝 로스보다 크도록 구성되는, 적어도 하나의 프로세서에 의해 수행되는디바이스 클러스터링을 이용한 연합 학습 방법."}
{"patent_id": "10-2023-0030073", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 클러스터링은 상기 복수의 클라이언트 각각 사이의 통신 거리를 기초로 수행되는, 적어도 하나의 프로세서에 의해 수행되는 디바이스 클러스터링을 이용한 연합 학습 방법."}
{"patent_id": "10-2023-0030073", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 클러스터링과 연관된 센트로이드를 기초로, 상기 일부 클라이언트 중 하나의 클라이언트를 리더 클라이언트를 결정하는 단계는,상기 클러스터링과 연관된 센트로이드를 기초로, 상기 일부 클라이언트 중 상기 센트로이드까지의 거리가 가장짧은 하나의 클라이언트를 상기 리더 클라이언트로 결정하는 단계를 포함하는, 적어도 하나의 프로세서에 의해 수행되는 디바이스 클러스터링을 이용한 연합 학습 방법."}
{"patent_id": "10-2023-0030073", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2024-0136731-3-제1항에 있어서,상기 클러스터링은 K-means 클러스터링을 포함하는, 적어도 하나의 프로세서에 의해 수행되는 디바이스 클러스터링을 이용한 연합 학습 방법."}
{"patent_id": "10-2023-0030073", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 일부 데이터를 이용하여 상기 대상 클라이언트의 컴퓨팅 리소스 양을 기초로 가중치를 산출하는 단계; 및상기 가중치 및 상기 대상 클라이언트의 모델의 적어도 하나의 파라미터를 이용하여 글로벌 모델을 생성하는 단계를 포함하는, 적어도 하나의 프로세서에 의해 수행되는 디바이스 클러스터링을 이용한 연합 학습 방법."}
{"patent_id": "10-2023-0030073", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 내지 6항 중 어느 한 항에 따른 디바이스 클러스터링을 이용한 연합 학습 방법을 실행시키도록 컴퓨터로판독 가능한 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0030073", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "통신 모듈;상기 통신 모듈을 통해 외부 장치와 데이터를 송수신하도록 구성된 적어도 하나의 프로세서; 및상기 데이터 중 적어도 일부를 저장하도록 구성된 메모리를 포함하고,상기 적어도 하나의 프로세서는,복수의 클라이언트에 대하여 클러스터링을 수행한 결과, 일부 클라이언트를 포함하는 임의의 클라이언트 그룹을획득하는 것,상기 클러스터링과 연관된 센트로이드(centroid)를 기초로, 상기 일부 클라이언트 중 하나의 클라이언트를 리더클라이언트를 결정하는 것 - 상기 리더 클라이언트는 상기 일부 클라이언트 각각으로부터 사전 학습된 모델의적어도 하나의 파라미터와 연관된 데이터를 수신함 -,상기 사전 학습된 모델의 컴퓨팅 리소스 양 및 상기 사전 학습된 모델의 트레이닝 로스(training loss)를 기초로, 상기 일부 클라이언트 중 적어도 하나의 클라이언트를 대상 클라이언트로 결정하는 것과상기 리더 클라이언트로부터 상기 대상 클라이언트의 모델의 적어도 하나의 파라미터와 연관된 일부 데이터를수신하는 것을 실행하도록 구성된 명령어들을 포함하는, 디바이스 클러스터링을 이용한 연합 학습 장치."}
{"patent_id": "10-2023-0030073", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 실시예에 따른 디바이스 클러스터링을 이용한 연합 학습 방법은 복수의 클라이언트에 대하여 클러 스터링을 수행한 결과, 일부 클라이언트를 포함하는 임의의 클라이언트 그룹을 획득하는 단계, 클러스터링과 연 관된 센트로이드(centroid)를 기초로, 일부 클라이언트 중 하나의 클라이언트를 리더 클라이언트를 결정하는 단 계 - 리더 클라이언트는 일부 클라이언트 각각으로부터 사전 학습된 모델의 적어도 하나의 파라미터와 연관된 데 이터를 수신함 -, 사전 학습된 모델의 컴퓨팅 리소스 양 및 사전 학습된 모델의 트레이닝-로스(training loss)를 기초로, 일부 클라이언트 중 적어도 하나의 클라이언트를 대상 클라이언트로 결정하는 단계 및 리더 클라이언트 로부터 대상 클라이언트의 모델의 적어도 하나의 파라미터와 연관된 일부 데이터를 수신하는 단계를 포함하고, 일부 데이터는 데이터에 포함 될 수 있다."}
{"patent_id": "10-2023-0030073", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 디바이스 클러스터링을 이용한 연합 학습 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0030073", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "빅데이터의 급속한 발전으로 인공지능(AI)의 활용이 크게 늘었다. International Data Corporation은 사물 인터 넷(IoT) 장치를 통해 생성되는 데이터의 양이 2025년에 79.4ZB에 이를 것이라고 예측했다. 또한 이는 전 세계적으로 IoT 및 모바일 장치의 용량을 초과할 것으로 예상된다. 장치에서 생성되는 대부분의 데이터는 로컬 또는 원격 클라우드 서버에서 처리된다."}
{"patent_id": "10-2023-0030073", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 배경 기술을 기초로 디바이스 클러스터링을 통해 클라이언트를 그룹핑하고, 클라이언트 그룹 내에서 연합 학습에 참여할 일부 클라이언트를 선별하는 방식에 따르는 연합 학습 방법 및 장치가 제공된다."}
{"patent_id": "10-2023-0030073", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따른 디바이스 클러스터링을 이용한 연합 학습 방법은 복수의 클라이언트에 대하여 클러 스터링을 수행한 결과, 일부 클라이언트를 포함하는 임의의 클라이언트 그룹을 획득하는 단계, 클러스터링과 연 관된 센트로이드(centroid)를 기초로, 일부 클라이언트 중 하나의 클라이언트를 리더 클라이언트를 결정하는 단 계 - 리더 클라이언트는 일부 클라이언트 각각으로부터 사전 학습된 모델의 적어도 하나의 파라미터와 연관된 데이터를 수신함 -, 사전 학습된 모델의 컴퓨팅 리소스 양 및 사전 학습된 모델의 트레이닝-로스(training loss)를 기초로, 일부 클라이언트 중 적어도 하나의 클라이언트를 대상 클라이언트로 결정하는 단계 및 리더 클 라이언트로부터 대상 클라이언트의 모델의 적어도 하나의 파라미터와 연관된 일부 데이터를 수신하는 단계를 포 함하고, 일부 데이터는 데이터에 포함 될 수 있다. 일 실시예에 따르면, 대상 클라이언트와 연관된 제1 트레이닝 로스는 일부 클라이언트 중 대상 클라이언트가 아 닌 임의의 클라이언트와 연관된 제2 트레이닝 로스보다 크도록 구성될 수 있다. 일 실시예에 따르면, 클러스터링은 복수의 클라이언트 각각 사이의 통신 거리를 기초로 수행될 수 있다. 일 실시예에 따르면, 클러스터링과 연관된 센트로이드를 기초로, 일부 클라이언트 중 하나의 클라이언트를 리더 클라이언트를 결정하는 단계는, 클러스터링과 연관된 센트로이드를 기초로, 일부 클라이언트 중 센트로이드까지 의 거리가 가장 짧은 하나의 클라이언트를 리더 클라이언트로 결정하는 단계를 포함할 수 있다. 일 실시예에 따르면, 클러스터링은 K-means 클러스터링을 포함할 수 있다. 일 실시예에 따르면, 일부 데이터를 이용하여 대상 클라이언트의 컴퓨팅 리소스 양을 기초로 가중치를 산출하는 단계 및 가중치 및 대상 클라이언트의 모델의 적어도 하나의 파라미터를 이용하여 글로벌 모델을 생성하는 단계 를 포함할 수 있다. 본 개시의 다른 실시예에 따르면, 디바이스 클러스터링을 이용한 연합 학습 방법을 실행시키도록 컴퓨터로 판 독 가능한 기록매체에 기록된 컴퓨터 프로그램이 제공될 수 있다. 본 개시의 또 다른 실시예에 따르면, 디바이스 클러스터링을 이용한 연합 학습 장치는 통신 모듈, 통신 모듈을 통해 외부 장치와 데이터를 송수신하도록 구성된 적어도 하나의 프로세서 및 데이터 중 적어도 일부를 저장하도 록 구성된 메모리를 포함하고, 적어도 하나의 프로세서는, 복수의 클라이언트에 대하여 클러스터링을 수행한 결 과, 일부 클라이언트를 포함하는 임의의 클라이언트 그룹을 획득하는 것, 클러스터링과 연관된 센트로이드 (centroid)를 기초로, 일부 클라이언트 중 하나의 클라이언트를 리더 클라이언트를 결정하는 것 - 리더 클라이 언트는 일부 클라이언트 각각으로부터 사전 학습된 모델의 적어도 하나의 파라미터와 연관된 데이터를 수신함 -, 사전 학습된 모델의 컴퓨팅 리소스 양 및 사전 학습된 모델의 트레이닝 로스(training loss)를 기초로, 일부 클라이언트 중 적어도 하나의 클라이언트를 대상 클라이언트로 결정하는 것과 리더 클라이언트로부터 대상 클라 이언트의 모델의 적어도 하나의 파라미터와 연관된 일부 데이터를 수신하는 것을 실행하도록 구성된 명령어들을 포함할 수 있다."}
{"patent_id": "10-2023-0030073", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 일부 실시예에 따르면, 근거리 무선 통신으로 인한 통신 자원 소모 및 네트워크 지연을 효과적으로 줄이고 시스템의 커버리지를 증가시킬 수 있다."}
{"patent_id": "10-2023-0030073", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 실시를 위한 구체적인 내용을 첨부된 도면을 참조하여 상세히 설명한다. 다만, 이하의 설명에 서는 본 개시의 요지를 불필요하게 흐릴 우려가 있는 경우, 널리 알려진 기능이나 구성에 관한 구체적 설명은 생략하기로 한다. 첨부된 도면에서, 동일하거나 대응하는 구성요소에는 동일한 참조부호가 부여되어 있다. 또한, 이하의 실시예들 의 설명에 있어서, 동일하거나 대응되는 구성요소를 중복하여 기술하는 것이 생략될 수 있다. 그러나 구성요소 에 관한 기술이 생략되어도, 그러한 구성요소가 어떤 실시예에 포함되지 않는 것으로 의도되지는 않는다. 개시된 실시예의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 후술되어 있는 실시예 들을 참조하면 명확해질 것이다. 그러나 본 개시는 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 개시가 완전하도록 하고, 본 개시가 통상의 기술 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것일 뿐이다. 본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 개시된 실시예에 대해 구체적으로 설명하기로 한다. 본 명세서에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 관련 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상 세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가 지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서에서의 단수의 표현은 문맥상 명백하게 단수인 것으로 특정하지 않는 한, 복수의 표현을 포함한다. 또 한, 복수의 표현은 문맥상 명백하게 복수인 것으로 특정하지 않는 한, 단수의 표현을 포함한다. 명세서 전체에 서 어떤 부분이 어떤 구성요소를 '포함'한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 도 1은 본 개시의 일 실시예에 따른 디바이스 클러스터링을 이용한 연합 학습 시스템의 예시를 나타낸다. 도시된 바와 같이, 시스템은 서버 및 복수의 클라이언트(120 내지 140)를 포함할 수 있다. 여기서, 서버는 복수의 클라이언트(120 내지 140) 중 적어도 일부의 클라이언트로부터 파라미터를 나타내는 데이터 를 수신하고, 수신된 데이터를 이용하여 글로벌 모델을 생성하게 된다. 도 1에서는 서버의 글로벌 모델 생성 과정을 설명하기 위하여 제1 클라이언트와 연관된 제1 클라이언트 그룹의 동작이 상세히 후술된다. 즉, 제1 클라이언트 그룹에 대한 서버의 동작은 제7 클라이언트와 연관된 제2 클라이언트 그룹 및 제 8 클라이언트와 연관된 제3 클라이언트 그룹에도 동일하게 적용될 수 있다. 한편, 본 개시에서는 설명의 편의를 위해 '파라미터를 나타내는 데이터'를 '파라미터'로 지칭한다. 복수의 클라이언트(120 내지 140) 각각은 사전에 각각의 학습 데이터셋을 이용하여 글로벌 모델의 기초가 되는 기계 학습 모델을 생성할 수 있다. 예를 들어, 제2 클라이언트(122_1)는 제1 세트의 학습 데이터셋을 이용하여 제1 파라미터를 갖는 기계 학습 모델을 생성하고, 제3 클라이언트(122_2)는 제2 세트의 학습 데이터셋을 이용하 여 제2 파라미터를 갖는 기계 학습 모델을 생성할 수 있다. 이와 같이 사전 생성된 기계 학습 모델의 파라미터 는, 이후 서버에 의해 글로벌 모델 생성 시 이용될 수 있다.서버는 복수의 클라이언트(120 내지 140) 중 파라미터를 획득할 일부 클라이언트를 결정할 수 있다. 구체 적으로, 서버는 복수의 클라이언트(120 내지 140)를 복수의 클라이언트 그룹으로 구분하고, 복수의 클라이 언트 그룹 각각에 서버와 직접 통신할 하나의 리더 클라이언트를 결정할 수 있다. 그리고 나서, 리더 클 라이언트에 의해 클라이언트 그룹 내에서 수집된 나머지 클라이언트(들)의 파라미터 중 서버에 의해 결정 된 대상 클라이언트 각각의 파라미터를 리더 클라이언트(도 1에서, 제1 클라이언트, 제7 클라이언트 및 제8 클라이언트)를 통해 획득할 수 있다. 여기서, 대상 클라이언트로부터 획득된 파라미터는 글로벌 모델의 생성에 이용할 수 있다. 이하에서는, 상술한 서버의 동작이 보다 상세히 후술된다. 서버는 복수의 클라이언트(120 내지 140)에 대하여 클러스터링을 수행할 수 있다. 예를 들어, 서버 는 복수의 클라이언트(120 내지 140)에 대하여 클러스터링을 수행한 결과 제1 클라이언트 내지 제6 클라이 언트(122_5)를 포함하는 제1 클라이언트 그룹을 생성할 수 있다. 이 때, 제1 클라이언트 그룹에 포함되는 제1 클라이언트 내지 제6 클라이언트(122_5) 각각 사이의 통신 거리는, 제1 클라이언트 내지 제6 클라이 언트(122_5) 중 임의의 클라이언트부터 제7 클라이언트까지의 통신 거리보다 짧도록 구성될 수 있다. 이 와 유사하게, 제1 클라이언트 그룹에 포함되는 제1 클라이언트 내지 제6 클라이언트(122_5) 각각 사이의 통신 거리는, 제1 클라이언트 내지 제6 클라이언트(122_5) 중 임의의 클라이언트부터 제8 클라이언트(14 0)까지의 통신 거리보다 짧도록 구성될 수 있다. 서버는 클러스터 그룹 내에서 하나의 리더 클라이언트를 결정할 수 있다. 구체적으로, 서버는 클러 스터링과 연관된 센트로이드(centroid)를 기초로 임의의 클러스터 그룹 내에서 하나의 리더 클라이언트를 결정 할 수 있다. 예를 들어, 서버는 제1 클러스터 그룹을 클러스터링하는 데에 기초가 된 센트로이드로부터 가장 가까운 거리에 위치한 제1 클라이언트를 리더 클라이언트로 결정할 수 있다. 한편, 클러스터링과 연 관된 센트로이드는 K-means 클러스터링을 통해 결정된 센트로이드를 지칭할 수 있다. 서버는 리더 클라이언트를 통해 나머지 일부 클라이언트(들) 중 적어도 일부의 파라미터를 획득하고, 획득 된 파라미터를 이용하여 연합 학습을 통해 글로벌 모델을 생성할 수 있다. 이를 위해, 우선 클라이언트 그룹 내 리더 클라이언트가 나머지 일부 클라이언트(들)의 파라미터를 획득할 수 있다. 예를 들어, 제1 클라이언트 는 일부 클라이언트(122_1 내지 122_5)들 각각의 사전 학습된 모델의 파라미터를 획득할 수 있다. 또한, 서버는 나머지 일부 클라이언트(들) 중 리더 클라이언트를 통해 파라미터를 전송할 하나 이상의 대상 클라 이언트를 결정할 수 있다. 예를 들어, 서버는 일부 클라이언트(122_1 내지 122_5) 각각의 사전 학습된 모 델의 컴퓨팅 리소스 양 및/또는 사전 학습된 모델의 트레이닝-로스(training loss)를 기초로 제3 클라이언트 (122_2) 및 제6 클라이언트(122_5)를 대상 클라이언트로 결정할 수 있다. 이에 응답하여, 리더 클라이언트인 제1 클라이언트는 제3 클라이언트(122_2)의 모델의 파라미터 및 제6 클라이언트(122_5)의 모델의 파라미터 를 서버로 전송할 수 있다. 서버는 리더 클라이언트로부터 수신한 데이터를 기초로 글로벌 모델을 생성(또는, 업데이트)할 수 있다. 구체적으로, 서버는 획득된 파라미터를 기초로 산출된 로스 함수를 갖는 글로벌 모델을 생성할 수 있다. 한편, 서버는 리더 클라이언트로부터 학습 라운드 마다 적어도 일부 상이한 클라이언트(들)의 파라미터를 획득할 수 있다. 예를 들어, 서버가 글로벌 모델을 트레이닝 하는 제1 라운드에서, 서버는 리더 클 라이언트로부터 제3 클라이언트(122_2)의 모델의 파라미터 및 제6 클라이언트(122_5)의 모델의 파라미터를 수신 할 수 있다. 그리고 나서, 서버는 제1 라운드 이후 제2 라운드에서, 제3 클라이언트(122_2)의 모델의 파 라미터 및 제4 클라이언트(122_3)의 모델의 파라미터를 수신할 수 있다. 즉, 서버는 학습의 매 라운드 마 다 일부 클라이언트(122_1 내지 122_5) 각각의 컴퓨팅 리소스 양 및/또는 트레이닝-로스를 획득하고, 획득된 컴 퓨팅 리소스 양 및/또는 트레이닝-로스를 기초로 파라미터를 획득할 대상 클라이언트를 업데이트할 수 있다. 도 2는 본 개시의 일 실시예에 따른 사용자 단말 및 정보 처리 시스템의 내부 구성을 나타내는 블록 도이다. 여기서, 사용자 단말은 본 개시에서 '클라이언트'에 대응될 수 있다. 또한, 정보 처리 시스템 은 본 개시에서 사용자 단말은 기계 학습 모델링 프로그램 등이 실행 가능하고 유/무선 통신이 가능 한 임의의 컴퓨팅 장치를 지칭할 수 있으며, 도시된 바와 같이, 사용자 단말은 메모리, 프로세서 , 통신 모듈 및 입출력 인터페이스를 포함할 수 있다. 이와 유사하게, 정보 처리 시스템 은 메모리, 프로세서, 통신 모듈 및 입출력 인터페이스를 포함할 수 있다. 도 2에 도시된 바와 같이, 사용자 단말 및 정보 처리 시스템은 각각의 통신 모듈(216, 236)을 이용하여 네트워크 를 통해 정보 및/또는 데이터를 통신할 수 있도록 구성될 수 있다. 또한, 입출력 장치는 입출력 인 터페이스를 통해 사용자 단말에 정보 및/또는 데이터(예를 들어, 사용자 정보, 목표 정보 등)를 입력하거나 사용자 단말로부터 생성된 정보 및/또는 데이터를 출력하도록 구성될 수 있다. 메모리(212, 232)는 비-일시적인 임의의 컴퓨터 판독 가능한 기록매체를 포함할 수 있다. 일 실시예에 따르면, 메모리(212, 232)는 RAM(random access memory), ROM(read only memory), 디스크 드라이브, SSD(solid state drive), 플래시 메모리(flash memory) 등과 같은 비소멸성 대용량 저장 장치(permanent mass storage device) 를 포함할 수 있다. 다른 예로서, ROM, SSD, 플래시 메모리, 디스크 드라이브 등과 같은 비소멸성 대용량 저장 장치는 메모리와는 구분되는 별도의 영구 저장 장치로서 사용자 단말 또는 정보 처리 시스템에 포함 될 수 있다. 또한, 메모리(212, 232)에는 운영 체제와 적어도 하나의 프로그램 코드(예를 들어, 사용자 단말 에 설치되어 기계 학습 모델링 프로그램을 실행하기 위한 코드)가 저장될 수 있다. 이러한 소프트웨어 구성요소들은 메모리(212, 232)와는 별도의 컴퓨터에서 판독 가능한 기록매체로부터 로딩될 수 있다. 이러한 별도의 컴퓨터에서 판독가능한 기록매체는 이러한 사용자 단말 및 정보 처리 시스템 에 직접 연결가능한 기록 매체를 포함할 수 있는데, 예를 들어, 플로피 드라이브, 디스크, 테이프, DVD/CD-ROM 드라이브, 메모리 카드 등의 컴퓨터에서 판독 가능한 기록매체를 포함할 수 있다. 다른 예로서, 소 프트웨어 구성요소들은 컴퓨터에서 판독 가능한 기록매체가 아닌 통신 모듈(216, 236)을 통해 메모리(212, 23 2)에 로딩될 수도 있다. 예를 들어, 적어도 하나의 프로그램은 개발자들 또는 어플리케이션의 설치 파일을 배 포하는 파일 배포 시스템이 네트워크를 통해 제공하는 파일들에 의해 설치되는 컴퓨터 프로그램에 기반하 여 메모리(212, 232)에 로딩될 수 있다. 프로세서(214, 234)는 기본적인 산술, 로직 및 입출력 연산을 수행함으로써, 컴퓨터 프로그램의 명령을 처리하 도록 구성될 수 있다. 명령은 메모리(212, 232) 또는 통신 모듈(216, 236)에 의해 프로세서(214, 234)로 제공 될 수 있다. 예를 들어, 프로세서(214, 234)는 메모리(212, 232)와 같은 기록 장치에 저장된 프로그램 코드에 따라 수신되는 명령을 실행하도록 구성될 수 있다. 통신 모듈(216, 236)은 네트워크를 통해 사용자 단말과 정보 처리 시스템이 서로 통신하기 위한 구성 또는 기능을 제공할 수 있으며, 사용자 단말 및/또는 정보 처리 시스템이 다른 사용자 단말 또 는 다른 시스템(일례로 별도의 클라우드 시스템 등)과 통신하기 위한 구성 또는 기능을 제공할 수 있다. 일례 로, 사용자 단말의 프로세서가 메모리 등과 같은 기록 장치에 저장된 프로그램 코드에 따라 생 성한 데이터(예를 들어, 사전 학습된 모델, 모델의 파라미터, 컴퓨팅 리소스 양, 트레이닝 로스 등의 정보를 포 함하는 데이터)는 통신 모듈의 제어에 따라 네트워크를 통해 정보 처리 시스템로 전달될 수 있 다. 역으로, 정보 처리 시스템의 프로세서의 제어에 따라 제공되는 제어 신호나 명령(예를 들어, 클 라이언트의 위치 정보 요청, 데이터 요청)이 통신 모듈과 네트워크를 거쳐 사용자 단말의 통신 모듈을 통해 사용자 단말에 수신될 수 있다. 입출력 인터페이스는 입출력 장치와의 상호 작용을 위한 수단일 수 있다. 구체적으로, 입출력 장치 는 오디오 센서 및/또는 이미지 센서를 포함한 카메라, 키보드, 마이크로폰, 마우스 등의 입력 장치를 포 함할 수 있다. 추가적으로, 입출력 장치는 디스플레이, 스피커, 햅틱 피드백 디바이스(haptic feedback device) 등과 같은 출력 장치를 포함할 수 있다. 다른 예로, 입출력 인터페이스는 터치 스크린 등과 같이 입력과 출력을 수행하기 위한 구성 또는 기능이 하나로 통합된 장치와의 인터페이스를 위한 수단일 수 있다. 도 2에서는 입출력 장치가 사용자 단말에 포함되지 않도록 도시되어 있으나, 이에 한정되지 않으며, 사용자 단말과 하나의 장치로 구성될 수 있다. 또한, 정보 처리 시스템의 입출력 인터페이스는 정보 처리 시스템과 연결되거나 정보 처리 시스템이 포함할 수 있는 입력 또는 출력을 위한 장치(미 도시)와의 인터페이스를 위한 수단일 수 있다. 도 2에서는 입출력 인터페이스(218, 238)가 프로세서(214, 23 4)와 별도로 구성된 요소로서 도시되었으나, 이에 한정되지 않으며, 입출력 인터페이스(218, 238)가 프로세서 (214, 234)에 포함되도록 구성될 수 있다. 사용자 단말 및 정보 처리 시스템은 도 2의 구성요소들보다 더 많은 구성요소들을 포함할 수 있다. 그러나, 대부분의 종래기술적 구성요소들을 명확하게 도시할 필요성은 없다. 일 실시예에 따르면, 사용자 단말 은 상술된 입출력 장치 중 적어도 일부를 포함하도록 구현될 수 있다. 또한, 사용자 단말은 트 랜시버(transceiver), GPS(Global Positioning system) 모듈, 카메라, 각종 센서, 데이터 베이스 등과 같은 다 른 구성요소들을 더 포함할 수 있다. 예를 들어, 사용자 단말이 스마트폰인 경우, 일반적으로 스마트폰이 포함하고 있는 구성요소를 포함할 수 있으며, 예를 들어, 가속도 센서, 자이로 센서, 마이크 모듈, 카메라 모듈, 각종 물리적인 버튼, 터치 패널을 이용한 버튼, 입출력 포트 등의 다양한 구성요소들이 사용자 단말(21 0)에 더 포함되도록 구현될 수 있다.사용자 단말의 프로세서는 연합 학습 모델링 기능을 포함하는 사용자 단말의 제어를 위한 프로 그램을 동작하도록 구성될 수 있다. 이 때, 해당 프로그램과 연관된 코드가 사용자 단말의 메모리에 로딩될 수 있다. 프로그램이 동작되는 동안에, 사용자 단말의 프로세서는 입출력 장치로부터 제공된 정보 및/또는 데이터를 입출력 인터페이스를 통해 수신하거나, 통신 모듈을 통해 정보 처리 시스템로부터 정보 및/또는 데이터를 수신할 수 있으며, 수신된 정보 및/또는 데이터를 처리하여 메모리 에 저장할 수 있다. 또한, 이러한 정보 및/또는 데이터는 통신 모듈(216, 236)을 통해 정보 처리 시스템 에 제공될 수 있다. 정보 처리 시스템의 프로세서는 복수의 사용자 단말 및/또는 복수의 외부 시스템으로부터 수신된 정 보 및/또는 데이터를 관리, 처리 및/또는 저장하도록 구성될 수 있다. 일 실시예에 따르면, 프로세서는 사용자 단말로부터 수신된 사용자 입력 및 해당 사용자 입력에 따라 가공된 데이터를 관리, 처리 및/또는 저장할 수 있다. 추가적으로 또는 대안적으로, 프로세서는 네트워크와 연결된 별도의 클라우드 시스 템, 데이터 베이스 등으로부터 사용자 단말의 인공지능 모델의 트레이닝 및/또는 모델링을 실행하기 위한 프로그램 등을 저장 및/또는 업데이트하도록 구성될 수 있다. 도 3는 본 개시의 일 실시예에 따른 인공신경망 모델을 나타내는 예시도이다. 인공신경망 모델은, 기계학습 모델의 일 예로서, 기계학습(Machine Learning) 기술과 인지과학에서, 생물학적 신경망의 구조에 기초 하여 구현된 통계학적 학습 알고리즘 또는 그 알고리즘을 실행하는 구조이다. 인공신경망 모델은, 생물학적 신경망에서와 같이 시냅스의 결합으로 네트워크를 형성한 인공 뉴런인 노드 (Node)들이 시냅스의 가중치를 반복적으로 조정하여, 특정 입력에 대응한 올바른 출력과 추론된 출력 사이의 오 차가 감소되도록 학습함으로써, 문제 해결 능력을 가지는 기계학습 모델을 나타낼 수 있다. 예를 들어, 인공신 경망 모델은 기계 학습, 딥러닝 등의 인공지능 학습법에 사용되는 임의의 확률 모델, 뉴럴 네트워크 모델 등을 포함할 수 있으며, 상술된 딥 뉴럴 네트워크와 연관된 모델을 포함할 수 있다. 인공신경망 모델은 다층의 노드들과 이들 사이의 연결로 구성된 다층 퍼셉트론(MLP: multilayer perceptron)으로 구현된다. 본 실시예에 따른 인공신경망 모델은 MLP를 포함하는 다양한 인공신경망 모델 구조들 중의 하나를 이용하여 구현될 수 있다. 도 3에 도시된 바와 같이, 인공신경망 모델은, 외부로부터 입력 신호 또는 데이터를 수신하는 입력층, 입력 데이터에 대응한 출력 신호 또는 데이터를 출 력하는 출력층, 입력층과 출력층 사이에 위치하며 입력층으로부터 신호를 받아 특성을 추 출하여 출력층으로 전달하는 n개(여기서, n은 양의 정수)의 은닉층(330_1 내지 930_n)으로 구성된다. 여 기서, 출력층은 은닉층(330_1 내지 930_n)으로부터 신호를 받아 외부로 출력한다. 인공신경망 모델의 학습 방법에는, 교사 신호(정답)의 입력에 의해서 문제의 해결에 최적화되도록 학습하 는 지도 학습(Supervised Learning) 방법과, 교사 신호를 필요로 하지 않는 비지도 학습(Unsupervised Learning) 방법이 있다. 예를 들어, 딥 뉴럴 네트워크와 연관된 인공신경망 모델은 학습 데이터를 이용하 여 지도 학습 및/또는 비지도 학습된 모델일 수 있다. 이렇게 학습된 인공신경망 모델은 컴퓨팅 장치의 메모리(미도시)에 저장될 수 있으며, 컴퓨팅 장치는 인공신경망 모델에 대한 양자화를 수행할 수 있다. 예를 들어, 컴퓨팅 장치는 32 비트 부동소수점(32-bit floating point)으로 학습된 인공신경망 모델의 가 중치(weight), 출력값 및/또는 입력값을 이산화 값(예를 들어, 정수)으로 양자화할 수 있다. 일 실시예에 따르면, 컴퓨팅 장치는 인공신경망 모델의 학습에 사용된 학습 데이터를 사용하지 않고도, 인 공신경망 모델에 대한 양자화를 수행할 수 있다. 예를 들어, 인공신경망 모델은 복수의 정규화 레이 어를 포함할 수 있으며, 양자화는 각 정규화 레이어의 후속 레이어의 입력값들에 대해 수행될 수 있다. 이 경 우, 컴퓨팅 장치는 정규화 레이어의 통계적 특성(정규화 레이어의 기준화 인수)을 이용하여 출력값(activation output) 등에 대한 양자화를 수행할 수 있다. 다시 말해, 컴퓨팅 장치는 인공신경망 모델의 학습 시 사용 된 학습 데이터의 적어도 일부 없이, 정규화 레이어로부터 추출된 통계 정보로부터 정규화 레이어의 복수의 출 력값들과 연관된 클리핑 값을 결정하고, 결정된 클리핑 값 및 인공신경망 모델에서의 추론 시 사용되는 데 이터의 비트 수를 이용하여, 후속 레이어의 입력값들과 연관된 이산화 간격을 결정할 수 있다. 도 4는 본 개시의 일 실시예에 따른 디바이스 클러스터링을 이용한 연합 학습 방법의 흐름도이다. 방법 은 클라이언트, 사용자 단말 등과 같은 컴퓨팅 장치의 적어도 하나의 프로세서에 의해 수행될 수 있다. 한편, 도시된 바와 같이 방법은 복수의 클라이언트에 대하여 클러스터링을 수행한 결과, 일부 클라이언트 를 포함하는 임의의 클라이언트 그룹을 획득하는 단계(S410)로 개시될 수 있다. 이 경우, 클러스터링은 복수의 클라이언트 각각 사이의 통신 거리를 기초로 수행될 수 있다. 또한, 클러스터링은 종래의 모든 클러스터링 방법이 이용될 수 있으며, 그 예로서 K-means 클러스터링 방법이 이용될 수 있다. 프로세서는 클러스터링과 연관된 센트로이드(centroid)를 기초로, 일부 클라이언트 중 하나의 클라이언트를 리 더 클라이언트를 결정할 수 있다(S420). 예를 들어, 프로세서는 클러스터링과 연관된 센트로이드를 기초로, 일 부 클라이언트 중 센트로이드까지의 거리가 가장 짧은 하나의 클라이언트를 리더 클라이언트로 결정할 수 있다. 이 경우, 리더 클라이언트는 일부 클라이언트 각각으로부터 사전 학습된 모델의 적어도 하나의 파라미터와 연관 된 데이터를 수신할 수 있다. 그리고 나서, 프로세서는 사전 학습된 모델의 컴퓨팅 리소스 양 및 사전 학습된 모델의 트레이닝-로스(training loss)를 기초로, 일부 클라이언트 중 적어도 하나의 클라이언트를 대상 클라이 언트로 결정할 수 있다(S430). 이 경우, 대상 클라이언트와 연관된 제1 트레이닝 로스는 일부 클라이언트 중 대상 클라이언트가 아닌 임의의 클라이언트와 연관된 제2 트레이닝 로스보다 크도록 구성될 수 있다. 프로세서는 리더 클라이언트로부터 대상 클라이언트의 모델의 적어도 하나의 파라미터와 연관된 일부 데이터를 수신할 수 있다(S440). 이 경우, 일부 데이터는 상술한 적어도 하나의 파라미터와 연관된 데이터에 포함 될 수 있다. 추가적으로, 프로세서는 일부 데이터를 이용하여 대상 클라이언트의 컴퓨팅 리소스 양을 기초로 가중치 를 산출하고, 가중치 및 대상 클라이언트의 모델의 적어도 하나의 파라미터를 이용하여 글로벌 모델을 생성할 수 있다. 본 개시의 다른 실시예에 따르면, 디바이스 클러스터링을 이용한 연합 학습 방법을 실행시키도록 컴퓨터로 판 독 가능한 기록매체에 기록된 컴퓨터 프로그램이 제공될 수 있다. 본 개시의 또 다른 실시예에 따르면, 디바이스 클러스터링을 이용한 연합 학습 장치는 통신 모듈, 통신 모듈을 통해 외부 장치와 데이터를 송수신하도록 구성된 적어도 하나의 프로세서 및 데이터 중 적어도 일부를 저장하도 록 구성된 메모리를 포함하고, 적어도 하나의 프로세서는, 복수의 클라이언트에 대하여 클러스터링을 수행한 결 과, 일부 클라이언트를 포함하는 임의의 클라이언트 그룹을 획득하는 것, 클러스터링과 연관된 센트로이드 (centroid)를 기초로, 일부 클라이언트 중 하나의 클라이언트를 리더 클라이언트를 결정하는 것 - 리더 클라이 언트는 일부 클라이언트 각각으로부터 사전 학습된 모델의 적어도 하나의 파라미터와 연관된 데이터를 수신함 -, 사전 학습된 모델의 컴퓨팅 리소스 양 및 사전 학습된 모델의 트레이닝 로스(training loss)를 기초로, 일부 클라이언트 중 적어도 하나의 클라이언트를 대상 클라이언트로 결정하는 것과 리더 클라이언트로부터 대상 클라 이언트의 모델의 적어도 하나의 파라미터와 연관된 일부 데이터를 수신하는 것을 실행하도록 구성된 명령어들을 포함할 수 있다. 도 5는 본 개시의 일 실시예에 따른 연합 학습 방법의 모델 수렴 및 리소스 소비 절감 효과를 나타내는 그래프 이다. 구체적으로, (a)는 학습 데이터셋으로 MNIST가, (b)는 학습 데이터셋으로 FashionMNIST가 사용된 예이다. 도시된 바와 같이, 본 개시의 연합 학습 방법에 따른 편향된(Biased) 클라이언트 선택 방법과 및 종래 방법에 따른 비편향(Unbiased) 클라이언트 선택 방법 모두 모델의 정확도가 유사하게 수렴하는 것을 확인할 수 있다. 특히, 본 개시의 연합 학습 방법과 같이 편향된 클라이언트 선택은 더 빠르게 일정 수치의 정확도에 수 렴하며, 정확도 또한 종래 방법보다 높게 나타나는 것을 확인할 수 있다. 이는 본 개시의 연합 학습 방법과 같 이 편향된 고객 선택 방법을 채택할 때, 적은 수의 클라이언트로도 충분한 결과를 낼 수 있음을 의미한다. 본 개시의 앞선 설명은 통상의 기술자들이 본 개시를 행하거나 이용하는 것을 가능하게 하기 위해 제공된다. 본 개시의 다양한 수정예들이 통상의 기술자들에게 쉽게 자명할 것이고, 본원에 정의된 일반적인 원리들은 본 개시의 취지 또는 범위를 벗어나지 않으면서 다양한 변형예들에 적용될 수도 있다. 따라서, 본 개시는 본원에 설명된 예들에 제한되도록 의도된 것이 아니고, 본원에 개시된 원리들 및 신규한 특징들과 일관되는 최광의의 범위가 부여되도록 의도된다."}
{"patent_id": "10-2023-0030073", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "본 명세서에서는 본 개시가 일부 실시예들과 관련하여 설명되었지만, 본 발명이 속하는 기술분야의 통상의 기술 자가 이해할 수 있는 본 개시의 범위를 벗어나지 않는 범위에서 다양한 변형 및 변경이 이루어질 수 있다는 점 을 알아야 할 것이다. 또한, 그러한 변형 및 변경은 본 명세서에서 첨부된 특허청구의 범위 내에 속하는 것으 로 생각되어야 한다."}
{"patent_id": "10-2023-0030073", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 디바이스 클러스터링을 이용한 연합 학습 시스템의 예시를 나타낸다. 도 2는 본 개시의 일 실시예에 따른 사용자 단말 및 정보 처리 시스템의 내부 구성을 나타내는 블록도이다. 도 3는 본 개시의 일 실시예에 따른 인공신경망 모델을 나타내는 예시도이다. 도 4는 본 개시의 일 실시예에 따른 디바이스 클러스터링을 이용한 연합 학습 방법의 흐름도이다. 도 5는 본 개시의 일 실시예에 따른 연합 학습 방법의 모델 수렴 및 리소스 소비 절감 효과를 나타내는 그래프 이다."}
