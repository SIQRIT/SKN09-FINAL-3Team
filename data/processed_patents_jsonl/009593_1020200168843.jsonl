{"patent_id": "10-2020-0168843", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0042845", "출원번호": "10-2020-0168843", "발명의 명칭": "외국어 문장 빈칸 추론 문제 자동 생성 방법 및 시스템", "출원인": "주식회사 렉스퍼", "발명자": "이형종"}}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "서버에 의해 수행되는 외국어 문장 빈칸 추론 문제 자동 생성 방법에 있어서,상기 서버가 하나 이상의 외국어 문장을 입력받는 단계;상기 서버가 상기 입력된 외국어 문장 중 빈칸으로 설정하고자 하는 범위를 지정받는 단계;상기 서버가 오답 선지 생성을 위한 설정정보를 지정받는 단계; 및상기 서버가 미리 설정된 인공지능 기반의 문장 생성 알고리즘을 이용하여 상기 빈칸 범위 및 설정정보에 따른빈칸 추론 문제를 생성하는 단계를 포함하고,상기 빈칸 추론 문제를 생성하는 단계는,상기 서버가 상기 입력된 외국어 문장 중 지정된 범위를 빈칸으로 설정하는 단계와,상기 서버가 상기 지정된 범위의 원문을 정답 선지로 생성하는 단계와,상기 서버가 상기 정답 선지에 기초하여 상기 미리 설정된 인공지능 기반의 문장 생성 알고리즘에 따라 복수의오답 선지를 생성하는 단계를 포함하고,상기 복수의 오답 선지를 생성하는 단계는,상기 서버가 상기 입력된 외국어 문장을 단어 기준의 토큰으로 분할하는 단계와,상기 서버가 상기 지정된 범위 중 일부 토큰에 대해 마스킹하는 단계와,상기 서버가 상기 마스킹된 각각의 토큰의 위치에 대체 가능한 복수의 단어를 확률값에 기반하여 예측하는 단계와,상기 서버가 상기 복수의 단어에 대한 샘플링을 수행하여 확률값을 기반으로 각각의 토큰의 위치에 대응하는 하나의 단어를 추출하는 단계와,상기 서버가 상기 추출된 단어를 상기 각각의 마스킹된 위치에 삽입하여 오답 후보 선지를 생성하는 단계를 포함하고,상기 오답 후보 선지를 생성하는 단계는,상기 서버가 상기 지정된 범위와 연결된 첫번째 토큰의 위치에 마스킹하는 단계와,상기 서버가 상기 첫번째 토큰의 위치에 마스킹한 이후, 상기 첫번째 토큰의 해당 위치에서의 원문 단어의 출현확률을 나타내는 제1확률값을 예측하는 단계와,상기 서버가 상기 추출된 단어를 상기 각각의 마스킹된 위치에 삽입한 이후, 상기 첫번째 토큰의 해당 위치에서의 원문 단어의 출현 확률을 나타내는 제2확률값을 예측하는 단계와,상기 서버가 상기 제1확률값 및 제2확률값에 기초하여 상기 오답 후보 선지를 생성하는 단계를 더 포함하는,외국어 문장 빈칸 추론 문제 자동 생성 방법."}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 복수의 오답 선지를 생성하는 단계는,상기 지정된 범위와 동일 또는 상이한 길이를 갖는 토큰 수로 구성된 오답 선지를 생성하거나, 상기 정답 선지와 기 설정된 유사도의 범위를 갖는 오답 선지 및 상기 정답 선지와 기 설정된 유사도의 범위 밖의 오답 선지공개특허 10-2021-0042845-3-중 하나 이상의 오답 선지를 포함하도록 생성하는 것인,외국어 문장 빈칸 추론 문제 자동 생성 방법."}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제1확률값 및 제2확률값에 기초하여 상기 오답 후보 선지를 생성하는 단계는,상기 서버가 상기 제2확률값이 제1확률값 이하인 경우 상기 지정된 범위의 마지막 토큰과 상기 첫번째 토큰의해당 위치 사이에 마스킹된 토큰을 추가하는 단계와,상기 서버가 상기 추가된 위치에서의 마스킹된 토큰에 대하여 확률값을 기반으로 하는 하나의 토큰을 추출하는단계와,상기 서버가 상기 추가된 위치에 대하여 추출된 토큰 삽입 이후, 상기 첫번째 토큰의 해당 위치에서의 원문 단어의 출현 확률을 나타내는 제3확률값을 예측하는 단계와,상기 서버가 상기 제1확률값 및 제3확률값에 기초하여 상기 오답 후보 선지를 생성하는 단계를 포함하는,외국어 문장 빈칸 추론 문제 자동 생성 방법."}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 제1확률값 및 제2확률값에 기초하여 상기 오답 후보 선지를 생성하는 단계는,상기 제2확률값이 제1확률값을 초과하는 경우, 상기 추출된 토큰에 대응하는 단어를 상기 마스킹된 위치에 삽입한 것을 상기 오답 후보 선지로 생성하는 것인,외국어 문장 빈칸 추론 문제 자동 생성 방법."}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "서버에 의해 수행되는 외국어 문장 빈칸 추론 문제 자동 생성 방법에 있어서,상기 서버가 하나 이상의 외국어 문장을 입력받는 단계;상기 서버가 상기 입력된 외국어 문장 중 빈칸으로 설정하고자 하는 범위를 지정받는 단계;상기 서버가 오답 선지 생성을 위한 설정정보를 지정받는 단계; 및상기 서버가 미리 설정된 인공지능 기반의 문장 생성 알고리즘을 이용하여 상기 빈칸 범위 및 설정정보에 따른빈칸 추론 문제를 생성하는 단계를 포함하고,상기 서버가 상기 빈칸 추론 문제를 생성하는 단계는,상기 서버가 상기 입력된 외국어 문장 중 지정된 범위를 빈칸으로 설정하는 단계와,상기 서버가 상기 지정된 범위의 원문을 정답 선지로 생성하는 단계와,상기 서버가 상기 정답 선지에 기초하여 상기 미리 설정된 인공지능 기반의 문장 생성 알고리즘에 따라 복수의오답 선지를 생성하는 단계를 포함하고,상기 서버가 상기 복수의 오답 선지를 생성하는 단계는,상기 서버가 상기 입력된 외국어 문장을 단어 기준의 토큰으로 분할하는 단계와,상기 서버가 상기 지정된 범위 중 임의로 선택된 토큰에 대해 마스킹하는 단계와,상기 서버가 상기 마스킹된 토큰의 위치에 대체 가능한 복수의 단어를 확률값에 기반하여 예측하는 단계와,상기 서버가 상기 복수의 단어에 대한 샘플링을 수행하여 확률값을 기반으로 하는 하나의 토큰을 추출하는 단계와,공개특허 10-2021-0042845-4-상기 서버가 상기 추출된 토큰에 대응하는 단어를 상기 마스킹된 위치에 삽입하여 오답 후보 선지를 생성하는단계를 포함하고,상기 복수의 오답 선지를 생성하는 단계는,상기 서버가 상기 복수의 오답 후보 선지 각각에 대하여 상기 원문의 각 토큰 대체시에 대한 출현 확률값을 산출하는 단계와,상기 서버가 상기 산출된 각 출현 확률값의 평균을 산출하는 단계와,상기 서버가 상기 산출된 각 평균으로부터 기 설정된 표준편차 범위를 벗어난 오답 후보 선지를 제거하여 최종오답 후보 선지를 확정하는 단계와상기 서버가 상기 정답 선지에 대한 히든 스테이트 벡터를 산출하는 단계와,상기 서버가 상기 최종 오답 후보 선지에 대한 히든 스테이트 벡터를 산출하는 단계와,상기 서버가 상기 정답 선지의 히든 스테이트 벡터와 최종 오답 후보 선지의 히든 스테이트 벡터 간의 관련도를산출하는 단계와,상기 서버가 상기 산출된 관련도가 가장 낮은 순으로 선정된 최종 오답 후보 선지를 상기 복수의 오답 선지로선택하는 단계를 포함하는,외국어 문장 빈칸 추론 문제 자동 생성 방법."}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 마스킹하는 단계, 상기 예측하는 단계, 상기 추출하는 단계 및 상기 오답 후보 선지를 생성하는 단계는,상기 지정된 범위 내 마스킹된 모든 토큰을 대상으로 반복하여 수행되는 것인,외국어 문장 빈칸 추론 문제 자동 생성 방법."}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 히든 스테이트 벡터는 상기 정답 선지 또는 최종 오답 후보 선지에 포함된 각 토큰별 히든 스테이트 벡터를 생성하고, 상기 생성된 각 토큰별 히든 스테이트 벡터를 평균화하여 산출되는 것인,외국어 문장 빈칸 추론 문제 자동 생성 방법."}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 각 토큰별 히든 스테이트 벡터는 각 토큰이 가지는 의미 정보를 포함하는 것인,외국어 문장 빈칸 추론 문제 자동 생성 방법."}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서,오답 선지 생성을 위한 설정정보를 지정받는 단계는, 상기 서버가 사용자로부터 난이도를 지정받는 단계를 포함하되,상기 최종 오답 후보 선지를 확정하는 단계는, 상기 오답 후보 선지에 포함된 상기 지정된 난이도를 초과하는 단어의 출현 빈도에 기초하여 상기 최종 오답 후보 선지를 확정하는 것인,외국어 문장 빈칸 추론 문제 자동 생성 방법.공개특허 10-2021-0042845-5-청구항 10 제6항에 있어서,오답 선지 생성을 위한 설정정보를 지정받는 단계는, 상기 서버가 사용자로부터 난이도를 지정받는 단계를 포함하되,상기 마스킹된 토큰의 위치에 대체 가능한 복수의 단어를 확률값에 기반하여 예측하는 단계는,상기 복수의 단어 중 상기 지정된 난이도를 초과하는 단어의 출현 빈도를 필터링하여 상기 지정된 난이도를 설정하는 것인,외국어 문장 빈칸 추론 문제 자동 생성 방법."}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "외국어 문장 빈칸 추론 문제 자동 생성 시스템에 있어서,사용자에 의해 입력된 하나 이상의 외국어 문장을 입력받고, 상기 입력된 외국어 문장 중 빈칸으로 설정하고자하는 범위 및 오답 선지 생성을 위한 설정정보를 수신하는 통신모듈;상기 통신모듈로부터 수신한 외국어 문장에 대하여 빈칸 추론 문제를 생성하기 위한 컴퓨터 프로그램이 저장된메모리; 및상기 메모리에 저장된 컴퓨터 프로그램을 실행시킴에 따라, 미리 설정된 인공지능 기반의 문장 생성 알고리즘을이용하여 상기 빈칸 범위 및 설정정보에 따른 빈칸 추론 문제를 생성하는 프로세서를 포함하고,상기 빈칸 추론 문제를 생성하는 것은, 상기 입력된 외국어 문장 중 지정된 범위를 빈칸으로 설정하고, 상기 지정된 범위의 원문을 정답 선지로 생성하고, 상기 정답 선지에 기초하여 상기 미리 설정된 인공지능 기반의 문장생성 알고리즘에 따라 복수의 오답 선지를 생성하는 것을 포함하고,상기 복수의 오답 선지를 생성하는 것은, 상기 입력된 외국어 문장을 단어 기준의 토큰으로 분할하고, 상기 지정된 범위 중 일부 토큰에 대해마스킹하고, 상기 마스킹된 각각의 토큰의 위치에 대체 가능한 복수의 단어를 확률값에 기반하여 예측하고, 상기 복수의 단어에 대한 샘플링을 수행하여 확률값을 기반으로 각각의 토큰의 위치에 대응하는 하나의 단어를 추출하고, 상기 추출된 단어를 상기 각각의 마스킹된 위치에 삽입하여 오답 후보 선지를 생성하는 것을 포함하고, 상기 오답 후보 선지를 생성하는 것은, 상기 지정된 범위와 연결된 첫번째 토큰의 위치에 마스킹하고, 상기 첫번째 토큰의 위치에 마스킹한 이후, 상기첫번째 토큰의 해당 위치에서의 원문 단어의 출현 확률을 나타내는 제1확률값을 예측하고, 상기 추출된 단어를상기 각각의 마스킹된 위치에 삽입한 이후, 상기 첫번째 토큰의 해당 위치에서의 원문 단어의 출현 확률을 나타내는 제2확률값을 예측하고, 상기 제1확률값 및 제2확률값에 기초하여 상기 오답 후보 선지를 생성하는 것인,외국어 문장 빈칸 추론 문제 자동 생성 시스템."}
{"patent_id": "10-2020-0168843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "컴퓨터와 결합하여 제1항 내지 제10항 중 어느 하나의 항의 외국어 문장 빈칸 추론 문제 자동 생성 방법을 수행하기 위하여 컴퓨터 판독가능 기록매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2020-0168843", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명에 따른 외국어 문장 빈칸 추론 문제 자동 생성 방법은 하나 이상의 외국어 문장을 입력받는 단계, 상기 입력된 외국어 문장 중 빈칸으로 설정하고자 하는 범위를 지정받는 단계, 오답 선지 생성을 위한 설정정보를 지 정받는 단계, 및 미리 설정된 인공지능 기반의 문장 생성 알고리즘을 이용하여 상기 빈칸 범위 및 설정정보에 따 른 빈칸 추론 문제를 생성하는 단계를 포함한다."}
{"patent_id": "10-2020-0168843", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 외국어 문장 빈칸 추론 문제 자동 생성 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2020-0168843", "section": "발명의_설명", "subsection": "배경기술", "item": 1, "content": "도 1은 빈칸 추론 문제를 설명하기 위한 도면이다. 빈칸 추론 문제는 외국어 능력을 평가하는 각종 시험에 출제되고 있는 유형으로, 빈칸의 앞뒤 문장을 읽고 해당 문맥과 가장 어울리는 선지(보기 또는 선택지)를 선택하도록 하는 문제이다."}
{"patent_id": "10-2020-0168843", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "빈칸 추론 문제는, 예를 들어 수능 외국어 영역과 같은 경우 다른 문제 유형인 문장 찾기, 장문 문제, 요약, 문 장배열 등의 문제들에 비해 매우 난이도가 높은 문제로서 수험생으로 하여금 많은 연습을 요하게 한다. 이와 같은 빈칸 추론 문제를 출제하기 위하여, 출제자들은 하나 이상의 외국어 지문을 선별한 다음 해당 지문 중에서 특정 구, 절 또는 문장을 빈칸 영역으로 지정한다. 그리고 지정된 빈칸 영역에 본래 기재되어 있는 원문을 정답 선지로 설정하고, 문법적으로는 맞지만 문맥과는 맞지 않는 오답 선지를 만들어 낸다. 종래에는 출제자가 직접 오답 선지까지 만들어내야 했기 때문에 하나의 빈칸 추론 문제를 출제하기 위해서 많은 시간이 소요되었고, 인위적으로 오답 선지를 만들어내기 때문에 문법적으로도 오류가 생기는 경우도 빈번하였다."}
{"patent_id": "10-2020-0168843", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 실시예는 인공지능 기반의 문장 생성 알고리즘을 이용하여 오답 선지를 생성함으로써 외국어 문장 빈 칸 추론 문제를 자동으로 생성하는 방법 및 시스템을 제공한다. 다만, 본 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제로 한정되지 않으며, 또 다른 기 술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2020-0168843", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 과제를 해결하기 위한 본 발명의 일 면에 따른 외국어 문장 빈칸 추론 문제 자동 생성 방법은, 하나 이 상의 외국어 문장을 입력받는 단계, 상기 입력된 외국어 문장 중 빈칸으로 설정하고자 하는 범위를 지정받는 단 계, 오답 선지 생성을 위한 설정정보를 지정받는 단계, 및 미리 설정된 인공지능 기반의 문장 생성 알고리즘을 이용하여 상기 빈칸 범위 및 설정정보에 따른 빈칸 추론 문제를 생성하는 단계를 포함한다. 상술한 과제를 해결하기 위한 본 발명의 다른 면에 따른 외국어 문장 빈칸 추론 문제 자동 생성 시스템은, 사용 자에 의해 입력된 하나 이상의 외국어 문장을 입력받고, 상기 입력된 외국어 문장 중 빈칸으로 설정하고자 하는 범위 및 오답 선지 생성을 위한 설정정보를 수신하는 통신모듈, 상기 통신모듈로부터 수신한 외국어 문장에 대 하여 빈칸 추론 문제를 생성하기 위한 컴퓨터 프로그램이 저장된 메모리, 및 상기 메모리에 저장된 컴퓨터 프로 그램을 실행시킴에 따라, 미리 설정된 인공지능 기반의 문장 생성 알고리즘을 이용하여 상기 빈칸 범위 및 설정 정보에 따른 빈칸 추론 문제를 생성하는 프로세서를 포함한다. 본 발명의 기타 구체적인 사항들은 상세한 설명 및 도면들에 포함되어 있다."}
{"patent_id": "10-2020-0168843", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본 발명의 과제 해결 수단 중 어느 하나에 의하면, 입력된 외국어 지문 중 임의의 부분이 빈칸으로 지정 되더라도, 앞뒤 문맥을 고려한 여러 오답 선지를 자동으로 생성할 수 있다는 장점이 있다. 또한, 정답과의 문맥적인 유사도를 산출하여, 문법적으로 맞지만 문맥적으로 맞지 않는 오답 선지를 자동으로 생성할 수 있다. 이와 더불어, 사용자로 하여금 어휘 난이도를 설정할 수 있도록 하고, 설정된 난이도에 따라 다양한 방법을 통 해 난이도를 초과하는 단어의 출현 빈도를 제어할 수 있다."}
{"patent_id": "10-2020-0168843", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과들은 이상에서 언급된 효과로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재로 부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2020-0168843", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나, 본 발명은 이하에서 개시되는 실시예들에 제한되는 것이 아니라 서 로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하고, 본 발명이 속하는 기술 분야의 통상의 기술자에게 본 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명 세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한다 (comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성요소 외에 하나 이상의 다른 구성요소의 존재 또는 추가를 배제하지 않는다. 명세서 전체에 걸쳐 동일한 도면 부호는 동일한 구성 요소를 지칭하며, \"및/또는\"은 언급된 구성요소들의 각각 및 하나 이상의 모든 조합을 포함한다. 비록 \"제1\", \"제2\" 등이 다양한 구성요소들을 서술하기 위해서 사용되나, 이들 구성요소들은 이들 용어에 의해 제한되지 않음은 물론이다. 이들 용어들은 단 지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사용하는 것이다. 따라서, 이하에서 언급되는 제1 구성 요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있음은 물론이다. 다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어(기술 및 과학적 용어를 포함)는 본 발명이 속하는 기술 분야의 통상의 기술자에게 공통적으로 이해될 수 있는 의미로 사용될 수 있을 것이다. 또한, 일반적으로 사용되 는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예를 상세하게 설명한다. 도 2는 본 발명의 일 실시예에 따른 외국어 문장 빈칸 추론 문제 자동 생성 방법의 순서도이다. 도 3a 내지 도 3c는 본 발명의 사용 예시를 설명하기 위한 도면이다. 한편, 도 2에 도시된 단계들은 외국어 문장 빈칸 추론 문제 생성 서비스를 제공하는 업체 또는 플랫폼의 서버 (이하 서버)에 의해 수행되는 것으로 이해될 수 있지만, 이에 제한되는 것은 아니다. 본 발명의 일 실시예에서 외국어는 도면에 도시된 영어로 한정되는 것이 아니라, 일본어, 중국어 등 모국어가 아닌 임의의 외국어가 그 대상이 될 수 있다. 또한, 본 발명의 일 실시예는 한국어를 배제하는 것이 아닌바 외 국인을 대상으로 할 경우에는 한국어도 외국어로 적용될 수 있음은 물론이다. 본 발명의 일 실시예는 먼저, 서버가 하나 이상의 외국어 문장으로 구성된 지문을 사용자로부터 입력받는다 (S110). 일 실시예로 도 3a를 참조하면, 사용자는 서비스를 제공하는 웹페이지에 접속하고 빈칸 추론 문제를 생성하기 위하여 먼저 하나 이상의 외국어 문장을 입력한다.그리고 사용자가 입력 버튼을 클릭하게 되면 해당 내용은 서버로 전달된다. *다음으로, 서버는 입력된 외국어 문장 중 빈칸으로 설정하고자 하는 범위를 지정받고(S120), 오답 선지 생성을 위한 설정정보를 지정받는다(S130). 일 실시예로 도 3b를 참조하면, 사용자는 입력된 외국어 문장 중에서 빈칸으로 설정하고자 하는 범위를 지정하 고, 어떠한 방법과 방식으로 오답 선지를 생성할 것인지에 대한 설정정보를 지정하게 된다. 예를 들어, 사용자가 지정할 수 있는 설정정보는 후술하는 문장 생성 프로세스를 몇 회 반복할 것인지, 오답 후 보를 몇 개 만들 것인지, 문법적인 오류가 있어 부자연스러운 문장도 오답 후보에 포함시킬 것인지 등에 관한 파라미터를 포함할 수 있으나, 이에 한정되는 것은 아니다. 본 발명의 일 실시예는 몇몇 설정정보를 각각 조합하여 다양한 결과로 오답 선지를 생성할 수 있다. 한편, 사용자는 사용자 단말을 통해 외국어 문장을 입력하거나, 범위 및 설정정보를 지정할 수 있다. 여기에서 사용자 단말은 스마트폰, 태블릿, PDA, 랩톱, 데스크톱, 서버 등과 같은 컴퓨터 장치 또는 전기 통신 장치를 의 미한다. 다음으로, 서버는 미리 설정된 인공지능 기반의 문장 생성 알고리즘을 이용하여 상기 빈칸 범위 및 설정정보에 따른 빈칸 추론 문제를 생성한다(S140). 서버는 입력된 외국어 문장 중 지정된 범위를 빈칸으로 설정하고, 지정된 범위의 원문을 정답 선지로 생성한 다 음, 정답 선지에 기초하여 미리 설정된 인공지능 기반의 문장 생성 알고리즘을 이용하여 복수의 오답 선지를 생 성할 수 있다. 이와 같이 생성된 빈칸 추론 문제의 예시는 도 3c와 같다. 서버는 사용자에 의해 입력된 외국어 문장에 대하여 사용자에 의해 지정된 범위를 함께 표시하여 출력시킨다. 그리고 지정 범위와 설정정보에 따라 생성한 오답 선지를 정답 선지와 함께 출력시키게 된다. 이때, 사용자는 몇몇 파라미터를 지정하여 서버에 의해 출력되는 방식을 설정할 수 있다. 예를 들어, 사용자는 생성된 문장의 metric을 표시할 것인지를 지정할 수 있으나, 이에 한정되는 것은 아니다. 한편, 본 발명의 일 실시예는 미리 설정된 인공지능 기반의 문장 생성 알고리즘의 적용을 위해 입력된 외국어 문장을 단어 기준의 토큰으로 분할하여 이용할 수 있다. 그리고 분할된 토큰을 이용하여 빈칸 추론 문제에서의 오답 선지를 어떠한 방법으로 생성할지를 선택할 수 있다. 이때, 서버는 복수의 오답 선지를 생성함에 있어, 사용자에 의해 지정된 범위와 동일 또는 상이한 길이를 갖는 토큰 수로 구성된 오답 선지를 생성할 수 있다. 또는, 정답 선지와 기 설정된 유사도의 범위를 갖는 오답 선지 및 정답 선지와 기 설정된 유사도의 범위 밖의 오답 선지 중 하나 이상의 오답 선지를 포함하도록 생성할 수 있다. 이들의 방법은 각각 서로 조합되어 적용될 수 있음은 물론이다. 일 예로, 서버는 사용자에 의해 지정된 범위와 동일한 토큰의 길이를 갖고, 정답과 가장 유사도가 높은 문맥 구 조로 오답 선지를 생성할 수 있다. 이러한 제1실시예의 경우, 생성된 토큰의 길이는 정답과 동일하고, 문맥 구 조 역시 정답과 유사할 수 있으며, 문법적인 오류 가능성 또한 낮을 수 있으나, 어휘의 다양성은 다소 낮을 수 있다. 반대로, 서버는 사용자에 의해 지정된 범위와 상이한 토큰의 길이를 갖고, 정답과 기 설정된 유사도 범위를 벗 어나는 문맥 구조로 오답 선지를 생성할 수 있다. 이러한 제2실시예의 경우, 생성된 토큰의 길이는 정답과 동일 또는 상이할 수 있으며, 문맥 구조는 정답과 유사도가 낮을 수 있고, 문법적인 오류 가능성은 다소 높을 수 있 으나, 어휘의 다양성은 높을 수 있다. 이와 같이 본 발명의 일 실시예는 학생들의 수준이 높을 경우에는 정답과 매우 유사한 오답 선지가 출제되도록 하고, 수준이 다소 낮은 경우 정답과 비유사한 오답 선지가 생성되도록 다양한 방식을 상호 조합하여 오답 선지를 생성할 수 있다는 장점이 있다. 한편, 본 발명의 일 실시예에서는 MLM(Masked Language Model) 기반의 BERT(Bidirectional Encoder Representations from Transformers) 알고리즘을 이용하여 오답 선지를 생성할 수 있으나, 이에 제한되는 것은 아니다. BERT 알고리즘은 양방향 딥러닝 모델로서, 주어진 문장에서 특정 어절(단어)을 마스킹하고, 이를 맞추 도록 미리 학습(Pretrained)된 딥러닝 모델을 의미한다(Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, \"BERT: Bidirectional Encoder Representations from Transformers\", 2019). 예를 들어, BERT 알고리즘은 \"the man went to the [MASK](store) to buy a [MASK](gallon) of milk\" 문장에서 'store'과 'gallon' 단어를 각각 마스킹한 후 이를 맞추도록 학습된 것이다. 하지만, 본 발명에서 목적으로 하는 외국어 문장 빈칸 추론 문제에 있어 객관식 선지의 경우에는, 최적의 1개 선지만을 만들어서는 안되고, 복수 개(통상적으로 5지선다형의 경우 정답 선지를 제외하고 4개)의 선지를 만들 어야 하며, 생성된 선지 간의 유사성이 낮아야 한다. 이러한 이유로 본 발명의 일 실시예는 기존의 BERT 알고리즘을 그대로 활용하지 않고 개량된 방법을 적용하고 있는바, 이하 도 4a 내지 도 7b를 참조하여 복수의 오답 선지를 생성하는 과정에 대해 설명하도록 한다. 도 4a 내지 도 4e는 본 발명의 제1실시예에서 복수의 오답 후보 선지를 생성하는 과정을 설명하기 위한 도면이 다. 도 5a 내지 도 5g는 본 발명의 제2실시예에서 복수의 오답 후보 선지를 생성하는 과정을 설명하기 위한 도 면이다. 도 6a 및 도 6b는 본 발명의 제1 및 제2실시예에서 복수의 최종 오답 후보 선지를 생성하는 과정을 설 명하기 위한 도면이다. 도 7a 및 도 7b는 본 발명의 일 실시예에서 복수의 오답 선지를 생성하는 과정을 설명하 기 위한 도면이다. 본 발명의 제1실시예는 복수의 오답 후보 선지를 생성하기 위하여, 먼저 도 4a와 같이 입력된 외국어 문장을 단 어 기준의 토큰으로 분할하고 사용자에 의해 지정된 범위를 확인한다. 도 4a의 예시에서는 사용자에 의해 지정된 범위를 포함하는 문장 \"He makes me happy and I love him always\" 가 각각 'He / makes / me / happy / and / I / love / him / always'와 같이 토큰화된 것을 확인할 수 있으며, 사용자에 의해 지정된 범위는 'me / happy / and / I / love / him'임을 확인할 수 있다. 다음으로 도 4b를 참조하면, 지정된 범위 중 임의로 토큰을 선택하고, 임의로 선택된 토큰에 대하여 마스킹 처 리한다. 도 4b의 예시에서는 두번째 토큰에 위치한 단어 'happy'가 마스킹 처리된 것이다. 다음으로 도 4c를 참조하면, 마스킹된 토큰의 위치에 대체 가능한 복수의 단어를 확률값에 기반하여 예측한다. 즉, 마스크를 씌운 토큰의 위치에 대체 가능한 단어를 예측하게 되는데, 이때 본 발명의 일 실시예는 BERT 알고 리즘을 이용하여 각 단어별로 대체 가능한 확률값을 도출할 수 있다. 예를 들어, 대체 가능한 단어는 'laugh'와 'angry'가 있을 수 있는데, 이때 'laugh'는 0.7의 확률값으로 'happy' 단어를 대체할 수 있고, 'angry'는 0.01의 확률값, 즉 거의 대체될 가능성이 없는 확률값으로 'happy' 단어를 대체할 수 있다. BERT 알고리즘을 그대로 활용한다면, 마스킹된 부분의 답으로 확률값이 가장 높은 'laugh'가 출력될 것이다. 이 경우에도 본래의 단어인 'happy'와는 다르면서, 문법적으로 오류가 없는 새로운 단어가 생성된다. 이처럼 본 발 명의 일 실시예는 BERT 알고리즘을 위에서 언급한 확률값에 따라 선택된 토큰을 다른 단어로 대체하기 위한 목 적으로 사용한다. 다음으로 도 4d를 참조하면, 예측된 복수의 단어 중 일정 비율의 단어에 대한 확률값을 0으로 강제 설정하는 커 널을 통과시킨다. 확률값이 0으로 강제 설정되는 단어들은 랜덤하게 결정된다. 본 발명의 일 실시예는, 커널을 통과시키는 과정을 수행함으로써 오답 선지 생성시 동일한 단어가 반복적으로 도출되는 것을 방지할 수 있다. 예를 들어, 서버는 랜덤하게 일정비율(10%)의 단어에 대한 확률값을 0으로 설정할 수 있으며, 도 4d의 예시에서 는 커널을 통과함에 따라 'happy'와 'cry'의 단어가 각각 0.5, 0.2의 확률값에서 0의 확률값으로 변경 설정된 것을 확인할 수 있다. 그리고 서버는 커널을 통과한 단어를 대상으로 샘플링을 수행하여 확률값을 기반으로 하는 하나의 단어를 추출 한다.도 4d의 예시에서는 샘플링이 수행된 결과 'laugh'가 추출됨을 확인할 수 있다. 예시에서, 커널을 통과한 단어 'laugh'와 'angry' 등은 각각의 확률값에 의존적으로 샘플로 추출될 수 있다. 즉, 샘플링 과정에서, 70%의 확률 로 laugh가 추출될 가능성이 높지만, 당연히 'angry'가 추출될 수도 있는 것이다. 이와 같이 본 발명의 일 실시예는 복수의 단어에 대하여 확률값에 기반한 샘플링을 수행함으로써 생성되는 오답 선지에 대하여 랜덤성을 부여할 수 있게 된다. 즉, 오답 선지 생성을 반복할 때, 다음 번에는 커널을 통과함에 따라 'laugh'와 'happy'의 확률값이 0으로 설정되고, 확률값을 기반으로 'cry'가 샘플링되어 추출될 수 있는 것 이다. 다음으로 도 4e를 참조하면, 서버는 디마스킹 과정 즉, 추출된 단어를 마스킹된 위치에 삽입하여 오답 후보 선 지를 생성하게 된다. 이때, 하나의 오답 후보 선지가 완전하게 생성되기 위해서는 사용자에 의해 지정된 범위에 포함된 다른 토큰, 즉 모든 토큰을 대상으로 전술한 도 4a 내지 도 4e에서의 마스킹하는 단계, 확률값 예측 단계, 추출(커널 적용 및 샘플링) 단계, 디마스킹하는 단계가 반복 수행되어야만 한다. 즉, 첫번째 디마스킹 과정까지 수행하고 나면 \"He makes me happy and I love him always\"라는 원문 문장에 대 해 단어 \"laugh\"만이 대체되게 되며, 이후 다른 토큰들을 대상으로 수행하게 되면, 최종적으로 \"He makes her laugh but she hate him always\"라는 오답 후보 선지가 생성되게 된다. 이와 같은 방식을 통해 오답 후보 선지가 생성될 수 있으며, 설정정보에 따른 개수의 오답 후보 선지가 생성될 때까지 상술한 과정은 반복 수행될 수 있다. 다음으로 도 5a 내지 도 5g를 참조하여 본 발명의 제2실시예에 대해 설명하면 다음과 같다. 이때, 본 발명의 제 2실시예는 생성되는 오답 후보 선지의 길이가 지정된 범위로 한정되는 것이 아니라 토큰이 추가됨에 따라 지정 된 범위의 길이가 가변될 수 있는 것을 특징으로 한다. 본 발명의 제2실시예는 복수의 오답 후보 선지를 생성하기 위하여, 먼저 도 5a와 같이 입력된 외국어 문장을 단 어 기준의 토큰으로 분할하고 사용자에 의해 지정된 범위를 확인한다. 도 5a의 예시에서는 사용자에 의해 지정된 범위를 포함하는 문장 \"He makes me happy and I love him always\" 가 각각 'He / makes / me / happy / and / I / love / him / always'와 같이 토큰화된 것을 확인할 수 있으며, 사용자에 의해 지정된 범위는 'me / happy / and / I / love / him'임을 확인할 수 있다. 이와 더불어, 지정된 범위 내에 포함된 토큰을 대상으로 마스킹을 수행하기 이전에, 지정된 범위와 연결된 첫번 째 토큰의 위치에 대하여 마스킹을 수행한다. 그리고 첫번째 토큰의 해당 마스킹된 위치에서의 제1확률값을 예 측한다. 즉, 도5a의 예시에서 지정된 범위 바로 다음의 첫번째 토큰 'always'에 대하여 마스킹을 하고, 마스킹된 해당 위치에서 원문의 단어 'always'가 출현할 제1확률값을 기록한다. 이에 따라 마스킹된 해당 위치에서 'so'는 0.7의 확률값으로 출현할 수 있고, 'truely'는 0.01의 확률값으로 출 현할 수 있으며, 원문에서의 토큰 단어인 'always'는 0.2의 확률값으로 출현할 수 있는바, 해당 토큰 단어인 'always'의 확률값 0.2를 제1확률값으로 기록한다. 이후 후술하는 바에 따라, 본 발명의 제2실시예는 지정된 범위의 길이가 가변되더라도 가변된 범위 바로 다음에 위치하는 단어가 원문과 동일한 토큰 즉, 예시에서의 토큰인 'always'가 위치할 수 있는 경우라면 자연스럽게 연결되는 지문인 것으로 볼 수 있다. 다음으로 도 5b를 참조하면, 서버는 지정된 범위 중 임의의 일부 토큰을 선택하고, 임의로 선택된 토큰에 대하 여 마스킹 처리한다. 도 5b의 예시에서는 지정된 범위 중 75%의 토큰에 마스킹 처리를 한 것이며, 전술한 바와 같이 지정된 범위 바로 다음 첫번째 토큰의 위치에 대해서도 마스킹 처리가 되었다. 이러한 마스킹 처리 결과, 도 5b에서는 'He / makes / me / [MASK] / [MASK] / [MASK] / love / [MASK] / [MASK]'와 같이 지정된 범위 중 'me'와 'love' 단어를 제외한 나머지 토큰들이 마스킹 처리되었고, 'always' 토 큰 위치 역시 마스킹됨을 확인할 수 있다. 다음으로 도 5c를 참조하면, 서버는 마스킹된 토큰의 위치에 대체 가능한 복수의 단어를 확률값에 기반하여 예 측한다.즉, 마스크를 씌운 토큰의 위치에 대체 가능한 단어를 예측하게 되는데, 이때 본 발명의 제2실시예 역시 전술한 BERT 알고리즘을 이용하여 각 단어별로 대체 가능한 확률값을 도출할 수 있다. 예를 들어, 대체 가능한 단어는 'laugh'와 'angry'가 있을 수 있는데, 이때 'laugh'는 0.7의 확률값으로 'happy' 단어를 대체할 수 있고, 'angry'는 0.01의 확률값, 즉 거의 대체될 가능성이 없는 확률값으로 'happy' 단어를 대체할 수 있다. 다음으로 도 5d를 참조하면, 서버는 예측된 복수의 단어 중 일정 비율의 단어에 대한 확률값을 0으로 강제 설정 하는 커널을 통과시킨다. 이때, 확률값이 0으로 강제 설정되는 단어들은 랜덤하게 결정된다. 본 발명의 제2실시예는, 커널을 통과시키는 과정을 수행함으로써 오답 선지 생성시 동일한 단어가 반복적으로 도출되는 것을 방지할 수 있다. 예를 들어, 서버는 랜덤하게 일정비율(10%)의 단어에 대한 확률값을 0으로 설정할 수 있으며, 도 5d의 예시에서 는 커널을 통과함에 따라 'happy'와 'cry'의 단어가 각각 0.5, 0.2의 확률값에서 0의 확률값으로 변경 설정된 것을 확인할 수 있다. 그리고 서버는 커널을 통과한 단어를 대상으로 샘플링을 수행하여 확률값을 기반으로 하는 하나의 단어를 추출 한다. 도 5d의 예시에서는 샘플링이 수행된 결과 'laugh'가 추출됨을 확인할 수 있다. 예시에서, 커널을 통과한 단어 'laugh'와 'angry' 등은 각각의 확률값에 의존적으로 샘플로 추출될 수 있다. 즉, 샘플링 과정에서, 70%의 확률 로 laugh가 추출될 가능성이 높지만, 당연히 'angry'가 추출될 수도 있는 것이다. 이와 같이 본 발명의 일 실시예는 복수의 단어에 대하여 확률값에 기반한 샘플링을 수행함으로써 생성되는 오답 선지에 대하여 랜덤성을 부여할 수 있게 된다. 즉, 오답 선지 생성을 반복할 때, 다음 번에는 커널을 통과함에 따라 'laugh'와 'happy'의 확률값이 0으로 설정되고, 확률값을 기반으로 'cry'가 샘플링되어 추출될 수 있는 것 이다. 다음으로 도 5e를 참조하면, 서버는 디마스킹 과정 즉, 추출된 단어를 마스킹된 위치에 삽입한다. 이때, 제2실시예는 사용자에 의해 지정된 범위의 마스킹된 모든 토큰을 대상으로 전술한 확률값 예측 단계, 추 출(커널 적용 및 샘플링) 단계, 디마스킹하는 단계가 반복 수행되어야만 한다. 즉, 첫번째 디마스킹 과정까지 수행하고 나면 \"He makes me [MASK] [MASK] [MASK] love [MASK] [MASK]\"라는 문 장에 대해 단어 \"laugh\"만이 대체되게 되며, 이후 다른 토큰들을 대상으로 수행하게 되면, \"He makes me laugh enough to love his [MASK]\" 문장이 생성되게 된다.. 다음으로 도 5f 및 도 5g를 참조하면, 서버는 지정된 범위 내 마스킹된 위치 대하여 추출된 토큰 삽입 이후, 지 정된 범위와 연속되는 해당 위치, 즉 도 5a에서의 'always' 토큰 위치에 대한 제2확률값을 산출한다. 그리고 전술한 제1확률값과 제2확률값에 기초하여 오답 후보 선지를 생성하게 된다. 구체적으로, 새롭게 산출된 제2확률값이 제1확률값을 초과하는 경우에는, 마스킹된 위치에 각각 삽입된 토큰을 포함하는 지정된 범위만을 대상으로 오답 후보 선지를 생성할 수 있다. 반대로, 제2확률값이 제1확률값 이하인 경우에는 지정된 범위의 마지막 토큰 및 지정된 범위와 연속되는 해당 위치 사이에 마스킹된 토큰을 새롭게 추가하고, 새롭게 추가된 위치에서의 마스킹된 토큰에 대하여 확률값을 기 반으로 하는 하나의 토큰을 추출한다. 그리고 추가된 위치에 대하여 추출된 토큰 삽입 이후, 서버는 해당 위치에서의 추출된 토큰에 대한 제3확률값을 산출하고, 제1확률값 및 제3확률값을 전술한 바와 같이 비교하여 오답 후보 선지를 생성하게 된다. 예를 들어, 서버는 알고리즘에 따라 도 5g에서와 같이 지정된 범위의 마지막에 위치한 'his' 단어 다음에 'always'가 출현할 제2확률값을 0.001로 매우 낮게 추정하게 된다. 이 경우 새롭게 추정된 제2확률값(0.001)은 제1확률값(0.2)을 초과하지 못하므로, 서버는 지정된 범위만으로는 오답 후보 선지로 확정할 수 없게 된다. 따라서, 서버는 도 5g와 같이 지정된 범위의 마지막 토큰 위치인 'his' 단어와, 지정된 범위와 연속되는 해당 위치인 원문에서의 'always'가 위치한 토큰 사이에 마스킹된 토큰을 새롭게 추가하고, 새롭게 추가된 마스킹된 토큰을 대상으로 전술한 예측 단계, 커널 적용 단계, 샘플링 및 추출 단계와 디마스킹 과정을 다시 수행한다.그 결과 마스킹된 토큰에 대해 'manner'라는 새로운 단어로 결정되었고, 이 상태에서 서버는 다시 'manner' 단 어 이후 'always' 단어가 출현할 제3확률값을 확인하게 된다. 확인 결과, 제3확률값(0.3)은 제1확률값(0.001)을 초과하므로, 지정된 범위는 새롭게 추가된 단어 'manner'를 포함하도록 가변되어 오답 후보 선지로 생성되게 된 다. 만약 제3확률값이 제1확률값 이하인 경우, 이후 전술한 과정을 반복 수행함에 따라 지정된 범위는 더 연장될 수 있다. 이와 같은 방식을 통해 오답 후보 선지가 생성될 수 있으며, 도 5a 내지 도 5g에 따른 과정 역시 설정정보에 따 른 개수의 오답 후보 선지가 생성될 때까지 반복 수행될 수 있다. 제1 또는 제2실시예에 따라 충분한 개수의 오답 후보 선지가 생성된 경우에는 최종 오답 후보 선지를 확정해야 하는 과정을 수행해야 한다. 도 6a를 참조하면, 먼저 서버는 복수의 오답 후보 선지 각각에 대하여 빈칸의 대체시에 대한 출현 확률값을 산 출한다. 즉, 여러 오답 후보 선지별로 원문을 선지로 대체할 경우에 대한 출현 확률값(보다 정확하게는, 우도 또는 가능도(likelihood))을 산출한다. 이때, 본 발명의 일 실시예에서는 출현 확률값으로 범위내 토큰에 대한 mean log-likelihood 값을 산출할 수 있 으나, 이에 제한되는 것은 아니다. 여기서, log를 사용하는 것은 곱을 합으로 변경하기 위한 것이다. 도 6a의 예시에서 첫번째 문장인 \"He makes her laugh but she hate him always\"에 대해서는 'her' 0.2, 'laugh' 0.3, 'but' 0.5, 'she' 0.2, 'hate' 0.01, 'him' 0.3의 각 토큰별 출현 확률값이 산출되게 되며, 최종 적으로 상기 문장의 출현 확률값은 이를 곱셈 연산한 값인 0.000018로 산출될 수 있다. 마찬가지로, 두번째 문장인 \"He makes true love but true hate love always\"에 대해서는 'true' 0.1, 'love' 0.3, 'but' 0.5, 'true' 0.001, 'hate' 0.01, 'love' 0.001의 각 토큰별 출현 확률값이 산출되게 되며, 최종적 으로 상기 문장의 출현 확률값은 이를 곱셈 연산한 값인 0.00000000015로 산출될 수 있다. 다음으로 도 6b를 참조하면, 서버는 오답 후보 선지에서의 각 출현 확률값의 평균을 산출하고, 산출된 각 평균 으로부터 기 설정된 표준편차 범위를 벗어난 오답 후보 선지를 제거하여 최종 오답 후보 선지를 확정하게 된다. 즉, 아웃라이어에 해당하는 오답 후보 선지를 제거하는 것이다. 이러한 과정은 문법적으로 오류가 없는 오답 후보 선지만을 선정하기 위한 것으로서, 최종적으로는 후술하는 바 와 같이 정답과의 관련성이 낮아 정답으로 오인하기 어려운 오답 후보 선지가 최종 오답 선지로 선정되게 된다. 앞선 도 6a 및 도 6b 단계에 따라 최종 오답 후보 선지가 생성되고 나면, 이들 후보 중에서 정답과 관련성이 낮 은 최종 오답 선지(예를 들어 4개)를 선정해야 한다. 이를 위해 본 발명의 일 실시예는 먼저, 정답에 대한 히든 스테이트(hidden state) 벡터를 산출하고, 또한 최종 오답 후보 선지에 대한 히든 스테이트 벡터를 산출한다. 히든 스테이트 벡터의 산출은 BERT 알고리즘에서 정의 된 방식을 활용한다. 즉, 도 7a와 같이 정답에 포함된 각 토큰별 히든 스테이트 벡터를 생성하고, 생성된 각 토큰별 히든 스테이트 벡터를 평균화하여 정답에 대한 히든 스테이트 벡터를 산출할 수 있다. 예를 들어, 'He / makes / me / happy / and / I / love / him / always'라는 토큰으로 분리된 정답에 있어서, 서버는 지정된 범위인 'me / happy / and / I / love / him' 부분에 대하여 각각 토큰별 히든 스테이트 벡터 H11 내지 H16을 산출하고, 이들을 평균화하여 정답에 대한 히든 스테이트 벡터 H1을 산출할 수 있다. 마찬가지로, 최종 오답 후보 선지에 포함된 각 토큰별 히든 스테이트 벡터를 생성하고, 생성된 각 토큰별 히든 스테이트 벡터를 평균화하여 최종 오답 후보 선지에 대한 히든 스테이트 벡터(H2 내지 H16)를 산출할 수 있다 (예를 들어, 최종 오답 후보 선지가 16개일 경우). 여기에서 각 토큰별 히든 스테이트 벡터는 각 토큰이 가지는 의미 정보를 포함할 수 있다. 다음으로 도 7b를 참조하면, 서버는 정답 선지의 히든 스테이트 벡터와 최종 오답 후보의 히든 스테이트 벡터 간의 관련도를 각각 산출하고, 산출된 관련도가 가장 낮은 순으로 최종 오답 후보 선지를 복수의 오답 선지로 선택하게 된다.예를 들어, 정답 선지에 대한 히든 스테이트 벡터 H1과 최종 오답 후보 선지에 대한 히든 스테이트 벡터 H2 내 지 H16을 각각 비교하여 관련도를 산출하고, 이들 중 관련도가 가장 낮게 산출된 H3, H4 등을 최종적인 복수의 오답 선지로 선택할 수 있다. 이때, 관련도는 각 히든 스테이트 벡터 간의 코사인 유사도(cosine-similarity)에 기반하여 산출할 수 있으나, 이에 제한되는 것은 아니다. 이러한 과정에 따라 도 7b에 도시된 예시와 같이 정답 선지 1개와 오답 선지 4개를 포함한 총 5개의 선지가 생 성될 수 있다. 한편, 본 발명의 일 실시예는 사용자가 빈칸 추론 문제를 생성하고자 하는 범위를 지정하고, 지정된 범위에 대 한 설정정보를 지정함에 있어, 난이도를 추가적으로 지정할 수 있다. 도 8a 및 도 8b는 본 발명의 일 실시예에서 난이도를 설정하는 방법을 설명하기 위한 도면이다. 예를 들어, 어휘 수준의 난이도가 총 6단계인 Y<YG<G<B<R<P로 구분되어 등급화된 경우, 사용자는 자신이 원하는 어휘 수준을 지정할 수 있다. 그리고 서버는 사용자가 지정한 어휘 수준 이하의 어휘로 오답 선지를 생성할 수 있다. 즉, 사용자가 G 등급 난이도를 선택한 경우, 서버는 Y, YG, G 등급의 어휘로 오답 선지를 생성하게 된다. 만약 가장 높은 P 등급 난이도를 선택하게 되면 서버는 어휘의 제약 없이 오답 선지를 생성할 수 있다. 이와 관련하여 서버는 마스킹된 토큰의 위치에 대체 가능한 복수의 단어를 확률값에 기반하여 예측함에 있어서, 복수의 단어 중 지정된 난이도를 초과하는 단어의 출현 빈도를 필터링하여 사용자에 의해 지정된 난이도를 설정 할 수 있다. 예를 들어 도 8a를 참조하면, 전술한 도 4a 이하의 예시인 \"He makes me happy and I love him always\"라는 문 장에 있어서, 마스킹된 토큰의 위치에 대체 가능한 복수의 단어를 확률값에 기반하여 예측하는 과정을 수행하게 된다. 여기에서, 서버는 확률값에 기반하여 예측된 복수의 단어에 대하여 난이도별 등급으로 구분하고, 사용자에 의해 지정된 난이도 등급에 따라 해당 난이도 등급을 초과하는 단어들의 출현 확률을 조절할 수 있다. 즉, 도 8a에서 사용자가 G 등급의 난이도를 지정한 경우, 서버는 G 등급을 초과하는 난이도를 갖는 B, R, P 등 급에 대한 토큰 출현 확률을 필터링할 수 있다. 일 예로, 필터 강도를 100%로 설정할 경우 B, R, P 등급에 대한 토큰은 출현하지 않게 된다. 다만, 필터링된 등급의 어휘가 모두 출현하지 않게 되면 다소 문법이나 문장 구성 이 어색한 오답 문장이 생성될 수 있으므로 출현 가능한 수준, 즉 필터 강도를 90%로 설정하는 것이 바람직하며, 이는 사용자가 실제 실시함에 있어 경우에 따라 자유롭게 설정이 가능함은 물론이다. 이 같은 확률 필터는 커널과 샘플링 사이에 배치된다. 또 다른 실시예로, 서버는 최종 오답 후보 선지를 확정함에 있어서, 오답 후보 선지에 포함된 단어 중 지정된 난이도를 초과하는 단어의 출현 빈도에 기초하여 최종 오답 후보 선지를 확정할 수 있다. 또는 실시예에 따라 최종적인 복수의 오답 선지를 확정함에 있어, 최종 오답 후보 선지에 포함된 단어 중 지정 된 난이도를 초과하는 단어의 출현 빈도에 따라 복수의 오답 선지를 확정할 수도 있다. 예를 들어 도 8b를 참조하면, 서버는 생성된 복수 개의 오답 후보 선지에서의 지정된 범위에 대하여 각각 난이 도 등급별 단어 수를 카운팅하고, 지정된 난이도 등급을 초과하는 단어의 출현 빈도에 따라 최종 오답 후보 선 지 또는 오답 선지를 확정할 수 있다. 즉, 사용자가 G 등급의 난이도를 선택한 경우에 있어서, 'our / brain / region / operate / in / an / isolated / manner', 'we / cannot / adapt / ourselves / to / natural / challenges', 'cultural / tools / stabilize / our / brain / functionality' 최종 오답 후보 선지에 포함된 각각의 단어에 대한 난이도 등급별 단어수를 카운팅한 결과, 첫번째 문장의 경우 G 등급의 난이도를 초과하는 B 등급 단어 1개(isolated), 세번째 문장의 경우 B 등급 단어 1개(stabilize), R 등급 단어 1개(functionality)를 포함하고 있기 때문에 이들을 제 외한 두번째 문장을 최종 오답 후보 선지로 선택할 수 있다. 한편, 전술한 도 8a 및 도 8b의 실시예는 최종 오답 후보 선지 또는 오답 선지를 확정함에 있어 각각 독립적으 로 적용될 수 있음은 물론이고, 상호 조합되어 동시에 적용될 수도 있음은 물론이다. 상술한 설명에서, 단계 S110 내지 S140은 본 발명의 구현예에 따라서, 추가적인 단계들로 더 분할되거나, 더 적 은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있다. 아울러, 기타 생략된 내용이라 하더라도 후술하는 도 9의 내용은 도 2 내지 도 8b의 외국어 문장 빈칸 추 론 문제 자동 생성 방법에도 적용될 수 있다. 이하에서는 도 9를 참조하여, 본 발명의 일 실시예에 따른 외국어 문장 빈칸 추론 문제 자동 생성 시스템 에 대하여 설명하도록 한다. 도 9는 본 발명의 일 실시예에 따른 외국어 문장 빈칸 추론 문제 자동 생성 시스템를 설명하기 위한 도면 이다. 도 9를 참조하면, 외국어 문장 빈칸 추론 문제 자동 생성 시스템은 통신모듈, 메모리 및 프로세 서를 포함한다. 통신모듈은 사용자에 의해 입력된 하나 이상의 외국어 문장을 수신한다. 또한, 입력된 외국어 문장 중 빈 칸으로 지정하고자 하는 범위 및 지정된 범위에 대한 설정정보를 수신한다. 메모리에는 통신모듈로부터 수신한 외국어 문장에 대하여 빈칸을 추론하는 문제를 생성하기 위한 프 로그램이 저장된다. 프로세서는 메모리에 저장된 프로그램을 실행시킨다. 프로세서는 메모리에 저장된 프로그 램을 실행시킴에 따라, 미리 설정된 인공지능 기반의 문장 생성 알고리즘을 이용하여 상기 빈칸 범위 및 설정정 보에 따른 빈칸 추론 문제를 생성한다. 프로세서에 의해 실행되는 빈칸 추론 문제의 생성 방법은 상술한 바와 같다. 도 9를 참조하여 설명한 외국어 문장 빈칸 추론 문제 자동 생성 시스템은 상술한 서버의 구성요소로 제공 될 수 있다. 이상에서 전술한 본 발명의 일 실시예에 따른 외국어 문장 빈칸 추론 문제 자동 생성 방법은, 하드웨어인 컴퓨 터와 결합되어 실행되기 위해 프로그램(또는 어플리케이션)으로 구현되어 매체에 저장될 수 있다. 상기 전술한 프로그램은, 상기 컴퓨터가 프로그램을 읽어 들여 프로그램으로 구현된 상기 방법들을 실행시키기 위하여, 상기 컴퓨터의 프로세서(CPU)가 상기 컴퓨터의 장치 인터페이스를 통해 읽힐 수 있는 C, C++, JAVA, Ruby, 기계어 등의 컴퓨터 언어로 코드화된 코드(Code)를 포함할 수 있다. 이러한 코드는 상기 방법들을 실행하 는 필요한 기능들을 정의한 함수 등과 관련된 기능적인 코드(Functional Code)를 포함할 수 있고, 상기 기능들 을 상기 컴퓨터의 프로세서가 소정의 절차대로 실행시키는데 필요한 실행 절차 관련 제어 코드를 포함할 수 있 다. 또한, 이러한 코드는 상기 기능들을 상기 컴퓨터의 프로세서가 실행시키는데 필요한 추가 정보나 미디어가 상기 컴퓨터의 내부 또는 외부 메모리의 어느 위치(주소 번지)에서 참조되어야 하는지에 대한 메모리 참조관련 코드를 더 포함할 수 있다. 또한, 상기 컴퓨터의 프로세서가 상기 기능들을 실행시키기 위하여 원격(Remote)에 있는 어떠한 다른 컴퓨터나 서버 등과 통신이 필요한 경우, 코드는 상기 컴퓨터의 통신 모듈을 이용하여 원격에 있는 어떠한 다른 컴퓨터나 서버 등과 어떻게 통신해야 하는지, 통신 시 어떠한 정보나 미디어를 송수신해야 하 는지 등에 대한 통신 관련 코드를 더 포함할 수 있다. 상기 저장되는 매체는, 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체가 아니라 반 영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 구체적으로는, 상기 저 장되는 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 데이터 저장장치 등이 있지만, 이에 제한되지 않는다. 즉, 상기 프로그램은 상기 컴퓨터가 접속할 수 있는 다양한 서버 상의 다양한 기록매체 또는 사용자의 상기 컴퓨터상의 다양한 기록매체에 저장될 수 있다. 또한, 상기 매체는 네트워크로 연결된 컴퓨터 시 스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장될 수 있다."}
{"patent_id": "10-2020-0168843", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본 발명의 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자는 본 발명 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다.본 발명의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으 로 해석되어야 한다."}
{"patent_id": "10-2020-0168843", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 빈칸 추론 문제를 설명하기 위한 도면이다. 도 2는 본 발명의 일 실시예에 따른 외국어 문장 빈칸 추론 문제 자동 생성 방법의 순서도이다. 도 3a 내지 도 3c는 본 발명의 사용 예시를 설명하기 위한 도면이다. 도 4a 내지 도 4e는 본 발명의 제1실시예에서 복수의 오답 후보 선지를 생성하는 과정을 설명하기 위한 도면이 다. 도 5a 내지 도 5g는 본 발명의 제2실시예에서 복수의 오답 후보 선지를 생성하는 과정을 설명하기 위한 도면이 다. 도 6a 및 도 6b는 본 발명의 제1 및 제2실시예에서 복수의 최종 오답 후보 선지를 생성하는 과정을 설명하기 위 한 도면이다. 도 7a 및 도 7b는 본 발명의 일 실시예에서 복수의 오답 선지를 생성하는 과정을 설명하기 위한 도면이다. 도 8a 및 도 8b는 본 발명의 일 실시예에서 난이도를 설정하는 방법을 설명하기 위한 도면이다. 도 9는 본 발명의 일 실시예에 따른 외국어 문장 빈칸 추론 문제 자동 생성 시스템를 설명하기 위한 도면이다."}
