{"patent_id": "10-2025-0000927", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0008147", "출원번호": "10-2025-0000927", "발명의 명칭": "복수의 센서들을 이용하여 객체의 정보를 획득하는 방법 및 장치", "출원인": "주식회사 아비커스", "발명자": "한대용"}}
{"patent_id": "10-2025-0000927", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로그램이 저장된 메모리; 및상기 적어도 하나의 프로그램을 실행하는 적어도 하나의 프로세서;를 포함하고,상기 적어도 하나의 프로세서는,영상 프레임에 포함된 객체에 대응되는 객체 영역을 생성하고, 상기 객체 영역에 포함된 포인트 데이터에 기초하여 상기 객체의 정보를 획득하는,복수의 센서들을 이용하여 객체의 정보를 획득하는 장치."}
{"patent_id": "10-2025-0000927", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 영상 프레임은 이미지를 획득하는 소정의 센서를 이용하여 획득되고, 상기 포인트 데이터는 상기 객체의정보를 획득하는 소정의 센서를 이용하여 획득되는, 장치."}
{"patent_id": "10-2025-0000927", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 적어도 하나의 프로세서는,상기 영상 프레임을 객체 특정 모델의 입력 데이터로써 입력하고, 상기 객체 특정 모델의 출력 데이터로써 상기객체를 특정하는 바운딩 박스를 획득하며, 상기 바운딩 박스를 이용하여 다차원의 객체 영역을 생성하는, 장치."}
{"patent_id": "10-2025-0000927", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,상기 적어도 하나의 프로세서는,객체 추적과 관련된 추적 알고리즘을 이용하여 상기 영상 프레임 각각에 포함된 상기 객체를 추적하고, 상기 영상 프레임 각각에 포함된 상기 객체의 연관관계에 기초하여 상기 영상 프레임 각각에 포함된 객체들이 동일한객체인지 여부를 판단하는, 장치."}
{"patent_id": "10-2025-0000927", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서,상기 적어도 하나의 프로세서는,상기 객체 영역에 포함된 포인트 데이터의 중간 값에 기초하여 상기 객체의 정보를 산출하는, 장치."}
{"patent_id": "10-2025-0000927", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2025-0008147-3-제 1 항에 있어서,상기 객체 영역에 포함된 상기 객체가 복수개인 경우,상기 적어도 하나의 프로세서는,상기 복수개의 객체들 각각의 포인트 데이터를 획득하고, 상기 획득된 복수개의 객체들 각각의 포인트 데이터중 우세 포인트 데이터에 기초하여 자선과 가까운 객체의 정보를 산출하는, 장치."}
{"patent_id": "10-2025-0000927", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 포인트 데이터의 중간 값은 기 설정된 수 이상의 상기 포인트 데이터를 이용하여 산출되는, 장치."}
{"patent_id": "10-2025-0000927", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서,상기 장치는 모니터링 영상을 제공하는 디스플레이부;를 더 포함하고,상기 디스플레이부는,상기 객체의 정보에 포함된 상기 객체의 거리 및 상기 객체의 방위각 중 적어도 하나가 표시된 모니터링 영상을제공하는, 장치."}
{"patent_id": "10-2025-0000927", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "객체의 영상 프레임 정보에 기초하여 생성되는 다차원의 객체 영역으로부터 상기 객체의 정보를 획득하는 동작;상기 객체의 영상 프레임 정보 및 상기 객체의 정보를 융합하여 자선과 상기 객체의 충돌 위험도를 산출하는 동작; 및상기 산출된 충돌 위험도에 기초하여 회피 경로를 결정하는 동작;을 포함하는,복수의 센서들을 이용한 항해 보조 방법."}
{"patent_id": "10-2025-0000927", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항의 방법을 컴퓨터에서 실행하기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2025-0000927", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "일 측면에 따른 복수의 센서들을 이용한 항해 보조 시스템은, 모니터링 영상을 제공하는 디스플레이부; 적어도 하나의 센서를 포함하는 센서부; 적어도 하나의 프로그램이 저장된 메모리; 및 상기 적어도 하나의 프로그램을 실행하는 적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는, 제 1 센서를 이용하여 객체의 영 상 프레임 정보를 획득하고, 제 2 센서를 이용하여 객체의 정보를 산출하고, 상기 객체의 영상 프레임 정보 및 상기 객체의 정보를 융합하여 자선과 객체의 충돌 위험도를 산출하며, 상기 산출된 충돌 위험도에 기초하여 회피 경로를 결정한다."}
{"patent_id": "10-2025-0000927", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는, 복수의 센서들을 이용하여 객체의 정보를 획득하는 방법 및 장치에 관한다."}
{"patent_id": "10-2025-0000927", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래 선박의 항해 보조 장치는 선박 인근의 객체만 단순 인지하고, 인지된 객체를 화면에 전시하는 기능만이 있 어 동력선박 운전자에게 장애물에 대한 알람 만을 제공하였다. 그러나 운전 중에는 전방 견시가 이루어지기 때문에 전시되는 정보를 운전자가 지속적으로 보고 있지 않으며, 또한 제공된 정보를 기반으로 운항 계획, 충돌 회피 등 행동을 취하는 것은 운전자 스스로 수행해야 하기 때문 에 실질적으로 운전자에게 운전 편의와 안전한 운항에 대한 기능을 제공하지 못하였다. 따라서, 특허문헌 1은 RADAR(레이더), 선박자동식별장치(AIS, Automatic Identification System), 전자해도 (ENC, Electronic Navigational Chart), 위성항법장치(GPS, Global Positioning System) 등의 센서를 이용하여 상대선의 동태를 관찰하고 충돌위험 여부를 판단할 수 있는 발명을 제공하였으나, 모든 선박에 상기 센서가 설 치된 것은 아니며, 항해 보조 장치는 어떤 센서가 설치되 어 있는지 알 수가 없다. 또한, 선박에 장착된 센서가 부족한 경우 카메라 화면을 보여주는 것 외에 다른 기능을 제공할 수가 없다는 문 제점이 존재하였다 더하여, 특허문헌 1에서는 국제해상충돌예방규칙에 따라 피항선 및 단일의 회피 경로를 생성하여 상황에 따라 유연한 회피 경로를 도출할 수 없고 단순히 국제해상충돌예방규칙에 따른 회피만 가능하였다. 따라서, 해상에서의 위험 상황 인식을 전적으로 운항자 개인에게 맡길 수 없어 복수의 센서를 이용하여 이를 보 조하도록 하고 있으나, 초대형 선박 등 일부 선박을 제외하고 상술한 복수의 센서가 모두 설치된 경우는 흔치 않으며, 어떤 센서가 설치되었는지 미리 파악하고, 설치된 센서에 따라 센서 데이터를 통합하여 정보를 제공하 는 것뿐만 아니라, 상황에 맞게 회피 경로를 도출할 수 있는 항해 보조 장치가 존재하지 않았다. 본 발명은, 위와 같은 문제를 센서 데이터를 통합하여 정보를 제공함으로써 해결하고자 한다. (특허문헌 1) KR 10-1693982 B1"}
{"patent_id": "10-2025-0000927", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "복수의 센서들을 이용하여 객체의 정보를 획득하는 방법 및 장치를 제공하는데 있다. 또한, 상기 방법을 컴퓨터 에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체를 제공하는 데 있다. 해결하려는 기 술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2025-0000927", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면에 따르면, 제 1 센서를 이용하여 실시간으로 영상 프레임을 획득하는 동작; 상기 영상 프레 임 내의 적어도 하나의 객체를 포함하는 바운딩 박스(bounding box)를 형성하는 동작; 상기 바운딩 박스를 이용 하여 상기 객체에 대응되는 객체 영역을 생성하는 동작; 제 2 센서를 이용하여 상기 객체 영역에 포함된 데이터 를 획득하는 동작; 및 상기 획득된 데이터에 기초하여 상기 객체의 정보를 산출하는 동작;을 포함하는, 복수의 센서들을 이용하여 객체의 정보를 획득하는 방법을 제공할 수 있다. 본 개시의 다른 측면에 따른 복수의 센서들을 이용한 항해 보조 시스템은, 모니터링 영상을 제공하는 디스플레 이부; 적어도 하나의 센서를 포함하는 센서부; 적어도 하나의 프로그램이 저장된 메모리; 및 상기 적어도 하나 의 프로그램을 실행하는 적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는, 제 1 센서를 이 용하여 객체의 영상 프레임 정보를 획득하고, 제 2 센서를 이용하여 객체의 정보를 산출하고, 상기 객체의 영상 프레임 정보 및 상기 객체의 정보를 융합하여 자선과 객체의 충돌 위험도를 산출하며, 상기 산출된 충돌 위험도 에 기초하여 회피 경로를 결정할 수 있다. 본 개시의 또 다른 측면에 따른 컴퓨터로 읽을 수 있는 기록매체는 상술한 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 기록매체를 포함한다."}
{"patent_id": "10-2025-0000927", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예에 의하면, 자선에 설치된 센서의 종류를 사전에 파악하고, 설치된 센서부로부터 자선 주위의 객체를 파악하여 충돌 위험 예측 및 회피 경로를 도출하고 도출된 경로 추종을 수행할 수 있다. 또한, 본 발명의 일 실시예에 의하면, 충돌 위험 예측 및 경로 설정에 필요한 복수의 센서가 모두 설치되지 않 았다고 하더라도 설치된 센서만을 이용하여 충돌 위험 예측 및 경로 설정을 수행할 수 있다. 또한, 객체의 정보를 얻기 위한 영상의 전처리 시간을 감소시킬 수 있다. 또한, 동적 객체와 정적 객체를 복수의 센서를 이용하여 구분하고, 구분된 객체의 정보를 획득할 수 있다. 본 발명의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 또 다른 목적들은 아래의 기재"}
{"patent_id": "10-2025-0000927", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "로부터 본 발명의 기술분야에서 통상의 지식을 지닌 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2025-0000927", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "실시 예들에서 사용되는 용어는 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 특정한 경우는 출원 인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 명세서에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 명세서의 전반에 걸친 내용 을 토대로 정의되어야 한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"~ 유닛\", \"~ 모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 명세서에서 사용되는 \"제 1\" 또는 \"제 2\" 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명 하는데 사용할 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나 의 구성 요소를 다른 구성 요소로부터 구별하는 목적으로 사용될 수 있다. 이하 첨부된 도면을 참조하여 본 개시를 상세히 설명하기로 한다. 그러나 실시 예는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 예에 한정되지 않는다. 도 1은 일 실시예에 따른 복수의 센서를 이용하여 객체의 정보를 획득하는 시스템의 일 예를 설명하기 위한 도 면이다. 이하, 도 1을 참조하여, 복수의 센서들을를 이용하는 객체 정보 획득 시스템의 일 예를 설명한다. 도 1을 참조하면, 복수의 센서들을 이용하는 객체 정보 획득 시스템(이하, 시스템)은 장치, 센서부 및 디스플레이부를 포함한다. 예를 들어, 장치는 제어부 및 연산부(미도시)를 포함할 수 있으며, 제어부는 통신부, 프로세서 및 메모리를 포함할 수 있다. 도 1의 장치에는 실시예와 관련된 구성요소들 만이 도시되어 있다. 따라서, 도 1에 도시된 구성요소들 외에 다른 범용적인 구성요소들이"}
{"patent_id": "10-2025-0000927", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "더 포함될 수 있음을 당해 기술분야의 통상의 기술자에게 자명하다. 통신부는 외부 서버 또는 외부 장치와 유선/무선 통신을 하게 하는 하나 이상의 구성 요소를 포함할 수 있 다. 예를 들어, 통신부는 외부 서버 또는 외부 장치와의 통신을 위한 근거리 통신부(미도시) 및 이동 통신 부(미도시) 등을 포함할 수 있다. 메모리는 장치 내에서 처리되는 각종 데이터들을 저장하는 하드웨어로서, 프로세서의 처리 및 제 어를 위한 프로그램을 저장할 수 있다. 예를 들어, 메모리에는 제 1 센서를 이용하여 획득한 객체의 영상 또는 이미지, 제 2 센서를 이용하여 획 득한 데이터, 프로세서의 동작에 따라 생성된 데이터 등 다양한 데이터가 저장될 수 있다. 또한, 메모리 에는 운영체제(OS)와 적어도 하나의 프로그램(예를 들어, 프로세서가 동작하는데 필요한 프로그램 등)이 저장될 수 있다. 프로세서는 장치의 전반적인 동작을 제어한다. 예를 들어, 프로세서는 메모리에 저장된 프 로그램들을 실행함으로써, 입력부(미도시), 연산부(미도시), 센서부, 디스플레이부, 통신부, 메 모리 등을 전반적으로 제어할 수 있다. 프로세서는 ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서 (microprocessors), 기타 기능 수행을 위한 전기적 유닛 중 적어도 하나를 이용하여 구현될 수 있다. 프로세서는, 메모리에 저장된 프로그램들을 실행함으로써, 장치, 센서부, 디스플레이부 중 적어도 하나의 동작을 제어할 수 있다. 일 예로서, 프로세서는 도 2 내지 도 9를 참조하여 서술 하는, 복수의 센서들을 이용하여 객체의 정보를 획득하는 방법 중 적어도 일부를 수행할 수 있다. 다른 예로서, 프로세서는 도 10 내지 도 14를 참조하여 서술하는, 퓨전된 센서 정보를 이용하는 항해 보조 방법 중 적어 도 일부를 수행할 수 있다. 메모리는 DRAM(dynamic random access memory), SRAM(static random access memory) 등과 같은 RAM(random access memory), ROM(read-only memory), EEPROM(electrically erasable programmable read-only memory), CD-ROM, 블루레이 또는 다른 광학 디스크 스토리지, HDD(hard disk drive), SSD(solid state drive), 또는 플래시 메모리 등을 포함할 수 있다. 센서부는 복수의 센서들을 포함할 수 있다. 예를 들어, 센서부는 레이더(RADAR, RAdio Detection And Ranging), 전자해도(ENC, Electronic Navigational Chart), 라이다(Lidar, Light Detection And Ranging), 선박자동식별장치(AIS, Automatic Identification System), 소나(Sonar), 관성측정장치(IMU, Inertial Measurement Unit), 및 자선 DB(DataBase), 영상 또는 이미지를 획득하는 센서, 레이더를 이용하여 물체를 감지하는 센서, 적외선 또는 가시광선을 이용하는 센서, 온도를 감지하는 센서, 움직임을 감지하는 센서 중 적어도 하나를 포함할 수 있다. 자선에 설치될 수 있는 복수의 센서는 이에 한정되지 않고, 선박에 설치되는 센서 종류는 제한되지 않는다. 실시 예에 따라, 센서부는 제 1 센서 및 제 2 센서를 포함할 수 있다. 제 1 센서는 영상 획득 센서이고, 제 2 센서는 거리 감지 센서일 수 있다. 예를 들어, 제 1 센서는 광학 카메라, 적외선 카메라, EO/IR 카메라 (Electro-Optical/Infra-Red) 중 적어도 하나를 포함하고, 제 2 센서는 레이더, 라이다 및 AIS 중 적어도 하나 를 포함할 수 있다. 디스플레이부는 사용자에게 모니터링 영상 또는 인터페이스를 제공할 수 있다. 여기에서, 모니터링 영상은, 영상 획득 장치로 획득되는 영상, 기 저장된 영상, 실시간 영상, 전자해도, 지도, 경로 안내 영상 등의 영상일 수 있다. 연산부(미도시)는 제어부에 포함될 수 있으며, 적어도 하나의 프로세서를 포함할 수 있다. 도 2는 일 실시예에 따른 복수의 센서들을 이용하여 객체의 정보를 획득하는 방법의 일 예를 설명하기 위한 흐 름도이다. 이하, 도 2를 참조하여, 프로세서가 복수의 센서들을 이용하여 객체의 정보를 획득하는 방법을 간략히 설 명한다. 도 2를 참조하면, 복수의 센서를 이용하여 객체의 정보를 획득하는 방법(S200)은 단계들(S210 내지 S250)을 포 함할 수 있다. 단계 S210에서, 프로세서는 제 1 센서를 이용하여 실시간으로 영상 프레임을 획득할 수 있다. 일 예로서, 제 1 센서는 EO 카메라 및 IR 카메라 중 적어도 하나일 수 있다. 제 1 센서가 EO 카메라인 경우, 프로세서는 가시광선을 이용한 실시간 영상 프레임을 획득할 수 있다. 제 1 센서가 IR 카메라인 경우, 프로세서는 적외선을 이용한 실시간 영상 프레임을 획득할 수 있다. 프로세서 가 EO 카메라 및 IR 카메라 중 적어도 하나를 이용하여 획득한 실시간 영상 프레임에는 소형선, 대형선, 어선, 요트, 제트스키, 사람, 부표, 섬, 암초, 섬 등의 다양한 객체가 포함될 수 있다. 제 1 센서가 EO 카메라 및 IR 카메라를 포함하는 경우, 프로세서는 가시광선 영상 프레임 또는 적외선 영 상 프레임을 각각 이용할 수 있고, 가시광선 영상 프레임 및 적외선 영상 프레임을 동시에 이용할 수도 있다. 프로세서는 EO 카메라와 IR 카메라를 이용하여 획득한 각각의 영상 프레임들을 융합한 영상 프레임을 이용 할 수 있다. 예를 들어, EO 카메라는 가시광선을 이용하는 카메라인 바, 어두운 환경에서 EO 카메라는 객체를 포착하지 못할 가능성이 높다. 따라서, 프로세서는 IR 카메라와 EO 카메라를 이용하여 획득한 각각의 영상 프레임을 융합 하여 어두운 환경에서도 객체를 포착할 수 있다. 즉, 프로세서는 밤과 같이 어두운 환경에서도 객체를 탐 지하고, 탐지된 객체의 정보를 획득할 수 있다. 단계 S220에서, 프로세서는 영상 프레임 내의 적어도 하나의 객체를 포함하는 바운딩 박스(bounding box) 를 형성할 수 있다. 프로세서는 획득된 영상 프레임에 포함된 객체를 특정하기 위해 바운딩 박스(bounding box)를 이용할 수 있다. 바운딩 박스(bounding box)란, 인공지능이 사진 또는 그림에서 감지하는 일정 구역을 의미할 수 있다. 실시 예에 따라, 프로세서는 객체 특정 모델을 이용하여 객체를 탐지하고, 객체를 포함하는 바운딩 박스를 형성하여 객체를 특정할 수 있다. 실시 예에 따라, 프로세서는 객체 특정 모델을 이용하여 특정된 객체의 종류를 구분할 수도 있다. 그러나 이에 제한되지 않으며, 프로세서는 객체 특정 모델을 이용하여 객체 탐 지, 객체 특정, 및 객체의 종류 분류 중 어느 하나만 수행할 수도 있다. 객체 특정 모델은 인공지능 모델 (Artificial Intelligence model)일 수 있다. 인공지능 모델이란, 기계 학습(Machine Learning) 기술과 인지과학에서, 심층 신경망(Deep Neural Network)을 기반으로 계층화된 알고리즘 구조를 사용하는 기계 학습 알고리즘의 집합을 의미한다. 예를 들어, 인공지능 모 델은, 외부로부터 입력 신호 또는 데이터를 수신하는 입력층(input layer)과 입력 데이터에 대응한 출력 신호 또는 데이터를 출력하는 출력층(output layer), 입력층과 출력층 사이에 위치하며 입력층으로부터 신호를 받아 특성을 추출하여 출력층으로 전달하는 적어도 하나의 은닉층(hidden layer)으로 구성될 수 있다. 출력층은 은닉 층으로부터 신호 또는 데이터를 수신하여 외부로 출력한다. 객체 특정 모델은, 로깅 데이터(logging data) 및 생성형 인공지능 모델을 이용하여 생성된 데이터 중 적어도 하나를 이용하여 학습될 수 있다. 여기에서, 로깅 데이터(logging data)란, 인공지능 모델의 학습을 위해 직접 촬영한 객체의 사진을 의미할 수 있다. 생성형 인공지능 모델이란, 기존의 인공지능 모델이 주어진 데이터를 분 석하고 판단하는 것과 달리, 텍스트, 문서, 그림 또는 영상을 생성하거나 응용하는 능력을 갖춘 인공지능 모델 을 의미할 수 있다. 따라서, 생성형 인공지능 모델을 이용하여 생성된 데이터는 객체 특정 모델을 학습시키기 위해 생성형 인공지능 모델을 이용하여 생성된 데이터를 의미할 수 있다. 따라서, 프로세서는 생성형 인공 지능 모델을 이용하여 로깅 데이터로 이용하기 어려운 객체 사진들을 획득할 수 있고, 이를 이용하여 객체 특정 모델을 학습할 수 있다. 객체 특정 모델의 학습을 위한 데이터는 종류에 한정이 없이 다양한 객체를 포함하는 데이터일 수 있다. 예를 들어, 객체 특정 모델의 학습을 위한 데이터는 소형선, 대형선, 보트, 요트, 제트스키, 부표, 사람, 암초, 빙산, 사람 등의 다양한 객체를 포함할 수 있으며, 이에 한정되지 아니한다. 단계 S230에서, 프로세서는 바운딩 박스를 이용하여 객체에 대응되는 객체 영역을 생성할 수 있다. 여기에 서, 객체 영역은, 객체의 깊이 정보를 획득할 수 있는 2차원 또는 3차원의 영역일 수 있다. 영상 프레임에서, 객체는 평면적으로 표시되는 바, 프로세서는 객체 영역을 생성하여 객체의 깊이 정보를 획득할 수 있다. 또한, 객체 영역은 바운딩 박스를 가상의 면에 투영시켜 생성되는, 영상 프레임과 가상의 면 사이의 영역일 수 있으며, 구체적인 실시예는 후술한다. 객체 영역은 제1 센서로부터 획득된 프레임에 기초하여 생성될 수 있다. 객체 영역은 3차원 또는 2차원일 수 있 다. 단계 S240에서, 프로세서는 제 2 센서를 이용하여 상기 객체 영역에 포함된 데이터를 획득할 수 있다. 제2 센서는 레이더, 라이다 및 AIS 중 적어도 하나를 포함할 수 있다. 여기에서, 레이더(Radio Detection And Ranging; RADAR)는 전자파를 조사하여 객체에 부딪힌 뒤 반사되는 전자 파를 측정함으로써 객체를 탐지하고, 탐지된 객체의 정보를 획득하는 센서로서, 객체에 대한 정보를 포인트 데 이터 형식으로 획득할 수 있다. 또한, 라이다(Light Detection And Ranging; LiDAR)는 레이저(laser)를 조사하 여 객체에 부딪힌 뒤 반사되는 레이저를 측정함으로써 객체를 탐지하고, 탐지된 객체의 정보를 획득하는 센서로 서, 객체에 대한 정보를 포인트 클라우드 데이터(point cloud data) 형식으로 획득할 수 있다. 또한, AIS(Automatic Identification System)은 선박자동식별장치로 호칭되며, AIS가 장착된 선박의 위치, 속도, 방 향을 포함하는 선박의 정보를 송, 수신할 수 있는 센서이다. 단계 S250에서, 프로세서는 획득된 데이터에 기초하여 상기 객체의 정보를 산출할 수 있다. 예를 들어, 제 2 센서로부터 획득된 데이터는 객체 영역에 포함된 객체와 대응되는 데이터일 수 있고, 이를 이 용하여 객체의 정보를 산출할 수 있다. 일 예로서, 프로세서는 객체 영역에 포함된 객체와 대응되며, 제 2 센서로부터 획득된 점군 데이터의 중간 값을 이용하여 객체의 정보를 산출할 수 있다. 다른 예로서, 프로세서 는 객체 영역에 포함된 객체와 대응되며, 제 2 센서로부터 획득된 위치 및 크기 데이터를 이용하여 객체의 정보를 산출할 수 있다. 여기에서, 산출되는 객체의 정보는, 자선과 객체 간의 거리, 자선 기준의 객체의 방위각, 객체의 좌표 값, 객체 의 속도, 객체의 선수각 등을 포함할 수 있으며, 이에 한정되지 아니한다. 이하에서는, 도 2를 참조하여 상술한 프로세서가 복수의 센서를 이용하여 객체의 정보를 획득하는 방법을 구체적으로 설명하고자 한다. 도 3은 일 실시예에 따른 객체 특정 모델을 이용하여 객체를 탐지하고, 탐지된 객체를 특정하는 방법의 일 예를 설명하기 위한 흐름도이고, 도 4는 일 실시예에 따른 영상에 포함된 적어도 하나의 객체를 탐지하여 특정하는 방법의 일 예를 설명하기 위한 도면이다. 도 3 및 도 4는 도 2의 단계(S220)를 보다 상세하게 설명하는 도면일 수 있다. 이하, 도 3 내지 도 4를 참조하여, 프로세서가 객체를 특정하는 방법을 설명한다. 먼저, 도 3을 참조하면, 프로세서는 단계 S310에서, 획득된 영상 프레임을 객체 특정 모델에 입력할 수 있 다. 프로세서는 제 1 센서를 이용하여 획득된 실시간 영상을 객체 특정 모델의 입력 데이터로써 사용할 수 있다. 프로세서는 단계 S320에서, 객체 특정 모델을 이용하여 객체를 특정하는 바운딩 박스를 형성할 수 있다. 예를 들어, 프로세서는 객체 특정 모델에 입력된 실시간 영상에서 적어도 하나의 객체를 탐지할 수 있다. 프로세서는 객체 특정 모델을 이용하여 실시간 영상에서 객체의 위치 및 종류에 대한 정보를 획득할 수 있 다. 또한, 프로세서는 학습된 객체 특정 모델을 이용하여 탐지된 객체의 크기 및 수에 대응되는 적어도 하나의 바운딩 박스를 형성하여 탐지된 객체를 마킹할 수 있다. 다시 말해, 프로세서는 인공지능 모델을 이용하여 객체의 크기에 따라 크거나 작은 바운딩 박스를 형성할 수 있다. 또한, 프로세서는 인공지능 모델을 이용 하여 탐지된 객체의 수와 동일한 수의 바운딩 박스를 형성할 수 있다. 여기에서, 바운딩 박스(bounding box)란, 인공지능이 사진 또는 그림에서 감지하는 일정 구역을 의미할 수 있다. 한편, 프로세서는 탐지된 객체의 수가 두개 이상인 경우, 각각의 객체를 바운딩 박스를 형성하여 특정할 수 있다.먼저, 탐지된 객체들이 겹치지 않은 경우, 프로세서는 하나의 객체와 다른 하나의 객체 각각 바운딩 박스 를 형성하여 특정할 수 있다. 즉, 프로세서는 학습된 객체 특정 모델을 이용하여 탐지되는 복수의 객체들 마다 각각의 바운딩 박스를 형성할 수 있다. 여기에서, 프로세서는 객체들 각각의 크기에 대응되는 바운딩 박스를 형성할 수 있다. 다시 말해, 프로세 서는 크기가 큰 1 객체는 큰 바운딩 박스를, 크기가 작은 객체는 작은 바운딩 박스를 형성할 수 있다. 한편, 탐지된 객체들이 겹친 경우에도, 프로세서는 하나의 객체와 다른 하나의 객체 각각의 크기에 대응되 는 바운딩 박스를 형성할 수 있다. 또한, 탐지된 객체들이 완전히 겹쳐 앞에 보이는 객체가 뒤에 보이는 객체에 포함되는 경우에도, 프로세서는 각각의 객체마다 바운딩 박스를 형성할 수 있다. 도 4를 참조하면, 프로세서는 탐지된 객체를 특정할 수 있다. 예를 들어, 프로세서는 객체 특정 모델을 이용하여 실시간 영상에 포함된 객체를 탐지할 수 있 다. 프로세서는 탐지된 객체의 수와 크기에 대응되는 바운딩 박스를 형성하여 객체를 특정 할 수 있다. 여기에서, 도 4의 바운딩 박스는 설명의 편의를 위하여 사각형으로 도시되어 있으나, 바운딩 박스의 형태는 이에 한정되지 아니하며, 원형, 다각형 또는 객체의 형태에 대응되는 선으로 형성될 수 있다. 또한, 선박의 특성상 항해 중에는 기상 환경 또는 해상 환경에 따라 선박이 앞뒤로 흔들리는 피칭(pitching)이 나 좌우로 흔들리는 롤링(rolling) 현상이 일어날 수 있다. 따라서, 프로세서가 탐지한 객체가 실시간 항 해 영상에 객체의 일부분만 포함되거나, 객체가 일정 시간동안(또는 적어도 하나의 프레임에서) 사라지는 경우 가 있는데, 프로세서는 탐지된 객체를 추적함으로써 객체가 일부만 보이거나, 객체가 사라졌다가 다시 나 타난 경우에도 동일한 객체로 특정할 수 있다. 예를 들어, 프로세서는 추적 알고리즘을 이용하여 실시간으로 획득된 영상프레임들을 추적하고, 영상 프레 임들에 포함된 객체들의 연관관계에 기초하여 프레임 내에 포함된 객체들이 동일한 객체인지 여부를 판단할 수 있다. 도 5는 일 실시예에 따른 추적 알고리즘을 이용하여, 탐지된 객체를 획득된 영상의 각 프레임별로 추적하는 일 예를 설명하기 위한 도면이다. 이하, 도 5를 참조하여, 프로세서가 탐지된 객체를 추적하여 각 프레임에 포함된 객체들이 동일한 객체임을 판단하는 일 예를 설명한다. 도 5를 참조하면, 상술한 바와 같은 선박의 움직임으로 인해, 실시간 영상에 포함되는 각 프레임들에 객체가 동 일하게 포함되지 않을 수 있다. 다시 말해, 제 1 프레임에는 객체의 전부가 포함되었다가, 제 2 프레임에는 상기 객체의 일부만 포함 될 수 있고, 다시 제 3 프레임에는 상기 객체의 전부가 포함될 수 있다. 또한, 도 5에는 도시되지 않았으 나, 실시간 영상의 어느 한 프레임에는 객체의 전부가 포함되었다가, 바로 다음 프레임에는 객체가 아예 포함되 지 않을 수 있다. 따라서, 프로세서는 추적 알고리즘을 이용하여 객체를 획득된 영상의 각 프레임별로 추적함으로써 제 2 프 레임에 객체의 일부만 포함되거나 아예 포함되지 않았더라도, 제 1 프레임, 제 2 프레임 및 제 3 프레임에 포함되는 객체들이 모두 동일한 객체임을 판단할 수 있다. 예를 들어, 프로세서는 프레임별로 추적된 객체들 사이의 연관관계에 기초하여 프레임별로 추적된 객체들 이 동일한 객체인지 여부를 판단할 수 있다. 구체적으로, 프로세서는 프레임별로 추적된 객체들의 특징 (feature) 연관 값이 기 설정된 값 이상인 경우에 각 프레임별로 추적된 객체들이 모두 동일한 객체인 것으로 판단할 수 있다. 여기에서, 프로세서는 칼만 필터, deep SORT 추적 알고리즘 중 적어도 어느 하나를 이용하여 객체들의 특 징 연관 값을 기반으로 동일한 객체임을 판단수 있다. 예를 들어, 프로세서는 선박의 롤링 또는 피칭으로 인해 제2 프레임에서 객체가 일부만 포함되거나 아예 포함되지 않았더라도, 상술한 칼만 필터, deep SORT 중 적어도 어느 하나를 이용하여 제1 프레임과 제3 프레임에서 탐지된 객체가 제 2 프레임에서 탐지된 객체와 동일한 객체임을 판단할 수 있다. 구체적으로, 상술한 바와 같이, 선박은 피칭 또는 롤링으로 인해 움직임이 크고, 탐지된 객체 역시 해상에 존재 하는 바 움직임이 크다. 이로 인해, 객체 특정 모델에 입력되는 항해 영상의 각 프레임들은 동일한 객체임에도 객체에 대한 정보의 차이가 클 수 있다. 즉, 객체 특정 모델에 입력되는 입력 데이터는, 연속되는 프레임들임에도 불구하고, 탐지된 객체가 프레임에 전부 보였다가, 일부만 보였다가, 아예 안보일 수도 있다. 따라서, 프로 세서는 추적 알고리즘을 이용함으로써 연속되는 프레임들 사이의 통일성을 유지하여 탐지된 객체를 안정적 으로 추적(tracking) 할 수 있다. 예를 들어, 프로세서는 제2 프레임에서의 객체 정보가 없더라도, 추적 알고리즘 이용하여 탐지된 객 체를 항해 영상의 각 프레임별로 추적할 수 있고, 제1 프레임에서 추적된 객체의 정보와 제3 프레임 에서 추적된 객체의 정보 사이의 연관관계(구체적으로, 특징 연관 값)에 기초하여 각 객체들이 동일한 객체인지 여부를 판단할 수 있다. 따라서, 프로세서는 탐지된 객체를 바운딩 박스를 형성하여 특정할 수 있다. 예를 들어, 프로세서는 형성된 바운딩 박스를 이용하여 객체를 포함하는 객체 영역을 생성할 수 있다. 여 기에서, 객체 영역이란, 객체의 정보를 획득하기 위해 이용되는 영역일 수 있으며, 2차원 또는 3차원의 영역일 수 있다. 다시 말해, 실시간 영상은 객체를 바라보는 평면적인 이미지들인 바, 영상에 포함된 객체의 깊이 정보 를 획득하기 위해 객체 영역을 이용할 수 있다. 도 6은 일 실시예에 따른 객체 영역을 설명하기 위한 도면이다. 이하, 도 6을 참조하여, 객체 영역의 예들을 설 명한다. 예를 들어, 프로세서는 바운딩 박스를 이용하여 상기 객체에 대응되는 객체 영역을 생성할 수 있다. 객체 영역은 제1 센서로부터 획득된 프레임에 기초하여 생성될 수 있다. 객체 영역은 3D 또는 2D로 형성될 수 있다. 객체 영역이 3차원인 경우(600(a)), 프로세서는 영상 프레임상에서 특정된 객체의 위치에 기초하여 객체 영역 를 생성할 수 있다. 여기에서, 3차원의 객체 영역은 절두체(view frustum)일 수 있으며, 절두체 는 카메라의 시야 영역을 설명하기 위한 것으로써, 원뿔 또는 사각뿔에서 뿔 부분을 잘라낸 형태일 수 있다. 다 시 말해, 객체 영역은 영상 프레임과 투영면 사이에 존재하는 육면체일 수 있다. 즉, 절두체를 이용하여 평면적으로 보여지는 화면을 입체적으로 설명할 수 있다. 예를 들어, 프로세서는 객체의 영상 프레임상 위치에 기초하여 투영면에 객체를 투영시킬 수 있 다. 여기에서, 영상 프레임과 투영면 사이에 형성되는 일정한 형태가 객체 영역일 수 있다. 여 기에서, 객체 영역은, 객체의 깊이 정보를 획득할 수 있는 2차원 또는 3차원의 영역일 수 있다. 예를 들어, 프로세서는 영상 프레임 상에서 객체를 특정하는 바운딩 박스를 투영면에 투영시킬 수 있다. 따라서, 프로세서는 영상 프레임과 투영면 사이에 여섯 개의 면이 사각형인 육면체를 생성할 수 있고, 생성된 육면체가 객체 영역일 수 있다. 여기에서, 투영면이란, 객체 또는 바운딩 박스 중 어느 하나를 투영시킨 가상의 면을 의미할 수 있으며, 제 2 센서의 종류에 기초하여 영상 프레임과 투영면 사이의 거리가 결정될 수 있다. 다시 말해, 제 2 센서가 3차원의 객체 정보를 획득할 수 있는 레이더 또는 라이다인 경우, 제 2 센서를 이용하여 객체의 정보를 획득할 수 있는 거리는 한계가 있는 바, 객체의 깊이 정보를 획득할 수 있는 객체 영역의 거리도 제 2 센서에 따라서 결정될 수 있다. 또한, 객체 영역이 2차원인 경우(600(b)), 프로세서는 객체를 포함하는 객체 영역을 생성할 수 있다. 여기에서, 2차원의 객체 영역은 3차원의 객체 영역을 위에서 바라본 탑뷰(top view) 형태일 수 있다. 예를 들어, 프로세서는 영상 프레임 상에서 객체를 특정하는 바운딩 박스를 가상의 면에 투영시킬 수 있다. 다만, 제 2 센서가 2차원의 객체 정보를 획득할 수 있는 AIS인 경우, 제 2 센서를 이용하여 객체의 정보 를 획득할 수 있는 거리는 한계가 없는 바, 바운딩 박스를 투영시킬 수 있는 가상의 면이 존재하지 않을 수 있 다. 따라서, 프로세서는 영상 프레임으로부터 일정 길이가 떨어진 영역을 2차원 객체 영역으로 생성할 수 있다. 또한, 2차원의 객체 영역의 깊이는 일반적인 선박의 길이 및 환경 정보에 기초하여 결정될 수 있다. 즉, 일반적인 선박의 길이는, 대형선과 소형선을 구분하여 결정될 수 있고, 환경 정보는 자선 주변에 객체의 수, 지형, 기상 환경, 해상 환경 등에 의하여 결정될 수 있다. 따라서, 프로세서는 객체 영역의 깊이 를 객체가 대형선인 경우에는 길게, 소형선인 경우에는 짧게 결정할 수 있다. 또한, 프로세서는 객체 영역의 깊이를 자선 주변의 객체가 적은 경우에는 길게, 객체가 많은 경우에는 짧게 결정할 수 있다. 도 7은 일 실시예에 따른 객체 영역을 생성하여 획득한 포인트 데이터를 이용하여 객체의 정보를 획득하는 방법 의 일 예를 설명하기 위한 도면이다. 이하, 도 7을 참조하여, 프로세서가 제 2 센서로부터 포인트 데이터를 획득하는 일 예를 설명한다. 도 7은 도 2의 단계(S240)를 상세하게 설명하는 도면일 수 있다. 예를 들어, 프로세서는 제 1 센서(또는 EO 카메라)를 이용하여 획득한 영상 프레임에 포함되는 객체 를 탐지하고, 탐지된 객체를 바운딩 박스를 이용하여 특정할 수 있다. 또한, 프로세서는 제 2 센서를 이용하여 획득한 영상 프레임에 포함되는 모든 객체들의 포인트 데이터를 획득할 수 있다. 포인트 데이터는 라이더로부터 출력되는 포인트 클라우드 데이터 및 레이더로부터 출력되는 포인트 데이터 중 적어도 하나를 포함할 수 있다. 일 예로서, 프로세서는 객체 영역에 포함된 객체와 대응되며, 제 2 센서로부터 획득된 포인트 클라우드 데 이터의 중간 값을 이용하여 객체의 정보를 산출할 수 있다. 또한, 객체 영역에 포함된 객체가 복수개인 경우, 프로세서는 객체 영역에 포함된 복수개의 객체들에 각각 대응되는 제 2 센서로부터 획득된 포인트 클라우 드 데이터 중 우세 포인트 클라우드 데이터의 중간 값을 이용하여 복수개의 객체들 중 자선과 가까운 객체의 정 보를 산출할 수 있다. 여기에서, 프로세서는 포인트 클라우드 데이터가 기 설정된 수 이상인 경우에 포인 트 클라우드 데이터의 중간 값을 산출할 수 있다. 다른 예로서, 프로세서는 객체 영역에 포함된 객체와 대응되며, 제 2 센서로부터 획득된 포인트 데이터의 중간 값을 이용하여 객체의 정보를 산출할 수 있다. 또한, 객체 영역에 포함된 객체가 복수개인 경우, 프로세서 는 객체 영역에 포함된 복수개의 객체들에 각각 대응되는 제 2 센서로부터 획득된 포인트 데이터 중 우세 포인트 데이터의 중간 값을 이용하여 복수개의 객체들 중 자선과 가까운 객체의 정보를 산출할 수 있다. 여기에 서, 프로세서는 포인트 데이터가 기 설정된 수 이상인 경우에 포인트 데이터의 중간 값을 산출할 수 있다. 도 8은 일 실시예에 따른 객체 영역 및 제 2 센서로부터 획득된 데이터를 이용하여 특정된 객체의 정보를 산출 하는 방법의 일 예를 설명하기 위한 도면이다. 이하, 도 8을 참조하여, 프로세서가 포인트 데이터를 이용하여 객체의 정보를 산출하는 일 예를 설명한다. 먼저, 도 7을 참조하여 상술한 바와 같이, 도 8에서도 마찬가지로 포인트 데이터는 라이더로부터 출력되는 포인 트 클라우드 데이터 및 레이더로부터 출력되는 포인트 데이터 중 적어도 하나를 포함할 수 있다. 예를 들어, 객체 영역(810(a))에 포함된 객체가 하나인 경우(800(a)), 프로세서는 도 7을 참조하여 상술한 바와 같이, 객체와 대응되는 포인트 데이터(또는 포인트 클라우드 데이터)(820(a))를 제 2 센서를 이용하여 획 득 할 수 있다. 여기에서, 포인트 데이터(Point Data) 또는 포인트 클라우드 데이터(Point Cloud Data)란, 3차 원 공간에서 점들의 집합으로 나타낸 정보로써, 일반적으로 (x, y, z) 좌표로 표현될 수 있다. 예를 들어, 프로 세서는 생성된 객체 영역(810(a)) 내에 존재하는 객체의 포인트 데이터(또는 포인트 클라우드 데이터) (820(a)), 즉 객체의 (x, y, z) 좌표를 획득할 수 있다. 따라서, 프로세서는 획득한 포인트 데이터(또는 포인트 클라우드 데이터) (820(a))를 이용하여 객체의 정 보를 획득할 수 있다. 일 예로서, 프로세서는 객체의 모든 포인트 데이터(또는 포인트 클라우드 데이 터)(820(a))의 평균 값을 산출하여 객체의 정보를 획득할 수 있다. 다른 예로서, 프로세서는 모든 포인트 데이터(또는 포인트 클라우드 데이터)(820(a))의 중간 값을 산출하여 객체의 정보를 획득할 수 있다. 그리고, 객체 영역(810(b))에 포함된 객체가 둘 이상인 경우(800(b)), 프로세서는 자선과 가까운 제 1 객 체의 포인트 데이터(또는 포인트 클라우드 데이터)(820(b))와 제 1 객체보다 멀리 있는 제 2 객체의 포인트 데 이터(또는 포인트 클라우드 데이터)(830(b))를 모두 획득할 수 있다. 다만, 제 2 객체는 제 1 객체에 의하여 일 부가 가려진 상태일 수 있는 바, 제 2 객체의 포인트 데이터(또는 포인트 클라우드 데이터)(830(b))도 일부만 획득될 수 있다. 이 경우에는, 프로세서는 제 1 객체의 포인트 데이터(또는 포인트 클라우드 데이터)(820(b))와 제 2 객체 의 포인트 데이터(또는 포인트 클라우드 데이터)(830(b)) 중 더 우세한 포인트 데이터(또는 포인트 클라우드 데 이터)를 이용하여 객체의 정보를 산출할 수 있다. 따라서, 프로세서는 자선과 더 가까운 위치에 존재하는 제 1 객체의 포인트 데이터(또는 포인트 클라우드 데이터)(820(b))가 더 우세한 포인트 데이터(또는 포인트 클 라우드 데이터)라고 판단할 수 있고, 제 1 객체의 정보를 산출할 수 있다. 또한, 도 8에는 도시되지 않았지만 객체 영역이 2차원인 경우, 프로세서는 제 2 센서로부터 획득된 위치 및 크기 데이터를 이용하여 객체의 정보를 산출할 수 있다. 다시 말해, 프로세서는 제 1 센서를 이용하여획득한 영상에 포함되는 객체가, 객체 영역에 대응되며 제 2 센서를 이용하여 획득된 위치 및 크기 데이터와 기 설정된 값 이상 동일하다고 판단된 경우에는 해당 객체의 정보를 산출할 수 있다. 여기에서, 객체의 정보는 객체의 거리 및 방위각 중 적어도 하나를 포함할 수 있으며, 속도를 더 포함할 수도 있다. 또한, 프로세서는 객체의 정보가 표시된 모니터링 영상을 제공할 수 있다. 도 9는 일 실시예에 따른 객체의 정보가 표시된 모니터링 영상의 일 예를 설명하기 위한 도면이다. 이하, 도 9를 참조하여, 프로세서가 객체의 정보가 표시된 모니터링 영상을 제공하는 일 예를 설명한다. 도 9를 참조하면, 프로세서는 특정된 객체와 특정된 객체의 정보가 표시된 모니터링 영상 을 제공할 수 있다. 예를 들어, 프로세서는 자선과 객체 간의 거리, 자선 기준의 객체의 방위각, 객체의 좌표 값, 객체의 속도, 객체의 선수각 등을 포함하는 객체 정보를 표시할 수 있고, 객체 정보가 표시된 모니터링 영상을 사용자에게 제공할 수 있다. 또한, 도 9에는 설명의 편의를 위하여, 객체의 거리 및 방위각만이 객체 정보로 표시되어 있으나, 프로세 서는 상술한 이외의 정보들도 표시되는 모니터링 영상을 제공할 수 있다. 도 10은 일 실시예에 따른 퓨전된 센서 정보를 이용하는 항해 보조 장치의 구성의 일 예를 설명하기 위한 블록 도이다. 도 10에 도시된 바와 같이, 일 실시예에 따른 퓨전된 센서정보를 이용하는 항해 보조 장치는, 자선에 설치되며 적어도 카메라 및 GPS를 포함하는 항해 관련된 복수의 센서를 포함하는 센서부, 복수의 센서 의 종류를 확인하고, 객체를 감지하도록 확인된 센서의 센싱 정보를 통합하고, 통합된 센싱 정보를 기반으로 감 지된 객체의 유무 및 종류를 판단하는 센서 퓨전부, 판단된 객체와 자선의 특징을 비교하여 자선의 충돌 위험도를 산출하고, 산출된 충돌위험도에 따라 회피 경로를 결정하는 충돌회피경로 설정부 및 결정된 회 피 경로 및 판단된 객체의 종류를 운항 중인 실시간 화면에 융합하여 표시하는 디스플레이부를 포함할 수 있다. 또한, 도 1의 센서부는 도 10 내지 도 11의 센서부는 동일한 센서부를 의미할 수 있으며, 도 1의 디 스플레이부는 도 10의 디스플레이부와 동일한 디스플레이부를 의미할 수 있다. 또한, 충돌회피경로 설정부에서 설정된 경로와 자선의 엔진 상태에 따라 충돌회피경로 설정부의 엔 진 제어 명령을 변환하는 엔진 조작부를 더 포함할 수 있다. 일 실시예로서, 센서 퓨전부는 복수의 센서의 종류를 확인하고, 객체를 감지하도록 카메라, GPS 및 추가적으로 설치된 사실이 확인된 센서의 센싱 정보를 통합시킬 수 있다. 본 발명의 일 실시예에 따른 복수의 센서는 레이더(RADAR, RAdio Detection And Ranging, 1012), 전자해도 (ENC, Electronic Navigational Chart, 1013), 라이다(Lidar, Light Detection And Ranging, 1014), 선박자동 식별장치(AIS, Automatic Identification System, 1016), 소나(Sonar, 1017), 관성측정장치(IMU, Inertial Measurement Unit, 미도시), 및 자선 DB(DataBase, 미도시) 중 적어도 하나를 포함할 수 있다. 자선에 설치될 수 있는 복수의 센서는 이에 한정되지 않고, 선박에 설치되는 센서 종류는 제한되지 않는다. 또한, 본 발명의 일 실시예에 따른 센서 퓨전부는, 도 1에 점선으로 표시된 바와 같이, 복수의 센서에 포 함되어 사용 가능한 센서 종류를 사전에 등록 받을 수 있다. 다시 말해, 선박마다 상이한 센서가 설치될 수 있 으며, 센서 퓨전부는 선박에 설치되어 사용이 가능한 센서들을 사전에 등록 받고, 사용 가능한 센서의 센 서 데이터만 수집하여 센싱 정보를 퓨전시킬 수 있다. 예를 들어, 자선이 카메라, 라이다 및 GPS가 설치된 소형선일 경우, 센서 퓨전부는 카 메라의 영상, 라이다의 포인트 클라우드 데이터, GPS가 수신한 자선의 위치를 획득하여, 자 선의 위치 인근에 대한 객체를 검출하고, 카메라 및 라이다의 동일한 객체에 대한 검출은 통합하고, 상이한 객체에 대한 검출은 남길 수 있다. 예를 들어, 카메라와 라이다에서 타선을 검출하였고, 검출된 각각의 타선이 기설정된 일정 범위 내 에 존재할 경우, 센서 퓨전부는 타선을 동일한 타선으로 판단하고 검출된 센싱 정보를 하나로 퓨전할 수 있으며, 충돌회피경로 설정부는 하나의 타선만을 우회하는 회피 경로를 설정할 수 있다.다시 말해, 카메라와 라이다는 타선, 해안구조물 등을 포함하는 장애물을 검출할 수 있다. 일 실시예로서, 사용 가능한 센서 종류를 사전에 등록 받기 위해서, 사용자가 별도의 사용자 인터페이스(1200, 도 13 참조)를 통해 설치된 센서를 입력할 수 있다. 또는, 다른 일 실시예로서, 센서 퓨전부가 센서부에 메시지를 브로드캐스팅하고, 센서부에 설치된 복수의 센서로부터 메시지에 대한 응답을 수신하여 수신된 응답에 따라 사용 가능한 센서를 등록할 수도 있다. 여기 기재되지 않았더라도 기타 사용 가능한 센서 종류를 확인할 수 있는 다른 임의의 방법을 적용할 수도 있다. 따라서, 센서 퓨전부가 카메라 및 GPS를 포함하는 사용 가능한 센서 종류를 사전에 확인하고, 확인된 사용 가능한 센서로부터 센싱 정보를 획득할 수 있다. 본 발명의 일 실시예에 따른 센서 퓨전부는 딥러닝 기반의 학습 모듈 또는 확률적 추론 방법을 이용하여 사용 가능한 센서로부터 획득한 센싱 정보를 통합할 수 있다. 사용 가능한 센서로부터 획득한 복수의 센서 데이터를 기초로 확률적 추론을 적용하여 하나의 객체에 대해 하나 의 센싱 정보를 획득할 수 있다. 복수의 센서는 각자의 방식으로 객체를 감지하며, 감지된 객체가 상이한 객체 일지 동일한 객체일지를 확률적 추론하여 센싱 정보를 통합할 수 있다. 또한, 사용 가능한 센서 중에서도 영상을 이용하여 영상 기반 객체 검출을 수행하는 센서는 센서로부터 획득한 영상에서 딥러닝 기반 객체 추적 알고리즘을 통해 객체를 추적할 수 있다. 구체적으로, 센서 퓨전부는, GPS로부터 자선의 위치를 판단하고, 카메라가 촬영한 영상으로 부터 자선의 전방 객체 정보를 획득할 수 있다. 더하여, 사용 가능한 센서가 AIS일 경우, AIS로부터 자선의 위치를 기준으로 설정 거리 내에 존재 하는 주변 해당 장비 보유 객체 정보를 획득할 수 있다. AIS는 해당 장비를 보유한 선박, 부표 등의 위치 와 운동을 파악할 수 있는 센서로서, AIS를 이용하여 AIS를 구비한 이동 객체 정보를 획득할 수 있 다. 사용 가능한 센서가 레이더일 경우, 레이더로부터 자선의 주변 고정 객체 및 이동 객체의 정보를 획득할 수 있다. 사용 가능한 센서가 전자해도일 경우, 전자해도로부터 주행 가능한 경로 후보군을 획득하도록 자선 의 인근 해역 정보를 획득할 수 있다. 예를 들어, 전자해도는 해안선, 등심선, 수심, 등대나 등부표와 같 은 항로표지 등의 정보를 포함한 것으로, 전자해도를 이용하여 해양 영역은 물론 육지 영역을 포함한 고 정 객체 정보를 획득할 수 있다. 사용 가능한 센서가 라이다일 경우, 라이다로부터 포인트 클라우드 데이터 기반 객체 정보를 획득 할 수 있다. 사용 가능한 센서가 소나일 경우, 소나로부터 해저 지형 또는 수중 장애물 정보를 획득할 수 있다. 사용 가능한 센서가 IMU 및 자선 DB일 경우, IMU 및 자선 DB로부터 자선의 운동 상태, 조종성 및 최초 경로 설 정 정보 중 적어도 하나를 획득할 수 있다. IMU로부터 자선의 운동 상태를 획득하고, 자선 DB로부터 최초 경로 설정 정보를 획득할 수 있으며, 또는 IMU로부터 자선의 운동 상태를 획득하고 자선 DB로부터 조종성을 획득하여 현재 주행 상태에 따른 이후 주행 상태를 예측할 수 있다. 다시 말해, 사용 가능한 센서에 따라 센서의 종류에 맞춰 센싱을 수행하며, 획득한 센싱 정보를 상호 비교하여 객체를 선별하고 추적할 수 있다. 예를 들어, 레이더 및 AIS이 기 설정된 일정 범위 내에서 객체를 검출할 경우, 객체는 하나의 객체 를 레이더 및 AIS이 각각 검출한 것으로 판단하고 센서 데이터를 통합할 수 있다. 또한, 레이더가 육지에 위치한 장애물을 검출한 경우, 객체는 자선의 주행과 무관한 데이터로 전자해도 의 센서 데이터와 통합하여 레이더가 검출한 객체 정보를 삭제할 수 있다. 또는, 소나가 해저 지형 또는 수중 장애물 정보를 획득하여 수중 장애물을 검출한 경우, 다른 센서들이 검출한 수상 장애물 정보와 통합하여 수상 또는 수중에서 주행을 방해하는 장애물 정보를 한 번에 제공할 수 있 다. 또는, 라이다가 객체를 검출하였으나, 자선의 주행과 무관한 객체일 경우 검출한 객체 정보를 삭제할 수 있다. 상술한 과정을 통해 센싱 정보를 서로 대조하여 센싱 오차를 줄이고 주행 관련된 장애물 정보를 선별할 수 있다. 센서 퓨전부는 입력된 센서의 종류에 따라 센싱 정보를 센서의 정확도를 기반으로 확률적 추론을 수행하 도록 하여 입력된 센서의 종류가 달라지더라도 정확한 센싱 정보를 출력할 수 있다. 하나의 센서가 아니라 상이 한 방식으로 센싱을 수행하는 복수의 센서를 이용하여 객체가 존재하는지를 추론하여 불확실성을 수치적인 확실 도로 나타내는 것으로, 센서부의 각각의 센서 데이터를 종합적으로 고려했을 때 실제할 가능성이 높은 객 체의 유무 또는 객체의 종류를 출력할 수 있다. 도 11은 일 실시예에 따른 퓨전 센서부의 구성의 일 예를 설명하기 위한 블록도이다. 도 11에 도시된 바와 같이, 센서 퓨전부는 센서부의 각 센서로부터 획득한 센서 데이터를 각 인터 페이스 모듈(1021, 1022, 1023, 1024, 1025, 1026, 1027)를 통해 동일한 형식의 데이터로 변환하고, 중복된 센 서 데이터를 정리하는 데이터 분석 및 융합을 수행하여 센싱 정보를 획득할 수 있다. 데이터 분석 및 융합된 센싱 정보에서 장애물이 될 객체를 선별하고 추적할 수 있다. 선박의 센서 부에 의해 감지된 객체에 대해 단순히 경고만 발하거나 사용자가 정확히 선택한 경우에만 객체를 추적하 는 것이 아니라, 센서부의 센서 데이터를 분석 및 융합하여 객체를 선별한 후, 선별된 객체를 지속적으로 추적할 수 있다. 예를 들어, 객체가 이동 객체인 선박일 경우, 선박의 속도, 이동 방향, 자선의 이동 경로와 겹치는 지점 등을 추적할 수 있다. 상대 선박의 움직임에 따라 운동 상태를 추정하여 현재 풍향 또는 조류 등의 환경 정보에서 상 대 선박을 추적할 수 있다. 객체 선별 및 추적한 결과 데이터를 충돌회피경로 설정부 또는 디스플레이부에 맞춰 데이터 변환을 수행할 수 있다. 각 센서 인터페이스 모듈(1021, 1022, 1023, 1024, 1025, 1026, 1027), 데이터 분석 및 융합, 객체 선별 및 추적, 데이터 변환은 센서 퓨전부의 기능을 논리적으로 분할한 것으로, 이는 하나의 컴퓨 팅 장치에서 통합하여 구현될 수도 있고, 각 논리 기능에 따라 분할 유닛에서 각각 구현될 수도 있다. 본 발명의 일 실시예에 따른 상기 충돌회피경로 설정부는, 센서 퓨전부가 판단한 객체의 유무 및 종류에 따라 회피 경로 후보를 결정하고, 자선의 조종성을 고려하여 최종 회피 경로를 결정하며, 결정된 최종 회피 경로에 따라 엔진 제어 명령을 생성할 수 있다. 도 12은 일 실시예에 따른 충돌회피경로 설정부의 제어 단계의 일 예를 설명하기 위한 흐름도이다. 구체적으로, 도 12에 도시된 바와 같이, 충돌회피경로 설정부는 획득한 센싱 정보에 따라 충돌 위험도를 산출(S1310)할 수 있다. 객체 선별 및 추적된 센싱 정보에 따라 자선과 객체의 근접 거리, 이동 방향, 경로의 중첩 등을 고려하여 확률 기반 충돌 가능성에 따라 충돌 위험도를 산출할 수 있다. 또한, 산출된 충돌 위험도에 따라 자선 정보 기반 제어 전략을 선택할 수 있다(S1320). 예를 들어, 전방에 해양 구조물이 있어 충돌 위험도가 상당히 높을 경우 자선은 회피 경로를 설정하여 우회하는 전략을 설정할 수 있다. 반면, 전방에 타선이 있고 타선이 자선보다 소형선박일 경우 타선이 선박안전기준에 따라 우회할 것인 바 자신 의 조종성을 고려하여 우회경로를 설정하지 않는 제어 전략을 설정할 수 있다. 그리고나서, 우회하는 전략을 선택한 경우, 회피 경로 후보 결정 및 회피 경로 후보 평가를 수행하여(S1330), 최종 회피 경로 결정할 수 있다(S1340). 예를 들어, 전방에 해수면 근처에 산호초 등의 해양구조물이 있을 경우 선체 바닥에 충돌 우려가 있는 바, 복수 의 우회 경로 중 더 먼 거리로 회피하는 우회 경로를 최종 회피 경로로 결정할 수 있으며, 반대의 경우 연료 소 모율을 줄이기 위해 최단 우회 경로를 최종 회피 경로로 결정할 수 있다. 일 실시예로서, 충돌회피경로 설정부는 복수의 회피 경로 후보를 고려하고 그 중 최적의 회피 경로 후보 를 최종 회피 경로로 결정할 수 있다. 항해 상황에 따라 국제해상충돌예방규칙을 고려한 회피 경로 후보, 연료 비 최소 사용을 위한 회피 경로 후보, 충돌 해소 확률이 가장 높은 회피 경로 후보 등을 도출하고, 현 자선 상 황에 따른 복수의 파라미터를 이용하여 최적의 회피 경로를 결정 및 디스플레이부를 통해 제시할 수 있다. 다시 말해, 충돌 회피 경로가 다수의 시나리오에 따라 설정되고 그 중 최적의 회피 경로를 결정하여 사용 자에게 제공할 수 있다. 따라서, 센서 퓨전부에서 퓨전된 센싱 정보를 제공하면, 충돌회피경로 설정부에서 객체의 유무 및 종류에 따라 충돌 위험도를 산출하고, 산출된 충돌 위험도 및 자선의 조종성을 고려하여 회피 경로를 최종 결정 할 수 있다. 충돌회피경로 설정부는 충돌 위험도 산출에서 센서부에서 획득한 해양 환경 정보 등을 추가적으로 고려하여 회피 경로를 최종 결정할 수도 있다. 또한, 충돌회피경로 설정부는 결정된 최종 회피 경로에 따라 엔진 제어 명령을 생성할 수 있다. 도 13에 도시된 바와 같이, 충돌회피경로 설정부로부터 엔진 제어 명령을 전달받는 엔진 조작부는 자선의 엔진 상태에 따라 상기 충돌회피경로 설정부의 엔진 제어 명령을 변환할 수 있다. 충돌회피경로 설정부가 최종 회피 경로를 결정하면, 최종 회피경로로 가기 위한 주행 방향, 속도 등에 맞 춰 자선의 엔진을 엔진 조작부를 통해 조작할 수 있다. 다시 말해, 충돌회피경로 설정부가 최종 회피 경로를 결정하면 최종 회피 경로를 추종하도록 엔진 을 제어하여야 하며, 엔진을 제어하기 위한 제어 명령을 엔진 조작부가 전달할 수 있다. 또한, 엔진 조작부는 충돌회피경로 설정부를 통해 엔진 제어 명령을 수신할 수도 있으며, 또는 사 용자 인터페이스를 통해 사용자 입력을 직접 수신할 수도 있다. 사용자 인터페이스는 조이스틱, 스티어링 휠, 쓰로틀 등 엔진을 제어하는 조종 장치를 포함할 수 있으며, 상술한 디스플레이부를 포함할 수도 있다. 사용자 인터페이스가 디스플레이부일 경우, 사용자 인터페이스를 통해 최종 회피 경로를 표시 하면서 동시에 엔진 제어 명령을 입력받을 수 있다. 엔진 조작부가 사용자 인터페이스로부터 사용자 입력을 수신 받고, 충돌회피경로 설정부로부 터 엔진 제어 명령을 수신 받아 사용자 입력과 엔진 제어 명령을 동시에 수신할 경우, 사용자 입력에 상기 충돌 회피경로 설정부의 제어 명령보다 더 높은 우선순위를 부여하여 엔진을 조작할 수 있다. 도 13은 일 실시예에 따른 엔진 조작부의 제어 단계의 일 예를 설명하기 위한 흐름도이다. 도 13에 도시된 바와 같이, 사용자 입력을 확인하고, 사용자 입력이 있으면(도 13의 예), 사용자 입력에 따라 엔진조작 명령을 생성하여 엔진을 제어할 수 있다. 또는 사용자 입력이 없으면(도 13의 아니오), 충돌회 피경로 설정부의 엔진 제어 명령을 변환하여 엔진조작 명령을 생성하고 엔진을 제어할 수 있다. 즉, 사용자가 센서 퓨전부의 센싱 정보가 현시된 디스플레이부를 통해 충돌회피경로 설정부 와 다른 결정을 내린 경우 사용자 선택을 우선하여 사용자에게 엔진 접근을 허용하는 것이다. 이때, 본 발명의 일 실시예에 따른 디스플레이부는 충돌회피경로 설정부 및 센서 퓨전부로부 터 수신한 데이터를 증강 현실 영상으로 출력할 수 있다. 일 실시예로서, 증강 현실 영상을 출력하기 위한 별도의 장치를 통해 센서 퓨전 결과와 카메라의 영상을 기반으로 증강현실에 탐지된 물체를 출력할 수 있다. 또는 다른 일 실시예로서, 기존 선박에 장착되어 있는 MFD(Multi Function Display) 장치에 충돌회피경로 설정 부 및 센서 퓨전부로부터 수신한 데이터를 표시할 수도 있다. 한편, 본 발명의 일 실시예에 따른 퓨전된 센서 정보를 이용하는 항해 보조 방법은, 적어도 카메라 및 GPS를 포함하는 항해 관련된 복수의 센서를 포함하는 센서부가 구비된 퓨전된 센서 정보를 이용하 는 항해 보조 장치에서 수행될 수 있다. 도 14는 일 실시예에 따른 퓨전된 센서 정보를 이용하는 항해 보조방법의 일 예를 설명하기 위한 흐름도이다. 도 14에 도시된 바와 같이, 복수의 센서에 따라 사용 가능한 센서를 확인하는 단계(S1410), 객체를 감지하도록 카메라 및 GPS를 포함하는 확인된 사용 가능한 센서로부터 획득한 센싱 정보를 통합하고, 통합된센싱 정보를 기반으로 감지된 객체의 유무 및 종류를 판단하는 단계(S1420), 판단된 객체와 자선의 특징을 비교 하여 자선의 충돌위험도를 산출하고, 산출된 충돌위험도에 따라 회피 경로를 결정하는 단계(S1430) 및 결정된 회피 경로 및 판단된 객체의 종류를 운항 중인 실시간 화면에 융합하여 표시하는 단계(S1440)를 포함할 수 있으 며, 결정된 회피 경로를 추종하도록 엔진을 제어하기 위한 제어 명령을 엔진 조작부에 전달하는 단계 (S1450)를 더 포함할 수 있다. 본 발명의 일 실시예에 따른 상기 복수의 센서는 선박자동식별장치(AIS, Automatic Identification System), 레이더(RADAR, RAdio Detection And Ranging), 라이다(Lidar, Light Detection And Ranging), 관성측정장치 (IMU,Inertial Measurement Unit), 자선 DB(DataBase), 소나(Sonar) 및 전자해도(ENC, Electronic Navigational Chart) 중 적어도 하나를 포함할 수 있다. 다시 말해, 모든 선박에 설치되는 카메라 및 GPS를 제외하고 선박의 종류 및 선주의 선택에 따라 설치된 센서 종류는 모두 상이한 바, 충돌회피경로 설정부가 센서 퓨전부의 센싱 정보에 따라 충돌 회피 경로를 결정하기 전에 사용 가능한 센서 정보를 획득할 수 있다. 본 발명의 일 실시예에 따른 상기 통합된 센싱 정보를 기반으로 감지된 객체의 유무 및 종류를 판단하는 단계 (S1420)는, GPS로부터 자선의 위치를 판단하고, 카메라가 촬영한 영상으로부터 자선의 전방 객체 정보를 획득하는 단계를 포함할 수 있다. 더하여, 사용 가능한 센서에 AIS가 있는 경우, AIS로부터 자선의 위치를 기준으로 설정 거리 내에 존재하는 주변 해당 장비 보유 객체 정보를 획득할 수 있다. 또는, 사용 가능한 센서에 레이더가 있는 경우, 레이더로부터 자선의 주변 고정 객체 및 이동 객체 의 정보를 획득할 수 있다. 또는, 사용 가능한 센서에 전자해도가 있는 경우, 전자해도로부터 주행 가능한 경로 후보군을 획득 하도록 자선의 인근 해역 정보를 획득할 수 있다. 또는, 사용 가능한 센서에 라이다가 있는 경우, 라이다로부터 포인트 클라우드 데이터 기반 객체 정보를 획득할 수 있다. 또는, 사용 가능한 센서에 소나가 있는 경우, 소나로부터 해저 지형 또는 수중 장애물 정보를 획득 할 수 있다. 또는, 사용 가능한 센서에 IMU 및 자선 DB가 있는 경우, IMU 및 자선 DB로부터 자선의 운동 상태, 조종성 및 최 초 경로 설정 정보 중 적어도 하나를 획득할 수 있다. 다시 말해, 사용 가능한 센서의 종류에 따라 상술한 단계를 추가적으로 수행하며, 획득한 정보를 상호 비교하여 객체를 선별하고 분류하는 단계를 더 포함할 수 있다. 이상 상술한 내용과 중복되는 내용은 생략하기로 한다. 본 발명의 일 실시예에 따른 산출된 충돌위험도에 따라 회피 경로를 결정하는 단계(S1430)는, 충돌회피경로 설 정부가 복수의 회피 경로 후보를 생성하는 단계 및 생성된 회피 경로 후보 중 현 자선 상황에 따른 복수 의 파라미터를 이용하여 최적의 회피 경로를 결정하는 단계를 더 포함할 수 있다. 다시 말해, 항상 동일한 상황 에서 동일한 하나의 경로가 결정되는 것이 아니라, 충돌회피경로 설정부가 현 자선 상황에 따른 복수의 파라미터를 고려하여 현 자선 상황에 따라 유연한 회피 경로를 탐색 및 결정할 수 있다. 디스플레이부는 최적의 회피 경로로 결정된 최종 회피 경로를 제시할 수 있다. 또한, 다른 실시예로서, 충돌회피경로 설정부가 결정하여 디스플레이부에 제시한 최종 회피 경로의 추종을 사용자가 원치 않을 경우, 사용자 인터페이스(1200. 도 13 참조)를 통해 사용자 입력을 입력 받아 수동 조작을 통해 자선을 조종할 수 있다. 또한, 본 발명의 일 실시예에 따른 결정된 회피 경로 및 판단된 상기 객체의 종류를 운항 중인 실시간 화면에 융합하여 표시하는 단계(S1440)는, 통합된 상기 센싱 정보를 기반으로 감지된 객체의 유무 및 종류 또는 결정된 회피 경로를 수신하여 증강현실 영상으로 출력할 수 있다. 생성된 경로를 단순히 화면에 표시하는 것이 아니라 사용자가 전방 주시를 유지하면서도 센싱 정보 또는 생성된 경로를 확인할 수 있도록 증강 현실을 이용할 수 있다. 더하여, 본 발명의 일 실시예에 따른 제어 명령을 엔진 조작부에 전달하는 단계(S1450)는 별도의 사용자 입력 없이 결정된 회피 경로를 추종하도록 대응하는 엔진 조작이 수행되도록 엔진 제어 명령을 생성하여 전달할 수 있다. 다시 말해, 최종 회피 경로 및 검출된 객체를 실시간 화면에 표시하는 동시에, 최종 회피 경로에 따라 자선이 운항하도록 대응하는 엔진 제어 명령을 엔진 조작부를 통해 생성 및 전달할 수 있다. 한편, 상술한 방법은 컴퓨터에서 실행될 수 있는 프로그램으로 작성 가능하고, 컴퓨터로 읽을 수 있는 기록매체 를 이용하여 상기 프로그램을 동작시키는 범용 디지털 컴퓨터에서 구현될 수 있다. 또한, 상술한 방법에서 사용 된 데이터의 구조는 컴퓨터로 읽을 수 있는 기록매체에 여러 수단을 통하여 기록될 수 있다. 상기 컴퓨터로 읽 을 수 있는 기록매체는 마그네틱 저장매체(예를 들면, 롬, 램, USB, 플로피 디스크, 하드 디스크 등), 광학적 판독 매체(예를 들면, 시디롬, 디브이디 등)와 같은 저장매체를 포함한다. 본 실시예와 관련된 기술 분야에서 통상의 지식을 가진 자는 상기된 기재의 본질적인 특성에서 벗어나지 않는 범위에서 변형된 형태로 구현될 수 있음을 이해할 수 있을 것이다. 그러므로 개시된 방법들은 한정적인 관점이 아니라 설명적인 관점에서 고려되어야 하며, 권리 범위는 전술한 설명이 아니라 특허청구범위에 나타나 있으며, 그와 동등한 범위 내에 있는 모든 차이점을 포함하는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2025-0000927", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 복수의 센서들을 이용하여 객체의 정보를 획득하는 시스템의 일 예를 설명하기 위한 도면이다. 도 2는 일 실시예에 따른 복수의 센서들을 이용하여 객체의 정보를 획득하는 방법의 일 예를 설명하기 위한 흐 름도이다. 도 3은 일 실시예에 따른 객체 특정 모델을 이용하여 객체를 탐지하고, 탐지된 객체를 특정하는 방법의 일 예를 설명하기 위한 흐름도이다. 도 4는 일 실시예에 따른 영상에 포함된 적어도 하나의 객체를 탐지하여 특정하는 방법의 일 예를 설명하기 위 한 도면이다. 도 5는 일 실시예에 따른 추적 알고리즘을 이용하여, 탐지된 객체를 획득된 영상의 각 프레임별로 추적하는 일 예를 설명하기 위한 도면이다. 도 6은 일 실시예에 따른 객체 영역을 설명하기 위한 도면이다. 도 7은 일 실시예에 따른 객체 영역을 생성하여 획득한 포인트 데이터를 이용하여 객체의 정보를 획득하는 방법 의 일 예를 설명하기 위한 도면이다. 도 8은 일 실시예에 따른 객체 영역 및 제 2 센서로부터 획득된 데이터를 이용하여 특정된 객체의 정보를 산출 하는 방법의 일 예를 설명하기 위한 도면이다. 도 9는 일 실시예에 따른 객체의 정보가 표시된 모니터링 영상의 일 예를 설명하기 위한 도면이다. 도 10은 일 실시예에 따른 퓨전된 센서 정보를 이용하는 항해 보조 장치의 구성의 일 예를 설명하기 위한 블록 도이다. 도 11는 일 실시예에 따른 퓨전 센서부의 구성의 일 예를 설명하기 위한 블록도이다. 도 12은 일 실시예에 따른 충돌회피경로 설정부의 제어 단계의 일 예를 설명하기 위한 흐름도이다. 도 13는 일 실시예에 따른 엔진 조작부의 제어 단계의 일 예를 설명하기 위한 흐름도이다. 도 14는 일 실시예에 따른 퓨전된 센서 정보를 이용하는 항해 보조방법의 일 예를 설명하기 위한 흐름도이다."}
