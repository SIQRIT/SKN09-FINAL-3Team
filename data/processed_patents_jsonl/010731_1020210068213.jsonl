{"patent_id": "10-2021-0068213", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0148918", "출원번호": "10-2021-0068213", "발명의 명칭": "언어 모델에 기반한 단어 벡터 획득 방법, 장치, 기기 및 기록매체", "출원인": "베이징 바이두 넷컴 사이언스 앤 테크놀로지 코.,", "발명자": "리, 쩐"}}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "언어 모델에 기반하여 단어 벡터를 획득하는 방법으로서,적어도 두 개의 제1 샘플 텍스트 언어 자료(first sample text corpus) 중 각각의 제1 샘플 텍스트 언어 자료를각각 언어 모델에 입력하고, 상기 언어 모델에 의해 각각의 상기 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크(first word mask)의 컨텍스트 벡터(context vector)를 출력하는 단계;각각의 상기 제1 샘플 텍스트 언어 자료 중 각각의 상기 제1 단어 마스크에 대하여 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행렬(first word vector parameter matrix)에 기반하여 각각의상기 제1 단어 마스크의 제1 확률 분포 행렬(first probability distribution matrix)을 얻으며, 각각의 상기제1 단어 마스크의 컨텍스트 벡터 및 제2 단어 벡터 파라미터 행렬(second word vector parameter matrix)에기반하여 각각의 상기 제1 단어 마스크의 제2 확률 분포 행렬(second probability distribution matrix)을 얻으며, 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완전 연결 행렬(full-connection matrix)에 기반하여각각의 상기 제1 단어 마스크의 제3 확률 분포 행렬(third probability distribution matrix)을 얻는 단계-상기 제1 단어 벡터 파라미터 행렬은 사전 훈련된 상기 언어 모델에 대응하는 단어 벡터 파라미터 행렬이며, 상기제2 단어 벡터 파라미터 행렬은 사전 훈련된 다른 언어 모델에 대응하는 단어 벡터 파라미터 행렬임-;각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계; 및상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 제1 사전 훈련 완료 조건을 만족할 때까지 상기 언어 모델 및 상기 완전 연결 행렬을 훈련시키며, 훈련된 상기 제1 단어 벡터 파라미터 행렬, 상기 제2 단어 벡터 파라미터 행렬 및 상기 완전 연결 행렬의 집합을 단어 벡터의 집합으로 하는 단계;를 포함하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 방법."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제1 확률 분포 행렬을 얻는 단계는： 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제1 단어 벡터 파라미터 행렬을 곱하여 상기 제1 단어 마스크의 제1 확률 분포 행렬을 얻는 단계를 포함하며； 및/또는，상기 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제2 단어 벡터 파라미터 행렬에 기반하여 각각의 상기제1 단어 마스크의 제2 확률 분포 행렬을 얻는 단계는： 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기제2 단어 벡터 파라미터 행렬을 곱하여 상기 제1 단어 마스크의 제2 확률 분포 행렬을 얻는 단계를 포함하며；및/또는 ，상기 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완전 연결 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제3 확률 분포 행렬을 얻는 단계는： 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 완전 연결 행렬을 곱하여 상기 제1 단어 마스크의 제3 확률 분포 행렬을 얻는 단계를 포함하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 방법."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,공개특허 10-2021-0148918-3-각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계는：각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬을 합하여 각각의 상기 제1 단어 마스크의 총 확률 분포 행렬을 얻는 단계；각각의 상기 제1 단어 마스크의 상기 총 확률 분포 행렬 중 확률 값을 정규화 처리하여 각각의 상기 제1 단어마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값을 얻는 단계；및각각의 상기 제1 단어 마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값에 기반하여 각각의 상기제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계; 를 포함하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 방법."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 내지 제3항 중 어느 한 항에 있어서,상기 제1 단어 마스크를 포함하는 제1 샘플 텍스트 언어 자료를 상기 언어 모델에 입력하고 상기 언어 모델에의해 상기 제1 단어 마스크의 컨텍스트 벡터를 출력하는 단계의 전에：제2 사전 훈련 완료 조건을 만족할 때까지 초기화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬을 훈련시켜 상기 언어 모델 및 상기 제1 단어 벡터 파라미터 행렬을 얻는 단계;를 더 포함하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 방법."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서， 제2 사전 훈련 완료 조건을 만족할 때까지 상기 초기화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬을 훈련시키는 단계는：코퍼스 중의 사전 설정 텍스트 언어 자료를 미리 사용하여 상기 초기화 언어 모델을 사전 훈련시키는 단계；제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어를 제2 단어 마스크로 각각 대체하여 적어도 하나의 제2 단어마스크를 포함하는 제2 샘플 텍스트 언어 자료를 얻는 단계；상기 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 상기 초기화 언어 모델에 입력하여, 상기 초기화 언어 모델에 의해 상기 적어도 하나의 제2 단어 마스크 중 각각의 상기 제2 단어 마스크의 컨텍스트 벡터를 출력하는 단계；각각의 상기 제2 단어 마스크의 컨텍스트 벡터 및 상기 초기화 제1 단어 벡터 파라미터 행렬에 기반하여 각각의상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하는 단계；및상기 적어도 하나의 제2 단어 마스크에 대응하는 단어 벡터에 기반하여 제2 사전 훈련 완료 조건을 만족할 때까지 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 파라미터 행렬을 훈련시키는 단계; 를 포함하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 방법."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어를 제2 단어 마스크로 각각 대체하는 단계는：상기 제2 샘플 텍스트 언어 자료를 단어 분할하고, 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트 언어 자료중 적어도 하나의 단어 중 각각의 단어를 하나의 제2 단어 마스크로 각각 대체하는 단계; 공개특허 10-2021-0148918-4-를 포함하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 방법."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 내지 제3항 중 어느 한 항에 있어서,상기 언어 모델은 지식 강화 시맨틱 표현(ERNIE) 모델을 포함하며；및/또는 ，상기 다른 언어 모델은 연속 단어 모음(CBOW) 모델을 포함하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 방법."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "언어 모델에 기반하여 단어 벡터를 획득하는 장치로서,적어도 두 개의 제1 샘플 텍스트 언어 자료(first sample text corpus) 중 각각의 제1 샘플 텍스트 언어 자료를수신하여 언어 모델에 입력하여, 각각의 상기 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크(first word mask)의 컨텍스트 벡터(context vector)를 출력하는데 사용되는 언어 모델；각각의 상기 제1 샘플 텍스트 언어 자료 중 각각의 상기 제1 단어 마스크에 대해 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행렬(first word vector parameter matrix)에 기반하여 각각의상기 제1 단어 마스크의 제1 확률 분포 행렬(first probability distribution matrix)을 얻으며, 각각의 상기제1 단어 마스크의 컨텍스트 벡터 및 제2 단어 벡터 파라미터 행렬(second word vector parameter matrix)에기반하여 각각의 상기 제1 단어 마스크의 제2 확률 분포 행렬(second probability distribution matrix)을 얻으며, 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완전 연결 행렬(full-connection matrix)에 기반하여각각의 상기 제1 단어 마스크의 제3 확률 분포 행렬(third probability distribution matrix)을 얻는데 사용되는 획득 유닛 - 상기 제1 단어 벡터 파라미터 행렬은 사전 훈련된 상기 언어 모델에 대응하는 단어 벡터 파라미터 행렬이고, 상기 제2 단어 벡터 파라미터 행렬은 사전 훈련된 다른 언어 모델에 대응하는 단어 벡터 파라미터행렬임-；각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는데 사용되는 제1 결정 유닛； 및상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 제1 사전 훈련 완료 조건을 만족할 때까지 상기 언어 모델 및 상기 완전 연결 행렬을 훈련시키며, 훈련된 상기 제1 단어 벡터 파라미터 행렬, 상기 제2 단어 벡터 파라미터 행렬 및 상기 완전 연결 행렬의 집합을 단어 벡터의 집합으로 하는 것에 사용되는 제1 훈련 유닛;을 포함하는 것을 특징으로 하는 언어 모델에 기반한 단어 벡터 획득 장치."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 획득 유닛은:각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제1 단어 벡터 파라미터 행렬을 곱하여 상기 제1 단어 마스크의 제1 확률 분포 행렬을 얻으며； 및/또는 ，각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제2 단어 벡터 파라미터 행렬을 곱하여 상기 제1 단어 마스크의 제2 확률 분포 행렬을 얻으며； 및/또는 ，각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 완전 연결 행렬을 곱하여 상기 제1 단어 마스크의 제3 확률 분포 행렬을 얻는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 장치.공개특허 10-2021-0148918-5-청구항 10 제8항에 있어서,각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬을 합하여 각각의 상기 제1 단어 마스크의 총 확률 분포 행렬을 얻는데 사용되는 합산 유닛； 및각각의 상기 제1 단어 마스크의 상기 총 확률 분포 행렬 중 확률 값을 정규화 처리하여 각각의 상기 제1 단어마스크에 대응하는 복수 개 단어 벡터의 복수 개 정규화 확률 값을 얻는데 사용되는 정규화 유닛；을 더 포함하며,상기 제1 결정 유닛은 각각의 상기 제1 단어 마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값에기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 장치."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항 내지 제10항 중 어느 한 항에 있어서,제2 사전 훈련 완료 조건을 만족할 때까지 초기화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬을 훈련시켜 상기 언어 모델 및 상기 제1 단어 벡터 파라미터 행렬을 얻는데 사용되는 제2 훈련 유닛을 더 포함하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 장치."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,코퍼스 중의 사전 설정 텍스트 언어 자료를 미리 사용하여 상기 초기화 언어 모델을 사전 훈련 학습시키는데 사용되는 사전 훈련 유닛；제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어를 제2 단어 마스크로 각각 대체하여 적어도 하나의 제2 단어마스크를 포함하는 제2 샘플 텍스트 언어 자료를 얻는데 사용되는 대체 유닛； 및각각의 상기 제2 단어 마스크의 컨텍스트 벡터 및 상기 초기화 제1 단어 벡터 파라미터 행렬에 기반하여 각각의상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하는데 사용되는 제2 결정 유닛；을 더 포함하고,상기 초기화 언어 모델은 상기 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 상기초기화 언어 모델에 입력하고 상기 초기화 언어 모델에 의해 상기 적어도 하나의 제2 단어 마스크 중 각각의 상기 제2 단어 마스크의 컨텍스트 벡터를 출력하는데 사용되며；상기 제2 훈련 유닛은 구체적으로 상기 적어도 하나의 제2 단어 마스크에 대응하는 단어 벡터에 기반하여 제2사전 훈련 완료 조건을 만족할 때까지 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 파라미터 행렬을훈련시키는데 사용되는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 장치."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서，상기 대체 유닛은： 상기 제2 샘플 텍스트 언어 자료를 단어 분할하고， 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어 중 각각의 단어를 하나의 제2 단어 마스크로 각각 대체하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 장치. 공개특허 10-2021-0148918-6-청구항 14 제8항 내지 제10항 중 어느 한 항에 있어서, 상기 언어 모델은 지식 강화 시맨틱 표현 (ERNIE) 모델을 포함하며； 및/또는 ，상기 다른 언어 모델은 연속 단어 모음 (CBOW) 모델을 포함하는 것을 특징으로 하는,언어 모델에 기반한 단어 벡터 획득 장치."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며,상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 명령이 상기적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제7항 중 어느 한항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는,전자 기기."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "컴퓨터 명령이 기억되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서,상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제7항 중 어느 한 항에 기재된 방법을 수행하도록 하는것을 특징으로 하는,기록 매체."}
{"patent_id": "10-2021-0068213", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "비 일시적 컴퓨터 판독 가능 기록 매체에 기억되어 있는 컴퓨터 프로그램에 있어서,상기 컴퓨터 프로그램은 상기 컴퓨터로 하여금 제1항 내지 제7항 중 어느 한 항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는,컴퓨터 프로그램."}
{"patent_id": "10-2021-0068213", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 출원은 언어 모델에 기반하여 단어 벡터를 획득하는 방법, 장치, 기기 및 기록매체를 개시하며 인공지능 중 자연어 처리"}
{"patent_id": "10-2021-0068213", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것이다. 구체적인 구현 방법은： 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 각 각의 제1 샘플 텍스트 언어 자료를 언어 모델에 입력하고 상기 언어 모델에 의해 각각의 상기 제1 샘플 텍스트 (뒷면에 계속) 대 표 도 - 도2"}
{"patent_id": "10-2021-0068213", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2021-0148918 언어 자료 중 제1 단어 마스크의 컨텍스트 벡터를 출력하며； 제1 단어 벡터 파라미터 행렬, 제2 단어 벡터 파라 미터 행렬 및 완전 연결 행렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하며； 상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언 어 모델 및 상기 완전 연결 행렬을 훈련시켜 단어 벡터를 얻는다. 본 출원은 단어 입도 학습으로 인한 정보 유출 위험을 피하고 언어 모델의 단어 시맨틱 정보에 대한 학습 능력을 강화하고 단어 벡터 수렴 속도를 빠르게 하며 훈련 효과를 향상시킨다. CPC특허분류 G06K 9/6277 (2013.01) G06N 3/0454 (2013.01) G06N 3/08 (2013.01)명 세 서 청구범위 청구항 1 언어 모델에 기반하여 단어 벡터를 획득하는 방법으로서, 적어도 두 개의 제1 샘플 텍스트 언어 자료(first sample text corpus) 중 각각의 제1 샘플 텍스트 언어 자료를 각각 언어 모델에 입력하고, 상기 언어 모델에 의해 각각의 상기 제1 샘플 텍스트 언어 자료 중 제1 단어 마스 크(first word mask)의 컨텍스트 벡터(context vector)를 출력하는 단계; 각각의 상기 제1 샘플 텍스트 언어 자료 중 각각의 상기 제1 단어 마스크에 대하여 각각의 상기 제1 단어 마스 크의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행렬(first word vector parameter matrix)에 기반하여 각각의 상기 제1 단어 마스크의 제1 확률 분포 행렬(first probability distribution matrix)을 얻으며, 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제2 단어 벡터 파라미터 행렬(second word vector parameter matrix)에 기반하여 각각의 상기 제1 단어 마스크의 제2 확률 분포 행렬(second probability distribution matrix)을 얻 으며, 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완전 연결 행렬(full-connection matrix)에 기반하여 각각의 상기 제1 단어 마스크의 제3 확률 분포 행렬(third probability distribution matrix)을 얻는 단계-상 기 제1 단어 벡터 파라미터 행렬은 사전 훈련된 상기 언어 모델에 대응하는 단어 벡터 파라미터 행렬이며, 상기 제2 단어 벡터 파라미터 행렬은 사전 훈련된 다른 언어 모델에 대응하는 단어 벡터 파라미터 행렬임-; 각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행 렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계; 및 상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 제1 사 전 훈련 완료 조건을 만족할 때까지 상기 언어 모델 및 상기 완전 연결 행렬을 훈련시키며, 훈련된 상기 제1 단 어 벡터 파라미터 행렬, 상기 제2 단어 벡터 파라미터 행렬 및 상기 완전 연결 행렬의 집합을 단어 벡터의 집합 으로 하는 단계; 를 포함하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 방법. 청구항 2 제1항에 있어서, 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제1 단 어 마스크의 제1 확률 분포 행렬을 얻는 단계는： 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제1 단 어 벡터 파라미터 행렬을 곱하여 상기 제1 단어 마스크의 제1 확률 분포 행렬을 얻는 단계를 포함하며； 및/또 는， 상기 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제2 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제2 확률 분포 행렬을 얻는 단계는： 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제2 단어 벡터 파라미터 행렬을 곱하여 상기 제1 단어 마스크의 제2 확률 분포 행렬을 얻는 단계를 포함하며； 및/또는 ， 상기 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완전 연결 행렬에 기반하여 각각의 상기 제1 단어 마스 크의 제3 확률 분포 행렬을 얻는 단계는： 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 완전 연결 행 렬을 곱하여 상기 제1 단어 마스크의 제3 확률 분포 행렬을 얻는 단계를 포함하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 방법. 청구항 3 제1항에 있어서,각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행 렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계는： 각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행 렬을 합하여 각각의 상기 제1 단어 마스크의 총 확률 분포 행렬을 얻는 단계； 각각의 상기 제1 단어 마스크의 상기 총 확률 분포 행렬 중 확률 값을 정규화 처리하여 각각의 상기 제1 단어 마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값을 얻는 단계；및 각각의 상기 제1 단어 마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값에 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계; 를 포함하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 방법. 청구항 4 제1항 내지 제3항 중 어느 한 항에 있어서, 상기 제1 단어 마스크를 포함하는 제1 샘플 텍스트 언어 자료를 상기 언어 모델에 입력하고 상기 언어 모델에 의해 상기 제1 단어 마스크의 컨텍스트 벡터를 출력하는 단계의 전에： 제2 사전 훈련 완료 조건을 만족할 때까지 초기화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬을 훈련시 켜 상기 언어 모델 및 상기 제1 단어 벡터 파라미터 행렬을 얻는 단계; 를 더 포함하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 방법. 청구항 5 제4항에 있어서， 제2 사전 훈련 완료 조건을 만족할 때까지 상기 초기화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬을 훈 련시키는 단계는： 코퍼스 중의 사전 설정 텍스트 언어 자료를 미리 사용하여 상기 초기화 언어 모델을 사전 훈련시키는 단계； 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어를 제2 단어 마스크로 각각 대체하여 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 얻는 단계； 상기 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 상기 초기화 언어 모델에 입력하 여, 상기 초기화 언어 모델에 의해 상기 적어도 하나의 제2 단어 마스크 중 각각의 상기 제2 단어 마스크의 컨 텍스트 벡터를 출력하는 단계； 각각의 상기 제2 단어 마스크의 컨텍스트 벡터 및 상기 초기화 제1 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하는 단계；및 상기 적어도 하나의 제2 단어 마스크에 대응하는 단어 벡터에 기반하여 제2 사전 훈련 완료 조건을 만족할 때까 지 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 파라미터 행렬을 훈련시키는 단계; 를 포함하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 방법. 청구항 6 제5항에 있어서, 상기 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어를 제2 단어 마스크로 각각 대체하는 단계는： 상기 제2 샘플 텍스트 언어 자료를 단어 분할하고, 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어 중 각각의 단어를 하나의 제2 단어 마스크로 각각 대체하는 단계; 를 포함하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 방법. 청구항 7 제1항 내지 제3항 중 어느 한 항에 있어서, 상기 언어 모델은 지식 강화 시맨틱 표현(ERNIE) 모델을 포함하며；및/또는 ， 상기 다른 언어 모델은 연속 단어 모음(CBOW) 모델을 포함하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 방법. 청구항 8 언어 모델에 기반하여 단어 벡터를 획득하는 장치로서, 적어도 두 개의 제1 샘플 텍스트 언어 자료(first sample text corpus) 중 각각의 제1 샘플 텍스트 언어 자료를 수신하여 언어 모델에 입력하여, 각각의 상기 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크(first word mas k)의 컨텍스트 벡터(context vector)를 출력하는데 사용되는 언어 모델； 각각의 상기 제1 샘플 텍스트 언어 자료 중 각각의 상기 제1 단어 마스크에 대해 각각의 상기 제1 단어 마스크 의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행렬(first word vector parameter matrix)에 기반하여 각각의 상기 제1 단어 마스크의 제1 확률 분포 행렬(first probability distribution matrix)을 얻으며, 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제2 단어 벡터 파라미터 행렬(second word vector parameter matrix)에 기반하여 각각의 상기 제1 단어 마스크의 제2 확률 분포 행렬(second probability distribution matrix)을 얻 으며, 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완전 연결 행렬(full-connection matrix)에 기반하여 각각의 상기 제1 단어 마스크의 제3 확률 분포 행렬(third probability distribution matrix)을 얻는데 사용되 는 획득 유닛 - 상기 제1 단어 벡터 파라미터 행렬은 사전 훈련된 상기 언어 모델에 대응하는 단어 벡터 파라미 터 행렬이고, 상기 제2 단어 벡터 파라미터 행렬은 사전 훈련된 다른 언어 모델에 대응하는 단어 벡터 파라미터 행렬임-； 각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행 렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는데 사용되는 제1 결정 유닛 ； 및 상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 제1 사 전 훈련 완료 조건을 만족할 때까지 상기 언어 모델 및 상기 완전 연결 행렬을 훈련시키며, 훈련된 상기 제1 단 어 벡터 파라미터 행렬, 상기 제2 단어 벡터 파라미터 행렬 및 상기 완전 연결 행렬의 집합을 단어 벡터의 집합 으로 하는 것에 사용되는 제1 훈련 유닛; 을 포함하는 것을 특징으로 하는 언어 모델에 기반한 단어 벡터 획득 장치. 청구항 9 제8항에 있어서, 상기 획득 유닛은: 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제1 단어 벡터 파라미터 행렬을 곱하여 상기 제1 단어 마 스크의 제1 확률 분포 행렬을 얻으며； 및/또는 ， 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제2 단어 벡터 파라미터 행렬을 곱하여 상기 제1 단어 마 스크의 제2 확률 분포 행렬을 얻으며； 및/또는 ， 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 완전 연결 행렬을 곱하여 상기 제1 단어 마스크의 제3 확 률 분포 행렬을 얻는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 장치.청구항 10 제8항에 있어서, 각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행 렬을 합하여 각각의 상기 제1 단어 마스크의 총 확률 분포 행렬을 얻는데 사용되는 합산 유닛； 및 각각의 상기 제1 단어 마스크의 상기 총 확률 분포 행렬 중 확률 값을 정규화 처리하여 각각의 상기 제1 단어 마스크에 대응하는 복수 개 단어 벡터의 복수 개 정규화 확률 값을 얻는데 사용되는 정규화 유닛； 을 더 포함하며, 상기 제1 결정 유닛은 각각의 상기 제1 단어 마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값에 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 장치. 청구항 11 제8항 내지 제10항 중 어느 한 항에 있어서, 제2 사전 훈련 완료 조건을 만족할 때까지 초기화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬을 훈련시 켜 상기 언어 모델 및 상기 제1 단어 벡터 파라미터 행렬을 얻는데 사용되는 제2 훈련 유닛 을 더 포함하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 장치. 청구항 12 제11항에 있어서, 코퍼스 중의 사전 설정 텍스트 언어 자료를 미리 사용하여 상기 초기화 언어 모델을 사전 훈련 학습시키는데 사 용되는 사전 훈련 유닛； 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어를 제2 단어 마스크로 각각 대체하여 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 얻는데 사용되는 대체 유닛； 및 각각의 상기 제2 단어 마스크의 컨텍스트 벡터 및 상기 초기화 제1 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하는데 사용되는 제2 결정 유닛； 을 더 포함하고, 상기 초기화 언어 모델은 상기 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 상기 초기화 언어 모델에 입력하고 상기 초기화 언어 모델에 의해 상기 적어도 하나의 제2 단어 마스크 중 각각의 상 기 제2 단어 마스크의 컨텍스트 벡터를 출력하는데 사용되며； 상기 제2 훈련 유닛은 구체적으로 상기 적어도 하나의 제2 단어 마스크에 대응하는 단어 벡터에 기반하여 제2 사전 훈련 완료 조건을 만족할 때까지 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 파라미터 행렬을 훈련시키는데 사용되는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 장치. 청구항 13 제12항에 있어서， 상기 대체 유닛은： 상기 제2 샘플 텍스트 언어 자료를 단어 분할하고， 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트 언어 자 료 중 적어도 하나의 단어 중 각각의 단어를 하나의 제2 단어 마스크로 각각 대체하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 장치. 청구항 14 제8항 내지 제10항 중 어느 한 항에 있어서, 상기 언어 모델은 지식 강화 시맨틱 표현 (ERNIE) 모델을 포함하며； 및/또는 ， 상기 다른 언어 모델은 연속 단어 모음 (CBOW) 모델을 포함하는 것을 특징으로 하는, 언어 모델에 기반한 단어 벡터 획득 장치. 청구항 15 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제7항 중 어느 한 항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는, 전자 기기. 청구항 16 컴퓨터 명령이 기억되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제7항 중 어느 한 항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는, 기록 매체. 청구항 17 비 일시적 컴퓨터 판독 가능 기록 매체에 기억되어 있는 컴퓨터 프로그램에 있어서, 상기 컴퓨터 프로그램은 상기 컴퓨터로 하여금 제1항 내지 제7항 중 어느 한 항에 기재된 방법을 수행하도록 하 는 것을 특징으로 하는, 컴퓨터 프로그램. 발명의 설명 기 술 분 야"}
{"patent_id": "10-2021-0068213", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 3, "content": "본 출원은 컴퓨터 기술분야 구체적으로 인공지능의 자연어 처리기술에 관한 것이고, 특히 언어 모델에 기반하여 단어 벡터를 획득하는 방법, 장치, 기기 및 기록매체에 관한 것이다."}
{"patent_id": "10-2021-0068213", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "중국어 자연어 처리(Natural Language Processing，NLP) 분야에서는 언어 모델의 자가 지도(self-supervision) 사전 훈련 학습(pre-training)에 대량의 비지도(unsupervised) 학습 텍스트가 사용되어 지도된(supervised) 작 업 데이터를 사용하여 언어 모델의 각각의 변수를 미세 조정(fine-tuning) 하며 이는 현재 NLP 분야의 고급 언 어 모델 훈련 기술이다. 종래 기술에서는 언어 모델의 자가 지도 사전 훈련 학습에서 언어 모델의 훈련 효과가 토크 나이저(tokenizer) 의 성능에 영향을 받는 것을 방지하기 위해 자가 지도 사전 훈련 언어 모델의 학습은 문자(letter) 입도를 기반 으로 수행되므로 언어 모델이 더 큰 시맨틱 입도(예: 단어(word))로 정보를 학습하기 어렵고 정보 유출 위험이 있어 언어 모델의 단어 자체에 대한 시맨틱 학습은 언어 모델의 예측 성능에 불리한 영향을 미친다."}
{"patent_id": "10-2021-0068213", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2021-0068213", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 출원의 복수 개 양태에 있어서, 언어 모델에 기반하여 단어 벡터 획득 방법, 장치, 기기 및 기록매체를 제공 하여 문자 입도 기반 학습으로 인한 정보 유출 위험을 방지하고 언어 모델의 단어 시맨틱 정보에 대한 학습 능 력을 향상시키고 단어 벡터 수렴 속도를 높이고 훈련 효과를 향상시킨다."}
{"patent_id": "10-2021-0068213", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 양태에 따르면 언어 모델에 기반하여 단어 벡터 획득 방법을 제공하며 상기 방법은： 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 각각의 제1 샘플 텍스트 언어 자료를 각각 언어 모델에 입력하고 상기 언어 모델에 의해 각각의 상기 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크의 컨텍스트 벡터를 출력하 며 ； 각각의 상기 제1 샘플 텍스트 언어 자료 중 각각의 상기 제1 단어 마스크에 대하여 각각의 상기 제1 단어 마스 크의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제1 확률 분포 행렬을 얻으며 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제2 단어 벡터 파라미터 행렬에 기반하여 각각 의 상기 제1 단어 마스크의 제2 확률 분포 행렬을 얻으며 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완 전 연결 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제3 확률 분포 행렬을 얻으며； 여기서 상기 제1 단어 벡터 파라미터 행렬은 사전 훈련된 상기 언어 모델에 대응하는 단어 벡터 파라미터 행렬이며 상기 제2 단어 벡 터 파라미터 행렬은 사전 훈련된 다른 언어 모델에 대응하는 단어 벡터 파라미터 행렬이며； 각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행 렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하며； 상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 제1 사 전 훈련 완료 조건을 만족할 때까지 상기 언어 모델 및 상기 완전 연결 행렬을 훈련시키며 훈련된 상기 언어 모 델, 상기 제1 단어 벡터 파라미터 행렬 및 상기 제2 단어 벡터 파라미터 행렬에 대응하는 단어의 단어 벡터를 얻는다. 제2 양태에 있어서, 언어 모델에 기반하여 단어 벡터를 획득하는 장치를 제공하며 상기 장치는： 언어 모델은 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 각각의 제1 샘플 텍스트 언어 자료를 수신하여 언어 모델에 입력하고 각각의 상기 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크의 컨텍스트 벡터를 출력하는데 사 용되며； 획득 유닛은 각각의 상기 제1 샘플 텍스트 언어 자료 중 각각의 상기 제1 단어 마스크에 대해 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제1 확률 분포 행렬을 얻으며 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제2 단어 벡터 파라미터 행렬에 기 반하여 각각의 상기 제1 단어 마스크의 제2 확률 분포 행렬을 얻으며 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완전 연결 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제3 확률 분포 행렬을 얻는데 사용되며； 여기서， 상기 제1 단어 벡터 파라미터 행렬은 사전 훈련된 상기 언어 모델에 대응하는 단어 벡터 파라미터 행 렬이고 상기 제2 단어 벡터 파라미터 행렬은 사전 훈련된 다른 언어 모델에 대응하는 단어 벡터 파라미터 행렬 이며; 제1 결정 유닛은 각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는데 사용되 며； 제1 훈련 유닛은 상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 제1 사전 훈련 완료 조건을 만족할 때까지 상기 언어 모델 및 상기 완전 연결 행렬을 훈련하는데 사용 되며 훈련된 상기 언어 모델, 상기 제1 단어 벡터 파라미터 행렬 및 상기 제2 단어 벡터 파라미터 행렬에 대응 하는 단어의 단어 벡터를 얻는다. 제3 양태에 있어서， 전자기기를 제공하며 상기 전자기기는： 적어도 하나의 프로세서； 및 상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서로 하여금 상기의 방면 및 임의의 가능 한 구현 방식을 수행하도록 하는 것을 포함한다. 제4 양태에 있어서, 컴퓨터 명령이 기억되어 있는 비 일시적 컴퓨터 판독 가능 기록매체를 제공하며, 상기 컴퓨 터 명령은 상기 컴퓨터로 하여금 상기 방면 및 임의의 가능한 구현 방식을 수행하도록 한다."}
{"patent_id": "10-2021-0068213", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기 기술 방안으로부터 알 수 있는 바, 본 출원 실시예는 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 각각 의 제1 샘플 텍스트 언어 자료를 언어 모델에 각각 입력하고 상기 언어 모델에 의해 각각의 상기 제1 샘플 텍스 트 언어 자료 중 제1 단어 마스크의 컨텍스트 벡터를 출력하며 각각의 상기 제1 샘플 텍스트 언어 자료 중 각각 의 상기 제1 단어 마스크에 대해 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행 렬에 기반하여 각각의 상기 제1 단어 마스크의 제1 확률 분포 행렬을 얻으며 각각의 상기 제1 단어 마스크의 컨 텍스트 벡터 및 제2 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제2 확률 분포 행렬을 얻으며 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완전 연결 행렬에 기반하여 각각의 상기 제1 단어 마 스크의 제3 확률 분포 행렬을 얻은 후 각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬에 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정 한다. 또한 상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반 하여 제1 사전 훈련 완료 조건을 만족할 때까지 상기 언어 모델 및 상기 완전 연결 행렬을 훈련시켜 훈련된 상 기 언어 모델, 상기 제1 단어 벡터 파라미터 행렬 및 상기 제2 단어 벡터 파라미터 행렬에 대응하는 단어의 단 어 벡터를 얻는다. 본 출원 실시예는 다른 언어 모델에 대응하는 제2 단어 벡터 파라미터 행렬을 도입함으로써 사전 훈련된 제1 단어 벡터 파라미터 행렬 및 제2 단어 벡터 파라미터 행렬에 동시에 기반하여 여러가지 고품질 단어 벡터를 결합하여 언어 모델 및 단어 벡터를 연합 훈련하여 언어 모델로 하여금 다중 소스 고품질의 단어 시맨틱 정보를 학습할 수 있도록 하며 언어 모델의 단어 시맨틱 정보 학습 능력을 강화하고 언어 모델의 예측 성능을 향상시킨다. 또한 본 출원에서 제공하는 기술 방안을 사용하여 여러가지 고품질 단어 벡터를 결합하여 언어 모델 및 단어 벡 터를 연합 훈련하여 언어 모델 및 단어 벡터의 수렴 속도를 빠르게 하고 훈련 효과를 향상시킨다. 또한 본 출원에서 제공하는 기술 방안을 사용하여 단어 마스크를 포함하는 샘플 텍스트 언어 자료를 사용하여 언어 모델 및 단어 벡터를 훈련시키며 문자 벡터에 비하여 단어 벡터에는 더욱 풍부한 시맨틱 정보 표현을 포함 하고 단어 마스크의 방식은 컨텍스트 모델링 단어 벡터를 기반으로 하므로 언어 모델의 단어 시맨틱 정보에 대 한 모델링을 강화하고 언어 모델의 단어 시맨틱 정보에 대한 학습 능력을 강화하며 문자에 기반한 완전 단어 커 버리지로 인한 정보 유출 위험을 효과적으로 방지할 수 있다. 본 부분에서 설명한 내용은 본 개시 내용의 실시예들의 핵심 또는 중요한 특징들을 식별하기 위한 것이 아니며, 본 개시 내용의 범위를 제한하려는 의도가 아님을 이해해야 한다. 본 개시 내용의 다른 특징은 다음 설명을 통 해 쉽게 이해될 것이다."}
{"patent_id": "10-2021-0068213", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 출원의 시범적인 실시예를 기술하는 바, 본 출원에 대한 이해를 돕기 위해 여기에는 본 출원 실시예의 다양한 세부 사항이 포함되며, 이러한 세부 사항을 단지 시범적인 것으로 간주해야 할 것이다. 따라서, 당업자는 본 발명의 범위 및 정신을 벗어나지 않는 전제 하에서, 여기서 설명되는 실시예에 대 해 다양한 변경 및 수정을 수행할 수 있음을 인식해야 한다. 마찬가지로, 명확성 및 간결성을 위하여 이하의 기 술에서는 잘 알려진 기능 및 구조의 기술을 생략하였다. 상술한 실시예는 전부 실시예가 아니라 본 출원 실시예의 일부이며 본 출원 실시예에 기반하여, 창의적인 작업 없이 당업자에 의해 획득된 다른 모든 실시예가 본 출원의 보호 범위 내에 속하는 것은 자명한 것이다. 설명해야 할 점은, 본 출원 실시예에 따른 단말기는 휴대 전화, 개인 정보 단말기(Personal Digital Assistant ，PDA), 무선 휴대 장치, 태블릿 컴퓨터（Tablet Computer）, 개인용 컴퓨터(Personal Computer， PC), MP3 플 레이어, MP4 플레이어, 웨어러블 장치（예: 스마트 안경, 스마트 시계, 스마트 팔찌 등）, 스마트 홈 장치 및 기타 스마트 장치를 포함할 수 있지만 이에 국한되지 않는다. 또한 본 명세서에서 \"및/또는\"이라는 용어는 관련 대상의 관련 관계를 설명할 뿐이며 세 가지 관계가 있을 수 있다. 즉, A 및/또는 B는 A만 존재하거나 A와 B가 동시에 존재하거나 B만 존재하는 세 가지 경우를 표시할 수 있다. 또한 이 텍스트의 \"/\"문자는 일반적으로 앞뒤의 관련 대상이 \"또는\" 관계에 있음을 나타낸다. 종래 기술은 언어 모델 자가 지도의 사전 훈련 학습에서 모두 문자(letter) 입도를 기반으로 수행되므로 언어 모델이 더 큰 시맨틱 입도（예를 들면 단어(word)）의 정보를 학습하기 어렵게 한다. 정보 유출 위험이 존재할 수 있으므로 언어 모델의 단어 자체에 대한 시맨틱 학습을 파괴하여 언어 모델의 예측 성능에 불리한 영향을 미 칠 수 있다. 예를 들면， 기존 언어 모델에서 ERNIE（Enhanced Representation from kNowledge IntEgration， 지식이 강화 된 시맨틱 표현）모델의 사전 훈련 학습에서 문자에 기반한 전체 단어 커버리지 방법을 사용하여 ERNIE 모델 학 습 실체의 표현을 학습하도록 한다. 그러나 문자의 전체 단어 커버리지 방법은 여전히 단어 벡터와 같이 더 큰 시맨틱 입도의 정보를 명시적으로 도입하지 않았다； 또한 정보 유출 위험이 존재할 수 있다. 예를 들면 “하얼 빈( )은 흑룡강（ )의 성도이다” 텍스트의 경우 “하( )”, “얼( )”, “빈( )” 세 개 문자 를 세 개 마스크（MASK）로 각각 교체하여 “[MASK][MASK][MASK]는 흑룡강의 성도이다”를 얻고 ERNIE 모델로 하여금 세 개 [MASK]가 “하( )”, “얼( )”, “빈( )” 세 개 문자를 학습하기를 희망하며 ERNIE 모델에 게 예측하려는 정보가 세 개 문자로 조성됨을 사전에 알려주는 것과 같으므로 이러한 정보는 모델의 단어 자체 에 대한 시맨틱의 학습을 파괴할 수 있다. 상술한 문제에 대하여 본 출원은 언어 모델에 기반한 단어 벡터를 획득하는 방법, 장치, 전자 기기 및 판독 가 능 기록매체를 개시하여 문자 입도에 기반한 학습으로 인한 정보 유출 위험을 피하고 언어 모델의 단어 시맨틱 정보에 대한 학습 능력을 강화하며 단어 벡터 수렴 속도를 높이고 훈련 효과를 향상시킨다. 도 1에 나타낸 바와 같이 본 출원 제1 실시예에 따른 개략도이다. 단계, 적어도 두 개의 제1 샘플 텍스트 언어 자료(first sample text corpus) 중 각각의 제1 샘플 텍스트 언어 자료를 언어 모델에 각각 입력하고 각각의 상기 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크의 컨텍스 트 벡터는 상기 언어 모델에 의해 출력된다. 단계, 각각의 상기 제1 샘플 텍스트 언어 자료 중 각각의 상기 제1 단어 마스크에 대하여 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제1 확률 분포 행렬을 얻고, 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제2 단어 벡터 파라미터 행렬에 기반 하여 각각의 상기 제1 단어 마스크의 제2 확률 분포 행렬을 얻으며, 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완전 연결 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제3 확률 분포 행렬을 얻는다. 여기서 상기 제1 단어 벡터 파라미터 행렬은 사전 훈련된 상기 언어 모델에 대응하는 단어 벡터 파라미터 행렬 이고 상기 제2 단어 벡터 파라미터 행렬은 사전 훈련된 다른 언어 모델에 대응하는 단어 벡터 파라미터 행렬이 다. 상기 완전 연결 행렬은 훈련되지 않은 초기화 된 행렬이다. 단계, 각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확 률 분포 행렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 얻는다. 단계, 상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반 하여 제1 사전 훈련 완료 조건을 만족할 때까지 상기 언어 모델 및 상기 완전 연결 행렬에 대해 훈련하여 훈련 된 상기 언어 모델 및 완전 연결 행렬을 얻으며 훈련된 상기 제1 단어 벡터 파라미터 행렬, 상기 제2 단어 벡터 파라미터 행렬 및 상기 완전 연결 행렬의 집합을 단어 벡터의 집합으로 한다. 구체적인 구현 과정에서 상기 제1 단어 벡터 파라미터 행렬 및 상기 제2 단어 벡터 파라미터 행렬의 파라미터 값이 변하지 않도록 유지될 수 있으며 상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델 및 상기 완전 연결 행렬에 대해 훈련한다. 즉, 상기 제1 사전 훈련 완료 조건을 만족할 때까지 상기 언어 모델 및 상기 완전 연결（Fully Connect，FC）행렬 중 파라미터 값 에 대해 조정한다. 본 출원 실시예에서 가능한 단어는 단어 목록을 통해 포함될 수 있다. 제1 단어 벡터 파라미터 행렬 및 제2 단 어 벡터 파라미터 행렬은 각각 복수 개 단어를 포함하는 단어 벡터의 행렬이고 제1 단어 벡터 파라미터 행렬 및 제2 단어 벡터 파라미터 행렬 중 단어 벡터는 단어 목록 중 각각의 단어의 단어 벡터이고 제1 단어 벡터 파라미 터 행렬과 제2 단어 벡터 파라미터 행렬의 차원은 동일하며 [단어 벡터 차원, 단어 목록 크기]로 표시할 수 있 다. 여기서 단어 목록 크기는 단어 목록에 포함된 단어의 수이다. 여기서 제1 확률 분포 행렬은 상기 제1 단어 마스크의 상기 제1 단어 벡터 파라미터 행렬을 기반으로 단어 목록 중 각각의 단어 벡터에 대응하는 확률 값을 표시하는데 사용되며 제2 확률 분포 행렬은 상기 제1 단어 마스크의 상기 제1 단어 벡터 파라미터 행렬에 기반 하여 단어 목록 중 각각의 단어 벡터에 해당하는 확률 값을 표시하는데 사용된다. 일 구체적인 예에서 훈련에 참여한 제1 단어 마스크에 대응하는 단어의 수（샘플 수라고도 한다）가 batch_size 이고, 각각의 단어의 단어 벡터 차원이 embedding_size이고, 단어 목록 크기가 vocab_size라고 가정하면, 상기 언어 모델에서 출력된 단어 벡터의 차원은 [batch_size, embedding_size]이고, 상기 제1 단어 벡터 파라미터 행 렬, 상기 제2 단어 벡터 파라미터 행렬 및 완전 연결 행렬의 차원은 모두 [embedding_size, vocab_size]이다. 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬의 차원은 모두 [batch_size, vocab_size]이다. 상기 제1 단어 벡터 파라미터 행렬이 사전 훈련된 상기 언어 모델에 대응하는 단어 벡터 파라미터 행렬이므로 단어 목록 중 각각의 단어의 단어 벡터를 정확하게 표시할 수 있다. 상기 제2 단어 벡터 파라미터 행렬은 사전 훈련된, 상기 다른 언어 모델에 대응하는 단어 벡터 파라미터 행렬이므로 또한 단어 목록 중 각각의 단어의 단 어 벡터를 정확하게 표시할 수 있다. 상기 언어 모델로 하여금 더욱 많고 더욱 풍부한 시맨틱 정보를 학습할 수 있도록 하기 위해 다른 언어 모델을 기반으로 훈련된 단어 벡터（제2 단어 벡터 파라미터 행렬）를 도입하여 언 어 모델을 추가로 훈련시킨다. 본 실시예에서 제1 단어 벡터 파라미터 행렬 및 제2 단어 벡터 파라미터 행렬이 각각 서로 다른 언어 모델에 대 응하는 사전 훈련된 단어 벡터 파라미터 행렬이므로 두 개의 서로 다른 언어 모델에 대응하는 단어 벡터 파라미 터 행렬 중 단어 벡터로 하여금 더욱 좋게 융합하도록 하기 위해 두 가지 서로 다른 언어 모델에 대응하는 단어 벡터 파라미터 행렬 융합 후의 단어 벡터를 지원, 보완하는 FC 행렬을 도입하여 상기 언어 모델의 두 가지 서로 다른 언어 모델에 대응하는 단어 벡터 파라미터 행렬에 대응하는 단어 벡터에 대한 학습 효과를 더욱 향상시킨 다. 여기서 상기 단계~는 반복 수행 과정일 수 있으며 수행 단계~를 반복 수행함으로써 언어 모델 및 완전 연결 행렬에 대한 훈련을 구현하며, 제1 사전 훈련 완료 조건을 만족할 경우 언어 모델 및 완전 연결 행렬에 대한 훈련이 완료된다. 훈련된 언어 모델은 단계~에 기반하여 텍스트 중 제1 단어 마스 크에 대응하는 단어 벡터를 정확하게 출력할 수 있다. 설명해야 할 점은 단계~의 수행 주체의 부분 또는 전부는 로컬 터미널에 위치한 응용일 수도 있고 또 는 로컬 터미널의 응용 중 플러그인 또는 소프트웨어 개발 키트（Software Development Kit，SDK） 등 기능 유 닛일 수도 있으며 또는 네트워크 측 서버에 위치한 프로세싱 엔진일 수 있으며 본 실시예에서 특별히 제한되지 않는다. 상기 응용은 단말기에 설치된 로컬 프로그램（nativeApp）일 수도 있고 또는 단말기 상의 브라우저의 웹 페이지 프로그램（webApp）일 수도 있음을 이해할 수 있으며 본 실시예에서는 제한되지 않는다. 본 실시예에서 다른 언어 모델에 대응하는 제2 단어 벡터 파라미터 행렬을 도입하고 사전 훈련된 제1 단어 벡터 파라미터 행렬 및 제2 단어 벡터 파라미터 행렬에 기반하여 여러가지 고품질 단어 벡터를 결합하여 언어 모델 및 단어 벡터를 연합 훈련하여 언어 모델로 하여금 다중 소스 고품질 단어 시맨틱 정보를 학습할 수 있도록 하 여 언어 모델의 단어 시맨틱 정보 학습 능력을 강화시키고 언어 모델의 예측 성능을 향상시킨다. 또한, 본 출원에서 제공하는 기술방안을 사용하여 여러가지 고품질 단어 벡터를 결합하여 훈련 언어 모델 및 단 어 벡터를 연합 훈련하여 언어 모델 및 단어 벡터의 수렴 속도를 빠르게 하여 훈련 효과를 향상시킨다. 또한, 본 출원에서 제공하는 기술방안을 사용하면 단어 마스크를 포함하는 샘플 텍스트 언어 자료를 사용하여 언어 모델 및 단어 벡터를 훈련시킨다. 문자 벡터에 비하여 단어 벡터에는 더욱 풍부한 시맨틱 정보 표현을 포 함하고 단어 마스크의 방식은 컨텍스트 모델링 단어 벡터를 기반으로 하므로 언어 모델의 단어 시맨틱 정보에 대한 모델링을 강화하고 언어 모델의 단어 시맨틱 정보에 대한 학습 능력을 강화하며 문자에 기반한 전체 단어 커버리지로 인한 정보 유출 위험을 효과적으로 방지할 수 있다. 선택적으로， 본 실시예의 가능한 구현방식에서， 단계에서 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제1 단어 벡터 파라미터 행렬에 대해 행렬 곱셈（ ，Matrix multiplication）을 수행하여 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제1 단어 벡터 파라미터 행렬 중 각각의 단어 벡터 사이의 관련성을 얻을 수 있으므로 상기 제1 단어 마스크의 제1 단어 벡터 파라미터 행렬 중 각각의 단어 벡터에 대응하는 제1 확률 분포 행렬을 얻는다. 선택적으로, 본 실시예의 가능한 구현방식에서 단계에서 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제 2 단어 벡터 파라미터 행렬에 대해 행렬 곱셈을 수행하여 각각의 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제2 단어 벡터 파라미터 행렬 중 각각의 단어 벡터 사이의 관련성을 얻을 수 있으므로 상기 제1 단어 마스크의 제1 단어 벡터 파라미터 행렬 중 각각의 단어 벡터에 대응하는 제2 확률 분포 행렬을 얻는다. 선택적으로, 본 실시예의 가능한 구현방식에서 단계에서 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 완 전 연결 행렬에 대해 행렬 곱셈을 수행하여 상기 제1 단어 마스크의 완전 연결 행렬 중 각각의 단어 벡터에 대 응하는 제3 확률 분포 행렬을 얻는다. 당해 구현 방식에서 제1 단어 마스크의 컨텍스트 벡터를 제1 단어 벡터 파라미터 행렬, 제2 단어 벡터 파라미터 행렬, 완전 연결 행렬과 각각 행렬 곱셈을 수행하는 방식을 통해 제1 단어 마스크의 제1 단어 벡터 파라미터 행 렬, 제2 단어 벡터 파라미터 행렬, 완전 연결 행렬에 각각 기반하고 복수 개 단어 벡터에 대응하는 확률 분포를 획득함으로써 제1 확률 분포 행렬, 제2 확률 분포 행렬, 제3 확률 분포 행렬에 기반하여 제1 단어 마스크에 대 응하는 단어 벡터를 종합적으로 결정한다. 선택적으로 본 실시예의 가능한 구현방식에서, 단계에서 각각의 상기 제1 단어 마스크의 상기 제1 확률 분 포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬을 합하여 각각의 상기 제1 단어 마스크에 대한 총 확률 분포 행렬을 얻은 후 각각의 상기 제1 단어 마스크에 대한 상기 총 확률 분포 행렬 중 확률 값에 대해 정규화 처리를 할 수 있다. 예를 들면 정규화된 지수 함수（softmax）를 통해 상기 총 확률 분포 행렬 중 확률 값에 대해 정규화 처리하여 각각의 상기 제1 단어 마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값을 얻으므로 나아가 각각의 상기 제1 단어 마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값에 기반하여 각각의 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정한다. softmax를 통해 상기 총 확률 분포 행렬 중 확률 값에 대해 정규화 처리를 수행하므로 제1 단어 벡터 파라미터 행렬 및 제2 단어 벡터 파라미터 행 렬은 또한 softmax 파라미터 행렬 또는 softmax 단어 벡터 파라미터 행렬이라고도 한다. 상기 구현 방식에서 제1 확률 분포 행렬, 제2 확률 분포 행렬 및 제3 확률 분포 행렬을 합하여 얻은 총 확률 분 포 행렬의 확률 값에 대해 정규화 처리하며 정규화된 확률 값에 기반하여 예를 들면 확률 값이 가장 높은 단어 벡터를 선택하여 제1 단어 마스크에 대응하는 단어 벡터로 할 수 있으므로 제1 단어 마스크에 대응하는 단어 벡 터를 정확하게 결정한다. 선택적으로 본 실시예의 일 가능한 구현 방식에서 상기 제1 사전 훈련 완료 조건은 실제 수요 설정에 기반하여 예를 들면 다음과 같은 임의의 하나 또는 복수 개를 포함할 수 있다： 제1 샘플 텍스트 언어 자료에 대응하는 언어 모델에 의해 출력되는 단어 벡터의 난이도（perplexity）가 제1 사 전 설정 임계 값에 도달한다； 상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크로 대체된 단어에는 단어 목록 중 복수 개 단어（부분 단어 또는 전체 단어일 수 있다）를 포함하며，단계를 통해 각각의 상기 제1 단어 마스크에 대응하는 복수 개 단어 벡터의 복수 개 정규화 확률 값을 얻은 후 훈련에 참여하는 전부 제1 단어 마스크의 확률 값은 가장 높은 정규화 확률 값이 최대화 된다； 언어 모델과 완전 연결 행렬의 훈련 차수（즉, 단계~의 반복 수행 차수）는 제2 사전 설정 임계 값에 도달한다. 선택적으로, 상기 제1 실시예 이전에 제2 사전 훈련 완료 조건을 만족할 때까지 초기화 언어 모델 및 초기화 제 1 단어 벡터 파라미터 행렬을 사전 훈련하여 상기 언어 모델 및 상기 제1 단어 벡터 파라미터 행렬을 얻을 수 있다. 본 실시예에서 초기화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬을 사전 훈련하여 훈련된 상기 언어 모 델 및 상기 제1 단어 벡터 파라미터 행렬을 얻은 후 다른 언어 모델의 단어 벡터 파라미터 행렬과 결합하여 상 기 언어 모델 및 상기 제1 단어 벡터 파라미터 행렬에 대해 추가로 훈련함으로써 상기 언어 모델 및 상기 제1 단어 벡터 파라미터 행렬의 훈련 속도를 빠르게 하고 훈련 효과를 향상시킬 수 있다. 도 2에 나타낸 바와 같이 본 출원 제3 실시예에 따른 개략도이다. 제2 사전 훈련 완료 조건을 만족할 때까지 초기화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬을 훈련하 는 것은 다음 방식을 통해 구현될 수 있다： 단계, 코퍼스( ) 중 사전 설정된 텍스트 언어 자료를 미리 사용하여 상기 초기화 언어 모델을 사전 훈련 학습 시킨다（pre-training）. 코퍼스 중 사전 설정 텍스트 언어 자료를 미리 사용하여 상기 언어 모델을 사전 훈련시키며 언어 모델로 하여금 텍스트 언어 자료 중 단어, 실체 및 실체 관계를 학습하도록 한다. 단계, 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어는 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 얻기 위해 각각 제2 단어 마스크로 대체된다. 여기서 제2 샘플 텍스트 언어 자료는 상기 제1 샘플 텍스트 언어 자료와 같을 수 있으며 서로 다를 수도 있다. 또한, 제2 샘플 텍스트 언어 자료는 코퍼스 중 사전 설정 텍스트 언어 자료 중 하나의 사전 설정 텍스트 언어 자료일 수 있으며 코퍼스 중 사전 설정 텍스트 언어 자료와 다른 기타 텍스트 언어 자료일 수도 있다. 선택적으로, 본 실시예의 일 가능한 구현 방식에서 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어를 제2 단 어 마스크로 각각 대체할 경우 여전히 문자에 기반하여 제2 단어 마스크의 컨텍스트를 표시한다. 단계, 상기 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 상기 초기화 언어 모 델에 입력하고 상기 초기화 언어 모델을 통해 상기 적어도 하나의 제2 단어 마스크 중 각각의 상기 제2 단어 마 스크의 컨텍스트 벡터를 출력한다. 단계, 각각의 상기 제2 단어 마스크의 컨텍스트 벡터 및 상기 초기화 제1 단어 벡터 파라미터 행렬에 기반 하여 각각의 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정한다. 단계, 상기 적어도 하나의 제2 단어 마스크에 대응하는 단어 벡터에 기반하여 제2 사전 훈련 완료 조건을 만족할 때까지 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 파라미터 행렬을 훈련시킨다. 여기서，상기 단계~는 반복 수행 과정일 수 있고, 단계~를 반복 수행하는 것을 통해 초기 화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬에 대한 훈련을 구현한다. 제2 사전 훈련 완료 조건을 만 족할 경우 초기화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬에 대한 훈련을 완료한다. 예를 들면， 일 구체예에서 코퍼스 중 사전 설정 텍스트 언어 자료를 미리 사용하여 초기화 언어 모델을 사전 훈련 학습시켜 “하얼빈”은 “흑룡강”의 성도 및 “하얼빈”은 눈의 도시라는 것을 학습하고, 제2 샘플 텍스 트 언어 자료 “하얼빈은 흑룡강의 성도” 중 “하얼빈”을 하나의 단어 마스크（MASK）로 대체하여 언어 모델 에 입력하고, 초기화 언어 모델에 의해 하나의 단어 벡터를 출력한다. 상기 초기화 언어 모델에 의해 출력된 단 어 벡터가 정확한지에 기반하여 초기화 언어 모델 및 초기화 제1 단어 벡터 파라미터 행렬을 훈련시키며 훈련 완료 후 언어 모델에 “[MASK]는 흑룡강의 성도”를 입력할 경우 언어 모델로 하여금 “하얼빈”이라는 단어 벡 터를 정확하게 출력할 수 있도록 한다. 본 실시예에서 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 언어 모델에 입력하여 초기화 언어 모 델에 의해 상기 제2 단어 마스크의 컨텍스트 벡터를 출력한 후 상기 제2 단어 마스크의 컨텍스트 벡터 및 초기화 제1 단어 벡터 파라미터 행렬에 기반하여 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정한다. 또한 상 기 제2 단어 마스크에 대응하는 단어 벡터에 기반하여 제2 사전 훈련 완료 조건을 만족할 때까지 상기 초기화 언어 모델 및 상기 제1 단어 벡터 파라미터 행렬을 훈련시키면 훈련된 언어 모델 및 제1 단어 벡터 파라미터 행 렬（제1 단어 벡터라고도 한다）을 얻을 수 있다. 문자 벡터에 비하여 단어 벡터에는 더욱 풍부한 시맨틱 정보 표현을 포함하고 더욱 큰 입도 시맨틱 정보 표현을 도입하며 단어 마스크의 방식을 사용하여 컨텍스트 모델링 단어 벡터에 기반하여 언어 모델의 단어 시맨틱 정보에 대한 모델링을 강화하고 언어 모델의 단어 시맨틱 정보 에 대한 학습 능력을 향상시킨다. 또한 본 실시예에서 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 사용하여 초기화 언어 모델을 훈 련시키므로 문자에 기반한 전체 단어 커버리지로 인한 정보 유출 위험을 효과적으로 피할 수 있다. 또한 본 실시예를 사용하여 초기화 언어 모델은 초기화 제1 단어 벡터 파라미터 행렬의 훈련과 결합되고 초기화 언어 모델을 초기화 제1 단어 벡터 파라미터 행렬과 연합 훈련시켜 언어 모델의 제1 단어 벡터 파라미터 행렬에 대응하는 단어 벡터의 수렴 속도를 빠르게 할 수 있으므로 훈련 효과를 향상시킨다. 선택적으로 본 실시예의 일 가능한 구현 방식에서 단계에서 상기 제2 샘플 텍스트 언어 자료를 단어 분할 할 수 있고 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어 중 각각의 단어 를 각각 하나의 제2 단어 마스크로 대체한다. 제2 마스크의 단어로 대체하는 외에 상기 제2 샘플 텍스트 언어 자료는 여전히 문자에 기반하여 제2 단어 마스크의 컨텍스트를 표현한다. 상기 구현 방식에서 제2 샘플 텍스트 언어 자료를 단어 분할하는 것을 통해 단어 분할 결과에 따라 제2 샘플 텍 스트 언어 자료 중 단어를 정확하게 결정할 수 있고 그중 하나 또는 복수 개 단어 중 각각의 단어를 하나의 제2 단어 마스크로 각각 대체함으로써 단어 마스크를 초기화 언어 모델을 훈련하는데 사용하도록 정확하게 설정할 수 있으며 초기화 언어 모델로 하여금 컨텍스트 모델링 단어 벡터에 기반하도록 하여 언어 모델의 단어 시맨틱 정보에 대한 모델링을 강화하고 언어 모델의 단어 시맨틱 정보에 대한 학습 능력을 향상시킨다. 선택적으로 본 실시예의 일 가능한 구현 방식에서 단계에서 상기 제2 단어 마스크의 컨텍스트 벡터 및 상 기 초기화 제1 단어 벡터 파라미터 행렬을 곱하여 각각의 상기 제2 단어 마스크의 컨텍스트 벡터와 상기 초기화 제1 단어 벡터 파라미터 행렬 중 각각의 단어 벡터 사이의 관련성을 얻을 수 있으므로 상기 제2 단어 마스크의 복수 개 단어 벡터에 대응하는 확률 값을 얻는다； 다음 상기 제2 단어 마스크의 복수 개 단어 벡터에 대응하는 확률 값에 대해 정규화 처리하여 상기 제2 단어 마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값을 얻으며 나아가 상기 복수 개 정규화 확률 값에 기반하여 상기 제2 단어 마스크에 대응하는 단어 벡터를 결 정한다. 구체적으로 정규화 확률 값이 가장 높은 단어 벡터를 상기 제2 단어 마스크에 대응하는 단어 벡터로 결 정한다. 일 구체적인 구현방식에서 1차 단어 목록을 통해 가능한 단어를 포함할 수 있다. 제1 단어 벡터 파라미터 행렬 에는 복수 개 단어 벡터의 구체적인 표현을 포함한다. 상기 제1 단어 벡터는 단어 목록 중 각각의 단어의 단어 벡터에 각각 대응되며 상기 제2 단어 마스크의 컨텍스트 벡터와 상기 초기화 제1 단어 벡터 파라미터 행렬을 곱 하여 각각의 상기 제2 단어 마스크의 컨텍스트 벡터와 상기 초기화 제1 단어 벡터 파라미터 행렬 중 각각의 단 어 벡터 사이의 관련성을 얻을 수 있으므로 상기 제2 단어 마스크의 단어 목록 중 각각의 단어 벡터에 대응하는 확률 값을 얻으며 상기 확률 값은 상기 제2 단어 마스크가 단어 벡터에 대응하는 확률이라는 것을 구현한다. 상기 구현 방식에서 제2 단어 마스크의 컨텍스트 벡터와 단어 벡터 파라미터 행렬을 곱하고 얻은 확률 값을 정 규화 처리하는 것을 통해 예를 들면 softmax를 통해 각각의 상기 제2 단어 마스크에 대응하는 복수 개 단어 벡 터의 확률 값을 정규화 처리하여 정규화된 확률 값에 기반하여 확률 값이 가장 높은 단어 벡터를 선택하여 제2 단어 마스크에 대응하는 단어 벡터로 하여 제2 단어 마스크에 대응하는 단어 벡터를 결정한다. softmax를 통해 각각의 상기 제2 단어 마스크의 복수 개 단어 벡터에 대응하는 확률 값을 정규화 처리할 경우 제1 단어 벡터 파 라미터 행렬은 softmax 파라미터 행렬 또는 softmax 단어 벡터 파라미터 행렬이라고도 할 수 있다. 선택적으로 본 실시예의 일 가능한 구현 방식에서 단계에서 상기 제2 사전 훈련 완료 조건은 실제 수요 설 정에 따라 예를 들면 다음과 같은 임의의 하나 또는 복수 개를 포함할 수 있다： 언어 모델에 의해 출력된 단어 벡터의 제2 샘플 텍스트 언어 자료에 대응하는 난이도（perplexity）가 제1 사전 설정된 임계 값에 도달한다； 복수 개 제2 샘플 텍스트 언어 자료를 이용하여 단계~를 수행하며 복수 개 제2 샘플 텍스트 언어 자 료 중 제2 단어 마스크로 대체되는 단어에는 단어 목록 중 복수 개 단어（부분 단어 또는 전체 단어일 수 있다）를 포함한다. 단계에서는 각각의 제2 단어 마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값을 얻은 후 훈련에 참여하는 전체 제2 단어 마스크의 확률 값이 가장 높은 정규화 확률 값을 최대화 한다； 상기 초기화 언어 모델 및 상기 초기화 단어 벡터 파라미터 행렬에 대한 훈련 차수（즉 단계~의 반복 수행 차수）는 제2 사전 설정 임계 값에 도달한다. 선택적으로 본 실시예의 일 가능한 구현 방식에서 상기 실시예 중 상기 언어 모델 및 상기 다른 언어 모델은 임 의의 두 개의 서로 다른 유형의 언어 모델일 수 있고 서로 다른 코퍼스 중 사전 설정 텍스트 언어 자료에 따라 훈련하여 얻은 같은 유형의 서로 다른 언어 모델일 수도 있다. 본 출원 실시예는 상기 언어 모델 및 상기 다른 언어 모델의 구체적인 유형에 대해 제한하지 않는다. 예를 들면, 그중 일 구체적인 구현 방식에서 예를 들면 상기 언어 모델은 ERNIE 모델일 수 있고 상기 다른 언어 모델은 연속 단어 모음（Continuous Bag of Word，CBOW, ）모델 또는 ERNIE 모델, CBOW 모델과 다른 언어 모델일 수 있다. 여기서，ERNIE 모델은 대용량 데이터에서 엔티티 개념과 같은 사전 시맨틱 지식을 모델링 하여 완전한 개념의 시맨틱 표현을 학습할 수 있으며 단어 및 엔티티 개념과 같은 시맨틱 유닛을 마스킹 하여 ERNIE 모델을 사전 훈 련하여 ERNIE 모델이 시맨틱 지식 유닛의 표현은 현실 세계에 더 가깝고 ERNIE 모델은 문자 특징을 기반으로 입 력 및 모델링 함과 동시에 사전 시맨틱 지식 유닛을 직접 모델링 하여 강력한 시맨틱 표현 능력을 가진다. 본 실시예에서 ERNIE 모델을 상기 언어 모델로 하여 ERNIE 모델의 강력한 시맨틱 표현 능력을 이용하여 대용량 데 이터 중 단어, 개체 및 개체 관계를 모델링 하고，실제 세계의 시맨틱 지식을 학습하여 모델 시맨틱 표현 능력 을 향상시킨다. 예를 들면 ，ERNIE 모델은 학습 단어와 개체의 표현을 학습하여 “하얼빈”과 “흑룡강”의 관 계를 모델링 하고 “하얼빈”은 “흑룡강”의 성도 및 “하얼빈”은 눈의 도시임을 학습할 수 있다. CBOW 모델은 중간 단어의 컨텍스트에 대응하는 단어 벡터를 기반으로 상기 중간 단어의 단어 벡터를 예측할 수 있다. CBOW 모델은 히든 레이어를 포함하지 않으므로 훈련 속도가 비교적 빠르고 CBOW 모델의 각각의 단어 벡터 에 대한 계산은 슬라이딩 윈도우에서 한정된 컨텍스트와 관련되므로 훈련 파라미터가 비교적 적고 모델 복잡성 이 낮으며 모델의 예측 정확도가 비교적 높다. 또한 사전 훈련된 CBOW 모델에 대응하는 단어 벡터 파라미터 행 렬（CBOW 단어 벡터라고도 함） 및 사전 훈련된 ERNIE 모델에 대응하는 단어 벡터 파라미터 행렬（ERNIE-WORD 단어 벡터라고도 함）을 결합하여 ERNIE 모델에 대해 추가로 훈련하여 ERNIE 모델로 하여금 고품질의 CBOW 단어 벡터 및 ERNIE-WORD 단어 벡터의 단어 시맨틱 정보를 동시에 학습할 수 있도록 하며 ERNIE 모델의 단어 시맨틱 정보 학습 능력을 강화하고 ERNIE 모델의 텍스트 중 단어에 대한 예측 능력을 향상시킨다. 또한 상기 실시예를 기반으로 제1 사전 훈련 완료 조건을 만족하여 훈련된 언어 모델을 얻은 후 감독된 NLP 작 업을 통해 언어 모델을 더욱 최적화할 수 있으며 언어 모델의 NLP 작업 중 예측 성능을 더욱 향상시킨다. 선택적으로 본 실시예의 일 가능한 구현 방식에서 훈련된 언어 모델을 이용하여 NLP 작업을 수행하여 처리 결과 를 얻는다. 또한 상기 처리 결과와 주석 결과 정보 사이의 차이에 따라 상기 언어 모델 중 파라미터 값을 사전 설정 조건을 만족할 때까지 미세 조정（finetuning）한다. 예를 들면 상기 처리 결과와 주석 결과 정보 사이의 차이는 사전 설정 차이보다 작고 및/또는 상기 언어 모델의 훈련 차수는 사전 설정 차수에 도달한다. 상기 주석 결과 정보는 수행하려는 NLP 작업에 대해 사전에 수동 주석한 정확한 처리 결과이다. 구체적으로 상기 NLP 작업은 예를 들면 분류, 매칭, 서열 주석 등 NLP 작업 중 임의의 하나 또는 복수 개일 수 있다. 본 실시예는 이에 대해 특별히 제한되지 않는다. 이에 따라 처리 결과는 예를 들면 분류 결과, 매칭 결과, 서열 주석 결과 등과 같은 특정 NLP 작업의 처리 결과이다. 일 구체적인 구현방식에서 훈련된 언어 모델을 이용하여 다른 분류, 매칭, 서열 주석을 구현하는데 사용되는 네 트워크 모델을 결합하여 예를 들면 컨볼루션 신경망 네트워크（convolutional neural network，CNN）, 장단기 기억（Long Short Term Memory，LSTM） 모델, BOW（Bag of Word， ） 모델을 사용하여 NLP 작업을 수행하여 처리 결과를 얻는다. 예를 들면， 다른 네트워크 모델은 분류, 매칭, 서열 주석을 구현하는데 사용되는 네트워 크 모델은 언어 모델의 출력에 기반하여 분류, 매칭, 서열 주석 등 처리를 수행하여 대응하는 분류 결과, 매칭 결과, 서열 주석 결과 등 처리 결과를 얻는다. 본 실시예에서 단어 벡터 파라미터 행렬이 필요하지 않으므로 언어 모델 전체 구조를 변경하지 않고 감독 데이 터（즉 주석 결과 정보）의 NLP작업을 통해 언어 모델을 더욱 최적화할 수 있어 언어 모델의 예측 성능을 향상 시키고 각각의 NLP작업에 따라 언어 모델을 쉽게 최적화 및 반복할 수 있다. 설명해야 할 점은, 전술한 각각의 방법 실시예의 경우, 설명의 편리를 위해 일련의 동작 조합으로 표현하였지만 당업자라면 본 출원은 설명된 동작 순서의 제한을 받지 않음을 알 수 있다. 원인은 본 출원에 따라 일부 단계는 다른 순서 또는 동시에 수행될 수 있기 때문이다. 둘째로 당업자라면 명세서에서 설명된 실시예가 모두 바람직 한 실시예이고 관련된 동작 및 모듈은 반드시 본 출원에 의해 요구되는 것은 아니라는 것을 알 수 있다. 상기 실시예에서 각각의 실시예에 대한 설명은 그 자체로 강조되어 있으며 어느 실시예에서 상세하게 설명하지 않은 부분에 대해서는 다른 실시예의 관련 설명을 참조할 수 있다. 도 3에 나타낸 바와 같이 본 출원 제4 실시예에 따른 개략도이다. 본 실시예의 언어 모델에 기반하여 단어 벡터 를 획득하는 장치는 언어 모델, 획득 유닛 , 제1 결정 유닛 및 제1 훈련 유닛을 포함 한다. 여기서 언어 모델은 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 각각의 제1 샘플 텍스트 언어 자 료를 수신하여 언어 모델에 입력하고 각각의 상기 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크의 컨텍스트 벡터를 출력하는데 사용되며； 획득 유닛은 각각의 상기 제1 샘플 텍스트 언어 자료 중 각각의 상기 제1 단어 마스크에 대해 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제1 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제1 확률 분포 행렬을 얻으며, 각각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 제2 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제2 확률 분포 행렬을 얻으며, 각 각의 상기 제1 단어 마스크의 컨텍스트 벡터 및 완전 연결 행렬에 기반하여 각각의 상기 제1 단어 마스크의 제3 확률 분포 행렬을 얻는다 ； 여기서 상기 제1 단어 벡터 파라미터 행렬은 사전 훈련된 상기 언어 모델에 대응하 는 단어 벡터 파라미터 행렬이고, 상기 제2 단어 벡터 파라미터 행렬은 사전 훈련된 다른 언어 모델에 대응하는 단어 벡터 파라미터 행렬이다 ； 제1 결정 유닛은 각각의 상기 제1 단어 마스크의 상기 제1 확률 분포 행 렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬에 각각 기반하여 각각의 상기 제1 단어 마스크에 대 응하는 단어 벡터를 결정하는데 사용된다； 제1 훈련 유닛은 상기 적어도 두 개의 제1 샘플 텍스트 언어 자료 중 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 제1 사전 훈련 완료 조건을 만족할 때까지 상기 언 어 모델 및 상기 완전 연결 행렬을 훈련시키며 훈련된 상기 제1 단어 벡터 파라미터 행렬, 상기 제2 단어 벡터 파라미터 행렬 및 상기 완전 연결 행렬의 집합을 단어 벡터의 집합으로 한다. 설명해야 할 점은, 본 실시예의 언어 모델의 훈련 장치의 수행 주체의 부분 또는 전부는 로컬 터미널에 위치한 응용일 수 있으며 또는 로컬 터미널의 응용에 설정된 플러그인 또는 소프트웨어 개발 키트（Software Development Kit，SDK）등 기능 유닛일 수도 있으며 또는 네트워크 측 서버에 위치한 처리 엔진이 될 수도 있으 며 본 실시예는 이에 대해 특별히 제한하지 않는다. 상기 응용은 단말기에 장착된 로컬 프로그램（nativeApp）일 수 있으며 또는 단말기 상의 브라우저의 웹 페이지 프로그램（webApp）일 수 있음을 이해할 수 있으며 본 실시예에서 이에 대해 한정하지 않는다. 본 실시예에서 다른 언어 모델에 대응하는 제2 단어 벡터 파라미터 행렬을 도입함으로써 사전 훈련된 제1 단어 벡터 파라미터 행렬 및 제2 단어 벡터 파라미터 행렬에 동시에 기반하여 여러가지 고품질 단어 벡터를 결합하여 언어 모델 및 단어 벡터를 연합 훈련하여 언어 모델로 하여금 다중 소스 고품질의 단어 시맨틱 정보를 학습하고 언어 모델의 단어 시맨틱 정보 학습 능력을 강화시키며 언어 모델의 예측 성능을 향상시킨다. 또한, 본 출원에서 제공한 기술 방안을 사용하여 여러가지 고품질 단어 벡터를 결합하여 언어 모델 및 단어 벡 터를 연합 훈련하여 언어 모델 및 단어 벡터의 수렴 속도를 빠르게 하고 훈련 효과를 향상시킨다. 또한 본 출원에서 제공한 기술 방안을 사용하여 단어 마스크를 포함하는 샘플 텍스트 언어 자료를 사용하여 언 어 모델 및 단어 벡터를 훈련시키며 문자 벡터에 비하여 단어 벡터에는 더욱 풍부한 시맨틱 정보 표현을 포함하 므로 단어 마스크의 방식을 사용하여 컨텍스트 모델링 단어 벡터에 기반하여 언어 모델의 단어 시맨틱 정보에 대한 모델링을 강화하고 언어 모델의 단어 시맨틱 정보에 대한 학습 능력을 강화하며 문자에 기반한 전체 단어 커버리지로 인한 정보 유출 위험을 효과적으로 방지할 수 있다. 선택적으로 본 실시예의 가능한 구현 방식에서 상기 획득 유닛은 구체적으로 상기 제1 단어 마스크의 컨텍 스트 벡터와 상기 제1 단어 벡터 파라미터 행렬을 곱하여 상기 제1 단어 마스크의 제1 확률 분포 행렬을 얻는데 사용되며； 및/또는 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 제2 단어 벡터 파라미터 행렬을 곱하여 상기 제1 단어 마스크의 제2 확률 분포 행렬을 얻는데 사용되며； 및/또는 상기 제1 단어 마스크의 컨텍스트 벡터와 상기 완전 연결 행렬을 곱하여 상기 제1 단어 마스크의 제3 확률 분포 행렬을 얻는데 사용된다. 도 4에 나타낸 바와 같이 본 출원 제5 실시예에 따른 개략도이다. 도 3에 나타낸 실시예에 기반하여 본 실시예 언어 모델에 기반하여 단어 벡터를 획득하는 장치는 또한 합산 유닛 및 정규화 유닛을 포함할수 있다. 여기서 합산 유닛은 상기 제1 확률 분포 행렬, 상기 제2 확률 분포 행렬 및 상기 제3 확률 분포 행렬을 합하여 총 확률 분포 행렬을 얻는다； 정규화 유닛은 상기 총 확률 분포 행렬 중 확률 값을 정규화 처리하여 상기 제1 단어 마스크의 복수 개 단어 벡터에 대응하는 복수 개 정규화 확률 값을 얻는데 사용된다. 이에 따라 본 실시예에서 상기 제1 결정 유닛은 구체적으로 상기 복수 개 정규화 확률 값에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는데 사용된다. 선택적으로 도 4를 참조하면 상기 실시예의 언어 모델에 기반하여 단어 벡터를 획득하는 장치는 또한 제2 훈련 유닛을 포함하며 제2 사전 훈련 완료 조건을 만족할 때까지 초기화 언어 모델 및 초기화 제1 단어 벡 터 파라미터 행렬을 훈련시켜 상기 언어 모델 및 상기 제1 단어 벡터 파라미터 행렬을 얻는다. 선택적으로 도 4를 참조하면 상기 실시예의 언어 모델에 기반하여 단어 벡터를 획득하는 장치는 또한 사전 훈련 유닛, 대체 유닛 및 제2 결정 유닛을 포함할 수 있다. 여기서 사전 훈련 유닛은 코퍼 스 중 사전 설정 텍스트 언어 자료를 미리 사용하여 상기 초기화 언어 모델을 사전 훈련 학습하는데 사용되며； 대체 유닛은， 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어를 제2 단어 마스크로 각각 대체하여 적 어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 언어 자료를 얻고 상기 초기화 언어 모델에 입력하는 데 사용되며； 상기 초기화 언어 모델은 상기 대체 유닛에 의해 입력된 적어도 하나의 제2 단어 마스크를 포함 하는 제2 샘플 텍스트 언어 자료에 기반하여 상기 적어도 하나의 제2 단어 마스크 중 각각의 상기 제2 단어 마 스크의 컨텍스트 벡터를 입력하는데 사용되며； 제2 결정 유닛은 각각의 상기 제2 단어 마스크의 컨텍스트 벡터 및 상기 초기화 제1 단어 벡터 파라미터 행렬에 기반하여 각각의 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하는데 사용되며； 상기 제2 훈련 유닛은 상기 적어도 하나의 제2 단어 마스크에 대응하는 단 어 벡터에 기반하여 제2 사전 훈련 완료 조건을 만족할 때까지 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 파라미터 행렬을 훈련시킨다. 선택적으로 본 실시예의 가능한 구현 방식에서 상기 대체 유닛은 구체적으로 상기 제2 샘플 텍스트 언어 자료에 대해 단어 분할하고 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트 언어 자료 중 적어도 하나의 단어 중 각각의 단어를 제2 단어 마스크로 각각 대체한다. 선택적으로, 본 실시예의 가능한 구현 방식에서 상기 실시예 중 상기 언어 모델 및 상기 다른 언어 모델은 임의 의 두 개 서로 다른 유형의 언어 모델일 수 있으며 서로 다른 코퍼스 중 사전 설정 텍스트 언어 자료에 의해 훈 련하여 얻은 같은 유형의 서로 다른 언어 모델일 수도 있으며 본 출원 실시예는 상기 언어 모델 및 상기 다른 언어 모델의 구체적인 유형에 대해 제한하지 않는다. 예를 들면, 그중 일 구체적인 구현 방식에서 예를 들면 상기 언어 모델은 ERNIE 모델일 수 있고 상기 다른 언어 모델은 CBOW 모델 또는 ERNIE 모델, CBOW 모델과 다른 언어 모델일 수 있다. 설명해야 할 점은 도 1~도 2에 대응하는 실시예의 방법은 상기 도 3~도 4 실시예에서 제공하는 상기 실시예의 언어 모델에 기반하여 단어 벡터를 획득하는 장치에 의해 구현될 수 있다. 상세한 설명은 도 1~도 2에 대응하는 실시예 중 관련 내용을 참고할 수 있으며 여기서 설명을 생략한다. 본 출원의 실시예에 따르면 본 출원은 전자기기 및 컴퓨터 명령이 기억되어 있는 비 일시적 컴퓨터 판독 가능 기록매체를 더 제공한다. 도 5는 본 출원 실시예의 언어 모델에 기반하여 단어 벡터를 획득하는 방법을 구현하기 위한 전자기기의 개략도 이다. 전자기기는 예를 들면 랩톱 컴퓨터, 데스크톱 컴퓨터, 워크 스테이션, 개인 디지털 보조기, 서버, 블레이 드 서버, 대형 컴퓨터 및 기타 적합한 컴퓨터와 같은 다양한 형태의 디지털 컴퓨터를 나타낸다. 전자기기 또한 예를 들면 개인 디지털 처리기, 셀폰, 스마트 전화, 웨어러블 기기 및 기타 유사한 계산 장치와 같은 다양한 형 태의 모바일 장치를 나타낼 수 있다. 본 명세서에 나타낸 구성 요소, 이들의 연결과 관계 및 이들의 기능은 단 지 예일 뿐이며, 본 명세서에서 기술하거나 및/또는 요구하는 본 발명의 구현을 한정하려는 것이 아니다. 도 5에 나타낸 바와 같이 당해 전자기기는 하나 또는 복수의 프로세서, 메모리 및 각각의 구성 요소 를 연결하기 위한 인터페이스를 구비하며, 당해 인터페이스는 고속 인터페이스 및 저속 인터페이스를 포함한다. 각각의 구성 요소는 서로 다른 버스를 통해 상호 연결되며, 공통 마더 보드에 설치되거나 또는 수요에 따라 기 타 방식으로 설치된다. 프로세서 전자기기 내에서 수행되는 명령에 대해 처리를 수행할 수 있으며, 메모리 내에 기억되어 외부 입력/출력 장치 （예를 들면 인터페이스에 연결된 디스플레이 기기） 상에 GUI(도면 유저 계면) 의 그래픽 정보를 표시하기 위한 명령을 포함한다. 기타 실시 방식에 있어서, 필요할 경우, 복수의 프로세서 및 /또는 복수의 버스와 복수의 메모리를 함께 사용할 수 있다. 마찬가지로, 복수의 전자기기를 연결할 수 있으며,각각의 기기는 부분적인 필요한 조작 （예를 들면, 서버 어레이, 일 그룹의 블레이드 서버, 또는 다중 프로세서 시스템）을 제공한다. 도 5에서는 하나의 프로세서의 예를 들었다. 메모리는 본 출원에 의해 제공되는 비 일시적 컴퓨터 판독 가능 기록 매체이다. 여기서, 상기 메모리에는 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 적어도 하나의 프로세서로 하여금 본 출원에 의해 제공되는 언어 모델에 기반하여 단어 벡터를 획득하는 방법을 수행하도록 한다. 본 출원의 비 일시적 컴퓨터 판독 가능 기록 매체는 컴퓨터 명령을 기억하며, 상기 컴퓨터 명령은 컴퓨터로 하여금 본 출원에 의해 제공되는 언어 모델에 기반하여 단어 벡터를 획득하는 방법을 수행하도록 한다. 메모리는 일종의 비 일시적 컴퓨터 판독 가능 기록 매체로서, 비 일시적 소프트웨어 프로그램, 비 일시적 컴퓨터 수행 가능 프로그램 및 모듈을 기억하는데 사용될 수 있는 바, 예를 들면 본 출원 실시예의 모델 기반 단어 벡터를 획득하는 방법에 대응하는 프로그램 명령/모듈（예를 들면, 도 3에 나타낸 언어 모델, 획득 유닛, 제1 결정 유닛 및 제1 훈련 유닛）을 기억하는데 사용될 수 있다. 프로세서는 메모 리 내에 기억된 비 일시적 소프트웨어 프로그램, 명령 및 모듈을 운행함으로써, 서버의 다양한 기능 응용 및 데이터 처리를 수행하는 바, 상술한 방법 실시예의 언어 모델에 기반하여 단어 벡터를 획득하는 방법을 구현 한다. 메모리는 프로그램 기억 영역 및 데이터 기억 영역을 포함할 수 있으며, 여기서, 프로그램 기억 영역은 운 영 체제 및 적어도 하나의 기능에 필요한 앱을 기억할 수 있고, 데이터 기억 영역은 본 출원 실시예에서 제공하 는 언어 모델에 기반하여 단어 벡터를 획득하는 방법의 전자기기의 사용을 통해 생성된 데이터 등을 기억할 수 있다. 또한, 메모리는 고속 랜덤 액세스 메모리를 포함할 수 있고, 비 일시적 메모리를 더 포함할 수 있는 바, 예를 들면 적어도 하나의 자기 디스크 저장 장치, 플래시 장치, 또는 기타 비 일시적 고체 저장 장치를 포 함할 수 있다. 일부 실시예에 있어서, 메모리는 선택적으로 프로세서에 대해 원격 설치한 메모리를 포함할 수 있으며, 이러한 원격 메모리는 네트워크를 통해 본 출원 실시예에서 제공하는 언어 모델에 기반하여 단어 벡터를 획득하는 방법의 전자기기에 연결될 수 있다. 상술한 네트워크의 실시예는 인터넷, 기업 인트라 넷, 근거리 통신망, 이동 통신 네트워크 및 이들의 조합을 포함하나 이에 한정되지 않는다. 언어 모델에 기반하여 단어 벡터를 획득하는 방법의 전자기기는 입력 장치 및 출력 장치를 더 포함할 수 있다. 프로세서, 메모리, 입력 장치 및 출력 장치는 버스 또는 다른 방식을 통해 연결 될 수 있으며 도 5에서는 버스를 통해 연결하는 예를 들었다. 입력 장치는 입력된 디지털 또는 문자 정보를 수신하고 또한 본 출원 실시예에서 제공하는 언어 모델에 기 반하여 단어 벡터를 획득하는 방법의 전자기기의 유저 설정 및 기능 제어에 관한 키 신호 입력을 생성할 수 있 다. 예를 들면 터치 스크린, 키패드, 마우스, 트랙 패드, 터치 패드, 포인팅 스틱, 하나 또는 복수의 마우스 버 튼, 트랙볼, 조이스틱 등 입력 장치를 포함할 수 있다. 출력 장치 는 디스플레이 기기, 보조 조명 장치（ 예를 들면 LED） 및 촉각 피드백 장치（예를 들면 진동 모터） 등을 포함할 수 있다. 당해 디스플레이 기기는 액정 디스플레이（LCD）, 발광 다이오드（LED） 디스플레이 및 등 플라즈마 디스플레이를 포함할 수 있으나 이 에 한정되지 않는다. 일부 실시 방식에 있어서, 디스플레이 기기는 터치 스크린일 수 있다. 여기서 설명하는 시스템 및 기술의 다양한 실시 방식은 디지털 전자 회로 시스템, 집적 회로 시스템, 전용 ASIC （전용 집적 회로）, 컴퓨터 하드웨어, 펌웨어, 소프트웨어 및/또는 이들의 조합에서 구현될 수 있다. 이러한 다양한 실시 형태는 하나 또는 복수의 컴퓨터 프로그램에서 실시되고, 당해 하나 또는 복수의 컴퓨터 프로그램 은 적어도 하나의 프로그램 가능 프로세서를 포함하는 프로그램 가능 시스템 상에서 수행 및/또는 해석될 수 있 으며, 당해 프로그램 가능 프로세서는 전용 또는 일반 프로그램 가능 프로세서일 수 있고, 기억 시스템, 적어도 하나의 입력 장치 및 적어도 하나의 출력 장치로부터 데이터 및 명령을 수신할 수 있으며, 또한 데이터 및 명령 을 당해 기억 시스템, 당해 적어도 하나의 입력 장치 및 당해 적어도 하나의 출력 장치에 전송할 수 있다. 이러한 계산 프로그램 （프로그램, 소프트웨어, 소프트웨어 응용 또는 코드로도 불림）은 프로그램 가능 프로세 서의 기계 명령을 포함하며, 또한 고급 과정 및/또는 객체 지향 프로그래밍 언어 및/또는 어셈블리/기계 언어를 이용하여 이러한 계산 프로그램을 실시할 수 있다. 본 명세서에서 사용되는 “기계 판독 가능 매체” 및 “컴퓨 터 판독 가능 매체”와 같은 용어는, 기계 명령 및/또는 데이터를 프로그램 가능 프로세서의 임의의 컴퓨터 프 로그램 제품, 기기 및/또는 장치 （예를 들면, 자기 디스크, 광 디스크, 메모리, 프로그램 가능 논리 장치（PLD ））에 제공하기 위한 것을 의미하며, 기계 판독 가능 신호로서의 기계 명령을 수신하는 기계 판독 가능 매체를 포함한다. “기계 판독 가능 신호”와 같은 용어는 기계 명령 및/또는 데이터를 프로그램 가능 프로세서에 제공 하기 위한 임의의 신호를 의미한다.유저와의 대화를 제공하기 위하여, 컴퓨터 상에서 여기서 설명하는 시스템 및 기술을 실시할 수 있으며, 당해 컴퓨터는 유저에게 정보를 표시하기 위한 디스플레이 장치 （예를 들면 CRT （음극선관） 또는 LCD （액정 디스 플레이） 모니터） 및 키보드와 포인팅 장치（예를 들면, 마우스 또는 트랙볼）를 구비할 수 있으며, 유저는 당 해 키보드 및 당해 포인팅 장치를 통해 입력을 컴퓨터에 제공할 수 있다. 기타 유형의 장치는 또한 유저와의 대 화를 제공하는데 사용될 수 있다. 예를 들면, 유저에 제공하는 피드백은 임의의 형태의 감각 피드백 （예를 들 면, 시각적 피드백, 청각적 피드백, 또는 촉각 피드백）일 수 있으며, 또한 임의의 형태（음향 입력, 음성 입력 또는 촉각 입력을 포함함）를 통해 유저로부터의 입력을 수신할 수 있다. 여기서 설명하는 시스템 및 기술을 백엔드 구성 요소를 포함하는 계산 시스템 （예를 들면 데이터 서버）, 또는 미들웨어 구성 요소를 포함하는 계산 시스템 （예를 들면 응용 서버）, 또는 프런트엔드 구성 요소를 포함하는 계산 시스템 （예를 들면 그래픽 유저 인터페이스 또는 웹 브라우저를 구비하는 유저 컴퓨터인 바, 유저는 당해 그래픽 유저 인터페이스 또는 당해 웹 브라우저를 통해 여기서 설명하는 시스템 및 기술의 실시 방식과 대화함 ）, 또는 이러한 백엔드 구성 요소, 미들웨어 구성 요소, 또는 프런트엔드 구성 요소의 임의의 조합을 포함하는 계산 시스템에서 실시할 수 있다. 임의의 형태 또는 매체의 디지털 데이터 통신 （예를 들면, 통신 네트워크） 을 통해 시스템의 구성 요소를 상호 연결할 수 있다. 통신 네트워크의 예는 근거리 통신망（LAN）, 광역 통신망 （WAN） 및 인터넷을 포함한다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트와 서버는 일반적으로 서로 멀리 떨어져 있 고, 또한 일반적으로 통신 네트워크를 통해 대화를 수행한다. 해당되는 컴퓨터 상에서 운행되고, 또한 클라이언 트 - 서버 관계를 갖는 컴퓨터 프로그램을 통해 클라이언트와 서버의 관계를 발생시킬 수 있다. 본 출원 실시예에 따른 기술 방안은 다른 언어 모델에 대응하는 제2 단어 벡터 파라미터 행렬을 도입하고 사전 훈련된 제1 단어 벡터 파라미터 행렬 및 제2 단어 벡터 파라미터 행렬에 기반하여 여러가지 고품질 단어 벡터를 결합하여 언어 모델 및 단어 벡터를 연합 훈련하여 언어 모델로 하여금 다중 소스 고품질의 단어 시맨틱 정보를 학습하도록 하여 언어 모델의 단어 시맨틱 정보 학습 능력을 강화하고 언어 모델의 예측 성능을 향상시킨다. 또한, 본 출원에서 제공하는 기술 방안을 사용하여 여러가지 고품질 단어 벡터를 결합하여 언어 모델 및 단어 벡터를 연함 훈련함으로써 언어 모델 및 단어 벡터의 수렴 속도를 빠르게 하고 훈련 효과를 향상시킨다. 또한 본 출원에서 제공하는 기술 방안을 사용하여 단어 마스크를 포함하는 샘플 텍스트 언어 자료를 사용하여 언어 모델 및 단어 벡터를 훈련시킨다. 문자 벡터에 비하여 단어 벡터에는 더욱 풍부한 시맨틱 정보 표현을 포 함하므로 단어 마스크의 방식으로 컨텍스트 모델링 단어 벡터를 기반으로 언어 모델의 단어 시맨틱 정보에 대한 모델링을 강화하고 언어 모델의 단어 시맨틱 정보에 대한 학습 능력을 강화하며 문자에 기반한 전체 단어 커버 리지로 인한 정보 유출 위험을 효과적으로 방지할 수 있다. 상기에 나타낸 다양한 형태의 흐름을 이용하여 단계를 재정렬, 증가 또는 삭제할 수 있음을 이해해야 한다. 예 를 들면, 본 발명에 기재된 각각의 단계는 병렬로 수행되거나 또는 차례로 수행되거나 또는 다른 순서로 수행될 수 있으며, 본 출원이 개시하는 기술 방안이 원하는 결과를 구현할 수 있는 한, 본 명세서는 이에 대해 한정하 지 않는다. 상술한 구체적인 실시 방식은 본 발명의 보호 범위를 한정하지 않는다. 당업자는 설계 요건 및 기타 요인에 따 라 다양한 수정, 조합, 서브 조합 및 대체를 수행할 수 있음을 이해해야 한다. 본 출원의 정신 및 원칙 내에서 이루어진 임의의 수정 동등한 대체 및 개선 등은 모두 본 발명의 보호 범위 내에 포함되어야 한다.도면 도면1 도면2 도면3 도면4 도면5"}
{"patent_id": "10-2021-0068213", "section": "도면", "subsection": "도면설명", "item": 1, "content": "이하에서는 본 출원의 실시예의 기술방안을 보다 명확하게 설명하기 위해 실시예 또는 종래 기술의 설명에 이용 되어야 할 도면을 간략히 소개한다. 아래에 설명하는 도면은 본 출원의 일부 실시예일 뿐이며, 당업자에 있어서, 이러한 도면들에 기초하여 다른 도면들이 창조적인 노력없이 얻어질 수 있다. 도면은 본 방안을 더 잘 이해하기 위해서만 사용되며 본 출원을 제한하지 않는다. 여기서： 도 1은 본 출원 제1 실시예에 따른 개략도이다； 도 2는 본 출원 제2 실시예에 따른 개략도이다； 도 3은 본 출원 제3 실시예에 따른 개략도이다； 도 4는 본 출원 제4 실시예에 따른 개략도이다； 도 5는 본 출원 실시예의 언어 모델에 기반하여 단어 벡터를 획득하는 방법의 전자 기기의 개략도이다."}
