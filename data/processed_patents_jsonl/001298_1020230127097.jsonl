{"patent_id": "10-2023-0127097", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0037868", "출원번호": "10-2023-0127097", "발명의 명칭": "인공지능 기반의 응답자 집단 표집 장치, 방법 및 프로그램", "출원인": "주식회사 글로랑", "발명자": "권용훈"}}
{"patent_id": "10-2023-0127097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "응답자 집단을 대표할 수 있도록 생성된 인공지능 기반의 응답 모델이 저장된 메모리; 및상기 응답 모델에 인적성 검사용 문장과 일반 아이 집단 및 특수 아이 집단의 응답 특성을 입력하여 응답 정보를 획득하고, 상기 응답 정보를 기반으로 상기 응답자 집단 내에서 상기 일반 아이 집단과 상기 특수 아이 집단을 표집하는 프로세서를 포함하는, 장치."}
{"patent_id": "10-2023-0127097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 응답 모델은,BERT 계열의 인공지능 기반의 응답 모델인, 장치."}
{"patent_id": "10-2023-0127097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 인공지능 모델은,응답자 집단 내 일반 아이 및 특수 아이의 분포를 모델링하여 상기 일반 아이 집단 및 상기 특수 아이 집단을대표할 수 있도록 생성된 것인, 장치."}
{"patent_id": "10-2023-0127097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 프로세서는,새롭게 생성된 질의에 대하여 상기 일반 아이 집단의 정보와 상기 특수 아이 집단에 대한 초기 표집 분포의 추출이 완료되면, 상기 일반 아이 집단의 정보 및 상기 특수 아이 집단의 정보를 표시하는 것을 특징으로 하는,장치."}
{"patent_id": "10-2023-0127097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 인공지능 모델은,상기 일반 아이 집단 및 상기 특수 아이 집단의 행동 특성을 고려하여 설계된 것인, 장치."}
{"patent_id": "10-2023-0127097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 프로세서는,상기 응답 정보를 기반으로, 상기 일반 아이 집단과 상기 특수 아이 집단의 분포를 모델링하고, 상기 모델링을공개특허 10-2024-0037868-3-통해 새로운 질의에 대한 상기 일반 아이 집단과 상기 특수 아이 집단의 초기 응답을 표집하는 것을 특징으로하는, 장치."}
{"patent_id": "10-2023-0127097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 응답 모델은,상기 인적성 검사용 문장을 학습하여 추천된 응답 정보를 출력하는 것을 특징으로 하는, 장치."}
{"patent_id": "10-2023-0127097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "응답자 집단을 대표할 수 있도록 생성된 질의 응답 네트워크에 인적성 검사용 문장과 일반 아이 집단 및 특수아이 집단의 응답 특성을 입력하여 응답 정보를 획득하고, 상기 응답 정보를 기반으로 상기 응답자 집단 내에서상기 일반 아이 집단과 상기 특수 아이 집단을 표집하는, 장치."}
{"patent_id": "10-2023-0127097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "장치에 의해 수행되는 방법으로,응답자 집단을 대표할 수 있도록 생성된 인공지능 기반의 응답 모델에 인적성 검사용 문장과 일반 아이 집단 및특수 아이 집단의 응답 특성을 입력하여 응답 정보를 획득하는 단계; 및상기 응답 정보를 기반으로 상기 응답자 집단 내에서 상기 일반 아이 집단과 상기 특수 아이 집단을 표집하는단계를 포함하는, 방법."}
{"patent_id": "10-2023-0127097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "하드웨어인 컴퓨터와 결합되어, 제9항의 방법을 실행시키기 위하여 매체에 저장된, 프로그램."}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 서버에 의해 수행되는 BERT 계열 모델을 기반으로한 응답자 집단을 대표하는 질의 응답 네트워크를 활 용한 인적성 검사의 표집 과정 방법에 있어서, 상기 인적성 검사를 위한 인적성 검사용 문장을 수신받는 단계; 상기 인적성 검사용 문장을 기반으로 인공지능 기반의 응답 모델을 통해 학습되어 추천된 해당 질의 응답 정보를 출력하는 단계; 및 상기 해당 질의 응답 정보를 기반으로 새로운 질의에 대한 각 집단의 초기 응답을 표집하는 단계; 를 포함할 수 있다."}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인적성 검사의 표집 과정 방법에 관한 것이다. 보다 상세하게는, 본 개시는 BERT 계열 모델을 기반으 로한 응답자 집단을 대표하는 질의 응답 네트워크를 활용한 인적성 검사의 표집 과정 방법에 관한 것이다. 이 때, BERT 계열 모델이라는 것은, BERT, RoBERTa, ALBERT 등과 같은 모델들을 의미하며, 이에 국한되지 않고 근 본적인 Transformer의 (Attention을 활용한) Encoder 구조를 바탕으로 파생된 Model들을 의미한다."}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일반적인 지능이나 적성검사를 개발하는 경우에는, 지역, 성별, 연령대를 고려해서 모집단을 잘 반영하는 표본 을 추출하고 개발하는 것이 우선이라, 일반인들을 대상으로 하는 것이 대부분이지만 지적장애의 진단을 위한 검 사를 개발하는 경우에는 특수아 집단에 대한 표집을 실시하는데 진단의 내용이 충동성이냐, 지적능력이냐에 따 라서 표집하는 집단에서 차이가 있을 수 있다. 지적능력을 진단하는 경우에는, ADHD 아동이나, 자폐스펙트럼군, 기타 경계선 지능판정을 받은 아동들을 대상으 로 실시할 수 있다. 현실적으로, 특수아 집단의 표집도 쉽지가 않은데 검사의 진행 자체도 원활하게 되는 편은 아니라서 먼저 일반 집단군을 표준화한 이후에 특수아 집단에게 검사를 실시해서 동일 연령대에서 차이를 얼마나 보이느냐에 따라 진단 기준을 세울 수 있고, 개발 목적에 따라 특수아 집단도 특정적으로 실시할 수 있다.예를 들면, 충동성의 경우 ADHD 아동들과 비교하는 것이 바람직하고, 정서나 사회성 영역같은 부분들을 진단하 는 경우에는 학교폭력이나 문제집단 학생들과 비교할 수 있다. 그런데, 인적성 문제는 검사별로 문항이 유사하지만, 해당 검사의 목적에 따라 서로 세세한 문제의 내용이 다르 다. 따라서, 종래 인적성 문제는 문항을 만들고 표집(특수 아이, 일반 아이에서 평균을 뽑는 것)하는데 손이 많이 가는 작업이므로, 인적성 문제 개발의 비용을 단축하는데에 어려움이 있었다. 선행기술문헌 특허문헌 (특허문헌 0001) 특허 공개공보 제10-2023-0018952호. 2023.02.07 공개"}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시에 개시된 실시예는, 일반 및 특수 집단을 대표할 만한 응답 모델을 생성하여, 인적성 문제 개발의 비용 을 단축할 수 있는 것을 제공하는데 그 목적이 있다. 본 개시에 개시된 실시예는, 학습한 모델을 기반으로 사후 문제에 대한 각 집단의 응답을 샘플링하여, 그 당위 성을 검증할 수 있는 것을 제공하는데 그 목적이 있다. 본 개시에 개시된 실시예는, 각 집단에 대한 응답 샘플을 수작업 방식이 아닌 모델의 응답을 통해 생성하는데에 그 목적이 있다. 본 개시에 개시된 실시예는, 모델의 응답을 통해 생성된 응답 샘플의 통계값을 활용하여 사전 문제를 개발하는 데에 그 목적이 있다. 본 개시가 해결하고자 하는 과제들은 이상에서 언급된 과제로 제한되지 않으며, 언급되지 않은 또 다른 과제들 은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 본 개시의 일 측면에 따른 서버에 의해 수행되는 BERT 계열 모델을 기반으 로한 응답자 집단을 대표하는 질의 응답 네트워크를 활용한 인적성 검사의 표집 과정 방법은, 상기 인적성 검사 를 위한 인적성 검사용 문장을 수신받는 단계; 상기 인적성 검사용 문장을 기반으로 인공지능 기반의 응답 모델 을 통해 학습되어 추천된 해당 질의 응답 정보를 출력하는 단계; 및 상기 해당 질의 응답 정보를 기반으로 새로 운 질의에 대한 각 집단의 초기 응답을 표집하는 단계; 를 포함할 수 있다. 또한, 상기 표집하는 단계는, 상기 응답자의 분포를 통계 분포로 모델링하는 것을 특징으로 할 수 있다. 또한, 상기 인적성 검사의 표집 과정 방법은, 새롭게 생성된 질의에 대하여, 각 집단에 대한 초기 표집 분포 추 출이 완료되면, 상기 각 집단의 정보를 표시하는 단계; 를 더 포함할 수 있다. 또한, 상기 해당 질의 응답 정보를 출력하는 단계는, 상기 인적성 검사용 문장으로부터 토큰화된 토큰 문장에 대응되는 입력 임베딩을 생성하는 단계; 상기 응답 모델에 포함된 상기 BERT계열 모델을 이용해, 상기 입력 임 베딩으로부터 BERT 계열 모델의 히든 벡터를 생성하는 단계; 및 상기 응답 모델에 포함된 분류기를 이용해, 상 기 히든 벡터로부터 응답에 대응되는 출력 벡터를 생성하는 단계를 포함하는 것을 특징으로 할 수 있다. 또한, 상기 입력 임베딩을 생성하는 단계는, 토크나이저를 이용하여 상기 인적성 검사용 문장을 서브워드 토큰 들의 목록으로 토큰화하는 단계; 상기 토큰 문장의 시작 및 끝 각각에 CLS 토큰 및 SEP 토큰을 각각 추가하는 단계; 미리 훈련된 토큰 임베딩 행렬을를 이용하여 토큰 임베딩을 획득하는 단계; 상기 인적성 검사용 문장의 위치에 기초하여 각 토큰에 대한 포지션 임베딩을 생성하는 단계; 그룹 대표 토큰이 속한 그룹을 가리키는 그룹 임베딩을 획득하는 단계; 질의 대표 토큰이 속한 질의를 가리키는 질의 임베딩을 획득하는 단계; 및 상기 입력 임베딩을 생성하기 위해, 상기 토큰 임베딩, 상기 포지션 임베딩, 상기 그룹 임베딩, 및 상기 질의 임베딩을 합하는 단계를 포함하는 것을 특징으로 할 수 있다. 또한, 상기 히든 벡터를 생성하는 단계는, 미리 학습된 가중치 행렬로 상기 입력 임베딩을 투영함으로써(by projecting) 쿼리 행렬, 키 행렬 및 벨류 행렬을 획득하는 단계; 그룹끼리의 유사도 및 그룹과 질의 간의 연관 성을 나타내는 매트릭스인 그룹 질의 바이어스 행렬, 표준화(normalize)하기 위한 차원 인자, 상기 쿼리 행렬, 및 상기 키 행렬을 상기 BERT계열 모델의 소프트맥스 함수에 대입함으로써, 상기 BERT계열 모델의 스코어를 획 득하는 단계; 상기 스코어와 상기 벨류 행렬을 곱함으로써(by multiplying), 어텐션 출력을 획득하는 단계; 및 상기 어텐션 출력을 기초로 CLS 히든 벡터를 포함하는 상기 히든 벡터를 생성하는 단계를 포함하는 것을 특징으 로 할 수 있다. 또한, 상기 BERT계열 모델의 스코어를 획득하는 단계는, [수학식 1]을 이용하여 상기 스코어를 계산하고, [수학 식 1] 여기서, 는 스코어이고, 는 소프트맥스 함수이고, 는 쿼리 벡터 에 대응되는 가중치 행렬로 입력 임베딩을 투영함으로써 생성되는 쿼리 행렬이고, 는 키 벡터에 대응되는 가중 치 행렬로 입력 임베딩을 투영함으로써 생성되는 키 행렬이고, 는 그룹 질의 바이어스 행렬이고, 는 차원 인 자인 것을 특징으로 할 수 있다. 또한, 상기 히든 벡터를 생성하는 단계는, 상기 토큰 문장에 대해 패드 값을 추가함으로써, 입력되는 상기 인적 성 검사용 문장에 대한 어텐션 마스킹을 수행하는 단계를 더 포함하는 것을 특징으로 할 수 있다. 또한, 상기 분류기는 FC(Fully Connected) Layer으로 구현된 것을 특징으로 할 수 있다. 상술한 기술적 과제를 달성하기 위한 본 개시의 다른 측면에 따른 서버는, BERT 계열 모델을 기반으로한 응답자 집단을 대표하는 질의 응답 네트워크를 활용한 인적성 검사의 표집 과정 방법을 수행하고, 메모리; 및 상기 인 적성 검사를 위한 인적성 검사용 문장을 수신받고, 상기 인적성 검사용 문장을 기반으로 인공지능 기반의 응답 모델을 통해 학습되어 추천된 해당 질의 응답 정보를 출력하고, 상기 해당 질의 응답 정보를 기반으로 새로운 질의에 대한 각 집단의 초기 응답을 표집하는, 프로세서를 포함할 수 있다."}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 전술한 과제 해결 수단에 의하면, 일반 및 특수 집단을 대표할 만한 응답 모델을 생성하여, 인적성 문제 개발의 비용을 단축할 수 있는 효과를 제공한다. 또한, 본 개시의 전술한 과제 해결 수단에 의하면, 학습한 모델을 기반으로 사후 실제 집단의 응답을 샘플링하 여, 그 당위성을 검증할 수 있는 효과를 제공한다. 또한, 본 개시의 전술한 과제 해결 수단에 의하면, 인공지능을 활용하여 초기 표집을 만듦으로써, 기존 초기 표 본 추출 과정을 건너뛸 수 있고, 이 과정을 통해 재정적 및 시간적 이득을 취할 수 있는 효과가 있다. 또한, 본 개시의 전술한 과제 해결 수단에 의하면, 단순히 선형적으로 결과를 예측하는 과정에서 벗어나 비선형 적인 방식을 채택함으로써, 보다 유연하고 다양한 잠재 특징을 활용한 예측을 수행할 수 있는 효과가 있다."}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시 전체에 걸쳐 동일 참조 부호는 동일 구성요소를 지칭한다. 본 개시가 실시예들의 모든 요소들을 설명"}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "하는 것은 아니며, 본 개시가 속하는 기술분야에서 일반적인 내용 또는 실시예들 간에 중복되는 내용은 생략한 다. 명세서에서 사용되는 '부, 모듈, 부재, 블록'이라는 용어는 소프트웨어 또는 하드웨어로 구현될 수 있으며, 실시예들에 따라 복수의 '부, 모듈, 부재, 블록'이 하나의 구성요소로 구현되거나, 하나의 '부, 모듈, 부재, 블 록'이 복수의 구성요소들을 포함하는 것도 가능하다. 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 직접적으로 연결되어 있는 경우뿐 아니라, 간접적으로 연결되어 있는 경우를 포함하고, 간접적인 연결은 무선 통신망을 통해 연결되는 것을 포함 한다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 명세서 전체에서, 어떤 부재가 다른 부재 \"상에\" 위치하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재가 존재하는 경우도 포함한다. 제 1, 제 2 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하기 위해 사용되는 것으로, 구성요소가 전술된 용어들에 의해 제한되는 것은 아니다. 단수의 표현은 문맥상 명백하게 예외가 있지 않는 한, 복수의 표현을 포함한다. 각 단계들에 있어 식별부호는 설명의 편의를 위하여 사용되는 것으로 식별부호는 각 단계들의 순서를 설명하는 것이 아니며, 각 단계들은 문맥상 명백하게 특정 순서를 기재하지 않는 이상 명기된 순서와 다르게 실시될 수 있다. 이하 첨부된 도면들을 참고하여 본 개시의 작용 원리 및 실시예들에 대해 설명한다. 본 명세서에서 본 개시에 따른 인공지능 기반의 인적성 검사의 표집 시스템은 연산처리를 수행하여 사용자에게 결과를 제공할 수 있는 다양한 장치들이 모두 포함된다. 예를 들어, 본 개시에 따른 인공지능 기반의 인적성 검 사의 표집 시스템은, 컴퓨터, 서버 및 휴대용 단말기를 모두 포함하거나, 또는 어느 하나의 형태가 될 수 있다. 여기에서, 컴퓨터는 예를 들어, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱(desktop), 랩톱(laptop), 태블릿 PC, 슬레이트 PC 등을 포함할 수 있다. 서버는 외부 장치와 통신을 수행하여 정보를 처리하는 것으로써, 애플리케이션 서버, 컴퓨팅 서버, 데이터베이 스 서버, 파일 서버, 메일 서버, 프록시 서버 및 웹 서버 등을 포함할 수 있다. 휴대용 단말기는 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), WiBro(Wireless Broadband Internet) 단말, 스마트 폰(Smart Phone) 등과 같은 모든 종류의 핸드헬드(Handheld) 기반의 무선 통신 장치와 시계, 반지, 팔찌, 발찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head-mounted- device(HMD) 등과 같은 웨어러블 장치를 포함할 수 있다. 본 개시에 따른 인공지능 기반의 인적성 검사의 표집 시스템은, 인적성 검사를 위한 인적성 검사용 문장을 입력 하고, 입력된 인적성 검사용 문장을 기반으로 인공지능 기반의 응답 모델을 통해 학습되어 추천된 해당 질의 응 답 정보를 출력하며, 해당 질의 응답 정보를 기반으로 응답자 집단 내에서 일반 아이 집단과 특수 아이 집단을 표집하도록 제공될 수 있다. 이러한, 인공지능 기반의 인적성 검사의 표집 시스템은, 일반 및 특수 집단을 대표할 만한 응답 모델을 생성하 여 인적성 문제 개발의 비용을 단축할 수 있고, 학습한 모델을 기반으로 사후 실제 집단의 응답을 샘플링하여 그 당위성을 검증할 수 있다. 이하에서는, 인공지능 기반의 인적성 검사의 표집 시스템을 자세하게 살펴보기로 한다. 도 1은 본 개시의 인공지능 기반의 인적성 검사의 표집 시스템을 나타낸 도면이다. 도 2는 도 1의 서버의 구성 을 도시한다. 도 1 및 도 2를 참조하면, 인공지능 기반의 인적성 검사의 표집 시스템은 단말기와 서버를 포함 할 수 있다. 단말기는 인적성 검사를 위한 인적성 검사용 문장을 입력할 수 있다. 예를 들어, 단말기는 휴대성과 이동성이 보장되는 무선 통신 장치로서, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)- 2000, W-CDMA(W-Code Division Multiple Access), WiBro(Wireless Broadband Internet) 단말, 스마트 폰 (Smart Phone) 등과 같은 모든 종류의 핸드헬드(Handheld) 기반의 무선 통신 장치와 시계, 반지, 팔찌, 발찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head-mounted-device(HMD) 등과 같은 웨어러블 장치를 포함 할 수 있다. 서버는 통신부, 메모리, 프로세서를 포함할 수 있다. 통신부는 단말기와 통신을 수행할 수 있다. 이때, 통신부는 와이파이(Wifi) 모듈, 와이브로 (Wireless broadband) 모듈 외에도, GSM(global System for Mobile Communication), CDMA(Code Division Multiple Access), WCDMA(Wideband Code Division Multiple Access), UMTS(universal mobile telecommunications system), TDMA(Time Division Multiple Access), LTE(Long Term Evolution), 4G, 5G, 6G 등 다양한 무선 통신 방식을 지원하는 무선 통신 모듈을 포함할 수 있다. 메모리는 본 장치 내의 구성요소들의 동작을 제어하기 위한 알고리즘 또는 알고리즘을 재현한 프로그램에 대한 데이터를 저장할 수 있고, 메모리에 저장된 데이터를 이용하여 전술한 동작을 수행하는 적어도 하나 의 프로세서로 구현될 수 있다. 여기에서, 메모리와 프로세서는 각각 별개의 칩으로 구현될 수 있다. 또한, 메모리와 프로세서는 단일 칩으로 구현될 수도 있다. 메모리는 본 장치의 다양한 기능을 지원하는 데이터와, 프로세서의 동작을 위한 프로그램을 저장할 수 있고, 입/출력되는 데이터들을 저장할 있고, 본 장치에서 구동되는 다수의 응용 프로그램(application program 또는 애플리케이션(application)), 본 장치의 동작을 위한 데이터들, 명령어들을 저장할 수 있다. 이러 한 응용 프로그램 중 적어도 일부는, 무선 통신을 통해 외부 서버로부터 다운로드 될 수 있다. 이러한, 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), SSD 타입 (Solid State Disk type), SDD 타입(Silicon Disk Drive type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(random access memory; RAM), SRAM(static random access memory), 롬(read-only memory; ROM), EEPROM(electrically erasable programmable read-only memory), PROM(programmable read-only memory), 자기 메모리, 자기 디스크 및 광디스 크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 또한, 메모리는 본 장치와는 분리되어 있으나, 유선 또는 무선으로 연결된 데이터베이스가 될 수도 있다. 프로세서는 인적성 검사의 표집 과정과 관련된 동작을 제어할 수 있다. 일부 실시예들에서, 프로세서(12 3)는 문장으로 표현된 문항 및 특정 집단의 응답 경향(또는 특성)을 BERT 계열의 인코더 모델에 학습시킬 수 있 다. 일부 실시예들에서, 프로세서는 BERT 계열의 인코더 모델을 활용하여, 새로운 문제에 대한 응답 샘플 을 생성할 수 있다. 여기서, BERT 계열의 인코더 모델은 문항을 입력받아 비선형적 방식으로 응답을 반환하는 모델일 수 있다. 구체적으로, BERT 계열의 인코더 모델은 각 응답자 집단에 대한 특정 문제의 응답을 비선형적 인 방식으로 추출해 낼 수 있는 인공지능 모델일 수 있다. BERT 계열의 인코더 모델은, 예를 들면, BERT,BERTa, ELECTRA 등으로 구현될 수 있다. 일부 실시예들에서, BERT 계열의 인코더 모델은 토큰 임베딩(Token Embedding), 포지션 임베딩(Position Embedding)을 포함할 수 있다. 다른 실시예들에서, BERT 계열의 인코더 모델은 모델이 집단 및 문항에 대한 정 보를 학습하기 위해 모델의 입력 임베딩에 각 집단을 나타내는 그룹 임베딩(Group Embedding)과 문항을 나타내 는 질의 임베딩(Question Embedding)을 더 포함할 수 있다. 샘플들의 분포에서 집단을 대표할 통계값(평균-표준편차, 최대-최소 등)이 추출될 수 있다. 추출된 통계값은 사 전에 문제개발에 활용될 수 있다. 일 실시예에서, 프로세서는 인적성 검사를 위한 인적성 검사용 문장을 수신받을 수 있고, 인적성 검사용 문장을 기반으로 인공지능 기반의 응답 모델을 통해 학습되어 추천된 해당 질의 응답 정보를 출력할 수 있다. 프로세서는 해당 질의 응답 정보를 기반으로 응답자 집단 내에서 일반 아이 집단과 특수 아이 집단을 표집 할 수 있다. 전술한 바에 의하면, 새로운 문항에 대하여 응답하는 모델을 생성함으로써, 문제 개발 및 표집에 대한 비용을 줄이는 효과가 있다. 도 3은 본 개시에 따른 인공지능 기반의 인적성 검사의 표집 방법을 나타낸 순서도이다. 도 4는 도 2의 프로세 서에서 인공지능 기반의 응답 모델을 통해 학습되어 추천된 해당 질의 응답 정보를 출력하는 과정을 일 예로 나 타낸 도면이다. 도 5 및 도 6은 본 개시에 따른 인공지능 기반의 응답 모델의 학습 방법을 나타낸 도면들이다. 도 3 및 도 4를 참조하면, 인공지능 기반의 인적성 검사의 표집 방법은, 수신 단계(S310), 출력 단계(S320), 표 집 단계(S330)를 포함할 수 있다. 수신 단계는 프로세서를 통해, 인적성 검사를 위한 인적성 검사용 문장(A)을 수신받을 수 있다(S310). 출력 단계는 프로세서를 통해, 인적성 검사용 문장(A)을 기반으로 인공지능 기반의 응답 모델(B, C)을 통 해 학습되어 추천된 해당 질의 응답 정보(D)를 출력할 수 있다(S320). 인공지능 기반의 응답 모델(B, C)의 학습 은 실제 일반 아이와 실제 특수 아이의 응답을 통해 진행될 수 있다. 여기에서, 인공지능 기반의 응답 모델(B,C)은 BERT 모델(B)과 FC(Fully Connected Layer)(C)를 포함할 수 있다. 이때, BERT 모델(B)은 사전 학습된 대용량의 레이블링 되지않는(unlabeled) 데이터를 이용하여 언어 모델 (Language Model)을 학습하고, 이를 토대로 특정 작업(문서 분류, 질의응답, 번역 등)을 위한 신경망을 추가하 는 전이 학습 방법이다. 이러한, BERT 모델(B)은 기본적으로 대량의 단어 임베딩 등에 대해 사전 학습이 되어 있는 모델을 제공하기 때문에, 상대적으로 적은 자원만으로도 충분히 자연어 처리의 여러 일을 수행할 수 있다. 또한, FC(Fully Connected Layer)(C)는 Convolution/Pooling 프로세스의 결과를 취하여 이미지를 정의된 라벨 로 분류하는데 사용될 수 있다. Convolution/Pooling의 출력은 각각 특정 입력 이미지내의 객체가 라벨에 속할 확률을 나타내는 값의 단일 벡터로 평탄화될 수 있다. 즉, FC(Fully Connected Layer)(C)는 한층의 모든 뉴런이 다음층의 모든 뉴런과 연결된 상태로, 2차원의 배열 형태 이미지를 1차원의 평탄화 작업을 통해 이미지를 분류 하는데 사용되는 계층일 수 있다. 이때, FC(Fully Connected Layer)(C)는 분류기(Softmax) 함수로 분류될 수 있다. 표집 단계는 프로세서를 통해, 해당 질의 응답 정보(D)를 기반으로 응답자 집단 내에서 일반 아이 집단과 특수 아이 집단을 표집할 수 있다(S330). 예를 들어, 프로세서는 4개 이상의 질의 응답 정보(D)를 기반으 로, 일반 아이들과 특수 아이들이 질의 응답한 것에 대해서 일반 아이 집단과 특수 아이 집단을 표집할 수 있다. 이때, 프로세서는 응답자의 분포를 가우시안 분포로 모델링할 수 있다. 즉, 프로세서는 일반 아이들과 특수 아이들이 질의 응답한 것에 대해서, 일반 아이 집단의 분포와 특수 아이 집단의 분포를 가우시안 분포로 모델링할 수 있다. 이러한, 본 개시는 일반 및 특수 집단을 대표할 만한 응답 모델을 생성하여, 인적성 문제 개발의 비용을 단축할 수 있다. 프로세서는 인공지능을 구현할 수 있다. 인공지능이란 사람의 신경세포(biological neuron)를 모사하여 기 계가 학습하도록 하는 인공신경망(Artificial Neural Network) 기반의 기계 학습법을 의미한다. 인공지능의 방 법론에는 학습 방식에 따라 훈련데이터로서 입력데이터와 출력데이터가 같이 제공됨으로써 문제(입력데이터)의 해답(출력데이터)이 정해져 있는 지도학습(supervised learning), 및 출력데이터 없이 입력데이터만 제공되어문제(입력데이터)의 해답(출력데이터)이 정해지지 않는 비지도학습(unsupervised learning), 및 현재의 상태 (State)에서 어떤 행동(Action)을 취할 때마다 외부 환경에서 보상(Reward)이 주어지는데, 이러한 보상을 최대 화 하는 방향으로 학습을 진행하는 강화학습(reinforcement learning) 으로 구분될 수 있다. 또한 인공지능의 방법론은 학습 모델의 구조인 아키텍처에 따라 구분될 수도 있는데, 널리 이용되는 딥러닝 기술의 아키텍처는, 합성곱신경망(CNN; Convolutional Neural Network), 순환신경망(RNN; Recurrent Neural Network), 트랜스포머 (Transformer), 생성적 대립 신경망(GAN; generative adversarial networks) 등으로 구분될 수 있다. 인공지능 모델은 하나의 인공지능 모델일 수 있고, 복수의 인공지능 모델로 구현될 수도 있다. 인공지능 모델은 뉴럴 네트워크(또는 인공 신경망)로 구성될 수 있으며, 기계학습과 인지과학에서 생물학의 신경을 모방한 통계 학적 학습 알고리즘을 포함할 수 있다. 뉴럴 네트워크는 시냅스의 결합으로 네트워크를 형성한 인공 뉴런(노 드)이 학습을 통해 시냅스의 결합 세기를 변화시켜, 문제 해결 능력을 가지는 모델 전반을 의미할 수 있다. 뉴 럴 네트워크의 뉴런은 가중치 또는 바이어스의 조합을 포함할 수 있다. 뉴럴 네트워크는 하나 이상의 뉴런 또는 노드로 구성된 하나 이상의 레이어(layer)를 포함할 수 있다. 예를 들어, 하나 이상의 레이어(layer)는 input layer, hidden layer, output layer를 포함할 수 있다. 뉴럴 네트워크는 뉴런의 가중치를 학습을 통해 변화시킴 으로써 임의의 입력(input)으로부터 예측하고자 하는 결과(output)를 추론할 수 있다. 프로세서는 뉴럴 네트워크를 생성하거나, 뉴럴 네트워크를 훈련(train, 또는 학습(learn))하거나, 수신되 는 입력 데이터를 기초로 연산을 수행하고, 수행 결과를 기초로 정보 신호(information signal)를 생성하거나, 뉴럴 네트워크를 재훈련(retrain)할 수 있다. 뉴럴 네트워크의 모델들은 GoogleNet, AlexNet, VGG Network 등 과 같은 CNN(Convolution Neural Network), R-CNN(Region with Convolution Neural Network), RPN(Region Proposal Network), RNN(Recurrent Neural Network), S-DNN(Stacking-based deep Neural Network), S- SDNN(State-Space Dynamic Neural Network), Deconvolution Network, DBN(Deep Belief Network), RBM(Restrcted Boltzman Machine), Fully Convolutional Network, LSTM(Long Short-Term Memory) Network, Classification Network 등 다양한 종류의 모델들을 포함할 수 있으나 이에 제한되지는 않는다. 프로세서 는 뉴럴 네트워크의 모델들에 따른 연산을 수행하기 위한 하나 이상의 프로세서를 포함할 수 있다. 예를 들어, 뉴럴 네트워크는 심층 뉴럴 네트워크 (Deep Neural Network)를 포함할 수 있다. 뉴럴 네트워크는 CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), 퍼셉트론(perceptron), 다층 퍼셉트론(multilayer perceptron), FF(Feed Forward), RBF(Radial Basis Network), DFF(Deep Feed Forward), LSTM(Long Short Term Memory), GRU(Gated Recurrent Unit), AE(Auto Encoder), VAE(Variational Auto Encoder), DAE(Denoising Auto Encoder), SAE(Sparse Auto Encoder), MC(Markov Chain), HN(Hopfield Network), BM(Boltzmann Machine), RBM(Restricted Boltzmann Machine), DBN(Depp Belief Network), DCN(Deep Convolutional Network), DN(Deconvolutional Network), DCIGN(Deep Convolutional Inverse Graphics Network), GAN(Generative Adversarial Network), LSM(Liquid State Machine), ELM(Extreme Learning Machine), ESN(Echo State Network), DRN(Deep Residual Network), DNC(Differentiable Neural Computer), NTM(Neural Turning Machine), CN(Capsule Network), KN(Kohonen Network) 및 AN(Attention Network)를 포함 할 수 있으나 이에 한정되는 것이 아닌 임의의 뉴럴 네트워크를 포함할 수 있음은 통상의 기술자가 이해할 것이다. 프로세서는 GoogleNet, AlexNet, VGG Network 등과 같은 CNN(Convolution Neural Network), R- CNN(Region with Convolution Neural Network), RPN(Region Proposal Network), RNN(Recurrent Neural Network), S-DNN(Stacking-based deep Neural Network), S-SDNN(State-Space Dynamic Neural Network), Deconvolution Network, DBN(Deep Belief Network), RBM(Restrcted Boltzman Machine), Fully Convolutional Network, LSTM(Long Short-Term Memory) Network, Classification Network, Generative Modeling, eXplainable AI, Continual AI, Representation Learning, AI for Material Design, 자연어 처리를 위한 BERT, SP-BERT, MRC/QA, Text Analysis, Dialog System, GPT-3, GPT-4, 비전 처리를 위한 Visual Analytics, Visual Understanding, Video Synthesis, ResNet 데이터 지능을 위한 Anomaly Detection, Prediction, Time- Series Forecasting, Optimization, Recommendation, Data Creation 등 다양한 인공지능 구조 및 알고리즘을 이용할 수 있으며, 이에 제한되지 않는다. CNN은 영상의 각 영역에 대해 복수의 필터를 적용하여 특징 지도(Feature Map)를 만들어 내는 컨볼루션 층 (Convolution Layer)과 특징 지도를 공간적으로 통합함으로써, 위치나 회전의 변화에 불변하는 특징을 추출할 수 있도록 하는 통합층(Pooling Layer)을 번갈아 수차례 반복하는 구조로 형성될 수 있다. 이를 통해, 점, 선, 면 등의 낮은 수준의 특징에서부터 복잡하고 의미 있는 높은 수준의 특징까지 다양한 수준의 특징을 추출해낼수 있다. 컨볼루션 층은 입력 영상의 각 패치에 대하여 필터와 국지 수용장(Local Receptive Field)의 내적에 비선형 활 성 함수(Activation Function)를 취함으로써 특징 지도(Feature Map)를 구할 수 있다. 다른 네트워크 구조와 비교하여, CNN은 희소한 연결성 (Sparse Connectivity)과 공유된 가중치(Shared Weights)를 가진 필터를 사용하는 특징을 가질 수 있다. 이러한 연결구조는 학습할 모수의 개수를 줄여주고, 역 전파 알고리즘을 통한 학습을 효율적으로 만들어 결과적으로 예측 성능을 향상시킬 수 있다. 이와 같이, 컨볼루션 층과 통합 층의 반복을 통해 최종적으로 추출된 특징은 다중 신경망(MLP: Multi-Layer Perceptron)이나 서포트 벡터 머신(SVM: Support Vector Machine)과 같은 분류 모델이 완전 연결 층(Fully- connected Layer)의 형태로 결합되어 압축모델 학습 및 예측에 사용될 수 있다. 도 5를 참조하면, 인공지능 기반의 응답 모델(NN)은 뉴럴 네트워크 구조일 수 있고, 예를 들어 합성곱 신경망 (CNN)의 구조일 수 있다. 인공지능 기반의 응답 모델(NN)은 복수의 레이어들(L1 내지 Ln)을 포함할 수 있다. 복수의 레이어들(L1 내지 Ln) 각각은 선형 레이어 또는 비선형 레이어일 수 있으며, 일 실시예에 있어서, 적어도 하나의 선형 레이어 및 적어도 하나의 비선형 레이어가 결합되어 하나의 레이어로 지칭될 수도 있다. 예시적으로, 선형 레이어는 컨볼 루션 레이어(convolution layer) 및 완전 연결 레이어(fully connected layer)를 포함할 수 있으며, 비선형 레 이어는 풀링(pooling layer) 및 활성 레이어(activation layer)를 포함할 수 있다. 예시적으로, 제1 레이어(L1)는 컨볼루션 레이어이고, 제2 레이어(L2)는 풀링 레이어이고, 제n 레이어(Ln)는 출 력 레이어로서 완전 연결 레이어일 수 있다. 인공지능 학습모델(NN)은 활성 레이어를 더 포함할 수 있으며, 다 른 종류의 연산을 수행하는 레이어를 더 포함할 수 있다. 복수의 레이어들(L1 내지 Ln) 각각은 입력되는 데이터(예컨대, 이미지 프레임) 또는 이전 레이어에서 생성된 피 처맵을 입력 피처맵으로서 수신하고, 입력 피처맵을 연산함으로써 출력데이터(QR)를 생성할 수 있다. 일 실시예 에서, 출력데이터(QR)다양한 네트워크 파라미터들(평균 값, 확률 웨이트, 표준편차)일 수 있다. 피처맵은 입력 데이터의 다양한 특징이 표현된 데이터를 의미한다. 피처맵들(FM1, FM2, FMn)은 예컨대 2차원 매 트릭스 또는 3차원 매트릭스(또는 텐서(tensor)) 형태를 가질 수 있다. 일 실시예에서, 입력되는 제1 피처맵 (FM1)은 현재 상태에 대응하는 데이터일 수 있다. 피처맵들(FM1, FM2, FMn)은 너비(W)(또는 칼럼), 높이(H)(또 는 로우) 및 깊이(D)를 가지며, 이는 좌표상의 x축, y축 및 z축에 각각 대응될 수 있다. 이때, 깊이(D)는 채널 수로 지칭될 수 있다. 제1 레이어(L1)는 제1 피처맵(FM1)을 웨이트 커널(WK)과 컨볼루션함으로써 제2 피처맵(FM2)을 생성할 수 있다. 웨이트 커널(WK)은 제1 피처맵(FM1)을 필터링할 수 있으며, 필터 또는 맵으로도 지칭될 수 있다. 웨이트 커널 (WK)의 깊이, 즉 채널 개수는 제1 피처맵(FM1)의 깊이, 즉 채널 개수와 동일하며, 웨이트 커널(WK)과 제1 피처 맵(FM1)의 동일한 채널끼리 컨볼루션 될 수 있다. 웨이트 커널(WK)이 제1 피처맵(FM1)을 슬라이딩 윈도우로 하 여 횡단하는 방식으로 시프트 될 수 있다. 시프트되는 양은 \"스트라이드(stride) 길이\" 또는 \"스트라이드\"로 지 칭될 수 있다. 각 시프트 동안, 웨이트 커널(WK)에 포함되는 웨이트 값들 각각이 제1 피처맵(FM1)과 중첩되는 영역에서의 모든 픽셀 데이터들과 곱해지고 더해질 수 있다. 웨이트 커널(WK)에 포함되는 웨이트 값들 각각이 제1 피처맵(FM1)과 중첩되는 영역에서의 제1 피처맵(FM1)의 데이터들을 추출 데이터라 칭할 수 있다. 제1 피처맵(FM1)과 웨이트 커 널(WK)이 컨볼루션 됨에 따라, 제2 피처맵(FM2)의 하나의 채널이 생성될 수 있다. 하나의 웨이트 커널(WK)이 제 공될 수 있고, 복수의 웨이트 맵들이 제1 피처맵(FM1)과 컨볼루션 되어, 제2 피처맵(FM2)의 복수의 채널들이 생 성될 수 있다. 다시 말해, 제2 피처맵(FM2)의 채널의 수는 웨이트 맵의 개수에 대응될 수 있다. 제2 레이어(L2)는 풀링을 통해 제2 피처맵(FM2)의 공간적 크기(spatial size)를 변경함으로써, 제3 피처맵(FM 3)을 생성할 수 있다. 풀링은 샘플링 또는 다운-샘플링으로 지칭될 수 있다. 2차원의 풀링 윈도우(PW)가 풀링 윈도우(PW)의 사이즈 단위로 제2 피처맵(FM2) 상에서 시프트 되고, 풀링 윈도우(PW)와 중첩되는 영역의 픽셀 데 이터들 중 최대값(또는 픽셀 데이터들의 평균값)이 선택될 수 있다. 이에 따라, 제2 피처맵(FM2)으로부터 공간 적 사이즈가 변경된 제3 피처맵(FM3)이 생성될 수 있다. 제3 피처맵(FM3)의 채널과 제2 피처맵(FM2)의 채널 개 수는 동일하다. 일 실시예에서, 제3 피처맵(FM3)은 컨볼루션이 완료된 출력 피처맵에 대응될 수 있다. 제n 레이어(Ln)는 제n 피처맵(FMn)의 피처들을 조합함으로써 입력 데이터의 클래스(class)(CL)를 분류할 수 있 다. 또한, 제n 레이어(Ln)는 클래스에 대응되는 출력데이터(QR)를 생성할 수 있다. 실시예에 있어서, 입력 데이 터는 현재 상태에 대응하는 데이터에 대응될 수 있으며, 제n 레이어(Ln)는 이전 레이어로부터 제공되는 제n 피 처맵(FMn)을 복수의 행동들에 대응하는 클래스들을 추출함으로써 최적의 행동을 판별하기 위한 출력데이터(QR) 를 생성할 수 있다. 도 6을 참조하면, 입력 피처맵들은 D개의 채널들을 포함하고, 각 채널의 입력 피처맵은 H행 W열의 크기를 가질 수 있다(D, H, W는 자연수). 커널들 각각은 R행 S열의 크기를 갖고, 커널들은 입력 피처맵들 의 채널 수(또는 깊이)(D) 에 대응되는 개수의 채널들을 포함할 수 있다(R, S는 자연수). 출력 피처맵들 은 입력 피처맵들과 커널들 간의 3차원 컨볼루션 연산을 통해 생성될 수 있고, 컨볼루션 연산에 따라 Y개의 채널들을 포함할 수 있다. 하나의 입력 피처맵과 하나의 커널 간의 컨볼루션 연산을 통해 출력 피처맵이 생성되는 과정은, 2차원 컨볼루션 연산이 전체 채널들의 입력 피처맵들과 전체 채널들의 커널들 간에 수행됨으로써, 전체 채널들의 출 력 피처맵들이 생성될 수 있다. 도 6을 다시 참조하면, 설명의 편의를 위해, 입력 피처맵은 6x6 크기(size)를 가지고, 원본 커널은 3x3 크기를 가지고, 출력 피처맵은 4x4 크기인 것으로 가정하나, 이에 제한되지 않으며 인공지능 기반의 응답 모델(NN)은 다양한 크기의 피처맵들 및 커널들로 구현될 수 있다. 또한, 입력 피처맵, 원본 커널 및 출력 피처맵에 정의된 값들은 모두 예시적인 값들일 뿐이고, 본 개시에 따른 실시예들이 이에 제 한되지 않는다. 원본 커널은 입력 피처 맵에서 3x3 크기의 윈도우 단위로 슬라이딩하면서 컨볼루션 연산을 수행할 수 있다. 컨볼루션 연산은 입력 피처 맵의 어느 윈도우의 각 피처 데이터 및 원본 커널에서 대응되는 위 치의 각 웨이트 값들 간의 곱셈을 함으로써 획득된 값들을 모두 합산함에 따라 출력 피처 맵의 각 피처 데 이터를 구하는 연산을 나타낼 수 있다. 다시 말해, 하나의 입력 피처 맵과 하나의 원본 커널 간의 컨볼루션 연산은 입력 피처 맵의 추 출 데이터 및 원본 커널의 대응되는 웨이트 값들의 곱셈 및 곱셈 결과들의 합산을 반복적으로 수행함으로 써 처리될 수 있고, 컨볼루션 연산의 결과로서 출력 피처 맵이 생성될 수 있다. 한편, 인공지능 기반의 응답 모델(B,C)은, 딥 러닝 기반으로 학습된 인공지능 모델을 의미할 수 있으며, 일 예 로, CNN(Convolutional Neural Network)을 이용하여 학습된 모델을 의미할 수도 있다. 또한, 인공지능 기반의 응답 모델(B,C)은, Natural Language Processing(NLP), Random Forest (RF), Support Vector Machine (SVC), eXtra Gradient Boost (XGB), Decision Tree (DC), Knearest Neighbors (KNN), Gaussian Naive Bayes (GNB), Stochastic Gradient Descent (SGD), Linear Discriminant Analysis (LDA), Ridge, Lasso 및 Elastic net 중 적어도 하나의 알고리즘을 포함할 수도 있다. 한편, 본 개시에 따른 인공지능 기반의 인적성 검사의 표집 방법은 표시 단계(S340)를 더 포함할 수 있다. 표시 단계(S340)는 프로세서를 통해, 응답자 집단 내에서 일반 아이 집단과 특수 아이 집단의 표집이 완료 되면, 단말기를 통해 일반 아이 집단의 정보와 특수 아이 집단의 정보를 표시할 수 있다. 예를 들어, 단말 기는 일반 아이 집단의 질의 응답 정보와 특수 아이 집단의 질의 응답 정보를 표시할 수 있다. 한편, 본 개시는 행동 특성을 고려하여 인공지능 기반의 응답 모델을 설계할 수 있다. 즉, 충동적인 아이의 경우, 어떤 일을 바로 하려고 하거나 충동을 참기 힘들어하는 행동 특성을 보일 수 있다. 충동성의 요인을 심리 검사의 하위 요인으로 측정하고자 하는 경우, 이러한 행동 특성을 기술하는 형태로 문항 을 만들 수 있다. 예를 들어, \"1)하고 싶은 일이 생기면 바로 해야 한다.\" \"2) 나는 사고 싶은 것이 있으면 참 기가 힘들다.\"라는 형태의 문항을 만들 수 있다. 한편, 본 개시는 전문가 검수 과정을 기반으로 인공지능 기반의 응답 모델을 설계할 수 있다. 즉, 전문가 검수 과정은 통계 분석을 가장 많이 사용할 수 있다. 예를 들어, 통계 분석은 요인 분석, 신뢰도 분 석, 수렴 타당도 분석을 포함할 수 있다. 요인 분석은, 상관이 높은 문항들끼리 묶어, 어떤 문항들이 각 요인을 대표하기 적절한지 확인하는 과정일 수 있다. 신뢰도 분석은 측정하고자 하는 것을 얼마나 일관성 있게 측정하 는지 확인하는 과정일 수 있다. 수렴 타당도 분석은 개발된 검사에서 측정하는 요인과 동일한 특성을 측정한 검 사 점수와의 상관 관계를 분석하여, 측정하려고 하는 것을 제대로 측정하고 있는지 확인하는 과정일 수 있다.한편, 본 개시는 충동성 검사, 능력 검사를 기반으로 인공지능 기반의 응답 모델을 설계할 수 있다. 즉, MFFT(충동성 검사)는, 도구 검사로서 검사자가 아이에게 그림 여러 개를 보여주며, 위에 있는 그림과 같은 그림을 고르게 하는 것으로, 정답 문항이 6개의 그림 중 1개이며, 반응 속도 및 맞은 개수에 따라 충동성의 유 형이 분류될 수 있다. 능력 검사는, 지능 검사와 역량 검사 등에서 정답이 있는 문제를 풀고 이에 대한 검사 결과를 받아볼 수 있는 것으로, 능력 검사는 1~4점으로 응답하는 자기 보고식 검사와는 다르다. 한편, 본 개시는 특수 아이의 심각성을 고려하여 인공지능 기반의 응답 모델을 설계할 수 있다. 즉, 특수 아이의 검사 결과는 또래의 일반 아이의 집단의 평균, 표준편차(규준)에 따라 산출된 표준점수로 표시 되기 때문에, 심리 검사 요인의 점수가 평균에서 1표준편차 이상 떨어져 있는 경우, 높거나 낮은 점수로 산출할 수 있다. 도 1 및 도 2에 도시된 구성 요소들의 성능에 대응하여 적어도 하나의 구성요소가 추가되거나 삭제될 수 있다. 또한, 구성 요소들의 상호 위치는 시스템의 성능 또는 구조에 대응하여 변경될 수 있다는 것은 당해 기술 분야 에서 통상의 지식을 가진 자에게 용이하게 이해될 것이다. 도 3은 복수의 단계를 순차적으로 실행하는 것으로 기재하고 있으나, 이는 본 실시예의 기술 사상을 예시적으로"}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술분야에서 통상의 지식을 가진 자라면 본 실시예의 본질 적인 특성에서 벗어나지 않는 범위에서 도 3에 기재된 순서를 변경하여 실행하거나 복수의 단계 중 하나 이상의 단계를 병렬적으로 실행하는 것으로 다양하게 수정 및 변형하여 적용 가능할 것이므로, 도 3은 시계열적인 순서 로 한정되는 것은 아니다. 도 7은 본 개시에 따른 인적성 검사의 표집 과정 방법을 나타낸 순서도이다. 도 7을 참조하면, 본 개시의 인적성 검사의 표집 과정 방법은, 서버에 의해 수행될 수 있고, 서버의 BERT 계열 모델을 기반으로 하는 방법이며, 응답자 집단을 대표하는 질의 응답 네트워크를 활용한 방법일 수 있 다. 본 개시의 인적성 검사의 표집 과정 방법은 수신 단계(S710), 출력 단계(S720), 및 표집 단계(S730)를 포함할 수 있다. 수신 단계는, 프로세서가 인적성 검사를 위한 인적성 검사용 문장을 수신하는 단계일 수 있다(S710). 출력 단계는, 프로세서가 인적성 검사용 문장을 기반으로, 인공지능 기반의 응답 모델을 통해 학습되어 추 천된 해당 질의 응답 정보를 출력하는 단계일 수 있다(S720). 일부 실시예들에서, 단계 S720는, 상기 인적성 검사용 문장으로부터 토큰화된 토큰 문장에 대응되는 입력 임베 딩을 생성하는 단계, 상기 응답 모델에 포함된 상기 BERT계열 모델을 이용해, 상기 입력 임베딩으로부터 BERT 계열 모델의 히든 벡터를 생성하는 단계, 및 상기 응답 모델에 포함된 분류기를 이용해, 상기 히든 벡터로부터 응답에 대응되는 출력 벡터를 생성하는 단계를 포함할 수 있다. 표집 단계는, 프로세서가 해당 질의 응답 정보를 기반으로 새로운 질의에 대한 각 집단의 초기 응답을 표 집하는 단계일 수 있다(S730). 기존의 표집 단계에서, 설계자가 초기에 검사 문제를 개발할 때, 검사 문제가 잘 개발되었는지 또는 학생들이 검사 문제에 대해 잘 응답할지 예측하기 위해서, 설계자가 실제 학생들을 대상으로 검사 문제에 대한 응답 결과 를 수집할 수 있다. 그리고, 설계자가 응답 결과의 평균, 표준 편차 등을 계산함으로써 초기 단계에서 검사 문 제의 유효성 등을 파악할 수 있다. 그런데, 이러한 과정에서 소모되는 비용이 매우 크다. 본 개시에 의하면, 학생들을 대신할 인공지능 모델이 검사 문제에 대한 응답을 출력함으로써, 문제 개발 과정에 서 드는 비용을 크게 절감할 수 있는 효과가 있다. 도 8은 본 개시에 따른 인적성 검사의 표집 과정 방법을 설명하기 위한 도면이다. 도 8을 참조하면, 본 개시의 인적성 검사의 표집 과정 방법에서 수행되는 인공지능 기반의 응답 모델은, 임베딩 레이어, BERT(Bidirectional Encoder Representations from Transformers, 820), 및 분류기(또는 클레시 파이어(classifier), 830)을 포함할 수 있다.인공지능 기반의 응답 모델에 입력되는 입력 정보의 경우, 입력 정보에 대응되는 문장(sentence, STS)이 토큰화 될 수 있다. 구체적으로, 입력될 문장(STS) 중 단어마다 단어에 대응되는 토큰(token)이 생성됨으로써, 입력될 문장(STS)이 복수의 토큰들로 분할될 수 있다. 예를 들면, 입력될 문장(STS)은 ”질문 주말에 집에서 쉬기보다 밖에서 활동하는 것을 좋아한다. 응답 그렇다”일 수 있고, 각 단어별로 토큰화될 수 있다. 토큰의 개수는 n개 일 수 있다. n은 자연수일 수 있다. 여기서, 문장(STS)이 토큰화된 토큰들 외에, CLS 토큰 또는 classification 토큰(도 8에 도시된 \"CLS\" 참조)이 추가될 수 있다. 또한, 문장(STS)이 토큰화된 토큰들에 SEP 토큰, Special Separator 토큰, 또는 separation 토큰이 추가될 수 있다. 이와 같이 입력 정보는 CLS 토큰, 문장(STS)이 토큰 화된 n개의 토큰들, 및 SEP 토큰을 포함할 수 있다. 입력 정보에 대응되는 토큰의 개수는 n+2일 수 있으나, 이 에 한정되는 것은 아니다. CLS 토큰과 SEP 토큰은 문장(STS)을 구분하기 위한 특수 토큰일 수 있다. CLS 토큰은 모든 문장(STS)의 가장 첫 번째 또는 문장(STS)의 시작하는 부분에 삽입되는 토큰일 수 있다. SEP 토큰은 첫 번 째 문장과 두 번째 문장을 구별하는 토큰으로, 각 문장 끝에 삽입될 수 있다. 예를 들면, 문장(STS)은 질문에 해당되는 질문 문장(QSN)과 응답에 해당되는 응답 문장(ANS)을 포함할 수 있다. 문장(STS)이 토큰화될 때, 질문 문장(QSN)에 해당되는 토큰들의 맨 첫 번째에 CLS 토큰이 삽입될 수 있다. 그리고, 질문 문장(QSN)에 대응되는 마지막 토큰과 응답 문장(ANS)에 대응되는 첫 번째 토큰 사이에 SEP 토큰이 삽입될 수 있다. 일부 실시예들에서, 문장(STS)에 포함되는 질문 문장(QSN)은 마스킹되지 않고, 문장(STS)에 포함되는 응답 문장 (ANS)은 마스킹될 수 있다. CLS 토큰, 마스킹되지 않은 질문 문장(QSN)의 토큰들, SEP 토큰, 및 마스킹된 응답 문장(ANS)의 토큰들이 BERT에 입력될 수 있다. 이에 따를 때, 문장(STS) 중 응답 부분이 마스킹되고, 질문 만 제공됨으로써, 인공지능 기반의 응답 모델이 마스킹된 응답 부분에 대해 학습할 수 있다. 문장(STS)이 토큰화되면, 토큰들이 임베딩 레이어에 입력될 수 있다. 임베딩 레이어를 통해 입력 임 베딩이 생성될 수 있다. 즉, 임베딩 레이어에서, 일부 실시예들에 따른 단계 S720에 포함되는, 인적성 검 사용 문장으로부터 토큰화된 토큰 문장에 대응되는 입력 임베딩을 생성하는 단계가 수행될 수 있다. 일부 실시 예들에서, 입력 임베딩은 토큰 임베딩, 포지션 임베딩, 그룹 임베딩, 및 질의 임베딩을 포함할 수 있다. BERT는 언어 표현을 사전 학습시키는 방법을 수행하는 것일 수 있다. 사전 학습은 BERT가 대량의 텍 스트 소스로 처음 학습되는 방법일 수 있다. 이후 학습 결과가 질문 답변 및 감정 분석과 같은 다른 자연어 처 리(NLP) 태스크에 적용될 수 있다. BERT는 복수의 인코더 블록들을 포함할 수 있다. 예를 들면, BERT는 제1 인코더 블록, 제2 인코 더 블록 내지 제12 인코더 블록을 포함할 수 있다. 도 8에서는 BERT이 12개의 인코더 블록을 포 함하는 것으로 도시되어 있으나, 이에 한정되는 것은 아니며, BERT이 12개보다 적은 개수의 인코더 블록을 포함하거나 12개보다 많은 개수의 인코더 블록을 포함할 수도 있다. 각 인코더 블록은 입력 그룹, 하나 이상의 애드 및 놈(add & norm), 및 피드 포워드를 포함할 수 있다. 예를 들면, 제2 인코더 블록은 입력 그룹 , 애드 및 놈(842, 844), 및 피드 포워드를 포함할 수 있다. 입력 그룹, 애드 및 놈(842, 844), 및 피드 포워드은 멀티-헤드 어텐션으로 지칭될 수 있다. 입력 그룹은 복수의 리니어들(8411, 8412, 8413), 복수의 스케일드 닷-프로덕트 어텐션들, 콘켓, 및 리니어을 포함할 수 있다. 복수의 리니어들(8411, 8412, 8413)은 각각 쿼리(Query) 행렬(Q), 키(key) 행렬(K), 및 벨류(Value) 행렬(V)을 입력받을 수 있다. 리니어, 스케일드 닷-프로덕트 어텐션, 콘켓, 애드 및 놈, 및 피드 포워드는 BERT에서 이용 되는 일반적인 구성요소(element)일 수 있으며, 리니어, 스케일드 닷-프로덕트 어텐션, 콘켓, 애드 및 놈, 및 피드 포워드 각각의 동작 및 특성은 통상의 기술자에게 알려진 것과 동일할 수 있으므로, 이에 대한 설명은 생 략하기로 한다. BERT는 토큰들(TQNG)을 입력받아, 토큰들(TQNG) 각각에 대응되는 히든 벡터들(HVT)을 출력할 수 있다. 히 든 벡터의 출력 값은, 각 토큰에 대한 히든 벡터들(HVT) 각각은 토큰들(TQNG) 각각에 일대일로 대응될 수 있다. 히든 백터를 의미할 수 있다. 여기서, 히든 벡터들(HVT)의 첫 번째에 삽입된 CLS 히든 벡터(HVTC)는 CLS 토큰에 대한 출력일 수 있다. 히든 벡터들(HVT)의 텐서 디맨전(tensor dimension)은 약 768일 수 있으나, 이에 한정되 는 것은 아니다. BERT에서, 일부 실시예들에 따른 단계 S720에 포함되는, 상기 응답 모델에 포함된 상기 BERT계열 모델을 이용해, 상기 입력 임베딩으로부터 BERT 계열 모델의 히든 벡터를 생성하는 단계가 수행될 수 있다. 히든 벡터들(HVT)은 분류기에 제공될 수 있다. 분류기는 히든 벡터들(HVT)을 기초로 복수의 디맨전의 출력 벡터를 출력할 수 있다. 여기서, 출력 벡터의 디맨전은 도 8에 도시된 바와 같이, 5일 수 있으나, 이에 한 정되는 것은 아니며, 5보다 작거나 큰(예를 들어, 7) 값을 가질 수 있다. 분류기는 FC Layer를 포함할 수있다. 각 출력 벡터는 \"매우 그렇다\", \"그렇다\", \"보통이다\", \"아니다\", 및 \"매우 아니다\"를 나타내는 확률 값 을 포함할 수 있다. 예를 들면, \"매우 그렇다\"에 해당되는 출력 벡터의 값은 0.6, \"그렇다\"에 해당되는 출력 벡 터의 값은 0.2, \"보통이다\"에 해당되는 출력 벡터의 값은 0.1, \"아니다\"에 해당되는 출력 벡터의 값은 0.1, 및 \"매우 아니다\"에 해당되는 출력 벡터의 값은 0.0일 수 있다. 하지만, 이에 한정되는 것은 아니다. 분류기에서, 일부 실시예들에 따른 단계 S720에 포함되는, 상기 히든 벡터로부터 응답에 대응되는 출력 벡 터를 생성하는 단계가 수행될 수 있다. 일부 실시예들에서, 문장(STS)에는 접두사(prefix) 부분 또는 비교 어구에 해당되는 문장이 포함될 수 있다. 접 두사(prefix)는 집단 별로 특정될 수 있다. 이에 따르면, 집단 별 프리픽스가 문장(STS)에 포함됨으로써, BERT의 출력이 집단 별로 특정되고, 이에 따라 분류기의 출력 벡터의 값이 집단별로 특정될 수 있다. 따라서, 집단별 표집을 더 명확히 구분할 수 있는 효과가 있다. 도 9는 본 개시에 따른 집단별 인적성 검사 결과를 학습하는 과정을 설명하기 위한 도면이다. 설계자는 집단별로 인적성 검사를 시행할 수 있다. 집단은, 제1 집단(G1), 제2 집단(G2) 내지 제N 집단(GN)을 포함할 수 있다. N은 2이상의 자연수일 수 있다. 각 집단에 포함되는 대상자의 특성이 집단별로 다를 수 있다. 인적성 검사를 시행한 정보를 포함하는 인적성 검사 데이터가 수집될 수 있다. 인적성 검사 데이터는 학습 데이 터베이스에 저장될 수 있다. 인공지능 기반의 응답 모델은, 학습 데이터베이스에 저장된 인적성 검사 데이터를 기반으로 학습할 수 있다. 여 기서, 인공지능 기반의 응답 모델은 언어모델로 지칭될 수 있다. 이때, 언어모델은 새로운 인적성 검 사 문항을 더 입력받아서 학습할 수 있다. 여기서, 일 실시예에 따른 새로운 인적성 검사 문항은 응답 문 구가 마스킹된 문장을 포함할 수 있다. 언어모델은 집단별 인적성 검사를 표집하고 그 표집 결과를 출력할 수 있다. 표집 결과를 나타내는 표집 결과 데이터는 표집 데이터베이스에 저장될 수 있다. 도 10은 인적성 검사의 표집 과정 방법을 수행하기 위한 인공지능을 예시적으로 나타낸 도면들이다. 도 10을 참조하면, 인적성 검사의 표집 과정 방법을 수행하기 위한 인공지능은 도 10에 도시된 알고리즘과 같이 구현될 수 있다. 인공지능을 구현하기 위한 도 10의 알고리즘에서 분류 밀집 층(Classification dense layer C)이 모델에 추가될 수 있다. 이에 따라, 집단별 표집 데이터가 획득될 수 있다. 도 10의 알고리즘에 요구되는 것은 BERT 및 분류 밀집 층(Classification dense layer C)이고, 도 10의 알고리 즘에서 입력은 심리 테스트 질문의 세트(a set of psychometric test questions Q)이며, 도 10의 알고리즘에서 출력은 심리 테스트 질문에 대응하는 응답 값의 각 확률이다. 도 10의 알고리즘에서, 문장을 토큰화하는 단계가 수행된다. 그 다음, 토큰들을 BERT에 대한 입력 텐서들로 전 환하는 단계가 수행된다. 그 다음, 입력 텐서를 BERT 모델에 통과시키는 단계가 수행된다. 그 다음, 토큰들 앞 에 있는 CLS 토큰의 히든 스테이트(hidden states H)를 획득하는 단계가 수행된다. 그 다음, 심리 테스트 질문 의 세트를 분류 밀집 층(Classification dense layer C)에 통과시키는 단계가 수행된다. 그 다음, 각 라벨에 대 한 로짓 값을 획득하는 단계가 수행된다. 각 라벨을 소프트맥스 함수에 대입하는 단계가 수행된다. 도 11은 본 개시에 따른 임베딩들을 예시적으로 나타낸 도면이다. 도 11을 참조하면, 임베딩은 토큰 임베딩, 포지션 임베딩, 그룹 임베딩, 및 질의 임베딩을 포함할 수 있다. 입 력이 예를 들어서, 입력이 ”주말에 집에서 쉬기보다 밖에서 활동하는 것을 좋아한다.”이라는 문장 형식일 수 있다. 이 경우, 토큰 임베딩은, CLS 토큰, 그룹 토큰, SEP 토큰, \"주말\"에 대한 토큰, \"##\"에 대한 토큰, \"집\" 에 대한 토큰, \"##에서\"에 대한 토큰, \"쉬\"에 대한 토큰, \"##기\"에 대한 토큰, \"##보\"에 대한 토큰, \"##다\"에 대한 토큰, \"밖에\"에 대한 토큰, \"##서\"에 대한 토큰, \"활동\"에 대한 토큰, \"##하\"에 대한 토큰, \"##는\"에 대한 토큰, \"것\"에 대한 토큰, \"##을\"에 대한 토큰, \"좋아한다\"에 대한 토큰, 및 SEP 토큰을 포함할 수 있다. 포지션 임베딩은 POS[0]부터 POS[19]까지의 복수의 POS를 포함할 수 있다. 그룹 임베딩에서 그룹은 각 집단에 대한 정보 값일 수 있다. 그룹 인베딩에서는 1부터 학습시키고자 하는 집단 의 수만큼 표현이될 수 있다. 그룹 임베딩의 경우, POS[0] 내지 POS[2] 각각에 대응되는 3개의 GRP[0]을 포함할 수 있다. POS[3] 내지 POS[19]에 대응되는 값은 0일 수 있다.질의 임베딩에서는 1부터 문항의 수만큼 표현이될 수 있다. 질의 임베딩은 POS[3] 내지 POS[19]에 대응되는 17 개의 QN[0]을 포함할 수 있다. POS[0] 내지 POS[2]에 대응되는 값은 0일 수 있다. 기존 임베딩에서 그룹 임베딩과 질의 임베딩 값을 더해줌으로써, 인공지능 모델이 두 정보에 대해 더 잘 학습할 수 있도록 설계될 수 있다. 도 12는 본 개시에 따른 그룹 질의 바이어스 행렬 및 스케일드 닷-프로덕트 어텐션을 설명하기 위한 도면이다. 도 12를 참조하면, 도 12의 토큰들(TQNG)은 도 8에 도시된 바와 동일할 수 있다. 도 12의 토큰들(TQNG)의 길이 (length)는 512일 수 있으나, 이에 한정되는 것은 아니다. 한편, 일부 실시예들에서, 토큰들(TQNG)의 길이를 매 칭하기 위해, 토큰들(TQNG)의 마지막 SEP 토큰의 후단에 패드 값(또는 패드, 패드 토큰)이 추가적으로 삽입될 수 있다. 그룹 질의 바이어스 행렬(β)은 토큰들(TQNG)에서 그룹과 그룹끼리의 유사성과 토큰들(TQNG)에서 그룹과 질의 간의 연관성을 나타내기 위한 행렬일 수 있다. 그룹 질의 바이어스 행렬(β)은 도 12의 토큰들(TQNG)끼리 일정 한 연산을 수행한 결과를 나타내는 행렬일 수 있다. 예를 들면, CLS 토큰, 그룹 토큰, 및 SEP 토큰 간의 유사성 이 매우 높으므로, 그룹 질의 바이어스 행렬(β)의 A1 부분(A1)에 해당되는 행렬은 1을 포함할 수 있다. 예를 들면, 질의에 해당되는 토큰들과 CLS 토큰, 그룹 토큰, 및 SEP 토큰 간의 연관성이 높으므로, 그룹 질의 바이어 스 행렬(β)의 A2 부분(A2)에 해당되는 행렬은 2를 포함할 수 있다. 질의에 해당되는 토큰들 간에 유사성이나 연관성이 매우 낮을 수 있으므로, 그룹 질의 바이어스 행렬(β)의 A3 부분(A3)에 해당되는 행렬은 0을 포함할 수 있다. 그룹 질의 바이어스 행렬(β)의 크기는 512X512일 수 있으나, 이에 한정되는 것은 아니다. 한편, 연산들(1210, 1220, 1230, 1240, 1250, 1260)은 BERT의 기본적인 어텐션 연산 구조를 나타낼 수 있 으며, 구체적으로, 전술한 연산들(1210, 1220, 1230, 1240, 1250, 1260)은 스케일드 닷-프로덕트 어텐션(예, 도 8에 도시된 복수의 스케일드 닷-프로덕트 어텐션들)에 의해 수행될 수 있다. matmul 연산에서, 쿼리 행렬(Q)과 키 행렬(K)이 서로 행렬 곱(matrix multiplication) 연산될 수 있다. 그리고, 행렬 곱된 결과 행렬(예, QK)의 전치(transpose) 행렬이 출력될 수 있다. scale 연산에서, 표준화하기 위한 차원 인자의 제곱근이 전치 행렬(예, QK의 전치 행렬)에 나눗셈 연산될 수 있다. 마지막 SEP 토큰에 패드 값(또는 패드, 패드 토큰)이 추가 삽입된 경우, 마스킹 연산에서, 추가된 패드 값을 제외함으로써 어텐션 마스킹하는 동작이 수행될 수 있다. 패드에 대해서는 어텐션 연산을 할 필요가 없기 때문에, 패드 값을 제외할 필요가 있다. 일부 실시예들에서, 마스킹 연산은 생략될 수 있다. 즉, 마스킹 연산은 선택적(optional)인 연산일 수 있다. 패드에 대해서 마스킹을 처리함으로써 어텐션 연산을 처리하 지 않게 하는 역할이 수행될 수 있으며, 이는 어텐션 연산의 효율성을 개선하기 위함이다. 어텐션이라는 것은 어느 부분에서 가장 유사도가 높은지에 대해 연산하는 것이므로, 패드는 어텐션 연산에 있어서 의미가 없는 부 분이기 때문이므로, 불필요한 부분을 마스킹 처리함으로써 어텐션 연산을 효율적으로 수행할 수 있다는 효과가 있다. 합산 연산에서, 전술한 QK의 전치 행렬과 차원 인자의 제곱근의 비율에 그룹 질의 바이어스 행렬(β)이 합산될 수 있다. softmax 연산에서, 합산된 결과가 소프트맥스 함수의 입력으로 반영되고, 소프트맥스 함수의 결과가 출력 될 수 있다. matmul 연산에서, 소프트맥스 함수의 결과와 벨류 행렬(V)이 서로 행렬 곱(matrix multiplication) 연산 될 수 있다. 이에 따라, 최종적으로 어텐션 행렬이 생성될 수 있다. 도 13은 본 개시에 따른 BERT계열 모델의 스코어를 획득하는 일 실시예와 분류기의 일 실시예를 나타낸 도면이 다. 도 8 및 도 13을 참조하면, 입력 임베딩은, 토큰 임베딩, 포지션 임베딩, 그룹 임베딩, 및 질의 임베딩을 포함 할 수 있다. BERT는 복수의 인코더 블록들을 포함할 수 있다. BERT에 포함된 각 인코더 블록은, 애드 및 놈(add & norm) 및 멀티-헤드 어텐션(multi-head Attention)을 포함할 수 있다. 멀티-헤드 어텐션은 도 8을 참조하여 전 술한 바와 같다. 멀티-헤드 어텐션에 포함된 스케일드 닷-프로덕트 어텐션들 각각은, 도 12를 참조하여 전술한연산들(1210, 1220, 1230, 1240, 1250, 1260)을 수행할 수 있다. 일부 실시예들에서, 각 스케일드 닷-프로덕트 어텐션을 통해 획득되는 스코어는 아래의 [수학식 1]을 이용하여 계산될 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, 는 스코어이고, 는 소프트맥스 함수이고, 는 쿼리 행렬이고, 는 키 행렬이고, 는 그 룹 질의 바이어스 행렬이고, 는 차원 인자이다. 한편, 어텐션 출력은 아래의 [수학식 2]를 이용하여 계산될 수 있다. [수학식 2]"}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, 은 어텐션 출력이고, 는 스코어이고, 는 벨류 행렬이다. 일부 실시예들에서, 상기 토큰 문장에 대해 추가된 패드 값을 제외함으로써, 입력되는 상기 인적성 검사용 문장 에 대한 어텐션 마스킹을 수행하는 단계가 수행될 수 있다. 다른 실시예들에서 어텐션 마스킹을 수행하는 단계 는 생략될 수도 있다. 분류기는 응답의 수만큼의 차원을 갖는 FC(Fully Connected) Layer를 포함할 수 있다. FC Layer는 Convolution/Pooling 프로세스의 결과를 취하여 이미지를 정의된 라벨로 분류하는데 사용될 수 있다. Convolution/Pooling의 출력은 각각 특정 입력 이미지내의 객체가 라벨에 속할 확률을 나타내는 값의 단일 벡터 로 평탄화될 수 있다. 즉, FC Layer는 한층의 모든 뉴런이 다음층의 모든 뉴런과 연결된 상태로, 2차원의 배열 형태 이미지를 1차원의 평탄화 작업을 통해 이미지를 분류하는데 사용되는 계층일 수 있다. 이때, FC Layer는 소프트맥스(Softmax) 함수로 분류될 수 있다. 분류기는 CLS 벡터에 대응되는 히득 벡터(예, CLS 히든 벡터 (HVTC))를 기초로 출력 벡터를 출력할 수 있다. 분류기는 어떤 질문이 들어왔을 때, 5가지 중에 하나의 답 을 출력할 수 있다. BERT의 히든 벡터의 차원(dimension)이 768이면, FC Layer의 입력 피쳐(input feature)의 개수는 768일 수 있다. 도 14는 본 개시에 따른 입력 임베딩을 생성하는 일 실시예를 설명하기 위한 도면이다. 도 14를 참조하면, 도 14에 도시된 알고리즘은 입력 임베딩을 생성하는 단계를 구현한 것으로, 입력 임베딩을 생성하는 단계는, 토크나이저를 이용하여 상기 인적성 검사용 문장을 서브워드 토큰들의 목록으로 토큰화하는 단계, 상기 토큰 문장의 시작 및 끝 각각에 CLS 토큰 및 SEP 토큰을 각각 추가하는 단계, 미리 훈련된 토큰 임 베딩 행렬을를 이용하여 토큰 임베딩을 획득하는 단계, 상기 인적성 검사용 문장의 위치에 기초하여 각 토큰에 대한 포지션 임베딩을 생성하는 단계, 그룹 대표 토큰이 속한 그룹을 가리키는 그룹 임베딩을 획득하는 단계, 질의 대표 토큰이 속한 질의를 가리키는 질의 임베딩을 획득하는 단계, 및 상기 입력 임베딩을 생성하기 위해, 상기 토큰 임베딩, 상기 포지션 임베딩, 상기 그룹 임베딩, 및 상기 질의 임베딩을 합하는 단계를 포함할 수 있 다. 도 14를 참조하여 구체적으로 예를 들면, 도 14의 알고리즘에서 요구되는 것은 토크나이저와 미리 훈련된 BERT 이고, 도 14의 알고리즘에서 입력은 심리 테스트 질문의 세트(a set of psychometric test questions)이며, 도 14의 알고리즘에서 출력은 입력 문장의 임력 임베딩 벡터(E)일 수 있다. 입력 문장은 전술한 인적성 검사용 문 장일 수 있다. 도 14의 알고리즘에서, 토크나이저를 이용하여 입력 문장을 서브워드 토큰들의 리스트로 토큰화하는 단계가 수 행된다. 그 다음, CLS 토큰을 토큰 문장의 시작에 추가하고 SEP 토큰을 토큰 문장의 끝에 추가하는 단계가 수행 된다. 그 다음, 미리 훈련된 토큰 임베딩 행렬을 가지고 토큰 식별자들(token ids) 및 토큰 임베딩을 획득하는 단계가 수행된다. 그 다음, 문장에서 문장의 위치를 기초로 문장에서 각 토큰에 대한 포지션 임베딩을 생성하는 단계가 수행된다. 그 다음, 그룹 대표 토큰이 속한 그룹을 가리키는 그룹 임베딩을 획득하는 단계가 수행된다. 그 다음, 질의 대표 토큰이 속한 질의를 가리키는 질의 임베딩을 획득하는 단계가 수행된다. 토큰 문장에 대한최종 입력 임베딩을 생산하기 위해, 토큰 임베딩, 포지션 임베딩, 그룹 임베딩, 및 질의 임베딩을 합 연산하는 단계가 수행된다. 도 15는 본 개시에 따른 어텐션 출력을 생성하는 일 실시예를 설명하기 위한 도면이다. 도 15를 참조하면, 히든 벡터를 생성하는 단계는 미리 학습된 가중치 행렬로 상기 입력 임베딩을 투영함으로써 (by projecting) 쿼리 행렬, 키 행렬 및 벨류 행렬을 획득하는 단계, 그룹끼리의 유사도 및 그룹과 질의 간의 연관성을 나타내는 매트릭스인 그룹 질의 바이어스 행렬, 표준화(normalize)하기 위한 차원 인자, 상기 쿼리 행 렬, 및 상기 키 행렬을 상기 BERT계열 모델의 소프트맥스 함수에 대입함으로써, 상기 BERT계열 모델의 스코어를 획득하는 단계, 상기 스코어와 상기 벨류 행렬을 곱함으로써(by multiplying), 어텐션 출력을 획득하는 단계, 및 상기 어텐션 출력을 기초로 CLS 히든 벡터를 포함하는 상기 히든 벡터를 생성하는 단계를 포함할 수 있다. 도 15에 도시된 알고리즘은 인코더 블록이 히든 벡터를 생성하는 단계의 일부를 구현한 것일 수 있다. 도 15의 알고리즘에서 요구되는 것은 미리 훈련된 BERT이고, 도 15의 알고리즘에서 입력은 입력 임베딩, 그룹 질의 바이 어스 행렬(β)이며, 도 15의 알고리즘에서 출력은 어텐션 출력일 수 있다. 도 15의 알고리즘에서 q는 어텐션 메 커니즘에 의해 생성된 현재 출력을 나타내는 쿼리 벡터일 수 있다. 도 15의 알고리즘에서 k는 어느 입력 위치가 현재 출력과 가장 관련되었는지 결정하는데 이용되는 키 벡터일 수 있다. 도 15의 알고리즘에서 u(또는 μ, υ 등)는 최종 출력을 얻기 위해 어텐션 스코어에 의해 가중되는 벨류 벡터일 수 있다. 도 15의 알고리즘에서, 입력 임베딩을 학습된 웨이트 행렬에 투영시킴으로써(by projecting) 쿼리 행렬, 키 행 렬, 및 벨류 행렬을 획득하는 단계가 수행된다. 쿼리 행렬은 쿼리 벡터에 대응되는 가중치 행렬로 입력 임베딩을 투영함으로써 생성될 수 있다. 키 행렬은 키 벡터에 대응되는 가중치 행렬로 입력 임베딩을 투영함으로써 생성될 수 있다. 벨류 행렬은 벨류 벡터에 대응되 는 가중치 행렬로 입력 임베딩을 투영함으로써 생성될 수 있다. 즉, 쿼리 행렬(Q), 키 행렬(K), 및 벨류 행렬 (V)은 아래의 [수학식 3]을 이용하여 계산될 수 있다. [수학식 3]"}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서, 은 쿼리 행렬(Q), 키 행렬(K), 및 벨류 행렬(V) 각각에 대응되는 가중치 행렬이고, 는 입력 임베딩이다. 도 15의 알고리즘에서, 전술한 [수학식 1]을 이용하여 BERT계열 모델의 스코어를 획득하는 단계가 수행된다. 이 때, [수학식 1]에서 이때, 는 스코어이고, 는 소프트맥스 함수이고, 는 쿼리 벡터에 대응되는 가중치 행렬로 입력 임베딩을 투영함으로써 생성되는 쿼리 행렬이고, 는 키 벡터에 대응되는 가중치 행렬로 입 력 임베딩을 투영함으로써 생성되는 키 행렬이고, 는 그룹 질의 바이어스 행렬이고, 는 차원 인자이다. 도 15의 알고리즘에서, 전술한 [수학식 2]를 이용하여 어텐션 출력을 획득하는 단계가 수행된다. 한편, 개시된 실시예들은 컴퓨터에 의해 실행 가능한 명령어를 저장하는 기록매체의 형태로 구현될 수 있다. 명 령어는 프로그램 코드의 형태로 저장될 수 있으며, 프로세서에 의해 실행되었을 때, 프로그램 모듈을 생성하여 개시된 실시예들의 동작을 수행할 수 있다. 기록매체는 컴퓨터로 읽을 수 있는 기록매체로 구현될 수 있다. 컴퓨터가 읽을 수 있는 기록매체로는 컴퓨터에 의하여 해독될 수 있는 명령어가 저장된 모든 종류의 기록 매체 를 포함한다. 예를 들어, ROM(Read Only Memory), RAM(Random Access Memory), 자기 테이프, 자기 디스크, 플 래쉬 메모리, 광 데이터 저장장치 등이 있을 수 있다."}
{"patent_id": "10-2023-0127097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "이상에서와 같이 첨부된 도면을 참조하여 개시된 실시예들을 설명하였다. 본 개시가 속하는 기술분야에서 통상 의 지식을 가진 자는 본 개시의 기술적 사상이나 필수적인 특징을 변경하지 않고도, 개시된 실시예들과 다른 형 태로 본 개시가 실시될 수 있음을 이해할 것이다. 개시된 실시예들은 예시적인 것이며, 한정적으로 해석되어서는 안 된다."}
{"patent_id": "10-2023-0127097", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시에 따른 인공지능 기반의 인적성 검사의 표집 시스템을 나타낸 도면이다. 도 2는 도 1의 서버의 구성을 도시한다. 도 3은 본 개시에 따른 인공지능 기반의 인적성 검사의 표집 방법을 나타낸 순서도이다. 도 4는 도 2의 프로세서에서 인공지능 기반의 응답 모델을 통해 학습되어 추천된 해당 질의 응답 정보를 출력하 는 과정을 일 예로 나타낸 도면이다. 도 5 및 도 6은 본 개시에 따른 인공지능 기반의 응답 모델의 학습 방법을 나타낸 도면들이다. 도 7은 본 개시에 따른 인적성 검사의 표집 과정 방법을 나타낸 순서도이다. 도 8은 본 개시에 따른 인적성 검사의 표집 과정 방법을 설명하기 위한 도면이다. 도 9는 본 개시에 따른 집단별 인적성 검사 결과를 학습하는 과정을 설명하기 위한 도면이다.도 10은 인적성 검사의 표집 과정 방법을 수행하기 위한 인공지능을 예시적으로 나타낸 도면들이다. 도 11은 본 개시에 따른 임베딩들을 예시적으로 나타낸 도면이다. 도 12는 본 개시에 따른 그룹 질의 바이어스 행렬 및 스케일드 닷-프로덕트 어텐션을 설명하기 위한 도면이다. 도 13은 본 개시에 따른 BERT계열 모델의 스코어를 획득하는 일 실시예와 분류기의 일 실시예를 나타낸 도면이 다. 도 14는 본 개시에 따른 입력 임베딩을 생성하는 일 실시예를 설명하기 위한 도면이다. 도 15는 본 개시에 따른 어텐션 출력을 생성하는 일 실시예를 설명하기 위한 도면이다."}
