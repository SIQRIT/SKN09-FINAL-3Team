{"patent_id": "10-2023-0041868", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0041201", "출원번호": "10-2023-0041868", "발명의 명칭": "인공지능 모델을 관리하기 위한 방법 및 장치", "출원인": "쿠팡 주식회사", "발명자": "콘다, 하르샤"}}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에서 인공지능 모델을 관리하는 방법에 있어서,클라이언트 장치로부터 리퀘스트를 수신하는 단계;상기 리퀘스트와 관련된 정보를 확인하는 단계;상기 정보에 기반하여, 상기 리퀘스트에 대응하는 적어도 하나의 인공지능 모델을 확인하는 단계;상기 정보에 기반하여 식별된 제어 정보를 상기 적어도 하나의 인공지능 모델에 전송하는 단계;상기 적어도 하나의 인공지능 모델로부터 상기 제어 정보에 따른 결과 정보를 수신하는 단계; 및상기 결과 정보를 상기 클라이언트 장치로 제공하는 단계를 포함하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 정보는 상기 리퀘스트에 대한 식별 정보, 상기 리퀘스트에 대응하는 상기 적어도 하나의 인공지능 모델에대한 정보, 상기 적어도 하나의 인공지능 모델의 입력 데이터에 대한 정보 및 상기 리퀘스트에 대응하는 실행타입에 대한 정보를 포함하는 것을 특징으로 하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 제어 정보를 상기 적어도 하나의 인공지능 모델에 전송하는 단계는,상기 적어도 하나의 인공지능 모델에 대한 입출력 관계에 대한 정보에 기반하여, 상기 적어도 하나의 인공지능모델의 실행 순서를 확인하는 단계; 및상기 실행 순서에 기반하여, 상기 제어 정보를 식별하는 단계를 포함하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,상기 적어도 하나의 인공지능 모델이 복수 개의 그룹으로 구분될 때, 상기 적어도 하나의 인공지능 모델의 실행순서는 상기 복수 개의 그룹 각각에 포함되는 인공지능 모델 간의 실행 순서를 포함하고, 및상기 복수 개의 그룹은 병렬적으로 실행되는 것을 특징으로 하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2 항에 있어서,상기 실행 타입에 대한 정보는 상기 결과 정보를 수신하기까지 예상 시간에 기반하여 결정되는 것을 특징으로하는 인공지능 모델을 관리하는 방법.공개특허 10-2024-0041201-2-청구항 6 제2 항에 있어서,상기 실행 타입이 제1 타입인 경우, 상기 결과 정보를 상기 클라이언트 장치로 제공하는 단계는,상기 결과 정보의 수신에 대응하여 상기 결과 정보를 대기 중인 상기 클라이언트 장치로 제공하는 단계를 포함하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제2 항에 있어서,상기 실행 타입이 제2 타입인 경우, 상기 결과 정보를 상기 클라이언트 장치로 제공하는 단계는,상기 결과 정보를 데이터 처리 플랫폼에 전송하는 단계를 포함하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서,상기 리퀘스트를 수신하는 단계는,상기 리퀘스트의 수신에 대응하여, 상기 식별 정보를 상기 클라이언트 장치로 전송하는 단계를 포함하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 데이터 처리 플랫폼에 저장된 상기 결과 정보는 상기 식별 정보에 기반하여 상기 클라이언트 장치에 의해식별되는 것을 특징으로 하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항에 있어서,상기 적어도 하나의 인공지능 모델을 확인하는 단계는,상기 적어도 하나의 인공지능 모델 각각을 제1 타입의 프로세서 유닛 기반 인공지능 모델 및 제2 타입의 프로세서 유닛 기반 인공지능 모델 중 하나로 확인하는 단계를 포함하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10 항에 있어서,상기 적어도 하나의 인공지능 모델 중 상기 제1 타입의 프로세서 유닛 기반 인공지능 모델을 포함하는 제1 그룹은 키워드 감지 모델을 포함하고, 및상기 적어도 하나의 인공지능 모델 중 상기 제2 타입의 프로세서 유닛 기반 인공지능 모델을 포함하는 제2 그룹은 광학 문자 인식 모델을 포함하는 것을 특징으로 하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "공개특허 10-2024-0041201-3-제10 항에 있어서,상기 적어도 하나의 인공지능 모델을 확인하는 단계는,상기 적어도 하나의 인공지능 모델 중 상기 제1 타입의 프로세서 유닛 기반 인공지능 모델을 포함하는 제1 그룹및 상기 적어도 하나의 인공지능 모델 중 상기 제2 타입의 프로세서 유닛 기반 인공지능 모델을 포함하는 제2그룹을 각각 제1 타입의 프로세서 유닛 서버 및 제2 타입의 프로세서 유닛 서버에 할당하는 단계를 포함하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12 항에 이어서,상기 제2 타입의 프로세서 유닛 서버는 복수 개의 인스턴스를 포함하고, 및상기 제2 그룹에 포함되는 인공지능 모델 각각에 대응되는 인스턴스는 상기 제2 그룹에 포함되는 인공지능 모델의 메모리 사용에 대한 히스토리 데이터에 기반하여 결정되는 것을 특징으로 하는 인공지능 모델을 관리하는 방법."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "인공지능 모델을 관리하기 위한 전자 장치에 있어서,트랜시버;하나 이상의 명령어를 저장하는 스토리지; 및클라이언트 장치로부터 리퀘스트를 수신하고,리퀘스트와 관련된 정보를 확인하고,상기 정보에 기반하여, 상기 리퀘스트에 대응하는 적어도 하나의 인공지능 모델을 확인하고,상기 정보에 기반하여 식별된 제어 정보를 상기 적어도 하나의 인공지능 모델에 전송하고,상기 적어도 하나의 인공지능 모델로부터 상기 제어 정보에 따른 결과 정보를 수신하고, 및상기 결과 정보를 상기 클라이언트 장치로 제공하는 프로세서를 포함하는 인공지능 모델을 관리하기 위한 전자장치."}
{"patent_id": "10-2023-0041868", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 비일시적 기록매체."}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치에서 인공지능 모델을 관리하는 방법이 개시된다. 구체적으로, 인공지능 모델을 관리하는 방법은 클라 이언트 장치로부터 리퀘스트를 수신하는 단계; 리퀘스트와 관련된 정보를 확인하는 단계; 정보에 기반하여, 리퀘 스트에 대응하는 적어도 하나의 인공지능 모델을 확인하는 단계; 정보에 기반하여 식별된 제어 정보를 적어도 하 나의 인공지능 모델에 전송하는 단계; 적어도 하나의 인공지능 모델로부터 제어 정보에 따른 결과 정보를 수신하 는 단계; 및 결과 정보를 클라이언트 장치로 제공하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 명세서의 실시 예는 인공지능 모델을 관리하기 위한 방법 및 장치에 관한 것이다. 본 명세서의 실시 예는 리 퀘스트와 관련된 정보에 기반하여 확인된 제어 정보를 인공지능 모델로 전송하고, 인공지능 모델로부터 수신한 결과 정보를 클라이언트 장치로 제공함으로써, 클라이언트 장치와 인공지능 사이의 상호 작용을 조절하는 인공 지능 모델을 관리하기 위한 방법 및 이를 위한 장치에 관한 것이다."}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 온라인 마케팅 등 인터넷 환경에서 서비스를 제공할 때, 비즈니스 의사 결정을 빠르게 결정하는 것이 요구 되고 있다. 과거에는 이러한 의사 결정을 수동으로 하였지만, 최근에는 머신러닝 모델을 이용함으로써 의사 결 정을 자동화하여 의사 결정을 하는 경향이 증가하고 있다. 다만, 서비스의 확장에 따른 높은 트래픽을 처리해야하는 바, 의사 결정에 소요되는 시간이 길어질 우려가 있었다. 따라서, 이와 같은 문제를 해결하기 위한 방법 및 장치가 요구된다."}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 문제점을 해결하기 위해 제안된 것으로, 인공지능 모델을 관리하기 위한 방법 및 장치를 제공 하는데 있다. 보다 구체적으로, 본 개시는 리퀘스트와 관련된 정보에 기반하여 확인된 제어 정보를 식별하고, 식별된 제어 정 보를 리퀘스트와 관련된 인공지능 모델로 전송하고, 인공지능 모델로부터 수신된 결과 정보를 클라이언트 장치 로 제공함으로써, 클라이언트 장치와 인공지능 모델 사이의 상호작용을 조절하기 위한 인공지능 모델을 관리하 는 방법 및 이를 위한 장치를 제공하는 것을 목적으로 한다. 본 실시 예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 이하의 실시 예들로부터 또 다른 기술적 과제들이 유추될 수 있다."}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 과제를 달성하기 위한 기술적 수단으로서, 본 개시의 제1측면에 따른 전자 장치에서 인공지능 모델을 관 리하는 방법은 클라이언트 장치로부터 리퀘스트를 수신하는 단계; 리퀘스트와 관련된 정보를 확인하는 단계; 정 보에 기반하여, 리퀘스트에 대응하는 적어도 하나의 인공지능 모델을 확인하는 단계; 정보에 기반하여 식별된 제어 정보를 적어도 하나의 인공지능 모델에 전송하는 단계; 적어도 하나의 인공지능 모델로부터 제어 정보에 따른 결과 정보를 수신하는 단계; 및 결과 정보를 클라이언트 장치로 제공하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 정보는 리퀘스트에 대한 식별 정보, 리퀘스트에 대응하는 적어도 하나의 인공지능 모델에 대한 정보, 적어도 하나의 인공지능 모델의 입력 데이터에 대한 정보 및 리퀘스트에 대응하는 실행 타입에 대한 정보를 포함하는 것을 특징으로 할 수 있다. 일 실시 예에 따르면, 제어 정보를 적어도 하나의 인공지능 모델에 전송하는 단계는 적어도 하나의 인공지능 모 델에 대한 입출력 관계에 대한 정보에 기반하여, 적어도 하나의 인공지능 모델의 실행 순서를 확인하는 단계; 및 실행 순서에 기반하여, 제어 정보를 식별하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 적어도 하나의 인공지능 모델이 복수 개의 그룹으로 구분될 때, 적어도 하나의 인공지능 모델의 실행 순서는 복수 개의 그룹 각각에 포함되는 인공지능 모델 간의 실행 순서를 포함할 수 있고, 복수 개 의 그룹은 병렬적으로 실행되는 것을 특징으로 할 수 있다. 일 실시 예에 따르면, 실행 타입에 대한 정보는 결과 정보를 수신하기까지 예상 시간에 기반하여 결정되는 것을 특징으로 할 수 있다. 일 실시 예에 따르면, 실행 타입이 제1 타입인 경우, 결과 정보를 클라이언트 장치로 제공하는 단계는 결과 정 보의 수신에 대응하여 결과 정보를 대기 중인 클라이언트 장치로 제공하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 실행 타입이 제2 타입인 경우, 결과 정보를 클라이언트 장치로 제공하는 단계는 결과 정 보를 데이터 처리 플랫폼에 전송하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 리퀘스트를 수신하는 단계는 리퀘스트의 수신에 대응하여, 식별 정보를 클라이언트 장치 로 전송하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 데이터 처리 플랫폼에 저장된 결과 정보는 식별 정보에 기반하여 클라이언트 장치에 의해 식별되는 것을 특징으로 할 수 있다. 일 실시 예에 따르면, 적어도 하나의 인공지능 모델을 확인하는 단계는 적어도 하나의 인공지능 모델 각각을 제 1 타입의 프로세서 유닛 기반 인공지능 모델 및 제2 타입의 프로세서 유닛 기반 인공지능 모델 중 하나로 확인 하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 적어도 하나의 인공지능 모델 중 제1 타입의 프로세서 유닛 기반 인공지능 모델을 포함하 는 제1 그룹은 키워드 감지 모델을 포함하고, 및 적어도 하나의 인공지능 모델 중 제2 타입의 프로세서 유닛 기반 인공지능 모델을 포함하는 제2 그룹은 광학 문자 인식 모델을 포함하는 것을 특징으로 할 수 있다. 일 실시 예에 따르면, 적어도 하나의 인공지능 모델을 확인하는 단계는 적어도 하나의 인공지능 모델 중 제1 타 입의 프로세서 유닛 기반 인공지능 모델을 포함하는 제1 그룹 및 적어도 하나의 인공지능 모델 중 제2 타입의 프로세서 유닛 기반 인공지능 모델을 포함하는 제2 그룹을 각각 제1 타입의 프로세서 유닛 서버 및 제2 타입의 프로세서 유닛 서버에 할당하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 제2 타입의 프로세서 유닛 서버는 복수 개의 인스턴스를 포함하고, 및 제2 그룹에 포함되 는 인공지능 모델 각각에 대응되는 인스턴스는 제2 그룹에 포함되는 인공지능 모델의 메모리 사용에 대한 히스 토리 데이터에 기반하여 결정되는 것을 특징으로 할 수 있다. 본 개시의 제2측면에 따른 인공지능 모델을 관리하기 위한 전자 장치는 트랜시버; 하나 이상의 명령어를 저장하 는 스토리지; 및 클라이언트 장치로부터 리퀘스트를 수신하고, 리퀘스트와 관련된 정보를 확인하고, 정보에 기 반하여, 리퀘스트에 대응하는 적어도 하나의 인공지능 모델을 확인하고, 정보에 기반하여 식별된 제어 정보를 적어도 하나의 인공지능 모델에 전송하고, 적어도 하나의 인공지능 모델로부터 제어 정보에 따른 결과 정보를 수신하고, 및 결과 정보를 클라이언트 장치로 제공하는 프로세서를 포함할 수 있다. 본 개시의 제3측면에 따른 기록매체는 인공지능 모델을 관리하는 방법을 컴퓨터에서 실행시키기 위한 프로그램 을 기록한 컴퓨터로 읽을 수 있는 비일시적 기록매체일 수 있다."}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 명세서의 실시 예에 따르면, 전자 장치는 클라이언트 장치로부터 리퀘스트를 수신함에 따라, 리퀘스트에 대 응하는 적어도 하나의 인공지능 모델 및 적어도 하나의 인공지능 모델을 효율적으로 실행시키기 위한 제어 정보 를 확인할 수 있다. 또한, 전자 장치는 제어 정보에 따라 실행된 인공지능 모델로부터 결과 정보를 수신할 수 있고, 수신된 결과 정보를 클라이언트 장치로 제공할 수 있다. 클라이언트 장치와 인공지능 모델 간의 상호 작 용을 조절하는 기능이 전자 장치로 일원화 됨에 따라 레이턴시가 낮아지고, 머신러닝 모델을 효율적으로 관리할 수 있다."}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "발명의 효과는 이상에서 언급한 효과로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 청구범위의 기재로부 터 당해 기술 분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "실시 예들에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들 을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의 미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 “포함”한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 “...부”, “...모듈” 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 명세서 전체에서 기재된 “a, b, 및 c 중 적어도 하나”의 표현은, ‘a 단독’, ‘b 단독’, ‘c 단독’, ‘a 및 b’, ‘a 및 c’, ‘b 및 c’, 또는 ‘a,b,c 모두’를 포괄할 수 있다. 이하에서 언급되는 \"단말\"은 네트워크를 통해 서버나 타 단말에 접속할 수 있는 컴퓨터나 휴대용 단말로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱(desktop), 랩톱 (laptop) 등을 포함하고, 휴대용 단말은 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, IMT(International Mobile Telecommunication), CDMA(Code Division Multiple Access), W-CDMA(W-Code Division Multiple Access), LTE(Long Term Evolution) 등의 통신 기반 단말, 스마트폰, 태블릿 PC 등과 같은 모든 종류의 핸드헬드(Handheld) 기반의 무선 통신 장치를 포함할 수 있다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시 예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식 을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 이하, 본 발명의 실시 예를 첨부된 도면을 참조하여 상세하게 설명한다. 실시 예를 설명함에 있어서 본 발명이 속하는 기술 분야에 익히 알려져 있고 본 발명과 직접적으로 관련이 없는 기술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 발명의 요지를 흐리지 않고 더 욱 명확히 전달하기 위함이다. 마찬가지 이유로 첨부 도면에 있어서 일부 구성요소는 과장되거나 생략되거나 개략적으로 도시되었다. 또한, 각 구성요소의 크기는 실제 크기를 전적으로 반영하는 것이 아니다. 각 도면에서 동일한 또는 대응하는 구성요소에 는 동일한 참조 번호를 부여하였다. 본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시 예들에 한정되는 것이 아니라 서 로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시 예들은 본 발명의 개시가 완전하도록 하고, 본 발명이"}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "속하는 기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 이 때, 처리 흐름도 도면들의 각 블록과 흐름도 도면들의 조합들은 컴퓨터 프로그램 인스트럭션들에 의해 수행 될 수 있음을 이해할 수 있을 것이다. 이들 컴퓨터 프로그램 인스트럭션들은 범용 컴퓨터, 특수용 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서에 탑재될 수 있으므로, 컴퓨터 또는 기타 프로그램 가 능한 데이터 프로세싱 장비의 프로세서를 통해 수행되는 그 인스트럭션들이 흐름도 블록(들)에서 설명된 기능들 을 수행하는 수단을 생성하게 된다. 이들 컴퓨터 프로그램 인스트럭션들은 특정 방식으로 기능을 구현하기 위해 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 지향할 수 있는 컴퓨터 이용 가능 또는 컴퓨터 판독 가능 메모리에 저장되는 것도 가능하므로, 그 컴퓨터 이용가능 또는 컴퓨터 판독 가능 메모리에 저장된 인스트 럭션들은 흐름도 블록(들)에서 설명된 기능을 수행하는 인스트럭션 수단을 내포하는 제조 품목을 생산하는 것도 가능하다. 컴퓨터 프로그램 인스트럭션들은 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에 탑재 되는 것도 가능하므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에서 일련의 동작 단계들이 수행되어 컴퓨터로 실행되는 프로세스를 생성해서 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 수행하는 인스트럭션들은 흐름도 블록(들)에서 설명된 기능들을 실행하기 위한 단계들을 제공하는 것도 가능하 다. 또한, 각 블록은 특정된 논리적 기능(들)을 실행하기 위한 하나 이상의 실행 가능한 인스트럭션들을 포함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있다. 또, 몇 가지 대체 실행 예들에서는 블록들에서 언급된 기능들이 순서를 벗어나서 발생하는 것도 가능함을 주목해야 한다. 예컨대, 잇달아 도시되어 있는 두 개의 블록들 은 사실 실질적으로 동시에 수행되는 것도 가능하고 또는 그 블록들이 때때로 해당하는 기능에 따라 역순으로 수행되는 것도 가능하다. 도 1은 다양한 실시 예에 따른 전자 장치가 인공지능 모델을 관리하는 방법이 구현될 수 있는 시스템을 설명하 기 위한 도면이다. 도 1을 참조하면, 다양한 실시 예에 따른 시스템은 다양한 종류의 장치들에 의해 구현될 수 있다. 예를 들 어, 시스템은 전자 장치, 클라이언트 장치, 서버 및 데이터 처리 플랫폼을 포함할 수 있다. 도1에 도시된 시스템은 본 실시 예와 관련된 구성요소들만 도시되어 있다. 따라서, 도 1에 도시된 구"}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "성요소들 외에 다른 범용적인 구성요소들이 더 포함될 수 있음을 본 실시 예와 관련된 기술분야에서 통상의 지 식을 가진 자라면 이해할 수 있다 전자 장치, 클라이언트 장치, 서버 및 데이터 처리 플랫폼 각각은 트랜시버, 스토리지 및 프로세서를 포함할 수 있다. 또한, 전자 장치, 클라이언트 장치, 서버 및 데이터 처리 플랫폼 각각은 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어나 소프트웨어, 또는, 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 한편 실시 예 전반에서 전자 장치, 클라이언트 장치 , 서버 및 데이터 처리 플랫폼 각각은 분리된 장치 또는 서버로 언급되나 이는 논리적으로 나누 어진 구조일 수 있으며, 이들 중 적어도 일부가 하나의 장치 또는 서버에서 분리된 기능에 의해 구현될 수 있다. 일 실시 예에 따르면, 전자 장치, 클라이언트 장치, 서버 및 데이터 처리 플랫폼은 네트워 크 서버로 구현되는 다수의 컴퓨터 시스템 또는 컴퓨터 소프트웨어를 포함할 수 있다. 예를 들면 전자 장치 , 클라이언트 장치, 서버 및 데이터 처리 플랫폼 중 적어도 일부는 인트라넷 또는 인터넷 과 같은 컴퓨터 네트워크를 통해 다른 네트워크 서버와 통신할 수 있는 하위 장치와 연결되어 작업 수행 요청을 접수하고, 그에 대한 작업을 수행하여 수행 결과를 제공하는 컴퓨터 시스템 및 컴퓨터 소프트웨어를 지칭할 수 있다. 이외에도, 전자 장치, 클라이언트 장치, 서버 및 데이터 처리 플랫폼 중 적어도 일 부는 네트워크 서버 상에서 동작할 수 있는 일련의 응용 프로그램과, 내부 혹은 연결된 다른 노드에 구축되어 있는 각종 데이터베이스를 포함하는 광의의 개념으로 이해될 수 있다. 예컨대, 전자 장치, 클라이언트 장 치, 서버 및 데이터 처리 플랫폼 중 적어도 일부는 도스(DOS), 윈도우(Windows), 리눅스 (Linux), 유닉스(UNIX), 또는 맥OS(MacOS) 등의 운영 체제에 따라 다양하게 제공되는 네트워크 서버 프로그램을 이용하여 구현될 수 있다. 전자 장치, 클라이언트 장치, 서버 및 데이터 처리 플랫폼은 네트워크(미도시)를 통해서 서로 통신할 수 있다. 네트워크는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN), 부가가치 통신망(Value Added Network; VAN), 이동 통신망(mobile radio communication network), 위성 통신망 및 이들의 상호 조합을 포함하며, 도 1에 도시된 각 네트워크 구성 주체가 서로 원활하게 통신을 할 수 있도록 하는 포괄적인 의미의 데이터 통신망이며, 유선 인터넷, 무선 인터넷 및 모바일 무선 통신망을 포함할 수 있다. 무선 통신은 예를 들어, 무선 랜(Wi-Fi), 블루투스, 블루투스 저 에너지(Bluetooth low energy), 지 그비, WFD(Wi-Fi Direct), UWB(ultra wideband), 적외선 통신(IrDA, infrared Data Association), NFC(Near Field Communication) 등이 있을 수 있으나, 이에 한정되는 것은 아니다. 일 실시 예에 따르면, 전자 장치는 클라이언트 장치로부터 리퀘스트를 수신하고, 리퀘스트와 관련된 정보를 확인하고, 정보에 기반하여 리퀘스트에 대응하는 적어도 하나의 인공지능 모델을 확인하고, 정보에 기반 하여 식별된 제어 정보를 적어도 하나의 인공지능 모델에 전송하고, 적어도 하나의 인공지능 모델로부터 제어 정보에 따른 결과 정보를 수신하고, 및 결과 정보를 클라이언트 장치에게 제공할 수 있다. 여기서, 전자 장치가 제어 정보를 적어도 하나의 인공지능 모델로 전송하는 동작은 적어도 하나의 인공지능 모델을 실행 하는 서버로 제어 정보를 전송하는 동작을 포함할 수 있다. 여기서, 클라이언트 장치는 네트워크를 통하여 전자 장치 등 다른 컴퓨터 시스템 상의 서비스에 접속할 수 있는 프로그램일 수 있고, 서버는 GPU 서버 또는 CPU 서버와 같은 임의의 프로세서 유닛 서버일 수 있으나, 클라이언트 장치 및 서버는 이에 한정되는 것은 아니다. 또한, 클라이언트 장치는 전자 장치와 연결된 채 대기 모드로 동작하거나 전자 장치와 비 연결 (disconnect)된 채 다른 테스크를 수행하고 있을 수 있다. 보다 상세하게는, 시간이 많이 소요되는 인공지능 모 델의 실행이 필요하거나 서버에 사용 가능한 메모리 사용량이 적으면, 클라이언트 장치는 비동기 모드로 동작하여 리퀘스트를 전송한 이후에 전자 장치와의 연결이 해제하는 것이 더 적절할 수 있다. 전자 장치와 클라이언트 장치 사이의 연결이 해제되면, 전자 장치는 서버로부터 수신한 결 과 정보를 데이터 처리 플랫폼에 퍼블리시할 수 있고, 클라이언트 장치는 리퀘스트에 대응하는 리퀘 스트 아이디에 기반하여 데이터 처리 플랫폼에 저장된 데이터 중 결과 정보를 식별하고, 식별된 결과 정보 를 구독(subscribe) 또는 컨숨(consume)할 수 있다. 이에 따라, 클라이언트 장치는 전자 장치로부터 간접적으로 결과 정보를 제공받을 수 있다. 여기서, 결과 정보는 적어도 하나의 인공지능 모델의 출력 데이터를 포함할 수 있고, 적어도 하나의 인공지능 모델의 출력 데이터는 데이터 처리 플랫폼에 분리되어 저장될 수 있다. 또한, 클라이언트 장치는 리퀘스트 아이디를 이용하여 데이터 처리 플랫폼에 저장된 적어도 하 나의 인공지능 모델의 출력 데이터를 포함하는 결과 정보를 식별할 수 있다. 여기서, 데이터 처리 플랫폼 은 서버, 전자 장치 등의 데이터 교환을 위한 데이터 처리 플랫폼일 수 있다. 예를 들어, 데이터 처리 플랫폼 은 오픈 소스 메시지 브로커 프로젝트인 아파치 카프카에 관한 플랫폼일 수 있으나, 이에 한정되는 것은 아니다. 일 실시 예에 따르면, 전자 장치는 클라이언트 장치, 서버 및 데이터 처리 플랫폼 간의 상 호 작용을 조절하는 오케스트레이터로 동작할 수 있고, 전자 장치는 논 블로킹 아이오(Non-blocking I/O) 를 포함할 수 있다. 이와 같은 시스템에 따라, 전자 장치가 클라이언트 장치로부터 리퀘스트를 수신하고, 결과 정보를 제공하기까지 클라이언트 장치의 대기 시간이 크게 줄어들 수 있고, 논 블로킹 아 이오를 도입함에 따라 입출력의 높은 비용을 낮출 수 있다. 또한, 논 블로킹 아이오를 이용함으로써, 출력 데이 터를 수신하기 전에 다른 인공지능 모델을 실행시키는 등의 다른 액션을 취할 수 있다. 즉, 전자 장치는 많은 트래픽의 리퀘스트를 보다 효율적으로 처리할 수 있다. 또한, 클라이언트 장치가 전송하는 리퀘스트가 데이터 처리 플랫폼을 통한 별도의 처리없이 전자 장 치를 통해 머신러닝 모델에 제공될 수 있다. 또한, 클라이언트 장치는 수신된 리퀘스트 아이디에 기 반하여 결과 정보를 용이하게 확인할 수 있는 바, 인공지능 모델의 출력 데이터를 취합하는 체계가 전자 장치 로 일원화되어 효율적일 수 있다. 하기에서 전자 장치가 인공지능 모델을 관리하는 구체적인 실시 예 는 자세히 살펴보기로 한다. 도 2는 전자 장치가 인공지능 모델을 관리하는 방법을 나타낸 흐름도이다."}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 4, "content": "도 2를 참조하면, 전자 장치가 인공지능 모델을 관리하는 각 동작은 본 발명이 속하는 기술분야에서 통상의 지 식을 가진 자에게 명확하게 이해되는 범위 내에서 일부 동작이 변경, 치환되거나 동작 간의 일부 순서가 변경될 수 있음은 자명하게 이해될 수 있다. 단계 S210에서, 전자 장치는 클라이언트 장치로부터 리퀘스트를 수신할 수 있다. 단계 S220에서, 전자 장치는 리퀘스트와 관련된 정보를 확인할 수 있다. 일 실시 예에 따르면, 전자 장치는 클라이언트 장치로부터 리퀘스트를 수신하고, 리퀘스트와 관련된 정보를 확인할 수 있다. 비지니스 의사 결정 등에 머신러닝을 이용할 때, 복수 개의 피처에 기반하여 의사 결정 을 내릴 수 있다. 이때, 클라이언트 장치는 비즈니스 의사 결정에 이용되는 복수 개의 피처 중 하나의 피 처를 결정할 때 적어도 하나의 인공지능 모델을 이용할 수 있다. 즉, 클라이언트 장치로부터 수신되는 리 퀘스트는 피처를 결정하는 것과 관련하여 인공지능 모델을 구동하기 위한 리퀘스트일 수 있다. 예를 들어, 인터 넷 환경에 아이템을 제공하는 퀄리티를 높이기 위한 의사 결정에는 부적절한 키워드 포함 여부에 대한 제1 피처, 이미지 검증에 대한 제2 피처 등 다양한 피처에 기반하여 종합적인 의사 결정이 요구될 수 있다. 이때, 부적절한 키워드 포함 여부에 대한 제1 피처를 확인하기 위해, 클라이언트 장치는 제1 피처에 대응하는 리 퀘스트를 전자 장치에 전송할 수 있다. 일 실시 예에 따르면, 리퀘스트는 리퀘스트에 대한 식별 정보, 리퀘스트에 대응하는 적어도 하나의 인공지능 모 델에 대한 정보, 인공지능 모델의 입력 데이터에 대한 정보 및 리퀘스트에 대응하는 실행 타입에 대한 정보를 포함할 수 있다. 리퀘스트에 대한 식별 정보는 리퀘스트를 다른 리퀘스트와 구별하기 위한 정보로, 클라이언트 장치에 의해 설정될 수 있다. 적어도 하나의 인공지능 모델은 서버를 통해 실행 가능한 복수 개의 인공지 능 모델 중 제1 피처를 결정하는데 필요한 인공지능 모델일 수 있다. 제1 피처를 결정하는데 필요한 인공지능 모델인지 여부는 클라이언트 장치에 의해 결정될 수 있다. 리퀘스트의 실행 타입은 제1 타입인 동기(Synchronous) 및 제2 타입인 비동기(Asynchronous) 중 하나로 결정될 수 있다. 전자 장치가 리퀘스트를 수신한 후에, 클라이언트 장치와 전자 장치의 연결 유무는 리퀘스트의 실행 타입에 따라 달라질 수 있다. 여기서, 실행 타입은 리퀘스트에 대응하는 결과 정보를 수신하기까 지 예상 시간에 기반하여 결정될 수 있다. 보다 구체적으로, 전자 장치는 리퀘스트에 대응하는 적어도 하나의 인공지능 모델을 확인하면, 적어도 하 나의 인공지능 모델의 추론과 관련된 히스토리 데이터에 기반하여, 적어도 하나의 인공지능 모델의 출력 데이터 를 모두 취합하기까지의 예상 시간을 확인할 수 있다. 이에 더해, 전자 장치는 현재 서버에 사용 가 능한 메모리 양에 더 기반하여, 예상 시간을 더 정확하게 확인할 수 있다. 예상 시간을 기반으로, 전자 장치 는 해당 리퀘스트의 실행 타입이 적절하게 설정되어 있는지 여부를 결정할 수 있다. 이와 관련된 일 실시 예는 도 6에서 자세히 살펴보기로 한다. 인공지능 모델의 입력 데이터는 기 학습된 적어도 하나의 인공지능 모델의 입력으로 이용되는 데이터일 수 있다. 보다 상세하게는, 입력 데이터는 복수 개의 이미지를 포함하는 리스트 타입의 데이터일 수 있다. 또한, 적어도 하나의 인공지능 모델은 하나의 입력 데이터에 기반할 수 있으나, 이에 한정되는 것은 아니다. 예를 들 어, 리퀘스트와 관련된 정보는 적어도 하나의 인공지능 모델 각각에 대응하는 입력 데이터에 대한 정보를 포함 할 수 있다. 단계 S230에서, 전자 장치는 정보에 기반하여, 리퀘스트에 대응하는 적어도 하나의 인공지능 모델을 확인 할 수 있다. 일 실시 예에 따르면, 전자 장치는 적어도 하나의 인공지능 모델의 실행 순서를 확인할 수 있다. 전자 장 치는 복수 개의 인공지능 모델의 실행 순서에 대한 그래프에 기반하여, 적어도 하나의 인공지능 모델의 실 행 순서를 확인할 수 있다. 보다 상세하게는, 전자 장치는 적어도 하나의 인공지능 모델에 대한 입출력 관 계에 대한 정보에 기반하여, 복수 개의 인공지능 모델의 실행 순서에 대한 그래프를 확인할 수 있다. 여기서, 그래프는 인공지능 모델의 실행 순서를 포함하는 일종의 트리 다이어그램 형태의 정보일 수 있다. 예를 들어, 제1 인공지능 모델은 데이터 A, 데이터 B를 각각 입력 데이터 및 출력 데이터로 하는 인공지능 모 델이고, 제2 인공지능 모델은 데이터 B 및 데이터 C를 각각 입력 데이터 및 출력 데이터로 할 수 있다. 이때, 전자 장치는 제1 인공지능 모델 및 제2 인공지능 모델의 입출력 관계에 대한 정보에 기반하여, 제1 인공지 능 모델을 실행한 후에 제2 인공지능 모델이 실행되도록 인공지능 모델의 실행 순서에 대한 그래프를 확인할 수 있다. 또한, 적어도 하나의 인공지능 모델은 복수 개의 그룹 별로 병렬적으로 수행될 수 있다. 예를 들어, 적어도 하 나의 인공지능 모델이 인공지능 모델의 특성, 기능 등에 따라 복수 개의 그룹으로 구분될 때, 인공지능 모델의 실행 순서를 확인하는 것은 복수개의 그룹 각각에 포함되는 인공지능 모델 간의 실행 순서를 확인하는 것을 포 함할 수 있다. 보다 상세하게는, 적어도 하나의 인공지능 모델을 인공지능 모델의 특성, 기능 등에 따라 복수 개의 그룹으로 구분하는 것은 입출력 관계에 서로 의존성이 없는 그룹으로 분리하는 것일 수 있다. 예를 들어, NLP 관련 인공지능 모델과 컴퓨터 비전 관련 인공지능 모델이 서로 의존성이 없는 경우, NLP 관련 인공지능 모 델 및 컴퓨터 비전 관련 인공지능 모델은 서로 병렬적으로 실행될 수 있다. 일 실시 예에 따르면, 전자 장치는 적어도 하나의 인공지능 모델 각각을 제1 타입의 프로세서 유닛 기반 인공지능 모델 및 제2 타입의 프로세서 유닛 기반 인공지능 모델 중 하나로 결정할 수 있다. 예를 들어, 제1 타 입의 프로세서 유닛 및 제2 타입의 프로세서 유닛은 각각 CPU(Central Processing Unit) 및 GPU(Graphics Processing Unit)에 대응할 수 있다. 본원에서, A 프로세서 유닛 기반 인공지능 모델은 A라는 프로세서 유닛 서버에서 더 효율적으로 동작하는 인공 지능 모델을 의미할 뿐, A라는 프로세서 유닛 서버에서만 동작하는 인공지능 모델을 의미하는 것은 아닐 수 있 다. 보다 상세하게는, 제1 타입의 프로세서 유닛 기반 인공지능 모델은 제1 타입의 프로세서 유닛 서버에서 더 효율적으로 동작하는 인공지능 모델을 의미할 뿐, 제2 타입의 프로세서 유닛에서 동작할 수 있다. 즉, 제1 타입 의 프로세서 유닛에서 더 효율적으로 동작하는 인공지능 모델이라도 제1 타입의 프로세서 유닛 서버에서 처리 중인 테스크가 많은 경우, 제2 타입의 프로세서 유닛 서버를 통해 구동되는 인공지능 모델로 동작할 수 있다. 마찬가지로, 제2 타입의 프로세서 유닛 기반 인공지능 모델은 제2 타입의 프로세서 유닛 서버에서 더 효율적으 로 동작하는 인공지능 모델을 의미할 뿐, 제1 타입의 프로세서 유닛 서버에서 동작할 수 있다. 인공지능 모델마다 GPU 서버 및 CPU 서버 중 어느 서버에서 인공지능 모델의 퍼포먼스가 우수한지 여부는 상이 할 수 있다. 예를 들어, 인공지능 모델 중 광학 문자 인식(Optical character recognition; OCR) 모델 및 로 고 감지 모델과 같은 컴퓨터 비전 관련 인공지능 모델은 GPU 서버를 기반으로 구동될 때 더 효율적인 모델일 수있다. 반대로, 인공지능 모델 중 자연어 처리 관련 인공지능 모델의 예인 키워드 감지 모델 및 나이 감지 모델 등은 CPU 서버를 기반으로 구동될 때 더 효율적인 모델일 수 있다. 여기서, 효율적인 모델의 기준은 인공지능 모델의 레이턴시, 비용 및 예측의 정확도 등을 포함할 수 있다. 전자 장치는 서버에서 실행 중이거나 실행 예정인 테스크의 종류, 테스크의 상태 정보와 적어도 하나 의 인공지능 모델의 타입 및 적어도 하나의 인공지능 모델의 실행 순서 등을 종합적으로 고려하여, 적어도 하나 의 인공지능 모델에 할당되는 하드웨어 자원을 유동적으로 변경할 수 있다. 예를 들어, 제1 타입의 프로세서 유 닛에서 더 효율적으로 동작하는 인공지능 모델이라도 제1 타입의 프로세서 유닛 서버에서 처리 중인 테스크가 많은 경우, 제2 타입의 프로세서 유닛 서버를 통해 구동되는 인공지능 모델로 동작할 수 있다. 하드웨어 자원이 유동적으로 변경될 때, 적어도 하나의 인공지능에 대응하는 서버에 대한 정보는 제어 정보에 포함될 수 있다. 단계 S240에서, 전자 장치는 정보에 기반하여 식별된 제어 정보를 적어도 하나의 인공지능 모델에 전송할 수 있다. 각각의 인공지능 모델이 구동되는 서버 및 인공지능 모델의 실행 순서가 결정되면, 전자 장치는 정보에 기 반하여 식별된 제어 정보를 적어도 하나의 인공지능 모델에 전송할 수 있다. 여기서, 제어 정보를 적어도 하나 의 인공지능 모델에 전송하는 동작은 제어 정보를 각각의 서버에 전송하는 동작 및 각각의 서버에서 제어 정보 에 대응되는 입력 데이터에 따라 인공지능 모델을 실행시키는 동작을 포함할 수 있다. 제어 정보는 인공지능 모 델이 구동되는 서버 및 인공지능 모델의 실행 순서에 대한 정보를 포함하는 정보일 수 있다. 단계 S250에서, 전자 장치는 적어도 하나의 인공지능 모델로부터 제어 정보에 따른 결과 정보를 수신할 수 있다. 일 실시 예에 따르면, 전자 장치는 인공지능 모델의 실행이 완료됨에 따라, 인공지능 모델의 출력 데이터 를 수신할 수 있다. 이에 따라, 전자 장치는 적어도 하나의 인공지능 모델의 결과 데이터를 취합하여 결과 정보를 확인할 수 있다. 이때, 적어도 하나의 인공지능 모델의 결과 데이터는 상이한 시간에 출력될 수 있는 바, 전자 장치는 적어도 하나의 인공지능 모델의 결과 데이터 모두를 수신할 때까지 대기한 후, 적어도 하 나의 인공지능 모델의 출력 데이터를 취합할 수 있다. 단계 S260에서, 전자 장치는 결과 정보를 클라이언트 장치로 제공할 수 있다. 일 실시 예에 따르면, 전자 장치는 리퀘스트에 대응하는 실행 타입이 제1 타입 또는 제2 타입인지 여부에 기반하여, 상이한 방법에 따라 결과 정보를 클라이언트 장치에게 제공할 수 있다. 또한, 병렬적으로 구동 된 적어도 하나의 인공지능 모델로부터 전자 장치는 인공지능 모델의 구동에 따른 출력 데이터를 취합하여 결과 정보를 식별할 수 있다. 이에 따라, 전자 장치는 식별된 결과 정보를 클라이언트 장치에 리턴할 수 있다. 실행 타입이 제1 타입(Synchronous)인 경우에 대한 결과 정보를 클라이언트 장치에게 제공하는 구체적인 동작은 도 3에서 살펴보기로 하고, 실행 타입이 제2 타입(Asynchronous)인 경우에 대한 결과 정보를 클라이언트 장치에게 제공하는 구체적인 동작은 도 4 및 도 5에서 살펴보기로 한다. 도 3은 실행 타입이 제1 타입인 경우, 전자 장치가 인공지능 모델을 관리하는 일 실시 예를 설명하기 위한 도면이다. 도 2에서 살펴본 바와 같이, 클라이언트 장치는 비즈니스 의사 결정에 이용되는 복수 개의 피처 중 하나의 피처를 결정할 때 적어도 하나의 인공지능 모델을 이용할 수 있다. 예를 들어, SDP 페이지의 컨텐츠 생성과 관 련된 피처를 결정할 때, OCR 모델, 키워드 감지 모델, 브랜드 감지 모델 및 로고 감지 모델 등이 필요할 수 있 다. 도 3을 참조하면, 도 3은 OCR 모델 및 브랜드 감지 모델을 이용하는 일 실시 예를 설명하고 있다. 또한, 다 른 페이지의 컨텐츠의 생성과 관련된 피처를 결정할 때, OCR 모델, 키워드 감지 모델, 브랜드 감지 모델, 로고 감지 모델 이외에 다른 인공지능 모델이 이용될 수 있다. 일 실시 예에 따르면, 전자 장치는 클라이언트 장치로부터 리퀘스트를 수신하고, 리퀘스트와 관련된 정보를 확인할 수 있다. 도3을 참조하면, 수신된 리퀘스트는 다음과 같은 정보를 포함할 수 있다. { \"requestId\" : \"01a-c140-bf972\", \"execution\" : \"sync\", \"detections\" : [\"OCR\",\"brand_detection\"], \"data\" : [{\"imageurl\" : \"http://urllink1\"}] } 도 3을 참조하면, 1) 리퀘스트에 대한 식별 정보인 리퀘스트 아이디는 '01a-c140-bf972'일 수 있고, 2) 실 행 타입은 '제1 타입'인 sync일 수 있고, 3) 적어도 하나의 인공지능 모델에 대한 정보인 Detection 은 'OCR 모델' 및 '브랜드 감지 모델'일 수 잇고, 4) 적어도 하나의 인공지능 모델에 대한 입력 데이터는 'http://urllink1'일 수 있다. 여기서, 'http://urllink1'는 설명을 위한 dummy URL일 수 있다. 리퀘스트에 대응하는 피처를 결정하는데 2개의 인공지능 모델만을 이용하는 바, 전자 장치가 리퀘스트에 대응하는 결과 정보를 수신하기까지 상대적으로 짧은 시간이 소요될 수 있다. 이에, 도 3의 일 실시 예에 따른 실행 타입은 제1 타입인 동기(Synchronous)로 결정되는 것이 적절할 수 있다. 실행 타입이 제1 타입인 바, 클라이언트 장치는 리퀘스트를 전송한 후, 전자 장치와 연결된 채 대기 모드로 동작할 수 있다. 일 실시 예에 따르면, 전자 장치는 적어도 하나의 인공지능 모델의 실행 순서를 결정할 수 있다. 예를 들 어, OCR 모델로 입력 데이터인 'http://urllink1'로 입력됨에 따라, OCR 모델은 'http://urllink1'을 입력 데 이터로 이용하여 디바이스가 판독 가능한 텍스트 데이터를 출력 데이터로 출력할 수 있다. 또한, OCR 모델의 출 력 데이터인 텍스트 데이터는 브랜드 감지 모델의 입력 데이터로 이용될 수 있는 바, OCR 모델 및 브랜드 감지 모델은 의존성이 있는 인공지능 모델에 해당될 수 있다. 따라서, 전자 장치는 OCR 모델 및 브랜드 감지 모 델의 입출력 관계에 대한 정보에 기반하여, OCR 모델 후에 브랜드 감지 모델이 실행되도록 하는 실행 순서를 확 인할 수 있다. 또한, 전자 장치는 확인된 실행 순서, 실행의 대상이 되는 적어도 하나의 인공지능 모델 및 입력 데이터인 'http://urllink1'를 포함하는 제어 정보를 생성할 수 있다. 일 실시 예에 따르면, 전자 장치는 제어 정보를 적어도 하나의 인공지능 모델에 전송할 수 있다. 이에 따 라, 적어도 하나의 인공지능 모델은 제어 정보에 기반하여 실행될 수 있다. 보다 상세하게는, 1) OCR 모델은 입 력 데이터인 'http://urllink1'에 기반하여, 출력 데이터로 텍스트 데이터를 생성할 수 있다. 생성된 텍스 트 데이터는 전자 장치로 전송될 수 있다. 또한, 생성된 텍스트 데이터는 브랜드 감지 모델의 입력 데이터 로 이용될 수 있다. 2) 브랜드 감지 모델은 입력 데이터로 수신된 텍스트 데이터에 기반하여, 출력 데이터로 브 랜드 데이터를 생성할 수 있다. 또한 생성된 브랜드 데이터는 전자 장치로 전송될 수 있다. 일 실시 예에 따르면, 전자 장치는 출력 데이터인 텍스트 데이터 및 브랜드 데이터를 취합하고, 출력 데이 터를 모두 포함하는 결과 정보를 대기 모드로 동작 중인 클라이언트 장치에게 전송할 수 있다. 또한, 전자 장치는 출력 데이터를 취합한 후, 출력 데이터를 포함하는 결과 정보를 클라이언트 장치에게 전송하 는 것을 특징으로 할 수 있다. 실행 타입이 제1 타입인 경우, 전자 장치는 대기 모드로 동작하고 있는 클라이언트 장치에게 결과 정 보를 제공하는 바, 리퀘스트에 대한 식별 정보인 리퀘스트 아이디는 사용되지 않는 데이터일 수 있다. 따라서, 실행 타입이 제1 타입인 경우, 리퀘스트에 대한 식별 정보인 리퀘스트 아이디는 선택적으로(Optional) 제공되는 정보일 수 있다. 도 4는 실행 타입이 제2 타입인 경우, 전자 장치가 인공지능 모델을 관리하는 일 실시 예를 설명하기 위한 도면 이다. 일 실시 예에 따르면, 전자 장치는 클라이언트 장치로부터 리퀘스트를 수신하고, 리퀘스트와 관련된 정보를 확인할 수 있다. 도4를 참조하면, 수신된 리퀘스트는 다음과 같은 정보를 포함할 수 있다. { \"requestId\" : \"01a-c140-bf972\", \"execution\" : \"async\", \"requesttype\" : [\"Type A\"], \"data\" : [{\"imageurl\" : \"http://urllink1\"}] } 도 4를 참조하면, 1) 리퀘스트에 대한 식별 정보인 리퀘스트 아이디는 '01a-c140-bf972'일 수 있고, 2) 실 행 타입은 '제2 타입'인 async일 수 있고, 3) 적어도 하나의 인공지능 모델에 대한 정보인 리퀘스트 타입 은 A 타입일 수 있다. 리퀘스트 타입은 클라이언트 장치 및 전자 장치에 미리 설정되어 있는 타 입일 수 있고, 리퀘스트 타입은 리퀘스트 타입에 대응되는 적어도 하나의 인공지능 모델에 대한 정보를 포함할 수 있다. 또한, 리퀘스트 타입에 포함되는 적어도 하나의 인공지능 모델에 대한 정보는 클라이언트 장치에 의해 변경될 수 있다. 도 4를 참조하면, 일 실시 예에 따른 A 타입은 OCR 모델, 키워드 감지 모델 및 나이 감지 모델 을 적어도 하나의 인공지능 모델로 하는 타입일 수 있다. 또한, 도 4의 리퀘스트 타입 및 도 3의 Detection는 모두 적어도 하나의 인공지능 모델에 대한 필드인 바, 리퀘스트 타입 및 Detection은 선택적 으로 정보에 포함되는 것을 특징으로 할 수 있다. 4) 적어도 하나의 인공지능 모델에 대한 입력 데이터는 'http://urllink1'일 수 있다. 도 4의 일 실시 예에 따른 실행 타입은 제2 타입인 비동기(Asynchronous)일 수 있다. 따라서, 클라이언트 장치 는 전자 장치로부터 리퀘스트 아이디를 수신함에 따라, 클라이언트 장치와 전자 장치의 연 결은 해제될 수 있다. 따라서, 인공지능 모델의 추론에 시간이 소요될 때, 클라이언트 장치는 다른 테스크 를 수행할 수 있다. 일 실시 예에 따르면, 전자 장치는 적어도 하나의 인공지능 모델의 실행 순서를 결정할 수 있다. 도 3의 일 실시 예와 비슷하게, OCR 모델의 출력 데이터가 키워드 감지 모델의 입력 데이터로 이용되고, 키워드 감지 모델의 출력 데이터는 나이 감지 모델의 입력 데이터로 이용될 수 있다. 따라서, 전자 장치는 OCR 모델, 키워드 감지 모델 및 나이 감지 모델 순서로 실행되는 실행 순서를 확인할 수 있다. 이에 따라, 전자 장치(10 0)는 확인된 실행 순서와 실행의 대상이 되는 적어도 하나의 인공지능 모델 및 OCR 모델의 입력 데이터인 'http://urllink1'를 포함하는 제어 정보를 생성할 수 있다. 일 실시 예에 따르면, 전자 장치는 제어 정보를 적어도 하나의 인공지능 모델을 구동하기 위해 서버로 전 송할 수 있다. 이에 따라, 적어도 하나의 인공지능 모델은 제어 정보에 기반하여 실행될 수 있다. 보다 상세하 게는, 1) OCR 모델은 입력 데이터인 'http://urllink1'에 기반하여, 출력 데이터로 텍스트 데이터를 생성 할 수 있다. 생성된 텍스트 데이터는 전자 장치로 전송될 수 있다. 또한, 생성된 텍스트 데이터는 키워드 감지 모델의 입력 데이터로 이용될 수 있다. 2) 키워드 감지 모델은 수신된 텍스트 데이터를 입력 데이터로 이 용하여 출력 데이터인 키워드 데이터를 생성할 수 있다. 또한 생성된 키워드 데이터는 전자 장치로 전송될 수 있다. 3) 나이 감지 모델은 수신된 키워드 데이터를 입력 데이터로 이용하여 출력 데이터인 나이에 대한 데 이터를 생성할 수 있다. 또한, 생성된 나이에 대한 데이터는 전자 장치로 전송할 수 있다. 리퀘스트의 실행 타입이 제2 타입인 Async인 바, 전자 장치는 클라이언트 장치로 적어도 하나의 인공 지능 모델의 출력 데이터를 포함하는 결과 정보를 직접적으로 제공하지 못할 수 있다. 이때, 전자 장치는 결과 정보를 데이터 처리 플랫폼에 퍼블리시할 수 있다. 데이터 처리 플랫폼은 아파치 카프카에 한정 되는 것은 아니나, 아래에서는 아파치 카프카를 이용하여 클라이언트 장치로 결과 정보를 전송하는 구체적인 실 시 예를 설명한다. 일 실시 예에 따르면, 전자 장치는 결과 정보를 카프카 메시지로 설정하여 데이터 처리 플랫폼으로 전송할 수 있다. 데이터 처리 플랫폼이 아파치 카프카에 대한 플랫폼인 경우, 데이터 처리 플랫폼은 카프카 클러스터를 포함할 수 있다. 이때, 데이터 처리 플랫폼은 전자 장치로부터 수신된 결과 정보 를 카프카 클러스터의 카프카 토픽에 저장할 수 있다. 보다 상세하게는, 결과 정보는 리퀘스트 아이디를 카프카 토픽의 이름으로 하는 데이터 처리 플랫폼의 카프카 토픽에 저장되는 것을 특징으로 할 수 있다. 이에 따 라, 클라이언트 장치는 카프카 토픽의 이름을 이용함으로써 리퀘스트에 대응하는 카프카 토픽을 용이하게 식별 할 수 있는 바, 관련 없는 리퀘스트를 필터링할 수 있다. 또한, 클라이언트 장치 별로 대응되는 카프카 토 픽이 미리 지정되어 있을 수도 있으나, 이에 한정되는 것은 아니다. 일 실 시예에 따르면, 카프카 토픽은 복수 개의 파티션으로 구분될 수 있다. 따라서, 데이터 처리 플랫폼(13 0)은 리퀘스트 아이디를 카프카 토픽의 이름으로 하는 데이터 처리 플랫폼의 카프카 토픽의 서로 다른 파 티션에 적어도 하나의 인공지능 모델의 출력 데이터를 각각 저장할 수도 있으나, 이에 한정되는 것은 아니다. 클라이언트 장치는 데이터 처리 플랫폼에 저장된 결과 정보를 컨숨할 수 있다. 보다 상세하게는, 클 라이언트 장치는 리퀘스트 아이디를 이용하여, 복수 개의 토픽 중 리퀘스트에 대응하는 카프카 토픽을 식 별할 수 있다. 이에 따라, 클라이언트 장치는 리퀘스트 아이디에 대응하는 카프카 토픽에 저장된 결과 정 보를 수신할 수 있다. 관련하여, 클라이언트 장치는 주기적으로 리퀘스트 아이디에 대응하는 카프카 토픽의 이름을 가지는 카프카 토픽이 데이터 처리 플랫폼에 있는지 여부를 확인할 수 있다. 다만, 클라이언트 장치가 임의의 시간에 주기적으로 리퀘스트 아이디에 대응하는 카프카 토픽의 이름을 가 지는 카프카 토픽이 데이터 처리 플랫폼에 있는지 여부를 확인하는 동작은 비효율적일 수 있다. 이와 관련 하여, 일 실시 예에 따른 전자 장치는 리퀘스트를 수신한 시점에서의 적어도 하나의 인공지능 모델의 실행 상태에 대한 정보, 적어도 하나의 인공지능 모델의 실행에 대한 히스토리 데이터 및 서버의 사용 가능한 메모리 양에 대한 정보 중 적어도 하나에 기반하여, 데이터 처리 플랫폼에 결과 정보를 전송하기까지의 예상 시간 을 예측할 수 있다. 일 실시 예에 따라 전자 장치가 예상 시간을 예측하면, 리퀘스트를 수신함에 따라, 전자 장치는 리퀘 스트 아이디 및 예상 시간에 대한 정보를 클라이언트 장치로 전송할 수 있다. 즉, 클라이언트 장치는 예상 시간에 대한 정보를 기반으로, 리퀘스트 아이디에 대응하는 카프카 토픽의 이름을 가지는 카프카 토픽이 데이터 처리 플랫폼에 있는지 여부를 확인하고, 결과 정보를 수신할 수 있다. 따라서, 시스템의 전체 적인 비용이 크게 절약될 수 있다. 도 5는 실행 타입이 제2 타입인 경우, 전자 장치가 인공지능 모델을 관리하는 다른 일 실시 예를 설명하기 위한 도면이다. 일 실시 예에 따르면, 전자 장치는 클라이언트 장치로부터 리퀘스트를 수신하고, 리퀘스트와 관련된 정보를 확인할 수 있다. 도5를 참조하면, 수신된 리퀘스트는 다음과 같은 정보를 포함할 수 있다. { \"requestId\": \"01a-c140-bf972\", \"execution\": \"async\", \"data\": [{\"imageurl\" : \"http://urllink1\", detection: [\"OCR\", \"keyword_detection\", \"age_detection\"]}, {\"imageurl\" : \"http://urllink2\", detections: \"Certification\"}] } 도 5를 참조하면, 1) 리퀘스트에 대한 식별 정보인 리퀘스트 아이디는 '01a-c140-bf972'일 수 있고, 2) 실 행 타입은 '제2 타입'인 Async일 수 있다. 3) 또한, 데이터는 복수 개의 입력 데이터 및 복수 개의 입력 데이터 각각에 대응하는 인공지능 모델에 대한 정보를 포함할 수 있다. 보다 상세하게는, 도 5를 참조하면, 제1 입력 데이터인 http://urllink1와 대응되는 적어도 하나의 인공지능 모델은 OCR 모델, 키워드 감 지 모델 및 나이 감지 모델을 포함할 수 있다. 또한, 제2 입력 데이터인 http://urllink2와 대응되는 적어도 하 나의 인공지능 모델은 인증 관련 모델을 포함할 수 있다. 따라서, 리퀘스트에 대응하는 입력 데이터가 복수 개 일 때, 도 3 및 도 4와 같이 requesttype 및 detection 등이 따로 기재되지 않고, data 필드 안에 detection 필드 또는 requesttype 필드가 함께 기재될 수 있다. 또한, http://urllink1 및 http://urllink2는 설명을 위 한 dummy URL일 수 있다. 일 실시 예에 따르면, 전자 장치는 적어도 하나의 인공지능 모델의 실행 순서를 결정할 수 있다. 보다 상 세하게는, 리퀘스트에 대응하는 인공지능 모델이 복수 개의 그룹으로 구분될 때, 적어도 하나의 인공지능 모델 의 실행 순서는 복수 개의 그룹 각각에 포함되는 인공지능 모델 간의 실행 순서를 포함할 수 있고, 복수 개의 그룹은 병렬적으로 실행되는 것을 특징으로 할 수 있다. 도 5를 참조하면, OCR 모델, 키워드 감지 모델, 나이 감지 모델 및 인증 관련 모델은 NLP를 통한 텍스트 분석과 관련된 제1 그룹과 컴퓨터 비전을 통한 이미지 분석과 관련된 제2 그룹으로 나누어질 수 있다. 예를 들어, 제1 그룹은 OCR 모델, 키워드 감지 모델 및 나이 감지 모델을 포함할 수 있고, 제2 그룹은 인증 관련 모델을 포함할 수 있다. 인증 관련 모델은 OCR을 통한 컴퓨터로 판독 가능한 텍스트 데이터에 의존하지 않는 바, 인증 관련 모 델은 제1 그룹에 포함되는 인공지능 모델과 병렬적으로 실행 가능할 수 있다. 따라서, 전자 장치는 제1 그 룹에 포함되는 OCR 모델, 키워드 감지 모델 및 나이 감지 모델의 실행 순서와 함께 제1 그룹과 제2 그룹이 병렬 적으로 실행되도록 하는 실행 순서를 확인할 수 있다. 또한, 클라이언트 장치로부터 수신된 리퀘스트와 관련된 정보는 적어도 하나의 인공지능 모델의 실행 순서 및 병렬 실행 여부에 관한 정보를 포함할 수 있다. 따라 서, 클라이언트 장치가 적어도 하나의 인공지능 모델의 실행 순서를 지정하는 것도 가능할 수 있다. 또한, 전자 장치는 확인된 실행 순서를 포함하는 제어 정보를 인공지능 모델에 전송할 수 있다. 보다 상세 하게는, 전자 장치는 제어 정보를 서버에 전송할 수 있고, 서버는 1) 제1 입력 데이터인 \"http://urllink1\"를 OCR 모델의 입력 데이터로 제공하고, 2) 제2 입력 데이터인 \"http://urllink2\"를 인증 관 련 모델의 입력 데이터로 제공할 수 있다. 인공지능 모델의 동작과 관련된 자세한 내용은 도 4와 유사한 바 생 략하기로 한다. 전자 장치는 인공지능 모델로부터 텍스트 데이터, 키워드 데이터, 나이에 대한 데이터 및 인증에 대한 데 이터를 포함하는 출력 데이터를 수신할 수 있다. 또한, 전자 장치는 수신한 출력 데이터를 취합하여 결과 정보를 확인할 수 있다. 전자 장치는 결과 정보가 리퀘스트 아이디를 카프카 토픽의 이름으로 하는 데이터 처리 플랫폼의 카프카 토픽에 저장되도록 리퀘스트 아이디 및 결과 정보를 데이터 처리 플랫폼에 제 공할 수 있다. 이에 따라, 클라이언트 장치는 리퀘스트 아이디에 대응하는 토픽의 이름을 가지는 토픽으로 부터 결과 정보를 수신할 수 있다. 도 6은 실행 타입을 변경하는 일 실시 예를 설명하기 위한 도면이다. 일 실시 예에 따르면, 전자 장치는 클라이언트 장치로부터 리퀘스트를 수신하고, 리퀘스트와 관련된 정보를 확인할 수 있다. 도6를 참조하면, 수신된 리퀘스트는 다음과 같은 정보를 포함할 수 있다. { \"requestId\": \"01a-c140-bf972\", \"execution\": \"sync\", \"data\": [{\"imageurl\" : \"http://urllink1\", detection: [\"OCR\", \"keyword_detection\", \"age_detection\"]}, {\"imageurl\" : \"http://urllink2\", detections: \"Certification\"}] } 도 6을 참조하면, 리퀘스트 아이디는 \"01a-c140-bf972\"이고, 실행 타입은 제1 타입이고, 데이터 는 복수 개의 입력 데이터 및 복수 개의 입력 데이터 각각에 대응하는 인공지능 모델에 대한 정보를 포함 할 수 있다. 리퀘스트에 대응하는 인공지능 모델은 총 4개로, 인공지능 모델을 통한 추론에 시간이 많이 소요될 우려가 있다. 또한, 현재 서버가 처리하고 있는 테스크가 많거나 또는 현재 전자 장치와 연결 중인 다른 클 라이언트 장치의 숫자가 많은 경우에 해당되는 경우에도 인공지능 모델을 통한 추론에 많은 시간이 소요될 우려 가 있다. 다만, 실행 타입이 제1 타입으로 동기인 바, 클라이언트 장치는 결과 정보를 수신할 때까지 전자 장치와 연결된 채 대기 모드로 동작할 수 있다. 따라서, 클라이언트 장치는 다른 기능을 수행하지 못 하고, 과도한 시간을 대기할 우려가 있을 수 있다. 일 실시 예에 따르면, 전자 장치는 리퀘스트에 대응하는 적어도 하나의 인공지능 모델의 실행 순서를 확인 한 후, 적어도 하나의 인공지능 모델의 실행 상태에 대한 정보에 기반하여 결과 정보를 수신하기까지의 시간을 예측할 수 있다. 예를 들어, 예상 시간이 설정된 시간 이상이면, 전자 장치는 실행 타입이 부적절하게 설 정되었다고 결정할 수 있다. 이에 따라, 전자 장치는 실행 타입을 제1 타입에서 제2 타입으로 변경할지 여 부를 포함하는 요청을 클라이언트 장치로 전송할 수 있다. 이때, 전자 장치가 클라이언트 장치로부터 실행 타입을 변경하지 않는다는 응답을 수신하거나 2) 설 정된 시간 동안 전자 장치가 클라이언트 장치로부터 응답을 수신하지 못하는 경우, 전자 장치는 리퀘스트에 대응하는 실행 타입을 제1 타입으로 하여 결과 정보를 클라이언트 장치에 제공할 수 있다. 이 때, 전자 장치는 출력 데이터인 텍스트 데이터, 키워드 데이터, 나이에 대한 데이터 및 인증에 대한 데이 터를 포함하는 결과 정보를 데이터 처리 플랫폼에 퍼블리시하지 않을 수 있고, 전자 장치는 결과 정 보를 대기 중인 클라이언트 장치로 직접 전송할 수 있다.반대로, 도 6의 일 실시 예와 같이 전자 장치가 설정된 시간 안에 클라이언트 장치로부터 실행 타입 을 변경하는 응답을 수신할 수 있다. 이에, 전자 장치는 출력 데이터인 텍스트 데이터, 키워드 데이터, 나 이에 대한 데이터 및 인증에 대한 데이터를 포함하는 결과 정보를 데이터 처리 플랫폼에 퍼블리시하고, 클 라이언트 장치는 리퀘스트 아이디를 이용하여 리퀘스트에 대응하는 카프카 토픽에 저장된 결과 정보를 컨 숨할 수 있다. 이때, 도 6의 전자 장치의 동작은 도 5의 전자 장치의 동작과 동일한 바, 자세한 내 용은 생략하기로 한다. 도 7은 인공지능 모델을 GPU 기반 인공지능 모델 및 CPU 기반 인공지능 모델로 나누어 관리하는 일 실시 예를 설명하기 위한 도면이다. 일 실시 예에 따르면, 전자 장치는 적어도 하나의 인공지능 모델 각각을 제1 타입의 프로세서 유닛 기반 인공지능 모델 및 제2 타입의 프로세서 유닛 기반 인공지능 모델 중 하나로 확인할 수 있다. 여기서, 제1 타입 의 프로세서 유닛 및 제2 타입의 프로세서 유닛은 각각 CPU(Central Processing Unit) 및 GPU(Graphics Processing Unit)에 대응할 수 있다. 또한, 전자 장치는 제1 타입의 프로세서 유닛 기반 인공지능 모델 및 제2 타입의 프로세서 유닛 기반 인공 지능 모델을 제1 타입의 프로세서 유닛 서버 및 제2 타입의 프로세서 유닛 서버에 할당할 수 있다. 다만, 앞서 살펴본 바와 같이 제1 타입의 프로세서 유닛 기반 인공지능 모델 및 제2 타입의 프로세서 유닛 기반 인공지능 모델은 각각 제1 타입의 프로세서 유닛 서버 및 제2 타입의 프로세서 유닛 서버에서만 구동 가능한 것은 아니다. 예를 들어, 제1 타입의 프로세서 유닛 서버에서 더 효율적으로 동작하는 인공지능 모델이라도 제1 타입 의 프로세서 유닛 서버에서 현재 처리 중인 테스크가 많은 경우, 제2 타입의 프로세서 유닛 서버를 통해 구동되 는 인공지능 모델로 동작할 수 있다. 도 7을 참조하면, 제1 타입의 프로세서 유닛 서버 및 제2 타입의 프로세서 유닛 서버는 각각 도 7의 CPU 서버 및 GPU 서버에 대응할 수 있다. 일 실시 예에 따르면, 인공지능 모델은 GPU 기반 인공지능 모델 및 CPU 기반 인공지능 모델로 나누어질 수 있다. 이에 따라, 전자 장치는 리퀘스트에 대응되는 인공지능 모델을 CPU 기반 인공지능 모델 및 GPU 기반 인공지능 모델 중 하나로 분류할 수 있다. 구체적으로, 인공지능 모델 중 광학 문자 인식(Optical character recognition, OCR) 모델 및 로고 감지 모델과 같은 컴퓨터 비전 관련 인공지능 모델은 GPU를 기반으로 할 때 더 효율적인 모델일 수 있다. 보다 상세하게는, 광학 문자 인식 모델 및 로고 감지 모델은 CPU 기반의 장치 및 GPU 기반의 장치 모두에서 구동 가능하지만, GPU 기반의 장치에서 더 효율적으로 구동하는 인공지능 모델일 수 있다. 반대로, 인공지능 모델 중 자연어 처리 관련 인공지능 모델의 예인 키워드 감지 모델 및 나이 감지 모델 등은 CPU를 기반으로 할 때 더 효율적인 모델일 수 있다. 보다 상세하게는, 키워드 감지 모델 및 나이 감지 모 델은 CPU 기반의 장치 및 GPU 기반의 장치 모두에서 구동 가능하지만, CPU 기반의 장치에서 더 효율적으로 구동 하는 인공지능 모델일 수 있다. 일 실시 예에 따르면, 도 7의 서버는 GPU 서버 및 CPU 서버를 포함할 수 있다. 인공지능 모델의 추론에 GPU 서버를 이용하면 CPU 서버만을 이용하는 것보다 여러가지 측면에서 더 효율적으로 인공지 능 모델을 관리할 수 있다. 예를 들어, GPU 인스턴스를 이용함에 따라, 리퀘스트에 따른 결과 정보를 제공하는 처리 속도 및 클라이언트 장치의 레이턴시 등을 고려한 비용 등의 측면에서, 시스템의 효율성이 증대될 수 있다. 여기서, GPU 인스턴스는 GPU 서버를 구성하는 일 요소로, 프레임워크와 애플리케이션 등에서 인공지 능 등에 이용되는 GPU 서버의 서브 서버일 수 있다. 일 실시 예에 따르면, 전자 장치는 CPU 기반 인공지능 모델을 포함하는 제1 그룹을 CPU 서버에 할당 하고, GPU 기반 인공지능 모델을 포함하는 제2 그룹을 GPU 서버에 할당할 수 있다. 이와 관련하여, 복수 개의 인공지능 모델에 대응되는 서버에 대한 정보는 전자 장치에 설정되어 있을 수 있다. 즉, 전자 장치 는 복수 개의 인공지능 모델에 대응되는 서버에 대한 정보에 기반하여, 인공지능 모델에 대응하는 서버를 결정할 수 있다. 서버를 결정하는 동작은 GPU 서버에 포함되는 GPU 인스턴스 또는 CPU 서버에 포함되 는 컨테이너를 결정하는 동작을 포함할 수 있다. 일 실시 예에 따르면, 전자 장치는 제1 그룹에 포함되는 인공지능 모델을 CPU 서버에 포함되는 복수 개의 컨테이너 중 하나에 할당할 수 있다. 여기서, CPU 서버는 Kubernetes 오픈소스 플랫폼일 수 있으나, 이에 한정되는 것은 아니다. 전자 장치는 제2 그룹에 포함되는 인공지능 모델을 GPU 서버에 포함되는 복수 개의 인스턴스 중 하나에 할당할 수 있다. 즉, 인공지능 모델이 동일한 그룹에 속하더라도 실제로 실행되 는 인스턴스 또는 컨테이너는 상이할 수 있다. 예를 들어, OCR 모델은 GPU 서버에 포함되는 제1 인스턴스 에 할당될 수 있고, 키워드 감지 모델은 CPU 서버에 포함되는 제1 컨테이너에 할당될 수 있다. GPU 인스턴스 및 컨테이너는 웹 서버를 통해 인공지능 모델과 연결될 수 있다. 인공지능 모델 별로 구동되는 서버 주소는 제어 정보에 포함될 수 있다. 보다 상세하게는, 인공지능 모델 별로 서버 주소와 관련된 엔드 포인트는 제어 정보에 포함될 수 있다. 전자 장치는 인공지능 모델 각각에 대응 하는 엔드 포인트를 호출할 수 있다. 여기서, 엔드 포인트는 서버에서 출력 데이터와 같은 리소스에 접근할 수 있도록 하는 URL 과 같은 포인트일 수 있다. 또한, 엔드 포인트를 이용하면 시스템하에 새로운 인공지능 모델을 추가하는 작업은 용이하게 수행될 수 있 다. 구체적으로, 1) 새로운 인공지능 모델의 입력 데이터 및 2) 새로운 인공지능 모델의 엔드 포인트를 설정하 면, 전자 장치는 새로운 인공지능 모델을 용이하게 관리할 수 있다. 예를 들어, 차트 감지 모델을 추가할 때, 1) 입력 데이터로 제1 데이터 및 2) 엔드 포인트로 제1 URL를 설정할 수 있고, 전자 장치는 제1 데이 터 및 제1 URL에 기반하여, 차트 감지 모델의 출력 데이터를 획득할 수 있다. GPU 서버의 GPU 인스턴스와 관련하여, GPU 서버는 온디맨드 인스턴스 이외에 스팟 인스펀스를 포함할 수 있다. 여기서, 스팟 인스턴스는 관리자에 의해 임의로 중단 가능하지만, 비용 측면에서 효율적인 인스턴스를 호칭할 수 있다. 다만, 임의로 중단 가능한 바, 스팟 인스턴스에 할당되는 메모리양을 적절하게 조절할 필요가 있다. 따라서, 스팟 인스턴스가 중단되면, 전자 장치는 미리 설정된 대체 규칙에 기반하여 다른 GPU 인스 턴스에 기반하여 인공지능 모델을 구동할 수 있다. 또한 각각의 GPU 인스턴스는 단일 스레드를 통해 대응되는 머신러닝 모델과 연결되는 것을 특징으로 할 수 있다. 도 8은 인공지능 모델에 대응되는 GPU 인스턴스를 인공지능 모델의 메모리 사용에 대한 히스토리 데이터에 기반 하여 할당하는 실시 예를 설명하기 위한 도면이다. 도 8은 전자 장치가 OCR 모델, 키워드 감지 모델, 성별 감지 모델, 나이 감지 모델 및 인증 관련 모델을 서버에 포함되는 GPU 서버 및 CPU 서버에 할당하는 실시 예를 도시하고 있다. 일 실시 예에 따르면, 전자 장치는 적어도 하나의 인공지능 모델에 대응하는 GPU 인스턴스를 제2 그룹에 포함되는 인공지능 모델의 메모리 사용에 대한 히스토리 데이터에 기반하여 결정할 수 있다. 예를 들어, 각각의 GPU 인스턴스에 16GB의 메모리가 할당될 수 있다. 또한, 인공지능 모델의 메모리 사용에 대한 히스토리 데이터 에 따르면, OCR 모델, 성별 감지 모델 및 나이 감지 모델은 각각 12GB, 6GB 및 6GB의 메모리를 평균적으로 사용 할 수 있다. 이때, OCR 모델과 다른 인공지능 모델이 동일한 GPU 인스턴스에 할당되면, GPU 인스턴스에 할당된 메모리 양을 초과할 우려가 있을 수 있다. 따라서, 전자 장치는 인공지능 모델의 메모리 사용에 대한 히스 토리 데이터에 기반하여 1) 제1 인스턴스에 OCR 모델을 할당하고, 2) 제2 인스턴스에 성별 감지 모델 및 나이 감지 모델을 할당할 수 있다. 또한, 앞서 살펴본 바와 같이 전자 장치는 제2 그룹에 포함되는 인공지능 모델에 새로운 인스턴스를 할당 할 수 있다. 예를 들어, 전자 장치는 제3 인스턴스를 추가함으로써, 성별 감지 모델 및 나이 감지 모델에 제2 인스턴스 및 제3 인스턴스를 각각 할당할 수 있다. 이와 같은 프로세스는 제1 그룹에 포함되는 인공지능 모델에 대응되는 컨테이너를 결정하는 동작에도 유사하게 적용 가능하다. 구체적으로, 인공지능 모델의 메모리 사용에 대한 히스토리 데이터에 따르면, 키워드 감지 모델 및 인증 관련 모델의 메모리 사용량은 클 수 있다. 따라서, 전자 장치는 인공지능 모델의 메모리 사용에 대한 히스토리 데이터에 기반하여 1) 제1 컨테이너에 키워드 감지 모델을 할당하고, 2) 제2 컨테이너에 인증 관 련 모델을 할당할 수 있다. 또한, GPU 서버 내의 GPU 인스턴스 및 CPU 서버 내의 컨테이너는 서로 연결될 수 있다. 여기서, 연결은 GPU 인 스턴스를 통한 인공지능 모델의 구동 전후에 컨테이너를 통한 인공지능 모델이 구동될 수 있음을 의미한다. 예 를 들어, 도 8을 참조하면, 제1 인스턴스 및 제1 컨테이너는 연결될 수 있고, 제1 컨테이너와 제2 인스턴스도 연결될 수 있다. 구체적으로, OCR 모델의 출력 데이터인 텍스트 데이터는 키워드 감지 모델의 입력 데이터로 이 용될 수 있다. 이때, GPU 서버의 제1 인스턴스를 통해 OCR 모델을 실행함에 따라 생성된 텍스트 데이터는 CPU 서버의 제1 컨테이너로 전달될 수 있다. 또한, CPU 서버의 제1 컨테이너를 통해 키워드 감지 모 델을 실행함에 따라 생성된 키워드 데이터는 GPU 서버의 제2 인스턴스로 전달될 수 있다. 또한, 각각의 인공지능 모델에 대응되는 인스턴스 또는 컨테이너를 결정하는 프로세스는 고정되는 것이 아니고, 인공지능 모델의 메모리 사용 히스토리 데이터에 기반하여 유동적으로 조절될 수 있다. 예를 들어, 1) OCR 모델, 성별 감지 모델 및 나이 감지 모델이 각각 4GB, 4GB 및 4GB의 메모리를 평균적으로 사용하면, OCR 모델, 성별 감지 모델 및 나이 감지 모델은 하나의 GPU 인스턴스에 할당될 수 있다. 반대로, 모델 및 입력 데이터의용량이 커짐에 따라, OCR 모델, 성별 감지 모델 및 나이 감지 모델이 각각 12GB, 10GB 및 10GB의 메모리를 평균 적으로 사용하면, OCR 모델, 성별 감지 모델 및 나이 감지 모델은 서로 다른 GPU 인스턴스에 할당될 수 있다. 또한, 앞서 살펴본 바와 같이, GPU 기반 인공지능 모델이더라도, GPU 서버 내의 제1 인스턴스 및 제2 인스 턴스의 테스크가 많은 경우, GPU 기반 인공지능 모델은 CPU 서버의 컨테이너를 통해 동작될 수도 있다. 도 9는 일 실시 예에 따른 인공지능 모델을 관리하기 위한 전자 장치를 도식화한 블록도이다. 도 9의 전자 장치는 본원 명세서의 전자 장치에 대응될 수 있다. 본 개시의 전자 장치는 일 실시 예에 따라, 트랜시버, 스토리지 및 프로세서를 포함할 수 있다. 도 9에 도시된 구성요소들은 전자 장치를 구현하는데 있어서 필수적인 것은 아니어서, 본 명세서 상에서 설명되는 전자 장치는 위에서 열거된 구성요소들 보다 많거나, 또는 적은 구성요소들을 가질 수 있음을 본"}
{"patent_id": "10-2023-0041868", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "실시 예와 관련된 기술분야에서 통상의 지식을 가진 자라면 이해할 수 있다. 한편 실시 예에서 프로세서는 적어도 하나의 프로세서를 포함할 수 있다. 트랜시버는 유무선 통신 기술을 이용하여 외부의 장치와 통신할 수 있으며 트랜시버를 포함할 수 있 다. 외부의 장치는 클라이언트 장치, 단말, 오픈소스 플랫폼 또는 서버가 될 수 있다. 또한, 트랜시버가 이용하는 통신 기술에는 GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), LTE(Long Term Evolution), 5G, WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), 블루투스(Bluetooth), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), ZigBee, NFC(Near Field Communication) 등이 있을 수 있으며, 이에 한정되는 것은 아니다. 일 실시 예에 따라, 트랜시버는 클라이언트 장치로부터 리퀘스트를 수신할 수 있고, 제어 정보를 적 어도 하나의 인공지능 모델에 전송할 수 있다. 또한, 트랜시버는 적어도 하나의 인공지능 모델로부터 제어 정보에 따른 결과 정보를 수신할 수 있다. 실행 타입이 제1 타입이면, 전자 장치는 결과 정보를 클라이언 트 장치에 직접적으로 전송할 수 있다. 실행 타입이 제2 타입이면, 전자 장치는 결과 정보를 데이터 처리 플랫폼에 전송할 수 있다. 스토리지는 도 1 내지 도 9를 통하여 전술한 적어도 하나의 방법을 수행하기 위한 정보를 저장할 수 있다. 스토리지는 메모리로 호칭될 수 있고, 휘발성 메모리 또는 비휘발성 메모리일 수 있다. 또한, 스토리지 는 프로세서의 동작을 수행하는데 필요한 하나 이상의 명령어를 저장할 수 있고, 플랫폼 상에 저장되 거나 외부 메모리에 저장되는 데이터를 임시적으로 저장할 수 있다. 예를 들어, 스토리지는 적어도 하나의 인공지능 모델에 대한 입출력 관계에 대한 정보 및 인공지능 모델의 메모리 사용에 대한 히스토리 데이터를 저 장할 수 있다. 프로세서는 전자 장치의 전반적인 동작을 제어하고 데이터 및 신호를 처리할 수 있다. 프로세서(93 0)는 도 1 내지 도 9를 통하여 전술한 하나의 방법을 수행할 수 있다. 프로세서는 트랜시버 및 스토 리지와, 나아가 전자 장치가 더 포함할 수 있는 구성요소들과의 상호 작용을 통해 전자 장치가 수행하는 실시 예들을 제어할 수 있다. 일 실시 예에 따라, 프로세서는 클라이언트 장치로부터 리퀘 스트를 수신하고, 리퀘스트와 관련된 정보를 확인하고, 정보에 기반하여, 리퀘스트에 대응하는 적어도 하나의 인공지능 모델을 확인하고, 정보에 기반하여 식별된 제어 정보를 적어도 하나의 인공지능 모델에 전송하고, 적 어도 하나의 인공지능 모델로부터 제어 정보에 따른 결과 정보를 수신하고, 및 결과 정보를 클라이언트 장치 에게 제공할 수 있다. 한편, 본 명세서와 도면에는 본 발명의 바람직한 실시 예에 대하여 개시하였으며, 비록 특정 용어들이 사용되었 으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사용된 것 이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 여기에 개시된 실시 예 외에도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 전술한 실시 예들에 따른 전자 장치 또는 단말은, 프로세서, 프로그램 데이터를 저장하고 실행하는 메모리, 디 스크 드라이브와 같은 영구 저장부(permanent storage), 외부 장치와 통신하는 통신 포트, 터치 패널, 키(key), 아이콘 등과 같은 사용자 인터페이스 장치 등을 포함할 수 있다. 소프트웨어 모듈 또는 알고리즘으로 구현되는 방법들은 상기 프로세서상에서 실행 가능한 컴퓨터가 읽을 수 있는 코드들 또는 프로그램 명령들로서 컴퓨터가 읽을 수 있는 기록 매체 상에 저장될 수 있다. 여기서 컴퓨터가 읽을 수 있는 기록 매체로 마그네틱 저장 매체 (예컨대, ROM(read-only memory), RAM(random-Access memory), 플로피 디스크, 하드 디스크 등) 및 광학적 판독 매체(예컨대, 시디롬(CD-ROM), 디브이디(DVD: Digital Versatile Disc)) 등이 있다. 컴퓨터가 읽을 수 있는 기록 매체는 네트워크로 연결된 컴퓨터 시스템들에 분산되어, 분산 방식으로 컴퓨터가 판독 가능한 코드가 저장 되고 실행될 수 있다. 매체는 컴퓨터에 의해 판독가능하며, 메모리에 저장되고, 프로세서에서 실행될 수 있다. 본 실시 예는 기능적인 블록 구성들 및 다양한 처리 단계들로 나타내어질 수 있다. 이러한 기능 블록들은 특정 기능들을 실행하는 다양한 개수의 하드웨어 또는/및 소프트웨어 구성들로 구현될 수 있다. 예를 들어, 실시 예 는 하나 이상의 마이크로프로세서들의 제어 또는 다른 제어 장치들에 의해서 다양한 기능들을 실행할 수 있는, 메모리, 프로세싱, 로직(logic), 룩 업 테이블(look-up table) 등과 같은 직접 회로 구성들을 채용할 수 있다. 구성 요소들이 소프트웨어 프로그래밍 또는 소프트웨어 요소들로 실행될 수 있는 것과 유사하게, 본 실시 예는 데이터 구조, 프로세스들, 루틴들 또는 다른 프로그래밍 구성들의 조합으로 구현되는 다양한 알고리즘을 포함하 여, C, C++, 자바(Java), 어셈블러(assembler), 파이썬(Python) 등과 같은 프로그래밍 또는 스크립팅 언어로 구현될 수 있다. 기능적인 측면들은 하나 이상의 프로세서들에서 실행되는 알고리즘으로 구현될 수 있다. 또한, 본 실시 예는 전자적인 환경 설정, 신호 처리, 및/또는 데이터 처리 등을 위하여 종래 기술을 채용할 수 있다. “매커니즘”, “요소”, “수단”, “구성”과 같은 용어는 넓게 사용될 수 있으며, 기계적이고 물리적인 구성 들로서 한정되는 것은 아니다. 상기 용어는 프로세서 등과 연계하여 소프트웨어의 일련의 처리들(routines)의 의미를 포함할 수 있다. 전술한 실시 예들은 일 예시일 뿐 후술하는 청구항들의 범위 내에서 다른 실시 예들이 구현될 수 있다."}
{"patent_id": "10-2023-0041868", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 다양한 실시 예에 따른 전자 장치가 인공지능 모델을 관리하는 방법이 구현될 수 있는 시스템을 설명하 기 위한 도면이다. 도 2는 전자 장치가 인공지능 모델을 관리하는 방법을 나타낸 흐름도이다. 도 3은 실행 타입이 제1 타입인 경우, 전자 장치가 인공지능 모델을 관리하는 일 실시 예를 설명하기 위한 도면 이다. 도 4는 실행 타입이 제2 타입인 경우, 전자 장치가 인공지능 모델을 관리하는 일 실시 예를 설명하기 위한 도면 이다. 도 5는 실행 타입이 제2 타입인 경우, 전자 장치가 인공지능 모델을 관리하는 다른 일 실시 예를 설명하기 위한 도면이다. 도 6은 실행 타입을 변경하는 일 실시 예를 설명하기 위한 도면이다. 도 7은 인공지능 모델을 GPU 기반 인공지능 모델 및 CPU 기반 인공지능 모델로 나누어 관리하는 일 실시 예를 설명하기 위한 도면이다. 도 8은 인공지능 모델에 대응되는 GPU 인스턴스를 인공지능 모델의 메모리 사용에 대한 히스토리 데이터에 기반 하여 할당하는 실시 예를 설명하기 위한 도면이다. 도 9는 일 실시 예에 따른 인공지능 모델을 관리하기 위한 전자 장치를 도식화한 블록도이다."}
