{"patent_id": "10-2020-0140886", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0108293", "출원번호": "10-2020-0140886", "발명의 명칭": "텍스트 생성 장치 및 방법", "출원인": "휴멜로 주식회사", "발명자": "최병주"}}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨팅 장치가 텍스트 생성 모델을 이용하여 입력 텍스트로부터 출력 텍스트를 생성하는 방법으로서,상기 입력 텍스트에 포함된 적어도 하나의 단어를 가리키는 데이터를 상기 텍스트 생성 모델에 입력하여 잠재변수(latent variable)를 획득하는 단계;상기 잠재 변수를 이용하여 상기 출력 텍스트에 포함될 제1 타깃 단어가 속하는 타깃 군집을 예측하는 단계; 및상기 타깃 군집 및 상기 잠재 변수를 이용하여 상기 제1 타깃 단어를 예측하는 단계를 포함하는,텍스트 생성 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 텍스트 생성 모델은, 복수의 군집을 가지는 학습 대상 말뭉치(corpus)를 이용하여 학습된 것이고,상기 복수의 군집들은, 각각의 군집 내에 포함된 단어들 각각의 상대 도수가 각각의 군집 내에서 균일하도록 상기 학습 대상 말뭉치가 분할된 것인,텍스트 생성 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 복수의 군집들은, 각각의 군집의 정규화된 엔트로피 값들의 평균이 최대가 되도록 상기 학습 대상 말뭉치가 분할된 것이며,상기 각각의 군집의 정규화된 엔트로피 값은, 상기 각각의 군집에 포함된 단어들의 상대 도수에 기초하여 계산된 값인,텍스트 생성 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 복수의 군집들은, 각각의 군집에 포함된 단어들의 상대 도수의 총합이 각각의 군집들 사이에 균일하도록상기 학습 대상 말뭉치가 분할된 것인,텍스트 생성 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 텍스트 생성 모델은, 학습 대상 말뭉치로부터 선택된 제1 단어 시퀀스의 마지막 단어를 제외한 제2 단어 시퀀스가 상기 텍스트 생성모델에 입력될 때, 상기 텍스트 생성 모델에 의해 상기 마지막 단어가 예측될 제1 확률이 최대가 되도록 학습된것이고,상기 학습 대상 말뭉치는 복수의 군집을 가지는 것이며,공개특허 10-2021-0108293-3-상기 제1 확률은,상기 제2 단어 시퀀스로부터 상기 마지막 단어가 속하는 군집이 예측되는 제2 확률과, 상기 제2 단어 시퀀스 및상기 마지막 단어가 속하는 군집으로부터 상기 마지막 단어가 예측되는 제3 확률을 곱한 값인,텍스트 생성 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 텍스트 생성 모델은, 복수의 군집을 가지는 학습 대상 말뭉치를 이용하여 학습된 것이고,상기 타깃 군집 및 상기 잠재 변수를 이용하여 상기 제1 타깃 단어를 예측하는 단계는,상기 학습 대상 말뭉치의 단어들 중 상기 타깃 군집에 속하지 않는 단어들이 예측될 확률을 0으로 부여한 채,상기 잠재 변수를 이용하여 상기 제1 타깃 단어를 샘플링하는 단계를 포함하는,텍스트 생성 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 잠재 변수를 획득하는 단계는,상기 제1 타깃 단어의 예측에 있어서 상기 입력 텍스트에 포함된 복수의 단어들 중에 집중할 부분을 가리키는어텐션 정보가 반영된 상기 잠재 변수를 획득하는 단계를 포함하는,텍스트 생성 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 예측된 제1 타깃 단어를 상기 텍스트 생성 모델에 입력하여 제2 타깃 단어를 예측하는 단계; 및상기 예측된 제1 타깃 단어 및 상기 예측된 제2 타깃 단어를 포함하는 출력 텍스트를 제공하는 단계를 더 포함하는,텍스트 생성 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "컴퓨팅 장치가 텍스트 생성 모델을 학습시키는 방법으로서,학습 대상 말뭉치로부터 선택된 제1 단어 시퀀스의 마지막 단어를 제외한 제2 단어 시퀀스가 상기 텍스트 생성모델에 입력될 때, 상기 텍스트 생성 모델에 의해 상기 마지막 단어가 예측될 제1 확률이 최대가 되도록 상기텍스트 생성 모델을 학습시키는 단계를 포함하되,상기 학습 대상 말뭉치는 복수의 군집을 가지는 말뭉치이고,상기 제1 확률은,상기 제2 단어 시퀀스로부터 상기 마지막 단어가 속하는 군집이 예측되는 제2 확률과, 상기 제2 단어 시퀀스 및상기 마지막 단어가 속하는 군집으로부터 상기 마지막 단어가 예측되는 제3 확률을 곱한 값인,텍스트 생성 모델 학습 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 복수의 군집들은, 각각의 군집 내에 포함된 단어들 각각의 상대 도수가 각각의 군집 내에서 균일하도록 상기 학습 대상 말뭉치가 분할된 것인,공개특허 10-2021-0108293-4-텍스트 생성 모델 학습 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 복수의 군집들은, 각각의 군집의 정규화된 엔트로피 값들의 평균이 최대가 되도록 상기 학습 대상 말뭉치가 분할된 것이며,상기 각각의 군집의 정규화된 엔트로피 값은, 상기 각각의 군집에 포함된 단어들의 상대 도수에 기초하여 계산된 값인,텍스트 생성 모델 학습 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,상기 복수의 군집들은, 각각의 군집에 포함된 단어들의 상대 도수의 총합이 각각의 군집들 사이에 균일하도록상기 학습 대상 말뭉치가 분할된 것인,텍스트 생성 모델 학습 방법."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "텍스트 생성 모델에 의해 입력 텍스트로부터 출력 텍스트를 생성하는 장치로서,상기 텍스트 생성 모델은,상기 입력 텍스트를 나타내는 데이터로부터 잠재 변수(latent variable)를 계산하는 인코더; 및상기 잠재 변수를 이용하여, 복수의 군집을 가지는 학습 대상 말뭉치(corpus)에 포함된 단어들 중 상기 출력 텍스트에 포함될 타깃 단어를 예측하는 디코더를 포함하고,상기 디코더는, 상기 잠재 변수를 이용하여, 상기 복수의 군집들 중 상기 타깃 단어가 속하는 타깃 군집을 예측하는 군집 예측부; 및상기 잠재 변수 및 상기 타깃 군집으로부터 상기 타깃 단어를 예측하는 단어 예측부를 포함하는,텍스트 생성 장치."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 텍스트 생성 모델은, 상기 디코더가 상기 타깃 단어를 예측함에 있어서 상기 입력 텍스트에 포함된 복수의단어들 중 집중할 부분을 결정하는 어텐션 모듈을 더 포함하는,텍스트 생성 장치."}
{"patent_id": "10-2020-0140886", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "컴퓨팅 장치가 시퀀스 생성 모델을 이용하여 입력 시퀀스로부터 출력 시퀀스를 생성하는 방법으로서,상기 입력 시퀀스의 적어도 일부 세그먼트를 상기 시퀀스 생성 모델에 입력하여 잠재 변수를 획득하는 단계;상기 잠재 변수를 이용하여 상기 출력 시퀀스에 포함될 타깃 세그먼트가 속하는 타깃 군집을 예측하는 단계; 및상기 타깃 군집 및 상기 잠재 변수를 이용하여 상기 타깃 세그먼트를 예측하는 단계공개특허 10-2021-0108293-5-를 포함하는,시퀀스 생성 방법."}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "텍스트 생성 모델을 이용하여 입력 텍스트로부터 출력 텍스트를 생성하는 방법이 제공된다. 본 발명의 일 실시예 에 따른 텍스트 생성 방법은, 입력 텍스트에 포함된 적어도 하나의 단어를 가리키는 데이터를 상기 텍스트 생성 모델에 입력하여 잠재 변수(latent variable)를 획득하는 단계와, 상기 잠재 변수를 이용하여 상기 출력 텍스트 에 포함될 제1 타깃 단어가 속하는 타깃 군집을 예측하는 단계와, 상기 타깃 군집 및 상기 잠재 변수를 이용하여 상기 제1 타깃 단어를 예측하는 단계를 포함한다."}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 텍스트 생성 장치 및 방법에 관한 것이다. 보다 자세하게는, 신경망 기반의 텍스트 생성 모델을 이용 한 텍스트 생성에 있어서, 생성되는 텍스트의 다양성을 촉진하는 방법 및 장치와, 그 텍스트 생성 모델을 구축 하는 방법에 관한 것이다."}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "기계 번역, 기사 요약, 챗봇 등 다양한 분야에서 텍스트 생성 기술이 활용되고 있다. 전통적인 텍스트 생성 기술들 중 하나인, 규칙 기반(룰 기반) 텍스트 생성 기술은, 사전에 설정된 규칙들을 바 탕으로 입력 문장 또는 단어에 대응하는 출력 텍스트를 생성한다. 규칙을 기반으로 텍스트를 생성하는 접근 방 식에서는, 사람이 모든 규칙을 사전에 일일이 생성해야 한다. 그런데 인간의 언어는 수 많은 예외와 불확실성을 가진다는 점에서, 규칙 기반 텍스트 생성 방식은 한계를 가진다. 머신 러닝 기술이 발전하고 다양한 분야에 적용되면서, 인공 신경망에 기반한 텍스트 생성 방법론들이 연구되고 있다. 인공 신경망 기반 텍스트 생성은, 예컨대 기존에 존재하는 방대한 분량의 다양한 예제 텍스트들로 구성된 학습 대상 말뭉치(corpus)를 이용하여, 입력 텍스트에 대응하는 텍스트를 출력하도록 텍스트 생성 신경망 모델 을 비지도 방식으로 학습(unsupervised learning) 시킬 수 있다. 텍스트 생성 모델의 활용 목적에 따라, 학습 대상 말뭉치는, 예컨대 온라인 백과사전으로부터 추출된 문장들로 구성된 말뭉치, 기존 뉴스 기사들의 문장들로 구성된 말뭉치, 기존 노래 가사의 문장들로 구성된 말뭉치, 사람 사이의 일상적인 질의 응답들로 구성된 말뭉치 등, 다양한 내용과 특성을 가지는 서로 다른 말뭉치들일 수 있다. 그런데 학습에 사용될 수 있는 대다수의 말뭉치에 포함된 단어들의 분포는 균일하지 못하다. 따라서 종래의 인 공 신경망 기반 텍스트 생성 모델의 경우, 학습 대상 말뭉치에 높은 빈도로 등장하는 단어들을 지나치게 자주 생성한다는 한계가 있다. 예를 들어, 노래 가사 말뭉치로 학습된 텍스트 생성 모델의 경우 \"사랑해\", \"그리워\" 등 노래 가사에 자주 등장하는 단어들을 지나치게 자주 생성할 가능성이 높으며, 일상적인 질의 응답 말뭉치로 학습된 텍스트 생성 모델의 경우 \"그렇습니다\", \"모르겠어요\" 등의 표현들 위주로 텍스트를 생성할 가능성이 높 다. 다시 말해, 인공 신경망이 생성하는 텍스트는 사람이 만들어 내는 텍스트에 비하여 다양성이 떨어진다. 따라서, 특정 단어로 편중되지 않고 다양하고 자연스러운 단어를 고르게 생성할 수 있는 텍스트 생성 방법이 요 구된다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제10-2017229호 (2019.09.02. 공고)"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 몇몇 실시예들을 통해 해결하고자 하는 기술적 과제는, 다양성이 증대된 시퀀스를 생성하는 장치 및 그 장치에서 수행되는 방법을 제공하는 것이다. 본 발명의 몇몇 실시예들을 통해 해결하고자 하는 다른 기술적 과제는, 다양성이 증대된 시퀀스를 생성할 수 있 는 신경망 기반의 시퀀스 생성 모델을 구축하는 장치 및 그 장치에서 수행되는 방법을 제공하는 것이다. 본 발명의 몇몇 실시예들을 통해 해결하고자 하는 또 다른 기술적 과제는, 다양성이 증대된 시퀀스를 생성할 수 있는 신경망 기반의 시퀀스 생성 모델을 구축하기 위해 학습 대상 시퀀스를 군집화하는 장치 및 그 장치에서 수 행되는 방법을 제공하는 것이다. 본 발명의 몇몇 실시예들을 통해 해결하고자 하는 또 다른 기술적 과제는, 다양성이 증대된 텍스트를 생성하는 장치 및 그 장치에서 수행되는 방법을 제공하는 것이다. 본 발명의 몇몇 실시예들을 통해 해결하고자 하는 또 다른 기술적 과제는, 다양성이 증대된 텍스트를 생성할 수 있는 신경망 기반의 텍스트 생성 모델을 구축하는 장치 및 그 장치에서 수행되는 방법을 제공하는 것이다. 본 발명의 몇몇 실시예들을 통해 해결하고자 하는 또 다른 기술적 과제는, 다양성이 증대된 텍스트를 생성할 수 있는 신경망 기반의 텍스트 생성 모델을 구축하기 위해 학습 대상 말뭉치(corpus)를 군집화하는 장치 및 그 장 치에서 수행되는 방법을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "과제들은 아래의 기재로부터 본 발명의 기술분야에서의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 기술적 과제를 해결하기 위한, 본 발명의 일 실시예에 따른 텍스트 생성 방법은, 입력 텍스트에 포함된 적 어도 하나의 단어를 가리키는 데이터를 텍스트 생성 모델에 입력하여 잠재 변수(latent variable)를 획득하는 단계와, 상기 잠재 변수를 이용하여 출력 텍스트에 포함될 제1 타깃 단어가 속하는 타깃 군집을 예측하는 단계 와, 상기 타깃 군집 및 상기 잠재 변수를 이용하여 상기 제1 타깃 단어를 예측하는 단계를 포함한다. 일 실시예에서, 상기 텍스트 생성 모델은, 복수의 군집을 가지는 학습 대상 말뭉치(corpus)를 이용하여 학습된 것이고, 상기 복수의 군집들은, 각각의 군집 내에 포함된 단어들 각각의 상대 도수가 각각의 군집 내에서 균일 하도록 상기 학습 대상 말뭉치가 분할된 것일 수 있다. 몇몇 실시예에서, 상기 복수의 군집들은, 각각의 군집의 정규화된 엔트로피 값들의 평균이 최대가 되도록 상기 학습 대상 말뭉치가 분할된 것이며, 상기 각각의 군집의 정규화된 엔트로피 값은, 상기 각각의 군집에 포함된 단어들의 상대 도수에 기초하여 계산된 값일 수 있다. 몇몇 실시예에서, 상기 복수의 군집들은, 각각의 군집에 포함된 단어들의 상대 도수의 총합이 각각의 군집들 사 이에 균일하도록 상기 학습 대상 말뭉치가 분할된 것일 수 있다. 일 실시예에서, 상기 학습 대상 말뭉치는, 복수의 군집을 가지는 것이고, 상기 텍스트 생성 모델은, 상기 학습 대상 말뭉치로부터 선택된 제1 단어 시퀀스의 마지막 단어를 제외한 제2 단어 시퀀스가 상기 텍스트 생성 모델 에 입력될 때, 상기 텍스트 생성 모델에 의해 상기 마지막 단어가 예측될 제1 확률이 최대가 되도록 학습된 것 이고, 상기 제1 확률은, 상기 제2 단어 시퀀스로부터 상기 마지막 단어가 속하는 군집이 예측되는 제2 확률과, 상기 제2 단어 시퀀스 및 상기 마지막 단어가 속하는 군집으로부터 상기 마지막 단어가 예측되는 제3 확률을 곱 한 값일 수 있다. 일 실시예에서, 상기 타깃 군집 및 상기 잠재 변수를 이용하여 상기 제1 타깃 단어를 예측하는 단계는, 상기 학 습 대상 말뭉치의 단어들 중 상기 타깃 군집에 속하지 않는 단어들이 예측될 확률을 0으로 부여한 채, 상기 잠 재 변수를 이용하여 상기 제1 타깃 단어를 샘플링하는 단계를 포함할 수 있다. 일 실시예에서, 상기 잠재 변수를 획득하는 단계는, 상기 제1 타깃 단어의 예측에 있어서 상기 입력 텍스트에 포함된 복수의 단어들 중에 집중할 부분을 가리키는 어텐션 정보가 반영된 상기 잠재 변수를 획득하는 단계를 포함할 수 있다. 일 실시예에서, 상기 방법은, 상기 예측된 제1 타깃 단어를 상기 텍스트 생성 모델에 입력하여 제2 타깃 단어를 예측하는 단계와, 상기 예측된 제1 타깃 단어 및 상기 예측된 제2 타깃 단어를 포함하는 출력 텍스트를 제공하 는 단계를 더 포함할 수 있다. 상술한 기술적 과제를 해결하기 위한 본 발명의 다른 일 실시예에 따른 텍스트 생성 모델 학습 방법은, 학습 대 상 말뭉치로부터 선택된 제1 단어 시퀀스의 마지막 단어를 제외한 제2 단어 시퀀스가 상기 텍스트 생성 모델에 입력될 때, 상기 텍스트 생성 모델에 의해 상기 마지막 단어가 예측될 제1 확률이 최대가 되도록 상기 텍스트 생성 모델을 학습시키는 단계를 포함한다. 이때, 상기 학습 대상 말뭉치는 복수의 군집을 가지는 말뭉치이고, 상기 제1 확률은, 상기 제2 단어 시퀀스로부터 상기 마지막 단어가 속하는 군집이 예측되는 제2 확률과, 상기 제2 단어 시퀀스 및 상기 마지막 단어가 속하는 군집으로부터 상기 마지막 단어가 예측되는 제3 확률을 곱한 값이다. 상술한 기술적 과제를 해결하기 위한 본 발명의 또 다른 일 실시예에 따른 텍스트 생성 장치는, 텍스트 생성 모 델을 포함하며, 상기 텍스트 생성 모델은, 상기 입력 텍스트를 나타내는 데이터로부터 잠재 변수(latent variable)를 계산하는 인코더와, 상기 잠재 변수를 이용하여, 복수의 군집을 가지는 학습 대상 말뭉치(corpus) 에 포함된 단어들 중 상기 출력 텍스트에 포함될 타깃 단어를 예측하는 디코더를 포함하며, 상기 디코더는, 상 기 잠재 변수를 이용하여, 상기 복수의 군집들 중 상기 타깃 단어가 속하는 타깃 군집을 예측하는 군집 예측부 와, 상기 잠재 변수 및 상기 타깃 군집으로부터 상기 타깃 단어를 예측하는 단어 예측부를 포함한다. 일 실시예에서, 상기 텍스트 생성 모델은, 상기 디코더가 상기 타깃 단어를 예측함에 있어서 상기 입력 텍스트 에 포함된 복수의 단어들 중 집중할 부분을 결정하는 어텐션 모듈을 더 포함할 수 있다. 상술한 기술적 과제를 해결하기 위한 본 발명의 또 다른 일 실시예에 따른 시퀀스 생성 방법은, 입력 시퀀스의 적어도 일부 세그먼트를 상기 시퀀스 생성 모델에 입력하여 잠재 변수를 획득하는 단계와, 상기 잠재 변수를 이 용하여 출력 시퀀스에 포함될 타깃 세그먼트가 속하는 타깃 군집을 예측하는 단계와, 상기 타깃 군집 및 상기 잠재 변수를 이용하여 상기 타깃 세그먼트를 예측하는 단계를 포함한다."}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명의 바람직한 실시예들을 상세히 설명한다. 본 발명의 이점 및 특징, 그리 고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시예들을 참조하면 명확해질 것 이다. 그러나 본 발명의 기술적 사상은 이하의 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "될 수 있으며, 단지 이하의 실시예들은 본 발명의 기술적 사상을 완전하도록 하고, 본 발명이 속하는 기술분야 에서 통상의 지식을 가진 자에게 본 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명의 기술적 사상은 청구항의 범주에 의해 정의될 뿐이다. 각 도면의 구성요소들에 참조부호를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시 되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 발명을 설명함에 있어, 관련 된 공지 구성 또는 기능에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세 한 설명은 생략한다. 다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어(기술 및 과학적 용어를 포함)는 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 사용될 수 있다. 또 일반적으로 사용되 는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니 다. 본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 또한, 본 발명의 구성 요소를 설명하는 데 있어서, 제1, 제2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이 러한 용어는 그 구성 요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성 요소의 본질 이나 차례 또는 순서 등이 한정되지 않는다. 어떤 구성 요소가 다른 구성요소에 \"연결\", \"결합\" 또는 \"접속\"된 다고 기재된 경우, 그 구성 요소는 그 다른 구성요소에 직접적으로 연결되거나 또는 접속될 수 있지만, 각 구성 요소 사이에 또 다른 구성 요소가 \"연결\", \"결합\" 또는 \"접속\"될 수도 있다고 이해되어야 할 것이다. 명세서에서 사용되는 \"포함한다 (comprises)\" 및/또는 \"포함하는 (comprising)\"은 언급된 구성 요소, 단계, 동 작 및/또는 소자는 하나 이상의 다른 구성 요소, 단계, 동작 및/또는 소자의 존재 또는 추가를 배제하지 않는다. 본 명세서에 대한 설명에 앞서, 본 명세서에서 사용되는 몇몇 용어들에 대하여 명확하게 하기로 한다. 본 명세서에서, 텍스트란 하나 이상의 단어들로 구성된 구절 및 문장 등을 포함하는 개념이다. 본 명세서에서, 토큰이란 의미를 가지는 문장 구성 단위로서, 본 발명의 실시예들의 구현예에 따라서는 어절, 단어, 형태소, 또는 그 보다 더 작은 단위에 해당할 수 있다. 본 명세서에서 토큰과 단어는, 텍스트를 구성하는 단위를 가리키는 범위 내에서 서로 혼용될 수 있다. 본 명세서에서, 시퀀스(sequence)란 순서를 가지는 일련의 데이터들의 모음 또는 선형적으로 배열될 수 있는 일 련의 데이터들의 모음을 의미한다. 본 명세서에서, 세그먼트(segment)란 시퀀스를 구성하는 단위를 가리킨다. 예를 들어, 단어들이 순서대로 나열된 구절, 문장, 텍스트 등은 본 명세서에서 텍스트 시퀀스로 이해될 수 있으 며, 토큰이나 단어는 세그먼트로 이해될 수 있다. 다른 예로서, 악곡의 악보는 시퀀스에 대응될 수 있으며, 악 보를 구성하는 소절, 마디, 음표 등은 세그먼트에 대응될 수 있다. 또한 악곡의 코드(chord)들의 나열은 시퀀스 에 대응될 수 있으며, 하나 하나의 코드 또는 몇몇 코드들의 묶음은 세그먼트에 대응될 수 있다. 이하, 본 발명의 몇몇 실시예들에 대하여 첨부된 도면에 따라 상세하게 설명한다. 도 1은 본 발명의 일 실시예에 따른 텍스트 생성 장치의 입력 및 출력을 설명하기 위한 도면이다. 도 1에 도시된 바와 같이, 텍스트 생성 장치는 입력 텍스트를 획득하여, 이를 기초로 텍스트를 생성하여 출력 텍스트를 제공하는 컴퓨팅 장치이다. 텍스트 생성 장치는, 예컨대 주요 키워드를 입력하면 가상의 신문 기사를 자동으로 생성하는 장치, 노래의 제목을 입력하면 노래 가사를 창작하는 장치, 소설의 도입부를 입 력하면 소설의 나머지 부분을 창작하는 장치, 사용자의 발화를 입력 받으면 응답을 출력하는 무인 대화 장치 또 는 챗봇 등일 수 있다. 상기 컴퓨팅 장치는 노트북, 데스크톱(desktop), 랩탑(laptop) 등이 될 수 있으나, 이에 국한되는 것은 아니며 컴퓨팅 기능이 구비된 모든 종류의 장치를 포함할 수 있다. 상기 컴퓨팅 장치의 일 예는 도 11을 더 참조하도록 한다. 도 1은 텍스트 생성 장치가 단일 컴퓨팅 장치로 구현된 것을 예로써 도시하고 있으나, 텍스트 생성 장치 의 제1 기능은 제1 컴퓨팅 장치에서 구현되고, 제2 기능은 제2 컴퓨팅 장치에서 구현될 수도 있다. 본 발명의 다양한 실시예들에 따르면, 텍스트 생성 장치는 다양한 텍스트를 생성하기 위해서 신경망(neural network) 기반의 텍스트 생성 모델을 구축하고, 텍스트 생성 모델을 통해 출력 텍스트를 생성할 수 있다. 텍스트 생성 모델은, 기존에 존재하는 예제 텍스트들로 구성된 학습 대상 말뭉치(corpus)를 이용하여 비지도 방 식으로 학습될 수 있다."}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "예를 들어, 기사 전문을 입력하면 요약문을 출력하는 텍스트 생성 모델의 경우, 학습 대상 기사 전문 및 이에"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "대응되는 학습 대상 요약문들의 수많은 쌍들로 구성된 말뭉치를 이용하여 학습될 수 있다. 구체적으로, 학습 대"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "상 기사 전문이 입력되면 학습 대상 요약문이 출력되도록 텍스트 생성 모델의 파라미터들을 조정함으로써, 기사"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "를 요약하는 텍스트 생성 모델이 학습될 수 있다. 또 다른 예를 들면, 노래 가사를 창작하는 텍스트 생성 모델의 경우, 학습 대상 가사들로 구성된 말뭉치를 이용 하여 학습될 수 있다. 구체적으로, 학습 대상 가사의 첫 단어가 입력되면 학습 대상 가사의 두 번째 단어가 출 력되고, 학습 대상 가사의 첫 단어와 두 번째 단어가 입력되면 학습 대상 가사의 세 번째 단어가 출력되도록 하 는 등의 방식으로, 입력된 텍스트의 바로 다음 단어가 예측되도록 텍스트 생성 모델의 파라미터들을 조정함으로 써, 노래 가사를 창작하는 텍스트 생성 모델이 학습될 수 있다. 본 발명의 다양한 실시예들에 따른 텍스트 생성 모델의 학습 방법에 대해서는 도 6을 참조하여 후술한다. 이하에서는 본 발명의 일 실시예에 따른 텍스트 생성 장치의 기능적인 구성에 대하여 도 2를 참조하여 설명 한다. 도 2에 도시된 바와 같이, 텍스트 생성 장치는 입력부, 텍스트 생성 모델, 출력부, 및 저장 부를 포함할 수 있다. 다만, 도 2에는 본 발명의 실시예와 관련 있는 구성 요소들만이 도시되어 있다. 따"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "라서, 본 발명이 속한 기술분야의 통상의 기술자라면 도 2에 도시된 구성요소들 외에 다른 범용적인 구성 요소 들이 더 포함될 수 있음을 알 수 있다. 또한, 도 2에 도시된 텍스트 생성 장치의 각각의 구성 요소들은 기 능적으로 구분되는 기능 요소들을 나타낸 것으로서, 복수의 구성 요소가 실제 물리적 환경에서는 서로 통합되는 형태로 구현될 수도 있음에 유의한다. 이하, 각 구성요소에 대하여 상세하게 설명한다. 입력부는 텍스트를 입력 받고, 입력 받은 텍스트에 대한 전처리를 수행한다. 입력받은 텍스트에 대한 전처 리는, 입력 받은 텍스트를 단어(또는 토큰) 단위로 분할하는 과정을 포함한다. 입력부는 입력 받은 텍스트 를 단어 단위로 쪼개어 토큰화된 텍스트를 표현하는 데이터를 텍스트 생성 모델에 제공할 수 있다. 텍 스트 생성 장치가 학습 대상 말뭉치의 군집화를 수행하는 몇몇 실시예에서는, 입력부가 학습 대상 말 뭉치를 입력 받는 기능을 더 수행할 수 있다. 텍스트 생성 모델은 입력부로부터 단어 단위로 제공받은 입력 텍스트에 기초하여, 새로운 단어를 생 성한다. 텍스트 생성 모델에 의해 생성된 단어는 출력부로 제공되며, 또한 텍스트 생성 모델에 다시 입력되어, 상기 생성된 단어의 후속 단어를 생성하는데 사용될 수도 있다. 텍스트 생성 모델은, 입력 텍스트를 저차원의 벡터로 임베딩하고 이로부터 잠재 변수를 계산하는 인코더 및 잠재 변수에 기초하여 출력 텍스트를 샘플링하는 디코더로 구성될 수 있다. 텍스트 생성 모델의 인코더 및 디코더는 하나 또는 그 이상의 인코더들과 디코더들을 포함하는 트랜스포머(transformer) 모델 등을 활용하 여 구성될 수 있는데, 본 발명이 그러한 실시예로 한정되는 것은 아니다. 예를 들어 텍스트 생성 모델의 인코더 및 디코더는 순환 신경망(RNN: Recurrent Neural Network) 또는 장단기 메모리(LSTM: Long Short-Term Memory) 모델 등을 활용하여 구성될 수도 있다. 텍스트 생성 모델의 세부 사항에 대해서는 도 6 및 도 7을 참조하여 후술하기로 한다. 출력부는 텍스트 생성 모델이 생성한 단어들에 대한 후처리를 수행한다. 예를 들어 출력부는 텍 스트 생성 모델이 순차적으로 생성한 단어들을 이어 붙여서(concatenate) 구절 또는 문장 단위의 출력 텍 스트를 출력할 수 있다. 저장부는 각종 데이터를 저장하고 관리한다. 특히 저장부는 텍스트 생성 모델의 학습에 사용되 는 학습 대상 말뭉치를 저장할 수 있다. 또한 저장부는 텍스트 생성 모델을 구성하는 신경망에 관한 각종 파라미터 및 설정들을 저장하고 관리할 수 있다. 지금까지 도 1 및 도 2를 참조하여 본 발명의 일 실시예에 따른 텍스트 생성 장치의 기능적인 구성과 입출 력을 설명하였다. 이하에서는, 본 발명의 다른 일 실시예에 따라, 텍스트 생성 장치의 텍스트 생성 모델 을 구축하고, 입력 텍스트에 기초하여 출력 텍스트를 생성하는 일련의 과정을 설명한다. 도 3은 본 발명의 일 실시예에 따라 학습 대상 말뭉치를 이용하여 텍스트 생성 모델을 구축하고 텍스트를 생성 하는 일련의 과정을 나타내는 예시적인 흐름도이다. 단, 이는 본 발명의 목적을 달성하기 위한 바람직한 실시예 일 뿐이며, 필요에 따라 일부 단계가 추가되거나 삭제될 수 있음은 물론이다. 도 3에 도시된 텍스트 생성 방법의 각 단계는 예컨대 텍스트 생성 장치와 같은 컴퓨팅 장치에 의해 수행될 수 있다. 다시 말하면, 상기 텍스트 생성 방법의 각 단계는 컴퓨팅 장치의 프로세서에 의해 실행되는 하나 이상 의 인스트럭션들로 구현될 수 있다. 상기 텍스트 생성 방법에 포함되는 모든 단계는 하나의 물리적인 컴퓨팅 장 치에 의하여 실행될 수도 있을 것이나, 상기 방법의 제1 단계들은 제1 컴퓨팅 장치에 의하여 수행되고, 상기 방 법의 제2 단계들은 제2 컴퓨팅 장치에 의하여 수행될 수도 있다. 예컨대, 도 3에 도시된 복수의 군집 식별 단계 (S100), 텍스트 생성 모델 학습 단계(S200), 텍스트 생성 단계(S300)는 서로 다른 컴퓨팅 장치에 의해 수행될수도 있다. 이하에서는, 상기 텍스트 생성 방법의 각 단계가 텍스트 생성 장치에 의해 수행되는 것을 가정 하여 설명을 이어가도록 한다. 다만, 설명의 편의를 위해, 상기 텍스트 생성 방법에 포함되는 각 단계의 동작 주체는 그 기재가 생략될 수도 있다. 도 3에 도시된 바와 같이, 본 실시예에 따른 텍스트 생성 방법은, 텍스트 생성 모델이 학습할 말뭉치를 복수의 군집으로 나누는 과정, 학습 대상 말뭉치를 사용하여 텍스트 생성 모델을 구축하는 학습 과정, 및 상기 텍스트 생성 모델을 통해 입력 텍스트로부터 출력 텍스트를 생성하는 과정으로 구성될 수 있다. 종래의 텍스트 생성 모델 구축 방법과는 달리, 본 발명의 몇몇 실시예들에서는 학습 대상 말뭉치를 사용하여 텍 스트 생성 모델을 학습시키기에 앞서서, 먼저 학습 대상 말뭉치가 군집화된다(단계 S100). 보다 구체적으로, 각 각의 군집에 포함된 단어들의 분포가 최대한 균일해지도록 하는 방식으로 학습 대상 말뭉치가 복수의 군집으로 구분될 수 있다. 이때, 학습 대상 말뭉치가 각각의 군집에 대응되는 복수의 데이터 세트들로 분할되어야 하는 것은 아니며, 학습 대상 말뭉치에 포함된 각각의 단어 또는 토큰들이 속하는 군집이 서로 구분되어 식별될 수 있으면 충분하다. 단계 S100에서 학습 대상 말뭉치를 군집화하는 과정은 텍스트 생성 장치에 의해 수행될 수도 있지만, 별도의 컴퓨팅 장치에 의해 수행되어 텍스트 생성 장치로 제공될 수도 있다. 학습 대상 말뭉치의 군집화 과정에 대해서는 도 4를 참조하여 보다 상세히 설명하기로 한다. 단계 S200에서는, 단계 S100에서 복수의 군집들이 식별된 학습 대상 말뭉치를 이용하여 텍스트 생성 모델 이 학습된다. 이때, 학습 대상 말뭉치로부터 식별된 복수의 군집 별로 각각 서로 다른 복수의 텍스트 생성 모델 이 학습되는 것이 아님에 유의한다. 텍스트 생성 모델은, 학습 대상 말뭉치에 포함된 기존 문장들로 부터 선택된 입력 텍스트로부터, 타깃 단어가 속하는 타깃 군집을 예측하고, 예측된 군집 내에서 타깃 단어를 예측할 수 있도록 학습된다. 여기서 상기 타깃 단어란, 예컨대 상기 학습 대상 말뭉치에서 선택된 상기 입력 텍 스트가 속하는 문장 내에서 상기 입력 텍스트의 바로 뒤에 이어지는 단어일 수 있다. 본 발명의 다양한 실시예들에 따른 텍스트 생성 모델의 학습 과정에 대해서는 도 6을 참조하여 후술하기로 한다. 단계 S300에서는, 학습된 텍스트 생성 모델에 텍스트가 입력되고, 이에 대응되는 출력 텍스트가 생성된다. 단계 S300은, 입력된 텍스트를 전처리하는 단계, 전처리된 텍스트를 텍스트 생성 모델에 입력하는 단계, 텍스트 생성 모델이 출력한 단어 또는 텍스트를 처리하여 출력하는 단계 등의 세부 과정들을 포함할 수 있다. 본 발명 의 다양한 실시예들에 따른 텍스트 생성 과정에 대해서는 도 8 및 도 9를 참조하여 후술하기로 한다. 지금까지 도 3를 참조하여 본 발명의 일 실시예에 따라 학습 대상 말뭉치를 이용하여 텍스트 생성 모델을 구축 하고 텍스트를 생성하는 일련의 과정에 대하여 설명하였다. 이하에서는, 도 4 및 도 5를 참조하여 단계 S100에 서 수행될 수 있는 학습 대상 말뭉치의 군집화 방법에 대하여 보다 상세하게 설명하도록 한다. 도 4는 본 발명의 일 실시예에 따른 텍스트 생성 모델이 학습하는 학습 대상 말뭉치를 군집화하는 과정을 나타내는 예시적인 흐름도이다. 단, 이는 본 발명의 목적을 달성하기 위한 바람직한 실시예일 뿐이며, 필요에 따라 일부 단계가 추가되거나 삭제될 수 있음은 물론이다. 전술한 바와 같이, 본 발명의 일 실시예에 따르면, 학습 대상 말뭉치를 사용하여 텍스트 생성 모델을 학습 시키기에 앞서서, 상기 학습 대상 말뭉치는 각각의 군집에 포함된 단어들의 분포가 최대한 균일해지도록 하는 방식으로 복수의 군집으로 구분된다. 도 4에 도시된 바와 같이, 상기 학습 대상 말뭉치의 군집화 방법은 학습 대상 말뭉치를 토큰화하는 단계 S110에 서 시작된다. 보다 구체적으로, 학습 대상 말뭉치에 포함된 문장 또는 텍스트들을 구성하는 각각의 단어 또는 토큰들이 식별된다. 단계 S120에서는, 각각의 단어들의 발생 빈도(또는 발생 회수)가 식별된다. 또한 각각의 단어들은 발생 빈도가 높은 순으로 정렬될 수 있다. 도 5에 도시된 그래프는 예시적인 학습 대상 말뭉치에 포함된 각각의 단어들의 발생 빈도를, 발생 빈도가 높은 단어부터 낮은 단어의 순으로 나타낸 예시적인 그래프이다. 그래프의 x축은 발생 빈도 순위를 나타내 고 y축은 발생 빈도를 나타낸다. 일상적으로 사용되는 언어에 있어서, 다양한 단어들이 사용되는 빈도는 균일하지 않다. 예를 들어 한국어의 단 어들 중에 조사(postpositional particle)들의 사용 빈도가 다른 단어들보다 높고, 영어의 단어들 중에 관사(article), 대명사(pronoun), 전치사(preposition), 접속사(conjunction)들의 사용 빈도는 다른 단어들보다 월 등히 높다. 또한, 각 단어가 사용되는 빈도는 서로 현격한 차이를 가지는 것이 일반적이다. 따라서 통상적으로 이용 가능한 학습 대상 말뭉치에 포함된 단어들의 발생 빈도를 카운트하면, 도 5의 그래프에 나타난 것처럼 특정 단어들의 발생 빈도가 현격히 높은 분포를 가질 가능성이 크다. 다시 말해, 학습 대상 말뭉치에 포함된 단 어들의 발생 빈도가 균일하지 않을 가능성이 크다. 전술한 바와 같이, 특정 단어들이 차지하는 비중이 매우 큰, 통상적인 말뭉치를 이용하여 텍스트 생성 모델을 학습시킬 경우, 텍스트 생성 모델에 의해 생성되는 텍스트도 상기 특정 단어들로 지나치게 편중되는 문제를 가진다. 단계 S130에서는 학습 대상 말뭉치가 복수의 군집들로 분할될 수 있다. 상기 복수의 군집들은, 각각의 군집에 속하는 단어들의 상대 도수(relative frequency)가 가급적 균일해지도록 하는 방식으로 결정될 수 있다. 다만 전술한 바와 같이, 학습 대상 말뭉치가 실제로 복수의 데이터 세트들로 분할되어야 하는 것은 아님에 유의한다. 학습 대상 말뭉치에 포함된 각각의 단어 또는 토큰들이 어느 군집에 속하는지 식별될 수 있으면 충분하다. 도 5는 단계 S130에서 학습 대상 말뭉치(C)가 복수의 군집들(C1 내지 C4)로 군집화 된 예시적인 모습을 도시한다. 도 5의 그래프(42a 내지 42d)를 참조하면, 군집(C1)에는 발생 빈도가 상대적으로 높은 단어들이 포함 되고, 군집(C4)에는 발생 빈도가 상대적으로 낮은 단어들이 포함되도록 말뭉치(C)가 군집화되었다. 즉, 발생 빈 도가 서로 비슷한 단어들이 하나의 군집에 속하도록 군집화됨으로써, 각각의 군집(C1 내지 C4) 내의 단어들의 상 대 도수의 분포는, 말뭉치(C) 내의 단어들의 상대 도수의 분포보다 균일해졌다는 것을 이해할 것이다. 몇몇 실시예에서, 각각의 군집에 속하는 단어들의 상대 도수가 균일해지도록 학습 대상 말뭉치를 분할하는 단계 는, 각각의 군집에 포함된 단어들의 상대 도수에 기초하여 계산된 정규화된 엔트로피 값들의 평균이 최대가 되 도록 함으로써 수행될 수 있다. 각각의 군집의 정규화된 엔트로피 값은 아래 수학식 1에 의해 계산될 수 있다. 수학식 1"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "(n은 군집에 포함된 단어의 수, p(xi)는 단어 xi의 상대 도수) 또한 몇몇 실시예에서, 각각의 군집에 속하는 단어들의 상대 도수가 균일해지도록 학습 대상 말뭉치를 분할하는 단계는, 각각의 군집에 포함된 단어들의 상대 도수의 합이 각각의 군집들 사이에 균일하도록 상기 학습 대상 말 뭉치를 분할하는 단계를 포함하는 것일 수 있다. 예를 들어, 군집의 개수가 4개일 경우 각 군집에 포함된 단어 들의 상대 도수의 합이 1/4에 근사하도록 군집이 결정될 수 있으며, 군집의 개수가 10개 인 경우 각 군집에 포 함된 단어들의 상대 도수의 합이 1/10에 근사하도록 군집이 결정될 수 있다. 몇몇 실시예에서, 각각의 군집의 정규화된 엔트로피 값들의 평균이 최대가 되며, 각각의 군집에 포함된 단어들 의 상대 도수의 합이 각각의 군집들 사이에 균일하도록, 말뭉치를 적절한 개수의 군집으로 분할하는 과정은, 아 래의 의사 코드로 표현된 알고리즘을 실행함으로써 수행될 수 있다. 다시 도 5를 참조하면, 학습 대상 말뭉치 전체에 포함된 단어들의 상대 도수에 따라 계산된 정규화된 엔트로피 는 0.932이다. 한편, 학습 대상 말뭉치를 4개의 군집 C1 내지 C4로 분할하고, 군집 C1 내지 C4에 대해 계산된 정 규화된 엔트로피는 각각 0.986, 0.993, 0.986, 0.995이다. 즉, 각각의 군집의 정규화된 엔트로피가 학습 대상 말뭉치 전체의 정규화된 엔트로피(0.932)보다 크다는 것을 알 수 있으며, 이는 각각의 군집에 포함된 단어들의 상대 도수의 분포가 학습 대상 말뭉치 전체에 포함된 단어들의 분포보다 균일하다는 것을 나타낸다. 지금까지 도 4 및 도 5를 참조하여 텍스트 생성 모델의 학습과 텍스트 생성에 사용될 말뭉치를 복수의 군 집으로 분할하되, 각각의 군집에 포함된 단어들의 상대 도수가 가급적 균일하도록 군집을 결정하고, 각각의 군 집에 포함된 단어들의 상대 도수의 총합이 군집들 사이에 서로 균일하도록 군집을 결정하는 방법에 대하여 설명 하였다. 이와 같은 방법으로 학습 대상 말뭉치를 균일한 분포를 가지는 군집들로 구성하고, 균일한 군집들로 구 분된 학습 대상 말뭉치를 이용하여 후술할 텍스트 생성 모델을 구축함으로써, 텍스트 생성 모델에 의한 텍스트 생성 결과가, 학습 대상 말뭉치에 높은 빈도로 포함된 특정 단어들로 편중되는 문제를 해결할 수 있다. 이하에서는 도 6 이하를 참조하여, 텍스트 생성 모델의 구조, 복수의 군집으로 분할된 말뭉치를 이용하여 텍스트 생성 모델을 학습시키는 방법, 및 텍스트 생성 모델을 이용하여 텍스트를 생성하는 방법에 관 하여 상세히 설명하기로 한다. 도 6은 본 발명의 일 실시예에 따른 텍스트 생성 장치의 텍스트 생성 모델을 나타내는 예시적인 블록 도이다. 텍스트 생성 모델은 입력부로부터 토큰화된 텍스트를 나타내는 데이터를 입력 받아서 새로운 단 어를 생성하고 출력한다. 텍스트 생성 모델은 인코더 및 디코더로 구성될 수 있다. 인코더 및 디코더는 신경망으로 구성될 수 있다. 인코더는 토큰화된 입력 텍스트를 하나 이상의 저차원의 벡터로 임베딩하고, 이로부터 잠재 변수를 계산한 다. 인코더에 의해 입력 텍스트가 벡터로 임베딩되는 과정은, 당해 기술 분야에서 널리 알려진 신경망 기 반 텍스트 인코딩 기술들을 참고하여 수행될 수 있다. 디코더는 잠재 변수를 입력 받아서 새로 생성될 타깃 단어가 속하게 될 타깃 군집을 예측하고, 상기 잠재 변수 및 예측된 타깃 군집에 기초하여 타깃 단어를 예측하며, 예측된 타깃 단어를 출력한다. 디코더에 의 한 타깃 군집의 예측은, 주어진 잠재 변수에 의해 타깃 군집을 예측하는 제1 조건부 확률 모델에, 인코더 가 제공한 잠재 변수를 입력하여 타깃 군집을 예측함으로써 수행될 수 있다. 또한 디코더에 의한 타깃 단 어의 예측은, 주어진 잠재 변수 및 주어진 군집에 의해 타깃 단어를 예측하는 제2 조건부 확률 모델에, 상기 인 코더가 제공한 잠재 변수 및 상기 예측된 타깃 군집을 입력하여 타깃 단어를 샘플링함으로써 수행될 수 있 다. 본 발명의 몇몇 실시예에 따른 텍스트 생성 모델의 디코더는, 잠재 변수로부터 타깃 단어의 군집을 예측하고, 다시 그 결과를 반영하여 타깃 단어를 예측한다는 점에서, 종래의 신경망 기반 디코더들과는 차이가 있음을 이해할 것이다. 지금까지 텍스트 생성 모델이 인코더와 디코더로 구성될 수 있음을 설명하였다. 이하에서는 본 발명의 몇몇 실시예에 따라, 학습 대상 말뭉치를 이용하여 상기 인코더 및 상기 디코더의 파라미터들 을 조정함으로써, 텍스트 생성 모델을 학습시키는 방법에 관하여 설명한다. 텍스트 생성 모델은 학습 대상 말뭉치를 이용하여 학습된다. 특히, 텍스트 생성 모델은 각 군집에 포 함된 단어들의 상대 도수가 최대한 균일하도록 군집화된 학습 대상 말뭉치를 이용하여 학습된다. 인공 신경망에 기반하여 텍스트를 생성하는 모델들은, 학습 대상 말뭉치에 포함된 기존 문장들로부터 선택된 입 력 텍스트가 주어질 때, 상기 입력 텍스트가 속하는 문장의 다음 단어를 예측하도록 학습될 수 있다. 가령, 학 습 대상 말뭉치에 (x1, x2, x3, ..., xi-1, xi)라는 텍스트가 존재하고, 상기 텍스트에서 마지막 단어를 제외한 (x1, x2, x3, ..., xi-1)가 텍스트 생성 모델에 입력될 경우, 상기 텍스트의 마지막 단어인 xi가 예측될 가 능성이 최대화되도록, 텍스트 생성 모델의 인코더 신경망과 디코더 신경망의 파라미터들이 조정될 수 있다. 본 발명의 몇몇 실시예에 따른 텍스트 생성 모델은, 학습 대상 말뭉치에 포함된 기존 문장들로부터 선택된 입력 텍스트가 주어질 때, 상기 입력 텍스트가 속하는 문장의 다음 단어(타깃 단어)가 속하는 타깃 군집을 예측 하고, 예측된 군집 내에서 상기 타깃 단어를 예측할 가능성이 최대화되도록 학습된다. 수학식 2"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "(x1:i-1은 입력 텍스트, xi는 타깃 단어, C(xi)는 타깃 단어가 속하는 군집) 상기 수학식 2에서, P1은 입력 텍스트(x1:i-1)가 주어질 때, 상기 입력 텍스트(x1:i-1)가 속하는 문장의 다음 단어 인 타깃 단어(xi)가 속하는 군집(C(xi))이 예측될 조건부 확률을 나타낸다. 상기 수학식 2에서, P2는 입력 텍스 트(x1:i-1)와 타깃 단어(xi)의 군집(C(xi))이 주어질 때 타깃 단어(xi)가 예측될 조건부 확률을 나타낸다. 본 실시예에 따른 텍스트 생성 모델은 각 군집에 포함된 단어들의 상대 도수가 최대한 균일해지도록 군집 화된 학습 대상 말뭉치로부터 선택된 텍스트들을, 텍스트 생성 모델에 반복적으로 입력하여, 상기 확률 P1 과 확률 P2를 곱한 값이 최대화되도록, 인코더 및 디코더의 파라미터를 조정함으로써 학습될 수 있다. 지금까지 도 6을 참조하여, 인코더 및 디코더를 구비한 텍스트 생성 모델의 구조를 설명한 후, 텍스트 생성 모델을 학습시키는 방법에 대하여 설명하였다. 이하에서는 텍스트 생성 모델의 인코더 및 디코더의 세부에 대하여 보다 자세히 설명한다. 도 7은 도 6을 참조하여 설명한 텍스트 생성 모델의 인코더 및 디코더의 기능적인 구성을 설명하기 위한 블록도이다. 인코더 및 디코더는 신경망으로 구성될 수 있다. 인코더는 단어 임베딩 모듈 및 잠재 변수 계산 모듈을 구비할 수 있다. 몇몇 실시예에서 인코더 는 단어 임베딩 모듈과 잠재 변수 계산 모듈의 사이에 어텐션 모듈을 추가로 구비할 수 있 다. 다른 몇몇 실시예에서는, 단어 임베딩 모듈과 어텐션 모듈이 하나의 모듈로 구현될 수도 있다. 단어 임베딩 모듈은 입력 텍스트 또는 입력 텍스트를 토큰화한 텍스트를 표현하는 데이터를 저차원 의 벡터로 임베딩한다. 보다 구체적으로, 단어 임베딩 모듈은 입력된 단어 또는 텍스트의 의미를 나타내는, 잠재 공간에 존재하는 벡터로 변환할 수 있다. 단어 임베딩 모듈이 입력 단어 또는 텍스트를 임 베딩하는 과정은, 당해 기술 분야에서 널리 알려진 기법(예컨대 word2vec 등)에 의해 학습되고 수행될 수 있다. 잠재 변수 계산 모듈은 단어 임베딩 모듈에 의해 임베딩된 벡터로부터 잠재 변수를 계산한다. 잠재 변수는, 단어 또는 텍스트의 의미를 나타내는 잠재 벡터 공간 내에서 선택되는 변수일 수 있다. 임베딩 벡터로부터 잠재 변수를 계산하는 과정은 당해 기술 분야에 널리 알려진 다양한 방법들을 사용할 수 있으며, 본 발명 의 논지를 흐리지 않기 위해 이에 관한 더 이상의 설명은 생략하도록 한다. 어텐션 모듈은 텍스트 생성 모델에 입력된 텍스트에 포함된 복수의 단어들 중에, 타깃 단어의 예측에 있어서 집중할 부분을 가리키는 어텐션 정보를 반영하는 모듈이다. 어텐션 모듈은, 예컨대 단어 임베딩 모 듈이 입력 텍스트에 포함된 복수의 단어들을 임베딩한 벡터들 각각에 대하여, 타깃 단어의 예측에 있어 서 집중할 부분과 그렇지 않은 부분을 가리키는 가중치(즉, 어텐션 정보)를 적용하여 계산한 벡터를 출력할 수 있다. 어텐션 모듈은 어텐션 정보가 반영된 벡터를 잠재 변수 계산 모듈에 제공함으로써, 잠재 변수 계산 모듈이 어텐션 정보가 반영된 잠재 변수를 계산할 수 있도록 한다. 다른 몇몇 실시예에서는, 별도의 어텐션 모듈을 구비하는 대신에, 단어의 임베딩 벡터에 단어의 위치에 관 한 정보가 함께 인코딩되는 포지셔널 인코딩 방식이 사용될 수도 있다. 지금까지 설명한 인코더의 단어 임베딩 모듈, 어텐션 모듈, 및 잠재 변수 계산 모듈은 하 나의 유기적인 인코더 신경망을 구성하는 레이어로서 구현될 수 있다. 디코더는 군집 예측부 및 단어 예측부로 구성될 수 있다. 군집 예측부는 인코더가 제공한 잠재 변수로부터 타깃 단어가 속하는 타깃 군집을 예측한다. 군집 예 측부는, 주어진 잠재 변수에 의해 타깃 군집을 예측하는 제1 조건부 확률 모델에, 인코더가 제공한 잠재 변수를 입력하여 타깃 군집을 예측할 수 있다. 단어 예측부는 인코더가 제공한 잠재 변수 및 군집 예측부에 의해 예측된 타깃 단어의 군집으로 부터, 타깃 단어를 예측한다. 단어 예측부는, 주어진 잠재 변수 및 군집에 의해 타깃 단어를 예측하는 제2 조건부 확률 모델에, 상기 인코더가 제공한 잠재 변수 및 상기 예측된 타깃 군집을 입력하여 타깃 단어를 샘플링함으로써 타깃 단어를 예측할 수 있다. 도 7에서는, 이해의 편의를 위하여 군집 예측부와 단어 예측부를 구분하여 도시하였으나, 군집 예측 부와 단어 예측부는 하나의 유기적인 디코더 신경망을 구성하는 레이어들로서 구현될 수 있다. 수학식 3"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "상기 수학식 3은 단어 예측부가 주어진 잠재 변수 및 군집으로부터 타깃 단어를 예측하는 예시적인 제2 조 건부 확률 모델을 나타낸다. 수학식 3으로부터 알 수 있듯이, 단어 예측부는 군집 예측부에 의해 예 측된 군집에 속하는 단어들에 대해서만 확률 값을 부여하며( ), 예측된 군집에 속하지 않는 단어들에 대해서는 확률 값을 0으로 고정한 확률 모델을 사용한다. 수학식 3에 나타난 것과 같은 확률 모 델을 사용함으로써 단어 예측부는 군집 예측부에 의해 예측된 군집에 속하는 단어들의 범주 내에서 타깃 단어를 샘플링할 수 있게 된다. 전술한 바와 같은 방법으로 균일한 군집들로 구분된 학습 대상 말뭉치를 이용하여 구축되며 전술한 확률 모델에 의해 타깃 단어를 예측하는 텍스트 생성 모델을 이용함으로써, 텍스트 생성 결과가 학습 대상 말뭉치에 높 은 빈도로 포함된 특정 단어들로 편중되는 문제를 해결할 수 있게 된다. 지금까지 도 7을 참조하여 텍스트 생성 모델의 인코더 및 디코더의 세부에 대하여 설명하였다. 이하에서는, 도 8 및 도 9를 참조하여, 지금까지 설명한 텍스트 생성 장치에 의해 텍스트를 생성하는 방법을 설명한다. 도 8은 본 발명의 일 실시예에 따라 텍스트 생성 모델을 이용하여 텍스트를 생성하는 방법을 나타내는 예시적인 흐름도이다. 도 8에 도시된 텍스트 생성 방법의 각 단계는 예컨대 텍스트 생성 장치와 같은 컴퓨팅 장치에 의해 수행될 수 있다. 이하에서는, 상기 텍스트 생성 방법의 각 단계가 텍스트 생성 장치에 의해 수행되는 것을 가정하여 설명을 이어가도록 한다. 다만, 설명의 편의를 위해, 각 단계의 동작 주체는 그 기재가 생략될 수도 있다. 먼저 단계 S310에서 입력 텍스트가 획득되고 단계 S320에서는 입력 텍스트에 대한 전처리가 수행된다. 입력 텍 스트에 대한 전처리는 입력 텍스트를 복수의 단어 또는 토큰으로 분할하는 과정을 포함할 수 있다. 입력 텍스트 를 복수의 단어 또는 토큰으로 분할하는 과정은 예컨대 텍스트 생성 장치의 입력부에 의해 수행될 수 있다. 단계 S330에서는 입력 텍스트의 잠재 변수가 계산된다. 상기 잠재 변수는, 예컨대 상기 입력 텍스트를 나타내는 데이터를 텍스트 생성 장치의 텍스트 생성 모델 내의 인코더에 입력함으로써 수행될 수 있다. 상 기 잠재 변수의 계산은, 상기 입력 텍스트를 나타내는 데이터를 임베딩 벡터로 변환하고, 임베딩 벡터로부터 잠 재 변수를 계산하는 일련의 과정을 포함할 수 있다. 입력 텍스트가 복수의 단어들을 포함한다면, 복수의 단어들에 대한 임베딩 벡터가 각각 획득되고, 복수의 단어 들에 대응되는 임베딩 벡터들을 기초로 잠재 변수가 계산될 수 있다. 이때, 상기 복수의 단어들 중에 타깃 단어 의 예측에 있어서 집중할 부분을 가리키는 어텐션 정보가 추가로 반영되어 잠재 변수가 계산될 수 있다. 예컨대 복수의 단어들에 대한 임베딩 벡터들 각각에 대하여, 타깃 단어의 예측에 있어서 집중할 부분과 그렇지 않은 부 분을 가리키는 가중치(즉, 어텐션 정보)를 적용하여 계산된 벡터를 기초로 잠재 변수가 계산될 수 있다. 단계 S340에서는, 상기 잠재 변수를 기초로 타깃 단어가 속하는 군집이 예측된다. 타깃 단어의 군집의 예측은, 예컨대 텍스트 생성 모델의 디코더 내의 군집 예측부에 의해 수행될 수 있다. 단계 S350에서는, 단계 330에서 계산된 잠재 변수와, 단계 S340에서 예측된 군집에 기초하여, 타깃 단어가 예측 된다. 이때 상기 타깃 단어는, 상기 예측된 군집에 속하는 단어들 중에서 예측된다. 예측된 군집 내에서 타깃 단어를 예측하는 것은, 수학식 3을 참조하여 전술한 확률 모델을 이용함으로써 달성될 수 있다. 타깃 단어의 예 측은, 예컨대 텍스트 생성 모델의 디코더 내의 단어 예측부에 의해 수행될 수 있다. 도 8에 도시되지는 않았지만, 단계 S350에서 예측된 단어는 다시 텍스트 생성 모델에 입력되어, 단계 S330 내지 S350을 거치면서 타깃 단어의 다음 단어를 예측하는 과정이 반복적으로 수행될 수 있다. 단계 S360에서는, 단계 S350에서 예측된 타깃 단어를 기초로 출력 텍스트가 생성되고 제공된다. 예를 들어 단계 S330 내지 S350을 반복적으로 수행하면서 순차적으로 생성된 단어들을 이어 붙여서 구절 또는 문장 단위의 출력 텍스트가 제공될 수 있다. 도 9는, 본 발명의 일 실시예에 따라, 입력된 텍스트에 포함된 복수의 단어들(W1, W2, W3, ... Wn)로부터 타 깃 단어들(T1, T2, T3, ... Tm)을 생성하는 텍스트 생성 장치의 동작과 텍스트 생성 모델 내부의 신경 망 구조를 나타내는 도면이다. 텍스트 생성 장치에 입력된 텍스트는 입력부에 의해 복수의 토큰들 또는 단어들(W1, W2, W3, ... Wn)로 분할되고, 텍스트 생성 모델의 인코더로 전달된다. 인코더의 단어 임베딩 모듈은 입력된 단어들(W1, W2, W3, ... Wn) 각각을 임베딩 벡터로 변환하여 어 텐션 모듈에 전달한다. 어텐션 모듈은 타깃 단어의 예측에 있어서 집중할 부분을 가리키는 어텐션 정보가 반영된 벡터를 잠재 변 수 계산부로 전달한다. 잠재 변수 계산부는 타깃 단어들(T1, T2, T3, ... Tm)의 생성을 위한 잠재 변수들(Z1, Z2, Z3, ... Zm)을 계 산하고, 잠재 변수들(Z1, Z2, Z3, ... Zm)을 디코더에 전달한다. 디코더의 군집 예측부는 각각의 잠재 변수(Z1, Z2, Z3, ... Zm)에 기초하여 타깃 단어(T1, T2, T3, ... Tm)가 속할 군집을 예측하며, 예측된 군집(C1, C2, C3, ... Cm)을 단어 예측부에 전달한다. 단어 예측부는 인코더의 잠재 변수 계산부가 계산한 잠재 변수(Z1, Z2, Z3, ... Zm) 및 상기 군 집 예측부에 의해 예측된 군집(C1, C2, C3, ... Cm)을 이용하여, 각각의 타깃 단어(T1, T2, T3, ... Tm)를 예측한다. 이때 각각의 타깃 단어(T1, T2, T3, ... Tm)는, 예측된 군집(C1, C2, C3, ... Cm) 내에 속하는 단어들 중에서 예측된다. 단어 예측부에 의해 예측된 타깃 단어들(T1, T2, T3, ... Tm)은 출력부에 의해 처리되어 출력 텍스트 로서 제공된다. 지금까지 도 8 및 도 9를 참조하여 본 발명의 일 실시예에 따른 텍스트 생성 장치의 세부 구성에 의해 텍스 트를 생성하는 방법을 설명하였다. 이하에서는, 도 10을 참조하여 본 발명의 몇몇 실시예들이 적용될 수 있는 예시적인 응용 분야를 설명한다. 도 10의 참조번호는, 본 발명의 몇몇 실시예에 따른 텍스트 생성 방법이 적용될 수 있는 가사 생성 소프트 웨어의 예시적인 입력과 출력을 나타낸다. 본 발명의 몇몇 실시예들은, 노래 가사의 첫 소절 또는 제목을 입력 으로 받으면 노래 가사의 나머지 부분을 자동으로 생성하는 인공지능 가사 생성 소프트웨어에 적용될 수 있다. 이 경우, 기성곡들의 가사들을 모은 텍스트 데이터를 사용하여 텍스트 생성 모델이 학습될 수 있다. 기성곡들의 가사들로 학습된 텍스트 생성 모델은, 마치 사람이 작사한 것과 같은 노래 가사를 창작해 낼 수 있다. 이때, 본 발명의 실시예들에 따라 학습 대상 가사들(말뭉치)을 균일하게 군집화함으로써, 기성곡들의 가사에 자주 사용되 는 특정 단어들로 편중되지 않은 다채로운 가사가 텍스트 생성 모델에 의해 생성될 수 있다. 도 10의 참조번호는, 본 발명의 몇몇 실시예에 따른 텍스트 생성 방법이 적용될 수 있는 가상 뉴스 기사 생성 소프트웨어의 예시적인 입력과 출력을 나타낸다. 본 발명의 몇몇 실시예들은, 뉴스 기사의 제목 또는 첫 단락을 입력으로 받으면 가상 뉴스 기사의 나머지 부분을 자동으로 생성하는 뉴스 기사 생성 소프트웨어에 적용 될 수 있다. 이 경우, 기존 뉴스 기사들을 모은 텍스트 데이터를 사용하여 텍스트 생성 모델이 학습될 수 있으 며, 텍스트 생성 모델에 의해 마치 사람이 작성한 것과 같은 가상의 뉴스 기사를 자동으로 생성할 수 있다. 도 10의 참조번호는, 본 발명의 몇몇 실시예에 따른 텍스트 생성 방법이 적용될 수 있는 무인 대화 시스템 의 예시적인 입력과 출력을 나타낸다. 본 발명의 몇몇 실시예들은, 마치 사람과 대화하는 것처럼 자연스러운 대 화를 제공할 수 있는 인공지능 스피커 또는 챗봇 등에 적용될 수 있다. 이 경우, 사람 사이의 일상적인 질의와 응답들로 구성된 말뭉치 데이터를 사용하여 텍스트 생성 모델이 학습될 수 있다. 이때, 본 발명의 실시예들에 따라 학습 대상 말뭉치를 균일하게 군집화함으로써, \"그렇습니다\" 또는 \"모르겠어요\" 등과 같은 특정 표현들에 편중되지 않은 다양성이 증대된 대화를 제공할 수 있다. 지금까지 도 10을 참조하여 본 발명의 몇몇 실시예들이 적용될 수 있는 예시적인 응용 분야를 설명하였다. 본 발명의 실시예들은 도 10에 나타낸 응용 분야 외에도 학습 데이터가 존재하는 모든 텍스트 생성"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "분야(예컨대, 문서 요약, 기계 번역 등)에 적용될 수 있다. 나아가, 본 발명의 실시예들은 단어들로 구성된 텍스트 시퀀스의 생성에 국한되지 않으며, 다양한 유형의 시퀀 스를 생성하는 방법 및 장치에 적용될 수도 있다. 다시 말해 본 발명은, 텍스트처럼 순서를 가지는 일련의 데이 터들의 모음 또는 선형적으로 배열될 수 있는 일련의 데이터들의 모음으로 표현되는 다른 유형의 시퀀스들을 생 성하기 위하여 사용될 수 있다. 당해 기술 분야의 기술자라면 본 명세서에서 설명된 텍스트 생성 방법에 관한 실시예들을 참고하여, 본 발명의 기술 사상을 다른 유형의 시퀀스 생성에 적용할 수 있을 것이다. 예를 들어, 본 발명의 실시예들을 인공 신경망을 이용한 자동 작곡 방법에 적용하여, 인공 신경망이 자동으로 생성하는 악곡의 구성과 진행에 다양성을 증진시킬 수 있다. 이 경우, 기성곡들의 코드(chord) 진행을 표현한 데이터들의 모음을 이용하여 시퀀스 생성 모델을 학습시키고, 학습된 시퀀스 생성 모델에 몇몇 코드를 입력함으 로써, 후속하는 코드들을 자동으로 생성할 수 있다. 또는, 기성곡들의 멜로디 진행을 표현한 데이터들의 모음을 이용하여 시퀀스 생성 모델을 학습시키고, 학습된 시퀀스 생성 모델에 악곡 도입부의 몇 소절의 멜로디를 입력 함으로써, 악곡의 나머지 부분의 멜로디를 자동으로 완성할 수 있다. 지금까지 도 1 내지 도 10을 참조하여, 본 발명의 몇몇 실시예들에 따른 텍스트 생성 방법 및 장치와, 그 응용 분야에 대해서 설명하였다. 이하에서는, 본 발명의 몇몇 실시예들에 따른 텍스트 생성 장치를 구현할 수 있 는 예시적인 컴퓨팅 장치에 대하여 설명하도록 한다.도 11은 본 발명의 몇몇 실시예들에 따른 텍스트 생성 장치를 구현할 수 있는 예시적인 컴퓨팅 장치 를 나타내는 하드웨어 구성도이다. 도 11에 도시된 바와 같이, 컴퓨팅 장치는 하나 이상의 프로세서, 버스, 통신 인터페이스 , 프로세서에 의하여 수행되는 컴퓨터 프로그램을 로드(load)하는 메모리와, 컴퓨터 프로그램을 저장하는 스토리지를 포함할 수 있다. 다만, 도 11에는 본 발명의 실시예와 관련 있는"}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "구성요소들만이 도시되어 있다. 따라서, 본 발명이 속한 기술분야의 통상의 기술자라면 도 11에 도시된 구성요 소들 외에 다른 범용적인 구성 요소들이 더 포함될 수 있음을 알 수 있다. 프로세서는 컴퓨팅 장치의 각 구성의 전반적인 동작을 제어한다. 프로세서는 CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit) 또는 본 발명의 기술 분야에 잘 알려진 임의의 형태의 프로세서를 포함하여 구성될 수 있다. 또한, 프로세서 는 본 발명의 실시예들에 따른 방법을 실행하기 위한 적어도 하나의 애플리케이션 또는 프로그램에 대한 연산을 수행할 수 있다. 컴퓨팅 장치는 하나 이상의 프로세서를 구비할 수 있다. 메모리는 각종 데이터, 명령 및/또는 정보를 저장한다. 메모리는 본 발명의 실시예들에 따른 텍스 트 생성 방법을 실행하기 위하여 스토리지로부터 하나 이상의 프로그램을 로드할 수 있다. 가령, 메모리에 컴퓨터 프로그램이 로드되면, 도 2에 도시된 바와 같은 모듈이 메모리 상에 구현될 수 있다. 메모리는 RAM과 같은 휘발성 메모리로 구현될 수 있을 것이나, 본 발명의 기술적 범위가 이에 한정되는 것은 아니다. 버스는 컴퓨팅 장치의 구성 요소 간 통신 기능을 제공한다. 버스는 주소 버스(Address Bus), 데이터 버스(Data Bus) 및 제어 버스(Control Bus) 등 다양한 형태의 버스로 구현될 수 있다. 통신 인터페이스는 컴퓨팅 장치의 유무선 인터넷 통신을 지원한다. 또한, 통신 인터페이스는 인터넷 통신 외의 다양한 통신 방식을 지원할 수도 있다. 이를 위해, 통신 인터페이스는 본 발명의 기술 분야에 잘 알려진 통신 모듈을 포함하여 구성될 수 있다. 몇몇 실시예들에 따르면, 통신 인터페이스는 생략될 수도 있다. 스토리지는 상기 하나 이상의 프로그램과 각종 데이터를 비임시적으로 저장할 수 있다. 가령, 컴퓨 팅 장치를 통해 텍스트 생성 장치가 구현되는 경우라면, 상기 각종 데이터는 저장부에 의해 관 리되는 데이터를 포함할 수 있다. 스토리지는 ROM(Read Only Memory), EPROM(Erasable Programmable ROM), EEPROM(Electrically Erasable Programmable ROM), 플래시 메모리 등과 같은 비휘발성 메모리, 하드 디스크, 착탈형 디스크, 또는 본 발명이 속하는 기술 분야에서 잘 알려진 임의의 형태의 컴퓨터로 읽을 수 있는 기록 매체를 포함하여 구성될 수 있다. 컴퓨터 프로그램은 메모리에 로드될 때 프로세서로 하여금 본 발명의 다양한 실시예에 따른 방법/동작을 수행하도록 하는 하나 이상의 인스트럭션들을 포함할 수 있다. 즉, 프로세서는 상기 하나 이 상의 인스트럭션들을 실행함으로써, 본 발명의 다양한 실시예에 따른 방법/동작들을 수행할 수 있다. 위와 같은 경우, 컴퓨팅 장치를 통해 본 발명의 몇몇 실시예들에 따른 텍스트 생성 장치가 구현될 수 있다. 지금까지 도 1 내지 도 11을 참조하여 본 발명의 다양한 실시예들 및 그 실시예들에 따른 효과들을 언급하였다. 본 발명의 기술적 사상에 따른 효과들은 이상에서 언급한 효과들로 제한되지 않으며, 언급되지 않은 또 다른 효 과들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다. 지금까지 도 1 내지 도 11을 참조하여 설명된 본 발명의 기술적 사상은 컴퓨터가 읽을 수 있는 매체 상에 컴퓨 터가 읽을 수 있는 코드로 구현될 수 있다. 상기 컴퓨터로 읽을 수 있는 기록 매체는, 예를 들어 이동형 기록 매체(CD, DVD, 블루레이 디스크, USB 저장 장치, 이동식 하드 디스크)이거나, 고정식 기록 매체(ROM, RAM, 컴퓨 터 구비 형 하드 디스크)일 수 있다. 상기 컴퓨터로 읽을 수 있는 기록 매체에 기록된 상기 컴퓨터 프로그램은 인터넷 등의 네트워크를 통하여 다른 컴퓨팅 장치에 전송되어 상기 다른 컴퓨팅 장치에 설치될 수 있고, 이로써 상기 다른 컴퓨팅 장치에서 사용될 수 있다. 이상에서, 본 발명의 실시예를 구성하는 모든 구성 요소들이 하나로 결합되거나 결합되어 동작하는 것으로 설명 되었다고 해서, 본 발명의 기술적 사상이 반드시 이러한 실시예에 한정되는 것은 아니다. 즉, 본 발명의 목적범위 안에서라면, 그 모든 구성요소들이 하나 이상으로 선택적으로 결합하여 동작할 수도 있다. 도면에서 동작들이 특정한 순서로 도시되어 있지만, 반드시 동작들이 도시된 특정한 순서로 또는 순차적 순서로 실행되어야만 하거나 또는 모든 도시 된 동작들이 실행되어야만 원하는 결과를 얻을 수 있는 것으로 이해되어서 는 안 된다. 특정 상황에서는, 멀티태스킹 및 병렬 처리가 유리할 수도 있다. 더욱이, 위에 설명한 실시예들에 서 다양한 구성들의 분리는 그러한 분리가 반드시 필요한 것으로 이해되어서는 안 되고, 설명된 프로그램 컴포 넌트들 및 시스템들은 일반적으로 단일 소프트웨어 제품으로 함께 통합되거나 다수의 소프트웨어 제품으로 패키 지 될 수 있음을 이해하여야 한다."}
{"patent_id": "10-2020-0140886", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "이상 첨부된 도면을 참조하여 본 발명의 실시예들을 설명하였지만, 본 발명이 속하는 기술분야에서 통상의 지식 을 가진 자는 그 기술적 사상이나 필수적인 특징을 변경하지 않고서 본 발명이 다른 구체적인 형태로도 실시될 수 있다는 것을 이해할 수 있다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적인 것이 아닌 것으로 이해해야만 한다. 본 발명의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 발명에 의해 정의되는 기술적 사상의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2020-0140886", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 텍스트 생성 장치의 입력 및 출력을 설명하기 위한 도면이다. 도 2는 본 발명의 일 실시예에 따른 텍스트 생성 장치를 나타내는 예시적인 블록도이다. 도 3은 본 발명의 일 실시예에 따라 학습 대상 말뭉치를 이용하여 텍스트 생성 모델을 구축하고 텍스트를 생성 하는 일련의 과정을 나타내는 예시적인 흐름도이다. 도 4는 본 발명의 일 실시예에 따른 텍스트 생성 모델의 학습에 사용되는 말뭉치를 군집화하는 방법을 나타내는 예시적인 흐름도이다. 도 5는 도 4를 참조하여 설명한 학습 대상 말뭉치의 군집화 방법을 설명하기 위한 도면이다. 도 6은 본 발명의 일 실시예에 따른 텍스트 생성 장치의 텍스트 생성 모델을 나타내는 예시적인 블록도이다. 도 7은 도 6을 참조하여 설명한 텍스트 생성 모델의 인코더 및 디코더를 설명하기 위한 예시적인 블록도이다. 도 8은 본 발명의 일 실시예에 따라 텍스트 생성 모델을 이용하여 텍스트를 생성하는 방법을 나타내는 예시적인 흐름도이다. 도 9는 본 발명의 일 실시예에 따른 텍스트 생성 장치의 동작과 텍스트 생성 모델 내부의 신경망 구조를 나타 내는 도면이다. 도 10은 본 발명의 다양한 실시예들이 적용될 수 있는 응용 분야를 설명하기 위한 도면이다. 도 11은 본 발명의 몇몇 실시예들에 따른 텍스트 생성 장치를 구현할 수 있는 예시적인 컴퓨팅 장치를 설명하기 위한 도면이다."}
