{"patent_id": "10-2021-0193919", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0103206", "출원번호": "10-2021-0193919", "발명의 명칭": "표현 학습을 이용하여 연속 학습을 수행하는 방법 및 그 학습 장치", "출원인": "성균관대학교산학협력단", "발명자": "우사이먼성일"}}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서를 구비하는 학습 장치가 연속 학습을 수행하는 방법에 있어서,(a) 학습 장치가 지식 증류(Knowledge Distillation)를 이용하여 미리 학습된 모델로부터 교사 네트워크 및 학생 네트워크를 생성하는 단계;(b) 상기 학습 장치가 상기 교사 네트워크 및 상기 학생 네트워크에 각각 특징 표현 값을 저장하기 위해 클래스(class)의 수만큼 복수 개의 스토리지로 구성된 표현 메모리를 생성하는 단계;(c) 상기 학습 장치가 학습에 진입하기 전에 상기 교사 네트워크를 통해 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장하는 단계;(d) 상기 학습 장치가 학습에 진입하여 상기 학생 네트워크를 통해 상기 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장하는 단계; 및(e) 상기 학습 장치가 상기 교사 네트워크 및 상기 학습 네트워크의 스토리지로부터 도출된 값들을 이용하여 표현 손실(representation loss)을 산출하는 단계;를 포함하는, 연속 학습의 수행 방법."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 (a) 단계는,미리 학습된 모델을 참조하여 큰 모델인 교사 네트워크의 지식을 상대적으로 작은 모델인 학생 네트워크에 전달하는 지식 증류에 의해 수행되며,상기 교사 네트워크 및 상기 학생 네트워크는 각각 이전 태스크에서 학습한 모델을 현재 태스크의 교사 모델 및학생 모델로 지정하는, 연속 학습의 수행 방법."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 (b) 단계는,상기 모델이 수행하고자 하는 태스크(task)에 따라 분류된 정답으로 클래스를 나누어 특징 표현 값을 저장하기위한 클래스의 수만큼 표현 메모리를 생성하되, 생성된 상기 메모리는 복수 개의 스토리지로 분할하며, 각각의스토리지마다 저장 가능한 값의 범위를 한정하여 설정되는, 연속 학습의 수행 방법."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,상기 (c) 단계는,(c1) 타겟 데이터를 입력받아 상기 교사 네트워크에 입력하는 단계;(c2) 출력된 값에 소프트맥스(softmax) 함수를 적용하여 소정 범위의 값을 결정하는 단계;(c3) 입력된 상기 타겟 데이터가 정답으로 추론된 경우 상기 교사 네트워크의 마지막 레이어에 속하는 특징 맵(feature map)을 추출하는 단계;(c4) 추출된 상기 특징 맵으로부터 맥스 풀링(max pooling)을 적용하여 풀링된 특징 맵의 평균값을 산출함으로써 특징 표현 값을 획득하는 단계; 및(c5) 결정된 상기 소정 범위의 값에 대응하는 스토리지에 획득된 상기 특징 표현 값을 저장하는 단계;를 포함하공개특허 10-2023-0103206-3-되,상기 (c) 단계는 학습에 진입하기 전에 수행되는, 연속 학습의 수행 방법."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서,상기 (c3) 단계는,입력된 상기 타겟 데이터가 상기 교사 네트워크에 학습시켰던 도메인(domain)과 상이하더라도 정답으로 예측한데이터 내에 소스(source) 도메인이 가졌던 특징과 유사한 부분이 존재하는 성질을 이용하는, 연속 학습의 수행방법."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 4 항에 있어서,(c6) 입력된 모든 타겟 데이터를 순회하였다면, 상기 스토리지에 저장된 특징 표현 값의 평균값을 각각 산출하여 상기 교사 네트워크의 스토리지를 모두 갱신하는 단계;를 더 포함하는, 연속 학습의 수행 방법."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항에 있어서,상기 (d) 단계는,(d1) 상기 타겟 데이터를 입력받아 상기 학생 네트워크에 입력하는 단계;(d2) 출력된 값에 소프트맥스 함수를 적용하여 소정 범위의 값을 결정하는 단계;(d3) 입력된 상기 타겟 데이터가 정답으로 추론된 경우 상기 학생 네트워크의 마지막 레이어에 속하는 특징 맵을 추출하는 단계;(d4) 추출된 상기 특징 맵으로부터 특징 표현 값을 획득하는 단계; 및(d5) 결정된 상기 소정 범위의 값에 대응하는 스토리지에 획득된 상기 특징 표현 값을 저장하는 단계;를 포함하되,상기 (d) 단계는 학습에 진입하여 수행되는, 연속 학습의 수행 방법."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 4 항에 있어서,(d6) 입력된 모든 타겟 데이터를 순회하였다면, 상기 스토리지에 저장된 특징 표현 값의 평균값을 각각 산출하여 상기 학생 네트워크의 스토리지를 모두 갱신하는 단계;를 더 포함하는, 연속 학습의 수행 방법."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항에 있어서,상기 (e) 단계는,상기 교사 네트워크의 스토리지에 저장된 평균값들 및 상기 학습 네트워크의 스토리지에 저장된 평균값들의 오차 평균(Mean Square)을 수행하여 표현 손실을 산출하는, 연속 학습의 수행 방법."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1 항 내지 제 9 항 중에 어느 한 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을수 있는 기록매체."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "적어도 하나 이상의 태스크 및 상기 태스크에 따른 타겟 데이터를 입력받는 입력부;공개특허 10-2023-0103206-4-특징 표현 값을 저장하기 위한 표현 메모리를 구성하는 저장부; 및상기 표현 메모리를 이용하여 연속 학습을 수행하는 프로그램을 실행하는 처리부;를 포함하고,상기 처리부에 의해 실행되는 프로그램은,지식 증류(Knowledge Distillation)를 이용하여 미리 학습된 모델로부터 교사 네트워크 및 학생 네트워크를 생성하고, 상기 교사 네트워크 및 상기 학생 네트워크에 각각 특징 표현 값을 저장하기 위해 클래스(class)의 수만큼 복수 개의 스토리지로 구성된 표현 메모리를 생성하고, 학습에 진입하기 전에 상기 교사 네트워크를 통해타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장하고, 학습에 진입하여 상기 학생 네트워크를 통해 상기 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장하며, 상기 교사 네트워크 및 상기 학습 네트워크의 스토리지로부터 도출된 값들을 이용하여 표현 손실(representation loss)을 산출하는 명령을 포함하는, 학습 장치."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서,상기 처리부에 의해 실행되는 프로그램은,미리 학습된 모델을 참조하여 큰 모델인 교사 네트워크의 지식을 상대적으로 작은 모델인 학생 네트워크에 전달하는 지식 증류에 의해 수행되며,상기 교사 네트워크 및 상기 학생 네트워크는 각각 이전 태스크에서 학습한 모델을 현재 태스크의 교사 모델 및학생 모델로 지정하는, 학습 장치."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 11 항에 있어서,상기 처리부에 의해 실행되는 프로그램은,상기 모델이 수행하고자 하는 태스크(task)에 따라 분류된 정답으로 클래스를 나누어 특징 표현 값을 저장하기위한 클래스의 수만큼 표현 메모리를 생성하되, 생성된 상기 메모리는 복수 개의 스토리지로 분할하며, 각각의스토리지마다 저장 가능한 값의 범위를 한정하여 설정되는, 학습 장치."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 11 항에 있어서,상기 처리부에 의해 실행되는 프로그램은,타겟 데이터를 입력받아 상기 교사 네트워크에 입력하고, 출력된 값에 소프트맥스(softmax) 함수를 적용하여 소정 범위의 값을 결정하고, 입력된 상기 타겟 데이터가 정답으로 추론된 경우 상기 교사 네트워크의 마지막 레이어에 속하는 특징 맵(feature map)을 추출하고, 추출된 상기 특징 맵으로부터 맥스 풀링(max pooling)을 적용하여 풀링된 특징 맵의 평균값을 산출함으로써 특징 표현 값을 획득하며, 결정된 상기 소정 범위의 값에 대응하는스토리지에 획득된 상기 특징 표현 값을 저장하는 명령을 포함하되,상기 교사 네트워크를 통한 스토리지 저장 과정은 학습에 진입하기 전에 수행되는, 학습 장치."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서,상기 처리부에 의해 실행되는 프로그램은,입력된 상기 타겟 데이터가 상기 교사 네트워크에 학습시켰던 도메인(domain)과 상이하더라도 정답으로 예측한데이터 내에 소스(source) 도메인이 가졌던 특징과 유사한 부분이 존재하는 성질을 이용하여 실행되는, 학습 장치."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "공개특허 10-2023-0103206-5-제 14 항에 있어서,상기 처리부에 의해 실행되는 프로그램은,입력된 모든 타겟 데이터를 순회하였다면, 상기 스토리지에 저장된 특징 표현 값의 평균값을 각각 산출하여 상기 교사 네트워크의 스토리지를 모두 갱신하는 명령을 더 포함하는, 학습 장치."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 11 항에 있어서,상기 처리부에 의해 실행되는 프로그램은,상기 타겟 데이터를 입력받아 상기 학생 네트워크에 입력하고, 출력된 값에 소프트맥스 함수를 적용하여 소정범위의 값을 결정하고, 입력된 상기 타겟 데이터가 정답으로 추론된 경우 상기 학생 네트워크의 마지막 레이어에 속하는 특징 맵을 추출하고, 추출된 상기 특징 맵으로부터 특징 표현 값을 획득하며, 결정된 상기 소정 범위의 값에 대응하는 스토리지에 획득된 상기 특징 표현 값을 저장하는 명령을 포함하되,상기 학생 네트워크를 통한 스토리지 저장 과정은 학습에 진입하여 수행되는, 학습 장치."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 14 항에 있어서,상기 처리부에 의해 실행되는 프로그램은,입력된 모든 타겟 데이터를 순회하였다면, 상기 스토리지에 저장된 특징 표현 값의 평균값을 각각 산출하여 상기 학생 네트워크의 스토리지를 모두 갱신하는 명령을 더 포함하는, 학습 장치."}
{"patent_id": "10-2021-0193919", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 11 항에 있어서,상기 처리부에 의해 실행되는 프로그램은,상기 교사 네트워크의 스토리지에 저장된 평균값들 및 상기 학습 네트워크의 스토리지에 저장된 평균값들의 오차 평균(Mean Square)을 수행하여 표현 손실을 산출하는, 학습 장치."}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 머신 러닝을 이용한 연속 학습 기술에 관한 것으로, 학습 장치가 연속 학습을 수행하는 방법은, 지식 증류(Knowledge Distillation)를 이용하여 미리 학습된 모델로부터 교사 네트워크 및 학생 네트워크를 생성하고, 교사 네트워크 및 학생 네트워크에 각각 특징 표현 값을 저장하기 위해 클래스(class)의 수만큼 복수 개의 스토 리지로 구성된 표현 메모리를 생성하고, 학습에 진입하기 전에 교사 네트워크를 통해 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장하고, 학습에 진입하여 학생 네트워크를 통해 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장하며, 교사 네트워크 및 학습 네트워크의 스토리지로부터 도 출된 값들을 이용하여 표현 손실(representation loss)을 산출한다."}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 머신 러닝을 이용한 연속 학습 기술에 관한 것으로, 특히 지식 증류(Knowledge Distillation) 기법에 따른 교사 네트워크 및 학생 네트워크를 이용하여 연속 학습 과정에서 새로운 태스크에 대한 연속 학습을 수행 하는 방법 및 그 방법을 이용한 학습 장치에 관한 것이다."}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 지능(Artificial Intelligence, AI) 연구자들은 인간의 인지 메커니즘을 모방하여 AI의 성능을 높이고자 노력해왔다. 그런 노력 중 하나가 전이 학습(Transfer Learning)이다. 인간은 과거에 학습했던 내용을 기반으로 새로운 내용을 빠르게 습득한다. AI도 과거에 학습했던 내용을 기반으로 비슷한 태스크(Task)를 잘 학습할 수 있을지에 관한 아이디어에서 출발한 전이 학습은 잘 학습된 모델의 사전훈련 가중치(pre-trained weights)를 새 로운 모델의 학습에 활용함으로써 학습 속도가 빠르고 성능이 우수한 모델을 만드는 기법이다. 이러한 머신 러닝 모델은 학습 프로세스에 대한 최종 결과를 강조하지만 종종 진화하는 작업 및 순차적 학습에 대한 견고성 및 적응성과 같은 인간 학습의 핵심 기능을 무시하곤 한다. 다른 한편으로, 이러한 견고함은 많은 양의 뒤섞이고 균형이 잡힌 균일한 데이터를 신중하게 제공할 때 일반적으로 탁월한 경향이 있는 가장 효율적인 딥 러닝 모델과 극명한 대조를 이룬다. 이러한 모델은 약간 다른 데이터 분포에 직면했을 때 성능이 저하될 뿐 만 아니라 이전에 학습된 작업에서 실패하거나 급격한 성능 저하를 경험한다. 즉, 새롭게 학습된 모델이 과거 학습 내용을 잊어버리는 치명적인 문제가 발견되었다.현재 인공 신경망(Neural Network)은 단일 태스크에 대해서는 뛰어난 성능을 보이지만, 다른 종류의 태스크를 학습하면 이전에 학습했던 태스크에 대한 성능이 현저하게 떨어지는 문제가 나타났는데, 이러한 현상을 파괴적 망각(Catastrophic forgetting)이라고 한다. 파괴적 망각에서는 이전 학습 데이터셋(dataset)과 새로운 학습 데 이터셋 사이에 연관성이 있더라도 이전 데이터셋에 대한 정보를 대량으로 손실한다. 이 현상을 명확하게 관찰할 수 있는 영역 중 하나는 가짜 멀티미디어 탐지, 특히 딥페이크 비디오 및 GAN 생성 이미지 탐지의 경우이다. 최근 인공지능(AI) 시스템이 발전한 이러한 형태의 합성 멀티미디어는 가짜 뉴스와 정보를 만들기 위한 소셜 미 디어와 온라인 포럼에서 더욱 널리 퍼지고 있다. 딥 러닝 기술의 최근 발전은 실제 이미지 및 비디오와 놀라울 정도로 유사한 합성 이미지 및 비디오를 생성하는 데 큰 도움이 되었다. 또한 FaceApp, FakeApp, ZAO 등과 같은 수많은 가짜 이미지 생성 도구도 사용 가능하여 상황을 악화시키고 있다. 딥페이크가 멀티미디어 기술에 심각한 피해를 줄 수 있다는 것은 비밀이 아니다. 가짜 멀티미디어는 일반적으로 딥페이크 비디오와 GAN 생성 합성 이 미지의 두 가지 형태로 인터넷에 존재한다. 한편, 최근 몇 년간 많은 가짜 미디어 탐지 방법이 제안되어 최첨단 성능을 달성하고 있다. 그러나, 이러한 가짜 미디어 탐지 방법들은 훈련 세트와 다른 데이터 분포로 평가할 때 동일한 견고성 및 일반화 문제를 겪는다. 따라서, 이러한 전이 학습 내지 연속 학습 과정에서 지식 망각 없이 사전 학습된 딥 러닝 모델의 탐지 성능을 유지하면서도, 새롭게 학습할 타겟 데이터에 대한 탐지 성능을 향상시킬 수 있는 새로운 기술이 요구된다. 선행기술문헌 비특허문헌 (비특허문헌 0001) Shruti Agarwal, Hany Farid, Tarek El-Gaaly, and Ser-Nam Lim. 2020. Detecting deep- fake videos from appearance and behavior. In 2020 IEEE International Workshop on Information Forensics and Security (WIFS). IEEE, 1-6."}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 기술적 과제는, 개별 모델을 학습시키는데 따른 다양한 제약에 대응하여 시도된 전 이 학습 과정에서 사전에 학습된 데이터들을 탐지하지 못하는 지식 망각(knowledge forgetting) 현상이 발생하 는 한계를 극복하고, 딥 러닝 네트워크의 탐지 성능을 유지하기 위해 대량이 소스 데이터(source data)가 요구 되는 약점을 극복하며, 새로운 도메인(domain)에 적용되는 태스크에 대해 학습 성능이 부족한 문제를 해소하고 자 한다."}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 기술적 과제를 해결하기 위하여, 본 발명의 일 실시예에 따른 적어도 하나의 프로세서를 구비하는 학습 장 치가 연속 학습을 수행하는 방법은, (a) 학습 장치가 지식 증류(Knowledge Distillation)를 이용하여 미리 학습 된 모델로부터 교사 네트워크 및 학생 네트워크를 생성하는 단계; (b) 상기 학습 장치가 상기 교사 네트워크 및 상기 학생 네트워크에 각각 특징 표현 값을 저장하기 위해 클래스(class)의 수만큼 복수 개의 스토리지로 구성 된 표현 메모리를 생성하는 단계; (c) 상기 학습 장치가 학습에 진입하기 전에 상기 교사 네트워크를 통해 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장하는 단계; (d) 상기 학습 장치가 학습에 진 입하여 상기 학생 네트워크를 통해 상기 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저 장하는 단계; 및 (e) 상기 학습 장치가 상기 교사 네트워크 및 상기 학습 네트워크의 스토리지로부터 도출된 값 들을 이용하여 표현 손실(representation loss)을 산출하는 단계;를 포함한다. 일 실시예에 따른 연속 학습의 수행 방법에서, 교사 네트워크 및 학생 네트워크를 생성하는 상기 (a) 단계는, 미리 학습된 모델을 참조하여 큰 모델인 교사 네트워크의 지식을 상대적으로 작은 모델인 학생 네트워크에 전달 하는 지식 증류에 의해 수행되며, 상기 교사 네트워크 및 상기 학생 네트워크는 각각 이전 태스크에서 학습한 모델을 현재 태스크의 교사 모델 및 학생 모델로 지정할 수 있다. 일 실시예에 따른 연속 학습의 수행 방법에서, 표현 메모리를 생성하는 상기 (b) 단계는, 상기 모델이 수행하고 자 하는 태스크(task)에 따라 분류된 정답으로 클래스를 나누어 특징 표현 값을 저장하기 위한 클래스의 수만큼 표현 메모리를 생성하되, 생성된 상기 메모리는 복수 개의 스토리지로 분할하며, 각각의 스토리지마다 저장 가 능한 값의 범위를 한정하여 설정될 수 있다. 일 실시예에 따른 연속 학습의 수행 방법에서, 교사 네트워크를 통해 특징 표현 값을 스토리지에 저장하는 상기 (c) 단계는, (c1) 타겟 데이터를 입력받아 상기 교사 네트워크에 입력하는 단계; (c2) 출력된 값에 소프트맥스 (softmax) 함수를 적용하여 소정 범위의 값을 결정하는 단계; (c3) 입력된 상기 타겟 데이터가 정답으로 추론된 경우 상기 교사 네트워크의 마지막 레이어에 속하는 특징 맵(feature map)을 추출하는 단계; (c4) 추출된 상기 특징 맵으로부터 맥스 풀링(max pooling)을 적용하여 풀링된 특징 맵의 평균값을 산출함으로써 특징 표현 값을 획득하는 단계; 및 (c5) 결정된 상기 소정 범위의 값에 대응하는 스토리지에 획득된 상기 특징 표현 값을 저장 하는 단계;를 포함하되, 상기 (c) 단계는 학습에 진입하기 전에 수행될 수 있다. 일 실시예에 따른 연속 학습의 수행 방법에서, 학생 네트워크를 통해 특징 표현 값을 스토리지에 저장하는 상기 (d) 단계는, (d1) 상기 타겟 데이터를 입력받아 상기 학생 네트워크에 입력하는 단계; (d2) 출력된 값에 소프트 맥스 함수를 적용하여 소정 범위의 값을 결정하는 단계; (d3) 입력된 상기 타겟 데이터가 정답으로 추론된 경우 상기 학생 네트워크의 마지막 레이어에 속하는 특징 맵을 추출하는 단계; (d4) 추출된 상기 특징 맵으로부터 특 징 표현 값을 획득하는 단계; 및 (d5) 결정된 상기 소정 범위의 값에 대응하는 스토리지에 획득된 상기 특징 표 현 값을 저장하는 단계;를 포함하되, 상기 (d) 단계는 학습에 진입하여 수행될 수 있다. 일 실시예에 따른 연속 학습의 수행 방법에서, 표현 손실을 산출하는 상기 (e) 단계는, 상기 교사 네트워크의 스토리지에 저장된 평균값들 및 상기 학습 네트워크의 스토리지에 저장된 평균값들의 오차 평균(Mean Square)을 수행하여 표현 손실을 산출할 수 있다. 나아가, 이하에서는 상기 기재된 연속 학습의 수행 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨 터로 읽을 수 있는 기록매체를 제공한다. 상기 기술적 과제를 해결하기 위하여, 본 발명의 일 실시예에 따른 학습 장치는, 적어도 하나 이상의 태스크 및 상기 태스크에 따른 타겟 데이터를 입력받는 입력부; 특징 표현 값을 저장하기 위한 표현 메모리를 구성하는 저 장부; 및 상기 표현 메모리를 이용하여 연속 학습을 수행하는 프로그램을 실행하는 처리부;를 포함하고, 상기 처리부에 의해 실행되는 프로그램은, 지식 증류(Knowledge Distillation)를 이용하여 미리 학습된 모델로부터 교사 네트워크 및 학생 네트워크를 생성하고, 상기 교사 네트워크 및 상기 학생 네트워크에 각각 특징 표현 값 을 저장하기 위해 클래스(class)의 수만큼 복수 개의 스토리지로 구성된 표현 메모리를 생성하고, 학습에 진입 하기 전에 상기 교사 네트워크를 통해 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장 하고, 학습에 진입하여 상기 학생 네트워크를 통해 상기 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장하며, 상기 교사 네트워크 및 상기 학습 네트워크의 스토리지로부터 도출된 값들을 이용하여 표 현 손실(representation loss)을 산출하는 명령을 포함한다. 일 실시예에 따른 학습 장치에서, 상기 처리부에 의해 실행되는 프로그램은, 미리 학습된 모델을 참조하여 큰 모델인 교사 네트워크의 지식을 상대적으로 작은 모델인 학생 네트워크에 전달하는 지식 증류에 의해 수행되며, 상기 교사 네트워크 및 상기 학생 네트워크는 각각 이전 태스크에서 학습한 모델을 현재 태스크의 교사 모델 및 학생 모델로 지정할 수 있다. 일 실시예에 따른 학습 장치에서, 상기 처리부에 의해 실행되는 프로그램은, 상기 모델이 수행하고자 하는 태스 크(task)에 따라 분류된 정답으로 클래스를 나누어 특징 표현 값을 저장하기 위한 클래스의 수만큼 표현 메모리 를 생성하되, 생성된 상기 메모리는 복수 개의 스토리지로 분할하며, 각각의 스토리지마다 저장 가능한 값의 범 위를 한정하여 설정될 수 있다. 일 실시예에 따른 학습 장치에서, 상기 처리부에 의해 실행되는 프로그램은, 타겟 데이터를 입력받아 상기 교사 네트워크에 입력하고, 출력된 값에 소프트맥스(softmax) 함수를 적용하여 소정 범위의 값을 결정하고, 입력된 상기 타겟 데이터가 정답으로 추론된 경우 상기 교사 네트워크의 마지막 레이어에 속하는 특징 맵(feature ma p)을 추출하고, 추출된 상기 특징 맵으로부터 맥스 풀링(max pooling)을 적용하여 풀링된 특징 맵의 평균값을 산출함으로써 특징 표현 값을 획득하며, 결정된 상기 소정 범위의 값에 대응하는 스토리지에 획득된 상기 특징 표현 값을 저장하는 명령을 포함하되, 상기 교사 네트워크를 통한 스토리지 저장 과정은 학습에 진입하기 전에 수행될 수 있다. 일 실시예에 따른 학습 장치에서, 상기 처리부에 의해 실행되는 프로그램은, 상기 타겟 데이터를 입력받아 상기 학생 네트워크에 입력하고, 출력된 값에 소프트맥스 함수를 적용하여 소정 범위의 값을 결정하고, 입력된 상기 타겟 데이터가 정답으로 추론된 경우 상기 학생 네트워크의 마지막 레이어에 속하는 특징 맵을 추출하고, 추출 된 상기 특징 맵으로부터 특징 표현 값을 획득하며, 결정된 상기 소정 범위의 값에 대응하는 스토리지에 획득된 상기 특징 표현 값을 저장하는 명령을 포함하되, 상기 학생 네트워크를 통한 스토리지 저장 과정은 학습에 진입 하여 수행될 수 있다. 일 실시예에 따른 학습 장치에서, 상기 처리부에 의해 실행되는 프로그램은, 상기 교사 네트워크의 스토리지에 저장된 평균값들 및 상기 학습 네트워크의 스토리지에 저장된 평균값들의 오차 평균(Mean Square)을 수행하여 표현 손실을 산출할 수 있다."}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들은, 지식 증류 기법의 교사-학생 네트워크 구조와 표현 학습을 이용하여 전이 학습 내지 연 속 학습 시 표현 메모리를 통해 소스 데이터 없이 기존 모델의 성능을 최대한 유지하면서도 지식 망각 현상을 방지할 수 있고, 다양한 타겟 도메인 학습에 있어서 효과적인 성능 향상이 가능하다."}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "사전 학습된 딥 러닝 모델을 통해 다양한 도메인에 속하는 태스크를 수행함에 있어서, 영역 이동으로 인한 치명 적인 망각(지식 망각) 문제가 주로 발생함을 소개한 바 있다. 특히, 파괴적 망각은 딥 러닝 모델이 새로운 작업 을 학습할 때 이전에 학습한 지식을 완전히, 부분적으로 또는 갑자기 잊어버리는 경향이 있는 현상이다. 예를 들어, 하나의 딥페이크 데이터셋으로 훈련된 분류기의 성능은 일반적으로 다른 딥페이크 데이터셋으로 테스트할 때 저하된다. 따라서 정보를 전달하거나 새로운 작업을 학습하는 동안 치명적인 망각을 최소화해야 한다. 한편, 지식을 이전하는 동안 지식을 유지하기 위해 소스 데이터를 재사용하여 모델의 데이터 분포 이동을 완화하는 접 근 방식이 활용될 수 있다. 최근접 이웃 방식에 기반한 특징 분류기를 사용함으로써 분포 이동 문제를 완화할 수 있다. 그러나 이 경우 지식을 전달하기 위해 소스 데이터를 저장하기 때문에 메모리 자원의 한계를 겪는다. 또한, 치명적인 망각을 극복하기 위해 전이 학습 동안 소스 도메인에서 몇 가지 데이터 샘플을 사용할 수 있다. 그러나 실제로 대부분의 사전 훈련된 모델의 경우 원본 도메인 데이터를 사용할 수 없거나, 원본 도메인 데이터 를 유지하면 개인 정보 보호 문제가 발생할 수 있다. 따라서 실제 시나리오에서 최대한의 적용 가능성을 장려하 기 위해 대상 도메인의 데이터만 사용하고 지식 증류(knowledge distillation, KD)를 적용하여 사전 훈련된 모 델(교사)로부터 학습할 수 있다. 이러한 지식 증류는 연속(continual) 및 평생 학습(lifelong learning) 시나리 오에서 효과적으로 활용될 수 있다. 이상과 같은 고려 요소들로부터 안출된 본 발명의 실시예들은 표현 학습(representation learning, RL)과 지식 증류를 사용하는 연속 학습(continual learning, CL) 기반의 접근 방식을 제안하며, 본 명세서를 통해 CoReD(Continual Representation using Distillation)라고 명명하였다. 본 발명의 실시예들은 CoReD 방식을 사 용하여 새로운 작업을 지속적으로 학습하되, 표현 학습 및 지식 증류를 결합한 연속 학습을 수행하여 파괴적 망 각을 크게 개선한다. 구체적으로, 본 발명의 실시예들은 학습된 작업에서 치명적인 망각을 최소화하기 위해 학 생 손실, 증류 손실 및 표현 손실을 사용하여 총 손실을 구성함으로써 다양한 딥페이크를 한 번에 탐지하기 위해 순차적으로 새로운 태스크를 효과적으로 학습할 수 있다. 한편, 평생 학습(life-long learning)이라고도 하는 연속 학습(Continual Learning, CL)은 지속적이고 적응적 으로 학습한다는 개념을 기반으로 한다. 특히, 연속 학습은 데이터의 무한한 흐름에서 학습하는 일종의 일반적 인 온라인 학습 프레임워크이다. 특히, 치명적인 망각 문제를 해결하고 동적으로 변화하는 작업에 적응하기 위 해 여러 CL 방법이 도입되었다. 또한 CL 시스템은 각 교육 단계에서 이전 데이터를 모두 다시 방문하지 않고도 전체 데이터셋에 적응하고 잘 수행하는 기능을 보여주었다. 이러한 CL의 장점은 일반화 및 새로운 작업 학습을 위한 딥 러닝 및 머신 러닝에서 만연한 주요 한계를 해결할 수 있다. 예를 들어, 시간이 지남에 따라 훈련된 모 델은 일반적으로 새로운 데이터셋의 크기가 엄청나게 증가하기 때문에 공변량(covariates)과 지식 이동 (knowledge shifts)을 겪는다. 이는 파괴적 망각(catastrophic forgetting)이라고도 한다. 이에 대해 이전 작 업에 대한 가중치의 중요성에 따라 가중치의 가소성을 선택적으로 억제하여 신경망에서 치명적인 망각을 완화하 기 위해 탄력적 가중치 통합(elastic weight consolidation, EWC)라는 제약 기반 접근 방식을 활용할 수 있다. 그러나, 이러한 접근 방식은 네트워크 크기가 작업 수와 관련하여 2차적으로 확장되기 때문에 확장성이 부족함 을 보여준다. 본 발명의 실시예들에서는 제약이 없는 대상 데이터와 원본 데이터의 특징을 참조하여 지식의 망 각을 방지하는 방법을 제안한다. 또한, 표현 학습(representation learning, RL)은 머신 러닝 작업을 더 쉽게 수행할 수 있도록 데이터에서 특징 을 변환하거나 추출하여 입력 데이터의 기본 표현을 학습하는 접근 방식이다. 최근 연구는 도메인 적응 작업에 서 특성 전달 가능성을 개선하기 위해 심층 적응 네트워크를 사용하여 전달 가능한 표현 학습을 탐구하였다. 이 러한 접근 방식은 모든 작업별 계층의 심층 기능을 커널 힐베르트 공간(kernel Hilbert spaces, RKHS)에 포함하 여 미니맥스(minimax) 게임을 형성하는 최적의 도메인 분포와 일치시킨다. 그러나, 이는 지속적인 학습 설정을 위해 설계된 것이 아니다. 또한, 표현 강도 변화에 강력한 새로운 시공간 특징 표현 학습을 고려할 수 있으나, 이러한 접근 방식은 일반화 성능을 평가하기에 충분하지 않을 수 있는 두 개의 다른 데이터셋만 고려했다는 점 에서 적합하지 않다. 선구적인 지식 증류(Knowledge distillation, KD)는 큰(교사) 모델의 지식을 압축하여 작은(학생) 모델로 전달 하기 위해 최초로 제안되었다. 지식 증류 훈련 과정의 본질은 학생 모델이 교사 모델의 능력을 효과적으로 모방 하는 것이다. 연속 학습 태스크에서 전이 학습 동안 지식 증류를 활용하여 파괴적 망각을 개선하기 위해 학습 없는 망각 프레임워크가 제안되었다. 또한, 클래스 증분 학습에서 치명적인 망각을 해결하기 위해 리허설 원칙 과 지식 증류 손실이 제안되었다. 이러한 작업은 원본 작업을 완전히 잊어버리는 것을 방지하기 위해 원본 작업 에 모형을 저장한다. 그러나, 복잡한 입력의 경우 이 접근 방식은 일반적으로 소스 도메인의 기능을 저장하기 위해 매우 큰 메모리 저장소가 필요한다. 이러한 큰 공간 요구 사항을 완화하기 위해 제안된 본 발명의 실시예 들은 지식 증류를 활용한 연속 및 표현 학습 기반의 CoReD를 통해 새로운 작업 학습 중에 소스 예제를 저장하거 나 사용할 필요가 없도록 설계되었다. 증류(Distillation) 및 회고(Retrospection)를 통해 보존과 적응 사이의 더 나은 균형을 모색함으로써 다중 작 업 평생 학습에 활용할 수 있다. 이에 관한 컨벌루션 신경망(Convolutional Neural Networks, CNN) 기반 접근 방식은 새로운 작업에 대한 학습을 도울 뿐만 아니라 이전 작업의 성능을 보존한다. 특히 회고(Retrospection) 는 오래된 작업에 대한 데이터의 작은 하위 집합을 캐시하도록 설계되어 특히 서로 다른 배포판에서 가져온 긴 작업 시퀀스에서 성능 보존에 큰 도움이 되는 것으로 판명되었다. 본 발명의 실시예들은 일부 유사한 접근 방식 을 채택하되, 지속적인 표현에 중점을 두었다. 앞서 예시한 바와 같이, 많은 새로운 딥페이크 비디오(또는 GAN 이미지) 생성 방법이 도입됨에 따라 모든 가짜 이미지를 탐지하는 것은 점점 더 어렵고 시간이 많이 소요되는 일이 되었다. 지속적인 학습 기반 솔루션은 특히 딥페이크 비디오(또는 GAN 이미지)의 경우와 같이 데이터 분포가 서로 다른 생성 방법 간에 중복되는 경우에 유 용할 수 있다. 따라서, 본 발명의 실시예들은 교사-학생 모델 설정에서 연속 학습을 사용하여 다양한 생성 방법 에서 가짜 미디어를 효과적으로 탐지하기 위한 기술적 수단을 제안한다. 이를 통해, 별도의 사전 데이터 없이도 기존 모델의 성능을 최대한 유지하면서 다른 도메인의 생성 기법이 적용된 타겟 데이터를 학습시키고자 하였다. 이하에서는 도면을 참조하여 본 발명의 실시예들을 구체적으로 설명하도록 한다. 다만, 하기의 설명 및 첨부된 도면에서 본 발명의 요지를 흐릴 수 있는 공지 기능 또는 구성에 대한 상세한 설명은 생략한다. 덧붙여, 명세서 전체에서, 어떤 구성 요소를 '포함'한다는 것은, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것 이 아니라, 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 제 1, 제 2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로 사용될 수 있다. 예를 들어, 본 발명의 권리 범위로부터 이탈되지 않은 채 제 1 구성 요소는 제 2 구성 요소 로 명명될 수 있고, 유사하게 제 2 구성 요소도 제 1 구성 요소로 명명될 수 있다. 본 발명에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"구비하다\" 등의 용어는 설시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또 는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 특별히 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본"}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "발명이 속하는 기술분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미이다. 일반적 으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미인 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해 석되지 않는다. 도 1은 지식 증류(Knowledge Distillation) 기법을 설명하기 위한 도면이다. 지식 증류의 목적은 미리 잘 학습된 큰 네트워크(Teacher network)의 지식을 실제로 사용하고자 하는 작은 네트 워크(Student network)에게 전달하는 것이다. 딥러닝 모델은 보편적으로 넓고 깊어서 파라미터 수가 많고 연산 량이 많으면 특징(feature) 추출이 더 잘 수행되고, 그에 따라 모델의 목적인 분류나 객체 탐지 등의 성능 또한 향상될 것으로 기대된다. 그러나, 작은 모델로 더 큰 모델만큼의 성능을 얻을 수 있다면, 컴퓨팅 자원(GPU 내지 CPU), 에너지(배터리 등), 메모리 측면에서 더 나은 효율을 달성할 수 있다. 지식 증류는 작은 네트워크도 큰 네트워크와 비슷한 성능을 낼 수 있도록, 학습 과정에서 교사 네트워크의 지식을 학생 네트워크에게 전달하여 학생 네트워크의 성능을 향상시킬 수 있도록 설계되었다. 도 1에 예시된 네트워크 구조를 참조하면, 교사 모델과 학생 모델을 통해 각각 입력에 대한 학습이 수행되고, 이로부터 출력되는 레이블 내지 예측 값으로부터 증류 손실(distillation loss) 및 학생 손실 (student loss)를 산출한다. 여기서, 학생 손실은 분류 성능에 대한 손실로 정답(ground truth)과 학생의 분류 결과의 차이를 크로스 엔트로피 손실(cross entropy loss)로 계산할 수 있다. 또한, 증류 손실 은 교사 네트워크와 학생 네트워크의 분류 결과의 차이를 손실에 포함시키는 것이다. 각각의 출 력을 소프트맥스(softmax)로 변환한 값의 차이를 크로스 엔트로피 손실로 계산할 수 있다. 이러한 두 가지 손실 (150, 170)로부터 총 손실 함수를 구성할 수 있다. 그러나, 앞서 소개한 바와 같이, 이러한 손실 함수에는 본 발명의 실시예들이 제안하고 있는 표현 손실 (representation loss)이 고려되어 있지 않다. 따라서, 이하에서는 표현 손실을 표현하고 기록하기 위한 기술적 수단을 제안한다. 도 2는 본 발명의 일 실시예에 따른 표현 학습을 이용하여 연속 학습을 수행하는 방법을 도시한 흐름도이다. 이 러한 연속 학습 과정은, 이하에서 기술되는 일련의 과정을 처리하는 명령을 포함하는 프로그램으로 구현될 수 있으며, 이러한 프로그램을 실행하는 적어도 하나의 프로세서를 구비하는 학습 장치에 의해 수행될 수 있다. S210 단계에서, 학습 장치는 지식 증류(Knowledge Distillation)를 이용하여 미리 학습된 모델로부터 교사 네트 워크 및 학생 네트워크를 생성한다. 이 과정은, 미리 학습된 모델을 참조하여 큰 모델인 교사 네트워크의 지식 을 상대적으로 작은 모델인 학생 네트워크에 전달하는 지식 증류에 의해 수행되며, 상기 교사 네트워크 및 상기 학생 네트워크는 각각 이전 태스크에서 학습한 모델을 현재 태스크의 교사 모델 및 학생 모델로 지정할 수 있다. S230 단계에서, 상기 학습 장치는 상기 교사 네트워크 및 상기 학생 네트워크에 각각 특징 표현 값을 저장하기 위해 클래스(class)의 수만큼 복수 개의 스토리지로 구성된 표현 메모리를 생성한다. 이 과정에서, 상기 모델이 수행하고자 하는 태스크(task)에 따라 분류된 정답으로 클래스를 나누어 특징 표현 값을 저장하기 위한 클래스 의 수만큼 표현 메모리를 생성하되, 생성된 상기 메모리는 복수 개의 스토리지로 분할하며, 각각의 스토리지마 다 저장 가능한 값의 범위를 한정하여 설정될 수 있다. 예를 들어, 가짜 또는 진짜 비디오 탐지를 하는 이진 분류 태스크의 경우, '가짜(fake)' 혹은 '진짜(real)'가 정답이 된다. 정답으로 분류된 데이터를 통해서 특징 표현 값을 획득하고, 표현 메모리에 저장하는데, 이때 클 래스별로 생성된 메모리에 저장하면 학습 과정에서 각 클래스끼리 정교하게 학습할 수 있다. 또한, 생성된 메모리는 여러 개의 스토리지로 분할되며, 스토리지마다 저장 가능한 값의 범위를 한정한다. 예를 들어, 0.5 값을 기준으로 0.1 단위 구간으로 5개의 스토리지로 나눌 수 있다. 따라서, 스토리지마다 {[0.5, 0.6),[0.6, 0.7),[0.7, 0.8),[0.8, 0.9),[0.9, 1.0]} 범위 값을 가질 수 있다. S250 단계에서, 상기 학습 장치는 학습에 진입하기 전에 상기 교사 네트워크를 통해 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장한다. 학습을 진행하기 전에 사전 학습된 딥러닝 모델로 교사 네 트워크 및 학생 네트워크를 생성하였고, 교사 네트워크의 표현 메모리를 생성하였으므로, 타겟 데이터를 입력시 켜 교사 특징 표현 값을 획득하고 적절한 스토리지에 값을 저장한다. S270 단계에서, 상기 학습 장치는 학습에 진입하여 상기 학생 네트워크를 통해 상기 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장한다. 학습에 진입하게 되면, S250 단계와 마찬가지로 타겟 데이 터를 학생 네트워크에 입력해서 특징 표현 값을 획득하고, 적절한 스토리지에 저장한다. S290 단계에서, 상기 학습 장치는 상기 교사 네트워크 및 상기 학습 네트워크의 스토리지로부터 도출된 값들을 이용하여 표현 손실(representation loss)을 산출한다. 이제, 이 과정에서는 교사 네트워크의 스토리지 평균값 들과 학생 네트워크의 스토리지 평균값들의 오차 평균(Mean Square)을 수행함으로써 표현 손실을 산출할 수 있 다. 도 3은 표현 학습을 이용한 교사-학생 프레임워크에 기반하여 연속 학습 과정을 도시한 도면이다. 도 3을 참조하면, 학습 수행 전 교사 네트워크를 통해 수행되는 단계(S320)와 학습 수행 중 학생 네트워크를 통 해 수행되는 단계(S330)로 나누어 도시하였다. 가장 먼저, S310 단계에서 사전 학습된 모델과 학습 데이터셋을 준비한다. 학습을 진행하기 전에 사전 학습된 딥러닝 모델로 교사 네트워크와 학생 네트워크를 생성하고, S321 단계에서 교사 네트워크의 표현 메모리를 생성한다. 그런 다음, S323 단계에서 타겟 데이터를 입력시켜 교사 특징 표현 값을 획득하고 적절한 스토리지에 값을 저장한다. 보다 구체적으로, S325 단계에서 정답 레이블로 예측된 경우, 교사 네트워크 레이어의 특징 표현 값을 추출하고, S327 단계에서 스토리에 해당 데이터의 특징 표현을 저장한 다음, S329 단계에서 각 스토리지마다 평균 값을 도출한다. 학습에 진입하게 되면 이상의 과정(S320)와 마찬가지로, S331 단계에서 S321 단계와 동일한 수의 스토리로 구성 된 표현 메모리를 생성하고, S333 단계에서 타겟 데이터를 학생 네트워크에 입력해서 특징 표현 값을 획득하고, 적절한 스토리지에 저장한다. 보다 구체적으로, S335 단계에서 정답 레이블로 예측된 경우, 학생 네트워크 레이 어의 특징 표현 값을 추출하고, S337 단계에서 스토리에 해당 데이터의 특징 표현을 저장한 다음, S339 단계에 서 각 스토리지마다 평균 값을 도출한다. 마지막으로, S340 단계에서 교사 네트워크의 스토리지 평균값들과 학생 네트워크의 스토리지 평균값들의 오차 평균(Mean Square)을 수행한다. 도 4는 표현 학습을 이용하여 연속 학습을 수행하는 아키텍쳐의 전체 파이프라인을 도시한 도면으로서, 예를 들 어, 가짜 멀티미디어 탐지를 위한 CoReD 방법의 워크플로우를 제안한다. 모든 생성 방법의 딥페이크 비디오(Xd) 또는 GAN 이미지(Xg)가 주어지면 본 실시예의 목표는 이를 진짜 또는 가 짜로 분류하는 것이다. 제안된 접근 방식의 전체 파이프라인은 도 4에서 1 단계에서 11 단계까지 도시되어 있다. 그러나 후속 프로세스는 이러한 단계의 반복일 뿐이므로 이하에서는 1 단계부터 6 단계까지의 처음 6 과 정만을 설명한다. (1 단계) 먼저 태스크 1 데이터셋을 사용하여 교사 모델 T1을 완전히 훈련한다. (2 단계) 태스크 1에서 훈련된 교사로부터 가중치를 학생 모델(S1)에 복사한다. (3 단계) 이제, 학생을 태스크 2(T2)의 교사로 변경하고 T2를 훈련 불가능으로 설정한다. (4 단계) 다음으로, 태스크 2 교사 T2로부터 가중치를 복사하고 S2를 학습 가능으로 설정하여 새 학생 모델(S2) 을 만든다.(5 단계) 이제, T2와 S2에 태스크 2의 데이터를 제공한다. 학생은 다음 세 가지 방법에 따라, (a) 교차 엔트로피 손실(학생 손실 LS)을 사용하여 데이터에서 직접, (b) 특징 표현 메모리(표현 손실 LR)를 비교함으로써 T2 및 S2 사이로부터 산출되는 표현 손실을 사용하여, (c) T2와 S2를 사용하여 산출된 지식 증류 손실(증류 손실 LD)을 사 용하여, 데이터에서 학습한다. 각 손실에 대한 자세한 내용은 이후에 다시 설명한다. 참고로, T2는 훈련 불가능 으로 설정되어 있기 때문에 기능 표현 메모리와 지식은 동일하게 유지된다. 그러나 S2의 경우 교육 기간 동안 점 진적으로 변경된다. (6 단계) 학생(S2)이 완전히 훈련되면, (3 단계)로 돌아가서 이를 다음 태스크의 교사(즉, T3)로 사용하고, 모든 태스크(즉, 도 4의 11 단계)을 마칠 때까지 이러한 과정을 반복한다. 도 5는 본 발명의 실시예들이 제안하는 표현 손실을 포함하는 목적 함수를 설명하기 위한 도면이다. 교사 네트 워크 및 학생 네트워크로부터 획득된 레이블 내지 예측된 값을 이용하여 CoReD에 대한 세 가지 손실 함수, 즉 학생 손실, 표현 손실 및 증류 손실을 계산하여 총 손실을 도출할 수 있다. 학생 손실(Student Loss) 앞서 도 4의 5 단계, 8 단계, 및 11 단계에서 볼 수 있는 것처럼 학생 모델(S)을 훈련할 때 교차 엔트로피 손실 을 사용하여 다음의 수학식 1과 같이 태스크의 데이터셋에서 직접 학습한다. 수학식 1"}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 σ는 소프트맥스(softmax) 함수이고, C1과 C2는 진짜 클래스와 가짜 클래스이다. t1[0,1]과 σ(·)1은 정 답(ground truth)과 C1의 점수(score)이고, t2=1-t1과 σ(·)2 = 1-σ(·)1은 C2에 대한 정답과 점수이다. 또한 yi는 출력 레이블(즉, 하드 레이블 y)이고 는 S의 출력(즉, 하드 예측)이다. 증류 손실(Distillation Loss) 도 4의 5 단계, 8 단계, 및 11 단계에서 학생 모델을 학습하는 동안 교사 모델의 학생을 사용하여 다음의 수학 식 2와 같이 증류 손실도 계산한다.수학식 2"}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서 σd는 온도 T가 증류 중에 τ로 초기화된 소프트맥스 함수이다. 그리고 yi는 T의 출력 레이블(즉, 소프트 레이블 y)이고, 는 S의 출력(즉, 소프트 예측)이다. 클래스에 대한 확률 분포를 부드럽게 함으로써 온도는 S 가 T를 모방하는 데 도움이 된다. T를 증가시키면 소프트맥스 함수의 확률 분포가 부드러워져 어떤 클래스 T가 예측된 클래스와 더 유사한지가 드러난다. 표현 손실(Representation Loss) 본 발명의 실시예들에서는, 다양한 생성 방식에서 생성된 다양한 가짜 멀티미디어(딥페이크 비디오 또는 GAN 이 미지) 간에 유사하거나 공통된 기본 특성이 있어야 한다고 생각하였다. 따라서, 태스크 i에 대해 교육을 받은 교사(T)는 더 적은 수의 샘플을 사용하여 학생(S)이 태스크 i+1을 학습하도록 도울 수 있다. 따라서 학생 모델 을 훈련하는 동안 훈련 데이터에 대한 T와 S의 특징 표현을 표현 메모리(Rmem.)에 저장한다. 모든 태스크 i+1 데 이터 특성을 저장하는 대신, 본 발명의 실시예들은 이전 기법에서 많은 수의 샘플을 저장하는 것과 달리 메모리 공간을 최소화하기 위해 고유한 특성만을 선택적으로 저장한다. 이를 달성하기 위해 본 발명의 실시예들은 표현 메모리 및 를 만드는 데 사용하는 T와 S의 출력에 소프트맥스를 적용한다. 이 표현 메모리를 m 값을 시작으로 크기 v 단위로 작은 블록(스토리지를 나타낸 다) b개로 분할하며, 다음과 같이 표현된다: Rmem. = {(m, m+v), (m+v, m+2v), ..., (m+(b-1)v, m+bv)}. 메모리 를 분할하면 학습 과정에서 컨텍스트 전환(context switching)을 감소시키는 데 도움이 된다. 실제 데이터와 가 짜 데이터의 분포가 다르기 때문에 이 작업을 실제 데이터와 가짜 데이터 모두에 대해 별도로 수행한다. 마지막 으로 및 의 차이를 다음의 수학식 3과 같이 계산한다. 수학식 3"}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서 이진 분류의 경우 특징 스토리지를 b=5로 나눈다. 각각의 크기 b=0.1는 m=0.5에서 시작한다. 예를 들어, 은 학생의 표현 메모리의 첫 번째 블록을 나타낸다. 총 손실 다음과 같이 CoReD의 총 손실 함수를 구성하기 위해 수학식 1 내지 3의 세 가지 손실을 모두 합산한다. 수학식 4"}
{"patent_id": "10-2021-0193919", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서 α, β, γ는 세 가지 손실 항을 제어하는 계수이다. 이하에서는, 교사 네트워크 및 학생 네트워크 별로 각각 표현 메모리의 저장 과정을 보다 구체적으로 설명한다. 도 6은 모델 학습의 수행 전(학습에 진입하기 전), 교사 네트워크의 표현 메모리 저장 과정을 도시한 흐름도이 다. S601 단계에서 이전 태스크에서 학습한 모델을 현재 태스크의 교사(T) 모델로 지정하고, S602 단계에서 교사 모 델의 표현 메모리를 생성한다. 이때, m 값을 시작으로 크기 v 단위로 b개의 스토리지를 생성할 수 있다. 즉, Rmem. = {(m, m+v), (m+v, m+2v), ..., (m+(b-1)v, m+bv)}와 같이 표현 메모리의 스토리지가 생성될 수 있다. 이제 순회할 타겟 데이터의 개수를 초기화하고(i=0), S603 단계를 통해 타겟 데이터를 입력받아 상기 교사 네트 워크에 입력한다. 또한, 출력된 값에 소프트맥스(softmax) 함수를 적용하여 미리 설정된 범위의 값을 결정한다. 예를 들어, 0과 1사이의 값을 갖도록 할 수 있다. S604 단계에서 입력된 상기 타겟 데이터가 정답으로 추론된 경우, S605 단계로 진행하여 상기 교사 네트워크의 마지막 레이어에 속하는 특징 맵(feature map)을 추출한다. 이 과정은, 입력된 상기 타겟 데이터가 상기 교사 네트워크에 학습시켰던 도메인(domain)과 상이하더라도 정답으로 예측한 데이터 내에 소스(source) 도메인이 가 졌던 특징과 유사한 부분이 존재하는 성질을 이용하는 것이다. 예를 들어, 서로 다른 방식의 가짜 이미지를 분 류하는 태스크라고 할지라도 그 대상은 얼굴 이미지라는 유사한 부분이 존재한다는 사실을 활용한다. S606 단계에서는 추출된 상기 특징 맵으로부터 맥스 풀링(max pooling)을 적용하여 풀링된 특징 맵의 평균값을 산출함으로써 특징 표현 값을 획득한다. 그런 다음, S606 단계에서 각 스토리지 중 범위에 속하는 곳에 추출한 값을 저장한다. 즉, 앞서 결정된 상기 미리 설정된 범위의 값에 대응하는 스토리지에 S606 단계를 통해 획득된 상기 특징 표현 값을 저장한다. 만약, S607 단계를 통해, 입력된 모든 타겟 데이터를 순회하였다면, S608 단계로 진행하여 상기 스토리지에 저 장된 특징 표현 값의 평균값을 각각 산출하여 상기 교사 네트워크의 스토리지를 모두 갱신한다. 도 7은 모델 학습 중, 학생 네트워크의 표현 메모리 저장 및 학습 과정을 도시한 흐름도이다. S701 단계에서 이전 태스크에서 학습한 모델을 현재 태스크의 학생(S) 모델로 지정하고, S702 단계에서 학생 모 델의 표현 메모리를 생성한다. 이때, m 값을 시작으로 크기 v 단위로 b개의 스토리지를 생성할 수 있다. 즉, Rmem. = {(m, m+v), (m+v, m+2v), ..., (m+(b-1)v, m+bv)}와 같이 표현 메모리의 스토리지가 생성될 수 있다. 이제 순회할 타겟 데이터의 개수를 초기화하고(i=0), S703 단계를 통해 타겟 데이터를 입력받아 상기 학생 네트 워크에 입력한다. 또한, 출력된 값에 소프트맥스(softmax) 함수를 적용하여 미리 설정된 범위의 값을 결정한다. 예를 들어, 0과 1사이의 값을 갖도록 할 수 있다. S704 단계에서 입력된 상기 타겟 데이터가 정답으로 추론된 경우, S705 단계로 진행하여 상기 학생 네트워크의 마지막 레이어에 속하는 특징 맵(feature map)을 추출한다. S706 단계에서는 추출된 상기 특징 맵으로부터 맥스 풀링(max pooling)을 적용하여 풀링된 특징 맵의 평균값을 산출함으로써 특징 표현 값을 획득한다. 그런 다음, S706 단계에서 각 스토리지 중 범위에 속하는 곳에 추출한 값을 저장한다. 즉, 앞서 결정된 상기 미리 설정된 범위의 값에 대응하는 스토리지에 S706 단계를 통해 획득된 상기 특징 표현 값을 저장한다. 만약, S707 단계를 통해, 입력된 모든 타겟 데이터를 순회하였다면, S708 단계로 진행하여 상기 스토리지에 저 장된 특징 표현 값의 평균값을 각각 산출하여 상기 학생 네트워크의 스토리지를 모두 갱신한다.마지막으로, S709 단계에서는, 상기 교사 네트워크의 스토리지에 저장된 평균값들 및 상기 학습 네트워크의 스 토리지에 저장된 평균값들의 오차 평균(Mean Square)을 수행하여 표현 손실을 산출한다. 도 8은 본 발명의 일 실시예에 따른 표현 학습을 이용하여 연속 학습을 수행하는 학습 장치를 도시한 블록 도로서, 도 2의 연속 학습을 수행하는 방법을 하드웨어 구성의 관점에서 재구성한 것이다. 따라서, 여기서는 설 명의 중복을 피하고자 각 구성별로 수행 동작 내지 기능을 약술하도록 한다. 입력부는 적어도 하나 이상의 태스크 및 상기 태스크에 따른 타겟 데이터를 입력받는 구성이다. 저장부는 특징 표현 값을 저장하기 위한 표현 메모리를 구성하며, 교사 네트워크 및 학생 네트워크에 대해 각각 별도의 스토리지(831, 832)를 구성할 수 있다. 처리부는 상기 표현 메모리를 이용하여 연속 학습을 수행하는 프로그램을 실행하는 구성이다. 처리부(82 0)에 의해 실행되는 프로그램은, 지식 증류(Knowledge Distillation)를 이용하여 미리 학습된 모델로부터 교사 네트워크 및 학생 네트워크를 생성하고, 상기 교사 네트워크 및 상기 학생 네트워크에 각각 특징 표현 값을 저 장하기 위해 클래스(class)의 수만큼 복수 개의 스토리지로 구성된 표현 메모리를 생성하고, 학습에 진입하기 전에 상기 교사 네트워크를 통해 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토리지에 저장하고, 학습에 진입하여 상기 학생 네트워크를 통해 상기 타겟 데이터에 대한 특징 표현 값을 추출하여 해당하는 스토 리지에 저장하며, 상기 교사 네트워크 및 상기 학습 네트워크의 스토리지로부터 도출된 값들을 이용하여 표현 손실(representation loss)을 산출하는 명령을 포함한다. 처리부에 의해 실행되는 프로그램은, 미리 학습된 모델을 참조하여 큰 모델인 교사 네트워크의 지식을 상 대적으로 작은 모델인 학생 네트워크에 전달하는 지식 증류에 의해 수행되며, 상기 교사 네트워크 및 상기 학생 네트워크는 각각 이전 태스크에서 학습한 모델을 현재 태스크의 교사 모델 및 학생 모델로 지정할 수 있다. 처리부에 의해 실행되는 프로그램은, 상기 모델이 수행하고자 하는 태스크(task)에 따라 분류된 정답으로 클래스를 나누어 특징 표현 값을 저장하기 위한 클래스의 수만큼 표현 메모리를 생성하되, 생성된 상기 메모리 는 복수 개의 스토리지로 분할하며, 각각의 스토리지마다 저장 가능한 값의 범위를 한정하여 설정될 수 있다. 처리부에 의해 실행되는 프로그램은, 타겟 데이터를 입력받아 상기 교사 네트워크에 입력하고, 출력된 값 에 소프트맥스(softmax) 함수를 적용하여 소정 범위의 값을 결정하고, 입력된 상기 타겟 데이터가 정답으로 추 론된 경우 상기 교사 네트워크의 마지막 레이어에 속하는 특징 맵(feature map)을 추출하고, 추출된 상기 특징 맵으로부터 맥스 풀링(max pooling)을 적용하여 풀링된 특징 맵의 평균값을 산출함으로써 특징 표현 값을 획득 하며, 결정된 상기 소정 범위의 값에 대응하는 스토리지에 획득된 상기 특징 표현 값을 저장하는 명령을 포함하 되, 상기 교사 네트워크를 통한 스토리지 저장 과정은 학습에 진입하기 전에 수행된다. 여기서, 처리부에 의해 실행되는 프로그램은, 입력된 상기 타겟 데이터가 상기 교사 네트워크에 학습시켰던 도메인(domain)과 상 이하더라도 정답으로 예측한 데이터 내에 소스(source) 도메인이 가졌던 특징과 유사한 부분이 존재하는 성질을 이용하여 실행될 수 있다. 또한, 처리부에 의해 실행되는 프로그램은, 입력된 모든 타겟 데이터를 순회하 였다면, 상기 스토리지에 저장된 특징 표현 값의 평균값을 각각 산출하여 상기 교사 네트워크의 스토리지를 모 두 갱신하는 명령을 더 포함할 수 있다. 처리부에 의해 실행되는 프로그램은, 상기 타겟 데이터를 입력받아 상기 학생 네트워크에 입력하고, 출력 된 값에 소프트맥스 함수를 적용하여 소정 범위의 값을 결정하고, 입력된 상기 타겟 데이터가 정답으로 추론된 경우 상기 학생 네트워크의 마지막 레이어에 속하는 특징 맵을 추출하고, 추출된 상기 특징 맵으로부터 특징 표 현 값을 획득하며, 결정된 상기 소정 범위의 값에 대응하는 스토리지에 획득된 상기 특징 표현 값을 저장하는 명령을 포함하되, 상기 학생 네트워크를 통한 스토리지 저장 과정은 학습에 진입하여 수행된다. 또한, 처리부 에 의해 실행되는 프로그램은, 입력된 모든 타겟 데이터를 순회하였다면, 상기 스토리지에 저장된 특징 표 현 값의 평균값을 각각 산출하여 상기 학생 네트워크의 스토리지를 모두 갱신하는 명령을 더 포함할 수 있다. 나아가, 처리부에 의해 실행되는 프로그램은, 상기 교사 네트워크의 스토리지에 저장된 평균값들 및 상기 학습 네트워크의 스토리지에 저장된 평균값들의 오차 평균(Mean Square)을 수행하여 표현 손실을 산출할 수 있 다. 상기된 본 발명의 실시예들에 따르면, 지식 증류 기법의 교사-학생 네트워크 구조와 표현 학습을 이용하여 전이 학습 내지 연속 학습 시 표현 메모리를 통해 소스 데이터 없이 기존 모델의 성능을 최대한 유지하면서도 지식 망각 현상을 방지할 수 있고, 다양한 타겟 도메인 학습에 있어서 효과적인 성능 향상이 가능하다.한편, 본 발명의 실시예들은 컴퓨터로 읽을 수 있는 기록 매체에 컴퓨터가 읽을 수 있는 코드로 구현하는 것이 가능하다. 컴퓨터가 읽을 수 있는 기록 매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록 장치를 포함한다. 컴퓨터가 읽을 수 있는 기록 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 데이터 저장장치 등을 포함한다. 또한, 컴퓨터가 읽을 수 있는 기록 매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산 방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 그리고 본 발명을 구현하기 위한 기능적인 (functional) 프로그램, 코드 및 코드 세그먼트들은 본 발명이 속하는 기술 분야의 프로그래머들에 의하여 용이 하게 추론될 수 있다. 이상에서 본 발명에 대하여 그 다양한 실시예들을 중심으로 살펴보았다. 본 발명에 속하는 기술 분야에서 통상 의 지식을 가진 자는 본 발명이 본 발명의 본질적인 특성에서 벗어나지 않는 범위에서 변형된 형태로 구현될 수 있음을 이해할 수 있을 것이다. 그러므로 개시된 실시예들은 한정적인 관점이 아니라 설명적인 관점에서 고려되 어야 한다. 본 발명의 범위는 전술한 설명이 아니라 특허청구범위에 나타나 있으며, 그와 동등한 범위 내에 있 는 모든 차이점은 본 발명에 포함된 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2021-0193919", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 지식 증류(Knowledge Distillation) 기법을 설명하기 위한 도면이다. 도 2는 본 발명의 일 실시예에 따른 표현 학습을 이용하여 연속 학습을 수행하는 방법을 도시한 흐름도이다. 도 3은 표현 학습을 이용한 교사-학생 프레임워크에 기반하여 연속 학습 과정을 도시한 도면이다. 도 4는 표현 학습을 이용하여 연속 학습을 수행하는 아키텍쳐의 전체 파이프라인을 도시한 도면이다. 도 5는 본 발명의 실시예들이 제안하는 표현 손실을 포함하는 목적 함수를 설명하기 위한 도면이다. 도 6은 모델 학습의 수행 전, 교사 네트워크의 표현 메모리 저장 과정을 도시한 흐름도이다. 도 7은 모델 학습 중, 학생 네트워크의 표현 메모리 저장 및 학습 과정을 도시한 흐름도이다. 도 8은 본 발명의 일 실시예에 따른 표현 학습을 이용하여 연속 학습을 수행하는 학습 장치를 도시한 블록도이 다."}
