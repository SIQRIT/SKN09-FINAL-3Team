{"patent_id": "10-2021-0182951", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0093927", "출원번호": "10-2021-0182951", "발명의 명칭": "시각적 질문 답변에 대한 자기주의를 갖는 영역설명 학습방법 및 이를 수행하는 학습장치", "출원인": "경북대학교 산학협력단", "발명자": "이민호"}}
{"patent_id": "10-2021-0182951", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "학습장치에서 수행되는 영역설명 학습방법에 있어서, 입력된 이미지, 영역설명 및 질문으로부터 객체특징 및 언어특징을 추출하는 추출단계; 상기 객체특징 및 상기 언어특징 각각의 위치정보를 추가하는 위치인코딩단계; 상기 객체특징, 상기 언어특징 및 상기 각각의 위치정보를 기설정된 조건에 따라 결합하여 임베딩하는 임베딩단계; 및상기 임베딩단계에서 임베딩된 객체특징 임베딩, 언어특징 임베딩을 서로 연결(concatenate)하여 상기 객체특징과 상기 언어특징 간의 관계를 학습하는 학습단계를 포함하고, 상기 영역설명은, 상기 입력된 이미지의 영역 내에 있는 객체에 대한 이름정보 및 설명정보를 포함하는 것을 특징으로 하는 영역설명 학습방법."}
{"patent_id": "10-2021-0182951", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 추출단계에서는, 상기 영역설명 및 상기 질문에 포함되는 문장을 단어단위로 토큰화하여 상기 언어특징을 추출하고, 상기 임베딩단계에서는, 상기 객체특징과 상기 객체특징의 위치정보를 결합하고, 상기 언어특징과 상기 언어특징의 위치정보를 결합하여임베딩하는 것을 특징으로 하는 영역설명 학습방법."}
{"patent_id": "10-2021-0182951", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 언어특징 임베딩은, 상기 영역설명에 대한 언어특징과 상기 언어특징의 위치정보를 결합하여 임베딩된 영역설명 언어특징 임베딩,그리고 상기 질문에 대한 언어특징과 상기 언어특징의 위치정보를 결합하여 임베딩된 질문 언어특징 임베딩을포함하고, 상기 임베딩단계는, 상기 영역설명에 대한 언어특징에서 기설정된 조건에 따라 선택된 하나의 단어토큰을 마스킹하여 마스킹된 상기영역설명의 언어특징과 상기 언어특징의 위치정보를 결합하여 임베딩하는 제1 임베딩단계를 포함하고, 상기 학습단계에서는, 상기 질문 언어특징 임베딩과 상기 제1 임베딩단계에서 마스킹된 영역설명 언어특징 임베딩을 포함하는 상기 언어특징 임베딩을 상기 객체특징 임베딩과 서로 연결하는 것을 특징으로 하는 영역설명 학습방법."}
{"patent_id": "10-2021-0182951", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 학습단계는, 상기 객체특징과 상기 언어특징에서 마스킹되지 않은 단어토큰에 기초하여 상기 마스킹된 단어토큰을 예측하여공개특허 10-2023-0093927-3-학습하는 제1 학습단계를 포함하는 것을 특징으로 하는 영역설명 학습방법."}
{"patent_id": "10-2021-0182951", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 임베딩단계는, 상기 객체특징에서 기설정된 조건에 따라 상기 이미지에 포함된 객체 중 하나를 마스킹하여 마스킹된 상기 객체특징과 상기 객체특징의 위치정보를 결합하여 임베딩하는 제2 임베딩단계를 더 포함하고, 상기 학습단계에서는, 상기 언어특징 임베딩과 상기 제2 임베딩단계에서 마스킹된 객체특징 임베딩을 서로 연결하는 것을 특징으로 하는 영역설명 학습방법."}
{"patent_id": "10-2021-0182951", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제6항에 있어서, 상기 학습단계는, 상기 언어특징에 기초하여 상기 마스킹된 객체를 예측하여 학습하는 제2 학습단계를 포함하는 것을 특징으로 하는 영역설명 학습방법."}
{"patent_id": "10-2021-0182951", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "입력된 이미지, 영역설명 및 질문으로부터 객체특징 및 언어특징을 추출하는 추출부;상기 객체특징 및 상기 언어특징 각각의 위치정보를 추가하는 위치인코딩부; 상기 객체특징, 상기 언어특징 및 상기 각각의 위치정보를 기설정된 조건에 따라 결합하여 임베딩하는임베딩부; 및 상기 임베딩단계에서 임베딩된 객체특징 임베딩, 언어특징 임베딩을 서로 연결(concat)하여 상기 객체특징과 상기 언어특징 간의 관계를 학습하는 학습부를 포함하고, 상기 영역설명은, 상기 영역설명은, 상기 입력된 이미지의 영역 내에 있는 객체에 대한 이름정보 및 설명정보를 포함하는 것을 특징으로 하는 학습장치."}
{"patent_id": "10-2021-0182951", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 추출부는, 상기 영역설명 및 상기 질문에 포함되는 문장을 단어단위로 토큰화하여 상기 언어특징을 추출하고,상기 임베딩부는, 상기 객체특징과 상기 객체특징의 위치정보를 결합하고, 상기 언어특징과 상기 언어특징의 위치정보를 연결하여임베딩하는 것을 특징으로 하는 학습장치."}
{"patent_id": "10-2021-0182951", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 언어특징 임베딩은, 상기 영역설명에 대한 언어특징과 상기 언어특징의 위치정보를 결합하여 임베딩된 영역설명 언어특징 임베딩,그리고 상기 질문에 대한 언어특징과 상기 언어특징의 위치정보를 결합하여 임베딩된 질문 언어특징 임베딩을공개특허 10-2023-0093927-4-포함하고, 상기 임베딩부는, 상기 영역설명에 대한 언어특징에서 기설정된 조건에 따라 선택된 하나의 단어토큰을 마스킹하여 상기 마스킹된상기 영역설명의 언어특징과 상기 언어특징의 위치정보를 결합하여 임베딩하고, 상기 학습부는, 상기 질문 언어특징 임베딩 및 마스킹된 영역설명 언어특징 임베딩을 포함하는 상기 언어특징 임베딩을 상기 객체특징 임베딩과 서로 연결하는 것을 특징으로 하는 영역설명 학습장치."}
{"patent_id": "10-2021-0182951", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 학습부는, 상기 객체특징과 상기 언어특징에서 마스킹되지 않은 단어토큰에 기초하여 상기 마스킹된 단어토큰을 예측하여학습하는 것을 특징으로 하는 영역설명 학습장치."}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은, 학습장치에서 수행되는 영역설명 학습방법에 관한 것으로, 입력된 이미지, 영역설명 및 질문으로부터 객체특징 및 언어특징을 추출하는 추출단계; 상기 객체특징 및 상기 언어특징 각각의 위치정보를 추가하는 위치 인코딩단계; 상기 객체특징, 상기 언어특징 및 상기 각각의 위치정보를 기설정된 조건에 따라 결합하여 임베딩하 는 임베딩단계; 및 상기 임베딩단계에서 임베딩된 객체특징 임베딩, 언어특징 임베딩을 서로 연결(concatenate) 하여 상기 객체특징과 상기 언어특징 간의 관계를 학습하는 학습단계를 포함하고, 상기 영역설명은, 상기 입력된 이미지의 영역 내에 있는 객체에 대한 이름정보 및 설명정보를 포함한다. 이에 의해 이미지 안에 객체 및 객체와 연관된 영역 설명을 활용하여 이미지와 질문-답변 사이에 올바른 정렬을 만들어 이미지와 질문-답변 간의 관계맥 락을 포착하여 정확한 답변을 제시할 수 있다."}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 시각적 질문 답변에 대한 자기주의를 갖는 영역설명 학습 접근방법 및 이를 수행하는 학습장치에 관 한 것으로 보다 상세하게는 시각적 질문에 대해 답변할 수 있는 시각적 질문 답변에 대한 자기주의를 갖는 영역 설명 학습 접근방법 및 이를 수행하는 학습장치에 관한 것이다."}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "근래에는 딥러닝(deep learning)을 위시한 기계 학습 기술의 발전과 더불어 컴퓨터 비전(computer vision), 자 연어 처리(natural language processing) 등과 같은 인공지능(AI)의 핵심 기술들이 혁신적으로 발전하고 있다. 그에 따라, 다시 고전적인 튜링 테스트(Turing Test)와 같이 인공지능이 얼마나 인간에 가까운 복합 지능을 발 휘할 수 있는지 알아보려는 매우 도전적인 과제들이 활발히 생겨나고 있다. 그 중에서도 시각적 질문-답변 (Visual Question Answering, VQA)은 영상과 질문이 입력되면 영상에 관한 자연어 질문에 인공지능이 얼마나 자 연스러운 답변을 자동 생성하는지를 알아보기 위한 지능 작업이다. 하지만 종래에는 시각적 질문-답변(VQA)의 대표적인 한계점은 이미지와 질문-답변을 강제적으로 매칭하게 되고, 이러한 강제적 매칭에 의해 이미지와 질문-답변 사이에 올바른 정렬을 만들 수 없다는 문제가 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허공보 제10-1934372호"}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기와 같은 문제를 해결하기 위해 안출된 것으로, 본 발명의 목적은 이미지 안에 객체 및 객체와 연 관된 영역 설명을 활용하여 이미지와 질문-답변 사이에 올바른 정렬을 만들어 이미지와 질문-답변 간의 관계맥 락을 포착하여 정확한 답변을 제시할 수 있는 시각적 질문 답변에 대한 자기주의를 갖는 영역설명 학습방법 및 이를 수행하는 학습장치를 제공하는 것이다."}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 학습장치에서 수행되는 영역설명 학습방법은, 입력된 이미지, 영역설명 및 질문으로부터 객체특징 및 언어특징을 추출하는 추출단계; 상기 객체특징 및 상기 언어특 징 각각의 위치정보를 추가하는 위치인코딩단계; 상기 객체특징, 상기 언어특징 및 상기 각각의 위치정보를 기 설정된 조건에 따라 결합하여 임베딩하는 임베딩단계; 및 상기 임베딩단계에서 임베딩된 객체특징 임베딩, 언어 특징 임베딩을 서로 연결(concatenate)하여 상기 객체특징과 상기 언어특징 간의 관계를 학습하는 학습단계를 포함하고, 상기 영역설명은, 상기 입력된 이미지의 영역 내에 있는 객체에 대한 이름정보 및 설명정보를 포함한 다. 그리고 상기 추출단계에서는, 상기 영역설명 및 상기 질문에 포함되는 문장을 단어단위로 토큰화하여 상기 언어 특징을 추출하고, 상기 임베딩단계에서는, 상기 객체특징과 상기 객체특징의 위치정보를 결합하고, 상기 언어특 징과 상기 언어특징의 위치정보를 결합하여 임베딩할 수 있다. 또한, 상기 언어특징 임베딩은, 상기 영역설명에 대한 언어특징과 상기 언어특징의 위치정보를 결합하여 임베딩 된 영역설명 언어특징 임베딩, 그리고 상기 질문에 대한 언어특징과 상기 언어특징의 위치정보를 결합하여 임베 딩된 질문 언어특징 임베딩을 포함하고, 상기 임베딩단계는, 상기 영역설명에 대한 언어특징에서 기설정된 조건 에 따라 선택된 하나의 단어토큰을 마스킹하여 마스킹된 상기 영역설명의 언어특징과 상기 언어특징의 위치정보 를 결합하여 임베딩하는 제1 임베딩단계를 포함하고, 상기 학습단계에서는, 상기 질문 언어특징 임베딩과 상기 제1 임베딩단계에서 마스킹된 영역설명 언어특징 임베딩을 포함하는 상기 언어특징 임베딩을 상기 객체특징 임 베딩과 서로 연결할 수 있다. 그리고, 상기 학습단계는, 상기 객체특징과 상기 언어특징에서 마스킹되지 않은 단어토큰에 기초하여 상기 마스 킹된 단어토큰을 예측하여 학습하는 제1 학습단계를 포함할 수 있다. 또한, 상기 임베딩단계는, 상기 객체특징에서 기설정된 조건에 따라 상기 이미지에 포함된 객체 중 하나를 마스 킹하여 마스킹된 상기 객체특징과 상기 객체특징의 위치정보를 결합하여 임베딩하는 제2 임베딩단계를 더 포함 하고, 상기 학습단계에서는, 상기 언어특징 임베딩과 상기 제2 임베딩단계에서 마스킹된 객체특징 임베딩을 서 로 연결할 수 있다. 그리고, 상기 학습단계는, 상기 언어특징에 기초하여 상기 마스킹된 객체를 예측하여 학습하는 제2 학습단계를 더 포함할 수 있다. 한편, 상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 학습장치 는 입력된 이미지, 영역설명 및 질문으 로부터 객체특징 및 언어특징을 추출하는 추출부; 상기 객체특징 및 상기 언어특징 각각의 위치정보를 추가하는 위치인코딩부; 상기 객체특징, 상기 언어특징 및 상기 각각의 위치정보를 기설정된 조건에 따라 결합하여 임베 딩하는 임베딩부; 및 상기 임베딩단계에서 임베딩된 객체특징 임베딩, 언어특징 임베딩을 서로 연결 (concatenate)하여 상기 객체특징과 상기 언어특징 간의 관계를 학습하는 학습부를 포함하고, 상기 입력된 이미 지의 영역 내에 있는 객체에 대한 이름정보 및 설명정보를 포함한다. 그리고 상기 추출부는, 상기 영역설명 및 상기 질문에 포함되는 문장을 단어단위로 토큰화하여 상기 언어특징을 추출하고, 상기 임베딩부는, 상기 객체특징과 상기 객체특징의 위치정보를 결합하고, 상기 언어특징과 상기 언 어특징의 위치정보를 연결하여 임베딩할 수 있다. 또한, 상기 언어특징 임베딩은, 상기 영역설명에 대한 언어특징과 상기 언어특징의 위치정보를 결합하여 임베딩 된 영역설명 언어특징 임베딩, 그리고 상기 질문에 대한 언어특징과 상기 언어특징의 위치정보를 결합하여 임베 딩된 질문 언어특징 임베딩을 포함하고, 상기 임베딩부는, 상기 영역설명에 대한 언어특징에서 기설정된 조건에 따라 선택된 하나의 단어토큰을 마스킹하여 상기 마스킹된 상기 영역설명의 언어특징과 상기 언어특징의 위치정 보를 결합하여 임베딩하고, 상기 학습부는, 상기 질문 언어특징 임베딩 및 마스킹된 영역설명 언어특징 임베딩 을 포함하는 상기 언어특징 임베딩을 상기 객체특징 임베딩과 서로 연결할 수 있다. 그리고 상기 학습부는, 상기 객체특징과 상기 언어특징에서 마스킹되지 않은 단어토큰에 기초하여 상기 마스킹 된 단어토큰을 예측하여 학습할 수 있다."}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상술한 본 발명의 일측면에 따르면, 시각적 질문 답변에 대한 자기주의를 갖는 영역설명 학습방법 및 이를 수행 하는 학습장치를 제공함으로써, 이미지 안에 객체 및 객체와 연관된 영역 설명을 활용하여 이미지와 질문-답변사이에 올바른 정렬을 만들어 이미지와 질문-답변 간의 관계맥락을 포착하여 정확한 답변을 제시할 수 있다."}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "후술하는 본 발명에 대한 상세한 설명은, 본 발명이 실시될 수 있는 특정 실시예를 예시로서 도시하는 첨부 도 면을 참조한다. 이들 실시예는 당업자가 본 발명을 실시할 수 있기에 충분하도록 상세히 설명된다. 본 발명의 다양한 실시예는 서로 다르지만 상호 배타적일 필요는 없음이 이해되어야 한다. 예를 들어, 여기에 기재되어 있 는 특정 형상, 구조 및 특성은 일 실시예와 관련하여 본 발명의 정신 및 범위를 벗어나지 않으면서 다른 실시예 로 구현될 수 있다. 또한, 각각의 개시된 실시예 내의 개별 구성요소의 위치 또는 배치는 본 발명의 정신 및 범 위를 벗어나지 않으면서 변경될 수 있음이 이해되어야 한다. 따라서, 후술하는 상세한 설명은 한정적인 의미로 서 취하려는 것이 아니며, 본 발명의 범위는, 적절하게 설명된다면, 그 청구항들이 주장하는 것과 균등한 모든 범위와 더불어 첨부된 청구항에 의해서만 한정된다. 도면에서 유사한 참조부호는 여러 측면에 걸쳐서 동일하거 나 유사한 기능을 지칭한다. 이하에서는 도면들을 참조하여 본 발명의 바람직한 실시예들을 보다 상세하게 설명하기로 한다. 도 1은 본 발명의 일 실시예에 따른 학습장치의 구성을 설명하기 위한 블록도, 도 2는 본 발명의 일 실시 예에 따른 학습모델의 구조를 설명하기 위한 도면, 그리고 도 3은 본 발명의 학습장치에 입력되는 영역설 명을 포함하는 이미지를 설명하기 위한 도면이다. 본 발명의 학습장치는 객체와 결합된 이미지 영역설명을 사용하여 질문-답변과 이미지 사이에 적절한 정렬 을 생성함으로써 사전학습을 수행하고, 이를 통해 사전학습 이후에 추가학습을 수행하는 경우 학습모델의 성능 을 향상시킬 수 있다. 그리고 본 발명의 학습장치는 트랜스포머 모델(transformer model)의 자기주의(self-attention) 메커니즘 을 사용해 이미지와 질문-답변에 관계 맥락을 포착하여 학습 정렬을 생성할 수 있다. 그리고 본 발명의 학습장 치는 영역설명 학습방법을 수행하기 위한 소프트웨어(어플리케이션)이 설치되어 실행될 수 있다. 본 발명의 학습장치에서 수행되는 전반적인 학습방법은 도 2에 도시된 학습모델의 구조에 기초하여 수행될 수 있으며, 이러한 학습모델을 간략하게 설명하면, 입력으로 질문, 객체 이름(label)이 있는 영역설명, 이미지 영역을 사용하여 이미지에서 객체 영역을 추출하고, 객체와 연관된 영역 설명을 추출하고 각 추출된 객체 이름 (label)과 영역설명을 결합할 수 있다. 그리고 이미지에 대한 질문을 추출하고, 추출한 정보들을 임베딩하고 다 중 계층 트랜스포머 네트워크(multi-layer Transformer network)에 입력으로 사용되어 사전학습을 수행한다. 이 러한 사전학습이 완료되면 이미지와 질문답변 사이에 올바른 정렬이 생성되고 관계맥락을 포착할 수 있게 된다. 따라서 영역설명을 활용하여 이미지와 질문답변을 매칭하는 다리 역할을 수행할 수 있다. 이를 위해 본 학습장치는 통신부, 입력부, 저장부, 제어부 및 출력부를 포함하 여 사용자로부터 입력된 이미지 및 질문에 대한 답변을 출력할 수 있다. 먼저 통신부는 학습장치가 유무선 네트워크를 통해 외부 장치와 각종 데이터를 송수신하기 위해 마련 될 수 있다. 이러한 통신부는 제어부가 분석할 이미지 데이터 세트를 수신하거나, 제어부가 이 미지 데이터 세트를 분석하거나 이를 이용하여 모델링하는 과정에서 필요한 신경망과 관련된 데이터를 수신하여 제어부에 제공할 수도 있다. 또한 통신부는 외부 장치에서 수신한 질문을 전달하거나 제어부에 서 연산한 답변을 외부장치로 전달할 수도 있다. 입력부 및 출력부는 데이터나 사용자 명령을 수신하고, 사용자의 입력에 따라 데이터를 연산하고 제 어부에서 처리한 결과를 출력하기 위해 마련될 수 있다. 이러한 입력부는 키보드, 마우스, 터치패널등의 사용자 입력수단을 포함할 수 있고, 출력부는 모니터, 스피커 등의 출력수단을 포함할 수 있다. 그리고 본 학습장치는 통신부 또는 입력부를 통해 질문, 객체 이름(label)이 있는 영역설명, 이 미지를 입력 데이터로 입력받을 수 있다. 이 때 영역설명의 경우에는 도 3에 도시된 바와 같이 이미지 하나 당 4~6개의 영역설명을 사용할 수 있으며, 이러한 영역설명은 도면에서와 같이 이미지에 포함된 객체의 이름정보 및 설명정보를 포함하여 이미지에서 객체 간의 관계를 정확하게 설명하는 설명일 수 있다. 또한 이미지 하나 당 사용되는 영역설명의 개수는 꼭 상술한 개수에 한정되는 것은 아니다. 저장부는 질의-답변에 필요한 각종 데이터나 프로그램을 저장할 수 있고, 이미지에 대한 질문-답변의 학습 과정에 필요한 데이터 세트를 저장할 수 있다. 이때 데이터 세트는 이미지 데이터, 이미지 데이터에 대응하는 질문 데이터와 답변이 될 선택지 데이터, 정답 데이터 등에 대한 정보를 포함할 수 있다. 특히 데이터 세트는 이미지에 대한 학습(training) 세트, 확인(Validation) 세트 및 테스트 세트를 모두 포함할 수 있다. 그리고 저 장부는 이미지에 대한 질의-답변을 학습하고, 그에 따라 질의에 대한 답변을 수행할 수 있도록 하는 프로 그램과 학습 및 질문-답변에 이용되는 각각의 모델이나 신경망에 연관된 데이터들을 적어도 일시적으로 보유하 거나 갱신할 수 있다. 한편 제어부는 CPU등과 같은 적어도 하나의 프로세서를 포함하고 학습장치의 전반적인 동작과 구성을 제어할 수 있고, 제어부는 저장부에 저장된 프로그램을 실행하거나 데이터를 읽어 모델링을 수행하고 그에 따라 질문에 대한 적절한 답변을 선택할 수 있다. 특히 본 발명의 제어부는 이미지의 해당 영역설명을 활용하여 이미지와 질문답변의 관계맥락을 포착하여 정확한 답변을 제공하기 위하여 추출부, 위치인코딩부, 임베딩부 및 학습부를 포함할 수 있다. 추출부는 통신부 또는 입력부를 통해 입력된 데이터인 이미지, 영역설명 및 질문으로부터 객체 특징 및 언어특징을 추출하기 위해 마련된다. 여기서 영역설명은, 객체 이름(label)이 포함되어 입력된 이미지 에 포함되는 객체를 설명하는 설명일 수 있다. 그리고 추출부는 이미지로부터 객체특징을 추출하기위해 Faster R-CNN을 사용할 수 있다. 이에 N개의 객체 가 있는 이미지가 입력되면, 각 객체 N차원 벡터의 객체특징을 추출할 수 있다. 이러한 객체특징은 입력된 이미 지에 포함되는 객체의 객체 이름(object label)을 포함할 수 있으며, 이를 위해 사전에 객체별로 라벨링하는 과 정이 수행되거나 저장되어 있을 수 있다. 또한 추출부는 입력된 질문 및 영역설명의 언어특징을 추출함에 있어서, 질문 및 영역설명에 포함된 문장 을 wordpiece tokenizer를 이용해 단어단위로 토큰화하여 언어특징을 추출할 수 있다. 한편, 위치인코딩부는 객체특징 및 언어특징 각각의 위치정보를 추가할 수 있다. 본 발명에서 사용하는 트 랜스포머 모델은 주의(attention)만을 사용해 Seq2Seq 테스크를 모델링한 아키텍처로, 들어온 시퀀스의 각 토큰 으로부터 키(key), 쿼리(query) 및 값(value)에 해당하는 정보를 추출한 후 이를 이용해 각 토큰과 관련이 높은 정보에 집중(attend)하여 representation을 생성할 수 있다. 하지만 트랜스포머 모델의 자기주의 메커니즘은 구 조적으로 문장의 순서를 반영할 수 없기 때문에 문장 내에서의 토큰의 위치에 대한 정보와 객체특징의 위치가 추가 입력으로 인코딩될 수 있으며, 이를 위치인코딩부에서 수행하도록 할 수 있다. 임베딩부는 객체특징, 언어특징, 각 특징의 위치정보를 기설정된 조건에 따라 결합하여 임베딩할 수 있다. 즉, 객체특징과 객체특징의 위치정보를 결합하고, 언어특징과 언어특징의 위치정보를 연결하여 임베딩할 수 있 다. 그리고 임베딩부는 특징 위치정보에 기초하여 객체특징에 대한 영역설명을 획득하고, 객체특징과 영역설명 을 연결하여 임베딩할 수도 있다. 임베딩부에서 임베딩된 언어특징 임베딩은 영역설명에 대한 언어특징과 해당 언어특징의 위치정보를 결합 하여 임베딩된 영역설명 언어특징 임베딩, 그리고 질문에 대한 언어특징과 해당 언어특징의 위치정보를 결합하 여 임베딩된 질문 언어특징 임베딩을 포함할 수 있다. 또한 임베딩부는, 영역설명에 대한 언어특징에서 기설정된 조건에 따라 선택된 하나의 단어토큰을 마스킹 하여 마스킹된 영역설명의 언어특징과 해당 언어특징의 위치정보를 결합하여 임베딩할 수 있다. 본 발명에서는 영역설명에 포함된 단어 중 하나가 마스킹되는 것이 바람직하나 도면에서는 질문에 포함된 단어가 마스킹된 것으로 도시되었으며, 이는 사전에 설정되는 값에 따라 얼마든지 변경가능하다. 도면에 도시된 바와 같이 임베딩부는 단어 중 하나를 일정확률에 기초하여 랜덤으로 마스킹할 수 있는데, 이러한 단어토큰의 마스킹은 일정 확률에 기초하여 무작위로 마스킹될 수 있다. 따라서 영역설명에 포함된 단어 토큰이 마스킹되거나 질문에 포함된 단어토큰이 마스킹될 수도 있다. 보다 구체적으로 객체 이름과 단어 토큰을 사용하는 영역설명은 동일한 의미공간 내에 있는 반면 이미지는 시각 적 의미공간 내에 있게 된다. 이에 이산 토큰 시퀀스 m(예를 들어 m=[w, z])은 15%의 확률로 무작위로 마스킹되 며 마스킹된 토큰 는 특수 토큰[MASK]으로 대체할 수 있다. 여기서 z는 객체 이름이 있는 영역설명, w는 단 어를 의미할 수 있다. 그리고 임베딩부는, 객체특징에서 기설정된 조건에 따라 이미지에 포함된 객체 중 하나를 마스킹하여 마스 킹된 객체특징과 객체특징의 위치정보를 임베딩할 수 있다. 한편 학습부는 임베딩부에서 임베딩된 객체특징 임베딩, 언어특징 임베딩을 서로 연결(concatenate) 하여 객체특징과 언어특징 간의 관계를 학습할 수 있다. 이를 위해 학습부는 임베딩된 특징들을 학습모델 에 입력하여 사전학습을 수행할 수 있다. 여기서 학습모델은 트랜스포머 네트워크를 포함할 수 있다. 학습부는 두가지 방법으로 동시에 학습을 수행할 수 있다. 두가지 방법 중 하나의 방법은, 임베딩부 에서 임베딩된 질문 언어특징 임베딩과 마스킹된 영역설명 언어특징 임베딩을 포함하는 언어특징 임베딩을 객체 특징 임베딩과 서로 연결할 수 있다. 그리고나서 객체특징과 언어특징에서 마스킹되지 않은 단어토큰, 즉 주변 단어에 기초하여 마스킹된 단어토큰을 예측하는 학습을 수행할 수 있다. 구체적으로 학습부는 영역설명의 단어 하나를 마스킹하고 마스킹된 단어를 예측하는 경우, 객체특징을 v, 객체 이름(label)이 있는 영역설명을 z, 그리고 단어를 w로 정의하였을 때 하기 수학식 1의 손실함수를 사용할 수 있다. [수학식 1]"}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 는 학습모델이 반환해야 하는 기댓값이고, P는 확률, D는 (v,m)으로부터 표본을 수집한 데이터를 의미 할 수 있다. 객체 이름과 단어토큰을 사용한 영역설명은 동일한 의미공간 내에 있는 반면 이미지는 시각적 의미공간 내에 있 게 된다. 이에 이산 토큰 시퀀스 m(예를 들어 m=[w, z])은 15%의 확률로 무작위로 마스킹되며 마스킹된 토큰 는 특수 토큰[MASK]으로 대체할 수 있다. 음의 로그 가능성을 최소화하여 주변 단어 및 모든 객체특징 v 를 기반으로 마스킹된 단어를 예측할 수 있다. 한편 두가지 방법 중 나머지 하나의 방법으로, 학습부는 임베딩부에서 임베딩된 언어특징 임베딩과 마스킹된 객체특징 임베딩을 서로 연결할 수 있다. 그리고나서 언어특징에 기초하여 마스킹된 객체를 예측하여 학습할 수 있다. 구체적으로 학습부는 객체특징과 객체특징에 대한 영역설명을 포함하는 데이터 세트 중에서 일정확률에 기 초하여 랜덤으로 다른 객체특징에 대한 영역설명으로 대체할 수 있다. 그리고나서 학습부는 객체특징이 마 스킹되고, 이후에 입력된 질문과 영역설명에 기초하여 마스킹된 이미지의 객체를 예측하는 과정을 수행할 수 있 다. 본 발명에서는 이미지 및 단어 매칭(VLM, visual-word matchin)의 경우 특수 토큰 [CLS]인 인코더 출력은 두 모달리티(modalities), 이미지와 단어의 공동표현을 의미할 수 있다. 따라서 영역설명 z를 객체특징 v로 그룹화하여 m'로 표시할 수 있다. 그리고 학습 중에 영역설명 z를 데이터 세 트와 다른 설명 시퀀스로 50%의 확률로 대체하여 객체특징을 마스킹할 수 있다. 그리고 m'과 w의 공동 표현인 인코더 출력은 완전히 연결된 레이어에 공급될 수 있다. 그 다음 시그모이드 함수는 0과 1 사이에 있는 출력 점 수를 예측할 수 있다. 따라서 본 발명에서는 시그모이드 함수 s(m', w)를 사용해서 출력점수를 표시하고, 최적 화를 위한 손실함수로 이진 교차 엔트로피를 사용할 수 있으며 손실함수는 하기의 수학식 2와 같을 수 있다. [수학식 2]"}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 는 상술한 바와 같이 학습모델이 반환해야 하는 기댓값이고, S는 시그모이드값, y는 출력값이며, D는 (m', w)로부터 표본을 수집한 데이터를 의미할 수 있다. 그리고 객체특징을 마스킹할 때 영역설명 자체가 자동으로 변경될 수 있다. 구체적으로 예를 들어 도 2에 도시 된 바와 같이 이미지가 오토바이를 포함하는 이미지일 때, 도 2에는 미도시되었지만 영역설명에 포함된 주어가 강아지로 바뀐다고 가정하면, 해당 이미지와 변경된 영역설명 간의 매칭확률 점수가 20%로 산출되면 실제로는 매칭을 수행하지 않지만, 학습을 하는 도중 주어로 오토바이를 포함하는 영역설명에 대해서는 매칭확률 점수가 90& 이상이 나오게 되므로 서로 매칭될 수 있는 것이다. 이러한 과정의 학습이 완료되면 이미지와 질문-답변 사이에 올바른 정렬이 만들어지고 관계 맥락을 포착할 수 있게 된다. 따라서 영역설명을 활용하여 이미지와 질문-답변을 매핑하는 다리 역할을 할 수 있게 된다. 이러한 학습과정을 통해 학습모델이 생성되면, 생성된 학습모델을 사전학습모델로 하고, 이러한 사전학습모델에 학습데이터를 추가로 입력하여 추가학습을 수행하여 본학습모델을 생성할 수 있다. 최종적으로 생성된 학습모델 에 기초하여 학습장치는 시각적 질문에 대한 답변을 출력할 수 있게 된다. 이 때 사전학습모델의 생성을 위한 학습은 비지도학습으로 수행되고, 본학습모델의 생성을 위한 학습은 지도학습으로 수행될 수 있다. 한편 도 4는 영역설명 학습방법을 설명하기 위한 흐름도로써 본 실시예에 따른 영역설명 학습방법은 도 1 내지 도 3에 도시된 영역설명 학습장치와 실질적으로 동일한 구성 상에서 수행되므로, 영역설명 학습장치 와 반복되는 설명은 생략하기로 한다. 본 발명의 영역설명 학습방법은 추출단계(S110), 위치인코딩단계(S130), 임베딩단계(S150) 및 학습단계(S170)를 포함할 수 있다. 추출단계(S110)는 입력된 이미지, 영역설명 및 질문으로부터 객체특징 및 언어특징을 추출하는 단계일 수 있다. 여기서 영역설명은, 입력된 이미지의 영역 내에 있는 객체에 대한 이름정보 및 설명정보를 포함하여 이미지에 포함된 객체를 설명하는 것일 수 있다. 그리고 추출단계(S110)에서는, 영역설명 및 상기 질문에 포함되는 문장을 단어단위로 토큰화하여 언어특징을 추 출할 수도 있다. 위치인코딩단계(S130)는 객체특징 및 언어특징 각각의 위치정보를 추가하는 단계일 수 있다. 객체특징에 대한 위치정보는 이미지 영역 내에서 객체특징의 위치이고, 언어특징에 대한 위치정보는 문장구조 내에서 해당 단어 의 위치를 포함할 수 있다. 한편 임베딩단계(S150)는 추출단계(S110)에서 추출된 객체특징 및 언어특징, 위치인코딩단계(S130)에서 추가된 객체특징의 위치정보와 언어특징의 위치정보를 기설정된 조건에 따라 결합하여 임베딩하는 단계일 수 있다. 이러한 임베딩단계(S150)에서는, 객체특징과 객체특징의 위치정보를 결합하고, 언어특징과 언어특징의 위치정보 를 결합하여 임베딩하고 임베딩된 객체특징 임베딩과 언어특징 임베딩을 학습단계(S170)로 전달할 수 있다. 여기서 언어특징 임베딩은 영역설명에 대한 언어특징과 해당 언어특징의 위치정보를 결합하여 임베딩된 영역설 명 언어특징 임베딩, 그리고 질문에 대한 언어특징과 해당 언어특징의 위치정보를 결합하여 임베딩된 질문 언어 특징 임베딩을 포함할 수 있다. 그리고 임베딩단계(S150)는 영역설명에 대한 언어특징에서 기설정된 조건에 따라 선택된 하나의 단어토큰을 마 스킹하여 마스킹된 영역설명의 언어특징과 언어특징의 위치정보를 결합하여 임베딩하는 제1 임베딩단계를 포함 할 수 있다. 여기서 기설정된 조건이라 함은 상술한 바와 같이 단어토큰들 중 하나의 단어가 15%의 확률로 무작 위로 마스킹되는 조건일 수 있다. 또한 임베딩단계(S150)는 객체특징에서 기설정된 조건에 따라 이미지에 포함된 객체 중 하나를 마스킹하여 마스 킹된 객체특징과 해당 객체특징의 위치정보를 결합하여 임베딩하는 제2 임베딩단계를 더 포함할 수도 있다. 이러한 임베딩단계(S150)에서는 추출된 객체특징과 객체특징의 위치를 선형투영해서 연결하여 임베딩할 수 있다. 그리고 특징 위치에 기초하여 객체특징에 대한 영역설명을 획득하고, 객체특징과 영역설명을 연결하여 임베딩할 수도 있다. 한편 학습단계(S170)는 임베딩단계(S150)에서 임베딩된 객체특징 임베딩, 언어특징 임베딩을 서로 연결 (concatenate)하여 객체특징과 언어특징 간의 관계를 학습하는 단계일 수 있다. 이를 위해 학습단계(S170)에서는 질문 언어특징 임베딩 및 제1 임베딩단계에서 마스킹된 영역설명 언어특징 임 베딩을 포함하는 언어특징 임베딩을 객체특징 임베딩과 서로 연결할 수 있다. 그리고 학습단계(S170)는 객체특징과 언어특징에서 마스킹되지 않은 단어토큰에 기초하여 마스킹된 단어토큰을 예측하여 학습하는 제1 학습단계를 포함할 수 있다. 또한 학습단계(S170)에서는 언어특징 임베딩과 제2 임베딩단계에서 마스킹된 객체특징 임베딩을 서로 연결할 수 있다. 그리고 학습단계(S170)는 언어특징에 기초하여 상기 마스킹된 객체를 예측하여 학습하는 제2 학습단계를 포함할 수 있다. 따라서 본 실시예에 따른 학습단계(S170)는 두 가지의 학습방법을 수행하게 하게 되는데, 이상의 제1 학습단계 및 제2 학습단계는 동시에 수행될 수 있다. 또한 본 실시예에 따른 영역설명 학습방법을 수행하여 생성된 학습모델을 사전학습모델로 하고, 이러한 사전학 습모델에 다른 학습 데이터를 추가로 입력받아 추가학습을 수행하여 본학습모델을 생성하는 단계를 더 포함할 수도 있다. 이 때 사전학습모델의 생성을 위한 학습은 상술한 바와 같이 비지도학습으로 수행되지만, 본학습모 델의 생성을 위한 학습은 지도학습으로 수행될 수 있다. 이와 같은 본 발명의 영역설명 학습방법은 다양한 컴퓨터 구성요소를 통하여 수행될 수 있는 프로그램 명령어의 형태로 구현되어 컴퓨터 판독 가능한 기록 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능한 기록 매체는 프로 그램 명령어, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터 판독 가능한 기록 매체에 기록되는 프로그램 명령어는 본 발명을 위하여 특별히 설계되고 구성된 것들이거니와 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능한 기록 매체의 예에는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD- ROM, DVD 와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 ROM, RAM, 플래시 메모리 등과 같은 프로그램 명령어를 저장하고 수행하도록 특별히 구성된 하드웨 어 장치가 포함된다. 프로그램 명령어의 예에는, 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사 용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함된다. 상기 하드웨어 장치는 본 발명에 따른 처 리를 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 이하에서는 본 발명의 학습장치를 통해 학습을 수행한 경우, 적절한 정렬이 생성되고 모델의 성능이 향상 되었는지를 확인하기 위한 실험과정 및 결과에 대해 설명하기로 한다. 본 발명의 학습을 통해 생성된 학습모델을 다른 모델과 비교하였으며, Test-Dev 및 Test-Std에 대한 결과는 모 두 VQA 평가 서버에서 얻었으며, VQA v2.0 데이터 세트에 대한 실험을 수행하였는데, 이는 이미지에 대한 개방 형 질문이 포함된 널리 사용되는 데이터 세트이다. 또한 데이터 세트에 사용된 이미지는 MSCOCO 이미지 데이터 세트에서 가져왔으며, 데이터 세트는 학습(83k 이미지가 있는 444k 질문), 검증(41k 이미지가 있는 214k 질문), 테스트(81k 이미지가 있는 448k 질문)으로 나뉜다. 그리고 질문에 대해 3,129개의 공유된 답변 세트가 제공되며, 모델은 각 질문에 대한 공유 집합에서 해당 답변을 선택하도록 하였다. 또한 파인튜닝(fine-tuning)동안 텍스트(질문 및 객체 이름이 있는 영역설명)과 이미지 특징을 연결하여 하나의 입력 시퀀스를 형성하였고, [CLS] 출력은 선형 분류기로 공급되고 이진 교차 엔트로피 손실로 분류기를 학습하 였다. 그리고 사람 10명의 답변과의 관련성을 기반으로 각 답변후보에 소프트 타겟 점수를 할당하고, AdamW를 옵티마 이저(optimizer)로 간주하고 학습률을 5e-5로 설정하고 가중치 감쇠를 0.01로 설정하였으며, 추론에서 답변 예 측을 위해 softmax 함수를 사용하였다. 하기의 표 1은 본 발명의 학습모델과 다른 모델을 비교하여 실험한 결과로써, 표 1을 보면 본 발명의 영역설명 으로 모델(our)을 사전학습하면 모델의 성능이 크게 향상되어 다른 최첨단 모델을 능가한다는 것을 알 수 있다. 특히 학습동안 트랜스포머의 자기주의 메커니즘의 도움으로 영역설명을 사용하는 것은 서로 다른 모달리티인 이 미지와 텍스트의 적절한 정렬에 중요한 것임을 알 수 있다. [표 1]"}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "본 발명의 학습모델을 구현하는 세부사항으로, Pytorch 프레임워크를 기반으로 구현하였으며, 모델을 위해 BERT 기반(base)를 트랜스포머 백본(backbone)으로 간주하였고, 더 빠른 학습을 위해 분산 교육에 Nvidia Apex를 사 용하였으며, Nvidia GPU에서 사전학습 및 파인튜닝을 실행하였다. 또한 본 발명의 모델은 BERT 기반과 유사한 구성을 사용하여 사전훈련되었다. 객체특징 차원은 2054이며, 이는 객체특징과 특징 위치를 모두 연결한 것이다. 특징 위치가 있는 객체특징은 Faster-RCNN+ResNet 101로 추출하였고, 영역설명에는 COCO 캡션을 사용하 였으며, 상술한 바와 같이 이미지 하나당 4~6개의 영역설명을 사용하였다. 도 5는 본 학습장치가 질문에 대한 답변을 생성할 때, 학습모델이 집중하는 부분을 보여주기 위한 도면이 다. 본 발명에 따른 영역설명 학습방법의 정성적 분석을 위해 가우시간 블러(Gaussian blur)를 사용하여 이미지 에서 집중하는 부분을 시각화하였는데, 이는 주어진 질문입력에 대해 학습모델의 주의메커니즘이 이미지에서 어 느 부분에 주의를 기울이는지를 파악하기 위함이다. 그리고 분석을 위해 무작위로 이미지를 선택하고 이에 대해 다양한 질문을 입력하였다. 그 결과 도 5에 에 도시 된 바와 같이 본 발명의 학습장치의 학습모델은 이미지의 해당 부분에 주목하여 답변을 제시할 수 있다. 구체적으로 도 5 (a)은 질문에 기초하여 이미지에서 카우치 부분에 집중하는 모습이고, 도 5 (b)는 질문에 기초 하여 이미지에 포함된 인물들의 얼굴에 집중하는 모습, 도 5 (c)는 질문에 기초하여 이미지에서 컵에 집중하는 모습으로, 집중하는 부분에 도시된 바와 같이 블러처리를 통해 시각화하였다. 또한 비판적 사고가 필요한 질문 에 대해 모델을 테스트했으며, 매번 올바른 답변을 생성하는 것을 확인하였다. 이것은 더 나은 정렬을 생성하고 모델의 언어 이해를 향상시키기 때문에 영역설명을 이용한 사전학습의 중요성을 의미할 수 있다. 또한 학습하는 동안 영역설명(RD) 및 객체 이름(OL, Object Label) 추가의 영향을 연구하기 위해 모델을 사전학 습하고 COCO 캡션(caption)에서 얻은 영역설명을 활용하였다. 하기의 표 2는 본 발명의 학습모델과 비교를 위해 BASELINE 모델을 사용한 경우의 결과이다. [표 2]"}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이상에서와 같이 객체 이름으로만 학습을 수행하는 경우(BASELINE+OL)의 BASELINE 모델의 성능은 그렇지 않은 경우(No RD with OL)에 비해 성능이 약간 증가하였지만, 객체 이름과 영역설명을 앵커 포인트로 사용한 경우 (BASELINE+RD+OL)에는 적절한 정렬이 생성되고 모델의 성능이 더 향상됨을 알 수 있다. 본 발명에 따른 구성요소들은 물리적인 구분이 아니라 기능적인 구분에 의해서 정의되는 구성요소들로서 각각이 수행하는 기능들에 의해서 정의될 수 있다. 각각의 구성요소들은 하드웨어 또는 각각의 기능을 수행하는 프로그램 코드 및 프로세싱 유닛으로 구현될 수 있을 것이며, 두 개 이상의 구성요소의 기능이 하나의 구성요소에 포 함되어 구현될 수도 있을 것이다. 따라서 이하의 실시예에서 구성요소에 부여되는 명칭은 각각의 구성요소를 물 리적으로 구분하기 위한 것이 아니라 각각의 구성요소가 수행되는 대표적인 기능을 암시하기 위해서 부여된 것 이며, 구성요소의 명칭에 의해서 본 발명의 기술적 사상이 한정되지 않는 것임에 유의하여야 한다. 이상에서는 본 발명의 다양한 실시예에 대하여 도시하고 설명하였지만, 본 발명은 상술한 특정의 실시예에 한정"}
{"patent_id": "10-2021-0182951", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 발명이 속하는 기술분야에서 통상 의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사 상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2021-0182951", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 학습장치의 구성을 설명하기 위한 블록도, 도 2는 본 발명의 일 실시예에 따른 학습모델의 구조를 설명하기 위한 도면, 도 3은 본 발명의 학습장치에 입력되는 영역설명을 포함하는 이미지를 설명하기 위한 도면, 도 4는 본 발명의 영역설명 학습방법을 설명하기 위한 흐름도, 그리고, 도 5는 본 발명의 학습장치가 답변을 생성할 때 학습모델이 집중하는 부분을 설명하기 위한 도면이다."}
