{"patent_id": "10-2020-0063124", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0146089", "출원번호": "10-2020-0063124", "발명의 명칭": "대화 스타일링 서비스를 위한 다중 페르소나 모델 생성 방법 및 이를 이용한 대화 스타일링", "출원인": "주식회사 케이티", "발명자": "이제훈"}}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치에서 대화 스타일링 서비스를 위한 다중 페르소나 모델을생성하는 방법으로서,대량의 텍스트 데이터와 상기 대량의 텍스트 데이터를 대표하는 각각의 페르소나 정보를 사전 학습 모델을 사용하여 학습하여 각각의 페르소나를 구분짓는 상기 텍스트 데이터에 대한 문맥 데이터를 추출하는 단계,페르소나 별 문맥 데이터를 제1 생성자 모델(Generative model)에 입력하여 상기 문맥 데이터의 페르소나에 대응하는 제1 변형문 데이터를 추출하는 단계,상기 제1 변형문 데이터를 제2 생성자 모델에 입력하여 페르소나가 반영되지 않은 제1 원문 데이터를 추출하는단계, 그리고상기 제2 생성자 모델이 추출한 데이터를 입력 데이터로 사용하여 상기 제1 생성자 모델을 반복 학습하고 상기제1 생성자 모델이 추출한 데이터를 입력 데이터로 사용하여 상기 제2 생성자 모델을 반복 학습하는 단계를 포함하고, 상기 제1 생성자 모델은, 상기 대화 스타일링 서비스를 위한 다중 페르소나 모델로 사용되는, 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서, 상기 제1 변형문 데이터를 추출하는 단계 이후,판별자 모델(Discriminator model)에 상기 제1 변형문 데이터를 입력하여 상기 제1 변형문 데이터의 페르소나정보를 추출하고 상기 제1 변형문 데이터가 상기 추출한 페르소나 정보에 대응하는 실제 변형문 데이터인지 판별하는 단계를 더 포함하고,상기 반복 학습하는 단계는,상기 판별자 모델의 판별 결과를 상기 제1 생성자 모델에 입력하여 상기 제1 변형문 데이터가 상기 추출한 페르소나 정보에 대응하는 실제 변형문 데이터에 최대한 가깝도록 손실값(loss)을 학습하는, 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에서, 상기 판별자 모델은, 상기 사전 학습 모델이 사용되는, 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에서, 상기 제1 변형문 데이터를 추출하는 단계 이전에,임의의 제2 원문 데이터와 상기 제2 원문 데이터에 페르소나가 반영된 제2 변형문 데이터로 구성된 데이터 쌍을사전 학습하여 상기 제1 생성자 모델 및 상기 제2 생성자 모델을 생성하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2021-0146089-3-제4항에서, 상기 생성하는 단계는,상기 제2 원문 데이터 및 상기 제2 변형문 데이터를 토큰화하고, 페르소나와 관련된 토큰에 마스킹을 하고, 마스킹된 토큰을 대상으로 사전 학습을 수행하는, 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에서, 상기 토큰화는, 형태소 단위로 이루어지는, 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에서, 상기 생성하는 단계는,객체명(Named-Entity)에 해당하는 토큰을 마스킹에서 제외하는, 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에서, 상기 문맥 데이터를 추출하는 단계는,상기 대량의 텍스트 데이터를 형태소 단위로 분리하는 토큰화 단계, 그리고형태소 단위로 분리된 각 토큰에 마스킹을 하고, 마스킹된 토큰에 페르소나 정보를 태깅하여 벡터화하는 임베딩단계를 포함하고,상기 임베딩 단계는,상기 각 토큰 중에서 특정 비율의 토큰에 마스킹을 취하되, 상기 특정 비율 중에서 일부는 페르소나와 관련된토큰에 마스킹을 취하고 나머지는 랜덤하게 선정한 토큰에 마스킹을 취하는, 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치에서 제공하는 대화 스타일링 방법으로서,대량의 텍스트 데이터와 상기 대량의 텍스트 데이터를 대표하는 각각의 페르소나 정보를 학습하여 추출된 상기각각의 페르소나를 구분짓는 문맥(context) 데이터를 생성적 적대 신경망(Generative Adversarial Networks,GAN)으로 학습하여 페르소나 별 텍스트 데이터를 모델링한 다중 페르소나 모델을 생성하는 단계,임의의 텍스트 데이터와 페르소나 정보를 입력받는 단계, 그리고상기 다중 페르소나 모델을 이용하여 상기 임의의 텍스트 데이터를 상기 페르소나 정보에 대응하는 스타일링 데이터로 변환하는 단계를 포함하는, 대화 스타일링 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에서,상기 변환하는 단계 이후,상기 임의의 텍스트 데이터와 상기 스타일링 데이터 간의 의미 유사도를 측정하고, 상기 의미 유사도가 임계값미만이면 미리 구축된 리플-다운 룰(ripple-down rules, RDR) 지식 베이스에 기초하여 상기 스타일링 데이터를상기 임계값을 초과하도록 보정하는 단계를 더 포함하는, 대화 스타일링 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2021-0146089-4-제9항에서,상기 생성하는 단계는,페르소나 별 문맥 데이터를 제1 생성자 모델(Generative model)에 입력하여 상기 문맥 데이터의 페르소나에 대응하는 제1 변형문 데이터를 추출하는 단계,상기 제1 변형문 데이터를 제2 생성자 모델에 입력하여 페르소나가 반영되지 않은 제1 원문 데이터를 추출하는단계, 그리고역전파(backpropagation) 알고리즘을 통하여, 상기 제2 생성자 모델이 추출한 데이터를 입력 데이터로 사용하여상기 제1 생성자 모델을 반복 학습하고 상기 제1 생성자 모델이 추출한 데이터를 입력 데이터로 사용하여 상기제2 생성자 모델을 반복 학습하는 단계를 포함하고,상기 제1 생성자 모델은, 상기 대화 스타일링 서비스를 위한 다중 페르소나 모델로 사용되는, 대화 스타일링 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에서,상기 제1 변형문 데이터를 추출하는 단계 이후,판별자 모델(Discriminator model)에 상기 제1 변형문 데이터를 입력하여 상기 제1 변형문 데이터의 페르소나정보를 추출하고 상기 제1 변형문 데이터가 상기 추출한 페르소나 정보에 대응하는 실제 변형문 데이터인지 판별하는 단계, 그리고상기 제1 변형문 데이터가 상기 페르소나 정보에 대응하는 실제 변형문 데이터에 최대한 가깝도록 손실값(loss)을 학습하는 단계를 더 포함하고,상기 제1 생성자 모델은,상기 손실값 학습이 반영되는, 대화 스타일링 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에서,상기 제1 변형문 데이터를 추출하는 단계 이전에,상기 대량의 텍스트 데이터를 형태소 단위로 분리하는 토큰화 단계, BERT(Bidirectional Encoder Representations from Transformers) 모델을 사용하여 형태소 단위로 분리된 각토큰에 마스킹을 하고 마스킹된 토큰에 페르소나 정보를 태깅하여 벡터화하는 임베딩 단계를 더 포함하고,상기 문맥 데이터는,상기 임베딩 단계를 통해 생성된 임베딩 벡터를 포함하는, 대화 스타일링 방법."}
{"patent_id": "10-2020-0063124", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에서,상기 임베딩 단계는,상기 각 토큰 중에서 특정 비율의 토큰에 마스킹을 취하되, 상기 특정 비율 중에서 일부는 페르소나와 관련된토큰에 마스킹을 취하고 나머지는 랜덤하게 선정한 토큰에 마스킹을 취하는, 대화 스타일링 방법."}
{"patent_id": "10-2020-0063124", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "대화 스타일링 서비스를 위한 다중 페르소나 모델 생성 방법은 대량의 텍스트 데이터와 상기 대량의 텍스트 데이 터를 대표하는 각각의 페르소나 정보를 사전 학습 모델을 사용하여 학습하여 각각의 페르소나를 구분짓는 상기 텍스트 데이터에 대한 문맥 데이터를 추출하는 단계, 페르소나 별 문맥 데이터를 제1 생성자 모델(Generative model)에 입력하여 상기 문맥 데이터의 페르소나에 대응하는 제1 변형문 데이터를 추출하는 단계, 상기 제1 변형 문 데이터를 제2 생성자 모델에 입력하여 페르소나가 반영되지 않은 제1 원문 데이터를 추출하는 단계, 그리고 상기 제2 생성자 모델이 추출한 데이터를 입력 데이터로 사용하여 상기 제1 생성자 모델을 반복 학습하고 상기 제1 생성자 모델이 추출한 데이터를 입력 데이터로 사용하여 상기 제2 생성자 모델을 반복 학습하는 단계를 포함 하고, 상기 제1 생성자 모델은, 상기 대화 스타일링 서비스를 위한 다중 페르소나 모델로 사용될 수 있다."}
{"patent_id": "10-2020-0063124", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 대화 스타일링 서비스를 위한 다중 페르소나 모델 생성 방법 및 이를 이용한 대화 스타일링 방법에 관한 것이다."}
{"patent_id": "10-2020-0063124", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 스피커라고 불리는 보이스 커맨드(Voice command) 디바이스 보급이 가속화되고 있다. 보이스 커맨드 디바이스는 오직 채널 조절 및 음량 조절 등 단순 명령만 수행하는 가상의 비서 역할을 주로 수행하였다. 그러 나, 동적 서비스, 개인화 서비스 등 다양한 서비스 유형을 제공하는 지능형 가상 비서로 진화하고 있다. 동적 서비스 및 개인화 서비스를 대표하는 서비스 중 하나로서 대화 스타일링 서비스가 있다. 이 서비스는 보이 스 커맨드 사용자의 성향을 분석하고 그에 맞는 답변 스타일을 제공한다. 가령, 독거 노인에게는 손녀 스타일의 답변을 제공하고 아이 사용자에게는 연령대가 같은 아이 스타일의 답변을 제공할 수 있다. 이러한 서비스를 '개 인 맞춤형 페르소나 대화 스타일링'이라고 부른다. '개인 맞춤형 페르소나 대화 스타일링'은 개인화된 인공지능 비서를 제공하기 위한 핵심기술임에도 불구하고 기 술적 한계로 인해 더딘 발전을 보이고 있다. 기존 대화 스타일링 기술에 따르면, 모든 대화 스타일에 대해 수작업으로 데이터를 구축하고, 구축된 데이터를 기반으로 패턴을 추출한 후 패턴 매칭을 통한 스타일링 서비스를 제공한다. 이때, 딥러닝 기술을 활용하여, '긍 정-부정' 그리고 '높임-낮춤' 정도의 단순한 스타일 변환 서비스를 제공한다. 그러나, 이러한 방식들은 특정 클러스터(예, 노인 그룹, 어린이 그룹 등)를 대상으로 제공할 수는 있으나, 사용 자 별로 개인화된 서비스를 제공하기에는 적합하지 않다는 한계를 지니고 있다."}
{"patent_id": "10-2020-0063124", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는 사전 학습 기반 모델과 생성적 적대 신경망(Generative Adversarial Networks, GAN)을 활용하여 학습한 다중 페르소나 모델을 생성하고 이를 이용하여 대화 스타일링 서비스를 제공 하는 방법을 제공하는 것이다."}
{"patent_id": "10-2020-0063124", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 하나의 특징에 따르면, 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치에서 대화 스타일링 서 비스를 위한 다중 페르소나 모델을 생성하는 방법으로서, 대량의 텍스트 데이터와 상기 대량의 텍스트 데이터를 대표하는 각각의 페르소나 정보를 사전 학습 모델을 사용하여 학습하여 각각의 페르소나를 구분짓는 상기 텍스 트 데이터에 대한 문맥 데이터를 추출하는 단계, 페르소나 별 문맥 데이터를 제1 생성자 모델(Generative model)에 입력하여 상기 문맥 데이터의 페르소나에 대응하는 제1 변형문 데이터를 추출하는 단계, 상기 제1 변 형문 데이터를 제2 생성자 모델에 입력하여 페르소나가 반영되지 않은 제1 원문 데이터를 추출하는 단계, 그리 고 상기 제2 생성자 모델이 추출한 데이터를 입력 데이터로 사용하여 상기 제1 생성자 모델을 반복 학습하고 상 기 제1 생성자 모델이 추출한 데이터를 입력 데이터로 사용하여 상기 제2 생성자 모델을 반복 학습하는 단계를 포함하고, 상기 제1 생성자 모델은, 상기 대화 스타일링 서비스를 위한 다중 페르소나 모델로 사용된다. 상기 제1 변형문 데이터를 추출하는 단계 이후, 판별자 모델(Discriminator model)에 상기 제1 변형문 데이터를 입력하여 상기 제1 변형문 데이터의 페르소나 정보를 추출하고 상기 제1 변형문 데이터가 상기 추출한 페르소나 정보에 대응하는 실제 변형문 데이터인지 판별하는 단계를 더 포함하고, 상기 반복 학습하는 단계는, 상기 판별 자 모델의 판별 결과를 상기 제1 생성자 모델에 입력하여 상기 제1 변형문 데이터가 상기 추출한 페르소나 정보 에 대응하는 실제 변형문 데이터에 최대한 가깝도록 손실값(loss)을 학습할 수 있다. 상기 판별자 모델은, 상기 사전 학습 모델이 사용될 수 있다. 상기 제1 변형문 데이터를 추출하는 단계 이전에, 임의의 제2 원문 데이터와 상기 제2 원문 데이터에 페르소나 가 반영된 제2 변형문 데이터로 구성된 데이터 쌍을 사전 학습하여 상기 제1 생성자 모델 및 상기 제2 생성자 모델을 생성하는 단계를 더 포함할 수 있다. 상기 생성하는 단계는, 상기 제2 원문 데이터 및 상기 제2 변형문 데이터를 토큰화하고, 페르소나와 관련된 토 큰에 마스킹을 하고, 마스킹된 토큰을 대상으로 사전 학습을 수행할 수 있다.상기 토큰화는, 형태소 단위로 이루어질 수 있다. 상기 생성하는 단계는, 객체명(Named-Entity)에 해당하는 토큰을 마스킹에서 제외할 수 있다. 상기 문맥 데이터를 추출하는 단계는, 상기 대량의 텍스트 데이터를 형태소 단위로 분리하는 토큰화 단계, 그리 고 형태소 단위로 분리된 각 토큰에 마스킹을 하고, 마스킹된 토큰에 페르소나 정보를 태깅하여 벡터화하는 임 베딩 단계를 포함하고, 상기 임베딩 단계는, 상기 각 토큰 중에서 특정 비율의 토큰에 마스킹을 취하되, 상기 특정 비율 중에서 일부는 페르소나와 관련된 토큰에 마스킹을 취하고 나머지는 랜덤하게 선정한 토큰에 마스킹 을 취할 수 있다. 본 발명의 다른 특징에 따르면, 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치에서 제공하는 대화 스타 일링 방법으로서, 대량의 텍스트 데이터와 상기 대량의 텍스트 데이터를 대표하는 각각의 페르소나 정보를 학습 하여 추출된 상기 각각의 페르소나를 구분짓는 문맥(context) 데이터를 생성적 적대 신경망(Generative Adversarial Networks, GAN)으로 학습하여 페르소나 별 텍스트 데이터를 모델링한 다중 페르소나 모델을 생성하 는 단계, 임의의 텍스트 데이터와 페르소나 정보를 입력받는 단계, 그리고 상기 다중 페르소나 모델을 이용하여 상기 임의의 텍스트 데이터를 상기 페르소나 정보에 대응하는 스타일링 데이터로 변환하는 단계를 포함한다. 상기 변환하는 단계 이후, 상기 임의의 텍스트 데이터와 상기 스타일링 데이터 간의 의미 유사도를 측정하고, 상기 의미 유사도가 임계값 미만이면 미리 구축된 리플-다운 룰(ripple-down rules, RDR) 지식 베이스에 기초하 여 상기 스타일링 데이터를 상기 임계값을 초과하도록 보정하는 단계를 더 포함할 수 있다. 상기 생성하는 단계는, 페르소나 별 문맥 데이터를 제1 생성자 모델(Generative model)에 입력하여 상기 문맥 데이터의 페르소나에 대응하는 제1 변형문 데이터를 추출하는 단계, 상기 제1 변형문 데이터를 제2 생성자 모델 에 입력하여 페르소나가 반영되지 않은 제1 원문 데이터를 추출하는 단계, 그리고 역전파(backpropagation) 알 고리즘을 통하여, 상기 제2 생성자 모델이 추출한 데이터를 입력 데이터로 사용하여 상기 제1 생성자 모델을 반 복 학습하고 상기 제1 생성자 모델이 추출한 데이터를 입력 데이터로 사용하여 상기 제2 생성자 모델을 반복 학 습하는 단계를 포함하고, 상기 제1 생성자 모델은, 상기 대화 스타일링 서비스를 위한 다중 페르소나 모델로 사 용될 수 있다. 상기 제1 변형문 데이터를 추출하는 단계 이후, 판별자 모델(Discriminator model)에 상기 제1 변형문 데이터를 입력하여 상기 제1 변형문 데이터의 페르소나 정보를 추출하고 상기 제1 변형문 데이터가 상기 추출한 페르소나 정보에 대응하는 실제 변형문 데이터인지 판별하는 단계, 그리고 상기 제1 변형문 데이터가 상기 페르소나 정보 에 대응하는 실제 변형문 데이터에 최대한 가깝도록 손실값(loss)을 학습하는 단계를 더 포함하고, 상기 제1 생 성자 모델은, 상기 손실값 학습이 반영될 수 있다. 상기 제1 변형문 데이터를 추출하는 단계 이전에, 상기 대량의 텍스트 데이터를 형태소 단위로 분리하는 토큰화 단계, BERT(Bidirectional Encoder Representations from Transformers) 모델을 사용하여 형태소 단위로 분리 된 각 토큰에 마스킹을 하고 마스킹된 토큰에 페르소나 정보를 태깅하여 벡터화하는 임베딩 단계를 더 포함하고, 상기 문맥 데이터는, 상기 임베딩 단계를 통해 생성된 임베딩 벡터를 포함할 수 있다. 상기 임베딩 단계는, 상기 각 토큰 중에서 특정 비율의 토큰에 마스킹을 취하되, 상기 특정 비율 중에서 일부는 페르소나와 관련된 토큰에 마스킹을 취하고 나머지는 랜덤하게 선정한 토큰에 마스킹을 취할 수 있다."}
{"patent_id": "10-2020-0063124", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "실시예에 따르면, 사전 학습 기반 모델과 적대적 생성 신경망(GAN)을 활용하여 다중 페르소나를 학습하므로, 효 율적이다. 또한, 각 페르소나 당 서비스 제공을 위한 시간을 절감하고 정확한 스타일링 문장을 생성할 수 있다."}
{"patent_id": "10-2020-0063124", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술분야에서 통상의 지식 을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였 다. 명세서 전체에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 명세서에 기재된 \"…부\", \"…기\", \"…모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 본 발명에서 설명하는 장치들은 적어도 하나의 프로세서, 메모리 장치, 통신 장치 등을 포함하는 하드웨어로 구 성되고, 지정된 장소에 하드웨어와 결합되어 실행되는 프로그램이 저장된다. 하드웨어는 본 발명의 방법을 실행 할 수 있는 구성과 성능을 가진다. 프로그램은 도면들을 참고로 설명한 본 발명의 동작 방법을 구현한 명령어 (instructions)를 포함하고, 프로세서와 메모리 장치 등의 하드웨어와 결합하여 본 발명을 실행한다. 본 명세서에서 \"전송 또는 제공\"은 직접적인 전송 또는 제공하는 것 뿐만 아니라 다른 장치를 통해 또는 우회 경로를 이용하여 간접적으로 전송 또는 제공도 포함할 수 있다. 본 명세서에서 단수로 기재된 표현은 \"하나\" 또는 \"단일\" 등의 명시적인 표현을 사용하지 않은 이상, 단수 또는 복수로 해석될 수 있다. 본 명세서에서 도면에 관계없이 동일한 도면번호는 동일한 구성요소를 지칭하며, \"및/또는\" 은 언급된 구성 요 소들의 각각 및 하나 이상의 모든 조합을 포함한다. 본 명세서에서, 제1, 제2 등과 같이 서수를 포함하는 용어들은 다양한 구성요소들을 설명하는데 사용될 수 있지 만, 상기 구성요소들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요 소로부터 구별하는 목적으로만 사용된다. 예를들어, 본 개시의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 본 명세서에서 도면을 참고하여 설명한 흐름도에서, 동작 순서는 변경될 수 있고, 여러 동작들이 병합되거나, 어느 동작이 분할될 수 있고, 특정 동작은 수행되지 않을 수 있다. 이제, 도면을 참고하여 본 발명의 실시예에 따른 대화 스타일링 서비스를 위한 다중 페르소나 모델 생성 방법 및 이를 이용한 대화 스타일링 방법에 대하여 설명한다. 도 1은 본 발명의 실시예에 따른 대화 스타일링 장치의 구성을 나타낸 블록도이다. 도 1을 참조하면, 대화 스타일링 장치는 텍스트 데이터를 페르소나에 대응하는 스타일링 데이터로 변환하 여 제공한다. 여기서, 페르소나는 대화 시스템이 적용되는 환경 및 목적 수행을 위한 가상의 인격을 지칭한다. 예를들어, 페르소나는 아이, 성인, 노인 등과 같이 정의될 수 있고, 사투리, 표준어 등과 같이 정의될 수 있고, 개인의 말투가 될 수도 있다. 이는 단지 예시이고 이에 국한되는 것은 아니다. 대화 스타일링 장치는 다중 페르소나 모델 학습부 및 대화 스타일링 데이터 생성부를 포함한다. 다중 페르소나 모델 학습부는 대량의 텍스트 데이터와 대량의 텍스트 데이터를 대표하는 각각의 페르소나 정보를 학습하여 추출된 각각의 페르소나를 구분짓는 문맥(context) 데이터를 생성적 적대 신경망(GenerativeAdversarial Networks, 이하, 'GAN'으로 통칭함)으로 학습하여 페르소나 별 텍스트 데이터를 모델링한 다중 페 르소나 모델을 생성한다. 대화 스타일링 데이터 생성부는 다중 페르소나 모델을 이용하여, 입력받은 임의의 텍스트 데이터를 입력받 은 페르소나 정보에 대응하는 스타일링 데이터로 변환하여 출력한다. 예를들어, 대화 스타일링 데이터 생성부 는 입력받은 원문 \"저는 영화 감상을 좋아합니다.\"를 페르소나 스타일링된 변형문 \"나는 영화 보는 것을 좋아해\"로 변환할 수 있다. 도 2는 본 발명의 실시예에 따른 다중 페르소나 모델 학습부의 구조도이고, 도 3은 본 발명의 실시예에 따른 다 중 페르소나 모델 생성 방법을 나타낸 순서도이며, 도 4는 본 발명의 실시예에 따른 사전 학습 과정을 설명하는 도면이다. 도 2를 참조하면, 다중 페르소나 모델 학습부는 사전 학습 모델, 제1 생성자 모델(Generative model), 제2 생성자 모델 및 판별자 (Discriminator) 모델을 학습한다. 도 3을 참조하면, 다중 페르소나 모델 학습부는 대량의 텍스트 데이터와 대량의 텍스트 데이터를 대표하는 각각의 페르소나 정보를 입력받는다(S101). 페르소나 정보는 텍스트 데이터의 페르소나로 지정되거나 또는 선택 된 정보이다. 텍스트 데이터는 문장이다. 이때, 대량의 텍스트 데이터는 데이터 쌍(원문/변형문)이 존재하지 않는 그 어느 데이터도 활용이 가능하다. 예 를들어, 대량의 텍스트 데이터는 발화 데이터, 위키(wiki dump) 데이터, 사투리 데이터, 뉴스 데이터 및 웹 크 롤링을 통해 얻은 데이터 등을 포함할 수 있다. 페르소나 정보는 텍스트 데이터 별로 특정된 페르소나 정보로서, 이는 운용자에 의해 입력된 정보일 수 있다. 다중 페르소나 모델 학습부는 대량의 텍스트 데이터를 형태소 단위로 토큰화한다(S103). 다중 페르소나 모 델 학습부는 사전 학습 모델을 학습하기전에 입력받은 대량의 텍스트 데이터를 형태소 단위로 분리한 다. 이는 제1 생성자 모델, 제2 생성자 모델 및 판별자 모델의 학습이 형태소 단위로 이루어지 기 때문이다. 다중 페르소나 모델 학습부는 어절 단위의 BPE(Byte Pair Encoding), Word piece 등 다양한 형태소 단위의 sub word 분리 기법을 사용할 수 있다. 다중 페르소나 모델 학습부는 S103 단계에서 형태소 분석 및 전처리된 데이터를 사전 학습 모델을 이 용하여 학습한다(S105, S107). 다중 페르소나 모델 학습부는 사전 학습 모델로서 BERT(Bidirectional Encoder Representations from Transformers) 모델을 사용할 수 있다. 이외에도 RoBERTa, ALBERT 등의 사전 학습 모델을 사용할 수 있다. 다중 페르소나 모델 학습부는 형태소 단위로 분리된 각 토큰에 마스킹을 취하고(S105), 마스킹된 토큰에 페르소나 정보를 태깅하는 임베딩(Embedding)을 통해 페르소나를 구분짓는 문맥(context) 데이터, 즉, 텍스트 데이터로부터 임베딩된 정보(또는 데이터)를 추출한다(S107). S105 단계에서, 다중 페르소나 모델 학습부는 사전 학습 모델을 사용하여 형태소 단위로 분리된 각 토큰에 마스킹을 하고 마스킹된 토큰에 페르소나 정보를 태깅하여 벡터화하는 임베딩을 수행한다. 이때, S105 단계에서, 일반적인 BERT 모델의 학습 방식과 다르게 페르소나 임베딩 방식이 추가된다. 문맥 데이터는, 입력된 텍스트 데이터에 대한 문맥 정보로서, 임베딩을 통해 생성된 임베딩 벡터를 포함할 수 있다. 달리 말하면, 문맥 데이터는 텍스트 정보와 페르소나 정보를 입력받아 사전 학습 모델, 즉, BERT 모 델이 출력한 정보로서, 입력받은 텍스트에 페르소나 정보가 임베딩된 정보라 할 수 있다. S105 단계에서, 다중 페르소나 모델 학습부는 각 토큰 중에서 특정 비율의 토큰에 마스킹을 취하되, 특정 비율 중에서 일부는 페르소나와 관련된 토큰에 마스킹을 취하고 나머지는 랜덤하게 선정한 토큰에 마스킹을 취 할 수 있다. 한편, 다중 페르소나 모델 학습부는 임의의 원문 데이터와 페르소나가 반영된 변형문 데이터로 구성된 데 이터 쌍을 입력(S109)받고, 이를 형태소 단위로 토큰화한다(S111). S111 단계는 S103 단계와 동일한 방식으로 수행될 수 있다. 다중 페르소나 모델 학습부는 페르소나와 관련된 토큰에 마스킹을 취하고(S113), 마스킹된 토큰을 대상으 로 GAN 학습을 수행하여 제1 생성자 모델 및 제2 생성자 모델을 생성한다(S109). 이때, 임의의 원문데이터와 변형문 데이터는 대량의 텍스트 데이터에서 선정 또는 발췌된 것일 수 있다. S105 단계, S113 단계에서, 다중 페르소나 모델 학습부는 객체명(Named-Entity, NE)에 해당하는 토큰을 마 스킹에서 제외할 수 있다. 다중 페르소나 모델 학습부는 형태소 분석 및 마스킹을 할 수 있다. 다중 페르소나 모델 학습부는 제 1 생성자 모델 및 제2 생성자 모델에 데이터를 입력하기 전 형태소 단위로 나누기 위해 형태소 분석 을 진행한다. 그리고 채팅(chatting)성 대화와 다르게 태스크(task)성 대화에서는 스타일링시 변환되지 말아야 할 단어들이 존재한다. 가령 \"KBS 채널로 이동합니다.\"라는 문구에서 KBS는 스타일링 시에도 변하지 않아야 하 는 단어이다. 이러한 객체명(NE)들에 대해서 마스킹되지 않게 한다. 또한, S105 단계, S113 단계에서, 다중 페르소나 모델 학습부는 형태소 단위로 분리된 각 토큰에 페르소나 정보를 태깅하여 벡터화하는 임베딩을 수행한다. 또한, S105 단계, S113 단계에서, 다중 페르소나 모델 학습부는 각 토큰 중에서 특정 비율의 토큰에 마스 킹을 취하되, 특정 비율 중에서 일부는 페르소나와 관련된 토큰에 마스킹을 취하고 나머지는 랜덤하게 선정한 토큰에 마스킹을 취할 수 있다. 이때, S101 단계 ~ S107 단계와 S109 단계 ~ S1105 단계는 병렬로 이루어질 수 있다. 다중 페르소나 모델 학습부는 S101 단계 ~ S107 단계를 통한 사전 학습 결과를 제1 생성자 모델 제2 생성 자 모델에 입력하여 GAN 학습을 수행한다(S117). 구체적으로, 다중 페르소나 모델 학습부는 S107 단계에서 추출한 페르소나 별 문맥 데이터를 제1 생성자 모델에 입력하여 문맥 데이터의 페르소나에 대응하는 제1 변형문 데이터를 추출하고 제1 변형문 데이터를 제2 생성자 모델에 입력하여 페르소나가 반영되지 않은 제1 원문 데이터를 추출한다. 다중 페르소나 모델 학습부는 역전파(Back propagation) 알고리즘을 통하여 제1 생성자 모델 및 제2 생성자 모델의 학습 결과를 피드백 학습할 수 있다. 이때, 다중 페르소나 모델 학습부는 제1 생성자 모델의 입력 데이터가 제2 생성자 모델의 출력 데이터에 최대한 가깝도록 반복 학습한다. 즉, 다중 페르소나 모델 학습부는 제1 생성자 모델의 입력 데이터로 사용된 텍스트 데이터와 제2 생성자 모델 이 출력한 제1 원문 데이터가 유사해지도록 제1 생성자 모델 및 제2 생성자 모델을 반복 학습한 다. 여기서, 텍스트 데이터는 사전 학습 모델에 입력된 텍스트 데이터를 지칭한다. 다중 페르소나 모델 학습부는 제1 생성자 모델이 추출한 제1 변형문 데이터를 판별자 모델에 입 력하여 제1 변형문 데이터의 페르소나 정보를 추출하고 제1 변형문 데이터가 추출한 페르소나 정보에 대응하는 실제 변형문 데이터인지 판별하는 GAN 학습을 수행한다(S119). S117 단계 및 S119 단계에서, 다중 페르소나 모델 학습부는 판별자 모델의 판별 결과를 제1 생성자 모델에 입력하여 제1 변형문 데이터가 추출한 페르소나 정보에 대응하는 실제 변형문 데이터에 최대한 가 깝도록 손실값(loss)을 학습한다. 여기서, 제1 생성자 모델은 대화 스타일링 서비스를 위한 다중 페르소나 모델로 사용된다. 판별자 모델 은 S101 단계 ~ S107 단계에서 사용된 사전 학습 모델이 사용된다. 예를들어, 다중 페르소나 모델 학습부는 스타일링 하고자 하는 원문 텍스트 데이터가 입력되면 사전 학습 모델을 통하여 스타일링하고자 하는 페르소나를 선택하고 페르소나 임베딩(embedding)값을 부여한 후 사전 학습 모델에 의해 원문 텍스트 데이터를 임베딩하여 문맥 데이터를 획득한다. 문맥 데이터에는 사전 학습 모델을 통해 획득(capture)한 특정 페르소나의 구성(feature)이 포함되어 있다. 다중 페르소나 모델 학습부는 특정 페르소나의 구성이 포함된 문장 임베딩인 문맥 데이터를 제1 생성자 모 델에 입력하고, 이를 특정 페르소나의 변형문으로 출력한다. 이때, 변형문이 제대로 스타일링이 된 것인지, 문법적으로 오류가 없는지 판별해야 하므로, 이러한 역할을 하는 모델이 판별자 모델이다. 판별자 모델은 제1 생성자 모델에 의해 출력된 변형문을 스타일링이 더욱 잘 되도록, 문법적으로 오 류가 없도록 제1 생성자 모델을 학습시키는 역할을 한다. 판별자 모델에 의해 스타일링이 잘되고 문 법적으로 오류가 없도록 학습된 제1 생성자 모델은 의미론적으로 변형이 잘 된 것인지 확인할 수 없다. 이 를 해결하기 위해 제1 생성자 모델에 의해 변형된 변형문을 제2 생성자 모델을 통해 다시 원문으로 복원시키고 복원된 원문이 사전 학습 모델에 입력된 원문과 의미적으로 동일하도록 학습을 진행한다. 이를 통해변형 오류, 문법적 오류, 의미론적 오류를 모두 해결할 수 있다. 이상 설명한 다중 페르소나 모델 학습부의 동작에 대해 좀더 구체적으로 설명하면, 다음과 같다. 다중 페르소나 모델 학습부는 사전 학습 모델로 임베딩(embedding)된 입력 데이터를 제1 생성자 모델 에 입력하여 페르소나가 반영된 스타일링 데이터로 변형한다. 사전 학습 모델에서는 BERT 모델이 이용될 수 있다. BERT 모델은 2018년 구글에서 제안한 언어 모델로서 다량의 데이터를 비지도학습 방식으로 사전 학습한 후 특정 태스크에 맞게 지도 학습 방식으로 전이 학습하는 모델이다. 본 발명의 실시예에서는 BERT 모델을 원문, 즉, 페르소나가 결합되지 않은 일반 문장에 적용하여 특 정 페르소나의 구성(feature)을 결합한 입력 임베딩(input embedding) 값을 추출하기 위해 활용한다. BERT 모델의 주요 특징으로는 2가지의 학습 방법과 한 문장을 3가지의 임베딩으로 표현하는데 있다. 도 4를 참조하면, BERT 모델의 3가지의 임베딩 표현 방식은 토큰 임베딩(Token Embedding), 세그먼트 임베딩 (Segment Embedding), 포지션 임베딩(Position Embedding)을 포함한다. 토큰 임베딩은 각 토큰에 대한 임베딩 이고, 세그먼트 임베딩은 앞 문장은 0, 뒷 문장은 1을 부여하여 서로 이어지는 문장인지에 대한 임베딩이며, 포 지션 임베딩은 각 토큰의 위치를 나타내는 임베딩이다. 이때, 본 발명의 실시예에서는 페르소나 정보를 보다 잘 학습하기 위해 페르소나 임베딩(Persona Embedding)을 추가로 수행한다. 즉, 페르소나 임베딩은 페르소나 구분을 위한 동작이다. 페르소나(p) 임베딩은 pn=n(n=1, 2, …, n) 형태의 값을 임베딩하는 방식으로 학습한다. 예를들어, 다중 페르소 나 모델 학습부는 원문에 대해서는 각 토큰에 해당하는 모드 값이 pn=0, 1번 페르소나는 각 토큰에 해당하 는 모든 값에 pn=1을, 2번 페르소나는 모든 값에 pn=2를 부여한다. 이러한 방식을 통해 특정 페르소나의 정보가 입력으로 들어온 것을 파악한다. BERT 모델의 2가지의 학습 방식은 MLM(Masked Language Model)과 NSP(Next Sentence Prediction)를 포함한다. MLM은 특정 위치의 단어는 마스킹(masking)한 후 주변 단어를 통해 마스킹(masking)된 단어를 예측하도록 학습 하는 방식이다. NSP는 두 문장이 이어지는 문장인지 아닌지를 판별하도록 학습하는 방식이다.이때, 본 발명의 실시예에 따르면, 특정 페르소나에 대한 데이터가 입력으로 들어오면 MLM의 마스킹(masking) 방식에도 기존 방 식과 차이가 있다. 즉, 특정 페르소나의 임베딩값으로 인하여 토큰 마스킹 방식에도 차이가 있다. 기존 BERT 모델의 MLM 마스킹 방식에서는단어를 랜덤으로 마스킹(masking) 한다. 반면, 본 발명의 실시예에서는 페르소나의 특징을 더욱 잘 학습할 수 있도록 페르소나 특징이 두드러지는 토큰에 마스킹(masking)을 취한다. 다중 페르소나 모델 학습부는 페르소나의 특징이 잘 두드러지는 토큰을 발견하기 위해서 원문의 토큰 사전 과 각 페르소나의 토큰 사전을 비교한다. 이때, 원문의 토큰 사전은 대량의 텍스트 데이터를 토큰화하여 생성된 다. 페르소나의 토큰 사전은 페르소나가 반영된 데이터를 토큰화하여 생성될 수 있다. 다중 페르소나 모델 학습부는 원문의 토큰 사전과 각 페르소나의 토큰 사전을 비교하여 원문에 잘 등장하 지 않지만 각 페르소나에서는 자주 등장하는 토큰, 또는 각 페르소나에서만 등장하는 토큰을 수집하고 이를 페 르소나 특징을 구분짓는 페르소나 토큰으로 결정한다. 다중 페르소나 모델 학습부는 기존 방식과 다른 비율로 토큰 마스킹을 수행한다. 기존 MLM 마스킹 방식에 서는, 전체 토큰 사전, 즉, 원문 토큰 사전과 페르소나 토큰 사전에서 랜덤으로 15% 정도를 마스킹 토큰으로 선 택한다. 그리고 15%에 의해 선택된 토큰에 대해서도 80%의 해당하는 토큰은 그대로 마스킹을 하며, 10%는 다른 토큰으로 치환하고, 10%는 원래의 토큰으로 원복한 후 MLM 학습을 진행한다. 반면, 본 발명의 실시예에서 다중 페르소나 모델 학습부는 전체 토큰 사전에서 15%를 마스킹 토큰으로 선 택하는데, 이때, 선택된 15% 중에서 70%는 페르소나 사전에 등록된 페르소나 토큰을 선택하고 30%는 페르소나 토큰과 상관없이 원문 토큰 사전에 등록된 토큰을 랜덤하게 선택한다. 예를들어, 100개의 토큰이 존재하는 원문 토큰 사전이 있을 경우, 마스킹 대상 토큰을 선택할 때 100개의 15%인 15개의 토큰을 마스킹 토큰으로 선택한다. 15개의 토큰 중에서 페르소나 사전에 등록된 토큰을 10개 선택하고, 그 외 5개의 토큰을 랜덤하게 선택한다. 또한, 본 발명의 실시예에 따르면, 마스킹된 토큰과 유사한 토큰에 대해서도 페르소나 특징이 잘 반영될 수 있 도록 학습하기 위해 마스킹 대상인 15%의 토큰에 대해서 40%는 그대로 마스킹 토큰으로 선택하고, 나머지 40%에대해서는 마스킹 대상 토큰과 유사한 토큰으로 치환하고나머지 10%는 다른 임의의 토큰으로 랜덤 치환하며, 나 머지 10%는 원 토큰으로 복구하여 MLM 학습을 진행한다. 이때, 페르소나 토큰에만 마스킹하지 않고 랜덤 토큰도 마스킹 토큰에 선택하는 것은 기존 토큰 사전에도 등장하는 단어를 선택함으로써 바이어스(bias)가 생기지 않도 록 하기 위함이다. 이와 같이 학습된 사전 학습 모델은 페르소나 특징을 포함하고 있는 LM(Language Model)으로 활용된다. 가령, 원문 데이터가 위키 데이터(wiki dump), 뉴스 데이터(news) 등 경어체를 구사하는 데이터(예, \"저는 공부 를 가장 잘합니다.\")이고 평어체를 구사하는 페르소나 데이터(예, \"나는 공부를 대빵/짱 잘해.\")라고 가정하면, 원문 토큰 사전에는 잘 등장하지 않는 \"나는\", \"대빵/짱\", \"잘해\"의 단어들이 페르소나 토큰 사전에 자주 등장 할 것이다. 그렇다면 \"대빵/짱\", \"잘해\" 등의 단어에 대해서 마스킹토큰으로 선택되어 MLM 학습이 이루어진다. 이렇게 학습 데이터를 구축한다면, 페르소나의 특징을 구분짓는 페르소나 토큰들이 마스킹된 토큰에도 존재하고 마스킹 주변 단어(형태소/음절/단어)에도 존재하여 페르소나 특징을 추출하기에 적합한 구성이 된다. 위와 같이 학습된 BERT 모델은 문장을 임베딩하는 동시에 특정 페르소나 값에 따라 페르소나 특징 값이 임베딩 결과 값에 포함되어 있을 수 있다. 다중 페르소나 모델 학습부는 사용자의 페르소나 정보와 대량의 텍스트 데이터를 BERT 모델에 의해 학습하 여 문장 임베딩을 완성하며 이는 페르소나 특징을 구분짓는 문맥(Context) 데이터가 된다. 여기서, 문장 임베딩 은 학습 과정과는 약간의 괴리가 존재한다. 학습 과정에서는 특정 페르소나 문장 데이터와 정보 값이 일치하는데 반해, 사용자 입력 문장을 임베딩하는 과 정에서는 원문 문장 데이터와 변환하고자 하는 특정 페르소나 값이 BERT 모델의 입력으로 들어간다. 예를 들어, 학습 과정에서 \"안녕하세요, 새로운 발명입니다.\"라는 입력 문장과 원문에 해당하는 페르소나 값인 \"0\"이 입력 으로 들어간다면, 실제 BERT 모델 활용 단계에서는 위의 원 문장과 변환하고자 하는 특정 페르소나 값(원문을 뜻하는 0을 제외한 값)이 페르소나 임베딩으로 들어간다. 이러한 과정을 통해 원문과 특정 페르소나의 특징이 결합된 값, 즉, 문맥 데이터를 얻을 수 있다. 하지만 이렇게 얻어진 임베딩 값은 단지 문장과 페르소나의 특징 이 결합되었을 뿐 완벽한 스타일을 가진 변형문이 아니다. 이에 따라, 임베딩 후에 스타일링을 위한 다중 페르 소나 모델이 필요하다. 다중 페르소나 모델은 사전 학습 모델의 뒷 부분으로서, 이 모델은 적대적 생성 신경망(GAN) 아키텍 처를 기반으로 학습한다. 일반적으로, GAN 아키텍처는 판별자 네트워크와 생성자 네트워크를 서로를 속이도록 대립시켜 학습한다. 판별자 네트워크는 생성자 네트워크에서 생성한 데이터를 거짓 데이터라고 잘 판별하도록 학습하며, 생성자 네 트워크는 해당 네트워크에서 생성된 데이터를 판별자가 진짜 데이터라고 판별하도록 학습한다. 하지만 현재까지 자연어 처리 분야에서 가장 많이 활용되고 있는 네트워크인 RNN(Recurrent Neural Networks) 기반 모델은 판별자 네트워크에서 생성된 손실값이 생성자 RNN 네트워크의 마지막 셀(cell)에만 그레디언트 (gradient)가 도달하고, 이 전 셀들에 대해서는 전달되지 않아 GAN 아키텍처 학습 방식에 어려움이 존재했다. 최근에는 강화 학습 기법을 활용하여 생성자 RNN의 모든 셀에 그레디언트를 전달하는 방식이 제시되었지만 여전 히 긴 문장을 생성하는 데에는 한계를 가지고 있다. 본 발명의 실시예에서는 트랜스포머(transformer) 모델을 GAN에 접목시켜 이러한 단점을 극복한다. 트랜스포머 모델은 별다른 신경망 네트워크 없이 모든 레이어가 어텐션(attention) 모델로 구성되어 있는 모델이다. 이 모 델은 이전 상태 값을 따로 유지하지 않고 입력 문장 전체를 한번의 계산식을 통해 다음 레이어로 전달하기 때문 에 GAN의 판별자 네트워크의 손실 값이 모든 문장의 토큰에 전달 가능하다. 그리고 본 발명의 실시예에서는 생 성자 네트워크를 총 2개, 즉, 제1 생성자 모델과 제2 생성자 모델로 구성하여 원문과 변형문의데이터 쌍이 없이도 학습 가능한 모델로 구현한다. 다중 페르소나 모델 학습부는 GAN 아키텍처 학습 절차는 표 1과 같을 수 있다. 표 1"}
{"patent_id": "10-2020-0063124", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "입력 단계: [원문 A, 변형문 B, BERT 모델 M, BERT 임베딩 데이터 WM(A), 페르소나 정보 Pn, 초기 생성자 A 모델 GAB, 초기 생성자 B 모델 GBA, 초기 판별자 모델 D] 사전 학습 단계: 원문 A를 BERT 모델로 학습한 결과(E)는 다음과 같고, BERT 임베딩 데이터는 WM(A)=M(E)로 나타낼 수 있다. 원문 A → (Token embedding(A), Segment embedding(A), position embedding(A), Persona embedding(Pn))=E 사전 학습을 수식으로 나타내면 다음과 같다. 위의 식( , )을 통해 최소값에 수렴할때까지 θAB, θBA를 사전 학습한다. 최소값은 반드 시 0은 아니며, 사전에 지정된 값일 수 있다."}
{"patent_id": "10-2020-0063124", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "는 제1 생성자 모델을 학습시키기 위한 손실(loss)값이고, 는 제2 생성자 모델을 학 습시키기 위한 손실(loss)값이다. 반복학습 단계: 원문 A의 미니 배치 데이터 {a, a, …, a(n)} 변형문 B의 미니 배치 데이터 {b, b, …, b(n)} 사전 학습된 모델을 이용하여 사전 학습된 모델을 이용하여 각 손실 함수 값을 통해 θAB, θBA를 역전파 학습하고, 판별자 모델 D는 Stop Gradient이다. 는 제2 생성자 모델에 의해 변형문을 원문으로 변형한 결과이고, 는 제1 생성자 모델에 의해 원 문을 변형문으로 변형한 결과이다. 는 제1 생성자 모델과 제2 생성자 모델을 GAN 방식으로 학습 하기 위한 손실(loss)값이다. 는 제1 생성자 모델 및 제2 생성자 모델의 결과를 학습시키기 위해 판별자 모델에서 출력한 손실(loss)값이다. 본 발명의 실시예에 따른 다중 페르소나 모델 학습부는 사전 학습을 통하여 두개의 생성자, 즉, 제1 생성 자 모델과 제2 생성자 모델을 생성함으로써, 효과적으로 GAN 아키텍처를 자연어 처리에 활용할 수 있 다. 이처럼, 제1 생성자 모델 및 제2 생성자 모델을 사전 학습하는 이유는 자연어 특성상 이미지와 다르 게 zero-base에서 GAN 학습을 진행하면 형편없는 결과로 인해 판별자 모델에 의해 모두 가짜 데이터로 판별되어 모드 충돌(mode collapsing)이 발생할 수 있으나, 이를 방지하기 위해서이다. 본 발명의 실시예에 따른 GAN 학습은 DAE(Denosing Auto-encoder) 방식을 차용하여 데이터 쌍이 존재하지 않는 환경에서도 비지도 학습이 가능하도록 한다. 일반적인 GAN 기법들에는 판별자 네트워크 또한 파라미터가 수렴할 때까지 사전 학습 시키는 절차를 거치는데, 본 발명의 실시예에서는 기 학습된 BERT 모델, 즉, 사전 학습 모델을 판별자 모델로 사용한다. 사전 학습된 LM 모델, 즉, 사전 학습 모델은 모델 특성상 판별자의 역할을 수행할 수 있다. 판별자 모델은 BERT 모델을 활용하기 때문에 추가 학습을 진행하지 않는다. 그 이유는 BERT 자체의 성능도 뛰어나며, 파인-튜닝(fine-tuning)이 된다면 판별자 네트워크는 판별의 능력이 뛰어나 생성자의 네트워크를 모 두 무시할 수 있다. 이렇게 되면 GAN의 근본적인 문제점인 모드 충돌(mode collapsing)이 발생할 수 있다. 따라 서, 본 발명의 실시예에서는 사전 학습된 두 생성자 모델(113, 115)과 BERT 모델을 사용하는 판별자 모델 을 통해 GAN 학습을 진행한다. 다중 페르소나 모델 학습부는 사전 학습 모델(111, BERT)을 통해 임베딩된 원문을 제1 생성자 모델에 입력하여 특정 페르소나의 스타일링문으로 변형한다. 이렇게 변형된 문장을 통해 총 3개의 손실값을 얻을 수 있 다. 하나의 손실값은 제1 생성자 모델에 의해 생성된 변형문은 판별자 모델이 정답 데이터라고 믿게 하도 록 학습하는 손실 함수, 즉, 표 1의 번 수식에 해당한다. 이 학습 과정을 통해 변형문의 능숙도(fluency)를 보장할 수 있다. 다른 손실값은 제1 생성자 모델에 의해 생성된 변형문이 판별자 모델에 의해 정확한 페르소나로 변형 되었는지 판별하도록 학습하는 손실 함수, 즉, 표 1의 번 수식에 해당한다. 이 학습 과정을 통해 변형문이 정확한 페르소나 변형문인지 판별한다. 또 다른 손실값은 제1 생성자 모델에 의해 생성된 변형문을 제2 생성자 모델을 통해 원문으로 재변형 한다. 이 학습 과정을 통해 원문의 의미가 훼손되는 것을 방지하며, 이는 표 1의 번 식에 해당한다. 이러한 손실값들의 역할을 구어체에서 평어체 스타일로 변형하는 문장을 예를 들어 설명하면 아래 표 2와 같다. 표 2"}
{"patent_id": "10-2020-0063124", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "-원문: \"저는 영화 감상을 좋아합니다. \" 원하는 변형문: \"나는 영화 보는 것을 좋아해\" -표 1의 번식은 제1 생성자 모델에 의해 변형된 문장이 \"나는 감상영화를 좋아해\"라고 구문론 적으로 틀린 문장을 교정하는 역할 -표 1의 번식은 제1 생성자 모델에 의해 변형된 문장이 \"저는 영화 감상하는 것을 좋아함에 틀림 없습니다.\"일 경우 구문론적으로는 문제 없지만 페르소나가 다르므로, 이런 경우 페르소나 분류 손실값을 통해 특정 페르소나로 교정하는 역할 -표 1의 번식은 제1 생성자 모델에 의해 변형된 문장이 \"나는 음악 듣는 걸 좋아해\"라고 변형 된 경우, 구문론적, 페르소나 모두 적합한 문장이지만 의미적 오류가 존재하므로, 이러한 경우를 교정 하는 역할 이와 같이, 기 학습한 BERT 모델과 이 모델을 feature-based로 활용하여 후 GAN을 부착한 모델(113, 115, 11 7)은 문장의 다양성, 유창성, 의미적 오류 없이 대부분의 상황에 대해서 스타일링이 가능하다. 하지만, 확률 기반 모델은 언제나 예외 상황이 발생하며, 이러한 문제를 극복하기 위해 본 발명의 실시예에서는 GAN 학습후 RDR(Ripple Down Rules) 보정을 수행할 수 있다. 이에 대해서는 도 5 내지 도 7을 참고하여 설명한 다. 도 5는 본 발명의 실시예에 따른 다중 스타일링 데이터 생성부의 구조도이고, 도 6은 본 발명의 실시예에 따른 대화 스타일링 데이터 생성 방법을 나타낸 순서도이며, 도 7은 본 발명의 실시예에 따른 페르소나 규칙을 설명 하는 도면이다. 먼저, 도 5를 참조하면, 다중 스타일링 데이터 생성부는 제1 생성자 모델을 포함한다. 다중 스타일링 데이터 생성부는 임의의 원문과 특정 페르소나 정보가 입력되면, 이를 GAN 학습하여 페르소나가 반영된 변 형문으로 생성하여 출력한다. 이러한 다중 스타일링 데이터 생성부의 동작에 대해 구체적으로 설명하면, 도 6과 같다. 도 6을 참조하면, 다중 스타일링 데이터 생성부는 임의의 원문 데이터 및 페르소나 정보를 입력받는다 (S201). 다중 스타일링 데이터 생성부는 제1 생성자 모델에 S201 단계에서 입력받은 데이터를 입력하여 원문 에 페르소나가 반영된 변형문 데이터를 출력다(S203). 다중 스타일링 데이터 생성부는 변형문 데이터를 형태소 어절 단위로 복원한다(S205). 다중 스타일링 데이 터 생성부는 변형문 데이터를 형태소 단위로 분리할 때 \"토큰<tab>형태소\" 형태로 결과를 출력한다. 이를 활용하여 각 형태소 토큰과 형태소를 파싱(parsing)하여 자동으로 어절 복원 사전을 생성하고 학습 데이터에 등 장하지 않았던 형태소에 대해서는 어절 복원 알고리즘, 예를들어, 자소 분리 후 다시 복원하는 알고리즘을 활용 하여 어절 복원을 진행할 수 있다. 다중 스타일링 데이터 생성부는 어절 복원된 변형문을 S201 단계에서 입력된 원문 데이터와의 의미 유사도 를 측정한다(S207). 의미 유사도 측정 알고리즘은 공지된 기술을 사용한다. 다중 스타일링 데이터 생성부는 의미 유사도가 임계값을 초과하는지 판단한다(S209). 다중 스타일링 데이터 생성부는 의미 유사도가 임계값을 초과하면, S205 단계에서 복원된 변형문 데이터를 출력한다(S211). 다중 스타일링 데이터 생성부는 의미 유사도가 임계값 미만이면, 페르소나 데이터베이스(미도시)로부터 획 득한 페르소나 규칙을 적용하여 S205 단계에서 복원된 변형문 데이터를 RDR 보정(S213)한 후 출력한다(S215). 여기서, 페르소나 데이터베이스(미도시)는 제1 생성자 모델 및 제2 생성자 모델의 학습시 사용한 데 이터를 포함한다. RDR 모델은 확률 모델인 제1 생성자 모델이 특정 스코어를 넘지 못하는 변형문을 출력했을 경우 변형문의 완성도를 보장하기 위한 후처리 과정으로 사용된다. 여기서, 스코어는 여러 종류의 유사도 알고리즘이 사용될 수 있는데, 예를들어, 코사인 유사도 또는 여러 알고리즘의 앙상블이 될 수 있으나 이에 국한되는 것은 아니다. 의미 유사도를 측정할 수 있는 알고리즘이라면 제한되지 않고 사용될 수 있다. RDR은 지식베이스 구조가 트리 형태로 표현되며, 트리의 노드는 하나의 룰이 된다. RDR 보정(S213)에 대해 설명하면, 다음과 같다. 도 7을 참조하면, RDR 규칙(Rule)은 트리 방식으로 수작업으로 규칙을 작성할 필요 없이 쌍으로 구성된 데이터 코퍼스(corpus)가 있다면 자동으로 규칙을 추출할 수 있는 모델이다. 이때, RDR은 공지된 기술을 사용할 수 있 으므로, 자세한 설명은 생략한다. 본 발명의 한 실시예에 따른 RDR의 규칙 정제 방식은 아래와 같다. 다중 스타일링 데이터 생성부는 데이터 쌍으로 이루어진 데이터를 바탕으로 특정 단어 앞뒤 단어의 빈도수 를 측정하여 해당 단어 앞뒤에 자주 등장하는 단어를 첫번째 규칙(표 3의 Rule 0)으로 설정한다. 여기서, 데이 터 쌍으로 이루어진 데이터는 앞서 사전 학습 모델에서 사용된 데이터가 사용될 수 있다. 다중 스타일링 데이터 생성부는 그 후 트리 형식구조로 해당 단어 앞뒤에 특정 단어가 존재한다면 이 특정 단어를 두번째 규칙(표 3의 Rule 1)으로 설정한다. 이렇게 조건이 성립이 되면 해당 단어를 특정 단어로 치환하 는 방식의 모델이 완성된다. 실제 모델의 학습 규칙 출력 결과 예시는 아래와 같다. 표 3 Rule 0 object.tag == \"음악을\" : object.conclusion = \"음악을\" Rule 1object[-2][0] == \"시청을 원하시면\" : object.conclusion = \"노래를\" Rule 1 object[2][0] == \"것만\" : object.conclusion = \"노래를\" Rule 1 object[1][0] == \"만드는\" : object.conclusion = \"노래를\" Rule 1 object[1][0] == \"감상해\" : object.conclusion = \"노래를\" Rule 1 object[1][0] == \"감상하고\" : object.conclusion = \"노래를\"Rule 1 object[-2][0] == \"원하신다면\" : object.conclusion = \"노래를\" 여기서, Rule 0과 Rule 1을 구분하는 기준은 해당 문장에서 연속된 어구가 될 수 있다. 위의 표에서 보이는 것 과 같이 \"음악을\" 이라는 단어 뒤에 \"시청을 원하시면\"이라는 어구가 나오게 되면 \"음악을\"을 \"노래를\"로 치환 하며, 해당 규칙을 만족하지 않으면 아래의 규칙을 계속하여 탐색해 나가게 되는 구조이다. 이러한 RDR 보정은 확률 모델인 제1 생성자 모델을 통해 변환되지 않은 어절 혹은 어구에 가중치를 주어 추가 보정이 되도록 한다. 도 8은 본 발명의 실시예에 따른 대화 시스템의 구성을 나타낸 블록도이다. 도 8을 참조하면, 대화 시스템은 입력부, STT(Speech-to-Text) 모듈, 응답 생성부, 대화 스타일링 장치 및 출력부를 포함한다. 입력부는 사용자의 발화를 입력 받는다. 입력부가 사용자의 발화를 입력 받는 방식은 음성 또는 문자 가 될 수 있다. 입력부가 음성을 통해 인식하는 경우 마이크(미도시)와 음성 인식부(미도시)를 구비하고 문자를 통해 입력 받는 경우 문자를 입력할 수 있는 입력패널(미도시)을 구비할 수 있다. 또한 입력부는 카메라(미도시)를 더 구비하여 영상의 음성을 통해 입력 받거나 영상에서 인식된 문자를 통해 입력받을 수도 있 다. STT 모듈은 입력부가 음성을 통해 발화를 입력받은 경우, 음성을 문자로 변환한다. 응답 생성부는 입력부 또는 STT 모듈로부터 입력받은 문자열 또는 문장에 대응하는 응답 문장을 생성한다. 대화 스타일링 장치는 응답 생성부로부터 입력받은 응답 문장을 문자열 또는 문장을 학습 기반의 인 코더(Encoder)-디코더(Decoder)를 통해 스타일링 데이터를 생성한다. 대화 스타일링 장치의 동작에 대해서 는 도 1 내지 도 7에서 설명한 대화 스타일링 장치와 동일하므로, 자세한 설명은 생략한다. 출력부는 대화 스타일링 장치가 출력하는 스타일링 문장을 출력한다. 출력부는 디스플레이부(미도시), 음성합성부(미도시) 및 스피커(미도시)를 구비할 수 있다. 음성합성부는 문자를 음성으로 변환하는 TTS(Text-To-Speech)모듈이다. 한편, 도 9는 본 발명의 실시예에 따른 컴퓨팅 장치의 하드웨어 구성을 나타낸 블록도로서, 특히, 도 1 ~ 도 8 에서 설명한 대화 스타일링 장치(100, 270), 대화 시스템의 하드웨어 구성을 나타낸다. 도 9를 참조하면, 대화 스타일링 장치(100, 270), 대화 시스템은 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치에서, 도 1 ~ 도 8에서 설명한 본 발명의 동작을 실행하도록 기술된 명령들(instructions)이 포함된 프로그램을 실행할 수 있다. 컴퓨팅 장치의 하드웨어는 적어도 하나의 프로세서, 메모리, 스토리지, 통신 인터페이스 를 포함할 수 있고, 버스(bus)를 통해 연결될 수 있다. 이외에도 입력 장치 및 출력 장치 등의 하드 웨어가 포함될 수 있다. 컴퓨팅 장치는 프로그램을 구동할 수 있는 운영 체제(Operating System, OS)를 비 롯한 각종 소프트웨어가 탑재될 수 있다. 프로세서는 컴퓨팅 장치의 동작을 제어하는 장치로서, 프로그램에 포함된 명령들을 처리하는 다양한 형태의 프로세서일 수 있고, 예를들면, CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit) 등 일 수 있다. 메모리는 본 발명의 동작을 실행하도록 기술된 명령들이 프로세서에 의해 처리되도록 해당 프로그램을 로드할 수 있다. 메모리는 예를들면, ROM(read only memory), RAM(random access memory) 등 일 수 있다. 스토리지는 본 발명의 동작을 실행하 는데 요구되는 각종 데이터, 프로그램 등을 저장할 수 있다. 통신 인터페이스는 유/무선 통신 모듈일 수 있다.이상 기재한 바에 따르면, 다량의 로(raw) 데이터를 활용하여 기존 제시된 방법론보다 적은 학습 데이터로 스타 일링이 가능하다. 단순 변환 뿐 아니라 개인화된 페르소나 스타일링을 보다 빠르게 적용할 수 있다. 기 학습된 모델을 통한 문장의 다양성, GAN 로직을 활용한 문장의 유창성 보장이 가능하다. 최종으로, 기존 데 이터보다 적은 페르소나 데이터를 통해 더욱 복잡한 문장 구조의 개인화된 페르소나 스타일링이 가능하다. 또한, 기존 발명보다 더 높은 성능을 보장하며 RDR 후처리 과정을 통해 서비스에 바로 적용할 수 있다. 이상에서 설명한 본 발명의 실시예는 장치 및 방법을 통해서만 구현이 되는 것은 아니며, 본 발명의 실시예의 구성에 대응하는 기능을 실현하는 프로그램 또는 그 프로그램이 기록된 기록 매체를 통해 구현될 수도 있다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속하는 것이다."}
{"patent_id": "10-2020-0063124", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 대화 스타일링 장치의 구성을 나타낸 블록도이다. 도 2는 본 발명의 실시예에 따른 다중 페르소나 모델 학습부의 구조도이다. 도 3은 본 발명의 실시예에 따른 다중 페르소나 모델 생성 방법을 나타낸 순서도이다. 도 4는 본 발명의 실시예에 따른 사전 학습 과정을 설명하는 도면이다.도 5는 본 발명의 실시예에 따른 다중 스타일링 데이터 생성부의 구조도이다. 도 6은 본 발명의 실시예에 따른 대화 스타일링 데이터 생성 방법을 나타낸 순서도이다. 도 7은 본 발명의 실시예에 따른 페르소나 규칙을 설명하는 도면이다. 도 8은 본 발명의 실시예에 따른 대화 시스템의 구성을 나타낸 블록도이다. 도 9는 본 발명의 실시예에 따른 컴퓨팅 장치의 하드웨어 구성을 나타낸 블록도이다."}
