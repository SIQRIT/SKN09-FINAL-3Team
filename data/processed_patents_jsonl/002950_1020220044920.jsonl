{"patent_id": "10-2022-0044920", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0146221", "출원번호": "10-2022-0044920", "발명의 명칭": "규칙기반 및 딥러닝기반 인공지능 컴퓨터 그래픽 게임 프로그래밍 시스템", "출원인": "주식회사 아군", "발명자": "이준서"}}
{"patent_id": "10-2022-0044920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자가 작성한 코딩블럭을 입력받는 입력모듈; 및상기 코딩블럭을 미리 설정된 프로그래밍 언어로 변환하여 실행코드를 생성하고, 상기 실행코드에 포함된 환경정보 및 객체정보에 기초하여 게임 시뮬레이션 환경을 구축하고, 상기 게임 시뮬레이션 과정에서 생성되는 결과데이터에 기초하여 학습 데이터를 생성하며, 미리 설정된 알고리즘에 기초하여 상기 학습 데이터를 학습하여 상기 시뮬레이션 환경을 업데이트하는 서버 시스템을 포함하는 규칙기반 및 딥러닝기반 인공지능 컴퓨터 그래픽 게임 프로그래밍 시스템."}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "사용자가 작성한 코딩블럭을 입력받는 입력모듈; 및 상기 코딩블럭을 미리 설정된 프로그래밍 언어로 변환하여 실행코드를 생성하고, 상기 실행코드에 포함된 환경정보 및 객체정보에 기초하여 게임 시뮬레이션 환경을 구축하 고, 상기 게임 시뮬레이션 과정에서 생성되는 결과 데이터에 기초하여 학습 데이터를 생성하며, 미리 설정된 알 고리즘에 기초하여 상기 학습 데이터를 학습하여 상기 시뮬레이션 환경을 업데이트하는 서버 시스템을 포함하는 인공지능 교육 목적의 컴퓨터 그래픽 게임 프로그래밍 시스템을 제공한다."}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 프로그래밍 시스템에 관한 것으로서, 구체적으로는 인공지능(artificial intelligence: AI)에 대한 교육을 사용자가 경험할 수 있는 규칙기반 및 딥러닝기반 인공지능 컴퓨터 그래픽 게임 프로그래밍 시스템에 관 한 것이다."}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "AI이란, 인간의 지능으로 할 수 있는 문장이해, 영상인식, 음성인식, 학습 등을 컴퓨터가 실행하는 영역이다. AI는 증강현실, 사물인터넷, 에지 컴퓨팅, 디지털트윈 등과 함께 이용되어 고도로 통합된 스마트 공간을 제공할 것이며, 궁극적으로, 애플리케이션의 기능적 측면과 비기능적 측면을 모두 자동화하는 매우 고도화된 인공지능 주도 개발 환경이 비전문가들도 인공지능 관련 도구를 이용하여 자동적으로 새로운 솔루션을 만들어낼 수 있는 시민 애플리케이션 개발자의 새 시대를 열 것이다. 비전문가들이 코딩 없이 애플리케이션을 만들 수 있게 해주 는 툴은 새로운 것이 아니지만, 우리는 인공지능 주도 시스템이 새로운 수준의 유연성을 제공할 것이다."}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 실시예는 컴퓨터 그래픽 게임, 예를 들어 축구 게임에 AI를 접목시키고 당해 접목을 위해 사용자에게 프로그래밍 또는 코딩 환경을 제공함으로써 AI 프로그래밍 또는 코딩에 대한 교육 경험을 사용자에게 제공하는 시스템을 제공한다."}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시예에 따르면, 사용자가 작성한 코딩블럭을 입력받는 입력모듈; 및 상기 코딩블럭을 미리 설정된 프로그래밍 언어로 변환하여 실행코드를 생성하고, 상기 실행코드에 포함된 환경정보 및 객체정보에 기초하여 게임 시뮬레이션 환경을 구축하고, 상기 게임 시뮬레이션 과정에서 생성되는 결과 데이터에 기초하여 학습 데이 터를 생성하며, 미리 설정된 알고리즘에 기초하여 상기 학습 데이터를 학습하여 상기 시뮬레이션 환경을 업데이 트하는 서버 시스템을 포함하는 규칙기반 및 딥러닝기반 인공지능 컴퓨터 그래픽 게임 프로그래밍 시스템을 제 공한다."}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예에 따르면 컴퓨터 그래픽 게임, 예를 들어 축구 게임에 AI를 접목시키고 당해 접목을 위해 사 용자에게 프로그래밍 또는 코딩 환경을 제공함으로써 AI 프로그래밍 또는 코딩에 대한 교육 경험을 사용자에게 제공할 수 있다."}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 다양한 실시예가 첨부된 도면을 참조하여 기재된다. 그러나, 이는 본 발명을 특정한 실시 형 태에 대해 한정하는 것이 아니며, 본 발명의 실시예의 다양한 변경(modification), 균등물(equivalent), 및/또 는 대체물(alternative)을 포함하는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해 서는 유사한 참조 부호가 사용될 수 있다. 본 문서에서, “가진다”, “가질 수 있다”, “포함한다”, 또는 “포함할 수 있다” 등의 표현은 해당 특징 (예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는 다. 본 문서에서, “A 또는 B”, “A 또는/및 B 중 적어도 하나”, 또는 “A 또는/및 B 중 하나 또는 그 이상” 등 의 표현은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, “A 또는 B”, “A 및 B 중 적어도 하나”, 또는 “A 또는 B 중 적어도 하나”는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 다양한 실시 예에서 사용된 “제1”, “제2”, “첫째”, 또는 “둘째” 등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없이 수식할 수 있고, 해당 구성요소들을 한정하지 않는다. 예를 들면, 제1 사용자 기기와 제2 사용자 기기는, 순서 또는 중요도와 무관하게, 서로 다른 사용자 기기를 나타낼 수 있다. 예를 들면, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 바꾸어 명명될 수 있다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 “(기능적으로 또는 통신적으로) 연결 되어((operatively or communicatively) coupled with/to)” 있다거나 “접속되어(connected to)” 있다고 언 급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성 요소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제1 구성요소)가 다른 구 성요소(예: 제2 구성요소)에 “직접 연결되어” 있다거나 “직접 접속되어” 있다고 언급된 때에는, 상기 어떤 구성요소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제3 구성요소)가 존재하지 않는 것으로 이해될 수 있 다. 본 문서에서 사용된 표현 “~하도록 구성된(또는 설정된)(configured to)”은 상황에 따라, 예를 들면, “~에 적합한(suitable for)”, “~하는 능력을 가지는(having the capacity to)”, “~하도록 설계된(designed to) ”, “~하도록 변경된(adapted to)”, “~하도록 만들어진(made to)”, 또는 “~를 할 수 있는(capable of)” 과 바꾸어 사용될 수 있다. 용어 “~하도록 구성(또는 설정)된”은 하드웨어적으로 “특별히 설계된 (specifically designed to)”것 만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, “~하도록 구성 된 장치”라는 표현은, 그 장치가 다른 장치 또는 부품들과 함께 “~할 수 있는” 것을 의미할 수 있다. 예를 들면, 문구 “A, B, 및 C를 수행하도록 구성(또는 설정)된 프로세서”는 해당 동작을 수행하기 위한 전용 프로 세서(예: 임베디드 프로세서), 또는 메모리 장치에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor: 예를 들어 CPU 또는 application processor)를 의미할 수 있다. 본 문서에서 사용된 용어들은 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 다른 실시 예의 범위를 한 정하려는 의도가 아닐 수 있다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명의 기술 분야에서 통 상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 일반적으로 사용되는 사전 에 정의된 용어들은 관련 기술의 문맥 상 가지는 의미와 동일 또는 유사한 의미를 가지는 것으로 해석될 수 있 으며, 본 문서에서 명백하게 정의되지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 경우 에 따라서, 본 문서에서 정의된 용어일지라도 본 발명의 실시 예들을 배제하도록 해석될 수 없다. AI는 학습과 문제 해결 등 인간의 능력과 관련된 기능을 모방하여 실행하는 인공적인 인간의 지능을 가리킨다. AI는 규칙들을 지식으로 구성하는 전문가 시스템과 알고리즘을 스스로 학습하고 습득하는 기계학습 등으로 구분 될 수 있다. 본 개시에서는 규칙기반의 AI 축구 예제코드와 학습기반의 예제코드를 제공한다. AI에서 머신러닝(Machine Learning)은 중요한 분야이다. AI는 도1과 같이 머신러닝을 포함하고, 머신러닝은 딥 러닝(Deep Learning)을 포함한다고 할 수 있다. 머신러닝은 주어진 데이터를 분석하여 데이터의 규칙과 특징을 찾을 수 있도록 스스로 학습(훈련)하는 기법이다. 사람이 학습하듯 컴퓨터에 데이터를 제공하여 학습하게 함으 로써 새로운 지식을 얻어내게 하는 분야이다. 머신러닝 알고리즘의 개발은 학습(learning = training), 시험 (testing = validation), 추론(inference) 세 단계로 구성된다. 학습 단계에서는 주어진 입력 데이터를 바탕 으로 원하는 작업(task)을 위한 매핑(mapping) 함수를 얻는다. 시험 단계에서는 아직 살펴보지 않은 데이터에 앞서 학습된 매핑 함수를 적용할 수 있는지 시험해본다. 추론 단계에서는 학습된 매핑 함수를 원하는 작업의 임의의 데이터에 적용해본다. 현재 머신러닝의 여러 가지 알고리즘들이 개발되어 있다. 머신러닝의 대표적인 기법으로 지도학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning)이 있다. 지도학습은 주어진 입력에 대해 이미 정해진 출력을 학습 단계에서 준다. 지도학습에 의해 훈련된 신경망은 매핑 함수의 역할과 같이 입력에 대한 출력값을 제공한다. 그러나 비지도 학습의 경우에는 입 력만 주어진다. 비지도 학습은 다양한 입력 데이터를 그룹으로 분류하는 클러스터링 (Clustering)에 주로 사용 된다. 강화학습은 누적 보상(Cumulative Reward)을 극대화하기 위해 에이전트(Agent)가 환경(Environment)에 서 어떤 행동(Action)을 취해야 하는지에 대해 다룬다. 예를 들어, 체스를 두는 에이전트는 체스판 위의 말을움직이는 행동을 취하고 나면 보상을 받게 된다. 이 경우 체스판과 말은 에이전트와 상호작용할 수 있는 환경 (environment)으로 간주된다. 상태(State)는 체스판에 있는 말들의 현재 위치로 정의할 수 있다. 누적 보상은 상대편 말을 제거하거나 경기에서 승리하는 등의 기준으로 설계할 수 있다. 이 경우, 상대편 말이 제거되거나 승리를 거두면 양수(positive number)의 보상을 받는다. 누적되고 있는 보상(미래에 받게 되는 보상)을 최대화 하기 위해, 에이전트의 목표는 상대편 말을 제거하면서 승리 가능성을 최대화하는 움직임을 취하게 될 것이다. 강화학습 알고리즘의 성능은 기본적으로 특정 환경에 대한 상태, 행동 및 보상을 어떻게 설계하는가에 따라 달 라진다. 딥러닝은 다중 계층(Multiple-layer)을 가진 인공신경망(Artificial Neural Network)을 이용해 데이터의 특징 을 추출해 스스로 다른 상황에 대처할 수 있게 하는 머신러닝의 한 종류이다. 인공신경망은 인간이나 동물 뇌 의 신경망에 초점을 맞추어 구현되는 시스템의 총칭이다. 딥러닝을 활용한 응용 분야는 컴퓨터 비전(Computer Vision), 음성 처리(Speech Processing), 자연어 처리(Natural Language Processing) 등이 있다. 최근 딥러닝 은 구글의 Youtube, 페이스북의 News feed 및 일반적인 이미지 분석 등에 사용되며 주목받고 있다. 심층학습이 획기적인 성과를 거두게 된 이유에는 빅데이터 수집이 가능해진 환경, GPU를 활용한 컴퓨터 계산 능력 향상, 새 로운 알고리즘 개발 등이 꼽힌다. 효과적인 심층학습 알고리즘을 얻기 위해서는 대량의 데이터가 필요하다. 빅데이터가 있어도 컴퓨터의 연산 속도가 느릴 때는 이를 처리하기 어렵다. 최근에는 GPU를 사용하여 컴퓨터 연산의 병렬처리가 가능하게 되면서 단기간에 많은 데이터를 처리할 수 있게 되었다. 인간의 언어와 같이 자연어로 표현된 언어를 컴퓨터가 이해할 수 있는 형태로 만드는 과정을 자연어 처리 (Natural Language Processing, NLP)라고 한다. 문장 분류, 영상 제목작성, 기계 번역, 챗봇은 자연어 처리의 대표적인 예이다. NLP 응용 프로그램에서 주로 사용되는 인공신경망을 ‘Recurrent Neural Networks (RNN)’이 라 한다. 순환신경망은 구글 번역과 같이 일상적으로 사용되는 응용프로그램에서 자연어 처리에 자주 사용된다. RNN을 사용하는 또 다른 응용 분야는 음성처리, 특히 음성 합성 및 음성 인식이 있다. 이 분야에서 많이 사용되고 있는 응용 프로그램으로는 애플 시리(Siri)와 구글 알렉사(Alexa)가 있다. 음성인식은 음성을 문자로 변환하고, 음성 합성은 문자를 음성으로 변환하는 것이다. 음성처리는 두 손을 자유롭게 움직일 수 있 는 동시에 컴퓨터에 명령을 내릴 수 있기 때문에, 차세대 인간-컴퓨터 인터페이스로 주목받고 있다. 가정용 및 자율 주행 자동차용 AI 스피커와 음성 보조기는 음성 처리를 사용하는 응용 프로그램이다. AI 연구 초기부터 게임은 AI 성능을 측정하는 데 널리 이용되었다. 1996년 개발된 AI 체스 프로그램인 딥블루 는 세계 챔피언 카스파로프를 제치고 인간 수준 이상의 플레이를 선보였다. 체스와 Backgammon 게임에서 좋은 결과를 얻은 후, 연구자들은 AI의 바둑 게임 적용에 대해 생각하기 시작하였다. 바둑은 시작할 때 361(19x19) 개의 착점이 있고 계속 진행됨에 따라 한쪽이 대략 [361-2(n-1)]!개(n=1,...,181)의 경우의 수를 모든 착점이 채워질 때까지 진행할 수 있기 때문에, 단순한 AI로는 프로기사를 이기는 것이 불가능하다고 여겨졌다. 2016년 딥마인드가 개발한 알파고는 최고의 바둑기사 이세돌을 4:1로 꺾고 AI의 위력을 입증했다. 알파고는 서로 다른 게임 전략을 가지고 자신과 경쟁함으로써 품질을 향상하는 것을 배우는 강화 학습 알고리즘을 사용한다. 알파 고는 자기 자신과 계속해서 다른 전략을 시도하는 강화학습(Reinforcement Learning, RL) 알고리즘을 사용하였 다. 강화학습 프레임워크의 장점은 유연성으로 보드게임과 비디오게임 등의 환경에도 효과적으로 적용된다. 최근에는 강화학습과 심층 학습을 결합한 방법인 ‘Deep Q Network (DQN)’가 Atari 게임에 적용되어 성공을 거 두어 많은 관심이 집중되고 있다. 강화학습은 신경망을 이용해 딥러닝과 결합하여 상태와 행동사이의 매핑을 나타내는 정책(Policy)을 설계하는 것을 목표로 한다. DQN의 출력은 특정 상태에서 수행할 수 있는 각 행동의 Q-값(Q-value)이다. Q-값은 에이전트가 해당 행동을 취할 경우 받게 될 미래의 누적 보상을 나타낸다. 에이전 트는 향후 최대의 누적 보상을 받을 행동을 취하기를 원하므로 Q-값의 최대치를 가지는 행동을 하려고 한다. 강화학습은 상태(State), 행동(Action) 보상(Reward)의 함수로 이루어져 있다. Atari 게임을 예로 들면, 상태 (State)는 게임 화면이 되고, 보상(Reward)함수는 현재 점수로 대체 가능하며, 행동(Action)은 도2에 도시된 바 와 같이 비디오 게임 컨트롤러에서 가능한 동작이 될 것이다. 사람이 게임을 하는 방식을 심층 신경망을 이용 해 구성하면 도3과 같다. 도3에서 “컨볼루션(Convolution)” 계층은 영상의 특징들을 추출하고 “풀리 커넥티 드(Fully connected): 계층은 이를 기반으로 행동을 결정한다. 따라서, 사람이 하던 게임을 도4에 도시된 바와 같이 인공신경망이 대신하게 된다. AI Soccer 알고리즘을 개발하기 위해서 심층 학습과 결합한 강화학습 프레 임 워크를 사용할 수 있다. Atari, 바둑, 체스 게임과 AI 축구의 차이점은 축구에는 역동적으로 상호작용하는 여러 로봇이 존재한다는 점이다. Atari 게임과 비슷한 AI 축구는 도5와 같이 해석할 수 있다. 도5에서 로봇들과 공의 좌표와 방향을 기반으로 게이머가 각 로봇의 동작을 결정해야 한다. 로봇이 어디로 어 떻게 움직여야 하는지 등의 행동에 관련된 부분은 이하에서 자세히 설명될 것이다. Atari 게임의 예처럼 신경망은 각 시간 간격(time step)마다 상태정보를 동작으로 매핑하는 용도로 사용될 수 있다. AI 축구는 각 팀에서 5 명의 로봇을 제어하여 상대팀을 이기는 알고리즘을 개발하는 5:5 로봇축구게임이다. 제 공된 프로그램을 사용하여 Python 프로그래밍 언어로 게이머 프로그램을 개발할 수 있다. 게이머 프로그램은 로봇과 공 좌표를 기반으로 각 로봇의 바퀴 속도를 조절하고, 킥 및 점프 동작을 조절하여 총 5 대의 로봇을 제 어할 수 있다. AI 축구 로봇의 이동은 눈에 보이지 않지만 코드에 의해 제어할 수 있는 바퀴의 속도에 의해 결 정된다. 자세한 내용은 이하에서 설명된다. Python에 대한 기초 지식을 갖춘 게이머는 자신만의 로봇 축구 전 략을 만들 수 있다. AI 축구 실행을 위해서는 예를 들어 다음과 같은 컴퓨터 하드웨어 사양이 요구된다. - 최소 클록 속도가 2GHz 이상의 dual-core CPU 와 2GB 이상의 RAM - NVIDIA 또는 AMD OpenGL(version 3.3 이상) 지원이 가능한 그래픽 어댑터와 최소 512MB 이상의 그래픽 RAM을 가진 그래픽카드 GPU를 사용한 심층학습을 이용하고자 한다면 NVIDIA 그래픽 카드를 사용하고, 제공되는 CUDA, cuDNN library를 설치하는 것이 바람직하다. GPU를 사용하지 않더라도 딥러닝 라이브러리 설치가 가능하다. 사용 가능한 컴퓨 터의 운영체제(OS)는 예를 들어 다음과 같다. - Windows 8.1 과 Window 10 - Ubuntu Linux OS(16.04 이후) AI Soccer simulator를 설치하기 위해서는 다음의 단계를 거쳐야 한다. 1) Webots simulator를 설치한다. 2) Python interpreter를 설치한다. 3) AI Soccer simulator 파일을 다운로드한다. 4) 심층학습 라이브러리 설치 만약 Python interpreter가 이미 설치되어 있다면, 단계 2)를 생략해도 무방하다. 게이머가 심층학습 예제 코 드를 사용할 계획이라면 딥러닝 라이브러리를 설치하는 추가 단계가 필요하다. 심층학습 예제코드를 실행하려 면, 2-3-4(WIDOWS) 또는 2-4-4(LINUX)의 “심층학습 라이브러리 설치”에 있는 심층학습 라이브러리의 설치를 따라야 한다. 심층학습 예제 코드에서는 PyTorch가 주요 딥러닝 라이브러리로 사용된다. 본 개시에서는 Tensorflow를 딥러닝 라이브러리의 또 다른 옵션으로 제시한다. GPU 지원 없이 딥러닝 라이브러리를 설치하려 면, 2-3-4(WINDOWS) 또는 2-4- 4(LINUX)의 “GPU를 사용하지 않는 심층학습 라이브러리 설치” 절차를 따른다. NVIDIA 그래픽 카드가 있고, 심층학습 알고리즘을 교육할 수 있는 기능을 사용하려면 2-3-4(WINDOWS) 또는 2-4- 4(LINUX)의 “GPU 를 사용하기 위한 심층학습 라이브러리 설치” 절차를 따른다. AI 축구는 다른 게임과 달리 Webots(Web에서 동작하는 Robot들을 가리킴) 시뮬레이터를 게임 환경의 기반으로 한다. 따라서 Webots 시뮬레이터를 설치하여야 AI 축구를 실행할 수 있다. 도7은 Webots 시뮬레이터의 예시로 서 Cyberbotics Ltd.에서 제공하는 Webots 시뮬레이터의 다운로드 페이지를 도시한다. Webots 시뮬레이터 설치 완료 후 Webots 시뮬레이터에 ‘Tools’ → ‘Preferences’ → ‘General’ → ‘ Startup mode’ 과정을 따르고 Start mode는 ‘Pause’로 설정한다. Python command는 ‘python’으로 설정한 다. Python command는 Python interpreter 설치에 따라 달라진다. 도8은 Webots 시뮬레이터 Preferences 선 택 창을 도시한다. Webots 시뮬레이터에 AI 축구 시뮬레이터를 구동하려면 환경변수(System Properties → Environment Variables → System Variables → New)에 Webots의 PYTHONPATH를 추가한다. Python 인터프리터를 설치한 후 컴퓨터에 설치된 Python 버전에 맞춰 PYTHONPATH를 한다. 예를 들어 다음과 같다.“PYTHONPATH = ${WEBOTS_HOME}/lib/controller/python3X” “python3x”는 설치된 파이썬 버전과 관련되어 있다. 이 과정은 도9에 도시된 바와 같다. Windows 상에서 AI 축구를 실행시키기 위해선 Python (interpreter)이 필요하다. 추천 버전은 Python 3.7이다. Python을 수동으로 설치하거나, Anaconda(기계 및 심층학습 프로그램 개발을 위한 numpy 등 필수 라 이브러리를 포함함)를 이용하여 설치하는 것이 가능하다. Python 수동 설치에 익숙하지 않은 경우, Anaconda를 이용하여 Python을 설치하길 권장한다. AI 축구 시뮬레이터는 AI 축구 게임 구현과 예제 코드가 담긴 패키지이다. AI 축구 시뮬레이터는 Webots 시뮬 레이터에 World(환경)형태로 탑재된다. https://github.com/aisoccer/aisoccer-3d/releases에 접속하여 AI 축 구 시뮬레이터를 다운로드한다. 도10은 AI 축구 시뮬레이터를 다운로드 페이지를 도시한다. Windows에서 AI 축구 시뮬레이터를 사용하려면 “v0.1 Release Windows”에 포함된 ‘aioccer-3d.zip’ 파일을 다운로드한다. AI 축구 시뮬레이터는 이미 규칙 기반 예제 코드에 대한 실행이 가능하다. 규칙 기반 예제 코드를 사용하여 설 치를 테스트할 수 있다. 한편, 심층 학습 사례를 실행하려면 심층 학습 라이브러리를 설치한다. 심층 학습 라 이브러리와 호환되는 NVIDIA 그래픽 카드가 있는 경우 다음과 같은 두 가지 옵션이 있다. - GPU 사용 없이 심층 학습 라이브러리 사용 - GPU를 사용하여 심층 학습 라이브러리 NVIDIA 그래픽 카드가 있는 경우 그래픽 카드를 사용하여 심층 학습 라이브러리를 사용하여 컴퓨터 성능을 향상 할 수 있다. NVIDIA 드라이버와 소프트웨어인 CUDA 및 cuDNN을 설치한다. 만약 GPU 지원 라이브러리를 사용하 지 않으려면 심층 학습 라이브러리를 CPU만을 이용하여 심층 학습을 실행할 수 있다. 다음과 같은 순서대로, 필요한 프로그램들을 설치할 것이다. - NVIDIA DRIVER - CUDA - cuDNN - 심층 학습 라이브러리: Tensorflow, PyTorch GPU를 사용하지 않는 심층 학습 라이브러리 설치의 경우, 터미널에서 ‘pip’패키지 관리자를 통해 도11에 도시 된 바와 같이 명령어를 실행시켜 Tensorflow 및 PyTorch를 설치한다. 이 경우 PyTorch만 설치해도 무방하다. GPU를 사용하기 위한 심층 학습 라이브러리 설치의 경우, AI 축구 실행시 NVIDIA 드라이버가 설치되어 있어야 원활한 속도로 게임 시뮬레이터를 사용할 수 있다. 자신의 컴퓨터에 NVIDIA GPU가 없다면 드라이버를 설치할 수 없으며 NVIDIA 드라이버가 자신의 컴퓨터에 없어도 시뮬레이터를 사용할 수 있으나 낮은 속도로 축구가 실행된 다. Webots Simulator는 GPU가 있을 경우 작동한다. 그렇지만 CUDA, cuDNN의 설치가 상대적으로 까다롭기 때문에 초보자는 PyTorch만 설치해도 무방하다. PyTorch 만으로도 학습 기반 코드의 작성이 가능하다. Tensorflow, PyTorch 등의 딥러닝 라이브러리를 사용하기 위하여 NVIDIA 드라이버 설치와 함께 CUDA, cuDNN를 설치한다. 학 습 기반 코드를 사용하기 위하여 NVIDIA 드라이버, CUDA, cuDNN을 설치한 후에 명령 프롬프트(cmd) 창이나 Anaconda 프롬프트를 사용하여 자신이 사용할 Tensorflow 나 PyTorch를 명령어로 설치한다. Ubuntu 18.04는 Windows 10처럼 컴퓨터 환경을 조성하는 운영 체제(OS) 중 하나이며 로봇 시뮬레이터 및 프로그 래밍 개발에 최적화된 운영체제이다. Ubuntu 18.04는 안정적 장기 지원(LTS) 버전이다. Windows와 달리 운영 체제 자체가 Python 언어로 되어 있기 때문에 별도로 Python을 설치하지 않아도 되며 대부분의 설치가 터미널 창에서 명령어를 통해 가능하다. Ubuntu 에서는 터미널에 명령어: webots을 통하여 프로그램 실행이 가능하다. Webots 시뮬레이터 설치 후에 Webots 시뮬레이터 메뉴의 ‘Tools’ → ‘Preferences’ → ‘General’ → ‘ Startup mode’ 로 이동하여 pause 모드로 설정한다. 또한, ‘Python command’를 ‘python’으로 설정한다. Webots 시뮬레이터에서 AI 축구 시뮬레이터를 사용하려면 환경변수에 Webots의 PYTHONPATH를 추가한다. Python 설치 후 Python 버전(‘python3x’ )에 맞춰서 PYTHONPATH를 설정한다. Ubuntu에서는 Webots 시뮬레이터 실행 전 매번 PYTHONPATH를 불러와야 한다. 터미널을 이용하여 별도의 페이지를 열지 않아도 아래의 명령어를 사용 하여 불러오는 작업을 수행할 수 있다. 만약 Webots를 실행할 때마다 PYTHONPATH를 매번 추가하지 않으려면,‘~/.bash_profile’ 파일에 PYTHONPATH 변수를 추가한다. 일반적으로 Ubuntu OS에는 기본적으로 Python interpreter가 설치되어 있다. PIP 패키지 관리자를 통해 필요한 라이브러리를 설치한다. - numpy - opencv-python Windows의 Anaconda Python 환경과 달리 Ubuntu에서는 터미널에 명령어를 사용하여 pip, Python 필수라이브러 리들을 설치하여야 한다. 학습 기반 코드를 사용하기 위하여 NVIDIA 드라이버, CUDA, cuDNN을 설치한 후에 터 미널에 명령어를 사용하여 Tensorflow 와 PyTorch를 설치한다. 다운로드하여 압축을 푼 AI 축구 시뮬레이터는 도12와 같은 구조를 가지고 있다. ‘controllers’ 폴더는 supervsor의 기능이 들어가 있고, 게이머 코드의 작성과는 무관하다. ‘pluggins’ 폴더는 게임 진행상화에서 발생하는 로봇간 충돌의 감지에 관련된 코드들이 있다. ‘protos’ 폴더는 게임의 로봇 및 경기장 등의 상세 내역이 있다. ‘reports’ 폴더는 게임 기사작성과 관련된 것이다. 게이머에게는 ‘worlds’와 ‘examples’ 폴더들이 중요하다. ‘worlds’ 폴더에는 Webots 시뮬레이터에서 열어야 하는 청소년 및 어른 로봇의 경기를 위한 환경이 모두 포함되어 있다. ‘examples’ 폴더에는 자신의 전략을 개발하기 위해 참고하여 사용하는 예 제 코드가 포함되어 있다. ‘worlds’ 폴더에는 ‘aisoccer_1.wbt’ 및 ‘aisoccer_2.wbt’의 2개 world 파일 이 있다. world 파일 ‘aisoccer_1.wbt’는 청소년 로봇들을 사용하여 경기를 하는 환경이고, world 파일 ‘ aisoccer_2.wbt’는 어른 로봇들을 사용하여 경기를 하는 환경이다. 설치를 확인하려면 Webot 시뮬레이터를 열 고 ‘Ctrl + O’를 눌러 월드 파일을 열 수 있다. ‘Worlds’ 폴더에서 실행하려는 World 파일을 선택하면 도 13에 도시된 바와 같이 시뮬레이터가 실행된다. AI 축구 시뮬레이터의 root 폴더에 있는 ‘config.json’ 파일 을 열고 e-mail 및 라이센스를 입력하면, 규칙 기반 두 팀 간의 경기가 진행된다. Webots 시뮬레이터 편집기를 사용하여 ‘config.json’ 파일을 열어 게임 옵션을 구성하고 라이센스(e-mail 주 소 및 시그니처 키)를 삽입하고 게이머 strategy codes를 설정하면 된다. 기본 구성만을 사용하면 로봇을 임의 로 이동하는 팀이 A팀, 간단한 규칙 기반에 따라 로봇을 작동시키는 팀이 B팀으로 경기가 진행될 것이다. 게이 머가 작성한 코드는 path를 정해서 사용할 수 있다. AI Soccer 게이머와 관련된 부분은 rule, team_a, team_b, tool이다. rule의 game_time 은 전후반 각각의 게임시간으로 게이머가 원하는 대로 바꿀 수 있다. 예 를 들어 ‘config.json’ 파일은 다음의 표1과 같이 구성될 수 있다. 표 1 항목 의미 rule Game_time(경기시간)과 deadlock(교착상태=공이 4초동 안 로봇간의 접촉으로 인한 교착으로 움직이지 않을 경 우 공이 재배치됨)으로 구성됨 Game_time Game time (default: 300 seconds) deadlock False 로 설정하면 교착 상태에 대한 규칙이 무시된다 (default: True). Team_a Team_b경기할 축구팀을 정함. name, executable, datapath, keyboard 로 구성됨 name 팀 이름 executable AI 축구 실행 파일 경로 (AI 축구를 실행하려면 두 팀 모두 올바르게 지정되어 야 함) datapath AI 가 일부 파일을 출력할 수 있는 경로 keyboard True 로 설정하면 키보드를 통해 로봇 조작이 가능하다. commentator reporterAI 축구 해설자와 리포터의 정보를 정한다 (다른 AI WorldCup 종목에서 사용) tool True 로 설정하면 게임 종료 후 동일한 게임 옵션으로 게임이 반복된다. True 일 때 ‘reset_reason’의 ‘ GAME_END’는 ‘EPISODE_END’로 대체된다 (default: False).multi_view True 로 두면 경기를 진행하는 동안 3 차원 카메라가 공을 따라 움직이고 몇몇 상황에서 사용되는 카메라가 바뀐다. 코드 개발 및 시험 단계에서는 False 로 두기를 권장한 다. record True 로 설정하면 경기를 녹화하고 게임이 끝났을 때 “record_path”에 저장된다. “repeat”과 함께 사용 할 수 없다. “repeat”이 True이면 “record” 내부적 으로 False 가 된다(default: False). 경기 종료 후 녹화된 비디오가 저장되는데 몇 분이 걸 린다. ‘Video creation finished.’라는 메시지가 Webots Console (Ctrl + L)에 나올 때까지 기다려야 한다. record_path 녹화된 비디오를 저장할 경로 (default: “”). 비디오 파일 이름이 아니라 경로이다. 파일이름은 자동으로 ‘ [{timestamp}]{team_a_name}_{team_b_name}. mp4’ 로 설정된다. 기본값으로 사용하면 루트 경로에 비디오를 저장한다. 비디오를 저장할 적절한 경로를 지정해야 한 다. replay True 로 설정하면 골이 들어갔을 때 골이 들어가기 3초 전부터 골이 들어가고 난 후 3초까지의 영상이 재생된 다. “multi_view” 옵션이 True이면 다른 각도의 리플 레이 영상이 재생된다. license 게이머의 정보를 확인. email, signature 로 구성됨 email 게이머의 email 주소 입력 signature 관리자로부터 받은 시그니처 키 입력 (다른 값을 입력 한 경우 게임이 진행되지 않는다.) config.json 파일 수정이 끝난 후, AI 축구를 ‘Ctrl + 2’ 키(Play Button)를 눌러 실행시킨다.청소년용 로봇 을 위한 AI 축구 경기장 및 어른용 로봇을 위한 AI 축구 경기장은 도14에 도시된 바와 같다. 어른용 로봇을 위 한 AI 축구 경기장이 청소년용 로봇을 위한 AI 축구 경기장보다 1.3 배 더 크다. AI 축구 로봇은 골키퍼(GK), 수비수(D1, D2), 공격수(F1, F2)의 세 종류로 나눌 수 있다. 도15는 청소년용 AI 축구 로봇 및 어른용 AI 축구 로봇을 도시한다. AI 축구 로봇 및 어른용 AI 축구 로봇의 차이점은 시각적 형태 와 크기뿐이다. 도15에 도시된 시각적 모양은 로봇이 Webots 시뮬레이터 화면에서 보이는것과 일치한다. 시각 적 형태는 팔, 다리, 머리를 포함한다. Webots 시뮬레이터에 의해 내부적으로 수행되는 물리적인 모양은 시각 적 모양을 단순화한 형태인 도16 같다. 도16에 표시된 로봇의 키와 발 크기의 물리적인 크기는 도15에 제시된 시각적 형상의 키와 발의 크기와 같다. 그 외의 부분들은 게임 실행의 편의성을 위해 시뮬레이터 내부적으로 단 순화된다. 단순화는 로봇이 어떻게 공, 벽, 그리고 다른 로봇과 접촉하게 되는 지와 관련이 있다. 로봇의 신체 사양은 머리와 몸통, 다리를 대신하는 상자 모양의 하체로 구성된다. 팔은 골키퍼의 경우에만 물리적인 모양에 포함된다. Webots 시뮬레이터에서 로봇을 클릭하여 로봇의 물리적인 사양을 확인할 수 있다. 로봇 안에는 도17 에 도시된 바와 같이 2개의 바퀴와 추가적으로 2개의 슬라이더가 있다. 바퀴는 왼쪽과 오른쪽 바퀴가 각각 움 직이며(차동) 로봇의 움직임에 사용된다. 차동 바퀴의 사용에 대한 자세한 내용은 이하에서 설명된다. 2개의 슬라이더는 로봇의 전면과 하단에 위치한다. 도17에서, 발 아래의 빨간색 부분은 하단 슬라이더에 해당한다. 발 앞쪽의 빨간 부분은 전면 슬라이더에 해당한다. 전면 슬라이더는 kick(cross, shoot, quickpass)에 사용된 다. 크로스(cross) 또는 높게 공을 찰 때 각도 조정은 전면 (앞쪽) 슬라이더의 높이를 조정하여 수행할 수 있 다. 하단 (아래쪽) 슬라이더는 헤딩을 할 경우 점프를 위해 사용된다. 골키퍼의 경우 슬라이더를 다르게 설계 해 세이브 (Save) 기회를 만들어 낸다. 슬라이더는 골키퍼 하단에 위치하고 앞쪽, 왼쪽, 오른쪽의 세이브를 시 도하기 위해 세가지 방향으로 슬라이딩 할 때 사용된다. 축구 로봇은 바퀴와 슬라이더의 속도를 변화시킴으로 써 다양한 행동 특성을 달성할 수 있다. 그 특징의 예는 도18에 도시되어 있다. 도18에서 로봇은 이동 (move), 점프 (jump), 상대방 골대로 슛 (shoot)이 가능함을 보여준다. 로봇의 공 제어 능력을 높이기 위해 드 리블 모드도 정의되었다. 드리블 모드는 로봇이 공을 점유하고 있을 때 활성화할 수 있다. 드리블 모드가 활성 화되고 로봇이 공을 점유할 때 로봇과 함께 공이 움직인다. 드리블 모드가 활성화되어도 상대팀이 볼 점유상태 로 돌입하는 것을 막지 못한다. 드리블 모드의 규칙은 다음과 같다. 첫째, 드리블 모드는 로봇 전방의 드리블 영역에 공이 있을 때 활성화할 수 있다. 드리블 영역은 정면 중앙선을 중심으로 좌우 45도 도합 90도의 부채모양이며, 원으로 보이는 머리의 원주에서 5~20cm의 길이(로봇의 속도에 따라 다름)로 정의된다. 로봇이 정지 한 상태이면 5cm이고 최대의 속도(maximum linear velocity)로 움직이면 20cm이다. 도19는 드리블 영역을 도 시한다. 둘째, 다른 로봇의 드리블 지역과 공이 겹칠 경우 드리블 모드가 해제되고 두 로봇 모두 볼 점유를 할 수 있기 때문에, 다툼을 벌인다. 셋째, 드리블 중 shoot 또는 jump를 사용하면 드리블이 종료된다. 넷째, 로 봇이 넘어진 경우 드리블이 종료된다. 게이머들의 편의를 위해 규칙 기반 예제 코드의 ‘action.py’ 파일에 kick (shoot)과 jump 동작이 미리 구현되어 있다. 게이머 코드는 각 시간 단계에서 각 로봇에 대해 바퀴, 슬라 이더 및 드리블 모드 변수를 설정하고 그것들을 AI 축구 시뮬레이터로 전송해야 한다. 바퀴 및 슬라이더 사양 에 대한 자세한 내용은 후술된다. 축구 로봇의 사양은 아래의 표2와 같으며, 역할에 따라 일부 사양은 다르다. 골키퍼가 다른 로봇보다 무거운 이유는 상대 팀 수비수와 공격수가 골키퍼를 골 지역 밖으로 밀어내는 상황을 피하기 위해서다. 공격수가 수비수보다 가벼운 이유는 공격수가 페널티지역에서 골대 바깥방향으로 수비수들을 밀어 넣어 득점 기회를 만드는 상황을 피하기 위해서다. 표 2 골키퍼 수비수 공격수 무게(Kg) 2.5 2.0 1.5 로봇 무게중심(Cm) 지상 1.5 좌동 좌동 바퀴 무게(Kg) 각 0.15 좌동 좌동 슬라이더 무게(Kg) 각 0.5 좌동 좌동 최대속도(m/s) 1.8 2.1 2.55 최대회전토크(N*m) 0.8 1.2 0.4 축구 공은 지름 10cm의 단순한 구 형태이고 무게는 18.4g이다. 도20에 도시된 바와 같이, 축구공에 있는 infinity 마크는 AI 축구에서 개발할 수 있는 무한한 종류의 전략을 나타낸다.AI 축구는 공격과 수비가 반복되 면서 이루어진다. 이를 위해 공격과 수비에 해당하는 경기장 영역과 각 영역에 적합한 로봇의 행동들이 정의되 어야 한다. 공격의 경우, 각 로봇들이 게임전략에 해당하는 포매이션을 유지하기 위해 정해진 위치를 적절한 속도로 이동해 크로스, 킥, 패스를 적당한 속도와 높이로 실행하는 것이 중요하다. 수비의 경우, 상대팀 공격 수의 움직임에 대응한 위치선정과 패스가 중요하다. 이러한 경기진행을 위해 로봇과 경기장의 특징에 대한 정 보가 필요하고, 매 시간단계마다 갱신되는 경기 상태 정보가 필요하다. 결국, 게이머의 코드는 이러한 정보를 이용해 로봇의 위치설정, 바퀴를 이용한 이동(move)의 속도 제어, 전면 슬라이더와 하단 슬라이더를 이용한 크 로스/킥/패스를 적절한 시간에 효과적으로 실행하기 위한 것이다. 로봇과 경기장에 관한 정보는 게임 시작후 1 회만 수신하는 ‘info’ 딕셔너리 (Dictionary)와 50ms마다 수신하는 ‘frame’ 딕셔너리로 받고 게임 전략은 update() 함수를 중심으로 구현된다. 참고로 시간 단계와 프레임(frame)은 똑같이 50ms의 시간 경과후에 바뀐 다. 도21은 AI 축구 시뮬레이터와 게이머 코드 간의 통신 구조를 예시하며, 도22는 AI 축구 시뮬레이터와 게이 머 코드가 송수신 하는 정보를 예시한다. 도22에 도시된 바와 같이, AI 축구 시뮬레이터는 게임에 관련된 로봇 및 공의 위치, 게임 상태 (Game State), 스코어와 시간 등을 게이머 코드의 호출이 있을 때마다 알려준다. 게 이머는 AI 축구 시뮬레이터로부터 게임에 관련된 데이터를 받는 3 가지 가상 함수를 이용하여 게임 상태에 맞게 바퀴속도 (wheel velocity)와 전면 슬라이더 등을 이용해서 로봇들을 제어할 수 있다. 세가지 가상 함수는 init(), update(), finish()이다. init()은 게이머가 시뮬레이터와 성공적으로 연결된 후에 호출된다. update()는 매 단위 시간 50ms마다 호출된다. 경기가 끝날때, finish()가 호출된다. AI Soccer 코드에서는 3 개의 가상 함수가 호출된다. 가상 함수 ‘init()’은 게이머 코드가 시뮬레이터와 연결에 성공한 직후 한번만 호출된다. 도21에 도시된 바 와 같이 AI 축구 시뮬레이터로부터 경기장 상황과 로봇의 상태에 대한 사전 정보(다음 시간 단계에서 쓰일 정보)를 수신하고 게이머 코드에 의해 게임 전략 구현에 필요한 변수를 초기화 하는데 사용된다. AI 축구 시뮬 레이터의 ‘examples’ 폴더에 위치한 예제 코드에서 변수의 초기화 및 데이터 기록이 어떻게 작동하는지에 대 한 것을 확인할 수 있다. 가상 함수 ‘update()’는 시뮬레이터가 게이머 코드로 새 데이터를 전송할 때마다 호출된다. AI 축구에서는 단위 시간 단계가 50ms로 설정되어 있어 시뮬레이터도 50ms마다 새로운 데이터를 전송하고 ‘update()’도 50ms 마다 호출된다. AI 축구 시뮬레이터가 게이머 코드에게 보낸 자료를 frame(프레임)이라고 한다. 게임의 각 프 레임에 관한 정보가 수록된 프레임 데이터는 update() 함수의 인자인 ‘frame’에 들어있다. ‘frame’변수에 는 도22에 도시된 바와 같이 게임의 현재 상태에 해당하는 이미지, 좌표, 재설정 이유(reset reason), 게임 상태, 점수 및 시간이 포함되어 있다. 게이머 코드는 수신된 현재 축구 경기 상태의 내용을 담은 프레임 데이터 를 확인하고, 생성할 제어 신호(로봇의 동작)를 결정하고, 제어 신호를 다시 시뮬레이터로 전송한다. 따라서 게이머의 게임 전략이 구현된 함수이다. 제어 신호 전송 방법은 후술된다. 제어 신호를 생성하기 위한 규칙 또 는 심층학습을 사용하는 예제 코드는 ‘examples’ 폴더에 제공된다. 가상 함수 ‘finish()’는 게이머 코드가 종료되기 전에 한번 만 호출된다. 여기서 게이머 코드는 게임 내내 기록된 모든 데이터를 저장할 수 있고, 다음 게임에 유용하게 사용될 수 있다. ‘info’ 딕셔너리에는 경기장의 크기, 로봇 사양 등 경기 중 변하지 않는 값들과 기본 정보를 담고 있다. 이 딕셔너리는 게이머 코드의 ‘init()’ 함수에서 수신한 ‘info’ 변수를 통해 접근 가능하다(예: ‘info[‘ Game_time’]’는 경기 시간을 나타낸다). 사용하고 싶은 변수가 있다면 ‘init()’에서 class 변수로써 저장 해야 한다. 이 딕셔너리에 있는 정보들은 AI 축구 기본 사양에 있는 정보와 동일하다. ‘info’ 딕셔너리의 정보는 표3에서 보이는 바와 같다. 이러한 정보와 딕셔너리 값의 사용방법은 예제 코드의 general_check- variables.py’와 ‘general_image-fetch.py’을 통해서 확인 가능하다. 표 3 Member Variable Data Type 설명 field list of floats (length 2) 경기장 크기 [x, y] (단위: m) goal list of floats (length 2) 골대 크기 [x, y] (단위: m) penalty_area list of floats (length 2) 페널티 구역 크기 [x, y] (단위: m) goal_area list of floats (length 2) 골 구역 크기 [x, y] (단위: m) ※ 이 구역과 관련된 규칙은 없음. ball_radius float 공 크기 (단위: m) ball_mass float 공 무게 (단위: kg) robot_size list of floats (length 5) 로봇 크기 [GK, D1, D2, F1, F2] (단위: m) ※ 로봇 하체 기준 (0.15m) robot_height list of floats (length 5) 로봇 높이 [GK, D1, D2, F1, F2] (단위: m) axle_length list of floats (length 5) 바퀴 사이 거리 [GK, D1, D2, F1, F2] ( 단위: m) robot_body_mass list of floats (length 5) 로봇 무게 [GK, D1, D2, F1, F2] (단위: kg) wheel_raidus list of floats (length 5) 바퀴 반지름 [GK, D1, D2, F1, F2] ( 단위: m) wheel_mass list of floats (length 5) 바퀴 무게 [GK, D1, D2, F1, F2] (단위: kg) max_linear_velocitylist of floats (length 5) 바퀴 최대 선속 [GK, D1, D2, F1, F2] ( 단위: m/s) max_torque list of floats (length 5) 바퀴 최대 토크 [GK, D1, D2, F1, F2] ( 단위: N*m) resolution list of ints (length 2) 이미지 크기 [가로, 세로] (단위: pixel) number_of_robots Int 로봇 개수 codewords list of ints (length 5) 로봇 식별 이미지에 부여된 10 진수의 해 밍 코드 [GK, D1, D2, F1, F2] game_time Float 경기 시간 (단위: s) team_info Dictionary 두 팀의 정보 포함 key String 랜덤 문자열 key Keyboard Bool 키보드 조작 사용 여부 ‘frame’ 딕셔너리는 게임의 각 프레임의 정보를 가지고 있으며, 현재 게임 상태와 연관되어 있다. 게임이 진 행됨에 따라 로봇과 공의 좌표 등 ‘frame’ 딕셔너리에서 수신되는 정보가 매 시간단계마다 다르다. ‘ update()’가 호출될 때마다 업데이트 된 데이터와 함께 새로운 ‘frame’ 변수가 수신된다. 예를 들어, FRAME[‘coordinates’][MY_TEAM][GK][X]는 우리 팀 골키퍼의 x 좌표이다. frame에서 x 좌표로 순차적인 top- down 방식의 표현으로 좌표값을 얻는다. MY_TEAM, GK, X는 미리 정의되어 있어야 한다. ‘frame’ 딕셔너리의정보는 표4에서 보이는 바와 같다. 표5는 좌표 정보를 나타낸다. 표 4 변수 데이터 종류 설명 Time float 현재 경기 시간 (단위: 초) Score list of ints (length 2)현재 점수 [자신 팀, 상대 팀] reset_reason int 현재 프레임 직전 경기가 일시 정지된 이유. 이 값은 다음의 값들 중 하나이다: NONE - 일시 정지되지 않음. GAME_START - 경기 시작 직후, 킥오프로 경기 진행. SCORE_MYTEAM - 자신 팀 득점, 킥오프로 경기 진행. SCORE_OPPONENT - 상대 팀 득점, 킥오프로 경기 진행. GAME_END - 경기 종료. DEADLOCK - 공 재배치. GOALKICK - 골 킥으로 경기 진행. CORNERKICK - 코너 킥으로 경기 진행 PENALTYKICK - 페널티 킥으로 경기 진행 HALFTIME - 후반전 시작 직후, 킥오프로 경기 진행 EPISODE_END - 경기 종료 직후 (‘repeat’ 옵션이 켜 져 있으면 GAME_END 로 대체). game_state int 현재 경기 상태 이 값은 다음의 값들 중 하나이다: STATE_DEFAULT - 기본 STATE_KICKOFF - 킥오프 STATE_GOALKICK - 골 킥 STATE_CORNERKICK - 코너 킥 STATE_PENALTYKICK - 페널티 킥 ball_ownershipBool 킥오프, 코너 킥, 페널티 킥, 골 킥과 같은 상태에서 자신의 팀이 공을 소유하는지(True) 아닌지(False)를 나타내는 지표. 이 값은 기본 상태에서는 아무 의미가 없음. half_passed Bool 현재 경기의 전반전인지(False) 후반전인지(True)를 나타내는 지표 subimages list of items 새 프레임을 얻으려면 이미지 파편을 이전 이미지 프 레임과 병합해야 한다. 로봇과 공의 좌표가 주어지므로 중요하지 않음 coordinates list of [my team coordinates list, opponent team coordinates list, ball coordinate list]nested list. 현재 로봇들과 공의 위치 ※ 세부 내용은 아래 표에 있음 EOF Bool 프레임의 끝을 의미하는 마커 표 5 ‘my team coordinate list’/‘opponent team coordinate list’의 정보 번호 데이터 종류 설명 0 robot coordinate listGK 로봇의 좌표 1 robot coordinate listD1 로봇의 좌표 2 robot coordinate listD2 로봇의 좌표 3 robot coordinate listF1 로봇의 좌표 4 robot coordinate listF2 로봇의 좌표 ‘robot coordinate list’의 정보 번호 데이터 종류 설명 0 float 로봇의 x 좌표 (단위: m)1 float 로봇의 y 좌표 (단위: m) 2 float 로봇의 z 좌표 (단위: m) 3 float 로봇의 방향 (단위: rad) ※ [-π, π] 범위, 실제 값을 확인하여 원하는 범위로 변환할 수 있음. 4 Bool 로봇이 현재 움직일 수 있는지 여부를 나 타내는 지표. 일부 로봇들은 킥오프, 코너 킥, 페널티 킥, 골 킥과 같은 특별한 상태 에서 움직일 수 없음. 또한 퇴장되었을 때 움직일 수 없음. 5 Bool 바로 전 프레임에서 로봇이 공과 접촉했는 지 여부를 나타내는 지표. 좌표나 이미지를 통해 공과 로봇이 접촉하 였는지 파악하기 어렵기 때문에 이 값을 제공함. 6 Bool 바로 전 프레임에서 공이 로봇 앞 특정 범 위에 들어왔는지 여부를 나타내는 지표. 이 지표를 공을 차는 기준으로 사용할 수 있음 ‘ball coordinate list’의 정보 번호 데이터 종류 설명 0 double 공의 x 좌표 (단위: m) 1 double 공의 y 좌표 (단위: m) 2 double 공의 z 좌표 (단위: m) 로봇의 공 좌표, 방향 외에도 시뮬레이터에서 이미지를 제공할 수 있다. 도23의 상단 좌우에 도시된 바와 같이, 제공된 이미지는 640 x 480의 수정된 이미지인데 경기영상과 별개로 제공되어 이미지 기반의 로봇과 공의 인식이 가능하도록 하였다. 도23에 도시된 바와 같이, 게이머 코드에 전송된 이미지는 게임 프레임과 함께 표 시된다. 검은색 배경은 경기장 바닥을, 주황색은 축구공을, 로봇은 특수 마커로 된 이미지가 제공된다. 왼쪽 상단 모서리의 이미지는 A 팀으로 전송되고 오른쪽 상단 모서리의 이미지는 B 팀으로 전송된다. 즉, 이미지를 사용하여 전략코드를 구현할 수도 있다. 이미지를 사용하여 전략 코드를 만들 수 있음에도 불구하고, 예제 코 드는 게이머의 이해를 단수화 하기 위해 로봇과 볼의 좌표와 방향에 초점을 맞춰 진행된다. AI Soccer에 필요 한 로봇과 공의 위치정보는 좌표값으로 제공되므로 이미지 데이터를 활용하여 알고리즘을 설계할 것이 아니라면, 게이머는 이미지 데이터를 사용할 필요가 없다.로봇은 아래 몸통 양 옆에 부착된 두 개의 바퀴로 움 직일 수 있다. 또한 몸통 아래쪽과 앞쪽에 부착된 슬라이더를 이용하여 점프하거나 공을 찰 수 있다. 마지막 으로 드리블 모드는 로봇이 드리블 기회를 가질 경우 활성화될 수 있다. 게이머 코드에서 바퀴, 슬라이더, 드 리블 모드 제어는 각 시간 단계에서 ‘set_speeds()’를 호출하여 이루어진다. ‘set_speeds()’ 기능은 원하 는 바퀴, 슬라이더, 드리블 속도 값을 시뮬레이터로 전송하기 위해 호출된다. 각 로봇은 6 개의 변수를 갖는다. 첫번째 변수는 로봇의 좌측 바퀴 속도와 관련이 있다. 두번째 변수는 로봇의 우측 바퀴 속도와 관련 이 있다. 세번째와 네번째 변수는 각각 전면 슬라이더의 속도 및 높이와 관련이 있다. 다섯번째 변수는 하단 슬라이더 속도와 연관되어 있다. 여섯번째 변수는 드리블 모드와 관련이 있다. 표6 내지 표10은 로봇 제어 변 수들을 나타낸다. 표6 내지 표10은 각각의 로봇에 필요한 6가지 제어 변수를 보여준다. 표6 내지 표10에서, GK는 골키퍼, D1은 수비수 1, D2는 수비수 2, F1은 공격수 1, F2는 공격수 2를 의미한다. 표 6"}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "[0] [1] [2] [3] [4] [5] 데이터 종류float float float float float float 설명 GK, 왼쪽 바퀴 속도GK, 오른쪽 바 퀴 속도GK, 전면 슬라 이더 속도GK, 전면 슬라 이더 높이GK, 하단 슬라 이더 속도GK, 드리블 모 드 전환 표 7"}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "[6] [7] [8] [9] [10] [11] 데이터 종류float float float float float float설명 D1, 왼쪽 바퀴 속도D1, 오른쪽 바 퀴 속도D1, 전면 슬라 이더 속도D1, 전면 슬라 이더 높이D1, 하단 슬라 이더 속도D1, 드리블 모 드 전환 표 8"}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "[12] [13] [14] [15] [16] [17] 데이터 종류float float float float float float 설명 D2, 왼쪽 바퀴 속도D2, 오른쪽 바 퀴 속도D2, 전면 슬라 이더 속도D2, 전면 슬라 이더 높이D2, 하단 슬라 이더 속도D2, 드리블 모 드 전환 표 9"}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "[18] [9] [20] [21] [22] [23] 데이터 종류float float float float float float 설명 F1, 왼쪽 바퀴 속도F1, 오른쪽 바 퀴 속도F1, 전면 슬라 이더 속도F1, 전면 슬라 이더 높이F1, 하단 슬라 이더 속도F1, 드리블 모 드 전환 표 10"}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "[24] [25] [26] [27] [28] [29] 데이터 종류float float float float float float 설명 F2, 왼쪽 바퀴 속도F2, 오른쪽 바 퀴 속도F2, 전면 슬라 이더 속도F2, 전면 슬라 이더 높이F2, 하단 슬라 이더 속도F2, 드리블 모 드 전환 표6 내지 표10을 예를 들어 설명하면, [24]과 [25]의 값을 바꾸면, F2가 움직이게 만들 수 있다. 이때, [24]와"}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "[25]의 값은 각각 왼쪽 바퀴와 오른쪽 바퀴의 값을 변경한다. 두 값이 양수이고 크기가 같다면 로봇은 일정한 속도로 전진한다. 두 값이 모두 음수이고 크기가 같다면 로봇은 일정한 속도로 후진한다. 같은 절댓값의 숫자 를 한쪽에는 음수값을 사용하고 다른 쪽에는 양수값을 사용한다면 로봇의 제자리 회전을 구현하여 각도 조절을 할 수 있다. [26]과 [27]의 값을 바꾸면, 슛의 세기와 높이를 각각 조절할 수 있다. 앞에서 슛은 크로스, 킥, 패스로 구성된다고 설명하였다. 도25의 두번째 그림에서 로봇 발 쪽에 있는 빨간색이 전면 슬라이더가 위치하 는 부분이다. [28]의 값을 바꾸면 F2가 하단 슬라이더를 움직여 점프를 한다. 속도값들이 시뮬레이터에 보내 지면, 시뮬레이터는 다음 중 하나가 발생할 때까지 동일한 속도들을 유지한다. 게이머 코드가 새로운 속도들 을 전송하였을 때. 이 경우 속도는 그에 따라 갱신된다. 경기가 킥오프, 코너킥, 페널티 킥, 골 킥과 같은 특수 상태로 들어간 경우, 모든 로봇의 속도는 0으로 설 정된다. 그 다음 상태가 시작되면 그 상태에서 움직일 수 있는 로봇의 속도를 다시 업데이트할 수 있다. 그 상태가 끝나면 그 상태에서 움직일 수 없었던 로봇의 속도를 다시 갱신할 수 있다. 로봇이 퇴장한 경우. 로봇의 속도는 0으로 설정된다. 로봇이 경기장으로 복귀하면 속도의 갱신이 가능하다. 게이머 코드에서 슬라이더의 속도에 0~10의 값을 줄 수 있는데 슬라이더들이 계속 나와있는 상황을 방지하기 위 하여 시뮬레이터 프로그램에서 슬라이더의 속도가 한번 0 이상이 되면 일정 시간 후 0으로 되돌리고 1초 동안은 계속 0을 유지한다. 전면 슬라이더의 높이도 0~10의 값을 줄 수 있는데 높이 조절의 효과가 현재 프레임(50ms 의 시간 단계)에서 바로 나타나지 않아서 높이 조절은 미리 해 두는 것을 권장한다. 상대 쪽 페널티 지역에서 는 전면 슬라이더가 비활성화된다. 골키퍼만 패널티 구역에서 높이 찰 수 있다. 슬라이더는 경기 장면에 보이 지 않지만 시뮬레이터는 슬라이더 위치를 인식하여 슬라이더가 공과 접촉할 경우 물리적 법칙에 의해 공이 움직 이게 된다. 블록 코딩은 특별한 소프트웨어 프로그래밍 기술이 없는 초급자를 위한 것이다. 블록 코딩에 의한 AI 축구 코 드 생성기(generator)는 AI 축구의 규칙 기반과 딥러닝 기반 코드를 만들기 위한 웹 기반 프레임워크이다. 도 25는 웹-베이스 AI 축구 코드 생성기를 예시한다. 코딩에 익숙하지 않고 로봇축구에 대해 배우기 시작하는 사 람이 기본 전략을 쉽게 만들 수 있도록 만들어져 있다. AI 축구를 설계할 때 중요한 기능을 블록단위로 간략화 한 것이 블록 코딩이다. 규칙 기반 시스템은 구글 블록클리 (Google Blockly)를 기반으로 하여 구현된 프로그 래밍 편집기이다. 딥러닝 시스템은 하나 또는 여러 개의 로봇이 게이머가 원하는 전략을 수행할 수 있도록 훈 련할 수 있는 여러 종류의 강화 학습 알고리즘을 제공한다. AI 축구 코드 생성기를 사용하려면 도25에 도시된 바와 같이 규칙기반 코드 생성 또는 딥러닝 기반 코드 생성 메뉴 중에서 한가지를 선택한다. 추천하는 인터넷 브라우저는 구글 크롬이다. 도26은 규칙 기반 AI 축구 코드 생성기를 예시한다. 규칙 기반 전략에서는 5대의 로봇 각각에 대해 각 시간 단 계에 적절한 행동을 하도록 할당해야 한다. 블록코딩의 경우 축구선수의 역할에 따라 GK, D1, D2, F1, F2 등 총 5대 로봇의 전략을 구현할 수 있는 탭으로 구성되었다. 블록코딩 시스템은 게임 환경변수와 기능을 나타내 는 블록을 제공한다. 로봇을 제어하는데 사용되는 6 가지 변수(왼쪽 바퀴 속도, 오른쪽 바퀴 속도, 전면 슬라 이더 속도, 전면 슬라이더 높이, 하단 슬라이더 속도, 드리블 모드)가 정의되어 있다. 로봇의 왼쪽 바퀴와 오 른쪽 바퀴는 로봇이 이동해야 할 위치를 나타내는 ‘Robot X’와 ‘Robot Y’ 변수를 기준으로 정의된다. 전면 슬라이더의 속도와 높이는 킥, 크로스, 또는 퀵 패스 동작으로 정의된다. 하단 슬라이더는 점프 변수에 의해 정의된다. 드리블 모드는 드리블 변수를 사용하여 활성화된다. 사용자는 현재 필드의 상태에 따라 각 시간 단 계에서 로봇별로 5개의 동작과 관련된 변수를 정의해야 한다. - 로봇의 X 위치 (필수): 로봇이 X 축에서 이동해야 하는 목적지 - 로봇의 Y 위치 (필수): 로봇이 Y 축에서 이동해야 하는 목적지 - 로봇의 슛, 크로스, 패스 변수(선택사항): 로봇이 현재 시간 단계에서 슛을 하거나, 공을 띄워 차거나, 패스 를 하는지 여부를 결정한다. 슛을 하려면 “shoot” 변수를 True 로 설정한다. 크로스를 하려면 “cross” 변 수를 True 로 설정한다. 패스를 하고 싶다면, “quickpass” 변수를 True 로 설정한다. - 로봇의 점프 변수(선택사항): 로봇이 현재 시간 단계에서의 점프 시도 여부 - 로봇의 드리블(선택사항): 로봇이 현재 시간 단계에서의 드리블 시도 여부 도27에 도시된 바와 같이, 블록 카테고리는 시스템의 좌하단에 표시된다. ‘Environment’ 카테고리는 로봇 축 구 경기와 관련된 변수 및 기능을 정의할 수 있다. 다른 카테고리는 논리 블록, 루프, 연산, 변수 및 특수 함 수를 생성하기 위한 옵션과 같은 프로그래밍 언어 블록으로 구성된다. 도28에 도시된 바와 같이, ‘Environment indices’항목에는 게임 상태(game state)와 게임 리셋 이유 (reset_reason)를 포함한다. 각각의 시간 단계마다 받는 ‘frame’ 딕셔너리도 포함된다. 도29에 도시된 바와 같이, ‘Environment Constants’는 AI 축구 게임 초기화(init()) 중 받은 상수를 포함한 다. Init()함수는 경기장의 크기와 로봇의 사양과 관련이 있다. 도30에 도시된 바와 같이, ‘Environment Variabales’는 AI 축구 경기를 위한 동적 변수를 포함한다. 여기에 는 현재 시간 공의 위치, 미래에 예상되는(2 프레임후에 예상되는) 공의 위치, 그리고 자기팀과 상대팀 로봇들 의 현재 위치도 포함된다. 또한 50ms 간격으로 update() 호출을 통해 ‘frame’ 딕셔너리에서 받는 정보와 관 련된다. 도31에 도시된 바와 같이, ‘Environment Functions’는 전략 구축에 필요한 기능을 포함한다. - Distance: 두 점사이의 거리 계산 - Degree in radians: 라디안을 도로 변환 - Radians in degrees: 도를 라디안으로 변환 - ball_is_own: 공이 내 팀의 소유인 경우, 공이 내 구역에 있는 경우, 공이 내 페널티 구역에 있는 경우, 또는 공이 내 필드의 특정 구역에 있는 경우 True 혹은 False를 return(반환)한다. - ball_is_opp: 공이 상대 팀의 소유인 경우, 공이 상대 구역에 있는 경우, 공이 상대 페널티 구역에 있는 경우, 또는 공이 상대방 필드의 특정 구역에 있는 경우 True 혹은 False를 반환한다.- Get attack angle: 공의 위치와 상대 골 사이의 각도를 계산한다. - Get defense angle: 공의 위치와 우리 팀의 골 사이의 각도를 계산한다. - Print: 디버깅을 위해 Webots 시뮬레이터 콘솔에 메시지를 인쇄한다. 전략의 강화를 위해 사용되어지는 Python 프로그래밍 언어는 다음과 같다. Logic 카테고리는 if-else(or if- elif-else) statement, comparison statement, and/or statement, not statement, 그리고 boolean statement 를 포함한다. Loops 카테고리 loop 와 while loop를 포함한다. Math 카테고리는 산술, 연산(+, -, ×, ÷, ^), 수학함수 (제곱근, 절대, 지수, 로그, 삼각함수, 역삼각함수), 그리고 다른 수학 블록들을 포함한다. Lists 카테고리는 list 생성을 위한 블록이 포함되며, list 연산을 위한 블록도 포함된다. Variables 카테고리 는 자신의 변수를 정의하기 위해 응용된다. 게이머는 새로운 변수를 정의하거나 원하는 대로 설정하거나 변경 할 수 있다. Functions 카테고리는 자신의 함수를 정의하기 위해 응용된다. 새로운 함수를 정의하고 자신의 전 략에 응용할 수 있다. 도32에 도시된 바와 같이, 예제 코드는 변수 ‘kick’ 과 ‘jump’를 False 로 세팅하는 것으로 시작한다. 로 봇들이 특정한 상황에서 킥이나 점프, 코드를 변수 ‘kick’ 과 ‘jump’를 False 로 세팅하는 것으로 시작한다. 그리고 나서, 로봇은 게임에서 4 가지 다른 상황에 대한 규칙을 가지게 된다. 1. 만약 공이 자기팀의 페널티 영역에 있을 때 로봇은 다른 로봇이 행동을 취할 때까지 경기장의 좌표(-0.5, -1)에서 대기한다. 2. 만약 공이 자신의 진영, 페널티 영역을 제외한 지역에 있을 시, 로봇은 공을 쫓기 위한 행동을 취하고, 공을 현재 위치에서 다른 곳으로 옮기는 행동을 시도한다. 3. 만약 공이 상대방 진영에 있을 시, 로봇은 공을 쫓기 위한 행동을 한다. 만약 BALL_POSSESSION(공 소유는 로 봇이 공 가까이에 있고 성공적으로 킥 될 확률이 높다는 의미를 가지고 있다.)을 가지고 있다면 킥 모션을 시도 한다. 규칙 기반 블록 코딩을 위해 다음 4 가지 상위레벨의 행동이 정의되어 있다. 1. ‘Kick’: True 로 설정하면 낮은 킥 동작을 수행. 2. ‘Cross’: True 로 설정된 경우 공을 뜨게 하는 높은 킥 동작을 수행 3. ‘quickpass’: True 로 설정된 경우 패스 수행 4. ‘jump’: True 로 설정하면 점프 수행 게이머 코드에서 이러한 동작들을 사용하는 방법이 도33에 도시되어 있다. 생성된 코드를 사용하는 경우, 도34에 도시된 바와 같이, 화면 하단의 ‘Generate Code’를 클릭하면 압축파일 ‘mystrategy.zip’이 다운될 것이다. 아카이브를 추출한 다음 시뮬레이터의 예제 폴더(또는 이전 전략을 구현 한 폴더)로 이동한다. config.json 파일에서 team_a 또는 team_b 를 “executable” 로 변경한 다음 규칙 기 반 “config/my strategy/main.py 또는 deep learning 기반 “examples/ mystrategy/train.py”를 바꾼다. 로봇과 공의 위치 및 방향 값은 경기 시작 전 제공된다. 도35에 도시된 바와 같이, 모든 좌표는 데카르트 좌표 계를 따라 m 단위로 표시된다. 방향 각도는 우측을 향할 때 0이고 시계반대 방향으로 라디안으로 표시된다. 모든 좌표 단위는 미터이고 방향을 나타내는 각도는 -π에서 π의 라디안으로 표현한다. 도35의 왼쪽 부분에서 보인 바와 같이 로봇의 방향은 로봇이 주시하는 화살표 방향으로 정의한다. 왼쪽 로봇은 90° 혹은 π/2 라디안 방향을 주시하고 있고, 오른쪽 로봇은 0° 혹은 0 라디안 방향을 주시하고 있다. 도36은 경기장의 필드 구역 명칭을 예시한다. AI 축구는 각 팀이 골키퍼 1 명, 수비수 2 명, 공격수 2 명으로 구성되는 5:5 로봇 축구 게 임이며 A 팀은 빨간색 로봇으로 처음에 경기장 왼쪽에 위치하고 B 팀은 파란색 로봇으로 처음에 경기장 오른쪽 에 위치한다. 경기는 전후반 각 5분으로 진행되고 전반전은 A 팀의 킥오프부터 시작된다. 후반전이 시작되면 A 팀이 오른쪽에, B 팀이 왼쪽에 있도록 위치가 바뀐다. 후반전은 B 팀의 킥오프부터 시작되는 방식이다. 골 은 공의 중심이 골대를 통과할 때 발생하고 점수는 이에 따라 증가한다. 골이 들어간 후 로봇과 공의 포지션이 초기화되며 실점한 팀이 킥오프로 경기를 재개한다. 플레이어가 구현한 알고리즘으로 움직이는 로봇들은 공에 쉽게 접근하지 못하고 방황하는 형태를 보일 수 있다. 경기장에 있는 10명의 로봇 플레이어가 모두 공에 다가 가지 못하는 경우를 교착상태라 한다. 즉, 경기장에 아무런 변화도 일어나지 않는 상태를 의미한다. 또한, AI축구 경기장은 일반 경기장과는 달리 단단한 벽으로 둘러싸여 있기 때문에 로봇들은 공을 벽 쪽으로 밀 수 있으 며 같은 시도를 하는 여러 로봇으로 인하여 공을 벽과 로봇들 사이에 끼는 상황이 연출될 수 있어 이런 상황으 로 교착상태를 야기할 수 있다. 이러한 교착 상태를 방지하기 위하여, 시뮬레이터 프로그램은 교착 상태를 감 지하도록 구성되어 있다. 참가자는 교착상태 규칙을 숙지하고 프로그래밍 하여야 한다. AI 축구에서 축구공이 4초간 0.4m/s 미만의 느린 속도로 것을 교착상태로 정의한다. 교착상태가 발생한 각 지역에 따라 교착상태를 다르게 처리한다. 4개의 코너 지역중 하나에서 교착상태가 발생하면, 코너 킥으로 경기를 재개한다. 코너 킥 의 공 소유권은 다음과 같이 정한다. 코너 지역에 팀의 로봇 수가 많은 경우, 해당 팀이 공의 소유권을 가진다. 양 팀의 로봇 수가 동일한 경우, 공까지의 평균거리를 계산하고 평균거리가 작은 팀이 공의 소유권을 가진다. 평균거리도 같은 경우, 그 지역 반대쪽 골대의 팀이 공의 소유권을 가진다. 2 개의 페널티 지역 중 하나에서 교착상태가 발생하면 페널티 킥이나 골 킥으로 경기를 재개한다. 페널티 킥 또는 골 킥의 공 소유권 은 다음과 같이 정한다. 해당 페널티 지역에 팀의 로봇 수가 많은 경우, 해당 팀이 공의 소유권을 가진다. 양 팀의 로봇 수가 동일한 경우, 공까지의 평균거리를 계산하고 평균거리가 짧은 팀이 공의 소유권을 가진다. 평 균거리가 같은 경우, 그 지역 반대쪽 골대의 팀이 소유권을 가진다. 그 지역 반대쪽 골대 팀이 공 소유권을 가 지면 경기는 페널티 킥으로, 그렇지 않으면 골 킥으로 진행한다. 코너 지역이나 페널티 지역이 아닌 곳에서 교 착상태가 발생하면 중앙 위치에 공을 재배치하여 경기를 재개한다. AI 축구는 단순화를 위하여 페널티지역에서 만 반칙을 적용한다. 이 지역 내부에 있을 수 있는 로봇 수를 제한하고, 해당 규칙은 수비팀이 골문을 완전히 봉쇄하거나 공격팀이 여러 로봇에 의하여 골문까지 공을 밀어 넣는 것을 막기 위한 것이다. 그리고 상대방 페 널티 지역에 있을 수 있는 시간도 제한된다. 이 규칙은 수비팀이 공격 팀 골키퍼를 밀어내는 상황을 막기 위한 것이다. 수비팀의 반칙 관련, 공이 페널티 지역에 있는 경우, 수비팀의 로봇은 3개만 그 페널티 지역에 있을 수 있다. 4 개 이상의 로봇이 그 지역에 있다면 공격팀의 페널티 킥으로 경기를 재개한다. 공격팀의 반칙 관 련, 공이 페널티 지역에 있는 경우, 공격팀의 로봇은 2개만 그 페널티 지역에 있을 수 있다. 3개 이상의 로봇 이 그 지역에 있다면 수비팀의 골 킥으로 경기를 재개한다. 로봇은 상대팀의 페널티 지역에 1초만 있을 수 있 다. 1초 이상 있는 경우 그 로봇의 기본 위치로 복귀한다. 그리고 GK는 페널티 지역밖에 1초만 있을 수 있다. 1초이상 있는 경우 GK 기본 위치로 복귀한다. 축구공이 골대 위 옆을 통해 경기장을 밖으로 나갈 수 있다. 공 이 밖으로 나가면 경기는 골 킥이나 코너 킥으로 재개한다. 코너 킥이나 골 킥의 공 소유권은 다음과 같이 결 정된다. 공이 나가기 전에 마지막으로 공을 터치한 로봇의 수가 적은 팀이 소유권을 가진다(2개 이상의 로봇이 마지막 터치를 하는 경우가 가끔 있음). 로봇의 수가 같은 경우, 그 골대 쪽 팀이 공의 소유권을 가진다. 그 골대 쪽 팀이 공 소유권을 가진다면 경기는 골 킥으로, 그렇지 않으면 코너 킥으로 진행된다. 킥오프는 경기 시작과 한 팀이 골을 넣은 후에 발생한다. 킥오프에서 공 소유권을 가진 팀의 F2 로봇이 공을 차는 것으로 경 기가 시작된다. 킥오프 동안 소유권을 가진 팀의 F2 로봇을 제외한 다른 로봇은 다음 중 하나가 발생할 때까지 움직일 수 없다. 공이 센터서클을 벗어난 경우 및 공이 3초 동안 센터서클을 벗어나지 못한 경우. 도37은 킥 오프 포메이션(공 소유팀: A)을 예시하며 표11은 킥오프의 위치 및 방향을 예시한다. 팀 B 가 공 소유권을 갖 는 경우, 위치와 방향은 π만큼 회전되고 두 팀의 역할이 바뀐다. 표 11 A 팀 B 팀 GK (-3.8, 0.0) π/2 (3.8, 0.0) -π/2 D1 (-2.25, 1.0) 0 (2.25, -1.0) π D2 (-2.25, -1.0) 0 (2.25, 1.0) π F1 (-0.9, 0) 0 (0.65, -0.3) π F2 (0.4, 0) π (0.65, 0.3) π 축구공 (0, 0) 코너 킥은 코너 지역에서의 교착상태나 공 아웃 후에 발생하며, 코너 킥에서 공 소유권을 가진 팀의 F2 로봇이 공을 차는 것으로 경기가 시작된다. 코너 킥 동안 소유권을 가진 팀의 F2 로봇을 제외한 다른 로봇은 다음 중 하나가 발생할 때까지 움직일 수 없다.- 공 소유권을 가진 팀의 F2 로봇이 공을 찬 경우 - 3초동안 공을 차지 못한 경우 코너 킥이 발생한 위치와 어느 팀이 공의 소유권을 가졌는지 여부에 따라 다른 코너 킥 로봇 대형이 적용된다. 수비팀의 코너 킥의 경우, 코너 킥 발생 지역이 공 소유권을 가진 팀의 골대 쪽이면 수비형 코너 킥 로봇 대형 을 사용한다. 도38은 수비시 코너킥 포메이션 1(공 소유팀: A)을 예시하며 표12는 수비시 코너킥 포메이션 1의위치 및 방향을 예시한다. 표 12 A 팀 B 팀 GK (-3.8, 0.0) π/2 (3.8, 0.0) -π/2 D1 (-2.25, 1.0) -π/2 (1.5, -0.45) π D2 (-3.25, 1.0) -π/2 (1.5, 0.45) π F1 (-3.25, 0) 0 (0.5, -0.8) π F2 (-2.75, 2.0) -π/2 (0.5, 0.8) Π 축구공 (-2.75, 1.5) 도39는 수비시 코너킥 포메이션 2(공 소유팀: A)을 예시하며 표13은 수비시 코너킥 포메이션 2의 위치 및 방향 을 예시한다. 표 13 A 팀 B 팀 GK (-3.8, 0.0) π/2 (3.8, 0.0) -π/2 D1 (-3.25, 1.0) π/2 (1.5, -0.45) π D2 (-2.25, 1.0) π/2 (1.5, 0.45) π F1 (-3.25, 0) 0 (0.5, -0.8) π F2 (-2.75, -2.0)π/2 (0.5, 0.8) π 축구공 (-2.75, -1.5) 팀 B 가 공 소유권을 갖는 경우, 위치와 방향은 π만큼 회전되고 두 팀의 역할은 바뀐다.공격팀의 코너 킥의 경 우, 코너 킥 발생 지역이 공 소유권을 가진 팀의 골대 반대쪽이면 공격형 코너 킥 로봇 대형을 적용한다. 도40 은 공격시 코너킥 포메이션 1(공 소유팀: A)을 예시하며 표14는 공격시 코너킥 포메이션 1의 위치 및 방향을 예 시한다. 도41은 공격시 코너킥 포메이션 2(공 소유팀: A)을 예시하며 표15는 공격시 코너킥 포메이션 2의 위치 및 방향을 예시한다. 팀 B 가 공 소유권을 갖는 경우, 위치와 방향은 π만큼 회전되고 두 팀의 역할은 바뀐다. 표 14 A 팀 B 팀 GK (-3.8, 0.0) π/2 (3.8, 0.0) -π/2 D1 (3.25, 1.0) -π/2 (3.25, -0.5)π/2 D2 (2.25, 1.0) -π/2 (3.25, 0.5) π/2 F1 (2.25, 0) 0 (2.25, -0.5)π/2 F2 (2.75, 2.0) -π/2 (2.25, 0.5) π/2 축구공 (2.75, 1.5) 표 15 A 팀 B 팀 GK (-3.8, 0.0) π/2 (3.8, 0.0) -π/2 D1 (2.25, -1.0) π/2 (3.25, -0.5) -π/2 D2 (3.25, -1.0) π/2 (3.25, 0.5) -π/2 F1 (2.25, 0) 0 (2.25, -0.5) -π/2 F2 (2.75, -2.0) π/2 (2.25, 0.5) -π/2 축구공 (2.75, -1.5) 페널티 킥은 페널티 지역에서 교착상태나 페널티 지역 반칙 직 후에 발생한다. 페널티 킥에서 공 소유권을 가 진 팀의 F2 로봇이 공을 차는 것으로 경기가 재개된다. 페널티 킥 동안 소유권을 가진 팀의 F2 로봇을 제외한 다른 로봇은 다음 중 하나가 발생할 때까지 움직일 수 없다.- 공 소유권을 가진 팀의 F2 로봇이 공을 찬 경우- 3초 동안 공을 차지 못한 경우 도42는 페널티 킥 포메이션(공 소유팀: A)을 예시하며 표16은 페널티 킥 포메이션의 위치 및 방향을 예시한다. 팀 B 가 공 소유권을 갖는 경우, 위치와 방향은 π만큼 회전되고 두 팀의 역할은 바뀐다. 표 16 A 팀 B 팀 GK (-3.8, 0.0) π/2 (3.8, 0.0) -π/2 D1 (0.5, -0.8) 0 (1.5, 0.8) -π/2 D2 (1.0, -0.8) 0 (1.5, 1.05) -π/2 F1 (1.5, -0.8) 0 (1.25, 0.8) -π/2 F2 (2.0, 0) 0 (1.25, 1.05) -π/2 축구공 (2.95, 0) 골 킥은 페널티 지역에서 교착상태나 페널티 지역 반칙 또는 공 아웃 후에 발생하며, 골 킥에서 공 소유권을 가 진 팀의 GK 로봇이 공을 차는 것으로 경기를 시작한다. 골 킥 동안 소유권을 가진 팀의 GK 로봇을 제외한 다른 로봇은 다음 중 하나가 발생할 때까지 움직일 수 없다.- 공 소유권을 가진 팀의 GK 로봇이 공을 찬 경우 - 3초 동안 공을 차지 못한 경우 도43은 골 킥 포메이션(공 소유팀: A)을 예시하며 표17은 골 킥 포메이션의 위치 및 방향을 예시한다. 팀 B 가 공 소유권을 갖는 경우, 위치와 방향은 π만큼 회전되고 두 팀의 역할은 바뀐다. 표 17 A 팀 B 팀 GK (-3.8, 0.0) 0 (3.8, 0.0) -π/2 D1 (-2.5, 0.45) 0 (0.5, -0.8) π D2 (-2.5, -0.45)0 (0.5, 0.8) π F1 (-1.5, 0.8) 0 (-0.5, -0.45)π F2 (-1.5, -0.8) 0 (-0.5, 0.45) π 축구공 (-3.25, 0) 공 재배치는 나머지 지역에서 교착 상태 후 발생되며, 4개의 지정된 위치가 있다. 공은 4개의 위치 중 현재 위 치와 가장 가까운 곳으로 재배치된다. 로봇들은 재배치되지 않는다. 공이 재배치되면 경기가 재개되고 모든 로봇은 즉시 움직일 수 있다. 표18은 공이 재배치되는 위치를 예시한다. 표 18 Position A Position B Position C Position D (-1.5, 1.0) (-1.5, -1.0) (1.5, 1.0) (1.5, -1.0) 때때로 로봇이 넘어지고 스스로 일어날 수 없는 경우가 발생한다. 로봇이 넘어지고 3초 안에 일어날 수 없는 경 우 로봇은 경기장 밖으로 옮겨지고 5초간 활동을 할 수 없다. 또 로봇이 아주 드물게 경기장을 벗어날 수 있는 데 이때도 밖으로 옮겨지고 5초간 활동정지 상태가 된다. 5초가 지나면 로봇은 다시 지정된 위치와 방향으로 경기장으로 복귀한다. 다른 로봇이나 공이 돌아올 위치에 있는 경우, 해당 위치가 사용가능 할 때까지 필드 밖 에 있게 된다. 표19는 로봇이 복귀하는 위치 및 방향을 예시한다. 표 19 A 팀 B 팀 GK (-3.8, 0.0) π/2 (3.8, 0.0) -π/2 D1 (-2.25, 1.0) 0 (2.25, -1.0) π D2 (-2.25, -1.0)0 (2.25, 1.0) π F1 (-0.65, 0.3) 0 (0.65, -0.3) π F2 (-0.65, -0.3)0 (0.65, 0.3) π이하에서는 예시적인 규칙 기반 AI 축구 게임 코드를 설명한다. 규칙 기반 알고리즘의 예를 들어보면, 현재 볼 의 위치나 플레이어의 위치에 따른 현재 상태를 기반으로 알고리즘을 설정하게 된다. 골키퍼의 역할을 예로 들 어 생각해 볼 수 있다. 공이 우리 골대와 멀리 떨어져 있을 때는 상대 팀의 중거리 슛을 막을 수 있도록 위치 만 잡으면 공을 막을 수 있을 것이다. 반대로, 공이 내 진영의 페널티 지역 근처에 있다면, 적극적으로 공 근 처에 접근해야 상대팀의 득점을 막거나 공을 걷어내 위험에서 벗어날 수 있을 것이다. 로봇에 대해 미리 설정 된 규칙대로 로봇은 움직이게 될 것이다.예시적인 규칙 기반 AI 축구 게임 코드로서, ‘player_rulebasedA_py’ 폴더는 4개 파일로 구성된다. AI 축구 시뮬레이터에서 정보를 받아 다시 보낼 수 있는 파일인 ‘ player_rulebasedA.py’, 게임 상태를 바탕으로 팀 전략을 구현한 players.py, 로봇 행동 및 바퀴 및 슬라이더 관련 계산이 가능한 action.py, 사용되는 기능의 구현에 필요한 함수들이 정의된 helper.py 로 구성된다. 예제 코드들은 크게 상수의 선언부분, init()를 활용한 변수들의 초기화, 로봇들과 공의 좌표를 얻는 get_coord(), 이 좌표값들을 이용해 알고리즘을 구현한 update()로 구성된다. 매 50ms 마다 시뮬레이터에서 보내주는 정보를 update() 부분에서 게임 전략이 구현된 알고리즘을 활용하여 경기를 진행한다. 모든 파일에서 직관적인 이해를 위해 상수가 전역 변수로 선언되어 있다. 상수의 선언은 도44와 같이 예시될 수 있다. 상기 선언에 포함된 상수들은 게임 규칙과 관련이 있다. 예를 들어, frame 딕셔너리에 관련된 상수들이다. 상 기 선언에 있는 ‘reset_reason’ 상수는 경기중 리셋(reset)이 발생했을 경우 이유를 알려주고, 게임의 로봇들 과 공이 특정 위치에 재배치되는 경우에 사용된다. 경기 시작 시, 득점 후, 코너킥, 하프타임, 파울의 발생등 의 경우 로봇들과 공의 위치가 재설정된다. 예를 들어, SCORE_MYTEAM 은 내 팀이 득점했을 경우 재설정되는데 이때 Game.SCORE_MYTEAM이라는 Supervisor에서 보내는 값으로 상수를 설정한다. 각 줄의 오른쪽 상수들은 이미 Supervisor에 의해 설정되었기 때문에 게이머에 의한 이름의 변경이 허용되지 않는다. 상기 선언의 ‘ game_state’는 특정 상태가 수행되고 있는지 감지하는 데 사용된다. 코너킥, 페널티킥 등 게임의 특정 상태에 서만 사용되는 전술을 선택하는 경우에 이 상수를 이용하면 된다. STATE_DEFAULT는 킥오프, 골 킥, 코너 킥 및 페널티 킥이 발생하지 않은 경우의 게임상태를 나타낸다. 각 줄의 오른쪽 상수들은 이미 설정되었기 때문에 이 름의 변경이 허용되지 않는다. 상기 선언에서 ‘coordinates’는 로봇들과 공의 위치 좌표를 x, y, z의 3 차원 좌표로 나타낼 수 있다. 예를 들어 FRAME[‘coordinates’][MY_TEAM][GK][X]는 내 팀 골키퍼의 x 좌표이다. frame에서 X 좌표로 순차적인 top-down 방식의 표현으로 좌표 값을 얻는다. MY_TEAM, GK, X는 미리 정의되어 있어야 한다. 가장 중요한 상수는 게이머 코드가 수신한 정보와 관련이 있다. 로봇들이 게임 상태를 포함하는 프레임에 접근하고 자신과 상대 팀의 좌표와 방향, 볼 위치, 그리고 공을 건드렸거나 현재 점유하고 있는 상태 (예를 들어, 이때 킥을 시도할 수 있다)인 경우와 관련된 정보를 얻을 수 있도록 도와준다. 도45는 player_rulebasedA.py 파일의 예시이다. player_rulebased.py 파일에서 게이머들이 변경해야 하는 값들은 없 다. 하지만, 전체적인 프로그램의 골격을 담고 있는 파일이기 때문에, 정확히 이해하고 가는 것이 필요하다. player_rulebasedA.py는 게이머 코드의 기본이 되는 함수들이 정의되어 있는 파일이다. player_rulebasedA.py 는 크게 ‘init()’과 ‘update()’ 두 부분으로 나눌 수 있고 ‘get_coord()’는 이름이 의미하듯이 로봇들과 공의 3 차원 좌표를 얻는 함수이다. player_rulebasedA.py는 AI 축구 시뮬레이터로부터 정보를 수신하여 게이 머 코드의 기본 변수들과 함수들을 정의한 후 AI 축구 시뮬레이터에게 제어 신호를 보낸다. 도46은 init() 함 수 안에 있는 변수를 초기화하는 과정의 예시이다. 먼저 ‘init()’에서는 변수들(예: cur_posture, 내팀 로봇 들의 좌표와 방향과 공 접촉여부와 현재 시간단계에서 움직임 가능여부를 보여주는 list)을 선언 및 초기화할 수 있고, 다른 파일에서 필요한 데이터를 불러올 수 있다. 예를 들어 게이머가 학습된 인공신경망을 자신의 전 략에 탑재하고 싶은 경우, init()함수에 추가할 수 있다. 앞에서 설명한 대로 player_rulebasedA_py는 ‘info’ 딕셔너리를 통해 게임에 대한 기본적인 정보를 수신한 다음, 지역 또는 전역변수로 저장한다. 또한 게임에 필 요한 다른 변수들도 초기화한다. AI 축구의 5개의 로봇은 ‘players.py’ 파일에 정의된 개체(class)로 선언된 다. 로봇 개체는 ‘self.GK’, ‘self.D1’, ‘self.D2’, ‘self.F1’, and ‘self.F2’ 이고, init()안에 구현되어 있다. 게이머의 필요에 따라 추가적인 변수를 선언할 수 있다. player_rulebasedA의 init()함수에 선언된 전역변수들은 아래의 표20과 같다. 표 20 변수 데이터 종류 설명 self.field list of floats (length 2)축구 경기장 크기 [x, y] (단위: m) self.max_linear_velocitylist of floats (length 5)바퀴 최대 선속 [GK, D1, D2, F1, F2] (단위: m/s)self.robot_size float 로봇 크기 (단위: m) ※ 5 개 로봇 크기 동일 self.goal list of floats (length 2)골대 크기 [x, y] (단위: m) self.penalty_area list of floats (length 2)페널티 구역 크기 [x, y] (단위: m) self.goal_area list of floats (length 2)골 구역 크기 [x, y] (단위: m) ※ 이 구역과 관련된 규칙은 없음. self.number_of_robots int 로봇 개수 self.end_of_frame Bool 프레임의 끝을 의미하는 마커 self._frame int 현재 프레임 번호 ※ 50ms/프레임 self.speeds list of floats (length 25)5 개 로봇의 바퀴 및 슬라이더 속도 self.cur_posture list of floats (size 5*7)내 팀 5 개 로봇의 좌표, 각도 및 상태 self.cur_posture_opp list of floats (size 5*7)상대 팀 5 개 로봇의 좌표, 각도 및 상태 self.cur_ball list of floats (length 3)공의 좌표 self.previous_ball list of floats (length 3)이전 프레임 공의 좌표 self.predicted_ball list of floats (length 3)두 프레임 후 공의 예측 좌표 ※ ‘update()’에서 예측 스텝 변경 가능 self.idx int 공과 가장 가까운 자신 팀 로봇의 번호 self.idx_opp int 공과 가장 가까운 상대 팀 로봇의 번호 self.previous_frame object 이전 프레임의 정보를 담고 있는 class self.defense_angle float 내 팀 골대 중앙과 공을 이은 직선과 자신 팀 골대 중앙과 경기장 중앙을 이은 직선 사이의 각도 (단위: rad) self.attack_angle float 상대 팀 골대 중앙과 공을 이은 직선과 상대 팀 골대 중앙과 경기장 중앙을 이은 직선 사이 의 각도 (단위: rad) self.gk_index self.d1_index self.d2_index self.f1_index self.f2_indexint 포지션 별 로봇 번호 self.GK self.D1 self.D2 self.F1 self.F2object 포지션 별 로봇이 필요한 정보를 초기화하고 매 업데이트 마다 현재 프레임을 기준으로 다 음 행동을 결정하는 class update() 함수는 50ms 마다 게임 프레임을 수신하여 ‘received_frame’ 변수에 할당한다. ‘end_of_frame’ flag 가 True 로 설정된 후에 전체 프레임이 성공적으로 수신된다. 따라서 update() 함수의 프로세스는 “ received_frame.end_of_frame”이 시작할 때 True이어야 수행된다. ‘received_frame’을 기반으로 ‘ get_coord()’함수를 호출하여 로봇과 볼의 좌표 및 방향 등 프로그램의 전역 변수를 갱신(업데이트)한다. 다 음 시간 단계의 공 위치 예측, 공에 가장 가까운 로봇, 유용한 수비/공격 각도와 같은 게이머 전략의 다른 중요 한 변수들은 보통 update()안에서 호출되는 ‘helper.py’ 파일에 정의된 기능을 사용하여 계산된다. 로봇 바 퀴와 슬라이더를 정의하는 전략에 필요한 정보를 가지고 로봇 class의 ‘move’기능을 호출하여 AI 축구 시뮬레 이터에게 보내야 하는 모든 로봇의 변수를 포함하는 self.speeds라는 list 를 만든다. ‘move’기능은 players.py 스크립트에 정의되어 있으며, 현재 게임 상태에 따라 다음 시간 단계에서 로봇이 이동할 위치와 로 봇이 슛 동작을 시도할지를 선택한다. 5개의 로봇(GK, D1, D2, F1, F2) 모두에 대해 ‘move’ 기능이 호출 된 후 ‘self.speed’ list 가 전송된다. 속도는 ‘self.set_speed’ 함수를 사용하여 AI 축구 시뮬레이터에게 전 송된다. update() 함수를 종료하기 위해 현재 게임 상태를 ‘self.previous_frame’변수에 저장하며, 다음 시간 단계에서 공의 속도 등 중요한 정보를 계산하는데 사용된다. 또한 ‘print_debug_flag’함수를 호출하여 전 략의 어느 부분이 수행되었는지를 보여준다. debug_ flag는 게이머가 자신의 전략이 계획대로 작동하였는지 확 인하는데 사용할 수 있다. 도47은 예시적인 update() 함수이다. update() 함수에서 사용되는 변수들은 게이머 가 임의로 명명하여 쓸 수 있다. 앞에 언급했듯이, update() 함수는 게이머 전략 알고리즘을 구현할 수 있는 부분이다. update() 함수에 있는 다양한 변수들, predicted_ball(다음 프레임에서 예측되는 공의 위치),"}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "find_closest_robot(공과 가장 가까운 위치에 있는 로봇)등은 helper.py에 구현되어 있다. 요약하면, 로봇들과 공의 위치를 이용하여 공격수, 수비수, 골키퍼의 위치 선정과 패스 및 킥과 드리블 등을 ‘helper.py’, ‘ player.py’, ‘action.py’을 통해 실행하는 부분이다.‘action.py’ 파일에는 바퀴, 슬라이더 및 드리블 모드 와 관련된 변수를 계산하는 기능이 포함되어 있다. 로봇이 이동해야 할 위치를 기준으로 좌우 바퀴 속도를 계 산하는 컨트롤러는 ‘go_to()’와 ‘go_fast()’ 함수에 정의되어 있다. ‘pass_to()’는 특정 방향으로 패스 를 할 때 로봇의 회전 운동을 돕기 위한 컨트롤러이다. 컨트롤러를 사용하는 이유는 Webots 시뮬레이터가 물리 적 법칙에 의해 동작하기 때문에 적절한 제어를 위해 필요하기 때문이다. ‘kick’ 함수는 3가지 동작 cross, shoot, quickpass를 정의한다. 원하는 경기 동작에 따라 전면 슬라이더 속도와 높이를 조절한다. ‘jump()’ 함수는 게이머가 헤더골을 얻기 위한 점프동작을 시도하고 싶을 때 등에 하단 슬라이더를 조절한다. 마지막으 로 ‘dribble()’기능은 드리블모드를 활성화 및 비활성화 한다. 도48은 예시적인 action.py 파일이고, 표21은 ‘action.py’에 정의되어 있는 함수들을 나타낸다. 표 21 함수(매개변수) 설명 go_to(id, x, y, cur_posture, cur_ball, max_linear_velocity)id에 해당하는 로봇이 현재 위치(cur_posture에서 x, y 로 정 의된 새로운 위치로 가기 위한 바퀴 속도 값 반환. 현재 공의 위치(cur_ball)는 go_to()에서 호출하는 go_fast()에서 필요함. max_linear_velocity는 반환하는 바퀴 속도가 최대값 을 넘지 않게 하기 위해 필요함. (더 정밀한 위치제어를 위하여 수정 가능) go_fast(id, cur_posture, cur_ball)느려지지 않아야 하는 상황을 판단해 True/False 반환. 공쪽으 로 갈 경우 현재 공의 위치가 필요함. (‘go_to()’에서 목적지 에 가까운 경우 속도가 느려짐) kick(cross, shoot, quikpass)각 flag의 True/False에 따라 전면 슬라이더 속도와 높이를 반 환. 1,0,0 이면 cross를 의미함. 두개 이상의 flag 가 실수로 참이 될 경우, cross, shoot, quickpass 순으로 우선순위를 가지게 되어 1 개만 유효하게 함. 빠른 kick 동작을 위해, 전면 슬라이 더 속도/높이가 cross=10/8, shoot=10/0, quickpass=5/0 로 셋 팅되어 있음. 슬라이더의 높이는 0 일때 땅볼이고 10 일떄 가장 높은 공중볼. shoot 과 quickpass의 차이는 속도. 슬라이더의 높이와 공의 궤적 관계를 주의할것. (필요에 따라 수정 가능) jump(flag) flag의 True/False에 따라 하단 슬라이더 속도(0~10)를 반환. 속도가 10 일때 청소년용 로봇 GK는 최대 25cm의 높이를 가질 수 있음. update()에서 self.speeds[4] = 10 으로 한 뒤 helper.py 의 self.printConsole 을 사용해 cur_posture[0] [Z]를 출력하면 확인해 볼 수 있다. 4는 GK의 하단 슬라이더 속 도로 정의되고 0 은 GK의 id 임. (필요에 따라 수정 가능) dribble(flag) flag의 True/False에 따라 드리블 모드의 전환(on/off) Pass_to(id, x, y, speed, height, cur_posture, cur_ball, max_linear_velocity)id 에 해당하는 로봇이 현재 위치에서 x,y 로 공을 보내기 위한 값들을 반환. 반환되는 값들은 ‘action.py’파일에서 확인. 슬라이더의 힘(속도에 해당)과 높이를 조정하여 공을 띄워서 패 스할 수 있음. 공이 가까이 있지 않다면, 로봇이 가까이 접근하도록 설정되어 있음 따라서, 게이머는 각 상황에서 로봇을 이동시킬 좌표를 결정하고 action.py 의 ‘go_to()’함수를 사용하여 좌 우 바퀴 속도를 계산해야 한다. 동작이 가능할 때 그 기회를 인식하기 위해 ‘BALL_POSSESSION’ flag를 사용 할 수 있다. 로봇이 공을 점유하고 있다면 kick을 시도하다가 성공할 가능성이 높다. 볼 위치 좌표 중 Z 축 좌표를 이용하여 점프 동작 기회를 예측할 수 있다.helper.py 파일에는 게이머가 자신의 전략을 실행하고 게임상태에 관한 중요한 상황 정보를 계산하는 데 도움이 되는 여러가지 보조 함수들이 포함되어 있다. player_rulebasedA.py, players.py, action.py 에서도 ‘helper.py 에서 선언된 함수들을 사용한다. 도49는 예시적인 helper.py 파일이고 표22는 당해 helper.py 파일에서 정의된 함수들을 나타낸다. 표 22 Function(parameters) 설명 distance(x1, x2, y1, y2) 두 점 (x1, y1), (x2, y2) 사이의 거리 반환 degree2radian(deg) 각도 단위 변환 (degree → radian) radian2degree(rad) 각도 단위 변환 (radian → degree) wrap_to_pi(theta) 각도를 [-π, π] 범위로 변환 predict_ball(cur_ball, previous_ball)한개 프레임(50ms) 경과 후 예측된 공 위치 반환 find_closest_robot(cur_ball, cur_posture, number_of_robots)공과 가장 가까운 로봇의 번호 반환 ball_is_own_goal(predicted_ball, field, goal_area) 공 위치를 예측해 내 진영의 goal area/penalty area/내 진영에 위치하면 True 또는 False. 내 진영의 나머지 지역은 내 진영의 goal area 및 내 진영의 penalty area를 제외한 나머지 지 역ball_is_own_penalty(predicted_ball, field, penalty_area) ball_is_own_field(predicted_ball) ball_is_opp_goal(predicted_ball, field, goal_area)공 위치를 예측해 상대팀 진영의 goal area/penalty area/상대팀 진영에 위치하면 True 또는 False. 상대팀 진영의 나머지 지역은 상대팀 진영의 goal area 및 상대팀 진영의 penalty area를 제외한 나 머지 지역ball_is_opp_penalty(predicted_ball, field, penalty_area) ball_is_opp_field(predicted_ball) get_defense_kick_angle(predicted_bal l, field, cur_ball)내 팀 골대 중앙과 공을 이은 직선과 내 팀 골대 중앙과 경기장 중앙을 이은 직선 사이의 각도 반환 get_attack_kick_angle(predicted_bal l, field)상대 팀 골대 중앙과 공을 이은 직선과 상대 팀 골 대 중앙과 경기장 중앙을 이은 직선 사이의 각도 반환 set_wheel_velocity(max_linear_veloci ty, left_wheel, right_wheel)왼쪽이나 오른쪽 바퀴 속도가 최대속도를 넘는 값 이면 비율을 유지하며 더 큰 속도를 최대속도에 맞 춘 뒤 변경된 바퀴 속도 값 반환 printConsole(message) message를 시뮬레이터 Console 창에 표시 print_debug_flag(self) 팀의 각 로봇의 flag를 시뮬레이터 Console 창에 표시. 알고리즘의 실행되고 있는 부분 확인에 사용 가능 players.py 는 팀의 로봇들의 class를 정의하는 파일이다. 도50에 보이는 바와 같이 Goalkeeper(GK), Defender_1(D1), Defender_2(D2), Forward_1(F1), Forward_2(F2)가 정의되어 있다. 로봇 class는 ‘init()’ 과 ‘move()’의 두가지 기능으로 나뉜다. 로봇 Forward_2에 의해 정의된 ‘init()’과 ‘move()’ 함수의 예 제는 도51에 도시된다. player_rulebasedA.py 의 ‘init()’ 함수에서 class 개체를 만들 때 ‘init()’ 함수 가 호출된다. ‘init()’ 함수는 5개의 전역 변수를 선언하여 필드, 골, 페널티 영역, 골 영역, 로봇 크기 및 최대 선형 속도 정보를 나중에 ‘move’ 함수에 사용할 수 있도록 한다. ‘move()’ 함수는 규칙 기반 예제에 서 가장 중요한 함수다. move() 함수는 게임의 주요 전략을 담고 있다. ‘move()’ 함수는 ‘ player_rulebasedA’의 ‘update()’ 함수쪽에서 현재의 게임 상태를 변수로 전달받아 왼쪽 바퀴 속도, 오른쪽 바퀴 속도, 전면 슬라이더 속도, 전면 슬라이더 높이, 하단 슬라이더 속도, 드리블 모드 flag를 ‘update()’ 쪽으로 반환한다. rulebasedA 예제 코드에서 GK, D1, D2, F1 로봇들의 움직임은 게임 상태에 상관없이 고정되 어 있다. 만약 player_rulebasedA.py 를 실행시켜 보면, GK는 상대 킥으로부터 골대를 보호하려고 움직이고, 축구장에서 D1, D2, F1 은 고정된 위치로 이동하는 것을 볼 수 있다. 여기서 앞서 예시된 init()’과 ‘move ()’ 함수에 제시되어 있는 로봇 F2의 전략 코드를 분석해본다. F2의 전략은 현재 공의 위치를 기준으로 6가지 상황으로 나눈다. 좌표값에 따른 로봇의 위치는 도52를 참조한다.1. 다음 시간 단계에 공이 내 진영 골 지역 안에 있을 것으로 예상되면 (-0.5, -1) 위치로 이동한다. 2. 다음 시간 단계에 공이 내 진영 페널티 지역에 있을 것으로 예상되면 (-0.5, -1) 위치로 이동한다. 3. 다음 시간 단계에 공이 내 진영의 나머지 지역에 있을 것으로 예상되면 현재 공 위치로 이동한다. 4. 다음 시간 단계에 공이 상대 진영 골 지역에 있을 것으로 예상되면 현재 공 위치로 이동하여 로봇이 공 점유 를 한 경우 킥을 시도한다. 5. 다음 시간 단계에 공이 상대 진영 페널티 지역에 있을 것으로 예상되면 현재 공 위치로 이동하고 로봇이 공 점유를 한 경우 킥을 시도한다. 6. 다음 시간 단계에 공이 상대 진영의 나머지 지역에 있을 것으로 예상되면 현재 공 위치로 이동하고 로봇이 공 점유를 한 경우 킥을 시도한다. 앞서 예시된 init()’과 ‘move()’ 함수 코드는 향상될 가능성이 많은 기본적인 행동 전략이다. 이러한 행동 전략을 개선하는 방법을 소개하겠다. 로봇을 현재 공 위치로 이동하는 것보다는 공이 갈 것으로 예상되는 위치 로 움직이도록 하는 것이다. 또다른 방법은 공의 궤적이 상대방 골대의 방향일 경우, 로봇이 킥을 하게 하는 것이다. 다음에 코드를 이용하여 여러 개의 로봇을 사용하여 보다 복잡한 행동을 구현하는 방법을 소개할 것이"}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "다. 요약하자면, 게이머 전략을 구현하는 규칙은 ‘player.py’에 정의되어 있다. 게임 상태에 따라서 게이머 는 if-else 문을 사용하여 로봇이 어떻게 움직이고 행동하는지 설계한다. 규칙기반 예제 코드를 사용하면 ‘ self.flag’값을 정의하여 알고리즘의 어느 부분이 현재 시간 단계에서 실행인지 확인함으로써 전략을 디버깅 할 수 있다. ‘self.flag’ 값들은 ‘player_ruleBasedA.py’파일 안에 있는 ‘print_debug_flag’ 함수를 이 용하여 콘솔의 알림창에 출력해서 확인 할 수 있다. 이상에서 player_rulebasedA.py의 예제 코드를 살펴보았다. 하지만 여러 로봇으로 더 조직적인 행동을 할 수 있는 방법에는 초점을 맞추지 못하였다. 이제 로봇 F2가 공을 로봇 F1에게 패스하려고 하는데, 로봇 F1이 스스 로 위치를 잡아 패스를 기다리며 공을 상대 골대로 차는 게임 상황을 가정하여 전략을 세워보고자 한다. 상대 수비를 상대로 하는 내 팀의 로봇 F1과 F2의 초기 위치에서 예상 동작은 다음과 같다. 먼저 공은 상대 필드 하 단에 위치하며, 로봇 F2가 공에 접근해 로봇 F1에 대한 패스를 준비해야 한다. 한편 F1은 패스를 받기 위해 위 치를 잡아야 한다. 이 전략은 도52에 도시되어 있다. 그 다음 프레임에서 F1이 패스를 기다리는 동안 F2는 패 스하고, F1은 골대로 킥을 차기 좋은 각도로 방향을 바꾼다. 이 행동은 도53에 설명되어 있다. F1은 상대 골 대 방향으로 킥을 시도하고 F2는 공을 쫓아간다. 이 행동은 도54에 설명되어 있다. 킥이 끝난 뒤 두 로봇은 모 두 상대 골키퍼가 공을 막아 냈을 때를 대비하여 또 다른 킥을 할 수 있도록 공을 쫓아간다. 이 행동은 도55에 설명되어 있다. 규칙 기반 환경에서 F1과 F2가 위에서 보여준 행동들을 도56 및 도57에 각각 예시하였다. 도 56 및 도57의 과정은 로봇의 class move() 함수에 위치한 것과 유사한 코드로 변환 해 볼 수 있다. 이하에서는 예시적인 딥러닝 기반 AI 축구 게임 코드를 설명한다. 딥러닝과 강화학습(Reinforcement Learning, RL)이 결합된 심층 강화 학습의 기본 사항들을 개시하기 위해, 예시적인 딥러닝 기반 AI 축구 게임 코드로서 ‘ player_deeplearning-multi-iql-dqn_py’ 예제코드가 사용된다. 심층 강화학습은 강화학습을 딥러닝용 신경망을 이용하여 실행한다. 심층 강화학습은 강화학습의 구조에서 사용되는 인공 신경망을 딥러닝에 사용되는 것을 활 용함을 의미한다. 강화학습에서 로봇은 보상 함수을 통해 행동을 취하고 다시 환경과 상호 작용한다. 심층 강 화학습 알고리즘에서 인공신경망은 게임의 현재 상태에 따라 로봇이 취할 동작을 선택해야 한다. 심층 강화학 습 알고리즘은 앞으로 받을 보상을 극대화하기 위해 다양한 전략을 시도함으로써 수준을 향상시키는 방법을 배 운다. 심층 강화학습 구조는 도58과 같이 개념적으로 보일 수 있다. 도58에 도시된 바와 같이 심층 강화학습 구조를 사용하려면 상태(state), 행동(action) 및 보상(reward)을 효과 적으로 설계하는 것이 중요하다. 여기서 상태란, 로봇이 환경을 보는 방식이다. AI 축구에서 로봇의 좌표와 방향은 각 시간 단계마다 수신되며, 이 정보는 상태를 정의하는 용도로 사용된다. 행동에는 로봇의 바퀴 속도 와 전면/하단 슬라이더의 높이와 속도가 포함되고, 보상함수는 로봇의 행동 목표를 정해준다. AI 축구에서의 목표는 전통적인 축구 게임과 마찬가지로 게임에서 이기는 것이다. 즉, 상대방보다 더 많은 골을 넣는 것이라 할 수 있다. 따라서 보상함수는 더 많은 골을 넣기 위한 직접적 행동이나 간접적인 행동이 만들어지도록 설계 해야 한다. 다음은 ‘player_deeplearning-multi-iql-dqn_py’ 폴더에 있는 딥 러닝 예제코드에 대한 설명이 다. 예제코드는 다음 파일들로 구성된다. - ‘episode_memory.py’: 이 파일에는 훈련 중 로봇 (에이전트)이 수집한 경험을 저장하는 버퍼 (메모리)가 포 함된다. 각 시간 단계의 상태, 행동 및 보상은 신경망을 훈련시키기 위해 저장된다. - ‘helper.py’: 이 파일에는 규칙 기반 예제코드와 동일한 방식으로 정의된 보조함수가 포함된다. 두 점 사 이의 거리를 계산하고 각도를 변환하며 로봇과 볼의 속도를 예측하는 용도로 사용된다. - ‘networks.py’: 이 파일에는 로봇의 심층 강화학습을 위한 인공신경망 구조가 포함된다. - ‘algorithm.py’: 이 파일에는 학습에 사용된 알고리즘과 관련한 정보가 들어 있다. 단일 로봇의 경우 알고 리즘은 ‘dqn.py’ 또는 ‘ddpg.py’이다. 다중 로봇의 경우 알고리즘은 ‘iql.py’ 또는 ‘qmix.py’이다. - ‘rl_utils.py’: 이 파일에서는 훈련 중에 사용된 상태, 동작 및 보상을 각각 get_state, get_action, get_reward의 함수로 설정한다. - ‘train.py’: 이 파일에는 AI Soccer 시뮬레이터에서 정보를 받고, 경험을 저장하고, RL 알고리즘을 훈련시 키고, 작업을 AI Soccer 시뮬레이터로 다시 보내는 훈련 스크립트가 포함된다. - ‘play.py’: 이 파일에는 훈련된 인공신경망을 사용하여 다른 상대방과 대결하는 플레이 스크립트가 포함된 다. - ‘parameters.json’: 이 파일에는 계층 수, 신경세포 수, 학습 속도, 버퍼 크기 및 동작 발생 빈도 (frame_skip 값)와 같은 중요한 인공신경망 훈련용 매개 변수의 정의가 포함된다. 앞에서 제시한 딥러닝 예제코드의 구성은 도59에 도시되었다. ‘train.py’는 훈련에 사용되는 기본 파일이다 (‘config.json’ 파일에서 사용됨). ‘train.py’파일은 ‘algorithm.py’ 파일의 RL 알고리즘 종류와 ‘ rl_utils.py’ 파일의 상태, 행동 및 보상 설정을 활용한다. ‘algorithm.py’ 파일은 ‘episode_memory.py’ 의 재생 버퍼를 이용하여 ‘networks.py’의 데이터 및 신경망 구조를 저장한다. ‘helper.py’ 파일에는 보조 기능이 포함되어 있다. ‘player_deeplearning-multi-iql-dqn_py’ 예제코드에서 5대의 로봇(1개의 골키퍼, 2 개의 수비수 및 2개의 공격수)을 훈련시켜 각 로봇에게 주는 개별 보상을 극대화한다. 예제코드 훈련과 관련된 상태, 동작 및 보상의 설계, RL 알고리즘, 인공신경망 구조 및 세부 사항이 다음에 제시된다. 특정 상태, 행동 및 보상은 로봇 역할(골키퍼, 수비수 또는 공격수)에 따라 설계된다. 예제코드에 정의된 상태, 행동 및 보상은 한 개의 예이며, 게이머가 이를 이용하여 각자에 맞는 상태, 행동 및 보상을 설계할 수 있다. 최종 목표를 배우기 위해, 상태는 로봇에게 필요한 정보를 제공해야 한다. 게임의 전체 정보(모든 로봇과 공의 좌표 및 방향)가 각 로봇에 제공되는 경우, 좋은 보상으로 이끄는 중요한 동작을 학습하려면 더 많은 교육 시간 이 요구된다. 따라서, 상태는 전체 정보를 단순화하도록 설계되어 훈련중인 로봇 및 공의 정보와 관련된 중요 한 정보만 제공하게 정의되어야 한다. 중요한 정보의 구체적 내용은 게이머의 게임 전략에 따라 달라진다. 공 과 로봇의 상대적 거리 및 각도는 도60에 도시된 바와 같이 상태를 설계하는데 사용된다. 각도 ?는 로봇의 방 향과 공의 위치에 의해 만들어지는 각도이다. 골키퍼(GK)의 상태는 다음과 같이 모델링된다. 1) 방어 위치와 관련된 GK의 상대적 위치; 2) 내 팀 영역에 대한 공의 상대적 위치; 3) 내 팀 영역과 관련하여 GK의 상대적 위 치; 4) GK 방향; 5) 공을 기준으로 계산된 GK 방향; 6) 공의 속도. 시각적인 GK 상태의 설계가 도61에 예시되 어 있다. ‘rl_utils.py’에 포함된 GK 상태 관련 코드는 도62에 예시되어 있다. 수비수(D1/D2)의 상태는 다 음과 같이 모델링된다. 1) 공에 대한 D1/D2의 상대적 위치; 2) 내 팀 영역에 대한 공의 상대 위치; 3) 내 팀 수비 로봇과 관련된 D1/D2의 상대적 위치(로봇이 D1인 경우 그의 팀원은 D2, 로봇이 D2인 경우 그의 팀원은 D1); 4) D1/D2 방향; 5) 공에 대한 D1/D2 방향; 6) 공의 속도. 시각적인 D1/D2 상태 설계는 도63에 예시되어 있다. D1/D2 상태를 정의하기 위해 ‘rl_utils.py’에 포함된 코드는 도64에 예시되어 있다. 공격수 (Forwards, F1/F2)의 상태는 다음과 같이 모델링된다. 1) 공에 대한 F1/F2의 상대적 위치; 2) 내 팀 영역에 대 한 공의 상대적 위치; 3) 팀원과 관련하여 F1/F2의 상대적 위치(로봇이 F1인 경우 그의 팀원은 F2, 로봇이 F2인 경우 그의 팀원은 F1); 4) F1/F2 방향; 5) 공에 관한 F1/F2 방향; 6) 공의 속도. 시각적인 F1/F2 상태 설계는 도65에 예시되어 있다. F1/F2 상태를 정의하기 위해 ‘rl_utils.py’에 포함된 코드는 도66에 예시되어 있다. AI 축구 시뮬레이터를 통해 게이머는 6개의 연속 변수들을 선택하여 로봇 속도, 발로 차는 동작 및 점프 움직임 을 제어할 수 있다. RL을 사용하여 교육을 시작할 때 대부분의 작업은 탐색 단계에서 무작위로 수행된다. 로 봇의 동작이 연속적이므로 6개의 연속 변수들로 무한개의 조합이 존재하지만 이러한 조합의 대부분은 실제 게임 상황에서 쓸모가 없다. 이 예제코드에서는 탐색을 용이하게 하기 위해 작업을 분리한다. 무한개의 동작을 한 정된 수의 작은 개별 동작(셀 수 있는)에 의해 매핑할 수 있게 한다. 각 로봇에 대해 13개의 개별 동작이 정의 된다 (드리블 모드는 모든 개별 동작에 대해 비활성화 됨). 개별 동작을 사용하면 훈련 속도 또한 향상된다. 골키퍼(GK)의 13개 개별 동작은 다음 동작들을 수행하기 위해 정의된다. 앞으로 이동([1,1,0,0,0,0], [0.2,0.2,0,0,0,0]), 앞으로 이동 및 패스([1,1,5,0,0,0]), 뒤로 이동([-1,-1,0,0,0,0], [-0.2,- 0.2,0,0,0,0]), 왼쪽으로 회전([0.8,1,0,0,0,0], [-0.1,0.1,0,0,0,0], [-0.2,0.2,0,0,0,0]), 오른쪽 으로 회전([1,0.8,0,0,0,0], [0.1,-0.1,0,0,0,0], [0.2,-0.2,0,0,0,0]), 왼쪽으로 점프([0,0,1,0,0,0]) 오른쪽으로 점프([0,0,2,0,0,0]). 도67은 IQL 알고리즘에 기반한 GK 행동 설계를 예시한다. 왼쪽/오른쪽 점프는 골 을 막기 위한 save 동작으로 이용된다. 수비수(D1/D2)의 13개 개별 동작은 다음 동작들을 수행하기 위해 정의 된다. 앞으로 이동([1,1,0,0,0,0]), 앞으로 이동하여 차기([1,1,5,0,0,0], [0.2,0.2,5,0,0,0]), 앞으로 이동 하고 크로스(높이 차기)([1,1,10,10,0,0], [0.2,0.2,10,10,0,0]), 뒤로 이동([-1,-1,0,0,0,0]), 왼쪽으로 회 전([0.8,1,0,0,0,0], [-0.1,0.1,0,0,0,0], [-0.2,0.2,0,0,0) , 0]), 오른쪽으로 회전([1,0.8,0,0,0,0], [0.1,-0.1,0,0,0,0], [0.2,-0.2,0,0,0,0]), 그리고 중지([0,0,0,0,0,0]). 높이 차기는 cross를 의미한다. 도 68은 IQL 알고리즘에 기반한 D1/D2 행동 설계를 예시한다. 공격수(F1/F2)의 13개 개별 동작은 다음 동작들을 수행하기 위해 정의된다. 앞으로 이동([1,1,0,0,0,0]), 앞으로 이동하여 패스([1,1,5,0,0,0], [0.2,0.2,5,0,0,0]), 앞으로 이동하고 크로스(높이 차기)([1,1,10,10,0,0], [0.2,0.2,10,10,0,0]), 뒤로 이동 ([- 1,-1,0,0,0,0]), 왼쪽으로 회전([0.8,1,0,0,0,0], [-0.1,0.1,0,0,0,0], [-0.2,0.2,0,0,0,0]), 오른쪽으로 회전([1,0.8,0,0,0,0], [0.1,-0.1,0,0,0,0], [0.2,-0.2,0,0,0,0 ]), 그리고 중지([0,0,0,0,0,0]). 도69는 IQL 알고리즘에 기반한 F1/F2 행동 설계를 예시한다. 보상 설계는 로봇이 학습한 최종 행동 측면에서 보면 가장 중요한 것으로 간주될 만하다. AI 축구에서 최적의 시나리오를 가정했을 때, 로봇은 경기의 최종 결과를 기반으로 배우며, 승리한 게임에서 긍정적인 보상을, 패배 하거나 비긴 게임에서 부정적인 보상을 받는다. 그러나 훈련 초반에 로봇이 임의의 행동을 취하는 시간대에는 로봇이 이길 가능성이 낮으므로 효과적으로 학습할 수 없다. 이 문제를 완화하기 위해, 먼저 공을 접근하고 쫓 는 방법이나 차거나 패스하는 방법을 배우고, 마지막으로 협동 행동을 배우는 것과 같이 커리큘럼 학습을 포함 하는 보상(더 쉬운 과제를 먼저 배우고 복잡한 과제를 배우는)을 이용하여 다수의 상대에게 승리하는 방식의 훈 련을 시킬 수 있다. 보상은 특정 목표를 기반으로 설계된다. - 상대팀의 공차기에 대한 방어: 골키퍼는 상대팀의 공차기 시도 (방어 위치까지의 거리)로부터 보호할 수 있는 위치에 있으면 보상을 받는다. 도70은 상대팀의 공차기에 대한 방어의 보상으로서 ‘차기 방지’ 보상을 예시 한다. - 공에 대한 거리 및 방향: 로봇이 공에 접근하는 경우, (공과의 거리 벡터, 공이 속도 벡터인 경우 볼에 대한 방향) 접근에 성공하면 로봇이 보상을 받는다. 도71 및 도72는 공과의 거리에 대한 보상 및 방향에 대한 보상을 각각 예시한다. - 슛, 패스, 크로스: 로봇이 슛/패스/크로스 동작을 효과적으로 수행하는 경우 보상을 받는다. (로봇이 볼을 터 치한 후 볼이 원하는 방향으로 이동하는 경우). 도73은 슛/패스/크로스 보상을 예시한다. - 득점과 실점: 로봇은 득점하면 긍정적인 보상을 받는다. 상대방이 득점하면 로봇은 부정적인 보상을 받는다. 도74는 득점/실점에 관한 보상을 예시한다. 골키퍼(GK)의 경우, 보상은 선방, 슛/패스/크로스 및 실점 보상으로 이루어진다. 도75는 GK에 대한 보상을 보 여준다. 수비수(D1/D2)의 경우, 보상은 공까지의 거리와 방향, 슛/패스/크로스 보상으로 이루어진다. 도75는 D1/D2에 대한 보상을 보여준다. 공격수(F1/F2)의 경우, 보상은 공까지의 거리와 방향, 슛/패스/크로스 및 득점 보상으로 이루어진다. 도75는 F1/F2에 대한 보상을 보여준다. 예제코드 ‘player_deeplearning-multi-iql-dqn_py’를 통해, 5개의 로봇을 교육하는 데 사용한 Independent Q Learning(IQL) 학습 기법을 설명한다. IQL 알고리즘에서 각 로봇(에이전트, agent)는 Deep Q Networks(DQN)을 사용하여 독립적으로 훈련된다. 각 에이전트는 다양한 보상 함수에 따라 각기 다른 역할과 행동을 학습해야 한 다. 다음으로 DQN 알고리즘과 여러 에이전트에 대한 DQN 알고리즘의 발전된 형태인 IQL 알고리즘을 소개하겠다. IQL을 사용할 때, 각 에이전트의 신경망은 DQN 알고리즘을 사용하여 학습된다. DQN 알고리즘은 신경망이 특정 게임 상태에서 얼마나 잘 행동할 수 있는지 예측하려고 한다. 해당 행동을 수행함으로써 얻는 단일의 보상 대 신에 Q-value라는 값을 지정한다. 특정 상태에서 얻을 수 있는 가치는 해당 조치를 취한 후(Q-value 라는 값을 지정한 후) 미래에 받을 수 있는 누적된 보상에 따라 모델링 된다. 달리 말하면, 로봇은 각 상태에서 최대 가 치를 가진 행동, 즉 미래에 받을 누적 보상을 극대화하는 행동을 선택한다. DQN 알고리즘은 ‘dqn.py’ (‘ algorithm.py’) 파일에서 구현된다. 도76은 DQN 알고리즘 구조를 예시한다. IQL 알고리즘은 여러 개의 로봇(에이전트, agent)들을 훈련시키는 가장 간단한 RL 알고리즘이다. IQL 알고리즘 은 다중 로봇 시나리오를 위한 DQN 알고리즘의 확장인데, 이 경우, 각 로봇마다 하나의 신경망과 하나의 보상함 수가 독립적으로 존재한다. 각 로봇은 자체 보상 기능에 따라 다른 역할을 배울 수 있다. IQL의 한 가지 장점은 학습이 상대적으로 쉽다는 것이다. 그러나 AI Soccer 와 같은 다중 로봇 환경에서 IQL은 결정을 내릴 때 다 른 로봇의 동작을 고려하지 않는 큰 단점이 있다. 이 경우, 로봇이 받는 보상이 자신의 행동으로 인한 것인지 혹은 다른 로봇의 행동으로 인한 것인지 확실하지 않은 상황이 존재한다. 도77은 IQL 알고리즘 구조를 예시한 다. ‘player_deeplearning-multi-iql-dqn_py’ 예제코드에 사용된, DQN을 이용한 IQL의 최종 구조는 도78에 예시 되어 있다. 각 로봇에 대해 하나의 신경망이 개별적으로 학습된다. 이 예제코드에서는 각 계층에 64개의 신경 세포가 있는 3개의 은닉 계층 구조의 신경망이 사용된다. 입력 계층에는 상태를 나타내는 값들이 들어가고 출 력 계층에는 행동을 나타내는 값들이 나온다. 상태를 정의하는 입력 값의 개수는 10이다. 행동을 정의하는 출 력 값의 개수는 13이다. 예제코드용 ‘networks.py’에서 신경망의 입력 및 출력은 자동으로 정의된다(‘ train.py ‘파일의’self.obs_size’ 및 ‘self.act_size’ 변수에 따라). 신경망 구조는 ‘networks.py’파일 에 정의되어 있다. 은닉 계층수와 각 계층의 신경세포 개수는 ‘num_layers’ 및 ‘hidden_dim’ 변수를 사용 하여 정의할 수 있다. 은닉 계층수와 각 계층의 신경세포 개수는 다음과 같이 ‘num_layers’ 및 ‘hidden_dim ’변수를 사용하여 ‘parameters.json’ 파일에서 정의할 수도 있다. 도80은 신경망의 실제 구조를 예시한다. DQN 알고리즘에 따른 신경망 구조 입력값은 상태 벡터 길이와 같은 10의 길이를 가지며, 64개의 신경세포를 가 진 3개의 은닉층으로 구성된다. 출력값은 행동의 수와 같은 13의 길이를 가진다. 각 시간 단계에서 수행할 최선의 행동을 찾으려면 먼저 로봇이 환경을 탐색해야 한다. 환경을 탐색하기 위해 훈련이 시작될 때 탐사 단계라고 알려진 시간대에는 무작위 로봇의 무작위 행동이 관측된다. 훈련이 진행됨에 따라 로봇은 의미 있는 행동을 배우기 시작하고 자신의 행동을 선택하기 위해 더 많은 학습을 하게 된다. 로봇 은 탐사 단계에서 시작하여 학습 단계를 거치며 대부분의 행동을 선택한다. 심층 강화 학습을 이용할 때 시간 이 지남에 따라 환경을 탐색/탐사하는 방법을 갱신하는 것은 딜레마 중 하나이다. 각 시간 단계에서 탐색/탐사 비율은 ‘dqn.py’ 파일의 변수 ‘self.epsilon’에 의해 제어된다. ‘self.epsilon’이 1인 경우, 모든 동작 은 일련의 개별 동작에서 임의로 선택되어 수행된다. ‘self.epsilon’이 0인 경우, 모든 동작이 신경망에 의 해 선택된다. 학습 과정에서 ‘self.epsilon’ 변수의 감소는 ‘self.dec_epsilon’ 및 ‘self.epsilon_steps ’ 단계 변수로 제어된다. ‘self.epsilon_steps’ 이후 ‘self.epsilon’의 현재 값이 ‘self.dec_epsilon’ 만큼 줄어든다. 훈련하는 동안 새로운 전략을 계속 탐색해야 하기 때문에, 훈련이 끝날 때마다 엡실론 (epsilon)의 최종 가치는 0이 되지 않는다. 이 최종 값은 ‘self.final_epsilon’ 변수에 의해 정의된다. ‘ player_deeplearning-multi-iql- dqn_py’에서 ‘self.epsilon’의 초기 값은 1이며, ‘self.epsilon_steps’ 는 30000회 반복, ‘self.dec_epsilon’은 0.025로, ‘self.final_epsilon’는 0.1로 설정되어 있다. 예를 들 어, 훈련 중 탐사량을 늘리려면 ‘parameters.json’파일의 ‘epsilon_steps’ 값을 30000에서 40000으로 늘릴 수 있다. 훈련 시간은 늘어나지만, 추가 탐사는 아마도 더 잘 배우는 방향으로 이어질 것이다. 이 예제코드를 훈련시키려면 ‘player_deeplearning-multi-iql-dqn_py’ 폴더에 있는 ‘train.py’ 스크립트를 실행해야 한다. 이 예제코드를 활용하기 위해 ‘player_random-walk_py’에 있는 무작위 움직임의 팀을 최초 상대팀으로 선택할 수 있다. 또한 훈련 성능을 향상시키려면 ‘game_time’을 100으로 줄이고 ‘반복’ 기능을 활성화하고 ‘교착 상태’ 상황을 비활성화하여 더 나은 경험 데이터를 얻는 것이 권장된다. 이 변수들은’config.json’ 파일에서 변경할 수 있다. 도81은 딥러닝 예제코드를 사용하기 위해 수정된 ‘config.json’ 파일을 예시한다. 언급된 예제코드는 일반적으로 1-2 일 동안 원하는 행동을 약 1백만번 반복하여 학습하도록 설계되었다. 더 복 잡한 행동을 배우려면 더 많은 훈련 시간이 필요할 수 있다. 성인 로봇과 관련된 ‘aisoccer_2.wbt’ 환경을 이용하여 약 1백만번 반복하여 얻은 보상을 보여주는 학습 보상 그래프가 도82에 예시되어 있다. 무작위 움직 임의 상대팀에 대한 훈련된 정책은 규칙 기반 상대팀에 대한 경기에 사용될 수 있지만, 이 경우 규칙 기반팀에 특화되지는 않은 것이 명확하다. 게이머는 훈련을 통하여 규칙 기반 상대방에 대해 다른 학습과정을 시작하여 경쟁력을 강화할 수 있다. 또한 성능을 개선하기 위해 자기팀을 상대로 훈련할 수도 있다. 훈련된 정책에 대 한 학습 진행 상황의 체크포인트는 200,000회 반복마다 저장되므로 게이머는 훈련 중에 훈련된 정책의 학습 진 행 상황을 확인할 수 있다. 훈련된 정책을 사용하려면 ‘play.py’ 스크립트를 사용해야 한다. 사용되는 기본 정책은 마지막으로 저장된 정책이다. 다른 모델 체크포인트를 사용하려면 ‘play.py’ 스크립트에서 ‘ CHECKPOINT’ 변수를 변경해야 한다. 실행할 때, ‘self.epsilon’의 값은 0으로 설정되어 있다 (신경망은 매 단계마다 작업을 선택한다). 보상함수는 효과적인 정책을 배우기 위한 심층 강화 학습(Deep Reinforcement Learning) 알고리즘의 가장 중요한 부분이다. 지금까지 로봇들이 공을 쫓고, 상대팀의 골대를 향해 공을 차고, 방어를 시도하고, 공을 소유하는 등 기본 축구 행동을 배우게 되는 간단한 보상함수가 소개되었다. 그러나 보 다 복잡한 보상 함수를 사용하여 더 강력한 전략을 훈련시킬 수 있다. AI 축구에는 게이머가 축구 게임에 대한 이해와 신념을 바탕으로 시도할 수 있는 보상함수의 종류가 무한하다. 상대팀을 포함하여 모든 로봇에 관한 정보를 고려하여 상태를 설계할 수 있다. 상태의 규모가 커지면 DQN 학습이 더욱 불안정해진다. 따라서 중요한 정보를 사용하여 전략을 익혀야 한다. 더 높은 차원의 상태, 더 많은 개별 동작 또는 복잡한 보상 함수를 사용 하려는 경우 심층 강화 학습 알고리즘을 훈련시키기 위한 탐색 단계도 늘려야 한다. IQL을 사용하는 것은 로봇 간의 협업 행동을 얻기 힘든 단점이 있다. 이 문제를 해결하기 위해 QMIX 및 그래프 콘볼루션 신경망과 같은 다른 알고리즘으로 팀 성능을 향상시킬 수 있다. 이러한 알고리즘을 사용하면, 상대팀의 상태와 작업을 고려하 여 보다 광범위한 전략을 구현할 수 있다. 무작위 움직임(random walk) 팀을 상대팀으로 할 경우 단점 중 하나 는 경기력이 너무 약해서 골키퍼가 효과적인 정책을 배우기가 어렵다는 것이다. 이 문제를 해결하기 위해 GK는 같은 팀을 상대팀으로 해서 경기하거나 보다 더 강한 상대팀과의 경기를 통해 재 훈련될 수 있다. 훈련 중에 자주 발생하지 않는 코너 킥과 페널티 킥과 같은 특수한 플레이 상황에 특화된 정책을 만들기 위해 RL 알고리즘 이 훈련될 특정 상황을 선택할 수 있다. 이러한 하이브리드 전략 즉 페널티 킥에 대한 심층 강화 학습 전략 훈 련에 대한 코드 구조는 도83에 예시되어 있다. 아래의 154 번째 줄의 코드는 신경망 페널티 킥 상태에서만 활 성화되는 것을 보여준다. 이처럼, 서로 다른 정책을 결합하여 강력한 팀을 만들 수 있다. 게임의 최종 목표는 전략이 다른 여러 상대팀에게 승리하는 것이다. 따라서, 정책을 보다 구체화하기 위해 게이머는 여러 다른 상 대팀을 대상으로 훈련하여 경기력을 향상시킬 수 있다. 알파 고 제로 (Alpha Go Zero)와 같은 경우 강력한 딥 러닝 프로그램을 훈련시키기 위해 훈련 중에 자기 자신을 상대하기도 한다. 축구게임과 같이 여러 에이전트가 동일한 팀의 승리라는 최종 목표를 공유하는 환경에서 에이전트는 최종 보상 에 대해 각기 다른 행동을 통해 최종 보상에 기여할 수 있다. 예를 들면, 경기에서 3골을 넣은 공격수가 3:0 승리에서 골키퍼 보다 더 많이 승리에 기여했다고 생각할 수 있다. 이러한 방식으로, 최종 보상에 대한 에이전 트들의 개별 기여를 계산할 수 있는 논리구조가 필요하다. 환경의 전역 상태(global state)를 기반으로 각 에 이전트가 보상에 대해 얼마나 기여했는지(중요한 역할을 했는지) 학습하게 된다. 이 문제를 풀어내기 위하여, ‘player_deeplearning-multi-qmix-dqn_py’ 예제코드에서 QMIX 알고리즘을 구현한다. QMIX 알고리즘은 각 로 봇(에이전트)들이 최종보상에 얼마나 기여했는지 학습하는 믹서(믹싱 네트워트)라는 네트워크를 추가한다. 이 과정을 통해 IQL의 단점 중 하나를 해결하고, 공유된 보상을 통해 에이전트들의 협력적인 행동을 달성하려고 한 다. 이 경우, 각 로봇에게는 현재 게임 상태에 따라 행동을 선택하도록 학습되어진 Actor 네트워크가 있다. Actor 네트워크는 DQN 아키텍처와 유사하다. 둘이상의 로봇이 최종 보상에 기여하는 경우, 이 로봇들의 각 Actor 네트워크의 출력은 믹서에 입력되어 믹서 네트워크가 최종 보상에 대한 기여를 학습하게 된다. 게임의 전역 상태는 믹서에 대한 입력이기도 하다. 믹서의 출력은 로봇이 나중에 받을 공유 보상을 추정하는 공유 Q- value 값이다. 믹서가 성공적으로 훈련되면 각 로봇에 대해 보상이 올바르게 할당되며, 따라서 모든 로봇들의 동일한 보상을 극대화하기 위하여 협력적인 행동이 장려될 수 있다. QMIX 알고리즘을 사용할 때 두가지 유형의 상태가 사용된다. 개별 상태는 로봇 네트워크에 대한 입력이다. 즉, 각 로봇들이 현재 게임의 상태를 어떻게 보고 있는지를 명시한다. 이 개별 상태는 IQL 예제 코드에서 사용되었던 상태와 유사하다. 도84에 예시된 QMIX 알고리즘과 같이 전역 상태(Global state)는 믹싱 네트워크의 입력으로서 사용된다. 전역상태는 전체 게 임의 정보를 담고 있다. 즉, 최종 보상의 기여도를 알기 위해 필요한 각 로봇의 정보 및 로봇간의 상관관계에 대한 정보를 포함한다. ‘player_deeplearning-multi-qmix-dqn_py’ 예제 코드에서는 수비수와 공격수 간의 협력적인 행동을 시도한다. 골키퍼는 개별적인 DQN을 이용하여 행동을 학습한다. ‘player_deeplearning- multi-qmix-dqn_py’ 예제 코드에 사용된 QMIX 프레임워크는 도85에 예시된다. 그림에서 두개의 믹싱 네트워크 가 있다는 것을 알 수 있다. 하나의 믹싱 네트워크는 수비수 D1, D2를 훈련시키는데 사용되고, 다른 하나는 공 격수 F1, F2를 훈련시키는데 사용된다. IQL 예제코드와 다른 점은 ‘qmix.py’ 스크립트의 존재이다. 학습 스 크립트는 Actor 네트워크뿐 아니라 믹싱 네트워크도 학습하도록 업데이트 되었다. 믹싱 네트워크 아키텍처는 ‘mixer.py’ 스크립트에 정의되어 있다. IQL 예제 코드를 위해 설계된 상태, 행동 및 보상은 QMIX 예제 코드 에서도 사용될 수 있다. 전역 상태는 ‘rl_utils.py’에 정의되어 있으며 경기장에 있는 모든 로봇의 좌표와 방향으로 구성된다. ‘player_deeplearning-multi-qmix-dqn_py’ 예제 코드에서 훈련된 모델을 학습하고 사용 하려면, IQL 예제 코드 설명에서 설명한 것과 같은 방법으로 따라한다. QMIX를 사용하여 정책을 학습시키면 로 봇 상호간의 협업 행동이 향상되지만, Actor 네트워크와 믹싱 네트워크가 아래와 같이 함께 훈련되므로, 더 많 은 탐색이 필요하다. QMIX를 사용하는 학습을 개선하려면, 수비수와 공격수가 공유하는 목표와 보상이 효과적 으로 설계되어야 한다. 예를 들어, 수비수를 위한 공동의 목표는 상대팀의 공을 다시 내 팀의 공으로 만드는 것이다. 공격수의 경우 공유목표는 골을 넣는 것이라 할 수 있다. 또한 전역 상태 설계에서는 각각의 에이전 트들이 보상에서 어느정도 기여를 했는가에 대한 정보만을 포함해야 한다. 보상 및 전역 상태 설계는 ‘ player_deeplearning-multi-qmix-dqn_py ‘폴더의 ‘rl_utils.py’파일에서 수정할 수 있다. 게이머는 로봇 바퀴를 제어하는 연속변수 (크기: [-max_velocity, max_velocity]), 전면 슬라이더(크기: [0, 10]) 및 하단 슬라이더(크기: [0, 10])를 사용하여 로봇의 움직임을 원하는 방식대로 제어한다. 이러한 제어 변수들이 연속적이기 때문에, 딥러닝 예제 코드에서 사용된 일련의 연속 동작들 또한 연속적이라는 것을 알 수 있다. 그러나 IQL 예제 코드에서는 연속된 동작들을 사용하는 것이 아니라, 미리 만들어진 13개로 미리 나누어 진 한정된 동작들을 선택하도록 되어 있다. (전진, 후진, 회전, 패스, 킥, 패스, 크로스 등) 이러한 나누어진 개별동작 세트들을 사용하면 탐색 및 훈련 성능을 높이는데 도움이 된다. 하지만, 실제로 로봇이 연속동작을 사용한다면 좀더 자연스럽고 부드러운 정밀한 동작을 수행할 수 있을 것이다. (예를 들어 뚝뚝 끊어지는 움직임 보다 부드럽게 움직이는 것이 더 정밀하고 세밀한 동작을 구현할 수 있을 것이다.) 상태를 연속 동작으로 맵핑 하기 위하여 Deep Deterministic Policy Gradients (DDPG) 알고리즘을 사용할 수 있다. ‘ player_deeplearning- single-ddpg_py’ 폴더에 DDPG 예제 코드가 구현되어 있다. DDPG 알고리즘은 연속적인 제어를 위한 강화학습 알고리즘으로 유명하다. DDPG와 DQN의 가장 큰 차이점은 작업에 대한 연속값을 처리하는 기능이다. DQN 예제에서 우리는 로봇이 13개의 구분 동작을 갖도록 하였지만, DDPG는 연속동작공간을 탐색하고 현재 상태를 연속동작으로 매핑할 수 있게 한다. IQL 예제 코드에 도입된 DQN 아키텍처에서 출력은 개별동작 세트에서 가능한 각 동작에 대한 Q-value이다. 반면, 연속세트를 이용하여 동일한 DQN 아키텍처가 사용된 경우, 출력 레이어의 크기는 무한대로 커지게 된다. 이러한 문제를 해결하기 위해, DDPG 알고리즘은 구조적으 로 Actor 네트워크와 Critic 네트워크(비평가 네트워크)로 나뉜다. Actor 네트워크는 현재 게임 상태에 따라 연속 동작을 선택해야 한다. Critic 네트워크는 해당 상태에서 특정 행동을 했을 때의 보상을 평가해야 한다. 따라서 그 상황에서 행동을 취했을 때 value를 할당한다. 해당 상태에서 해당 행동을 특정 상황에서 행동을 취 했을 때의 Q-value는 미래의 보상을 통해 모델링 되어진다. DDPG 알고리즘은 ‘ddpg.py’ 예제 파일에 구현되 었다. IQL 예제코드에서 설명된 방식을 똑같이 적용하면 된다. 도86은 DDPG 알고리즘을 예시한다. 지도학습은 컴퓨터 비전, 자연어 처리 및 음성 처리 영역에서 지난 몇 년간 획기적인 성과를 이뤘다. 지도학습 기법을 이용하여 신경망은 입력 및 출력의 상관관계를 학습하도록 훈련된다. 예를 들어 영상 데이터에서 입력 은 실제 영상이며, 출력은 영상에 나타난 물체의 종류일 수 있고, 혹은 보다 구체적인 특정 물체일 수 있다. 이 예제에서는 10시간의 게임을 두 가지 규칙 기반 알고리즘을 이용하여 생성된 데이터셋을 사용하여 원하는 규 칙 기반 알고리즘의 동작을 모방하려고 한다. 이 경우 입력은 게임 상태이고 출력은 해당 상태에서 선택한 행 동이다. 샘플 코드인 ‘play.py’ 스크립트를 훈련시키려면 상기 데이터셋(dataset)을 ‘ player_supervised_py’ 폴더의 ‘data’ 폴더로 복사한다. 훈련은 ‘player_supervised_py’ 폴더에 있는 ‘ train_supervised.py’ 스크립트를 사용하여 실행된다. 이 예제코드에서 상태 및 행동(‘rl_utils.py’에 정의)은 다음과 같이 모델링된다. 상태는 공을 기준으로 로봇의 상대 위치, 각 로봇의 내 팀 및 상대팀의 다른 로봇까지의 상대 위치 및 로봇 속도로 규정된 37개의 값(37차원 벡터)으로 나타난다. 도87은 지도학습 코드에 서 상태 설계를 예시한다. 규칙 기반 알고리즘의 출력값들의 유사한 패턴들을 묶어서 20개의 개별 행동을 정의 한다. 이러한 개별행동들은 규칙 기반 알고리즘에 의해 가장 자주 관측되는 20가지 동작으로 해석될 수 있다. 출력에서 softmax 분류 기법을 사용하여 신경망 지도학습을 단순화하기 위해 자주 관측되는 연속 동작과 유사한 개별 동작이 사용된다. 도88은 지도학습 코드에서 행동 설계를 예시한다. 일 실시예에서, 사용자 단말은 스마트폰(smartphone), 스마트패드(smartpad), 태블릿 PC(tablet personal computer), 데스크탑 PC(desktop PC), 랩탑 PC(laptop PC), 넷북 컴퓨터(netbook computer), 워크스테이션 (workstation), 웨어러블 장치(wearable device)(예: 스마트 안경, 머리 착용형 장치(head-mounted- device(HMD)), 또는 스마트 와치(smart watch)), TV 박스(예: 삼성 HomeSync™, 애플TV™, 또는 구글 TV™), 또는 게임 콘솔(예: Xbox™, PlayStation™, Switch™) 중 적어도 하나를 포함할 수 있다. 본 발명의 다양한 실시 예에 따르면, 사용자 단말은 상기 객체, 상기 AI 알고리즘, 상기 입력 데이터 형식, 및 상기 행동 패턴을 사용자로부터 입력 받기 위한 UI(User Interface) 화면을 디스플레이 장치를 통해 사용자에게 제공할 수 있다. 사용자 단말은 상기 UI 화면을 자체적으로 가지고 있는 데이터를 이용하여 제공할 수도 있고, 서버 시스템으로 부터 상기 UI 화면에 대응하는 데이터를 수신하여 제공할 수도 있다. 상기 객체, 상기 AI 알고리즘, 상기 입력 데이터 형식, 및 상기 행동 패턴 각각은 하나의 UI 화면에 포함될 수도 있고, 각각이 개별적인 UI 화면을 가질 수도 있다. 본 발명의 다양한 실시 예에 따르면, 상기 디스플레이 장치는, 예를 들어, 전자 장치에 일체형으로 구비된 디스플레이(예를 들어, 액정 디스플레이(LCD), 발광 다이오드(LED) 디스플레이, 유기 발광 다이오드 (OLED) 디스플레이, 또는 마이크로 전자기계 시스템(microelectromechanical systems, MEMS) 디스플레이, 또는 전자 종이(electronic paper) 디스플레이 등), LED 모니터 및 LCD 모니터, IPTV, Smart TV, LED TV, 및 LCD TV, 또는 프로젝터 등을 포함할 수 있다. 네트워크는 단말들 및 서버들과 같은 각각의 노드 상호 간에 정보 교 환이 가능한 연결 구조를 의미하는 것으로, 통신 네트워크(telecommunications network), 예를 들면, 컴퓨터 네 트워크(computer network)(예: LAN 또는 WAN), 인터넷(Internet), 또는 전화 망(telephone network) 등을 포함할 수 있다. 본 발명의 다양한 실시 예에 따르면, 사용자 단말 및 서버 시스템은 무선 통신 또는 유선 통신을 통해서 네트워크에 연결되어 상호 통신을 수행할 수 있다. 무선 통신은, 셀룰러 통신 프로토콜로서, 예를 들어, LTE, LTE-A, CDMA, WCDMA, UMTS, WiBro, 또는 GSM 등을 포함할 수 있다. 또한 무선 통신은, 근거리 통 신으로서, 예를 들어, Wi-Fi, Bluetooth, NFC(near field communication), 또는 GPS(global positioning system) 등 중 적어도 하나를 포함할 수 있다. 유선 통신은, 예를 들어, USB(universal serial bus), HDMI(high definition multimedia interface), RS-232(recommended standard 232), 또는 POTS(plain old telephone service) 등 중 적어도 하나를 포함할 수 있다. 본 발명의 일 실시예는 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행 가능한 명령어를 포 함하는 기록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스될 수 있는 임의 의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독가능 매체는 컴퓨터 저장 매체 및 통신 매체를 모두 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구 현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈, 또는 반송파와 같은 변조된 데이터 신호의 기타 데이터, 또는 기타 전송 메커니즘을 포함하며, 임의의 정보 전달 매체를 포함한다."}
{"patent_id": "10-2022-0044920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "전술한 본 발명의 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자는 본 발명 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 발명의 범위는 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으로 해석되어야 한다."}
{"patent_id": "10-2022-0044920", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도1은 인공지능, 머신러닝 및 딥러닝 간 상관관계를 나타내는 도면 도2는 상태, 행동, 보상을 포함한 Atari 게임 프레임워크를 나타내는 도면 도3은 화면에 표시된 Atari 게임의 상태에 따라 행동을 결정하는 심층 신경망을 나타내는 도면 도4는 게임 화면의 상태를 기준으로 Atari 게임의 행동을 선택하는 심층 신경망으로 대체된 Atari 게임의 구조 를 나타내는 도면 도5는 게임 상태에 따라 로봇 동작을 선택하는 심층신경망으로 대체한 AI 축구 구조를 나타내는 도면 도6은 AI 축구 로봇 플레이 환경을 예시하는 도면 도7은 Cyberbotics Ltd.에서 제공하는 Webots 시뮬레이터의 다운로드 페이지를 나타내는 도면 도8은 Webots 시뮬레이터 Preferences 선택 창을 나타내는 도면 도9는 시스템 환경변수에 PYTHONPATH 추가하는 인터페이스를 예시하는 도면 도10은 AI 축구 시뮬레이터를 다운로드 페이지를 도시하는 도면 도11은 GPU를 사용하지 않는 심층 학습 라이브러리 설치의 경우 명령어를 예시하는 도면 도12는 AI 축구 시뮬레이터 구조를 예시하는 도면 도13은 Webots 시뮬레이터 실행 창을 예시하는 도면 도14는 청소년용 로봇을 위한 AI 축구 경기장 및 어른용 로봇을 위한 AI 축구 경기장을 예시한 도면 도15는 청소년용 AI 축구 로봇 및 어른용 AI 축구 로봇을 예시한 도면 도16은 Webots 시뮬레이터에 의해 내부적으로 수행되는 청소년용 AI 축구 로봇 및 어른용 AI 축구 로봇의 물리 적인 모양을 단순화한 도면 도17은 AI 축구 로봇을 예시한 도면 도18은 로봇의 세가지 행동을 예시한 도면 도19는 드리블 영역을 예시한 도면 도20은 축구공을 예시한 도면 도21은 AI 축구 시뮬레이터와 게이머 코드 간의 통신 구조를 예시하는 도면 도22는 AI 축구 시뮬레이터와 게이머 코드가 송수신 하는 정보를 예시하는 도면 도23은 게이머에게 제공되는 이미지 정보를 예시하는 도면 도24는 로봇의 가능한 행동들을 예시하는 도면 도25는 웹-베이스 AI 축구 코드 생성기를 예시하는 도면 도26은 규칙 기반 AI 축구 코드 생성기를 예시하는 도면 도27은 규칙 기반 AI 축구 코드 생성기의 블록 카테고리를 예시한 도면도28은 규칙 기반 AI 축구 코드 생성기의 블록 카테고리 중에서 ‘Environment indices’를 예시한 도면 도29는 규칙 기반 AI 축구 코드 생성기의 블록 카테고리 중에서 ‘Environment Constants’를 예시한 도면 도30은 규칙 기반 AI 축구 코드 생성기의 블록 카테고리 중에서 ‘Environment variables’를 예시한 도면 도31은 규칙 기반 AI 축구 코드 생성기의 블록 카테고리 중에서 ‘Environment fucntions’를 예시한 도면 도32는 규칙 기반 AI 축구 코드 생성기에 의한 코드를 예시한 도면 도33은 상위 레벨의 행동들로 정의된 규칙기반 예제 코드 도면 도34는 생성된 코드를 예시한 도면 도35는 경기용 좌표계와 각도 시스템을 예시한 도면 도36은 경기장의 필드 구역 명칭을 예시한 도면 도37은 킥오프 포메이션(공 소유팀: A)을 예시한 도면 도38은 수비시 코너킥 포메이션 1(공 소유팀: A)을 예시한 도면 도39는 수비시 코너킥 포메이션 2(공 소유팀: A)을 예시한 도면 도40은 공격시 코너킥 포메이션 1(공 소유팀: A)을 예시한 도면 도41은 공격시 코너킥 포메이션 2(공 소유팀: A)을 예시한 도면 도42는 페널티 킥 포메이션(공 소유팀: A)을 예시한 도면 도43은 골 킥 포메이션(공 소유팀: A)을 예시한 도면 도44는 전역 변수로 선언된 상수 코드를 예시한 도면 도45는 player_rulebasedA.py 파일을 예시한 도면 도46은 init() 함수 안에 있는 변수를 초기화하는 과정을 예시한 도면 도47은 예시적인 update() 함수를 나타내는 도면 도48은 예시적인 action.py 파일을 나타내는 도면 도49는 예시적인 helper.py 파일을 나타내는 도면 도50은 Goalkeeper(GK), Defender_1(D1), Defender_2(D2), Forward_1(F1), Forward_2(F2)의 정의를 예시하는 도면 도51은 로봇 Forward_2에 의해 정의된 ‘init()’과 ‘move()’ 함수의 예제를 나타내는 도면 도52는 좌표값에 따른 로봇의 위치를 예시하는 도면 도53은 공격수들에게 기대하는 움직임을 예시하는 도면 도54는 공격수들에게 기대하는 움직임을 예시하는 도면 도55는 공격수들에게 기대하는 움직임을 예시하는 도면 도56은 F1의 기대되는 행동 흐름을 예시한 흐름도 도57은 F2의 기대되는 행동 흐름을 예시한 흐름도 도58은 심층 강화학습 구조에 대한 개념도 도59는 딥러닝 예제코드의 구성을 나타내는 도면 도60은 상태 설계의 예시로서 공과 로봇의 상대적 거리와 각도를 나타내는 도면 도61은 시각적인 GK 상태의 설계를 예시하는 도면 도62는 GK 상태 관련 코드를 예시하는 도면도63은 시각적인 D1/D2 상태의 설계를 예시하는 도면 도64는 D1/D2 상태 관련 코드를 예시하는 도면 도65는 시각적인 F1/F2 상태의 설계를 예시하는 도면 도66은 F1/F2 상태 관련 코드를 예시하는 도면 도67은 IQL 알고리즘에 기반한 GK 행동 설계를 예시하는 도면 도68은 IQL 알고리즘에 기반한 D1/D2 행동 설계를 예시하는 도면 도69는 IQL 알고리즘에 기반한 F1/F2 행동 설계를 예시하는 도면 도70은 상대팀의 공차기에 대한 방어의 보상으로서 ‘차기 방지’ 보상을 예시하는 도면 도71은 공과의 거리에 대한 보상을 예시하는 도면 도72는 방향에 대한 보상을 예시하는 도면 도73은 슛/패스/크로스 보상을 예시하는 도면 도74는 득점/실점에 관한 보상을 예시하는 도면 도75는 GK, D1/D2 및 F1/F2에 대한 보상을 예시하는 도면 도76은 DQN 알고리즘 구조를 예시하는 도면 도77은 IQL 알고리즘 구조를 예시하는 도면 도78은 DQN을 이용한 IQL의 최종 구조를 예시하는 도면 도79는 은닉 계층수와 각 계층의 신경세포 개수를 ‘num_layers’ 및 ‘hidden_dim’ 변수를 사용하여 ‘ parameters.json’ 파일에서 정의하는 예시를 나타내는 도면 도80은 신경망의 실제 구조를 예시하는 도면 도81은 딥러닝 예제코드를 사용하기 위해 수정된 ‘config.json’ 파일을 예시하는 도면 도82는 학습 보상 그래프를 예시하는 도면 도83은 하이브리드 전략 즉 페널티 킥에 대한 심층 강화 학습 전략 훈련에 대한 코드 구조를 예시하는 도면 도84는 QMIX 알고리즘을 예시하는 도면 도85는 QMIX 프레임워크를 예시하는 도면 도86은 DDPG 알고리즘을 예시하는 도면 도87은 지도학습 코드에서 상태 설계를 예시하는 도면 도88은 지도학습 코드에서 행동 설계를 예시하는 도면"}
