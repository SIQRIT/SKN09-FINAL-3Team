{"patent_id": "10-2018-0102689", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0027087", "출원번호": "10-2018-0102689", "발명의 명칭": "로봇 및 그의 제어 방법", "출원인": "삼성전자주식회사", "발명자": "홍순혁"}}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇의 제어방법에 있어서,영상을 촬영하는 단계;상기 영상에서 바닥면을 분할한 제1 영상을 획득하는 단계;학습된 제1 인공지능모델을 이용하여 상기 영상에 포함된 장애물을 인식하는 단계;상기 제1 영상과 상기 인식된 장애물을 포함하는 영역에 대한 정보를 바탕으로 상기 로봇이 태스크를 수행하기위한 제2 영상을 획득하여 저장하는 단계; 및상기 제2 영상을 바탕으로 상기 로봇이 이동하는 단계;를 포함하는 제어방법."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 영상을 촬영하는 상기 로봇의 이동상태정보 및 촬영방향정보를 획득하는 단계;를 더 포함하고,상기 저장하는 단계는,상기 이동상태정보 및 상기 방향정보를 상기 제2 영상과 함께 저장하는 단계;를 더 포함하는 제어방법."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 제어방법은,상기 제2 영상과 상기 이동상태정보를 이용하여 상기 로봇과 벽면 간의 거리를 판단하는 단계;를 더 포함하는제어방법."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 제어방법은,상기 벽면까지의 거리를 바탕으로 상기 로봇의 이동속도를 결정하는 단계;를 더 포함하는 제어방법."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2 항에 있어서,상기 제어방법은,상기 이동상태정보 및 촬영방향정보와 함께 저장된 상기 제2 영상을 이용하여 상기 로봇의 이동가능영역에 대한네비게이션 맵을 생성하는 단계;를 더 포함하는 제어방법."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 제어방법은, 상기 네비게이션 맵 및 촬영방향정보와 함께 저장된 상기 제2 영상을 이용하여 로봇의 위치를 판단하고 이동계획을 수립하는 단계;를 더 포함하는 제어방법. 공개특허 10-2020-0027087-3-청구항 7 제1항에 있어서,상기 제1 영상을 획득하는 단계는,영상 속에 바닥면 영역을 감지하도록 학습된 제2 인공지능 모델을 이용하여 제1 영상을 획득하는 제어방법."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서,상기 장애물을 포함하는 영역은,상기 제1 인공지능 모델을 통해 인식된 장애물의 유형에 따라 상기 영역의 크기가 변하는 것을 특징으로 하는제어방법."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 제어방법은,상기 촬영한 영상의 뎁스 정보를 포함하는 뎁스 맵 영상을 획득하는 단계;를 더 포함하고, 상기 뎁스 맵 영상과 상기 인식된 장애물을 포함하는 영역에 대한 정보를 바탕으로 상기 청소 로봇이 태스크를수행하기 위한 제3 영상을 획득하여 저장하는 단계; 및상기 제3 영상을 바탕으로 상기 로봇이 이동하는 단계;를 포함하는 제어방법."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "로봇에 있어서,카메라;구동부;메모리; 및영상을 촬영하도록 상기 카메라를 제어하고, 상기 영상에서 바닥면을 분할한 제1 영상을 획득하고, 학습된 제1 인공지능모델을 이용하여 상기 영상에 포함된 장애물을 인식하고, 상기 제1 영상과 상기 인식된 장애물을 포함하는 영역에 대한 정보를 바탕으로 상기 로봇이 태스크를 수행하기위한 제2 영상을 획득하여 상기 메모리에 저장하고, 상기 제2 영상을 바탕으로 상기 로봇이 이동하도록 상기 구동부를 제어하는 프로세서;를 포함하는 청소 로봇."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 로봇은, 센서;를 더 포함하고, 상기 프로세서는,상기 센서를 통해 영상을 촬영하는 상기 로봇의 이동상태정보 및 촬영방향정보를 획득하면, 상기 이동상태정보및 상기 방향정보를 상기 제2 영상과 함께 상기 메모리에 저장하는 로봇."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 프로세서는,공개특허 10-2020-0027087-4-상기 제2 영상과 상기 이동상태정보를 이용하여 상기 로봇과 벽면 간의 거리를 판단하는 로봇."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 프로세서는,상기 벽면까지의 거리를 바탕으로 상기 로봇의 이동속도를 결정하는 로봇."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 프로세서는,상기 이동상태정보 및 촬영방향정보와 함께 저장된 상기 제2 영상을 이용하여 상기 로봇의 이동가능영역에 대한네비게이션 맵을 생성하여 상기 메모리에 저장하는 로봇."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 프로세서는,상기 네비게이션 맵 및 촬영방향정보와 함께 저장된 상기 제2 영상을 이용하여 로봇의 위치를 판단하고 이동계획을 수립하는 로봇."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제10항에 있어서,상기 프로세서는,영상 속에 바닥면 영역을 감지하도록 학습된 제2 인공지능 모델을 이용하여 제1 영상을 획득하는 로봇."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제10항에 있어서,상기 프로세서는,상기 제1 인공지능 모델을 통해 인식된 장애물의 유형에 따라 상기 장애물을 포함하는 영역의 크기를 변경하는로봇."}
{"patent_id": "10-2018-0102689", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제10항에 있어서,상기 프로세서는,상기 촬영한 영상의 뎁스 정보를 포함하는 뎁스 맵 영상을 획득하고,상기 뎁스 맵 영상과 상기 인식된 장애물을 포함하는 영역에 대한 정보를 바탕으로 상기 로봇이 청소를 수행하기 위한 제3 영상을 획득하여 상기 메모리에 저장하고,상기 제3 영상을 바탕으로 상기 청소 로봇이 이동하도록 상기 구동부를 제어하는 로봇."}
{"patent_id": "10-2018-0102689", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 로봇 및 그의 제어 방법에 관한 것으로, 더욱 구체적으로는, 로봇의 전면 영상을 이용하여 주변 장애 물 및 구조물을 파악하여 효율적이고 안전한 이동 경로를 제공할 수 있는 로봇 및 그의 제어 방법에 관한 것이다. 본 개시의 일 실시 예에 따른 로봇의 제어방법은 영상을 촬영하는 단계, 영상에서 바닥면을 분할한 제1 영상을 획득하는 단계, 학습된 제1 인공지능모델을 이용하여 영상에 포함된 장애물을 인식하는 단계, 제1 영상과 인식된 장애물을 포함하는 영역에 대한 정보를 바탕으로 로봇이 태스크를 수행하기 위한 제2 영상을 획득하여 저장하는 단계, 제2 영상을 바탕으로 로봇이 이동하는 단계를 포함할 수 있다."}
{"patent_id": "10-2018-0102689", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 로봇 및 그의 제어 방법에 관한 것으로, 더욱 구체적으로는, 로봇의 전면 영상을 이용하여 주변 장애 물 및 구조물을 파악하여 효율적이고 안전한 이동 경로를 제공할 수 있는 로봇 및 그의 제어 방법에 관한 것이다."}
{"patent_id": "10-2018-0102689", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "로봇 기술의 발전에 따라 전문화된 학술 분야 또는 대규모의 노동력을 요하는 산업 분야뿐만 아니라 일반적인 가정에도 로봇의 공급이 보편화 되고 있다. 사용자에게 가사 서비스를 제공하는 서비스 로봇, 청소 로봇, 애완 용 로봇 등이 많이 보급되고 있다. 특히 청소 로봇과 같은 경우, 청소 가능 영역을 판단하고 가능한 넓은 영역을 안전하게 청소해야 하는 것이 매 우 중요하다. 종래의 청소 로봇은 제한된 센서 조합과 센싱 범위로 인해 근거리의 장애물과 구조물의 유무 정보 만을 가지고 회피 주행을 수행하였다. 이에, 종래의 청소 로봇은 넓은 청소 영역을 활용한 효율적인 청소를 수 행하지 못하기 때문에, 청소 로봇의 사용자는 의도와 다르게 움직이는 청소 로봇을 보고 답답함을 느낄 수 있었 다. 또한, 종래의 청소 로봇은 구조물과 주변 장애물을 구분하지 않고 청소를 수행하였기 때문에, 장애물도 구조물 과 마찬가지로 근거리 센서에 의해서 감지될 때까지 근접하였다. 이에 따라, 민감한 장애물의 경우에 청소 로봇 에 의해 훼손되는 문제점이 존재하였다."}
{"patent_id": "10-2018-0102689", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 필요성에 따른 것으로, 본 개시의 목적은, 청소 로봇의 전방카메라 영상을 이용해 주변 장애 물과 구조물을 구분하고, 이동 가능한 영역에 대한 정보를 정확히 파악하여 사용자에게 원활한 서비스를 제공할 수 있는 청소 로봇 및 그의 제어 방법을 제공함에 있다."}
{"patent_id": "10-2018-0102689", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 서비스를 제공하기 위한 본 개시의 일 실시 예에 따른 청소 로봇의 제어방법은 영상을 촬영하는 단계; 상기 영상에서 바닥면을 분할한 제1 영상을 획득하는 단계; 학습된 제1 인공지능모델을 이용하여 상기 영상에 포함된 장애물을 인식하는 단계; 상기 제1 영상과 상기 인식된 장애물을 포함하는 영역에 대한 정보를 바탕으로 상기 청소 로봇이 청소를 수행하기 위한 제2 영상을 획득하여 저장하는 단계; 및 상기 제2 영상을 바탕으로 상 기 청소 로봇이 이동하는 단계;를 포함할 수 있다. 또한, 상기 제어방법은, 영상을 촬영하는 상기 청소 로봇의 이동상태정보 및 촬영방향정보를 획득하는 단계;를 더 포함하고, 상기 저장하는 단계는, 상기 이동상태정보 및 상기 방향정보를 상기 제2 영상과 함께 저장하는 단 계;를 더 포함할 수 있다. 또한, 상기 제어방법은, 상기 제2 영상과 상기 이동상태정보를 이용하여 상기 청소 로봇과 벽면 간의 거리를 판 단하는 단계;를 더 포함할 수 있다. 또한, 상기 제어방법은, 상기 벽면까지의 거리를 바탕으로 상기 청소 로봇의 이동속도를 결정하는 단계;를 더 포함할 수 있다. 또한, 상기 제어방법은, 상기 이동상태정보 및 촬영방향정보와 함께 저장된 상기 제2 영상을 이용하여 상기 청 소 로봇의 이동가능영역에 대한 네비게이션 맵을 생성하는 단계;를 더 포함할 수 있다. 또한, 상기 제어방법은, 상기 네비게이션 맵 및 촬영방향정보와 함께 저장된 상기 제2 영상을 이용하여 청소 로 봇의 위치를 판단하고 이동계획을 수립하는 단계;를 더 포함할 수 있다. 또한, 상기 제1 영상을 획득하는 단계는, 영상 속에 바닥면 영역을 감지하도록 학습된 제2 인공지능 모델을 이 용하여 제1 영상을 획득할 수 있다. 또한, 상기 장애물을 포함하는 영역은, 상기 제1 인공지능 모델을 통해 인식된 장애물의 유형에 따라 상기 영역 의 크기가 변할 수 있다. 또한, 상기 제어방법은, 상기 촬영한 영상의 뎁스 정보를 포함하는 뎁스 맵 영상을 획득하는 단계;를 더 포함하 고, 상기 뎁스 맵 영상과 상기 인식된 장애물을 포함하는 영역에 대한 정보를 바탕으로 상기 청소 로봇이 청소 를 수행하기 위한 제3 영상을 획득하여 저장하는 단계; 및 상기 제3 영상을 바탕으로 상기 청소 로봇이 이동하 는 단계;를 포함할 수 있다. 상술한 서비스를 제공하기 위한 본 개시의 일 실시 예에 따른 청소 로봇은, 카메라; 구동부; 메모리; 및 영상을 촬영하도록 상기 카메라를 제어하고, 상기 영상에서 바닥면을 분할한 제1 영상을 획득하고, 학습된 제1 인공지 능모델을 이용하여 상기 영상에 포함된 장애물을 인식하고, 상기 제1 영상과 상기 인식된 장애물을 포함하는 영 역에 대한 정보를 바탕으로 상기 청소 로봇이 청소를 수행하기 위한 제2 영상을 획득하여 상기 메모리에 저장하 고, 상기 제2 영상을 바탕으로 상기 청소 로봇이 이동하도록 상기 구동부를 제어하는 프로세서;를 포함할 수 있 다. 또한, 상기 청소 로봇은, 센서;를 더 포함하고, 상기 프로세서는, 상기 센서를 통해 영상을 촬영하는 상기 청소 로봇의 이동상태정보 및 촬영방향정보를 획득하면, 상기 이동상태정보 및 상기 방향정보를 상기 제2 영상과 함 께 상기 메모리에 저장할 수 있다. 또한, 상기 프로세서는, 상기 제2 영상과 상기 이동상태정보를 이용하여 상기 청소 로봇과 벽면 간의 거리를 판 단할 수 있다. 또한, 상기 프로세서는, 상기 벽면까지의 거리를 바탕으로 상기 청소 로봇의 이동속도를 결정할 수 있다. 또한, 상기 프로세서는, 상기 이동상태정보 및 촬영방향정보와 함께 저장된 상기 제2 영상을 이용하여 상기 청 소 로봇의 이동가능영역에 대한 네비게이션 맵을 생성하여 상기 메모리에 저장할 수 있다. 또한, 상기 프로세서는, 상기 네비게이션 맵 및 촬영방향정보와 함께 저장된 상기 제2 영상을 이용하여 청소 로 봇의 위치를 판단하고 이동계획을 수립할 수 있다. 또한, 상기 프로세서는, 영상 속에 바닥면 영역을 감지하도록 학습된 제2 인공지능 모델을 이용하여 제1 영상을 획득할 수 있다. 또한, 상기 프로세서는, 상기 제1 인공지능 모델을 통해 인식된 장애물의 유형에 따라 상기 장애물을 포함하는 영역의 크기를 변경할 수 있다. 또한, 상기 프로세서는, 상기 촬영한 영상의 뎁스 정보를 포함하는 뎁스 맵 영상을 획득하고, 상기 뎁스 맵 영 상과 상기 인식된 장애물을 포함하는 영역에 대한 정보를 바탕으로 상기 청소 로봇이 청소를 수행하기 위한 제3 영상을 획득하여 상기 메모리에 저장하고, 상기 제3 영상을 바탕으로 상기 청소 로봇이 이동하도록 상기 구동부 를 제어할 수 있다."}
{"patent_id": "10-2018-0102689", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상과 같은 본 개시의 다양한 실시 예에 따르면, 청소 로봇이 장애물과 구조물을 정확히 파악하고, 넓은 공간 을 이용한 효율적인 서비스를 제공할 수 있다."}
{"patent_id": "10-2018-0102689", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 문서의 다양한 실시예가 첨부된 도면을 참조하여 기재된다. 그러나 이는 본 문서에 기재된 기술을 특 정한 실시 형태에 대해 한정하려는 것이 아니며, 본 문서의 실시예의 다양한 변경(modifications), 균등물 (equivalents), 및/또는 대체물(alternatives)을 포함하는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조 부호가 사용될 수 있다. 또한, 본 문서에서 사용된 \"제 1,\" \"제 2,\" 등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없 이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들을 한정하지 않는 다. 예를 들면, 제 1 사용자 기기와 제 2 사용자 기기는, 순서 또는 중요도와 무관하게, 서로 다른 사용자 기기 를 나타낼 수 있다. 예를 들면, 본 문서에 기재된 권리 범위를 벗어나지 않으면서 제 1 구성요소는 제 2 구성요 소로 명명될 수 있고, 유사하게 제 2 구성요소도 제 1 구성요소로 바꾸어 명명될 수 있다. 어떤 구성요소(예: 제 1 구성요소)가 다른 구성요소(예: 제 2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결 되어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제 3 구성요소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제 1 구성요소)가 다 른 구성요소(예: 제 2 구성요소)에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 상기 어 떤 구성요소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제 3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 문서에서 사용된 용어들은 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 다른 실시예의 범위를 한정 하려는 의도가 아닐 수 있다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 문서에 기재된 기술 분야에서 통상 의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 문서에 사용된 용어들 중 일반적인 사전에 정의된 용어들은, 관련 기술의 문맥상 가지는 의미와 동일 또는 유사한 의미로 해석될 수 있으 며, 본 문서에서 명백하게 정의되지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 경우에 따라서, 본 문서에서 정의된 용어일지라도 본 문서의 실시 예들을 배제하도록 해석될 수 없다. 이하에서, 첨부된 도면을 이용하여 본 발명의 다양한 실시 예들에 대하여 구체적으로 설명한다. 도 1은 본 개시 의 일 실시 예에 따른, 청소 로봇의 구성을 설명하기 위한 간단한 블록도이다. 도 1을 참조하면, 청소 로 봇은 구동부, 카메라, 메모리, 프로세서를 포함할 수 있다. 청소 로봇은 스스로 이동하며 사용자에게 청소 서비스를 제공하는 장치로, 다양한 형태의 전자장치로 구현 될 수 있다. 예를 들면, 청소 로봇은 가정용 청소 로봇, 대형건물용 청소 로봇, 공항용 청소 로봇 등 다양 한 목적을 위해 원기둥 형태, 직육면체 형태 등 다양한 형태로 구현될 수 있다. 일 실시 예에 따르면, 청소 로 봇은 바닥의 이물질을 단순히 제거하는 태스크를 수행할 뿐만 아니라, 사용자의 명령에 따라 물체를 옮기 는 태스크도 수행할 수 있다. 구동부는 청소 로봇을 구동시킬 수 있다. 예를 들어, 구동부는 프로세서의 제어에 의해 태 스크를 수행할 위치로 청소 로봇을 이동시킬 수 있다. 이러한 경우에, 구동부는 바닥면과 접촉하는적어도 하나의 바퀴, 바퀴에 동력을 제공하는 모터 및 모터를 제어할 드라이버를 포함할 수 있다. 다른 예로, 구동부는 태스크를 수행하기 위한 동작을 구현할 수 있다. 물체 이동 태스크의 경우에 구동부는 물체 를 집어드는 동작 등을 수행하기 위한 모터를 포함할 수 있다. 카메라는 청소 로봇의 주변 영상을 다양한 방면으로 촬영하기 위한 구성이다. 특히, 카메라는 청소 로봇의 전면 영상을 촬영할 수도 있고, 주행 방향과 다른 방향에 대한 영상을 촬영할 수도 있다. 카메라는 복수의 카메라를 포함하는 구성일 수 있다. 특히 카메라는 청소 로봇의 상방 및 전방 에 모두 설치되어 있을 수도 있고, 상방 및 전방 중 적어도 하나에만 설치되어 있을 수도 있다. 메모리는 카메라가 촬영한 영상 및 촬영 당시의 청소 로봇의 이동상태정보 및 촬영 방향정보를 저장할 수 있다. 또한 메모리는 청소 로봇이 태스크를 수행하기 위한 장소에 대한 네비게이션 맵 정 보를 저장할 수 있다. 이는 일 실시 예에 불과할 뿐, 메모리는 청소 로봇을 동작시키기 위해 필요한 각종 프로그램 등이 저장될 수 있다. 메모리는 청소 로봇에서 구동되는 다수의 응용 프로그램(application program 또는 애플리케이션 (application)), 청소 로봇의 동작을 위한 데이터들, 명령어들을 저장할 수 있다. 이러한 응용 프로그램 중 적어도 일부는, 무선 통신을 통해 외부 서버로부터 다운로드 될 수 있다. 또한, 이러한 응용 프로그램 중 적 어도 일부는, 청소 로봇의 기본적인 기능을 위하여 출고 당시부터 청소 로봇 상에 존재할 수 있다. 응용 프로그램은, 메모리에 저장되고, 프로세서에 의하여 청소 로봇의 동작(또는 기능)을 수행 하도록 구동될 수 있다. 메모리는 비휘발성 메모리, 휘발성 메모리, 플래시메모리(flash-memory), 하드디스크 드라이브(HDD) 또는 솔리드 스테이트 드라이브(SSD) 등으로 구현될 수 있다. 메모리는 프로세서에 의해 액세스 되며, 프 로세서에 의한 데이터의 독취/기록/수정/삭제/갱신 등이 수행될 수 있다. 본 개시에서 메모리라는 용어는 메모리, 프로세서 내 ROM(미도시), RAM(미도시) 또는 청소 로봇 에 장착되는 메모리 카드(미도 시)(예를 들어, micro SD 카드, 메모리 스틱)를 포함할 수 있다. 프로세서는 청소 로봇의 전반적인 구성을 제어할 수 있다. 예를 들어, 프로세서는 청소 로봇 주변의 영상을 촬영하도록 카메라를 제어할 수 있다. 프로세서는 촬영된 영상에서 바닥면을 특 징적으로 분할한 제1 영상을 생성할 수 있다. 이때, 바닥면을 분할하기 위해서 프로세서는 영상 분할 기법 을 사용할 수 있다. 이에 대해서는 추후에 자세히 설명하기로 한다. 프로세서는 장애물과 같은 객체를 인식하도록 학습된 제1 인공지능모델을 통해 영상에 포함된 장애물을 인 식할 수 있다. 즉, 프로세서는 제1 인공지능모델에 장애물을 포함하는 영상을 입력하고, 장애물 유형에 대 한 정보를 포함하는 출력 결과를 획득할 수 있다. 프로세서는 장애물의 유형에 따라 상이한 접근금지 영역 의 크기를 결정할 수 있다. 이때 접근금지 영역은 장애물을 포함하는 영역으로, 청소 로봇이 청소 태스크 를 수행 시 접근하지 않을 영역을 의미한다. 한편, 제1 인공지능 모델은 학습 되어 메모리에 저장된 것일 수도 있고, 외부 서버에 저장된 것일 수도 있다. 이에 대하여 추후에 자세히 설명하도록 한다. 이하에서는 청소 로봇에 제1 인공지능모델을 저장하고 있는 실시 예를 가정하고 설명하기로 한다. 프로세서는 인식된 장애물에 대한 접근금지 영역과 제1 영상을 오버래핑한 제2 영상을 생성할 수 있다. 프 로세서는 제2 영상에 포함된 정보를 바탕으로, 청소 로봇의 주변의 구조물과 장애물의 위치를 인식할 수 있고, 청소 로봇이 이동할 방향 및 속도를 결정할 수 있다. 프로세서는 결정된 이동방향 및 속도 에 따라 청소 로봇이 이동되도록 구동부를 제어할 수 있다. 한편, 프로세서는 CPU, 램(RAM), 롬(ROM), 시스템 버스를 포함할 수 있다. 여기서, 롬은 시스템 부팅을 위 한 명령어 세트가 저장되는 구성이고, CPU는 롬에 저장된 명령어에 따라 청소 로봇의 메모리에 저장된 운 영체제를 램에 복사하고, O/S를 실행시켜 시스템을 부팅시킨다. 부팅이 완료되면, CPU는 메모리에 저장된 각종 애플리케이션을 램에 복사하고, 실행시켜 각종 동작을 수행할 수 있다. 이상에서는 프로세서가 하나 의 CPU만을 포함하는 것으로 설명하였지만, 구현 시에는 복수의 CPU(또는 DSP, SoC 등)으로 구현될 수 있다. 본 발명의 일 실시 예에 따라, 프로세서는 디지털 신호를 처리하는 디지털 시그널 프로세서(digital signal processor(DSP), 마이크로 프로세서(microprocessor), TCON(Time controller)으로 구현될 수 있다. 다 만, 이에 한정되는 것은 아니며, 중앙처리장치(central processing unit(CPU)), MCU(Micro Controller Unit), MPU(micro processing unit), 컨트롤러(controller), 어플리케이션 프로세서(application processor(AP)), 또 는 커뮤니케이션 프로세서(communication processor(CP)), ARM 프로세서 중 하나 또는 그 이상을 포함하거나,해당 용어로 정의될 수 있다. 또한, 프로세서는 프로세싱 알고리즘이 내장된 SoC(System on Chip), LSI(large scale integration)로 구현될 수도 있고, FPGA(Field Programmable gate array) 형태로 구현될 수도 있다. 도 2는 본 개시의 일 실시 예에 따른, 청소 로봇의 구성을 설명하기 위한 상세한 블록도이다. 도 2를 참조하면, 청소 로봇은 구동부, 카메라, 메모리, 통신부, 집진부, 센서, 전원부 및 상술한 구성을 제어하기 위한 프로세서를 포함할 수 있다. 구동부, 카메라, 메모리 , 프로세서에 대하여는 이미 설명하였으므로, 중복되는 설명은 생략하도록 한다. 통신부는 외부 장치와 데이터, 제어 명령 등을 송수신할 수 있다. 예를 들어, 통신부는 외부 장치로 부터 청소 로봇이 동작할 공간에 대한 위치 정보를 포함하는 전역 지도 정보를 수신할 수 있다. 또한, 통 신부는 외부 장치로 전역 지도 정보를 갱신하기 위한 정보를 송신할 수 있다. 다른 예로, 통신부는 사용자가 원격제어장치를 이용하여 송신한 청소 로봇을 제어하기 위한 신호를 수신할 수 있다. 이때 원격 제어장치는 리모컨, 모바일 장치 등 다양한 형태로 구현될 수 있다. 또한 통신부는 외부 서버(미도시)와 데이터 등을 송수신할 수 있다. 예를 들어 외부 서버에 인공지능모델 이 저장되어 있는 경우, 통신부는 외부 서버로 카메라를 통해 촬영한 영상을 송신할 수 있고, 외부 서버에 저장된 인공지능 모델을 이용하여 인식된 장애물에 대한 정보를 수신할 수 있다. 이는 일 실시 예에 불 과하며, 통신부는 외부 서버로부터 청소 로봇이 태스크를 수행할 공간에 대한 이동 가능 영역에 대한 정보를 수신할 수 있다. 한편, 통신부는 무선 통신 방식으로 NFC(Near Field Communication), 무선 LAN(Wireless LAN), IR(InfraRed) 통신, Zigbee 통신, WiFi, 블루투스(Bluetooth) 등 다양한 방식을 이용할 수 있다. 집진부는 먼지를 집진하기 위한 구성이다. 구체적으로, 집진부는 공기를 흡입하고, 흡입된 공기 중의 먼지를 집진할 수 있다. 예를 들어, 집진부는 흡입구에서 배출구까지 이어지는 가이드 배관을 통해 공기를 통과시키는 모터와 흡입된 공기 중의 먼지를 거르는 필터 및 걸러진 먼지를 담는 먼지통 등을 포함할 수 있다. 센서는 다양한 종류의 센서를 포함할 수 있다. 구체적으로 센서는 먼지 센서, 냄새 센서, 레이저 센 서, UWB 센서, 이미지 센서, 장애물 센서 등과 같은 주변탐지를 위한 센서 및 자이로 센서, GPS 센서 등과 같은 이동상태를 감지하기 위한 센서 등을 포함할 수 있다. 이때, 주변탐지를 위한 센서와 청소 로봇의 이동상태를 감지하기 위한 센서는 상이한 구성으로 구현될 수도 있고, 하나의 구성으로 구현될 수도 있다. 또한, 센서(17 0)의 각각의 센서는 별도의 구성으로 구현될 수 있다. 센서는 이외에도 다양한 종류의 센서를 더 포함할 수 있으며, 청소 로봇이 수행할 태스크에 따라 도시된 센서 중 일부는 포함하지 않을 수도 있다. 먼지 센서 및 냄새 센서는 청소 로봇 주변 환경의 오염도를 감지하는데 이용될 수 있다. 먼지 센서는 청소 로봇 주변의 먼지 농도를 감지할 수 있다. 예를 들어, 먼지 센서는 집진부에서 흡입되는 먼지의 농도 를 감지할 수 있다. 먼지 센서(는 먼지 입자에 의한 산란광을 이용하여 먼지 농도를 감지할 수 있다. 또한, 냄 새 센서는 특정 냄새를 감지할 수 있다. 예를 들어, 냄새 센서는 애완동물의 배설물과 같은 오염 물질을 감지할 수 있다. 냄새 센서는 냄새 분자와의 접촉으로 인한 전기 저항의 변화를 통하여 특정 냄새를 감지할 수 있다. 레이저 센서는 청소 로봇 주변 환경의 복잡도를 감지할 수 있다. 예를 들어, 레이저 센서는 전 방향으로 레이저 빔을 발광하여 청소 로봇 주변에 물체가 존재하는지 감지할 수 있다. 청소 로봇 주변에 의자 와 같은 가구가 배치된 경우에 복잡도가 높게 평가될 것이고, 청소 로봇 주변에 물체가 없는 경우 복잡도 가 낮게 평가될 것이다. UWB 센서는 전파 송수신 시에 소요되는 시간(Time of Flight)을 이용하여 거리를 측정할 수 있다. UWB 센서는 초광대역(ultra wide band)의 신호 특성을 이용하여 높은 정확도로 거리를 측정할 수 있다. 또한, 복수의 UWB 센서를 청소 로봇에 배치함으로써, UWB 태그가 위치한 곳의 방향을 파악할 수 있다. 예를 들어, UWB 센서 를 이용할 경우에, UWB 태그가 위치한 곳을 추적할 수 있기 때문에, UWB 태그가 설치된 충전 스테이션의 위치를 변경하더라도, 청소 로봇은 충전 스테이션의 정확한 위치를 찾아 복귀할 수 있다. 장애물 센서는 청소 로봇의 이동을 방해하는 장애물을 감지할 수 있다. 예를 들어, 장애물 센서는 청소 로 봇이 통과할 수 없는 벽, 틈새, 기둥, 문턱, 둔덕 등을 식별할 수 있는 비접촉식 감지 센서 및 접촉식 충 돌/범퍼 센서를 포함할 수 있다. 자이로 센서는 회전하는 청소 로봇의 역학운동을 이용한 개념으로 위치 측정과 방향 설정 등에 활용되는 센서이다. 자이로 센서는 가속도를 측정하는 가속도 센서와 달리 각속도를 측정한다. 자이로스코프(Gyroscope) 가 각속도를 측정하는 기구인데 MEMS 기술을 적용한 칩 형태의 자이로센서도 각속도를 측정할 수 있다. 전원부는 청소 로봇의 구동에 필요한 전원을 공급한다. 예를 들어, 전원부는 충방전이 가능한 배터리로 구현될 수 있다. 프로세서는 청소 로봇의 잔여 전원이 기설정된 레벨 이하로 떨어진 경우, 또는 태스크를 완료한 경우에 충전 스테이션으로 이동하도록 구동부를 제어할 수 있다. 전원부의 충 전 방식은 접촉식 및 비접촉식 충전이 모두 가능하다. 도 3은 본 개시의 일 실시 예에 따른, 청소 로봇이 이동 방향을 결정하기 위해 촬영한 영상을 분석하는 방법을 설명하기 위한 흐름도이다. 도 3을 참조하면, 청소 로봇은 주변의 환경을 캡쳐한 영상을 촬영할 수 있다(S310). 이때, 캡쳐한 영상은 청소 로봇이 이동하는 방향과 동일한 방향에 대한 환경을 캡쳐한 것일 수도 있고, 이동하는 방향과 상이한 방향에 대한 환경을 캡쳐한 것일 수도 있다. 청소 로봇은 촬영한 영상에서 바닥면을 분할한 제1 영상(바닥면 분할영상)을 획득할 수 있다(S320). 또한, 청소 로봇은 획득한 영상을 객체를 인식하기 위해 학습된 인공지능모델에 입력하고, 영상에 포함된 장애물 을 인식한 결과 장애물의 유형에 대한 정보를 출력할 수 있다(S330). 또한, 청소 로봇은 장애물의 유형에 대한 정보를 바탕으로 장애물을 포함하는 접근금지 영역의 크기를 결정할 수 있다. 제1 인공지능 모델은 CNN(Convolutional Neural Network)일 수 있으나, 이는 일 실시예에 불과할 뿐, GAN (Generative Adversarial Network), DNN(Deep Neural Network), RNN(Recurrent Neural Network), BRDNN(Bidirectional Recurrent Deep Neural Network)등과 같은 다양한 신경망을 이용할 수 있다. 청소 로봇은 제1 영상과 인식된 장애물을 포함하는 영역(접근금지 영역)에 대한 정보를 바탕으로 제2 영상 을 획득할 수 있다(S340). 즉, 제2 영상은 청소 로봇이 청소 태스크를 수행할 수 있는 영역만을 나타내는 영상일 수 있다. 청소 로봇은 제2 영상을 바탕으로 이동속도 및 이동방향을 결정하고, 청소 태스크를 수행 하기 위해 이동할 수 있다(S350). 도 4A 및 4B는 본 개시의 일 실시 예에 따른, 청소 로봇이 촬영한 영상에서 바닥 면 분할영상을 획득하는 것을 설명하기 위한 도면이다. 도 4A는 카메라를 통해 청소 로봇의 전방의 영상을 촬영한 것이다. 이후 청소 로봇은 획득한 영 상을 영상 분할(scene segmentation)을 통해 도 4B와 같은 바닥 면 분할영상을 획득할 수 있다. 이때, 영상 분할(scene segmentation)이란, 영상의 구성 요소인 픽셀을 분류하여 영상 내에서 객체의 경계선을 찾아 원하는 객체를 추출하는 기술을 말한다. 예를 들어, 영상 중에 객체의 형태 및 크기를 알려면 객체와 배경 의 2개의 영역으로 분할하지 않으면 안 된다. 객체는 영상 해석과 표현에 있어서 중요한 역할을 한다. 특히, MPEG(Moving Picture Experts Group)-4 비주얼 표준은 영상을 객체 단위로 부호화한다. 이러한 기술을 객체기 반 부호화(object-based coding)라고 한다. 필요에 따라 객체를 조합, 제거, 변형하는 등의 객체에 대한 다양한 편집기술을 이용하여 영상을 재생산하거나 효율적으로 압축할 수 있다. 그러나 각각의 획득한 영상의 명확한 경계가 없으며, 영상마다 특징이 다르기 때문에, 청소 로봇은 획득한 영상에서 밝기 값(luminance), 에지 정보(edge), 혹은 기하학적 정보와 같은 영상의 특성에 기반하여 비슷 한 값을 가지는 영역(homogeneous region)을 구분할 수 있다. 청소 로봇은 구분해낸 영역들 중 비슷한 특 성을 가지는 영역들을 서로 오버래핑하여 바닥 면 영역을 획득할 수 있고, 이외의 영역을 마스킹(maskin g)할 수 있다. 청소 로봇은 영상 분할 알고리즘들로서, 경계 값 처리법, 영역확장법, 분할 통합법, 워터쉐드법 (watershed), 및 에지에 기반한 방법을 사용하여 도 4B와 같은 바닥 면 분할영상을 획득할 수 있다. 또는 청소 로봇은 획득한 영상을 프레임 단위로 구분하고, 분리하고자 하는 바닥 면을 추정하여 바닥 면에 해당하는 윈도우들의 위치를 결정할 수 있다. 또한, 청소 로봇은 결정된 윈도우들의 위치에 존재하는 윈도 우들의 크기 및 간격 중 적어도 하나를 조절하고, 크기 및 간격 중 적어도 하나가 조절된 윈도우들을 이용하여 입력된 영상에서 도 4B와 같이 바닥 면만을 분리할 수 있다. 한편, 본 개시의 일 실시 예에 따르면, 청소 로봇은 바닥 면을 인식하도록 학습된 제2 인공지능 모델을 이 용하여 바닥 면 분할영상을 획득할 수도 있다. 이때, 제2 인공지능 모델은 청소 로봇 내에 저장되어 있을 수도 있고, 외부 서버에 저장되어 있을 수도 있다. 제2 인공지능 모델이 외부 서버에 저장되어 있는 경우에는청소 로봇은 통신부를 통해 촬영 영상에 대한 정보를 외부 서버로 송신할 수 있고, 제2 인공지능모델 에서 출력된 바닥 면 분할영상에 대한 정보를 외부 서버로부터 수신할 수 있다. 제2 인공지능 모델은 CNN(Convolutional Neural Network)일 수 있으나, 이는 일 실시예에 불과할 뿐, GAN (Generative Adversarial Network), DNN(Deep Neural Network), RNN(Recurrent Neural Network), BRDNN(Bidirectional Recurrent Deep Neural Network)등과 같은 다양한 신경망을 이용할 수 있다. 상술한 바와 같이, 청소 로봇은 영상분할 기법을 뿐만 아니라, 제2 인공지능 모델을 통해 입력된 영상에서 도 4B와 같이 바닥 면만을 분리할 수 있다. 도 5A 내지 5C는 본 개시의 일 실시 예에 따른, 청소 로봇이 촬영한 영상에서 장애물에 대한 정보를 획득하는 것을 설명하기 위한 도면이다. 도 5A는 카메라를 통해 청소 로봇의 이동 방향과 동일한 방향의 영상을 촬영한 것이다. 도 5A를 참조하면 촬영된 영상은 벽면과 같은 구조물이 아닌 장애물을 포함하고 있다. 청소 로봇은 제1 인공 지능 모델에 획득한 영상을 입력하여 장애물을 인식할 수 있고, 장애물의 유형에 대한 정보를 출력할 수 있다. 청소 로봇은 인식된 장애물에 대한 정보를 바탕으로, 장애물을 포함하는 접근금지 영역을 판단할 수 있다. 도 5B를 참조하면, 입력된 영상에 꽃병과 같은 장애물이 포함되어 있는 경우, 제1 인공지능 모델은 입력된 영상에 장애물이 포함되어 있다는 정보, 포함된 장애물의 유형이 꽃병이라는 정보를 출력할 수 있다. 또 한, 청소 로봇은 인식된 장애물의 유형에 따라 청소 태스크 수행 시 접근금지 영역의 범위를 판단할 수 있다. 한편, 청소 로봇은 장애물의 종류에 따라서 접근하지 않을 영역의 범위를 다르게 출력할 수 있다. 상 술한 예시와 같이 인식된 장애물이 꽃병인 경우, 청소 로봇은 꽃병의 폭의 2배에 해당하는 영역을 접근금 지 영역으로 판단할 수 있다. 반면, 인식된 장애물이 인형인 경우, 청소 로봇은 인형의 폭의 1.5배에 해당 하는 영역을 접근하지 않을 영역으로 판단할 수 있다. 인형의 경우 청소 로봇이 접근하여 청소하더라도 파 손의 위험이 상대적으로 작기 때문이다. 장애물의 유형별 접근금지 영역을 설정하기 위한 데이터는 메모리(13 0)에 저장되어 있을 수도 있고, 통신부를 통해 외부 서버로부터 수신할 수도 있다. 도 5C를 참조하면, 청소 로봇은 인식된 장애물을 포함하는 접근금지 영역을 영상 분할을 통해 획득한 바닥 면 분할영상과 오버래핑하여 저장할 수 있다. 청소 로봇은 장애물을 포함하는 접근금지 영역을 마스 킹 처리할 수 있다. 그 결과, 청소 로봇은 마스킹 처리된 영역이 바닥면 분할영상에 오버래핑된 영상 을 획득할 수 있다. 청소 로봇은 오버래핑 영상을 바탕으로 태스크를 수행하기 위한 이동 경로 를 설정할 수 있다. 이에 대하여는 추후에 자세히 설명하기로 한다. 도 6A 및 6B는 본 개시의 일 실시 예에 따른, 청소 로봇이 획득한 영상에서 뎁스 맵 영상을 획득하는 것을 설명 하기 위한 도면이다. 도 4 내지 5에서 설명한 바와 같이 청소 로봇은 이동 경로를 설정하기 위해 획득한 영상에서 바닥면 분할 영상을 획득할 수도 있으나, 도 6A와 같이 영상에 대한 뎁스(depth) 맵을 획득할 수도 있다. 청소 로봇은 획득한 영상에 대한 뎁스 맵 영상을 통해서, 벽면까지의 거리를 추정할 수 있다. 본 개시의 일 실시 예에 의하면, 청소 로봇은 상술한 바와 같이 인식된 장애물을 포함하는 접근금지 영역 과 뎁스 맵을 오버래핑할 수 있다. 또 다른 실시 예에 의하면, 청소 로봇은 획득한 뎁스 맵을 바닥면 을 분할한 부분 및 접근 금지 영역과 함께 오버래핑하여 저장할 수도 있다. 상술한 실시 예를 통해, 청소 로봇은 주변 환경의 뎁스 정보까지 포함한 정보를 바탕으로, 더욱 정확한 구 조물 정보를 획득할 수 있고, 더 안전하게 청소 로봇의 경로를 계획할 수 있다. 도 7A 및 7B는 본 개시의 일 실시 예에 따른, 청소 로봇이 획득한 영상을 통해 벽면까지의 거리를 추정하는 방 법을 설명하기 위한 도면이다. 도 7A를 참조하면, 청소 로봇은 도 4 및 5에서와 같이 청소 로봇의 주변의 영상을 획득할 수 있다. 도 7A에서는 청소 로봇이 메모리에 기저장되어 있지 않은 태스크 수행장소에 위치한 후, 태스크 수행 을 위해 영상을 획득하는 것을 도시한다. 청소 로봇은 카메라의 화각에 대응하는 범위에 해당하 는 제1 영상을 획득할 수 있다. 이때, 획득하는 영상은 청소 로봇의 주행 방향을 기준으로, 청소 로봇의 전방에 대응하는 영상일 수 있다. 청소 로봇은 영상을 획득한 후, 태스크를 수행하기 위해 주행 방향으로 이동할 수 있다. 청소 로봇 은 멈춰있는 상태에서 제1 영상을 획득할 수도 있고, 이동 중에 제1 영상을 획득할 수도 있다. 이때, 청소 로봇은 센서를 통해 제1 위치에서 제2 위치로 이동하는 청소 로봇의 이동상태에 대한 정보를 획 득할 수 있다. 구체적으로, 이동상태에 대한 정보는 청소 로봇의 이동 방향 및 이동 속도에 대한 정보를 포함할 수 있다. 청소 로봇은 제1 위치에서 제1 영상을 획득하는 경우, 제1 영상을 획득하는 때의 청소 로 봇의 이동상태에 대한 정보를 함께 메모리에 저장할 수 있다. 한편, 청소 로봇은 주행 방향으로 이동하는 동안 청소 로봇의 이동상태에 대한 정보를 획득할 수 있다. 청소 로봇은 제1 영상을 획득한 후 기설정된 시간 간격으로 도 7B와 같이 제2 위치에서 제2 영상 을 획득할 수 있다. 마찬가지로 청소 로봇은 제2 영상을 획득하는 때의 청소 로봇의 이동상태에 대한 정보를 획득할 수 있고, 제2 영상과 함께 메모리에 저장할 수 있다. 청소 로봇은 제1 영상, 제2 영상 및 청소 로봇의 이동상태에 대한 정보를 바탕으로 청소 로봇과 벽면 까지의 거리를 추정할 수 있다. 예를 들어, 청소 로봇은 제1 영상, 제2 영상에 대한 바닥면 분할영상(각각 제1 바닥 분할영상, 제2 바닥 분할영상)을 획득할 수 있다. 청소 로봇은 제1 영상을 획득한 후 제2 영상을 획득할 때까지의 이동 속도에 대한 정보를 바탕으로 이동 거리를 계산할 수 있다. 청소 로봇은 제1 바닥 분할영상(미도시)와 제2 바닥 분할영상(미도시)의 차이 및 청소 로봇의 이동 거리에 대한 정보를 바탕으로 벽면까지의 거리를 계산할 수 있다. 청소 로봇은 획득한 영상에 포함된 벽면까지의 거리를 계산한 후, 청소 로봇의 이동 방향 및 이동 속 도를 설정할 수 있다. 이에 대해서는 도 8A 및 8B를 통해 설명하도록 한다. 도 8A 및 8B는 본 개시의 일 실시 에에 따른, 청소 로봇이 태스크를 수행하기 위해 효율적으로 이동하는 것을 설명하기 위한 도면이다. 종래의 근거리 센서를 이용한 청소 로봇은 넓은 공간으로 이동하기 위해서는 청소 로봇이 제1 벽면(800-1)에 접근한 후 이동 방향을 변경하고, 제2 벽면(800-2)에 접근하고 다시 이동 방향을 변경하는 제1 경로를 통해 이동한다. 반면 본 개시의 청소 로봇은 도 7A 및 7B에서 설명한 바와 같이, 청소 로봇은 벽면까지의 거리를 계 산하고, 청소 로봇의 이동 속도 및 이동 방향을 설정할 수 있다. 청소 로봇은 제1 벽면(800-1)과 제2 벽면 (800-2)까지의 거리를 바탕으로, 영상 내의 넓은 공간에 한 번에 도달하는 제2 경로에 대응하는 이동 방향을 판단할 수 있다. 구체적으로, 청소 로봇은 1) 청소 로봇과 제1 벽면(800-1)까지의 거리 및 제2 벽면(800-2)까지의 거 리, 2) 청소 로봇과 제1 벽면(800-1) 및 제2 벽면(800-2) 간 이루는 각도를 이용하여 이동 방향을 판단할 수 있다. 더욱 구체적으로, 청소 로봇은 청소 로봇과 제1 벽면(800-1)까지의 거리 및 청소 로봇(10 0)과 제2 벽면(800-2)까지의 거리가 동일하게 되는 경로를 판단할 수 있고, 해당 경로를 향한 방향을 청소 로봇 의 이동 방향으로 판단할 수 있다. 또한, 청소 로봇은 영상 내의 넓은 공간에 도달하기 위해 판단된 방향으로 이동하는 청소 로봇 의 이동 속도를 결정할 수 있다. 구체적으로, 청소 로봇은 영상에 포함된 제3 벽면(800-3)까지의 거리에 대한 정보를 획득할 수 있다. 청소 로봇은 이동 중에 기설정된 시간 간격으로 전방에 대한 영상을 획득할 수 있고, 영상이 획득될 때마다 청 소 로봇의 이동상태정보를 바탕으로 각 벽면까지의 거리를 계산할 수 있다. 더욱 구체적으로, 청소 로봇 은 제2 경로를 통해 이동하던 중 청소 로봇과 제1, 2, 3 벽면까지의 거리 중 적어도 하나의 거리가 기설정된 제1 거리 내에 도달하기 전까지 제1 속도로 이동할 수 있다. 청소 로봇과 제1, 2, 3 벽면까지의 거리 중 적어도 하나의 거리가 기설정된 제1 거리보다 작아지는 경우, 청소 로봇은 제1 속도보다 작은 제2 속도로 이동할 수 있다. 또한, 청소 로봇은 제1, 2, 3 벽면까지의 거리 중 적어도 하나의 거리가 기설정된 제2 거리보다 작아지는 경우, 이동을 멈출 수 있다. 본 예시에서는 청소 로봇은 속도를 변경하기 위한 제1, 제2 기설정된 거리를 설명하였으나, 이는 일 실시 예에 불과하며, 2개 이상의 기설정된 거리를 설정하여 연속적인 속도 제어를 제공할 수 있다. 청소 로봇이 획득한 영상에 도 8B와 같이 장애물이 포함되어 있을 경우, 청소 로봇은 제1 인공 지능 모델을 통해 장애물을 인식할 수 있고, 장애물의 유형에 따라 청소 로봇이 접근하지 않을 영역 을 결정할 수 있다. 청소 로봇은 접근하지 않을 영역을 벽면 등과 같은 구조물로 인식할 수 있 다. 청소 로봇은 도 8A에서와 마찬가지로 제1, 2, 3 벽면까지의 거리, 상기 영역까지의 거리 중 적어 도 하나의 거리를 바탕으로 넓은 공간까지 효율적으로 이동하기 위한 경로에 대응하는 이동 방향을 판단할 수 있다. 구체적으로, 청소 로봇은 1) 청소 로봇과 제1 벽면(800-1)까지의 거리 및 접근금지 영역까지의 거리, 2) 청소 로봇과 제2 벽면(800-2) 및 제3 벽면(800-3)까지의 거리, 3) 청소 로봇과 제1 벽면 (800-1) 및 접근금지 영역 간 이루는 각도 등을 이용하여 이동 방향을 판단할 수 있다. 더욱 구체적으로, 청소 로봇은 청소 로봇과 제1 벽면(800-1)까지의 거리 및 청소 로봇과 접근금지 영역까지 의 거리가 동일하게 되는 경로를 판단할 수 있고, 해당 경로를 향한 방향을 청소 로봇의 이동 방향으로 판 단할 수 있다. 또한, 청소 로봇은 접근금지 영역과 청소 로봇의 이동 방향을 이루는 각도가 90 도를 초과하는 것을 감지하면, 청소 로봇과 제2 벽면(800-2)까지의 거리 및 제3 벽면(800-3)까지의 거리가 동일하게 되는 경로를 판단할 수 있고, 해당 경로를 향한 방향을 청소 로봇의 이동 방향으로 판단할 수 있 다. 또한, 청소 로봇은 제1, 2, 3 벽면까지의 거리 및 상기 영역까지의 거리 중 적어도 하나의 거리를 바 탕으로 이동 속도를 설정할 수 있다. 구체적으로, 청소 로봇은 상기 경로를 통해 이동하던 중 청소 로봇과 제1, 2, 3 벽면까지의 거리 및 상기 영역까지의 거리 중 적어도 하나의 거리가 기설정된 제1 거리 내에 도달하기 전까지 제1 속도로 이동할 수 있다. 청소 로봇과 제1, 2, 3 벽면까지의 거리 및 상기 영역까지의 거리 중 적어도 하나의 거리가 기설정된 제1 거리보다 작아지는 경우, 청소 로봇은 제1 속도보다 작은 제2 속도로 이동할 수 있다. 또한, 청소 로봇은 제1, 2, 3 벽면까지의 거리 및 상기 영역 까지의 거리 중 적어도 하나의 거리가 기설정된 제2 거리보다 작아지는 경우, 이동을 멈출 수 있다. 상술한 실시 예에 의하면, 본 개시의 청소 로봇은 종래의 청소 로봇과 달리 구조물과 장애물의 위치 를 고려하여 넓은 공간까지 가장 효율적인 이동 방향 및 속도를 통해 이동해 청소 서비스를 제공할 수 있다. 이 를 통해 청소 로봇은 넓은 공간으로 먼저 이동 후 부수적인 공간을 청소함으로써, 마치 사람이 청소하는 것과 같은 효율적인 청소 서비스를 제공하는 효과가 있다. 도 9A 및 9B는 청소 로봇이 태스크를 수행할 공간에 대한 네비게이션 맵을 생성하는 것을 도시한 도면이다. 도 9A를 참조하면, 청소 로봇은 카메라의 화각에 대응하는 영상을 촬영할 수 있다. 청소 로봇 은 영상을 촬영하던 때의 이동상태정보 및 촬영방향에 대한 정보를 획득할 수 있다. 또한, 청소 로봇(10 0)은 청소 태스크를 수행하기 위해 이동하며 획득한 복수의 영상을 바탕으로 각각의 바닥면 분할영상을 획득할 수 있고, 각각의 영상의 촬영방향에 대한 정보 및 이동상태정보를 바닥면 분할 정보와 함께 메모리에 저장할 수 있다. 이는 일 실시 예에 불과하고, 청소 로봇은 인공지능모델을 통해 인식된 장애물을 포함하는 영역을 바닥면 분할영상과 오버래핑할 수 있고, 상기 오버래핑된 영상을 각각의 영상의 촬영방향에 대한 정보 및 이동 상태정보와 함께 저장할 수 있다. 이때 장애물을 포함하는 영역은 청소 로봇이 구조물로 인식하여 접근하 지 않을 영역이며, 인공지능모델을 통해 인식한 장애물의 유형에 따라 영역의 크기가 달라질 수 있다. 청소 로봇은 촬영된 영상의 바닥면 분할영상, 장애물을 포함하는 영역, 각 영상의 촬영방향에 대한 정보, 영상 촬영시 청소 로봇의 이동상태에 대한 정보를 바탕으로 청소 태스크를 수행하기 위한 네비게이션 맵을 생성할 수 있다. 예를 들어, 청소 로봇은 제1 위치에서 제1 바닥면 분할영상을 획득할 수 있고, 촬영방향을 유지한 채 제2 위치에서 제2 바닥면 분할영상을 획득할 수 있다. 이때 제1, 제2 바닥면 분할영상은 제1 벽면을 포함할 수 있다. 청소 로봇의 제1 위치에서부터 제2 위치까지의 이동하는 청소 로봇의 이동상태에 대한 정보를 바탕으로 제1, 제2 위치로부터 제1 벽면까지의 거리를 계산할 수 있다. 또한, 청소 로봇은 제2 위치에서 촬영방향을 다르게 하여 영상을 촬영한 후, 제3 바닥면 분할영상을 획득할 수 있고, 촬영방향을 유지한 채 제3 위치에서 제4 바닥면 분할영상을 획득할 수 있다. 이때 제3, 제4 바닥면 분할영상은 제1 벽면 및 제2 벽면을 포 함할 수 있다. 청소 로봇은 제2 위치에서부터 제3 위치까지 이동하는 청소 로봇의 이동상태에 대한 정보를 바탕으로 제2, 제3 위치로부터 제2 벽면까지의 거리를 계산할 수 있다. 청소 로봇은 제1 위치에서 제1 벽면까지의 거리, 제2 위치에서 제1, 2 벽면까지의 거리, 제3 위치에서 제2 벽면까지의 거리를 바탕으로 제 1 벽면과 제2 벽면이 이루는 형태에 대해서 판단할 수 있다. 상술한 방법에 의해, 청소 로봇은 청소를 수행하기 위한 공간의 벽면 등과 같은 구조물과 장애물의 형태에 대하여 판단할 수 있고, 도 9B와 같이 청소 태스크를 수행하기 위한 전체 네비게이션 맵을 생성할 수 있다. 청소 로봇은 사용자에 의해 납치되는 등 청소 로봇의 위치가 새롭게 변경되는 경우, 네비게이션 맵을 통해 청소 로봇의 위치를 판단할 수 있다. 구체적으로, 청소 로봇은 새롭게 변경된 위치에서 주변의 영상을 촬영할 수 있고, 촬영된 영상에 대한 바닥면 분할영상 및 촬영 방향에 대한 정보를 획득할 수 있다. 청 소 로봇은 네비게이션 맵을 생성할 때 이용한 복수의 바닥면 분할영상 및 촬영방향정보와 새로운 위치에서 획득한 바닥면 분할영상 및 촬영방향정보를 비교할 수 있고, 비교결과 청소 로봇의 네비게이션 맵 상 위치 를 판단할 수 있다. 도 10은 청소 로봇이 네비게이션 맵을 이용하여, 청소 태스크 수행을 위한 이동계획을 수립하는 것을 도시한 도 면이다. 네비게이션 맵 상의 청소 로봇의 위치가 판단되면, 청소 로봇은 청소 태스크를 수행하기 위한 이동계 획을 수립할 수 있다. 도 10을 참조하면, 청소 로봇은 구조물의 넓은 공간(1020, 1030, 1040)으로 이동하 며 효율적으로 태스크를 수행할 수 있다. 이때, 청소 로봇은 네비게이션 맵을 생성할 때 포함된 장애물을 포함하는 접근하지 않을 영역에 대한 정보를 바탕으로, 이동 계획을 수립할 수 있다. 다만, 청소 로봇은 수립한 이동 계획에 따라 이동하는 동안에도, 카메라를 통해 주변의 영상을 촬영 할 수 있다. 청소 로봇은 영상에 포함된 장애물을 인공지능모델을 통해 인식할 수 있고, 실시간으로 장애 물의 유형을 판단하여 접근하지 않을 영역을 판단할 수 있다. 청소 로봇은 장애물이 포함된 판단된 영역을 바탕으로 실시간으로 이동 계획을 변경할 수 있다. 또한, 청소 로봇은 판단된 장애물을 포함하는 접근하지 않을 영역에 대한 정보를 네비게이션 맵에 업데이트할 수 있다. 도 11A 및 도 11B는, 다양한 실시예에 따른 학습부 및 인식부를 나타내는 블록도이다. 도 11A를 참조하면, 프로세서는 학습부 및 인식부 중 적어도 하나를 포함할 수 있다. 도 11A 의 프로세서는 도 2의 청소 로봇의 프로세서 또는 데이터 학습 서버(미도시)의 프로세서에 대 응될 수 있다. 학습부는 소정의 상황 판단을 위한 기준을 갖는 인식 모델을 생성 또는 학습시킬 수 있다. 학습부 는 수집된 학습 데이터를 이용하여 판단 기준을 갖는 인식 모델을 생성할 수 있다. 일 예로, 학습부는 객체가 포함된 이미지를 학습 데이터로써 이용하여 이미지에 포함된 객체가 어떤 것인 지 판단하는 기준을 갖는 객체 인식 모델을 생성, 학습 또는 갱신시킬 수 있다. 또 다른 예로, 학습부는 객체가 포함된 화면에 포함된 주변 정보를 학습 데이터로써 이용하여 이미지에 포함된 객체 주변에 다양한 추가 정보를 판단하는 기준을 갖는 주변 정보 인식 모델을 생성, 학습 또는 갱신시 킬 수 있다. 또 다른 예로, 학습부는 카메라에 의해 촬영된 이미지를 학습 데이터로써 이용하여 이미지에 포함된 장애 물을 판단하는 기준을 갖는 장애물 인식 모델을 생성, 학습 또는 갱신시킬 수 있다. 인식부는 소정의 데이터를 학습된 인식 모델의 입력 데이터로 사용하여, 소정의 데이터에 포함된 인식 대 상을 추정할 수 있다. 일 예로, 인식부는 객체가 포함된 객체 영역(또는, 이미지)을 학습된 인식 모델의 입력 데이터로 사용하 여 객체 영역에 포함된 객체에 대한 객체 정보를 획득(또는, 추정, 추론)할 수 있다. 다른 예로, 인식부는 객체 정보를 학습된 인식 모델에 적용하여 검색 결과를 제공할 검색 카테고리를 추 정(또는, 결정, 추론)할 수 있다. 이때, 검색 결과는 우선 순위에 따라 복수 개가 획득될 수도 있다. 학습부의 적어도 일부 및 인식부의 적어도 일부는, 소프트웨어 모듈로 구현되거나 적어도 하나의 하드웨어 칩 형태로 제작되어 전자 장치에 탑재될 수 있다. 예를 들어, 학습부 및 인식부 중 적어 도 하나는 인공 지능(AI; artificial intelligence)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또는 기존의 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제 작되어 전술한 청소 로봇에 탑재될 수도 있다. 이때, 인공 지능을 위한 전용 하드웨어 칩은 확률 연산에 특화된 전용 프로세서로서, 기존의 범용 프로세서보다 병렬처리 성능이 높아 기계 학습과 같은 인공 지능 분야 의 연산 작업을 빠르게 처리할 수 있다. 학습부 및 인식부가 소프트웨어 모듈(또는, 인스트럭션(instruction) 포함하는 프로그램 모듈)로 구현되는 경우, 소프트웨어 모듈은 컴퓨터로 읽을 수 있는 판독 가능 한 비일시적 판독 가능 기록매체(non-transitory computer readable media)에 저장될 수 있다. 이 경우, 소프 트웨어 모듈은 OS(Operating System)에 의해 제공되거나, 소정의 애플리케이션에 의해 제공될 수 있다. 또는, 소프트웨어 모듈 중 일부는 OS(Operating System)에 의해 제공되고, 나머지 일부는 소정의 애플리케이션에 의해 제공될 수 있다. 이 경우, 학습부 및 인식부는 하나의 전자 장치에 탑재될 수도 있으며, 또는 별개의 전자 장치들 에 각각 탑재될 수도 있다. 예를 들어, 학습부 및 인식부 중 하나는 청소 로봇에 포함되고, 나머지 하나는 외부의 서버에 포함될 수 있다. 또한, 학습부 및 인식부는 유선 또는 무선으로 통하 여, 학습부가 구축한 모델 정보를 인식부로 제공할 수도 있고, 인식부로 입력된 데이터가 추 가 학습 데이터로서 학습부로 제공될 수도 있다. 도 11B는, 다양한 실시예에 따른 학습부 및 인식부의 블록도이다. 도 11B의 (a)를 참조하면, 일부 실시예에 따른 학습부는 학습 데이터 획득부(1110-1) 및 모델 학습부 (1110-4)를 포함할 수 있다. 또한, 학습부는 학습 데이터 전처리부(1110-2), 학습 데이터 선택부(1110-3) 및 모델 평가부(1110-5) 중 적어도 하나를 선택적으로 더 포함할 수 있다. 학습 데이터 획득부(1110-1)는 인식 대상을 추론하기 위한 인식 모델에 필요한 학습 데이터를 획득할 수 있다. 본 문서의 실시예로, 학습 데이터 획득부(1110-1)는 객체를 포함하는 전체 이미지, 객체 영역에 대응하는 이미 지, 객체 정보를 학습 데이터로서 획득할 수 있다. 학습 데이터는 학습부 또는 학습부의 제조사가 수집 또는 테스트한 데이터가 될 수도 있다. 모델 학습부(1110-4)는 학습 데이터를 이용하여, 인식 모델이 소정의 인식 대상을 어떻게 판단할지에 관한 판단 기준을 갖도록 학습시킬 수 있다. 예로, 모델 학습부(1110-4)는 학습 데이터 중 적어도 일부를 판단 기준으로 이용하는 지도 학습(supervised learning)을 통하여, 인식 모델을 학습시킬 수 있다. 또는, 모델 학습부(1110- 4)는, 예를 들어, 별다른 지도 없이 학습 데이터를 이용하여 스스로 학습함으로써, 상황의 판단을 위한 판단 기 준을 발견하는 비지도 학습(unsupervised learning)을 통하여, 인식 모델을 학습시킬 수 있다. 또한, 모델 학습 부(1110-4)는, 예를 들어, 학습에 따른 상황 판단의 결과가 올바른 지에 대한 피드백을 이용하는 강화 학습 (reinforcement learning)을 통하여, 인식 모델을 학습시킬 수 있다. 또한, 모델 학습부(1110-4)는, 예를 들어, 오류 역 전파법(error back-propagation) 또는 경사 하강법(gradient descent)을 포함하는 학습 알고리 즘 등을 이용하여 인식 모델을 학습시킬 수 있다 또한, 모델 학습부(1110-4)는 입력 데이터를 이용하여 인식 대상을 추정하기 위하여 어떤 학습 데이터를 이용해 야 하는지에 대한 선별 기준을 학습할 수도 있다. 모델 학습부(1110-4)는 미리 구축된 인식 모델이 복수 개가 존재하는 경우, 입력된 학습 데이터와 기본 학습 데 이터의 관련성이 큰 인식 모델을 학습할 인식 모델로 결정할 수 있다. 이 경우, 기본 학습 데이터는 데이터의 타입별로 기 분류되어 있을 수 있으며, 인식 모델은 데이터의 타입별로 미리 구축되어 있을 수 있다. 예를 들어, 기본 학습 데이터는 학습 데이터가 생성된 지역, 학습 데이터가 생성된 시간, 학습 데이터의 크기, 학습 데이터의 장르, 학습 데이터의 생성자, 학습 데이터 내의 오브젝트의 종류 등과 같은 다양한 기준으로 기분류되 어 있을 수 있다. 인식 모델이 학습 되면, 모델 학습부(1110-4)는 학습된 인식 모델을 저장할 수 있다. 이 경우, 모델 학습부 (1110-4)는 학습된 인식 모델을 청소 로봇의 메모리에 저장할 수 있다. 또는, 모델 학습부(1110-4)는 학습된 인식 모델을 청소 로봇과 유선 또는 무선 네트워크로 연결되는 서버의 메모리에 저장할 수도 있다. 학습부는 인식 모델의 분석 결과를 향상시키거나, 인식 모델의 생성에 필요한 자원 또는 시간을 절약하기 위하여, 학습 데이터 전처리부(1110-2) 및 학습 데이터 선택부(1110-3)를 더 포함할 수도 있다. 학습 데이터 전처리부(1110-2)는 상황 판단을 위한 학습에 획득된 데이터가 이용될 수 있도록, 획득된 데이터를 전처리할 수 있다. 학습 데이터 전처리부(1110-2)는 모델 학습부(1110-4)가 상황 판단을 위한 학습을 위하여 획 득된 데이터를 이용할 수 있도록, 획득된 데이터를 기 설정된 포맷으로 가공할 수 있다. 학습 데이터 선택부(1110-3)는 학습 데이터 획득부(1110-1)에서 획득된 데이터 또는 학습 데이터 전처리부 (1110-2)에서 전처리된 데이터 중에서 학습에 필요한 데이터를 선택할 수 있다. 선택된 학습 데이터는 모델 학 습부(1110-4)에 제공될 수 있다. 학습 데이터 선택부(1110-3)는 기설정된 선별 기준에 따라, 획득되거나 전처리된 데이터 중에서 학습에 필요한 학습 데이터를 선택할 수 있다. 또한, 학습 데이터 선택부(1110-3)는 모델 학 습부(1110-4)에 의한 학습에 의해 기설정된 선별 기준에 따라 학습 데이터를 선택할 수도 있다. 학습부는 데이터 인식 모델의 분석 결과를 향상시키기 위하여, 모델 평가부(1110-5)를 더 포함할 수도 있 다. 모델 평가부(1110-5)는 인식 모델에 평가 데이터를 입력하고, 평가 데이터로부터 출력되는 분석 결과가 소정 기 준을 만족하지 못하는 경우, 모델 학습부(1110-4)로 하여금 다시 학습하도록 할 수 있다. 이 경우, 평가 데이터 는 인식 모델을 평가하기 위한 기 정의된 데이터일 수 있다. 예를 들어, 모델 평가부(1110-5)는 평가 데이터에 대한 학습된 인식 모델의 분석 결과 중에서, 분석 결과가 정 확하지 않은 평가 데이터의 개수 또는 비율이 미리 설정된 임계치를 초과하는 경우 소정 기준을 만족하지 못한 것으로 평가할 수 있다. 한편, 학습된 인식 모델이 복수 개가 존재하는 경우, 모델 평가부(1110-5)는 각각의 학습된 인식 모델에 대하여 소정 기준을 만족하는지를 평가하고, 소정 기준을 만족하는 모델을 최종 인식 모델로서 결정할 수 있다. 이 경 우, 소정 기준을 만족하는 모델이 복수 개인 경우, 모델 평가부(1110-5)는 평가 점수가 높은 순으로 미리 설정 된 어느 하나 또는 소정 개수의 모델을 최종 인식 모델로서 결정할 수 있다. 도 11B의 (b)를 참조하면, 일부 실시예에 따른 인식부는 인식 데이터 획득부(1120-1) 및 인식 결과 제공 부(1120-4)를 포함할 수 있다. 또한, 인식부는 인식 데이터 전처리부(1120-2), 인식 데이터 선택부(1120-3) 및 모델 갱신부(1120-5) 중 적어도 하나를 선택적으로 더 포함할 수 있다. 인식 데이터 획득부(1120-1)는 상황 판단에 필요한 데이터를 획득할 수 있다. 인식 결과 제공부(1120-4)는 인식 데이터 획득부(1120-1)에서 획득된 데이터를 입력 값으로 학습된 인식 모델에 적용하여 상황을 판단할 수 있다. 인식 결과 제공부(1120-4)는 데이터의 분석 목적에 따른 분석 결과를 제공할 수 있다. 인식 결과 제공부(1120- 4)는 후술할 인식 데이터 전처리부(1120-2) 또는 인식 데이터 선택부(1120-3)에 의해 선택된 데이터를 입력 값 으로 인식 모델에 적용하여 분석 결과를 획득할 수 있다. 분석 결과는 인식 모델에 의해 결정될 수 있다. 일 실시예로, 인식 결과 제공부(1120-4)는 인식 데이터 획득부(1120-1)에서 획득한 객체가 포함된 객체 영역을 학습된 인식 모델 적용하여 객체 영역에 대응하는 객체 정보를 획득(또는, 추정)할 수 있다. 다른 실시예로, 인식 결과 제공부(1120-4)는 인식 데이터 획득부(1120-1)에서 획득한 객체 영역, 객체 정보 및 컨텍스트 정보 중 적어도 하나를 학습된 인식 모델에 적용하여 검색 결과를 제공할 검색 카테고리를 획득(또는, 추정)할 수 있다 인식부는 인식 모델의 분석 결과를 향상시키거나, 분석 결과의 제공을 위한 자원 또는 시간을 절약하기 위하여, 인식 데이터 전처리부(1120-2) 및 인식 데이터 선택부(1120-3)를 더 포함할 수도 있다. 인식 데이터 전처리부(1120-2)는 상황 판단을 위해 획득된 데이터가 이용될 수 있도록, 획득된 데이터를 전처리 할 수 있다. 인식 데이터 전처리부(1120-2)는 인식 결과 제공부(1120-4)가 상황 판단을 위하여 획득된 데이터를 이용할 수 있도록, 획득된 데이터를 기 정의된 포맷으로 가공할 수 있다. 인식 데이터 선택부(1120-3)는 인식 데이터 획득부(1120-1)에서 획득된 데이터 또는 인식 데이터 전처리부 (1120-2)에서 전처리된 데이터 중에서 상황 판단에 필요한 데이터를 선택할 수 있다. 선택된 데이터는 인식 결 과 제공부(1120-4)에게 제공될 수 있다. 인식 데이터 선택부(1120-3)는 상황 판단을 위한 기 설정된 선별 기준 에 따라, 획득되거나 전처리된 데이터 중에서 일부 또는 전부를 선택할 수 있다. 또한, 인식 데이터 선택부 (1120-3)는 모델 학습부(1110-4)에 의한 학습에 의해 기 설정된 선별 기준에 따라 데이터를 선택할 수도 있다. 모델 갱신부(1120-5)는 인식 결과 제공부(1120-4)에 의해 제공되는 분석 결과에 대한 평가에 기초하여, 인식 모 델이 갱신되도록 제어할 수 있다. 예를 들어, 모델 갱신부(1120-5)는 인식 결과 제공부(1120-4)에 의해 제공되 는 분석 결과를 모델 학습부(1110-4)에게 제공함으로써, 모델 학습부(1110-4)가 인식 모델을 추가 학습 또는 갱 신하도록 요청할 수 있다. 도 12는, 일 실시예에 따른 청소 로봇 및 서버가 서로 연동함으로써 데이터를 학습하고 인식하는 예 시를 나타내는 도면이다.도 12를 참조하면, 서버는 상황 판단을 위한 기준을 학습할 수 있으며, 청소 로봇는 서버에 의 한 학습 결과에 기초하여 상황을 판단할 수 있다. 이 경우, 서버의 모델 학습부(1110-4)는 도 11B에 도시된 학습부의 기능을 수행할 수 있다. 서버 의 모델 학습부(1110-4)는 소정의 상황을 판단하기 위하여 어떤 객체 영상, 객체 정보 또는 컨텍스트 정보 를 이용할지, 상기 데이터를 이용하여 상황을 어떻게 판단할 지에 관한 기준을 학습할 수 있다. 또한, 청소 로봇의 인식 결과 제공부(1120-4)는 인식 데이터 선택부(1120-3)에 의해 선택된 데이터를 서버 에 의해 생성된 인식 모델에 적용하여 객체 정보 또는 검색 카테고리를 판단할 수 있다. 또는, 청소 로봇 의 인식 결과 제공부(1120-4)는 서버에 의해 생성된 인식 모델을 서버로부터 수신하고, 수신된 인식 모델을 이용하여 상황을 판단할 수 있다. 이 경우, 청소 로봇의 인식 결과 제공부(1120-4)는 인식 데 이터 선택부(1120-3)에 의해 선택된 객체 영상을 서버로부터 수신된 인식 모델에 적용하여, 객체 영상에 대응하는 객체 정보를 판단할 수 있다. 또는, 인식 결과 제공부(1120-4)는 컨텍스트 정보 및 컨텍스트 인식 정 보 중 적어도 하나를 이용하여 검색 결과를 획득할 검색 카테고리를 판단할 수 있다. 도 13은, 본 개시의 일 실시 예에 따른 인식 모델을 이용하는 네트워크 시스템의 흐름도이다. 여기서, 제1 구성 요소는 전자 장치(A)이고, 제2 구성 요소는 인식 모델이 저장된 서버가 될 수 있다. 또는, 제1 구성 요소는 범용 프로세서이고, 제2 구성 요소는 인공 지능 전용 프로세서가 될 수 있다. 또는, 제1 구성 요소는 적어도 하나의 어플리케이션이 될 수 있고, 제2 구성 요소는 운영 체제(operating system, OS)가 될 수 있다. 즉, 제2 구성 요소는 제1 구성 요소보다 더 집적 화되거나, 전용화되거나, 딜레이(delay)가 작거나, 성능이 우세하거나 또는 많은 리소스를 가진 구성 요소로서 데이터 인식 모델의 생성, 갱신 또는 적용 시에 요구되는 많은 연산을 제1 구성 요소보다 신속하고 효과 적으로 처리 가능한 구성 요소가 될 수 있다. 도 13을 참조하면, 제1 구성 요소는 객체를 포함하는 주변 환경을 촬영하여 캡처 이미지를 생성할 수 있 다. 제1 구성 요소는 캡처 이미지를 제2 구성 요소로 전송할 수 있다. 이때, 제1 구성 요소는 캡처 이미지와 함께 선택된 객체에 대응되는 객체 영역에 대한 정보를 전송할 수 있다. 제2 구성 요소는 수신된 캡처 이미지를 객체 영역 및 주변 영역으로 분리할 수 있다. 이때, 제2 구 성 요소는 수신된 객체 영역에 대한 정보를 바탕으로 객체 영역 및 주변 영역을 분리할 수 있다. 제2 구성 요소는 분리된 객체 영역 및 주변 영역을 인식 모델로 입력하여 객체 정보 및 객체에 대한 추가 정보를 획득할 수 있다. 이때, 제2 구성 요소는 객체 영역을 객체 인식 모델에 입력하여 객체 정보 를 획득할 수 있으며, 주변 영역을 주변 정보 인식 모델에 입력하여 객체에 대한 추가 정보를 획득할 수 있다. 또한, 제2 구성 요소는 객체 정보 및 객체에 대한 추가 정보를 바탕으로 검색 카테고리 및 검색 카테고리 의 우선 순위를 결정할 수 있다. 제2 구성 요소는 획득된 객체 정보 및 추가 정보를 이용하여 객체와 연관된 결과를 획득할 수 있다 . 이때, 제2 구성 요소는 객체 정보 및 추가 정보를 입력 데이터로서 인식 모델에 적용하여 객체와 연관된 결과를 획득할 수 있다. 이때, 제2 구성 요소는 검색 카테고리를 함께 이용하여 결과를 획득할 수 있다. 또한, 제2 구성 요소는 객체 정보 및 추가 정보 이외에 부가 데이터(예를 들어, 장애물의 위험도, 장애물의 사용자에 대한 중요도)을 이용하여 결과를 획득할 수 있다. 이때, 부가 데이터는 제1 구성 요소 또는 다른 요소로부터 전송되거나 제2 구성 요소에 기 저장될 수 있다. 제2 구성 요소가 객체와 연관된 결과를 제1 구성 요소로 전송하면, 제1 구성 요소는 수신한 객체와 연관된 결과를 청소 로봇의 이동 경로 계획에 반영할 수 있다. 도 14는, 본 개시의 일 실시 예에 따른 청소 로봇이 인식 모델을 이용하여 제1 영역에 대한 검색 결과를 제공하 는 실시예를 설명하기 위한 흐름도이다. 도 14를 참조하면. 청소 로봇은 주변 환경을 캡쳐하여 이미지를 생성할 수 있다. 청소 로봇은 생성된 이미지를 입력 데이터 사용하는 학습된 제1 모델을 통해 제1 영역에 대한 제1 정보를 획득할 수 있다 . 이때, 제1 모델은 청소 로봇에 저장될 수 있으나, 이는 일 실시예에 불과할 뿐, 외부 서버에 저장 될 수 도 있다. 청소 로봇은 제1 정보 및 생성된 이미지를 입력 데이터로 사용하는 학습된 제2 모델을 통해 제2 영역에 대 한 제2 정보를 획득할 수 있다. 이때, 제2 모델은 청소 로봇에 저장될 수 있으나, 이는 일 실시예에불과할 뿐, 외부 서버에 저장될 수 도 있다. 외부 서버에 제1 모델 및 제2 모델이 저장된 경우, 청소 로봇은 생성된 이미지를 외부 서버로 전송하고, 외부 서버로부터 이미지를 제1 모델에 입력하여 획득한 제1 정보 및 이미지와 제1 정보를 제2 모델에 입력하여 획득한 제2 정보를 수신할 수 있다. 상술한 바와 같이, 사용자 입력이 감지된 제1 영역에 대한 제1 정보뿐만 아니라 제1 영역 주위의 제2 영역에 대 한 제2 정보를 획득함으로써, 제1 영역에 대한 정보를 더욱 정확하게 획득할 수 있게 된다. 도 15는, 본 개시의 일 실시 예에 따른 인식 모델을 이용하는 시스템의 흐름도이다. 도 15에서, 청소 로봇은 주변의 환경을 캡쳐하여 이미지를 생성할 수 있다. 청소 로봇은 생성 된 이미지를 입력 데이터로 사용하는 학습된 제1 모델을 통해 제1 영역에 대한 제1 정보를 획득할 수 있다 . 청소 로봇은 생성된 이미지 및 제1 정보를 서버로 전송할 수 있다. 서버는 제1 정보 및 생성된 이미지를 입력 데이터로 사용하는 학습된 제2 모델을 통해 제2 영역에 대한 제 2 정보를 획득할 수 있다. 서버는 제1 정보 및 제2 정보를 바탕으로 제1 영역과 관련된 정보를 검색할 수 있다. 서버는 제1 영역과 관련된 검색 결과를 청소 로봇로 전송할 수 있으며, 청소 로봇는 수신 된 검색 결과를 제공할 수 있다. 즉, 상술한 실시예에서는 객체를 인식하기 위한 제1 모델을 통해 제1 정보를 획득하는 동작은 청소 로봇이 수행하고, 컨텍스트 정보를 추정하기 위한 제2 모델을 통해 제2 정보를 획득하는 동작은 서버가 수행할 수 있다. 즉, 적은 처리량으로도 정보를 수행할 수 있는 객체 인식 동작은 청소 로봇가 수행할 수 있으며, 많 은 처리량이 필요한 컨텍스트 추정 동작은 서버가 수행할 수 있다 한편, 도 15에서 설명한 실시예에서는 하나의 서버가 학습된 모델을 통해 제1 정보 또는 제2 정보를 획득 하고, 제1 영역과 관련된 정보를 검색하는 것으로 설명하였으나, 이는 일 실시예에 불과할 뿐, 복수의 서버가 상술한 동작을 나누어 수행할 수 있다. 즉, 제1 서버가 학습된 모델을 통해 제1 정보 및 제2 정보를 획득하고, 제2 서버가 제1 서버로부터 획득된 제1 정보 및 제2 정보를 바탕으로 제1 영역과 관련된 정보를 검색할 수 있다. 이는 일 실시 예에 불과하며, 서버에서 수행되는 모든 과정은 청소 로봇에서 수행될 수도 있다. 또한, 이상에서는 본 발명의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 발명은 상술한 특정의 실시"}
{"patent_id": "10-2018-0102689", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "예에 한정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 발명이 속하는 기술분야 에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2018-0102689", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시 예에 따른 전자 시스템을 나타내는 도면이다. 도 2는 본 개시의 일 실시 예에 따른, 청소 로봇의 구성을 설명하기 위한 상세한 블록도이다. 도 3은 본 개시의 일 실시 예에 따른, 청소 로봇이 이동 방향을 결정하기 위해 촬영한 영상을 분석하는 방법을 설명하기 위한 흐름도이다. 도 4A 및 4B는 본 개시의 일 실시 예에 따른, 청소 로봇이 촬영한 영상에서 바닥 면 분할영상을 획득하는 것을 설명하기 위한 도면이다. 도 5A 내지 5C는 본 개시의 일 실시 예에 따른, 청소 로봇이 촬영한 영상에서 장애물에 대한 정보를 획득하는 것을 설명하기 위한 도면이다. 도 6A 및 6B는 본 개시의 일 실시 예에 따른, 청소 로봇이 획득한 영상에서 뎁스 맵 영상을 획득하는 것을 설명 하기 위한 도면이다. 도 7A 및 7B는 본 개시의 일 실시 예에 따른, 청소 로봇이 획득한 영상을 통해 벽면까지의 거리를 추정하는 방 법을 설명하기 위한 도면이다. 도 8A 및 8B는 본 개시의 일 실시 에에 따른, 청소 로봇이 태스크를 수행하기 위해 효율적으로 이동하는 것을 설명하기 위한 도면이다.도 9A 및 9B는 청소 로봇이 태스크를 수행할 공간에 대한 네비게이션 맵을 생성하는 것을 도시한 도면이다. 도 10은 청소 로봇이 네비게이션 맵을 이용하여, 청소 태스크 수행을 위한 이동계획을 수립하는 것을 도시한 도 면이다. 도 11A 및 도 11B는, 다양한 실시예에 따른 학습부 및 인식부를 나타내는 블록도이다. 도 12는, 본 개시의 일 실시예에 따른 청소 로봇 및 서버가 서로 연동함으로써 데이터를 학습하고 인 식하는 예시를 나타내는 도면이다. 도 13은, 본 개시의 일 실시 예에 따른 인식 모델을 이용하는 네트워크 시스템의 흐름도이다. 도 14는, 본 개시의 일 실시 예에 따른 청소 로봇이 인식 모델을 이용하여 제1 영역에 대한 검색 결과를 제공하 는 실시예를 설명하기 위한 흐름도이다. 도 15는, 본 개시의 일 실시 예에 따른 인식 모델을 이용하는 시스템의 흐름도이다."}
