{"patent_id": "10-2019-0177904", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0085158", "출원번호": "10-2019-0177904", "발명의 명칭": "문맥 정보를 고려한 개체명 인식 방법 및 장치", "출원인": "한국과학기술원", "발명자": "이재길"}}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 개체명을 인식하는 방법으로서,복수의 단어들에 개체명 라벨을 태깅한 학습 텍스트를 입력받는 단계, 각 단어와 상기 각 단어에 대응되어 태깅된 개체명 라벨 간의 관계를 학습하는 단항 개체명 예측 모델과, 임의의 개체명 라벨이 서로 이웃할 가능성이 출력되도록, 상기 학습 텍스트를 구성하는 인접 단어쌍과 각 인접 단어쌍에 대응되는 개체명 라벨 쌍의 관계를 학습하는 이항 개체명 예측 모델을 각각 학습시키는 단계, 그리고학습된 단항 개체명 예측 모델과 학습된 이항 개체명 예측 모델의 출력단에 라벨 결정 모델을 결합하여 개체명예측 모델을 생성하는 단계를 포함하는, 개체명 인식 방법."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서, 상기 입력받는 단계는,상기 학습 텍스트를 단어 임베딩 모델로 전처리하는, 개체명 인식 방법."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에서, 상기 입력받는 단계는, 상기 복수의 단어들에 IOBES 방식 또는 BIO 방식으로 상기 개체명 라벨을 태깅하고, 상기 IOBES 방식은, 임의의 단어가 복수의 단어들로 구성된 개체명의 시작에 해당하면 B, 개체명의 중간에 해당하면 I, 개체명의 끝에 해당하면 E, 개체명이 아닌 단어이면 O, 한 단어로 구성된 개체명에 해당하면 S로 표시하는 방식이고,상기 BIO 방식은, 상기 임의의 단어가 상기 복수의 단어들로 구성된 개체명의 시작에 해당하면 B, 시작이 아닌개체명에 해당하면 I, 개체명이 아닌 단어이면 O로 표시하는 방식인, 개체명 인식 방법."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에서,상기 학습시키는 단계는, 상기 단항 개체명 예측 모델의 출력값과 상기 이항 개체명 예측 모델의 출력값에 각각 별개의 라벨 결정 모델을연결하는, 개체명 인식 방법."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에서, 상기 개체명 예측 모델은, 상기 단항 개체명 예측 모델에 포함된 양방향 장단기 기억 구조(Bidirectional Long Short-Term Memory,BiLSTM) 레이어의 파라미터들과 상기 이항 개체명 예측 모델에 포함된 BiLSTM 레이어의 파라미터들이 통합된 모델인, 개체명 인식 방법."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2021-0085158-3-제1항에서, 상기 개체명 예측 모델로 텍스트를 입력하는 단계, 그리고상기 개체명 예측 모델을 이용하여 상기 텍스트에 포함된 단어들의 개체명 라벨이 나열된 라벨 시퀀스를 예측하는 단계 를 더 포함하는, 개체명 인식 방법."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에서, 상기 라벨 시퀀스는, 상기 학습된 단항 개체명 예측 모델이 출력하는 확률 값과 상기 학습된 이항 개체명 예측 모델이 출력하는 확률값을 곱한 값이 최대가 되도록 하는, 개체명 인식 방법."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에서, 상기 텍스트는 단어 임베딩 모델에 의해 전처리된 단어 임베딩 벡터인, 개체명 인식 방법."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 개체명을 인식하는 방법으로서,텍스트를 단항 개체명 예측 모델과 이항 개체명 예측 모델로 입력하는 단계, 상기 단항 개체명 예측 모델을 이용하여 상기 텍스트를 구성하는 각 단어에 해당하는 개체명 라벨을 예측하고,상기 이항 개체명 예측 모델을 이용하여 상기 텍스트를 구성하는 인접 단어쌍에 대응되는 예측된 개체명 라벨쌍이 출력될 가능성을 예측하는 단계, 그리고 예측된 결과를 바탕으로, 상기 텍스트에 포함된 단어들의 개체명 라벨이 나열된 라벨 시퀀스를 출력하는 단계를 포함하고, 상기 단항 개체명 예측 모델은, 복수의 단어들에 개체명 라벨을 태깅한 학습 텍스트로 각 단어와 상기 각 단어에 대응되어 태깅된 상기 개체명 라벨 간의 관계를 학습한 모델이고,상기 이항 개체명 예측 모델은, 상기 학습 텍스트로 임의의 개체명 라벨이 서로 이웃할 가능성이 출력되도록 태깅된 개체명 라벨들 간의 인접 관계를 학습한 모델인, 개체명 인식 방법."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에서,상기 출력하는 단계는,상기 단항 개체명 예측 모델의 예측 결과와 상기 이항 개체명 예측 모델의 예측 결과를 하나의 라벨 결정 모델로 결합하는, 개체명 인식 방법."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "컴퓨팅 장치로서,메모리, 그리고상기 메모리에 로드된 프로그램의 명령들(instructions)을 실행하는 적어도 하나의 프로세서를 포함하고,상기 프로그램은개체명 라벨이 태깅된 학습 텍스트로 개체명 예측 모델을 학습시키는 단계,학습된 개체명 예측 모델로 텍스트를 입력하는 단계, 공개특허 10-2021-0085158-4-상기 개체명 예측 모델을 이용하여, 상기 텍스트를 구성하는 각 단어의 개체명 라벨과, 이웃한 단어들에 대응되는 개체명 라벨들이 이웃하여 출력될 수 있는지 여부를 예측하는 단계, 그리고 예측된 개체명 라벨 중 이웃 가능한 개체명 라벨들을 상기 텍스트의 라벨 시퀀스로 출력하는 단계를 실행하도록 기술된 명령들을 포함하는, 컴퓨팅 장치."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에서, 상기 입력하는 단계는, 임의의 단어 임베딩 모델을 이용하여 상기 텍스트를 구성하는 단어들을 단어 임베딩 벡터로 생성하고, 생성된단어 임베딩 벡터들을 상기 개체명 예측 모델로 입력하는, 컴퓨팅 장치."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에서, 상기 개체명 예측 모델은,상기 텍스트를 구성하는 각 단어의 개체명 라벨을 예측하는 단항 개체명 예측 모델과 상기 텍스트를 구성하는이웃한 단어들에 대응되는 개체명 라벨들이 이웃하여 출력될 수 있는지에 대한 가능성을 예측하는 이항 개체명예측 모델을 포함하는, 컴퓨팅 장치."}
{"patent_id": "10-2019-0177904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에서, 상기 학습시키는 단계는,상기 단항 개체명 예측 모델과 상기 이항 개체명 예측 모델에 서로 다른 라벨 결정 모델을 각각 연결하는, 컴퓨팅 장치."}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 개체명을 인식하는 방법으로서, 복수의 단어들에 개체명 라벨을 태깅한 학습 텍스트를 입력받는 단계, 각 단어와 상기 각 단어에 대응되어 태깅된 개체명 라벨 간의 관계 를 학습하는 단항 개체명 예측 모델과, 임의의 개체명 라벨이 서로 이웃할 가능성이 출력되도록, 상기 학습 텍스 트를 구성하는 인접 단어쌍과 각 인접 단어쌍에 대응되는 개체명 라벨 쌍의 관계를 학습하는 이항 개체명 예측 모델을 각각 학습시키는 단계, 그리고 학습된 단항 개체명 예측 모델과 학습된 이항 개체명 예측 모델의 출력단 에 라벨 결정 모델을 결합하여 개체명 예측 모델을 생성하는 단계를 포함한다."}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 문맥 정보를 고려한 개체명 인식 기술에 관한 것이다."}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "개체명 인식(Named Entity Recognition, NER)은 입력된 텍스트에서 관심 대상이 되는 개체를 찾는 것으로, 예를 들어, 일반적인 뉴스 텍스트에서 사람, 조직, 지리적 위치 같은 대상들, 또는 생물 의학 분야에서의 질병, 단백 질, 유전자, 화학 성분 등의 대상들을 찾아내는 것이다. 따라서 개체명 인식은 자연어 처리(Natural Language Processing, NLP), 정보 추출(Information Extraction), 및 지식 서비스(Knowledge Service)에서의 기본이 된 다. 대규모 텍스트에서 개체명 인식을 자동으로 수행하기 위해, 기존에는 해당 분야의 전문 지식과 경험을 갖춘 전 문가가 수작업으로 규칙(Rule) 또는 특징(Feature)을 설계하고 이를 라벨 시퀀스를 얻는 알고리즘에 대입하였다. 최근에는 기계 학습(Machine Learning)을 통해 대규모 텍스트로부터 개체명 인식에 적합한 특징 또는 표현 (Representation)을 고정된 길이의 벡터로 추출하고, 영상 처리 및 자연어 처리를 포함한 다양한 분야에서 채택, 이용되고 있는 딥러닝 기술을 이용하여 개체명 인식을 수행한다. 예를 들어, 입력 텍스트를 구성하는 단어들간의 관계를 설명하기 위한 딥러닝 모델과 단어를 구성하는 문자들간 의 관계를 설명하기 위한 딥러닝 모델을 결합하고, 조건부 무작위 장(Conditional Random Field, 이하 ‘CRF’ 라 호칭함)을 이용하여 텍스트를 구성하는 각각의 개별 단어에 라벨이 태깅된 결과를 도출한다. 이때 CRF 모델은 전이 행렬을 이용하여 이웃하는 라벨 사이의 관계를 반영한 라벨 시퀀스(Label Sequence)를 출 력하는 역할을 한다. 라벨 시퀀스란 텍스트를 구성하는 각각의 단어마다 상응하는 개별 라벨들의 연속된 순열을 의미한다. 전이 행렬이란, 텍스트 상에서 두 개의 연속된 입력 단어들에 대한 이웃한 라벨들 사이의 관계를 모델링하기 위 한 것이다. 전이 행렬 기반의 모델은 학습 데이터에만 의존하여 기계 학습을 진행하며, 새로운 텍스트가 입력되 어도, 기존에 학습된 전이 행렬의 값을 참조할 뿐, 새로 입력된 텍스트의 문맥을 이웃 라벨들간의 관계에 반영 하지 못한다는 한계가 있다. 한편, 한국어 텍스트에서 개체명 인식 성능을 향상시키기 위해, 하나의 어절에 여러 형태소가 조합되는 교착어 인 한국어에서는 개체명이 어절 어두에 위치하는 경향을 이용한 방법이 개발되었다. 이 방법은 한국어 텍스트에 서 음절-바이그램 단위로 벡터를 학습하고, 각각의 음절-바이그램마다 하나의 라벨을 부여한다. 각각의 음절-바 이그램에 대한 라벨을 인식하기 위해 인공신경망을 이용하는 특징이 있지만, 이웃하는 연속된 음절-바이그램의 상관관계를 활용하지는 않는다. 따라서 입력 텍스트에 대해 개별 단어뿐만이 아니라 이웃하는 단어와의 문맥 관계를 파악하고, 파악한 문맥 관 계를 반영하여 라벨 시퀀스를 예측하는 방법이 요구된다."}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "해결하고자 하는 과제는 입력 텍스트의 라벨 시퀀스를 예측하기 위해, 입력 텍스트에 포함된 개별 단어의 독립 적인 특성과 개별 단어와 이웃한 단어들과의 관계를 고려하여 각 단어들의 개체명을 인식하는 방법 및 장치를 제공하는 것이다."}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "한 실시예에 따른 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 개체명을 인식하는 방법으로서, 복수 의 단어들에 개체명 라벨을 태깅한 학습 텍스트를 입력받는 단계, 각 단어와 상기 각 단어에 대응되어 태깅된 개체명 라벨 간의 관계를 학습하는 단항 개체명 예측 모델과, 임의의 개체명 라벨이 서로 이웃할 가능성이 출력 되도록, 상기 학습 텍스트를 구성하는 인접 단어쌍과 각 인접 단어쌍에 대응되는 개체명 라벨 쌍의 관계를 학습 하는 이항 개체명 예측 모델을 각각 학습시키는 단계, 그리고 학습된 단항 개체명 예측 모델과 학습된 이항 개 체명 예측 모델의 출력단에 라벨 결정 모델을 결합하여 개체명 예측 모델을 생성하는 단계를 포함한다. 상기 입력받는 단계는, 상기 학습 텍스트를 단어 임베딩 모델로 전처리할 수 있다. 상기 입력받는 단계는, 상기 복수의 단어들에 IOBES 방식 또는 BIO 방식으로 상기 개체명 라벨을 태깅하고, 상 기 IOBES 방식은, 임의의 단어가 복수의 단어들로 구성된 개체명의 시작에 해당하면 B, 개체명의 중간에 해당하 면 I, 개체명의 끝에 해당하면 E, 개체명이 아닌 단어이면 O, 한 단어로 구성된 개체명에 해당하면 S로 표시하 는 방식이고, 상기 BIO 방식은, 상기 임의의 단어가 상기 복수의 단어들로 구성된 개체명의 시작에 해당하면 B, 시작이 아닌 개체명에 해당하면 I, 개체명이 아닌 단어이면 O로 표시하는 방식일 수 있다. 상기 학습시키는 단계는, 상기 단항 개체명 예측 모델의 출력값과 상기 이항 개체명 예측 모델의 출력값에 각각 별개의 라벨 결정 모델을 연결할 수 있다. 상기 개체명 예측 모델은, 상기 단항 개체명 예측 모델에 포함된 양방향 장단기 기억 구조(Bidirectional Long Short-Term Memory, BiLSTM) 레이어의 파라미터들과 상기 이항 개체명 예측 모델에 포함된 BiLSTM 레이어의 파 라미터들이 통합된 모델일 수 있다. 상기 개체명 예측 모델로 텍스트를 입력하는 단계, 그리고 상기 개체명 예측 모델을 이용하여 상기 텍스트에 포 함된 단어들의 개체명 라벨이 나열된 라벨 시퀀스를 예측하는 단계를 더 포함할 수 있다. 상기 라벨 시퀀스는, 상기 학습된 단항 개체명 예측 모델이 출력하는 확률 값과 상기 학습된 이항 개체명 예측 모델이 출력하는 확률 값을 곱한 값이 최대가 되도록 할 수 있다. 상기 텍스트는 단어 임베딩 모델에 의해 전처리된 단어 임베딩 벡터일 수 있다. 다른 실시예에 따른 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 개체명을 인식하는 방법으로서, 텍 스트를 단항 개체명 예측 모델과 이항 개체명 예측 모델로 입력하는 단계, 상기 단항 개체명 예측 모델을 이용하여 상기 텍스트를 구성하는 각 단어에 해당하는 개체명 라벨을 예측하고, 상기 이항 개체명 예측 모델을 이용 하여 상기 텍스트를 구성하는 인접 단어쌍에 대응되는 예측된 개체명 라벨쌍이 출력될 가능성을 예측하는 단계, 그리고 예측된 결과를 바탕으로, 상기 텍스트에 포함된 단어들의 개체명 라벨이 나열된 라벨 시퀀스를 출력하는 단계를 포함하고, 상기 단항 개체명 예측 모델은, 복수의 단어들에 개체명 라벨을 태깅한 학습 텍스트로 각 단 어와 상기 각 단어에 대응되어 태깅된 상기 개체명 라벨 간의 관계를 학습한 모델이고, 상기 이항 개체명 예측 모델은, 상기 학습 텍스트로 임의의 개체명 라벨이 서로 이웃할 가능성이 출력되도록 태깅된 개체명 라벨들 간 의 인접 관계를 학습한 모델이다. 상기 출력하는 단계는, 상기 단항 개체명 예측 모델의 예측 결과와 상기 이항 개체명 예측 모델의 예측 결과를 하나의 라벨 결정 모델로 결합할 수 있다. 한 실시예에 따른 컴퓨팅 장치로서, 메모리, 그리고 상기 메모리에 로드된 프로그램의 명령들(instructions)을 실행하는 적어도 하나의 프로세서를 포함하고, 상기 프로그램은 개체명 라벨이 태깅된 학습 텍스트로 개체명 예 측 모델을 학습시키는 단계, 학습된 개체명 예측 모델로 텍스트를 입력하는 단계, 상기 개체명 예측 모델을 이 용하여, 상기 텍스트를 구성하는 각 단어의 개체명 라벨과, 이웃한 단어들에 대응되는 개체명 라벨들이 이웃하 여 출력될 수 있는지 여부를 예측하는 단계, 그리고 예측된 개체명 라벨 중 이웃 가능한 개체명 라벨들을 상기 텍스트의 라벨 시퀀스로 출력하는 단계를 실행하도록 기술된 명령들을 포함한다. 상기 입력하는 단계는, 임의의 단어 임베딩 모델을 이용하여 상기 텍스트를 구성하는 단어들을 단어 임베딩 벡 터로 생성하고, 생성된 단어 임베딩 벡터들을 상기 개체명 예측 모델로 입력할 수 있다. 상기 개체명 예측 모델은, 상기 텍스트를 구성하는 각 단어의 개체명 라벨을 예측하는 단항 개체명 예측 모델과 상기 텍스트를 구성하는 이웃한 단어들에 대응되는 개체명 라벨들이 이웃하여 출력될 수 있는지에 대한 가능성 을 예측하는 이항 개체명 예측 모델을 포함할 수 있다. 상기 학습시키는 단계는, 상기 단항 개체명 예측 모델과 상기 이항 개체명 예측 모델에 서로 다른 라벨 결정 모 델을 각각 연결할 수 있다."}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 개별 단어의 특성뿐만 아니라 이웃하는 단어와들과 상관관계를 반영하여 개체명 인식 결과인 라벨 시퀀스를 생성하므로 개체명 인식의 성능과 정확도를 높일 수 있다. 또한 본 발명에 따르면 뉴스 등의 일반적인 텍스트뿐만 아니라, 생물이나 의학 분야의 전문적인 텍스트에 대한 개체명 인식 성능을 향상시킬 수 있다."}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위 해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를붙였다. 명세서 전체에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 명세서에 기재 된 \"…부\", \"…기\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드 웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 도 1은 한 실시예에 따른 개체명 인식 장치의 구조도이고, 도 2는 한 실시예에 따른 개체명 인식 결과의 예시도 이다. 도 1을 참고하면, 개체명 인식 장치는 입력 문장을 벡터화하는 전처리부, 전처리된 학습 텍스트를 이용하여 두 개의 개별적인 딥러닝 모델(210, 220)을 각각 학습시키는 학습부 그리고 학습된 모델들(210, 220)과 라벨 결정 모델을 포함하는 개체명 예측 모델을 이용하여 입력 문장의 개체명을 예측하고 라 벨 시퀀스를 출력하는 예측부를 포함한다. 설명을 위해, 전처리부, 학습부 그리고 예측부로 명명하여 부르나, 이들은 적어도 하나의 프로 세서에 의해 동작하는 컴퓨팅 장치이다. 여기서, 전처리부, 학습부 그리고 예측부는 하나의 컴 퓨팅 장치에 구현되거나, 별도의 컴퓨팅 장치에 분산 구현될 수 있다. 별도의 컴퓨팅 장치에 분산 구현된 경우, 전처리부, 학습부 그리고 예측부는 통신 인터페이스를 통해 서로 통신할 수 있다. 컴퓨팅 장치 는 본 발명을 수행하도록 작성된 소프트웨어 프로그램을 실행할 수 있는 장치이면 충분하고, 예를 들면, 서버, 랩탑 컴퓨터 등일 수 있다. 전처리부, 학습부 그리고 예측부 각각은 하나의 인공지능 모델일 수 있고, 복수의 인공지능 모 델로 구현될 수도 있다. 그리고 단어 임베딩 모델, 단항 딥러닝 모델, 이항 딥러닝 모델, 단항 라벨 결정 모델, 이항 라벨 결정 모델, 라벨 결정 모델, 그리고 개체명 예측 모델도 하나 의 인공지능 모델일 수 있고, 복수의 인공지능 모델로 구현될 수도 있다. 개체명 인식 장치는 하나의 인 공지능 모델일 수 있고, 복수의 인공지능 모델로 구현될 수도 있다. 이에 따라, 상술한 구성들에 대응하는 하나 또는 복수의 인공지능 모델은 하나 또는 복수의 컴퓨팅 장치에 의해 구현될 수 있다. 개체명 인식 장치는 하나의 문장이 입력되면, 문장을 구성하는 단어마다 개체명 인식 결과인 라벨을 부착 하여, 개체명 라벨들이 배열된 것인 라벨 시퀀스를 출력한다. 인공지능 모델의 관점에서 단어를 노드라고 호칭 할 수 있다. 한편 라벨 시퀀스를 구성하는 개체명을 출력하는 방식으로서, BIO 태깅(Tagging)이 사용될 수 있다. BIO 태깅이 란 개체명 인식 결과를 표시할 때 여러 개의 단어로 이루어진 개체명의 시작 단어를 B(Beginning)로 표시하고 중간에 있는 단어를 I(Inside)로, 개체명이 아닌 단어의 경우에는 O(Outside)로 표시하는 태깅 방법이다. 다른 예로서, BIO에 추가로 개체명의 끝에 있는 문자를 E(End), 하나의 단어 자체가 개체명인 경우는 S(Singleton)으로 추가로 표시하는 IOBES 태깅이 사용될 수도 있다. 개체명을 태깅하는 방식은 어느 하나에 제 한되지 않는다. 도 2를 참고하면, 입력 텍스트의 첫번째 줄에 대해, 첫번째 줄을 구성하는 각 단어들의 개체명 인식 결과가 라 벨 시퀀스로 출력된다. 예를 들어, ureteric은 Disease에 해당하는 개체명의 시작이고, obstruction은 Disease 에 해당하는 개체명의 끝이므로, ureteric obstruction은 Disease에 해당하는 하나의 개체명임을 알 수 있다. 또한 caused, by 등은 개체명이 아니고, indinavir은 단어 하나로 구성된 개체명임을 알 수 있다. 다시 도 1로 돌아가서, 개체명 인식 장치를 구성하는 부분들의 역할에 대해 설명한다. 전처리부는 학습 텍스트 및 입력 텍스트의 단어들을 임베딩 벡터로 변환한다. 변환 방법은 예를 들어, 입 력 단어에 대한 사전을 검색하여 생성하거나, 사전에 학습된 모델(Pre-trained Model)을 통해 출력된 값을 이용 할 수 있다. 입력 단어를 구성하는 문자들을 새로운 딥러닝 모델에 넣어 그 모델로부터 출력된 벡터를 이용할 수도 있다. 또한 복수의 방법들을 이용할 수도 있으며, 어느 하나에 한정되지 않는다. 본 명세서에서는 단어 임베딩 모델을 이용하며, 한 예로서 입력되는 전체 문장을 고려하여 단어를 임베딩 하는 사전 훈련된 언어 모델(Embeddings from Language Model, ELMo)을 이용한 경우에 대해 설명한다. 전처리부는 학습 텍스트를 단어 임베딩 벡터로 변환하여 학습부에 전달하고, 입력 텍스트를 단어 임 베딩 벡터로 변환하여 예측부에 전달한다. 학습부는 단어 임베딩 벡터로 변환된 학습 텍스트와 두 개의 라벨 결정 모델(230, 240)을 이용하여 단항 딥러닝 모델과 이항 딥러닝 모델을 각각 학습시킨다. 단항 딥러닝 모델이란 하나의 단어 자체만을 고려하여 그 단어의 개별적인 라벨의 확률을 계산하는 모델을 의미하고, 이항 딥러닝 모델은 하나의 단어 주위에 위치한 이웃 단어들의 라벨들을 고려하여 그 단어의 라 벨의 확률을 계산하는 역할을 하는 딥러닝 모델을 의미한다. 단항 딥러닝 모델 또는 이항 딥러닝 모델은 개체명 인식 장치가 적용되는 태스크의 특성에 따 라 적합한 모델로서 구현될 수 있다. 예를 들어 합성곱 신경망(Convolutional Neural Network, CNN), 순환 신 경망(Recurrent Neural Network, RNN), 장단기 기억 구조(Long Short-Term Memory, LSTM), 어텐션(Attention) 메커니즘 기반의 Transformer 등으로 구현될 수 있으며, 어느 하나로 한정되는 것은 아니다. 본 명세서에서 단항 딥러닝 모델과 이항 딥러닝 모델은 양방향 장단기 기억 구조(Bidirectional Long Short-Term Memory, BiLSTM)로 구현된 경우에 대해 설명한다. 단항 딥러닝 모델과 이항 딥러닝 모델(22 0)의 자세한 구조 및 각 모델을 학습시키는 과정에 대해서는 도 4를 통해 설명한다. 예측부는 학습부를 통해 학습된 단항 딥러닝 모델과 학습된 이항 딥러닝 모델을 하나의 라벨 결정 모델로 결합하여 개체명 예측 모델을 생성한다. 단항 딥러닝 모델과 이항 딥러닝 모 델이 하나의 라벨 결정 모델을 통해 결합된 개체명 예측 모델의 구조는 도 6과 도 7을 통해 자 세히 설명한다. 그리고 예측부는 생성한 개체명 예측 모델을 이용하여 입력 텍스트의 개체명 인식 결과로서 하나의 라벨 시퀀스를 출력한다. 라벨 결정 모델은 단항 딥러닝 모델과 이항 딥러닝 모델을 통해 계산된 확률 값을 수집하여, 이 웃하는 라벨과의 관계를 고려하여 가장 적합한 하나의 라벨 시퀀스를 출력한다. 본 명세서에서는 라벨 결정 모델을 CRF로 구현하였으나, 반드시 이에 한정되는 것은 아니며 은닉 마르코프 모델(Hidden Markov Model, HMM), 최대 엔트로피 마르코프 모델(Maximum Entropy Markov Model, MEMM) 등으로 구현할 수 있다. 도 3은 한 실시예에 따른 개체명 인식 장치가 동작하는 방법의 흐름도이다. 도 3을 참고하면, 개체명 인식 장치는 학습 텍스트를 입력받는다(S110). 입력되는 학습 텍스트의 분야는 뉴스, 생물 분야, 의학 분야 등일 수 있으며, 어느 하나에 한정되지 않는다. 학습 텍스트는 각 단어에 IOBES 태 깅이 이루어진 데이터를 의미한다. 개체명 인식 장치의 전처리부는 단어 임베딩 모델을 이용하여 학습 텍스트를 단어 임베딩 벡터 로 변환한다(S120). 단어를 벡터로 변환하는 단어 임베딩 방법은 어느 하나에 한정되지 않으며, 한 예로서 단어 를 벡터 차원으로 표현하는 분산 표상 모델인 Word2Vec와 사전 훈련된 언어 모델(Pre-trained language mode l)을 사용하는 ELMo(Embeddings from Language Model)를 단어 임베딩 모델로 사용할 수 있다. 학습부는 단항 딥러닝 모델에 출력단에 단항 라벨 결정 모델을 연결하고, S120 단계에서 변환된 단어 임베딩 벡터로 단항 딥러닝 모델을 학습시킨다(S130). 즉 단항 딥러닝 모델은 학습 텍스트를 구 성하는 각각의 단어와 태깅된 IOBES 결과와의 관계를 학습한다. S130 단계와 병렬적으로, 학습부는 이항 딥러닝 모델에 출력단에 이항 라벨 결정 모델을 연결하 고, S120 단계에서 변환된 단어 임베딩 벡터로 이항 딥러닝 모델을 학습시킨다(S140). 즉 이항 딥러닝 모 델은 학습 텍스트를 구성하는 복수의 단어들과 해당 단어들에 태깅된 IOBES 결과를 이용하여 출력되는 개 체명들 간의 관계를 학습한다. 개체명 인식 장치의 예측부는, S130 단계와 S140 단계를 통해 학습된 단항 딥러닝 모델과 이항 딥러닝 모델을 결합하고, 출력단에 라벨 결정 모델을 연결하여 개체명 예측 모델을 생성한다 (S150). 이때, 두 개의 모델을 학습시킬 때 사용한 라벨 결정 모델(230, 240)이 아닌, 새로운 라벨 결정 모델 에 학습된 두 모델을 결합한다. 이는 도 2에서 설명한 바와 같이, 학습 과정에서는 단항 딥러닝 모델(21 0)과 이항 딥러닝 모델의 매개 변수 학습을 용이하게 하기 위해 두 개의 딥러닝 모델에 각각 라벨 결정 모 델(230, 240)을 연결한 것이기 때문이다. 이후 개체명 인식 장치는 입력 텍스트를 입력받고, 전처리부에서 입력 텍스트를 단어 임베딩 벡터로 변환한다(S170). 사용되는 단어 임베딩 방법은 S120 단계와 동일할 수 있다. 예측부는 변환된 단어 임베딩 벡터들을 개체명 예측 모델에 넣고, 입력 텍스트에 대한 개체명 인식 결과인 라벨 시퀀스를 출력한다(S180). 출력된 결과인 라벨 시퀀스는 도 2의 형태일 수 있다. 이하에서는, S130 단계와 S140 단계에서 학습부가 단항 딥러닝 모델과 이항 딥러닝 모델을 각각 학습시키는 방법을, 각 딥러닝 모델의 구조를 통해 설명한다. 도 4는 한 실시예에 따른 단항 딥러닝 모델과 이항 딥러닝 모델의 구조도이고, 도 5는 한 실시예에 따른 전이 행렬의 예시도이다. 도 4의 (a)는 단항 딥러닝 모델의 구조를 나타낸 것이고, 도 4의 (b)는 이항 딥러닝 모델의 구조를 나타낸 것이다. 학습 텍스트가 전처리부에서 단어 임베딩 벡터로 생성되어 학습부에 입력되면, 학습부는 단항 딥러닝 모델과 이항 딥러닝 모델을 각각 학습시킨다. 예측부에서는 두 개의 딥러닝 모델이 하나의 라벨 결정 모델을 통해 결합되지만, 학습 과정에서는 단 항 딥러닝 모델과 이항 딥러닝 모델의 매개 변수 학습을 용이하게 하기 위해 각 딥러닝 모델이 개별 적으로 학습된다. 즉, 단항 라벨 결정 모델과 이항 라벨 결정 모델이 각각 이용된다. 각 딥러닝 모델(210, 220)은 각각의 개별적인 라벨 결정 모델(230, 240)을 이용하여 학습 텍스트에 대한 우도 (Likelihood)를 최대화시키기 위한 모델 학습을 진행한다. 단항 딥러닝 모델은 BiLSTM 레이어, 결합 레이어를 포함하고 단항 라벨 결정 모델과 연결 되며, 이항 딥러닝 모델은 BiLSTM 레이어, 결합 레이어를 포함하고 이항 라벨 결정 모델과 연결된다. 이하에서는, 단항 딥러닝 모델과 이항 딥러닝 모델에 각각 포함된 BiLSTM 레이어와 BiLSTM 레이 어의 역할은 유사하므로 같이 설명하고, 단항 딥러닝 모델에서의 결합 레이어와 이항 딥러닝 모 델에서의 결합 레이어 및 단항 라벨 결정 모델과 이항 라벨 결정 모델의 역할은 따로 설명 한다. BiLSTM 레이어(211, 221)는 전처리부에서 생성된 단어 임베딩 벡터를 입력받아 그에 따른 임의의 차원을 갖는 상태 벡터를 생성한다. 구체적으로, BiLSTM을 통해 생성되는 시간 t에서의 상태 벡터는, 정방향 LSTM의 시 간 t에서의 상태 벡터와 역방향 LSTM의 시간 t에서의 상태 벡터의 단순 연결 연산을 통해 생성된다. 이때, 정방향 LSTM의 시간 t에서의 상태 벡터는 시간 t에서의 단어 임베딩 벡터와 이전 시간 t-1에서의 정방향 LSTM의 상태 벡터를 입력으로 하는 함수 연산을 통해 생성된다. 마찬가지로, 역방향 LSTM의 시간 t에서의 상태 벡터는 시간 t에서의 단어 임베딩 벡터와 미래 시간 t+1에서의 역방향 LSTM의 상태 벡터를 입력으로 하는 함수 연산을 통해 생성된다. BiLSTM 레이어(211, 221)를 통해 생성된 임의의 차원을 갖는 상태 벡터는 상위의 결합 레이어(212, 222)의 입력 벡터가 된다. 한편 결합 레이어(212, 222)를 통해 출력되는 벡터의 크기는 가능한 라벨의 개수와 일치할 수 있 다. 예를 들어, 텍스트를 구성하는 질병(Disease)과 화학 물질(Chemical)의 개체명을 인식하고자 하고, IOBES 태깅 방식을 이용하는 문제를 설명한다. 이 경우 각 단어가 가질 수 있는 라벨은 Disease와 Chemical 유형의 개 체명에 I, B, E, S가 가능하고, O가 가능하므로 가능한 라벨의 개수는 2*4+1로 총 9개일 수 있다. 결합 레이어(212, 222)는 적어도 한 개의 선형 변환 연산으로 구성되며, 반드시 이에 제한되는 것은 아니며 경 우에 따라 복수 개의 선형 및 비선형 변환 연산들의 결합으로 구성될 수 있다. 이하에서는 도면을 통해 단항 딥 러닝 모델의 결합 레이어와 이항 딥러닝 모델의 결합 레이어의 역할에 대해 설명한다. 먼저 도 4의 (a)를 참고하면, 단항 딥러닝 모델의 결합 레이어는 임의의 함수를 통해 스코어 벡터를 생성한다. 스코어란, 입력된 텍스트를 구성하는 각 단어가 임의의 라벨에 해당할 수치를 의미하고, 스코어 벡터란 단어별 스코어를 벡터 형식으로 나열한 것을 의미한다. 앞서 설명한 예에 대해서, 텍스트를 구성하는 각 단어가 “B- Disease\", \"I-Disease\", \"E-Disease\", \"S-Disease\", \"B-Chemical\", \"I-Chemical\", \"E-Chemical\", \"S- Chemical\", \"O\" 라는 각 라벨에 해당할 스코어가 출력될 수 있다. 이 경우, 단항 딥러닝 모델에서의 결합 레이어는 각 노드에 대응되는 스코어 벡터를 생성하며, 스코 어 벡터의 크기는 9가 될 것이다. 또한 나머지 임의의 노드의 스코어 벡터 역시, 그 노드 단어가 9개의 라벨 각 각에 해당할 스코어를 포함할 것이다. 단항 딥러닝 모델에 연결된 단항 라벨 결정 모델은 수학식 1과 같이 학습 텍스트에 대한 라벨 시퀀스 확률을 계산한다. 수학식 1"}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서, X는 입력되는 학습 텍스트이고, Y는 출력되는 라벨 시퀀스이고, 는 출력되는 라벨 시 퀀스의 확률 값이다. 는 출력 값을 확률로 정규화 시키기 위하여 가능한 라벨 시퀀스의 확률 값을 모두 더한 값이고, N은 학습 데이터에 포함된 단어의 개수이자 출력되는 라벨의 개수이다. 는 i번째 단어에 대해 라는 라벨을 할당하기 위한 단항 스코어이고, 는 전이 행렬 A의 i-1번째 행과 i번째 열에 해당하는 값 을 의미하며, 도 5를 통해 자세히 설명한다. 한편 는 다음 수학식 2를 통해 계산될 수 있다. 수학식 2"}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 3, "content": "도 5를 참고하면, 전이 행렬은 두 개의 라벨들로 조합할 수 있는 가능한 모든 라벨 쌍들의 조합들을 나타낸 행 렬을 의미한다. 즉 행렬의 각 성분(Entry)은 각 행과 열에 해당하는 라벨들의 쌍(Pair)이 라벨 시퀀스에서 함께 이웃하여 나타날 상호 호환성에 대한 스코어이다. 전이 행렬은 기계 학습 과정에서 만들어져 저장된 값을 사용하기 때문에, 새로운 텍스트가 입력되어도 임의의 라벨 쌍에 대해서는 항상 같은 값을 제공한다. 따라서, 텍스트의 문맥에 따른 이웃 라벨과의 관계 및 호환성을 고려하지 못하는 단점이 있다. 따라서, 본 발명에서는 텍스트 입력에 대해 출력 단계의 라벨 시퀀스에서 이웃하 는 라벨들 간의 관계를 고려하는 이항 딥러닝 모델을 함께 사용한다. 이제 도 4의 (b)를 참고하여, 이항 딥러닝 모델의 결합 레이어의 역할에 대해 설명한다. 이항 딥러닝 모델의 결합 레이어는 하단의 BiLSTM 레이어에서 출력된 이웃하는 위치의 두 개의 상태 벡터들 을 입력으로 받고, 이웃하는 라벨 쌍의 상호 호환성에 대한 스코어 벡터를 생성한다. 이때 스코어 벡터의 크기는, 조합될 수 있는 모든 라벨 쌍에 대한 스코어를 포함하도록 결정될 수 있다. 예를 들어, 앞서 설명한 텍스트를 구성하는 질병(Disease)과 화학 물질(Chemical)의 개체명을 인식하고자 하고, IOBES 태깅 방식을 이용하는 문제에 대해 설명한다. 이 경우, 텍스트를 구성하는 각 단어는 대응되는 스코어 벡 터를 갖는데, 이 스코어 벡터의 크기는 9*9로 81일 것이다. 이항 딥러닝 모델은 라벨 결정 모델에서 출력되는 두 개의 연속한 라벨 간의 관계를 입력된 텍스트로 부터 추론한다. 모델 학습 단계에서, 이항 딥러닝 모델에 연결된 이항 라벨 결정 모델은 수학식 3과 같이 학습 텍스트에 대한 라벨 시퀀스 확률을 계산한다.수학식 3"}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수학식 3에서, X는 입력되는 학습 텍스트이고, Y는 학습 텍스트에 상응하는 출력으로서 하나의 라벨 시퀀스이고, 는 라벨 시퀀스에 대한 확률 값이다. 는 라벨 시퀀스를 확률 값으로 정규화하기 위해, 생성 가능한 모든 라벨 시퀀스들의 스코어들을 모두 더한 값이고, N은 텍스트 입력에 포함된 단어의 개수 이자 출력되는 라벨의 개수이다. 는 i-1번째 위치에서의 임의의 라벨과 i번째 위치의 임의의 라벨 의 상호 호환성에 대한 스코어이다. 상호 호환성이란, 해당 위치에서 임의의 라벨 쌍이 나타날 수 있는 가능성 을 의미한다. 한편 는 다음 수학식 4를 통해 계산될 수 있다. 수학식 4"}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "한편 스코어 벡터를 생성하는 연산 방법은 어느 하나로 제한되지 않는다. 예를 들어, 하단의 신경망 네트워크로 부터 출력된 벡터들을 기반으로 이웃하는 벡터들을 단순히 하나의 벡터로 이어 붙여서 구성하는 방법, 두 이웃 하는 벡터들을 서로 곱(Element-wise multiplication) 또는 아다마르 곱(Hadamard Product)하여 하나의 벡터로 형성하는 방법을 사용할 수 있다. 또한, 같은 크기를 갖는 두 이웃하는 벡터들의 차이에 절대값을 씌워서 하나의 벡터로 형성하는 방법을 이용할 수 있다. 이 경우, 수학식 3의 는 의 형태일 수 있다. 이때 는 Bilinear Model의 j번째 행 렬을 가리킨다. 한편 이항 딥러닝 모델에서 인접한 상태 벡터들 사이에서의 관계만을 고려하는 것으로 설명하였으나, 반드 시 이에 한정되는 것은 아니다. 도 6은 한 실시예에 따른 단항 딥러닝 모델과 이항 딥러닝 모델이 결합된 구조를 나타내는 도면이고, 도 7은 한 실시예에 따른 개체명 예측 모델의 구조도이다. 도 6을 참고하면, X는 단항 딥러닝 모델에 입력되는 단어이고, Y는 입력 텍스트를 구성하는 각 단어에 대 한 출력 라벨을 의미한다. 즉 는 시간 t-1에서 출력되는 라벨, 는 시간 t에서 출력되는 라벨을 의미한 다. 단항 딥러닝 모델은 라벨 결정 모델이 출력하는 개별 라벨에 연결되고, 이항 딥러닝 모델은 라 벨 결정 모델이 출력하는 인접한 라벨 사이에 연결된다. 도 7을 참고하면, 예측부는 학습된 단항 딥러닝 모델과 학습된 이항 딥러닝 모델을 하나의 라벨 결정 모델에서 통합하여 개체명 예측 모델을 생성한다. 개체명 예측 모델은 입력 텍스트에 대해 가장 적합한 하나의 라벨 시퀀스를 제공한다. 이때 단항 딥러닝 모델의 BiLSTM 레이어와 이항 딥러닝 모델의 BiLSTM 레이어의 각각의 파라미터들은 서로 통합될 수 있다. 개체명 예측 모델의 라벨 결정 모델은, 단항 딥러닝 모델의 출력 값과 이항 딥러닝 모델의 출력 값을 모두 이용하여 수학식 5를 통해 계산된 값을 제공할 수 있다. 수학식 5"}
{"patent_id": "10-2019-0177904", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "수학식 5에서, X는 입력되는 텍스트이고, Y는 출력되는 라벨 시퀀스이고, 는 단항 딥러닝 모델에 서 계산된 해당 라벨 시퀀스의 확률 값이고, 는 이항 딥러닝 모델에서 계산된 해당 라벨 시퀀스 의 확률 값이다. 한 예로서, 개체명 예측 모델은 단항 딥러닝 모델의 결과와 이항 딥러닝 모델의 결과를 곱하여 얻을 수 있다. 이 경우, 개체명 라벨을 예측하는 것은 곱한 값이 최대화되도록 하는 하나의 라벨 시퀀스를 얻어 내는 과정이다. 이때, 확률 값 자체가 아니라 확률 값을 최대화시키는 라벨 시퀀스를 구하는 것이 목적이므로, 확률 값을 위한 정규화 과정은 생략될 수 있다. 구체적으로, 비터비(Viterbi) 디코딩과 같은 디코딩 기법들이 사용될 수 있다. 도 8은 한 실시예에 따른 컴퓨팅 장치의 하드웨어 구성도이다. 도 8을 참고하면, 전처리부, 학습부 그리고 예측부는 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치에서, 본 발명의 동작을 실행하도록 기술된 명령들(instructions)이 포함된 프로그램을 실행한 다. 컴퓨팅 장치의 하드웨어는 적어도 하나의 프로세서, 메모리, 스토리지, 통신 인터페이스 를 포함할 수 있고, 버스를 통해 연결될 수 있다. 이외에도 입력 장치 및 출력 장치 등의 하드웨어가 포함 될 수 있다. 컴퓨팅 장치는 프로그램을 구동할 수 있는 운영 체제를 비롯한 각종 소프트웨어가 탑재될 수 있다. 프로세서는 컴퓨팅 장치의 동작을 제어하는 장치로서, 프로그램에 포함된 명령들을 처리하는 다양한 형태의 프로세서일 수 있고, 예를 들면, CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit) 등 일 수 있다. 메모리는 본 발명의 동작을 실행하도록 기술된 명령들이 프로세서에 의해 처리되도록 해당 프로그램을 로드한다. 메모리는 예를 들면, ROM(read only memory), RAM(random access memory) 등 일 수 있다. 스토리지는 본 발명의 동작을 실행하는데 요구되는 각종 데이터, 프로그램 등을 저장한다. 통신 인터페이스는 유/무선 통신 모듈일 수 있다. 본 발명의 개체명 인식 장치는 일반적인 뉴스 텍스트에서 사람의 이름, 조직, 회사 또는 기관의 이름, 지리적 명칭 등에 대한 개체명 인식에 활용될 수 있다. 또한, 본 발명의 개체명 인식 장치는 일반적인 분야를 다루는 뉴스 텍스트뿐만 아니라 각 분야에 특화된 텍스트 에서 활용될 수 있다. 특히 생물 및 의학 분야의 텍스트의 경우, 단백질, 유전자, 화학 성분, 질병, DNA, RNA의 이름이 개체명이 될 수 있다. 전문 분야의 개체명은 각 분야가 연구됨에 따라 새롭게 발견되거나 만들어지고, 개체명을 명명하는 규칙의 표준 이 없으며, 개체를 설명하거나 형용하는 형태로 명명되는 경우가 있다. 따라서 본 발명의 개체명 인식 장치는, 이웃하는 단어 간 문맥을 파악하여 개체명 및 라벨 시퀀스를 결정하므로, 전문 분야에서 복수의 단어로 연결된 개체명을 인식하는 경우에도 활용될 수 있다. 이상에서 설명한 본 발명의 실시예는 장치 및 방법을 통해서만 구현이 되는 것은 아니며, 본 발명의 실시예의 구성에 대응하는 기능을 실현하는 프로그램 또는 그 프로그램이 기록된 기록 매체를 통해 구현될 수도 있다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속하는 것이다."}
{"patent_id": "10-2019-0177904", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 한 실시예에 따른 개체명 인식 장치의 구조도이다. 도 2는 한 실시예에 따른 개체명 인식 결과의 예시도이다. 도 3은 한 실시예에 따른 개체명 인식 장치가 동작하는 방법의 흐름도이다. 도 4는 한 실시예에 따른 단항 딥러닝 모델과 이항 딥러닝 모델의 구조도이다. 도 5는 한 실시예에 따른 전이 행렬의 예시도이다. 도 6은 한 실시예에 따른 단항 딥러닝 모델과 이항 딥러닝 모델이 결합된 구조를 나타내는 도면이다. 도 7은 한 실시예에 따른 개체명 예측 모델의 구조도이다. 도 8은 한 실시예에 따른 컴퓨팅 장치의 하드웨어 구성도이다."}
