{"patent_id": "10-2023-0102889", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0021798", "출원번호": "10-2023-0102889", "발명의 명칭": "태스크 특화 지시문 프롬프트 중심의 한국어 대규모 언어 모델의 추론 능력을 평가하고 언어", "출원인": "동의대학교 산학협력단", "발명자": "이정훈"}}
{"patent_id": "10-2023-0102889", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "언어 모델 성능 향상 장치에 의한 언어 모델 성능 향상 방법에 있어서,범용 지시문 프롬프트를 수집하여 제1 지시문 프롬프트를 획득하는 단계;특정 태스크 말뭉치 데이터를 프롬프트 변환 규칙에 따라 변환하여 제2 지시문 프롬프트를 획득하는 단계; 및상기 제1 지시문 프롬프트 및 상기 제2 지시문 프롬프트를 사전 학습된 언어 모델에 입력하고 상기 사전 학습된언어 모델이 자연어 처리 결과를 출력하도록 재학습하는 단계를 포함하는, 언어 모델 성능 향상 방법."}
{"patent_id": "10-2023-0102889", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 프롬프트 변환 규칙은 특정 태스크에 대응하는 프롬프트 포맷, 입력 포맷, 및 응답 포맷이 트리플릿 구조로 매칭되어 저장되는, 언어 모델 성능 향상 방법."}
{"patent_id": "10-2023-0102889", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 사전 학습된 언어 모델은 트랜스포머 모델에서 디코더를 적용한 대화형 인공지능 챗봇 모델을 적용하고,상기 사전 학습된 언어 모델은 트랜스포머 모델의 각 레이어마다 학습 가능한 순위 분해 매트릭스를 추가하여학습되는, 언어 모델 성능 향상 방법."}
{"patent_id": "10-2023-0102889", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 재학습된 언어 모델의 자연어 처리 성능 평가 결과를 출력하는 단계를 더 포함하며,상기 자연어 처리 성능 평가 결과를 출력하는 단계는, 태스크 특화 지시문 프롬프트에 해당하는 상기 제2 지시문 프롬프트의 개수를 상이하게 설정하여 상기 재학습된 언어 모델을 평가하는, 언어 모델 성능 향상 방법."}
{"patent_id": "10-2023-0102889", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 제2 지시문 프롬프트를 획득하는 단계는,상기 특정 태스크 말뭉치 데이터를 복수의 화자에 의해 발화한 음성 데이터를 소리 데이터, 텍스트 데이터, 및이미지 데이터로 변환하고, 상기 소리 데이터, 상기 텍스트 데이터, 및 상기 이미지 데이터로부터 제1 분리 데이터 및 제2 분리 데이터를 분리하여 추출하고, 상기 추출된 제1 분리 데이터 또는 상기 추출된 제2 분리 데이터를 상기 프롬프트 변환 규칙에 따라 변환하는, 언어 모델 성능 향상 방법."}
{"patent_id": "10-2023-0102889", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "언어 모델 성능 향상 장치에 있어서,범용 지시문 프롬프트를 수집하여 제1 지시문 프롬프트를 획득하는 프롬프트 획득부;특정 태스크 말뭉치 데이터를 프롬프트 변환 규칙에 따라 변환하여 제2 지시문 프롬프트를 획득하는 프롬프트변환부; 상기 제1 지시문 프롬프트 및 상기 제2 지시문 프롬프트를 사전 학습된 언어 모델에 입력하고 상기 사전 학습된언어 모델이 자연어 처리 결과를 출력하도록 재학습하는 모델 학습부; 및공개특허 10-2025-0021798-3-상기 재학습된 언어 모델의 자연어 처리 성능 평가 결과를 출력하는 모델 평가부를 포함하는, 언어 모델 성능향상 장치."}
{"patent_id": "10-2023-0102889", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 기재된 언어 모델 성능 향상 방법을 수행하는 프로그램이 기록된 컴퓨터 판독 가능한 기록 매체."}
{"patent_id": "10-2023-0102889", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 기재된 언어 모델 성능 향상 방법을 수행하기 위해 컴퓨터 판독 가능한 기록 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0102889", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "언어 모델 성능 향상 방법 및 장치를 제시한다. 언어 모델 성능 향상 방법은 범용 지시문 프롬프트를 수집하여 제1 지시문 프롬프트를 획득하는 단계, 특정 태스크 말뭉치 데이터를 프롬프트 변환 규칙에 따라 변환하여 제2 지시문 프롬프트를 획득하는 단계, 및 상기 제1 지시문 프롬프트 및 상기 제2 지시문 프롬프트를 사전 학습된 언 어 모델에 입력하고 상기 사전 학습된 언어 모델이 자연어 처리 결과를 출력하도록 재학습하는 단계를 포함한다."}
{"patent_id": "10-2023-0102889", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명이 속하는 기술 분야는 언어 모델 성능 향상 방법 및 장치에 관한 것으로, 태스크 특화 지시문 프롬프트 중심의 한국어 대규모 언어 모델의 추론 능력을 평가하는 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0102889", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이 부분에 기술된 내용은 단순히 본 실시예에 대한 배경 정보를 제공할 뿐 종래기술을 구성하는 것은 아니다. 자연어 처리 기술은 사전 학습 언어 모델의 등장과 함께 큰 발전을 이루었다. 이전 연구는 사전 학습된 언어 모 델에 태스크에 적합한 예측 레이어를 추가하고 파인 튜닝(Fine-tuning)하는 방법이 주를 이루었다. 이후 모델의 파라미터 개수가 많아진 대규모 언어 모델이 공개되면서 다양한 도메인 및 언어에서 일반화 능력이 향상되면서 GPT(Generative Pre-trained Transformer)와 같은 생성형 모델에 관한 관심이 커지게 되었다. 생성형 모델의 학습에는 모델에게 답을 얻기 위해 질문하는 방법인 프롬프트의 구성 방법에 따라 동일한 모델과 동일한 태스크를 적용했다고 하더라도 성능이 크게 달라질 수 있다. InstructGPT은 이러한 한계를 극복하기 위해서 질문에 있는 명확한 지시대로 출력을 얻기 위해서 지시 프롬프트 와 결과물로 먼저 학습을 하고, 출력 결과에 대한 사람의 선호도를 예측하는 모델을 추가해서 강화학습을 통해 파인 튜닝하는 RLHF(Reinforcement Learning with Human Feedback) 방법을 적용해서 주어진 지시문에 더 맞게 행동할 수 있는 모델 학습을 가능하게 하였다. 지시문 기반 프롬프트를 이용한 생성 모델 학습은 기존의 프롬프트 기반의 학습 방법에 비해서 좋은 결과를 보 였지만, 지시문 데이터를 어떻게 생성하는지에 따라 성능이 크게 달라질 수 있으며 다양한 도메인의 태스크에서 잘 작동하지만 특정한 태스크에 특화된 지시문에서는 성능이 하락하는 문제가 있다. 또한 AIHub, 모두의 말뭉치 등 다양한 데이터 배포 플랫폼에서 다양한 자연어처리 말뭉치를 배포하고 있다. 하지만 특정한 태스크에 특화된 해당 말뭉치 데이터를 지시문 프롬프트처럼 바로 적용했을 때 일반적인 지시문 처리 성능 결과가 하락할 수 있 는 등 예상치 못한 결과가 발생할 수 있다. 특허문헌 1은 복합 태스크 기계 학습 시스템에 관한 발명이고, 특허문헌 2는 언어 모델을 이용한 개방형 도메인 대화 모델 구축을 위한 방법에 관한 발명으로, 일반적인 프롬프트를 처리하는 기술을 기재하나, 특정 태스크에 대한 지시문 기반 프롬프트를 고려하지 아니한다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국공개특허공보 제10-2022-0024147호 (2022.03.03) (특허문헌 0002) 한국공개특허공보 제10-2023-0071673호 (2023.05.23)"}
{"patent_id": "10-2023-0102889", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2023-0102889", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 실시예들은 특정한 태스크에 특화된 말뭉치를 기존의 지시문 프롬프트와 학습할 수 있도록 변형하고, 지시문 프롬프트와 함께 학습에 적용하여, 태스크에 특화된 프롬프트가 추가되었을 때, 지시문 프롬프트를 일반 화하여 처리할 수 있는지 또는 다양한 태스크를 동시에 처리할 수 있는지에 관한 학습된 언어 모델 평가를 통해 태스크에 학습된 프롬프트의 영향을 개선하는데 주된 목적이 있다. 본 발명의 명시되지 않은 또 다른 목적들은 하기의 상세한 설명 및 그 효과로부터 용이하게 추론할 수 있는 범 위 내에서 추가적으로 고려될 수 있다."}
{"patent_id": "10-2023-0102889", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 실시예의 일 측면에 의하면, 언어 모델 성능 향상 방법은 범용 지시문 프롬프트를 수집하여 제1 지시문 프롬 프트를 획득하는 단계, 특정 태스크 말뭉치 데이터를 프롬프트 변환 규칙에 따라 변환하여 제2 지시문 프롬프트 를 획득하는 단계, 및 상기 제1 지시문 프롬프트 및 상기 제2 지시문 프롬프트를 사전 학습된 언어 모델에 입력 하고 상기 사전 학습된 언어 모델이 자연어 처리 결과를 출력하도록 재학습하는 단계를 포함한다. 상기 프롬프트 변환 규칙은 특정 태스크에 대응하는 프롬프트 포맷, 입력 포맷, 및 응답 포맷이 트리플릿 구조 로 매칭되어 저장될 수 있다. 상기 사전 학습된 언어 모델은 트랜스포머 모델에서 디코더를 적용한 대화형 인공지능 챗봇 모델을 적용하고, 상기 사전 학습된 언어 모델은 트랜스포머 모델의 각 레이어마다 학습 가능한 순위 분해 매트릭스를 추가하여 학습될 수 있다. 상기 언어 모델 성능 향상 방법은 상기 재학습된 언어 모델의 자연어 처리 성능 평가 결과를 출력하는 단계를 더 포함할 수 있다. 상기 자연어 처리 성능 평가 결과를 출력하는 단계는, 태스크 특화 지시문 프롬프트에 해당하는 상기 제2 지시 문 프롬프트의 개수를 상이하게 설정하여 상기 재학습된 언어 모델을 평가할 수 있다. 상기 제2 지시문 프롬프트를 획득하는 단계는, 상기 특정 태스크 말뭉치 데이터를 복수의 화자에 의해 발화한 음성 데이터를 소리 데이터, 텍스트 데이터, 및 이미지 데이터로 변환하고, 상기 소리 데이터, 상기 텍스트 데 이터, 및 상기 이미지 데이터로부터 제1 분리 데이터 및 제2 분리 데이터를 분리하여 추출하고, 상기 추출된 제 1 분리 데이터 또는 상기 추출된 제2 분리 데이터를 상기 프롬프트 변환 규칙에 따라 변환할 수 있다. 본 실시예의 다른 측면에 의하면, 언어 모델 성능 향상 장치는 범용 지시문 프롬프트를 수집하여 제1 지시문 프 롬프트를 획득하는 프롬프트 획득부, 특정 태스크 말뭉치 데이터를 프롬프트 변환 규칙에 따라 변환하여 제2 지 시문 프롬프트를 획득하는 프롬프트 변환부, 상기 제1 지시문 프롬프트 및 상기 제2 지시문 프롬프트를 사전 학 습된 언어 모델에 입력하고 상기 사전 학습된 언어 모델이 자연어 처리 결과를 출력하도록 재학습하는 모델 학 습부, 및 상기 재학습된 언어 모델의 자연어 처리 성능 평가 결과를 출력하는 모델 평가부를 포함한다. 본 실시예의 또 다른 측면에 의하면, 기록 매체는 언어 모델 성능 향상 방법을 수행하는 프로그램이 기록된 컴 퓨터 판독 가능한 기록 매체이다. 본 실시예의 또 다른 측면에 의하면, 컴퓨터 프로그램은 언어 모델 성능 향상 방법을 수행하기 위해 컴퓨터 판 독 가능한 기록 매체에 저장된 컴퓨터 프로그램이다."}
{"patent_id": "10-2023-0102889", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상에서 설명한 바와 같이 본 발명의 실시예들에 의하면 대규모 언어 모델을 지시문 프롬프트에 파인 튜닝할 때 해당 프롬프트에 대한 성능을 떨어뜨리지 않으면서 특정 태스크를 수행하거나 또는 특정 태스크의 말뭉치를 통해서 모형의 전체적인 범용 지시문에 대한 처리 능력을 향상시킬 수 있는 효과가 있다. 본 발명의 실시예들에 의하면 특정 태스크의 말뭉치에 대해서 복수의 화자에 의해 음성 데이터가 혼재된 상황에 서 화자별로 말뭉치에 대한 데이터를 분리하여 지시문 프롬프트 기반의 언어 모델을 학습시킬 수 있는 효과가 있다. 여기에서 명시적으로 언급되지 않은 효과라 하더라도, 본 발명의 기술적 특징에 의해 기대되는 이하의 명세서에 서 기재된 효과 및 용이하게 추론 가능한 다른 효과는 본 발명의 명세서에 기재된 것과 같이 취급된다."}
{"patent_id": "10-2023-0102889", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명을 설명함에 있어서 관련된 공지기능에 대하여 이 분야의 기술자에게 자명한 사항으로서 본 발명 의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명을 생략하고, 본 발명의 일부 실시예들 을 예시적인 도면을 통해 상세하게 설명한다. 범용 지시문 프롬프트를 대규모 언어 모델에 파인튜닝을 수행했을 때, 학습되지 않은 다양한 태스크에 적응하면 서 일반화된 뛰어난 성능을 보였다. 범용 지시문 프롬프트에 대한 파인튜닝만으로는 특정한 태스크에 특화된 프 롬프트를 처리하는 능력이 부족할 수 있으며, 너무 특정한 태스크에 특화된 프롬프트를 학습 데이터로 많이 추 가하는 것 역시 기존의 범용적인 지시문에 대한 처리 능력을 떨어뜨릴 수 있다. 본 실시예에 따른 언어 모델 성능 향상 장치는 다양한 데이터 플랫폼에서 존재하는 자연어 처리 태스크 말뭉치 를 범용 지시문 프롬프트의 형태로 변환하고 범용 지시문 프롬프트와 함께 대규모 언어모형의 학습 데이터로 사 용했을 때 언어 모델의 추론 능력 및 각 태스크에 대한 성능 변화를 평가하고, 범용 지시문 프롬프트만으로는 특정 태스크의 성능에 대한 보장이 어렵지만, 특정 태스크에 대한 말뭉치를 범용 지시문의 형태에 최대한 맞춰 서 변환하고 학습 데이터로 추가하였을 때 많은 데이터를 사용하지 않더라도 기존의 언어 모델의 지시문 처리 능력을 해치지 않으면서 태스크에 대한 성능을 향상시킨다. 도 1은 본 발명의 일 실시예에 따른 언어 모델 성능 향상 장치를 예시한 도면이다. 본 실시예에 따른 언어 모델 성능 향상 장치가 모델을 학습하기 위해 사용한 데이터는 다양한 도메인과 다양한 지시에 관련된 프롬프트 구성되어 있는 범용 지시문 프롬프트와 특정 자연어처리 태스크 말뭉치를 변형시킨 태 스크 특화 프롬프트의 두 가지 데이터를 사용한다. 도 1을 참조하면, 언어 모델 성능 향상 장치는 프롬프트 획득부, 프롬프트 변환부, 모델 학습부 , 및 모델 평가부를 포함학고, 각 구성은 상호 연결될 수 있다. 프롬프트 변환부는 음성 처리부 를 포함할 수 있다. 프롬프트 획득부는 범용 지시문 프롬프트를 수집하여 제1 지시문 프롬프트를 획득할 수 있다. 프롬프트는 인공지능(AI) 등의 모델에 입력하는 명령어를 의미하고, 지시문 프롬프트는 프롬프트의 형식이 지시문으로 이루 어진 명령어이다. 범용 지시문 프롬프트는 특정 분야 등에 한정하지 않고 사용 가능하도록 형식이 일반화된 지 시문 프롬프트이다. 프롬프트 변환부는 특정 태스크 말뭉치 데이터를 프롬프트 변환 규칙에 따라 변환하여 제2 지시문 프롬프 트를 획득할 수 있다. 프롬프트 변환 규칙은 특정 태스크에 대응하는 프롬프트 포맷, 입력 포맷, 및 응답 포맷 이 트리플릿(triplet) 구조로 매칭되어 저장될 수 있다. 모델 학습부는 제1 지시문 프롬프트 및 제2 지시문 프롬프트를 사전 학습된 언어 모델에 입력하고 사전 학 습된 언어 모델이 자연어 처리 결과를 출력하도록 재학습할 수 있다. 사전 학습된 언어 모델은 트랜스포머 모델 에서 디코더를 적용한 대화형 인공지능 챗봇 모델을 적용할 수 있고, 사전 학습된 언어 모델은 트랜스포머 모델 의 각 레이어마다 학습 가능한 순위 분해(rank decomposition) 매트릭스를 추가하여 학습될 수 있다.모델 평가부는 재학습된 언어 모델의 자연어 처리 성능 평가 결과를 출력할 수 있다. 모델 평가부는 태스크 특화 지시문 프롬프트에 해당하는 제2 지시문 프롬프트의 개수를 상이하게 설정하여 재학습된 언어 모델 을 평가할 수 있다. 도 2는 본 발명의 일 실시예에 따른 언어 모델 성능 향상 장치에 적용된 언어 모델의 입력 변환을 예시한 도면 이다. 언어 모델은 네트워크 모델을 적용하며, 네트워크 모델은 특징을 추출하고 특징을 데이터 가공 처리한다. 네트 워크 모델은 다수의 레이어가 네트워크로 연결되며 히든 레이어를 포함한다. 레이어는 파라미터를 포함할 수 있 고, 파라미터는 노드 간의 가중치 및/또는 바이어스를 포함한다. 언어 모델은 미리 정의된 손실 함수를 최소화 하도록 파라미터를 학습한다. 학습을 위한 언어 모델은 트랜스포머 모델 구조에서 디코더를 이용하는 GPT 기반의 모델을 이용할 수 있다. 예 컨대, GPT-NeoX를 사용할 수 있다. 대규모 언어 모델을 학습 및 생성에 이용하는 경우, 큰 파라미터를 가지는 모델의 가중치를 전부 학습하는 것은 많은 학습 시간과 GPU 메모리를 요구하게 된다. 사용 가능한 GPU 메모리 한계 내에서 테스트를 진행하기 위해서 전체 파라미터를 전부 업데이트하는 대신 모델이 기존에 가지고 있던 파라미터는 고정하고, 트랜스포머의 각 레 이어마다 학습 가능한 순위 분해(rank decomposition) 매트릭스를 추가하고 해당 파라미터를 학습하는 모델(예 컨대, LoRA)을 적용할 수 있다. 댜양한 태스크 특화 말뭉치에 대해서 지시문 프롬프트 변환 동작을 수행할 때 프롬프트 변환 규칙을 기준으로 프롬프트, 입력, 응답(출력) 형식으로 변환한다. 도 3 내지 도 5는 본 발명의 일 실시예에 따른 언어 모델 성능 향상 장치에 저장된 프롬프트 변환 규칙을 예시 한 도면이다. 프롬프트 변환부는 특정 태스크 말뭉치 데이터를 프롬프트 변환 규칙에 따라 변환하여 제2 지시문 프롬프 트를 획득할 수 있다. 프롬프트 변환 규칙은 특정 태스크에 대응하는 프롬프트 포맷, 입력 포맷, 및 응답 포맷 이 트리플릿(triplet) 구조로 매칭되어 저장될 수 있다. 지시문 프롬프트는 ko-alpaca에서 적용했던 지시문 프롬프트 형식을 이용할 수 있다. 입력이 있는 프롬프트와 입력 없이 지시문만 존재하는 프롬프트로 구분하여 프롬프트를 생성할 수 있다. 도 3 내지 도 5는 각 자연어 태스크별로 지시문 프롬프트와 유사한 형태로 데이터를 변환하기 위해 적용한 규칙 을 나타낸다. 도 3 내지 도 5에서 {민원 질문}와 같이 괄호로 묶여 있는 부분은 해당 말뭉치에서 제공되는 특수한 입력 혹은 태깅된 레이블 텍스트를 나타낸다. 한 가지 형태의 지시문 프롬프트나 출력을 보여주고 있지만, 테스트에서는 형태의 다양성을 주기 위해서 각 프롬프트 또는 출력 당 최소 5개 이상의 형태를 가지도록 설정할 수 있다. 트 리플릿 구조에서 항목(예컨대, 입력)이 생략되면 NULL 등의 초기값으로 설정할 수 있다. 도 6은 본 발명의 일 실시예에 따른 언어 모델 성능 향상 장치의 성능 평가 결과를 예시한 도면이다. 모델 평가부는 재학습된 언어 모델의 자연어 처리 성능 평가 결과를 출력할 수 있다. 모델 평가부는 태스크 특화 지시문 프롬프트에 해당하는 제2 지시문 프롬프트의 개수를 상이하게 설정하여 재학습된 언어 모델 을 평가할 수 있다. 사전 학습 언어 모델로 EleutherAI에서 공개한 한국어 GPT-NeoX 모형인 polyglot을 이용할 수 있다. polyglot 은 1.3b, 3.8b, 5.8b, 12.8b의 다양한 크기의 모형을 제공하고, RTX3090 4개를 사용하는 환경에서 원활한 진행 을 위해서 3.8b 크기의 모형을 선택하여 테스트를 진행할 수 있다. 학습 파라미터에 관한 정보는 epoch는 2, learning rate는 0.001, fp16은 True, RoLA rank는 16, RoLA alpha는 0.05로 설정할 수 있다. 태스크 특화 말뭉치를 지시문 프롬프트 데이터로 변환하여 기존의 데이터와 함께 학습했을 때 발생하는 성능의 변화를 측정하기 위해서 태스크 특화 지시문 프롬프트의 개수를 달리하여 모델을 학습하고 비교하는 테스트를 진행할 수 있다. 하였다. 도 6은 학습 모델의 각 태스크에 대한 성능을 나타내며, 0.3K, 3K, 10K, 15K는 태스크 특화 지시문 입력 데이터 가 학습에 사용된 개수를 의미하며, only는 범용 지시문 프롬프트를 학습 데이터에 사용하지 않은 모델을 의미한다. 대부분의 태스크에서는 태스크에 특화된 지시문 프롬프트로 학습하지 않았을 때, 낮은 성능을 보였으며 3k 모델 과 같이 적당한 개수의 태스크 특화 지시문 프롬프트를 학습 데이터에 추가했을 때 안정적인 성능을 보였다. 번 역과 같이 데이터 추가 전에는 거의 태스크 수행이 불가능했던 경우에도 0.3K와 같이 아주 적은 개수의 데이터 만 추가해도 태스크 수행이 가능해진 것을 확인할 수 있다. 범용 지시문 프롬프트가 포함되지 않는다고 해서 태 스크에서의 성능이 크게 개선되는 것은 확인되지 않았으며, 적은 개수의 태스크 특화 프롬프트를 추가했을 때 태스크에 대한 성능이 일정 부분 보장되면서 기존의 범용 지시문에 대한 처리 능력도 크게 영향을 받지 않거나 오히려 더 향상되는 것을 확인할 수 있다. 도 7은 본 발명의 일 실시예에 따른 언어 모델 성능 향상 장치의 음성 처리부의 동작을 예시한 도면이다. 프롬프트 변환부의 음성 처리부는 제2 지시문 프롬프트를 획득하는 동작을 수행할 때, 특정 태스크 말뭉치 데이터를 복수의 화자에 의해 발화한 음성 데이터를 소리 데이터, 텍스트 데이터, 및 이미지 데이터로 변환하고, 소리 데이터, 상기 텍스트 데이터, 및 이미지 데이터로부터 제1 분리 데이터 및 제2 분리 데이터를 분리하여 추출하고, 추출된 제1 분리 데이터 또는 추출된 제2 분리 데이터를 프롬프트 변환 규칙에 따라 변환할 수 있다. 음성 처리부는 음성 데이터에는 복수의 화자 소리가 혼재하고, 이러한 음성 데이터를 소리 주파수로 구성 된 소리 데이터, 특정 언어의 기본 소리 단위(예컨대, 한국어 자모, 영어 알파벳)를 기준으로 타이핑 변환된 텍 스트 데이터, 주파수 또는 텍스트에 다양한 색상을 매칭시키고 시계열적으로 이어 붙여서 2차원의 이미지로 변 환된 이미지 데이터로 변환할 수 있다. 음성 처리부가 변환한 유형 데이터는 소리 데이터, 텍스트 데이터, 및 이미지 데이터를 포함한다. 음성 처리부는 제1 화자의 발화 패턴(예컨대, 습관, 억양, 어투, 반복 단어, 호흡, 속도)을 기준으로 소리 데이터의 특정 위치에 가상 주파수(예컨대, 가청 주파수의 저음 영역)를 추가하여 제1 변환 소리 데이터로 변환 한다. 음성 처리부는 제1 화자의 발화 패턴을 기준으로 텍스트 데이터의 특정 위치에 가상 글자(예컨대, 습관적 사용 단어, 스페이스)를 추가하여 제1 변환 텍스트 데이터로 변환한다. 음성 처리부는 제1 화자의 발화 패턴을 기준으로 이미지 데이터의 특정 위치에 가상 픽셀(예컨대, 대응하는 억양을 특정 색상으로 변환)을 추가하여 제1 변환 이미지 데이터로 변환한다. 음성 처리부가 변환한 제1 변환 데이터는 제1 변환 소리 데이터(Data1freq), 제1 변환 텍스트 데이터 (Data1text), 및 제1 변환 이미지 데이터(Data1imag)를 포함한다. 음성 처리부는 제2 화자의 발화 패턴(예컨대, 습관, 억양, 어투, 반복 단어, 호흡, 속도)을 기준으로 소리 데이터의 특정 위치에 가상 주파수(예컨대, 가청 주파수의 고음 영역)를 추가하여 제2 변환 소리 데이터로 변환 한다. 음성 처리부는 제2 화자의 발화 패턴을 기준으로 텍스트 데이터의 특정 위치에 가상 글자(예컨대, 습관적 사용 단어, 스페이스)를 추가하여 제2 변환 텍스트 데이터로 변환한다. 음성 처리부는 제2 화자의 발화 패턴을 기준으로 이미지 데이터의 특정 위치에 가상 픽셀(예컨대, 대응하는 억양을 특정 색상으로 변환)을 추가하여 제2 변환 이미지 데이터로 변환한다. 음성 처리부가 변환한 제2 변환 데이터는 제2 변환 소리 데이터(Data2freq), 제2 변환 텍스트 데이터 (Data2text), 및 제2 변환 이미지 데이터(Data2imag)를 포함한다. 음성 처리부는 제1 변환 데이터에 제1 화자의 발화 패턴에 대한 제1 가중치(스케일링 비율)를 적용하고, 제2 변환 데이터에 제2 화자의 발화 패턴에 대한 제2 가중치(스케일링 비율)를 적용하고, 제1 가중치와 제2 가 중치를 변경해가며 제1 가중치를 적용한 제1 변환 데이터 및 제2 가중치를 적용한 제2 변환 데이터 간에 중첩된 분포(distribution)가 분산하는 방향으로 제1 가중치와 제2 가중치를 도출한다.수학식 1"}
{"patent_id": "10-2023-0102889", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "즉, 제1 가중치를 적용한 제1 변환 소리 데이터 및 제2 가중치를 적용한 제2 변환 소리 데이터 간에 중첩된 영 역에서 주파수 분리 영역이 증가하고, 제1 가중치를 적용한 제1 변환 텍스트 데이터 및 제2 가중치를 적용한 제 2 변환 텍스트 데이터 간에 중첩된 영역에서 인식 가능한 글자 구분 영역이 증가하고, 제1 가중치를 적용한 제1 변환 이미지 데이터 및 제2 가중치를 적용한 제2 변환 이미지 데이터 간에 중첩된 영역에서 시각적 색상 구분 영역이 증가하도록, 제1 가중치 및 제2 가중치를 변경한다. 복수의 데이터를 중첩하기 위해서 데이터 형식을 대응하는 데이터 구조로 매칭할 필요가 있다. 미리 설정된 제1 매칭 테이블(M1)을 통해 제1 변환 소리 데이터(Data1freq) 및 제2 변환 소리 데이터(Data2freq)를 특정 데이터 구조로 변환한다. 예컨대, 특정 데이터 구조는 소리 데이터, 텍스트 데이터, 이미지 데이터, 또는 이들의 조합으로 정의된 데이터 구조로 매칭되도록 데이터 변환 매트릭스가 적용될 수 있다. 데이터 변환 매트 릭스는 소리 데이터, 텍스트 데이터, 이미지 데이터 간에 변환되도록 요소들이 미리 학습될 수 있다. 미리 설정된 제2 매칭 테이블(M2)을 통해 제1 변환 텍스트 데이터(Data1text) 및 제2 변환 텍스트 데이터 (Data2text)를 특정 데이터 구조로 변환한다. 예컨대, 특정 데이터 구조는 소리 데이터, 텍스트 데이터, 이미지 데이터, 또는 이들의 조합으로 정의된 데이터 구조로 매칭되도록 데이터 변환 매트릭스가 적용될 수 있다. 데이 터 변환 매트릭스는 소리 데이터, 텍스트 데이터, 이미지 데이터 간에 변환되도록 요소들이 미리 학습될 수 있 다. 미리 설정된 제3 매칭 테이블(M3)을 통해 제1 변환 이미지 데이터(Data1imag) 및 제2 변환 이미지 데이터 (Data2imag)를 특정 데이터 구조로 변환한다. 예컨대, 특정 데이터 구조는 소리 데이터, 텍스트 데이터, 이미지 데이터, 또는 이들의 조합으로 정의된 데이터 구조로 매칭되도록 데이터 변환 매트릭스가 적용될 수 있다. 데이 터 변환 매트릭스는 소리 데이터, 텍스트 데이터, 이미지 데이터 간에 변환되도록 요소들이 미리 학습될 수 있 다. 음성 처리부는 제1 가중치와 제2 가중치를 도출하는 과정에서 중첩된 분포를 분산한 결과가 기준치를 만족 하지 않으면, 제1 가중치를 적용한 제1 변환 데이터 및 제2 가중치를 적용한 제2 변환 데이터를 초기 음성 데이 터 대신 입력하여 유형 데이터로 재분류하여 제1 변환 데이터와 제2 변환 데이터를 생성하는 과정을 다시 진행 한다. 도 8은 본 발명의 다른 실시예에 따른 언어 모델 성능 향상 방법을 예시한 흐름도이다. 도 8은 본 발명의 다른 실시예에 따른 언어 모델 성능 향상 방법을 예시한 흐름도이다. 언어 모델 성능 향상 방 법은 언어 모델 성능 향상 장치에 의해 수행될 수 있으며, 언어 모델 성능 향상 장치가 수행하는 각각의 동작을 개별적으로 수행하거나 동작들을 복합적으로 수행할 수 있다. S810에서는 범용 지시문 프롬프트를 수집하여 제1 지시문 프롬프트를 획득하는 단계를 수행한다. S820에서는 특정 태스크 말뭉치 데이터를 프롬프트 변환 규칙에 따라 변환하여 제2 지시문 프롬프트를 획득하는 단계를 수행한다. 프롬프트 변환 규칙은 특정 태스크에 대응하는 프롬프트 포맷, 입력 포맷, 및 응답 포맷이 트 리플릿 구조로 매칭되어 저장될 수 있다. S830에서는 제1 지시문 프롬프트 및 제2 지시문 프롬프트를 사전 학습된 언어 모델에 입력하고 사전 학습된 언 어 모델이 자연어 처리 결과를 출력하도록 재학습하는 단계를 수행한다. 사전 학습된 언어 모델은 트랜스포머 모델에서 디코더를 적용한 대화형 인공지능 챗봇 모델을 적용하고, 사전 학습된 언어 모델은 트랜스포머 모델의 각 레이어마다 학습 가능한 순위 분해 매트릭스를 추가하여 학습될 수 있다. S840에서는 재학습된 언어 모델의 자연어 처리 성능 평가 결과를 출력하는 단계를 수행한다. 자연어 처리 성능 평가 결과를 출력하는 단계(S840)는, 태스크 특화 지시문 프롬프트에 해당하는 제2 지시문 프롬프트의 개수를상이하게 설정하여 재학습된 언어 모델을 평가할 수 있다. 제2 지시문 프롬프트를 획득하는 단계(S820)는, 특정 태스크 말뭉치 데이터를 복수의 화자에 의해 발화한 음성 데이터를 소리 데이터, 텍스트 데이터, 및 이미지 데이터로 변환하고, 소리 데이터, 텍스트 데이터, 및 이미지 데이터로부터 제1 분리 데이터 및 제2 분리 데이터를 분리하여 추출하고, 추출된 제1 분리 데이터 또는 추출된 제2 분리 데이터를 프롬프트 변환 규칙에 따라 변환할 수 있다. 언어 모델 성능 향상 방법은 대규모 언어 모델을 지시문 프롬프트에 파인튜닝할 때 해당 프롬프트에 대한 성능 을 떨어뜨리지 않으면서 특정 태스크를 수행하거나 혹은 특정 태스크의 말뭉치를 통해서 모델의 전체적인 범용 지시문에 대한 처리 능력을 향상시킬 수 있다. 언어 모델 성능 향상 방법은 적은 개수의 데이터 추가만으로도 다양한 태스크에서 기존 범용 지시문만으로 학습 된 모형에 비해서 향상된 결과를 보이는 것을 확인할 수 있고, 기존의 범용 지시문에 대한 처리 능력도 일부분 에서는 오히려 향상되는 것을 확인할 수 있다. 언어 모델 성능 향상 방법은 생성 모델을 이용하여 지시문과 출력문의 형태를 다양하게 생성하여 학습 과정에서 과적합될 수 있는 부분을 개선할 수 있다. 도 9는 본 발명의 또 다른 실시예에 따른 언어 모델 성능 향상 장치를 예시한 블록도이다. 언어 모델 성능 향상 장치는 처리부, 저장부, 통신 버스를 포함한다. 처리부는 도 1을 참조하여 설명한 프롬프트 획득부, 프롬프트 변환부, 음성 처리부, 모델 학습부, 및 모델 평가부의 동작을 수행할 수 있다. 처리부는 언어 모델 성능 향상 장치를 동작하도록 제어할 수 있다. 예컨대, 처리부는 저장부 에 저장된 하나 이상의 프로그램들을 실행할 수 있다. 하나 이상의 프로그램들은 하나 이상의 컴퓨터 실행 가능 명령어를 포함할 수 있으며, 컴퓨터 실행 가능 명령어는 처리부에 의해 실행되는 경우 언어 모델 성 능 향상 장치로 하여금 예시적인 실시예에 따른 동작들을 수행하도록 구성될 수 있다. 저장부는 컴퓨터 실행 가능 명령어 내지 프로그램 코드, 프로그램 데이터 및/또는 다른 적합한 형태의 정 보를 저장하도록 구성된다. 컴퓨터 실행 가능 명령어 내지 프로그램 코드, 프로그램 데이터 및/또는 다른 적합 한 형태의 정보는 입출력 인터페이스나 통신 인터페이스를 통해서도 주어질 수 있다. 저장부에 저장된 프로그램은 처리부에 의해 실행 가능한 명령어의 집합을 포함한다. 일 실시예에서, 저장부 는 메모리(랜덤 액세스 메모리와 같은 휘발성 메모리, 비휘발성 메모리, 또는 이들의 적절한 조합), 하나 이상의 자기 디스크 저장 디바이스들, 광학 디스크 저장 디바이스들, 플래시 메모리 디바이스들, 그 밖에 언어 모델 성능 향상 장치에 의해 액세스되고 원하는 정보를 저장할 수 있는 다른 형태의 저장 매체, 또는 이들 의 적합한 조합일 수 있다. 통신 버스는 언어 모델 성능 향상 장치의 다른 다양한 컴포넌트들을 상호 연결한다. 언어 모델 성능 향상 장치는 또한 하나 이상의 입출력 장치를 위한 인터페이스를 제공하는 하나 이상의 입 출력 인터페이스 및 하나 이상의 통신 인터페이스를 포함할 수 있다. 입출력 인터페이스 및 통 신 인터페이스는 통신 버스에 연결된다. 입출력 장치(미도시)는 입출력 인터페이스를 통해 언어 모델 성능 향상 장치의 다른 컴포넌트들에 연결될 수 있다. 처리부는 범용 지시문 프롬프트를 수집하여 제1 지시문 프롬프트를 획득하고, 특정 태스크 말뭉치 데이터 를 프롬프트 변환 규칙에 따라 변환하여 제2 지시문 프롬프트를 획득하고, 제1 지시문 프롬프트 및 제2 지시문 프롬프트를 사전 학습된 언어 모델에 입력하고 사전 학습된 언어 모델이 자연어 처리 결과를 출력하도록 재학습 한다. 언어 모델 성능 향상 장치는 하드웨어, 펌웨어, 소프트웨어 또는 이들의 조합에 의해 로직회로 내에서 구현될 수 있고, 범용 또는 특정 목적 컴퓨터를 이용하여 구현될 수도 있다. 장치는 고정배선형(Hardwired) 기기, 필드 프로그램 가능한 게이트 어레이(Field Programmable Gate Array, FPGA), 주문형 반도체(Application Specific Integrated Circuit, ASIC) 등을 이용하여 구현될 수 있다. 또한, 장치는 하나 이상의 프로세서 및 컨트롤러를 포함한 시스템온칩(System on Chip, SoC)으로 구현될 수 있다. 언어 모델 성능 향상 장치는 하드웨어적 요소가 마련된 컴퓨팅 디바이스 또는 서버에 소프트웨어, 하드웨어, 또 는 이들의 조합하는 형태로 탑재될 수 있다. 컴퓨팅 디바이스 또는 서버는 각종 기기 또는 유무선 통신망과 통신을 수행하기 위한 통신 모뎀 등의 통신장치, 프로그램을 실행하기 위한 데이터를 저장하는 메모리, 프로그램 을 실행하여 연산 및 명령하기 위한 마이크로프로세서 등을 전부 또는 일부 포함한 다양한 장치를 의미할 수 있 다. 도 8에서는 각각의 과정을 순차적으로 실행하는 것으로 기재하고 있으나 이는 예시적으로 설명한 것에 불과하고, 이 분야의 기술자라면 본 발명의 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 도 8에 기재된 순서를 변경하여 실행하거나 또는 하나 이상의 과정을 병렬적으로 실행하거나 다른 과정을 추가하는 것으로 다 양하게 수정 및 변형하여 적용 가능할 것이다. 본 실시예들에 따른 동작은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨 터 판독 가능한 매체에 기록될 수 있다. 컴퓨터 판독 가능한 매체는 실행을 위해 프로세서에 명령어를 제공하는 데 참여한 임의의 매체를 나타낸다. 컴퓨터 판독 가능한 매체는 프로그램 명령, 데이터 파일, 데이터 구조 또는 이들의 조합을 포함할 수 있다. 예를 들면, 자기 매체, 광기록 매체, 메모리 등이 있을 수 있다. 컴퓨터 프로그 램은 네트워크로 연결된 컴퓨터 시스템 상에 분산되어 분산 방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수도 있다. 본 실시예를 구현하기 위한 기능적인(Functional) 프로그램, 코드, 및 코드 세그먼트들은 본"}
{"patent_id": "10-2023-0102889", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "실시예가 속하는 기술분야의 프로그래머들에 의해 용이하게 추론될 수 있을 것이다. 본 실시예들은 본 실시예의 기술 사상을 설명하기 위한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상 의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2023-0102889", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 언어 모델 성능 향상 장치를 예시한 도면이다. 도 2는 본 발명의 일 실시예에 따른 언어 모델 성능 향상 장치에 적용된 언어 모델의 입력 변환을 예시한 도면 이다. 도 3 내지 도 5는 본 발명의 일 실시예에 따른 언어 모델 성능 향상 장치에 저장된 프롬프트 변환 규칙을 예시 한 도면이다. 도 6은 본 발명의 일 실시예에 따른 언어 모델 성능 향상 장치의 성능 평가 결과를 예시한 도면이다. 도 7은 본 발명의 일 실시예에 따른 언어 모델 성능 향상 장치의 음성 처리부의 동작을 예시한 도면이다. 도 8은 본 발명의 다른 실시예에 따른 언어 모델 성능 향상 방법을 예시한 흐름도이다. 도 9는 본 발명의 또 다른 실시예에 따른 언어 모델 성능 향상 장치를 예시한 블록도이다."}
