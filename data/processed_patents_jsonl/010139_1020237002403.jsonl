{"patent_id": "10-2023-7002403", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0027252", "출원번호": "10-2023-7002403", "발명의 명칭": "차량 캐빈에서의 음성 명령 제어 방법 및 관련 디바이스", "출원인": "후아웨이 테크놀러지 컴퍼니 리미티드", "발명자": "치우 메이칭"}}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 명령 제어 방법으로서,타깃 기간에서 차량 캐빈에서의 N개의 포지션(position)에 위치된(located) 차량 내 멤버(in-vehicle member)의 입술 움직임 정보 및 제1 유형 명령을 획득하는 단계 - 상기 제1 유형 명령은 상기 차량 캐빈에서 수집된 타깃 오디오 데이터에 기반하여 획득되고, 상기 차량 내 멤버의 입술 움직임 정보는 상기 제1 유형 명령이 상기타깃 오디오 데이터로부터 인식될 때 획득되며, 상기 타깃 기간은 상기 오디오 데이터에서 상기 제1 유형 명령에 대응하는 기간임 -;상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령을 매칭하고, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령의 매칭 결과에 기반하여 타깃 포지션을 획득하는 단계 - 상기 타깃 포지션은 상기 매칭 결과에서 상기 제1 유형 명령과 매칭하는 입술 움직임 정보를 갖는 차량 내 멤버의 포지션임 -; 및상기 타깃 포지션에서의 상기 제1 유형 명령을 실행하도록 지시하는 지시 정보를 송신하는 단계를 포함하는 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 차량 캐빈에서의 N개의 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제1 유형 명령을 획득하는단계는 구체적으로,상기 차량 캐빈에서 상기 타깃 오디오 데이터를 획득하는 단계;상기 타깃 오디오 데이터가 상기 제1 유형 명령을 포함하는 것으로 인식할 때, 상기 차량 캐빈에서 이미지 데이터를 획득하는 단계; 및상기 차량 캐빈에서의 상기 이미지 데이터로부터 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의입술 움직임 정보를 추출하는 단계인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 차량 캐빈에서의 상기 이미지 데이터로부터 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의입술 움직임 정보를 추출하는 단계는 구체적으로,둘 이상의 차량 내 멤버를 인식할 때, 상기 차량 캐빈에서의 이미지 데이터로부터 상기 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하는 단계인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 내지 제3항 중 어느 한 항에 있어서,상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령을 매칭하고, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령의 매칭 결과에 기반하여 타깃 포지션을 획득하는 단계는 구체적으로, 상기 차량 캐빈에 위치된 N명의 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령에 기반하여, 상기 N개의 포지션 각각에 있는 차량 내 멤버의 입술 움직임 정보와 상기 명령 정보 사이의 매칭도를 획득하는 단계 - N은 1보다 큰 정수임 -; 및공개특허 10-2023-0027252-3-매칭도가 가장 높은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용하는 단계인,음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항 내지 제4항 중 어느 한 항에 있어서,상기 제1 유형 명령은 상기 오디오 데이터로부터 추출된 음성 파형 시퀀스(speech waveform sequence) 또는 상기 오디오 데이터에 기반하여 인식된 텍스트 명령 정보인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제4항 중 어느 한 항에 있어서,상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보는 상기 타깃 기간에서의 상기차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임의 이미지 시퀀스인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서, 상기 음성 명령 제어 방법은, 상기 N개의 포지션 및 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보 사이의 대응 관계를 생성하는 단계를 더 포함하고,상기 매칭도가 가장 높은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용하는 단계는, 상기 매칭도가 가장 높은 타깃 입술 움직임 정보를 획득하는 단계; 및상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션 사이의 대응 관계에기반하여, 상기 타깃 입술 움직임 정보에 대응하는 포지션을 상기 타깃 포지션으로 결정하는 단계를 포함하는, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제5항에 있어서,상기 음성 명령 제어 방법은, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있는 차량 내 멤버의 아이덴티티 사이의 대응 관계를 생성하는 단계를 더 포함하고,상기 매칭도가 가장 높은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용하는 단계는, 상기 매칭도가 가장 높은 타깃 입술 움직임 정보를 획득하는 단계;상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있는 차량 내 멤버의 아이덴티티 사이의 대응 관계에 기반하여, 상기 타깃 차량 내 멤버를 결정하는 단계; 및상기 타깃 차량 내 멤버의 포지션 정보를 상기 타깃 포지션으로 결정하는 단계 - 상기 타깃 차량 내 멤버의 포지션 정보는 상기 차량에서의 센서로부터의 데이터에 기반하여 결정됨 -를 포함하는, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제6항 중 어느 한 항에 있어서,공개특허 10-2023-0027252-4-상기 차량 캐빈에서의 오디오 데이터는 상기 차량 캐빈에서의 복수의 마이크로폰에 의해 수집된 데이터에 기반하여 획득되며; 또는상기 차량 캐빈에서의 오디오 데이터는 상기 차량 캐빈에서의 지정된 포지션 영역에서 마이크로폰에 의해 수집된 오디오 데이터에 기반하여 획득되는, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제9항 중 어느 한 항에 있어서상기 제1 유형 명령은 상기 차량 캐빈에서의 제어 명령인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "음성 명령 제어 방법으로서,차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제1 유형 명령을 획득하는 단계 - 상기 제1유형 명령은 차량 캐빈에서 수집된 타깃 오디오 데이터에 기반하여 획득되고, 상기 제1 포지션에 있는 차량 내멤버의 입술 움직임 정보는 제2 유형 명령이 상기 타깃 오디오 데이터로부터 인식될 때 획득되며, 타깃 기간은상기 오디오 데이터에서 상기 제2 유형 명령에 대응하는 기간임 -;상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령을 매칭하여 매칭 결과를 획득하는 단계; 및상기 매칭 결과에 기반하여, 상기 제2 유형 명령이 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와매칭하는 것으로 결정할 때, 상기 제2 유형 명령을 실행하도록 지시하는 지시 정보를 송신하는 단계를 포함하는 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 제1 포지션은 운전석인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항 및 제12항에 있어서,상기 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제1 유형 명령을 획득하는 단계는 구체적으로,상기 차량 캐빈에서 상기 타깃 오디오 데이터를 획득하는 단계;상기 타깃 오디오 데이터가 상기 제2 유형 명령을 포함하는 것으로 인식할 때, 상기 차량 캐빈에서 이미지 데이터를 획득하는 단계; 및상기 차량 캐빈에서의 상기 이미지 데이터로부터 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보를추출하는 단계인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항 및 제12항에 있어서,상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령을 매칭하여 매칭 결과를 획득하는 단계는 구체적으로,상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령 사이의 매칭도 및 미리 설정된임계값에 기반하여, 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령의 매칭 결과를 결정하는 단계인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "공개특허 10-2023-0027252-5-제11항, 제12항 및 제14항 중 어느 한 항에 있어서,상기 제2 유형 명령은 상기 오디오 데이터로부터 추출된 음성 파형 시퀀스 또는 상기 오디오 데이터에 기반하여인식된 텍스트 명령 정보인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항 내지 제14항 중 어느 한 항에 있어서,상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보는 상기 타깃 기간에서 상기 제1 포지션에 있는 차량내 멤버의 입술 움직임의 이미지 시퀀스인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항 내지 제16항 중 어느 한 항에 있어서상기 음성 명령 제어 방법은, 상기 오디오 데이터가 상기 제2 유형 명령을 포함할 때, 상기 차량에서 다른 N개의 포지션에 있는 차량 내 멤버의 이미지 데이터를 획득하는 단계; 및상기 차량에서 다른 N개의 포지션에 있는 차량 내 멤버의 이미지 데이터로부터, 상기 타깃 기간에서의 차량에서상기 다른 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하는 단계를 더 포함하고,상기 차량에서 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령을 매칭하여 매칭 결과를 획득하는 단계는 구체적으로,상기 차량에서 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있는 차량 내멤버의 입술 움직임 정보를 상기 제2 유형 명령과 매칭하여, (N+1)명의 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령 사이의 매칭도를 획득하고, 매칭도가 가장 높은 입술 움직임 정보를 획득하는 단계이며, 그리고 상기 매칭 결과에 기반하여, 상기 제2 유형 명령이 상기 차량에서 상기 제1 포지션에 있는 차량 내 멤버의 입술움직임 정보와 매칭하는 것으로 결정할 때, 상기 제2 유형 명령을 실행하도록 지시하는 지시 정보를 송신하는단계는 구체적으로,상기 매칭도가 가장 높은 입술 움직임 정보가 상기 차량에서 상기 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보일 때, 상기 제2 유형 명령을 실행하도록 지시하는 지시 정보를 송신하는 단계인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항 내지 제17항 중 어느 한 항에 있어서,상기 차량 캐빈에서의 오디오 데이터는 상기 차량 캐빈에서의 복수의 마이크로폰에 의해 수집된 데이터에 기반하여 획득되며; 또는상기 차량 캐빈에서의 오디오 데이터는 상기 차량 캐빈에서의 지정된 포지션 영역에서 마이크로폰에 의해 수집된 오디오 데이터에 기반하여 획득되는, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항 내지 제18항 중 어느 한 항에 있어서,상기 제2 유형 명령은 차량 주행 제어 명령인, 음성 명령 제어 방법."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "음성 명령 제어 디바이스로서,프로세서공개특허 10-2023-0027252-6-를 포함하고,상기 프로세서는, 타깃 기간에서 차량 캐빈에서의 N개의 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제1 유형 명령을획득하고 - 상기 제1 유형 명령은 상기 차량 캐빈에서 수집된 타깃 오디오 데이터에 기반하여 획득되고, 상기차량 내 멤버의 입술 움직임 정보는 상기 제1 유형 명령이 상기 타깃 오디오 데이터로부터 인식될 때 획득되며,상기 타깃 기간은 상기 오디오 데이터에서 상기 제1 유형 명령에 대응하는 기간임 -;상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령을 매칭하고, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령의 매칭 결과에 기반하여 타깃 포지션을 획득하며 - 상기 타깃 포지션은 상기 매칭 결과에서 상기 제1 유형 명령과 매칭하는 입술 움직임 정보를 갖는 차량 내 멤버의 포지션임 -; 그리고상기 타깃 포지션에서의 상기 제1 유형 명령을 실행하도록 지시하는 지시 정보를 송신하도록 구성되는, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제20항에 있어서,상기 프로세서는, 상기 차량 캐빈에서 상기 타깃 오디오 데이터를 획득하고;상기 타깃 오디오 데이터가 상기 제1 유형 명령을 포함하는 것으로 인식할 때, 상기 차량 캐빈에서 이미지 데이터를 획득하며; 그리고상기 차량 캐빈에서의 상기 이미지 데이터로부터 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의입술 움직임 정보를 추출하도록 구성되는, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제20항에 있어서,상기 프로세서는 추가로, 둘 이상의 차량 내 멤버를 인식할 때, 상기 차량 캐빈에서의 이미지 데이터로부터 상기 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하도록 구성되는, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제20항 내지 제22항 중 어느 한 항에 있어서,상기 프로세서가 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1유형 명령을 매칭하고, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령의 매칭 결과에 기반하여 타깃 포지션을 획득하는 것은 구체적으로, 상기 프로세서가 상기 차량 캐빈에 위치된 N명의 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령에 기반하여, 상기 N개의 포지션 각각에 있는 차량 내 멤버의 입술 움직임 정보와 상기 명령 정보 사이의 매칭도를획득하고 - N은 1보다 큰 정수임 -; 그리고매칭도가 가장 높은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용하는 것인, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제20항 내지 제23항 중 어느 한 항에 있어서,상기 프로세서는 추가로, 상기 N개의 포지션 및 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보 사이의 대응 관계를 생성하도록 구성되고, 상기 프로세서가 상기 매칭도가 가장 높은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션공개특허 10-2023-0027252-7-으로 사용하는 것은, 상기 프로세서가 상기 매칭도가 가장 높은 타깃 입술 움직임 정보를 획득하고; 그리고 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션 사이의 대응 관계에 기반하여, 상기 타깃 입술 움직임 정보에 대응하는 포지션을 상기 타깃 포지션으로 결정하는 것을 포함하는, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제20항 내지 제24항 중 어느 항에 있어서,상기 프로세서는, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있는 차량 내 멤버의 아이덴티티 사이의 대응 관계를 생성하고,상기 프로세서가 상기 매칭도가 가장 높은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용하는 것은, 상기 프로세서가 상기 매칭도가 가장 높은 타깃 입술 움직임 정보를 획득하고; 상기 N개의 포지션에 있는 차량내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있는 차량 내 멤버의 아이덴티티 사이의 대응 관계에 기반하여, 상기 타깃 차량 내 멤버를 결정하며; 그리고 상기 타깃 차량 내 멤버의 포지션 정보를 상기 타깃 포지션으로 결정하는 것 - 상기 타깃 차량 내 멤버의 포지션 정보는 상기 차량에서의 센서로부터의 데이터에 기반하여결정됨 - 을 포함하는, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "음성 명령 제어 디바이스로서,프로세서를 포함하고,상기 프로세서는,차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제1 유형 명령을 획득하고 - 상기 제1 유형명령은 차량 캐빈에서 수집된 타깃 오디오 데이터에 기반하여 획득되고, 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보는 제2 유형 명령이 상기 타깃 오디오 데이터로부터 인식될 때 획득되며, 타깃 기간은 상기오디오 데이터에서 상기 제2 유형 명령에 대응하는 기간임 -;상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령을 매칭하여 매칭 결과를 획득하며; 그리고상기 매칭 결과에 기반하여, 상기 제2 유형 명령이 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와매칭하는 것으로 결정할 때, 상기 제2 유형 명령을 실행하도록 지시하는 지시 정보를 송신하도록 구성되는, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제26항에 있어서, 상기 제1 포지션은 운전석인, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "제26항 및 제27항에 있어서,상기 프로세서가 상기 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제1 유형 명령을 획득하는 것은 구체적으로,상기 프로세서가 상기 차량 캐빈에서 상기 타깃 오디오 데이터를 획득하고; 상기 타깃 오디오 데이터가 상기 제2 유형 명령을 포함하는 것으로 인식할 때, 상기 차량 캐빈에서 이미지 데이터를 획득하며; 그리고 상기 차량캐빈에서의 상기 이미지 데이터로부터 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하는 것인, 음성 명령 제어 디바이스. 공개특허 10-2023-0027252-8-청구항 29 제26항 내지 제28항 중 어느 한 항에 있어서,상기 프로세서가 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령을 매칭하여매칭 결과를 획득하는 것은 구체적으로,상기 프로세서가 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령 사이의 매칭도 및 미리 설정된 임계값에 기반하여, 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2유형 명령의 매칭 결과를 결정하는 것인, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_30", "content": "제26항 내지 제29항 중 어느 한 항에 있어서,상기 제2 유형 명령은 상기 오디오 데이터로부터 추출된 음성 파형 시퀀스 또는 상기 오디오 데이터에 기반하여인식된 텍스트 명령 정보인, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_31", "content": "제26항 내지 제29항 중 어느 한 항에 있어서,상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보는 상기 타깃 기간에서 상기 제1 포지션에 있는 차량내 멤버의 입술 움직임의 이미지 시퀀스인, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_32", "content": "제26항 내지 제31항 중 어느 한 항에 있어서상기 프로세서는 추가로, 상기 오디오 데이터가 상기 제2 유형 명령을 포함할 때, 상기 차량에서 다른 N개의 포지션에 있는 차량 내 멤버의 이미지 데이터를 획득하고; 그리고 상기 차량에서 다른 N개의 포지션에 있는 차량 내 멤버의 이미지 데이터로부터, 상기 타깃 기간에서의 차량에서 상기 다른 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하도록 구성되고, 상기 프로세서가 상기 차량에서 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령을 매칭하여 매칭 결과를 획득하는 것은 구체적으로,상기 프로세서가 상기 차량에서 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 상기 제2 유형 명령과 매칭하여, (N+1)명의 차량 내 멤버의 입술움직임 정보와 상기 제2 유형 명령 사이의 매칭도를 획득하고, 매칭도가 가장 높은 입술 움직임 정보를 획득하는 것이며, 그리고 상기 매칭 결과에 기반하여, 상기 제2 유형 명령이 상기 차량에서 상기 제1 포지션에 있는 차량 내 멤버의 입술움직임 정보와 매칭하는 것으로 결정할 때, 상기 프로세서가 상기 제2 유형 명령을 실행하도록 지시하는 지시정보를 송신하는 것은 구체적으로,상기 매칭도가 가장 높은 입술 움직임 정보가 상기 차량에서 상기 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보일 때, 상기 프로세서가 상기 제2 유형 명령을 실행하도록 지시하는 지시 정보를 송신하는 것인, 음성명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_33", "content": "제26항 내지 제32항 중 어느 한 항에 있어서,상기 제2 유형 명령은 차량 주행 제어 명령인, 음성 명령 제어 디바이스."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_34", "content": "칩 시스템으로서, 공개특허 10-2023-0027252-9-상기 칩 시스템은 적어도 하나의 프로세서, 메모리 및 인터페이스 회로를 포함하고, 상기 메모리, 상기 인터페이스 회로 및 상기 적어도 하나의 프로세서는 라인을 통해 상호 연결되고, 적어도 하나의 메모리는 명령을 저장하며, 상기 명령이 상기 프로세서에 의해 실행될 때, 제1항 내지 제19항 중 어느 한 항에 따른 방법이구현되는, 칩 시스템."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_35", "content": "컴퓨터가 판독 가능한 저장 매체로서, 상기 컴퓨터가 판독 가능한 저장 매체는 프로그램 코드를 저장하도록 구성되고, 상기 프로그램 코드는 제1항 내지 제19항 중 어느 한 항에 따른 방법을 포함하는, 컴퓨터가 판독 가능한 저장 매체."}
{"patent_id": "10-2023-7002403", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_36", "content": "컴퓨터 프로그램으로서,상기 컴퓨터 프로그램은 명령을 포함하고, 상기 컴퓨터 프로그램이 실행될 때 제1항 내지 제19항 중 어느 한 항에 따른 방법이 구현되는, 컴퓨터 프로그램."}
{"patent_id": "10-2023-7002403", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 실시예는 특히 지능형 조종석 분야에 적용되며, 차량 캐빈에서의 음성 제어 방법을 개시한다. 음성 제 어 방법은, 차량 캐빈에서 오디오 데이터를 획득하는 단계; 오디오 데이터가 타깃 유형의 명령 정보를 포함하는 것으로 인식할 때, 차량 캐빈에서의 오디오 데이터에서 명령 정보 관련 이벤트 세그먼트의 이미지 데이터를 획득 하는 단계; 이미지 데이터에 기반하여, 차량 캐빈에서 특정 포지션에 있는 차량 내 멤버의 이미지 데이터를 획득 하고, 이미지 데이터로부터 상기 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하는 단계; 명령 정보 및 입술 움직임 정보를 타깃 특징 매칭 모델에 입력하여, 특정 포지션에 있는 차량 내 멤버의 입술 움직임 정보 와 명령 정보 사이의 매칭도를 획득하는 단계; 및 매칭도에 기반하여 명령 정보에 대응하는 명령의 실행 여부를 판정하는 단계를 포함한다."}
{"patent_id": "10-2023-7002403", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 2020년 7월 3일에 중국 특허청에 출원되고 명칭이 \"차량 캐빈에서의 음성 명령 제어 방법 및 관련 디 바이스\"인 중국 특허 출원 번호 제202010631879.5호에 대한 우선권을 주장하는 바이며, 이러한 문헌의 내용은 원용에 의해 전체적으로 본 명세서에 포함된다. 본 발명은 인간-기계 상호 작용 기술 분야에 관한 것으로, 특히 차량 캐빈(cabin)에서의 음성 명령(speech instruction) 제어 방법 및 관련 디바이스에 관한 것이다."}
{"patent_id": "10-2023-7002403", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "자율 주행 기술이 발전함에 따라 차량은 점점 더 지능화되고 있으며, 이에 따라 차량 캐빈에서의 지능형 음성 제어가 현재 지능형 운전석에 대한 주류 요건(requirement)이 되었다. 더 많은 차량이 음성 상호 작용 기능을 갖기 시작하고, 차량 내 멤버(in-vehicle member)의 음성 명령에 따라 대응하는 기능을 수행할 수 있다. 차량 캐빈은 복수의 멤버를 수용할 수 있고, 상이한 차량 내 멤버는 상이한 제어 요건을 가질 수 있다. 결과적 으로, 특정 음성 명령을 실행할 때, 차량 내 지능형 디바이스는 명령 발신자(sender)의 특정 포지션(position) 을 획득하고 그런 다음 대응하는 명령을 어떻게 명령할지를 결정해야 한다. 차량 캐빈의 다인 시나리오(multi- person scenario)에서, 차량의 일부 명령은 차량 캐빈의 특정 포지션에서 작동해야 하며, 예를 들어, 특정 배기 구의 풍량(air volume)을 조정하거나 특정 스피커의 볼륨을 조정해야 한다. 이 경우, 배기구의 풍량을 감소시키 는 음성 명령을 수신할 때, 차량 지능형 디바이스가 현재 명령 발신자의 특정 포지션을 인식하지 못하면 음성을 통해 차량 캐빈에서 정교한 제어 조정을 수행할 수 없다. 일부 차량은 음성을 통해 차량 주행(vehicle runnin g)을 제어할 수 있으며, 예를 들어, 자율 주행이나 자동 주차를 수행할 수 있다. 그러나, 차량에 복수의 멤버가 있을 때, 예를 들어, 차량에 어린이가 있을 때, 운전 관련 음성 명령을 실행할 수 있는 경우와 운전 관련 음성 명령을 실행할 수 없는 경우를 어떻게 결정합니까?. 따라서, 차량의 오작동을 방지하기 위해 명령 발신자의 아 이덴티티(identity) 및 권한(permission)을 결정하는 것이, 차량 캐빈에서의 인간-기계 상호 작용 제어를 위해 해결해야 할 자율 주행 분야의 문제점 중 하나이다. 본 발명의 실시예는 음성 명령 실행의 정확성을 향상시키고 차량에서의 다인 시나리오에서 오작동 또는 오식별 을 감소시키기 위한 음성 명령 제어 방법 및 관련 디바이스를 제공한다. 제1 측면에 따르면, 본 발명의 실시예는 차량 캐빈에서의 음성 명령 제어 방법을 제공하며, 상기 음성 명령 제 어 방법은, 타깃 기간(time period)에서 차량 캐빈에서의 N개의 포지션(position)에 위치된(located) 차량 내 멤버(in-vehicle member)의 입술 움직임(lip motion) 정보 및 제1 유형 명령을 획득하는 단계 - 상기 제1 유형 명령은 상기 차량 캐빈에서 수집된 타깃 오디오 데이터에 기반하여 획득되고, 상기 차량 내 멤버의 입술 움직임정보는 상기 제1 유형 명령이 상기 타깃 오디오 데이터로부터 인식될 때 획득되며, 상기 타깃 기간은 상기 오디 오 데이터에서 상기 제1 유형 명령에 대응하는 기간임 -; 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령을 매칭하고(matching), 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령의 매칭 결과에 기반하여 타깃 포지션을 획득하는 단계 - 상기 타깃 포지션은 상기 매칭 결과에서 상기 제1 유형 명령과 매칭하는 입술 움직임 정보를 갖는 차량 내 멤버의 포 지션임 -; 및 상기 타깃 포지션에서의 상기 제1 유형 명령을 실행하도록 지시하는 지시 정보를 송신하는 단계를 포함한다. 본 발명의 이 실시예는 차량 캐빈에서 정제된 음성 명령 제어에 적용될 수 있다. 음성 명령이 포지션을 추가로 결정한 후에만 실행할 수 있는 명령인 것으로 인식될 때, 예를 들어, 비디오 재생, 스피커 조정, 에어컨 조정, 또는 좌석 조정 등과 같은 차량 캐빈에서의 장치에 대한 작동 명령은 명령의 특정 포지션을 결정하여 타깃화된 방식으로 지역적 작동(local operation)을 수행해야 한다. 차량에서 각 위치에 있는 멤버의 입술 움직임 정보와 인식된 특정 명령 정보를 획득하여 명령을 보내는(send) 멤버의 포지션을 결정하므로, 특정 포지션 영역에서 타 깃화된 방식으로 작동 제어를 수행할 수 있다. 음성 제어 방법에서, 비디오 데이터를 처리하여 비디오 데이터의 일부 특징과 명령 정보의 매칭도를 분석하기 때문에, 국부적으로 발생할 수 있으며 즉, 차량 또는 차량에 탑재 된 지능형 디바이스가 방법을 수행한다. 다르게는, 비디오 데이터의 처리 및 매칭 동작(action)을 클라우드에서 수행할 수 있다. 본 발명의 이 실시예의 방법은 임의의 수량의 사람이 있는 차량의 시나리오에 적용 가능하고, 특히 차량에 복수 의 멤버가 있고 복수의 사람들이 동시에 말하는 시나리오에 적용 가능하다. 이 경우, 입술 움직임 정보와 명령 정보의 매칭 및 차량 내 포지션 분포 정보(in-vehicle position distribution information)를 참조하여, 음성 명령을 보내는 멤버의 포지션 정보를 정확하게 위치 결정할 수 있다. 이는 명령을 실행할 포지션 영역(position area)을 결정할 수 있다. 본 발명의 이 실시예의 방법에서 언급된 N은 반드시 차량의 모든 멤버를 나타내는 것 은 아니며, 차량의 모든 멤버일 수도 있고 일부 멤버일 수도 있다. 가능한 구현에서, 상기 차량 캐빈에서의 N개의 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제1 유형 명령을 획득하는 단계는 구체적으로: 상기 차량 캐빈에서 상기 타깃 오디오 데이터를 획득하는 단계; 상기 타깃 오디오 데이터가 상기 제1 유형 명령을 포함하는 것으로 인식할 때, 상기 차량 캐빈에서 이미지 데이터를 획득 하는 단계; 및 상기 차량 캐빈에서의 상기 이미지 데이터로부터 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하는 단계이다. 본 발명의 이 실시예에서, 언급된 차량 캐빈에서의 이 미지 데이터로부터 차량에서 N개의 포지션에 있는 멤버의 입술 움직임 정보를 추출하는 것은 구체적으로: 얼굴 인식 알고리즘에 기반하여 비디오 데이터에서 N개의 얼굴 영역을 인식하고, N개의 얼굴 영역 각각에서 샘플링을 통해 입술 움직임 비디오를 추출하거나 비디오 프레임 시퀀스를 추출하며, 각 얼굴 영역의 입술 움직임 비디오 또는 비디오 프레임 시퀀스에 기반하여 N명의 멤버의 입술 움직임 정보를 결정하는 것일 수 있다. 이미지 데이 터는 통상적으로 차량에서의 하나 이상의 카메라에 의한 수집을 통해 획득되며, 복수의 유형의 카메라가 있을 수 있다. 또한, 일반적으로 차량 캐빈의 환경에는 복수의 카메라가 있기 때문에, 일부 카메라는 촬영 각도를 변경하여 다 양한 화각의 비디오 데이터를 획득할 수 있다. 따라서, 본 발명의 실시예에서 언급하는 이미지 데이터는 하나의 각도에서 하나의 카메라에 의해 획득된 이미지 데이터일 수 있다. 여러 각도에서 촬영이 수행될 때, 차량에서 서로 다른 포지션에 있는 멤버가 서로 가려지는 경우가 존재할 수 있다. 따라서, 본 발명의 이 실시예에서 이미 지 데이터는 또한, 상이한 각도에서 동일한 카메라로부터의 이미지 데이터, 또는 복수의 카메라로부터의 이미지 데이터, 또는 전술한 경우들의 다른 조합일 수 있다. 가능한 구현에서, 상기 차량 캐빈에서의 상기 이미지 데이터로부터 상기 차량 캐빈에서 상기 N개의 포지션에 있 는 차량 내 멤버의 입술 움직임 정보를 추출하는 단계는 구체적으로, 둘 이상의 차량 내 멤버를 인식할 때, 상 기 차량 캐빈에서의 이미지 데이터로부터 상기 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하는 단계이다. 본 발명의 이 실시예의 특정 구현 프로세스에서, 컴퓨팅 리소스의 낭비를 피하기 위 해, 제1 명령 유형이 인식된 후, 획득된 차량 내 이미지 데이터에 기반하여 차량 캐빈에서의 사람의 수량을 결 정할 수 있다. 입술 움직임 정보는 차량 캐빈에서 사람이 단지 1명일 때는 추출될 필요가 없으며, 둘 이상의 사 람이 있을 때만 사람이 취하는 포지션에 있는 사람의 입술 움직임 정보를 추출한다. 가능한 구현에서, 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령을 매칭하고, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령의 매칭 결과에 기반하여 타깃 포지션을 획득하는 단계는 구체적으로: 상기 차량 캐빈에 위치된 N명의 차량 내 멤버 의 입술 움직임 정보와 상기 제1 유형 명령에 기반하여, 상기 N개의 포지션 각각에 있는 차량 내 멤버의 입술 움직임 정보와 상기 명령 정보 사이의 매칭도를 획득하는 단계 - N은 1보다 큰 정수임 -; 및 매칭도가 가장 높 은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용하는 단계이다. 매칭도는 복수의 방식으로 획득될 수 있으며, 통상적으로 타깃 매칭 모델을 사용하여 획득될 수 있다. 타깃 특 징 매칭 모델은 트레이닝 사용자의 입술 움직임 정보와 하나 이상의 음성 정보(음성 정보는 음성 파형 시퀀스 (speech waveform sequence) 또는 음성에 대응하는 텍스트 정보일 수 있음)를 입력으로 사용하고, 트레이닝 사 용자의 입술 움직임 정보와 M개의 음성 정보 사이의 매칭도를 M개의 레이블로 사용하는 트레이닝을 통해 획득된 특징 매칭 모델이다. 본 발명의 이 실시예의 모델 트레이닝 방식에서, 모델 추론 방식은 상이한 트레이닝 샘플 데이터에 기반하여 변 화한다. 복수 그룹의 입술 움직임 정보, 예를 들어, 5개의 입술 움직임 정보가 입력에 사용될 때, 5개의 입술 움직임 정보와 M개의 음성 정보 각각이 입력으로서 M개의 샘플 그룹으로 구성되고, 샘플에서 5개의 입술 움직임 정보 각각과 음성 정보 사이의 매칭도를 출력으로 사용하며, 레이블 정보는 트레이닝의 타깃 출력 결과이다. 트 레이닝을 통해 획득된 모델도 추론 시 5개의 입술 움직임 정보와 타깃 명령 정보를 입력으로 사용한다. 차량에 5명 미만의 사람이 있을 때, 빈 포지션에서의 입술 정보를 입력을 위해 기본값(default value) 예를 들어, 올제 로 시퀀스(all-zero sequence)로 사용할 수 있으며, 명령 정보에 대응하는 매칭도를 출력한다. 다르게는, 하나 의 입술 움직임 정보와 음성 정보를 샘플의 그룹으로 사용할 수 있으며, 매칭 레이블은 트레이닝을 위한 타깃 트레이닝 결과이다. 이러한 트레이닝을 통해 획득된 모델이 추론을 수행할 때, 차량에서 각 멤버의 입술 움직임 명령과 명령 정보를 입력으로 사용해야 하며, 복수의 포지션에 있는 멤버에 대응하는 매칭도를 별도로 획득한다. 가능한 구현에서, 상기 제1 유형 명령은 상기 오디오 데이터로부터 추출된 음성 파형 시퀀스 또는 상기 오디오 데이터에 기반하여 인식된 텍스트 명령 정보이다. 가능한 구현에서, 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보는 상기 타깃 기간에서의 상기 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임의 이미지 시퀀스이다. 본 발명의 이 실시예에서는, 획득한 음성 명령과 각 포지션에 있는 차량 내 멤버의 입술 정보의 매칭도에 기반 하여, 멤버가 명령과 관련된 음성을 보내는 포지션이 학습되고, 매칭 모델을 사용하여 명령 정보와 멤버의 입술 움직임 정보 사이의 매칭도를 학습한다. 획득될 명령 정보는 매칭 모델에 따라 다르다. 모델 트레이닝 요건에 따라 입술 움직임 비디오로부터 서로 다른 입술 움직임 정보를 추출할 수 있다. 예를 들어, 입술 움직임 정보는 타깃 기간에서의 입술 움직임의 이미지 시퀀스이거나, 위-아래 입술 거리의 시간 시퀀스 변화를 지시하는 (indicating) 벡터 파라미터일 수 있다. 이와 유사하게, 제1 유형 명령도 복수의 형태를 가질 수 있으며, 음성 파형 시퀀스일 수 있거나, 명령에 대응하는 텍스트 정보일 수 있다. 가능한 구현에서, 상기 차량 캐빈에서의 오디오 데이터는 상기 차량 캐빈에서 지정된 포지션에 있는 마이크로폰 에 의해 수집된 오디오 데이터에 기반하여 획득된다. 가능한 구현에서, 상기 차량 캐빈에서의 오디오 데이터는 상기 차량 캐빈에서의 복수의 마이크로폰에 의해 수집 된 오디오 데이터에 기반하여 획득된다. 복수의 마이크로폰은 통상적으로 차량 캐빈에 존재하며, 차량 캐빈에서 서로 다른 포지션에 배치된다. 따라서, 차량 캐빈에서 수집된 오디오 데이터에 대한 최적의 오디오 효과를 획득하기 위해, 본 발명의 이 실시예에서 언 급된 차량 캐빈에서의 오디오 데이터는 포괄적인 처리가 복수의 오디오 데이터에 대해 수행된 후에 획득될 수 있으며; 차량 캐빈에서 복수의 마이크로폰에 의해 수집된 오디오 데이터를 종합적으로 비교한 후 미리 설정된 규칙에 따라 선택된 최적의 파라미터, 즉 최적의 음성 품질로 녹음된 오디오 데이터; 또는 명령 포지션에서 마 이크로폰에 의해 수집된 오디오 데이터, 예를 들어, 차량의 중앙 포지션 영역에 배치된 마이크로폰에 의해 수집 된 오디오 데이터일 수 있다. 가능한 구현에서, 상기 타깃 특징 매칭 모델은 제1 모델, 제2 모델 및 제3 모델을 포함한다. 차량 캐빈에서 N개 의 포지션에 있는 차량 내 멤버의 입술 움직임 정보 및 명령 정보를 타깃 특징 매칭 모델에 타이핑하여 (typing), 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 명령 정보 사이의 매칭도를 획득하는 것은: 명령 정보를 제1 모델에 타이핑하여 음성 특징을 획득하고 - 음성 특징은 K차원 음성 특징(K- dimensional speech feature)이고, K는 0보다 큰 정수임 -; 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 제2 모델에 타이핑하여 N개의 이미지 시퀀스 특징을 획득하며 - N개의 이미지 시퀀스 특 징 각각은 K차원 이미지 시퀀스 특징임 -; 그리고 음성 특징과 N개의 이미지 시퀀스 특징을 제2 모델에 타이핑 하여, N개의 이미지 시퀀스 특징과 음성 특징의 사이의 매칭도를 획득하는 것을 포함한다. 가능한 구현에서, 상기 타깃 특징 매칭 모델은 제1 모델 및 제2 모델을 포함한다. 차량 캐빈에서 N개의 포지션 에 있는 차량 내 멤버의 입술 움직임 정보 및 명령 정보를 타깃 특징 매칭 모델에 타이핑하여, 차량 캐빈에서 N 개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 명령 정보 사이의 매칭도를 획득하는 것은: 오디오 데 이터를 제1 모델에 타이핑하여 대응하는 명령 정보를 획득하고; 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤 버의 입술 움직임 정보를 제2 모델에 타이핑하며 - 각 차량 내 멤버의 입술 움직임 정보는 하나의 이미지 시퀀 스 특징에 대응함 -; N개의 이미지 시퀀스 특징을 동시에 또는 개별적으로 제2 모델에 타이핑하여 각각의 입술 움직임 정보에 대응하는 명령 정보를 획득하고; 그리고 2개의 모델의 인식 결과에 기반하여, 명령을 보내는 타 깃 포지션 멤버를 결정하는 것을 포함한다. 본 발명의 이 실시예에서 모델의 구현에서, 제1 모델에 의해 출력되는 명령 정보는 명령 및 매칭도에 대응하는 식별자일 수 있다. 모델은 매칭도가 가장 높은 명령에 대응하는 명령 식별자를 선택하고 출력하며, 여기서 식별 자는 명령 코드일 수 있거나, 또는 명령에 대응하는 텍스트 특징일 수 있다. 제1 모델과 제2 모델의 출력 결과 에 기반하여 결정이 수행된다. 복수의 결정 규칙이 있을 수 있다. 예를 들어, 제1 모델이 출력한 명령 정보와 동일한 명령을 갖는 포지션 멤버를 인식한다. 제1 모델이 인식한 명령 정보와 동일한 명령을 가진, 복수의 포지 션에 있는 인식된 멤버이면, 제2 모델이 출력한 매칭도를 비교하여 매칭도가 가장 높은 타깃 포지션을 명령을 실행하기 위해 타깃 포지션으로 선택하거나; 또는 제2 모델에서 인식된 매칭도가 가장 높은 명령 정보를 선택하 여 제1 모델로부터의 명령 정보와 비교하고, 명령 정보가 동일하면, 제2 모델에서 매칭도가 가장 높은 명령 정 보에 대응하는 포지션을 타깃 포지션으로 결정한다. 가능한 구현에서, 상기 음성 명령 제어 방법은, 상기 N개의 포지션 및 상기 N개의 포지션에 있는 차량 내 멤버 의 입술 움직임 정보 사이의 대응 관계를 생성하는 단계를 더 포함하고, 상기 매칭도가 가장 높은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용하는 단계는, 상기 매칭도가 가장 높은 타깃 입 술 움직임 정보를 획득하는 단계; 및 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션 사이의 대응 관계에 기반하여, 상기 타깃 입술 움직임 정보에 대응하는 포지션을 상기 타깃 포지션으로 결정하는 단계를 포함한다. N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 N개의 포지션 사이의 대응 관계를 생성하는 것은, 차 량에서 이미지를 수집하여 각 포지션에서의 멤버 관계를 획득하고, 그런 다음 각 멤버의 입술 움직임 정보를 포 지션 관계에 매핑하는 것일 수 있으며, 여기서 이미지 데이터는 입술 정보로부터 추출된 동일한 이미지일 수 있 거나, 독립적인 수집 프로세스에서 가져온 것일 수도 있다. 가능한 구현에서, 상기 음성 명령 제어 방법은, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있는 차량 내 멤버의 아이덴티티 사이의 대응 관계를 생성하는 단계를 더 포함하고, 상기 매칭도가 가장 높은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용하는 단계는, 상기 매칭도가 가장 높은 타깃 입술 움직임 정보를 획득하는 단계; 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있는 차량 내 멤버의 아이덴티티 사이의 대응 관계에 기반하여, 상기 타깃 차량 내 멤버를 결정하는 단계; 및 상기 타깃 차량 내 멤버의 포지션 정보를 상기 타깃 포지션으로 결정하 는 단계 - 상기 타깃 차량 내 멤버의 포지션 정보는 상기 차량에서의 센서로부터의 데이터에 기반하여 결정됨 - 를 포함한다. 본 발명의 이 실시예에서 언급된 제1 유형 명령은 차량 캐빈에서의 제어 명령일 수 있으며, 명령을 실행하기 위 한 타깃 영역을 결정하기 위해 포지션 정보를 차량 캐빈에서 인식해야 하는 명령 상호 작용 시나리오에 주로 적 용 가능하다. 차량 내 사용자는 음성 명령을 실행할 때 예를 들어, 오른쪽 뒷좌석에서 차량 창을 닫기와 같은 특정 포지션 정보를 제공하는 경우가 있다. 따라서, 가능한 구현에서, 특정 포지션 정보를 가진 이러한 유형의 명령은 제1 유형 명령이 아닌 것으로 간주될 수 있다. 제1 유형 명령은 포지션 영역을 구별하여 수행해야 하는 명령이지만, 포지션 영역 정보를 포함하지 않는 명령일 수 있다. 제2 측면에 따르면, 본 발명의 실시예는 차량 캐빈에서의 음성 제어 방법을 제공하며, 상기 음성 제어 방법은, 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제1 유형 명령을 획득하는 단계 - 상기 제1 유형 명령은 차량 캐빈에서 수집된 타깃 오디오 데이터에 기반하여 획득되고, 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보는 제2 유형 명령이 상기 타깃 오디오 데이터로부터 인식될 때 획득되며, 타깃 기간은상기 오디오 데이터에서 상기 제2 유형 명령에 대응하는 기간임 -; 상기 제1 포지션에 있는 차량 내 멤버의 입 술 움직임 정보와 상기 제2 유형 명령을 매칭하여 매칭 결과를 획득하는 단계; 및 상기 매칭 결과에 기반하여, 상기 제2 유형 명령이 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 매칭하는 것으로 결정할 때, 상기 제2 유형 명령을 실행하도록 지시하는 지시 정보를 송신하는 단계를 포함한다. 본 발명의 이 실시예는 차량 캐빈에서의 정제된 음성 명령 제어에 적용될 수 있다. 음성 명령이 명령 발신자의 아이덴티티 또는 권한이 결정된 후에만 실행될 수 있는 명령, 예를 들어, 차량의 주행 제어 명령(running control instruction), 또는 운전 모드 전환, 운전 방향 제어, 통화 내역 조회와 같이, 프라이버시 정보 (privacy information)에 관한 작동과 관련된 다른 명령인 것으로 인식할 때, 차량의 특수한 환경에 기반하여, 운전석(driver's seat)의 멤버가 기본적으로 가장 높은 권한을 갖는다. 따라서, 발신자의 아이덴티티를 확인해 야 하는 명령을 획득할 때, 특정 포지션(예를 들어, 운전석)에 있는 멤버의 입술 움직임 정보와 인식된 특정 명 령 정보를 획득하여, 명령이 운전석에 있는 멤버에 의해 송신된 것인지를 결정하므로, 특정 포지션 영역에 대한 작동 제어가 타깃화된 방식으로 수행될 수 있다. 음성 제어 방법에서, 비디오 데이터를 처리하여 비디오 데이터 의 일부 특징과 명령 정보의 매칭도를 분석하기 때문에, 국부적으로 발생할 수 있는 즉, 차량 또는 차량에 탑재 된 지능형 디바이스가 상기 방법을 수행한다. 다르게는, 비디오 데이터의 처리 및 매칭 동작을 클라우드에서 수 행할 수 있다. 운전석에 있는 멤버의 입술 움직임 정보를 수집하는 것 외에도, 하나 이상의 포지션에 있는 멤버 가 명령 유형의 제어 권한을 갖는 것이 미리 설정되거나 수동으로 설정될 수도 있다. 이러한 명령이 인식될 때, 매칭 분석을 위해 대응하는 포지션에 있는 사람의 입술 움직임 정보를 획득한다. 가능한 구현에 있어서, 얼굴 인식 기술을 사용하여 현재 차량 내 환경에서 특정 사용자(예를 들어, 차량 소유자)의 포지션도 인식할 수 있고, 특정 사용자의 포지션을 제1 포지션으로 사용한다. 운전석에 있는 멤버의 입술 움직임 정보를 통상 기본적으로 획득한다. 본 발명의 이 실시예의 방법은 임의의 사람 수량을 가지는 차량의 시나리오에 적용 가능하며, 특히 차량에 복수 의 멤버가 있고 복수의 사람이 동시에 말하는 시나리오에 적용 가능하다. 이 경우, 차량 내 포지션 분포 정보 및 입술 움직임 정보와 명령 정보 사이의 매칭을 참조하여, 음성 명령이 특정 포지션에 있는 멤버가 보낸 것인 지를 정확하게 판정할 수 있으며, 명령을 수행할지를 추가로 판정할 수 있다. 매칭 방식은 매칭을 위해 차량에 서의 복수의 멤버(제1 포지션에 있는 멤버를 포함)를 획득하고, 제1 포지션에 있는 멤버가 가장 높은 매칭도를 갖는지를 판정하는 것일 수 있고; 또는 매칭을 위해 제1 포지션에 있는 멤버만을 획득하고, 매칭도가 임계값에 도달할 때 결과가 매칭이라고 결정하는 것일 수 있으며, 명령을 실행할 수 있다. 명령이 제2 유형 명령인지를 판정하는 실시예에서, 일반적으로 특정 포지션을 결정해야 하는 경우는, 특정 포지 션에 있는 멤버만이 이러한 유형의 명령을 실행할 수 있는 권한을 갖는 시나리오에 대한 것이다. 예를 들어, 제 2 유형의 명령은 일반적으로 차량 제어 명령이며, 오작동을 방지하기 위해, 이러한 유형의 명령은 일반적으로, 음성을 사용하여 차량 주행 제어를 수행하는 능력을 가지는, 운전석과 같은 특정 포지션에 있는 멤버에게만 설 정된다. 공통 구현은 제2 명령 정보와의 매칭을 위해 운전석에 있는 사용자의 입술 움직임 정보를 획득하고, 그 결과가 매칭될 때, 제2 유형 명령이 운전석에 있는 사용자에 의해 송신된 것으로 결정하고, 제2 유형 명령을 실 행하는 것이다. 일반적으로 운전석에 있는 사용자가 차량을 제어하는 권한을 갖는 것이 기본으로 간주된다. 다 른 포지션에 있는 탑승자가 음성 제어 권한을 갖도록 수동으로 설정할 수 있으며, 제1 포지션은 또한 차량의 다 른 포지션이다. 다르게는, 일부 명령은 특정 포지션에 있는 멤버에 의해서만 실행될 수 있도록 요건에 따라 설정될 수 있다. 이 경우, 이러한 유형의 명령의 실행 규칙도 제2 유형 명령의 실행 방식을 참조하여 결정될 수 있다. 본 발명의 이 실시예에서, 상기 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제2 유형 명령을 획득하는 단계는 구체적으로, 차량 캐빈에서 타깃 오디오 데이터를 획득하는 단계; 상기 타깃 오디오 데 이터가 상기 제2 유형 명령을 포함하는 것으로 인식할 때, 상기 차량 캐빈에서 이미지 데이터를 획득하는 단계; 및 상기 차량 캐빈에서의 상기 이미지 데이터로부터 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보 를 추출하는 단계일 수 있다. 전술한 솔루션의 구현 프로세스에서, 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유 형 명령을 매칭하여 매칭 결과를 획득하는 단계는 구체적으로, 상기 제1 포지션에 있는 차량 내 멤버의 입술 움 직임 정보와 상기 제2 유형 명령 사이의 매칭도 및 미리 설정된 임계값에 기반하여, 상기 제1 포지션에 있는 차 량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령의 매칭 결과를 결정하는 단계이다. 본 발명의 이 실시예에서 상기 제2 유형 명령은 상기 오디오 데이터로부터 추출된 음성 파형 시퀀스 또는 상기 오디오 데이터에 기반하여 인식된 텍스트 명령 정보일 수 있다. 차량 내 멤버의 입술 움직임 정보는 상기 타깃 기간에서의 차량 내 멤버의 입술 움직임의 이미지 시퀀스이다. 구현 프로세스에서, 본 발명의 이 실시예는: 상기 오디오 데이터가 상기 제2 유형 명령을 포함할 때, 상기 차량 에서 다른 N개의 포지션에 있는 차량 내 멤버의 이미지 데이터를 획득하는 단계; 및 상기 차량에서 다른 N개의 포지션에 있는 차량 내 멤버의 이미지 데이터로부터, 상기 타깃 기간에서의 차량에서 상기 다른 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하는 단계를 더 포함하고, 여기서, 상기 차량에서 상기 제1 포지 션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령을 매칭하여 매칭 결과를 획득하는 단계는 구 체적으로, 상기 차량에서 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있 는 차량 내 멤버의 입술 움직임 정보를 상기 제2 유형 명령과 매칭하여, (N+1)명의 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령 사이의 매칭도를 획득하고, 매칭도가 가장 높은 입술 움직임 정보를 획득하는 단계 이며, 그리고 상기 매칭 결과에 기반하여, 상기 제2 유형 명령이 상기 차량에서 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 매칭하는 것으로 결정할 때, 상기 제2 유형 명령을 실행하도록 지시하는 지시 정 보를 송신하는 단계는 구체적으로, 상기 매칭도가 가장 높은 입술 움직임 정보가 상기 차량에서 상기 제1 포지 션에 위치된 차량 내 멤버의 입술 움직임 정보일 때, 상기 제2 유형 명령을 실행하도록 지시하는 지시 정보를 송신하는 단계이다. 본 발명의 이 실시예에서, 차량에서 포지션에 있는 멤버의 입술 움직임 정보는 차량에서 상기 포지션에 있는 멤 버의 비디오 데이터로부터 추출된다. 구체적인 방법은 얼굴 인식 알고리즘에 기반하여 비디오 데이터에서 복수 의 얼굴 영역을 인식하고, 복수의 얼굴 영역 각각에서 입술 움직임 비디오를 추출하며, 각 얼굴 영역의 입술 움 직임 비디오에 기반하여, 각 얼굴에 대응하는 입술 움직임 정보를 결정하는 것이다. 가능한 구현에서, 상기 차량 캐빈에서의 오디오 데이터는 상기 차량 캐빈에서의 복수의 마이크로폰에 의해 수집 된 데이터에 기반하여 획득되며; 또는 상기 차량 캐빈에서의 오디오 데이터는 상기 차량 캐빈에서의 지정된 포 지션 영역에서 마이크로폰에 의해 수집된 오디오 데이터에 기반하여 획득된다. 제3 측면에 따르면, 본 발명은 프로세서를 포함하는 음성 명령 제어 디바이스를 제공한다. 상기 프로세서는, 타 깃 기간에서 차량 캐빈에서의 N개의 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제1 유형 명령을 획 득하고 - 상기 제1 유형 명령은 상기 차량 캐빈에서 수집된 타깃 오디오 데이터에 기반하여 획득되고, 상기 차 량 내 멤버의 입술 움직임 정보는 상기 제1 유형 명령이 상기 타깃 오디오 데이터로부터 인식될 때 획득되며, 상기 타깃 기간은 상기 오디오 데이터에서 상기 제1 유형 명령에 대응하는 기간임 -; 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령을 매칭하고, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령의 매칭 결과에 기반하여 타깃 포지션을 획득하며 - 상기 타깃 포지션은 상기 매칭 결과에서 상기 제1 유형 명령과 매칭하는 입술 움직임 정보를 갖는 차량 내 멤 버의 포지션임 -; 그리고 상기 타깃 포지션에서의 상기 제1 유형 명령을 실행하도록 지시하는 지시 정보를 송신 하도록 구성된다. 본 발명의 이 실시예는 차량 캐빈에서 정제된 음성 명령 제어에 적용될 수 있다. 음성 명령이 포지션을 추가로 결정한 후에만 실행할 수 있는 명령인 것으로 인식될 때, 예를 들어, 비디오 재생, 스피커 조정, 에어컨 조정, 또는 좌석 조정 등과 같은 차량 캐빈에서의 장치에 대한 작동 명령은 명령의 특정 포지션을 결정하여 타깃화된 방식으로 지역적 작동을 수행해야 한다. 차량에서 각 위치에 있는 멤버의 입술 움직임 정보와 인식된 특정 명령 정보를 획득하여 명령을 보내는 멤버의 포지션을 결정하므로, 특정 포지션 영역에서 타깃화된 방식으로 작동 제 어를 수행할 수 있다. 음성 제어 디바이스는 비디오 데이터를 처리하여 비디오 데이터의 일부 특징과 명령 정보 의 매칭도를 분석해야 한다. 따라서, 상기 디바이스는 로컬 디바이스일 수 있으며, 예를 들어, 지능형 차량 탑 재 디바이스 또는 차량 탑재 프로세서 칩일 수도 있고, 마이크로폰과 카메라를 포함하는 차량 탑재 시스템일 수 도 있고, 지능형 차량일 수도 있다. 또한, 솔루션의 다양한 구현에 따라, 클라우드 서버는 차량에서의 스피커와 차량 탑재 카메라로부터 데이터를 획득한 후 비디오 데이터를 처리하고 매칭시키는 동작을 수행할 수도 있다. 가능한 구현에서, 상기 프로세서는, 상기 차량 캐빈에서 상기 타깃 오디오 데이터를 획득하고; 상기 타깃 오디 오 데이터가 상기 제1 유형 명령을 포함하는 것으로 인식할 때, 상기 차량 캐빈에서 이미지 데이터를 획득하며; 그리고 상기 차량 캐빈에서의 상기 이미지 데이터로부터 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하도록 구성된다. 본 발명의 이 실시예에서, 프로세서가 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하는 것은 구체적으로, 프로세서가 얼굴 인식 알고리즘에 기반하여 비디오 데이터에서의 N개의 얼굴 영역을 인식하고, N개의 얼굴 영역 각각에서 샘플링을 통해 입술 움직임 비디오를 추출하거나 비디오 프레임 시퀀스를 추출하고, 각 얼굴 영역에서 입술 움직임 비디오 또는 비 디오 프레임 시퀀스에 기반하여 N명의 멤버의 입술 움직임 정보를 결정하는 것일 수 있다. 이미지 데이터는 통 상적으로 차량에서의 하나 이상의 카메라에 의한 수집을 통해 획득되며, 복수의 유형의 카메라가 있을 수 있다. 가능한 구현에서, 상기 프로세서는 추가로, 둘 이상의 차량 내 멤버를 인식할 때, 상기 차량 캐빈에서의 이미지 데이터로부터 상기 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하도록 구성된 다. 본 발명의 이 실시예의 특정 구현 프로세스에서, 컴퓨팅 리소스의 낭비를 피하기 위해, 제1 명령 유형이 인 식된 후, 획득된 차량 내 이미지 데이터에 기반하여 차량 캐빈에서의 사람의 수량을 결정할 수 있다. 입술 움직 임 정보는 차량 캐빈에서 사람이 단지 1명일 때는 추출될 필요가 없으며, 둘 이상의 사람이 있을 때만 사람이 취하는 포지션에 있는 사람의 입술 움직임 정보를 추출한다. 가능한 구현에서, 상기 프로세서가 상기 차량 캐빈에서 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령을 매칭하고, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제 1 유형 명령의 매칭 결과에 기반하여 타깃 포지션을 획득하는 것은 구체적으로, 상기 프로세서가 상기 차량 캐 빈에 위치된 N명의 차량 내 멤버의 입술 움직임 정보와 상기 제1 유형 명령에 기반하여, 상기 N개의 포지션 각 각에 있는 차량 내 멤버의 입술 움직임 정보와 상기 명령 정보 사이의 매칭도를 획득하고 - N은 1보다 큰 정수 임 -; 그리고 매칭도가 가장 높은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용 하는 것이다. 매칭도는 상이한 타깃 매칭 모델을 사용하여 프로세서에 의해 획득될 수 있다. 타깃 특징 매칭 모델은 트레이닝 사용자의 입술 움직임 정보와 하나 이상의 음성 정보(음성 정보는 음성 파형 시퀀스 또는 음성에 대응하는 텍스 트 정보일 수 있음)를 입력으로 사용하고, 트레이닝 사용자의 입술 움직임 정보와 M개의 음성 정보 사이의 매칭 도를 M개의 레이블로 사용하는 트레이닝을 통해 획득된 특징 매칭 모델이다. 모델의 트레이닝은 일반적으로 클 라우드 측에서 수행되며 모델 사용과 독립적이다. 즉, 모델의 트레이닝은 디바이스에서 수행되고, 트레이닝이 완료된 후, 상기 모델이 실행 및 사용을 위한 명령 매칭을 수행해야 하는 디바이스로 송신된다. 본 발명의 이 실시예의 모델 트레이닝 방식에서, 모델 추론 방식은 상이한 트레이닝 샘플 데이터에 기반하여 변 화한다. 복수 그룹의 입술 움직임 정보, 예를 들어, 5개의 입술 움직임 정보가 입력에 사용될 때, 5개의 입술 움직임 정보와 M개의 음성 정보 각각이 입력으로서 M개의 샘플 그룹으로 구성되고, 샘플에서 5개의 입술 움직임 정보 각각과 음성 정보 사이의 매칭도를 출력으로 사용하며, 레이블 정보는 트레이닝의 타깃 출력 결과이다. 트 레이닝을 통해 획득된 모델도 추론 시 5개의 입술 움직임 정보와 타깃 명령 정보를 입력으로 사용한다. 차량에 5명 미만의 사람이 있을 때, 빈 포지션에서의 입술 정보를 입력을 위해 기본값 예를 들어, 올제로 시퀀스로 사 용할 수 있으며, 명령 정보에 대응하는 매칭도를 출력한다. 다르게는, 하나의 입술 움직임 정보와 음성 정보를 샘플의 그룹으로 사용할 수 있으며, 매칭 레이블은 트레이닝을 위한 타깃 트레이닝 결과이다. 이러한 트레이닝 을 통해 획득된 모델이 추론을 수행할 때, 차량에서 각 멤버의 입술 움직임 명령과 명령 정보를 입력으로 사용 해야 하며, 복수의 포지션에 있는 멤버에 대응하는 매칭도를 별도로 획득한다. 가능한 구현에서, 상기 프로세서는 추가로, 상기 N개의 포지션 및 상기 N개의 포지션에 있는 차량 내 멤버의 입 술 움직임 정보 사이의 대응 관계를 생성하도록 구성되고, 상기 프로세서가 상기 매칭도가 가장 높은 입술 움직 임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용하는 것은, 상기 프로세서가 상기 매칭도가 가장 높은 타깃 입술 움직임 정보를 획득하고; 그리고 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션 사이의 대응 관계에 기반하여, 상기 타깃 입술 움직임 정보에 대응하는 포지션을 상 기 타깃 포지션으로 결정하는 것을 포함한다. 프로세서가 차량 내 멤버의 입술 움직임 정보와 포지션 사이의 대응 관계를 생성하는 것은, 차량에서 이미지를 수집하여 각 포지션에서의 멤버 관계를 획득하고, 그런 다음 각 멤버의 입술 움직임 정보를 포지션 관계에 매핑 하는 것일 수 있으며, 여기서 이미지 데이터는 입술 정보로부터 추출된 동일한 이미지일 수 있거나, 독립적인 수집 프로세스에서 가져온 것일 수도 있다. 가능한 구현에서, 상기 프로세서는, 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있는 차량 내 멤버의 아이덴티티 사이의 대응 관계를 생성하고; 상기 프로세서가 상기 매칭도가 가장 높은 입술 움직임 정보에 대응하는 차량 내 멤버의 포지션을 타깃 포지션으로 사용하는 것은, 상기 프로세서가 상기 매칭도가 가장 높은 타깃 입술 움직임 정보를 획득하고; 상기 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개의 포지션에 있는 차량 내 멤버의 아이덴티티 사이의 대응 관계에 기반하여, 상기 타깃차량 내 멤버를 결정하며; 그리고 상기 타깃 차량 내 멤버의 포지션 정보를 상기 타깃 포지션으로 결정하는 것 을 포함하며, 상기 타깃 차량 내 멤버의 포지션 정보는 상기 차량에서의 센서로부터의 데이터에 기반하여 결정 된다. 본 발명의 이 실시예에서 언급된 제1 유형 명령은 차량 캐빈에서의 제어 명령일 수 있으며, 명령을 실행하기 위 한 타깃 영역을 결정하기 위해 포지션 정보를 차량 캐빈에서 인식해야 하는 명령 상호 작용 시나리오에 주로 적 용 가능하다. 차량 내 사용자는 음성 명령을 실행할 때 예를 들어, 오른쪽 뒷좌석에서 차량 창을 닫기와 같은 특정 포지션 정보를 제공한다. 이 경우에, 프로세서는 명령에 대응하는 타깃 포지션을 직접 인식할 수 있다. 따 라서, 가능한 구현에서, 프로세서가 획득된 명령이 특정 포지션 정보를 가진 이러한 유형의 명령인 것으로 인식 할 때, 프로세서는 상기 명령이 제1 유형 명령이 아닌 것으로 결정한다. 제1 유형 명령은 포지션 영역을 구별하 여 수행해야 하는 명령이지만, 포지션 영역 정보를 포함하지 않는 명령일 수 있다. 제4 측면에 따르면, 본 발명의 실시예는 프로세서를 포함하는 음성 명령 제어 디바이스를 제공한다. 상기 프로 세서는, 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제1 유형 명령을 획득하고 - 상기 제1 유형 명령은 차량 캐빈에서 수집된 타깃 오디오 데이터에 기반하여 획득되고, 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보는 제2 유형 명령이 상기 타깃 오디오 데이터로부터 인식될 때 획득되며, 타깃 기간 은 상기 오디오 데이터에서 상기 제2 유형 명령에 대응하는 기간임 -; 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령을 매칭하여 매칭 결과를 획득하며; 그리고 상기 매칭 결과에 기반하여, 상기 제2 유형 명령이 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 매칭하는 것으로 결정할 때, 상기 제2 유형 명령을 실행하도록 지시하는 지시 정보를 송신하도록 구성된다. 본 발명의 이 실시예는 차량 캐빈에서의 정제된 음성 명령 제어에 적용될 수 있다. 음성 명령이 명령 발신자의 아이덴티티 또는 권한이 결정된 후에만 실행될 수 있는 명령, 예를 들어, 차량의 주행 제어 명령, 또는 운전 모 드 전환, 운전 방향 제어, 통화 내역 조회와 같이, 프라이버시 정보에 관한 작동과 관련된 다른 명령인 것으로 인식할 때, 차량의 특수한 환경에 기반하여, 운전석의 멤버가 기본적으로 가장 높은 권한을 갖는다. 따라서, 발 신자의 아이덴티티를 확인해야 하는 명령을 획득할 때, 차량은 특정 포지션(예를 들어, 운전석)에 있는 멤버의 입술 움직임 정보와 인식된 특정 명령 정보를 획득하여, 명령이 운전석에 있는 멤버에 의해 송신된 것인지를 결 정하므로, 특정 포지션 영역에 대한 작동 제어가 타깃화된 방식으로 수행될 수 있다. 음성 제어 디바이스는 비 디오 데이터를 처리하고 비디오 데이터의 일부 특징과 명령 정보의 매칭도를 분석한다. 따라서 상기 디바이스는 로컬 디바이스일 수 있으며, 예를 들어, 지능형 차량 탑재 디바이스 또는 차량 탑재 칩, 마이크로폰과 카메라를 포함하는 차량 탑재 시스템, 지능형 차량일 수 있으며, 또는 클라우드 측의 클라우드 프로세서일 수 있다. 운전 석에 있는 멤버의 입술 움직임 정보를 수집하는 것 외에도, 프로세서는 시스템에 의해 미리 설정되거나 수동으 로 설정된 하나 이상의 위치에 있는 멤버에 기반하여 타깃화된 방식으로 수집을 수행하고, 이러한 명령이 인식 될 때 매칭 분석을 위해 대응하는 포지션에 있는 사람의 입술 움직임 정보를 획득할 수 잇다. 가능한 구현에서, 상기 제1 포지션은 운전석이다. 가능한 구현에서, 상기 제2 유형 명령은 차량 주행 제어 명령이다. 본 발명의 이 실시예의 디바이스는 임의의 사람 수량을 가지는 차량의 시나리오에 적용 가능하며, 특히 차량에 복수의 멤버가 있고 복수의 사람이 동시에 말할 때 음성이 인식되고 결정되는 시나리오에 적용 가능하다. 이 경 우, 프로세서는 획득된 차량 내 포지션 분포 정보 및 입술 움직임 정보와 명령 정보 사이의 매칭을 참조하여, 음성 명령이 특정 포지션에 있는 멤버가 보낸 것인지를 정확하게 판정할 수 있으며, 명령을 수행할지를 추가로 판정할 수 있다. 매칭 방식은 매칭을 위해 차량에서의 복수의 멤버(제1 포지션에 있는 멤버를 포함)를 획득하고, 제1 포지션에 있는 멤버가 가장 높은 매칭도를 갖는지를 판정하는 것일 수 있고; 또는 매칭을 위해 제1 포지션에 있는 멤버만을 획득하고, 매칭도가 임계값에 도달할 때 결과가 매칭이라고 결정하는 것일 수 있으 며, 명령을 실행할 수 있다. 명령이 제2 유형 명령인지를 판정하는 실시예에서, 프로세서가 특정 포지션을 결정해야 하는 경우는, 특정 포지 션에 있는 멤버만이 이러한 유형의 명령을 실행할 수 있는 권한을 갖는 시나리오에 대한 것이다. 예를 들어, 제 2 유형의 명령은 일반적으로 차량 제어 명령이며, 오작동을 방지하기 위해, 이러한 유형의 명령은 일반적으로, 음성을 사용하여 차량 주행 제어를 수행하는 능력을 가지는, 운전석과 같은 특정 포지션에 있는 멤버에게만 설 정된다. 공통 구현은 다음과 같다: 프로세서가 제2 명령 정보와의 매칭을 위해 운전석에 있는 사용자의 입술 움 직임 정보를 획득하고, 그 결과가 매칭될 때, 제2 유형 명령이 운전석에 있는 사용자에 의해 송신된 것으로 결 정하고, 제2 유형 명령을 실행한다. 일반적으로 운전석에 있는 사용자가 차량을 제어하는 권한을 갖는 것이 기본으로 간주된다. 다른 포지션에 있는 탑승자가 음성 제어 권한을 갖도록 수동으로 설정할 수 있으며, 제1 포지 션은 또한 차량의 다른 포지션이다. 가능한 구현에서, 상기 프로세서가 상기 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 제 1 유형 명령을 획득하는 것은 구체적으로, 상기 프로세서가 상기 차량 캐빈에서 상기 타깃 오디오 데이터를 획 득하고; 상기 타깃 오디오 데이터가 상기 제2 유형 명령을 포함하는 것으로 인식할 때, 상기 차량 캐빈에서 이 미지 데이터를 획득하며; 그리고 상기 차량 캐빈에서의 상기 이미지 데이터로부터 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하는 것이다. 전술한 솔루션의 구현 프로세스에서, 상기 프로세서가 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정 보와 상기 제2 유형 명령을 매칭하여 매칭 결과를 획득하는 것은 구체적으로, 상기 프로세서가 상기 제1 포지션 에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령 사이의 매칭도 및 미리 설정된 임계값에 기반 하여, 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령의 매칭 결과를 결정하는 것이다. 본 발명의 이 실시예에서 상기 제2 유형 명령은 상기 오디오 데이터로부터 추출된 음성 파형 시퀀스 또는 상기 오디오 데이터에 기반하여 인식된 텍스트 명령 정보일 수 있다. 차량 내 멤버의 입술 움직임 정보는 상기 타깃 기간에서의 차량 내 멤버의 입술 움직임의 이미지 시퀀스이다. 가능한 구현에서, 상기 프로세서는 추가로, 상기 오디오 데이터가 상기 제2 유형 명령을 포함할 때, 상기 차량 에서 다른 N개의 포지션에 있는 차량 내 멤버의 이미지 데이터를 획득하고; 그리고 상기 차량에서 다른 N개의 포지션에 있는 차량 내 멤버의 이미지 데이터로부터, 상기 타깃 기간에서의 차량에서 상기 다른 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하도록 구성되고; 상기 프로세서가 상기 차량에서 상기 제1 포지 션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 제2 유형 명령을 매칭하여 매칭 결과를 획득하는 것은 구체 적으로, 상기 프로세서가 상기 차량에서 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 상기 N개 의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 상기 제2 유형 명령과 매칭하여, (N+1)명의 차량 내 멤버 의 입술 움직임 정보와 상기 제2 유형 명령 사이의 매칭도를 획득하고, 매칭도가 가장 높은 입술 움직임 정보를 획득하는 것이며; 그리고 상기 매칭 결과에 기반하여, 상기 제2 유형 명령이 상기 차량에서 상기 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 매칭하는 것으로 결정할 때, 상기 프로세서가 상기 제2 유형 명령을 실행하도록 지시하는 지시 정보를 송신하는 것은 구체적으로, 상기 매칭도가 가장 높은 입술 움직임 정보가 상 기 차량에서 상기 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보일 때, 상기 프로세서가 상기 제2 유형 명령을 실행하도록 지시하는 지시 정보를 송신하는 것이다. 본 발명의 이 실시예에서, 차량에서 포지션에 있는 멤버의 입술 움직임 정보는 차량에서 상기 포지션에 있는 멤 버의 비디오 데이터로부터 추출된다. 구체적인 방법은 얼굴 인식 알고리즘에 기반하여 비디오 데이터에서 복수 의 얼굴 영역을 인식하고, 복수의 얼굴 영역 각각에서 입술 움직임 비디오를 추출하며, 각 얼굴 영역의 입술 움 직임 비디오에 기반하여, 각 얼굴에 대응하는 입술 움직임 정보를 결정하는 것이다. 제5 측면에 따르면, 본 발명의 실시예는 칩 시스템을 제공한다. 상기 칩 시스템은 적어도 하나의 프로세서, 메 모리 및 인터페이스 회로를 포함한다. 상기 메모리, 상기 인터페이스 회로 및 상기 적어도 하나의 프로세서는 라인을 통해 연결되며, 상기 적어도 하나의 메모리는 명령을 저장한다. 상기 명령은 상기 프로세서에 의해 실행 되어 제1 측면 또는 제2 측면 중 어느 하나의 방법을 구현한다. 제6 측면에 따르면, 본 발명의 실시예는 컴퓨터가 판독 가능한 저장 매체를 제공한다. 상기 컴퓨터가 판독 가능 한 매체는 프로그램 코드를 저장하도록 구성되고, 상기 프로그램 코드는 제1 측면 또는 제2 측면 중 어느 하나 의 방법을 포함한다. 제7 측면에 따르면, 본 발명의 실시예는 컴퓨터 프로그램을 제공한다. 상기 컴퓨터 프로그램은 명령을 포함하고, 상기 컴퓨터 프로그램이 실행될 때 상기 컴퓨터 프로그램은 제1 측면 또는 제2 측면의 임의의 방법을 구현하는 데 사용된다."}
{"patent_id": "10-2023-7002403", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 본 발명의 실시예에 있어서 첨부된 도면을 참조하여 본 발명의 실시예를 설명한다. 본 출원의 명세서, 특허청구범위 및 첨부된 도면에 있어서 \"제1\", \"제2\", \"제3\", \"제4\" 등의 용어는 서로 다른 타깃을 구별하기 위한 것이지 특정한 순서를 나타내는 것은 아니다. 또한, \"포함하는\" 및 \"갖는\" 및 이들의 다 른 변형 용어는 비배타적 포함을 포함하도록 의도된다. 예를 들어, 일련의 단계 또는 유닛을 포함하는 프로세스, 방법, 시스템, 제품 또는 디바이스는 나열된 단계 또는 유닛에 제한되지 않고, 선택적으로 나열되지 않은 단계 또는 유닛을 더 포함하거나, 선택적으로 프로세스, 방법, 제품 또는 디바이스의 또 다른 고유 단계 또는 유닛을 더 포함한다. 본 명세서에서 언급되는 \"실시예\"는 실시예를 참조하여 설명된 특정한 특징, 구조 또는 특징이 본 출원의 적어 도 하나의 실시예에 포함될 수 있음을 의미한다. 본 명세서의 다양한 위치에 기재된 문구는 반드시 동일한 실시 예를 지칭하는 것은 아니며, 다른 실시예와 배타적인 독립적이거나 선택적 실시예가 아니다. 본 명세서에 기재 된 실시예는 다른 실시예와 조합될 수 있음은 당업자에게 명시적 및 묵시적으로 이해될 것이다. 본 명세서에서 사용되는 \"컴포넌트\", \"모듈\", \"시스템\" 등의 용어는 컴퓨터와 관련된 개체, 하드웨어, 펌웨어, 하드웨어와 소프트웨어의 조합, 소프트웨어 또는 실행 중인 소프트웨어를 가리키는 데 사용된다. 예를 들어, 컴 포넌트는 프로세서, 프로세서, 객체, 실행가능한 파일, 실행 스레드(thread), 프로그램 및/또는 컴퓨터에서 실 행되는 프로세스일 수 있지만 이에 제한되지 않는다. 도면을 사용하여 예시된 바와 같이, 컴퓨팅 디바이스 및 컴퓨팅 디바이스에서 실행되는 애플리케이션은 모두 컴포넌트일 수 있다. 하나 이상의 컴포넌트는 프로세스 및/ 또는 실행 스레드 내에 상주할 수 있으며, 컴포넌트는 한 컴퓨터에 위치하거나 두 대 이상의 컴퓨터 간에 분산 될 수 있다. 또한, 이러한 컴포넌트는 다양한 데이터 구조를 저장하는 다양한 컴퓨터가 판독 가능한 매체에서 실행될 수 있다. 예를 들어, 컴포넌트는 로컬 및/또는 원격 프로세스를 사용하고 하나 이상의 데이터 패킷(예를 들어, 로컬 시스템의 다른 컴포넌트, 분산 시스템 및/또는 네트워크를 통해, 예를 들어, 신호를 사용하여 다른 시스템과 상호 작용하는 2개의 컴포넌트로부터의 데이터)을 가지는 신호에 기반하여, 통신할 수 있다. 본 출원의 일부 용어는 당업자의 이해를 돕기 위해 먼저 설명된다. 비트맵(Bitmap): 비트맵은 래스터 그래픽(Raster graphic) 또는 도트 행렬이라고도 하며, 픽셀 어레이 (Pixel-array/Dot-matrix 도트 행렬)로 표현되는 이미지이다. 비트맵은 비트 심도에 기반하여, 1비트, 4비트, 8 비트, 16비트, 24비트, 또는 32비트 이미지로 분류될 수 있다. 각 픽셀이 더 많은 비트의 정보를 사용할 때, 더많은 색상을 사용할 수 있고, 색상이 더 선명하게 표현되며, 그에 따라 데이터 양이 더 커짐을 지시한다. 예를 들어, 비트 심도가 1인 픽셀 비트맵은 2개의 가능한 값(흑과 백)만 가지며, 이에 따라 픽셀 비트맵을 이진 비트 맵이라고도 한다. 비트 심도가 8인 이미지에는 28(즉, 256)개의 가능한 값이 있다. 비트 심도가 8인 그레이스케 일(grayscale) 모드 이미지에는 256개의 가능한 회색 값이 있다. RGB 이미지는 세 가지 색상 채널을 포함한다. 8비트/채널의 RGB 이미지의 각 채널에는 256개의 가능한 값이 있다. 이는 이미지에 가능한 색상 값이 최소 1,600만 개 이상 있음을 의미한다. 8비트/채널(bpc)의 RGB 이미지는 때때로 24비트 이미지(8비트 × 3채널 = 24 비트 데이터/픽셀)라고도 한다. [2] 조합된 RGB 데이터의 24비트로 표현되는 비트맵은 일반적으로 트루 컬러 비 트맵(true color bitmap)이라고 한다. 음성 인식(Automatic Speech Recognition, ASR) 기술: 음성 인식은 자동 음성 인식이라고도 하며, 사람의 오디오에 있는 단어를 키(key), 이진 코드 또는 문자 시퀀스와 같은 컴퓨터가 판독 가능한 입력으로 변환하는 것을 목표로 한다. 성문(Voiceprint): 성문은 전기 음향 기기에 의해 디스플레이되면서 또한 언어 정보를 운반하는 음파(sound wave) 스펙트럼이다. 성문은 파장, 주파수, 강도 등 100개 이상의 특징 차원을 포함하는 생물학적 특징이다. 성 문 인식은 하나 이상의 음성 신호(speech signal)의 하나 이상의 특징을 분석하여 미지의 사운드(unknown sound)를 인식하는 기술이다. 성문 인식은 쉽게 말해 특정 사람이 말하는 문장인지 아닌지를 인식하는 기술이다. 성문에 기반하여 발언자(speaker)의 아이덴티티(identity)를 결정할 수 있으므로, 타깃화된 답변을 제공할 수 있다. 멜-주파수 켑스트럼 계수(Mel-Frequency Cepstrum Coefficient, MFCC): 사운드 처리 분야에서 멜-주파수 켑스트럼(Mel-Frequency Cepstrum)은 사운드 주파수의 비선형 멜 스케일(mel scale)에 기반한 로그 출력 스펙트 럼의 선형 변환이다. MFCC(Mel-frequency cepstrum coefficient)는 음성 인식 기능에 널리 적용된다. 다방향 교차 엔트로피 손실(Multi-way cross-Entropy Loss): 교차 엔트로피 손실은 두 확률 분포 사이의 거리를 기술한다. 낮은 교차 엔트로피 손실은 두 확률 분포 사이의 거리가 더 짧다는 것을 지시한다. 신경망: 신경망은 뉴런을 포함할 수 있다. 뉴런은 xs와 절편 1(intercept of 1)을 입력으로 하는 연산 유닛일 수 있다. 연산 유닛의 출력은 다음과 같다."}
{"patent_id": "10-2023-7002403", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "s=1, 2, ..., 또는 n이고, n은 1보다 큰 자연수이며, Ws는 xs의 가중치이고, b는 뉴런의 바이어스이며, f는 뉴 런의 활성화 함수(activation function)이고, 뉴런의 입력 신호를 출력 신호로 변환하기 위해 비선형 특성을 신 경망에 도입하는 데 사용된다. 활성화 함수의 출력 신호는 다음 콘볼루션 레이어의 입력으로 사용될 수 있다. 활성화 함수는 시그모이드 함수일 수 있다. 신경망은 많은 단일 뉴런을 함께 연결하여 형성된 네트워크이다. 구 체적으로, 뉴런의 출력은 다른 뉴런의 입력일 수 있다. 각 뉴런의 입력은 이전 레이어의 로컬 수용 필드(local receptive field)에 연결되어 로컬 수용 필드의 특징을 추출할 수 있다. 로컬 필드는 여러 뉴런을 포함하는 영 역일 수 있다. 심층 신경망: 심층 신경망(deep neural network, DNN)은 다층 신경망이라고도 하며, 은닉(hidden) 레이어가 많은 신경망으로 이해될 수 있다. 여기에는 \"많은(many)\"에 대한 특별한 메트릭이 없다. DNN은 서로 다른 레이어의 위치 (location)에 기반하여 나눠지며, DNN의 신경망은 입력 레이어, 은닉 레이어 및 출력 레이어의 세 가지 유형으 로 나눌 수 있다. 일반적으로, 제1 레이어는 입력 레이어이고, 마지막 레이어는 출력 레이어이며, 중간 레이어 는 은닉 레이어이다. 레이어가 완전히(fully) 연결된다. 구체적으로, i번째 레이어의 임의의 뉴런은 (i+1)번째 레이어의 임의의 뉴런과 확실히 연결된다. DNN이 복잡해 보이지만 실제로 DNN은 각 레이어에서의 작업(work) 측 면에서 복잡하지 않으며, 다음의 선형 관계식: 으로 간단하게 표현되고, 여기서 는 입력 벡터이 며, 는 출력 벡터이고, 는 편향(bias) 벡터이며, 는 가중치 행렬(계수라고도 함)이고, 는 활성화 함수이다. 각 레이어에서, 출력 벡터 는 입력 벡터 x에 대해 이러한 간단한 연산을 수행하는 것에 의해 획득된다. DNN에는 레이어가 많기 때문에, 계수 W와 편향 벡터 b도 많다. DNN에서 이러한 파라미터의 정의는 다 음과 같다: 계수 W가 예로 사용된다. 3-레이어 DNN에서, 제2 레이어의 제4 뉴런에서 제3 레이어의 제2 뉴런까지 의 선형 계수는 로 정의된다고 가정한다. 위 첨자(superscript) 3은 계수 W가 위치된 레이어를 지시하며, 아래 첨자는 출력 3-레이어 인덱스 2와 입력 2-레이어 인덱스 4에 대응한다. 결론적으로, (L-1)번째 레이어의 k 번째 뉴런에서 L번째 레이어의 j번째 뉴런까지의 계수는 로 정의된다. 입력 레이어에는 파라미터 W가 없다 는 점에 유의해야 한다. 심층 신경망에서 은닉 레이어가 많을수록 네트워크가 실제 세계의 복잡한 사례를 더 잘 설명할 수 있다. 이론적으로 파라미터가 더 많은 모델은 더 복잡하고, 더 큰 \"용량(capacity)\"을 가지며, 이는 모델이 더 복잡한 학습 태스크(task)를 완료할 수 있음을 의미한다. 심층 신경망을 트레이닝시키는 것은 가중치 행렬을 학습하는 프로세스이며, 트레이닝의 최종 목적은 트레이닝된 심층 신경망의 모든 레이어의 가중치 행렬 (많은 레이어에서 벡터 W로 형성된 가중치 행렬)을 획득하는 것이다. 콘볼루션 신경망: 콘볼루션 신경망(CNN, convolutional neuron network)은 콘볼루션 구조의 심층 신경망이다. 콘볼루션 신경망은 콘볼루션 레이어와 서브 샘플링 레이어를 포함하는 특징 추출기를 포함한다. 특징 추출기는 필터로 간주될 수 있다. 콘볼루션 프로세스는 트레이닝 가능한 필터와 입력 이미지 또는 콘볼루션 특징 맵(feature map)을 사용하 여 콘볼루션을 수행한다고 볼 수 있다. 콘볼루션 레이어는 콘볼루션 신경망에 있고 입력 신호에 대해 콘볼루션 처리가 수행되는 뉴런 레이어이다. 콘볼루션 신경망의 콘볼루션 레이어에서는 하나의 뉴런이 이웃 레이어의 일 부 뉴런에만 연결될 수 있다. 콘볼루션 레이어는 일반적으로 여러 개의 특징 맵을 포함하며, 각 특징 맵에는 사 각형으로 배열된 일부 뉴런이 포함될 수 있다. 동일한 특징 맵의 뉴런들은 가중치를 공유하며, 여기서 공유하는 가중치는 콘볼루션 커널(convolution kernel)이다. 가중치 공유는 이미지 정보 추출 방식이 위치와 무관하다는 의미로 이해될 수 있다. 여기에 내포된 원칙은 이미지의 일부 통계 정보는 다른 부분의 통계 정보와 동일하다는 것이다. 이는 한 부분에서 학습된 이미지 정보를 다른 부분에서도 사용할 수 있음을 의미한다. 따라서, 학습을 통해 획득된 동일한 이미지 정보를 이미지의 모든 위치에 사용할 수 있다. 동일한 콘볼루션 레이어에서, 서로 다른 이미지 정보를 추출하기 위해 복수의 콘볼루션 커널이 사용될 수 있다. 일반적으로 콘볼루션 커널의 수가 많을수록 콘볼루션 연산에 반영된 이미지 정보가 더 풍부함을 지시한다. 콘볼루션 커널은 임의의 크기의 행렬 형태로 초기화될 수 있다. 콘볼루션 신경망을 트레이닝시키는 프로세스에 서, 콘볼루션 커널은 학습을 통해 적절한 가중치를 획득할 수 있다. 또한, 가중치 공유로 인한 직접적인 이점은 콘볼루션 신경망의 레이어 간 연결이 줄어들고 과적합(overfitting) 위험이 낮아진다는 것이다. 손실 함수: 심층 신경망을 트레이닝시키는 프로세스에서, 심층 신경망의 출력이 실제로 예측해야 하는 값에 최대한 근접할 것으로 예상되기 때문에, 신경망의 현재 예측 값과 실제로 원하는 타깃 값을 비교할 수 있으며, 그런 다음 현재 예측 값과 타깃 값의 차이에 기반하여 각 신경망 레이어의 가중치 벡터를 업데이트한다(물론, 일반적으로 제1 업데이트 전의 프로세스가 있으며, 구체적으로, 심층 신경망의 모든 레이어에 대해 파라미터가 미리 구성된다). 예를 들어, 신경망의 예측값이 크면, 심층 신경망이 실제로 원하는 타깃 값 또는 실제 원하는 타깃값에 더 근접 한 값을 예측할 수 있을 때까지, 예측값을 낮추도록 가중치 벡터를 조정한다. 따라서, \"비교를 통해 예측값과 타깃값의 차이를 어떻게 구할 것인가\"가 미리 정의되어 있어야 한다. 이것은 손실 함수(loss function) 또는 목 적 함수(objective function)이다. 손실 함수와 목적 함수는 예측값과 타깃값의 차이를 측정하는 중요한 수식이 다. 손실 함수가 예로 사용된다. 손실 함수의 출력 값(손실)이 높을수록 차이가 크다는 것을 지시한다. 따라서, 심층 신경망의 트레이닝은 가능한 한 손실을 최소화하는 프로세스이다. 역전파 알고리즘: 콘볼루션 신경망은 역전파(back propagation, BP) 알고리즘을 사용하여 트레이닝 프로세스에서 콘볼루션 신경망 의 파라미터 값을 정제하므로(refine), 콘볼루션 신경망의 재구성 에러(error) 손실을 점점 더 작게 할 수 있다. 구체적으로, 입력 신호는 출력 중 에러 손실이 발생할 때까지 순방향으로 전달되고(transferred), 역전파 에러 손실 정보를 사용하여 콘볼루션 신경망의 파라미터를 업데이트하여 에러 손실이 수렴되도록 한다. 역전파 알고리즘은 에러 손실에 의해 지배되는 역전파 모션이며, 최적의 콘볼루션 신경망의 가중치 행렬과 같은 파라미터를 획득하기 위한 것이다. 픽셀 값: 비디오의 픽셀 값은 RGB(red green blue) 색상 값일 수 있으며, 픽셀 값은 색상을 나타내는 배장 정수(long integer)일 수 있다. 예를 들어, 픽셀 값은 256*Red + 100*Green + 76Blue이며, 여기서 Blue는 파란색 컴포넌 트를 나타내고, Green은 녹색 컴포넌트를 나타내며, Red는 빨간색 컴포넌트를 나타낸다. 각 색상 컴포넌트에서, 값이 작을수록 밝기가 낮음을 지시하고, 값이 클수록 밝기가 높음을 지시한다. 그레이스케일 이미지의 경우, 픽 셀 값은 그레이스케일 값일 수 있다. 먼저, 본 발명의 실시예에 대한 이해를 용이하게 하기 위하여, 본 출원에서 구체적으로 해결하고자 하는 기술적 과제를 추가적으로 분석하여 제공한다. 종래 기술에서, 차량에서의 다인 시나리오에서 검출된 음성 명령의 발신 자의 포지션을 인식하기 위한 구현이 복수일 수 있으며, 예를 들어, 차량 캐빈에서의 성문 인식 및/또는 사운드 소스 위치 파악(localization)을 통해 구현될 수 있다. 다음은 일반적으로 사용되는 두 가지 솔루션의 예이다. 솔루션 1 성문(Voiceprint)은 전기 음향 기기에 의해 디스플레이되면서 또한 언어 정보를 운반하는 음파 스펙트럼이다. 성문은 파장, 주파수, 강도 등 100개 이상의 특징 차원을 포함하는 생물학적 특징이다. 성문 인식은 하나 이상 의 음성 신호의 특징을 하나 이상 분석하여 미지의 사운드를 인식하는 기술이다. 성문 인식은 쉽게 말해 특정 사람이 말하는 문장인지 아닌지를 인식하는 기술이다. 성문에 기반하여 발언자의 아이덴티티를 결정할 수 있으 므로 타깃화된 답변을 제공할 수 있다. 등록 페이즈(registration phase)와 검증 페이즈의 두 페이즈가 주로 포 함된다. 등록 페이즈에서, 발언자의 음성의 성문 특징에 기반하여 대응하는 성문 모델을 구축한다. 검증 페이즈 에서는 발언자의 음성을 수신하여 음성의 성문 특징을 추출하고, 등록 페이즈에서 구축된 성문 모델과 성문 특 징을 매칭한다. 매칭에 성공하면, 발언자가 원래 등록된 발언자임을 지시한다. 솔루션 2 음원 위치 파악 기술은 음향(acoustics) 및 전자 장치를 사용하여 타깃 음장(sound field) 정보를 입력받아 타 깃 음원의 포지션을 결정하는 기술이다. 마이크로폰 어레이에 의한 음원 위치 파악시, 마이크로폰 어레이는 음 원 신호를 픽업하고, 복수 채널의 사운드 신호를 분석 및 처리하며, 하나 이상의 음원 평면 또는 공간 도메인에 서의 공간 좌표, 즉, 음원의 위치를 획득한다. 또한, 마이크로폰 어레이의 빔이 스피커를 향하도록 제어된다. 차량 캐빈에서 명령 발신자의 포지션 분석에 솔루션 1 및 솔루션 2를 적용하는 것의 단점 성문 인식을 적용하기 위해서, 탑승자의 성문 정보가 미리 저장되어 있어야 한다. 성문 인식 및 녹음을 하지 않 은 사람은 인식되지 않는다. 동일한 사람의 보이스(voice)는 휘발성이며, 차량이 다인 환경에 있고, 복수의 사 람이 동시에 말을 할 때, 성문 특징 추출이 어렵거나 큰 환경 소음이 인식에 방해가 된다. 음원 위치 파악 기술에 있어서, 차량은 상대적으로 좁고 붐비는 공간이며, 특히 뒷좌석 탑승자와의 공간 거리가 매우 가깝고, 멤버들이 말을 할 때 흔들리거나 기울어질 수 있다. 위와 같은 요인으로 인해 음원 위치 파악의 정확도가 떨어질 수 있다. 또한, 차량 캐빈에서 복수의 사람들이 동시에 말하는 경우가 주로 발생하는데, 이 경 우 음원 위치 파악 정확도에도 영향을 미친다. 결론적으로, 전술한 두 가지 솔루션을 차량에서 음성 명령 발신자의 포지션 인식에 적용한다면, 특히 복수의 사 람이 동시에 사운드를 내는 차량 내 시나리오에서 명령 발신자의 포지션의 인식에 적용하면, 수집된 명령에 대 응하는 차량 내 멤버의 특정 포지션을 정확하게 인식할 수 없다. 이것은 더 정확하고 효과적인 인간-기계 상호 작용을 구현할 수 없다. 따라서, 본 출원에서 해결하고자 하는 기술적인 문제는 다음 측면: 차량 캐빈에 복수의 사용자가 있을 때, 특정 유형의 명령이 수집되는 경우, 음성 발신자의 특정 포지션을 정확하게 결정하고 구체적 으로 대응하는 명령을 명령하는 방법을 포함한다. 본 출원의 실시예에서 제공되는 음성 매칭 방법은 지능형 차량의 인간-기계 상호 작용 시나리오에 적용될 수 있 다. 다음은 본 출원의 음성 명령 제어 방법이 적용되는 인간-기계 상호 작용 시나리오의 예를 제공한다. 다음 두 가지 시나리오가 포함될 수 있다. 차량 내 상호 작용 시나리오 1 일반적으로 복수의 스피커가 차량에 분산되어 있으며, 차량 캐빈의 서로 다른 포지션에 각각 분산되어 있다. 복 수의 스피커는 탑승자와 운전자의 요건에 따라 차량에서 서로 다른 영역의 탑승자에게 서로 다른 음량의 음악을제공할 수 있다. 예를 들어, 탑승자 A가 휴식을 취하고 싶다면 조용한 환경이 필요하므로 탑승자 A가 위치된 영 역의 스피커 볼륨을 가장 낮은 레벨로 조정할 수 있으며; 탑승자 B가 정상적으로 음악을 들을 필요가 있으면, 탑승자 B가 위치된 영역의 스피커 볼륨은 정상 값으로 설정될 수 있다. 다르게는, 복수의 스피커는 서로 다른 영역의 사용자에게 서로 다른 오디오 재생 콘텐츠를 제공할 수도 있다. 예를 들어, 어린이가 뒷줄에 있으면, 뒷 줄의 어린이를 위해 동화가 재생되도록 선택할 수 있으며; 그리고 앞줄의 운전자와 동승자가 팝 음악을 듣고 싶 다면 앞줄 스피커에서 팝 음악이 재생될 수 있다. 그러나, 본 발명의 실시예들은 음성 명령을 보내는 멤버가 위치된 영역에서 음성을 통해 스피커를 제어하는 방 법을 차량 캐빈에 있는 멤버에게 제공할 수 있다. 예를 들어, 도 1에 도시된 바와 같이, 차량에서 운전석, 조수 석, 왼쪽 뒷줄 좌석, 오른쪽 뒷줄 좌석을 차지하는 A, B, C, D의 4명의 사람이 있을 때, 이 경우, 멤버 D가 \"볼 륨을 낮추세요\"라고 말한다. 이 경우, 도 7a에 도시된 바와 같이, 차량 캐빈에서 카메라와 마이크로폰을 사용하 여 복수의 차량 내 멤버의 오디오 명령 명령 정보와 비디오 정보를 별도로 획득할 수 있으며, 차량 내 멤버의 입술 움직임 정보를 명령 정보의 특징과 매칭하여, 발언하는 멤버가 보낸 명령 및 그 포지션에 따라 발언하는 멤버 및 발언하는 멤버의 포지션에서의 스피커를 제어한다. 발언자가 오른쪽 뒤 탑승자로 인식하면, 오른쪽 뒤 영역의 스피커를 음소거한다. 멤버 C의 음성 명령이 \"노래 **** 나에게 들려줘\"이면, 차량 내 멤버의 오디오 명령 정보 및 비디오 정보도 차 량 캐빈에서 카메라와 마이크로폰을 사용하여 별도로 획득할 수 있으며, 차량 내 멤버 A, B, C, D의 입술 움직 임 정보와 음성 정보를 처리 및 분석하여, 발언하는 멤버가 멤버 C인 것으로 결정하고, 발언하는 멤버가 C가 보 낸 명령 및 발언하는 멤버의 포지션에 따라, 발언하는 멤버의 포지션에 있는 스피커를 제어한다. 발언하는 멤버 C가 왼쪽 뒤 탑승자인 것으로 인식하면, 왼쪽 뒷좌석 스피커를 제어하여 노래 ****를 재생한다. 유사한 애플리케이션 시나리오는 다음을 더 포함할 수 있다: 에어컨의 복수의 배기구가 차량에 분포되어 있고, 각각 차량 캐빈의 서로 다른 포지션에 분포되어 있다. 복수의 에어컨의 배기구는 탑승자와 운전자의 요건에 따 라 차량에서 서로 다른 영역의 탑승자에게 서로 다른 풍량을 제공함으로써, 일부 영역에서 차별화된 온도 조절 을 구현할 수 있다. 예를 들어, 탑승자 A의 온도가 낮으면, 탑승자 A가 위치된 영역의 풍량을 증가시키도록 선 택할 수 있고; 탑승자 B가 추위를 느끼면, 탑승자 B가 위치된 영역의 배기구의 공기 배출 방향을 사람에게 직접 불어주지 않도록 조정하거나 명령을 사용하여 풍량을 줄일 수 있다. 다르게는, 차량에서 좌석의 각도와 높이를 독립적으로 조정할 수 있을 때, 서로 다른 영역의 탑승자도 자신의 요건에 따라 좌석의 파라미터를 조정한다. 전술한 시나리오에서, 본 발명의 실시예에서의 음성 인식 방법을 사용함으로써 편리한 제어가 수행될 수 있다. 이와 유사하게, 차량 캐빈에서 카메라와 마이크로폰을 사용하여 차량 내 멤버의 오디오 명령 정보와 비디오 정 보를 별도로 획득할 수 있으며, 차량 내 멤버의 입술 움직임 정보와 보이스 정보를 처리 및 분석하여, 발언하는 멤버가 보낸 명령과 발언하는 멤버의 포지션에 기반하여, 발언하는 멤버의 포지션에 있는 에어컨의 배기구의 풍 량이나 공기 배출 방향, 또는 좌석의 각도 또는 좌석의 높이를 제어한다. 차량 내 상호 작용 시나리오 2 차량 캐빈에서 음성 명령에서의 전술한 시나리오에서 언급한 차량 내 설정에 대한 제어 외에도, 일부 차량 내 설비의 제어가 특정 명령이 구현되는 타깃 영역을 구별해야 하기 때문에, 음성 명령을 보낸 멤버의 포지션이 인 식되어야 한다. 전술한 시나리오 외에도, 운전자가 차량의 주행 제어를 수행해야 할 때, 운전자는 음성을 통해 차량의 주행 제어를 선택할 수도 있다. 이러한 음성 명령 상호 작용 시나리오에서는 현재 차량 제어 명령이 운 전석에 있는 멤버에 의해 보내지는지도 인식해야 한다. 따라서, 본 발명의 실시예는 차량 주행 제어를 위한 음성 명령 권한 인식 방법을 제공할 수 있다. 예를 들어, 차량에 복수의 사람이 있을 때, 차량 주행 제어와 관련된 음성 명령, 예를 들어, \"자동 운전 모드로 전환하세요\"가 수신되면, 차량 시스템은 기본적으로 운전석에 있는 멤버만 이러한 명령을 실행할 권한이 있는 것으로 간주한다. 이 경우, 차량은 도 2에 도시된 바와 같이 운전석에 있는 멤버의 입술 움직임 정보를 획득하 고, 도 7b에 도시된 바와 같이, 운전석에 있는 멤버의 획득된 입술 움직임 정보와 음성 명령 정보에 대해 특징 매칭을 수행하여, 입술 움직임 정보와 명령 정보 사이의 매칭도를 획득하며, 매칭도에 기반하여, 명령이 운전석 에 있는 멤버가 보낸 음성 명령인지를 판정하고 명령을 실행할지를 판정한다. 구체적으로, 명령이 운전석에 있는 멤버가 보낸 음성 명령인지를 판정하는 것은, 복수의 차량 내 멤버의 입술 움직임 정보를 획득하고, 입술 움직임 정보와 명령 정보 사이의 매칭도를 분석하며, 운전석에 있는 멤버의 입술 움직임 정보에 대응하는 매칭도가 가장 높은지를 확인하며(check), 추가로 명령 수행 여부를 판정한다.도 1 및 도 2의 차량 캐빈에서의 애플리케이션 시나리오는 본 발명의 실시예들에서 단지 몇 가지 예시적인 구현 들임을 이해할 수 있다. 본 발명의 실시예들은 구체적으로 다양한 유연한 구현으로 구현될 수 있다. 예를 들어, 시나리오 1에서, 차량 내 멤버 모두의 입술 움직임 정보를 획득할 필요는 없으며, 특정 명령 유형에 기반하여 일부 멤버의 입술 움직임 정보만 획득할 수 있으며, 예를 들어, 앞좌석만 조정할 수 있을 때, 좌석을 조정하기 위한 명령이 검출될 때 앞 줄에 있는 멤버의 입술 움직임 정보만 획득한다. 시나리오 2에서, 운전석에 있는 멤 버의 입술 움직임 정보를 획득할 필요가 없다. 차량은 기본적으로 차량 소유자가 인식된 명령의 작동 권한을 갖 는 것으로 간주할 때, 차량 소유자의 포지션을 획득하고 차량 소유자의 입술 움직임 정보를 추출하여, 명령이 차량 소유자가 보낸 것인지를 판정한다. 명령 정보는 차량 내 멤버의 입술 움직임 정보와 매칭되고, 모델 트레이닝 방식으로 모델을 획득할 수 있으며, 입술 움직임 정보와 명령 정보를 입력하여 대응하는 매칭도를 출력한다. 따라서, 다음은 본 출원에서 제공하는 방법을 모델 트레이닝 측과 모델 애플리케이션 측에서 설명한다. 본 출원에서 제공하는 임의의 신경망 트레이닝 방법은 컴퓨터 청각과 시각의 융합 처리에 관한 것으로, 구체적 으로 데이터 트레이닝, 기계 학습 또는 딥 러닝과 같은 데이터 처리 방법에 적용되어, 트레이닝 데이터(예를 들 어, 본 출원에서는 트레이닝 사용자의 입술 움직임 정보 및 M개의 음성 정보)에 대한 상징적이고 포맷적인 지능 정보 모델링, 추출, 전처리(preprocessing), 트레이닝 등을 수행한다. 최종적으로 트레이닝된 타깃 특징 매칭 모델을 획득한다. 또한, 트레이닝된 타깃 특징 매칭 모델은 본 출원에서 제공하는 임의의 음성 매칭 방법에 사 용될 수 있으며, 입력 데이터(예를 들어, 본 출원에서 N명의 사용자의 입술 움직임 정보 및 인식할 음성 정보 (to-be-recognized speech information))를 트레이닝된 타깃 특징 매칭 모델에 입력하여, 출력 데이터(예를 들 어, 본 출원에서 N명의 사용자의 입술 움직임 정보와 인식할 음성 정보 사이의 매칭도)를 획득한다. 본 출원의 실시예에서 제공되는 신경망 트레이닝 방법 및 음성 매칭 방법은 동일한 개념에 기반하여 생성된 발명이며, 시 스템의 두 부분 또는 전체 프로세스의 두 페이즈, 예를 들어, 모델 트레이닝 페이즈 및 모델 애플리케이션 페이 즈로 이해될 수도 있음을 유의해야 한다. 도 3은 본 발명의 실시예에 따른 시스템 아키텍처를 도시한다. 시스템 아키텍처에 도시된 바와 같이, 데이터 수집 디바이스는 트레이닝 데이터를 수집하도록 구성된다. 본 출원에서 데이터 수집 디바이스(16 0)는 마이크로폰과 카메라를 포함할 수 있다. 본 발명의 이 실시예에서, 트레이닝 데이터(즉, 모델 트레이닝 측 의 입력 데이터)는 비디오 샘플 데이터 및 음성 샘플 데이터, 즉 본 발명의 실시예에서 트레이닝 사용자의 입술 움직임 정보 및 M개의 음성 정보를 각각 포함할 수 있다. M개의 음성 정보는 트레이닝 사용자의 입술 움직임 정 보와 매칭되는 음성 정보를 포함할 수 있다. 예를 들어, 비디오 샘플 데이터는 트레이닝 사용자가 \"오늘 아주 좋은 날이다, 나들이 어디로 갈까요\"라는 음성을 보낼 때 입술 움직임의 이미지 시퀀스이고, 음성 샘플 데이터 는 트레이닝 사용자가 보낸 \"오늘 아주 좋은 날이다, 나들이 어디로 갈까요\"라는 음성의 음성 파형 시퀀스 및 (M-1)개의 다른 음성 파형 시퀀스(네거티브(negative) 오디오 샘플로 사용)를 포함한다. 비디오 샘플 데이터 및 오디오 샘플 데이터는 데이터 수집 디바이스에 의해 수집되거나 클라우드로부터 다운로드될 수 있다. 도 3 은 단지 예시적인 아키텍처를 도시하며, 제한을 구성하지 않는다. 또한, 데이터 수집 디바이스는 트레이닝 데이터를 데이터베이스에 저장한다. 트레이닝 디바이스는 데이터베이스에 유지되는 트레이닝 데 이터에 기반한 트레이닝을 통해 타깃 특징 매칭 모델/규칙(여기서 타깃 특징 매칭 모델은 본 발명의 실시예에서의 타깃 특징 매칭 모델, 예를 들어, 전술한 트레이닝 페이즈에서 트레이닝을 통해 획득된 모델, 또 는 음성과 입술 움직임 궤적 간의 특징 매칭에 사용되는 신경망 모델임)을 획득한다. 다음은 트레이닝 디바이스가 트레이닝 데이터에 기반하여 타깃 특징 매칭 모델/규칙을 획득하는 방법 을 더 상세히 설명한다. 타깃 특징 매칭 모델/규칙은 본 발명의 실시예에서 제공되는 임의의 음성 매칭 방 법을 구현하는 데 사용될 수 있으며, 즉, 데이터 수집 디바이스에 의해 획득된 오디오 데이터 및 이미지 데이터에 대해 관련된 처리가 수행된 후, 오디오 데이터 및 이미지 데이터를 타깃 특징 매칭 모델/규칙에 입력하여, 복수의 사용자의 입술 움직임의 이미지 시퀀스 특징과 인식할 음성 특징 사이의 매칭도/신뢰도를 획 득한다. 본 발명의 이 실시예에서 타깃 특징 매칭 모델/규칙은 구체적으로 시공간 콘볼루션 네트워크 (spatio-temporal convolutional network, STCNN)일 수 있다. 본 출원에서 제공되는 이 실시예에서, 시공간 콘 볼루션 네트워크는 콘볼루션 신경망을 트레이닝하는 것에 의해 획득될 수 있다. 실제 애플리케이션에서, 데이터 베이스에 유지되는 트레이닝 데이터는 반드시 데이터 수집 디바이스에 의해 모두 수집되는 것은 아니 며, 다른 디바이스로부터 수신될 수 있음을 유의해야 한다. 트레이닝 디바이스는 반드시 데이터베이스 에 유지되는 트레이닝 데이터에 전적으로 기반하여 타깃 특징 매칭 모델/규칙에 대한 트레이닝을 수 행할 필요는 없으며, 또는 모델 트레이닝을 위해 클라우드 또는 다른 장소로부터 트레이닝 데이터를 획득할 수있음을 유의해야 한다. 전술한 설명은 본 발명의 실시예에 대한 제한으로 해석되어서는 안 된다. 도 3에 도시된 바와 같이, 타깃 특징 매칭 모델/규칙은 트레이닝 디바이스에 의한 트레이닝을 통해 획득된다. 타깃 특징 매칭 모델/규칙은 본 출원의 실시 예에서 시청각 교차 콘볼루션 신경망(audio-visual cross convolutional neural network (V&A Cross CNN)/시공간 콘볼루션 신경망으로 지칭될 수 있다. 구체적으 로, 본 발명의 이 실시예에서 제공되는 타깃 특징 매칭 모델은 제1 모델, 제2 모델 및 제3 모델을 포함할 수 있 다. 제1 모델은 음성 특징을 추출하는 데 사용된다. 제2 모델은 복수의 사용자(본 출원에서는 N명의 사용자)의 입술 움직임의 이미지 시퀀스 특징을 추출하는 데 사용된다. 제3 모델은 N명의 사용자의 이미지 시퀀스 특징과 음성 특징 사이의 매칭도/신뢰도를 계산하는 데 사용된다. 본 발명의 이 실시예에서 제공되는 타깃 특징 매칭 모델에서, 제1 모델, 제2 모델 및 제3 모델은 각각 콘볼루션 신경망일 수 있다. 달리 말하면, 타깃 특징 매칭 모델/규칙은 전체 시공간 콘볼루션 신경망으로 볼 수 있으며, 시공간 콘볼루션 신경망은 복수의 독립적인 네트워크, 예를 들어, 제1 모델, 제2 모델, 제3 모델을 더 포함한다. 전술한 모델 트레이닝 및 실행 방식 외에, 본 발명의 실시예는 다른 모델 트레이닝 및 실행 솔루션을 사용하여 구현될 수 있다. 전술한 트레이닝 방법의 샘플 데이터 수집 소스와 동일하게, 본 발명의 이 실시예에서 트레이닝 데이터(즉, 모 델 트레이닝 측의 입력 데이터)는 비디오 샘플 데이터 및 음성 샘플 데이터, 즉 본 발명의 이 실시예에서 트레 이닝 사용자의 입술 움직임 정보 및 M개의 음성 정보 각각을 포함할 수 있다. 입술 움직임 정보는 서로 다른 사 용자의 다양한 음성 명령에 대응하는 입술 움직임 정보를 포함하고, 음성 정보는 서로 다른 사용자가 보낸 음성 명령을 포함한다. 선택적으로, 일부 네거티브 샘플, 즉 음성 명령이 아닌 서술(statement)에 대응하는 입술 움 직임 정보 및 음성 명령이 아닌 음성 정보도 포함될 수 있다. 여기서 음성 명령은 차량 탑재 시스템이 대응하는 음성 정보를 인식하고 응답할 수 있는 것을 의미하며, 키워드일 수도 있고 완전한 문장일 수도 있다. 비디오 샘 플 데이터 및 오디오 샘플 데이터는 데이터 수집 디바이스에 의해 수집될 수 있고, 클라우드로부터 다운로 드될 수 있거나, 제3자 데이터 홀더(holder)에 의해 제공될 수 있다. 또한, 데이터 수집 디바이스는 트레 이닝 데이터를 데이터베이스에 저장한다. 트레이닝 디바이스는 데이터베이스에 유지되는 트레이 닝 데이터에 기반하여 트레이닝을 통해 타깃 특징 매칭 모델/규칙(여기서 타깃 특징 매칭 모델은 본 발명의 실시예에서 타깃 특징 매칭 모델, 예를 들어, 전술한 트레이닝 페이즈에서 트레이닝을 통해 획득된 모델, 또는 음성과 입술 움직임 궤적 간의 특징 매칭에 사용되는 신경망 모델임)을 획득한다. 다음은 트레이닝 디바이스가 트레이닝 데이터에 기반하여 타깃 특징 매칭 모델/규칙을 획득하는 방법 을 더 상세히 설명한다. 타깃 특징 매칭 모델/규칙은 본 발명의 실시예에서 제공되는 임의의 음성 매칭 방 법을 구현하는 데 사용될 수 있으며, 즉, 데이터 수집 디바이스에 의해 획득된 오디오 데이터 및 이미지 데이터에 대해 관련된 처리가 수행된 후, 오디오 데이터 및 이미지 데이터를 타깃 특징 매칭 모델/규칙에 입력하여, 복수의 사용자의 입술 움직임의 이미지 시퀀스 특징과 인식할 음성 특징 사이의 매칭도/신뢰도를 획 득한다. 본 발명의 이 실시예에서 타깃 특징 매칭 모델/규칙은 특히 본 출원에서 제공되는 이 실시예에서 콘볼루션 네트워크(convolutional network, CNN)일 수 있다. 실제 애플리케이션에서 데이터베이스에 유지 되는 트레이닝 데이터는 반드시 데이터 수집 디바이스에 의해 모두 수집되는 것은 아니며, 다른 디바이스 로부터 수신될 수 있음을 유의해야 한다. 트레이닝 디바이스는 반드시 데이터베이스에 유지되는 트레 이닝 데이터에 전적으로 기반하여 타깃 특징 매칭 모델/규칙에 대한 트레이닝을 수행할 필요는 없으며, 모 델 트레이닝을 위해 클라우드 또는 다른 장소로부터 트레이닝 데이터를 획득할 수 있음을 유의해야 한다. 전술 한 설명은 본 발명의 실시예에 대한 제한으로 해석되어서는 안 된다. 도 3에 도시된 바와 같이, 타깃 특징 매칭 모델/규칙은 트레이닝 디바이스에 의한 트레이닝을 통해 획득된다. 구체적으로, 본 발명의 이 실시예에서 제공되는 타깃 특징 매칭 모델은 제1 모델 및 제2 모델을 포함 할 수 있다. 제1 모델은 음성 명령에 대한 매칭을 수행하여 음성 명령에 대응하는 명령 정보를 인식하는 데 사 용되며, 여기서 명령 정보는 구체적으로 명령 식별자 또는 명령의 텍스트 특징일 수 있다. 제2 모델은: N명의 사용자의 이미지 시퀀스 특징에 기반하여, 각각의 입술 움직임 정보에 대응하는 음성 명령에 대응하는 대응 관 계, 예를 들어, 매칭 및 출력될 수 있는 그것의 대응하는 명령의 식별자 및 매칭도를 인식하고; 마지막으로, 음 성 명령에 대응하는 명령 식별자, 각 사용자의 입술 움직임 정보에 대응하는 음성 식별자 및 이들의 매칭도에 기반하여, 음성 명령을 보내는 타깃 사용자를 출력한다. 본 발명의 이 실시예에서 제공되는 타깃 특징 매칭 모 델에서, 제1 모델 및 제2 모델은 각각 CNN, RNN, DBN, DNN 등일 수 있다. 제1 모델은 레이블로서의 입력 음성 명령에 대응하는 명령 식별자(식별자는 코드로 표현될 수 있음)로 음성 명 령을 사용하여 학습된다. 제2 모델은 사용자의 입술 움직임 정보(입술 움직임 정보는 구체적으로 입술 움직임 비디오 시퀀스 특징일 수 있으며, 예를 들어, 시간에 따라 샘플링된 입술 개폐 진폭(lip opening and closing amplitude)은 벡터 시퀀스일 수 있음)를 입력으로 사용하고, 입술 움직임 정보와 이의 매칭도에 대응하는 명령 식별자를 출력으로 사용하며, 여기서, 명령 식별자는 명령에 대응되는 코드일 수 있고, 매칭도는 출력 매칭 값 일 수 있다. 결과가 매칭하는지는 매칭 값에 기반하여 결정된다. 예를 들어, 값이 0.5보다 크면 결과가 매칭하 고, 값이 0.5보다 작으면 결과가 매칭하지 않는다. 트레이닝 디바이스에 의한 트레이닝을 통해 획득된 타깃 특징 매칭 모델/규칙은 상이한 시스템 또는 디바이스에 적용될 수 있으며, 예를 들어, 도 4에 도시된 실행 디바이스에 적용될 수 있다. 실행 디바이스 는 모바일폰 단말, 태블릿 컴퓨터, 노트북 컴퓨터, 증강 현실(Augmented Reality, AR)/가상 현실(Virtual Reality, VR) 디바이스, 스마트 웨어러블 디바이스, 스마트 로봇, 차량 탑재 단말 또는 지능형 조종실 환경과 같은 단말일 수 있거나, 서버, 클라우드 등일 수 있다. 도 4에서, I/O 인터페이스는 실행 디바이스에 구성되며, 외부 디바이스와 데이터를 교환하는데 사용된다. 사용자는 클라이언트 디바이스를 사용하여 I/O 인터페이스에 데이터를 입력할 수 있다(본 출원의 클라이언트 디바이스는 마이크로폰 및 카메라와 같은 데 이터 수집 디바이스도 포함할 수 있음). 본 발명의 이 실시예에서, 입력 데이터(즉, 모델 애플리케이션 측의 입 력 데이터)는: N명의 사용자의 입술 움직임 정보 및 인식할 음성 정보, 즉 본 발명의 실시예에서 N명의 사용자 의 입술 움직임 정보 중 사용자의 입술 움직임 정보에 포함된 타깃 기간에서의 대응하는 사용자의 입술 움직임 의 이미지 시퀀스 및 타깃 기간에서의 음성 파형 시퀀스를 포함할 수 있다. 예를 들어, \"내일 날씨는 어때? 나 들이 가기 좋은 곳은?\"의 음성 정보를 보내는 특정 사람을 사람 그룹으로부터 인식해야 할 때, \"내일 날씨는 어 때? 나들이 가기 좋은 곳은?\"에 대응하는 음성 파형 시퀀스 및 모든 사람에 대응하는 입술 움직임의 이미지 시 퀀스가 입력 데이터로 사용된다. 여기에서 입력 데이터는 사용자에 의해 입력될 수 있거나, 관련 데이터베이스 에 의해 제공될 수 있으며, 특정 애플리케이션 시나리오에 따라 달라질 수 있음을 이해할 수 있다. 이는 본 발 명의 이 실시예에서 특별히 제한되지 않는다. 본 발명의 이 실시예에서, 클라이언트 디바이스 및 실행 디바이스는 동일한 디바이스에 있을 수 있으 며; 데이터 수집 디바이스, 데이터베이스 및 트레이닝 디바이스는 또한, 실행 디바이스 및 클라이언트 디바이스와 동일한 디바이스에 있을 수 있다. 예를 들어, 본 출원은 로봇에 의해 수행된다. 클 라이언트 디바이스(마이크로폰, 카메라, 프로세서를 포함)가 수집한 오디오 데이터 및 이미지 데이터를 추 출하여 N명의 사용자의 입술 움직임 정보 및 음성 인식 정보를 획득한 후, 로봇은 추가로, 로봇 내부의 실행 디 바이스를 사용하여, 추출된 음성 정보와 입술 움직임 정보 간의 특징 매칭을 수행하고, 최종적으로 클라이 언트 디바이스에 결과를 출력함으로써, 클라이언트 디바이스 내의 프로세서가 분석을 통해 N명의 사 용자 중 인식할 음성 정보가 속하는 타깃 사용자를 획득한다. 또한, 모델 트레이닝 측의 디바이스(데이터 수집 디바이스, 데이터베이스 및 트레이닝 디바이스)는 로봇 또는 클라우드에 위치될 수 있다. 디바 이스가 로봇에 위치될 때, 로봇에 모델 트레이닝 기능 또는 모델 업데이트 및 최적화 기능이 있다고 볼 수 있다. 이 경우, 로봇은 모델 트레이닝 측의 기능뿐만 아니라 모델 애플리케이션 측의 기능도 갖는다. 디바이스 가 클라우드에 위치될 때, 로봇은 모델 애플리케이션 측의 기능만 갖는다. 선택적으로, 클라이언트 디바이스 와 실행 디바이스는 동일한 디바이스에 있지 않을 수 있으며, 즉, 클라이언트 디바이스(예를 들 어, 스마트폰 또는 지능형 로봇)는 오디오 데이터 및 이미지 데이터를 수집하고, 인식할 음성 정보 및 N명의 사 용자의 입술 움직임 정보를 추출할 수 있으며, 실행 디바이스(예를 들어, 클라우드 서버 또는 서버)는 인 식할 음성 정보와 N명의 사용자의 입술 움직임 정보 간의 특징 매칭을 수행하는 프로세스를 수행할 수 있고; 또 는 선택적으로, 클라이언트 디바이스는 오디오 데이터 및 이미지 데이터를 수집하고, 실행 디바이스 는 N명의 사용자의 입술 움직임 정보 및 인식할 음성 정보를 추출하고, 인식할 음성 정보와 N명의 사용자의 입 술 움직임 정보 간의 특징 매칭을 수행하는 프로세스를 수행한다. 도 3에 도시된 경우에, 사용자는 수동으로 입력 데이터를 제공할 수 있다. 입력 데이터는 I/O 인터페이스 를 사용하여 제공되는 화면에 수동으로 제공될 수 있다. 다른 경우에, 클라이언트 디바이스는 입력 데이터 를 I/O 인터페이스에 자동으로 송신할 수 있다. 클라이언트 디바이스가 입력 데이터를 자동으로 송신 하기 위해 사용자로부터 승인을 얻어야 하면, 사용자는 클라이언트 디바이스에 대응하는 권한을 설정할 수 있다. 사용자는 클라이언트 디바이스 상에서, 실행 디바이스에 의해 출력된 결과를 볼 수 있다. 구체 적으로, 결과를 디스플레이하거나 사운드, 액션 등의 형태로 제시할 수 있다. 클라이언트 디바이스는 또한, I/O 인터페이스에 입력될 입력 데이터 및 I/O 인터페이스로부터 출력될 출력 결과를 수집하기 위해 데이터 수집단(예를 들어, 마이크로폰 또는 카메라)으로 사용될 수 있으며, 도면에 도시된 바와 같이, 입 력 데이터 및 출력 결과를 신규 샘플 데이터로 사용하고, 신규 샘플 데이터를 데이터베이스에 저장한다.클라이언트 디바이스가 다르게는 수집을 수행하지 않을 수 있다는 것은 명백하다. 대신에, I/O 인터페이스 는 도면에서 I/O 인터페이스에 입력된 입력 데이터 및 I/O 인터페이스로부터 출력된 출력 결과 를 신규 샘플 데이터로서 데이터베이스에 직접 저장한다. 전처리 모듈은 I/O 인터페이스를 통해 수신된 입력 데이터(예를 들어, 음성 데이터)를 전처리하도록 구성된다. 본 발명의 이 실시예에서, 전처리 모듈은 오디오 데이터를 전처리하도록 구성될 수 있으며, 예 를 들어, 오디오 데이터로부터 인식할 음성 정보를 추출한다. 전처리 모듈은 I/O 인터페이스를 통해 수신된 입력 데이터(예를 들어, 이미지 데이터)를 전처리하도 록 구성된다. 본 발명의 이 실시예에서, 전처리 모듈은 이미지 데이터를 전처리하도록 구성될 수 있으며, 예를 들어, 이미지 데이터로부터 인식할 음성 정보에 대응하는 N명의 사용자의 입술 움직임 정보를 추출한다. 실행 디바이스가 입력 데이터를 전처리하는 프로세스에서 또는 실행 디바이스 내의 계산 모듈이 계산 등의 관련 처리를 수행하는 프로세스에서, 실행 디바이스는 대응하는 처리를 위해 데이터 저장 시스 템에서 데이터, 코드 등을 호출할 수 있으며, 추가로, 대응하는 처리를 통해 획득된 데이터, 명령 등을 데 이터 저장 시스템에 저장할 수 있다. 마지막으로, I/O 인터페이스는 출력 결과, 예를 들어, N명의 사 용자의 입술 움직임 정보와 인식할 음성 정보 사이의 매칭도 또는 본 실시예에서 매칭도가 가장 높은 타깃 사용 자 ID를 본 발명의 실시예에서의 클라이언트 디바이스에 반환한다. 클라이언트 디바이스는 매칭도에 기반하여 타깃 사용자의 사용자 정보를 결정하고, 사용자 정보에 기반하여 사용자 정보에 매칭되는 제어 명령을 생성한다. 트레이닝 디바이스는 상이한 트레이닝 데이터에 기반하여 상이한 타깃 또는 상이한 태스크에 대한 대응하 는 타깃 특징 매칭 모델/규칙을 생성할 수 있다는 점에 유의해야 한다. 대응하는 타깃 특징 매칭 모델/규 칙은 사용자에게 필요한 결과를 제공하기 위해, 전술한 타깃을 구현하거나 전술한 태스크를 완료하는 데 사용될 수 있다. 도 4는 본 발명의 실시예에 따른 시스템 아키텍처의 개략도일 뿐임을 유의해야 한다. 도면에 도시된 디바이스, 컴포넌트, 모듈 등의 포지션 관계는 어떠한 제한도 구성하지 않는다. 예를 들어, 도 4에서, 데이터 저장 시스템 은 실행 디바이스에 대한 외부 저장 디바이스이며, 다른 경우 데이터 저장 시스템은 다르게는 실행 디바이스에 배치될 수 있다. 이하에서는 시스템 아키텍처에 대한 설명을 바탕으로, 본 발명의 실시예에서 신경망 모델, 즉 콘볼루션 신경망 을 모델 트레이닝 및 모델 애플리케이션의 관점에서 설명한다. 콘볼루션 신경망(convolutional neural network, CNN)은 콘볼루션 구조의 심층 신경망으로 딥러닝(deep learning) 아키텍처이다. 딥 러닝 아키텍처는 기계 학습 알고리즘을 사용하여 서로 다른 추상화 레이어(abstract layer)에서 복수 레벨의 학습을 수행한다. 딥 러닝 아 키텍처로서 CNN은 피드 포워드(feed-forward) 인공 신경망이다. 피드 포워드 인공 신경망의 뉴런은 CNN에 입력 된 이미지의 중첩 영역에 응답한다. 도 4는 본 발명의 실시예에 따른 콘볼루션 신경망의 개략도이다. 콘볼루션 신경망(convolutional neural network, CNN)은 입력 레이어, 콘볼루션 레이어/풀링 레이어 및 신경망 레이어를 포함할 수 있다. 풀링 레이어는 선택 사항이다. 콘볼루션 레이어/풀링 레이어 도 4에 도시된 바와 같이, 콘볼루션 레이어/풀링 레이어는 레이어들(221~226)을 포함할 수 있다. 일 구현 에서, 레이어는 콘볼루션 레이어이고, 레이어는 풀링 레이어이며, 레이어는 콘볼루션 레이어이 고, 레이어는 풀링 레이어이며, 레이어는 콘볼루션 레이어이고, 레이어는 풀링 레이어이다. 다 른 구현에서, 레이어(221 및 222)는 콘볼루션 레이어이고, 레이어는 풀링 레이어이며, 레이어(224 및 22 5)는 콘볼루션 레이어이고, 레이어는 풀링 레이어이다. 달리 말하면, 콘볼루션 레이어의 출력을 다음 풀링 레이어의 입력으로 사용하거나, 다른 콘볼루션 레이어의 입력으로 사용하여, 콘볼루션 연산을 계속 수행할 수 있다. 콘볼루션 레이어 콘볼루션 레이어를 예로 사용한다. 콘볼루션 레이어는 복수의 콘볼루션 연산자를 포함할 수 있다. 콘 볼루션 연산자는 커널이라고도 한다. 이미지 처리에서, 콘볼루션 연산자는 입력 이미지 행렬로부터 특정 정보를 추출하는 필터로서 기능한다. 콘볼루션 연산자는 본질적으로 가중치 행렬일 수 있으며, 가중치 행렬은 일반적으로 미리 정의된다. 이미지에 대해 콘볼루션 연산을 수행하는 프로세스에서, 일반적으로 가중치 행렬은 입력 이 미지에 대해 수평 방향으로 1 픽셀(또는 스트라이드(stride) 값에 따라 2 픽셀)의 그래뉼래러티 레벨에서 픽셀 을 처리하여, 이미지로부터 특정 특징을 추출하는 데 사용된다. 가중치 행렬의 크기는 이미지의 크기와 관련이 있어야 한다. 가중치 행렬의 깊이 차원(depth dimension)은 입력 이미지의 깊이 차원과 동일함을 유의해야 한다. 콘볼루션 연산 중에 가중치 행렬은 입력 이미지의 전체 깊이로 확장된다. 따라서, 단일 가중치 행렬로 콘 볼루션을 수행하여 단일 깊이 차원의 콘볼루션 출력을 생성한다. 그러나, 대부분의 경우, 단일 가중치 행렬이 아닌 동일한 차원의 복수의 가중치 행렬을 사용한다. 가중치 행렬의 출력은 누적되어 콘볼루션 이미지의 깊이 차원을 형성한다. 서로 다른 가중치 행렬을 사용하여 이미지의 서로 다른 특징을 추출할 수 있다. 예를 들어, 하나의 가중치 행렬은 이미지의 에지(edge) 정보를 추출하는 데 사용되고, 다른 가중치 행렬은 이미지의 특정 색상을 추출하는 데 사용되며, 또 다른 가중치 행렬은 이미지의 불필요한 노이즈를 흐리게 하는(blur) 데 사용 된다. 복수의 가중치 행렬은 동일한 차원을 가지기 때문에, 동일한 차원의 복수의 가중치 행렬을 사용하여 추출 한 특징 맵도 동일한 차원을 갖는다. 그런 다음 동일한 차원의 추출된 복수의 특징 맵을 결합하여 콘볼루션 연 산의 출력을 형성한다. 실제 애플리케이션에서, 이러한 가중치 행렬에서 가중치 값을 획득하기 위해서는 많은 트레이닝이 필요하다. 트 레이닝을 통해 획득된 가중치를 사용하여 형성된 가중치 행렬은 콘볼루션 신경망이 정확한 예측을 할 수 있도록 입력 비디오에서 정보를 추출하는데 사용될 수 있다. 콘볼루션 신경망이 복수의 콘볼루션 레이어를 포함할 때, 일반적으로 초기 콘볼루션 레이어(예를 들어, 221개)에서 비교적 많은 양의 일반 특징이 추출된다. 일반 특징은 로우 레벨 특징(low-level feature)이라고도 한다. 콘볼루션 신경망의 깊이가 깊어질수록, 특징, 예를 들어 후속 콘볼루션 레이어(예를 들어, 226)에서 추출되는 하이 레벨 시맨틱(semantic) 특징이 복잡해진다. 더 높은 레벨의 시맨틱을 가진 특징은 해결해야 할 문제에 더 적합하다. 풀링 레이어 일반적으로 트레이닝 파라미터의 수량을 줄여야 하기 때문에, 일반적으로 풀링 레이어는 콘볼루션 레이어 이후 에 주기적으로 도입해야 한다. 구체적으로, 도 4에 도시된 콘볼루션 레이어/풀링 레이어의 레이어 (221~226)에 대해, 하나의 콘볼루션 레이어 다음에 하나의 풀링 레이어가 올 수도 있고, 복수의 콘볼루션 레이 어 뒤에 하나 이상의 풀링 레이어가 올 수도 있다. 이미지 처리 중에, 풀링 레이어는 이미지의 공간 크기를 줄 이는 데만 사용된다. 풀링 레이어는 평균 풀링 연산자 및/또는 최대 풀링 연산자를 포함하여, 입력 픽처에 대해 샘플링을 수행하여 상대적으로 작은 크기의 픽처를 획득할 수 있다. 평균 풀링 연산자는 특정 범위 내에서 이미 지의 픽셀 값을 계산하여 평균 값을 생성할 수 있다. 최대 풀링 연산자는 특정 범위에서 최대 값을 갖는 픽셀을 최대 풀링 결과로 선택하는 데 사용될 수 있다. 또한, 콘볼루션 레이어에서 가중치 행렬의 크기가 이미지의 크 기와 관련되어야 하는 것과 마찬가지로, 풀링 레이어의 연산자도 이미지의 크기와 관련되어야 한다. 풀링 레이 어로부터 출력되는 처리된 이미지의 크기는 풀링 레이어에 입력되는 이미지의 크기보다 작을 수 있다. 풀링 레 이어로부터 출력된 이미지의 각 픽셀은 풀링 레이어에 입력된 이미지의 대응하는 서브 영역의 평균값 또는 최대 값을 나타낸다. 신경망 레이어 콘볼루션 레이어/풀링 레이어에서 수행된 처리 이후에, 콘볼루션 신경망은 원하는 출력 정보를 출력 할 준비가 되어 있지 않다. 위에서 설명한 바와 같이, 콘볼루션 레이어/풀링 레이어에서는 특징만 추출하 고 입력 비디오에서 가져온 파라미터를 감소시킨다. 그러나, 최종 출력 정보(원하는 클래스 정보 또는 기타 관 련 정보)를 생성하기 위해서, 콘볼루션 신경망은 신경망 레이어을 사용하여 원하는 클래스 또는 원하 는 클래스 그룹의 출력을 생성해야 한다. 따라서, 신경망 레이어는 복수의 은닉 레이어(도 4의 231, 232, ..., 23n) 및 출력 레이어를 포함할 수 있다. 복수의 은닉 레이어에 포함된 파라미터는 특정 태스크 유형 의 관련된 트레이닝 데이터에 대한 사전 트레이닝을 통해 획득될 수 있다. 예를 들어, 태스크 유형은 이미지 인 식, 이미지 분류, 초고해상도 이미지 재구성 등을 포함할 수 있다. 신경망 레이어에서, 복수의 은닉 레이어 뒤에는 출력 레이어 즉, 전체 콘볼루션 신경망의 마지 막 레이어가 온다. 출력 레이어는 범주형 교차 엔트로피 손실과 유사한 손실 함수를 가지며, 손실 함수는 특히 예측 에러를 계산하는 데 사용된다. 전체 콘볼루션 신경망의 순방향 전파(도 4의 210에서 240 방향으 로의 전파)가 완료되면, 역전파(도 4의 240에서 210 방향으로의 전파)를 시작하여 위에서 언급한 각 레이어의 가중치와 편차를 업데이트하여, 출력 레이어를 통한 콘볼루션 신경망이 출력한 결과와 이상적인 결과 사이의 에러 및 콘볼루션 신경망의 손실을 감소시킨다. 도 4에 도시된 콘볼루션 신경망은 콘볼루션 신경망의 예로서만 사용됨을 유의한다. 특정 애플리케이션에서, 콘볼루션 신경망은 다른 네트워크 모델의 형태, 예를 들어, 복수의 병렬 콘볼루션 레이어/풀 링 레이어로 존재할 수 있으며, 추출된 특징은 모두 전체 신경망 레이어에 입력되어 처리된다. 본 출원의 정규화 레이어는 CNN의 함수 레이어(function layer)로 사용된다. 원칙적으로, 정규화 레이어는 CNN 의 전술한 레이어 중 임의의 레이어 이후 또는 이전에 구현될 수 있다. 이전 레이어로부터 출력된 특징 행렬은 정규화 레이어의 입력으로 사용되며, 정규화 레이어의 출력은 CNN의 임의의 함수 레이어의 입력으로 사용될 수 도 있다. 그러나, CNN의 실제 애플리케이션에서, 정규화 레이어는 일반적으로 콘볼루션 레이어 이후에 구현된다. 콘볼루션 레이어로부터 출력된 특징 행렬이 입력 행렬로 사용된다. 도 3 및 도 4의 콘볼루션 신경망의 시스템 아키텍처 및 관련 기능의 전술한 설명에 기반하여, 다음은 전술한 애플리케이션 시나리오, 시스템 아키텍처, 콘볼루션 신경망의 구조 및 신경망 프로세서의 구조를 참조하 여, 본 출원에서 제공하는 신경망 트레이닝 방법 및 음성 매칭 방법의 실시예를 모델 트레이닝 측 및 모델 애플 리케이션 측에서 설명하며, 그리고 본 출원에서 제안된 기술적 문제에 대한 구체적인 분석 및 솔루션을 제공한 다. 도 5는 본 발명의 실시예에 따른 신경망 트레이닝 방법의 개략적인 흐름도이다. 상기 방법은 도 1 및 도 2의 애 플리케이션 시나리오에 적용될 수 있으며, 시스템 아키텍처는 특히 도 3의 트레이닝 디바이스에 적용될 수 있다. 이하에서는 실행 주체가 도 3의 트레이닝 디바이스 또는 트레이닝 디바이스를 포함하는 디바이 스인 예를 사용하여 도 5를 참조하여 설명한다. 상기 방법은 다음의 단계(S701 및 S702)를 포함할 수 있다. S701: 트레이닝 샘플을 획득하며, 트레이닝 샘플은 트레이닝 사용자의 입술 움직임 정보 및 M개의 명령 정보를 포함한다. 구체적으로, 예를 들어, 트레이닝 사용자의 입술 움직임 정보는 사용자 Xiaofang이 보낸 \"안녕하세요, 제 이름 은 Xiaofang입니다. 저는 중국 후난성 출신입니다. 당신은 어떤가요?\"인 음성 정보이다. 대응하는 입술 움직임 정보는 입술 움직임 비디오나 연속적인 입술 움직임의 이미지 시퀀스, 또는 시간 시퀀스 관계에 따라 입술을 열 고 닫는 움직임을 반영할 수 있는 위-아래 입술 거리로 구성된 벡터 파라미터이다. M개의 명령 정보는 명령 샘 플로서 \"에어컨 온도를 높여줘\"의 명령 정보 및 네거티브 샘플로서의 다른 명령 정보, 예를 들면 \"좌석 등받이 각도를 낮춰줘\", \"자동차 창을 열어줘\" 및 \"음악을 꺼줘\"라는 음성 정보의 파형 시퀀스 또는 텍스트 정보를 포 함한다. 선택적으로, M개의 명령 정보는 트레이닝 사용자의 입술 움직임 정보와 매칭하는 명령 정보 및 트레이 닝 사용자의 입술 움직임 정보와 매칭하지 않는 (M-1)개의 명령 정보를 포함한다. 예를 들어, 입술 움직임 정보 는 사용자 A가 보낸 \"에어컨 온도를 높여줘\"라는 명령 정보에 대응하는 연속적인 입술 움직임의 이미지 시퀀스 (즉, 발음 입 모양의 비디오)이고, M개의 명령 정보는 포지티브(positive) 음성 샘플의 음성 파형 시퀀스와 (M - 1)개의 네거티브 샘플의 음성 파형 시퀀스를 포함한다. M개의 명령 정보는 다르게는 복수의 포지티브 샘플과 복수의 네거티브 샘플을 교대로 포함할 수 있음을 이해할 수 있다. 달리 말하면, 포지티브 샘플과 네거티브 샘 플이 모두 포함되는 한, 포지티브 샘플의 수량과 네거티브 샘플의 수량은 특별히 한정되지 않는다. S702: 트레이닝 사용자의 입술 움직임 정보와 M개의 음성 정보를 트레이닝 입력으로 사용하고, 트레이닝 사용자 의 입술 움직임 정보와 M개의 음성 정보 사이의 매칭도를 M개의 레이블로 사용하여, 초기화된 신경망을 트레이 닝하여 타깃 특징 매칭 모델을 획득한다. 구체적으로, 예를 들어, 트레이닝 사용자의 입술 움직임 정보와 포지티브 샘플의 \"에어컨 온도를 높여줘\"라는 명령 정보 사이의 레이블은 \"매칭도 = 1\"이다. 트레이닝 사용자의 입술 움직임 정보와 네거티브 샘플의 \"좌석 등받이 각도를 낮춰줘\", \"자동차 창을 열어줘\", \"음악을 꺼줘\" 등의 명령 정보 사이의 레이블은 \"매칭도 = 0.2\", \"매칭도 = 0\", \"매칭도 = 0\" 등이다. 자세한 내용은 여기서 다시 설명하지 않는다. 즉, 트레이닝 입력과 미리 설정된 레이블을 사용하여, 초기화된 신경망 모델을 트레이닝하여, 본 출원에서 사용해야 하는 타깃 특징 매칭 모델을 획득할 수 있다. 타깃 특징 매칭 모델은 인식할 명령 정보와 복수의 사용자의 입술 움직임 정보 사 이의 매칭 관계를 매칭하는데 사용될 수 있으며, 이로써 본 출원에서 임의의 음성 매칭 방법을 구현한다. 가능한 구현에서, 트레이닝 사용자의 입술 움직임 정보와 M개의 음성 정보를 트레이닝 입력으로 사용하고, 트레 이닝 사용자의 입술 움직임 정보와 M개의 음성 정보 사이의 매칭도를 M개의 레이블로 사용하여, 초기화된 신경 망을 트레이닝하여 타깃 특징 매칭 모델을 획득하는 것은: 트레이닝 사용자의 입술 움직임 정보와 M개의 명령 정보를 초기화된 신경망에 타이핑하여, 계산을 통해 M개의 명령 정보와 트레이닝 사용자의 입술 움직임 정보의매칭도를 획득하고; 트레이닝을 통해 획득된 것이면서 또한 M개의 명령 정보와 트레이닝 사용자의 입술 움직임 정보 사이의 매칭도를 M개의 레이블과 비교하며, 초기화된 신경망을 트레이닝하여 타깃 특징 매칭 모델을 획득 하는 것을 포함한다. 가능한 구현에서, 타깃 특징 매칭 모델은 제1 모델, 제2 모델 및 제3 모델을 포함한다. 트레이닝 사용자의 입술 움직임 정보와 M개의 명령 정보를 초기화된 신경망에 타이핑하여, 계산을 통해 M개의 명령 정보와 트레이닝 사 용자의 입술 움직임 정보의 매칭도를 획득하는 것은: M개의 명령 정보를 제1 모델에 입력하여 M개의 음성 특징 을 획득하고 - M개의 음성 특징 각각은 K 차원 음성 특징이고, K는 0보다 큰 정수임 -; 트레이닝 사용자의 입술 움직임 정보를 제2 모델에 타이핑하여 트레이닝 사용자의 이미지 시퀀스 특징을 획득하며 - 트레이닝 사용자의 이미지 시퀀스 특징은 K 차원 이미지 시퀀스 특징임 -; 트레이닝 사용자의 이미지 시퀀스 특징 및 M개의 음성 특징을 제3 모델에 타이핑하여, 계산을 통해 트레이닝 사용자의 이미지 시퀀스 특징과 M개의 음성 특징 사이의 매칭도를 획득하는 것을 포함한다. 본 출원에서 타깃 특징 매칭 모델을 획득하기 위해 초기화된 신경망 모델을 트레이닝하는 특정 방식은, 도 7에 대응하는 모델 애플리케이션 측의 후속 방법 실시예에서 설명될 것이다. 따라서, 자세한 내용은 여기에서 설명 하지 않는다. 본 발명의 이 실시예에서, 트레이닝 사용자의 입술 움직임 정보와, 매칭된 명령 정보 및 복수의 매칭되지 않은 명령 정보를 초기화된 신경망의 입력으로 사용하고, M개의 명령 정보와 트레이닝 사용자의 입술 움직임 정보 사 이의 실제 매칭도를 레이블로 사용하여 초기 신경망 모델을 트레이닝하여, 타깃 특징 매칭 모델을 획득한다. 예 를 들어, 완전 매칭(full matching)에 대응하는 매칭도는 레이블 1이고, 비-매칭(non-matching)에 대응하는 매 칭도는 레이블 0이다. 트레이닝 후 초기 신경망 모델에 의해 계산을 통해 획득되는 것이면서 또한 트레이닝 사 용자의 입술 움직임 정보와 M개의 명령 정보 사이의 매칭도가 M개의 레이블에 가까울 때, 트레이닝 후 초기화된 신경망이 타깃 특징 매칭 모델에 더 가깝다. 도 9는 본 발명의 실시예에 따른 또 다른 음성 명령 제어 방법의 개략적인 흐름도이다. 상기 방법은 주로 차량 내 멤버가 차량에서의 차량 탑재 디바이스에 대해 음성 상호 작용 제어를 수행하는 시나리오에 적용될 수 있다. 일반적으로, 차량에 복수의 차량 내 멤버가 있는 시나리오에서, 차량 탑재 디바이스는 차량 탑재 디바이스를 제 어하기 위한 음성 명령을 수신한다. 차량 탑재 디바이스가 차량에서의 멤버가 명령을 보내는 포지션을 결정하고, 특정 포지션 영역에서 응답 제어를 수행해야 하는 시나리오에서, 이 솔루션을 사용하여 멤버가 음성 명령을 보내는 포지션을 정확하게 인식할 수 있다. 상기 방법은 차량 캐빈에서의 애플리케이션 시나리오 및 시 스템 아키텍처에 적용될 수 있으며, 구체적으로는 도 3의 클라이언트 디바이스 및 실행 디바이스에 적용될 수 있다. 클라이언트 디바이스와 실행 디바이스 모두 차량에 배치될 수 있음을 이해할 수 있 다. 다음은 도 9를 참조하여 실행 주체가 지능형 차량인 예를 사용하여 설명한다. 상기 방법은 다음의 단계 (S1601 내지 S1605)를 포함할 수 있다. 단계 S1601: 차량에서 오디오 데이터를 획득한다. 구체적으로, 차량 탑재 마이크로폰에 의해 수집된 차량에서의 오디오 데이터를 획득한다. 오디오 데이터는 차량 내부의 주변 사운드, 예를 들어, 스피커의 음악, 에어컨의 엔진 소음, 차량 외부의 주변 사운드, 사용자가 보낸 음성 명령 등을 포함한다. 일반적으로 지능형 차량의 차량 캐빈에는 마이크로폰 어레이가 존재하며, 즉 차량에 복수의 마이크로폰이 존재 하며 차량 캐빈에서의 서로 다른 포지션에 분산되어 있다. 따라서, 차량에 마이크로폰 어레이가 존재할 때, 이 경우 단계(S1601)는 구체적으로 다음과 같을 수 있다. S1601a: 차량에서 복수의 마이크로폰에 의해 수집된 오디오 데이터를 획득한다. 인간-기계 상호 작용 시나리오에서, 마이크로폰 어레이가 오디오 데이터를 수집하거나, 차량이 시동된 후 또는 차량 소유자와 같은 차량 내 멤버에 의해 특정 작동이 수행된 후 차량의 마이크로폰 어레이가 실시간으로 오디 오 데이터 수집 상태에 있으며, 예를 들어, 오디오 수집 기능이 활성화된 후 마이크로폰 어레이가 오디오 수집 상태로 들어간다. 마이크로폰 어레이가 오디오 데이터를 수집하는 방식은, 복수의 마이크로폰 각각이 차량 캐빈 의 서로 다른 포지션에서 오디오 데이터를 수집하는 것이다. S1601b: 복수의 마이크로폰에 의해 수집된 오디오 데이터에 기반하여 타깃 오디오 데이터를 획득한다. 차량의 마이크로폰 어레이는 일반적으로 차량의 서로 다른 포지션에 배치된 복수의 마이크로폰이다. 따라서, 차 량 내 환경의 오디오 데이터는 복수의 오디오 소스로부터 획득되고 선택될 수 있다. 상이한 위치에서 수집된 오 디오 데이터의 효과는 상이하다. 예를 들어, 음성 명령을 보내는 사람이 차량 뒷좌석에 앉아 있고, 앞줄 조수석 에 있는 멤버가 노래를 듣고 있고 조수석에 있는 스피커가 이때 노래를 재생하고 있을 때, 이 경우, 조수석에서 수집된 오디오 데이터는 조수석에 있는 스피커로 인해 상대적으로 큰 음악 사운드를 가지고, 뒷좌석 탑승자의 명령 정보의 사운드는 상대적으로 작지만, 뒷줄의 스피커는 비교적 작은 음악 사운드를 가지는 비교적 선명한 음성 신호를 수집한다. 이 경우, 오디오 데이터를 획득할 때, 마이크로폰에서 수집한 오디오 데이터에 대해 전 처리를 수행하고, 그런 다음 분석과 비교를 통해 타깃 오디오 데이터를 선택한다. 예를 들어, 환경 소음, 음악 사운드 및 음성 명령이 서로 다른 주파수 대역에 위치되기 때문에, 전처리는 복수의 마이크로폰에 의해 수집된 오디오 데이터에 대해 필터링 처리를 수행할 수 있다. 필터링 처리 후 가장 강한 음성 신호를 가진 오디오 신호 를 타깃 오디오 신호로 선택한다. 여기서, 마이크로폰에 의해 수집된 오디오 데이터에서 음성 명령과 관련된 신호의 신호 품질이 가장 좋은 것으 로 결정하고, 이 오디오 신호를 타깃 오디오 신호로 선택하기 위해 종래의 전처리 방식도 사용될 수 있다. 선택 된 타깃 오디오 신호는 마이크로폰이 수집한 원본 오디오 신호일 수도 있고, 전처리된 오디오 신호일 수도 있다. 단계 S1602: 오디오 데이터가 제1 유형 명령 정보를 포함하는 것으로 인식될 때 이미지 데이터를 획득한다. 단계(S1601)에서 획득한 오디오 데이터가 제1 유형의 명령 정보를 포함하는지를 인식하는 방식은 여러 가지가 있다. 예를 들어, RNN 모델에 기반하여 오디오 정보의 시맨틱 인식을 수행하고, 인식을 통해 획득된 텍스트 정 보에 기반하여 명령 콘텐츠를 인식하고, 명령 콘텐츠에 기반하여 명령 유형을 결정하며; 또는 명령 유형은 텍스 트 정보의 특징 정보, 예를 들어, 키워드에 기반하여 직접 결정된다. 구체적으로, 종래 기술에서 음성에 기반하 여 명령 인식을 수행하기 위한 솔루션은 다수 존재하며, 여기서는 솔루션을 일일이 나열하지 않는다. 모델 입력 에 사용되는 오디오 데이터는 수집된 오디오 데이터에 대해 환경 노이즈 필터링 등의 전처리를 수행한 오디오 데이터일 수도 있고, 수집된 오디오 데이터일 수도 있다. 다르게는, 종래 기술의 다른 음성 인식 방식을 사용하 여 명령 정보의 포함 여부를 판정할 수 있다. 본 실시예에서 제1 유형 명령 정보는 차량 탑재 디바이스가 수신 및 인식할 수 있는 명령 정보를 의미하며, 명 령 개시자(instruction initiator)의 포지션을 결정하고 그다음 포지션 영역에서의 대응하는 작동 응답, 즉 차 량 캐빈 내부의 설비의 제어 명령, 예를 들어, 차량 캐빈의 에어컨 조절 명령, 음량 조절, 또는 오디오 콘텐츠 선택 및 조절과 관련된 명령을 수행해야 한다. 명령 정보는 오디오 데이터에서 명령 기간에 대응하는 음성 파형 시퀀스일 수도 있고, 오디오 데이터로부터 추 출된 타깃 기간의 텍스트 정보의 텍스트 특징 시퀀스일 수도 있다. 본 명세서에서 모델 설명 중에 언급되는 인 식할 음성 정보는 본질적으로 음성 명령이 송신되는 대응하는 기간의 음성 파형 시퀀스이다. 따라서, 도 9에서 언급한 명령 정보가 음성 파형 시퀀스의 형태일 때, 명령 정보도 인식할 음성 정보다. 마이크로폰에 의한 오디오 수집과 유사하게, 실시간 차량 내 오디오 및 이미지 데이터 수집은 차량 시동 후 자 동으로 수행되거나, 사용자의 명령에 따라 실시간 수집 기능이 활성화되거나, 이미지 데이터 수집은 기본적으로 오디오 수집이 시작될 때 동시에 활성화된다. 차량에는 일반적으로 복수의 카메라가 배치되며, 예를 들어, 단안 카메라, 양안 카메라, TOF 카메라, 적외선 카메라 등 다양한 유형의 카메라도 배치된다. 이 솔루션에서, 차량 내 이미지 데이터를 수집하기 위한 카메라의 배치 위치 및 수량 그리고 카메라의 유형은 제한되지 않으며, 당업 자는 특정 솔루션을 구현하기 위한 요건에 따라 대응하는 선택 및 배치를 수행할 수 있다. 단계(S1601)에서의 마이크로폰은 독립적으로 배치된 마이크로폰일 수도 있고, 카메라에 내장된 마이크로폰일 수도 있다. 이미지 데이터는 카메라를 사용하여 차량 내 처리 시스템에서 획득될 수 있고, 보이스 데이터는 마이크로폰을 사용하여 획득될 수 있다. 즉, 오디오 데이터와 이미지 데이터는 기간에서의 원본 오디오 데이터와 원본 이미지 데이터 즉, 오디오 데이터 소스와 이미지 데이터 소스이다. 선택적으로, 오디오 데이터와 이미지 데이터는 동일 한 시나리오에서 동일한 기간에 수집된다. 차량 캐빈에는 보통 2열 이상의 좌석이 있고, 이미지 데이터를 획득하기 위해 하나의 카메라를 사용할 때 멤버 간의 차단(blocking)이 주로 발생한다. 따라서, 각 멤버의 입술 움직임 정보를 명확하게 수집하기 위해서는 통 상적으로 차량 캐빈에서 서로 다른 포지션에 있는 복수의 카메라를 사용하여 이미지 데이터를 수집해야 한다. 그러나, 오디오 데이터 소스의 수량이 이미지 데이터 소스의 수량과 반드시 매칭할 필요는 없다. 예를 들어, 차 량에서의 포지션에 배치된 마이크로폰을 사용하여 오디오 데이터를 수집할 수 있으며, 차량에서 글로벌 카메라를 사용하여 차량 캐빈에서의 이미지 데이터를 수집할 수 있으며; 또는 지정된 마이크로폰을 사용하여 오디오 데이터를 수집할 수 있으며, 차량에서 복수의 포지션에 있는 카메라를 사용하여 차량 캐빈에서의 이미지 데이터 를 수집할 수 있다. 단계 S1603: 이미지 데이터로부터 차량에서 N개의 포지션에 있는 멤버의 입술 움직임 정보를 추출한다. 구체적으로, 수집된 차량 내 이미지 정보에 기반하여 차량 내 멤버의 포지션 분포를 결정하고, 각 포지션 있는 멤버의 입술 움직임 정보를 추출하며, 여기서 입술 움직임 정보는 대응하는 포지션 식별자를 운반한다. 차량에서 복수 멤버의 입술 움직임 정보 중 각 멤버의 입술 움직임 정보는 대응하는 타깃 기간에서 대응하는 사 용자의 입술 움직임 비디오 시퀀스를 포함하며, 타깃 기간은 오디오에서의 명령 정보에 대응하는 타깃 기간이다. 즉, 원본 이미지 데이터로부터 추출된 각 멤버의 입술 비디오, 즉 연속적인 입술 움직임 비디오 시퀀 스는, 대응하는 멤버의 연속적인 입 모양 변화 특징을 포함한다. 예를 들어, 카메라를 사용하여 수집한 이미지 데이터에서 이미지의 각 프레임의 포맷은 24비트 BMP 비트맵이다. BMP 이미지 파일(Bitmap-File) 포맷은 윈도우 (Windows)에서 이미지 파일을 저장하는 데 사용되는 포맷이다. 24비트 이미지의 경우, 3바이트가 색상 값을 저 장하는 데 사용되며, 각 바이트는 하나의 색상을 나타내고, 색상은 빨간색(R), 녹색(R) 및 파란색(B)으로 배열 된다. RGB 색상 이미지는 그레이스케일 이미지로 변환된다. 지능형 디바이스는 얼굴 인식 알고리즘에 기반하여 카메라를 사용하여 수집한 이미지 데이터로부터 적어도 하나의 얼굴 영역을 획득하고, 추가로 각 얼굴 영역을 하나의 유닛으로 사용하여 얼굴 ID(로봇 또는 스마트 스피커 시나리오와는 달리, 얼굴 ID는 차량에서의 포지션 에 대응함)를 각 얼굴 영역에 할당한다. 입 영역의 비디오 시퀀스 스트림을 추출하며, 비디오의 프레임 레이트 는 30 f/s(프레임 레이트(Frame rate) = 프레임(Frames)/시간(Time)이고, 단위는 초당 프레임 수(f/s, frames per second, fps))이다. 9개의 연속 이미지 프레임이 0.3초 길이의 비디오 스트림을 형성한다. 9개의 이미지 프 레임(비디오 속도: 30fps)의 이미지 데이터는 차원이 9 × 60 × 100인 큐브로 연결(concat)되며, 여기서 9는 시간 정보를 나타내는 프레임의 수량(시간적 특징)을 나타낸다. 각 채널은 구강 영역(mouth cavity area)에 대 한 60 × 100 그레이스케일 이미지(2d 공간 특징)를 나타낸다. 0.3초 동안의 N명의 사용자 각각에 대응하는 입 술 움직임의 이미지 시퀀스를 비디오 특징의 입력으로 사용하고, 0.3초를 타깃 기간으로 한다. 이미지 데이터로부터 복수의 멤버의 입술 움직임 정보를 추출하는 방법에 대한 자세한 내용은 본 발명의 이전 실시예에서 대응하는 기술 솔루션의 설명을 참조한다. 단계 S1604: 명령 정보와 N명의 멤버의 입술 움직임 정보를 타깃 특징 매칭 모델에 타이핑하여, N명의 멤버의 입술 움직임 정보와 명령 정보 사이의 매칭도를 획득한다. 구체적으로, 타깃 기간에서 N명의 멤버 각각의 입술 움직임의 이미지 시퀀스 및 명령 정보를 각각 비디오 특징 의 입력 및 오디오 특징의 입력으로 사용하고 타깃 특징 매칭 모델에 입력하여, 명령 정보와 N명의 멤버의 입술 움직임 특징 각각의 매칭도를 별도로 계산한다. 매칭도는 구체적으로 0보다 크거나 같고 1보다 작거나 같은 값 일 수 있다. 여기서 명령 정보는 도 6에 도시될 수 있으며, 도 6은 본 발명의 이 실시예에 따른 사운드 파형 또는 명령의 식 별자 정보의 형태, 예를 들어, 시퀀스 번호의 형태 또는 명령문의 형태의 예시도이다. 가능한 구현에서, 오디오 데이터에서의 복수의 사용자가 동시에 말하는 것으로 가정한다. 이 경우, 음성 정보의 세그먼트를 송신하는 사 용자를 결정하고, 오디오 데이터에서의 타깃 음성 정보, 즉 인식할 음성 정보를 먼저 인식하여 추출해야 한다. 다르게는, 오디오 데이터는 사용자가 발화한 복수의 음성 정보 세그먼트를 포함한다고 가정한다. 지능형 디바이 스는 음성 정보의 한 세그먼트만 인식하면 되고, 음성 정보의 세그먼트는 인식할 음성 정보이다. 예를 들어, 지 능형 디바이스는 S801에서 마이크로폰 어레이가 획득한 오디오 데이터로부터 오디오 특징을 추출한다. 구체적인 방법은 멜 주파수 켑스트럼 계수를 사용하여 음성 특징을 추출하는 것일 수 있다. 멜-주파수 켑스트럼 계수 (Mel-frequency cepstrum coefficient, MFCC)를 사용하여 프레임 길이가 20ms인 데이터로부터 40차원 특징을 추출한다. 프레임이 겹치지 않는다(non-overlapping). 15 프레임마다(0.3초 오디오 클립에 대응) 차원이 15 × 40 × 3(15는 시간적 특징이고 40 × 3은 2d 공간적 특징임)인 큐브로 연결(concat)된다. 0.3초 이내의 음성 파 형 시퀀스는 오디오 특징의 입력으로 사용되며, 여기서 0.3초는 타깃 기간이다. 전술한 방식 외에, 종래 기술의 또 다른 방식을 음성의 세그먼트에서 타깃 서술을 분리하기 위해 사용할 수 있다. 타깃 특징 매칭 모델의 구체적인 구현은 아래에 구체적으로 설명된다. 모델 구조에 대해서는 도 7에 대한 후속 설명 그리고 도 3에서의 모델 트레이닝 및 획득에 대한 전술한 설명을 참조한다. 단계 S1605: 매칭도에 기반하여, 명령 정보에 대응하는 명령을 실행할 포지션 영역을 결정한다. 매칭도는 일반적으로 값이다. 따라서, S1605에서의 대응하는 결정 정책은 멤버 중 매칭도가 가장 높은 입술 움 직임 정보에 대응하는 멤버의 차량 내 포지션을 명령 정보를 실행하기 위한 타깃 영역으로 결정하고, 명령을 실 행하는 것일 수 있다. 명령이 에어컨의 온도나 풍량을 낮추는 것에 관한 것이면, 온도나 배기구의 풍량을 낮추는 작동은 타깃 영역에 서만 수행된다. 또한, S1604 및 S1605는 다음과 같을 수 있다. 단계 S1604: 어느 한 멤버의 입술 움직임 정보 및 명령 정보를 타깃 특징 매칭 모델에 타이핑하여, 멤버의 입술 움직임 정보와 명령 정보 사이의 매칭도를 획득한다. 단계 S1605: 매칭도가 알려진 임계값보다 클 때, 멤버의 포지션 영역에서 명령 정보에 대응하는 명령을 실행한 다. 매칭도가 알려진 임계값보다 작으면, 매칭도가 알려진 임계값보다 큰 입술 움직임 정보가 획득될 때까지, 특정 규칙에 따라 차량에서 다른 포지션에 있는 멤버의 입술 움직임 정보와 명령 정보 사이의 매칭도를 계속해서 결 정하거나, 또는 차량에서 내 모든 멤버에 대해 매칭을 수행하고, 매칭 프로세스가 종료된다. 전술한 실시예에서 명령 개시자의 포지션을 인식하고 특정 포지션에서 대응하는 작동을 수행하는 것 외에도, 차 량 캐빈에서 명령 개시자의 아이덴티티를 결정해야 하는 시나리오가 있다. 예를 들어, 차량 제어와 관련된 음성 명령을 인식할 때, 이 경우 명령이 실행 가능한지를 결정하기 위해, 운전자가 명령을 내렸는지 여부를 판정해야 한다. 이러한 유형의 시나리오에 대한 구체적인 구현은 다음과 같다. 도 10은 본 발명의 실시예에 따른 또 다른 음성 매칭 방법의 개략적인 흐름도이다. 상기 방법은 주로 차량이 음 성 명령에 따라 주행 관련 작동 제어를 수행하는 시나리오에 적용될 수 있다. 일반적으로 차량에 복수의 멤버가 있기 때문에, 일반적으로 운전자만이 차량 주행에 대한 음성 작동 제어를 수행할 수 있는 권한이 있는 것으로 간주된다. 오작동 및 오식별을 방지하기 위해, 차량 주행 제어에 사용되는 음성 명령을 수신할 때, 차량 탑재 디바이스는 음성 명령이 운전자에 의해 보내지는 지를 판정하고, 인식 결과에 기반하여 차량 주행 명령을 실행 할지를 판정해야 한다. 상기 방법은 다음의 단계(S1701 내지 S1705)를 포함할 수 있다. 단계 S1701: 차량에서 오디오 데이터를 획득한다. S1701의 구체적인 구현은 S1601의 구현과 동일하다. 단계 S1702: 오디오 데이터가 제2 유형 명령 정보를 포함하는 것으로 인식될 때 이미지 데이터를 획득한다. S1702의 제2 유형 명령 정보는 차량의 주행 제어, 예를 들어, 차량의 선회, 가속, 출발 및 주행 모드 전환과 관 련된 명령 정보만을 참조하면 된다. 이런 유형의 명령 정보를 인식할 때, 운전석에 있는 멤버의 이미지 데이터 를 획득해야 한다. 특정 명령 인식 방식 및 이미지 데이터 획득 방식에 대해서는 S1602를 참조한다. 단계 S1703: 이미지 데이터로부터 제1 포지션에 있는 멤버의 입술 움직임 정보를 추출한다. 입술 움직임 정보를 추출하는 방법과 입술 움직임 정보를 인식하는 방법은 S1603을 참조한다. 단계 S1704: 제1 포지션에 있는 멤버의 입술 움직임 정보 및 명령 정보를 타깃 특징 매칭 모델에 타이핑하여, 운전석에 있는 멤버의 입술 움직임 정보와 명령 정보 사이의 매칭도를 획득한다. 단계 S1705: 매칭도에 기반하여, 명령 정보에 대응하는 명령을 실행할지를 판정한다. S1705에서 결정하는 방식은 여러 가지가 있다. 매칭도는 일반적으로 값이기 때문에, S1705에서, 매칭도가 미리 설정된 임계값보다 큰지에 기반하여 명령 정보를 실행할지를 판정할 수 있다. 즉, 매칭도가 미리 설정된 임계값 보다 클 때, 명령이 제1 포지션에 있는 멤버에 의해 보내진 것으로 간주하고, 차량 주행 제어 명령이 실행된다. 그렇지 않으면, 명령이 실행되지 않는다. 다르게는, S1705는 다음: 제1 포지션에 있는 멤버의 입술 정보에 대응하는 매칭도가, 차량에서의 모든 멤버의 입술 움직임 정보와 명령 정보 사이의 매칭도 중에서 가장 높은지의 여부일 수 있다. 이 경우, 제1 포지션에 있 는 멤버의 입술 움직임 정보를 추출하는 것 이외에, S1703에서 차량에서 다른 멤버의 입술 움직임 정보도 추출 해야 한다. 이와 유사하게, S1704에서 제1 포지션에 있는 멤버의 입술 움직임 정보와 명령 정보를 타깃 특징 매칭 모델에 입력하는 것 이외에, 다른 멤버의 입술 정보도 타깃 특징 매칭 모델에 입력하여, 대응하는 매칭도를 획득해야 한다. 솔루션의 특정 구현 동안, 전술한 실시예의 제1 포지션은 일반적으로 운전석이며, 예를 들어, 운전석의 멤버가 음성으로 차량 주행 작동을 제어하기 위한 권한을 갖는 것이 차량 내 제어 시스템에서 기본적으로 초기에 설정 될 수 있으며, 또는 사용자가 차량에 탑승할 때마다 특정 포지션 분포를 변경하도록 사용자가 수동으로 설정할 수 있으며, 예를 들어, 운전석과 조수석 모두 차량 주행 제어 권한을 갖는 것으로 설정된다. 이 경우, 제1 포지 션은 운전석과 조수석이다. 다르게는, 본 발명의 이 실시예는 차량이 초기화될 때 구체적으로 구현된다. 집에서 차량을 사용하는 멤버의 이 미지 정보 및 권한 정보는 차량 프롬프트 요건에 따라 차량에 입력되거나 차량 소유자가 능동적으로 설정할 수 있다. 이 경우, 본 발명의 이 실시예의 솔루션의 특정 구현에서, 차량이 주행하기 전에 또는 차량이 시동을 시 작한 후에, 차량에서의 카메라가 주행 제어 권한을 가진 등록된 멤버의 포지션 정보를 획득하고, 그다음 차량 제어 관련 명령이 인식될 때, 이 명령이 상기 포지션에 있는 멤버가 송신한 음성 명령인지의 여부가, 제어 권한 을 가진 멤버가 위치된 포지션에 대응하는 입술 움직임 정보에 기반하여 결정될 수 있다. 차량 주행 제어형 음성 명령이 운전석에 있는 멤버에 의해 보내지는지를 판정하는 것 외에도, 본 발명의 이 실 시예는 다른 유형의 명령이 실행 가능한지를 판정하는 데에도 적용될 수 있다. 예를 들어, 차량 소유자 또는 운 전자만이 통화 기능(call function)에 대한 음성 제어를 수행할 수 있도록 차량에서 기본적으로 설정하거나 수 동으로 설정할 수 있다. 전술한 실시예는 단지 특정 예일 뿐이며, 특정 명령 유형 또는 특정 고정 포지션을 제 한하지 않는다. 차량 내 상호 작용의 두 가지 실시예에서, 명령이 차량에서 멤버의 좌석 상에서 송신되는 것으로 결정하는 것에 의해 명령 작동이 타깃화된 방식으로 수행될 수 있어, 차량 내 상호 작용 제어를 보다 정확하게 사용자에게 제 공할 수 있다. 차량 주행 제어에 대한 음성 제어의 경우, 이는 오작동 및 오식별을 잘 방지할 수 있으며, 운전자만이 대응하는 차량 주행 제어를 수행할 수 있도록 보장하여, 차량 주행 제어의 보안성을 제공한다. 본 발명의 실시예는 또 다른 음성 명령 제어 방법을 더 제공한다. 상기 방법은 도 1 및 도 2의 차량 내 애플리 케이션 시나리오 및 시스템 아키텍처에 적용될 수 있으며, 구체적으로 도 3의 실행 디바이스에 적용될 수 있다. 이 경우 클라이언트 디바이스와 실행 디바이스는 동일한 물리적 디바이스에 있지 않을 수 있음 을 이해할 수 있다. 도 8은 본 발명의 실시예에 따른 음성 명령 제어 시스템의 아키텍처 다이어그램이다. 시스 템은 예를 들어, 오디오 데이터 및 이미지 데이터를 수집하기 위한 디바이스로 사용되고 추가로, N명의 사용자 의 입술 정보 및 인식할 명령 정보를 추출하는 디바이스로 사용되는 지능형 차량을 포함한다. 추출된 N명 의 사용자의 입술 정보와 인식할 명령 정보의 매칭이 실행 디바이스가 위치된 서버/서비스 디바이스/서비 스 장치/클라우드 서비스 디바이스에서 수행될 수 있다. 선택적으로, 인식할 명령 정보 및 N개의 사용자 입술 정보의 추출은 실행 디바이스가 위치된 디바이스 측에서도 수행될 수 있다. 이는 본 발명의 이 실시 예에서 특별히 제한되지 않는다. 다음은 도 8의 클라우드 서비스 디바이스를 설명을 위한 예로 사용한다. 도 11에 도시된 바와 같이, 상기 방법은 다음의 단계(S1001) 내지 단계(S1003)를 포함할 수 있다. 단계 S1001: 차량 캐빈에 위치된 N명의 차량 내 멤버의 입술 움직임 정보 및 명령 정보를 획득한다. 전술한 단계에서, 차량 캐빈에서 수집된 오디오 데이터에 기반하여 명령 정보를 획득하고, 명령 정보에 대응하 는 명령이 제1 유형이라고 결정될 때 차량 내 멤버의 입술 움직임 정보를 획득한다. 입술 움직임 정보는 타깃 기간에서 차량에서의 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 비디오 시퀀스를 포함하며, 타깃 기간은 오디오 데이터에서의 명령에 대응하는 기간이다. 단계 S1002: 차량 캐빈에 위치된 N개의 차량 내 멤버의 입술 움직임 정보와 명령 정보를 타깃 특징 매칭 모델에 타이핑하여, N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 명령 정보 사이의 매칭도를 획득된다. 단계 S1003: 매칭도가 가장 높은 사용자의 입술 움직임 정보에 대응하는 멤버의 포지션을 명령 정보에 대응하는 명령을 실행하기 위한 타깃 포지션으로 사용한다. 또한, 도 12에 도시된 바와 같이, 명령의 타깃 실행 영역을 결정하기 위해, 특히 명령을 보내는 멤버의 권한을 인식할 필요가 있는 클라우드 솔루션이 있다.단계 S1021: 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 명령 정보를 획득한다. 전술한 단계에서, 차량 캐빈에서 수집된 오디오 데이터에 기반하여 명령 정보를 획득하고, 명령 정보에 대응하 는 명령이 제2 유형 명령인 것으로 인식될 때, 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 획득한다. 입술 움직임 정보는 타깃 기간에서 차량에서의 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 이 미지 시퀀스를 포함하며, 타깃 기간은 오디오 데이터에서의 명령에 대응되는 기간이다. 단계 S1022: 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보 및 명령 정보를 타깃 특징 매칭 모델에 타이핑하여, 차량에서 제1 포지션에 위치된 차량 내 멤버 입술 움직임 정보와 명령 정보 사이의 제1 매 칭도를 획득한다. 단계 S1023: 제1 매칭도에 기반하여, 명령 정보에 대응하는 명령을 실행할지를 판정한다. 가능한 구현에서, 타깃 특징 매칭 모델은 제1 모델, 제2 모델 및 제3 모델을 포함하고; 그리고 N명의 사용자의 입술 움직임 정보와 인식할 음성 정보를 타깃 특징 매칭 모델에 타이핑하여, N명의 사용자의 입 술 움직임 정보와 인식할 음성 정보 사이의 매칭도를 획득하는 것은, 인식할 음성 정보를 제1 모델에 타이핑하여 음성 특징을 획득하고 - 음성 특징은 K차원 음성 특징이고, K는 0보 다 큰 정수임 -; N명의 사용자의 입술 움직임 정보를 제2 모델에 타이핑하여 N개의 이미지 시퀀스 특징을 획득하며 - N개의 이미 지 시퀀스 특징 각각은 K차원 이미지 시퀀스 특징임 -; 그리고 음성 특징 및 N개의 이미지 시퀀스 특징을 제3 모델에 타이핑하여 N개의 이미지 시퀀스 특징과 음성 특징 사이 의 매칭도를 획득하는 것을 포함한다. 가능한 구현에서, 타깃 특징 매칭 모델은 트레이닝 사용자의 입술 움직임 정보와 M개의 명령 정보를 입력으로 사용하고, 트레이닝 사용자의 입술 움직임 정보와 M개의 음성 정보 사이의 매칭도를 M개의 레이블로 사용하는 트레이닝을 통해 획득된 특징 매칭 모델이다. 가능한 구현에서, 상기 방법은: 타깃 사용자의 사용자 정보를 결정하는 단계 - 사용자 정보는 개인 속성 정보, 인식할 음성 정보에 대응하는 얼 굴 표정 정보, 및 인식할 음성 정보에 대응하는 환경 정보 중 하나 이상을 포함함 -; 및 사용자 정보에 기반하여, 사용자 정보와 매칭하는 제어 명령을 생성하는 단계를 더 포함한다. 가능한 구현에서, 상기 방법은: 이미지 데이터로부터 N명의 사용자의 입술 움직임 정보를 추출하는 단계를 더 포함하고; 또한, 상기 이미지 데이터로부터 N명의 사용자의 입술 움직임 정보를 추출하는 단계는: 얼굴 인식 알고리즘에 기반하여 이미지 데이터에서 N개의 얼굴 영역을 인식하고, N개의 얼굴 영역 각각에서 입 술 움직임 비디오를 추출하는 단계; 및 각 얼굴 영역에서 입술 움직임 비디오에 기반하여 N명의 사용자의 입술 움직임 정보를 결정하는 단계를 포함한 다. 가능한 구현에서, 상기 방법은: 오디오 데이터로부터 인식할 음성 정보를 추출하는 단계를 더 포함하고; 또한, 상기 오디오 데이터로부터 인식할 음성 정보를 추출하는 단계는: 스펙트럼 인식 알고리즘에 기반하여, 오디오 데이터에서 서로 다른 스펙트럼의 오디오 데이터를 인식하고, 타깃 스펙트럼의 오디오 데이터를 인식할 음성 정보로 인식하는 단계를 포함한다. 클라우드 서비스 디바이스에 의해 수행되고 본 발명의 이 실시예에서 설명된 방법의 절차에 대해서는, 도 9 내 지 도 12의 관련 방법 실시예를 참조한다는 점에 유의해야 하며, 자세한 내용은 여기에서 다시 설명하지 않는다. 도 13은 본 발명의 실시예에 따른 지능형 디바이스의 구조의 개략도이거나, 도 13은 본 발명의 실시예에 따른 지능형 디바이스의 기능 원리의 개략도이다. 지능형 디바이스는 차량 탑재 디바이스, 차량 탑재 시스템 또는 지 능형 차량일 수 있다. 지능형 디바이스는 프로세서, 프로세서에 결합된 마이크로폰 및 카메 라를 포함할 수 있다. 지능형 디바이스가 지능형 차량 또는 차량 내 음성 처리 시스템일 때, 일반적으로복수의 마이크로폰 및 카메라가 있으며, 예를 들어, 도 12의 애플리케이션 시나리오가 있다. 마이크로폰은 오디오 데이터를 수집하도록 구성된다. 카메라는 이미지 데이터를 수집하도록 구성된다. 오디오 데이터와 이미지 데이터는 동일한 시나리오에서 수집된다. 프로세서는 차량 캐빈에서 오디오 데이터를 획득하며, 차량 캐빈에서의 오디오 데이터가 제1 유형 명령을 포함하는 것으로 인식될 때, 차량 캐빈에서 이미지 데이터를 획득하고, 차량 캐빈에서의 이미지 데이터로부터, 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출한다. 프로세서는 차량 캐빈에서 N 개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 제1 유형 명령에 대응하는 명령 정보를 타깃 특징 매칭 모델에 입력하여, N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 명령 정보 사이의 매칭도를 획득하 며; 매칭도가 가장 높은 사용자의 입술 움직임 정보에 대응하는 멤버의 포지션을 명령 정보에 대응하는 명령을 실행하기 위한 타깃 포지션으로 사용하도록 구성된다. 예를 들어, 도 12에 대응하는 애플리케이션 시나리오에서, 마이크로폰은 오디오 데이터를 수집하도록 구성 된다. 카메라는 이미지 데이터를 수집하도록 구성된다. 오디오 데이터와 이미지 데이터는 동일한 시나리오에서 수집된다. 프로세서는 차량 캐빈에서 오디오 데이터를 획득하며, 차량 캐빈에서의 오디오 데이터가 제2 유형 명령을 포함하는 것으로 인식될 때, 차량 캐빈에서 이미지 데이터를 획득하고, 차량 캐빈에서의 이미지 데이터로부터 제1 이미지 데이터를 획득하며 - 제1 이미지 데이터는 차량 캐빈에서 제1 포지션에 있는 차량 내 멤버의 이미지 데이터임 -, 그리고 제1 이미지 데이터로부터 차량에서 제1 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출한다. 프로세서는, 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보와 제2 유형 명령에 대 응하는 명령 정보를 타깃 특징 매칭 모델에 입력하여, 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직 임 정보와 명령 정보 사이의 매칭도를 획득하며; 제1 매칭도에 기반하여 명령 정보에 대응하는 명령을 실행할지 를 판정하도록 구성된다. 가능한 구현에서, 인식할 음성 정보는 타깃 기간에서의 음성 파형 시퀀스를 포함하고, N명의 사용자의 입술 움 직임 정보에서 각 사용자의 입술 움직임 정보는 타깃 기간에서 대응하는 사용자의 입술 움직임의 이미지 시퀀스 를 포함한다. 가능한 구현에서, 프로세서는 구체적으로: N명의 사용자의 입술 움직임 정보와 인식할 음성 정보를 타깃 특징 매칭 모델에 입력하여, N명의 사용자의 입술 움직임 정보와 인식할 음성 정보 사이의 매칭도를 획득하고; 매칭도가 가장 높은 사용자의 입술 움직임 정보에 대응하는 사용자를, 인식할 음성 정보가 속하는 타깃 사용자 로 결정하도록 구성된다. 가능한 구현에서, 타깃 특징 매칭 모델은 제1 모델, 제2 모델 및 제3 모델을 포함한다. 프로세서는 구체적 으로, 인식할 음성 정보를 제1 모델에 입력하여 음성 특징을 획득하고 - 음성 특징은 K차원 음성 특징이고, K는 0보다 큰 정수임 -; N명의 사용자의 입술 움직임 정보를 제2 모델에 입력하여 N개의 이미지 시퀀스 특징을 획득 하며 - N개의 이미지 시퀀스 특징 각각은 K차원 이미지 시퀀스 특징임 -; 그리고 음성 특징 및 N개의 이미지 시 퀀스 특징을 제3 모델에 입력하여 N개의 이미지 시퀀스 특징과 음성 특징 사이의 매칭도를 획득하도록 구성된다. 가능한 구현에서, 타깃 특징 매칭 모델은 트레이닝 사용자의 입술 움직임 정보와 M개의 음성 정보를 입력으로 사용하고, 트레이닝 사용자의 입술 움직임 정보와 M개의 음성 정보 사이의 매칭도를 M개의 레이블로 사용한 트 레이닝을 통해 획득된 특징 매칭 모델이며, M개의 음성 정보는 트레이닝 사용자의 입술 움직임 정보와 매칭된 음성 정보를 포함한다. 가능한 구현에서, 프로세서는 추가로, 타깃 사용자의 사용자 정보를 결정하고 - 사용자 정보는 개인 속성 정보, 인식할 음성 정보에 대응하는 얼굴 표정 정보 및 인식할 음성 정보에 대응하는 환경 정보 중 하나 이상을 포함함 -; 그리고 사용자 정보에 기반하여, 사용자 정보와 매칭하는 제어 명령을 생성하도록 구성된다. 가능한 구현에서, 프로세서는 구체적으로, 얼굴 인식 알고리즘에 기반하여 이미지 데이터에서 N개의 얼굴 영역을 인식하고, N개의 얼굴 영역 각각에서 입술 움직임 비디오를 추출하며; 각 얼굴 영역에서의 입술 움직임비디오에 기반하여 N명의 사용자의 입술 움직임 정보를 결정하도록 구성된다. 가능한 구현에서, 프로세서는 구체적으로, 스펙트럼 인식 알고리즘에 기반하여 오디오 데이터에서 상이한 스펙트럼의 오디오 데이터를 인식하고; 타깃 스펙트럼의 오디오 데이터를 인식할 음성 정보로 인식한다. 본 발명의 이 실시예에서 설명된 지능형 디바이스의 관련 모듈의 기능에 대해서는 도 9 내지 도 12의 관련 방법 실시예를 참조한다는 점에 유의해야 하며, 자세한 내용은 여기에서 다시 설명하지 않는다. 도 14는 본 발명의 실시예에 따른 신경망 트레이닝 장치의 구조의 개략도이거나, 도 14는 본 발명의 실시예에 따른 지능형 디바이스의 기능 원리의 개략도이다. 신경망 트레이닝 장치에 의해 트레이닝된 모델은 차량 탑재 디바이스, 차량 탑재 시스템, 지능형 차량, 클라우드 서버 등에 사용될 수 있다. 신경망 트레이닝 장치는 획득 유닛 및 트레이닝 유닛을 포함할 수 있다. 획득 유닛은 트레이닝 샘플을 획득하도록 구성되며, 트레이닝 샘플은 트레이닝 사용자의 입술 움직임 정보 및 M개의 명령 정보를 포함하고, 선택적으로 M개의 명령 정보는 트레이닝 사용자의 입술 움직임 정보와 매칭하 는 명령 정보 및 트레이닝 사용자의 입술 움직임 정보와 매칭하지 않는 (M - 1)개의 명령 정보를 포함한다. 트레이닝 유닛은 트레이닝 사용자의 입술 움직임 정보와 M개의 명령 정보를 트레이닝 입력으로 사용하고, 트레이닝 사용자의 입술 움직임 정보와 M개의 명령 정보를 M개의 레이블로 사용하여 초기화된 신경망을 트레이 닝하여, 타깃 특징 매칭 모델을 획득하도록 구성된다. 가능한 구현에서, 트레이닝 사용자의 입술 움직임 정보는 트레이닝 사용자의 입술 움직임 이미지 시퀀스를 포함 하고, M개의 명령 정보는 트레이닝 사용자의 입술 움직임 이미지 시퀀스와 매칭하는 음성 파형 시퀀스 및 트레 이닝 사용자의 입술 움직임 이미지 시퀀스와 매칭하지 않는 (M - 1)개의 음성 파형 시퀀스를 포함한다. 가능한 구현에서, 트레이닝 유닛은 구체적으로: 트레이닝 사용자의 입술 움직임 정보와 M개의 명령 정보를 초기화된 신경망에 입력하여, 계산을 통해 M개의 명 령 정보와 트레이닝 사용자의 입술 움직임 정보 사이의 매칭도를 획득하고; 그리고 계산을 통해 획득된 것이면서 또한 M개의 명령 정보와 트레이닝 사용자의 입술 움직임 정보 간의 매칭도를 M개 의 레이블과 비교하는 것을 통해 초기화된 신경망을 트레이닝하여, 타깃 특징 매칭 모델을 획득하도록 구성된다. 가능한 구현에서, 타깃 특징 매칭 모델은 제1 모델, 제2 모델 및 제3 모델을 포함한다. 트레이닝 유닛은 구체적으로: M개의 명령 정보를 제1 모델에 입력하여 M개의 음성 특징을 획득하고 - M개의 음성 특징 각각은 K차원 음성 특 징이고, K는 0보다 큰 정수임 -; 트레이닝 사용자의 입술 움직임 정보를 제2 모델에 입력하여 트레이닝 사용자의 이미지 시퀀스 특징을 획득하며 - 트레이닝 사용자의 이미지 시퀀스 특징은 K차원 이미지 시퀀스 특징임 -; 트레이닝 사용자의 이미지 시퀀스 특징 및 M개의 음성 특징을 제3 모델에 입력하여, 계산을 통해 트레이닝 사용 자의 이미지 시퀀스 특징과 M개의 음성 특징 사이의 매칭도를 획득하고; 그리고 계산을 통해 획득된 것이면서 또한 M개의 음성 특징과 트레이닝 사용자의 이미지 시퀀스 특징 사이의 매칭도를 M개의 레이블과 비교하는 것을 통해 초기화된 신경망을 트레이닝하여, 타깃 특징 매칭 모델을 획득하도록 구성 된다. 도 15는 본 발명의 실시예에 따른 시스템의 구조도이다. 지능형 디바이스의 구조에 대한 개략도 및 서버 디 바이스의 구조에 대한 개략도가 포함된다. 지능형 디바이스는 지능형 차량일 수 있다. 지능형 디바이스(7 0)는 프로세서 그리고 프로세서에 결합된 마이크로폰 및 카메라를 포함할 수 있다. 마이크로폰은 오디오 데이터를 수집하도록 구성된다. 카메라는 이미지 데이터를 수집하도록 구성된다. 프로세서는 오디오 데이터 및 이미지 데이터를 획득하고; 오디오 데이터로부터 인식할 음성 정보를 추출하고 - 인식할 음성 정보는 타깃 기간의 음성 파형 시퀀스를 포함 함 -; 그리고이미지 데이터로부터 N명의 사용자의 입술 움직임 정보를 추출하도록 구성되며, N명의 사용자의 입술 움직임 정 보에서 각 사용자의 입술 움직임 정보는 타깃 기간에서 대응하는 사용자의 입술 움직임의 이미지 시퀀스를 포함 하며, N은 1보다 큰 정수이다. 지능형 차량 또는 차량 내 음성 상호 작용 시스템에 적용될 때, 프로세서는 오디오 데이터를 획득하고; 오 디오 데이터가 타깃 명령을 포함할 때 차량 캐빈에서 이미지 데이터를 획득하며; 차량 캐빈에서의 이미지 데이 터로부터 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보를 추출하도록 구성된다. 여기서, 차량 내 멤버의 입술 움직임 정보를 획득하여 서비스 장치로 송신할 수 있으며, 또는 수집된 차량 내 이미지 정보를 서비스 장치로 송신할 수 있으며, 서비스 장치가 입술 움직임 정보를 추출할 수 있다. 다르게는, 프로세서는 차량 캐빈에서 오디오 데이터를 획득하고; 오디오 데이터가 제2 유형 명령을 포함하 는 것으로 인식할 때 제1 이미지 데이터를 획득하며 - 제1 이미지 데이터는 차량에서 제1 포지션에 위치된 차량 내 멤버를 포함하는 이미지 데이터임 -; 그리고 제1 이미지 데이터로부터 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보를 추출하도록 구성된다. 본 발명의 이 실시예에서 설명된 지능형 디바이스의 관련 모듈의 기능에 대해서는 도 9 내지 도 12의 관련 방법 실시예를 참조한다는 점에 유의해야 하며, 자세한 내용은 여기에서 다시 설명하지 않는다. 서비스 장치 구조의 개략도는 도 15에 포함되어 있다. 서비스 장치는 서버, 클라우드 서버 등일 수 있다. 서비 스 장치는 프로세서를 포함할 수 있다. 선택적으로, 프로세서는 신경망 프로세서 및 신경망 프로세서에 결 합된 프로세서를 포함하거나, 프로세서를 직접 포함할 수 있다. 차량 내 구현 시나리오에서, 신경망 프로세서는, 차량 캐빈에서 N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 제1 유형 명령에 대응하는 명령 정보를 타깃 특징 매칭 모델에 입력하여, N개의 포지션에 있는 차량 내 멤버의 입술 움직임 정보와 명령 정보 사이의 매칭도를 획득하고; 매칭도가 가장 높은 사용자의 입술 움직임 정보에 대응하는 멤버의 포지션을 명령 정보에 대응하는 명령을 실행하기 위한 타깃 포지션으로 사용하도록 구성되거나; 또는 제2 유형 명령에 대응하는 명령 정보 및 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보를 타 깃 특징 매칭 모델에 입력하여, 차량에서 제1 포지션에 위치된 차량 내 멤버의 입술 움직임 정보와 명령 정보 사이의 제1 매칭도를 획득하고, 제1 매칭도에 기반하여 명령 정보에 대응하는 명령을 실행할지를 판정하도록 구 성된다. 가능한 구현에서, 타깃 특징 매칭 모델은 제1 모델, 제2 모델 및 제3 모델을 포함한다. 프로세서는 구체적 으로: 인식할 음성 정보 또는 명령 정보를 제1 모델에 입력하여 음성 특징을 획득하고 - 음성 특징은 K차원 음 성 특징이고, K는 0보다 큰 정수임 -; N명의 사용자의 입술 움직임 정보를 제2 모델에 입력하여 N개의 이미지 시퀀스 특징을 획득하고 - N개의 이미지 시퀀스 특징 각각은 K차원 이미지 시퀀스 특징임 -; 음성 특징 및 N개 의 이미지 시퀀스 특징을 제3 모델에 입력하여 N개의 이미지 시퀀스 특징과 음성 특징 사이의 매칭도를 획득하 도록 구성된다. 가능한 구현에서, 타깃 특징 매칭 모델은, 트레이닝 사용자의 입술 움직임 정보와 M개의 음성 정보를 입력으로 사용하고, 트레이닝 사용자의 입술 움직임 정보와 M개의 음성 정보 사이의 매칭도를 M개의 레이블로 사용한 트 레이닝을 통해 획득된 특징 매칭 모델이다. 가능한 구현에서, 서버는 프로세서를 더 포함한다. 프로세서는 타깃 사용자의 사용자 정보를 결정하 고 - 사용자 정보는 개인 속성 정보, 인식할 음성 정보에 대응하는 얼굴 표정 정보 및 인식할 음성 정보에 대응 하는 환경 정보 중 하나 이상을 포함함 -; 그리고 사용자 정보에 기반하여 사용자 정보와 매칭하는 제어 명령을 생성하도록 구성된다. 가능한 구현에서, 서버는 프로세서를 더 포함한다. 프로세서는 추가로, 얼굴 인식 알고리즘에 기반하 여 이미지 데이터에서 N개의 얼굴 영역을 인식하고, N개의 얼굴 영역 각각에서 입술 움직임 비디오를 추출하며; 그리고 각 얼굴 영역에서의 입술 움직임 비디오에 기반하여 N명의 사용자의 입술 움직임 정보를 결정하도록 구 성된다. 가능한 구현에서, 서버는 프로세서를 더 포함한다. 프로세서는 추가로, 스펙트럼 인식 알고리즘에 기 반하여 오디오 데이터에서 상이한 스펙트럼의 오디오 데이터를 인식하고; 타깃 스펙트럼의 오디오 데이터를 인식할 음성 정보로 인식하도록 구성된다. 본 발명의 실시예는 컴퓨터 저장 매체를 더 제공한다. 컴퓨터 저장 매체는 프로그램을 저장할 수 있다. 프로그 램이 실행될 때, 임의의 방법 실시예의 단계 중 적어도 일부 또는 전부가 수행될 수 있다. 본 발명의 실시예는 컴퓨터 프로그램을 더 제공한다. 컴퓨터 프로그램에는 명령이 포함되어 있다. 컴퓨터 프로 그램이 컴퓨터에 의해 실행될 때, 컴퓨터는 전술한 방법 실시예 중 임의의 하나에 기술된 단계의 일부 또는 전 부를 수행하도록 인에이블된다. 전술한 실시예에서, 각 실시예의 설명은 각각의 초점을 갖는다. 실시예에서 구체적으로 설명되지 않은 부분에 대해서는 다른 실시예의 관련 설명을 참조한다. 간략한 설명을 위해 전술한 방법 실시예는 일련의 동작(action)으로 표현된다는 점에 유의해야 한다. 그러나, 당업자는 본 출원이 동작의 설명된 순서로 제한되지 않음을 이해해야 하는데, 이는 본 출원에 따르면 일부 단계 가 다른 순서로 또는 동시에 수행될 수 있기 때문이다. 본 명세서에 기술된 실시예는 모두 바람직한 실시예에 속하며, 관련 동작 및 모듈이 본 출원에서 반드시 요구되는 것은 아님이 당업자에 의해 추가로 이해되어야 한다. 본 출원에서 제공된 여러 실시예에서, 개시된 장치가 다른 방식으로 구현될 수 있음을 이해해야 한다. 예를 들 어, 기술된 장치 실시예는 단지 예일 뿐이다. 예를 들어, 유닛으로의 구분은 논리적인 기능 구분일 뿐 실제 구 현에서는 다른 구분일 수 있다. 예를 들어, 복수의 유닛 또는 컴포넌트가 다른 시스템에 조합 또는 통합될 수 있거나, 일부 특징이 무시되거나 수행되지 않을 수 있다. 또한, 디스플레이되거나 논의된 상호 결합 또는 직접 결합 또는 통신 연결은 일부 인터페이스를 통해 구현될 수 있다. 장치 또는 유닛 간의 간접 결합 또는 통신 연 결은 전자 또는 기타 형태로 구현될 수 있다. 전술한 별도의 부분으로 설명된 유닛은 물리적으로 분리될 수도 있고 그렇지 않을 수도 있으며, 유닛으로 디스 플레이되는 부분은 물리적 유닛일 수도 있고 아닐 수도 있으며, 한 위치에 위치할 수도 있고, 복수의 네트워크 유닛에 분산되어 있을 수도 있다. 일부 또는 모든 유닛은 실시예에서의 솔루션의 목적을 달성하기 위해 실제 요 건에 따라 선택될 수 있다. 또한, 본 출원의 실시예에서의 기능 유닛은 하나의 처리 유닛으로 통합될 수 있거나, 각각의 유닛이 물리적으로 단독으로 존재할 수 있거나, 둘 이상의 유닛이 하나의 유닛으로 통합될 수 있다. 통합 유닛은 하드웨어 형태로 구현될 수도 있고, 소프트웨어 기능 유닛 형태로 구현될 수도 있다. 전술한 통합 유닛이 소프트웨어 기능 유닛의 형태로 구현되어 독립된 제품으로 판매 또는 사용될 때, 통합 유닛 은 컴퓨터가 판독 가능한 저장 매체에 저장될 수 있다. 이러한 이해를 바탕으로 본 출원의 본질적인 기술 솔루 션 또는 기존 기술에 기여하는 부분 또는 기술 솔루션의 전부 또는 일부가 소프트웨어 제품의 형태로 구현될 수 있다. 컴퓨터 소프트웨어 제품은 저장 매체에 저장되며, 컴퓨터 디바이스(개인용 컴퓨터, 서버 또는 네트워크 디바이스일 수 있으며 구체적으로 컴퓨터 디바이스의 프로세서일 수 있음)에게 본 출원의 실시예에 기술된 방법 에서의 단계 모두 또는 일부를 수행하도록 명령하기위한 여러 명령을 포함할 수 있다. 전술한 저장 매체는 USB 플래시 드라이브, 이동식 하드 디스크, 자기 디스크, 광 디스크, 읽기 전용 메모리(Read-Only Memory, 줄여서 ROM) 또는 랜덤 액세스 메모리(Random Access Memory, 줄여서 RAM)와 같은, 프로그램 코드를 저장할 수 있는 임 의의 매체를 포함한다. 전술한 실시예들은 단지 본 출원을 제한하는 것이 아니라 본 출원의 기술적 솔루션을 설명하기 위한 것이다. 본 출원은 전술한 실시예를 참조하여 상세하게 설명되지만, 당업자는 여전히 전술한 실시예에 기술된 기술적 솔루 션을 수정하거나, 본 출원의 실시예의 기술 솔루션의 정신과 범위를 벗어나지 않고, 그것의 일부 기술적 특징에 대해 동등한 대체를 할 수 있음을 이해해야 한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7a 도면7b 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15"}
{"patent_id": "10-2023-7002403", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 출원의 실시예 또는 배경에서 기술 솔루션을 보다 명확하게 설명하기 위해, 다음은 본 출원 또는 배경의 실 시예를 설명하기 위한 첨부 도면을 간략하게 설명한다. 도 1은 본 발명의 실시예에 따른 차량에서의 다인 상호 작용 시나리오의 개략도이다.도 2는 본 발명의 실시예에 따른 차량에서의 다인 상호 작용 시나리오의 개략도이다. 도 3은 본 발명의 실시예에 따른 시스템 아키텍처를 도시한다. 도 4는 본 발명의 실시예에 따른 콘볼루션 신경망의 개략도이다. 도 5는 본 발명의 실시예에 따른 신경망 트레이닝 방법의 개략적인 흐름도이다. 도 6은 본 발명의 실시예에 따른 사운드 파형의 예시도이다. 도 7a는 본 발명의 실시예에 따른 음성 명령 매칭 방법이다. 도 7b는 본 발명의 실시예에 따른 음성 명령 매칭 방법이다. 도 8은 본 발명의 실시예에 따른 클라우드 상호 작용 시나리오의 개략도이다. 도 9는 본 발명의 실시예에 따른 방법의 흐름도이다. 도 10은 본 발명의 실시예에 따른 방법의 흐름도이다. 도 11은 본 발명의 실시예에 따른 방법의 흐름도이다. 도 12는 본 발명의 실시예에 따른 방법의 흐름도이다. 도 13은 본 발명의 실시예에 따른 명령 제어 장치의 구조에 대한 개략도이다. 도 14는 본 발명의 실시예에 따른 신경망 트레이닝 장치의 구조에 대한 개략도이다. 도 15는 본 발명의 실시예에 따른 다른 명령 제어 시스템을 도시한다."}
