{"patent_id": "10-2023-0099341", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0018581", "출원번호": "10-2023-0099341", "발명의 명칭": "인공지능 기반 사용자 맞춤형 가창 음성 제공 방법, 장치 및 시스템", "출원인": "비케이앤미디어 주식회사", "발명자": "박병준"}}
{"patent_id": "10-2023-0099341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자로부터 합성대상사용자 및 합성대상노래를 포함하는 가창음성 합성요청을 획득하는 단계;상기 가창음성 합성요청에 기초하여, 상기 합성대상사용자의 가창음성 데이터, 상기 가창음성 데이터와 연관된운율 데이터 및 연관된 가사텍스트 데이터를 포함하는 전이대상 데이터를 획득하는 단계;상기 전이대상 데이터에 기초하여, 상기 가창음성 합성 장치의 인공신경망을 전이 학습하는 단계;상기 가창음성 합성요청에 기초하여, 상기 합성대상사용자의 가창음성 데이터, 상기 합성대상노래의 운율 데이터 및 가사텍스트 데이터를 획득하는 단계;상기 가창음성 데이터, 상기 합성대상노래의 운율 데이터 및 상기 합성대상노래의 가사텍스트 데이터에 기초하여, 상기 합성대상사용자의 음색을 나타내는 음색 특징벡터, 상기 합성대상사용자의 창법을 나타내는 창법 특징벡터, 상기 합성대상노래의 발음기호를 나타내는 발음기호 특징벡터 및 상기 합성대상노래의 음고를 나타내는음고 특징벡터를 추출하는 단계;상기 가창음성 합성 장치의 발음구조 생성부에 의해, 상기 음색 특징벡터 및 상기 발음기호 특징벡터에 기초하여 상기 합성대상사용자의 발음특성을 나타내는 발음구조 스펙트로그램을 생성하는 단계;상기 가창음성 합성 장치의 음고골격구조 생성부에 의해, 상기 창법 특징벡터 및 상기 음고 특징벡터에 기초하여 상기 합성대상사용자의 창법이 반영된 상기 합성대상노래의 음고골격을 나타내는 음고골격구조 스펙트로그램을 생성하는 단계; 및상기 발음구조 스펙트로그램 및 상기 음고골격구조 스펙트로그램에 기초하여 상기 합성대상사용자의 가창음성으로 표현된 상기 합성대상노래에 대한 가창음성 신호를 생성하는 단계를 포함하는 인공지능 기반 사용자 맞춤형 가창 음성 제공 방법."}
{"patent_id": "10-2023-0099341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 학습용 데이터 세트는 복수의 사용자의 가창음성 데이터, 복수의 노래에 대한 운율 데이터 및 가사텍스트데이터를 포함하고,상기 합성대상사용자는 상기 복수의 사용자에 포함되지 않은 사용자인 인공지능 기반 사용자 맞춤형 가창 음성 제공 방법."}
{"patent_id": "10-2023-0099341", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "사용자로부터 합성대상사용자 및 합성대상노래를 포함하는 가창음성 합성요청을 획득하고, 상기 가창음성 합성요청에 기초하여, 상기 합성대상사용자의 가창음성 데이터, 상기 가창음성 데이터와 연관된 운율 데이터 및 연관된 가사텍스트 데이터를 포함하는 전이대상 데이터를 획득하는 입력부;상기 가창음성 합성요청에 기초하여, 상기 합성대상사용자의 가창음성 데이터, 상기 합성대상노래의 운율 데이터 및 가사텍스트 데이터를 획득하고, 상기 가창음성 데이터, 상기 합성대상노래의 운율 데이터 및 상기 합성대상노래의 가사텍스트 데이터에 기초하여, 상기 합성대상사용자의 음색을 나타내는 음색 특징벡터, 상기 합성대상사용자의 창법을 나타내는 창법 특징벡터, 상기 합성대상노래의 발음기호를 나타내는 발음기호 특징벡터 및상기 합성대상노래의 음고를 나타내는 음고 특징벡터를 추출하는 전처리부;상기 음색 특징벡터 및 상기 발음기호 특징벡터에 기초하여, 상기 합성대상사용자의 발음특성을 나타내는 발음구조 스펙트로그램을 생성하는 발음구조 생성부;상기 창법 특징벡터 및 상기 음고 특징벡터에 기초하여, 상기 합성대상사용자의 창법이 반영된 상기 합성대상노공개특허 10-2025-0018581-3-래의 음고골격을 나타내는 음고골격구조 스펙트로그램을 생성하는 음고골격구조 생성부; 및상기 발음구조 스펙트로그램 및 상기 음고골격구조 스펙트로그램에 기초하여 상기 합성대상사용자의 가창음성으로 표현된 상기 합성대상노래에 대한 가창음성 신호를 생성하는 보코더부를 포함하고,상기 전처리부, 상기 발음구조 생성부 및 상기 음고골격구조 생성부는 상기 전이대상 데이터에 기초하여 전이학습하는, 인공지능 기반 사용자 맞춤형 가창 음성 제공 장치."}
{"patent_id": "10-2023-0099341", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공지능 기반 사용자 맞춤형 가창 음성 제공 방법은 사용자로부터 합성대상사용자 및 합성대상노래를 포함하는 가창음성 합성요청을 획득하고, 가창음성 합성요청에 기초하여, 합성대상사용자의 가창음성 데이터, 가창음성 데 이터와 연관된 운율 데이터 및 연관된 가사텍스트 데이터를 포함하는 전이대상 데이터를 획득하고, 전이대상 데 (뒷면에 계속)"}
{"patent_id": "10-2023-0099341", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "아래 실시예들은 인공지능 기반 사용자 맞춤형 가창 음성 제공 방법, 장치 및 시스템에 관한 것이다."}
{"patent_id": "10-2023-0099341", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "음성합성(TTS; Text To Speech) 기술은 컴퓨터를 이용하여 텍스트로 입력된 임의의 문장을 사람의 목소리, 즉 음성신호로 생성하는 기술을 의미한다. 종래의 음성합성 기술은 음성신호를 생성할 때 미리 녹음된 한 음절의 음성신호를 결합하여 문장 전체에 대한 음성신호를 생성하는 결합형 음성합성(Concatenative TTS) 방식과 음성 의 특징이 표현된 고차원 파라미터로부터 보코더(vocoder)를 이용하여 음성신호를 생성하는 매개변수 음성합성 (Parametric TTS) 방식으로 구분된다. 그러나, 종래의 결합형 음성합성 방식은 음성신호에서 문장의 억양, 운율 등이 표현되지 않아 음성 사이의 연결 이 어색하고 사람의 목소리와 이질감이 느껴지는 문제점이 있었고, 또한, 자연스로운 음성신호를 생성하기 위해 방대한 양의 음성 및 텍스트 데이터 세트가 필요하다는 단점이 있었다. 이러한 단점들은 가사 텍스트 및 악보 데이터 등을 이용하여 가창음성신호를 생성하는 기술인 가창음성 합성 (SVS; Singing Voice Synthesis) 기술에서도 마찬가지이다."}
{"patent_id": "10-2023-0099341", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "실시예들은 인공지능 기반 사용자 맞춤형 가창 음성 제공 방법, 장치 및 시스템에 관한 것이다."}
{"patent_id": "10-2023-0099341", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일실시예에 따르면, 인공지능 기반 사용자 맞춤형 가창 음성 제공 방법은 사용자로부터 합성대상사용자 및 합성 대상노래를 포함하는 가창음성 합성요청을 획득하는 단계; 상기 가창음성 합성요청에 기초하여, 상기 합성대상 사용자의 가창음성 데이터, 상기 가창음성 데이터와 연관된 운율 데이터 및 연관된 가사텍스트 데이터를 포함하 는 전이대상 데이터를 획득하는 단계; 상기 전이대상 데이터에 기초하여, 상기 가창음성 합성 장치의 인공신경 망을 전이 학습하는 단계; 상기 가창음성 합성요청에 기초하여, 상기 합성대상사용자의 가창음성 데이터, 상기 합성대상노래의 운율 데이터 및 가사텍스트 데이터를 획득하는 단계; 상기 가창음성 데이터, 상기 합성대상노래 의 운율 데이터 및 상기 합성대상노래의 가사텍스트 데이터에 기초하여, 상기 합성대상사용자의 음색을 나타내 는 음색 특징벡터, 상기 합성대상사용자의 창법을 나타내는 창법 특징벡터, 상기 합성대상노래의 발음기호를 나 타내는 발음기호 특징벡터 및 상기 합성대상노래의 음고를 나타내는 음고 특징벡터를 추출하는 단계; 상기 가창 음성 합성 장치의 발음구조 생성부에 의해, 상기 음색 특징벡터 및 상기 발음기호 특징벡터에 기초하여 상기 합 성대상사용자의 발음특성을 나타내는 발음구조 스펙트로그램을 생성하는 단계; 상기 가창음성 합성 장치의 음고 골격구조 생성부에 의해, 상기 창법 특징벡터 및 상기 음고 특징벡터에 기초하여 상기 합성대상사용자의 창법이 반영된 상기 합성대상노래의 음고골격을 나타내는 음고골격구조 스펙트로그램을 생성하는 단계; 및 상기 발음구 조 스펙트로그램 및 상기 음고골격구조 스펙트로그램에 기초하여 상기 합성대상사용자의 가창음성으로 표현된 상기 합성대상노래에 대한 가창음성 신호를 생성하는 단계를 포함한다. 상기 학습용 데이터 세트는 복수의 사용자의 가창음성 데이터, 복수의 노래에 대한 운율 데이터 및 가사텍스트 데이터를 포함하고, 상기 합성대상사용자는 상기 복수의 사용자에 포함되지 않은 사용자이다.인공지능 기반 사용자 맞춤형 가창 음성 제공 장치는, 사용자로부터 합성대상사용자 및 합성대상노래를 포함하 는 가창음성 합성요청을 획득하고, 상기 가창음성 합성요청에 기초하여, 상기 합성대상사용자의 가창음성 데이 터, 상기 가창음성 데이터와 연관된 운율 데이터 및 연관된 가사텍스트 데이터를 포함하는 전이대상 데이터를 획득하는 입력부; 상기 가창음성 합성요청에 기초하여, 상기 합성대상사용자의 가창음성 데이터, 상기 합성대상 노래의 운율 데이터 및 가사텍스트 데이터를 획득하고, 상기 가창음성 데이터, 상기 합성대상노래의 운율 데이 터 및 상기 합성대상노래의 가사텍스트 데이터에 기초하여, 상기 합성대상사용자의 음색을 나타내는 음색 특징 벡터, 상기 합성대상사용자의 창법을 나타내는 창법 특징벡터, 상기 합성대상노래의 발음기호를 나타내는 발음 기호 특징벡터 및 상기 합성대상노래의 음고를 나타내는 음고 특징벡터를 추출하는 전처리부; 상기 음색 특징벡 터 및 상기 발음기호 특징벡터에 기초하여, 상기 합성대상사용자의 발음특성을 나타내는 발음구조 스펙트로그램 을 생성하는 발음구조 생성부; 상기 창법 특징벡터 및 상기 음고 특징벡터에 기초하여, 상기 합성대상사용자의 창법이 반영된 상기 합성대상노래의 음고골격을 나타내는 음고골격구조 스펙트로그램을 생성하는 음고골격구조 생성부; 및 상기 발음구조 스펙트로그램 및 상기 음고골격구조 스펙트로그램에 기초하여 상기 합성대상사용자의 가창음성으로 표현된 상기 합성대상노래에 대한 가창음성 신호를 생성하는 보코더부를 포함하고, 상기 전처리부, 상기 발음구조 생성부 및 상기 음고골격구조 생성부는 상기 전이대상 데이터에 기초하여 전이 학습한 다."}
{"patent_id": "10-2023-0099341", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "실시예들은 합성대상사용자가 실제로 부르지 않은 노래를 합성대상사용자가 실제로 부른 것과 유사하게 가창음 성신호를 합성 및 생성할 수 있다. 실시예들은 인공신경망을 통해 사용자의 실제 가창음성과 가깝게 가창음성을 합성할 수 있다. 한편, 실시예들에 따른 효과들은 이상에서 언급한 것으로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아 래의 기재로부터 해당 기술 분야의 통상의 지식을 가진 자에게 명확히 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0099341", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서, 첨부된 도면을 참조하여 실시예들을 상세하게 설명한다. 그러나, 실시예들에는 다양한 변경이 가해 질 수 있어서 특허출원의 권리 범위가 이러한 실시예들에 의해 제한되거나 한정되는 것은 아니다. 실시예들에 대한 모든 변경, 균등물 내지 대체물이 권리 범위에 포함되는 것으로 이해되어야 한다. 실시예들에 대한 특정한 구조적 또는 기능적 설명들은 단지 예시를 위한 목적으로 개시된 것으로서, 다양한 형 태로 변경되어 실시될 수 있다. 따라서, 실시예들은 특정한 개시형태로 한정되는 것이 아니며, 본 명세서의 범 위는 기술적 사상에 포함되는 변경, 균등물, 또는 대체물을 포함한다. 제1 또는 제2 등의 용어를 다양한 구성요소들을 설명하는데 사용될 수 있지만, 이런 용어들은 하나의 구성요소 를 다른 구성요소로부터 구별하는 목적으로만 해석되어야 한다. 예를 들어, 제1 구성요소는 제2 구성요소로 명 명될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이해되어야 할 것이다. 실시예에서 사용한 용어는 단지 설명을 목적으로 사용된 것으로, 한정하려는 의도로 해석되어서는 안된다. 단 수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또 는 \"가지다\" 등의 용어는 명세서 상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 실시예가 속 하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의 미를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다. 또한, 첨부 도면을 참조하여 설명함에 있어, 도면 부호에 관계없이 동일한 구성 요소는 동일한 참조부호를 부여 하고 이에 대한 중복되는 설명은 생략하기로 한다. 실시예를 설명함에 있어서 관련된 공지 기술에 대한 구체적 인 설명이 실시예의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 실시예들은 퍼스널 컴퓨터, 랩톱 컴퓨터, 태블릿 컴퓨터, 스마트 폰, 텔레비전, 스마트 가전 기기, 지능형 자동 차, 키오스크, 웨어러블 장치 등 다양한 형태의 제품으로 구현될 수 있다. 이하의 본 발명의 실시예들에 대한 상세한 설명에서 기재된 용어는 다음과 같은 의미를 갖는다. “가창음성(singing voice)”은 사람의 목소리로 표현된 노래를 의미하고, “가창음성 신호”는 사람의 목소리 로 표현된 노래를 나타내는 신호를 의미한다. 다시 말해, 가창음성은 임의의 사람의 발성기관을 통하여 발생된 노래 소리를 의미한다. 가창음성은 노래를 부른 사람(즉, 사용자)에 따라 달라질 수 있으며, 이는 노래를 부른 사람의 “음색(timbre)” 및 “창법(singing style)”에 의하여 달라질 수 있다. 여기에서, “음색”은 신체 발 성기관의 구조에 의해 물리적으로 결정되는 사용자 고유의 목소리 특색으로 가창음성의 배음 구조(harmonic structure)에 의해 달라진다. “창법”은 후천적인 훈련을 통해 형성된 기교에 해당하는 사용자 고유의 목소리 특색으로서, 바이브레이션, 음의 세기, 높낮이 등을 포함한다. “음고골격 구조(pitch skeleton)”은 가창합성 의 골격이 되고 음의 높낮이 및 음색을 결정하는 배음 구조(harmonic structure)의 시계열적 데이터를 의미한다. “발음구조(formant mask)”는 발음에 의해 결정되는 고유한 주파수 성분인 포먼트(formant)의 시계 열적 데이터를 의미한다. 여기서 “포먼트”는 각 발음에 의해 결정되는 고유한 주파수 성분을 의미한다. 가창음성 합성 장치는 프로세서, 입력부, 전처리부, 음고골격구조 생성부, 발음구조 생성부, 보코더부, 출력부 및 스토리지를 포함한다. 프로세서는 가창음성 합성 장치 의 일반적인 테스크를 처리한다. 가창음성 합성 장치의 입력부는 사용자로부터 음성을 합성하고자 하는 대상인 합성대상사용자 및 사용 자의 음성으로 출력되기를 원하는 합성대상노래를 포함하는 가창음성 합성요청을 획득한다. 입력부는 사용 자로부터 가창음성의 합성을 원하는 대상 사용자 및 대상 노래에 대한 입력을 수신한다. 입력부의 예로는 키보드, 마우스, 터치 패널 등을 들 수 있다. 가창음성 합성 장치의 전처리부는 입력부를 통하여 입력된 가창음성 데이터, 운율 데이터 및 가 사텍스트 데이터를 인공신경망에 입력할 수 있는 형태(예를 들어, 특징벡터(feature vector))로 변환한다. 보다 구체적으로, 전처리부는 가창음성 합성요청에 기초하여, 합성대상사용자의 가창음성 데이터 및 합성 대상노래의 운율 데이터 및 가사텍스트 데이터를 스토리지로부터 획득한다. 전처리부는 획득된 가창 음성 데이터, 운율 데이터 및 가사텍스트 데이터로부터 합성대상사용자의 음색 특징벡터와 창법 특징벡터, 및 합성대상노래의 발음기호 특징벡터와 음고 특징벡터를 추출한다. 전처리부는 사용자특징추출부, 발음특징추출부 및 음고추출부를 포함할 수 있다. 사용 자특징추출부는 합성대상사용자의 가창음성 데이터로부터 합성대상사용자의 음색을 나타내는 음색 특징벡 터 및 합성대상사용자의 창법을 나타내는 창법 특징벡터를 추출한다. 이상에서 설명한 바와 같이 음색은 사용자 고유의 목소리 특색으로 가창음성 신호의 파형에 따라 달라지고, 창법은 후천적으로 형성된 고유의 목소리 특색 으로 음의 떨림(바이브레이션)을 포함한다. 음색 특징벡터는 합성대상사용자의 음색을 나타내고, 창법 특징벡터 는 합성대상사용자의 창법을 나타낸다. 가창음성 합성 장치의 발음특징추출부는 합성대상노래의 가사텍스트 데이터로부터 노래 가사의 발음 기호를 나타내는 발음기호 특징벡터를 추출한다. 보다 구체적으로, 발음특징추출부는 합성대상노래의 가 사텍스트 데이터에 포함된 문자들 각각을 대응되는 발음기호로 변환한다. 발음특징추출부는 노래 가사에 포함된 문자의 발음기호를 변환함으로써 발음기호 특징벡터를 추출한다. 본 발명의 실시예들에 따른 가창음성 합성 장치에서, 가사텍스트에 포함된 문자들을 발음기호로 변환할 때, 국제음성기호(IPA, International Phonetic Alphabet)으로 변환한다. 국제음성기호는 언어학에서 주로 사용되는 음성기록 체계로서, 영어, 한국어, 스페인어 등 현존하는 대부분의 언어의 소리를 표시할 수 있다. 여기에서, 발음기호는 국제음성기호(IPA)일 수 있으나, 이에 한정되지 않으며 발음을 표현하기 위한 별도의 규칙일 수 있다. 가창음성 합성 장치의 음고추출부는 합성대상노래의 운율 데이터로부터 합성대상노래의 음고(pitch) 를 나타내는 음고 특징벡터를 추출한다. 운율 데이터는 노래의 음고와 박자를 표현하는 데이터이다. 보다 구체 적으로, 운율 데이터는 노래에 포함된 음의 음고, 음의 길이(duration) 및 음의 세기(velocity)를 포함한다. 운 율 데이터의 예로는 MIDI(musical instrument interface) 데이터, MusicXML 데이터 등을 포함한다. 운율 데이 터는 다양한 악기 등에 의해 발생한 소리를 디지털적으로 표현하는 데이터이다. 가창음성 합성 장치의 발음구조 생성부는 합성대상노래의 가사 텍스트와 합성대상사용자의 가창음성 입력으로부터 각기 추출된 특징 벡터들로부터 발음 구조를 나타내는 발음구조 스펙트로그램을 생성한다. 본 발 명에서, 발음구조는 혀·이·입술 등을 이용하여 소리를 내기 위한 구조로서, 모음의 포먼트(formant) 성분 및 자음의 무성음 성분을 포함한다. 포먼트는 음성학에서 사용되는 용어로 각 발음에 의해 결정되는 고유한 주파수 성분을 의미하고, 무성음 성분은 공기가 치아 사이 좁은 틈을 통과하면서 발생하는 마찰음인 치찰음(sibilance) 및 폐에서 나오는 공기를 막았다가 그 막은 자리를 터뜨리면서 발생하는 파열음(plosive sound)을 포함한다. 발음구조 생성부는 전처리부에서 추출된 음색 특징벡터 및 발음기호 특징벡터에 기초하여 발음구조 스펙트로그램을 생성한다. 발음구조 생성부는 합성대상사용자의 고유의 음색을 합성대상노래의 가사에 반 영함으로써, 합성대상사용자의 고유한 목소리 특색이 반영된 발음구조를 생성한다. 이에 따라, 본 발명은 합성 대상 사용자가 합성대상노래의 가사텍스트에 포함된 문자, 단어들을 발음할 때 나타나는 고유한 특징을 표현할 수 있다. 가창음성 합성 장치의 음고골격구조 생성부는 전처리부에서 추출된 창법 특징벡터 및 음고 특징 벡터에 기초하여 음고골격구조 스펙트로그램을 생성한다. 음고(pitch)는 음의 높낮이를 의미하고, 음고골격 구 조(harmonic structure)는 음의 높낮이, 음의 시작점 및 길이(duration)을 나타낸다. 음고골격 구조는 사용자의 가창음성의 유성음(voiced sound) 또는 악기음과 같은 단선율의 오디오 신호에서 음의 높낮이와 관련된다. 여기 에서, 유성음은 직접적인 음의 높낮이를 결정하는 기본주파수(fundamental frequency) 및 기본주파수의 정수배 주파수를 갖는 배음으로 구성된다. 음고골격구조 생성부는 합성대상사용자의 특유의 창법을 합성대상노래 의 멜로디에 반영함으로써, 합성대상사용자의 특유의 창법이 반영된 음고골격구조를 생성한다. 가창음성 합성 장치의 보코더부는 잠재변수화된 특징 벡터들을 결합하여 음성신호로 합성한다. 보코더 부는 발음구조 스펙트로그램 및 음고골격구조 스펙트로그램에 기초하여 합성대상사용자의 가창음성으로 표 현된 합성대상노래에 대한 가창음성 신호를 생성한다. 보코더부는 발음구조 생성부 및 음고골격구조 생성부에서 생성된 발음구조 스펙트로그램을 이용하여 합성대상노래의 음고골격구조를 마스킹(masking)함 으로써 합성대상사용자의 목소리로 합성대상노래를 표현한 가창음성 신호를 생성한다. 여기에서, 생성된 가창음성 신호는 선형 또는 멜-스케일로 표현될 수 있다. 가창음성 신호가 멜 스케일로 표현 된 경우, 보코더부는 멜 스케일의 가창음성 신호를 선형 스케일로 변환하는 변환부를 더 포함할 수 있다. 본 실시예에 따른 가창음성 합성 장치의 전처리부, 발음구조 생성부, 음고골격구조 생성부 및 보코더부 각각은 다수의 레이어(layer)들을 포함하는 인공신경망으로 구현될 수 있다. 여기서, 인공신 경망은 여러 개의 퍼셉트론을 포함하는 다층 구조 퍼셉트론(multi layer perceptron), 다수의 컨볼루션 레이어 (convolution layer)를 포함하는 CNN(convolutional neural network), 순환구조를 갖는 RNN(recurrent neural network) 등으로 구현될 수 있다. 본 발명의 실시예에 따른 가창음성 합성 장치의 인공신경망은 컨볼루션 레이어를 포함하는 CNN으로 구현될 수 있고, 현재 및 과거의 스펙트로그램으로부터 미래의 스펙트로그램을 예측하도록 트레이닝되어 초기조건 입력 값으로부터 다음 프레임의 스펙트로그램을 출력할 수 있다. 예를 들어, 초기 조건으로 ‘0’이 사용될 수 있으 며, 본 발명의 실시예에 따른 가창음성 합성 장치의 인공신경망은 인공신경망의 출력이 피드백되어 인공신 경망의 입력되는 방식으로 자동 회귀적으로 생성된다. 인공신경망의 트레이닝 단계에서 인공신경망은 역전파 (back-propagation) 알고리즘을 통하여 계층 사이의 가중치가 트레이닝될 수 있으며, 이때 가중치의 기울기가 잘 전파될 수 있도록 인공신경망의 각 레이어에 연결 건너뛰기(skip connection)가 추가될 수 있다. 가창음성 합성 장치의 출력부는 가창음성 신호를 사용자가 들을 수 있는 청각적 신호로 변환하여 출력 한다. 출력부의 예로는 스피커를 들 수 있다. 가창음성 합성 장치의 스토리지는 가창음성 합성을 위하여 필요한 데이터를 저장한다. 예를 들어, 스토리지는 가창음성 합성 장치를 구성하는 인공신경망 을 트레이닝하기 위한 학습용 데이터 세트를 저장한다. 여기에서, 학습용 데이터 세트는 복수의 사용자의 가창음성 데이터, 복수의 노래에 대한 가사텍스트 데이터 및 운율 데이터를 포함한다. 또한, 학습용 데이터 세트는 복수의 사용자의 가창음성 데이터와 복수의 노래에 대한 가사텍스트 데이터 및 운율 데이터 사이의 연관성을 포 함할 수 있다. 본 발명의 실시예에 따른 가창음성 합성 장치에서, 전처리부, 음고골격구조 생성부, 발음구조 생 성부 및 보코더부는 프로세서와는 다른 별개의 전용 프로세서에 의해 구현될 수 있으며, 프로세 서에 의해 수행되는 컴퓨터 프로그램의 실행에 의하여 구현될 수도 있다. 가창음성 합성 장치는 이상에서 설명된 구성요소들 외에 추가적인 구성요소를 더 포함할 수 있다. 예를 들 어, 가창음성 합성 장치는 여러 구성요소들 간에 데이터를 전송하기 위한 버스를 포함하고, 각 구성요소에 구동전원을 공급하는 전원부, 합성된 가창음성 신호와 실제(GT, Ground Truth) 가창음성 신호를 구별하는 구별 기 및 인공신경망의 트레이닝을 위한 트레이닝부를 더 포함할 수 있다. 가창음성 합성 방법을 수행하는 가창음성 합성 장치는 본 발명의 일 실시예에 따른 가창음성 합성 방법을 수행 하기 전에 가창음성 합성 장치에 포함된 인공신경망(예를 들어, 전처리부, 발음구조 생성부, 음고골 격구조 생성부 및 보코더부)는 복수의 사용자에 대한 가창음성 데이터, 복수의 노래에 대한 운율 데 이터 및 가사텍스트 데이터를 포함하는 학습용 데이터 세트에 의해 미리 트레이닝되었다고 가정한다. 201 단계에서, 가창음성 합성 장치의 입력부는 사용자로부터 합성대상사용자 및 합성대상노래를 포함 하는 가창음성 합성요청을 획득한다. 입력부는 사용자로부터 사용자가 듣기를 원하는 합성대상사용자 및 합성대상노래를 입력받는다. 입력부는 합성대상사용자 및 합성대상노래를 포함하는 가창음성 합성요청을 전처리부로 입력한다. 여기에서, 합성대상사용자는 가창음성 합성 장치를 구성하는 인공신경망을 트레 이닝하는데 사용된 학습용 데이터 세트에 포함되는 사용자고, 학습용 데이터 세트는 복수의 사용자의 가창음성 데이터, 복수의 노래의 운율 데이터 및 가사텍스트 데이터를 포함한다. 합성대상노래의 운율 데이터 및 가사텍 스트 데이터는 스토리지에 저장된 데이터일 수 있으며, 사용자에 의해 입력된 데이터일 수 있다. 202 단계에서, 가창음성 합성 장치의 전처리부는 가창음성 합성요청에 기초하여, 합성대상사용자의 가 창음성 데이터 및 합성대상노래의 운율 데이터 및 가사텍스트 데이터를 스토리지로부터 획득한다. 여기에 서, 운율 데이터는 다양한 악기 등에 의해 발생한 소리를 디지털적으로 표현하는 데이터를 의미한다. 운율 데이 터는 음고(pitch), 음의 길이 및 음의 세기를 포함한다. 전처리부는 사용자가 가창음성 합성을 원하는 합 성대상 사용자의 가창음성 데이터를 스토리지로부터 획득하고, 가창음성 합성을 원하는 합성대상노래의 운율 데이 터 및 가사텍스트 데이터를 획득한다. 203 단계에서, 가창음성 합성 장치의 전처리부는 획득된 합성대상사용자의 가창음성데이터, 합성대상 노래의 운율 데이터 및 가사텍스트 데이터에 기초하여 합성대상사용자의 음색 특징벡터와 창법 특징벡터, 및 합 성대상노래의 가사에 대한 발음기호 특징벡터와 음고 특징벡터를 추출한다. 보다 구체적으로, 전처리부의 사용자 특징추출부는 획득된 합성대상사용자의 가창음성을 나타내는 가창음성 데이터로부터 합성대상사용 자의 음색을 나타내는 음색 특징벡터 및 합성대상사용자의 창법을 나타내는 창법 특징벡터를 추출한다. 여기에 서, 가창음성 데이터는 주파수가 멜 스케일(Mel-scale)로 표현되는 멜-스펙트로그램일 수 있다. 본 발명의 실시예에 따른 가창음성 합성 장치는 발음구조 생성부로 입력되는 텍스트 특성과 오디오 특 성의 시간 정렬을 위한 어텐션부(attention unit)를 더 포함할 수 있다. 본 발명의 실시예에 따른 어텐션부는 합성대상노래의 텍스트 특성인 발음기호 특성벡터와 오디오 특성인 음고 특성벡터를 정렬한다. 어텐션부는 정렬 된 음고 특성벡터를 생성한다. 2031 단계에서, 전처리부의 사용자특징추출부는 합성대상사용자의 가창음성 데이터로부터 합성대상 사용자의 음색을 나타내는 음색 특징벡터 및 합성대상사용자의 창법을 나타내는 창법 특징벡터를 추출한다. 사 용자특징추출부는 합성대상사용자의 고유한 목소리 특색을 나타내는 가창음성의 파형을 분석하고, 분석결 과에 기초하여 합성대상사용자의 음색을 나타내는 음색 특징벡터를 추출한다. 또한, 사용자특징추출부는 합성대상사용자의 가창음성에서 음성의 떨림(바이브레이션)과 같은 사용자 고유의 스타일을 분석하고, 분석 결 과에 기초하여 합성대상사용자의 창법을 나타내는 창법 특징벡터를 추출한다. 2032 단계에서, 전처리부의 발음특징추출부는 합성대상노래의 가사텍스트 데이터로부터 노래 가사에 포함된 문자의 발음기호를 나타내는 발음기호 특징벡터를 추출한다. 발음특징추출부는 가사텍스트에 포함 된 문자들을 분석하여 문자들의 발음에 해당하는 발음기호로 변환한다. 발음특징추출부는 변환된 발음기호로부터 합성대상노래의 발음을 나타내는 발음기호 특징벡터를 추출한다. 여기에서, 발음특징추출부는 문자가 아닌 발음기호를 이용하여 발음기호 특징벡터를 추출함으로써, 특정 언어에 제한되지 않고 다양한 언어 를 포함하는 가사텍스트로부터 발음기호 특징벡터를 추출할 수 있다. 예를 들어, 발음특징추출부는 문자 들의 발음에 해당하는 발음기호로 변환할 때 국제음성기호(IPA)로 변환할 수 있다. 2033 단계에서, 전처리부의 음고추출부는 합성대상노래의 운율 데이터로부터 합성대상노래의 음고를 나타내는 음고 특징벡터를 추출한다. 이상에서 설명한 바와 같이, 운율 데이터는 노래의 음고, 음의 길이 및 음 의 세기를 포함한다. 음고추출부는 합성대상노래의 운율 데이터로부터 합성대상 노래의 음고 특징벡터를 추출한다. 여기에서, 사용자특징추출부, 발음특징추출부 및 음고추출부는 사전에 트레이닝된 인공신경 망으로 구성된다. 사용자특징추출부는 복수의 사용자에 대한 복수의 가창음성 데이터를 포함하는 학습용 데이터셋으로 사전에 트레이닝되고, 발음특징추출부는 복수의 노래에 대한 가사텍스트 데이터를 포함하는 학습용 데이터셋으로 사전에 트레이닝되고, 음고추출부는 복수의 노래에 대한 운율 데이터를 포함하는 학 습용 데이터셋으로 사전에 트레이닝된 인공신경망이다. 사용자특징추출부. 발음특징추출부 및 음고 추출부는 각각 별개의 인공신경망일 수 있으나, 하나의 인공신경망일 수도 있다. 203 단계에서, 전처리부는 합성대상사용자의 가창음성데이터, 합성대상노래의 운율 데이터 및 가사텍스트 데이터로부터 추출된 음색 특징벡터, 창법 특징벡터, 발음기호 특징벡터 및 음고 특징벡터를 발음구조 생성부 및 음고골격구조 생성부로 입력한다. 보다 구체적으로, 전처리부는 음색 특징벡터 및 발음기호 특징벡터를 발음구조 생성부로 입력하고, 창법 특징벡터 및 음고 특징벡터를 음고골격구조 생성부로 입력한다. 204 단계에서, 가창음성 합성 장치의 발음구조 생성부는 추출된 음색 특징벡터 및 발음기호 특징벡터 에 기초하여 발음구조 스펙트로그램을 생성한다. 발음구조 생성부는 음색 특징벡터 및 발음기호 특징벡터 를 결합하고, 결합된 음색 특징벡터 및 발음기호 특징벡터로부터 합성대상사용자의 발음특성을 나타내는 발음구 조 스펙트로그램을 생성한다. 보다 구체적으로, 발음구조 생성부는 입력된 합성대상사용자의 음색을 나타 내는 음색 특징벡터와 합성대상노래의 가사텍스트의 발음을 나타내는 발음기호 특징벡터를 이용하여 합성대상사 용자가 합성대상노래의 가사텍스트에 포함된 문자, 단어들을 발음할 때 나타나는 고유한 특징을 나타내는 발음 구조 스펙트로그램을 생성한다. 발음구조 생성부는 합성대상사용자의 고유한 발음 특성을 나타내는 발음구조 스펙트로그램을 생성한다. 위 에서 설명한 바와 같이, 발음구조는 소리를 내기 위한 구조로서, 포먼트 성분 및 무성음 성분을 포함한다. 본 발명의 실시예들에 따른 가창음성 합성 방법은 발음구조 스펙트로그램을 생성할 때, 사용자별 고유한 발음 특색 을 고려하여 발음구조 스펙트로그램을 생성한다. 발음구조 생성부는 생성된 발음구조 스펙트로그램을 보코 더부로 입력한다. 205 단계에서, 가창음성 합성 장치의 음고골격구조 생성부는 추출된 창법 특징벡터 및 음고 특징벡터 에 기초하여 음고골격구조 스펙트로그램을 생성한다. 음고골격구조 생성부는 창법 특징벡터 및 음고 특징 벡터를 결합하고, 결합된 창법 특징벡터 및 음고 특징벡터로부터 합성대상사용자의 창법이 반영된 합성대상노래 의 음고골격을 나타내는 음고골격구조 스펙트로그램을 생성한다. 음고골격 구조(harmonic structure) 스펙트로 그램은 음의 높낮이, 음의 시작점 및 길이를 나타낸다. 음고골격 구조는 사용자의 음성의 유성음(voiced sound) 또는 악기음과 같은 단선율의 오디오 신호에서의 음고음의 높낮이인 음고와 관련되고, 사용자마다 상이한 창법 이 반영된 특성이다. 본 발명의 실시예들에 따른 가창음성 합성 방법은 음고골격구조 스펙트로그램을 생성할 때, 합성대상노래의 음고에 사용자별 고유한 창법이 반영된 음고골격구조 스펙트로그램을 생성한다. 음고골격구 조 생성부는 생성된 음고골격구조 스펙트로그램을 보코더부로 입력한다. 여기에서, 발음구조 생성부 및 음고골격구조 생성부는 사전에 트레이닝된 인공신경망으로 구성된다. 발음구조 생성부는 복수의 사용자에 대한음색 특징벡터 및 복수의 노래에 대한 발음기호 특징벡터를 포함 하는 학습용 데이터셋으로 사전에 트레이닝되고, 음고골격구조 생성부는 복수의 사용자에 대한 창법 특징 벡터 및 복수의 노래에 대한 음고 특징벡터를 포함하는 학습용 데이터셋으로 사전에 트레이닝된 인공신경망이다. 발음구조 생성부 및 음고골격구조 생성부는 각각 별개의 인공신경망이다. 이에 따라, 발음구조 스펙트로그램을 생성하는 204 단계와 음고골격구조 스펙트로그램을 생성하는 205 단계는 별개의 인공신경망에 의해 수행된다.206 단계에서, 가창음성 합성 장치의 보코더부는 발음구조 스펙트로그램 및 음고골격구조 스펙트로그 램에 기초하여 합성대상사용자의 가창음성으로 표현된 합성대상노래에 대한 가창음성 신호를 생성한다. 보코더 부는 발음구조 스펙트로그램 및 음고골격구조 스펙트로그램을 결합하여 합성대상사용자 및 합성대상노래의 가창음성 신호를 생성한다. 이상에서 설명한 바와 같이, 발음구조 생성부에서 합성대상사용자의 발음 특색 이 반영된 합성대상노래의 가사텍스트에 대한 발음구조 스펙트로그램을 생성하고, 음고골격구조 생성부에 서 합성대상사용자의 창법이 반영된 합성대상노래의 음고골격에 대한 음고골격구조 스펙트로그램을 생성한다. 보코더부는 상술한 방식으로 생성된 발음구조 스펙트로그램를 이용하여 합성대상노래의 음고골격구조를 마 스킹(masking)함으로써 합성대상사용자의 목소리로 합성대상노래를 표현한 가창음성 신호를 생성한다. 여기에서, 생성된 가창음성 신호는 선형 또는 멜-스케일로 표현될 수 있다. 생성된 가창음성 신호가 멜-스케일 스펙트로그램으로 생성된 경우, 본 발명의 일 실시예에 따른 가창음성 합성 방법은 생성된 멜 스케일 스펙트로 그램의 가창음성신호를 선형 스펙트로그램으로 변환하는 단계를 더 포함한다. 가창음성 신호가 멜 스케일 스펙 트로그램으로 표현된 경우, 보코더부는 멜 스케일 스펙트로그램의 가창음성 신호를 선형 스펙트로그램으로 변환하는 변환부를 더 포함할 수 있다. 보코더부는 생성된 가창음성 신호를 출력부로 입력한다. 207 단계에서, 가창음성 합성 장치의 출력부는 가창음성 신호를 출력한다. 출력부는 가창음성 신 호를 음파(sound wave)로 변환하여 출력한다. 가창음성 합성 방법은 사용자가 입력한 합성대상사용자가 합성대상노래를 부른 음파를 출력한다. 가창음성 합성 방법은 합성대상노래에 합성대상사용자 특유의 음색 및 창법이 반영된 음파를 사용자에게 제공할 수 있다. 본 발명의 일 실시예에 따른 가창음성 합성 방법은 인공신경망을 복수의 사용자에 대한 가창음성으로 트레이닝 함으로써, 복수의 사용자에 포함된 사용자와 동일·유사한 음색, 창법으로 표현된 노래에 대한 가창음성을 합성 할 수 있다. 이에 따라, 실제 사용자가 부르지 않은 노래라고 하더라도 사용자가 실제 부른 노래와 동일·유사 한 가창음성을 생성할 수 있다. 다만, 상술한 본 발명의 일 실시예에 따른 가창음성 합성 방법은 인공신경망을 트레이닝하는데 사용된 학습용 데이터 세트에 포함된 사용자에 한하여 가창음성을 합성할 수 있다. 인공신경망이 트레이닝되지 않은 사용자의 가창 음성과 동일·유사한 가창음성 신호는 생성할 수 없다. 학습용 데이터 세트에 포함되지 않은 사용자의 가창음성을 합성하기 위한 가창음성 합성 방법을 수행하는 가창 음성 합성 장치는 본 발명의 다른 실시예에 따른 가창음성 합성 방법을 수행하기 전에 가창음성 합성 장치에 포 함된 인공신경망(예를 들어, 전처리부, 발음구조 생성부, 음고골격구조 생성부 및 보코더부 )는 복수의 사용자에 대한 가창음성 데이터, 복수의 노래에 대한 운율 데이터 및 가사텍스트 데이터를 포 함하는 학습용 데이터 세트에 의해 미리 트레이닝되었다고 가정한다. 501 단계에서 가창음성 합성 장치의 입력부는 사용자로부터 합성대상사용자 및 합성대상노래를 포함하 는 가창음성 합성요청을 획득한다. 여기에서, 합성대상사용자는 가창음성 합성 장치를 구성하는 인공신경망 을 트레이닝하는데 사용된 학습용 데이터 세트에 포함되지 않은 사용자다. 502 단계에서, 가창음성 합성 장치의 입력부는 합성대상사용자의 가창음성 데이터, 가창음성 데이터와 연관된 운율 데이터, 및 가창음성 데이터와 연관된 가사텍스트 데이터를 포함하는 전이대상 데이터를 획득한다. 여기에서, 연관된 운율 데이터 및 가사텍스트 데이터 각각은 합성대상사용자의 가창음성 데이터에 의해 표현되 는 노래의 운율 데이터 및 가사텍스트 데이터이다. 입력부는 사용자로부터 합성대상사용자의 가창음성 데 이터, 가창음성 데이터와 연관된 운율 데이터, 및 가창음성 데이터와 연관된 가사텍스트 데이터를 입력받는 방 식으로 획득할 수 있다. 또한, 입력부는 사용자로부터 입력된 가창음성 합성요청에 기초하여 스토리지에 저장된 합성대상사용 자의 가창음성 데이터, 가창음성 데이터와 연관된 운율 데이터, 및 가창음성 데이터와 연관된 가사텍스트 데이 터를 획득할 수 있다. 전이대상 데이터는 사전에 트레이닝된 인공신경망을 전이 학습(transfer learning)시키기 위한 데이터이다. 입력부는 획득된 전이대상 데이터를 전처리부, 발음구조 생성부 및 음고골격 구조 생성부로 입력한다. 획득된 합성대상사용자의 가창음성 데이터는 최소필요시간 이상의 합성대상사용자의 가창음성을 포함한다. 최소 필요시간은 예를 들어, 5분, 7분 또는 10분일 수 있다. 최소필요시간은 미리 트레이닝된 인공신경망이 새로 입 력된 m사용자의 가창음성을 합성할 수 있도록 전이학습이 가능한 최소한의 가창음성의 시간을 의미한다. 예를 들어, 최소필요시간이 5분인 경우, 가창음성 합성 장치는 인공신경망을 전이학습시키기 위하여 합성대상사용자의 가창 음성이 5분 이상 포함된 가창음성 데이터, 연관된 운율 데이터 및 연관된 가사텍스트 데이터를 필 요로 한다. 503 단계에서, 가창음성 합성 장치의 인공신경망(즉, 전처리부, 발음구조 생성부 및 음고골격구 조 생성부)은 입력된 전이대상 데이터에 기초하여 전이 학습한다. 인공신경망인 전처리부, 발음구조 생성부 및 음고골격구조 생성부는 전이대상 데이터에 포함된 합성대상사용자의 가창음성 데이터, 가 창 음성 데이터와 연관된 노래의 운율 데이터 및 가사텍스트 데이터를 이용하여 전이 학습한다. 이상에서 설명한 바와 같이, 전처리부, 발음구조 생성부 및 음고골격구조 생성부 각각은 학습용 데이터 세트에 의해 트레이닝된 인공신경망이다. 여기에서, 학습용 데이터 세트에 포함되지 않은 사용자의 가창 음성 데이터, 가창음성 데이터와 연관된 노래의 운율 데이터 및 가사텍스트 데이터를 전처리부, 발음구조 생성부 및 음고골격구조 생성부에 입력하여 전이 학습을 수행한다. 전이 학습은 미리 트레이닝된 인 공신경망에 트레이닝에 사용되지 않은 사용자의 가창음성 데이터, 연관된 운율 데이터, 및 연관된 가사텍스트 데이터를 입력하여 인공신경망을 다시 트레이닝하는 과정을 의미한다. 각 인공신경망은 가창음성 데이터, 운율 데이터 및 가사텍스트 데이터로부터 음색 특징벡터, 창법특징 벡터, 발 음기호 특징벡터 및 음고 특징벡터를 추출하도록 학습용 데이터 세트에 의하여 사전에 트레이닝되어 있다. 다시 말해, 각 인공신경망은 인공신경망을 구성하는 계층(layer)들 사이의 필터 및 가중치가 이미 결정되어 있기 때 문에, 소정의 가창음성 데이터, 연관된 운율 데이터 및 연관된 가사텍스트 데이터를 포함하는 전이대상 데이터 세트의 입력만으로 새로운 사용자에 대한 트레이닝을 완료할 수 있다. 새로 트레이닝된 인공신경망은 전이대상 데이터 세트에 포함된 사용자와 유사한 가창음성 신호를 합성할 수 있다. 다시 말해, 전이학습된 전처리부, 발음구조 생성부 및 음고골격구조 생성부는 학습용 데이터 세 트에 포함되지 않았던 합성대상사용자의 가창음성 데이터로부터 합성대상사용자의 음색특징 벡터, 창법 특징벡 터, 발음구조 스펙트로그램 및 음고골격구조 스펙트로그램을 생성할 수 있다. 504 단계에서, 가창음성 합성 장치의 전처리부는 가창음성 합성요청에 기초하여 합성대상사용자의 가 창 음성 데이터 및 합성대상노래의 운율 데이터 및 가사텍스트 데이터를 스토리지로부터 획득한다. 합성대 상사용자의 가창음성 데이터 및 합성대상노래의 운율 데이터 및 가사텍스트 데이터를 획득하는 것에 대한 상세 한 설명은 202 단계에 관한 설명으로 갈음하기로 한다. 505 단계에서, 가창음성 합성 장치의 전처리부는 획득된 합성대상사용자의 가창음성데이터, 합성대상 노래의 운율 데이터 및 가사텍스트 데이터에 기초하여 합성대상사용자의 음색 특징벡터와 창법 특징벡터, 및 합 성대상노래의 가사에 대한 발음기호 특징벡터와 음고 특징벡터를 추출한다. 여기에서, 전처리부는 503 단 계에서 전이학습된 전처리부이다. 전이학습된 전처리부의 사용자특징추출부는 합성대상사용자 의 가창음성을 나타내는 가창음성 데이터로부터 합성대상사용자의 음색을 나타내는 음색 특징벡터 및 합성대상 사용자의 창법을 나타내는 창법 특징벡터를 추출한다. 이상에서 설명한 바와 같이, 사용자특징추출부는 사용자가 요청한 합성대상사용자가 학습용 데이터 세트 에 포함되어 있지 않으나 전처리부의 사용자특징추출부가 503 단계에서 전이학습됨에 따라, 합성대 상사용자의 가창음성데이터로부터 합성대상사용자의 음색 특징벡터 및 창법 특징벡터를 추출할 수 있다. 합성대 상사용자의 음색특징벡터와 창법 특징벡터, 및 합성대상노래의 가사에 대한 발음기호 특징벡터와 음고 특징벡터 를 추출하는 것에 대한 상세한 설명은 203 단계에 관한 설명으로 갈음하기로 한다. 506 단계에서, 가창음성 합성 장치의 발음구조 생성부는 추출된 음색 특징벡터 및 발음기호 특징벡터 에 기초하여 발음구조 스펙트로그램을 생성한다. 여기에서, 발음구조 생성부는 503 단계에서 전이학습된 발음구조 생성부이다. 이에 따라, 발음구조 생성부는 음색 특징벡터 및 발음기호 특징벡터를 결합하 고, 결합된 음색 특징벡터 및 발음기호 특징벡터로부터 합성대상사용자의 발음특성을 나타내는 발음구조 스펙트 로그램을 생성할 수 있다. 발음구조 스펙트로그램을 생성하는 것에 대한 상세한 설명은 204 단계에 관한 설명으 로 갈음하기 한다. 507 단계에서, 가창음성 합성 장치의 음고골격구조 생성부는 추출된 창법 특징벡터 및 음고 특징벡터 에 기초하여 음고골격구조 스펙트로그램을 생성한다. 여기에서, 음고골격구조 생성부는 503 단계에서 전이 학습된 음고골격구조 생성부이다. 이에 따라, 음고골격구조 생성부는 창법 특징벡터 및 음고 특징벡 터를 결합하고, 결합된 창법 특징벡터 및 음고 특징벡터로부터 합성대상사용자의 창법이 반영된 합성대상노래의 음고골격을 나타내는 음고골격구조 스펙트로그램을 생성할 수 있다. 음고골격구조 스펙트로그램을 생성하는 것에 대한 상세한 설명은 205 단계에 관한 설명으로 갈음하기로 한다. 508 단계에서, 가창음성 합성 장치의 보코더부는 발음구조 스펙트로그램 및 음고골격구조 스펙트로그 램에 기초하여 합성대상사용자의 가창음성으로 표현된 합성대상노래에 대한 가창음성 신호를 생성한다. 가창음 성 신호를 생성하는 것에 대한 상세한 설명은 206 단계에 관한 설명으로 갈음하기로 한다. 509 단계에서, 가창음성 합성 장치의 출력부는 가창음성 신호를 출력한다. 출력부는 보코더부 에서 생성된 가창음성 신호를 음파로 변환하여 출력한다. 본 발명의 다른 실시예에 따른 가창음성 합성 방법은 학습용 데이터 세트로 트레이닝된 인공신경망을 학습용 데 이터 세트에 포함되지 않은 사용자의 소정의 가창음성 데이터, 연관된 운율 데이터 및 연관된 가사텍스트 데이 터를 이용하여 전이 학습함으로써, 학습용 데이터 세트에 포함되지 않은 사용자의 가창음성을 합성할 수 있다. 이상에서 설명한 바와 같이 본 발명의 실시예들에 따른 가창음성 합성 장치는 복수 개의 인공신경망을 포함 한다. 가창음성 합성 장치는 상술한 구성요소 이외에 구별기(discriminator)를 더 포함할 수 있다. 구별기 는 보코더부에서 생성된 가창음성 신호가 진짜인지 가짜인지를 판별한다. 보다 구체적으로, 구별기는 전처 리부, 음고골격구조 생성부, 발음구조 생성부 및 보코더부에서 생성된 가창음성신호와 실 제(ground truth) 가창음성 신호를 비교하여 오류를 측정할 수 있다. 구별기는 별개의 인공신경망으로 구성될 수 있다. 이하에서, 설명의 편의를 위하여 가창음성 신호를 생성하는 전처리부, 음고골격구조 생성부, 발음구 조 생성부 및 보코더부를 합쳐서 생성기(generator)라고 하기로 한다. 생성기 및 구별기를 포함하는 가창음성 합성 장치는 대립적 손실(adversarial loss)을 이용한 방식으로 트레이닝될 수 있다. 이상에서 설명된 실시예들은 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨 어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치, 방법 및 구성요소는, 예를 들 어, 프로세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마 이크로컴퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령(instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 운영 체제 상에서 수행되는 하나 이 상의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설"}
{"patent_id": "10-2023-0099341", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치 는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단 독으로 또는 조합하여 포함할 수 있다. 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기 록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD 와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 된 하드웨어 장치는 실시예의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역 도 마찬가지이다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(signal wave)에 영구적으로, 또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서,분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다."}
{"patent_id": "10-2023-0099341", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같이 실시예들이 비록 한정된 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가진 자라면 를 기초로 다양한 기술적 수정 및 변형을 적용할 수 있다. 예를 들어, 설명된 기술들이 설명된 방법과 다른 순 서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태로 결 합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 청구범위의 범위에 속한다."}
{"patent_id": "10-2023-0099341", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일실시예에 따른 인공지능 기반 사용자 맞춤형 가창 음성 제공 장치의 블록도이다. 도 2는 일실시예에 따른 인공지능 기반 사용자 맞춤형 가창 음성 제공 방법의 순서도이다. 도 3은 다른 실시예에 따른 인공지능 기반 사용자 맞춤형 가창 음성 제공 방법의 순서도이다."}
