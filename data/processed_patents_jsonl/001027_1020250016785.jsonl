{"patent_id": "10-2025-0016785", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0026790", "출원번호": "10-2025-0016785", "발명의 명칭": "인공지능 코어, 인공지능 코어 시스템 및 인공지능 코어 시스템의 로드/스토어 방법", "출원인": "리벨리온 주식회사", "발명자": "오진욱"}}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "2차원 매트릭스 연산을 수행하여, 아웃풋 액티베이션(activation)을 생성하도록 구성된 프로세싱 유닛;제1 메모리; 및상기 제1 메모리 및 제2 메모리 사이의 메모리 액세스 작업을 수행하도록 구성된 로드/스토어 유닛(LSU)을 포함하고,상기 메모리 액세스 작업은,상기 프로세싱 유닛에 의해 수행되는 현재의 프로세싱 작업에 대한 메인 메모리 액세스 작업 및 상기 현재 실행되는 프로세싱 동작 이후에 상기 프로세싱 유닛에 의해 수행되는 대기(standby) 프로세싱 작업에 대한 대기 메모리 액세스 작업을 포함하고,상기 대기 메모리 액세스 작업은,데이터 채널의 밴드위스(bandwidth) 중 상기 메인 메모리 액세스 작업에 의해서 사용되지 않는 밴드위스를 사용하여 수행되는,인공지능 코어."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 프로세싱 유닛은,인풋 액티베이션 및 가중치를 수신하고,상기 인풋 액티베이션 및 상기 가중치를 이용하여 2차원 매트릭스 연산을 수행하고,상기 2차원 매트릭스 연산의 결과를 아웃풋 액티베이션으로 생성하도록 구성된,인공지능 코어."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 제1 메모리는,상기 프로세싱 유닛에 의해 수행되는 연산에 사용되는 프로그램, 상기 인풋 액티베이션 및 상기 가중치를 일시적으로 저장하고,상기 저장된 프로그램, 상기 저장된 인풋 액티베이션 및 상기 저장된 가중치를 상기 프로세싱 유닛으로 전송하고, 상기 프로세싱 유닛으로부터 수신된 상기 아웃풋 액티베이션을 일시적으로 저장하도록 구성된,인공지능 코어."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "공개특허 10-2025-0026790-3-제2 항에 있어서,상기 인풋 액티베이션을 상기 프로세싱 유닛으로 제공하고, 상기 프로세싱 유닛으로부터 상기 아웃풋 액티베이션을 수신하고, 상기 인풋 액티베이션 및 상기 아웃풋 액티베이션을 일시적으로 저장하도록 구성된 액티베이션버퍼 및 상기 제1 메모리에서 상기 액티베이션 버퍼로 상기 인풋 액티베이션을 전송하고, 상기 액티베이션 버퍼에서 상기 제1 메모리로 상기 아웃풋 액티베이션을 전송하도록 구성된 액티베이션 로드/스토어 유닛을 더 포함하는,인공지능 코어."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 제1 메모리는 상기 프로세싱 유닛과 동일한 칩에 형성되고,상기 제2 메모리가 형성된 칩은 상기 제1 메모리가 형성된 칩과 상이한,인공지능 코어."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 로드/스토어 유닛은,태스크 컨트롤러로부터 대기 로드 인스트럭션 또는 대기 스토어 인스트럭션을 수신하고, 상기 수신된 대기 로드인스트럭션 또는 상기 수신한 대기 스토어 인스트럭션을 페치(fetch) 및 이슈(issue)하고, 상기 대기 로드 인스트럭션에 대한 데이터를 상기 제2 메모리에서 상기 제1 메모리로 로드하고, 상기 대기 스토어 인스트럭션에 대한 데이터를 상기 제1 메모리에서 상기 제2 메모리로 저장하도록 구성된,인공지능 코어."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 로드/스토어 유닛은,메인 로드 인스트럭션 및 메인 스토어 인스트럭션을 페치 및 이슈하고, 상기 메인 로드 인스트럭션에 대한 데이터를 상기 제2 메모리에서 상기 제1 메모리로 로드하고, 상기 메인 스토어 인스트럭션에 대한 데이터를 상기 제1 메모리에서 상기 제2 메모리로 저장하도록 구성된,인공지능 코어."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항에 있어서,상기 메인 메모리 액세스를 위해 전송된 데이터는 상기 대기 메모리 액세스를 위해 전송된 데이터보다 더 높은우선 순위(priority)를 가진,인공지능 코어."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "공개특허 10-2025-0026790-4-제8 항에 있어서,상기 우선 순위는 상기 제1 메모리 및 상기 제2 메모리 사이의 데이터에 태깅되는,인공지능 코어."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항에 있어서,상기 로드/스토어 유닛은 라운드 로빈 방식으로 상기 제1 메모리 및 상기 제2 메모리 사이의 데이터를 전송하도록 더 구성되는,인공지능 코어."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "각각 적어도 하나의 인공지능 코어;상기 적어도 하나의 인공지능 코어 사이에 공유되는 제1 공유 메모리; 및상기 적어도 하나의 인공지능 코어 및 상기 공유 메모리 사이에 데이터를 전송하도록 구성된 데이터 채널을 포함하는 글로벌 인터커넥션을 포함하고,상기 적어도 하나 인공지능 코어는,2차원 매트릭스 연산을 수행하도록 구성된 프로세싱 유닛;프라이빗 메모리; 및상기 프라이빗 메모리 및 상기 적어도 하나의 공유 메모리 사이의 메모리 액세스 작업을 수행하도록 구성된 로드/스토어 유닛을 포함하고,상기 메모리 액세스 작업은 상기 프로세싱 유닛에 의해 수행되는 현재의 프로세싱 작업에 대한 메인 메모리 액세스 작업 및 상기 현재 프로세싱 작업 이후에 상기 프로세싱 유닛에 의해 수행되는 대기 프로세싱 작업에 대한대기 메모리 액세스 작업을 포함하고상기 대기 메모리 액세스 작업은 상기 데이터 채널의 밴드위스 중 상기 메인 메모리 액세스 작업에 사용되지 않은 밴드위스를 사용하여 수행되는,인공지능 코어 시스템."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,로컬 인터커넥션을 더 포함하고,상기 로컬 인터커넥션은 상기 적어도 하나의 인공지능 코어 사이의 데이터를 전송하도록 구성되고,상기 로드/스토어 유닛은 상기 로컬 인터커넥션의 밴드위스 중 상기 메인 메모리 액세스 작업에 사용되지 않은밴드위스를 사용하는 상기 대기 메모리 액세스 작업을 수행하는,인공지능 코어 시스템."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "공개특허 10-2025-0026790-5-제11 항에 있어서,상기 메인 메모리 액세스 작업은 상기 대기 메모리 액세스 작업보다 높은 우선 순위를 가지는,인공지능 코어 시스템."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11 항에 있어서,상기 적어도 하나의 공유 메모리는 상기 제1 공유 메모리 및 제2 공유 메모리를 포함하고,상기 제2 공유 메모리는 상기 적어도 하나의 공유 메모리 외부에 배치되도록 구성된,인공지능 코어 시스템."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 로드/스토어 유닛은 상기 데이터 채널을 통해 상기 제1 공유 메모리 또는 상기 제2 공유 메모리 중 적어도하나와 동작 가능하도록 결합되는,인공지능 코어 시스템."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서,상기 프라이빗 메모리는 상기 제1 공유 메모리 및 상기 제2 공유 메모리와 다른 칩에 형성된,인공지능 코어 시스템."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항에 있어서,상기 메인 메모리 액세스 작업은,메인 로드 인스트럭션 및 메인 스토어 인스트럭션을 포함하고,상기 대기 메모리 액세스 작업은,대기 로드 인스트럭션 및 대기 스토어 인스트럭션을 포함하고,상기 로드/스토어 유닛은 상기 로드 인스트럭션 또는 상기 대기 로드 인스트럭션을 위하여 상기 제2 메모리에서상기 제1 메모리로의 데이터를 로드하고, 상기 스토어 인스트럭션 또는 상기 대기 로드 인스트럭션을 위하여 상기 제1 메모리에서 상기 제2 메모리로의 데이터를 저장하는,인공지능 코어 시스템."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "로드/스토어 유닛 및 프로세싱 유닛을 포함하는 인공지능 코어 시스템의 로드/스토어 방법에 있어서, 상기 로드/스토어 유닛에 의해, 제1 작업에 대한 제1 데이터를 제2 메모리에서 제1 메모리로 로드하는 단계;공개특허 10-2025-0026790-6-상기 프로세싱 유닛에 의해, 상기 제1 데이터를 사용하여 상기 제1 작업을 실행하는 단계;상기 로드/스토어 유닛에 의해, 상기 제1 작업 이후에 수행되는 제2 작업에 대한 제2 데이터를 로드하는 단계;및상기 제1 작업을 실행하는 단계 및 상기 제2 데이터를 로드하는 단계가 완료된 이후에, 상기 프로세싱 유닛에의해, 상기 제2 데이터를 사용하여 상기 제2 작업을 실행하는 단계;상기 제2 데이터는 데이터 채널의 밴드위스 중 상기 제1 데이터를 로드하는 데 사용되지 않는 밴드위스를 사용하여 로드되는,인공지능 코어 시스템의 로드/스토어 방법."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 제1 작업을 실행하는 단계는,액티베이션 버퍼 내에서 인풋 액티베이션을 저장하는 단계;상기 액티베이션 버퍼로부터 상기 인풋 액티베이션을 수신하여, 상기 프로세싱 유닛에 의해, 아웃풋 액티베이션을 생성하는 단계; 및상기 액티베이션 버퍼 내에서 상기 아웃풋 액티베이션을 저장하는 단계를 포함하는,인공지능 코어 시스템의 로드/스토어 방법."}
{"patent_id": "10-2025-0016785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제18항에 있어서,상기 제1 작업은 신경망의 제1 레이어의 매트릭스 연산 작업이고, 상기 제2 작업은 신경망의 제2 레이어의 매트릭스 연산 작업인,인공지능 코어 시스템의 로드/스토어 방법."}
{"patent_id": "10-2025-0016785", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공지능 코어, 인공지능 코어 시스템 및 인공지능 코어 시스템의 로드/스토어 방법을 개시한다. 상기 인공지능 코어는, 인풋 액티베이션 및 웨이트를 수신하여 2차원 매트릭스 연산을 통해 아웃풋 액티베이션을 생성 하는 프로세싱 유닛 및 외부 인터페이스를 통해서 수신한 상기 프로그램 및 상기 입력 데이터를 상기 온 칩 버퍼 로 전달하고, 상기 출력 데이터를 상기 온 칩 버퍼로부터 상기 인터페이스로 전달하는 로드/스토어 작업을 수행 하고, 상기 로드/스토어 작업은 상기 프로세싱 유닛이 수행하는 현재 실행 작업에 대한 메인 로드/스토어 작업과, 상기 프로세싱 유닛이 상기 현재 실행 작업 다음에 실행하는 대기 실행 작업에 대한 대기 로드/스토어 작업을 포함하는 로드/스토어 유닛을 포함한다."}
{"patent_id": "10-2025-0016785", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 코어, 인공지능 코어 시스템 및 인공지능 코어 시스템의 로드/스토어 방법에 관한 것이다. 구체적으로, 본 발명은 인공지능 코어를 최대한 활용하기 위한 인공지능 코어, 인공지능 코어 시스템 및 인공지 능 코어 시스템의 로드/스토어 방법에 관한 것이다."}
{"patent_id": "10-2025-0016785", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "지난 몇년간, 인공지능(Artificial Intelligence) 기술은 4차 산업혁명의 핵심 기술로 전세계적으로 가장 유망 한 기술로 거론되고 있다. 이러한 인공지능 기술의 가장 큰 문제는 컴퓨팅 성능이다. 인간의 학습능력과 추론능 력, 지각능력, 자연언어의 이행능력 등을 실현해내는 인공지능 기술은 많은 데이터를 빠르게 처리하는 것이 가 장 중요하다. 초기 인공지능의 딥러닝 학습과 추론에는 기성 컴퓨터의 중앙처리장치(CPU; Central processing unit)나 그래픽 처리장치(GPU; Graphics Processing Unit)가 쓰였지만, 높은 워크 로드를 가지는 딥러닝 학습 및 추론의 작업에 는 한계가 있어 구조적으로 딥러닝 작업에 특화된 인공지능 코어가 각광받고 있다. 인공지능 코어는 내부에 많은 수의 곱셈 연산 장치를 포함하고 있고, 이러한 연산 장치의 연산 작업을 위해서 필요한 데이터와 프로그램을 불러오기 위한 밴드위스(bandwidth)가 충분하게 확보되는 것은 어렵다.따라서, 시계열적으로 미리 다음 작업에 필요한 프로그램과 데이터를 불러오는 방식을 통해서 인공지능 코어의 성능을 향상시키는 것은 매우 좋은 방법일 수 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 등록특허공보 제10-2258566호"}
{"patent_id": "10-2025-0016785", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 과제는, 효율적으로 극대화할 수 있는 인공지능 코어를 제공하는 것이다. 또한, 본 발명의 다른 과제는, 외부 인터페이스와의 밴드위스를 효율적으로 극대화할 수 있는 인공지능 코어 시 스템을 제공하는 것이다. 또한, 본 발명의 또 다른 과제는, 외부 인터페이스와의 밴드위스를 효율적으로 극대화할 수 있는 인공지능 코어 시스템의 로드/스토어 방법을 제공하는 것이다. 본 발명의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 발명의 다른 목적 및 장점들 은 하기의 설명에 의해서 이해될 수 있고, 본 발명의 실시예에 의해 보다 분명하게 이해될 것이다. 또한, 본 발 명의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것 이다."}
{"patent_id": "10-2025-0016785", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 과제를 해결하기 위한 본 발명의 몇몇 실시예에 따른 인공지능 코어는, 인풋 액티베이션 및 웨이트를 수신 하여 2차원 매트릭스 연산을 통해 아웃풋 액티베이션을 생성하는 프로세싱 유닛 및 외부 인터페이스를 통해서 수신한 상기 프로그램 및 상기 입력 데이터를 상기 온 칩 버퍼로 전달하고, 상기 출력 데이터를 상기 온 칩 버 퍼로부터 상기 외부 인터페이스로 전달하는 로드/스토어 작업을 수행하고, 상기 로드/스토어 작업은 상기 프로 세싱 유닛이 현재 수행하는 현재 실행 작업에 대한 메인 로드/스토어 작업과, 상기 프로세싱 유닛이 상기 현재 실행 작업 이후에 실행하는 대기 실행 작업에 대한 대기 로드/스토어 작업을 포함하는 로드/스토어 유닛을 포함 한다. 또한, 상기 인풋 액티베이션을 상기 프로세싱 유닛으로 제공하고, 상기 아웃풋 액티베이션을 상기 프로세싱 유 닛으로부터 수신하고, 상기 인풋 액티베이션 및 상기 아웃풋 액티베이션을 일시적으로 저장하는 액티베이션 버 퍼와, 상기 프로세싱 유닛이 연산을 하기위한 프로그램 및 입력 데이터를 일시적으로 저장하여 상기 프로세싱 유닛으로 전달하고, 상기 프로세싱 유닛으로부터 수신한 출력 데이터를 일시적으로 저장하고, 상기 입력 데이터 는 상기 인풋 액티베이션 및 상기 웨이트를 포함하는 온 칩 버퍼와, 상기 온 칩 버퍼로부터 상기 인풋 액티베이 션을 상기 액티베이션 버퍼로 전달하고, 상기 액티베이션 버퍼로부터 상기 아웃풋 액티베이션을 상기 온 칩 버 퍼로 전달하는 액티베이션 로드/스토어 유닛을 포함할 수 있다. 또한, 상기 대기 로드/스토어 작업은 상기 외부 인터페이스의 밴드위스 중 상기 메인 로드/스토어 작업에 의해 서 사용되지 않는 밴드위스를 사용하여 이루어질 수 있다. 또한, 상기 로드/스토어 유닛은, 상기 메인 로드/스토어 작업을 수행하고, 상기 온 칩 버퍼로 제1 로드 데이터 및 제1 스토어 데이터를 전송하는 메인 로드/스토어 유닛과, 상기 대기 로드/스토어 작업을 수행하고, 상기 온 칩 버퍼로 제2 로드 데이터 및 제2 스토어 데이터를 전송하는 히든 로드/스토어 유닛을 포함할 수 있다. 또한, 상기 히든 로드/스토어 유닛은, 태스크 컨트롤러로부터 수신된 대기 로드 인스트럭션을 페치하여 대기 로 드 인스트럭션 이슈를 수행하는 히든 로드 유닛과, 상기 태스크 컨트롤러로부터 수신된 대기 스토어 인스트럭션 을 페치하여 대기 스토어 인스트럭션 이슈를 수행하는 히든 스토어 유닛과, 상기 히든 로드 유닛으로부터 상기 로드 인스트럭션에 대응하는 메모리 액세스 요청을 순차적으로 수신하는 히든 로드 버퍼와, 상기 히든 스토어 유닛으로 상기 로드 인스트럭션에 대응하는 메모리 액세스 요청을 순차적으로 수신하는 히든 스토어 버퍼와, 상기 히든 로드 버퍼로부터 메모리 액세스 요청을 수신하여 상기 제2 로드 데이터를 상기 온 칩 버퍼로 전달하는 히든 로드 엔진과, 상기 히든 스토어 버퍼로부터 메모리 액세스 요청을 수신하여 상기 제2 스토어 데이터를 상 기 온 칩 버퍼로 전달하는 히든 스토어 엔진을 포함할 수 있다. 또한, 상기 로드/스토어 유닛은 최근 사용된 가상 메모리 어드레스와 물리 메모리 어드레스의 변환 테이블을 저 장하는 변환 색인 버퍼를 더 포함할 수 있다. 또한, 상기 메인 로드/스토어 유닛은, 로드 인스트럭션을 페치하여 로드 인스트럭션 이슈를 수행하는 로드 유닛 과, 스토어 인스트럭션을 페치하여 스토어 인스트럭션 이슈를 수행하는 스토어 유닛과, 상기 로드 유닛으로부터 메모리 액세스 요청을 순차적으로 수신하는 로드 버퍼와, 상기 스토어 유닛으로 메모리 액세스 요청을 순차적으 로 수신하는 스토어 버퍼와, 상기 로드 버퍼로부터 메모리 액세스 요청을 수신하여 제1 로드 데이터를 상기 온 칩 버퍼로 전달하는 로드 엔진과, 상기 스토어 버퍼로부터 메모리 액세스 요청을 수신하여 제1 스토어 데이터를 상기 온 칩 버퍼로 전달하는 스토어 엔진을 포함할 수 있다. 또한, 상기 제1 로드 데이터는 상기 제2 로드 데이터보다 우선 순위가 높고, 상기 제1 스토어 데이터는 상기 제 2 스토어 데이터보다 우선 순위가 높을 수 있다. 또한, 상기 우선 순위는 상기 제1 및 제2 로드 데이터와, 상기 제1 및 제2 스토어 데이터에 태깅될 수 있다. 또한, 상기 우선 순위는 상기 로드 엔진 또는 상기 스토어 엔진에 의해서 태깅될 수 있다. 또한, 상기 로드/스토어 유닛은 상기 제1 및 제2 로드 데이터와, 상기 제1 및 제2 스토어 데이터를 수신하여 라 운드 로빈 방식으로 상기 온 칩 버퍼로 전달하는 아비터를 더 포함할 수 있다. 또한, 상기 온 칩 버퍼는 복수의 뱅크를 포함하고, 단위 클럭 사이클당 상기 제1 로드 데이터, 상기 제2 로드 데이터, 상기 제1 스토어 데이터 및 상기 제2 스토어 데이터의 입력 수를 상기 온 칩 버퍼의 뱅크의 수로 나눈 값은 상기 아비터의 기준 입출력비보다 작고, 상기 기준 입출력비는 상기 아비터에 의해서 상기 제1 로드 데이 터, 상기 제2 로드 데이터, 상기 제1 스토어 데이터 및 상기 제2 스토어 데이터 각각의 대기 시간이 발생하지 않는 범위에서 가장 큰 입력과 출력의 비율값일 수 있다. 또한, 상기 히든 로드/스토어 유닛과, 상기 메인 로드/스토어 유닛은 서로 적어도 일부의 하드웨어를 공유할 수 있다. 또한, 상기 히든 로드/스토어 유닛과, 상기 메인 로드/스토어 유닛은 서로 다른 하드웨어로 구현될 수 있다. 또한, 상기 프로세싱 유닛은, 상기 인풋 액티베이션과 상기 웨이트를 순차적으로 곱하는 2차원 매트릭스 연산을 수행하고, 상기 아웃풋 액티베이션을 생성하는 PE 어레이와, 1차원 연산을 수행하는 벡터 유닛을 포함할 수 있 다. 또한, 상기 외부 인터페이스는 데이터 버스, 외부 칩 인터페이스 또는 로컬 버스 중 어느 하나를 포함할 수 있 다. 상기 다른 과제를 해결하기 위한 본 발명의 몇몇 실시예에 따른 인공지능 코어 시스템은, 연산을 하기 위한 프 로그램 및 입력 데이터를 저장하는 메모리, 상기 메모리로부터 상기 입력 데이터 및 제어 신호를 전달하는 버스, 상기 프로그램, 상기 입력 데이터 및 상기 제어 신호를 수신하여 2차원 매트릭스 연산을 수행하고, 출력 데이터를 생성하는 인공지능 코어로서, 상기 인공지능 코어는, 상기 메모리로부터 상기 프로그램 및 상기 입력 데이터를 로드하고, 상기 출력 데이터를 상기 메모리로 저장하는 로드/스토어 유닛과, 상기 프로그램 및 상기 입력 데이터를 이용하여 연산을 수행하는 프로세싱 유닛과, 상기 프로세싱 유닛과 상기 로드/스토어 유닛 사이 에서 상기 프로그램, 상기 입력 데이터 및 상기 출력 데이터를 임시 저장하는 온 칩 버퍼를 포함하고, 상기 버 스는, 상기 제어 신호를 전달하는 컨트롤 버스와, 상기 입력 데이터 및 상기 출력 데이터를 전달하는 데이터 버 스를 포함하고, 상기 로드/스토어 유닛은, 상기 프로세싱 유닛이 현재 수행하는 현재 실행 작업에 대한 메인 로 드/스토어 작업과, 상기 프로세싱 유닛이 상기 현재 실행 작업 이후에 실행하는 대기 실행 작업에 대한 대기 로 드/스토어 작업을 수행하고, 상기 대기 로드/스토어 작업은 상기 데이터 버스의 밴드위스 중 상기 메인 로드/스 토어 작업에 의해서 사용되지 않는 밴드위스를 사용하여 수행된다. 또한, 상기 메모리는 상기 인공지능 코어와 동일한 칩 내에 형성된 온 칩 메모리와, 상기 인공지능 코어와 분리 되어 형성된 오프 칩 메모리를 포함할 수 있다. 또한, 상기 인공지능 코어는 제1 인공지능 코어이고, 상기 제1 인공지능 코어와 다른 제2 인공지능 코어를 더 포함하고, 상기 버스는 상기 제1 및 제2 인공지능 코어 사이에서 상기 입력 데이터 및 상기 출력 데이터를 전달 하는 로컬 버스를 더 포함하고, 상기 로드/스토어 유닛은 상기 로컬 버스의 밴드위스 중 상기 메인 로드/스토어 작업에 의해서 사용되지 않는 밴드위스를 사용하여 상기 대기 로드/스토어 작업을 수행할 수 있다. 또한, 상기 로드/스토어 유닛은, 상기 메인 로드/스토어 작업을 수행하는 메인 로드/스토어 유닛과, 상기 대기 로드/스토어 작업을 수행하는 히든 로드/스토어 유닛을 포함하고, 상기 대기 로드/스토어 작업은 상기 메인 로 드/스토어 작업에 비해서 낮은 우선 순위를 가질 수 있다. 또한, 상기 우선 순위는 태깅된 형태로 식별될 수 있다. 또한, 상기 인공지능 코어는, 상기 프로세싱 유닛으로 인풋 액티베이션을 제공하고, 상기 프로세싱 유닛으로부 터 아웃풋 액티베이션을 수신하는 액티베이션 버퍼와, 상기 온 칩 버퍼에서 상기 인풋 액티베이션을 불러와서 상기 액티베이션 버퍼로 전달하고, 상기 액티베이션 버퍼로부터 상기 아웃풋 액티베이션을 상기 온 칩 버퍼로 전달하는 액티베이션 로드/스토어 유닛을 더 포함할 수 있다. 상기 또 다른 과제를 해결하기 위한 본 발명의 몇몇 실시예에 따른 인공지능 코어 시스템의 로드/스토어 방법은, 메인 로드/스토어 유닛이 제1 작업에 대한 제1 프로그램을 로드하고, 상기 제1 프로그램을 이용하여 제 1 작업을 수행하고, 상기 제1 작업 중에 상기 메인 로드/스토어 유닛이 동작하지 않음을 확인하면, 히든 로드/ 스토어 유닛이 상기 제1 작업 이후에 실행이 대기된 제2 작업에 대한 제2 프로그램을 로드하고, 상기 제1 작업 및 상기 제2 프로그램의 로드 작업이 종료되면 상기 제2 프로그램을 이용하여 제2 작업을 수행하는 것을 포함한 다. 또한, 상기 제2 프로그램을 로드하는 것은, 상기 제2 프로그램에 대한 대기 로드 인스트럭션을 페치하고, 상기 페치된 대기 로드 인스트럭션을 이슈하고, 상기 이슈된 대기 로드 인스트럭션에 대응하는 메모리 액세스 요청을 히든 로드 버퍼로 전송하고, 상기 메모리 액세스 요청을 상기 히든 로드 버퍼가 순차적으로 로드 엔진으로 전송 하고, 상기 로드 엔진은 상기 메모리 액세스 요청에 따라 데이터 버스를 통해서 오프 칩 메모리로부터 제2 로드 데이터를 수신하고, 상기 제2 로드 데이터를 온 칩 버퍼로 전달하는 것을 포함할 수 있다. 또한, 상기 제1 프로그램을 로드하는 것은, 상기 제1 프로그램에 대한 로드 인스트럭션을 페치하고, 상기 페치 된 로드 인스트럭션을 이슈하고, 상기 이슈된 로드 인스트럭션에 대응하는 메모리 액세스 요청을 로드 버퍼로 전송하고, 상기 메모리 액세스 요청을 상기 로드 버퍼가 순차적으로 로드 엔진으로 전송하고, 상기 로드 엔진은 상기 메모리 액세스 요청에 따라 데이터 버스를 통해서 오프 칩 메모리로부터 제1 로드 데이터를 수신하고, 상 기 제1 로드 데이터를 온 칩 버퍼로 전달하는 것을 포함할 수 있다. 또한, 상기 제1 로드 데이터는 상기 제2 로드 데이터보다 우선 순위가 높을 수 있다. 상기 또 다른 과제를 해결하기 위한 본 발명의 몇몇 실시예에 따른 인공지능 코어 시스템의 로드/스토어 방법은, 메인 로드/스토어 유닛이 제1 작업에 대한 제1 데이터의 로드 작업을 수행하고, 상기 제1 데이터를 이 용하여 제1 작업을 수행하고, 상기 제1 작업 중에 상기 메인 로드/스토어 유닛이 동작하지 않음을 확인하면, 히 든 로드/스토어 유닛이 상기 제1 작업 이후에 실행이 대기된 제2 작업에 대한 제2 데이터의 로드 작업을 수행하 고, 상기 제1 작업 및 상기 제2 데이터의 로드 작업이 종료되면 상기 제2 데이터를 이용하여 제2 작업을 수행하 는 것을 포함한다. 또한, 상기 제1 작업은 신경망의 제1 레이어의 매트릭스 연산 작업이고, 상기 제2 작업은 신경망의 제2 레이어 의 매트릭스 연산 작업이고, 상기 제2 데이터는 상기 제2 레이어의 커널 데이터일 수 있다. 또한, 상기 제1 데이터는 인풋 액티베이션을 포함하고, 상기 제1 작업을 수행하는 것은, 상기 인풋 액티베이션 을 액티베이션 버퍼에 저장하고, 프로세싱 유닛이 상기 액티베이션 버퍼로부터 상기 인풋 액티베이션을 수신하 여 아웃풋 액티베이션을 생성하고, 상기 액티베이션 버퍼가 상기 아웃풋 액티베이션을 저장하는 것을 포함할 수 있다."}
{"patent_id": "10-2025-0016785", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 인공지능 코어, 인공지능 코어 시스템 및 인공지능 코어 시스템의 로드/스토어 방법은, 인공지능 코 어와 외부와의 연결 인터페이스의 밴드위스를 최적으로 활용하여 다음 작업의 데이터나 프로그램을 미리 로드할 수 있다. 또한, 다음 작업에 대한 프로그램 및 데이터의 로드/스토어 작업이 현재 실행 작업에 대한 프로그램 및 데이터 의 로드/스토어 작업의 정체를 유발하지 않게 하여 현재 작업의 지연도 차단할 수 있다. 나아가, 메인 로드/스토어 유닛과 히든 로드/스토어 유닛이 하드웨어를 공유하여 하드웨어 활용의 효율을 극대 화할 수 있다. 상술한 내용과 더불어 본 발명의 구체적인 효과는 이하"}
{"patent_id": "10-2025-0016785", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서 및 특허청구범위에서 사용된 용어나 단어는 일반적이거나 사전적인 의미로 한정하여 해석되어서는 아 니된다. 발명자가 그 자신의 발명을 최선의 방법으로 설명하기 위해 용어나 단어의 개념을 정의할 수 있다는 원 칙에 따라, 본 발명의 기술적 사상과 부합하는 의미와 개념으로 해석되어야 한다. 또한, 본 명세서에 기재된 실 시예와 도면에 도시된 구성은 본 발명이 실현되는 하나의 실시예에 불과하고, 본 발명의 기술적 사상을 전부 대 변하는 것이 아니므로, 본 출원시점에 있어서 이들을 대체할 수 있는 다양한 균등물과 변형 및 응용 가능한 예 들이 있을 수 있음을 이해하여야 한다. 본 명세서 및 특허청구범위에서 사용된 제1, 제2, A, B 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. '및/또는' 이라는 용어는 복수의 관련된 기재된 항목들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함 한다. 본 명세서 및 특허청구범위에서 사용된 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서 \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해서 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의 미를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다. 또한, 본 발명의 각 실시예에 포함된 각 구성, 과정, 공정 또는 방법 등은 기술적으로 상호 간 모순되지 않는 범위 내에서 공유될 수 있다. 이하, 도 1 내지 도 8을 참조하여, 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템을 설명한다. 도 1은 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템을 설명하기 위한 블록도이다. 도 1을 참조하면, 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템은 인공지능 코어, 메모리 및 외부 인터페이스를 포함한다 인공지능 코어는 딥러닝 연산 작업을 위해서 특화된 프로세싱 모듈일 수 있다. 인공지능 코어는 별도 의 단일 혹은 복수의 칩으로 구현될 수도 있고, 시스템 상에 결합된 SoC(System on Chip)의 일부로 구현될 수도 있다. 인공지능 코어는 컨볼루션 연산 즉, 매트릭스 곱셈 연산에 특화되어 기존의 CPU나 GPU보다 훨씬 효 율적으로 딥러닝 학습 및 추론 작업을 수행할 수 있다. 인공지능 코어는 하드웨어로 모듈로서 구현될 수 있다. 메모리는 인공지능 코어에 외부 인터페이스를 통해서 프로그램, 입력 데이터 및 제어 신호를 전 송할 수 있다. 또한, 메모리는 인공지능 코어로부터 출력 데이터를 수신하여 저장할 수 있다. 메모리는 온 칩 메모리(On-chip memory) 및 오프 칩 메모리(Off-chip memory)를 포함할 수 있 다. 온 칩 메모리는 예를 들어, 인공지능 코어와 같은 칩에 형성된 SRAM(Static Random Access Memory)일 수 있다. 온 칩 메모리는 여러 코어에 의해서 공유되는 공유 메모리(Shared memory)일 수 있다. 단, 본 실시예가 이에 제한되는 것은 아니다. 오프 칩 메모리은 인공지능 코어와 별도로 형성된 외부 메모리(external memory)일 수 있다. 오프 칩 메모리는 예를 들어, DRAM(Dynamic Random-Access Memory), 낸드 플래시 메모리(NAND flash memory), 노 어 플래시 메모리(NOR flash memory) 및 3D 크로스 포인트 메모리 중 적어도 하나를 포함할 수 있다. 단, 본 실 시예가 이에 제한되는 것은 아니다. 메모리는 외부 인터페이스를 통해서 인공지능 코어에 프로그램 및 입력 데이터를 제공하고, 외 부 인터페이스를 통해서 인공지능 코어로부터 출력 데이터를 수신하여 저장할 수 있다. 외부 인터페이스는 인공지능 코어와 메모리 사이의 데이터 교환을 수행할 수 있다. 외부 인터페 이스는 데이터뿐만 아니라 프로그램이나 제어 신호를 이동시킬 수 있다. 외부 인터페이스는 다양한 형태로 구현될 수 있다. 구체적으로, 인공지능 코어가 SoC 형태로 구현된 경우에는, 외부 인터페이스는 메인 데이터 버스일 수 있다. 또는, 인공지능 코어가 단일 칩 형태로 구현되는 경우에는, 외부 인터페이스는 외부 칩 인터페이스일 수 있다. 도 2는 도 1의 인공지능 코어의 구조를 세부적으로 설명하기 위한 블록도이다. 도 2를 참조하면, 인공지능 코어는 프로세싱 유닛, 액티베이션 버퍼, 액티베이션 로드/스토어 유닛, 온 칩 버퍼 및 로드/스토어 유닛을 포함할 수 있다. 프로세싱 유닛은 연산을 수행하는 모듈일 수 있다. 프로세싱 유닛은 1차원 연산뿐만 아니라 2차원 매 트릭스 연산 즉, 컨볼루션 연산을 수행할 수 있다. 프로세싱 유닛은 인풋 액티베이션(Act_In)을 수신하여 웨이트와 곱한뒤 이를 더하여 아웃풋 액티베이션(Act_Out)을 생성할 수 있다. 도 3은 도 2의 프로세싱 유닛의 구조를 세부적으로 설명하기 위한 블록도이다. 도 2 및 도 3을 참조하면, 프로세싱 유닛은 PE 어레이 및 벡터 유닛을 포함할 수 있다. PE 어레이는 인풋 액티베이션(Act_In)을 수신하여 가중치와 각각 곱셈을 수행할 수 있다. 이때, 인풋 액티 베이션(Act_In)과 가중치는 매트릭스를 구성하고, 컨볼루션을 통해서 연산될 수 있다. 이를 통해서, PE 어레이 는 아웃풋 액티베이션(Act_Out)을 생성할 수 있다. PE 어레이는 적어도 하나의 프로세싱 엘리먼트(111a)를 포함할 수 있다. 프로세싱 엘리먼트(111a)는 서로 정렬되어 각각 하나의 인풋 액티베이션(Act_In)과 하나의 가중치(weight)에 대한 곱셈을 수행할 수 있다. PE 어레이는 각각의 곱셈에 대한 값을 합한 부분합을 생성할 수 있다. 이러한 부분합은 아웃풋 액티베이션 (Act_Out)으로 활용될 수 있다. PE 어레이는 이차원 매트릭스 곱셈을 수행하므로 이차원 매트릭스 연산 유 닛(2D matrix compute unit)으로 지칭될 수도 있다. 벡터 유닛은 주로 1차원 연산을 수행할 수 있다. 벡터 유닛은 PE 어레이와 함께 딥러닝 연산을 수행할 수 있다. 이를 통해서 프로세싱 유닛은 필요한 연산에 특화될 수 있다. 즉, 인공지능 코어는 대량의 2차원 매트릭스 곱셈과 1차원 연산을 수행하는 연산 모듈이 각각 있어 효율적으로 딥러닝 작업을 수행할 수 있다. 도 4는 프로세싱 유닛에 의해서 실행되는 딥러닝 작업의 신경망 네트워크의 구조를 설명하기 위한 개념도이다. 도 4를 참조하면, PE 어레이에 의해서 구현되는 신경망 네트워크는 인풋 데이터를 입력 받는 입력노드를 포함하는 입력 레이어(Input1~k)와, 출력 데이터를 출력하는 출력노드를 포함하는 출력 레이어(Output1~i)와, 입력 레이어와 출력 레이어 사이에 배치되는 M 개의 히든 레이어를 포함할 수 있다. 여기서, 각 레이어들의 노드를 연결하는 에지(Edge)에는 가중치가 설정될 수 있다. 이러한 가중치 혹은 에지의 유무는 학습 과정에서 추가, 제거, 또는 업데이트 될 수 있다. 따라서, 학습 과정을 통하여, k개의 입력노드와 i개의 출력노드 사이에 배치되는 노드들 및 에지들의 가중치는 업데이트될 수 있다. 신경망 네트워크가 학습을 수행하기 전에는 모든 노드와 에지는 초기값으로 설정될 수 있다. 그러나, 누적하여 정보가 입력될 경우, 노드 및 에지들의 가중치는 변경되고, 이 과정에서 학습인자로 입력되는 파라미터들과 출 력노드로 할당되는 값 사이의 매칭이 이루어질 수 있다. 또한, 신경망 네트워크를 구성하는 입력노드와 출력노드 사이의 노드 및 에지의 가중치는 신경망 네트워크의 학 습 과정에 의해 업데이트될 수 있다. 다시, 도 2를 참조하면, 액티베이션 버퍼는 프로세싱 유닛으로 인풋 액티베이션(Act_In)을 제공하고, 프로세싱 유닛으로부터 아웃풋 액티베이션(Act_Out)을 수신할 수 있다. 액티베이션 버퍼는 인풋 액티 베이션(Act_In)과 아웃풋 액티베이션(Act_Out)을 일시적으로 저장할 수 있다. 인풋 액티베이션(Act_In) 및 아웃풋 액티베이션(Act_Out)은 신경망 네트워크의 레이어의 입력값과 출력값을 의 미할 수 있다. 이때, 신경망 네트워크의 레이어가 복수인 경우 이전 레이어의 출력값이 다음 레이어의 입력값이 되므로 이전 레이어의 아웃풋 액티베이션(Act_Out)이 다음 레이어의 인풋 액티베이션(Act_In)으로 활용될 수 있 다. 액티베이션 버퍼는 연산량이 많은 프로세싱 유닛, 특히, PE 어레이에 빠르게 액티베이션을 제공 하고, 빠르게 액티베이션을 수신하여 인공지능 코어의 연산 속도를 높일 수 있다. 액티베이션 로드/스토어 유닛은 온 칩 버퍼로부터 인풋 액티베이션(Act_In)을 액티베이션 버퍼 로 전달하고, 액티베이션 버퍼로부터 아웃풋 액티베이션(Act_Out)을 상기 온 칩 버퍼로 전달할 수 있다. 즉, 액티베이션 로드/스토어 유닛은 액티베이션의 로드 작업과 스토어 작업을 모두 수행할 수 있다. 온 칩 버퍼는 인공지능 코어 내부에 위치한 메모리로서, 인공지능 코어가 작업에 필요한 모든 입력 데이터를 외부로부터 수신하여 임시로 저장할 수 있다. 또한, 온 칩 버퍼는 인공지능 코어에 의해서 연산된 출력 데이터를 외부로 전송하기 위해서 일시적으로 저장할 수 있다. 온 칩 버퍼는 액티베이션 로드/스토어 유닛에 의해서 인풋 액티베이션(Act_In)을 액티베이션 버퍼 로 전송하고, 아웃풋 액티베이션(Act_Out)을 수신할 수 있다. 온 칩 버퍼는 액티베이션 로드/스토어 유닛 외에도, 프로세싱 유닛과 직접 데이터를 송수신할 수 있다. 즉, 온 칩 버퍼는 PE 어레이 및 벡터 유닛 각각과 데이터를 주고받을 수 있다. 로드/스토어 유닛은 외부 인터페이스를 통해서 외부에서 입력 데이터, 프로그램 및 제어 신호 중 적 어도 하나를 수신할 수 있다. 로드/스토어 유닛은 온 칩 버퍼로 수신한 입력 데이터, 프로그램 및 제 어 신호 중 적어도 하나를 전송할 수 있다. 유사하게 로드/스토어 유닛은 외부 인터페이스를 통해서 출력 데이터를 외부로 전달 수 있다. 로드/ 스토어 유닛은 프로세싱 유닛이 생성한 출력 데이터를 전송할 수 있다. 도 5는 도 2의 로드/스토어 유닛의 동작을 설명하기 위한 블록도이다. 도 5를 참조하면, 태스크 컨트롤러는 인공지능 코어에 의해서 구현될 수 있다. 태스크 컨트롤러는 인공지능 코어의 작업을 제어하는 모듈일 수 있다. 태스크 컨트롤러는 인공지능 코어에 의해서 로지컬하게 구현된 모듈일 수 있다. 단, 본 실시예가 이에 제한되는 것은 아니다. 외부 인터페이스는 인공지능 코어가 SoC인 경우 컨트롤 버스 및 데이터 버스를 포함할 수 있다. 이때, 컨트롤 버스는 제어신호를 전달하는 버스이고, 데이터 버스는 입력 데이터 및 출력 데이 터를 전달하는 버스일 수 있다. 컨트롤 버스는 태스크 컨트롤러에게 현재 작업에 대한 로드 또는 스토어에 대한 제어 신호를 전송할 수 있다. 예를 들어, 태스크 컨트롤러는 로드/스토어 유닛으로 로드 인스트럭션 및 대기 로드 인스트 럭션 중 적어도 하나를 전송할 수 있다. 또는, 태스크 컨트롤러는 로드/스토어 유닛으로 스토어 인스 트럭션 및 대기 스토어 인스트럭션 중 적어도 하나를 전송할 수 있다. 로드/스토어 유닛은 로드 인스트럭 션, 스토어 인스트럭션, 대기 로드 인스트럭션 및 대기 스토어 인스트럭션 중 적어도 하나에 따라 로드/스토어 작업을 수행할 수 있다. 이때, 로드 인스트럭션 및 스토어 인스트럭션은 프로세싱 유닛이 현재 실행하고 있는 작업에 대한 프로그 램이나 데이터에 대한 인스트럭션을 의미하고, 대기 로드 인스트럭션 및 대기 스토어 인스트럭션은 프로세싱 유 닛이 다음에 실행할 작업에 대한 프로그램이나 데이터에 대한 인스트럭션을 의미할 수 있다. 로드 인스트럭션, 대기 로드 인스트럭션, 스토어 인스트럭션 및 대기 스토어 인스트럭션은 각각 하기와 같은 세 부사항을 포함할 수 있다. Dscrptr{src, dst, burst size, #burst} 이때, src는 소스 즉, 로드나 스토어할 데이터의 주소, dst는 데스티네이션 즉, 데이터를 전송할 주소, burst size는 버스트 사이즈 즉, 분할 크기 및 #burst 버스트 넘버 즉, 분할 개수를 의미할 수 있다. 단, 본 실시예가 이에 제한되는 것은 아니다. 로드/스토어 유닛은 메인 로드/스토어 유닛 및 히든 로드/스토어 유닛을 포함할 수 있다. 메인 로드/스토어 유닛은 로드/스토어 작업 중 메인 로드/스토어 작업을 수행할 수 있다. 예를 들면, 메인 로드/스토어 유닛은 로드 인스트럭션을 페치(fetch)하고, 로드 인스트럭션을 이슈(issu e)할 수 있다. 여기서 이슈란, 인스트럭션 수행이 가능하지 않은 조건인지를 판단하고, 가능한 경우 이를 계속 해서 진행하기 위한 작업을 의미할 수 있다. 메인 로드/스토어 유닛은 이슈된 로드 인스트럭션에 따라 데이터 버스를 통해서 오프 칩 메모리(22 0)에 메모리 액세스 하여 제1 로드 데이터(Dpr)를 수신하고, 온 칩 버퍼로 전송할 수 있다. 이때, 제1 로 드 데이터(Dpr)는 높은 우선 순위를 가지는 데이터일 수 있다. 히든 로드/스토어 유닛은 로드/스토어 작업 중 대기 로드/스토어 작업을 수행할 수 있다. 예를 들면, 히든 로드/스토어 유닛은 대기 로드 인스트럭션을 페치하고, 대기 로드 인스트럭션을 이슈할 수 있다. 히든 로드/스토어 유닛은 이슈된 로드 인스트럭션에 따라 데이터 버스를 통해서 오프 칩 메모리(22 0)에 메모리 액세스 하여 제2 로드 데이터(Dnpr)를 수신하고, 온 칩 버퍼로 전송할 수 있다. 이때, 제2 로 드 데이터(Dnpr)는 낮은 우선 순위를 가지는 데이터일 수 있다. 즉, 제1 로드 데이터(Dpr)는 제2 로드 데이터 (Dnpr)에 비해서 상대적으로 높은 우선 순위를 가질 수 있다. 즉, 온 칩 버퍼는 제2 로드 데이터(Dnpr)보 다 제1 로드 데이터(Dpr)를 먼저 저장할 수 있다. 이때, 우선 순위는 데이터에 태깅(tagging)된 형태로 식별될 수 있다. 이에 따라서, 현재 실행되고 있는 작업에 대한 메인 로드/스토어 작업은 대기 로드/스토어 작업에 의해서 지연되지 않을 수 있다. 즉, 대기 로드/스토어 작업은 메인 로드/스토어 작업의 수행에 전혀 방해되지 않을 수 있다. 또한, 대기 로드/스토어 작업은 메인 로 드/스토어 작업에 의해서 사용되는 외부 인터페이스의 밴드위스(bandwidth)를 제외한 나머지 밴드위스를 이용하여 수행될 수 있다. 즉, 시계열적으로 프로그램과 데이터의 로드 작업이 먼저 수행되어야 그에 대한 연산 작업이 이루어지고, 연산 작업의 수행 시간은 로드 작업보다 훨씬 길 수 있다. 이에 따라서, 본 실시예에 따른 인공지능 코어 시스템은 연산 작업 중에 활용되지 않는 밴드위스를 대기 작업에 대해서 할당하여 밴드위스의 활용을 극대화할 수 있다. 도 6은 도 5의 로드/스토어 유닛의 구조를 세부적으로 설명하기 위한 블록도이다. 도 6을 참조하면, 로드/스토어 유닛은 로드 유닛(151a), 스토어 유닛(151b), 로드 버퍼(151a_b), 스토어 버퍼(151b_b), 히든 로드 유닛(152a), 히든 로드 버퍼(152a_b), 히든 스토어 유닛(152b), 히든 스토어 버퍼 (152b_b), 로드 엔진, 스토어 엔진, 변환 색인 버퍼 및 아비터를 포함할 수 있다. 로드 유닛(151a)은 태스크 컨트롤러로부터 로드 인스트럭션을 페치하고, 로드 인스트럭션을 이슈할 수 있다. 로드 유닛(151a)이 이슈된 로드 인스트럭션을 로드 버퍼(151a_b)에 제공하면 로드 버퍼(151a_b)가 입력된 순서에 따라서 순차적으로 로드 엔진으로 메모리 액세스 요청을 전송할 수 있다. 또한, 스토어 유닛(151b)은 태스크 컨트롤러로부터 스토어 인스트럭션을 페치하고, 스토어 인스트럭션을 이 슈할 수 있다. 스토어 유닛(151b)이 이슈된 스토어 인스트럭션을 스토어 버퍼(151b_b)에 제공하면 스토어 버퍼 (151b_b)가 입력된 순서에 따라서 순차적으로 스토어 엔진으로 메모리 액세스 요청을 전송할 수 있다. 히든 로드 유닛(152a)은 태스크 컨트롤러로부터 대기 로드 인스트럭션을 페치하고, 대기 로드 인스트럭션을 이슈할 수 있다. 히든 로드 유닛(152a)이 이슈된 대기 로드 인스트럭션을 히든 로드 버퍼(152a_b)에 제공하면 히든 로드 버퍼(152a_b)가 입력된 순서에 따라서 순차적으로 로드 엔진으로 메모리 액세스 요청을 전송할 수 있다. 또한, 히든 스토어 유닛(152b)은 태스크 컨트롤러로부터 대기 스토어 인스트럭션을 페치하고, 대기 스토어 인스트럭션을 이슈할 수 있다. 히든 스토어 유닛(152b)이 이슈된 대기 스토어 인스트럭션을 히든 스토어 버퍼 (152b_b)에 제공하면 히든 스토어 버퍼(152b_b)가 입력된 순서에 따라서 순차적으로 스토어 엔진으로 메모 리 액세스 요청을 전송할 수 있다. 로드 엔진은 메모리 액세스 요청을 수신하여 데이터 버스를 통해서 제1 로드 데이터(Dpr) 및 제2 로 드 데이터(Dnpr)를 불러올 수 있다. 이때, 로드 엔진은 변환 색인 버퍼에서 최근에 사용된 가상 주소 와 물리 주소의 변환 테이블을 이용하여 빠르게 데이터를 찾을 수 있다. 로드 엔진의 가상 주소가 변환 색 인 버퍼에 없는 경우에는 메모리에서 주소 변환 정보를 찾을 수 있다. 제1 로드 데이터(Dpr)는 로드 버퍼(151a_b)로부터 수신한 메모리 액세스 요청에 대응한 데이터이고, 제2 로드 데이터(Dnpr)는 히든 로드 버퍼(152a_b)로부터 수신한 메모리 액세스 요청에 대응한 데이터일 수 있다. 이때, 로드 버퍼(151a_b)와 히든 로드 버퍼(152a_b)는 동시에 메모리 액세스 요청을 로드 엔진으로 전송하 지는 않는다. 즉, 히든 로드 유닛(152a)과 히든 로드 버퍼(152a_b)는 로드 유닛(151a)과 로드 버퍼(151a_b)가 로드 엔진으로 메모리 액세스 요청을 전달하지 않은 때를 식별(identify)하여 메모리 액세스 요청을 로드 엔진으로 전송할 수 있다. 즉, 로드 버퍼(151a_b)에서 인스트럭션 이슈 작업이 스톨(stall)된 경우에만 히 든 로드 버퍼(152a_b)가 동작할 수 있다. 아비터는 로드 엔진으로부터 제1 로드 데이터(Dpr) 및 제2 로드 데이터(Dnpr)를 수신할 수 있다. 아 비터는 라운드 로빈 방식으로 입력된 제1 로드 데이터(Dpr) 및 제2 로드 데이터(Dnpr)를 온 칩 버퍼의 뱅크(B)에 각각 전달할 수 있다. 즉, 아비터는 데이터를 순차적으로 온 칩 버퍼의 뱅크(B)에 분배 하므로 제2 로드 데이터(Dnpr)가 추가되는 경우 일반적으로는 제1 로드 데이터(Dpr)들의 지연이 발생할 수 있다. 그러나, 본 발명의 몇몇 실시예들에 따른 인공지능 코어는 제1 로드 데이터(Dpr)에 높은 우선 순위를 부여하여 제2 로드 데이터(Dnpr)가 추가됨에도 제1 로드 데이터(Dpr)의 처리가 지연되는 것을 방지할 수 있다. 이러한 우선 순위는 로드 엔진에 의해서 태깅될 수 있다. 단, 본 실시예가 이에 제한되는 것은 아니다. 즉, 로드 유닛(151a) 및 히든 로드 유닛(152a)에서 우선 순위에 대한 정보가 미리 결정되어 전달되는 것도 얼마 든지 가능할 수 있다. 스토어 엔진은 메모리 액세스 요청을 수신하여 데이터 버스를 통해서 제1 스토어 데이터로컬 버스 및 제2 스토어 데이터로컬 버스를 불러올 수 있다. 이때, 스토어 엔진은 변환 색인 버퍼 에서 최근에 사용된 가상 주소와 물리 주소의 변환 테이블을 이용하여 빠르게 데이터를 찾을 수 있다. 스토어 엔진의 가상 주소가 변환 색인 버퍼에 없는 경우에는 메모리에서 주소 변환 정보를 찾을 수 있 다. 제1 스토어 데이터로컬 버스는 스토어 버퍼(151b_b)로부터 수신한 메모리 액세스 요청에 대응한 데이터이 고, 제2 스토어 데이터로컬 버스는 히든 스토어 버퍼(152b_b)로부터 수신한 메모리 액세스 요청에 대응한 데이터일 수 있다. 이때, 스토어 버퍼(151b_b)와 히든 스토어 버퍼(152b_b)는 동시에 메모리 액세스 요청을 스토어 엔진으로 전송하지는 않는다. 즉, 히든 스토어 유닛(152b)과 히든 스토어 버퍼(152b_b)는 스토어 유닛(151b)과 스토어 버 퍼(151b_b)가 스토어 엔진으로 메모리 액세스 요청을 전달하지 않은 때를 식별하여 메모리 액세스 요청을 스토어 엔진으로 전송할 수 있다. 즉, 스토어 버퍼(151b_b)에서 인스트럭션 이슈 작업이 스톨된 경우에만 히든 스토어 버퍼(152b_b)가 동작할 수 있다. 아비터는 스토어 엔진으로부터 제1 스토어 데이터로컬 버스 및 제2 스토어 데이터로컬 버스 를 수신할 수 있다. 아비터는 라운드 로빈 방식으로 입력된 제1 스토어 데이터로컬 버스 및 제2 스토어 데이터로컬 버스를 온 칩 버퍼의 뱅크(B)에서 데이터 버스로 각각 전달할 수 있다. 즉, 아비터는 데이터를 순차적으로 온 칩 버퍼의 뱅크(B)에서 가져오므로로 제2 스토어 데이터로컬 버스 가 추가되는 경우 일반적으로는 제1 스토어 데이터로컬 버스들의 처리의 지연이 발생할 수 있다. 그러나, 본 발명의 몇몇 실시예들에 따른 인공지능 코어는 제1 스토어 데이터로컬 버스에 높은 우선 순위 를 부여하여 제2 스토어 데이터로컬 버스가 추가됨에도 제1 스토어 데이터로컬 버스의 처리가 지연되 는 것을 방지할 수 있다. 이러한 우선 순위는 스토어 엔진에 의해서 태깅될 수 있다. 단, 본 실시예가 이에 제한되는 것은 아니다. 즉, 스토어 유닛(151b) 및 히든 스토어 유닛(152b)에서 우선 순위에 대한 정보가 미리 결정되어 전달되는 것도 얼마든지 가능할 수 있다. 이때, 로드 유닛(151a), 로드 버퍼(151a_b), 스토어 유닛(151b), 스토어 버퍼(151b_b), 로드 엔진, 스토 어 엔진, 변환 색인 버퍼 및 아비터는 메인 로드/스토어 유닛에 포함될 수 있다. 한편, 히든 로드 유닛(152a), 히든 로드 버퍼(152a_b), 히든 스토어 유닛(152b), 히든 스토어 버퍼(152b_b), 로드 엔진, 스토어 엔진, 변환 색인 버퍼 및 아비터는 히든 로드/스토어 유닛에 포함 될 수 있다. 즉, 메인 로드/스토어 유닛과 히든 로드/스토어 유닛은 서로 로드 엔진, 스토어 엔진, 변 환 색인 버퍼 및 아비터를 공유할 수 있다. 로드 엔진, 스토어 엔진 및 변환 색인 버퍼 중 적어도 하나는 하드웨어로 구현될 수 있다. 로드 엔진과 스토어 엔진은 현실적으로 메인 로드/스토어 유닛과 히든 로드/스토어 유닛이 사용 시간이 다를 수밖에 없으므로 동일한 하드웨어를 일부 공유할 수 있다. 이에 따라, 본 실시예의 리소스 활 용 효율이 극대화될 수 있다. 도 7은 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 프로그램 로드 동작을 시계열적으로 설명하기 위한 타임 다이어그램이다.도 7을 참조하면, 먼저 태스크 컨트롤러에 의해서 제1 프로그램 로드(PrLD1)가 수행될 수 있다. 제1 프로그 램은 제1 작업 실행(EXEC1)을 위해서 필요한 프로그램으로 딥러닝 작업을 위한 프로그램일 수 있다. 제1 프로그 램 로드(PrLD1)는 제1 작업 실행(EXEC1)에 선행되어야 하므로 제1 작업 실행(EXEC1)이 제1 프로그램 로드 (PrLD1)에 의존적일 수 있다. 일반적인 인공지능 코어의 경우 제1 작업 실행(EXEC1)이 끝나고 제2 프로그램 로드(PrLD2)가 수행될 수 있다. 이에 반해서, 본 실시예 따른 인공지능 코어는 딥러닝 작업의 제1 작업 실행(EXEC1)과 병렬적으로 제2 프 로그램 로드(PrLD2)가 수행될 수 있다. 이에 따라서, 제1 작업 실행(EXEC1)이 종료되는 시점에서 제2 작업 실행 (EXEC2)이 바로 시작될 수 있다. 이를 통해서, 본 실시예에 따른 인공지능 코어는 딥러닝 작업의 속도를 비약적으로 상승시킬 수 있다. 도 8은 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 데이터 프리페치 동작을 시계열적으로 설명하 기 위한 타임 다이어그램이다. 도 8을 참조하면, 먼저 태스크 컨트롤러에 의해서 제1 프로그램 로드(PrLD1)가 수행될 수 있다. 이어서, 제 1 페치(Fetch1)가 수행될 수 있다. 제1 페치(Fetch1)는 딥러닝 학습 및 추론을 하기 위한 데이터를 가져오는 단 계일 수 있다. 제1 작업 실행(EXEC1)은 프로그램과 데이터의 로드가 있어야 하므로 의존적일 수 있다. 마찬가지로 제2 작업 실 행(EXEC2)도 제2 프리페치(PreFetch2)와 같이 데이터의 로드가 있어야 하므로 의존적일 수 있다. 제2 프리페치 (PreFetch2)는 예를 들어, CNN(Convolutional Neural Network)이나 LSTM(Long Short-Term Memory)의 다음 레이 어의 커널 데이터를 가져오는 것일 수도 있다. 본 실시예에 따른 인공지능 코어 시스템은 제1 작업 실행(EXEC1) 중에 제2 작업 실행(EXEC2)에 대응하는 데이터 를 미리 가져오는 제2 프리페치(PreFetch2)를 수행하여 제1 작업 실행(EXEC1)이 끝나자 마자 제2 작업 실행 (EXEC2)을 바로 시작하게 할 수 있다. 이를 통해서, 본 실시예에 따른 인공지능 코어의 처리 속도가 더욱 빨라 질 수 있다. 이하, 도 9 및 도 10을 참조하여, 본 발명의 몇몇 실시예들에 따른 인공지능 코어 및 인공지능 코어 시스템을 설명한다. 상술한 내용과 중복되는 경우 간략히 하거나 생략한다. 도 9는 본 발명의 몇몇 실시예들에 따른 인공지능 코어의 메인 로드/스토어 유닛을 세부적으로 설명하기 위한 블록도이고, 도 10은 본 발명의 몇몇 실시예들에 따른 인공지능 코어의 히든 로드/스토어 유닛을 세부적으로 설 명하기 위한 블록도이다. 도 9 및 도 10을 참조하면, 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 로드/스토어 유닛은 하드 웨어로 분리될 수 있다. 즉, 메인 로드/스토어 유닛은 로드 유닛(151a), 스토어 유닛(151b), 로드 버퍼 (151a_b), 스토어 버퍼(151b_b), 제1 로드 엔진(153_1), 제1 스토어 엔진(154_1) 및 제1 변환 색인 버퍼 (155_1)를 포함할 수 있다. 또한, 히든 로드/스토어 유닛은 히든 로드 유닛(152a), 히든 스토어 유닛(152b), 히든 로드 버퍼(152a_b), 히든 스토어 버퍼(152b_b), 제2 로드 엔진(153_2), 제2 스토어 엔진(154_2) 및 제2 변환 색인 버퍼(155_2)를 포함할 수 있다. 본 실시예는 메인 로드/스토어 유닛과 히든 로드/스토어 유닛이 서로 물리적으로 분리되어 있어 인공 지능 코어의 설계 난이도가 낮아지고, 로드 엔진과 스토어 엔진이 서로 공유되지 않아 각각의 내구성이 오래 유지될 수 있다. 다만, 아비터의 경우에는 서로 동일하게 세팅하여 더 정확한 수치를 획득 할 수 있다. 이하, 도 11을 참조하여, 본 발명의 몇몇 실시예들에 따른 인공지능 코어 및 인공지능 코어 시스템을 설명한다. 상술한 내용과 중복되는 경우 간략히 하거나 생략한다. 도 11은 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템을 설명하기 위한 블록도이다. 도 11을 참조하면, 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템은 로드/스토어 유닛이 확장 아 비터(156_1)를 포함하고, 로드 엔진과 스토어 엔진이 우선 순위가 다른 데이터를 사용하지 않을 수 있다. 대신에 온 칩 버퍼가 보유한 뱅크(B)의 수를 더 늘려 확장 뱅크(Be)가 온 칩 버퍼에 포함될 수 있다. 즉, 입력 수가 더 늘어난 만큼 뱅크(B)의 수가 늘어나면 기존의 데이터들이 대기할 필요가 굳이 없으므로 이를 통해서 인공지능 코어의 연산 속도의 지연을 방지할 수 있다. 확장 아비터(156_1)는 기준 입출력 비를 가질 수 있다. 이때, 기준 입출력 비는 입력의 대기시간이 발생되지 않 는 범위에서 가장 큰 입력과 출력의 비를 의미할 수 있다. 확장 아비터(156_1)에 입력으로 들어오는 제1 로드 데이터, 제2 로드 데이터, 제1 스토어 데이터 및 상기 제2 스토어 데이터의 입력 수를 온 칩 버퍼의 뱅크 (B) 및 확장 뱅크(Be)의 수로 나눈 값은 기준 입출력 비보다 작을 수 있다. 따라서, 로드 데이터에 우선 순위 태깅 없이 온 칩 버퍼에 뱅크(B)의 수를 늘리는 것만으로 메인 로드/스 토어 작업의 손해는 발생하지 않을 수 있다. 이하, 도 12 및 도 13을 참조하여, 본 발명의 몇몇 실시예들에 따른 인공지능 코어 및 인공지능 코어 시스템을 설명한다. 상술한 내용과 중복되는 경우 간략히 하거나 생략한다. 도 12는 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템을 설명하기 위한 블록도이고, 도 13은 도 12의 제1 인공지능 코어의 구조 및 동작을 세부적으로 설명하기 위한 블록도이다. 도 12를 참조하면, 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템은 제1 인공지능 코어, 제2 인공 지능 코어 및 로컬 버스를 포함할 수 있다. 제1 인공지능 코어는 도 1의 인공지능 코어와 동일할 수 있다. 제2 인공지능 코어는 제1 인공지 능 코어와 분리된 별개의 코어일 수 있다. 제1 인공지능 코어는 제2 인공지능 코어와 로컬 버스 를 이용하여 서로 데이터를 주고받을 수 있다. 로컬 버스는 코어 간의 데이터를 전송하기 위한 통로일 수 있다. 로컬 버스는 코어 간의 통신을 통해 서 멀티 코어 시스템의 속도를 향상시킬 수 있다. 도 13을 참조하면, 제1 인공지능 코어의 로드/스토어 유닛은 로컬 버스를 통해서 제2 인공지능 코어와 소통할 수 있다. 특히, 메인 로드/스토어 유닛과 히든 로드/스토어 유닛은 각각 로컬 버 스를 통해서 데이터의 로드/스토어 작업을 수행할 수 있다. 본 실시예는 이를 통해서, 코어 간의 데이터 교환에서도 밴드위스의 활용도를 극대화시킬 수 있다. 이하, 도 6, 도 7 및 도 14 내지 도 16을 참조하여, 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 로드/스토어 방법을 설명한다. 상술한 내용과 중복되는 경우 간략히 하거나 생략한다. 도 14는 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 로드/스토어 방법을 설명하기 위한 순서도이 고, 도 15는 도 14의 제1 프로그램을 로드하는 단계를 세부적으로 설명하기 위한 순서도이다. 도 16은 도 14의 제2 프로그램을 로드하는 단계를 세부적으로 설명하기 위한 순서도이다. 도 14를 참조하면, 메인 로드/스토어 유닛이 제1 프로그램을 로드한다(S100). 더 자세히 도 15를 참조하면, 제1 프로그램에 대한 로드 인스트럭션을 페치하고(S110), 페치된 로드 인스트럭션 을 이슈할 수 있다(S120). 이어서, 이슈된 로드 인스트럭션에 대응하는 메모리 액세스 요청을 로드 버퍼로 전송하고(S130), 메모리 액세스 요청을 로드 버퍼가 순차적으로 로드 엔진으로 전송한다(S140). 이어서, 데이터 버스를 통해서 오프 칩 메모리로부터 제1 로드 데이터를 수신하고(S150), 제1 로드 데이터를 온 칩 버퍼로 전달한다(S160). 다시, 도 14를 참조하면, 제1 프로그램을 이용하여 제1 작업을 수행한다(S200). 구체적으로, 도 7을 참조하면, 1 프로그램은 제1 작업 실행(EXEC1)을 위해서 필요한 프로그램으로 딥러닝 작업 을 위한 프로그램일 수 있다. 제1 프로그램 로드(PrLD1)는 제1 작업 실행(EXEC1) 즉, 제1 작업에 선행되어야 하 므로 제1 작업 실행(EXEC1)이 제1 프로그램 로드(PrLD1)에 의존적일 수 있다. 다시, 도 14를 참조하면, 메인 로드/스토어 유닛이 동작하지 않음을 확인하고(S300), 히든 로드/스토어 유닛이 제2 작업에 대한 제2 프로그램을 로드한다(S400). 더 자세히 도 16을 참조하면, 제2 프로그램에 대한 대기 로드 인스트럭션을 페치하고(S410), 페치된 대기 로드 인스트럭션을 이슈할 수 있다(S420). 이어서, 이슈된 대기 로드 인스트럭션에 대응하는 메모리 액세스 요청을 히든 로드 버퍼로 전송하고(S430), 메 모리 액세스 요청을 히든 로드 버퍼가 순차적으로 로드 엔진으로 전송한다(S440). 이어서, 데이터 버스를 통해서 오프 칩 메모리로부터 제2 로드 데이터를 수신하고(S450), 제2 로드 데이터를 온 칩 버퍼로 전달한다(S460). 구체적으로, 도 7을 참조하면, 본 실시예 따른 인공지능 코어는 딥러닝 작업의 제1 작업 실행(EXEC1)과 병 렬적으로 제2 프로그램 로드(PrLD2)가 수행될 수 있다. 이에 따라서, 제1 작업 실행(EXEC1)이 종료되는 시점에 서 제2 작업 실행(EXEC2)이 바로 시작될 수 있다. 이를 통해서, 본 실시예에 따른 인공지능 코어는 딥러닝 작업의 속도를 비약적으로 상승시킬 수 있다. 또한, 도 6을 참조하면, 히든 로드 유닛(152a)과 히든 로드 버퍼(152a_b)는 로드 유닛(151a)과 로드 버퍼 (151a_b)가 로드 엔진으로 메모리 액세스 요청을 전달하지 않은 때를 감지하여 메모리 액세스 요청을 로드 엔진으로 전송할 수 있다. S300 및 S400 단계는 S200 단계와 병렬적으로 수행될 수 있다. 다시, 도 14를 참조하면, 제2 프로그램을 이용하여 제2 작업을 수행한다(S500). 구체적으로, 도 7을 참조하면, 제2 프로그램은 제2 작업 실행(EXEC2)을 위해서 필요한 프로그램으로 딥러닝 작 업을 위한 프로그램일 수 있다. 제2 프로그램 로드(PrLD2)는 제2 작업 실행(EXEC2) 즉, 제2 작업에 선행되어야 하므로 제2 작업 실행(EXEC2)이 제2 프로그램 로드(PrLD2)에 의존적일 수 있다. 본 실시예에 따른 인공지능 코어의 로드/스토어 방법은 제1 작업의 수행과 제2 작업에 대한 제2 프로그램의 로 드가 병렬적으로 이루어져 작업의 효율이 올라가고, 기존에 활용되지 못했던 외부 인터페이스의 밴드위스 를 최대한 활용할 수 있다. 이하, 도 17 및 도 18을 참조하여, 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 로드/스토어 방법 을 설명한다. 상술한 내용과 중복되는 경우 간략히 하거나 생략한다. 도 17은 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 로드/스토어 방법을 설명하기 위한 순서도이 고, 도 18은 도 17의 제1 작업을 수행하는 단계를 세부적으로 설명하기 위한 순서도이다. 도 17을 참조하면, 메인 로드/스토어 유닛이 제1 데이터를 로드한다(S1100). 구체적으로, 도 8을 참조하면, 제1 페치(Fetch1)가 수행될 수 있다. 제1 페치(Fetch1)는 딥러닝 학습 및 추론을 하기 위한 데이터를 가져오는 단계일 수 있다. 다시, 도 17을 참조하면, 제1 데이터를 이용하여 제1 작업을 수행한다(S1200). 더 자세히, 도 18을 참조하면, 인풋 액티베이션을 액티베이션 버퍼에 저장한다(S1210). 구체적으로, 도 2를 참조하면, 액티베이션 로드/스토어 유닛은 온 칩 버퍼로부터 인풋 액티베이션 (Act_In)을 액티베이션 버퍼로 전달할 수 있다. 액티베이션 버퍼는 인풋 액티베이션(Act_In)을 일시 적으로 저장할 수 있다. 다시, 도 18을 참조하면, 프로세싱 유닛이 액티베이션 버퍼로부터 인풋 액티베이션을 수신하여 아웃풋 액티베이 션을 생성한다(S1220). 이어서, 액티베이션 버퍼가 아웃풋 액티베이션을 저장한다(S1230). 다시, 도 17을 참조하면, 메인 로드/스토어 유닛이 동작하지 않음을 확인하고(S1300), 히든 로드/스토어 유닛이 제2 작업에 대한 제2 데이터를 로드한다(S1400).S1300 및 S1400 단계는 S1200 단계와 병렬적으로 수행될 수 있다. 다시, 도 17을 참조하면, 제2 데이터를 이용하여 제2 작업을 수행한다(S1500). 구체적으로 도 8을 참조하면, 제2 작업 실행(EXEC2)도 제2 프리페치(PreFetch2)와 같이 데이터의 로드가 있어야 하므로 의존적일 수 있다. 본 실시예에 따른 인공지능 코어 시스템은 제1 작업 실행(EXEC1) 중에 제2 작업 실행 (EXEC2)에 대응하는 데이터를 미리 가져오는 제2 프리페치(PreFetch2)를 수행하여 제1 작업 실행(EXEC1)이 끝나 자 마자 제2 작업 실행(EXEC2)을 바로 시작하게 할 수 있다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2025-0016785", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템을 설명하기 위한 블록도이다. 도 2는 도 1의 인공지능 코어의 구조를 세부적으로 설명하기 위한 블록도이다. 도 3은 도 2의 프로세싱 유닛의 구조를 세부적으로 설명하기 위한 블록도이다. 도 4는 프로세싱 유닛에 의해서 실행되는 딥러닝 작업의 신경망 네트워크의 구조를 설명하기 위한 개념도이다. 도 5는 도 2의 로드/스토어 유닛의 동작을 설명하기 위한 블록도이다. 도 6은 도 5의 로드/스토어 유닛의 구조를 세부적으로 설명하기 위한 블록도이다. 도 7은 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 프로그램 로드 동작을 시계열적으로 설명하기 위한 타임 다이어그램이다. 도 8은 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 데이터 프리페치 동작을 시계열적으로 설명하 기 위한 타임 다이어그램이다. 도 9는 본 발명의 몇몇 실시예들에 따른 인공지능 코어의 메인 로드/스토어 유닛을 세부적으로 설명하기 위한 블록도이다. 도 10은 본 발명의 몇몇 실시예들에 따른 인공지능 코어의 히든 로드/스토어 유닛을 세부적으로 설명하기 위한 블록도이다. 도 11은 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템을 설명하기 위한 블록도이다. 도 12는 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템을 설명하기 위한 블록도이다. 도 13은 도 12의 제1 인공지능 코어의 구조 및 동작을 세부적으로 설명하기 위한 블록도이다. 도 14는 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 로드/스토어 방법을 설명하기 위한 순서도이 다. 도 15는 도 14의 제1 프로그램을 로드하는 단계를 세부적으로 설명하기 위한 순서도이다. 도 16은 도 14의 제2 프로그램을 로드하는 단계를 세부적으로 설명하기 위한 순서도이다. 도 17은 본 발명의 몇몇 실시예들에 따른 인공지능 코어 시스템의 로드/스토어 방법을 설명하기 위한 순서도이 다. 도 18은 도 17의 제1 작업을 수행하는 단계를 세부적으로 설명하기 위한 순서도이다."}
