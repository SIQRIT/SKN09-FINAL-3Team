{"patent_id": "10-2022-0170243", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0085415", "출원번호": "10-2022-0170243", "발명의 명칭": "외부 지식 데이터를 기반으로 대화 서비스를 제공하는 장치, 방법 및 컴퓨터 프로그램", "출원인": "주식회사 케이티", "발명자": "서영경"}}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "외부 지식 데이터를 기반으로 대화 서비스를 제공하는 장치에 있어서,사용자와 시스템 간의 대화 히스토리 및 상기 사용자의 사용자 발화를 입력 받는 발화 입력부;상기 사용자 발화에 대한 외부 지식 데이터 및 인터넷 데이터를 추출하는 외부 데이터 추출부;상기 사용자 발화, 상기 대화 히스토리, 상기 외부 지식 데이터 및 상기 인터넷 데이터 중 적어도 하나에 기초하여 응답 생성 모델을 학습시키는 학습 수행부; 및상기 응답 생성 모델에 기초하여 상기 사용자 발화에 대한 응답을 생성하는 응답 생성부를 포함하는, 대화 서비스 제공 장치."}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 학습 수행부는,입력된 문장에 기초하여 다음 단어를 예측하는 문장 사전 학습 모델에 대한 학습을 수행하는 사전 학습 모듈;및상기 사전 학습 모델에 기초하여 상기 응답 생성 모델에 대한 학습을 수행하는 미세 조정 모듈을 포함하는, 대화 서비스 제공 장치."}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 미세 조정 모듈에 대한 1차 학습 데이터는 채팅 데이터, 로그 데이터 및 STT 데이터 중 적어도 하나를 포함하는 것인, 대화 서비스 제공 장치."}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 미세 조정 모듈에 대한 2차 학습 데이터는 기설정된 도메인의 대화 데이터인 것인, 대화 서비스 제공장치."}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 2 항에 있어서,상기 미세 조정 모듈은,어댑터 레이어(Adapter layer)에 더 기초하여 학습을 수행하는 것인, 대화 서비스 제공 장치."}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2024-0085415-3-외부 지식 데이터를 기반으로 대화 서비스를 제공하는 방법에 있어서,사용자와 시스템 간의 대화 히스토리 및 상기 사용자의 사용자 발화를 입력 받는 단계;상기 사용자 발화에 대한 외부 지식 데이터 및 인터넷 데이터를 추출하는 단계;상기 사용자 발화, 상기 대화 히스토리, 상기 외부 지식 데이터 및 상기 인터넷 데이터 중 적어도 하나에 기초하여 응답 생성 모델을 학습시키는 단계; 및상기 응답 생성 모델에 기초하여 상기 사용자 발화에 대한 응답을 생성하는 단계를 포함하는, 대화 서비스 제공 방법."}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 응답 생성 모델을 학습시키는 단계는,입력된 문장에 기초하여 다음 단어를 예측하는 문장 사전 학습 모델에 대한 학습을 수행하는 사전 학습 단계;및상기 사전 학습 모델에 기초하여 상기 응답 생성 모델에 대한 학습을 수행하는 미세 조정 단계를 포함하는, 대화 서비스 제공 방법."}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서,상기 미세 조정 단계에 대한 1차 학습 데이터는 채팅 데이터, 로그 데이터 및 STT 데이터 중 적어도 하나를 포함하는 것인, 대화 서비스 제공 방법."}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8 항에 있어서,상기 미세 조정 단계에 대한 2차 학습 데이터는 기설정된 도메인의 대화 데이터인 것인, 대화 서비스 제공방법."}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 7 항에 있어서,상기 미세 조정 단계는,어댑터 레이어(Adapter layer)에 더 기초하여 학습을 수행하는 것인, 대화 서비스 제공 방법."}
{"patent_id": "10-2022-0170243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "외부 지식 데이터를 기반으로 대화 서비스를 제공하는 명령어들의 시퀀스를 포함하는 컴퓨터 판독가능 기록매체에 저장된 컴퓨터 프로그램에 있어서,상기 컴퓨터 프로그램은 컴퓨팅 장치에 의해 실행될 경우,사용자와 시스템 간의 대화 히스토리 및 상기 사용자의 사용자 발화를 입력 받고,공개특허 10-2024-0085415-4-상기 사용자 발화에 대한 외부 지식 데이터 및 인터넷 데이터를 추출하고,상기 사용자 발화, 상기 대화 히스토리, 상기 외부 지식 데이터 및 상기 인터넷 데이터 중 적어도 하나에 기초하여 응답 생성 모델을 학습시키고,상기 응답 생성 모델에 기초하여 상기 사용자 발화에 대한 응답을 생성하도록 하는 명령어들의 시퀀스를 포함하는, 컴퓨터 판독가능 기록매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2022-0170243", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "대화 서비스 제공 장치는 사용자와 시스템 간의 대화 히스토리 및 상기 사용자의 사용자 발화를 입력 받는 발화 입력부, 상기 사용자 발화에 대한 외부 지식 데이터 및 인터넷 데이터를 추출하는 외부 데이터 추출부, 상기 사 용자 발화, 상기 대화 히스토리, 상기 외부 지식 데이터 및 상기 인터넷 데이터 중 적어도 하나에 기초하여 응답 생성 모델을 학습시키는 학습 수행부, 및 상기 응답 생성 모델에 기초하여 상기 사용자 발화에 대한 응답을 생성 하는 응답 생성부를 포함한다."}
{"patent_id": "10-2022-0170243", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 외부 지식 데이터를 기반으로 대화 서비스를 제공하는 장치, 방법 및 컴퓨터 프로그램에 관한 것이다."}
{"patent_id": "10-2022-0170243", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "딥러닝 기반 자연어 처리 학습을 진행하는 대화 모델의 경우, 높은 정확도를 위해 대용량 언어 모델의 사전 학 습을 진행하는데 많은 시간이 소요된다. 딥러닝 모델 사이즈가 증가함에 따라, 파라미터 수가 많은 모델은 사전 학습 이후 미세 조정 학습 시에도 세부 도메인에 특화된 지식을 학습하는 데 매우 긴 시간이 필요하다. 특히, 위키피디아와 같은 특정 외부 지식을 반영한 발화를 학습에 추가하는 경우, 지식으로부터 나타날 수 있는 모든 유형의 발화를 학습 데이터로 만들어야 하며 양이 많아 모델 학습을 완료하기까지 많은 인력과 시간이 든 다. 그러나 지식을 즉각적으로 반영해야 하는 실시간 대화 시스템에서는 별도의 학습 데이터 생성 시간 또는 모 델 학습 시간 없이 서비스를 제공하기 어렵다. 한편, 대화 서비스는 AI 비서, AI 스피커 또는 챗봇과 같은 채팅 시스템의 핵심 기능으로 사용자 발화에 따른 적절한 응답을 제공하는 서비스이다. 최근에는 딥러닝 기반 대화 서비스가 개발되고 있으며, 단순한 대화 영역 에서 벗어나 영화, 지식 백과, 뉴스 등의 다양한 지식과 결합하여 높은 수준의 답변을 생성할 수 있는 대화 서 비스를 위해 이를 효과적으로 추론할 수 있는 외부 지식 대화 서비스가 핵심적인 역할을 하고 있다. 딥러닝 기반 외부 지식 대화 서비스는 사용자와 시스템이 대화할 때 사용자의 질문에 보다 풍성한 답변 제공을 할 수 있도록 외부 지식을 사용하여 답변하는 서비스이다. 사용자의 대화 내용을 넘어서 지식 백과 등의 지식 내용을 활용하거나 세부 도메인에 특화된 지식을 사용하여 풍부하고 다양한 답변을 생성해낼 수 있다. 기존의 외부 지식을 활용하는 딥러닝 기반 실시간 대화 서비스의 경우, 다양한 발화로 답변할 수 있도록 도메인 별 나타날 수 있는 발화를 모두 직접 생성하여 입력으로 넣고 모델을 학습하고 있다. 하지만 대화에서 나타날 수 있는 모든 형태의 발화들을 생성하는 것은 불가능에 가까우며, 매번 새로 학습해야 하기 때문에 실시간 대응 이 어려운 문제가 있다."}
{"patent_id": "10-2022-0170243", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "외부 지식 데이터를 기반으로 대화 서비스를 제공하는 장치, 방법 및 컴퓨터 프로그램을 제공하고자 한다. 다만, 본 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2022-0170243", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 수단으로서, 본 발명의 일 실시예는, 사용자와 시스템 간의 대화 히스토리 및 상기 사용자의 사용자 발화를 입력받는 발화 입력부, 상기 사용자 발화에 대한 외부 지식 데이터 및 인터넷 데이터를 추출하는 외부 데이터 추출부, 상기 사용자 발화, 상기 대화 히스토리, 상기 외부 지식 데이터 및 상 기 인터넷 데이터 중 적어도 하나에 기초하여 응답 생성 모델을 학습시키는 학습 수행부, 및 상기 응답 생성 모 델에 기초하여 상기 사용자 발화에 대한 응답을 생성하는 응답 생성부를 포함하는 대화 서비스 제공 장치를 제 공할 수 있다.본 발명의 다른 실시예는, 사용자와 시스템 간의 대화 히스토리 및 상기 사용자의 사용자 발화를 입력 받는 단 계, 상기 사용자 발화에 대한 외부 지식 데이터 및 인터넷 데이터를 추출하는 단계, 상기 사용자 발화, 상기 대 화 히스토리, 상기 외부 지식 데이터 및 상기 인터넷 데이터 중 적어도 하나에 기초하여 응답 생성 모델을 학습 시키는 단계, 및 상기 응답 생성 모델에 기초하여 상기 사용자 발화에 대한 응답을 생성하는 단계를 포함하는 대화 서비스 제공 방법을 제공할 수 있다. 본 발명의 또 다른 실시예는, 컴퓨터 프로그램은 컴퓨팅 장치에 의해 실행될 경우, 사용자와 시스템 간의 대화 히스토리 및 상기 사용자의 사용자 발화를 입력 받고, 상기 사용자 발화에 대한 외부 지식 데이터 및 인터넷 데 이터를 추출하고, 상기 사용자 발화, 상기 대화 히스토리, 상기 외부 지식 데이터 및 상기 인터넷 데이터 중 적 어도 하나에 기초하여 응답 생성 모델을 학습시키고, 상기 응답 생성 모델에 기초하여 상기 사용자 발화에 대한 응답을 생성하도록 하는 명령어들의 시퀀스를 포함하는, 컴퓨터 판독가능 기록매체에 저장된 컴퓨터 프로그램을 제공할 수 있다. 상술한 과제 해결 수단은 단지 예시적인 것으로서, 본 발명을 제한하려는 의도로 해석되지 않아야 한다. 상술한 예시적인 실시예 외에도, 도면 및 발명의 상세한 설명에 기재된 추가적인 실시예가 존재할 수 있다."}
{"patent_id": "10-2022-0170243", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본 발명의 과제 해결 수단 중 어느 하나에 의하면, 사용자와 시스템 간의 대화 히스토리, 사용자의 사용 자 발화, 사용자 발화에 대한 외부 지식 데이터 및 인터넷 데이터 중 적어도 하나에 기초하여 외부 지식 데이터 를 기반으로 한 대화 서비스를 제공할 수 있다."}
{"patent_id": "10-2022-0170243", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 발명의 실시예를 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관 계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\"되어 있는 경우도 포함한다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아 니라 다른 구성요소를 더 포함할 수 있는 것을 의미하며, 하나 또는 그 이상의 다른 특징이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해 되어야 한다. 본 명세서에 있어서 '부(部)'란, 하드웨어에 의해 실현되는 유닛(unit), 소프트웨어에 의해 실현되는 유닛, 양 방을 이용하여 실현되는 유닛을 포함한다. 또한, 1 개의 유닛이 2 개 이상의 하드웨어를 이용하여 실현되어도 되고, 2 개 이상의 유닛이 1 개의 하드웨어에 의해 실현되어도 된다. 본 명세서에 있어서 단말 또는 디바이스가 수행하는 것으로 기술된 동작이나 기능 중 일부는 해당 단말 또는 디 바이스와 연결된 서버에서 대신 수행될 수도 있다. 이와 마찬가지로, 서버가 수행하는 것으로 기술된 동작이나 기능 중 일부도 해당 서버와 연결된 단말 또는 디바이스에서 수행될 수도 있다. 이하 첨부된 도면을 참고하여 본 발명의 일 실시예를 상세히 설명하기로 한다. 도 1은 본 발명의 일 실시예에 따른 대화 서비스 제공 장치의 구성도이다. 도 1을 참조하면, 본 발명의 일 실시예에 따른 대화 서비스 제공 장치는 발화 입력부, 외부 데이터 추출부, 학습 수행부, 및 응답 생성부를 포함할 수 있다. 다만, 도 1에 도시된 대화 서비스 제 공 장치는 본 발명의 하나의 구현 예에 불과하며, 도 1에 도시된 구성요소들을 기초로 하여 여러 가지 변 형이 가능하다. 대화 서비스 제공 장치는 외부 지식 데이터를 기반으로 대화 서비스를 제공하는 장치일 수 있다. 여기서 외부 지식 데이터는 위키피디아, 백과사전 등의 일반 지식과 의학 지식 등의 전문 지식을 포함하는 지식 전문을 의미할 수 있다. 이하에서는 도 2 내지 도 4를 함께 참조하여 도 1을 설명하기로 한다. 발화 입력부는 사용자와 시스템 간의 대화 히스토리 및 사용자의 사용자 발화를 입력받을 수 있다. 일 실시예에서, 발화 입력부는 대화 데이터를 입력 받고, 입력 받은 대화 데이터를 사용자와 시스템 간의 대화 히스토리와 사용자의 마지막 발화인 사용자 발화로 나누는 작업을 수행할 수 있다. 구체적으로, 대화 데이터의 경우, <대화 히스토리, 사용자 발화>의 형식으로 구성되며, 싱글 턴(Single Turn) 또는 멀티 턴(Multi Turn)의 발화로 구성될 수 있다. 여기서, 턴(Turn)은 두 명 이상의 화자가 주고 받는 문장의 단위일 수 있으며, 두 명이 주고 받는 대화의 1턴은 2문장으로 구성될 수 있다. 멀티 턴은 두 명 이상의 화자가 각각 한 문장 이상의 대화를 나눌 때의 문장 단위일 수 있으며, 멀티 턴 대화를 구성하는 전체 문장의 수는 2문장을 초과할 수 있다. 싱글 턴은 두 명 이상의 화자 가 각각 한 문장만 대화를 나눌 때늬 문장 단위일 수 있으며, 싱글 턴 대화를 구성하는 전체 문장의 수는 2문장 일 수 있다. 여기서, 대화 히스토리는 사용자와 시스템 간의 발화를 주고 받은 데이터로, <사용자 발화, 시스템 답변>의 형 식으로 구성될 수 있다. 멀티 턴의 경우, <사용자 발화1, 시스템 답변1, 사용자 발화2, 시스템 답변2, ...>의 형식으로 구성될 수 있다. 이러한 대화 히스토리 및 사용자 발화는 후술할 미세 조정 모듈의 인코더(Encoder) 모듈에 입력될 수 있다. 도 2는 본 발명의 일 실시예에 따른 대화 데이터의 예시이다. 도 2에 도시된 바와 같이, 대화 데이터가 <사용자 발화1, 시스템 답변1, 사용자 발화2>의 형식 으로 구성될 수 있다. 여기서, 사용자 발화는 사용자의 마지막 발화인 사용자 발화2로, \"USER: 그러면 어떤 것을 개발했는지 알 아?\"이고, 대화 히스토리는 사용자 발화1 및 시스템 답변1으로, \"USER: KT는 디지털 플랫폼 기업이야.\", \"SYSTEM: 그래 나도 알고 있어!\"일 수 있다. 본 발명의 일 실시예에 따르면, 외부 지식 데이터를 기반으로 시스템이 \"SYSTEM: KT는 셋탑박스에도 탑재되어 있는 인공지능 스피커 기가지니를 개발했어.\"와 같이 응답을 출력하도록 생성 응답을 생성할 수 있다. 다시 도 1로 돌아오면, 외부 데이터 추출부는 사용자 발화에 대한 외부 지식 데이터 및 인터넷 데이터를 추출할 수 있다. 구체적으로, 외부 데이터 추출부는 사용자 발화에 대해 사전에 저장된 외부 지식 데이터 를 추출하고, 인터넷을 검색하여 관련된 인터넷 데이터를 추출할 수 있다. 일 실시예에서, 외부 데이터 추출부는 사용자의 발화와 관련된 외부 지식 데이터 및 인터넷 데이터를 유사 도(Similarity) 알고리즘 기반으로 추출할 수 있다. 연관성이 높은 지식 정보를 추출하는 유사도 알고리즘은 코 사인 유사도, 자카드 유사도 등 다양한 방법을 포함할 수 있다. 외부 데이터 추출부는 사용자의 발화에 기초하여 기존에 저장된 지식 백과 등의 정보에 접근하여 해당 발 화와 가장 연관성이 높은 지식 정보를 추출함으로써, 외부 지식 데이터를 추출할 수 있다. 외부 데이터 추출부는 사용자의 발화를 활용해 인터넷 API로 웹 지식에 접근하여 해당 발화와 가장 연관성 이 높은 지식 정보를 추출함으로써, 인터넷 데이터를 추출할 수 있다. 이러한 외부 지식 데이터 및 인터넷 데이터는 후술할 미세 조정 모듈의 인코더(Encoder) 모듈에 입력될 수 있다. 학습 수행부는 사용자 발화, 대화 히스토리, 외부 지식 데이터 및 인터넷 데이터 중 적어도 하나에 기초하 여 응답 생성 모델을 학습시킬 수 있다. 학습 수행부는 사전 학습 모듈 및 미세 조정 모듈을 포함할 수 있다. 사전 학습 모듈은 입력된 문장에 기초하여 다음 단어를 예측하는 문장 사전 학습 모델에 대한 학습을 수행 할 수 있다. 다시 말해, 사전 학습(Pre-training)은 딥러닝 모델에서 문장 생성을 위해 사전 학습하는 단계로, 문장을 단어로 나눈 후 현재 단어 이후에 나올 단어를 예측하여 문장 구조에 대해 사전 학습을 수행하는 것을 의미할 수 있다. 사전 학습 모듈은 학습에 필요한 입력 문장 데이터와 Transformer 기반의 문장 사전 학습 모델로 구성될 수 있다. 구체적으로, 사전 학습 모듈은 사용자의 입력 문장을 수신하여, 문장 사전 학습 모델을 통해 입 력 문장에 기초하여 다음 단어를 예측하는 Masked Learning을 수행할 수 있다. 이 때, 문장 사전 학습 모델의 경우, BERT, Xlnet 등 인코더(Encoder) 기반 모델 또는 T5와 같은 인코더 및 디 코더(Encoder-Decoder) 기반 모델을 모두 활용할 수 있으며, GPT-1, GPT-2, GPT-3 등 GPT 계열의 디코더 (Decoder) 기반 모델 또한 모두 활용할 수 있다. 여기서 인코더는 입력 문장을 하나의 벡터 표현으로 압축할 수 있으며, 디코더는 인코더에서 출력한 벡터 표현 을 통해 변환하여 출력 문장을 만들어낼 수 있다. 미세 조정 모듈은 학습된 문장 사전 학습 모델에 기초하여 응답 생성 모델에 대한 학습을 수행할 수 있다. 다시 말해, 미세 조정 학습(Fine-tuning)은 사전 학습된 딥러닝 모델을 이용하여 이러한 모델을 목표 태스크 (Target Task)에 맞추어 미세 조정하는 단계로, 이 과정에서 목표 도메인에 맞게 추가적으로 학습을 수행하는 것을 의미할 수 있다. 구체적으로, 미세 조정 모듈은 사용자 발화, 대화 히스토리, 외부 지식 데이터, 인터넷 데이터를 입력 받 아, 문장 사전 학습 모델에 기초하여 인코더 및 디코더 모델을 통해 이들의 관계 학습을 수행할 수 있다. 실시 간으로 대화를 생성할 때는 학습이 완료된 미세 조정 모듈을 활용해 응답을 생성할 수 있다. 미세 조정 모듈은 인코더 및 디코더 기반 모델을 활용하여 인코더 모듈과 디코더 모듈로 구성되며, 사용자 발화, 대화 히스토리, 외부 지식 데이터 및 인터넷 데이터를 입력받아 응답을 생성할 수 있다. 인코더 및 디코더 기반 모델을 활용하게 되면, 인코더의 언어 이해 학습 능력과 디코더의 언어 생성 학습 능력 의 장점을 모두 활용할 수 있으므로 시스템이 사용자의 발화를 이해하고 생성하는 데 적합한 모델이 될 수 있다. 인코더에서는 양방향(Bi-directional)하게 학습하며 스팬 마스킹(Span masking)을 진행하여 노이즈 제거 (Denoising)된 토큰을 예측하는 방식으로 학습할 수 있고, 디코더에서는 학습을 진행하면서 인코더의 예측한 토 큰 벡터를 텍스트 토큰으로 변환하는 방식으로 학습할 수 있다. 이 때, 인코더 모델(Encoder-only)만을 활용하게 되면 문장 생성 능력의 학습이 부족할 수 있으므로, 본 발명의 일 실시예에서는 디코더를 부착하여 인코더와 joint하게 학습할 수 있도록 구성할 수 있다. 인코더 모듈에서는 문장을 보다 정확한 수치를 가진 벡터로 변환할 수 있도록 학습 및 해석하며, 디코더 모듈은 벡터를 정확한 문장으로 변환할 수 있도록 학습 및 해석을 진행할 수 있다. 각각의 입력은 인코더 모듈을 통해 문장이 각각의 벡터로 변환된 후, 하나의 벡터로 결합하여 결합 데이터 (Concatenated Data)를 생성할 수 있다. 여기서, 결합 데이터는 입력 받은 여러 벡터 값을 이어붙여 만든 하나 의 벡터 값을 의미할 수 있다. 결합 데이터의 벡터 형태는 <사용자 발화 벡터, 대화 히스토리 벡터, 인터넷 데 이터 벡터, 외부 지식 데이터 벡터>와 같이 구성되며, 이 때 벡터의 차원(Dimension)과 구조는 활용하는 인코더 모듈에 따라 달라질 수 있다. 하나의 벡터로 결합된 결합 데이터는 디코더 모듈의 입력으로 활용되어, 디코더 모듈에서는 결합 데이터의 벡터 정보를 통해 사용자 발화, 대화 히스토리, 인터넷 데이터 그리고 외부 지식 데이터를 결합한 응답을 생성할 수 있다. 인코더, 디코더 모듈은 효율적으로 학습할 수 있도록 모두 사전 학습과 미세 조정 학습을 활용한 모델을 사용하 게 되며, 사전 학습과 미세 조정 학습 각각의 학습 방식에 따라 사용자가 입력해야 하는 문장의 형식이 다를 수 있다. 인코더, 디코더 모듈의 사전 학습의 경우, 여러 계층의 레이어(Layer)가 쌓인 문장 사전 학습 모델을 통해 다음 단어를 예측하는 방식인 Next Sentence Prediction 방법으로 학습을 수행할 수 있다. 문장 사전 학습 모델에 문장을 입력하면, 문장의 컨텍스트(Context) 벡터, 임베딩(Embedding) 벡터, 및 위치 임 베딩 벡터를 생성할 수 있다. 여기서, 컨텍스트 벡터는 단어의 문맥이 포함된 벡터이고, 임베딩 벡터는 사전에 있는 단어와 대조한 임베딩 벡터이고, 위치 임베딩 벡터는 단어의 위치 정보가 포함된 벡터일 수 있다. 미세 조정 모델의 인코더 및 디코더 모델의 경우, 동일한 구조로 구성되어 있으며 앞서 문장 사전 학습 모델을 통해 얻은 컨텍스트 벡터, 임베딩 벡터, 및 위치 임베딩 벡터를 활용해 적절한 문장을 생성할 수 있도록 학습될 수 있다. 미세 조정 학습 시 데이터를 도메인에 따라 두 가지로 분류하여 학습을 진행하게 되는데, 먼저 전반적인 대화 데이터가 포함된 데이터를 1차 학습하고, 풀고자 하는 도메인에 직접적으로 관련된 데이터를 나누어 2차 학습하 는 방식으로, 2단계로 나누어 학습될 수 있다. 여기서, 미세 조정 모듈에 대한 1차 학습 데이터는 채팅 데이터, 로그 데이터 및 STT 데이터 중 적어도 하 나를 포함할 수 있다. 여기서, 미세 조정 모듈에 대한 2차 학습 데이터는 기설정된 도메인의 대화 데이터일 수 있다. 학습이 오래 진행될수록 이전에 학습했던 내용을 상대적으로 기억하기 어렵기 때문에, 이 방법을 통해 도메인과 직접적으로 관련된 데이터에 적합하도록 효과적으로 학습되도록 할 수 있다. 도 3은 본 발명의 일 실시예에 따른 인코더 및 디코더 모듈을 학습하는 방법을 설명하기 위한 도면이다. 인코더 모듈 및 디코더 모듈의 각 딥러닝 모델은 입력 문장을 분석하여 문장의 내재된 정보(단어 간의 관계, 내 포하는 의미 등)을 추론하는 Transformer 구조 12개로 구성될 수 있다. Transformer 구조는 입력 문장의 일부 단어에 Mask를 씌워 Mask에 등장할 단어를 추론하며 학습하며, 이 때, Attention을 추가하여 효율적으로 학습하기 위한 Masked Self Attention layer와 정방향으로 학습하기 위한 Feed Forward layer으로 구성될 수 있다. 사전 학습은 각각 Masked Self Attention layer 및 Feed Forward layer를 진행하는 순서로 수행하며 다음 단어 를 예측하는 학습을 수행할 수 있다. 미세 조정 학습은 사전 학습과 마찬가지로 Transformer 구조를 활용하며, 인코더 및 디코더 모델로 나누어 학습 할 수 있다. 도 3에 도시된 바와 같이, 인코더 모델은 Multi-head Attention layer와 Fully-connected layer가 포함된 Transformer 구조 12개로 구성되고, 디코더 모델은 인코더 모델과 유사한 Multi-headed Attention layer, Fully-connected layer가 포함되며, 인코더 모델의 입력을 joint하게 학습할 수 있도록 Masked Multi-head Attention layer가 추가된 형태인 Transformer 구조 12개로 구성될 수 있다. 일 실시예에서, 미세 조정 모듈은 어댑터 레이어(이하, Adapter layer)에 더 기초하여 학습을 수행할 수 있다. 즉, 외부 지식을 효율적으로 학습하기 위하여 Transformer 구조에 Adapter layer(322, 324)를 추가 하여 학습하게 되는데, 이러한 Adapter layer는 도 4의 일 실시예에서 도시된 것과 동일한 구조로 학습할 수 있 다.. 도 4를 참조하면, Adapter layer는 Feed forward layer(411, 412) 2개로 이루어졌으며 이전 layer의 결과 와 Feed forward layer(411, 412) 2개를 거친 결과의 Weighted Sum을 한 결과값을 다음 layer로 보내며 학습을 진행할 수 있다. Adapter layer는 미세 조정 학습 시 모든 파라미터를 새로 학습하는 것이 아니라 사전 학습된 파라미터를 그대로 사용하고 Feed forward layer(411, 412) 2개 층의 파라미터만 학습하는 방식으로 하여 기존 사전 학습된 파라미터를 활용하면서도 외부 지식을 동시에 학습할 수 있도록 할 수 있다. 다시 도 3을 참조하면, 인코더 모델의 입력은 <사용자 발화, 대화 히스토리, 외부 지식 데이터 , 인터넷 데이터>로 구성될 수 있다. 각 입력 데이터는 도 3과 같은 예시 문장으로 구성될 수 있다. 각각의 문장은 인코더 모델을 통과하여 서로 다른 4개의 벡터값으로 변환될 수 있다. 이 때, 벡터 간의 관 계를 학습하기 위해 벡터를 하나의 차원(Dimension)으로 연결(Concatenate)하여 하나의 결합 벡터를 생성 할 수 있다. 인코더 모델로부터 생성된 하나의 결합 벡터는 디코더 모델의 입력으로 하여, 해당 벡터에 대한 적절한 발화를 생성할 수 있도록 학습될 수 있다. 인코더 모델에서 학습된 결과를 디코더 모델에서 활용하여 joint하게 학습할 수 있도록 인코더 모델 과는 다르게 Multi-head attention layer가 추가될 수 있다. 즉, 디코더 모델의 경우 인코더 모델에서 입력 받은 벡터 값과 모델 내에서 예측한 값을 비교하여 확률이 높은 벡터에 가중치(weight)를 부여할 수 있도록 Multi-head Attention layer를 추가하여 구성될 수 있다. 다시 도 1로 돌아오면, 응답 생성부는 응답 생성 모델에 기초하여 사용자 발화에 대한 응답을 생성할 수 있다. 일 실시예에서, 응답 생성부는 생성된 응답에 대한 문장을 최종 출력하기 전에 일정 조건을 통해 후처리 하는 기능을 수행할 수 있다. 즉, 응답 생성부는 응답을 생성할 때는 생성된 문장들의 중복을 제거하거나 일정 길이 이하의 문장을 제거 하는 등 문장의 완성도를 위해 대화 시스템에서 사용하는 다양한 필터링 방법을 적용할 수 있다. 응답 생성부 는 필터 후 생성된 응답 문장으로 최종 시스템의 발화를 얻을 수 있다. 도 5는 본 발명의 일 실시예에 따른 대화 서비스 제공 장치에서 외부 지식 데이터를 기반으로 대화 서비스를 제 공하는 방법의 순서도이다. 도 5에 도시된 대화 서비스 제공 장치에서 외부 지식 데이터를 기반으로 대화 서비스를 제공하는 방법은 도 1 내지 도 4에 도시된 실시예에 따라 시계열적으로 처리되는 단계들을 포함한다. 따라서, 이하 생략된 내용이라고 하더라도 도 1 내지 도 4에 도시된 실시예에 따라 대화 서비스 제공 장치(10 0)에서 외부 지식 데이터를 기반으로 대화 서비스를 제공하는 방법에도 적용된다. 단계 S510에서 대화 서비스 제공 장치는 사용자와 시스템 간의 대화 히스토리 및 사용자의 사용자 발화를 입력받을 수 있다. 단계 S520에서 대화 서비스 제공 장치는 사용자 발화에 대한 외부 지식 데이터 및 인터넷 데이터를 추출할 수 있다. 단계 S530에서 대화 서비스 제공 장치는 사용자 발화, 대화 히스토리, 외부 지식 데이터 및 인터넷 데이터 중 적어도 하나에 기초하여 응답 생성 모델을 학습시킬 수 있다. 단계 S540에서 대화 서비스 제공 장치는 응답 생성 모델에 기초하여 사용자 발화에 대한 응답을 생성할 수 있다. 상술한 설명에서, 단계 S510 내지 S540은 본 발명의 구현예에 따라서, 추가적인 단계들로 더 분할되거나, 더 적 은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 전환될 수도 있다. 도 1 내지 도 5를 통해 설명된 대화 서비스 제공 장치에서 외부 지식 데이터를 기반으로 대화 서비스를 제 공하는 방법은 컴퓨터에 의해 실행되는 매체에 저장된 컴퓨터 프로그램 또는 컴퓨터에 의해 실행 가능한 명령어 를 포함하는 기록 매체의 형태로도 구현될 수 있다. 또한, 도 1 내지 도 6을 통해 설명된 대화 서비스 제공 장 치에서 외부 지식 데이터를 기반으로 대화 서비스를 제공하는 방법은 컴퓨터에 의해 실행되는 매체에 저장 된 컴퓨터 프로그램의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매 체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독가능 매체는 컴퓨터 저장 매체를 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보 의 저장을 위한 임의의 방법 또는 기술로 구현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한 다."}
{"patent_id": "10-2022-0170243", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본 발명의 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자는 본 발명 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 발명의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으 로 해석되어야 한다."}
{"patent_id": "10-2022-0170243", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 대화 서비스 제공 장치의 구성도이다. 도 2는 본 발명의 일 실시예에 따른 대화 데이터의 예시이다. 도 3은 본 발명의 일 실시예에 따른 인코더 및 디코더 모듈을 학습하는 방법을 설명하기 위한 도면이다. 도 4는 본 발명의 일 실시예에 따른 어댑터 레이어의 구조를 설명하기 위한 도면이다. 도 5는 본 발명의 일 실시예에 따른 대화 서비스 제공 장치에서 외부 지식 데이터를 기반으로 대화 서비스를 제 공하는 방법의 순서도이다."}
