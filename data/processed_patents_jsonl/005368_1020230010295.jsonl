{"patent_id": "10-2023-0010295", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0117891", "출원번호": "10-2023-0010295", "발명의 명칭": "음향 이벤트 및 방향 탐지 모델의 트레이닝 장치, 음향 이벤트 및 방향 탐지 모델의 트레이닝", "출원인": "주식회사 케이티", "발명자": "강상익"}}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "각각이, 복수의 음원에서 발생하여 동시간에 수집된 음향으로부터 복수의 음향 이벤트 및 상기 복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지하여 복수의 트랙쌍을 출력하도록 구성되는 복수의 단일모델;상기 복수의 단일모델이 출력한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하도록 구성되는 앙상블 모델; 및상기 복수의 단일모델 및 상기 앙상블 모델을 트레이닝하는 제어부;를 포함하고,상기 복수의 트랙쌍 각각은,음향 이벤트의 발생 확률을 나타내는 이벤트 트랙 및 음향 이벤트의 방향을 나타내는 방향 트랙을 포함하는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 복수의 단일모델은, 서로 다른 구조를 가지는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 복수의 음원은, 동일한 클래스에 속하는 둘 이상의 음원을 포함하고,상기 동일 클래스에 속하는 둘 이상의 음원에 대한 음향 이벤트 및 음향 이벤트의 방향은, 상기 복수의 트랙쌍중 서로 다른 트랙쌍에 기록되는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1항에 있어서,상기 복수의 단일모델 각각은, 특정 음원의 음향 이벤트 및 음향 이벤트의 방향을 상기 복수의 트랙쌍 중 무작위로 선정된 어느 하나의 트랙쌍에 기록하는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4항에 있어서,상기 제어부는,지도 학습 알고리즘에 기반하여, 훈련용 음향을 트레이닝 데이터로 사용하고 정답 트랙쌍을 레이블링 데이터로사용하여 상기 복수의 단일모델을 트레이닝하는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5항에 있어서,공개특허 10-2024-0117891-3-상기 제어부는,상기 복수의 단일모델 중 제1 단일모델이 상기 훈련용 음향에 기초하여 N개의 트랙쌍을 출력하면, 상기 출력된N개의 트랙쌍과 N개의 정답 트랙쌍을 비교하여 각 트랙쌍에 가장 가까운 정답 트랙쌍을 선정하고,상기 각 트랙쌍과 선정된 정답 트랙쌍의 차이를 이용하여 상기 제1 단일모델을 트레이닝하는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 5항에 있어서,상기 제어부는,상기 복수의 단일모델 중 제1 단일모델이 상기 훈련용 음향에 기초하여 복수의 트랙쌍을 출력하면, 상기 출력된복수의 트랙쌍의 이벤트 트랙 및 복수의 정답 트랙쌍의 이벤트 트랙을 비교하여 이벤트 손실값을 산출하고, 상기 출력된 복수의 트랙쌍의 방향 트랙 및 상기 복수의 정답 트랙쌍의 방향 트랙을 비교하여 방향 손실값을 산출하고, 상기 이벤트 손실값 및 상기 방향 손실값을 합산한 전체 손실값을 이용하여 상기 제1 단일모델을 트레이닝하는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서,상기 복수의 단일모델 각각은, 상기 음향 이벤트의 탐지를 위한 제1 태스크 네트워크 및 상기 음향 이벤트의 방향 탐지를 위한 제2 태스크 네트워크를 포함하고,상기 제어부는,멀티 태스크 러닝 기법을 사용하여 상기 제1 태스크 네트워크 및 상기 제2 태스크 네트워크를 트레이닝 하는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 4항에 있어서,상기 제어부는,지도 학습 알고리즘에 기반하여, 상기 복수의 단일모델이 출력한 트랙쌍들을 트레이닝 데이터로 사용하고 최종정답 트랙쌍들을 레이블링 데이터로 사용하여 상기 앙상블 모델을 트레이닝하는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9항에 있어서,상기 제어부는,상기 앙상블 모델이 상기 복수의 단일모델이 출력한 트랙쌍들을 이용하여 최종 트랙쌍들을 출력하면, 상기 출력된 최종 트랙쌍들과 상기 최종 정답 트랙쌍들의 차이를 이용하여 상기 앙상블 모델을 트레이닝 하는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10항에 있어서,상기 복수의 단일모델의 개수는 M이고,공개특허 10-2024-0117891-4-상기 복수의 단일모델 각각은, N개의 트랙쌍을 출력하고,상기 앙상블 모델은,상기 복수의 단일모델이 출력한 N*M개의 트랙쌍들을 제공받아 연산함으로써 상기 N개의 최종 트랙쌍을 출력하는음향 이벤트 및 방향 탐지 모델의 트레이닝 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "복수의 단일모델 각각이 복수의 음원에서 발생하여 동시간에 수집된 음향으로부터 복수의 음향 이벤트 및 상기복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지하여 복수의 트랙쌍을 출력하도록, 상기 복수의 단일모델을 트레이닝 하는 단계; 및앙상블 모델이 상기 복수의 단일모델이 출력한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하도록,상기 앙상블 모델을 트레이닝하는 단계;를 포함하고,상기 복수의 트랙쌍 각각은,음향 이벤트의 발생 확률을 나타내는 이벤트 트랙 및 음향 이벤트의 방향을 나타내는 방향 트랙을 포함하는음향 이벤트 및 방향 탐지 모델의 트레이닝 방법."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12항에 있어서,상기 복수의 단일모델은, 서로 다른 구조를 가지는음향 이벤트 및 방향 탐지 모델의 트레이닝 방법."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 12항에 있어서,상기 복수의 음원은, 동일한 클래스에 속하는 둘 이상의 음원을 포함하고,상기 동일 클래스에 속하는 둘 이상의 음원에 대한 음향 이벤트 및 음향 이벤트의 방향은, 상기 복수의 트랙쌍중 서로 다른 트랙쌍에 기록되는음향 이벤트 및 방향 탐지 모델의 트레이닝 방법."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 12항에 있어서,상기 복수의 단일모델 각각은, 특정 음원의 음향 이벤트 및 음향 이벤트의 방향을 상기 복수의 트랙쌍 중 무작위로 선정된 어느 하나의 트랙쌍에 기록하는음향 이벤트 및 방향 탐지 모델의 트레이닝 방법."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15항에 있어서,상기 복수의 단일모델을 트레이닝 하는 단계는,지도 학습 알고리즘에 기반하여, 훈련용 음향을 트레이닝 데이터로 사용하고 정답 트랙쌍을 레이블링 데이터로사용하여 상기 복수의 단일모델을 트레이닝하는 단계;를 포함하는음향 이벤트 및 방향 탐지 모델의 트레이닝 방법."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16항에 있어서,공개특허 10-2024-0117891-5-상기 훈련용 음향을 트레이닝 데이터로 사용하고 정답 트랙쌍을 레이블링 데이터로 사용하여 상기 복수의 단일모델을 트레이닝하는 단계는,상기 복수의 단일모델 중 제1 단일모델이 상기 훈련용 음향에 기초하여 N개의 트랙쌍을 출력하면, 상기 출력된N개의 트랙쌍과 N개의 정답 트랙쌍을 비교하여 각 트랙쌍에 가장 가까운 정답 트랙쌍을 선정하는 단계; 및상기 각 트랙쌍과 선정된 정답 트랙쌍의 차이를 이용하여 상기 제1 단일모델을 트레이닝하는 단계;를 포함하는음향 이벤트 및 방향 탐지 모델의 트레이닝 방법."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 15항에 있어서,상기 앙상블 모델을 트레이닝하는 단계는,지도 학습 알고리즘에 기반하여, 상기 복수의 단일모델이 출력한 트랙쌍들을 트레이닝 데이터로 사용하고 최종정답 트랙쌍들을 레이블링 데이터로 사용하여 상기 앙상블 모델을 트레이닝하는 단계;를 포함하는음향 이벤트 및 방향 탐지 모델의 트레이닝 방법."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "복수의 마이크로 구성되어 외부의 복수의 음원에서 발생한 음향을 수신하는 멀티채널 마이크; 및상기 복수의 음원에서 발생하여 동시간에 수집된 음향을 음향 이벤트 및 방향 탐지 모델에 제공하는 제어부;를포함하고,상기 음향 이벤트 및 방향 탐지 모델은,각각이, 복수의 음원에서 발생하여 동시간에 수집된 음향으로부터 복수의 음향 이벤트 및 상기 복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지하여 복수의 트랙쌍을 출력하는 복수의 단일모델; 및상기 복수의 단일모델이 출력한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하는 앙상블 모델;을포함하고,상기 복수의 트랙쌍 각각은,음향 이벤트의 발생 확률을 나타내는 이벤트 트랙 및 음향 이벤트의 방향을 나타내는 방향 트랙을 포함하는음향 이벤트 및 방향 탐지 장치."}
{"patent_id": "10-2023-0010295", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "복수의 단일모델 각각이, 복수의 음원에서 발생하여 동시간에 수집된 음향으로부터 복수의 음향 이벤트 및 상기복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지하여 복수의 트랙쌍을 출력하는 단계; 및앙상블 모델이, 상기 복수의 단일모델이 출력한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하는단계;를 포함하는 음향 이벤트 및 방향 탐지 방법을 수행하기 위하여 컴퓨터가 읽을 수 있는 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0010295", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "음향 이벤트 및 방향 탐지 모델의 트레이닝 장치가 개시된다. 본 발명에 따른 음향 이벤트 및 방향 탐지 모델의 트레이닝 장치는, 각각이, 복수의 음원에서 발생하여 동시간에 수집된 음향으로부터 복수의 음향 이벤트 및 상기 복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지하여 복수의 트랙쌍을 출력하도록 구성되는 복수의 단일 모델, 상기 복수의 단일모델이 출력한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하도록 구성되는 앙상블 모델, 및, 상기 복수의 단일모델 및 상기 앙상블 모델을 트레이닝하는 제어부를 포함하고, 상기 복수의 트랙쌍 각각은, 음향 이벤트의 발생 확률을 나타내는 이벤트 트랙 및 음향 이벤트의 방향을 나타내는 방향 트랙 을 포함한다."}
{"patent_id": "10-2023-0010295", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은, 음향 이벤트 탐지 결과 및 방향 탐지 결과를 별도로 생성하되 이들을 복수의 트랙으로 생성하고, 복 수의 트랙을 이용하여 최종 트랙을 산출함으로써, 동일 클래스에 속하는 음향 이벤트들이 동시간에 발생하는 경 우에도 음향 이벤트 및 방향을 정확하게 검출할 수 있는, 음향 이벤트 및 방향 탐지 모델의 트레이닝 장치, 음 향 이벤트 및 방향 탐지 모델의 트레이닝 방법, 및 음향 이벤트 및 방향 탐지 장치에 관한 것이다."}
{"patent_id": "10-2023-0010295", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "수집된 음향을 이용하여 음향 이벤트 및 음향 이벤트의 방향을 탐지하는 기술은, 로봇, CCTV, 환경 모니터링, 도심 차량 트래픽 예측, 자율 주행 환경 분석 등 다양한 분야에 적용 할 수 있는 기술이다. 도 1에서 도시한 바와 같이, 실제 음향 환경에서는 복수의 음원에서 생성된 음향이 혼합되어 마이크에 입력되기 때문에, 다수의 음향 이벤트를 인식하고 동시에 해당 음향의 위치를 탐지하기 위한 기술의 필요성이 대두되었다."}
{"patent_id": "10-2023-0010295", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상술한 문제점을 해결하기 위한 것으로, 본 발명의 목적은, 음향 이벤트 탐지 결과 및 방향 탐지 결 과를 별도로 생성하되 이들을 복수의 트랙으로 생성하고, 복수의 트랙을 이용하여 최종 트랙을 산출함으로써, 동일 클래스에 속하는 음향 이벤트들이 동시간에 발생하는 경우에도 음향 이벤트 및 방향을 정확하게 검출할 수 있는, 음향 이벤트 및 방향 탐지 모델의 트레이닝 장치, 음향 이벤트 및 방향 탐지 모델의 트레이닝 방법, 및 음향 이벤트 및 방향 탐지 장치를 제공하기 위함이다."}
{"patent_id": "10-2023-0010295", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명에 따른 음향 이벤트 및 방향 탐지 모델의 트레이닝 장치는, 각각이, 복수의 음원에서 발생하여 동시간 에 수집된 음향으로부터 복수의 음향 이벤트 및 상기 복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지 하여 복수의 트랙쌍을 출력하도록 구성되는 복수의 단일모델, 상기 복수의 단일모델이 출력한 트랙쌍들을 제공 받아 연산함으로써 최종 트랙쌍들을 출력하도록 구성되는 앙상블 모델, 및, 상기 복수의 단일모델 및 상기 앙상 블 모델을 트레이닝하는 제어부를 포함하고, 상기 복수의 트랙쌍 각각은, 음향 이벤트의 발생 확률을 나타내는 이벤트 트랙 및 음향 이벤트의 방향을 나타내는 방향 트랙을 포함한다. 이 경우 상기 복수의 단일모델은, 서로 다른 구조를 가질 수 있다. 한편 상기 복수의 음원은, 동일한 클래스에 속하는 둘 이상의 음원을 포함하고, 상기 동일 클래스에 속하는 둘 이상의 음원에 대한 음향 이벤트 및 음향 이벤트의 방향은, 상기 복수의 트랙쌍 중 서로 다른 트랙쌍에 기록될 수 있다. 한편 상기 복수의 단일모델 각각은, 특정 음원의 음향 이벤트 및 음향 이벤트의 방향을 상기 복수의 트랙쌍 중 무작위로 선정된 어느 하나의 트랙쌍에 기록할 수 있다. 이 경우 상기 제어부는, 지도 학습 알고리즘에 기반하여, 훈련용 음향을 트레이닝 데이터로 사용하고 정답 트랙 쌍을 레이블링 데이터로 사용하여 상기 복수의 단일모델을 트레이닝할 수 있다. 이 경우 상기 제어부는, 상기 복수의 단일모델 중 제1 단일모델이 상기 훈련용 음향에 기초하여 N개의 트랙쌍을 출력하면, 상기 출력된 N개의 트랙쌍과 N개의 정답 트랙쌍을 비교하여 각 트랙쌍에 가장 가까운 정답 트랙쌍을 선정하고, 상기 각 트랙쌍과 선정된 정답 트랙쌍의 차이를 이용하여 상기 제1 단일모델을 트레이닝할 수 있다. 한편 상기 제어부는, 상기 복수의 단일모델 중 제1 단일모델이 상기 훈련용 음향에 기초하여 복수의 트랙쌍을 출력하면, 상기 출력된 복수의 트랙쌍의 이벤트 트랙 및 복수의 정답 트랙쌍의 이벤트 트랙을 비교하여 이벤트 손실값을 산출하고, 상기 출력된 복수의 트랙쌍의 방향 트랙 및 상기 복수의 정답 트랙쌍의 방향 트랙을 비교하 여 방향 손실값을 산출하고, 상기 이벤트 손실값 및 상기 방향 손실값을 합산한 전체 손실값을 이용하여 상기 제1 단일모델을 트레이닝할 수 있다. 한편 상기 복수의 단일모델 각각은, 상기 음향 이벤트의 탐지를 위한 제1 태스크 네트워크 및 상기 음향 이벤트 의 방향 탐지를 위한 제2 태스크 네트워크를 포함하고, 상기 제어부는, 멀티 태스크 러닝 기법을 사용하여 상기 제1 태스크 네트워크 및 상기 제2 태스크 네트워크를 트레이닝 할 수 있다. 한편 상기 제어부는, 지도 학습 알고리즘에 기반하여, 상기 복수의 단일모델이 출력한 트랙쌍들을 트레이닝 데 이터로 사용하고 최종 정답 트랙쌍들을 레이블링 데이터로 사용하여 상기 앙상블 모델을 트레이닝할 수 있다. 이 경우 상기 제어부는, 상기 앙상블 모델이 상기 복수의 단일모델이 출력한 트랙쌍들을 이용하여 최종 트랙쌍 들을 출력하면, 상기 출력된 최종 트랙쌍들과 상기 최종 정답 트랙쌍들의 차이를 이용하여 상기 앙상블 모델을 트레이닝 할 수 있다. 이 경우 상기 복수의 단일모델의 개수는 M이고, 상기 복수의 단일모델 각각은, N개의 트랙쌍을 출력하고, 상기 앙상블 모델은, 상기 복수의 단일모델이 출력한 N*M개의 트랙쌍들을 제공받아 연산함으로써 상기 N개의 최종 트 랙쌍을 출력할 수 있다. 한편 본 발명에 따른 음향 이벤트 및 방향 탐지 모델의 트레이닝 방법은, 복수의 단일모델 각각이 복수의 음원 에서 발생하여 동시간에 수집된 음향으로부터 복수의 음향 이벤트 및 상기 복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지하여 복수의 트랙쌍을 출력하도록, 상기 복수의 단일모델을 트레이닝 하는 단계, 및, 앙상블 모델이 상기 복수의 단일모델이 출력한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하도록, 상기 앙상블 모델을 트레이닝하는 단계를 포함하고, 상기 복수의 트랙쌍 각각은, 음향 이벤트의 발생 확률을 나타내 는 이벤트 트랙 및 음향 이벤트의 방향을 나타내는 방향 트랙을 포함한다. 이 경우 상기 복수의 단일모델은, 서로 다른 구조를 가질 수 있다. 한편 상기 복수의 음원은, 동일한 클래스에 속하는 둘 이상의 음원을 포함하고, 상기 동일 클래스에 속하는 둘 이상의 음원에 대한 음향 이벤트 및 음향 이벤트의 방향은, 상기 복수의 트랙쌍 중 서로 다른 트랙쌍에 기록될 수 있다. 한편 상기 복수의 단일모델 각각은, 특정 음원의 음향 이벤트 및 음향 이벤트의 방향을 상기 복수의 트랙쌍 중 무작위로 선정된 어느 하나의 트랙쌍에 기록할 수 있다. 이 경우 상기 복수의 단일모델을 트레이닝 하는 단계는, 지도 학습 알고리즘에 기반하여, 훈련용 음향을 트레이 닝 데이터로 사용하고 정답 트랙쌍을 레이블링 데이터로 사용하여 상기 복수의 단일모델을 트레이닝하는 단계를 포함할 수 있다. 이 경우 상기 훈련용 음향을 트레이닝 데이터로 사용하고 정답 트랙쌍을 레이블링 데이터로 사용하여 상기 복수 의 단일모델을 트레이닝하는 단계는, 상기 복수의 단일모델 중 제1 단일모델이 상기 훈련용 음향에 기초하여 N 개의 트랙쌍을 출력하면, 상기 출력된 N개의 트랙쌍과 N개의 정답 트랙쌍을 비교하여 각 트랙쌍에 가장 가까운 정답 트랙쌍을 선정하는 단계, 및, 상기 각 트랙쌍과 선정된 정답 트랙쌍의 차이를 이용하여 상기 제1 단일모델 을 트레이닝하는 단계를 포함할 수 있다. 한편 상기 앙상블 모델을 트레이닝하는 단계는, 지도 학습 알고리즘에 기반하여, 상기 복수의 단일모델이 출력 한 트랙쌍들을 트레이닝 데이터로 사용하고 최종 정답 트랙쌍들을 레이블링 데이터로 사용하여 상기 앙상블 모 델을 트레이닝하는 단계를 포함할 수 있다. 한편 본 발명에 따른 음향 이벤트 및 방향 탐지 장치는, 복수의 마이크로 구성되어 외부의 복수의 음원에서 발 생한 음향을 수신하는 멀티채널 마이크, 및, 상기 복수의 음원에서 발생하여 동시간에 수집된 음향을 음향 이벤 트 및 방향 탐지 모델에 제공하는 제어부를 포함하고, 상기 음향 이벤트 및 방향 탐지 모델은, 각각이, 복수의 음원에서 발생하여 동시간에 수집된 음향으로부터 복수의 음향 이벤트 및 상기 복수의 음향 이벤트에 각각 대응 하는 복수의 방향을 탐지하여 복수의 트랙쌍을 출력하는 복수의 단일모델, 및, 상기 복수의 단일모델이 출력한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하는 앙상블 모델을 포함하고, 상기 복수의 트랙쌍 각 각은, 음향 이벤트의 발생 확률을 나타내는 이벤트 트랙 및 음향 이벤트의 방향을 나타내는 방향 트랙을 포함한 다. 음향 이벤트 및 방향 탐지 방법을 수행하기 위하여 컴퓨터가 읽을 수 있는 매체에 저장된 컴퓨터 프로그램으로, 음향 이벤트 및 방향 탐지 방법은, 복수의 단일모델 각각이, 복수의 음원에서 발생하여 동시간에 수집된 음향으 로부터 복수의 음향 이벤트 및 상기 복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지하여 복수의 트랙 쌍을 출력하는 단계, 및, 앙상블 모델이, 상기 복수의 단일모델이 출력한 트랙쌍들을 제공받아 연산함으로써 최 종 트랙쌍들을 출력하는 단계를 포함한다."}
{"patent_id": "10-2023-0010295", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명 에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용 되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이 해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함한다\" 또 는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이 들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 발명을 구현함에 있어서 설명의 편의를 위하여 구성요소를 세분화하여 설명할 수 있으나, 이들 구성요소가 하나의 장치 또는 모듈 내에 구현될 수도 있고, 혹은 하나의 구성요소가 다수의 장치 또는 모듈들에 나뉘어져서 구현될 수도 있다. 도 2는 음향 이벤트 탐지 결과 및 방향 탐지 결과를 정의하는 일반적인 방법을 설명하기 위한 도면이다. 음향 이벤트 탐지 결과 및 방향 탐지 결과를 정의하는 방법으로는 크게 두가지 방법을 들 수 있다. 첫번째 방법은, 도 2a에서 도시한 바와 같이 음향 이벤트의 발생 여부(구체적으로 음향 이벤트가 발생한 확률) 을 표시한 음향 이벤트 탐지 결과와 이벤트의 방향(구체적으로 음향 이벤트가 발생한 방향의 x, y, z 좌 표)를 표시한 방향 탐지 결과를 정의하는 방법이다. 즉 첫번째 방법에서는, 음향 이벤트 탐지 결과(201 0)와 방향 탐지 결과가 독립적으로 생성되게 된다. 두번째 방법은, 도 2b에서 도시한 바와 같이 음향 이벤트의 발생 여부(구체적으로 음향 이벤트가 발생한 확률) 와 이벤트의 방향(구체적으로 음향 이벤트가 발생한 방향의 x, y, z 좌표)을 통합하여 하나의 탐지 결과 로 정의하는 방법이다. 두번째 방법에서는, 음향 이벤트의 방향이 x, y, z 좌표로 정의되며, 음향 이벤트가 발 생한 확률은 x, y, z 좌표에 대한 벡터의 길이값( )으로 정의된다. 즉 x, y, z 좌표에 대한벡터의 길이값( 은, 0 내지 1의 범위 내의 임의의 값으로 산출되어, 음향 이벤트의 확률을 나 타낼 수 있다. 두번째 방법은 ACCDOA(Activity-Coupled Cartesian Direction Of Arrival) 방법이라 명칭된다. 위 방법들로, 동시간에 발생한 서로 다른 클래스에 속하는 복수의 이벤트를 검출하는 것이 가능하다. 일 예로, 도 2b를 참고하면, 탐지 결과는 복수의 클래스(a, b, c, d, e, f) 각각에 대하여 음향 이벤트의 발생 확률 및 음향 이벤트의 방향을 나타내는 행렬로 표시된다. 예를 들어 도 2b에서 a 클래스에 해당하는 이벤트가 발생 한 확률은 으로 표현되며, a 클래스에 해당하는 이벤트가 발생한 방향은 (0.26, 0.73, 0)으로 표현된다. 다른 예를 들어 도 2b에서 e 클래스에 해당하는 이벤트가 발생한 확률은 으로 표현되며, e 클래스에 해당하는 이벤트가 발생한 방향은 (0.05, 0, 0.94)로 표 현된다. 위 방법들은, 동시간에 발생한 서로 다른 클래스에 속하는 복수의 음향 이벤트를 탐지하는 것이 가능한 반면, 동시간에 발생한 동일 클래스에 속하는 복수의 음향 이벤트를 탐지하는 것이 불가능한 문제가 있다. 동시간에 발생한 동일 클래스에 속하는 복수의 이벤트의 예시로, 두명의 남성이 이야기를 하는 것을 들 수 있다. 즉 두개 의 음원(두명의 남성)이 존재하기 때문에 두개의 음향 이벤트가 탐지되어야 하나, 두 음향 이벤트가 동일 클래 스(\"남성\" 클래스)에 속하므로, 두개의 음향 이벤트를 개별적으로 탐지하지 못하는 문제가 발생한다. 이러한 단점을 고려하여, 동시간에 발생한 동일 클래스에 속하는 복수의 이벤트를 검출하기 위한 ADPIT (Auxiliary Duplicating Permutation Invariant Training) 방법이 적용된 Multi-ACCDOA 방법이 고안되었다. 이 와 관련해서는 도 3을 참고하여 설명한다. 도 3은 트랙을 이용하여 탐지 결과를 정의하는 일반적인 방법을 설명하기 위한 도면이다. Multi-ACCDOA 방법에서는 트랙을 도입하여 탐지 결과를 나타낸다. 구체적으로 Multi-ACCDOA 방법은, 동시간에 복수의 음원에서 생성된 음향에 대하여, 클래스가 다른 음향 이벤트에 대해서는 중복을 허용하여 하나의 트랙에 표시하고, 클래스가 같은 음향 이벤트에 대해서는 다른 트랙에 표시하는 방법이다. 예를 들어 제1 트랙(Track 1)에는 a 클래스에 해당하는 제1 음향 이벤트의 발생 확률 및 방향과, e 클래스에 해당하는 제2 음향 이벤트의 발생 확률 및 방향이 표시된다. 또한 제2 트랙(Track2)에는 a 클래스에 해당하는 제3 음향 이벤트의 발생 확률 및 방향이 표시될 수 있다. 즉 제1 음향 이벤트와 제3 음향 이벤트가 동일 클래스(예를 들어 \"남성\" 클래스)에 속함에도 불구하고, 제1 음향 이벤트와 제3 음향 이벤트가 서로 다른 트랙에 기록되기 때문에, 동일 클래스에 속하는 복수의 음향 이벤트가 개별적으로 탐지될 수 있다. N개의 트랙이 생성되도록 기 설정되어있으나 실제로 는 N-1개의 트랙에만 이벤트가 기록되는 경우, 이벤트가 기록된 트랙들 중 하나를 빈 트랙에 복사함으로써, 인 공지능 모델의 학습 시 빈 트랙으로 인하여 학습이 잘 되지 않는 문제를 해결할 수 있다. 한편 ACCDOA(Activity-Coupled Cartesian Direction Of Arrival) 방법 및 Multi-ACCDOA 방법 모두 음향 이벤트 의 발생 확률과 이벤트의 방향을 통합한 탐지 결과를 정의하고 있으며, 이러한 방식은 통합에 의한 부작용으로 인공지능 네트워크의 학습이 어려워지고 성능이 떨어지는 단점이 존재한다. 즉 음향 이벤트의 탐지와 음향 이벤 트의 방향의 탐지라는 서로 다른 문제를 해결해야 함에도 불구하고, 음향 이벤트의 발생 확률과 이벤트의 방향 을 통합하여 탐지하는 것이기 때문에, 인공지능 네트워크의 학습이 어려워지고 성능이 떨어지는 부작용이 발생 하게 된다. 도 4는 본 발명에 따른, 음향 이벤트 및 방향 탐지 모델의 트레이닝 장치의 구성요소를 설명하기 위한 도면이다. 음향 이벤트 및 방향 탐지 모델의 트레이닝 장치(이하 \"장치\"라 함)는, 데이터 수집부, 제어부 및 메모리를 포함할 수 있다. 데이터 수집부는 음향 이벤트 및 방향 탐지 모델(이하 \"모델\"이라 함)을 트레이닝하기 위한 훈 련용 음향 및 레이블링 데이터를 수집할 수 있다. 데이터 수집부는 음향 수집 모듈을 포함하고, 훈련용 음향을 수집할 수 있다. \"훈련용\"이라는 용어는 모델 의 트레이닝에 사용하는 음향을 다른 음향과 구분짓기 위해 사용된 것으로, 본 명세서에서의 \"훈련용 음향\"에 대한 설명은 본 명세서에서의 \"음향\"에도 적용될 수 있다.훈련용 음향은 하나 이상의 음원으로부터 수집된 음향일 수 있다. 또한 훈련용 음향은 복수의 음원에서 발생하 여 동시간에 수집된 음향일 수 있다. 복수의 음원에서 발생하여 동시간에 수집된 음향이란, 하나의 음향 수집 모듈이 복수의 음원들에서 발생한 음향 들을 수집했다는 것을 의미할 수 있다. 예를 들어 복수의 음원에서 발생하여 동시간에 수집된 음향은, 두명의 남자 사이의 마이크가 두명의 남자의 대화를 수집한 것으로, 한명의 남자가 발생시킨 제1 음향 및 또 다른 남자 가 발생시킨 제2 음향을 포함할 수 있다. 다른 예를 들어 복수의 음원에서 발생하여 동시간에 수집된 음향은, 동일한 마이크가 여자의 목소리와 고양이 소리를 수집한 것으로, 여자가 발생시킨 제1 음향 및 고양이가 발생시 킨 제2 음향을 포함할 수 있다. 다른 예를 들어 복수의 음원에서 발생하여 동시간에 수집된 음향은, 동일한 마 이크가 노크 소리, 발걸음 소리 및 에어컨 소리를 수집한 것일 수 있다. 또한 복수의 음원에서 발생하여 동시간에 수집된 음향이란, 복수의 음원들에서 발생한 음향들을 수집한 시간 구 간이 겹치는 것을 의미할 수 있다. 예를 들어 음향 수집 모듈이 0초 내지 5초에 제1 음원으로부터 발생한 음향 을 수집하였고, 1초 내지 6초에 제2 음원으로부터 발생한 음향을 수집한 경우, 음향이 복수의 음원으로부터 동 시간에 수집되었다고 표현할 수 있다. 음향 이벤트의 방향을 탐지하기 위해, 데이터 수집부의 음향 수집 모듈은 복수의 마이크로 구성되는 멀티 채널 마이크를 포함할 수 있다. 멀티채널 마이크가 세개의 마이크로 구성되는 경우 2차원의 방향 탐지가 가능하 며, 멀티채널 마이크가 네개의 마이크로 구성되는 경우 3차원의 방향 탐지가 가능하다. 데이터 수집부는 음향 수집 모듈을 포함하여 훈련용 음향을 직접 수집할 수 있지만 이에 한정되지 않으며, 입력 모듈 또는 통신 모듈을 포함하여 외부 또는 외부 장치로부터 훈련용 음향을 수신할 수도 있다. 데이터 수집부는 입력 모듈 또는 통신 모듈을 포함하여, 외부 또는 외부 장치로부터 레이블링 데이터를 수 신할 수 있다. 여기서 레이블링 데이터는 지도 학습 알고리즘에서 사용되는 정답을 의미하는 것으로, 이벤트 트 랙 및 방향 트랙으로 구성되는 트랙쌍을 포함할 수 있다. 이와 관련해서는 추후에 자세히 설명한다. 제어부는 로봇 제어 장치의 전반적인 동작을 제어할 수 있다. 용어 “제어부”는, “마이크로 프로세 서”, “컨트롤러”, “마이크로 컨트롤러”, “프로세서” 등의 용어와 혼용되어 사용될 수 있다. 또한 제어부는 메모리로부터 모델을 독출하여 실행할 수 있다. 이에 따라, 아래에서 설명하는 모델의 동작은, 제어부의 동작인 것으로 볼 수도 있다. 또한 제어부는 모델을 트레이닝할 수 있다. 구체적으로 제어부는 모델 내 복수의 단일 모 델 및 앙상블 모델을 트레이닝할 수 있다. 메모리는 장치의 구동을 위한 프로그램, 명령어 및 기타 데이터를 저장할 수 있다. 또한 메모리는 모델을 저장할 수 있다. 구체적으로 모델은 소프트웨어 또는 하드웨어와 소프트 웨어의 조합으로 구현되는 인공지능 모델로, 모델을 구성하는 하나 이상의 명령어는 메모리에 저장될 수 있다. 도 5는 본 발명에 따른 음향 이벤트 및 방향 탐지 모델의 구조를 설명하기 위한 도면이다. 모델은 복수의 단일모델(210, 220, 230)을 포함할 수 있다. 이하에서는 복수의 단일모델(210, 220, 230)이 세개의 단일모델로 구성되는 것으로 설명하나 이에 한정되지 않으며, 더 많은 또는 더 적은 수의 단일모델로 구 성될 수도 있다. 복수의 단일모델(210, 220, 230) 각각은, 복수의 음원에서 발생하여 동시간에 수집된 훈련용 음향에서 복수의 음향 이벤트 및 복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지하여 복수의 트랙쌍을 생성하도록 구성 될 수 있다. 구체적으로 복수의 단일모델(210, 220, 230) 각각은, 훈련용 음향을 입력받고, 훈련용 음향에 대한 연산을 수행 하여 복수의 음향 이벤트 및 복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지할 수 있다. 또한 복수의 단일모델(210, 220, 230) 각각은, 복수의 음향 이벤트 및 복수의 방향을 탐지한 결과인 복수의 트랙쌍을 출력할 수 있다. 예를 들어 세개의 트랙쌍을 생성하는 것으로 설정되는 경우, 제1 단일모델은 제1-1 이벤트 트랙 및 제1-1 방향 트랙을 포함하는 제1-1 트랙쌍, 제1-2 이벤트 트랙 및 제1-2 방향 트랙을 포함하는 제1-2 트랙쌍, 제1-3이벤트 트랙 및 제1-3 방향 트랙을 포함하는 제1-3 트랙쌍을 생성할 수 있다. 제2 단일모델은 제2-1 이벤 트 트랙 및 제2-1 방향 트랙을 포함하는 제2-1 트랙쌍, 제2-2 이벤트 트랙 및 제2-2 방향 트랙을 포함하는 제2- 2 트랙쌍, 제2-3 이벤트 트랙 및 제2-3 방향 트랙을 포함하는 제2-3 트랙쌍을 생성할 수 있다. 제3 단일모델 은 제3-1 이벤트 트랙 및 제3-1 방향 트랙을 포함하는 제3-1 트랙쌍, 제3-2 이벤트 트랙 및 제3-2 방향 트 랙을 포함하는 제3-2 트랙쌍, 제3-3 이벤트 트랙 및 제3-3 방향 트랙을 포함하는 제3-3 트랙쌍을 생성할 수 있 다. 복수의 단일모델(210, 220, 230) 각각은, CNN부, RNN부 및 분류부를 포함할 수 있다. 본 명세서에서의 용어 \"특징(feature)\"은, \"특징 벡터\"라는 용어와 혼용되어 사용될 수 있다. CNN부는 훈련용 음향으로부터 음향 이벤트 및 방향과 관련된 특징을 추출할 수 있다. CNN부에는 컨볼루션 신경 망(Convolutional Neural Network, CNN), 2d CNN 기반의 ResNet, MobileNet, ConvNext 등이 사용될 수 있으나 이에 한정되지 않으며, 음향 또는 전처리되어 시각화된 음향으로부터 음향 이벤트 및 방향과 관련된 특징을 추 출할 수 있는 다양한 네트워크 및 기법이 사용될 수 있다. RNN부는 CNN부에서 추출된 특징의 시계열 데이터로부터 음향 이벤트의 발생 확률 및 음향 이벤트의 방향과 관련 된 특징을 추출할 수 있다. RNN부에는 순환 신경망(Recurrent Neural Network, RNN), GRU(Gated Recurrent Unit), LSTM(Long Short-Term Memory), bi-GRU, bi-LSTM, transformer, conformer 등이 사용될 수 있으나 이에 한정되지 않으며, 시계열 데이터로부터 특징을 검출할 수 있는 다양한 네트워크 및 기법이 사용될 수 있다. 분류부는 RNN부에서 추출된 특징을 이용하여 클래스 별 음향 이벤트의 발생 확률 및 음향 이벤트의 방향을 산출 하고, 클래스 별 음향 이벤트의 발생 확률 및 음향 이벤트의 방향을 포함하는 트랙쌍을 출력할 수 있다. 또한 분류부는 동시간에 복수의 음원에서 발생한 음향에 대하여, 클래스가 다른 음향 이벤트에 대해서는 중복을 허용 하여 하나의 트랙쌍에 표시하고, 클래스가 같은 음향 이벤트에 대해서는 다른 트랙쌍에 표시할 수 있다. 분류부는 FC (Fully Connected) 레이어를 포함할 수 있으나 이에 한정되지 않으며, RNN부에서 추출된 특징을 이 용하여 클래스 별 음향 이벤트의 발생 확률 및 음향 이벤트의 방향을 산출할 수 있는 다양한 네트워크 및 기법 이 사용될 수 있다. 한편 복수의 단일모델(210, 220, 230)은 서로 다른 구조를 가질 수 있다. 이에 따라 복수의 단일모델(210, 220, 230)은, 동일한 훈련용 음향을 입력받음에도 불구하고, 서로 다른 결과(트랙쌍)을 출력할 수 있다. 앙상블 모델은 복수의 단일모델이 생성한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하도록 구성될 수 있다. 구체적으로 앙상블 모델은 복수의 단일모델이 생성한 트랙쌍들을 입력받고, 트랙쌍들에 대한 연산을 수행하여 최종 트랙쌍들을 출력할 수 있다. 앙상블 모델이 출력하는 최종 트랙쌍들의 개수는, 복수의 단일모델(210, 220, 230) 각각이 출력하는 트랙 쌍들의 개수와 동일할 수 있다. 예를 들어 제1 단일 모델이 세개의 트랙쌍을 출력하고, 제2 단일 모델 이 세개의 트랙쌍을 출력하고, 제3 단일 모델이 세개의 트랙쌍을 출력하는 경우, 앙상블 모델은 아홉개의 트랙쌍을 입력받을 수 있다. 또한 앙상블 모델은 아홉개의 트랙쌍에 대한 연산을 수행하여 세개 의 최종 트랙쌍을 출력할 수 있다. 도 6은 단일모델에 적용될 수 있는 멀티 태스크(multi-task) 구조의 다양한 예시를 도시한 도면이다. 복수의 단일모델(210, 220, 230)의 일부 또는 전부는, 멀티 태스크(multi-task) 러닝 기법의 적용을 위해 멀티 태스크(multi-task) 구조를 가질 수 있다. 멀티 태스크(multi-task) 기법이란, 독립적인 문제들(예를 들어, 학 생의 성적 추이 예측, 학생의 신장 추이 예측)을 풀기 위한 네트워크들을 독립적으로 구성하는 것과는 달리, 연 관된 문제들(예를 들어, 음성의 존재확률, 음성 품질향상 문제)을 풀기위한 네트워크들을 서로 연관시키는 것이다. 즉 제1 문제를 풀기 위한 제1 네트워크 및 제2 문제를 풀기위한 제2 네트워크가 존재하며, 제1 네트워크와 제2 네트워크는 레이어 또는 파라미터를 공유한다. 그리고 제1 네트워크는 제1 문제를 풀기 위한 힌트를 제2 네 트워크로부터 얻어 제1 문제를 해결하고, 제2 네트워크는 제2 문제를 풀기위한 힌트를 제1 네트워크로부터 얻어 제2 문제를 해결하는 방식으로, 두 문제를 모두 효과적으로 해결할 수 있다. 음향 이벤트의 탐지 및 음향 이벤트의 방향의 탐지가 독립된 구성들이나 기술들(예를 들어 인공지능을 이용한 음향 이벤트 탐지 기술, 신호처리 기반의 방향(위치) 탐지 기술)에 의해 독립적으로 수행되는 경우, 복수의 음 원에서 발생한 음향이 혼합되어 입력될 때 탐지된 음향 이벤트와 방향의 타겟 음향이 일치하지 않는 문제가 발 생한다. 따라서 본 발명에서는, 음향 이벤트의 탐지와 음향 이벤트의 방향의 탐지라는 두개의 문제를 해결하기위해, 복수의 단일모델(210, 220, 230)을 멀티 태스크(multi-task) 구조로 구성하고, 멀티 태스크(multi-task) 러닝 기법으로 복수의 단일모델(210, 220, 230)을 트레이닝할 수 있다. 멀티 태스크(multi-task) 구조는 크게 hard parameter sharing 구조(도 6a), soft parameter sharing 구조(도 6b), cross stitch 구조(도 6c)로 나뉜다. hard parameter sharing 구조(도 6a)는 하나의 네트워크가 음향 이벤트 탐지 및 음향 이벤트의 방향 탐지를 위 해 공유되는 구조를 의미한다. soft parameter sharing 구조(도 6b)는 음향 이벤트 탐지를 위한 네트워크와 음 향 이벤트의 방향 탐지를 위한 네트워크가 파라미터를 공유하는 구조를 의미한다. cross stitch 구조(도 6c)는, 음향 이벤트 탐지를 위한 네트워크와 음향 이벤트의 방향 탐지를 위한 네트워크가 파라미터를 공유할 때, 파라 미터가 스티치(stitch) 네트워크 블록을 통과하면서 이동의 대상이 되는 네트워크에 적합하게 변환되는 구조를 의미한다. 도 7 및 도 8은 CNN부와 RNN부가 결합된 멀티 태스크(multi-task) 구조의 보다 구체적인 예시를 도시한 도면이 다. 음향 이벤트 탐지를 위한 네트워크를 제1 태스크 네트워크로, 음향 이벤트의 방향 탐지를 위한 네트워크를 제2 태스크 네트워크로 명칭하도록 한다. 단일 모델은 제1 태스크 네트워크 및 제2 태스크 네트워크를 포함할 수 있 다. 도 7을 참고하면 CNN부는 hard parameter sharing 구조, RNN부는 soft parameter sharing 구조로 구성될 수 있 다. 즉 제1 태스크 네트워크와 제2 태스크 네트워크는 하나의 CNN부를 공유할 수 있다. 또한 제 1 태스크 네트워크는 음향 이벤트 탐지를 위한 제1 RNN부를 포함하고, 제2 태스크 네트워크는 음향 이벤트의 방향 탐지를 위한 제2 RNN부를 포함하며, 제1 RNN부와 제2 RNN부는 파라미터를 공유할 수 있다. 도 8을 참고하면 CNN부와 RNN 부 모두 cross stitch multi-task 구조로 구성될 수 있다. 즉 제1 태스크 네트워 크는 음향 이벤트 탐지를 위한 제1 CNN부 및 제1 RNN부를 포함하고, 제2 태스크 네트워크는 음향 이 벤트의 방향 탐지를 위한 제2 CNN부 및 제2 RNN부를 포함할 수 있다. 또한, 제1 태스크 네트워크의 제1 CNN부는 제2 태스크 네트워크의 제2 CNN부와 변환된 파라미터(구체적으로, 스티치 네트워크 블록에 의해 변환된 파라미터)를 공유하고, 제2 태스크 네트워크의 제1 RNN부는 제2 태스크 네트워크의 제2 RNN부 와 변환된 파라미터(구체적으로, 스티치 네트워크 블록에 의해 변환된 파라미터)를 공유할 수 있다. 한편 단일모델은 RNN에서 추출된 특징을 이용하여 트랙쌍을 생성하는 트랙 생성부(760, 870)를 더 포함할 수 있 다. 트랙 생성부는 복수의 스티치(stitch) 네트워크 블록을 포함하고, 제1 태스크 네트워크의 제1 RNN부에서 추 출된 특징 및 제2 태스크 네트워크의 제2 RNN부에서 추출된 특징을 이용하여 이벤트 트랙 및 방향 트랙으로 구 성되는 트랙쌍(Track 1, Track2, Track3)을 생성할 수 있다. 도 9는 본 발명에 따른, 음향 이벤트 및 방향 탐지 모델의 트레이닝 방법을 설명하기 위한 순서도이다. 본 발명에 따른 음향 이벤트 및 방향 탐지 모델의 트레이닝 방법은, 복수의 단일모델 각각이 복수의 음원에서 발생하여 동시간에 수집된 음향으로부터 복수의 음향 이벤트 및 상기 복수의 음향 이벤트에 각각 대응하는 복수 의 방향을 탐지하여 복수의 트랙쌍을 출력하도록, 복수의 단일모델을 트레이닝하는 단계(S910), 및, 앙상블 모 델이 복수의 단일모델이 생성한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하도록, 앙상블 모델을 트레이닝하는 단계(S930)를 포함할 수 있다. S910를 설명하기 전, 훈련용 음향을 전처리하는 방법에 대해서 먼저 설명한다. S910 이전에, 제어부는 훈련용 음향을 전처리할 수 있다. 구체적으로 제어부는 수집된 음향을 전처리 하여, 음향 이벤트를 탐지하는데 사용되는 이벤트 데이터 및 음향 이벤트의 방향을 탐지하는데 사용되는 방향 데이터를 생성할 수 있다. 이벤트 데이터는, 주파수 별 신호의 세기, 파형, 진폭 등, 음향 이벤트와 관련된 특징이 시각화되어 나타나는 데이터로, 예를 들어 스펙토그램일 수 있다. 이벤트 데이터를 생성하기 위해 사용되는 기법으로는 spectrogram, log-Mel spectrogram 등이 사용될 수 있으나 이에 한정되지 않으며, CNN부가 음향 이벤트와 관련된 특징을 추출 할 수 있도록 음향을 시각화할 수 있는 다양한 기법이 사용될 수 있다. 또한 이벤트 데이터는, 복수의 마이크에 각각 대응하는 복수의 이벤트 채널 데이터를 포함할 수 있다. 예를 들 어 멀티채널 마이크가 네개의 마이크로 구성되는 경우, 이벤트 데이터는 제1 마이크에서 수신된 음향을 시각화 하여 나타내는 제1 이벤트 채널 데이터, 제2 마이크에서 수신된 음향을 시각화하여 나타내는 제2 이벤트 채널 데이터, 제3 마이크에서 수신된 음향을 시각화하여 나타내는 제3 이벤트 채널 데이터 및 제4 마이크에서 수신된 음향을 시각화하여 나타내는 제4 이벤트 채널 데이터를 포함할 수 있다. 또한 이벤트 데이터는 소정의 시간 구간의 음향 이벤트와 관련된 특징을 시각화하여 나타낸 시계열 데이터일 수 있다. 예를 들어 이벤트 데이터는 0초 내지 5초의 시간 구간에 제1 마이크에서 수신된 음향을 시각화하여 나타 내는 제1 이벤트 채널 데이터, 동일 시간 구간(0초 내지 5초)에 제2 마이크에서 수신된 음향을 시각화하여 나타 내는 제2 이벤트 채널 데이터, 및 동일 시간 구간(0초 내지 5초)에 제3 마이크에서 수신된 음향을 시각화하여 나타내는 제3 이벤트 채널 데이터, 및, 동일 시간 구간(0초 내지 5초)에 제4 마이크에서 수신된 음향을 시각화 하여 나타내는 제4 이벤트 채널 데이터를 포함할 수 있다. 방향 데이터는, 서로 다른 위치에 배치되는 다수의 마이크가 동일한 음향 신호를 수신하는 경우에 발생하는 주 파수별 위상차 등, 음향 이벤트의 방향과 관련된 특징이 시각화되어 나타나는 데이터일 수 있다. 방향 데이터를 생성하기 위해 사용되는 기법으로는 intensity vector (IV), inter-channel phase difference (IPD), normalized IPD 등이 사용될 수 있으나 이에 한정되지 않으며, CNN부가 음향 이벤트의 방향과 관련된 특징을 추 출할 수 있도록 음향을 시각화할 수 있는 다양한 기법이 사용될 수 있다. 또한 방향 데이터는, 복수의 마이크쌍에 각각 대응하는 복수의 방향 채널 데이터를 포함할 수 있다. 예를 들어 멀티채널 마이크가 네개의 마이크로 구성되는 경우, 방향 데이터는 제1-제2 마이크쌍의 위상차를 시각화하여 나 타내는 제1 방향 채널 데이터, 제1-제3 마이크쌍의 위상차를 시각화하여 나타내는 제2 방향 채널 데이터, 제1- 제4 마이크쌍의 위상차를 시각화하여 나타내는 제3 방향 채널 데이터를 포함할 수 있다, 또한 이벤트 데이터는 소정의 시간 구간의 음향 이벤트의 방향과 관련된 특징을 시각화하여 나타낸 시계열 데이 터일 수 있다. 예를 들어 이벤트 데이터는 0초 내지 5초의 시간 구간에 제1-제2 마이크쌍의 위상차를 시각화하 여 나타내는 제1 방향 채널 데이터, 동일 시간 구간(0초 내지 5초)에 제1-제3 마이크쌍의 위상차를 시각화하여 나타내는 제2 방향 채널 데이터, 및 동일 시간 구간(0초 내지 5초)에 제1-제4 마이크쌍의 위상차를 시각화하여 나타내는 제3 방향 채널 데이터를 포함할 수 있다. 다음으로, 제어부는 복수의 단일모델을 트레이닝할 수 있다(S910). 구체적으로 제어부는 지도 학습 알고리즘에 기반하여 훈련용 음향(더욱 구체적으로는, 전처리된 음향)을 트레이닝 데이터로 사용하여 복수의 단 일모델을 트레이닝할 수 있다. 훈련용 음향은 이벤트 데이터 및 방향 데이터를 포함할 수 있다. 단일모델들은 서로 다른 멀티 태스크 구조를 가질 수 있으므로, 제어부는 단일모델의 멀티 태스크 구조에 상응하도록 훈련용 음향을 제공할 수 있다. 예를 들어 도 7의 멀티 태스그 구조에서는 제1 태스크 네트워크 와 제2 태스크 네트워크가 하나의 CNN부를 공유하므로, 제어부는 해당 멀티 태스그 구조를 가지는 단일모델의 CNN부에 복수의 이벤트 채널 데이터 및 복수의 방향 채널 데이터를 입력할 수 있다. 반 면 도 8의 멀티 태스그 구조를 가지는 단일모델에 대하여, 제어부는 단일모델의 제1 태스크 네트워크(83 0)의 CNN부에 복수의 이벤트 채널 데이터를 입력하고, 단일모델의 제2 태스크 네트워크의 CNN부에 복수의 이벤트 채널 데이터 및 복수의 방향 채널 데이터를 입력할 수 있다. 훈련용 음향이 입력되면, 제1 태스크 네트워크의 CNN부는, 자신의 모델 파라미터(가중치, 편향 등)에 기반하여, 훈련용 음향으로부터 음향 이벤트와 관련된 특징을 추출하여 제1 태스크 네트워크의 RNN부로 출력할 수 있다. 또한 제2 태스크 네트워크의 CNN부는, 자신의 모델 파라미터(가중치, 편향 등)에 기반하여, 훈련용 음향으로부 터 음향 이벤트의 방향과 관련된 특징을 추출하여 제2 태스크 네트워크의 RNN부로 출력할 수 있다. 또한 앞서 설명한 바와 같이, Hard parameter sharing, soft parameter sharing, Soft parameter sharing 등에 의해 네트 워크, 파라미터, 변환된 파라미터 등이 제1 태스크 네트워크 및 제2 태스크 네트워크 사이에서 공유될 수 있다. 다음으로, 제1 태스크 네트워크의 RNN부는, 자신의 모델 파라미터(가중치, 편향 등)에 기반하여, 제1 태스크 네 트워크의 CNN부에서 추출된 특징의 시계열 데이터로부터 음향 이벤트와 관련된 특징을 추출할 수 있다. 또한 제 2 태스크 네트워크의 RNN부는, 자신의 모델 파라미터(가중치, 편향 등)에 기반하여, 제2 태스크 네트워크의 CNN 부에서 추출된 특징의 시계열 데이터로부터 음향 이벤트의 방향과 관련된 특징을 추출할 수 있다. 또한 앞서 설 명한 바와 같이, Hard parameter sharing, soft parameter sharing, Soft parameter sharing 등에 의해 네트워 크, 파라미터, 변환된 파라미터 등이 제2 태스크 네트워크 및 제2 태스크 네트워크 사이에서 공유될 수 있다. 다음으로, 단일모델의 트랙 생성부는, 자신의 모델 파라미터(가중치, 편향 등)에 기반하여, 제1 태스크 네트워 크의 RNN부에서 추출된 특징 및 제2 태스크 네트워크의 RNN부에서 추출된 특징을 이용하여 복수의 트랙쌍을 생 성하여 출력할 수 있다. 복수의 트랙쌍 각각은, 음향 이벤트의 발생 확률을 나타내는 이벤트 트랙 및 음향 이벤 트의 방향을 나타내는 방향 트랙을 포함할 수 있다. 이와 관련해서는 도 10을 참고하여 구체적으로 설명한다. 도 10은 하나의 단일모델이 출력한 복수의 트랙쌍을 설명하기 위한 도면이다. 단일모델은 음향 이벤트의 발생 확률을 나타내는 제1 이벤트 트랙을 출력하였으며, 제1 이벤트 트랙 에 포함되는 음향 이벤트의 방향을 나타내는 제1 방향 트랙을 출력하였다. 제1 방향 트랙은 제1 이벤트 트랙에 기록된 음향 이벤트의 방향을 나타내기 때문에, 제1 이벤트 트랙과 제1 방향 트 랙은 제1 트랙쌍을 구성할 수 있다. 이러한 방식으로, 단일모델은 n개의 트랙쌍을 출력할 수 있다. 또한 단일모델은, 동시간에 복수의 음원에서 생성된 음향에 대하여, 클래스가 다른 음향 이벤트를 하나의 트랙 쌍에 표시할 수 있다. 예를 들어 도 10의 제1 이벤트 트랙을 참고하면 a 클래스에 해당하는 음향 이벤트 의 발생 확률이 0.4로 표시되어 있으며, 도 10의 제1 방향 트랙을 참고하면 a 클래스에 해당하는 음향 이 벤트의 방향이 (0.3, 0.5, 0)으로 표시되어 있다. 또한 도 10의 제1 이벤트 트랙을 참고하면 e 클래스에 해당하는 음향 이벤트의 발생 확률이 0.7로 표시되어 있으며, 도 10의 제1 방향 트랙을 참고하면 e 클래 스에 해당하는 음향 이벤트의 방향이 (0.1, 0, 0.4)로 표시되어 있다. 즉 다른 클래스에 속하는 복수의 음향 이 벤트 및 그 방향이 하나의 트랙쌍에 표시될 수 있다. 여기서 클래스는, 남자의 목소리, 여자의 목소리, 고양이 의 울음 소리, 사람의 걸음 소리, 노크 소리, 개의 걸음 소리 등으로 다양하게 설정될 수 있다. 또한 단일모델은, 동시간에 복수의 음원에서 생성된 음향에 대하여, 클래스가 같은 음향 이벤트들을 다른 트랙 쌍들에 표시할 수 있다. 예를 들어 제1 트랙쌍에는 a 클래스에 해당하는 하나의 음향 이벤트가 표시되어 있다. 그리고 a 클래스에 해당하는 다른 음향 이벤트가 추가로 탐지되면, 단일모델은 다른 음향 이벤트를 제2 트랙쌍 에 표시할 수 있다. 이 경우 제2 트랙쌍의 제2 이벤트 트랙은 a 클래스에 해당하는 다른 음향 이벤트의 발생 확 률을 포함하고, 제2 트랙쌍의 제2 방향 트랙은 a 클래스에 해당하는 다른 음향 이벤트의 방향을 포함할 수 있다. 즉 동일한 클래스에 속하는 둘 이상의 음원(예를 들어 두명의 남자)이 존재하는 경우, 둘 이상의 음원에 대한 음향 이벤트 및 음향 이벤트의 방향은 복수의 트랙쌍 중 서로 다른 트랙쌍에 기록될 수 있다. 한편 제어부는 단일모델이 출력한 복수의 트랙쌍에 대한 후처리를 수행할 수도 있다. 이 경우 제어부(12 0)는 유사도가 임계값보다 큰 두개의 트랙을 하나로 통합할 수 있다. 유사도는 음향 이벤트의 유사도 및 방향의 유사도 중 적어도 하나에 기초하여 산출될 수 있다. 다음으로 제어부는 지도 학습 알고리즘에 기반하여, 단일모델이 출력한 복수의 트랙쌍 및 레이블링 데이터 를 이용하여 단일모델의 모델 파라미터를 업데이트할 수 있다. 여기서 레이블링 데이터는 단일모델이 훈련용 음 향을 이용하여 추론해야 하는 정답을 의미할 수 있다. 단일모델이 출력한 결과가 복수의(N개의) 트랙쌍인 것과 마찬가지로, 레이블링 데이터도 동일한 개수의(N개의) 정답 트랙쌍을 포함할 수 있다. 또한 정답 트랙쌍은 실제로 음원이 존재하였는지를 나타내는 이벤트 트랙 및 실 제로 존재한 음원의 방향을 나타내는 방향 트랙을 포함할 수 있다. 한편 복수의 단일모델 각각은, 특정 음원의 음향 이벤트 및 음향 이벤트의 방향을 복수의 트랙쌍 중 무작위로 선정된 어느 하나의 트랙쌍에 기록할 수 있다. 트랙쌍의 개수가 세개이고, 복수의 음원이 남자 1 및 남자 2를 포함한다고 가정하여 설명한다. 단일모델은 남자 1의 음향 이벤트 및 음향 이벤트의 방향을 복수의 트랙쌍 중 무작위로 선정된 어느 하나의 트 랙쌍에 기록할 수 있다. 즉 트랙쌍이 무작위로 선정되므로, 남자 1의 음향 이벤트 및 음향 이벤트의 방향은 제1 트랙쌍에도, 제2 트랙쌍에도, 제3 트랙쌍에도 기록될 수 있으며, 어느 트랙쌍에 기록되었는지는 알 수 없다. 마 찬가지로 남자 2의 음향 이벤트 및 음향 이벤트의 방향은 제1 트랙쌍에도, 제2 트랙쌍에도, 제3 트랙쌍에도 기 록될 수 있으며, 어느 트랙쌍에 기록되었는지는 알 수 없다. 따라서, 단일모델이 훈련용 음향에 기초하여 N개의 트랙쌍을 생성하면, 제어부는 단일모델이 생성한 N개의 트랙쌍 및 N개의 정답 트랙쌍을 비교하여 각 트랙쌍에 가장 가까운 정답 트랙쌍을 선정할 수 있다. 또한 제어부 는 각 트랙쌍과 선정된 정답 트랙쌍의 차이를 이용하여 단일모델을 트레이닝할 수 있다. 구체적으로 제어부는, 단일모델이 출력한 제1 트랙쌍과 세개의 정답 트랙쌍 간의 손실값을 산출할 수 있다. 즉 제어부는, 제1 트랙쌍과 제1 정답 트랙쌍 간의 제1 손실값, 제1 트랙쌍과 제2 정답 트랙쌍 간의 제2 손실값, 제1 트랙쌍과 제3 정답 트랙쌍 간의 제3 손실값을 산출할 수 있다. 그리고 나서 제어부는 제1내지 제3 손실값 중 가장 작은 제1 손실값을 선정하고, 제1 손실값을 이용하여 단일모델의 모델 파라미터를 업 데이트할 수 있다. 또한 제어부는 단일모델이 출력한 제2 트랙쌍과 세개의 정답 트랙쌍 간의 손실값을 산출할 수 있다. 즉 제 어부는, 제2 트랙쌍과 제1 정답 트랙쌍 간의 제4 손실값, 제2 트랙쌍과 제2 정답 트랙쌍 간의 제5 손실값, 제2 트랙쌍과 제3 정답 트랙쌍 간의 제6 손실값을 산출할 수 있다. 그리고 나서 제어부는 제4 내지 제6 손 실값 중 가장 작은 제6 손실값을 선정하고, 제6 손실값을 이용하여 단일모델의 모델 파라미터를 업데이트할 수 있다. 제어부는 단일모델이 출력한 제3 트랙쌍에 대해서도 동일한 동작을 수행할 수 있다. 한편 제어부는 단일모델이 출력한 복수의 트랙쌍과 복수의 정답 트랙쌍을 이용하여 손실값을 산출하고, 산 출된 손실값을 이용하여 단일모델의 모델 파라미터를 업데이트할 수 있다. 이는 다음과 같은 수학식으로 표현될 수 있다. 수학식 1"}
{"patent_id": "10-2023-0010295", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, n은 트랙쌍의 넘버, N은 트랙쌍의 개수, c는 음향 이벤트가 속하는 클래스, C는 클래스의 개수, t는 시 간, 는 레이블링 데이터(정답 트랙쌍), 는 단일모델이 출력한 결과(단일모델이 출력한 트랙쌍)을 의미한다. 또한 LF는 손실함수를 의미하며, 손실함수에는 MSE (Mean Squared Error), MAE (Mean Absolute Error), BCE (Binary Cross Entropy) 등이 사용될 수 있으나 이에 한정되지는 않는다. 수학식 1을 참고하면, 제어부는 클래스 별로 그리고 트랙쌍 별로 단일모델이 출력한 트랙쌍을 정답 트랙쌍 과 비교하고, 이들의 차이를 합산하여 손실값( )을 산출할 수 있다. 그리고 제어부는 손실값 ( )을 이용하여 단일모델의 모델 파라미터를 업데이트할 수 있다. 한편 앞서 설명한 바와 같이, 훈련용 음향은 소정의 시간 구간의 음향을 포함하는 시계열 데이터일 수 있다. 따 라서 제어부는 소정의 시간 구간 동안의 손실값들을 합산하여 시간 구간의 손실값을 산출하고, 시간 구간 의 손실값을 이용하여 단일모델의 모델 파라미터를 업데이트할 수 있다. 이는 다음과 같은 수학식으로 표현될 수 있다. 수학식 2"}
{"patent_id": "10-2023-0010295", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "( : 시간 구간의 손실값, t: 시간, : 해당 시간의 손실값, T: 시간 구간) 한편 두 개의 태스크(음향 이벤트 탐지, 음향 이벤트의 방향 탐지)가 존재하기 때문에, 제어부는 멀티 태 스크 러닝 기법을 사용하여 모델을 구성하는 제1 태스크 네트워크 및 제2 태스크 네트워크를 트레이닝할 수 있다. 이 경우 제어부는 음향 이벤트 탐지라는 제1 태스크에 상응하는 이벤트 손실값 및 음향 이벤트의 방향 탐지라는 제2 태스크에 상응하는 방향 손실값을 별도로 산출하고, 태스크 손실값 및 방향 손실값을 합산하 여 전체 손실값을 산출할 수 있다. 이는 다음과 같은 수학식으로 표현될 수 있다.수학식 3"}
{"patent_id": "10-2023-0010295", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "(L: 전체 손실값, LSED: 이벤트 손실값, LDOA: 방향 손실값), : 0 내지 1 사이의 상수인 가중치) 구체적으로 단일모델은 훈련용 음향에 기초하여 N개의 트랙쌍을 생성하였다. 그리고 제어부는 단일모델에 의해 생성된 N개의 트랙쌍의 이벤트 트랙 및 N개의 정답 트랙쌍의 이벤트 트랙을 비교하여 이벤트 손실값(LSED) 을 산출할 수 있다. 또한 제어부는 단일모델에 의해 생성된 N개의 트랙쌍의 방향 트랙 및 N개의 정답 트랙 쌍의 방향 트랙을 비교하여 방향 손실값(LDOA)을 산출할 수 있다. 또한 제어부는 이벤트 손실값(LSED) 및 방 향 손실값(LDOA)을 합산한 전체 손실값(L)을 이용하여 단일모델을 트레이닝할 수 있다. 즉 제어부는 전체 손실값(L)이 최소화 되는 방향으로 단일모델의 모델 파라미터를 업데이트할 수 있다. 또한 제어부는 도 9 내지 도 10에서 설명한 트레이닝을 반복함으로써 단일모델의 모델 파라미터를 최적화 할 수 있다. 또한 복수의 단일모델(210, 220, 230)은 각각 도 9 내지 도 10에서 설명한 방법에 의해 트레이닝될 수 있다. 한편 복수의 단일모델(210, 220, 230)은 서로 다른 구조를 가지거나, 서로 다른 트레이닝 데이터 셋에 의해 트 레이닝될 수 있다. 이에 따라 복수의 단일모델(210, 220, 230)은, 동일한 훈련용 음향을 입력받음에도 불구하고, 서로 다른 결과(트랙쌍)을 출력할 수 있다. 한편 복수의 단일모델(210, 220, 230) 각각에 대한 트레이닝이 완료되면, 제어부는 앙상블 모델이 복 수의 단일모델이 생성한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하도록 앙상블 모델을 트 레이닝할 수 있다(S930). 이와 관련해서는 도 11을 참고하여 설명한다. 도 11은 본 발명에 따른, 앙상블 모델의 일 실시예를 도시한 도면이다. 일반적인 앙상블 기법에서는, 다수의 결과들을 평균 또는 가중평균하여 최종 결과를 산출한다. 다만 본 발명에 서와 같이 단일모델이 출력한 결과가 복수의 트랙쌍인 경우, 단일모델들에서 도출된 결과들을 평균 또는 가중평 균하는 일반적인 앙상블 기법을 적용할 수 없다. 이와 관련해서는 표 1을 참고하여 설명한다. 표 1 첫번째 트랙쌍 두번째 트랙쌍 세번째 트랙쌍 제1 단일모델a 클래스의 음향 이벤트 확률: 0.4 음향 이벤트의 방향: 10 도a 클래스의 음향 이벤트 확률: 0.4 음향 이벤트의 방향: 28 도c 클래스의 음향 이벤트 확률: 0.6 음향 이벤트의 방향: 18 도 제2 단일모델c 클래스의 음향 이벤트 확률: 0.5 음향 이벤트의 방향: -10도a 클래스의 음향 이벤트 확률: 0.6 음향 이벤트의 방향: 8 도a 클래스의 음향 이벤트 확률: 0.4 음향 이벤트의 방향: 32 도 제3 단일모델d 클래스의 음향 이벤트 확률: 0.4 음향 이벤트의 방향: 10 도e 클래스의 음향 이벤트 확률: 0.6 음향 이벤트의 방향: 30 도f 클래스의 음향 이벤트 확률: 0.4 음향 이벤트의 방향: - 20도 먼저, 앞서 설명한 바와 같이 복수의 단일모델 각각은 특정 음원의 음향 이벤트 및 음향 이벤트의 방향을 복수 의 트랙쌍 중 무작위로 선정된 어느 하나의 트랙쌍에 기록하기 때문에, 단일 모델이 출력한 트랙쌍들의 순서가 동일하다는 보장이 없다. 예를 들어 제1 단일 모델은 남자 1의 음향 이벤트 및 음향 이벤트의 방향을 첫번째 트 랙쌍에 기록하는 반면, 제2 단일 모델은 남자 1의 음향 이벤트 및 음향 이벤트의 방향을 두번째 트랙쌍에 기록 할 수 있다. 따라서 미리 특정된 결과물들을 평균 또는 가중평균 하는 일반적인 앙상블 기법을 사용할 수 없다.또한 제1 단일모델의 세번째 트랙쌍은 c 클래스의 음향 이벤트를 기록하고 있으며, 제2 단일모델의 첫번째 트랙 쌍 역시 c 클래스의 음향 이벤트를 기록하고 있으나. 이들이 동일한 음원으로부터 발생한 것인지 알 수 없다. 예를 들어 제1 단일모델의 세번째 트랙쌍은 \"여자\" 클래스에 속하는 여자 1의 음향을 기록하는 반면, 제2 단일 모델의 첫번째 트랙쌍은 \"여자\" 클래스에 속하는 여자 2의 음향을 기록하는 것일 수 있다. 따라서 미리 특정된 결과물들을 평균 또는 가중평균 하는 일반적인 앙상블 기법을 사용할 수 없다. 또한 제1 단일모델 및 제2 단일모델은 a 클래스에 속하는 두개의 음향 이벤트와 c 클래스에 속하는 하나의 음향 이벤트를 기록하는 반면, 제3 단일 모델은 전혀 다른 음향 이벤트(d 클래스에 속하는 음향 이벤트, e 클래스에 속하는 음향 이벤트, f 클래스에 속하는 음향 이벤트)를 기록하고 있다. 이러한 상황에서 단순 평균 또는 가중 평균을 적용하는 경우, 성능이 하락하게 된다. 따라서 본 발명에서는 뉴럴 네트워크로 앙상블 모델를 구성하고, 복수의 단일모델에서 생성한 트랙쌍들을 트레이닝 데이터로 사용하여 앙상블 모델을 트레이닝 하는 방법을 사용한다. 먼저 앙상블 모델의 구조를 설명하면, 앙상블 모델을 복수의 단일모델에서 출력된 트랙쌍들로부터 음 향 이벤트의 발생 확률 및 음향 이벤트의 방향과 관련된 특징을 추출할 수 있다. 또한 앙상블 모델은 RNN부를 포함할 수 있다. RNN부에는 순환 신경망(Recurrent Neural Network, RNN), GRU(Gated Recurrent Unit), LSTM(Long Short-Term Memory), bi-GRU, bi-LSTM, transformer, conformer 등이 사용될 수 있으나 이에 한정되지 않으며, 시계열 데이터로부터 특징을 검출할 수 있는 다양한 네트워크 및 기법 이 사용될 수 있다. 이 경우 앙상블 모델의 RNN부는 복수의 단일모델에서 출력된 트랙쌍들의 시계열 데이 터로부터 음향 이벤트의 발생 확률 및 음향 이벤트의 방향과 관련된 특징을 추출할 수 있다. 또한 앙상블 모델 역시 멀티 태스크(multi-task) 구조를 가질 수 있다. 예를 들어 앙상블 모델은 복 수의 단일모델에서 출력된 트랙쌍들로부터 음향 이벤트와 관련된 특징을 추출하는 제1 태스크 네트워크 및 음향 이벤트의 방향과 관련된 특징을 추출하는 제2 태스크 네트워크를 포함할 수 있다. 또한 1 태스크 네트워크와 제2 태스크 네트워크는 네트워크, 파라미터 및 변환된 파라미터 중 적어도 하나를 공유 할 수 있다. 한편 앙상블 모델은 분류부를 포함하고, 분류부는 RNN부에서 추출된 특징을 이용하여 클래스 별 음향 이벤 트의 발생 확률 및 음향 이벤트의 방향을 산출하고, 클래스 별 음향 이벤트의 발생 확률 및 음향 이벤트의 방향 을 포함하는 트랙쌍을 출력할 수 있다. 또한 분류부는 동시간에 복수의 음원에서 생성된 음향에 대하여, 클래스 가 다른 음향 이벤트에 대해서는 중복을 허용하여 하나의 트랙쌍에 표시하고, 클래스가 같은 음향 이벤트에 대 해서는 다른 트랙쌍에 표시할 수 있다. 다음으로, 앙상블 모델을 트레이닝하는 방법에 대하여 설명한다. 앞선 단일 모델을 트레이닝하는 방법에 대한 설명은, 모순되지 않는 범위에서 앙상블 모델을 트레이닝 하는 방법에도 적용될 수 있다. 제어부는 앙상블 모델이 복수의 단일모델이 생성한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들 을 출력하도록 앙상블 모델을 트레이닝할 수 있다(S930). 먼저 제어부는 동일한 훈련용 음향을 복수의 단일모델(210, 220, 230)에 제공하여 복수의 단일모델(210, 220, 230)이 생성한 트랙쌍들을 획득할 수 있다. 구체적으로 동일한 훈련용 음향이 제공되면, 복수의 단일모델 (210, 220, 230) 각각은 자신의 구조 및 자신의 모델 파라미터에 기반하여 복수의 트랙쌍을 생성할 수 있다. 예 를 들어 제1 단일모델은 제1 내지 제3 트랙쌍을, 제2 단일모델은 제4 내지 제6 트랙쌍을, 제3 단일모 델은 제7 내지 제9 트랙쌍을 생성할 수 있다. 또한 각각의 트랙쌍은 이벤트 트랙 및 방향 트랙을 포함할 수 있다. 앞서 설명한 바와 같이 복수의 단일모델(210, 220, 230)은 서로 다른 구조를 가지거나, 서로 다른 트레이닝 데 이터 셋에 의해 트레이닝되었기 때문에, 복수의 단일모델(210, 220, 230)은 동일한 훈련용 음향을 입력받음에도 불구하고, 서로 다른 결과(트랙쌍)을 출력할 수 있다. 다음으로, 제어부는 복수의 단일모델(210, 220, 230)이 생성한 트랙쌍들(1191, 1192, 1193)을 앙상블 모 델에 제공할 수 있다. 복수의 단일모델의 개수가 M이고 복수의 단일모델 각각이 N개의 트랙쌍을 생성한 경 우, 제어부는 M*N개의 트랙쌍들을 앙상블 모델에 제공할 수 있다. 예를 들어 복수의 단일모델의 개수 가 3이고 복수의 단일모델 각각이 3개의 트랙쌍을 생성한 경우, 제어부는 9개의 트랙쌍들을 앙상블 모델 에 제공할 수 있다.또한 제어부가 앙상블 모델에 제공하는 데이터는, 복수의 단일모델(210, 220, 230)이 생성한 트랙쌍 들(1191, 1192, 1193)의 시계열 데이터일 수 있다. 즉 제어부는 소정의 시간 구간의 트랙쌍들(1191, 1192, 1193)을 포함하는 시계열 데이터를 앙상블 모델에 제공할 수 있다. 그리고나서 제어부는, 지도 학습 알고리즘에 기반하여, 복수의 단일모델이 생성한 트랙쌍들(1191, 1192, 1193)을 트레이닝 데이터로 사용하고 정답 트랙쌍들을 레이블링 데이터로 사용하여 앙상블 모델을 트레이닝할 수 있다. 구체적으로 복수의 단일모델(210, 220, 230)이 생성한 트랙쌍들(1191, 1192, 1193)이 앙상블 모델에 제공 되면, 앙상블 모델은 자신의 모델 파라미터(가중치, 편향 등)에 기반하여 복수의 단일모델(210, 220, 23 0)이 생성한 트랙쌍들(1191, 1192, 1193)로부터 특징을 추출할 수 있다. 또한 앙상블 모델은 자신의 모델 파라미터(가중치, 편향 등)에 기반하여 복수의 최종 트랙쌍을 생성하여 출력할 수 있다. 복수의 최종 트랙쌍 각 각은, 음향 이벤트의 발생 확률을 나타내는 이벤트 트랙 및 음향 이벤트의 방향을 나타내는 방향 트랙을 포함할 수 있다. 일 예로, 앙상블 모델은 복수의 단일모델이 생성한 N*M개의 트랙쌍들을 제공받아 연산함으로써 N개의 최종 트랙쌍을 출력할 수 있다. 예를 들어 복수의 단일모델의 개수가 3이고 복수의 단일모델 각각이 3개의 트랙쌍을 생성하여 총 9개의 트랙쌍들이 앙상블 모델에 제공된 경우, 제어부는 9개의 트랙쌍들에 대한 연산을 수행하여 3개의 최종 트랙쌍을 출력할 수 있다. 최종 트랙쌍의 형식은, 복수의 단일모델 각각이 출력하는 트랙쌍의 형식과 동일할 수 있다. 즉 도 10에서의 단 일모델이 출력한 트랙쌍에 대한 설명은, 앙상블 모델이 출력한 최종 트랙쌍에도 적용될 수 있다. 한편 레이블링 데이터는 지도 학습 알고리즘에서 정답을 나타내는 것으로, 복수의 최종 정답 트랙쌍을 포함할 수 있다. 또한 복수의 최종 정답 트랙쌍 각각은, 이벤트 트랙 및 방향 트랙을 포함할 수 있다. 또한 레이블링 데이터는, 복수의 단일모델(210, 220, 230)에 제공된 훈련용 음향에 상응하는 실제 상황을 나타 낼 수 있다. 예를 들어 남자 1이 0도 방향에, 남자 2가 40도 방향에, 여자 1이 170도 방향에 위치하는 경우, 복 수의 단일모델(210, 220, 230)에 제공되는 훈련용 음향은 0도 방향의 남자 1의 음향, 40도 방향의 남자 2의 음 향 및 170도 방향의 여자 1의 음향을 하나의 멀티 채널 마이크로 녹음한 것일 수 있다. 또한 레이블링 데이터는, 0도 방향에 남자 클래스에 속하는 음원이 존재한다는 정보, 40도 방향에 남자 클래스에 속하는 또다 른 음원이 존재한다는 정보, 170도 방향에 여자 클래스에 속하는 음원이 존재한다는 정보를 포함하는 복수의 최 종 정답 트랙쌍을 포함할 수 있다. 앙상블 모델이 최종 트랙쌍들을 출력하면, 제어부는 출력된 최종 트랙쌍들과 최종 정답 트랙쌍들의 차이를 이용하여 앙상블 모델을 트레이닝 할 수 있다. 구체적으로 제어부는 앙상블 모델이 출력 한 최종 트랙쌍들과 최종 정답 트랙쌍들 간의 손실값을 산출하고, 산출된 손실값을 이용하여 앙상블 모델(26 0)의 모델 파라미터를 업데이트할 수 있다. 이 경우 앙상블 모델이 N개의 최종 트랙쌍을 출력하면, 제어부는, 출력된 N개의 최종 트랙쌍과 N개의 최종 정답 트랙쌍을 비교하여 각 최종 트랙쌍에 가장 가까운 최종 정답 트랙쌍을 선정하고, 각 최종 트랙쌍과 선정된 최종 정답 트랙쌍의 차이를 이용하여 앙상블 모델을 트레이닝할 수 있다. 또한 앙상블 모델이 멀티 태스크 구조를 가지는 경우, 제어부는 멀티 태스크 러닝 기법을 사용하여 앙상블 모델을 구성하는 제1 태스크 네트워크 및 제2 태스크 네트워크를 트레이닝할 수 있다. 이 경우 제 어부는 음향 이벤트 탐지라는 제1 태스크에 상응하는 이벤트 손실값 및 음향 이벤트의 방향 탐지라는 제2 태스크에 상응하는 방향 손실값을 별도로 산출하고, 이벤트 손실값 및 방향 손실값을 합산하여 전체 손실값을 산출할 수 있다. 더욱 구체적으로 제어부는, 앙상블 모델에 의해 생성된 최종 트랙쌍의 이벤트 트랙 및 최종 정답 트랙쌍의 이벤트 트랙을 비교하여 이벤트 손실값을 산출하고, 앙상블 모델에 의해 생성된 최 종 트랙쌍의 방향 트랙 및 최종 정답 트랙쌍의 방향 트랙을 비교하여 방향 손실값을 산출하고, 이벤트 손실값 및 방향 손실값을 합산하여 전체 손실값을 산출할 수 있다. 또한 제어부는 이벤트 손실값 및 방향 손실값 에 가중치를 부여한 후 합산할 수 있다. 또한, 앞서 설명한 바와 같이, 트레이닝 데이터는 소정의 시간 구간의 트랙쌍들을 포함하는 시계열 데이터일 수 있다. 따라서 제어부는 소정의 시간 구간 동안의 손실값들을 합산하여 시간 구간의 손실값을 산출하고, 시 간 구간의 손실값을 이용하여 앙상블 모델의 모델 파라미터를 업데이트할 수 있다. 제어부는 손실값이 최소화 되는 방향으로 앙상블 모델의 모델 파라미터를 업데이트할 수 있다. 또한 도 11에서 설명한 트레이닝을 반복함으로써, 앙상블 모델의 모델 파라미터를 최적화할 수 있다. 한편 앙상블 모델의 트레이닝이 완료되면, 복수의 단일모델(210, 220, 230) 및 앙상블 모델을 포함하 는 음향 이벤트 및 방향 탐지 모델은 음향 이벤트 및 방향 탐지 장치에 탑재될 수 있다. 도 12는 본 발명에 따른, 음향 이벤트 및 방향 탐지 모델이 탑재된 음향 이벤트 및 방향 탐지 장치의 구성요소 를 설명하기 위한 블록도이다. 앞선 도 1 내지 11에서의 설명은, 모순되지 않는 범위에서 음향 이벤트 및 방향 탐지 장치에도 적용될 수 있다. 음향 이벤트 및 방향 탐지 장치는, 멀티채널 마이크, 제어부 및 메모리를 포함할 수 있다. 음향 이벤트 및 방향 탐지 장치는 복수의 마이크로 구성되는 멀티채널 마이크를 포함할 수 있다. 멀티채널 마이크가 세개의 마이크로 구성되는 경우 2차원의 방향 탐지가 가능하며, 다채널 마이크가 네개 의 마이크로 구성되는 경우 3차원의 방향 탐지가 가능하다. 멀티채널 마이크는 외부의 하나 이상의 음원에서 발생한 음향을 수신할 수 있다. 제어부는 음향 이벤트 및 방향 탐지 장치의 전반적인 동작을 제어할 수 있다. 용어 “제어부”는, “마이크로 프로세서”, “컨트롤러”, “마이크로 컨트롤러”, “프로세서” 등의 용어와 혼용되어 사용될 수 있다. 또한 제어부는 메모리로부터 음향 이벤트 및 방향 탐지 모델을 독출하여 실행할 수 있다. 이 에 따라, 아래에서 설명하는 음향 이벤트 및 방향 탐지 모델의 동작은, 제어부의 동작인 것으로 볼 수도 있다. 메모리는 음향 이벤트 및 방향 탐지 장치의 구동을 위한 프로그램, 명령어 및 기타 데이터를 저장 할 수 있다. 또한 메모리는 트레이닝이 완료된 음향 이벤트 및 방향 탐지 모델을 저장할 수 있다. 한편 음향 이벤트 및 방향 탐지 장치는 출력부(미도시)를 더 포함하고, 제어부 또는 음향 이벤트 및 방향 탐지 모델의 처리 결과를 외부로 출력할 수 있다. 이를 위해 출력부(미도시)는 외부 장치와 통신하기 위한 통신모듈 및 디스플레이 모듈 중 적어도 하나를 포함할 수 있다. 멀티채널 마이크가 외부의 복수의 음원에서 발생한 음향을 수신하면, 제어부는 복수의 음원에서 발 생하여 동시간에 수집된 음향을 음향 이벤트 및 방향 탐지 모델에 제공할 수 있다. 이 경우, 제어부 는 수집된 음향을 전처리하고, 전처리된 음향을 음향 이벤트 및 방향 탐지 모델에 제공할 수 있다. 이 경우 음향 이벤트 및 방향 탐지 모델은 앞서 도 1 내지 도 11에서 설명한 동작을 수행할 수 있다. 구체적으로 복수의 단일 모델(210, 220, 230) 각각은, 복수의 음원에서 발생하여 동시간에 수집된 음향으로부터 복수의 음향 이벤트 및 복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지하여 복수의 트랙쌍을 출력할 수 있다. 여기서 복수의 트랙쌍 각각은, 음향 이벤트의 발생 확률을 나타내는 이벤트 트랙 및 음향 이벤트의 방 향을 나타내는 방향 트랙을 포함할 수 있다. 또한 앙상블 모델은 복수의 단일모델이 생성한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력할 수 있다. 최종 트랙쌍들 각각은, 음향 이벤트의 발생 확률을 나타내는 이벤트 트랙 및 음향 이벤트의 방향을 나 타내는 방향 트랙을 포함할 수 있다. 한편 앙상블 모델이 최종 트랙쌍들을 출력하면, 제어부는 최종 트랙쌍들에 상응하는 탐지 결과를 출 력부(미도시)를 통해 출력할 수 있다. 예를 들어 제어부는 남자 클래스에 속하는 음원 1이 10도 방향에, 남자 클래스에 속하는 음원 2가 160도 방향에, 여자 클래스에 속하는 음원 3이 -40도 방향이 있음을 출력할 수 있다. 이와 같이 본 발명에 따르면, 음향 이벤트의 발생 확률과 이벤트의 방향을 별도로 분리하여 탐지하되 이 과정에 서 트랙의 개념을 도입함으로써, 동시간에 동일 클래스에 속하는 복수의 이벤트가 발생하더라도 이벤트의 발생 여부 및 그 방향을 정확하게 탐지할 수 있는 장점이 있다. 또한 음향 이벤트의 발생 확률과 이벤트의 방향을 별 도로 분리하여 탐지하기 위한 멀티 태스크 구조에 트랙의 개념을 결합함으로써, 이벤트의 발생 여부 및 그 방향을 더욱 정확하게 탐지할 수 있다. 복수의 단일모델(210, 220, 230)은 서로 다른 구조를 가지거나 서로 다른 트레이닝 데이터 셋에 의해 트레이닝 되기 때문에 특성 및 장점이 서로 상이하다. 그리고 본 발명에 따르면, 앙상블 모델을 이용함으로써, 복수의 단 일모델의 특성 및 장점을 혼합하여, 최적의 결과를 도출해낼 수 있는 장점이 있다. 또한 이 과정에서, 뉴럴 네 트워크로 앙상블 모델를 구성하고 복수의 단일모델에서 생성한 트랙쌍들을 트레이닝 데이터로 사용하여 앙상블 모델을 트레이닝 함으로써, 복수의 단일모델의 출력이 복수의 트랙쌍임에도 불구하고 이들을 이용하여 최적의 결과를 도출해낼 수 있다. 한편 음향 이벤트 및 방향 탐지 방법은 음향 이벤트 및 방향 탐지 장치에 수행될 수 있다. 음향 이벤트 및 방향 탐지 방법은 복수의 단일모델 각각이, 복수의 음원에서 발생하여 동시간에 수집된 음향으로부터 복수의 음향 이벤트 및 상기 복수의 음향 이벤트에 각각 대응하는 복수의 방향을 탐지하여 복수의 트랙쌍을 출력하는 단계, 및, 앙상블 모델이, 상기 복수의 단일모델이 출력한 트랙쌍들을 제공받아 연산함으로써 최종 트랙쌍들을 출력하는 단계를 포함할 수 있다. 또한 음향 이벤트 및 방향 탐지 방법을 수행하기 위한 컴퓨터 프로그램은 컴 퓨터가 읽을 수 있는 매체에 저장될 수 있다. 전술한 본 발명은, 프로그램이 기록된 매체에 컴퓨터가 읽을 수 있는 코드로서 구현하는 것이 가능하다. 컴퓨터 가 읽을 수 있는 매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 컴퓨터가 읽을 수 있는 매체의 예로는, HDD(Hard Disk Drive), SSD(Solid State Disk), SDD(Silicon Disk Drive), ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있다. 또한, 상기 컴 퓨터는 제어부를 포함할 수도 있다. 따라서, 상기의 상세한 설명은 모든 면에서 제한적으로 해석되어서는 아니 되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적 해석에 의해 결정되어야 하 고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다."}
{"patent_id": "10-2023-0010295", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 복수의 음원에서 발생한 음향이 입력되는 상황을 도시한 도면이다. 도 2는 음향 이벤트 탐지 결과 및 방향 탐지 결과를 정의하는 일반적인 방법을 설명하기 위한 도면이다. 도 3은 트랙을 이용하여 탐지 결과를 정의하는 일반적인 방법을 설명하기 위한 도면이다. 도 4는 본 발명에 따른, 음향 이벤트 및 방향 탐지 모델의 트레이닝 장치의 구성요소를 설명하기 위한 도면이다.도 5는 본 발명에 따른 음향 이벤트 및 방향 탐지 모델의 구조를 설명하기 위한 도면이다. 도 6은 단일모델에 적용될 수 있는 멀티 태스크(multi-task) 구조의 다양한 예시를 도시한 도면이다. 도 7 및 도 8은 CNN부와 RNN부가 결합된 멀티 태스크(multi-task) 구조의 보다 구체적인 예시를 도시한 도면이 다. 도 9는 본 발명에 따른, 음향 이벤트 및 방향 탐지 모델의 트레이닝 방법을 설명하기 위한 순서도이다. 도 10은 하나의 단일모델이 출력한 복수의 트랙쌍을 설명하기 위한 도면이다. 도 11은 본 발명에 따른, 앙상블 모델의 일 실시예를 도시한 도면이다. 도 12는 본 발명에 따른, 음향 이벤트 및 방향 탐지 모델이 탑재된 음향 이벤트 및 방향 탐지 장치의 구성요소 를 설명하기 위한 블록도이다."}
