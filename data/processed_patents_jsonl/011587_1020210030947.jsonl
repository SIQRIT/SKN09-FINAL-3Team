{"patent_id": "10-2021-0030947", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0067463", "출원번호": "10-2021-0030947", "발명의 명칭": "전자 장치 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "박승호"}}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,디스플레이;학습된 인공 지능 모델이 저장된 메모리; 및상기 학습된 인공 지능 모델을 이용하여 임계 해상도 미만의 저해상도 입력 이미지로부터 상기 임계 해상도 이상의 고해상도 출력 이미지를 획득하고 상기 획득된 고해상도 이미지를 표시하도록 상기 디스플레이를 제어하는프로세서를 포함하고,상기 프로세서는,상기 저해상도 입력 이미지에서 제1 특성 정보를 포함하는 제1 영역 및 제2 특성 정보를 포함하는 제2 영역을식별하고, 상기 제1 영역에 대해 제1 강도의 텍스처 처리를 수행한 제1 이미지 및 상기 제2 영역에 대해 상기 제1 강도와상이한 제2 강도의 텍스처 처리를 수행한 제2 이미지를 획득하고, 상기 제1 이미지 및 상기 제2 이미지를 각각 업스케일링하고, 상기 업스케일링된 제1 및 제2 이미지를 결합하여 상기 고해상도 이미지를 획득하는, 전자 장치."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 특성 정보는 오브젝트 영역 정보를 포함하고 상기 제2 특성 정보는 백그라운드 영역 정보를 포함하며, 상기 프로세서는,상기 오브젝트 영역 정보를 포함하는 영역을 상기 제1 영역으로 식별하고 상기 백그라운드 영역 정보를 포함하는 영역을 상기 제2 영역으로 식별하는, 전자 장치."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 프로세서는, 상기 제1 영역에 포함된 오브젝트의 타입이 제1 타입이면 상기 제1 영역에 대해 상기 제1 타입에 대응되는 강도의 텍스처 처리를 수행하고, 상기 제1 영역에 포함된 오브젝트의 타입이 제2 타입이면 상기 제1 영역에 대해 상기 제2 타입에 대응되는 강도의 텍스처 처리를 수행하는, 전자 장치."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 프로세서는, 상기 저해상도 입력 이미지를 제1 인공 지능 모델에 입력하여 상기 제1 영역에 대한 식별 정보, 상기 제1 강도,상기 제2 영역에 대한 식별 정보 및 상기 제2 강도를 획득하며,상기 제1 인공 지능 모델은, 이미지가 입력되면 입력된 이미지에서 특성이 상이한 영역에 대한 식별 정보 및 식별된 각 영역에 대한 텍스처공개특허 10-2022-0067463-3-강도를 출력하도록 학습된, 전자 장치."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 제1 영역에 대한 식별 정보는, 상기 저해상도 입력 이미지에 포함된 상기 제1 영역에 대한 좌표 정보를 포함하고, 상기 제2 영역에 대한 식별 정보는, 상기 저해상도 입력 이미지에 포함된 상기 제2 영역에 대한 좌표 정보를 포함하는, 전자 장치."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,상기 프로세서는, 상기 제1 영역 및 상기 제2 영역에 대한 식별 정보를 포함하는 이미지, 상기 제1 강도 및 상기 제2 강도를 제2인공 지능 모델에 입력하여 상기 텍스처 처리 및 업스케일링 처리된 제1 및 제2 이미지를 획득하는, 전자 장치."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 제2 인공 지능 모델은,SFT(Spartial Feature Transformation)를 수행하는 RRDB(Residual-in-Residual Dense Block)을 포함하도록 구현된, 전자 장치."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 제2 인공 지능 모델은,복수의 파라미터를 가지는 레이어를 포함하고, 상이한 특성을 가지는 복수의 영역으로 식별된 이미지 및 상기 복수의 영역에 대응되는 텍스처 강도에 기초하여획득된 복수의 이미지를 기준 이미지와 비교하여 손실 값을 산출하고, 상기 산출된 손실 값에 기초하여 상기 복수의 파라미터를 학습시키는, 전자 장치."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 제2 인공 지능 모델은,재구성 손실(reconstruction loss), 적대 손실(adversarial loss) 또는 인지 손실(perceptual loss) 중 적어도하나에 기초하여 상기 손실 값을 산출하는, 전자 장치."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서,상기 제2 인공 지능 모델은,GAN(Generative Adversarial Network)으로 구현되는, 전자 장치."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "임계 해상도 미만의 저해상도 입력 이미지로부터 상기 임계 해상도 이상의 고해상도 출력 이미지를 획득하도록학습된 인공 지능 모델을 저장하는 전자 장치의 제어 방법에 있어서, 공개특허 10-2022-0067463-4-상기 저해상도 입력 이미지에서 제1 특성 정보를 포함하는 제1 영역 및 제2 특성 정보를 포함하는 제2 영역을식별하는 단계;상기 제1 영역에 대해 제1 강도의 텍스처 처리를 수행한 제1 이미지 및 상기 제2 영역에 대해 상기 제1 강도와상이한 제2 강도의 텍스처 처리를 수행한 제2 이미지를 획득하는 단계;상기 제1 이미지 및 상기 제2 이미지를 각각 업스케일링하는 단계; 및상기 업스케일링된 제1 및 제2 이미지를 결합하여 상기 고해상도 이미지를 획득하는 단계;를 포함하는, 제어 방법."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 제1 특성 정보는 오브젝트 영역 정보를 포함하고 상기 제2 특성 정보는 백그라운드 영역 정보를 포함하며, 상기 제1 영역 및 상기 제2 영역을 식별하는 단계는,상기 오브젝트 영역 정보를 포함하는 영역을 상기 제1 영역으로 식별하고 상기 백그라운드 영역 정보를 포함하는 영역을 상기 제2 영역으로 식별하는, 제어 방법."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 제1 이미지 및 상기 제2 이미지를 획득하는 단계는,상기 제1 영역에 포함된 오브젝트의 타입이 제1 타입이면 상기 제1 영역에 대해 상기 제1 타입에 대응되는 강도의 텍스처 처리를 수행하고, 상기 제1 영역에 포함된 오브젝트의 타입이 제2 타입이면 상기 제1 영역에 대해 상기 제2 타입에 대응되는 강도의 텍스처 처리를 수행하는, 제어 방법."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 저해상도 입력 이미지를 제1 인공 지능 모델에 입력하여 상기 제1 영역에 대한 식별 정보, 상기 제1 강도,상기 제2 영역에 대한 식별 정보 및 상기 제2 강도를 획득하는 단계를 더 포함하고,상기 제1 인공 지능 모델은, 이미지가 입력되면 입력된 이미지에서 특성이 상이한 영역에 대한 식별 정보 및 식별된 각 영역에 대한 텍스처강도를 출력하도록 학습된, 제어 방법."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 제1 영역에 대한 식별 정보는, 상기 저해상도 입력 이미지에 포함된 상기 제1 영역에 대한 좌표 정보를 포함하고, 상기 제2 영역에 대한 식별 정보는, 상기 저해상도 입력 이미지에 포함된 상기 제2 영역에 대한 좌표 정보를 포함하는, 제어 방법."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서,상기 제1 이미지 및 상기 제2 이미지를 획득하는 단계 및 상기 제1 이미지 및 상기 제2 이미지를 각각 업스케일링하는 단계는, 공개특허 10-2022-0067463-5-상기 제1 영역 및 상기 제2 영역에 대한 식별 정보를 포함하는 이미지, 상기 제1 강도 및 상기 제2 강도를 제2인공 지능 모델에 입력하여 상기 텍스처 처리 및 업스케일링 처리된 제1 및 제2 이미지를 획득하는, 제어 방법."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 제2 인공 지능 모델은,SFT(Spartial Feature Transformation)를 수행하는 RRDB(Residual-in-Residual Dense Block)을 포함하도록 구현된, 제어 방법."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항에 있어서,상기 제2 인공 지능 모델은,복수의 파라미터를 가지는 레이어를 포함하고, 상이한 특성을 가지는 복수의 영역으로 식별된 이미지 및 상기 복수의 영역에 대응되는 텍스처 강도에 기초하여획득된 복수의 이미지를 기준 이미지와 비교하여 손실 값을 산출하고, 상기 산출된 손실 값에 기초하여 상기 복수의 파라미터를 학습시키는, 제어 방법."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 제2 인공 지능 모델은,재구성 손실(reconstruction loss), 적대 손실(adversarial loss) 또는 인지 손실(perceptual loss) 중 적어도하나에 기초하여 상기 손실 값을 산출하는, 제어 방법."}
{"patent_id": "10-2021-0030947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제18항에 있어서,상기 제2 인공 지능 모델은,GAN(Generative Adversarial Network)으로 구현되는, 제어 방법."}
{"patent_id": "10-2021-0030947", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 본 전자 장치는 디스플레이, 학습된 인공 지능 모델이 저장된 메모리 및 학습된 인공 지 능 모델을 이용하여 임계 해상도 미만의 저해상도 입력 이미지로부터 임계 해상도 이상의 고해상도 출력 이미지 를 획득할 수 있고 획득된 고해상도 이미지를 표시하도록 디스플레이를 제어하는 프로세서를 포함하고, 프로세서 는 저해상도 입력 이미지에서 제1 특성 정보를 포함하는 제1 영역 및 제2 특성 정보를 포함하는 제2 영역을 식별 하고 제1 영역에 대해 제1 강도의 텍스처 처리를 수행한 제1 이미지 및 제2 영역에 대해 제1 강도와 상이한 제2 강도의 텍스처 처리를 수행한 제2 이미지를 획득하고, 제1 이미지 및 제2 이미지를 각각 업스케일링할 수 있고, 업스케일링된 제1 및 제2 이미지를 결합하여 고해상도 이미지를 획득한다."}
{"patent_id": "10-2021-0030947", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 그 제어방법에 관한 것으로, 더욱 상세하게는 입력 이미지를 복수의 영역으로 구분하여 해상도를 변경하는 전자 장치 및 그 제어방법에 대한 것이다."}
{"patent_id": "10-2021-0030947", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 시스템은 인간 수준의 지능을 구현하는 컴퓨터 시스템으로서 기계가 스스로 학습하고 판단하며, 사용 할수록 인식률이 향상되는 시스템이다. 인공지능 기술은 입력 데이터들의 특징을 스스로 분류/학습하는 알고리즘을 이용하는 기계학습(딥러닝) 기술 및 기계학습 알고리즘을 활용하여 인간 두뇌의 인지, 판단 등의 기능을 모사하는 요소 기술들로 구성된다. 요소기술들은, 예로, 인간의 언어/문자를 인식하는 언어적 이해 기술, 사물을 인간의 시각처럼 인식하는 시각적 이해 기술, 정보를 판단하여 논리적으로 추론하고 예측하는 추론/예측 기술, 인간의 경험 정보를 지식데이터로 처리하는 지식 표현 기술 및 차량의 자율 주행, 로봇의 움직임을 제어하는 동작 제어 기술 중 적어도 하나를 포 함할 수 있다.인공 지능 모델은 저해상도 이미지에서 고해상도 이미지로 변환하는데 이용될 수 있다. 초해상도(Super Resolution) 이미지를 생성하기 위하여 인지 손실(perceptual loss) 및 적대 손실(adversarial loss)에 기초하여 학습된 인공 지능 모델이 이용될 수 있다. 인지 손실 및 적대 손실을 이용하는 인공 지능 모 델은 재구성의 전반적인 시각적 품질이 향상될 수 있다. 특히, 생성적 대립 신경망(Generative Adversarial Network, GAN)을 이용하여 이미지 변환 동작을 수행하는 경우, 고해상도 이미지를 생성하는데 이용될 수 있다. 다만, 재학습이 이루어지지 않은 상태에서 부자연스러운 영역을 변환할 수 없다는 문제점이 있을 수 있다. 또한, 연속 이미지 효과를 전환함에 있어서 한계가 있다. 또한, 전체 영역에 동일한 이미지 변환 방식이 적용될 뿐, 일부 영역별로 이미지 변환 방식을 다르게 하지 않아 부자연스러운 이미지를 생성할 수 있다. 또한, 일반적인 이미지 변환 모델은 동일한 기능 공간에서 이미지의 전체 영역에 대한 인지 손실을 계산하기 때 문에 결과가 단조롭고 부자연스럽다는 문제점이 있었다."}
{"patent_id": "10-2021-0030947", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 문제를 개선하기 위해 고안된 것으로, 본 개시의 목적은 입력 이미지를 복수의 영역으로 구분 하고 구분된 영역을 서로 다른 스타일 정보에 기초하여 변환하는 전자 장치 및 그의 제어 방법을 제공함에 있다."}
{"patent_id": "10-2021-0030947", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 목적을 달성하기 위한 본 실시 예에 따른 전자 장치는 디스플레이, 학습된 인공 지능 모델이 저장된 메 모리 및 상기 학습된 인공 지능 모델을 이용하여 임계 해상도 미만의 저해상도 입력 이미지로부터 상기 임계 해 상도 이상의 고해상도 출력 이미지를 획득할 수 있고 상기 획득된 고해상도 이미지를 표시하도록 상기 디스플레 이를 제어하는 프로세서를 포함하고, 상기 프로세서는 상기 저해상도 입력 이미지에서 제1 특성 정보를 포함하 는 제1 영역 및 제2 특성 정보를 포함하는 제2 영역을 식별하고 상기 제1 영역에 대해 제1 강도의 텍스처 처리 를 수행한 제1 이미지 및 상기 제2 영역에 대해 상기 제1 강도와 상이한 제2 강도의 텍스처 처리를 수행한 제2 이미지를 획득하고, 상기 제1 이미지 및 상기 제2 이미지를 각각 업스케일링할 수 있고, 상기 업스케일링된 제1 및 제2 이미지를 결합하여 상기 고해상도 이미지를 획득한다. 한편, 상기 제1 특성 정보는 오브젝트 영역 정보를 포함할 수 있고 상기 제2 특성 정보는 백그라운드 영역 정보 를 포함할 수 있고, 상기 프로세서는 상기 오브젝트 영역 정보를 포함하는 영역을 상기 제1 영역으로 식별할 수 있고 상기 백그라운드 영역 정보를 포함하는 영역을 상기 제2 영역으로 식별할 수 있다. 한편, 상기 프로세서는 상기 제1 영역에 포함된 오브젝트의 타입이 제1 타입이면 상기 제1 영역에 대해 상기 제 1 타입에 대응되는 강도의 텍스처 처리를 수행할 수 있고, 상기 제1 영역에 포함된 오브젝트의 타입이 제2 타입 이면 상기 제1 영역에 대해 상기 제2 타입에 대응되는 강도의 텍스처 처리를 수행할 수 있다. 한편, 상기 프로세서는 상기 저해상도 입력 이미지를 제1 인공 지능 모델에 입력하여 상기 제1 영역에 대한 식 별 정보, 상기 제1 강도, 상기 제2 영역에 대한 식별 정보 및 상기 제2 강도를 획득할 수 있고, 상기 제1 인공 지능 모델은 이미지가 입력되면 입력된 이미지에서 특성이 상이한 영역에 대한 식별 정보 및 식별된 각 영역에 대한 텍스처 강도를 출력하도록 학습될 수 있다. 한편, 상기 제1 영역에 대한 식별 정보는 상기 저해상도 입력 이미지에 포함된 상기 제1 영역에 대한 좌표 정보 를 포함할 수 있고, 상기 제2 영역에 대한 식별 정보는 상기 저해상도 입력 이미지에 포함된 상기 제2 영역에 대한 좌표 정보를 포함할 수 있다. 한편, 상기 프로세서는 상기 제1 영역 및 상기 제2 영역에 대한 식별 정보를 포함하는 이미지, 상기 제1 강도 및 상기 제2 강도를 제2 인공 지능 모델에 입력하여 상기 텍스처 처리 및 업스케일링 처리된 제1 및 제2 이미지 를 획득할 수 있다. 한편, 상기 제2 인공 지능 모델은 SFT(Spartial Feature Transformation)를 수행하는 RRDB(Residual-in- Residual Dense Block)을 포함하도록 구현될 수 있다. 한편, 상기 제2 인공 지능 모델은 복수의 파라미터를 가지는 레이어를 포함할 수 있고, 상이한 특성을 가지는 복수의 영역으로 식별된 이미지 및 상기 복수의 영역에 대응되는 텍스처 강도에 기초하여 획득된 복수의 이미지 를 기준 이미지와 비교하여 손실 값을 산출할 수 있고, 상기 산출된 손실 값에 기초하여 상기 복수의 파라미터 를 학습시킬 수 있다. 한편, 상기 제2 인공 지능 모델은 재구성 손실(reconstruction loss), 적대 손실(adversarial loss) 또는 인지 손실(perceptual loss) 중 적어도 하나에 기초하여 상기 손실 값을 산출할 수 있다. 한편, 상기 제2 인공 지능 모델은 GAN(Generative Adversarial Network)으로 구현될 수 있다. 본 개시의 일 실시 예에 따른, 임계 해상도 미만의 저해상도 입력 이미지로부터 상기 임계 해상도 이상의 고해 상도 출력 이미지를 획득하도록 학습된 인공 지능 모델을 저장하는 전자 장치의 제어 방법은 상기 저해상도 입 력 이미지에서 제1 특성 정보를 포함하는 제1 영역 및 제2 특성 정보를 포함하는 제2 영역을 식별하는 단계, 상 기 제1 영역에 대해 제1 강도의 텍스처 처리를 수행한 제1 이미지 및 상기 제2 영역에 대해 상기 제1 강도와 상 이한 제2 강도의 텍스처 처리를 수행한 제2 이미지를 획득하는 단계, 상기 제1 이미지 및 상기 제2 이미지를 각 각 업스케일링하는 단계 및 상기 업스케일링된 제1 및 제2 이미지를 결합하여 상기 고해상도 이미지를 획득하는 단계를 포함한다. 한편, 상기 제1 특성 정보는 오브젝트 영역 정보를 포함할 수 있고 상기 제2 특성 정보는 백그라운드 영역 정보 를 포함할 수 있고, 상기 제1 영역 및 상기 제2 영역을 식별하는 단계는 상기 오브젝트 영역 정보를 포함하는 영역을 상기 제1 영역으로 식별할 수 있고 상기 백그라운드 영역 정보를 포함하는 영역을 상기 제2 영역으로 식 별할 수 있다. 한편, 상기 제1 이미지 및 상기 제2 이미지를 획득하는 단계는 상기 제1 영역에 포함된 오브젝트의 타입이 제1 타입이면 상기 제1 영역에 대해 상기 제1 타입에 대응되는 강도의 텍스처 처리를 수행할 수 있고, 상기 제1 영 역에 포함된 오브젝트의 타입이 제2 타입이면 상기 제1 영역에 대해 상기 제2 타입에 대응되는 강도의 텍스처 처리를 수행할 수 있다. 한편, 상기 저해상도 입력 이미지를 제1 인공 지능 모델에 입력하여 상기 제1 영역에 대한 식별 정보, 상기 제1 강도, 상기 제2 영역에 대한 식별 정보 및 상기 제2 강도를 획득하는 단계를 더 포함할 수 있고, 상기 제1 인공 지능 모델은 이미지가 입력되면 입력된 이미지에서 특성이 상이한 영역에 대한 식별 정보 및 식별된 각 영역에 대한 텍스처 강도를 출력하도록 학습된, 제어 방법. 한편, 상기 제1 영역에 대한 식별 정보는 상기 저해상도 입력 이미지에 포함된 상기 제1 영역에 대한 좌표 정보 를 포함할 수 있고, 상기 제2 영역에 대한 식별 정보는 상기 저해상도 입력 이미지에 포함된 상기 제2 영역에 대한 좌표 정보를 포함할 수 있다. 한편, 상기 제1 이미지 및 상기 제2 이미지를 획득하는 단계 및 상기 제1 이미지 및 상기 제2 이미지를 각각 업 스케일링하는 단계는, 상기 제1 영역 및 상기 제2 영역에 대한 식별 정보를 포함하는 이미지, 상기 제1 강도 및 상기 제2 강도를 제2 인공 지능 모델에 입력하여 상기 텍스처 처리 및 업스케일링 처리된 제1 및 제2 이미지를 획득할 수 있다. 한편, 상기 제2 인공 지능 모델은 SFT(Spartial Feature Transformation)를 수행하는 RRDB(Residual-in- Residual Dense Block)을 포함하도록 구현될 수 있다. 한편, 상기 제2 인공 지능 모델은 복수의 파라미터를 가지는 레이어를 포함할 수 있고, 상이한 특성을 가지는 복수의 영역으로 식별된 이미지 및 상기 복수의 영역에 대응되는 텍스처 강도에 기초하여 획득된 복수의 이미지 를 기준 이미지와 비교하여 손실 값을 산출할 수 있고, 상기 산출된 손실 값에 기초하여 상기 복수의 파라미터 를 학습시킬 수 있다. 한편, 상기 제2 인공 지능 모델은 재구성 손실(reconstruction loss), 적대 손실(adversarial loss) 또는 인지 손실(perceptual loss) 중 적어도 하나에 기초하여 상기 손실 값을 산출할 수 있다. 한편, 상기 제2 인공 지능 모델은 GAN(Generative Adversarial Network)으로 구현될 수 있다."}
{"patent_id": "10-2021-0030947", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부 도면을 참조하여 본 개시를 상세히 설명한다. 본 개시의 실시 예에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 개시의 설명 부 분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수 치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. A 또는/및 B 중 적어도 하나라는 표현은 \"A\" 또는 \"B\" 또는 \"A 및 B\" 중 어느 하나를 나타내는 것으로 이해되어 야 한다. 본 명세서에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들 을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 어떤 구성요소가 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또 는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시에서 \"모듈\" 혹은 \"부\"는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨어로 구현되 거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\" 혹은 복수의 \"부\"는 특정한 하드 웨어로 구현될 필요가 있는 \"모듈\" 혹은 \"부\"를 제외하고는 적어도 하나의 모듈로 일체화되어 적어도 하나의 프 로세서(미도시)로 구현될 수 있다. 본 명세서에서, 사용자라는 용어는 전자 장치를 사용하는 사람 또는 전자 장치를 사용하는 장치(예: 인공지능 전자 장치)를 지칭할 수 있다. 이하 첨부된 도면들을 참조하여 본 개시의 일 실시 예를 보다 상세하게 설명한다. 본 개시에 따른 인공지능과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등 과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU(Neural Network Processing Unit)와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미 한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도 형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경 망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의 해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 한편, 지식 표현은 인간의 경험정보를 지식데이터로 자동화 처리하는 기술로서, 지식 구축(데이터 생성/분류), 지식 관리(데이터 활용) 등을 포함할 수 있다. 도 1은 일 실시 예에 따른 이미지 변환 동작을 설명하기 위한 도면이다. 도 1을 참조하면, 이미지 변환 모델은 입력 이미지를 수신하여 출력 이미지를 생성할 수 있다. 여 기서, 입력 이미지는 저해상도 이미지를 의미할 수 있고, 출력 이미지는 고해상도 이미지를 의미할 수 있다. 여기서, 이미지 변환 모델이 수행하는 이미지 변환 동작은 이미지 초해상화(super resolution)일 수 있다. 여기서, 이미지 변환 모델은 초해상화 네트워크에 해당할 수 있다. 여기서, 이미지 변환 모델는 스타일 식별 모델을 이용하여 초해상화 동작을 수행할 수 있다. 여기서, 스타일 식별 모델은 이미지 변환 동작에 적용되는 스타일을 결정할 수 있다. 구체적으로, 스타일 식별 모 델은 복수의 스타일 정보를 포함하고 있을 수 있다. 그리고, 스타일 식별 모델은 입력 이미지에 대응되는 스타일을 결정할 수 있다. 여기서, 스타일 정보는 스타일맵을 의미할 수 있다. 여기서, 스타일맵은 영역맵, 제어맵 등으로 기재될 수 있다. 이미지 변환 모델는 입력 이미지에 대한 정보를 스타일 식별 모델에 전송할 수 있으며, 스타일 식별 모델은 수신된 입력 이미지에 대한 정보에 기초하여 입력 이미지에 적합한 스타일을 식별할수 있다. 그리고, 스타일 식별 모델은 식별된 스타일을 이미지 변환 모델에 전송할 수 있다. 여기서, 이미지 변환 모델은 스타일 식별 모델에서 식별된 스타일에 기초하여 입력 이미지를 출 력 이미지로 초해상화할 수 있다. 도 2는 다른 실시 예에 따른 이미지 변환 동작을 설명하기 위한 도면이다. 도 2를 참조하면, 이미지 변환 모델은 입력 이미지를 수신하여 출력 이미지를 생성할 수 있다. 여 기서, 이미지 변환 모델은 스타일 식별 모델을 포함할 수 있다. 여기서, 스타일 식별 모델은 입 력 이미지에 대응되는 스타일을 결정할 수 있다. 구체적으로, 스타일 식별 모델은 입력 이미지에 적합한 스타일을 식별할 수 있으며, 이미지 변환 모델은 식별된 스타일에 기초하여 입력 이미지를 출 력 이미지로 초해상화할 수 있다. 도 3은 본 개시의 일 실시 예에 따른 전자 장치를 도시한 블록도이다. 도 3을 참조하면, 전자 장치는 메모리 및 프로세서로 구성될 수 있다. 본 명세서의 다양한 실시 예들에 따른 전자 장치는, 예를 들면, 스마트폰, 태블릿 PC, 이동 전화기, 데스 크탑 PC, 랩탑 PC, PDA, PMP(portable multimedia player) 중 적어도 하나를 포함할 수 있다. 어떤 실시 예들 에서, 전자 장치는, 예를 들면, 텔레비전, DVD(digital video disk) 플레이어, 미디어 박스(예: 삼성 HomeSyncTM, 애플TVTM, 또는 구글 TVTM)중 적어도 하나를 포함할 수 있다. 메모리는 프로세서에 포함된 롬(ROM)(예를 들어, EEPROM(electrically erasable programmable read- only memory)), 램(RAM) 등의 내부 메모리로 구현되거나, 프로세서와 별도의 메모리로 구현될 수도 있다. 이 경우, 메모리는 데이터 저장 용도에 따라 전자 장치에 임베디드된 메모리 형태로 구현되거나, 전 자 장치에 탈부착이 가능한 메모리 형태로 구현될 수도 있다. 예를 들어, 전자 장치의 구동을 위한 데이터의 경우 전자 장치에 임베디드된 메모리에 저장되고, 전자 장치의 확장 기능을 위한 데이터의 경우 전자 장치에 탈부착이 가능한 메모리에 저장될 수 있다. 한편, 전자 장치에 임베디드된 메모리의 경우 휘발성 메모리(예: DRAM(dynamic RAM), SRAM(static RAM), 또는 SDRAM(synchronous dynamic RAM) 등), 비휘발성 메모리(non-volatile Memory)(예: OTPROM(one time programmable ROM), PROM(programmable ROM), EPROM(erasable and programmable ROM), EEPROM(electrically erasable and programmable ROM), mask ROM, flash ROM, 플래시 메모리(예: NAND flash 또는 NOR flash 등), 하드 드라이브, 또는 솔리드 스테이트 드라이브(solid state drive(SSD)) 중 적어도 하나로 구현되고, 전자 장 치에 탈부착이 가능한 메모리의 경우 메모리 카드(예를 들어, CF(compact flash), SD(secure digital), Micro-SD(micro secure digital), Mini-SD(mini secure digital), xD(extreme digital), MMC(multi-media card) 등), USB 포트에 연결 가능한 외부 메모리(예를 들어, USB 메모리) 등과 같은 형태로 구현될 수 있다. 프로세서는 전자 장치의 전반적인 제어 동작을 수행할 수 있다. 구체적으로, 프로세서는 전자 장치의 전반적인 동작을 제어하는 기능을 한다. 프로세서는 디지털 신호를 처리하는 디지털 시그널 프로세서(digital signal processor(DSP), 마이크로 프 로세서(microprocessor), TCON(Time controller)으로 구현될 수 있다. 다만, 이에 한정되는 것은 아니며, 중앙 처리장치(central processing unit(CPU)), MCU(Micro Controller Unit), MPU(micro processing unit), 컨트롤 러(controller), 어플리케이션 프로세서(application processor(AP)), GPU(graphics-processing unit) 또는 커 뮤니케이션 프로세서(communication processor(CP)), ARM 프로세서 중 하나 또는 그 이상을 포함하거나, 해당 용어로 정의될 수 있다. 또한, 프로세서는 프로세싱 알고리즘이 내장된 SoC(System on Chip), LSI(large scale integration)로 구현될 수도 있고, FPGA(Field Programmable gate array) 형태로 구현될 수도 있다. 또 한, 프로세서는 메모리에 저장된 컴퓨터 실행가능 명령어(computer executable instructions)를 실 행함으로써 다양한 기능을 수행할 수 있다. 프로세서는 학습된 인공 지능 모델을 이용하여 임계 해상도 미만의 저해상도 입력 이미지로부터 임계 해상 도 이상의 고해상도 출력 이미지를 획득하고 획득된 고해상도 이미지를 표시하도록 디스플레이를 제어할 수 있다. 그리고, 프로세서는 저해상도 입력 이미지에서 제1 특성 정보를 포함하는 제1 영역 및 제2 특성 정보를 포함하는 제2 영역을 식별하고, 제1 영역에 대해 제1 강도의 텍스처 처리를 수행한 제1 이미지 및 제2 영역에 대해 제1 강도와 상이한 제2 강도의 텍스처 처리를 수행한 제2 이미지를 획득하고, 제1 이미지 및 제2 이미지를 각각 업스케일링하고, 업스케일링된 제1 및 제2 이미지를 결합하여 고해상도 이미지를 획득할 수있다. 여기서, 인공 지능 모델은 임계 해상도 미만의 저해상도 이미지를 임계 해상도 이상의 고해상도 이미지로 변환 하는 모델일 수 있다. 여기서, 임계 해상도는 사용자의 설정에 의해 변경 가능하다. 예를 들어, 프로세서 는 제1 해상도의 입력 이미지를 제1 해상도보다 큰 크기의 제2 해상도로 변경할 수 있다. 여기서, 프로세서 는 생성된 고해상도 이미지를 표시하도록 디스플레이를 제어할 수 있다. 일 실시 예에 따라, 인공 지능 모델은 전자 장치의 메모리에 저장될 수 있다. 따라서, 프로세서(12 0)는 메모리에 저장된 인공 지능 모델을 이용하여 저해상도 이미지를 고해상도 이미지로 변환할 수 있다. 다른 실시 예에 따라, 인공 지능 모델은 외부 서버에 저장될 수 있다. 따라서, 프로세서는 통신 인터페이 스를 통해 저해상도 이미지를 외부 서버에 전송할 수 있다. 여기서, 외부 서버는 수신된 저해상도 이미지 를 고해상도 이미지로 변환할 수 있으며, 변환된 고해상도 이미지를 다시 전자 장치에 전송할 수 있다. 프 로세서는 통신 인터페이스를 통해 외부 서버로부터 고해상도 이미지를 수신할 수 있다. 그리고, 프로 세서는 수신된 고해상도 이미지를 표시하도록 디스플레이를 제어할 수 있다. 여기서, 프로세서는 저해상도 입력 이미지에서 제1 영역 및 제2 영역을 식별(또는 구분)할 수 있다. 여기 서, 제1 영역은 저해상도 입력 이미지의 전체 영역에서 제1 특성 정보가 식별되는 영역을 의미할 수 있으며, 제 2 영역은 저해상도 입력 이미지의 전체 영역에서 제2 특성 정보가 식별되는 영역을 의미할 수 있다. 한편, 제1 특성 정보는 오브젝트 영역 정보를 포함하고 제2 특성 정보는 백그라운드 영역 정보를 포함할 수 있 고, 오브젝트 영역 정보를 포함하는 영역을 제1 영역으로 식별하고 백그라운드 영역 정보를 포함하는 영역을 제 2 영역으로 식별할 수 있다. 예를 들어, 프로세서는 저해상도 입력 이미지의 전체 영역에서 오브젝트 영역 및 백그라운드 영역을 식별 할 수 있다. 여기서, 오브젝트 영역은 사람, 사물 등이 포함된 영역을 의미할 수 있다. 또한, 오브젝트 영역은 텍스처 처리가 많이 필요한 영역을 의미할 수 있다. 여기서, 백그라운드 영역은 하늘, 구름, 바다 등이 포함된 영역을 의미할 수 있다. 또한, 백그라운드 영역은 텍스처 처리가 많이 필요하지 않은 영역을 의미할 수 있다. 여기서, 특성 정보는 오브젝트 영역 정보 또는 백그라운드 영역 정보를 포함할 수 있다. 여기서, 오브젝트 영역 정보는 오브젝트를 포함하고 있음을 나타내는 특성을 의미할 수 있으며, 백그라운드 영역 정보는 백그라운드를 포함하고 있음을 나타내는 특성을 의미할 수 있다. 여기서, 프로세서는 저해상도 입력 이미지에 기초하여 특성 정보를 획득할 수 있다. 구체적으로, 프로세서 는 저해상도 입력 이미지를 분석하여 위치별 특성 정보를 판단할 수 있다. 그리고, 프로세서는 판단 된 특성 정보에 기초하여 저해상도 입력 이미지의 전체 영역을 복수의 영역으로 구분할 수 있다. 예를 들어, 프로세서는 저해상도 이미지를 분석하여 위치에 따라 오브젝트 특성을 갖는 위치 및 배경 특성 을 갖는 위치를 획득할 수 있다. 그리고, 프로세서는 획득된 오브젝트 특성을 갖는 위치 및 배경 특성을 갖는 위치에 기초하여 복수의 영역을 식별할 수 있다. 구체적으로, 프로세서는 오브젝트 특성을 갖는 위치 들을 그룹핑하여 하나의 영역으로 결정하고, 배경 특성을 갖는 위치들을 그룹핑하여 하나의 영역으로 결정할 수 있다. 여기서, 설명의 편의를 위해 2개의 영역으로 구분하는 실시 예를 기재하였지만, 사용자 설정에 따라서 2개의 영 역이 아닌 3개 이상의 영역으로 구분될 수 있다. 한편, 프로세서는 제1 영역에 포함된 오브젝트의 타입이 제1 타입이면 제1 영역에 대해 제1 타입에 대응되 는 강도의 텍스처 처리를 수행하고, 제1 영역에 포함된 오브젝트의 타입이 제2 타입이면 제1 영역에 대해 제2 타입에 대응되는 강도의 텍스처 처리를 수행할 수 있다. 여기서, 프로세서는 오브젝트 특성을 갖는 영역에서 오브젝트 타입에 기초하여 오브젝트 영역을 복수의 영 역으로 구분할 수 있다. 그리고, 오브젝트 타입이 상이한 영역이 식별된 경우, 프로세서는 오브젝트 타입 에 따라 상이한 강도의 텍스처 처리를 수행할 수 있다. 예를 들어, 프로세서는 오브젝트 영역에서 사람이 포함된 영역과 건물이 포함된 영역을 구분할 수 있다. 그리고, 사람이 포함된 영역에 대하여 사람에 대응되는 강도(예를 들어, 레벨 3)의 텍스처 처리를 수행하고 건 물이 포함된 영역에 대하여 건물에 대응되는 강도(예를 들어, 레벨 5)의 텍스처 처리를 수행할 수 있다. 여기서, 제1 강도 및 제2 강도 등의 강도 정보는 스타일 정보로 기재할 수 있다. 텍스처 처리를 위한 강도가 상 이하다는 의미는 스타일이 상이하다는 의미일 수 있다. 스타일 정보는 특정 영역에 대하여 어떤 처리를 수행해 야 하는지 나타내는 정보를 포함할 수 있다. 예를 들어, 제1 스타일 정보는 제1 강도의 텍스처 처리를 수행해야 된다는 정보를 포함할 수 있으며, 제2 스타일 정보는 제2 강도의 텍스처 처리를 수행해야 된다는 정보를 포함할 수 있다. 프로세서는 입력 이미지에 포함된 제1 영역에 대응되는 제1 특성 정보 및 제2 영역에 대응되 는 제2 특성 정보 각각에 기초하여 제1 스타일 정보 및 제2 스타일 정보를 획득할 수 있고, 제1 스타일 정보 및 제2 스타일 정보를 인공 지능 모델에 각각 입력하여 제1 영역에 대응되는 고해상도 이미지 및 제2 영역에 대응 되는 고해상도 이미지를 획득할 수 있고, 제1 영역에 대응되는 고해상도 이미지 및 제2 영역에 대응되는 고해상 도 이미지에 기초하여 출력 이미지를 획득할 수 있다. 여기서, 프로세서는 입력 이미지를 획득할 수 있다. 여기서, 입력 이미지는 저해상도 이미지를 의 미할 수 있다. 또한, 프로세서는 획득된 입력 이미지에 대응되는 전체 영역을 복수의 영역으로 구분(또는 식별)할 수 있다. 구체적으로, 프로세서는 입력 이미지를 분석하여 영역별로 특성이 상이 한지 판단할 수 있다. 입력 이미지에 포함된 복수의 영역의 특성이 다르다면 상이한 방식으로 고해상도 변환 동작을 수행할 필요 성이 있기 때문이다. 따라서, 프로세서는 입력 이미지에 포함된 서로 다른 특성을 갖는 복수의 영역을 구분할 수 있다. 구체적으로, 프로세서는 입력 이미지에서 오브젝트를 식별할 수 있으며, 식별된 오브젝트에 기초하여 복수의 영역을 구분할 수 있다. 한편, 프로세서는 입력 이미지에 포함된 제1 오브젝트 및 제2 오브젝트를 식별할 수 있고, 제1 오브젝 트의 위치 및 제2 오브젝트의 위치에 기초하여 제1 오브젝트를 포함하는 제1 영역 및 제2 오브젝트를 포함하는 제2 영역을 구분할 수 있다. 여기서, 특성 정보는 구분된(또는 식별된) 영역에 대응되는 특성을 포함할 수 있다. 한편, 제1 특성 정보는 제1 오브젝트의 특성 정보를 포함할 수 있고, 제2 특성 정보는 제2 오브젝트의 특성 정 보를 포함할 수 있다. 예를 들어, 특성 정보는 오브젝트의 종류, 이름을 포함할 수 있다. 제1 오브젝트가 배경 오브젝트이면, 제1 특 성 정보는 배경 특성일 수 있으며, 추가적으로 하늘 배경, 숲 배경, 건물 배경, 땅 배경 등 특정된 배경을 포함 할 수 있다. 또한, 제2 오브젝트가 사물 오브젝트이면, 제2 특성 정보는 사물 특성일 수 있다. 제2 오브젝트가 사물 오브젝 트이면, 제2 특성 정보는 사물 특성일 수 있으며, 추가적으로 사람 사물, 동물 사물, 건축물 사물, 금속 표면 사물 등을 의미할 수 있다. 한편, 제1 스타일 정보는 기 결정된 이미지 스타일의 제1 레벨에 기초하여 제1 영역에 대응되는 이미지를 변환 하기 위한 정보를 포함할 수 있고, 제2 스타일 정보는 기 결정된 이미지 스타일의 제1 레벨과 상이한 제2 레벨 에 기초하여 제2 영역에 대응되는 이미지를 변환하기 위한 정보를 포함할 수 있다. 여기서, 기 결정된 이미지 스타일은 인공 지능 모델이 고해상도 이미지를 생성하면서 추가적으로 고려할 스타일 을 의미할 수 있다. 여기서, 인공 지능 모델은 복수의 이미지 스타일 중 적어도 하나의 스타일에 기초하여 고해 상도 이미지를 생성할 수 있다. 예를 들어, 기 결정된 이미지 스타일은 질감(texture) 효과를 반영하는 스타일 을 의미할 수 있다. 또한, 기 결정된 이미지 스타일은 복수의 레벨로 이미지 변환 동작에 반영될 수 있다. 여기서, 레벨은 기 결정 된 이미지 스타일의 반영 정도를 나타내는 정보를 의미할 수 있다. 예를 들어, 질감(texture) 효과를 반영하는 스타일은 3개의 레벨로 이미지 변환 동작에 반영될 수 있다. 여기서, 제1 레벨은 질감 효과를 최소로 반영하거 나 질감 효과를 전혀 반영하지 않는 레벨이고, 제2 레벨은 질감 효과를 중간 정도로 반영하는 레벨이고, 제3 레 벨은 질감 효과를 최대로 반영하는 레벨일 수 있다. 따라서, 프로세서는 획득된 특성 정보에 대응되는 레 벨을 식별하고, 식별된 레벨에 기초하여 이미지 변환 동작을 수행할 수 있다. 스타일 정보는 기 결정된 이미지 스타일 또는 기 결정된 이미지 스타일의 레벨 정보를 포함할 수 있다. 여기서, 기 결정된 이미지 스타일에 대응되는 레벨이 존재하지 않는 경우, 스타일 정보는 레벨 정보를 포함하지 않을 수 있다. 또한, 기 결정된 이미지 스타일 자체에 이미 레벨 정보가 반영될 수 있다. 예를 들어, 스타일 정보는 제1 레벨의 질감 효과를 반영하는 제1 스타일, 제2 레벨의 질감 효과를 반영하는 제2 스타일, 제3 레벨의 질감 효과를 반영하는 제3 스타일 중 적어도 하나를 포함할 수 있다. 한편, 인공 지능 모델은 복수의 이미지 스타일을 포함하는 스타일 식별 모델 및 입력 이미지를 출력 이미지로 변환하는 이미지 변환 모델을 포함할 수 있다. 한편, 프로세서는 저해상도 입력 이미지를 제1 인공 지능 모델에 입력하여 제1 영역에 대한 식별 정보, 제 1 영역을 처리할 텍스처 강도인 제1 강도, 제2 영역에 대한 식별 정보 및 제2 영역을 처리할 텍스처 강도인 제2 강도를 획득하며, 제1 인공 지능 모델은 이미지가 입력되면 입력된 이미지에서 특성이 상이한 영역에 대한 식별 정보 및 식별된 각 영역에 대한 텍스처 강도를 출력하도록 학습할 수 있다. 여기서, 제1 인공 지능 모델은 도12의 스타일 식별 모델일 수 있다. 여기서, 제1 인공 지능 모델은 이미지 를 입력 받아 영역 및 영역에 대응되는 텍스처 강도를 출력할 수 있다. 구체적으로, 제1 인공 지능 모델을 이미 지를 복수의 영역으로 구분하고, 구분된 영역에 대응되는 특성을 획득하고, 획득된 특성에 대응되는 텍스처 강 도를 획득할 수 있다. 예를 들어, 제1 인공 지능 모델은 이미지를 제1 영역 및 제2 영역으로 구분하고, 제1 영 역에는 제1 강도의 텍스처 처리가 필요하고 제2 영역은 제2 강도의 텍스처 처리가 필요하다고 결정할 수 있다. 여기서, 제1 영역에 대한 식별 정보는 제1 영역의 위치 정보 또는 좌표 정보를 의미할 수 있다. 여기서, 제2 영 역에 대한 식별 정보는 제2 영역의 위치 정보 또는 좌표 정보를 의미할 수 있다. 한편, 제1 영역에 대한 식별 정보는 저해상도 입력 이미지에 포함된 제1 영역에 대한 좌표 정보를 포함하고, 제 2 영역에 대한 식별 정보는 저해상도 입력 이미지에 포함된 제2 영역에 대한 좌표 정보를 포함할 수 있다. 여기서, 좌표 정보는 이미지 상의 위치 정보를 의미할 수 있다. 한편, 프로세서는 제1 영역 및 제2 영역에 대한 식별 정보를 포함하는 이미지, 제1 강도 및 제2 강도를 제 2 인공 지능 모델에 입력하여 텍스처 처리 및 업스케일링 처리된 제1 및 제2 이미지를 획득할 수 있다. 여기서, 프로세서는 복수의 영역 각각에 대한 식별 정보 및 복수의 영역 각각에 대한 텍스처 강도 정보를 제2 인공 지능 모델에 입력할 수 있다. 여기서, 제2 인공 지능 모델은 도 12의 이미지 변환 모델일 수 있 다. 여기서, 제2 인공 지능 모델은 제1 영역에 대하여 제1 강도의 텍스처 처리를 수행하여 제1 이미지를 획득할 수 있다. 그리고, 제2 인공 지능 모델은 제2 영역에 대하여 제2 강도의 텍스처 처리를 수행하여 제2 이미지를 획득 할 수 있다. 여기서, 제2 인공 지능 모델은 획득된 제1 이미지를 업스케일링하여 업스케일링된 제1 이미지를 획득할 수 있다. 그리고, 제2 인공 지능 모델은 획득된 제2 이미지를 업스케일링하여 업스케일링된 제2 이미지를 획득할 수 있다. 한편, 제2 인공 지능 모델은 SFT(Spartial Feature Transformation)를 수행하는 RRDB(Residual-in-Residual Dense Block)을 포함하도록 구현될 수 있다. 구체적으로, 인공 지능 모델에 포함된 이미지 변환 모델이 SFT 레이어를 포함하는 RRDB를 기본 블록으로 이용할 수 있다. 이와 관련된 구체적인 설명은 도 13에서 후술한 다. 여기서, 이미지 변환 모델및 스타일 식별 모델은 별개의 인공 지능 모델로 구현될 수 있다. 각 모델 이 별개의 동작을 구분하여 수행함으로써 데이터 처리 속도를 향상시키거나 동작 알고리즘을 간소화할 수 있다. 여기서, 이미지 변환 모델은 저해상도 이미지를 고해상도 이미지로 변환하는 동작을 수행할 수 있으며, 변 환 동작에서 스타일 식별 모델에서 식별된 스타일 정보를 추가적으로 고려할 수 있다. 예를 들어, 스타일 식별 모델에서 제1 영역에 질감 효과를 최소로 반영하고 제2 영역에 질감 효과를 최대 로 반영하는 스타일 정보를 생성하고, 생성된 스타일 정보를 이미지 변환 모델에 전송할 수 있다. 여기서, 이미지 변환 모델는 스타일 식별 모델로부터 수신한 스타일 정보에 기초하여 제1 영역에 대응되는 이 미지는 질감 효과를 최소로 반영하면서 고해상도 이미지로 변환하고, 제2 영역에 대응되는 이미지는 질감 효과 를 최대로 반영하면서 고해상도 이미지로 변환할 수 있다. 또한, 프로세서는 이미지 변환 모델에 기초하여 제1 특성 정보 및 제2 특성 정보를 획득할 수 있고, 스타일 식별 모델에 기초하여 제1 특성 정보에 대응되는 제1 스타일 정보 및 제2 특성 정보에 대응되는 제 2 스타일 정보를 획득할 수 있다.한편, 프로세서는 제1 스타일 정보에 기초하여 제1 영역을 저해상도의 제1 이미지로 변환할 수 있고, 제2 스타일 정보에 기초하여 제2 영역을 저해상도 제2 이미지로 변환할 수 있고, 제1 이미지를 업스케일링하여 제1 영역에 대응되는 고해상도 이미지를 획득할 수 있고, 제2 이미지를 업스케일링하여 제2 영역에 대응되는 고해상 도 이미지를 획득할 수 있다. 여기서, 프로세서는 이미지 변환 모델에 기초하여 이미지 변환 동작을 수행할 수 있는데, 저해상도 처리 동작, 업스케일링 동작 및 고해상도 처리 동작을 수행할 수 있다. 여기서, 저해상도 처리 동작은 업스케일 링 동작 이전에 입력 이미지를 변환하는 동작을 의미할 수 있다. 여기서, 저해상도 처리 동작은 도 12의 저 해상도 처리부에 의하여 수행될 수 있다. 여기서, 업스케일링 동작은 도 12의 업스케일링부에 의하 여 수행될 수 있다. 여기서, 고해상도 처리 동작은 도 12의 고해상도 처리부에 의하여 수행될 수 있다. 여기서, 이미지 변환 모델 및 스타일 식별 모델을 이용한 다양한 실시 예는 도 1 및 도 2에서 기재하 였다. 또한, 구체적인 동작은 도 12에서 후술한다. 한편, 상술한 설명은 입력 이미지를 출력 이미지로 변환하는 동작과 관련된 설명이었다. 변환 동작에서 이미지 변환 모델 및 스타일 식별 모델이 이용될 수 있으며, 이미지 변환 모델에서 이미지 변환 동작에 이용하는 파라미터는 이미 학습된 최종 파라미터일 수 있다. 즉, 이미지 변환 모델은 이미 복수의 입력 이미지, 복수의 입력 이미지에 대응되는 기준 이미지에 기초하여 학습을 완료한 상태일 수 있다. 학습이 완료된 이미지 변환 모델에 기초하여 전자 장치는 이미지 변환 동작을 수행할 수 있다. 한편, 제2 인공 지능 모델은 복수의 파라미터를 가지는 레이어를 포함하고, 상이한 특성을 가지는 복수의 영역 으로 식별된 이미지 및 복수의 영역에 대응되는 텍스처 강도에 기초하여 획득된 복수의 이미지를 기준 이미지와 비교하여 손실 값을 산출하고, 산출된 손실 값에 기초하여 복수의 파라미터를 학습시킬 수 있다. 이미지 변환 모델이 학습되는 과정에 대하여 설명한다. 인공 지능 모델은 학습을 위하여 손실값 계산 모델 을 추가적으로 이용할 수 있다. 구체적으로, 인공 지능 모델은 복수의 이미지 스타일을 포함하는 스타일 식별 모델, 입력 이미지를 출 력 이미지로 변환하는 이미지 변환 모델 및 이미지 변환 모델에 대응되는 복수의 파라미터를 결 정하기 위한 학습 동작에 이용되는 손실값 계산 모델을 포함할 수 있고, 이미지 변환 모델은 스타일 정보에 기초하여 학습 입력 이미지를 학습 출력 이미지로 변환할 수 있고, 손실값 계산 모델은 스타일 정 보에 기초하여 학습 출력 이미지에 대응되는 손실을 획득할 수 있고, 이미지 변환 모델은 손실에 기초하여 복수의 파라미터를 결정할 수 있다. 한편, 제2 인공 지능 모델은 재구성 손실(reconstruction loss), 적대 손실(adversarial loss) 또는 인지 손실 (perceptual loss) 중 적어도 하나에 기초하여 손실 값을 산출할 수 있다. 한편, 제2 인공 지능 모델은 GAN(Generative Adversarial Network)으로 구현될 수 있다. 여기서, 인지 손실은 스타일 정보에 기초하여 획득되고, 이미지 변환 모델은 적어도 하나의 손실에 기초하 여 복수의 파라미터를 결정할 수 있다. 여기서, 손실값 획득 동작에서 이용되는 이미지는 이미지 변환 모델에서 생성한 출력 이미지와 입력 이미지에 대응되는 기준 이미지일 수 있다. 여기서, 기준 이미지는 입력 이미지의 고해상도 이 미지를 의미할 수 있으며 실측 자료(Ground Truth)를 의미할 수 있다. 여기서, 재구성 손실은 출력 이미지와 기준 이미지의 차이값을 포함하는 정보일 수 있다. 재구성 손실 과 관련된 구체적인 계산은 도 11의 수학식에서 후술한다. 여기서, 인지 손실은 지각 손실일 수 있으며, 다른 딥러닝 기반의 영상 분류 네트워크에 학습 이미지들(입력 이 미지, 출력 이미지 또는 기준 이미지 중 적어도 하나)을 통과시킨 후 획득되는 특징맵(feature map) 사이의 손실을 의미할 수 있다. 여기서, 영상 기반 분류 네트워크는 스타일 정보에 대응되는 이미지 변환 네트워크를 의미할 수 있다. 예를 들어, 제1 스타일 정보를 반영하기 위하여 이미지 변환 모델은 제1 이미 지 변환 네트워크를 이용할 수 있다. 여기서, 손실값 계산 모델은 제1 이미지 변환 네트워크에 기초하여 출력 이미지 및 기준 이미지 사이의 차이값을 획득할 수 있다. 구체적인 계산 과정은 도 11의 수학식 및 수학식에서 후술한다. 여기서, 적대 손실은 적대적 생성 네트워크(Generative Adversarial Network, GAN)에 기초하여 획득되는 손실 을 의미할 수 있다. 여기서, 적대 손실은 진짜 이미지(기준 이미지)를 진짜로 판단할 확률 및 가짜 이미지(출력 이미지)를 가짜로 판단할 확률값의 차이에 기초하여 획득되는 손실을 의미할 수 있다. 구체적인 계산 과정은 도 11의 수학식, 수학식 및 수학식에서 후술한다. 한편, 손실값 계산 모델은 손실값 계산 모델에 포함된 판별기(discriminator)에 기초하여 판별기 손 실(discriminator loss)을 획득할 수 있고, 이미지 변환 모델은 판별기 손실에 기초하여 복수의 파라미터 를 결정할 수 있다. 여기서, 판별기(discriminator)는 적대적 생성 네트워크(Generative Adversarial Network, GAN)에서 이용되는 분류 네트워크를 의미할 수 있다. 여기서, 판별기는 진짜 이미지(기준 이미지)를 진짜로 판단할 확률 및 가짜 이미지(출력 이미지)를 가짜로 판단하는 동작을 수행할 수 있다. 여기서, 판별기 손실은 판별기의 판별 동작 자체를 학습하는 과정에서 획득되는 손실을 의미할 수 있다. 적대 손실과 판별기 손실은 진짜 이미지(기준 이미지)를 진짜로 판단할 확률 및 가짜 이미지(출력 이미지)를 가 짜로 판단하는 동작에서 획득되는 것이라는 점에서 공통될 수 있다. 하지만, 적대 손실은 이미지 변환 동작에서 획득되는 손실이며, 판별기 손실은 판별 동작에서 획득되는 손실이라는 점에서 차이가 있다. 구체적인 계산 과 정은 도 15의 수학식, 수학식 및 수학식에서 후술한다. 한편, 전자 장치는 다양한 스타일 정보에 기초하여 이미지 변환을 수행하기 위해 복수의 지각 손실의 가중 합계를 이용하여 이미지 변환 모델을 최적화할 수 있다. 또한, 전자 장치는 재구성 스타일을 생성할 수 있는 조건부 목표 함수를 이용하는 이미지 변환 모델 을 이용하여 복수의 영역마다 적응적인 고해상도 이미지를 생성할 수 있다. 한편, 인공 지능 모델을 이용하여 입력 데이터를 출력 이미지로 변환하는 다양한 실시 예가 있을 수 있다. 일 실시 예에 따라, 전자 장치는 인공 지능 모델을 저장하고 있으며, 입력 데이터를 수신하고, 인공 지능 모델에 기초하여 수신된 입력 데이터에 대응되는 출력 이미지를 획득하고, 출력 이미지를 사용자에게 제공할 수 있다. 여기서, 출력 이미지를 사용자에게 제공하는 것은 이미지를 디스플레이에 표시하거나 오디오를 스피커를 통해 출력하는 것을 의미할 수 있다. 예를 들어, TV(전자 장치)는 입력 이미지를 수신하고, 수신된 입력 이미지 의 해상도를 변경하여 출력 이미지를 획득하고, 획득된 출력 이미지를 TV(전자 장치)의 디스플레이에 직접 표시 할 수 있다. 다른 실시 예에 따라, 전자 장치는 인공 지능 모델을 저장하고 있으며, 입력 데이터를 수신하고, 인공 지 능 모델에 기초하여 수신된 입력 데이터에 대응되는 출력 데이터를 획득하고, 획득된 출력 데이터를 외부 장치 에 전송할 수 있다. 여기서, 외부 장치는 TV, 스마트폰 등 사용자의 단말 장치일 수 있다. 전자 장치는 출 력 이미지를 획득하기 위한 장치일 뿐, 실질적으로 출력 이미지를 사용자에게 제공하는 동작은 외부 장치에서 수행될 수 있다. 예를 들어, TV(외부 장치)는 입력 이미지를 수신하고, 수신된 입력 이미지를 인공 지능 서버 (전자 장치)에 전송할 수 있다. 인공 지능 서버(전자 장치)는 수신된 입력 이미지를 출력 이미지로 변환하고, 변환된 출력 이미지를 다시 TV(외부 장치)에 전송할 수 있다. 또 다른 실시 예에 따라, 전자 장치 및 외부 장치가 함께 출력 데이터를 획득할 수 있다. 출력 데이터를 획득하기 위해 복수의 동작이 필요하다고 가정한다. 여기서, 복수의 동작 중 일부는 전자 장치에서 수행되 고, 복수의 동작 중 나머지는 외부 장치에서 수행될 수 있다. 따라서, 출력 데이터를 획득하기 위해선 인공 지 능 모델을 저장하고 있는 전자 장치뿐 아니라 외부 장치도 필요할 수 있다. 예를 들어, TV(외부 장치)는 입력 이미지를 수신하고, 수신된 입력 이미지를 복수의 영역으로 구분할 수 있다. 그리고, TV(외부 장치)는 구 분된 복수의 영역 각각에 대응되는 스타일 정보를 획득할 수 있다. 그리고, TV(외부 장치)는 입력 이미지 및 복 수의 영역 각각에 대응되는 스타일 정보를 인공 지능 서버(전자 장치)에 전송할 수 있다. 여기서, 인공 지능 서 버(전자 장치)는 TV(외부 장치)로부터 수신한 입력 이미지 및 복수의 영역 각각에 대응되는 스타일 정보에 기초 하여 출력 이미지를 획득할 수 있다. 그리고, 인공 지능 서버(전자 장치)는 출력 이미지를 TV(외부 장치)에 전 송할 수 있다. 그리고, TV(외부 장치)는 인공 지능 서버(전자 장치)로부터 수신된 출력 이미지를 표시할 수 있 다. 구체적으로, TV(외부 장치)는 스타일 식별 모델을 저장하고 있고, 인공 지능 서버(전자 장치)는 이미 지 변환 모델을 저장할 수 있다. 한편, 상술한 설명에서는 전자 장치가 텍스처 처리를 수행하여 고해상도 이미지를 획득하는 것으로 기재하 였다. 하지만, 구현 예에 따라, 전자 장치는 텍스처 처리 이외의 다양한 처리 동작을 수행할 수 있다.한편, 상술한 설명에서는 텍스처 처리를 수행한 이후에 업스케일링 동작을 수행하는 것으로 기재하였다. 하지만, 구현 예에 따라, 전자 장치는 업스케일링 동작을 먼저 수행하고, 업스케일링된 이미지에 텍스처 처리를 수행할 수 있다. 여기서, 업스케일링 동작은 복수의 영역 각각에 적용될 수 있으며, 텍스처 처리도 업스 케일링된 복수의 영역 각각에 적용될 수 있다. 한편, 이상에서는 전자 장치를 구성하는 간단한 구성에 대해서만 도시하고 설명하였지만, 구현 시에는 다 양한 구성이 추가로 구비될 수 있다. 이에 대해서는 도 4를 참조하여 이하에서 설명한다. 도 4는 도 3의 전자 장치의 구체적인 구성을 설명하기 위한 블록도이다. 도 4를 참조하면, 전자 장치는 메모리, 프로세서, 통신 인터페이스, 디스플레이, 조 작 인터페이스 및 입출력 인터페이스로 구성될 수 있다. 한편, 메모리 및 프로세서의 동작 중에서 앞서 설명한 것과 동일한 동작에 대해서는 중복 설명은 생 략한다. 통신 인터페이스는 다양한 유형의 통신 방식에 따라 다양한 유형의 외부 장치와 통신을 수행하는 구성이다. 통신 인터페이스는 와이파이 모듈, 블루투스 모듈, 적외선 통신 모듈 및 무선 통신 모듈 등을 포함한다. 여기서, 각 통신 모듈은 적어도 하나의 하드웨어 칩 형태로 구현될 수 있다. 통신 인터페이스는 와이파이 모듈, 블루투스 모듈은 각각 와이파이 방식, 블루투스 방식으로 통신을 수행 할 수 있다. 와이파이 모듈이나 블루투스 모듈을 이용하는 경우에는SSID 및 세션 키 등과 같은 각종 연결 정보 를 먼저 송수신하여, 이를 이용하여 통신 연결한 후 각종 정보들을 송수신할 수 있다. 적외선 통신 모듈은 가시 광선과 밀리미터파 사이에 있는 적외선을 이용하여 근거리에 무선으로 데이터를 전송 하는 적외선 통신(IrDA, infrared Data Association)기술에 따라 통신을 수행한다. 무선 통신 모듈은 상술한 통신 방식 이외에 지그비(zigbee), 3G(3rd Generation), 3GPP(3rd Generation Partnership Project), LTE(Long Term Evolution), LTE-A(LTE Advanced), 4G(4th Generation), 5G(5th Generation)등과 같은 다양한 무선 통신 규격에 따라 통신을 수행하는 적어도 하나의 통신 칩을 포함할 수 있다. 그 밖에 통신 인터페이스는LAN(Local Area Network) 모듈, 이더넷 모듈, 페어 케이블, 동축 케이블, 광섬 유 케이블 또는 UWB(Ultra Wide-Band) 모듈 등을 이용하여 통신을 수행하는 유선 통신 모듈 중 적어도 하나를 포함할 수 있다. 일 예에 따라 통신 인터페이스는 원격 제어 장치와 같은 외부 장치 및 외부 서버와 통신하기 위해 동일한 통신 모듈(예를 들어, 와이파이 모듈)을 이용할 수 있다. 다른 예에 따라 통신 인터페이스는 원격 제어 장치와 같은 외부 장치 및 외부 서버와 통신하기 위해 상이 한 통신 모듈을 이용할 수 있다. 예를 들어, 통신 인터페이스는 외부 서버와 통신하기 위해 이더넷 모듈 또는 와이파이 모듈 중 적어도 하나를 이용할 수 있고, 원격 제어 장치와 같은 외부 장치와 통신하기 위해 블루 투스 모듈을 이용할 수도 있다. 다만 이는 일 실시 예에 불과하며 통신 인터페이스는 복수의 외부 장치 또 는 외부 서버와 통신하는 경우 다양한 통신 모듈 중 적어도 하나의 통신 모듈을 이용할 수 있다. 디스플레이는 LCD(Liquid Crystal Display), OLED(Organic Light Emitting Diodes) 디스플레이, PDP(Plasma Display Panel) 등과 같은 다양한 형태의 디스플레이로 구현될 수 있다. 디스플레이내에는 a- si TFT, LTPS(low temperature poly silicon) TFT, OTFT(organic TFT) 등과 같은 형태로 구현될 수 있는 구동 회로, 백라이트 유닛 등도 함께 포함될 수 있다. 한편, 디스플레이는 터치 센서와 결합된 터치 스크린, 플 렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display) 등으로 구현될 수 있다. 또한, 본 개시의 일 실시 예에 따른, 디스플레이는 영상을 출력하는 디스플레이 패널뿐만 아니라, 디스플 레이 패널을 하우징하는 베젤을 포함할 수 있다. 특히, 본 개시의 일 실시 예에 따른, 베젤은 사용자 인터렉션 을 감지하기 위한 터치 센서(미도시)를 포함할 수 있다. 일 실시 예에 따라, 전자 장치는 디스플레이를 포함할 수 있다. 구체적으로, 전자 장치는 획득 된 이미지 또는 컨텐츠를 디스플레이에 직접 표시할 수 있다. 한편, 다른 실시 예에 따라, 전자 장치는 디스플레이를 포함하지 않을 수 있다. 전자 장치는 외 부 디스플레이 장치와 연결될 수 있으며, 전자 장치에 저장된 이미지 또는 컨텐츠를 외부 디스플레이 장치 에 전송할 수 있다. 구체적으로, 전자 장치는 외부 디스플레이 장치에서 이미지 또는 컨텐츠가 표시되도록 제어하기 위한 제어 신호와 함께 이미지 또는 컨텐츠를 외부 디스플레이 장치에 전송할 수 있다. 여기서, 외부 디스플레이 장치는 전자 장치와 통신 인터페이스 또는 입출력 인터페이스를 통해 연결될 수 있 다. 예를 들어, 전자 장치는 STB(Set Top Box)와 같이 디스플레이를 포함하지 않을 수 있다. 또한, 전자 장치는 텍스트 정보 등의 간단한 정보만을 표시할 수 있는 소형 디스플레이만을 포함할 수 있다. 여기서, 전자 장치는 이미지 또는 컨텐츠를 통신 인터페이스를 통해 유선 또는 무선으로 외부 디스플레이 장 치에 전송하거나 입출력 인터페이스를 통해 외부 디스플레이 장치에 전송할 수 있다. 조작 인터페이스는 버튼, 터치 패드, 마우스 및 키보드와 같은 장치로 구현되거나, 상술한 디스플레이 기 능 및 조작 입력 기능도 함께 수행 가능한 터치 스크린으로도 구현될 수 있다. 여기서, 버튼은 전자 장치 의 본체 외관의 전면부나 측면부, 배면부 등의 임의의 영역에 형성된 기계적 버튼, 터치 패드, 휠 등과 같은 다 양한 유형의 버튼이 될 수 있다. 입출력 인터페이스는 HDMI(High Definition Multimedia Interface), MHL (Mobile High-Definition Link), USB (Universal Serial Bus), DP(Display Port), 썬더볼트(Thunderbolt), VGA(Video Graphics Array) 포트, RGB 포트, D-SUB(D-subminiature), DVI(Digital Visual Interface) 중 어느 하나의 인터페이스일 수 있 다. 입출력 인터페이스는 오디오 및 비디오 신호 중 적어도 하나를 입출력 할 수 있다. 구현 예에 따라, 입출 력 인터페이스는 오디오 신호만을 입출력하는 포트와 비디오 신호만을 입출력하는 포트를 별개의 포트로 포함하거나, 오디오 신호 및 비디오 신호를 모두 입출력하는 하나의 포트로 구현될 수 있다. 한편, 전자 장치는 입출력 인터페이스를 통해 오디오 및 비디오 신호 중 적어도 하나를 외부 장치(예 를 들어, 외부 디스플레이 장치 또는 외부 스피커)에 전송할 수 있다. 구체적으로, 입출력 인터페이스에 포함된 출력 포트가 외부 장치와 연결될 수 있으며, 전자 장치는 오디오 및 비디오 신호 중 적어도 하나를 출력 포트를 통해 외부 장치에 전송할 수 있다. 전자 장치는 마이크를 더 포함할 수 있다. 마이크는 사용자 음성이나 기타 소리를 입력 받아 오디오 데이터로 변환하기 위한 구성이다. 마이크는 활성화 상태에서 사용자의 음성을 수신할 수 있다. 예를 들어, 마이크는 전자 장치의 상측이나 전면 방향, 측면 방향 등에 일체형으로 형성될 수 있다. 마이크는 아날로그 형태의 사용자 음성 을 수집하는 마이크, 수집된 사용자 음성을 증폭하는 앰프 회로, 증폭된 사용자 음성을 샘플링하여 디지털 신호 로 변환하는 A/D 변환회로, 변환된 디지털 신호로부터 노이즈 성분을 제거하는 필터 회로 등과 같은 다양한 구 성을 포함할 수 있다. 전자 장치가 마이크를 통해 수신된 사용자 음성 신호에 대응되는 동작을 수행하는 다양한 실시 예가 있을 수 있다. 일 예로, 전자 장치는 마이크를 통해 수신된 사용자 음성 신호에 기초하여 디스플레이를 제어할 수 있다. 예를 들어, A 컨텐츠를 표시하기 위한 사용자 음성 신호가 수신되면, 전자 장치는 A컨텐츠를 표 시하도록 디스플레이를 제어할 수 있다. 다른 예로, 전자 장치는 마이크를 통해 수신된 사용자 음성 신호에 기초하여 전자 장치와 연결 된 외부 디스플레이 장치를 제어할 수 있다. 구체적으로, 전자 장치는 사용자 음성 신호에 대응되는 동작 이 외부 디스플레이 장치에서 수행되도록 외부 디스플레이 장치를 제어하기 위한 제어 신호를 생성하고, 생성된 제어 신호를 외부 디스플레이 장치에 전송할 수 있다. 여기서, 전자 장치는 외부 디스플레이 장치를 제어 하기 위한 원격 제어 어플리케이션을 저장할 수 있다. 그리고, 전자 장치는 생성된 제어 신호를 블루투스, 와이파이 또는 적외선 중 적어도 하나의 통신 방법을 이용하여 외부 디스플레이 장치에 전송할 수 있다. 예를 들어, A 컨텐츠를 표시하기 위한 사용자 음성 신호가 수신되면, 전자 장치는 A 컨텐츠가 외부 디스플레이 장치에서 표시되도록 제어하기 위한 제어 신호를 외부 디스플레이 장치에 전송할 수 있다. 여기서, 전자 장치 는 스마트폰, AI 스피커 등 원격 제어 어플리케이션을 설치할 수 있는 다양한 단말 장치를 의미할 수 있다. 또 다른 예로, 전자 장치는 마이크를 통해 수신된 사용자 음성 신호에 기초하여 전자 장치와 연 결된 외부 디스플레이 장치를 제어하기 위하여 원격 제어 장치를 이용할 수 있다. 구체적으로, 전자 장치는 사용자 음성 신호에 대응되는 동작이 외부 디스플레이 장치에서 수행되도록 외부 디스플레이 장치를 제어하 기 위한 제어 신호를 원격 제어 장치에 전송할 수 있다. 그리고, 원격 제어 장치는 전자 장치로부터 수신 된 제어 신호를 외부 디스플레이 장치에 전송할 수 있다. 예를 들어, A 컨텐츠를 표시하기 위한 사용자 음성 신 호가 수신되면, 전자 장치는 A 컨텐츠가 외부 디스플레이 장치에서 표시되도록 제어하기 위한 제어 신호를 원격 제어 장치에 전송하고, 원격 제어 장치는 수신된 제어 신호를 외부 디스플레이 장치에 전송할 수 있다. 도 5는 일 실시 예에 따른 이미지 변환 동작을 설명하기 위한 흐름도이다. 도 5를 참조하면, 전자 장치는 입력 이미지를 획득할 수 있다 (S505). 그리고, 전자 장치는 입력 이미지를 복수의 영역으로 구분할 수 있다 (S510). 여기서, 전자 장치는 입력 이미지에서 식별되 는 오브젝트에 기초하여 입력 이미지를 복수의 영역으로 구분할 수 있다. 예를 들어, 전자 장치는 입 력 이미지에 배경 오브젝트 및 사물 오브젝트를 구분하고, 배경 오브젝트가 포함된 영역과 사물 오브젝트가 포함된 영역을 식별할 수 있다. 또한, 전자 장치는 구분된 복수의 영역 각각에 대응되는 특성 정보를 획득할 수 있다 (S515). 여기서, 특 성 정보는 복수의 영역에 포함된 오브젝트에 대응되는 특성을 포함할 수 있다. 예를 들어, 특성 정보는 배경 오 브젝트와 관련된 정보 또는 사물 오브젝트와 관련된 정보일 수 있다. 또한, 전자 장치는 특성 정보에 대응되는 스타일 정보를 복수의 영역별로 식별할 수 있다 (S520). 여기서, 스타일 정보는 이미지 변환 동작에 이용되는 스타일을 의미할 수 있다. 예를 들어, 스타일 정보는 질감, 선명도, 명암 중 적어도 하나를 의미할 수 있다. 예를 들어, 스타일 정보는 질감(텍스처)의 강도, 선명도의 강 도, 명암의 강도 중 적어도 하나를 포함할 수 있다. 예를 들어, 제1 스타일 정보는 제1 강도로 텍스처 처리를 수행해야 함을 나타내는 정보를 포함할 수 있으며, 제2 스타일 정보는 제2 강도로 텍스처 처리를 수행해야 함을 나타내는 정보를 포함할 수 있다. 또한, 전자 장치는 식별된 스타일 정보에 기초하여 출력 이미지를 획득할 수 있다 (S525). 여기서, 전 자 장치는 식별된 스타일 정보에 기초하여 입력 이미지를 출력 이미지로 변환할 수 있다. 도 6은 도 5의 이미지 변환 동작을 구체적으로 설명하기 위한 흐름도이다. 도 6을 참조하면, 전자 장치는 입력 이미지를 획득할 수 있다 (S605). 그리고, 전자 장치는 입력 이미지를 제1 영역 및 제2 영역으로 구분할 수 있다 (S610). 그리고, 전자 장치는 제1 영역에 대응되 는 제1 특성 정보 및 제2 영역에 대응되는 제2 특성 정보를 획득할 수 있다 (S615). 또한, 전자 장치는 제1 특성 정보에 대응되는 제1 스타일 정보를 획득하고, 제2 특성 정보에 대응되는 제2 스타일 정보를 획득(또는 식별)할 수 있다 (S620). 또한, 전자 장치는 제1 스타일 정보에 기초하여 제1 영역에 대응되는 이미지를 고해상도 이미지로 변환하 고, 제2 스타일 정보에 기초하여 제2 영역에 대응되는 이미지를 고해상도의 이미지로 변환할 수 있다 (S625). 또한, 전자 장치는 각 영역에 대응되는 고해상도 이미지들에 기초하여 출력 이미지를 획득할 수 있다 (S630). 구체적으로, 전자 장치는 변환된 제1 영역에 대응되는 고해상도 이미지 및 제2 영역에 대응되는 고해상도 이미지를 결합하여 전체 이미지인 출력 이미지를 획득할 수 있다. 도 7은 입력 이미지를 복수의 영역으로 구분하는 동작을 설명하기 위한 도면이다. 도 7을 참조하면, 전자 장치는 입력 이미지를 분석하여 입력 이미지를 복수의 영역으로 구분할 수 있다. 일 실시 예에 따라, 전자 장치는 입력 이미지에 포함된 오브젝트를 식별할 수 있고, 식별된 오브젝트 의 위치에 기초하여 입력 이미지를 복수의 영역으로 구분할 수 있다. 다른 실시 예에 따라, 전자 장치는 입력 이미지에 포함된 외곽선(또는 경계선)을 식별하고, 식별된 외 곽선(또는 경계선)에 기초하여 입력 이미지를 복수의 영역으로 구분할 수 있다. 입력 이미지를 복수의 영역으로 구분하는 이유는 영역 별로 다른 스타일의 이미지 변환을 수행하기 위함이 다. 예를 들어, 배경 영역은 질감 효과가 거의 필요하지 않고 사물 영역은 질감 효과가 필요할 수 있다. 여기서, 배경 영역에도 질감 효과를 추가하지 않아야 더 높은 완성도를 갖는 이미지를 획득할 수 있다. 따라서, 전자 장치는 서로 다른 영역에 서로 다른 스타일의 이미지 변환 동작을 수행할 수 있다.여기서, 전자 장치는 입력 이미지의 전체 영역을 제1 영역 및 제2 영역으로 구분할 수 있다. 한편, 구현 예에 따라, 전자 장치는 제1 영역을 식별한 이후, 제1 영역 이외의 영역을 제2 영역 으로 식별할 수 있다. 도 8은 이미지 변환 모델이 학습되는 동작을 설명하기 위한 흐름도이다. 도 8을 참조하면, 전자 장치는 저해상 이미지 및 고해상 이미지를 획득할 수 있다 (S805). 여기서, 저해상 이미지는 이미지 변환 동작에서 학습 입력 이미지로 이용될 수 있다. 그리고, 고해상 이미지는 이미지 변환 동 작에서 손실값을 획득하는 기준 이미지로 이용될 수 있다. 또한, 전자 장치는 저해상 이미지를 복수의 영역으로 구분할 수 있다 (S810). 그리고, 전자 장치는 복수의 영역 각각에 대응되는 특성 정보를 획득할 수 있다 (S815). 그리고, 전자 장치는 특성 정보에 대응 되는 스타일 정보를 복수의 영역별로 식별할 수 있다 (S820). 또한, 전자 장치는 S820 단계에서 식별된 스타일 정보에 기초하여 저해상 이미지를 출력 이미지로 변환할 수 있다. 여기서, 출력 이미지로 변환되는 동작은 S810단계에서 구분된 영역별로 수행될 수 있다. 여기서, 출력 이미지는 이미지 변환 모델의 목적에 부합하는 추정 이미지일 수 있다. 이미지 변환 모델이 이미지의 초해상화를 위한 모델이라면, 출력 이미지는 이미지 변환 모델에 의해 생성된 고해상도의 이미지일 수 있 다. 여기서, 전자 장치는 기 저장된 손실 함수에 기초하여 S825단계에서 획득한 출력 이미지 및 S805단계에서 획득한 고해상 이미지 사이의 손실값을 획득할 수 있다. 고해상 이미지는 이미 해상도가 높은 정답 이미지에 해 당하므로, 이미지 변환 모델을 통한 이미지 변환이 얼마나 정확히 이루어졌는지 기준이 될 수 있다. 따라 서, 전자 장치는 이미지 변환 모델에 의해 획득된 출력 이미지가 고해상 이미지와 얼마나 일치하 는지 판단하기 위하여 손실값을 획득할 수 있다. 여기서, 전자 장치는 손실값을 최소로 하기 위한 스타일 식별 모델의 이미지 변환 파라미터를 결정할 수 있다 (S835). 그리고, 전자 장치는 학습을 종료시킬 것인지 식별할 수 있다 (S840). 학습이 종료되었다고 판단되면 (S840-Y), 전자 장치는 S835 단계에서 결정된 파라미터를 확정할 수 있다. 여기서, 학습 종료를 식별하는 기준은 손실값일 수 있다. 예를 들어, 손실값이 임계값 미만이면, 전자 장치는 학습을 종료할 수 있다. 학습이 종료되지 않은 것으로 식별되면 (S840-N), 전자 장치는 새로운 저해상 이미지 및 새로운 고해상 이 미지를 획득하여 S805 내지 S840 단계를 반복할 수 있다. 한편, 구현 예에 따라, 전자 장치는 동일한 저해상 이미지 및 동일한 고해상 이미지를 변경된 파라미터에 의해 이미지 변환 동작을 수행할 수 있다. 구체적으로, 전자 장치는 이미지 변환 모델에 이용되었던 파라미터를 변경할 수 있다. 그리고, 전자 장치는 변경된 파라미터에 기초하여 다시 S805 내지 S840 단계 를 수행할 수 있다. 도 9는 본 개시의 일 실시 예에 따른 이미지 변환 모델을 설명하기 위한 도면이다. 도 9를 참조하면, 전자 장치는 이미지 변환 모델, 스타일 식별 모델 및 손실값 계산 모델 을 포함할 수 있다. 여기서, 이미지 변환 모델은 입력 이미지를 획득하여 출력 이미지로 변환할 수 있다. 여기서, 스타일 식별 모델은 입력 이미지에 대응되는 스타일 정보를 결정하고, 결정된 스타 일 정보를 스타일 식별 모델에 전송할 수 있다. 이미지 변환 모델은 스타일 식별 모델에서 전송 한 스타일 정보에 기초하여 입력 이미지를 출력 이미지로 변환할 수 있다. 여기서, 손실값 계산 모델은 출력 이미지 및 기준 이미지에 기초하여 손실값을 획득할 수 있다. 여기서, 기준 이미지는 입력 이미지의 고해상 이미지일 수 있다. 손실값 계산 모델은 손실 함수 를 포함할 수 있다. 여기서, 손실값 계산 모델은 손실 함수에 기초하여 출력 이미지 및 기 준 이미지 사이의 손실값을 계산할 수 있다. 여기서, 손실값은 평균 제곱 오차(Mean Squared Error, MSE), 인지 손실 또는 적대 손실 중 적어도 하나를 포함 할 수 있다.여기서, 전자 장치는 사전 학습된 분류 네트워크의 레이어 결과를 특징 공간으로 이용하여 인지 손실을 획 득할 수 있다. 그리고, 전자 장치는 특징 공간에서의 에러(거리)를 감소함으로써 이미지 변환 모델을 최적화할 수 있다. 이미지 변환 모델에 포함된 각 레이어는 영상의 복원 스타일 또는 텍스쳐 생성 모양에 따라 상이하게 배치될 수 있다. 또한, 손실값 계산 모델은 스타일 식별 모델로부터 수신되는 스타일 정보에 기초하여 다양한 종류의 손실값을 다르게 가중합할 수 있다. 그리고, 전자 장치는 다양한 복원 스타일을 단일의 이미지 변환 모델 이 초해상화 동작에 이용할 수 있도록 학습할 수 있다. 도 10은 이미지 변환 동작에 이용하는 계산을 설명하기 위한 도면이다. 도 10을 참조하면, 이미지 변환 모델은 수학식에 기초하여 이미지를 변환할 수 있다. 구체적으로, I^-HR은 출력 이미지를 의미할 수 있다. 그리고, G_θ는 θ를 파라미터로 하는 이미지 변환 함수를 의미할 수 있다. I-LR은 입력 이미지를 의미할 수 있다. T는 스타일 정보를 의미할 수 있다. 결과적으로 출력 이미 지(I^-HR)는 이미지 변환 함수(G_θ)에 의해 생성된 이미지일 수 있고, 이미지 변환 함수(G_θ)는 스타일 정보 (T)에 기초하여 입력 이미지(I-LR)를 변환할 수 있다. 여기서, 수학식은 이미지 변환 모델의 θ 파라미터를 결정하기 위한 수학식일 수 있다. θ^는 최종 적으로 결정된 파라미터를 의미할 수 있다. Ii^-HR은 i번째 출력 이미지를 의미하고, Ii-HR은 i번째 기준 이미지를 의미하고, Ti는 i번째 스타일 정보를 의미할 수 있다. 여기서, arg min(f(θ))는 f(θ)를 최소로 만드는 θ를 결정하는 함수를 의미할 수 있다. L(a,b)는 a와 b 사이 의 손실값을 계산하는 함수를 의미할 수 있다. 따라서, L(Ii^-HR, Ii-HR |Ti)은 스타일 정보(Ti)를 고려하여 출 력 이미지(Ii^-HR) 및 기준 이미지(Ii-HR) 사이의 손실값을 계산하는 함수를 의미할 수 있다. 도 11은 본 개시의 일 실시 예에 따른 학습 동작에 이용하는 계산을 설명하기 위한 도면이다. 도 11을 참조하면, 수학식은 이미지 변환 모델의 파라미터를 결정하는 학습 동작에서 손실값 계산 동작에 이용될 수 있다. 전자 장치는 수학식을 이용하여 출력 이미지 및 기준 이미지 사이 의 손실값을 획득할 수 있다. 여기서, 전자 장치는 손실값 계산 모델을 이용하여 손실값을 획득할 수 있다. 수학식을 참조하면, O는 손실값 함수를 의미할 수 있다. O는 λ_rec*L_rec+λ_per*L_per+ λ_adv*L_adv 일 수 있다. 여기서, L_rec은 재구성 손실(reconstruction loss)을 의미하고, L_per는 인지 손실(perceptual loss)을 의미 하고, L_adv는 적대 손실(adversarial loss)을 의미할 수 있다. 그리고, λ_rec는 재구성 손실의 가중치를 의미 하고, λ_per는 인지 손실의 가중치를 의미하고, λ_adv는 적대 손실의 가중치를 의미할 수 있다. λ_rec, λ _per 및 λ_adv는 서로 상이한 값을 가질 수 있다. λ_rec, λ_per 및 λ_adv는 0 내지 1 사이의 값을 가질 수 있으며, λ_rec, λ_per 및 λ_adv의 합은 1일 수 있다. 여기서, 재구성 손실(L_rec)은 수학식에 기초하여 계산될 수 있다. 여기서, E는 평균값을 계산하는 함수 를 의미할 수 있다. 여기서, Ii^-HR은i번째 출력 이미지를 의미할 수 있다. 여기서, Ii-HR은 i번째 기준 이미지 일 수 있다. 여기서, ||Ii^-HR-Ii-HR||_1은 i번째 출력 이미지와 i번째 기준 이미지의 유클리디언 거리 (Euclidean Distance) 차이값의 절대값들을 합한 값일 수 있다. 그리고, E[||Ii^-HR-Ii-HR||_1]은 i번째 출력 이미지와 i번째 기준이미지의 차이값들의 평균값을 의미할 수 있다. 여기서, 인지 손실(L_per)은 수학식에 기초하여 계산될 수 있다. 여기서, l은 스타일을 의미할 수 있다. 여기서, λ_l은 스타일 정보를 적용하는 함수와 관련된 가중치일 수 있다. 여기서, w_l(T)은 스타일 정보 자체 와 관련된 가중치일 수 있다. 그리고, w_l(T)은 학습 중 랜덤 변수인 스타일 정보(또는 스타일맵)에 기초하여 변경될 수 있다. 따라서, λ_l은 스타일 정보에 기초하여 이미지 변환 동작을 수행하는 복수의 함수 중 특정 함수에 대응되는 가 중치를 의미할 수 있다. 그리고, w_l(T)은 복수의 스타일 중 특정 스타일에 대응되는 가중치를 의미할 수 있다. 여기서, 지각 손실은 서로 다른 수준의 기능 공간에서 여러 인지 손실의 가중 합계일 수 있다. 여기서, L_l은 스타일 l의 손실값을 의미할 수 있으며 수학식에 기초하여ㅏ 계산될 수 있다. 여기서, φ _l은 스타일 l에 기초하여 이미지를 변환하는 함수를 의미할 수 있다. 여기서, φ_l(Ii^-HR)는 i번째 출력 이미지(Ii^-HR)를 스타일 l에 대응되는 특성 공간으로 변환하는 함수를 의미할 수 있다. 여기서, φ_l(Ii-HR)는 i번 째 기준 이미지(Ii-HR)를 스타일 l에 대응되는 특성 공간으로 변환하는 함수를 의미할 수 있다. ||Ii^-HR-Ii- HR||_2는 i번째 출력 이미지(Ii^-HR) 및 i번째 기준 이미지(Ii-HR)의 차이값의 제곱값들을 합한값일 수 있다. 그리고, E[||Ii^-HR-Ii-HR||_2]는 i번째 출력 이미지(Ii^-HR) 및 i번째 기준 이미지(Ii-HR)의 차이값의 제곱값 들의 합을 제곱근하는 함수를 의미할 수 있다. 여기서, 수학식은 L1 norm을 이용하고, 수학식는 L2 norm을 이용하는 계산식일 수 있다. 여기서, 적대 손실(L_adv)은 수학식에 기초하여 계산될 수 있다. 여기서, E_I^-HR(f(x))은 출력 이미지들 에 대한 f(x)의 평균값을 의미할 수 있다. 여기서, f(x)는 log(D~(I^-HR)일 수 있다. 여기서, E_I-HR(f(x))은 기준 이미지들에 대한 f(x)의 평균값을 의미할 수 있다. 여기서, f(x)는 log(D~(I-HR)일 수 있다. 또한, D~(I-HR)은 수학식에 기초하여 계산될 수 있으며, D~(I-HR)은 진짜 이미지(기준 이미지)를 진짜(기 준 이미지)로 식별할 확률을 의미할 수 있다. 여기서, D~(I^-HR)은 수학식에 기초하여 계산될 수 있으며, D~(I^-HR)는 가짜 이미지(출력 이미지)를 가짜(출력 이미지)로 식별할 확률을 의미할 수 있다. 여기서, C(I^- HR)은 판별기를 통해 획득되는 출력 이미지에 관련된 값을 의미할 수 있다. 그리고, C(I-HR)은 판별기를 통해 획득되는 기준 이미지에 관련된 값을 의미할 수 있다. 도 12는 본 개시의 다른 실시 예에 따른 이미지 변환 모델을 구체적으로 설명하기 위한 도면이다. 도 12를 참조하면, 전자 장치는 이미지 변환 모델, 스타일 식별 모델 및 손실값 계산 모델(10 3)을 포함할 수 있다. 일반적인 이미지 변환 동작에서는 이미지 변환 모델 및 스타일 식별 모델이 이용될 수 있으며, 이미 지 학습 동작에서는 이미지 변환 모델, 스타일 식별 모델 및 손실값 계산 모델이 모두 이용될 수 있다. 이미지 변환 모델은 저해상도 처리부, 업스케일링부 및 고해상도 처리부를 포함할 수 있다. 여기서, 저해상도 처리부는 SR(Super Resolution) 분기(branch)에 해당할 수 있으며 적어도 하나의 컨벌 루션 레이어 및 복수의 기본 블록을 포함할 수 있다. 여기서, 저해상도 처리부는 입력 이미지가 업스 케일링 되기 이전에 저해상도 상태에서 이미지를 변환할 수 있다. 여기서, 업스케일링부는 변환된 저해상 도의 이미지를 업스케일링할 수 있다. 여기서, 고해상도 처리부는 업스케일링된 이미지를 적어도 하나의 컨벌루션 레이어 및 ReLU 레이어에 기초하여 변환할 수 있다. 여기서, 저해상도 처리부는 SFT(Spatial Feature Transform) 레이어가 포함된 RRDB(Residual in Residual Dense Block)일 수 있다. 여기서, SFT 레이어 는 스타일 정보(T)에 기초하여 변조 파라미터를 출력하는 매핑 기능을 학습하는데 이용될 수 있다. 따라서, SFT 레이어는 목표의 최적화를 위한 필수적 레이어일 수 있다. 여기서, 스타일 식별 모델은 사용자 제어부, 스타일 식별 네트워크, 스타일맵 생성부 및 조건 네트워크를 포함할 수 있다. 여기서, 사용자 제어부는 직접 입력 이미지의 변환에 이 용될 스타일을 사용자로부터 선택 받을 수 있다. 여기서, 스타일 식별 네트워크는 스타일맵 생성을 위한 네트워크로서 입력 이미지에 적합한 스타일을 식별할 수 있다. 여기서, 스타일맵 생성부는 사용자 제 어부 또는 스타일 식별 네트워크 중 적어도 하나를 통해 결정된 스타일에 기초하여 입력 이미지 에 대응되는 스타일맵을 생성할 수 있다. 여기서, 조건 네트워크는 생성된 스타일맵을 이미지 변환 모델에 전송할 수 있다. 구체적으로, 조건 네트워크는 생성된 스타일맵을 이미지 변환 모델에 포함된 기본 블록에 전달할 수 있다. 구체적으로, 조건 네트워크는 스타일맵을 이미지 변환 모델의 피쳐 모듈레이션 레이어(예를 들어, SFT(Spatial Feature Transform)) 레이어에 전송할 수 있다. 한편, 조건 네트워크에 포함된 모든 컨볼루션 레이어는 다른 영역의 간섭을 피하기 위해 1*1 커널을 이용할 수 있다. 여기서, 스타일맵은 객체, 배경, 경계를 구분하는 정보를 포함할 수 있으며, 객체, 배경, 경계 등에 적용될 스 타일 정보를 포함할 수 있다. 한편, 다른 실시 예에 따라, 전자 장치는 스타일 정보를 이미지 변환 모델 에 반영하기 위하여 입력 이미지에 부가 정보를 연결(concatenation)할 수 있다. 여기서, 스타일맵은 SFT(Spatial Feature Transform) 레이어를 통해 복수의 영역의 가장 자리 및 텍스처 복구 스타일을 제어하는데 이용될 수 있다. 따라서, 이미지 변환 모델은 스타일 식별 모델에서 생성된 스타일맵을 이용하여 출력 이미지를 생성할 수 있다. 한편, 상술한 동작은 일반 이미지 변환 동작에서 수행되는 동작일 수 있다. 한편, 이미지 변환 모델에서 이용되는 다양한 파라미터를 결정하기 위해 학습 동작이 필요할 수 있다. 여 기서, 학습 동작을 위하여 손실값 계산 모델이 이용될 수 있다. 여기서, 손실값 계산 모델은 손실 함수(1208, loss function), 판별기(1209, discriminator) 및 VGG 네트 워크를 포함할 수 있다. 여기서, 손실 함수는 출력 이미지 및 기준 이미지 사이의 손실값 을 획득할 수 있다. 여기서, 판별기는 출력 이미지 및 기준 이미지에 대응되는 판별기 손실을 획 득할 수 있다. 여기서, VGG 네트워크는 복수의 스타일에 대응되는 스타일 적용 함수를 의미할 수 있다. 여기서, 스타일 식별 모델은 생성된 스타일맵을 손실값 계산 모델에 전달할 수 있다. 그리고, 손실값 계산 모 델은 스타일 식별 모델로부터 획득한 스타일맵에 기초하여 손실값을 계산할 수 있다. 한편, 손실값 계산 모델은 스타일 식별 모델에서 전송하는 스타일 정보(또는 스타일맵)에 기초하여 손실 함수의 계산을 변경할 수 있다. 또한, 손실값 계산 모델은 스타일 식별 모델에서 전송하는 스타일 정보(또는 스타일맵)에 기초하여 다양한 레벨의 특성 공간에서 평균 제곱 오차(Mean Squared Error, MSE), 인지 손실 또는 지각 손실 중 적어도 하나를 조합하여 손실값을 획득할 수 있다. 도 13은 본 개시의 일 실시 예에 따른 이미지 변환 모델에 이용되는 기본 블록을 설명하기 위한 도면이다. 도 13을 참조하면, 저해상도 처리부에 포함되는 기본 블록은 복수의 단위 블록(1301, 1302, 1303, 1304) 및 컨벌루션 레이어를 포함하는 RRDB(Residual in Residual Dense Block)일 수 있다. 여기서, 단위 블록 (1301, 1302, 1303, 1304)은 SFT(Spatial Feature Transform) 레이어, 컨벌루션 레이어 및 LRelu(Leaky ReLU) 레이어를 포함할 수 있다. 여기서, 각 SFT 레이어는 스타일 식별 모델에서 생성한 스타일 정보(또는 스타일맵)를 수신할 수 있다. 도 14는 본 개시의 일 실시 예에 따른 스타일 식별 모델을 설명하기 위한 도면이다. 도 14를 참조하면, 스타일맵 생성부는 입력 이미지에 기초하여 스타일맵을 생성할 수 있다. 구체적으로, 스타일맵 생성부는 컨벌루션 레이어(1402, 1404) 및 복수의 기본 블록(1403-1,1403-2,1403- n)을 포함할 수 있다. 스타일맵은 스타일 적용 여부에 따라 흰색 및 검은색 사이의 색상으로 표현될 수 있다. 예를 들어, 스타일맵에서 흰색에 해당하는 픽셀은 질감 스타일을 전혀 적용하지 않는 픽셀일 수 있 다. 또한, 스타일맵에서 검은색에 해당하는 픽셀은 질감 스타일을 최대한으로 적용하는 픽셀일 수 있다. 또한, 스타일맵에서 회색에 해당하는 픽셀은 질감 스타일을 중간 정도로 적용하는 픽셀일 수 있다. 한편, 스타일맵을 색상으로 표현하였으나, 스타일맵은 0과 1 사이의 값을 가지는 행렬 데이터로 표 현될 수 있다. 도 15는 본 개시의 다른 실시 예에 따른 학습 동작에 이용하는 계산을 설명하기 위한 도면이다. 도 15를 참조하면, 판별기 손실(L_dis)은 수학식에 기초하여 계산될 수 있다. 여기서, E_I-HR(f(x))은 기 준 이미지들에 대한 f(x)의 평균값을 의미할 수 있다. 여기서, f(x)는 log(D~(I-HR)일 수 있다. 여기서, E_I^- HR(f(x))은 출력 이미지들에 대한 f(x)의 평균값을 의미할 수 있다. 여기서, f(x)는 log(D~(I^-HR)일 수 있다. 수학식 및 수학식은 도 11의 수학식 및 수학식에 대응될 수 있다. 여기서, D~(I- HR)은 수학식에 기초하여 계산될 수 있으며, D~(I-HR)은 진짜 이미지(기준 이미지)를 진짜(기준 이미지) 로 식별할 확률을 의미할 수 있다. 여기서, D~(I^-HR)은 수학식에 기초하여 계산될 수 있으며, D~(I^- HR)는 가짜 이미지(출력 이미지)를 가짜(출력 이미지)로 식별할 확률을 의미할 수 있다. 여기서, C(I^-HR)은 판 별기를 통해 획득되는 출력 이미지에 관련된 값을 의미할 수 있다. 그리고, C(I-HR)은 판별기를 통해 획득되는 기준 이미지에 관련된 값을 의미할 수 있다. 도 16은 본 개시의 다양한 실시 예에 따른 전자 장치의 이미지 변환 동작의 효과를 설명하기 위한 도면이다. 도 16을 참조하면, 그래프는 x축이 PSNR(Peak Signal-to-Noise Ratio)로 하고 y축이 LPIPS(Learned Perceptual Image Patch Similarity)일 수 있다. PSNR은 픽셀 단위로 값의 차이를 나타내는 값일 수 있으며, PSNR이 높을수록 인지 손실이 낮음을 의미할 수 있다. 또한, LPIPS는 인지 유사도를 의미할 수 있으며, 값이 낮 을수록 성능이 좋을 수 있다. SRResNET, NatSR, SRGAN, SFTGAN, ESRGAN, SPSR은 다양한 초해상 모델을 의미할 수 있다. 여기서, FxSR-D는 이 미지 변환 모델이 제1 스타일 정보(예를 들어, 질감 효과를 최소로 반영하는 스타일)에 기초하여 초해상화하는 실시 예일 수 있다. 여기서, FxSR-P는 이미지 변환 모델이 제2 스타일 정보(예를 들어, 질감 효과를 최대로 반 영하는 스타일)에 기초하여 초해상화하는 실시 예일 수 있다. 그래프에서 FxSR-D 및 FxSR-P가 좌측 그리고 하측에 위치한다는 점에서, 다른 초해상화 모델보다 성능이 우수하다고 평가될 수 있다. 도 17은 일 실시 예에 따른 전자 장치의 이미지 변환 동작을 비교하기 위한 도면이다. 도 17을 참조하면, 입력 이미지(1710, 1720, 1730)에 대하여 다양한 이미지 변환 방식을 적용할 수 있다. HR에 해당하는 이미지(1711, 1721, 1731)는 기준 이미지를 의미할 수 있다. FxSR-D에 해당하는 이미지(1712, 1722, 1732)는 질감 효과를 최소로 반영하는 스타일(T=0)로 초해상화가 수행된 이미지일 수 있다. 또한, FxSR-E에 해 당하는 이미지(1713,1723,1733)는 질감 효과를 중간으로 반영하는 스타일(T=0.5)로 초해상화가 수행된 이미지일 수 있다. 또한, FxSR-P에 해당하는 이미지(1714,1724,1734)는 질감 효과를 최대로 반영하는 스타일(T=1)로 초해 상화가 수행된 이미지일 수 있다. 여기서, 전자 장치는 입력 이미지에서 특정 영역(1710-1)을 식별하고, 특정 영역(1710-1)에 대응되 는 이미지(예를 들어, 건물 오브젝트를 포함하는 이미지)를 초해상화할 수 있다. 이미지는 특정 영역 (1710-1)에 대응되는 기준 이미지일 수 있다. 이미지(1712, 1713, 1714)는 질감 효과를 상이하게 반영하는 스타 일로 초해상화가 수행된 이미지일 수 있다. 이미지가 기본 이미지와 가장 유사할 수 있다. 여기서, 전자 장치는 입력 이미지에서 특정 영역(1720-1)을 식별하고, 특정 영역(1720-1)에 대응되 는 이미지(예를 들어, 스크래치가 나 있는 벽돌 오브젝트를 포함하는 이미지)를 초해상화할 수 있다. 이미지 는 특정 영역(1720-1)에 대응되는 기준 이미지일 수 있다. 이미지(1722, 1723, 1724)는 질감 효과를 상이 하게 반영하는 스타일로 초해상화가 수행된 이미지일 수 있다. 이미지가 기본 이미지와 가장 유사 할 수 있다. 여기서, 전자 장치는 입력 이미지에서 특정 영역(1730-1)을 식별하고, 특정 영역(1730-1)에 대응되 는 이미지(예를 들어, 식물 오브젝트를 포함하는 이미지)를 초해상화할 수 있다. 이미지는 특정 영역 (1730-1)에 대응되는 기준 이미지일 수 있다. 이미지(1732, 1733, 1734)는 질감 효과를 상이하게 반영하는 스타 일로 초해상화가 수행된 이미지일 수 있다. 이미지가 기본 이미지와 가장 유사할 수 있다. 도 18은 다른 실시 예에 따른 전자 장치의 이미지 변환 동작을 비교하기 위한 도면이다. 도 18을 참조하면, 이미지는 기준 이미지일 수 있으며, 이미지(1800-2)는 제1 영역에 대응되는 이 미지일 수 있고, 이미지(1800-3)은 제2 영역에 대응되는 이미지일 수 있다. 스타일맵(1810-1, 1820-1, 1830-1, 1840-1, 1850-1)은 스타일 효과(예를 들어, 질감 효과)를 어느 정도 반영할 지 여부를 나타내는 데이터를 의미할 수 있다. 여기서 검은색은 질감 효과를 최소로 반영하는 데이터를 의미하 고, 흰색은 질감 효과를 최대로 반영하는 데이터를 의미할 수 있다. 회색은 질감 효과를 중간으로 반영하는 데 이터를 의미할 수 있다. 따라서, 회색의 정도에 따라 질감 효과가 반영되는 정도가 상이할 수 있다. 또한, 이미지는 전체 영역에 대하여 질감 효과를 최소로 반영하는 스타일(T=0)로 초해상화가 수행된 이미 지일 수 있다. 또한, 이미지(1810-1)는 스타일맵을 의미할 수 있다. 이미지(1810-2)는 이미지 중 제1 영 역에 대응되는 이미지일 수 있다. 이미지(1810-3)는 이미지 중 제2 영역에 대응되는 이미지 일 수 있다. 또한, 이미지는 전체 영역에 대하여 질감 효과를 최대로 반영하는 스타일(T=1)로 초해상화가 수행된 이미 지일 수 있다. 또한, 이미지(1820-1)는 스타일맵을 의미할 수 있다. 이미지(1820-2)는 이미지 중 제1 영 역에 대응되는 이미지일 수 있다. 이미지(1820-3)는 이미지 중 제2 영역에 대응되는 이미지 일 수 있다. 또한, 이미지는 영역 별로 질감 효과를 다르게 반영하는 스타일로 초해상화가 수행된 이미지일 수 있다. 또한, 이미지(1830-1)는 스타일맵을 의미할 수 있다. 이미지(1830-2)는 이미지 중 제1 영역에 대응 되는 이미지일 수 있다. 이미지(1830-3)는 이미지 중 제2 영역에 대응되는 이미지일 수 있다. 또한, 이미지는 일 실시 예에 따라 사용자가 인위적으로 영역을 지정하여 질감 효과를 다르게 반영하는 스타일로 초해상화가 수행된 이미지일 수 있다. 또한, 이미지(1840-1)는 스타일맵을 의미할 수 있다. 이미지(1840-2)는 이미지 중 제1 영역에 대응되는 이미지일 수 있다. 이미지(1840-3)는 이미지 중 제2 영역에 대응되는 이미지일 수 있다. 또한, 이미지는 다른 실시 예에 따라 사용자가 인위적으로 영역을 지정하여 질감 효과를 다르게 반영하는 스타일로 초해상화가 수행된 이미지일 수 있다. 또한, 이미지(1850-1)는 스타일맵을 의미할 수 있다. 이미지 (1850-2)는 이미지 중 제1 영역에 대응되는 이미지일 수 있다. 이미지(1850-3)는 이미지 중 제2 영역에 대응되는 이미지일 수 있다. 전체 영역에 대하여 질감 효과를 최대로 반영한 이미지 및 스타일 식별 네트워크(도12의 1205)를 통해 자 동으로 스타일맵을 결정하여 생성된 이미지가 기본 이미지에 가장 유사할 수 있다. 다만, 이미지의 자연스러움을 고려하면 이미지보다 이미지가 기본 이미지에 더 유사할 수 있다. 도 19는 또 다른 실시 예에 따른 전자 장치의 이미지 변환 동작을 비교하기 위한 도면이다. 도 19를 참조하면, 이미지(1910, 1920)는 입력 이미지일 수 있고, 특정 영역(1910-1, 1920-1)을 포함할 수 있다. 이미지(1910-2, 1920-2)는 스타일 식별 네트워크(도12의 1205)를 통해 자동으로 스타일맵을 결정하는 실 시 예에 따라 생성된 스타일맵을 의미할 수 있다. 이미지(1910-3, 1920-3)는 사용자 제어부(도 12의 1204)를 통 해 사용자가 직접 스타일맵을 결정하는 실시 예에 따라 생성된 스타일맵을 의미할 수 있다. 여기서, 이미지(1911, 1912, 1913, 1914, 1915, 1916)는 입력 이미지의 전체 영역 중 특정 영역(1910- 1)에 대응되는 이미지일 수 있다. 이미지(1921, 1922, 1923, 1924, 1925, 1926)는 입력 이미지의 전체 영역 중 특정 영역(1920-1)에 대응되는 이미지일 수 있다. 이미지(1911, 1921)는 입력 이미지(1910, 1920)에 대응되는 기준 이미지일 수 있다. 또한, 이미지(1912, 192 2)는 전체 영역에 대하여 질감 효과를 최대로 반영하는 스타일(T=0)로 초해상화가 수행된 이미지일 수 있다. 또 한, 이미지(1913, 1923)는 영역 별로 질감 효과를 다르게 반영하는 스타일로 초해상화가 수행된 이미지일 수 있 다. 또한, 이미지(1914, 1924)는 사용자가 인위적으로 영역을 지정하여 질감 효과를 다르게 반영하는 스타일로 초해상화가 수행된 이미지일 수 있다. 또한, 이미지(1915, 1925)는 이미지(1910-2, 1920-2)의 전체 영역 중 특 정 영역(1910-1, 1920-1)에 대응되는 이미지일 수 있다. 또한, 이미지(1916, 1926)는 이미지(1910-3, 1920- 3)의 전체 영역 중 특정 영역(1910-1, 1920-1)에 대응되는 이미지일 수 있다. 한편, 여기서, 이미지(1910-2, 1910-3, 1915, 1916, 1920-2, 1920-3, 1925, 1926)는 스타일맵 이미지를 의미할 수 있다. 전체 영역에 대하여 질감 효과를 최대로 반영한 이미지(1912,1922) 및 스타일 식별 네트워크(도12의 1205)를 통 해 자동으로 스타일맵을 결정하여 질감 효과를 반영한 이미지(1913,1923)가 기본 이미지(1911, 1922)와 가장 유 사할 수 있다. 다만, 이미지의 자연스러움을 고려하면 이미지(1912,1922)보다 이미지(1913,1923)가 더 기본 이 미지(1911, 1922)에 가까울 수 있다. 또한, 스타일 식별 네트워크(도12의 1205)를 통해 자동으로 스타일맵을 결정하여 생성된 스타일맵 이미지(1915, 1925)가 사용자가 직접 생성한 스타일맵 이미지(1916, 1926)보다 더 정교할 수 있다. 도 20은 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 도면이다. 도 20을 참조하면, 임계 해상도 미만의 저해상도 입력 이미지로부터 임계 해상도 이상의 고해상도 출력 이미지 를 획득하도록 학습된 인공 지능 모델을 저장하는 전자 장치의 제어 방법은 저해상도 입력 이미지에서 제1 특성 정보를 포함하는 제1 영역 및 제2 특성 정보를 포함하는 제2 영역을 식별하는 단계 (S2005), 제1 영역에 대해 제1 강도의 텍스처 처리를 수행한 제1 이미지 및 제2 영역에 대해 제1 강도와 상이한 제2 강도의 텍스처 처리를 수행한 제2 이미지를 획득하는 단계 (S2010), 제1 이미지 및 제2 이미지를 각각 업스케일링하는 단계(S2015) 및 업스케일링된 제1 및 제2 이미지를 결합하여 고해상도 이미지를 획득하는 단계(S2020)를 포함할 수 있다. 한편, 제1 특성 정보는 오브젝트 영역 정보를 포함할 수 있고 제2 특성 정보는 백그라운드 영역 정보를 포함할 수 있고, 제1 영역 및 제2 영역을 식별하는 단계(S2005)는 오브젝트 영역 정보를 포함하는 영역을 제1 영역으로 식별할 수 있고 백그라운드 영역 정보를 포함하는 영역을 제2 영역으로 식별할 수 있다. 한편, 제1 이미지 및 제2 이미지를 획득하는 단계(S2010)는 제1 영역에 포함된 오브젝트의 타입이 제1 타입이면 제1 영역에 대해 제1 타입에 대응되는 강도의 텍스처 처리를 수행할 수 있고, 제1 영역에 포함된 오브젝트의 타 입이 제2 타입이면 제1 영역에 대해 제2 타입에 대응되는 강도의 텍스처 처리를 수행할 수 있다.한편, 저해상도 입력 이미지를 제1 인공 지능 모델에 입력하여 제1 영역에 대한 식별 정보, 제1 강도, 제2 영역 에 대한 식별 정보 및 제2 강도를 획득하는 단계를 더 포함할 수 있고, 제1 인공 지능 모델은 이미지가 입력되 면 입력된 이미지에서 특성이 상이한 영역에 대한 식별 정보 및 식별된 각 영역에 대한 텍스처 강도를 출력하도 록 학습된, 제어 방법. 한편, 제1 영역에 대한 식별 정보는 저해상도 입력 이미지에 포함된 제1 영역에 대한 좌표 정보를 포함할 수 있 고, 제2 영역에 대한 식별 정보는 저해상도 입력 이미지에 포함된 제2 영역에 대한 좌표 정보를 포함할 수 있다. 한편, 제1 이미지 및 제2 이미지를 획득하는 단계(S2010) 및 제1 이미지 및 제2 이미지를 각각 업스케일링하는 단계(S2015)는, 제1 영역 및 제2 영역에 대한 식별 정보를 포함하는 이미지, 제1 강도 및 제2 강도를 제2 인공 지능 모델에 입력하여 텍스처 처리 및 업스케일링 처리된 제1 및 제2 이미지를 획득할 수 있다. 한편, 제2 인공 지능 모델은 SFT(Spartial Feature Transformation)를 수행하는 RRDB(Residual-in-Residual Dense Block)을 포함하도록 구현될 수 있다. 한편, 제2 인공 지능 모델은 복수의 파라미터를 가지는 레이어를 포함할 수 있고, 상이한 특성을 가지는 복수의 영역으로 식별된 이미지 및 복수의 영역에 대응되는 텍스처 강도에 기초하여 획득된 복수의 이미지를 기준 이미 지와 비교하여 손실 값을 산출할 수 있고, 산출된 손실 값에 기초하여 복수의 파라미터를 학습시킬 수 있다. 한편, 제2 인공 지능 모델은 재구성 손실(reconstruction loss), 적대 손실(adversarial loss) 또는 인지 손실 (perceptual loss) 중 적어도 하나에 기초하여 손실 값을 산출할 수 있다. 한편, 제2 인공 지능 모델은 GAN(Generative Adversarial Network)으로 구현될 수 있다. 한편, 도 20과 같은 전자 장치의 제어 방법은 도 3 또는 도 4의 구성을 가지는 전자 장치 상에서 실 행될 수 있으며, 그 밖의 구성을 가지는 전자 장치 상에서도 실행될 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 설치 가능한 어플리케이션 형태 로 구현될 수 있다. 또한, 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 대한 소프트웨어 업그레이드, 또 는 하드웨어 업그레이드 만으로도 구현될 수 있다. 또한, 상술한 본 개시의 다양한 실시 예들은 전자 장치에 구비된 임베디드 서버, 또는 전자 장치 및 디스플레이 장치 중 적어도 하나의 외부 서버를 통해 수행되는 것도 가능하다. 한편, 본 개시의 일시 예에 따르면, 이상에서 설명된 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media)에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실 시 예들에 따른 전자 장치를 포함할 수 있다. 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 프 로세서의 제어 하에 다른 구성요소들을 이용하여 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또 는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장 매체는, 비일시적 (non-transitory) 저장 매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장 매체가 신호(signal)를 포함 하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장 매체에 반영구적 또는 임시적으로 저장됨을 구분하지 않는다. 또한, 본 개시의 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품 (computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있 다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 또한, 상술한 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구 성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요 소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로 그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는유사하게 수행할 수 있다. 다양한 실시 예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동 작들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2021-0030947", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형 실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2021-0030947", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시 예에 따른 이미지 변환 동작을 설명하기 위한 도면이다. 도 2는 다른 실시 예에 따른 이미지 변환 동작을 설명하기 위한 도면이다.도 3은 본 개시의 일 실시 예에 따른 전자 장치를 도시한 블록도이다. 도 4는 도 3의 전자 장치의 구체적인 구성을 설명하기 위한 블록도이다. 도 5는 일 실시 예에 따른 이미지 변환 동작을 설명하기 위한 흐름도이다. 도 6은 도 5의 이미지 변환 동작을 구체적으로 설명하기 위한 흐름도이다. 도 7은 입력 이미지를 복수의 영역으로 구분하는 동작을 설명하기 위한 도면이다. 도 8은 이미지 변환 모델이 학습되는 동작을 설명하기 위한 흐름도이다. 도 9는 본 개시의 일 실시 예에 따른 이미지 변환 모델을 설명하기 위한 도면이다. 도 10은 이미지 변환 동작에 이용하는 계산을 설명하기 위한 도면이다. 도 11은 본 개시의 일 실시 예에 따른 학습 동작에 이용하는 계산을 설명하기 위한 도면이다. 도 12는 본 개시의 다른 실시 예에 따른 이미지 변환 모델을 구체적으로 설명하기 위한 도면이다. 도 13은 본 개시의 일 실시 예에 따른 이미지 변환 모델에 이용되는 기본 블록을 설명하기 위한 도면이다. 도 14는 본 개시의 일 실시 예에 따른 스타일 식별 모델을 설명하기 위한 도면이다. 도 15는 본 개시의 다른 실시 예에 따른 학습 동작에 이용하는 계산을 설명하기 위한 도면이다. 도 16은 본 개시의 다양한 실시 예에 따른 전자 장치의 이미지 변환 동작의 효과를 설명하기 위한 도면이다. 도 17은 일 실시 예에 따른 전자 장치의 이미지 변환 동작을 비교하기 위한 도면이다. 도 18은 다른 실시 예에 따른 전자 장치의 이미지 변환 동작을 비교하기 위한 도면이다. 도 19는 또 다른 실시 예에 따른 전자 장치의 이미지 변환 동작을 비교하기 위한 도면이다. 도 20은 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 도면이다."}
