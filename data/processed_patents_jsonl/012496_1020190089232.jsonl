{"patent_id": "10-2019-0089232", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0011844", "출원번호": "10-2019-0089232", "발명의 명칭": "전자 장치 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "이동수"}}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,적어도 하나의 인스트럭션을 저장하는 메모리; 및상기 인스트럭션을 실행함으로써,입력 데이터를 복수의 레이어를 포함하는 인공 지능 모델에 입력하여 출력 데이터를 획득하는 프로세서;를 포함하며,상기 인공 지능 모델은, 상기 복수의 레이어를 통한 연산에 기초하여 상기 출력 데이터를 출력하고,상기 프로세서는,상기 복수의 레이어 중 어느 하나에서 출력되는 연산 데이터를 인코딩하여 상기 메모리에 저장하고, 상기 메모리에 저장된 상기 인코딩 된 데이터를 디코딩하여 상기 연산 데이터에 대응되는 복원 데이터를 획득하며, 상기획득된 복원 데이터를 상기 복수의 레이어 중 다른 하나로 제공하는 전자 장치."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 복수의 레이어는, 제1 레이어, 제2 레이어 및 상기 제1 및 제2 레이어 사이의 제1 히든 레이어 및 제2 히든 레이어를 포함하고,상기 인공 지능 모델은, 상기 제1 레이어를 재구성(reconstruction)한 상기 제2 히든 레이어를 생성하도록 상기제1 및 제2 히든 레이어의 가중치를 학습하며,상기 프로세서는,상기 제1 히든 레이어의 가중치에 기초하여, 상기 연산 데이터를 인코딩하고, 상기 제2 히든 레이어의 가중치에기초하여, 상기 인코딩 된 데이터를 디코딩하는, 전자 장치."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 인공 지능 모델은,상기 제1 레이어의 출력 데이터 및 상기 제1 히든 레이어의 가중치의 연산에 기초하여 제1 출력 데이터를 출력하고,상기 제1 출력 데이터 및 상기 제2 히든 레이어의 가중치의 연산에 기초하여 제2 출력 데이터를 출력하며,상기 제1 레이어의 출력 데이터 및 상기 제2 히든 레이어의 제2 출력 데이터의 오차가 최소가 되도록 상기 제1및 제2 히든 레이어의 가중치를 학습하는, 전자 장치."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 제1 히든 레이어는, 상기 제1 레이어의 차원보다 상대적으로 낮고, 상기 제2 히든 레이어는, 상기 제1 레이어의 차원과 같은, 전자 장치."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서,공개특허 10-2021-0011844-2-상기 인공 지능 모델은,상기 복수의 레이어, 상기 제1 및 제2 히든 레이어를 포함하는 전체 시스템의 학습을 통해, 상기 제1 및 제2 히든 레이어의 가중치를 획득하는, 전자 장치."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제2항에 있어서,상기 인공 지능 모델은,상기 제1 및 제2 히든 레이어가 포함되지 않은 복수의 레이어의 학습이 완료된 상태에서 상기 제1 및 제2 히든레이어가 추가되는 경우, 상기 복수의 레이어의 가중치를 고정한 채로, 상기 복수의 레이어, 상기 제1 및 제2히든 레이어를 포함하는 전체 시스템의 학습을 통해, 상기 제1 및 제2 히든 레이어의 가중치를 획득하는, 전자장치."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 복수의 레이어는,제1 레이어, 제2 레이어, 제3 레이어, 제1 및 제2 레이어 사이의 제1 및 제2 히든 레이어와 상기 제2 및 제3 레이어 사이의 제3 및 제4 히든 레이어를 포함하고, 상기 인공 지능 모델은,상기 제1 레이어를 재구성한 상기 제2 히든 레이어를 생성하도록 상기 제1 및 제2 히든 레이어의 가중치를 학습하고, 상기 제2 레이어를 재구성한 상기 제4 히든 레이어를 생성하도록 상기 제3 및 제4 히든 레이어의 가중치를 학습하며, 상기 제1 및 제3 히든 레이어가 동일한 가중치를 갖고, 상기 제2 및 제4 히든 레이어가 동일한 가중치를 갖도록학습하는, 전자 장치."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 프로세서는,상기 제1 히든 레이어의 입력 데이터의 인코딩 및 상기 제3 인코더의 입력 데이터의 인코딩을 하나의 인코더를통해 수행하고,상기 제2 디코더의 입력 데이터의 디코딩 및 상기 제4 디코더의 입력 데이터의 디코딩을 하나의 디코더를 통해수행하는, 전자 장치."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "전자 장치의 제어 방법에 있어서,인공 지능 모델의 복수의 레이어 중 어느 하나에서 출력되는 연산 데이터를 인코딩하여 메모리에 저장하는단계;상기 메모리에 저장된 상기 인코딩 된 데이터를 디코딩하여 상기 연산 데이터에 대응되는 복원 데이터를 획득하는 단계; 및 상기 획득된 복원 데이터를 상기 복수의 레이어 중 다른 하나로 제공하는 단계;를 포함하는, 전자 장치의 제어방법."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 복수의 레이어는, 제1 레이어, 제2 레이어 및 상기 제1 및 제2 레이어 사이의 제1 히든 레이어 및 제2 히공개특허 10-2021-0011844-3-든 레이어를 포함하고,상기 인공 지능 모델은,상기 제1 레이어를 재구성(reconstruction)한 상기 제2 히든 레이어를 생성하도록 상기 제1 및 제2 히든 레이어의 가중치를 학습하며, 상기 인코딩하는 단계는, 상기 제1 히든 레이어의 가중치에 기초하여, 상기 연산 데이터를 인코딩하고,상기 디코딩하는 단계는, 상기 제2 히든 레이어의 가중치에 기초하여, 상기 인코딩 된 데이터를 디코딩하는, 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 인공 지능 모델은,상기 제1 레이어의 출력 데이터 및 상기 제1 히든 레이어의 가중치의 연산에 기초하여 제1 출력 데이터를 출력하고,상기 제1 출력 데이터 및 상기 제2 히든 레이어의 가중치의 연산에 기초하여 제2 출력 데이터를 출력하며,상기 제1 레이어의 출력 데이터 및 상기 제2 히든 레이어의 제2 출력 데이터의 오차가 최소가 되도록 상기 제1및 제2 히든 레이어의 가중치를 학습하는, 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 제1 히든 레이어는, 상기 제1 레이어의 차원보다 상대적으로 낮고, 상기 제2 히든 레이어는, 상기 제1 레이어의 차원과 같은, 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10항에 있어서,상기 인공 지능 모델은,상기 복수의 레이어, 상기 제1 및 제2 히든 레이어를 포함하는 전체 시스템의 학습을 통해, 상기 제1 및 제2 히든 레이어의 가중치를 획득하는, 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제10항에 있어서,상기 인공 지능 모델은,상기 제1 및 제2 히든 레이어가 포함되지 않은 복수의 레이어의 학습이 완료된 상태에서 상기 제1 및 제2 히든레이어가 추가되는 경우, 상기 복수의 레이어의 가중치를 고정한 채로, 상기 복수의 레이어, 상기 제1 및 제2히든 레이어를 포함하는 전체 시스템의 학습을 통해, 상기 제1 및 제2 히든 레이어의 가중치를 획득하는, 전자장치의 제어 방법."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서,상기 복수의 레이어는,제1 레이어, 제2 레이어, 제3 레이어, 제1 및 제2 레이어 사이의 제1 및 제2 히든 레이어와 상기 제2 및 제3 레이어 사이의 제3 및 제4 히든 레이어를 포함하고, 상기 인공 지능 모델은,상기 제1 레이어를 재구성한 상기 제2 히든 레이어를 생성하도록 상기 제1 및 제2 히든 레이어의 가중치를 학습공개특허 10-2021-0011844-4-하고, 상기 제2 레이어를 재구성한 상기 제4 히든 레이어를 생성하도록 상기 제3 및 제4 히든 레이어의 가중치를 학습하며, 상기 제1 및 제3 히든 레이어가 동일한 가중치를 갖고, 상기 제2 및 제4 히든 레이어가 동일한 가중치를 갖도록학습하는, 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 제1 히든 레이어의 입력 데이터의 인코딩 및 상기 제3 히든 레이어의 입력 데이터의 인코딩을 하나의 인코더를 통해 수행하고,상기 제2 히든 레이어의 입력 데이터의 디코딩 및 상기 제3 히든 레이어의 입력 데이터의 디코딩을 하나의 디코더를 통해 수행하는, 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0089232", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "적어도 하나의 인스트럭션을 저장하는 컴퓨터 판독 가능 매체에 있어서, 상기 인스트럭션을 실행함으로써 전자장치로 하여금,인공 지능 모델의 복수의 레이어 중 어느 하나에서 출력되는 연산 데이터를 인코딩하여 메모리에 저장하는단계;상기 메모리에 저장된 상기 인코딩 된 데이터를 디코딩하여 상기 연산 데이터에 대응되는 복원 데이터를 획득하는 단계; 및 상기 획득된 복원 데이터를 상기 복수의 레이어 중 다른 하나로 제공하는 단계;를 수행하게 하는 컴퓨터 판독가능 매체."}
{"patent_id": "10-2019-0089232", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 본 전자 장치는 적어도 하나의 인스트럭션을 저장하는 메모리 및 인스트럭션을 실행함으 로써, 입력 데이터를 복수의 레이어를 포함하는 인공 지능 모델에 입력하여 출력 데이터를 획득하는 프로세서를 포함하며, 인공 지능 모델은, 복수의 레이어를 통한 연산에 기초하여 출력 데이터를 생성하고, 프로세서는, 복수 의 레이어 중 어느 하나에서 출력되는 연산 데이터를 인코딩하여 메모리에 저장하고, 메모리에 저장된 인코딩 된 데이터를 디코딩하여 연산 데이터에 대응되는 복원 데이터를 획득하며, 획득된 복원 데이터를 복수의 레이어 중 다른 하나로 제공할 수 있다."}
{"patent_id": "10-2019-0089232", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 그 제어 방법에 관한 것으로, 보다 상세하게는 인공 지능 기술을 기반으로 동작하는 전 자 장치 및 그 제어 방법에 관한 것이다."}
{"patent_id": "10-2019-0089232", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 인간 수준의 지능을 구현하는 인공 지능 시스템이 개발되고 있다. 인공 지능 시스템은, 기존의 룰(rule) 기반 스마트 시스템과 달리 기계가 스스로 학습하고 판단하는 시스템으로써, 음성 인식, 이미지 인식 및 미래 예측 등과 같은 다양한 범위에서 활용되고 있다. 특히, 최근에는 딥 러닝(deep learning)에 기반하여 주어진 문 제를 해결하는 인공 지능 시스템이 개발되고 있다. 한편, 인공 지능 모델을 통해 인공 지능 기술을 구현하기 위해서는, 인공 지능 모델의 파라미터 및 입력 데이터 의 연산에 기초하여 출력되는 연산 데이터를 메모리에 저장해야 한다. 여기에서, 인공 지능 모델의 파라미터는 인공 지능 모델의 각 레이어의 가중치(weight)로서, 이는 학습 이후에 는 고정된 값이기 때문에 고정 데이터(static data)로 불릴 수 있다. 그리고, 입력 데이터의 연산에 기초하여 출력되는 연산 데이터는 인공 지능 모델의 각 레이어에 의해 출력되는 값으로써, 이는 입력 데이터에 의해 결정 되는 값이기 때문에 다이나믹 데이터(dynamic data)로 불릴 수 있다. 한편, 상술한 고정 데이터는 양자화 등의 방법을 통해 압축하는 기술이 개발되고 있으나, 다이나믹 데이터의 압 축과 관련해서는 현재 어떠한 기술도 개발되지 않은 상태이다."}
{"patent_id": "10-2019-0089232", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 문제점을 해결하기 위해 안출된 것으로, 본 개시의 목적은 인공 지능 모델의 각 레이어에 의 해 출력된 연산 데이터, 즉 다이나믹 데이터를 압축하여 저장할 수 있는 전자 장치 및 그 제어 방법을 제공함에있다."}
{"patent_id": "10-2019-0089232", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 개시의 일 실시 예에 따른 전자 장치는, 적어도 하나의 인스트럭션을 저장하는 메모리 및 상기 인스트럭션을 실행함으로써, 입력 데이터를 복수의 레이어를 포함하는 인공 지능 모델에 입력하 여 출력 데이터를 획득하는 프로세서를 포함하며, 상기 인공 지능 모델은, 상기 복수의 레이어를 통한 연산에 기초하여 상기 출력 데이터를 출력하고, 상기 프로세서는, 상기 복수의 레이어 중 어느 하나에서 출력되는 연산 데이터를 인코딩하여 상기 메모리에 저장하고, 상기 메모리에 저장된 상기 인코딩 된 데이터를 디코딩하여 상기 연산 데이터에 대응되는 복원 데이터를 획득하며, 상기 획득된 복원 데이터를 상기 복수의 레이어 중 다른 하나 로 제공할 수 있다. 여기에서, 상기 복수의 레이어는, 제1 레이어, 제2 레이어 및 상기 제1 및 제2 레이어 사이의 제1 히든 레이어 및 제2 히든 레이어를 포함하고, 상기 인공 지능 모델은, 상기 제1 레이어를 재구성(reconstruction)한 상기 제 2 히든 레이어를 생성하도록 상기 제1 및 제2 히든 레이어의 가중치를 학습하며, 상기 프로세서는, 상기 제1 히 든 레이어의 가중치에 기초하여, 상기 연산 데이터를 인코딩하고, 상기 제2 히든 레이어의 가중치에 기초하여, 상기 인코딩 된 데이터를 디코딩할 수 있다. 그리고, 상기 인공 지능 모델은, 상기 제1 레이어의 출력 데이터 및 상기 제1 히든 레이어의 가중치의 연산에 기초하여 제1 출력 데이터를 출력하고, 상기 제1 출력 데이터 및 상기 제2 히든 레이어의 가중치의 연산에 기초 하여 제2 출력 데이터를 출력하며, 상기 제1 레이어의 출력 데이터 및 상기 제2 히든 레이어의 제2 출력 데이터 의 오차가 최소가 되도록 상기 제1 및 제2 히든 레이어의 가중치를 학습할 수 있다. 여기에서, 상기 제1 히든 레이어는, 상기 제1 레이어의 차원보다 상대적으로 낮고, 상기 제2 히든 레이어는, 상 기 제1 레이어의 차원과 같을 수 있다. 그리고, 상기 인공 지능 모델은, 상기 복수의 레이어, 상기 제1 및 제2 히든 레이어를 포함하는 전체 시스템의 학습을 통해, 상기 제1 및 제2 히든 레이어의 가중치를 획득할 수 있다. 또한, 상기 인공 지능 모델은, 상기 제1 및 제2 히든 레이어가 포함되지 않은 복수의 레이어의 학습이 완료된 상태에서 상기 제1 및 제2 히든 레이어가 추가되는 경우, 상기 복수의 레이어의 가중치를 고정한 채로, 상기 복 수의 레이어, 상기 제1 및 제2 히든 레이어를 포함하는 전체 시스템의 학습을 통해, 상기 제1 및 제2 히든 레이 어의 가중치를 획득할 수 있다. 그리고, 상기 복수의 레이어는, 제1 레이어, 제2 레이어, 제3 레이어, 제1 및 제2 레이어 사이의 제1 및 제2 히 든 레이어와 상기 제2 및 제3 레이어 사이의 제3 및 제4 히든 레이어를 포함하고, 상기 인공 지능 모델은, 상기 제1 레이어를 재구성한 상기 제2 히든 레이어를 생성하도록 상기 제1 및 제2 히든 레이어의 가중치를 학습하고, 상기 제2 레이어를 재구성한 상기 제4 히든 레이어를 생성하도록 상기 제3 및 제4 히든 레이어의 가중치를 학습 하며, 상기 제1 및 제3 히든 레이어가 동일한 가중치를 갖고, 상기 제2 및 제4 히든 레이어가 동일한 가중치를 갖도록 학습할 수 있다. 그리고, 상기 프로세서는, 상기 제1 히든 레이어의 입력 데이터의 인코딩 및 상기 제3 인코더의 입력 데이터의 인코딩을 하나의 인코더를 통해 수행하고, 상기 제2 디코더의 입력 데이터의 디코딩 및 상기 제4 디코더의 입력 데이터의 디코딩을 하나의 디코더를 통해 수행할 수 있다. 한편, 상기 목적을 달성하기 위한 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법은, 인공 지능 모델의 복 수의 레이어 중 어느 하나에서 출력되는 연산 데이터를 인코딩하여 메모리에 저장하는 단계, 상기 메모리에 저 장된 상기 인코딩 된 데이터를 디코딩하여 상기 연산 데이터에 대응되는 복원 데이터를 획득하는 단계 및 상기 획득된 복원 데이터를 상기 복수의 레이어 중 다른 하나로 제공하는 단계를 포함할 수 있다. 여기에서, 상기 복수의 레이어는, 제1 레이어, 제2 레이어 및 상기 제1 및 제2 레이어 사이의 제1 히든 레이어 및 제2 히든 레이어를 포함하고, 상기 인공 지능 모델은, 상기 제1 레이어를 재구성(reconstruction)한 상기 제 2 히든 레이어를 생성하도록 상기 제1 및 제2 히든 레이어의 가중치를 학습하며, 상기 인코딩하는 단계는, 상기 제1 히든 레이어의 가중치에 기초하여, 상기 연산 데이터를 인코딩하고, 상기 디코딩하는 단계는, 상기 제2 히 든 레이어의 가중치에 기초하여, 상기 인코딩 된 데이터를 디코딩할 수 있다. 그리고, 상기 인공 지능 모델은, 상기 제1 레이어의 출력 데이터 및 상기 제1 히든 레이어의 가중치의 연산에 기초하여 제1 출력 데이터를 출력하고, 상기 제1 출력 데이터 및 상기 제2 히든 레이어의 가중치의 연산에 기초 하여 제2 출력 데이터를 출력하며, 상기 제1 레이어의 출력 데이터 및 상기 제2 히든 레이어의 제2 출력 데이터 의 오차가 최소가 되도록 상기 제1 및 제2 히든 레이어의 가중치를 학습할 수 있다. 여기에서, 상기 제1 히든 레이어는, 상기 제1 레이어의 차원보다 상대적으로 낮고, 상기 제2 히든 레이어는, 상 기 제1 레이어의 차원과 같을 수 있다. 그리고, 상기 인공 지능 모델은, 상기 복수의 레이어, 상기 제1 및 제2 히든 레이어를 포함하는 전체 시스템의 학습을 통해, 상기 제1 및 제2 히든 레이어의 가중치를 획득할 수 있다. 또한, 상기 인공 지능 모델은, 상기 제1 및 제2 히든 레이어가 포함되지 않은 복수의 레이어의 학습이 완료된 상태에서 상기 제1 및 제2 히든 레이어가 추가되는 경우, 상기 복수의 레이어의 가중치를 고정한 채로, 상기 복 수의 레이어, 상기 제1 및 제2 히든 레이어를 포함하는 전체 시스템의 학습을 통해, 상기 제1 및 제2 히든 레이 어의 가중치를 획득할 수 있다. 그리고, 상기 복수의 레이어는, 제1 레이어, 제2 레이어, 제3 레이어, 제1 및 제2 레이어 사이의 제1 및 제2 히 든 레이어와 상기 제2 및 제3 레이어 사이의 제3 및 제4 히든 레이어를 포함하고, 상기 인공 지능 모델은, 상기 제1 레이어를 재구성한 상기 제2 히든 레이어를 생성하도록 상기 제1 및 제2 히든 레이어의 가중치를 학습하고, 상기 제2 레이어를 재구성한 상기 제4 히든 레이어를 생성하도록 상기 제3 및 제4 히든 레이어의 가중치를 학습 하며, 상기 제1 및 제3 히든 레이어가 동일한 가중치를 갖고, 상기 제2 및 제4 히든 레이어가 동일한 가중치를 갖도록 학습할 수 있다. 그리고, 상기 제1 히든 레이어의 입력 데이터의 인코딩 및 상기 제3 히든 레이어의 입력 데이터의 인코딩을 하 나의 인코더를 통해 수행하고, 상기 제2 히든 레이어의 입력 데이터의 디코딩 및 상기 제3 히든 레이어의 입력 데이터의 디코딩을 하나의 디코더를 통해 수행할 수 있다. 한편, 상기 목적을 달성하기 위한 본 개시의 일 실시 예에 따른 적어도 하나의 인스트럭션을 저장하는 컴퓨터 판독 가능 매체는, 상기 인스트럭션을 실행함으로써 전자 장치로 하여금, 인공 지능 모델의 복수의 레이어 중 어느 하나에서 출력되는 연산 데이터를 인코딩하여 메모리에 저장하는 단계, 상기 메모리에 저장된 상기 인코딩 된 데이터를 디코딩하여 상기 연산 데이터에 대응되는 복원 데이터를 획득하는 단계 및 상기 획득된 복원 데이 터를 상기 복수의 레이어 중 다른 하나로 제공하는 단계를 수행하게 할 수 있다."}
{"patent_id": "10-2019-0089232", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상과 같은 본 개시의 다양한 실시 예에 따르면, 다이나믹 데이터의 압축을 통해 제한된 메모리를 가진 모바일 장치 등에서도 인공 지능 기술을 효율적으로 구현할 수 있다."}
{"patent_id": "10-2019-0089232", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "먼저, 본 명세서 및 청구범위에서 사용되는 용어는 본 개시의 기능을 고려하여 일반적인 용어들을 선택하였다. 하지만, 이러한 용어들은 당 분야에 종사하는 기술자의 의도나 법률적 또는 기술적 해석 및 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 일부 용어는 출원인이 임의로 선정한 용어도 있다. 이러한 용어에 대해서는 본 명세서에서 정의된 의미로 해석될 수 있으며, 구체적인 용어 정의가 없으면 본 명세서의 전반적인 내용 및당해 기술 분야의 통상적인 기술 상식을 토대로 해석될 수도 있다. 또한, 본 개시를 설명함에 있어서, 관련된 공지 기능 혹은 구성에 대한 구체적인 설명이 본 개시의 요지를 불필 요하게 흐릴 수 있다고 판단되는 경우, 그에 대한 상세한 설명은 축약하거나 생략한다. 나아가, 이하 첨부 도면들 및 첨부 도면들에 기재된 내용들을 참조하여 본 개시의 실시 예를 상세하게 설명하지 만, 본 개시가 실시 예들에 의해 제한되거나 한정되는 것은 아니다. 이하, 첨부된 도면을 참조하여 본 개시를 상세히 설명한다. 도 1은 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 블록도이다. 도 1을 참조하면, 본 개시의 일 실시 예에 따른 전자 장치는 메모리 및 프로세서를 포함한다. 본 개시의 일 실시 예에 따른 전자 장치는 인공 지능 모델을 이용하여 입력 데이터에 대한 출력 데이터를 획득하는 장치로써, 예를 들어, 전자 장치는 데스크탑 PC, 노트북, 스마트 폰, 태블릿 PC, 서버 등일 수 있다. 또는, 전자 장치는 클라우딩 컴퓨팅 환경이 구축된 시스템 자체일 수도 있다. 다만, 이에 한정되는 것은 아니며, 전자 장치는 인공 지능 모델을 이용한 연산이 가능한 장치라면 어떠한 장치라도 무방하다. 메모리는 프로세서와는 별도로 구비되며, 하드 디스크, 비휘발성 메모리 및 휘발성 메모리 등으로 구 현될 수 있다. 여기에서, 비휘발성 메모리는 OTPROM(one time programmable ROM), PROM(programmable ROM), EPROM(erasable and programmable ROM), EEPROM(electrically erasable and programmable ROM), mask ROM, flash ROM 등이 될 수 있고, 휘발성 메모리는 DRAM(dynamic RAM), SRAM(static RAM), 또는 SDRAM(synchronous dynamic RAM) 등이 될 수 있다. 또는, 메모리는 프로세서 내 롬(미도시) 및 램(미도시) 중 적어도 하 나로 구현될 수도 있다. 메모리는 프로세서에 의해 액세스되며, 프로세서에 의한 데이터의 독취/기록/수정/삭제/갱신 등 이 수행될 수 있다. 메모리는 인공 지능 모델을 저장할 수 있다. 여기에서, 인공 지능 모델은 인공 지능 알고리즘을 통해 학습 (training)된 모델이 될 수 있다. 인공 지능 모델은, 복수의 레이어들로 구성될 수 있다. 여기에서, 각 레이어는 적어도 하나 이상의 노드(또는, 뉴런)를 포함할 수 있고, 각 노드에는 적어도 하나 이상의 가중치(weight)가 할당되어 있을 수 있다. 이와 같은, 인공 지능 모델은 뉴럴 네트워크(Neural Network)를 기반으로 하는 모델이 될 수 있다. 일 예로, 인 공 지능 모델은 RNN(Recurrent Neural Network)에 기반한 모델일 수 있다. 여기에서, RNN은 순환 신경망을 의 미하며, 시계열 데이터와 같이 시간의 흐름에 따라 변화하는 데이터를 학습하기 위한 딥 러닝 모델의 일종이다. 다만, 이에 한정되는 것은 아니며, 인공 지능 모델은 CNN(Convolutional Neural Network), DNN (Deep Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network) 또는 BRDNN(Bidirectional Recurrent Deep Neural Network) 등과 같은 다양한 네트워크에 기반한 모델이 될 수 있다. 또는, 메모리 는 인공 지능 알고리즘을 통해 학습된 모델이 아닌 룰(rule) 기반으로 생성된 모델을 저장할 수도 있으며, 메모 리에 저장된 모델에는 특별한 제한이 없다. 프로세서는 전자 장치의 동작을 전반적으로 제어한다. 이를 위해, 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 여기에서, 하나 또는 복수의 프로세서는 중앙처리장치(central processing unit(CPU)), 어플리케이션 프로세서(application processor(AP)), 또는 커뮤니케이션 프로세서(communication processor(CP)) 등과 같은 범용 프로세서가 될 수 있음은 물론, 그래픽 처리 장치(graphic processing unit, GPU) 등과 같은 그래픽 전용 프로세서 또는 신경망 처리 장치(neural network processing unit, NPU)와 같은 인공 지능 전용 프로세서가 될 수 있다. 프로세서는 메모리에 저장된 적어도 하나의 인스트럭션을 실행함으로써, 본 개시의 다양한 실시 예에 따른 전자 장치의 동작을 수행할 수 있다. 예를 들어, 프로세서는 적어도 하나의 인스트럭션을 실행 함으로써, 복수의 레이어를 포함하는 인공 지능 모델을 이용하여 입력 데이터에 대한 출력 데이터를 획득할 수 있다. 여기에서, 입력 데이터는 텍스트, 이미지 또는 사용자 음성 등이 될 수 있다. 일 예로, 텍스트는 전자 장 치의 키보드 또는 터치패드 등과 같은 입력부(미도시)를 통해 입력된 텍스트가 될 수 있고, 이미지는 전자 장치의 카메라를 통해 촬영된 이미지가 될 수 있다. 또한, 사용자 음성은 전자 장치의 마이크에 입력 된 사용자 음성이 될 수 있다. 다만, 이는 일 실시 예일 뿐, 프로세서는 다양한 경로를 통해 입력 데이터를 획득할 수 있다. 가령, 프로세서는 서버 등의 외부 장치로부터 이미지를 수신하거나, 리모컨 등의 외부 장치로부터 사용자 음성을 수신할 수도 있다. 또한, 상술한 입력 데이터는 일 예로서, 입력 데이터는 일정 기간 동안의 날씨 데이터, 주식 데이터 등 다양한 종류의 데이터가 될 수 있다. 한편, 출력 데이터는 입력 데이터 및/또는 인공 지능 모델의 종류에 따라 상이할 수 있다. 즉, 출력 데이터는 어떠한 입력 데이터가 어떠한 인공 지능 모델에 입력되는지에 따라 상이할 수 있다. 예를 들어, 본 개시의 인공 지능 모델이 언어 번역을 위한 모델인 경우, 프로세서는 제1 언어로 표현된 입력 데이터에 대해 제2 언어 로 표현된 출력 데이터를 획득할 수 있다. 또는, 본 개시의 인공 지능 모델이 이미지 분석을 위한 모델인 경우, 프로세서는 이미지를 인공 지능 모델의 입력 데이터로 입력하고, 해당 이미지에서 검출된 오브젝트에 관한 정보를 출력 데이터로 획득할 수 있다. 일 예로, 프로세서는 강아지를 포함하는 이미지가 입력 데이터로 입력되면, 해당 이미지는 강아지를 포함하는 이미지라는 정보를 출력 데이터로 획득할 수 있다. 또는, 본 개시 의 인공 지능 모델이 이미지 해석을 위한 모델인 경우, 프로세서는 이미지를 인공 지능 모델의 입력 데이 터로 입력하고, 해당 이미지를 단어로 표현한 텍스트를 출력 데이터로 획득할 수 있다. 일 예로, 프로세서(12 0)는 소년이 사과를 잡고 있는 이미지가 입력 데이터로 입력되면, '소년이 사과를 고르고 있다'라는 텍스트를 출력 데이터로 획득할 수 있다. 또한, 본 개시의 인공 지능 모델이 음성 인식을 위한 모델인 경우, 프로세서 는 사용자 음성을 입력 데이터로, 사용자 음성에 대응되는 텍스트를 출력 데이터로 획득할 수도 있다. 한 편, 상술한 출력 데이터는 일 실시 예로서, 본 개시의 출력 데이터의 종류가 이에 제한되는 것은 아니라 할 것 이다. 프로세서는 입력 데이터가 입력되면, 입력 데이터를 벡터(또는, 행렬이나 텐서)로 표현할 수 있다. 여기에 서, 입력 데이터를 벡터(또는, 행렬이나 텐서)로 표현하는 방법은 입력 데이터의 종류, 유형에 따라 상이할 수 있다. 일 예로, 프로세서는 입력 데이터로 텍스트(또는, 사용자 음성을 변환한 텍스트)가 입력되는 경우, 원 핫 인코딩(One hot Encoding)을 통해 텍스트를 벡터로 표현하거나, 워드 임베딩(Word Embedding)을 통해 텍 스트를 벡터로 표현할 수 있다. 여기에서, 원 핫 인코딩은 특정 단어의 인덱스의 값만 1으로 표현하고 나머지 인덱스의 값은 0으로 표현하는 방식이고, 워드 임베딩은 사용자에 의해 설정된 벡터의 차원(가령, 128차원)으로 단어를 실수로 표현하는 방식이다. 워드 임베딩 방법으로는 일 예로, Word2Vec, FastText, Glove 등이 이용될 수 있다. 한편, 프로세서는 입력 데이터로 이미지가 입력되는 경우이면, 이미지의 각 픽셀을 행렬로 표현 할 수 있다. 일 예로, 프로세서는 이미지의 각 픽셀을 RGB 컬러별로 0에서 255의 값으로 표현하거나, 0에 서 255로 표현된 값을 기설정된 값(가령, 255)로 나눈 값으로 이미지를 행렬로 표현할 수 있다. 프로세서는 인공 지능 모델을 이용하여, 벡터(또는, 행렬이나 텐서)로 표현된 입력 데이터에 대한 연산을 수행할 수 있다. 일 예로, 인공 지능 모델이 입력 레이어, 히든 레이어 및 출력 레이어로 구성되는 경우, 입력 레이어는 벡터(또 는, 행렬이나 텐서)로 표현된 입력 데이터에 대한 정보를 포함할 수 있다. 이 경우, 프로세서는 입력 레이 어의 벡터(또는, 행렬이나 텐서)에 메모리로부터 독출한 입력 레이어 및 히든 레이어 간의 가중치를 연산 함으로써, 입력 레이어의 연산 데이터를 출력할 수 있다. 여기에서, 편향(bias)이 더 고려될 수 있으나, 설명의 편의를 위해 편향은 생략하고 설명한다. 그리고, 프로세서는 입력 레이어의 연산 데이터를 히든 레이어의 입력으로 하여 연산을 수행할 수 있다. 구체적으로, 프로세서는 입력 레이어의 연산 데이터 및, 메모리 로부터 독출한 히든 레이어 및 출력 레이어 간의 가중치에 기초하여 연산을 수행함으로써, 히든 레이어의 연산 데이터를 출력할 수 있다. 그리고, 프로세서는 히든 레이어의 연산 데이터를 출력 레이어의 입력으로 하여 출력 데이터를 획득할 수 있다. 구체적으로, 프로세서는 히든 레이어의 연산 데이터에 기설정된 함수 (일 예로, 소프트맥스 함수(softmax fuction)를 적용하여 출력 데이터를 출력할 수 있다. 여기에서, 출력 데이 터는 상술한 바와 같이, 인공 지능 모델이 언어 번역을 위한 모델이면 입력 데이터로 입력된 텍스트와는 상이한 언어의 텍스트가 될 수 있고, 인공 지능 모델이 이미지 분석을 위한 모델이면, 이미지에 포함된 오브젝트에 관 한 정보를 포함하는 데이터가 될 수 있으나, 반드시 이에 한정되는 것은 아니다. 한편, 본 개시의 인공 지능 모 델이 복수의 히든 레이어를 포함하는 모델이면, 이전 히든 레이어에서 출력된 연산 데이터는 다음 히든 레이어 의 입력 데이터가 될 수 있다. 프로세서는 인공 지능 모델의 각 레이어의 연산 데이터를 메모리에 저장할 수 있다. 이는, 인공 지능 모델을 학습시키는 단계이면, 인공 지능 모델의 오차를 최소화 하기 위한 역전파(Back Propagation) 단계에서 각 레이어의 가중치를 업데이트하기 위해 연산 데이터가 필요하고, 인공 지능 모델의 학습 이후 추론하는 단계 이면, 다음 레이어의 연산을 수행하기 위해 이전 레이어의 연산 데이터가 필요하기 때문이다. 일 예로, 본 개시 의 인공 지능 모델이 순환 신경망 (Recurrent Neural Network, RNN)인 경우, 프로세서는 이전 시점의 입력 데이터에 기초하여 출력된 연산 데이터를 다음 시점의 입력 데이터의 연산 데이터를 출력하는 과정에서 이용 하기 위해 메모리에 저장할 수 있다. 또한, 본 개시의 인공 지능 모델이 합성곱 신경망(Convolutional Neural Network, CNN)인 경우, 프로세서는 이전 레이어를 컨벌루션 연산 처리하여 생성한 연산 데이터(즉, 특징 맵(feature map))를 다음 레이어의 연산에 이용하기 위해 메모리에 저장할 수 있다. 한편, 연산 데이터는 상술한 바와 같이, 각 레이어에 입력되는 데이터에 기초하여 결정되는 데이터이기 때문에 다이나믹 데이터, 중간 데이터 또는 액티베이션(activation)으로 불릴 수도 있다. 그런데, 이와 같이 연산 데이터를 메모리에 저장할 경우, 제한된 메모리를 가진 스마트 폰과 같은 모바일 장치 등에서는 메모리의 용량을 초과하는 문제가 발생할 수 있다. 이와 같은 문제점을 해결하기 위해서, 프로세서는 연산 데이터를 인코딩(또는, 압축)하여 메모리에 저장할 수 있다. 이하, 도 2를 참조하여 설명한다. 도 2는 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 상세 블록도이다. 도 2를 참조하면, 본 개시의 일 실시 예에 따른 전자 장치는 메모리, 인코더, 디코더 및 프로세서를 포함할 수 있다. 이하, 설명한 설명과 중복되는 부분은 생략하거나 축약하여 설명한다. 프로세서는 전자 장치를 구성하는 하나 이상의 구성과 연결되어, 전자 장치의 동작을 전반적으 로 제어한다. 구체적으로, 프로세서는 메모리, 인코더 또는 디코더 중 적어도 하나와 연결 되어, 각 구성의 동작을 제어할 수 있다. 프로세서는 입력 데이터를 획득할 수 있다. 일 예로, 프로세서는 전자 장치의 키보드 또는 터치 패드 등과 같은 입력부(미도시)를 통해 입력된 텍스트를 입력 데이터로 획득할 수 있음은 물론, 전자 장치(10 0)의 카메라를 통해 촬영된 이미지를 입력 데이터로 획득할 수 있다. 또한, 프로세서는 전자 장치의 마이크를 통해 입력된 사용자 음성을 입력 데이터로 획득할 수 있다. 프로세서는 입력 데이터를 벡터(또는, 실시 예에 따라 행렬이나 텐서)로 표현할 수 있다. 여기에서, 입력 데이터를 벡터(또는, 행렬이나 텐서)로 표현하는 방법은 입력 데이터의 종류, 유형에 따라 상이할 수 있다. 일 예로, 프로세서는 입력 데이터로 'I am a boy'라는 텍스트가 획득되는 경우, 원 핫 인코딩을 통해 'I'를 [1, 0, 0, 0]로 표현하고, 'am'을 [0, 1, 0, 0]으로 표현하고 'a'를 [0, 0, 0, 1]으로 표현하며, 'boy'를 [0, 0, 0, 1]으로 표현할 수 있다. 또는, 프로세서는 워드 임베딩을 통해 통해 'I'를 [0.1, 4.2, 1.5, 2.8]로 표현하고, 'am'을 [1.0, 3.1, 2.5, 1.1]으로 표현하고 'a'를 [0.3, 2.1, 0.9, 1.1]로 표현하며, 'boy'를 [0.7, 1.7, 0.5, 0.2]로 표현할 수 있다. 또는, 프로세서는 입력 데이터로 이미지가 획득되는 경우, 이미지의 각 픽셀의 RGB 값에 기초하여 이미지를 M*N 형태(여기에서, M, N은 서로 다른 정수가 될 수 있음은 물론 동일한 정 수가 될 수도 있다.)의 행렬로 표현할 수 있다. 프로세서는 인공 지능 모델을 이용하여, 벡터(또는, 행렬이나 텐서)로 표현된 입력 데이터에 대한 연산을 수행할 수 있다. 일 예로, 인공 지능 모델이 입력 레이어, 히든 레이어 및 출력 레이어로 구성되는 경우, 입력 레이어는 벡터(또는, 행렬이나 텐서)로 표현된 입력 데이터에 대한 정보를 포함할 수 있다. 이 경우, 프로세서 는 입력 레이어의 벡터(또는, 행렬이나 텐서)에 메모리로부터 독출한 입력 레이어 및 히든 레이어 간 의 가중치를 연산함으로써, 입력 레이어의 연산 데이터를 출력할 수 있다. 여기에서, 연산은 y=M*x(여기에서, M은 메모리로부터 독출한 가중치를 나타내는 벡터, 행렬 또는 텐서이고, x는 입력 데이터를 나타내는 백터, 행렬 또는 텐서이며, y는 연산 데이터를 나타내는 벡터, 행렬 또 는 텐서를 의미한다.)의 형태의 딥러닝 연산이 될 수 있다. 예를 들어, 입력 데이터가 [x1 ; x2]이고 메모리 로부터 독출한 가중치가 [w11, w12 ; w21, w22]인 경우, 연산 데이터는 [w11*x1 + w12*x2 ; w21*x1+w22*x2]가 될 수 있다. 이를 위해, 프로세서는 데이터의 연산을 수행하는 프로세싱 엘리먼트 (processing element, PE)를 포함할 수 있고, 메모리의 인터페이스에 연결될 수 있다. 그리고, 프로세서는 연산 데이터를 메모리에 저장하는 동작을 수행할 수 있다. 특히, 프로세서는 연산 데이터를 인코딩하여 메모리에 저장할 수 있다. 상술한 바와 같이, 인공 지능 모델의 연산 데이터는 벡터, 행렬 또는 텐서로 표현될 수 있고, 이와 같은 벡터, 행렬 또는 텐서가 본 개시의 인코딩 대상이 된다. 이를 위해, 프로세서는 도 2에 도시된 바와 같이 인코더에 연결될 수 있다. 그리고, 프로세서는 딥 러닝 연산에 기초하여 생성된 연산 데이터를 인코더로 전송할 수 있다. 인코더는 프로세서에 의해 출력된 연산 데이터를 인코딩 할 수 있다. 여기에서, 인코딩은 연산 데이 터의 압축을 의미하는 것으로서, 인코더는 인코딩을 통해, 연산 데이터를 연산 데이터의 크기보다 작은 크 기의 데이터로 변환 또는 표현할 수 있다. 일 실시 예에 의하면, 인코딩에 의해 연산 데이터는 연산 데이터가 가진 차원보다 낮은 차원의 데이터로 변환 또는 표현될 수 있다. 즉, 연산 데이터가 n차원의 벡터(또는, 행렬, 텐서)인 경우, 인코더는 연산 데이터 를, n보다 낮은 차원의 벡터(또는, 행렬, 텐서)로 변환 또는 표현할 수 있다. 예를 들어, 상술한 딥러닝 연산을 통해 출력된 연산 데이터가 [0.7, 1.7, 0.5, 0.2]와 같이 4차원의 벡터로 표현되는 경우, 인코더는 [0.7, 1.7, 0.5, 0.2]인 연산 데이터를 [0.5, 1.5, 0.4]와 같이 3차원의 벡터로 표현할 수 있다. 이를 위해, 인코더는 프로세서로부터 연산 데이터가 수신되면, 메모리로부터 연산데이터를 인코 딩하기 위한 가중치를 독출할 수 있다. 구체적으로, 인코더는 프로세서로부터 Layer n의 연산 데이터가 수신되면, 메모리로부터 Layer n에 대응되는 가중치를 독출할 수 있다. 여기에서, Layer n에 대응되는 가중치는 Layer n의 연산데이터를 인코 딩하기 위해 학습된 가중치를 의미한다. 본 개시의 인공 지능 모델의 학습과 관련해서는 도 4를 참조하여 후술 하기로 한다. 인코더는 프로세서로부터 수신한 연산 데이터 및 메모리로부터 독출한 가중치에 기초하여 딥러 닝 연산을 수행하여, Layer n의 연산 데이터를 인코딩할 수 있다. 즉, 인코더는 n차원의 벡터(또는, 행렬, 텐서)로 표현된 Layer n의 연산 데이터를 n보다 낮은 차원의 벡터(또는, 행렬, 텐서)로 변환 또는 표현할 수 있 다. 이후, 인코더는 인코딩 된 데이터를 메모리에 저장할 수 있다. 이를 위해, 인코더는 도 2에 도 시된 바와 같이, 메모리에 연결될 수 있다. 한편, 이상에서는 입력 데이터 및 히든 레이어의 가중치에 기초한 연산 데이터를 인코딩하는 것으로 설명하였으 나 이는 일 실시 예일 뿐이다. 본 개시의 인공 지능 모델이 복수의 히든 레이어를 포함하는 경우이면, 프로세서 는 제1 히든 레이어의 출력 데이터(이는, 제2 히든 레이어의 입력 데이터가 된다.) 및 제2 히든 레이어의 가중치에 기초한 연산 데이터를 인코더를 통해 인코딩할 수도 잇다. 또한, 인코딩의 대상이 되는 연산 데 이어는 이전 레이어의 출력 데이터가 될 수 있음은 물론, 이전 레이어의 출력 데이터에 현재 레이어의 활성화 함수(가령, ReLu, sigmiod)를 적용한 데이터가 될 수도 있다. 디코더는 메모리에 저장된 인코딩 된 데이터를 디코딩 할 수 있다. 디코딩은 인코딩 된 데이터의 복원을 의미하는 것으로서, 디코더는 디코딩을 통해, 인코딩 된 데이터를, 인코딩 전의 데이터에 대응되는 데이터로 복원할 수 있다. 여기에서, 인코딩 전의 데이터에 대응되는 데이터는, 인코딩 전의 데이터에 근사한 데이터로서, 인코딩 전의 데이터와 완전히 일치하지는 않으나, 인코딩 전의 데이 터를 재구성(reconstruction)했다고 볼 수 있는 데이터가 될 수 있다. 일 실시 예에 의하면, 메모리에 저장된 인코딩 된 데이터는, 인코더에 의해 인코딩 된 데이터, 즉 프 로세서에 의해 출력된 연산 데이터가 인코더에 의해 낮은 차원의 데이터로 변환 또는 표현된 데이터 가 될 수 있다. 이 경우, 디코더는 메모리에 저장된 인코딩 된 데이터를 디코딩하여 상술한 연산 데 이터에 대응되는 복원 데이터를 획득할 수 있다. 여기에서, 연산 데이터에 대응되는 복원 데이터는, 프로세서에 의해 출력된 연산 데이터에 근사한 데이터 로서, 연산 데이터와 완전히 일치하지는 않으나, 연산 데이터를 재구성했다고 볼 수 있는 데이터가 될 수 있다. 일 실시 예에 의하면, 디코딩에 의해, 인코딩 된 데이터는 인코딩 된 데이터가 가진 차원보다 높은 차원의 데이 터로 변환 또는 표현될 수 있다. 즉, 인코딩 된 데이터가 n차원의 행렬인 경우, 디코더는 인코딩 된 데이 터를 n보다 높은 차원의 행렬로 변환 또는 표현할 수 있다. 예를 들어, 인코딩에 의해 인코딩 된 데이터가 [0.5, 1.5, 0.4]와 같이 3차원의 벡터로 표현되는 경우, 디코더는 [0.5, 1.5, 0.4]인 인코딩 데이터를 [0.7, 1.7, 0.5, 0.2]와 같이 4차원의 벡터로 표현할 수 있다. 이를 위해, 디코더는 메모리에 저장된 인코딩 데이터 및 인코딩 데이터를 디코딩 하기 위한 가중치를 메모리로부터 독출할 수 있다. 그리고, 디코더는 메모리로부터 독출한 인코딩 데이터 및 가중치에 딥러닝 연산을 수행하여, 인코딩 데이터를 디코딩할 수 있다. 즉, 디코더는 n차원의 행렬로 표현된 인 코딩 데이터를 n보다 높은 차원의 행렬로 복원할 수 있다. 이를 위해, 디코더는 도 2에 도시된 바와 같이, 메모리에 연결될 수 있다. 이후, 디코더는 디코딩을 통해 획득한 디코딩 데이터를 프로세서로 전송할 수 있다. 이를 위해, 디 코더는 도 2에 도시된 바와 같이, 프로세서에 연결될 수 있다. 이후, 프로세서는 디코더로부터 수신한 디코딩 데이터를 이용하여 연산을 수행할 수 있다. 이는 도 3 을 참조하여 후술하기로 한다. 이와 같이, 본 개시는 연산 데이터를 인코딩하여 메모리에 저장함으로써, 제한된 용량의 메모리에서도 인공 지 능 모델을 효과적으로 구현할 수 있다. 한편, 도 2에서는 메모리, 프로세서, 인코더 및 디코더를 별개의 구성으로 도시하였으나, 이는 일 실시 예일 뿐이다. 예를 들어, 인코더 및 디코더는 메모리에 포함될 수 있다. 이 경우, 인코더는 메모리 내부에서 프로세서에 의해 출력된 연산 데이터의 인코딩을 수행하고, 인코딩 된 데이터를 메모리에 저장하는 동작을 수행할 수 있다. 그리고, 디코더는 인코더에 의해 인코 딩 된 데이터의 디코딩을 메모리 내부에서 수행할 수 있다. 이에 따라, 본 개시는 인코더 및 메모리 간의 데이터 송수신에 의해 발생하는 전력 소모 및 디코더 및 메모리간의 데이터 송수신에 의해 발생하는 전력 소모를 제거 또는 축소할 수 있다. 또한, 인코더 및 디코더는 프로세서에 포함될 수도 있다. 이 경우, 인코더는 프로세서 내부에서 프로세서에 의해 생성된 연산 데이터의 인코딩을 수행하고, 디코더는 프로세서 내부에서 인코더에 의해 인코딩 된 데이터의 디코딩을 수행할 수 있다. 이에 따라, 본 개시는 인코더 및 프로세서간의 데이터 송수신에 의해 발생하는 전력 소모 및 디코더 및 프로세서간의 데이터 송수신에 의해 발생하는 전력 소모를 제거 또는 축소할 수 있다. 또한, 프로세서, 인코더 및 디코더가 메모리에 포함될 수도 있다. 이 경우, 프로세서(12 0)는 입력 데이터로부터 출력 데이터를 획득하기 위한 일련의 연산을 메모리 내부에서 수행하고, 연산 결 과 생성된 연산 데이터를 메모리에 저장할 수 있다. 그리고, 인코더는 메모리 내부에서 프로세 서에 의해 생성된 연산 데이터의 인코딩을 수행하고, 디코더는 메모리 내부에서 인코더에 의해 인코딩 된 데이터의 디코딩을 수행할 수 있다. 도 3은 본 개시의 일 실시 예에 따른 인코딩 된 데이터를 이용하여 출력 데이터를 생성하는 실시 예를 설명하기 위한 도면이다. 본 개시의 일 실시 예에 따른 인공 지능 모델은, 입력 데이터가 입력되는 입력 레이어, 출력 데이터를 출력하는 출력 레이어 및, 입력 레이어와 출력 레이어 사이에 위치하고, 딥러닝 연산에 기초한 연산 데이터를 출력하는 복수의 히든 레이어를 포함할 수 있다. 특히, 도 3을 참조하면, 본 개시의 일 실시 예에 따른 인공 지능 모델은 특정 레이어(이는, 입력 레이어가 될 수 있음은 물론, 복수의 히든 레이어 중 어느 하나가 될 수 있다.)가 출력하는 연산 데이터를 인코딩한 인코 딩 데이터를 포함하는 제1 히든 레이어 및 제1 히든 레이어의 인코딩 데이터를 디코딩한 디코딩 데이 터를 포함하는 제2 히든 레이어를 더 포함할 수 있다. 여기에서, 제1 히든 레이어는 인코딩 레이어로 명명되고, 제2 히든 레이어는 디코딩 레이어로 명명될 수도 있다. 제1 히든 레이어에 포함된 인코딩 데이터는 상술한 바와 같이 메모리에 저장될 수 있다. 일 실시 예에 따라 본 개시의 인공 지능 모델이 합 성곱 신경망(Convolutional Neural Network, CNN)으로 구현되는 경우, 프로세서는 특정 레이어의 이 전 레이어(미도시)를 컨벌루션 연산 처리하여 생성한 연산 데이터(즉, 특징 맵(feature map))를 인코더를 통해 인코딩하여 메모리에 저장할 수 있다. 그리고, 프로세서는 다음 레이어의 연산을 위해 인 코딩 데이터를 복원하여 디코딩 데이터를 획득하고, 디코딩 데이터 및 다음 레이어의 가중치에 기초한 연 산을 수행할 수 있다. 또는, 본 개시의 인공 지능 모델이 순환 신경망 (Recurrent Neural Network, RNN)인 경우, 프로세서는 이 전 시점(t-1)의 입력 데이터에 기초하여 출력된 연산 데이터를 인코더를 통해 인코딩하여 메모리에 저장할 수 있다. 그리고, 프로세서는 현재 시점(t)의 입력 데이터의 연산 데이터를 출력하는 단계에서 메 모리에 저장된 인코딩 데이터를 복원한 디코딩 데이터를 현재 시점의 입력 데이터와 함께 고려하여 연산데이터를 출력할 수 있다. 한편, 도 3에서는 하나의 인코딩 레이어 및 하나의 디코딩 레이어를 도시하였으나, 실시 예에 따라 인코딩 레이 어 및 디코딩 레이어는 복수가 될 수도 있다. 도 4는 본 개시의 일 실시 예에 따른 인공 지능 모델의 학습 방법을 설명하기 위한 도면이다. 도 4를 참조하면, 본 개시의 일 실시 예에 따른 인공 지능 모델은 복수의 레이어를 포함할 수 있다. 그리고, 인 공 지능 모델은 연산 데이터의 인코딩을 위한 제1 히든 레이어(미도시) 및 인코딩 된 데이터의 디코딩을 위한 제2 히든 레이어(미도시)를 더 포함할 수 있다. 여기에서, 제1 히든 레이어는, Layer n의 차원보다 상대적으로 낮고, 제2 히든 레이어는, Layer n의 차원과 같을 수 있다. 예를 들어, 본 개시의 일 실시 예에 따른 인공 지능 모델은 Layer n 및 Layer n+1 사이에 Layer n의 연산 데이 터를 인코딩 하기 위한 제1 히든 레이어 및 제1 히든 레이어의 인코딩 데이터를 디코딩한 하기 위한 제2 히든 레이어를 포함할 수 있다. 이와 같이, 본 개시는 인공 지능 모델을 구성하는 복수의 레이어 중 일부 레이어의 사이에서 인코딩 및 디코딩이 수행되는 것을 특징으로 한다. 한편, 실시 예에 따라 인공 지능 모델은 복수의 레이어 사이에서 복수의 인코딩 및 디코딩이 수행될 수도 있다. 이하에서는 설명의 편의를 위해, 인공 지능 모델은 인코딩을 위한 제1 히든 레이어 및 디코딩을 위한 제2 히든 레이어를 포함하는 것으로 상정하여 설명한다. Layer n 및 Layer n+1 사이에 제1 및 제2 히든 레이어를 포함하는 인공 지능 모델은, Layer n을 재구성 (reconstruction)한 제2 히든 레이어를 생성하도록, 제1 및 제2 히든 레이어의 가중치를 학습할 수 있다. 구체적으로, 인공 지능 모델은 Layer n의 노드(또는, 뉴런) 수보다 적은 수의 노드를 가진 레이어로 Layer n의 데이터를 재구성(reconstruction) 하도록 제1 및 제2 히든 레이어의 가중치를 학습 할 수 있다. 예를 들어, Layer n 및 Layer n+1 사이에 제1 및 제2 히든 레이어가 추가된 경우, 인공 지능 모델은 Layer n의 연산 데이터(즉, Layer n의 출력 데이터) 및 제1 히든 레이어의 가중치의 연산에 기초하여, 제1 히든 레이어의 출력 데이터를 출력하고, 제1 히든 레이어의 출력 데이터 및 제2 히든 레이어의 가중치의 연산에 기초하여, 제2 히든 레이어의 출력 데이터를 출력할 수 있다. 그리고, 인공 지능 모델은 Layer n의 출력 데이터 및 제2 히든 레이어의 출력 데이터의 오차가 최소가 되도록, 제1 및 제2 히든 레이어의 가중치를 학습할 수 있다. 일 실시 예에 따르면, 본 개시의 인공 지능 모델은 오토인코더와 유사한 구조가 될 수 있다. 오토인코더는 입력 데이터와 유사한 데이터를 출력하도록 학습을 수행하는 인공 지능 모델로서, 예를 들어, Layer n 및 Layer n+1 사이에서 인코딩 및 디코딩이 수행되는 경우, 본 개시의 제1 및 제2 히든 레이어는 오토인코더와 유사하게 Layer n으로부터 출력된 데이터와 유사한 데이터를 출력하도록 학습될 수 있다. 다만, 오토인코더는 오토인코더 자체가 하나의 인공 지능 모델로 존재하지만, 본 개시의 인코딩 및 디코딩은 인공 지능 모델의 복수의 레이어 중 일부 레이어 사이에서만 수행된다는 차이가 있다. 한편, 인공 지능 모델의 학습은 제1 및 제2 히든 레이어를 포함하는 전체 시스템의 학습을 통해 수행될 수 있다. 이 경우, 인공 지능 모델은 복수의 레이어, 제1 및 제2 히든 레이어를 포함하는 전체 시스템의 오류가 최 소가 되도록 학습을 수행할 수 있다. 여기에서, 학습은 일반적인 Forward Propagation 및 Back Propagation을 통해 수행될 수 있는 바, 구체적인 설명은 생략한다. 이와 같이, 복수의 레이어 및 복수의 레이어 중 일부 레이어의 사이에 추가된 제1 및 제2 히든 레이어를 포함하 는 전체 시스템을 학습함으로써, 본 개시의 인공 지능 모델은 복수의 레이어의 가중치, 제1 및 제2 히든 레이어 의 가중치를 한번에 획득할 수 있다. 한편, 이는 일 실시 예일 뿐, 본 개시는 제1 및 제2 히든 레이어를 독립적으로 학습시킬 수도 있다. 예를 들어, Layer n 및 Layer n+1 사이에 제1 및 제2 히든 레이어가 추가된 경우, 본 개시는 Layer n의 연산 데이터를 제1 히든 레이어의 입력으로 설정하고, Layer n의 연산 데이터를 제2 히든 레이어의 출력으로 설정할 수 있다. 그리 고, Layer n의 데이터 크기보다 상대적으로 작은 크기로, Layer n의 데이터를 재구성 하도록 제1 및 제2 히든 레이어의 가중치를 학습시킴으로서, 제1 및 제2 히든 레이어의 가중치를 획득할 수 잇다. 이에 따라, 인코더는 프로세서로부터 연산 데이터가 수신되면, 수신된 연산 데이터를 학습된 가중치 를 이용하여 인코딩 할 수 있다. 또한, 디코더는 메모리에 저장된 인코딩 된 데이터를 학습된 가중치 를 이용하여 디코딩 할 수 있다. 한편, 제1 및 제2 히든 레이어가 포함되지 않은 복수의 레이어의 학습이 완료된 상태에서, 제1 및 제2 히든 레 이어가 추가된 경우이면, 본 개시의 일 실시 예에 따른 인공 지능 모델은 제1 및 제2 히든 레이어의 가중치를 추가적으로 학습할 수도 있다. 이 경우, 인공 지능 모델은 학습이 완료된 복수의 레이어의 가중치는 고정할 수 있다. 즉, 인공 지능 모델은 학 습이 완료된 복수의 레이어의 가중치는 고정하고, 복수의 레이어, 제1 및 제2 히든 레이어를 포함하는 전체 시 스템의 오류가 최소가 되도록 학습을 수행할 수 있다. 이에 따라, 본 개시는 압축의 필요성 여부에 따라 선택적으로 제1 및 제2 히든 레이어를 인공 지능 모델에 추가 할 수 있다. 도 5는 본 개시의 일 실시 예에 따른 복수의 인코더 및 복수의 디코더를 설명하기 위한 도면이다. 본 개시의 일 실시 예에 따른 인공 지능 모델은 복수의 인코딩 및 복수의 디코딩을 수행할 수 있다. 예를 들어, 도 5를 참조하면, 인공 지능 모델은 제1 및 제2 레이어 사이의 제1 히든 레이어(미도시) 및 제2 히든 레이어(미 도시), 제3 및 제4 레이어 사이의 제3 히든 레이어(미도시) 및 제4 히든 레이어(미도시)를 포함할 수 있다. 여 기에서, 제1 히든 레이어는 제1 레이어의 연산 데이터를 인코딩 하기 위한 레이어이고, 제2 히든 레이어는 제1 히든 레이어의 인코딩 데이터를 디코딩 하기 위한 레이어가 될 수 있다. 그리고, 제3 히든 레이어는 제3 레이어 의 연산 데이터를 인코딩 하기 위한 레이어이고, 제4 히든 레이어는 제3 히든 레이어의 인코딩 데이터를 디코딩 하기 위한 레이어가 될 수 있다. 인공 지능 모델에 포함된 제1 내지 제4 히든 레이어 각각은 이전 레이어를 재구성한 출력 레이어를 생성하도록 학습될 수 있다. 예를 들어, 상술한 실시 예에서 제1 및 제2 히든 레이어는 제1 레이어를 재구성하도록 학습되 고, 제3 및 제4 히든 레이어는 제3 레이어를 재구성도록 학습될 수 있다. 이 경우, 본 개시의 일 실시 예에 따른 인코더는 복수 개로 구현될 수 있다. 즉, 본 개시의 일 실시 예에 따른 전자 장치는 제1 히든 레이어의 가중치를 이용하여 제1 레이어의 출력 데이터를 인코딩 하기 위한 제 1 인코더(미도시) 및 제3 히든 레이어의 가중치를 이용하여 제3 레이어의 출력 데이터를 인코딩 하기 위한 제2 인코더(미도시)를 포함할 수 있다. 마찬가지로, 본 개시의 일 실시 예에 따른 디코더도 복수 개로 구현될 수 있다. 즉, 본 개시의 일 실시 예 에 따른 전자 장치는 제1 인코더(미도시)에 의해 인코딩 된 데이터를 제2 히든 레이어의 가중치를 이용하 여 디코딩 하기 위한 제1 디코더(미도시) 및 제2 인코더(미도시)에 의해 인코딩 된 데이터를 제4 히든 레이어의 가중치를 이용하여 디코딩 하기 위한 제2 디코더(미도시)를 포함할 수 있다. 이는, 각 레이어가 가진 특징(feature)은 상이할 수 있고, 이에 따라 각 레이어 마다 인코딩을 위한 가중치 및 디코딩을 위한 가중치가 달라질 수 있음을 고려한 것으로써, 본 개시는 복수의 인코더 및 복수의 디코더를 통해 다양한 데이터를 인코딩 및 디코딩 할 수 있다. 한편, 본 개시의 일 실시 예에 따른 인공 지능 모델은 인코딩을 위한 복수의 히든 레이어가 가중치를 공유하고, 디코딩을 위한 복수의 히든 레이어가 가중치를 공유하도록 학습을 수행할 수도 있다. 구체적으로, 인공 지능 모델은 제1 및 제3 히든 레이어의 가중치를 공유시키고, 제2 및 제4 히든 레이어의 가중 치를 공유시킴으로써, 제1 및 제3 히든 레이어가 동일한 가중치를 갖도록 학습하고, 제2 및 제4 히든 레이어가 동일한 가중치를 갖도록 학습할 수 있다. 이 경우, 인공 지능 모델은 학습을 수행하는 단계에서, 제1 및 제3 히 든 레이어가 가 동일한 가중치를 갖도록 제1 및 제3 히든 레이어의 가중치의 값을 업데이트하고, 제2 및 제4 히 든 레이어가 동일한 가중치를 갖도록 제2 및 제4 히든 레이어의 가중치의 값을 업데이트할 수 있다. 이에 따라, 본 개시의 전자 장치는 상이한 레이어의 출력 데이터를 하나의 인코더를 통해 인코딩 할 수 있다. 마찬가지로, 본 개시의 전자 장치는 상이한 레이어에서 인코딩 된 데이터를 하나의 디코더 를 통해 디코딩 할 수 있다. 이에 따라, 본 개시는 복수의 인코더 및 복수의 디코더를 포함하는 경우와 비교하였을 때, 비교적 작은 크기의 전자 장치에서도 구현될 수 있다. 또한, 가중치를 공유함으로써 학습에 소요되는 시간을 줄일 수 있다. 도 6은 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 상세 블록도이다. 도 6을 참조하면, 본 개시의 일 실시 예에 따른 전자 장치는 메모리, 인코더, 디코더, 입 력부, 통신부, 디스플레이, 스피커 및 프로세서를 포함할 수 있다. 이하, 설명한 설명과 중복되는 부분은 생략하거나 축약하여 설명한다. 입력부는 사용자 명령을 수신할 수 있다. 이를 위해, 입력부는 터치 센서, (디지털) 펜 센서, 압력 센서, 또는 키를 포함할 수 있다. 터치 센서는, 예를 들면, 정전식, 감압식, 적외선 방식, 또는 초음파 방식 중 적어도 하나의 방식을 사용할 수 있다. (디지털) 펜 센서는, 예를 들면, 터치 패널의 일부이거나, 별도의 인식 용 쉬트를 포함할 수 있다. 키는, 예를 들면, 물리적인 버튼, 광학식 키, 또는 키패드를 포함할 수 있다. 또한, 입력부는 마이크를 포함할 수 있다. 여기에서, 마이크는 사용자 발화 음성을 수신할 수 있다. 통신부는 외부 장치와 통신을 수행하여 다양한 데이터를 송수신할 수 있다. 구체적으로, 통신부는 외 부 장치와 통신을 수행하여 인공 지능 모델을 수신할 수 있다. 여기에서, 인공 지능 모델은 인코딩을 위한 레이 어 및 디코딩을 위한 레이어를 포함하는 모델이 될 수 있음은 물론, 인코딩을 위한 레이어 및 디코딩을 위한 레 이어가 없는 모델이 될 수도 있다. 후자의 경우, 전자 장치는 인코딩을 위한 레이어 및 디코딩을 위한 레 이어를 인공 지능 모델에 추가하고, 인코딩을 위한 레이어 및 디코딩을 위한 레이어를 포함하는 인공 지능 모델 을 학습시킬 수 있다. 또한, 통신부는 외부 장치와 통신을 수행하여 인공 지능 모델의 생성을 위한 다양한 데이터를 수신할 수 있다. 이를 위해, 통신부는 무선 통신 칩, 와이 파이 칩, 블루투스 칩 등을 포함할 수 있다. 디스플레이는 다양한 화면을 표시할 수 있다. 특히, 디스플레이는 인공 지능 모델에 의해 출력된 출 력 데이터에 대응되는 화면을 표시할 수 있다. 여기에서, 출력 데이터에 대응되는 화면은 출력 데이터에 기초하 여 생성된 메시지, 이미지 등이 될 수 있다. 디스플레이는 LCD(Liquid Crystal Display), OLED(Organic Light Emitting Diodes), AM-OLED(Active- Matrix Organic Light-Emitting Diode), LcoS(Liquid Crystal on Silicon) 또는 DLP(Digital Light Processing) 등과 같은 다양한 디스플레이 기술로 구현될 수 있다. 또한, 디스플레이는 플렉서블 디스플레 이(flexible display)의 형태로 전자 장치의 전면 영역 및, 측면 영역 및 후면 영역 중 적어도 하나에 결 합될 수도 있다. 또한, 디스플레이는 터치 센서를 구비한 터치 스크린으로 구현될 수도 있다. 스피커는 오디오 처리부(미도시)에 의해 디코딩이나 증폭, 노이즈 필터링과 같은 다양한 처리 작업이 수행 된 각종 오디오 데이터를 출력하는 구성이다. 또한, 스피커는 각종 알림 음이나 음성 메시지를 출력할 수 있다. 본 개시에 따르면 인공 지능 모델의 학습이 완료된 경우 또는 인공 지능 모델에 의해 출력 데이터가 출력 되는 경우, 스피커는 알림 음 등을 출력할 수 있다. 도 7은 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 순서도이다. 본 개시의 일 실시 예에 따른 전자 장치는, 인공 지능 모델의 복수의 레이어 중 어느 하나에서 출력되는 연산 데이터를 인코딩하여 메모리에 저장(S710)할 수 있다. 구체적으로, 전자 장치는 인공 지능 모델 의 복수의 레이어 중 어느 하나에서 출력되는 연산 데이터를 인코더를 통해 인코딩하고, 인코딩 된 데이터를 메 모리에 저장할 수 있다. 그리고, 전자 장치는 메모리에 저장된 인코딩 된 데이터를 디코딩하여 연산 데이터에 대응되는 복원 데이터를 획득(S720)할 수 있다. 구체적으로, 전자 장치는 디코더를 통해 인코딩 된 데이터를 디코딩하여 연산 데이터에 대응되는 복원 데이터를 획득할 수 있다. 이와 같이, 본 개시는 다이나믹 데이터의 압축을 통해, 제한된 메모리를 가진 모바일 장치 등에서도 가진 인공 지능 기술을 효율적으로 구현할 수 있다. 그리고, 전자 장치는 획득된 복원 데이터를 인공 지능 모델의 복수의 레이어 중 다른 하나로 제공(S730)할 수 있다. 이에 따라, 인공 지능 모델의 출력 레이어는 이전 레이어의 연산 데이터 및 복원 데이터에 기초한 출 력 데이터를 생성할 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 대한 소프트웨어/하드웨어 업그 레이드 만으로도 구현될 수 있다. 또한, 상술한 본 개시의 다양한 실시 예들은 전자 장치에 구비된 임베디드 서버, 또는 외부의 서버를 통해 수행 되는 것도 가능하다.상술한 다양한 실시 예에 따른 전자 장치의 제어 방법은 프로그램으로 구현되어 다양한 기록 매체에 저장될 수 있다. 즉, 각종 프로세서에 의해 처리되어 상술한 다양한 제어 방법을 실행할 수 있는 컴퓨터 프로그램이 기록 매체에 저장된 상태로 사용될 수도 있다. 비일시적 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체가 아니 라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 구체적으로는, 상 술한 다양한 어플리케이션 또는 프로그램들은 CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등과 같은 비일시적 판독 가능 매체에 저장되어 제공될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2019-0089232", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 발명이 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2019-0089232", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 블록도이다. 도 2는 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 상세 블록도이다. 도 3은 본 개시의 일 실시 예에 따른 인코딩 된 데이터를 이용하여 출력 데이터를 생성하는 실시 예를 설명하기 위한 도면이다. 도 4는 본 개시의 일 실시 예에 따른 인공 지능 모델의 학습 방법을 설명하기 위한 도면이다. 도 5는 본 개시의 일 실시 예에 따른 복수의 인코딩 및 복수의 디코딩을 설명하기 위한 도면이다. 도 6은 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 상세 블록도이다. 도 7은 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 순서도이다."}
