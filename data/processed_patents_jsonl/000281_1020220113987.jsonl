{"patent_id": "10-2022-0113987", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0035656", "출원번호": "10-2022-0113987", "발명의 명칭": "메타버스 서버, 이를 포함하는 인공지능 캐릭터 제공 시스템 및 인공지능 캐릭터 제공 방법", "출원인": "주식회사 에이엘아이", "발명자": "이청안"}}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "시청자가 소지하는 시청자 단말기와 연동하는 메타버스 서버에 있어서, 상기 시청자의 아바타와 상대 캐릭터가 서로 감정 및 대화를 교류하여 상호작용하는 메타버스 캐릭터 콘텐츠를생성하는 콘텐츠 생성부; 상기 시청자 단말기로부터 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나를 포함하는 실시간 시청자 정보를 수집하는 수집부; 및상기 시청자 정보, 상기 시청자가 시청 중인 동영상의 영상 정보, 소리 정보 및 댓글 정보 중 적어도 하나를 미리 마련된 감정예측모델에 입력하여 상기 시청자가 느낄 것으로 예측되는 예측 감정 및 상기 시청자의 현재 감정을 인식하는 감정인식부를 포함하고, 상기 콘텐츠 생성부는, 상기 예측 감정 및 상기 현재 감정에 기반하여 상기 상대 캐릭터의 감정을 생성하고, 상기 상대 캐릭터가 생성된 감정을 표현하도록 하는 것을 특징으로 하는 메타버스 서버."}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 동영상의 상황을 텍스트로 설명하는 동영상 설명 정보를 생성하는 동영상 설명 생성부를 더 포함하고, 상기 콘텐츠 생성부는, 상기 동영상 설명 정보 및 상기 댓글 정보에 기반하여 상기 동영상의 맥락에 맞는 대화를 생성하고 상기 상대캐릭터가 생성된 대화를 출력하도록 하는 것을 특징으로 하는 메타버스 서버."}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 감정예측모델은, 하향식 주의집중 인공신경망 모델로 마련되되,상기 하향식 주의집중 인공신경망 모델은,상기 예측 감정 및 현재 감정을 인식하면, 이를 회귀시켜 상기 실시간 시청자 정보, 상기 동영상의 영상 정보,소리 정보 및 댓글 정보 중에서 집중해야 하는 정보를 판단하고, 상기 집중해야하는 정보에 기초하여 다음번 예측 감정 및 현재 감정을 인식하는 것을 특징으로 하는 메타버스서버."}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서, 상기 감정인식부는, 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나가 감정별로 카테고리화된 히스토리 시청자 정보를사전에 학습하여 마련된 시청자 임베딩 벡터를 로딩하고, 상기 감정예측모델에 상기 시청자 임베딩 벡터를 상기 실시간 시청자 정보와 함께 입력하여 상기 현재 감정을인식하는 것을 특징으로 하는 메타버스 서버.공개특허 10-2024-0035656-3-청구항 5 제4항에 있어서, 상기 콘텐츠 생성부는, 상기 대화가 출력될 때마다 상기 상대 캐릭터의 감정과 대화를 각각 임베딩하여 감정 임베딩 벡터 및 대화 임베딩 벡터를 생성하고, 상기 감정 임베딩 벡터 및 대화 임베딩 벡터에 기초하여 연속 임베딩 벡터를 생성하며, 상기 연속 임베딩 벡터를 이용하여 상기 상대 캐릭터의 감정 및 대화를 동시에 생성하는 것을 특징으로 하는 메타버스 서버."}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서, 상기 콘텐츠 생성부는, 상기 히스토리 시청자 정보 및 상기 실시간 시청자 정보 중 적어도 하나에 기초하여 상기 시청자의 대화 스타일에 대응되는 스타일로 대화가 출력되도록 하는 것을 특징으로 하는 메타버스 서버."}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 감정인식부는, 복수의 감정들을 일정 차원의 공간상에 맵핑한 공간 좌표를 미리 마련하고,상기 현재 감정을 상기 공간 좌표 상에 맵핑하여 상기 현재 감정의 세기를 결정하는 것을 특징으로 하는 메타버스 서버."}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 콘텐츠 생성부는, 상기 현재 감정의 세기 및 감정별로 미리 설정된 역치에 따라 상기 상대 캐릭터의 반응 여부를 결정하는 것을특징으로 하는 메타버스 서버."}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 콘텐츠 생성부는, 상기 아바타와 상기 상대 캐릭터 간의 상호 작용을 통해 상기 역치를 조절하여 상기 상대 캐릭터의 감정 표현에대한 민감도를 조절하는 것을 특징으로 하는 메타버스 서버."}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "시청자가 소지하는 시청자 단말기와 연동하는 메타버스 서버에서의 인공지능 캐릭터 제공 방법에 있어서, 상기 시청자 단말기로부터 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나를 포함하는 실시간 시청자 정보를 수집하는 단계; 상기 시청자 정보, 상기 시청자가 시청 중인 동영상의 영상 정보, 소리 정보 및 댓글 정보 중 적어도 하나를 미리 마련된 감정예측모델에 입력하여 상기 시청자가 느낄 것으로 예측되는 예측 감정 및 상기 시청자의 현재 감정을 인식하는 단계; 및상기 시청자의 아바타와 상대 캐릭터가 서로 감정 및 대화를 교류하여 상호작용하는 메타버스 캐릭터 콘텐츠를생성하는 단계를 포함하고, 상기 메타버스 캐릭터 콘텐츠를 생성하는 단계에서는, 공개특허 10-2024-0035656-4-상기 예측 감정 및 상기 현재 감정에 기반하여 상기 상대 캐릭터의 감정을 생성하고, 상기 상대 캐릭터가 생성된 감정을 표현하도록 하는 것을 특징으로 하는 인공지능 캐릭터 제공 방법."}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 동영상의 상황을 텍스트로 설명하는 동영상 설명 정보를 생성하는 단계를 더 포함하고, 상기 메타버스 캐릭터 콘텐츠를 생성하는 단계에서는, 상기 동영상 설명 정보 및 상기 댓글 정보에 기반하여 상기 동영상의 맥락에 맞는 대화를 생성하고, 상기 상대캐릭터가 생성된 대화를 출력하도록 하는 것을 특징으로 하는 인공지능 캐릭터 제공 방법."}
{"patent_id": "10-2022-0113987", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나를 포함하는 실시간 시청자 정보를 획득하는 시청자 단말기; 및상기 시청자의 아바타와 상대 캐릭터가 서로 감정 및 대화를 교류하여 상호작용하는 메타버스 캐릭터 콘텐츠를생성하고, 상기 시청자 단말기로부터 상기 실시간 시청자 정보를 수집하며, 상기 실시간 시청자 정보, 상기 시청자가 시청 중인 동영상의 영상 정보, 소리 정보 및 댓글 정보 중 적어도 하나를 미리 마련된 감정예측모델에입력하여 상기 시청자가 느낄 것으로 예측되는 예측 감정 및 상기 시청자의 현재 감정을 인식하는 메타버스 서버를 포함하고, 상기 메타버스 서버는, 상기 예측 감정 및 상기 현재 감정에 기반하여 상기 상대 캐릭터의 감정을 생성하고, 상기 상대 캐릭터가 생성된 감정을 표현하도록 하는 것을 특징으로 하는 인공지능 캐릭터 제공 시스템."}
{"patent_id": "10-2022-0113987", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 메타버스 서버, 이를 포함하는 인공지능 캐릭터 제공 시스템 및 인공지능 캐릭터 제공 방법에 관한 것으로, 메타버스 서버는, 메타버스 캐릭터 콘텐츠를 생성하는 콘텐츠 생성부; 실시간 시청자 정보를 수집하는 수 집부; 및 시청자가 느낄 것으로 예측되는 예측 감정 및 상기 시청자의 현재 감정을 인식하는 감정인식부를 포함 하고, 콘텐츠 생성부는, 예측 감정 및 현재 감정에 기반하여 상대 캐릭터의 감정을 생성하고, 상대 캐릭터가 생 성된 감정을 표현하도록 한다. 이에 의해 메타버스 상에서 동영상을 시청 중인 시청자와 감정적으로 공감하여 감 정을 교류하고, 동영상 및 시청자가 느끼는 감정에 기반하여 시청자와 대화를 나눌 수 있는 인공지능 캐릭터가 시청자의 아바타와 상호작용할 수 있는 메타버스 캐릭터 콘텐츠를 생성하여 제공함으로써 시청자가 느끼는 외로 움을 감소시킬 수 있다."}
{"patent_id": "10-2022-0113987", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 메타버스 서버, 이를 포함하는 인공지능 캐릭터 제공 시스템 및 인공지능 캐릭터 제공 방법에 관한 것으로, 보다 상세하게는 메타버스 상에서 감정 및 대화를 교류할 수 있는 메타버스 캐릭터 콘텐츠를 제공하는 메타버스 서버, 이를 포함하는 인공지능 캐릭터 제공 시스템 및 인공지능 캐릭터 제공 방법에 관한 것이다."}
{"patent_id": "10-2022-0113987", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "2019년 시장조사전문기업 엠브레인 트렌드 모니터의 조사 결과에 따르면 한국 성인 10명 중 6명 정도가 일상생 활에서 외로움을 느끼는 것으로 나타났으며, 외로움은 우울증으로 악화될 수 있기에 심각한 문제가 될 수 있다. 우울증의 경우 OECD에서 2021년 조사한 결과에 따르면 한국의 유병률이 36.8%로 OECD국가 중 가장 높았다. 또한 건강보험심사평가원의 '연도별 연령대별 우울증 환자 현황' 자료를 보면 5년 사이 우울증 환자 수가 약 30% 증 가하여 우울증 환자가 증가하는 추세인 것을 알 수 있다. 또한 세계적으로도 코로나의 영향으로 인하여 외로움을 느끼는 사람의 수가 증가하고 있다. 2021년 Ipsos의 조 사에 따르면 41%의 사람들이 6개월 전에 비하여 더 외로움을 느끼는 것으로 보고하였다. 반면, 서울시의 2019년 조사에 따르면 반려동물을 키우는 경우 외로움 감소 등 긍정적인 효과가 있는 것으로 조 사되었다. 하지만 반려동물을 키우는 것은 반려동물의 건강 문제, 비용 등 쉽지는 않은 것이 현실이며 이로 인 해 검역본부의 2020년 조사를 보면 약 13만마리의 반려동물이 유기되는 등 사회적 문제가 되고 있다. 한편 OTT는 Over the top의 약자로 유튜브(YouTube)나 넷플릭스(Netflix)와 같이 인터넷을 통해 영상을 볼 수 있는 서비스를 의미하는데, Statista의 2021년 연구에 따르면 2022년 전세계 OTT 시장은 2천억 달러에 달할 것으로 예상되며 2026년까지 매년 9.72% 성장할 것으로 예상되고 있다. 이런 영상 서비스를 즐기는 방법에 있어 최근에는 단순히 영상만 보는 것이 아니라 같은 영상을 시청하는 시청 자들끼리 소통하는 방식으로 변화하고 있다. 구체적으로 많은 OTT 서비스에서 기본적으로 제공하는 댓글 기능을 많은 사람들이 적극적으로 사용하고 있고 심지어는 댓글을 영상에 결합한 댓글모음 영상이 많은 인기를 얻고 있다. 또한 Maru/Matchbox의 2020년 조사에 따르면 미국 성인 약 20%가 와치파티(watch party) 서비스를 사용해 본 적이 있는 것으로 나타났다. 와치파티 서비스는 여러 사람들이 인터넷상에 같이 모여 같은 영상을 보며 채팅 을 할 수 있는 서비스이다. 이에 동일한 영상을 시청하는 상대방과 시청자가 감정적으로 공감하고 한담을 나누되, 시청자의 성향이 반영될 수 있는 개인화된 디지털 서비스가 제공될 필요가 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국공개특허공보 제10-2013-0113739호"}
{"patent_id": "10-2022-0113987", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기와 같은 문제를 해결하기 위해 안출된 것으로, 본 발명의 목적은 시청자의 외로움을 감소시키기 위해 메타버스 상에서 동영상을 시청 중인 시청자와 감정적으로 공감하여 감정을 교류하고, 동영상 및 시청자가 느끼는 감정에 기반하여 시청자와 대화를 나눌 수 있는 인공지능 캐릭터와 시청자의 아바타가 서로 상호작용할 수 있는 메타버스 캐릭터 콘텐츠를 생성하여 제공하는 메타버스 서버, 이를 포함하는 인공지능 캐릭터 제공 시 스템 및 인공지능 캐릭터 제공 방법을 제공하는 것이다."}
{"patent_id": "10-2022-0113987", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 시청자가 소지하는 시청자 단말기와 연동하는 메타버스 서버는, 상기 시청자의 아바타와 상대 캐릭터가 서로 감정 및 대화를 교류하여 상호작용하는 메타버스 캐릭터 콘텐츠를 생성하는 콘텐츠 생성부; 상기 시청자 단말기로부터 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나를 포함하는 실시간 시청자 정보를 수집하는 수집부; 및 상기 시청자 정보, 상기 시청자가 시청 중 인 동영상의 영상 정보, 소리 정보 및 댓글 정보 중 적어도 하나를 미리 마련된 감정예측모델에 입력하여 상기 시청자가 느낄 것으로 예측되는 예측 감정 및 상기 시청자의 현재 감정을 인식하는 감정인식부를 포함하고, 상 기 콘텐츠 생성부는, 상기 예측 감정 및 상기 현재 감정에 기반하여 상기 상대 캐릭터의 감정을 생성하고, 상기 상대 캐릭터가 생성된 감정을 표현하도록 한다. 그리고 상기 동영상의 상황을 텍스트로 설명하는 동영상 설명 정보를 생성하는 동영상 설명 생성부를 더 포함하 고, 상기 콘텐츠 생성부는, 상기 동영상 설명 정보 및 상기 댓글 정보에 기반하여 상기 동영상의 맥락에 맞는 대화를 생성하고 상기 상대 캐릭터가 생성된 대화를 출력하도록 할 수 있다. 또한 상기 감정예측모델은, 하향식 주의집중 인공신경망 모델로 마련되되,상기 하향식 주의집중 인공신경망 모 델은, 상기 예측 감정 및 현재 감정을 인식하면, 이를 회귀시켜 상기 실시간 시청자 정보, 상기 동영상의 영상 정보, 소리 정보 및 댓글 정보 중에서 집중해야 하는 정보를 판단하고, 상기 집중해야하는 정보에 기초하여 다 음번 예측 감정 및 현재 감정을 인식할 수 있다. 그리고 상기 감정인식부는, 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나가 감정별로 카테고리 화된 히스토리 시청자 정보를 사전에 학습하여 마련된 시청자 임베딩 벡터를 로딩하고, 상기 감정예측모델에 상 기 시청자 임베딩 벡터를 상기 실시간 시청자 정보와 함께 입력하여 상기 현재 감정을 인식할 수 있다. 또한 상기 콘텐츠 생성부는, 상기 대화가 출력될 때마다 상기 상대 캐릭터의 감정과 대화를 각각 임베딩하여 감 정 임베딩 벡터 및 대화 임베딩 벡터를 생성하고, 상기 감정 임베딩 벡터 및 대화 임베딩 벡터에 기초하여 연속 임베딩 벡터를 생성하며, 상기 연속 임베딩 벡터를 이용하여 상기 상대 캐릭터의 감정 및 대화를 동시에 생성할 수 있다. 그리고 상기 콘텐츠 생성부는, 상기 히스토리 시청자 정보 및 상기 실시간 시청자 정보 중 적어도 하나에 기초 하여 상기 시청자의 대화 스타일에 대응되는 스타일로 대화가 출력되도록 할 수 있다. 또한 상기 감정인식부는, 복수의 감정들을 일정 차원의 공간상에 맵핑한 공간 좌표를 미리 마련하고, 상기 현재 감정을 상기 공간 좌표 상에 맵핑하여 상기 현재 감정의 세기를 결정할 수 있다. 그리고 상기 콘텐츠 생성부는, 상기 현재 감정의 세기 및 감정별로 미리 설정된 역치에 따라 상기 상대 캐릭터 의 반응 여부를 결정할 수 있다. 또한 상기 콘텐츠 생성부는, 상기 아바타와 상기 상대 캐릭터 간의 상호 작용을 통해 상기 역치를 조절하여 상 기 상대 캐릭터의 감정 표현에 대한 민감도를 조절할 수 있다. 한편 상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 시청자가 소지하는 시청자 단말기와 연동하는 메 타버스 서버에서의 인공지능 캐릭터 제공 방법은, 상기 시청자 단말기로부터 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나를 포함하는 실시간 시청자 정보를 수집하는 단계; 상기 시청자 정보, 상기 시청자가 시청 중인 동영상의 영상 정보, 소리 정보 및 댓글 정보 중 적어도 하나를 미리 마련된 감정예측모델에 입력하 여 상기 시청자가 느낄 것으로 예측되는 예측 감정 및 상기 시청자의 현재 감정을 인식하는 단계; 및 상기 시청 자의 아바타와 상대 캐릭터가 서로 감정 및 대화를 교류하여 상호작용하는 메타버스 캐릭터 콘텐츠를 생성하는 단계를 포함하고, 상기 메타버스 캐릭터 콘텐츠를 생성하는 단계에서는, 상기 예측 감정 및 상기 현재 감정에 기반하여 상기 상대 캐릭터의 감정을 생성하고, 상기 상대 캐릭터가 생성된 감정을 표현한다. 그리고 상기 동영상의 상황을 텍스트로 설명하는 동영상 설명 정보를 생성하는 단계를 더 포함하고, 상기 메타 버스 캐릭터 콘텐츠를 생성하는 단계에서는, 상기 동영상 설명 정보 및 상기 댓글 정보에 기반하여 상기 동영상 의 맥락에 맞는 대화를 생성하고, 상기 상대 캐릭터가 생성된 대화를 출력하도록 할 수 있다. 한편 상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 인공지능 캐릭터 제공 시스템은, 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나를 포함하는 실시간 시청자 정보를 획득하는 시청자 단말기; 및 상기 시청자의 아바타와 상대 캐릭터가 서로 감정 및 대화를 교류하여 상호작용하는 메타버스 캐릭터 콘텐츠를 생성하고, 상기 시청자 단말기로부터 상기 실시간 시청자 정보를 수집하며, 상기 실시간 시청자 정보, 상기 시 청자가 시청 중인 동영상의 영상 정보, 소리 정보 및 댓글 정보 중 적어도 하나를 미리 마련된 감정예측모델에 입력하여 상기 시청자가 느낄 것으로 예측되는 예측 감정 및 상기 시청자의 현재 감정을 인식하는 메타버스 서 버를 포함하고, 상기 메타버스 서버는, 상기 예측 감정 및 상기 현재 감정에 기반하여 상기 상대 캐릭터의 감정 을 생성하고, 상기 상대 캐릭터가 생성된 감정을 표현하도록 한다."}
{"patent_id": "10-2022-0113987", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상술한 본 발명의 일측면에 따르면, 메타버스 서버, 이를 포함하는 인공지능 캐릭터 제공 시스템 및 인공지능 캐릭터 제공 방법을 제공함으로써, 메타버스 상에서 동영상을 시청 중인 시청자와 감정적으로 공감하여 감정을 교류하고, 동영상 및 시청자가 느끼는 감정에 기반하여 시청자와 대화를 나눌 수 있는 인공지능 캐릭터와 시청 자의 아바타가 서로 상호작용할 수 있는 메타버스 캐릭터 콘텐츠를 생성하여 제공함으로써 시청자가 느끼는 외 로움을 감소시킬 수 있다."}
{"patent_id": "10-2022-0113987", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "후술하는 본 발명에 대한 상세한 설명은, 본 발명이 실시될 수 있는 특정 실시예를 예시로서 도시하는 첨부 도 면을 참조한다. 이들 실시예는 당업자가 본 발명을 실시할 수 있기에 충분하도록 상세히 설명된다. 본 발명의 다양한 실시예는 서로 다르지만 상호 배타적일 필요는 없음이 이해되어야 한다. 예를 들어, 여기에 기재되어 있 는 특정 형상, 구조 및 특성은 일 실시예와 관련하여 본 발명의 정신 및 범위를 벗어나지 않으면서 다른 실시예로 구현될 수 있다. 또한, 각각의 개시된 실시예 내의 개별 구성요소의 위치 또는 배치는 본 발명의 정신 및 범 위를 벗어나지 않으면서 변경될 수 있음이 이해되어야 한다. 따라서, 후술하는 상세한 설명은 한정적인 의미로 서 취하려는 것이 아니며, 본 발명의 범위는, 적절하게 설명된다면, 그 청구항들이 주장하는 것과 균등한 모든 범위와 더불어 첨부된 청구항에 의해서만 한정된다. 도면에서 유사한 참조부호는 여러 측면에 걸쳐서 동일하거 나 유사한 기능을 지칭한다. 본 발명에 따른 구성요소들은 물리적인 구분이 아니라 기능적인 구분에 의해서 정의되는 구성요소들로써 각각이 수행하는 기능들에 의해서 정의될 수 있다. 각각의 구성요소들은 하드웨어 또는 각각의 기능을 수행하는 프로그 램 코드 및 프로세싱 유닛으로 구현될 수 있을 것이며, 두 개 이상의 구성요소의 기능이 하나의 구성요소에 포 함되어 구현될 수도 있을 것이다. 따라서 이하의 실시예에서 구성요소에 부여되는 명칭은 각각의 구성요소를 물 리적으로 구분하기 위한 것이 아니라 각각의 구성요소가 수행되는 대표적인 기능을 암시하기 위해서 부여된 것 이며, 구성요소의 명칭에 의해서 본 발명의 기술적 사상이 한정되지 않는 것임에 유의하여야 한다. 이하에서는 도면들을 참조하여 본 발명의 바람직한 실시예들을 보다 상세하게 설명하기로 한다. 도 1은 본 발명의 실시예에 따른 인공지능 캐릭터 제공 시스템의 구성을 설명하기 위한 도면이다. 본 발명의 인공지능 캐릭터 제공 시스템(10, 이하 시스템)은 시청자가 느끼는 외로움을 감소시키기 위하여 시청 자가 시청 중인 동영상의 맥락과 시청자의 감정에 기반하여 감정과 대화를 표현하는 인공지능 캐릭터를 생성할 수 있다. 그리고 시스템은 메타버스 상에서 시청자의 아바타와 인공지능 캐릭터인 상대 캐릭터가 서로 감정 및 대화를 교류하여 상호작용하는 메타버스 캐릭터 콘텐츠를 생성하고, 생성한 콘텐츠를 시청자에게 제공할 수 있다. 이를 위해 시스템은 시청자 단말기 및 메타버스 서버를 포함한다. 이러한 시스템은 네트워크 통신을 기반으로 하여 시청자에게 인공지능 캐릭터 제공 방법을 제공할 수 있으 며, 시청자 단말기 및 메타버스 서버에는 인공지능 캐릭터 제공 방법을 수행하기 위한 소프트웨어(어 플리케이션)가(이) 설치되어 실행될 수 있다. 시청자 단말기는 동영상을 시청하는 시청자가 소지하는 단말로써, 동영상을 시청 중인 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나를 포함하는 실시간 시청자 정보를 획득할 수 있다. 그리고 시청자 단말기는 획득한 실시간 시청자 정보를 메타버스 서버로 전달할 수 있다. 시청자 단말기는 도 1에서와 같이 모바일 단말기로 마련될 수 있으나, 이는 설명의 편의를 위한 예시적 사 항에 불과하며 스마트폰, 테블릿 PC, PDA, 스마트 패드 및 넷북 등 카메라를 탑재하거나 시청자의 얼굴 및 표정 을 감지할 수 있는 각종 센서를 탑재한 다양한 통신장치로 마련될 수 있다. 한편 메타버스 서버(100, 이하 서버)는, 시청자의 아바타와 인공지능 캐릭터인 상대 캐릭터가 서로 감정 및 대 화를 교류하여 상호작용하는 메타버스 캐릭터 콘텐츠를 생성할 수 있다. 서버는 시청자 단말기와 연동하고, 시청자 단말기로 메타버스 캐릭터 콘텐츠를 제공할 수 있다. 그리고 서버는 시청자 단말기로부터 동영상을 시청 중인 시청자의 음성 정보, 채팅 정보 및 표정 정 보 중 적어도 하나를 포함하는 실시간 시청자 정보를 수집할 수 있다. 이에 서버는 실시간 시청자 정보, 동영상의 영상 정보, 소리 정보 및 댓글 정보 중 적어도 하나를 미리 마 련된 감정예측모델에 입력하여 시청자가 느낄 것으로 예측되는 예측 감정 및 시청자의 현재 감정을 인식할 수 있다. 이때 댓글 정보는 유튜브와 같은 플랫폼 상에서 해당 동영상을 시청한 시청자들이 작성한 댓글 또는 동영 상을 업로드한 제작자가 작성하여 동영상과 함께 업로드한 스크립트를 의미할 수 있다. 그리고 서버는 예측 감정 및 현재 감정에 기반하여 상대 캐릭터의 감정을 생성하고, 메타버스 상에서 상대 캐릭터가 생성된 감정을 표현하도록 할 수 있다. 또한 서버는 동영상의 상황을 텍스트로 설명하는 동영상 설명 정보를 생성하고, 동영상 설명 정보와 동영 상에 포함된 댓글 정보에 기반하여 동영상의 맥락에 맞는 대화를 생성할 수 있다. 이에 서버는 상대 캐릭 터가 생성된 대화를 출력하도록 할 수 있다. 이에 의해 본 발명의 시스템이 동영상의 맥락과 시청자의 감정에 기반하여 상대 캐릭터가 감정을 표현하고 대화를 출력하도록 함으로써 시청자는 상대 캐릭터와 감정 및 대화를 교류하고 서로 상호작용할 수 있게 되어 시청자가 느끼는 외로움을 감소시킬 수 있게 된다. 한편 도 2는 도 1의 메타버스 서버의 구성을 설명하기 위한 도면, 도 3 및 도 4는 본 발명의 실시예에 따 른 감정인식부를 설명하기 위한 도면, 도 5는 본 발명의 콘텐츠 생성부를 설명하기 위한 도면, 도 6 은 본 발명의 콘텐츠 생성부에서 생성되는 메타버스 캐릭터 콘텐츠를 설명하기 위한 도면, 그리고 도 7은 본 발명의 콘텐츠 생성부에서 상대 캐릭터를 개인화하는 과정을 설명하기 위한 도면이다. 서버는 메타버스 캐릭터 콘텐츠를 생성하여 메타버스 상에서 시청자에게 메타버스 캐릭터 콘텐츠를 제공하 기 위해 마련되며, 이를 위해 수집부, 저장부 및 제어부를 포함할 수 있다. 그리고 서버는 인공지능 캐릭터 제공 방법을 수행하기 위한 소프트웨어(어플리케이션)가(이) 설치되어 실 행될 수 있으며, 수집부, 저장부 및 제어부는 인공지능 캐릭터 제공 방법을 수행하기 위한 소프 트웨어(어플리케이션)에 의해 제어될 수 있다. 수집부는 시청자 단말기로부터 동영상을 시청 중인 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나를 포함하는 실시간 시청자 정보를 수집할 수 있다. 이러한 수집부는 네트워크 통신을 수행하 기 위한 통신부로 마련되어 시청자 단말기와 각종 정보를 송수신할 수 있다. 저장부는 서버가 시청자 단말기와 송수신하는 각종 정보를 저장하기 위해 마련되는 것으로, 이 러한 저장부에는 인공지능 캐릭터 제공 방법을 수행하기 위한 소프트웨어(어플리케이션)이 저장될 수 있다. 그리고 저장부는 인공지능 캐릭터 제공 방법을 수행하면서 누적되는 데이터가 저장할 수 있다. 한편 제어부는 인공지능 캐릭터 제공 방법을 수행하는 전체 과정을 제어하기 위해 마련되며, 감정인식부 , 동영상 설명 생성부 및 콘텐츠 생성부를 포함할 수 있다. 감정인식부는 수집부로부터 전달받은 실시간 시청자 정보, 동영상의 영상 정보, 소리 정보 및 댓글 정보 중 적어도 하나를 미리 마련된 감정예측모델에 입력하여 시청자가 느낄 것으로 예측되는 예측 감정 및 상 기 시청자의 현재 감정을 인식할 수 있다. 여기서 감정예측모델은, 도 3에 도시된 바와 같이 하향식 주의집중 인공신경망 모델로 마련되어 멀티모달 (multi-modal)을 융합시킬 수 있다. 이를 위해 감정인식부는 영상 정보와 표정 정보와 같은 이미지와, 소리 정보 및 음성 정보와 같은 오디오, 댓글 정보 및 채팅 정보와 같은 텍스트와 같은 모달을 각각 인식할 수 있는 별도의 인식기를 각 모달별로 마련 하고, 각 인식기에서 학습 및 테스트가 수행되도록 할 수 있다. 이러한 별도의 인식기는 각 모달별로 감정을 인식하기 위해 합성곱(convolution) 신경망 모델 및 트랜스포머 (transformer) 기반의 모델을 사용할 수 있다. 구체적으로 감정인식부에 포함되는 별도의 인식기 중 실시간 시청자 정보의 표정 정보나 동영상의 영상 정 보로부터 감정을 인식하기 위한 인식기는 표정 정보 또는 영상 정보에 포함된 이미지(또는 프레임)에 대해 먼저 공간에 대한 합성곱을 수행하여 특징을 추출한 이후에 시간에 대한 합성곱을 수행함으로써 시간변화 특징을 추 출할 수 있다. 그리고 감정인식부에 포함되는 별도의 인식기 중 실시간 시청자 정보의 음성 정보 및 채팅 정보나 동영상 의 소리 정보 및 댓글 정보로부터 감정을 인식하기 위한 인식기는 트랜스포머 기반의 모델인 BERT(Bidirectional Encoder Representations from Transformers)나 GPT(Generative Pre-trained Transformer)을 이용할 수 있다. 그리고 감정인식부는 상술한 하향식 주의집중식 인공신경망 모델을 통해 각 인식기를 통합함으로써 통합된 인식 결과를 얻을 수 있게 된다. 이러한 하향식 주의집중 인공신경망 모델은 도 3과 같이 하향식 연결이 있으므로 전체 모델이 회귀성 신경망 (RNN, Recurrent Neural networks)이 되지만, 무한한 길이의 피드포워드 네트워크(feedforward network)로 풀 어서 오차역전파 방법으로 학습시킬 수 있다. 따라서 본 발명에서 하향식 주의집중 인공신경망 모델은, 예측 감정 및 현재 감정을 인식하면, 이를 회귀시켜 실시간 시청자 정보, 동영상의 영상 정보, 소리 정보 및 댓글 정보 중에서 집중해야 하는 정보를 판단할 수 있다. 그리고 하향식 주의집중 인공신경망 모델을 통해 감정인식부는 집중해야하는 정보에 기초하여 다음번 예측 감정 및 현재 감정을 인식할 수 있다. 구체적으로 본 발명의 감정예측모델은 하향식 주의집중 인공신경망 모델에 기반하므로 도 3에 도시된 바와 같이 소리 정보(Audio Input)로부터 추출된 소리 특징(Audio Feature)으로만 정보를 얻는 것이 아니라, 인식된 소리 (Audio Output)를 통해서 추가 정보(Attended Features)를 얻을 수 있다. 예를 들면, 감정예측모델이 동영상에 포함된 영상 정보를 입력받아 해당 영상 정보로부터 즐거운 감정에 가깝다 는 것은 인식한 경우, 이를 통해 소리 정보에서 웃음소리가 들리는지를 확인하는 방식일 수 있다. 따라서 입력 된 정보의 어떤 부분이 인식 결과에 기여하였는지를 파악할 수 있다. 또한 도 3에서는 설명의 편의를 위해 영상 정보(Video Input)와 소리 정보(Audio Input)의 멀티 모달 융합만 표 현되었으나, 실제로는 실시간 시청자 정보, 영상 정보, 소리 정보 및 댓글 정보에 대해 멀티 모달 융합이 수행 될 수 있다. 한편, 동영상을 시청하는 시청자들은 평균적으로 동영상의 제작자가 의도한 것과 비슷한 감정을 느끼게 되지만, 시청자마다 느끼는 감정의 정도는 차이가 있기 때문에 시청자마다 개인화된 감정을 예측할 수 있어야 한다. 이를 위해 본 발명의 감정인식부는 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나가 감정별 로 카테고리화된 히스토리 시청자 정보를 사전에 학습하여 마련된 시청자 임베딩 벡터를 저장부로부터 로 딩할 수 있다. 여기서 감정별로 카테고리화된 히스토리 시청자 정보는, 시청자가 특정 감정을 느낄 때의 시청자의 표정 정보, 음성 정보 및 채팅 정보가 감정에 대응되어 저장된 정보를 의미할 수 있다. 그리고 감정인식부는 로딩된 시청자 임베딩 벡터를 실시간 시청자 정보와 함께 감정예측모델에 입력하여 시청자의 현재 감정을 인식할 수 있다. 이를 위해 감정인식부는 시청자 임베딩 벡터를 확률 분포로 모델링하고 손실 함수에 정형화 (regularization) 손실을 추가할 수 있다. 그리고 감정인식부는 동영상으로부터 예측 감정을 인식하고 실시간 시청자 정보로부터 시청자의 현재 감정 을 인식하기 위해서, 예측 감정을 인식하기 위한 감정예측모델과 현재 감정을 인식하기 위한 감정예측모델을 별 개의 데이터로 따로 학습시키되 동일한 구조를 사용할 수 있다. 또한 감정인식부는 도 4와 같이 복수의 감정들을 일정 차원의 공간상에 맵핑한 공간 좌표를 미리 마련하고, 시청자의 현재 감정을 공간 좌표 상에 맵핑하여 현재 감정의 세기를 결정할 수 있다. 복수의 감정들은 행복(Happy), 화(Angry), 슬픔(Sad), 놀람(Surprise), 공포(Fear), 혐오(Disgusting) 및 중립 을 포함할 수 있다. 이러한 감정들은 서로 관련도가 높기 때문에 이를 VA(Valence(Pleasant-Unpleasant), Arousal(Activation-Deactivation))의 2차원의 공간 상에 맵핑할 수 있다. 이러한 공간 좌표는 2축의 VA에 한정되는 것은 아니며, 3축의 VAD(Valence, Arousal, Dominance) 공간 좌표일 수도 있다. 따라서 감정인식부는 이러한 2차원 좌표에 현재 감정을 맵핑함으로써 현재 감정의 세기를 결정할 수 있게 되고, 결정된 현재 감정의 세기를 콘텐츠 생성부로 전달할 수 있다. 한편 동영상 설명 생성부는 동영상의 상황을 텍스트로 설명하는 동영상 설명 정보를 생성할 수 있다. 동영상 속 상황을 설명하는 동영상 설명 정보를 생성하기 위해 동영상 설명 생성부는 클립(CLIP) 기반의 선행학습 모델을 활용하여 학습하는 비디오 캡셔닝(video captioning) 기술을 활용할 수 있다. 그리고 동영상 설명 생성부는 생성된 동영상 설명 정보를 콘텐츠 생성부로 전달할 수 있다. 한편, 콘텐츠 생성부는 도 6에 도시된 바와 같이 시청자의 아바타와 상대 캐릭터가 서로 감정 및 대화를 교류하여 상호작용하는 메타버스 캐릭터 콘텐츠를 생성할 수 있다. 도 6에서 메타버스 캐릭터 콘텐츠에 포함되는 상대 캐릭터가 디지털 반려동물로서 강아지로 제공되는 것으로 도 시되었으나, 이는 설명의 편의를 위한 예시적 사항으로 강아지가 아닌 다른 동물로 제공되는 것은 물론, 다양한형상(예를 들어, 시청자가 원하는 특정 인물, 특정 캐릭터 등)으로 마련될 수 있다. 이러한 콘텐츠 생성부는 메타버스 캐릭터 콘텐츠를 생성하기 위해 감정인식부에서 인식한 예측 감정 및 현재 감정에 기반하여 상대 캐릭터의 감정을 생성하고, 상대 캐릭터가 생성된 감정을 표현하도록 할 수 있다. 그리고 콘텐츠 생성부는, 감정인식부에서 인식된 현재 감정의 세기 및 감정별로 미리 설정된 역치에 따라 상대 캐릭터의 반응 여부를 결정할 수 있다. 이에 콘텐츠 생성부는, 시청자의 아바타와 상대 캐릭터 간의 상호 작용을 통해 역치를 조절하여 상대 캐릭 터의 감정 표현에 대한 민감도를 조절할 수도 있다. 다시 말해 콘텐츠 생성부는 상대 캐릭터가 시청자 아바타와의 상호작용을 통해 상대 캐릭터의 개성을 갖도 록 할 수 있다. 구체적으로 도 7의 (a)는 재미있는 동영상에 반응하지 않는 상황이고, 도 7의 (b)는 시청자 아바타의 지도(호루 라기)를 통해 행복 감정에 대한 민감도를 향상시키는 상황, 도 7의 (c)는 민감도가 향상되어 도 7의 (a)와 비슷 한 동영상을 시청했을 때 행복한 감정을 표현하는 상황, 그리고 도 7의 (d)는 시청자 아바타의 보상(간식)을 통 해 행복 감정에 비슷한 감정을 유지하는 상황을 예시적으로 도시한 도면이다. 이러한 지도와 보상의 과정을 통해 콘텐츠 생성부는 시청자 아바타와 상호작용하는 상대 캐릭터가 복수개 로 마련되는 경우, 복수개의 상대 캐릭터마다 서로 다른 성격을 가지도록 할 수 있다. 여기서 성격은 상대 캐릭 터의 감정에 따른 역치를 의미할 수 있다. 본 발명의 실시예에 따른 콘텐츠 생성부는 상대 캐릭터가 너무 민감하게 반응하는 경우에는 상대 캐릭터가 반응했을 때 시청자 아바타의 지도를 통해 역치를 높여 민감도가 낮아지도록 할 수 있고, 반대로 현재 동영상이 재밌는 장면임에도 불구하고 상대 캐릭터가 즐거워하지 않는 경우 지도를 통해 역치를 낮춰 민감도가 높아지도 록 할 수 있다. 이를 통해 콘텐츠 생성부는 상대 캐릭터가 시청자가 원하는 정도로 적절하게 감정을 표현하고 시청자와의 친밀감을 형성할 수 있는 콘텐츠를 제공할 수 있다. 그리고 콘텐츠 생성부는 동영상 설명 생성부에서 생성된 동영상 설명 정보 및 동영상의 댓글 정보에 기반하여 동영상의 맥락에 맞는 대화를 생성하고, 상대 캐릭터가 생성된 대화를 출력하도록 할 수 있다. 이를 위해 콘텐츠 생성부는 긴 동영상의 문맥을 활용하여 대화를 생성도록 감정인식부에서 설명한 3 개층의 계층 구조를 갖는 계층적 GPT(Hierarchical GPT) 모델을 이용할 수 있다. 이러한 계층적 GPT 모델은 동영상의 영상 정보 없이 텍스트 기반의 문맥을 입력으로 사용하게 되는데, 본 발명 의 실시예에 따른 동영상 설명 생성부는 동영상의 문맥에 맞는 대화 모델을 생성하기 위해 동영상의 댓글 정보를 활용할 수 있다. 그리고 콘텐츠 생성부는 영상 기반의 대화가 가능하도록 기존의 텍스트 기반의 대화 모델을 변형하고 조정 할 수 있다. 이를 위해 기존의 대화 모델을 파인 튜닝(Fine-tuning)함으로써 영상 기반의 대화 모델에 기초하여 학습할 수 있다. 이러한 영상을 입력으로 사용하는 모델은 감정예측모델과 유사하게 ResNet 등의 모델로 특징을 추출하여 사용할 수 있다. 또한 콘텐츠 생성부는, 대화가 출력될 때마다 상대 캐릭터의 감정과 대화를 각각 임베딩하여 감정 임베딩 벡터 및 대화 임베딩 벡터를 생성할 수 있다. 그리고 콘텐츠 생성부는 감정 임베딩 벡터 및 대화 임베딩 벡터에 기초하여 연속 임베딩 벡터를 생성하며, 연속 임베딩 벡터를 이용해 상대 캐릭터의 감정 및 대화를 동시 에 생성할 수 있다. 구체적으로 도 5b에 도시된 종래의 일반적인 트랜스포머(Transformer) 모델을 활용한 대화 또는 감정 생성의 경 우에 입력에 대한 결과는 토큰 단위(단어 또는 더 작은 단위)로 순차적으로 생성된다. 이때 출력되는 토큰이 생 성될 때 해당 토큰의 오른쪽에 위치한 토큰은 입력으로 사용되지 않는다. 즉, 일반적인 트랜스포머에서는 token1은 시작 토큰을 입력으로 하여 생성되고, 생성된 token1의 값과 기존에 있던 시작 토큰을 입력으로 사용하여 token2가 생성된다. 그리고 동일한 방식으로 시작 토큰 token1 토큰, token2 토큰을 입력으로 사용하여 마지막으로 token3이 생성될 수 있다. 다시 말해 도 5b와 같은 일반적인 트랜스포머 모델을 이용하는 경우 대화와 감정을 생성하기 위해서는 일반적으 로 감정 또는 대화 중 하나를 우선 생성하게 된다. 그리고 일반적인 트랜스포머 모델을 이용하는 경우 감정 우 선 또는 대화 우선 생성의 경우에는 감정 또는 대화 하나를 먼저 정한 후, 해당 감정에 대한 대화문 또는 대화 에 대한 감정을 생성해야 한다. 이에 의해 감정 또는 대화의 선후관계에 의해 문제가 발생할 수 있다. 하지만 본 발명에서의 콘텐츠 생성부는, 도 5a에 도시된 바와 같이 Hierarchical GPT 모델을 이용하여 Document/Dialogue generation block에서 문장 단위로 토큰 생성이 이루어진다. 따라서 본 발명의 콘텐츠 생성부는 문장 벡터를 대화 임베딩과 감정 임베딩을 연결하여 하나의 벡터로 구 성하여 대화와 감정 정보를 포함하고 있는 임베딩이 동시에 생성되도록 한 이후에 Sentence/Uteerance Block을 통해 대화와 감정이 토큰 단위로 생성되도록 할 수 있다. 이를 통해 콘텐츠 생성부는 시청자의 아바타와 상대 캐릭터 간의 매 대화마다 감정과 대화를 생성함에 있 어, 감정을 먼저 인식하고 난 다음에 대화를 생성하거나 대화를 먼저 생성하고 난 다음에 감정을 인식하는 방식 과 같이 선후관계에 의해 발생할 수 있는 문제점을 해결할 수 있다. 그리고 콘텐츠 생성부는 히스토리 시청자 정보 및 실시간 시청자 정보 중 적어도 하나에 기초하여 시청자 의 대화 스타일에 대응되는 스타일로 대화가 출력되도록 할 수 있다. 여기서 스타일은, 시청자의 억양, 말투, 자주 사용하는 단어 또는 문장, 말 버릇 등을 의미할 수 있다. 한편 도 8은 도 2의 메타버스 서버에서의 인공지능 캐릭터 제공 방법을 설명하기 위한 흐름도로써, 본 발 명의 일 실시예에 따른 인공지능 캐릭터 제공 방법은 도 1 및 2에 도시된 메타버스 서버를 포함하는 인공 지능 캐릭터 제공 시스템과 실질적으로 동일한 구성 상에서 진행되므로, 도 1 및 도 2의 메타버스 서버 를 포함하는 인공지능 캐릭터 제공 시스템과 동일한 구성요소에 대해 동일한 도면 부호를 부여하고, 반복되는 설명은 생략하기로 한다. 본 발명의 인공지능 캐릭터 제공 방법은, 실시간 시청자 정보를 수집하는 단계(S110), 시청자의 예측 감정 및 현재 감정을 인식하는 단계(S130) 및 콘텐츠를 생성하는 단계(S150)를 포함할 수 있다. 먼저 실시간 시청자 정보를 수집하는 단계(S110)에서는 서버가 시청자 단말기로부터 동영상을 시청 중인 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나를 포함하는 실시간 시청자 정보를 수집할 수 있다. 시청자의 예측 감정 및 현재 감정을 인식하는 단계(S130)는 실시간 시청자 정보를 수집하는 단계(S110) 이후에 수행될 수 있다. 이러한 시청자의 예측 감정 및 현재 감정을 인식하는 단계(S130)에서는, 서버가 실시간 시청자 정보를 수 집하는 단계(S110)에서 수집한 실시간 시청자 정보와, 시청자가 시청 중인 동영상의 영상 정보, 소리 정보 및 댓글 정보 중 적어도 하나를 미리 마련된 감정예측모델에 입력하여 시청자가 느낄 것으로 예측되는 예측 감정 및 시청자의 현재 감정을 인식할 수 있다. 여기서 감정예측모델은, 하향식 주의집중 인공신경망 모델로 마련될 수 있다. 이러한 하향식 주의집중 인공신경 망 모델은 예측 감정 및 현재 감정을 인식하면, 이를 회귀시켜 실시간 시청자 정보, 동영상의 영상 정보, 소리 정보 및 댓글 정보 중에서 집중해야 하는 정보를 판단할 수 있다. 그리고 집중해야하는 정보에 기초하여 다음번 예측 감정 및 현재 감정을 인식할 수 있다. 그리고 시청자의 예측 감정 및 현재 감정을 인식하는 단계(S130)에서는 서버가 시청자의 음성 정보, 채팅 정보 및 표정 정보 중 적어도 하나가 감정별로 카테고리화된 히스토리 시청자 정보를 사전에 학습하여 마련된 시청자 임베딩 벡터를 로딩할 수 있다. 그리고 시청자의 예측 감정 및 현재 감정을 인식하는 단계(S130)에서는 로딩된 시청자 임베딩 벡터를 실시간 시 청자 정보와 함께 감정예측모델에 입력함으로써 시청자의 현재 감정을 인식할 수 있다. 이를 통해 서버는 시청자의 감정을 시청자별로 개인화시킬 수 있게 되어 현재 감정의 인식에 대한 정확도를 높일 수 있다. 그리고 본 발명의 실시예에 따른 인공지능 캐릭터 제공 방법은, 시청자의 예측 감정 및 현재 감정을 인식하는 단계(S130)와 동시에 또는 시청자의 예측 감정 및 현재 감정을 인식하는 단계(S130)가 수행된 이후에 서버가 동영상의 상황을 텍스트로 설명하는 동영상 설명 정보를 생성하는 단계를 더 포함할 수 있다. 한편 콘텐츠를 생성하는 단계(S150)에서는 서버가 시청자의 아바타와 상대 캐릭터가 서로 감정 및 대화를 교류하여 상호작용하는 메타버스 캐릭터 콘텐츠를 생성할 수 있다. 그리고 콘텐츠를 생성하는 단계(S150)에서는, 서버가 시청자의 예측 감정 및 현재 감정을 인식하는 단계 (S130)에서 인식한 예측 감정 및 현재 감정에 기반하여 상대 캐릭터의 감정을 생성할 수 있다. 또한 콘텐츠를 생성하는 단계(S150)에서는 서버가 상대 캐릭터가 생성된 감정을 표현하도록 할 수 있다. 그리고 콘텐츠를 생성하는 단계(S150)에서는 서버가 동영상 설명 정보를 생성하는 단계에서 생성된 동영상 설명 정보 및 실시간 시청자 정보에 포함되는 댓글 정보에 기반하여 동영상의 맥락에 맞는 대화를 생성하고, 상 대 캐릭터가 생성된 대화를 출력하도록 할 수 있다. 또한 콘텐츠를 생성하는 단계(S150)에서는 서버가 히스토리 시청자 정보 및 실시간 시청자 정보 중 적어도 하나에 기초하여 생성된 대화가 출력될 때 시청자의 대화 스타일에 대응되는 스타일로 대화가 출력되도록 할 수 있다. 이러한 콘텐츠를 생성하는 단계(S150)에서는 서버가 시청자의 예측 감정 및 현재 감정을 인식하는 단계 (S130)에서 결정된 현재 감정의 세기 및 감정별로 미리 설정된 역치에 따라 상대 캐릭터의 반응 여부를 결정할 수 있다. 즉 현재 감정의 세기가 설정된 상대 캐릭터의 역치보다 작으면 상대 캐릭터가 반응하지 않도록 할 수 있다. 또한 콘텐츠를 생성하는 단계(S150)에서는 서버가 시청자 아바타와 상대 캐릭터 간의 상호 작용을 통해 역 치를 조절하여 상대 캐릭터의 감정 표현에 대한 민감도를 조절할 수 있다. 이와 같은 본 발명의 인공지능 캐릭터 제공 방법은 다양한 컴퓨터 구성요소를 통하여 수행될 수 있는 프로그램 명령어의 형태로 구현되어 컴퓨터 판독 가능한 기록 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능한 기록 매 체는 프로그램 명령어, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터 판독 가능한 기록 매체에 기록되는 프로그램 명령어는 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능한 기록 매체의 예에는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD- ROM, DVD 와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 ROM, RAM, 플래시 메모리 등과 같은 프로그램 명령어를 저장하고 수행하도록 특별히 구성된 하드웨 어 장치가 포함된다. 프로그램 명령어의 예에는, 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사 용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함된다. 상기 하드웨어 장치는 본 발명에 따른 처 리를 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 이상에서는 본 발명의 다양한 실시예에 대하여 도시하고 설명하였지만, 본 발명은 상술한 특정의 실시예에 한정"}
{"patent_id": "10-2022-0113987", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 발명이 속하는 기술분야에서 통상 의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사 상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2022-0113987", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 인공지능 캐릭터 제공 시스템의 구성을 설명하기 위한 도면, 도 2는 도 1의 메타버스 서버의 구성을 설명하기 위한 도면, 도 3 및 도 4는 본 발명의 실시예에 따른 감정인식부를 설명하기 위한 도면, 도 5는 본 발명의 콘텐츠 생성부를 설명하기 위한 도면, 도 6은 본 발명의 콘텐츠 생성부에서 생성되는 메타버스 캐릭터 콘텐츠를 설명하기 위한 도면, 도 7은 본 발명의 콘텐츠 생성부에서 상대 캐릭터를 개인화하는 과정을 설명하기 위한 도면, 그리고, 도 8은 도 2의 메타버스 서버에서의 인공지능 캐릭터 제공 방법을 설명하기 위한 흐름도이다."}
