{"patent_id": "10-2022-0156553", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0074058", "출원번호": "10-2022-0156553", "발명의 명칭": "인공신경망을 적용한 최적 경로 탐색 방법 및 장치", "출원인": "김태호", "발명자": "김태호"}}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "각 단계의 적어도 일부가 프로세서에 의해 수행되는, 경로 탐색 알고리즘과 인공신경망의 결합을 통해 물류 관리를 위한 최적 경로 탐색을 수행하는 인공신경망을 적용한 최적 경로 탐색 방법으로서,복수의 경로 탐색 알고리즘 중 선택된 경로 탐색 알고리즘을 기반으로, 이동을 결정하고자 하는 물류 상태를 시작 노드로 설정하여, 상기 시작 노드에서 그래프를 확장하며 기 설정된 횟수만큼 경로를 탐색하는 단계;상기 시작 노드에 연계된 각 자식(child) 노드 그룹에 각각 속해 있는 기 선택되어 더 이상 확장 불가한 노드수에 기반하여, 상기 시작 노드에서의 각 엣지(edge)에 대한 실행 확률을 추론하는 단계; 및상기 추론한 실행 확률에 기반하여 엣지를 결정하고 다음 물류 상태를 선택해가며 최적 경로를 탐색하는 단계를포함하는,인공신경망을 적용한 최적 경로 탐색 방법."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 기 설정된 횟수만큼 경로를 탐색하는 단계는,상기 시작 노드를 시작으로 확장 가능한 개방 노드 리스트에서 가장 작은 평가함수 값을 가진 개방 노드를 선택하는 단계;상기 선택된 개방 노드의 자식 노드들을 생성하는 확장 과정을 통해 상기 시작 노드와 연결된 상기 시작 노드의자식 노드를 분기점으로 상기 시작 노드의 각 자식 노드 그룹을 구분하는 단계; 및상기 각 자식 노드 그룹에 포함되어 있는 기 선택되어 더 이상 확장 불가한 폐쇄(closed) 노드들의 총 개수를산출하는 단계를 포함하는,인공신경망을 적용한 최적 경로 탐색 방법."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 시작 노드의 각 자식 노드 그룹을 구분하는 단계는,상기 시작 노드의 자식 노드 확장 시, 상기 자식 노드로 확장 가능한 개방 노드에 대한 개방 노드 리스트와, 기선택되어 더 이상 확장 불가한 폐쇄 노드에 대한 폐쇄 노드 리스트와, 상기 자식 노드의 확장 후 상기 개방 노드에 대한 상기 개방 노드 리스트를 업데이트하는 단계를 포함하고,상기 업데이트하는 단계는,상기 선택된 개방 노드의 자식 노드들 중 적어도 하나의 평가함수 값을 계산하여 상기 개방 노드 리스트에 추가하고, 기 선택된 개방 노드를 상기 개방 노드 리스트에서 삭제함과 동시에 상기 폐쇄 노드 리스트에 추가하는단계를 포함하는,인공신경망을 적용한 최적 경로 탐색 방법."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 업데이트하는 단계는,상기 선택된 노드의 자식 노드를 확장했을 때, 새롭게 생성될 자식 노드의 상태와, 상기 폐쇄 노드 리스트에 포공개특허 10-2023-0074058-3-함된 폐쇄 노드의 상태 또는 상기 개방 노드 리스트에 포함된 개방 노드의 상태가 중복되는 경우, 상기 자식 노드를 생성하지 않는 단계를 포함하는,인공신경망을 적용한 최적 경로 탐색 방법."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 2 항에 있어서,상기 평가함수 값은, 상기 시작 노드에서 0으로 시작하여 확장된 현재 자식 노드까지의 거리 값과, 현재 노드에서의 가중치 값에 기반하여 산출되며,상기 가중치 값은, 현재 노드에서 각 물류들이 갖는 출고 순서인 상태를 입력으로 한 인공신경망의 출력으로 계산되는,인공신경망을 적용한 최적 경로 탐색 방법."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1 항에 있어서,상기 기 설정된 횟수만큼 경로를 탐색하는 단계는,상기 시작 노드를 시작으로 선택 가능한 노드 정보를 포함하는 큐 리스트에서 가장 작은 거리 값을 가진 방문하지 않은 노드를 선택하는 단계;상기 선택된 방문하지 않은 노드의 자식 노드들을 생성하는 확장 과정을 통해 상기 시작 노드와 연결된 상기 시작 노드의 자식 노드를 분기점으로 상기 시작 노드의 각 자식 노드 그룹을 구분하고 상기 각각의 자식 노드에가중치를 저장하는 단계; 및상기 각 자식 노드 그룹에 포함되어 있는 기 선택되어 더 이상 확장 불가한 방문(visited) 노드들의 최단 거리테이블을 도출하는 단계를 포함하는,인공신경망을 적용한 최적 경로 탐색 방법."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 시작 노드의 각 자식 노드 그룹을 구분하고 상기 각각의 자식 노드에 가중치를 저장하는 단계는,상기 시작 노드의 인접 노드 중 적어도 하나의 선택된 방문하지 않은 노드를 거쳐 상기 선택된 방문하지 않은노드의 자식 노드로 넘어가는 가중치를 산출하여 최단 거리 테이블을 업데이트하는 단계를 포함하고,상기 업데이트하는 단계는,상기 선택된 방문하지 않은 노드로부터 적어도 하나의 상기 선택된 방문하지 않은 노드의 자식 노드까지의 거리와, 상기 시작 노드와 상기 선택된 방문하지 않은 노드까지의 거리의 합으로, 상기 시작 노드로부터 상기 자식노드까지의 거리를 산출하여, 기 저장된 상기 자식 노드의 최단 거리와의 비교를 통해 적은 값으로 업데이트하는 단계를 포함하는,인공신경망을 적용한 최적 경로 탐색 방법."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서,상기 업데이트하는 단계는,상기 선택된 노드의 자식 노드를 확장했을 때, 새롭게 생성될 자식 노드의 상태와, 이동 가능한 노드들로 구성된 노드 리스트에 포함된 방문하지 않은 노드의 상태가 중복되는 경우, 상기 자식 노드를 생성하지 않는 단계를포함하는,인공신경망을 적용한 최적 경로 탐색 방법.공개특허 10-2023-0074058-4-청구항 9 제 1 항에 있어서,상기 기 설정된 횟수만큼 경로를 탐색하는 단계는,상기 시작 노드를 시작으로 상기 시작 노드의 자식 노드들을 생성하는 확장 과정을 통해 상기 시작 노드와 연결된 상기 시작 노드의 자식 노드를 분기점으로 상기 시작 노드의 각 자식 노드 그룹을 구분하는 단계;상기 각 자식 노드 그룹에서 가장 큰 다항식 상위 신뢰 트리(polynomial upper confidence trees, PUCT) 값을가진 자식 노드를 선택하여 trajectory에 추가하는 단계; 및상기 trajectory의 노드들의 총 개수를 산출하는 단계를 포함하고,상기 다항식 상위 신뢰 트리(PUCT) 값은, 상기 각 자식 노드 그룹의 각 자식 노드의 평균 정책 값, 상기 각 자식 노드를 선택할 사전 확률 및 상기 각 자식 노드에 방문한 횟수를 기반으로 산출된 것인, 인공신경망을 적용한 최적 경로 탐색 방법."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서,상기 시작 노드의 각 자식 노드 그룹을 구분하는 단계는,상기 시작 노드의 자식 노드 확장 시, 상기 시작 노드의 상태를 기반으로 한 가중치와 정책을 산출하여 각 자식노드에 저장하는 단계; 및상기 자식 노드의 확장 후 상기 trajectory의 속성을 업데이트하는 단계를 포함하는,인공신경망을 적용한 최적 경로 탐색 방법."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "경로 탐색 알고리즘과 인공신경망의 결합을 통해 물류 관리를 위한 최적 경로 탐색을 수행하는 인공신경망을 적용한 최적 경로 탐색 장치로서,메모리; 및상기 메모리와 연결되고, 상기 메모리에 포함된 컴퓨터 판독가능한 명령들을 실행하도록 구성된 적어도 하나의프로세서를 포함하고,상기 적어도 하나의 프로세서는,복수의 경로 탐색 알고리즘 중 선택된 경로 탐색 알고리즘을 기반으로, 이동을 결정하고자 하는 물류 상태를 시작 노드로 설정하여, 상기 시작 노드에서 그래프를 확장하며 기 설정된 횟수만큼 경로를 탐색하는 동작,상기 시작 노드에 연계된 각 자식(child) 노드 그룹에 각각 속해 있는 기 선택되어 더 이상 확장 불가한 노드수에 기반하여, 상기 시작 노드에서의 각 엣지(edge) 에 대한 실행 확률을 추론하는 동작, 및상기 추론한 실행 확률에 기반하여 엣지를 결정하고 다음 물류 상태를 선택해가며 최적 경로를 탐색하는 동작을수행하도록 설정된,인공신경망을 적용한 최적 경로 탐색 장치."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서,상기 기 설정된 횟수만큼 경로를 탐색하는 동작은,상기 시작 노드를 시작으로 확장 가능한 개방 노드 리스트에서 가장 작은 평가함수 값을 가진 개방 노드를 선택하는 동작,상기 선택된 개방 노드의 자식 노드들을 생성하는 확장 과정을 통해 상기 시작 노드와 연결된 상기 시작 노드의공개특허 10-2023-0074058-5-자식 노드를 분기점으로 상기 시작 노드의 각 자식 노드 그룹을 구분하는 동작, 및상기 각 자식 노드 그룹에 포함되어 있는 기 선택되어 더 이상 확장 불가한 폐쇄(closed) 노드들의 총 개수를산출하는 동작을 수행하도록 설정된,인공신경망을 적용한 최적 경로 탐색 장치."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12 항에 있어서,상기 시작 노드의 각 자식 노드 그룹을 구분하는 동작은,상기 시작 노드의 자식 노드 확장 시, 상기 자식 노드로 확장 가능한 개방 노드에 대한 개방 노드 리스트와, 기선택되어 더 이상 확장 불가한 폐쇄 노드에 대한 폐쇄 노드 리스트와, 상기 자식 노드의 확장 후 상기 개방 노드에 대한 상기 개방 노드 리스트를 업데이트하는 동작을 수행하도록 설정되고,상기 업데이트하는 동작은,상기 선택된 개방 노드의 자식 노드들 중 적어도 하나의 평가함수 값을 계산하여 상기 개방 노드 리스트에 추가하고, 기 선택된 개방 노드를 상기 개방 노드 리스트에서 삭제함과 동시에 상기 폐쇄 노드 리스트에 추가하는동작을 수행하도록 설정된,인공신경망을 적용한 최적 경로 탐색 장치."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13 항에 있어서,상기 업데이트하는 동작은,상기 선택된 노드의 자식 노드를 확장했을 때, 새롭게 생성될 자식 노드의 상태와, 상기 폐쇄 노드 리스트에 포함된 폐쇄 노드의 상태 또는 상기 개방 노드 리스트에 포함된 개방 노드의 상태가 중복되는 경우, 상기 자식 노드를 생성하지 않는 동작을 수행하도록 설정된,인공신경망을 적용한 최적 경로 탐색 장치."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 12 항에 있어서,상기 평가함수 값은, 상기 시작 노드에서 0으로 시작하여 확장된 현재 자식 노드까지의 거리 값과, 현재 노드에서의 가중치 값에 기반하여 산출되며,상기 가중치 값은, 현재 노드에서 각 물류들이 갖는 출고 순서인 상태를 입력으로 한 인공신경망의 출력으로 계산되는, 인공신경망을 적용한 최적 경로 탐색 장치."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 11 항에 있어서,상기 기 설정된 횟수만큼 경로를 탐색하는 동작은,상기 시작 노드를 시작으로 선택 가능한 노드 정보를 포함하는 큐 리스트에서 가장 작은 거리 값을 가진 방문하지 않은 노드를 선택하는 동작,상기 선택된 방문하지 않은 노드의 자식 노드들을 생성하는 확장 과정을 통해 상기 시작 노드와 연결된 상기 시작 노드의 자식 노드를 분기점으로 상기 시작 노드의 각 자식 노드 그룹을 구분하고 상기 각각의 자식 노드에가중치를 저장하는 동작, 및상기 각 자식 노드 그룹에 포함되어 있는 기 선택되어 더 이상 확장 불가한 방문(visited) 노드들의 최단 거리테이블을 도출하는 동작을 수행하도록 설정된,공개특허 10-2023-0074058-6-인공신경망을 적용한 최적 경로 탐색 장치."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16 항에 있어서,상기 시작 노드의 각 자식 노드 그룹을 구분하고 상기 각각의 자식 노드에 가중치를 저장하는 동작은,상기 시작 노드의 인접 노드 중 적어도 하나의 선택된 방문하지 않은 노드를 거쳐 상기 선택된 방문하지 않은노드의 자식 노드로 넘어가는 가중치를 산출하여 최단 거리 테이블을 업데이트하는 동작을 수행하도록설정되고,상기 업데이트하는 동작은,상기 선택된 방문하지 않은 노드로부터 적어도 하나의 상기 선택된 방문하지 않은 노드의 자식 노드까지의 거리와, 상기 시작 노드와 상기 선택된 방문하지 않은 노드까지의 거리의 합으로, 상기 시작 노드로부터 상기 자식노드까지의 거리를 산출하여, 기 저장된 상기 자식 노드의 최단 거리와의 비교를 통해 적은 값으로 업데이트하는 동작을 수행하도록 설정된,인공신경망을 적용한 최적 경로 탐색 장치."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 17 항에 있어서,상기 업데이트하는 동작은,상기 선택된 노드의 자식 노드를 확장했을 때, 새롭게 생성될 자식 노드의 상태와, 이동 가능한 노드들로 구성된 노드 리스트에 포함된 방문하지 않은 노드의 상태가 중복되는 경우, 상기 자식 노드를 생성하지 않는 동작을수행하도록 설정된,인공신경망을 적용한 최적 경로 탐색 장치."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 11 항에 있어서,상기 기 설정된 횟수만큼 경로를 탐색하는 동작은,상기 시작 노드를 시작으로 상기 시작 노드의 자식 노드들을 생성하는 확장 과정을 통해 상기 시작 노드와 연결된 상기 시작 노드의 자식 노드를 분기점으로 상기 시작 노드의 각 자식 노드 그룹을 구분하는 동작,상기 각 자식 노드 그룹에서 가장 큰 다항식 상위 신뢰 트리(polynomial upper confidence trees, PUCT) 값을가진 자식 노드를 선택하여 trajectory에 추가하는 동작, 및상기 trajectory의 노드들의 총 개수를 산출하는 동작을 포함하도록 설정되며,상기 다항식 상위 신뢰 트리(PUCT) 값은, 상기 각 자식 노드 그룹의 각 자식 노드의 평균 정책 값, 상기 각 자식 노드를 선택할 사전 확률 및 상기 각 자식 노드에 방문한 횟수를 기반으로 산출된 것인, 인공신경망을 적용한 최적 경로 탐색 장치."}
{"patent_id": "10-2022-0156553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 19 항에 있어서,상기 시작 노드의 각 자식 노드 그룹을 구분하는 동작은,상기 시작 노드의 자식 노드 확장 시, 상기 시작 노드의 상태를 기반으로 한 가중치와 정책을 산출하여 각 자식노드에 저장하는 동작, 및상기 자식 노드의 확장 후 상기 trajectory의 속성을 업데이트하는 동작을 수행하도록 설정된,공개특허 10-2023-0074058-7-인공신경망을 적용한 최적 경로 탐색 장치."}
{"patent_id": "10-2022-0156553", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공신경망을 적용한 최적 경로 탐색 방법 및 장치가 개시된다. 본 개시의 일 실시 예에 따른 인공신경망을 적용 한 최적 경로 탐색 방법은, 복수의 경로 탐색 알고리즘 중 선택된 경로 탐색 알고리즘을 기반으로, 이동을 결정 하고자 하는 물류 상태를 시작 노드로 설정하여, 시작 노드에서 그래프를 확장하며 기 설정된 횟수만큼 경로를 탐색하는 단계와, 시작 노드에 연계된 각 자식(child) 노드 그룹에 각각 속해 있는 기 선택되어 더 이상 확장 불 가한 노드 수에 기반하여, 시작 노드에서의 각 엣지(edge) 에 대한 실행 확률을 추론하는 단계와, 추론한 실행 확률에 기반하여 엣지를 결정하고 다음 물류 상태를 선택해가며 최적 경로를 탐색하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0156553", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 다양한 물류 관리 태스크에 있어, 최적화를 위한 경로 탐색 알고리즘과 인공신경망의 결합 방안을 제 공하는 인공신경망을 적용한 최적 경로 탐색 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2022-0156553", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일반적으로, 그래프는 노드(node)와 엣지(edge)로 이루어진 데이터의 형태로, 최단 경로를 구하거나 통신망, 소 셜 네트워크 등의 여러 상황을 모델링 할 수 있어 많은 분야에 사용되고 있다. 예를 들어, 컨테이너 선별(pre-marshalling)은 현재 컨테이너가 쌓여져 있는 상태를 노드로 하여, 각 상태 간의 관계가 그래프로 표현될 수 있다. 한편, 인공신경망(Artificial Neural Network)은 기계 학습(machine learning)의 세부 방법론 중 하나로, 신경 세포인 뉴런(neuron)이 여러 개 연결된 망의 형태이며, 네트워크 구조에서 각각의 구성(노드)들 간의 데이터 전 송을 통해 결과를 도출하고자 하는 것이다. 즉, 인공신경망은 시냅스의 결합으로 네트워크를 형성한 인공 뉴런(노드)이 학습을 통해 시냅스의 결합 세기를 변화시켜, 문제 해결 능력을 가지는 모델 전반을 가리킨다. 따라서, 그래프 구조의 어떠한 데이터는 인공신경망을 기반으로, 상기 데이터에 대한 태스크의 문제 해결이 가 능할 수 있다. 예를 들어, 상술한 컨테이너 선별과 같은 물류 관리 태스크가 인공신경망을 기반으로 수행될 수 있는 것이다. 즉, 컨테이너 선별과 같은 물류 관리 태스크를 효율적으로 해결하기 위해서는, 어떠한 인공신경망을 어떻게 적 용하여 최적화 할 수 있을 지 연구할 필요가 있다. 이에, 최적화를 위하여, 그래프와 강화학습의 결합을 통해, 그래프로 표현 가능한 다양한 물류 관리 태스크에 있어, 최적 경로(최적 결정) 탐색이 가능하도록 할 수 있다. 그래프에서 각 상태는 노드가 되며, 액션은 엣지로 치환 가능하여, 각 노드(상태)에서 여러 엣지(행동)를 통해 목표 노드까지 도달할 수 있다. 여기서, 강화학습(Reinforcement Learning)은 머신러닝(지도학습, 비지도학습, 강화학습)의 한 종류로, 에이전 트(Agent)가 환경(Environment)에 대한 정보를 모르는 상태에서 상호작용을 하며 학습해가는 과정을 의미한다. 특히, 강화학습 시나리오에서 해를 정의하기 위한 수학적 프레임워크인 마르코프 결정 과정(Markov Decision Process, MDP)은 일종의 확률과정으로, 이러한 과정을 관찰하며 매 시간 경과에 따라 확률과정에 영향을 미치는 행동을 선택할 수 있는 의사결정자를 고려한 확률과정이다. 의사결정자는 선택과 점유상태에 따라 일련의 보상 을 받는다. 즉, 환경에 행동을 가함으로써 미래의 상태와 보상을 바꿀 수 있다. MDP는 <유한한 상태의 집합, 유한한 행동의 집합, 상태 천이 행렬(State Transition Matrix), 보상함수, 감소율>인 튜플(tuple)이라고 할 수 있다. 즉, MDP는 시스템 혹은 환경이 가질 수 있는 상태(state)와, 시스템 이 상태 S에 있을 때 의사결정자가 선택할 수 있는 가능한 행동(action)과, 상태 S에서 a 행동을 선택하여 의사 결정자가 받을 수 있는 보상(reward) 등을 구성요소로 포함할 수 있다. 이러한 강화학습을 기반으로 최적해(optimal solution)에 가까운 해를 도출함으로써, 적은 수의 행동으로도 목 표 상태(terminal state)에 도달하도록 할 수 있다. 각 상태에서 행동을 선택하여 다음 상태 생성을 반복할 수 있으며, 즉 다음 상태가 목표 상태가 될 때까지 반복 시행하여 최종적으로 목표 상태에 도달하도록 할 수 있다. 이때, 행동을 선택하기 위해 경로 탐색을 이용하여 정책을 계산하게 된다. 즉, 경로 탐색 알고리즘을 이용해 탐색을 하며 정책을 생성하고, 생성한 정책으로 행동 을 선택하여 다음 상태를 생성할 수 있다. 그러나 일반적인 단순한 환경에서는 그래프의 구조를 미리 파악하여 경로 탐색을 통해 최적해를 찾는 것이 가능 하지만, 환경이 복잡하면 노드의 개수가 많아져 그래프의 구조를 파악하는데 어려움이 있으며, 그래프 구조 파 악의 시간 소요가 커 비효율적이다. 또한, 예를 들어, 컨테이너 선별의 경우, 환경이 시시각각 바뀌어 각기 다 른 그래프의 구조를 가지기 때문에 모든 환경에 따른 그래프의 구조를 파악하는 것에도 어려움이 있을 수 있다. 따라서, 복잡한 환경에서 적용 가능한 최적 경로 탐색 방법이 필요하다. 또한 최적 경로 탐색 방법은, 상술한 컨테이너 선별뿐만 아니라, 다양한 의사결정 태스크에 적용될 수 있으므로, 해당 태스크에 맞는 경로 탐색 알고리즘이 적용될 필요가 있다."}
{"patent_id": "10-2022-0156553", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "전술한 배경기술은 발명자가 본 발명의 도출을 위해 보유하고 있었거나, 본 발명의 도출 과정에서 습득한 기술 정보로서, 반드시 본 발명의 출원 전에 일반 공중에게 공개된 공지기술이라 할 수는 없다. 선행기술문헌 비특허문헌 (비특허문헌 0001) 선행기술 1: '컨테이너 터미널에서 베이 내 컨테이너의 최적 재정돈을 위한 A*알고리 즘'(Journal of the Korean Institute of Industrial Engineers, Vol. 38, No 2, pp. 157-172, June 2012.)"}
{"patent_id": "10-2022-0156553", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 실시 예의 일 과제는, 그래프 구조로 나타낼 수 있는 다양한 물류 관리 태스크에 있어, 인공신경망 기반 학습 모델과 경로 탐색 알고리즘의 결합을 통해 태스크에 적절한 최적 경로를 도출하여, 다음 상태를 선택 하는 실행 확률을 생성하고자 하는 것이다. 본 개시의 실시 예의 일 과제는, 이동하고자 하는 시작 노드만으로 시작하여 그래프를 그리는 과정과 경로 탐색 알고리즘을 통한 탐색을 동시에 진행하여 생성한 실행 확률을 기반으로, 각 노드에서 행동을 선택해 다음 노드 를 생성하는 과정을 반복 수행하여, 복잡한 환경으로 인해 그래프의 구조가 파악되지 않더라도 최적 경로 탐색 이 가능하도록 하는데 있다. 본 개시의 실시 예의 일 과제는, 의사결정(선별, 이동, 보관 등)에 적합한 데이터 생성 과정에서 정방향 및 역 방향으로의 에피소드 데이터 생성을 수행하여, 환경이 복잡해질수록 의사결정 완료 상태의 비율이 작아져 목표 상태에 도달 할 확률이 작아짐에 따라 학습 초기 보상을 부여 받는 빈도가 적은 문제(sparse reward)가 발생하 는 문제를 해결하고자 하는 것이다. 본 개시의 실시 예의 일 과제는, 다양한 경로 탐색 방법이 사용 가능하도록 하고, 경로 탐색 방법을 적절하게 변형하여 사용 가능하도록 하는데 있다. 본 개시의 실시예의 목적은 이상에서 언급한 과제에 한정되지 않으며, 언급되지 않은 본 발명의 다른 목적 및 장점들은 하기의 설명에 의해서 이해될 수 있고, 본 발명의 실시 예에 의해 보다 분명하게 이해될 것이다. 또한, 본 발명의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 알 수 있을 것이다."}
{"patent_id": "10-2022-0156553", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 인공신경망을 적용한 최적 경로 탐색 방법은, 복수의 경로 탐색 알고리즘 중 선택 된 경로 탐색 알고리즘을 기반으로, 이동을 결정하고자 하는 물류 상태를 시작 노드로 설정하여, 시작 노드에서 그래프를 확장하며 기 설정된 횟수만큼 경로를 탐색하는 단계와, 시작 노드에 연계된 각 자식(child) 노드 그룹 에 각각 속해 있는 기 선택되어 더 이상 확장 불가한 노드 수에 기반하여, 시작 노드에서의 각 엣지(edge) 에 대한 실행 확률을 추론하는 단계와, 추론한 실행 확률에 기반하여 엣지를 결정하고 다음 물류 상태를 선택해가 며 최적 경로를 탐색하는 단계를 포함할 수 있다. 이 외에도, 본 발명의 구현하기 위한 다른 방법, 다른 시스템 및 상기 방법을 실행하기 위한 컴퓨터 프로그램이 저장된 컴퓨터로 판독 가능한 기록매체가 더 제공될 수 있다. 전술한 것 외의 다른 측면, 특징, 이점이 이하의 도면, 특허청구범위 및 발명의 상세한 설명으로부터 명확해질 것이다."}
{"patent_id": "10-2022-0156553", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 실시 예에 의하면, 그래프 구조로 나타낼 수 있는 다양한 물류 관리 태스크에 있어, 인공신경망 기반 학습 모델과 경로 탐색 알고리즘의 결합을 통해 태스크에 적절한 최적 경로를 도출하여, 각 노드에서의 실행 확 률을 생성함으로써, 복잡한 환경의 물류 관리의 최적화가 가능하도록 할 수 있다. 또한, 이동하고자 하는 물류의 현재 상태를 속성으로 하는 특정 노드인시작 노드만으로 시작하여 그래프를 그리 는 과정과 경로 탐색 알고리즘을 통한 탐색을 동시에 진행하여 생성한 실행 확률을 기반으로, 각 노드에서 행동 을 선택해 다음 노드를 생성하는 과정을 반복 수행함으로써, 복잡한 환경으로 인해 그래프의 구조가 파악되지 않더라도 최적 경로 탐색이 가능하도록 할 수 있다. 또한, 의사결정에 적합한 데이터 생성 과정에서 정방향 및 역방향으로의 에피소드 데이터 생성을 수행함으로써, 환경이 복잡해질수록 의사결정 완료 상태의 비율이 작아져 목표 상태에 도달 할 확률이 작아짐에 따라 학습 초 기 보상을 부여 받는 빈도가 적은 문제(sparse reward)가 발생하는 문제를 해결할 수 있다. 또한, 다양한 경로 탐색 방법이 사용 가능하도록 하고, 경로 탐색 방법을 적절하게 변형하여 사용 가능하도록 함으로써, 다양하고 복잡한 환경에 있어 보다 정확하고 최적화된 경로가 탐색되도록 할 수 있다. 본 개시의 효과는 이상에서 언급된 것들에 한정되지 않으며, 언급되지 아니한 다른 효과들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0156553", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 설명되는 실시 예들 을 참조하면 명확해질 것이다. 그러나 본 개시는 아래에서 제시되는 실시 예들로 한정되는 것이 아니라, 서로 다른 다양한 형태로 구현될 수 있고, 본 개시의 사상 및 기술 범위에 포함되는 모든 변환, 균등물 내지 대체물을 포함하는 것으로 이해되어야"}
{"patent_id": "10-2022-0156553", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한다. 아래에 제시되는 실시 예들은 본 개시가 완전하도록 하며, 본 개시가 속하는 기술분야에서 통상의 지식을 가진 자에게 개시의 범주를 완전하게 알려주기 위해 제공되는 것이다. 본 개시를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 본 출원에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 개시를 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조 합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 구성요소들은 상기 용어들에 의해 한정되어서 는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 이하, 본 개시에 따른 실시 예들을 첨부된 도면을 참조하여 상세히 설명하기로 하며, 첨부 도면을 참조하여 설 명함에 있어, 동일하거나 대응하는 구성요소는 동일한 도면번호를 부여하고 이에 대한 중복되는 설명은 생략하 기로 한다. 도 1은 일 실시 예에 따른 최적 경로 탐색 시스템 환경을 개략적으로 나타낸 도면이고, 도 2는 일 실시 예에 따 른 최적 경로 탐색 시스템을 설명하기 위해 개략적으로 나타낸 도면이다. 도 1에 도시된 바와 같이, 최적 경로 탐색 시스템은 그래프 구조로 나타낼 수 있는 다양한 물류 관리 태스크 에 있어, 인공신경망 기반 학습 모델과 경로 탐색 알고리즘의 결합을 통해 태스크에 적절한 최적 경로를 도출하 여, 각 노드에서의 실행 확률을 생성함으로써, 복잡한 환경의 물류 관리의 최적화가 가능하도록 하는 것이다. 또한, 일 실시 예의 최적 경로 탐색 시스템은 강화학습 환경의 상태(state)와 행동(action)이 개별적인 경우, 해당 태스크에 적절한 경로 탐색 알고리즘과 인공신경망의 결합을 통해 특정 노드에 대해 생성된 강화학 습의 정책(policy)을 기반으로, 목표 노드까지의 최적 경로를 탐색하는 것이라고 할 수 있다. 다만, 일 실시 예 에서는, 강화학습의 정책을 기반으로 최적화가 가능한 것에 대해 개시하고 있으나, 강화학습에 한정되지 않고, 경로 탐색과 인공신경망의 결합을 통한 최적 경로 탐색이 수행될 수 있다. 또한, 일 실시 예에서는, 상태, 행동, 정책과 같이, 강화학습에서 사용되는 용어를 사용하고 있는데 이는 경로 탐색을 쉽게 설명하고자 차용한 것으로, 사용되는 다양한 경로 탐색 알고리즘에 따라 그 개념이 달라질 수 있다. 따라서 상술한 바와 같이, 일 실시 예에서는 최적 경로 탐색을 하는데 있어 강화학습이 사용되지 않을 수 도 있다(예를 들어, 경로가 구성되어 있는 내비게이션 등). 즉, 도 2에 도시된 바와 같이, 최적 경로 탐색 시스템은 그래프 구조로 나타낼 수 있는 다양한 물류 관리 태 스크에서, 경로 탐색 알고리즘을 통해 생성된 정책(Policy, pi)을 기반으로, 루트 노드로 설정되는 시작 노드 (initial node, i)를 시작으로 다음 노드를 선택하는 과정을 수행하여, 목표 노드(target node, t)까지의 최적 경로, 즉 최적의 의사결정(예를 들어, 물류 선별, 물류 이동, 물류 출고, 물류 보관 등)이 가능하도록 할 수 있 다. 일 실시 예에서, 의사결정이란 어떤 문제를 해결하기 위해 여러 대안 중 가장 적합한 대안을 선택하는 것을 의미할 수 있으며, 상술한 바와 같이 물류 관리 시 가장 적합한 물류 선별, 물류 이동, 물류 출고, 물류 보관 등을 결정하는 것을 의미할 수 있다. 종래에는 이러한 물류 관리 태스크에 강화학습(value iteration, policy gradient, DQN 등)을 적용하여, 최적 경로가 탐색되도록 하였다. 그러나 단순한 환경에서는 종래의 강화학습 방법으로도 문제가 해결되어 그 효과가확인되었으나, 예를 들어, 컨테이너 선별, 하역, 내비게이션 등과 같이 전체 상태의 개수가 많은 복잡한 환경에 서는 의사결정을 어렵게 하고, 의사결정 완료 상태에 도달하는 것 자체가 어려워 리워드를 부여 받지 못하는 빈 도가 드문 보상(sparse reward) 문제가 발생하는 등 효과가 없었다. 즉, 복잡한 문제의 경우 종래의 방법으로 생성된 정책은 부정확할 수 있기 때문에, 일 실시 예의 최적 경로 탐 색 시스템에서는, 강화학습의 정책 생성을 위해 그래프 탐색 알고리즘과 인공신경망을 결합하여 적용할 수 있으며, 다양한 물류 관리 태스크에 대응되는 다양한 그래프 탐색 알고리즘을 통하여 정책을 생성할 수 있다. 이에 일 실시 예에서는, 그래프의 노드(node)와 엣지(edge)와, 강화학습 시나리오에서 해를 정의하기 위한 수학 적 프레임워크인 마르코프 결정 프로세스(Markov Decision Process, MDP)의 상태 및 행동, 가치(value based), 정책(policy based)을 결합할 수 있다. 즉, 물류 관리 태스크의 어떤 문제의 여러 대안은 상태(예를 들어, 물류가 현재 쌓여 있는 상태, 물류 이동 시 가능한 상태 등)를 의미할 수 있으며, 이러한 각 상태 간의 관계는 그래프로 표현될 수 있다. 따라서, 그래프에 서 각 상태는 노드로 치환될 수 있고, 각 상태에서 행하는 행동은 엣지로 치환될 수 있다. 다시 말해, 그래프의 각 노드는 상태를 속성으로 가지고, 그래프의 엣지는 행동의 속성을 가질 수 있다. 따라서, 최적 경로 탐색 시스템은 그래프의 노드/엣지와 강화학습의 상태/행동을 매칭시키고, 그래프의 노드 /엣지와 강화학습의 가치/정책을 매칭시킴으로써, 터미널(terminal) 상태가 아닌 임의의 상태에서 터미널 상태 까지의 경로를 경로 탐색 알고리즘을 통해 찾을 수 있으며, 터미널 상태를 속성으로 가진 노드에 도달하면 리워 드나 패널티를 부여하여 각 노드(상태)에서 여러 엣지(행동)를 통해 목표 노드(터미널 상태)까지 도달하는 최적 경로를 탐색할 수 있다. 이때, 최적 경로 탐색 시스템은 복잡한 환경에서 인공신경망을 활용할 수 있는데, 노드와 엣지의 가중치를 상태의 가치와 정책에 매칭시켜 상태를 입력으로 하여 가치와 정책이 출력되도록 할 수 있다. 한편 본 실시 예에서는, 사용자들이 사용자 단말에서 구현되는 어플리케이션 또는 웹사이트에 접속하여, 최적 경로 탐색을 위한 데이터를 입력하거나, 최적 경로 탐색 결과를 확인하거나, 학습 모델의 하이퍼 파라미터 를 설정(입력)하는 등의 과정을 수행할 수 있다. 이러한 사용자 단말은 최적 경로 탐색 어플리케이션 또는 최적 경로 탐색 웹사이트에 접속한 후 인증 과정을 통하여 서비스를 제공받을 수 있다. 인증 과정은 회원가입 등 사용자 정보를 입력하는 인증, 사용자 단말에 대한 인증 등을 포함할 수 있으나, 이에 한정되지 않고 최적 경로 탐색 장치 및/또는 서버에서 전송되는 링크에 접속하는 것만으로 인증 과정이 수행될 수도 있다. 본 실시 예에서, 사용자 단말은 사용자가 조작하는 데스크 탑 컴퓨터, 스마트폰, 노트북, 태블릿 PC, 스마 트 TV, 휴대폰, PDA(personal digital assistant), 랩톱, 미디어 플레이어, 마이크로 서버, GPS(global positioning system) 장치, 전자책 단말기, 디지털방송용 단말기, 네비게이션, 키오스크, MP3 플레이어, 디지털 카메라, 가전기기 및 기타 모바일 또는 비모바일 컴퓨팅 장치일 수 있으나, 이에 제한되지 않는다. 또한, 사용 자 단말은 통신 기능 및 데이터 프로세싱 기능을 구비한 시계, 안경, 헤어 밴드 및 반지 등의 웨어러블 단 말기 일 수 있다. 사용자 단말은 상술한 내용에 제한되지 아니하며, 웹 브라우징이 가능한 단말기는 제한 없이 차용될 수 있다. 한편, 일 실시 예에서, 최적 경로 탐색 시스템은 최적 경로 탐색 장치 및/또는 서버에 의해 구현 될 수 있다. 다시 말하면, 일 실시 예에서, 최적 경로 탐색 장치는 서버에서 구현될 수 있는데, 이때 서버는 최적 경로 탐색 장치가 포함되는 최적 경로 탐색 시스템을 운용하기 위한 서버이거나 최적 경로 탐색 장치의 일부분 또는 전 부분을 구현하는 서버일 수 있다. 또한, 서버는 최적 경로 탐색 장치를 동작시키는 데이터를 제공하는 데이터베이스 서버일 수 있다. 그 밖에 서버는 웹 서버 또는 어플리케이션 서버 또는 딥러닝 네트워크 제공 서버를 포함할 수 있다. 그리 고 서버는 각종 인공 지능 알고리즘을 적용하는데 필요한 빅데이터 서버 및 AI 서버, 각종 알고리즘의 연 산을 수행하는 연산 서버 등을 포함할 수 있다. 또한 일 실시 예에서, 서버는 상술하는 서버들을 포함하거나 이러한 서버들과 네트워킹 할 수 있다. 즉, 본 실시 예에서, 서버는 상기의 웹 서버 및 AI 서버를 포함하거나 이러한 서버들과 네트워킹 할 수 있다. 한편, 최적 경로 탐색 시스템에서 최적 경로 탐색 장치 및 서버는 네트워크에 의해 연결될 수 있다. 이러한 네트워크는 예컨대 LANs(local area networks), WANs(Wide area networks), MANs(metropolitan area networks), ISDNs(integrated service digital networks) 등의 유선 네트워크나, 무선 LANs, CDMA, 블루투스, 위성 통신 등의 무선 네트워크를 망라할 수 있으나, 본 개시의 범위가 이에 한정되는 것 은 아니다. 또한 네트워크는 근거리 통신 및/또는 원거리 통신을 이용하여 정보를 송수신할 수 있다. 또한, 네트워크는 허브, 브리지, 라우터, 스위치 및 게이트웨이와 같은 네트워크 요소들의 연결을 포함할 수 있다. 네트워크는 인터넷과 같은 공용 네트워크 및 안전한 기업 사설 네트워크와 같은 사설 네트워크를 비롯한 하나 이상의 연결된 네트워크들, 예컨대 다중 네트워크 환경을 포함할 수 있다. 네트워크에의 액세 스는 하나 이상의 유선 또는 무선 액세스 네트워크들을 통해 제공될 수 있다. 더 나아가 네트워크는 사물 등 분산된 구성 요소들 간에 정보를 주고받아 처리하는 IoT(Internet of Things, 사물인터넷) 망 및/또는 5G 통 신을 지원할 수 있다. 도 3은 일 실시 예에 따른 최적 경로 탐색 장치를 개략적으로 나타낸 블록도이다. 도 3을 참조하면, 최적 경로 탐색 장치는 통신부, 사용자 인터페이스, 메모리 및 프로세서 를 포함할 수 있다. 통신부는 네트워크와 연동하여 외부 장치간의 송수신 신호를 패킷 데이터 형태로 제공하는 데 필요한 통신 인터페이스를 제공할 수 있다. 또한 통신부는 다른 네트워크 장치와 유무선 연결을 통해 제어 신호 또는 데이터 신호와 같은 신호를 송수신하기 위해 필요한 하드웨어 및 소프트웨어를 포함하는 장치일 수 있다. 이러한 통신부는 각종 사물 지능 통신(IoT(internet of things), IoE(internet of everything), IoST(internet of small things) 등)을 지원할 수 있으며, M2M(machine to machine) 통신, V2X(vehicle to everything communication) 통신, D2D(device to device) 통신 등을 지원할 수 있다. 통신부는 WiFi 모듈, Bluetooth 모듈, 무선 통신 모듈, 및 NFC 모듈 중 적어도 하나를 포함할 수 있다. 사용자 인터페이스는 최적 경로 탐색 장치의 최적 경로 탐색을 위한 사용자 요청 및 명령들이 입력되 는 입력 인터페이스를 포함할 수 있다. 그리고 사용자 인터페이스는 최적 경로 탐색 장치에서 수행된 최적 경로 탐색 결과가 출력되는 출력 인터페이스를 포함할 수 있다. 즉 사용자 인터페이스는 최적 경로 탐색을 위한 사용자 요청 및 명령에 따른 결과를 출력할 수 있다. 이러한 사용자 인터페이스의 입력 인터페이스와 출력 인터페이스는 동일한 인터페이스에서 구현될 수 있다. 메모리는 최적 경로 탐색 장치의 동작의 제어(연산)에 필요한 각종 정보들을 저장하고, 제어 소프트 웨어를 저장할 수 있는 것으로, 휘발성 또는 비휘발성 기록 매체를 포함할 수 있다. 메모리는 하나 이상의 프로세서와 연결되는 것으로, 하나 이상의 프로세서와 전기적 또는 내부 통신 인터페이스로 연결될 수 있으며, 프로세서에 의해 실행될 때, 프로세서로 하여금 이미지 기반 이상 탐지 장치를 제어하도록 야기하는(cause) 코드들을 저장할 수 있다. 즉, 메모리는 프로세서로 하여금, 정책을 생성하고자 하는 상태를 루트 노드로 설정하고, 복수의 경 로 탐색 알고리즘 중 선택된 경로 탐색 알고리즘을 기반으로, 루트 노드를 시작점으로 그래프를 확장하며 기 설 정된 횟수만큼 경로를 탐색하며, 루트 노드에 연계된 각 자식 노드 그룹에 각각 속해 있는 기 선택되어 더 이상 확장 불가한 노드 수에 기반하여, 루트 노드에서의 각 행동에 대한 실행 확률인 정책을 추론하고, 추론한 정책 에 기반하여 행동을 결정하고 다음 상태를 선택해가며 최적 경로를 탐색할 수 있도록 하는 코드들을 저장할 수 있다. 여기서, 메모리는 자기 저장 매체(magnetic storage media) 또는 플래시 저장 매체(flash storage medi a)를 포함할 수 있으나, 본 발명의 범위가 이에 한정되는 것은 아니다. 이러한 메모리는 내장 메모리 및/ 또는 외장 메모리를 포함할 수 있으며, DRAM, SRAM, 또는 SDRAM 등과 같은 휘발성 메모리, OTPROM(one time programmable ROM), PROM, EPROM, EEPROM, mask ROM, flash ROM, NAND 플래시 메모리, 또는 NOR 플래시 메모리 등과 같은 비휘발성 메모리, SSD. CF(compact flash) 카드, SD 카드, Micro-SD 카드, Mini-SD 카드, Xd 카드, 또는 메모리 스틱(memory stick) 등과 같은 플래시 드라이브, 또는 HDD와 같은 저장 장치를 포함할 수 있다. 특히, 본 실시 예에서, 메모리에는 본 개시에 따른 신경망 모델, 신경망 모델을 이용하여 본 개시의 다양 할 실시 예를 구현할 수 있도록 구현된 모듈이 저장될 수 있다. 그리고, 메모리에는 본 개시에 따른 학습 을 수행하기 위한 알고리즘에 관련된 정보가 저장될 수 있다. 또한 메모리에는 본 개시에 따른 학습을 수행하기 위한 알고리즘 및 수행 코드에 관련된 정보가 일시적 또는 비 일시적으로 저장될 수 있다. 그 밖에도 본 개시의 목적을 달성하기 위한 범위 내에서 필요한 다양한 정보가 메모리에 저장될 수 있으며, 메모리(13 0)에 저장된 정보는 서버 또는 외부 장치로부터 수신되거나 사용자에 의해 입력됨에 따라 갱신될 수도 있다. 프로세서는 최적 경로 탐색 장치의 전반적인 동작을 제어할 수 있다. 구체적으로, 프로세서는 상술한 바와 같은 메모리를 포함하는 최적 경로 탐색 장치의 구성과 연결되며, 상술한 바와 같은 메 모리에 저장된 적어도 하나의 명령을 실행하여 최적 경로 탐색 장치의 동작을 전반적으로 제어할 수 있다. 프로세서는 다양한 방식으로 구현될 수 있다. 예를 들어, 프로세서는 주문형 집적 회로(Application Specific Integrated Circuit, ASIC), 임베디드 프로세서, 마이크로 프로세서, 하드웨어 컨트롤 로직, 하드웨 어 유한 상태 기계(Hardware Finite State Machine, FSM), 디지털 신호 프로세서(Digital Signal Processor, DSP) 중 적어도 하나로 구현될 수 있다. 프로세서는 일종의 중앙처리장치로서 메모리에 탑재된 제어 소프트웨어를 구동하여 최적 경로 탐색 장치의 전체의 동작을 제어할 수 있다. 프로세서는 데이터를 처리할 수 있는 모든 종류의 장치를 포 함할 수 있다. 여기서, '프로세서(processor)'는, 예를 들어 프로그램 내에 포함된 코드 또는 명령어로 표현된 기능을 수행하기 위해 물리적으로 구조화된 회로를 갖는, 하드웨어에 내장된 데이터 처리 장치를 의미할 수 있 다. 이와 같이 하드웨어에 내장된 데이터 처리 장치의 일 예로써, 마이크로프로세서(microprocessor), 중앙처리 장치(central processing unit: CPU), 프로세서 코어(processor core), 멀티프로세서(multiprocessor), ASIC(application-specific integrated circuit), FPGA(field programmable gate array) 등의 처리 장치를 망 라할 수 있으나, 본 발명의 범위가 이에 한정되는 것은 아니다. 본 실시 예에서 프로세서는 최적 경로 탐색 장치가 최적의 경로 탐색 결과를 출력하도록, 딥러닝 (Deep Learning) 등 머신 러닝(machine learning)을 수행할 수 있고, 메모리는, 머신 러닝에 사용되는 데 이터, 결과 데이터 등을 저장할 수 있다. 도 4는 일 실시 예에 따른 물류 관리 태스크의 각 상태 간의 연결 관계가 그래프로 표현됨을 설명하기 위한 도 면이다. 일 실시 예에서, 프로세서는 도 4에 도시된 바와 같이, 그래프 구조로 나타낼 수 있는 다양한 물류 관리 태스크에서 최적 경로, 즉 최적의 의사결정이 가능하도록 탐색할 수 있다. 종래에는 단순한 물류 관리 태스크에 강화학습을 적용하여, 최적의 물류 관리가 가능하도록 하였으나, 예를 들 어, 컨테이너 선별 태스크의 경우, 실제 컨테이너 환경은 매우 복잡하고 다양하여 최소 1015 이상의 상태를 가진 환경에서 적용이 가능해야 할 수 있다. 또한, 환경이 복잡해질수록 선별 완료 상태에 도달할 확률이 작아지고, 터미널 상태에 도달할 때만 리워드가 부여되기 때문에, 학습 초기 리워드를 부여 받는 빈도가 적어지는 등 보상 을 받을 확률이 극히 낮을 수 있다. 즉, 복잡한 문제의 경우 종래의 방법으로 생성된 정책은 부정확할 수 있기 때문에, 일 실시 예에서는, 강화학습 의 정책 생성을 위해 그래프 탐색 알고리즘과 인공신경망을 결합하여 적용할 수 있으며, 다양한 물류 관리 태스 크에 대응하여 다양한 그래프 탐색 알고리즘을 통한 정책을 생성할 수 있다. 이에 프로세서는 강화학습의 환경의 상태와 행동이 개별적인 경우, 그래프로 표현 가능한 물류 관리 태스 크의 어떤 문제의 여러 대안(상태)에 대해 그래프에서 각 상태를 노드로 치환하고, 각 상태에서 행하는 행동을 엣지로 치환할 수 있다. 그리고 프로세서는 터미널 상태가 아닌 임의의 상태에서 터미널 상태까지의 경로, 즉 정책을 경로 탐색 알 고리즘을 통해 찾을 수 있으며, 터미널 상태를 속성으로 가진 노드에 도달하면 리워드나 패널티를 부여하여 각 노드(상태)에서 여러 엣지(행동)를 통해 목표 노드(터미널 상태)까지 도달하는 최적 경로를 탐색할 수 있다. 한편, 일 실시 예에서, 물류 관리 태스크는, 예를 들어, 컨테이너 선별(pre-marshalling), WMS/하역(material handling) 부분으로 제철소나 조선소의 철판 보관소, 컨테이너 터미널의 컨테이너 보관서(bay), autostore 등을 포함할 수 있다. 컨테이너 선별이란, 입고된 컨테이너를 출고 일정을 고려하여 추가 정리 작업 없이 바로 출고할 수 있도록 크레 인을 이용하여 정리하는 태스크를 말한다. 컨테이너 선별의 범위는 베이(bay)로 한정되어 있으며 베이는 스택(stack)과 티어(tier)로 구분될 수 있다. 각 스택에 쌓인 컨테이너는 후입선출(Last In First Out, LIFO) 규칙 을 가지며, 베이에는 최대로 쌓을 수 있는 컨테이너 개수가 정해져 있어(최대 티어가 정해져 있음), 그 이상 컨 테이너를 쌓을 수 없다. 컨테이너 선별 과정은 크레인을 이용하여 컨테이너를 이동시키는 과정으로, 베이의 각 컨테이너는 출고일자가 정해져 있어 정해진 순서대로 컨테이너를 출고하게 된다. 컨테이너 선별은 추가 정리 작업이 있어야 모든 컨테 이너를 출고할 수 있는 상태에서 추가 정리 작업 없이 모든 컨테이너를 출고할 수 있는 상태인 선별 완료 상태 를 만드는 것이 목적이다. 선별 완료 상태는 추가 정리 작업이 필요 없다는 조건만 만족하면 되므로 다양한 형 태가 가능할 수 있다. 즉, 최적 경로를 학습한다는 것은 선별 완료 상태에 도달하기까지 크레인 작업을 최소화 할 수 있는 방법을 탐색하는 것이다. 또한 Autostore는 입출고 및 보관에 쓰이는 ASRS(Auto Storage Retrieval System)의 하나로, GTP(Goods To Person) 혹은 GTM(Goods To Man) 방식으로 불리는 설비이며, 최상단에서 로봇이 주문에 맞춰 상품이 보관된 Bin(보관 상자)을 찾아내어 설비와 작업자 간 접점인 Port(입고 또는 출고 작업장)로 이송시켜주면, 작업자가 주문에 따라 해당 상품을 호출한 Bin에서 피킹하는 자동화 설비이다. 도 5는 일 실시 예에 따른 다양한 하역(material handling)에 대한 상태를 설명하기 위한 예시도이다. 하역은 화물수송 과정에서 짐을 싣고 내리는 일체의 운반활동을 말한다. 하역은 적화, 운반, 적재, 반출, 오더 피킹, 분류 및 정돈의 7가지 개념을 의미할 수 있다. 적화는 운반기기에 물품을 적입 및 적출하는 작업, 운반은 공장 또는 창고 내에서 비교적 단거리 이동을 통하여 물품을 취급하고 이동시키는 작업, 적재는 창고 등 보관시 설의 소정장소에 이동된 물품을 어떠한 형태로 쌓는 작업, 반출은 보관장소에서 물품을 꺼내는 작업, 오더피킹 은 주문된 물품을 보관장소까지 꺼내는 작업, 분류는 물품을 품목별 발송 지역, 고객별로 구분하는 작업, 정돈 은 출하하는 물품을 운송기기에 즉시 적입 할 수 있도록 정리하는 작업을 말한다. 도 5(a)를 참조하면, stacking(container bay)은 베이에 컨테이너 출고일자를 고려하여 쌓는 것으로, 쌓는 작업 과 선별 작업을 동시에 시행하여 선별 완료 상태를 만들 수 있고, 산업체에 따라 선별을 하지 않고 쌓기만 하여 선별완료상태를 만들지 않을 수 있다. 또한, 도 5(b)를 참조하면, BRP(Block Relocation Problem)은 선별을 하며 출고 가능한 컨테이너를 출고하는 작업으로, 베이에 출고일자가 임박하지 않는 컨테이너는 남겨둘 수 있으며, 컨테이너를 남길 경우 터미널 상태 는 1개 이상의 컨테이너가 남을 수 있다. 또한, 적화(stowage, container ship)는 컨테이너 선박에 컨테이너를 쌓는 작업으로, 선박의 항구 방문 순서와 컨테이너의 목적지를 고려하여 쌓을 수 있으며, 선박의 좌우 균형 및 컨테이너의 중량도 고려하여 쌓을 수 있도 록 하는 것이다. 더불어, 도 5(c)를 참조하면, 물류 관리 태스크는 조선소 강재 적치장에서의 태스크도 의미할 수 있는데, 조선 소 강재 적치장은 선박 건조 자재인 철판을 보관하는 장소이다. 강재의 보관 및 출고 프로세스는, 강재의 입고 시 임시 적치장에 보관하고, 한달치 가공 예정 강재를 1차 적치장으로 이동시키며, 일주일치 가공 예정 강재를 2차 적치장으로 이동시킬 수 있고, 2차 적치장에서 가공이 임박한 강재 출고를 수행할 수 있다(pre- marshalling, BRP) 이러한 조선소의 강재 적치장은 BPR과 stacking의 특징을 동시에 가지고 있어서, 1차 적치장 에서는 BPR을 통해 이동 대상 강재를 선별해 낼 수 있고, 2차 적치장에서는 stacking을 통해 강재를 쌓는 작업 을 수행할 수 있다. 상술한 하역 문제는 그래프로 표현될 수 있으나, 트리 탐색을 통해서도 탐색될 수 있도록 변환할 수도 있을 것 이다. 그 외에도 고도화된 작업에 따라 적합한 인공신경망을 구성하고 역선별 없이 데이터를 생성하거나, BRP로 학습된 인공신경망이 스태킹에도 사용되도록 할 수 있을 것이다. 한편, 상술한 물류 관리 태스크는 물류 관리 분야가 아닌 내비게이션 등에도 적용되어, 내비게이션의 최단 시간 소요되는 경로를 계산할 수 있다. 상술한 하역 문제와 다르게 그래프(지도)가 완성된 상태일 수 있으며, 인공신 경망으로 시간별/구간별 교통상황을 예측하여 최단 경로가 예측되도록 할 수 있다. 일 실시 예에서는, 하나의 노드에서 다른 노드로 한번 이동할 때마다 정해진 횟수만큼 경로 탐색을 하는 방법을 사용할 수 있으며, 그래프 에서 트리 탐색을 통해서도 최단 경로를 구할 수 있을 것이다. 또한, 내비게이션의 경로 탐색은 일 실시 예의 정책 생성만 사용함으로써 경로 예측이 되도록 할 수 있다. 즉, 별도 강화학습을 사용하지 않아 환경과 에이전트의 상호작용으로 인공신경망을 학습시키기 위한 데이터를 생성하지 않고 인공신경망의 학습 데이터를 실제 교통상황 데이터로 사용할 수 있다. 특히, 내비게이션의 경우, 시 시각각 변하는 교통량에 따라 가장 적합한 경로를 찾아야 한다. 그래프 상에서 노드는 분기점, 엣지는 구간이 되며, 강화학습의 정책을 이용하면 구간의 막히는 정도를 나타낼 수 있다. 즉, 일 실시 예에서는, 빅데이터 등 을 이용해 시간별/구간별 막히는 정도를 인공신경망으로 학습하여 가치/정책을 출력하도록 할 수 있다. 즉, 상술한 물류 관리 태스크들은 환경이 복잡하여 노드의 개수가 많기 때문에, 그래프의 구조를 미리 파악하여 경로 탐색을 통해 최적해를 찾는 것이 불가능하고, 그래프 구조 파악의 시간 소요가 커 비효율적이며, 그 환경 이 시시각각 바뀌어 각기 다른 그래프의 구조를 갖기 때문에 모든 환경에 따른 그래프의 구조를 파악하는 것 또 한 어려움이 있다. 한편, 마르코프 결정 프로세스는 이산시간 확률제어 과정으로, 상태, 행동 및 전이확률 기반 최적의 의사결정 정책을 탐색하는 강화학습 기법이다. MDP의 핵심 문제는 최적의 의사결정 정책(policy)을 결정하는 탐색 (Exploration)을 통한 개발(Exploitation) 수행에 있다. 즉 '다양한 행동들을 시도해보는 경향'과 '가장 좋다고 판단한 행동에 집중하는 경향'의 균형을 맞추는 것이 중요할 수 있다. 일 실시 예에서, 컨테이너 선별을 마르코프 결정 프로세스를 기반으로 수행하는 경우, 하나의 베이를 표로 표현 한 것을 상태라고 할 수 있으며, 하나의 스택 맨 위의 컨테이너를 빼 다른 하나의 스택 맥 위에 쌓는 과정을 행 동이라고 할 수 있고, 터미널 상태에 도달하면 리워드 1을 부여할 수 있다. 보다 구체적으로, 각 상태는 출고 일자를 나타낸 번호가 컨테이너의 위치에 대응하여 표로 표현될 수 있다. 예 를 들어, 빈 공간은 0, 가장 빨리 출고되는 컨테이너는 1, 가장 늦게 출고되는 컨테이너는 3으로 표현될 수 있 다. 또한 컨테이너 이동은 행동으로 정의할 수 있으며, 베이 내에서만 이동하므로 행동의 개수는 (스택의 개수) X (스택의 개수 - 1)일 수 있다. 예를 들어, 스택이 3개인 경우 6가지의 행동이 가능할 수 있다. 또한, 반출하는 스택에 컨테이너가 없거나 쌓고자 하는 스택에 컨테이너가 최대로 쌓여 있으면 컨테이너의 이동이 불가능하여 불가능한 행동이라고 할 수 있다. 이때, 선별되어 가는 과정이 하나의 해가 되며, 최소의 행동 횟수로 선별하는 해가 최적해가 된다. 즉, 최적해를 찾는 것이 목표가 된다. 이를 위해, 선별 완료 상태(terminal state)에 도달한 경우 리워드를 부여할 수 있다. 이때 터미널 상태는 모든 스택의 위에서부터 출고 순서가 오름차순으로 정렬된 상태를 기준으로 할 수 있으며, 컨테이너 선별의 목적이 추가적인 정리 작업 없이 베이 내의 모든 컨테이너를 모두 출고할 수 있도록 하는 것이므로 다양한 형태가 가능 할 수 있다. 상술한 바와 같이, 컨테이너 선별은 임의의 초기 상태로부터 선별 완료 상태에 도달하는 과정을 구하는 것이 목 적이 된다. 그리고 여러 선별해(solution) 중 그 크기가 가장 짧은 해가 최적해(optimal solution)가 된다. 즉, 일 실시 예의 목적은 최적해에 가까운 해를 도출함으로써 적은 수의 행동으로도 터미널 상태에 도달하는 것이다. 즉, 일 실시 예는, 그래프 탐색을 통해 정책 생성을 수행하여 의사결정 태스크에서 최적의 의사결정이 수행되도 록 탐색하는 것으로, 전체 그래프의 구조가 파악된 후 시작 노드부터 목적 노드까지의 경로를 찾는 그래프 탐색 과 달리, 전체 그래프의 구조를 몰라도 적용할 수 있다. 도 6은 일 실시 예에 따른 강화학습 적용을 위한 인공신경망의 학습 데이터 생성 방법을 설명하기 위한 흐름도 이고, 도 7은 일 실시 예에 따른 정책 생성 기반 최적 경로 탐색 방법을 설명하기 위한 흐름도이며, 도 8은 일 실시 예에 따른 에이 스타 알고리즘 기반 트리 구조의 정책 생성 과정을 설명하기 위한 흐름도이고, 도 9는 일 실시 예에 따른 에이 스타 알고리즘 기반 그래프 구조의 정책 생성 과정을 설명하기 위한 흐름도이며, 도 10 내 지 도 12는 일 실시 예에 따른 정책 생성을 설명하기 위한 도면이다. 그리고 도 13은 일 실시 예에 따른 역방향 및 정방향 선별 에피소드를 통해 생성된 데이터의 일례를 그래프 상 에 나타낸 도면이며, 도 14는 일 실시 예에 따른 평가함수 산출 방법을 설명하기 위한 도면이고, 도 15는 일 실 시 예에 따른 역방향 선별 에피소드를 인공신경망의 학습 데이터로서 활용하기 위한 가중치 값을 설명하기 위한 도면이며, 도 16은 일 실시 예에 따른 정방향 선별 에피소드를 인공신경망의 학습 데이터로서 활용하기 위한 가 중치 값을 설명하기 위한 도면이고, 도 17은 일 실시 예에 따른 에이 스타 알고리즘에서 각 노드의 가중치 값이 갖는 의미를 설명하기 위한 도면이다. 이하에서는, 도 6 내지 도 17을 참조하여, 물류 관리 태스크에 대해 컨테이너 선별을 실시 예로 하여, 컨테이너 선별에 적합한 정책 생성 방법 및 컨테이너 선별에 적합한 데이터 생성 방법에 대해 설명한다. 본 명세서에서 컨테이너 선별은 하나의 실시 예로 다른 다양한 물류 관리 태스크에도 이하에서 설명하는 정책 생성 방법 및 데 이터 생성 방법이 동일하게 적용될 수 있으며, 물류 관리 태스크에 따라 경로 탐색 알고리즘을 달리하거나, 데 이터 생성 방법을 달리할 수 있다(실시 예에 따라 역방향은 수행하지 않을 수 있음). 상술한 바와 같이, 컨테이너 선별의 경우 복잡한 환경이기 때문에, 컨테이너 선별의 각 상태 간의 관계를 그래 프로 표현하고, 그래프 상의 노드, 엣지와 강화학습 가치, 정책을 매칭할 수 있다. 먼저, 강화학습의 마르코프 결정 프로세스의 각 용어에 대하여 정의한다. '상태(state)'는 높이인 티어(tier)와 스택(stack)의 개수가 정의된 베이(bay)에 출고일자를 표기한 각 컨테이너가 쌓인 상태를 의미하고, '행동 (action)'은 컨테이너를 옮기는 프로그램 또는 프로세서에 의해 일어나는 행위 또는 동작을 의미하고, '상태전 환확률(state transition probability)'은 일 실시 예에서 1.0으로 설정할 수 있으며, '보상(reward)'은 어떠 한 행동(action)을 취하고 해당 환경으로부터 새로운 상태와 함께 결정되는 것으로, 일 실시 예에서 선별이 완 료된 경우 결정될 수 있으며, '할인계수(discount factor)'는 0보다 크고 1보다 작은 실수를 적용할 수 있으나, 바람직하게는 0.8로 적용하여 사용할 수 있다. 즉, 상태는 베이에 각각의 출고일자를 가진 컨테이너가 쌓인 상태를 의미할 수 있다. 그리고 행동은 컨테이너의 이동을 정의하며, 베이 내에서만 이동하므로 행동의 개수는 '(stack의 개수)*(stack 의 개수 - 1)'로 정의할 수 있다. 또한, 선별되어가는 과정이 하나의 해가 될 수 있고, 최소 행동의 횟수로 선 별하는 해가 최적해가 되며, 최적해를 찾는 것이 일 실시 예에 강화학습을 적용하는 목적이자 목표라고 할 수 있다. 한편, 불가능한 행동은 반출하는 스택에 컨테이너가 없거나, 쌓고자 하는 스택에 컨테이너가 최대로 쌓여 있는 경우가 있으며, 이러한 경우 컨테이너의 이동이 불가능하므로 각 상태마다 가능한 행동의 개수가 다를 수 있다. 상기 보상은 추가적인 행동 없이 모든 컨테이너가 출고되는 상태가 터미널 상태로 되어 리워드를 부여 받을 수 있다. 일 실시 예에서는, 터미널 상태에서만 리워드를 부여 받을 수 있으며, 패널티는 적용하지 않는다. 여기서, 터미널 상태의 판단 기준은 각 스택의 컨테이너의 출고 순서가 오름차순으로 되어 있으면 선별이 완료 됨을 의미하며, 이러한 선별완료상태(terminal state)는 다양할 수 있다. 일 실시 예에서, 컨테이너 선별의 각 상태 간의 연결 관계는 도 4에 도시된 바와 같은 그래프의 형태를 가질 수 있다. 해당 그래프에서 각 노드는 하나의 상태를 속성으로 갖고, 각 엣지는 행동을 의미한다. 따라서 선별 되지 않은 초기 상태에서 행동을 통해 선별완료상태에 도달한다는 것은 선별 되지 않은 상태를 속성으로 갖는 노드에 서 엣지를 선택하고 다른 노드로 이동을 반복하여 선별완료상태를 속성으로 갖는 노드에 도달하는 것과 같은 의 미이다. 그러나, 간단한 환경에 기존의 강화학습 방법(DQN)을 적용한 경우 선별완료상태에 도달하는 목표를 해결할 수 있으나, 복잡한 환경에 적용하는 경우 해결되기 어렵다. 이는 선별완료상태의 비율이 기하급수적으로 작아지게 되어 학습 초반 선별완료상태에 도달하지 못하게 되며, 이런 경우 보상을 받지 못하는 'sparse reward(빈도가 드문 보상)' 문제가 발생하기 때문으로 이런 경우 전체 학습에 문제가 생기게 된다. 이러한 이유로 기존의 강화 학습 방법인 'policy gradient(정책 경사)'나 'value iteration(가치 반복)'과 같은 방법으로는 선별되지 않는 다. 추가적으로, 상태의 개수가 상당히 많기 때문에 기존의 강화학습 방법의 적용이 어려운 것으로 예상된다. 도 6을 참조하면, 프로세서의 컨테이너 선별 태스크의 강화학습 적용을 위한 인공신경망의 학습 데이터 생 성 방법은 역방향 선별 에피소드 생성 단계(S100), 정방향 선별 에피소드 생성 단계(S200), 역방향 선별 에피소 드 각 상태의 가중치 값 결정 단계(S300) 및 정방향 선별 에피소드 각 상태의 가중치 값 결정 단계(S400) 및 학 습 데이터 증가 단계(S500) 중 적어도 하나를 포함할 수 있다. 역방향 선별 에피소드 생성 단계(S100)에서는, 스택에 대한 출고 순서가 오름차순으로 정렬되어 최적해가 0이 되어 선별완료상태인 터미널 상태를 시작으로 스택에 대한 출고 순서가 빈 공간을 제외하고 내림차순으로 정렬 되어 최적해가 최대값인 하드 상태(hard state)까지 도달하기 위하여, 현재 상태(current state)에서 생성된 정 책을 통해 행동을 결정하여 다음 상태(next state)의 선택을 반복하며 정해진 개수의 상태를 갖는 역방향 선별 에피소드를 생성해 나가되, 정책 생성 과정에서는 정책을 생성하고자 하는 현재 상태를 루트 노드(root node)로 하여 경로 탐색 알고리즘을 통한 경로 탐색과 그래프를 그려가는 과정을 동시에 진행하는 방법으로 다음 상태를 결정하기 위한 정책을 생성할 수 있다. 역방향 선별 에피소드의 최초 상태는 스택에 대한 출고 순서가 오름차순으로 정렬되어 선별 완료된 터미널 상태 를 의미하며, 예를 들어 스택에 대하여 출고 순서가 오름차순으로 정렬되어 더 이상 선별 과정이 필요 없어 최 적해가 0인 다양한 선별완료상태의 조건을 갖춘 터미널 상태들을 포함할 수 있다. 또한, 역방향 선별 에피소드가 목표로 하는 상태는 터미널 상태와 반대로 각 스택에 대하여 빈 공간을 제외하고 출고 순서가 내림차순으로 정렬되는 조건을 가져 최적해가 정의된 환경에서 최대값을 갖는 상태(또는 후술하는 하드 상태)를 의미할 수 있으며, 상기 역방향 선별 에피소드의 초기 상태가 되는 터미널 상태는 각 스택의 출고 순서가 오름차순이라는 규칙에 맞는 다양한 터미널 상태를 랜덤하게 생성하는 것이 가능하며 역방향 선별 에피 소드 생성 단계(S100)에서 랜덤하게 터미널 상태를 생성해 역방향 선별 에피소드의 초기 상태로 지정해준다. 상술한 바와 같이, 역방향 선별 에피소드 생성 단계(S100)에서는, 현재 상태에서 정책을 생성 및 실행하여 행동 을 결정하고, 다음 상태를 선택하는 과정의 반복으로 터미널 상태에서 하드 상태까지 도달하기 위한 역방향 선 별(선별이 완료된 상태에서 후술될 하드 상태를 목표로 역방향으로 선별) 과정을 통하여 다양한 경로와 각 경로 상에 있는 상태들을 탐색할 수 있으며, 이후 후술하겠으나 선별 에피소드는 에피소드 각 상태의 가중치 값을 결 정하는 단계인 역방향 선별 에피소드 각 상태의 가중치 값 결정 단계(S300)에 활용하게 된다. 한편, 정방향 선별 에피소드 생성 단계(S200)에서는, 빈 공간을 제외한 스택에 대한 출고 순서가 내림차순으로 정렬되어 정의된 환경에서 최적해가 최대인 하드 상태를 정방향 선별 에피소드의 시작으로 스택에 대한 출고 순 서가 오름차순으로 정렬되어 선별 완료되어 최적해가 0인 터미널 상태까지 도달하기 위하여 현재 상태에서 생성 된 정책을 통해 행동을 결정하여 다음 상태의 선택을 반복하며 정해진 개수의 상태를 갖는 정방향 선별 에피소 드를 생성해나가되, 정책 생성 과정에서는 정책을 생성하고자 하는 현재 상태를 루트 노드로 하여 에이 스타 알 고리즘을 통한 경로 탐색과 그래프를 그려가는 과정을 동시에 진행하는 방법으로 다음 상태를 결정하기 위한 정 책을 생성할 수 있다. 상기 정방향 선별 에피소드의 최초 상태는 빈 공간(즉, 출고 순서가 0으로 컨테이너가 없는 공간)을 제외한 스 택에 대하여 출고 순서가 내림차순으로 정렬되어 최적해가 최대인 하드 상태를 의미하며, 예를 들어 빈 공간을 제외한 공간에 컨테이너가 존재하되, 그들의 출고 순서가 스택을 기준으로 내림차순으로 정렬되어 정의된 환경 에서 최적해가 최대값을 갖는 상태들을 포함할 수 있다. 또한, 정방향 선별 에피소드가 목표로 하는 상태는 하드 상태와 반대로 스택에 대한 출고 순서가 오름차순으로 정렬되어 선별 완료된 조건을 갖는 상태인 터미널 상태를 의미할 수 있으며, 상기 정방향 선별 에피소드의 초기 상태가 되는 하드 상태는 각 스택의 출고 순서가 내림차순이라는 규칙에 맞는 다양한 하드 상태를 랜덤하게 생 성하는 것이 가능하며 정방향 선별 에피소드 생성 단계(S200)에서 랜덤하게 하드 상태를 생성해 정방향 선별 에 피소드의 초기 상태로 지정해준다. 상술한 바와 같이, 정방향 선별 에피소드 생성 단계(S200)에서는, 현재 상태에서 정책을 생성 및 실행하여 행동 을 결정하고 다음 상태를 선택하는 과정의 반복으로 하드 상태에서 터미널 상태까지 도달하기 위한 정방향 선별 (최적해가 최대값인 상태에서 최적해가 0인 터미널 상태를 목표로 정방향으로 선별) 과정을 통하여 다양한 경로 와 각 경로 상에 있는 상태들을 탐색할 수 있으며, 이후 후술 하겠으나 선별 에피소드는 에피소드 각 상태의 가 중치 값을 결정하는 단계인 정방향 선별 에피소드 각 상태의 가중치 값 결정 단계(S400)에 활용하게 된다. 일 실시 예에서, 정책은 인공신경망 학습 데이터 생성을 위한 에피소드 생성과 컨테이너 물류에 대한 선별 과정 에서 사용되며 임의의 현재 상태에서 다음 상태를 선택하기 위해 사용된다. 이러한 정책을 생성하는 과정을 중심으로, 일 실시 예의 최적 경로 탐색 방법에 대해 설명하면, S1000단계에서, 프로세서는 복수의 경로 탐색 알고리즘 중 선택된 경로 탐색 알고리즘을 기반으로, 시작 노드에서 그래프 를 확장하며 기 설정된 횟수만큼 경로를 탐색할 수 있다. 즉, 프로세서는 각 상태에서 행동을 선택하여 다음 상태 생성을 반복할 수 있으며, 다음 상태가 터미널 상 태가 될 때까지 시행하여 최종적으로 터미널 상태에 도달하도록 할 수 있다. 이때 프로세서는 행동을 선택하기 위해 경로 탐색을 이용하여 정책을 계산하게 되는데, 예를 들어, 10번의 행동을 통해 터미널 상태에 도달했다면 10번의 경로 탐색을 시행한 것과 같다고 볼 수 있다. 그래프 상의 노드, 엣지와 강화학습 가치, 정책의 결합에 있어, 정책 기반 방법의 정책은 임의의 상태에서 각 행동을 실행할 확률을 말하며, 그래프에서 각 행동은 엣지로 표현되므로 각 엣지는 확률값을 속성으로 가질 수 있다. 예를 들어, 행동의 수가 4가지인 경우, 각 행동의 정책 값은 0.08, 0.9, 0.01, 0.01의 값을 가질 수 있는데, 행 동 1 ~ 4까지의 정책 값의 합은 1이 되어 각 행동을 실행할 확률로서 행동을 선택하는 기준이 된다. 이때, 0.9(90%)의 확률을 가진 행동 2가 실행될 확률이 높다고 볼 수 있다. 따라서, 학습이 정상적으로 되었다면 높은 확률의 행동을 실행해 가면서 터미널 상태에 도달하게 된다. 한편, 일 실시 예에서는, 각 노드가 개별적인 환경에서 그래프와 강화학습의 결합되어 그래프 탐색이 수행될 수 있는데, 이처럼 강화학습의 상태/행동과 가치/정책을 그래프의 노드/엣지와 매칭시키면 그래프 탐색을 통해 최 단 경로를 찾는 것이 가능할 수 있다. 즉, 그래프 탐색을 통해 터미널 상태를 속성으로 하는 노드에 도달하는 최단 경로를 찾을 수 있는 것이다. 일 실시 예에서, 이러한 그래프 탐색에 적용 가능한 그래프 탐색 알고리즘의 종류는, DFS(Depth First Search), BFS(Breadth First Search), Dijkstra algorithm, A* algorithm, Iterative Deepening, MCTS(Monte Carlo Tree Search) 등이 포함될 수 있다. 이때 MCTS의 경우 트리 탐색이지만 그래프 탐색에도 적용 가능할 수 있으며, 그 외에 다른 트리 탐색 방법도 적용 가능할 수 있다. 그리고 S2000단계에서, 프로세서는 시작 노드에 연계된 각 자식 노드 그룹에 각각 속해 있는 기 선택되어 더 이상 확장 불가한 노드 수에 기반하여, 시작 노드에서의 각 행동에 대한 실행 확률인 정책을 추론할 수 있다. 따라서, S3000단계에서, 프로세서는 상기 추론한 정책에 기반하여 행동을 결정하고 다음 상태를 선 택해가며 최적 경로를 탐색할 수 있다. 이하에서는, 에이 스타 알고리즘을 실시 예로 하여 탐색 과정과 루트 노드를 중심으로 그래프를 그려가는 과정 을 동시에 진행하는 최적 경로 탐색 방법에 대해 보다 구체적으로 설명한다. 에이 스타 알고리즘 이외의 알고리 즘에 대해서는 후술하도록 한다. 도 8 및 도 9를 참조하면, 프로세서는 그래프, 개방 노드 리스트, 폐쇄 노드 리스트를 초기화하고(S11), 이동을 결정하고자 하는 노드를 루트 노드로 지정할 수 있다(S12). 그리고 프로세서는 인공신경망을 이용하여 루트 노드의 가중치 값을 계산할 수 있으며(S13), 계산된 가중 치로 h(n) 값을 결정하고, g(n) 값은 0으로 하여, 평가함수 값 f(n)을 h(n)+g(n)으로 계산함으로써, 노드의 속 성으로 하여 개방 노드 리스트에 추가할 수 있다(S14). 이후 프로세서는 개방 노드 리스트에서 가장 작은 평가 함수 값을 가진 노드를 선택할 수 있다(S15). 그리고 프로세서는 선택된 노드에서 이동 가능한 노드들로 이뤄진 노드 리스트를 구성할 수 있다(S16). 이후 프로세서는 다시 i=0에서 확인된 노드 리스트를 초기화하고(S17), 확인된 노드 리스트가 노드 리스트 임을 확인한 후(S19), 인공신경망을 이용하여 확인된 노드 리스트(i)의 노드 가중치와 선택된 노드를 연결하는 엣지의 가중치를 계산하고, 계산된 노드의 가중치로는 h(n) 값을 결정하고, 엣지의 가중치에는 선택된 노드의 g(n) 값을 더하여 g(n) 값을 계산할 수 있다(S20). 그리고 프로세서는 h(n)+g(n)의 수식으로 계산된 평가함수 값 f(n)과 확인된 노드 리스트(i)를 속성으로 하는 노드를 생성하고(S21), 생성된 노드와 선택된 노드 간 엣지를 연결하고, 생성된 노드를 개방 노드 리스트 에 추가할 수 있다(S22). 이후 프로세서는 개방 노드 리스트에 포함된 노드들 중 선택된 노드로 확장하여 자식 노드들을 생성한 경 우(S23, S24), 선택되었던 노드를 개방 노드 리스트에서 삭제하고 폐쇄 노드 리스트에 추가할 수 있다(S25). 그리고 프로세서는 폐쇄 노드 리스트에 정해진 개수만큼의 노드가 존재할 때까지 상술한 노드 확장 과정을 반복할 수 있다(S26). 또한, 도 9를 참조하면, S17단계 이후, 프로세서는 새롭게 생성된 노드를 기존에 그래프를 구성하는 노드 들과 비교할 수 있다(S18-1). 즉, 프로세서는 그래프를 구성하는 노드들 중 노드 리스트(i)와 같은 노드가 있는지 확인할 수 있다. 프로세서는 같은 노드가 없는 경우, 확인된 노드 리스트에 노드 리스트(i)를 추가할 수 있고(S18-2), 같은 노드가 있는 경우, 같은 노드와 개방 노드 리스트에서 선택된 노드를 엣지로 연결할 수 있다(S18-3). 이후 프로세서는 선택된 노드로 확장하여 자식 노드 생성을 위한 리스트를 확인하고(S18-4, S18-5), 다시 i=0를 수행한다(S18-6). 즉, 트리 구조에 대한 도 8과 달리, 그래프 구조에 대한 도 9에서는 새롭게 생성된 노 드들을 기존에 존재하던 노드들과 비교하는 과정이 추가되어 있는데, 일 실시 예에서는, 이러한 그래프 구조뿐만 아니라 트리 구조의 탐색을 수행하여 중복 노드를 확인하는 과정에 소요되는 시간과 비용을 감소시킬 수 있 다. 일 실시 예에서, 트리 구조의 탐색을 수행하여 중복 노드를 확인하는 과정 없이 중복 노드에 대해서도 자식 노 드로 생성하더라도 해당 노드를 선택할 확률이 낮기 때문에 그래프 구조의 탐색과 유사한 결과가 도출될 수 있 다. 이하에서는, 상술한 에이 스타 알고리즘 기반 정책 생성 과정에 대해, 도 10 내지 도 13을 참조하여, 실시 예를 기반으로 설명한다. 이때, 역방향 선별 에피소드 생성 단계(S100) 및 정방향 선별 에피소드 선별 단계(S200)는 정책 생성 과정이 동일할 수 있고, 시작 노드에서 목표 노드의 설정, 즉 목표 노드에서 시작 노드로 경로 탐색 을 하거나 시작 노드에서 목표 노드로 경로 탐색을 하는 등 그 방향만 다를 뿐 정책 생성 과정은 동일하게 수행 될 수 있다. 일 실시 예에서는, 정책을 생성하고자 하는 현재 상태를 속성으로 갖는 루트 노드를 시작으로 하여 정해진 횟수 만큼 노드의 선택과 선택된 노드의 자식 노드 확장을 반복하는데, 이때 노드의 선택은 개방 노드 리스트의 개방 노드 중 가장 작은 평가 함수 값(f(n))을 가진 노드를 선택하게 된다. 그리고 노드의 확장은 선택된 노드의 상태에서 가능한 행동을 각각 실행하였을 때 생성된 상태를 속성으로 갖는 노드를 생성하는 것을 의미하며, 새롭게 생성되는 노드는 기존에 생성된 노드들의 상태 중 어느 하나라도 같은 상태가 없는 경우 새로운 노드를 생성하는 방법으로 그래프를 그려가게 된다. 이때, 정해진 횟수만큼 노드의 선택과 확장이 시행되면 루트 노드의 자식 노드를 분기점으로 자식 노드 그룹을 구분할 수 있으며, 루트 노드의 각 자식 노드 그룹에 포함되어 있는 폐쇄 노드의 수를 산출하여 정책을 생성하 면 현재 상태에 대한 정책 생성은 종료되며 생성된 정책을 실행하여 행동을 결정하고 에피소드 상에서 다음 상 태를 선택할 수 있게 된다. 역방향 선별 에피소드 생성 단계(S100) 및 정방향 선별 에피소드 생성 단계(S200)의 정책 생성 과정에서는, 루 트 노드를 시작으로 에이 스타 알고리즘을 통한 경로 탐색을 하기 위해 미리 설정된 횟수만큼 노드의 선택과 선 택된 노드의 자식 노드 확장을 반복하는데, 이 과정에서 선택될 수 있는 개방 노드(open node)들에 대한 정보를 담고 있는 개방 노드 리스트와, 기 선택되어 더 이상 확장 불가한 폐쇄 노드(closed node)들에 대한 정보를 담 고 있는 폐쇄 노드 리스트를 관리하고, 노드의 선택 및 확장 후 개방 노드와 폐쇄 노드에 대해 지속적으로 업데 이트하여 관리할 수 있다. 예를 들어, 도 10을 참조하면, 최초 루트 노드에서 가능한 모든 행동을 실행하여 루트 노드의 자식 노드인 1~6 번 노드를 생성했을 때(정책 생성 할 때 최초로 개방 노드 리스트에 추가 되는 노드는 루트 노드), 루트 노드는 더 이상 선택 될 수 없는 폐쇄 노드 리스트(closed node list)에 추가되고, 1~6번 노드는 선택되고 자식 노드를 확장하여 새로운 노드를 생성할 수 있는 개방 노드 리스트(open node list)에 추가될 수 있다. 그리고 개방 노드 리스트에 포함된 노드들 중 2번 노드가 선택되고 2번 노드를 확장하여 2번 노드의 자식 노드 들을 생성한 경우, 새롭게 생성된 2번 노드의 자식 노드들은 개방 노드 리스트에 추가 되고, 2번 노드는 개방 노드 리스트에서 삭제되고, 폐쇄 노드 리스트에 추가될 수 있다. 개방 노드 리스트에서 개방 노드를 선택하는 기준은 평가함수 값(f(n))을 이용하는 방법으로 개방 노드 리스트 에서 가장 작은 평가함수 값을 갖는 노드를 선택하게 되며 개방 노드 리스트에 추가 되는 노드들은 각 노드마다 평가함수 값을 미리 계산하여 각 노드의 속성으로 추가되어 해당 노드는 개방 노드 리스트에 추가 된다. 에이 스타 알고리즘의 평가함수(f(n))는 기존의 전통 에이 스타 알고리즘과 마찬가지로 f(n) = g(n)+h(n)의 수 식으로 정의될 수 있다. 새롭게 확장된 각 노드는 f(n) 값이 계산되어 개방 노드 리스트에 추가된다. h(n)은 heuristic으로 적합한 방법 이 적용될 수 있다. g(n)은 루트 노드로부터의 거리의 합으로, g(n)도 적합한 방법이 적용될 수 있다. 도 14(a)는 그래프에서 각 노드간의 거리를 1로 고정한 경우 g(n) 값 계산 방법을 설명하기 위한 것으로, A노드 는 R노드(루트)로부터의 거리가 1이므로, g(n) 값은 1이다. 그리고 B노드는 R노드로부터의 거리가 2이므로 g(n) 값은 2이다. 또한, C노드는 R노드로부터 거리가 4이므로, g(n) 값은 4이다. 다만 그래프 상에서는 최단 거리가 3인 것이 확인 가능하지만, 탐색 경로(R -> 1 -> 2 -> 3 -> C)를 따라가보면 4이다. 즉, 상술한 방법으로 평가 함수 f(n) 값을 계산하기 위한 g(n) 값을 구할 수 있다.또한, h(n) 값은 그 산출 방법이 다양할 수 있으며, 인공신경망을 기반으로 산출될 수 있다. 도 14(a)를 참조하 면, 각 노드의 평가함수 f(n) 값을 구하기 위해, 먼저 인공신경망을 이용하여 R노드의 h(n) 값을 구한다. 예를 들어, 5라고 한다면 R노드의 평가 함수는 g(n) 값 0과 h(n) 값 5의 합으로 f(n)은 5가 된다. 그리고 C노드의 h(n) 값도 역시 인공신경망으로 산출될 수 있다. h(n) 값이 3이라 한다면 C노드의 평가 함수는 g(n) 값 4와 h(n) 값 3의 합으로 f(n)은 7이 된다. 또한, B노드에서는 h(n) 값이 4라면 g(n) 값 2와의 합으로 f(n)은 6이 된다. 한편, 상술한 설명에서는 인공신경망을 이용하여 h(n) 값을 구하는 것으로 기재하였으나, 인공신경망을 이용해 노드의 가중치를 구한 후 추가 연산이 수행될 수 있다. 또한, 상술한 방법에서는 각 노드 간의 거리를 1로 고정했으나, 각 노드 간의 거리를 인공신경망으로 계산할 수 있다. 즉, 각 엣지의 가중치를 인공신경망으로 계산하는 것이다. 도 14(b)는 인공신경망을 이용하여 노드의 가중치와 엣지의 가중치를 구하는 방법을 설명하기 위한 것이다. R노 드의 속성인 상태를 인공신경망에 입력하여 g(n) 값과 h(n) 값을 한번에 계산할 수 있다. 예를 들어, R노드의 h(n) 값이 5이고, R과 1 노드 간의 g(n) 값이 0.8, R과 2 노드 간의 g(n) 값이 1.4, R과 3 노드 간의 g(n) 값 이 1.1, R과 4 노드 간의 g(n) 값이 2.1, R과 5 노드 간의 g(n) 값이 0.5, R과 6 노드 간의 g(n) 값이 1.05로 계산될 수 있다. 또한, 도 14(c)를 참조하여, 계속해서 확장을 해 나가면서 B노드의 평가함수를 구하는 방법을 살펴보면, 3번 노 드의 상태를 인공신경망에 입력으로 하여 g(n) 값과 h(n) 값을 구할 수 있다. 이때 3번 노드에서 B노드로 가는 엣지의 g(n) 값이 0.7이 나왔다면 B노드의 g(n) 값은 1.1+0.7로 하여 1.8이 될 수 있다. 이때, 노드 3과 노드 R을 연결하는 2개의 노드가 갖는 가중치는 서로 다를 수 있다. 이는 각 노드 3과 노드 R의 속성인 상태를 이용하여 엣지의 가중치를 구하기 때문이다. 일 실시 예에서는, g(n) 값도 상술한 바와 같이, 인공신경망으로 계산된 후 그대로 사용되지 않고 특정 연산을 수행하여 g(n) 값으로 사용될 수 있다. 한편, 상기에서는 하나의 인공신경망에 대해서만 개시하였으나, 두 개의 인공신경망이 사용될 수도 있다. 하나 의 인공신경망은 노드의 가중치만 계산하고, 다른 하나는 엣지의 가중치만 계산하도록 할 수 있다. 상술한 내용을 다시 정리하면, g(n)은 루트 노드에서 0으로 시작하여 확장된 현재 자식 노드까지의 거리 값이며, 1씩 증가되는 값이거나 인공신경망을 기반으로 산출될 수 있고, h(n)은 역방향 선별의 경우 -logdiscount factor(value), 정방향 선별의 경우 logdiscount factor * c value(value)의 수식으로 정의되고, value는 현재 노드에서의 가중치 값으로서, 인공신경망을 통해 얻을 수 있다. 즉, 가중치 값은, 현재 노드에서 각 컨테이너들이 갖는 출고 순서인 상태를 입력으로 한 인공신경망의 출력으로 계산되며, 그 값은 0과 1 사이의 범위를 가질 수 있으며, 가중치 값을 구하고자 하는 상태가 선별 완료된 터미 널 상태인 경우 인공 신경망을 사용하지 않고 그 가중치를 1.0으로 결정할 수 있다. 다만 정방향 선별의 경우, discount factor * c value는 1보다 작은 값이 되어야 하므로 할인계수(discount factor)가 0.8로 설정되어있는 일 실시 예에서는, c value가 1보다 크고 1.25보다는 작은 값을 가지며 루트 노 드의 상태의 가중치 값에 따라 c value의 값은 달리 할 수 있다. 좀 더 구체적으로, 정방향 선별 에피소드 생성을 위한 정책 생성 과정에서 임의의 노드의 평가함수 값을 구하는 경우 c value는 루트 노드 상태의 가중치 값이 0.6보다 작으면 \"c value=1+0.24*루트 노드 상태의 가중치 값 (value)\"의 방법으로 구할 수 있고, 루트 노드 상태의 가중치 값이 0.6보다 크거나 같은 경우 \"c value = 1.2 4\"의 방법으로 구할 수 있다. 인공신경망을 이용한 가중치 값을 구하는 방법에 대한 보다 구체적인 설명은 후술한다. 개방 노드 리스트에서 개방 노드의 선택과 선택된 노드의 자식 노드 확장으로 새롭게 생성될 노드의 상태와 이 미 생성되어 그래프를 구성하고 있는 노드들(개방 노드 리스트의 노드와 폐쇄 노드 리스트의 노드들) 중 어느 하나의 노드의 상태가 중복되는 경우, 중복되는 노드를 새롭게 생성하지 않고 중복되는 노드(개방 노드 혹은 폐 쇄 노드)에 따라 처리 방법이 달라질 수 있으며, 이미 생성되어 그래프를 구성하고 있는 노드들 중 같은 상태를 가진 노드가 없어 중복되지 않는다면 해당 노드를 상기 개방 노드 리스트에 추가할 수 있다. 예를 들어, 도 11에 도시된 바와 같이, 개방 노드 리스트에서 4번 노드를 선택한 후, 가능한 모든 행동을 실 행해 4번 노드의 자식 노드를 새롭게 생성하여 확장할 수 있는데, 이때, 새롭게 생성될(또는 생성된) 자식 노드들의 상태(스택과 티어로 나타내어 지는 출고 순번)를 기존에 존재하던 노드들의 상태들과 비교할 수 있 다. 여기서, 기존에 존재하던 노드들은 폐쇄 노드 리스트에 포함된 노드의 상태와 개방 노드 리스트에 포함된 노드 의 상태로 나눌 수 있다. 우선, 노드 확장 과정을 통해 생성될(또는 생성된) 자식 노드의 상태가 폐쇄 노드 리 스트에 있는 어느 노드의 상태와 중복되는 경우(4-1)와 생성될 자식 노드의 상태가 개방 노드 리스트에 있는 어 느 노드의 상태와 중복되는 경우(4-2), 생성될 해당 자식 노드를 생성하지 않거나, 생성된 해당 자식 노드를 제 거할 수 있고, 개방 노드 리스트의 어느 노드의 상태와 중복되는 경우(4-2) 도 12와 같이 관리하게 된다. 이후, 해당 노드를 제외한 새롭게 생성될(또는 생성된) 자식 노드의 상태와 중복되는 노드가 없는 경우, 새롭게 생성될 자식 노드를 그대로 생성하도록 하거나, 생성된 자식 노드를 제거하지 않고 그대로 두고, 개방 노 드 리스트에 추가함으로써, 새로운 노드들이 연결되어 추가된 그래프를 생성할 수 있다. 개방 노드 리스트에서 선택된 개방 노드의 확장으로 새롭게 생성될 자식 노드의 상태가 개방 노드 리스트에 포 함된 개방 노드의 상태와 동일하게 구성되는 경우(중복 되는 경우), 새롭게 생성된 자식 노드와 연결된 부모 노 드(개방 노드 리스트에서 선택된 개방 노드) 및 새롭게 생성될 자식 노드와 같은 상태를 가진 개방 노드와 연결 된 부모 노드 중, 새롭게 생성된 자식 노드와 연결된 부모 노드에서 루트 노드까지의 거리 값(g(n))과, 새롭게 생성될 노드와 같은 상태를 가진 개방 노드의 부모 노드에서 루트 노드까지의 거리 값(g(n))을 비교하여, 더 짧 은 거리 값을 갖는 부모 노드와 해당 자식 노드 간의 연결을 유지하고, 더 긴 거리 값을 갖는 부모 노드와 자식 노드 간의 연결을 끊을 수 있다. 예를 들어, 도 11에 도시된 바와 같이 개방 노드 리스트의 노드와 중복되는 노드가 발생되는 경우 거리 값 (g(n))의 비교 과정을 통해 연결되는 부모 노드를 결정(4-2)하는 방법으로, 좀 더 구체적으로는, 도 12에 도시 된 바와 같이, 5번 노드가 개방 노드 리스트에서 선택되고(a-1), 5번 노드를 확장했을 때 생성된 노드의 상태가 15번 노드의 상태와 같지만 15번 노드는 이미 12번 노드와 연결되어 있다(b-1). 그리고 15번 노드를 5번 노드를 연결했을 때 5번 노드가 거리 값(g(n))이 더 작은 값을 갖게 되므로, 15번 노드 와 12번 노드와의 연결을 끊고(c-1), 5번 노드와 15번 노드가 새롭게 연결된 그래프를 생성할 수 있으며(d-1), 이런 경우 개방 노드 리스트의 15번 노드의 평가함수 값(f(n))은 새로운 g(n)값을 이용하여 다시 계산하여 저장 하게 된다. 또한, 15번 노드가 개방 노드 리스트에서 선택되고(a-2), 15번 노드를 확장했을 때 생성된 노드의 상태가 7번 노드의 상태와 같지만 7번 노드는 이미 5번 노드와 연결되어 있으며(b-2), 7번 노드를 15번 노드와 연결했을 때 거리 값(g(n))이 더 큰 값을 갖게 되므로, 7번 노드가 5번 노드와 연결을 유지하도록 그대로 둘 수 있다. 이에 따라, 15번 노드는 7번 노드와 연결하지 않고 새롭게 연결된 그래프를 생성할 수 있다(d-2). 도 11의 처럼 선택된 개방 노드를 확장하면서 생성된 자식 노드들을 그대로 개방 노드에 추가하지 못하고 같 은 상태를 갖는 노드를 검사하는 이유는 에이 스타 알고리즘을 통한 경로 탐색과 그래프를 그려가는 과정을 동 시에 시행하기 때문이다. 이는, 정책을 생성 할 때 도 10에 도시된 바와 같이 초기에는 루트 노드만으로 시작해서 노드의 선택과 확장을 반복하며 점차 그래프의 크기를 키워가는 과정(노드의 개수를 늘려가는 과정)을 통해 확장됨으로써 새롭게 생성 된 노드의 각 상태가 이미 생성된 노드(개방 노드나 폐쇄 노드)의 상태와 같을 수 있기 때문에 같은 상태를 가 진 노드를 검사하게 된다. 이와 같은 방법으로 도 10에 도시된 바와 같이 루트 노드(R)에서 미리 결정된 횟수(예를 들어 총 30회)만큼 노 드의 선택과 확장한 경우, 루트 노드(R)에서 직접 확장된 1~6번 노드(루트 노드의 자식 노드)를 기점으로 다수 의 노드 그룹으로 분기되어 있다. 이때, 1번 노드의 경우 총 2개, 2번 노드의 경우 총 3개, 3번 노드의 경우 총 5개, 4번 노드의 경우 총 12개, 5 번 노드의 경우 총 4개, 6번 노드의 경우 총 4개로 확장되어, 1번 노드의 경우 0.066(2/30), 2번 노드의 경우 0.100(3/30), 3번 노드의 경우 0.166(5/30), 4번 노드의 경우 0.400(12/30), 5번 노드의 경우 0.133(4/30), 6 번 노드의 경우 0.133(4/30)의 정책 값(루트 노드의 각 자식 노드 그룹의 총 폐쇄 노드 수/총 확장 횟수)을 갖 게 된다. 각 정책 값은 루트 노드의 각 자식 노드의 선택될 확률을 의미하며 이러한 정책 값에 따라 정책을 실행하면 4번 노드의 상태가 선택될 확률이 높으며, 4번 노드의 상태가 정책에 따라 선택되었다고 가정하면 4번 노드의 상태 가 에피소드에서 다음 상태로 선택되는 것이며, 이는 강화학습에서 현재 상태에서 정책을 생성하고, 정책에 따 라 행동을 결정하여 다음 상태를 선택 한 것이라고 할 수 있다. 이후 새롭게 선택된 다음 상태에서 정책을 생성하기 위해 해당 상태를 루트 노드로 하여 다시 도 10과 같은 에 이 스타 알고리즘을 통한 탐색과 동시에 그래프를 그려가는 과정을 통해 정책 생성을 반복하게 되며 최종적으로 하드 상태에 도달 할 수 있다. 다만, 정해진 개수(또는 횟수)만큼 행동을 실행하여 정해진 크기(에피소드의 상태의 개수)의 에피소드 가 완성 된 경우 다시 새로운 정책을 생성하지 않을 수 있다. 또한, 정책이 생성되면 각 정책 값에 \"temperature parameter\"를 적용 할 수 있다. 이러한 방법은 각 정책 값에 약 50승을 하여 행동의 실행 확률을 다시 계산해주는 방법으로, 이러한 방법을 적용하면 본래 계산된 정책 값들 중 차이가 얼마 나지 않더라도 가장 큰 값을 가진 행동이 선택될 확률이 비약적으로 높아지게 된다. 예를 들어, 4가지 행동이 가능하고 각 정책 값이 (0.1, 0.2, 0.3, 0.4)로 계산되었다고 했을 때 \"temperature parameter\"로 50승을 적용해주면 (0.150, 0.250, 0.350, 0.450)이 되며 다시 정책 값을 계산하면, 각 정책 값은 (7.88e-31, 8.88e-16, 5.66e-07, 0.99)로 계산되어 4번째 행동이 선택 될 확률이 0.4에서 0.99로 증가하고 나 머지 행동들은 감소한 것을 확인할 수 있다. 상기와 같은 방법으로 현재 상태에서 생성된 정책에 따라 행동을 결정하고, 다음 상태를 선택하는 과정을 반복 함으로써 역방향 선별 에피소드(터미널 상태를 초기 상태로 하는 상태의 집합) 또는 정방향 선별 에피소드(하드 상태를 초기 상태로 하는 상태의 집합)를 생성할 수 있다. 이와 같은 정방향 선별 과정은 가중치 값(value)이 작은 노드에서 큰 가중치 값을 가진 노드를 찾아가는 과정이 며, 이러한 과정은 테스트 에피소드(test episode)의 생성과 동일하지만 그 에피소드의 크기(상태의 개수)는 더 작다. 이러한 정방향 선별 과정이 필요한 이유에 대하여 설명하면, 역방향 선별로 생성된 에피소드로만 활용하여 인공 신경망을 학습하는 경우 최적해가 큰 상태(작은 가중치 값을 갖는 상태)를 학습하지 못하기 때문에, 실제로 낮 은 가중치 값을 가져야 하는 상태도 높은 가중치 값으로 계산되는 경우가 종종 있다. 좋은 결과를 얻기 위해 오랜 시간이 걸리게 되는데, 정방향 선별로 생성된 에피소드는 주로 낮은 가중치 값를 갖는 상태들로 구성되어 있어 역방향 선별과 함께 사용할 경우 인공신경망의 학습이 더 빨라지게 된다. 또한, 일 실시 예에서 상기와 같은 방법으로 정책을 생성한 이유는 다음과 같다. 컨테이너 선별은 각 노드의 상 태 간의 연결이 그래프로 표현되어 에이 스타 알고리즘의 적용이 가능한데, 각 노드의 상태는 그래프 상의 각 노드가 하나씩 속성으로 갖게 된다. 그러나, 실제 컨테이너 선별 환경은 그 노드(상태)의 개수가 많고, 그래프의 구조가 시시 때때로 바뀌어 그래프 의 구조를 미리 파악하기는 매우 어렵다. 더욱이, 일 실시 예에서는 대략 1015~1039의 상태 또는 노드를 갖는 컨 테이너 선별 환경을 대상으로 하기 때문에, 전통적인 에이 스타 알고리즘을 컨테이너 선별에 적용하기가 쉽지 않다. 따라서, 일 실시 예에서는, 에이 스타 알고리즘을 심층 강화학습(deep reinforcement learning)에 적용하여 그 래프 전체의 구성이 파악 되지 않더라도 최단에 가까운 경로를 찾도록 변형된 에이 스타 알고리즘을 이용한 정 책 생성 방안을 제안한다. 또한, 각 상태의 가중치 값은, 에이 스타 알고리즘을 통해 탐색하는 경우 각 상태의 최적해를 [0, 1] 범위로 표 현한 것으로 임의의 상태의 가중치 값은 해당 상태의 최적해(자연수)와 0보다 크고 1보다 작은 할인계수(γ, discount factor)를 이용하여 가중치 값을 계산 할 수 있으며 최적해가 클수록 가중치 값은 점차 작아지게 된다. 예를 들어 이론적으로 임의의 상태의 최적해가 2인 경우 가중치 값은 γ2이고 최적해가 10인 경우 가중치 값은 γ10의 값을 갖는 것이 바람직하지만, 전체 상태의 개수가 천문학적으로 매우 크기 때문에 모든 상태의 실제 최 적해를 구하는 것은 매우 어려워 인공신경망으로 근사 값을 계산하는 것이다. 이러한 근사 값은 인공신경망의학습을 반복할 수록 정확해 진다. 컨테이너 선별 환경이 복잡해질수록 터미널 상태의 비율이 작아지기 때문에, 보상 빈도가 낮은 문제가 발생한다. 이에 역방향 선별 과정은, 터미널 상태로 시작하는 역방향 선별 과정을 거쳐 생성된 에피소드를 만들 어 각각 노드의 순서를 역으로 배치하면 터미널 상태에 도달하는 에피소드를 만들 수 있어 보상 빈도가 낮은 문 제를 해결 할 수 있다. 따라서, 역방향 선별 과정은 선별이 완료된 상태에서 점점 더 작은 가중치 값를 가진 상 태를 찾아가는 과정이라 할 수 있다. 한편, 상술한 바와 같이 인공신경망이 가중치 값을 계산하여 결정하는 방법에 대하여 상세히 설명하면 아래와 같다. 인공신경망은 간단하게 표현한다면 사실상 y=f(x)와 같은 함수의 형태가 될 수 있으며, 다양한 x의 값을 넣었을 때 각 x값에 대한 정확한 y 값을 출력하는 것이 목적이다. 예를 들어, f(x) = ax2+bx+c의 형태를 갖는 인공신경 망이 있고, [x1, y1], [x2, y2], ... [xn, yn] 와 같은 형식의 데이터를 이용하여 학습한다고 가정했을 때, 초반 에는 a, b, c가 거의 랜덤한 값을 갖기 때문에 정확한 yi의 값을 출력하지 못한다. 학습 과정에서는 학습 데이터 [xi, yi](i는 1≤i≤ 범위의 자연수)를 알고 있으므로 axi2+bxi+c의 값과 yi의 값의 차이를 줄이는 방향으로 a, b, c값을 조정하게 된다. 따라서 학습을 한다는 것은 다양한 학습 데이터를 이용해 axi2+bxi+c와 yi 의 값의 차이가 최소가 되는 a, b, c값을 구하는 것이 목표가 된다. 이와 같이, 학습이 되면 학습 데이터가 아닌 x값을 넣어도 실제 y에 가까운 값을 계산할 수 있게 된다. 모든 값 을 완전하고 정확히 계산하기는 매우 어렵지만, 데이터의 형태에 적합한 인공 신경망의 구성과 다양하고 정확한 데이터의 사용으로 인공 신경망의 계산값(ax2+bx+c)을 실제 값(y)과 매우 비슷하게 출력하는 것은 가능하다. 일 실시 예에서 적용할 수 있는 인공신경망의 구조는 다양하며, 예를 들어, 인공신경망의 한 종류인 MLP(Multi- Layer Perceptron)에서는 각 weight와 bias의 값을 수정하여 output을 계산할 수 있고, 4가지 input값과 2개의 output값이 있는 인경 신경망의 예시가 될 수 있으며, 해당 인공신경망은 f(x1, x2, x3 ,x4) = y1, y2와 같은 형 태로 표현할 수 있다. 이 외에 CNN, RNN 등 다양한 형태의 인공신경망이 있다. 일 실시 예에서, 인공신경망은 역방향 선별 에피소드를 생성하고, 정방향 선별 에피소드를 생성하기 위하여, 정 책 생성을 위한 에이 스타 알고리즘을 이용한 탐색 과정에서 에이 스타 알고리즘의 평가함수(f(n) = g(n)+h (n))를 구해야 한다. 이때, 인공신경망은 h(n)을 구하기 위한 가중치 값을 결정할 때 이용된다. 일 실시 예에 따른 인공신경망은 상술한 MLP를 대신하여 알파고 제로에서 사용되는 Resnet이라는 인공 신경망이 적용될 수 있으나, 이 밖에 다양한 형태의 인공신경망의 적용이 가능하다. 이러한 인공신경망을 y=f(x)라고 표 현했을 때, x값 즉 입력은 노드에서 각 컨테이너들에 대한 출고 순서인 상태가 되며, f(x)가 가중치 값이 된다. 이와 같이 일 실시 예에서 인공신경망은 각 노드의 상태를 이루는 컨테이너들의 출고 순서에 대한 가중치 값을 근사하기 위한 용도로 이용될 수 있다. 상술한 바와 같이 각 상태의 가중치 값은 [0,1] 범위의 값을 가질 수 있 으며, 터미널 상태에 가까울수록 1에 가까운 값을 갖는다. 이에 따라, 각 상태의 가중치 값은 터미널 상태로부터의 거리를 나타낸다고 할 수 있다. 정책 생성을 위한 에이 스타 알고리즘을 이용한 탐색 과정에서 h(n)을 계산할 때 각 노드 상태의 가중치 값이 이용될 수 있는데, 하나 의 정책을 계산할 때 여러 노드의 상태에 대한 각각의 가중치 값을 구하는데 인공신경망이 이용된다. 이때 각 노드가 터미널 상태로부터의 거리를 알 수 있는 방법이 없기 때문에 인공신경망을 이용해 근사 값을 구할 수 있 다. 도 13에 도시된 바와 같이, 역방향 선별 과정과 정방향 선별 과정을 통해 생성된 인공 신경망 학습 데이터는 [state, value] 형태를 가지며, 이는 상기 생성된 역방향 선별 에피소드와 정방향 선별 에피소드 각 상태의 'value'를 결정하는 방법으로 위와 같은 형태를 가질 수 있다. 앞서 설명된 인공 신경망 f(x)=y의 형태에서 'state'는 배열 형태를 갖는 인공 신경망의 입력 값인 x가 되며, 'value'는 입력인 x에 대해 계산해야 될 y값이 되는데, 이 때 y값인 'value'는 할인계수인 'discount factor' 와 각각의 에피소드 상에서 각 상태의 순서를 이용하여 결정될 수 있다. 후술하겠으나 역방향 선별 에피소드와정방향 선별 에피소드 각 상태의 가중치 값을 결정하는 방법은 다르다. 도 15를 참조하면, 역방향 선별 에피소드 각 상태의 가중치 값 결정 단계(S300)는, 역방향 선별 에피소드 생성 단계(S100)를 통해 생성된 역방향 선별 에피소드를 인공신경망을 학습시키기 위한 데이터로 활용하기 위하여, 역방향 선별 에피소드 각 상태의 가중치 값을 결정할 수 있다. 이때, 역방향 선별 에피소드 각 상태의 가중치 값은 γi로 정의될 수 있다. 여기서, γ는 할인계수로서 0보다 크 고 1보다 작은 (0,1) 범위의 실수 값이 될 수 있으며, i는 상기 역방향 선별 에피소드의 초기 상태인 터미널 상 태에서 0으로 시작하여 다음 상태로 이동 할 때마다 1씩 증가되는 값일 수 있으며 결정된 가중치 값인 γi는 i가 커질수록 점차 작아지게 되어 상기 역방향 선별 에피소드 생성 단계(S100)에서 생성된 역방향 선별 에피소드 각 상태의 가중치 값을 결정하는데 사용된다. 이와 같은 할인계수를 이용한 가중치 값의 계산 방법에 대하여 좀 더 구체적으로 설명하면 다음과 같다. 역방향 선별 에피소드 각 상태의 순번에 γ를 적용하여 결정할 수 있다. 역방향 선별 에피소드 각 상태의 가중 치 값을 결정하는 방법에 대한 예를 들면, 생성된 에피소드가 (s0, s1, ... sn)으로 에피소드 전체 상태의 개수 가 n+1개이고, γ를 이용하여 각 상태의 가중치 값을 결정하고자 할 때, 결정되는 각 상태의 가중치 값은 (v0= γ0 , v1=γ1, ... vn=γn)가 되며, 생성되는 학습 데이터는 ([s0, v0], [s1, v1], ... [sn, vn])가 된다. 도 16을 참조하면, 정방향 선별 에피소드 각 상태의 가중치 값 결정 단계(S400)는, 정방향 선별 에피소드 생성 단계(S200)를 통해 생성된 정방향 선별 에피소드(sh, sh-1, … sh-n)를 인공 신경망을 학습시키기 위한 데이터로 활용하기 위하여, 정방향 선별 에피소드에 대한 각 상태의 가중치 값을 계산할 수 있다. 이때, 정방향 선별 에피소드에 대한 각 상태의 가중치 값은 γh-j로 결정될 수 있다. 여기서, γ는 할인계수로서 0보다 크고 1보다 작은 (0,1) 범위의 실수 값이 될 수 있으며, j는 상기 하드 상태에서 0으로 시작하여 다음 상 태로 이동 시 1씩 증가되는 값일 수 있으며, 정방향 선별 에피소드 생성 단계(S200)에서 생성된 정방향 선별 에 피소드 각 상태의 가중치 값을 결정하는데 사용된다. 이와 같은 할인계수의 이용한 가중치 값의 계산 방법에 대하여 좀 더 구체적으로 설명하면 다음과 같다. 정방향 선별 에피소드 각 상태의 순번에 γ와 h를 적용하여 결정할 수 있는데 역방향 선별 에피소드 각 상태의 가중치 값을 결정하는 예를 들면, 생성된 에피소드가 (sh, sh-1, … sh-n)이고, γ를 이용하여 각 상태의 가중치 값을 결 정하고자 할 때, 결정되는 각 가중치 값은 (vh=γh , vh-1=γh-1, ... vh-n=γh-n)가 되며, 생성되는 학습 데이터는 ([sh, vh], [sh-1, vh-1], ... [sh-n, vh-n])가 되며 정방향 선별 에피소드 상태의 개수는 n+1개가 된다. h값은 정의된 환경에서 최적해의 최대값을 추측한 것이며, 일 실시 예에서는 h = log10(total state)로 계산하고 소수점 첫째 자리에서 반올림하여 구할 수 있으며, 여기서 total state란 정의된 환경이 가질 수 있는 전체 상 태의 개수를 뜻한다. 상기 γ를 이용하여 상기 역방향 선별 에피소드와 정방향 선별 에피소드의 각 상태의 가중치 값을 결정하여 인 공신경망을 학습 시킬 수 있는 이유는, 에이 스타 알고리즘을 통해 정책 생성을 반복하여 생성된 에피소드의 각 상태에 대한 가중치 값이 γ로 결정하는 것이 인공신경망을 이용하여 계산한 상태의 가중치 값보다 더 정확하기 때문이다. 또한 정책 생성을 위해 에이 스타 알고리즘을 시행하는 과정에서는 h(n)을 계산할 때 γ와 i를 이용할 수 없기 때문에 인공신경망을 사용하며, γ는 선별 에피소드가 생성된 이후 결정하는 것이므로 실제 선별(test)에서도 사용할 수 없다. 인공신경망으로 계산할 때보다 더 정확한 학습 데이터로 학습하면 점차 더 정확한 값을 계산할 수 있다. 여기서, 더 정확하다는 의미는 각 상태의 실제 최적해에 더 가까운 값을 계산할 수 있다는 의미한다. 즉, 인공 신경망이 학습될수록 계산된 가중치 값이 각 상태의 최적해에 가까운 값을 계산해낼 수 있으며, 인공신경망이 학습이 정확히 될수록 실제 선별에서 성공 가능성이 높아진다는 것이다. 일 실시 예에서, 가중치 값은 임의의 상태의 최적해를 [0, 1] 범위의 실수 값으로 나타냈지만 본래 강화학습의 측면에서 가중치 값인 기대 보상(expected reward)으로서의 의미도 갖고 있다. 임의의 상태에서 행동을 계속 실행해갈 경우 기대되는 보상이 있는데, 터미널 상태에서 가까운 상태의 경우 보 상을 받을 확률이 높으므로 1에 가까운 값을 가지며, 터미널 상태로부터 먼 상태의 경우, 즉 많은 수의 행동을 거쳐야만 터미널 상태에 도달 할 수 있는 상태의 경우 보상을 받을 확률이 낮으므로 0에 가까운 값을 가진다. 일 실시 예에서는, 터미널 상태에서만 보상을 받고, 패널티가 없으므로 [0, 1] 범위의 값으로 표현이 가능하다. 역방향 선별 에피소드 생성 단계(S100)와 정방향 선별 에피소드 생성 단계(S200)의 각 정책 생성 과정에서 가중 치 값이 갖는 의미를 그래프 상에서 설명하면, 예를 들어 그래프 상에서 목표 노드(target node)가 하나가 있고 모든 엣지들의 가중치 값이 1로 같은 값을 갖는 경우 도 16에 도시된 바와 같이 표현될 수 있다. 각각의 노드가 갖는 가중치 값은 목표 노드로부터의 거리를 나타낸다. 그러나, 인공신경망을 사용하는 경우 특 정 노드의 가중치 값을 자연수로 출력하는 것보다 0~1 사이의 값을 출력하는 것이 더 효과적이므로, 할인계수 (γ, 범위: [0, 1])를 적용하여 목표 노드와 가까운 노드일수록 가중치 값이 1에 가까운 값을 갖고 거리가 멀수 록 0에 가까운 값을 갖도록 하였다. 자연수로 각 노드의 가중치 값이 표시된 도 16에서 각 노드의 가중치 값은 목표 노드로의 최적해의 크기라고 할 수 있고, 일 실시 예에서는, 목표 노드가 터미널 상태라고 할 수 있다. 각 노드의 최적해의 값을 정확히 알 수 있으면 복잡한 경로 탐색 알고리즘을 사용할 필요가 없지만 노드의 개수 가 많고 그래프의 구조의 파악이 쉽지 않아 최적해는 계산하기가 매우 어렵기 때문에 인공신경망을 이용하여 최 적해의 값을 [0, 1] 범위의 값으로 추측한 것이다. 학습 데이터 증가 단계(S500)는, 역방향 선별 에피소드 생성 단계(S100), 정방향 선별 에피소드 생성 단계 (S200), 역방향 선별 에피소드 각 상태의 가중치 값 결정 단계(S300)와 정방향 선별 에피소드 각 상태의 가중치 값 결정 단계(S400)를 통한 각각의 학습 데이터가 생성된 후, 생성된 학습 데이터에 포함된 각 상태 내 스택 간 의 위치를 교환하여 동일한 상태의 가중치 값을 갖는 새로운 상태를 생성하여 데이터의 개수를 증가(augment)시 킬 수 있다. 상기한 역방향 선별 과정과 정방향 선별 과정을 통해 각각의 데이터를 생성한 후, 생성된 데이터를 증가 즉, 생 성된 상태를 좀 더 다양하게 조합하여 다른 상태를 추가적으로 생성할 수 있는데, 이렇게 추가적으로 생성되어 증가된 상태들은 원래 상태와 동일한 가중치 값을 갖지만 서로 다른 상태일 수 있다. 증가되는 상태의 개수는 스택의 개수에 따라 달라질 수 있는데, 예를 들어 3 X 3 상태의 경우 스택이 3개로 3!(=6)만큼 증폭이 가능하다. 일 실시 예에서 시행된 스택이 8개인 환경의 경우 하나의 상태를 8!(40,320)만큼 증폭이 가능하지만, 그 중 무 작위로 10,000개 정도를 선택하여 증폭 하는 것도 가능하다. 일 실시 예에서는, 예를 들어, 8개의 스택과 5개의 층(tier)과 32개의 컨테이너를 가져 전체 상태의 개수가 1.6*1039의 환경에서 역방향 및 정방향 선별 에피소드의 데이터가 생성된 후 데이터 증가 과정을 거쳐 1,000,000 이상의 데이터가 생성되면 인공신경망 학습을 시작할 수 있다. 1.6*1039의 상태를 가진 환경에서는 역방향 및 정방향 선별 에피소드의 크기는 학습 반복(training iteration) 횟수에 따라 달라질 수 있어 일정하지 않으나 서로 동일하게 설정할 수 있으며 대략 1,000,000~1,500,000개의 데이터를 생성하여 학습할 수 있다. 역방향 선별 및 정방향 선별 데이터의 크기는 동일하게 설정했으므로 전체 데이터의 크기가 학습 횟수에 따라 달라질 수 있다. 상기 생성된 학습 데이터를 이용하여 인공신경망을 학습한 후 하드 상태를 시작으로 제한된 개수의 행동으로 실 제 선별 테스트를 진행하게 된다. 실제 선별 테스트의 정책 생성 방법은 상기 정방향 선별 에피소드 생성 단계 (S200)의 방법과 동일하다. 단, 일 실시 예에서는, 선별 테스트 에피소드의 크기의 경우, 61로 크게 설정되어 60번의 행동을 취할 수 있고, 60번의 행동 내에 하드 상태에서 터미널 상태까지 도달할 수 있다면 선별 테스트에 성공했다고 판단하며 60번의 행동 내에 터미널 상태에 도달하지 못하면 선별 테스트는 실패했다고 판단한다. 또한, 정방향 선별 에피소드 생성 과정에서 정책이 생성되면 각 정책 값에 \"temperature parameter\"를 적용 할 수 있다. 이러한 방법은 각 정책 값에 약 50승을 하여 행동의 실행 확률을 다시 계산해주는 방법으로, 상술한역방향에서와 그 방법이 동일할 수 있다. 상기 선별 테스트는 하드 상태에서 시작하여 제한된 개수의 행동 이내에 터미널 상태에 도달하면 테스트를 성공 했다고 할 수 있다. 테스트가 종료 된 후 정방향 선별 에피소드 생성 단계(S100)부터 시작하여 인공 신경망의 학습 데이터 생성과 인공 신경망 학습과 선별 테스트를 다시 반복하게 된다. 이러한 인공신경망 학습 데이터 생성과 생성된 데이터로 인공 신경망의 학습 그리고 선별 테스트를 하나의 학습 반복이라고 하며, 일 실시 예에서는, 1.6*1039 환경의 선별 테스트에서 행동 횟수 제한을 60으로 할 경우 20번의 학습 반복 횟수 이내에 선별 테스트를 통과 할 수 있는 것으로 판단될 수 있다. 도 18은 일 실시 예에 따른 다익스트라 알고리즘 기반 트리 구조의 정책 생성 과정을 설명하기 위한 흐름도이고, 도 19는 일 실시 예에 따른 다익스트라 알고리즘 기반 그래프 구조의 정책 생성 과정을 설명하기 위한 흐름도이며, 도 20은 일 실시 예에 따른 MCTS 기반 정책 생성 과정을 설명하기 위한 흐름도이다. 이하에서는, 도 18 내지 도 20을 참조하여, 다익스트라 알고리즘 및 MCTS 기반의 정책 생성 방법의 실시 예에 대해 설명한다. 도 18 및 도 19를 참조하여 다익스크라 알고리즘 기반 정책 생성에 대해 살펴보면, 프로세서는 그래프, 큐 (queue) 정보, 거리(distance) 정보, 방문(visited) 정보를 초기화하고(S41), 이동을 결정하고자 하는 노드를 루트 노드로 지정한다(S42). 그리고 프로세서는 거리 정보에 루트 노드의 거리 값인 0을 저장하고, 큐 정보에 루트 노드와 루트 노드의 거리 정보를 저장한다(S43). 이후 프로세서는 확장횟수를 0으로 초기화하고(S44), 가장 작은 거리 값을 가진 노드를 큐 정보가 포함된 큐 리스트에서 선택하고 선택한 노드는 삭제한다(S45). 그리고 프로세서는 선택된 노드가 방문 리스트에 이미 존재하는지 확인하여(S46-1), 선택된 노드가 방문 리스트에 없으면 방문 리스트에 노드를 추가한다(S46-2). 프로세서는 선택된 노드에서 이동 가능한 누드들로 이뤄진 노드 리스트를 구성하고, 인공신경망으로 노드 와 노드 리스트의 각 노드 간 엣지의 가중치를 계산한다(S47). 이후, i=0에서 자식 노드 리스트를 초기화하고(S48-1), 그래프를 구성하는 노드들 중 노드 리스트(i)와 같은 노 드가 있는지 확인하여(S48-2), 같은 노드가 없는 경우 자식 노드 리스트에 노드 리스트(i)를 속성으로 하는 노 드를 추가하고 선택된 노드와 엣지를 연결한다(S48-3). 반면 같은 노드가 있는 경우에는 같은 노드와 선택된 노 드를 엣지로 연결한다(S48-4). 그리고 프로세서는 연결된 엣지에 가중치 값을 저장하고(S48-5), 노드 리스트를 확인하는 과정을 통해 (S48-6), 자식 노드 확장을 위한 선택된 노드의 자식 노드 리스트를 구성한다(S49). 한편, S46-1단계에서 선택된 노드가 방문 리스트에 이미 존재하는 경우에는 바로 자식 노드 확장을 위한 선택된 노드의 자식 노드 리스트를 구성한다(S49). 그리고 프로세서는 다시 i=0 상태에서(S50), 노드 i는 자식 노드 리스트(i)로 저장하고(S51), 거리 정보에 노드 i의 거리가 저장되어 있는지 확인하여(S52), 거리가 저장되어 있는 경우 선택된 노드로부터 노드 i의 거리 와 루트 노드와 선택된 노드까지의 거리의 합으로 노드 i까지의 거리, dist를 구한다(S53). 이때, 거리 정보에 노드 i의 거리가 저장되어 있지 않은 경우에는 노드 i까지의 거리를 무한(infinite)로 하여 거리 정보에 저장한 다(S54). 이후 프로세서는 dist와, 거리 정보에 저장된 노드 i의 거리를 비교하여(S55), dist가 거리 정보에 저장된 노드 i의 거리보다 작으면 거리 정보에 노드 i까지의 거리를 dist로 초기화하고 큐 리스트에 노드 i와 dist에 대한 정보를 저장하고(S56), 확장 과정을 수행한 후(S57) 방문 리스트에 정해진 개수만큼의 노드가 있으면 종료 할 수 있다(S58). 한편, 도 17은 트리 구조의 다익스트라의 정책 생성 과정에 대한 것으로, 그래프 구조와 달리 S45단계를 수행한 후, 방문 정보에 선택된 노드가 이미 존재하는지 여부나 그래프를 구성하는 노드들 중 같은 노드가 있는지 확인 하는 과정 없이 선택된 노드에서 이동 가능한 노드들로 이뤄진 노드 리스트를 구성하고, 인공신경망으로 노드와 노드 리스트의 각 노드 간 엣지의 가중치를 계산할 수 있다(S47). 그리고 프로세서는 생성된 각 노드 리스트의 각 노드를 속성으로 하는 노드를 생성하고, 선택된 노드와 생 성된 각 노드 간 엣지를 연결하며, 연결된 엣지에 인공신경망으로 계산된 값을 가중치로 저장할 수 있다(S48). 이후 S49단계를 수행할 수 있다. 즉, 상술한 에이스타 알고리즘에서와 같이, 트리 구조에 대한 도 17과 달리, 그래프 구조에 대한 도 18에서는 새롭게 생성된 노드들을 기존에 존재하던 노드들과 비교하는 과정이 추가되어 있는데, 일 실시 예에서는, 이러 한 그래프 구조뿐만 아니라 트리 구조의 탐색을 수행하여 중복 노드를 확인하는 과정에 소요되는 시간과 비용을 감소시킬 수 있다. 도 20을 참조하여 MCTS 기반 정책 생성에 대해 살펴보면, 프로세서는 먼저 그래프를 초기화하고(S71), 이 동을 결정하고자 하는 노드를 루트 노드로 지정한다(S72). 그리고 프로세서는 루트 노드의 방문 횟수 (Nsa), 총 행동 값(Wsa), 상태를 선택할 확률(Psa)을 0으로 저장한다(S73). 그리고 프로세서는 i=0 상태에서(S74), 선택된 노드로 루트 노드를 선택하고, trajectory(node)를 초기화 하고 루트 노드를 추가한다(S75). 프로세서는 선택된 노드의 자식 노드 리스트가 존재하는지 여부를 확인하여(S76), 자식 노드 리스트가 존 재하는 경우, 자식 노드 리스트의 각 노드에 저장되어 있던 방문 횟수(Nsa), 총 행동 값(Wsa), 상태를 선택할 확률(Psa)을 기반으로 puct score를 계산한다(S77). 이때, 각 노드의 puct score 계산식을 다음 수학식 1과 같으며, 여기서 c는 탐색의 레벨을 결정하는 상수이고, Nsa_p는 선택된 노드의 Nsa 값이다. 수학식 1"}
{"patent_id": "10-2022-0156553", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "그리고 프로세서는 자식 노드 리스트의 각 노드의 puct score 중 가장 큰 값을 가진 노드를 선택하고 (S78), 선택된 노드를 trajectory에 추가한다(S79). 상술한 과정을 선택 노드의 자식 노드 리스트가 존재하지 않을 때까지 반복 할 수 있다. 프로세서는 S76단계에서 선택된 노드의 자식 노드 리스트가 존재하지 않는 경우, 선택된 노드의 이동 가능 한 노드들로 이뤄진 자식 노드 리스트를 구성한다(S80). 그리고 프로세서는 인공 신경망을 이용하여 선택된 노드의 가중치와 자식 노드 리스트의 각 노드 간 엣지 의 가중치를 계산한다(S81). 다음으로 프로세서는 자식 노드 리스트의 각 노드의 방문 횟수(Nsa), 총 행동 값(Wsa)을 0으로 저장하고, 인공신경망으로 계산된 각 엣지의 값을 해당 자식 노드의 선택할 확률(Psa)로 하여 저장한다(S82). 이후 프로세서는 자식 노드(j)=0 상태에서(S83), trajectory[j]의 속성을 업데이트하고(S84), 확장하는 과 정을 통해(S85, S86), i가 정해진 횟수에 도달할 때까지 반복할 수 있다(S87, S88). 한편, 일 실시 예에서는, 상태, 행동, 정책과 같이, 강화학습에서 사용되는 용어를 사용하고 있는데 이는 경로 탐색을 쉽게 설명하고자 차용한 것으로, 사용되는 다양한 경로 탐색 알고리즘에 따라 그 개념이 달라질 수 있다. 따라서 일 실시 예에서는 최적 경로 탐색을 하는데 있어 강화학습이 사용되지 않을 수도 있다(예를 들어, 경로가 구성되어 있는 내비게이션 등). 이상 설명된 본 개시에 따른 실시 예는 컴퓨터 상에서 다양한 구성요소를 통하여 실행될 수 있는 컴퓨터 프로그 램의 형태로 구현될 수 있으며, 이와 같은 컴퓨터 프로그램은 컴퓨터로 판독 가능한 매체에 기록될 수 있다. 이 때, 매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등과 같은, 프로그램 명령어를 저장하고 실행하도록 특별히 구성된 하드웨어 장치를 포함할 수 있다.한편, 상기 컴퓨터 프로그램은 본 개시를 위하여 특별히 설계되고 구성된 것이거나 컴퓨터 소프트웨어 분야의 통상의 기술자에게 공지되어 사용 가능한 것일 수 있다. 컴퓨터 프로그램의 예에는, 컴파일러에 의하여 만들어 지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용하여 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함될 수 있다. 본 개시의 명세서(특히 특허청구범위에서)에서 \"상기\"의 용어 및 이와 유사한 지시 용어의 사용은 단수 및 복수 모두에 해당하는 것일 수 있다. 또한, 본 개시에서 범위(range)를 기재한 경우 상기 범위에 속하는 개별적인 값 을 적용한 발명을 포함하는 것으로서(이에 반하는 기재가 없다면), 발명의 상세한 설명에 상기 범위를 구성하는 각 개별적인 값을 기재한 것과 같다. 본 개시에 따른 방법을 구성하는 단계들에 대하여 명백하게 순서를 기재하거나 반하는 기재가 없다면, 상기 단 계들은 적당한 순서로 행해질 수 있다. 반드시 상기 단계들의 기재 순서에 따라 본 개시가 한정되는 것은 아니 다. 본 개시에서 모든 예들 또는 예시적인 용어(예들 들어, 등등)의 사용은 단순히 본 개시를 상세히 설명하기 위한 것으로서 특허청구범위에 의해 한정되지 않는 이상 상기 예들 또는 예시적인 용어로 인해 본 개시의 범위 가 한정되는 것은 아니다. 또한, 통상의 기술자는 다양한 수정, 조합 및 변경이 부가된 특허청구범위 또는 그 균등물의 범주 내에서 설계 조건 및 팩터에 따라 구성될 수 있음을 알 수 있다. 따라서, 본 개시의 사상은 상기 설명된 실시 예에 국한되어 정해져서는 아니 되며, 후술하는 특허청구범위뿐만 아니라 이 특허청구범위와 균등한 또는 이로부터 등가적으로 변경된 모든 범위는 본 개시의 사상의 범주에 속한 다고 할 것이다."}
{"patent_id": "10-2022-0156553", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시 예에 따른 최적 경로 탐색 시스템 환경을 개략적으로 나타낸 도면이다. 도 2는 일 실시 예에 따른 최적 경로 탐색 시스템을 설명하기 위해 개략적으로 나타낸 도면이다. 도 3은 일 실시 예에 따른 최적 경로 탐색 장치를 개략적으로 나타낸 블록도이다. 도 4는 일 실시 예에 따른 물류 관리 태스크의 각 상태 간의 연결 관계가 그래프로 표현됨을 설명하기 위한 도 면이다. 도 5는 일 실시 예에 따른 다양한 하역(material handling) 에 대한 상태를 설명하기 위한 예시도이다. 도 6은 일 실시 예에 따른 강화학습 적용을 위한 인공신경망의 학습 데이터 생성 방법을 설명하기 위한 흐름도 이다. 도 7은 일 실시 예에 따른 정책 생성 기반 최적 경로 탐색 방법을 설명하기 위한 흐름도이다. 도 8은 일 실시 예에 따른 에이 스타 알고리즘 기반 트리 구조의 정책 생성 과정을 설명하기 위한 흐름도이다. 도 9는 일 실시 예에 따른 에이 스타 알고리즘 기반 그래프 구조의 정책 생성 과정을 설명하기 위한 흐름도이다. 도 10 내지 도 12는 일 실시 예에 따른 정책 생성을 설명하기 위한 도면이다. 도 13은 일 실시 예에 따른 역방향 및 정방향 선별 에피소드를 통해 생성된 데이터의 일례를 그래프 상에 나타 낸 도면이다. 도 14는 일 실시 예에 따른 평가함수 산출 방법을 설명하기 위한 도면이다. 도 15는 일 실시 예에 따른 역방향 선별 에피소드를 인공신경망의 학습 데이터로서 활용하기 위한 가중치 값을 설명하기 위한 도면이다. 도 16은 일 실시 예에 따른 정방향 선별 에피소드를 인공신경망의 학습 데이터로서 활용하기 위한 가중치 값을 설명하기 위한 도면이다. 도 17은 일 실시 예에 따른 에이 스타 알고리즘에서 각 노드의 가중치 값이 갖는 의미를 설명하기 위한 도면이 다. 도 18은 일 실시 예에 따른 다익스트라 알고리즘 기반 트리 구조의 정책 생성 과정을 설명하기 위한흐름도이다. 도 19는 일 실시 예에 따른 다익스트라 알고리즘 기반 그래프 구조의 정책 생성 과정을 설명하기 위한 흐름도이 다. 도 20은 일 실시 예에 따른 MCTS 기반 정책 생성 과정을 설명하기 위한 흐름도이다."}
