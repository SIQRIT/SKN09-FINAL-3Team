{"patent_id": "10-2022-0111720", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0032569", "출원번호": "10-2022-0111720", "발명의 명칭": "신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치", "출원인": "한양대학교 산학협력단", "발명자": "김광욱"}}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "미리 설정된 시간 동안 사용자의 움직임을 감지하여 상기 사용자의 신체 움직임 정보를 생성하는 센서부;상기 신체 움직임 정보를 기초로 미리 설정된 관절 항목에 따라 상기 사용자의 관절 별 움직임 정보를산출하고, 상기 관절 별 움직임 정보를 기초로 미리 설정된 감정 항목에 따라 상기 사용자의 감정 별 움직임 정보를 산출하는 감정 정보 생성부; 및상기 관절 별 움직임 정보 및 상기 감정 별 움직임 정보를 기초로 상기 사용자의 바디 모션 맵(BMMs) 정보를 생성하는 바디 모션 맵 정보 생성부;를 포함하는 것을 특징으로 하는,신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 바디 모션 맵 정보는,상기 감정 항목에 대해 상기 사용자의 관절 별 움직임 정도에 대한 정보가 시각화 되어 있는 정보를 포함하는것을 특징으로 하는, 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 바디 모션 맵 정보 생성부는,상기 감정 항목별로 상기 사용자의 관절의 평균 움직임 정보를 산출한 후, 상기 감정 별 움직임 정보에서 상기평균 움직임 정보를 차감한 정보를 기초로 상기 사용자의 바디 모션 맵 정보를 생성하는 것을 특징으로 하는,신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 감정 정보 생성부는,기 생성된, 상기 감정 항목마다 복수의 사용자가 평균적으로 움직이는 관절 별 정보를 기초로, 상기 사용자의감정 별 움직임 정보를 산출하는 것을 특징으로 하는, 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 미리 설정된 관절 항목은,골반, 아래 허리, 위 허리, 목, 머리, 왼쪽 어깨, 왼쪽 팔, 왼쪽 팔뚝, 왼쪽 손, 오른쪽 어깨, 오른쪽 팔, 오른쪽 팔뚝, 오른쪽 손, 오른쪽 다리, 오른쪽 종아리, 오른쪽 발, 왼쪽 다리, 왼쪽 종아리 및 왼쪽 발 중 적어도하나를 포함하는 것을 특징으로 하는, 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치.공개특허 10-2024-0032569-3-청구항 6 제1항에 있어서,상기 미리 설정된 감정 항목은,행복, 슬픔, 놀람, 화남, 역겨움, 공포 및 중립 중 적어도 하나를 포함하는 것을 특징으로 하는, 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "미리 설정된 시간 동안 사용자의 움직임을 감지하여 상기 사용자의 신체 움직임 정보를 생성하는 센서부;상기 움직임 정보를 입력 정보로 하고, 상기 움직임 정보에 대응되는 상기 사용자의 감정 정보를 출력하는 기학습된, 감정 판단 인공신경망 모듈;을 포함하고,상기 감정 판단 인공신경망 모듈은, 상기 움짐익 정보를 입력 정보로 하고, 상기 움직임 정보에 대응되는 제1특징 맵을 출력 정보로 출력하는 제1인공신경망;상기 제1특징 맵을 입력 정보로 하고, 상기 제1특징 맵에 대응되는 제2특징 맵을 출력 정보로 출력하는 제2인공신경망; 및상기 제2특징 맵을 입력 정보로 하고, 제2특징 맵에 대응되는 감정 정보를 출력 정보로 출력하는제3인공신경망;을 포함하는 것을 특징으로 하는,신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 제1인공신경망은,복수 개의 컨볼루션 레이어(Convolution Layer)와 복수 개의 폴링 레이어(Pooling Layer)를 포함하며, 상기 움직임 정보에 포함되어 있는 특징(feature)을 입력된 차원보다 더 작은 차원으로 변환한 제1특징 맵(featuremap)을 출력 정보로 출력하는 것을 특징으로 하는,신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서, 상기 제2인공신경망은,복수 개의 컨볼루션 레이어(Convolution Layer)를 포함하는 컨볼루션 네트워크를 복수 개 포함하며, 상기 제1특징 맵에 포함되어 있지 않은 다른 특징을 추출한 제2특징 맵을 출력 정보로 출력하는 것을 특징으로 하는, 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서,상기 제3인공신경망은,복수 개의 컨볼루션 레이어(Convolution Layer)와 복수 개의 폴링 레이어(Pooling Layer)를 포함하며, 상기 제2특징 맵을 기초로 상기 신체 움직임 정보에 대응되는 감정 정보를 출력 정보로 출력하는 것을 특징으로 하는, 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2024-0032569-4-미리 설정된 시간 동안 사용자의 움직임을 감지하여 상기 사용자의 신체 움직임 정보를 생성하는 센서부;상기 신체 움직임 정보를 기초로 미리 설정된 관절 항목에 따라 상기 사용자의 관절 별 움직임 정보를산출하고, 상기 관절 별 움직임 정보를 기초로 미리 설정된 감정 항목에 따라 상기 사용자의 감정 별 움직임 정보를 산출하는 감정 정보 생성부; 상기 관절 별 움직임 정보 및 상기 감정 별 움직임 정보를 기초로 상기 사용자의 바디 모션 맵(BMMs) 정보를 생성하는 바디 모션 맵 정보 생성부; 및상기 움직임 정보를 입력 정보로 하고, 상기 움직임 정보에 대응되는 상기 사용자의 감정 정보를 출력하는 기학습된, 감정 판단 인공신경망 모듈;을 포함하는, 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 바디 모션 맵 정보는,상기 감정 항목에 대해 상기 사용자의 관절 별 움직임 정도에 대한 정보가 시각화 되어 있는 정보를 포함하는것을 특징으로 하는, 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 바디 모션 맵 정보는,복수의 사용자에 대한 실험 결과를 기초로 상기 감정 항목에 대해 상기 사용자의 관절 별 움직임 정도에 대한정보가 수치화 되어 있는 정보를 포함하는 것을 특징으로 하는, 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 감정 판단 인공신경망 모듈이 출력하는 상기 사용자의 감정 정보 및 상기 바디 모션 맵 정보를 기초로 상기 사용자의 감정의 정도를 수치적으로 표현한 상기 사용자의 감정 정도 정보를 산출하는 감정 강도 산출부;를더 포함하는, 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치."}
{"patent_id": "10-2022-0111720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 감정 정도 산출부는,상기 바디 모션 맵 정보에 포함되어 있는 상기 감정 항목에 대한 상기 사용자의 관절 별 움직임 정도에 대한 정보를 기초로, 상기 사용자의 감정의 정도를 산출하는,"}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "일 실시예에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는 미리 설정된 시간 동안 사용 자의 움직임을 감지하여 상기 사용자의 신체 움직임 정보를 생성하는 센서부, 상기 신체 움직임 정보를 기초로 미리 설정된 관절 항목에 따라 상기 사용자의 관절 별 움직임 정보를 산출하고, 상기 관절 별 움직임 정보를 기 초로 미리 설정된 감정 항목에 따라 상기 사용자의 감정 별 움직임 정보를 산출하는 감정 정보 생성부 및 상기 관절 별 움직임 정보 및 상기 감정 별 움직임 정보를 기초로 상기 사용자의 바디 모션 맵(BMMs) 정보를 생성하는 바디 모션 맵 정보 생성부를 포함할 수 있다."}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치에 관한 발명으로서, 보다 상세하게는 입력된 사용자의 움직임 정보 및 기 학습된 데이터를 이용하여 실시간으로 사용자의 감정 정보를 판단하는 기술 에 관한 발명이다."}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간의 움직임 행동은 인간을 이해하기 위한 비언어적 의사소통의 매개체로써 문화산업, 정신의학, 뇌 신경과학 분야에서 활용되고 있다. 움직임 행동과 관련된 연구 개발은 주로 문화산업 중 엔터테인먼트 산업(영화, 애니메 이션) 위주로 이루어지고 있는데, 모션 캡쳐 기반의 특수 영상 제작과 관련된 연구개발이 주로 이루어졌다. 그 에 반해 움직임을 통한 인간 감성의 이해나 정서 분석이 체계적으로 이루어지지 않고 있다. 예를 들어, 아동의 주의산만 행동을 검사하는 경우 주로 설문지에 의한 검사, 부모 면담, 혈액검사, 뇌기능 검 사 등으로 이루어져, 비언어적 의사소통 수단인 움직임을 통한 분석은 이루어지지 않았다. 움직임을 관찰하는 경우에도 전문 상담사가 아동의 행동을 계속 지켜보는 방식으로 수행되어, 표준화된 방식으로 움직임 특성을 판 단하지 못하고 이상행동 판단 기준도 정확히 적용되지 못하는 문제가 있었다. 이를 보완하기 위해 설문지 기반의 감정 측정 방법이 제안되고 있는데, 사람의 감정 상태는 심혈관계, 근골격계, 신경내분비계 등의 다양한 기관이 영향을 주는데 설문지 기반의 감정 측정 방법은 이를 자세히 반영 하지 못하는 단점이 있다. 구체적으로. 연구 결과에 따르면 사람의 감정은 사람의 신체 감각(Sensation) 뿐만 아니라 신체 움직임(Body Motion)을 통해서도 감정 별 특징이 드러나는 것으로 보고되어 있는데, 설문지 기반의 측정은 개인의 의식적인 감각에 기반하기 때문에, 무의식적으로 표현되는 신체 움직임을 측정할 수 없다는 문제점을 가지고 있다. 또한, 메타버스(Metaverse) 시대가 도래함에 따라, 실시간으로 사용자의 감정을 판단해야 하는 기술이 요구되고 있는데, 기존의 설문 기반 감정 측정 방법은 사용자의 의식적인 보고가 필요하기 때문에 실시간으로 시스템에 반영할 수 없는 문제점이 존재하며, 감정상태를 기반으로 하는 어플리케이션을 통해 자가 보고를 하는 경우, 자 가 보고 행위 자체가 감정상태에 부정적인 영향을 줄 수 있는 단점이 존재한다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 공개 특허 제10-2020-0017797 호 - 사용자의 감정을 판단하기 위한 방법 및 이를 위 한 장치 (2020.02.19.) (특허문헌 0002) 대한민국 공개 특허 제10-2018-0000026호 - 특징점을 이용한 감정 판단 방법 (2018.01.02.)"}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "따라서, 일 실시예에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는, 상기 설명한 문제 점을 해결하기 위해 고안된 발명으로서, 실시간으로 입력되는 사용자의 신체 움직임 정보를 기초로 사용자의 감 정 정보를 판단하는 기술에 관한 발명이다. 보다 구체적으로, 사용자의 신체 움직임 정보를 기초로 사용자의 감정 정보를 실시간으로 정확히 판단하여, 사 용자의 감정 정보가 실시간으로 필요로 하는 원격 통신, 보안, 교육, 의료 및 게임 등의 분야에 본 기술이 적용 되어, 각종 서비스의 기능을 향상시키는 것에 목적이 있다."}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는 미리 설정된 시간 동안 사 용자의 움직임을 감지하여 상기 사용자의 신체 움직임 정보를 생성하는 센서부, 상기 신체 움직임 정보를 기초 로 미리 설정된 관절 항목에 따라 상기 사용자의 관절 별 움직임 정보를 산출하고, 상기 관절 별 움직임 정보를 기초로 미리 설정된 감정 항목에 따라 상기 사용자의 감정 별 움직임 정보를 산출하는 감정 정보 생성부 및 상 기 관절 별 움직임 정보 및 상기 감정 별 움직임 정보를 기초로 상기 사용자의 바디 모션 맵(BMMs) 정보를 생성 하는 바디 모션 맵 정보 생성부를 포함할 수 있다. 상기 바디 모션 맵 정보는 상기 감정 항목에 대해 상기 사용자의 관절 별 움직임 정도에 대한 정보가 시각화 되 어 있는 정보를 포함할 수 있다. 상기 바디 모션 맵 정보 생성부는, 상기 감정 항목별로 상기 사용자의 관절의 평균 움직임 정보를 산출한 후, 상기 감정 별 움직임 정보에서 상기 평균 움직임 정보를 차감한 정보를 기초로 상기 사용자의 바디 모션 맵 정 보를 생성할 수 있다. 상기 감정 정보 생성부는, 기 생성된, 상기 감정 항목마다 복수의 사용자가 평균적으로 움직이는 관절 별 정보 를 기초로, 상기 사용자의 감정 별 움직임 정보를 산출할 수 있다. 상기 미리 설정된 관절 항목은, 골반, 아래 허리, 위 허리, 목, 머리, 왼쪽 어깨, 왼쪽 팔, 왼쪽 팔뚝, 왼쪽 손, 오른쪽 어깨, 오른쪽 팔, 오른쪽 팔뚝, 오른쪽 손, 오른쪽 다리, 오른쪽 종아리, 오른쪽 발, 왼쪽 다리, 왼 쪽 종아리 및 왼쪽 발 중 적어도 하나를 포함할 수 있다. 상기 미리 설정된 감정 항목은, 행복, 슬픔, 놀람, 화남, 역겨움, 공포 및 중립 중 적어도 하나를 포함할 수 있 다. 다른 실시예에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는, 미리 설정된 시간 동안 사용자의 움직임을 감지하여 상기 사용자의 신체 움직임 정보를 생성하는 센서부, 상기 움직임 정보를 입력 정 보로 하고, 상기 움직임 정보에 대응되는 상기 사용자의 감정 정보를 출력하는 기 학습된, 감정 판단 인공신경 망 모듈을 포함하고, 상기 감정 판단 인공신경망 모듈은, 상기 움짐익 정보를 입력 정보로 하고, 상기 움직임 정보에 대응되는 제1특징 맵을 출력 정보로 출력하는 제1인공신경망, 상기 제1특징 맵을 입력 정보로 하고, 상 기 제1특징 맵에 대응되는 제2특징 맵을 출력 정보로 출력하는 제2인공신경망 및 상기 제2특징 맵을 입력 정보 로 하고, 제2특징 맵에 대응되는 감정 정보를 출력 정보로 출력하는 제3인공신경망을 포함할 수 있다. 상기 제1인공신경망은, 복수 개의 컨볼루션 레이어(Convolution Layer)와 복수 개의 폴링 레이어(Pooling Layer)를 포함하며, 상기 움직임 정보에 포함되어 있는 특징(feature)을 입력된 차원보다 더 작은 차원으로 변 환한 제1특징 맵(feature map)을 출력 정보로 출력할 수 있다. 상기 제2인공신경망은, 복수 개의 컨볼루션 레이어(Convolution Layer)를 포함하는 컨볼루션 네트워크를 복수 개 포함하며, 상기 제1특징 맵에 포함되어 있지 않은 다른 특징을 추출한 제2특징 맵을 출력 정보로 출력할 수 있다. 상기 제3인공신경망은, 복수 개의 컨볼루션 레이어(Convolution Layer)와 복수 개의 폴링 레이어(Pooling Layer)를 포함하며, 상기 제2특징 맵을 기초로 상기 신체 움직임 정보에 대응되는 감정 정보를 출력 정보로 출 력할 수 있다. 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는, 미리 설정된 시간 동안 사용자의 움직임을 감지하여 상기 사용자의 신체 움직임 정보를 생성하는 센서부, 상기 신체 움직임 정보를 기초로 미리 설정된 관 절 항목에 따라 상기 사용자의 관절 별 움직임 정보를 산출하고, 상기 관절 별 움직임 정보를 기초로 미리 설정 된 감정 항목에 따라 상기 사용자의 감정 별 움직임 정보를 산출하는 감정 정보 생성부, 상기 관절 별 움직임 정보 및 상기 감정 별 움직임 정보를 기초로 상기 사용자의 바디 모션 맵(BMMs) 정보를 생성하는 바디 모션 맵 정보 생성부 및 상기 움직임 정보를 입력 정보로 하고, 상기 움직임 정보에 대응되는 상기 사용자의 감정 정보 를 출력하는 기 학습된, 감정 판단 인공신경망 모듈을 포함할 수 있다. 상기 바디 모션 맵 정보는, 상기 감정 항목에 대해 상기 사용자의 관절 별 움직임 정도에 대한 정보가 시각화 되어 있는 정보를 포함할 수 있다. 상기 바디 모션 맵 정보는, 복수의 사용자에 대한 실험 결과를 기초로 상기 감정 항목에 대해 상기 사용자의 관 절 별 움직임 정도에 대한 정보가 수치화 되어 있는 정보를 포함할 수 있다. 상기 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는, 상기 감정 판단 인공신경망 모듈이 출 력하는 상기 사용자의 감정 정보 및 상기 바디 모션 맵 정보를 기초로 상기 사용자의 감정의 정도를 수치적으로 표현한 상기 사용자의 감정 정도 정보를 산출하는 감정 강도 산출부를 더 포함할 수 있다. 상기 감정 정도 산출부는, 상기 바디 모션 맵 정보에 포함되어 있는 상기 감정 항목에 대한 상기 사용자의 관절 별 움직임 정도에 대한 정보를 기초로, 상기 사용자의 감정의 정도를 산출할 수 있다."}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는, 특정 감정 항목에 대한 사 용자의 관절 별 움직임 정보를 기초로 사용자의 감정 정보를 판단하기 때문에 종래 기술보다 정확히 사용자의감정 정보를 판단할 수 있다. 또한, 본 발명에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는 감정 항목에 대한 사용 자의 관절 별 움직임 정보를 기초로 생성한 바디 모션 맵 정보(BMMs) 정보를 이용하여 인공신경망을 학습하고 추론하기 때문에, 종래 기술에 비해 정확성을 높이면서 동시에 유연하게 감정 상태를 판단할 수 있는 장점이 존 재한다. 또한, 본 발명에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는 사용자의 신체 움직임 정보를 기초로 사용자의 감정 정보를 실시간으로 정확히 판단할 수 있기 때문에, 사용자의 감정 정보를 실시간 으로 필요로 하는 원격 통신, 보안, 교육, 의료 및 게임 등의 분야에 본 기술이 적용되어, 각종 서비스의 기능 을 향상시킬 수 있는 장점이 존재한다."}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래 의 기재들로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명에 따른 실시 예들은 첨부된 도면들을 참조하여 설명한다. 각 도면의 구성요소들에 참조 부호를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 부호를 가 지도록 하고 있음에 유의해야 한다. 또한, 본 발명의 실시 예를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 발명의 실시예에 대한 이해를 방해한다고 판단되는 경우에는 그 상세한 설명은 생략한 다. 또한, 이하에서 본 발명의 실시 예들을 설명할 것이나, 본 발명의 기술적 사상은 이에 한정되거나 제한되지 않고 당업자에 의해 변형되어 다양하게 실시될 수 있다. 또한, 본 명세서에서 사용한 용어는 실시 예를 설명하기 위해 사용된 것으로, 개시된 발명을 제한 및/또는 한정 하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\", \"구비하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들 이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는다. 또한, 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"간접적으로 연결\"되어 있는 경우도 포함하며, 본 명세서에 서 사용한 \"제 1\", \"제 2\" 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지 만, 상기 구성 요소들은 상기 용어들에 의해 한정되지는 않는다. 아래에서는 첨부한 도면을 참고하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략한다. 또한, 설명의 편의를 위해 안구 기반 개인화 헤드 마운티드 디스플레이 장치 는 헤드 마운티드 디스플레이 장치로 표현하여 설명한다. 한편, 본 발명의 명칭은 '신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 방법 및 장치'로 기재하였 으나, 이하 설명의 편의를 위해 '신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치'는 '사용자 감 정 판단 장치'로 축약 지칭하여 설명하도록 한다. 도 1은 본 발명의 일 실시예에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치의 일부 구 성 요소를 도시한 블럭도이다. 도 1을 참조하면, 사용자 감정 판단 장치는 센서부, 통신부, 메모리, 출력부 및 프로 세서를 포함할 수 있다. 다만 상술한 모든 구성요소가 본 발명의 일 실시 예에 따른 움직임 코드 구축 방 법을 수행할 때 필수적으로 필요한 것은 아니며, 상술한 구성요소 이외에도 다양한 구성요소들이 추가적으로 포 함될 수 있다. 센서부는 사용자의 움직임을 다양한 센서 장치를 이용하여 감지할 수 있다. 구체적으로, 센서부는 이미지 센서, 움직임 센서, 카메라, CCD(Charge Coupled Device) 소자, CMOS(Complementary Metal Oxide Semiconductor) 소자, 키넥트, 마이크 등의 다양한 장치로 구현되어 대상의 신체, 얼굴, 시선, 소리, 움직임 등을 검출할 수 있다. 통신부는 유무선 네트워크를 이용하여 서버 등의 외부 장치와 통신할 수 있다. 예를 들어, 통신부는 센서부가 감지한 정보를 프로세서로 송신하거나, 프로세서가 생성한 각종 정보를 외부 서버 등으로 송출할 수 있다. 따라서, 통신부는 무선통신 방식으로 블루투스, Zigbee 통 신, WiFi, 적외선(InfraRed, IR) 통신, NFC(Near Field Communication) 등 다양한 방식을 이용할 수 있다. 또 한, 통신부는 유선통신 방식으로 HDMI(High Definition Multimedia Interface), LVDS(Low Voltage Differential Signaling), LAN(Local Area Network) 등을 이용할 수 있다. 메모리는 사용자 감정 판단 장치를 구동하기 위한 다양한 모듈, 소프트웨어, 데이터를 저장할 수 있 다. 예를 들어, 메모리에는 이미지 센서에게 감지된 영상을 분석하기 위한 소프트웨어, 영상 분석 결과를 움직임 코드로 변환하기 위한 소프트웨어, 구축된 움직임 코드 데이터, 센서부에서 획득한 데이터(영상, 음성 등), 외부 장치와의 통신 프로토콜과 같은 제어신호 데이터 등이 저장될 수 있다. 또한, 메모리에는 후술할 바디 모션 맵 정보 생성부가 사용자의 바디 모션 맵 정보를 생성하기 위해, 복수의 사용자들에 대해 감정별 관절의 활성화 또는 비활성화된 정도가 정리되어 있는 정보들이 저장될 수 있으 며, 이러한 정보를 기초로 바디 모션 맵 정보 생성부가 생성한 바디 모션 맵 정보가 저장될 수 있다. 또한, 후술할 감정 판단 인공신경망 모듈이 학습하기 위한 각종 레퍼런스 데이터가 저장될 수 도 있다. 따라서, 메모리는 플래쉬 메모리, HDD(Hard Disk Drive), SSD(Solid State Drive) 등의 형태로 구현될 수 있다. 예를 들어, 메모리는 사용자 감정 판단 장치의 동작 수행을 위한 프로그램 및/또는 어플리케이 션을 저장하기 위한 ROM, 사용자 감정 판단 장치의 동작 수행에 따른 데이터를 일시적으로 저장하기 위한 RAM을 구비할 수 있다. 또한, 메모리는 각종 참조 데이터를 저장하기 위한 EEPROM(Electrically Erasable and Programmable ROM) 등을 더 구비할 수 있다. 출력부는 프로세서가 생성한 사용자의 감정 정보를 디스플레이 또는 스피커를 통해 출력하거나, 사용 자의 특정 움직임을 유도하기 위한 자극 신호, 대상의 움직임을 자연스럽게 유도할 수 있는 콘텐츠 등을 출력할 수 있다. 출력부는 영상을 출력하는 디스플레이, 음성을 출력하는 스피커, 점멸 신호를 출력하는 LED 광원, 가상현실(VR, AR 및 MR을 포함) 출력 장치 등 다양한 형태로 구현될 수 있다.프로세서는 사용자 감정 판단 장치의 상술한 구성들을 전반적으로 제어할 수 있다. 예를 들어, 프로 세서는 외부 장치나 외부 서버에 센서부를 통해 수신한 정보 또는 프로세서가 생성한 각종 정보 를 전송하도록 통신부를 제어할 수 있다. 또한, 프로세서는 사용자의 행동 특성 또는 감정을 분석하 기 위해, 사용자의 움직임을 유도하기 위한 콘텐츠나 자극 신호를 출력하도록 출력부를 제어할 수 있다. 프로세서는 하나 또는 복수의 하드웨어 칩 형태로 제작되어 사용자 감정 판단 장치에 탑재될 수 있다. 예를 들어, 프로세서는 범용 프로세서(예를 들어, CPU 또는 Application Processor)로 제작될 수도 있고, 인공지능을 위한 전용 하드웨어 칩 형태로 제작될 수도 있다. 프로세서는 센서부를 구성하는 적어도 하나의 장치에서 수신된 데이터를 통합하거나 전처리할 수 있 다. 예를 들어, 프로세서는 센서 퓨전 기술을 적용하여 카메라, 이미지 센서, 적외선 센서, UWB 센서, 레 이더 센서 등에서 감지한 대상의 움직임 영상을 통합할 수 있다. 프로세서는 다양한 센서에서 측정된 데이 터를 시계열적으로 동기화하는 전처리 과정을 수행할 수 있다. 또한, 프로세서는 센서부를 통해 수신 된 데이터에 대해 노이즈 제거 등의 전처리를 수행할 수 있다. 그리고 프로세서는 전처리된 데이터를 이용하여 행동 및 감성을 인지할 수 있다. 예를 들어, 프로세서 는 이미지 센서 등으로 검출된 신체의 관절 움직임을 분석하여 대상의 행동을 인식할 수 있다. 프로세서 는 관절 움직임을 분석하는데 통계적인 방법을 사용할 수 있다. 또한, 프로세서는 이미지 센서 등으로 검출된 얼굴과 시선 방향을 통해 대상의 표정을 인식할 수 도 있다. 프로세서는 표정을 인식하는데 3차원 POSIT(Pose for Orthography and Scaling with Iterations) 및/또는 ASM(Active Shape Model) 등을 사용할 수 있다. 그리고 프로세서는 마이크 등으로 검출된 대상의 음성을 인식할 수 있다. 프로세서는 음성 신호로부터 대상의 발화가 포함된 음성 구간과 발화가 포함되지 않은 음 성 구간을 검출하는 방식으로 인식 정확도를 높일 수 있다. 이와 같이 프로세서는 다양한 센싱 정보를 취득하고, 취득한 다양한 정보를 기초로 사용자의 행동 및/또는 감정을 판단할 수 있다. 도 2는 본 발명의 일 실시예에 따른, 프로세서의 일부 구성 요소를 도시한 블럭도이며 도 3은 본 발명의 일 실 시예에 따른 프로세스의 작동 순서를 도시한 순서도이다. 도 4는 본 발명의 일 실시예에 따라 미리 설정된 사용 자의 관절 항목을 도시한 도면이고, 도 5는 본 발명의 일 실시예에 기 생성된, 미리 설정된 감정 항목마다 복수 의 사용자가 평균적으로 움직이는 관절 별 정보를 도식화한 도면이며, 도 6은 본 발명의 일 실시예에 따른 사용 자의 감정 별 관절 움직임 정보를 생성하는 방법을 설명하기 위한 도면이다. 프로세서는 감정 정보 생성부, 바디 모션 맵 정보(Body Motion Maps, BMMs, 220) 생성부, 감정 강도 산출부 및 감정 판단 인공신경망 모듈을 포함할 수 있다. 한편, 도 2에서서는 감정 판단 인공신경망 모듈을 프로세서의 일 구성 요소로 도시하였지만, 이로 본 발명의 실시예가 한정되는 것은 아니고, 감정 판단 인공신경망 모듈은 프로세서와 별도의 구성 요소로 구현될 수 도 있다. 도 3을 참조하면, 센서부는 사용자의 움직임을 감지하여 사용자의 신체 움직임 정보를 생성할 수 있다. (S110) 일 예로 사용자의 19개(골반, 허리1, 허리2, 목, 머리, 양 어깨, 양 팔, 양 팔뚝, 양 손, 양 다리, 양 종아리, 양 발)의 관절 움직임을, 18개의 카메라를 이용하여 초당 30프레임으로 5초간 측정할 수 있으며, 보다 정확한 정보를 생성하기 위해 이러한 측정은 복수 회 실행할 수 있으며, 이렇게 생성된 정보는 신체 움직임 정보로 도 2의 아래에 도시된 바와 같이 정렬된 정보로 생성될 수 있다. 여기서 5초와 30프레임은 일 예에 불과한 것으로써, 사용 환경에 따라 프레임과 초는 다양한 범위로 설정될 수 있다. 감정 정보 생성부는 센서부가 취득한 사용자의 신체 움직임 정보를 기초로 미리 설정된 관절 항 목에 따라 사용자의 관절 별 움직임 정보를 산출할 수 있다. (도 3의 S120). 구체적으로, 감정 정보 생성부는 앞서 설명한 센서부의 다양한 센서를 이용하여 사용자의 신체 움직 임을 기초로 사용자의 관절 별 움직임 정보를 계산하고, 총 관절 움직임 양에 대한 정보를 도 3에 도시된 바와 같이(도면 부호 11참조) 생성할 수 있다. 일 예로, 관절 별 움직임 정보는 도 3에 도시된 바와 같이 각 관절별로 사용자가 총 몇 회 움직였는지에 대한 정보를 포함할 수 있다. 만약, 복수 회 측정한 경우에는 총 합에 대한 정보가 표현되거나, 복수 회에 대한 평균 정보로 표현될 수 있다. 미리 설정된 관절 항목은 도 4에 도시된 바와 같이, 사람의 주요 관절 부위에 대한 항목을 의미하는데, 구체적 으로 미리 설정된 관절 항목은 골반, 아래 허리, 위 허리, 목, 머리, 왼쪽 어깨, 왼쪽 팔, 왼쪽 팔뚝, 왼쪽 손, 오른쪽 어깨, 오른쪽 팔, 오른쪽 팔뚝, 오른쪽 손, 오른쪽 다리, 오른쪽 종아리, 오른쪽 발, 왼쪽 다리, 왼쪽 종아리 및 왼쪽 발 중 적어도 하나를 포함할 수 있다. 사용자의 관절 별 움직임 정보가 생성되었으면, 감정 정보 생성부는 사용자의 관절 별 움직임 정보를 기초 로 미리 설정된 감정 항목에 따라 사용자의 감정 별 움직임 정보를 산출할 수 있다. 미리 설정된 감정 항목은 사람이 일반적으로 느끼는 감정에 대한 항목을 의미하는데, 일 예로 미리 설정된 감정 항목은 행복, 슬픔, 놀람, 화남, 역겨움, 공포 및 중립 중 적어도 하나를 포함할 수 있다. 일반적으로 사람마다 차이는 존재하지만 특정 감정에서 특정 관절을 많이 움직이는 경향성이 존재한다. 일 예로, 사람이 행복한 감정을 느끼는 경우 전반적으로 모든 관절이 활성화되고, 공포 감정을 느끼는 경우 팔을 제외한 대부분의 관절들은 비활성화 상태가 된다. 따라서, 본 발명의 경우 사전에 복수의 사용자들을 통해 학습된 결과를 바탕으로 도 5에 도시된 바와 같이 사람 이 각 감정 마다 활성화/비활성되는 관절 부분을 대응시키고, 이를 기초로 실시간으로 측정된 사용자의 감정 정 보를 산출할 수 있다. (S130) 일 예로, 감정 정보 생성부는 도 3에 도시된 바와 같이(도면 부호 12 참조) 미리 설정된 7개의 감정 항목 에 대해, 각각의 관절이 어느 정도 활성화 되어 있는지에 대한 정보를 사용자의 감정 별 움직임 정보로 생성할 수 다. 도면에서 진하게 빨간색으로 표현된 부분일 수록, 그 부분에 대응되는 관절이 상대적으로 많이 움직인 것을 의미하고, 도면에서 연하게 빨간색으로 표현된 부분일 수록, 그 부분에 대응되는 관절이 상대적으로 적게 움직인 것을 의미한다. 사용자의 감정 별 움직임 정보가 생성되었으면, 감정 정보 생성부는 생성된 사용자의 관절 별 움직임 정보 및 감정 별 움직임 정보를 기초로 사용자의 바디 모션 맵(BMMs) 정보를 생성할 수 있다. (S140) 바디 모션 맵 정보는 감정 항목에 대해 각각의 관절이 어느 정도 활성화 되어 있는지에 대한 정보를 의미하는 것으로, 도 3도시된 바와 같이(도면 부호 13 참조), 시각화 되어 있는 정보로 생성될 수 있다. 따라서, 최종적 으로 프로세서는 바디 모션 맵 정보를 기초로 현재 사용자의 감정이 어떤 감정인지 판단할 수 있다. 한편, 바디 모션 맵 정보 생성부는 측정 대상이 되는 사용자에 대해 복수 회 신체 움직임 정보를 취득하였 다면, 사용자의 관절 별 평균 움직임 정보를 산출하고, 산출된 평균 움직임 정보를 기초로 사용자의 관절 별 움 직임 정보를 최종적으로 생성할 수 있다. 구체적으로, 도 6의 (a)에 도시된 바와 같이, 사용자의 관절 별 정보가 생성되었으면, 바디 모션 맵 정보 생성부는 각 감정 항목에 대해 관절이 평균적으로 얼마나 움직였는지 여부에 대한 정보인 평균 움직임 정 보를 산출하고, 사용자의 관절 별 정보에서 평균 움직임 정보를 차감한 후, 차감 후 생성된 정보를 기초로 도 6의 (b)에 도시된 바와 같이 바디 모션 맵 정보를 생성할 수 있다. 구체적으로, 이를 수학식으로 설명하면, 사용자의 관절 별 움직임의 총 합 정보는 아래 수학식 과 같이 정의 될 수 있고, 사용자의 평균 움직임 정보는 아래 수학식 와 같이 정의될 수 있으며, 최종적으로 바디 모션 맵 정보는 아래 수학식 과 같이 정의될 수 있다. (수학식 1)"}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "(수학식 2) (수학식 3)"}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "위 수학식들에서, 관절(joint)은 19개로 설정하고, 초당 프레임은 150으로 설정하였으며, x, y, z는 사용자의 신체 움직임 좌표를 의미한다. 일반적으로 사람들은 각각의 특정 감정에 대해 특정 관절이 평균적으로 움직이는 특성을 가지고 있다. 따라서, 본 발명과 같이 각각의 감정에 대해 평균 움직임 정보를 차감하게 되면, 기본적인 관절 움직임 정보는 배제된 후의 관절 정보를 기초로 사용자의 감정을 판단할 수 있으므로, 보다 정확하게 사용자의 감정 정보를 판단할 수 있는 장점이 존재한다. 사용자의 바디 모션 맵 정보가 생성되었으면, 프로세스는 상기 과정들을 복수의 사용자들에 대해 반복적으 로 수행하여, 도 3의 도시된 바와 같이(도면 부호 14참조) 복수의 사용자들에 대한 바디 모션 맵 정보를 생성할 수 있다. 이렇게 생성된 정보는 S130 과정에서 감정 정보 생성부가 사용자의 감정 별 관절 정보를 생성하 는데 기초 정보가 될 수 있으며, 후술할 감정 판단 인공신경망 모듈이 파라미터를 학습하는 기초 데이터가 될 수 있다. 이하 감정 판단 인공신경망 모듈에 대해 도면을 통해 구체적으로 설명하도록 한다. 도 7과 도 8은 본 발명의 일 실시예에 따른 감정 판단 인공신경망 모듈의 입력 정보와 출력 정보를 도시한 도면 이고, 도 9는 본 발명의 일 실시예에 따른 감정 판단 인공신경망 모듈이 포함하고 있는 세부 인공신경망을 도시 한 블럭도이고, 도 10과 도 11은 본 발명의 일 실시예에 따른 감정 판단 인공신경망 모듈이 포함하고 있는 인공 신경망의 구조를 구체적으로 도시한 도면이고, 도 12는 본 발명의 일 실시예에 따른 제1인공신경망의 구조를 도 시한 도면이다. 도 7 내지 도 11을 참조하면, 본 발명에 따른 감정 판단 인공신경망 모듈은 센서부까 취득한 신체 움 직임 정보를 입력 정보로 하고, 입력된 신체 움직임 정보를 분석하여 사용자의 감정 정보를 출력 정보로 출력하는 인공신경망으로서, 입력 정보, 출력 정보 및 레퍼런스 정보(ground truth, 50)를 기초로 인공 신경망을 학습하는 학습 세션과, 입력되는 신체 움직임 정보를 분석하여 감정 정보를 추론하고 출 력하는 추론 세션을 포함할 수 있다. 본 발명에 따른 감정 판단 인공신경망 모듈의 학습 세션과 추론 세션은 도면에 도시된 바와 같 이 별도의 구성으로 구현될 수 있으나, 하나의 세션으로 구현될 수 도 있다. 감정 판단 인공신경망 모듈이 출력하는 도 9에 도시된 바와 같이 7개의 감정 항목(행복, 슬픔, 놀람, 화남, 역겨움, 공포 및 중립) 중 감정 판단 인공신경망 모듈이 판단한 1개의 감정이 출력 정보로 출력될 수 있다. 본 발명에 따른 레퍼런스 정보는 메모리에 저장될 수 있으며, 레퍼런스 정보는 인공신경망이 출력한 정보에 대응되는 실제 학습 자료를 의미한다. 본 발명의 경우, 앞서 도면을 통해 설명하였던, 복수의 사용자에 대한 바디 모션 맵 정보가 레퍼런스 정보로 활용될 수 있다. 본 발명에 따른 감정 판단 인공신경망 모듈은 총 3개의 인공 신경망을 포함할 수 있는데, 구체적으로 제1 인공신경망, 제2인공신경망 및 제3인공신경망을 포함할 수 있다. 구체적으로, 제1인공신경망에 출력하는 출력 정보는 제2인공신경망의 입력 정보로 입력될 수 있으며, 제2인공신경망에 출력하는 출력 정보는 제3인공신경망의 입력 정보로 입력될 수 있으며, 제3인공신경 망이 출력하는 출력 정보는 감정 판단 인공신경망 모듈의 최종 출력 정보로 출력될 수 있다. 한편 도 면에서는 설명의 편의를 위해 감정 판단 인공신경망 모듈이 3개의 인공신경망을 독립되게 포함하고 있는 것으로 표시하였으나, 본 발명의 실시예가 이로 한정되는 것은 아니고, 사용 목적에 따라 3개의 인공신경망은 하나의 인공신경망 또는 2개의 인공신경망으로도 구현될 수 있다. 제1인공신경망은 도 10 및 도 11에 도시된 바와 같이 복수 개의 컨볼루션 레이어(Convolution Layer)와 복 수 개의 폴링 레이어(Pooling Layer)를 포함할 수 있으며, 제1인공신경망은 감정 판단 인공신경망 모듈에 입력되는 입력 정보인 신체 움직임 정보를 입력 정보로 하여, 움직임 정보에 포함되어 있는 특징 (feature)을 입력된 차원보다 더 작은 차원으로 변환한 제1특징 맵(feature map,21) 출력 정보로 출력할 수 있 다. 제2인공신경망은 도면에 도시된 바와 같이, 복수 개의 컨볼루션 레이어(Convolution Layer)를 포함하는 컨 볼루션 네트워크를 복수 개 포함하며, 제1인공신경망이 출력한 제1특징 맵에 대한 정보를 기초로 제2 특징 맵에 대한 정보를 출력 정보로 출력할 수 있다. 구체적으로, 제2인공신경망은 제1특징 맵에 포함되어 있지 않은 다른 특징을 추출한 제2특징 맵을 출력 정보로 출력할 수 있다. 본 발명과 같이 특징을 작은 차원으로 특징을 변환시키는 경우 추후 제2인공신경 망과 제3인공신경망이 감정 정보를 추출함에 있어서 보다 효율적으로 수행할 수 있다. 제2인공신경망은 특정 감정 항목에 대해 학습을 구체적으로 진행하는 인공신경망을 의미한다. 따라서, 제2 인공신경망은 이에 대한 정확도를 높이기 제1특징 맵에 대해 복수 개의 컨볼루션 레이어를 포함하는 컨볼루션 네트워크를 복수 회 거치도록 함으로써, 입력된 특징 맵에 대해 특징을 좀 더 세밀하게 추출할 수 있 다. 도면에서는 컨볼루션 네트워크가 8번 반복되는 것으로 도시하였으나, 이는 일 실시예가 불과할 뿐, 컨볼루션 네 트워크가 반복되는 횟수는 사용 목적에 따라 다양하게 변경될 수 있다. 제3인공신경망은 도 10 및 도 11에 도시된 바와 같이 복수 개의 컨볼루션 레이어(Convolution Layer)와 복수 개의 폴링 레이어(Pooling Layer)를 포함할 수 있으며, 제3인공신경망은 제2인공신경망이 출력 하는 제2특징 맵에 대한 정보에 대해, 상기 신체 움직임 정보에 대응되는 특정 감정 정보를 출력 정보로 출 력하는 인공신경망을 의미한다. 구체적으로, 제3인공신경망은 제2특징 맵을 분석하여, 최종적인 감정 정보를 추출하는 인공신경망으로 서, 제2특징 맵에 대한 정보로부터 감정에 관련된 특징들을 뽑아 낸 후 이를 기초로 최종적인 사용자의 감 정 정보를 출력할 수 있다. 본 발명과 같이 감정 판단 인공신경망 모듈이 3개의 인공신경망으로 구현되는 경우, 학습의 효율성 및 추 론 결과의 정확도를 높일 수 있다. 구체적으로, 네트워크 학습에 사용될 데이터의 형태가 변할 경우, 제1인공신경망의 파라미터를 수정함으로 써, 사용될 데이터의 형태에 맞춰 인공신경망의 파라미터가 조정될 수 있으며, 학습해야 될 특성이 복잡해지면 (예를 들어, 학습 해야 될 감정이 기본적인 감정 행복, 슬픔, 놀람 등에서 더 상위의 감정 불안, 우울, 자부심 등으로 바뀌는 경우) 제2인공신경망을 구성하고 있는 컨볼루션 네트워크의 반복 횟수를 더 증가시켜 숨겨 진 특성을 더 세밀하게 학습할 수 있다. 또한, 추출 해야 될 전체 감정의 개수가 달라질 경우, 마지막 인공신경망은 제3인공신경망의 파라미터를 수정함으로써, 용이하게 추출할 감정 항목만 변경할 수 있다. 또한, 본 발명에 따른 감정 판단 장치는 감정 강도 정보 산출부(230, 도 2 참조)를 이용하여 감정 판단 인 공신경망 모듈이 출력한 감정 정보와 바디 모션 맵 정보 생성부까 출력한 바디 모션 맵 정보를 기초 로, 판단된 감정에 대한 강도 정보를 출력할 수 있다. 일반적으로 사람의 감정은 수학처럼 딱 분류되기는 어렵고, 감정이 분류되었다 하더라고, 그 감정의 정도가 다 를 수 있다. 예를 들어, 같은 슬픔이어도 우울한 정도의 슬픔일 수 있으며, 주체할 수 없을 만큼의 감정일 수 있다. 따라서, 이러한 감정의 정도 또한 사람의 감정 정보를 판단하는데 매우 중요한 인자에 해당할 수 있다. 따라서, 본 발명에 따른 감정 강도 정보 산출부는, 동일한 신체 움직임 정보에 대해 감정 판단 인공신경망 모듈이 출력한 감정 정보에 대해, 바디 모션 맵 정보 생성부가 생성한 복수의 사용자에 대한 바디 모 션 맵 정보를 상호 비교하여 현재 판단된 감정의 강도 정보를 수치적으로 변환하여 출력할 수 있다. 구체적으로, 바디 모션 맵 정보에는 복수의 사용자들에 대한 실험 결과를 기초로 감정 항목별로 움직이는 관절 에 대한 정보가 수치적으로 정리된 정보가 포함되어 있으므로(일 예로, 슬픈 감정에서는 주로 어떤 관절이 움직 이고, 이 관절이 움직이는 정도에 따라 슬픔의 정도가 어느 정도인지에 대한 정보), 감정 판단 인공신경망 모듈 에서 사용자의 감정이 슬픔으로 판단이 되었으면, 감정 감도 산출부는 감정이 슬픈 경우에 어떤 관절 이 어느 정도 움직이는지에 대한 정보가 저장되어 있는 바디 모션 맵 정보를 이용하여, 사용자의 감정의 강도에대한 정보를 수치적으로 산출할 수 있다. 이를 통해 사용자의 감정의 종류 뿐만 아니라, 해당 감정에서 어느 정 도로 감정이 깊은지, 약한지에 대한 감정의 강도에 대한 정보도 알 수 있다. 도 13은 본 발명에 따른 감정 판단 결과와 종래 기술에 따른 감정 판단 결과를 비교 도시한 도면이다. 도 13의 아래쪽 도면은 신체 감각 기반(Sensation)으로 감정 별 특징을 추출하는 감정 판단 장치(Nummenmaa, 2013)를 통해 본 연구에서 진행한 29명의 피험자로 부터 얻은 confusion matrix를 도시한 도면이며, 도 13의 위 쪽 도면은 본 발명에서 제안하는 신체 움직임 정보를 기반으로 한 사용자 감정 판단 장치를 통한 confusion matrix를 도시한 도면이다. 도 13의 confusion martix의 경우 대각선 칸(true positive ratio)이 진할수록 감정 판단이 더욱 정확하게 이 뤄지는 것을 의미하는데, 도 13에서 비교 도시되어 있다시피, 본 발명에 따라 제안되는 신체 움직임 정보 기반 의 감정 판단 장치가 기존의 설문 기반 방식보다 더욱 정확하게 감정 판단을 하고 있음을 알 수 있다. 지금까지 도면을 통해 본 발명의 실시예에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 장치(10 0)에 대해 자세히 알아보았다. 본 발명에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는, 특정 감정 항목에 대한 사용 자의 관절 별 움직임 정보를 기초로 사용자의 감정 정보를 판단하기 때문에 종래 기술보다 정확히 사용자의 감 정 정보를 판단할 수 있다. 또한, 본 발명에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는 감정 항목에 대한 사용 자의 관절 별 움직임 정보를 기초로 생성한 바디 모션 맵 정보(BMMs) 정보를 이용하여 인공신경망을 학습하고 추론하기 때문에, 종래기술에 비해 정확성을 높이면서 동시에 유연하게 감정 상태를 판단할 수 있는 장점이 존 재한다. 또한, 본 발명에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치는 사용자의 신체 움직임 정보를 기초로 사용자의 감정 정보를 실시간으로 정확히 판단할 수 있기 때문에, 사용자의 감정 정보를 실시간 으로 필요로 하는 원격 통신, 보안, 교육, 의료 및 게임 등의 분야에 본 기술이 적용되어, 각종 서비스의 기능 을 향상시킬 수 있는 장점이 존재한다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 예를 들어, 프로세 서, 컨트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴퓨 터, FPA(field programmable array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 운영 체제 상에서 수행되는 하나 이상의 소 프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설명된 경"}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소(processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 컨트롤러를 포함할 수 있다. 또한, 병렬 프로세서(parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처 리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치에 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터 는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계 되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가 능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2022-0111720", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범 위에 속한다."}
{"patent_id": "10-2022-0111720", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 발명의 상세한 설명에서 인용되는 도면을 보다 충분히 이해하기 위하여 각 도면의 간단한 설명이 제공된다. 도 1은 본 발명의 일 실시예에 따른 신체 움직임 정보를 기반으로 한 실시간 사용자 감정 판단 장치의 일부 구 성 요소를 도시한 블럭도이다. 도 2는 본 발명의 일 실시예에 따른, 프로세서의 일부 구성 요소를 도시한 블럭도이다. 도 3은 본 발명의 일 실시예에 따른 프로세스의 작동 순서를 도시한 순서도이다. 도 4는 본 발명의 일 실시예에 따라 미리 설정된 사용자의 관절 항목을 도시한 도면이다. 도 5는 본 발명의 일 실시예에 기 생성된, 미리 설정된 감정 항목마다 복수의 사용자가 평균적으로 움직이는 관 절 별 움직임 정보를 도식화한 도면이다. 도 6은 본 발명의 일 실시예에 따른 사용자의 감정 별 관절 움직임 정보를 생성하는 방법을 설명하기 위한 도 면이다. 도 7과 도 8은 본 발명의 일 실시예에 따른 감정 판단 인공신경망 모듈의 입력 정보와 출력 정보를 도시한 도면 이다. 도 9는 본 발명의 일 실시예에 따른 감정 판단 인공신경망 모듈이 포함하고 있는 세부 인공신경망을 도시한 블 럭도이다. 도 10과 도 11은 본 발명의 일 실시예에 따른 감정 판단 인공신경망 모듈이 포함하고 있는 인공신경망의 구조를 구체적으로 도시한 도면이다. 도 12는 본 발명의 일 실시예에 따른 제1인공신경망의 구조를 구체적으로 도시한 도면이다. 도 13은 본 발명에 따른 감정 판단 결과와 종래 기술에 따른 감정 판단 결과를 비교 도시한 도면이다."}
