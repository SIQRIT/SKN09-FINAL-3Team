{"patent_id": "10-2022-0133807", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0080305", "출원번호": "10-2022-0133807", "발명의 명칭": "ANN 모델과 NPU의 최적화", "출원인": "주식회사 딥엑스", "발명자": "김녹원"}}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "ANN(Artificial Neural Network)과 해당 ANN을 구동하기 위한 NPU(Neural Processing Unit)의 최적 디자인을찾기 위하여, 컴퓨터에 설치되어 실행되는 소프트웨어 툴로서,상기 ANN의 레이어의 개수 L에 대한 후보 값들의 제1 세트와 그리고 각 레이어 당 채널의 개수 C에 대한 후보값들의 제2 세트가 존재할 때, 최적의 L 값과 최적의 C 값을 검색하는 제1 동작부와; 그리고상기 최적의 L 값과 상기 최적의 C 값을 출력하는 제2 동작부를 포함하고,상기 NPU는 파이프라인 형태로 연결된 복수의 블록들을 포함할 때, 상기 복수의 블록들의 개수는 상기 최적의 L값과 관련되어 있는, 소프트웨어 툴."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 복수의 블록들 각각은 라인 버퍼와, XNOR 논리 게이트와, Pop-Count 명령 수행부와, 배치 정규화부 중 하나 이상을 포함하는, 소프트웨어 툴."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 제1 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 상기 NPU의 하드웨어 비용과 상기NPU의 전력 소모량 중 적어도 하나 이상에 기초하여 검색하는, 소프트웨어 툴."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 NPU의 하드웨어 비용은 플리-플롭의 개수에 기초하여 결정되는, 소프트웨어 툴."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 제1 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 요구되는 최소 추론 정확도에 기초하여 검색하는, 소프트웨어 툴."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 추론 정확도 저하가 발생하는 경우, 상기 최적의 C 값은 한 단계 혹은 두 단계 증가되는, 소프트웨어 툴."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 최적의 C 값은 상기 ANN의 모든 레이어에서 동일한 상수 값인, 소프트웨어 툴."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "컴퓨터 장치로서,적어도 하나의 프로세서와; 그리고ANN Neural Network)과 해당 ANN을 구동하기 위한 NPU(Neural Processing Unit)의 최적 디자인을 찾는 소프트웨어 툴을 위한 명령어(instructions)를 저장하고, 상기 적어도 하나의 프로세서와 동작 가능하게(operably) 전기적으로 연결가능한, 적어도 하나의 메모리를 포함하고,상기 소프트웨어 툴을 위한 명령어가 상기 적어도 하나의 프로세서에 의해서 실행되는 것에 기초하여, 수행되는공개특허 10-2023-0080305-3-동작은:상기 ANN의 레이어의 개수 L에 대한 후보 값들의 제1 세트와 그리고 각 레이어 당 채널의 개수 C에 대한 후보값들의 제2 세트가 존재할 때, 최적의 L 값과 최적의 C 값을 검색하는 단계와; 그리고상기 최적의 L 값과 상기 최적의 C 값을 출력하는 단계를 포함하고,상기 NPU는 파이프라인 형태로 연결된 복수의 블록들을 포함할 때, 상기 복수의 블록들의 개수는 상기 최적의 L값과 관련되어 있는, 컴퓨터 장치."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 복수의 블록들 각각은 라인 버퍼와, XNOR 논리 게이트와, Pop-Count 명령 수행부와, 배치 정규화부 중 하나 이상을 포함하는, 컴퓨터 장치."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서, 상기 제1 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 상기 NPU의 하드웨어 비용과 상기NPU의 전력 소모량 중 적어도 하나 이상에 기초하여 검색하는, 컴퓨터 장치."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 NPU의 하드웨어 비용은 플리-플롭의 개수에 기초하여 결정되는, 컴퓨터 장치."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항에 있어서, 상기 제1 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 요구되는 최소 추론 정확도에 기초하여 검색하는, 컴퓨터 장치."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 추론 정확도 저하가 발생하는 경우, 상기 최적의 C 값은 한 단계 혹은 두 단계 증가되는, 컴퓨터 장치."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8항에 있어서, 상기 최적의 C 값은 상기 ANN의 모든 레이어에서 동일한 상수 값인, 컴퓨터 장치."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "ANN(Artificial Neural Network)과 해당 ANN을 구동하기 위한 NPU(Neural Processing Unit)의 최적 디자인을찾는 소프트웨어 툴을 위한 명령어들을 기록하고 있는 비휘발성(non-volatile) 컴퓨터 판독가능 저장 매체로서,상기 명령어들은, 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금:상기 ANN의 레이어의 개수 L에 대한 후보 값들의 제1 세트와 그리고 각 레이어 당 채널의 개수 C에 대한 후보값들의 제2 세트가 존재할 때, 최적의 L 값과 최적의 C 값을 검색하는 단계와; 그리고상기 최적의 L 값과 상기 최적의 C 값을 출력하는 단계를 수행시킬 수 있고,상기 NPU는 파이프라인 형태로 연결된 복수의 블록들을 포함할 때, 상기 복수의 블록들의 개수는 상기 최적의 L값과 관련되어 있는, 컴퓨터 판독가능 저장 매체."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서, 상기 복수의 블록들 각각은 라인 버퍼와, XNOR 논리 게이트와, Pop-Count 명령 수행부와, 배치 정규화부 중 하나 이상을 포함하는, 컴퓨터 판독가능 저장 매체.공개특허 10-2023-0080305-4-청구항 17 제15항에 있어서, 상기 제1 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 상기 NPU의 하드웨어 비용과 상기NPU의 전력 소모량 중 적어도 하나 이상에 기초하여 검색하는, 컴퓨터 판독가능 저장 매체."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제15항에 있어서, 상기 NPU의 하드웨어 비용은 플리-플롭의 개수에 기초하여 결정되는, 컴퓨터 판독가능 저장매체."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제15항에 있어서, 상기 제1 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 요구되는 최소 추론 정확도에 기초하여 검색하는, 컴퓨터 판독가능 저장 매체."}
{"patent_id": "10-2022-0133807", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "ANN(Artificial Neural Network)을 처리하도록 구성된 NPU(Neural Processing Unit)으로서,복수의 블록들을 포함하고,상기 복수의 블록들은 파이프라인 형태로 연결되고, 상기 복수의 블록들 각각은 라인 버퍼와, XNOR 논리 게이트와, Pop-Count 명령 수행부와, 배치 정규화부 중 하나 이상을 포함하고,상기 복수의 블록들의 개수는 상기 ANN 내의 복수 레이어들의 개수 L과 동일하고, 상기 복수 레이어들의 개수 L과 레이어 당 채널의 개수 C는 전력 소모 및 하드웨어 구현 비용에 기초하여 결정된 것을, 특징으로 하는 NPU."}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 예시에 따르면, ANN(Artificial Network)과 해당 ANN을 구동하기 위한 NPU(Neural Processing Unit)의 최적 디자인을 찾기 위하여, 컴퓨터에 설치되어 실행되는 소프트웨어 툴이 제시된다. 상기 소프트웨어 툴은 상기 ANN의 레이어의 개수 L에 대한 후보 값들의 제1 세트와 그리고 각 레이어 당 채널의 개수 C에 대한 후 보 값들의 제2 세트가 존재할 때, 최적의 L 값과 최적의 C 값을 검색하는 제1 동작부와; 그리고 상기 최적의 L 값과 상기 최적의 C 값을 출력하는 제2 동작부를 포함할 수 있다. 상기 NPU는 파이프라인 형태로 연결된 복수의 블록들을 포함할 때, 상기 복수의 블록들의 개수는 상기 최적의 L 값과 관련되어 있을 수 있다."}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공 신경망(artificial neural network)에 관한 것이다."}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간은 인식(Recognition), 분류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정 (Control/Decision making) 등을 할 수 있는 지능을 갖추고 있다. 인공지능(artificial intelligence: AI)은 인간의 지능을 인공적으로 모방하는 것을 의미한다. 인간의 뇌는 뉴런(Neuron)이라는 수많은 신경세포로 이루어져 있다. 각각의 뉴런은 시냅스(Synapse)라고 불리는 연결부위를 통해 수백에서 수천 개의 다른 뉴런들과 연결되어 있다. 인간의 지능을 모방하기 위하여, 생물학적 뉴런의 동작원리와 뉴런 간의 연결 관계를 모델링한 것을, 인공신경망(Artificial Neural Network, ANN) 모델이 라고 한다. 즉, 인공 신경망은 뉴런들을 모방한 노드들을 레이어(Layer: 계층) 구조로 연결시킨, 시스템이다. 이러한 인공신경망 모델은 레이어 수에 따라 '단층 신경망'과 '다층 신경망'으로 구분한다. 일반적인 다층신경 망은 입력 레이어와 은닉 레이어, 출력 레이어로 구성된다. 입력 레이어(input layer)은 외부의 자료들을 받아들이는 레이어로서, 입력 레이어의 뉴런 수는 입력되는 변수의 수와 동일하다. 은닉 레이어(hidden layer)은 입력 레이어와 출력 레이어 사이에 위치하며 입력 레이어로부터 신호를 받아 특성을 추출하여 출력층 으로 전달한다. 출력 레이어(output layer)은 은닉 레이어로부터 신호를 받아 외부로 출력한다. 뉴런 간의 입력신호는 0에서 1 사이의 값을 갖는 각각의 연결강도와 곱해진 후 합산되며 이 합이 뉴런의 임계치보다 크면 뉴런이 활성화되어 활성화 함수를 통하여 출력 값으로 구현된다. 한편, 보다 높은 인공 지능을 구현하기 위하여, 인공 신경망의 은닉 레이어의 개수를 늘린 것을 심층 신경망 (Deep Neural Network, DNN)이라고 한다.다른 한편, DNN에는 여러 종류가 있으나, 컨볼루션 신경망(Convolutional Neural Network, CNN)은 입력 데이터 의 특징들을 추출하고, 특징들의 패턴을 파악하기에 용이한 것으로 알려져 있다. 컨볼루션 신경망(CNN)은 인간 뇌의 시각 피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망이다. 컨볼 루션 신경망은 영상처리에 적합한 것으로 알려져 있다. 그러나, CNN의 특성, 즉 부동 소수점을 위하여 매우 많은 양의 파라미터를 필요로 하기 때문에 메모리 요구량이 큰 문제점이 있다. 이런 문제를 해결하기 위해 신경망의 레이어 입력 값의 크기 또는 파라미터의 크기를 줄이는 low precision network에 대한 연구가 진행되었다. Low precision network에 대한 연구로 인하여, BNN(Binarized Neural Network)이 제시되었다. 그러나, 기존의 연구들은 BNN에 대해서 이론적으로만 접근하였고, 하드웨어 상에서 구동되는 최적의 BNN 모델을 제시하지 못하였다. 또한, 기존의 연구들은 실제 구현을 위한 최적의 하드웨어 구조를 제시하지 못하였다."}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "따라서, 본 개시의 예시는 하드웨어 상에서 구동되는 최적의 BNN 모델을 찾는 방법을 제시한다. 또한, 본 개시 의 예시는 BNN을 구현하기 위한 최적의 하드웨어 아키텍처를 제시하는 것을 목적으로 한다."}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 예시에 따르면, ANN(Artificial Network)과 해당 ANN을 구동하기 위한 NPU(Neural Processing Unit)의 최적 디자인을 찾기 위하여, 컴퓨터에 설치되어 실행되는 소프트웨어 툴이 제시된다. 상기 소프트웨어 툴은 상기 ANN의 레이어의 개수 L에 대한 후보 값들의 제1 세트와 그리고 각 레이어 당 채널의 개수 C에 대한 후보 값들의 제2 세트가 존재할 때, 최적의 L 값과 최적의 C 값을 검색하는 제1 동작부와; 그리고 상기 최적의 L 값과 상기 최적의 C 값을 출력하는 제2 동작부를 포함할 수 있다. 상기 NPU는 파이프라인 형태로 연결된 복수 의 블록들을 포함할 때, 상기 복수의 블록들의 개수는 상기 최적의 L 값과 관련되어 있을 수 있다. 상기 복수의 블록들 각각은 라인 버퍼와, XNOR 논리 게이트와, Pop-Count 명령 수행부와, 배치 정규화부 중 하 나 이상을 포함할 수 있다. 상기 제1 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 상기 NPU의 하드웨어 비용과 상기 NPU의 전력 소모 량 중 적어도 하나 이상에 기초하여 검색할 수 있다. 상기 NPU의 하드웨어 비용은 플리-플롭의 개수에 기초하여 결정될 수 있다. 상기 제1 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 요구되는 최소 추론 정확도에 기초하여 검색할 수 있다. 추론 정확도 저하가 발생하는 경우, 상기 최적의 C 값은 한 단계 혹은 두 단계 증가될 수 있다. 상기 최적의 C 값은 상기 ANN의 모든 레이어에서 동일한 상수 값일 수 있다. 본 개시의 일 예시에 따르면, 컴퓨터 장치가 또한 제시된다. 상기 컴퓨터 장치는 적어도 하나의 프로세서와; 그 리고 ANN(Artificial Neural Network)과 해당 ANN을 구동하기 위한 NPU(Neural Processing Unit)의 최적 디자 인을 찾는 소프트웨어 툴을 위한 명령어(instructions)를 저장하고, 상기 적어도 하나의 프로세서와 동작 가능 하게(operably) 전기적으로 연결가능한, 적어도 하나의 메모리를 포함할 수 있다. 상기 소프트웨어 툴을 위한 명령어가 상기 적어도 하나의 프로세서에 의해서 실행되는 것에 기초하여, 수행되는 동작은: 상기 ANN의 레이어 의 개수 L에 대한 후보 값들의 제1 세트와 그리고 각 레이어 당 채널의 개수 C에 대한 후보 값들의 제2 세트가 존재할 때, 최적의 L 값과 최적의 C 값을 검색하는 단계와; 그리고 상기 최적의 L 값과 상기 최적의 C 값을 출 력하는 단계를 포함할 수 있다. 본 개시의 일 예시에 따르면, ANN(Artificial Neural Network)과 해당 ANN을 구동하기 위한 NPU(Neural Processing Unit)의 최적 디자인을 찾는 소프트웨어 툴을 위한 명령어들을 기록하고 있는 비휘발성(non- volatile) 컴퓨터 판독가능 저장 매체가 제시된다. 상기 컴퓨터 판독가능 저장 매체는 상기 명령어들은, 하나 이상의 프로세서들에 의해 실행될 때, 상기 하나 이상의 프로세서들로 하여금: 상기 ANN의 레이어의 개수 L에대한 후보 값들의 제1 세트와 그리고 각 레이어 당 채널의 개수 C에 대한 후보 값들의 제2 세트가 존재할 때, 최적의 L 값과 최적의 C 값을 검색하는 단계와; 그리고 상기 최적의 L 값과 상기 최적의 C 값을 출력하는 단계 를 수행시킬 수 있다. 상기 NPU는 파이프라인 형태로 연결된 복수의 블록들을 포함할 때, 상기 복수의 블록들의 개수는 상기 최적의 L 값과 관련되어 있을 수 있다. 본 개시의 일 예시에 따르면, ANN(Artificial Neural Network)을 위한 NPU(Neural Processing Unit)가 제시된 다. 상기 NPU는 복수의 블록들을 포함할 수 있다. 상기 복수의 블록들은 파이프라인 형태로 연결될 수 있다. 상 기 복수의 블록들 각각은 라인 버퍼와, XNOR 논리 게이트와, Pop-Count 명령 수행부와, 배치 정규화부 중 하나 이상을 포함할 수 있다. 상기 복수의 블록들의 개수는 상기 ANN 내의 복수 레이어들의 개수 L과 동일할 수 있다. 상기 복수 레이어들의 개수 L과 레이어 당 채널의 개수 C는 전력 소모 및 하드웨어 구현 비용에 기초하여 결정될 수 있다."}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 예시들에 따르면, 하드웨어 구현 비용이 낮고 전력 소모가 낮은 최적의 NPU를 설계할 수 있다."}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시 또는 출원에 개시되어 있는 본 개시의 개념에 따른 실시 예들에 대해서 특정한 구조적 내지 단계적 설 명들은 단지 본 개시의 개념에 따른 실시 예를 설명하기 위한 목적으로 예시된 것이다. 따라서 본 개시의 개념 에 따른 실시 예들은 다양한 형태로 실시될 수 있으며 본 개시의 개념에 따른 실시 예들은 다양한 형태로 실시 될 수 있으며 본 개시 또는 출원에 설명된 실시 예들에 한정되는 것으로 해석되어서는 아니 된다. 본 개시의 개념에 따른 실시 예는 다양한 변경을 가할 수 있고 여러 가지 형태를 가질 수 있으므로 특정 실시 예들을 도면에 예시하고 본 개시 또는 출원에 상세하게 설명하고자 한다. 그러나, 이는 본 개시의 개념에 따른 실시 예를 특정한 개시 형태에 대해 한정하려는 것이 아니며, 본 개시의 사상 및 기술 범위에 포함되는 모든 변 경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1 및/또는 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만, 예컨대 본 개시의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관 계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃하는\"과 \"~에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 본 개시에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 개시를 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 서술된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함을 지정하려는 것이다. 따라서 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미 를 가지는 것으로 해석되어야 하며, 본 개시에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 실시 예를 설명함에 있어서 본 개시가 속하는 기술 분야에 익히 알려져 있고 본 개시와 직접적으로 관련이 없는 기술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 개시의 요지를 흐리지 않고 더 욱 명확히 전달하기 위함이다. <용어의 정의> 이하, 본 개시에서 제시되는 개시들의 이해를 돕고자, 본 개시에서 사용되는 용어들에 대하여 간략하게 정리하 기로 한다. NPU: 신경망 프로세싱 유닛(Neural Processing Unit)의 약어로서, CPU(Central processing unit)과 별개로 인 공 신경망 모델의 연산을 위해 특화된 프로세서를 의미할 수 있다. ANN: 인공 신경망(artificial neural network)의 약어로서, 인간의 지능을 모방하기 위하여, 인간 뇌 속의 뉴런 들(Neurons)이 시냅스(Synapse)를 통하여 연결되는 것을 모방하여, 노드들을 레이어(Layer: 계층) 구조로 연결 시킨, 네트워크를 의미할 수 있다. DNN: 심층 신경망(Deep Neural Network)의 약어로서, 보다 높은 인공 지능을 구현하기 위하여, 인공 신경망의 은닉 레이어의 개수를 늘린 것을 의미할 수 있다. CNN: 컨볼루션 신경망(Convolutional Neural Network)의 약어로서, 인간 뇌의 시각 피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망이다. 컨볼루션 신경망은 영상처리에 적합한 것으로 알려져 있으며, 입력 데이 터의 특징들을 추출하고, 특징들의 패턴을 파악하기에 용이한 것으로 알려져 있다. 이하, 첨부한 도면을 참조하여 본 개시의 바람직한 실시 예를 설명함으로써, 본 개시를 상세히 설명한다. 이하, 본 개시의 실시 예를 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 이하 신경망 프로세싱 유닛에서 작동될 수 있는 예시적인 인공신경망모델(110a)의 연산에 대하여 설명한다. 도 1의 예시적인 인공신경망모델(110a)은 객체 인식, 음성 인식 등 다양한 추론 기능을 수행하도록 학습된 인공 신경망일 수 있다. 인공신경망모델(110a)은 심층 신경망(DNN, Deep Neural Network)일 수 있다. 단, 본 개시의 예시들에 따른 인공신경망모델(110a)은 심층 신경망에 제한되지 않는다. 예를 들어, 인공신경망모델(110a)은 Transformer, YOLO, BiseNet, RCNN, VGG, VGG16, DenseNet, SegNet, DeconvNet, DeepLAB V3+, U-net, SqueezeNet, Alexnet, ResNet18, MobileNet-v2, GoogLeNet, Resnet-v2,Resnet50, Resnet101, Inception-v3 등의 모델로 구현될 수 있다. 단, 본 개시는 상술한 모델들에 제한되지 않 는다. 또한 인공신경망모델(110a)은 적어도 두 개의 서로 다른 모델들에 기초한 앙상블 모델일 수도 있다. 이하 예시적인 인공신경망모델(110a)에 의해서 수행되는 추론 과정에 대해서 설명하기로 한다. 인공신경망모델(110a)은 입력 레이어(110a-1), 제1 연결망(110a-2), 제1 은닉 레이어(110a-3), 제2 연결망 (110a-4), 제2 은닉 레이어(110a-5), 제3 연결망(110a-6), 및 출력 레이어(110a-7)을 포함하는 예시적인 심층 신경망 모델이다. 단, 본 개시는 도 1에 도시된 인공신경망모델에만 제한되는 것은 아니다. 제1 은닉 레이어 (110a-3) 및 제2 은닉 레이어(110a-5)는 복수의 은닉 레이어로 지칭되는 것도 가능하다. 입력 레이어(110a-1)는 예시적으로, x1 및 x2 입력 노드를 포함할 수 있다. 즉, 입력 레이어(110a-1)는 2개의 입력 값에 대한 정보를 포함할 수 있다. 제1 연결망(110a-2)은 예시적으로, 입력 레이어(110a-1)의 각각의 노드를 제1 은닉 레이어(110a-3)의 각각의 노 드로 연결시키기 위한 6개의 가중치 값에 대한 정보를 포함할 수 있다. 각각의 가중치 값은 입력 노드 값과 곱 해지고, 곱해진 값들의 누산된 값이 제1 은닉 레이어(110a-3)에 저장된다. 제1 은닉 레이어(110a-3)는 예시적으로 a1, a2, 및 a3 노드를 포함할 수 있다. 즉, 제1 은닉 레이어(110a-3)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 제2 연결망(110a-4)은 예시적으로, 제1 은닉 레이어(110a-3)의 각각의 노드를 제2 은닉 레이어(110a-5)의 각각 의 노드로 연결시키기 위한 9개의 가중치 값에 대한 정보를 포함할 수 있다. 상기 제2 연결망(110a-4)의 가중치 값은 제1 은닉 레이어(110a-3)로부터 입력되는 노드 값과 각기 곱해지고, 곱해진 값들의 누산된 값이 제2 은닉 레이어(110a-5)에 저장된다. 제2 은닉 레이어(110a-5)는 예시적으로 b1, b2, 및 b3 노드를 포함할 수 있다. 즉, 제2 은닉 레이어(110a-5)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 제3 연결망(110a-6)은 예시적으로, 제2 은닉 레이어(110a-5)의 각각의 노드와 출력 레이어(110a-7)의 각각의 노 드를 연결하는 6개의 가중치 값에 대한 정보를 포함할 수 있다. 제3 연결망(110a-6)의 가중치 값은 제2 은닉 레 이어(110a-5)로부터 입력되는 노드 값과 각기 곱해지고, 곱해진 값들의 누산된 값이 출력 레이어(110a-7)에 저 장된다. 출력 레이어(110a-7)는 예시적으로 y1, 및 y2 노드를 포함할 수 있다. 즉, 출력 레이어(110a-7)는 2개의 노드 값에 대한 정보를 포함할 수 있다. 도 2a은 컨볼루션 신경망(CNN)의 기본 구조를 설명하기 위한 도면이다. 도 2a을 참조하면, 입력 이미지는 특정 크기의 행과 특정 크기의 열로 구성된 2차원적 행렬로 표시될 수 있다. 입력 이미지는 복수의 채널을 가질 수 있는데, 여기서 채널은 입력 데이터 이미지의 컬러 성분의 수를 나타낼 수 있다. 컨볼루션 과정은 입력 이미지를 지정된 간격으로 순회하면서 커널과 합성곱 연산을 수행하는 것을 의미한다. 컨볼루션 신경망은 현재 레이어에서 다음 레이어로 갈 때, 합성곱(컨볼루션)을 통해 레이어간의 가중치를 반영 하여 다음 레이어에 전달될 수 있다. 예를 들면, 합성곱(컨볼루션)은, 두 개의 주요 파라미터에 의해 정의되는데, 입력 이미지의 크기(통상적으로 1 Х1, 3Х3 또는 5Х5 행렬)와 출력 피처 맵(Feature Map)의 깊이(커널의 수)는 컨볼루션에 의해 연산 될 수 있 다. 이들 컨볼루션은, 깊이 32에서 시작하여, 깊이 64로 계속되며, 깊이 128 또는 256에서 종료될 수 있다. 합성곱(컨볼루션)은, 3D 입력 피처 맵 위로 3Х3 또는 5Х5 크기의 이들 윈도우를 슬라이딩 하고, 모든 위치에 서 정지하고, 주변 피처의 3D 패치를 추출함으로써 동작할 수 있다. 이러한 각 3D 패치는 가중치라고 하는 동일한 학습 가중치 행렬을 갖는 텐서(tensor) 곱을 통해 1D 벡터로 변환 될 수 있다. 이러한 벡터는 3D 출력 맵으로 공간적으로 재조립될 수 있다. 출력 피처 맵의 모든 공간 위치는 입 력 피처 맵의 동일한 위치에 대응될 수 있다. 컨볼루션 신경망은, 학습 과정동안 많은 그라디언트 업데이트 반복에 걸쳐 학습되는 커널(즉, 가중치 행렬)과 입력 데이터 간의 합성곱(컨볼루션) 동작을 수행하는 컨볼루션 레이어를 포함할 수 있다. (m, n)을 커널 크기라 고 하고 W를 가중치 값이라고 설정하면, 컨볼루션 레이어는 내적을 계산함으로써 입력 데이터와 가중치 행렬의합성곱(컨볼루션)을 수행할 수 있다. 커널이 입력 데이터를 가로질러 슬라이딩 하는 단차 크기를 보폭이라고 하며, 커널 면적(mХn)을 수용장 (receptive field)이라고 할 수 있다. 동일한 컨볼루션 커널이 입력의 상이한 위치에 걸쳐 적용되며, 이는 학습 되는 커널의 수가 감소시킨다. 이것은, 또한, 위치 불변 학습을 가능하게 하며, 중요한 패턴이 입력에 존재하는 경우, 컨볼루션 필터는 시퀀스의 위치에 관계없이 그 패턴을 학습할 수 있다. 컨볼루션 신경망은, 입력 데이터가 특정 출력 추정 값으로 이어지도록 조정되거나 학습될 수 있다. 컨볼루션 신 경망은, 출력 추정 값이 실측 자료(ground truth)에 점진적으로 일치하거나 근접할 때까지 출력 추정 값과 실측 자료 간의 비교에 기초하여 역전파(backpropagation)를 이용하여 조정될 수 있다. 컨볼루션 신경망은, 실측 자료와 실제 출력 간의 차이에 기초하는 뉴런들 간의 가중치를 조정함으로써 학습될 수 있다. 도 2b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 2b을 참조하면, 예시적으로 입력 이미지가 5 x 5 크기를 갖는 2차원적 행렬인 것으로 나타나 있다. 또한, 도 2b에는 예시적으로 3개의 노드, 즉 채널 1, 채널 2, 채널 3이 사용되는 것으로 나타내었다. 먼저, 레이어 1의 합성곱 동작에 대해서 설명하기로 한다. 입력 이미지는 레이어 1의 첫 번째 노드에서 채널 1을 위한 커널 1과 합성곱 되고, 그 결과로서 피처 맵1이 출 력된다. 또한, 상기 입력 이미지는 레이어 1의 두 번째 노드에서 채널 2를 위한 커널 2와 합성곱 되고 그 결과 로서 피처 맵 2가 출력된다. 또한, 상기 입력 이미지는 세 번째 노드에서 채널 3을 위한 커널 3과 합성곱 되고, 그 결과로서 피처 맵3이 출력된다. 다음으로, 레이어 2의 풀링(pooling) 동작에 대해서 설명하기로 한다. 상기 레이어 1 로부터 출력되는 피처 맵1, 피처 맵2, 피처 맵3은 레이어 2의 3개의 노드로 입력된다. 레이어 2 는 레이어 1로부터 출력되는 피처 맵들을 입력으로 받아서 풀링(pooling)을 수행할 수 있다. 상기 풀링이라 함 은 크기를 줄이거나 행렬 내의 특정 값을 강조할 수 있다. 풀링 방식으로는 최대값 풀링과 평균 풀링, 최소값 풀링이 있다. 최대값 풀링은 행렬의 특정 영역 안에 값의 최댓값을 모으기 위해서 사용되고, 평균 풀링은 특정 영역내의 평균을 구하기 위해서 사용될 수 있다. 도 2b의 예시에서는 5 x 5 행렬의 피처맵이 풀링에 의하여 4x4 행렬로 크기가 줄어지는 것으로 나타내었다. 구체적으로, 레이어 2의 첫 번째 노드는 채널 1을 위한 피처 맵1을 입력으로 받아 풀링을 수행한 후, 예컨대 4x4 행렬로 출력한다. 레이어 2의 두 번째 노드는 채널 2을 위한 피처 맵2을 입력으로 받아 풀링을 수행한 후, 예컨대 4x4 행렬로 출력한다. 레이어 2의 세 번째 노드는 채널 3을 위한 피처 맵3을 입력으로 받아 풀링을 수행 한 후, 예컨대 4x4 행렬로 출력한다. 다음으로, 레이어 3의 합성곱 동작에 대해서 설명하기로 한다. 레이어 3의 첫 번째 노드는 레이어 2의 첫 번째 노드로부터의 출력을 입력으로 받아, 커널 4와 합성곱을 수행하 고, 그 결과를 출력한다. 레이어 3의 두 번째 노드는 레이어 2의 두 번째 노드로부터의 출력을 입력으로 받아, 채널 2를 위한 커널 5와 합성곱을 수행하고, 그 결과를 출력한다. 마찬가지로, 레이어 3의 세 번째 노드는 레이 어 2의 세 번째 노드로부터의 출력을 입력으로 받아, 채널 3을 위한 커널 6과 합성곱을 수행하고, 그 결과를 출 력한다. 이와 같이 합성곱과 풀링이 반복되고 최종적으로는, 완전 연결(fully connected) 레이어로 출력될 수 있다. 해 당 출력은 다시 이미지 인식을 위한 인공 신경망으로 입력될 수 있다. 지금까지 설명한 CNN은 다양한 심층신경망(DNN) 방법 중에서도 컴퓨터 비전(Vision) 분야에서 가장 많이 쓰이는 방법이다. 특히, CNN은 이미지 분류(image classification) 및 객체 검출(objection detection)과 같은 다양한 작업을 수행하는 다양한 연구 영역에서 놀라운 성능을 보였다. <CNN을 위해 필요한 하드웨어 자원> CNN과 같은 DNN은 다양한 응용 분야로 확대되기 위하여, 데이터 세트의 크기가 더 커지고 요구사항도 더 많아진 탓에, DNN은 더 깊고 더 큰 모델 크기를 갖도록 개선되었다. 이로 인해 더 많은 계산, 메모리 리소스 및 전력소비를 요구하게 되었다. 특히, CNN을 수행하는데 단점 중 하나는 매우 많은 양의 부동소수점을 연산해야 한다는 점 그리고 부동 소수점 을 위한 추가적인 파라미터가 필요하다는 점이었다. 그래서 CNN 연산은 통상적으로 하드웨어, 즉 GPU(Graphic Processing Unit)를 이용하여, 가속하는 경우가 많았다. 특히 TensorFlow, ONNX, PyTorch등의 다양한 Deep learning 개발 프레임워크들이 등장하였고, 이 프레임워크들은 사용자로 하여금 GPU를 이용하여, 연산을 쉽게 가속할 수 있게 해주었다. 하지만 GPU는 전력을 많이 소모한다는 단점이 있었기 때문에 소형 시스템에서 CNN을 수행하는데 적합하지 않았다. 그래서 GPU 보다 처리 속도는 낮지만 전력을 훨씬 덜 소모하는 하드웨어인 FPGA(Field Programmable Gate Array) 또는 ASIC(Application Specific Integrated Circuit) 기반을 이용하여 CNN을 가속 처리 하고자 연구가 진행되었다. ASIC 기반의 가속기는 통상적으로 FPGA 기반 가속기 보다 성능과 에너지 효율 두 가지 측면에서 앞 선다. 가장 큰 이유는 FPGA 보다, ASIC이 더 낮은 소비 전력과 더 빠른 클럭으로 동작할 수 있기 때문이다. 다른 한편, CNN의 특성, 즉 부동 소수점을 위하여 매우 많은 양의 파라미터를 필요로 하기 때문에 메모리 요구 량이 컸다. 2012년도 ImageNet recognition challenge에서 우승했던 CNN 구조인 AlexNet의 경우, 부동 소수점 을 위하여 약 240 MB의 파라미터를 필요로 했다. 이것은 소형 시스템의 메모리에 저장하기에는 부적합한 문제점 이 있다. 이상과 같이 DNN(Deep Neural Networks)은 많은 하드웨어 자원(예컨대, 메모리, 컴퓨팅 파워, 전력)를 이용하여, 정확도를 높여 왔다. 그러나 리소스가 제한된 임베디드 환경이 있는 애플리케이션을 위해서는, 에너지 및 하드웨어 리소스 요구 사항 과 관련하여 DNN 모델을 최적화하는 것이 매우 중요하다. <DNN 모델의 최적화 기법> DNN 모델을 메모리 및 계산 측면에서 최적화하려는 연구들이 진행되고 있다. 몇몇 연구들은 딥 러닝 모델을 최 적화하는 데 효과적인 다양한 기법들을 제시하였다. 그 중 가지치기(Pruning) 기법은 신경망에서 불필요한 가중 치를 제거할 수 있도록 한다. 양자화(Quantization) 기법은 가중치와 활성화의 비트폭(bitwitdh)을 줄일 수 있 다. 이러한 기법들은 더 가볍고 빠른 신경망을 생성할 수 있도록 한다. 양자화 기법에 따라, 가중치와 활성화 함수가 모두 1비트 정밀도로 줄어들 수 있다. 네트워크는 1비트 가중치와 활성화 함수만 사용하기 때문에 산은 매우 빠르고 가벼워질 수 있다. 그러나 네트워크 규모의 축소는, 정확도를 저하시키는 단점이 있다. 따라서 정 확도를 유지하면서 최고의 전력 및 자원 효율성으로 구현할 수 있는 이진 신경망 모델을 생성하는 것이 요구된 다. < Binarized Neural Network (BNN)> Binarized Neural Network(BNN)은 Low precision neural network 구조 에서도 극단적인 구조로서 가중치 (weight)와 레이어 입력 값이 +1/-1로 이진화(binarizing)한 구조이다. 즉, BNN은 1 비트의 파라미터로 구성된 신경망이다. BNN에서는 CNN의 Multiplication and Accumulation(MAC) 연산이 간소화되며, 복잡도가 작은 이미 지(CIFAR-10, MNIST, SVHN)에 대해서 부동 소수점을 사용한 CNN과 정확도 차이가 거의 나지 않는다 이러한 BNN은 하드웨어로 가속 처리하는데 효율적인 구조를 가지고 있다. 가장 큰 이유는 기존에 필요한 파라미 터를 적재하기 위해 필요한 메모리 크기가 약 32배 정도 줄어들었고, 이것은 on-chip RAM에 대부분을 적재하는 데 무리가 없기 때문이다. 이와 같이, BNN은 곱셈 연산을 필요로 하지 않고 메모리 사용량이 극단적으로 감소되기 때문에, 상당히 효율적 으로 하드웨어 리소스를 절감할 수 있다. 보다 구체적으로, BNN은 1 비트 연산을 수행하기 위하여 논리 연산인 XNOR 연산을 이용한다. XNOR 연산을 통해 곱셈을 구현할 수 있으며, 레지스터 내의 1로 설정된 비트의 수를 알 수 있는 Pop-Count 명령을 통해 누적 덧셈 을 구현할 수 있다. 따라서, 실수나 정수 곱셈, 덧셈이 필요하지 않게 됨으로써, 연산 속도가 증가한다. 즉, 연 산 단위가 32 비트에서 1 비트로 줄어들게 되기 때문에, 이론적으로 메모리 대역폭이 32배 증가하게 된다. 아래의 표 1은 XNOR 연산의 일 예이다. 표 1 입력 출력 a b a XNOR b 0 0 1 1 1 0 1 0 0 1 1 1 곱셈 후 누적 덧셈을 구현하기 위해서는 Pop-Count 명령을 사용하여 구현이 가능하다. Pop-Count 명령은 아래의 표 2와 같이 비트 내에서 1로 설정된 비트의 개수를 반환한다. Pop-Count 명령의 수행 결과에 2를 곱하고 총 비 트 수를 빼면 누적 덧셈이 가능하다. 표 2 8 비트 레지스터 a 1011 0100 8 비트 레지스터 b 0110 1101 a XNOR b 0010 0110 Pop-Count (a XNOR b) 3 2 * Pop-Count (a XNOR b) - 8 -2 BNN의 파라미터를 아래의 수식과 같이 이진화 한 후 N 개의 파라미터를 N 비트 레지스터에 패킹(packing)하여 N 개의 곱셈을 한번의 XNOR 논리 연산을 통해 가속화할 수 있다. 수학식 1"}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "지금까지는 BNN의 일반 개념에 대해서 설명하였다. <본 개시의 예시들> I. 서언 최근, 기계 학습(machine learning: ML)은 다양한 필드에서의 쉽게 적용될 수 있는 탓에 가장 유명한 기술이 되 었다. 특히, DNN은 컴퓨터 비전 및 음성 인식 분야에서 분류 작업을 수행하는데, 높은 정확도와 놀라운 성능을 갖는다고 입증되어 왔다. 방대한 데이터 세트를 가지면서, 더 높은 정확도를 필요로 하는 많은 애플리케이션을 위하여, 더 많은 파라미터, 더 큰 모델 크기의 레이어를 포함하는 심층 네트워크를 연구하는 경향이 있다. DNN이 복잡해질수록, 파라미터 저장을 위한 메모리 요구 사항도 증가되고, 더 많은 연산을 요구하기 때문에, 전력 및 리소스 효율성 에도 큰 영향을 미친다. 특히 증가된 연산 횟수를 처리하기 위해서는, FPGA 또는 ASIC로 구현하기 위한 설계에서, 더 많은 수의 로직 게 이트가 필요하게 되고 결과적으로 에너지 소비도 증가되는 반면, 처리 성능은 낮아진다. 한편, 대부분의 대형 DNN에 필요한 데이터는 내부(On-chip) 메모리에 완전히 저장되지 않을 수 있으므로 외부 메모리(예컨대, off- chip DRAM)에 빈번히 액세스해야 한다. 이러한 액세스는 상당한 에너지를 소비하고, 성능도 저하시키는 단점을 야기한다. 최근 연구 중에는, DNN 성능을 향상시키기 위한 최적화 방안들이 제시된 적이 있고, 이러한 여러 방안들은 잠재 적으로 중복 정보라고 판단되는 것에 대해서는 계산 정밀도를 낮추는 방안이다. 그 중 하나의 방안은 이진 신경 망(BNN)이다. 해당 방안은 모든 파라미터들을 이진화(binarizing)하기 때문에, 메모리 크기를 획기적으로 줄일 수 있고, 곱셈 연산을 XNOR 함수로 대체하기 때문에, 연산의 크기를 줄여서, 에너지 소모를 획기적으로 줄일 수 있다. 그러나, 모든 파라미터와 피처 맵 값들을 이진화(binarizing) 하는 것은 정확도를 저하시키는 단점이 있다. 상기 이진 신경망(BNN)은 요구되는 하드웨어 자원을 감소시킬 수 있어서, 인상적인 결과를 보이기도 하지만, 적 은 매개변수(parameter)를 사용하는 탓에, 정확도 저하가 불가피하다. 따라서, 구현 비용과 정확도 사이의 균형을 맞추는 것이 중요하다. 이를 위하여, 이진 신경망(BNN)의 장점을 극 대화할 수 있는, 특수 하드웨어 가속기를 설계하는 것이 요구된다. 또한, 이진 신경망(BNN)을 소프트웨어 적으 로 최적화할 수 있다. 그러나 오늘날 사용 가능한 하드웨어는 광범위하기 때문에, 소프트웨어 네트워크 구조를 독립적으로 평가하는 것만으로는 최종 네트워크 모델을 결정하기에 충분하지 않다. 따라서, 본 개시의 예시는 대상 플랫폼에서 하드웨어 구현을 위한 최상의 이진 신경망 모델을 달성하기 위해 설 계 시점의 하드웨어 성능 추정을 기반으로 하는 아키텍처 탐색 알고리즘을 제안한다. 또한, 본 개시의 예시는 XNOR-net을 기반 아키텍처 그리고, Field Programmable Gate Array(FPGA), Graphic Processing Unit(GPU), Resistive Random Access Memory(RRAM)을 포함한 대상 플랫폼을 사용하여, Floating- point Operation(FLOP) 또는 Multiply-Accumulate(MAC) 연산 보다 더 정확도를 높일 수 있는 효율적인 방법을 제안한다. 소프트웨어 수준에서 최적화 목표를 달성하기 위해 특정 소프트웨어 신경망 모델에 대한 특수 가속기에 대한 연 구가 제안되었다. 또한 MAC 연산과 같은 일반화된 메트릭을 기반으로 매개변수의 개수와 컴퓨팅 파워를 최소화 하기 위한 연구가 진행되기도 하였다. 이러한 연구들은 간략한 1차 튜닝 과정에서 평가 시간을 효율적으로 줄이 고 신경망을 구현할 때 하드웨어 오버헤드에 대해 빠르게 살펴볼 수 있게 하였다. 그러나, 종래의 연구는 특정 목표 하드웨어에 대한 세부 튜닝 과정에 필요한 자료 또는 정보를 충분하게 제공하 지 않는다. 또한 구현할 관련 모델을 선택할 때 고유한 속성으로 인해 잘못된 결정을 내릴 수 있는 단점이 있다. 보다 구체적으로, 네트워크 설계는 동일하게 되었더라도, 하드웨어 플랫폼 마다 비용(cost)가 다르게 산출될 수 있다. 또한 BNN 모델은 GPU 플랫폼 또는 FPGA 또는 ASIC과 같은 맞춤형 하드웨어 플랫폼에서 구현될 때, 하드웨 어 플랫폼이 비용(coast)에 따라 최적화된 다양한 하드웨어 작업을 제공할 수 있다. 이에 최적의 모델 구성은 크게 달라질 수 있다. 따라서 설계 시점에 하드웨어 성능을 측정하는 것은 어려운 일이지만, 해당 하드웨어 플 랫폼에 가장 적합한 신경망 모델을 찾는 것이 매우 요구된다. 본 개시에서는, 하드웨어 플랫폼을 고려한 최적의 BNN을 설계하기 위해 하드웨어 성능을 추정하고 학습 (training) 기간에 대한 아키텍처 탐색을 기반으로 궁극적인 소프트웨어 모델을 분석하고 탐색할 수 있는 새로 운 프레임워크를 제안한다. 구체적으로, 본 개시에서 제시되는 방법은 아래의 과정들을 통해서 수행된다. i) 학습(training) 이전 과정: 학습(training) 전에 비용 추정 차트가 개발된다. 비용 추정 차트는 대상 하드웨 어에서 다양한 모델을 배포하고 실제 비용을 계산 해봄으로써, 개발될 수 있다. ii) 학습(training) 과정: 넓은 초공간을 탐색하고 가변 깊이를 갖는 일련의 효율적인 BNN 모델을 제공할 수 있 는 Deepbit 방법이 제안될 수 있다. iii) 학습(training) 이후 과정: 마지막으로 비용 추정 차트를 사용하여 실제 하드웨어에서 네트워크 성능이 예 측될 수 있다. 예측을 기반으로 가장 효율적인 네트워크가 선택될 수 있다. 본 개시에서, FPGA, GPU, RRAM은 대상 하드웨어 플랫폼이고 MNIST 데이터 세트는 본 개시에서 제시되는 방법의 효과를 입증하기 위한 학습(training)을 위해서 사용될 수 있다. 비록 본 개시에서 제시하는 방법은 학습(training) 과정에서 계산량을 증가시키지만, 본 개시에서 제시하는 방 법은 DNN 모델 아키텍처 검색을 위한 정확한 메트릭(metric)을 제공할 수 있다. 컴퓨팅 성능, 전력, 온도 면에 서 리소스가 풍부한 환경인 GPU 서버에서 학습(training)이 수행된다. 따라서, 임베디드 환경으로 개발될 목표 하드웨어에 대하여 효율적인 네트워크가 도출될 수 있는 경우라면, 학습(training) 시간에서 연산량을 늘리는 것이 유리할 수 있다. 이하, 섹션 II에서는, 본 개시에서 제시하는 방법과 관련된 다양한 내용을 설명한다. 섹션 III에서는 검색 공간 을 줄이기 위해 네트워크에 사용되는 기본 설계 전략에 대해서 설명한다. 섹션 IV에서는 비용 추정 차트를 개발 하기 위해 제안된 방법을 설명한다. 섹션 V에서는 딥비트 방법을 이용한 아키텍처 탐색에 대해서 설명한다. 섹 션 IV에서는 실험 결과를 나타낸다. II. 본 개시에서 제시하는 방법과 관련된 다양한 내용 II-1. Binarized Neural Network(BNN) 최적화 하드웨어 구현을 위한 BNN 최적화는 몇 가지 카테고리로 구분될 수 있다. 첫 번째 카테고리에서, 사용 가능한 소프트웨어 신경망의 하드웨어 비용을 줄이기 위해 하드웨어 기술이 제안될 수 있다. 예를 들어, 이진 특성의 장점을 기반으로 최대 풀링을 OR 연산으로 대체한다. 또한, 일괄 정규화를 위 한 임계값인 Matrix-Vector-Threshold Unit(MVTU)를 사용하여 하드웨어 오버헤드 및 바이너리 컨볼루션 레이어 의 전력 소비를 줄일 수 있다. 또한, 이진 컨볼루션 례이어와 완전 연결 레이어 모두에서 계산량을 줄이기 위해 이진 가중치 배열 간의 차이를 사용하는 것이 제안될 수 있다. 팝-카운트(pop-count) 압축과 XNOR 기반 바이너 리 MAC 연산은 BNN 모델의 하드웨어 자원을 줄이기 위해 적용될 수 있다. 메모리 내부 컴퓨팅(in-memory computing, IMC)의 경우, 행 단위 데이터 액세스 없이 이진 신경망에서 삼진(ternary) XAC(XNOR-and- accumulate) 연산을 계산할 수 있는 혼합 신호 IMC SRAM 매크로가 제안될 수 있다. 여기서 삼진 XAC는 삼진법 (trinary digit)을 사용할 수 있다. 이러한 기술은, 기존의 디지털 하드웨어보다 더 나은 에너지 효율을 제공할 수 있다. 또한 RRAM은 동등한 XNOR 및 비트 카운팅 작업을 구현할 수 있는 시냅스 아키텍처(XNOR-RRAM)에서도 활용될 수 있다. 이러한 기술을 기반으로 하드웨어 구현 결과에서 직접 피드백을 평가하여 BNN을 효과적으로 최 적화할 수 있다. 호환되는 모든 하드웨어 기술은 BNN의 소프트웨어 기능에 영향을 주지 않고 설계를 최적화하기 위해 유연하게 사용될 수 있다. 그러나 하드웨어 최적화에만 집중하는 것은 모든 신경망에 대한 포괄적인 최적 화는 아닐 수 있다. 고유한 장점을 가진 특정 하드웨어 플랫폼에 대해 최적의 아키텍처를 제공할 수 있는 것은, 소프트웨어 최적화일 수 있다. 따라서, 두 번째 카테고리는, 소프트웨어 최적화일 수 있다. 이 카테고리의 대부분의 방법은 정확도, Floating- point Operation(FLOP), MAC 연산과 같은 일반화된 메트릭을 기반으로 소프트웨어 모델을 독립적으로 최적화하 는 것이다. 예를 들어, 네트워크의 성능을 추정하기 위해, 정확도와 학습(training) 시간을 기반으로 최적의 신 경망 모델을 탐색하기 위해 강화 학습을 사용하는 아키텍처 신경망 *?*검색이 제안될 수 있다. Transferable Architectures는 작은 데이터 세트의 학습(training) 결과를 기반으로 큰 데이터 세트에 대한 검색 공간을 줄일 수 있다. 솔루션의 효율성을 평가하기 위해 FLOP 및 정확도 기준도 비교에 사용될 수 있다. 또한 최근 DNN 모델 에서는 skip-connection, depth-wise separable convolution, Squeeze and Excitation, inverted-bottleneck, leaky-ReLU(Rectified Linear Unit)와 같은 기술을 이용하여, 높은 정확도를 제공하면서도 매개변수의 개수 또 는 MAC 작업을 줄 일 수 있다. 그러나, 최소 FLOP 또는 MAC 연산을 가진 네트워크는 실제 하드웨어 오버헤드 또 는 특정 하드웨어 플랫폼의 기타 중요한 비용을 반영하지 못할 수 있다. 세 번째 카테고리는 하드웨어 구현 결과의 피드백을 기반으로 소프트웨어 모델을 최적화하는 것이다. DNN 모델 의 추론 성능이 고려될 수 있다. 이전에는 한 가지 유형의 하드웨어 플랫폼인 모바일 장치용 ARM 아키텍처만을 고려되었다. 하지만 최근에는 다양한 모바일 CPU에 대한 직접 메트릭을 사용하기도 한다. 최적의 신경망의 효율 성을 평가하기 위해, 모바일 플랫폼에서 구현할 때 시간 지연(latency)이 크게 고려될 수 있다. 게다가, 특정 신경망의 각 레이어에 대한 양자화에 대한 정책을 자동으로 결정하기 위해 강화 학습 프로세스에 대한 시간 지 연(latency)과 에너지를 생성하는 하드웨어 시뮬레이터가 제작될 수 있다. 모바일 장치의 실제 시간 지연을 측 정하는 모델 지연을 고려하여, 자동화된 모바일 신경 아키텍처 검색이 제작될 수 있다. 게다가, 검색을 통해 정확도와 대기 시간 간의 바람직한 균형을 이루는 모델을 식별할 수 있다. 그러나 스마트 디바이스들은 GPU, CPU, FPGA, ReRam, DSP, 그리고 구글의 TPU, 인텔의 Movidius, 엔비디아(Nvidia)의 Jetson 과 같은 AI 가속기 등과 같은 다양한 하드웨어를 갖추고 있는 점을 고려해야 한다. 이러한 모든 장치는 하드웨 어 설계와 작동 특성이 다를 수 있다. 이로 인해 동일한 네트워크 아키텍처가 각기 다른 작동 특성(대기 시간, 처리량, 전력 소비, 열 등)에 따라 동작될 수 있다. 따라서 설계 과정에서 하드웨어 성능도 고려될 수 있도록 하기 위해, 대상 하드웨어에서 네트워크 아키텍처를 재귀적으로 실행하는 것도 최적의 방안은 아닐 수 있다. 일 반적으로 학습(training)을 위한 하드웨어는 대상 하드웨어와 동일하지 않을 수 있고, 학습(training) 및 하드 웨어 피드백을 포함하여 반복시에는, 실질적으로 상당한 시간이 걸리게 될 수 있다. 이러한 기존 시스템의 한계를 고려하여 본 개시는 새로운 접근 방식을 제안한다. 목표 네트워크 아키텍처를 학 습(training)하고 최적화하기 전에 제안된 방법과 하드웨어 플랫폼 유형을 기반으로, 본 개시는 개선된 비용 추 정 차트를 제시한다. 본 개시의 예시에 따르면, 일반적인 FLOP 또는 MAC 연산에 비해 훨씬 더 정확한 하드웨어 성능 추정치가 제공될 수 있다. II-2. BNN의 이론적 배경 BNN은 하드웨어-친화적인 특성으로 인하여 많은 조명을 받고 있다. 가중치와 활성화에 대하여 단지 2개의 상태 만을 가지고, BNN은 메모리 사용량과 연산량을 줄임으로써, 성능을 획기적으로 증가시킬 수 있다. 구체적으로, BNN은 가중치와 활성 출력이 양수와 음수, 즉 -1과 +1로 제한되는 인공신경망의 일종이다. 실수 변수를 이러한 값들로 변환하기 위하여, 2개의 다른 이진화(binarizing) 함수를 사용할 수 있다. 첫 번째로, 결정적 함수 (deterministic function)는 다음과 같다. 수학식 2"}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 xb는 이진화 이후 함수의 출력이다. 두번째로, 확률적 함수(stochastic function)는 다음과 같다. 수학식 3"}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, σ (x) = max(0; min(1, (x+1)/2))이다. 결정적 함수는 실질적 하드웨어 구현을 위해서 사용되는 반면, 확률적 함수도 사용된다. 위와 같은 특성을 기반으로 BNN에서는 도 3에 도시된 바와 같이 컨볼루션 및 완전 연결 레이어의 곱셈 연산을 간단한 XNOR 연산으로 단순화할 수 있으며 누적 연산을 정확도 저하 없이 팝카운트 연산으로 대체할 수 있다. 또한, 32 비트의 가중치 및 활성화에 비하여, 가중치 및 활성화가 1비트만으로 표현되기 때문에 거의 32배 감소 시킬 수 있다. 결과적으로, 메모리 액세스를 위한 전력 소비는 기존의 컨볼루션 또는 완전 연결 레이어에 비하 여, 상당히 감소될 수 있다. 도 3은 컨볼루션 레이어에서 XNOR 연산과 누적 연산을 수행하기 위한 구조를 나타낸 예시도이다. BNN에서 이진화(binarizing) 함수를 사용한다. 따라서 컨볼루션 레이어 및 완전 연결 레이어를 위한 모든 가중 치 및 출력들은 다음 연산을 위해서 사용되기 전에 하나의 비트로 줄어들게 된다. 따라서, 하드웨어 자원을 많이 소모하는 모든 곱셈 연산들은 도 3에 도시된 바와 같이 훨씬 간단해진 XNOR 로직 게이트(logic gate)으로 대체될 수 있다. 그 다음 과정인 누적을 위해서 사용되는 가산기 트리는 pop-count 수행부를 포함함으로써, 그 구조가 훨씬 간단하게 될 수 있다. 또한, MAC(multiply-accumulate) 연산은 신경망을 과부하시키는 주요 요인이었기 때문에, 배치 정규화(batch Normalization: Batchnorm)와 최대 풀링(max pool)을 이용하여, BNN 성능을 향상시킬 수 있다. 연산의 각 타입 에 대해서 적용되는 기법들은 다음과 같다. a. BNN에서 배치 정규화(Batchnorm) 및 이진화 MAC 연산과 다르게, 배치 정규화 함수는 부동소수점(floating-point) 파라미터와 나눗셈(division), 제곱근 (root square) 그리고 곱셈(multiplication)과 같은 연산을 사용한다. 일반적으로, X의 배치 정규화 된 값은 다 음과 같이 계산될 수 있다.수학식 4"}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서 ε는 반올림(round-off) 문제를 피하기 위한 작은 수이다. μ 와 var는 평균을 나타내고, 학습 데이터의 변수인 γ와 β는 학습 과정에서 얻어진 상수이다. 이와 같이 정규화 된 값 Y는 다음과 같이 이진화 될 수 있다. 수학식 5"}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "만약 이라면, 다음과 같은 수학식이 사용될 수 있다. 게다가, 하드웨어 구현에서 배치 정규화와 이진화의 조합은, 비교기 및 연속적인 XNOR 게이트의 출력이 된다. b. BNN에서 최대 풀링(pooling) 연산(operation) BNN에서, 배치 정규화 연산 이후에, 일부 레이어들은 연속적인 레이어들을 위한 입력의 활성화를 줄이기 위하여, 최대 풀링을 사용할 수 있다. 이론적으로, 최대 풀링 연산의 출력은 다음 레이어로 전달되기 전에 이진 화 될 수 있다. 이진화 모듈과 최대 풀링 모듈을 서로 교환함으로써, 배치 정규화와 이진 함수는 결합될 수 있 고, 결과 (Z)를 출력할 수 있다. 게다가, 이진 윈도우의 최대 값들을 산출하는 것은 이진 값을 입력 받아 OR 연 산의 출력을 찾는 것과 동일하다. 본 개시는, BNN을 리소스가 제한된 하드웨어에서 활용하는 방법을 제시한다. BNN의 첫 번째 레이어의 입력 픽셀 은 이진화되지 않을 수 있다. 따라서 설계 시점에 하드웨어 성능을 추정할 수 있는 아키텍처 탐색 알고리즘을 수행하여 타겟 하드웨어에 명시적으로 최적화된 BNN을 탐색할 수 있다. 이후의 섹션 III에서는 BNN의 설계 전략 에 대해 자세히 설명하기로 한다. 또한 설계 시 아키텍처 탐색 알고리즘 및 하드웨어 비용 추정에 대해서도 설 명한다. III. 기본 설계 전략 (Basic Design Strategies) DNN 모델 아키텍처 검색은 검색 공간이 무한히 크기 때문에, 계산하는데 상당한 자원을 소모한다. 따라서, 본 개시는 몇 가지 기본 디자인 원칙을 정의하여 검색 공간을 줄이는 방안을 제시한다. 이에 따르면, 검색 공간이 효과적으로 좁혀질 수 있고, 아키텍처 검색의 계산 복잡성을 상당히 줄일 수 있다. 일반적으로 본 개시에서는, MNIST 데이터셋과 BNN(Binarized Neural Network)를 사용한 실험 결과를 제시한다. 본 개시의 예시에 따르면, 첫 번째 컨볼루션 레이어의 입력 픽셀을 제외하고 모든 가중치와 활성화가 이진화 될 수 있다. 또한 본 개시의 예시에 따르면, 이미지 분류(image classification) 작업을 지원하는 신경망을 위한 효율적인 네트워크 아키텍처를 찾기 위해, 신경망의 끝단에 존재하는 하나의 완전히 연결된 레이어를 유지시키 면서, 컨볼루션 레이어만 조정하는 안을 제시한다. 한편, 계산량을 더 줄이기 위하여, 실험적 신경망에는 기본적인 설계 전략으로 잘 알려진 풀링 기법도 추가될 수 있다. 섹션 I절에서 언급한 바와 같이 MAC 연산의 개수가 궁극적인 구현 모델을 결정하는 기준은 아니지만, 간략한 튜닝 단계에서 탐색 공간을 좁히는 데 도움을 줄 수 있다. 아래의 표 3은 풀링 레이어의 추가 효과를 나타내고, 표 4는 최대 풀링과 평균 풀링의 차이를 나타낸다. 마지막 2개의 컨볼루션에 대해 최대 2개의 풀링 레이어를 추가하면, 정확도에 약간의 영향을 미치며 0.1% 저하와 38%로MAC 작업을 크게 감소시킬 수 있다. 한편, 하나의 풀링 레이어를 추가하면 MAC 작업의 수를 미미하게 줄일 수 있다. 따라서 본 개시에서는 아키텍처 탐색 알고리즘은 두 개의 풀링 레이어를 사용하는 것을 선택하였다. 또한 검색 공간을 위해 풀링 레이어의 유형도 고려될 수 있다. 표 3 Pooling Layers (#) Channels per Layers (#) Accuracy (%) MAC Operations 0 32 98.54 29352960 1 32 98.48 28976640 2 32 98.44 18063360 표 4 Type of pooling Channels per Layers (#) Accuracy (%) Average 32 98.35 Max 32 98.44 최대 풀링 레이어 만을 사용한 모델의 정확도와 평균 풀링 레이어를 사용한 모델의 정확도가 표 4에 비교되어 나타나 있다. 평균 풀링보다, 최대 풀링이 더 높은 정확도를 나타내는 것을 알 수 있다. 본 개시에 다르면, 검색 공간은 MNIST 데이터 세트에 대해 높은 정확도를 제공하기에 적당한 3x3 커널 크기를 가진 표준 컨볼루션 레이어 만을 고려할 수 있다. 한편, 높은 메모리 액세스를 방지하고 검색 알고리즘의 검색 시간과 복잡성을 줄이기 위해, point-wise, depth-wise convolution, 잔여 신경망 및 3x3보다 큰 커널 크기는 검색 공간에서 고려되지 않을 수 있다. 단, 본 개시는 point-wise, depth-wise convolution, 잔여 신경망 및 3x3보다 큰 커널 크기의 팩터를 제외하지 않는다. 활성화 함수와 관련하여 배치 정규화 hard-Tanh 활성화 함수 가 각 컨볼루션 레이어에 적용될 수 있다. 아키텍처의 기본 모듈은 섹션 II에서 설명되었다. 설계 고려 사항에 따라 최적의 아키텍처를 찾기 위해 레이어 개수와 채널의 개수가 조정될 수 있다. 학습 리소스에 따라 설계 전 략이 달라질 수 있다. 본 개시에서는 설계 전략에 따라 레이어 당 채널 측면에서 6개 모델을 최적화하려고 시도 하며, 각 모델은 깊이(즉, 레이어의 개수)가 3에서 8 사이일 수 있다. 더 많은 학습 리소스가 주어지면, 총 12 개의 모델, 즉 마지막 2개 레이어에서 최대 풀링이 있는 6개 모델과 그리고 마지막 두 레이어에서 평균 풀링이 있는 6개의 모델을 얻을 수 있다. IV. 하드웨어 비용 추정 (Hardware Cost Estimation) 본 절에서는, 비용 추정 차트를 개발하는 데 사용되는 방법을 제시한다. 이전 섹션에서 언급했듯이 설계 전략을 기반으로 하드웨어 구현에 가장 적합한 신경망 모델을 찾기 위해 아키텍처 검색 알고리즘은 네트워크의 적절한 깊이(레이어의 수, 즉 L)와 각 레이어의 채널의 개수(즉, C), 그리고 레이어(채널 수)를 식별하는 역할만을 수 행할 수 있다. 결과적으로 탐색 알고리즘에 대한 비용 추정 차트를 개발하기 위해 네트워크 깊이(즉, 레이어의 개수)와 채널의 개수에 대한 범위가 준비되어야 할 수 있다. 특히, 네트워크 채널의 개수의 범위는 5에서 50 사 이의 범위로, 간격은 5로 정의될 수 있고, 네트워크 깊이(즉, 레이어의 개수)는 3에서 10 사이의 범위로 다음과 같이 정의될 수 있다. 레이어의 개수 L , 즉 깊이(depth) = 3, 4, 5, 6, 7, 8, 9, 10 채널의 개수 C, 즉 너비(width) = 5, 10, 15, 20, 25, 30, 35, 40, 45, 50 도 4a 및 도 4b는 네트워크 아키텍처의 작성 모듈을 나타낸다. 구체적으로 도 4a에는 본 개시에서 제시되는 네트워크 아키텍처를 위한 레이어 모듈이 나타나 있고, 도 4b는 최 대 풀링(max pooling)을 갖는 네트워크 아키텍처를 위한 레이어 모듈이 나타나 있다. 아래의 표 5는 전력 소모에 대한 비용 추정 차트를 나타낸다. 표 5 3 4 5 6 7 8 9 10 ↑ ↑ 채널(C) ↓ ↓ ↓10 0.6110.6760.74 0.7910.836 15 0.7210.8770.9851.1271.26 1.4221.561 200.77 1.0531.2811.4941.7181.9832.2512.499 250.9661.1171.6922.1432.525 301.4292.079 ← 레이어 (L) → 위 표의 값들은 와트(watt)를 나타낸다. 전술한 범위에 기초하여, 위 표 5에 나타난 바와 같은 세트 layer(L)과 세트 channel(C)의 Cartesian 곱에 의해 서 주어지는 LxC BNN 모델들이 하드웨어 평가 및 학습 과정을 위하여 제공될 수 있다. 여기서 L은 합성곱 레이 어의 개수이고, C는 각 레이어에서의 채널의 개수를 나타낸다. 보다 구체적으로, 모든 모델들은 임의(random) 가중치에 의해서 초기화될 수 있고, 실제 하드웨어 비용을 알아내기 위하여 목표 하드웨어 플랫폼에 배치될 수 있다. 목표 하드웨어 플랫폼에 따라, 비용 메트릭은 다른 기준으로부터 얻어질 수 있지만, 대응하는 목표 하드 웨어의 대부분의 중요 자원을 나타낼 수 있다. 그러나, FPGA에서, 전력 소모, LUT(Look Up Table) 그리고 플리- 플롭(flip flop)은 비용 메트릭을 위해서 사용되는 조건들일 수 있다. 이러한 비용들에 기초하여, 각기 대응하 는 정확도에 대한 최고의 소프트웨어 모델을 찾기 위한 탐색 시간 동안에 하드웨어 비용 추정이 수행될 수 있다. 이는, 최적 모델을 학습시킨 이후에, 엄격한 하드웨어 테스트를 수행 정도를 저감 시킬 수 있다. 비용 추 정 차트는 특정 하드웨어 플랫폼에 대해서 개발될 필요가 있다. 단 하나의 조건은 아키텍처 탐색에 대한 탐색 공간이 비용 추정 차트의 탐색 공간의 일부여야 한다는 점이다. 도 5a 및 도 5b와 표 5는 BNN 모델의 전력 소모 에 대한 예시적인 차트일 수 있다. 도 5a 및 도 5b는 FPGA 상에서 서로 다른 L 값과 C 값을 갖는 BNN 모델들의 리스트에 대하여, 하드웨어 및 전력 비용 추정을 나타낸다. 구체적으로, 도 5a는 FPGA 상에서 서로 다른 L 값과 C 값을 갖는 BNN 모델들의 리스트에 대하여 하드웨어 구현 비용을 나타낸다. 도 5a를 참조하면, 레이어의 개수가 3일 때, 레이어 당 채널의 개수가 증가되면, LUT의 개수 혹은 플리-플롭의 개수가 증가되어 결과적으로 하드웨어 구현 비용이 증가됨을 알 수 있다. 마찬가지로, 레이어 의 개수가 4일 때에도, 레이어 당 채널의 개수가 증가되면, LUT의 개수 혹은 플리-플롭의 개수가 증가되어 결과 적으로 하드웨어 구현 비용이 증가됨을 알 수 있다. 또한, 레이어의 개수가 3일 때 하드웨어 구현 비용에 비하 여 레이어의 개수가 4일 때 하드웨어 구현 비용이 더 큼을 알 수 있다. 도 5b는 FPGA 상에서 서로 다른 L 값과 C 값을 갖는 BNN 모델들의 리스트에 대하여 전력 소모를 나타낸다. 도 5b를 참조하면, 레이어의 개수가 3일 때, 레이어 당 채널의 개수가 증가되면, 전력 소모가 증가됨을 알 수 있다. 마찬가지로, 레이어의 개수가 4일 때에도, 레이어 당 채널의 개수가 증가되면, 전력 소모가 증가됨을 알 수 있다. 또한, 레이어의 개수가 3일 때 전력 소모에 비하여 레이어의 개수가 4일 때 전력 소모가 더 큼을 알 수 있다. V. DeepBit를 이용한 아키텍처 탐색 대응하는 하드웨어 플랫폼을 위하여 최적의 BNN 아키텍처를 탐색하기 위하여, 본 절에서는 DeepBit로 명명된 아 키텍처 탐색 알고리즘에 대해서 설명하기로 한다. DeepBit라고 불리는 아키텍처 탐색 알고리즘의 입력은 아래와 같다. 1. BNN 모델의 깊이(즉, 컨볼루션 레이어의 개수)에 대한 범위를 세트 L로 아래와 같이 정의한다. 세트 L = {3, 4, 5, 6, 7, 8} 2. 레이어 당 균일한 채널의 최대 개수는 일 실시예에 따르면, 50으로 설정될 수 있다. 3. 수락 가능한 모델에 대한 임계값은 대체로 수락가능한 최소 정확도에 대한 임계 값일 수 있다. 알고리즘의 출력은 일련의 모델들일 수 있다. 각 모델은 레이어 당 최소 채널 개수를 갖는 최적의 BNN 모델일 수 있고, 레이어들의 개수는 L 입력 세트에서 대응하는 값일 수 있다. 따라서, 출력 모델의 개수는 세트 L의 사이즈와 같을 수 있다. DeepBit 알고리즘은 3개의 단계를 포함할 수 있다. 첫 번째 단계를 위한 알고리즘이 도 6에 나타나 있고, 두 번째 단계를 위한 알고리즘이 도 7에 나타나 있고, 세 번째 단계를 위한 알고리즘이 도 8에 나타나 있다. 도 6은 주어진 BNN 모델에서 최적의 균일한 채널 검색을 위한 알고리즘 1을 나타낸다. 첫 번째 단계에서, 임의의 주어진 모델에 대하여 네트워크의 각 컨볼루션 레이어에서 출력 채널 C의 개수 (즉, 너비(width))는 각 레이어에 대해서 균일하게 줄어들 수 있다. 다르게 말하면, 주어진 모델에 대해서, 네트워크 의 채널 개수는 모든 레이어에서 상수로 존재할 수 있다. 상기 첫 번째 단계는 세트 L 내에서 최소 값과 동일한 값을 갖는 깊이(즉, 레이어의 개수)를 갖는 모델을 정의함으로써 개시될 수 있다. 상기 네트워크의 채널의 개수 는 상기 언급된 두 번째 입력(즉, 일 실시예에 따르면, 50)에 의해서 정의될 수 있다. 이진(binary) 검색 알고 리즘은 상기 세 번째 입력으로 정의되는 최소 정확도 임계 값과 일치하는 최소 균일 채널의 개수를 갖는 BNN 모 델을 탐색하는 것이다. 도 7은 주어진 BNN 모델에서 최적의 채널의 개수 검색(Optimal width search)을 위한 알고리즘 2을 나타낸다. 두 번째 단계에서, 각 레이어에 대하여 최적의 채널 개수 C를 찾기 위한 탐색이 수행될 수 있다. 이 두번째 단 계는 첫 번째 단계의 출력에 대해서 수행될 수 있다. 이 두번째 단계에서, 일 실시예에 따르면 네트워크의 채널 의 개수는 첫 번째 단계에서 찾아진 네트워크의 최대 채널의 개수에 따라 축소될 수 있다. 첫 번째 레이어에 대 하여 최적의 채널의 개수를 찾기 위하여 이진(Binary) 탐색이 다시 한번 사용될 수 있고, 다른 레이어에 대한 채널의 개수는 최적의 균일한 채널의 개수에 대하여 상수로 설정될 수 있다. 일단 첫 번째 레이어에 대한 최적 의 채널의 개수가 찾아지면, 나머지 레이어의 최적 채널의 개수를 찾기 위하여 동일한 과정이 수행될 수 있다. 일반적인 레이어를 탐색하는 과정에서, 나머지 레이어들의 채널의 개수는 초기 값으로 유지될 필요가 있다. 모 든 컨볼루션 레이어에 대한 최적 채널의 개수 값들은 저장되고 다음 단계를 위하여 재사용될 수 있다. 도 8은 최적의 채널의 개수 (즉, 최적의 너비 설정(optimal width configuration))에 기초한 최적의 BNN 탐색 을 위한 알고리즘 3을 나타낸다. 탐색 알고리즘의 마지막 단계인 세번째 단계에서, 두 번째 단계로부터 전달되는 각 레이어의 최적 채널의 개수 값은 최적 BNN 모델을 정의하기 위해서 사용될 수 있다. 특정 레이어의 최적 채널의 개수를 탐색하는 동안 모든 레이어들의 채널의 개수가 균일하게 되도록 설정되었기 때문에, 각 레이어의 최적 채널의 개수들을 모두 조합할 때, 정확도 저하가 예상될 수 있다. 정확도 저하를 보상하기 위하여, 각 레이어의 채널의 개수는 각 레이어에 대해서 2개 이상의 채널 만큼 증가될 수 있다. 만약 임의 레이어의 채널의 개수가 첫 번째 단계에서의 초기 값 보다 큰 경우, 최적 값에 2를 더한 값을 사용하는 것 대신에, 상기 초기 채널의 개수가 이 레이어를 위해서 적 용될 수 있다. 다음으로, 학습(training) 과정은 재설정된 채널의 개수 값을 갖는 새로운 BNN 모델 상에서 수행 될 수 있다. 정확도가 임계 값과 일치하는 경우, 상기 모델이 주어진 초기 깊이(즉, 레이어의 개수)를 갖는 최 적 BNN 모델로 선택될 수 있다. 만약 정확도가 임계값 보다 작은 경우, 각 레이어의 채널의 개수는 1씩 증가될 수 있고, 상기 모델은 정확도를 체크하기 위하여 재학습 될 수 있다. 이 과정은 사이 임계 값과 일치하는 정확 도를 갖는 최적 모델이 찾아질 때까지 또는 임의 레이어의 채널의 개수가 상기 첫 번째 단계의 초기 균일 채널 의 개수 값과 동일하게 될 때까지, 반복될 수 있다. 이는 세트 L 내의 모든 깊이(즉, 레이어의 개수) 값에 대해 서 적용된다. 결과적으로, 최적 채널의 개수 설정을 갖는 일련의 BNN 모델들과 가변 깊이(즉, 레이어의 개수)가 생성될 수 있다. 이에 따라, 3부터 8까지의 깊이(즉, 레이어의 개수)를 갖는 네트워크가 만들어질 수 있다. 도 9는 DeepBit 아키텍처 탐색 알고리즘 4를 나타낸다. 마지막으로, 전술한 세 번째 단계로부터 얻어진 일련의 최적 BNN 모델들에 기초하여 가장 하드웨어-친화적인 모 델을 찾기 위하여, 섹션 IV에서 설명한 하드웨어 비용 추정 방법에 따라 각 모델에 대한 목표 하드웨어 비용이 추정될 수 있다. 일반적으로 하드웨어 비용 차트에서, 모든 모델은 레이어 당 채널의 개수를 나타내기 위하여 균일한 채널의 개수를 사용한다. 이는, 레이어 당 채널의 개수가 모델의 균일 채널의 개수와 동일함을 의미한다. 게다가, 최적 모델을 위한 하드웨어 비용을 평가하기 위하여, 레이어 당 채널의 평균 개수는 모델의 최적 채널의 개수와 동일한 역할을 수행할 수 있고, 레이어 당 채널의 평균 개수는 하드웨어 추정 차트에 기반 한 최적 모델을 찾는데 사용될 수 있다. 추정 차트가 모든 최적 채널의 개수 값에 대한 하드웨어 비용을 모두 보여주지 않기 때문에, 선형 회귀법(linear regression method)은 각 모델에 대한 하드웨어 비용을 추정하기 위 해서 추가적으로 사용될 수 있다. 2개 이상의 모델이 상기 탐색 과정 동안에 레이어 당 채널의 평균 개수를 동일하게 갖을 때, 정확하게 평가하기 위해서 대응하는 하드웨어 디자인이 구현될 수 있다. 전체 과정 이후에, 최 소 하드웨어 비용을 갖는 BNN 모델이 선택될 수 있다. DeepBit 아키텍처 탐색 알고리즘이 도 9에 나타나 있다. VI. 실험 결과 본 절에서는 제안된 방법의 효용성을 증명하기 위하여 실험을 수행한 결과를 설명한다. 상기 제안된 방법에 기 초하여 최적의 BNN 아키텍처를 찾기 위하여, GPU, RRAM, 그리고 FPGA가 목표 플랫폼으로 사용될 수 있다. 타겟 플랫폼의 타입에 따라, 대응 하드웨어 비용 추정이 민감한 자원을 위하여 준비될 수 있다. 도 10은 FPGA을 이용한 BNN 하드웨어 아키텍처를 나타낸 블록도이다. 도 10을 참고하여 알 수 있는 바와 같이, BNN 하드웨어 아키텍처는 BNN 전용 가속기(즉, BNN 전용 NPU)를 포함할 수 있다. 상기 BNN 전용 가속기(즉, BNN 전용 NPU)는 첫 번째 레이어를 위한 제1 블록, 두번째 레이어를 위한 제2 블록, i번째 레이어를 위한 제3 블록 그리고 n번째 레이어를 위한 제4 블록을 포함할 수 있 다. 도 10에는 예시적으로 첫 번째 레이어(Layer 1로 도시됨)와 두번째 레이어(Layer 2로 도시됨)는 컨볼루션 레이어이고, i번째 레이어(Layer i로 도시됨)는 컨볼루션 레이어이고, n번째 레이어(Layer n로 도시됨)는 완전 연결 레이어인 것으로 나타내었다. 각 레이어를 위한 블록은 라인 버퍼와, XNOR 로직 게이트(logic gate)와, 팝-카운트(pop-count) 명령 처리부와, 배치 정규화(Batch normalization) 수행부를 포함할 수 있다. 예를 들어, 첫 번째 레이어를 위한 제1 블록(11 0)은 라인 버퍼, XNOR 로직 게이트와, 팝-카운트 명령 처리부와, 배치 정규화 수행부를 포 함할 수 있다. 이전 레이어로부터 전달되는 픽셀들의 값들은 각 블록의 라인 버퍼로 전달된다. 예를 들어, 제1 블록의 라 인 버퍼는 픽셀들의 값들을 XNOR 논리 게이트로 전달한다. 구체적으로, 일정 개수의 픽셀 값들이 라 인 버퍼 내에 로딩되면, 상기 라인 버퍼는 윈도우 값들을 생성하고, 상기 XNOR 논리 게이트로 전달한다. 상기 XNOR 논리 게이트의 출력은 상기 팝-카운트 명령 처리부로 전달된다. 상기 팝-카운트 명령 처리 부는 XNOR 논리 게이트의 결과들을 덧셈한 후, 비트 쉬프트를 통해 곱하기 2를 수행한 후, 총 비트수 (vec_len)에 대한 차감을 수행함으로써, 누적 덧셈을 구현할 수 있다. 상기 배치 정규화 수행부는 임계 값과 비교를 수행한다. 인공 신경망의 오직 1개 레이어 만을 처리할 수 있었던 종래의 인공신경망 가속기(예를 들어, 일반적인 NPU)와 달리, 본 개시에서 제시되는 BNN 전용 가속기(즉, BNN 전용 NPU)는 전체 인공 신경망을 하드웨어로 구현할 수 있다. 본 개시에서 제시되는 BNN 전용 가속기(즉, BNN 전용 NPU)는 파이프라인 형태의 스트리밍 아키텍처에 기반 한다. 즉, 본 개시의 예시에 따른 BNN 전용 가속기(즉, BNN 전용 NPU)는 추론을 수행하면서 발생되는 부하 를 레이어들로 분산한다. 파이프라인의 개수는 레이어의 개수와 같다. 따라서, 입력 이미지의 픽셀들이 계속 수 신된다면, 모든 레이어들은 동시에 동작할 수 있고, 매우 높은 성능을 낼 수 있다. 추가적으로, 이전 레이어의 출력이 중간 저장 없이 바로 다음 레이어로 전달되기 때문에, 전달 지연을 줄일 수 있고, 필요로 하는 메모리의 크기를 획기적으로 줄일 수 있다. 한편, 모든 레이어들은 다른 하드웨어 모듈로 구현되기 때문에, 입력 데이터는 중단 없이 계속하여 처리될 수 있다. 레이어의 개수가 증가하게 되면 파이프라인만 늘리면 되므로, 성능 저하가 발생하지 않을 수 있다. 예를 들어, E*F 크기의 이미지가 입력된다고 가정하면, 상기 입력 이미지의 데이터는 매 클럭 사이클 마다 전달될 수 있다. 이 경우, 본 개시의 예시에 따른 BNN 전용 가속기(즉, BNN 전용 NPU)는 E*F 클럭 사이클 만에 이미 지를 분류하는 추론을 끝낼 수 있다. 결과적으로, 본 명세의 개시에 따른 BNN 전용 가속기(즉, BNN 전용 NPU)의 성능은 매 클럭 사이클 마다 입력되는 이미지 픽셀의 개수에 따라서 유연하게 상향될 수 있다.구체 적으로, 각 레이어를 위한 블록들은 파이프라인 기법 하에서 구현될 수 있다. 부연 설명하면, 하드웨어 모듈은 FPGA로 성능 검증 후, ASIC으로 양산될 수 있다. 도 10을 참고하여 알 수 있는 바와 같이, 전술한 방법을 평가하기 위하여, FPGA 관점에서, BNN 스트리밍 아키텍 처가 구현될 수 있다. 구체적으로, 본 개시는 BNN의 각 레이어가 개별 하드웨어 블록에 의해서 가속되게끔 설계된, 멀티-레이어 아키텍처를 제시한다. 거친(coarse-grained) 파이프라인 방식에서, 추론 부하를 레이어 단위로 분할하는 설계가 채용될 수 있다. 이러한 파이프라인 방식에서, 파이프라인 단계들의 개수는 레이어의 개수와 같다. 도 10을 참조하면, 레이어 레벨에서는 병렬화 기법을 동반한 파이프라이닝이 효과적으로 적용된다. 일반적으로 경량의 가중치 라인 버퍼는 레이어 간의 데이터 전달을 담당하는 반면 모든 곱셈 및 누적은 특정 병렬 수준의 XNOR-pop count에 의해 처리될 수 있다. 반면 이진 가중치는 전체 정밀도 가중치보다 훨씬 작은 메모리를 사용 하기 때문에 모든 매개변수는 FPGA의 레지스터에 완전히 저장될 수 있다. 이 접근 방식을 사용하는 설계는 전력 소모, 외부 메모리 및 온칩 메모리로부터 데이터 전송의 시간 지연을 감소시킬 수 있다. 하드웨어 장치 측면에 서 BNN 아키텍처는 xczu3eg-sbva484-1-e Ultrascale+ MPSoC를 포함하는 Ultra96 평가 보드에서 구현될 수 있다. 특히 PS(Process Subsystem)는 쿼드 코어 Arm Cortex-A53 애플리케이션 처리 장치와 듀얼 코어 Arm Cortex-R5 실시간 처리 장치로 구현될 수 있다. 프로그래머블 로직(PL)의 경우 141,120개의 플립플롭, 70,560개 의 LUT(룩업 테이블), 360개의 DSP 슬라이스 및 7.6Mbits 블록 램으로 구현될 수 있다. 레지스터 전송 레벨 (RTL) 설계는 Synopsys VCS에서 시뮬레이션 된 다음, Vivado 2018.3에서 구현될 수 있다. LUTS 개수, 플립플롭 개수, 예상 전력 소비를 설명하는 모든 실험 결과는 Vivado의 보고서에 기록될 수 있다. RRAM 실험의 경우 물리 적 RRAM 장치가 이용되지는 않았다. 따라서 PyTorch에서 RRAM 시뮬레이터가 구현되고, RRAM 장치에 의해 주입된 에러가 시뮬레이터에서 복제될 수 있다. 소프트웨어 시뮬레이터를 기반으로 실제 RRAM 장치에서 최적으로 수행 되는 BNN 모델을 찾을 수 있다. GPU 실험을 위해 우리는 RTX 2080TI를 사용할 수 있다. 마지막으로 해당 대상 플랫폼에 대한 모든 실험에 대한 학습 프로세스는 GPU에서 수행될 수 있다. IV-1. 최적 BNN 탐색 섹션 III에서 설명한 기본 설계 전략과 DeepBit 기법에 기초하여, 본 절에서는 특정 설정을 이용하여 수행되는 실험 상에서 탐색 과정을 설명한다. 게다가, 대응하는 결과들이 제안된 방법의 효용성을 입증하기 위해서 설명 된다. 특히 FPGA의 경우 최적의 BNN 아키텍처의 목표는 최소한의 하드웨어 리소스로 98.4% 정확도의 성능을 보 이는 것이다. 한편, GPU 및 RRAM의 목표는 단순히 최대 정확도로 BNN 아키텍처를 개발하는 것이다. 이전 섹션에 서 언급했듯이 DeepBit 방법은 아래와 같이 세 가지 입력을 수신한다. 1. L = {3,4,5,6,7,8} 2. 채널의 최대 개수 = 50 3. 최소 정확도 임계값 = 98.4% 학습 과정에서, Adaptive Moment Estimation(Adam)과 Stochastic gradient descent(SGD) 옵티마이저의 조합이 적용될 수 있다. 보다 구체적으로, Adam 옵티마이저는 처음 30개의 epoch에 대해서 사용된 다음 SGD 옵티마이저 가 나머지 70개의 epoch에 대해서 사용될 수 있다. 학습률과 운동량은 0.03과 0.5로 각기 설정될 수 있다. 각 학습은 아키텍처 탐색 방법의 첫 번째 단계 와 두번째 단계에 대해 10회 수행될 수 있다. 최종 단계에서는 최종 모델을 찾는 시점에서 학습이 총 100번 반복될 수 있다. 3에서 8까지의 깊이에 해당하는 6개의 최적 BNN 모델이 제시될 수 있다. 각 모델에 대한 채널의 개수 구성이 표 6에 나타나 있다. 반면에 섹션 III에서 언급한 것처럼 BNN 모델에 대한 레이어 모듈은 도 4a와 도 4b에 나타나 있다. 각 모듈은 컨볼루션 연산과 배치 놈 및 hard- Tanh 활성화 함수를 포함할 수 있다. 게다가 마지막 2개의 convolution layer는 추가적으로 Max pooling layer 를 포함하며, 모든 BNN 모델의 마지막 layer는 항상 full connected layer로 구성될 수 있다. 마지막으로 BNN 모델의 각 레이어에 대한 최적의 채널의 개수는 DeepBit 아키텍처 검색 방법으로 찾을 수 있다. VI-2. 최적 모델에 대한 하드웨어 비용 추정 하드웨어 비용을 추정하기 위해 세션 IV에서 설명한 방법을 기반으로 각 하드웨어 플랫폼에 따라 가장 민감한 하드웨어 자원을 선택하여 추정한다. 특히 FPGA의 경우에는 하드웨어 비용을 추정하기 위하여 LUT(Look Up Table), Flip Flops, 소비 전력량이 선택될 수 있다. 네트워크 깊이(즉, 레이어의 개수)와 채널의 개수의 범위 는 다음과 같습니다. L = {3, 4, 5, 6, 7, 8} C = {5, 10, 15, 20, 25, 30} 아래의 표 6은 DeepBit 방법에 의해서 찾아진 최적 BNN들의 채널의 개수 설정을 나타낸다. 표 6 md1 md2 md3 md4 md5 md6 3 Layers4 Layers5 Layers 6 Layers7 Layers8 Layers Layers ChannelsChannels Channels ChannelsChannelsChannels 1 26 23 20 19 11 8 2 24 21 21 19 9 15 3 31 22 21 14 20 17 4 21 21 16 15 17 5 20 18 20 17 6 19 20 10 7 20 17 8 17 MAC OPs 5989368 7756896 10206896 8916432 9964640 9854684 세트 L과 세트 C에 의해 정의된 깊이(즉, 레이어의 개수)와 채널의 개수로 가능한 모든 모델은 임의의 가중치 값으로 초기화될 수 있다. 다음으로 이러한 모델은 해당 대상 하드웨어 플랫폼에서 물리적으로 테스트될 수 있 다. 구현된 결과를 바탕으로 검색 방법에 대한 하드웨어 추정 차트가 작성될 수 있다. 표 7 내지 9에서 Look Up Table, Flip-Flops, Power Consumption을 기반으로 한 비용추정 차트가 FPGA 플랫폼에 대해서 제공될 수 있다. 구체적으로, 5개 채널의 너비 간격을 갖는 다양한 모델에 대한 실제 하드웨어 비용이 사전에 준비될 수 있다. 이전 섹션에서 설명한 바와 같이, 비용 추정 차트에서는 각 모델이 각 레이어에 대해 균일한 채널의 개수(즉, 너비)를 가지므로 레이어당 평균 채널의 개수(즉, 너비)를 최적의 BNN 모델의 균일한 채널의 개수(즉, 너비)로 간주할 수 있다. 하드웨어 비용 추정 차트에 존재하는 균일한 채널의 개수(즉, 너비)와 레이어 개수로 최적의 모델에 대한 실제 하드웨어 비용을 찾아낼 수 있다. 이 경우 최적의 채널의 개수(즉, 너비)가 추정 차트에 없는 경우 선형 보간법을 적용하여 최적 모델의 하드웨어 비용을 추정할 수 있다. 예를 들어 표 6과 같이 3레이어의 최적 모델을 사용할 수 있다. 아래의 표 7은 LUT 추정 차트를 나타낸다. 표 7 3 4 5 6 7 8 9 10↑ ↑ 채널(C) ↓ ↓ ↓10 919210329119351238113259 15 11003133401554617823200582226824592 201264816230193872329926911311883882743374 251684522801292063853545095 302546535192 ←레이어 (L)→ 아래의 표 8은 플리-플롭(Flip Flop) 추정 차트를 나타낸다. 표 8 3 4 5 6 7 8 9 10↑ ↑ 채널(C) ↓ ↓ ↓10 60376732786285908859 15 70768288951110755115511272013975 2074499157104831225813987162311755319777 25872311123139701637918788 301129614528 ← 레이어 (L) → 아래의 표 9는 소비 전력 추청 차트를 나타낸다. 표 9 3 4 5 6 7 8 9 10↑ ↑ 채널(C) ↓ ↓ ↓10 0.6110.6760.74 0.7910.836 15 0.7210.8770.9851.1271.26 1.4221.561 200.77 1.0531.2811.4941.7181.9832.2512.499 250.9661.1171.6922.1432.525 301.4292.079 ← 레이어 (L) → 모델의 레이어 당 평균 너비는 27일 수 있다. 레이어당 25개 채널이 있는 BNN 모델의 경우 비용 추정 차트는 하 드웨어 비용 C1을 보여주고, 레이어당 30개의 채널이 있는 BNN 모델의 경우 비용 추정 차트는 하드웨어 비용 C2 를 보여줍니다. 마지막으로 모델(C_est)의 하드웨어 비용은 다음 공식과 같이 선형 보간법으로 추정될 수 있다. 수학식 8"}
{"patent_id": "10-2022-0133807", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "추정된 방법으로 찾은 최적의 BNN 모델의 하드웨어 비용이 표 12에 나타나 있다. 따라서 4개의 레이어를 갖는 모델은 가장 적은 수의 LUT(18,764)와 에너지(1.151)를 소비하는 반면, 3개의 레이어를 갖는 모델은 플립-플롭 을 가장 적게 소비할 수 있다. 최적의 모델 간의 MAC 연산 회수를 비교하면, MAC 연산이 최소인 모델이 FPGA에 서 하드웨어 구현에 최적일 수도 있고 아닐 수도 있다는 결론을 내릴 수 있다. 따라서 소프트웨어 모델에 대한 실제 하드웨어 비용을 추정하는 것은 특정 대상 플랫폼에서 구현하기 위한 최상의 모델을 탐색하는 데 정말 중 요합니다. GPU 및 RRAM 장치와 관련하여 실험 범위에서 아키텍처 검색의 유일한 목표는 정확도이다. 따라서 최대 정확도를 갖는 최적의 BNN 모델은 GPU 및 RRAM에 대한 아키텍처 검색의 출력일 수 있다. 특히 GPU에 대한 정확도 차트는 표 10에 나타나 있다. 다른 장치와 달리 GPU의 계산 비용은 MAC 연산 회수에 따라 달라질 수 있다. 따라서 최적 의 BNN 모델은 MAC 연산 횟수가 가장 적고 정확도가 가장 높은 모델일 것이다. 결과에 따르면 채널 개수가 40개 를 초과하고 레이어 개수가 5개 이상일 때, 정확도가 포화되는 경향이 있다. 즉, 실험에 따르면, 채널 또는 레 이어 개수의 증가(MAC 연산이 증가함)는 정확도를 항상 높일 수 있는 것은 아니고, 되려 계산에 부담을 줄 수 있다는 것을 알 수 있다. 실험에 따르면 GPU에 대한 최적의 BNN 모델은 최고의 정확도(98.88%)와 최적의 MAC 연 산 회수를 제공하는 6개의 컨볼루션 레이어와 50개의 출력 채널을 가진 모델임을 알 수 있다. RRAM 장치의 경우 RRAM의 작동 메커니즘을 에뮬레이트(emulate)하기 위하여 시뮬레이터가 준비될 수 있다. 여기 서 I/R 드롭, 백색 잡음으로 인한 에러가 시뮬레이터에 주입된다. 이론적으로 RRAM 장치에 구현된 컨볼루션 레 이어에서 채널 개수가 많을수록 더 큰 오류가 발생할 수 있다. 이 특성을 이용하여 시뮬레이션이 수행되고, RRAM 장치에 대한 정확도 추정은 표 11과 같이 정리될 수 있다. 따라서 RRAM 오류의 영향이 미미하더라도 10개 채널로는 높은 정확도를 제공하기에는 부족할 수 있다. 한편, 50개 채널 모델은 오차의 영향이 크므로 레이어 당 채널 개수가 너무 많아도 정확도가 향상되지 않을 수 있다. 실험 결과 7개의 레이어와 30개의 채널로 구성된 모델이 98.45%로 가장 좋은 성능을 보여 RRAM 기반 장치에 가장 적합한 후보로 여겨진다. 아래의 표 10은 GPU 정확도 추정 차트를 나타낸다. 표 10 3 4 5 6 7 8 9 10↑ ↑ ↑ ↑ ↑ 채널(C) ↓ ↓ ↓ ↓ ↓589.7991.8792.1491.6193.0794.0993.6993.41 1095.4696.2396.6696.9796.8697.0997.5797.29 1596.6197.4697.5698.0197.9597.9198.1998.15 2097.7297.8598.0598.2598.4498.3298.2498.47 2597.8598.3398.3798.3598.5198.4498.798.4 3098.1598.3498.4898.5998.5598.5798.7198.62 3598.4598.4498.3898.698.6598.8298.798.6 4098.4998.5798.7498.7198.798.8398.6498.75 4598.498.4898.6898.7698.8598.898.7898.86 5098.4798.4898.8298.8898.8498.7598.8498.88 ← 레이어 (L) → 아래의 표 11은 RRAM 정확도 추정 차트를 나타낸다. 표 11 3 4 5 6 7 8 9 10 ↑ ↑ 채널(C) ↓1095.4395.3396.296.5296.2596.3997.1496.96 3097.3898.0797.7298.1598.4597.7898.3798.24 5096.9397.7897.8697.3497.9495.6397.6196.92 ← 레이어 (L) → VI-3.실험 결과의 분석 목표 비용 추정에 따라 선택된 최적의 모델이 달라질 수 있다. FPGA의 경우 표 12와 같이 3개의 레이어를 갖는 모델이 가장 적은 플립-플롭을 갖는 반면 4개의 레이어를 갖는 모델은 가장 적은 룩업 테이블을 가지며 가장 적 은 전력을 소모하는 것을 알 수 있다. 소비 전력이 가장 중요한 자원으로 간주된다면, 4개의 레이어를 갖는 모 델이 최적의 후보로 선정될 수 있다. 표 13은 최상의 모델에 대한 실제 하드웨어 비용을 나타낸다. 선택된 모델의 하드웨어 비용을 MAC 연산이 가장 적은 md1과 비교하면, 본 개시에서 제안되는 방법이 MAC 연산을 사용하는 것보다 타겟 하드웨어의 전력 소모를 줄일 수 있음을 알 수 있다. 또한 표 12에 따르면 추정 방법은 실제 하드웨어 오버헤드와 비교하여 하드웨어 비 용과 관련된 합리적인 데이터를 제공하는 것을 알 수 있다. 따라서 제안된 방법을 복잡한 네트워크 및 데이터 세트에 적용할 때, 임계 비용에 대해 네트워크 설계를 상당히 최적화할 수 있다. 예를 들어 실험에서 6개의 컨 볼루션 레이어가 있는 모델 md4는 3개의 레이어가 있는 모델보다 거의 1.5배 더 많은 MAC 연산을 가지고 있지만, 3개의 레이어가 있는 모델과 유사한 하드웨어 통계를 나타낸다. 이것은 MAC 연산이 오해의 소지가 있고 실제 하드웨어 성능에 대한 좋은 추정치가 아니라는 사실을 증명한다. 아래의 표 12는 최적 BNN 모델에 대한 하드웨어 비용 추청치를 나타낸다. 표 12 Model Layers LUTS FFlops Power Mac Ops md1 3 20293 9752.2 1.1512 5989368 md2 4 18764 10000.5 1.151 7756896 md3 5 20565 10901 1.33 10206896 md4 6 19422 10884.5 1.2395 8916432 md5 7 20422.17 11679 1.296 9964640 md6 8 19651 11366 1.234 9854684 아래의 표 13은 다양한 모델을 위한 실제 하드웨어 성능과 MAC 연산을 비교하여 나타낸다. 표 13 Model Layers (#) LUTs FFlops Power MAC Ops md1 3 19211 9104 1.126 5989368 md2 4 20692 10082 1.123 14652176 md4 6 21410 10936 1.256 8916432 마지막으로 다양한 타겟 하드웨어에 대하여, 본 개시의 예시에 따라 탐색된 최적의 BNN 모델을 표 14에 나타내 었다. 이는, FLOP 또는 MAC 연산과 같은 일반화된 메트릭을 사용하는 모든 하드웨어에 대한 최적의 네트워크일 수 있다. 각 하드웨어에는 서로 다른 중요한 리소스가 있을 수 있다. 대상 하드웨어에 대한 최적의 모델을 찾기 위해 하드웨어 별 메트릭이 요구된다. 아래의 표 14는 다른 하드웨어들에 대해서 최적의 BNN 모델을 나타낸다. 표 14 Layers Width Configuration Accuracy on Target Device (%) FPGA 4 23-21-22-21 98.37 RRAM 7 30-30-30-30-30-30-30 98.45 GPU 6 50-50-50-50-50-50 98.88 VII. 소결 하드웨어 구현을 위해 DNN 모델을 최적화하는 것은 도전적인 과제였으며, 특히 리소스가 제한된 임베디드 환경 이 있는 애플리케이션의 경우 상당히 어려운 일이었다. BNN은 신경망 모델을 상당히 단순화할 수 있지만, 가중 치와 활성화를 이진화 할 때 정확도가 저하되는 단점이 있다. 최적의 가속기를 사용하고, FLOP과 MAC 연산과 같 은 일반화된 메트릭과 정확도를 기반으로 최적화하는 것은 대부분의 신경망 모델에 적용가능한 인기 있는 방법 이다. 그러나 하드웨어 플랫폼마다 특성이 다르기 때문에, 소프트웨어 수준에서 평가하는 것만으로는 하드웨어 구현을 위한 최적의 모델을 찾는게 쉽지 않다. 따라서, 본 개시는 특정 하드웨어 플랫폼에 대한 최종 최적 BNN 모델을 탐색하기 위해 학습 전략 및 하드웨어 비용 추정 기술을 포함하는 아키텍처 검색 방법(DeepBit)의 가능성을 성공적으로 제시하였다. 일반적으로 DeepBit 알고리즘을 사용하면 검색 프로세스의 범위를 좁힐 수 있다. 한편, 소프트웨어 수준에서 독립적인 최적 화와 비교하여, 중요한 리소스를 기반으로, 하드웨어 비용 추정은 BNN 모델 목록을 입력 받아, 특정 하드웨어 플랫폼에 대해 더 나은 최적의 BNN 모델을 선택하는 데 도움이 될 수 있다. 그러나 모든 실험이 MNIST(Small Dataset)와 BNN(Simple Neural Networks)에 대해서만 수행되었기 때문에 제안된 방법을 실제 적용하기 위해서는 다양한 과제를 해결해야 합니다. 따라서 DNN용 DeepBit는 제시된 솔루션의 타당성을 평가하기 위해, 추가로 개 선될 수 있다. VIII. 본 개시의 예시에 대한 정리 도 11은 일 실시 예에 따른 소프트웨어 툴의 동작을 나타낸 예시적인 흐름도이다. 본 개시의 일 예시에 따르면, BNN(Binarized Neural Network)과 해당 BNN을 구동하기 위한 NPU(Neural Processing Unit)의 최적 디자인을 찾기 위하여, 컴퓨터에 설치되어 실행되는 소프트웨어 툴이 제시된다. 먼저 상기 소프트웨어 툴에 의하면, 상기 BNN의 레이어의 개수 L에 대한 후보 값들의 제1 세트와 그리고 각 레 이어 당 채널의 개수 C에 대한 후보 값들의 제2 세트가 정의될 수 있다(S1110). 그리고, 상기 소프트웨어 툴에 의하면, 최적의 L 값과 최적의 C 값이 검색될 수 있다(S1120). 이어서, 상기 소프트웨어 툴에 의하면 상기 최적의 L 값과 상기 최적의 C 값이 출력될 수 있다(S1130). 상기 NPU는 파이프라인 형태로 연결된 복수의 블록들을 포함할 때, 상기 복수의 블록들의 개수는 상기 최적의 L 값과 관련되어 있을 수 있다. 상기 복수의 블록들 각각은 라인 버퍼와, XNOR 논리 게이트와, Pop-Count 명령 수행부와, 배치 정규화부 중 하 나 이상을 포함할 수 있다.상기 S1120 과정에서, 상기 최적의 L 값과 상기 최적의 C 값은 상기 NPU의 하드웨어 비용과 상기 NPU의 전력 소 모량 중 적어도 하나 이상에 기초하여 검색될 수 있다. 상기 NPU의 하드웨어 비용은 플리-플롭의 개수에 기초하여 결정될 수 있다. 상기 S1120 과정에서, 상기 최적의 L 값과 상기 최적의 C 값은 요구되는 최소 정확도에 기초하여 검색될 수 있 다. 이때, 정확도 저하가 발생하는 경우, 상기 최적의 C 값은 한 단계 혹은 두 단계 증가될 수 있다. 상기 최적의 C 값은 상기 BNN의 모든 레이어에서 동일한 상수 값일 수 있다. 도 12는 본 개시의 일 예시에 따른 컴퓨터 장치의 구성 블록도이다. 도 12를 참조하면, 본 개시의 일 예시에 따르면 컴퓨터 장치가 제시된다. 상기 컴퓨터 장치는 BNN(Binarized Neural Network)과 해당 BNN을 구동하기 위한 NPU(Neural Processing Unit)의 최적 디자인을 찾기 위한 소프트 웨어 툴을 구동할 수 있다. 상기 컴퓨터 장치는 메모리와 프로세서를 포함할 수 있다. 상기 메모리는 ROM(read-only memory), RAM(random access memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 상기 메모리는 BNN(Binarized Neural Network)과 해당 BNN을 구동하기 위한 NPU(Neural Processing Unit)의 최 적 디자인을 찾는 소프트웨어 툴을 위한 명령어(instructions)를 저장할 수 있다. 상기 프로세서는 상기 명령어를 실행함에 의해서 제1 동작부와 제2 동작부를 포함하게 될 수 있다. 상기 제1 동작부는 상기 BNN의 레이어의 개수 L에 대한 후보 값들의 제1 세트와 그리고 각 레이어 당 채널의 개 수 C에 대한 후보 값들의 제2 세트가 존재할 때, 최적의 L 값과 최적의 C 값을 검색할 수 있다. 그리고, 상기 제2 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 출력하는 단계를 포함할 수 있다. 상기 NPU는 파이프라인 형태로 연결된 복수의 블록들을 포함할 때, 상기 복수의 블록들의 개수는 상기 최적의 L 값과 관련되어 있을 수 있다. 상기 복수의 블록들 각각은 라인 버퍼와, XNOR 논리 게이트와, Pop-Count 명령 수행부와, 배치 정규화부 중 하 나 이상을 포함할 수 있다. 상기 제1 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 상기 NPU의 하드웨어 비용과 상기 NPU의 전력 소모 량 중 적어도 하나 이상에 기초하여 검색할 수 있다. 상기 NPU의 하드웨어 비용은 플리-플롭의 개수에 기초하여 결정될 수 있다. 상기 제1 동작부는 상기 최적의 L 값과 상기 최적의 C 값을 요구되는 최소 정확도에 기초하여 검색할 수 있다. 이때 만약 정확도 저하가 발생하는 경우, 상기 최적의 C 값은 한 단계 혹은 두 단계 증가될 수 있다. 상기 최적의 C 값은 상기 BNN의 모든 레이어에서 동일한 상수 값일 수 있다. 한편, 위와 같은 소프트웨어 툴에 의해서 찾아진 BNN을 위한 NPU는 아래와 같을 수 있다. 구체적으로 NPU는 복수의 블록들을 포함할 수 있다. 상기 복수의 블록들은 파이프라인 형태로 연결되어 있을 수 있다. 상기 복수의 블록들 각각은 라인 버퍼와, XNOR 논리 게이트와, Pop-Count 명령 수행부와, 배치 정규화부 중 하나 이상을 포함할 수 있다. 상기 복수의 블록들의 개수는 상기 BNN 내의 복수 레이어들의 개수 L과 동일할 수 있다. 상기 복수 레이어들의 개수 L과 레이어 당 채널의 개수 C는 전력 소모 및 하드웨어 구현 비용에 기초하여 결정된 것일 수 있다. 상기 소프트웨어 툴에 의해서 찾아진 최적의 NPU는 일차적으로 FPGA로 구현될 수 있다. 이후 상기 FPGA로 구현 되었던 FPGA는 성능을 높이기 위하여 ASIC으로 구현될 수 있다. 본 개시의 예시에 따라 찾아진 최적의 BNN은 전력 소모가 낮기 때문에, AI의 always-on이 요구되는 핸드헬드 (handheld) 기기를 위하여 사용될 수 있다. 또한, 본 개시의 예시에 따라 찾아진 최적의 NPU는 전력 소모가 낮 기 때문에, 핸드헬드(handheld) 기기에 장착되어, always-on AI 서비스를 제공할 수 있다.본 개시와 도면에 게시된 본 개시의 예시들은 본 개시의 기술내용을 쉽게 설명하고 본 개시의 이해를 돕기 위해 특정 예를 제시한 것뿐이며, 본 명의 범위를 한정하고자 하는 것은 아니다. 여기에 게시된 예시들 이외에도 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다."}
{"patent_id": "10-2022-0133807", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 도 2a은 컨볼루션 신경망(CNN)의 기본 구조를 설명하기 위한 도면이다. 도 2b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 3은 컨볼루션 레이어에서 XNOR 연산과 누적 연산을 수행하기 위한 구조를 나타낸 예시도이다. 도 4a 및 도 4b는 네트워크 아키텍처의 작성 모듈을 나타낸다. 도 5a 및 도 5b는 FPGA 상에서 서로 다른 L 값과 C 값을 갖는 BNN 모델들의 리스트에 대하여, 하드웨어 및 전력 비용 추정을 나타낸다. 도 6은 주어진 BNN 모델에서 최적의 균일한 채널 검색을 위한 알고리즘 1을 나타낸다. 도 7은 주어진 BNN 모델에서 최적의 채널의 개수 (즉, 최적의 너비) 검색(Optimal width search)을 위한 알고리 즘 2을 나타낸다. 도 8는 최적의 채널의 개수 (즉, 최적의 너비 설정(optimal width configuration))에 기초한 최적의 BNN 탐색 을 위한 알고리즘 3을 나타낸다. 도 9는 Deepbit 아키텍처 탐색 알고리즘 4를 나타낸다. 도 10은 FPGA을 이용한 BNN 하드웨어 아키텍처를 나타낸 블록도이다. 도 11은 일 실시 예에 따른 소프트웨어 툴의 동작을 나타낸 예시적인 흐름도이다. 도 12는 본 개시의 일 예시에 따른 컴퓨터 장치의 구성 블록도이다."}
