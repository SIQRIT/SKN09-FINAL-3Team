{"patent_id": "10-2020-0093720", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0065827", "출원번호": "10-2020-0093720", "발명의 명칭": "로봇의 파지 방법 및 이를 위한 학습 방법", "출원인": "코가플렉스 주식회사", "발명자": "서일홍"}}
{"patent_id": "10-2020-0093720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇의 엔드 이펙터 및 적어도 하나의 파지 물체를 포함하는 작업 공간 영상을 입력받는 단계;복수의 분리 레벨 중에서 선택된 레벨에 따라서, 상기 작업 공간 영상으로부터 상기 엔드 이펙터 및 타겟 파지물체가 포함된 타겟 영역 이외의 영역이 제거된 분리 영상을 생성하는 단계; 및상기 분리 영상 및 미리 학습된 제1인공 신경망을 이용하여, 상기 타겟 파지 물체에 대한 상기 엔드 이펙터의파지 자세를 결정하는 단계를 포함하는 로봇의 파지 방법."}
{"patent_id": "10-2020-0093720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 복수의 분리 레벨은상기 작업 공간 영상으로부터, 상기 타겟 영역 이외의 영역이 삭제된 제1분리 영상이 생성되는 제1분리 레벨;상기 타겟 영역의 엔드 이펙터가 포함된 제2분리 영상 및 상기 타겟 영역의 타겟 파지 물체가 포함된 제3분리영상이 생성되는 제2분리 레벨; 및상기 제2분리 영상, 상기 제3분리 영상, 상기 작업 공간 영상에서의 상기 엔드 이펙터의 위치 정보가 포함된 제4분리 영상 및 상기 작업 공간 영상에서의 상기 타겟 파지 물체의 위치 정보가 포함된 제5분리 영상이 생성되는제3분리 레벨을 포함하는 로봇의 파지 방법."}
{"patent_id": "10-2020-0093720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서,상기 분리 영상의 사이즈는 상기 작업 공간 영상의 사이즈와 동일하며,상기 분리 영상을 생성하는 단계는상기 제3분리 레벨이 선택된 경우, 상기 타겟 파지 물체가 확대된 제3분리 영상을 생성하는로봇의 파지 방법."}
{"patent_id": "10-2020-0093720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 2항에 있어서,상기 분리 영상의 사이즈는 상기 작업 공간 영상의 사이즈와 동일하며,상기 분리 영상을 생성하는 단계는상기 제3분리 레벨이 선택된 경우, 상기 작업 공간 영상에서의 상기 타겟 파지 물체의 크기에 따라서, 상기 타겟 파지 물체가 확대된 제3분리 영상을 생성하는 공개특허 10-2021-0065827-3-로봇의 파지 방법."}
{"patent_id": "10-2020-0093720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 2항에 있어서,상기 분리 영상을 생성하는 단계는상기 제3분리 레벨이 선택된 경우, 상기 엔드 이펙터 또는 상기 타겟 파지 물체가 중앙에 위치하는 상기 제2분리 영상 또는 제3분리 영상을 생성하는로봇의 파지 방법."}
{"patent_id": "10-2020-0093720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 2항에 있어서,상기 분리 영상을 생성하는 단계는상기 타겟 파지 물체의 텍스쳐에 따라서, 상기 제2 및 제3분리 레벨 중 하나를 선택하는로봇의 파지 방법."}
{"patent_id": "10-2020-0093720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 2항에 있어서,상기 분리 영상을 생성하는 단계는작업 공간에 배치된 상기 파지 물체의 개수, 크기 및 형상 중 어느 하나에 따라서, 상기 제1 내지 제3분리 레벨중 하나를 선택하는로봇의 파지 방법."}
{"patent_id": "10-2020-0093720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서,상기 제1인공 신경망은, 제1컨벌루션 레이어를 이용하여, 상기 분리 영상에 대한 특징값을 생성하며,상기 제1컨벌루션 레이어의 파라미터는,미리 학습된 제2인공 신경망의 제2컨벌루션 레이어로부터 제공되는로봇의 파지 방법."}
{"patent_id": "10-2020-0093720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8항에 있어서,상기 제2인공 신경망은,상기 제2컨벌루션 레이어를 이용하여 입력 영상의 특징값을 생성하는 인코딩 네트워크; 및디컨벌루션 레이어를 이용하여, 상기 특징값으로부터 상기 입력 영상을 복원하는 디코딩 네트워크공개특허 10-2021-0065827-4-를 포함하는 로봇의 파지 방법."}
{"patent_id": "10-2020-0093720", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "로봇의 엔드 이펙터 및 적어도 하나의 파지 물체를 포함하는 훈련용 영상을 입력받는 단계; 복수의 분리 레벨 중에서 선택된 레벨에 따라서, 상기 훈련용 영상으로부터 상기 엔드 이펙터 및 타겟 파지 물체가 포함된 타겟 영역 이외의 영역이 제거된 분리 영상을 생성하는 단계; 및 인공 신경망 기반으로, 상기 분리 영상에 포함된 상기 타겟 파지 물체에 대한 상기 엔드 이펙터의 파지 자세를학습하는 단계를 포함하는 로봇의 파지를 위한 학습 방법."}
{"patent_id": "10-2020-0093720", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "보다 향상된 파지 성공률을 제공할 수 있는 로봇의 파지 방법 및 이를 위한 학습 방법이 개시된다. 개시된 로봇 의 파지 방법은 로봇의 엔드 이펙터 및 적어도 하나의 파지 물체를 포함하는 작업 공간 영상을 입력받는 단계; 복수의 분리 레벨 중에서 선택된 레벨에 따라서, 상기 작업 공간 영상으로부터 상기 엔드 이펙터 및 타겟 파지 물체가 포함된 타겟 영역 이외의 영역이 제거된 분리 영상을 생성하는 단계; 및 상기 분리 영상 및 미리 학습된 제1인공 신경망을 이용하여, 상기 타겟 파지 물체에 대한 상기 엔드 이펙터의 파지 자세를 결정하는 단계를 포함 한다."}
{"patent_id": "10-2020-0093720", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 로봇의 파지 방법 및 이를 위한 학습 방법에 관한 발명으로서, 더욱 상세하게는 보다 향상된 파지 성 공률을 제공할 수 있는 로봇의 파지 방법 및 이를 위한 학습 방법에 관한 것이다."}
{"patent_id": "10-2020-0093720", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 알파고(AlphaGo)가 프로 바둑 기사에게 승리한 사건이 굉장한 이슈가 되었다. 알파고는 구글이 개발한 인 공지능 바둑 프로그램으로서, 심층 강화 학습을 통해 바둑을 학습한 프로그램이다. 심층 강화 학습이란, 심층 학습(Deep learning)과 강화 학습 Reinforcement learning)이 결합한 형태의 기계 학습 방법이다. 심층 학습이란, 신경 세포의 기능을 모방한 인공 신경망을 이용한 기계 학습 방법이며, 강화 학습이란, 에이전 트(agent)가 주어진 환경(environment)에 대해 어떠한 행동을 취하고 이로부터 어떠한 보상(reward)를 얻으면서 학습하는 방법이다. 심층 강화 학습으로 파지 동작과 같은 연속 행동을 수행하는 연구는 크게 두 가지로 나누어진다. 하나는 정책/ 가치망 기반의 강화 학습 방법(Actor-Critic RL)으로서, 대표적으로 DDPG, D4PG등이 있다. 이러한 알고리즘이 높은 성능을 나타내는 경우의 대부분은, 고차원의 영상을 입력으로 하는 경우가 아니라 사람이 설계한 저차원의 특징을 입력으로 할 경우이다. 고차원의 영상을 입력으로 하는 경우의 파지 성공률은 10~20%에 불과하다. 또 하나는 바둑이나 게임 등과 같은 이산 행동 생성 분야에 이용되는 가치망 기반 강화 학습 방법(Value-based RL)이다. 구글은 이러한 강화 학습 방법의 대표적 알고리즘인 Deep Q-Network(DQN)을 확장하여, 연속 행동 생성 이 가능한 Qt-Opt라는 알고리즘을 개발하였으며, 모르는 물체에 대해서 80%정도의 높은 파지 성공률을 나타내었 다. 하지만 이 방법의 경우, 일반적인 정책/가치망 기반 강화 학습 방법에 비해서, 한번의 행동을 생성하는데 100배 정도의 연산량을 필요로 한다. 따라서 구글에서 선보인 TPU(Tensor Process Unit)와 같이, 강력한 컴퓨팅 파워를 제공하는 하드웨어가 아닌 이상, Qt-Opt 알고리즘을 이용해 로봇의 파지 동작을 생성하는 것은 불가능하 다. 이와 같이, 정책/가치망 기반의 강화 학습 방법은 적은 연산량을 필요로하지만 파지 성공률이 낮은 편인데 반해, Qt-Opt 알고리즘은, 높은 파지 성공률을 제공할 수 있지만, 높은 연산량을 필요로 하는 문제가 있다. 따라서 높은 파지 성공률을 제공하면서, 연산량을 줄일 수 있는 학습 방법에 대한 개발이 필요하다. 관련 선행 문헌으로, 대한민국 공개특허 제2019-0088093호, 제2018-0114200호가 있다."}
{"patent_id": "10-2020-0093720", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 연산량 대비 높은 파지 성공률을 제공할 수 있는 로봇의 파지 방법 및 이를 위한 학습 방법을 제공하 기 위한 것이다."}
{"patent_id": "10-2020-0093720", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명의 일 실시예에 따르면, 로봇의 엔드 이펙터 및 적어도 하나의 파지 물체 를 포함하는 작업 공간 영상을 입력받는 단계; 복수의 분리 레벨 중에서 선택된 레벨에 따라서, 상기 작업 공간 영상으로부터 상기 엔드 이펙터 및 타겟 파지 물체가 포함된 타겟 영역 이외의 영역이 제거된 분리 영상을 생성 하는 단계; 및 상기 분리 영상 및 미리 학습된 제1인공 신경망을 이용하여, 상기 타겟 파지 물체에 대한 상기 엔드 이펙터의 파지 자세를 결정하는 단계를 포함하는 로봇의 파지 방법이 제공된다. 또한 상기한 목적을 달성하기 위한 본 발명의 다른 실시예에 따르면, 로봇의 엔드 이펙터 및 적어도 하나의 파 지 물체를 포함하는 훈련용 영상을 입력받는 단계; 복수의 분리 레벨 중에서 선택된 레벨에 따라서, 상기 훈련 용 영상으로부터 상기 엔드 이펙터 및 타겟 파지 물체가 포함된 타겟 영역 이외의 영역이 제거된 분리 영상을 생성하는 단계; 및 인공 신경망 기반으로, 상기 분리 영상에 포함된 상기 타겟 파지 물체에 대한 상기 엔드 이 펙터의 파지 자세를 학습하는 단계를 포함하는 로봇의 파지를 위한 학습 방법이 제공된다."}
{"patent_id": "10-2020-0093720", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일실시예에 따르면, 학습 효율이 증가함으로써, 로봇의 파지 성공률이 향상될 수 있다. 또한, 본 발명의 일실시예에 따르면, 훈련 영상으로부터 생성된 분리 영상을 이용함으로써, 연산량의 급격한 증 가없이도 높은 파지 성공률을 제공할 수 있다."}
{"patent_id": "10-2020-0093720", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 각 도면을 설명하면서 유사한 참조부호를 유사한 구성요소에 대해 사용하였다. 이하에서, 본 발명에 따른 실시예들을 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 본 발명의 일실시예에 따른 로봇의 파지를 위한 학습 방법을 설명하기 위한 도면으로서, 학습에 이용되 는 인공 신경망을 나타내는 도면이다. 본 발명의 일실시예에 따른 학습 방법은 프로세서 및 메모리를 포함하는 컴퓨팅 장치에서 수행될 수 있으며, 데 스크탑, 노트북, 서버, 모바일 단말이나 별도의 학습 전용 장치가 컴퓨팅 장체에 포함될 수 있다. 도 1을 참조하면, 본 발명의 일실시예에 따른 컴퓨팅 장치는 제1인공 신경망을 이용하여, 훈련용 영상에 포함된 파지 물체에 대한 로봇의 엔드 이펙터(end effector)의 파지 자세를 학습한다. 일실시예로서, 제1인공 신경망은 강화 학습에 이용되는 정책/가치망일 수 있으며, 실시예에 따라서는 감독 학습에 이용되는 신경 망도 제1인공 신경망에 포함될 수 있다. 또한 파지 자세는 로봇의 관절각이나 또는 엑츄에이터에 대한 제어값에 대응될 수 있다. 제1인공 신경망은, 제1컨벌루션 레이어(convolution layer, 111) 및 완전 연결 레이어(fully connected layer, 113)를 포함하며, 제1컨벌루션 레이어로 훈련용 영상이 입력된다. 훈련용 영상은 로봇의 엔드 이펙 터와, 작업 공간에 배치된 파지 물체들이 포함된 영상일 수 있으며, 엔드 이펙터보다 높은 위치에서, 엔드 이펙 터와 파지 물체들이 포함되도록 촬영될 수 있다. 강화 학습 과정을 통해, 제1컨벌루션 레이어의 가중치와 완전 연결 레이어의 가중치가 학습된다. 예 컨대, 엔드 이펙터가 현재 제1자세에서, 타겟 파지 물체에 대해 제2자세를 취했을 때 엔드 이펙터와 타겟 파지 물체의 거리가 가까우면 큰 보상이 제공되고, 로봇이 현재 제3자세에서, 타겟 파지 물체에 대해 제4자세를 취했 을 때 엔드 이펙터와 타겟 물체의 거리가 멀면 작은 보상이 제공될 수 있다. 그리고 큰 보상이 제공될 수 있도 록, 가중치가 학습된다. 다만, 제1컨벌루션 레이어와 완전 연결 레이어의 학습이 동시에 이루어질 경우, 학습 효율이 저하될 수 있으므로, 본 발명의 일실시예에 따른 컴퓨팅 장치는 제2인공 신경망을 추가로 이용하여, 학습을 수행 한다. 제2인공 신경망은 제1컨벌루션 레이어와 동일한 구조의 제2컨벌루션 레이어를 포함하는 신경망 으로서, 컴퓨팅 장치는 훈련용 영상을 이용하여, 제2인공 신경망을 미리 학습한다. 그리고 학습 과정을 통 해 획득된 제2컨벌루션 레이어의 파라미터를 제1컨벌루션 레이어의 파라미터로 이용한다. 즉, 제2컨벌루션 레이어의 파라미터가 제1컨벌루션 레이어의 파리미터로 제공되며, 여기서 파라미터는 컨벌루션에 이용되는 필터의 가중치일 수 있다. 즉, 본 발명의 일실시예는 제1컨벌루션 레이어와 완전 연결 레이어의 학습을 동시에 수행하지 않고, 먼저 제1컨벌루션 레이어에 대한 학습을 제2인공 신경망을 통해 수행한다. 그리고 제1컨벌루션 레이 어에 대한 학습이 수행된 이후, 완전 연결 레이어에 대한 학습을 수행하는 것이다. 일실시예로서, 제2인공 신경망은 상태 표현 학습(State Representation Learning)에 이용되는 신경망일 수 있으며, 인코딩 네트워크와 디코딩 네트워크를 포함할 수 있다. 인코딩 네트워크는 제2컨벌루션 레이어를 포함하며, 제2컨벌루션 레이어를 이용하여, 입력 영상의 특징값 을 생성한다. 디코딩 네트워크는 디컨벌루션 레이어를 포함하며, 디컨벌루션 레이어를 이용하여, 인코딩 네트워 크에서 생성된 특징값으로부터 입력 영상을 복원한다. 제2인공 신경망은 입력 영상인 훈련용 영상이 잘 복원될 수 있도록 학습되며, 이러한 학습 과정을 통해 제 2컨벌루션 레이어 및 디컨벌루션 레이어의 가중치가 결정된다. 이와 같이 본 발명의 일실시예에 따른 컴퓨팅 장치는 제2인공 신경망에 대한 1차 학습을 수행하고, 1차 학습 과 정을 통해 획득된 제2컨벌루션 레이어의 파라미터를 제1컨벌루션 레이어에 할당한다. 그리고 제2컨벌루션 레이 어의 파라미터가 할당된 제1인공 신경망 중 완전 연결 레이어에 대한 2차 학습을 수행함으로써, 학습 효율을 높 일 수 있다. 도 2 및 도 3은 본 발명의 다른 실시예에 따른 따른 로봇의 파지를 위한 학습 방법을 설명하기 위한 도면으로서, 도 2는 훈련용 영상을 나타내며, 도 3은 분리 영상을 나타내는 도면이다. 본 발명의 일실시예는, 학습 효율을 더욱 높이기 위해, 훈련용 영상을 전처리하여 이용한다. 훈련용 영상 에는 엔드 이펙터와, 작업 공간에 배치된 파지 물체가 포함되는데, 도 2에 도시된 바와 같이, 다양한 파지 물체가 포함될 경우, 학습 효율이 낮아질 수 있기 때문에, 본 발명의 일실시예는 복수의 분리 레벨 중에서 선택된 레벨에 따라서, 훈련용 영상으로부터 분리 영상을 생성하고 이러한 분리 영상을 이용하여, 제2인공 신경망에 대한 학습을 수행한다. 분리 영상은, 훈련용 영상에서, 엔드 이펙터와 타겟 파지 물체가 포함된 타겟 영역 이외의 영역 이 제거된 영상이다. 타겟 파지 물체는 훈련용 영상에서, 사용자에 의해 지정될 수 있다. 본 발명의 일실시예는 학습 효율을 높이기 위해, 실제 학습에 이용되는 객체인 엔드 이펙터와 타겟 파지 물체가 나머지 객체들과 분리된 상태의 영상인 분리 영상을 이용한다. 이러한 분리 영상은 분리 레벨에 따라서, 다양한 형태로 생성될 수 있으며, 이 때 생성되는 분리 영상의 사이즈는 훈련용 영상과 동일할 수 있다. 분리 레벨은 일실시예로서, 제1 내지 제3분리 레벨(L1 내지 L3)을 포함할 수 있으며, 제1분리 레벨(L1)은 훈련 용 영상으로부터, 타겟 영역 이외의 영역이 삭제된 제1분리 영상이 생성되는 레벨이다. 즉, 컴퓨팅 장치는 제1분리 레벨(L1)이 선택된 경우, 훈련용 영상에서 타겟 영역 이외의 영역을 제거하여, 제1분리 영상을 생성한다. 제2분리 레벨(L2)은 타겟 영역의 엔드 이펙터가 포함된 제2분리 영상 및 타겟 영역의 타겟 파지 물체 가 포함된 제3분리 영상이 생성되는 레벨이다. 즉, 컴퓨팅 장치는 제2분리 레벨(L2)이 선택된 경우, 훈련용 영상에서 엔드 이펙터가 포함된 영역 이외의 영역을 제거하여 제2분리 영상을 생성하고, 훈련용 영상에서 타겟 파지 물체가 포함된 영역 이외의 영역을 제거하여, 제3분리 영상을 생성 한다. 제3분리 레벨(L3)은 제2분리 영상, 제3분리 영상, 훈련용 영상에서의 엔드 이펙터의 위치 정보가 포함된 제4분리 영상 및 훈련용 영상에서의 타겟 파지 물체의 위치 정보가 포함된 제5분 리 영상이 생성되는 레벨이다. 즉, 컴퓨팅 장치는, 제3분리 레벨(L3)이 선택된 경우, 제2분리 레벨(L2)과 같이, 제2 및 제3분리 영상(312, 313)을 생성하되, 엔드 이펙터 및 타겟 파지 물체의 위치 정보가 포함된 제4 및 제5분리 영상(314, 315)을 추가로 생성한다. 엔드 이펙터 및 타겟 파지 물체의 위치 정보는 제4 및 제5분리 영상(314, 315)에 표시된 바와 같이, 훈련용 영 상에서 엔드 이펙터 및 타겟 파지 물체가 위치한 영역이 흑백으로 처리된 형태로 생성될 수 있다. 제3분리 레벨(L3)에서는, 제4 및 제5분리 영상(314, 315)에 엔드 이펙터 및 타겟 파지 물체의 위치 정보가 포함되기 때문에, 컴퓨팅 장치는 엔드 이펙터 또는 타겟 파지 물체가 중앙에 위치하는 제2 및 제3분리 영상(322, 323)을 생성할 수 있다. 그리고 컴퓨팅 장치는 제3분리 영상과 같이, 훈련용 영상(20 0)에서의 타겟 파지 물체보다, 타겟 파지 물체를 확대하여 제3분리 영상을 생성할 수 있다. 실시예에 따라 서, 컴퓨팅 장치는 훈련용 영상에서의 타겟 파지 물체의 크기에 따라서, 타겟 파지 물체가 확대된 제 3분리 영상을 생성할 수 있으며, 타겟 파지 물체의 크기가 미리 설정된 임계 크기보다 클 경우에는 확대없이 제 3분리 영상을 생성할 수 있다. 본 발명의 일실시예에 따른 컴퓨팅 장치는 파지 성공률이 높아지면서도 연산량이 줄어들 수 있는 방향으로, 제1 내지 제3분리 레벨(L1 내지 L3) 중 하나를 선택하여, 분리 영상을 생성할 수 있다. 제3분리 레벨(L3)에 따라 분 리 영상이 생성될 경우, 파지 성공률은 높아질 수 있지만 연산량이 증가하며, 제1분리 레벨(L1)에 따라 분리 영 상이 생성될 경우 연산량은 낮아질 수 있지만, 파지 성공률 역시 낮아질 수 있다. 컴퓨팅 장치는 일실시예로서, 작업 공간에 배치된 파지 물체의 개수, 크기 및 형상 중 어느 하나에 따라서, 제1 내지 제3분리 레벨(L1 내지 L3) 중 하나를 선택할 수 있다. 작업 공간에 배치된 파지 물체의 개수가 많거나 크 기가 작거나 또는 형상이 복잡한 경우에는, 컨벌루션 레이어를 통해 파지 물체의 특징값이 정확하게 학습되기 어려우므로, 제3분리 레벨(L3)에 따라 분리 영상이 생성될 수 있다. 반대로 작업 공간에 배치된 파지 물체의 개 수가 적거나 크기가 크거나 또는 형상이 단순한 경우에는, 컨벌루션 레이어를 통해 파지 물체의 특징값이 비교 적 정확하게 학습될 수 있으며, 이러한 경우에도 제3분리 레벨(L3)에 따라 분리 영상을 생성하는 것은 불필요하 게 연산량을 증가시킬 수 있으므로, 제1 또는 제2분리 레벨(L1 또는 L2)에 따라 분리 영상이 생성되는 것이 바 람직하다. 또는 컴퓨팅 장치는 타겟 파지 물체의 텍스쳐에 따라서, 제2 및 제3분리 레벨(L2, L3) 중 하나를 선택하여, 분 리 영상을 생성할 수 있다. 예컨대 타겟 파지 물체에 화려한 무늬가 디자인되어 있거나, 타겟 파지 물체가 독특 한 질감을 나타내는 재질로 이루어진 경우에 컴퓨팅 장치는, 제3분리 레벨(L3)에 따라서 분리 영상을 생성할 수 있다. 반대로 타겟 파지 물체가 별다른 디자인없이 단일 색상으로 디자인되어 있거나, 매끈한 질감의 재질로 이 루어진 경우에 컴퓨팅 장치는, 제2분리 레벨(L2)에 따라서 분리 영상을 생성할 수 있다. 이와 같이 생성된 분리 영상은, 상태 표현 학습을 위한 제2인공 신경망으로 입력되어 제2인공 신경망의 학습에 이용되며, 학습 과정을 통해 얻어진 제2인공 신경망의 제2컨벌루션 레이어의 파라미터는, 제1인공 신경망으로 제공될 수 있다. 또는 실시예에 따라서는, 분리 영상은 제1인공 신경망으로 입력되어 제1인공 신경망의 학습에 이용될 수 있다. 결국, 본 발명의 일실시예에 따른 컴퓨팅 장치는 선택된 분리 레벨에 따라서 입력된 훈련용 영상으로부터 분리 영상을 생성하고, 인공 신경망 기반으로, 분리 영상에 포함된 타겟 파지 물체에 대한 엔드 이펙터의 파지 자세 를 학습한다. 도 4는 본 발명의 일실시예에 따른 로봇의 파지 방법을 설명하기 위한 도면이다. 본 발명의 일실시예에 따른 로봇의 파지 방법은, 프로세서 및 메모리를 컴퓨팅 장치에서 수행될 수 있으며, 이 러한 컴퓨팅 장치는 로봇에 탑재되거나 또는 로봇의 외부에 위치할 수 있다. 로봇의 외부에 위치하는 컴퓨팅 장 치에서 수행되어 획득된 파지 자세는 로봇으로 제공될 수 있다. 본 발명의 일실시예에 따른 컴퓨팅 장치는, 로봇의 엔드 이펙터 및 적어도 하나의 파지 물체를 포함하는 작업 공간 영상을 수신(S410) 즉, 입력받는다. 작업 공간 영상은, 전술된 훈련용 영상에 대응되는 영상으로서, 엔드 이펙터보다 높은 위치에서 엔드 이펙터와 작업 공간의 파지 물체들이 포함되도록 촬영될 수 있다. 그리고 컴퓨팅 장치는 복수의 분리 레벨 중에서 선택된 레벨에 따라서, 작업 공간 영상으로부터 엔드 이펙터 및 타겟 파지 물체가 포함된 타겟 영역 이외의 영역이 제거된 분리 영상을 생성(S420)한다. 분리 영상의 사이즈는 작업 공간 영상의 사이즈와 동일할 수 있으며, 타겟 파지 물체는 작업 공간 영상에서 사용자에 의해 지정되거나 객체 인식을 통해 지정될 수 있다. 복수의 분리 레벨은 일실시예로서, 작업 공간 영상으로부터, 타겟 영역 이외의 영역이 삭제된 제1분리 영상이 생성되는 제1분리 레벨을 포함할 수 있다. 또한 타겟 영역의 엔드 이펙터가 포함된 제2분리 영상 및 상기 타겟 영역의 타겟 파지 물체가 포함된 제3분리 영상이 생성되는 제2분리 레벨을 포함할 수 있다. 또한 제2분리 영상, 제3분리 영상, 작업 공간 영상에서의 엔드 이펙터의 위치 정보가 포함된 제4분리 영상 및 작업 공간 영상에서의 타겟 파지 물체의 위치 정보가 포함된 제5분리 영상이 생성되는 제3분리 레벨을 포함할 수 있다. 실시예에 따라 서, 제1 내지 제3분리 레벨 모두가 아닌 일부가 선택적으로 이용될 수도 있다. 단계 S420에서 컴퓨팅 장치는 작업 공간에 배치된 파지 물체의 개수, 크기 및 형상 중 어느 하나에 따라서, 제1 내지 제3분리 레벨 중 하나를 선택하거나 또는 타겟 파지 물체의 텍스쳐에 따라서, 제2 및 제3분리 레벨 중 하 나를 선택할 수 있다. 컴퓨팅 장치는 제3분리 레벨이 선택된 경우, 엔드 이펙터 또는 타겟 파지 물체가 중앙에 위치하는 제2분리 영상 또는 제3분리 영상을 생성할 수 있다. 또한 컴퓨팅 장치는 제3분리 레벨이 선택된 경우, 작업 공간 영상에서의 타겟 파지 물체의 크기에 따라서, 타겟 파지 물체가 확대된 제3분리 영상을 생성할 수 있다. 타겟 파지 물체의 크기가 임계 크기보다 작을 경우, 타겟 파지 물체가 확대되어 제3분리 영상이 생성될 수 있으며, 실시예에 따라서는 타겟 파지 물체의 크기에 상관없이 타겟 파지 물체가 확대되어 제3분리 영상이 생성될 수 있다. 다양한 영상 처리 알고리즘을 통해, 작업 공간 영상에서, 타겟 파지 물체의 크기, 텍스쳐 등이 분석될 수 있다. 본 발명의 일실시예에 따른 컴퓨팅 장치는 단계 S420에서 생성된 분리 영상 및 미리 학습된 제1인공 신경망을 이용하여, 엔드 이펙터의 파지 자세를 결정(S430)한다. 컴퓨팅 장치는 엔드 이펙터의 파지 자세를 나타내는 정 보로서, 로봇의 관절각이나 또는 엑츄에이터에 대한 제어값을 출력할 수 있다. 제1인공 신경망은, 도 1 내지 도 3에서 설명된 학습 방법에 의해 학습된 강화 학습에 이용되는 정책/가치망일 수 있으며, 제1컨벌루션 레이어를 이용하여, 분리 영상에 대한 특징값을 생성한다. 이 때, 제1컨벌루션 레이어 의 파라미터는 미리 학습된 제2인공 신경망의 제2컨벌루션 레이어로부터 제공될 수 있다. 제2인공 신경망은, 상태 표현 학습에 이용되는 신경망일 수 있으며, 제2컨벌루션 레이어를 이용하여 입력 영상 의 특징값을 생성하는 인코딩 네트워크; 및 디컨벌루션 레이어를 이용하여, 인코딩 네트워크에서 생성된 특징값 으로부터 입력 영상을 복원하는 디코딩 네트워크를 포함할 수 있다. 도 5는 본 발명의 일실시예에 따른 파지 성공률을 설명하기 위한 도면이다. 도 5는 상태 표현 학습에 이용되는 인공 신경망과, 정책 가치망을 이용한 강화 학습 결과에 따른 파지 성공률을 나타내는 도면으로서, L0는 분리 영상이 이용되지 않은 학습 결과에 따른 파지 성공률을 나타낸다. 그리고 L1 내지 L3는 각각 제1 내지 제3분리 레벨에 따른 분리 영상을 통해 학습된 결과에 따른 파지 성공률을 나타낸다. 그리고 도 5(a) 내지 도 5(c)는 서로 다른 상태 표현 학습 방법을 이용한 학습 결과에 따른 파지 성공률을 나타 낸다. 도 5(a)는 Spatial Auto Encoder(SAE), 도 5(b)는 Variational Auto Encoder(VAE), 도 5(c)는 Forward Model + Auto Encoder(FM+AE)라는 상태 표현 학습 방법에 따른 파지 성공률을 나타낸다. 도 5에 도시된 바와 같이, 제2분리 레벨 및 제3분리 레벨에 따른 분리 영상을 이용한 학습에 의한 파지 성공률 (Grasp Success Rate)이 제1분리 레벨을 이용하거나, 분리 영상을 이용하지 않은 경우와 비교하여, 매우 높음을알 수 있다. 본 발명의 일실시예에 따르면, 훈련 영상으로부터 생성된 분리 영상을 이용함으로써, 높은 파지 성공률을 제공 할 수 있다. 그리고 분리 영상의 생성은, 많은 연산량을 필요로하지 않는 작업으로서, 본 발명의 일실시예에 따르면, 연산량 의 급격한 증가없이도 높은 파지 성공률을 제공할 수 있으며, 연산량 대비 높은 파지 성공률을 제공할 수 있다. 앞서 설명한 기술적 내용들은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴 퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예들을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴 퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행 하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포 함한다. 하드웨어 장치는 실시예들의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성 될 수 있으며, 그 역도 마찬가지이다. 이상과 같이 본 발명에서는 구체적인 구성 요소 등과 같은 특정 사항들과 한정된 실시예 및 도면에 의해 설명되 었으나 이는 본 발명의 보다 전반적인 이해를 돕기 위해서 제공된 것일 뿐, 본 발명은 상기의 실시예에 한정되 는 것은 아니며, 본 발명이 속하는 분야에서 통상적인 지식을 가진 자라면 이러한 기재로부터 다양한 수정 및 변형이 가능하다. 따라서, 본 발명의 사상은 설명된 실시예에 국한되어 정해져서는 아니되며, 후술하는 특허청 구범위뿐 아니라 이 특허청구범위와 균등하거나 등가적 변형이 있는 모든 것들은 본 발명 사상의 범주에 속한다 고 할 것이다."}
{"patent_id": "10-2020-0093720", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른 로봇의 파지를 위한 학습 방법을 설명하기 위한 도면이다. 도 2 및 도 3은 본 발명의 다른 실시예에 따른 따른 로봇의 파지를 위한 학습 방법을 설명하기 위한 도면이다. 도 4는 본 발명의 일실시예에 따른 로봇의 파지 방법을 설명하기 위한 도면이다. 도 5는 본 발명의 일실시예에 따른 파지 성공률을 설명하기 위한 도면이다."}
