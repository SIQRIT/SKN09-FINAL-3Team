{"patent_id": "10-2023-0031372", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0137942", "출원번호": "10-2023-0031372", "발명의 명칭": "의료 영상을 기초로 딥러닝 알고리즘을 이용하여 인체 성분을 분석하기 위한 방법 및 장치", "출원인": "연세대학교 산학협력단", "발명자": "이광석"}}
{"patent_id": "10-2023-0031372", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치의 동작 방법에 있어서, 상기 전자 장치는 입력 장치, 메모리, 적어도 하나의 프로세서, 출력 장치를포함하고,제1 환자의 신체 중 전체 또는 일정 영역에 대하여 3차원 CT (computed tomography) 촬영을 통해 획득된 라벨링되지 않은 복수의 제1 2차원 의료 영상들을 상기 입력 장치에 의하여 입력 받는 단계;상기 복수의 제1 2차원 의료 영상들에 대하여 인체 성분 별로 라벨링이 수행된 복수의 제2 2차원 의료 영상들을상기 입력 장치에 의하여 입력 받는 단계;상기 제1 2차원 의료 영상들에 기반하여 라벨링 되지 않은 제1 3차원 의료 영상을 상기 프로세서에 의하여 생성하는 단계;상기 제2 2차원 의료 영상들에 기반하여 라벨링 된 제2 3차원 의료 영상을 상기 프로세서에 의하여 생성하는 단계;상기 제1 3차원 의료 영상 내 복수의 무작위의 위치들에서 복수의 제1 3차원 부분 의료 영상들을 상기 프로세서에 의하여 결정하는 단계;상기 제2 3차원 의료 영상 내 상기 복수의 제1 3차원 부분 의료 영상들에 대응하는 복수의 위치들에서 복수의제2 3차원 부분 의료 영상들을 상기 프로세서에 의하여 결정하는 단계;상기 복수의 제1 3차원 부분 의료 영상들 및 상기 복수의 제2 3차원 부분 의료 영상들에 기반하여 상기 메모리에 저장된 인체 성분 분석 모델의 학습을 상기 프로세서에 의하여 수행하는 단계;제2 환자의 신체 중 전체 또는 일정 영역에 대하여 3차원 CT 촬영을 통해 획득된 라벨링 되지 않은 복수의 제32차원 의료 영상들을 상기 입력 장치에 의하여 입력 받는 단계;상기 제3 2차원 의료 영상들에 기반하여 라벨링 되지 않은 제3 3차원 의료 영상을 상기 프로세서에 의하여 생성하는 단계;상기 제3 3차원 의료 영상 내 복수의 순차적 위치들에서 복수의 제3 3차원 부분 의료 영상들을 상기 프로세서에의하여 결정하는 단계;상기 복수의 제3 3차원 부분 의료 영상들에 기반하여 상기 학습된 인체 성분 분석 모델을 이용하여 라벨링 된복수의 제4 3차원 부분 의료 영상들을 상기 프로세서에 의하여 생성하는 단계;상기 복수의 제4 3차원 부분 의료 영상들에 기반하여 라벨링 된 제4 3차원 의료 영상을 상기 프로세서에 의하여생성하는 단계;상기 제4 3차원 의료 영상에 기반하여 상기 제2 환자의 신체에 대한 인체 성분 분석 정보를 상기 프로세서에 의하여 생성하는 단계;상기 제2 환자의 신체에 대한 인체 성분 분석 정보를 상기 출력 장치에 의하여 출력하는 단계를 포함하는,방법."}
{"patent_id": "10-2023-0031372", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 라벨링은 피부, 뼈, 내장 장기, 혈관, 근육, 지방, 종양 중 하나 이상의 인체 성분에 대한 구분을 포함하고,공개특허 10-2024-0137942-3-상기 인체 성분 분석은 환자의 신체 내 인체 성분의 분포 비율, 분포 정보, 부피 정보, 또는 질량 정보의 분석을 포함하는,방법."}
{"patent_id": "10-2023-0031372", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 제4 3차원 의료 영상에 기반하여 라벨링 된 복수의 제4 2차원 의료 영상들을 상기 프로세서에 의하여 생성하는 단계;상기 복수의 제4 2차원 의료 영상들을 상기 출력 장치에 의하여 출력하는 단계를 더 포함하는,방법."}
{"patent_id": "10-2023-0031372", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,각각의 상기 복수의 제4 2차원 의료 영상들은 각각의 상기 복수의 제3 2차원 의료 영상들에 대응하는,방법."}
{"patent_id": "10-2023-0031372", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 복수의 제1 3차원 부분 의료 영상들, 상기 복수의 제2 3차원 부분 의료 영상들, 상기 복수의 제3 3차원 부분 의료 영상들, 상기 복수의 제4 3차원 부분 의료 영상들은 동일하게 설정된 크기 및 설정된 구조로 구성되고,상기 복수의 제1 3차원 부분 의료 영상들, 상기 복수의 제2 3차원 부분 의료 영상들의 개수는 설정된 수 이상의무작위의 수에 해당하고,상기 복수의 제3 3차원 부분 의료 영상들, 상기 복수의 제4 3차원 부분 의료 영상들의 개수는 상기 제3 3차원의료 영상, 상기 제4 3차원 의료 영상의 크기를 상기 설정된 크기로 나눈 수에 해당하고,상기 제1 3차원 의료 영상, 상기 제2 3차원 의료 영상은 크기 및 구조가 서로 동일하고,상기 제3 3차원 의료 영상, 상기 제4 3차원 의료 영상은 크기 및 구조가 서로 동일한,방법."}
{"patent_id": "10-2023-0031372", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 복수의 제2 3차원 부분 의료 영상들 중 이웃한 제2 3차원 부분 의료 영상들은 서로 오버랩 되거나 또는 서로 갭이 사이에 존재하고,상기 복수의 제4 3차원 부분 의료 영상들 중 이웃한 제4 3차원 부분 의료 영상들은 서로 오버랩 되지 않으며,또한 서로 갭이 사이에 존재하지 않는,방법."}
{"patent_id": "10-2023-0031372", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "공개특허 10-2024-0137942-4-제1 항에 있어서,상기 복수의 제1 3차원 부분 의료 영상들 및 상기 복수의 제2 3차원 부분 의료 영상들에 기반하여 상기 메모리에 저장된 인체 성분 분석 모델의 학습을 수행하는 단계는,상기 복수의 제1 3차원 부분 의료 영상들 중 하나의 제1 3차원 부분 의료 영상, 상기 복수의 제2 3차원 부분 의료 영상들 중 상기 하나의 제1 3차원 부분 의료 영상에 대응하는 하나의 제2 3차원 부분 의료 영상에 기반하여상기 인체 성분 분석 모델의 학습을 수행하는 단계가 반복되어 수행되는 단계를 포함하고,상기 인체 성분 분석 모델의 학습이 수행될 때마다 상기 복수의 제1 3차원 부분 의료 영상들 중 서로 다른 하나의 제1 3차원 부분 의료 영상 및 서로 다른 하나의 제2 3차원 부분 의료 영상에 기반하여 상기 인체 성분 분석모델의 학습이 수행되는,방법."}
{"patent_id": "10-2023-0031372", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항에 있어서,상기 복수의 제3 3차원 부분 의료 영상들에 기반하여 상기 학습된 인체 성분 분석 모델을 이용하여 라벨링 된복수의 제4 3차원 부분 의료 영상들을 생성하는 단계는,상기 복수의 제3 3차원 부분 의료 영상들 중 하나의 제3 3차원 부분 의료 영상에 기반하여 상기 학습된 인체 성분 분석 모델을 이용하여 상기 복수의 제4 3차원 부분 의료 영상들 중 상기 하나의 제3 3차원 부분 의료 영상에대응하는 하나의 제4 3차원 부분 의료 영상을 생성하는 단계가 반복되어 수행되는 단계를 포함하고,상기 제4 3차원 부분 의료 영상을 생성하는 단계가 반복되어 수행될 때마다 미리 정해진 규칙에 따라서 상기 복수의 제3 3차원 부분 의료 영상들 중 서로 다른 하나의 제3 3차원 부분 의료 영상에 기반하여 서로 다른 하나의제4 3차원 부분 의료 영상이 생성되는,방법."}
{"patent_id": "10-2023-0031372", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "전자 장치에 있어서, 상기 전자 장치는 입력 장치, 메모리, 적어도 하나의 프로세서, 출력 장치를 포함하고,상기 적어도 하나의 프로세서는 제1 항 내지 제8 항 중 어느 한 항에 따른 방법을 수행하도록 구성된,전자 장치."}
{"patent_id": "10-2023-0031372", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항 내지 제8 항 중 어느 한 항에 따른 방법을 수행하도록 구성되며, 컴퓨터 판독 가능한 저장 매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0031372", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 의료 영상을 기초로 딥러닝 알고리즘을 이용하여 인체성분을 분석하기 위한 방법 및 장치에 관한 것이다. 본 발명의 다양한 실시 예들은 너무 큰 FOV(field of view) 영상이 입력되더라도 특징 추출의 퍼포먼스가 높 은 딥러닝 모델 학습을 수행함으로써 인체 성분에 대한 정확한 라벨링을 수행하기 위한 방법 및 장치를 제공할 (뒷면에 계속)"}
{"patent_id": "10-2023-0031372", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 의료 영상을 기초로 인체 성분을 분석하기 위한 방법 및 장치에 관한 것이다. 구체적으로, 본 발명은 의료 영상을 기초로 딥러닝 알고리즘을 이용하여 인체 성분을 분석하기 위한 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0031372", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일반적으로 인체 성분 분석은 생체 임피던스를 측정하여 체지방 및 근육량 등을 산출하는 방법으로 이루어진다. 생체 임피던스 측정 방법은 피측정자의 몸에 전극을 연결하고 미세한 전류를 흘려 보내어 저항값을 측정하여 근 육량과 지방량을 추정하는 방법이다. 그러나, 생체 임피던스 방법은 측정 중에 피측정자의 움직임이나 근육의 긴장 정도에 따라 측정 정확도가 달라지며 피부 면적, 개별 내장 장기나 뼈 등의 성분을 측정하는 문제점이 존 재한다. 이러한 문제점을 개선하기 대하여, 딥러닝 알고리즘을 이용하여 의료 영상으로부터 라벨링을 학습하는 기술(대 한민국 등록특허 10-2321427호)이 개발된 바 있다. 그러나, 이러한 기술은 CNN(Convolutional Neural Network), U-net 등에 기반하여 convolution(합성 곱)의 operation(동작)으로 구성되어 있다. Convolution은 국소 특징의 추출에는 능하지만 전체적인 특징의 추출은 어렵기 때문에, 너무 큰 FOV(field of view) 영상이 입 력될 경우 특징을 추출하여 분할(segmentation)하는 작업의 퍼포먼스가 떨어지는 문제점이 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 등록특허 제10-2321427호(의료영상을 이용한 인체성분 분석 방법 및 그 장치)"}
{"patent_id": "10-2023-0031372", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상술한 바와 같은 논의를 바탕으로, 본 발명의 다양한 실시 예들은 의료 영상을 기초로 딥러닝 알고리즘을 이용 하여 인체 성분을 분석하기 위한 방법 및 장치를 제공한다. 또한, 본 발명의 다양한 실시 예들은 너무 큰 FOV(field of view) 영상이 입력되더라도 특징 추출의 퍼포먼스가 높은 딥러닝 모델 학습을 수행함으로써 인체 성분에 대한 정확한 라벨링을 수행하기 위한 방법 및 장치를 제공 한다. 또한, 본 발명의 다양한 실시 예들은 인체 성분에 대한 정확한 라벨링에 기반하여 인체 성분에 대한 정확한 분 석 정보를 제공하기 위한 방법 및 장치를 제공한다. 또한, 본 발명의 다양한 실시 예들은 딥러닝 모델의 학습 단계에서 3차원 영상을 재구성한 뒤 랜덤한 위치, 즉, 이웃한 부분 영상들 간에 overlap이나 gap이 존재하는 위치에서 3차원 cropping을 수행한 부분 영상들에 대하여 학습을 수행하고, 실제 사용 시에는 2차원 영상들에 대하여 3차원 영상을 재구성한 뒤 규칙적인 위치, 즉, 이웃 한 부분 영상들 간에 overlap이나 gap이 없는 순차적 위치에서 3차원 cropping을 수행한 부분 영상들에 대하여 인체 성분 분석을 수행한 뒤, 분석이 수행된 부분 영상들로부터 3차원 영상을 재구성함으로써 인체 성분에 대한 정확한 라벨링을 수행하기 위한 방법 및 장치를 제공한다."}
{"patent_id": "10-2023-0031372", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 다양한 실시 예들에 따르면, 전자 장치의 동작 방법에 있어서, 상기 전자 장치는 입력 장치, 메모리, 적어도 하나의 프로세서, 출력 장치를 포함하고, 제1 환자의 신체 중 전체 또는 일정 영역에 대하여 3차원 CT (computed tomography) 촬영을 통해 획득된 라벨링 되지 않은 복수의 제1 2차원 의료 영상들을 상기 입력 장치 에 의하여 입력 받는 단계; 상기 복수의 제1 2차원 의료 영상들에 대하여 인체 성분 별로 라벨링이 수행된 복수 의 제2 2차원 의료 영상들을 상기 입력 장치에 의하여 입력 받는 단계; 상기 제1 2차원 의료 영상들에 기반하여 라벨링 되지 않은 제1 3차원 의료 영상을 상기 프로세서에 의하여 생성하는 단계; 상기 제2 2차원 의료 영상들 에 기반하여 라벨링 된 제2 3차원 의료 영상을 상기 프로세서에 의하여 생성하는 단계; 상기 제1 3차원 의료 영 상 내 복수의 무작위의 위치들에서 복수의 제1 3차원 부분 의료 영상들을 상기 프로세서에 의하여 결정하는 단 계; 상기 제2 3차원 의료 영상 내 상기 복수의 제1 3차원 부분 의료 영상들에 대응하는 복수의 위치들에서 복수 의 제2 3차원 부분 의료 영상들을 상기 프로세서에 의하여 결정하는 단계; 상기 복수의 제1 3차원 부분 의료 영 상들 및 상기 복수의 제2 3차원 부분 의료 영상들에 기반하여 상기 메모리에 저장된 인체 성분 분석 모델의 학습을 상기 프로세서에 의하여 수행하는 단계; 제2 환자의 신체 중 전체 또는 일정 영역에 대하여 3차원 CT 촬영 을 통해 획득된 라벨링 되지 않은 복수의 제3 2차원 의료 영상들을 상기 입력 장치에 의하여 입력 받는 단계; 상기 제3 2차원 의료 영상들에 기반하여 라벨링 되지 않은 제3 3차원 의료 영상을 상기 프로세서에 의하여 생성 하는 단계; 상기 제3 3차원 의료 영상 내 복수의 순차적 위치들에서 복수의 제3 3차원 부분 의료 영상들을 상기 프로세서에 의하여 결정하는 단계; 상기 복수의 제3 3차원 부분 의료 영상들에 기반하여 상기 학습된 인체 성분 분석 모델을 이용하여 라벨링 된 복수의 제4 3차원 부분 의료 영상들을 상기 프로세서에 의하여 생성하는 단계; 상기 복수의 제4 3차원 부분 의료 영상들에 기반하여 라벨링 된 제4 3차원 의료 영상을 상기 프로세서에 의하여 생성하는 단계; 상기 제4 3차원 의료 영상에 기반하여 상기 제2 환자의 신체에 대한 인체 성분 분석 정보를 상 기 프로세서에 의하여 생성하는 단계; 상기 제2 환자의 신체에 대한 인체 성분 분석 정보를 상기 출력 장치에 의하여 출력하는 단계를 포함하는 방법이 제공된다. 본 발명의 다양한 실시 예들에 따르면, 전자 장치에 있어서, 전자 장치는 전자 장치는 입력 장치, 메모리, 적어 도 하나의 프로세서, 출력 장치를 포함하고, 본 발명의 다양한 실시 예들에 따른 전자 장치의 동작 방법을 수행 하도록 구성된 전자 장치가 제공된다. 본 발명의 다양한 실시 예들에 따르면, 컴퓨터 프로그램에 있어서, 본 발명의 다양한 실시 예들에 따른 전자 장 치의 동작 방법을 수행하도록 구성되며, 컴퓨터 판독 가능한 저장 매체에 기록된 컴퓨터 프로그램이 제공된다."}
{"patent_id": "10-2023-0031372", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 다양한 실시 예들은 의료 영상을 기초로 딥러닝 알고리즘을 이용하여 인체 성분을 분석하기 위한 방 법 및 장치를 제공할 수 있다. 또한, 본 발명의 다양한 실시 예들은 너무 큰 FOV(field of view) 영상이 입력되더라도 특징 추출의 퍼포먼스가 높은 딥러닝 모델 학습을 수행함으로써 인체 성분에 대한 정확한 라벨링을 수행하기 위한 방법 및 장치를 제공 할 수 있다. 또한, 본 발명의 다양한 실시 예들은 인체 성분에 대한 정확한 라벨링에 기반하여 인체 성분에 대한 정확한 분 석 정보를 제공하기 위한 방법 및 장치를 제공할 수 있다. 또한, 본 발명의 다양한 실시 예들은 딥러닝 모델의 학습 단계에서 3차원 영상을 재구성한 뒤 랜덤한 위치, 즉, 이웃한 부분 영상들 간에 overlap이나 gap이 존재하는 위치에서 3차원 cropping을 수행한 부분 영상들에 대하여 학습을 수행하고, 실제 사용 시에는 2차원 영상들에 대하여 3차원 영상을 재구성한 뒤 규칙적인 위치, 즉, 이웃 한 부분 영상들 간에 overlap이나 gap이 없는 순차적 위치에서 3차원 cropping을 수행한 부분 영상들에 대하여 인체 성분 분석을 수행한 뒤, 분석이 수행된 부분 영상들로부터 3차원 영상을 재구성함으로써 인체 성분에 대한 정확한 라벨링을 수행하기 위한 방법 및 장치를 제공할 수 있다. 본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은 아래의 기재로부터 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0031372", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부한 도면을 참고로 하여 본 발명의 실시 예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 본 발명은 의료 영상을 기초로 딥러닝 알고리즘을 이용하여 인체성분을 분석하기 위한 방법 및 장치에 관한 것 이다. 구체적으로, 본 발명은 인체 성분 별로 라벨링을 한 복수의 2차원 의료 영상들에 대하여 딥러닝 모델이 학습을 할 때 복수의 2차원 의료 영상들을 쌓아서 3차원 영상을 재구성한 후 재구성된 3차원 영상에서 무작위하 게 잘라낸 3차원 부분들로 인체성분 분석 모델의 학습을 하는 구성, 학습된 인체 성분 분석 모델을 이용하여 라 벨링 되지 않은 복수의 2차원 의료 영상들을 입력 받고 3차원 영상을 재구성한 후 설정된 순서에 따라서 잘라낸 3차원 부분들에 기반하여 인체 성분 분석을 수행하는 구성을 포함하는 방법, 상기 방법을 수행하도록 구성된 전 자 장치, 상기 방법을 수행하도록 구성된 컴퓨터 프로그램을 포함한다. 도 1은 본 발명의 다양한 실시 예들에 따른 전자 장치의 동작 개요를 도시한다. 도 1을 참고하면, 본 발명의 다양한 실시 예들에 따른 전자 장치는 의료 영상을 입력 받아서 인체 성 분 분석 정보을 출력하는 구조로 동작한다. 도 2는 본 발명의 다양한 실시 예들에 따른 전자 장치의 구성을 도시한다. 도 2를 참고하면, 전자 장치는 메모리, 프로세서, 입력 장치, 출력 장치를 포함한다. 메모리는, 메모리, 프로세서, 입력 장치, 출력 장치와 연결되고, 입력 장치를 통해 입력된 정보 등을 저장할 수 있다. 또한, 메모리는, 프로세서와 연결되고 프로세서의 동작 을 위한 기본 프로그램, 응용 프로그램, 설정 정보, 프로세서의 연산에 의하여 생성된 정보 등의 데이터를 저장할 수 있다. 메모리는 휘발성 메모리, 비휘발성 메모리 또는 휘발성 메모리와 비휘발성 메모리의 조합 으로 구성될 수 있다. 그리고, 메모리는 프로세서의 요청에 따라 저장된 데이터를 제공할 수 있다. 프로세서는, 본 발명에서 제안한 절차 및/또는 방법들을 구현하도록 구성될 수 있다. 프로세서는 전 자 장치의 전반적인 동작들을 제어한다. 예를 들어, 프로세서는 메모리에 데이터를 기록하고, 읽는다. 또한, 프로세서는 입력 장치를 통해 정보를 입력 받는다. 또한, 프로세서는 출력 장치 를 통해 정보를 출력한다. 프로세서는 적어도 하나의 프로세서(processor)를 포함할 수 있다. 입력 장치는, 프로세서와 연결되고 정보 등을 입력할 수 있다. 입력 장치는 터치 디스플레이, 키 패드, 키보드, 정보 입력 모듈 등을 포함할 수 있다. 일 실시 예에 따라서, 전자 장치는 송수신기를 더 포함할 수 있으며, 입력 장치는 송수신기를 통해 유/무선 통신 네트워크로 연결된 다른 장치로부터 수신한 정보 등을 입력할 수 있다. 출력 장치는, 프로세서와 연결되고 정보 등을 영상/음성 등의 형태로 출력할 수 있다. 출력 장치 는 디스플레이, 스피커, 정보 출력 모듈 등을 포함할 수 있다. 일 실시 예에 따라서, 전자 장치는 송 수신기를 더 포함할 수 있으며, 출력 장치는 송수신기를 통해 유/무선 통신 네트워크로 연결된 다른 장치 에게 정보 등을 전송 출력할 수 있다. 도 2에는 도시되지 않았으나, 일 실시 예에 따라서, 전자 장치는 송수신기를 더 포함할 수 있다. 송수신기 는, 프로세서와 연결되고 신호를 전송 및/또는 수신한다. 송수신기의 전부 또는 일부는 송신기 (transmitter), 수신기(receiver), 또는 트랜시버(transceiver)로 지칭될 수 있다. 송수신기는 유선 접속 시스 템 및 무선 접속 시스템들인 IEEE(institute of electrical and electronics engineers) 802.xx 시스템, IEEE Wi-Fi 시스템, 3GPP(3rd generation partnership project) 시스템, 3GPP LTE(long term evolution) 시스템, 3GPP 5G NR(new radio) 시스템, 3GPP2 시스템, 블루투스(bluetooth) 등 다양한 무선 통신 규격 중 적어도 하나 를 지원할 수 있다. 도 3은 본 발명의 다양한 실시 예들에 따른 전자 장치의 동작 방법을 도시한다. 도 3의 실시 예에서, 전자 장치는 입력 장치, 메모리, 적어도 하나의 프로세서, 출력 장치를 포함한다. 도 3을 참고하면, S301 단계에서, 전자 장치는 제1 환자의 신체 중 전체 또는 일정 영역에 대하여 3차원 CT (computed tomography) 촬영을 통해 획득된 라벨링 되지 않은 복수의 제1 2차원 의료 영상들을 상기 입력 장치 에 의하여 입력 받는다. S302 단계에서, 전자 장치는 상기 복수의 제1 2차원 의료 영상들에 대하여 인체 성분 별로 라벨링이 수행된 복 수의 제2 2차원 의료 영상들을 상기 입력 장치에 의하여 입력 받는다. S303 단계에서, 전자 장치는 상기 제1 2차원 의료 영상들에 기반하여 라벨링 되지 않은 제1 3차원 의료 영상을 상기 프로세서에 의하여 생성한다. S304 단계에서, 전자 장치는 상기 제2 2차원 의료 영상들에 기반하여 라벨링 된 제2 3차원 의료 영상을 상기 프 로세서에 의하여 생성한다. S305 단계에서, 전자 장치는 상기 제1 3차원 의료 영상 내 복수의 무작위의 위치들에서 복수의 제1 3차원 부분 의료 영상들을 상기 프로세서에 의하여 결정한다. S306 단계에서, 전자 장치는 상기 제2 3차원 의료 영상 내 상기 복수의 제1 3차원 부분 의료 영상들에 대응하는 복수의 위치들에서 복수의 제2 3차원 부분 의료 영상들을 상기 프로세서에 의하여 결정한다. S307 단계에서, 전자 장치는 상기 복수의 제1 3차원 부분 의료 영상들 및 상기 복수의 제2 3차원 부분 의료 영 상들에 기반하여 상기 메모리에 저장된 인체 성분 분석 모델의 학습을 상기 프로세서에 의하여 수행한다. S308 단계에서, 전자 장치는 제2 환자의 신체 중 전체 또는 일정 영역에 대하여 3차원 CT 촬영을 통해 획득된 라벨링 되지 않은 복수의 제3 2차원 의료 영상들을 상기 입력 장치에 의하여 입력 받는다. S309 단계에서, 전자 장치는 상기 제3 2차원 의료 영상들에 기반하여 라벨링 되지 않은 제3 3차원 의료 영상을 상기 프로세서에 의하여 생성한다. S310 단계에서, 전자 장치는 상기 제3 3차원 의료 영상 내 복수의 순차적 위치들에서 복수의 제3 3차원 부분 의 료 영상들을 상기 프로세서에 의하여 결정한다. S311 단계에서, 전자 장치는 상기 복수의 제3 3차원 부분 의료 영상들에 기반하여 상기 학습된 인체 성분 분석 모델을 이용하여 라벨링 된 복수의 제4 3차원 부분 의료 영상들을 상기 프로세서에 의하여 생성한다. S312 단계에서, 전자 장치는 상기 복수의 제4 3차원 부분 의료 영상들에 기반하여 라벨링 된 제4 3차원 의료 영 상을 상기 프로세서에 의하여 생성한다. S313 단계에서, 전자 장치는 상기 제4 3차원 의료 영상에 기반하여 상기 제2 환자의 신체에 대한 인체 성분 분 석 정보를 상기 프로세서에 의하여 생성한다. S314 단계에서, 전자 장치는 상기 제2 환자의 신체에 대한 인체 성분 분석 정보를 상기 출력 장치에 의하여 출 력한다. 본 발명의 다양한 실시 예들에 따르면, 상기 라벨링은 피부, 뼈, 내장 장기, 혈관, 근육, 지방, 종양 중 하나 이상의 인체 성분에 대한 구분을 포함할 수 있다. 상기 인체 성분 분석은 환자의 신체 내 인체 성분의 분포 비 율, 분포 정보, 부피 정보, 또는 질량 정보의 분석을 포함할 수 있다. 본 발명의 다양한 실시 예들에 따르면, 도 3의 실시 예는, 상기 제4 3차원 의료 영상에 기반하여 라벨링 된 복 수의 제4 2차원 의료 영상들을 상기 프로세서에 의하여 생성하는 단계; 상기 복수의 제4 2차원 의료 영상들을 상기 출력 장치에 의하여 출력하는 단계를 더 포함할 수 있다. 본 발명의 다양한 실시 예들에 따르면, 각각의 상기 복수의 제4 2차원 의료 영상들은 각각의 상기 복수의 제3 2 차원 의료 영상들에 대응할 수 있다. 본 발명의 다양한 실시 예들에 따르면, 상기 복수의 제1 3차원 부분 의료 영상들, 상기 복수의 제2 3차원 부분 의료 영상들, 상기 복수의 제3 3차원 부분 의료 영상들, 상기 복수의 제4 3차원 부분 의료 영상들은 동일하게 설정된 크기 및 설정된 구조로 구성될 수 있다. 상기 복수의 제1 3차원 부분 의료 영상들, 상기 복수의 제2 3차 원 부분 의료 영상들의 개수는 설정된 수 이상의 무작위의 수에 해당할 수 있다. 상기 복수의 제3 3차원 부분 의료 영상들, 상기 복수의 제4 3차원 부분 의료 영상들의 개수는 상기 제3 3차원 의료 영상, 상기 제4 3차원 의 료 영상의 크기를 상기 설정된 크기로 나눈 수에 해당할 수 있다. 상기 제1 3차원 의료 영상, 상기 제2 3차원 의료 영상은 크기 및 구조가 서로 동일할 수 있다. 상기 제3 3차원 의료 영상, 상기 제4 3차원 의료 영상은 크 기 및 구조가 서로 동일할 수 있다. 본 발명의 다양한 실시 예들에 따르면, 상기 복수의 제2 3차원 부분 의료 영상들 중 이웃한 제2 3차원 부분 의 료 영상들은 서로 오버랩 되거나 또는 서로 갭이 사이에 존재할 수 있다. 상기 복수의 제4 3차원 부분 의료 영 상들 중 이웃한 제4 3차원 부분 의료 영상들은 서로 오버랩 되지 않으며, 또한 서로 갭이 사이에 존재하지 않을 수 있다. 본 발명의 다양한 실시 예들에 따르면, 상기 복수의 제1 3차원 부분 의료 영상들 및 상기 복수의 제2 3차원 부 분 의료 영상들에 기반하여 상기 메모리에 저장된 인체 성분 분석 모델의 학습을 수행하는 S307 단계는, 상기 복수의 제1 3차원 부분 의료 영상들 중 하나의 제1 3차원 부분 의료 영상, 상기 복수의 제2 3차원 부분 의료 영 상들 중 상기 하나의 제1 3차원 부분 의료 영상에 대응하는 하나의 제2 3차원 부분 의료 영상에 기반하여 상기 인체 성분 분석 모델의 학습을 수행하는 단계가 반복되어 수행되는 단계를 포함할 수 있다. 본 발명의 다양한 실시 예들에 따르면, 상기 인체 성분 분석 모델의 학습이 수행될 때마다 상기 복수의 제1 3차 원 부분 의료 영상들 중 서로 다른 하나의 제1 3차원 부분 의료 영상 및 서로 다른 하나의 제2 3차원 부분 의료 영상에 기반하여 상기 인체 성분 분석 모델의 학습이 수행될 수 있다. 본 발명의 다양한 실시 예들에 따르면, 상기 복수의 제3 3차원 부분 의료 영상들에 기반하여 상기 학습된 인체 성분 분석 모델을 이용하여 라벨링 된 복수의 제4 3차원 부분 의료 영상들을 생성하는 S311 단계는, 상기 복수 의 제3 3차원 부분 의료 영상들 중 하나의 제3 3차원 부분 의료 영상에 기반하여 상기 학습된 인체 성분 분석 모델을 이용하여 상기 복수의 제4 3차원 부분 의료 영상들 중 상기 하나의 제3 3차원 부분 의료 영상에 대응하 는 하나의 제4 3차원 부분 의료 영상을 생성하는 단계가 반복되어 수행되는 단계를 포함할 수 있다.본 발명의 다양한 실시 예들에 따르면, 상기 제4 3차원 부분 의료 영상을 생성하는 단계가 반복되어 수행될 때 마다 미리 정해진 규칙에 따라서 상기 복수의 제3 3차원 부분 의료 영상들 중 서로 다른 하나의 제3 3차원 부분 의료 영상에 기반하여 서로 다른 하나의 제4 3차원 부분 의료 영상이 생성될 수 있다. 본 발명의 다양한 실시 예들에 따르면, 전자 장치에 있어서, 상기 전자 장치는 입력 장치, 메모리, 적어도 하나 의 프로세서, 출력 장치를 포함하고, 상기 프로세서는 도 3의 실시 예에 따른 전자 장치의 동작 방법을 수행하 도록 구성된 단말이 제공된다. 본 발명의 다양한 실시 예들에 따르면, 도 3의 실시 예에 따른 전자 장치의 동작 방법을 수행하도록 구성되며, 컴퓨터 판독 가능한 저장 매체에 기록된 컴퓨터 프로그램이 제공된다. 도 4는 본 발명의 다양한 실시 예들에 따른 딥러닝 알고리즘을 구현하기 위한 다층 인공 신경망(multi-layer perceptron, MLP)의 구조를 도시한다. 본 발명의 다양한 실시 예들에 따른 인체 성분 분석 모델은 도 4의 실시 예에 따라서 MLP의 구조로 구성될 수 있다. 인체 성분 분석 모델은 CNN(Convolutional Neural Network), DenseNet, U-net, GoogLeNet, Generative adversarial network 등 다양한 아키텍처로 구현될 수 있다. 예를 들어, 인체 성분 분석 모델이 CNN으로 구현되 는 경우에, 인체 성분 분석 모델은 학습 데이터를 이용하여 인공 신경망의 연결 가중치를 조정하는 학습 과정을 수행한다. 딥 러닝(deep learning)은 최근 기계학습 분야에서 대두되고 있는 기술 중 하나로써, 복수 개의 은닉 계층 (hidden layer)과 이들에 포함되는 복수 개의 유닛(hidden unit)으로 구성되는 신경망(neural network)이다. 딥 러닝 모델에 기본 특성(low level feature)들을 입력하는 경우, 이러한 기본 특성들이 복수 개의 은닉 계층 을 통과하면서 예측하고자 하는 문제를 보다 잘 설명할 수 있는 상위 레벨 특성(high level feature)로 변형된 다. 이러한 과정에서 전문가의 사전 지식 또는 직관이 요구되지 않기 때문에 특성 추출에서의 주관적 요인을 제 거할 수 있으며, 보다 높은 일반화 능력을 갖는 모델을 개발할 수 있게 된다. 나아가, 딥 러닝의 경우 특징 추 출과 모델 구축이 하나의 세트로 구성되어 있기 때문에 기존의 기계학습 이론들 대비 보다 단순한 과정을 통하 여 최종 모델을 형성할 수 있다는 장점이 있다. 다층 인공 신경망(multi-layer perceptron, MLP)는 딥 러닝에 기반하여 여러 개의 노드가 있는 인공 신경망 (artificial neural network, ANN)의 한 종류이다. 각 노드는 동물의 연결 패턴과 유사한 뉴런으로 비선형 활성 화 기능을 사용한다. 이 비선형 성질은 분리할 수 없는 데이터를 선형적으로 구분할 수 있게 한다. 도 4를 참고하면, 본 발명의 다양한 실시 예들에 따른 MLP 모델의 인공 신경망은 입력 계층(input layer), 복수 개의 은닉 계층(hidden layer), 출력 계층(output layer)으로 구성된다. 입력 계층의 노드에는 CT 영상에 관련된 복수의 인자들과 같은 입력 데이터가 입력된다. 여기서, CT 영상 에 관련된 복수의 인자들은 딥 러닝 모델의 기본 특성(low level feature)에 해당한다. 은닉 계층의 노드에서는 입력된 인자들에 기초한 계산이 이루어진다. 은닉 계층은 CT 영상에 관련된 복수의 인자들을 규합시켜 형성된 복수 개의 노드로 정의되는 유닛들이 저장된 계층이다. 은닉 계층 은 도 4에 도시된 바와 같이 복수 개의 은닉 계층으로 구성될 수 있다. 예를 들어, 은닉 계층이 제1 은닉 계층 및 제2 은닉 계층으로 구성될 경우, 제1 은닉 계층(43 1)은 가장 하위 특징인 CT 영상에 관련된 복수의 인자들을 규합시켜 형성된 복수 개의 노드로 정의되는 제 1 유닛들이 저장되는 계층으로서, 제1 유닛은 CT 영상에 관련된 복수의 인자들의 상위 특징에 해당된 다. 제2 은닉 계층은 제1 은닉 계층의 제1 유닛들을 규합시켜 형성된 복수 개의 노드로 정의되는 제2 유닛들이 저장되는 계층으로, 제2 유닛은 제1 유닛의 상위 특징에 해당된다. 출력 계층의 노드에서는 계산된 예측 결과를 나타낸다. 출력 계층에는 복수 개의 예측 결과 유닛들 이 구비될 수 있다. 구체적으로 복수 개의 예측 결과 유닛들은 참(True) 유닛 및 거짓(False) 유닛의 두 개의 유닛들로 구성될 수 있다. 구체적으로, 참 유닛은 CT 영상이 특정 특징과 관계된다는 의미를 지닌 예측 결과 유닛이고, 거짓 유닛은 CT 영상이 특정 특징과 관계되지 않는다는 의미를 지닌 예측 결과 유닛이다. 은닉 계층 중 마지막 계층인 제2 은닉 계층에 포함된 제2 유닛들과 예측 결과 유닛들 간의 연결에 대하여 각각의 가중치들이 부여되게 된다. 이러한 가중치에 기초하여 CT 영상의 특정 특징과 관련 여부 를 예측하게 된다. 예를 들어, 제2 유닛 중 어느 하나의 유닛이 CT 영상이 특정 특징과 관계될 것으로 예측하는 경우 참 유닛 및 거짓 유닛과 각각 연결되는데, 참 유닛 과의 연결에 대해서는 양의 값을 갖는 가중치가 부여될 것이고, 거짓 유닛과의 연결에 대해서는 음의 값을 갖는 가중치가 부여될 것이다. 반대로, 제2 유닛 중 어느 하나의 유 닛이 CT 영상이 특정 특징과 관계되지 않을 것으로 예측하는 경우 참 유닛 및 거짓 유닛과 각각 연결되는데, 참 유닛 과의 연결에 대해서는 음의 값을 갖는 가중치가 부여될 것이고, 거짓 유닛과의 연결에 대해서는 양의 값을 갖는 가중치가 부여될 것이다. 복수 개의 제2 유닛들과 참 유닛 사이에는 복수 개의 연결선들이 형성될 것이다. 복수 개의 연결선들의 총 합이 양의 값을 갖는 경우, 입력 계층에서의 CT 영상에 관련된 복수의 인자들은 CT 영상이 특정 특징 과 관계되는 인자들로 예측될 것이다. 일 실시 예에 따라서, 이러한 CT 영상이 특정 특징과 관계되는지 가능 여 부는 복수 개의 연결선들의 총 합과 미리 설정된 값을 비교하여 예측할 수도 있다. MLP 모델의 인공 신경망은 학습 파라미터들을 조정하여 학습한다. 일 실시 예에 따라서, 학습 파라미터들 은 가중치 및 편차 중 적어도 하나를 포함한다. 학습 파라미터들은 경사 하강법(gradient descent)이라는 최적 화 알고리즘을 통해 반복적으로 조정된다. 주어진 데이터 샘플로부터 예측 결과가 계산될 때마다(순방향 전파, forward propagation), 예측 오류를 측정하는 손실 함수를 통해 네트워크의 성능이 평가된다. 인공 신경망(40 0)의 각 학습 파라미터는 손실 함수의 값을 최소화하는 방향으로 조금식 증가하여 조정되는데, 이 과정은 역 전 파(back-propagation)라고 한다. 도 5는 본 발명의 다양한 실시 예들에 따른 딥러닝 알고리즘에 기반하여 인체 성분을 식별하기 위한 학습 과정 을 도시한다. 도 5는 U-net의 예시로 도시되었으나, 본 발명의 다양한 실시 예들에 따른 인체 성분 분석 모델은 U-net 외에 CNN(Convolutional Neural Network), DenseNet, GoogLeNet, Generative adversarial network 등 다양한 아키 텍처로 구현될 수 있다. 도 5를 참고하면, 상단부는 학습 데이터를 이용하여 U-net의 파라미터를 찾아가는 과정을 나타내고, 하단부는 학습을 통해 알게 된 U-net의 파라미터를 이용하여 인체 성분의 segmentation(분할) 예측 결과를 성분 별로 각 각 다른 색으로 표시함으로써 라벨링(labeling) 하여 제공하는 과정을 나타낸다. 도 5의 상단부를 참고하면, 정답이 있는 데이터를 이용한 학습을 통해 U-net 파라미터, 예를 들어, N개의 파라 미터, w1, ..., wN이 결정될 수 있다. 학습시에, 랜덤 위치에서 crop된 3D CT patch 가 입력되어서 동일한 위치 에서 3D segmentation patch 가 출력될 수 있도록 U-net의 파라미터를 결정한다. 이 때 랜덤 crop(잘라내기)을 매우 여러 번 시행하여 (최소 몇백 번) 공간 상에 miss 되는 영역이 학습시 최소화되도록 한다. 만약 crop 사이 즈가 10x10x10 라 하면, 학습이 끝난 후 얻게 될 U-net 은 10x10x10 image를 입력 받아서 10x10x10 segmentation을 출력하는 구조를 갖는다. 도 5의 하단부를 참고하면, 실제 예측 결과의 제공시에, 라벨링이 없는 영상, 예를 들어 200x200x100 크기의 CT 영상을 여러 개의 10x10x10 크기의 부분 영상으로 규칙적으로 나눈 후 (총 20x20x10 = 4000개), 각 10x10x10 크기의 부분 영상을 이미 학습된 U-net 모델에 입력하여 10x10x10 segmentation patch를 얻을 수 있고, 이런 식으로 반복해서 얻는 총 4000개의 segmentation patch를 이어 붙여서 원 CT 영상 크기와 동일한 200x200x100 크기의 segmentation 결과를 얻을 수 있다. U-net은 인코더와 디코더로 구성되는데, U-net의 인코더(즉, 왼쪽 날개)를 구성하는 핵심인 convolution(합성 곱) 이라는 operation(동작)이 국소 특징 추출에는 능하지만 글로벌한 특징 추출은 어려운 문제점이 존재한다. 만약, 도 5와 같이 crop이 없이 테스트 영상을 한 번에 학습한다면, 예를 들어, crop을 하지 않고 200x200x100 크기의 원 CT 영상을 통으로 U-net에 입력하여 학습한다면, 그와 같이 학습된 U-net에 대하여 실제 예측 결과의 제공시에 너무 큰 FOV (field of view) 영상이 입력으로 들어왔을 때 segmentation을 수행하는 성능이 떨어지게 될 수 있다.의료 영상에서 3D 영상은 영상의 작은 체적 요소를 나타내는 복셀(voxel, volume pixel, 체적 픽셀) 그리드로 구성된다. 3D 영상의 치수는 일반적으로 x축의 200복셀, y축의 200복셀, z축의 100복셀과 같이 각 차원의 복셀 수로 표현된다. 따라서, 3D CT 영상의 크기 관련하여 \"200x200x100\"이라는 표현은 3D 영상의 3차원 각각의 복셀 수를 나타낸다. 도 6은 본 발명의 다양한 실시 예들에 따른 딥러닝 알고리즘에 기반하여 인체 성분을 식별하는 과정을 도시한다. 구체적으로, 도 6은 본 발명의 다양한 실시 예들에서 의료 영상으로부터 인체 성분을 식별하기 위한 인체 성분 분석 모델의 동작 과정을 도시한다. 인체 성분 분석 모델은 피부, 뼈, 내장장기, 혈관, 근육, 지방, 또는 종양 중 하나 이상의 인체 성분 영역을 라 벨링 한 학습데이터를 이용하여 학습시킬 수 있다. 예를 들어, 의료 영상에서 지방 영역을 구분하는 인공지능 모델을 생성하고자 하는 경우에, 학습용 의료영상에 지방 영역을 라벨링 한 학습데이터를 이용하여 인체 성분 분석 모델을 학습시킬 수 있다. 다른 예로, 피부, 지방과 함께 근육이나 뼈, 또는 신경 등 복수의 인체 성분을 동시에 구분하는 인체 성분 분석 모델을 생성하고자 하는 경우에는 학습용 의료 영상에 구분하고자 하는 각 인 체 조직을 라벨링한 학습 데이터를 이용하여 인체 성분 분석 모델을 학습시킬 수 있다. 학습 완료된 인체 성분 분석 모델은 검사 대상 의료 영상을 입력 받으면 검사 대상 의료 영상으로부터 각 인체 성분을 구분한 결과를 출력한다. 도 7은 본 발명의 다양한 실시 예들에 따른 딥러닝 알고리즘에 기반하여 인체 성분을 식별하는 과정을 도시한다. 구체적으로, 도 7은 본 발명의 다양한 실시 예들에서 의료 영상으로부터 인체 성분을 식별하기 위한 인체 성분 분석 모델의 구조를 도시한다. 도 7은 예시적으로 Vision transformer를 이용한 인체 성분 식별의 학습 과정 구조를 도시한다. 도 7에서, H, W, D는 3차원 입력 영상의 가로, 세로, 깊이 방향 크기를 나타낸다. 도 7에서, Z3, Z6, Z9, Z12는 합성 곱 기반의 인코더의 각 레이어 마다의 출력 텐서(tensor)를 의미한다. 예를 들어, Z3은 H/16 X W/16 X D/16 X 768 의 크기를 갖는 4차원 텐서를 의미한다. 기계 학습에서 텐서는 숫자 값의 다차원 배열로 표현될 수 있는 수학적 개체이다. 기계 학습 알고리즘에서 입력, 출력 및 중간 계산을 나타내는 데 사용되는 기본 데이터 구조이다. 텐서는 순위라고 하는 다양한 수의 차 원을 가질 수 있다. 예를 들어 스칼라(단일 값)는 순위 0 텐서이고 벡터(값 배열)는 순위 1 텐서이고 행렬(값의 2D 배열)은 순위 2 텐서이다. 텐서는 딥 러닝, 컨볼루션 신경망, 순환 신경망 등 다양한 유형의 기계 학습 알고 리즘에 사용된다. 또한 물리학 및 공학과 같은 다른 많은 과학 분야에서 데이터를 나타내는 데 사용된다. 텐서 는 기계 학습 알고리즘에서 데이터를 나타내는 데 사용되는 숫자 값의 다차원 배열이다. 도 7을 참고하면, 인체 성분 분석 모델은 입력된 영상을 특정 설정된 크기로 분할하여, 각각의 분할된 부분 영 상에 대하여 특징을 추출하여 인체 성분의 식별을 통한 인체 성분 별 영상 영역의 segmentation(분할)을 수행하 고, 분할된 영역 별로 라벨링으로 표시하고, 각각의 라벨링 된 부분 영상을 합쳐서 하나의 라벨링 된 출력 영상 을 생성하여 출력한다. 인체 성분 분석 모델은 입력 영상이 2차원 영상이든 3차원 영상이든 적용될 수 있다. 입력 영상을 동일한 크기 의 부분 영상들로 분할한 뒤 인체 성분의 식별을 통해 segmentation, labeling을 수행하고, labeling 된 각각의 부분 영상들로부터 하나의 전체 출력 영상을 생성한다는 동작 원리가 2차원 영상이든 3차원 영상이든 동일하게 적용될 수 있다. 도 7의 인체 성분 분석 모델의 구조에서 좌측이 인코더이고 우측이 디코더에 해당한다. 도 7의 예시적인 인체 성분 분석 모델의 인코더는 트랜스포머 기반 아키텍처에 해당하는 ViT(Vision Transformer)로 나타나 있다. 그러나, ViT는 예시적인 것에 불과하며, 인체 성분 분석 모델의 인코더는 다양한 아키텍쳐가 적용될 수 있다. 예를 들어, CNN에 속하는 U-Net이 본 발명의 다양한 실시 예들에 따른 인체 성분 분석 모델의 인코더로 사용될 수도 있다. 인코더-디코더 아키텍처에서 인코더는 입력 데이터를 처리하고 압축된 표현을 생성한 다음 디코더로 전달되어 출력을 생성하는 역할을 한다. 인코더 아키텍처의 선택은 입력 데이터의 특성과 당면한 작업에 따라 다르다. 다음은 인코더-디코더 아키텍처에서 사용할 수 있는 인코더 아키텍처의 몇 가지 예시이다. 컨볼루션 신경망(convolutional neural networks, CNN): CNN은 일반적으로 이미지 및 비디오 처리 작업에서 인 코더로 사용된다. 이미지와 같은 그리드와 같은 구조의 입력 데이터를 처리하도록 설계되었으며 컨벌루션 및 풀 링 작업을 연속적으로 적용하여 입력 데이터의 계층적 표현을 학습할 수 있다. 순환 신경망(recurrent neural networks, RNN): RNN은 일반적으로 입력 데이터가 일련의 단어인 자연어 처리 작업에서 인코더로 사용된다. RNN은 가변 길이의 시퀀스를 처리하고 시퀀스의 다른 부분 간의 종속성을 캡처하 는 방법을 학습할 수 있다. 트랜스포머 기반 아키텍처(transformer-based architectures): 자연어 처리를 위한 원래 Transformer 아키텍처 에 사용된 Transformer와 같은 Transformer 기반 아키텍처도 인코더로 사용할 수 있다. 이러한 아키텍처는 self-attention 메커니즘을 사용하여 입력 데이터를 처리하고 입력의 서로 다른 부분 간의 장거리 종속성을 캡 처할 수 있다. 오토인코더(autoencoders): 오토인코더는 입력 데이터를 재구성하기 위해 인코더와 디코더가 함께 훈련되는 인 코더-디코더 아키텍처로 사용할 수 있는 신경망 유형이다. Autoencoder는 입력 데이터의 압축된 표현을 학습할 수 있으며, 이는 이미지 압축 또는 이상 탐지와 같은 작업에 유용할 수 있다. 전반적으로 인코더 아키텍처의 선택은 입력 데이터의 특성과 당면한 작업에 따라 달라지며 다른 유형의 입력 데 이터 및 작업에는 다른 인코더 아키텍처가 더 적합할 수 있다. 도 7의 예시적인 인체 성분 분석 모델에서 각각의 인코더 구성에 대한 개략적인 설명은 다음과 같다. ViT(Vision Transformer)는 입력 이미지를 처리하기 위해 셀프-어텐션 메커니즘(self-attention mechanism)을 사용하는 컴퓨터 비전 작업을 위한 인기 있는 딥 러닝 아키텍처이다. 다음은 ViT의 핵심 구성 요소이다. 임베디드 패치(Embedded patches): 입력 이미지는 겹치지 않는 패치로 분할된 다음 학습 가능한 선형 프로젝션 레이어를 사용하여 저차원 기능 공간에 선형으로 프로젝션된다. 이 변환을 통해 네트워크는 글로벌 컨텍스트를 유지하면서 이미지의 로컬 기능에서 작동할 수 있다. 멀티 헤드 어텐션(multi-head attention): 내장된 패치는 이미지의 중요한 영역에 주의(attention)를 기울이는 방법을 학습하는 셀프-어텐션 메커니즘(self-attention mechanism)에 의해 처리된다. 어텐션 메커니즘은 서로 다른 학습된 가중치 세트(즉, 헤드(head))로 여러 번 적용되며, 이를 통해 네트워크는 입력에서 서로 다른 패턴 과 종속성을 캡처할 수 있다. 정규화(normalization): 학습 과정을 안정화하고 네트워크의 일반화 성능을 향상시키기 위해 레이어 정규화를 각 어텐션 레이어의 출력과 MLP에 적용한다. 다층 퍼셉트론(multi-layer perceptron, MLP): 어텐션 메커니즘의 출력은 각 패치 표현에 개별적으로 비선형 변 환을 적용하는 다층 퍼셉트론(multi-layer perceptron, MLP)을 통해 전달된다. 전반적으로 ViT 아키텍처는 이러한 구성 요소를 사용하여 입력 이미지를 일련의 패치 표현으로 변환한 다음 일 련의 어텐션 및 MLP 레이어에서 처리하여 최종 출력을 생성한다. 도 8은 본 발명의 다양한 실시 예들에 따른 인체 성분이 라벨링 된 영상의 일 예를 도시한다. 도 8을 참고하면, 인체의 복부-골반 CT(Abdomen & Pelvis CT, APCT)에서 피하 지방(Subcutaneous fat)(label 1), 내장 지방(Visceral fat)(label 4), 그 외 지방(그 외 fat)(label 5), 골격근(Skeletal muscle)(label 2), 요근(Psoas muscle)(label 3), 척추 기립근(Erector spinae)(label 8), 다열근(Multifidus)(label 9)로 라벨링 되었음을 확인할 수 있다.도 8은 7개의 종류로 라벨링 한 경우에 해당한다. 도 8은 실제 임상의가 segmentation을 수행한 결과에 해당하며, 도 8과 같은 학습 데이터에 기반하여 본 발명에 서 제안하는 인체 성분 분석 모델의 학습을 수행하게 된다. 도 9은 본 발명의 다양한 실시 예들에 따른 인체 성분이 라벨링 된 영상의 일 예를 도시한다. 도 9을 참고하면, 인체의 복부-골반 CT(abdominal-pelvic CT, APCT)에서 피하 지방(Subcutaneous fat)(label 1), 내장 지방(Visceral fat)(label 4), 그 외 지방(그 외 fat)(label 5), 골격근(Skeletal muscle)(label 2), 요근(Psoas muscle)(label 3), 척추 기립근(Erector spinae)(label 2), 다열근(Multifidus)(label 2)로 라벨링 되었음을 확인할 수 있다. 도 9은 도 8과 비교하여 5개의 종류로 라벨링 한 경우에 해당한다. 도 9은 실제 임상의가 segmentation을 수행한 결과에 해당하며, 도 9와 같은 학습 데이터에 기반하여 본 발명에 서 제안하는 인체 성분 분석 모델의 학습을 수행하게 된다. 도 10는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 10에서, 좌측 이미지는 도 8에 대하여 실제 CT 영상과 라벨링한 영상을 비교한 이미지이고, 우측 이미지는 도 9에 대하여 실제 CT 영상과 라벨링한 영상을 비교한 이미지이다. 도 11는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 한 성능 결과의 일 예를 도 시한다. 도 11은 본 발명에서 제안하는 인체 성분 분석 모델에 기반하여 인체 성분의 segmentation을 수행하였을 경우, 실제 임상의가 segmentation을 수행한 것과 비교하여 상대적인 정확도를 나타낸다. DC(dice coefficient)는 원본 이미지의 실제 값(ground truth, GT)과 인공지능 모델이 예측한 segmentation 값 의 차이를 나타내는 지표로서 이미지의 픽셀 단위 예측을 나타낸다. 피하 지방 조직(subcutaneous adipose tissue, SAT), 골격근(skeletal muscle, SM), 요근(psoas muscle, PM), 내장 지방 조직(visceral adipose tissue, VAT), 기타 지방 조직(other adipose tissue, OAT)에 대하여 인체 성분 segmentation을 수행한 결과, 본 발명에서 제안하는 인체 성분 분석 모델은 평균 89.20%의 정확도를 나타 내었다. 이것은 원 CT 영상을 crop 하지 않고 원 CT 영상을 통으로 U-net에 입력하여 인체 성분 segmentation을 학습하는 경우에 비하여 segmentation의 성능이 높은 수치를 나타낸다. 도 12는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 12는 축면(axial plane)에 대하여 촬영된 CT 영상에 기반한 인체 성분 segmentation 영상을 나타낸다. 축면 (axial plane)은 가로면(transverse plane)이라고도 하며 신체의 장축(long axis)에 수직인 면을 말한다. CT 영상에서 축상 영상(axial image)은 다양한 각도에서 신체의 여러 X선 영상을 촬영한 다음 일련의 단면 영상으 로 재구성하여 얻는다. Axial 이미지는 일반적으로 뷰어가 환자의 머리에 해당하는 이미지 상단과 함께 환자의 발에서 올려다보는 것처럼 표시된다. 도 12는 실제 CT 영상(image), 임상의에 의하여 segmentation이 수행된 GT(ground truth, GT) 영상, 본 발명에 서 제안하는 인체 성분 분석 모델에 의하여 segmentation이 수행된 예측(prediction) 영상을 나타낸다. 도 12를 참고하면, 본 발명에서 제안하는 인체 성분 분석 모델은 임상의에 의하여 segmentation이 수행된 것과 유사한 결과를 출력한다.도 13는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 13는 축면(axial plane)에 대하여 촬영된 CT 영상에 기반한 인체 성분 segmentation 영상을 나타낸다. 도 13는 실제 CT 영상(image), 임상의에 의하여 segmentation이 수행된 GT(ground truth, GT) 영상, 본 발명에 서 제안하는 인체 성분 분석 모델에 의하여 segmentation이 수행된 예측(prediction) 영상을 나타낸다. 도 13를 참고하면, 본 발명에서 제안하는 인체 성분 분석 모델은 임상의에 의하여 segmentation이 수행된 것과 유사한 결과를 출력한다. 도 14는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 14는 시상면(sagittal plane)에 대하여 촬영된 CT 영상에 기반한 인체 성분 segmentation 영상을 나타낸다. 시상면(sagittal plane)은 몸을 좌우로 나누는 면을 말한다. CT 촬영에서 시상면 이미지(sagittal image)는 측 면에서 신체의 X-선 이미지를 촬영하여 얻을 수 있으며 신체의 좌우 축을 따라 움직이는 구조를 보는 데 유용하 다. 시상 이미지는 일반적으로 보는 사람이 환자의 한쪽에 서서 측면에서 신체를 직접 바라보고 있는 것처럼 표 시된다. 도 14는 실제 CT 영상(image), 임상의에 의하여 segmentation이 수행된 GT(ground truth, GT) 영상, 본 발명에 서 제안하는 인체 성분 분석 모델에 의하여 segmentation이 수행된 예측(prediction) 영상을 나타낸다. 도 14를 참고하면, 본 발명에서 제안하는 인체 성분 분석 모델은 임상의에 의하여 segmentation이 수행된 것과 유사한 결과를 출력한다. 도 15는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 15는 시상면(sagittal plane)에 대하여 촬영된 CT 영상에 기반한 인체 성분 segmentation 영상을 나타낸다. 도 15는 실제 CT 영상(image), 임상의에 의하여 segmentation이 수행된 GT(ground truth, GT) 영상, 본 발명에 서 제안하는 인체 성분 분석 모델에 의하여 segmentation이 수행된 예측(prediction) 영상을 나타낸다. 도 15를 참고하면, 본 발명에서 제안하는 인체 성분 분석 모델은 임상의에 의하여 segmentation이 수행된 것과 유사한 결과를 출력한다. 도 16는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 16는 관상면(coronal plane)에 대하여 촬영된 CT 영상에 기반한 인체 성분 segmentation 영상을 나타낸다. 관상면은 몸을 앞뒤로 나누는 면을 말한다. CT 영상에서 관상 영상(coronal image)은 신체의 X선 영상을 정면 또는 후면에서 촬영하여 얻어지며, 신체의 전후축을 따라 흐르는 구조를 관찰하는 데 유용하다. 관상 영상은 일 반적으로 보는 사람이 환자의 앞이나 뒤에 서서 정면이나 뒤에서 몸을 직접 바라보는 것처럼 표시된다. 도 16는 실제 CT 영상(image), 임상의에 의하여 segmentation이 수행된 GT(ground truth, GT) 영상, 본 발명에 서 제안하는 인체 성분 분석 모델에 의하여 segmentation이 수행된 예측(prediction) 영상을 나타낸다. 도 16를 참고하면, 본 발명에서 제안하는 인체 성분 분석 모델은 임상의에 의하여 segmentation이 수행된 것과 유사한 결과를 출력한다. 도 17는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 17는 실제 CT 영상(image), 임상의에 의하여 segmentation이 수행된 GT(ground truth, GT) 영상, 본 발명에 서 제안하는 인체 성분 분석 모델에 의하여 segmentation이 수행된 예측(prediction) 영상을 나타낸다. 도 17를 참고하면, 본 발명에서 제안하는 인체 성분 분석 모델은 임상의에 의하여 segmentation이 수행된 것과 유사한 결과를 출력한다. 하드웨어를 이용하여 본 발명의 실시 예를 구현하는 경우에는, 본 발명을 수행하도록 구성된 ASICs(application specific integrated circuits) 또는 DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays) 등이 본 발명의 프로 세서에 구비될 수 있다. 한편, 상술한 방법은, 컴퓨터에서 실행될 수 있는 프로그램으로 작성 가능하고, 컴퓨터 판독 가능 매체를 이용 하여 상기 프로그램을 동작시키는 범용 디지털 컴퓨터에서 구현될 수 있다. 또한, 상술한 방법에서 사용된 데이 터의 구조는 컴퓨터 판독 가능한 저장 매체에 여러 수단을 통하여 기록될 수 있다. 본 발명의 다양한 방법들을 수행하기 위한 실행 가능한 컴퓨터 코드를 포함하는 저장 디바이스를 설명하기 위해 사용될 수 있는 프로그램 저장 디바이스들은, 반송파(carrier waves)나 신호들과 같이 일시적인 대상들은 포함하는 것으로 이해되지는 않 아야 한다. 상기 컴퓨터 판독 가능한 저장 매체는 마그네틱 저장매체(예를 들면, 롬, 플로피 디스크, 하드 디스 크 등), 광학적 판독 매체(예를 들면, 시디롬, DVD 등)와 같은 저장 매체를 포함한다. 이상에서 설명된 실시 예들은 본 발명의 구성요소들과 특징들이 소정 형태로 결합된 것들이다. 각 구성요소 또 는 특징은 별도의 명시적 언급이 없는 한 선택적인 것으로 고려되어야 한다. 각 구성요소 또는 특징은 다른 구 성요소나 특징과 결합되지 않은 형태로 실시될 수 있다. 또한, 일부 구성요소들 및/또는 특징들을 결합하여 본 발명의 실시 예를 구성하는 것도 가능하다. 발명의 실시 예들에서 설명되는 동작들의 순서는 변경될 수 있다. 어느 실시 예의 일부 구성이나 특징은 다른 실시 예에 포함될 수 있고, 또는 다른 실시 예의 대응하는 구성 또 는 특징과 교체될 수 있다. 특허청구범위에서 명시적인 인용 관계가 있지 않은 청구항들을 결합하여 실시 예를 구성하거나 출원 후의 보정에 의해 새로운 청구항으로 포함시킬 수 있음은 자명하다. 본 발명이 본 발명의 기술적 사상 및 본질적인 특징을 벗어나지 않고 다른 형태로 구체화될 수 있음은 본 발명 이 속한 분야 통상의 기술자에게 명백할 것이다. 따라서, 상기 실시 예는 제한적인 것이 아니라 예시적인 모든 관점에서 고려되어야 한다. 본 발명의 권리범위는 첨부된 청구항의 합리적 해석 및 본 발명의 균등한 범위 내 가능한 모든 변화에 의하여 결정되어야 한다."}
{"patent_id": "10-2023-0031372", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 다양한 실시 예들에 따른 전자 장치의 동작 개요를 도시한다. 도 2는 본 발명의 다양한 실시 예들에 따른 전자 장치의 구성을 도시한다. 도 3은 본 발명의 다양한 실시 예들에 따른 전자 장치의 동작 방법을 도시한다. 도 4는 본 발명의 다양한 실시 예들에 따른 딥러닝 알고리즘을 구현하기 위한 다층 인공 신경망(multi-layer perceptron, MLP)의 구조를 도시한다. 도 5는 본 발명의 다양한 실시 예들에 따른 딥러닝 알고리즘에 기반하여 인체 성분을 식별하기 위한 학습 과정 을 도시한다. 도 6은 본 발명의 다양한 실시 예들에 따른 딥러닝 알고리즘에 기반하여 인체 성분을 식별하는 과정을도시한다. 도 7은 본 발명의 다양한 실시 예들에 따른 딥러닝 알고리즘에 기반하여 인체 성분을 식별하는 과정을 도시한다. 도 8은 본 발명의 다양한 실시 예들에 따른 인체 성분이 라벨링 된 영상의 일 예를 도시한다. 도 9은 본 발명의 다양한 실시 예들에 따른 인체 성분이 라벨링 된 영상의 일 예를 도시한다. 도 10는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 11는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 한 성능 결과의 일 예를 도 시한다. 도 12는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 13는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 14는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 15는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 16는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다. 도 17는 본 발명의 다양한 실시 예들에 따른 학습된 딥러닝 모델을 이용하여 라벨링 된 영상의 일 예를 도시한 다."}
