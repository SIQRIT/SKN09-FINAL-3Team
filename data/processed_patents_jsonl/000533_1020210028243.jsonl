{"patent_id": "10-2021-0028243", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0124511", "출원번호": "10-2021-0028243", "발명의 명칭": "저지연 인공지능 서비스 제공 방법, 그리고 이를 위한 네트워크 시스템", "출원인": "주식회사 케이티", "발명자": "유현"}}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서에 의해 동작하는 관리 장치의 동작 방법으로서, 인공지능(Artificial Intelligence, AI) 서비스 태스크 처리를 위해 계층적으로 연결된 워커 장치들로부터, 컴퓨팅 자원 상태 및/또는 네트워크 상태를 포함하는 상태 알림 메시지를 수신하는 단계,각 워커 장치의 컴퓨팅 자원 상태 및/또는 네트워크 상태에 따라, AI 데이터에 대한 AI 서비스 태스크를 처리할적어도 하나의 워커 장치 및 데이터 분담 정책을 결정하는 단계, 그리고상기 AI 데이터에 대한 AI 서비스 태스크를 처리할 적어도 하나의 워커 장치로, 상기 데이터 분담 정책을 전송하는 단계를 포함하며,상기 AI 데이터는 상기 워커 장치들 중 최하위 워커 장치에 연결된 데이터 수집 장치로부터 입력되는, 동작 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서,상기 데이터 분담 정책을 전송하는 단계는상기 최하위 워커 장치에서의 처리 데이터와, 상기 최하위 워커 장치의 상위 워커 장치에서의 처리 데이터의 비율을 포함하는 상기 데이터 분담 정책을 상기 최하위 워커 장치로 전송하는, 동작 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에서,상기 적어도 하나의 워커 장치의 컴퓨팅 자원 상태 및/또는 네트워크 상태가 변경되면, 상기 AI 데이터에 대한AI 서비스 태스크를 처리할 적어도 하나의 워커 장치 및 데이터 분담 정책을 재결정하고, 재결정한 데이터 분담정책을 해당 워커 장치로 전송하는 단계를 더 포함하는, 동작 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에서,상기 데이터 분담 정책을 결정하는 단계는상기 최하위 워커 장치에서 상기 AI 데이터에 대해 제공하는 AI 서비스 지연을 최소화하는 상기 데이터 분담 정책을 결정하는, 동작 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에서상기 데이터 분담 정책을 결정하는 단계는각 워커 장치의 컴퓨팅 자원 상태를 기초로 계산된 데이터 처리 시간, 워커 장치 간 네트워크 상태를 기초로 계산된 데이터 전송 시간을 고려하여, 상기 AI 서비스 지연을 최소화하는 상기 데이터 분담 정책을 결정하는, 동작 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에서,공개특허 10-2022-0124511-3-상기 계층적으로 연결된 워커 장치들은 차등된 컴퓨팅 자원을 포함하는, 동작 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "적어도 하나의 프로세서에 의해 동작하는 워커 장치가 저지연 인공지능(Artificial Intelligence, AI) 서비스를제공하는 방법으로서,연결된 데이터 수집 장치로부터 AI 서비스 태스크의 처리 대상인 AI 데이터를 입력받는 단계,데이터 분담 정책에 따라 상기 AI 데이터를 내부 처리 데이터와 외부 처리 데이터로 분배하는 단계,상기 내부 처리 데이터에 대해 상기 AI 서비스 태스크를 수행하여, 상기 내부 처리 데이터에 대한 분석 결과를생성하는 단계,상위 워커 장치로 상기 외부 처리 데이터에 분석을 요청하고, 상기 상위 워커 장치로부터 상기 외부 처리 데이터에 대한 분석 결과를 획득하는 단계, 그리고상기 내부 처리 데이터 및 상기 외부 처리 데이터에 대한 분석 결과를 이용하여 상기 AI 데이터에 대한 AI 서비스를 제공하는 단계를 포함하는, AI 서비스 제공 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에서,관리 장치로, 컴퓨팅 자원 상태 및/또는 네트워크 상태를 포함하는 상태 알림 메시지를 전송하는 단계, 그리고상기 관리 장치로부터 상기 상태 알림 메시지를 기초로 생성된 상기 데이터 분담 정책을 수신하는 단계를 더 포함하는, AI 서비스 제공 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에서,상기 네트워크 상태는상기 상위 워커 장치와 연결된 네트워크의 전송 속도 및/또는 전송 지연을 포함하는, AI 서비스 제공 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에서,상기 상태 알림 메시지는측정된 AI 서비스 지연을 더 포함하는, AI 서비스 제공 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "코어망의 에지에 구축되고, 적어도 하나의 프로세서에 의해 동작하는 워커 장치가 저지연 인공지능(ArtificialIntelligence, AI) 서비스를 제공하는 방법으로서,하위 워커 장치로부터, AI 서비스 태스크의 처리 대상인 AI 데이터를 입력받는 단계,데이터 분담 정책에 따라 상기 AI 데이터를 내부 처리 데이터와 외부 처리 데이터로 분배하는 단계,상기 내부 처리 데이터에 대해 상기 AI 서비스 태스크를 수행하여, 상기 내부 처리 데이터에 대한 분석 결과를생성하는 단계,상위 워커 장치로 상기 외부 처리 데이터에 분석을 요청하고, 상기 상위 워커 장치로부터 상기 외부 처리 데이터에 분석 결과를 획득하는 단계, 그리고상기 내부 처리 데이터 및 상기 외부 처리 데이터에 대한 분석 결과를 상기 하위 워커 장치로 전송하는 단계를 포함하는, AI 서비스 제공 방법.공개특허 10-2022-0124511-4-청구항 12 제11항에서,무선 통신망을 통해 상기 하위 워커 장치와 통신하고,유선 통신망을 통해 상기 상위 워커 장치와 통신하는, AI 서비스 제공 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에서,상기 AI 데이터를 입력받는 단계는다중 접속 에지 컴퓨팅(Multi-Access Edge Computing, MEC) 서비스 지역에 설치된 무선 접속망을 통해 상기 AI데이터를 입력받는, AI 서비스 제공 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에서,관리 장치로, 컴퓨팅 자원 상태 및/또는 네트워크 상태를 포함하는 상태 알림 메시지를 전송하는 단계, 그리고상기 관리 장치로부터 상기 상태 알림 메시지를 기초로 생성된 상기 데이터 분담 정책을 수신하는 단계를 더 포함하는, AI 서비스 제공 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에서,상기 네트워크 상태는상기 상위 워커 장치와 연결된 네트워크의 전송 속도 및/또는 전송 지연을 포함하는, AI 서비스 제공 방법."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "저지연 인공지능(Artificial Intelligence, AI) 서비스를 제공하는 네트워크 시스템으로서,AI 서비스 태스크 처리를 위해 계층적으로 연결된 워커 장치들을 포함하고,상기 워커 장치들 중 최하위 워커 장치는데이터 수집 장치에 연결되고, 상기 데이터 수집 장치에서 입력된 AI 데이터를 데이터 분담 정책에 따라 제1 내부 처리 데이터와 제1 외부 처리 데이터로 분배하고, 상기 제1 내부 처리 데이터에 대해 상기 AI 서비스 태스크를 수행하여, 상기 제1 내부 처리 데이터에 대한 분석 결과를 생성하며, 상위 워커 장치로부터 상기 제1 외부처리 데이터에 대한 분석 결과를 획득한 후, 상기 제1 내부 처리 데이터 및 상기 제1 외부 처리 데이터에 대한분석 결과를 이용하여 상기 AI 데이터에 대한 AI 서비스를 제공하는, 네트워크 시스템."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에서,상기 워커 장치들 중 상기 상위 워커 장치는상기 최하위 워커 장치로부터, 상기 제1 외부 처리 데이터에 대한 분석 요청을 수신하고, 상기 제1 외부 처리데이터에 대해 상기 AI 서비스 태스크를 수행하여, 상기 제1 외부 처리 데이터에 대한 분석 결과를 생성한 후,상기 최하위 워커 장치로 상기 제1 외부 처리 데이터에 대한 분석 결과를 응답하는, 네트워크 시스템."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항에서,상기 워커 장치들 중 상기 상위 워커 장치는공개특허 10-2022-0124511-5-상기 최하위 워커 장치로부터, 상기 제1 외부 처리 데이터에 대한 분석 요청을 수신하고, 상기 제1 외부 처리데이터를 데이터 분담 정책에 따라 제2 내부 처리 데이터와 제2 외부 처리 데이터로 분배하고, 상기 제2 내부처리 데이터에 대해 상기 AI 서비스 태스크를 수행하여, 상기 제2 내부 처리 데이터에 대한 분석 결과를 생성하며, 최상위 워커 장치로부터 상기 제2 외부 처리 데이터에 대한 분석 결과를 획득한 후, 상기 제2 내부 처리 데이터 및 상기 제2 외부 처리 데이터에 대한 분석 결과를 상기 최하위 워커 장치로 응답하는, 네트워크 시스템."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제16항에서,상기 워커 장치들 각각은 관리 장치로, 컴퓨팅 자원 상태 및/또는 네트워크 상태를 포함하는 상태 알림 메시지를 전송하고,상기 최하위 워커 장치는상기 관리 장치로부터 상기 데이터 분담 정책을 수신하는, 네트워크 시스템."}
{"patent_id": "10-2021-0028243", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제16항에서,상기 최하위 워커 장치는 다중 접속 에지 컴퓨팅(Multi-Access Edge Computing, MEC) 서비스 지역에 설치된 무선 접속망을 통해 상기 상위 워커 장치에 접속하는 AI 서비스 제공 단말이고,상기 상위 워커 장치는 코어망의 에지에 구축된 MEC 장치인, 네트워크 시스템."}
{"patent_id": "10-2021-0028243", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "적어도 하나의 프로세서에 의해 동작하는 관리 장치의 동작 방법으로서, 인공지능(Artificial Intelligence, AI) 서비스 태스크 처리를 위해 계층적으로 연결된 워커 장치들로부터, 컴퓨팅 자원 상태 및/또는 네트워크 상태를 포함하는 상태 알림 메시지를 수신하는 단계, 각 워커 장치의 컴퓨팅 자원 상태 및/또는 네트워크 상태에 따라, AI 데이터에 대한 AI 서비스 태스크를 처리할 적어도 하나의 워커 장치 및 데이터 분담 정책을 결정하는 단계, 그리고 상기 AI 데이터에 대한 AI 서비스 태스크를 처리할 적어도 하나의 워커 장치로, 상기 데이터 분담 정책을 전송하는 단계를 포함한다. 상기 AI 데이터는 상기 워커 장치들 중 최하위 워커 장치에 연결된 데이터 수집 장치 로부터 입력된다."}
{"patent_id": "10-2021-0028243", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 에지 컴퓨팅(Edge Computing)에 관한 것이다."}
{"patent_id": "10-2021-0028243", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(Artificial Intelligence, AI) 서비스란 학습된 AI 모델을 통해 제공되는 각종 서비스로서, 예를 들면, 실시간 영상 스트림 분석을 통한 객체 인식 서비스, AR/VR 콘텐츠 서비스 등이 있다. 단독형 AI 서비스의 경우, AI 프로세서를 탑재한 장치가 카메라 근처에 설치되고, 카메라로부터 수신한 영상을 바로 처리함으로써, 초저지연 AI 서비스를 제공할 수 있다. 하지만, 장치가 저사양의 AI 프로세서를 탑재한 경 우, 제공할 수 있는 AI 서비스가 한정될 수 밖에 없고, 상당한 컴퓨팅 자원이 요구되는 고성능의 AI 서비스를 제공하지 못한다. 예를 들어, 실시간 영상에서 많은 객체들을 인식해야 하는 서비스는 데이터 처리를 위한 상당 한 컴퓨팅 자원이 필요한데, 이를 처리할 수 있는 고사양의 AI 프로세서는 고가라서 장치 가격이 높아지게 된다. 서비스형 AI 서비스의 경우, 중앙 서버가 다수의 로컬 장치들로부터 영상을 수신하고, 분석 결과를 해당 로컬 장치로 회신한다. 로컬 장치는 수신한 분석 결과에 따라 동작하기 때문에, AI 프로세서 없이, 카메라와 통신 모 듈, 저사양 프로세서로 구현되더라도 고성능의 AI 서비스를 제공하면서도, 가격이 저렴하다. 하지만, 로컬 장치 가 영상을 인코딩해서 통신망을 통해 중앙 서버로 전송하고, 분석 결과를 통신망을 통해 수신하는 과정에서 지 연이 발생하기 때문에, 초저지연 서비스를 제공하지 못한다. 특히 방역 로봇과 같은 이동형 AI 서비스는 무선 통신망을 사용하는데, 네트워크 상황에 따라 지연이 발생하여 실시간 서비스에 차질이 발생할 수 있다."}
{"patent_id": "10-2021-0028243", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 AI 서비스 태스크를 처리하는 워커(Worker) 장치들을 관리하고, 워커 장치들의 컴퓨팅 자원 그리고 워커 장치간 네트워크 상태를 기초로, 워커 장치들의 데이터 분담 정책을 결정하는 관리 장치를 제공하는 것이다. 본 개시는 차등 컴퓨팅 자원을 가진 워커 장치들을 계층적으로 연결하고, AI 서비스 품질과 태스크 처리에 필요 한 컴퓨팅 자원을 고려하여, 태스크 처리를 담당하는 적어도 하나의 워커 장치를 결정하는 관리 장치를 제공하 는 것이다. 본 개시는 데이터 분담 정책에 따라 일부 데이터에 대한 분석 결과를 생성하고, 상위 워커 장치로 적어도 일부 데이터에 대한 분석을 요청하여 분석 결과를 획득하고, 분석 결과를 통합하여 AI 서비스를 제공하거나, 하위 워 커 장치로 전달하는 워커 장치를 제공하는 것이다. 본 개시는 최하위 워커 장치에서 영상 스트림 또는 측정 데이터에 대한 AI 서비스를 제공하는 경우, 최하위 워 커 장치의 컴퓨팅 자원뿐만 아니라 상위 워커 장치들의 컴퓨팅 자원까지 통합적으로 사용하여, 입력 데이터에 대한 AI 서비스 태스크를 처리하는 네트워크 시스템을 제공하는 것이다."}
{"patent_id": "10-2021-0028243", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "한 실시예에 따른 적어도 하나의 프로세서에 의해 동작하는 관리 장치의 동작 방법으로서, 인공지능(Artificial Intelligence, AI) 서비스 태스크 처리를 위해 계층적으로 연결된 워커 장치들로부터, 컴퓨팅 자원 상태 및/또 는 네트워크 상태를 포함하는 상태 알림 메시지를 수신하는 단계, 각 워커 장치의 컴퓨팅 자원 상태 및/또는 네 트워크 상태에 따라, AI 데이터에 대한 AI 서비스 태스크를 처리할 적어도 하나의 워커 장치 및 데이터 분담 정 책을 결정하는 단계, 그리고 상기 AI 데이터에 대한 AI 서비스 태스크를 처리할 적어도 하나의 워커 장치로, 상 기 데이터 분담 정책을 전송하는 단계를 포함한다. 상기 AI 데이터는 상기 워커 장치들 중 최하위 워커 장치에 연결된 데이터 수집 장치로부터 입력된다. 상기 데이터 분담 정책을 전송하는 단계는 상기 최하위 워커 장치에서의 처리 데이터와, 상기 최하위 워커 장치 의 상위 워커 장치에서의 처리 데이터의 비율을 포함하는 상기 데이터 분담 정책을 상기 최하위 워커 장치로 전 송할 수 있다. 상기 동작 방법은 상기 적어도 하나의 워커 장치의 컴퓨팅 자원 상태 및/또는 네트워크 상태가 변경되면, 상기 AI 데이터에 대한 AI 서비스 태스크를 처리할 적어도 하나의 워커 장치 및 데이터 분담 정책을 재결정하고, 재 결정한 데이터 분담 정책을 해당 워커 장치로 전송하는 단계를 더 포함할 수 있다. 상기 데이터 분담 정책을 결정하는 단계는 상기 최하위 워커 장치에서 상기 AI 데이터에 대해 제공하는 AI 서비 스 지연을 최소화하는 상기 데이터 분담 정책을 결정할 수 있다. 상기 데이터 분담 정책을 결정하는 단계는 각 워커 장치의 컴퓨팅 자원 상태를 기초로 계산된 데이터 처리 시간, 워커 장치 간 네트워크 상태를 기초로 계산된 데이터 전송 시간을 고려하여, 상기 AI 서비스 지연을 최소 화하는 상기 데이터 분담 정책을 결정할 수 있다. 상기 계층적으로 연결된 워커 장치들은 차등된 컴퓨팅 자원을 포함할 수 있다. 한 실시예에 따른 적어도 하나의 프로세서에 의해 동작하는 워커 장치가 저지연 인공지능(Artificial Intelligence, AI) 서비스를 제공하는 방법으로서, 연결된 데이터 수집 장치로부터 AI 서비스 태스크의 처리 대 상인 AI 데이터를 입력받는 단계, 데이터 분담 정책에 따라 상기 AI 데이터를 내부 처리 데이터와 외부 처리 데 이터로 분배하는 단계, 상기 내부 처리 데이터에 대해 상기 AI 서비스 태스크를 수행하여, 상기 내부 처리 데이 터에 대한 분석 결과를 생성하는 단계, 상위 워커 장치로 상기 외부 처리 데이터에 분석을 요청하고, 상기 상위 워커 장치로부터 상기 외부 처리 데이터에 대한 분석 결과를 획득하는 단계, 그리고 상기 내부 처리 데이터 및 상기 외부 처리 데이터에 대한 분석 결과를 이용하여 상기 AI 데이터에 대한 AI 서비스를 제공하는 단계를 포함 한다. 상기 AI 서비스 제공 방법은 관리 장치로, 컴퓨팅 자원 상태 및/또는 네트워크 상태를 포함하는 상태 알림 메시 지를 전송하는 단계, 그리고 상기 관리 장치로부터 상기 상태 알림 메시지를 기초로 생성된 상기 데이터 분담 정책을 수신하는 단계를 더 포함할 수 있다. 상기 네트워크 상태는 상기 상위 워커 장치와 연결된 네트워크의 전송 속도 및/또는 전송 지연을 포함할 수 있 다. 상기 상태 알림 메시지는 측정된 AI 서비스 지연을 더 포함할 수 있다. 한 실시예에 따라, 코어망의 에지에 구축되고, 적어도 하나의 프로세서에 의해 동작하는 워커 장치가 저지연 인 공지능(Artificial Intelligence, AI) 서비스를 제공하는 방법으로서, 하위 워커 장치로부터, AI 서비스 태스크 의 처리 대상인 AI 데이터를 입력받는 단계, 데이터 분담 정책에 따라 상기 AI 데이터를 내부 처리 데이터와 외 부 처리 데이터로 분배하는 단계, 상기 내부 처리 데이터에 대해 상기 AI 서비스 태스크를 수행하여, 상기 내부 처리 데이터에 대한 분석 결과를 생성하는 단계, 상위 워커 장치로 상기 외부 처리 데이터에 분석을 요청하고, 상기 상위 워커 장치로부터 상기 외부 처리 데이터에 분석 결과를 획득하는 단계, 그리고 상기 내부 처리 데이 터 및 상기 외부 처리 데이터에 대한 분석 결과를 상기 하위 워커 장치로 전송하는 단계를 포함한다. 무선 통신망을 통해 상기 하위 워커 장치와 통신하고, 유선 통신망을 통해 상기 상위 워커 장치와 통신할 수 있 다. 상기 AI 데이터를 입력받는 단계는 다중 접속 에지 컴퓨팅(Multi-Access Edge Computing, MEC) 서비스 지역에 설치된 무선 접속망을 통해 상기 AI 데이터를 입력받을 수 있다. 상기 AI 서비스 제공 방법은 관리 장치로, 컴퓨팅 자원 상태 및/또는 네트워크 상태를 포함하는 상태 알림 메시 지를 전송하는 단계, 그리고 상기 관리 장치로부터 상기 상태 알림 메시지를 기초로 생성된 상기 데이터 분담 정책을 수신하는 단계를 더 포 함할 수 있다. 상기 네트워크 상태는 상기 상위 워커 장치와 연결된 네트워크의 전송 속도 및/또는 전송 지연을 포함할 수 있 다. 한 실시예에 따라 저지연 인공지능(Artificial Intelligence, AI) 서비스를 제공하는 네트워크 시스템으로서, AI 서비스 태스크 처리를 위해 계층적으로 연결된 워커 장치들을 포함하고, 상기 워커 장치들 중 최하위 워커 장치는 데이터 수집 장치에 연결되고, 상기 데이터 수집 장치에서 입력된 AI 데이터를 데이터 분담 정책에 따라 제1 내부 처리 데이터와 제1 외부 처리 데이터로 분배하고, 상기 제1 내부 처리 데이터에 대해 상기 AI 서비스 태스크를 수행하여, 상기 제1 내부 처리 데이터에 대한 분석 결과를 생성하며, 상위 워커 장치로부터 상기 제1 외부 처리 데이터에 대한 분석 결과를 획득한 후, 상기 제1 내부 처리 데이터 및 상기 제1 외부 처리 데이터에 대한 분석 결과를 이용하여 상기 AI 데이터에 대한 AI 서비스를 제공한다. 상기 워커 장치들 중 상기 상위 워커 장치는 상기 최하위 워커 장치로부터, 상기 제1 외부 처리 데이터에 대한 분석 요청을 수신하고, 상기 제1 외부 처리 데이터에 대해 상기 AI 서비스 태스크를 수행하여, 상기 제1 외부 처리 데이터에 대한 분석 결과를 생성한 후, 상기 최하위 워커 장치로 상기 제1 외부 처리 데이터에 대한 분석 결과를 응답할 수 있다. 상기 워커 장치들 중 상기 상위 워커 장치는 상기 최하위 워커 장치로부터, 상기 제1 외부 처리 데이터에 대한 분석 요청을 수신하고, 상기 제1 외부 처리 데이터를 데이터 분담 정책에 따라 제2 내부 처리 데이터와 제2 외 부 처리 데이터로 분배하고, 상기 제2 내부 처리 데이터에 대해 상기 AI 서비스 태스크를 수행하여, 상기 제2 내부 처리 데이터에 대한 분석 결과를 생성하며, 최상위 워커 장치로부터 상기 제2 외부 처리 데이터에 대한 분 석 결과를 획득한 후, 상기 제2 내부 처리 데이터 및 상기 제2 외부 처리 데이터에 대한 분석 결과를 상기 최하 위 워커 장치로 응답할 수 있다. 상기 워커 장치들 각각은 관리 장치로, 컴퓨팅 자원 상태 및/또는 네트워크 상태를 포함하는 상태 알림 메시지 를 전송할 수 있다. 상기 최하위 워커 장치는 상기 관리 장치로부터 상기 데이터 분담 정책을 수신할 수 있다. 상기 최하위 워커 장치는 다중 접속 에지 컴퓨팅(Multi-Access Edge Computing, MEC) 서비스 지역에 설치된 무 선 접속망을 통해 상기 상위 워커 장치에 접속하는 AI 서비스 제공 단말일 수 있다. 상기 상위 워커 장치는 코 어망의 에지에 구축된 MEC 장치일 수 있다."}
{"patent_id": "10-2021-0028243", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 종단에서 AI 서비스를 제공하는 최하위 워커 장치가 한정된 컴퓨팅 자원을 가지고 있더라도, 상위 워커 장치들의 컴퓨팅 자원까지 사용할 수 있어서, 고성능의 컴퓨팅 자원이 요구되는 AI 서비스를 초저지 연으로 제공할 수 있다. 본 개시에 따르면, AI 서비스에서 요구되는 서비스 품질이 달라지거나, AI 서비스에서 요구되는 컴퓨팅 자원이 가변되더라도, 이를 처리할 수 있는 워커 장치로 교체할 필요 없이, 워커 장치들의 데이터 분담 정책을 재설정하면 되므로, 고객에게 다양한 AI 서비스를 빠르게 제공할 수 있다. 본 개시에 따르면, 저가의 AI 서비스 단말 기반에서도 초저지연, 고품질의 서비스를 저렴하게 제공할 수 있어서, 고객 만족도를 높일 수 있고, 다양한 AI 서비스로의 사업 확장이 가능하다."}
{"patent_id": "10-2021-0028243", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위 해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 개시에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구 성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 네트워크를 구성하는 장치 들은 하드웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 본 개시에서, 워커 장치, 관리 장치, 또는 각종 모듈은 적어도 하나의 프로세서에 의해 동작하고, 하나 이상의 프로세서, 프로세서에 의하여 실행되는 컴퓨터 프로그램을 로드하는 메모리, 컴퓨터 프로그램 및 각종 데이터를 저장하는 저장 장치, 통신 인터페이스를 포함할 수 있다. 컴퓨터 프로그램은 메모리에 로드될 때 프로세서로 하 여금 본 개시의 다양한 실시예에 따른 방법/동작을 수행하도록 하는 명령어들(instruction)을 포함할 수 있다. 즉, 프로세서는 명령어들을 실행함으로써, 본 개시의 다양한 실시예에 따른 방법/동작들을 수행할 수 있다. 명 령어는 기능을 기준으로 묶인 일련의 컴퓨터 판독가능 명령어들로서 컴퓨터 프로그램의 구성 요소이자 프로세서 에 의해 실행되는 것을 가리킨다. 본 개시에서, 인공지능(Artificial Intelligence, AI) 서비스는 학습된 AI 모델을 통해 입력 데이터에 대한 결 과를 제공하는 서비스로서, 입력 데이터 및 결과에 따라 다양할 수 있다. 실시간 영상 스트림에 대한 객체 인식 결과를 제공하는 AI 서비스로서, 얼굴 인식 서비스, 얼굴 기반 출입 인증 서비스, 보안감시 서비스, 영상에 부 가 콘텐츠를 제공하는 AR/VR 콘텐츠 서비스를 예로 들 수 있다. 센서 데이터에 대한 분석 결과를 제공하는 서비 스로서, 체온 측정 서비스, 이상 감지 서비스 등을 예로 들 수 있다. 본 개시에서, AI 모델은 AI 서비스를 위한 인식, 분류, 예측 등의 태스크를 학습한 모델로서, 컴퓨팅 장치에서 실행되는 소프트웨어/프로그램으로 구현될 수 있는데, GPU 등의 AI 프로세서에 의해 구동될 수 있다. 도 1은 한 실시예에 따른 네트워크 시스템의 구성도이다. 도 1을 참고하면, AI 서비스를 제공하는 네트워크 시스템은 AI 서비스 태스크(task)를 처리하는 워커 (Worker) 장치들(100, 200, 300), 그리고 워커 장치들의 컴퓨팅 자원 그리고 워커 장치간 네트워크 상태를 기초 로 워커 장치들의 데이터 분담 정책을 결정하는 관리 장치를 포함한다. AI 서비스 태스크는 학습된 AI 모 델을 통해 해결하고자 하는 과제 또는 처리하고자 하는 작업을 지칭하는데, 예를 들어, 입력 데이터에 대한 인 식, 분류, 예측 등을 의미하고, AI 서비스에 따라 AI 서비스 태스크는 달라질 수 있다. 워커 장치들(100, 200, 300)은 코어망의 에지에 구축된 에지 컴퓨팅 장치일 수 있고, 다중 접속 에지 컴퓨팅 (Multi-Access Edge Computing, MEC) 기술 기반의 MEC 장치일 수 있다. 설명에서, 3종류의 워커 장치들을 예로 들어 설명하는데, 워커 장치들의 종류는 변경될 수 있다. 설명에서, 각 워커 장치를 구분하기 위해, 워커 장치 A, 워커 장치B, 워커 장치C라고 부른다.데이터 수집 장치에서 수집된 데이터는 워커 장치들에서 분석되는데, 데이터 수집 장치는 워커 장치 A에 연결될 수 있다. 데이터 수집 장치는 AI 서비스의 입력에 따라 카메라, 센서, 터치디스플레이 등 으로 다양할 수 있다. 설명에서는 데이터 수집 장치로서 카메라를 주로 예를 들어 설명하는데, 이 경우 워 커 장치들에서 분석되는 데이터는 영상 스트림이다. 워커 장치A는 데이터 수집 장치로부터 데이터를 입력받고, 입력 데이터에 대한 분석 결과에 따라 AI 서비스를 제공하는 단말 장치라고 가정한다. 이와 달리, 워커 장치A는 AI 서비스를 제공하는 별도의 단말 장치에 연결되고, 별도의 단말 장치로 분석 결과를 제공할 수 있는데, 설명에서는 최하위 워커 장치를 단 말 장치라고 가정한다. 워커 장치들(100, 200, 300)은 유무선 통신망을 통해 연결될 수 있다. 워커 장치A와 워커 장치B는 무 선 통신망으로 연결될 수 있다. 이 경우, 워커 장치A는 MEC 서비스 지역(MEC zone)에 설치된 무선 접속망 (Radio Access Network, RAN)을 통해 워커 장치B에 연결되는 5G 디바이스이거나, 5G 디바이스에 연결 될 수 있다. 워커 장치B는 워커 장치A로부터 전송된 데이터를 처리하는 MEC 장치일 수 있고, 데이터 전송을 위한 에지 사용자 플레인 기능(edge User Plane Function, eUPF)을 포함하는 5G 기능들을 포함할 수 있 다. 워커 장치B는 데이터망에 위치한 서버인 워커 장치C로 데이터를 전송하고, 워커 장치C로부 터 데이터에 대한 분석 결과를 수신할 수 있다. 워커 장치들(100, 200, 300)은 계층적으로 연결되고, 계층에 따라 성능이 차등된 AI 컴퓨팅 자원을 탑재한다고 가정한다. 상위 계층일수록 고성능의 AI 컴퓨팅 자원을 보유한다고 가정한다. 여기서, AI 컴퓨팅 자원은 AI 서 비스 태스크를 처리하는 AI 프로세서 및 메모리를 포함할 수 있다. 이때, 워커 장치들은, 상위 워커 장치가 복 수의 하위 워커 장치들과 연결되는 트리 구조로 연결될 수 있다. 워커 장치들(100, 200, 300)은 상위 계층일수록 AI 서비스를 제공받는 사용자와의 거리가 멀어진다고 가정한다. 예를 들면, 기업의 출입 인증 서비스를 제공하는 경우, 최하위 워커 장치A는 출입구에서 사용자를 촬영하 는 카메라를 탑재하거나, 카메라 근처에 위치하는 AI 서비스 제공 단말일 수 있다. 워커 장치B는 MEC 서비 스 지역(예를 들면, 기업 건물 내)에 구축된 MEC 장치일 수 있고, 워커 장치C는 데이터센터에 구축된 대용 량 서버일 수 있다. 관리 장치는 워커 장치들(100, 200, 300)과 연결되고, 워커 장치들(100, 200, 300)로부터 컴퓨팅 자원 상 태를 보고받는다. 또한, 관리 장치는 워커 장치들(100, 200, 300)로부터 상위 워커 장치와 연결된 네트워 크 상태(지연 등)를 보고받을 수 있다. 이때, 최상위 워커 장치는 네트워크 상태를 보고하지 않아도 된다. 관리 장치는 AI 서비스 단위로 정책을 관리하고, 관리자로부터 AI 서비스 등록 정보를 입력받을 수 있다. AI 서비스 등록 정보는, 서비스 이름, 허용 가능한 최대 서비스 지연(서비스 보장 지연), 데이터 분배 방법 등 을 포함할 수 있다. 관리 장치 또는 별도의 배포 장치(미도시)가, 등록된 AI 서비스에 해당하는 AI 처리기 및 에이전트 모듈을 계층적 워커 장치들로 배포하고 설치할 수 있다. 관리 장치는 워커 장치들(100, 200, 300)의 컴퓨팅 자원 상태 및 네트워크 상태를 기초로, AI 서비스 태스 크를 처리하기 위한 워커 장치들의 데이터 분담 정책을 결정할 수 있다. 관리 장치는 각 워커 장치의 컴퓨 팅 자원 상태, 워커 장치 간 네트워크 상태, AI 서비스 태스크 처리를 위해 요구되는 컴퓨팅 자원을 이용하여, 최고의 AI 서비스 품질을 만족시키는 데이터 분담 정책을 결정할 수 있다. 예를 들면, 관리 장치는 최소 AI 서비스 지연을 만족하는 워커 장치들에서의 데이터 분담 정책을 결정하는 최적화 함수를 계산할 수 있다. 이 때, AI 서비스 지연으로서, 컴퓨팅 자원 상태를 기초로 계산된 데이터 처리 시간, 그리고 네트워크 상태를 기초 로 계산된 워커 장치 간 데이터 전송 시간이 고려될 수 있다. 이때, 데이터 전송 시간에 워커 장치 간에 데이터 를 전송하기 위한 인코딩 및 디코딩 시간이 포함될 수 있다. 관리 장치는 워커 장치들(100, 200, 300)의 컴퓨팅 자원 상태 및 네트워크 상태가 변경되면, 변경 정보를 기초로 데이터 분담 정책을 재결정하고, 데이터 분담 정책을 업데이트할 수 있다. 관리 장치는 AI 서비스 태스크를 처리하는 워커 장치로 데이터 분담 정책을 전송할 수 있다. 관리 장치 는 데이터 분담 처리에 참여하는 모든 워커 장치들로 전송할 수 있고, 또는 데이터를 상위 워커 장치로 분 배하는 워커 장치에게만 전송할 수 있다. 데이터 분담 정책은 데이터 분담 비율을 포함할 수 있다. 한편, 데이터 분담 정책은 분담 태스크를 포함할 수 있는데, 설명에서는 주로 데이터 분담 비율이라고 설명할 수 있다. 데이터 분담 정책을 수신한 각 워커 장치는데이터 분담 정책에 따라 데이터 분석, 데이터 전송, 분석 결과 전송 등의 절차를 수행한다. 데이터 분담 비율 을 수신한 워커 장치는 비율에 따라 분담할 데이터를 처리하고, 나머지 데이터에 대해서는 상위 워커 장치로 분 석 요청한 후, 자신이 분담한 데이터에 대한 분석 결과, 그리고 상위 워커 장치로부터 전달된 분석 결과를 하위 워커 장치로 전달할 수 있다. 분담 태스크를 수신한 워커 장치는 데이터에 대해 처리할 태스크(예를 들면, 자동 차 인식 태스크, 사람 인식 태스크)를 분담하고, 상위 워커 장치로 데이터에 대한 분석을 요청한다. 최하위 워커 장치A는 데이터 수집 장치로부터 입력된 데이터에 대한 분석 결과를 기초로 AI 서비스 (예를 들면, 얼굴 인식 서비스 등)를 제공하는데, 상위 워커 장치로부터 전달된 분석 결과와 자신이 처리한 분 석 결과가 있으면, 분석 결과를 통합해서 AI 서비스를 제공할 수 있다. 이처럼, 최하위 워커 장치A에서 영상 스트림 또는 측정 데이터에 대한 AI 서비스를 제공하는 경우, 최하위 워커 장치A의 컴퓨팅 자원뿐만 아니라 상위 워커 장치들(200, 300)의 컴퓨팅 자원까지 통합적으로 고려하 여 AI 서비스가 제공된다. 따라서, 최하위 워커 장치A가 한정된 컴퓨팅 자원을 가지고 있더라도, 상위 워 커 장치들의 컴퓨팅 자원까지 사용할 수 있어서, 사용자에게 고성능의 컴퓨팅 자원이 요구되는 AI 서비스를 초 저지연으로 제공할 수 있다. 도 2는 한 실시예에 워커 장치들의 구성도이고, 도 3과 도 4는 한 실시예에 따른 워크 장지들 사이의 데이터 분 담을 도식적으로 설명하는 도면이다. 도 2를 참고하면, 워커 장치A는 데이터 수집 장치로부터 데이터를 입력받고, 입력 데이터에 대한 분 석 결과에 따라 AI 서비스를 제공하는 단말이다. 워커 장치A는 관리 장치로부터 수신한 데이터 분담 정책에 따라 AI 서비스 태스크 처리를 수행하고, 상위의 워커 장치B로 나머지 데이터를 전송할 수 있다. 그리고 워커 장치B 및/또는 워커 장치C는 데이터 분담 정책에 따라 자신이 분담한 데이터에 대한 AI 서비스 태스크의 분석 결과를 생성하고, 이를 하위 워커 장치로 전달할 수 있다. 워커 장치들 각각은 적어도 하나의 프로세서 및 메모리를 포함하는 하드웨어 장치를 포함하고, 하드웨어 장치에 의해 제공되는 컴퓨팅 자원을 이용하여 구동되는 복수의 모듈들을 포함할 수 있다. 하드웨어 장치에 의해 제공 되는 컴퓨팅 자원은 워커 장치의 계층에 따라 차등될 수 있다. 각 모듈은 응용 레이어에 설치된 응용 (applications)으로 구현되고, 프로세서에 의해 실행되는 명령어들(instructions)을 포함하는 컴퓨터 프로그램 을 포함할 수 있다. 각 모듈은 물리적인 컴퓨팅 자원을 가상화한 가상 컴퓨팅 자원에 의해 구동될 수 있다. 여 기서 프로세서는 워커 장치를 전반적으로 제어하는 메인 프로세서, 그리고 AI 서비스 태스크를 처리하는 AI 프 로세서를 포함할 수 있다. 워커 장치A는 하드웨어 장치, 그리고 하드웨어 장치에 의해 구동되는 복수의 모듈들(120, 130, 140, 150, 160)을 포함한다. 하드웨어 장치는 적어도 하나의 프로세서, 메모리, 저장 장치, 네트워크 인터 페이스를 포함할 수 있다. 워커 장치A는 AI 서비스를 제공받는 사용자에 최근접한 단말 장치일 수 있다. 워커 장치A는 데이터 수집 장치로부터 AI 서비스 태스크의 처리 대상인 AI 데이터를 입력받는 AI 데 이터 인터페이스(AI data interface) 모듈, 데이터 분담 정책에 따라 AI 데이터를 나누는 부하분담기(Load Balancer), 부하분담기에서 상위 워커 장치로 분담된 데이터를 상위 워커 장치로 전송하는 스트리머 (Streamer), 부하분담기에서 내부 처리로 분담된 데이터를 AI 모델을 이용하여 분석한 분석 결과를 생성하고, 분석 결과를 통합하여 AI 서비스를 출력하는 AI 처리기, 관리 장치와 통신하는 에이전트 모듈을 포함할 수 있다. AI 데이터 인터페이스 모듈은 데이터 수집 장치로부터 AI 데이터를 입력받고, 이를 부하분담기 로 전달한다. 데이터 수집 장치가 카메라인 경우, AI 데이터는 영상 스트림일 수 있다. 데이터 수집 장치 가 센서인 경우, AI 데이터는 측정 데이터일 수 있다. 부하분담기는 에이전트 모듈로부터 전달된 데이터 분담 정책에 따라, 입력된 AI 데이터를 내부 처리 데이터와 상위 워커 장치에서의 처리 데이터로 나눈다. 부하분담기는 내부 처리 데이터(Local_DATA)를 AI 처리기로 전달하고, 상위 워커 장치에서의 외부 처리 데이터(Remote_DATA)를 스트리머로 전달한다. 스트리머는 부하분담기로부터 수신한 외부 처리 데이터를 지정된 상위 워커 장치로 전달하고, 상위 워커 장치로부터 전달한 데이터에 대한 분석 결과를 수신한다. 스트리머는 수신한 분석 결과를 AI 처리기 로 전달할 수 있다. 스트리머는 무선 접속망을 통해 워커 장치B로 데이터를 전송할 수 있다. 스트리머는 워커 장치B와 연결된 네트워크 상태를 측정하고, 네트워크 상태를 에이전트 모듈로 전달할 수 있다. AI 처리기는 부하분담기로부터 입력된 데이터(Local_DATA)에 대해서 AI 서비스 태스크를 처리하는 모 듈로서, AI 서비스 태스크를 학습한 AI 모델을 통해 입력 데이터에 대한 분석 결과를 획득할 수 있다. 이때, AI 모델은 하드웨어 장치의 AI 프로세서에 의해 구동될 수 있고, AI 처리기는 AI 서비스에 따라 다양한 모델을 탑재할 수 있다. 한편, AI 처리기는 스트리머로부터 상위 워커 장치로 전송한 데이터 (Remote_DATA)에 대해 처리된 분석 결과를 획득할 수 있다. 그리고, AI 처리기는 에이전트 모듈로부 터 수신한 태스크 분담 정책에 따라, 분석 결과를 통합하여 AI 데이터에 대한 AI 서비스를 출력할 수 있다. 예 를 들면, AI 서비스는 얼굴을 촬영한 영상 스트림에서 인식한 얼굴을 기초로 제공되는 각종 서비스일 수 있고, 얼굴 기반 출입 인증 서비스, 얼굴의 표정에 맞는 캐릭터를 제공하는 AR/VR 서비스 등일 수 있다. 이 경우, AI 처리기는 영상 스트림에 포함된 이미지들(프레임들)에서 특징을 추출하는 태스크를 수행할 수 있고, 자신 이 분담한 이미지들에서 추출한 특징과 상위 워커 장치에서 분담한 이미지들에서 추출한 특징을 통합하여, 입력 데이터에 대한 AI 서비스를 제공할 수 있다. 에이전트 모듈은 관리 장치로부터 수신한 데이터 분담 정책을 하위 모듈들인 부하분담기, 스트 리머, 그리고 AI 처리기에 적용할 수 있다. 에이전트 모듈은 하드웨어 장치의 컴퓨팅 자원 상태를 관리 장치로 전송할 수 있다. 에이전트 모듈은 스트리머로부터, 상위 워커 장치와 연결 된 네트워크 상태를 획득하고, 네트워크 상태를 관리 장치로 전송할 수 있다. 워커 장치B는 하드웨어 장치, 그리고 하드웨어 장치에 의해 구동되는 복수의 모듈들(220, 230, 240, 250, 260)을 포함한다. 하드웨어 장치는 적어도 하나의 프로세서, 메모리, 저장 장치, 네트워크 인터 페이스를 포함할 수 있다. 워커 장치B는 무선 접속망을 통해 워커 장치A로부터 AI 서비스 태스크의 처리 대상인 AI 데이터 를 수신하는 5G 코어 모듈, 데이터 분담 정책에 따라 AI 데이터를 나누는 부하분담기, 부하분담기 에서 상위 워커 장치로 분담된 데이터를 상위 워커 장치로 전송하는 포워딩 모듈, 부하분담기에 서 내부 처리로 분담된 데이터를 AI 모델을 이용하여 분석한 분석 결과를 생성하고, 상위 워커 모듈에서 분석 결과를 통합하여 5G 코어 모듈을 통해 하위 워커 장치로 전송하는 AI 처리기, 관리 장치와 통신 하는 에이전트 모듈을 포함할 수 있다. 워커 장치B는 코어망의 에지에 구축된 MEC 장치일 수 있다. 5G 코어 모듈은 복수의 네트워크 기능들(Network Functions)을 통해, 워커 장치A 및 워커 장치 C와 통신하는 세션을 제공한다. 5G 코어 모듈은 예를 들면, 기업 전용 에지 서비스를 제공하기 위한 네트워크 기능들을 포함할 수 있다. 5G 코어 모듈은 워커 장치A로부터 AI 데이터를 입력받고, 이를 부하분담기로 전달한다. 5G 코어 모듈은 포워딩 모듈로부터 출력된 데이터를 상위 워커 장치로 전달하고, 상위 워커 장치로부터 수신한 분석 결과를 포워딩 모듈로 전달하고, AI 처리기에서 출력된 분석 결과를 하위 워커 장치로 전달할 수 있다. 5G 코어 모듈은 워커 장치A가 무선 접속망을 통해 접속할 수 있고, 데이터망으로 트래픽을 송수 신하는 5G 코어망 기능들을 포함할 수 있다. 5G 코어 모듈은 복수의 네트워크 기능들로 구성될 수 있는데, 워커 장치A와 세션을 생성하고, 데이터망에 연결되어 트래픽을 처리하는 에지 사용자 플레인 기능(eUPF)을 포함할 수 있다. 부하분담기는 에이전트 모듈로부터 전달된 데이터 분담 정책에 따라, 입력된 AI 데이터(Remote_DAT A)를 내부 처리 데이터(Local_DATA2)와 상위 워커 장치에서의 외부 처리 데이터(Remote_DATA2)로 나눈다. 부하 분담기는 입력된 AI 데이터(Remote_DATA)를 디코딩한 후 분할할 수 있다. 부하분담기는 내부 처리 데 이터(Local_DATA2)를 AI 처리기로 전달하고, 상위 워커 장치에서의 외부 처리 데이터(Remote_DATA2)를 포 워딩 모듈로 전달한다. 포워딩 모듈은 부하분담기로부터 수신한 외부 처리 데이터를 지정된 상위 워커 장치로 전달하고, 상 위 워커 장치로부터 전달한 데이터에 대한 분석 결과를 수신한다. 포워딩 모듈는 수신한 분석 결과를 AI 처리기로 전달할 수 있다. 포워딩 모듈은 5G 코어 모듈을 통해 워커 장치C로 데이터를 전송할 수 있다. 포워딩 모듈 은 워커 장치C와 연결된 네트워크 상태를 측정하고, 네트워크 상태를 에이전트 모듈로 전달할 수 있 다. AI 처리기는 부하분담기로부터 입력된 데이터(Local_DATA2)에 대한 AI 서비스 태스크를 처리하는 모 듈로서, AI 서비스 태스크를 학습한 AI 모델을 통해 입력 데이터에 대한 분석 결과를 획득할 수 있다. 이때, AI 모델은 하드웨어 장치의 AI 프로세서에 의해 구동될 수 있고, AI 처리기는 AI 서비스에 따라 다양한 모델을 탑재할 수 있다. 한편, AI 처리기는 포워딩 모듈로부터 상위 워커 장치로 전송한 데이터 (Remote_DATA2)에 대해 처리된 분석 결과를 획득할 수 있다. 그리고, AI 처리기는 에이전트 모듈로부 터 수신한 태스크 분담 정책에 따라, 분석 결과를 통합하여 분석을 요청한 하위 워커 장치로 전송할 수 있다. 에이전트 모듈은 관리 장치로부터 수신한 데이터 분담 정책을 하위 모듈들인 부하분담기, 포워 딩 모듈, 그리고 AI 처리기에 적용할 수 있다. 에이전트 모듈은 하드웨어 장치의 컴퓨팅 자원 상태를 관리 장치로 전송할 수 있다. 에이전트 모듈은 포워딩 모듈로부터, 상위 워커 장치 와 연결된 네트워크 상태를 획득하고, 네트워크 상태를 관리 장치로 전송할 수 있다. 워커 장치C는 하드웨어 장치, 그리고 하드웨어 장치에 의해 구동되는 복수의 모듈들(320, 330) 을 포함한다. 하드웨어 장치는 적어도 하나의 프로세서, 메모리, 저장 장치, 네트워크 인터페이스를 포함 할 수 있다. 워커 장치C는 고성능의 컴퓨팅 자원을 통해 데이터 처리하는 대용량 서버로 구현될 수 있고, 데이터센터에 위치할 수 있다. 워커 장치C는 AI 처리기, 그리고 관리 장치와 통신하는 에이전트 모듈을 포함할 수 있다. AI 처리기는 워커 장치B로부터 AI 서비스 태스크의 처리 대상인 AI 데이터(Remote_DATA2)를 수신하고, AI 서비스 태스크를 학습한 AI 모델을 이용하여 분석한 분석 결과를 생성하고, 하위 워커 장치로 전 송한다. 이때, AI 모델은 하드웨어 장치의 AI 프로세서에 의해 구동될 수 있고, AI 처리기는 AI 서비 스에 따라 다양한 모델을 탑재할 수 있다. 에이전트 모듈은 관리 장치로부터 수신한 데이터 분담 정책을 AI 처리기에 적용할 수 있다. 에 이전트 모듈은 하드웨어 장치의 컴퓨팅 자원 상태를 관리 장치로 전송할 수 있다. 한편, 하드웨어 장치에 의해 제공되는 컴퓨팅 자원은 워커 장치의 계층에 따라 다르고, 워커 장치들과 사용자와 의 거리도 달라진다. 이러한 차이에 의해, 상대적으로 저렴한 AI 프로세서를 탑재한 워커 장치A는 상대적 으로 낮은 성능을 제공하지만 초저지연 서비스를 제공할 수 있다. 에지(edge)에 MEC 서버로 구축되는 워커 장치 B는 고사양 AI 프로세서를 탑재해서 저지연을 보장할 수 있지만, 컴퓨팅 자원이 제한적일 수 있다. 데이터 센터의 대용량 서버로 구축되는 워커 장치C는 대용량 컴퓨팅 자원으로 데이터 처리할 수 있으나, 거리가 멀어서 저지연 서비스 보장이 어렵다. 따라서, 관리 장치는 다양한 위치에서 다양한 컴퓨팅 자원을 가지는 워커 장치들의 컴퓨팅 자원 상태 및 네트워크 상태를 관리하고, 워커 장치들의 컴퓨팅 자원 상태 및 네트워크 상태를 기초로, AI 서비스 지연을 최 소화하는 데이터 분담 정책을 결정한다. AI 서비스 지연은 컴퓨팅 자원 상태를 기초로 계산된 데이터 처리 시간, 그리고 네트워크 상태를 기초로 계산된 워커 장치 간 데이터 전송 시간이 고려될 수 있다. 이때, 데이터 전송 시간에 워커 장치 간에 데이터를 전송하기 위한 인코딩 및 디코딩 시간이 포함될 수 있다. 예를 들어, 워커 장치들이 영상 스트림의 얼굴 인식 태스크를 수행하는 AI 처리기를 포함하는 경우, 워커 장치 A는 자신의 컴퓨팅 자원으로 초저지연으로 서비스 가능한 데이터(예를 들면, 약 10fps 영상에서 두 명의 얼굴을 분석)에 대해서는, 자신의 AI 처리기에서 처리된 분석 결과를 이용하여 AI 서비스를 제공할 수 있 다. 만약, 워커 장치A가 자신의 컴퓨팅 자원으로 초저지연으로 서비스가 어려운 데이터(예를 들면, 15fps 이상에서 5명 이상의 얼굴을 포함하는 영상)를 입력받는 경우, 자신의 컴퓨팅 자원으로 데이터 분담 정책에 따 른 데이터 처리만 분담하고, 나머지 데이터에 대해서는 상위 워커 장치로부터 분석 결과를 수신할 수 있다. 이 를 위해, 관리 장치는 각 워커 장치에서의 데이터 분담 정책에 따라 AI 서비스 지연을 계산하고, AI 서비 스 지연이 최소가 되는 데이터 분담 정책을 결정할 수 있다. 도 3을 참고하면, 워커 장치A의 부하분담기는 AI 데이터를 내부 처리 데이터(Local_DATA) 및 외부 처 리 데이터(Remote_DATA)로 분배하고, 내부 처리 데이터(Local_DATA)를 AI 처리기로 전달하고, 외부 처리 데이터(Remote_DATA)를 스트리머로 전달한다. 스트리머는 무선 접속망을 통해 워커 장치B로 외부 처리 데이터(Remote_DATA)를 전송한다. . 워커 장치B의 부하분담기는 입력된 AI 데이터(Remote_DATA)를 내부 처리 데이터(Local_DATA2)와 외 부 처리 데이터(Remote_DATA2)로 나누고, 내부 처리 데이터(Local_DATA2)를 AI 처리기로 전달하고 외부 처리 데이터(Remote_DATA2)를 포워딩 모듈로 전달한다.포워딩 모듈은 외부 처리 데이터(Remote_DATA2)를 워커 장치C로 전달한다. 워커 장치C의 AI 처리기는 AI 데이터(Remote_DATA2)를 수신하고, AI 서비스 태스크를 학습한 AI 모 델을 이용하여 분석한 분석 결과를 생성한다. 도 4를 참고하면, 워커 장치C의 AI 처리기는 AI 데이터(Remote_DATA2)에 대한 분석 결과(Result2)를 워커 장치B로 전송한다. 워커 장치B의 포워딩 모듈은 외부 처리 데이터(Remote_DATA2)의 응답으로 수신한 분석 결과(Result 2)를 AI 처리기로 전달한다. AI 처리기은 내부 처리 데이터(Local_DATA2)에 대한 분석 결과와, 외부로부터 수신한 분석 결과(Result2) 를 통합한 분석 결과(Result)를 워커 장치A로 전송한다. 워커 장치A의 스트리머는 외부 처리 데이터(Remote_DATA)의 응답으로 수신한 분석 결과(Result)를 AI 처리기로 전달한다. AI 처리기는 내부 처리 데이터(Local_DATA)에 대한 분석 결과와, 외부로부터 수신한 분석 결과(Result)를 통합한 분석 결과를 이용하여 AI 데이터에 대한 AI 서비스를 출력할 수 있다. 도 5와 도 6 각각은 한 실시예에 따른 저지연 인공지능 서비스 제공 방법의 흐름도이다. 도 5를 참고하면, 먼저, 관리 장치는 워커 장치들을 관리하기 위해, 상위의 워커 장치B로부터 컴퓨팅 자원 상태 및 네트워크 상태를 포함하는 상태 알림 메시지(Status Notification-B)를 수신하고(S110), 또한 워 커 장치B의 상위 워커 장치C로부터 컴퓨팅 자원 상태를 포함하는 상태 알림 메시지(Status Notification-B)를 수신한다(S112). 이후에, 워커 장치A가 워커 장치B의 하위로 연결되고, 데이터 수집 장치로부터 AI 서비스 태스 크의 처리 대상인 AI 데이터를 수신한다(S120). 예를 들면, AI 데이터는 영상 스트림이고, AI 서비스 태스크는 영상에서 얼굴 인식이며, 분석 결과는 영상에서 추출된 얼굴 특징값일 수 있다. 워커 장치A는 전체 AI 데이터에 대한 처리를 부담하고, 전체 AI 데이터에 대한 분석 결과를 획득한다 (S122). 워커 장치A는 입력 데이터에 대한 AI 서비스 태스크를 학습한 AI 모델을 통해, 입력 데이터에 대 한 AI 서비스 태스크의 결과를 획득할 수 있다. 초기에 설치된 워커 장치A는 데이터 분담 정책을 아직 할 당받지 못하거나, AI 데이터 전체 처리가 디폴트로 설정된 데이터 분담 정책을 가지고 있을 수 있다. 워커 장치A는 분석 결과에 해당하는 AI 서비스를 제공한다(S124). 예를 들면, AI 서비스는 얼굴 특징값에 해당하는 출입 허가 서비스, AI 서비스에 해당하는 AR/VR 캐릭터 제공 서비스 등일 수 있다. 한편, 워커 장치A는 제공한 AI 서비스 지연을 측정한다(S130). 워커 장치A는 상위의 워커 장치B와 연결되는 네트워크 상태를 측정한다(S132). 워커 장치A와 워 커 장치B가 무선 통신망으로 연결될 수 있고, 네트워크 상태는 전송 지연 또는 전송 속도일 수 있다. 워커 장치A는 관리 장치로, AI 서비스 지연 정보, 컴퓨팅 자원 상태 및 네트워크 상태를 포함하는 상 태 알림 메시지(Status Notification-A)를 전송한다(S134). 컴퓨팅 자원 상태는 예를 들면, 메인 프로세서 (CPU) 및 AI 프로세서, 메모리의 사용량 등을 포함할 수 있다. 네트워크 상태는 전송 지연 또는 전송 속도일 수 있다. 관리 장치는 워커 장치A로부터 수신한 상태 알림 메시지(Status Notification-A)를 기초로, 워커 장 치A를 위한 데이터 분담 정책을 결정한다(S140). 관리 장치는 워커 장치A의 상위 워커 장치들로 부터 수신한 상태 알림 메시지를 고려하여, 데이터 분담 정책을 결정할 수 있다. 예를 들면, 관리 장치는 워커 장치A에서의 내부 처리 데이터(Local_DATA) 비율을 80%이하, 상위 워커 장치에서의 외부 처리 데이터 (Remote_DATA) 비율을 20% 이상으로 결정할 수 있다. 관리 장치는 데이터 분담 비율을 포함하는 데이터 분담 정책(Policy-A)을 워커 장치A로 전달한다 (S142). 데이터 분담 정책(Policy-A)은 예를 들면, “local<8, remote>2”를 포함할 수 있다. 한편, 관리 장치는 워커 장치B에서의 데이터 분담 비율을 포함하는 데이터 분담 정책(Policy-B)을 워 커 장치B로 전달할 수 있다(S144). 데이터 분담 정책(Policy-B)는 예를 들면, “local=all” 포함할 수있다. 데이터 분담 정책을 수신한 이후에, 워커 장치A가 데이터 수집 장치로부터 AI 데이터를 수신한다 (S150). 워커 장치A는 데이터 분담 정책에 따라 AI 데이터를 내부 처리 데이터와 상위 워커 장치에서의 외부 처리 데이터로 분배한다(S160). 워커 장치A는 내부 처리 데이터에 대한 AI 서비스 태스크를 수행하여, 내부 처리 데이터에 대한 분석 결과 를 생성한다(S162). 또한, 워커 장치A는 상위의 워커 장치B로 외부 처리 데이터에 대한 분석을 요청한다(S164). 이때, 워 커 장치A는 인코딩한 외부 처리 데이터를 워커 장치B로 전송할 수 있다. 워커 장치B는 데이터 분담 정책에 따라, 하위의 워커 장치A로부터 수신한 데이터에 대한 대한 분석 결과를 생성한다(S170). 워커 장치B 역시, 입력 데이터에 대한 AI 서비스 태스크를 학습한 AI 모델을 통해, 입력 데이터에 대한 AI 서비스 태스크의 분석 결과를 획득할 수 있다. 워커 장치B는 워커 장치A로, 분석 요청된 데이터에 대한 분석 결과를 응답한다(S172). 워커 장치A는 내부 처리 데이터에 대한 분석 결과와, 외부 처리 데이터에 대한 분석 결과를 통합하여, 분 석 결과에 해당하는 AI 서비스를 제공한다(S180). 도 6을 참고하면, 워커 장치B는 워커 장치C와 연결되는 네트워크 상태를 측정한다(S210). 워커 장치 B와 워커 장치C는 유선 통신망으로 연결될 수 있고, 네트워크 상태는 전송 지연 또는 전송 속도일 수 있다. 워커 장치B는 관리 장치로, 컴퓨팅 자원 상태 및 네트워크 상태를 포함하는 상태 알림 메시지(Status Notification-B)를 전송한다(S212). 한편, 데이터 분담 정책을 수신한 이후에, 워커 장치A는 컴퓨팅 자원 상태 및/또는 네트워크 상태가 변경 되면, 변경 정보를 포함하는 상태 알림 메시지(Status Notification-A)를 전송한다(S220). 예를 들면, 워커 장 치A는 네트워크 상태가 열화되거나, 인코딩 부하로 인한 외부 처리 데이터 스트리밍이 지연되거나, 내부 AI 컴퓨팅 자원의 가용 용량이 줄어든 경우, 변경 정보를 포함하는 상태 알림 메시지를 전송함으로써, 데이터 분담 정책의 갱신을 요청할 수 있다. 그러면, 관리 장치는 워커 장치들의 상태 알림 메시지들을 고려하여, 워커 장치들에서의 데이터 분담 정책 을 조정한다(S230). 예를 들면, 관리 장치는 워커 장치A에서의 내부 처리 데이터(Local_DATA) 비율을 50%이하, 상위 워커 장치에서의 외부 처리 데이터(Remote_DATA) 비율을 50% 이상으로 결정할 수 있다. 그리고, 관리 장치는 워커 장치B에서의 내부 처리 데이터(Local_DATA2) 비율을 90%이하, 상위 워커 장치에서 의 외부 처리 데이터(Remote_DATA2) 비율을 10% 이상으로 결정할 수 있다. 관리 장치는 워커 장치A로 데이터 분담 비율을 포함하는 데이터 분담 정책(Policy-A)을 전달한다 (S232). 데이터 분담 정책(Policy-A)은 예를 들면, “local<5, remote>5”를 포함할 수 있다. 관리 장치는 워커 장치B로, 데이터 분담 비율을 포함하는 데이터 분담 정책(Policy-B)을 전달한다 (S234). 데이터 분담 정책(Policy-B)는 “local<9, remote>1”를 포함할 수 있다. 추가적으로, 관리 장치는 워커 장치C로, 데이터 분담 비율을 포함하는 데이터 분담 정책(Policy-C)을 전달할 수 있다(S236). 데이터 분담 정책(Policy-C)는 “local=all” 포함할 수 있다. 갱신된 데이터 분담 정책을 수신한 이후에, 워커 장치A가 데이터 수집 장치로부터 AI 데이터를 수신 한다(S240). 워커 장치A는 데이터 분담 정책에 따라 AI 데이터를 내부 처리 데이터와 외부 처리 데이터로 분배한다 (S250). 워커 장치A는 내부 처리 데이터에 대한 분석 결과를 생성한다(S252). 또한, 워커 장치A는 상위의 워커 장치B로 외부 처리 데이터에 대한 분석을 요청한다(S254). 이때, 워 커 장치A는 인코딩한 외부 처리 데이터를 워커 장치B로 전송할 수 있다.워커 장치B는 데이터 분담 정책에 따라, 하위의 워커 장치A로부터 수신한 AI 데이터를 내부 처리 데 이터와 외부 처리 데이터로 분배한다(S260). 워커 장치B는 내부 처리 데이터에 대한 분석 결과를 생성한다(S262). 또한, 워커 장치B는 상위의 워커 장치C로 외부 처리 데이터에 대한 분석을 요청한다(S264). 워커 장치C는 분석 요청된 데이터에 대한 분석 결과를 생성한다(S270). 워커 장치C는 워커 장치B로, 분석 요청된 데이터에 대한 분석 결과를 응답한다(S272). 워커 장치B는 내부 처리 데이터에 대한 분석 결과와 외부 처리 데이터에 대한 분석 결과를 통합하고, 워커 장치A로 통합한 분석 결과를 응답한다(S280). 워커 장치A는 내부 처리 데이터에 대한 분석 결과와, 외부 처리 데이터에 대한 분석 결과를 통합하여, 분 석 결과에 해당하는 AI 서비스를 제공한다(S290). 이외에도, 관리 장치는 워커 장치B로부터 컴퓨팅 자원 상태 및/또는 네트워크 상태가 변경된 정보를 포함하는 상태 알림 메시지를 수신하거나, 워커 장치C로부터 컴퓨팅 자원 상태가 변경된 정보를 포함하는 상태 알림 메시지를 수신하는 경우, 각 워커 장치에서의 데이터 분담 정책을 조정한 후, 조정된 데이터 분담 정 책을 각 워커 장치로 전달할 수 있다. 다음에서, 데이터 분담 정책에 따른 각 워커 장치의 동작 방법에 대해 설명한다. 먼저, 데이터 분담 정책이 데이터 분담 비율인 경우에 대해 설명한다. 각 워커 장치는 데이터 분담 정책에 따라 내부 처리 데이터와 외부 처리 데이터로 분배하고, 분석 결과를 통한다. AI 서비스는 실시간 영상 스트림에서 얼굴을 인식하고, 인식한 얼굴을 캐릭터로 변환하는 서비스라고 가정한다. 워커 장치A는 데이터 수집 장치인 카메라를 통해 입력되는 초당 30장의 프레임마다 얼굴 좌표를 추출 하고, 얼굴 랜드마크 및 특징(feature)을 추출한다. 워커 장치A는 특징값을 통해 해당 프레임에서 얼굴 부 분을 캐릭터로 변환해서, 모니터에 출력할 수 있다. 이때, 워커 장치A는 저사양 AI 프로세서로 인해, 한 얼굴에 대해서만 AI 서비스 태스크를 저지연으로 처리할 수 있다고 가정한다. 따라서, 워커 장치A가 여러 얼굴들을 포함하는 영상 스트림을 입력받는 경우, 어쩔 수 없이 여러 얼굴들을 인식하고, 모든 얼굴들을 캐릭터 로 변환하는 시간이 오래 걸리게 된다. 워커 장치A는 이러한 서비스 지연을 관리 장치로 전달한다. 예를 들어, AI 서비스에 설정된 서비스 보장 지연이 100ms인데, AI 데이터에 대한 서비스 지연이 200ms인 경우, 관리 장치는 데이터 분담 정책을 조정하여, 워커 장치A로 전달할 수 있다. 그러면, 워커 장치A 는 데이터 분담 정책에 따라 AI 데이터를 내부 처리 데이터와 외부 처리 데이터로 분배한다. 데이터 분담 정책 이 데이터 분담 비율을 포함하는 경우, 워커 장치들은 다음과 같이 동작할 수 있다. 예를 들면, 워커 장치A와 워커 장치B의 데이터 분담 정책이 5:5인경우(“local<5, remote>5”), 워 커 장치A는 카메라로부터 수신되는 프레임 순서대로, 1번째 및 3번째 프레임(F1 및 F3)은 내부 처리 데이 터, 2번째 및 4번째 프레임(F2 및 F4)은 외부 처리 데이터로 분배하고, 2번째 및 4번째 프레임(F2 및 F4)을 워 커 장치B로 전달할 수 있다. 워커 장치B는 2번째 및 4번째 프레임(F2 및 F4)에 대해, AI 서비스 태스크 처리한 결과를 워커 장치A(10 0)로 전달한다. 여기서 워커 장치A로 전달하는 분석 결과는 2번째 및 4번째 프레임(F2 및 F4)에 대해, 얼 굴 좌표 추출, 얼굴 랜드마크 및 특징 추출, 특징값을 이용한 캐릭터 변환하는 태스크를 수행한 결과로서, 캐릭 터 변환된 프레임(F2’ 및 F4’)일 수 있다. 워커 장치A는 1번째 및 3번째 프레임(F1 및 F3)에 대해 AI 서비스 태스크 처리하여, 캐릭터 변환된 프레임 (F1’ 및 F3’)을 획득하고, 외부에서 처리된 캐릭터 변환된 프레임(F2’ 및 F4’)을 획득한 후, 이들을 순서대 로 정렬하여 모니터에 출력할 수 있다. 마찬가지로, 워커 장치A, 워커 장치B, 그리고 워커 장치C의 데이터 분담 정책이 데이터 분담 비 율(예를 들면, 1:6:3)인 경우, 워커 장치A는 카메라로부터 수신되는 프레임들 중에서 1번째 프레임(F1)은 내부 처리 데이터, 2번째부터 10번째 프레임(F2-F10)은 외부 처리 데이터로 분배하고, 2번째부터 10번째 프레임 (F2-F10)을 워커 장치B로 전달할 수 있다. 워커 장치B는 2번째부터 10번째 프레임(F2-F10) 중에서 2 번째부터 7번째 프레임(F2-F7)은 내부 처리 데이터, 8번째부터 10번째 프레임(F8-F10)은 외부 처리 데이터로 분배하고, 8번째부터 10번째 프레임(F8-F10)을 워커 장치C로 전달할 수 있다. 이후, 워커 장치A는 1번 째 프레임(F1)에 대해 AI 서비스 태스크를 수행하여 캐릭터 변환한 프레임(F1’)을 획득하고, 워커 장치B 에서 처리된 캐릭터 변환한 프레임(F2’- F7’) 및 워커 장치C에서 처리된 캐릭터 변환한 프레임(F8’- F10’)을 순서대로 정렬하여 모니터에 출력할 수 있다. 다음으로, 데이터 분담 정책이 분담 태스크인 경우에 대해 설명한다. 각 워커 장치는 데이터 분담 정책에 따라 데이터에서 처리할 태스크(예를 들면, 자동차 인식 태스크, 사람 인식 태스크)를 분담할 수 있다. AI 서비스는 실시간 영상 스트림에서 자동차, 사람, 기타 물체를 인식하고, 인식한 객체 기반으로 자율 주행 서비스를 제공 한다고 가정한다. 워커 장치A는 데이터 수집 장치인 카메라를 통해 입력되는 영상 프레임들에서 자동사, 사람, 기타 물 체를 인식해야 하는데, 객체 수가 늘어나면, 한정된 컴퓨팅 자원에 의해 서비스 지연이 발생한다. 예를 들어, AI 서비스에 설정된 서비스 보장 지연이 10ms인데, AI 데이터에 대한 서비스 지연이 20ms인 경우, 관리 장치는 데이터 분담 정책을 조정하여, 워커 장치A로 전달할 수 있다. 그러면, 워커 장치A 는 데이터 분담 정책에 따라 AI 데이터 중에서 자동차를 인식하는 태스크만 수행하고, 나머지 객체 인식을 상위 워커 장치에 요청할 수 있다. 워커 장치B는 데이터 분담 정책에 따라 AI 데이터 중에서 사람 및 기타을 물 체를 인식하는 태스크를 수행한 후, 사람 및 기타 물체를 인식한 결과를 워커 장치A로 전달할 수 있다. 그 러면, 워커 장치A는 AI 데이터에서 인식한 자동차, 사람, 기타 물체의 위치 및 거리를 기초로 자율 주행 서비스를 제공할 수 있다. 이러한, 분담 태스크인 경우라면, 도 2에서 설명한 부하분담기는 사용될 필요 없이, 스트리머나 포워딩 모 듈이 AI 데이터를 상위 워커 장치로 전달하면 되고, 각 AI 처리기는 데이터 분담 정책에 따라, AI 데이터 에 대한 서로 다른 태스크를 수행하고, 수행 결과를 하위 워커 장치로 응답할 수 있다. 도 7은 한 실시예에 따른 관리 장치의 데이터 분담 정책 생성 방법의 흐름도이다. 도 7을 참고하면, 관리 장치는 AI 서비스 등록 정보를 입력받는다(S310). AI 서비스 등록 정보는, 서비스 이름, 허용 가능한 최대 서비스 지연(서비스 보장 지연), 데이터 분배 방법 등을 포함할 수 있다. 관리 장치 또는 별도의 배포 장치(미도시)가, 등록된 AI 서비스에 해당하는 AI 처리기 및 에이전트 모듈을 계층적 워커 장치들로 배포하고 설치할 수 있다. 관리 장치는 AI 서비스 태스크 처리를 위해 계층적으로 연결된 워커 장치들로부터 컴퓨팅 자원 상태 및/또 는 네트워크 상태를 포함하는 상태 알림 메시지를 수신한다(S320). 관리 장치는 워커 장치들로부터 주기적 으로 상태 알림 메시지를 수신하거나, 컴퓨팅 자원 상태 및/또는 네트워크 상태가 변경되는 경우 상태 알림 메 시지를 수신할 수 있다. 관리 장치는 상태 알림 메시지를 통해 각 워커 장치의 컴퓨팅 자원 상태 및/또는 네트워크 상태를 모니터 링한다(S330). 관리 장치는 각 워커 장치의 컴퓨팅 자원 상태 및/또는 네트워크 상태에 따라, AI 데이터에 대한 AI 서비 스 태스크를 처리할 워커 장치 및 데이터 분담 정책을 결정한다(S340). AI 데이터는 최하위 워커 장치에 연결된 데이터 수집 장치로부터 입력될 수 있다. 관리 장치는 AI 데이터에 대한 AI 서비스 태스크를 처리할 각 워커 장치로, 데이터 분담 정책을 전송한다 (S350). 데이터 분담 정책은 데이터 분담 비율 또는 분담 태스크를 포함할 수 있다. 데이터 분담 정책을 수신한 각 워커 장치는 데이터 분담 정책에 따라 데이터 분석, 데이터 전송, 분석 결과 전 송 등의 절차를 수행한다. 각 워커 장치는 데이터 분담 정책에 따라 분담한 데이터에 대해서 AI 서비스 태스크 처리를 수행하고, 상위 워커 장치로 남아있는 데이터를 전송할 수 있다. 그리고 각 워커 장치는 자신이 분담한 데이터에 대한 분석 결과, 그리고 상위 워커 장치로부터 전달된 분석 결과를 하위 워커 장치로 전달할 수 있다. 도 8은 한 실시예에 따른 하드웨어 구성도이다. 도 6을 참고하면, 워커 장치들은 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치로서, 하드웨어 장치 (110, 210, 310) 각각은 적어도 하나의 프로세서, 메모리, 저장 장치, 네트워크 인터페이스 를 포함할 수 있고, 버스를 통해 연결될 수 있다. 적어도 하나의 프로세서, 메모리, 저장 장치 , 네트워크 인터페이스의 컴퓨팅 자원은 워커 장치의 계층에 따라 차등될 수 있다. 이외에도 입력 장치 및 출력 장치 등의 하드웨어가 포함될 수 있다. 하드웨어 장치에 의해 제공되는 물리적인 컴퓨팅 자원은 가 상화될 수 있다. 프로세서는 컴퓨팅 장치의 동작을 제어하는 장치로서, 컴퓨터 프로그램에 포함된 명령들을 처리하는 다양 한 형태의 프로세서일 수 있고, 예를 들면, CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit) 등 일 수 있다. 프로세서는 메인 프로세서 및 AI 서비스 태스크 처리를 위한 AI 프로세서를 포함할 수 있다. 또한, 프로세서는 위에서 설명한 방법을 실행하기 위한 컴퓨터 프로그램에 대한 연산을 수행할 수 있다. 메모리는 각종 데이터, 명령 및/또는 정보를 저장한다. 메모리는 본 개시의 동작을 실행하도록 기술 된 명령어들이 프로세서에 의해 처리되도록 해당 컴퓨터 프로그램을 저장 장치로부터 로드할 수 있다. 메모리는 예를 들면, ROM(read only memory), RAM(random access memory) 등 일 수 있다. 저장 장치는 본 개시의 동작을 실행하는데 요구되는 각종 데이터, 컴퓨터 프로그램 등을 저장할 수 있다. 저장 장치는 컴퓨터 프로그램을 비임시적으로 저장할 수 있다. 저장 장치는 비휘발성 메모리로 구현 될 수 있다. 네트워크 인터페이스는 유/무선 통신 모듈일 수 있다. 하드웨어 장치(110, 210, 310) 각각은 운영 체제를 비롯한 각종 소프트웨어, 컴퓨터 프로그램들을 탑재하고, 이 를 실행할 수 있다. 도 2에서 설명한 다양한 모듈들은 각 하드웨어 장치에 의해 실행되는데, 가상화된 컴퓨팅 자원을 이용하여 실행될 수 있다. 각 모듈을 위한 가상 컴퓨팅 자원은 각 모듈에 포함된 명령어들을 실행하여 해당 모듈의 기능을 제공할 수 있다. 한편, 모듈들 각각은 독립적인 프로세서 및 메모리를 포함하는 컴퓨팅 장 치로 구현될 수 있다. 한편 관리 장치 역시, 적어도 하나의 프로세서, 메모리, 저장 장치, 네트워크 인터페이스 를 포함하는 컴퓨팅 장치로 구현될 수 있다. 이와 같이, 본 개시에 따르면, 종단에서 AI 서비스를 제공하는 최하위 워커 장치가 한정된 컴퓨팅 자원을 가지 고 있더라도, 상위 워커 장치들의 컴퓨팅 자원까지 사용할 수 있어서, 고성능의 컴퓨팅 자원이 요구되는 AI 서 비스를 초저지연으로 제공할 수 있다. 본 개시에 따르면, AI 서비스에서 요구되는 서비스 품질이 달라지거나, AI 서비스에서 요구되는 컴퓨팅 자원이 가변되더라도, 이를 처리할 수 있는 워커 장치로 교체할 필요 없이, 워커 장치들의 데이터 분담 정책을 재설정 하면 되므로, 고객에게 다양한 AI 서비스를 빠르게 제공할 수 있다. 본 개시에 따르면, 저가의 AI 서비스 단말 기반에서도 초저지연, 고품질의 서비스를 저렴하게 제공할 수 있어서, 고객 만족도를 높일 수 있고, 다양한 AI 서비스로의 사업 확장이 가능하다. 이상에서 설명한 본 개시의 실시예는 장치 및 방법을 통해서만 구현이 되는 것은 아니며, 본 개시의 실시예의 구성에 대응하는 기능을 실현하는 프로그램 또는 그 프로그램이 기록된 기록 매체를 통해 구현될 수도 있다. 이상에서 본 개시의 실시예에 대하여 상세하게 설명하였지만 본 개시의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 개시의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 개시의 권리범위에 속하는 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8"}
{"patent_id": "10-2021-0028243", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 한 실시예에 따른 네트워크 시스템의 구성도이다. 도 2는 한 실시예에 워커 장치들의 구성도이다. 도 3과 도 4는 한 실시예에 따른 워크 장지들 사이의 데이터 분담을 도식적으로 설명하는 도면이다. 도 5와 도 6 각각은 한 실시예에 따른 저지연 인공지능 서비스 제공 방법의 흐름도이다. 도 7은 한 실시예에 따른 관리 장치의 데이터 분담 정책 생성 방법의 흐름도이다. 도 8은 한 실시예에 따른 하드웨어 구성도이다."}
