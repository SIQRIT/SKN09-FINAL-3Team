{"patent_id": "10-2017-0136888", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0013390", "출원번호": "10-2017-0136888", "발명의 명칭": "전자 장치 및 이의 검색 결과 제공 방법", "출원인": "삼성전자주식회사", "발명자": "최윤희"}}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치의 검색 결과 제공 방법에 있어서,애플리케이션 실행 화면을 표시하는 동작;상기 애플리케이션 실행 화면을 표시하는 동안 사용자 입력을 수신하는 동작;상기 사용자 입력에 따라, 상기 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성하는 동작; 및상기 생성된 이미지 상의 상기 사용자 입력에 대응하는 제1 영역에 대한 제1 정보 및 상기 생성된 이미지 상의상기 제1 영역과 다른 제2 영역에 대한 제2 정보를 이용하여 수행된 검색 결과를 표시하는 동작을 포함하고,상기 제1 정보 및 상기 제2 정보는 학습된 모델에 의해 획득되는 것인,검색 결과 제공 방법."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 영역은,상기 사용자 입력에 대응하는 터치 좌표에 기반하여, 상기 생성된 이미지로부터 획득되는 것인검색 결과 제공 방법."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 방법은,상기 생성된 이미지를 상기 애플리케이션 실행 화면을 대신하여 표시하는 동작을 더 포함하는 검색 결과 제공 방법."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 방법은,인공지능 에이전트의 실행에 의해 수행되는 것인검색 결과 제공 방법."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 제1 정보는, 공개특허 10-2019-0013390-3-상기 생성된 이미지의 상기 제1 영역을 인식하여 획득된 정보인검색 결과 제공 방법."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 제2 정보는, 상기 생성된 이미지의 상기 제2 영역을 인식하여 획득된 정보인검색 결과 제공 방법."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 사용자 입력은,상기 애플리케이션 실행 화면에 대한 입력인검색 결과 제공 방법."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 방법은,상기 검색 결과의 표시에 따른 사용자 피드백을 외부 장치로 전송하는 동작을 더 포함하는 검색 결과 제공 방법."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 생성된 이미지의 적어도 일부를 외부 검색 서버로 전송하는 동작; 및상기 검색 결과를 상기 외부 검색 서버로부터 수신하는 동작을 더 포함하는 검색 결과 제공 방법."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,상기 검색 결과를 표시하는 동작은,상기 애플리케이션 실행 화면이 표시되는 동안 상기 검색 결과가 표시되는 동작인 검색 결과 제공 방법."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2019-0013390-4-전자 장치에 있어서,디스플레이;사용자 입력부; 통신부;상기 디스플레이, 상기 사용자 입력부 및 상기 통신부와 전기적으로 연결된 프로세서; 및상기 프로세서에 의해 실행되는 하나 이상의 컴퓨터 프로그램을 저장하는 메모리를 포함하며,상기 하나 이상의 컴퓨터 프로그램은,상기 디스플레이를 통해 애플리케이션 실행 화면을 표시하는 동작;상기 애플리케이션 실행 화면을 표시하는 동안 사용자 입력을 수신하는 경우, 상기 사용자 입력에 따라, 상기애플리케이션 실행 화면을 캡쳐하여 이미지를 생성하는 동작; 및상기 생성된 이미지 상의 상기 사용자 입력에 대응하는 제1 영역에 대한 제1 정보 및 상기 생성된 이미지 상의상기 제1 영역과 다른 제2 영역에 대한 제2 정보를 이용하여 수행된 검색 결과를 표시하는 동작;을 포함하는 전자 장치."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 제1 영역은,상기 사용자 입력에 대응하는 터치 좌표에 기반하여, 상기 생성된 이미지로부터 획득되는 것인전자 장치."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 하나 이상의 컴퓨터 프로그램은,상기 생성된 이미지를 상기 애플리케이션 실행 화면을 대신하여 표시하는 동작을 더 포함하는 전자 장치."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 하나 이상의 컴퓨터 프로그램은,인공지능 에이전트의 실행에 의해 수행되는 것인전자 장치."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 제1 정보는, 상기 생성된 이미지의 상기 제1 영역을 인식하여 획득된 정보인공개특허 10-2019-0013390-5-전자 장치."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 제2 정보는, 상기 생성된 이미지의 상기 제2 영역을 인식하여 획득된 정보인전자 장치."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항에 있어서,상기 사용자 입력은,상기 애플리케이션 실행 화면에 대한 입력인전자 장치."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항에 있어서,상기 하나 이상의 컴퓨터 프로그램은,상기 검색 결과의 표시에 따른 사용자 피드백을 외부 장치로 전송하는 동작을 더 포함하는 전자 장치."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항에 있어서,상기 검색 결과를 표시하는 동작은,상기 애플리케이션 실행 화면이 표시되는 동안 상기 검색 결과가 표시되는 동작인 전자 장치."}
{"patent_id": "10-2017-0136888", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "전자 장치의 학습된 모델을 이용한 정보 획득 방법에 있어서,애플리케이션 실행 화면을 표시하는 동작;상기 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성하는 동작;상기 생성된 이미지를 입력 데이터로 사용하는 학습된 제1 모델을 통해, 상기 생성된 이미지 상의 제1 영역에대한 제1 정보를 획득하는 동작; 및상기 획득된 제1 정보 및 상기 생성된 이미지를 입력 데이터 사용하는 학습된 제2 모델을 통해, 상기 생성된 이미지 상의 제1 영역과 다른 제2 영역에 대한 제2 정보를 획득하는 동작;을 포함하는 정보 획득 방법.공개특허 10-2019-0013390-6-"}
{"patent_id": "10-2017-0136888", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 문서의 다양한 실시예는 전자 장치의 검색 결과 제공 방법 및 이를 위한 전자 장치에 관한 것이다. 이 때, 전 자 장치의 검색 결과 제공 방법은, 객체를 포함하는 화면을 표시하는 동작, 상기 객체를 선택하는 사용자 입력을 감지하는 동작, 상기 사용자 입력에 응답하여, 상기 객체를 포함하는 화면을 캡쳐하여 캡쳐 이미지를 생성하는 동작, 상기 캡쳐 이미지의 적어도 일부를 외부 장치로 전송하는 동작, 상기 객체에 대응하는 객체 정보 및 상기 객체와 관련된 추가 정보에 기반하여 검색된 검색 결과를 외부 장치로부터 수신하는 동작, 및 상기 검색 결과를 표시하는 동작을 포함한다."}
{"patent_id": "10-2017-0136888", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 문서는 검색 결과를 제공하는 전자 장치 및 검색 결과를 제공하는 방법에 관한 것이다. 특히, 본 문서는 전자 장치의 화면에 포함된 객체와 연관된 검색 결과를 제공하는 방법에 관한 것이다. 또한, 본 문서는 기계 학습 알고리즘을 활용하여 인간 두뇌의 인지, 판단 등의 기능을 모사하는 인공 지능 (Artificial Intelligence, AI) 시스템 및 그 응용에 관한 것이다."}
{"patent_id": "10-2017-0136888", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "전자 장치의 통신 기술 및 사용자 인터페이스가 발전함에 따라, 사용자는 장소 및 시간에 제약 없이 필요한 정 보를 쉽게 전자 장치를 통하여 제공 받을 수 있다. 전자 장치가 객체를 포함하는 화면을 제공하는 경우, 사용자는 제공된 객체와 관련된 연관 정보를 검색하기를 원할 수 있다. 이를 위해, 사용자는 객체를 별도로 저장하고, 저장된 객체를 키워드로 하여 영상 검색을 수행하거나 또는 객체 에 관한 텍스트를 직접 입력하여 텍스트 검색을 수행할 수 있다. 또한, 근래에는 인간 수준의 지능을 구현하는 인공 지능 시스템이 다양한 분야에서 이용되고 있다. 인공 지능 시스템은 기존의 룰(rule) 기반 스마트 시스템과 달리 기계가 스스로 학습하고 판단하며 똑똑해지는 시스템이다. 인공 지능 시스템은 사용할수록 인식률이 향상되고 사용자 취향을 보다 정확하게 이해할 수 있게 되어, 기존 룰 기반 스마트 시스템은 점차 딥러닝 기반 인공 지능 시스템으로 대체되고 있다. 인공 지능 기술은 기계학습(예로, 딥러닝) 및 기계학습을 활용한 요소 기술들로 구성된다. 기계학습은 입력 데이터들의 특징을 스스로 분류/학습하는 알고리즘 기술이며, 요소기술은 딥러닝 등의 기계학 습 알고리즘을 활용하여 인간 두뇌의 인지, 판단 등의 기능을 모사하는 기술로서, 언어적 이해, 시각적 이해, 추론/예측, 지식 표현, 동작 제어 등의 기술 분야로 구성된다. 인공 지능 기술이 응용되는 다양한 분야는 다음과 같다. 언어적 이해는 인간의 언어/문자를 인식하고 응용/처리 하는 기술로서, 자연어 처리, 기계 번역, 대화시스템, 질의 응답, 음성 인식/합성 등을 포함한다. 시각적 이해 는 사물을 인간의 시각처럼 인식하여 처리하는 기술로서, 객체 인식, 객체 추적, 영상 검색, 사람 인식, 장면 이해, 공간 이해, 영상 개선 등을 포함한다. 추론 예측은 정보를 판단하여 논리적으로 추론하고 예측하는 기술 로서, 지식/확률 기반 추론, 최적화 예측, 선호 기반 계획, 추천 등을 포함한다. 지식 표현은 인간의 경험정보 를 지식데이터로 자동화 처리하는 기술로서, 지식 구축(데이터 생성/분류), 지식 관리(데이터 활용) 등을 포함 한다. 동작 제어는 차량의 자율 주행, 로봇의 움직임을 제어하는 기술로서, 움직임 제어(항법, 충돌, 주행), 조 작 제어(행동 제어) 등을 포함한다."}
{"patent_id": "10-2017-0136888", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "종래에는 사용자가 화면에 포함된 객체와 관련된 연관 정보를 검색하기 위하여, 여러 단계를 거쳐야 하는 불편 함이 있었다. 특히, 전자 장치가 영상 검색을 지원하지 않는 경우, 사용자는 객체에 관한 텍스트를 직접 입력해 야만 했다. 또한, 객체와 관련된 연관 정보가 다양한 분야에서 검색되는 경우, 사용자가 검색 결과에 기초하여 다시 재검색을 수행해야 하는 불편함이 빈번하게 발생하였다."}
{"patent_id": "10-2017-0136888", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따른, 전자 장치의 검색 결과 제공 방법은, 애플리케이션 실행 화면을 표시하는 동작; 상기 애플리케이션 실행 화면을 표시하는 동안 사용자 입력을 수신하는 동작; 상기 사용자 입력에 따라, 상기 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성하는 동작; 및 상기 생성된 이미지 상의 상기 사용자 입력에 대응하는 제1 영역에 대한 제1 정보 및 상기 생성된 이미지 상의 상기 제1 영역과 다른 제2 영역에 대한 제2 정 보를 이용하여 수행된 검색 결과를 표시하는 동작을 포함하고, 상기 제1 정보 및 상기 제2 정보는 학습된 모델에 의해 획득될 수 있다. 그리고, 상기 제1 영역은, 상기 사용자의 입력에 대응하는 터치 좌표에 기반하여, 상기 생성된 이미지로부터 획 득될 수 있다. 또한, 상기 방법은, 상기 생성된 이미지를 상기 애플리케이션 실행 화면을 대신하여 표시하는 동작을 더 포함할 수 있다. 그리고, 상기 방법은, 인공지능 에이전트의 실행에 의해 수행될 수 있다. 또한, 상기 제1 정보는, 상기 생성된 이미지의 상기 제1 영역을 인식하여 획득된 정보일 수 있다. 그리고, 상기 제2 정보는, 상기 생성된 이미지의 상기 제2 영역을 인식하여 획득된 정보일 수 있다. 또한, 상기 사용자 입력은, 상기 애플리케이션 실행 화면에 대한 입력일 수 있다. 그리고, 상기 방법은, 상기 검색 결과의 표시에 따른 사용자 피드백을 외부 장치로 전송하는 동작을 더 포함할 수 있다. 또한, 상기 생성된 이미지의 적어도 일부를 외부 검색 서버로 전송하는 동작; 및 상기 검색 결과를 상기 외부 검색 서버로부터 수신하는 동작을 더 포함할 수 있다. 상기 검색 결과를 표시하는 동작은, 상기 애플리케이션 실행 화면이 표시되는 동안 상기 검색 결과가 표시되는 동작일 수 있다. 본 개시의 일 실시예에 따른, 전자 장치는, 디스플레이; 사용자 입력부; 통신부; 상기 디스플레이, 상기 사용자 입력부 및 상기 통신부와 전기적으로 연결된 프로세서; 및 상기 프로세서에 의해 실행되는 하나 이상의 컴퓨터 프로그램을 저장하는 메모리를 포함하며, 상기 하나 이상의 컴퓨터 프로그램은, 상기 디스플레이를 통해 애플리 케이션 실행 화면을 표시하는 동작; 상기 애플리케이션 실행 화면을 표시하는 동안 사용자 입력을 수신하는 경 우, 상기 사용자 입력에 따라, 상기 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성하는 동작; 및 상기 생성 된 이미지 상의 상기 사용자 입력에 대응하는 제1 영역에 대한 제1 정보 및 상기 생성된 이미지 상의 상기 제1 영역과 다른 제2 영역에 대한 제2 정보를 이용하여 수행된 검색 결과를 표시하는 동작;을 포함한다. 그리고, 상기 제1 영역은, 상기 사용자의 입력에 대응하는 터치 좌표에 기반하여, 상기 생성된 이미지로부터 획 득될 수 있다. 또한, 상기 하나 이상의 컴퓨터 프로그램은, 상기 생성된 이미지를 상기 애플리케이션 실행 화면을 대신하여 표 시하는 동작을 더 포함할 수 있다. 그리고, 상기 하나 이상의 컴퓨터 프로그램은, 인공지능 에이전트의 실행에 의해 수행될 수 있다. 또한, 상기 제1 정보는, 상기 생성된 이미지의 상기 제1 영역을 인식하여 획득된 정보일 수 있다. 그리고, 상기 제2 정보는, 상기 생성된 이미지의 상기 제2 영역을 인식하여 획득된 정보일 수 있다. 또한, 상기 사용자 입력은, 상기 애플리케이션 실행 화면에 대한 입력일 수 있다. 그리고, 상기 하나 이상의 컴퓨터 프로그램은, 상기 검색 결과의 표시에 따른 사용자 피드백을 외부 장치로 전 송하는 동작을 더 포함할 수 있다. 또한, 상기 검색 결과를 표시하는 동작은, 상기 애플리케이션 실행 화면이 표시되는 동안 상기 검색 결과가 표 시되는 동작일 수 있다. 한편, 본 개시의 일 실시예에 따른, 전자 장치의 학습된 모델을 이용한 정보 획득 방법은, 애플리케이션 실행 화면을 표시하는 동작; 상기 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성하는 동작; 상기 생성된 이미지 를 입력 데이터로 사용하는 학습된 제1 모델을 통해, 상기 생성된 이미지 상의 제1 영역에 대한 제1 정보를 획 득하는 동작; 및 상기 획득된 제1 정보 및 상기 생성된 이미지를 입력 데이터 사용하는 학습된 제2 모델을 통해, 상기 생성된 이미지 상의 제1 영역과 다른 제2 영역에 대한 제2 정보를 획득하는 동작;을 포함할 수 있다."}
{"patent_id": "10-2017-0136888", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 문서에 따르면, 사용자는 화면에 포함된 객체와 연관된 검색 결과를 쉽게 검색할 수 있다. 또한, 객체의 선택에 기반하여 획득된 컨텍스트 정보를 이용하여 검색 결과를 제공함으로써, 사용자의 검색 의 도가 잘 반영된 사용자 맞춤형 검색 결과가 제공될 수 있다. 이에 따라, 사용자의 검색 단계 및 재검색 횟수가 줄어들어 전자 장치를 사용하는 사용자의 만족도 및 편의성이 향상될 수 있다."}
{"patent_id": "10-2017-0136888", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 문서의 다양한 실시 예가 첨부된 도면을 참조하여 기재된다. 그러나, 이는 본 문서에 기재된 기술을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 문서의 실시 예의 다양한 변경(modifications), 균등물 (equivalents), 및/또는 대체물(alternatives)을 포함하는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조 부호가 사용될 수 있다. 본 문서에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. 본 문서에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\"등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 문서에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중 요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들을한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요 소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제1 구성요소)가 다른 구성 요소(예: 제2 구성요소)에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 상기 어떤 구성요 소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제 3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 문서에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for),\" \"~하는 능력을 가지는(having the capacity to),\" \"~하도록 설계된(designed to),\" \"~하도 록 변경된(adapted to),\" \"~하도록 만들어진(made to),\" 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장치가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행하도록 구성된(또는 설정된) 부프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메모리 장치에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 본 문서의 다양한 실시예들에 따른 전자 장치는, 예를 들면, 스마트폰, 태블릿 PC, 이동 전화기, 영상 전화기, 전자책 리더기, 데스크탑 PC, 랩탑 PC, 넷북 컴퓨터, 워크스테이션, 서버, PDA, PMP(portable multimedia player), MP3 플레이어, 의료기기, 카메라, 또는 웨어러블 장치 중 적어도 하나를 포함할 수 있다. 웨어러블 장 치는 액세서리형(예: 시계, 반지, 팔찌, 발찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head- mounted-device(HMD)), 직물 또는 의류 일체형(예: 전자 의복), 신체 부착형(예: 스킨 패드 또는 문신), 또는 생체 이식형 회로 중 적어도 하나를 포함할 수 있다. 어떤 실시예들에서, 전자 장치는, 예를 들면, 텔레비전, DVD(digital video disk) 플레이어, 오디오, 냉장고, 에어컨, 청소기, 오븐, 전자레인지, 세탁기, 공기 청정기, 셋톱 박스, 홈 오토매이션 컨트롤 패널, 보안 컨트롤 패널, 미디어 박스(예: 삼성 HomeSyncTM, 애플TVTM, 또는 구글 TVTM), 게임 콘솔(예: XboxTM, PlayStationTM), 전자 사전, 전자 키, 캠코더, 또는 전자 액자 중 적어도 하나를 포함할 수 있다. 다른 실시예에서, 전자 장치는, 각종 의료기기(예: 각종 휴대용 의료측정기기(혈당 측정기, 심박 측정기, 혈압 측정기, 또는 체온 측정기 등), MRA(magnetic resonance angiography), MRI(magnetic resonance imaging), CT(computed tomography), 촬영기, 또는 초음파기 등), 네비게이션 장치, 위성 항법 시스템(GNSS(global navigation satellite system)), EDR(event data recorder), FDR(flight data recorder), 자동차 인포테인먼 트 장치, 선박용 전자 장비(예: 선박용 항법 장치, 자이로 콤파스 등), 항공 전자기기(avionics), 보안 기기, 차량용 헤드 유닛(head unit), 산업용 또는 가정용 로봇, 드론(drone), 금융 기관의 ATM, 상점의 POS(point of sales), 또는 사물 인터넷 장치 (예: 전구, 각종 센서, 스프링클러 장치, 화재 경보기, 온도조절기, 가로등, 토 스터, 운동기구, 온수탱크, 히터, 보일러 등) 중 적어도 하나를 포함할 수 있다. 본 문서에서, 사용자라는 용어는 전자 장치를 사용하는 사람 또는 전자 장치를 사용하는 장치(예: 인공지능 전 자 장치)를 지칭할 수 있다. 도 1은, 다양한 실시예에 따른 검색 결과를 제공하는 전자 장치의 사용도를 나타낸다. 먼저, 도 1의 (a)와 같이, 전자 장치(A)는 객체(예로, 건축물)를 포함하는 화면을 표시할 수 있다. 객체는, 예로, 콘텐트에 포함된 복수의 객체들 중 일 객체가 될 수 있다. 이 경우, 전자 장치(A)의 사용자 (U)가 객체와 관련된 정보를 추가적으로 알고 싶은 상황이 발생할 수 있다. 예로, 사용자(U)는 객체 인 건물의 이름 또는 용도를 알고 싶을 수 있다. 또한, 객체를 포함하는 화면은 애플리케이션의 실행 화면 일 수 있다. 이에, 도 1의 (b)와 같이, 사용자(U)는 객체를 선택할 수 있다. 예로, 사용자(U)는 객체가 표시된 디 스플레이 영역의 일 지점을 롱 프레스(long press)할 수 있다. 또는, 사용자(U)는 객체를 선택하기 위하여, 손가락 또는 전자펜 등을 이용하여 객체를 멀티 터치하거나, 강하게 터치하거나, 객체의 주변을 드로잉하거나, 객체의 적어도 일부를 경유하도록 대각선으로 드래그할 수 있다. 또는, 사용자(U)는 전자 장치 (A)에 구비된 버튼(예를 들어, 인공지능 기능을 실행하기 위한 버튼)을 누른 후(또는, 누르는 동안에), 객체를 터치할 수 있다. 또는, 사용자는 사전에 정의한 액션을 이용하여 객체를 선택할 수도 있다. 이에 대한 구체적인 예들은 이하 다양한 실시예들을 통하여 후술될 예정이다. 전자 장치(A)는 일 지점을 선택하는 사용자 입력을 감지할 수 있다. 사용자 입력에 응답하여, 전자 장치(A)는 객체를 포함하는 화면을 캡쳐(capture)하여 캡쳐 이미지를 생성할 수 있다. 캡쳐 이미지는, 예로, 전자 장 치(A)의 메모리에 저장될 수 있다. 이 때, 전자 장치(A)는 생성된 캡쳐 이미지를 객체를 포함하는 화면 대신에 표시할 수도 있다. 다음으로, 전자 장치(A)는 사용자 입력에 대응하는 터치 좌표에 기반하여 캡쳐 이미지에서 객체와 관련된 객체 영역(또는, 제1 영역)을 검출할 수 있다. 이 때, 검출된 객체 영역은 하이라이트되어 표시되거나 또는 팝 업 화면으로 표시될 수도 있다. 여기서, 하이라이트되어 표시된다는 것은, 선택된 객체 영역이 다른 영역과 구 별되도록 표시되는 것을 의미할 수 있다. 예로, 하이라이트되어 표시된다는 것은, 예로, 다른 음영, 다른 명암 또는 보색을 갖도록 표시되거나, 객체 영역의 경계를 점선 또는 실선 등으로 구분하여 표시하거나, 객체 영역을 지시하는 인디케이터를 표시하는 것 등을 포함할 수 있다. 객체 영역이 하이라이트되는 구체적인 예들은 이하 다양한 실시예를 통하여 후술될 예정이다. 다음으로,전자 장치(A)는 검출된 객체 영역을 바탕으로 객체 영역에 포함된 객체에 대한 정보(또는, 제1 정보) 를 획득할 수 있으며, 캡쳐된 이미지 중 객체 영역을 주변의 주변 영역(또는 제2 영역)을 바탕으로 객체에 대한 컨텍스트 정보(또는 제2 정보)를 획득할 수 있다. 이때, 전자 장치(A)는 학습된 모델을 통해 객체에 대한 정보 및 컨텍스트 정보를 획득할 수 있다. 이에 대해서는 상세히 후술하도록 한다. 전자 장치(A)는 객체에 대한 정보 및 객체의 선택에 따라 획득된 컨텍스트 정보를 이용하여, 객 체와 연관된 검색 결과를 획득할 수 있다. 이 때, 컨텍스트 정보는 객체의 선택에 따라 획득된 정보로서, 사용자가 객체를 선택하는 시점에 객체 를 포함하는 화면에서 객체의 주변 영역에 대한 주변 정보를 포함할 수 있다. 객체의 주변 정보는 선 택된 객체에 근접하게 위치하는 다른 객체(예로, 텍스트 또는 이미지)가 될 수도 있고, 또는, 객체를 포함 하는 전체 문서 중 현재 디스플레이로 보여지는 문서에 포함된 일부 텍스트가 될 수도 있다. 다른 예로, 사용자에 의해 선택된 객체를 포함하는 화면이 하나의 이미지인 경우, 객체의 선택에 따 라 획득된 주변 정보는 이미지 분석을 통해 획득된 이미지가 촬영된 것으로 추정되는 위치 정보 또는 시간 정보, 이미지 중 선택된 객체 외의 다른 객체에 대한 정보 또는 다른 객체에 대한 추가 정보 등이 포함될 수 있 다. 또는, 컨텍스트 정보는 사용자가 객체를 선택 시에, 전자 장치(A)에 구비된 카메라로 촬영된 촬영 정보를 포함할 수 있다. 여기서, \"사용자가 객체를 선택 시\"라는 것은, 사용자가 객체를 선택한 시점이 될 수도 있고, 객체를 선택하고 일정 시간(예로, 1초 내지 10초) 이내가 될 수도 있다. 한편, 전자 장치(A)는 컨텍스트 정보뿐만 아니라, 사용자의 전자 장치(A)의 사용 이력 정보를 이용하여 객체 와 연관된 검색 결과를 획득할 수 있다. 또는, 전자 장치(A)는 전자 장치(A)의 사용자의 프로파일을 이용 하여 객체와 연관된 검색 결과를 획득할 수도 있다. 도 1의 (c)에서, 전자 장치(A)는 획득된 객체와 연관된 검색 결과를 화면에 표시할 수 있다. 검색 결과를 화면에 표시하는 방법으로는, 선택된 객체와 오버랩 되지 않도록 객체가 표시되지 않는 디스플레이 영역의 일부 에 검색 결과가 표시될 수 있다. 이 경우, 객체와 검색 결과가 오버랩 되지 않도록 하기 위해서 객체의 표시 위 치를 변경할 수 있다. 예를 들어 검색 결과가 디스플레이의 하단 영역에 표시되는 경우 객체는 디스플레이 상단 영역에 표시되도록 객체의 표시 위치가 변경되고, 검색 결과가 디스플레이의 상단 영역에 표시되는 경우 객체는 디스플레이 하단 영역에 표시되도록 객체의 표시 위치가 변경될 수 있다. 또한, 객체가 표시되는 화면과 별도의 화면으로 검색 결과를 표시할 수 있다. 예를 들어, 객체가 표시되는 화면 상에 별도의 팝업 윈도우가 표시되고, 그 윈도우를 통해 검색 결과를 표시할 수 있다. 이 경우, 팝업 윈도우의 표시 위치는 선택된 객체의 위치에 따라 결정되며, 팝업 윈도우의 크기는 객체의 화면 상의 크기와 전체 화면에 서 객체를 제외한 화면의 크기에 따라 결정된다. 객체가 표시되는 화면과 별도의 화면으로 검색 결과를 표시하 는 경우에도 팝업 윈도우가 객체와 겹쳐지지 않도록 표시될 수 있다.물론, 검색 결과를 표시하는 별도의 화면은 객체가 표시되는 화면 대신에 표시될 수 있다. 이 경우 검색 결과를 표시하는 별도의 화면에는 선택된 객체의 이미지와 객체와 연관된 검색 결과가 함께 표시될 수 있다. 이 경우 별도의 화면의 배경 부분은 투명하게 표시되어 객체를 포함하는 화면이 보이도록 제공할 수 있다. 객체와 연관된 검색 결과는, 예로, 객체에 대한 상세 정보, 객체와 관련된 광고 정보, 객체의 구매 정보, 객체 와 연관된 다른 객체 정보 등이 포함될 수 있다. 구체적으로, 객체에 대한 상세 정보는 검색 엔진을 통한 신문 기사, SNS에 업로드된 문서, 웹사이트에 개시된 문서 등에서 웹 문서 검색을 통해 수집될 수 있다. 또한, 객체 와 관련된 광고 정보는 객체와 관련된 상품 및 서비스의 광고를 제공하는 웹 사이트 또는 상품을 제조하는 제조 사 또는 서비스를 제공하는 회사의 웹 사이트 등에서 수집될 수 있다. 또한, 객체와 관련된 구매 정보는 검색 엔진을 통한 상품 및 서비스를 판매하는 웹사이트 등에서 상품 및 서비스 검색을 통해 수집될 수 있다. 또한 객 체와 연관된 다른 객체 정보는 검색 엔진에서 객체에 대응하는 쿼리와 유사도 및 관련도가 높은 다른 쿼리를 이 용한 검색을 통해 수집될 수 있다. 또한, 객체와 연관된 검색 결과는 다양한 컨텐트(예로, 텍스트, 이미지, 동영상 등) 및 UI 엘리먼트(예를 들어, 아이콘, 하이퍼링크 등)로 구성될 수 있다. 예를 들어, 선택된 객체가 \"건축물\"인 경우, 객체와 연관된 검색 결 과는 선택된 객체를 안내하기 위한 텍스트 컨텐트 및 이미지 컨텐트를 포함할 수 있으며, 선택된 객체에 대한 제어 동작(예를 들어, 저장, 공유 등)을 수행하기 위한 아이콘을 포함할 수 있으며, 선택된 객체에 대한 추가 정보(예를 들어, 위치 정보, 관광 정보)를 포함하는 웹 페이지를 액세스하기 위한 하이퍼링크를 포함할 수 있다. 또한, 객체와 연관된 검색 결과는 사용자 입력에 의해 변경될 수 있다. 예를 들어, 선택된 객체가 \"호텔\"인 경 우, 객체와 연관된 검색 결과로 \"호텔에 대한 상세정보\"가 포함된 화면(예를 들어, 팝업 화면)이 제공될 수 있 다. \"호텔에 대한 상세정보\"가 제공되는 동안 사용자 입력(예를 들어, 드래그 입력)이 수신되면, \"호텔에 대한 상세정보\"를 포함하는 화면이 제거되고, \"호텔 후기 정보\"를 포함하는 새로운 화면이 제공될 수 있다. 또한, 객체와 연관된 검색 결과는 시각적 정보뿐만 아니라, 청각적, 촉각적, 후각적 정보 중 적어도 하나의 결 합으로 제공될 수도 있다. 객체와 연관된 검색 결과에 대한 구체적인 예들은 이하 다양한 실시예들을 통하여 후 술된다. 다양한 실시예에 따르면, 전자 장치(A)는 객체 영역 및 객체의 주변 정보를 학습된 객체 인식 모델의 입력 데이 터로 사용하여 컨텍스트가 반영된 객체 정보(예. 객체명과 객체의 위치, 객체명과 객체 주변의 다른 객체, 객체 명과 객체가 포함된 문서의 주제 등)와 객체와 관련된 추가 정보를 획득하고, 추가 정보가 반영된 객체 정보를 이용하여 객체와 연관된 검색 결과를 획득할 수 있다. 또는, 전자 장치(A)는 객체 영역을 학습된 객체 인식 모델의 입력 데이터로 사용하여 일반적인 객체 정보(예로, 객체명, 객체의 아이디, 객체의 종류, 객체의 속성 등)를 획득하고, 획득된 일반적인 객체 정보 및 별도로 수집 된 컨텍스트 정보(예. 객체의 위치 정보, 객체 주변의 다른 객체, 객체가 포함된 문서의 주제 등)를 이용하여 객체와 연관된 검색 결과를 획득할 수 있다. 본 문서에서 학습된 객체 인식 모델은 인식 모델의 적용 분야 또는 장치의 컴퓨터 성능 등을 고려하여 구축될 수 있다. 예로, 학습된 객체 인식 모델은 객체 영역 및 객체의 주변 정보를 입력 데이터로 사용하여 컨텍스트가 반영된 객체 정보를 추정하도록 설정될 수 있다. 학습된 객체 인식 모델은, 예로, 신경망(Neural Network)을 기반으로 하는 모델일 수 있다. 객체 인식 모델은 인간의 뇌 구조를 컴퓨터 상에서 모의하도록 설계될 수 있으며 인간의 신경망의 뉴런(neuron)을 모의하는, 가중치를 가지는 복수 의 네트워크 노드들을 포함할 수 있다. 복수의 네트워크 노드들은 뉴런이 시냅스(synapse)를 통하여 신호를 주 고 받는 뉴런의 시냅틱(synaptic) 활동을 모의하도록 각각 연결 관계를 형성할 수 있다. 또한 객체 인식 모델은, 일 예로, 신경망 모델, 또는 신경망 모델에서 발전한 딥 러닝 모델을 포함할 수 있다. 딥 러닝 모델에 서 복수의 네트워크 노드들은 서로 다른 깊이(또는, 레이어)에 위치하면서 컨볼루션(convolution) 연결 관계에 따라 데이터를 주고 받을 수 있다. 객체 인식 모델의 예에는 DNN(Deep Neural Network), RNN(Recurrent Neural Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 등이 있을 수 있으나 이에 한정되지 않는다. 또한, 전자 장치(A)는 상술한 바와 같은 사용자에 의해 선택된 객체와 관련된 정보를 검색하기 위하여 인공지능 에이전트(Artificial intelligence agent)를 이용할 수 있다. 이때, 인공지능 에이전트는 AI(Artificial Intelligence) 기반의 서비스(예를 들어, 음성 인식 서비스, 비서 서비스, 번역 서비스, 검색 서비스 등)를 제 공하기 위한 전용 프로그램으로서, 기존의 범용 프로세서(예를 들어, CPU) 또는 별도의 AI 전용 프로세서(예를들어, GPU 등)에 의해 실행될 수 있다. 특히, 인공지능 에이전트는 후술할 다양한 모듈을 제어할 수 있다. 구체적으로, 기설정된 사용자 입력(예를 들어, 롱프레스 등)에 의해 화면 상에 객체가 선택되거나 전자 장 치(A)에 구비된 버튼(예를 들어, 인공지능 에이전트를 실행하기 위한 버튼)이 눌러진 후 객체가 선택된 경 우, 인공지능 에이전트가 동작할 수 있다. 그리고, 인공지능 에이전트는 객체를 포함하는 화면을 캡쳐 (capture)하여 캡쳐 이미지를 생성하며, 사용자 입력에 대응하는 터치 좌표에 기반하여 캡쳐 이미지에서 객체 와 관련된 객체 영역을 검출하며, 검출된 객체 영역 및 객체의 선택에 따라 획득된 컨텍스트 정보 (예를 들어, 객체의 주변 정보 등)를 이용하여, 객체와 연관된 검색 결과를 획득하여 제공할 수 있다. 물론, 화면 상에 특정 아이콘이 터치되거나 전자 장치(A)에 구비된 버튼(예를 들어, 인공지능 에이전트를 실행 하기 위한 버튼)이 눌러지면 인공지능 에이전트가 동작할 수도 있다. 그 경우, 인공지능 에이전트는 현재 표시 되는 화면을 캡쳐(capture)하여 캡쳐 이미지를 생성하며, 그 이후 입력된 객체를 선택하기 위한 사용자 입력에 따라 사용자 입력에 대응하는 터치 좌표에 기반하여 캡쳐 이미지에서 객체와 관련된 객체 영역을 검출하며, 검출된 객체 영역 및 객체의 선택에 따라 획득된 컨텍스트 정보를 이용하여, 객체와 연관된 검색 결과를 획득하고, 연관된 검색 결과를 제공할 수 있다. 인공지능 에이전트는 객체에 대해 기설정된 사용자 입력이 감지되거나 전자 장치(A)에 구비된 버튼이 선택 된 이전에 기 실행된 상태일 수 있다. 이 경우, 객체에 대해 기설정된 사용자 입력이 감지되거나 전자 장 치(A)에 구비된 버튼이 선택된 이후에는 전자 장치(A)의 인공지능 에이전트가 화면을 캡쳐한 후 객체의 관 련 정보를 검색하여 제공할 수 있다. 예를 들어, 인공지능 에이전트가 AI 전용 프로세서에 의해 실행되는 경우, 객체에 대해 기설정된 사용자 입력이 감지되거나 전자 장치(A)에 구비된 버튼이 선택되기 전에는 범용 프 로세서에 의해 전자 장치(A)의 기능이 실행되며, 객체에 대해 기설정된 사용자 입력이 감지되거나 전자 장 치(A)에 구비된 버튼이 선택된 이후에는 AI 전용 프로세서에 의해 전자 장치(A)의 기능이 실행될 수 있다. 또한, 인공지능 에이전트는 객체에 대해 기설정된 사용자 입력이 감지되거나 전자 장치(A)에 구비된 버튼 이 선택된 이전에 대기 상태일 수 있다. 여기서 대기 상태란, 인공지능 에이전트의 동작 시작을 제어하기 위해 미리 정의된 사용자 입력이 수신되는 것을 감지하는 상태이다. 인공지능 에이전트가 대기 상태인 동안 객체 에 대해 기설정된 사용자 입력이 감지되거나 전자 장치(A)에 구비된 버튼이 선택되면, 전자 장치(A)는 인 공지능 에이전트를 동작시키고, 동작된 인공지능 에이전트를 이용하여 화면을 캡쳐한 후 객체의 관련 정보 를 검색하여 제공할 수 있다. 또한, 인공지능 에이전트는 객체에 대해 기설정된 사용자 입력이 감지되거나 전자 장치(A)에 구비된 버튼 이 선택된 이전에 종료된 상태일 수 있다. 인공지능 에이전트가 종료된 상태에서 객체에 대해 기설정된 사 용자 입력이 감지되거나 전자 장치(A)에 구비된 버튼이 선택되면, 전자 장치(A)는 인공지능 에이전트를 실행시 키고, 실행된 인공지능 에이전트를 이용하여 화면을 캡쳐한 후 객체의 관련 정보를 검색하여 제공할 수 있 다. 한편, 인공지능 에이전트는 후술할 다양한 장치 또는 모듈을 제어할 수 있다. 이에 대해서는 추후 상세히 설명 하기로 한다. 또한, 전자 장치(A) 및 서버 간의 학습된 객체 인식 모델을 이용하여 객체와 연관된 검색 결과를 획득하는 구체 적인 예들은 이하 다양한 실시예들을 통하여 후술된다. 도 2a는, 다양한 실시예에 따른 전자 장치(A)의 블록도를 나타낸다. 전자 장치(A)는 도 2a에 도시된 바와 같이, 디스플레이, 메모리, 사용자 입력부, 통신부 및 프로세서를 포함한다. 도 2에 도시된 구성들은 본 개시의 실시 예들을 구현하기 위한 예시도이며, 당업자에게 자명한 수준의 적절한 하드웨어/소프트 웨어 구성들이 전자 장치(A)에 추가로 포함될 수 있다. 디스플레이는 다양한 화면을 제공할 수 있다. 특히, 디스플레이는 객체를 포함하는 화면을 제공할 수 있다. 또한, 디스플레이는 객체를 포함하는 화면 상에 객체와 관련된 검색 결과를 포함하는 화면(예를 들 어, 팝업 화면)을 추가로 제공할 수 있다. 또는 디스플레이는 객체와 관련된 검색결과를 포함하는 화면을 별도로 제공할 수 있다. 메모리는 전자 장치(A)의 적어도 하나의 다른 구성요소에 관계된 명령 또는 데이터를 저장할 수 있다. 특 히, 메모리는 비휘발성 메모리, 휘발성 메모리, 플래시메모리(flash-memory), 하드디스크 드라이브(HDD) 또는 솔리드 스테이트 드라이브(SSD) 등으로 구현될 수 있다. 메모리는 프로세서에 의해 액세스되며, 프로세서에 의한 데이터의 독취/기록/수정/삭제/갱신 등이 수행될 수 있다. 본 개시에서 메모리라는 용어 는 메모리, 프로세서 내 롬(미도시), 램(미도시) 또는 전자 장치(A)에 장착되는 메모리 카드(미도 시)(예를 들어, micro SD 카드, 메모리 스틱)를 포함할 수 있다. 또한, 메모리에는 디스플레이부의 디스플레이 영역에 표시될 각종 화면을 구성하기 위한 프로그램 및 데이터 등이 저장될 수 있다. 또한, 메모리는 선택된 객체와 관련된 정보를 제공하기 위한 인공지능 에이전트를 저장할 수 있으며, 본 개시의 인식 모델(객체 인식 모델, 주변 정보 인식 모델, 얼굴 인식 모델 등)을 저장할 수 있다. 또한, 메모리는 도 2b에 도시된 검색 액션 감지 모듈, 화면 캡쳐/좌표 수집 모듈, 사용자 UI 모 듈, 객체 영역 검출/분류 모듈, 얼굴 검출 모듈 및 검색 결과 획득 모듈 등을 저장할 수 있다. 사용자 입력부는 다양한 사용자 입력을 수신하여 프로세서로 전달할 수 있다. 특히, 사용자 입력부 는 터치 센서, (디지털) 펜 센서, 압력 센서, 또는 키를 포함할 수 있다. 터치 센서는, 예를 들면, 정전식, 감압식, 적외선 방식, 또는 초음파 방식 중 적어도 하나의 방식을 사용할 수 있다. (디지털) 펜 센서는, 예를 들면, 터치 패널의 일부이거나, 별도의 인식용 쉬트를 포함할 수 있다. 키는, 예를 들면, 물리적 인 버튼, 광학식 키, 또는 키패드를 포함할 수 있다. 특히, 사용자 입력부는 객체를 선택하기 위한 기설정된 사용자 터치(예를 들어, 롱프레스 터치) 또는 특정 버튼을 누른 후 객체를 터치하는 사용자 입력에 따른 입력 신호를 획득할 수 있다. 그리고, 사용자 입력부(13 0)는 입력 신호를 프로세서로 전송할 수 있다. 통신부는 외부의 전자장치 또는 서버와 통신을 수행할 수 있다. 이때, 통신부는 외부의 서버로 캡쳐 이미지를 전송하거나 객체 영역에 대한 정보 및 컨텍스트 정보(예를 들어, 객체의 주변 정보 등)를 전송할 수 있다. 또한, 통신부는 캡쳐 이미지 또는 객체 영역에 대한 정보 및 컨텍스트 정보에 응답한 객체에 대한 검색 결과를 수신할 수 있다. 프로세서는 디스플레이, 메모리 및 사용자 입력부와 전기적으로 연결되어 전자 장치(A)의 전반적인 동작 및 기능을 제어할 수 있다. 특히, 프로세서는 검색 액션 감지 모듈, 화면 캡쳐/좌표 수집 모듈, 사용자 UI 모듈, 객체 영역 검출/분류 모듈, 얼굴 검출 모듈 및 검색 결과 획 득 모듈 등을 이용하여 사용자에 의해 선택된 객체와 관련된 정보를 검색하는 기능을 수행할 수 있다. 구체적으로, 프로세서는 사용자 입력부를 이용하여 디스플레이에 표시되는 화면의 객체를 선택 하기 위한 사용자 입력에 따른 입력 신호를 획득할 수 있다. 또한, 프로세서는 입력 신호에 응답하여, 객 체를 포함하는 화면을 캡쳐하여 캡쳐 이미지를 생성하고, 생성된 캡쳐 이미지를 메모리에 저장할 수 있다. 또한, 프로세서 는 캡쳐 이미지에 포함된 객체에 대응하는 객체 영역 및 객체의 선택에 따라 획득된 컨텍 스트(context) 정보를 이용하여 객체와 연관된 검색 결과를 획득하고, 객체와 관련된 검색 결과가 표시되도록 디스플레이를 제어할 수 있다. 또한, 프로세서는 캡쳐 이미지의 적어도 일부를 외부의 서버로 전송하도록 통신부를 제어할 수 있다. 이때, 프로세서는 및 선택된 객체 영역에 정보(예를 들어, 객체 영역의 좌표 정보 등)를 함께 전송하도록 통신부를 제어할 수 있다. 그리고, 프로세서는 캡쳐 이미지에 포함된 객체에 대한 객체 정보 및 객체 와 관련된 추가 정보를 기반하여 검색된 검색 결과를 외부의 서버로부터 통신부를 통해 수신하며, 객체와 관련된 검색 결과가 표시되도록 디스플레이를 제어할 수 있다. 프로세서에 대한 설명은 추후 도 2b에 서 더욱 상세히 설명하기로 한다. 도 2b는, 다양한 실시예에 따른, 전자 장치(A)를 포함하는 네트워크 시스템의 블록도들을 나타낸다. 네트워크 시스템은 전자 장치(A), 추천 장치(B), 객체 인식 장치(C), 사용자 특성 인식 장치(D) 및 데이터 수집 장치(E)를 포함할 수 있다. 이 때, 추천 장치(B), 객체 인식 장치(C), 사용자 특성 인식 장치(D) 및 데이터 수집 장치(E)는 설명의 편의상 구분한 것이며, 추천 장치(B), 객체 인식 장치(C), 사용자 특성 인식 장치(D) 및 데이터 수집 장치(E) 각각에 포함된 모듈들 중 적어도 일부가 결합하여 하나 이상의 서버(예로, 복수의 장치들로 구성된 클라우드)로 구성될수도 있다. 또한, 추천 장치(B), 객체 인식 장치(C), 사용자 특성 인식 장치(D) 및 데이터 수집 장치(E) 각각에 포함된 모듈들 중 적어도 일부가 전자 장치(A)에 마련될 수도 있다. 여기서, 모듈은 하드웨어, 소프트웨어 또는 펌웨어로 구성된 유닛을 포함하며, 예를 들면, 로직, 논리 블록, 부 품, 또는 회로 등의 용어와 상호 호환적으로 사용될 수 있다. 도 2b에서, 전자 장치(A)는 디스플레이, 메모리, 사용자 입력부, 통신부, 카메라 및 프로세서를 포함할 수 있다. 디스플레이, 메모리, 사용자 입력부, 통신부에 대해서는 도 2a에서 상세히 설명하였으므로, 중복되는 설명은 생략하기로 한다. 프로세서의 적어도 일부는 특정 목 적 또는 기능에 따라 구별되는 복수의 모듈들로 동작할 수 있다. 프로세서가 복수의 모듈들로 동작하기 위 한 소프트웨어는 메모리에 저장될 수 있다. 이 경우, 메모리는 프로세서가 상기 복수의 모듈들 각각으로 동작하기 위한 복수의 명령어들을 저장할 수 있다. 프로세서는 검색 액션 감지 모듈, 화면 캡쳐/좌표 수집 모듈, 사용자 UI 모듈, 객체 영역 검출/분류 모듈, 얼굴 검출 모듈 및 검색 결과 획득 모듈을 포함할 수 있다. 검색 액션 감지 모듈은 사용자 입력부를 통하여 디스플레이에 표시된 화면에 포함된 객체를 선 택하는 사용자 입력(예로, 롱 터치, 멀티 터치, 펜 액션 등)에 따른 입력 신호를 획득할 수 있다. 검색 액션 감지 모듈이 입력 신호를 획득하면, 화면 캡쳐/좌표 수집 모듈는 화면을 캡쳐하여 캡쳐 이 미지를 생성하고, 디스플레이 상의 일 지점에 대응하는 좌표를 수집할 수 있다. 예로, 화면 캡쳐/좌표 수 집 모듈은 현재 디스플레이에 표시되는 화면을 캡쳐하여 캡쳐 이미지를 생성하고 사용자가 선택한 디 스플레이의 일 지점에 대응하는 좌표를 수집할 수 있다. 다른 예로, 사용자 입력부로서 물리적인 버튼, 베젤 상의 터치 센서를 통하여 입력 신호가 획득되면, 검색 액션 감지 모듈은 디스플레이에 표시 중인 화면을 캡쳐하여 캡쳐 이미지를 생성하고, 사용자 입력부 를 통해 추가적으로 획득한 입력 신호에 따라 사용자가 선택한 디스플레이의 일 지점에 대응하는 좌 표를 수집할 수 있다. 객체 영역 검출/분류 모듈은 캡쳐 이미지 및 수집된 선택 좌표에 기반하여, 캡쳐 이미지에서 객체와 관련 된 객체 영역을 검출하고, 객체 영역을 메모리에 저장할 수 있다. 이 때, 검출된 객체 영역은, 예로, 캡쳐 이미지 중 객체 영역에 대응하는 이미지를 포함할 수 있다. 또한, 객체 영역 검출/분류 모듈은 검출된 객 체 영역에 대응하는 객체의 범주(또는, 객체 부류)를 결정할 수 있다. 예로, 객체 영역 검출/분류 모듈은 검출된 객체 영역의 범주를, 얼굴, 상품, 옷 또는 음식 중 하나로 결정할 수 있다. 다양한 실시예로, 객체 영역 검출/분류 모듈은 객체 인식 장치(C) 중 하나에 선택적으로 마련될 수도 있다. 이 경우, 객체 인식 장치(C)의 객체 영역 검출/분류 모듈이 전자 장치(A)의 객체 영역 검출/분류 모 듈에 대응될 수 있다. 또한, 객체 영역 검출/분류 모듈는 복수의 모듈로 구분되어 구성될 수도 있다. 이 경우, 객체 영역 검출 모듈은 전자 장치(A)에서 동작하도록 구성되고, 객체 영역 분류 모듈은 객체 인식 장치(C) 등과 같은 외부 장치 에서 동작하도록 구성될 수 있다. 얼굴 검출 모듈은 전자 장치(A)에 마련된 카메라를 제어하여 전자 장치(A)를 사용하는 사용자의 얼굴 을 포함하는 이미지를 획득하고, 이미지로부터 사용자의 얼굴을 검출하고, 검출된 얼굴에 대한 안면 정보(예로, 안면 이미지, 안면 영상 등)를 메모리에 저장할 수 있다. 예로, 얼굴 검출 모듈은 객체의 선택에 따 라 획득된 컨텍스트 정보로서 안면 정보를 획득하여 메모리에 저장할 수 있다. 검색 결과 획득 모듈은 검출된 객체 영역 및/또는 컨텍스트 정보(예로, 주변 정보 또는 안면 정보)를 통신 부를 통하여 적어도 하나의 외부 장치로 전송할 수 있다. 예로, 검색 결과 획득 모듈은 상기 객체 영 역 검출/분류 모델를 통해 검출된 캡쳐 이미지의 객체 영역은 객체 인식 장치(C)로 전송하고, 상기 얼굴 검출 모듈을 통해 검출된 얼굴에 대한 안면 정보는 사용자 특성 인식 장치(D)로 전송할 수 있다. 검색 결과 획득 모듈은 객체 영역 및 컨텍스트 정보를 외부 장치로 전송한 결과로서, 객체와 연관된 검색 결과를 획득할 수 있다. 예로, 검색 결과 획득 모듈은 추천 장치(B)로부터 객체와 연관된 검색 결과를 획 득할 수 있다. 사용자 UI 모듈는 검색 결과 획득 모듈에서 획득된 객체와 연관된 검색 결과가 사용자에게 제공되도 록 디스플레이를 제어할 수 있다. 객체 인식 장치(C)는 객체 영역 검출/분류 모듈, 세부 객체 인식 모듈 및 컨텍스트 인식 모듈을 포함할 수 있다. 객체 영역 검출/분류 모듈은 전자 장치(A)로부터 캡쳐 이미지 및 사용자가 선택한 좌표를 수신하는 경우, 캡쳐 이미지 및 선택 좌표에 기반하여, 캡쳐 이미지에서 사용자가 선택한 객체 영역을 검출하고, 검출된 객체 영역의 객체 범주(또는, 객체 부류)를 결정할 수 있다. 예로, 객체 영역 검출/분류 모듈은 검출된 객체 영 역의 범주를, 얼굴, 상품, 옷 또는 음식 중 하나로 결정할 수 있다. 객체 영역 검출/분류 모듈은 별개로 구성될 수도 있으며, 객체 영역 검출 모듈은 전자 장치(A)에서 동작하 도록 구성되고, 객체 영역 분류 모듈은 객체 인식 장치(C)에서 동작하도록 구성될 수 있다. 이 경우, 전자 장치 의 객체 영역 검출 모듈은 캡쳐 이미지에서 사용자가 선택한 객체 영역을 검출한 후 객체 영역에 대한 정보를 객체 인식 장치(C)에 전송하고, 객체 인식 장치의 객체 영역 분류 모듈은 전자 장치(A)로부터 수신한 객체 영역 에 대한 정보를 이용하여 객체 영역의 객체 범주를 결정할 수 있다. 객체 인식 장치(C)에는 다수의 객체 범주들 각각에 대응하는 다수의 세부 객체 인식 모듈들이 존재할 수 있다. 이 경우, 객체 범주가 결정되면, 객체 범주에 대응하는 세부 객체 인식 모듈은 검출된 객체 영역을 더욱 세부적으로 인식할 수 있다. 예로, 세부 객체 인식 모듈은 객체 영역을 인식하여 객체 영역과 관련된 객체 정보를 획득할 수 있다. 구체적으로, 객체 영역 검출/분류 모듈에서 검출된 객체 영역과 관련된 객체 범주 가 얼굴인 경우, 세부 객체 인식 모듈은 객체 정보로서 누구의 얼굴인지를 인식할 수 있다. 특히, 객체 인식 장치(C)는 검출된 객체 영역을 학습된 객체 인식 모델의 입력 데이터로 사용하여 객체 영역과 관련된 객체 정보를 추정할 수도 있다. 학습된 객체 인식 모델은, 예로, 신경망 또는 딥러닝 기반의 인식 모델 이 될 수 있다. 또한, 객체 인식 장치(C)는 검출된 객체 영역을 규칙 기반의 객체 인식 모델을 통해 분석하여 객체 영역과 관련된 객체 정보를 추정할 수도 있다. 다양한 실시예로, 객체 영역 검출/분류 모듈은 전자 장치(A) 및 객체 인식 장치(C) 중 하나에 선택적으로 마련될 수도 있다. 이 경우, 객체 인식 장치(C)의 객체 영역 검출/분류 모듈은 전자 장치(A)의 객체 영역 검출/분류 모듈에 대응될 수 있다. 예로, 객체 영역 검출/분류 모듈은 전자 장치(A)에 마련되고, 세 부 객체 인식 모듈은 객체 인식 장치(C)에 마련될 수 있다. 또는, 객체 영역 검출/분류 모듈 및 세부 객체 인식 모듈 모두가 전자 장치(A) 또는 객체 인식 장치(C)에 마련될 수도 있다. 또한, 객체 인식 장치(C)에는 객체와 연관성 있는 주변 정보를 인식하는 주변 정보 인식 모듈이 존재할 수 있다. 주변 정보 인식 모듈은 객체와 연관성 있는 주변 정보를 획득할 수 있다. 예로, 주변 정보 인식 모듈(20 3)는 객체의 주변에 위치한 주변 정보 중 객체와 연관성 있는 정보를 주변 정보로서 획득할 수 있다. 예로, 선 택된 객체가 화면에 표시된 문서에 포함된 단어 또는 이미지인 경우, 객체의 선택에 기반하여 획득된 주변 정보는 객체가 표시되는 화면에서 추출된 정보로, 객체와 가장 관련도가 높은 정보, 또는 객체(11 1)를 유사한 다른 객체와 구분하는데 가장 유용하게 사용될 수 있는 정보가 객체와 관련된 구변 정보로 결정될 수 있다. 주변 정보 인식 모듈은 학습된 모델인 주변 정보 인식 모델을 이용하여 주변 정보를 결정(또는, 추정, 판 단)할 수 있다. 예로, 객체 인식 장치(C)는 객체가 표시되는 화면의 객체의 주변 영역에 대한 이미지를 학습된 주변 정보 인식 모델의 입력 데이터로 사용하여 객체와 연관성 있는 주변 정보를 결정할 수 있다. 학습된 주변 정보 인식 모델은, 예로, 신경망 또는 딥러닝 기반의 인식 모델이 될 수 있다. 또한 주변 정보 인식 모듈은 규칙 기반의 주변 정보 인식 모델을 이용하여 주변 정보를 결정(또는 추정, 판단)할 수 있다. 예로, 객체 인식 장치(C)는 객체가 표시되는 화면의 객체의 주변 영역에 대한 이미지를 기 정 의된 규칙을 통해 분석하여 객체와 연관성 있는 주변 정보를 결정할 수 있다. 일 실시예로, 객체와 주변 정보 간의 관련도가 가장 높은 단어가 주변 정보로서 주변 정보 인식 모델을 통해 획 득될 수 있다. 이 때, 주변 정보 인식 모델은 객체 및 객체의 주변 정보를 학습 데이터로서 이용하여 학습된 모 델일 수 있다. 일 예로, 객체와 주변 정보 간의 관련도가 가장 높은 단어는 객체와 주변 정보가 한 화면에 표시될 확률 또는 객체에 대한 검색시 주변 정보가 함께 검색될 확률에 기반하여 가장 높은 확률을 갖는 주변 정보 로 결정될 수 있다. 예를 들어, 사용자에 의해 선택된 객체가 '에펠탑'의 이미지 또는 '에펠탑'의 텍스트인 경우, 주변 정보 인식 모듈은 객체의 주변 정보를 주변 정보 인식 모델에 적용한 결과로서, '여행', '역사'및 '건축정보'를 \"에 펠탑\"이라는 객체와 관련도가 가장 높은 주변 정보로 결정할 수 있다. 또한, 사용자에 의해 선택된 객체가 '유명 가수'의 이미지 또는 '유명 가수'의 텍스트인 경우, 주변 정보 인식 모듈은 객체의 주변 정보를 주변 정보 인식 모델에 적용한 결과로서, '콘서트', '앨범' 및 '노래 제목'을 유명 가수에 관한 객체와 관련도가 가장 높은 주변 정보로 결정할 수 있다. 다른 실시예로, 객체를 유사한 다른 객체와 구분하는데 가장 유용하게 사용될 수 있는 정보가 주변 정보 인식 모델을 통해 획득될 수 있다. 객체를 유사한 다른 객체와 구분하는데 가장 유용하게 사용될 수 있는 정보의 일 예로, 객체에 대한 긍정적인/부정적인 속성을 갖는 단어가 있다. 주변 정보 인식 모델은 객체 및 객 체와 관련된 정보 중 긍정적인/부정적인 속성을 갖는 단어를 학습 데이터로서 이용하여 학습된 모델일 수 있다. 긍정적인/부정적인 속성을 갖는 단어는 객체가 다른 객체와 식별 가능한 특징에 대한 정보이다. 이 때, 주변 정 보 인식 모델은 객체와 주변 정보 간의 관련도를 추정하는 주변 인식 모델과 동일한 인식 모델일 수도 있고, 서 로 다른 인식 모델일 수도 있다. 예를 들어, 사용자에 의해 선택된 객체가 '자동차'의 이미지 또는 '자동차'의 텍스트인 경우, 주변 정보 인식 모듈은 객체의 주변 정보를 주변 정보 인식 모델에 적용한 결과로서, '자동차'라는 객체에 대한 관련도가 높으며 긍정적인/부정적인 속성을 갖는 단어인 '연비', '판매량' 및 '가격' 등을 객체에 대한 주변 정보로 결정 할 수 있다. 또한, 사용자에 의해 선택된 객체가 '가전제품'의 이미지 또는 '가전 제품'의 텍스트인 경우, 주변 정보 인식 모듈은 객체의 주변 정보를 주변 정보 인식 모델에 적용한 결과로서, '가전제품'이라는 객체에 대한 관련 도가 높으며 긍정적인/부정적인 속성을 갖는 단어인 '전력소모량', '탑재된 기능', 또는 '가격' 등을 객체에 대 한 주변 정보로 결정할 수 있다. 객체를 유사한 다른 객체와 구분하는데 가장 유용하게 사용될 수 있는 정보의 다른 예로, 객체의 추가 식 별 정보가 있다. 객체의 추가 식별 정보는 학습된 주변 정보 인식 모델을 이용하여 객체에 대한 정보를 입력하 면 다른 객체와 구분할 수 있는 주변 정보로서 획득될 수 있다. 이 때 추가 식별 정보는 다수 획득될 수 있으며 확률에 기반하여 다수의 추가 식별 정보 중 가장 유용하게 사용될 수 있는 정보가 선택된다. 그 경우, 객체가 표시되는 화면 중 객체의 주변 영역에 대한 이미지에서 다수의 주변 정보를 획득하고, 획득된 다수의 주변 정보 중 확률에 기반하여 높은 확률을 갖는 주변 정보가 추가 식별정보로 선택된다. 또한, 사용자에 의해 선택된 객체가 '사람의 얼굴'이미지 또는 '사람의 이름'에 대응하는 텍스트인 경우, 주변 정보 인식 모듈은 객체의 주변 정보를 주변 정보 인식 모델에 적용한 결과로서, '직업', '성별' 및 '나 이'를 객체의 추가 식별 정보로 결정할 수 있다. 또한, 사용자에 의해 선택된 객체가 '자동차'나 '제품'의 이미지인 경우, 주변 정보 인식 모듈은 객체의 주변 정보를 주변 정보 인식 모델에 적용한 결과로서, '제조사', '모델명' 및 '제원' 등을 객체의 추가 식별 정 보로 결정할 수 있다. 또한, 사용자에 의해 선택된 객체가 '건물'이나 '장소'의 이미지인 경우, 주변 정보 인식 모듈은 객체의 주변 정보를 주변 정보 인식 모델에 적용한 결과로서, '건물명 또는 장소명', '지역명 또는 나라명'및 '다른 랜 드마크' 를 객체의 추가 식별 정보로 결정할 수 있다. 다양한 실시예로, 주변 정보 인식 모델은 객체의 주변 정보가 객체의 종류를 고려하여 결정되도록 학습될 수 있 다. 이 때, 객체와 유사한 다른 객체를 구분하기 위하여 가장 유용하게 사용될 수 있는 정보를 결정하도록 주변 정보 인식 모델이 지도 학습될 수 있다. 또한, 객체의 종류에 적합한 상세 정보가 검색될 수 있도록 객체의 종류를 고려하여 객체의 주변 정보의 종류가 결정되도록 주변 정보 인식 모델이 지도 학습될 수 있다. 그 밖에, 객체가 포함된 문서나 이미지에 포함되는 주 변 정보의 빈도수 등에 기반한 비지도 학습을 통해 객체와 주변 정보와의 관련도가 학습될 수도 있다. 또한, 검색 결과에 대한 사용자의 피드백 등을 이용한 강화 학습을 통하여 주변 정보 인식 모델이 학습될 수 있다. 사용자 특성 인식 장치(D)는 전자 장치(A)로부터 안면 정보를 수신할 수 있다. 예로, 사용자 특성 인식 장치 (D)는 전자 장치(A)로부터 전자 장치(A)의 통신부를 통해 전자 장치(A)의 얼굴 검출 모듈이 검출한 사용자의 얼굴에 대한 안면 정보를 수신할 수 있다. 사용자 특성 인식 장치(D)는 수신된 안면 정보로부터 사용자의 특징을 결정하고, 사용자의 특징 정보를 저장할 수 있다. 사용자의 특징 정보는, 예로, 사용자의 나이, 성별 또는 표정 등을 포함할 수 있다. 특히, 사용자 특성 인식 장치(D)는 안면 정보를 학습된 얼굴 인식 모델의 입력 데이터로 사용하여 안면 정보와 관련된 사용자 특징 정보를 추정할 수도 있다. 학습된 얼굴 인식 모델은, 예로, 신경망 또는 딥러닝 기반의 인 식 모델이 될 수 있다. 부가 데이터 수집 장치(E)는 전자 장치(A)로부터, 예로, 전자 장치(A)의 사용자 프로파일, 전자 장치의 사용 이 력 정보 또는 사용자의 피드백을 수집할 수 있다. 사용자 프로파일은, 사용자가 전자 장치(A)에 기 등록한 사용자 정보로서, 예로, 사용자의 이름, 성별, 아이디, 선호 카테고리 및 생체 정보(예로, 키, 몸무게, 병력 등) 중 적어도 하나를 포함할 수 있다. 사용 이력 정보는, 예로, 사용자의 객체 검색 이력, 검색 결과에 대한 피드백 이력 또는 상품 구매 이력 등의 정보를 포함할 수 있 다. 사용자 피드백은, 예로, 객체와 연관된 검색 결과에 대한 사용자의 피드백 또는 객체 정보에 대한 사용자의 피드백 중 적어도 하나를 포함할 수 있다. 구체적으로, 객체와 연관된 검색 결과에 대한 사용자의 피드백은, 예 로 객체와 연관된 검색 결과에 대한 추가 선택 여부, 객체와 연관된 웹 사이트에 대한 사용자의 액세스 여부, 객체와 연관된 검색 결과에 대한 사용자의 저장, 공유 여부, 객체와 연관된 물품에 대한 사용자의 구매 여부 등 을 포함할 수 있다.객체 정보에 대한 사용자 피드백은, 예로, 객체 정보에 대한 사용자의 확인 여부 정보, 객체 정보에 대한 사용자의 수정 정보 또는 사용자의 객체 재선택 정보 등을 포함할 수 있다. 추천 장치(B)는 객체 정보, 컨텍스트 정보 및 컨텍스트 인식 정보 중 적어도 하나를 이용하여 객체와 연관된 검 색 결과를 제공할 수 있다. 예로, 추천 장치(B)는 객체 인식 장치(C)에서 수신한 객체 정보, 컨텍스트 정보 및 컨텍스트 인식 정보 중 적어도 하나를 이용할 수 있고, 또한, 사용자 특성 인식 장치(D) 및/또는 부가 데이터 수집 장치(E)에서 수신한 사용자의 특징 정보 및/또는 부가 데이터를 함께 이용할 수도 있다. 더욱 상세하게는, 추천 장치(B)는 별도의 검색 엔진에 수신한 객체 정보, 컨텍스트 정보 및 컨텍스트 인식 정보 를 이용한 검색을 요청할 수 있다. 추천 장치(B)는 검색 엔진으로부터 수신한 검색된 결과를 우선 순위를 적용 하여 전자 장치(A)로 제공할 수 있다. 예로, 추천 장치(B)는 사용자의 특징 정보 및/또는 부가 데이터를 이용하 여 검색 결과에 우선 순위를 적용하여 전자 장치(A)로 제공할 수 있다. 즉, 컨텍스트 정보는 객체와 연관된 검 색 결과를 획득하는데 이용될 수도 있고, 객체와 연관된 검색 결과에 우선 순위를 적용하여, 검색 결과를 우선 순위화하기 위하여 이용될 수도 있다. 검색 엔진은 검색 서비스 제공자가 사용/유지/관리하는 서버에서 동작하는 소프트웨어로, 외부 장치(예로, 추천 장치(B))로부터 객체 정보 및/또는 컨텍스트 정보를 검색어로서 수신하고, 검색 요청을 수신하면, 수신된 검색 어를 이용하여 미리 수집하여 저장하고 있는 데이터들 중 검색어와 관련된 데이터를 검색하여 외부 장치로 제공 할 수 있다. 추천 장치(B)가 객체 정보뿐만 아니라 객체의 컨텍스트 정보, 사용자의 특징 정보 및 부가 데이터 중 적어도 하 나를 이용하여 검색 결과를 제공함에 따라, 사용자 의도에 부합하는 검색 결과가 제공될 수 있다. 카메라는 영상을 촬영할 수 있다. 특히, 카메라는 컨텍스트 정보로서 사용자의 안면 이미지를 획득하 기 위해, 전자 장치(A)를 사용하는 사용자의 얼굴을 촬영하여 사용자의 얼굴을 포함하는 이미지를 획득할 수 있 다.도 3은, 다양한 실시예에 따른 전자 장치(A)를 포함하는 네트워크 시스템의 흐름도를 나타낸다. 먼저, 도 3에서, 전자 장치(A)의 사용자는 화면에 포함된 객체를 선택할 수 있다. 예로, 사용자는 미리 정 의된 액션 또는 사용자가 스스로 정의한 액션(예로, 롱 터치, 멀티 터치, 펜 액션 등)을 수행하여 일 객체를 선 택할 수 있다. 전자 장치(A)의 화면 캡쳐/좌표 수집 모듈은 사용자 입력에 응답하여, 객체를 포함하는 화면을 캡쳐하고, 디스플레이에서 사용자가 선택한 일 지점에 대응하는 좌표(예로, 터치 좌표)를 수집할 수 있다. 전자 장치(A)는 화면이 캡쳐된 캡쳐 이미지 및 수집된 좌표를 객체 인식 장치(C)에게 전송할 수 있다. 이 때, 객체 인식 장치(C)는 전자 장치(A)에 마련될 수도 있고, 또는 전자 장치(A)와 통신 연결하는 별도의 서버에 마련될 수도 있다. 객체 인식 장치(C)의 객체 영역 검출/분류 모듈은 수신된 캡쳐 이미지 및 선택 좌표에 기반하여, 캡쳐 이 미지에서 선택 좌표에 대응하는 객체 영역을 검출하고, 객체 영역을 분류할 수 있다. 이 때, 객체 범주는, 예로, 인물, 글자, 상품, 장소, 식물 또는 음식 중에 하나가 될 수 있으며, 전자 장치(A)가 제공하는 응용 서비 스가 따라 각각 다르게 정의될 수도 있다. 다양한 실시예로, 전자 장치(A)의 객체 영역 검출/분류 모듈이 수신된 캡쳐 이미지 및 선택 좌표에 기반하 여, 캡쳐 이미지에서 선택 좌표에 대응하는 객체 영역을 검출하고, 객체 영역을 분류할 수 있다. 이 경우, 전자 장치(A)는 객체 영역 및 객체 범주를 객체 인식 장치(C)에게 전송할 수도 있다. 다양한 실시예로, 전자 장치(A)의 객체 영역 검출 모듈이 객체 영역을 검출하고, 전자 장치(A)가 검출된 객체 영역을 객체 인식 장치(C)에 전송할 수 있다. 이 경우, 객체 인식 장치(C)의 객체 영역 분류 모듈이 객체 영역 의 객체 범주를 결정할 수도 있다. 객체 영역 검출/분류 모듈은 분류된 객체 범주를 고려하여 서로 다른 복수 개의 세부 객체 인식 모듈 중 일 세부 객체 인식 모듈로 객체 영역을 전달할 수 있다. 예로, 세부 객체 인식 모듈의 종류로는 얼굴 객체 인식 모듈, 상품 객체 인식 모듈, 글자 객체 인식 모듈, 장소 객체 인식 모듈 및 음식 객체 인식 모듈이 각각 존재할 수 있다. 객체 인식 장치(C)의 세부 객체 인식 모듈은 객체 영역 검출/분류 모듈에서 분류된 객체의 범주보다 더욱 세부적으로 객체 영역을 인식하여 객체 정보를 획득할 수 있다. 예로, 얼굴 객체 인식 모듈은, 객체 정보로서 인물의 이름을 획득할 수 있다. 또는, 상품 객체 인식 모듈은 객체 정보로서 상품의 아이디를 인식할 수 있다. 한편, 세부 객체 인식 모듈은 더욱 세부적인 분류를 수행하고, 분류된 세부 범주에 기반하여 객체 정보를 획득할 수도 있다. 예로, 상품 객체 인식 모듈은 상품 범주를 객체를 패션, 가방, 신발, 모자, 포장 상품 또는 신선 식품 등과 같은 세부 범주로 세부 분류를 수행할 수 있다. 그리고, 세부 객체 인식 모듈은 세부 범주 내에서 객체를 인식하여 객체 정보를 획득할 수 있다. 객체 정보가 획득되면, 객체 인식 장치(C)는 획득된 객체 정보를 컨텍스트 분석/추천 장치(G)로 전송할 수 있다 . 컨텍스트 분석/추천 장치(G)는 전술한 도 2의 추천 장치(B) 및 객체 인식 장치(C)의 컨텍스트 인식 모듈 중 적어도 하나에 대응될 수 있다. 또한, 컨텍스트 분석/추천 장치(G)는 객체의 선택에 따라 획득된 컨텍스트 정보를 획득할 수 있다. 예로, 컨텍스트 분석/추천 장치(G)는 전자 장치(A)로부터 전송된 객체의 선택에 따라 획득된 컨텍스트 정보로서, 객체 가 포함된 화면에서 객체의 주변에 위치한 주변 정보 및 전자 장치(A)에 구비된 카메라로 촬영된 촬영 정보 중 적어도 하나를 획득할 수 있다. 컨텍스트 분석/추천 장치(G)는 객체 정보 및 사용자의 객체의 선택 시에 획득한 컨텍스트 정보에 기반하여, 객 체와 연관된 검색 결과로서 추천 정보를 획득할 수 있다. 이 때, 컨텍스트 분석/추천 장치(G)는 객체 정보 및 컨텍스트 정보를 분석하여 최적의 추천 정보를 획득할 수 있다. 예로, 사용자가 선택한 객체의 객체 정보가 유명인의 이름이고, 컨텍스트 정보는 사용자가 선택한 객체의 주변 에 위치한 텍스트 정보일 수 있다. 이 경우, 컨텍스트 분석/추천 장치(G)는 텍스트 정보를 분석하고, 분석된 컨 텍스트 인식 정보가 콘서트에 관한 내용인 경우, 객체와 연관된 검색 결과로서 콘서트 티켓 정보를 획득할 수있다. 다른 예로, 객체 정보가 상품과 관련된 정보이고, 전자 장치(A)의 사용 이력 정보가 사용자가 지속적으로 상품 을 선택하여 가격 비교를 수행한 이력 정보인 경우, 컨텍스트 분석/추천 장치(G)는 객체와 연관된 검색 결과로 서 가격 비교 정보를 획득할 수 있다. 또 다른 예로, 객체 정보가 상품과 관련된 정보이고, 전자 장치(A)의 사용 이력 정보가 구매 이력 정보로서 사 용자의 선호 정보 또는 관심사 정보를 포함하는 경우, 컨텍스트 분석/추천 장치(G)는 과거에 사용자가 구매한 상품의 취향(예로, 옷의 스타일)에 부합하는 정보를 객체와 연관된 검색 결과로서 획득할 수 있다. 또 다른 예로, 컨텍스트 정보가 전자 장치(A)에 구비된 카메라로 촬영된 촬영 정보일 수 있다. 이 경우, 분석된 촬영 정보 사용자 특징 정보로서, 사용자의 나이, 외모, 성별, 표정 또는 기분을 나타내는 정보인 경우, 컨텍스 트 분석/추천 장치(G)는 사용자의 나이, 외모, 성별 또는 기분을 고려한 객체와 연관된 검색 결과를 획득할 수 있다. 또 다른 예로, 객체 정보가 상품 관련 정보이고, 컨텍스트 정보는 객체의 주변에 위치한 텍스트 정보일 수 있다. 이 때, 텍스트 정보에 가격 정보가 이미 포함되어 있는 경우, 컨텍스트 분석/추천 장치(G)는 사용자가 상 품에 대한 가격 정보 보다 상품의 스펙과 같은 상세 정보에 더욱 관심이 있는 것으로 판단하여, 상품과 관련된 스펙 정보를 획득할 수 있다. 다음으로, 컨텍스트 분석/추천 장치(G)는 추천 정보로서 객체와 연관된 검색 결과를 전자 장치(A)로 전송할 수 있다. 이 때, 추천 정보는, 사용자의 선택 의도에 부합하는 객체와 관련된 맞춤형 검색 결과를 포함할 수 있다. 전자 장치(A)는 수신한 추천 정보로서 객체와 연관된 검색 결과를 디스플레이 영역에 표시할 수 있다 . 도 4는, 다양한 실시예에 따른 전자 장치(A)를 포함하는 네트워크 시스템의 흐름도이다. 도 4에서, 전자 장치(A)가 사용자의 객체의 선택에 따라, 캡쳐 이미지 및 수집된 좌표를 객체 인식 장치(C)로 전송하는 동작 401 내지 동작 405는 도 3의 동작 301 내지 동작 305에 대응되어 중복되는 설명은 생략한다. 한편, 전자 장치(A)는 캡쳐 이미지 및 수집된 좌표뿐만 아니라 컨텍스트 정보를 객체 인식 장치(C)로 전송할 수 도 있다. 이 때, 컨텍스트 정보는, 예로, 사용자에 의하여 선택된 객체의 주변에 위치한 주변 정보(예로, 텍스 트 정보 또는 영상 정보) 등이 될 수 있다. 객체 인식 장치(C)는 객체 영역을 검출하고 분류할 수 있다. 또한, 객체 인식 장치(C)는 분류된 객체 범주 를 고려하여, 세부적으로 객체 영역을 인식할 수 있다. 이 때, 객체 인식 장치(C)는 동작 407에서 수신된 컨텍스트 정보를 활용하여 객체 영역을 인식함으로써 인식 결과를 개선할 수 있다. 객체 인식 장치(C)는 개선된 인식 결과로서 객체 정보를 획득하고, 획득된 객체 정보를 컨텍스트 분석/추천 장 치(G)로 전송할 수 있다. 컨텍스트 분석/추천 장치(G)가 수신한 객체 정보에 기반하여 전자 장치(A)로 객체와 연관된 검색 결과를 전송하 면, 전자 장치(A)는 검색 결과를 디스플레이 영역에 표시할 수 있다. 이에 대응하는 동작 417 내지 동작 421은 도 3의 동작 315 내지 동작 319에 대응되어 중복되는 설명은 생략한다. 도 5는, 다양한 실시예에 따른 전자 장치(A)를 포함하는 네트워크 시스템의 흐름도이다. 도 5에서, 객체 인식 장치(C)는 사용자가 선택한 객체에 대응하는 객체 영역의 인식 결과(예로, 객체 정보)를 전자 장치(A)로 전송할 수 있다. 이에 대한 응답으로, 전자 장치(A)는 인식 결과에 대한 사용자 피드백을 객체 인식 장치(C)로 전송할 수 있다 . 예로, 전자 장치(A)는 객체 인식 장치(C)에서 수신된 객체 영역의 인식 결과를 획득하여, 디스플레이 영역에 표 시할 수 있다. 사용자는 표시된 인식 결과에 대한 사용자 피드백을 제공할 수 있다. 예로, 사용자는 인식 결과 의 확인 여부를 묻는 팝업에 대한 응답 피드백을 입력할 수 있다.객체 인식 장치(C)가 학습된 객체 인식 모델을 이용하여 객체를 인식하는 경우, 수신한 응답 피드백을 이용하여 객체를 인식하는 객체 인식 모델을 업데이트할 수 있다. 이 경우, 인식 결과에 따른 사용자 피드백이 많아 질 수록 객체 인식 모델의 인식 기능은 계속하여 향상될 수 있다. 다양한 실시예로, 사용자 피드백이 없는 경우에도, 객체 인식 장치(C)은 전자 장치(A)가 객체 인식을 위하여 전 송한 캡쳐 이미지 또는 객체 영역을 이용하여 지도/비지도 학습 방식으로 객체 인식 모델을 학습시킬 수도 있다. 객체 인식 장치(C)는 객체 인식 모델의 출력 값인 인식 결과(예로, 객체 정보)를 컨텍스트 분석/추천 장치(G)로 전송할 수 있다. 또한, 전자 장치(A)는 컨텍스트 정보, 사용자 프로파일 및 사용 이력 정보 중 적어도 하나를 데이터 수집 장치 (E)로 전송할 수 있다(509~513). 사용 이력 정보는, 전자 장치(A)의 사용 이력 정보로서, 예로, 검색 결과에 대한 사용 이력 정보를 포함할 수 있다. 검색 결과에 대한 사용 이력 정보는, 예로, 검색 결과에 기반하여 상품을 구매한 구매 이력, 사용자의 검 색 결과 평가 이력 및 검색 결과를 탐색 또는 연결 링크를 선택하는 탐색 이력 중 적어도 하나를 포함할 수 있 다. 또한, 데이터 수집 장치(E)는 검색 결과에 대한 사용자 특징 정보를 수집할 수 있다. 이 때의 사용자 특징 정보는 검색 결과 제공 후 카메라로 촬영된 정보에 기반하여 분석된 정보로서 검색 결과에 대한 사용자의 표정 또는 기분을 나타내는 정보 등을 포함할 수 있다. 데이터 수집 장치(E)가 수집한 수집 데이터들(예로, 컨텍스트 정보, 사용자 프로파일, 사용 이력 정보 또는 사 용자의 특징 정보 등)은 컨텍스트 분석/추천 장치(G)로 전송될 수 있다. 데이터 수집 장치(E)는, 상기 데 이터들을 주기적(예로, 매시간 또는 매일)으로 컨텍스트 분석/추천 장치(G)로 전송할 수도 있고, 특정 이벤트 신호 발생 시(예로, 데이터 요청 시) 컨텍스트 분석/추천 장치(G)로 전송할 수도 있다. 컨텍스트 분석/추천 장치(G)는 동작 517에서 객체 인식 장치(C)로부터 수신한 객체 정보와 데이터 수집 장치 (E)로부터 수신한 수집 데이터들(예로, 컨텍스트 정보, 사용자 프로파일, 사용자 특징 정보 또는 사용자 사용 이력 등)에 기반하여, 사용자 의도에 부합하는 객체와 연관된 검색 결과를 획득할 수 있다. 이때, 컨텍스트/추천 장치(G)는 객체 인식 장치(C)로부터 수신한 객체 정보 및 데이터 수집 장치(E)로부터 수신 한 수집 데이터들 바탕으로 검색 엔진에 검색을 요청할 수 있다. 컨텍스트/추천 장치(G)는 검색 엔진으로부터 수신한 검색된 결과에 대해 우선 순위를 적용하여 전자 장치(A)로 제공할 수 있다. 예로, 컨텍스트/추천 장치 (G)는 검색 엔진을 통해 객체 정보를 이용하여 검색 결과를 획득하고, 사용 이력 정보(예로, 구매 이력 정보, 탐색 이력 정보, 인식 결과 만족도 정보 등)를 바탕으로 검색 결과에 우선 순위를 적용하여 전자 장치(A)에 제 공 또는 추천할 수 있다. 즉, 컨텍스트/추천 장치(G)는 사용 이력이 있는 검색 결과에 대해 우선순위를 높게 적 용하여 전자 장치(A)에 제공 또는 추천할 수 있다.또 다른 예로, 사용자의 전자 장치(A)의 사용 이력이 객체 영 역의 인식 결과와 유사한 인식 결과에 대한 사용 이력(예로, 구매 이력, 탐색 이력, 인식 결과 만족도 등)을 포 함하는 경우, 컨텍스트 분석/추천 장치(G)은 사용 이력을 이용하여, 인식 결과에 대한 사용자의 주 관심 분야를 판단할 수 있고, 판단 결과를 바탕으로 사용자 의도에 부합하는 맞춤형 정보를 전자 장치(A)에 제공 또는 추천 할 수 있다. 다음으로, 컨텍스트 분석/추천 장치(G)가 획득된 객체와 연관된 검색 결과를 전자 장치(A)로 전송하면, 전 자 장치(A)는 수신한 검색 결과를 디스플레이 영역에 표시할 수 있다. 도 6은, 다양한 일 실시예에 따른 객체와 연관된 검색 결과를 제공하는 과정을 도식화한 도면이다. 먼저, 도 6의 (a)에서, 전자 장치(A)는 터치 액션 또는 펜 액션을 이용하여 화면에 포함된 객체를 선택하는 사 용자 입력을 감지할 수 있다. 사용자 입력에 응답하여, 도 6의 (b)와 같이, 전자 장치(A)는 객체를 포함하는 화 면을 캡쳐하여 캡쳐 이미지를 생성할 수 있다. 이 때, 전자 장치(A)는 백그라운드 쓰레드(background thread)를 통하여 화면 캡쳐를 수행할 수 있다. 도 6의 (c)에서, 전자 장치(A)는 캡쳐 이미지에서 사용자가 선택에 대응하는 터치 좌표에 기반하여 객체 영역을 검출할 수 있다. 그리고, 전자 장치(A)는 객체 영역을 인식하여 일 객체 범주로 분류할 수 있 다. 그리고, 전자 장치(A)는 세부 객체 인식을 위하여 분류된 범주와 관련된 세부 객체 인식 장치(H)로 객체 영역을 전송할 수 있다. 세부 객체 인식 장치(H)는, 예로, 도 2의 세부 객체 인식 모듈에 대응될 수 있다. 또한, 전자 장치(A)는 전자 장치(A)에서 촬영된 촬영 정보를 사용자 특성 인식 장치(D)로 전송할 수 있다. 사용 자 특성 인식 장치(D)는 수신된 촬영 정보에 기반하여 사용자 특성을 인식할 수 있다. 예로, 사용자 특성 인식 장치(D)는 사용자의 나이, 성별 또는 표정 등을 인식할 수 있다. 세부 객체 인식 장치(H)는 객체 영역에 대응하는 객체를 인식하여 객체 정보(예로, 객체명, 객체의 식별자, 객 체의 상세 레벨)를 획득할 수 있다. 그리고, 세부 객체 인식 장치(H)는 객체 정보를 상세 정보 획득 장치(I)에 전송할 수 있다. 상세 정보 획득 장치(I)는 수신한 객체 정보에 기반하여, 객체 정보와 관련된 1차 검색 결과로서 객체에 관한 상세 정보를 획득할 수 있다. 객체에 관한 상세 정보는 객체의 일반 정보 또는 구매 정보 등을 포함할 수 있다. 일반 정보는, 예로, 객체가 인물인 경우, 인물의 이름, 나이, 성명, 성별 또는 경력 등의 정보를 포함할 수 있 다. 또한, 구매 정보는, 인물과 관련된 상품(예로, 콘서트 티켓, 도서, 음반, VOD 등) 또는 이를 구매할 수 있 는 정보를 포함할 수 있다. 세부 객체 인식 장치(H)는 상세 정보 획득 장치(I)로부터 객체 정보에 관한 상세 정보를 획득하고, 상기 상세 정보를 컨텍스트 분석/추천 장치(G)로 전송할 수 있다. 또는, 상세 정보 획득 장치(I)가 객체 정보에 관한 상세 정보를 직접 컨텍스트 분석/추천 장치(G)로 전송할 수도 있다. 컨텍스트 분석/추천 장치(G)는 세부 객체 인식 장치(H)에서 수신한 객체 정보 또는 상세 정보, 사용자 특성 인 식 장치(D)에서 수신한 사용자 특성 정보, 전자 장치(A)에서 수신한 사용자 프로파일, 컨텍스트 정보 중 적어도 하나를 이용하여 2차 검색 결과로서 추천 정보를 획득할 수 있다. 예로, 컨텍스트 분석/추천 장치(G)는 사용자 가 선택한 객체의 주변에 위치한 주변 정보(예로, 텍스트 정보, 영상 정보), 촬영 정보에 기반하여 인식된 사용 자 특성 정보(예로, 사용자의 나이, 성별 기분), 및 전자 장치(A)의 사용 이력 데이터를 이용하여 분석된 사용자의 선호 및 관심 정보에 기반하여 최적의 추천 정보를 획득할 수 있다. 이때, 컨텍스트/추천 장치(G)는 세부 객체 인식 장치(H)로부터 수신한 객체 정보 및 상세 정보, 사용자 특성 인 식 장치(D)에서 수신한 사용자 특성 정보, 전자 장치(A)에서 수신한 사용자 프로파일, 컨텍스트 정보 중 적어도 하나를 바탕으로 검색 엔진에 검색을 요청할 수 있다. 컨텍스트/추천 장치(G)는 검색 엔진으로부터 수신한 검색 된 결과에 대해 사용자가 선호하는 검색 결과를 전자 장치(A)로 제공할 수 있다. 예로, 컨텍스트/추천 장치(G) 는 검색 엔진을 통해 객체 정보 및 상세 정보를 이용하여 검색 결과를 획득하고, 사용자 특성 정보(예를 들어, 사용자 성별, 나이, 기분, 표정 등), 사용자 프로파일(예를 들어, 사용자 선호 분야, 사용자 검색 이력, 사용자 구매 이력 등) 및 컨텍스트 정보를 바탕으로 검색 결과 중 사용자가 선호하는 검색 결과를 판단하여 전자 장치 (A)에 제공 또는 추천할 수 있다. 또는, 컨텍스트/추천 장치(G)는 사용자 특성 정보, 사용자 프로파일 및 컨텍 스트 정보 중 적어도 하나를 바탕으로 검색 결과의 우선순위를 판단하여 전자 장치(A)에 제공 또는 추천할 수 있다. 다음으로, 컨텍스트 분석/추천 장치(G)는 획득된 추천 정보인 객체와 연관된 검색 결과를 전자 장치(A)로 전송 할 수 있다. 전자 장치(A)는 수신한 객체와 연관된 검색 결과를 화면을 통하여 사용자에게 제공할 수 있다. 도 7은 다양한 실시예에 따른 주변 정보를 이용하여 객체와 연관된 검색 결과를 제공하는 도면이다. 도 7에서, 전자 장치(A)는 선택된 객체 및 객체 주변의 주변 정보(예로, 텍스트 또는 이미지 등)에 기반하여, 객체와 연관된 검색 결과를 제공할 수 있다. 예로, 전자 장치(A)에 이미지와 텍스트가 포함된 신문 기사가 표시될 수 있다. 이 경우, 사용자가 객체로서 이미지에 포함된 특정 가수의 얼굴을 선택하면, 객체 인식 장치(C)는 이미지에서 선택된 얼굴을 분석하여 얼굴에 대응하는 특정 가수의 이름을 객체 정보로서 획득할 수 있다. 또한, 객체 인식 장치(C)는 이미지가 포함된 신문 기사의 텍스트를 분석한 결과, 신문 기사의 전체 내용 중 '연 말 콘서트'라는 키워드를 특정 가수와 관련된 컨텍스트 인식 정보로서 획득할 수 있다. 이 경우, 컨텍스트 분석/추천 장치(G)는 객체 인식 장치(C)로부터 수신한 '특정 가수의 이름' 및 '연말 콘서트'각각을 쿼리로서 이용할 수 있다. 예로, 컨텍스트 분석/추천 장치(G)는 상기 쿼리들을 이용하여 검색 엔진을 대상으로 관련 정보에 대한 검색을 요청할 수 있다. 그리고, 검색 엔진으로부터 상기 쿼리에 기초한 검색 결과를 수신할 수 있다. 객 체 인식 장치(C)는 수신된 검색 결과를 사용자의 특성 정보 및 부가 데이터 등을 이용하여 선별적으로 제공할 수도 있다. 다양한 실시예로, 객체 인식 장치(C)는 객체와 관련된 컨텍스트 인식 정보를 도출하기 위해서, 객체가 포함된 문서의 전체 내용 또는 객체의 주변에 위치한 문장으로부터 객체와 관련도가 가장 높은 단어 등을 추론할 수 있 다. 또한, 객체 인식 장치(C)는 객체가 포함된 이미지가 촬영된 것으로 추정되는 장소 또는 시간 등을 추론할 수 있다. 이 경우, 객체 인식 장치(C)는 객체의 주변 정보를 학습된 주변 정보 인식 모델의 입력 데이터로 사용 하여 객체와 관련된 컨텍스트 인식 정보를 추론할 수 있다. 학습된 주변 정보 인식 모델은, 예로, 신경망 또는 딥러닝 기반의 인식 모델이 될 수 있다. 도 8은 다양한 실시예에 따른 객체를 선택하는 전자 장치의 사용도들을 나타낸다. 예로, 도 8의 (a)와 같이, 사용자는 객체를 선택하기 위하여 객체가 표시된 디스플레이 영역의 일 지점을 터치(예로, 롱터치 또는 더블터치)할 수 있다. 또는, 도 8의 (b)와 같이, 사용자는 전자 펜을 이용하여 객체가 표시된 디스플레이 영역의 일부를 드로잉할 수 있다. 또는, 도 8의 (c-1)과 같이, 전자 장치(A)는 화면에 객체와 연관된 검색 결과를 제공하는 UI(예로, 아이 콘)을 표시할 수 있다. 예로, 사용자가 전자 장치(A)에서 전자 펜을 인출하면, 전자 장치(A)는, 전자 펜의 인출에 따라 발생된 이벤트에 기반하여, 객체와 연관된 검색 결과를 제공하는 UI를 표시할 수 있다. 다른 예로, 디스플레이 영역의 가장자리(예로, 엣지 영역)에서 중심으로 드래그하는 사용자 입력에 따라, 전자 장치 (A)는 객체와 연관된 검색 결과를 제공하는 UI를 표시할 수 있다. 이러한 상황에서, 사용자가 상기 UI를 선택하면, 전자 장치(A)는 UI 표시 전에 디스플레이 영역에 표시 중 이던, 객체를 포함하는 화면을 캡쳐하고, 도 8의 (c-1)과 같이, 전자 장치(A)는 화면의 캡쳐 결과로서 캡쳐 이 미지를 표시할 수 있다. 캡쳐 이미지가 표시되면, 예로, 사용자는 캡쳐 이미지의 일 영역을 전자 펜으로 드로잉하여 객체를 선택할 수 있다. 도 9a 및 도 9b는, 다양한 실시예에 따른 객체와 연관된 검색 결과를 제공하는 도면이다. 일 예로, 도 9a의 (a)에서, 사용자가 객체인 건축물을 선택하면, 전자 장치(A)는 객체와 연관된 검색 결과 로서 건축물에 대한 상세 정보를 디스플레이 영역의 일부에 표시할 수 있다. 이 경우, 건축물이 숙박이 가 능한 건축물인 경우, 전자 장치(A)는 검색 결과로서 객실 예약 등이 가능한 UI 엘리먼트를 함께 표시할 수 도 있다. 다른 예로, 도 9a의 (b)에서, 사용자가 유명인을 선택하면, 전자 장치(A)는 검색 결과로서 유명인에 대한 프로필 정보를 디스플레이 영역의 일부에 표시할 수 있다. 이 때, 전자 장치(A)는 선택된 객체와 관련된 다른 검색 결과를 제공할 수 있는 UI 엘리먼트를 함께 표시할 수도 있다. 예로, 객체가 유명인인 경우, 전자 장치(A)는 검색 결과로서 사용자 프로필, 방송 진행 정보, 영화 출연 정보, 앨범 정보 또는 공연 정보 등과 같은 검색 카테고리를 선택할 수 있는 UI 엘리먼트를 함께 표시할 수 있다. 이 경우, 방송 진행 정보를 선택하는 사용자 입력이 수신되면, 전자 장치(A)는 방송 진행 정보와 관련된 검색 결과 를 디스플레이 영역의 일부에 표시할 수 있다. 한편, 상기 각각의 카테고리 정보들을 선택할 수 있는 UI 엘리먼트는 검색 카테고리들의 우선 순위에 따라 순차 적인 순서로 표시될 수도 있다. 예로, 객체와 연관된 검색 결과로서, 사용자의 의도에 부합하는 검색 카테고리 들이 여러 개인 경우, 사용자의 의도에 가장 적합한 우선 순위에 따라 각각의 카테고리 정보들을 선택할 수 있 는 UI 엘리먼트들이 순차적으로 나열될 수 있다. 다른 예로, 도 9a의 (c)에서, 사용자는 객체로서 텍스트를 선택할 수도 있다. 예로, 텍스트가 영화 제목인 경우, 전자 장치(A)는 텍스트와 관련된 검색 결과로서 영화와 관련된 내용, 평점, 썸네일 이 미지, 티저 영상 및 영화 예약 UI 엘리먼트 중 적어도 하나를 디스플레이 영역의 일부에 표시할 수 있다. 또 다른 예로, 도 9b의 (d)에서, 사용자는 전자 펜을 이용하여 대각선으로 드래그하여 객체를 선택할 수 있다. 이 경우, 전자 장치(A)는 대각선 드래그에 대응하는 객체 영역을 하이라이트하여 표시할 수 있다. 그리고, 전자 장치(A)는 객체와 연관된 검색 결과로서 객체에 대한 상세 정보를 디스플레이 영역의 일부에 표시할 수 있다. 예로, 전자 장치(A)를 객체를 상품으로 간주하여, 객체와 관련된 상품 가격, 판매 처, 유사 상품 등을 표시할 수 있다. 또 다른, 예로, 도 9b의 (e)와 같이, 사용자는 전자 펜을 이용하여 객체의 경계를 따라 드로잉할 수 있다. 이 경우, 전자 장치(A)는 드로잉 입력에 대응하는 세그멘테이션된 객체 영역을 하이라이트하여 표시할 수 있다. 그리고, 전자 장치(A)는 객체와 연관된 검색 결과로서 객체에 대한 상세 정보를 디스플레이 영역의 일부에 표시할 수 있다. 도 10은, 다양한 실시예에 따른 객체와 연관된 검색 결과를 검색 중인 것을 나타내는 도면이다. 전술한 도 9b의 (d)와 같이, 사용자가 객체를 선택하는 경우, 도 10의 (a)와 같이 전자 장치(A)는 트랜지션 효 과로서 선택된 객체와 함께, 객체와 연관된 검색 결과를 검색 중인 것을 나타내는 메시지(예로, 검색 중 입니다)를 화면에 표시할 수 있다. 또는, 전술한 도 9b의 (e)와 같이, 사용자가 객체를 선택하는 경우, 도 10의 (b)와 같이 전자 장치(A)는 선택된 객체와 함께, 객체와 연관된 검색 결과를 검색 중인 것을 나타내는 메시지 를 화면에 표시할 수 있다. 이와 같이, 사용자와 선택한 객체에 대응하는 객체 영역 및 검색 상태 메시지를 표시하는 경우, 사용자에게는 객체와 연관된 검색 결과를 검색하기까지의 시간이 짧게 느껴지는 효과를 제공할 수 있다. 도 11은, 다양한 실시예에 따른 객체와 연관된 검색 결과를 제공하는 도면이다. 일 예로, 도 11의 (a)에서, 사용자는 객체가 포함된 화면을 캡쳐하는 사용자 입력을 수행할 수 있 다. 사용자 입력에 따라, 도 11의 (b)에서, 전자 장치(A)는 캡쳐 이미지를 생성하고, 생성된 캡쳐 이미지 을 대상으로 텍스트 인식(예로, OCR(optical character reader)) 또는 영상 인식을 수행하여 텍스트 및 객체 영역을 검출할 수 있다. 그리고, 전자 장치(A)는 검출된 객체 영역 및 텍스트를 이용하여 객체와 연관된 검색 결과를 제공할 수 있다. 예로, 도 11의 (a)에서, 화면이 객체가 포함된 웹 페이지인 경우, 전자 장치(A)는 텍스트 인식 및 영상 인식을 기반으로 캡쳐된 웹 페이지를 분석하여 객체 영역 및 텍스트를 검출하고, 텍스트 를 컨텍스트 정보로서 이용하여, 객체와 연관된 검색 결과인 상품 정보을 디스플레이 영역에 표시 할 수 있다. 상품 정보에는, 예로, 객체와 연관된 상품을 구매할 수 있는 UI 엘리먼트(미도시)를 포함할 수도 있다. 이 경우, UI 엘리먼트의 선택에 응답하여, 전자 장치(A)는 상품 구매 화면을 디스플레이 영역에 표 시할 수도 있다. 도 12는, 다양한 실시예에 따른 촬영 정보를 이용하여 객체와 연관된 검색 결과를 제공하는 도면이다. 도 12의 (a)에서, 사용자가 객체를 선택하는 경우, 전자 장치(A)는 카메라(예로, 전면 카메라)를 이용하 여 객체를 선택하는 사용자를 촬영할 수 있다. 전자 장치(A)는 사용자에 촬영에 따라 생성된 촬영 정보를 이용하여 사용자의 특징 정보를 획득할 수 있다. 예 로, 전자 장치(A)는 촬영 정보에 기반하여 사용자의 성별, 나이, 표정, 기분 등을 인식할 수 있다. 다음으로, 전자 장치(A)는 선택된 객체에 대응하는 객체 영역 및 사용자의 특징 정보를 이용하여 객체와 연관된 검색 결과 를 획득할 수 있다. 그리고, 도 12의 (b)와 같이 디스플레이 영역에 객체와 관련된 검색 결과를 표시할 수 있다. 이에 따라, 사용자의 특성 및 상태를 고려한 검색 결과가 제공되어 전자 장치(A)를 이용하는 사용자 만족도가 증가할 수 있다. 도 13은, 다양한 실시예에 따른 객체와 연관된 검색 결과를 제공하는 도면이다. 도 13에서, 전자 장치(A)가 본 문서에 따른 객체와 연관된 검색 결과를 제공하는 경우, 팝업 화면에 검색 결과 를 포함하여 제공할 수도 있다. 이 경우, 도 13의 (a)와 같이, 팝업 화면이 디스플레이 영역의 일 측으로부터 점차적으로 나타나면서, 도 13의 (b)와 같이, 미리 정의된 디스플레이 영역의 일부에 표시될 수도 있다. 이 때, 팝업 화면은 객체를 오버랩하지 않도록 디스플레이 영역에 표시될 수 있다. 이 경우, 팝업 화면의 표시 위치는 선택된 객체의 위치에 따라 결정되며, 팝업 화면의 크기는 객체의 화 면 상의 크기와 전체 화면에서 객체를 제외한 화면의 크기에 따라 결정될 수 있다. 또는, 팝업 화면의 배경 부 분은 투명하게 표시되어 객체를 포함하는 화면이 보이도록 제공할 수 있다. 검색 결과를 화면에 표시하는 방법(미도시)으로는, 선택된 객체와 오버랩 되지 않도록 객체를 포함하는 화면의 일부 영역에 검색 결과가 표시될 수 있다. 이 경우 객체와 검색 결과가 오버랩 되지 않도록 하기 위해서 객체의 표시 위치를 변경할 수 있다. 예를 들어, 검색 결과가 화면의 아래 영역에 표시되는 경우 객체는 화면 위 영역 에 표시되도록 객체의 표시 위치가 변경되고, 검색 결과가 화면의 위 영역에 표시되는 경우 객체는 화면의 아래 영역에 표시되도록 객체의 표시 위치가 변경될 수 있다. 도 14는, 다양한 실시예에 따른 객체와 연관된 복수 개의 검색 결과들을 제공하는 도면이다. 도 14에서, 전자 장치(A)는 검색 카테고리 별로 본 문서에 따라 객체와 연관된 검색 결과를 획득할 수 있다. 예 로, 사용자가 선택한 객체가 호텔 건물인 경우, 검색 카테고리는 '호텔 정보', '호텔 방문기', '호텔 위치' 등 을 포함할 수 있다. 이 경우, 전자 장치(A)는 사용자 입력에 따라, 사용자 의도에 맞는 우선 순위에 기반하여 상기 검색 카테고리에 해당하는 검색 결과들을 순차적으로 제공할 수 있다. 예로, 도 14의 (a)에서, 전자 장치(A)는 객체와 연관된 검색 결과로서 최상위 순위의 검색 결과(예로, 호텔 정 보)를 표시할 수 있다. 이 때, 사용자 입력(예로, 스와이프 입력)이 수신되면, 도 14의 (b)와 같이, 전자 장치(A)는 차순위의 검색 결과(예로, 호텔 방문기)를 표시할 수 있다. 계속하여, 사용자 입력이 수신되면, 도 14의 (c)와 같이, 전자 장치(A)는 다음 순위의 검색 결과(예로, 호텔 위 치)를 표시할 수 있다. 그리고, 추가 사용자 입력이 수신되면, 도 14의 (d)와 같이, 전자 장치(A)는 정보 (예로, 호텔 이용 문의)를 전송할 수 있는 입력창을 표시할 수 있다. 이 경우, 사용자가 입력창에 문의 사항을 작성하고 보내기 버튼을 선택하면, 전자 장치(A)는 입력된 문의 사항을 객체와 관련된 외부 장치(예로, 관리자 메일 서버)로 전송할 수 있다. 도 15는, 다양한 실시예에 따른 객체와 연관된 검색 결과를 제공하는 도면이다. 도 15의 (a)에서, 전자 장치(A)는 본 문서에 따라 획득된 객체와 관련된 검색 결과를 디스플레이 영역에 표시할 수 있다. 이 때, 객체와 관련된 검색 결과가 복수 개의 검색 카테고리 중 일 카테고리와 연관된 결과인 경우, 디스플레이 영역의 일부에는 다른 복수 개의 검색 카테고리들 각각의 식별 정보들(1502~1505)이 표시될 수 있다. 이 때, 복수 개의 식별 정보들(1502~1505) 중 일 식별 정보를 선택하는 사용자 입력에 응답하여, 도 15의 (b)와 같이, 전자 장치(A)는 선택된 카테고리와 관련된 검색 결과를 디스플레이 영역에 표시할 수 있다. 이 때, 디스플레이 영역의 일부에는 선택된 카테고리를 제외한 다른 카테고리들의 식별 정보들 (1502,1504,1505,1506)이 표시될 수 있다. 도 16a 내지 도 16c는 다양한 실시예에 따른, 다양한 어플리케이션이 실행되는 동안 사용자에 의해 선택된 객체 와 연관된 검색 결과를 제공하는 도면이다. 어플리케이션이 실행되는 동안 어플리케이션의 실행 화면에 포함된 객체를 선택하는 기 설정된 사용자 입력(예 를 들어, 롱프레스 터치, 더블 탭 터치, 가로 드래그 터치, 강한 압력의 터치, 폐곡선으로 객체가 표시된 영역 을 드로잉하는 입력 등)이 감지되면, 전자 장치(A)는 현재 표시되는 어플리케이션의 실행 화면을 캡쳐하고, 선 택된 객체를 식별하여 관련 정보를 검색할 수 있다. 또는 어플리케이션이 실행되는 동안 어플리케이션의 실행화면에 포함된 특정 아이콘 또는 전자 장치(A)에 구비된 특정 버튼을 선택하고 객체를 선택하는 사용자 입력이 감지되면, 전자 장치(A)는 현재 표시되는 어플리케이션의 실행 화면을 캡쳐하고, 선택된 객체를 식별하여 관련 정보를 검색할 수 있다. 이때, 전자 장치(A)는 어플리케이션의 실행 화면 상에서 아래에서 위 방향으로 선택된 객체에 대한 검색 결과를 포함하는 화면(예를 들어, 팝업 화면)을 제공할 수 있다. 그리고, 제공되는 검색 결과는 복수의 검색 카테고리 별로 획득될 수 있으며, 사용자 특성 정보(예를 들어, 사 용자 나이, 성별, 기분 등) 및 사용자 프로파일(예를 들어, 사용자 검색 이력, 사용자 구매 이력, 피드백 정보 등)을 정보를 바탕으로 복수의 검색 카테고리에 대한 우선 순위가 결정될 수 있다. 그리고, 검색 결과는 높은 우선 순위를 가지는 검색 카테고리에 대해서는 상세 정보를 제공하고, 다른 검색 카 테고리에 대해서는 메뉴로 제공할 수 있다. 예를 들어, 선택된 객체가 \"호텔\"인 경우, 우선 순위가 높은 검색 카테고리인 \"호텔 후기\"에 대해서는 상세 정보를 제공하고, \"호텔 예약\", \"호텔 가는 길\", \"호텔 주변 관광지\" 등과 같은 검색 카테고리에 대해서는 메뉴(또는 아이콘)으로 검색 결과를 제공할 수 있다. 그리고, 전자 장치(A)는 복수의 검색 카테고리에 대한 사용자 선택 및 사용자 감정 상태를 바탕으로 피드백 정 보를 생성하여 사용자 프로파일을 업데이트할 수 있다. 본 발명의 일 실시예로, 웹 브라우저 어플리케이션이 실행되는 동안, 전자 장치(A)는 사용자 입력에 따라 웹 브 라우저 어플리케이션의 실행 화면에 표시된 객체를 선택하는 사용자 입력을 수신할 수 있다. 이때, 사용자 입력 은 표시 중인 객체를 기설정된 시간 이상 터치하는 롱프레스 터치 입력, 표시 중인 객체를 복수회 터치하는 더 블 탭 터치 입력, 표시 중인 객체를 포함하는 영역을 가로 방향으로 드래그하는 드래드 입력, 표시 중인 객체를 기설정된 이상의 압력으로 터치하는 터치 입력, 표시 중인 객체를 포함하는 영역을 폐곡선으로 드로잉하는 터치 입력이 포함될 수 있다. 또한, 전자 장치(A)는 도 16a의 (a)에 도시된 바와 같이, 웹 브라우저 어플리케이션의 실행 화면 상에 검색 버튼을 표시할 수 있으며, 사용자에 의해 검색 버튼이 선택된 후 객체를 선택 하는 사용자 입력을 수신할 수 있다. 또한, 전자 장치(A)는 특정 버튼(예를 들어, 인공지능 에이전트를 실행하 기 위한 버튼 등)이 선택된 후 객체를 선택하는 사용자 입력을 수신할 수 있다. 상술한 바와 같은 사용자 입력이 수신되면, 전자 장치(A)는 표시중인 웹 브라우징 어플리케이션의 실행 화면을 캡쳐하여 캡쳐 이미지를 생성할 수 있다. 그리고, 전자 장치(A)는 캡쳐 이미지 중 사용자에 의해 선택된 객체에 대한 객체 정보를 바탕으로 선택된 객체 에 대한 검색 결과를 획득할 수 있다. 이때, 전자 장치(A)는 도 16a의 (b),(c)와 같이, 선택된 객체에 대한 검 색 결과를 포함하는 팝업 화면을 아래 방향에서 윗 방향으로 이동하여 제공할 수 있다. 그리고, 전자 장 치(A)는 도 16a의 (c)에 도시된 바와 같이, 웹 브라우징 어플리케이션의 실행 화면 상에 팝업 화면을 포 함할 수 있다. 즉, 전자 장치(A)는 웹 브라우징 어플리케이션의 실행 화면을 디스플레이 상에 제거할 수 있다. 특히, 팝업 화면 상에는 복수의 카테고리(예를 들어, 구매 정보 카테고리, 관련 뉴스 카테고리, 후기 카 테고리 등)가 포함될 수 있다. 이때, 사용자 프로파일 정보를 바탕으로 우선 순위가 높게 결정된 구매 정보 카 테고리는 상세 정보가 제공되며, 우선 순위가 낮게 결정된 관련 뉴스 카테고리, 후기 카테고리는 관련 아이콘만 제공될 수 있다. 전자 장치(A)는 복수의 카테고리에 대한 사용자 선택 결과를 반영하여 사용자 피드백 정보를 업데이트할 수 있 다. 또한, 전자 장치(A)는 사용자 음성을 함께 이용하여 선택된 객체에 대한 검색 결과를 획득할 수 있다. 예를 들 어, 객체를 선택하는 동안 \"어디서 구매할 수 있어\"라는 사용자 음성이 입력된 경우, 전자 장치(A)는 선택된 객 체에 대한 검색 카테고리로 \"구매 정보 카테고리\"를 결정하고, 결정된 객체 정보 및 검색 카테고리를 바탕으로 검색 결과를 제공할 수 있다. 본 발명의 다른 실시예로, 갤러리 어플리케이션이 실행되는 동안, 전자 장치(A)는 도 16b의 (a)에 도시된 바와 같이, 사용자 입력에 따라 갤러리 어플리케이션의 실행 화면에 표시된 객체를 선택하는 사용자 입력을 수신할 수 있다. 이때, 사용자 입력은 도 16a에서 설명한 바와 같으므로, 중복되는 설명은 생략한다. 사용자 입력이 수신되면, 전자 장치(A)는 표시중인 갤러리 어플리케이션의 실행 화면을 캡쳐하여 캡쳐 이미지를 생성할 수 있다. 그리고, 전자 장치(A)는 캡쳐 이미지 중 사용자에 의해 선택된 객체에 대한 객체 정보를 바탕으로 선택된 객체 에 대한 검색 결과를 획득할 수 있다. 이때, 선택된 객체가 텍스트를 포함하는 경우, 전자 장치(A)는 OCR을 이 용하여 객체 내에 포함된 텍스트를 인식하고, 인식된 텍스트를 객체 정보로 이용할 수 있다. 또는 전자 장치 (A)는 갤러리 어플리케이션의 실행 화면 상에 포함된 이미지의 메타 데이터(예를 들어, 촬영 위치 정보, 촬영 시간 정보 등)를 컨텍스트 정보로 이용할 수 있다. 예를 들어, 선택된 객체가 에펠 탑인 경우, 전자 장치(A)는 이미지의 메타 데이터에 포함된 위치 정보가 \"프랑스, 파리\"임을 이용하여 사용자에 의해 선택된 객체가 에펠 탑임을 인식할 수 있다. 전자 장치(A)는 선택된 객체에 대한 검색 결과를 포함하는 팝업 화면을 아래 방향에서 윗 방향으로 이동 하여, 도 16b의 (b) 와 같이, 실행 화면의 하단 영역에 팝업 화면을 제공할 수 있다. 특히, 팝업 화면 상에는 복수의 카테고리(예를 들어, 상세 정보 카테고리, 주변 정보 카테고리, 관련 이 미지 카테고리 등)가 포함될 수 있다. 이때, 사용자 프로파일 정보를 바탕으로 우선 순위가 높게 결정된 상세 정보 카테고리는 상세 정보가 제공되며, 우선 순위가 낮게 결정된 관련 주변 정보 카테고리, 관련 이미지 카테 고리는 아이콘만 제공될 수 있다. 전자 장치(A)는 복수의 카테고리에 대한 사용자 선택 결과를 반영하여 사용자 피드백 정보를 업데이트할 수 있 다. 본 발명의 또 다른 실시예로, 카메라 어플리케이션이 실행되는 동안, 전자 장치(A)는 사용자 입력에 따라 갤러 리 어플리케이션의 실행 화면에 표시된 객체를 선택하는 사용자 입력을 수신할 수 있다. 이때, 사용자 입력은 도 16a에서 설명한 바와 같으므로, 중복되는 설명은 생략한다. 또한, 전자 장치(A)는 도 16c의 (a)에 도시된 바와 같이, 카메라 어플리케이션의 실행 화면 상에 AR 기능 버튼 을 표시할 수 있으며, 사용자에 의해 AR 기능 버튼이 선택된 후 도 16c의 (b)에 도시된 바와 같이, 객체를 선택하는 사용자 입력을 수신할 수 있다. 사용자 입력이 수신되면, 전자 장치(A)는 표시중인 카메라 어플리케이션의 실행 화면을 캡쳐하여 캡쳐 이미지를 생성할 수 있다. 다만, 전자 장치(A)는 별도로 화면을 캡쳐하지 않고, 촬영된 이미지 중 객체를 포함하는 이미 지를 이용할 수 있다. 그리고, 전자 장치(A)는 캡쳐 이미지 중 사용자에 의해 선택된 객체에 대한 객체 정보를 바탕으로 선택된 객체 에 대한 검색 결과를 획득할 수 있다. 이때, 선택된 객체가 텍스트를 포함하는 경우, 전자 장치(A)는 OCR을 이 용하여 객체 내에 포함된 텍스트를 인식하고, 인식된 텍스트를 객체 정보로 이용할 수 있다. 또는 전자 장치 (A)는 촬영 이미지의 메타 데이터(예를 들어, 촬영 위치 정보, 촬영 시간 정보 등)를 컨텍스트 정보로 이용할 수 있다. 전자 장치(A)는 선택된 객체에 대한 검색 결과를 포함하는 팝업 화면을 아래 방향에서 윗 방향으로 이동 하여, 도 16c의 (c) 와 같이, 실행 화면의 하단 영역에 팝업 화면을 제공할 수 있다. 특히, 팝업 화면 상에는 복수의 카테고리(예를 들어, 상세 정보 카테고리, 주변 정보 카테고리, 관련 이 미지 카테고리 등)가 포함될 수 있다. 이때, 사용자 프로파일 정보를 바탕으로 우선 순위가 높게 결정된 상세 정보 카테고리는 상세 정보가 제공되며, 우선 순위가 낮게 결정된 관련 주변 정보 카테고리, 관련 이미지 카테 고리는 아이콘만 제공될 수 있다. 전자 장치(A)는 복수의 카테고리에 대한 사용자 선택 결과를 반영하여 사용자 피드백 정보를 업데이트할 수 있 다. 도 17은, 다양한 실시예에 따른 외부 장치와 연동하여 객체와 연관된 검색 결과를 제공하는 흐름도이다. 먼저, 전자 장치(A)는 객체를 포함하는 화면을 표시할 수 있다. 이 경우, 객체를 선택하는 사용자 입력이 감지되면(1702-Y), 전자 장치(A)는 객체를 포함하는 화면을 캡쳐하여 캡쳐 이미지를 생성할 수 있다. 이 때, 캡쳐 이미지는 객체를 포함하는 화면을 대신하여 디스플레이 영역 에 표시될 수 있다. 또한, 객체는 사용자의 입력에 대응하는 터치 좌표에 기반하여 캡쳐 이미지로부터 검출될수 있다. 그리고, 전자 장치(A)는 캡쳐 이미지 중 적어도 일부를 외부 장치로 전송할 수 있다. 이때, 전자 장치 (A)는 선택된 객체에 대응하는 객체 영역에 대한 정보(예를 들어, 객체 영역의 좌표 등)을 함께 전송할 수 있다. 그리고, 전자 장치(A)는 캡쳐 이미지에 포함된 객체에 대응하는 객체 정보 및 객체와 관련된 추가 정보를 기반 하여 검색된 검색 결과를 외부 장치로부터 수신 할 수 있다. 이때, 객체 정보는 캡쳐 이미지의 적어도 일부 중 객체에 대응하는 객체 영역을 인식하여 획득된 정보이고, 추 가 정보는 캡쳐 이미지의 적어도 일부 중 객체 영역의 주변 영역을 인식하여 획득한 정보일 수 있다. 특히, 객 체 정보는 객체 영역을 객체 정보를 추정하도록 설정된 학습된 인식 모델에 적용하여 획득된 정보이고, 추가 정 보는 주변 영역을 통해 추가 정보를 추정하도록 설정된 학습된 인식 모델에 적용하여 획득된 정보일 수 있다. 또한, 검색된 검색 결과는 객체 정보 및 추가 정보를 검색 카테고리를 추정하도록 설정된 학습된 인식 모델에 적용하여 획득된 검색 카테고리에 기반하여 검색된 검색 결과일 수 있다. 이때, 검색된 검색 결과는 객체 정보 및 추가 정보 이외에 전자 장치(A)의 사용 이력 정보, 전자 장치(A)의 사용자의 사용자 프로파일, 전자 장치 (A)에 구비된 카메라로 촬영된 촬영 정보 중 적어도 하나를 추가적으로 이용하여 검색될 수 있다. 다음으로, 전자 장치(A)는 획득된 검색 결과를 디스플레이 영역에 표시할 수 있다. 이 때, 전자 장치(A) 는 검색 결과가 객체와 오버랩되지 않도록 디스플레이 영역의 일 측에 표시할 수 있다. 또한, 전자 장치(A)는 검색 결과와 함께 객체 정보를 함께 표시할 수 있다. 또한, 전자 장치(A)는 검색 결과 또는 객체 정보의 표시에 따른 사용자 피드백을 외부 장치로 전송할 수 있다. 도 18은, 다양한 실시예에 따른 전자 장치의 구성을 나타내는 블록도이다. 도 18에 도시된 바와 같이, 전자 장치(A)는 영상 획득부, 영상 처리부, 디스플레이부, 통신 부, 메모리, 오디오 처리부, 오디오 출력부, 사용자 입력부 및 프로세서 중 적어도 하나를 포함할 수 있다. 한편, 도 18에 도시된 전자 장치(A)의 구성은 일 예에 불과하므로, 반드시 전술된 블록도에 한정되는 것은 아니다. 따라서, 전자 장치(A)의 종류 또는 전자 장치(A)의 목적에 따라 도 18 에 도시된 전자 장치(A의 구성의 일부가 생략 또는 변형되거나, 추가될 수도 있음은 물론이다. 영상 획득부는 다양한 소스를 통해 영상 데이터를 획득할 수 있다. 예로, 영상 획득부는 카메라로 구현되어 외부 환경을 촬영하여 촬영 정보를 획득할 수 있다. 영상 획득부를 통해 획득된 영상 데이터는 영상 처리부에서 처리될 수 있다. 영상 처리부는 영상 획득부에서 수신한 영상 데이터에 대한 처리를 수행할 수 있다. 영상 처리부 에서는 영상 데이터에 대한 디코딩, 스케일링, 노이즈 필터링, 프레임 레이트 변환 또는 해상도 변환 등 과 같은 다양한 이미지 처리를 수행할 수 있다. 디스플레이부는 영상 처리부에서 처리한 영상 데이터를 디스플레이 영역(또는, 디스플레이)에 디스 플레이할 수 있다. 디스플레이 영역은 전자 장치(A)의 하우징의 일면에 노출된 디스플레이부의 적어도 일 부를 의미할 수 있다. 디스플레이부의 적어도 일부는 플렉서블 디스플레이(flexible display)의 형태로 전자 장치(A)의 전면 영 역 및, 측면 영역 및 후면 영역 중 적어도 하나에 결합될 수도 있다. 플렉서블 디스플레이는 종이처럼 얇고 유 연한 기판을 통해 손상 없이 휘거나 구부리거나 말 수 있는 것을 특징으로 할 수 있다. 디스플레이부는 터치 패널과 결합하여 레이어 구조의 터치 스크린으로 구현될 수 있다. 터치 스크 린은 디스플레이 기능뿐만 아니라 터치 입력 위치, 터치된 면적뿐만 아니라 터치 입력 압력까지도 검출하는 기 능을 가질 수 있고, 또한 실질적인 터치(real-touch)뿐만 아니라 근접 터치(proximity touch)도 검출하는 기능 을 가질 수 있다. 통신부는 다양한 유형의 통신방식에 따라 다양한 유형의 외부 기기와 통신을 수행할 수 있다. 통신부 는 와이파이칩, 블루투스 칩, 무선 통신 칩, NFC 칩 중 적어도 하나를 포함할 수 있다. 프로세서는 통신부를 이용하여 외부 서버 또는 각종 외부 기기와 통신을 수행할 수 있다. 메모리는 전자 장치(A)의 동작에 필요한 각종 프로그램 및 데이터를 저장할 수 있다. 메모리는 비 휘발성 메모리, 휘발성 메모리, 플래시메모리(flash-memory), 하드디스크 드라이브(HDD) 또는 솔리드 스테이트 드라이브(SSD) 등으로 구현될 수 있다. 메모리는 프로세서에 의해 액세스되며, 프로세서에 의한 데이터의 독취/기록/수정/삭제/갱신 등이 수행될 수 있다. 본 문서에서 메모리라는 용어는 메모리, 프로세서 내 롬(미도시), 램(미도시) 또는 전자 장치(A)에 장착되는 메모리 카드(미도시)(예를 들어, micro SD 카드, 메모리 스틱)를 포함할 수 있다. 또한, 메모리에는 디스플레이부의 디스플레이 영역에 표시될 각종 화면을 구성하기 위한 프로그램 및 데이터 등이 저장될 수 있다. 또한, 메모리는 본 문서의 다양한 인식 모델을 저장할 수도 있다. 오디오 처리부는 오디오 데이터에 대한 처리를 수행하는 구성요소이다. 오디오 처리부에서는 오디 오 데이터에 대한 디코딩이나 증폭, 노이즈 필터링 등과 같은 다양한 처리가 수행될 수 있다. 오디오 처리부 에서 처리된 오디오 데이터는 오디오 출력부로 출력될 수 있다. 오디오 출력부는 오디오 처리부에 의해 디코딩이나 증폭, 노이즈 필터링과 같은 다양한 처리 작업 이 수행된 각종 오디오 데이터뿐만 아니라 각종 알림 음이나 음성 메시지를 출력하는 구성이다. 특히, 오디오 출력부는 스피커로 구현될 수 있으나, 이는 일 실시 예에 불과할 뿐, 오디오 데이터를 출력할 수 있는 출 력 단자로 구현될 수 있다. 사용자 입력부는 다양한 사용자 입력을 수신하여 프로세서로 전달할 수 있다. 사용자 입력부(188 0)는, 예를 들면, 터치 패널, (디지털) 펜 센서 또는 키를 포함할 수 있다. 터치 패널 은, 예를 들면, 정전식, 감압식, 적외선 방식, 또는 초음파 방식 중 적어도 하나의 방식을 사용할 수 있 다. 또한, 터치 패널은 제어 회로를 더 포함할 수도 있다. 터치 패널은 택타일 레이어(tactile layer)를 더 포함하여, 사용자에게 촉각 반응을 제공할 수 있다. (디지털) 펜 센서는, 예를 들면, 터치 패널의 일부이거나, 별도의 인식용 쉬트를 포함할 수 있다. 키는, 예를 들면, 물리적인 버튼, 광학식 키, 또는 키패드를 포함할 수 있다. 프로세서(또는, 제어부)는 메모리에 저장된 각종 프로그램을 이용하여 전자 장치(A)의 전반적인 동 작을 제어할 수 있다. 프로세서는 RAM, ROM, 그래픽 처리부, 메인 CPU, 제1 내지 n 인터페이스(1895- 1~1895-n), 버스로 구성될 수 있다. 이때, RAM, ROM, 그래픽 처리부, 메인 CPU, 제1 내지 n 인터페이스(1895-1~1895-n) 등은 버스를 통해 서로 연결될 수 있다. 도 19a를 참조하면, 프로세서는 학습부 및 인식부 중 적어도 하나를 포함할 수 있다. 도 19a 의 프로세서는 도 18의 전자 장치(A)의 프로세서 또는 데이터 학습 서버(미도시)의 프로세서에 대 응될 수 있다. 학습부는 소정의 상황 판단을 위한 기준을 갖는 인식 모델을 생성 또는 학습시킬 수 있다. 학습부 는 수집된 학습 데이터를 이용하여 판단 기준을 갖는 인식 모델을 생성할 수 있다. 일 예로, 학습부는 객체가 포함된 이미지를 학습 데이터로서 이용하여 이미지에 포함된 객체가 어떤 것인 지 판단하는 기준을 갖는 객체 인식 모델을 생성, 학습 또는 갱신시킬 수 있다. 또 다른 예로, 학습부는 객체가 포함된 화면에 포함된 주변 정보를 학습 데이터로서 이용하여 이미지에 포함된 객체 주변에 다양한 추가 정보를 판단하는 기준을 갖는 주변 정보 인식 모델을 생성, 학습 또는 갱신시 킬 수 있다. 또 다른 예로, 학습부는 카메라에 의해 촬영된 이미지를 학습 데이터로서 이용하여 이미지에 포함된 사용 자의 얼굴을 판단하는 기준을 갖는 얼굴 인식 모델을 생성, 학습 또는 갱신시킬 수 있다. 인식부는 소정의 데이터를 학습된 인식 모델의 입력 데이터로 사용하여, 소정의 데이터에 포함된 인식 대 상을 추정할 수 있다. 일 예로, 인식부는 객체가 포함된 객체 영역(또는, 이미지)를 학습된 인식 모델의 입력 데이터로 사용하 여 객체 영역에 포함된 객체에 대한 객체 정보를 획득(또는, 추정, 추론)할 수 있다.다른 예로, 인식부는 객체 정보 및 컨텍스트 정보 중 적어도 하나를 학습된 인식 모델에 적용하여 검색 결과를 제공할 검색 카테고리를 추정(또는, 결정, 추론)할 수 있다. 이 때, 검색 결과는 우선 순위에 따라 복수 개가 획득될 수도 있다. 또 다른 예로, 인식부는 컨텍스트 정보(예를 들어, 객체의 주변 정보)를 학습된 인식 모델에 적용하여 객 체와 연관성이 있는 컨텍스트 인식 정보(예를 들어, 객체와 관련된 추가 정보 등)를 추정할 수 있다. 학습부의 적어도 일부 및 인식부의 적어도 일부는, 소프트웨어 모듈로 구현되거나 적어도 하나의 하드웨어 칩 형태로 제작되어 전자 장치에 탑재될 수 있다. 예를 들어, 학습부 및 인식부 중 적어 도 하나는 인공 지능(AI; artificial intelligence)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또는 기존의 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제 작되어 전술한 각종 전자 장치 또는 객체 인식 장치에 탑재될 수도 있다. 이 때, 인공 지능을 위한 전용 하드웨 어 칩은 확률 연산에 특화된 전용 프로세서로서, 기존의 범용 프로세서보다 병렬처리 성능이 높아 기계 학습과 같은 인공 지능 분야의 연산 작업을 빠르게 처리할 수 있다. 학습부 및 인식부가 소프트웨어 모듈 (또는, 인스트럭션(instruction) 포함하는 프로그램 모듈)로 구현되는 경우, 소프트웨어 모듈은 컴퓨터로 읽을 수 있는 판독 가능한 비일시적 판독 가능 기록매체(non-transitory computer readable media)에 저장될 수 있 다. 이 경우, 소프트웨어 모듈은 OS(Operating System)에 의해 제공되거나, 소정의 애플리케이션에 의해 제공될 수 있다. 또는, 소프트웨어 모듈 중 일부는 OS(Operating System)에 의해 제공되고, 나머지 일부는 소정의 애플 리케이션에 의해 제공될 수 있다. 이 경우, 학습부 및 인식부는 하나의 전자 장치에 탑재될 수도 있으며, 또는 별개의 전자 장치들 에 각각 탑재될 수도 있다. 예를 들어, 학습부 및 인식부 중 하나는 전자 장치(A0)에 포함되고, 나 머지 하나는 외부의 서버에 포함될 수 있다. 또한, 학습부 및 인식부는 유선 또는 무선으로 통하여, 학습부가 구축한 모델 정보를 인식부로 제공할 수도 있고, 인식부로 입력된 데이터 가 추가 학습 데이터로서 학습부로 제공될 수도 있다. 도 19b는, 다양한 실시예에 따른 학습부 및 인식부의 블록도이다. 도 19b의 (a)를 참조하면, 일부 실시예에 따른 학습부는 학습 데이터 획득부(1910-1) 및 모델 학습부 (1910-4)를 포함할 수 있다. 또한, 학습부는 학습 데이터 전처리부(1910-2), 학습 데이터 선택부(1910- 3) 및 모델 평가부(1910-5) 중 적어도 하나를 선택적으로 더 포함할 수 있다. 학습 데이터 획득부(1910-1)는 인식 대상을 추론하기 위한 인식 모델에 필요한 학습 데이터를 획득할 수 있다. 본 문서의 실시예로, 학습 데이터 획득부(1910-1)는 객체를 포함하는 전체 이미지, 객체 영역에 대응하는 이미 지, 객체 정보 및 컨텍스트 정보 중 적어도 하나를 학습 데이터로서 획득할 수 있다. 학습 데이터는 학습부 또는 학습부의 제조사가 수집 또는 테스트한 데이터가 될 수도 있다. 모델 학습부(1910-4)는 학습 데이터를 이용하여, 인식 모델이 소정의 인식 대상을 어떻게 판단할 지에 관한 판 단 기준을 갖도록 학습시킬 수 있다. 예로, 모델 학습부(1910-4)는 학습 데이터 중 적어도 일부를 판단 기준으 로 이용하는 지도 학습(supervised learning)을 통하여, 인식 모델을 학습시킬 수 있다. 또는, 모델 학습부 (1910-4)는, 예를 들어, 별다른 지도 없이 학습 데이터를 이용하여 스스로 학습함으로써, 상황의 판단을 위한 판단 기준을 발견하는 비지도 학습(unsupervised learning)을 통하여, 인식 모델을 학습시킬 수 있다. 또한, 모 델 학습부(1210-4)는, 예를 들어, 학습에 따른 상황 판단의 결과가 올바른 지에 대한 피드백을 이용하는 강화 학습(reinforcement learning)을 통하여, 인식 모델을 학습시킬 수 있다. 또한, 모델 학습부(1910-4)는, 예를 들어, 오류 역전파법(error back-propagation) 또는 경사 하강법(gradient descent)을 포함하는 학습 알고리즘 등을 이용하여 인식 모델을 학습시킬 수 있다 또한, 모델 학습부(1910-4)는 입력 데이터를 이용하여 인식 대상을 추정하기 위하여 어떤 학습 데이터를 이용해 야 하는 지에 대한 선별 기준을 학습할 수도 있다. 모델 학습부(1910-4)는 미리 구축된 인식 모델이 복수 개가 존재하는 경우, 입력된 학습 데이터와 기본 학습 데 이터의 관련성이 큰 인식 모델을 학습할 인식 모델로 결정할 수 있다. 이 경우, 기본 학습 데이터는 데이터의 타입 별로 기 분류되어 있을 수 있으며, 인식 모델은 데이터의 타입 별로 미리 구축되어 있을 수 있다. 예를 들 어, 기본 학습 데이터는 학습 데이터가 생성된 지역, 학습 데이터가 생성된 시간, 학습 데이터의 크기, 학습 데이터의 장르, 학습 데이터의 생성자, 학습 데이터 내의 오브젝트의 종류 등과 같은 다양한 기준으로 기 분류되 어 있을 수 있다. 인식 모델이 학습되면, 모델 학습부(1910-4)는 학습된 인식 모델을 저장할 수 있다. 이 경우, 모델 학습부 (1910-4)는 학습된 인식 모델을 전자 장치(A)의 메모리에 저장할 수 있다. 또는, 모델 학습부(1910-4)는 학습된 인식 모델을 전자 장치(A)와 유선 또는 무선 네트워크로 연결되는 서버의 메모리에 저장할 수도 있다. 학습부는 인식 모델의 분석 결과를 향상시키거나, 인식 모델의 생성에 필요한 자원 또는 시간을 절약하기 위하여, 학습 데이터 전처리부(1910-2) 및 학습 데이터 선택부(1910-3)를 더 포함할 수도 있다. 학습 데이터 전처리부(1910-2)는 상황 판단을 위한 학습에 획득된 데이터가 이용될 수 있도록, 획득된 데이터를 전처리할 수 있다. 학습 데이터 전처리부(1910-2)는 모델 학습부(1910-4)가 상황 판단을 위한 학습을 위하여 획 득된 데이터를 이용할 수 있도록, 획득된 데이터를 기 설정된 포맷으로 가공할 수 있다. 학습 데이터 선택부(1910-3)는 학습 데이터 획득부(1910-1)에서 획득된 데이터 또는 학습 데이터 전처리부 (1910-2)에서 전처리된 데이터 중에서 학습에 필요한 데이터를 선택할 수 있다. 선택된 학습 데이터는 모델 학 습부(1910-4)에 제공될 수 있다. 학습 데이터 선택부(1910-3)는 기 설정된 선별 기준에 따라, 획득되거나 전처 리된 데이터 중에서 학습에 필요한 학습 데이터를 선택할 수 있다. 또한, 학습 데이터 선택부(1910-3)는 모델 학습부(1910-4)에 의한 학습에 의해 기 설정된 선별 기준에 따라 학습 데이터를 선택할 수도 있다. 학습부는 데이터 인식 모델의 분석 결과를 향상시키기 위하여, 모델 평가부(1910-5)를 더 포함할 수도 있 다. 모델 평가부(1910-5)는 인식 모델에 평가 데이터를 입력하고, 평가 데이터로부터 출력되는 분석 결과가 소정 기 준을 만족하지 못하는 경우, 모델 학습부(1910-4)로 하여금 다시 학습하도록 할 수 있다. 이 경우, 평가 데이터 는 인식 모델을 평가하기 위한 기 정의된 데이터일 수 있다. 예를 들어, 모델 평가부(1910-5)는 평가 데이터에 대한 학습된 인식 모델의 분석 결과 중에서, 분석 결과가 정 확하지 않은 평가 데이터의 개수 또는 비율이 미리 설정된 임계치를 초과하는 경우 소정 기준을 만족하지 못한 것으로 평가할 수 있다. 한편, 학습된 인식 모델이 복수 개가 존재하는 경우, 모델 평가부(1910-5)는 각각의 학습된 인식 모델에 대하여 소정 기준을 만족하는지를 평가하고, 소정 기준을 만족하는 모델을 최종 인식 모델로서 결정할 수 있다. 이 경 우, 소정 기준을 만족하는 모델이 복수 개인 경우, 모델 평가부(1910-5)는 평가 점수가 높은 순으로 미리 설정 된 어느 하나 또는 소정 개수의 모델을 최종 인식 모델로서 결정할 수 있다. 도 19b의 (b)를 참조하면, 일부 실시예에 따른 인식부는 인식 데이터 획득부(1920-1) 및 인식 결과 제공 부(1920-4)를 포함할 수 있다. 또한, 인식부는 인식 데이터 전처리부(1920-2), 인식 데이터 선택부(1920-3) 및 모델 갱신부(1920-5) 중 적어도 하나를 선택적으로 더 포함할 수 있다. 인식 데이터 획득부(1920-1)는 상황 판단에 필요한 데이터를 획득할 수 있다. 인식 결과 제공부(1920-4)는 인식 데이터 획득부(1920-1)에서 획득된 데이터를 입력 값으로 학습된 인식 모델에 적용하여 상황을 판단할 수 있다. 인식 결과 제공부(1920-4)는 데이터의 분석 목적에 따른 분석 결과를 제공할 수 있다. 인식 결과 제공부(1920- 4)는 후술할 인식 데이터 전처리부(1920-2) 또는 인식 데이터 선택부(1920-3)에 의해 선택된 데이터를 입력 값 으로 인식 모델에 적용하여 분석 결과를 획득할 수 있다. 분석 결과는 인식 모델에 의해 결정될 수 있다. 일 실시예로, 인식 결과 제공부(1920-4)는 인식 데이터 획득부(1920-1)에서 획득한 객체가 포함된 객체 영역을 학습된 인식 모델 적용하여 객체 영역에 대응하는 객체 정보를 획득(또는, 추정)할 수 있다. 다른 실시예로, 인식 결과 제공부(1920-4)는 인식 데이터 획득부(1920-1)에서 획득한 객체 영역, 객체 정보 및 컨텍스트 정보 중 적어도 하나를 학습된 인식 모델에 적용하여 검색 결과를 제공할 검색 카테고리를 획득(또는, 추정)할 수 있다인식부는 인식 모델의 분석 결과를 향상시키거나, 분석 결과의 제공을 위한 자원 또는 시간을 절약하기 위하여, 인식 데이터 전처리부(1920-2) 및 인식 데이터 선택부(1920-3)를 더 포함할 수도 있다. 인식 데이터 전처리부(1920-2)는 상황 판단을 위해 획득된 데이터가 이용될 수 있도록, 획득된 데이터를 전처리 할 수 있다. 인식 데이터 전처리부(1920-2)는 인식 결과 제공부(1920-4)가 상황 판단을 위하여 획득된 데이터를 이용할 수 있도록, 획득된 데이터를 기 정의된 포맷으로 가공할 수 있다. 인식 데이터 선택부(1920-3)는 인식 데이터 획득부(1920-1)에서 획득된 데이터 또는 인식 데이터 전처리부 (1920-2)에서 전처리된 데이터 중에서 상황 판단에 필요한 데이터를 선택할 수 있다. 선택된 데이터는 인식 결 과 제공부(1920-4)에게 제공될 수 있다. 인식 데이터 선택부(1920-3)는 상황 판단을 위한 기 설정된 선별 기준 에 따라, 획득되거나 전처리된 데이터 중에서 일부 또는 전부를 선택할 수 있다. 또한, 인식 데이터 선택부 (1920-3)는 모델 학습부(1910-4)에 의한 학습에 의해 기 설정된 선별 기준에 따라 데이터를 선택할 수도 있다. 모델 갱신부(1920-5)는 인식 결과 제공부(1920-4)에 의해 제공되는 분석 결과에 대한 평가에 기초하여, 인식 모 델이 갱신되도록 제어할 수 있다. 예를 들어, 모델 갱신부(1920-5)는 인식 결과 제공부(1920-4)에 의해 제공되 는 분석 결과를 모델 학습부(1910-4)에게 제공함으로써, 모델 학습부(1910-4)가 인식 모델을 추가 학습 또는 갱 신하도록 요청할 수 있다. 도 20은, 일 실시예에 따른 전자 장치(A) 및 서버(S)가 서로 연동함으로써 데이터를 학습하고 인식하는 예시를 나타내는 도면이다. 도 20을 참조하면, 서버(S)는 상황 판단을 위한 기준을 학습할 수 있으며, 전자 장치(A)는 서버(S)에 의한 학습 결과에 기초하여 상황을 판단할 수 있다. 이 경우, 서버(S)의 모델 학습부(1910-4)는 도 1919a에 도시된 학습부의 기능을 수행할 수 있다. 서버 (S)의 모델 학습부(1910-4)는 소정의 상황을 판단하기 위하여 어떤 객체 영상, 객체 정보 또는 컨텍스트 정보를 이용할 지, 상기 데이터를 이용하여 상황을 어떻게 판단할 지에 관한 기준을 학습할 수 있다. 또한, 전자 장치(A)의 인식 결과 제공부(1920-4)는 인식 데이터 선택부(1920-3)에 의해 선택된 데이터를 서버 (S)에 의해 생성된 인식 모델에 적용하여 객체 정보 또는 검색 카테고리를 판단할 수 있다. 또는, 전자 장치 (A)의 인식 결과 제공부(1920-4)는 서버(S)에 의해 생성된 인식 모델을 서버(S)로부터 수신하고, 수신된 인식 모델을 이용하여 상황을 판단할 수 있다. 이 경우, 전자 장치(A)의 인식 결과 제공부(1920-4)는 인식 데이터 선 택부(1920-3)에 의해 선택된 객체 영상을 서버(S)로부터 수신된 인식 모델에 적용하여, 객체 영상에 대응하는 객체 정보를 판단할 수 있다. 또는, 인식 결과 제공부(1920-4)는 컨텍스트 정보 및 컨텍스트 인식 정보 중 적어 도 하나를 이용하여 검색 결과를 획득할 검색 카테고리를 판단할 수 있다. 도 21은, 일 실시예에 따른 인식 모델을 이용하는 전자 장치의 흐름도이다. 도 21을 참조하면, 전자 장치(A)는 객체를 포함하는 화면을 표시할 수 있다. 전자 장치(A)는 객체를 선택하는 사용자 입력이 감지되면(2102-Y), 객체를 포함하는 화면을 캡쳐하여 캡쳐 이미 지를 생성할 수 있다. 다음으로, 전자 장치(A)는 캡쳐 이미지에 포함된 객체 영역에 대응하는 객체 정보 및 주변 영역에 대응되는 객 체에 대한 추가 정보 중 적어도 하나를 이용하여 객체와 연관된 검색 결과를 획득할 수 있다. 이때, 객체 영역 및 주변에 대한 정보 중 적어도 하나를 인식 모델에 적용한 결과는, 예로, 객체 정보, 객체에 대한 추가 정보 및 검색 카테고리 중 적어도 하나가 될 수 있다. 전자 장치(A)는 객체 정보, 객체에 대한 추가 정보 및 검색 카테고리 중 적어도 하나를 이용하여 객체와 관련된 검색 결과를 획득할 수 있다. 예로, 전자 장 치(A)는 객체 정보, 추가 정보 및 검색 카테고리를 추천 장치(B)에 전송하고, 이에 대한 응답으로 객체와 관련 된 검색 결과를 획득할 수 있다. 검색 결과가 획득되면, 전자 장치(A)는 획득된 객체와 연관된 검색 결과를 디스플레이 영역에 표시할 수 있다 . 도 22 내지 도 25는, 다른 실시예에 따른 인식 모델을 이용하는 네트워크 시스템의 흐름도이다. 도 22 내지 도 25에서, 인식 모델을 이용하는 네트워크 시스템은 제1 구성 요소(2201,2301,2401,2501), 제2 구 성 요소(2202,2302,2402,2502) 및 제3 구성 요소(2203,2303,2403)를 포함할 수 있다. 여기서, 제1 구성 요소(2201,2301,2401,2501)는 전자 장치(A)이고, 제2 구성 요소(2202,2302,2402,2502)는 인 식 모델이 저장된 서버(S)가 될 수 있다. 또는, 제1 구성 요소(2201,2301,2401,2501)는 범용 프로세서이고, 제2 구성 요소(2202,2302,2402,2502)는 인공 지능 전용 프로세서가 될 수 있다. 또는, 제1 구성 요소 (2201,2301,2401,2501)는 적어도 하나의 어플리케이션이 될 수 있고, 제2 구성 요소(2202,2302,2402,2502)는 운영 체제(operating system, OS)가 될 수 있다. 즉, 제2 구성 요소(2202,2302,2402,2502)는 제1 구성 요소 (2201,2301,2401,2501)보다 더 집적화되거나, 전용화되거나, 딜레이(delay)가 작거나, 성능이 우세하거나 또는 많은 리소스를 가진 구성 요소로서 데이터 인식 모델의 생성, 갱신 또는 적용 시에 요구되는 많은 연산을 제1 구성 요소(2201,2301,2401,2501)보다 신속하고 효과적으로 처리 가능한 구성 요소가 될 수 있다. 이 경우, 제1 구성 요소(2201,2301,2401,2501) 및 제2 구성 요소(2202,2302,2402,2502) 간에 데이터를 송/수신 하기 위한 인터페이스가 정의될 수 있다. 예로, 인식 모델에 적용할 학습 데이터를 인자 값(또는, 매개 값 또는 전달 값)으로 갖는 API(application program interface)가 정의될 수 있다. API는 어느 하나의 프로토콜(예로, 전자 장치(A)에서 정의된 프로토콜) 에서 다른 프로토콜(예로, 서버(S)에서 정의된 프로토콜)의 어떤 처리를 위해 호출할 수 있는 서브 루틴 또는 함수의 집합으로 정의될 수 있다. 즉, API를 통하여 어느 하나의 프로토콜에서 다른 프로토콜의 동작이 수행될 수 있는 환경을 제공될 수 있다. 한편, 제3 구성 요소(2203,2303,2403)는 제1 구성 요소(2201,2301,2401,2501) 및 제2 구성 요소 (2202,2302,2402,2502) 중 적어도 하나에서 수신한 데이터에 기반하여 객체와 연관된 검색 결과를 획득할 수 있 다. 제3 구성 요소(2203,2303,2403)는 예로, 도 2b의 추천 장치(B)에 대응될 수 있다. 이 때, 제3 구성 요소 (2203,2303,2403)가 수신하는 데이터는, 예로, 객체 영역, 컨텍스트 정보, 객체 정보 또는 검색 카테고리 중 적 어도 하나가 될 수 있다. 또한, 일 실시예에 따라, 제3 구성 요소(2203,2303,2403)는 제2 구성 요소 (2202,2302,2402,2502)와 하나의 장치로 구현될 수 있다. 일 실시예로, 도 22에서, 먼저, 제1 구성 요소는 객체를 포함하는 화면을 표시할 수 있다. 이 경우, 객체를 선택하는 사용자 입력이 수신되면, 제1 구성 요소는 객체를 포함하는 화면을 캡쳐하여 캡쳐 이미지를 생성할 수 있다. 다음으로, 제1 구성 요소는 캡쳐 이미지에 포함된 객체에 대응하는 객체 영역을 획득하고, 획득된 객체 영역을 제2 구성 요소로 전송할 수 있다. 제2 구성 요소는 수신한 객체 영역을 인식 모델로 입력하여 객체 정보를 획득할 수 있다. 제2 구성 요소은 객체 정보를 제3 구성 요소로 전송할 수 있다. 제3 구성 요소는 객체 정보를 이용하여 객체와 연관된 검색 결과를 획득할 수 있다. 이때, 제3 구성 요소는 객체 정보 이 외에 부가 데이터(예를 들어, 사용자 특징 정보, 사용자 프로필, 사용자 피드백 등)을 추가적으로 이용하여 검 색 결과를 획득할 수 있다. 제3 구성 요소가 객체와 연관된 검색 결과를 제1 구성 요소로 전송하면, 제1 구성 요소 는 수신한 객체와 연관된 검색 결과를 디스플레이 영역에 표시할 수 있다. 제3 구성 요소은 객체와 연관된 검색 결과를 제1 구성요소로 직접 전송할 수도 있으며, 제2 구성 요소를 통해 전송 할 수도 있다. 다른 실시예로, 도23에서, 먼저, 제1 구성 요소는 객체를 포함하는 화면을 표시할 수 있다. 이 경우, 객체를 선택하는 사용자 입력이 수신되면, 제1 구성 요소는 객체를 포함하는 화면을 캡쳐하여 캡쳐 이미지를 생성할 수 있다. 제1 구성 요소는 캡쳐 이미지를 제2 구성 요소로 전송할 수 있다. 이때, 제1 구성 요소 는 캡쳐 이미지와 함께 선택된 객체에 대응되는 객체 영역에 대한 정보를 전송할 수 있다. 제2 구성 요소는 수신된 캡쳐 이미지를 객체 영역 및 주변 영역으로 분리할 수 있다. 이때, 제2 구 성 요소는 수신된 객체 영역에 대한 정보를 바탕으로 객체 영역 및 주변 영역을 분리할 수 있다. 다른 실 시예로, 제1 구성 요소은 캡쳐 이미지를 객체 영역 및 주변 영역으로 분리하여 제2 구성 요소로 전 송할 수 있다. 제2 구성 요소는 분리된 객체 영역 및 주변 영역을 인식 모델로 입력하여 객체 정보 및 객체에 대한 추가 정보를 획득할 수 있다. 이때, 제2 구성 요소는 객체 영역을 객체 인식 모델에 입력하여 객체 정보 를 획득할 수 있으며, 주변 영역을 주변 정보 인식 모델에 입력하여 객체에 대한 추가 정보를 획득할 수 있다. 또한, 제2 구성 요소는 객체 정보 및 객체에 대한 추가 정보를 바탕으로 검색 카테고리 및 검색 카테고리 의 우선 순위를 결정할 수 있다. 또는, 제2 구성 요소는 객체 영역을 인식 모델로 입력하여 객체 정보를 획득하고, 객체를 포함하는 화면 중 주변 영역에 대응하는 부분의 애플리케이션 리소스(resource) 분석을 통해 추가 정보를 획득할 수 있다. 이 경우, 제2 구성요소는 객체 정보 및 객체에 대한 추가 정보를 바탕으로 검색 카테고리 및 검색 카테고리 의 우선순위를 결정할 수 있다. 애플리케이션 리소스 분석은 애플리케이션이 실행되었을 때 실행 화면 상에 표시되는 UI(user interface)요소의 종류, 내용(예. 이미지나 텍스트의 내용 등) 및 화면 상의 위치 등을 파싱(parsing)하는 것을 의미한다. 주변 영역에 대응하는 부분의 애플리케이션 리소스 분석은, 캡쳐 이미지를 생성하는 시점의 애플리케이션 실행 화면 에서 UI요소의 화면 상의 위치를 이용하여 주변 영역에 대응하는 위치에 표시되는 UI요소의 종류나 내용을 파악 하는 것을 의미한다. 제2 구성 요소는 UI 요소의 종류나 내용을 파악하여 객체에 대한 추가 정보를 획득할 수 있다. 이 경우, 제1 구성 요소는 UI 요소의 종류 및 내용을 파악하여 객체에 대한 추가 정보를 획득하여 제2 구성 요소 로 전송할 수도 있다. 제2 구성 요소는 동작 2315의 인식 모델 적용 결과인 객체 정보 및 추가 정보를 제3 구성 요소로 전송할 수 있다. 이때, 제2 구성 요소는 검색 카테고리에 대한 정보를 함께 제3 구성 요소로 전송할 수 있다. 제3 구성 요소는 수신한 객체 정보 및 추가 정보를 이용하여 객체와 연관된 검색 결과를 획득할 수 있다 . 이때, 제3 구성 요소는 검색 카테고리를 함께 이용하여 검색 결과를 획득할 수 있다. 또한, 제3 구성 요소는 객체 정보 및 추가 정보 이외에 부가 데이터(예를 들어, 사용자 특징 정보, 사용자 프로필, 사용자 피드백 등)를 추가적으로 이용하여 검색 결과를 획득할 수 있다. 이때, 부가 데이터는 제1 구성 요소 또는 다른 요소로부터 전송되거나 제3 구성 요소에 기 저장될 수 있다. 제3 구성 요소가 객체와 연관된 검색 결과를 제1 구성 요소로 전송하면, 제1 구성 요소 는 수신한 객체와 연관된 검색 결과를 디스플레이 영역에 표시할 수 있다. 다른 실시예로, 도24에서, 먼저, 제1 구성 요소는 객체를 포함하는 화면을 표시할 수 있다. 이 경우, 객체를 선택하는 사용자 입력이 수신되면, 제1 구성 요소는 객체를 포함하는 화면을 캡쳐하여 캡쳐 이미지를 생성할 수 있다. 제1 구성 요소는 생성된 캡쳐 이미지를 객체 영역 및 주변 영역으로 분리할 수 있다. 이때, 제1 구 성 요소는 사용자 터치 지점을 바탕으로 객체 영역을 판단하고, 판단된 객체 영역에 대한 정보를 바탕으 로 객체 영역 및 주변 영역을 분리할 수 있다.제1 구성 요소를 분리된 객체 영역 및 주변 영역을 제2 구 성 요소로 전송할 수 있다. 제2 구성 요소는 객체 영역 및 주변 정보를 인식 모델로 입력하여 객체 정보 및 객체에 대한 추가 정보를 획득할 수 있다. 이때, 제2 구성 요소는 객체 영역을 객체 인식 모델에 입력하여 객체 정보를 획득 할 수 있고, 주변 영역을 주변 정보 인식 모델에 입력하여 객체에 대한 추가 정보를 획득할 수 있다. 또한, 제2 구성 요소는 객체 정보 및 추가 정보를 바탕으로 검색 카테고리 및 검색 카테고리의 우선 순위를 결정할 수 있다.제2 구성 요소는 획득한 객체 정보 및 추가 정보를 제3 구성 요소로 전송할 수 있다 . 이때, 제2 구성 요소는 검색 카테고리에 대한 정보를 함께 제3 구성 요소로 전송할 수 있다. 제3 구성 요소는 수신한 객체 정보 및 추가 정보를 이용하여 객체와 연관된 검색 결과를 획득할 수 있다 . 이때, 제3 구성 요소는 객체 정보 및 추가 정보를 입력 데이터로서 인식 모델에 적용하여 객체와 연관된 검색 결과를 획득할 수 있다. 이때, 제3 구성 요소는 검색 카테고리를 함께 이용하여 검색 결과를 획득할 수 있다. 또한, 제3 구성 요소는 객체 정보 및 추가 정보 이외에 부가 데이터(예를 들어, 사용자 특징 정보, 사용자 프로필, 사용자 피드백 등)을 이용하여 검색 결과를 획득할 수 있다. 이때, 부가 데이터는 제1 구성 요소 또는 다른 요소로부터 전송되거나 제3 구성 요소에 기 저장될 수 있다. 제3 구성 요소가 객체와 연관된 검색 결과를 제1 구성 요소로 전송하면, 제1 구성 요소 는 수신한 객체와 연관된 검색 결과를 디스플레이 영역에 표시할 수 있다. 다른 실시예로, 도25에서, 먼저, 제1 구성 요소는 객체를 포함하는 화면을 표시할 수 있다. 이 경우, 객체를 선택하는 사용자 입력이 수신되면, 제1 구성 요소는 객체를 포함하는 화면을 캡쳐하여 캡쳐 이미지를 생성할 수 있다. 제1 구성 요소는 캡쳐 이미지를 제2 구성 요소로 전송할 수 있다. 이때, 제1 구성 요소 는 캡쳐 이미지와 함께 선택된 객체에 대응되는 객체 영역에 대한 정보를 전송할 수 있다. 제2 구성 요소는 수신된 캡쳐 이미지를 객체 영역 및 주변 영역으로 분리할 수 있다. 이때, 제2 구 성 요소는 수신된 객체 영역에 대한 정보를 바탕으로 객체 영역 및 주변 영역을 분리할 수 있다. 제2 구성 요소는 분리된 객체 영역 및 주변 영역을 인식 모델로 입력하여 객체 정보 및 객체에 대한 추가 정보를 획득할 수 있다. 이때, 제2 구성 요소는 객체 영역을 객체 인식 모델에 입력하여 객체 정보 를 획득할 수 있으며, 주변 영역을 주변 정보 인식 모델에 입력하여 객체에 대한 추가 정보를 획득할 수 있다. 또한, 제2 구성 요소는 객체 정보 및 객체에 대한 추가 정보를 바탕으로 검색 카테고리 및 검색 카테고리 의 우선 순위를 결정할 수 있다. 제2 구성 요소는 획득된 객체 정보 및 추가 정보를 이용하여 객체와 연관된 검색 결과를 획득할 수 있다 . 이때, 제2 구성 요소는 객체 정보 및 추가 정보를 입력 데이터로서 인식 모델에 적용하여 객체와 연관된 검색 결과를 획득할 수 있다. 이때, 제2 구성 요소는 검색 카테고리를 함께 이용하여 검색 결과를 획득할 수 있다. 또한, 제2 구성 요소는 객체 정보 및 추가 정보 이외에 부가 데이터(예를 들어, 사용자 특징 정보, 사용자 프로필, 사용자 피드백 등)을 이용하여 검색 결과를 획득할 수 있다. 이때, 부가 데이터는 제1 구성 요소 또는 다른 요소로부터 전송되거나 제2 구성 요소에 기 저장될 수 있다. 제2 구성 요소가 객체와 연관된 검색 결과를 제1 구성 요소로 전송하면, 제1 구성 요소 는 수신한 객체와 연관된 검색 결과를 디스플레이 영역에 표시할 수 있다. 도 25 및 도 26는, 다른 실시예에 따른 전자 장치가 인식 모델을 이용하여 사용자에 의해 선택된 제1 영역에 대 한 검색 결과를 제공하는 실시예를 설명하기 위한 흐름도이다. 도 25에서, 전자 장치(A)는 애플리케이션 실행 화면을 표시할 수 있다. 이때, 애플리케이션 실행 화면에 는 적어도 하나의 객체가 포함될 수 있다. 전자 장치(A)는 애플리케이션 실행 화면을 표시하는 동안 사용자 입력을 수신할 수 있다. 이때, 사용자 입력은 애플리케이션 실행 화면을 중 객체를 선택하기 위해, 애플리케이션 실행 화면에 대한 입력일 수 있다. 특히, 사용자 입력이 수신된 경우, 전자 장치(A)는 선택된 객체를 포함하는 제1 영역에 대한 검색을 수행하기 위해 인공지능 에이전트을 실행할 수 있다. 전자 장치(A)는 사용자 입력에 따라 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성할 수 있다. 이때, 전자 장치(A)는 생성된 이미지를 애플리케이션 실행 화면을 대신하여 표시할 수 있다. 전자 장치(A)는 생성된 이미지 상의 사용자 입력에 대응되는 제1 영역에 대한 제1 정보 및 제1 영역과 다른 제2 영역에 대한 제2 정보를 이용하여 수행된 검색 결과를 표시할 수 있다(S2640). 이때, 제1 영역은 사용자 입력에대응하는 터치 좌표에 기반하여 생성된 이미지로부터 획득된 영역일 수 있다. 제2 영역은 생성된 이미지 중 제1 영역을 제외한 영역일 수 있다. 또한, 제1 영역에 대한 제1 정보 및 제2 영역에 대한 제2 정보는 학습된 모델에 의해 획득될 수 있다. 즉, 생 성된 이미지를 학습된 제1 모델에 입력하여 제1 정보를 획득할 수 있으며, 제1 정보 및 생성된 이미지를 학습된 제2 모델에 입력하여 제2 정보를 획득할 수 있다. 이때, 학습된 모델은 전자 장치(A) 혹은 외부의 서버에 저장 될 수 있다. 일 실시예로, 제1 정보 및 제2 정보를 획득하고 제1 영역에 대한 정보를 검색하는 동작은 외부 검색 서버에 의 해 수행될 수 있다. 즉, 전자 장치(A)는 생성된 이미지의 적어도 일부를 외부 검색 서버로 전송하고, 외부 검색 서버로부터 검색 결과를 수신할 수 있다. 또한, 전자 장치(A)는 애플리케이션 실행 화면 상에 검색 결과를 표시할 수 있으며, 검색 결과의 표시에 따른 사용자 피드백을 학습된 모델을 저장하는 외부 장치로 전송할 수 있다. 도 26에서, 전자 장치(A)는 애플리케이션 실행 화면을 표시할 수 있다. 이때, 애플리케이션 실행 화면에 는 적어도 하나의 객체가 포함될 수 있다. 전자 장치(A)는 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성할 수 있다. 이때, 애플리케이션 실행 화면 중 객체를 포함하는 제1 영역에 대한 사용자 입력이 수신된 경우, 전자 장치(A)는 애플리케이션 실행 화면 을 캡쳐하여 이미지를 생성할 수 있다. 이때, 전자 장치(A)는 애플리케이션 실행 화면 상에 생성된 이미지를 표 시할 수 있다. 전자 장치(A)는 생성된 이미지를 입력 데이터 사용하는 학습된 제1 모델을 통해 제1 영역에 대한 제1 정보를 획 득할 수 있다. 이때, 제1 모델은 전자 장치(A)에 저장될 수 있으나, 이는 일 실시예에 불과할 뿐, 외부 서버에 저장될 수 도 있다. 전자 장치(A)는 제1 정보 및 생성된 이미지를 입력 데이터로 사용하는 학습된 제2 모델을 통해 제2 영역에 대한 제2 정보를 획득할 수 있다. 이때, 제1 모델은 전자 장치(A)에 저장될 수 있으나, 이는 일 실시예에 불과 할 뿐, 외부 서버에 저장될 수 도 있다. 외부 서버에 제1 모델 및 제2 모델이 저장된 경우, 전자 장치(A)는 생성된 이미지를 외부 서버로 전송하고, 외 부 서버로부터 이미지를 제1 모델에 입력하여 획득한 제1 정보 및 이미지와 제1 정보를 제2 모델에 입력하여 획 득한 제2 정보를 수신할 수 있다. 상술한 바와 같이, 사용자 입력이 감지된 제1 영역에 대한 제1 정보뿐만 아니라 제1 영역 주위의 제2 영역에 대 한 제2 정보를 획득함으로써, 제1 영역에 대한 정보를 더욱 정확하게 검색할 수 있게 된다. 도 28 및 도 29는, 다양한 실시예에 따른 인식 모델을 이용하는 시스템의 흐름도이다. 도 28에서, 전자 장치(A)는 애플리케이션 실행 화면을 표시할 수 있다. 이때, 애플리케이션 실행 화면에 는 적어도 하나의 객체가 포함될 수 있다. 전자 장치(A)는 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성할 수 있다. 구체적으로, 애플리케이션 실행 화면이 표시되는 동안 객체를 선택하기 위한 사용자 명령이 수신되면, 전자 장치(A)는 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성할 수 있다. 전자 장치(A)는 생성된 이미지를 서버(S)로 전송할 수 있다. 이때, 서버(S)는 학습된 모델을 저장하며, 검색된 정보를 바탕으로 검색 기능을 수행할 수 있다. 서버(S)는 생성된 이미지를 입력 데이터 사용하는 학습된 제1 모델을 통해 제1 영역에 대한 제1 정보를 획득할 수 있다. 이때, 제1 영역은 사용자 입력이 수신된 터치 좌표점을 바탕으로 검출된 객체가 포함된 영역일 수 있다. 서버(S)는 제1 정보 및 생성된 이미지를 입력 데이터로 사용하는 학습된 제2 모델을 통해 제2 영역에 대한 제2 정보를 획득할 수 있다. 이때, 제2 영역은 생성된 이미지 중 제1 영역을 제외한 영역일 수 있다.서버(S)는 제1 정보 및 제2 정보를 바탕으로 제1 영역과 관련된 정보를 검색할 수 있다(S2860). 즉, 서버(S)는 객체에 대한 정보 및 객체 주변의 추가 정보를 바탕으로 객체를 포함하는 제1 영역에 관한 정보를 검색할 수 있 다. 서버(S)는 제1 영역과 관련된 검색 결과를 전자 장치(A)로 전송할 수 있으며, 전자 장치(A)는 수신된 검 색 결과를 제공할 수 있다. 즉, 도 28에 도시된 바와 같이, 전자 장치(A)는 애플리케이션 실행 화면을 캡쳐하여 생성된 이미지를 서버(S)에 제공하며, 서버(S)는 생성된 이미지를 학습된 인식 모델에 입력하여 제1 정보 및 제2 정보를 획득할 수 있고, 제1 정보 및 제2 정보를 바탕으로 정보를 검색할 수 있다. 한편, 상술한 실시예에서는 하나의 서버(S)가 학습된 모델을 통해 제1 정보 및 제2 정보를 획득하고, 제1 영역 과 관련된 정보를 검색하는 것으로 설명하였으나, 이는 일 실시예에 불과할 뿐, 복수의 서버가 상술한 동작을 나누어 수행할 수 있다. 즉, 제1 서버가 학습된 모델을 통해 제1 정보 및 제2 정보를 획득하고, 제2 서버가 제1 서버로부터 획득된 제1 정보 및 제2 정보를 바탕으로 제1 영역과 관련된 정보를 검색할 수 있다. 도 28에서, 전자 장치(A)는 애플리케이션 실행 화면을 표시할 수 있다. 이때, 애플리케이션 실행 화면에 는 적어도 하나의 객체가 포함될 수 있다. 전자 장치(A)는 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성할 수 있다. 구체적으로, 애플리케이션 실행 화면이 표시되는 동안 객체를 선택하기 위한 사용자 명령이 수신되면, 전자 장치(A)는 애플리케이션 실행 화면을 캡쳐하여 이미지를 생성할 수 있다. 전자 장치(A)는 생성된 이미지를 입력 데이터 사용하는 학습된 제1 모델을 통해 제1 영역에 대한 제1 정보를 획 득할 수 있다. 전자 장치(A)는 생성된 이미지 및 제1 정보를 서버(S)로 전송할 수 있다. 서버(S)는 제1 정보 및 생성된 이미지를 입력 데이터로 사용하는 학습된 제2 모델을 통해 제2 영역에 대한 제2 정보를 획득할 수 있다. 서버(S)는 제1 정보 및 제2 정보를 바탕으로 제1 영역과 관련된 정보를 검색할 수 있다. 서버(S)는 제1 영역과 관련된 검색 결과를 전자 장치(A)로 전송할 수 있으며, 전자 장치(A)는 수신된 검 색 결과를 제공할 수 있다. 즉, 상술한 실시예에서는 객체를 인식하기 위한 제1 모델을 통해 제1 정보를 획득하는 동작은 전자 장치(A)가 수행하고, 컨텍스트 정보를 추정하기 위한 제2 모델을 통해 제2 정보를 획득하는 서버(S)가 수행할 수 있다. 즉, 적은 처리량으로도 정보를 수행할 수 있는 객체 인식 동작은 전자 장치(A)가 수행할 수 있으며, 많은 처리 량이 필요한 컨텍스트 추정 동작은 서버(S)가 수행할 수 있다 한편, 도 28 및 도 29에서 설명한 실시예에서는 하나의 서버(S)가 학습된 모델을 통해 제1 정보 또는 제2 정보 를 획득하고, 제1 영역과 관련된 정보를 검색하는 것으로 설명하였으나, 이는 일 실시예에 불과할 뿐, 복수의 서버가 상술한 동작을 나누어 수행할 수 있다. 즉, 제1 서버가 학습된 모델을 통해 제1 정보 및 제2 정보를 획 득하고, 제2 서버가 제1 서버로부터 획득된 제1 정보 및 제2 정보를 바탕으로 제1 영역과 관련된 정보를 검색할 수 있다. 본 문서에서 사용된 용어 \"모듈\"은 하드웨어, 소프트웨어 또는 펌웨어로 구성된 유닛을 포함하며, 예를 들면, 로직, 논리 블록, 부품, 또는 회로 등의 용어와 상호 호환적으로 사용될 수 있다. 모듈은, 일체로 구성된 부품 또는 하나 또는 그 이상의 기능을 수행하는 최소 단위 또는 그 일부가 될 수 있다. 예를 들면, 모듈은 ASIC(application-specific integrated circuit)으로 구성될 수 있다. 본 문서의 다양한 실시예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호 출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실시예들에 따른 전자 장치(예: 전자 장치(A)) 를 포함할 수 있다. 상기 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 상기 프로세서의 제어하에 다른 구성요소들을 이용하여 상기 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또는 인터프리 터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장매체는, 비일시적(non- transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장매체가 신호(signal)를 포함하지 않 으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임시적으로 저장됨을 구분하지 않는다. 일시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있다. 온라인 배포의 경 우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 다양한 실시예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요소가 다 양한 실시예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작들은 순차 적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다."}
{"patent_id": "10-2017-0136888", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은, 다양한 실시예에 따른 검색 결과를 제공하는 전자 장치의 사용도를 나타낸다. 도 2는, 다양한 실시예에 따른, 전자 장치를 포함하는 네트워크 시스템의 블록도들을 나타낸다. 도 3 내지 도 5는, 다양한 실시예에 따른 전자 장치를 포함하는 네트워크 시스템의 흐름도를 나타낸다. 도 6은, 다양한 일 실시예에 따른 객체와 연관된 검색 결과를 제공하는 과정을 도식화한 도면이다. 도 7은 다양한 실시예에 따른 주변 정보를 이용하여 객체와 연관된 검색 결과를 제공하는 도면이다. 도 8은 다양한 실시예에 따른 객체를 선택하는 전자 장치의 사용도들을 나타낸다. 도 9a 및 도 9b는, 다양한 실시예에 따른 객체와 연관된 검색 결과를 제공하는 도면이다. 도 10은, 다양한 실시예에 따른 객체와 연관된 검색 결과를 검색 중인 것을 나타내는 도면이다. 도 11은, 다양한 실시예에 따른 객체와 연관된 검색 결과를 제공하는 도면이다. 도 12는, 다양한 실시예에 따른 촬영 정보를 이용하여 객체와 연관된 검색 결과를 제공하는 도면이다. 도 13 내지 도 16c는, 다양한 실시예에 따른 객체와 연관된 검색 결과를 제공하는 도면이다. 도 17은, 다양한 실시예에 따른 전자 장치 및 외부 장치가 연동하여 데이터를 학습하고 인식하는 예시를 나타내 는 도면이다. 도 18은, 다양한 실시예에 따른 전자 장치의 구성을 나타내는 블록도이다. 도 19a 및 도 19b는, 다양한 실시예에 따른 학습부 및 인식부를 나타내는 블록도이다. 도 20은, 다양한 실시예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도이다. 도 21은, 다양한 실시예에 따른 인식 모델을 이용하는 전자 장치의 흐름도이다. 도 22 내지 도 25는, 다양한 실시예에 따른 인식 모델을 이용하는 네트워크 시스템의 흐름도이다. 도 26 및 도 27은, 다양한 실시예에 따른, 인식 모델을 이용하는 전자 장치의 흐름도이다. 도 28 및 도 29는, 다양한 실시예에 따른 인식 모델을 이용하는 시스템의 흐름도이다."}
