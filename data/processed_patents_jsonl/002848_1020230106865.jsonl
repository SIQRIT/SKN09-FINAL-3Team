{"patent_id": "10-2023-0106865", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0025891", "출원번호": "10-2023-0106865", "발명의 명칭": "분산 연합학습 성능 개선 방법 및 장치", "출원인": "충북대학교 산학협력단", "발명자": "최영환"}}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "노드 j에 대해, 주변의 노드 h들로부터 제1 모델 파라미터를 수신하고, 선정된 기간 경과 후, 수신된 복수의 상기 제1 모델 파라미터를 집계(Aggregation)하여 생성되는 제2 모델 파라미터를, 상기 노드 j의 모델 파라미터와함께 다른 주변의 노드 i로 전송하는 집계 단계; 및상기 노드 i에 대해, 수신된 상기 제2 모델 파라미터와 상기 노드 j의 모델 파라미터를 이용하여 로컬 버전의모델을 학습하고, 상기 노드 i의 모델 파라미터를 업데이트하는 학습 단계를 포함하는, 분산 연합학습 성능 개선 방법."}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 모델을 학습 함에 따라 업데이트되는 상기 노드 i의 모델 파라미터를, 상기 제1 모델 파라미터로서 주변의노드에 배포하는 단계를 더 포함하는, 분산 연합학습 성능 개선 방법."}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 모델의 학습으로 인해, 상기 로컬 버전의 모델이, 글로벌 버전의 모델로 수렴될 때까지, 상기 집계 단계와상기 학습 단계를 반복하는 단계를 더 포함하는, 분산 연합학습 성능 개선 방법."}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 집계 단계는,상기 노드 h로부터 수신되는 상기 제1 모델 파라미터가 일 경우, [수식 3]을 만족하여 상기 제2 모델 파라미터 를 생성하는 단계를 포함하고,상기 [수식 3]인- 상기 은 노드 j와 노드 h 간의 연결 가중치이고, 상기 는 라운드임-분산 연합학습 성능 개선 방법."}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,공개특허 10-2025-0025891-3-상기 학습 단계는,상기 노드 j의 모델 파라미터가 일 경우, [수식 4]를 만족하여 상기 노드 i의 모델 파라미터를 업데이트하는 단계를 포함하고,상기 [수식 4]인- 상기 은 노드 i와 노드 j 간의 연결 가중치임-분산 연합학습 성능 개선 방법."}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "노드 j에 대해, 주변의 노드 h들로부터 제1 모델 파라미터를 수신하고, 선정된 기간 경과 후, 수신된 복수의 상기 제1 모델 파라미터를 집계(Aggregation)하여 생성되는 제2 모델 파라미터를, 상기 노드 j의 모델 파라미터와함께 다른 주변의 노드 i로 전송하는 파라미터 집계부; 및상기 노드 i에 대해, 수신된 상기 제2 모델 파라미터와 상기 노드 j의 모델 파라미터를 이용하여 로컬 버전의모델을 학습하고, 상기 노드 i의 모델 파라미터를 업데이트하는 모델 학습부를 포함하는, 분산 연합학습 성능 개선 장치."}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 모델을 학습 함에 따라 업데이트되는 상기 노드 i의 모델 파라미터를, 상기 제1 모델 파라미터로서 주변의노드에 배포하는 배포부를 더 포함하는, 분산 연합학습 성능 개선 장치."}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 모델 학습부는,상기 모델의 학습으로 인해, 상기 로컬 버전의 모델이, 글로벌 버전의 모델로 수렴될 때까지, 상기 제2 모델 파라미터 및 상기 노드 j의 모델 파라미터의 수신과, 상기 모델의 학습 및 상기 노드 i의 모델 파라미터의 업데이트를 반복하는분산 연합학습 성능 개선 장치."}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서,상기 파라미터 집계부는,공개특허 10-2025-0025891-4-상기 노드 h로부터 수신되는 상기 제1 모델 파라미터가 일 경우, [수식 3]을 만족하여 상기 제2 모델 파라미터 를 생성하고,상기 [수식 3]인- 상기 은 노드 j와 노드 h 간의 연결 가중치이고, 상기 는 라운드임-분산 연합학습 성능 개선 장치."}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 모델 학습부는,상기 노드 j의 모델 파라미터가 일 경우, [수식 4]를 만족하여 상기 노드 i의 모델 파라미터를 업데이트하고,상기 [수식 4]인- 상기 은 노드 i와 노드 j 간의 연결 가중치임-분산 연합학습 성능 개선 장치."}
{"patent_id": "10-2023-0106865", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항 내지 제5항 중 어느 한 항의 방법을 실행시키기 위한 프로그램을 기록한 컴퓨터 판독 가능한 기록매체."}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "분산 연합학습 성능 개선 방법 및 장치가 개시된다. 본 발명의 일실시예에 따른, 분산 연합학습 성능 개선 방법 은, 노드 j에 대해, 주변의 노드 h들로부터 제1 모델 파라미터를 수신하고, 선정된 기간 경과 후, 수신된 복수의 상기 제1 모델 파라미터를 집계(Aggregation)하여 생성되는 제2 모델 파라미터를, 상기 노드 j의 모델 파라미터 와 함께 다른 주변의 노드 i로 전송하는 집계 단계; 및 상기 노드 i에 대해, 수신된 상기 제2 모델 파라미터와 상기 노드 j의 모델 파라미터를 이용하여 로컬 버전의 모델을 학습하고, 상기 노드 i의 모델 파라미터를 업데이 트하는 학습 단계를 포함 할 수 있다."}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은, 분산 시스템의 여러 개체에서 값을 수렴해나가는 방법인 Average based consensus protocol의 기법 중 하나인 Virtual topology-based Time Synchronization Protocol(VTSP) 방법을, 분산 연합학습에 적용하여, 학습 시간 단축을 통해 학습에 사용되는 자원을 최적화하는, 분산 연합학습 성능 개선 방법 및 장치에 관한 것 이다."}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "등록번호 10-2554676 (2023.07.07) \"무선 네트워크를 통한 베이지안 연합학습 구동 방법 및 시스템\" 등록번호 10-2554676은, 정량화된 기울기 정보를 최적으로 집계하기 위한 베이지안 연합학습 알고리즘을 제안하 는 기술에 관한 것이다. 공개번호 10-2023-0062092 (2023.05.09) \"기지국 부하 제어 방법 및 장치\" 공개번호 10-2023-0062092는, 기지국의 과부하 상태를 효과적으로 모니터링하고 부하를 분산할 수 있는 기지국 부하 제어 방법 및 장치에 관한 것이다. 공개번호 10-2023-0055644 (2023.04.26) \"연합 학습을 수행하는 엣지 클라우드 기반의 컴퓨팅 시스템\" 공개번호 10-2023-0055644는, 엣지 클라우드 기반 분산된 환경에서 계층적 인공지능 학습을 수행함으로써 보안 성을 향상시키고, 엣지 컴퓨팅 환경에서 실시간 발생하는 대량의 데이터를 빠르게 처리할 수 있도록 한 연합 학 습을 수행하는 엣지 클라우드 기반의 컴퓨팅 시스템에 관한 것이다. '연합학습(Federated Learning)'은, IoT 기기의 발전과 함께 증가하는 데이터를 효과적으로 처리하기 위한 개념 으로 등장하였다. 연합학습은, 기계 학습 및 딥러닝 분야에서 중요한 개념으로, 데이터를 공유하지 않고 분산된 여러 기기 또는 서버에서 연합하여 모델을 학습하는 기술이다. 연합학습은, 데이터를 중앙 집중화하지 않기 때문에, 개인 정보 보호 측면에서 중앙 서버가 개별 장치의 데이터 를 확인할 수 없으므로, 개인 정보 노출에 대한 우려를 줄일 수 있는 이점이 있다. 또한, 연합학습은, 단일 장치에서의 학습과 달리, 다양한 위치에서 데이터를 합쳐서 사용하므로 데이터의 다양 성과 풍부성을 보장할 수 있으며, 이러한 분산된 데이터를 사용하여 보다 정확하고 효율적인 모델을 구축할 수 있다는 이점이 있다. 이러한 이점들로 인하여 연합학습은, 의료, 금융, 제조 등 다양한 산업에서 중요한 역할을 하고 있으며, 개인 정보 보호에 민감한 데이터를 다루는 다양한 응용 분야에서 활용되고 있다. 연합학습은, 오케스트레이션인 중앙 서버와, 학습에 참여하는 클라이언트 노드인 장치들로 구성될 수 있다. 연합학습에서 중앙 서버는, 모델의 초기 버전을 각 장치에 제공할 수 있다. 또한, 연합학습에 참여하는 장치들은, 각각의 로컬 데이터를 통해 모델을 학습하여 모델 파라미터를 업데이트 하고, 업데이트된 모델 파라미터를 중앙 서버로 보내게 된다. 이러한 과정을 통해, 연합학습은, 최종적으로 모델의 성능을 최적화하는 것을 목표로 한다. 종래에는, 중앙 서버를 기반으로 한 연합학습을 중심으로 많은 아이디어들이 제안되어 왔다. 하지만, 중앙 서버를 기반으로 한 연합학습인 중앙 집중형 연합학습(Centralized Federated Learning)은, 근래 들어 많은 문제점이 제기되고 있는 실정이다. 중앙 집중형 연합학습은, 모델 파라미터를 하나의 중앙 서버에서 관리하여 의존성이 높아짐으로써, 중앙 서버가 고장 나거나 오작동하게 되면 전체 학습 과정에 장애가 발생하는 단일 장애점 문제를 가지고 있다. 또한, 중앙 집중형 연합학습은, 수많은 장치에서 업데이트 한 모델 파라미터를 중앙 서버로 전송함에 있어서, 대규모 데이터를 처리해야 하기에 상당한 대역폭 부담과 병목현상이 발생하는 문제가 발생할 수 있다. 중앙 집중형 연합학습은, 이러한 문제들로 인해 모델이 수렴하는데 시간이 오래 걸리게 되고, 성능 저하가 발생 할 수 있다. 분산 연합학습(Decentralized Federated Learning)은, IoT 및 엣지 장치들의 발전과 함께, 중앙 집중형 연합 학습의 문제를 해결하기 위해 연구되고 있다. 분산 연합학습은, 중앙 집중형 연합학습과 달리, 중앙 서버 없이 인공지능 모델 훈련을 위한 분산 시나리오를 통해 학습에 참여하는 장치들이 이웃 장치들과 직접 통신하여 모델 파라미터를 교환하여 모델의 학습을 진행한 다. 분산 연합학습에서 학습에 참여하는 장치들은, 각자의 로컬 데이터를 통해 모델을 학습하여 모델 파라미터를 업 데이트하고, 업데이트된 모델 파라미터를 주변의 장치로 전송하게 된다. 분산 연합학습은, 학습 및 전송 과정이 끝나면, 주변의 장치로부터 전달 받은 모델 파라미터들을 집계한 다음, 모델을 학습하게 되며, 이러한 과정을 모델의 수렴까지 반복하게 된다. 분산 연합학습은, 기존의 연합학습과 마찬가지로 장치에서 로컬 데이터를 통해 모델의 학습을 진행하므로, 데이 터를 외부로 전송하지 않아도 되기에, 개인 정보 보호에 대한 우려를 줄일 수 있다. 또한, 분산 연합학습은, 중앙 서버에 의존하여 업데이트 된 모델 파라미터를 배포하지 않고 장치 간의 모델 파 라미터의 교환을 통해 업데이트 된 모델 파라미터를 배포하기 때문에 단일 장애점 문제를 해결할 수 있으며, 네 트워크의 견고성을 향상시킬 수 있다. 분산 연합학습은, 신뢰성 및 중앙 서버의 병목 현상에 대한 문제를 개선할 수 있다. 분산 연합학습은 기존의 중앙 집중형 연합학습과 달리, 중앙 집중식 제어를 통한 중앙 서버에 의존하는 대신, 모델 파라미터를 집계하는데 필요한 컴퓨팅 성능을 학습에 참여하는 모든 장치에 분산시킴으로써 자원을 보다 효율적으로 사용할 수 있으며, 시스템의 규모가 커져도 더욱 효율적으로 대응할 수 있기에 스케일링과 확장성 측면에서의 장점이 있다. 이러한 장점들로 인해 분산 연합학습은, 개인 정보 보호와 보안 측면에서 우수한 선택지가 되고, 대규모 데이터 의 처리와 효율적인 모델의 학습을 진행할 수 있게 된다. 분산 연합학습은, 분산된 장치들이 협력하여 모델을 학습하여 민감한 데이터의 처리와 안정성, 신뢰성을 보장하 는데 매우 유용한 방법이다. 분산 연합학습은 이러한 이점에도 불구하고 통신 오버헤드 감소, 학습 최적화와 같은 과제들이 남아 있으며, 통 신 방법의 변경, 모델 간소화 및 압축 등을 통해 여러 가지 방법들이 제안되고 있다. 따라서, 분산 연합학습이 갖는 문제를 해소하는, 보다 개선된 방식의 등장이 절실히 요구되고 있다."}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 실시예는, Virtual topology-based Time Synchronization Protocol(VTSP)를 분산 연합학습에 적용하 여, 학습 시간 단축을 통해 학습에 사용되는 자원을 최적화하는, 분산 연합학습 성능 개선 방법 및 장치를 제공 하는 것을 해결과제로 한다. 또한, 본 발명의 실시예는, 분산화된 연합학습에서 추가적인 파라미터 교환을 통해 2-hop 거리에 있는 노드의 모델 정보를 얻어 학습시간을 최소화하는 것을 목적으로 한다."}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일실시예에 따른, 분산 연합학습 성능 개선 방법은, 노드 j에 대해, 주변의 노드 h들로부터 제1 모델 파라미터를 수신하고, 선정된 기간 경과 후, 수신된 복수의 상기 제1 모델 파라미터를 집계(Aggregation)하여 생성되는 제2 모델 파라미터를, 상기 노드 j의 모델 파라미터와 함께 다른 주변의 노드 i로 전송하는 집계 단계; 및 상기 노드 i에 대해, 수신된 상기 제2 모델 파라미터와 상기 노드 j의 모델 파라미터를 이용하여 로컬 버전의 모델을 학습하고, 상기 노드 i의 모델 파라미터를 업데이트하는 학습 단계를 포함할 수 있다. 또한, 본 발명의 실시예에 따른, 분산 연합학습 성능 개선 장치는, 노드 j에 대해, 주변의 노드 h들로부터 제1 모델 파라미터를 수신하고, 선정된 기간 경과 후, 수신된 복수의 상기 제1 모델 파라미터를 집계(Aggregation) 하여 생성되는 제2 모델 파라미터를, 상기 노드 j의 모델 파라미터와 함께 다른 주변의 노드 i로 전송하는 파라 미터 집계부; 및 상기 노드 i에 대해, 수신된 상기 제2 모델 파라미터와 상기 노드 j의 모델 파라미터를 이용하 여 로컬 버전의 모델을 학습하고, 상기 노드 i의 모델 파라미터를 업데이트하는 모델 학습부를 포함하여 구성할 수 있다."}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일실시예에 따르면, Virtual topology-based Time Synchronization Protocol(VTSP)를 분산 연합학습 에 적용하여, 학습 시간 단축을 통해 학습에 사용되는 자원을 최적화하는, 분산 연합학습 성능 개선 방법 및 장 치를 제공 할 수 있다. 또한, 본 발명의 일실시예에 따르면, 분산화된 연합학습에서 추가적인 파라미터 교환을 통해 2-hop 거리에 있는 노드의 모델 정보를 얻어 학습시간을 최소화 할 수 있다."}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서, 첨부된 도면을 참조하여 실시예들을 상세하게 설명한다. 그러나, 실시예들에는 다양한 변경이 가해 질 수 있어서 특허출원의 권리 범위가 이러한 실시예들에 의해 제한되거나 한정되는 것은 아니다. 실시예들에 대한 모든 변경, 균등물 내지 대체물이 권리 범위에 포함되는 것으로 이해되어야 한다. 실시예에서 사용한 용어는 단지 설명을 목적으로 사용된 것으로, 한정하려는 의도로 해석되어서는 안된다. 단 수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또 는 \"가지다\" 등의 용어는 명세서 상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 실시예가 속 하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의 미를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다. 또한, 첨부 도면을 참조하여 설명함에 있어, 도면 부호에 관계없이 동일한 구성 요소는 동일한 참조부호를 부여 하고 이에 대한 중복되는 설명은 생략하기로 한다. 실시예를 설명함에 있어서 관련된 공지 기술에 대한 구체적 인 설명이 실시예의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 도 1은 본 발명의 일실시예에 따른, 분산 연합학습 성능 개선 장치의 구성을 도시한 블록도이다. 도 1을 참조하면, 본 발명의 일실시예에 따른, 분산 연합학습 성능 개선 장치는, 파라미터 집계부, 모델 학습부, 및 배포부를 포함하여 구성 할 수 있다. 우선, 파라미터 집계부는, 노드 j에 대해, 주변의 노드 h들로부터 제1 모델 파라미터를 수신한다. 즉, 파 라미터 집계부는, 분산 연합학습에 참여하는 여러 클라이언트인 노드들 중에서, 주변의 노드들로부터 모델 파라미터를 수집하는 특정의 노드(여기서는 노드 j)를 지정하고, 지정된 노드 j에서, 정해진 기간 동안, 주변의 노드 h로부터 제1 모델 파라미터를 수신하는 역할을 할 수 있다. 여기서, 제1 모델 파라미터는, 노드 h에서, 센싱하여 획득한 데이터 등의 로컬 데이터를 이용하여 로컬 버전의 모델을 학습 함에 따라, 상기 로컬 버전의 모델과 연관되어 업데이트되는 모델 파라미터일 수 있다. 노드 h는 실시간으로 생성되는 로컬 데이터를 이용하여, 이식된 로컬 버전의 모델을 학습시키고, 학습의 결과에 따라 업데이트되는 모델 파라미터를, 상기 제1 모델 파라미터로서, 주변의 노드에 배포할 수 있다. 또한, 파라미터 집계부는, 선정된 기간 경과 후, 수신된 복수의 상기 제1 모델 파라미터를 집계 (Aggregation)하여 생성되는 제2 모델 파라미터를, 상기 노드 j의 모델 파라미터와 함께 다른 주변의 노드 i로 전송한다. 즉, 파라미터 집계부는, 정해진 기간 동안 주변의 노드 h들로부터 수집한 복수의 제1 모델 파 라미터를 집계하여 제2 모델 파라미터를 생성하고, 생성된 제2 모델 파라미터와, 노드 j 자신이 보유하고 있는 모델 파라미터를, 노드 j의 주변에 위치하고 있는 다른 노드 i로 송신하는 역할을 할 수 있다. 제2 모델 파라미터의 생성에 있어, 파라미터 집계부는, 노드 h와 노드 j 간에 규정되는 연결 가중치를 활 용하는 수식을 통해, 상기 제2 모델 파라미터를 생성할 수 있다. 즉, 파라미터 집계부는, 상기 노드 h로부터 수신되는 상기 제1 모델 파라미터가 일 경우, [수식 3]을 만족하여 상기 제2 모델 파라미터 를 생성할 수 있다. [수식 3]"}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, 상기 은, 노드 j와 노드 h 간의 연결 가중치이고, 상기 는 라운드일 수 있다. 상기 연결 가중치 는, 노드 j와 노드 h 사이의, 데이터/정보 교환 빈도, 교환되는 데이터/정보의 중요도, 일중 접촉 횟수, 주종 관계 등에 따라, 예컨대 본 발명의 운영자에 의해 유연하게 규정될 수 있다. 모델 학습부는, 상기 노드 i에 대해, 수신된 상기 제2 모델 파라미터와 상기 노드 j의 모델 파라미터를 이 용하여 로컬 버전의 모델을 학습하고, 상기 노드 i의 모델 파라미터를 업데이트한다. 즉, 모델 학습부는, 노드 i에 이식되어 있는 로컬 버전의 모델에, 상기 제2 모델 파라미터와 노드 j의 모델 파라미터를 적용시켜 학 습 함으로써, 학습에 따른 노드 i의 모델 파라미터를 업데이트하는 역할을 할 수 있다. 노드 i의 모델 파라미터의 업데이트에 있어, 모델 학습부는, 노드 j와 노드 i 간에 규정되는 연결 가중치 를 활용하는 수식을 통해, 상기 노드 i의 모델 파라미터를 업데이트 할 수 있다. 즉, 모델 학습부는, 상기 노드 j의 모델 파라미터가 일 경우, [수식 4]를 만족하여 상기 노드 i의 모델 파라미터 를 업데이트 할 수 있다. [수식 4]"}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, 상기 은, 노드 i와 노드 j 간의 연결 가중치 일 수 있다. 상기 연결 가중치 는, 노드 i와 노드 j 사이의, 데이터/정보 교환 빈도, 교환되는 데이터/정보의 중요 도, 일중 접촉 횟수, 주종 관계 등에 따라, 예컨대 본 발명의 운영자에 의해 유연하게 규정될 수 있다. 실시예에 따라, 분산 연합학습 성능 개선 장치는, 상기 모델의 학습으로 인해, 상기 로컬 버전의 모델이, 글로벌 버전의 모델로 수렴될 때까지, 상기 집계 단계와 상기 학습 단계를 반복할 수 있다. 즉, 모델 학습부는, 상기 모델의 학습으로 인해, 노드 i에 이식되어 있는 로컬 버전의 모델이, 글로벌 버 전의 모델로 수렴될 때까지, 상기 제2 모델 파라미터 및 상기 노드 j의 모델 파라미터의 수신과, 상기 모델의 학습 및 상기 노드 i의 모델 파라미터의 업데이트를 반복할 수 있다. 배포부는, 상기 모델을 학습 함에 따라 업데이트되는 상기 노드 i의 모델 파라미터를, 상기 제1 모델 파라 미터로서 주변의 노드에 배포한다. 즉, 배포부는 업데이트된 모델 파라미터를 제1 모델 파라미터로서 주 변의 노드에 배포 함으로써, 상기 주변의 노드에서의, 모델 파라미터의 집계, 제2 모델 파라미터의 생성, 다른주변의 노드로의 제2 모델 파라미터의 전송, 및 상기 다른 주변의 노드에서의, 모델 학습, 모델 파라미터의 업 데이트 등이 계속적으로 반복되도록 하는 역할을 할 수 있다. 본 발명의 일실시예에 따르면, Virtual topology-based Time Synchronization Protocol(VTSP)를 분산 연합학습 에 적용하여, 학습 시간 단축을 통해 학습에 사용되는 자원을 최적화하는, 분산 연합학습 성능 개선 방법 및 장 치를 제공 할 수 있다. 또한, 본 발명의 일실시예에 따르면, 분산화된 연합학습에서 추가적인 파라미터 교환을 통해 2-hop 거리에 있는 노드의 모델 정보를 얻어 학습시간을 최소화 할 수 있다. Average-based consensus protocol은, 분산 시스템에서 여러 개체가 상호작용하여 공통의 값을 도출하는 프로토 콜로, 시간이 지남에 따라 모든 개체들이 동일한 값을 가지도록 조율하는 것을 목표로 하며, 데이터 합산, 분산 계산, consensus 도달 등 다양한 문제에 적용될 수 있다. Average-based consensus protocol은, 분산된 데이터의 평균 계산과 같이, 전체 시스템의 합의를 필요로 하는 상황에서 유용하게 사용될 수 있다. Average-based consensus protocol은, 분산 시스템의 안정성과 신뢰성을 향상시키는데 기여할 수 있다. Average-based consensus protocol은 다음과 같이 작동할 수 있다. Average-based consensus protocol은, 먼저 모든 개체들이 각자의 로컬 데이터나 무작위 값으로 초기화된 값을 주변 개체와 교환한다. Average-based consensus protocol은, 주변 개체로부터 받은 값들을 이용하여 평균을 계산하고, 계산된 평균을 기반으로 하여 자신의 값을 업데이트하는 과정을, 모든 개체들이 동일한 값을 가지게 될 때까지 반복하게 된다. VTSP는 average-based consensus protocol의 기법 중 하나로, 분산 환경에서 빠른 시간 동기화를 위해 제안되 었다. 시간 동기화는, 무선 센서 네트워크에서 필수적인 서비스로, 전송 스케줄링을 통한 안정적인 데이터 전송, 효율 적인 에너지 소비, 동기화된 논리 시간을 이용한 이벤트 로깅과 같은 다양한 작업에서 사용될 수 있다. 부분의 네트워크에서는, 독립적인 시간을 동기화하기 위해 consensus 방식에 의존한다. 기존의 분산 시간 동기 화 프로토콜은, 이웃 노드로부터 시간 정보가 담긴 메시지를 받게 되면, 각 이웃 노드들로부터 상대적 시간 차 이인 relative skew를 구한 뒤 이를 집계하여 시간 정보를 업데이트한다. 반면, VTSP는, 이웃 노드들로부터 전달받은 시간 메시지를 통해 relative skew를 구한 뒤, 이 정보들을 다시 한 번 또 다른 이웃 노드로 전송하게 된다. VTSP는, 이 정보를 전달 받은 후 시간 정보 업데이트를 진행한다. VTSP는, 이처럼 이웃 노드로부터 시간 정보를 받은 뒤 바로 업데이트를 진행하는 것이 아니라 집계하여 다시 전 송함으로써 한 번의 업데이트에 더 많은 노드의 시간 정보를 포함시킬 수 있다. VTSP는, 기존의 1-hop 거리에 있는 노드의 시간 정보만을 이용하다가 직접적으로 연결되어 있지 않은 2-hop 거 리에 있는 노드의 시간 정보를 업데이트에 포함시키게 되어 전체 시스템의 시간 동기화를 빠르게 진행할 수 있 다. 본 발명에 따른, 분산 연합학습 성능 개선 장치는, 분산 연합학습에 VTSP를 적용함으로써, 분산 연합학습 에 참여하는 여러 노드에서 gossip protocol 통신 방식을 기반으로 한 모델 파라미터의 교환을 통해 모델의 학 습 및 수렴을 진행할 수 있다. Gossip protocol은, 큰 규모의 분산 시스템이나 peer-to-peer 네트워크에서 정보를 효율적으로 전파하는 프로토 콜로, 중앙 서버가 없이 각 노드들이 상호 작용하여 정보를 교환하는 방식일 수 있다. Gossip protocol은 각 노드들이 주기적으로 랜덤하게 주변 노드들과 연결하여 정보를 서로 교환하는 방식으로, 정보를 자연스럽게 확산시켜 나가기 때문에, 정보를 빠르게 전파할 수 있고, 높은 신뢰성과 뛰어난 확장성을 갖 을 수 있다. 분산 연합학습 성능 개선 장치는, Gossip protocol의 특징을 적용한 분산 연합학습을 다음과 같이 진행할 수 있다.각 노드에서는 각자의 로컬 데이터를 통해 모델을 학습하며, 이때 학습 방법은, 확률적 경사 하강법(Stochastic Gradient Descent) 방식을 따른다. 이 후, 각 노드는, 모델의 학습을 통해 업데이트되는 모델 파라미터를 주변 노드로 Gossip protocol을 따라 전 송할 수 있다. 모델 파라미터를 전송 받은 노드들은, 수식 1에 따라, 주변 노드들의 모델 파라미터를 집계할 수 있다. [수식 1]"}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이 때, 은, 노드 의 연결된 이웃 노드의 수이고, 는 현재 학습의 라운드를 의미할 수 있다. 는, 네트워크에 따라 결정되는 연결 가중치 매트릭스로, 연결된 노드끼리 영향을 주는 정도에 따라 가중 치를 두어 표현될 수 있다. 는, 노드 와 간의 연결 가중치 값을 나타낼 수 있다. 는, 노드 의 모델 파라미터를 의미하며, 는, 노드 가 이웃 노드들로부터 얻은 모델 파라미터를 집계한 값을 나타낼 수 있다. 주변 노드들의 모델 파라미터를 집계한 후, 노드는, 자신의 데이터 셋을 통해 모델의 gradient값을 이용하여 확 률적 경사 하강법을 수행하여 모델을 학습할 수 있다. 확률적 경사 하강법을 수행하여 모델을 학습하는 과정은, 수식 2로 표현될 수 있다. [수식 2]"}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "는, learning rate를 의미하고, 는, 노드 의 데이터 셋을 의미할 수 있다. 는, 노드 의 파라미터와 데이터 셋을 이용하여 모델의 손실 함수의 gradient를 계산 한 값일 수 있다. 노드는, 집계된 모델 파라미터들과 자신의 모델 파라미터를 통해, 다음 라운드에 사용될 모델 파라미터를 업데 이트할 수 있다. 이 과정은, 로컬 데이터 셋과 신경망을 이용한 연산이므로, 파라미터 간의 연산인 [수식 2]에 비해 상대적으로 많은 시간과 자원을 필요로 할 수 있다. 위와 같은 정보 교환 및 로컬 학습을 진행하는 과정은, 분산 연합학습의 라운드라 지칭할 수 있다. 라운드는, 크게 이웃 노드와의 모델 파라미터 집계와 업데이트 과정으로 나눌 수 있다. 각 노드들은, 모델이 최적의 성능에 수렴할 때까지 이러한 라운드를 반복하여 수행할 수 있다. 본 발명의 분산 연합학습 성능 개선 장치는, average-based consensus protocol과 분산 연합학습, 모두 분산 시스템 상의 각 노드들이 정보 교환을 통해 하나의 목표를 수렴시켜 나간다는 점에 착안하여, 기존의 분산 연합학습에 VTSP 방식을 적용하여 성능을 향상시킬 수 있다. 기존의 분산 연합학습 알고리즘에서는, 주변 노드와 모델 파라미터를 교환하여 학습을 진행하며, 1-hop 거리에 있는 노드 간의 모델 파라미터 전송 후 바로 모델 집계 및 업데이트 과정을 진행하기에, 2-hop 거리에 있는 노 드의 정보를 모델에 반영하려면 최소 2번의 라운드가 소모되었다. 반면, 분산 연합학습 성능 개선 장치는, 분산 연합학습에 VTSP의 학습 방식을 적용하여, 한 번의 라운드에 2-hop 거리에 있는 노드의 정보를 반영할 수 있도록 하여, 학습 모델의 수렴에 걸리는 라운드 수를 최소화할 수 있다. VTSP에서는, 노드 간 시간 정보 전달이 이루어진 후 바로 업데이트를 진행하는 것이 아닌, 업데이트를 진행하기 전에 각 노드 간 상대적 시간 차이인 relative skew를 계산하고 이를 다시 전달해주는 과정을 추가해줌으로써 성능을 향상시킬 수 있다. 분산 연합학습 성능 개선 장치는, VTSP의 방식과 유사하게, 노드에서 이웃 노드들의 모델 파라미터를 전달 받아 집계한 후에 바로 모델을 학습하여 모델 파라미터를 업데이트하지 않고, 자신의 모델 파라미터와 집계된 모델 파라미터를 또 다른 이웃 노드로 전송해주는 과정을 추가함으로써, 한 번의 라운드를 진행할 때 기존의 방 식에 비해 정보 전달 과정을 추가하고 학습에 더 많은 정보가 반영될 수 있도록 한다. 분산 연합학습 성능 개선 장치는, 학습이 더 빠르게 진행될 수 있도록 하여 수렴에 필요한 라운드 수를 줄 이고 학습 과정 전체의 필요한 자원의 양을 줄이는 효과를 기대할 수 있다. 도 2는 본 발명의 분산 연합학습 성능 개선 장치에 따른 상세 절차를 설명하기 위한 도이다. 단계 210에서, 분산 연합학습 성능 개선 장치는, 라운드 에서, 노드 에 대해, 로컬 데이터 셋을 통 해 모델을 학습하고 모델 파라미터를 업데이트 할 수 있다. 단계 220에서, 분산 연합학습 성능 개선 장치는, 노드 에서 노드 로, 업데이트 된 로컬 모델 파라미 터 을 전송할 수 있다. 단계 230에서, 분산 연합학습 성능 개선 장치는, 노드 로부터 로컬 모델 파라미터 을 전송받은 노드 에 대해, 수식 3에 의거하여 모델 파라미터를 집계할 수 있다. [수식 3]"}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서, 은 노드 와 간의 연결 가중치를 나타내고, 는 전송 받은 노드 의 로컬 모델 파 라미터의 값이며, 는 전송 받은 로컬 모델 파라미터를 노드 에서 집계한 값을 나타낼 수 있다. 단계 240에서, 분산 연합학습 성능 개선 장치는, 노드 에서 집계된 값인 을, 노드 의 로 컬 모델 파라미터 와 함께, 또 다른 이웃 노드인 노드 로 전송할 수 있다. 단계 250에서, 분산 연합학습 성능 개선 장치는, 노드 에 대해, 모델을 학습하고, [수식 4]에 따라 모 델 업데이트를 진행할 수 있다. [수식 4]"}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "수식 4는 모델 파라미터 값들을 집계하는 식을 나타낼 수 있다. 는 노드 와 노드 의 연결 가중치 값을 의미할 수 있다. 는 전송 받은 모델 파라미터를 집계한 값을 나타낼 수 있다. 수식 4는, 노드 에서 파라미터 집계를 나타낸 과정으로, 노드 와 그 이웃 노드인 간의 모델 파라미터를 집계한 와 노드 의 파라미터인 가 이용될 수 있다. 따라서 노드 는, 자신의 모델 파라미터에 2-hop 거리에 있는 노드 의 값들을 포함할 수 있으며, 이후 집 계된 값과 자신의 모델 파라미터 값, 데이터 셋을 이용하여 수식 2와 동일한 확률적 경사 하강법을 통해 모델 파라미터 업데이트를 진행할 수 있다. 분산 연합학습 성능 개선 장치는, 기존의 방식에 비해 추가적인 집계 과정과 전송 과정을 거치므로 한 번 의 라운드를 진행하는데 시간은 오래 걸리지만, 모델 파라미터 집계에 비해 상대적으로 자원과 시간을 소모하는 모델 업데이트 과정에 더 많은 정보를 포함시켜 업데이트를 진행하므로 모델의 수렴까지 필요한 라운드 수 자체 를 줄일 수 있다. 분산 연합학습 성능 개선 장치는, average based consensus protocol 기법 중 하나인 VTSP 방식을 분산 연합학습에 적용하여 모델의 수렴에 필요한 라운드 수를 줄이는 효과를 볼 수 있다. 이를 통해, 분산 연합학습 성능 개선 장치는, 중앙 서버 없이 동작하는 분산 연합학습의 학습에 필요한 시 간을 줄이고, 전체적인 자원의 소모를 기존의 분산 연합학습에 비해 줄일 수 있다. 분산 연합학습 성능 개선 장치는, 기존의 Decentralized Federated learning에서의 모델의 수렴 속도를 개선할 수 있다. 분산 연합학습 성능 개선 장치는, 분산화된 연합학습에서 추가적인 파라미터 교환을 통해 2-hop 거리에 있 는 노드의 모델 정보를 얻을 수 있다. 이하, 도 3에서는 본 발명의 실시예들에 따른 분산 연합학습 성능 개선 장치의 작업 흐름을 상세히 설명한 다. 도 3은 본 발명의 일실시예에 따른, 분산 연합학습 성능 개선 방법을 도시한 흐름도이다. 본 실시예에 따른 분산 연합학습 성능 개선 방법은 분산 연합학습 성능 개선 장치에 의해 수행될 수 있다. 우선, 분산 연합학습 성능 개선 장치는, 노드 j에 대해, 주변의 노드 h들로부터 제1 모델 파라미터를 수신 한다. 단계은, 분산 연합학습에 참여하는 여러 클라이언트인 노드들 중에서, 주변의 노드들로부터 모델 파라미터를 수집하는 특정의 노드(여기서는 노드 j)를 지정하고, 지정된 노드 j에서, 정해진 기간 동안, 주변의 노드 h로부터 제1 모델 파라미터를 수신하는 과정일 수 있다. 여기서, 제1 모델 파라미터는, 노드 h에서, 센싱하여 획득한 데이터 등의 로컬 데이터를 이용하여 로컬 버전의 모델을 학습 함에 따라, 상기 로컬 버전의 모델과 연관되어 업데이트되는 모델 파라미터일 수 있다. 노드 h는, 실시간으로 생성되는 로컬 데이터를 이용하여, 이식된 로컬 버전의 모델을 학습시키고, 학습의 결과 에 따라 업데이트되는 모델 파라미터를, 상기 제1 모델 파라미터로서, 주변의 노드에 배포할 수 있다. 또한, 분산 연합학습 성능 개선 장치는, 선정된 기간 경과 후, 수신된 복수의 상기 제1 모델 파라미터를 집계(Aggregation)하여 생성되는 제2 모델 파라미터를, 상기 노드 j의 모델 파라미터와 함께 다른 주변의 노드 i로 전송한다. 단계는, 정해진 기간 동안 주변의 노드 h들로부터 수집한 복수의 제1 모델 파라미터 를 집계하여 제2 모델 파라미터를 생성하고, 생성된 제2 모델 파라미터와, 노드 j 자신이 보유하고 있는 모델 파라미터를, 노드 j의 주변에 위치하고 있는 다른 노드 i로 송신하는 과정일 수 있다. 제2 모델 파라미터의 생성에 있어, 분산 연합학습 성능 개선 장치는, 노드 h와 노드 j 간에 규정되는 연결 가중치를 활용하는 수식을 통해, 상기 제2 모델 파라미터를 생성할 수 있다. 즉, 분산 연합학습 성능 개선 장치는, 상기 노드 h로부터 수신되는 상기 제1 모델 파라미터가 일 경 우, [수식 3]을 만족하여 상기 제2 모델 파라미터 를 생성할 수 있다. [수식 3]"}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, 상기 은, 노드 j와 노드 h 간의 연결 가중치이고, 상기 는 라운드일 수 있다. 상기 연결 가중치 는, 노드 j와 노드 h 사이의, 데이터/정보 교환 빈도, 교환되는 데이터/정보의 중요도, 일중 접촉 횟수, 주종 관계 등에 따라, 예컨대 본 발명의 운영자에 의해 유연하게 규정될 수 있다. 계속해서, 분산 연합학습 성능 개선 장치는, 상기 노드 i에 대해, 수신된 상기 제2 모델 파라미터와 상기 노드 j의 모델 파라미터를 이용하여 로컬 버전의 모델을 학습하고, 상기 노드 i의 모델 파라미터를 업데이트한 다. 단계는, 노드 i에 이식되어 있는 로컬 버전의 모델에, 상기 제2 모델 파라미터와 노드 j의 모델 파라미터를 적용시켜 학습 함으로써, 학습에 따른 노드 i의 모델 파라미터를 업데이트하는 과정일 수 있다. 노드 i의 모델 파라미터의 업데이트에 있어, 분산 연합학습 성능 개선 장치는, 노드 j와 노드 i 간에 규정 되는 연결 가중치를 활용하는 수식을 통해, 상기 노드 i의 모델 파라미터를 업데이트 할 수 있다. 즉, 분산 연합학습 성능 개선 장치는, 상기 노드 j의 모델 파라미터가 일 경우, [수식 4]를 만족하 여 상기 노드 i의 모델 파라미터 를 업데이트 할 수 있다.[수식 4]"}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기서, 상기 은, 노드 i와 노드 j 간의 연결 가중치 일 수 있다. 상기 연결 가중치 는, 노드 i와 노드 j 사이의, 데이터/정보 교환 빈도, 교환되는 데이터/정보의 중요 도, 일중 접촉 횟수, 주종 관계 등에 따라, 예컨대 본 발명의 운영자에 의해 유연하게 규정될 수 있다. 실시예에 따라, 분산 연합학습 성능 개선 장치는, 상기 모델의 학습으로 인해, 상기 로컬 버전의 모델이, 글로벌 버전의 모델로 수렴될 때까지, 상기 집계 단계와 상기 학습 단계를 반복할 수 있다. 즉, 분산 연합학습 성능 개선 장치는, 상기 모델의 학습으로 인해, 노드 i에 이식되어 있는 로컬 버전의 모델이, 글로벌 버전의 모델로 수렴될 때까지, 상기 제2 모델 파라미터 및 상기 노드 j의 모델 파라미터의 수신 과, 상기 모델의 학습 및 상기 노드 i의 모델 파라미터의 업데이트를 반복할 수 있다. 또한, 분산 연합학습 성능 개선 장치는, 상기 모델을 학습 함에 따라 업데이트되는 상기 노드 i의 모델 파 라미터를, 상기 제1 모델 파라미터로서 주변의 노드에 배포할 수 있다. 즉, 분산 연합학습 성능 개선 장치 는 업데이트된 모델 파라미터를 제1 모델 파라미터로서 주변의 노드에 배포 함으로써, 상기 주변의 노드에 서의, 모델 파라미터의 집계, 제2 모델 파라미터의 생성, 다른 주변의 노드로의 제2 모델 파라미터의 전송, 및 상기 다른 주변의 노드에서의, 모델 학습, 모델 파라미터의 업데이트 등이 계속적으로 반복되도록 할 수 있다. 본 발명의 일실시예에 따르면, Virtual topology-based Time Synchronization Protocol(VTSP)를 분산 연합학습 에 적용하여, 학습 시간 단축을 통해 학습에 사용되는 자원을 최적화하는, 분산 연합학습 성능 개선 방법 및 장 치를 제공 할 수 있다. 또한, 본 발명의 일실시예에 따르면, 분산화된 연합학습에서 추가적인 파라미터 교환을 통해 2-hop 거리에 있는 노드의 모델 정보를 얻어 학습시간을 최소화 할 수 있다. 실시예에 따른 분산 연합학습 성능 개선 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형 태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이 터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매 체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같 은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저 장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어 지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 실시예의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치에 저장 될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 분산 연합학습 성능 개선 방법으로 저장되거나 실행될 수도 있다. 소프"}
{"patent_id": "10-2023-0106865", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "트웨어 및 데이터는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다.이상과 같이 실시예들이 비록 한정된 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가진 자라면 상기를 기초로 다양한 기술적 수정 및 변형을 적용할 수 있다. 예를 들어, 설명된 기술들이 설명된 분산 연합 학습 성능 개선 방법과 다른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 분산 연합학습 성능 개선 방법과 다른 형태로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하 여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 청구범위의 범위에 속한다."}
{"patent_id": "10-2023-0106865", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른, 분산 연합학습 성능 개선 장치의 구성을 도시한 블록도이다. 도 2는 본 발명의 분산 연합학습 성능 개선 장치에 따른 상세 절차를 설명하기 위한 도이다. 도 3은 본 발명의 일실시예에 따른, 분산 연합학습 성능 개선 방법을 도시한 흐름도이다."}
