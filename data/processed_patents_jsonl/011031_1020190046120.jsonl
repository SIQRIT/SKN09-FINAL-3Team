{"patent_id": "10-2019-0046120", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0122834", "출원번호": "10-2019-0046120", "발명의 명칭": "가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법 및", "출원인": "한국전자통신연구원", "발명자": "장수영"}}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법으로,실제 사물을 가상 환경에서 재현한 가상 사물에서 학습된 지능형 에이전트를 이용하여, 상기 실제 사물의 초기상태에 대한 초기 동작값을 결정하는 단계;상기 초기 동작값을 상기 실제 사물에 입력하여 상기 초기 상태의 다음 상태로서의 제1 상태를 획득하는 단계;상기 지능형 에이전트를 이용하여 상기 제1 상태에 대한 제1 동작값을 결정하는 단계;상기 실제 사물의 상태 변화가 상기 가상 사물의 상태 변화와 일치하도록 상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계; 및상기 제2 동작값을 상기 실제 사물에 입력하는 단계를 포함하는, 실제 사물의 움직임을 제어하는 방법."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에서,상기 초기 상태는,상기 실제 사물의 위치, 방향, 속도, 고도, 회전 중 적어도 하나를 포함하는, 실제 사물의 움직임을 제어하는방법."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에서,상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계는,미리 학습된 추가 행동 예측 모델을 이용하여, 상기 지능형 에이전트의 동작 오차를 보정하기 위한 추가 동작값을 획득하는 단계; 및상기 추가 동작값과 상기 제1 동작값을 이용하여 상기 제2 동작값을 획득하는 단계를 포함하는, 실제 사물의 움직임을 제어하는 방법."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "에서,상기 제1 상태를 획득하는 단계는,상기 초기 상태 및 상기 초기 동작값을 미리 학습된 상태 예측 모델에 입력하여 상기 제1 상태에 대한 예측값을획득하는 단계;획득된 예측값, 상기 초기 상태 및 상기 초기 동작값을 상기 추가 행동 예측 모델에 입력하여 상기 지능형 에이전트의 초기 동작 오차를 보정하기 위한 추가 동작값을 획득하는 단계;상기 초기 동작 오차를 보정하기 위한 추가 동작값을 이용하여 상기 초기 동작값을 보정하는 단계; 및보정된 초기 동작값을 상기 실제 사물에 입력하여 상기 제1 상태를 획득하는 단계를 포함하는, 실제 사물의 움직임을 제어하는 방법."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에서,상기 추가 행동 예측 모델은,상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상태를 상기 가상 사물에 대해예측하는 포워드 인공신경망(forward neural network); 및상기 포워드 인공신경망에 의해 예측된 다음 상태 및 상기 제1 상태를 입력받아 상기 추가 동작값을 예측하여출력하는 인버스 인공신경망(inverse neural network)을 포함하는, 실제 사물의 움직임을 제어하는 방법.공개특허 10-2020-0122834-3-청구항 6"}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에서,상기 상태 예측 모델은,상기 실제 사물의 현재 상태 및 상기 현재 상태에서 상기 지능형 에이전트에 의해 판단된 동작값을 기초로 상기현재 상태의 다음 상태를 예측하도록, 실제 환경에 위치한 상기 실제 사물에서 미리 학습되는, 실제 사물의 움직임을 제어하는 방법."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 7에서,상기 상태 예측 모델은,상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상태를 상기 실제 사물에 대해예측하는 포워드 인공신경망(forward neural network)을 포함하는, 실제 사물의 움직임을 제어하는 방법."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 1에서,상기 방법은 명령어(instruction)로서 구현되고, 상기 실제 사물에 포함된 프로세서가 상기 명령어를 실행함으로써 수행되는, 실제 사물의 움직임을 제어하는 방법."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 1에서,상기 방법은 명령어로서 구현되고, 상기 실제 사물의 외부에 위치한 별도의 장치에 포함된 프로세서가 상기 명령어를 실행함으로써 수행되는, 실제 사물의 움직임을 제어하는 방법."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 장치로서,적어도 하나의 프로세서(processor); 및상기 적어도 하나의 프로세서가 적어도 하나의 단계를 수행하도록 지시하는 명령어들(instructions)을 저장하는메모리(memory)를 포함하고,상기 적어도 하나의 단계는,실제 사물을 가상 환경에서 재현한 가상 사물에서 학습된 지능형 에이전트를 이용하여, 상기 실제 사물의 초기상태에 대한 초기 동작값을 결정하는 단계;공개특허 10-2020-0122834-4-상기 초기 동작값을 상기 실제 사물에 입력하여 상기 초기 상태의 다음 상태로서의 제1 상태를 획득하는 단계;상기 지능형 에이전트를 이용하여 상기 제1 상태에 대한 제1 동작값을 결정하는 단계;상기 실제 사물의 상태 변화가 상기 가상 사물의 상태 변화와 일치하도록 상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계; 및상기 제2 동작값을 상기 실제 사물에 입력하는 단계를 포함하는, 실제 사물의 움직임을 제어하는 장치."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "청구항 11에서,상기 초기 상태는,상기 실제 사물의 위치, 방향, 속도, 고도, 회전 중 적어도 하나를 포함하는, 실제 사물의 움직임을 제어하는장치."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "청구항 11에서,상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계는,미리 학습된 추가 행동 예측 모델을 이용하여, 상기 지능형 에이전트의 동작 오차를 보정하기 위한 추가 동작값을 획득하는 단계; 및상기 추가 동작값과 상기 제1 동작값을 이용하여 상기 제2 동작값을 획득하는 단계를 포함하는, 실제 사물의 움직임을 제어하는 장치."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "청구항 13에서,상기 추가 행동 예측 모델은,사물의 연속된 2개의 상태 및 상기 사물의 연속된 상태 변화를 유도한 동작값을 기초로 상기 추가 동작값을 예측하도록, 상기 가상 사물에서 미리 학습되는, 실제 사물의 움직임을 제어하는 장치."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "청구항 14에서,상기 추가 행동 예측 모델은,상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상태를 상기 가상 사물에 대해예측하는 포워드 인공신경망(forward neural network); 및상기 포워드 인공신경망에 의해 예측된 다음 상태 및 상기 제1 상태를 입력받아 상기 추가 동작값을 예측하여출력하는 인버스 인공신경망(inverse neural network)을 포함하는, 실제 사물의 움직임을 제어하는 장치."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "청구항 14에서,상기 제1 상태를 획득하는 단계는,상기 초기 상태 및 상기 초기 동작값을 미리 학습된 상태 예측 모델에 입력하여 상기 제1 상태에 대한 예측값을획득하는 단계;획득된 예측값, 상기 초기 상태 및 상기 초기 동작값을 상기 추가 행동 예측 모델에 입력하여 상기 지능형 에이전트의 초기 동작 오차를 보정하기 위한 추가 동작값을 획득하는 단계;상기 초기 동작 오차를 보정하기 위한 추가 동작값을 이용하여 상기 초기 동작값을 보정하는 단계; 및공개특허 10-2020-0122834-5-보정된 초기 동작값을 상기 실제 사물에 입력하여 상기 제1 상태를 획득하는 단계를 포함하는, 실제 사물의 움직임을 제어하는 장치."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "청구항 16에서,상기 상태 예측 모델은,상기 실제 사물의 현재 상태 및 상기 현재 상태에서 상기 지능형 에이전트에 의해 판단된 동작값을 기초로 상기현재 상태의 다음 상태를 예측하도록, 실제 환경에 위치한 상기 실제 사물에서 미리 학습되는, 실제 사물의 움직임을 제어하는 장치."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "청구항 17에서,상기 상태 예측 모델은,상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상태를 상기 실제 사물에 대해예측하는 포워드 인공신경망(forward neural network)을 포함하는, 실제 사물의 움직임을 제어하는 장치."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "청구항 11에서,상기 장치는, 상기 실제 사물에 내장되거나 상기 실제 사물과 일체로서 구성되는, 실제 사물의 움직임을 제어하는 장치."}
{"patent_id": "10-2019-0046120", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "청구항 11에서,상기 장치는, 상기 실제 사물의 외부에 위치한 별도의 장치인, 실제 사물의 움직임을 제어하는 장치."}
{"patent_id": "10-2019-0046120", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법 및 장치가 개시된다. 가 상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법은, 실제 사물을 가상 환경 에서 재현한 가상 사물에서 학습된 지능형 에이전트를 이용하여, 상기 실제 사물의 초기 상태에 대한 초기 동작 값을 결정하는 단계, 상기 초기 동작값을 상기 실제 사물에 입력하여 상기 초기 상태의 다음 상태로서의 제1 상 태를 획득하는 단계, 상기 지능형 에이전트를 이용하여 상기 제1 상태에 대한 제1 동작값을 결정하는 단계, 상기 실제 사물의 상태 변화가 상기 가상 사물의 상태 변화와 일치하도록 상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계 및 상기 제2 동작값을 상기 실제 사물에 입력하는 단계를 포함할 수 있다. 가상 환경에서 학습된 지능형 에이전트를 그대로 실제 사물에 탑재하여 사용하더라도 동작 오차를 보정할 수 있다."}
{"patent_id": "10-2019-0046120", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법 및 장치에 관한 것으로, 더욱 상세하게는 동일한 입력에 대하여 실제 사물과 가상 사물이 동일한 상태 변화를 나타낼 수 있도록 가상 환경에 학습된 지능형 에이전트의 출력을 보정하여 실제 사물의 움직임을 제어하는 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2019-0046120", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근에는 드론, 자율주행차 등에는 스스로 상황을 인식하고 인식된 상황에 대하여 적절한 판단을 수행하고 동작 하는 지능형 에이전트(Intelligent Agent)가 탑재된다. 이러한 지능형 에이전트는 특정한 목적에 대하여 사용자 를 대신해 작업을 수행하는 자율적 프로세스(또는 그러한 프로세스를 수행하는 소프트웨어)라고 할 수 있다. 이때, 실제 환경에서 발생할 수 있는 모든 경우를 개발자가 고려하는 것은 불가능하거나 매우 어렵다. 따라서, 지능형 에이전트가 주어진 상황에 대하여 적절한 판단을 수행하는데 필요한 추론 규칙을 개발자가 직접 지정하 는 데에는 한계가 있다. 이러한 한계를 극복하기 위한 수단으로 최근에는 인공신경망(Neural Network)을 이용하 여 적절한 판단을 수행하는 지능형 에이전트에 대한 연구가 활발하다. 지능형 에이전트에 인공신경망을 사용하려면, 인공신경망을 학습하는 과정이 선행되어야 한다. 지능형 에이전트 가 탑재된 실제 사물을 동작시켜 인공신경망을 학습시킬 경우, 많은 시간과 비용이 소모되고 실제 환경에서 큰 사고를 초래할 가능성이 있다. 이 때문에 가상 환경에서 실제 사물과 동기화된 가상 사물을 동작시킴으로써 인 공신경망을 학습하는 방법이 대안으로 제시되고 있다. 실제 사물과 동일하게 동작하는 가상 사물을 재현할 경우, 가상 사물의 모델링 오차 뿐만 아니라 실제 환경에 대한 모델링 오차(예를 들면 노면 종류에 따른 마찰 계수 차이) 등이 발생한다. 종래에는 어려한 오차를 보정하 기 위하여 모델링을 최대한 정교하게 하고 직접 수동으로 모델링 매개변수를 조정하였다. 그러나, 이러한 종래의 방법은 실제 환경이 달라질때마다 개발자가 매번 오차를 보정해주어야 하고 오차 보정을 하더라도 실제 환경을 완벽하게 재현하기는 어려운 문제가 있다."}
{"patent_id": "10-2019-0046120", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상기와 같은 문제점을 해결하기 위한 본 발명의 목적은, 가상 환경에서 학습된 지능형 에이전트를 이용하여 실 제 사물의 움직임을 제어하는 방법을 제공하는 데 있다. 상기와 같은 문제점을 해결하기 위한 본 발명의 다른 목적은, 가상 환경에서 학습된 지능형 에이전트를 이용하 여 실제 사물의 움직임을 제어하는 장치를 제공하는 데 있다."}
{"patent_id": "10-2019-0046120", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명의 일 측면은, 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물 의 움직임을 제어하는 방법을 제공한다. 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법은, 실제 사물을 가상 환경에서 재현한 가상 사물에서 학습된 지능형 에이전트를 이용하여, 상기 실제 사물의 초기 상태에 대한 초기 동작값을 결정하는 단계, 상기 초기 동작값을 상기 실제 사물에 입력하여 상기 초기 상태의 다음 상태로서의 제 1 상태를 획득하는 단계, 상기 지능형 에이전트를 이용하여 상기 제1 상태에 대한 제1 동작값을 결정하는 단계, 상기 실제 사물의 상태 변화가 상기 가상 사물의 상태 변화와 일치하도록 상기 제1 동작값을 보정하여 제2 동작 값을 획득하는 단계 및 상기 제2 동작값을 상기 실제 사물에 입력하는 단계를 포함할 수 있다. 상기 초기 상태는, 상기 실제 사물의 위치, 방향, 속도, 고도, 회전 중 적어도 하나를 포함할 수 있다. 상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계(S130)는, 미리 학습된 추가 행동 예측 모델을 이용하 여, 상기 지능형 에이전트의 동작 오차를 보정하기 위한 추가 동작값을 획득하는 단계 및 상기 추가 동작값과 상기 제1 동작값을 이용하여 상기 제2 동작값을 획득하는 단계를 포함할 수 있다. 상기 추가 행동 예측 모델은, 사물의 연속된 2개의 상태 및 상기 사물의 연속된 상태 변화를 유도한 동작값을 기초로 상기 추가 동작값을 예측하도록, 상기 가상 사물에서 미리 학습될 수 있다. 상기 추가 행동 예측 모델은, 상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상 태를 상기 가상 사물에 대해 예측하는 포워드 인공신경망(forward neural network) 및 상기 포워드 인공신경망 에 의해 예측된 다음 상태 및 상기 제1 상태를 입력받아 상기 추가 동작값을 예측하여 출력하는 인버스 인공신 경망(inverse neural network)을 포함할 수 있다. 상기 제1 상태를 획득하는 단계는, 상기 초기 상태 및 상기 초기 동작값을 미리 학습된 상태 예측 모델에 입력 하여 상기 제1 상태에 대한 예측값을 획득하는 단계, 획득된 예측값, 상기 초기 상태 및 상기 초기 동작값을 상 기 추가 행동 예측 모델에 입력하여 상기 지능형 에이전트의 초기 동작 오차를 보정하기 위한 추가 동작값을 획 득하는 단계, 상기 초기 동작 오차를 보정하기 위한 추가 동작값을 이용하여 상기 초기 동작값을 보정하는 단계 및 보정된 초기 동작값을 상기 실제 사물에 입력하여 상기 제1 상태를 획득하는 단계를 포함할 수 있다. 상기 상태 예측 모델은, 상기 실제 사물의 현재 상태 및 상기 현재 상태에서 상기 지능형 에이전트에 의해 판단 된 동작값을 기초로 상기 현재 상태의 다음 상태를 예측하도록, 실제 환경에 위치한 상기 실제 사물에서 미리 학습될 수 있다. 상기 상태 예측 모델은, 상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상태를 상기 실제 사물에 대해 예측하는 포워드 인공신경망(forward neural network)을 포함할 수 있다. 상기 방법은 명령어(instruction)로서 구현되고, 상기 실제 사물에 포함된 프로세서가 상기 명령어를 실행함으 로써 수행될 수 있다. 상기 방법은 명령어로서 구현되고, 상기 실제 사물의 외부에 위치한 별도의 장치에 포함된 프로세서가 상기 명 령어를 실행함으로써 수행될 수 있다. 상기 목적을 달성하기 위한 본 발명의 다른 측면은, 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사 물의 움직임을 제어하는 장치를 제공한다. 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 장치는, 적어도 하나의 프로 세서(processor) 및 상기 적어도 하나의 프로세서가 적어도 하나의 단계를 수행하도록 지시하는 명령어들 (instructions)을 저장하는 메모리(memory)를 포함할 수 있다. 상기 적어도 하나의 단계는, 실제 사물을 가상 환경에서 재현한 가상 사물에서 학습된 지능형 에이전트를 이용 하여, 상기 실제 사물의 초기 상태에 대한 초기 동작값을 결정하는 단계, 상기 초기 동작값을 상기 실제 사물에 입력하여 상기 초기 상태의 다음 상태로서의 제1 상태를 획득하는 단계, 상기 지능형 에이전트를 이용하여 상기 제1 상태에 대한 제1 동작값을 결정하는 단계, 상기 실제 사물의 상태 변화가 상기 가상 사물의 상태 변화와 일 치하도록 상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계 및 상기 제2 동작값을 상기 실제 사물에 입 력하는 단계를 포함할 수 있다. 상기 초기 상태는, 상기 실제 사물의 위치, 방향, 속도, 고도, 회전 중 적어도 하나를 포함할 수 있다. 상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계(S130)는, 미리 학습된 추가 행동 예측 모델을 이용하 여, 상기 지능형 에이전트의 동작 오차를 보정하기 위한 추가 동작값을 획득하는 단계 및 상기 추가 동작값과 상기 제1 동작값을 이용하여 상기 제2 동작값을 획득하는 단계를 포함할 수 있다. 상기 추가 행동 예측 모델은, 사물의 연속된 2개의 상태 및 상기 사물의 연속된 상태 변화를 유도한 동작값을 기초로 상기 추가 동작값을 예측하도록, 상기 가상 사물에서 미리 학습될 수 있다. 상기 추가 행동 예측 모델은, 상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상 태를 상기 가상 사물에 대해 예측하는 포워드 인공신경망(forward neural network) 및 상기 포워드 인공신경망 에 의해 예측된 다음 상태 및 상기 제1 상태를 입력받아 상기 추가 동작값을 예측하여 출력하는 인버스 인공신 경망(inverse neural network)을 포함할 수 있다. 상기 제1 상태를 획득하는 단계는, 상기 초기 상태 및 상기 초기 동작값을 미리 학습된 상태 예측 모델에 입력 하여 상기 제1 상태에 대한 예측값을 획득하는 단계, 획득된 예측값, 상기 초기 상태 및 상기 초기 동작값을 상 기 추가 행동 예측 모델에 입력하여 상기 지능형 에이전트의 초기 동작 오차를 보정하기 위한 추가 동작값을 획 득하는 단계, 상기 초기 동작 오차를 보정하기 위한 추가 동작값을 이용하여 상기 초기 동작값을 보정하는 단계 및 보정된 초기 동작값을 상기 실제 사물에 입력하여 상기 제1 상태를 획득하는 단계를 포함할 수 있다. 상기 상태 예측 모델은, 상기 실제 사물의 현재 상태 및 상기 현재 상태에서 상기 지능형 에이전트에 의해 판단 된 동작값을 기초로 상기 현재 상태의 다음 상태를 예측하도록, 실제 환경에 위치한 상기 실제 사물에서 미리 학습될 수 있다. 상기 상태 예측 모델은, 상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상태를 상기 실제 사물에 대해 예측하는 포워드 인공신경망(forward neural network)을 포함할 수 있다. 상기 장치는, 상기 실제 사물에 내장되거나 상기 실제 사물과 일체로서 구성될 수 있다. 상기 장치는, 상기 실제 사물의 외부에 위치한 별도의 장치일 수 있다."}
{"patent_id": "10-2019-0046120", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기와 같은 본 발명에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법 및 장치를 이용할 경우에는 실제 사물과 가상 사물의 움직임 차이를 최소화할 수 있다. 또한, 실제 환경이 달라지더라도 가상 사물과 실제 사물의 움직임을 자동으로 동기화할 수 있는 장점이 있다."}
{"patent_id": "10-2019-0046120", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 각 도면을 설명하면서 유사한 참조부호를 유사한 구성요소에 대해 사용하였다. 제1, 제2, A, B 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어 들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있 고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 및/또는 이라는 용어는 복수의 관련된 기재된 항 목들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 본 출원에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조 합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의 미를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다. 이하, 본 발명에 따른 바람직한 실시예를 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법 및 장치에 대한 개념도이다. 본 발명에서는 가상 환경에 위치한 가상 사물을 통해 학습된 인공 신경망을 실제 사물에 이식할 경우 발생하는 오차를 해소하고, 가상 환경에서 학습된 인공 신경망을 그대로 실제 사물에 적용하기 위한 방법 및 장치를 제안 할 수 있다. 도 1을 참조하면, 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법 및 장치는, 가상 환경에 위치한 적어도 하나의 가상 사물(가상 사물#1, 가상사물 #2, 가상사물 #3, 10), 실제 환경에 위치한 적어도 하나의 실제 사물(실제사물#1, 실제사물#2, 실제사물#3, 20) 및 실제 사물의 움직임을 제어하는 장치를 통해 수행될 수 있다. 가상 사물은 실제 사물을 가상 환경에서 모델링하여 생성한 장치로서, 실제 사물의 동작과 상태를 모방 하고 가상 환경에서 실제 사물을 재현할 수 있다. 예를 들어, 도 1에서 가상사물#1은 실제사물#1을 모델링한 장 치로 서로 매칭될 수 있고, 가상 사물#2는 실제사물 #2를 모델링한 장치로 서로 매칭될 수 있으며, 가상사물#3 은 실제사물 #3를 모델링한 장치로 서로 매칭될 수 있다. 이때, 가상 사물에는 실제 사물에 탑재할 지 능형 에이전트(인공 신경망을 통해 현재 사물의 상태에서 사물의 동작값을 결정하는 소프트웨어 모듈일 수 있음)가 탑재되어있고, 가상 환경에서 가상 사물이 동작하면서 지능형 에이전트(또는 지능형 에이전트가 사 용하는 인공 신경망)에 대한 학습을 수행할 수 있다. 실제 사물은 가상 환경에서 학습된 지능형 에이전트(Intelligent Agent)가 탑재되고, 탑재된 지능형 에이전 트를 이용해 스스로 실제 환경에 대한 판단을 수행하고, 판단 결과에 따라 동작하는 각종 장치들로서, 드론, 자 율주행차, 로봇청소기 등이 해당할 수 있다. 실제 사물의 움직임을 제어하는 장치는, 가상 환경에서 학습한 지능형 에이전트가 실제 환경에 위치한 실제 사물에서 동작할 때 발생하는 오차를 보정하는 장치 또는 소프트웨어 모듈일 수 있다. 구체적으로, 실제 사 물의 움직임을 제어하는 장치는 실제 사물에 이식된 지능형 에이전트의 동작 명령을 획득하고, 획득된 동작 명령에 보상값을 추가하여 보정함으로써 보정 동작 명령을 생성할 수 있다. 보상기는 보정 동작 명령 을 실제 사물로 전송할 수 있다. 여기서 보상값은 가상 환경에 위치한 가상 사물의 동작과 실제 환경에 위치한 실제 사물의 동작 사이에서 발생 할 수 있는 오차를 보상(또는 제거)하는 값으로서, 지능형 에이전트가 학습하게 되는 가상 환경과 지능형 에이 전트가 판단을 수행해야 되는 실제 환경 사이의 차이, 가상 사물과 가상 환경에 대한 모델링 오차 등을 보 상하기 위한 값일 수 있다. 또한, 보정된 동작 명령은 실제 사물의 움직임을 유도하는 동작 명령값일 수 있 으나, 실제 사물이 가상 사물과의 오차를 상쇄 또는 제거하는데 필요한 정보(예를 들면 가상 사물과 실제 사물 사이의 상태 차이, 산출된 오차)일 수도 있다. 실제 사물은 실제 사물의 움직임을 제어하는 장치로부터 수신한 보정 동작 명령을 기반으로 동작을 수 행할 수 있다. 이처럼, 본 발명의 일 실시예에서는 다양한 실제 사물의 상태 변화에 따라 가상 사물을 모델링하 는 매개변수를 조정하고 가상 사물의 상태를 제어하는 대신에, 가상 사물에서 학습된 지능형 에이전트를 그대로 실제 사물에 이식하고, 실제 사물에 피드백 동작 명령을 입력하기 위한 보상기를 사용할 수 있다. 또한, 실제 사물의 움직임을 제어하는 장치는 실제 사물과 별도의 장치로서 기술하였으나, 실제 사물과 일체로서 구현될 수 도 있다. 도 1에 따른 본 발명의 일 실시예에 따르면, 가상 환경에서 학습된 지능형 에이전트를 그대로 실제 사물에 이식 하여 사용하더라도, 실제 사물의 움직임을 제어하는 장치가 보상값을 산출하여 실제 사물과 가상 사물의 상 태 오차를 제거하므로, 기존과 같이 가상 사물의 모델링을 지속적으로 수정하고 조정하는 번거로운 절차를 생략 할 수 있다."}
{"patent_id": "10-2019-0046120", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 2, "content": "도 2는 본 발명의 일 실시예에 따른 가상 사물의 구성을 기능적으로 도시한 블록도이다. 도 2를 참조하면, 가상 사물은 상태 모니터링부, 지능학습부 및/또는 동작제어부를 포함할 수 있다. 여기서 상태 모니터링부는 가상 환경에 위치한 가상 사물의 상태 정보(예를 들면 가상 사물의 온도, 위치, 고도, 방향, 속도, 회전 등) 및/또는 가상 환경에 대한 상태 정보(예를 들면 가상 환경에 대해 설정된 온도, 습도, 풍향, 풍속, 마찰력, 지열 등)를 모니터링하고 수집할 수 있다. 가상 사물과 가상 환경의 상태 정보를 통칭 하여 가상 상태 정보로 지칭할 수도 있다. 지능 학습부는 상태 모니터링부에 의해 수집된 가상 상태 정보를 입력으로 수신하고, 가상 상태 정보에 따른 최적의 동작 명령을 출력할 수 있다. 예를 들어, 지능 학습부는 인공 신경망(Neural Network)일 수 있 고, 더욱 상세하게는 합성곱 신경망(Convolutional Neural Network)일 수 있다. 또는, 지능 학습부는 도 1 에서 설명한 지능형 에이전트일 수 있다. 동작 제어부는 지능 학습부에서 출력된 동작 명령에 따라 가상 환경에서 가상 사물의 동작을 구현할 수 있다. 여기서 상태 정보는 가상 사물 또는 실제 사물이 동작을 수행함에 따라 발생한 사물 자체의 상태 변화(온도, 위 치, 고도, 방향, 속도, 회전 등의 변화)를 포함할 수 있다. 도 3은 본 발명의 일 실시예에 따른 실제 사물의 구성을 기능적으로 도시한 블록도이다. 도 3을 참조하면, 실제 사물은 상태 모니터링부, 지능학습부, 동기화부 및/또는 동작제어부 를 포함할 수 있다. 여기서 상태 모니터링부는 실제 환경에 위치한 실제 사물의 상태 정보(예를 들면 실제 사물의 온도, 위치, 고도, 방향, 속도, 회전 등) 및/또는 실제 환경에 대한 상태 정보(예를 들면 실제 환경에 대해 측정된 온도, 습 도, 풍향, 풍속, 마찰력, 지열 등)를 모니터링하고 수집할 수 있다. 실제 사물과 실제 환경의 상태 정보를 통칭 하여 실제 상태 정보로 지칭할 수도 있다. 지능 학습부는 상태 모니터링부에 의해 수집된 실제 상태 정보를 입력으로 수신하고, 실제 상태 정보에 따른 최적의 동작 명령을 출력할 수 있다. 예를 들어, 지능 학습부는 인공 신경망(Neural Network)일 수 있 고, 더욱 상세하게는 합성곱 신경망(Convolutional Neural Network)일 수 있다. 또는, 지능 학습부는 가상 환경에 위치한 가상 사물에서 학습된 인공 신경망(또는 인공 신경망을 이용한 지능 에이전트)이 가상 사물에 이 식된 것일 수 있다. 동기화부는 외부의 실제 사물의 움직임을 제어하는 장치로부터 입력받은 보정 동작 명령을 기초로 지능 학 습부로부터 출력된 동작 명령을 수정할 수 있다. 즉, 동기화부는 외부의 실제 사물의 움직임을 제어하 는 장치로부터 제공받은 보정 동작 명령을 이용하여 가상 환경에 위치한 가상 사물의 동작 결과와 실제 사물의 동작 결과가 서로 동일할 수 있도록 지능 학습부의 동작 명령을 수정할 수 있다. 여기서 동작 결과는 실제/ 가상 사물의 이동 방향, 이동 경로, 이동 거리, 높이, 속도, 공간 상 위치 등을 포함할 수 있다. 또한, 동기화부는 도 1에 따른 실제 사물의 움직임을 제어하는 장치가 소프트웨어 모듈형식으로 내장되어 있는 기능부일 수도 있다. 이러한 경우, 동기화부는 지능 학습부로부터 획득한 동작값 및 상태 모니터 링부로부터 획득한 상태 정보를 이용하여 보정 동작 명령을 생성하고, 보정 동작 명령을 동작 제어부에 입 력할 수 있다. 동작 제어부는 지능 학습부에 따른 동작 명령에 따라 실제 사물의 동작을 구현하되, 동기화부에서 출력된 보정 동작 명령이 있는 경우, 동기화부에서 출력된 동작 명령에 따른 동작을 우선적으로 수행할 수 있다. 예를 들어, 동작 제어부는 실제 사물에 장착된 관절이나 기어, 모터 등이거나 관절이나 기어 등에 입 력신호를 전달하는 장치일 수 있다. 도 4는 본 발명의 일 실시예에 따른 가상 사물과 실제 사물의 동작 차이를 보상하기 위한 방법을 설명하기 위한 개념도이다. 가상 환경에 위치한 가상 사물의 현재 상태가 s이고, 가상 사물이 어떠한 동작값 a를 수행하였을 때, 가상 사물 의 다음 상태가 ssim'가 된다고 하자. 또한, 실제 환경에 위치한 실제 사물의 현재 상태가 s이고, 실제 사물이 가상 사물과 동일한 동작값 a를 수행하였을 때, 실제 사물의 다음 상태는 sreal'가 된다고 하자. 이때, 가장 이상 적인 경우는 가상 사물의 다음 상태(ssim')가 실제 사물의 다음 상태(sreal')와 동일한 경우일 수 있다. 그러나, 일반적으로 가상 사물에서 학습된 지능 에이전트를 실제 사물에 이식시키고, 실제 사물을 동작시키면 가상 사물에서의 상태 변화와 실제 사물의 상태 변화에는 차이가 발생할 수 있다. 즉, 가상 사물과 실제 사물에 동일한 동작값(a)을 입력하였을 때, 가상 사물의 다음 상태(ssim')와 실제 사물의 다음 상태(sreal')는 서로 다를 수 있 다. 따라서, 이러한 상태 차이를 기반으로 실제 사물의 움직임을 보정하기 위해서 실제 사물의 다음 상태(sreal')에서 가상 사물의 다음 상태(ssim')로 실제 사물이 변화하기 위해 필요한 추가 동작값(adiff)을 예측하는 것이 필요할 수 있다. 또한, 실제 환경에 위치한 실제 사물은 현재 상태(s)에서 곧바로 가상 환경에서와 동일한 다음 상태(ssim')가 될 수 있어야 가상 사물에서 학습된 지능형 에이전트가 실제 사물에서도 올바르게 동작하는 것을 담보할 수 있다. 따라서, 실제 사물의 동작값을 보정하는 피드백 동작 명령을 생성하는 보상기(도 1에 따른 보상기일 수 있음)는 현재 입력된 동작값(a)과 추가 동작값(adiff)을 이용하여 실제 사물이 현재 상태(s)에서 가상 사물의 다음 상태 (ssim')와 동일하게 변화하기 위한 동작값(φ(a, adiff))을 산출할 수 있다. 즉, 본 발명의 일 실시예에 따르면, 실제 사물의 다음 상태(sreal')에서 가상 사물의 다음 상태(ssim')로 실제 사 물이 변화하기 위해 필요한 추가 동작값(adiff)을 예측하는 모델 및 실제 사물이 현재 상태(s)에서 가상 사물의 다음 상태(ssim')와 동일하게 변화하기 위한 동작값(φ(a, adiff))을 산출하는 보상기를 제안한다. 도 5는 본 발명의 일 실시예에 따른 가상 사물과 실제 사물의 동작 차이를 보상하기 위해 필요한 구성요소를 설 명하기 위한 개념도이다. 먼저, 가상 사물에서 학습된 지능형 에이전트가 실제 사물에서도 동일하게 동작하는 것을 보장하기 위한 첫번째 구성요소로서, 지능형 에이전트가 있다. 지능형 에이전트는 사물에 주어진 현재 상태(s)에서 사물이 수 행해야 하는 최적의 동작(a)를 결정할 수 있다. 이때, 지능 에이전트는 앞서 설명한 것처럼 가상 사물에 탑 재되어 가상 환경에서 미리 학습될 수 있고, 여기서 학습된 지능 에이전트는 가상 사물과 매칭되는 실제 사 물에 이식될 수 있다. 다음 구성요소로, 주어진 사물의 현재 상태에서 추가 동작값(adiff)을 예측하기 위한 추가동작 예측 모델이 있다. 여기서 추가 동작값은 도 4에서 설명한 것과 같이, 실제 사물과 가상 사물에 대하여 동일한 동작 입력값 을 부여했을 때 발생하는 다음 상태들(sreal', ssim') 간의 차이를 보정하기 위하여, 실제 사물의 다음 상태 (sreal')에서 가상 사물의 다음 상태(ssim')로 사물이 변화하기 위해 필요한 동작값을 의미할 수 있다. 이때, 추가 동작 예측 모델은 포워드 인공신경망(forward neural network, 52a) 및 인버스 인공신경망(Inverse neural network, 52b)를 포함할 수 있다. 포워드 인공신경망(52a)는 포워드 다이나믹스(forward dymanics)로도 지칭될 수 있고, 인버스 인공신경망(52b) 는 인버스 다이나믹스(inverse dynamics)로 지칭될 수도 있다. 여기서 포워드 인공신경망(52a)은 가상 환경에서 사물의 현재 상태(s)에 동작값(a)이 입력되었을 때 발생하는 다음 상태(ssim')를 예측하는 인공신경망일 수 있다. 따라서, 포워드 인공신경망(52a)는 사물의 현재 상태(s) 및 동작값(a)를 입력받아 가상 환경에서의 다음 상태(ssim')를 출력할 수 있다. 인버스 인공신경망(52b)은 가상 환 경에서의 다음 상태(ssim') 및 실제 환경에서 가상 환경과 동일한 동작값(a)이 입력되었을 때 발생하는 다음 상 태(sreal')를 입력받아 가상 환경과 실제 환경의 다음 상태들 간의 차이를 보정하기 위한 추가동작값(adiff)을 예 측하는 인공신경망일 수 있다. 이때, 인버스 인공신경망(52b)에 입력되는 가상 환경에서의 다음 상태는 포워드 인공 신경망(52a)에서 예측되어 출력된 값이 사용될 수 있다. 즉, 포워드 인공신경망(52a)의 출력은 인버스 인 공신경망(52b)에 입력될 수 있다. 여기서 추가행동 예측모델은 가상 환경에 위치한 가상 사물에 탑재되어 포워드 인공신경망(52a)과 인버스 인공신경망(52b)이 학습된 후, 추가행동 예측을 위해 사용될 수 있다. 추가동작 예측 모델은 이하에서, 가 상 환경에서 학습된다는 의미에서 가상 세계 다이나믹스 모델(Virtual-world dynamics model)로 지칭될 수도 있 다. 또한 추가행동 예측모델는 사물의 현재 상태(s)를 인공신경망에서 사용가능한 입력 포맷으로 변환하고,입력받을 수 있다(더욱 상세하게는 포워드 인공신경망에 대한 입력 포맷으로 변환될 수 있음). 지능형 에이전트에 의해 판단되는 행동값(a) 및 추가행동 예측 모델에 의해 예측되는 추가행동값(adif f)은 보상기로 전달될 수 있고, 보상기는 전달받은 행동값(a) 및 추가행동값(adiff)을 이용하여 실제 사물이 현재 상태(s)에서 가상 사물의 다음 상태(ssim')와 동일하게 변화하기 위한 동작값(φ(a, adiff))을 산출할 수 있다. 여기서 보상기에 의해 산출된 동작값이 실제 사물에 대한 피드백 동작 명령으로 입력될 수 있다. 즉, 실제 사물 은 피드백 동작 명령을 입력받으면, 보상기에 의해 산출된 동작값에 따른 동작 제어(관절, 기어, 모터 등)를 수 행할 수 있다. 도 6은 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법에 대한 제어 흐름도이다. 도 6을 참조하면, 먼저 실제 환경에 위치한 실제 사물의 초기 상태(Sreal0)를 기초로 지능형 에이전트 (Intelligent Agent)가 적합한 초기 동작값(a0)을 결정하고, 결정된 동작값에 따라 실제 사물이 1차 상태 (Sreal1)에 도달할 수 있다. 이때, 지능형 에이전트는 항상 실제 사물과 매칭되는 가상 사물을 이용하여 가상 환 경에서 미리 학습된 인공 신경망을 포함하도록 구성될 수 있다. 실제 사물이 1차 상태(Sreal1)에 도달하면, 다시 1차 상태(Sreal1)를 기초로 지능형 에이전트는 적합한 1차 동작값 (a1)을 결정하여 출력할 수 있다. 또한, 1차 상태(Sreal1), 초기 상태(Sreal0) 및 초기 동작값(a1)이 추가행동 예 측모델(또는 가상세계 다이나믹스 모델, Virtual-world dynamics model로 표기)에 입력되고, 추가행동 예측 모 델은 가상환경에서 학습된 지능형 에이전트의 동작 오차를 보정하기 위한 초기 추가 동작값(adiff0)을 출력할 수 있다. 이때, 추가행동 예측모델은 가상 환경에서의 가상 사물에 미리 탑재되어 학습된 후 사용될 수 있다. 보상기(Compensator)는 지능형 에이전트에서 출력된 1차 동작값(a1)과 추가 행동 예측 모델에서 출력된 초기 추 가 동작값(adiff0)을 입력받고, 실제 사물이 가상 환경에서의 상태 변화와 동일하게 변화하기 위한 동작값(φ(a1, adiff0))을 출력할 수 있다. 보상기에 의해 출력된 동작값(φ(a1, adiff0))에 따라 실제 사물이 동작하면, 실제 사 물은 2차 상태(Sreal2)에 도달할 수 있다. 실제 사물이 2차 상태(Sreal2)에 도달하면, 지능형 에이전트에 의해 동작값(a2)을 판단하고, 판단된 동작값(a2)을 보상기를 이용하여 보정함으로써, 3차 상태(Sreal3)에 도달할 수 있다. 이때에도, 보상기에 대한 입력으로 추가 행동 예측 모델에서 출력된 추가 동작값(Adiff1)이 사용될 수 있다. 이러한 과정은 실제 사물이 다음 상태로 동작 하는 과정마다 반복해서 수행될 수 있다. 도 5 및 도 6에서 설명한 지능형 에이전트, 추가행동 예측모델, 보상기는 실제 사물에 소프트웨어 모듈(또는 프 로세서에 의해 수행되는 명령어)로서 탑재될 수 있으나, 실제 사물과 별도의 외부 장치에 탑재되어 구동되고, 보상기의 출력이 실제 사물에 전달되는 방식으로도 구현될 수 있다. 한편, 도 6에서 확인될 수 있는 것과 같이 추가행동 예측 모델은 특정 상태와 특정 상태의 다음 상태를 입력받 아야 추가 동작값을 예측할 수 있다. 즉, 실제 사물의 초기 상태(Sreal0)에서는 실제 사물의 다음 상태(Sreal1)를 미리 알 수 없으므로 보상기를 통해 지능형 에이전트의 동작값(a0)을 보정할 수 없고, 동작 간격이 긴 경우, 초 기 상태에서 다음 상태로 이동되기까지의 시간동안 타임 랙(time lag)이 발생하는 문제가 있다. 따라서, 이하에서는 실제 사물의 다음 상태를 미리 예측할 수 있는 수단을 제공함으로써, 타임 랙(time lag)을 방지하는 방법을 설명한다. 도 7은 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법에서 초기 상태에 대한 다음 상태를 예측하는 수단을 설명하기 위한 제어 흐름도이다. 도 6에서는 실제 사물의 초기 상태에서 다음 상태를 미리 획득할 수 없어, 동작 실행 간격이 긴 경우 타입 랙이 발생할 가능성이 있었다. 이러한 문제를 해결하기 위한 수단으로서 실제 사물의 다음 상태를 예측하는 상태 예 측 모델을 추가로 사용할 수 있다. 구체적으로 도 7을 참조하면, 실제 사물의 초기 상태(Sreal0)를 기초로 지능형 에이전트(Intelligent Agent)는 실 제 사물이 수행하기에 적합한 동작값(a)을 출력할 수 있다. 또한, 도 6과 달리 도 7에서는 상태 예측 모델(Real Dynamics model로 표기)이 초기 상태(Sreal0)를 기초로 실제 사물의 다음 상태(Sreal')를 예측하여 출력할 수 있다. 이때, 상태 예측 모델은 실제 환경에서의 다음 상태를 예 측하는 모델인 점에서, 리얼 다이나믹스 모델(Real Dynamics model)로 지칭되고 표기될 수 있다. 추가행동 예측 모델(Virtual Dynamics model로 표기)은 실제 사물의 현재 상태, 상태 예측 모델로부터 예측된 다음 상태 및 지능형 에이전트에서 출력된 동작값을 입력받아 가상환경에서 학습된 지능형 에이전트의 동작 오 차를 보정하기 위한 추가 동작값(adiff)을 출력할 수 있다. 보상기는 추가행동 예측 모델로부터 출력된 추가 동작값과 지능형 에이전트에서 출력된 동작값을 이용하여 실제 사물이 가상 환경에서의 상태 변화와 동일하게 변화하기 위한 동작값(φ(a, adiff))을 출력할 수 있다. 보상기에 의해 출력된 동작값(φ(a, adiff))에 따라 실제 사물이 동작하면, 실제 사물은 1차 상태(Sreal1)에 도달할 수 있다. 도 8은 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법에서 초기 상태에 대한 다음 상태를 예측하는 수단의 구성을 나타낸 개념도이다. 도 8을 참조하면, 도 7에서 설명한 상태 예측 모델의 구성을 확인할 수 있다. 구체적으로, 본 발명의 일 실시예에 따른 상태 예측 모델은 도 5에서 설명한 포워드 인공신경망(forward neural network)를 포함할 수 있다. 여기서 포워드 인공 신경망은, 실제 환경에 위치한 실제 사물에 대한 현재 상태(s) 및 동작값(a)을 입력받고, 실제 사물의 다음 상태(sreal')를 예측하여 출력하는 인공신경망일 수 있다. 즉, 도 5 에서 설명한 포워드 인공신경망은 가상 환경에 위치한 가상 사물의 다음 상태를 예측하는 반면, 도 8에 따른 포 워드 인공신경망은 실제 환경에 위치한 실제 사물의 다음 상태를 예측할 수 있다. 따라서, 도 8에 따른 포워드 인공신경망(또는 상태 예측 모델)은 실제 환경에 위치한 실제 사물에 탑재되어 미 리 학습된 후 사용될 수 있다. 또한, 도 5에서의 추가 행동 예측 모델과 마찬가지로, 포워드 인공신경망(또는 상태 예측 모델)에 입력되는 현 재 상태(s)는 인공신경망에 대한 입력 포맷으로 변환(Transform)된 후 포워드 인공신경망에 입력될 수 있다. 한편, 도 5에 따른 지능형 에이전트, 추가행동 예측모델 및 도 8에 따른 상태 예측 모델에서 사용되는 인공신경 망은 VGG, ResNet, ResNext, Mobilenet 등을 비롯하여 다양한 딥러닝 기반 인공신경망이 사용될 수 있다. 또한, 지능형 에이전트, 추가행동 예측모델, 상태 예측 모델은 반드시 인공신경망이 사용되어야 하는 것은 아니며, 주 어진 입력에 대하여 목적한 출력값을 예측 또는 추정(approximation)할 수 있는 함수 연산일 수도 있다. 이때의 함수는 실험적 근사화를 통해 결정된 함수이거나, 수학적 또는 통계적 기법으로 결정되는 함수일 수 있다. 도 9는 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법에 대한 대표 흐름도이다. 도 9를 참조하면, 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법은, 실제 사물을 가상 환경에서 재현한 가상 사물에서 학습된 지능형 에이전트를 이용하여, 상기 실제 사물의 초기 상태에 대한 초기 동작값을 결정하는 단계(S100), 상기 초기 동작값을 상기 실제 사물에 입력하여 상기 초기 상 태의 다음 상태로서의 제1 상태를 획득하는 단계(S110), 상기 지능형 에이전트를 이용하여 상기 제1 상태에 대 한 제1 동작값을 결정하는 단계(S120), 상기 실제 사물의 상태 변화가 상기 가상 사물의 상태 변화와 일치하도록 상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계(S130) 및 상기 제2 동작값을 상기 실제 사물에 입 력하는 단계(S140)를 포함할 수 있다. 상기 초기 상태는, 상기 실제 사물의 위치, 방향, 속도, 고도, 회전 중 적어도 하나를 포함할 수 있다. 상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계(S130)는, 미리 학습된 추가 행동 예측 모델을 이용하 여, 상기 지능형 에이전트의 동작 오차를 보정하기 위한 추가 동작값을 획득하는 단계 및 상기 추가 동작값과 상기 제1 동작값을 이용하여 상기 제2 동작값을 획득하는 단계를 포함할 수 있다. 상기 추가 행동 예측 모델은, 사물의 연속된 2개의 상태 및 상기 사물의 연속된 상태 변화를 유도한 동작값을 기초로 상기 추가 동작값을 예측하도록, 상기 가상 사물에서 미리 학습될 수 있다. 상기 추가 행동 예측 모델은, 상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상 태를 상기 가상 사물에 대해 예측하는 포워드 인공신경망(forward neural network) 및 상기 포워드 인공신경망 에 의해 예측된 다음 상태 및 상기 제1 상태를 입력받아 상기 추가 동작값을 예측하여 출력하는 인버스 인공신 경망(inverse neural network)을 포함할 수 있다. 상기 제1 상태를 획득하는 단계(S110)는, 상기 초기 상태 및 상기 초기 동작값을 미리 학습된 상태 예측 모델에 입력하여 상기 제1 상태에 대한 예측값을 획득하는 단계, 획득된 예측값, 상기 초기 상태 및 상기 초기 동작값 을 상기 추가 행동 예측 모델에 입력하여 상기 지능형 에이전트의 초기 동작 오차를 보정하기 위한 추가 동작값 을 획득하는 단계, 상기 초기 동작 오차를 보정하기 위한 추가 동작값을 이용하여 상기 초기 동작값을 보정하는 단계 및 보정된 초기 동작값을 상기 실제 사물에 입력하여 상기 제1 상태를 획득하는 단계를 포함할 수 있다. 상기 상태 예측 모델은, 상기 실제 사물의 현재 상태 및 상기 현재 상태에서 상기 지능형 에이전트에 의해 판단 된 동작값을 기초로 상기 현재 상태의 다음 상태를 예측하도록, 실제 환경에 위치한 상기 실제 사물에서 미리 학습될 수 있다. 상기 상태 예측 모델은, 상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상태를 상기 실제 사물에 대해 예측하는 포워드 인공신경망(forward neural network)을 포함할 수 있다. 상기 방법은 명령어(instruction)로서 구현되고, 상기 실제 사물에 포함된 프로세서가 상기 명령어를 실행함으 로써 수행될 수 있다. 상기 방법은 명령어로서 구현되고, 상기 실제 사물의 외부에 위치한 별도의 장치에 포함된 프로세서가 상기 명 령어를 실행함으로써 수행될 수 있다. 도 10은 본 발명의 일 실시예에 따른 가상 사물과 실제 사물을 동기화하여 실제 사물의 움직임을 제어하는 장치 에 대한 하드웨어 구성도이다. 도 10을 참조하면, 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 장치 는, 적어도 하나의 프로세서(processor, 110) 및 상기 적어도 하나의 프로세서가 적어도 하나의 단계 를 수행하도록 지시하는 명령어들(instructions)을 저장하는 메모리(memory, 120)를 포함할 수 있다. 또한, 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 장치는, 유무선 네트워크를 통해 기지국과 통신을 수행하는 송수신 장치(transceiver, 130)를 포함할 수 있다. 또한, 가상 환경 에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 장치는 입력 인터페이스 장치 , 출력 인터페이스 장치, 저장 장치 등을 더 포함할 수 있다. 가상 환경에서 학습된 지능형 에 이전트를 이용하여 실제 사물의 움직임을 제어하는 장치에 포함된 각각의 구성 요소들은 버스(bus)에 의해 연결되어 서로 통신을 수행할 수 있다. 여기서 프로세서는 중앙 처리 장치(central processing unit, CPU), 그래픽 처리 장치(graphics processing unit, GPU), 또는 본 발명의 실시예들에 따른 방법들이 수행되는 전용의 프로세서를 의미할 수 있다. 메모리 및 저장 장치 각각은 휘발성 저장 매체 및 비휘발성 저장 매체 중에서 적어도 하나로 구성될 수 있다. 예를 들어, 메모리는 읽기 전용 메모리(read only memory, ROM) 및 랜덤 액세스 메모리 (random access memory, RAM) 중에서 적어도 하나로 구성될 수 있다. 상기 적어도 하나의 단계는, 실제 사물을 가상 환경에서 재현한 가상 사물에서 학습된 지능형 에이전트를 이용 하여, 상기 실제 사물의 초기 상태에 대한 초기 동작값을 결정하는 단계, 상기 초기 동작값을 상기 실제 사물에 입력하여 상기 초기 상태의 다음 상태로서의 제1 상태를 획득하는 단계, 상기 지능형 에이전트를 이용하여 상기 제1 상태에 대한 제1 동작값을 결정하는 단계, 상기 실제 사물의 상태 변화가 상기 가상 사물의 상태 변화와 일 치하도록 상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계 및 상기 제2 동작값을 상기 실제 사물에 입 력하는 단계를 포함할 수 있다. 상기 초기 상태는, 상기 실제 사물의 위치, 방향, 속도, 고도, 회전 중 적어도 하나를 포함할 수 있다. 상기 제1 동작값을 보정하여 제2 동작값을 획득하는 단계(S130)는, 미리 학습된 추가 행동 예측 모델을 이용하 여, 상기 지능형 에이전트의 동작 오차를 보정하기 위한 추가 동작값을 획득하는 단계 및 상기 추가 동작값과 상기 제1 동작값을 이용하여 상기 제2 동작값을 획득하는 단계를 포함할 수 있다. 상기 추가 행동 예측 모델은, 사물의 연속된 2개의 상태 및 상기 사물의 연속된 상태 변화를 유도한 동작값을 기초로 상기 추가 동작값을 예측하도록, 상기 가상 사물에서 미리 학습될 수 있다. 상기 추가 행동 예측 모델은, 상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상 태를 상기 가상 사물에 대해 예측하는 포워드 인공신경망(forward neural network) 및 상기 포워드 인공신경망 에 의해 예측된 다음 상태 및 상기 제1 상태를 입력받아 상기 추가 동작값을 예측하여 출력하는 인버스 인공신 경망(inverse neural network)을 포함할 수 있다. 상기 제1 상태를 획득하는 단계는, 상기 초기 상태 및 상기 초기 동작값을 미리 학습된 상태 예측 모델에 입력 하여 상기 제1 상태에 대한 예측값을 획득하는 단계, 획득된 예측값, 상기 초기 상태 및 상기 초기 동작값을 상 기 추가 행동 예측 모델에 입력하여 상기 지능형 에이전트의 초기 동작 오차를 보정하기 위한 추가 동작값을 획 득하는 단계, 상기 초기 동작 오차를 보정하기 위한 추가 동작값을 이용하여 상기 초기 동작값을 보정하는 단계 및 보정된 초기 동작값을 상기 실제 사물에 입력하여 상기 제1 상태를 획득하는 단계를 포함할 수 있다. 상기 상태 예측 모델은, 상기 실제 사물의 현재 상태 및 상기 현재 상태에서 상기 지능형 에이전트에 의해 판단 된 동작값을 기초로 상기 현재 상태의 다음 상태를 예측하도록, 실제 환경에 위치한 상기 실제 사물에서 미리 학습될 수 있다. 상기 상태 예측 모델은, 상기 초기 동작값 및 상기 초기 상태를 입력받아, 상기 초기 상태에 대한 다음 상태를 상기 실제 사물에 대해 예측하는 포워드 인공신경망(forward neural network)을 포함할 수 있다. 상기 장치는, 상기 실제 사물에 내장되거나 상기 실제 사물과 일체로서 구성될 수 있다. 상기 장치는, 상기 실제 사물의 외부에 위치한 별도의 장치일 수 있다. 도 11 내지 도 12는 본 발명의 일 실시예에 따른 가상 사물과 실제 사물을 동기화하여 실제 사물의 움직임을 제 어하는 방법 및 장치에 대한 응용 예시도이다. 도 11 내지 도 12를 참조하면, 실제 사물이 드론인 경우에 대하여 본 발명의 일 실시예에 따른 방법 및 장치가 적용된 예시를 확인할 수 있다. 먼저, 도 11을 참조하면, 실제 드론과 가상 환경에서 구현한 가상 드론의 프로펠러에 1 N·m의 토크를 주는 동 작 명령을 입력할 수 있다. 그러나, 동일한 동작 명령을 가상 드론과 실제 드론에 입력하더라도 모델링 오차 등 의 한계로 상태 차이가 발생할 수 있다. 예를 들어, 실제 드론이 이동한 고도 및 거리가 가상 드론이 이동한 고 도 및 거리와 다를 수 있다. 이러한 차이는 실제 환경에 존재하는 바람, 지열 등에 의해 발생하게 된다. 이처럼 실제 환경은 시간에 따라 계속해서 변할 수 있으므로 가상 환경에서 실제 환경을 완벽하게 모델링하는 것이 불 가능하다. 그러나, 본 발명의 일 실시예에 따른 방법 및 장치를 적용하면 실제 사물을 가상 사물과 동일한 상태 변화를 야 기하도록 조정하기 때문에 모델링 파라미터를 계속 조절하지 않더라도 가상 사물과 실제 사물의 상태를 동기화 시킬 수 있다. 즉, 도 12를 참조하면, 본 발명의 일 실시예에 따른 가상 사물과 실제 사물을 동기화하여 실제 사물의 움직임을 제어하는 장치는 가상 환경과 실제 환경에서 각각 동일한 행동 명령에 대한 상태 정보 변화를 학습하여 지능형 에이전트, 추가 행동 예측모델, 상태 예측모델, 보상기 등을 구성하고, 구성된 구성요소들을 이용하여 실제 사물의 상태 정보에 따른 보정 동작 명령을 입력할 수 있다. 예를 들어, 실제 드론이, 가상 드론에 1 N·m의 토크 를 입력했을 때의 상태(고도, 높이, 방향, 회전 등)와 동일한 상태에 도달할 수 있도록 실제 드론에는 1 N·m가 아니라, 보정된 동작 명령인 0.6 N·m를 입력할 수 있다. 본 발명에 따른 방법들은 다양한 컴퓨터 수단을 통해 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독 으로 또는 조합하여 포함할 수 있다. 컴퓨터 판독 가능 매체에 기록되는 프로그램 명령은 본 발명을 위해 특별 히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 매체의 예에는 롬(ROM), 램(RAM), 플래시 메모리(flash memory) 등과 같이 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함될 수 있다. 프로그램 명령의 예에는 컴파일러 (compiler)에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터(interpreter) 등을 사용해서 컴퓨 터에 의해 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 상술한 하드웨어 장치는 본 발명의 동작을 수행하 기 위해 적어도 하나의 소프트웨어 모듈로 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 또한, 상술한 방법 또는 장치는 그 구성이나 기능의 전부 또는 일부가 결합되어 구현되거나, 분리되어 구현될 수 있다. 상기에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특 허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다."}
{"patent_id": "10-2019-0046120", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법 및 장치에 대한 개념도이다. 도 2는 본 발명의 일 실시예에 따른 가상 사물의 구성을 기능적으로 도시한 블록도이다.도 3은 본 발명의 일 실시예에 따른 실제 사물의 구성을 기능적으로 도시한 블록도이다. 도 4는 본 발명의 일 실시예에 따른 가상 사물과 실제 사물의 동작 차이를 보상하기 위한 방법을 설명하기 위한 개념도이다. 도 5는 본 발명의 일 실시예에 따른 가상 사물과 실제 사물의 동작 차이를 보상하기 위해 필요한 구성요소를 설 명하기 위한 개념도이다. 도 6은 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법에 대한 제어 흐름도이다. 도 7은 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법에서 초기 상태에 대한 다음 상태를 예측하는 수단을 설명하기 위한 제어 흐름도이다. 도 8은 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법에서 초기 상태에 대한 다음 상태를 예측하는 수단의 구성을 나타낸 개념도이다. 도 9는 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 방법에 대한 대표 흐름도이다. 도 10은 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물의 움직임을 제어하는 장치에 대한 하드웨어 구성도이다. 도 11 내지 도 12는 본 발명의 일 실시예에 따른 가상 환경에서 학습된 지능형 에이전트를 이용하여 실제 사물 의 움직임을 제어하는 방법 및 장치에 대한 응용 예시도이다."}
