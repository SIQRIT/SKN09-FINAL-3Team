{"patent_id": "10-2024-7028161", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0002117", "출원번호": "10-2024-7028161", "발명의 명칭": "사용자가 제어하는 3차원 장면", "출원인": "쿤두 말레이", "발명자": "쿤두 말레이"}}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "3차원 장면에서 사용자의 가상 표현을 제어하기 위한 컴퓨터 구현 방법으로서, 이미징 유닛을 사용하여 사용자를 적어도 부분적으로 캡처한 사용자의 2차원 비디오 스트림 데이터를 수신하는단계;적어도 하나의 가상 객체와 3차원 장면 내의 해당 위치와 연관된 데이터를 포함하는 3차원 장면을 수신하는 단계;2차원 비디오 스트림 데이터에서 사용자의 사용자 표현을 분리하는 단계;2차원 비디오 스트림에서 사용자의 이미징 유닛에 대한 상대적 위치를 기반으로 식별되는 사용자의 위치 정보를식별하는 단계; 2차원 비디오 스트림에서 사용자의 포즈 정보를 식별하는 단계;위치 정보를 사용하여, 3차원 장면을 구성하는 복셀과 연관된 데이터를 수정하여 사용자 표현을 3차원 장면에추가하는 단계; 및3차원 장면과 추가된 사용자 표현을 디스플레이 유닛에 디스플레이하는 단계를 포함하고, 디스플레이 유닛에 디스플레이된 추가된 사용자 표현은 사용자와 위치 정보 중 적어도 하나의 변경을 감지한 것을 기반으로 제어되는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,사용자의 위치 정보를 식별하는 단계는 2차원 비디오 스트림에서 깊이 정보를 추출하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,깊이 정보를 추출하는 단계는 2차원 비디오 스트림에서 깊이 단서를 인식하는 것을 포함하는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,사용자 표현은 사용자의 적어도 일부의 이미지를 포함하는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,사용자 표현에서 누락된 신체 부위를 감지하는 단계;데이터 저장 유닛에서 누락된 신체 부위의 표현을 추출하는 단계; 및누락된 신체 부위의 추출된 표현을 사용자 표현에 추가하는 단계를 더 포함하는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,2차원 비디오 스트림 데이터에서 사람을 감지하는 단계;감지된 사람을 의도된 사용자와 비교하는 단계; 공개특허 10-2025-0002117-3-감지된 사람이 의도된 사용자인지 판별하는 단계; 및감지된 사람이 의도된 사용자라는 판별에 응답하여 감지된 사람을 사용자로 식별하는 단계를 더 포함하는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,사용자가 수행한 제스처 또는 포즈를 감지하는 것에 대한 응답으로 추가된 사용자 표현을 업데이트하는 단계를더 포함하고, 추가된 사용자 표현의 업데이트에는 사전 정의된 동작을 수행하는 추가된 사용자 표현이 포함되는방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,사용자, 위치 정보 및 포즈 정보 중 적어도 하나의 변경에 따라 3차원 장면에서 추가된 사용자 표현을 업데이트하는 단계를 더 포함하는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,변경에는 사용자가 추가된 사용자 표현이 가상 객체와 인터랙션하도록 하는 것이 포함되는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,사용자가 추가된 사용자 표현이 가상 객체와 인터랙션하도록 하는 것에 대한 응답하여 가상 객체를 업데이트하는 단계를 더 포함하고, 가상 객체를 업데이트하는 것에는 가상 객체가 사전 정의된 동작을 수행하도록 하는 것이 포함되는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,사용자가 추가된 사용자 표현이 제스처를 디스플레이하도록 하는 것에 대한 응답하여 가상 객체를 업데이트하는단계를 더 포함하고, 가상 객체를 업데이트하는 것에는 제스처를 감지하면 가상 객체가 사전 정의된 동작을 수행하도록 하는 것이 포함되는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "3차원 장면에서 사용자의 가상 표현을 제어하기 위한 컴퓨터 구현 방법으로서,제1 이미징 유닛을 사용하여 제1 사용자를 적어도 부분적으로 캡처한 제1 사용자의 첫 번째 2차원 비디오 스트림 데이터를 수신하는 단계;적어도 하나의 가상 객체와 3차원 장면 내의 해당 위치와 관련된 데이터가 포함된 3차원 장면을 수신하는 단계;제1 사용자의 제1 사용자 표현을 첫 번째 2차원 비디오 스트림 데이터에서 분리하는 단계;제1 사용자의 제1 이미징 유닛에 대한 상대적 위치를 기반으로 식별되는 제1 사용자의 제1 위치 정보를 첫 번째2차원 비디오 스트림에서 식별하는 단계;제2 이미징 유닛을 사용하여 제2 사용자를 적어도 부분적으로 캡처한 제2 사용자의 두 번째 2차원 비디오 스트림 데이터를 수신하는 단계;제2 사용자의 제2 사용자 표현을 두 번째 2차원 비디오 스트림 데이터에서 분리하는 단계;2차원 비디오 스트림에서 제2 사용자의 제2 이미징 유닛에 대한 상대적 위치를 기반으로 식별되는 제2 사용자의제2 위치 정보를 식별하는 단계;3차원 장면을 구성하는 복셀과 연관된 데이터를 수정하여 3차원 장면에 제1 및 제2 사용자 표현을 추가하는 단공개특허 10-2025-0002117-4-계; 및3차원 장면, 추가된 제1 사용자 표현, 추가된 제2 사용자 표현을 제1 디스플레이 유닛과 제2 디스플레이 유닛에디스플레이하는 단계를 포함하고, 제1 사용자 표현과 연관된 데이터는 제1 위치 정보를 사용하고 제2 사용자 표현과 연관된 데이터는 제2 위치 정보를 사용하며, 제1 디스플레이 유닛에 디스플레이된 제1 추가된 사용자 표현은 제1 사용자와 제1 위치 정보 중 적어도 하나에대한 변경을 감지한 것을 기반으로 제어되고, 제2 디스플레이 유닛에 디스플레이된 제2 추가된 사용자 표현은제2 사용자와 제2 위치 정보 중 적어도 하나에 대한 변경을 감지한 것을 기반으로 제어되는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제15항에 있어서,제2 사용자와 제2 위치 정보 중 적어도 하나의 변경에 따라 3차원 장면에서 제2 추가된 사용자 표현을 업데이트하는 단계를 더 포함하는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제16항에 있어서,변경에는 제2 사용자가 추가된 제2 사용자 표현이 추가된 제1 사용자 표현과 인터랙션하도록 하는 것이 포함되는 방법."}
{"patent_id": "10-2024-7028161", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "3차원 장면에서 사용자의 가상 표현을 제어하기 위한 시스템으로서,사용자의 2차원 비디오 스트림 데이터를 캡처하는 이미징 유닛; 컴퓨터 판독 가능 프로그램 명령어가 저장된 저장 유닛; 및저장 유닛과 통신하여, 컴퓨터 판독 가능 프로그램 명령어를 실행하여 시스템이 적어도:이미징 유닛에서 사용자를 적어도 부분적으로 캡처한 사용자의 2차원 비디오 스트림 데이터를 수신하고, 적어도 하나의 가상 객체와 3차원 장면 내의 해당 위치와 관련된 데이터가 포함된 3차원 장면을 수신하며,2차원 비디오 스트림 데이터에서 사용자의 적어도 일부의 이미지가 포함된 사용자의 사용자 표현을 분리하고, 이미징 유닛에 대한 사용자의 상대적 위치를 기반으로 식별되고, 2차원 비디오 스트림에서 추출된 깊이 정보가포함된 사용자의 위치 정보를 2차원 비디오 스트림에서 식별하며, 2차원 비디오 스트림에서 사용자의 포즈 정보를 식별하고,위치 정보 및 포즈 정보를 사용하여 3차원 장면을 구성하는 복셀과 관련된 데이터를 수정하여 3차원 장면에 사용자 표현을 추가하고, 3차원 장면과 추가된 사용자 표현을 디스플레이 유닛에 디스플레이하도록 구성된 프로세서를 포함하고, 디스플레이 유닛에 디스플레이된 추가된 사용자 표현은 사용자, 위치 정보 및 포즈 정보 중 적어도 하나의 변경을 감지한 것을 기반으로 제어되는 시스템."}
{"patent_id": "10-2024-7028161", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 일반적으로 3차원 가상 세계 내에서 사용자가 자신의 가상 표현을 제어하기 위한 시스템 및 방법에 관 한 것이다. 본 시스템 및 방법은 추출된 깊이 정보와 사용자의 2차원 이미지 또는 비디오 데이터를 활용하여 3차 원 장면에서 사용자 자신의 위치를 파악할 수 있도록 한다. 또한, 사용자가 자신의 가상 표현을 포함하는 3차원 공간에서 출력된 비디오를 시각적 피드백 메커니즘으로 사용하여 자신의 가상 표현을 제어할 수 있는 제어 시스 템 및 방법을 제공한다. 사용자는 장면의 다른 가상 객체 또는 아이템과 인터랙션하거나 장면에 시각화된 다른 사용자와도 인터랙션한다."}
{"patent_id": "10-2024-7028161", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 일반적으로 사용자가 3차원 가상 세계 내에서 자신의 가상 표현을 제어하기 위한 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2024-7028161", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "가상 현실 세계와 인터랙션하기 위해서는 현재 가상 현실(VR) 헤드셋을 사용하거나, 특수 3D 센서를 활용하거나, 키보드 및 컴퓨터에서 타이핑하거나, 휴대폰을 스와이프하여 가상 환경을 통과하고 인터랙션하는 아바타를 제어해야 한다. 본 개시는 사용자/액터의 이미지 또는 표현을 3차원 가상 세계에 삽입하는 데 사용될 수 있다. 그러면, 본 발명 은 사용자가 그 가상 세계 내에서 자신을 볼 수 있고, 자신의 이미지 또는 표현의 움직임과 자신이 보고 있는 가상 세계의 물체와의 인터랙션을 제어하는 방식으로 자신의 물리적 세계에서 움직일 수 있도록 할 수 있다. 본 개시는 액터가 다른 콘텐츠와 실시간으로 스크린 상에 존재해야 하지만, 가상 세계의 일부 객체와 겹치거나 가려지지 않도록 해야 하는 식으로 많은 상황에 유용한 시스템 및 방법을 제공한다. 이는 액터의 표현을 '장 면'에 삽입하여 수행된다. 이러한 장면은, 시청자의 관점에서, 일부 콘텐츠가 액터의 뒤에 있고 일부 콘텐츠가 액터의 앞에 있어 액터에 의해 가려지지 않는 다양한 상이한 레벨 또는 깊이 층의 콘텐츠로 구성된다. 장면은 X-Y-Z 3차원 공간에서 복셀(3차원 픽셀)로 구성될 수 있으며, 사용자의 표현은 장면에 포함된 복셀의 세 트로 삽입된다. 본 개시는 사용자(들)가 장면 내의 다른 객체 또는 아이템과 실시간으로 인터랙션하거나, 심지 어 다수의 사용자가 있는 경우 서로 인터랙션할 수 있는 시스템 및 방법을 제공한다. 현재, 배경 이미지 또는 비디오 위에 사람의 이미지를 배치하기 위해 그린 스크린 및 가상 그린 스크린 기술이 사용되는 경우가 많다. 저녁 뉴스의 날씨 코너에 등장하는 기상캐스터처럼, '기상캐스터' 효과는 피사체의 시야 를 가려서 사람이 배경의 일부를 가리지 않고 가리키기 위해 좌우로 움직여야만 한다. 이는 원격 프레젠테이션에서 특히 문제가 된다. 오늘날, 액터의 비디오 스트림은 일반적으로 콘텐츠와 완전히 분리된 윈도우에 디스플레이되므로, 액터가 (마우스 포인터 이외의 다른 도구로) 콘텐츠를 가리키고 청중과 더 효과적으로 소통하기가 더 어려워진다. 액터가 (예를 들어, 크로마키 그린 스크린을 사용함으로써) 콘텐츠 앞에 놓이면, 표현하려는 콘텐츠의 일부를 액터가 가린다."}
{"patent_id": "10-2024-7028161", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 내용에 포함됨."}
{"patent_id": "10-2024-7028161", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본원의 주제는 경우에 따라 상호 관련된 제품, 특정 문제에 대한 대체 솔루션 및/또는 단일 시스템 또는 물품의 복수의 다른 용도를 포함할 수 있다. 따라서, 본 개시는 다음의 실시예들을 포함하되, 이에 국한되지 않는다. 본 개시는 일반적으로 사용자가 3차원 가상 세계 내에서 자신의 가상 표현을 제어할 수 있는 시스템 및 방법에 관한 것이다. 본 시스템 및 방법은 추출된 깊이 정보를 갖는 사용자의 2차원 이미지 또는 비디오 데이터를 활용 하여 3차원 장면에서 자신의 위치를 파악할 수 있다. 일부 실시예는 3차원 장면에서 사용자의 가상 표현을 제어하기 위한 컴퓨터 구현 방법으로서, 이미징 유닛을 사 용하여 사용자를 적어도 부분적으로 캡처한 사용자의 2차원 비디오 스트림 데이터를 수신하는 단계; 적어도 하 나의 가상 객체와 3차원 장면 내의 해당 위치와 연관된 데이터를 포함하는 3차원 장면을 수신하는 단계; 2차원 비디오 스트림 데이터에서 사용자의 사용자 표현을 분리하는 단계; 2차원 비디오 스트림에서 사용자의 이미징 유닛에 대한 상대적 위치를 기반으로 식별되는 사용자의 위치 정보를 식별하는 단계; 2차원 비디오 스트림에서 사용자의 포즈 정보를 식별하는 단계; 위치 정보를 사용하여, 3차원 장면을 구성하는 복셀과 연관된 데이터를 수정하여 사용자 표현을 3차원 장면에 추가하는 단계; 및 3차원 장면과 추가된 사용자 표현을 디스플레이 유닛 에 디스플레이하는 단계를 포함하고, 디스플레이 유닛에 디스플레이된 추가된 사용자 표현은 사용자와 위치 정 보 중 적어도 하나의 변경을 감지한 것을 기반으로 제어되는 방법을 제공한다. 일부 실시예는 3차원 장면에서 사용자의 가상 표현을 제어하기 위한 컴퓨터 구현 방법으로서, 제1 이미징 유닛 을 사용하여 제1 사용자를 적어도 부분적으로 캡처한 제1 사용자의 첫 번째 2차원 비디오 스트림 데이터를 수신 하는 단계; 적어도 하나의 가상 객체와 3차원 장면 내의 해당 위치와 관련된 데이터가 포함된 3차원 장면을 수 신하는 단계; 제1 사용자의 제1 사용자 표현을 첫 번째 2차원 비디오 스트림 데이터에서 분리하는 단계; 제1 사 용자의 제1 이미징 유닛에 대한 상대적 위치를 기반으로 식별되는 제1 사용자의 제1 위치 정보를 첫 번째 2차원 비디오 스트림에서 식별하는 단계; 제2 이미징 유닛을 사용하여 제2 사용자를 적어도 부분적으로 캡처한 제2 사용자의 두 번째 2차원 비디오 스트림 데이터를 수신하는 단계; 제2 사용자의 제2 사용자 표현을 두 번째 2차원 비디오 스트림 데이터에서 분리하는 단계; 2차원 비디오 스트림에서 제2 사용자의 제2 이미징 유닛에 대한 상대 적 위치를 기반으로 식별되는 제2 사용자의 제2 위치 정보를 식별하는 단계; 3차원 장면을 구성하는 복셀과 연 관된 데이터를 수정하여 3차원 장면에 제1 및 제2 사용자 표현을 추가하는 단계; 및 3차원 장면, 추가된 제1 사 용자 표현, 추가된 제2 사용자 표현을 제1 디스플레이 유닛과 제2 디스플레이 유닛에 디스플레이하는 단계를 포 함하고, 제1 사용자 표현과 연관된 데이터는 제1 위치 정보를 사용하고 제2 사용자 표현과 연관된 데이터는 제2 위치 정보를 사용하며, 제1 디스플레이 유닛에 디스플레이된 제1 추가된 사용자 표현은 제1 사용자와 제1 위치 정보 중 적어도 하나에 대한 변경을 감지한 것을 기반으로 제어되고, 제2 디스플레이 유닛에 디스플레이된 제2 추가된 사용자 표현은 제2 사용자와 제2 위치 정보 중 적어도 하나에 대한 변경을 감지한 것을 기반으로 제어되 는 방법을 제공한다. 일부 실시예는 3차원 장면에서 사용자의 가상 표현을 제어하기 위한 시스템으로, 사용자의 2차원 비디오 스트림 데이터를 캡처하는 이미징 유닛; 컴퓨터 판독 가능 프로그램 명령어가 저장된 저장 유닛; 및 저장 유닛과 통신 하여, 컴퓨터 판독 가능 프로그램 명령어를 실행하여 시스템이 적어도: 이미징 유닛에서 사용자를 적어도 부분 적으로 캡처한 사용자의 2차원 비디오 스트림 데이터를 수신하고, 적어도 하나의 가상 객체와 3차원 장면 내의 해당 위치와 관련된 데이터가 포함된 3차원 장면을 수신하며, 2차원 비디오 스트림 데이터에서 사용자의 적어도 일부의 이미지가 포함된 사용자의 사용자 표현을 분리하고, 이미징 유닛에 대한 사용자의 상대적 위치를 기반으 로 식별되고, 2차원 비디오 스트림에서 추출된 깊이 정보가 포함된 사용자의 위치 정보를 2차원 비디오 스트림 에서 식별하며, 2차원 비디오 스트림에서 사용자의 포즈 정보를 식별하고, 위치 정보 및 포즈 정보를 사용하여 3차원 장면을 구성하는 복셀과 관련된 데이터를 수정하여 3차원 장면에 사용자 표현을 추가하고, 3차원 장면과 추가된 사용자 표현을 디스플레이 유닛에 디스플레이하도록 구성된 프로세서를 포함하고, 디스플레이 유닛에 디 스플레이된 추가된 사용자 표현은 사용자, 위치 정보 및 포즈 정보 중 적어도 하나의 변경을 감지한 것을 기반 으로 제어되는 시스템을 제공한다. 본 개시의 이러한 특징, 양상 및 장점들은 아래에 간략하게 설명되는 첨부 도면들과 함께 다음의 상세한 설명을 읽음으로써 명백해질 것이다. 본 개시는 본 개시에 명시된 그러한 특징 또는 요소가 본 명세서에 설명된 특정 실시예에서 명시적으로 결합되거나 달리 인용되는지 여부에 관계없이 본 개시에 명시된 특징 또는 요소의 2개, 3개, 4개 또는 그 이상의 임의의 조합을 포함한다. 본 개시는 본 개시의 모든 측면 및 실시예에서 본 개시의 임 의의 분리 가능한 특징 또는 요소가, 본 개시의 문맥이 달리 명시하지 않는 한, 결합 가능한 것으로 간주되어야 하도록 전체적으로 읽혀져야 한다."}
{"patent_id": "10-2024-7028161", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "따라서, 본 요약은 단지 본 개시의 일부 양상에 대한 기본적인 이해를 제공하기 위해 일부 실시예들을 요약하기 위한 목적으로만 제공되는 것으로 이해될 것이다. 따라서, 위에서 설명한 실시예들은 단지 예시일 뿐이며, 본 개시의 범위나 기술사상을 어떤 식으로든 좁히는 것으로 해석되어서는 안 된다는 것을 이해할 것이다. 다른 예 시 구현, 양상 및 장점은 설명된 일부 예시 구현의 원리를 예시적으로 설명하는 첨부 도면과 함께 결부해 다음 의 상세한 설명으로부터 명백해질 것이다."}
{"patent_id": "10-2024-7028161", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 내용에 포함됨."}
{"patent_id": "10-2024-7028161", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "첨부 도면 또는 도면과 관련하여 아래에 개시된 상세한 설명은 본 개시의 현재 바람직한 실시예에 대한 설명으 로서 의도된 것이며, 본 개시가 구성 및/또는 활용될 수 있는 유일한 형태를 나타내는 것은 아니다. 본 설명은 예시된 실시예들과 관련하여 본 개시의 구성 및 작동을 위한 기능 및 단계의 순서를 설명한다. 설명과 관련하여, 개시된 예들에 대한 완전한 이해를 제공하기 위해 구체적인 세부 사항이 명시되어 있다. 다른 예들에서, 본 개시가 불필요하게 길어지지 않도록 하기 위해, 잘 알려진 방법, 절차, 구성요소 및 재료는 상세 히 설명되지 않았다. 본 개시의 이러한 특징, 양상 및 장점들은 아래에 간략하게 설명된 첨부 도면과 함께 다음의 상세한 설명을 읽 음으로써 명백해질 것이다. 본 개시는 본 개시에 명시된 특징 또는 요소가 본 명세서에 설명된 특정 실시예에서 명시적으로 결합되거나 달리 인용되는지 여부에 관계없이 본 개시에 명시된 특징 또는 요소의 2개, 3개, 4개 또 는 그 이상의 임의의 조합을 포함한다. 본 개시는 본 개시의 모든 양상 및 실시예에서 본 개시의 임의의 분리 가능한 특징 또는 요소는, 본 개시의 문맥이 달리 명시하지 않는 한, 결합 가능한 것으로 간주되어야 하도록 전 체적으로 읽히도록 의도되었다. 본 명세서에서 어떤 요소 또는 부품이 다른 요소 또는 부품에 \"위에\", \"반대편에\", \"통신\", \"연결\", \"부착\" 또 는 \"결합\"된 것으로 언급되는 경우, 그 요소 또는 부품은 다른 요소 또는 부품에 직접적으로 위에, 반대편에, 통신, 연결, 부착 또는 결합될 수 있거나, 중간 요소 또는 부품이 존재할 수 있음을 이해해야 한다. 사용될 경 우, \"및/또는\"이라는 용어는, 있다면, 하나 이상의 관련 나열된 항목의 임의의 모든 조합을 포함한다. 본 명세서에서 사용되는 용어는 특정 실시예를 설명하기 위한 목적으로만 사용되며, 이를 제한하기 위한 것이 아니다. 본 명세서에서 사용되는 단수 형태인 \"a\", \"an\" 및 \"the\"는 문맥에서 명확히 달리 명시되지 않는 한 복 수 형태도 포함하는 것으로 의도되어 있다. 본 명세서에서 사용될 때, \"포함\" 및/또는 \"포함하는\"이라는 용어는 명시된 특징, 정수, 단계, 연산, 요소 및/또는 구성요소의 존재를 명시하지만, 명시적으로 언급되지 않은 하나 이상의 다른 특징, 정수, 단계, 연산, 요소, 구성요소 및/또는 이들의 그룹의 존재 또는 추가를 배제하지 않는 다는 것을 더 이해해야 한다. 또한, 달리 명시되지 않는 한, \"데이터\", \"콘텐츠\", \"디지털 콘텐츠\", \"정보\" 및 이와 유사한 용어들은 때때로 상호 교환적으로 사용될 수 있음을 이해해야 한다. 또한, 달리 명시되지 않는 한, \"실시간\"이라는 용어는 시스템의 처리 한계를 감안하여 의도적인 지연 없이 데이 터를 처리, 디스플레이, 캡처, 감지, 식별 또는 분석하는 것을 의미함을 이해해야 한다. 다양한 연산들은 실시예들을 이해하는 데 도움이 될 수 있는 방식으로 다수의 이산 연산들로 차례로 설명될 수 있지만, 설명의 순서가 이러한 연산들이 순서에 의존적이라는 것을 암시하는 것으로 해석되어서는 안 된다. \"밑\", \"아래\", \"하부\", \"위\", \"상부\", \"근위\", \"원위\", \"뒤\", \"앞\" 등과 같은 공간적으로 상대적인 용어들은, 다양한 도면에 예시된 바와 같이 하나의 요소 또는 특징과 다른 요소(들) 또는 특징(들)의 관계를 설명하기 위 해 설명 및/또는 예시의 용이성을 위해 본원에서 사용될 수 있다. 그러나, 공간적으로 상대적인 용어는 도면에 묘사된 방향 외에 사용 또는 작동 중인 장치의 다른 방향을 포함하도록 의도되었음을 이해해야 한다. 예를 들어, 도면에서 장치를 뒤집으면, 다른 요소 또는 특징의 \"아래\" 또는 \"밑에\"로 설명된 요소는 다른 요소 또는 특징의 \"위\"로 향하게 된다. 따라서, \"아래\"와 같은 상대적인 공간 용어는 위와 아래의 방향을 모두 포함할 수 있다. 장치는 다른 방향(90도 회전되거나 다른 방향)으로 배치될 수 있으며, 여기에 사용된 공간 상대 설명어는 그에 따라 해석되어야 한다. 마찬가지로, \"근위\"와 \"원위\"라는 상대적 공간 용어는 해당되는 경우 서로 바꿔 사 용할 수 있다. 이러한 설명은 단지 논의를 용이하게 하기 위해 사용될 뿐이며, 개시된 실시예들의 적용을 제한 하기 위한 것이 아니다. 제1, 제2, 제3 등의 용어는 본원에서 다양한 요소, 구성요소, 영역, 부분 및/또는 섹션을 설명하기 위해 사용될 수 있다. 이러한 요소, 구성요소, 영역, 부분 및/또는 섹션은 이러한 용어들에 의해 제한되어서는 안 된다는 것을 이해해야 한다. 이러한 용어는 하나의 요소, 구성요소, 영역, 부분 또는 섹션을 다른 영역, 부분 또는 섹션 과 구별하기 위해서만 사용되었다. 따라서, 아래에서 논의되는 제1 요소, 구성요소, 영역, 부분 또는 섹션은 본 원의 가르침에서 벗어남이 없이 제2 요소, 구성요소, 영역, 부분 또는 섹션이라고 할 수 있다. 본 개시의 일부 실시예들은, 일반적으로, 정보 및 명령어를 처리하기 위한 하나 또는 복수의 프로세서, 정보 및 명령어를 저장하기 위한 RAM, 정적 정보 및 명령어를 저장하기 위한 ROM, 정보 및 명령어를 저장하기 위한 자기 또는 광학 디스크 및 디스크 드라이브와 같은 데이터 저장 유닛, 프로세서에서 실행되는 소프트웨어 유닛으로서 의 모듈, 컴퓨터 사용자에게 정보를 디스플레이하기 위한 디스플레이 스크린 디바이스(예를 들어, 모니터) 및 선택적 사용자 입력 디바이스를 포함하는 컴퓨터 시스템에서 실행될 수 있다. 당업자에게 인식될 수 있듯이, 본 실시예들은 적어도 부분적으로, 컴퓨터 사용 가능한 프로그램 코드가 저장된 임의의 유형의 표현 매체에 구현된 컴퓨터 프로그램 제품으로 구현될 수 있다. 예를 들어, 방법, 장치(시스템) 및 컴퓨터 프로그램 제품의 순서도 도면 및/또는 블록 다이어그램을 참조하여 아래에 설명되는 일부 실시예는 컴퓨터 프로그램 명령어에 의해 구현될 수 있다. 컴퓨터 프로그램 명령어는 컴퓨터, 컨트롤러 또는 기타 프로그 래밍 가능한 데이터 처리 장치가 특정 방식으로 작동하도록 지시할 수 있는 컴퓨터 판독 가능 매체에 저장될 수 있으며, 컴퓨터 판독 가능 매체에 저장된 명령어는 순서도 및/또는 블록 다이어그램에 지정된 기능/행위/단계를 구현하는 명령어 및 프로세스를 포함하는 제조물품을 구성한다. 이러한 컴퓨터 프로그램 명령어는 머신을 생산 하는 범용 컴퓨터, 특수 목적 컴퓨터 또는 다른 프로그램 가능한 데이터 처리 장치의 프로세서에 제공되어, 컴 퓨터 또는 다른 프로그램 가능한 데이터 처리 장치의 프로세서를 통해 실행되는 명령어\\가 순서도 및/또는 블록 다이어그램 블록에 지정된 기능/행위를 구현하기 위한 수단을 생성한다. 이하의 설명에서는, 개시된 시스템이 실시될 수 있는 실시예의 예시인 첨부된 도면을 참조한다. 그러나, 당업자 는 본 개시의 신규성 및 범위에서 벗어나지 않고 다른 구조적 및 기능적 수정들을 개발할 수 있다는 것을 이해 해야 한다. 시스템은 시스템의 상이한 기능들을 수행하기 위해 함께 작동하는 하나 이상의 컴퓨터 또는 통신 중인 컴퓨터화 된 요소들을 포함할 수 있다. 본원에서 고려되는 시스템 및 방법은, 본원에 설명된 바와 같이, 시스템 및 방법 의 단계 및 기능을 수행하도록 컴퓨터 또는 컴퓨터들에 지시하도록 구성된 비일시적 컴퓨터 판독 가능 매체와 같은 저장 유닛을 더 포함할 수 있다. 일부 실시예들에서, 하나 이상의 컴퓨터 또는 하나 이상의 프로세서들 사 이의 통신은 다양한 유형의 데이터의 복수의 암호화/복호화 방법 및 메커니즘을 지원할 수 있다. 컴퓨터화된 사용자 인터페이스는 서로 네트워크로 통신하는 하나 이상의 컴퓨팅 디바이스로 구성될 수 있다. 본 명세서에서 고려되는 컴퓨터화된 사용자 인터페이스의 컴퓨터 또는 컴퓨터들은 메모리, 프로세서 및 입력/출력 시스템을 포함할 수 있다. 일부 실시예에서, 컴퓨터는 네트워크 연결 및/또는 디스플레이 스크린을 더 포함할 수 있다. 이러한 컴퓨터화된 요소들은 네트워크 내에서 함께 작동하여 컴퓨터화된 사용자 인터페이스에 기능을 제공할 수 있다. 컴퓨터화된 사용자 인터페이스는 사용자가 데이터를 입력하고 그로부터 피드백을 수신할 수 있 는 당업자에게 공지된 임의의 유형의 컴퓨터화된 인터페이스일 수 있다. 컴퓨터화된 사용자 인터페이스는 본 명 세서에서 고려되는 시스템에 의해 실행되는 출력을 더 제공할 수 있다. 본 명세서에서 고려되는 저장 유닛 또는 데이터는 임의의 연결 유형: 직렬, 이더넷 등을 통해 임의의 프로토콜: UDP, TCP 등을 통한, XML, JSON, CSV, 바이너리를 포함하되 이에 국한되지 않는 포맷일 수 있다. 본 개시에 고려되는 컴퓨터 또는 컴퓨팅 디바이스는 가상 시스템, 클라우드/원격 시스템, 데스크톱 컴퓨터, 랩 탑 컴퓨터, 태블릿 컴퓨터, 휴대용 컴퓨터, 스마트폰 및 기타 휴대 전화, 및 유사한 인터넷 지원 모바일 디바이 스, 디지털 카메라, 본 개시에 고려되는 방법을 구체적으로 수행하도록 구성된 맞춤형 컴퓨팅 디바이스 등을 포 함할 수 있지만 이에 국한되지 않는다. 본 명세서에서 고려되는 네트워크는, 예를 들어, 인터넷, 광역 네트워크(WAN), 근거리 통신망(LAN), 아날로그 또는 디지털 유선 및 무선 전화 네트워크(예를 들어, PSTN, 통합 서비스 디지털 네트워크(ISDN), 셀룰러 네트워 크 및 디지털 가입자 회선(xDSL)), 라디오, 텔레비전, 케이블, 위성 및 데이터를 전송하기 위한 다른 전달 또는 터널링 메커니즘 중 하나 이상을 포함할 수 있다. 네트워크는 여러 네트워크 또는 하위 네트워크를 포함할 수 있으며, 각 네트워크는 예를 들어 유선 또는 무선 데이터 경로를 포함할 수 있다. 네트워크에는 회로 전환 음성 네트워크, 패킷 전환 데이터 네트워크 또는 전자 통신을 전송할 수 있는 기타 네트워크가 포함될 수 있다. 예들 로, 인터넷 프로토콜(IP)을 통한 영상 전송 프로토콜(PTP), 블루투스를 통한 IP, 와이파이를 통한 IP 및 PTP/IP 네트워크(PTP/IP)가 포함되나 이에 국한되지 않는다.본 명세서에서 고려되는 이미징 유닛 또는 비디오 캡처 장치는 웹캠, 휴대폰 카메라, 깊이 감지 이미징 유닛(스 테레오스코픽 카메라, 함께 사용되는 다수의 카메라, 라이다로 보완된 2D 카메라, 깊이 감지 기술을 갖는 휴대 폰 상의 카메라 등을 포함하되 이에 국한되지 않음), DSLR, non-SLR 디지털 카메라(예를 들어, 컴팩트 디지캠 및 SLR과 유사한 브릿지 디지털 카메라(고급 디지털 카메라라고도 함), 및 SLR형 렌즈 교환식 디지털 카메라) 및 비디오 레코더(예를 들어, 캠코더, 2D 아날로그 카메라 및 2D IP 카메라, 3D 카메라 등; DVR과 같이 임의의 길이의 비디오 피드를 제공할 수 있는 디바이스; 태블릿 컴퓨터, 노트북 컴퓨터, 스마트폰과 같은 카메라를 갖 는 휴대용 컴퓨팅 디바이스) 등을 포함할 수 있지만 이에 한정되지는 않는다. 이미징 유닛은 액터의 정면, 액터 의 얼굴, 액터의 측면, 액터의 윗모습, 액터의 뒷모습, 액터의 사시도 등을 캡처하기 위해 마주볼 수 있다. 마 찬가지로, 이미징 유닛은 액터 또는 객체의 캡처된 영역을 변화시키기 위해 줌인 및 줌아웃할 수 있다. 일부 실시예에서, 시스템은 시스템 사용 시 사용자의 동작이 웨어러블 디바이스에 의해 방해받지 않도록 사용자 로부터 멀리 배치된 디스플레이를 활용할 수 있다. 마찬가지로, 시스템은 사용자의 동작이 그러한 웨어러블 디 바이스에 의해 제한되지 않도록 사용자로부터 멀리 배치되는 이미징 유닛을 포함할 수 있다. 본 명세서에서 고려되는 이미지 또는 비디오 파일은 컴퓨터 또는 컴퓨팅 디바이스에 의해 해석될 수 있는 임의 의 디지털 이미지 포맷일 수 있다. 본 명세서에서 고려되는 이미지 파일의 예로는 JPEG, GIF, TIFF, PNG, 비트 맵, RAW, PNM, WEBP 등이 포함되나, 이에 국한되지 않는다. 본 개시는 3차원 장면 내에서 사용자가 표현되고, 이미징 유닛(2차원 이미징 유닛을 포함)을 사용하여 3차원 장 면 내에서 사용자의 표현의 이동 및 인터랙션을 제어할 수 있는 시스템 및 방법을 제공한다. 보다 구체적으로, 본 개시는 해당 장면의 복셀을 수정하여 3차원 장면에 (예를 들어, 이미징 유닛에 의해 캡처된 2차원 비디오 스 트림에서 분리된) 사용자의 표현 또는 이미지를 추가한 다음, 해당 표현이 실제 사용자의 위치, 포즈 및/또는 제스처의 변화에 응답하여 (다른 사용자의 표현을 포함한) 3차원 장면의 가상 객체와 인터랙션하도록 하는 시스 템 및 방법을 제공한다. 최종적으로 생성된 3차원 장면은 디스플레이 유닛을 사용하여 볼 수 있다. 본 개시는 또한 일반적으로 멀티레이어 장면을 생성하기 위한 시스템 및 방법을 제공한다. 보다 구체적으로, 본 개시는 액터(디지털로 이미지화되거나 이미징 유닛에 의해 캡처된 사람)를 캡처하는 비디오와 같은 데이터 피드 (예를 들어, 하나 이상의 이미지, 비디오 스트림, 라이브 비디오 스트림)를 멀티레이어 장면에 주입 또는 삽입 하여 액터를 멀티레이어 장면의 하나 이상의 레이어 앞 및 멀티레이어 장면의 다른 레이어 뒤에 표시함으로써 깊이를 갖는 3차원 공간을 근사화하는 비디오(예를 들어, 합성 비디오)를 생성하기 위한 시스템 및 방법을 제공 한다. 본 개시는 다수의 미디어 입력(예컨대, 이미지, 비디오 등)을 수신하여 디스플레이 가능한 장면을 생성하 기 위해 실행될 수 있는 시스템 및 방법에 대해 설명하며, 디스플레이 가능한 장면은 다수의 그래픽 또는 시각 데이터의 레이어를 포함한다. 멀티레이어 장면의 최종 출력 비디오는 디스플레이 유닛을 사용하여 볼 수 있다. 무엇보다도, 시스템은 액터를 나타내는 데이터 피드와 같은 데이터 피드가 멀티레이어 장면의 적어도 일부 또는 하나 이상의 레이어들을 방해하지 않고 디스플레이 상에 공존, 배치 또는 디스플레이되도록 다양한 깊이(즉, 멀 티레이어 장면의 하나 이상의 레이어들의 앞, 뒤 또는 사이를 포함하여 멀티레이어 장면의 각 레이어)에 데이터 피드를 디스플레이, 주입 및/또는 배치한다. 또한, 시스템은 주입된 데이터 피드가 하나 이상의 레이어 중 적어 도 일부와 인터랙션할 수 있도록 한다. 예를 들어, 액터 또는 사용자를 데이터 피드로 캡처할 수 있으며, 액터 의 움직임은 액터 이미지 또는 비디오 피드가 주입된 멀티레이어 장면의 다른 레이어에 또는 다른 레이어에 의 해 표시되는 객체와 가상 인터랙션을 일으킬 수 있다. 이러한 멀티레이어 장면과 데이터 피드의 병합이 출력 비 디오 또는 출력 이미지로 디스플레이에 디스플레이된다. 비제한적인 예로서, 객체는 디스플레이에 의해 고체 객 체, 불투명 객체, 반투명 객체, 투명 객체 또는 임의의 다양한 투명도 또는 반투명도로 디스플레이되는 객체로 나타날 수 있다. 본 개시의 상세한 설명을 참조하여, 시스템 및 방법의 실시예들은 주로 액터 데이터 피드(즉, 사용자의 비디오 스트림과 같은 이미징 유닛에 의해 캡처되는 사용자와 같은 액터를 나타내는 데이터 피드)를 멀티레이어 장면에 디스플레이하는 것을 제시한다. 도면의 일부 세부 사항은 단순화되었으며 엄격한 구조적 정확성, 세부 사항 및 스케일을 유지하기보다는 이해를 돕기 위해 그려졌다는 점에 유의해야 한다. 특히, 사람(예를 들어, 사용자, 액 터 등)의 비디오 스트림은 촬영된 피사체 또는 대상의 실제 사진, 비디오 또는 기타 유형의 데이터 기록 대신 단순화된 도면이나 표현으로 설명된다. 이는 본 발명의 기능을 명확하게 설명하기 위한 것이다. 본 명세서에서 고려되는 시각적 표현은 주로 사람, 사물 또는 기타 눈에 보이는 사물의 실제 레코딩에 초점을 맞추고 있다. 도 면에서의 단순화된 특성화는 실제 구현에서 본 개시의 문자 그대로의 데모가 아니라 대표이다.본 명세서 및 청구범위에서 \"사용자\" 및 \"액터\"라는 단어는 상호 교환적으로 사용된다. \"실제\" 또는 \"물리적\"이라는 용어는 \"가상\"과는 대조적으로 \"비-가상\" 또는 \"실체적\" 또는 \"실제\"를 의미하기 위한 것이다. \"추가\" 및 \"위치\"라는 단어는 본 명세서에서 사용자, 액터 또는 액터 레이어와 관련하여 상호 교환적으로 사용 된다. 복셀 기반 3차원 장면에 \"추가\" 또는 \"위치\"와 관련하여, \"추가\" 또는 \"위치\"는 3차원 장면 내의 복셀(x- y-z 공간의 3차원 픽셀) 세트에 대한 데이터 또는 값의 수정을 포함하는 것으로 이해된다. \"멀티레이어\"라는 용어는 또한 \"다중 깊이\" 및 \"3차원\"을 의미하는 것으로 이해된다. \"포즈\"와 \"제스처\"는 인체의 상호 보완적인 특징으로서 매우 밀접하게 관련되어 있다. \"포즈 추정 또는 인식\"은 인체의 형태 또는 자세를 인식 또는 식별하는 것으로, 팔, 다리와 같은 큰 신체 부위뿐만 아니라 손가락, 얼굴 점 및 눈과 같은 세밀한 부분의 위치와 방향을 모두 포함하는 것으로 이해될 수 있다. 제스처 인식에는 포즈 정 보에 대한 추가 해석이 포함된다. 예를 들어, 집게손가락을 펴고 다른 손가락을 말아 올린 손의 포즈 정보는 제 스처 인식을 통해 일반적인 가리키는 제스처로 해석될 수 있다. \"포즈 추정\" 또는 \"포즈 인식\" 등이 언급된 경 우, \"제스처 인식\"의 단계가 뒤따를 수 있는 것으로 이해된다. 마찬가지로, \"제스처 인식\"이 언급되는 경우, \" 포즈 추정\"의 단계로 진행되는 경우가 많다는 것이 이해된다. 일반적으로 알려진 포즈 추정 기법은 컨볼루션 신 경망과 히트맵을 사용하여 사용자의 포즈를 추정한다. 이제 도 1을 참조하면, 멀티레이어 장면에서 데이터를 표시하기 위한 시스템(이하 데이터 처리 시스템이라 고 함)은 이미징 유닛, 데이터 획득 유닛, 저장 유닛, 제어 유닛 및 디스플레이 유닛(11 0)을 포함한다. 이러한 시스템 구성 유닛은 컴퓨터 시스템의 프로세서 또는 처리 회로에서 실행되는 소프트웨어 유닛, 본 명세서에 설명된 방법을 실행하는 하나 이상의 하드웨어 구성요소, 또는 이들 둘의 임의의 조합을 나 타낸다. 도 1은 단일 시스템으로 그룹화된 시스템 구성요소 유닛들을 예시하고 있지만, 이러한 연관성은 단일 물리적 하드웨어 유닛(예컨대, 단일 컴퓨팅 디바이스에 형성된 개별 유닛)을 형성하는 것으로 제한되지 않으며, 본원에 설명된 방법을 집합적으로 수행하는 시스템 유닛들의 기능적 연관성을 나타낼 수도 있다. 이미징 유닛 은 이미지(들) 및 비디오(들)와 같은 데이터 피드를 캡처한다. 일부 예들에서, 이미징 유닛은 액터 (또는 사용자)의 이미지 또는 비디오를 기록하거나 캡처한다. 데이터 획득 유닛은 이미징 유닛에 의 해 캡처된 데이터를 제어하고 수신한다. 제어 유닛은 저장 유닛을 사용하여 본 명세서에 설명된 방법 을 처리 및 실행한다. 디스플레이 유닛은 데이터 후처리 결과물을 출력 영상으로 출력한다. 도 2 및 도 3은 본 시스템에 의해 다양한 데이터 유형(예컨대, 데이터 피드 및 다층(3차원) 입력 데이터 )이 처리되어 멀티레이어 장면을 포함하는 출력 비디오를 디스플레이하는 방법을 설명한다. 프로세스 는 순차적으로 설명되지만, 입력 데이터 중 하나가 다른 데이터 피드가 처리되는 프로세스 또는 진행에 종속되 지 않고 개별적으로 또는 병렬로 다른 입력 데이터를 처리할 수 있다. 도 2에 설명된 바와 같이, 시스템(데이터 처리 시스템이라고 함)은 데이터 처리 시스템의 데이터 획득 유닛을 통해 데이터 피드 및 다층(3차원) 입력 데이터를 수신한다. 비디오 스트림과 같은 데이터 피드가 이미징 유닛을 사용하여 획득된다. 데이터 획득 단계 동안, 입력 데이터(예컨대, 다층(3차원) 입력 데이터 및 합성 입력 데이터)가 데이터 획득 유닛에 의해 획득된다. 데이터 제어 유닛은 데이터 제어 단계()에 서 이미징 유닛 및 장면 레이어에서 캡처한 다양한 유형의 입력 데이터와 데이터를 식별하고 추출 및 병합한다. 마지막으로, 디스플레이 유닛은 출력 비디오로 렌더링된 멀티레이어 장면을 나타낸다. 일부 예들에서, 입력 데이터는 컴퓨터의 메모리 또는 데이터 저장 유닛에 포함된 3차원 입력 데이터 이다. 일부 예들에서, 입력 데이터는 적어도 개별적이지만 조정된 입력 장면 레이어들의 세트를 포함 하는 멀티레이어 입력 데이터이다. 조정된 입력 장면 레이어들을 갖는 멀티레이어 입력 데이터는 다 수의 레이어들을 갖는 시간 동기화된 입력 데이터(즉, 멀티레이어 입력 데이터)일 수 있다. 예를 들어, 멀티레 이어 입력 데이터는 시간 동기화된 배경 레이어 비디오 피드와 피사체 레이어 비디오 피드를 포함한 다. 일부 예들에서, 멀티레이어 입력 데이터의 장면 레이어는 입력 데이터에서 캡처된 다양한 객체들의 연 관성에 의해 조정될 수 있다. 예를 들어, 배경 레이어는 피사체 레이어에 의해 캡처된 다른 객체와 연관되거나 상관관계가 있는 객체를 포함할 수 있고, 그 반대의 경우도 마찬가지이다. 일부 예들에서, 입력 데이터는 고정 이미지, 슬라이드, 비디오 등과 같은 다양한 데이터를 포함하는 합성 입력 데이터이다. 도 3에 도시된 바와 같이, 합성 입력 데이터는 배경 레이어와 피사체 레이어(즉, 추출 레이어)를 추출하도록 처리된다. 이와 같이, 레이어들의 추출은 입력 데이터가 멀티레이어 입력 데이터와 같은 레이어 데이터를 포함하지 않을 때 필요할 수 있다. 일부 실시예들에서, 도 2에 설명된 바와 같이, 시스템은 마찬가지로 배경 레이어 및 피사체 레이어와 같은 다수의 장면 레이어들을 포함하는 멀티레이어 입력 데이터를 수신할 수 있다. 이 예에서, 멀티레이어 입력 데이터는 두 개의 레이어를 포함한다. 멀티레이어 입력 데이터는 배경 레이어와 피사체 레 이어의 모음일 수 있다. 이러한 다수의 레이어는 멀티레이어(3차원) 입력 데이터 장면의 3차원 효과 와 디테일을 향상시킨다. 마찬가지로, 멀티레이어 입력 데이터는 배경 레이어와 피사체 레이어 의 두 개의 개별 입력 데이터가 될 수 있다. 비디오 스트림과 같은 데이터 피드로부터, 시스템은 하나 이상의 액터를 캡처하는 이미지(또는 비디 오 클립과 같은 이미지의 모음)를 추출한다(즉, 액터 추출). 이러한 이미징 유닛에 의해 캡처된 데이터 피 드(예컨대, 비디오 스트림)는 액터의 적어도 일부를 포함하며, 액터의 적어도 일부를 포함하는 데이터 피 드의 부분을 분리하기 위해 시스템에 의해 처리된다. 일부 예에서, 액터의 추출 또는 분리 또는 분할은 크로마 키 그린 스크린, 가상 그린 스크린 기술, 골격/인간 포즈 인식/추정, 이미지 또는 비디오 스트림 내에서 액터를 분리하도록 훈련된 신경망 등을 사용할 수 있다. 데이터 피드 또는 비디오 스트림에서 액터를 추출하 거나 분리하기 위해 다양한 공지된 이미지 처리 및 비디오 처리 기술이 적용될 수 있다. 그런 다음, 추출된 액 터 표현은 분리된 액터를 포함하는 불투명 영역과 분리된 액터를 포함하지 않는 투명 영역으로 구성된 액터 레 이어에 추가된다. 일부 실시예에서, 추출된 액터 표현은 3차원 가상 장면 내의 적절한 3차원 위치 또는 복 셀(3차원 또는 \"볼류메트릭\" 픽셀)과 관련된 데이터를 수정하여 3차원 가상 장면에 추가된다. 예를 들어, 도 13a 및 도 13b의 3차원 장면을 참조하라. 일단 액터 레이어가 비디오 스트림과 같은 데이터 피드로부터 식별되고 멀티레이어가 멀티 레이어 입력 데이터 또는 합성 입력 데이터로부터 식별되면, 격리된 액터를 포함하는 액터 레이 어가 배경 레이어 앞 및 피사체 레이어 뒤에 추가되어, 3개의 레이어(즉, 멀티레이어 장면)를 구성하는 장면이 생성된다. 즉, 액터 레이어가 배경 레이어와 피사체 레이어 사이에 추가 된다. 시각적 또는 디스플레이 가능한 데이터를 묘사하는 적어도 세 개의 레이어, 즉 배경 레이어, 액터 레이어 및 피사체 레이어의 집합이 멀티레이어 장면을 형성한다. 멀티레이어 장면은 이미지(들) 및 비디오(들)와 같은 데이터의 하나 이상의 레이어드 시각적 표현을 말한다. 배경 레이어, 액터 레이어 및 피사체 레이어는 출력 비디오를 렌더링하기 위해 렌더링/병합(예를 들어, 평탄화)된 다음 디스플레이 유닛을 사용하여 디스플레이된다. 일부 예들에서, 장면 내의 레이어들의 평탄화는 결과 출력 비디오 또는 이미지의 임의의 영역이 그 영역의 최전방 레이어가 불투명한 경우 동일한 영역의 최전방 레이어로부터의 비디오 또는 이미지로 구성되도록 수행될 수 있다. 영역의 최전방 레이어가 반투명한 경 우, 출력 비디오 또는 이미지의 해당 영역은 최전방 레이어와 그 뒤에 있는 평탄화된 레이어(들)의 시각적 조합(예를 들어, 레이어들의 색상이 평균화되거나, 혼합되거나, 다른 방식으로 시각적으로 결합된)으로 재귀적 으로 구성된다. 디스플레이 유닛은 모니터, 프로젝터 휴대폰 디스플레이, 3차원 디스플레이, 증강 현실 안 경, 가상 현실 헤드셋, 다른 웨어러블에 의해 제공되는 디스플레이, 또는 시각적 프리젠테이션을 위해 이미지 또는 비디오를 디스플레이하는 임의의 다른 디바이스일 수 있다. 일부 실시예들에서, 도 13a 및 13b에 설명된 바와 같이, 시스템은 더 후방 객체들 및 더 전방 객체들 과 같이 3차원 공간 내의 복수의 위치들에 있는 (인터랙션 또는 수동적이고 움직이거나 경사 또는 언덕과 같은 환경의 고정 수동 특징을 포함한 고정된) 객체들을 포함하는 3차원 입력 데이터(\"3차원 장면\"이라고 도 함)를 수신할 수 있다. 이 예에서, 3차원 입력 데이터는 3차원 공간 내의 서로 다른 두 위치에 있는 객체를 포함한다. 3차원 입력 데이터는 3차원 공간 내에 위치한 객체들(1308 및 1310)의 모음일 수 있다. 다수의 위치에 있는 이러한 객체들은 3차원 입력 데이터의 인지된 풍부함과 디테일을 향상시킨다. 마찬가 지로, 3차원 입력 데이터는 매우 적은 수의 객체로 드문드문 채워질 수도 있다. 다른 속성 중에서도, 3차 원 객체(1308 및 1310)는 서로 다른 수준의 투명도 및 불투명도를 가질 수 있으며, 이는 렌더링 중에 고 려될 것이다. 3차원 입력 데이터는 저장 유닛에 저장될 수 있다. 데이터 처리 시스템의 구성요 소들은 다양한 통신 프로토콜을 통해 직간접적으로 통신한다. 비디오 스트림과 같은 데이터 피드로부터, 시스템은 하나 이상의 물리적 액터를 캡처하는 이미 지(또는 비디오 클립과 같은 이미지의 모음)를 추출한다. 이러한 이미징 유닛에 의해 캡처된 데이터 피드 (예컨대, 비디오 스트림)는 액터의 적어도 일부를 포함하며, 시스템에 의해 액터의 적어도 일부를 포함하 는 데이터 피드의 일부를 분리하기 위해 처리된다. 일부 예에서, 액터의 추출 또는 분리 또는 분할은 크로마키 그린 스크린, 가상 그린 스크린 기술, 골격/인간 포즈 인식/추정, 이미지 또는 비디오 스트림 내에서 액터를 분리하도록 훈련된 신경망 등을 사용할 수 있다. 데이터 피드 또는 비디오 스트림에서 액터를 추출하거 나 분리하기 위해 다양한 공지된 이미지 처리 및 비디오 처리 기법이 적용될 수 있다. 일단 추출된 액터 표현이 비디오 스트림과 같은 데이터 피드로부터 식별되고 3차원 장면(130 2)이 이용 가능하면, 추출된 액터 표현이 3차원 장면 내에 위치 지정된다. 위치 지정은 3차 원 공간/장면 내에서 적절한 복셀(또는 3차원 픽셀)의 데이터 또는 값을 수정하여 다양한 객체와 액터 표현을 포함하는 3차원 가상 장면/공간을 생성함으로써 수행된다. 즉, 액터 표현은 1308 및 1310과 같은 3 차원 장면 객체들 사이에 위치된다. (도면에서 점선으로 표시된 바와 같이) 위치 지정 및 업데이트 단계는 선택적으로, 위치/깊이 정보(깊이 또는 거리뿐만 아니라 측면 및 수직 위치, 즉 카메라에 대한 X, Y, Z 좌표를 포함한 카메라에 대한 액터의 물리 적 위치에 대한 정보) 및 비디오 스트림으로부터 식별한 액터의 물리적 포즈 및/또는 제스처에 대한 포즈 및/또는 제스처 정보를 포함하나 이에 국한되지 않는 추가적인 액터 정보를 입력으로 취할 수도 있다. 위 치/깊이 정보는 비디오 스트림에서 캡처된 사용자에 기초하여 결정될 수 있다. 포즈 및 제스 처 정보는 비디오 스트림에서 캡처된 사용자에 기초하여 식별될 수 있다. 업데이트의 일부로서, 단계에서, 시스템은 또한 액터의 위치, 포즈 또는 제스처에 기초하여 장면 을 업데이트하기 위해 추가 액터 정보를 입력으로 취하고 활용할 수 있다. 예를 들어, 위치/깊이 정보 및/또는 포즈 및 제스처 정보를 활용하여, 위치 및 업데이트 단계는 가상 액터가 3차원 장 면 내의 인터랙티브 객체와 접촉하는지 여부 및 그 방법을 결정하여 해당 동작을 트리거하는 것을 포함한다. 동 작이 트리거되면, 단계에서, 시스템은 그에 따라 3차원 장면을 업데이트하여 동작(예를 들어, 시각적 상 태 변경 등)을 반영한다. 예를 들어, 일 실시예에서, 가상 액터의 가상 위치 및 포즈가 가상 액터가 3차원 가상 공간에서 누를 때 색이 변경되는 버튼과 접촉하고 있음을 나타내는 경우, 시스템은 단계에서 버튼이 눌러 진 것으로 간주하고, 해당 동작을 트리거 및 실행하며, 버튼이 색이 변경되었음을 반영하도록 가상 3차원 공간 또는 장면을 업데이트한다. 다른 예로서, 다른 실시예에서, 물리적 액터가 마치 펜을 들고 있는 것처럼 손가락 으로 집는 제스처를 하는 것으로 판단되면, 단계에서 시스템은 물리적 액터의 물리적 위치를 가상 3차원 공간 내의 가상 액터의 대응하는 가상 위치로 매핑하며, 집는 제스처(즉, 펜 제스처)의 인식으로 인해 단계 에서 시스템은 가상 3차원 공간에서 가상 액터의 손(또는 손으로 들고 있는 가상 펜)의 가상 위치에 글씨 또는 도을 나타내도록 가상 3차원 공간을 업데이트한다. 이와 같이, 전술한 추가 액터 정보와 같은 사용자 의 시각적으로 인식 가능한 특정 변화는 시스템에 의해 활용되어 단계에서 설명된 바와 같이 위치 지정 또는 업데이트를 유발한다. 즉, 사용자는 시스템에 의해 제공되는 출력 비디오를 사용하여, 비디오 스트림 데이터 입력을 통해 시스템에 시각적으로 인식 가능한 변경을 도입함으로써 액터 표현을 제 어할 수 있다. 일부 실시예들에서, 이미징 유닛 또는 카메라는 비디오 스트림에 포함된 사용자의 전체 뷰를 제공하기 위해 사 용자를 완전히 포착하도록 배치된다. 일부 실시예들에서, 이미징 유닛 또는 카메라는 비디오 스트림에 포함된 사용자의 일부 뷰를 제공하기 위해 사 용자 자신의 적어도 일부를 캡처하도록 배치된다. 3차원 장면은, 한 관점에서 복수의 깊이 또는 거리에 있는 하나 이상의 객체가 있는 3차원 공간을 말한다. 더 먼 후방 객체, 액터 표현 및 더 가까운 전방 객체가 출력 비디오를 렌더링하기 위해 2D 투영(직교, 경사, 원근 등)에 의해 렌더링되고(단계), 디스플레이 유닛을 사용하여 디스플레이(22 4)된다. 디스플레이 유닛은 모니터, 프로젝터, 증강 현실 안경, 가상 현실 헤드셋, 다른 웨어러블에 의해 제공되는 디스플레이, 또는 시각적 프리젠테이션을 위해 이미지 또는 비디오를 디스플레이하는 임의의 다른 디 바이스일 수 있다. 디스플레이 디바이스가 3차원 콘텐츠를 수신할 수 있는 경우와 같은 일부 실시예에서, 렌더링은 생성된 3차원 장면을 디스플레이 디바이스로 직접 전달하여 디스플레이될 수 있도록 실질적으로 변경되지 않은 상태로 둘 수 있다. 컴퓨터 프로그램 명령어들은 시스템, 그 구성요소들, 또는 다른 프로그램 가능한 데이터 처리 장치가 특정 방식 으로 기능하도록 지시할 수 있는 컴퓨터 판독 가능 미디어에 저장될 수 있으며, 컴퓨터 판독 가능 미디어에 저 장된 명령어들은 본원에 제공되는 흐름도 및/또는 블록 다이어그램에 지정된 기능/행위/단계를 구현하는 명령어 및 프로세스를 포함하는 제조물품을 구성할 수 있다. 이러한 컴퓨터 프로그램 명령어는 시스템의 프로세서에 제 공될 수 있으며, 이러한 명령어는 흐름도 및/또는 블록 다이어그램 블록 또는 블록들에 명시된 기능/행위를 구 현하기 위한 수단을 생성한다. 이하의 설명에서는, 개시된 시스템이 실시될 수 있는 실시예의 예시인 첨부된 도면을 참조한다. 본 개시의 시스템은 본 개시에서 고려되는 방법들을 구체적으로 수행하도록 구성된다. 그러나, 당업자들은 본 개시의 신규성 및 범위를 벗어나지 않고 다른 구조적 및 기능적 수정들을 개발할 수 있다는 것을 이해해야 한다. 도 16에 도시된 바와 같이, 일부 실시예에서, 시스템은 가상 3차원 세계 내에서 물리적 사용자가 자신의 가상 자아(즉, 디스플레이 상에 생성된 가상 사용자 또는 사용자 표현)를 제어하고 가상 세계 내에서 보이는 가상 사 용자의 실시간 표현을 물리적 사용자에게 디스플레이함으로써 가상 세계 내의 객체와 인터랙션할 수 있게 하는 사용자 제어 메커니즘으로서 동작한다. 사용자가 인터랙션하고자 하는 객체를 보는 범위 내에서, 사용자는 물리 적 세계에서 자신의 신체를 움직일 수 있어(또는 유사한 효과를 내는 제스처를 취할 수 있어), 가상 사용자 표 현이 가상 세계 내에서 마찬가지로 신체를 움직여 그러한 동작을 취할 수 있다. 도 16에 도시된 바와 같이, 일부 실시예들에서, 사용자 제어 메커니즘은 다음과 같이 동작한다: 단계에서, 시스템은 이미징 디바이스로부터 (실제 사용자의 이미지를 포함하는) 비디오 프레임과 같은 비 디오 데이터를 수신한다. 이미징 디바이스는 2차원 또는 3차원 카메라 또는 복수의 카메라일 수 있다. 카메라의 유형은 랩탑의 웹캠 및 (휴대 전화의 \"전면\" 또는 \"셀카\" 카메라를 포함한) 휴대 전화의 카메라를 포함하지만, 이에 국한되지는 않는다. 단계에서, 시스템은 단계에서 수신된 비디오 데이터로부터 사용자 표현을 분리한다. 일부 실시예에 서, 분리는 사용자의 이미지를 주변 실제 환경의 이미지로부터 분리 또는 분할하여 보다 바람직한 배경을 제거 및 대체하기 위해 비디오 컨퍼런싱 소프트웨어에서 일반적으로 사용되는 인체 분할 또는 \"셀카 분할\"에 의해 수 행된다. 일단 사용자의 이미지가 분리되면, 시스템의 일부 실시예는 이 이미지의 전부 또는 일부를 사용자 표현(\"가상 사용자\" 또는 \"가상 사용자 표현\"이라고도 함)으로 사용할 수 있다. 일부 실시예에서, 가상 사용자 표현은 전적 으로 물리적 사용자의 \"실제\" 이미지일 수 있다. 다른 실시예에서, 가상 사용자 표현은 부분적으로 \"실제\" 이미 지일 수 있고(예를 들어, 얼굴만), 가상 사용자의 다른 부분은 가상(예를 들어, 아바타의 신체) 또는 증강(예를 들어, 가상 의상을 입거나 가상 물체를 들고 있음)될 수 있다. 또한, 일부 실시예에서, 가상 사용자는 가상 아 바타(물리적 사용자의 아바타화된 표현을 포함하되 이에 국한되지 않음)와 같이 완전히 가상일 수 있다. 일부 실시예에서, 사용자 표현은 (페이퍼 컷아웃과 같이) 평평하거나 고정된 두께를 가질 수 있다. 다른 실시예에서 는, 둥글거나 실제 인체와 더 유사할 수 있다. 아바타화된 사용자 표현과 같은 일부 실시예에서, 사용자 표현은 명시적인 훈련 단계(예를 들어, 사용자가 카메라 앞에 'T 포즈'로 서서 회전해야 하는 경우)에서 또는 사용자가 시스템을 사용하는 정상적인 과정에서 다수의 이미지를 캡처하여 사용자에 대해 캡처된 다수의 이미지에 기반할 수 있다. 단계에서 사용자 표현을 수신한 후, 단계에서 시스템은 사용자 표현 또는 가상 사용자를 3차원 장 면에 추가한다. \"가상 세계/장면/공간\" 또는 \"3차원 공간\"이라고도 하는 3차원 장면은 객체, 환경 특징(예를 들 어, 언덕, 산, 나무), 다른 가상 사용자 등과 같은 다양한 특징을 포함할 수 있는 3차원 공간이다. 3차원 장면/ 공간은 복셀(즉, 3차원 또는 \"볼류메트릭\" 픽셀) 및/또는 x-y-z 좌표 위치를 가진 벡터로 구성되고 컴퓨터 메모 리에 보관되는 3차원 표현일 수 있다. 3차원 장면은 정적이거나 동적일 수 있다. 일부 실시예에서, 이는 게임과 같은 가상 현실 애플리케이션에 의해 생성될 수 있다. 일부 실시예들에서, 단계는 \"프레임 단위\"로 3차원 장면에 가상 사용자 표현을 추가하여, 물리적 사용자 가 실제 물리적 사용자 자신의 프레임 단위 이동 또는 동작에 의해 가상 사용자를 제어할 수 있도록 하는 것을 포함할 수 있다. 그러나, 도 20b 및 도 20c에 예시된 바와 같이, 일부 실시예에서, 매 프레임마다 가상 사용자 표현을 다시 추가 및 재추가하는 대신, 단계는 가상 사용자 표현을 3차원 장면 내의 객체로서 처음에 한 번 인스턴스화 및 추가하는 단계(1606a)를 포함하고, 이후 \"프레임\"에 대응할 수 있는 후속 반복들에서 3차원 장면 내의 가상 사용자 표현 객체를 업데이트하기 위해 진행할 수 있다(1606b 단계). 이러한 방식으로, 가상 사 용자 표현 객체는 매 프레임마다 다시 인스턴스화되고 다시 추가되는 것이 아니라, 한 번만(또는 매 프레임보다 덜 빈번하게) 추가 및 인스턴스화되고 (매 프레임만큼 빈번하게) 업데이트된다. 이제 가상 사용자 또는 사용자 표현을 포함하는 3차원 장면을 (예를 들어, 그것이 저장된 컴퓨터 메모리에 대한 포인터에 의해) 수신하면, 단계의 시스템은 3차원 장면을 물리적 또는 실제 사용자에게 다시 디스플레이 하기 위해 진행한다. 사용자에 대한 이러한 실시간 피드백을 통해 사용자는 가상 3차원 세계 내에서 자신의 가 상 자아를 계속 제어할 수 있다. 일부 실시예에서, 디스플레이는 2D 모니터와 같은 2차원 디스플레이를 통해 이루어진다. 이러한 디스플레이에서, 사용자는 가상 세계 내에서 자신의 이미지를 볼 수 있다. (2차원 모니터에 디스플레이할 때, (사용자 표현을 포함하는) 3차원 장면은 먼저 직교, 경사, 원근 등과 같은 2차원 투영의 일반 적인 수단에 의해 렌더링된다) 다른 실시예들에서, 디스플레이는 3차원 디스플레이 유닛을 통해 수행될 수 있다. 단계 이후에, 프로세스 루프는 3차원 가상 세계 내에서 가상 자아를 제어하기 위해 사용자에게 제어 및 시각적 피드백을 계속 제공하기 위해 단계와 함께 새로운 반복을 시작하는 것을 반복한다. 도 21은 도 16에 설명된 단계를 적용하는 예시를 도시한다. 사용자는 실제 방에 있는 카메라 앞에 물리적으로 서 있다. 단계에서, 시스템은 카메라로부터 비디오의 제1 프레임을 수신한다. 해당 비디오를 수신하면, 단계에서 시스템은 인체 분할을 통해 비디오에서 사용자의 이미지를 분리하고 이 이미지를 사용자 표현의 일부로 사용하도록 처리한다. 보다 구체적으로, 도 21에 도시된 이 실시예에서, 단계에서 시스템은 사용 자 표현으로 (위치 및 스케일의 고정 오프셋이 있는) 사용자 신체 이미지의 사본 세 개를 사용한다. 가상 댄스 홀의 \"비어 있는\" (즉, 가상 사용자 표현이 없는) 3차원 장면 또는 공간이 주어지면, 단계에서 시스템은 사용자 표현(예를 들어, 이 특정 예에서는 카메라의 가장 최근 비디오 프레임에서 사용자의 현재 신체 이미지 사본 3개)을 추가하기 위해 진행한다. 명확히 하기 위해, 단일 사본을 포함하여 임의의 수의 사용자 현재 신체 이미지 사본이 가상 댄스홀의 3차원 장면에 사용자 표현으로 사용될 수 있다. 단계에서 시스템은 3차원 장면을 2D 이미지로 렌더링한 다음 사용자가 볼 수 있도록 모니터 스크린과 같은 디스플레이 유닛에 이미지를 디스플레이한다. 사용자는 가상 세계 내에서 자신의 현재 가상 사용자 표현이 어떻게 보이는지 확인하면서, 자 신의 신체 위치와 포즈를 조금씩 조정한다. 이렇게 하면, 프로세스 루프가 단계에서 다시 반복되어 사용 자가 조정한 자세와 위치(즉, 시각적으로 인식할 수 있는 사용자의 변화)가 포함된 다음 비디오 프레임을 수신 한다. 이 최신 비디오 프레임을 입력으로 하여, 사용자 이미지를 분리하고 (사용자 신체의 세 개의 이미지를 갖 는) 사용자 표현을 생성하는 단계, 빈 가상 장면에 최신 사용자 표현을 추가하는 단계 및 가상 3차 원 공간에서 가상 사용자의 현재 모습을 반영하는 최신 2D 렌더링을 사용자에게 디스플레이하는 단계를 통해 프로세스가 다시 진행된다. 도 17에 도시된 바와 같이, 일부 실시예들은 3차원 장면을 업데이트하기 위한 단계를 추가로 포함할 수 있다. 일부 실시예들에서, 단계는 가상 사용자 표현과 별개로 3차원 장면을 업데이트하는 것을 포함한다. 일부 실시예에서, 단계는 가상 사용자 표현을 업데이트 또는 수정하는 것을 포함하여 3차원 장면을 업데 이트하는 것을 포함한다. 단계에서, 시스템은 3차원 장면이 동적일 수 있도록 한다. 일부 실시예들에서, 단계에서, 시스템은 시간의 경과에 기초하여 또는 시간에 따라 무작위로 장면을 업데이트한다. 예를 들어, 프로세스에서 단계(171 0)가 없으면, 가상 사용자는 절대 변하지 않는 정적인 댄스홀의 가상 공간에 추가될 수 있다. 프로세스의 일부 로 단계를 포함하면, 가상 댄스홀은 회전하는 디스코 볼과 다양한 색상으로 진동하는 타일로 구성된 바닥 을 가질 수 있다. 일부 실시예에서, 3차원 장면은 외부의 날씨, 주식 시장, 또는 키보드 또는 게임 컨트롤러로 부터 전송된 신호와 같은 외부 입력에 기초하여 업데이트될 수 있다. 도 18a에 도시된 바와 같이, 일부 실시예들은 사용자의 물리적 위치를 인식하는 단계를 포함할 수 있다. 위치는 측면 또는 \"x\" 위치, 수직 또는 \"y\" 위치, 깊이 또는 \"z\" 위치 중 하나 이상뿐만 아니라, 세 축 모두에 대한 회전 또는 방향을 포함할 수 있으나, 이에 국한되지 않는다. 일부 실시예에서, 위치는 카메라 또는 이미징 유닛을 기준으로 결정된다. 다른 실시예에서, 위치는 사용자가 위치한 방 또는 물리적 공간과 관련하여 결정된 다. 위치는 인체 추적, 깊이 추정 등을 위한 기술에 의해 결정될 수 있다. 깊이 추정과 관련하여, 일부 실시예들에서, 깊이 감지 이미징 유닛(비행 시간(TOF) 카메라/센서, 스테레오스코 픽 카메라, 함께 사용되는 다수의 카메라, 라이더로 보완된 2D 카메라, 깊이 감지 기술이 탑재된 휴대 전화의 카메라 등을 포함하되 이에 국한되지 않음)은 이미징 유닛에 의해 캡처된 장면으로부터 깊이 정보를 추출하기 위해 사용될 수 있다. 다른 실시예에서, 2차원 카메라와 같은 이미징 유닛은 장면에서 깊이 정보를 추출하기 위 해 장면의 깊이 단서를 인식하는 것과 함께 사용될 수 있다. 깊이 단서의 예로는 양쪽 눈 또는 신체의 여러 관 절 사이의 거리와 같이 인체에서 감지 가능한 특징들 사이의 거리가 포함된다. 이러한 특징들은 카메라에 가까 울수록 더 크게 나타나고 카메라로부터 멀어질수록 더 작게 나타나며, 이러한 방식으로 사용자로부터의 깊이 또 는 \"Z\" 거리를 추정하는 데 도움이 될 수 있다. 단계에서 위치 정보를 생성한 후, 단계의 시스템은 이 정보를 활용하여 사용자 표현 또는 가상 사 용자를 3차원 장면에 추가할 때 정확히 어디에 위치시킬지 결정할 수 있다. 일 실시예에서, 물리적 액터가 (단계에서 결정된 바와 같이) 1미터 왼쪽으로 이동하면, 단계의 시스템은 가상 공간에서 왼쪽으로 이 동된 위치에 가상 사용자를 추가할 수 있다. 그러나, 다른 실시예에서, 물리적 사용자에 의한 동일한 왼쪽 이동 은 단계의 시스템이 \"미러\" 효과를 나타내는 (왼쪽 대신) 오른쪽에 가상 사용자 표현을 추가하게 하거나, 다른 실시예에서 \"웜홀\" 또는 \"텔레포트\" 효과를 나타내는 3차원 공간 내의 완전히 다른 영역에 사용자를 추가 하게 할 수 있다. 단계에서 시스템에 의해 제공되는 위치 및 방향의 변경을 \"시프트\"라고 한다. 일부 실시예들에서, 단계 는 실제 물리적 사용자의 특정 유형의 시프트를 가상 사용자 표현에 의해 특정 대응하는 유형의 시프트로 \"매핑\"하는 것을 포함할 수 있다. 일부 실시예에서, 사용자에 의한 왼쪽에서 오른쪽으로의 시프트는 가상 사용 자 표현에 의해 왼쪽에서 오른쪽으로의 시프트를 초래할 수 있지만, 다른 실시예에서는 그 반대일 수 있다. 일 부 실시예에서, 사용자가 카메라 또는 이미징 디바이스를 향해 앞으로 시프팅하면 가상 사용자 표현이 앞으로 시프트될 수 있고, 그 반대의 경우도 마찬가지이다. 대안으로, 다른 실시예에서, 카메라를 향한 사용자의 시프 트(즉, 깊이 또는 \"Z\" 거리의 변화)는 완전히 무시될 수 있으며(즉, 대응하는 가상 시프트가 없는 것으로 매핑 되며), 따라서 가상 사용자 표현은 좌우로만 시프트하고 앞뒤로 시프트하지 않도록 할 수 있다. 방향 또는 회전 의 변화도 마찬가지로 다양한 방식으로 매핑되거나 완전히 무시될 수 있다. 전술한 바와 같이, 실제 사용자의 물리적 세계와 사용자 표현의 가상 세계 사이의 위치 및/또는 시프트의 매핑은 구현되는 특정 실시예만큼 다양 할 수 있다. 도 22는 가상 현실 게임을 플레이하는 사용자의 예시적인 실시예를 도시한다. 도 22에 대한 다음의 설명에서, 도 19에 제시된 단계들은 도 22의 특정 실시예와 상관된다. 예를 들어, 사용자는 노트북 또는 휴대 전화의 디스 플레이에서 가상 세계 내의 가상 자아를 보게 된다. 도 22에 도시된 4개의 스냅샷에서 볼 수 있듯이, 스크린에 서 자신의 가상 자아를 보면서, 사용자는 실수하여 물에 빠지지 않고 왼쪽의 가상 돌에서 오른쪽의 다른 돌로 조심스럽게 발을 옮기기 위해 프레임별로 자신의 가상 자아의 동작을 보정하고 제어한다. 제1 스냅샷과 관련하여: 사용자는 디스플레이를 통해 자신의 가상 자아가 인접한 돌로 넘어가야 한다는 것을 확인한다. 실제 사용자는 물리적으로 첫 발을 내딛는다. 단계에서 시스템은 실제 사용자가 실제 방 에 있는 모습을 보여주는 (사용자 노트북 또는 휴대폰의 전면 '셀카' 카메라로 캡처한) 제1 비디오 프레임을 수 신한다. 단계에서 시스템은 방에 있는 사용자의 이미지에서 사용자 이미지를 분리 또는 분할하고 해당 사 용자 이미지를 사용자 표현으로 사용한다. 단계에서 시스템은 실제 방 내에서 실제 사용자의 깊이를 포함 한 물리적 위치를 결정한다. 그 물리적 위치를 가상 장면 내의 해당 가상 위치로 매핑한 후, 단계에서 시 스템은 가상 3차원 장면의 왼쪽 돌 바로 위에 그녀의 이미지(즉, 사용자 표현)를 추가한다. 그런 다음, 단계 에서 시스템은 3차원 장면을 3D 표현에서 사용자의 노트북 또는 휴대폰 화면에 표시할 2차원 이미지로 렌 더링한다. 그리고 프로세스 루프의 다음 반복은 단계에서 다시 시작될 준비가 된다. 제2 스냅샷과 관련하여, 사용자는 (제1 스냅샷에 도시된 바와 같이) 디스플레이 유닛을 통해 가상 사용자가 올바른 방향 및 위치로 발을 뻗었다는 것을 2D 이미지에서 보고 있다는 긍정적인 피드백을 수신한 후, 실제 사용자는 가상 사용자가 (제2 스냅샷에 도시된 바와 같이) 이제 두 개의 돌 위에 발을 단단히 고정 하도록 자신의 물리적 발을 내려놓도록 진행한다. 이는 실제 사용자가 실제 발을 내려놓으면, 단계의 시 스템이 해당 이미지를 캡처하고, 단계에서 해당 이미지를 나머지 방에서 분리 및 분할하고, 단계에 서 깊이를 포함한 실제 위치를 결정하고, 단계에서 가상 사용자 표현(이미지)을 적절한 위치(두 돌 사이 의 간격 바로 위)에 추가하고, 단계에서 업데이트된 3차원 장면을 제2 스냅샷으로 렌더링 및 표시 함으로써 이루어진다. 그리고 프로세스 루프의 다음 반복은 단계에서 다시 시작할 준비가 된다. 이는 사 용자가 디스플레이된 장면을 피드백으로 활용하여 가상 자아를 제어하는 예시이다. 사용자가 움직이면, 시스템 은 해당 움직임이 디스플레이에 도시된 가상 자아에 반영되도록 한다. 사용자가 한 스텝에서 다른 스텝으로 성공적으로 건너는 것으로 보이는 긍정적인 피드백을 통해, 다음 두 스냅 샷(2203 및 2204)의 과정을 걸쳐, (도 16-19에 설명된 바와 같이) 프로세스 루프가 계속 반복 및 반복될 수 있 고, (물리적 사용자에 의해 제어되는) 가상 사용자가 먼저 오른쪽 돌 체중을 옮기고 양 발을 오른쪽 돌 으로 가져오는 것을 본다. 이는 사용자가 디스플레이된 장면을 피드백으로 활용하여 가상 자아를 제어하 는 예시이다. 사용자가 움직이면, 시스템은 디스플레이에 도시된 가상 자아에 해당 움직임이 반영되도록 한다. 도 18b에 도시된 바와 같이, 일부 실시예들은 비디오 데이터에서 사용자의 이미지로부터 사용자의 포즈 또는 제 스처를 인식하는 단계를 포함할 수 있다. 포즈 추정은 잘 문서화된 다양한 기법에 의해 수행될 수 있다. 제스처 인식 역시 잘 문서화되어 있으며, 포즈 정보를 해석하여 관심 있는 다양한 제스처를 인식하는 데 사용될수 있다. 단계에서 포즈 및/또는 제스처 정보를 인식한 후, 단계의 시스템은 이 정보를 활용하여 사용자 표 현 또는 가상 사용자를 3차원 장면에 추가할 때 정확히 어디에 위치시킬지 결정할 수 있다. 예를 들어, 사용자 가 각 무릎을 번갈아 구부리는 경우, 단계의 시스템은 이를 다리로 걷는 제스처를 취하는 것으로 인식할 수 있다. 이에 대응하여, 일부 실시예에서, 프로세스의 반복 루프에 의해 처리되는 각 연속 비디오 데이터(즉, 비디오 프레임)에 대해, 단계의 시스템은 루프의 각 반복(즉, 각 비디오 프레임)에 대해 이전 가상 위치 로부터 앞으로 이동된 가상 사용자를 추가하여, 가상 사용자가 가상 3차원 장면을 걷는 것처럼 위치를 앞으로 이동시킬 수 있다. 다른 실시예에서, 단계에서 시스템에 의해 점프와 같은 제스처를 한 것으로 인식되는 사용자는 가상 3차원 장면 내에서 위로 점프하는 가상 사용자를 나타내기 위해 루프의 다음 몇 개의 연속적인 반복 동안 그 이전 위치에서 위로 추가될 수 있다. 다른 실시예에서, 사용자의 얼굴 제스처 또는 표정이 단계 의 시스템에 의해 찡그린 얼굴로 인식되면 단계의 시스템이 푸른 색조를 갖는 사용자 표현이 생성 되게 할 수 있는 반면, 미소를 인식하는 단계의 시스템은 단계의 시스템이 노란색 색조를 갖는 사 용자 표현을 생성하게 할 수 있다. 다른 실시예에서, 도 20a에 도시된 바와 같이, 팔을 뻗은 사용자는 이미징 유닛에 의해 캡처 된 비디오 스트림을 사용하여 단계의 시스템에 의해 슈퍼 히어로 비행 포즈를 취하는 것으로 인식될 수 있다. 이에 대응하여, 단계의 시스템은 예를 들어 증강된 가상 슈퍼히어로 망토를 착용한 사용자의 실 제 이미지가 포함된 사용자 표현을 출력한다. 그런 다음, 단계의 시스템은 프로세스 루프의 연속적 인 반복, 즉 제1 반복(1312a), 제2 반복(1312b) 및 제3 반복(1312c)에 걸쳐, 가상 사용자 표현을 루프(1312a, 1312b, 1312c)의 각 패스에서 점진적으로 위쪽과 앞으로 반복적으로 추가하여 가상 사용자 표현이 3차원 장면 내에서 마치 비행하는 것처럼 이동하도록 할 것이다. 3차원 장면은 1308 및 1310와 같은 3차 원 장면 객체를 포함할 수 있다. 이는 사용자의 포즈/제스처 및 시스템이 그러한 포즈/제스처를 감지하여 프로 그래밍된 해당 동작 또는 미리 정의된 동작을 트리거하거나 유발하는 예시적인 예이다. 도 19에 도시된 바와 같이, 다양한 선택적 단계들이 함께 결합될 수도 있다. 일부 실시예에서, 3차원 장면을 업 데이트하는 단계의 일부로서, 시스템은 모든 가상 사용자들과 3차원 장면 내의 모든 객체들 사이의 인터 랙션 또는 충돌을 지속적으로 또는 계속 모니터링하고, 또한 이들 각각의 상태를 유지/업데이트한다. 앞서 언급 한 예시의 회전하는 디스코 볼과 같은 일부 객체의 경우, 단계는 시스템이 이를 지속적으로 업데이트하여 계속 움직이도록 할 수 있다. 벽과 같은 일부 수동적인 객체의 경우, 단계는 가상 사용자가 벽을 통과하 지 못하도록 하는 단계를 포함할 수 있다. 인터랙티브 객체의 경우, 단계는 가상 사용자(또는 다른 객 체)와 객체 간의 인터랙션을 모니터링 및 감지하여 필요한 동작을 트리거하고 실행하는 단계를 포함할 수 있다. 일부 실시예에서, 3차원 장면을 업데이트하는 프로세스 단계(단계)는 도 28에 도시된 바와 같이 구현될 수 있다. 시스템은 가상 사용자(또는 사용자 표현)와 인터랙션 객체들 간의 인터랙션(단계), 다수의 가상 사용자들 간의 인터랙션(단계), 객체들 간의 인터랙션(단계) 및 시간 경과, 날씨, 버튼 누름, 게임 컨트롤러 입력 등과 같은 외부 트리거(단계)를 감지한다. 시스템이 이러한 인터랙션 및/또는 인터랙션 객 체와 관련된 동작의 트리거를 감지한 후 트리거된 동작을 실행하면, 단계의 시스템은 그에 따라 객체를 업데이트하고, 단계에서 가상 사용자를 그에 따라 업데이트한다. 가상 사용자(또는 사용자 표현)도 그 자 체로 객체로 간주된다. 따라서, 객체 간의 인터랙션에는 가상 사용자와 객체 간의 인터랙션이 포함된다. 마찬가 지로, 가상 사용자들 간의 인터랙션도 포함된다. 일부 실시예에서, 단계는 3차원 장면을 업데이트하기 위해 단계의 위치(깊이 포함) 및/또는 방향 데이터를 활용하는 단계를 포함할 수 있다. 단계에서, 시스템은 물리적 사용자의 위치 및/또는 가상 사용 자 표현의 가상 위치를 활용하여 인터랙티브 객체의 트리거링을 감지한 다음 관련 동작을 실행함으로써 그렇게 할 수 있다. 일 실시예에서, 예를 들어, 가상 사용자 표현이 가상 3차원 장면 내의 가상 자동 슬라이딩 도어 앞 에 서 있는 상황을 고려한다. 시스템이 단계에서 실제 물리적 사용자가 물리적 공간 내에서 앞으로 이동 한 것(즉, 깊이 정보의 변화)을 감지하면, 시스템은 단계에서 가상 사용자를 이전 위치에서 한 발 앞으로 3차원 장면에 추가한다. 가상 사용자가 이제 가상 자동 슬라이딩 도어 바로 앞에 있는 것을 감지하면, 시스템은 단계에서 슬라이딩 도어 인터랙티브 객체가 트리거될 것을 결정하고, 따라서 도어가 슬라이딩으로 열리게 하는 관련 동작을 실행한다. 이와 같이, 단계의 시스템은 가상 도어를 슬라이딩으로 열어 3차원 장면을 업데이트한다. 다른 실시예에서, 단계에서, 시스템은 단계에서 시스템으로부터 제공된 포즈 및/또는 제스처 정보 를 활용하여 3차원 장면을 업데이트할 수 있다. 예를 들어, 가상 화이트보드 애플리케이션의 일 실시예에서, 손 바닥을 펴서 왼쪽에서 오른쪽으로 길게 스와이프하는 제스처는 단계에서 시스템에 의해 (연속적인 비디오 프레임의 과정 동안) \"모두 지우기\" 제스처로 인식될 수 있다. 시스템이 \"모두 지우기\" 제스처의 발생에 대한 정보를 제공함에 따라, 시스템은 단계에서 수행되었을 수 있는 모든 가상 글씨 또는 도을 제거해 가상 3 차원 장면을 업데이트하기 시작한다. 이러한 제거는 쓰기에 관련된 복셀(\"복셀\"은 3차원 또는 \"볼류메트릭\" 픽 셀을 말함)을 원래의 \"쓰지 않은\" 상태로 되돌림으로써 수행된다. 가상 화이트보드의 예를 계속하면, 시스템이 단계에서 물리적 사용자가 마치 펜을 들고 있는 것처럼 손가 락으로 꼬집는 제스처를 취하고 있다고 판단하는 경우를 고려한다. 이 예시 시나리오에서, 시스템은 단계(181 2)에서 물리적 공간 내에서 물리적 사용자의 물리적 위치도 결정한다. 시스템이 수행한 단계(1812 및 1814)를 통해, 시스템은 단계에서 해당 물리적 위치를 가상 사용자 표현을 추가하는 3차원 가상 공간 내의 가상 위치로 매핑한다. 단계에서, 시스템에 의해 핀치 제스처(즉, 펜 제스처)가 인식되면, 시스템은 단계 에서 가상 3차원 공간에서 가상 사용자의 손(또는 손에 의해 잡히는 가상 펜)의 가상 위치에 쓰기 또는 그리기를 (즉, 적절한 복셀의 색을 변경하여) 나타내도록 가상 3차원 공간을 업데이트한다. 도 29는 가상 화이트보드 시스템의 일 실시예에 대한 프로세스 흐름을 도시한다. 단계에서, 시스템 은 사용자에 의해 수행되는 펜 잡기 제스처(예컨대, 손가락을 꼬집는 제스처)를 인식한다. 단계에 서, 시스템은 사용자 표현의 손가락의 3차원 위치에서 복셀(또는 2D의 경우 픽셀)의 색상을 그리거나 수 정하여 장면을 업데이트한다. 단계에서, 시스템은 업데이트된 장면을 디스플레이한다. 도 29의 프로세스 는 반복될 수 있다. 다른 예로서, 가상 사용자의 가상 위치 및 포즈가 가상 사용자가 3차원 가상 공간에서 버튼으로 접촉하고 있음 을 나타내는 경우, 시스템은 버튼이 눌리는 것을 감지한 다음, 상응하는 동작을 트리거하고 실행한다. 예를 들 어, 이러한 대응 동작은 액터가 버튼을 누르고 있는 것에 응답하여 버튼의 색상을 변경하는 것(즉, 사용자의 위 치와 가상 버튼과 접촉하고 있음을 나타내는 포즈를 인식하는 것)이 될 수 있다. 가상 3차원 공간 또는 장면은 버튼의 색상을 변경하도록 업데이트된다. 일부 실시예에서, 시스템은 단계에서 가상 사용자 자체를 업데이트할 수 있다. 일 실시예에서, 가상 사용 자가 가상 지뢰(인터랙티브 객체)에 너무 가까이 접근하면, 프로그래밍된 대응 동작이 트리거될 수 있는데, 이 는 가상 3차원 세계에서 지뢰와의 인터랙션으로 인해 지뢰가 폭발하게 하는 동작이 될 수 있다. 이러한 폭발은 주변의 다른 객체에 손상을 입힐 수 있다. 폭발 효과를 표현하기 위해, 이 예에서, 시스템은 단계에서 3 차원 장면 내의 인근 물체의 외형 및 기타 속성/특성(예를 들어, 형상, 회전, 체력, 갑옷, 속도, 강도 등)을 업 데이트한다. 이러한 영향을 받는 객체 중 하나는 가상 사용자 표현 자체일 수도 있다. 이러한 경우, 시스템은 단계에서 가상 사용자 표현을 폭발 위치에서 뒤로 던지는 애니메이션과 가상 사용자를 (실제 물리적 사용 자가 여전히 서 있음에도 불구하고) 누워 있는 위치에 배치함으로써 가상 사용자 표현에 대한 업데이트를 제공할 수 있다. 결국, 이러한 일련의 3차원 장면의 표현의 다음 반복에서, 시스템은 단계에서 가상 사용 자가 다시 일어서도록 업데이트할 수 있지만, 단계에서 시스템은 피를 흘리거나 다친 것처럼 보이도록 표 현을 업데이트할 수 있다. 또한, 가상 사용자 표현의 한쪽 팔이 (실제 사용자의 포즈와 상관없이) 더 이상 움직 일 수 없을 수도 있다. 시스템은 또한 단계에서 가상 사용자 표현의 속성을 변경하여 현재 부상당한 상태 에서 움직일 수 있는 속도를 느리게 할 수도 있다. 도 23은 제스처를 활용하여 인터랙티브 객체, 이 예에서는 가상 장면에서 플레이되는 게임의 일부로서 사 탕 조각을 잡고 이동하는 사용자의 예시적 실시예를 도시한다. 제1 스냅샷에서, 사용자는 손을 펴 노란색 직사각형 사탕 조각(인터랙티브 객체)에 다가간다. 시스템은 휴대폰의 카메라와 같은 이미징 유닛에서 실 제 방에 있는 실제 사용자의 비디오 피드의 프레임을 캡처한다. 그런 다음 시스템은 해당 프레임에서 사용자 표 현으로 활용하도록 처리하는 사용자 이미지를 분리하거나 분할한다. 시스템은 사용자 손의 포즈 또는 제 스처를 '잡기' 제스처의 시작을 나타내는 편 손인 것으로 인식한다. 시스템은 사용자 표현(즉, 손을 편 사용자 이미지)을 장면에 추가한다. 시스템은 아직 객체 인터랙션을 감지하지 못하므로 장면을 더 이상 업데이 트하지 않는다. 시스템은 사용자가 볼 수 있는 이미지를 렌더링하여 휴대폰 스크린에 디스플레이한다. 그 리고 프로세스 루프는 다음 반복을 시작할 준비가 된다. 제2 스냅샷과 관련하여, 사용자는 디스플레이에서 자신의 가상 자아의 편 손이 인터랙티브 객체 뒤 에 있는 것을 보고, 실제 사용자는 물리적으로 자신의 주먹을 쥐는 것을 진행한다. 이 예에서, 사용자는 디스플 레이에 나타낸 장면을 긍정적 피드백으로 사용하여 자신의 가상 표현을 제어할 수 있다. 시스템은 이 포즈를 취한 실제 사용자의 비디오 프레임을 캡처한다. 시스템은 가장 최근의 사용자 신체 이미지(즉, 주먹을 쥔 상태)를 분리한다. 시스템은 (이전에는 편 손 다음에) 쥔 주먹을 '잡기' 제스처를 나타내는 것으로 인식한다. 시스템은 가장 최근의 사용자 표현(즉, 주먹을 쥔 사용자 이미지)을 장면에 추가한다. 시스템이 객체 인터랙션 (즉, 잡기)을 감지하더라도, 이 실시예에서는 결과적으로 사탕을 재인출할 필요가 없으므로 장면의 추가 업데이 트가 수행되지 않을 수 있다. 시스템은 사용자가 휴대 전화의 디스플레이에서 볼 수 있도록 이미지를 렌 더링하여 디스플레이한다. 그리고 프로세스 루프는 다음 반복을 시작할 준비가 된다. 제3 스냅샷과 관련하여, 그의 가상 자아가 올바른 사탕 조각을 잡았다는 피드백을 받은 실제 사용 자는 주먹을 오른쪽으로 움직이기 시작한다. 프로세스는 이전과 동일하게 진행된다. 시스템은 움직이는 주먹을 '드래그' 제스처로 식별한다. 따라서, 시스템은 이번에는 인터랙티브 객체가 오른쪽으로 드래그되는 것을 표시하도록 장면을 업데이트한다. 시스템은 다음 반복을 시작하기 위해 프로세스 루프가 진행되기 전에 사용자 에게 장면을 디스플레이한다. 스냅샷과 관련하여, 가상의 자아가 가상의 사탕 조각을 원하는 최종 위치로 성공적으로 드래그했다 는 시각적 피드백을 받은 실제 사용자는 주먹을 편다. 사용자의 가상 사용자 표현이 사탕 조각(인터랙션 객체)을 최종 원하는 위치에 놓아둔 것을 보여주는 이미지는 디스플레이된다. 이는 사용자의 제스 처/포즈가 인터랙션 객체의 프로그래밍된 동작을 트리거하는 예시이다. 도 24는 사용자가 하나의 인터랙티브 객체를 제어하고 사용하여 다른 인터랙티브 객체에 의해 동작을 트리거할 수 있는 실시예의 또 다른 예를 도시한다. 스냅샷에서, 사용자 표현의 신체의 대부분이 스냅샷의 중앙에 나타나는 핸들(인터랙티브 객체)에 의해 가려져 있지만, 들어 올린 팔은 손에 들고 있는 가상 채찍(인터 랙티브 객체)을 캐스팅(또는 채찍질)하는 행위에서 보일 수 있다. 가상 채찍은 프로세스 루프의 모든 반 복에서 시스템에 의해 인출되고 업데이트된다. 스냅샷에서, 채찍이 스냅샷 중앙에 있는 핸들을 감 싸고 있는 것을 볼 수 있다. 시스템은 채찍과 손잡이 사이의 접촉 인터랙션을 감지하여 핸들 주위를 감싼 채찍 을 나타내도록 장면을 업데이트한다. 그런 다음, 스냅샷에서, 사용자가 채찍을 뒤로 당기는 제스처 를 취하면, 가상 사용자가 가상 채찍을 뒤로 당겨서 가상 핸들을 아래로 당긴다. 시스템이 제스처 를 인식한 상태에서, 시스템은 채찍이 핸들을 아래로 당기는 것을 반영하도록 장면을 업데이트한다. 도 16-19의 설명은 3차원 가상 공간을 포함하는 실시예들에 대해 논의하지만, 일부 실시예들은 2차원 가상 공간 을 활용하여 동일하거나 유사한 개념을 구현한다는 점에 유의해야 한다. 일부 실시예들에서, 각 가상 사용자의 관점은 사용자의 정면을 향하는 3인칭 시점이다. 일부 실시예에서, (3차 원 장면의 렌더링이 수행되는) 관점은 가상 사용자가 전방에 고정된 거리를 유지하면서 이동한다. 다른 실시예 에서, 관점은 고정되어 있다. 그리고, 다른 실시예들에서, 관점의 사용자로부터의 거리 및 시야각은 상황에 따 라 달라진다. 일부 실시예들에서, 각 가상 사용자의 관점은 사용자 뒤에서 3인칭 시점이다. 이러한 경우, 사용자 표현은 사용 자의 후면(또는 \"뒤로부터\") 이미지 또는 표현을 사용하거나 표시할 수 있다. 일부 실시예에서, 그러한 표현은 바디 컷오프에 대한 논의 및 도 15에 설명된 것과 유사한 저장된 바디 표현으로부터 추출될 수 있다. 사용자의 뒷모습을 필요로 하는 애플리케이션 또는 게임과 같은 일부 실시예에서, 일부 실시예는 시스템이 사용자의 뒷모 습 표현을 가질 수 있도록 뒷모습을 캡처하기 위해 사용자에게 이미징 디바이스에 자신의 뒷면을 보여줄 것을 요청하는 명시적 이미지 캡처 단계를 활용할 수 있다. 그런 다음 뒷모습은 가상 세계에서 가상 자아를 디스플레 이할 때 사용자가 보는 사용자 표현으로 사용될 수 있다. 복수의 사람 및 복수의 사용자 일부 실시예들에서, 복수의 가상 사용자들이 동일한 가상 3차원 장면에 추가되고, 참여하며, 인터랙션할 수 있 다. 일부 실시예에서, 복수의 사용자는 네트워크 상의 상이한 위치로부터 원격으로 시스템에 접속할 수 있다. 이러 한 경우, 도 16-19에 도시된 프로세스 루프 예시 실시예의 일부 단계들은 클라우드, 사용자의 로컬 디바이스, 또는 이들의 조합에서 수행될 수 있다. 마찬가지로, 프로세스 루프의 단계들은 서로 다른 순서로 수행될 수 있 다. 도 25는 복수의 사용자가 원격으로 동일한 가상 공간에서 게임에 추가되고 참여하는 예시를 도시한다. 이와 같 은 실시예들에서, 각 가상 사용자의 관점은 가상 사용자의 정면을 향하는 3인칭 시점이다. 이와 같은 실시예에 서, 가상 사용자 표현은 실제 사용자의 미러링된 이미지이며, 이는 사람들이 자신의 미러 이미지를 보는 것이더 자연스럽고 직관적으로 느껴지기 때문이다(예를 들어, 실제 사용자가 실제 왼손을 움직이면, 가상 사용자의 관점에서 기술적으로는 가상 사용자의 오른손이지만, 실제 사용자의 관점에서 왼쪽에 나타나는 가상의 손이 움 직이는 것을 볼 것이다). 각 사용자의 얼굴을 통합한 사용자 표현을 통해 각 사용자는 서로의 실제 얼굴 표정을 볼 수 있으며, 주변 가상 사용자들 간에 시스템을 통해 전달되는 오디오를 통해 소통할 수도 있다. 이를 통해 사용자는 공동의 존재감을 강화할 수 있다. 또한, 각 사용자는 자신의 표정을 볼 수 있기 때문에 다른 사용자가 자신을 어떻게 보는지도 확인할 수 있다. 또한, 이러한 함께 있는 느낌은 디스플레이 상에 두 사용자를 포함하 는 가상 장면을 각 사용자에게 디스플레이함으로써 향상될 수 있다. 일부 실시예들에서, 복수의 사용자들은 동일한 이미징 디바이스를 통해 로컬로 시스템에 접속할 수 있다. 일부 실시예들에서, 본 명세서에 설명된 임의의 사용자별 단계들은 각 사용자에 대해 개별적으로 수행될 수 있다. 불필요하거나 의도하지 않은 사람(예컨대, 카메라 뷰에 나타나는 행인 또는 구경꾼)이 의도된 사용자로 취급되 는 것을 방지하기 위해, 시스템은 이미지에서 감지된 모든 사람을 필터링할 수 있다. 일부 실시예에서, 도 26에 도시된 바와 같은 사람 필터 방법은 비디오 데이터(또는 비디오 프레임)에서 감지된 각 사람에 대해 실행된다. 단계의 시스템은 비디오 프레임에 나타나는 사람을 감지한다. 감지된 각 사람에 대해, 단계의 시스 템은 해당 사람을 의도된 사용자 목록과 비교한다. 일부 실시예에서, 의도된 사용자 목록은 의도된 사용자의 적 어도 부분적인 신체 이미지를 포함한다. 비교는 얼굴 비교, 특징 비교, 컬러 히스토그램 비교 등과 같은 잘 알 려진 이미지 비교 방법으로 수행될 수 있다. 일부 실시예에서, 의도된 사용자 목록은 시스템 사용을 시작하기 전에 사용자 인터페이스에 의해 프롬프트될 수 있는 것과 같은 명시적인 의도된 사용자 등록 프로세스를 통해 채워질 수 있다. 다른 실시예들에서, 의도된 사용자 목록은 가장 최근 및/또는 가장 일반적인 반복 사용자의 이 미지를 촬영함으로써 암시적으로 채워질 수 있다. 단계에서 비교가 완료되면, 단계의 시스템은 감 지된 사람이 실제로 의도된 사용자인지 여부를 결정한다. 그렇다면, 프로세스가 단계로 진행하여 감지된 사람과 해당 이미지를 사용자로 진행하도록 허용하고 해당 사용자 표현을 분리하여 가상 장면에 추가한다. 그렇 지 않으면, 프로세스는 단계로 진행하여감지된 사람과 해당 이미지가 사용자로 진행하도록 허용되지 않고 해당 사용자 표현이 분리되거나 가상 장면에 추가된다. 감지된 모든 사람에 대해 필터 프로세스가 성공적으로 실행된 후에는, 의도하지 않은 사람이 가상 3차원 장면에 포함되거나 해당 장면의 디스플레이에 나타나지 않아 야 한다. 도 27에 도시된 바와 같이, 일부 실시예는 감지된 사람의 수가 예상되는 사람의 수보다 많은 경우에만 사람 필 터를 적용할 수 있다. 단계의 시스템은 비디오 이미지에서 사람의 수를 감지한다. 단계의 시스템은 그 수를 예상 사용자 수와 비교한다. 일부 실시예에서, 예상 사용자 수는 애플리케이션에 의해 미리 결정될 수 있다. 예를 들어, 1인용 게임은 한 명의 사용자만 예상할 수 있다. 다른 실시예에서, 예상 사용자 수는 애플리 케이션의 시작 시 사용자(들)에게 제공되는 사용자 인터페이스에 의해 명시적으로 수신될 수 있다. 단계 의 시스템은 단계의 결과를 고려하여 감지된 인원 수가 예상 사용자 수를 초과하는지 여부를 결정한다. 만약 그렇다면, 도 26에 설명된 것과 같은 개인별 필터 프로세스가 감지된 각 사람에 대해 실행된다. 그렇지 않 은 경우, 감지된 모든 사람을 사용자로 처리될 수 있다. 도 4는 디스플레이 유닛을 사용하여 표현 가능한 출력 비디오의 예시적인 장면(402, 404)을 도시한다. 장면 에서, 피사체 레이어는 액터 레이어 앞에 오버레이된다. 액터는 피사체 레이어의 피사체가 액터 에 의해 가려지지 않는 액터 레이어에 표현된다. 배경 레이어는 액터 레이어 뒤에 표시된다. 도시된 바와 같이, 각 장면 레이어는 출력 비디오가 배경 레이어 위에 액터 및 피사체 레이어의 피사체를 묘 사하는 렌더링된 멀티레이어 장면을 재생하도록 투명한 부분을 포함할 수 있다. 이 예에서, 피사체 레이어는 액 터 레이어 앞에 배치된 그래프인 피사체를 도시하고 있으며, 액터 레이어는 분리되거나 추출된 액터 와 함께 도시된다. 배경 레이어는 다른 모든 레이어 뒤에 배치된다. 장면은 피사체 레이어와 배경 레 이어 사이에 액터 레이어를 사용하기 전의 장면을 도시한다. 이 장면에서, 액터 레이어의 가상 액터 는 피사체 레이어와 배경 레이어 사이에 위치하지 않는다. 일부 예들에서, 배경 레이어와 피사체 레 이어는 합성 입력 데이터로부터 추출된다. 일부 예들에서, 배경 레이어 및 피사체 레이어는 멀티레이어 입 력 데이터로부터 소싱된다. 일부 예들에서, 3차원 효과(예를 들어, 각 레이어가 서로에 대해 깊이에 변화로 표현된 복수의 레이어를 갖는 출력 비디오)는 합성 입력 데이터로부터 복수의 레이어를 식별함으로써 달성된다. 합성 입력 데이터를 분석하여 배경 레이어와 피사체 레이어를 식별할 수 있다. 출력 비디오의 각 프레임은 액터 뒤에 있는 적어도 하나의 배 경 레이어, 적어도 하나의 배경 레이어 앞에 있는 적어도 하나의 액터 레이어, 및 적어도 하나의 액터 레이어 앞에 있는 적어도 하나의 피사체 레이어로 구성된다. 즉, 출력 비디오는 적어도 3개의 서로 다른 레이어로 구성된 장면, 즉 멀티레이어 장면으로 구성된다. 마찬가지로, 출력 비디오는 3차원 효과를 생성하기 위해 레이어들 이 배치되거나 스택되는(즉, 레이어/레이어 스택 내의 깊이 위치) 3개 이상의 서로 다른 레이어를 갖는 하나 이 상의 장면을 포함할 수 있다. 도면에 도시된 본 개시의 예시적인 구현은 일차적으로 장면에 3개의 레이어(예를 들어, 배경 레이어, 액터 레이어 및 피사체 레이어)가 있는 멀티레이어 장면을 보여주지만, 복수의 레이어가 멀 티레이어 장면을 구성할 수 있다. 예를 들어, 복수의 레이어는 배경 레이어와 피사체 레이어 사이에 배치되어 입체 효과를 생성하거나 다양한 깊이 정도(즉, 레이어의 다양한 깊이 위치)를 생성할 수 있다. 또 다른 예로, 멀티레이어 장면은 배경 레이어의 여러 레이어로 구성되어 나무 이미지 뒤에 더 산의 이미지(또는 비디오 프레 임)를 배치하는 것과 같이 배경에 다양한 깊이 정도를 만들 수 있다. 마찬가지로, 장면 피사체 레이어에도 여러 레이어를 도입할 수 있다. 즉, 멀티레이어 장면은 다수의 배경 레이어, 다수의 액터 레이어, 다수의 피사체 레 이어로 구성될 수 있다. 멀티레이어 장면을 구성하는 다수의 레이어로, 입체적인 효과를 달성하는 시각화 효과 가 향상된다. 프레젠테이션 소프트웨어를 이용한 슬라이드 재구성. 멀티레이어 입력 데이터의 레이어는 몇 가지 다른 방법으 로 식별될 수 있다. 일례로, 프레젠테이션 소프트웨어(예컨대, 파워포인트 또는 구글 슬라이드)는 슬라이드가 일반적인 크기의 배수(슬라이드에 포함된 레이어 수에 해당)가 되도록 구성될 수 있다. 도 5에 설명된 바와 같 이, 슬라이드는 동일한 슬라이드에 피사체 레이어와 배경 레이어를 포함하도록 일반적인 크기의 두 배로 구성된다. 절반은 배경 레이어로 사용되고 나머지 절반은 피사체 레이어로 사용된다. 이는 프레 젠테이션 소프트웨어 자체를 사용하여 만들어지기 때문에, 각 절반은 프레젠테이션 소프트웨어를 통해 사용할 수 있는 모든 기능(예를 들어, 애니메이션 등)을 유지한다. 피사체 레이어는 특정 색상을 사용하여 레이어의 영 역에서 투명성을 나타내고, 레이어의 영역에서 특정 색상이 없으면 불투명도 또는 반투명을 나타낼 수 있다. 불 투명 또는 반투명 영역은 피사체 레이어에서 피사체(즉, 관심 있는 주제)를 나타낸다. 이 예제에서는, 그래프가 피사체 레이어에서 관심 있는 주제로 표시된다. 그리고 배경 레이어는 미적 디자인 요소를 제공한다. 일부 예들에서, 프레젠테이션 소프트웨어를 사용하여, 일반적인 크기의 슬라이드가 액터 레이어의 배치를 나타 내는 적어도 하나의 미리 결정된 자동 인식 가능한 플레이스홀더 데이터 위치와 결합하여 사용된다. 도 6에 도 시된 바와 같이, 플레이스홀더 데이터 위치는 슬라이드에 삽입되어 배경 레이어에 할당된 모든 콘텐 츠 앞과 피사체 레이어에 할당된 모든 콘텐츠 뒤에 추가된다. 플레이스홀더 데이터 위치는 액터 레이어가 추가될 액터 레이어의 영역을 나타내도록 슬라이드에서 크기와 모양을 지정할 수도 있다. 플레이스홀더 데이터 위치가 액터 레이어 내에 지정되면, 결과 출력 비디오는 지정된 플레이스홀더 데이터 위치 내에 배치 된 액터 레이어를 포함하는 멀티레이어 장면을 갖게 된다. 디스플레이 시, 액터 레이어가 플레이스홀더 이 미지 대신 디스플레이된다. 일부 예들에서, 플레이스홀더 이미지 대신에 액터 레이어를 대체하는 것은 출력 비 디오 또는 이미지의 렌더링 전에 레이어의 위치 지정 또는 스태킹 중에 이루어진다. 일부 예들에서, 출력 비디 오의 이미지 또는 프레임은 플레이스홀더 이미지를 포함하여 렌더링되고, 시스템이 플레이스홀더 이미지를 식별 하면, 플레이스홀더 이미지의 가시 부분이 액터 레이어의 대응하는 부분으로 치환된다. 프레젠테이션 소프트웨어의 사용자 인터페이스. 일부 예들에서, 프레젠테이션 소프트웨어(예컨대, 파워포인트 또는 구글 슬라이드)의 사용자 인터페이스는 장면 상의 액터 레이어를 포함하도록 활용, 재설계 또는 수정된다. 액터 레이어는 슬라이드에 배치되고 \"뒤로 보내기(Send backwards)\", \"뒤로 보내기(Send to back)\", \"앞으로 가져오기(Bring forwards)\", \"앞으로 가져오기(Bring to front)\" 등의 프레젠테이션 소프트웨어 옵션을 사용하 여 피사체 레이어 또는 다른 레이어의 다른 피사체의 앞이나 뒤에 배치될 수 있다. 일부 예에서는 사용자가 프 레젠테이션 소프트웨어(예를 들어, 파워포인트 또는 Google 슬라이드)를 사용하여 이미 생성된 슬라이드를 조작 할 수 있도록 사용자 인터페이스가 구현된다. 사용자 인터페이스는 슬라이드의 어떤 레이어에 있는지(즉, 어떤 객체가 다른 객체의 앞 또는 뒤에 있는지) 명확히 하는 방식으로 슬라이드의 피사체 또는 객체를 디스플레이한 다. 그런 다음 사용자 인터페이스를 통해 사용자는 이러한 레이어 중 적어도 하나의 액터 레이어를 배치할 위치, 즉 어느 레이어 앞이나 뒤 또는 어느 레이어 사이에 배치할지 선택할 수 있다. 일부 예시에서는 자동화된 레이어링이 사용된다. 자동화된 레이어링은 자동화된 수단을 사용하여 배경 레이어와 피사체 레이어를 구분하고 그 사이에 액터 레이어를 삽입하는 것을 포함한다. 인공지능 및 딥 러닝 기법은 시스템이 배경 레이어와 피사체 레이어를 식별하기 위해 사용될 수 있다. 일부 예 들에서, 인공 지능 또는 딥 러닝은 배경 레이어 및 피사체 레이어의 인식을 위해 사용될 수 있다. 일부 예들에 서, 이는 이미지, 비디오, 슬라이드, 화면 공유 등과 같은 데이터의 훈련 세트(즉, 잠재적 합성 입력 데이터 )에 주석을 달아 데이터의 어떤 레이어가 배경 레이어 및 피사체 레이어와 같은 어떤 레이어 깊이 위치에 있어야 하는지 식별하고 추출함으로써 수행된다. 그런 다음 훈련 세트를 사용하여 신경망을 훈련시켜 분류기를생성한다. 그런 다음 분류기를 합성 입력 데이터에 적용하여 배경 레이어와 피사체 레이어로 구성된 장면 레이 어를 식별한다. 그런 다음, 액터 레이어는 두 레이어 사이에 삽입될 수 있다. 다른 예에서, 시스템은 피처의 존재 또는 비존재를 사용하여 합성 입력 데이터의 어느 영역이 어느 레이어에 할 당되는지를 결정한다. 예를 들어, 하늘에 대해 떠 있는 풍선의 합성 입력 데이터와 풍선을 식별하도록 훈련된 특징 분류기를 활용하는 경우, 풍선이 존재하는 합성 입력 데이터의 영역은 피사체 레이어에 할당되고, 풍선이 존재하지 않는 영역(즉, 하늘의 이미지)은 배경 레이어에 할당된다. 마찬가지로, 비이진 출력을 갖는 다른 특징 분류기는 입력 데이터(예컨대, 합성 입력 데이터 및 멀티레이어 입력 데이터)의 영역을 복수의 레이어에 할당하 기 위해 임계값을 설정하거나 다른 방식으로 활용될 수 있다. 전문가 시스템 접근법은 시스템이 배경 레이어 및 피사체 레이어를 식별하기 위해 사용될 수 있다. 전문가 시스 템 접근법은 배경 레이어와 피사체 레이어를 구별하기 위한 일련의 규칙을 사용한다. 일부 예에서는, 특정 유형 의 콘텐츠가 더 앞에 있어 방해받을 가능성이 적은 피사체 레이어에 우선순위를 지정하여 할당될 수 있다. 예를 들어, 텍스트 콘텐츠는 액터 레이어 앞에 위치하여 액터 레이어에 의해 가려지지 않도록 우선순위를 지정하여 피사체 레이어로 할당될 수 있다. 그런 다음 이미지의 다른 영역을 배경 레이어에 할당할 수 있다. 마찬가지로, 멀티레이어 입력 데이터의 복수의 장면 레이어는 콘텐츠의 유형에 따라 멀티레이어 장면에서 순서 또는 레이어/깊이 위치를 결정하기 위해 순위가 매겨질 수 있다. 예를 들어, 특정 키워드가 포함된 텍스트는 다 른 키워드보다 순위가 높거나 우선순위가 지정될 수 있다. 특정 유형의 이미지, 색상, 모양, 데이터 유형, 크기 또는 기타 특징은 다른 것보다 더 높은 순위 또는 더 높은 우선순위를 나타낼 수 있다. 일부 예들에서, 동일한 레이어 입력 데이터(예컨대, 장면, 레이어, 이미지, 비디오 피드 등)의 두 상이한 영역 은 두 개의 상이한 레이어 위치(즉, 레이어들이 적층될 때의 깊이 위치)에 위치될 수 있다. 두 개의 서로 다른 영역 중 하나는 배경 레이어에 배치되고 다른 영역은 피사체 레이어에 할당될 수 있다. 일부 예에서는, 입력 데 이터의 일부 영역(예를 들어, 장면, 레이어, 이미지, 비디오 피드 등)을 배경 레이어에 할당하고 다른 영역을 피사체 레이어에 할당할 때 애플리케이션별 지식이 고려된다. 예를 들어, 스프레드시트 애플리케이션(예를 들어, Excel 또는 Google 스프레드시트)의 경우, 시스템은 격자선을 배경 레이어에 할당하고 스프레드시트 셀의 숫자와 텍스트는 피사체 레이어에 할당될 수 있다. 따라서, 배경 레이어와 피사체 레이어 사이에 액터 레이어를 추가하면, 액터가 격자선 앞에 서지만 셀 자체의 콘텐츠를 가리지 않는다. 다른 예로, 시스템은 애플리케이션 윈도우 상단의 툴바와 같이 메뉴 조작과 관련된 입력 데이터 영역의 우선순위를 낮추어 이들이 백그라운드 레이 어에 할당될 수 있다. 이렇게 하면 피사체 레이어 내의 관심 콘텐츠가 일반적으로 프레젠테이션과 관련이 없는 영역에 의해 가려지지 않고 표현될 수 있다. 도 4는 그래프(즉, 관심 있는 피사체 콘텐츠)가 액터 앞에 오버레 이된 예시를 도시한다. 장면에서 레이어의 위치(즉, 레이어, 예를 들어 피사체 레이어와 배경 레이어 내의 깊이 위치)는 레이어 내 또 는 레이어 상의 콘텐츠 위치에 따라 결정될 수 있다. 예를 들어, 장면의 상단에 있는 데이터 콘텐츠의 우선순위 가 낮아져 배경 레이어에 할당되는 반면, 장면의 중간 또는 하단에 있는 콘텐츠는 우선순위가 높아져 피사체 레 이어에 배치된다. 이에 따라, 액터의 머리가 장면의 상부에 위치한 콘텐츠에 의해 가려지지 않고 액터의 하부 및 가능한 중간 부분을 가리고 있는 콘텐츠를 액터가 서서 표현할 수 있다. 시스템은 사용자 지원 레이어링을 사용할 수 있다. 여기에는 사용자 인터페이스를 사용하여 레이어링을 더 변경 하거나 사용자 정의하도록 진행될 수 있는 시작점을 사용자에게 제공하기 위한 디폴트 \"제안\" 또는 \"권장\" 레이 어링 순서를 제공하기 위해 전술한 자동화된 레이어링 접근법을 활용하는 것이 포함된다. 멀티레이어 장면의 레이어들은, 액터를 나타내는 비디오 스트림과 같은 데이터 피드가 멀티레이어 장면의 레이 어들 중 적어도 일부 또는 하나 이상을 방해하지 않고 공존, 배치 및/또는 디스플레이 유닛에 디스플레이되도록, 다양한 깊이 정도로 위치 및 표시된다. 일부 예에서, 깊이 정보와 같은 위치 정보는 입력 데 이터에서 추출되어 서로에 대해 레이어를 배치하는 데 사용된다. 깊이 정보는 입력 데이터 또는 데이터 피드에 서 추출될 수 있다. 장면과 관련된 깊이 정보가 추출되면 카메라와의 거리(깊이)에 따라 장면의 일부가 서로 다 른 레이어에 배치된다. 이러한 부분은 장면에 정의된 레이어 중 하나에 할당된다. 배경 레이어와 피사체 레이어 는 모두 장면 내 레이어의 인스턴스이다. 액터 레이어는 카메라로부터 적어도 한 명의 액터의 거리에 따라 장면 의 레이어들 사이에 추가될 수 있다(또는 마찬가지로 액터 표현은 3차원 공간 내의 특정 깊이에 추가될 수 있다). 따라서, 액터가 장면의 다양한 레이어 또는 3차원 공간의 깊이에 디스플레이되는 이미지 부분 사이에서 앞뒤로 움직이는 것으로 볼 수 있다. 그리고 액터는 이러한 레이어 또는 깊이에 배치된 피사체 또는 콘텐츠와 인터랙션할 수 있다.일부 예들에서, 깊이 감지 이미징 유닛(스테레오스코픽 카메라, 함께 사용되는 복수의 카메라, 라이다로 보완된 2D 카메라, 깊이 감지 기술을 갖는 휴대 전화의 카메라 등을 포함하나 이에 국한되지 않음)은 이미징 유닛에 의 해 캡처된 장면으로부터 깊이 정보를 추출하기 위해 사용된다. 일부 예에서, 이미징 유닛은 장면에서 깊이 정보 를 추출하기 위해 장면의 깊이 단서를 인식하는 것과 함께 사용된다. 이러한 깊이 정보는 액터를 캡처하는 데이 터 피드(예를 들어, 비디오 스트림)와 다층(3차원) 입력 데이터에서 식별될 수 있다. 단일 장면의 상이한 부분 또는 영역은 상이한 깊이 레벨로 할당될 수 있으며, 상기 깊이 레벨은 3차원 장면의 레이어 또는 다양한 깊이 내의 부분 또는 영역의 위치를 정의한다. 시스템은 크로마키 그린 스크린, 가상 그린 스크린 기술, 골격/인간 포즈 인식/추정, 이미지 내에서 액터를 분 리하도록 훈련된 신경망을 포함하되 이에 한정되지 않는 다양한 기법을 사용하여 비디오 스트림 또는 데이터 피 드에서 액터의 이미지를 추출한다. 신경망을 활용하는 실시예의 예시적인 예로서, 신경망은 주석이 달린 인간(즉, 액터) 얼굴 및 신체 부위를 포함 하는 이미지의 훈련 세트를 사용하여 훈련된다. 훈련이 완료되면, 신경망은 이미지의 어느 영역이 사람의 얼굴 또는 신체 부위의 일부일 가능성이 가장 높은지 이진 방식으로 태그를 지정할 수 있는 분류기로 사용된다. 식별 된 영역은 프레임 단위로 액터를 캡처하는 비디오 스트림 또는 데이터 피드에서 추출될 수 있는 액터의 이미지 로 간주된다. 본 명세서에 설명된 시스템은 입력의 스케일링 및 정의된 레이어링 순서를 추가로 사용한다. 일부 예들에서, 둘 이상의 액터가 공유 공간을 갖는 동일한 장면에 디스플레이된다. 공간은 가상 공간(예를 들어, 가상 댄스 플로 어)이거나 이미징 유닛으로 캡처된 액터의 거실 공간과 같은 실제 물리적 공간일 수 있다. 예를 들어, 공유 공 간에 한 명 이상의 액터를 디스플레이할 때, 액터를 캡처하는 이미지의 스케일링 및 정규화는 비디오 스트림 또 는 데이터 피드에 캡처된 액터의 이미지 크기의 비례를 제공하여 디스플레이에서 비슷한 크기가 되도록 한다. 이는 액터에 특징 인식을 적용하고 스케일을 정규화하여 수행할 수 있다. 공유 공간이 액터의 실제 물리적 공간 인 경우, 해당 액터는 디스플레이에 표시되는 다른 액터의 스케일링 및 정규화를 위한 참조로 사용되어 두 액터 가 동일한 스케일에 비례하여 크기가 조정될 수 있다. 전술한 바와 같이, 일부 실시예에서, 깊이 정보가 데이터 피드에서 추출되고, 카메라로부터의 거리(깊이)에 따 라 서로 상대적인 레이어들을 배치하는 데 사용된다. 일부 예에서, 한 명 이상의 액터를 공유 공간에 결합할 때, 액터의 이미징 유닛(예를 들어, 카메라) 사이의 거리를 사용하여 다른 액터의 로컬 공간에 추가될 때 해당 액터의 스케일을 조절할 수 있다. 보다 구체적으로, 액터가 카메라에 가까워지면 더 커지고 카메라에서 멀어지 면 더 작아진다. 또한, 이미징 유닛과 액터 사이의 동적 거리에 따라 액터의 깊이 또는 레이어 위치를 결정할 수 있다. 액터가 카메라에 가까워지면, 해당 액터를 캡처하는 레이어의 깊이 위치가 멀티레이어 장면에서 레이 어의 앞쪽으로 지정될 수 있다(또는, 다른 방식으로 말하자면, 액터 표현이 3차원 공간이나 장면에서 더 앞쪽으 로 추가될 수 있다). 마찬가지로, 액터가 카메라로부터 더 멀리 이동하면, 해당 액터를 캡처하는 레이어의 깊이 또는 레이어 위치가 멀티레이어 장면의 레이어들 사이에서 더 뒤로 할당될 수 있다(또는, 다른 방식으로 말하자 면, 액터 표현은 3차원 공간 또는 장면에서 더 뒤로 추가될 수 있다). 레이어들의 크기 및 동적 위치 조절의 더 많은 예들이 아래에 제시되어 있다. 일부 예들에서, 특징 인식과 같은 수단에 의해 객체(액터 또는 다른 논액터 레이어 상의 객체 포함)에서 식별된 특징(예컨대, 크기, 모양, 선명도, 밝기, 방향, 존재/비존재 또는 기타 특징)은 그러한 동적 위치 지정을 가능하게 하는 깊이 큐로서 사용 된다. 일부 예에서는, (액터에서 식별된 특징의 크기를 비교하여 결정되기 때문에) 액터의 상대적 크기가 이러 한 깊이 큐로 사용된다. 일예에서, 액터(B)가 액터(B)를 캡처하는 카메라에 서 있는 것보다 액터(A)가 액터(A) 를 캡처하는 카메라에 액터(B)가 더 가까이 서 있는 경우, 액터(A)의 크기가 비례적으로 더 크게 디스플레이되 고 디스플레이(예를 들어, 출력 비디오)에서 액터(B)의 디스플레이되는 크기 및 깊이/레이어 위치와 비교하여 더 앞쪽에 있는 깊이/레이어에 추가된다. 다른 예에서, 액터(B)가 액터(B)를 캡처하는 카메라에 서 있는 것보다 액터(A)가 액터(A)를 캡처하는 카메라에 액터(B)가 더 멀리 서 있는 경우, 액터(A)는 비례적으로 작게 디스플레 이되고 디스플레이(예를 들어, 출력 비디오)에서 액터(B)의 디스플레이되는 크기 및 레이어 위치와 비교하여 앞 쪽에서 더 멀리 떨어진 깊이/레이어에 추가될 수 있다. 일부 예들에서, 이러한 기능은 두 명 이상의 액터가 가 상(즉, 두 명 이상의 액터가 동일한 물리적 공간에 존재하지 않는) 셀카 또는 사진을 함께 촬영할 수 있도록 하 며, 제1 액터가 카메라로부터 더 멀리 떨어져서 그 액터가 제2 액터의 등을 가로질러 팔을 뻗어 제2 액터의 어 깨를 바로 지나 다시 전개되어 나타나는 손을 제외한 대부분의 팔을 가릴 수 있도록 할 수 있다. 도 7은 둘 이상의 액터들 간의 참여를 위한 깊이 기반 레이어링/포지셔닝의 예를 도시한다. 배경 레이어는 식별되어 레이어들 중 가장 뒤쪽에 배치된다. 제1 액터는 제1 데이터 피드 또는 제1 비디오 스트림에서 추출되 어 시스템에 의해 식별된다. 제2 액터는 제2 비디오 스트림의 제2 데이터 피드에서 추출되어 시스템에 의해 식 별된다. 액터와 카메라 사이의 각 거리(예를 들어, 이미징 유닛)에 기초하여, 제1 액터 레이어/표현과 제2 액터 레이어/표현 사이의 깊이/레이어 위치가 결정된다. 장면은 제1 액터 레이어/표현이 제2 액터 레이어/ 표현 앞에 추가되는 것을 도시한다. 이 경우, 제1 액터와 제1 액터를 녹화하는 카메라 사이의 거리가 제2 액터와 제2 액터를 녹화하는 카메라 사이의 거리보다 작다. 출력 비디오는 제1 액터 레이어 가 제2 액터 레이어 뒤에 추가되는 것을 도시한다. 이 경우, 제1 액터와 제1 액터를 녹화하는 카메라 사이의 거리는 제2 액터와 제2 액터를 녹화하는 카메라 사이의 거리보다 더 크다. 따라서, 이 장면 에서 제1 액터의 팔의 일부가 제2 액터에 의해 가려진다. 또 다른 예에서, 둘 이상의 액터 간의 참여를 위 한 깊이 기반 레이어링/포지셔닝은 각 액터가 초기 위치에서 다음 위치까지 이동하는 거리를 식별할 수 있다. 즉, 깊이 기반 레이어링/포지셔닝은 액터와 해당 카메라 사이의 거리 대신에 각 액터의 이동 거리를 녹화하는 카메라에 기초할 수 있다. 레이어들의 자동화된 동적 순서에 대한 대안으로서, 수동 사용자 인터페이스도 사용될 수 있다. 일부 예들에서, 트리거(리모컨, 음성 또는 음성 트리거, 제스처 또는 키 프레스를 포함하되 이에 한정되지 않음)를 사용하여, 액터는 자신의 레이어를 앞뒤로 이동하여 장면 내의 레이어들의 위치를 변경할 수 있다. 예를 들어, 이러한 접 근 방식은 카메라로부터 더 가까이 또는 더 멀리 이동하지 않고도 앞서 설명한 셀카 또는 사진 레이어링 문제를 해결할 수 있다. 크기 또는 스케일을 수정할 수 있는 것처럼, 출력 비디오 내에서 액터의 측면 위치도 수정할 수 있다. 일부 예 들에서, 액터 레이어 내의 액터의 측면 위치는 단순히 액터 자신의 로컬 공간 내에서 액터의 위치를 식별함으로 써 제어된다. 액터가 자신의 카메라 앞에 있는 액터의 자신의 로컬 공간 내에서 옆으로(예를 들어, 왼쪽 또는 오른쪽) 이동하면, 출력 비디오에 표시되는 액터 레이어 내의 액터도 역시 이동한다. 대안으로, 그 레이어 내의 액터의 위치는 액터의 자신의 로컬 공간 내의 액터의 위치를 직접적으로 표현하는 것은 아니다. 일부 예들에서, 멀티레이어 장면의 해당 액터 레이어 내의 액터의 위치는 제자리에 고정되어 있다. 디스플레이 유닛에 의해 묘사된 실제 액터가 움직이는 동안, 레이어 내의 액터의 위치는 출력 내의 위치에 고정된다. 예를 들어, 도 6의 멀티레이어 장면의 액터 레이어에 캡처된 액터가 카메라의 시야를 따라 이동할 수 있으며, 각 비 디오 프레임에서 그의 얼굴 이미지는 기본적인 얼굴 추적 방법을 사용하여 추출되고 출력 비디오에서 고정된 위 치에 배치된다. 일부 예들에서, 액터 레이어 내에서 액터의 측면 또는 수직 위치의 변화는 액터에 의해 수행되거나 실행된 제스 처의 인식에 의해 트리거된다. 액터(즉, 사용자 또는 카메라에 의해 캡처된 사람)가 수행하는 제스처가 이미징 유닛에 의해 캡처된다. 예를 들어, 이러한 제스처 인식에는 시스템이 액터의 모의 달리기 포즈를 인식하여 차례 로 액터 레이어 내에서 액터의 위치를 변경하도록 트리거하는 것이 포함된다. 즉, 이미징 유닛에 의해 캡처된 액터가 수행하거나 실행한 제스처가 시스템에 의해 식별된다. 그 결과, 액터 레이어 내에 묘사된 액터의 측면 또는 수직 위치(즉, 카메라로 촬영된 액터의 그래픽 표현)가 변경된다. 도 8은 제스처 유도 동작(예를 들어, 시 각적 표현에서 모션, 움직임 또는 변화)의 또 다른 예를 도시한다. 도 8에서, 본 개시의 이러한 특징은 액터가 비디오 게임을 플레이하는 동안 이미징 유닛을 사용하여 액터의 이미지 또는 비디오가 묘사되는 비디오 게임 환 경의 애플리케이션에서 제공된다. 이 장면에서, 이미징 시스템에 의해 캡처된 액터의 달리기 제스처는, 액 터의 초기 위치 및 이 장면에 묘사된 액터가 달리는 것을 나타내는 제스처 유도 동작에 의해 설 명되는 바와 같이, 액터 레이어 내에서 액터의 위치가 달리도록 할 수 있는 시스템에 의해 인식된다. 장면(80 0)은 시스템이 달리기 제스처를 인식하기 전의 액터와 그 후의 액터의 레이어 내에서 액터의 위치를 도시한다. 피사체 레이어가 액터 레이어 앞에 위치하기 때문에 피사체 레이어의 피사체(예를 들어, 집)가 액터 이미지를 가리고, 배경 레이어의 피사체(예를 들어, 성 , 나무, 탑)가 액터 레이어 뒤에 도시된다. 마찬가지로, 이미징 유닛에 의해 녹화되는 액터의 모의 점프 제스처는 물리적 공간(즉, 가상이 아닌/ 현실 공간)의 물리적 액터가 실제로 지상을 떠나지 않았더라도 액터 레이어 내의 가상 액터의 위치를 위치(80 6)에서 위치로 스크린 상에서 수직 점프하게 할 수 있다. 다시 말하지만, 피사체(예를 들어, 물음표 박스 )가 액터 레이어 위의 레이어에 표시되므로, 액터 레이어에 표시된 점프하는 가상 액터를 가린다. 장면 은 또한 이 시스템을 활용하여 장면 내 동일한 레이어의 깊이 위치에 두 명 이상의 액터를 멀티레이어 장 면에 표현되는 방법을 도시한다. 이 장면에는 두 명의 액터(808 804)가 표시된다. 이 장면에서 물음 표 박스는 인터랙티브 객체일 수 있으며, 이에 대한 자세한 내용은 아래에 제공된다. 일부 예에서, 액터 레이어에서 액터로 묘사될 액터로부터 액터의 일부 이미지(예를 들어, 얼굴, 머리 또는 흉상)가 추출될 수 있다. 이러한 기능의 비제한적인 예로는, 도 6의 장면에 도시된 바와 같이, 프레젠테이 션 중에 비디오 피드에서 추출된 액터의 흉상을 액터 레이어 내의 고정된 위치에 디스플레이하는 것이 포함된다. 도 6에 도시된 바와 같이, 액터가 카메라의 시야를 이동함에 따라, 각 비디오 프레임에서 그의 얼굴 이미지가 기본 얼굴 추적 방법을 사용하여 추출되고 프레젠테이션의 멀티레이어 장면에서 고정된 위치에 배치된 다. 다른 예로, 도 9에 도시된 바와 같이, 비디오 스트림에서 추출된 액터의 얼굴은 액터 레이어에 추가된 가상 캐릭터 또는 아바타에 부착된다. 이 장면은 배경 레이어, (아바타를 나타내는) 피사 체 레이어 및 (액터를 나타내는) 액터 레이어를 포함한다. 이러한 가상 캐릭터는 특히 본 명세서에 설명된 제스처 및 동작 인식 기술을 사용하여 비디오 게임 설정에서 액터에 의해 제어될 수 있다. 각 비디오 프레임에 서 액터의 얼굴 이미지는 기본적인 얼굴 추적 방법을 사용하여 추출되고 아바타의 얼굴 대신에 배치된다. 액터 의 일부 이미지가 본 명세서에서 별도로 논의되지만, 액터의 이미지 또는 그래픽 표현은 이미징 유닛으로 액터 전체를 캡처하는 것에 제한되지 않고 이미징 유닛에 의해 캡처된 액터의 신체 일부를 포함한다는 것이 이해되어 야 한다. 본 개시는 시스템이 디스플레이 유닛을 사용하여 볼 수 있는 피사체(이하, 객체, 예를 들어, 피사체 레이어에 포함된 그래픽 객체라고도 함)와 액터가 인터랙션할 수 있도록 하는 특징을 제공한다. 이러한 객체는 액터 레이 어 내의 액터를 통해 액터에 의해 인터랙션될 수 있다. 물리적 액터는 시스템의 제스처 인식 및 동작 인식 기능 을 사용하여 이러한 인터랙션을 가능하게 할 수 있다. 본 명세서에 설명된 바와 같이, 인터랙션은 멀티레이어 장면에 디스플레이되는 액터 이미지가 장면에 디스플레이되는 객체 이미지 주위 또는 객체 이미지에 대한 액터 이미지의 존재, 제스처 또는 동작에 의해 객체와 접촉(또는 \"충돌\"을 갖는다고도 함), 제어 또는 그렇지 않으면 객체가 트리거되도록 하는 것을 말한다. 객체는 멀티레이어 장면의 임의의 장면 레이어(즉, 배경 레이어, 피사체 레이어, 또는 액터 레이어)에 위치할 수 있다. 일부 객체는 적어도 하나의 액터에 의해 인터랙션될 수 있다. 이를 인터랙티브 객체라고 한다. 인터랙 티브 객체는 멀티레이어 장면을 구성하는 여러 레이어 중 적어도 하나에 있는 장면에 묘사된 가상 객체이다. 여 기서 설명하는 인터랙티브 객체는 장면에 배치되거나 생성된 가상 객체로, 액터와 독립적이며(즉, 액터와 분리 가능하며) 인터랙션을 통해 시각적 상태 변화와 같은 일부 동작을 트리거하는 가상 객체이다. 장면의 레이어에 묘사된 피사체, 객체 또는 액터 간의 인터랙션 측면에서, 인터랙티브 객체는 액터 이미지 자체의 전체 또는 적 어도 일부를 포함한다. 예를 들어, 장면에서 두 액터들 사이에 가상 인터랙션이 있는 경우, 그러한 인터랙션은 제3자 액터(들)(즉, 한 액터에 대한 다른 액터(들))가 객체로 취급되는 유사한 방식을 사용하여 설정된다. 일부 예들에서, 액터의 움직임 또는 동작의 인식은 인터랙티브 객체와의 인터랙션을 유발하기 위해 사용된다. 액터의 동작 또는 움직임의 인식은 유 온리 룩 원스(YOLO), 실시간 객체 감지 방법, (골격 기반, 윤곽 기반 및/ 또는 볼륨 기반 모델에 기초한) 인간 포즈 추정 등과 같은 비디오 인식 접근법을 사용하여 수행될 수 있다. 마 찬가지로, 액터의 신체 부위(손, 발, 머리 등)는 YOLO, 사람 포즈 추정(골격 기반, 윤곽 기반 및/또는 볼륨 기 반 모델 기반 포함) 등과 같은 객체 인식 접근법을 통해 추적할 수 있다. 도 10에 도시된 바와 같이, 장면 에서 액터의 신체 부위의 위치가 인터랙티브 객체와 겹치거나 접촉하면, 액션이 트리거된다. 이 문맥에서 액션은 디스플레이 유닛에 디스플레이되는 장면 내에서 특정 시각적 또는 청각적 변화를 의미한다. 트리거된 동작은 특정 애니메이션의 디스플레이, 특정 사운드 제공 등이 될 수 있다. 일부 예에서, 인터랙션은 장면에서 인터랙션 객체(및/또는 액터 이미지 자체)의 시각적 상태의 변화가 관찰되는 곳에서 그러한 동작이 발 생하게 한다. 이러한 변화의 예로는 소정의 규정된 애니메이션, 움직임, 색상의 변화, 변형, 상태의 변화 등이 있다. 프로세스는 액터 인터랙션으로 인한 동작의 프로세스를 나타낸다. 단계에서, 시스템은 인터 랙티브 객체와 인터랙션하도록 할당된 특정 액터의 신체 부위를 식별한다. 출력 비디오 또는 그 장면 내에서 액 터의 신체 부위(이 예에서는 손)가 인터랙티브 객체와 접촉하여 인터랙티브 객체와 인터랙션하면 (즉, 액터의 신체 부위가 적어도 부분적으로 인터랙티브 객체와 겹치는 경우), 단계에서 신 체 부위와 인터랙티브 객체 간의 인터랙션이 식별된다. 이에 대한 응답으로, 동작이 시스템에 의해 트리거된다. 일부 예들에서, 디스플레이 유닛을 사용하여, 물리적 액터(예컨대, 이미징 유닛에 의해 감지된 실 제 게임 플레이어)의 비디오 또는 이미지가 합성 비디오 출력 내에 제시되는 가운데, 액터와 인터랙티브 객체 사이의 인터랙션이 표현될 수 있다. 이러한 액터의 그래픽 표현은 예를 들어 비디오 게임 환경에서 이 기능에 대한 사용자의 경험을 향상시킨다. 도 8의 제스처 유도 동작도 트리거된 동작의 한 유형이다. 도 8에 설명된 바 와 같이, 액터가 서 있는 포즈에서 달리는 제스처로 제스처를 변경(이 예에서는 포즈 변경)하면 장면에 있 는 그 장면에서 액터는 액터의 동작이 달리게 된다. 이미징 유닛은 게임 플레이어인 액터가 제스처를 취하는 모습을 캡처한다. 그런 다음 데이터 획득 유닛이 해당 정보를 획득하고, 이에 따라 제어 유닛(10 8)이 장면 내의 액터가 해당 동작을 수행하도록 한다. 도 8에 도시된 예들에서, 인터랙티브 객체는 제스처 유도동작에 참여하지 않는다. 일부 예들에서, 그러한 인터랙션 기능은 액터가 파워포인트 또는 구글 슬라이드와 같은 소프트웨어의 프레젠테 이션 슬라이드에서 애니메이션을 트리거하는 데 사용될 수 있다. 일부 예들에서, 가상 교실(즉, 장면)의 학생 (즉, 액터)은 가상 플래시카드(즉, 인터랙션 객체)를 가상으로 터치하여 이를 뒤집도록(즉, 트리거되거나 유발 되는 동작을) 할 수 있다. 일부 예에서는, 그 동작은 비디오 게임에서 점수가 올라가는 것과 같은 다른 상태의 변화이다. 물론, 다양한 유형의 객체와 액션이 결합될 수 있다. 도 11에 예시적인 인터랙션이 나와 있다. 도 11 에서 캡처된 멀티레이어 장면인 장면에서, 액터는 장면 내의 인터랙티브 객체와 인터랙션하 고 있다. 액터는 이 멀티레이어 장면에서 피사체 레이어와 배경 레이어 사이의 액터 레이어 에 추가된다. 장면에서, 액터는 피사체 레이어 및 배경 레이어의 인터랙티브 객체(예를 들어, 1108, 1114, 1112)와 인터랙션할 수 있다. 이 장면에서, 인터랙티브 객체(또는 피사체 레이어의 피사체라 고도 함)는 피사체 레이어에 위치(즉, 장면의 적층된 레이어들에 대한 깊이 위치)하고, 인터랙티브 객체 (1108 및 1114)는 이 장면의 모든 객체가 액터에 의해 인터랙션 가능한 배경 레이어에 위치한다. 일례로, 인터 랙션은 손가락으로 인터랙티브 객체를 가리켜 동작을 트리거하는 것과 같은 동작일 수 있다. 이는 제스처 기반 인터랙션의 예로, 액터의 신체 부위(이 예에서는 손가락)가 특정 제스처가 감지된 것으로 식별되는 경우이다. 이러한 제스처는 이미징 유닛을 사용하여 시스템에서 감지한다. 대안으로, 도 10에 도시된 예시적인 구현과 마 찬가지로, 액터의 신체 부위가 인터랙티브 객체와 접촉(따라서 인터랙션)하거나 겹치는 경우 인터랙티브 객체가 취할 동작을 트리거할 수 있다. 이는 접촉 기반 인터랙션의 예이다. 다른 예로는 근접 기반 인터 랙션이 있는데, 액터가 특정 임계 거리 이상으로 인터랙티브 객체에 가까워지면 인터랙티브 객체에 의해 동작이 시연된다. 장면에서, 동작이 트리거된다. 이 예에서는, 계산 함수가 트리거되고 계산된 결과의 그래픽 표 현이 동작이다. 장면 1108에서 1110까지 인터랙티브 객체의 시각적 상태가 변경이 관찰된다. 도 11에 도시된 바 와 같이, 본 개시의 시스템 및 방법은 학습 과정에 인터랙티브 요소를 추가하여 학생의 학습 능력을 향상시키기 위한 교육 환경에 적용될 수 있다. 예를 들어, 액터가 프레젠테이션의 한 장면에서 다음 장면으로 넘어가기 위해 손으로 왼쪽에서 오른쪽으로 스와 이프 동작을 하거나, 이전 슬라이드로 장면을 변경하기 위해 오른쪽에서 왼쪽으로 스와이프 동작을 할 수 있다. 마찬가지로 다른 예에서, 액터가 손으로 필기구를 쥐는 동작을 흉내내는 제스처를 만들 수 있다. 이 제스처를 인식하면 액터가 가상의 필기구를 들고 있는 것처럼 장면의 슬라이드 가상 공간에 드로잉이 트리거된다. 게임 예시에서는, 액터의 제스처를 인식하여 달리기, 점프, 가상 농구공 던지기, 가상 골프채 휘두르기, 마법 주문 시전, 인터랙티브 객체 밟기, 발차기, 당기기, 주먹질 등과 같은 동작을 트리거하는 데 사용할 수 있다. 본 개 시된 시스템 및 방법의 이러한 애플리케이션 및 기타 애플리케이션은 비디오 지원 인터랙티브 운동의 의도된 목 적에 대한 사용자의 경험을 향상시키기 위해 실현될 수 있다. 일부 예들에서, 한 명 이상의 액터가 관련될 수 있다. 한 액터는 다른 액터와 인터랙션할 수 있고, 두 명 이상 의 액터가 인터랙티브 객체와 협력적으로 인터랙션할 수 있고, 두 명 이상의 액터는 각각 다양한 인터랙티브 객 체와 인터랙션할 수 있을 수 있다. 예를 들어, 일 실시예(예를 들어, 비디오 게임)에서, 제1 액터의 손은 제2 액터를 가상으로 타격하는 데 사용될 수 있다. 이 인터랙션에서, 제1 액터의 '타격' 제스처는 제2 액터와의 인 터랙션으로, 제2 액터는 인터랙티브 객체로 취급된다. 제1 액터가 제2 액터와 가상으로 접촉하면, 제2 액터인 인터랙티브 객체의 움직임, 변형 또는 색상 변경과 같은 동작(즉, 시각적 상태의 변화)이 트리거된다. 예를 들 어, 인터랙티브 객체와 인터랙션하기 위해 사용될 수 있는 제스처에는 타격, 발차기, 가라테-지르기, 누르기, 두드리기, 잡기, 비틀기, 밟기, 점프, 돌진, 당기기, 밀기, 들어올리기, 떨어뜨리기, 던지기, 먹기, 마시기 등 이 포함될 수 있다. 액터들이 서로 인터랙션할 수 있는 것처럼, 액터에 의해 제어되는 객체도 다른 객체(다른 액터 포함)와 인터랙 션할 수 있다. 예를 들어, 액터가 출력 비디오 상의 인터랙티브 객체와 인터랙션할 수 있는 장난감 검(또는 장 난감 광선검)을 들고 있다. 일부 예에서, (검이 가상인지 실제인지, 액터가 물리적으로 같은 공간에 있는지 또 는 원격 위치에 있는지 간에) 액터가 다른 액터 또는 다른 액터의 검을 포함한 인터랙티브 객체와 인터랙션할 수 있는 (제스처 인식 및 증강 현실 접근 방식을 적용하여 구현된) 가상 무기를 들고 있다. 액터가 제어하는 객 체의 비제한적인 예로는 맨손 전투 무기, 검, 광선검과 같은 가상 무기, 총이나 눈덩이와 같은 발사체 무기가 있다. 교육 또는 비즈니스 애플리케이션에서, 그러한 액터 제어 객체는 (향상된 추적을 위해 한쪽 또는 양쪽 끝 에 시각적으로 인식 가능한 특징이 있는) 필기구, 포인팅 장치 등을 포함한다. 일부 예들에서, 액터는 원격 제어 장치와 같은 아이템을 사용하여 장면에서 동작을 트리거한다. 이러한 원격 제 어 장치는 원격 슬라이드 어드밴서, 비디오 게임 컨트롤러, 장난감 광선검, 장난감 총 등을 포함하나 이에 국한되지 않는다. 일부 예에서는, 액터가 음성이나 소리를 사용하여 장면에서 동작을 트리거한다. 구체적으로, 액터 는 \"다음 슬라이드\"라고 말하여 장면을 변경하는 동작을 트리거하거나 프레젠테이션의 한 슬라이드에서 다음 슬 라이드로의 동작을 트리거하고, 마찬가지로, \"이전 슬라이드\"라고 말하여 프레젠테이션의 한 슬라이드에서 이전 슬라이드로의 장면을 변경하는 동작을 트리거할 수 있다. 도 12a에 도시된 바와 같이, 둘 이상의 액터들이 가상 공간에서 동일한 인터랙티브 객체와 인터랙션할 수 있다. 도 12a의 이 장면에서, 두 명의 액터, 즉 제1 액터 및 제2 액터가 배경 레이어 전방에 시각 화된다. 도 12a의 장면은 출력 비디오에 디스플레이되거나 이 멀티레이어 장면에서 시각화된 두 액터(1204, 1206) 사이의 가상 인터랙션을 나타낸다. 이 예에서 인터랙션은 두 액터(1204, 1206)가 인터랙션 객체와 접촉하거나 겹치는 것으로 도시된다. 이러한 인터랙션에 의해 트리거되는 동작은 인터랙션이 두 액터에 의해 동 시에 완료되거나 두 액터(1204, 1206) 모두에 의해 완료될 때만 트리거된다. 이 예에서, 멀티레이어 장면의 레 이어들은 다른 모든 레이어들 뒤에 배치된 배경 레이어, 배경 레이어 앞에 배치된 제2 액터 레이어, 제2 액터 레이어 앞에 배치된 인터랙티브 객체 레이어, 그리고 인터랙티브 객체 레이어 앞에 배치된 제1 액터 레이 어를 포함한다. 예시된 바와 같이, 도 12a는 2개의 액터 레이어를 포함하여 총 4개의 서로 다른 레이어를 디시 플레이에 예시한다. 일부 예들에서, 복수의 액터들이 동일한 액터 레이어에 포함될 수 있다. 일부 예들에서, 하나 이상의 액터들이 동일한 이미징 유닛 앞에 물리적으로 동일한 공간에 있을 때, 단일 액터 레이어가 복수의 액터들을 묘사할 수 있다. 해당 액터 이미지를 분리하면, 이들이 동일한 액터 레이어에 포함된다. 다른 예로, 복수의 액터가 서로 다른 이미징 유닛 앞에 물리적으로 떨어져 있는 경우 단일 액터 레이어가 복수의 액터를 묘사할 수 있다. 이들 의 액터 이미지를 분리하면, 액터 이미지가 동일한 액터 레이어에 삽입된다. 도 12b는 인터랙티브 객체의 개념에 대한 또 다른 시각화이다. 이 실시예에서, 인터랙티브 객체는 3차원 장면에 배치되고, 복수의 액터/사용자 표현(1204,1206)은 인터랙티브 객체와 인터랙션한다. 복수의 사용자들(1204,1206)은 시스템의 대응하는 이미징 유닛들에 의해 캡처된 실제 사용자들에 의해 인터랙티 브 객체와 인터랙션하도록 유발된다. 신체 컷오프 및 외삽법 사용자/액터로부터 카메라의 거리가 주어지면, 사용자 신체의 일부가 카메라 뷰의 경계를 넘어 전개될 수 있다. 이러한 경우, 사용자의 \"누락된\" 또는 \"잘린\" 신체 부분은 액터 레이어 및/또는 가상 세계에 포함된 사용자/액 터의 표현에 인위적으로 추가될 것이다. 마찬가지로, 사용자의 가상 표현의 균형을 인위적으로 \"채우거나\" \"페이크\"하면서 사용자의 가상 표현에 물리 적 (카메라 뷰에서 보이는 것처럼) 사용자의 일부만을 반영하고자 하는 다른 경우가 있을 수 있다. 예를 들어, 사용자/액터는 하나의 물리적 자세(예컨대, 앉거나 누워있는 것과 같은 편안한 자세)에서 시스템을 조작하도록 선택할 수 있고, 사용자의 가상 표현은 다른 자세(예컨대, 서 있거나 걷거나 뛰는 것과 같은 자세) 에 있기를 원할 수 있다. 이러한 경우, 사용자의 가상 표현의 일부(예를 들어, 상체)는 사용자의 실제 신체 이 미지 및 움직임을 보다 직접적으로 반영할 수 있는 반면, 액터의 가상 표현의 다른 일부(예를 들어, 하체)는 액 터의 실제 신체와 유사한 부분을 반영하지 않는 상태로 인위적으로 표현될 수 있다. \"신체가 잘린\" 경우뿐만 아니라 \"이완된 자세\"의 경우 및 다른 경우 모두 카메라 이미지에 나타나는 것과 다르 게 신체의 일부가 표현될 수 있다. 일 예에서, 사용자의 신체 일부(손, 머리, 다리, 발을 포함하되 이에 국한되지 않음)가 카메라 뷰의 경계를 넘 어 확장될 수 있다. 시스템은 신체의 적어도 일부(손, 머리, 다리, 발, 전체 하체 등을 포함하되 이에 국한되지 않음)가 카메라 뷰의 경계를 넘어 연장되는 경우 이를 감지한다. 이를 결정하는 한 가지 방법에는 신체의 형상 또는 (인체 분할에 의해 결정되는) 실루엣이 카메라 이미지의 경계와 접촉하고 있는지를 결정하는 것이 포함된 다. 일 실시예에서, 사용자, 전체 사용자 또는 개별 신체 부위/부분의 이미지가 메모리에 저장된다. 일부 실시예에 서, 사용자의 이미지는 카메라로 신체의 적어도 일부를 캡처하기 위해 (예컨대, 사용자가 카메라 앞에 \"T 포 즈\"로 서서 회전해야 하는 경우) 신체 이미지 캡처 프로세스의 일부로서 캡처된다. 다른 실시예들에서, 사용자 신체의 이미지는 사용자가 시스템을 사용하는 과정 중에 독점적인 캡처 프로세스를 필요로 하지 않고 원활하게 캡처된다.캡처된 이미지에 기초하여, 시스템은 표현의 적어도 일부가 필요에 따라 시스템에 의해 활용될 수 있도록 신체 의 표현을 생성하고 저장한다. 도 14 및 도 15에 도시된 바와 같이, 시스템은 단계에서 신체 컷오프의 발생 및 카메라 뷰에서 어 떤 신체 부위가 누락되었는지 감지하거나 결정한다. 그런 다음, 단계에서 시스템은 전술한 바와 같이 저 장된 신체 표현으로부터 누락된 신체 부위의 표현을 추출한다. 단계에서, 누락된 신체 부위 의 추출된 표현이 사용자 표현에 추가되거나 증강되어, 사용자 표현이 증강된 신체 부위와 함께 전체로서 도시되고 디스플레이 유닛에 디스플레이된다. 상술한 시스템이 어떻게 활용될 수 있는지에 대한 일 실시예는 다음과 같다: 일 실시예에서, 사용자/액터는 휴 대 전화의 셀카 카메라를 사용하여 시스템을 활용할 수 있다. 특히 가로 또는 수평 방향으로, 사용자/액터는 자 신의 전신을 보기 위해 휴대폰에서 멀리 떨어져 서 있어야 하므로 상대적으로 작은 휴대폰 화면에서 자신의 모 습을 충분히 볼 수 없다고 느낄 수 있다. 대신, 이러한 경우, 사용자가 일시적으로 휴대폰에서 충분히 뒤로 물 러나 시스템이 사용자의 전신을 캡처하고 학습할 수 있게 할 수 있다. 그런 다음, 사용자가 휴대폰에 더 가까이 다가가 (심지어 앉거나 눕게 선택할 수 있어) 카메라가 상체는 볼 수 있지만 완전한 하체는 볼 수 없다. 미리 저장된 이미지와 사용자/액터의 전신 표현을 활용하여, 액터 레이어 및/또는 가상 세계에 대한 사용자/액터의 전신 표현을 시스템이 생성할 수 있다. 그러면, 화면에 나타낸 이미지가 사용자의 전신 표현을 보여줄 수 있다. 마찬가지로, 가상 세계의 표현을 보는 임의의 다른 사용자들이 그 사용자의 전신 표현을 보게 된다. 위에서 설명되고 아래에 반복되는 바와 같이, 본 개시는 다음의 예시적인 구현들을 포함하되, 이에 국한되지 않 는다. 제1항. 3차원 장면에서 사용자의 가상 표현을 제어하기 위한 컴퓨터로 구현된 방법으로서, 이미징 유닛을 사용 하여 사용자를 적어도 부분적으로 캡처한 사용자의 2차원 비디오 스트림 데이터를 수신하는 단계; 적어도 하나 의 가상 객체와 3차원 장면 내의 해당 위치와 연관된 데이터를 포함하는 3차원 장면을 수신하는 단계; 2차원 비 디오 스트림 데이터에서 사용자의 사용자 표현을 분리하는 단계; 2차원 비디오 스트림에서 이미징 유닛에 대한 사용자의 상대 위치를 기반으로 식별되는 사용자의 위치 정보를 식별하는 단계; 위치 정보를 사용하여, 3차원 장면을 구성하는 복셀과 연관된 데이터를 수정하여 사용자 표현을 3차원 장면에 추가하는 단계; 및 3차원 장면 과 추가된 사용자 표현을 디스플레이 유닛에 디스플레이하는 단계를 포함하고, 디스플레이 유닛에 디스플레이된 추가된 사용자 표현은 사용자와 위치 정보 중 적어도 하나의 변경을 감지한 것을 기반으로 제어되는 방법. 제2항. 제1항에 있어서, 사용자의 위치 정보를 식별하는 단계는 2차원 비디오 스트림에서 깊이 정보를 추출하는 단계를 포함하는 방법. 제3항. 제2항에 있어서, 깊이 정보를 추출하는 단계는 2차원 비디오 스트림에서 깊이 단서를 인식하는 것을 포 함하는 방법. 제4항. 제1항에 있어서, 사용자 표현은 사용자의 적어도 일부의 이미지를 포함하는 방법. 제5항. 제1항에 있어서, 사용자 표현에서 누락된 신체 부위를 감지하는 단계; 데이터 저장 유닛에서 누락된 신 체 부위의 표현을 추출하는 단계; 및 누락된 신체 부위의 추출된 표현을 사용자 표현에 추가하는 단계를 더 포 함하는 방법. 제6항. 제1항에 있어서, 2차원 비디오 스트림 데이터에서 사람을 감지하는 단계; 감지된 사람을 의도된 사용자 와 비교하는 단계; 감지된 사람이 의도된 사용자인지 판별하는 단계; 및 감지된 사람이 의도된 사용자라는 판별 에 응답하여 감지된 사람을 사용자로 식별하는 단계를 더 포함하는 방법. 제7항. 제1항에 있어서, 사용자가 수행한 제스처 또는 포즈를 감지하는 것에 대한 응답으로 추가된 사용자 표현 을 업데이트하는 단계를 더 포함하고, 추가된 사용자 표현의 업데이트에는 사전 정의된 동작을 수행하는 추가된 사용자 표현이 포함되는 방법. 제8항. 제1항에 있어서, 2차원 비디오 스트림으로부터 사용자의 포즈 정보를 식별하는 단계를 더 포함하는 방법. 제9항. 제8항에 있어서, 사용자, 위치 정보 및 포즈 정보 중 적어도 하나의 변경을 기반으로 3차원 장면에서 추 가된 사용자 표현을 업데이트하는 단계를 더 포함하는 방법. 제10항. 제9항에 있어서, 변경에는 사용자가 추가된 사용자 표현이 가상 객체와 인터랙션하도록 하는 것이 포함 되는 방법. 제11항. 제10항에 있어서, 사용자가 추가된 사용자 표현이 가상 객체와 인터랙션하도록 하는 것에 대한 응답으 로 가상 객체를 업데이트하는 단계를 더 포함하는 방법. 제12항. 제11항에 있어서, 가상 객체를 업데이트하는 단계는 가상 객체가 미리 정의된 동작을 수행하도록 하는 단계를 포함하는 방법. 제13항. 제1항에 있어서, 사용자가 추가된 사용자 표현이 제스처를 표시하도록 하는 것에 대한 응답으로 가상 객체를 업데이트하는 단계를 더 포함하는 방법. 제14항. 제13항에 있어서, 가상 객체를 업데이트하는 단계는 제스처를 감지하면 가상 객체가 사전 정의된 동작 을 수행하도록 하는 단계를 포함하는 방법. 제15항. 3차원 장면에서 사용자의 가상 표현을 제어하기 위한 컴퓨터 구현 방법으로서, 제1 이미징 유닛을 사용 하여 제1 사용자를 적어도 부분적으로 캡처한 제1 사용자의 첫 번째 2차원 비디오 스트림 데이터를 수신하는 단 계; 적어도 하나의 가상 객체와 3차원 장면 내의 해당 위치와 관련된 데이터가 포함된 3차원 장면을 수신하는 단계; 제1 사용자의 제1 사용자 표현을 첫 번째 2차원 비디오 스트림 데이터에서 분리하는 단계; 제1 이미징 유 닛에 대한 제1 사용자의 상대 위치를 기반으로 식별되는 제1 사용자의 제1 위치 정보를 첫 번째 2차원 비디오 스트림에서 식별하는 단계; 제2 이미징 유닛을 사용하여 제2 사용자를 적어도 부분적으로 캡처한 제2 사용자의 두 번째 2차원 비디오 스트림 데이터를 수신하는 단계; 제2 사용자의 제2 사용자 표현을 두 번째 2차원 비디오 스트림 데이터에서 분리하는 단계; 2차원 비디오 스트림에서 제2 이미징 유닛에 대한 제2 사용자의 상대 위치를 기반으로 식별되는 제2 사용자의 제2 위치 정보를 식별하는 단계; 3차원 장면을 구성하는 복셀과 연관된 데이터 를 수정하여 3차원 장면에 제1 및 제2 사용자 표현을 추가하는 단계; 및 3차원 장면, 추가된 제1 사용자 표현, 추가된 제2 사용자 표현을 제1 디스플레이 유닛과 제2 디스플레이 유닛에 디스플레이하는 단계를 포함하고, 제1 사용자 표현과 연관된 데이터는 제1 위치 정보를 사용하고 제2 사용자 표현과 연관된 데이터는 제2 위치 정보를 사용하며, 제1 디스플레이 유닛에 디스플레이된 제1 추가된 사용자 표현은 제1 사용자와 제1 위치 정보 중 적어 도 하나에 대한 변경을 감지한 것을 기반으로 제어되고, 제2 디스플레이 유닛에 디스플레이된 제2 추가된 사용 자 표현은 제2 사용자와 제2 위치 정보 중 적어도 하나에 대한 변경을 감지한 것을 기반으로 제어되는 방법. 제16항. 제15항에 있어서, 제2 사용자와 제2 위치 정보 중 적어도 하나의 변경에 따라 3차원 장면에서 제2 추가 된 사용자 표현을 업데이트하는 단계를 더 포함하는 방법. 제17항. 제16항에 있어서, 변경에는 제2 사용자가 추가된 제2 사용자 표현이 추가된 제1 사용자 표현과 인터랙 션하도록 하는 것이 포함되는 방법. 제18항. 제15항에 있어서, 제1 사용자 및 상기 제2 사용자의 위치 정보를 식별하는 단계는 첫 번째 2차원 비디 오 스트림 및 두 번째 2차원 비디오 스트림으로부터 각각 깊이 정보를 추출하는 단계를 포함하는 방법. 제19항. 제18항에 있어서, 깊이 정보를 추출하는 단계는 첫 번째 2차원 비디오 스트림 및 두 번째 2차원 비디오 스트림 중 하나 이상에서 깊이 단서를 인식하는 단계를 포함하는 방법. 제20항. 제15항에 있어서, 제1 사용자 표현은 사용자의 적어도 일부의 이미지를 포함하는 방법. 제21항. 제15항에 있어서, 제1 또는 제2 사용자 표현으로부터 누락된 신체 부위를 검출하는 단계; 데이터 저장 유닛으로부터 누락된 신체 부위의 표현을 추출하는 단계; 및 추출된 신체 부위의 표현을 제1 및 제2 사용자 표 현 중 하나 이상에 추가하는 단계를 더 포함하는 방법. 제22항. 제15항에 있어서, 첫 번째 2 차원 비디오 스트림 데이터에서 사람을 감지하는 단계; 감지된 사람을 의 도된 사용자와 비교하는 단계; 감지된 사람이 의도된 사용자인지 판단하는 단계; 및 감지된 사람이 의도된 사용 자라는 판단에 응답하여, 감지된 사람을 제1 사용자로 식별하는 단계를 더 포함하는 방법. 제23항. 제15항에 있어서, 제1 사용자에 의해 수행된 제스처 또는 포즈를 감지하는 것에 응답하여 추가된 제1 사용자 표현을 업데이트하는 단계를 더 포함하고, 추가된 제1 사용자 표현의 업데이트에는 추가된 제1 사용자 표현이 사전 정의된 동작을 수행하는 것이 포함되는 방법. 제24항. 제15항에 있어서, 제2 사용자에 의해 수행되는 제스처 또는 포즈를 감지하는 것에 응답하여 추가된 제2 사용자 표현을 업데이트하는 단계를 더 포함하고, 추가된 제2 사용자 표현의 업데이트에는 추가된 제2 사용자 표현이 사전 정의된 동작을 수행하는 것이 포함되는 방법.제25항. 제15항에 있어서, 첫 번째 2 차원 비디오 스트림으로부터 제1 사용자의 제1 포즈 정보와, 두 번째 2 차 원 비디오 스트림으로부터 제2 사용자의 제2 포즈 정보를 식별하는 단계를 더 포함하는 방법. 제26항. 제25항에 있어서, 제1 사용자, 제1 위치 정보 및 제1 포즈 정보 중 적어도 하나의 변경에 기초하여 3차 원 장면에서 추가된 제1 사용자 표현을 업데이트하는 단계 및 제2 사용자, 제2 위치 정보 및 제2 포즈 정보 중 적어도 하나의 변경에 기초하여 3차원 장면에서 추가된 제2 사용자 표현을 업데이트하는 단계를 더 포함하는 방 법. 제27항. 제26항에 있어서, 변경에는 제1 사용자 또는 제2 사용자가 추가된 제1 또는 제2 사용자 표현을 가상 객 체와 인터랙션하게 하는 것이 포함되는 방법. 제28항. 제27항에 있어서, 제1 사용자 및 제2 사용자가 추가된 제1 사용자 및 추가된 제2 사용자 표현이 가상 객체와 인터랙션하도록 하는 것에 대한 응답으로, 가상 객체를 업데이트하는 단계를 더 포함하는 방법. 제29항. 제28항에 있어서, 가상 객체를 업데이트하는 단계는 가상 객체가 사전 정의된 동작을 수행하도록 하는 단계를 포함하는 방법. 제30항. 제15항에 있어서, 제1 사용자 및 제2 사용자가 해당 추가된 사용자 표현이 제스처를 디스플레이하도록 하는 것에 응답하여 가상 객체를 업데이트하는 단계를 더 포함하는 방법. 제31항. 제30항에 있어서, 가상 객체를 업데이트하는 단계는 제스처를 감지하면 가상 객체가 사전 정의된 동작 을 수행하도록 하는 단계를 더 포함하는 방법. 제32항. 3차원 장면에서 사용자의 가상 표현을 제어하기 위한 시스템으로서, 사용자의 2차원 비디오 스트림 데 이터를 캡처하는 이미징 유닛; 컴퓨터 판독 가능 프로그램 명령어가 저장된 저장 유닛; 및 저장 유닛과 통신하 여, 컴퓨터 판독 가능 프로그램 명령어를 실행하여 시스템이 적어도: 이미징 유닛에서 사용자를 적어도 부분적 으로 캡처한 사용자의 2차원 비디오 스트림 데이터를 수신하고, 적어도 하나의 가상 객체와 3차원 장면 내의 해 당 위치와 관련된 데이터가 포함된 3차원 장면을 수신하며, 2차원 비디오 스트림 데이터에서 사용자의 적어도 일부의 이미지가 포함된 사용자의 사용자 표현을 분리하고, 이미징 유닛에 대한 사용자의 상대적 위치를 기반으 로 식별되고, 2차원 비디오 스트림에서 추출된 깊이 정보가 포함된 사용자의 위치 정보를 2차원 비디오 스트림 에서 식별하며, 2차원 비디오 스트림에서 사용자의 포즈 정보를 식별하고, 위치 정보 및 포즈 정보를 사용하여 3차원 장면을 구성하는 복셀과 관련된 데이터를 수정하여 3차원 장면에 사용자 표현을 추가하고, 3차원 장면과 추가된 사용자 표현을 디스플레이 유닛에 디스플레이하도록 구성된 프로세서를 포함하고, 디스플레이 유닛에 디 스플레이된 추가된 사용자 표현은 사용자, 위치 정보 및 포즈 정보 중 적어도 하나의 변경을 감지한 것을 기반 으로 제어되는 시스템. 제33항. 제32항에 있어서, 사용자의 위치 정보가 2차원 비디오 스트림으로부터 추출된 깊이 정보를 포함하는 시 스템. 제34항. 제34항에 있어서, 깊이 정보의 추출이 2차원 비디오 스트림에서 깊이 단서의 인식을 포함하는 시스템. 제35항. 제32항에 있어서, 사용자 표현이 사용자의 적어도 일부의 이미지를 포함하는 시스템. 제36항. 제32항에 있어서, 프로세서는 시스템이: 사용자 표현으로부터 누락된 신체 부위를 검출하고; 데이터 저 장 유닛으로부터 누락된 신체 부위의 표현을 추출하며; 누락된 신체 부위의 추출된 표현을 사용자 표현에 추가 하도록 하기 위한 컴퓨터 판독 가능한 프로그램 명령을 실행하도록 더 구성되는 시스템. 제37항. 제32항에 있어서, 프로세서는 시스템이: 2차원 비디오 스트림 데이터에서 사람을 감지하고; 감지된 사 람을 의도된 사용자와 비교하며; 감지된 사람이 의도된 사용자인지 판단하고; 감지된 사람이 의도된 사용자라는 판단에 응답하여, 감지된 사람을 사용자로 식별하도록 하기 위한 컴퓨터 판독 가능한 프로그램 명령을 실행하도 록 더 구성되는 시스템. 제38항. 제32항에 있어서, 프로세서가 사용자에 의해 수행되는 제스처 또는 포즈를 감지하는 것에 응답하여 시 스템이 상기 추가된 사용자 표현을 업데이트하도록 하기 위한 컴퓨터 판독 가능한 프로그램 명령을 실행하도록 더 구성되고, 추가된 사용자 표현의 업데이트에는 추가된 사용자 표현이 사전 정의된 동작을 수행하는 것이 포 함되는 시스템. 제39항. 제32항에 있어서, 프로세서는 시스템이: 2차원 비디오 스트림으로부터 사용자의 포즈 정보를 식별하도 록 하기 위한 컴퓨터 판독 가능한 프로그램 명령을 실행하도록 더 구성되는 시스템. 제40항. 제39항에 있어서, 프로세서는 시스템이: 사용자, 위치 정보 및 포즈 정보 중 적어도 하나의 변경에 기 초하여 3차원 장면에서 추가된 사용자 표현을 업데이트하도록 하기 위한 컴퓨터 판독 가능 프로그램 명령을 실 행하도록 더 구성되는 시스템. 제41항. 제40항에 있어서, 변경에는 사용자가 추가된 사용자 표현이 가상 객체와 인터랙션하도록 하는 것이 포 함되는 시스템. 제42항. 제41항에 있어서, 프로세서는 시스템이: 가상 객체와 인터랙션하도록 하는 사용자에 대한 응답으로 추 가 사용자 표현이 가상 객체를 업데이트하도록 하는 컴퓨터 판독 가능한 프로그램 명령을 실행하도록 더 구성되 는 시스템. 제43항. 제42항에 있어서, 가상 객체의 업데이트에는 가상 객체가 사전 정의된 동작을 수행하도록 하는 것이 포 함되는 시스템. 제44항. 제32항에 있어서, 프로세서는: 사용자가 추가 사용자 표현이 제스처를 디스플레이하도록 하는 것에 응 답하여 시스템이 가상 객체를 업데이트하도록 하는 컴퓨터 판독 가능한 프로그램 명령을 실행하도록 더 구성되 는 시스템. 제45항. 제44항에 있어서, 가상 객체의 업데이트에는 제스처를 감지할 때 가상 객체가 사전 정의된 동작을 수행 하도록 하는 것이 포함되는 시스템. 본 개시의 여러 변형이 특정 설명에서 예시를 통해 예시되었지만, 본 개시의 기술사상 및 범위 또는 본 개시의 발명 개념 내에서 추가적인 실시예가 개발될 수 있음이 명백하다. 당업자들은 본 개시의 교시를 유지하면서 장 치 및 방법의 수많은 수정, 적용, 변형 및 변경이 이루어질 수 있음을 쉽게 관찰할 수 있을 것이다. 따라서, 본 개시는 개시된 특정 구현에 한정되어서는 안 되며, 수정 및 기타 구현은 첨부된 청구 범위 내에 포함되도록 의 도되었음을 이해해야 한다. 또한, 전술한 명세서 및 관련 도면은 요소 및/또는 기능의 특정 예시 조합의 맥락에 서 예시 구현을 설명하지만, 첨부된 청구 범위를 벗어나지 않고 다른 요소 및/또는 기능의 다른 조합이 대체 구 현에 의해 제공될 수 있음을 인식해야 한다. 이와 관련하여, 예를 들어, 첨부된 청구 범위 중 일부에 명시된 바 와 같이 위에서 명시적으로 설명된 것과 다른 요소 및/또는 기능의 조합도 고려될 수 있다. 본 명세서에서 특정 용어가 사용되었지만, 이는 제한의 목적이 아닌 일반적이고 설명적인 의미로만 사용된다. 상기 개시된 특징 및 기능의 변형 및 기타 특징 및 기능 또는 그 대안이 다른 많은 다른 시스템 또는 애플리케이션에 결합될 수 있음 을 이해할 수 있을 것이다. 당업자에 의해 현재 예측할 수 없거나 예상하지 못한 다양한 대안, 수정, 변형 또는 개선이 후속적으로 이루어질 수 있으며, 이는 또한 다음의 청구범위에 포함되도록 의도되어 있다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12a 도면12b 도면13a 도면13b 도면14 도면15 도면16 도면17 도면18a 도면18b 도면19 도면20a 도면20b 도면20c 도면21 도면22 도면23 도면24 도면25 도면26 도면27 도면28 도면29"}
{"patent_id": "10-2024-7028161", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 명세서에 통합되어 본 명세서의 일부를 구성하는 첨부 도면들은 본 개시의 양상들을 예시하고, 명세서와 함 께 본 개시의 원리를 설명하는 역할을 한다. 도 1은 일부 실시예에 따른 시스템을 도시한다. 도 2 및 도 3은 본 개시의 다양한 실시예에 따른, 멀티레이어 장면을 생성하는 방법의 다양한 단계를 예시하는 흐름도이다. 도 4 내지 도 9는 본 개시의 시스템 및 방법에 따른 예시적인 구현을 도시한다. 도 10 및 도 11은 본 개시의 시스템 및 방법에 따른, 인터랙티브 객체와 인터랙션하는 복수의 액터들의 예시적 인 구현을 도시한다. 도 12a 및 도 12b는 본 개시의 시스템 및 방법에 따라, 인터랙티브 객체와 인터랙션하는 복수의 액터들의 예시적인 구현을 도시한다. 도 13a 내지 도 29는 본 개시의 시스템 및 방법에 따른 다양한 예시적인 구현을 도시한다. 도면의 일부 세부 사항은 단순화되었으며, 엄격한 구조적 정확성, 디테일 및 스케일을 유지하기보다는 이해를 용이하게 하기 위해 그려졌음에 유의해야 한다."}
