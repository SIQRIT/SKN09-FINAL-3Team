{"patent_id": "10-2017-0170642", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0070386", "출원번호": "10-2017-0170642", "발명의 명칭": "시각 정보와 촉각 정보를 함께 이용하여 객체를 파지하는 로봇 핸드 및 그 제어방법", "출원인": "한국로봇융합연구원", "발명자": "황희선"}}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 제 1 객체에 대한 영상을 획득하는 카메라;상기 획득된 영상을 이용하여 상기 제 1 객체의 위치 위치 좌표, 크기 및 중심 중 적어도 하나의 팩터(factor)를 결정하는 제어부;상기 결정된 팩터를 이용하여 상기 제 1 객체를 파지하는 핸드; 및상기 핸드가 상기 파지를 위해 제 1 객체와 접촉되는 경우, 상기 제 1 객체의 수직항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나의 촉각 팩터를 감지하는 촉각센서;를 포함하되,상기 핸드가 상기 제 1 객체의 파지를 실패하는 경우,상기 제어부는 상기 실패 이벤트를 기초로 상기 제 1 객체의 위치 위치 좌표, 크기 및 중심 중 적어도 하나의팩터를 다시 결정하고,상기 핸드는, 상기 다시 결정된 팩터 및 상기 촉각센서에서 감지한 적어도 하나의 촉각 팩터를 이용하여 상기제 1 객체를 다시 파지하는 것을 특징으로 하는 로봇 핸드."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 제어부는,상기 핸드가 상기 제 1 객체의 파지를 재실패하는 경우,상기 재실패 이벤트를 기초로 상기 제 1 객체의 위치 위치 좌표, 크기 및 중심 중 적어도 하나를 다시결정하고, 상기 핸드가 상기 다시 결정된 팩터 및 상기 촉각센서에서 감지한 적어도 하나의 촉각 팩터를 이용하여 상기 제 1 객체를 다시 파지하도록 제어하는 동작을 반복하는 것을 특징으로 하는 로봇 핸드."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 제어부는,상기 제 1 객체의 수직항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나를 이용하여 상기 제 1 객체를 파지하기 위한 힘 및 토크 중 적어도 하나를 결정하고,상기 핸드는, 상기 다시 결정된 팩터 및 상기 결정된 힘 및 토크 중 적어도 하나를 이용하여 상기 제 1 객체를 다시 파지하는것을 특징으로 하는 로봇 핸드."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1항에 있어서,상기 핸드는 복수의 손가락을 포함하고,상기 제어부는,상기 복수의 손가락 중 적어도 하나의 제 1 손가락이 지면에 닿아있는 상태에서, 상기 핸드가 상기 제 1 객체를다시 파지하도록 제어하는 것을 특징으로 하는 로봇 핸드."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2019-0070386-3-제 1항에 있어서,상기 제 1 객체의 무게 중심이 변화되는 외란 이벤트가 발생되는 경우,상기 제어부는,제 1 객체의 수직항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나를 이용하여상기 제 1 객체를 파지하기 위한 힘 및 토크 중 적어도 하나의 변화를 결정하고,상기 핸드는, 상기 다시 결정된 팩터 및 상기 변화된 힘 및 토크 중 적어도 하나를 이용하여 상기 제 1 객체를 다시 파지하는것을 특징으로 하는 로봇 핸드."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1항에 있어서,상기 제 1 객체에 대한 제 1 정보를 저장하는 메모리 또는 상기 제 1 정보를 외부로부터 수신하는 통신부;를 더포함하고,상기 제어부는, 상기 핸드가 상기 제 1 정보를 추가적으로 이용하여 상기 제 1 객체를 다시 파지하도록 제어하는 것을 특징으로하는 로봇 핸드."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1항에 있어서복수의 모델에 대한 파지 정보를 저장하는 메모리 또는 상기 제 1 정보를 외부로부터 수신하는 통신부;를 더 포함하고,상기 제어부는,상기 제 1 객체의 영역 중 상기 파지되는 제 1 영역을 결정하고,상기 복수의 모델 중 상기 제 1 영역에 대응되는 제 1 모델을 결정하며,상기 제 1 모델에 대한 파지 정보를 기초로 상기 핸드가 상기 제 1 객체를 다시 파지하도록 제어하는 것을 특징으로 하는 로봇 핸드."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서,상기 핸드는 복수의 손가락을 포함하고,상기 복수의 손가락 각각은 상기 제 1 객체와 이격된 거리를 센싱하는 거리 센서를 포함하며,상기 제 1 객체를 파지하는 경우,상기 복수의 거리 센서를 통해 획득된 정보를 기초로, 상기 복수의 손가락 각각은 동일한 속도로 상기 제 1 객체에 근접 이동하는 것을 특징으로 하는 로봇 핸드."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "카메라가 적어도 하나의 제 1 객체에 대한 영상을 획득하는 제 1 단계;제어부가 상기 획득된 영상을 이용하여 상기 제 1 객체의 위치 위치 좌표, 크기 및 중심 중 적어도 하나의 팩터(factor)를 결정하는 제 2 단계;상기 결정된 팩터를 이용하여 핸드가 상기 제 1 객체를 파지하는 제 3 단계;상기 핸드가 상기 파지를 위해 제 1 객체와 접촉되는 경우, 촉각 센서를 통해 상기 제 1 객체의 수직항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나의 촉각 팩터를 감지하는 제 4 단공개특허 10-2019-0070386-4-계;상기 핸드가 상기 제 1 객체의 파지를 실패하는 제 5 단계;상기 제어부는 상기 실패 이벤트를 기초로 상기 제 1 객체의 위치 위치 좌표, 크기 및 중심 중 적어도 하나의팩터를 다시 결정하는 제 6 단계; 및상기 핸드가 상기 다시 결정된 팩터 및 상기 촉각센서에서 감지한 적어도 하나의 촉각 팩터를 이용하여 상기 제1 객체를 다시 파지하는 제 7 단계;를 포함하는 로봇 핸드의 제어방법."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9항에 있어서,상기 핸드가 상기 제 1 객체의 파지를 재실패하는 경우,상기 제 1 단계 내지 제 7 단계의 동작은 반복하여 수행되는 것을 특징으로 하는 로봇 핸드의 제어방법."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 9항에 있어서,상기 제 4 단계에서,상기 제어부는 상기 제 1 객체의 수직항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나를 이용하여 상기 제 1 객체를 파지하기 위한 힘 및 토크 중 적어도 하나를 결정하고,상기 제 7 단계에서,상기 핸드는 상기 다시 결정된 팩터 및 상기 결정된 힘 및 토크 중 적어도 하나를 이용하여 상기 제 1 객체를다시 파지하는 것을 특징으로 하는 로봇 핸드의 제어방법."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 9항에 있어서,상기 핸드는 복수의 손가락을 포함하고,상기 제 7 단계에서,상기 복수의 손가락 중 적어도 하나의 제 1 손가락이 지면에 닿아있는 상태에서, 상기 핸드가 상기 제 1 객체를다시 파지하는 것을 특징으로 하는 로봇 핸드의 제어방법."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 9항에 있어서,상기 제 4 단계에서,상기 제 1 객체의 무게 중심이 변화되는 외란 이벤트가 발생되는 경우, 상기 제어부는 제 1 객체의 수직항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나를 이용하여 상기 제 1 객체를 파지하기 위한 힘 및 토크 중 적어도 하나의 변화를 결정하고,상기 제 7 단계에서,상기 핸드는 상기 다시 결정된 팩터 및 상기 변화된 힘 및 토크 중 적어도 하나를 이용하여 상기 제 1 객체를다시 파지하는 것을 특징으로 하는 로봇 핸드의 제어방법."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 9항에 있어서,상기 제 7 단계에서,상기 핸드는 메모리에 저장된 상기 제 1 객체에 대한 제 1 정보 또는 통신부를 통해 외부로부터 수신된 상기 제공개특허 10-2019-0070386-5-1 정보를 이용하여 상기 제 1 객체를 다시 파지하는 것을 특징으로 하는 로봇 핸드의 제어방법."}
{"patent_id": "10-2017-0170642", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 9항에 있어서,상기 제 6 단계와 제 7 단계 사이에는,메모리가 복수의 모델에 대한 파지 정보를 저장하거나 통신부가 상기 제 1 정보를 외부로부터 수신하는 단계;상기 제어부가 상기 제 1 객체의 영역 중 상기 파지되는 제 1 영역을 결정하는 단계; 및상기 복수의 모델 중 상기 제 1 영역에 대응되는 제 1 모델을 결정하는 단계;를 더 포함하고,상기 제 7 단계에서,상기 제어부는 상기 제 1 모델에 대한 파지 정보를 기초로 상기 핸드가 상기 제 1 객체를 다시 파지하도록 제어하는 것을 특징으로 하는 로봇 핸드의 제어방법."}
{"patent_id": "10-2017-0170642", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 로봇 핸드 및 그 제어방법에 관한 것으로, 객체를 시각 및 촉각센서를 활용해서 정밀하게 핸들링 할 수 있는 로봇 핸드 및 그 제어방법에 관한 것이다. 본 발명의 일 양상인 로봇 핸드는, 적어도 하나의 제 1 객체 에 대한 영상을 획득하는 카메라; 상기 획득된 영상을 이용하여 상기 제 1 객체의 위치 위치 좌표, 크기 및 중심 (뒷면에 계속)"}
{"patent_id": "10-2017-0170642", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 로봇 핸드 및 그 제어방법에 관한 것으로, 객체를 시각 및 촉각센서를 활용해서 정밀하게 핸들링 할 수 있는 로봇 핸드 및 그 제어방법에 관한 것이다."}
{"patent_id": "10-2017-0170642", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "로봇 핸드(robot hand)는 복수의 손가락에 의해서 물체를 구속 또는 이동시키는 기계의 손을 의미하고, 로봇 암 (arm)이 넓은 작업 영역 내에서의 대체적인 위치 결정을 하는 데 반해 로봇 핸드는 한정된 영역 내에서의 미세 한 조작이나 물체의 파악을 하는데 이용되고 있다. 또한, 이러한 인공지능 및 로봇기술은 다양한 분야에 활용이 확산 중이며, 제조환경에서의 부품파지 및 조립의 핸들링 기술돌파(Breakthrough)를 위한 핵심기술로 활용될 수 있다. 단, 전술한 로봇 핸드를 기초로 실제 제조환경에서 수행되는 대부분의 작업은, 부품을 잡고 조립하는 핸들링 작 업이며 정형화된 환경에서의 단순반복 작업을 제외하고 대부분의 핸들링 작업은 사람의 수 공정에 의해 수행된 다는 문제점이 있다. 따라서 상기 문제점을 해소하고, 제조환경의 변화에 대응하여 생산효율을 높이기 위해 인공지능이 탑재된 파지/ 조립 기술 개발이 시급한 실정이다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 특허청 등록번호 제10-1323217호 (특허문헌 0002) 대한민국 특허청 등록번호 제 10-1048762호"}
{"patent_id": "10-2017-0170642", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 객체를 시각 및 촉각센서를 활용해서 정밀하게 핸들링 할 수 있는 기술을 제공하는 것을 목적으로 한 다. 또한, 본 발명은 물체에 대한 정보를 기반으로 Grasping을 시도하고, task의 수행을 실패하는 경우에는 알고리 즘에 따라 실패한 이벤트를 학습하며, 시각 정보 이외에 추가적으로 촉각 센서를 통한 정보를 함께 이용하여 Regrasping을 시도하는 핸들링 기술을 제공하는 것을 목적으로 한다.또한, 본 발명은 촉각센서를 추가적으로 활용하여 기존에 많이 활용되는 normal force 측정은 물론이고 shear force, 대상 물체의 전도성 등을 측정하여 대상물 분류 및 학습할 수 있는 핸들링 기술을 제공하는 것을 목적으 로 한다. 한편, 본 발명에서 이루고자 하는 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급하"}
{"patent_id": "10-2017-0170642", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "지 않은 또 다른 기술적 과제들은 아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에 게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2017-0170642", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기의 기술적 과제를 달성하기 위한 본 발명의 일 양상인 로봇 핸드는, 적어도 하나의 제 1 객체에 대한 영상 을 획득하는 카메라; 상기 획득된 영상을 이용하여 상기 제 1 객체의 위치 위치 좌표, 크기 및 중심 중 적어도 하나의 팩터(factor)를 결정하는 제어부; 상기 결정된 팩터를 이용하여 상기 제 1 객체를 파지하는 핸드; 및 상 기 핸드가 상기 파지를 위해 제 1 객체와 접촉되는 경우, 상기 제 1 객체의 수직항력(normal force), 전단력 (shear force) 및 전도성(conductive) 중 적어도 하나의 촉각 팩터를 감지하는 촉각센서;를 포함하되, 상기 핸 드가 상기 제 1 객체의 파지를 실패하는 경우, 상기 제어부는 상기 실패 이벤트를 기초로 상기 제 1 객체의 위 치 위치 좌표, 크기 및 중심 중 적어도 하나의 팩터를 다시 결정하고, 상기 핸드는, 상기 다시 결정된 팩터 및 상기 촉각센서에서 감지한 적어도 하나의 촉각 팩터를 이용하여 상기 제 1 객체를 다시 파지할 수 있다. 또한, 상기 제어부는, 상기 핸드가 상기 제 1 객체의 파지를 재실패하는 경우, 상기 재실패 이벤트를 기초로 상 기 제 1 객체의 위치 위치 좌표, 크기 및 중심 중 적어도 하나를 다시 결정하고, 상기 핸드가 상기 다시 결정된 팩터 및 상기 촉각센서에서 감지한 적어도 하나의 촉각 팩터를 이용하여 상기 제 1 객체를 다시 파지하도록 제 어하는 동작을 반복할 수 있다. 또한, 상기 제어부는, 제 1 객체의 수직항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나를 이용하여 상기 제 1 객체를 파지하기 위한 힘 및 토크 중 적어도 하나를 결정하고, 상기 핸드는, 상기 다시 결정된 팩터 및 상기 결정된 힘 및 토크 중 적어도 하나를 이용하여 상기 제 1 객체를 다시 파지할 수 있다. 또한, 상기 핸드는 복수의 손가락을 포함하고, 상기 제어부는, 상기 복수의 손가락 중 적어도 하나의 제 1 손가 락이 지면에 닿아있는 상태에서, 상기 핸드가 상기 제 1 객체를 다시 파지하도록 제어할 수 있다. 또한, 상기 제 1 객체의 무게 중심이 변화되는 외란 이벤트가 발생되는 경우, 상기 제어부는, 제 1 객체의 수직 항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나를 이용하여 상기 제 1 객체 를 파지하기 위한 힘 및 토크 중 적어도 하나의 변화를 결정하고, 상기 핸드는, 상기 다시 결정된 팩터 및 상기 변화된 힘 및 토크 중 적어도 하나를 이용하여 상기 제 1 객체를 다시 파지할 수 있다. 또한, 상기 제 1 객체에 대한 제 1 정보를 저장하는 메모리 또는 상기 제 1 정보를 외부로부터 수신하는 통신부;를 더 포함하고, 상기 제어부는, 상기 핸드가 상기 제 1 정보를 추가적으로 이용하여 상기 제 1 객체를 다시 파지하도록 제어할 수 있다. 또한, 복수의 모델에 대한 파지 정보를 저장하는 메모리 또는 상기 제 1 정보를 외부로부터 수신하는 통신부;를 더 포함하고, 상기 제어부는, 상기 제 1 객체의 영역 중 상기 파지되는 제 1 영역을 결정하고, 상기 복수의 모 델 중 상기 제 1 영역에 대응되는 제 1 모델을 결정하며, 상기 제 1 모델에 대한 파지 정보를 기초로 상기 핸드 가 상기 제 1 객체를 다시 파지하도록 제어할 수 있다. 또한, 상기 핸드는 복수의 손가락을 포함하고, 상기 복수의 손가락 각각은 상기 제 1 객체와 이격된 거리를 센 싱하는 거리 센서를 포함하며, 상기 제 1 객체를 파지하는 경우, 상기 복수의 거리 센서를 통해 획득된 정보를 기초로, 상기 복수의 손가락 각각은 동일한 속도로 상기 제 1 객체에 근접 이동할 수 있다. 한편, 상기의 기술적 과제를 달성하기 위한 본 발명의 다른 양상인 로봇 핸드의 제어방법은, 카메라가 적어도 하나의 제 1 객체에 대한 영상을 획득하는 제 1 단계; 제어부가 상기 획득된 영상을 이용하여 상기 제 1 객체의 위치 위치 좌표, 크기 및 중심 중 적어도 하나의 팩터(factor)를 결정하는 제 2 단계; 상기 결정된 팩터를 이용 하여 핸드가 상기 제 1 객체를 파지하는 제 3 단계; 상기 핸드가 상기 파지를 위해 제 1 객체와 접촉되는 경우, 촉각 센서를 통해 상기 제 1 객체의 수직항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나의 촉각 팩터를 감지하는 제 4 단계; 상기 핸드가 상기 제 1 객체의 파지를 실패하는 제 5 단계; 상 기 제어부는 상기 실패 이벤트를 기초로 상기 제 1 객체의 위치 위치 좌표, 크기 및 중심 중 적어도 하나의 팩터를 다시 결정하는 제 6 단계; 및 상기 핸드가 상기 다시 결정된 팩터 및 상기 촉각센서에서 감지한 적어도 하 나의 촉각 팩터를 이용하여 상기 제 1 객체를 다시 파지하는 제 7 단계;를 포함할 수 있다. 또한, 상기 핸드가 상기 제 1 객체의 파지를 재실패하는 경우, 상기 제 1 단계 내지 제 7 단계의 동작은 반복하 여 수행될 수 있다. 또한, 상기 제 4 단계에서, 상기 제어부는 상기 제 1 객체의 수직항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나를 이용하여 상기 제 1 객체를 파지하기 위한 힘 및 토크 중 적어도 하나를 결정하고, 상기 제 7 단계에서, 상기 핸드는 상기 다시 결정된 팩터 및 상기 결정된 힘 및 토크 중 적어도 하나 를 이용하여 상기 제 1 객체를 다시 파지할 수 있다. 또한, 상기 핸드는 복수의 손가락을 포함하고, 상기 제 7 단계에서, 상기 복수의 손가락 중 적어도 하나의 제 1 손가락이 지면에 닿아있는 상태에서, 상기 핸드가 상기 제 1 객체를 다시 파지할 수 있다. 또한, 상기 제 4 단계에서, 상기 제 1 객체의 무게 중심이 변화되는 외란 이벤트가 발생되는 경우, 상기 제어부 는 제 1 객체의 수직항력(normal force), 전단력(shear force) 및 전도성(conductive) 중 적어도 하나를 이용 하여 상기 제 1 객체를 파지하기 위한 힘 및 토크 중 적어도 하나의 변화를 결정하고, 상기 제 7 단계에서, 상 기 핸드는 상기 다시 결정된 팩터 및 상기 변화된 힘 및 토크 중 적어도 하나를 이용하여 상기 제 1 객체를 다 시 파지할 수 있다. 또한, 상기 제 7 단계에서, 상기 핸드는 메모리에 저장된 상기 제 1 객체에 대한 제 1 정보 또는 통신부를 통해 외부로부터 수신된 상기 제 1 정보를 이용하여 상기 제 1 객체를 다시 파지할 수 있다. 또한, 상기 제 6 단계와 제 7 단계 사이에는, 메모리가 복수의 모델에 대한 파지 정보를 저장하거나 통신부가 상기 제 1 정보를 외부로부터 수신하는 단계; 상기 제어부가 상기 제 1 객체의 영역 중 상기 파지되는 제 1 영 역을 결정하는 단계; 및 상기 복수의 모델 중 상기 제 1 영역에 대응되는 제 1 모델을 결정하는 단계;를 더 포 함하고, 상기 제 7 단계에서, 상기 제어부는 상기 제 1 모델에 대한 파지 정보를 기초로 상기 핸드가 상기 제 1 객체를 다시 파지하도록 제어할 수 있다."}
{"patent_id": "10-2017-0170642", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 로봇 핸드 및 그 제어방법에 관한 것으로, 본 발명은 객체에 대한 세밀한 모델링이나 프로그래밍 작 업 없이 시각 및 촉각센서를 활용해서 핸들링 할 수 있는 로봇 핸드 및 그 제어방법을 사용자에게 제공할 수 있 다. 본 발명에 따르면, 기계학습을 통한 파지 및 조작 방법 개발을 위해 기존 제안된 model-free 방법과 model- based 방법을 함께 개발해서 오래 걸리는 학습 시간 등의 약점을 극복하고 모델 불확실성이 있는 상황에서도 활 용할 수 있도록 상호 보완적인 개발이 될 수 있다. 또한, 본 발명에 따르면, 기존 많이 활용되는 normal force 측정은 물론이고 shear force까지 감지할 수 있고 대상 물체의 전도성 등을 측정하여 대상물 분류를 할 수 있는 촉각 센서를 개발하고 인간처럼 이러한 촉각 정보 를 활용 가능하다. 또한, 본 발명은 시각 및 촉각정보를 이용한 부품의 실시간 위치/자세/상태 인식 기술, 부품의 안정파지를 위한 최적 파지형태 추론지능 기술, 인식정보와 경험에 기반한 지능적 파지기술(단일 그리퍼/손 이용, 30종 이상 물 체), 시각 및 촉각정보를 이용한 부품의 위치/방향 조작(In-Hand) 기술, 경험기반 다양한 부품의 조립 전략 학 습 기술을 통해, 단순반복 작업을 제외하고 대부분의 핸들링 작업은 사람의 수 공정에 의해 수행되고 있는 현재 의 문제점을 해소할 수 있다. 한편, 본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효"}
{"patent_id": "10-2017-0170642", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "과들은 아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2017-0170642", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 구체적인 설명에 앞서, 본 발명에 적용되는 로봇 핸드 시스템의 구성을 도면을 참조하여 설명한다. 도 1는 본 발명이 제안하는 로봇 핸드 시스템의 구성을 설명하는 블록 구성도이다. 도 1을 참조하면, 로봇 핸드 시스템은 무선 통신부, A/V(Audio/Video) 입력부, 사용자 입력부 , 센싱부, 출력부, 메모리, 인터페이스부, 제어부, 전원 공급부 및 로봇 핸드 등을 포함할 수 있다. 단, 도 1에 도시된 구성요소들이 필수적인 것은 아니어서, 그보다 많은 구성요소들을 갖거나 그보다 적은 구성 요소들을 갖는 로봇 핸드 시스템이 구현될 수도 있다. 이하, 상기 구성요소들에 대해 차례로 살펴본다. 무선 통신부는 로봇 핸드 시스템과 무선 통신 시스템 사이 또는 기기와 기기가 위치한 네트워크 사이의 무 선 통신을 가능하게 하는 하나 이상의 모듈을 포함할 수 있다. 예를 들어, 무선 통신부는 이동통신 모듈, 무선 인터넷 모듈, 근거리 통신 모듈 및 위치정 보 모듈 등을 포함할 수 있다. 이동통신 모듈은, 이동 통신망 상에서 기지국, 외부의 기기, 서버 중 적어도 하나와 무선 신호를 송수신한 다. 문자/멀티미디어 메시지 송수신에 따른 다양한 형태의 데이터를 포함할 수 있다. 무선 인터넷 모듈은 무선 인터넷 접속을 위한 모듈을 말하는 것으로, 로봇 핸드 시스템에 내장되거나 외장 될 수 있다. 무선 인터넷 기술로는 WLAN(Wireless LAN)(Wi-Fi), Wibro(Wireless broadband), Wimax(World Interoperability for Microwave Access), HSDPA(High Speed Downlink Packet Access) 등이 이용될 수 있다. 근거리 통신 모듈은 근거리 통신을 위한 모듈을 말한다. 근거리 통신(short range communication) 기술로 블루투스(Bluetooth), RFID(Radio Frequency Identification), 적외선 통신(IrDA, infrared Data Association), UWB(Ultra Wideband), ZigBee, 와이파이(Wireless Fidelity, Wi-Fi) 등이 이용될 수 있다. 위치정보 모듈은 로봇 핸드 시스템의 위치를 획득하기 위한 모듈로서, 그의 대표적인 예로는 GPS(Global Position System) 모듈이 있다. 도 1을 참조하면, A/V(Audio/Video) 입력부는 오디오 신호 또는 비디오 신호 입력을 위한 것으로, 이에는 카메라와 마이크 등이 포함될 수 있다. 카메라는 촬영 모드에서 이미지 센서에 의해 얻어지는 정지영상 또는 동영상 등의 화상 프레임을 처리한다. 처리된 화상 프레임은 디스플레이부에 표시될 수 있 다. 카메라에서 처리된 화상 프레임은 메모리에 저장되거나 무선 통신부를 통하여 외부로 전송될 수 있다. 카메라는 사용 환경에 따라 2개 이상이 구비될 수도 있다. 마이크는 녹음모드, 음성인식 모드 등에서 마이크로폰(Microphone)에 의해 외부의 음향 신호를 입력받아 전기적인 음성 데이터로 처리한다. 처리된 음성 데이터는 이동통신 모듈을 통하여 이동통신 기지국으로 송 신 가능한 형태로 변환되어 출력될 수 있다. 마이크에는 외부의 음향 신호를 입력받는 과정에서 발생되는 잡음(noise)을 제거하기 위한 다양한 잡음 제거 알고리즘이 구현될 수 있다. 사용자 입력부는 사용자가 로봇 핸드 시스템의 동작 제어를 위한 입력 데이터를 발생시킨다. 사용자 입력 부는 키 패드(key pad) 돔 스위치 (dome switch), 터치 패드(정압/정전), 조그 휠, 조그 스위치 등으로 구성될 수 있다. 센싱부는 로봇 핸드 시스템의 개폐 상태, 로봇 핸드 시스템의 위치, 사용자 접촉 유무, 로봇 핸드 시스템 의 방위, 로봇 핸드 시스템의 가속/감속 등과 같이 로봇 핸드 시스템의 현 상태를 감지하여 로봇 핸드 시스템의 동작을 제어하기 위한 센싱 신호를 발생시킨다. 센싱부는 전원 공급부의 전원 공급 여부, 인터페이스부의 외부 기기 결합 여부 등을 센싱할 수 도 있다. 한편, 상기 센싱부는 근접 센서(미도시)를 포함할 수 있다. 출력부는 시각, 청각 또는 촉각 등과 관련된 출력을 발생시키기 위한 것으로, 이에는 디스플레이부, 음향 출력 모듈, 알람부, 햅틱 모듈 및 프로젝터 모듈 등이 포함될 수 있다. 디스플레이부는 로봇 핸드 시스템에서 처리되는 정보를 표시(출력)한다. 디스플레이부는 액정 디스플레이(liquid crystal display, LCD), 박막 트랜지스터 액정 디스플레이(thin film transistor-liquid crystal display, TFT LCD), 유기 발광 다이오드(organic light-emitting diode, OLED), 플렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display) 중에서 적어도 하나를 포함할 수 있다. 이들 중 일부 디스플레이는 그를 통해 외부를 볼 수 있도록 투명형 또는 광투과형으로 구성될 수 있다. 이는 투 명 디스플레이라 호칭될 수 있는데, 상기 투명 디스플레이의 대표적인 예로는 TOLED(Transparant OLED) 등이 있 다. 디스플레이부의 후방 구조 또한 광 투과형 구조로 구성될 수 있다. 로봇 핸드 시스템의 구현 형태에 따라 디스플레이부가 2개 이상 존재할 수 있다. 예를 들어, 로봇 핸드 시 스템에는 복수의 디스플레이부들이 하나의 면에 이격되거나 일체로 배치될 수 있고, 또한 서로 다른 면에 각각 배치될 수도 있다. 디스플레이부와 터치 동작을 감지하는 센서(이하, '터치 센서'라 함)가 상호 레이어 구조를 이루는 경우 (이하, '터치 스크린'이라 함)에, 디스플레이부는 출력 장치 이외에 입력 장치로도 사용될 수 있다. 터치 센서는, 예를 들어, 터치 필름, 터치 시트, 터치 패드 등의 형태를 가질 수 있다. 터치 센서는 디스플레이부의 특정 부위에 가해진 압력 또는 디스플레이부의 특정 부위에 발생하는 정 전 용량 등의 변화를 전기적인 입력신호로 변환하도록 구성될 수 있다. 터치 센서는 터치 되는 위치 및 면적뿐 만 아니라, 터치 시의 압력까지도 검출할 수 있도록 구성될 수 있다. 터치 센서에 대한 터치 입력이 있는 경우, 그에 대응하는 신호(들)는 터치 제어기로 보내진다. 터치 제어기는 그 신호(들)를 처리한 다음 대응하는 데이터를 제어부로 전송한다. 이로써, 제어부는 디스플레이부 의 어느 영역이 터치 되었는지 여부 등을 알 수 있게 된다. 상기 근접 센서(미도시)는 상기 터치스크린에 의해 감싸지는 로봇 핸드 시스템의 내부 영역 또는 상기 터치 스 크린의 근처에 배치될 수 있다. 상기 근접 센서는 소정의 검출면에 접근하는 물체, 혹은 근방에 존재하는 물체 의 유무를 전자계의 힘 또는 적외선을 이용하여 기계적 접촉이 없이 검출하는 센서를 말한다. 근접 센서는 접촉 식 센서보다는 그 수명이 길며 그 활용도 또한 높다. 상기 근접 센서의 예로는 투과형 광전 센서, 직접 반사형 광전 센서, 미러 반사형 광전 센서, 고주파 발진형 근 접 센서, 정전용량형 근접 센서, 자기형 근접 센서, 적외선 근접 센서 등이 있다. 상기 터치스크린이 정전식인 경우에는 상기 포인터의 근접에 따른 전계의 변화로 상기 포인터의 근접을 검출하도록 구성된다. 이 경우 상기 터치 스크린(터치 센서)은 근접 센서로 분류될 수도 있다. 이하에서는 설명의 편의를 위해, 상기 터치스크린 상에 포인터가 접촉되지 않으면서 근접되어 상기 포인터가 상 기 터치스크린 상에 위치함이 인식되도록 하는 행위를 \"근접 터치(proximity touch)\"라고 칭하고, 상기 터치스 크린 상에 포인터가 실제로 접촉되는 행위를 \"접촉 터치(contact touch)\"라고 칭한다. 상기 터치스크린 상에서 포인터로 근접 터치가 되는 위치라 함은, 상기 포인터가 근접 터치될 때 상기 포인터가 상기 터치스크린에 대해 수직으로 대응되는 위치를 의미한다. 상기 근접센서는, 근접 터치와, 근접 터치 패턴(예를 들어, 근접 터치 거리, 근접 터치 방향, 근접 터치 속도, 근접 터치 시간, 근접 터치 위치, 근접 터치 이동 상태 등)을 감지한다. 상기 감지된 근접 터치 동작 및 근접 터치 패턴에 상응하는 정보는 터치 스크린상에 출력될 수 있다. 또한, 본 발명에 따른 로봇 핸드 시스템은 자이로 센서를 포함할 수 있다. 자이로 센서는 지구의 회전과 관계없이 높은 정확도로 항상 처음에 설정한 일정 방향을 유지하는 성질을 이용하여 물체의 방위 변화를 측정하는 센서이고, 자이로스코프에는 기계적인 방식과 광을 이용하는 광학식이 있다. 또한, 본 발명에 따른 로봇 핸드 시스템는 가속도 센서를 포함할 수 있다. 가속도센서는 출력신호를 처리하여 물체의 가속도, 진동, 충격 등의 동적 힘을 측정하는 것이다. 가속도 센서는 검출 방식으로 크게 분류하면 관성식, 자이로식, 실리콘반도체식이 있는데, 진도계나 경사 계 등도 가속도센서의 한 종류로 볼 수 있다. 또한, 본 발명에 따른 로봇 핸드 시스템는 압력 센서를 포함할 수 있다. 압력 센서는 액체 또는 기체의 압력을 검출하고, 계측이나 제어에 사용하기 쉬운 전기 신호로 변환하여 전 송하는 장치 및 소자를 말한다. 측정의 원리는 변위나 변형을 비롯하여 분자 밀도의 열전도율을 이용하는 등 매우 많은 종류가 쓰이고 있는데, 최근에는 실리콘을 재료로 한 변형 게이지형의 압력 센서가 개발되어 정밀한 압력 계측에 사용되고 있으며. 집 적 회로를 동일한 기판 위에 만들어 넣어 신호 처리까지 하는 집적화 압력 센서도 개발되어 있다. 또한, 본 발명에 따른 로봇 핸드 시스템은 촉각 센서를 포함할 수도 있다. 촉각 센서(144, Tactile Sensor)는 로봇에서 인공적으로 인간의 촉각을 실현하려는 압력 센서로스 크게 접촉 센 서, 압력 센서, 미끄러짐 센서, 온도 센서 등으로 구분되는데, 인간의 고도화된 촉각 시스템을 구현하기 위해 필요한 기술이다. 촉각센서 어레이(tactile sensor array)는 접촉각 센서나 압각센서를 평면 형상으로 수 개~수 십개 나열하 여 2차원적 정보를 얻기 위한 센서로서, 형상 또는 운동의 검출에도 이용할 수 있다. 이들 센서의 다수는 도전성 고무 또는 압전성 고분자, 감압고분자의 양면의 전극 중 어느 한쪽을 분할하여 배열 형 센서를 구성하고 있는데, 2차원적 압력분포는 상대하는 전극간의 저항변화 또는 전압출력으로부터 검출된다. 특히, 본 발명에 따른 촉각 센서는 로봇 핸드가 접촉하는 객체의 직항력(normal force), 전단력 (shear force) 및 전도성(conductive) 중 적어도 하나를 감지할 수 있다. 한편, 음향 출력 모듈은 녹음 모드, 음성인식 모드, 방송수신 모드 등에서 무선 통신부로부터 수신되 거나 메모리에 저장된 오디오 데이터를 출력할 수 있다. 음향 출력 모듈은 로봇 핸드 시스템에서 수 행되는 기능과 관련된 음향 신호를 출력하기도 한다. 이러한 음향 출력 모듈에는 리시버(Receiver), 스피 커(speaker), 버저(Buzzer) 등이 포함될 수 있다. 알람부는 로봇 핸드 시스템의 이벤트 발생을 알리기 위한 신호를 출력한다. 알람부는 비디오 신호나 오디오 신호 이외에 다른 형태, 예를 들어 진동으로 이벤트 발생을 알리기 위한 신호를 출력할 수도 있다. 상기 비디오 신호나 오디오 신호는 디스플레이부나 음성 출력 모듈을 통해서도 출력될 수 있어서, 그 들(151,152)은 알람부의 일부로 분류될 수도 있다. 햅틱 모듈(haptic module)은 사용자가 느낄 수 있는 다양한 촉각 효과를 발생시킨다. 햅틱 모듈이 발 생시키는 촉각 효과의 대표적인 예로는 진동이 있다. 햅택 모듈이 발생하는 진동의 세기와 패턴 등은 제어 가능하다. 예를 들어, 서로 다른 진동을 합성하여 출력하거나 순차적으로 출력할 수도 있다. 햅틱 모듈은, 진동 외에도, 접촉 피부면에 대해 수직 운동하는 핀 배열, 분사구나 흡입구를 통한 공기의 분사력이나 흡입력, 피부 표면에 대한 스침, 전극(eletrode)의 접촉, 정전기력 등의 자극에 의한 효과와, 흡열 이나 발열 가능한 소자를 이용한 냉온감 재현에 의한 효과 등 다양한 촉각 효과를 발생시킬 수 있다. 햅틱 모듈은 직접적인 접촉을 통해 촉각 효과의 전달할 수 있을 뿐만 아니라, 사용자가 손가락이나 팔 등 의 근 감각을 통해 촉각 효과를 느낄 수 있도록 구현할 수도 있다. 햅틱 모듈은 로봇 핸드 시스템의 구성 태양에 따라 2개 이상이 구비될 수 있다. 프로젝터 모듈은, 로봇 핸드 시스템을 이용하여 이미지 프로젝트(project) 기능을 수행하기 위한 구성요소 로서, 제어부의 제어 신호에 따라 디스플레이부상에 디스플레이되는 영상과 동일하거나 적어도 일부 가 다른 영상을 외부 스크린 또는 벽에 디스플레이할 수 있다. 구체적으로, 프로젝터 모듈은, 영상을 외부로 출력하기 위한 빛(일 예로서, 레이저 광)을 발생시키는 광원 (미도시), 광원에 의해 발생한 빛을 이용하여 외부로 출력할 영상을 생성하기 위한 영상 생성 수단 (미도시), 및 영상을 일정 초점 거리에서 외부로 확대 출력하기 위한 렌즈(미도시)를 포함할 수 있다. 또한, 프로젝터 모 듈은, 렌즈 또는 모듈 전체를 기계적으로 움직여 영상 투사 방향을 조절할 수 있는 장치(미도시)를 포함할 수 있다. 프로젝터 모듈은 디스플레이 수단의 소자 종류에 따라 CRT(Cathode Ray Tube) 모듈, LCD(Liquid Crystal Display) 모듈 및 DLP(Digital Light Processing) 모듈 등으로 나뉠 수 있다. 특히, DLP 모듈은, 광원에서 발 생한 빛이 DMD(Digital Micromirror Device) 칩에 반사됨으로써 생성된 영상을 확대 투사하는 방식으로 프로젝 터 모듈의 소형화에 유리할 수 있다. 바람직하게, 프로젝터 모듈은, 로봇 핸드 시스템의 측면, 정면 또는 배면에 길이 방향으로 구비될 수 있다. 물론, 프로젝터 모듈은, 필요에 따라 로봇 핸드 시스템의 어느 위치에라도 구비될 수 있음은 당연하 다. 한편, 메모리부는 제어부의 처리 및 제어를 위한 프로그램이 저장될 수도 있고, 입/출력되는 데이터 들(예를 들어, 메시지, 오디오, 정지영상, 동영상 등)의 임시 저장을 위한 기능을 수행할 수도 있다. 상기 메모 리부에는 상기 데이터들 각각에 대한 사용 빈도도 함께 저장될 수 있다. 또한, 상기 메모리부에는 상 기 터치스크린 상의 터치 입력시 출력되는 다양한 패턴의 진동 및 음향에 관한 데이터를 저장할 수 있다. 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램 (Random Access Memory, RAM), SRAM(Static Random Access Memory), 롬(Read-Only Memory, ROM), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 로봇 핸드 시스템은 인터넷(internet)상에서 상기 메모리의 저장 기능을 수행하는 웹 스토리지(web storage)와 관련되어 동작할 수 도 있다. 인터페이스부는 로봇 핸드 시스템에 연결되는 모든 외부기기와의 통로 역할을 한다. 인터페이스부는 외부 기기로부터 데이터를 전송받거나, 전원을 공급받아 로봇 핸드 시스템 내부의 각 구성 요소에 전달하거나, 로봇 핸드 시스템 내부의 데이터가 외부 기기로 전송되도록 한다. 예를 들어, 유/무선 헤드셋 포트, 외부 충전 기 포트, 유/무선 데이터 포트, 메모리 카드(memory card) 포트, 식별 모듈이 구비된 장치를 연결하는 포트, 오 디오 I/O(Input/Output) 포트, 비디오 I/O(Input/Output) 포트, 이어폰 포트 등이 인터페이스부에 포함될 수 있다. 식별 모듈은 로봇 핸드 시스템의 사용 권한을 인증하기 위한 각종 정보를 저장한 칩으로서, 사용자 인증 모듈 (User Identify Module, UIM), 가입자 인증 모듈(Subscriber Identify Module, SIM), 범용 사용자 인증 모듈 (Universal Subscriber Identity Module, USIM) 등을 포함할 수 있다. 식별 모듈이 구비된 장치(이하 '식별 장 치')는, 스마트 카드(smart card) 형식으로 제작될 수 있다. 따라서 식별 장치는 포트를 통하여 로봇 핸드 시스 템과 연결될 수 있다. 상기 인터페이스부는 로봇 핸드 시스템이 외부 크래들(cradle)과 연결될 때 상기 크래들로부터의 전원이 상기 로봇 핸드 시스템에 공급되는 통로가 되거나, 사용자에 의해 상기 크래들에서 입력되는 각종 명령 신호가 상기 이동기기로 전달되는 통로가 될 수 있다. 상기 크래들로부터 입력되는 각종 명령 신호 또는 상기 전원은 상기 이동기기가 상기 크래들에 정확히 장착되었음을 인지하기 위한 신호로 동작될 수도 있다. 제어부(controller, 180)는 통상적으로 로봇 핸드 시스템의 전반적인 동작을 제어한다. 전원 공급부는 제어부의 제어에 의해 외부의 전원, 내부의 전원을 인가 받아 각 구성요소들의 동작에 필요한 전원을 공급한다. 여기에 설명되는 다양한 실시예는 예를 들어, 소프트웨어, 하드웨어 또는 이들의 조합된 것을 이용하여 컴퓨터 또는 이와 유사한 장치로 읽을 수 있는 기록매체 내에서 구현될 수 있다. 하드웨어적인 구현에 의하면, 여기에 설명되는 실시예는 ASICs (application specific integrated circuits), DSPs (digital signal processors), DSPDs (digital signal processing devices), PLDs (programmable logic devices), FPGAs (field programmable gate arrays, 프로세서(processors), 제어기(controllers), 마이크로 컨 트롤러(micro-controllers), 마이크로 프로세서(microprocessors), 기타 기능 수행을 위한 전기적인 유닛 중 적어도 하나를 이용하여 구현될 수 있다. 일부의 경우에 본 명세서에서 설명되는 실시예들이 제어부 자체 로 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시예들은 별도의 소프트웨어 모 듈들로 구현될 수 있다. 상기 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 작동을 수행할 수 있다. 적절한 프로그램 언어로 쓰여진 소프트웨어 어플리케이션으로 소프트웨어 코드가 구현될 수 있 다. 상기 소프트웨어 코드는 메모리에 저장되고, 제어부에 의해 실행될 수 있다. 또한, 본 발명에 따른 시스템은 로봇 핸드를 포함할 수 있다. 로봇 핸드는 크게 손바닥부 및 손가락부를 포함한다. 손가락부는 적어도 하나의 마디를 갖는 구조로 구성 가능하고, 거리 센서(미도시)를 통해, 객체에 대해 동 일한 속도로 이동하는 것도 가능하다. 본 발명에 따른 객체는 정보가 아예 존재하지 않는 상태의 객체, 외부로부터 전달받은 정보가 존재하는 상태의 객체, 실패 이벤트를 통해 정보가 축적된 정보가 존재하는 상태의 객체 등의 대상이 될 수 있다. 본 발명에 따른 객체에 대한 정보가 존재하는 상태는, 객체에 대한 완벽한 정보를 구축한 상태뿐만 아니라 실패 이벤트 및 러능을 통해 정보가 어느 정도 축적된 상태를 포함한다. 한변, 로봇 핸드는 복수로 시스템에 구비될 수 있고, 복수의 로봇 핸드를 통해 단순한 파지 작 업뿐만 아니라 특정 순서 및 결합 방향 등을 갖는 태스크(task)를 수행하는 것도 가능하다. 전술한 본 발명의 시스템 구성을 기초로 본 명세서에서 제안하고자 하는 구체적인 기술에 대해 도면을 참 조하여 설명한다.제 1 실시예 - unknown object를 시각 센서를 이용하여 핸들링하는 기술 종래의 로봇 핸드를 기초로 실제 제조환경에서 수행되는 대부분의 작업은, 부품을 잡고 조립하는 핸들링 작업이 며 정형화된 환경에서의 단순반복 작업을 제외하고 대부분의 핸들링 작업은 사람의 수 공정에 의해 수행된다는 문제점이 있었다. 따라서 본 발명은 상기 문제점을 해소하고, 제조환경의 변화에 대응하여 생산효율을 높이기 위해 인공지능이 탑 재된 파지/조립 기술을 제안하고자 한다. 즉, 본 발명은 전술한 시스템 구성을 기초로, 물체의 대략적인 위치, 모양 등을 비젼을 통해 파악하여 Grasping을 시도하고, task의 수행을 실패하는 경우에는 알고리즘에 따라 실패한 이벤트를 학습하며, Regrasping을 시도하는 핸들링 기술을 제공하고, 추가적으로 촉각센서를 활용하여 학습 알고리즘의 효율을 더 높이는 기술을 제공하고자 한다. 또한, 본 발명은, 종래의 기계학습을 통한 파지 및 조작 방법 개발을 위해 기존 제안된 model-free 방법과 model-based 방법을 함께 개발해서 오래 걸리는 학습 시간 등의 약점을 극복하고, 모델 불확실성이 있는 상황에 서도 활용할 수 있는 핸들링 기술을 제공하는 것을 목적으로 한다. 도 2는 본 발명이 제안하는 로봇 핸드의 구성을 설명하는 도면이다. 도 2를 참조하면, 객체를 파지하기 위한 로봇 핸드가 도시된다. 또한, 본 발명에 따른 시스템에는 복수의 카메라가 배치된다. 여기서 제어부는 객체에 대한 정보가 전혀 없는 상황에서 로봇 핸드를 제어하여 객체를 파지 하는 동작을 수행할 수 있다. 즉, 본 발명에서는 객체에 대한 획득한 정보 또는 학습된 정보가 없으므로, 복수의 카메라를 통한 비젼 시 스템을 기초로, 대략적인 물체의 위치, 모양을 파악하고, 파악된 정보를 기초로 로봇 핸드를 제어하여 객 체를 파지할 수 있다. 구체적으로 이 경우, 제어부는 획득된 영상을 이용하여 객체의 좌표, 크기 및 중심 중 적어도 하나를 결정하고, 파악된 정보를 기초로 로봇 핸드가 바로 객체를 파지하도록 제어할 수 있다. 이 경우, 제어부는 로봇 핸드가 정밀한 grasping 보다는 power grasping을 기초로 객체를 파지 하도록 할 수 있다. 로봇 핸드는 정확한 정보를 정보를 기초로 객체를 파지하는 것이 아니므로, 객체를 파지하지 못하 거나 파지한 이후에 객체를 해방하는 이벤트가 발생될 수 있다. 이때, 본 발명에서는 실패한 이벤트에 대한 기계 학습 알고리즘을 적용하고자 한다. 즉, 로봇 핸드가 객체의 파지를 실패하는 경우, 제어부는 실패 이벤트를 기초로 객체의 좌 표, 크기 및 중심 중 적어도 하나를 다시 결정하게 된다. 이후, 로봇 핸드는 다시 결정된 팩터를 이용하여 객체를 다시 파지하려는 시도를 한다. 이러한 기계 학습 알고리즘을 객체를 파지하지 못하는 경우에 반복적으로 수행될 수 있고, 결국, 구체적인 정보를 획득한 이후에 객체를 파지하는 방법보다 훨씬 더 적은 시도, 연산량, 시간을 이용하여 객체를 파지할 수 있게 된다. 도 3은 본 발명의 비젼, 제어 및 기계 학습에 따라 객체를 파지하는 구체적인 일례를 도시한 것이다. 도 3을 참조하면, 본 발명에 따른 로봇 핸드는 1개의 손바닥부와 3개의 손가락부를 포함하고, 각 손가락부는 복수의 마디로 구성된다. 도 3의 (a)를 참조하면, 본 발명에 따른 로봇 핸드는 손바닥부를 기준으로 복수의 손가락부를 펼쳐서 오므리는 동작을 통해 객체를 파지할 수 있다. 즉, 카메라를 통해 획득된 영상을 기초로, 제어부는 객체의 좌표, 크기 및 중심 중 적어도 하나 를 결정하고, 파악된 정보를 기초로 로봇 핸드가 바로 객체를 파지하도록 제어한다. 이때, 로봇 핸드가 객체를 파지하지 못하거나 파지한 이후에 객체를 해방하는 이벤트가 발생하는 경우, 제어부는 실패 이벤트를 기초로 객체의 좌표, 크기 및 중심 중 적어도 하나를 다시 결정하고, 로봇 핸드는 다시 결정된 팩터를 이용하여 객체를 다시 파지하려는 시도를 한다. 이러한 기계 학습 알고리즘에 따라 로봇 핸드가 객체를 파지하는 모습은 변화될 수 있다. 즉, 도 3의 (a)와 같이 객체의 우 측면에서 파지하는 시도를 할 수 있고, (b)에 도시된 것과 같이 좌 측면 에서 파지하는 시도를 할 수 있으며, (c)에 도시된 것과 같이 객체의 하단을 받치면서 좌 측면에서 다른 손 가락부를 통해 파지하는 것도 가능하다. 도 4는 비젼 시스템을 기초로 객체를 파지하는 본 발명에 따른 로봇 핸드 시스템의 구성을 설명하는 블록 구성 도이다. 도 4를 참조하면, 시스템의 비젼 시스템을 이용하는 구성 이외에 촉각 센서를 추가적으로 이용할 수 있는데, 이에 대해서는 도면을 참조하여 구체적으로 후술한다. 또한, 객체를 그립함에 있어, 제어부는 비전정보(촉각 센서 정보를 함께 이용 가능)를 활용해서 현재 잡고 있는 unknown object의 grasping 안전성을 판단할 수도 있다. 구체적으로, 제어부는 파지하고 있는 물체를 움직여 보면서 전체적으로 필요한 힘/토크를 측정해서 안 전성을 확인할 수 있다. 또한, 제어부는 물체의 예상되는 물성치를 고려해서 현재 grasping 형태에서 필요한 힘/토크를 예상할 수도 있다. 이때, 제어부는 로봇 핸드의 grasping 안전성이 미리 설정된 값보다 낮다고 판단되는 경우, 로봇 핸 드가 객체를 해방하고, 학습된 정보를 기초로 재파지하도록 제어할 수도 있다. 재파지(regrasping)을 하기 위해 비전 정보 및 촉각 정보로 최적의 조건 파악하고, regrasing 과정에서 물체를 잡고 조작하려는 목적에 맞게 물건을 잡을 수 있도록 할 수도 있다. 도 5는 본 발명에 따라 물체의 대략적인 위치, 모양 등을 비젼을 통해 파악하여 Grasping을 시도하고, task의 수행을 실패하는 경우에는 알고리즘에 따라 실패한 이벤트를 학습하는 방법을 설명하는 순서도이다. 도 5를 참조하면, 가장 먼저, 적어도 하나의 제 1 객체에 대한 영상을 카메라가 획득하는 단계(S10)가 진행된다. 여기서 본 발명은 제 1 객체에 대한 사전 정보가 전혀 없고, 카메라를 통해 획득되는 정보만을 기초로 제 1 객체에 대한 파지를 시도하고자 한다. 이후, 획득된 영상을 이용하여 제 1 객체의 좌표, 크기 및 중심 중 적어도 하나를 제어부가 결정하는 단계(S11)가 진행된다. S11 단계에서는 영상을 통해 제어부가 좌표, 크기 및 중심에 대한 모든 정보를 획득하는 것이 아니라 좌표, 크기 및 중심 중 적어도 하나의 팩터(factor)가 결정되는 경우, 결정된 팩터를 이용하여 핸드가 바 로 제 1 객체를 파지하도록 제어한다(S12). S12 단계에서 로봇 핸드는 정확한 정보를 보유하고 있지 못하므로, 제 1 객체 파지를 실패할 수 있다 (S13). 예를 들어, 제 1 객체가 로봇 핸드가 움직인 장소에 존재하지 않거나 로봇 핸드의 손가락부(22 0)를 벌린 정도가 제 1 객체의 너비보다 작거나 예상했던 1 객체의 중심부와 다른 중심부를 향해 로봇 핸드가 이동하는 등의 요소로 제 1 객체를 파지하지 못하는 이벤트가 발생될 수 있다. 이때, 제어부는 실패 이벤트를 기초로 제 1 객체의 좌표, 크기 및 중심 중 적어도 하나를 다시 결정(S14) 하고, 로봇 핸드는 다시 결정된 팩터를 이용하여 제 1 객체를 다시 파지하게 된다(S15). 객체 파지에 성공할 때까지, 본 발명에 따른 S13 단계 내지 S15 단계는 반복적으로 수행될 수 있다. 또한, 객체 파지에 성공한 경우라도 전술한 것과 같이, 안정성 테스트를 통해, 파지가 불안정하다고 결정되 면 객체 파지를 해방하고, 재파지 할 수도 있다. 따라서 본 발명에 따르면, 기계학습을 통한 파지 및 조작 방법 개발을 위해 기존 제안된 model-free 방법과 model-based 방법을 함께 개발해서 오래 걸리는 학습 시간 등의 약점을 극복하고 모델 불확실성이 있는 상황에 서도 활용할 수 있도록 상호 보완적인 개발이 될 수 있다. 제 2 실시예 - 객체를 카메라 및 촉각센서를 함께 이용하여 핸들링하는 기술 본 발명에서는 제 1 실시예에서 설명한 기계 학습의 효율을 높이기 위해 카메라 이외에 촉각센서를 함께 이용하여 로봇 핸드가 파지를 시도하는 기술을 제안하고자 한다. 즉, 본 발명은, 종래의 기계학습을 통한 파지 및 조작 방법 개발을 위해 기존 제안된 model-free 방법과 model- based 방법을 함께 개발해서 오래 걸리는 학습 시간 등의 약점을 극복하고, 모델 불확실성이 있는 상황에서도 활용할 수 있는 핸들링 기술을 제공하는 것을 목적으로 한다. 또한, 제 2 실시예에서는 촉각센서를 추가적으로 활용하여 기존에 많이 활용되는 normal force 측정은 물 론이고 shear force, 대상 물체의 전도성 등을 측정하여 대상물 분류 및 학습할 수 있는 핸들링 기술을 제공하 는 것을 목적으로 한다. 도 6은 비젼 시스템 이외에 촉각 센서를 추가적으로 이용하여 객체를 파지하는 본 발명에 따른 로봇 핸드 시스 템의 구성을 설명하는 블록 구성도이다. 도 6에 도시된 구성은 전술한 도 2에 도시된 것과 동일하나 추가적으로 촉각 센서를 이용한다. 본 발명에 따른 촉각 센서는 로봇 핸드가 파지하는 객체의 수직항력(normal force), 전단력 (shear force) 및 전도성(conductive) 중 적어도 하나를 센싱할 수 있다. 도 6을 참조하면, 본 발명에 따른 시스템에는 복수의 카메라가 배치되고, 로봇 핸드의 손가락부 의 적어도 일부에는 촉각 센서가 복수로 구비될 수 있다. 여기서 본 발명은 객체에 대한 획득한 정보 또는 학습된 정보가 없으므로, 복수의 카메라를 통한 비젼 시 스템을 기초로, 대략적인 물체의 위치, 모양을 파악하고, 파악된 정보를 기초로 로봇 핸드를 제어하여 객 체를 파지할 수 있는데, 구체적으로 제어부는 획득된 영상을 이용하여 객체의 좌표, 크기 및 중심 중 적어도 하나를 결정하고, 파악된 정보를 기초로 로봇 핸드가 바로 객체를 파지하도록 제어할 수 있 다. 로봇 핸드는 정확한 정보를 정보를 기초로 객체를 파지하는 것이 아니므로, 객체를 파지하지 못하 거나 파지한 이후에 객체를 해방하는 이벤트가 발생될 수 있고, 본 발명에서는 실패한 이벤트에 대한 기계 학습 알고리즘을 적용한다. 즉, 로봇 핸드가 객체의 파지를 실패하는 경우, 제어부는 실패 이벤트를 기초로 객체의 좌 표, 크기 및 중심 중 적어도 하나를 다시 결정하게 된다. 이 경우, 도 2에서 설명한 것과 달리, 도 6에서는 로봇 핸드의 손가락부가 파지를 위해 객체와 접촉되는 경우, 촉각센서를 통해 획득된 객체의 수직항력(normal force), 전단력(shear force), 전도 성(conductive) 중 적어도 하나가 재파지에 추가적으로 활용된다. 즉, 로봇 핸드는 다시 결정된 팩터와 촉각센서에서 감지한 적어도 하나의 촉각 팩터를 이용하여 객체 를 다시 파지하게 되므로, 비젼 시스템만을 이용하는 경우보다 훨씬 더 적은 시도, 연산량, 시간을 이용하 여 객체를 파지할 수 있게 된다. 도 7a 및 도 7b는 본 발명과 관련하여, 비젼 정보와 촉각 정보를 함께 이용한 로봇 파지의 구체적인 모습의 일 례를 도시한 것이다. 도 7a의 (a)를 참조하면, 본 발명에 따른 로봇 핸드는 1개의 손바닥부와 2개의 손가락부를 포함 하고, 각 손가락부는 복수의 마디로 구성된다. 본 발명에 따른 로봇 핸드는 도 7a의 (a) 내지 (d)에 도시된 것과 같이, 손바닥부를 기준으로 복수의 손가락부를 펼쳐서 오므리는 동작을 통해 객체를 파지할 수 있다. 즉, 카메라를 통해 획득된 영상을 기초로, 제어부는 객체의 좌표, 크기 및 중심 중 적어도 하나 를 결정하고, 파악된 정보를 기초로 로봇 핸드가 바로 객체를 파지하도록 제어한다. 이때, 도 7a의 (d)에서 로봇 핸드가 객체를 파지하지 못하거나 파지한 이후에 객체를 해방하는 이 벤트가 발생하는 경우, 제어부는 실패 이벤트를 기초로 객체의 좌표, 크기 및 중심 중 적어도 하나를 다시 결정하고, 로봇 핸드는 다시 결정된 팩터를 이용하여 객체를 다시 파지하려는 시도를 하는데, 추 가적으로 로봇 핸드의 손가락부가 파지를 위해 객체와 접촉되는 경우, 촉각센서를 통해 획 득된 객체의 수직항력(normal force), 전단력(shear force), 전도성(conductive) 중 적어도 하나가 재파지 에 더 활용된다. 이러한 기계 학습 알고리즘에 따라 로봇 핸드가 객체를 파지하는 것은 단순한 파지 이외에 특정 태스 크(task)를 수행하기 위해 이용될 수도 있다. 즉, 도 7b에 도시된 것과 같이, 2개의 로봇 핸드가 구비되고, 제 1 로봇 핸드(200a)는 제 1 손바닥부 (210a) 및 제 1 손가락부(220a)를 포함하며, 제 2 로봇 핸드(200b)는 제 1 손바닥부(210b) 및 제 1 손가락부 (220b)를 포함할 수 있다. 또한, 도 7b에서는 제 1 객체(10a)에 제 2 객체(10b)를 삽입하는 동작을 수행하기 위해, 제 1 로봇 핸드(200a) 와 제 2 로봇 핸드(200b)가 이용된다. 단순한 파지 동작이 아닌 제 1 객체(10a)에 제 2 객체(10b)를 삽입하는 동작을 수행해야 하므로, 2개의 로봇 핸 드는 계속적으로 파지를 실패할 수 있고, 이때 전술한 비젼 시스템에 따른 학습 알고리즘이 반복적으로 이 용된다. 더 나아가 촉각 센서를 통해 획득된 객체의 수직항력(normal force), 전단력(shear force), 전도성 (conductive) 중 적어도 하나가 재파지에 더 활용된다. 예를 들어, 물이 반만 들어있는 물병을 조작하는 경우와 같이, 촉각 정보를 이용해서 물체의 외란(무게중심 이 바뀌는 효과)에 맞게 힘 조절을 하면서 물건을 grasping & manipulation할 수 있다. 또한, 본 발명에서는 학습된 정보로 물체를 잡는 손의 모양 등을 미리 계산하여 적용할 수도 있다. 또한, 전술한 것과 같이, 특정 태스크(task)를 수행하기 위해, 물체를 잡고 조작하려는 목적에 맞게 물건 을 잡는 방법, 방향 등을 변경하는 알고리즘이 적용될 수 있다. 제 2 실시예에서 이용되는 촉각 정보는 normal force, shear force, 전기 전도성 등의 정보를 포함하고, 추가적 으로 작은 물체를 잡을 때 외부 환경(바닥면 등)과 접촉된 상태에서 잡을 수 있는 정보에 대해서도 추가적 으로 획득할 수 있다. 따라서 비전 정보와 촉각 정보를 함께 활용해서 정밀한 힘(shear force 포함)를 측정하여 정밀한 조립 작업이 가능해질 수 있다. 도 8은 본 발명과 관련하여, 비젼 정보와 촉각 정보를 함께 이용한 로봇 파지의 동작을 설명하는 순서도이다. 도 8을 참조하면, 가장 먼저, 카메라가 적어도 하나의 제 1 객체에 대한 영상을 획득하는 단계(S20)가 진행된다. 이후, 제어부는 획득된 영상을 이용하여 상기 제 1 객체의 좌표, 크기 및 중심 중 적어도 하나의 팩터 (factor)를 결정(S21)하고, 결정된 팩터를 이용하여 핸드가 제 1 객체를 파지한다(S22). 이때, 도 5에서의 방법과 달리, 제 2 실시예에서는 핸드가 상기 파지를 위해 제 1 객체와 접촉되는 경 우, 제 1 객체의 수직항력(normal force), 전단력(shear force), 전도성(conductive) 중 적어도 하나를 촉 각센서가 감지하는 단계(S22)가 진행된다. S22 단계 이후, 부족한 정보로 인해, 로봇 핸드가 제 1 객체의 파지를 실패하는 경우(S23), 제어부 는 실패 이벤트를 기초로 제 1 객체의 좌표, 크기 및 중심 중 적어도 하나의 팩터를 다시 결정한다(S24). S24 단계를 기초로, 핸드가 제 1 객체를 다시 파지하고자 하는 경우, 도 5에서의 방법과 달리 제 2 실시예 에서는 다시 결정된 팩터 및 촉각센서에서 감지한 적어도 하나의 촉각 팩터를 이용하여 제 1 객체를 다시 파지 하게 된다(S25). 따라서 본 발명에 따르면, 기존 많이 활용되는 normal force 측정은 물론이고 shear force까지 감지할 수 있고 대상 물체의 전도성 등을 측정하여 대상물 분류를 할 수 있는 촉각 센서를 개발하고 인간처럼 이러한 촉각 정보 를 활용 가능하다.또한, 시각 및 촉각정보를 이용한 부품의 실시간 위치/자세/상태 인식 기술, 부품의 안정파지를 위한 최적 파지 형태 추론지능 기술, 인식정보와 경험에 기반한 지능적 파지기술(단일 그리퍼/손 이용, 30종 이상 물체), 시각 및 촉각정보를 이용한 부품의 위치/방향 조작(In-Hand) 기술, 경험기반 다양한 부품의 조립 전략 학습 기술을 통해, 단순반복 작업을 제외하고 대부분의 핸들링 작업은 사람의 수 공정에 의해 수행되고 있는 현재의 문제점 을 해소할 수 있다. 제 3 실시예 - 학습된 정보를 기초로 객체를 더 정밀하게 파지하는 방법 본 발명에 따른 제 3 실시예에서는 전술한 제 1 실시예 및 제 2 실시예를 기초로 객체에 대한 정보가 학습 된 경우, 더 빠르고 더 정확하게 객체를 그립할 수 있는 추가적인 방법을 제안한다. 제 3 실시예에서 적용되는 추가적인 방법은 다음과 같다. 1) 미리 저장된 참조 화상과 입력 화상의 차이를 오차 함수(에러 function)을 통해 해결하는 방법 2) 잡는 부위 결정에 있어 2차원적인 지점을 정해서 소형상 모델을 결정하고, 3차원 위치로 피팅시켜 파지에 적 용하는 방법 먼저, 미리 저장된 참조 화상과 입력 화상의 차이를 오차 함수(에러 function)을 통해 해결하는 방법에 대해 설 명한다. 첫 번째 방법에서는 본 시스템은 메모리를 통해 객체와 관련된 정보를 미리 저장하거나 무선 통 신부를 통해 외부로부터 수신할 수 있다. 또한, 카메라 또는 촉각 센서를 통해 객체에 대한 파지를 시도하면서 객체에 대한 정보를 획 득할 수 있다. 이때, 제어부는 미리 저장되거나 외부로부터 수신한 객체에 대한 정보와 카메라 등을 통해 획득 된 정보를 서로 비교하여 오차 정도를 판단하고, 미리 저장된 정보를 이용하거나 획득된 정보를 수정하는 방법 등을 수행할 수 있다. 이때, 제어부는 미리 저장되거나 외부로부터 수신한 객체에 대한 정보와 카메라 등을 통해 획득 된 정보를 서로 비교하기 위한 오차 함수는 다음과 같다. 수학식 1"}
{"patent_id": "10-2017-0170642", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 커버 오차는 εcover(u,v,x,y)이며 레인지 오차는 range(u,v,x,y,z)이다. 이들의 오차항은 입력 레인지 화상의 좌표(u,v)에 있는 화소 마다에 평가된다. 참조 레인지 화상 의 화소 평행이동값(x, y, z)은 입력 레인지 화상 에 대한 그 위치를 결정한다. 함수는 가중치 λ를 이용해 모든 화상 화소(u,v)에 걸쳐 총합된다(예를 들면 λ=10). 표준화 계수 Ncover 및 Nrange는 오차를 물체 및 화상 사이즈에서 독립시킨다. 오차는 화상 R가 입력 이미지 I 중의 아마 부분 차폐된 물체와 위치 맞춤 될 경우에 최소가 될 수 있다. 다음으로, 잡는 부위 결정에 있어 2차원적인 지점을 정해서 소형상 모델을 결정하고, 3차원 위치로 피팅시켜 파 지에 적용하는 방법에 대해 설명한다. 도 9는 본 발명의 파지에 이용되는 소형상 모델 및 3차원 위치 피팅의 일례를 도시한 것이고, 도 10은 본 발명 에 따란 3차원 위치로 피팅시켜 파지에 적용하는 일례를 구체적으로 도시한 것이다. 도 9를 참조하면, 제어부는 촬영 화상 내에서 「인식 영역」과 해야 할 범위의 지정을 입력 장치를 통해 받아들인다. 여기서 「인식 영역」이란, 핸드 에 파지시키는 부위를 지정하기 위해 제어부가 촬영 화 상 내로 설정하는 영역이다. 인식 영역은 거리 계측의 대상 영역으로서 사용될 수 DT다. 도 9의 (a)를 참조하면, 제어부가 객체에서 파지하고자 하는 영역을 나타내는 인식영역(11, 12, 13)이 도시된다. 인식영역은 컵의 동체 부분을 지정하고, 인식영역은 컵 개구의 테두리 부분을 지정하며, 인식영역 은 핸들 부분을 지정한다. 또한, 도 9의 (b)에 도시된 것과 같이, 해당 인식영역과 연관된 2차원 도형을 나타내는 아이콘 리스트 가 디스플레이부 상에 더 표시될 수 있다. 도 9의 (b)에 도시된 아이콘 21~24는 각각 소형상 모델의 일례를 나타내고 있다. 즉, 아이콘 41은 사각기둥 모델, 아이콘 42는 평판 모델, 아이콘 43은 원기둥 모델, 아이콘 43은 원추대 모델이 다. 각각의 소형상 모델은 형태 및 크기를 규정하기 위한 형상 파라미터, 위치 및 자세를 규정하기 위한 배치 파라 미터를 가진다. 덧붙여 소형상 모델의 종류는 도 9의 (b)에 나타낸 것에 한정되지 않는 것은 물론이며 2차 타원체, L자형 각주, C자형 원기둥 등의 다양한 형상이 더 이용될 수 있다. 또한, 도 9의 (c)를 참조하면, 인식 영역(11, 12, 13)에 대응하는 작업 공간의 3차원 위치 데이터의 취득을 나타내는 개념도이다. 도 9의 (c)에서의 3 차원 위치 데이터는 인식 영역과 함께 작업 공간의 로봇 에서 본 깊이를 나타낸다. 또한, 도 9의 (d)를 참조하면, 소형상 모델(구체적으로는 원기둥 모델, 40)을 겹쳐 표시한 것이며 사용자가 원 기둥 모델을 선택했을 경우의 피팅 결과를 나타내고 있다. 또한, 도 10을 참조하면, 데이터 구조 구체적인 예이며 원통 모델에 적용 가능한 파지 패턴에 관한 데이터 내용을 나타내고 있다. 로봇 핸드의 종류가 다르면, 실행 가능한 파지 패턴이 다르며 같은 소형상 모델에 적용 가능한 파지 패턴 도 달라질 수 있다. 즉, 도 10에서 핸드는 핸드 타입이 「평면 3 관절 2 손가락 핸드」이며 소형상 모델이 원통 모델일 때 적 용 가능한 파지 패턴으로서, 4개의 파지 패턴이 적용 조건과 함께 기록되어 있다. 구체적으로는 4개의 파지 패턴은 측면 가위 파지, 단면 가위 파지, 잡아 포함 파지 및 가장자리 면 가위 파지 등이 이용될 수 있다. 따라서 본 발명에서는 획득된 영상을 기초로 제어부가 로봇 핸드가 파지하고자 하는 지점에 대응되는 2차원 모델을 확정하고, 깊이 정보를 통해 2차원 모델을 3차원 모델로 결정하며, 결정된 3차원 모델의 파지에 최적화된 자세를 결정하여 로봇 핸드가 객체를 파지하도록 할 수 있다. 이를 통해, 획득된 정보를 기초로 추가적인 정보 획득 없이 로봇 핸드는 신속하게 객체를 그립할 수 있게 된다. 제 4 실시예 - 비젼 정보 및 촉각 정보를 기초로 파지하는 동작을 통해 태스크(task)를 수행하는 방법 또한, 본 발명에서는 전술한 제 1 실시예 내지 제 3 실시예를 기초로 로봇 핸드가 객체를 파지하고, 특정 태스크를 수행하는 방법에 대해 설명한다. 도 11은 본 발명과 관련하여, 비젼 정보와 촉각 정보를 함께 이용한 로봇 파지의 전체적인 동작을 설명하는 순 서도이고, 도 12는 본 발명과 관련하여, 러닝을 이용한 객체 파지를 설명하는 순서도의 일례를 도시한 것이다. 도 11을 참조하면, 촉각 센서 및 비금속 센서에서 획득된 정보 및 카메라를 통해 획득된 정보를 기초로 제어부는 객체에 대한 물성 등의 성질을 예측하고, 이를 기초로 객체에 대한 파지를 시도 한다. 이때, 객체에 대한 파지는 실패할 수 있고, 제어부는 실패 이벤트가 발생될 때마다 카메라를 통해 획 득된 정보를 기초로 재파지를 위한 팩터를 조절하고, 촉각 센서 등을 통해 획든된 정보를 기초로 중추 촉 감 프로세스(S30)를 진행한다.즉, S30 단계에서는 제어신호를 피드백하거나 압력, 거침 정보를 제공하거나 객체에 대한 물성 정보를 제어 부에 전달한다. 이때, 제어부는 카메라를 통해 획득된 정보와 상기 S30 단계를 통해 획득된 정보를 기초로 객체에 대 한 정보를 학습(S31)하고, 학습된 정보를 기초로 객체에 대한 파지 조건을 재 설정(S32)하며, 반영된 재 설 정 조건을 기초로 반복적으로 객체를 파지하여 특정 태스크를 수행할 수 있다. 도 12를 참조하면, 촉각 센서를 통해 획득된 정보를 기초로 학습하는 과정과 카메라를 통해 획 득된 정보를 기초로 학습하는 과정이 도시되고, 이러한 시각입력 정보와 촉각입력 정보를 기초로 로봇핸 드가 파지 동작을 수행하는 과정이 도시된다. 제 5 실시예 - 재파지 동작을 통한 태스크(task)를 수행하는 방법 한편, 특정 업무(task)를 로봇 핸드가 수행하기 위해서는, 현재 grasping 한 형태로는 주어진 목적에 맞는 manipulation을 할 수 없을 때 regrasping을 사용하는 것이 필요할 수 있다. 따라서 본 발명에서는 특정 업무(task)를 로봇 핸드가 수행하기 위해, 현재 grasping 한 형태로는 주어진 목적 에 맞는 manipulation을 할 수 없을 때 regrasping을 사용하여 특정 업무를 완수할 수 있는 기술을 제안하는 것 을 목적으로 한다. 즉, 본 발명은 양팔 로봇 핸드가 물체를 조립작업 할 때, 현재 물체를 잡고 있는 상태에서는 조립 작업이 이루어질 수 없는 경우, 한쪽이나 양쪽 물건을 놓고 조립작업이 가능 하도록 다시 잡는데 필요한 최적 알고리즘 을 제안하는 것을 목적으로 한다. 또한, 본 발명은 재파지 동작에서 촉각센서를 추가적으로 활용하여 기존에 많이 활용되는 normal force 측 정은 물론이고 shear force, 대상 물체의 전도성 등을 측정하여 대상물 분류 및 학습할 수 있는 핸들링 기술을 제공하는 것을 목적으로 한다. 도 13은 본 발명과 관련하여, 물체의 방향 및 위치 변화에 따라 파지 방법이 변화하는 일례를 도시한 것이다. 도 13의 (a)를 참조하면, 물체는 외부의 힘 또는 파지하고자 하는 동작에 의해 위치 또는 방향이 변화될 수 있다. 또한, 복수의 객체가 존재하고, 특정 태스크(task)를 수행하기 위해, 복수의 객체가 결합되는 순서가 정해져 있는데, 해당 순서에 대응하지 않는 객체를 파지하는 이벤트가 발생할 수도 있다. 이 경우에는 도 13의 (b)에 도시된 것과 같이, 변화된 물체의 방향 및 위치 변화를 고려하여 객체에 대 한 파지가 수행되어야 한다. 또한, 도 13의 (b)에 도시된 것과 같이, 복수의 객체가 존재하고, 특정 태스크(task)를 수행하기 위해, 복 수의 객체가 결합되는 순서가 정해져 있는데, 해당 순서에 대응하지 않는 객체를 파지하는 이벤트가 발 생하는 경우에는, 파지한 객체를 해방하고, 새롭게 객체를 파지하는 방법이 적용되어야 한다. 또한, 재파지에 있어, 다음 프로세스에 최적화된 객체의 지점을 그립하거나 객체의 위치, 방향 등을 다 음 프로세스에 적합하도록 변화시킨 수 객체를 그립하는 방법 등도 이용될 수 있다. 도 14는 본 발명에 따른 재파지의 필요성을 설명하기 위한 도면이다. 도 14에서는 객체가 복수로 존재하고, 복수의 객체를 조립하는 특정 태스크를 수행하는 것을 목적으로 한다. 이때, 제어부는 특정 태스크를 수행하기 위해, 복수의 객체를 조립하는 순서 등에 대한 정보를 미리 저장하거나 통신부를 통해 외부로부터 지시 받을 수 있다. 도 14의 (a)에 도시된 것과 같이, 객체는 제 1 객체(10a), 제 2 객체(10b) 및 제 3 객체(10c)가 차례로 결 합되어야 완성될 수 있고, 먼저, 제 1 객체(10a)와 제 2 객체(10b)가 결합된 이후에 제 3 객체(10c)가 제 2 객 체(10b)와 결합되어야 해당 태스크가 수행될 수 있다. 그러나 이때, 도 14의 (b)에 도시된 것과 같이, 복수의 로봇 핸드가 제 2 객체(10b) 및 제 3 객체(10c)를 먼저 파지하여 결합하는 동작을 수행하는 이벤트가 발생될 수 있다. 이러한 도 14의 (b)와 같은 동작을 통해, 도 14의 (c)에 도시된 것과 같이 제 2 객체(10b) 및 제 3 객체(10c)가 결합되어 좌측에 위치되고, 제 1 객체(10a)는 우측에 위치할 수 있다. 이때, 도 14의 (d)에 도시된 것과 같이, 같이 제 2 객체(10b) 및 제 3 객체(10c)가 결합된 형태의 하단에 제 1 객체(10a)가 결합되는 이벤트가 발생되어 결국 객체를 조립할 수 없는 문제점이 발생한다. 따라서 이 경우, 미리 지정된 순서대로 먼저, 제 1 객체(10a)와 제 2 객체(10b)가 결합된 이후에 제 3 객체 (10c)가 제 2 객체(10b)와 결합되어야 해당 태스크가 수행될 수 있도록 하는 방법을 재파지를 통해 수행할 수 있다. 즉, 도 14의 (b)에서 제 1 객체(10a)와 제 2 객체(10b)를 2개의 로봇 핸드가 파지해야 하는데, 제 2 객체 (10b) 및 제 3 객체(10c)를 파지하는 이벤트가 발생된 경우, 로봇 핸드가 파지한 객체들을 해방하고, 다시 올바른 객체를 파지하도록 할 수 있다. 또한, 본 발명에서는 잘못된 이벤트가 발생된 경우, 해당 이벤트에 대응하여 파지해야 하는 객체 대상 및 순서 등을 변경하여 재파지를 수행할 수도 있다. 즉, 도 14의 (c)에서 제 2 객체(10b) 및 제 3 객체(10c)를 파지하여 결합한 잘못된 이벤트가 발생된 경우, 해당 이벤트에 대응하여 제 2 객체(10b) 및 제 3 객체(10c)가 결합된 구조를 해방하고, 결합된 구조를 좌우로 회전시 키거나 제 1 객체(10a)를 죄측으로 이동 및 회전시킨 후, 결합된 구조와 제 1 객체(10a)를 결합시키는 재파지 동작을 통해 특정 태스크를 수행할 수 있다. 도 15는 본 발명에 따른 재파지 학습 모델의 일례를 도시한 것이다. 도 15를 참조하면, ①에서 로봇 핸드는 객체를 수직으로 파지하는 것에 성공한다. 그러나 제어부는 특정 태스크를 수행함에 있어, ①의 형태가 바람직하지 않다는 것을 판단할 수 있고, ② 및 ③과 같이 파지 동작에서 객체를 해방할 수 있다. 또한, 제어부는 특정 태스크 완료를 위해, 다음 프로세스를 알고 있으므로, ② 및 ③의 해방 동작에서 객 체가 해방될 때 배치되는 위치 및 방향 등을 변화시킬 수 있다. 이후, 제어부의 제어에 따라 로봇 핸드는 좌측 상단에서 일정 각도로 객체를 파지하여 들어 올리 는 ④ 동작을 수행한다. ④ 동작의 수행에 있어, ① 동작을 기초로 촉각 센서를 통해 획득된 정보와 카메라를 통해 획득된 정 보를 함께 이용하여 파지하는 ⑤ 동작이 수행된다. 이후, ⑤ 동작을 통해 파지된 객체를 미리 지정된 순서 및 프로세스에 따라 이동시키는 ⑥ 동작을 수행할 수 있다. 또한, 도 16은 본 발명의 파지 성공 확률에 따른 재파지 수행 방법을 설명하기 위한 도면이다. 도 16의 (a)를 참조하면, 본 발명에서는 Kernel density estimation 방법을 활용할 수 있다. 즉, 재파지 동작 후 파지 성공 확률인 P(grasped | s, a)를 기초로 어떤 파지 동작이 성공율이 높은지 여부를 이용하고, 재파지 성공 후 물체 상태 확률인 P(s_new| s, a, grasped)를 기초로 물체 중심을 잡으려면 어떤 동 작이 좋은지 여부에 대한 것을 판단하여 이용할 수 있다. 도 16의 (b)에 도시된 것과 같이, 객체를 재파지 함에 있어, Kernel density estimation 방법을 활용하고, 재파지 동작 후 파지 성공 확률인 P(grasped | s, a)를 기초로 어떤 파지 동작이 성공율이 높은지 여부를 이용 하며, 도 16의 (c)에 도시된 것과 같이, 재파지 성공 후 물체 상태 확률인 P(s_new| s, a, grasped)를 기초로 물체 중심을 잡으려면 어떤 동작이 좋은지 여부에 대한 것을 판단하여 이용할 수 있다. 도 17은 본 발명과 관련하여, 재파지를 적용한 task의 수행을 설명하는 도면이다. 도 17의 (a)는 미리 정해진 프로세스에 따라 복수의 객체를 이동시켜 특정 태스크를 수행하는 과정에서, 미 리 정해진 프로세스와 어긋나는 이벤트가 발생되는 경우, 바로 해방 및 파지하는 재파지를 수행하는 과정을 도 시한 것이다. 이에 반해, 도 17의 (b)는 미리 정해진 프로세스에 따라 복수의 객체를 이동시켜 특정 태스크를 수행하는 과정에서, 미리 정해진 프로세스와 어긋나는 이벤트가 발생되는 경우, 해당 상태에서 특정 태스크를 수행하기에 가장 적절한 프로세스를 새롭게 제어부가 결정하고, 그에 따라 히제 및 파지하는 재파지 수행의 과정이 도시된다. 본 발명에서는 전술한 도 17의 (a) 및 (b)에서 도시된 내용이 모두 적용될 수 있다. 또한, 도 18은 본 발명과 관련하여, 현재 grasping 한 형태로는 주어진 목적에 맞는 manipulation을 할 수 없을 때, regrasping을 사용하여 특정 업무를 완수할 수 있는 방법을 설명하는 순서도이다. 도 18을 참조하면, 복수의 객체에 대한 영상을 카메라가 획득하는 단계(S40)가 진행된다. 이후, 제어부의 제어를 기초로, 미리 설정된 순서에 따라 로봇 핸드가 상기 복수의 객체 중 제 1 객 체를 파지하여 이동시키는 단계(S41)가 진행된다. 이후, 제어부는 태스크를 수행할 수 없는지 여부를 판단한다(S41). 여기서 태스크를 수행할 수 없는 이벤트는 제 1 객체의 위치 및 방향 중 적어도 하나가 변경되어 원하는 대로 제 1 객체를 파지하지 못하거나 이미 잘못된 파지 및 결합이 이루어져 정해진 프로세스 대로는 태스크를 수행할 수 없는 경우 등을 포함할 수 있다. S41 단계에서 태스크를 수행할 수 없다고 판단되는 경우, 제어부는 로봇 핸드가 상기 파지한 제 1 객 체를 해방하도록 제어(S42)한다. S42 단계 이후, 제어부는 원래의 계획대로 제 1 객체를 파지하거나 잘못 진행된 프로세스에 적합한 계획을 새롭게 수립하고 제 1 객체를 파지할 수 있도록 카메라를 통해 획득된 영상을 기초로 제 1 객체의 변화된 방향 및 위치 중 적어도 하나를 결정(S43)한다. 또한, 로봇 핸드는 변화된 방향 및 위치를 기초로 제 1 객체를 재파지하는 단계(S44)를 진행한다. 제어부는 특정 태스크가 수행될 때까지, S41 내지 S44 단계가 반복적으로 학습되어 수행되도록 제어할 수 있다. 본 발명은 조립할 때의 regrasping으로서, 이동모양 및 조립 순서 등에 대한 정보를 미리 알고 있으므로, 다음 프로세스를 알고 있고, 다음 작업에 대해 좋은 배치가 되도록 파지에서 해방시 물체를 놓으며, 재파지 동작을 수행할 수 있다. 또한, 본 발명에서는 객체가 예상 모양과 달라 파지하지 못하는 이벤트 발생되는 경우, 객체의 해당 부 분을 로봇 핸드를 통해 회전시키고, 다시 시각 정보를 획득한 후, 재파지 동작을 수행할 수도 있다. 또한, 본 발명에서는 획득된 정보를 바탕으로 각 부품의 조립을 위한 geometric 특징 반영 파일을 생성(구멍의 조립방향, 크기등)하고, Permutation하며, 모든 부품의 조합을 통해 1차 조립 작업 시퀀스 확보한 후, 조립되어 야 하는 각 파트의 구멍은 모두 사용되었는지 확인할 수 있다. 또한, 조립 작업 시퀀스를 searching하고, Constraint satisfaction problem으로 문제를 해결할 수 있다(조립 과정에서 조립 방향으로 부품간 간섭이 일어 나는지 확인). 따라서 본 발명은 특정 업무를 로봇 핸드가 수행하기 위해, 현재 grasping 한 형태로는 주어진 목적에 맞는 manipulation을 할 수 없을 때 regrasping을 사용하여 특정 업무를 완수할 수 있는 기술을 제공할 수 있다. 또한, 본 발명에 따르면, 기존 많이 활용되는 normal force 측정은 물론이고 shear force까지 감지할 수 있고 대상 물체의 전도성 등을 측정하여 대상물 분류를 할 수 있는 촉각 센서를 개발하고 인간처럼 이러한 촉각 정보 를 활용할 수 있다. 또한, 시각 및 촉각정보를 이용한 부품의 실시간 위치/자세/상태 인식 기술, 부품의 안정파지를 위한 최적 파지 형태 추론지능 기술, 인식정보와 경험에 기반한 지능적 파지기술(단일 그리퍼/손 이용, 30종 이상 물체), 시각 및 촉각정보를 이용한 부품의 위치/방향 조작(In-Hand) 기술, 경험기반 다양한 부품의 조립 전략 학습 기술을 통해, 단순반복 작업을 제외하고 대부분의 핸들링 작업은 사람의 수 공정에 의해 수행되고 있는 현재의 문제점 을 해소할 수 있다. 한편, 상술한 본 발명의 실시예들은 다양한 수단을 통해 구현될 수 있다. 예를 들어, 본 발명의 실시예들은 하 드웨어, 펌웨어(firmware), 소프트웨어 또는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 실시예들에 따른 방법은 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal ProcessingDevices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 컨트롤러, 마이크로 컨트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 실시예들에 따른 방법은 이상에서 설명된 기능 또는 동작 들을 수행하는 모듈, 절차 또는 함수 등의 형태로 구현될 수 있다. 소프트웨어 코드는 메모리 유닛에 저장되어 프로세서에 의해 구동될 수 있다. 상기 메모리 유닛은 상기 프로세서 내부 또는 외부에 위치하여, 이미 공지된 다양한 수단에 의해 상기 프로세서와 데이터를 주고 받을 수 있다. 상술한 바와 같이 개시된 본 발명의 바람직한 실시예들에 대한 상세한 설명은 당업자가 본 발명을 구현하고 실 시할 수 있도록 제공되었다. 상기에서는 본 발명의 바람직한 실시예들을 참조하여 설명하였지만, 해당 기술 분 야의 숙련된 당업자는 본 발명의 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시 킬 수 있음을 이해할 수 있을 것이다. 예를 들어, 당업자는 상술한 실시예들에 기재된 각 구성을 서로 조합하는 방식으로 이용할 수 있다. 따라서, 본 발명은 여기에 나타난 실시형태들에 제한되려는 것이 아니라, 여기서 개 시된 원리들 및 신규한 특징들과 일치하는 최광의 범위를 부여하려는 것이다. 본 발명은 본 발명의 정신 및 필수적 특징을 벗어나지 않는 범위에서 다른 특정한 형태로 구체화될 수 있다. 따 라서, 상기의 상세한 설명은 모든 면에서 제한적으로 해석되어서는 아니 되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적 해석에 의해 결정되어야 하고, 본 발명의 등가적 범위 내에서 의 모든 변경은 본 발명의 범위에 포함된다. 본 발명은 여기에 나타난 실시형태들에 제한되려는 것이 아니라, 여기서 개시된 원리들 및 신규한 특징들과 일치하는 최광의 범위를 부여하려는 것이다. 또한, 특허청구범위에서 명시적인 인용 관계가 있지 않은 청구항들을 결합하여 실시예를 구성하거나 출원 후의 보정에 의해 새로운 청구 항으로 포함할 수 있다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7a 도면7b 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17 도면18"}
{"patent_id": "10-2017-0170642", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1는 본 발명이 제안하는 로봇 핸드 시스템의 구성을 설명하는 블록 구성도이다. 도 2는 본 발명이 제안하는 로봇 핸드의 구성을 설명하는 도면이다.도 3은 본 발명의 비젼, 제어 및 기계 학습에 따라 객체를 파지하는 구체적인 일례를 도시한 것이다. 도 4는 비젼 시스템을 기초로 객체를 파지하는 본 발명에 따른 로봇 핸드 시스템의 구성을 설명하는 블록 구성 도이다. 도 5는 본 발명에 따라 물체의 대략적인 위치, 모양 등을 비젼을 통해 파악하여 Grasping을 시도하고, task의 수행을 실패하는 경우에는 알고리즘에 따라 실패한 이벤트를 학습하는 방법을 설명하는 순서도이다. 도 6은 비젼 시스템 이외에 촉각 센서를 추가적으로 이용하여 객체를 파지하는 본 발명에 따른 로봇 핸드 시스 템의 구성을 설명하는 블록 구성도이다. 도 7a 및 도 7b는 본 발명과 관련하여, 비젼 정보와 촉각 정보를 함께 이용한 로봇 파지의 구체적인 모습의 일 례를 도시한 것이다. 도 8은 본 발명과 관련하여, 비젼 정보와 촉각 정보를 함께 이용한 로봇 파지의 동작을 설명하는 순서도이다. 도 9는 본 발명의 파지에 이용되는 소형상 모델 및 3차원 위치 피팅의 일례를 도시한 것이다. 도 10은 본 발명에 따란 3차원 위치로 피팅시켜 파지에 적용하는 일례를 구체적으로 도시한 것이다. 도 11은 본 발명과 관련하여, 비젼 정보와 촉각 정보를 함께 이용한 로봇 파지의 전체적인 동작을 설명하는 순 서도이다. 도 12는 본 발명과 관련하여, 러닝을 이용한 객체 파지를 설명하는 순서도의 일례를 도시한 것이다. 도 13은 본 발명과 관련하여, 물체의 방향 및 위치 변화에 따라 파지 방법이 변화하는 일례를 도시한 것이다. 도 14는 본 발명에 따른 재파지의 필요성을 설명하기 위한 도면이다. 도 15는 본 발명에 따른 재파지 학습 모델의 일례를 도시한 것이다. 도 16은 본 발명의 파지 성공 확률에 따른 재파지 수행 방법을 설명하기 위한 도면이다. 도 17은 본 발명과 관련하여, 재파지를 적용한 task의 수행을 설명하는 도면이다. 도 18은 본 발명과 관련하여, 현재 grasping 한 형태로는 주어진 목적에 맞는 manipulation을 할 수 없을 때, regrasping을 사용하여 특정 업무를 완수할 수 있는 방법을 설명하는 순서도이다."}
