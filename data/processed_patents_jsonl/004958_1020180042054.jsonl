{"patent_id": "10-2018-0042054", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0118766", "출원번호": "10-2018-0042054", "발명의 명칭": "추론을 위한 제한된 볼츠만 머신 구축 방법 및 추론을 위한 제한된 볼츠만 머신을 탑재한 컴", "출원인": "포항공과대학교 산학협력단", "발명자": "김재준"}}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터 장치가 외부 데이터를 기반으로 데이터 뉴런 및 라벨 뉴런을 구축하는 단계;상기 컴퓨터 장치가 상기 데이터 뉴런의 출력값을 기반으로 숨겨진 뉴런을 구축하는 단계; 및상기 컴퓨터 장치가 상기 숨겨진 뉴런의 출력값을 기반으로 상기 데이터 뉴런 및 상기 라벨 뉴런을 재구축하는단계를 포함하는 추론을 위한 제한된 볼츠만 머신 구축 방법."}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 컴퓨터 장치가 상기 재구축된 데이터 뉴런의 출력값을 기반으로 상기 숨겨진 뉴런을 재구축하는 단계를 더포함하는 추론을 위한 제한된 볼츠만 머신 구축 방법."}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 컴퓨터 장치는 재구축 전의 숨겨진 뉴런을 기준으로 상기 라벨 뉴런과 상기 숨겨진 뉴런 사이의 가중치에대한 수정치를 결정하는 추론을 위한 제한된 볼츠만 머신 구축 방법."}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 컴퓨터 장치는 아래 수식을 이용하여 상기 라벨 뉴런과 상기 숨겨진 뉴런 사이의 가중치에 대한 수정치(ΔUi,j)를 결정하는 추론을 위한 제한된 볼츠만 머신 구축 방법.(여기서 η(eta)는 학습률, i는 라벨 뉴런의 노드 식별자, j는 숨겨진 뉴런의 노드 식별자, lp,i는 재구축 전의라벨 뉴런, ln,i는 재구축된 라벨 뉴런, hp,i는 재구축 전의 숨겨진 뉴런)"}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "컴퓨터 장치가 데이터 뉴런, 라벨 뉴런 및 숨겨진 뉴런을 포함하는 ClassRBM(Classification RestrictedBoltzmann Machine)을 학습하는 단계; 및상기 컴퓨터 장치가 입력 데이터를 상기 학습된 ClassRBM에 입력하여 결과값을 생성하는 단계를 포함하되,상기 학습하는 단계에서 상기 컴퓨터 장치는 제어 신호의 모드에 따라 상기 데이터 뉴런만을 사용하여 상기 숨겨진 뉴런을 재구축하고, 상기 재구축된 숨겨진 뉴런을 기반으로 상기 데이터 뉴런과 상기 라벨 뉴런을 재구축하는 추론을 위한 제한된 볼츠만 머신 운용 방법."}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 학습하는 단계에서 상기 컴퓨터 장치는 상기 재구축된 데이터 뉴런의 출력값을 기반으로 상기 숨겨진 뉴런을 재구축하는 단계를 더 포함하는 추론을 위한 제한된 볼츠만 머신 운용 방법."}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서,공개특허 10-2019-0118766-3-상기 학습하는 단계에서 상기 컴퓨터 장치는 재구축 전의 숨겨진 뉴런을 기준으로 상기 라벨 뉴런과 상기 숨겨진 뉴런 사이의 가중치에 대한 수정치를 결정하는 추론을 위한 제한된 볼츠만 머신 운용 방법."}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제5항에 있어서,상기 학습하는 단계에서 상기 컴퓨터 장치는 상기 컴퓨터 장치는 아래 수식을 이용하여 상기 라벨 뉴런과 상기숨겨진 뉴런 사이의 가중치에 대한 수정치 (ΔUi,j)를 결정하는 추론을 위한 제한된 볼츠만 머신 운용 방법.(여기서 η(eta)는 학습률, i는 라벨 뉴런의 노드 식별자, j는 숨겨진 뉴런의 노드 식별자, lp,i는 재구축 전의라벨 뉴런, ln,i는 재구축된 라벨 뉴런, hp,i는 재구축 전의 숨겨진 뉴런)"}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "훈련 데이터를 입력받는 입력 장치;데이터 뉴런, 라벨 뉴런 및 숨겨진 뉴런을 포함하는 학습된 ClassRBM(Classification Restricted BoltzmannMachine)을 저장하는 저장 장치; 및상기 훈련 데이터를 이용하여 상기 데이터 뉴런 및 상기 라벨 뉴런을 구축하고, 상기 데이터 뉴런의 출력값을기반으로 숨겨진 뉴런을 구축하고, 상기 숨겨진 뉴런의 출력값을 기반으로 상기 데이터 뉴런 및 상기 라벨 뉴런을 재구축하여 상기 학습된 ClassRBM을 마련하는 연산 장치를 포함하는 추론을 위한 제한된 볼츠만 머신을 탑재한 컴퓨터 장치."}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 연산 장치는 상기 재구축된 데이터 뉴런의 출력값을 기반으로 상기 숨겨진 뉴런을 재구축하여 상기 학습된ClassRBM을 마련하는 추론을 위한 제한된 볼츠만 머신을 탑재한 컴퓨터 장치."}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 연산 장치는 재구축 전의 숨겨진 뉴런을 기준으로 결정된 상기 라벨 뉴런과 상기 숨겨진 뉴런 사이의 가중치에 대한 수정치를 이용하여 상기 학습된 ClassRBM을 마련하는 추론을 위한 제한된 볼츠만 머신을 탑재한 컴퓨터 장치."}
{"patent_id": "10-2018-0042054", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,상기 연산 장치는 아래 수식을 이용하여 결정되는 상기 라벨 뉴런과 상기 숨겨진 뉴런 사이의 가중치에 대한 수정치 (ΔUi,j)를 이용하여 상기 학습된 ClassRBM을 마련하는 추론을 위한 제한된 볼츠만 머신을 탑재한 컴퓨터장치.(여기서 η(eta)는 학습률, i는 라벨 뉴런의 노드 식별자, j는 숨겨진 뉴런의 노드 식별자, lp,i는 재구축 전의라벨 뉴런, ln,i는 재구축된 라벨 뉴런, hp,i는 재구축 전의 숨겨진 뉴런)"}
{"patent_id": "10-2018-0042054", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "추론을 위한 제한된 볼츠만 머신 구축 방법은 컴퓨터 장치가 외부 데이터를 기반으로 데이터 뉴런 및 라벨 뉴런 을 구축하는 단계, 상기 컴퓨터 장치가 상기 데이터 뉴런의 출력값을 기반으로 숨겨진 뉴런을 구축하는 단계 및 상기 컴퓨터 장치가 상기 숨겨진 뉴런의 출력값을 기반으로 상기 데이터 뉴런 및 상기 라벨 뉴런을 재구축하는 단계를 포함한다."}
{"patent_id": "10-2018-0042054", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "이하 설명하는 기술은 추론을 위한 제한된 볼츠만 머신을 생성하는 기법에 관한 것이다."}
{"patent_id": "10-2018-0042054", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "생성 모델(Generative model)은 주어진 데이터를 포괄적으로 가장 잘 설명할 수 있는 확률론적 모형을 구축하는 모델이다. 한편 식별 모델(Discriminative model)은 의의 주어진 데이터 값만을 가지고 분류를 수행하는 모델에 해당한다. 따라서, 식별 모델은 주어진 데이터를 기반으로 하여 학습을 수행하고 나면 임의의 입력받은 데이터 가 어떤 분류에 속하는지를 추론하는 데에 그치지만, 생성 모델은 주어진 데이터를 통해 학습을 수행하고 나면 임의의 입력받은 데이터를 추론할 수 있을 뿐만 아니라 학습한 모델을 기반으로 새로운 데이터를 자체적으로 생 성해낼 수도 있다. 생성 모델의 일종인 제한된 볼츠만 머신(Restricted Boltzmann Machine, 이하 RBM)은 데이터 뉴런(Data Neuron)과 숨겨진 뉴런(Hidden Neuron)으로 구성된다. RBM은 주로 입력된 데이터를 전처리(Pre-processing)하 여 노이즈를 제거하거나, 부족한 부분을 복구하고, 이를 인코딩하여 이후의 네트워크가 보다 학습하기 쉬운 형 태로 바꾸어주는 역할을 한다. 그리고 RBM의 변형인 추론을 위한 제한된 볼츠만 머신(Classification Restricted Boltzmann Machine, 이하 ClassRBM)은 데이터 뉴런, 라벨 뉴런(Label Neuron), 그리고 숨겨진 뉴런 으로 구성된다. CRBM은 단순히 입력된 데이터의 전처리뿐만이 아니라, 그 데이터가 어떠한 분류에 속하는지 추 론할 수 있는 역할 또한 할 수 있다. 선행기술문헌 비특허문헌 (비특허문헌 0001) Hugo Larochelle 외, \"Learning Algorithms for the Classification Restricted Boltzmann Machine\", Journal of Machine Learning Research 13, 2012, 643-669페이지"}
{"patent_id": "10-2018-0042054", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "종래 ClassRBM은 대조 발산 알고리즘(Contrastive Divergence Algorithm) 기반으로 학습되었다. 종래 ClassRBM 은 학습 과정에서 데이터 뉴런과 라벨 뉴런을 모두 사용하여 숨겨진 뉴런을 구축하고, 추론 과정에서는 데이터 뉴런만을 사용하여 숨겨진 뉴런을 추론한다. 따라서 입력 조건의 차이로 인하여 추론 정확도가 떨어지는 문제점 이 있었다. 이하 설명하는 기술은 비대칭 대조 발산 알고리즘(Asymmetric Contrastive Divergence Algorithm) 을 사용하여 ClassRBM을 학습하는 기법을 제공하고자 한다."}
{"patent_id": "10-2018-0042054", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "추론을 위한 제한된 볼츠만 머신 구축 방법은 컴퓨터 장치가 외부 데이터를 기반으로 데이터 뉴런 및 라벨 뉴런 을 구축하는 단계, 상기 컴퓨터 장치가 상기 데이터 뉴런의 출력값을 기반으로 숨겨진 뉴런을 구축하는 단계 및 상기 컴퓨터 장치가 상기 숨겨진 뉴런의 출력값을 기반으로 상기 데이터 뉴런 및 상기 라벨 뉴런을 재구축하는 단계를 포함한다. 추론을 위한 제한된 볼츠만 머신 운용 방법은 컴퓨터 장치가 데이터 뉴런, 라벨 뉴런 및 숨겨진 뉴런을 포함하 는 ClassRBM을 학습하는 단계 및 상기 컴퓨터 장치가 입력 데이터를 상기 학습된 ClassRBM에 입력하여 결과값을 생성하는 단계를 포함한다. 상기 학습하는 단계에서 상기 컴퓨터 장치는 제어 신호의 모드에 따라 상기 데이터 뉴런만을 사용하여 상기 숨겨진 뉴런을 재구축하고, 상기 재구축된 숨겨진 뉴런을 기반으로 상기 데이터 뉴런과 상기 라벨 뉴런을 재구축한다. 추론을 위한 제한된 볼츠만 머신을 탑재한 컴퓨터 장치는 훈련 데이터를 입력받는 입력 장치, 데이터 뉴런, 라 벨 뉴런 및 숨겨진 뉴런을 포함하는 학습된 ClassRBM(Classification Restricted Boltzmann Machine)을 저장하 는 저장 장치 및 상기 훈련 데이터를 이용하여 상기 데이터 뉴런 및 상기 라벨 뉴런을 구축하고, 상기 데이터 뉴런의 출력값을 기반으로 숨겨진 뉴런을 구축하고, 상기 숨겨진 뉴런의 출력값을 기반으로 상기 데이터 뉴런 및 상기 라벨 뉴런을 재구축하여 상기 학습된 ClassRBM을 마련하는 연산 장치를 포함한다."}
{"patent_id": "10-2018-0042054", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이하 설명하는 기술은 비대칭 대조 발산 알고리즘을 사용하여 추론과 동일한 입력 조건을 ClassRBM을 학습하여, 추론의 정확도를 높인다."}
{"patent_id": "10-2018-0042054", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 설명하는 기술은 다양한 변경을 가할 수 있고 여러 가지 실시례를 가질 수 있는 바, 특정 실시례들을 도면 에 예시하고 상세하게 설명하고자 한다. 그러나, 이는 이하 설명하는 기술을 특정한 실시 형태에 대해 한정하려 는 것이 아니며, 이하 설명하는 기술의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하 는 것으로 이해되어야 한다. 제1, 제2, A, B 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 해당 구성요소들은 상기 용어 들에 의해 한정되지는 않으며, 단지 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예 를 들어, 이하 설명하는 기술의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 및/또는 이라는 용어는 복수의 관련된 기재된 항목들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다. 본 명세서에서 사용되는 용어에서 단수의 표현은 문맥상 명백하게 다르게 해석되지 않는 한 복수의 표현을 포함 하는 것으로 이해되어야 하고, \"포함한다\" 등의 용어는 설시된 특징, 개수, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함을 의미하는 것이지, 하나 또는 그 이상의 다른 특징들이나 개수, 단계 동작 구성요 소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 배제하지 않는 것으로 이해되어야 한다. 도면에 대한 상세한 설명을 하기에 앞서, 본 명세서에서의 구성부들에 대한 구분은 각 구성부가 담당하는 주기 능 별로 구분한 것에 불과함을 명확히 하고자 한다. 즉, 이하에서 설명할 2개 이상의 구성부가 하나의 구성부로 합쳐지거나 또는 하나의 구성부가 보다 세분화된 기능별로 2개 이상으로 분화되어 구비될 수도 있다. 그리고 이 하에서 설명할 구성부 각각은 자신이 담당하는 주기능 이외에도 다른 구성부가 담당하는 기능 중 일부 또는 전 부의 기능을 추가적으로 수행할 수도 있으며, 구성부 각각이 담당하는 주기능 중 일부 기능이 다른 구성부에 의 해 전담되어 수행될 수도 있음은 물론이다. 또, 방법 또는 동작 방법을 수행함에 있어서, 상기 방법을 이루는 각 과정들은 문맥상 명백하게 특정 순서를 기 재하지 않은 이상 명기된 순서와 다르게 일어날 수 있다. 즉, 각 과정들은 명기된 순서와 동일하게 일어날 수도 있고 실질적으로 동시에 수행될 수도 있으며 반대의 순서대로 수행될 수도 있다. 이하 설명하는 기술은 ClassRBM을 구축하는 기법이다. 또 이하 설명하는 기술은 구축된 ClassRBM을 운용하여 일 정한 서비스를 제공하는 기법에 관한 것이다. ClassRBM은 ClassRBM 자체로 운용될 수도 있고, 다른 신경망과 함 께 사용될 수도 있다. ClassRBM은 일정한 명령(내지 프로그램)을 처리할 수 있는 연산 장치에서 구동된다. 이하 ClassRBM을 구축하고, 구축된 ClassRBM을 이용하여 일정한 데이터 추론을 수행하는 장치를 컴퓨터 장치라고 명 명한다. 컴퓨터 장치는 ClassRBM 모델(프로그램)을 탑재한 PC, 스마트폰, 네트워크 상의 서버 등과 같은 장치를 포함하는 의미이다.한편 ClassRBM은 다양한 유형의 데이터를 학습하여 일정한 데이터를 출력할 수 있다. 예컨대, ClassRBM은 영상 데이터, 음성 데이터, 텍스트 데이터 등 다양한 데이터를 기반으로 구축될 수 있다. 이하 ClassRBM은 데이터의 종류를 제한하지 않는다고 전제한다. 이하 훈련 데이터는 ClassRBM을 학습하기 위한 데이터이고, 입력 데이터는 학습된 ClassRBM을 이용하여 일정한 결과를 도출하기 위한 데이터라고 가정한다. 도 1은 RBM 구조를 도시한 예이다. RBM은 차원 감소, 분류, 선형 회귀 분석, 협업 필터링(collaborative filtering), 특징값 학습(feature learning) 및 주제 모델링(topic modelling)에 사용할 수 있는 모델이다. RBM은 가시층(Visible Layer) 및 은닉층(Hidden Layer)로 구성된다. 한편 가시층은 입력층이라고도 한다. 도 1 에서는 6개의 노드(h1 내지 h6)를 갖는 은닉층과 8개의 노드(v1 내지 v8)를 갖는 가시층을 예로 도시하였다. 가 시층은 데이터가 입력되는 계층이고, 은닉층은 특징값이 학습되는 계층이다. RBM에서 동일 계층의 노드(뉴런)는 서로 연결되지 않고, 가시층의 노드와 은닉층의 노드는 각각 모두 대칭적으로 연결된다. RBM은 학습 데이터가 입력되는 가시 뉴런(neuron) 과 확률적으로 값이 결정되는 숨은 뉴런이 가중치를 통하여 연결되어 있는 형태이 다. RBM 학습 과정에 대하여 간략하게 설명한다. 이하 설명은 RBM 학습 과정 알고리즘 하나 또는 일부이며 다른 알 고리즘이 사용될 수도 있다. 가시층은 데이터를 입력받고, 입력받은 데이터를 은닉층에 얼마나 전달할 것인지를 확률에 따라 결정(stochastic decision)한다. RBM은 입력 데이터 입력받은 가시층 노드가 x라는 값을 출력하고, 가시층 출력값 x는 은닉층의 노드에 있는 가중치 w와 곱해지고, 최종적으로 바이어스(bias) b를 더한 값(w*x+ b)이 활성함수 f()에 입력된다. 최종적으로 활성함수 f()이 은닉층 노드의 출력값이 된다. 하나의 은닉층 노드 는 가시층의 모든 노드로부터 값을 전달받아 각각 가중치를 곱한 결과를 합산하여 활성 함수에 입력한다. 한편 RBM은 데이터를 스스로 재구성(reconstruction)한다. 은닉층 노드는 자신의 값에 가중치를 적용하여 가시 층 노드에 전달하고, 가시층 각 노드는 은닉층 노드들로부터 전달받은 값을 합산하고, 추가적으로 바이어스를 더한 값으로 재구성 값을 결정한다. 한편 초기 입력값과 재구성값에 차이가 오차가 되는데, RBM은 가시층과 은 닉층 사이의 데이터 전달 과정을 반복하면서 오차를 줄이는 방향으로 가중치를 최적화한다. 도 2는 ClassRBM 구조를 도시한 예이다. ClassRBM은 데이터 뉴런(V)와 라벨 뉴런(L)으로 구성되는 가시층 및 숨 겨진 뉴런(H)을 포함하는 은닉층을 포함한다. ClassRBM은 RBM과 달리 가시층에 라벨 뉴런(L)을 더 포함한다. 라벨(label)은 학습 데이터에 대한 식별 내지 분류를 수행할 수 있는 정보를 제공할 수 있다. 라벨 뉴런(L)은 훈련 데이터를 기반으로 생성될 수 있다. 숨겨진 뉴런(H)은 각각 데이터 뉴런(V)의 노드와 라벨 뉴런(L)의 노드 에 대한 가중치를 갖는다. 따라서 ClassRBM은 데이터 뉴런(V), 라벨 뉴런(L), 숨겨진 뉴런(H), 데이터 뉴런-숨 겨진 뉴런 간 가중치(W), 라벨 뉴런-숨겨진 뉴런 간 가중치(U)라는 구성 요소를 포함한다. 또 전술한 바와 같이 ClassRBM은 각 뉴런에 추가적인 가중치를 부여하는 바이어스 뉴런을 더 포함할 수 있다. 또 도 2의 ClassRBM의 구조를 적층하여 복수의 네트워크 계층을 갖는 모델을 사용할 수 있다. 일반적으로 데이터 뉴런(V)와 라벨 뉴런(L)는 외부에서 입력된 데이터나, 이전 네트워크 계층(전술한 적층된 구 조 경우)의 숨겨진 뉴런의 값 등이 될 수 있다. 데이터 뉴런(V), 라벨 뉴런부(L), 그리고 숨겨진 뉴런(H)는 일 반적으로 정수형 혹은 실수형의 데이터를 갖는 1차원 벡터 형태 혹은 그 이상의 차원을 갖는 형태로 구성될 수 있다. 이하 데이터 뉴런(V), 라벨 뉴런부(L), 그리고 숨겨진 뉴런(H)는0 또는 1의 값을 갖는 1차원 정수 벡터 형태라고 가정한다. 또한, 데이터 뉴런-숨겨진 뉴런 간 가중치(W)와 라벨 뉴런-숨겨진 뉴런 간 가중치(U)는 일 반적으로 정수형 혹은 실수형의 데이터를 갖는 2차원 매트릭스 형태 혹은 그 이상의 차원을 갖는 형태로 구성될 수 있다. 이하 데이터 뉴런-숨겨진 뉴런 간 가중치(W)와 라벨 뉴런-숨겨진 뉴런 간 가중치(U)는 2차원 매트릭스 형태라고 가정한다. 도 3은 ClassRBM을 학습하는 과정 및 ClassRBM 이용한 추론 과정에 대한 예이다. 도 3은 종래 ClassRBM에서 수 행하는 가중치 학습 과정과 추론 과정에 대한 예이다. 이하 컴퓨터 장치가 ClassRBM 학습 및 추론을 수행한다고 가정한다. 도 3(A)는 ClassRBM을 학습하는 과정에 대한 예이다. 컴퓨터 장치는 훈련 데이터를 입력받는다. 컴퓨터 장치는 훈련 데이터를 데이터 뉴런(V)과 라벨 뉴런(L)에 입력하여 초기 데이터 뉴런(Vp) 및 라벨 뉴런(Lp)을 구축한다. 컴퓨터 장치는 데이터 뉴런(Vp) 및 라벨 뉴런(Lp)의 출력값에 각각 가중치 W와 U를 부여한 값을 사용하여 숨겨진 뉴런(Hp)을 구축한다. 또 컴퓨터 장치는 숨겨진 뉴런(Hp)의 값에 각각 가중치를 부여하여 데이터 뉴런(Vn)와 라 벨 뉴런(Ln)을 재구축한다. 이제 컴퓨터 장치는 재구축된 데이터 뉴런(Vn)와 라벨 뉴런(Ln)의 출력값으로 다시 숨겨진 뉴런(Hn)을 재구축할 수 있다. 컴퓨터 장치는 이와 같은 재구축 과정을 반복하면서 학습된 ClassRBM을 생성한다. 각각의 뉴런을 계산할 때 컴퓨터 장치는 계산에 사용되는 뉴런의 값과 뉴런과 뉴런간의 가중치의 곱을 축적하고, 축적된 값을 의사 난수 발생기(Pseudo Random Number Generator)에서 생성된 임의의 값과 비교하여 축적값이 더 클 경우 해당 뉴런의 값을 1, 작을 경우 0으로 발현시킨다. 도 3(B)는 ClassRBM 이용한 추론 과정에 대한 예이다. 도 3(B)는 도 3(A)과 같은 학습 과정을 거쳐서 학습된 ClassRBM이 마련된 것을 전제한다. 컴퓨터 장치는 입력 데이터를 가시층의 데이터 뉴런(Vp)에 입력하고, 데이터 뉴런(Vp)의 출력값에 가중치를 부가하여 숨겨진 뉴런(Hp)의 값을 생성한다. 또 컴퓨터 장치는 숨겨진 뉴런(Hp)의 값을 기준으로 가중치를 부여하여 재구축된 데이터 뉴런(Vn)와 라벨 뉴런(Ln)의 값을 생성할 수 있다. 추론 과 정을 통해서 컴퓨터 장치는 현재 입력받은 데이터가 어떤 라벨에 속하는지 추론을 하거나 데이터의 손실된 부분 을 복구하거나 데이터에 추가된 노이즈를 제거하는 과정을 수행할 수 있다. 도 3에서 숨겨진 뉴런은 데이터 뉴런과 라벨 뉴런을 모두 이용하여 생성되고, 재구축된다. 도 3과 같은 학습 알 고리즘을 대조 발산 알고리즘이라고 한다. 도 4는 ClassRBM을 학습하는 과정 및 ClassRBM 이용한 추론 과정에 대한 다른 예이다. 도 4는 도 3과 다른 방식 으로 뉴런의 가중치를 학습한다. 도 4에서 추가적으로 설명하지 않는 내용은 도 3의 과정이나 내용과 동일하다. 도 4(A)는 ClassRBM을 학습하는 과정에 대한 예이다. 컴퓨터 장치는 훈련 데이터를 입력받는다. 컴퓨터 장치는 훈련 데이터를 데이터 뉴런(V)과 라벨 뉴런(L)에 입력하여 초기 데이터 뉴런(Vp) 및 라벨 뉴런(Lp)을 구축한다. 컴퓨터 장치는 데이터 뉴런(Vp)의 출력값에 가중치 W를 부여한 값을 사용하여 숨겨진 뉴런(Hp)을 구축한다. 즉 도 3(A)와 달리 숨겨진 뉴런(Hp)을 구축하는데 라벨 뉴런(Lp)을 이용하지 않고, 데이터 뉴런(Vp)만을 이용한다. 또 컴퓨터 장치는 숨겨진 뉴런(Hp)의 값에 각각 가중치 W 및 U를 부여하여 데이터 뉴런(Vn)와 라벨 뉴런(Ln)을 재구축한다. 컴퓨터 장치는 재구축된 데이터 뉴런(Vn)의 출력값에 가중치를 부여하여 다시 숨겨진 뉴런(Hn)을 재구축할 수 있 다. 결국 숨겨진 뉴런을 구축하는 과정 및 재구축 과정에서 데이터 뉴런의 출력값만을 이용한다. 컴퓨터 장치는 이와 같은 재구축 과정을 반복하면서 학습된 ClassRBM을 생성한다. 도 4(B)는 ClassRBM 이용한 추론 과정에 대한 예이다. 추론 과정은 도 3(B)와 동일하다. 정리하면, 도 4의 ClassRBM은 추론 과정은 도 3의 ClassRBM과 동일하나, 학습 과정에서 숨겨진 뉴런(H)을 계산 할 때 도 3의 ClassRBM과 달리 라벨 뉴런(L)을 사용하지 않는다. 이는 학습 과정에서 숨겨진 뉴런 계산과 숨겨 진 뉴런 재계산 과정에서 모두 동일하게 적용된다. 숨겨진 뉴런(H)을 계산할 때 라벨 뉴런(L)을 사용하지 않는 이유는 학습 시에 데이터 뉴런(V)과 라벨 뉴런(L)을 모두 사용하여 숨겨진 뉴런(H)을 사용하다가, 데이터 뉴런 (V)만을 사용하여 라벨 뉴런(L)을 추론을 하게 되었을 때의 입력 조건의 차이로 인해 학습이 진행됨에 따라 추 론 정확도가 조금씩 떨어지는 현상을 해결하기 위함이다. 다만, 숨겨진 뉴런(H)을 이용하여 데이터 뉴런(V)과 라벨 뉴런(L)을 재구축할 때에는 도 3의 알고리즘과 차이가 없다. 도 4에서 숨겨진 뉴런은 데이터 뉴런만을 모두 이용하여 생성되고, 재구축된다. 도 4와 같은 학습 알고리즘을 비대칭 대조 발산 알고리즘이라고 한다. 도 5는 라벨 뉴런과 숨겨진 뉴런 사이의 가중치 수정치에 대한 예이다. ClassRBM에서 가중치 수정 학습(Weight Update) 과정에서 ClassRBM은 입력된 또는 재구축된 데이터 뉴런(V) 또 는 라벨 뉴런(L)을 사용하여 숨겨진 뉴런(H)을 계산하고, 다시 계산된 숨겨진 뉴런(H)을 통해 데이터 뉴런(V)과 라벨 뉴런(L)을 재구축하는 과정을 반복한다. 최초 생성된 데이터 뉴런을 VP라고 하고, 데이터 뉴런에 속한 노드를 vp라고 한다. 최초 생성된 라벨 뉴런을 LP 라고 하고, 라벨 뉴런에 속한 노드를 lP라고 한다. 데이터 뉴런 VP을 기반으로 계산된 숨겨진 뉴런을 HP라고 하 고, 숨겨진 뉴런에 속한 노드를 hP라고 한다. 숨겨진 뉴런 HP을 기반으로 재구축된 데이터 뉴런을 Vn라고 하고, 재구축된 데이터 뉴런에 속한 노드를 vn라고 한다. 숨겨진 뉴런 HP을 기반으로 재구축된 라벨 뉴런을 Ln라고 하 고, 재구축된 라벨 뉴런에 속한 노드를 ln라고 한다. 그리고 데이터 뉴런을 Vn를 기반으로 재구축된 숨겨진 뉴런 을 Hn라고 하고, 재구축된 숨겨진 뉴런에 속한 노드를 hn라고 한다. 데이터 뉴런-숨겨진 뉴런 간 가중치를 W라고 하고, W의 수정치를 ΔW라고 한다. 라벨 뉴런-숨겨진 뉴런 간 가중 치를 U라고 하고, U의 수정치를 ΔU라고 한다. ΔW는 아래 수학식 1을 이용하여 결정할 수 있다. 수학식 1"}
{"patent_id": "10-2018-0042054", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서 i는 데이터 뉴런에 속한 노드의 식별자(번호)이고, j는 숨겨진 뉴런에 속한 노드의 식별자에 해당 한다. η(eta)는 학습률을 나타내는 파라미터이다. 학습률은 한 번의 학습에서 매개변수 값을 얼마나 갱신해야 하는지를 정하는 값이다. ΔW는 도 3의 ClassRBM이나 도 4의 ClassRBM에서 동일하게 연산된다. 도 5(A)는 도 3의 ClassRBM에서 라벨 뉴런과 숨겨진 뉴런 사이의 가중치 수정치를 계산하는 예이다. 도 5(A)는 대조 발산 알고리즘에 대한 예이다. 대조 발산 알고리즘에서 ΔU는 아래 수학식 2를 이용하여 결정할 수 있다. 숨겨진 뉴런을 재구축하는 과정에서 숨겨진 뉴런 hP와 재계산된 숨겨진 뉴런 hn를 모두 사용하여 ΔU을 결정한다. 수학식 2"}
{"patent_id": "10-2018-0042054", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 2에서 i는 라벨 뉴런에 속한 노드의 식별자(번호)이고, j는 숨겨진 뉴런에 속한 노드의 식별자에 해당한 다. η(eta)는 학습률을 나타내는 파라미터이다. 도 5(B)는 도 4의 ClassRBM에서 라벨 뉴런과 숨겨진 뉴런 사이의 가중치 수정치를 계산하는 예이다. 도 5(B)는 비대칭 대조 발산 알고리즘에 대한 예이다. 비대칭 대조 발산 알고리즘에서 ΔU는 아래 수학식 3을 이용하여 결 정할 수 있다. 숨겨진 뉴런을 재구축하는 과정에서 숨겨진 뉴런 hP만 사용하여 ΔU을 결정한다. 수학식 3 수학식 3에서 i는 라벨 뉴런에 속한 노드의 식별자(번호)이고, j는 숨겨진 뉴런에 속한 노드의 식별자에 해당한 다. η(eta)는 학습률을 나타내는 파라미터이다. 이하 도 4의 ClassRBM을 학습하는 장치에 대하여 설명한다. 즉 전술한 컴퓨터 장치에 해당하는 장치이다. 컴퓨 터 장치는 실제 제품으로 구현되면 하나의 회로 기판, 온 칩 등과 같은 형태로 구현될 수도 있다. 도 6은 ClassRBM을 학습하는 장치의 구조에 대한 예이다. ClassRBM을 학습하는 장치는 서비스를 위한 ClassRBM을 마련한다. ClassRBM을 학습하는 장치와 인공지능 서비스를 수행하는 장치는 서로 다를 수 있다. ClassRBM을 학습하는 장치는 제어부 및 연산부를 포함한다. 연산부는 축적-발현부 및 가 중치 수정부를 포함한다. 제어부는 외부에서 들어오는 제어 신호(D1)을 수신한다. 제어부는 D1에 따라 일정한 정보를 연산부에 전달한다. 제어부는 제어 신호 D2에 따라 축적-발현부를 제어하고, 제어 신호 D3에 따라 가중치 수정부를 제어한다. 제어 신호 D1은 하나 이상의 제어 신호를 포함한다. D1은 연산부가 대조 발산 알고리즘 또는 비대칭 대조 발산 알고리즘을 사용하여 ClassRBM을 학습하게 하는 신호 를 포함할 수 있다. 도 7은 ClassRBM을 학습하는 연산부의 구조에 대한 예이다. 도 7(A)는 축적-발현부에 대한 예이다. 축적- 발현부는 가중치 저장부, 축적부 및 발현부를 포함한다. 제어 신호 D2는 가중치 저장부 가 축적부에 전달하는 신호를 조절한다. D2는 대조 발산 알고리즘 또는 비대칭 대조 발산 알고리즘을 선택하는 정보를 포함할 수 있다. 가중치 저장부는 뉴런 사이의 가중치를 저장한다. 가중치 저장부는 메모리 기능을 하는 소자들로 구 성된 요소이다. 가중치 저장부는 축적-발현부의 외부에 별도로 존재하는 저장 매체일 수 있다. 전술한 바와 같이 숨겨진 뉴런부의 값과 가중치의 곱을 축적할 때 대조 발산 알고리즘은 데이터 뉴런과 라벨 뉴 런을 모두 사용하여 숨겨진 뉴런의 값을 계산한다. 숨겨진 뉴런의 값과 가중치의 곱을 축적할 때 비대칭 대조 발산 알고리즘은 데이터 뉴런의 값만을 사용하여 숨겨진 뉴런의 값을 계산한다. 가중치 저장부는 외부 제어 신호 D2의 값에 따라 저장된 가중치의 값을 축적부에 보낸다. 가중치 저 장부는 학습 방식이 대조 발산 알고리즘으로 선택된 경우 데이터 뉴런-숨겨진 뉴런과 라벨 뉴런-숨겨진 뉴 런 사이의 가중치를 모두 보낸다. 가중치 저장부는 비대칭 대조 발산 알고리즘으로 선택된 경우 데이터 뉴런-숨겨진 뉴런 사이의 가중치만을 보낸다. 축적부는 숨겨진 뉴런부의 값과 가중치의 곱을 축적한다. 축적부는 축적한 값을 발현부로 보내 어 숨겨진 뉴런의 값을 발현시킨다. 도 7(B)는 가중치 수정부에 대한 예이다. 가중치 수정부는 오차 계산부, 가중치 수정치 계산부 및 가중치 저장부를 포함한다. 가중치 저장부는 도 7(A)에서 설명한 구성과 동일한 구성이다. 외부 제어 신호 D3는 오차 계산부가 오차를 계산하고 가중치 수정치 계산부가 가중치의 수정치를 계 산하는데 필요한 정보를 포함한다. D3는 대조 발산 알고리즘 또는 비대칭 대조 발산 알고리즘을 선택하는 정보 를 포함할 수 있다. 오차 계산부는 훈련 과정에서 구축된 뉴런의 값과 훈련 데이터의 값에 대한 오차를 계산한다. 가중치 수정 치 계산부는 오차에 따라 가중치에 대한 수정치를 연산한다. 가중치 수정치 계산부는 가중치의 수정 치를 가중치 저장부에 보낸다. 가중치 저장부는 전달된 수정치를 반영하여 가중치를 갱신한다. 가중 치 저장부는 가중치 수정부에 속하지 않는 별도의 저장 매체일 수도 있다. 가중치 수정치 계산부는 데이터 뉴런-숨겨진 뉴런 간 가중치에 대한 ΔW를 상기 수학식 1을 사용하여 결정 할 수 있다. 대조 발산 알고리즘의 경우, 가중치 수정치 계산부는 라벨 뉴런-숨겨진 뉴런 간 가중치의 수 정치 ΔU를 상기 수학식 2를 사용하여 결정할 수 있다. 비대칭 대조 발산 알고리즘의 경우, 가중치 수정치 계산 부는 라벨 뉴런-숨겨진 뉴런 간 가중치의 수정치 ΔU를 상기 수학식 3을 사용하여 결정할 수 있다. 가중치수정치 계산부는 D3에 따라 ΔU를 어떤 방식으로 결정할지 선택한다. 도 8은 ClassRBM을 탑재한 장치의 구조에 대한 예이다. ClassRBM을 탑재한 장치는 훈련 데이터를 이용하여 ClassRBM을 학습하여 마련할 수 있다. 또 ClassRBM을 탑재한 장치는 학습된 ClassRBM을 이용하여 서비스를 제공 하는 장치에 해당한다. 도 8에서 설명하는 장치는 전술한 과정을 이용하여 ClassRBM을 구축할 수 있다. 도 8(A)는 ClassRBM을 생성하는 PC와 같은 장치에 대한 예이다. PC는 훈련데이터 DB로부터 훈련 데이터를 수신한다. PC는 물리적으로 연결된 저장 매체 등을 통해서도 훈련 데이터를 수신할 수 있다. PC는 ClassRBM을 생성하면서 전술한 비대칭 대조 발산 알고리즘을 사용하여 라벨 뉴런-숨겨진 뉴런 간 가 중치의 수정치 ΔU를 연산할 수 있다. PC는 데이터 뉴런만을 이용하여 숨겨진 뉴런을 구축하고 재구축할 수 있다. 도 8(B)는 ClassRBM을 생성하는 서버와 같은 장치에 대한 예이다. 서버는 클라이언트 장치로부터 훈련 데이터를 수신한다. 물론 서버는 네트워크에 연결된 다른 객체로부터 훈련 데이터를 수신할 수도 있다. 서버는 ClassRBM을 생성하면서 전술한 비대칭 대조 발산 알고리즘을 사용하여 라벨 뉴런-숨겨진 뉴런 간 가중치의 수정치 ΔU를 연산할 수 있다. 서버는 데이터 뉴런만을 이용하여 숨겨진 뉴런을 구축하고 재구 축할 수 있다. ClassRBM은 메모리 및 연산 소자로 구성되는 회로 내지 칩셋에 마련될 수도 있다. 도 8(C)는 물리적인 구성을 제한하지 않고, ClassRBM을 탑재한 장치에 대한 구성을 도시한 예이다. 도 8(C)는 PC, 서버 또 는 인공지능 탑재된 회로 기판(또는 칩) 등의 구성일 수 있다. ClassRBM을 탑재한 장치는 입력장치, 연산장치 및 저장장치를 포함한다. 나아가 ClassRBM을 탑재한 장치는 출력장치를 더 포함할 수도 있다. 입력 장치는 훈련 데이터 내지 입력 데이터를 입력받는다. 입력 장치는 네트워크로 데이터를 수신하 는 통신 장치 내지 인터페이스 장치일 수 있다. 또 입력 장치는 유선 네트워크로 데이터를 수신하는 인터 페이스 장치일 수도 있다. 한편 입력 장치는 외부 제어 신호를 수신할 수도 있다. 저장장치는 기본적으로 ClassRBM 모델을 저장할 수 있다. 저장장치는 데이터를 저장할 수 있는 다양 한 매체로 구현될 수 있다. 저장장치는 학습전의 초기 ClassRBM을 저장하고, 학습 과정에 사용되는 다양한 정보 및 파라미터를 저장하고, 학습 완료된 ClassRBM을 저장할 수 있다. 연산장치는 훈련 데이터를 이용하여 ClassRBM을 학습하여 마련한다. 연산장치는 훈련 데이터를 기반 으로 데이터 뉴런 및 라벨 뉴런을 구축하고, 데이터 뉴런의 출력값을 기반으로 숨겨진 뉴런을 구축할 수 있다. 또 연산장치는 숨겨진 뉴런의 출력값을 기반으로 데이터 뉴런 및 라벨 뉴런을 재구축할 수 있다. 연산장치 는 전술한 과정에 따라 ClassRBM을 학습할 수 있다. 따라서 연산장치는 데이터 뉴런만을 이용하여 숨 겨진 뉴런을 구축하고 재구축할 수 있다. 연산장치는 ClassRBM을 생성하면서 전술한 비대칭 대조 발산 알 고리즘을 사용하여 라벨 뉴런-숨겨진 뉴런 간 가중치의 수정치 ΔU를 연산할 수 있다. 연산장치는 입력 데이터를 학습된 ClassRBM에 입력하여 일정한 결과값을 도출할 수 있다. 연산장치는 일정한 명령 내지 프로그램을 구동하여 데이터를 처리하는 장치에 해다한다. 연산장치는 명령 내지 정보를 임시 저장하는 메모리(버퍼) 및 연산 처리를 수행하는 프로세서로 구현될 수 있다. 프로세서 는 장치의 종류에 따라 CPU, AP, FPGA 등으로 구현될 수 있다. 출력장치는 외부로 필요한 데이터를 송신하는 통신 장치일 수 있다. 출력장치는 학습한 ClassRBM이 도출한 결과값을 외부로 전송할 수 있다. 경우에 따라서 출력장치는 ClassRBM 학습과정이나, 학습한 ClassRBM이 도출할 결과값을 화면으로 출력하는 장치일 수도 있다. 또한, 상술한 바와 같은 ClassRBM 학습 방법, ClassRBM을 이용한 추론 방법 및 ClassRBM에 대한 운용 방법은 컴 퓨터에서 실행될 수 있는 실행가능한 알고리즘을 포함하는 프로그램(또는 어플리케이션)으로 구현될 수 있다. 상기 프로그램은 비일시적 판독 가능 매체(non-transitory computer readable medium)에 저장되어 제공될 수 있다.비일시적 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체가 아니 라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 구체적으로는, 상 술한 다양한 어플리케이션 또는 프로그램들은 CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등과 같은 비일시적 판독 가능 매체에 저장되어 제공될 수 있다. 본 실시례 및 본 명세서에 첨부된 도면은 전술한 기술에 포함되는 기술적 사상의 일부를 명확하게 나타내고 있 는 것에 불과하며, 전술한 기술의 명세서 및 도면에 포함된 기술적 사상의 범위 내에서 당업자가 용이하게 유추 할 수 있는 변형 예와 구체적인 실시례는 모두 전술한 기술의 권리범위에 포함되는 것이 자명하다고 할 것이다."}
{"patent_id": "10-2018-0042054", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 RBM 구조를 도시한 예이다. 도 2는 ClassRBM 구조를 도시한 예이다. 도 3은 ClassRBM을 학습하는 과정 및 ClassRBM 이용한 추론 과정에 대한 예이다. 도 4는 ClassRBM을 학습하는 과정 및 ClassRBM 이용한 추론 과정에 대한 다른 예이다. 도 5는 라벨 뉴런과 숨겨진 뉴런 사이의 가중치 수정치에 대한 예이다. 도 6은 ClassRBM을 학습하는 장치의 구조에 대한 예이다. 도 7은 ClassRBM을 학습하는 연산부의 구조에 대한 예이다. 도 8은 ClassRBM을 탑재한 장치의 구조에 대한 예이다."}
