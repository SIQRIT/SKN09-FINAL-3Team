{"patent_id": "10-2025-0007199", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0034922", "출원번호": "10-2025-0007199", "발명의 명칭": "동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치 및 그 동작", "출원인": "주식회사 파일러", "발명자": "박동찬"}}
{"patent_id": "10-2025-0007199", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "저장부; 및프로세서;를 포함하고,상기 프로세서는,크로스-어텐션 모델을 이용해, 복수의 동영상 클립 및 제1 텍스트 쿼리로부터 복수의 제1 동영상 특징을 획득하고,트랜스포머 인코더를 이용해, 상기 복수의 제1 동영상 특징 및 제1 중요도 토큰(Saliency Token)으로부터 복수의 제2 동영상 특징, 제2 중요도 토큰 및 상기 제1 텍스트 쿼리에 대한 상기 복수의 동영상 클립 중 적어도 하나의 동영상 클립의 적어도 하나의 제1 중요도(Saliency Score)를 획득하고,상기 크로스-어텐션 모델을 이용해, 상기 복수의 동영상 클립 및 제2 텍스트 쿼리로부터 복수의 제3 동영상 특징을 획득하고,상기 트랜스포머 인코더를 이용해, 상기 복수의 제3 동영상 특징 및 상기 제1 중요도 토큰으로부터 복수의 제4동영상 특징, 제3 중요도 토큰 및 상기 제2 텍스트 쿼리에 대한 상기 적어도 하나의 동영상 클립의 적어도 하나의 제2 중요도를 획득하고,상기 제1 중요도는 상기 제2 중요도보다 크고, 상기 제1 중요도가 증가하고 상기 제2 중요도는 감소하도록 상기크로스-어텐션 모델, 상기 트랜스포머 인코더 및 상기 제1 중요도 토큰을 수정하며,상기 제1 텍스트 쿼리는 상기 적어도 하나의 동영상 클립과 양의 쌍이며,상기 제2 텍스트 쿼리는 상기 적어도 하나의 동영상 클립과 음의 쌍인, 동영상 장면 검색 및 하이라이트 검출중 적어도 하나를 학습하기 위한 전자 장치."}
{"patent_id": "10-2025-0007199", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 프로세서는, 상기 크로스-어텐션 모델을 이용해,상기 복수의 동영상 클립에 대한 복수의 동영상 특징으로부터 복수의 동영상 쿼리를 획득하고,상기 제1 텍스트 쿼리에 대한 제1 텍스트 쿼리 특징으로부터 제1 텍스트 키를 획득하고,상기 제1 텍스트 쿼리 특징으로부터 제1 텍스트 벨류를 획득하고,상기 복수의 동영상 쿼리, 상기 제1 텍스트 키 및 상기 제1 텍스트 벨류를 이용해 복수의 제1 어텐션 점수를 획득하고, 상기 복수의 제1 어텐션 점수로부터 복수의 제1 가중치를 획득하고, 상기 복수의 제1 가중치 및 상기 복수의 동영상 특징을 각각 곱함으로써 상기 복수의 제1 동영상 특징을 획득하는, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치."}
{"patent_id": "10-2025-0007199", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,공개특허 10-2025-0034922-3-상기 프로세서는, 상기 크로스-어텐션 모델을 이용해,상기 복수의 동영상 클립에 대한 복수의 동영상 특징으로부터 복수의 동영상 쿼리를 획득하고,상기 제2 텍스트 쿼리에 대한 제2 텍스트 쿼리 특징으로부터 제2 텍스트 키를 획득하고,상기 제2 텍스트 쿼리 특징으로부터 제2 텍스트 벨류를 획득하고,상기 복수의 동영상 쿼리, 상기 제2 텍스트 키 및 상기 제2 텍스트 벨류를 이용해 복수의 제2 어텐션 점수를 획득하고,상기 복수의 제2 어텐션 점수로부터 복수의 제2 가중치를 획득하고, 상기 복수의 제2 가중치 및 상기 복수의 동영상 특징을 각각 곱함으로써 상기 복수의 제3 동영상 특징을 획득하는, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치."}
{"patent_id": "10-2025-0007199", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 프로세서는,제1 완전 연결 층을 통해, 상기 복수의 제2 동영상 특징 중 상기 적어도 하나의 동영상 클립과 대응하는 적어도하나의 동영상 특징으로부터 적어도 하나의 제1 출력값을 획득하고,제2 완전 연결 층을 통해, 상기 제2 중요도 토큰으로부터 제2 출력값을 획득하고,상기 적어도 하나의 제1 출력값과 상기 제2 출력값을 내적하여 상기 적어도 하나의 제1 중요도를 획득하는, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치."}
{"patent_id": "10-2025-0007199", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 프로세서는,제1 완전 연결 층을 통해, 상기 복수의 제4 동영상 특징 중 상기 적어도 하나의 동영상 클립과 대응하는 적어도하나의 동영상 특징으로부터 적어도 하나의 제1 출력값을 획득하고,제2 완전 연결 층을 통해, 상기 제3 중요도 토큰으로부터 제2 출력값을 획득하고, 상기 적어도 하나의 제1 출력값과 상기 제2 출력값을 내적하여 상기 적어도 하나의 제2 중요도를 획득하는, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치."}
{"patent_id": "10-2025-0007199", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "(a) 크로스-어텐션 모델을 이용해, 복수의 동영상 클립 및 제1 텍스트 쿼리로부터 복수의 제1 동영상 특징을 획득하는 단계;(b) 트랜스포머 인코더를 이용해, 상기 복수의 제1 동영상 특징 및 제1 중요도 토큰(Saliency Token)으로부터복수의 제2 동영상 특징, 제2 중요도 토큰 및 상기 제1 텍스트 쿼리에 대한 상기 복수의 동영상 클립 중 적어도하나의 동영상 클립의 적어도 하나의 제1 중요도(Saliency Score)를 획득하는 단계;(c) 상기 크로스-어텐션 모델을 이용해, 상기 복수의 동영상 클립 및 제2 텍스트 쿼리로부터 복수의 제3 동영상특징을 획득하는 단계;(d) 상기 트랜스포머 인코더를 이용해, 상기 복수의 제3 동영상 특징 및 상기 제1 중요도 토큰으로부터 복수의제4 동영상 특징, 제3 중요도 토큰 및 상기 제2 텍스트 쿼리에 대한 상기 적어도 하나의 동영상 클립의 적어도하나의 제2 중요도를 획득하는 단계; 및공개특허 10-2025-0034922-4-(e) 상기 제1 중요도는 상기 제2 중요도보다 크고, 상기 제1 중요도가 증가하고 상기 제2 중요도는 감소하도록상기 크로스-어텐션 모델, 상기 트랜스포머 인코더 및 상기 제1 중요도 토큰을 수정하는 단계를 포함하며,상기 제1 텍스트 쿼리는 상기 적어도 하나의 동영상 클립과 양의 쌍이며,상기 제2 텍스트 쿼리는 상기 적어도 하나의 동영상 클립과 음의 쌍인, 동영상 장면 검색 및 하이라이트 검출중 적어도 하나를 학습하기 위한 전자 장치의 동작 방법."}
{"patent_id": "10-2025-0007199", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 (a) 단계는,(a1) 상기 복수의 동영상 클립에 대한 복수의 동영상 특징으로부터 복수의 동영상 쿼리를 획득하는 단계;(a2) 상기 제1 텍스트 쿼리에 대한 제1 텍스트 쿼리 특징으로부터 제1 텍스트 키를 획득하는 단계;(a3) 상기 제1 텍스트 쿼리 특징으로부터 제1 텍스트 벨류를 획득하는 단계;(a4) 상기 복수의 동영상 쿼리, 상기 제1 텍스트 키 및 상기 제1 텍스트 벨류를 이용해 복수의 제1 어텐션 점수를 획득하는 단계;(a5) 상기 복수의 제1 어텐션 점수로부터 복수의 제1 가중치를 획득하는 단계; 및(a6) 상기 복수의 제1 가중치 및 상기 복수의 동영상 특징을 각각 곱함으로써 상기 복수의 제1 동영상 특징을획득하는 단계를 포함하는, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치의동작 방법."}
{"patent_id": "10-2025-0007199", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6 항에 있어서,상기 (c) 단계는,(c1) 상기 복수의 동영상 클립에 대한 복수의 동영상 특징으로부터 복수의 동영상 쿼리를 획득하는 단계;(c2) 상기 제2 텍스트 쿼리에 대한 제2 텍스트 쿼리 특징으로부터 제2 텍스트 키를 획득하는 단계;(c3) 상기 제2 텍스트 쿼리 특징으로부터 제2 텍스트 벨류를 획득하는 단계;(c4) 상기 복수의 동영상 쿼리, 상기 제2 텍스트 키 및 상기 제2 텍스트 벨류를 이용해 복수의 제2 어텐션 점수를 획득하는 단계;(c5) 상기 복수의 제2 어텐션 점수로부터 복수의 제2 가중치를 획득하는 단계; 및(c6) 상기 복수의 제2 가중치 및 상기 복수의 동영상 특징을 각각 곱함으로써 상기 복수의 제3 동영상 특징을획득하는 단계를 포함하는, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치의동작 방법."}
{"patent_id": "10-2025-0007199", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6 항에 있어서,상기 (b) 단계는,(b1) 제1 완전 연결 층을 통해, 상기 복수의 제2 동영상 특징 중 상기 적어도 하나의 동영상 클립과 대응하는적어도 하나의 동영상 특징으로부터 적어도 하나의 제1 출력값을 획득하는 단계;(b2) 제2 완전 연결 층을 통해, 상기 제2 중요도 토큰으로부터 제2 출력값을 획득하는 단계; 및공개특허 10-2025-0034922-5-(b3) 상기 적어도 하나의 제1 출력값과 상기 제2 출력값을 내적하여 상기 적어도 하나의 제1 중요도를 획득하는단계를 포함하는, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치의 동작 방법."}
{"patent_id": "10-2025-0007199", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제6 항에 있어서,상기 (d) 단계는,(d1) 제1 완전 연결 층을 통해, 상기 복수의 제4 동영상 특징 중 상기 적어도 하나의 동영상 클립과 대응하는적어도 하나의 동영상 특징으로부터 적어도 하나의 제1 출력값을 획득하는 단계;(d2) 제2 완전 연결 층을 통해, 상기 제3 중요도 토큰으로부터 제2 출력값을 획득하는 단계; 및(d3) 상기 적어도 하나의 제1 출력값과 상기 제2 출력값을 내적하여 상기 적어도 하나의 제2 중요도를 획득하는단계를 포함하는, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치의 동작 방법."}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치가 개시된다. 이 전자 장치는 저장부 및 프로세서를 포함하고, 프로세서는, 크로스-어텐션 모델을 이용해, 복수의 동영상 클립 및 제1 텍스트 쿼리로부터 복수의 제1 동영상 특징을 획득하고, 트랜스포머 인코더를 이용해, 복수의 제1 동영상 특징 및 제1 (뒷면에 계속)"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 전자 장치 및 그 동작 방법에 관한 것이다. 보다 구체적으로, 본 발명은 동영상 장면 검색 및 하이라 이트 검출 중 적어도 하나를 위한 전자 장치 및 그 동작 방법에 관한 것이다. 추가적으로, 본 발명은 동영상 장 면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장치 및 그 동작 방법에 관한 것이다. 여기 서, 동영상 장면 검색(Video Moment Retrieval)은 동영상에서 주어진 텍스트 쿼리와 관련된 순간을 찾는 태스크 를 말한다. 한편, 하이라이트 검출(Highlight Detection)은 주어진 비디오에서 주어진 텍스트 쿼리와 관련하여 각 시점 또는 시구간의 중요도를 측정하는 것을 목적으로 한다."}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "디지털 장치 및 플랫폼의 발전과 함께 동영상 데이터의 양이 폭발적으로 증가하고 있다. 그러나 동영상 데이터 의 양적 증가와 함께 점점 더 많은 시청자들은 동영상에서 원하는 장면만을 검색하거나 중요 장면만을 보길 원 하고 있다. 따라서 동영상 장면 검색 및 하이라이트 검출 기술에 대한 연구들이 이루어지고 있으나 아직 기술의 성능 개선이 필요하다. 예를 들어, 기존의 연구들은 텍스트 쿼리의 영향을 제대로 모델링하지 못했다. 예를 들 어, 텍스트 쿼리의 관련성이 동영상 장면 검색 결과 및 하이라이트 검출 결과에 큰 영향을 미치지 못했다. 따라 서 텍스트 쿼리의 영향력을 제대로 모델링함으로써 성능이 향상된 동영상 장면 검색 및 하이라이트 검출 기술에 대한 연구가 필요하다."}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는 향상된 성능의 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 위한 전자 장치 및 그 동작 방법을 제공하는 것이다. 본 발명이 해결하고자 하는 과제는 향상된 성능의 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습 하기 위한 전자 장치 및 그 동작 방법을 제공하는 것이다."}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 위한 전자 장치는 저장 부 및 프로세서를 포함하고, 상기 프로세서는, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응 하는 복수의 제1 동영상 특징을 획득하고, 텍스트 쿼리로부터 텍스트 쿼리 특징을 획득하고, 상기 복수의 제1 동영상 특징 및 상기 텍스트 쿼리 특징으로부터 복수의 가중치를 획득하고, 상기 복수의 가중치 및 상기 복수의 제1 동영상 특징으로부터 복수의 제2 동영상 특징을 획득하고, 인코더를 이용해 상기 복수의 제2 동영상 특징으 로부터 복수의 제3 동영상 특징을 획득하고, 디코더를 이용해 상기 복수의 제3 동영상 특징 및 시간 쿼리로부터 복수의 제4 동영상 특징을 획득하고, 상기 복수의 제4 동영상 특징을 이용해 상기 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하나를 선택할 수 있다. 본 발명의 일 실시예에 따르면, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 위한 전자 장치의 동작 방법으로서, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 동영상 특징을 획 득하는 단계, 텍스트 쿼리로부터 텍스트 쿼리 특징을 획득하는 단계, 상기 복수의 제1 동영상 특징 및 상기 텍 스트 쿼리 특징으로부터 복수의 가중치를 획득하는 단계, 상기 복수의 가중치 및 상기 복수의 제1 동영상 특징 으로부터 복수의 제2 동영상 특징을 획득하는 단계, 인코더를 이용해 상기 복수의 제2 동영상 특징으로부터 복 수의 제3 동영상 특징을 획득하는 단계, 디코더를 이용해 상기 복수의 제3 동영상 특징 및 시간 쿼리로부터 복 수의 제4 동영상 특징을 획득하는 단계, 및 상기 복수의 제4 동영상 특징을 이용해 상기 동영상의 서로 다른 복 수의 시점 또는 시구간 중 적어도 하나를 선택하는 단계;를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장 치는, 저장부 및 프로세서를 포함하고, 상기 프로세서는, 크로스-어텐션 모델 및 인코더를 이용해 동영상, 상기 동영상과 양의 쌍인 제1 텍스트 쿼리, 및 중요도 토큰(Saliency Token)으로부터 복수의 제1 동영상 특징, 상기 제1 텍스트 쿼리에 대한 상기 동영상의 제1 시점 또는 제1 시구간의 제1 중요도, 및 상기 제1 텍스트 쿼리에 대 한 상기 동영상의 제2 시점 또는 제2 시구간의 제2 중요도를 획득하고, 상기 크로스-어텐션 모델 및 인코더를 이용해 상기 동영상, 상기 동영상과 음의 쌍인 제2 텍스트 쿼리, 및 상기 중요도 토큰으로부터 복수의 제2 동영 상 특징 및 상기 제2 텍스트 쿼리에 대한 상기 동영상의 제3 중요도를 획득하고, 상기 제1 중요도는 상기 제2 중요도보다 크고, 상기 제1 중요도가 증가하고 상기 제2 중요도 및 상기 제3 중요도가 감소하도록 상기 크로스- 어텐션 모델, 상기 인코더, 및 상기 중요도 토큰을 수정할 수 있다. 본 발명의 일 실시예에 따르면, 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전자 장 치의 동작 방법은, 크로스-어텐션 모델 및 인코더를 이용해 동영상, 상기 동영상과 양의 쌍인 제1 텍스트 쿼리, 및 중요도 토큰으로부터 복수의 제1 동영상 특징, 상기 제1 텍스트 쿼리에 대한 상기 동영상의 제1 시점 또는 제1 시구간의 제1 중요도, 및 상기 제1 텍스트 쿼리에 대한 상기 동영상의 제2 시점 또는 제2 시구간의 제2 중 요도를 획득하는 단계, 상기 크로스-어텐션 모델 및 인코더를 이용해 상기 동영상, 상기 동영상과 음의 쌍인 제 2 텍스트 쿼리, 및 상기 중요도 토큰으로부터 복수의 제2 동영상 특징 및 상기 제2 텍스트 쿼리에 대한 상기 동 영상의 제3 중요도를 획득하는 단계, 및 상기 제1 중요도는 상기 제2 중요도보다 크고, 상기 제1 중요도가 증가 하고 상기 제2 중요도 및 상기 제3 중요도가 감소하도록 상기 크로스-어텐션 모델, 상기 인코더, 및 상기 중요 도 토큰을 수정하는 단계를 포함할 수 있다."}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 텍스트 쿼리와 동영상 클립들 사이의 크로스-어텐션을 통해 텍스트 쿼리가 동영 상 특징에 미치는 영향을 모델링할 수 있다. 따라서 텍스트 쿼리가 동영상 장면 검색 및 하이라이트 검출의 출 력 값에 영향을 미치게 할 수 있어 동영상 장면 검색 및 하이라이트 검출 성능이 향상될 수 있다. 본 발명의 일 실시예에 따르면, 동영상과 관련 없는 텍스트 쿼리에 대한 중요도가 감소하도록 모델들을 학습시 킴으로써 텍스트 쿼리에 따른 영향을 효과적으로 학습할 수 있다. 따라서 텍스트 쿼리가 동영상 장면 검색 및 하이라이트 검출의 출력 값에 미치는 영향을 보다 정확하게 모델링할 수 있어 동영상 장면 검색 및 하이라이트 검출 성능이 향상될 수 있다. 본 발명의 일 실시예에 따르면, 입력된 동영상 및 텍스트 쿼리에 따라 값이 변하는 중요도 토큰을 이용함으로써 입력-적응적으로(쿼리-의존적으로) 중요도 예측이 가능할 수 있다. 따라서 동영상 하이라이트 검출 성능이 향상 될 수 있다."}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 첨부된 도면을 참조하여 본 발명의 바람직한 실시예에 대한 동작 원리를 상세히 설명한다. 또한, 발명에 대한 실시예를 설명함에 있어 관련된 공지 기능 또는 구성에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명을 생략할 것이다. 그리고 하기에서 사용되는 용어들은 본 발명에서의 기능을 고려하여 정의된 용어들로써, 이는 사용자, 운용자의 의도 또는 관례 등에 따라 달라질 수 있다. 그러므 로 사용된 용어들의 정의는 본 명세서 전반에 걸친 내용 및 이에 상응한 기능을 토대로 해석되어야 할 것이다. 도 1은 본 발명의 일 실시예에 따른 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 위한 전자 장치의 개략적인 블록도이다. 도 1을 참조하면, 전자 장치는 저장부 및 프로세서를 포함할 수 있다. 저장부는 각종 데이터 및 프로그램을 저장할 수 있다. 예를 들어, 저장부는 인공지능 모델(MD, MD-4, 각각 도 2 및 도 4 참조)을 저장할 수 있다. 일 실시예에서, 저장부는 인공지능 모델에 입력되는 동영상 데이터 및 텍스트 쿼리 데이터를 저장할 수 있다. 저장부는 휘발성 메모리 및 비휘발성 메모리 중 적어도 하나를 포함할 수 있다. 예를 들어, 휘발성 메모리는 DRAM, SRAM, SDRAM, DDR SDRAM, FeRAM, MRAM, PRAM, PoRAM, 또는 ReRAM을 포함할 수 있다. 예를 들어, 비휘발성 메모리는 플래시 메모리, mask ROM, PROM, OTPROM, EPROM, EEPROM, 하드디스크, 또는 광디스크를 포함할 수 있다. 프로세서는 전자 장치의 동작 전반을 제어할 수 있다. 프로세서는 저장부를 제어할 수 있 다. 예를 들어, 프로세서는 중앙 처리 장치(CPU) 및 그래픽 처리 장치(GPU) 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 프로세서는, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 동영상 특징을 획득하고, 텍스트 쿼리로부터 텍스트 쿼리 특징을 획득하고, 복수의 제1 동영상 특징 및 텍 스트 쿼리 특징으로부터 복수의 가중치를 획득하고, 복수의 가중치 및 복수의 제1 동영상 특징으로부터 복수의 제2 동영상 특징을 획득하고, 인코더를 이용해 복수의 제2 동영상 특징으로부터 복수의 제3 동영상 특징을 획득 하고, 디코더를 이용해 복수의 제3 동영상 특징 및 시간 쿼리로부터 복수의 제4 동영상 특징을 획득하고, 복수의 제4 동영상 특징을 이용해 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하나를 선택할 수 있다. 일 실시예에서, 인코더는 트랜스포머의 인코더이고, 디코더는 트랜스포머의 디코더일 수 있다. 일 실시예에서, 시간 쿼리는 시구간을 정의하는 중앙 시점 및 길이를 포함할 수 있다. 일 실시예에서, 프로세서는, 복수의 가중치를 획득하기 위해, 복수의 제1 동영상 특징으로부터 동영상 쿼 리를 획득하고, 텍스트 쿼리 특징으로부터 텍스트 키를 획득하고, 텍스트 쿼리 특징으로부터 텍스트 벨류를 획 득하고, 동영상 쿼리, 텍스트 키, 및 텍스트 벨류를 이용해 복수의 어텐션 점수를 획득하고, 복수의 어텐션 점 수로부터 복수의 가중치를 획득할 수 있다. 일 실시예에서, 프로세서는, 복수의 제3 동영상 특징을 획득하기 위해, 인코더를 이용해 복수의 제2 동영 상 특징 및 제1 중요도 토큰으로부터 복수의 제3 동영상 특징 및 제2 중요도 토큰을 획득할 수 있다. 일 실시예에서, 프로세서는 복수의 제3 동영상 특징 및 제2 중요도 토큰으로부터 서로 다른 복수의 시점 또는 시구간의 중요도를 획득할 수 있다. 도 2는 도 1의 전자 장치에 사용되는 인공지능 모델의 개념도이다. 도 2를 참조하면, 인공지능 모델(MD)은 동영 상 인코더(EV), 텍스트 인코더(ET), 크로스-어텐션 모델(MD-1), 인코더(MD-2) 및 디코더(MD-3)를 포함할 수 있 다. 동영상 인코더(EV)는 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 동영상 특 징(V1A 내지 VtA)을 획득할 수 있다. 즉, 프로세서는 동영상 인코더(EV)를 이용해 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 동영상 특징(V1A 내지 VtA)을 획득할 수 있다. 예를 들어, 프로세서는 동영상 인코더(EV)를 이용해 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 동영 상 클립(V1 내지 Vt)으로부터 복수의 제1 동영상 특징(V1A 내지 VtA)을 획득할 수 있다. 일 실시예에서, 동영상 인코더(EV)는 합성 곱(convolution) 신경망 기반 모델 및 트랜스포머 기반 모델 중 적어도 하나를 포함할 수 있 다. 각각의 제1 동영상 특징(V1A 내지 VtA)은 벡터일 수 있다. 텍스트 인코더(ET)는 텍스트 쿼리(T)로부터 텍스트 쿼리 특징(Tq)을 획득할 수 있다. 즉, 프로세서는 텍스 트 인코더(ET)를 이용해 텍스트 쿼리(T)로부터 텍스트 쿼리 특징(Tq)을 획득할 수 있다. 일 실시예에서, 텍스트 인코더(ET)는 순환 신경망 기반 모델 및 트랜스포머 기반 모델 중 적어도 하나를 포함할 수 있다. 텍스트 쿼리가 n개의 토큰을 포함하는 경우, 텍스트 쿼리 특징(Tq)은 n개의 벡터를 포함할 수 있다. 크로스-어텐션 모델(MD-1)은 텍스트 쿼리 특징(Tq)을 이용해 복수의 제1 동영상 특징(V1A 내지 VtA)을 보정함으로 써 복수의 제1 동영상 특징(V1A 내지 VtA) 및 텍스트 쿼리 특징(Tq)으로부터 복수의 제2 동영상 특징(V1B 내지 VtB)을 획득할 수 있다. 즉, 프로세서는 크로스-어텐션 모델(MD-1)을 이용해 그리고 텍스트 쿼리 특징(Tq) 을 이용해 복수의 제1 동영상 특징(V1A 내지 VtA)을 보정함으로써 복수의 제1 동영상 특징(V1A 내지 VtA) 및 텍스 트 쿼리 특징(Tq)으로부터 복수의 제2 동영상 특징(V1B 내지 VtB)을 획득할 수 있다. 크로스-어텐션 모델(MD-1)은 텍스트 쿼리 특징(Tq)을 이용해 복수의 제1 동영상 특징(V1A 내지 VtA)을 보정함으로써 텍스트 쿼리(T)와 각 동영 상 클립(V1 내지 Vt) 사이의 관계를 모델링할 수 있다. 따라서 크로스-어텐션 모델(MD-1)은 장면 검색 및 하이라 이트 검출 성능을 향상시키는데 기여할 수 있다. 도 3은 도 1의 전자 장치에 사용되는 크로스-어텐션 모델의 개념도이다. 도 3을 참조하면, 크로스-어텐션 모델 (MD-1)은 복수의 제1 동영상 특징(V1A 내지 VtA) 및 텍스트 쿼리 특징(Tq)으로부터 복수의 가중치(W1 내지 Wt)를 획득하고, 복수의 가중치(W1 내지 Wt) 및 복수의 제1 동영상 특징(V1A 내지 VtA)으로부터 복수의 제2 동영상 특징 (V1B 내지 VtB)을 획득할 수 있다. 즉, 프로세서는 크로스-어텐션 모델(MD-1)을 이용해 복수의 제1 동영상 특징(V1A 내지 VtA) 및 텍스트 쿼리 특징(Tq)으로부터 복수의 가중치(W1 내지 Wt)를 획득하고, 복수의 가중치(W1 내지 Wt) 및 복수의 제1 동영상 특징(V1A 내지 VtA)으로부터 복수의 제2 동영상 특징(V1B 내지 VtB)을 획득할 수 있다.일 실시예에서, 프로세서는 복수의 가중치(W1 내지 Wt)를 획득하기 위해, 복수의 제1 동영상 특징(V1A 내지 VtA)으로부터 복수의 동영상 쿼리(query)(Q1 내지 Qt)를 획득하고, 텍스트 쿼리 특징(Tq)으로부터 텍스트 키 (key)(Kq)를 획득하고, 텍스트 쿼리 특징(Tq)으로부터 텍스트 벨류(value)(Vq)를 획득하고, 복수의 동영상 쿼리 (Q1 내지 Qt), 텍스트 키(Kq), 및 텍스트 벨류(Vq)를 이용해 복수의 어텐션 점수(AT-V)를 획득하고, 복수의 어텐 션 점수(AT-V)로부터 복수의 가중치(W1 내지 Wt)를 획득할 수 있다. 일 실시예에서, 프로세서는 예를 들어, 다층 퍼셉트론(Multi-layer Perceptron, MLP)과 같은 인공 신경망 을 이용해 복수의 제1 동영상 특징(V1A 내지 VtA)으로부터 복수의 동영상 쿼리(Q1 내지 Qt)를 획득할 수 있다. 또 한, 프로세서는 예를 들어, 다층 퍼셉트론과 같은 인공 신경망을 이용해 텍스트 쿼리 특징(Tq)으로부터 텍 스트 키(Kq) 및 텍스트 벨류(Vq)를 획득할 수 있다. 텍스트 쿼리(T)가 n개의 토큰을 포함하는 경우, 텍스트 쿼리 특징(Tq)은 n개의 벡터들을 포함할 수 있고, 텍스트 키(Kq)는 n개의 벡터들을 포함할 수 있고, 텍스트 벨류(Vq) 는 n개의 벡터들을 포함할 수 있다. 프로세서는 [수학식 1]을 이용해 복수의 동영상 쿼리(Q1 내지 Qt), 텍 스트 키(Kq) 및 텍스트 벨류(Vq)로부터 복수의 어텐션 점수(AT-V)를 획득할 수 있다. 수학식 1"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 Q는 동영상 쿼리(Q1 내지 Qt)이고, Kq는 텍스트 키이고, Vq는 텍스트 벨류이다. d는 동영상 쿼리(Q1 내지 Qt), 텍스트 키(Kq), 및 텍스트 벨류(Vq)의 차원이다. 소프트맥스 값이 텍스트 키(Kq)와 동영상 쿼리(Q1 내지 Q t)에 의존적이므로 동영상 클립들은 텍스트 키(Kq)와의 유사도에 기초해 표현될 수 있다. 프로세서는 예를 들어, 다층 퍼셉트론과 같은 인공 신경망을 이용해 어텐션 점수(AT-V)로부터 복수의 제1 동영상 특징(V1A 내지 VtA) 각각에 대한 복수의 가중치(W1 내지 Wt)를 획득할 수 있다. 프로세서는 예를 들어, 복수의 제1 동영상 특징(V1A 내지 VtA) 각각에 복수의 가중치(W1 내지 Wt)를 각각 곱 함으로써 복수의 제2 동영상 특징(V1B 내지 VtB)을 획득할 수 있다. 이하에서 도 2를 다시 참조한다. 인코더(MD-2)는 복수의 제2 동영상 특징(V1B 내지 VtB)으로부터 복수의 제3 동영상 특징(V1C 내지 VtC)을 획득할 수 있다. 즉, 프로세서는 인코더(MD-2)를 이용해 복수의 제2 동영상 특징(V1B 내지 VtB)으로부터 복수의 제 3 동영상 특징(V1C 내지 VtC)을 획득할 수 있다. 일 실시예에서, 인코더(MD-2)는 예를 들어 트랜스포머의 인코더 일 수 있다. 디코더(MD-3)는 복수의 제3 동영상 특징(V1C 내지 VtC) 및 시간 쿼리(Mq)로부터 복수의 제4 동영상 특징(V1D 내지 VtD)을 획득할 수 있다. 즉, 프로세서는 디코더(MD-3)를 이용해 복수의 제3 동영상 특징(V1C 내지 VtC) 및 시간 쿼리(Mq)로부터 복수의 제4 동영상 특징(V1D 내지 VtD)을 획득할 수 있다. 일 실시예에서, 디코더(MD-3)는 예를 들어 트랜스포머의 디코더일 수 있다. 일 실시예에서, 복수의 제3 동영상 특징(V1C 내지 VtC)에 텍스트 쿼리 (T)에 대한 정보가 포함되어 있으므로 텍스트 쿼리는 디코더(MD-3)에 별도로 입력되지 않을 수 있다. 일 실시예에서, 시간 쿼리(Mq)는 시구간을 정의하는 중앙 시점 및 시구간의 길이를 포함할 수 있다. 예를 들어, 시간 쿼리(Mq)가 중앙 시점 100 및 시구간 길이 20을 포함하는 경우, 시간 쿼리(Mq)는 80 내지 120의 시구간을 의미할 수 있다. 시간 쿼리(Mq)는 학습된 값을 가질 수 있다. 일 실시예에서, 프로세서(120, 도 1 참조)는, 중 앙 시점 주변의 특징들을 추출할 수 있으며, 시구간 길이로 크로스-어텐션 맵을 변조할 수 있다. 중앙 시점과 시구간 길이는 디코더(MD-3)의 각 층마다 학습될 수 있다. 일 실시예에서, 프로세서는 복수의 제4 동영상 특징(V1D 내지 VtD)을 이용해 동영상의 서로 다른 복수의 시 점 또는 시구간(1 내지 t) 중 적어도 하나를 선택할 수 있다. 즉, 프로세서는 주어진 텍스트 쿼리에 대한 동영상의 장면 검색을 수행할 수 있다. 도 4는 도 1의 전자 장치에 사용되는 인공지능 모델의 개념도이다. 도 4를 참조하면, 인공지능 모델(MD-4)의 인 코더(MD-42)는 복수의 제2 동영상 특징(V1B 내지 VtB) 및 제1 중요도 토큰(S1)으로부터 복수의 제3 동영상 특징 (V1C 내지 VtC) 및 제2 중요도 토큰(S2)을 획득할 수 있다. 즉, 프로세서는 인코더(MD-42)를 이용해 복수의 제2 동영상 특징(V1B 내지 VtB) 및 제1 중요도 토큰(S1)으로부터 복수의 제3 동영상 특징(V1C 내지 VtC) 및 제2 중 요도 토큰(S2)을 획득할 수 있다. 예를 들어, 프로세서는 제2 동영상 특징(V1B 내지 VtB) 및 제1 중요도 토 큰(S1)을 연결(concatenate)해 하나의 입력 텐서를 획득하고, 입력 텐서로부터 인코더(MD-42)를 이용해 복수의 제3 동영상 특징(V1C 내지 VtC) 및 제2 중요도 토큰(S2)을 획득할 수 있다. 제2 중요도 토큰(S2)는 입력-적응적 중요도 예측자(predictor)라 할 수 있으며, 인코더(MD-42)에 의해 입력-의존적 문맥으로 재구성된다. 일 실시예에서, 프로세서는 복수의 제3 동영상 특징(V1C 내지 VtC) 및 제2 중요도 토큰(S2)으로부터 서로 다 른 복수의 시점 또는 시구간 각각의 중요도(S1C 내지 StC)를 획득할 수 있다. 일 실시예에서, 제1 중요도 토큰 (S1) 및 제2 중요도 토큰(S2) 각각은 학습 가능한 벡터일 수 있으며 학습 시작 시 무작위적으로 설정될 수 있다. 예를 들어, 프로세서는 [수학식 2]에 따라 서로 다른 복수의 시점 또는 시구간 각각의 중요도(S1C 내지 StC)를 획득할 수 있다. 수학식 2"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 Si는 i번째 시점 또는 시구간에 대응하는 중요도이고, FCS() 및 FCc()는 각각 완전 연결 층(fully connected layer)이고, S2는 제2 중요도 토큰이고, Vi는 복수의 제3 동영상 특징(V1C 내지 VtC) 중 i번째 시점 또 는 시구간에 대응하는 제3 동영상 특징이고, d는 FCS(S2) 및 FCC(Vi)의 차원이다. ·는 내적이다. 인코더(MD- 42)는 텍스트 쿼리(T) 및 동영상 클립(V1 내지 Vt)에 따라 다른 제2 중요도 토큰(S2)을 사용해 각 시점 또는 시 구간의 중요도를 획득하므로 보다 정확하게 중요도를 계산할 수 있으며, 텍스트 쿼리(T)에 따른 중요도 변화를 보다 정확하게 모델링할 수 있다. 도 5는 본 발명의 일 실시예에 따른 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전 자 장치의 개략적인 블록도이다. 도 5를 참조하면, 전자 장치는 저장부 및 프로세서를 포함할 수 있다. 저장부는 각종 데이터 및 프로그램을 저장할 수 있다. 예를 들어, 저장부는 인공지능 모델(MD2, MD2- 4 각각 도 7 및 도 8 참조)을 저장할 수 있다. 일 실시예에서, 저장부는 인공지능 모델에 입력되는 동영상 데이터 및 텍스트 쿼리 데이터를 저장할 수 있다. 저장부는 휘발성 메모리 및 비휘발성 메모리 중 적어도 하나를 포함할 수 있다. 예를 들어, 휘발성 메모리는 DRAM, SRAM, SDRAM, DDR SDRAM, FeRAM, MRAM, PRAM, PoRAM, 또는 ReRAM을 포함할 수 있다. 예를 들어, 비휘발성 메모리는 플래시 메모리, mask ROM, PROM, OTPROM, EPROM, EEPROM, 하드디스크, 또는 광디스크를 포함할 수 있다. 프로세서는 전자 장치의 동작 전반을 제어할 수 있다. 프로세서는 저장부를 제어할 수 있 다. 예를 들어, 프로세서는 중앙 처리 장치(CPU) 및 그래픽 처리 장치(GPU) 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 프로세서는, 크로스-어텐션 모델 및 인코더를 이용해 동영상, 동영상과 양의 쌍인 제1 텍 스트 쿼리, 및 중요도 토큰으로부터 복수의 제1 동영상 특징, 제1 텍스트 쿼리에 대한 동영상의 제1 시점 또는 제1 시구간의 제1 중요도, 및 제1 텍스트 쿼리에 대한 동영상의 제2 시점 또는 제2 시구간의 제2 중요도를 획득 하고, 크로스-어텐션 모델 및 인코더를 이용해 동영상, 동영상과 음의 쌍인 제2 텍스트 쿼리, 및 중요도 토큰으 로부터 복수의 제2 동영상 특징 및 제2 텍스트 쿼리에 대한 동영상의 제3 중요도를 획득하고, 제1 중요도는 제2중요도보다 크고, 제1 중요도가 증가하고 제2 중요도 및 제3 중요도가 감소하도록 크로스-어텐션 모델, 인코더, 및 중요도 토큰을 수정할 수 있다. 일 실시예에서, 프로세서는 디코더를 이용해 복수의 제1 동영상 특징 및 시간 쿼리로부터 복수의 제3 동영 상 특징을 획득하고, 복수의 제3 동영상 특징을 이용해 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하나를 선택할 수 있다. 일 실시예에서, 프로세서는 선택된 시점 또는 시구간과 정답 시점 또는 시구간 사이의 차이를 감소시키도 록 크로스-어텐션 모델, 인코더, 및 디코더를 수정할 수 있다. 일 실시예에서, 프로세서는 선택된 적어도 하나의 시점 또는 시구간이 전경(foreground)인지 배경 (background)인지 분류하고, 전경인지 배경인지에 대한 예측이 실제 정답과 같아지도록 크로스-어텐션 모델, 인 코더, 및 디코더를 수정할 수 있다. 도 6은 도 5의 전자 장치를 설명하기 위한 개념도이다. 도 6을 참조하면, 프로세서는 인공지능 모델(MD2) 을 이용해 동영상(V), 동영상(V)과 양의 쌍인 제1 텍스트 쿼리(Tq1), 및 중요도 토큰(S)으로부터 복수의 제1 동 영상 특징(V1C1 내지 VtC1) 및 제1 텍스트 쿼리(Tq1)에 대한 동영상(V)의 서로 다른 복수의 시점 또는 시구간(1 내 지 t) 각각의 중요도(S1C1 내지 StC1)를 획득할 수 있다. 또한, 프로세서는 인공지능 모델(MD2)을 이용해 동 영상(V), 동영상(V)과 음의 쌍인 제2 텍스트 쿼리(Tq2), 및 중요도 토큰(S)으로부터 복수의 제2 동영상 특징(V1C2 내지 VtC2) 및 제2 텍스트 쿼리(Tq2)에 대한 동영상(V)의 서로 다른 복수의 시점 또는 시구간(1 내지 t) 각각의 중요도(S1C2 내지 StC2)를 획득할 수 있다. 본 명세서에서, 동영상과 텍스트 쿼리가 양의 쌍은 텍스트 쿼리가 동 영상과 관련된 것을 의미하며, 동영상과 텍스트 쿼리가 음의 쌍은 텍스트 쿼리가 동영상과 무관한 것을 의미한 다. 도 7은 도 5의 전자 장치에 사용되는 인공지능 모델의 개념도이다. 먼저, 인공지능 모델(MD2-4)의 학습 동안 동 영상 인코더(EV) 및 텍스트 인코더(ET)는 고정되어 학습되지 않을 수 있음을 참조한다. 도 7을 참조하면, 인공지 능 모델(MD2)은 크로스-어텐션 모델(MD2-1) 및 인코더(MD2-2)를 포함할 수 있다. 인공지능 모델(MD2)은 제1 텍 스트 쿼리(Tq1)에 대한 동영상(V)의 서로 다른 복수의 시점 또는 시구간(1 내지 t) 각각의 중요도(S1C1 내지 StC1) 중 제1 중요도가 제2 중요도보다 큰 제1 중요도 및 제2 중요도를 고려할 수 있다. 또한, 인공지능 모델(MD2)은 제2 텍스트 쿼리(Tq2)에 대한 동영상(V)의 제3 중요도를 고려할 수 있다. 즉, 제1 중요도는 동영상(V) 내의 서로 다른 시점 또는 시구간들 중 제1 텍스트 쿼리(Tq1)와 관련이 높은 시점 또는 시구간의 중요도이고, 제2 중요도는 동영상(V) 내의 서로 다른 시점 또는 시구간들 중 제1 텍스트 쿼리(Tq1)와 관련이 낮은 시점 또는 시구간의 중요 도이고, 동영상(V)과 제1 텍스트 쿼리(Tq1)는 양의 쌍일 수 있다. 반면, 제3 중요도는 동영상(V)과 음의 쌍인 제 2 텍스트 쿼리(Tq2)에 대한 중요도일 수 있다. 일 실시예에서, 프로세서는 제1 중요도가 증가하고 제2 중요도 및 제3 중요도가 감소하도록 인공지능 모델 (MD2)을 수정할 수 있다. 예를 들어, 프로세서는 제1 중요도가 증가하고 제2 중요도 및 제3 중요도가 감소 하도록 크로스-어텐션 모델(MD2-1), 인코더(MD2-2), 및 중요도 토큰(S1)을 수정할 수 있다. 예를 들어, 프로세 서는 제1 중요도 및 제2 중요도를 이용해 [수학식 3]의 마진 랭킹 손실을 획득하고, 마진 랭킹 손실이 감 소되도록 즉 제1 중요도가 증가하고 제2 중요도가 감소하도록 인공지능 모델(MD2)을 수정할 수 있다. [수학식 3]에서, Lmargin은 마진 랭킹 손실이고, Δ는 마진이고, Shigh는 제1 중요도이고, Slow는 제2 중요도이다. 수학식 3"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "일 실시예에서, 프로세서는 [수학식 4]의 랭킹 기반 비교 손실을 획득하고 랭킹 기반 비교 손실이 감소되 도록 인공지능 모델(MD2)을 수정함으로써 제1 중요도가 증가하고 제2 중요도 및 제3 중요도가 감소하게 할 수 있다. 프로세서는 랭킹 기반 비교 손실을 이용함으로써 정밀한 중요도 예측이 가능할 수 있다.수학식 4"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "[수학식 4]에서, Lcont는 랭킹 기반 비교 손실이고, Xrpos는 양의 집합(r보다 높은 중요도를 가짐)을 의미하고, Xrneg는 음의 집합(r보다 낮은 중요도를 가짐)을 의미하고, τ는 온도 스케일링 파라미터이고, S(x)는 중요도이다. R은 최대 랭킹 값이고, 각각의 동영상 클립 미니-배치는 R보다 낮은 중요도(Saliency Score)를 갖 는다. 이 미니-배치를 R회 순환하며, 매 순환마다 순환 인덱스 r(r∈{0, 1, ..., R-1})보다 중요도가 높은 샘플 들로부터 양의 집합(Xrpos)을 생성한다. 순환 인덱스 r보다 낮은 랭크의 샘플들로부터 음의 집합(Xrneg)을 생성한 다. 음의 집합(Xrneg)은 [수학식 5]의 음의 손실 계산에 사용되는 음의 쌍을 포함할 수 있다. 수학식 5"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "일 실시예에서, 프로세서는 [수학식 5]의 음의 손실을 획득하고 음의 손실이 감소되도록 인공지능 모델 (MD2)을 수정함으로써 제3 중요도가 감소하게 할 수 있다. [수학식 5]에서, Lneg는 음의 손실이고, Sneg는 제3 중 요도, 즉, 음의 쌍인 동영상-텍스트 쿼리의 중요도이다. 이러한 손실 함수를 사용해 인공지능 모델(MD2)을 학습 시킴으로써 인공지능 모델(MD2)이 텍스트 쿼리가 동영상과 양의 쌍인지 음의 쌍인지에 무관하게 장면 또는 중요 도를 획득하는 것을 방지할 수 있다. 도 8은 도 5의 전자 장치에 사용되는 인공지능 모델의 개념도이다. 도 8을 참조하면, 프로세서는 디코더 (MD2-3)를 이용해 복수의 제3 동영상 특징(V1C 내지 VtC) 및 시간 쿼리(Mq)로부터 복수의 제4 동영상 특징(V1D 내 지 VtD)을 획득하고, 복수의 제4 동영상 특징(V1D 내지 VtD)을 이용해 동영상의 서로 다른 복수의 시점 또는 시구 간 중 적어도 하나를 선택할 수 있다. 즉, 프로세서는 복수의 제4 동영상 특징(V1D 내지 VtD)을 이용해 장 면 검색을 수행할 수 있다. 일 실시예에서, 프로세서는 선택된 시점 또는 시구간과 정답 시점 또는 시구간 사이의 차이를 감소시키도 록 크로스-어텐션 모델(MD2-1), 인코더(MD2-2), 및 디코더(MD2-3)를 수정할 수 있다. 예를 들어, 프로세서(22 0)는 [수학식 6]의 L1 손실 및 gIoU 손실을 획득하고, L1 손실 및 gIoU 손실을 감소시킴으로써 선택된 시점 또 는 시구간과 정답 시점 또는 시구간 사이의 차이를 감소시킬 수 있다. [수학식 6]에서, L1은 L1 손실이고, m은 정답 시점 또는 시구간이고, 은 프로세서가 선택한 시점 또는 시구간이다. gIoU 손실은 hamid Rezatofighi et al., \"generalized intersection over union: A metric and a loss for bounding box regression\", In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 658-666, 2019에 개시된 정의를 따라 계산될 수 있다. 수학식 6"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "일 실시예에서, 프로세서는 선택된 시점 또는 시구간이 전경인지 배경인지 분류하고, 전경인지 배경인지에 대한 예측이 실제 정답과 같아지도록 크로스-어텐션 모델(MD2-1), 인코더(MD2-2), 및 디코더(MD2-3)를 수정할 수 있다. 예를 들어, 프로세서는 [수학식 7]의 크로스-엔트로피 손실을 획득하고, 크로스-엔트로피를 감소 시키도록 크로스-어텐션 모델(MD2-1), 인코더(MD2-2), 및 디코더(MD2-3)를 수정할 수 있다. [수학식 7]에서, LCE는 크로스-엔트로피 손실이고, y는 실제 전경인지 배경인지 여부이고, 는 프로세서가 예측한 전경인 지 배경인지 여부이다. 수학식 7"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "일 실시예에서, 종합적으로, 프로세서는 [수학식 8]의 총 손실을 획득하고, 손실이 감소하도록 인공지능 모델(MD2-4)을 학습시킬 수 있다. 수학식 8"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "[수학식 8]에서, Ltot은 총 손실이고, Lhl은 하이라이트 검출 손실이고, Lmr은 장면 검색 손실이고, Lneg는 음의 손 실([수학식 5] 참조)이고, λneg는 음의 손실에 대한 균형 상수이다. 하이라이트 검출 손실(Lhl)은 중요도를 추정 하는데 사용될 수 있다. 장면 검색 손실(Lmr)은 실제 정답 시점 또는 시구간과 예측된 시점 또는 시구간 사이의 차이를 측정할 수 있다. 예를 들어, 하이라이트 검출 손실(Lhl) 및 장면 검색 손실(Lmr)은 각각 [수학식 9] 및 [수학식 10]과 같이 정의될 수 있다. 수학식 9"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "[수학식 9]에서, Lhl은 하이라이트 검출 손실이고, Lmargin은 [수학식 3]의 마진 랭킹 손실이고, λmargin은 마진 랭 킹 손실에 대한 균형 상수이고, Lcont는 [수학식 4]의 랭킹 기반 비교 손실이고, λcont는 랭킹 기반 비교 손실에 대한 균형 상수이다. 수학식 10"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "[수학식 10]에서, Lmr은 장면 검색 손실이고, L1은 [수학식 6]의 L1 손실이고, λL1은 L1 손실에 대한 균형 상수 이고, LgIoU는 gIoU 손실이고, λgIoU는 gIoU 손실에 대한 균형 상수이다. LCE는 [수학식 7]의 크로스-엔트로피 손 실이고, λCE는 크로스-엔트로피 손실에 대한 균형 상수이다. 도 9는 도 1의 전자 장치의 동작 방법의 흐름도이다. 도 9를 참조하면, 전자 장치의 동작 방법은, 동영상으로부 터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 동영상 특징을 획득하는 단계(S910), 텍스 트 쿼리로부터 텍스트 쿼리 특징을 획득하는 단계(S920), 복수의 제1 동영상 특징 및 텍스트 쿼리 특징으로부터 복수의 가중치를 획득하는 단계(S930), 복수의 가중치 및 복수의 제1 동영상 특징으로부터 복수의 제2 동영상 특징을 획득하는 단계(S940), 인코더를 이용해 복수의 제2 동영상 특징으로부터 복수의 제3 동영상 특징을 획득하는 단계(S950), 디코더를 이용해 복수의 제3 동영상 특징 및 시간 쿼리로부터 복수의 제4 동영상 특징을 획득 하는 단계(S960), 및 복수의 제4 동영상 특징을 이용해 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하나를 선택하는 단계(S970)를 포함할 수 있다. 일 실시예에서, 인코더는 트랜스포머의 인코더이고, 디코더는 트랜스포머의 디코더일 수 있다. 일 실시예에서, 시간 쿼리는 시구간을 정의하는 중앙 시점 및 길이를 포함할 수 있다. 도 10은 도 1의 전자 장치의 동작 방법의 흐름도이다. 도 10을 참조하면, 복수의 가중치를 획득하는 단계(S930, 도 9 참조)는, 복수의 제1 동영상 특징으로부터 동영상 쿼리를 획득하는 단계(S1031), 텍스트 쿼리 특징으로부 터 텍스트 키를 획득하는 단계(S1032), 텍스트 쿼리 특징으로부터 텍스트 벨류를 획득하는 단계(S1033), 동영상 쿼리, 텍스트 키, 및 텍스트 벨류를 이용해 어텐션 점수를 획득하는 단계(S1034), 및 어텐션 점수로부터 복수의 가중치를 획득하는 단계(S1035)를 포함할 수 있다. 즉, 도 10의 전자 장치의 동작 방법은, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복 수의 제1 동영상 특징을 획득하는 단계(S1010), 텍스트 쿼리로부터 텍스트 쿼리 특징을 획득하는 단계(S1020), 복수의 제1 동영상 특징으로부터 동영상 쿼리를 획득하는 단계(S1031), 텍스트 쿼리 특징으로부터 텍스트 키를 획득하는 단계(S1032), 텍스트 쿼리 특징으로부터 텍스트 벨류를 획득하는 단계(S1033), 동영상 쿼리, 텍스트 키, 및 텍스트 벨류를 이용해 어텐션 점수를 획득하는 단계(S1034), 어텐션 점수로부터 복수의 가중치를 획득하 는 단계(S1035), 복수의 가중치 및 복수의 제1 동영상 특징으로부터 복수의 제2 동영상 특징을 획득하는 단계 (S1040), 인코더를 이용해 복수의 제2 동영상 특징으로부터 복수의 제3 동영상 특징을 획득하는 단계(S1050), 디코더를 이용해 복수의 제3 동영상 특징 및 시간 쿼리로부터 복수의 제4 동영상 특징을 획득하는 단계(S1060), 및 복수의 제4 동영상 특징을 이용해 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하나를 선택하는 단계(S1070)를 포함할 수 있다. 도 11은 도 1의 전자 장치의 동작 방법의 흐름도이다. 도 11을 참조하면, 복수의 제3 동영상 특징을 획득하는 단계(S950, 도 9 참조)는 인코더를 이용해 복수의 제2 동영상 특징 및 제1 중요도 토큰으로부터 복수의 제3 동 영상 특징 및 제2 중요도 토큰을 획득하는 단계(S1150)를 포함할 수 있다. 즉, 도 11의 전자 장치의 동작 방법 은, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 동영상 특징을 획득하는 단계(S1110), 텍스트 쿼리로부터 텍스트 쿼리 특징을 획득하는 단계(S1120), 복수의 제1 동영상 특징 및 텍스트 쿼리 특징으로부터 복수의 가중치를 획득하는 단계(S1130), 복수의 가중치 및 복수의 제1 동영상 특징으로부터 복수의 제2 동영상 특징을 획득하는 단계(S1140), 인코더를 이용해 복수의 제2 동영상 특징 및 제1 중요도 토큰 으로부터 복수의 제3 동영상 특징 및 제2 중요도 토큰을 획득하는 단계(S1150), 디코더를 이용해 복수의 제3 동 영상 특징 및 시간 쿼리로부터 복수의 제4 동영상 특징을 획득하는 단계(S1160), 및 복수의 제4 동영상 특징을 이용해 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하나를 선택하는 단계(S1170)를 포함할 수 있다. 도 12는 도 1의 전자 장치의 동작 방법의 흐름도이다. 도 12를 참조하면, 전자 장치의 동작 방법은 복수의 제3 동영상 특징 및 제2 중요도 토큰으로부터 서로 다른 복수의 시점 또는 시구간의 중요도를 획득하는 단계(S128 0)를 더 포함할 수 있다. 즉, 도 12의 전자 장치의 동작 방법은, 동영상으로부터 서로 다른 복수의 시점 또는 시구간에 각각 대응하는 복수의 제1 동영상 특징을 획득하는 단계(S1210), 텍스트 쿼리로부터 텍스트 쿼리 특징 을 획득하는 단계(S1220), 복수의 제1 동영상 특징 및 텍스트 쿼리 특징으로부터 복수의 가중치를 획득하는 단 계(S1230), 복수의 가중치 및 복수의 제1 동영상 특징으로부터 복수의 제2 동영상 특징을 획득하는 단계 (S1240), 인코더를 이용해 복수의 제2 동영상 특징 및 제1 중요도 토큰으로부터 복수의 제3 동영상 특징 및 제2 중요도 토큰을 획득하는 단계(S1250), 디코더를 이용해 복수의 제3 동영상 특징 및 시간 쿼리로부터 복수의 제4 동영상 특징을 획득하는 단계(S1260), 복수의 제4 동영상 특징을 이용해 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하나를 선택하는 단계(S1270), 및 복수의 제3 동영상 특징 및 제2 중요도 토큰으로부터 서로 다른 복수의 시점 또는 시구간의 중요도를 획득하는 단계(S1280)를 포함할 수 있다. 도 13은 도 5의 전자 장치의 동작 방법의 흐름도이다. 도 13을 참조하면, 전자 장치의 동작 방법은, 크로스-어 텐션 모델 및 인코더를 이용해 동영상, 동영상과 양의 쌍인 제1 텍스트 쿼리, 및 중요도 토큰으로부터 복수의 제1 동영상 특징, 제1 텍스트 쿼리에 대한 상기 동영상의 제1 시점 또는 제1 시구간의 제1 중요도, 및 제1 텍스 트 쿼리에 대한 상기 동영상의 제2 시점 또는 제2 시구간의 제2 중요도를 획득하는 단계(S1310), 크로스-어텐션 모델 및 인코더를 이용해 동영상, 동영상과 음의 쌍인 제2 텍스트 쿼리, 및 중요도 토큰으로부터 복수의 제2 동 영상 특징 및 제2 텍스트 쿼리에 대한 상기 동영상의 제3 중요도를 획득하는 단계(S1320), 및 제1 중요도는 제2 중요도보다 크고, 제1 중요도가 증가하고 제2 중요도 및 제3 중요도가 감소하도록 크로스-어텐션 모델, 인코더,및 중요도 토큰을 수정하는 단계(S1330)를 포함할 수 있다. 도 14는 도 5의 전자 장치의 동작 방법의 흐름도이다. 도 14를 참조하면, 전자 장치의 동작 방법은 디코더를 이 용해 복수의 제1 동영상 특징 및 시간 쿼리로부터 복수의 제3 동영상 특징을 획득하는 단계(S1440) 및 복수의 제3 동영상 특징을 이용해 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하나를 선택하는 단계 (S1450)를 더 포함할 수 있다. 즉, 도 14의 전자 장치의 동작 방법은, 크로스-어텐션 모델 및 인코더를 이용해 동영상, 동영상과 양의 쌍인 제1 텍스트 쿼리, 및 중요도 토큰으로부터 복수의 제1 동영상 특징, 제1 텍스트 쿼 리에 대한 상기 동영상의 제1 시점 또는 제1 시구간의 제1 중요도, 및 제1 텍스트 쿼리에 대한 상기 동영상의 제2 시점 또는 제2 시구간의 제2 중요도를 획득하는 단계(S1410), 크로스-어텐션 모델 및 인코더를 이용해 동영 상, 동영상과 음의 쌍인 제2 텍스트 쿼리, 및 중요도 토큰으로부터 복수의 제2 동영상 특징 및 제2 텍스트 쿼리 에 대한 상기 동영상의 제3 중요도를 획득하는 단계(S1420), 제1 중요도는 제2 중요도보다 크고, 제1 중요도가 증가하고 제2 중요도 및 제3 중요도가 감소하도록 크로스-어텐션 모델, 인코더, 및 중요도 토큰을 수정하는 단 계(S1430), 디코더를 이용해 복수의 제1 동영상 특징 및 시간 쿼리로부터 복수의 제3 동영상 특징을 획득하는 단계(S1440), 및 복수의 제3 동영상 특징을 이용해 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하 나를 선택하는 단계(S1450)를 포함할 수 있다. 도 15는 도 5의 전자 장치의 동작 방법의 흐름도이다. 도 15를 참조하면, 전자 장치의 동작 방법은, 선택된 시 점 또는 시구간과 정답 시점 또는 시구간 사이의 차이를 감소시키도록 크로스-어텐션 모델, 인코더, 및 디코더 를 수정하는 단계(S1560)를 더 포함할 수 있다. 즉, 도 15의 전자 장치의 동작 방법은, 크로스-어텐션 모델 및 인코더를 이용해 동영상, 동영상과 양의 쌍인 제1 텍스트 쿼리, 및 중요도 토큰으로부터 복수의 제1 동영상 특 징, 제1 텍스트 쿼리에 대한 상기 동영상의 제1 시점 또는 제1 시구간의 제1 중요도, 및 제1 텍스트 쿼리에 대 한 상기 동영상의 제2 시점 또는 제2 시구간의 제2 중요도를 획득하는 단계(S1510), 크로스-어텐션 모델 및 인 코더를 이용해 동영상, 동영상과 음의 쌍인 제2 텍스트 쿼리, 및 중요도 토큰으로부터 복수의 제2 동영상 특징 및 제2 텍스트 쿼리에 대한 상기 동영상의 제3 중요도를 획득하는 단계(S1520), 제1 중요도는 제2 중요도보다 크고, 제1 중요도가 증가하고 제2 중요도 및 제3 중요도가 감소하도록 크로스-어텐션 모델, 인코더, 및 중요도 토큰을 수정하는 단계(S1530), 디코더를 이용해 복수의 제1 동영상 특징 및 시간 쿼리로부터 복수의 제3 동영상 특징을 획득하는 단계(S1540), 복수의 제3 동영상 특징을 이용해 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하나를 선택하는 단계(S1550), 및 선택된 시점 또는 시구간과 정답 시점 또는 시구간 사이의 차이를 감소시키도록 크로스-어텐션 모델, 인코더, 및 디코더를 수정하는 단계(S1560)를 포함할 수 있다. 도 16은 도 5의 전자 장치의 동작 방법의 흐름도이다. 도 16을 참조하면, 전자 장치의 동작 방법은 선택된 적어 도 하나의 시점 또는 시구간이 전경인지 배경인지 분류하는 단계(S1660) 및 전경인지 배경인지에 대한 예측이 실제 정답과 같아지도록 크로스-어텐션 모델, 인코더, 및 디코더를 수정하는 단계(S1670)를 더 포함할 수 있다. 즉, 도 16의 전자 장치의 동작 방법은, 크로스-어텐션 모델 및 인코더를 이용해 동영상, 동영상과 양의 쌍인 제 1 텍스트 쿼리, 및 중요도 토큰으로부터 복수의 제1 동영상 특징, 제1 텍스트 쿼리에 대한 상기 동영상의 제1 시점 또는 제1 시구간의 제1 중요도, 및 제1 텍스트 쿼리에 대한 상기 동영상의 제2 시점 또는 제2 시구간의 제 2 중요도를 획득하는 단계(S1610), 크로스-어텐션 모델 및 인코더를 이용해 동영상, 동영상과 음의 쌍인 제2 텍 스트 쿼리, 및 중요도 토큰으로부터 복수의 제2 동영상 특징 및 제2 텍스트 쿼리에 대한 상기 동영상의 제3 중 요도를 획득하는 단계(S1620), 제1 중요도는 제2 중요도보다 크고, 제1 중요도가 증가하고 제2 중요도 및 제3 중요도가 감소하도록 크로스-어텐션 모델, 인코더, 및 중요도 토큰을 수정하는 단계(S1630), 디코더를 이용해 복수의 제1 동영상 특징 및 시간 쿼리로부터 복수의 제3 동영상 특징을 획득하는 단계(S1640), 및 복수의 제3 동영상 특징을 이용해 동영상의 서로 다른 복수의 시점 또는 시구간 중 적어도 하나를 선택하는 단계(S1650), 선택된 적어도 하나의 시점 또는 시구간이 전경인지 배경인지 분류하는 단계(S1660), 및 전경인지 배경인지에 대한 예측이 실제 정답과 같아지도록 크로스-어텐션 모델, 인코더, 및 디코더를 수정하는 단계(S1670)를 포함할 수 있다. [실시예들과 비교예들] 제1 실시예: 동영상 특징 사용 제2 실시예: 동영상 특징 및 오디오 특징 사용 제3 실시예: VGG를 이용해 추출한 동영상 특징 사용 제4 실시예: VGG를 이용해 추출한 동영상 특징 사용 + 오디오 특징 제5 실시예: C3D를 이용해 추출한 동영상 특징 사용 제6 실시예: Slowfast 및 CLIP을 이용해 추출한 동영상 특징 사용 제7 실시예: 크로스-어텐션 모델 대신 4개의 셀프-어텐션 층을 포함 제8 실시예: 크로스-어텐션 모델이 4개의 크로스-어텐션 층을 포함 제1 비교예: BeautyThumb (Yale Song et al., \"To click or not to click: Automatic selection of beautiful thumbnails from videos\", In Proceedings of the 25th ACM international on conference on information and knowledge management, 659-668, 2016) 제2 비교예: DVSE (Wu Liu et al., \"Multi-task deep visual-semantic embedding for video thumbnail selection\", In Proceedings of the IEEE conference on computer vision and pattern recognition, 3707- 3715 2015) 제3 비교예: MCN (Lisa Anne Hendricks et al., \"localizing moments in video with natural language\", In Proceedings of the IEEE international conference on computer vision, 5803-5812, 2017) 제4 비교예: CAL (Victor Escorcia et al., \"Temporal localization of moments in video collections with natural language\", arXiv 2019) 제5 비교예: XML (Jie Lei et al., \"Tvr: A large-scale dataset for video-subtitle moment retrieval\", In European Conference on Computer Vision, 447-463, 2020) 제6 비교예: XML+ (Jie Lei et al., \"Tvr: A large-scale dataset for video-subtitle moment retrieval\", In European Conference on Computer Vision, 447-463, 2020) 제7 비교예: Moment-DETR (Jie Lei et al., \"Detecting moments and highlights in videos via natural language queries\", Advances in Neural Information Processing Systems, 34:11846-11858, 2021) 제8 비교예: UMT (Ye Liu et al., \"Umt:Unified multi-modal transformers for joint video moment retrieval and highlight detection\", In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3042-3051, 2022) 제9 비교예: sLSTM (Ke Zhang et al., \"Video summarization with long short-term memory\", In European conference on computer vision, 766-782, 2016) 제10 비교예: SG (Behrooz Mahasseni et al., \"Unsupervised video summarization with adversarial lstm networks\", In Proceedings of the IEEE conference on computer Vision and Pattern Recognition, 202-211, 2017) 제11 비교예: LIM-S (Bo Xiong et al., \"Less is more: Learning highlight detection from video duration\", In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1258-1267, 2019) 제12 비교예: Trailer (Lezi Wang et al., \"Learning trailer moments in full-length movies with co- contrastive attention\", In European Conference on Computer Vision, 300-316, 2020) 제13 비교예: SL-Module (Minghao Xu et al., \"Cross-category video highlight detection via set-based learning\", In Proceedings of the IEEE/CVF International Conference on Computer Vision, 7970-7979, 2021) 제14 비교예: MINI-NET (Fa-Ting Hong et al., \"Mini-net: Multiple instance ranking network for video highlight detection\", In European Conference on Computer Vision, 345-360, 2020) 제15 비교예: TCG (Qinghao Ye et al., \"Temporal cue guided video highlight detection with low-rank audio-visual fusion\", In Proceedings of the IEEE/CVF International Conference on Computer Vision, 7950-7959, 2021) 제16 비교예: Joint-VA (Taivanbat Badamdorj et al., \"Joint visual and audio learning for video highlight detection\", In Proceedings of the IEEE/CVF International Conference on Computer Vision,8127-8137, 2021) 제17 비교예: SAP 제18 비교예: TripNet 제19 비교예: SM-RL 제20 비교예: MAN 제21 비교예: 2D_TAN 제22 비교예: FVMR 제23 비교예: CTRL 제24 비교예: ACL 제25 비교예: RWM-RL 제26 비교예: DEBUG 제27 비교예: VSLNet 도 17은 본 발명의 일 실시예 및 일 비교예의 텍스트 쿼리의 관련도(양의 쌍: 관련 있음, 음의 쌍: 관련 없음) 에 따른 동영상의 장면 검색 및 하이라이트 검출 결과를 도시한다. 도 17을 참조하면, 제7 비교예의 경우 중요 도 예측 결과가 텍스트 쿼리가 양의 쌍인지 음의 쌍인지에 무관하게 동일하였다. 즉, 제7 비교예의 경우 텍스트 쿼리가 동영상 장면 검색 및 하이라이트 검출에 미치는 영향이 미미하였으며 실제 정답과 전혀 동떨어진 부분을 선택하였다. 반면, 본 발명에 따른 일 실시예에 따르면, 텍스트 쿼리가 양의 쌍인지 음의 쌍인지에 따라 중요도 가 크게 달랐으며, 선택된 장면은 실제 정답 장면과 가까웠다. 본 발명에 따르면, 텍스트 쿼리와 동영상 사이의 크로스-어텐션 및 음의 쌍을 이용한 학습을 통해 동영상 특징이 텍스트 쿼리에 의존하게 할 수 있기 때문이다. 표 1"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "표 1은 QVHighlights(Jie Lei et al., \"Detecting moments and highlights in videos via natural language queries\", Advances in Neural Information Processing Systems, 34:11846-11858, 2021) 데이터셋을 이용해 장면 검색 및 하이라이트 검출 태스크에 대한 비교예들과 실시예들의 성능을 비교한다. 표 1을 참조하면, 본 발 명의 실시예들이 비교예들보다 높은 성능을 보였다. 또한, 본 발명의 실시예들에서 제2 실시예의 성능이 제1 실 시예보다 높은 것에 비추어 동영상뿐만 아니라 오디오를 함께 입력함으로써 성능이 향상될 수 있음을 알 수 있 다.표 2"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "표 2는 TVsum(Yale song et al, \"TVsum: Summarizing web videos using titles\", In Proceedings of the IEEE conference on computer vision and pattern recognition, 5179-5187, 2015) 데이터셋을 이용해 하이라이트 검 출 태스크에 대한 비교예들과 실시예들의 성능을 비교한다. 표 2를 참조하면, 전반적으로 본 발명의 실시예들이 비교예들보다 높은 성능을 보였다. 표 3"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "표 3은 Charades-STA(Jiyang Gao et al. \"Tall: Temporal activity localization via language query\", In Proceedings of the IEEE international conference on computer vision, 5267-5275, 2017) 데이터셋을 이용 해 장면 검색 태스크에 대한 비교예들과 실시예들의 성능을 비교한다. 표 3을 참조하면, 본 발명의 실시예들이 비교예들보다 높은 성능을 보였다.표 4"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "표 4는 본 발명의 각 구성의 효과를 알아보기 위한 실험 결과를 도시한다. MR은 장면 검색 성능을 말하며, HD는 하이라이트 검출 성능을 말한다. 표 4를 참조하면, 실시예 (e)는 (a) 대비 장면 검색 성능만을 향상시켰는데, 이는 시간 쿼리는 디코더에만 영향을 미치기 때문이다. 반면, 실시예 (b), (c), (d)는 (a) 대비 장면 검색 성능 및 하이라이트 검출 성능 둘 모두를 향상시켰다. 이는 크로스-어텐션 모델, 음의 쌍 손실, 중요도 토큰은 공통 적으로 텍스트 쿼리-의존적인 동영상 표현을 얻을 수 있기 때문이다. 도 18은 중요도(Saliency Score)에 본 발명의 일 실시예의 구성들이 미치는 영향을 확인하기 위한 실험 결과를 도시한다. 도 18을 참조하면, 제7 비교예에서는 양의 쌍의 그래프와 음의 쌍의 그래프가 상당 부분 중첩되었다. 즉, 제7 비교예에서는 텍스트 쿼리가 출력에 미치는 영향이 적은 것이다. 반면, 실시예에서는 양의 쌍의 그래프 와 음의 쌍의 그래프 사이의 중첩이 상당 부분 감소하여 텍스트 쿼리가 출력에 상당한 영향을 미침을 알 수 있 다. 표 5"}
{"patent_id": "10-2025-0007199", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "표 5는 크로스-어텐션을 추가함으로써 모델 파라미터 수가 증가된 효과로 인해 본 발명의 실시예가 비교예들보 다 우수한 성능을 보이는 것이 아닐지에 대한 우려를 해소하기 위한 실험 결과이다. *은 인코더 조건으로 텍스 트 쿼리 특징들만 사용하고 동영상 특징들만이 디코더에 의해 처리됨을 나타낸다. 표 5를 참조하면, 비교예에서 같은 수의 셀프-어텐션보다 크로스-어텐션이 더 나은 성능을 보였다. 또한, 제7 실시예와 제8 실시예를 비교하 면, 마찬가지로 같은 수의 셀프-어텐션보다 크로스-어텐션이 더 나은 성능을 보였다. 따라서 파라미터 수 증가 가 아닌 크로스-어텐션의 성질로 인해 본 발명의 실시예들이 우수한 성능을 나타냄을 알 수 있다.도 19는 본 발명의 텍스트 쿼리의 관련도에 따른 동영상의 장면 검색 및 하이라이트 검출 결과를 도시한다. 도 19를 참조하면, 텍스트 쿼리와 동영상 장면 사이의 실제 관련도가 높을수록 중요도가 높음을 알 수 있다. 예를 들어, 낮은 관련도를 갖는 음의 쌍의 경우 가장 낮은 중요도를 보였고, 높은 관련도를 갖는 양의 쌍의 경우 가 장 높은 중요도를 보였다. 이상으로, 본 발명의 실시예들이 도시되고 설명되었지만, 당업자는 첨부된 청구항들 및 그에 동등한 것들에 의 해 정의되는 바와 같은 본 실시예의 사상 및 범위를 벗어나지 않고 형태 및 세부 사항들에 있어 다양한 변경이 이루어질 수 있음을 이해할 것이다."}
{"patent_id": "10-2025-0007199", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 위한 전자 장치의 개략적인 블록도이다. 도 2는 도 1의 전자 장치에 사용되는 인공지능 모델의 개념도이다. 도 3은 도 1의 전자 장치에 사용되는 크로스-어텐션 모델의 개념도이다. 도 4는 도 1의 전자 장치에 사용되는 인공지능 모델의 개념도이다. 도 5는 본 발명의 일 실시예에 따른 동영상 장면 검색 및 하이라이트 검출 중 적어도 하나를 학습하기 위한 전 자 장치의 개략적인 블록도이다. 도 6은 도 5의 전자 장치를 설명하기 위한 개념도이다. 도 7은 도 5의 전자 장치에 사용되는 인공지능 모델의 개념도이다. 도 8은 도 5의 전자 장치에 사용되는 인공지능 모델의 개념도이다. 도 9 내지 12는 도 1의 전자 장치의 동작 방법의 흐름도이다. 도 13 내지 16은 도 5의 전자 장치의 동작 방법의 흐름도이다. 도 17은 본 발명의 일 실시예 및 일 비교예의 텍스트 쿼리의 관련도(양의 쌍: 관련 있음, 음의 쌍: 관련 없음) 에 따른 동영상의 장면 검색 및 하이라이트 검출 결과를 도시한다. 도 18은 중요도(Saliency Score)에 본 발명의 일 실시예의 구성들이 미치는 영향을 확인하기 위한 실험 결과를 도시한다. 도 19는 본 발명의 텍스트 쿼리의 관련도에 따른 동영상의 장면 검색 및 하이라이트 검출 결과를 도시한다."}
