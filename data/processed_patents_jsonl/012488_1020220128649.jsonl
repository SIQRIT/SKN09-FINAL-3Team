{"patent_id": "10-2022-0128649", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0048843", "출원번호": "10-2022-0128649", "발명의 명칭": "학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법 및 장치", "출원인": "한국전자통신연구원", "발명자": "이종률"}}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "원본 신경망 모델에서 서브 네트워크를 추출하고, 기 학습된 신경망 모델에서 상기 서브 네트워크에 대응하는대체 블록을 추출하여 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관계를 구성하며, 상기 서브 네트워크와상기 대체 블록에 관한 성능 정보를 포함하는 프로파일링 정보를 생성하는 사전처리 단계; 및질의를 입력받고, 질의 분석(Query Parsing)을 통해 상기 질의에 포함된 조건을 추출하고, 상기 조건, 상기 원본 신경망 모델, 상기 대체 블록, 상기 매핑 관계 및 상기 프로파일링 정보를 기초로 최종 모델을 생성하는 질의처리 단계;를 포함하는 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 사전처리 단계는,상기 원본 신경망 모델에서 상기 서브 네트워크를 추출하는 단계;기 학습된 신경망 모델에서 상기 서브 네트워크에 대응하는 대체 블록을 추출하여 상기 서브 네트워크와 상기대체 블록 간의 매핑 관계를 구성하는 단계; 및상기 서브 네트워크와 상기 대체 블록을 기초로 상기 프로파일링 정보를 생성하는 단계;를 포함하고,상기 서브 네트워크는 하나의 연결된 신경망인 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 질의처리 단계는,상기 질의를 입력받고, 질의 분석을 통해 상기 조건을 추출하는 단계;상기 원본 신경망 모델, 상기 대체 블록 및 상기 매핑 관계를 기초로 후보 신경망 모델을 생성하는 단계; 및상기 조건과 상기 프로파일링 정보를 기초로 상기 후보 신경망 모델을 평가하고, 상기 후보 신경망 모델에 대한평가 결과를 기초로 상기 후보 신경망 모델에서 상기 최종 모델을 선정하는 단계;를 포함하는 것인 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서, 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관계를 구성하는 단계는,상기 서브 네트워크와 상기 대체 블록 간의 호환 가능성을 판단하고, 상기 호환 가능성에 기초하여 상기 매핑관계를 구성하는 것이고,상기 호환 가능성은, 상기 서브 네트워크와 상기 대체 블록의 입력과 출력이 각각 동일한 차원 수와 채널 수를 가지고, 상기 서브 네트워크를 데이터가 통과할 경우의 공간 차원 변화와 상기 대체 블록을 데이터가 통과할 경우의 공간 차원 변화가 동일한 것을 의미하는 것인 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관계를 구성하는 단계는,공개특허 10-2024-0048843-3-상기 서브 네트워크와 상기 대체 블록 간의 호환 가능성을 판단하고, 상기 대체 블록이 대응되는 서브 네트워크와 입력 채널 수 및 출력 채널 수 중 적어도 어느 하나가 상이하여 호환 가능성을 만족하지 못할 경우, 가지치기(Pruning) 및 프로젝션 레이어 추가 중 적어도 어느 하나의 방식을 통하여 상기 대체 블록의 채널 수를 조정하는 것인 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 사전처리 단계는,상기 매핑 관계를 구성한 이후에, 대체 블록 학습용 데이터, 상기 원본 신경망 모델 및 상기 매핑 관계를 기초로 지식증류 기법을 이용하여 상기 대체 블록을 학습시키고, 상기 서브 네트워크와 상기 학습된 대체 블록에 관한 성능 정보를 포함하는 프로파일링 정보를 생성하는 것인 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 프로파일링 정보는,상기 서브 네트워크와 상기 대체 블록의 추론 계산 시간 및 메모리 사용량 중 적어도 어느 하나 또는 이들의 조합을 포함하는 것인 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 조건은,타겟 플랫폼, 목표 레이턴시(Latency) 및 목표 메모리 사용량 중 적어도 어느 하나 또는 이들의 조합을 포함하는 것인 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서, 상기 질의처리 단계는,최종 모델 학습용 데이터와 상기 원본 신경망 모델을 기초로 지식증류 기법을 이용하여 상기 최종 모델을 학습시키고, 상기 학습된 최종 모델을 출력하는 것인 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "원본 신경망 모델에서 서브 네트워크를 추출하고, 기 학습된 신경망 모델에서 상기 서브 네트워크에 대응하는대체 블록을 추출하여 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관계를 구성하며, 상기 서브 네트워크와상기 대체 블록에 관한 성능 정보를 포함하는 프로파일링 정보를 생성하는 사전처리 모듈; 및질의를 입력받고, 질의 분석(Query Parsing)을 통해 상기 질의에 포함된 조건을 추출하고, 상기 조건, 상기 원본 신경망 모델, 상기 대체 블록, 상기 매핑 관계 및 상기 프로파일링 정보를 기초로 최종 모델을 생성하는 질의처리 모듈;을 포함하는 경량 모델 탐색 장치."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 사전처리 모듈은,상기 원본 신경망 모델에서 상기 서브 네트워크를 추출하는 서브 네트워크 생성부;공개특허 10-2024-0048843-4-기 학습된 신경망 모델에서 상기 서브 네트워크에 대응하는 대체 블록을 추출하여 상기 서브 네트워크와 상기대체 블록 간의 매핑 관계를 구성하는 대체 블록 생성부; 및상기 서브 네트워크와 상기 대체 블록을 기초로 상기 프로파일링 정보를 생성하는 프로파일링부를 포함하고,상기 서브 네트워크는 하나의 연결된 신경망인 경량 모델 탐색 장치."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서, 상기 질의처리 모듈은,상기 질의를 입력받고, 질의 분석을 통해 상기 조건을 추출하는 질의 분석부;상기 원본 신경망 모델, 상기 대체 블록 및 상기 매핑 관계를 기초로 후보 신경망 모델을 생성하는 후보 모델생성부; 및상기 조건과 상기 프로파일링 정보를 기초로 상기 후보 신경망 모델을 평가하고, 상기 후보 신경망 모델에 대한평가 결과를 기초로 상기 후보 신경망 모델에서 상기 최종 모델을 선정하는 후보 모델 평가부를 포함하는 것인 경량 모델 탐색 장치."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서, 상기 대체 블록 생성부는,상기 서브 네트워크와 상기 대체 블록 간의 호환 가능성을 판단하고, 상기 호환 가능성에 기초하여 상기 매핑관계를 구성하는 것이고,상기 호환 가능성은, 상기 서브 네트워크와 상기 대체 블록의 입력과 출력이 각각 동일한 차원 수와 채널 수를 가지고, 상기 서브 네트워크를 데이터가 통과할 경우의 공간 차원 변화와 상기 대체 블록을 데이터가 통과할 경우의 공간 차원 변화가 동일한 것을 의미하는 것인 경량 모델 탐색 장치."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 대체 블록 생성부는,상기 서브 네트워크와 상기 대체 블록 간의 호환 가능성을 판단하고, 상기 대체 블록이 대응되는 서브 네트워크와 입력 채널 수 및 출력 채널 수 중 적어도 어느 하나가 상이하여 호환 가능성을 만족하지 못할 경우, 가지치기(Pruning) 및 프로젝션 레이어 추가 중 적어도 어느 하나의 방식을 통하여 상기 대체 블록의 채널 수를 조정하는 것인 경량 모델 탐색 장치."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제10항에 있어서, 상기 사전처리 모듈은,상기 매핑 관계를 구성한 이후에, 대체 블록 학습용 데이터, 상기 원본 신경망 모델 및 상기 매핑 관계를 기초로 지식증류 기법을 이용하여 상기 대체 블록을 학습시키고, 상기 서브 네트워크와 상기 학습된 대체 블록에 관한 성능 정보를 포함하는 프로파일링 정보를 생성하는 것인 경량 모델 탐색 장치."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제10항에 있어서, 상기 프로파일링 정보는,상기 서브 네트워크와 상기 대체 블록의 추론 계산 시간 및 메모리 사용량 중 적어도 어느 하나 또는 이들의 조공개특허 10-2024-0048843-5-합을 포함하는 것인 경량 모델 탐색 장치."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제10항에 있어서, 상기 조건은,타겟 플랫폼, 목표 레이턴시(Latency) 및 목표 메모리 사용량 중 적어도 어느 하나 또는 이들의 조합을 포함하는 것인 경량 모델 탐색 장치."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제10항에 있어서, 상기 질의처리 모듈은,최종 모델 학습용 데이터와 상기 원본 신경망 모델을 기초로 지식증류 기법을 이용하여 상기 최종 모델을 학습시키고, 상기 학습된 최종 모델을 출력하는 것인 경량 모델 탐색 장치."}
{"patent_id": "10-2022-0128649", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제13항에 있어서, 상기 대체 블록 생성부는,상기 호환 가능성을 가지면서도 상기 서브 네트워크와 다른 구조를 가진 대체 블록을 상기 기 학습된 신경망 모델에서 추출하여 상기 매핑 관계를 구성하는 것이고,상기 다른 구조는,파라미터, 레이어의 수, 레이어의 배치, 레이어 간의 연결 구조 및 변환 함수 중 적어도 어느 하나의 기준 또는이들의 조합된 기준이 상이한 것을 의미하는 것인 경량 모델 탐색 장치."}
{"patent_id": "10-2022-0128649", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법과 경량 모델 탐색 장치에 관한 것이다. 상기 경량 모델 탐색 방법은, 원본 신경망 모델에서 서브 네트워크를 추출하고, 기 학습된 신경망 모델 에서 상기 서브 네트워크에 대응하는 대체 블록을 추출하여 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관 계를 구성하며, 상기 서브 네트워크와 상기 대체 블록에 관한 성능 정보를 포함하는 프로파일링 정보를 생성하는 사전처리 단계 및 질의를 입력받고, 질의 분석(Query Parsing)을 통해 상기 질의에 포함된 조건을 추출하고, 상 기 조건, 상기 원본 신경망 모델, 상기 대체 블록, 상기 매핑 관계 및 상기 프로파일링 정보를 기초로 최종 모델 을 생성하는 질의처리 단계를 포함한다."}
{"patent_id": "10-2022-0128649", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 내지 기계학습 방법 및 장치에 관한 것으로서, 구체적으로는 신경망 모델의 경량화 및 질의 에 부합하는 경량 모델의 탐색 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2022-0128649", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "딥러닝 기술이 보편화되면서 다양한 경량 디바이스를 타겟으로 최적화된 신경망 모델을 필요로 하는 경우가 많 아지고 있다. 각각의 타겟 디바이스 및 환경에 대하여 최적화된 신경망 모델을 별도로 찾는 경우 탐색 및 학습 비용이 매우 증가하므로, 상기 비용을 줄이기 위한 다양한 방법들이 제안되었다. 그러나 기존 방법들은 경량 모 델 탐색에 있어서 이미 학습된 다른 모델들의 가중치를 전혀 사용하지 않거나 지식 증류와 같이 간접적으로 활 용하는 방식을 취하고 있어서 여전히 높은 GPU 비용이 요구되고 있다. 즉, 기존 방법들은 다양한 조건들을 만족 시키기 위한 모델을 생성하는 데 여전히 높은 GPU 연산 비용을 요구한다는 한계가 있다. AI 기술의 대중화 및 가치를 증대시키기 위해서는, 낮은 GPU 비용으로 엣지 디바이스의 자원(CPU/ GPU 점유율, 여유 메모리 크기 등)에 최적화된 경량 모델을 동적으로 탐색하는 방법이 필요하다."}
{"patent_id": "10-2022-0128649", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명에서는 원본 신경망 모델을 포함하여 이미 학습된 다양한 신경망 모델들의 서브 네트워크의 교체를 통해 낮은 비용으로 타겟 디바이스/런타임 환경 별로 최적화된 경량 모델을 탐색하는 방법과 장치를 제공하는 것을그 목적으로 한다. 또한, 본 발명은 특정 태스크나 환경에 종속되지 않고, 신경망 모델을 적용할 수 있는 임의의 태스크 및 환경 에서 사용 가능한 경량 모델 탐색 방법 및 장치를 제공하는 것을 그 목적으로 한다. 본 발명의 목적은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 또 다른 목적들은 아래의 기재로 부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0128649", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른, 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법은, 원본 신경망 모델에서 서브 네트워크를 추출하고, 기 학습된 신경망 모델에서 상기 서브 네트워크에 대응하는 대체 블록을 추출하여 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관계를 구성하며, 상기 서브 네트워크와 상기 대체 블록에 관한 성능 정보를 포함하는 프로파일링 정보를 생성하는 사전처리 단계; 및 질의를 입력받고, 질의 분석(Query Parsing)을 통해 상기 질의에 포함된 조건을 추출하고, 상기 조건, 상기 원본 신경망 모델, 상기 대 체 블록, 상기 매핑 관계 및 상기 프로파일링 정보를 기초로 최종 모델을 생성하는 질의처리 단계를 포함한다. 본 발명의 일 실시예에서, 상기 사전처리 단계는, 상기 원본 신경망 모델에서 상기 서브 네트워크를 추출하는 단계; 기 학습된 신경망 모델에서 상기 서브 네트워크에 대응하는 대체 블록을 추출하여 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관계를 구성하는 단계; 및 상기 서브 네트워크와 상기 대체 블록을 기초로 상기 프로 파일링 정보를 생성하는 단계를 포함할 수 있다. 여기서 상기 서브 네트워크는 하나의 연결된 신경망이다. 본 발명의 일 실시예에서, 상기 질의처리 단계는, 상기 질의를 입력받고, 질의 분석을 통해 상기 조건을 추출하 는 단계; 상기 원본 신경망 모델, 상기 대체 블록 및 상기 매핑 관계를 기초로 후보 신경망 모델을 생성하는 단 계; 및 상기 조건과 상기 프로파일링 정보를 기초로 상기 후보 신경망 모델을 평가하고, 상기 후보 신경망 모델 에 대한 평가 결과를 기초로 상기 후보 신경망 모델에서 상기 최종 모델을 선정하는 단계를 포함할 수 있다. 본 발명의 일 실시예에서, 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관계를 구성하는 단계는, 상기 서브 네트워크와 상기 대체 블록 간의 호환 가능성을 판단하고, 상기 호환 가능성에 기초하여 상기 매핑 관계를 구성 하는 것일 수 있다. 이 경우, 상기 호환 가능성은, 상기 서브 네트워크와 상기 대체 블록의 입력과 출력이 각각 동일한 차원 수와 채널 수를 가지고, 상기 서브 네트워크를 데이터가 통과할 경우의 공간 차원 변화와 상기 대 체 블록을 데이터가 통과할 경우의 공간 차원 변화가 동일한 것을 의미한다. 본 발명의 일 실시예에서, 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관계를 구성하는 단계는, 상기 서브 네트워크와 상기 대체 블록 간의 호환 가능성을 판단하고, 상기 대체 블록이 대응되는 서브 네트워크와 입력 채 널 수 및 출력 채널 수 중 적어도 어느 하나가 상이하여 호환 가능성을 만족하지 못할 경우, 가지치기(Pruning) 및 프로젝션 레이어 추가 중 적어도 어느 하나의 방식을 통하여 상기 대체 블록의 채널 수를 조정하는 것일 수 있다. 본 발명의 일 실시예에서, 상기 사전처리 단계는, 상기 매핑 관계를 구성한 이후에, 대체 블록 학습용 데이터, 상기 원본 신경망 모델 및 상기 매핑 관계를 기초로 지식증류 기법을 이용하여 상기 대체 블록을 학습시키고, 상기 서브 네트워크와 상기 학습된 대체 블록에 관한 성능 정보를 포함하는 프로파일링 정보를 생성하는 것일 수 있다. 본 발명의 일 실시예에서, 상기 프로파일링 정보는, 상기 서브 네트워크와 상기 대체 블록의 추론 계산 시간 및 메모리 사용량 중 적어도 어느 하나 또는 이들의 조합을 포함할 수 있다. 본 발명의 일 실시예에서, 상기 조건은, 타겟 플랫폼, 목표 레이턴시(Latency) 및 목표 메모리 사용량 중 적어 도 어느 하나 또는 이들의 조합을 포함할 수 있다. 본 발명의 일 실시예에서, 상기 질의처리 단계는, 최종 모델 학습용 데이터와 상기 원본 신경망 모델을 기초로 지식증류 기법을 이용하여 상기 최종 모델을 학습시키고, 상기 학습된 최종 모델을 출력하는 것일 수 있다. 그리고, 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는, 원본 신경망 모델에서 서브 네트워크를 추출하고, 기 학습된 신경망 모델에서 상기 서브 네트워크에 대응하는 대체 블록을 추출하여 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관계를 구성하며, 상기 서브 네트워크와 상기 대체 블록에 관한 성능 정보를 포함하는 프 로파일링 정보를 생성하는 사전처리 모듈; 및 질의를 입력받고, 질의 분석(Query Parsing)을 통해 상기 질의에 포함된 조건을 추출하고, 상기 조건, 상기 원본 신경망 모델, 상기 대체 블록, 상기 매핑 관계 및 상기 프로파일링 정보를 기초로 최종 모델을 생성하는 질의처리 모듈을 포함한다. 본 발명의 일 실시예에서, 상기 사전처리 모듈은, 상기 원본 신경망 모델에서 상기 서브 네트워크를 추출하는 서브 네트워크 생성부; 기 학습된 신경망 모델에서 상기 서브 네트워크에 대응하는 대체 블록을 추출하여 상기 서브 네트워크와 상기 대체 블록 간의 매핑 관계를 구성하는 대체 블록 생성부; 및 상기 서브 네트워크와 상기 대체 블록을 기초로 상기 프로파일링 정보를 생성하는 프로파일링부를 포함할 수 있다. 여기서 상기 서브 네트 워크는 하나의 연결된 신경망이다. 본 발명의 일 실시예에서, 상기 질의처리 모듈은, 상기 질의를 입력받고, 질의 분석을 통해 상기 조건을 추출하 는 질의 분석부; 상기 원본 신경망 모델, 상기 대체 블록 및 상기 매핑 관계를 기초로 후보 신경망 모델을 생성 하는 후보 모델 생성부; 및 상기 조건과 상기 프로파일링 정보를 기초로 상기 후보 신경망 모델을 평가하고, 상 기 후보 신경망 모델에 대한 평가 결과를 기초로 상기 후보 신경망 모델에서 상기 최종 모델을 선정하는 후보 모델 평가부를 포함할 수 있다. 본 발명의 일 실시예에서, 상기 대체 블록 생성부는, 상기 서브 네트워크와 상기 대체 블록 간의 호환 가능성을 판단하고, 상기 호환 가능성에 기초하여 상기 매핑 관계를 구성할 수 있다. 이 경우 상기 호환 가능성은, 상기 서브 네트워크와 상기 대체 블록의 입력과 출력이 각각 동일한 차원 수와 채널 수를 가지고, 상기 서브 네트워 크를 데이터가 통과할 경우의 공간 차원 변화와 상기 대체 블록을 데이터가 통과할 경우의 공간 차원 변화가 동 일한 것을 의미한다. 본 발명의 일 실시예에서, 상기 대체 블록 생성부는, 상기 서브 네트워크와 상기 대체 블록 간의 호환 가능성을 판단하고, 상기 대체 블록이 대응되는 서브 네트워크와 입력 채널 수 및 출력 채널 수 중 적어도 어느 하나가 상이하여 호환 가능성을 만족하지 못할 경우, 가지치기(Pruning) 및 프로젝션 레이어 추가 중 적어도 어느 하나 의 방식을 통하여 상기 대체 블록의 채널 수를 조정할 수 있다. 본 발명의 일 실시예에서, 상기 사전처리 모듈은, 상기 매핑 관계를 구성한 이후에, 대체 블록 학습용 데이터, 상기 원본 신경망 모델 및 상기 매핑 관계를 기초로 지식증류 기법을 이용하여 상기 대체 블록을 학습시키고, 상기 서브 네트워크와 상기 학습된 대체 블록에 관한 성능 정보를 포함하는 프로파일링 정보를 생성할 수 있다. 본 발명의 일 실시예에서, 상기 프로파일링 정보는, 상기 서브 네트워크와 상기 대체 블록의 추론 계산 시간 및 메모리 사용량 중 적어도 어느 하나 또는 이들의 조합을 포함할 수 있다. 본 발명의 일 실시예에서, 상기 조건은, 타겟 플랫폼, 목표 레이턴시(Latency) 및 목표 메모리 사용량 중 적어 도 어느 하나 또는 이들의 조합을 포함할 수 있다. 본 발명의 일 실시예에서, 상기 질의처리 모듈은, 최종 모델 학습용 데이터와 상기 원본 신경망 모델을 기초로 지식증류 기법을 이용하여 상기 최종 모델을 학습시키고, 상기 학습된 최종 모델을 출력할 수 있다. 본 발명의 일 실시예에서, 상기 대체 블록 생성부는, 상기 호환 가능성을 가지면서도 상기 서브 네트워크와 다 른 구조를 가진 대체 블록을 상기 기 학습된 신경망 모델에서 추출하여 상기 매핑 관계를 구성할 수 있다. 이 경우, 상기 다른 구조는, 파라미터, 레이어의 수, 레이어의 배치, 레이어 간의 연결 구조 및 변환 함수 중 적어 도 어느 하나의 기준 또는 이들의 조합된 기준이 상이한 것을 의미한다."}
{"patent_id": "10-2022-0128649", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 기 학습된 다양한 신경망 모델에서 얻어지는 서브 네트워크의 교체를 통하여 최종 모델을 도출하는 경량 모델 탐색 방법 및 장치로서, 다음의 효과를 기대할 수 있다. 기존 탐색 방법들은 탐색 공간을 정의하기 위해 어떤 레이어, 블록 등을 사용할 것인지 등 다양한 파라미터 를 정의하고 탐색해야 했으나, 본 발명은 기존의 학습된 모델들을 활용한다. 즉, 본 발명은 기존의 다른 연구에 서 이미 효과가 검증된 모듈 블록을 탐색 공간에 포함시켜 사용하기 때문에 탐색 공간을 정의하기 위한 비용이 거의 0에 가깝다. 요컨대 본 발명은 불필요한 탐색 공간 정의를 줄일 수 있는 효과가 있다. 본 발명은 최종 출력 모델을 구성하는데 있어서, 랜덤하게 초기화된 가중치가 아닌, 다른 학습에서 계산된 가중치를 사용하기 때문에 재학습 과정 및 지식 증류를 통한 대체 블록 학습 과정을 더 빠르게 수행되도록 한다. 따라서 본 발명은 질의처리 시간을 크게 줄일 수 있으므로 기존 기술과 대비하여 더 신속하게 조건에 맞 는 모델을 출력할 수 있다. 기존의 모델 지연시간 예측 방법의 경우 레이어 레벨을 기준으로 측정된 실제 지연시간을 기초로 전체 모델 의 지연시간을 예측하지만, 복잡하게 레이어들이 연결될 경우 예측의 정확도가 낮아질 수 있다는 문제가 있다. 본 발명은 레이어가 아닌 블록 레벨을 기준으로 지연시간을 예측하기 때문에 기존 예측 방법에 대비하여 간단하 면서도 정확하게 전체 모델의 지연시간을 예측할 수 있다는 효과가 있다. 본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은"}
{"patent_id": "10-2022-0128649", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0128649", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 신경망 모델의 경량화 및 질의에 부합하는 경량 모델의 탐색 방법 및 장치에 관한 것이다. 구체적으 로 본 발명은 엣지 디바이스와 같이 제한된 자원을 가진 디바이스에서도 계산이 가능한 경량 모델을 이미 학습 된 신경망 모델의 서브 네트워크 교체를 통하여 구성하는 방법 및 장치에 관한 것이다. 본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2022-0128649", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 한편, 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포 함한다. 명세서에서 사용되는 \"포함한다(comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성소자, 단계, 동작 및/또는 소자는 하나 이상의 다른 구성소자, 단계, 동작 및/또는 소자의 존재 또는 추가를 배제하지 않는다. 본 발명을 설명함에 있어서, 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 이하, 본 발명의 실시예를 첨부한 도면들을 참조하여 상세히 설명한다. 본 발명을 설명함에 있어 전체적인 이해 를 용이하게 하기 위하여 도면 번호에 상관없이 동일한 수단에 대해서는 동일한 참조 번호를 사용하기로 한다. 도 1은 사전처리(Preprocessing) 단계와 질의처리(Query Processing) 단계로 구성된 경량 모델 탐색 방법의 예 시도이다. 상기 탐색 방법은 입력 받은 원본 신경망 모델(N)과 기 학습된 모델들의 집합(P)을 입력으로 하는 사 전처리(Preprocessing) 단계와 타겟 환경에 대한 기술(記述, Description) 및 제약 조건(Constraint)을 포함한 질의를 처리하는(Query Processing) 단계를 포함한다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 원본 신경망 모델 N과 기 학습된 모델들의 집합 P를 가지고 사전처리를 수행한다. 사전처리 단계에 대해서는 도 2를 참조하여 상세히 후술한다. 이 후, 타겟 플랫폼에 대한 조건이 포함된 질의가 주어졌을 때, 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 질의처리 단계에서 사전처리로 얻은 정보에 기초하여 해당 질의에 대한 최적 모델을 탐색 하여 최종 결과 모델 NQ를 출력한다. 질의처리 단계에 대해서는 도 7을 참조하여 상세히 후술한다. 본 발명에서 사용되는 기 학습된 모델들에 대한 제한 조건은 없고, 임의의 어떠한 모델도 여기에 포함될 수 있다. 도 2는 도 1의 사전처리 단계를 상세히 나타낸 도면으로서, 원본 신경망 모델(N)의 교체 가능한 서브 네트 워크를 탐색하고, 대체 블록을 생성 및 학습하는 방법의 예시도이다. 먼저 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 원본 신경망 모델 N을 입력받고 N에서 교체 가능한 서브 네트워크들의 집합 SN을 생성하는 과정을 수행한다. '교체 가능한 서브 네트워크'란 원본 신경망 모델의 서브 네트워크이면서, 분리되지 않은 하나의 연결된 네트워크(신경망)인 것을 의미한다. 다음으 로 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 SN의 교체 가능한 블록들에 호환 가능한 서브 네트 워크들을 후보 블록들에서 샘플링하여 대체 블록을 생성하는 과정을 수행한다. 이 과정에서 얻어진 대 체 블록 정보 집합 AN의 i번째 요소는 편의상 (Oi, Ai)로 표현되며, Oi는 원본 신경망 모델(N)의 교체 가능 한 블록으로서 SN에 속하며, Ai는 Oi에 대하여 호환 가능한 대체 블록들 중 하나이다. 대체 블록 정보 집합 (AN,26)은 교체 가능한 블록(Si)과 호환 가능한 대체 블록(Bk) 간의 매핑 정보와 함께, 매핑 되는 실제 블록(교 체 가능한 블록(Si)과 호환 가능한 대체 블록(Bk))을 포함하여 구성될 수 있다. SN에 속하는 하나의 서브 네트워 크가 AN의 원소로 여러 번 나타날 수 있다. 즉, AN의 임의의 서로 다른 (Oi, Ai)와 (Oj, Aj)에 대해 Oi와 Oj는 동일한 서브 네트워크일 수 있다. AN의 각 대체 블록 Ai는 함께 매핑된 Oi을 사용하여 지식 증류 기반의 학습 과 정을 통해 학습되며, 이 과정을 거친 대체 블록 정보 집합은 A* N(학습된 대체 블록 정보 집합, 28)으로 표현 된다. 마지막으로 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 A* N과 N을 기초로 프로파일링 과정 을 통해 각 교체 가능한 블록과 각 호환 가능한 대체 블록에 대해 소요되는 추론 계산 시간, 메모리 사용량 등을 측정한다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 프로파일링 과정에서 계산된 결과 를 프로파일링 정보인 Cost에 저장한다. Cost는 추후 질의가 들어왔을 때 사용된다. 도 3은 원본 신경망 모델(N) 내부의 교체 가능한 서브 네트워크를 추출하는 과정의 예시도이다. 즉, 도 3은 도 2의 서브 네트워크 생성 과정을 구체화한 것이다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 원본 신경망 모델 N에서 랜덤 시작 위치를 지정하 고, 시작 위치로부터 모델의 그래프 구조를 입력에서 출력 방향으로 탐색(Traverse)하되 랜덤하게 종료하는 방 식으로 랜덤 서브 네트워크를 구성할 수 있다. 도 3에서 a 내지 h는 레이어(Layer)를 의미한다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치가 랜덤 서브 네트워크를 구성할 때, 서브 네트워크 블록은 1개의 입력 을 가질 수 있고, 2개 이상의 입력을 가질 수도 있다. 또한, 서브 네트워크 블록은 1개의 출력을 가질 수 있고, 2개 이상의 출력을 가질 수도 있다. Si, Sj, Sk는 이렇게 생성된 랜덤 서브 네트워크의 예이며, SN에 포함된다. 본 발명은 서브 네트워크 탐색 방법에 제한을 두지 않는다. 따라서 서브 네트워크 탐색을 위해 도 3에서 예시한 방법 외에 다른 방법을 적용할 수도 있다. 도 4는 도 2에서 대체 블록들의 도메인 셋(B, 24)을 생성하는 방법에 대한 예시도이다. P1, …, Pn은 기 학습된 신경망 모델들이며 원본 신경망 모델(N) 또한 신경망 모델 경량화 관점에서 학습된 신경망 모델들에 포함될 수 있다.도 3을 참조하여 설명한 서브 네트워크 탐색 방법과 동일한 방법으로, 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 기 학습된 신경망 모델에서 모델 별로 서브 네트워크를 추출할 수 있다. 기 학습된 신경 망 모델에서 추출된 서브 네트워크 Bi는 이후 대체 블록으로 사용된다. Bi는 다시 여러 신경망 모델 경 량화 기법(예: Channel Pruning, Decomposition, Weight Pruning, Quantization 등)을 다양한 스케일(scale)로 적용하여 추가적으로 활용할 수 있는 대체 블록들을 만들어내는 오프라인 익스팬션(Offline Expansion) 과정을 거친다. Bj, Bk, Bl은 오프라인 익스팬션 과정을 통해 생성된 대체 블록의 예이다. 이렇게 얻어진 모든 서브 네트워크(대체 블록)들은 하나의 도메인 셋 B로 구성되어 도 2의 대체 블록 생성 과정에서 사용된다. 도 5는 도 2에서 원본 신경망 모델(N)의 교체 가능한 서브 네트워크를 대신하여 사용 가능한 대체 블록을 생성 하는 과정에 대한 예시도이다. 도 5는 도 2의 원본 신경망 모델(N)의 서브 네트워크에 호환 가능한 대체 블 록을 생성하는 과정 중 입출력 채널 값을 보정하는 과정을 구체화한 것이다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는, 대체 블록 생성 과정에서 앞서 구해진 SN과 B에 속하 는 블록들로 구성되는 모든 쌍을 활용할 수도 있고, 랜덤하게 각각 Si, Bk을 샘플링하여 두 블록 간에 호환 가능 성을 검사한 후 호환 가능하면 AN에 (Si, Bk)을 추가할 수도 있다. 또한, 경량 모델 탐색 장치는 원본 신경 망 모델(N)의 서브 네트워크와 호환 가능하면서도 다른 구조(예: 파라미터, 레이어의 수, 레이어의 배치, 레이 어 간의 연결 구조, 변환 함수 등)를 가진 대체 블록(Bk)을 Si와 매핑하여 대체 블록 정보 집합(AN)에 추가할 수 있다. 여기서 원본 신경망 모델 N의 임의의 서브 네트워크 Si에 대해 어떤 대체 블록 Bk가 호환 가능하다는 것은, 같은 차원 수 및 채널 수를 갖는 Si와 Bk의 입출력(입력 텐서와 출력 텐서)들 간에 일대일 대응이 가능하 고, 두 블록을 통과했을 때 데이터의 공간(Spatial) 차원 변화가 동일한 것을 의미한다. 만약 Si에 대한 Bk 의 호환 가능 조건 중에서 입출력의 채널 수(채널 크기) 동일 조건이 충족되지 않는 경우, 대체 블록의 채널들 을 가지치기(Pruning, 52 및 53)하거나 프로젝션(Projection) 레이어를 추가하는 방식(54 및 55)을 통하여 대체 블록(Bi, Bk)이 원본 신경망 모델의 서브 네트워크(Si)에 호환되도록 만들 수 있다. 프로젝션(Projection) 레이 어를 추가하는 경우, 해당 레이어의 초기 가중치는 랜덤 값으로 정한 후 학습을 통해 계산된다. 도 5에서 B'i은 채널 가지치기를 통해 대체 블록을 조정한 결과이고, B'k는 입출력의 전후에 프로젝션 (Projection) 레이어를 추가하여 맞지 않는 채널을 조정한 경우이다. 후자의 경우 B'k의 입력 또는 출력 채널이 Sj의 입력 또는 출력 채널보다 작은 경우에 활용된다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는, 만약 Sj 대비 입력 채널은 큰데, 출력 채널이 작은 경우, 채널 가지치기와 프로젝션 레이어 삽입 을 복합적으로 적용하여 호환 가능한 대체 블록을 생성할 수 있다. 서브 네트워크와 호환 가능한 대체 블록의 쌍(56, 57)은 AN의 원소가 된다. 도 6은 대체 블록을 지식 증류를 통해 학습시키는 과정의 예시도이다. 도 6은 도 2에서 찾은 대체 블록을 지식 증류를 통해 학습시키는 과정을 구체화한 것이다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는, 학습 데이터에서 얻어진 입력 데이터를 바탕으로, 원본 신경망 모델 N의 서브 네트워크 Oj의 입력에 해당하는 부분을 Aj도 처리하게 하고, 둘 간의 출력을 비교하여 손실함수를 계산한 후 지식증류 손실함수 값(지식증류 손실, Distillation Loss) 를 최소화하는 방식(지식 증류)으로 Aj를 학습시킨다. 여기서 두 서브 네트워크들(Oj, Aj)의 출력에 기초한 지식 증류 손실(Distillation Loss)은 쿨백 라이블러 발산(Kullback-Leibler Divergence)이나 평균 제곱 오차(Mean Squared Error) 등 다양한 손실함수를 이용하여 산출될 수 있다. 본 발명에서는 지식증류 손실(Distillation Loss)을 산출하기 위해 사용하는 손실함수에 제한을 두지 않는다. 도 7은 도 1의 질의처리 단계를 상세히 나타낸 도면으로서, 학습된 대체 블록 정보 집합(A* N)과 원본 신경망 모델(N)을 미리 로드한 후 질의가 주어졌을 때 최적 모델을 탐색하는 방법의 예시도이다. 먼저 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 학습된 대체 블록 정보 집합(A* N,71), 원본 신경 망 모델 N 및 프로파일링 정보(Cost, 74)를 로드한다. 경량 모델 탐색 장치는 질의(Q)가들어오면, 질의를 분석하여, 질의(Q)에 포함된 조건을 추출한다. 예를 들어, 질의(Q)는 타겟 플랫폼, 목표 레이 턴시(Latency), 목표 메모리 사용량에 관한 조건을 포함할 수 있다. 참고로, 타겟 플랫폼에 관한 조건은 모델이 동작하는 기기(device) 또는 런타임 환경에 관한 것일 수 있다. 목표 레이턴시에 관한 조건은 최종 모델의 추론 계산에 대한 레이턴시 목표값을 말한다. 경량 모델 탐색 장치는 타겟 플랫폼, 목표 레이턴시(Latency), 목 표 메모리 사용량 등의 조건을 포함한 질의 Q가 들어오면, 질의를 분석하고, 질의(Q)에 포함된 조건을 만족하는 최적 모델을 찾는 과정을 수행한다. 최적 모델 탐색 과정은 일반적인 최적화 과정으로서, 후 보 모델 생성 과정과 평가 과정을 포함하여 구성된다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장 치는 후보 모델 생성 과정에서 후보 출력 신경망 모델을 생성한다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 평가 과정에서 주어진 질의(Q)에 기초하여 후보 출력 신경망 모델들을 평가한다. 경 량 모델 탐색 장치는 질의(Q)에 포함된 조건과 프로파일링 정보에 기초하여 후보 출력 신경망 모델을 평가 할 수 있다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 가장 평가 점수가 높게 계산된 모델(N Q)에 대하여 추가 학습 과정을 수행하고, 추가 학습이 완료된 최종 모델 N* Q를 출력한다. 도 8은 호환 가능한 대체 블록을 기반으로 서브 네트워크 대체(Replacement)를 통해 후보 출력 모델을 만들어내 는 과정의 예시도이며, 도 7의 후보 모델 생성 과정을 구체화한 것이다. 먼저 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 학습된 대체 블록 정보 집합 A* N으로부터 부 분집합 Asel을 추출한다. 이 때 부분집합의 원소를 추출하는 방법은 어떠한 방법이든 상관없고, 랜덤 샘 플링 방법도 가능하다. 다만, 이 부분집합에 속하는 임의의 원소 (Oi, Ai)에 포함된 서브 네트워크 Oi는, 원본 신경망 모델(N)에서, 이 부분집합에 속하는 다른 원소의 서브 네트워크와 중첩되는 레이어를 갖지 않는다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 이렇게 생성된 Asel를 서브네트워크 대체 과정에 서 사용하는데, 이 과정에서 Asel의 모든 원소 (Oi, Ai)에 대해 원본 신경망 모델(N, 94)에서 Oi 대신 Ai가 사용되도록 경로 재편성 작업(reroute)을 수행한다. 두 블록 Oi, Ai의 입출력들은 일대일 대응으로 매핑 되어있 기 때문에 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 그 대응 관계에 따라 경로 재편성 작업 (reroute)을 수행할 수 있다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 서브 네트워크 대체 과 정을 통해 후보 신경망 모델인 Ncand을 생성한다. 도 9는 도 8의 서브 네트워크 대체 과정을 구체화한 것으로 원본 모델에서 교체 가능한 블록들을 호환되는 대체 블록들로 교체하여 후보 출력 모델을 만들어내는 과정의 예시도이다. 도 9에서 Ai, Aj, Oi, Oj는 Asel에 속하는 (Oi, Ai), (Oj, Aj)의 블록들을 의미하며, 원본 신 경망 모델(N)에서 Oi 대신 Ai가 사용되고, Oj 대신 Aj가 사용된다. 도 9에서 Ok는 A* N 에 속하는 다른 서브 네트워크이지만, 원본 신경망 모델(N, 103)의 f 레이어에서 Oj와 중첩되기 때문에 Asel에 속하지 않는다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 Asel의 모든 원소에 대하여 서브 네트워크 대체 작업을 수행함으로써 출력으로 Ncand를 생성한다. 도 10은 최종적으로 만들어진 출력 모델을 원본 모델의 지식 증류를 통해 추가 학습시키는 과정의 예시도이다. 도 10은 도 7의 추가 학습 과정을 구체화한 것이다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 원본 신경망 모델(N, 115)을 기초로 지식 증류 기법을 이용하여 최종 출력 모델인 NQ를 추가 학습시켜 추가 학습된 최종 출력 모델인 N* Q를 생성한다. 도 10에 도시된 추가 학습 과정은 최종 출력 모델인 NQ에 대한 finetuning(재학습, retraining) 과정 이며, 이 과정에서 지식 증류를 통해 더 안정적으로 학습하는 것을 꾀한다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는, 학습 데이터셋에서 추출한 입력 데이터를 기반으로, NQ를 생성하는데 사용된 Asel의 원소에 대하여, 지식 증류를 위한 티쳐-스튜던트(teacher-student) 간의 비교(지식증류 손실의 산출) 작 업을 수행한다. 예를 들어, 도 10에서는 (Oi, Ai), (Oj, Aj)가 NQ를 생성하기 위해 사용되었기 때문에 Oi와 Ai, Oj와 Aj 간에 지식 증류를 위한 손실(Distillation Loss, 119 및 120)이 정의된다. 도 6과 마찬가지로 손실함수의 종류에는 제한이 없다. 한편, 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 학습 데이터셋에서 추출한 레이블을 기초로 최종 출력 모델 NQ의 태스크 손실을 산출하고, 태스크 손실을 기초로 NQ를 학습시 킬 수 있다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 지식 증류 손실(119,120)과 태스크 손실에 기초 하여 최종 출력 모델 NQ를 학습시킬 수 있다. 도 11 내지 도 13은 본 발명의 일 실시예에 따른 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법을 설명하기 위한 흐름도이다. 본 발명의 일 실시예에 따른 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법은 S200 단 계와 S300 단계를 포함한다. S200 단계는 사전처리 단계이다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 원본 신경망 모델 (N)과 기 학습된 신경망 모델(P)을 기초로, 원본 신경망 모델(N)의 교체 가능한 블록(서브 네트워크)의 집합 (SN)과, 상기 서브 네트워크를 대신할 수 있는 학습된 대체 블록의 쌍의 집합(A* N)과, 교체 가능한 블록과 학습 된 대체 블록에 관한 성능 정보인 프로파일링 정보(Cost)를 생성한다. S200 단계에 관한 상세한 내용은 도 2를 참조하여 전술하였다. S200 단계는 도 12를 참조하여 하기에 상세하게 설명한다. S200 단계는 S210 단계 내지 S240 단계를 포함한다. S210 단계는 서브 네트워크 생성 단계이다. 경량 모델 탐색 장치는 원본 신경망 모델(N)을 기초로 원본 신 경망 모델(N)의 교체 가능한 서브 네트워크를 추출하여, 교체 가능한 서브 네트워크들의 집합(SN)을 생성한다. 경량 모델 탐색 장치는 랜덤 샘플링 기법을 이용하여 교체 가능한 서브 네트워크를 추출할 수 있다. 다만, 본 발명은 서브 네트워크 탐색 방법에 제한을 두지 않는다. S210 단계에 관한 상세한 내용은 도 3을 참조하여 전술하였다. S220 단계는 대체 블록 생성 단계이다. 경량 모델 탐색 장치는 교체 가능한 서브 네트워크들의 집합(SN)과 대체 블록 도메인 셋(B)을 기초로 대체 블록 정보 집합(AN)을 생성한다. 구체적으로, 경량 모델 탐색 장치(40 0)는 기 학습된 신경망 모델(P)을 기초로 대체 블록 도메인 셋(B)을 생성한다. 이 과정에서 경량 모델 탐색 장 치는 신경망 모델 경량화 기법을 이용할 수 있다. 그리고, 경량 모델 탐색 장치는 교체 가능한 서브 네트워크와 대체 블록의 쌍의 집합(대체 블록 정보 집합, AN)을 생성한다. 이때, 경량 모델 탐색 장치는 원본 신경망 모델(N)의 서브 네트워크와 호환 가능하면서도 다른 구조(예: 파라미터, 레이어의 수, 레이어의 배 치, 레이어 간의 연결 구조, 변환 함수 등)를 가진 대체 블록(Bk)을 Si와 매핑하여 대체 블록 정보 집합(AN)에 추가할 수 있다. 전술한 바와 같이, 경량 모델 탐색 장치는 원본 신경망 모델(N)의 서브 네트워크(Sj)와 대체 블록의 채널 수를 일치시키기 위하여 가지치기(Pruning)나 프로젝션(Projection) 레이어 추가 등의 기법을 이용하여 대체 블록의 입출력 채널 수를 변경시킬 수 있다. S220 단계에 관한 상세한 내용은 도 4 및 도 5를 참 조하여 전술하였다. S230 단계는 대체 블록 학습 단계이다. 경량 모델 탐색 장치는 원본 신경망 모델(N)과 대체 블록 정보 집 합(AN)을 기초로 지식 증류 기법을 이용하여 학습된 대체 블록 정보 집합(A* N)을 생성한다. 즉, 경량 모델 탐색 장치는 지식 증류 기법을 이용하여 대체 블록을 학습시켜, 원본 신경망 모델(N)의 서브 네트워크(Sj)와 학 습된 호환 가능한 대체 블록의 쌍으로 구성된 집합(A* N)을 생성한다. S230 단계에서 사용되는 손실 함수에는 제 한이 없으며, 쿨백 라이블러 발산, 평균 제곱근 오차 등 다양한 손실함수가 사용될 수 있다. S230 단계에 관한 상세한 내용은 도 6을 참조하여 전술하였다. S240 단계는 프로파일링 단계이다. 경량 모델 탐색 장치는 학습된 대체 블록 정보 집합(A* N)과 원본 신경망 모델(N)을 기초로 프로파일링을 통해 원본 신경망 모델(N)의 교체 가능한 블록 및 학습된 호환 가능한 대체 블 록에 관한 성능 정보인 프로파일링 정보(Cost)를 생성한다. 예를 들어, 경량 모델 탐색 장치는 각 교체 가 능한 블록과 각 호환 가능한 대체 블록에 대해 소요되는 추론 계산 시간, 메모리 사용량 등을 측정하여 프로파일링 정보(Cost)를 생성한다. S300 단계는 질의처리 단계이다. 먼저 경량 모델 탐색 장치는 학습된 대체 블록 정보 집합(A* N), 원본 신경 망 모델(N) 및 프로파일링 정보(Cost)를 로드한다. 경량 모델 탐색 장치는 질의(Q)를 입력받고 질의 (Q)에 포함된 조건을 질의 분석(Query Parsing)를 통해 추출한다. 경량 모델 탐색 장치는 질의(Q)에 포함 된 조건을 만족하는 최적 모델을 찾는다. 즉, 경량 모델 탐색 장치는 질의(Q)에 포함된 조건, 원본 신경망 모델(N), 학습된 대체 블록 정보 집합(A* N) 및 프로파일링 정보(Cost)를 기초로 질의(Q)에 포함된 조건에 가장 잘 부합하는 최종 모델(N* Q)을 생성한다. S300 단계에 관한 상세한 내용은 도 7을 참조하여 전술하였다. S300 단계는 도 13을 참조하여 하기에 상세하게 설명한다. S300 단계는 S310 단계 내지 S340 단계를 포함한다. S310 단계는 질의 분석 단계이다. 경량 모델 탐색 장치는 질의(Q)가 들어오면, 질의를 분석하여, 질의(Q) 에 포함된 조건을 추출한다. 예를 들어, 질의(Q)는 타겟 플랫폼, 목표 레이턴시(Latency), 목표 메모리 사용량 에 관한 조건을 포함할 수 있다. 참고로, 타겟 플랫폼에 관한 조건은 모델이 동작하는 기기(device) 또는 런타 임 환경에 관한 것일 수 있다. 목표 레이턴시에 관한 조건은 최종 모델의 추론 계산에 대한 레이턴시 목표값을 말한다. S320 단계는 후보 모델 생성 단계이다. 본 단계에서 경량 모델 탐색 장치는 원본 신경망 모델(N)과 학습된 대체 블록 정보 집합(A* N)을 기초로 후보 신경망 모델(Ncand)을 생성한다. 본 단계는 학습된 대체 블록 정보 집합 (A* N)에서 부분집합(Asel)을 추출하는 단계와 원본 신경망 모델(N)의 서브 네트워크(Oi)를 부분집합(Asel)에 포함 되어 있는 대체 블록(Ai)으로 대체하여 후보 신경망 모델(Ncand)을 생성하는 단계로 세분화할 수 있다. 하나의 후 보 신경망 모델(Ncand)을 생성하는 과정에서 원본 신경망 모델(N)의 교체 대상 서브 네트워크는 레이어가 서로 중 첩되어서는 안 된다. 경량 모델 탐색 장치는 부분집합(Asel)의 모든 원소 (Oi, Ai)에 대해 원본 신경망 모델 (N)에서 Oi 대신 Ai가 사용되도록 경로 재편성 작업(reroute)을 수행한다. S320 단계에 관한 상세한 내용은 도 8 및 도 9를 참조하여 전술하였다. S330 단계는 평가 단계이다. 경량 모델 탐색 장치는 질의(Q)에 포함된 조건에 기초하여 후보 신경망 모델 (Ncand)을 평가하고, 가장 평가 점수가 높은 모델을 최종 모델(NQ)로 선정한다. 경량 모델 탐색 장치는 질의 (Q)에 포함된 조건과 프로파일링 정보에 기초하여 후보 출력 신경망 모델을 평가할 수 있다. 한편, S320 단계와 S330 단계를 묶어 최적 모델 탐색 단계라고 명명할 수 있다. 최적 모델 탐색 단계는 일반적 인 최적화 과정이다. S340 단계는 추가 학습 단계이다. 경량 모델 탐색 장치는 학습 데이터셋을 기초로 지식증류 기법을 이용하여 최종 모델(NQ)을 추가 학습시켜 학습된 최종 모델(N* Q)을 생성한다. S230 단계와 마찬가지로, 지식증류 손실(Distillation Loss)이나 태스크 손실(Task Loss)을 계산하기 위한 손실함수의 종류에는 제한이 없다. S340 단계에 관한 상세한 내용은 도 10을 참조하여 전술하였다. 전술한, 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법은, 도면에 제시된 흐름도를 참 조로 하여 설명되었다. 간단히 설명하기 위하여 상기 방법은 일련의 블록들로 도시되고 설명되었으나, 본 발명 은 상기 블록들의 순서에 한정되지 않고, 몇몇 블록들은 다른 블록들과 본 명세서에서 도시되고 기술된 것과 상 이한 순서로 또는 동시에 일어날 수도 있으며, 동일한 또는 유사한 결과를 달성하는 다양한 다른 분기, 흐름 경 로, 및 블록의 순서들이 구현될 수 있다. 또한, 본 명세서에서 기술되는 방법의 구현을 위하여 도시된 모든 블 록들이 요구되지 않을 수도 있다. 한편 도 11 내지 도 13를 참조한 설명에서, 각 단계는 본 발명의 구현예에 따라서, 추가적인 단계들로 더 분할 되거나, 더 적은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서 가 변경될 수도 있다. 아울러, 기타 생략된 내용이라 하더라도 도 1 내지 도 10의 내용은 도 11 내지 도 13의 내용에 적용될 수 있다. 또한, 도 11 내지 도 13의 내용은 도 1 내지 도 10의 내용에 적용될 수 있다. 도 14는 본 발명의 일 실시예에 따른 경량 모델 탐색 장치의 구성을 나타낸 블록도이다. 본 발명의 일 실시예에 따른 경량 모델 탐색 장치는 사전처리 모듈 및 질의처리 모듈을 포함한 다. 사전처리 모듈은 서브 네트워크 생성부, 대체 블록 생성부, 대체 블록 학습부 및 프로파일 링부를 포함한다. 본 발명에 따른 경량 모델 탐색 장치 및 사전처리 모듈의 구성요소들이 도 14 에 도시된 실시예에 한정되는 것은 아니며, 필요에 따라 부가, 변경 또는 삭제될 수 있다. 서브 네트워크 생성부는 원본 신경망 모델(N)을 기초로 원본 신경망 모델(N)의 교체 가능한 서브 네트워크 를 추출하여, 교체 가능한 서브 네트워크들의 집합(SN)을 생성한다. 서브 네트워크 생성부는 랜덤 샘플링 기법을 이용하여 교체 가능한 서브 네트워크를 추출할 수 있다. 다만, 본 발명은 서브 네트워크 생성부의 서브 네트워크 탐색 방법에 제한을 두지 않는다. 서브 네트워크 생성부의 동작에 관한 상세한 내용은 도 3 과 도 3에 관한 설명을 참조하여 이해할 수 있다. 대체 블록 생성부는 교체 가능한 서브 네트워크들의 집합(SN)과 대체 블록 도메인 셋(B)을 기초로 대체 블 록 정보 집합(AN)을 생성한다. 구체적으로, 대체 블록 생성부는 기 학습된 신경망 모델(P)을 기초로 대체 블록 도메인 셋(B)을 생성한다. 이 과정에서 대체 블록 생성부는 신경망 모델 경량화 기법을 이용할 수 있 다. 그리고, 대체 블록 생성부는 교체 가능한 서브 네트워크와 대체 블록의 쌍의 집합(대체 블록 정보 집 합, AN)을 생성한다. 이때, 대체 블록 생성부는 원본 신경망 모델(N)의 서브 네트워크와 호환 가능하면서 도 다른 구조(예: 파라미터, 레이어의 수, 레이어의 배치, 레이어 간의 연결 구조, 변환 함수 등)를 가진 대체 블록(Bk)을 Si와 매핑하여 대체 블록 정보 집합(AN)에 추가할 수 있다. 대체 블록 생성부는 원본 신경망 모 델(N)의 서브 네트워크(Sj)와 대체 블록의 채널 수를 일치시키기 위하여 가지치기(Pruning)나 프로젝션 (Projection) 레이어 추가 등의 기법을 이용하여 대체 블록의 입출력 채널 수를 변경시킬 수 있다. 대체 블록 생성부의 동작에 관한 상세한 내용은 도 4, 도 4에 관한 설명, 도 5 및 도 5에 관한 설명을 참조하여 이해 할 수 있다. 대체 블록 학습부는 원본 신경망 모델(N)과 대체 블록 정보 집합(AN)을 기초로 지식 증류 기법을 이용하여 학습된 대체 블록 정보 집합(A* N)을 생성한다. 즉, 대체 블록 학습부는 지식 증류 기법을 이용하여 대체 블 록을 학습시켜, 원본 신경망 모델(N)의 서브 네트워크(Sj)와 학습된 호환 가능한 대체 블록의 쌍으로 구성된 집 합(A* N)을 생성한다. 대체 블록 학습부가 사용하는 손실 함수에는 제한이 없으며, 쿨백 라이블러 발산, 평 균 제곱근 오차 등 다양한 손실함수가 사용될 수 있다. 대체 블록 학습부의 동작에 관한 상세한 내용은 도 6과 도 6에 관한 설명을 참조하여 이해할 수 있다. 프로파일링부는 학습된 대체 블록 정보 집합(A*N)과 원본 신경망 모델(N)을 기초로 프로파일링을 통해 원 본 신경망 모델(N)의 교체 가능한 블록 및 학습된 호환 가능한 대체 블록에 관한 성능 정보인 프로파일링 정보 (Cost)를 생성한다. 예를 들어, 프로파일링부는 각 교체 가능한 블록과 각 호환 가능한 대체 블록에 대해 소요되는 추론 계산 시간, 메모리 사용량 등을 측정하여 프로파일링 정보(Cost)를 생성한다. 질의처리 모듈은 질의 분석부, 후보 모델 생성부, 후보 모델 평가부 및 추가 학습부 를 포함한다. 본 발명에 따른 경량 모델 탐색 장치에 포함된 질의처리 모듈의 구성요소들이 도 14에 도시된 실시예에 한정되는 것은 아니며, 필요에 따라 부가, 변경 또는 삭제될 수 있다. 질의 분석부는 질의(Q)가 들어오면, 질의를 분석하여, 질의(Q)에 포함된 조건을 추출한다. 예를 들어, 질 의(Q)는 타겟 플랫폼, 목표 레이턴시(Latency), 목표 메모리 사용량에 관한 조건을 포함할 수 있다. 참고로, 타 겟 플랫폼에 관한 조건은 모델이 동작하는 기기(device) 또는 런타임 환경에 관한 것일 수 있다. 목표 레이턴시 에 관한 조건은 최종 모델의 추론 계산에 대한 레이턴시 목표값을 말한다. 후보 모델 생성부는 원본 신경망 모델(N)과 학습된 대체 블록 정보 집합(A* N)을 기초로 후보 신경망 모델 (Ncand)을 생성한다. 후보 모델 생성부의 동작은 학습된 대체 블록 정보 집합(A* N)에서 부분집합(Asel)을 추 출하는 동작과 원본 신경망 모델(N)의 서브 네트워크(Oi)를 부분집합(Asel)에 포함되어 있는 대체 블록(Ai)으로 대체하여 후보 신경망 모델(Ncand)을 생성하는 동작으로 구분할 수 있다. 후보 모델 생성부가 하나의 후보신경망 모델(Ncand)을 생성하는 과정에서 원본 신경망 모델(N)의 교체 대상 서브 네트워크는 레이어가 서로 중첩 되어서는 안 된다. 후보 모델 생성부는 부분집합(Asel)의 모든 원소 (Oi, Ai)에 대해 원본 신경망 모델(N)에 서 Oi 대신 Ai가 사용되도록 경로 재편성 작업(reroute)을 수행한다. 후보 모델 생성부의 동작에 관한 상세 한 내용은 도 8, 도 8에 관한 설명, 도 9 및 도 9에 관한 설명을 참조하여 이해할 수 있다. 후보 모델 평가부는 질의(Q)에 포함된 조건에 기초하여 후보 신경망 모델(Ncand)을 평가하고, 가장 평가 점 수가 높은 모델을 최종 모델(NQ)로 선정한다. 후보 모델 평가부는 질의(Q)에 포함된 조건과 프로파일링 정 보에 기초하여 후보 신경망 모델(Ncand)을 평가할 수 있다. 추가 학습부는 학습 데이터셋을 기초로 지식증류 기법을 이용하여 최종 모델(NQ)을 추가 학습시켜 학 습된 최종 모델(N* Q)을 생성한다. 대체 블록 학습부와 마찬가지로, 추가 학습부가 지식증류 손실 (Distillation Loss)이나 태스크 손실(Task Loss)을 계산하기 위한 손실함수의 종류에는 제한이 없다. 추가 학 습부에 관한 상세한 내용은 도 10 및 도 10에 관한 설명을 참조하여 이해할 수 있다. 도 15는 본 발명의 실시예에 따른 방법을 구현하기 위한 컴퓨터 시스템을 나타낸 블록도이다. 도 15를 참조하면, 컴퓨터 시스템은, 버스를 통해 통신하는 프로세서, 메모리, 입력 인터페이스 장치, 출력 인터페이스 장치 및 저장 장치 중 적어도 하나를 포함할 수 있다. 컴 퓨터 시스템은 또한 네트워크에 결합된 통신 장치를 더 포함할 수 있다. 프로세서는 중앙 처 리 장치(central processing unit, CPU)이거나, 또는 메모리 또는 저장 장치에 저장된 명령을 실 행하는 반도체 장치일 수 있다. 메모리 및 저장 장치는 다양한 형태의 휘발성 또는 비휘발성 저장 매체를 포함할 수 있다. 예를 들어, 메모리는 ROM(read only memory) 및 RAM(random access memory)를 포함할 수 있다. 본 기재의 실시예에서 메모리는 프로세서의 내부 또는 외부에 위치할 수 있고, 메모리는 이미 알려진 다양한 수단을 통해 프로세서와 연결될 수 있다. 메모리는 다양한 형태의 휘발성 또는 비휘발성 저장 매체이며, 예를 들어, 메모리는 읽기 전용 메모리(read-only memory, ROM) 또는 랜덤 액세스 메모리(random access memory, RAM)를 포함할 수 있다. 따라서, 본 발명의 실시예는 컴퓨터에 구현된 방법으로서 구현되거나, 컴퓨터 실행 가능 명령이 저장된 비일시 적 컴퓨터 판독 가능 매체로서 구현될 수 있다. 한 실시예에서, 프로세서에 의해 실행될 때, 컴퓨터 판독 가능 명령은 본 기재의 적어도 하나의 양상에 따른 방법을 수행할 수 있다. 통신 장치는 유선 신호 또는 무선 신호를 송신 또는 수신할 수 있다. 또한, 본 발명의 실시예에 따른 방법은 다양한 컴퓨터 수단을 통해 수행될 수 있는 프로그램 명령 형태로 구현 되어, 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터 판독 가능 매체에 기록되는 프로그램 명령은, 본 발명의 실시예를 위해 특별히 설계되어 구성된 것이거나, 컴퓨터 소프트웨어 분야의 통상의 기술자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체는 프로그램 명령을 저장하고 수행하도록 구성된 하드웨어 장치를 포함할 수 있다. 예를 들 어, 컴퓨터 판독 가능 기록 매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광 기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 롬(ROM), 램(RAM), 플래시 메모리 등일 수 있다. 프로그램 명령은 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라, 인터프리터 등을 통해 컴퓨터에 의해 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 한편, 도 1 내지 도 13의 내용은 도 14 및 도 15의 내용에 적용될 수 있다. 또한, 도 14 및 도 15의 내용은 도 1 내지 도 13의 내용에 적용될 수 있다. 참고로, 본 발명의 실시예에 따른 구성 요소들은 소프트웨어 또는 DSP(digital signal processor), FPGA(Field Programmable Gate Array) 또는 ASIC(Application Specific Integrated Circuit)와 같은 하드웨어 형태로 구현 될 수 있으며, 소정의 역할들을 수행할 수 있다. 그렇지만 '구성 요소들'은 소프트웨어 또는 하드웨어에 한정되는 의미는 아니며, 각 구성 요소는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 구성 요소는 소프트웨어 구성 요소들, 객체지향 소프트웨어 구성 요소들, 클래스 구성 요소 들 및 태스크 구성 요소들과 같은 구성 요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로 그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로 코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테 이블들, 어레이들 및 변수들을 포함한다. 구성 요소들과 해당 구성 요소들 안에서 제공되는 기능은 더 작은 수의 구성 요소들로 결합되거나 추가적인 구 성 요소들로 더 분리될 수 있다. 한편, 흐름도 도면들의 각 블록과 흐름도 도면들의 조합들은 컴퓨터 프로그램 인스트럭션들에 의해 수행될 수 있음을 이해할 수 있을 것이다. 이들 컴퓨터 프로그램 인스트럭션들은 범용 컴퓨터, 특수용 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서에 탑재될 수 있으므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서를 통해 수행되는 그 인스트럭션들이 흐름도 블록(들)에서 설명된 기능들을 수 행하는 수단을 생성하게 된다. 컴퓨터 프로그램 인스트럭션들은 컴퓨터 또는 기타 프로그램 가능한 데이터 프로 세싱 장비 상에 탑재되는 것도 가능하므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에서 일 련의 동작 단계들이 수행되어 컴퓨터로 실행되는 프로세스를 생성해서 컴퓨터 또는 기타 프로그램 가능한 데이 터 프로세싱 장비를 수행하는 인스트럭션들은 흐름도 블록(들)에서 설명된 기능들을 실행하기 위한 단계들을 제 공하는 것도 가능하다. 또한, 각 블록은 특정된 논리적 기능(들)을 실행하기 위한 하나 이상의 실행 가능한 인스트럭션들을 포함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있다. 또, 몇 가지 대체 실행 예들에서는 블록들에서 언급된 기 능들이 순서를 벗어나서 발생하는 것도 가능함을 주목해야 한다. 예컨대, 잇달아 도시되어 있는 두 개의 블록들 은 사실 실질적으로 동시에 수행되는 것도 가능하고 또는 그 블록들이 때때로 해당하는 기능에 따라 역순으로 수행되는 것도 가능하다. 본 실시예에서 사용되는 '~부' 또는 '~모듈'이라는 용어는 소프트웨어 또는 FPGA또는 ASIC과 같은 하드웨어 구 성요소를 의미하며, '~부' 또는 '~모듈'은 어떤 역할들을 수행한다. 그렇지만 '~부' 또는 '~모듈'은 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. '~부' 또는 '~모듈'은 어드레싱할 수 있는 저장 매체에 있도록 구성 될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 '~부' 또 는 '~모듈'은 소프트웨어 구성요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소 들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들, 및 변수 들을 포함한다. 구성요소들과 '~부' 또는 '~모듈'들 안에서 제공되는 기능은 더 작은 수의 구성요소들 및 '~부' 또는 '~모듈'들로 결합되거나 추가적인 구성요소들과 '~부' 또는 '~모듈'들로 더 분리될 수 있다. 뿐만 아니라, 구성요소들 및 '~부' 또는 '~모듈'들은 디바이스 또는 보안 멀티미디어카드 내의 하나 또는 그 이상의 CPU들을 재생시키도록 구현될 수도 있다. 이상, 본 발명의 구성에 대하여 첨부 도면을 참조하여 상세히 설명하였으나, 이는 예시에 불과한 것으로서, 본"}
{"patent_id": "10-2022-0128649", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "발명이 속하는 기술분야에 통상의 지식을 가진 자라면 본 발명의 기술적 사상의 범위 내에서 다양한 변형과 변 경이 가능함은 물론이다. 따라서 본 발명의 보호범위는 상기 상세한 설명보다는 후술한 특허청구범위에 의하여 정해지며, 특허청구의 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태는 본 발명의 기 술적 범위에 포함되는 것으로 해석되어야 한다."}
{"patent_id": "10-2022-0128649", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 사전처리(Preprocessing) 단계와 질의처리(Query Processing) 단계로 구성된 모델 탐색 방법의 예시도. 도 2는 도 1의 사전처리 단계를 상세히 나타낸 도면으로서, 원본 신경망 모델의 교체 가능한 서브 네트워크를 탐색하고, 대체 블록을 생성 및 학습하는 방법의 예시도. 도 3은 원본 신경망 모델 내부의 교체 가능한 서브 네트워크를 추출하는 과정의 예시도. 도 4는 도 2에서 대체 블록들의 도메인 셋(B)을 생성하는 방법에 대한 예시도. 도 5는 도 2에서 원본 신경망 모델의 교체 가능한 서브 네트워크 대신 사용 가능한 대체 블록을 만들어내는 과 정에 대한 예시도. 도 6은 대체 블록을 지식 증류를 통해 학습시키는 과정의 예시도. 도 7은 도 1의 질의처리 단계를 상세히 나타낸 도면으로서, 학습된 대체 블록 정보 집합과 원본 신경망 모델을 미리 로드 한 후 질의가 주어졌을 때 최적 모델을 탐색하는 방법의 예시도. 도 8 은 대체 블록 정보에서 선택된 함께 사용 가능한 대체 블록들을 기반으로 서브 네트워크 대체 (Replacement)를 통해 후보 출력 모델을 만들어내는 과정의 예시도. 도 9는 도 8의 서브 네트워크 대체 과정을 구체화한 것으로 원본 모델에서 교체 가능한 블록들을 호환되는 대체 블록들로 교체하여 후보 출력 모델을 만들어내는 과정의 예시도. 도 10은 최종적으로 만들어진 출력 모델을 원본 모델의 지식 증류를 통해 추가 학습시키는 과정의 예시도. 도 11 내지 도 13은 본 발명의 일 실시예에 따른 학습된 신경망 모델의 서브 네트워크 교체를 통한 경량 모델 탐색 방법을 설명하기 위한 흐름도. 도 14는 본 발명의 일 실시예에 따른 경량 모델 탐색 장치의 구성을 나타낸 블록도. 도 15는 본 발명의 실시예에 따른 방법을 구현하기 위한 컴퓨터 시스템을 나타낸 블록도."}
