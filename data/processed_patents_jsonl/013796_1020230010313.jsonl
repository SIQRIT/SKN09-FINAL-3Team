{"patent_id": "10-2023-0010313", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0053498", "출원번호": "10-2023-0010313", "발명의 명칭": "전자 장치 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "전용권"}}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,입력 벡터를 생성하는 제1 생성기, 합성 데이터(synthetic data)를 생성하는 제2 생성기 및 상기 합성 데이터를분석하는 제1 학습 모델을 포함하는 학습 모듈을 저장하는 메모리;상기 메모리와 연결되어 상기 전자 장치를 제어하는 적어도 하나의 프로세서;를 포함하고,상기 적어도 하나의 프로세서는,상기 제1 생성기를 통해 입력 벡터를 획득하고,상기 입력 벡터를 상기 제2 생성기에 입력함으로써, 상기 입력 벡터에 대응되는 합성 데이터를 획득하고,상기 합성 데이터를 상기 제1 학습 모델에 입력함으로써, 상기 합성 데이터를 분석한 출력 데이터를 획득하고,상기 출력 데이터에 기초하여 상기 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습하는, 전자 장치."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 적어도 하나의 프로세서는,상기 출력 데이터에 기초하여 손실값을 획득하고,상기 손실값이 최소가 되도록 상기 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습하는, 전자 장치."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 출력 데이터는,상기 합성 데이터의 통계적 특성 데이터를 포함하는, 전자 장치."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 출력 데이터는,상기 합성 데이터의 평균값 및 표준 편차값을 포함하고,상기 적어도 하나의 프로세서는,상기 합성 데이터의 상기 평균값 및 상기 제1 학습 모델의 BN(Batch Normalization) 레이어의 평균값의 제1 차이값을 획득하고,상기 합성 데이터의 상기 표준 편차값 및 상기 제1 학습 모델의 BN(Batch Normalization) 레이어의 표준 편차값의 제2 차이값을 획득하고,상기 제1 차이값 및 상기 제2 차이값에 기초하여 상기 손실값을 획득하는, 전자 장치."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,공개특허 10-2024-0053498-3-상기 적어도 하나의 프로세서는,상기 제1 학습 모델에 포함된 적어도 하나의 컨볼루션 레이어의 스트라이드(stride) 데이터를 획득하고,상기 적어도 하나의 컨볼루션 레이어 중 상기 스트라이드 데이터의 크기가 2 이상인 컨볼루션 레이어가 식별되면, 상기 식별된 컨볼루션 레이어를 스윙(swing) 컨볼루션으로 대체하고,상기 스윙 컨볼루션 레이어는,패딩 데이터에 기초하여 연산의 대상을 랜덤하게 선택하는 컨볼루션 레이어인, 전자 장치."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 스윙 컨볼루션 레이어는,제1 데이터가 입력되면, 상기 제1 데이터에 패딩 데이터를 추가하여 제2 데이터를 획득하는 동작,상기 제1 데이터의 크기에 기초하여 상기 제2 데이터에서 일부 데이터 영역을 선택하여 제3 데이터를 획득하는동작, 및상기 제3 데이터 및 상기 식별된 컨볼루션 레이어의 커널 데이터에 기초하여 컨볼루션 연산을 수행하는 동작을포함하는 레이어인, 전자 장치."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 제1 생성기는,적어도 하나의 파라미터에 기초하여 잠재 벡터(latent vector)를 생성하는 생성기이고,상기 제1 생성기에 포함된 적어도 하나의 파라미터는,사용자에 의해 설정된 타겟과 관련된 합성 데이터를 생성하는데 이용되는 파라미터인, 전자 장치."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 합성 데이터는,사용자에 의해 설정된 타겟과 관련된 이미지 데이터인, 전자 장치."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 적어도 하나의 프로세서는,상기 제1 학습 모델을 양자화(quantization)함으로써 제2 학습 모델을 획득하고,상기 제2 학습 모델은 상기 제1 학습 모델이 압축된 모델인, 전자 장치."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 전자 장치는,통신 인터페이스;를 더 포함하고,상기 적어도 하나의 프로세서는,상기 통신 인터페이스를 통해, 상기 제2 학습 모델을 외부 장치에 전송하는, 전자 장치.공개특허 10-2024-0053498-4-청구항 11 입력 벡터를 생성하는 제1 생성기, 합성 데이터(synthetic data)를 생성하는 제2 생성기 및 상기 합성 데이터를분석하는 제1 학습 모델을 포함하는 학습 모듈을 저장하는 전자 장치의 제어 방법에 있어서,상기 제1 생성기를 통해 입력 벡터를 획득하는 단계;상기 입력 벡터를 상기 제2 생성기에 입력함으로써, 상기 입력 벡터에 대응되는 합성 데이터를 획득하는 단계;상기 합성 데이터를 상기 제1 학습 모델에 입력함으로써, 상기 합성 데이터를 분석한 출력 데이터를 획득하는단계; 및상기 출력 데이터에 기초하여 상기 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습하는 단계;를 포함하는, 제어 방법."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 학습하는 단계는,상기 출력 데이터에 기초하여 손실값을 획득하고,상기 손실값이 최소가 되도록 상기 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습하는, 제어 방법."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 출력 데이터는,상기 합성 데이터의 통계적 특성 데이터를 포함하는, 제어 방법."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 출력 데이터는,상기 합성 데이터의 평균값 및 표준 편차값을 포함하고,상기 손실값을 획득하는 단계는,상기 합성 데이터의 상기 평균값 및 상기 제1 학습 모델의 BN(Batch Normalization) 레이어의 평균값의 제1 차이값을 획득하고,상기 합성 데이터의 상기 표준 편차값 및 상기 제1 학습 모델의 BN(Batch Normalization) 레이어의 표준 편차값의 제2 차이값을 획득하고,상기 제1 차이값 및 상기 제2 차이값에 기초하여 상기 손실값을 획득하는, 제어 방법."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 제어 방법은,상기 제1 학습 모델에 포함된 적어도 하나의 컨볼루션 레이어의 스트라이드(stride) 데이터를 획득하는 단계;상기 적어도 하나의 컨볼루션 레이어 중 상기 스트라이드 데이터의 크기가 2 이상인 컨볼루션 레이어가 식별되면, 상기 식별된 컨볼루션 레이어를 스윙(swing) 컨볼루션으로 대체하는 단계;를 더 포함하고,상기 스윙 컨볼루션 레이어는,패딩 데이터에 기초하여 연산의 대상을 랜덤하게 선택하는 컨볼루션 레이어인, 제어 방법.공개특허 10-2024-0053498-5-청구항 16 제15항에 있어서,상기 스윙 컨볼루션 레이어는,제1 데이터가 입력되면, 상기 제1 데이터에 패딩 데이터를 추가하여 제2 데이터를 획득하는 동작,상기 제1 데이터의 크기에 기초하여 상기 제2 데이터에서 일부 데이터 영역을 선택하여 제3 데이터를 획득하는동작, 및상기 제3 데이터 및 상기 식별된 컨볼루션 레이어의 커널 데이터에 기초하여 컨볼루션 연산을 수행하는 동작을포함하는 레이어인, 제어 방법."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항에 있어서,상기 제1 생성기는,적어도 하나의 파라미터에 기초하여 잠재 벡터(latent vector)를 생성하는 생성기이고,상기 제1 생성기에 포함된 적어도 하나의 파라미터는,사용자에 의해 설정된 타겟과 관련된 합성 데이터를 생성하는데 이용되는 파라미터인, 제어 방법."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항에 있어서,상기 합성 데이터는,사용자에 의해 설정된 타겟과 관련된 이미지 데이터인, 제어 방법."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항에 있어서,상기 제어 방법은,상기 제1 학습 모델을 양자화(quantization)함으로써 제2 학습 모델을 획득하는 단계;를 더 포함하고,상기 제2 학습 모델은 상기 제1 학습 모델이 압축된 모델인, 제어 방법."}
{"patent_id": "10-2023-0010313", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서, 상기 제어 방법은,상기 제2 학습 모델을 외부 장치에 전송하는 단계;를 더 포함하는, 제어 방법."}
{"patent_id": "10-2023-0010313", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 전자 장치는 입력 벡터를 생성하는 제1 생성기, 합성 데이터(synthetic data)를 생성하는 제2 생성기 및 합성 데이터를 분석하는 제1 학습 모델을 포함하는 학습 모듈을 저장하는 메모리, 메모리와 연결되어 전자 장치를 제 어하는 적어도 하나의 프로세서를 포함하고, 적어도 하나의 프로세서는 제1 생성기를 통해 입력 벡터를 획득하고, 입력 벡터를 제2 생성기에 입력함으로써, 입력 벡터에 대응되는 합성 데이터를 획득하고, 합성 데이터 를 제1 학습 모델에 입력함으로써, 합성 데이터를 분석한 출력 데이터를 획득하고, 출력 데이터에 기초하여 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습한다."}
{"patent_id": "10-2023-0010313", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 그 제어방법에 관한 것으로, 더욱 상세하게는 합성 데이터(synthetic data)를 이용하여 인공 지능 모델을 학습하는 전자 장치 및 그 제어방법에 대한 것이다."}
{"patent_id": "10-2023-0010313", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "생성기(Generator)는 입력 벡터에 기초하여 가상 데이터를 생성할 수 있다. 가상 데이터는 실제 데이터가 아닌 생성기(Generator)에 의하여 만들어진 데이터를 의미할 수 있다. 실제 데이터가 없는 경우 가상 데이터를 이용하여 인공 지능 네트워크가 학습될 수 있다. 보안 또는 비용의 문제로 인하여 실제 데이터를 이용하여 학습 동작을 수행할 수 없는 경우, 가상 데이터가 이 용될 수 있다. 하지만, 가상 데이터를 이용하는 경우 실제 데이터를 이용하는 경우보다 학습 정확도가 떨어지는 문제점이 있었다. 또한, 가상 데이터를 이용하여 학습 동작을 수행하는 경우 스트라이드 크기에 따라 정보 손실이 발생하고, 학습 성능이 떨어지는 문제점이 있었다. 또한, 인공 지능 네트워크에서 이용되는 사전 학습된 모델(pre trained model)의 저장 크기가 커져 단말 장치 (예를 들어, 모바일 기기 등)에서 이용되기 어렵다는 문제점이 있었다."}
{"patent_id": "10-2023-0010313", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시는 상술한 문제를 개선하기 위해 고안된 것으로, 본 개시의 목적은 입력 벡터를 생성하는 제1 생성기, 합성 데이터를 생성하는 제2 생성기, 합성 데이터를 분석하는 제1 학습 모델을 포함하는 학습 모듈에 대하여 제 1 생성기와 관련된 파라미터 및 제2 생성기와 관련된 파라미터를 학습하는 전자 장치 및 그의 제어 방법을 제공 함에 있다. 다양한 실시 예에 따른 전자 장치는 입력 벡터를 생성하는 제1 생성기, 합성 데이터(synthetic data)를 생성하 는 제2 생성기 및 상기 합성 데이터를 분석하는 제1 학습 모델을 포함하는 학습 모듈을 저장하는 메모리, 상기 메모리와 연결되어 상기 전자 장치를 제어하는 적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세 서는 상기 제1 생성기를 통해 입력 벡터를 획득하고, 상기 입력 벡터를 상기 제2 생성기에 입력함으로써, 상기 입력 벡터에 대응되는 합성 데이터를 획득하고, 상기 합성 데이터를 상기 제1 학습 모델에 입력함으로써, 상기 합성 데이터를 분석한 출력 데이터를 획득하고, 상기 출력 데이터에 기초하여 상기 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습한다. 한편, 상기 적어도 하나의 프로세서는 상기 출력 데이터에 기초하여 손실값을 획득하고, 상기 손실값이 최소가 되도록 상기 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습할 수 있다. 한편, 상기 출력 데이터는 상기 합성 데이터의 통계적 특성 데이터를 포함할 수 있다. 한편, 상기 출력 데이터는 상기 합성 데이터의 평균값 및 표준 편차값을 포함할 수 있고, 상기 적어도 하나의 프로세서는 상기 합성 데이터의 상기 평균값 및 상기 제1 학습 모델의 BN(Batch Normalization) 레이어의 평균 값의 제1 차이값을 획득하고, 상기 합성 데이터의 상기 표준 편차값 및 상기 제1 학습 모델의 BN(Batch Normalization) 레이어의 표준 편차값의 제2 차이값을 획득하고, 상기 제1 차이값 및 상기 제2 차이값에 기초하 여 상기 손실값을 획득할 수 있다. 한편, 상기 적어도 하나의 프로세서는 상기 제1 학습 모델에 포함된 적어도 하나의 컨볼루션 레이어의 스트라이 드(stride) 데이터를 획득하고, 상기 적어도 하나의 컨볼루션 레이어 중 상기 스트라이드 데이터의 크기가 2 이 상인 컨볼루션 레이어가 식별되면, 상기 식별된 컨볼루션 레이어를 스윙(swing) 컨볼루션으로 대체할 수 있고, 상기 스윙 컨볼루션 레이어는 패딩 데이터에 기초하여 연산의 대상을 랜덤하게 선택하는 컨볼루션 레이어일 수 있다. 한편, 상기 스윙 컨볼루션 레이어는 제1 데이터가 입력되면, 상기 제1 데이터에 패딩 데이터를 추가하여 제2 데 이터를 획득하는 동작, 상기 제1 데이터의 크기에 기초하여 상기 제2 데이터에서 일부 데이터 영역을 선택하여 제3 데이터를 획득하는 동작 및 상기 제3 데이터 및 상기 식별된 컨볼루션 레이어의 커널 데이터에 기초하여 컨 볼루션 연산을 수행하는 동작을 포함하는 레이어일 수 있다. 한편, 상기 제1 생성기는 적어도 하나의 파라미터에 기초하여 잠재 벡터(latent vector)를 생성하는 생성기일 수 있고, 상기 제1 생성기에 포함된 적어도 하나의 파라미터는 사용자에 의해 설정된 타겟과 관련된 합성 데이터를 생성하는데 이용되는 파라미터일 수 있다. 한편, 상기 합성 데이터는 사용자에 의해 설정된 타겟과 관련된 이미지 데이터일 수 있다. 한편, 상기 적어도 하나의 프로세서는 상기 제1 학습 모델을 양자화(quantization)함으로써 제2 학습 모델을 획 득할 수 있고, 상기 제2 학습 모델은 상기 제1 학습 모델이 압축된 모델일 수 있다. 한편, 상기 전자 장치는 통신 인터페이스를 더 포함할 수 있고, 상기 적어도 하나의 프로세서는 상기 통신 인터 페이스를 통해, 상기 제2 학습 모델을 외부 장치에 전송할 수 있다. 다양한 실시 예에 따른 입력 벡터를 생성하는 제1 생성기, 합성 데이터(synthetic data)를 생성하는 제2 생성기 및 상기 합성 데이터를 분석하는 제1 학습 모델을 포함하는 학습 모듈을 저장하는 전자 장치의 제어 방법은 상 기 제1 생성기를 통해 입력 벡터를 획득하는 단계, 상기 입력 벡터를 상기 제2 생성기에 입력함으로써, 상기 입 력 벡터에 대응되는 합성 데이터를 획득하는 단계, 상기 합성 데이터를 상기 제1 학습 모델에 입력함으로써, 상 기 합성 데이터를 분석한 출력 데이터를 획득하는 단계 및 상기 출력 데이터에 기초하여 상기 제1 생성기에 포 함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습하는 단계를 포함한다. 한편, 상기 학습하는 단계는 상기 출력 데이터에 기초하여 손실값을 획득하고, 상기 손실값이 최소가 되도록 상 기 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습할 수 있다. 한편, 상기 출력 데이터는 상기 합성 데이터의 통계적 특성 데이터를 포함할 수 있다. 한편, 상기 출력 데이터는 상기 합성 데이터의 평균값 및 표준 편차값을 포함할 수 있고, 상기 손실값을 획득하 는 단계는 상기 합성 데이터의 상기 평균값 및 상기 제1 학습 모델의 BN(Batch Normalization) 레이어의 평균값 의 제1 차이값을 획득하고, 상기 합성 데이터의 상기 표준 편차값 및 상기 제1 학습 모델의 BN(Batch Normalization) 레이어의 표준 편차값의 제2 차이값을 획득하고, 상기 제1 차이값 및 상기 제2 차이값에 기초하 여 상기 손실값을 획득할 수 있다. 한편, 상기 제어 방법은 상기 제1 학습 모델에 포함된 적어도 하나의 컨볼루션 레이어의 스트라이드(stride) 데 이터를 획득하는 단계, 상기 적어도 하나의 컨볼루션 레이어 중 상기 스트라이드 데이터의 크기가 2 이상인 컨 볼루션 레이어가 식별되면, 상기 식별된 컨볼루션 레이어를 스윙(swing) 컨볼루션으로 대체하는 단계를 더 포함 하고, 상기 스윙 컨볼루션 레이어는 패딩 데이터에 기초하여 연산의 대상을 랜덤하게 선택하는 컨볼루션 레이어 일 수 있다. 한편, 상기 스윙 컨볼루션 레이어는 제1 데이터가 입력되면, 상기 제1 데이터에 패딩 데이터를 추가하여 제2 데 이터를 획득하는 동작, 상기 제1 데이터의 크기에 기초하여 상기 제2 데이터에서 일부 데이터 영역을 선택하여 제3 데이터를 획득하는 동작 및 상기 제3 데이터 및 상기 식별된 컨볼루션 레이어의 커널 데이터에 기초하여 컨 볼루션 연산을 수행하는 동작을 포함하는 레이어일 수 있다. 한편, 상기 제1 생성기는 적어도 하나의 파라미터에 기초하여 잠재 벡터(latent vector)를 생성하는 생성기이고, 상기 제1 생성기에 포함된 적어도 하나의 파라미터는 사용자에 의해 설정된 타겟과 관련된 합성 데 이터를 생성하는데 이용되는 파라미터일 수 있다. 한편, 상기 합성 데이터는 사용자에 의해 설정된 타겟과 관련된 이미지 데이터일 수 있다. 한편, 상기 제어 방법은 상기 제1 학습 모델을 양자화(quantization)함으로써 제2 학습 모델을 획득하는 단계를 더 포함하고, 상기 제2 학습 모델은 상기 제1 학습 모델이 압축된 모델일 수 있다. 한편, 상기 제어 방법은 상기 제2 학습 모델을 외부 장치에 전송하는 단계를 더 포함할 수 있다."}
{"patent_id": "10-2023-0010313", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부 도면을 참조하여 본 개시를 상세히 설명한다. 본 개시의 실시 예에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 개시의 설명 부 분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수 치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. A 또는/및 B 중 적어도 하나라는 표현은 \"A\" 또는 \"B\" 또는 \"A 및 B\" 중 어느 하나를 나타내는 것으로 이해되어 야 한다. 본 명세서에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들 을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 어떤 구성요소가 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또 는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시에서 \"모듈\" 혹은 \"부\"는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨어로 구현되 거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\" 혹은 복수의 \"부\"는 특정한 하드 웨어로 구현될 필요가 있는 \"모듈\" 혹은 \"부\"를 제외하고는 적어도 하나의 모듈로 일체화되어 적어도 하나의 프 로세서(미도시)로 구현될 수 있다. 본 명세서에서, 사용자라는 용어는 전자 장치를 사용하는 사람 또는 전자 장치를 사용하는 장치(예: 인공지능 전자 장치)를 지칭할 수 있다. 본 개시에서, 인공 지능 모델이 학습된다는 것은, 기본 인공 지능 모델(예를 들어 임의의 랜덤한 파라미터를 포 함하는 인공 지능 모델)이 학습 알고리즘에 의하여 다수의 훈련 데이터들을 이용하여 학습됨으로써, 원하는 특 성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공 지능 모델이 만들어짐을 의미한다. 이러한 학습은 별도의 서버 및/또는 시스템을 통해 이루어질 수 있으나, 이에 한정되는 것은 아니며 전자 장치에 서 이루어질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도형 학습 (unsupervised learning), 준지도형 학습(semi-supervised learning), 전이 학습(transfer learning) 또는 강 화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 여기서, 인공 지능 모델 각각은, 예를 들어, CNN (Convolutional Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등으로 구현될 수 있으나, 이에 한정되지 않는 다. 본 개시의 일 실시 예에 따른 인공 지능 모델을 실행하기 위한 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공 지능 전용 프로세서와 소프트웨어의 조합을 통해 구현될 수 있다. 프로세서는, 메모리 에 저장된 기 정의된 동작 규칙 또는 인공 지능 모델에 따라, 입력 데이터를 처리하도록 제어할 수 있다. 또는, 프로세서가 전용 프로세서(또는 인공 지능 전용 프로세서)인 경우, 특정 인공 지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 예를 들어, 특정 인공 지능 모델의 처리에 특화된 하드웨어는 ASIC, FPGA 등의 하드웨어 칩으로 설계될 수 있다. 프로세서가 전용 프로세서로 구현되는 경우, 본 개시의 실시 예를 구현하기 위한 메모리를 포함하도록 구현되거나, 외부 메모리를 이용하기 위한 메모리 처리 기능을 포함하 도록 구현될 수 있다. 다른 예에 따라, 메모리는 복수의 레이어를 포함하는 인공 지능 모델에 관한 정보를 저장할 수 있다. 여기 서, 인공 지능 모델에 관한 정보를 저장한다는 것은 인공 지능 모델의 동작과 관련된 다양한 정보, 예를 들어 인공 지능 모델에 포함된 복수의 레이어에 대한 정보, 복수의 레이어 각각에서 이용되는 파라미터(예를 들어, 필터 계수, 바이어스 등)에 대한 정보 등을 저장한다는 것을 의미할 수 있다. 이하 첨부된 도면들을 참조하여 본 개시의 일 실시 예를 보다 상세하게 설명한다. 도 1은 전자 장치 및 외부 장치를 포함하는 시스템을 설명하기 위한 도면이다. 도 1을 참조하면, 시스템은 전자 장치 및 외부 장치를 포함할 수 있다. 전자 장치는 특정 학습 모델을 학습(training) 및 압축하는 기기일 수 있다. 특정 학습 모델은 인공 지능 모델을 의미할 수 있다. 외부 장치는 압축 과정에서 획득한 압축된 학습 모델을 수신하는 기기일 수 있다. 외부 장치는 압축된(수신된) 학습 모델에 기초하여 사용자에게 서비스를 제공할 수 있다. 도 2는 전자 장치에 포함된 구성을 도시한 블록도이다. 도 2를 참조하면, 전자 장치는 메모리, 적어도 하나의 프로세서 또는 통신 인터페이스 중 적어도 하나를 포함할 수 있다. 메모리는 프로세서에 포함된 롬(ROM)(예를 들어, EEPROM(electrically erasable programmable read- only memory)), 램(RAM) 등의 내부 메모리로 구현되거나, 프로세서와 별도의 메모리로 구현될 수도 있다. 이 경우, 메모리는 데이터 저장 용도에 따라 전자 장치에 임베디드된 메모리 형태로 구현되거나, 전자 장치에 탈부착이 가능한 메모리 형태로 구현될 수도 있다. 예를 들어, 전자 장치의 구동을 위한 데이터의 경우 전자 장치에 임베디드된 메모리에 저장되고, 전자 장치의 확장 기능을 위한 데이터의 경우 전자 장치에 탈부착이 가능한 메모리에 저장될 수 있다. 전자 장치에 임베디드된 메모리의 경우 휘발성 메모리(예: DRAM(dynamic RAM), SRAM(static RAM), 또는 SDRAM(synchronous dynamic RAM) 등), 비휘발성 메모리(non-volatile Memory)(예: OTPROM(one time programmable ROM), PROM(programmable ROM), EPROM(erasable and programmable ROM), EEPROM(electrically erasable and programmable ROM), mask ROM, flash ROM, 플래시 메모리(예: NAND flash 또는 NOR flash 등), 하드 드라이브, 또는 솔리드 스테이트 드라이브(solid state drive(SSD)) 중 적어도 하나로 구현되고, 전자 장 치에 탈부착이 가능한 메모리의 경우 메모리 카드(예를 들어, CF(compact flash), SD(secure digital), Micro-SD(micro secure digital), Mini-SD(mini secure digital), xD(extreme digital), MMC(multi-media card) 등), USB 포트에 연결 가능한 외부 메모리(예를 들어, USB 메모리) 등과 같은 형태로 구현될 수 있다. 프로세서는 전자 장치의 전반적인 제어 동작을 수행할 수 있다. 구체적으로, 프로세서는 전자 장치의 전반적인 동작을 제어하는 기능을 한다. 프로세서는 디지털 신호를 처리하는 디지털 시그널 프로세서(digital signal processor(DSP), 마이크로 프 로세서(microprocessor), TCON(Time controller)으로 구현될 수 있다. 다만, 이에 한정되는 것은 아니며, 중앙 처리장치(central processing unit(CPU)), MCU(Micro Controller Unit), MPU(micro processing unit), 컨트롤 러(controller), 어플리케이션 프로세서(application processor(AP)), GPU(graphics-processing unit) 또는 커 뮤니케이션 프로세서(communication processor(CP)), ARM(advanced reduced instruction set computer (RISC) machines) 프로세서 중 하나 또는 그 이상을 포함하거나, 해당 용어로 정의될 수 있다. 또한, 프로세서는 프로세싱 알고리즘이 내장된 SoC(System on Chip), LSI(large scale integration)로 구현될 수도 있고, FPGA(Field Programmable gate array) 형태로 구현될 수도 있다. 또한, 프로세서는 메모리에 저장된 컴퓨 터 실행가능 명령어(computer executable instructions)를 실행함으로써 다양한 기능을 수행할 수 있다. 전자 장치는 인공 지능 모델을 학습하는 서버일 수 있다. 메모리는 입력 벡터를 생성하는 제1 생성기, 합성 데이터(synthetic data)를 생성하는 제2 생성기 및 합성 데이터를 분석하는 제1 학습 모델을 포함하는 학습 모듈을 저장할 수 있다. 적어도 하나의 프로세서는 메모리와 연결되어 전자 장치를 제어할 수 있다. 적어도 하나의 프로세서는 제1 생성기를 통해 입력 벡터를 획득하고, 입력 벡터를 제2 생성기에 입력함으로써, 입력 벡터에 대응되는 합성 데이터를 획득하고, 합성 데이터를 제1 학습 모델에 입력함으로 써, 합성 데이터를 분석한 출력 데이터를 획득하고, 출력 데이터에 기초하여 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습할 수 있다. 적어도 하나의 프로세서는 제1 생성기를 통해 램덤하게 생성된 입력 벡터를 획득할 수 있다. 제1 생 성기는 랜덤하게 생성된 난수를 통해 가우시안 분포를 갖는 입력 벡터를 생성할 수 있다. 입력 벡터는 잠 재 벡터(latent vector)를 의미할 수 있다. 또한, 입력 벡터는 가우시안 분포(Gaussian distribution, (N(0,I)))가 적용된 벡터일 수 있다. 입력 벡터는 제1 생성기의 출력 데이터일 수 있다. 적어도 하나의 프로세서는 제1 생성기를 통해 생성된 입력 벡터를 제2 생성기에 입력(또는 제공)할 수 있다. 적어도 하나의 프로세서는 제2 생성기를 통해 입력 벡터에 대응되는 합성 데이터를 제2 생성기의 출력 데이터로써 획득할 수 있다. 합성 데이터는 사용자 설정에 기초하여 설정된 제2 생성기를 통해 생성되는 가상 데이터를 의미할 수 있다. 사용자가 생성하고자 의도하는 데이터를 타겟 데이터로 기재할 수 있다. 제2 생성기는 입력 벡터에 기초하여 타겟 데이터와 관련된 합성 데이터를 생성할 수 있다. 예를 들어, 타겟 데이터가 개(dog)라고 가정한다. 제2 생성기는 입력 벡터에 기초하여 개(dog)와 관련된 합성 데이터(또는 가상 이미지)를 생성할 수 있다. 입력 벡터는 개(dog)와 관련된 가상 이미지를 생성하기 위해 필요한 파라미터를 포함할 수 있다. 예를 들어, 입력 벡터의 파라미터는 눈, 코, 입, 귀, 종(species), 털 색상 중 적어도 하나와 관련된 파라미터를 포함할 수 있다. 제1 생성기는 개(dog)와 관련된 파라미터와 관련하 여 랜덤하게 생성된 난수에 기초하여 입력 벡터를 생성할 수 있다. 적어도 하나의 프로세서는 제1 생성기 를 통해 획득한 입력 벡터를 제2 생성기에 제공함으로써 개(dog)와 관련된 합성 데이터(또는 가상 이미지)를 획득할 수 있다. 적어도 하나의 프로세서는 제2 생성기를 통해 획득한 합성 데이터를 제1 학습 모델에 입력(또는 제공)할 수 있다. 적어도 하나의 프로세서는 제1 학습 모델을 통해 합성 데이터에 대응되는 출력 데 이터를 획득할 수 있다. 제1 학습 모델은 입력 데이터를 분석하여 분석 결과를 출력 데이터로써 출력하는 모델일 수 있다. 다양한 실시 예에 따라, 제1 학습 모델은 입력 데이터에 대응되는 통계적 특성 데이터를 출력 데이터로써 출력하는 모델일 수 있다. 적어도 하나의 프로세서는 통계적 특성 데이터에 기초하여 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습할 수 있다. 다양한 실시 예에 따라, 제1 학습 모델은 입력 데이터에 대응되는 카테고리 확률값(또는 오브젝트 확률 값)을 출력 데이터로써 출력하는 모델일 수 있다. 적어도 하나의 프로세서는 카테고리 확률값(또는 오브젝 트 확률값)에 기초하여 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습할 수 있다. 다양한 실시 예에 따라, 제1 학습 모델은 입력 데이터가 타겟 데이터와 관련하여 진짜 데이터인지 가짜 데 이터를 판단하는 판별기(discriminator) 모델일 수 있다. 적어도 하나의 프로세서는 판별기의 출력값에 기 초하여 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미 터를 학습할 수 있다. 적어도 하나의 프로세서는 학습 모듈을 통해 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습할 수 있다. 학습 모듈은 제1 생성기, 제2 생성기 및 제1 학습 모델를 포함할 수 있다. 학습 모듈과 관련된 구체적인 설명은 도 5, 도 6, 도 7 등에서 기재한다. 한편, 적어도 하나의 프로세서는 출력 데이터에 기초하여 손실값을 획득하고, 손실값이 최소가 되도록 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습할 수 있다. 제1 생성기에 포함된 적어도 하나의 파라미터는 잠재 벡터를 구성하는 요소를 포함할 수 있다. 제2 생성기 에 포함된 적어도 하나의 파라미터는 제2 생성기에서 적용되는 가중치를 포함할 수 있다. 한편, 출력 데이터는 합성 데이터의 통계적 특성 데이터를 포함할 수 있다. 통계적 특성 데이터는 평균값, 표준 편차값 또는 분산값 중 적어도 하나를 포함할 수 있다. 한편, 출력 데이터는 합성 데이터의 평균값 및 표준 편차값을 포함할 수 있으며, 적어도 하나의 프로세서 는 합성 데이터의 평균값 및 제1 학습 모델의 BN(Batch Normalization) 레이어의 평균값의 제1 차이값을 획득하고, 합성 데이터의 표준 편차값 및 제1 학습 모델의 BN(Batch Normalization) 레이어의 표준 편차값 의 제2 차이값을 획득하고, 제1 차이값 및 제2 차이값에 기초하여 손실값을 획득할 수 있다. 다양한 실시 예에 따라, 적어도 하나의 프로세서는 표준 편차값 대신 분산값을 이용할 수 있다. 제1 차이값은 도 9의 “μl^s-μl”을 의미할 수 있다. 제2 차이값은 도 9의 “σl^s-σl”을 의미할 수 있다. 손실값을 획득하는 구체적인 동작은 도 9, 도 13, 도 14 등에서 기재한다. 한편, 적어도 하나의 프로세서는 제1 학습 모델에 포함된 적어도 하나의 컨볼루션 레이어의 스트라이 드(stride) 데이터를 획득하고, 적어도 하나의 컨볼루션 레이어 중 스트라이드 데이터의 크기가 2 이상인 컨볼 루션 레이어가 식별되면, 식별된 컨볼루션 레이어를 스윙(swing) 컨볼루션으로 대체하고, 스윙 컨볼루션 레이어 는 패딩 데이터에 기초하여 연산의 대상을 랜덤하게 선택하는 컨볼루션 레이어일 수 있다. 적어도 하나의 프로세서는 제1 학습 모델에 포함된 적어도 하나의 컨볼루션 레이어 중 스트라이드 크 기가 2 이상인 컨볼루션 레이어를 식별할 수 있다. 그리고, 적어도 하나의 프로세서는 식별된 컨볼루션 레 이어를 스윙 컨볼루션 레이어로 대체할 수 있다. 대체 동작이 수행되기 전의 모델을 제1 학습 모델로 기재하고 대체 동작이 수행된 후의 모델을 변경된 제1 학습 모델로 기재할 수 있다. 변경된 제1 학습 모델은 제2 학습 모델로 기재될 수 있다. 스트라이드는 컨볼루션 연산에서 계산 단위(또는 스텝)를 의미할 수 있다. 컨볼루션 연산과 트랜스포즈드 컨볼 루션 연산에 대한 설명은 도 8에서 기재한다. 한편, 스윙 컨볼루션 레이어는 제1 데이터가 입력되면, 제1 데이터에 패딩 데이터를 추가하 여 제2 데이터를 획득하는 동작, 제1 데이터의 크기에 기초하여 제2 데이터에서 일부 데이터 영역을 선택하여 제3 데이터를 획득하는 동작 및 제3 데이터 및 식별된 컨볼루션 레이어의 커널 데 이터에 기초하여 컨볼루션 연산을 수행하는 동작을 포함하는 레이어일 수 있다. 스윙 컨볼루션 레이어와 관련된 구체적인 설명은 도 11, 도 12, 도 15 등에서 기재한다. 한편, 제1 생성기는 적어도 하나의 파라미터에 기초하여 잠재 벡터(latent vector)를 생성하는 생성기이고, 제1 생성기에 포함된 적어도 하나의 파라미터는 사용자에 의해 설정된 타겟과 관련된 합성 데 이터를 생성하는데 이용되는 파라미터일 수 있다. 한편, 합성 데이터는 사용자에 의해 설정된 타겟과 관련된 이미지 데이터일 수 있다. 한편, 적어도 하나의 프로세서는 제1 학습 모델을 양자화(quantization)함으로써 제2 학습 모델(15 3)을 획득하고, 제2 학습 모델은 제1 학습 모델이 압축된 모델일 수 있다. 제2 학습 모델의 저장 크기는 제1 생성기의 저장 크기보다 작을 수 있다. 따라서, 제2 학습 모델 는 제1 학습 모델보다 경량화된 모델을 의미할 수 있다. 다양한 실시 예에 따라, 변경된 제1 학습 모델은 제2 학습 모델로 기재되면, 제2 학습 모델은 제3 학습 모델로 기재될 수 있다. 적어도 하나의 프로세서는 압축 모듈을 이용하여 양자화 동작을 수행할 수 있다. 이와 관련된 구체적 인 설명은 도 16 내지 도 18에서 기재한다. 한편, 전자 장치는 통신 인터페이스를 더 포함할 수 있고, 적어도 하나의 프로세서는 통신 인터 페이스를 통해, 제2 학습 모델을 외부 장치에 전송할 수 있다. 통신 인터페이스는 다양한 유형의 통신 방식에 따라 다양한 유형의 외부 장치와 통신을 수행하는 구성이다. 통신 인터페이스는 무선 통신 모듈 또는 유선 통신 모듈을 포함할 수 있다. 여기서, 각 통신 모 듈은 적어도 하나의 하드웨어 칩 형태로 구현될 수 있다. 무선 통신 모듈은 무선으로 외부 장치와 통신하는 모듈일 수 있다. 예를 들어, 무선 통신 모듈은 와이파이 모듈, 블루투스 모듈, 적외선 통신 모듈 또는 기타 통신 모듈 중 적어도 하나의 모듈을 포함할 수 있다. 와이파이 모듈, 블루투스 모듈은 각각 와이파이 방식, 블루투스 방식으로 통신을 수행할 수 있다. 와이파이 모 듈이나 블루투스 모듈을 이용하는 경우에는 SSID(service set identifier) 및 세션 키 등과 같은 각종 연결 정 보를 먼저 송수신하여, 이를 이용하여 통신 연결한 후 각종 정보들을 송수신할 수 있다. 적외선 통신 모듈은 가시 광선과 밀리미터파 사이에 있는 적외선을 이용하여 근거리에 무선으로 데이터를 전송 하는 적외선 통신(IrDA, infrared Data Association)기술에 따라 통신을 수행한다. 기타 통신 모듈은 상술한 통신 방식 이외에 지그비(zigbee), 3G(3rd Generation), 3GPP(3rd Generation Partnership Project), LTE(Long Term Evolution), LTE-A(LTE Advanced), 4G(4th Generation), 5G(5th Generation)등과 같은 다양한 무선 통신 규격에 따라 통신을 수행하는 적어도 하나의 통신 칩을 포함할 수 있다. 유선 통신 모듈은 유선으로 외부 장치와 통신하는 모듈일 수 있다. 예를 들어, 유선 통신 모듈은 LAN(Local Area Network) 모듈, 이더넷 모듈, 페어 케이블, 동축 케이블, 광섬유 케이블 또는 UWB(Ultra Wide-Band) 모듈 중 적어도 하나를 포함할 수 있다. 외부 장치는 전자 장치에 압축된 모델을 요청할 수 있다. 적어도 하나의 프로세서는 외부 장치 의 요청에 응답하여 압축된 제2 학습 모델을 전송할 수 있다. 이와 관련된 구체적인 설명은 도 19에 서 기재한다. 적어도 하나의 프로세서는 학습 모듈을 통해 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습하는 동작이 완료되면, 합성 데이터 생성과 관련된 화 면을 제공할 수 있다. 적어도 하나의 프로세서는 전자 장치에 포함된 디스플레이(미도시) 또는 전자 장치에 연결된 디스플레이(미도시)를 이용하여 화면을 표시할 수 있다. 이와 관련된 구체적인설명은 도 20에서 기재한다. 다양한 실시 예에 따른 전자 장치는 합성 데이터를 생성함에 있어 제1 생성기 및 제2 생성기를 학습(또는 업데이트)할 수 있다. 입력 벡터 생성 및 합성 데이터 생성 모두를 업데이트함으로써 합성 데이터의 품질을 개선시킬 수 있다. 다양한 실시 예에 따른 전자 장치는 제1 학습 모델에 포함된 특정 컨볼루션 레이어(스트라이드 크기 가 2 이상인 레이어)를 스윙 컨볼루션 레이어로 대체할 수 있다. 스윙 컨볼루션 레이어는 랜덤하게 연산 대상을 선택함으로써 스트라이드에 따라 정보가 손실되는 것을 방지할 수 있다. 다양한 실시 예에 따른 전자 장치는 압축 모듈을 이용하여 제1 학습 모델 또는 변경된 제1 학습 모델을 양자화할 수 있다. 양자화된 모델은 상대적으로 연산 처리 능력이 서버 등에 비하여 낮은 단말 장 치(예를 들어, 모바일 기기) 등에서 이용될 수 있다. 도 3은 외부 장치에 포함된 구성을 도시한 블록도이다. 도 3을 참조하면, 외부 장치는 메모리, 적어도 하나의 프로세서, 통신 인터페이스, 디스플 레이, 조작 인터페이스, 입출력 인터페이스, 스피커 또는 마이크 중 적어도 하나를 포함할 수 있다. 메모리, 적어도 하나의 프로세서 및 통신 인터페이스는 도 2의 메모리, 적어도 하나의 프 로세서 및 통신 인터페이스에 대응될 수 있다. 따라서, 중복 설명을 생략한다. 디스플레이는 LCD(Liquid Crystal Display), OLED(Organic Light Emitting Diodes) 디스플레이, PDP(Plasma Display Panel) 등과 같은 다양한 형태의 디스플레이로 구현될 수 있다. 디스플레이내에는 a- si TFT(amorphous silicon thin film transistor), LTPS(low temperature poly silicon) TFT, OTFT(organic TFT) 등과 같은 형태로 구현될 수 있는 구동 회로, 백라이트 유닛 등도 함께 포함될 수 있다. 디스플레이 는 터치 센서와 결합된 터치 스크린, 플렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display, three-dimensional dispaly) 등으로 구현될 수 있다. 또한, 본 개시의 일 실시 예에 따른, 디스플레이는 이미지를을 출력하는 디스플레이 패널뿐만 아니라, 디스플레이 패널을 하우징하는 베젤을 포함할 수 있다. 특히, 본 개시의 일 실시 예에 따른, 베젤은 사용자 인터렉션을 감지하기 위한 터치 센서(미도시)를 포함할 수 있다. 조작 인터페이스는 버튼, 터치 패드, 마우스 및 키보드와 같은 장치로 구현되거나, 상술한 디스플레이 기 능 및 조작 입력 기능도 함께 수행 가능한 터치 스크린으로도 구현될 수 있다. 여기서, 버튼은 외부 장치 의 본체 외관의 전면부나 측면부, 배면부 등의 임의의 영역에 형성된 기계적 버튼, 터치 패드, 휠 등과 같은 다 양한 유형의 버튼이 될 수 있다. 입출력 인터페이스는 HDMI(High Definition Multimedia Interface), MHL (Mobile High-Definition Link), USB (Universal Serial Bus), DP(Display Port), 썬더볼트(Thunderbolt), VGA(Video Graphics Array) 포트, RGB 포트, D-SUB(D-subminiature), DVI(Digital Visual Interface) 중 어느 하나의 인터페이스일 수 있 다. 입출력 인터페이스는 오디오 및 비디오 신호 중 적어도 하나를 입출력 할 수 있다. 구현 예에 따라, 입출력 인터페이스는 오디오 신호만을 입출력하는 포트와 비디오 신호만을 입출력하는 포트를 별개의 포트 로 포함하거나, 오디오 신호 및 비디오 신호를 모두 입출력하는 하나의 포트로 구현될 수 있다. 외부 장치(20 0)는 입출력 인터페이스를 통해 오디오 및 비디오 신호 중 적어도 하나를 외부 장치(예를 들어, 외부 디스 플레이 장치 또는 외부 스피커)에 전송할 수 있다. 구체적으로, 입출력 인터페이스에 포함된 출력 포트가 외부 장치와 연결될 수 있으며, 외부 장치는 오디오 및 비디오 신호 중 적어도 하나를 출력 포트를 통해 외부 장치에 전송할 수 있다. 스피커는 각종 오디오 데이터뿐만 아니라 각종 알림 음이나 음성 메시지 등을 출력하는 구성요소일 수 있 다. 마이크는 사용자 음성이나 기타 소리를 입력 받아 오디오 데이터로 변환하기 위한 구성이다. 마이크 는 활성화 상태에서 사용자의 음성을 수신할 수 있다. 예를 들어, 마이크는 외부 장치의 상측이나 전 면 방향, 측면 방향 등에 일체형으로 형성될 수 있다. 마이크는 아날로그 형태의 사용자 음성을 수집하는 마이크, 수집된 사용자 음성을 증폭하는 앰프 회로, 증폭된 사용자 음성을 샘플링하여 디지털 신호로 변환하는 A/D 변환회로, 변환된 디지털 신호로부터 노이즈 성분을 제거하는 필터 회로 등과 같은 다양한 구성을 포함할수 있다. 도 4는 학습 모듈 및 압축 모듈을 설명하기 위한 도면이다. 도 4를 참조하면, 전자 장치는 학습 모듈 및 압축 모듈을 포함할 수 있다. 전자 장치는 메 모리에 학습 모듈 및 압축 모듈을 저장할 수 있다. 전자 장치는 메모리에 학습 데이 터를 저장할 수 있다. 학습 모듈은 학습 시스템, 학습 네트워크, 학습 모델 등으로 기재될 수 있다. 학습 데이터는 제1 학습 모델을 포함할 수 있다. 제1 학습 모델은 입력 데이터를 분석하여 분석 결과 를 출력 데이터로써 출력하는 모델일 수 있다. 다양한 실시 예에 따라, 제1 학습 모델은 입력 데이터에 대응되는 통계적 특성 데이터를 출력 데이터로써 출력하는 모델일 수 있다. 다양한 실시 예에 따라, 제1 학습 모델은 입력 데이터에 대응되는 카테고리 확률값(또는 오브젝트 확률 값)을 출력 데이터로써 출력하는 모델일 수 있다. 다양한 실시 예에 따라, 제1 학습 모델은 입력 데이터가 타겟 데이터와 관련하여 진짜 데이터인지 가짜 데 이터를 판단하는 판별기(discriminator) 모델일 수 있다. 학습 모듈은 입력 벡터 생성기(제1 생성기), 합성 데이터 생성기(제2 생성기)를 학습하는 모델 일 수 있다. 학습 모듈은 합성 데이터(synthetic data) 및 기 설정된 데이터를 비교하여 제1 생성기 및 제2 생성기를 업데이트하는 모델일 수 있다. 합성 데이터(synthetic data)는 증류 데이터(distilled data)로 기재될 수 있다. 업데이트 동작은 각 생성기에 포함된 적어도 하나의 파라미터를 학습하는 동작을 포함 할 수 있다. 학습 모듈은 업데이트 동작(또는 학습 동작)을 완료한 후, 제1 학습 모델을 압축 모듈 에 전송할 수 있다. 압축 모듈은 학습 모듈로부터 수신한 제1 학습 모델을 양자화(quantization)하는 모델일 수 있 다. 양자화 결과에 따라, 압축 모듈은 제1 학습 모델을 압축함으로써 제2 학습 모델을 획득할 수 있다. 제2 학습 모델의 저장 크기는 제1 학습 모델의 저장 크기보다 작을 수 있다. 따라서, 제2 학습 모델이 메모리 처리 능력이 상대적으로 낮은 단말 장치(예를 들어, 외부 장치)에서도 구현될 수 있다. 압축 모듈은 제2 학습 모델을 외부 장치에 전송할 수 있다. 외부 장치는 전자 장치의 압축 모듈로부터 수신한 제2 학습 모델에 기초하여 인공 지능과 관련된 서비스를 사용자에게 제공할 수 있다. 도 5는 학습 모듈이 수행하는 동작을 설명하기 위한 도면이다. 도 5를 참조하면, 학습 모듈은 제1 생성기, 제2 생성기 및 제1 학습 모델을 포함할 수 있 다. 제1 생성기는 입력 벡터 생성기일 수 있다. 제1 생성기는 입력 벡터를 생성하는 동작을 수행할 수 있 으며, 생성된 입력 벡터는 난수(Random Number)에 기초하여 생성되는 벡터일 수 있다. 또한, 입력 벡터는 잠재 벡터(latent vector)를 의미할 수 있다. 또한, 입력 벡터는 가우시안 분포(Gaussian distribution, (N(0,I))) 가 적용된 벡터일 수 있다. 제1 생성기는 생성된 입력 벡터를 제2 생성기에 전송할 수 있다. 제2 생성기는 합성 데이터 생성기일 수 있다. 제2 생성기는 제1 생성기로부터 생성된 입력 벡터 를 수신할 수 있다. 제2 생성기는 입력 벡터에 기초하여 합성 데이터를 생성할 수 있다. 합성 데이터는 사 용자에 의해 설정된 타겟과 관련된 데이터일 수 있다. 사용자는 타겟이 생성되기 위한 설정 명령(또는 제어 명 령)을 입력할 수 있다. 학습 모듈은 사용자 입력에 기초하여 타겟과 관련된 합성 데이터를 랜덤하게 생성 할 수 있다. 학습 모듈은 랜덤하게 생성된 입력 벡터에 기초하여 타겟과 관련된 합성 데이터를 생성할 수 있다. 제2 생성기는 합성 데이터를 제1 학습 모델에 전송할 수 있다. 제1 학습 모델은 제2 생성기로부터 합성 데이터를 수신할 수 있다. 제1 학습 모델은 합성 데이 터를 입력 데이터로써 이용할 수 있다. 제1 학습 모델은 합성 데이터에 대응되는 출력 데이터를 획득할 수 있다. 출력 데이터는 통계적 특성 데이터일 수 있다. 통계적 특성 데이터는 평균값 및 표준 편차값을 포함할 수 있다. 제1 학습 모델은 합성 데이터에 대응되는 평균값 및 표준 편차값을 출력 데이터로써 획득할 수 있다. 학습 모듈은 제1 생성기, 제2 생성기 및 제1 학습 모델에 기초하여 학습 동작을 수행할 수 있다. 학습 모듈은 합성 데이터에 대응되는 출력 데이터에 기초하여 제1 생성기, 제2 생성기 또 는 제1 학습 모델 중 적어도 하나를 학습할 수 있다. 도 6은 학습 모듈이 수행하는 동작을 구체적으로 설명하기 위한 도면이다. 도 6의 제1 생성기, 제2 생성기 및 제1 학습 모델의 설명을 도 5에서 기재하였으므로, 중복 설 명을 생략한다. 학습 모듈은 제2 생성기를 통해 합성 데이터(142-1)를 획득할 수 있다. 합성 데이터(142-1)는 학습 동작이 반복됨에 따라 계속하여 변경될 수 있다. 예를 들어, 타겟이 개(dog)이며 제2 생성기는 개 이미지를 생성하는 것으로 가정한다. t=0 시점에의 최초 의 학습 동작에서는 합성 데이터(142-1)가 노이즈 데이터일 수 있다. 하지만, t=T 시점에의 학습 동작에서는 합 성 데이터(142-1)가 개를 나타내는 오브젝트를 명확히 포함할 수 있다. 제1 학습 모델은 컨볼루션 레이어(또는 트랜스포즈드(Transposed) 컨볼루션 레이어), BN(Batch Normalization) 레이어 중 적어도 하나를 포함할 수 있다. 또한, 제1 학습 모델은 스트라이드의 크기가 2 이상인 컨볼루션 레이어를 포함할 수 있다. 스트라이드의 크기가 2 이상인 컨볼루션 레이어를 “Strided Convolution layer”로 기재할 수 있다. 스트라이드는 컨볼루션 연산을 수행하는데 이용되는 계산 단위(또는 스 텝)을 의미할 수 있다. 학습 모듈은 제1 학습 모델에 포함된 적어도 하나의 레이어 중 스트라이드의 크기가 2 이상인 컨볼루 션 레이어(143-1)를 변경(또는 변형)할 수 있다. 학습 모듈은 스트라이드의 크기가 2 이상인 컨볼루션 레 이어(143-1)를 스윙 컨볼루션 레이어(Swing Convolution layer, 143-2)로 변경할 수 있다. 스윙 컨볼루션 레이어(143-2)는 기존 컨볼루션 연산 방식을 변경한 연산 레이어일 수 있다. 이와 관련된 구체적 인 설명은 도 11 및 도 12에서 기재한다. 학습 모듈은 합성 데이터에 대한 연산 동작을 수행하여 특징 맵(Feature maps)을 획득할 수 있으며, 특징 맵에 기초하여 손실값(제1 손실값)을 획득할 수 있다. 그리고, 학습 모듈은 손실값이 최소화되도록 학습 동작을 수행할 수 있다. 도 7은 학습 모듈이 수행하는 순전파(Forward Propagation) 및 역전파(Back Propagation) 과정을 설명하 기 위한 도면이다. 도 7을 참조하면, 학습 모듈은 순전파 및 역전파 과정을 이용하여 학습 동작을 수행할 수 있다. 제1 학습 모델이 순전파 과정에서 컨볼루션 레이어를 이용하였다고 가정한다. 제1 학습 모델은 역전파 과정에서 컨볼루션 레이어 대신 트랜스포즈드 컨볼루션 레이어를 이용할 수 있다. 제1 학습 모델이 순전파 과정에서 스트라이드 크기가 2 이상인 컨볼루션 레이어를 이용하였다고 가정 한다. 제1 학습 모델은 역전파 과정에서 컨볼루션 레이어 대신 트랜스포즈드 컨볼루션 레이어를 이용할 수 있다. 도 8은 컨볼루션 연산과 트랜스포즈드(Transposed) 컨볼루션 연산을 설명하기 위한 도면이다. 도 8의 실시 예는 컨볼루션 레이어에서 수행되는 연산 과정을 나타낸다. 실시 예에서, 스트라이드 크 기는 1로 가정한다. 전자 장치는 입력 데이터 및 커널 데이터에 대한 컨볼루션 연산을 수행함으 로써 출력 데이터를 획득할 수 있다. 컨볼루션 연산을 통해 입력 데이터의 크기가 작아질 수 있다. 따라서, 컨볼루션 연산은 다운 샘플링(down sampling)을 의미할 수 있다. 도 8의 실시 예는 트랜스포즈드 컨볼루션 레이어에서 수행되는 연산 과정을 나타낸다. 실시 예에서, 스트라이드 크기는 1로 가정한다. 전자 장치는 입력 데이터 및 커널 데이터에 대한 컨볼루션 연 산을 수행함으로써 출력 데이터를 획득할 수 있다. 트랜스포즈드 컨볼루션 연산을 통해 입력 데이터의 크 기가 커질 수 있다. 따라서, 컨볼루션 연산은 업 샘플링(up sampling)을 의미할 수 있다. 도 9는 학습 모듈에서 수행되는 학습 동작을 설명하기 위한 도면이다. 도 9의 알고리즘은 학습 모듈에서 수행되는 학습 동작을 나타낸다. 학습 모듈은 입력 데이터로 써 사전 학습 모델(fp)을 수신할 수 있다. 사전 학습 모델(fp)은 제1 학습 모델을 의미할 수 있다. 학습 모듈은 출력 데이터로써 합성 데이터(x^r)를 획득할 수 있다. 학습 모듈은 사전 학습 모델(fp)에 포함된 특정 컨볼루션 레이어(스트라이드 크기가 2 이상인 레이어)를 스윙 컨볼루션 레이어로 변경할 수 있다. 변경된 레이어를 포함하는 모델을 변경된 사전 학습 모델(fp^)로 기재 할 수 있다. 변경된 사전 학습 모델(fp^)은 도 10의 변경된 제1 학습 모델을 의미할 수 있다. 학습 모듈 은 레이어 변경 동작을 통해 변경된 사전 학습 모델(fp^)을 획득할 수 있다. 학습 모듈은 잠재 벡터(z)를 초기화할 수 있다. 잠재 벡터(z)는 제1 생성기에서 생성되는 입력 벡터 를 의미할 수 있다. 잠재 벡터(z)는 가우시안 분포(Gaussian distribution, (N(0,I)))를 따를 수 있다. 학습 모듈은 생성기(G)에 포함된 가중치(Wg)를 초기화할 수 있다. 생성기(G)는 제2 생성기를 의미할 수 있다. 학습 모듈은 생성기(G)에 잠재 벡터(z)를 입력하여 합성 데이터(x^r)를 획득할 수 있다. 학습 모듈은 변경된 사전 학습 모델(fp^)에 합성 데이터(x^r)를 입력할 수 있다. 학습 모듈은 잠재 벡터(z) 및 생성기(G)에 포함된 가중치(Wg)를 손실값(L_BNS)에 기초하여 업데이트(또는 학습)할 수 있다. BNS는 Batch Normalization layers를 의미할 수 있다. 학습 모듈은 특정 조건이 만족될때까지 업데이트 동작을 반복할 수 있다. 학습 모듈은 특정 조건이 만족될때까지 생성기(G)에 잠재 벡터(z)를 입력하여 합성 데이터(x^r)를 획득하는 동작, 변경된 사전 학습 모델 (fp^)에 합성 데이터(x^r)를 입력하는 동작, 잠재 벡터(z) 및 생성기(G)에 포함된 가중치(Wg)를 손실값(L_BNS) 에 기초하여 업데이트하는 동작을 반복할 수 있다. 도 9의 수학식은 손실값(L_BNS)을 계산하는 과정을 나타낸다. l은 BN 레이어를 특정하기 위한 번호를 나타낸다. L은 제1 학습 모델에 포함된 BN 레이어의 전체 개수를 나타낸다. μl^s는 합성 데이터에 대응되는 평균값을 나타낼 수 있다. μl은 제1 학습 모델에 포함된 특정 BN 레이어의 평균값을 나타낼 수 있다. σl^s는 합성 데이터에 대응되는 표준 편차값을 나타낼 수 있다. σl 제1 학습 모델에 포함된 특정 BN 레이어의 표준 편차값을 나타낼 수 있다. “|| ||”는 노름(norm) 연산 기호를 나타낼 수 있다. 학습 모듈은 수학식에 기초하여 손실값(L_BNS)을 획득할 수 있다. 학습 모듈은 손실값(L_BNS)이 최소화되도록 잠재 벡터(z) 및 생성기(G)에 포함된 가중치(Wg)를 업데이트(또는 학습)할 수 있다. 도 10은 제1 학습 모델을 변경하는 동작을 설명하기 위한 도면이다. 도 10을 참조하면, 제1 생성기, 제2 생성기는 도 5에서 기재하였으므로, 중복 설명을 생략한다. 학습 모듈은 제1 학습 모델에 포함된 특정 컨볼루션 레이어를 변경할 수 있다. 학습 모듈은 스 트라이드 크기가 2 이상인 컨볼루션 레이어를 스윙 컨볼루션 레이어로 변경할 수 있다. 스윙 컨볼루션 레이어와 관련된 설명은 도 11 및 도 12에서 기재한다. 제1 학습 모델에서 스윙 컨볼루션 레이어가 변경되면, 전자 장치는 변경된 제1 학습 모델을 획득할 수 있다. 변경된 제1 학습 모델은 제2 학습 모델로 기재할 수 있다. 만약, 제2 학습 모델을 기재하는 경 우 도 4의 양자화된 학습 모델은 제3 학습 모델로 기재될 수 있다. 도 11은 스윙 컨볼루션 레이어에서 연산 대상을 선택하는 동작을 설명하기 위한 도면이다. 도 11을 참조하면, 학습 모듈은 스윙 컨볼루션 레이어를 통해 컨볼루션 연산을 수행할 수 있다. 학습 모듈 은 제1 데이터에 패딩 데이터를 추가할 수 있다. 패딩 데이터를 추가하는 동작은 “ reflection padding”일 수 있다. 제1 데이터는 스윙 컨볼루션 레이어에 입력되는 입력 데이터일 수있다. 패딩 데이터는 제1 데이터의 크기가 커지도록 변경하기 위해 제1 데이터의 외곽 영역을 확장 한 데이터일 수 있다. 학습 모듈은 제1 데이터 및 패딩 데이터가 결합된 제2 데이터를 획득할 수 있다. 학습 모듈은 제2 데이터에서 제1 데이터의 크기에 기초하여 특정 영역을 선택함으로써 제3 데 이터를 획득할 수 있다. 특정 영역을 선택하는 기준은 랜덤할 수 있다. 제2 데이터에서 제1 데이터의 크기에 기초하여 특정 영역을 선택하는 경우 복수의 후보 데이터가 존재할 수 있다. 선택 가능한 후보 데이터 중 랜덤하게 하나의 데이터가 선택될 수 있다. 랜덤하게 선택 된 데이터를 제3 데이터로 기재할 수 있다. 제2 데이터에서 특정 영역을 선택하는 동작은 “random cropping”으로 기재할 수 있다. 도 12는 스윙 컨볼루션 레이어에서 연산 대상에 대하여 연산을 수행하는 동작을 설명하기 위한 도면이다. 도 12를 참조하면, 학습 모듈은 스윙 컨볼루션 레이어를 통해 컨볼루션 연산을 수행할 수 있다. 도 11의 실시 예에서 제2 데이터 중 제3 데이터가 랜덤하게 선택되었다고 가정한다. 실시 예를 참조하면, 학습 모듈은 입력 데이터에 대하여 컨볼루션 연산을 수행할 수 있다. 입 력 데이터는 제3 데이터를 의미할 수 있다. 학습 모듈은 입력 데이터 및 커널 데이터 에 기초하여 컨볼루션 연산을 수행함으로써 출력 데이터를 획득할 수 있다. 도 13은 학습 모듈의 학습 동작을 설명하기 위한 흐름도이다. 도 13을 참조하면, 전자 장치는 제1 생성기를 통해 입력 벡터를 획득할 수 있다 (S1310). 전자 장치는 제2 생성기를 통해 입력 벡터에 대응되는 합성 데이터를 획득할 수 있다 (S1320). 전자 장치는 제1 학습 모델의 통계적 특성 데이터 및 합성 데이터의 통계적 특성 데이터를 획득할 수 있다 (S1330). 통계적 특성 데이터는 제1 학습 모델로부터 획득되는 데이터일 수 있다. 제1 학습 모델 의 통계적 특성 데이터는 제1 학습 모델에 포함된 BN(Batch Normalization) 레이어와 관련된 통계적 특성 데이터를 포함할 수 있다. 합성 데이터의 통계적 특성 데이터는 제1 학습 모델에 합성 데이터를 입력하여 출력 데이터로써 획득되는 데이터를 의미할 수 있다. 전자 장치는 제1 학습 모델의 통계적 특성 데이터 및 합성 데이터의 통계적 특성 데이터에 기초하여 손실값을 획득할 수 있다 (S1340). 손실값을 획득하는 동작은 도 9의 수학식이 이용될 수 있다. 전자 장치는 손실값이 최소가되도록 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기 에 포함된 적어도 하나의 파라미터를 학습(또는 업데이트)할 수 있다 (S1350). 제1 생성기에 포함된 적어도 하나의 파라미터는 입력 벡터(또는 잠재 벡터)를 구성하는 파라미터를 포함할 수 있다. 제2 생성기(14 2)에 포함된 적어도 하나의 파라미터는 제2 생성기에 적용되는 적어도 하나의 가중치를 포함할 수 있다. 도 14는 학습 모듈의 학습 동작을 구체적으로 설명하기 위한 흐름도이다. 도 14의 S1410, S1420, S1450 단계는 도 13의 S1310, S1320, S1350 단계에 대응될 수 있다. 따라서, 중복 설명 을 생략한다. 제2 생성기를 통해 합성 데이터가 획득된 후, 전자 장치는 제1 학습 모델을 통해 합성 데이터에 대응되는 평균값 및 표준 편차값을 획득할 수 있다 (S1431). 전자 장치는 제1 학습 모델의 BN 레이어의 평균값 및 표준 편차값을 획득할 수 있다 (S1432). 전자 장치는 합성 데이터의 평균값 및 제1 학습 모델의 BN 레이어의 평균값의 제1 차이값을 획득할 수 있다 (S1441). 제1 차이값은 도 9의 “μl^s-μl”을 의미할 수 있다. 전자 장치는 합성 데이터의 표준 편차값 및 제1 학습 모델의 BN 레이어의 표준 편차값의 제2 차이값 을 획득할 수 있다 (S1442). 제2 차이값은 도 9의 “σl^s-σl”을 의미할 수 있다. 전자 장치는 제1 차이값 및 제2 차이값에 기초하여 손실값을 획득할 수 있다 (S1443). 손실값은 도9의 “ L_BNS”을 의미할 수 있다.도 15는 특정 컨볼루션 레이어를 스윙 컨볼루션 레이어로 대체하는 동작을 명하기 위한 흐름도이다. 도 15의 S1510, S1520, S1530, S1540, S1550 단계는 도 13의 S1310, S1320, S1330, S1340, S1350 단계에 대응 될 수 있다. 따라서, 중복 설명을 생략한다. 통계적 특성 데이터를 획득한 후, 전자 장치는 제1 학습 모델에 포함된 적어도 하나의 컨볼루션 레이 어의 스트라이드(stride) 데이터를 획득할 수 있다 (S1535). 전자 장치는 스트라이드 데이터의 크기가 2 이상인 컨볼루션 레이어가 식별되는지 여부를 판단할 수 있다 (S1536). 스트라이드 데이터의 크기가 2 이상인 컨볼루션 레이어가 식별되면 (S1536-Y), 전자 장치는 식별된 컨볼루 션 레이어를 스윙 컨볼루션 레이어로 대체(또는 변경)할 수 있다 (S1537). 스윙 컨볼루션 레이어에 대한 설명은 도 11 및 도 12에서 기재한다. 이후, 전자 장치는 S1540 및 S1550 단계를 수행할 수 있다. 스트라이드 데이터의 크기가 2 이상인 컨볼루션 레이어가 식별되지 않으면 (S1536-N), 전자 장치는 S1540 및 S1550 단계를 수행할 수 있다. 도15에서는 S1535 내지 S1537 단계가 S1530 단계 이후에 수행되는 것으로 기재하였다. 하지만, 다양한 실시 예 에 따라, S1535 내지 S1537 단계가 S1530 단계 이전에 수행될 수 있다. 도 16은 양자화 과정을 설명하기 위한 도면이다. 도 16을 참조하면, 전자 장치는 압축 모듈을 이용하여 사전 학습 모델에 대하여 양자화 (quantization)를 수행할 수 있다. 사전 학습 모델은 도4의 제1 학습 모델, 도 5의 제1 학습 모델 또는 도 10의 변경된 제1 학습 모델일 수 있다. 압축 모듈은 사전 학습 모델을 티쳐 모 델(Teacher model)로써 이용할 수 있다. 압축 모듈은 사전 학습 모델을 양자화하여 양자화된 모델을 획득할 수 있다. 양자화된 모델 은 도 4의 제2 학습 모델일 수 있다. 압축 모듈은 제2 학습 모델을 스튜던트 모델 (Student model)로써 이용할 수 있다. 압축 모듈은 학습 데이터를 사전 학습 모델 및 양자화된 모델에 모두 공급할 수 있다. 또한, 압축 모듈은 사전 학습 모델에서 출력된 출력 데이터와 양자화된 모델에서 출력된 출력 데이 터에 기초하여 손실값(제2 손실값)을 획득할 수 있다. 그리고, 압축 모듈은 손실값에 기초하여 양자화된 모델을 학습할 수 있다. 구분의 편의를 위해, 학습 모듈에서 획득되는 손실값을 제1 손실값으로 기재하고 압축 모듈에서 획득 되는 손실값을 제2 손실값으로 기재할 수 있다. 도 17은 양자화 과정에서 압축 동작을 설명하기 위한 도면이다. 양자화 동작은 맵핑 동작 또는 스케일링 동작 중 적어도 하나를 포함할 수 있다. 양자화 동작은 범주화 동작으 로 기재될 수 있다. 도 17의 실시 예는 데이터를 맵핑하는 동작을 나타낸다. 데이터는 제1 학습 모델에 포함된 정보(또 는 가중치)를 의미할 수 있다. 예를 들어, 0부터 1a 사이에 복수의 데이터가 존재하는 것으로 가정할 수 있다. 압축 모듈은 스텝 사이즈 (step size)에 기초하여 데이터를 양자화할 수 있다. 스텝 사이즈는 양자화하는데 필요한 데이터 단위를 의미할 수 있다. 스텝 사이즈는 스케일링 펙터(scailing factor)로 기재될 수 있다. 실시 예에서 스텝 사이즈는 a라고 가정한다. 압축 모듈은 0부터 1a 사이에 존재하는 데이터를 “0” 또는 “1a”로 분류할 수 있다. 분류 동작에서 반올림, 올림, 내림 중 적어도 하나의 함수가 이용될 수 있다. 분류 동작은 맵핑 동작으로 기재될 수 있다. 도 17의 실시 예는 데이터를 스케일링하는 동작을 나타낸다. 데이터는 사용자의 설정 또는 기 설정된 크 기만큼 스케일링 될 수 있다. 스케일링 동작에서 스케일링 이전의 데이터가 절대적인 위치로 변경 되는 것이 아닌 상대적인 위치로 변경하는 것이 필요할 수 있다. 이러한 목적을 위해서 압축 모듈에 서 특정 알고리즘이 적용될 수 있다.도 18은 압축 모듈에서 수행되는 학습 동작을 설명하기 위한 도면이다. 도 18의 알고리즘은 압축 모듈에서 수행되는 학습 동작을 나타낸다. 압축 모듈은 수학식(182 0)을 이용하여 스텝 사이즈(self.s)를 획득할 수 있다. 압축 모듈은 수학식을 이용하여 기본 정수형 행렬(base integer matrix, self.B)을 획득할 수 있다. 압축 모듈은 스텝 사이즈(self.s) 및 기본 정수형 행렬(self.B)에 기초하여 소프트비트 행렬(Softbit, self.V)을 획득할 수 있다. 압축 모듈은 제1 학습 모델의 가중치(W)에서 기본 정수형 행렬(self.B) 을 뺄셈함으로써 소프트비트 행렬(self.V)을 획득할 수 있다. 압축 모듈은 스텝 사이즈(self.s), 소프트비트 행렬(self.V) 및 기본 정수형 행렬(self.B)에 기초하여 제2 학습 모델의 가중치(Wq)를 획득할 수 있다. 압축 모듈은 수학식을 이용하여 가중치(Wq)를 획 득할 수 있다. 수학식은 스텝 사이즈(s*)를 계산하기 위한 식일 수 있다. s*는 스텝 사이즈를 나타낼 수 있다. argmin_s(fx)은 fx를 최소로하는 s값을 찾는 함수를 나타낼 수 있다. s는 스텝 사이즈를 나타내는 미지수를 나타낼 수 있다. W는 제1 학습 모델의 가중치를 나타낼 수 있다. clip(gx) 함수는 gx에 해당하는 실수를 정수로 변환하는 함수를 나타낼 수 있다. 수학식의 clip(gx) 함수 는 반올림(nearest-rounding) 기능을 이용할 수 있다. n은 clip(gx)함수에서 변환 하한값을 나타낼 수 있다. p는 clip(gx)함수에서 변환 상한값을 나타낼 수 있다. “|| ||F”는 프로비니우스 노름(Frobinious norm)을 나타낼 수 있다. 수학식은 기본 정수형 행렬(B)를 계산하기 위한 식일 수 있다. clip(gx) 함수는 gx에 해당하는 실수를 정수로 변환하는 함수를 나타낼 수 있다. 수학식의 clip(gx) 함수 는 내림(floor) 기능을 이용할 수 있다. W는 제1 학습 모델의 가중치를 나타낼 수 있다. s는 수학식의 스텝 사이즈(s*)를 나타낼 수 있다. n은 clip(gx)함수에서 변환 하한값을 나타낼 수 있다. p는 clip(gx)함수에서 변환 상한값을 나타낼 수 있다. 수학식은 제2 학습 모델의 가중치(Wq)를 계산하기 위한 식일 수 있다. Wq는 제2 학습 모델의 가중치를 나타낼 수 있다. s는 수학식의 스텝 사이즈(s*)를 나타낼 수 있다. B는 수학식의 기본 정수형 행렬(B)을 나타낼 수 있다. V는 0과 1사이의 값을 갖는 소프트비트(softbit) 행렬(V)을 나타낼 수 있다. 예를 들어, 가중치(1.4)를 1로 변환하였다고 가정한다. 소수점에 해당하는 0.4가 V에 해당할 수 있다. 수학식은 제2 학습 모델의 가중치(Wq)를 미분하는 식일 수 있다. 압축 모듈은 제2 학습 모델 의 가중치(Wq)를 스텝 사이즈(s)로 미분하거나 제2 학습 모델의 가중치(Wq)를 소프트 비트와 관련 된 값(v)으로 미분할 수 있다. 압축 모듈은 스텝 사이즈(s) 및 소프트 비트와 관련된 값(v)에 기초하여 학 습 동작을 수행할 수 있다. 압축 모듈은 기본 정수형 행렬과 관련된 값(b)에 대하여 학습 동작을 수행하지 않을 수 있다. 기본 정수형 행렬과 관련된 값(b)은 상수를 의미할 수 있기 때문이다.도 19는 외부 장치에 제2 학습 모델을 전송하는 동작을 설명하기 위한 흐름도이다. 도 19를 참조하면, 외부 장치는 전자 장치에 타겟 모델을 요청할 수 있다 (S1910). 전자 장치는 외부 장치로부터 수신된 요청에 응답하여 제2 학습 모델을 획득할 수 있다. 전자 장치는 제1 학습 모델의 가중치(W)를 획득할 수 있다. 전자 장치는 제1 학습 모델의 스텝 사이즈(s)를 획득할 수 있다 (S1920). 전자 장치는 도 18의 수학식을 이용하여 스텝 사이즈(s)를 획득할 수 있다. 제1 학습 모델는 변경된 제1 학습 모델 로 대체될 수 있다. 전자 장치는 스텝 사이즈(s)에 기초하여 제1 학습 모델의 가중치(W)에 대응되는 기본 정수형 행렬 (B)을 획득할 수 있다 (S1930). 전자 장치는 도 18의 수학식을 이용하여 기본 정수형 행렬(B)을 획 득할 수 있다. 전자 장치는 스텝 사이즈(s), 제1 학습 모델의 가중치(W), 기본 정수형 행렬(B)에 기초하여 소프트 비트 행렬(V)을 획득할 수 있다 (S1935). 전자 장치는 스텝 사이즈(s), 기본 정수형 행렬(B) 및 소프트 비트 행렬(V)에 기초하여 제2 학습 모델 의 가중치에 대한 식(도 18의 1840)을 획득할 수 있다 (S1940). 전자 장치는 제2 학습 모델의 가중치에 대한 식(도 18의 1840)을 스텝 사이즈(s) 및 소프트 비트 행 렬에 대한 값(v)에 기초하여 학습할 수 있다. 학습 동작은 도 18의 수학식을 이용하여 수행될 수 있다. 전자 장치는 학습 결과로써 제2 학습 모델을 획득할 수 있다 (S1960). 전자 장치는 제2 학습 모 델을 외부 장치에 전송할 수 있다 (S1970). 외부 장치는 전자 장치로부터 수신한 제2 학습 모델에 기초하여 타겟 데이터를 생성할 수 있다 (S1980). 타겟 데이터는 제2 학습 모델의 출력 데이 터이며, 사용자에게 제공되는 서비스 정보를 의미할 수 있다. 도 20은 가상 이미지 생성 프로그램과 관련된 화면을 설명하기 위한 도면이다. 도 20을 참조하면, 전자 장치에서 생성되는 합성 데이터가 가상 이미지 데이터인 것으로 가정한다. 전자 장치는 디스플레이(미도시)를 통해 가상 이미지 생성과 관련된 화면을 제공할 수 있다. 화면은 타겟 데이터를 나타내는 UI(User, Interface, 2010), 생성된 가상 이미지 데이터를 나타내는 UI, 사용자 선택을 가이드하는 UI 중 적어도 하나를 포함할 수 있다. UI는 사용자에 의하여 설정된 타겟 정보(또는 타겟 데이터)를 의미할 수 있다. 도 20의 실시 예에서는 개 (dog)를 생성하도록 사용자가 결정하였다고 가정한다. UI는 제2 생성기를 통해 생성되는 합성 데이터(가상 이미지)를 포함할 수 있다. UI는 생성된 합성 데이터(가상 이미지)를 사용자 입력에 대응하여 처리하기 위한 적어도 하나의 세부 UI(2031, 2032, 2033, 2034, 2035)를 포함할 수 있다. UI는 가상 이미지 갱신을 위한 아이콘 또는 텍스트 중 적어도 하나를 포함할 수 있다. UI는 가상 이미지 저장을 위한 아이콘 또는 텍스트 중 적어도 하나를 포함할 수 있다. UI는 가상 이미지 공유를 위한 아이콘 또는 텍스트 중 적어도 하나를 포함할 수 있다. UI는 오류 이미지 선택을 위한 아이콘 또는 텍스트 중 적어도 하나를 포함할 수 있다. UI는 타겟 데이터의 변경을 위한 아이콘 또는 텍스트 중 적어도 하나를 포함할 수 있다. 다양한 실시 예에 따라, 전자 장치는 학습 모듈에 의해 학습 동작이 완료된 후 합성 데이터 생성과 관련된 화면을 표시할 수 있다. 학습이 완료된 학습 모듈의 성능을 확인해볼 수 있기 때문이다. 오 류 이미지를 선택하는 경우 학습 모듈의 학습 동작을 새로 수행할 수 있다. UI를 통해 오류 이미지 가 선택되면, 전자 장치는 학습 모듈을 재학습(또는 재업데이트)할 수 있다. 도 21은 다양한 실시 예에 따라, 학습 모듈의 학습 동작을 설명하기 위한 흐름도이다. 도 21을 참조하면, 전자 장치는 제1 학습 모델의 BN(Batch Normalization) 레이어 별 엑티베이션 (activation)의 통계적 특성 데이터(평균값 및 표준 편차값) 획득할 수 있다 (S2110). 전자 장치는 제1 학습 모델에 포함된 컨볼루션 레이어 중 스트라이드 크기가 2 이상인 컨볼루션 레이 어의 연산을 변경할 수 있다 (S2120). 연산 변경 동작을 통해 합성 데이터의 품질이 개선될 수 있다. 전자 장치는 제1 생성기의 입력 벡터(잠재 벡터) 초기화할 수 있다 (S2130). 전자 장치는 입력 벡터를 제2 생성기에 입력함으로써 합성 데이터 획득할 수 있다 (S2140). 전자 장치는 합성 데이터의 통계적 특성 데이터(평균값 및 표준 편차값)를 획득할 수 있다 (S2150). 전자 장치는 제1 학습 모델의 BN(Batch Normalization) 레이어 별 엑티베이션(activation)의 통계적 특성 데이터와 합성 데이터의 통계적 특성 데이터에 기초하여 손실값 획득할 수 있다 (S2160). 전자 장치는 손실값이 최소가 되도록 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기 에 포함된 적어도 하나의 파라미터를 학습할 수 있다 (S2170). 학습 모듈에 의한 학습 동작에서 제1 학습 모델이 하나인 것으로 기재하였다. 다양한 실시 예에 따라, 제1 학습 모델은 복수의 모델을 포함할 수 있다. 도 21에서 수행되는 동작은 표준 편차 값 대신 분산값이 이용될 수 있다. 도 22는 다양한 실시 예에 따라, 압축 모듈의 학습 동작을 설명하기 위한 흐름도이다. 도 22를 참조하면, 전자 장치는 적어도 하나의 합성 데이터에 기초하여 합성 데이터 세트 생성할 수 있다 (S2210). 적어도 하나의 합성 데이터는 제2 생성기를 통해 생성된 데이터일 수 있다. 전자 장치는 제1 학습 모델(Teacher Model, 143)로부터 양자화된 제2 학습 모델 (Student Model, 153) 초 기화할 수 있다 (S2220). 전자 장치는 생성된 합성 데이터 세트에 기초하여 제1 학습 모델 및 제2 학습 모델 사이의 양자 화 오류를 최소화하도록 제2 학습 모델의 라운딩(rounding) 및 스케일(scale)을 학습할 수 있다 (S2230). 제2 학습 모델의 양자화 학습 동작은 레이어 단위, 블록 단위, Network 단위, 또는 여러 레이어의 출력을 결합한 단위로 최적화될 수 있다. 가상의 데이터를 생성하는 동작과 모델 양자화 동작은 동시에 진행될 수도 있다. 제2 생성기는 양자화 오 류를 고려하여 합성 데이터를 생성하도록 학습될 수 있다. 전자 장치는 bit-code와 scale factor간의 mutual dependency를 해제하여 joint optimization을 수행할 수 있다. 제2 학습 모델의 양자화 학습 동작은 합성 데이터(가상 데이터)가 아닌 실제 데이터로 수행될 수 있다. 전자 장치는 학습 데이터에 접근할 수 없는 경우(학습 데이터가 존재하지 않는 경우)에 매우 적은 학습 시 간만으로도 기존보다 더 높은 양자화 정확도를 가지는 모델을 학습할 수 있다. 양자화 정확도가 높아져 외부 장치(200, 모바일 기기)에서 더 정확한 모델 추론이 가능해질 수 있다. 같은 정확 도에서 더 낮은 양자화 비트 수를 가질 수 있기 때문에 더 효율적이고 빠른 AI 모델 추론이 가능해진다. 도 23은 다양한 실시 예에 따라, 전자 장치의 제어 방법을 설명하기 위한 도면이다. 도 23을 참조하면, 다양한 실시 예에 따른 입력 벡터를 생성하는 제1 생성기, 합성 데이터(synthetic data)를 생성하는 제2 생성기 및 합성 데이터를 분석하는 제1 학습 모델을 포함하는 학습 모듈을 저장하는 전자 장치의 제어 방법은 제1 생성기를 통해 입력 벡터를 획득하는 단계 (S2305), 입력 벡터를 제2 생성기 에 입력함으로써, 입력 벡터에 대응되는 합성 데이터를 획득하는 단계 (S2310), 합성 데이터를 제1 학습 모델에 입력함으로써, 합성 데이터를 분석한 출력 데이터를 획득하는 단계 (S2315) 및 출력 데이터에 기초하여 제1 생성기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학 습하는 단계 (S2320)를 포함한다. 한편, 학습하는 단계 (S2320)는 출력 데이터에 기초하여 손실값을 획득하고, 손실값이 최소가 되도록 제1 생성 기에 포함된 적어도 하나의 파라미터 및 제2 생성기에 포함된 적어도 하나의 파라미터를 학습할 수 있다.한편, 출력 데이터는 합성 데이터의 통계적 특성 데이터를 포함할 수 있다. 한편, 출력 데이터는 합성 데이터의 평균값 및 표준 편차값을 포함할 수 있고, 손실값을 획득하는 단계는 합성 데이터의 평균값 및 제1 학습 모델의 BN(Batch Normalization) 레이어의 평균값의 제1 차이값을 획득하고, 합성 데이터의 표준 편차값 및 제1 학습 모델의 BN(Batch Normalization) 레이어의 표준 편차값의 제2 차이값을 획득 하고, 제1 차이값 및 제2 차이값에 기초하여 손실값을 획득할 수 있다. 한편, 제어 방법은 제1 학습 모델에 포함된 적어도 하나의 컨볼루션 레이어의 스트라이드(stride) 데이터를 획 득하는 단계, 적어도 하나의 컨볼루션 레이어 중 스트라이드 데이터의 크기가 2 이상인 컨볼루션 레이어가 식별 되면, 식별된 컨볼루션 레이어를 스윙(swing) 컨볼루션으로 대체하는 단계를 더 포함하고, 스윙 컨볼루션 레이 어는 패딩 데이터에 기초하여 연산의 대상을 랜덤하게 선택하는 컨볼루션 레이어일 수 있다. 한편, 스윙 컨볼루션 레이어는 제1 데이터가 입력되면, 제1 데이터에 패딩 데이터를 추가하여 제2 데이터를 획 득하는 동작, 제1 데이터의 크기에 기초하여 제2 데이터에서 일부 데이터 영역을 선택하여 제3 데이터를 획득하 는 동작 및 제3 데이터 및 식별된 컨볼루션 레이어의 커널 데이터에 기초하여 컨볼루션 연산을 수행하는 동작을 포함하는 레이어일 수 있다. 한편, 제1 생성기는 적어도 하나의 파라미터에 기초하여 잠재 벡터(latent vector)를 생성하는 생성기이고, 제1 생성기에 포함된 적어도 하나의 파라미터는 사용자에 의해 설정된 타겟과 관련된 합성 데 이터를 생성하는데 이용되는 파라미터일 수 있다. 한편, 합성 데이터는 사용자에 의해 설정된 타겟과 관련된 이미지 데이터일 수 있다. 한편, 제어 방법은 제1 학습 모델을 양자화(quantization)함으로써 제2 학습 모델을 획득하는 단계를 더 포함하 고, 제2 학습 모델은 제1 학습 모델이 압축된 모델일 수 있다. 한편, 제어 방법은 제2 학습 모델을 외부 장치에 전송하는 단계를 더 포함할 수 있다. 한편, 도 23과 같은 전자 장치의 제어 방법은 도 2의 구성을 가지는 전자 장치 상에서 실행될 수 있으며, 그 밖 의 구성을 가지는 전자 장치 상에서도 실행될 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 설치 가능한 어플리케이션 형태 로 구현될 수 있다. 또한, 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 대한 소프트웨어 업그레이드, 또 는 하드웨어 업그레이드 만으로도 구현될 수 있다. 또한, 상술한 본 개시의 다양한 실시 예들은 전자 장치에 구비된 임베디드 서버, 또는 전자 장치 및 디스플레이 장치 중 적어도 하나의 외부 서버를 통해 수행되는 것도 가능하다. 한편, 본 개시의 일시 예에 따르면, 이상에서 설명된 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media)에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실 시 예들에 따른 전자 장치를 포함할 수 있다. 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 프 로세서의 제어 하에 다른 구성요소들을 이용하여 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또 는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장 매체는, 비일시적 (non-transitory) 저장 매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장 매체가 신호(signal)를 포함 하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장 매체에 반영구적 또는 임시적으로 저장됨을 구분하지 않는다. 또한, 본 개시의 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품 (computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있 다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 또한, 상술한 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구 성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로 그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시 예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동 작들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2023-0010313", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형 실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2023-0010313", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 전자 장치 및 외부 장치를 포함하는 시스템을 설명하기 위한 도면이다. 도 2는 전자 장치에 포함된 구성을 도시한 블록도이다. 도 3은 외부 장치에 포함된 구성을 도시한 블록도이다. 도 4는 학습 모듈 및 압축 모듈을 설명하기 위한 도면이다.도 5는 학습 모듈이 수행하는 동작을 설명하기 위한 도면이다. 도 6은 학습 모듈이 수행하는 동작을 구체적으로 설명하기 위한 도면이다. 도 7은 학습 모듈이 수행하는 순전파(Forward Propagation) 및 역전파(Back Propagation) 과정을 설명하기 위 한 도면이다. 도 8은 컨볼루션 연산과 트랜스포즈드(Transposed) 컨볼루션 연산을 설명하기 위한 도면이다. 도 9는 학습 모듈에서 수행되는 학습 동작을 설명하기 위한 도면이다. 도 10은 제1 학습 모델을 변경하는 동작을 설명하기 위한 도면이다. 도 11은 스윙 컨볼루션 레이어에서 연산 대상을 선택하는 동작을 설명하기 위한 도면이다. 도 12는 스윙 컨볼루션 레이어에서 연산 대상에 대하여 연산을 수행하는 동작을 설명하기 위한 도면이다. 도 13은 학습 모듈의 학습 동작을 설명하기 위한 흐름도이다. 도 14는 학습 모듈의 학습 동작을 구체적으로 설명하기 위한 흐름도이다. 도 15는 특정 컨볼루션 레이어를 스윙 컨볼루션 레이어로 대체하는 동작을 명하기 위한 흐름도이다. 도 16은 양자화 과정을 설명하기 위한 도면이다. 도 17은 양자화 과정에서 압축 동작을 설명하기 위한 도면이다. 도 18은 압축 모듈에서 수행되는 학습 동작을 설명하기 위한 도면이다. 도 19는 외부 장치에 제2 학습 모델을 전송하는 동작을 설명하기 위한 흐름도이다. 도 20은 가상 이미지 생성 프로그램과 관련된 화면을 설명하기 위한 도면이다. 도 21은 다양한 실시 예에 따라, 학습 모듈의 학습 동작을 설명하기 위한 흐름도이다. 도 22는 다양한 실시 예에 따라, 압축 모듈의 학습 동작을 설명하기 위한 흐름도이다. 도 23은 다양한 실시 예에 따라, 전자 장치의 제어 방법을 설명하기 위한 도면이다."}
