{"patent_id": "10-2019-0029736", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0109914", "출원번호": "10-2019-0029736", "발명의 명칭": "자연어 처리 시스템, 그 학습 방법 및 프로그램이 기록된 컴퓨터 판독가능한 기록매체", "출원인": "에스케이텔레콤 주식회사", "발명자": "이진식"}}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "의미론적 벡터 학습에 기반한 자연어 처리 시스템에 있어서,원본 텍스트를 기초로 제1 시멘틱 벡터를 생성하는 텍스트 리더부;상기 원본 텍스트에 대응되는 원본 시멘틱 프레임을 기초로 제2 시멘틱 벡터를 생성하는 프레임 리더부; 및상기 제1 시멘틱 벡터 또는 상기 제2 시멘틱 벡터 중 하나를 기초로 상기 원본 시멘틱 프레임과 동일하거나 축소된 형식을 갖는 재구성 시멘틱 프레임을 생성하는 프레임 생성부를 포함하되,상기 제1 시멘틱 벡터와 상기 제2 시멘틱 벡터의 차이 및 상기 원본 프레임과 상기 재구성 시멘틱 프레임의 차이 중 적어도 어느 하나가 작아지도록 학습하는자연어 처리 시스템."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 제1 시멘틱 벡터 및 상기 제2 시멘틱 벡터의 차이에 상응하는 거리 손실 및 상기 원본프레임에 대한 상기 재구성 시멘틱 프레임의 재구성 손실에 대응하는 콘텐츠 손실이 작아지도록 학습하는자연어 처리 시스템."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서, 상기 원본 텍스트 및 상기 원본 시멘틱 프레임은 동일한 의미를 가지나 상이한 구조로 표현되는 대응쌍들을 포함하는,자연어 처리 시스템."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서, 상기 원본 텍스트은 순차적으로 나열된 문자열로 구성되고, 상기 원본 시멘틱 프레임은 의도태그, 슬롯 태그 및 슬롯 값으로 구성되는,자연어 처리 시스템."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서, 상기 텍스트 리더부는 RNN 셀로 구성되는,자연어 처리 시스템."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4 항에 있어서, 상기 프레임 리더부는 원본 시멘틱 프레임의 의도 태그를 분산 표현된 벡터로 반환하도록 구현된 의도 리더부 및 RNN 셀로 구성되며 슬롯 태그 및 슬롯 값들의 시퀀스를 판독하여 분산 표현된 벡터로 반환하도록 구현된 슬롯 리더부를 포함하는,자연어 처리 시스템."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4 항에 있어서, 상기 프레임 생성부는 재구성 의도 태그를 생성하는 의도 생성부 및 재구성 슬롯 태그를 생성하는 슬롯 태그 생성부를 포함하고, 상기 프레임 생성부는 상기 재구성 의도 태그 및 상기 재구성 슬롯 태그로구성된 축소된 시멘틱 프레임을 생성하고, 상기 축소된 시멘틱 프레임을 기초로 콘텐츠 손실이 계산되는,공개특허 10-2020-0109914-3-자연어 처리 시스템."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "입력된 원본 텍스트를 기초로 제1 시멘틱 벡터를 생성하는 텍스트 리더부 및 하나 이상의 자연어 이해 유닛에서생성된 시멘틱 프레임을 기초로 하나 이상의 제2 시멘틱 벡터를 생성하는 하나 이상의 프레임 리더부를 포함하며, 생성된 제1 시멘틱 벡터 및 하나 이상의 제2 시멘틱 벡터를 기초로 상기 하나 이상의 자연어 이해 유닛의성능을 계산하는 평가부를 포함하는,자연어 처리 시스템."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서, 상기 평가부는 제1 시멘틱 벡터 및 상기 하나 이상의 제2 시멘틱 벡터 사이의 거리를 계산하고 계산된 거리를 기초로 상기 하나 이상의 자연어 이해 유닛의 성능을 평가하는,자연어 처리 시스템."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9 항에 있어서, 상기 평가부는 제1 시멘틱 벡터에 대한 복수의 자연어 이해 유닛에서 생성된 제2 시멘틱 벡터들의 거리를 계산하고 계산된 거리가 가장 짧은 시멘틱 벡터에 대응하는 시멘틱 프레임을 생성하는 자연어 이해유닛을 최우선 순위로 결정하는,자연어 처리 시스템."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "원본 텍스트를 제1 시멘틱 벡터로 인코딩하는 단계;원본 시멘틱 프레임을 제2 시멘틱 벡터로 인코딩하는 단계;상기 제1 시멘틱 프레임을 기초로 재구성 시멘틱 프레임을 생성하는 단계; 및상기 제1 시멘틱 벡터와 상기 제2 시멘틱 벡터의 차이 및 상기 원본 시멘틱 프레임과 상기 재구성 시멘틱 프레임 사이의 차이 중 하나 이상이 감소되도록 학습되는 학습단계를 포함하는 자연어 처리 시스템의 학습 방법."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "상기 학습단계는,상기 제1 시멘틱 벡터 및 상기 제2 시멘틱 벡터 사이의 거리 손실을 계산하는 단계;상기 원본 시멘틱 프레임과 상기 재구성 시멘틱 프레임 사이의 컨텐츠 손실을 계산하는 단계; 및상기 거리 손실 및 상기 컨텐츠 손실이 감소되도록 학습되는 단계를 포함하는 자연어 처리 시스템의 학습 방법."}
{"patent_id": "10-2019-0029736", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11 항에 따른 방법을 구현하기 위한 프로그램을 기록한 컴퓨터 판독 가능한 기록매체."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "의미론적 벡터 학습에 기반한 자연어 처리 시스템이 개시된다. 자연어 처리 시스템은, 원본 텍스트를 기초로 제1 시멘틱 벡터를 생성하는 텍스트 리더부; 상기 원본 텍스트와 상이한 형식을 갖는 원본 시멘틱 프레임을 기초로 제2 시멘틱 벡터를 생성하는 프레임 리더부; 및 상기 제1 시멘틱 벡터 또는 상기 제2 시멘틱 벡터 중 하나를 기 초로 상기 원본 시멘틱 프레임과 동일하거나 축소된 형식을 갖는 재구성 시멘틱 프레임을 생성하는 프레임 생성 부를 포함한다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 자연어 처리 시스템, 그 학습 방법 및 프로그램이 기록된 컴퓨터 판독가능한 기록매체에 관한 것이다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "사람의 언어는 풍부하고 복잡하여, 복잡한 문법 및 문맥 의미를 갖는 많은 수의 어휘들을 포함하고 있다. 사람 의 언어를 기계가 해석하는 것은 매우 복잡한 작업이며 그 효율화를 위해 지속적으로 연구되고 있다. 특히, 최 근 소위 딥러닝 이라고 불리는 새로운 인공지능 학습 방법이 대두되고 있으며, 이 딥러닝을 이용해 복잡한 자연 어 이해의 문제를 해결하려는 시도가 이루어지고 있다. 딥러닝을 이용해 자연어 처리 시스템 또는 자연어 이해 시스템을 학습하기 위해서는, 자연어, 즉, 단어들이 순 차적으로 배열된 문장에서 각 단어들의 유사성을 추출할 수 있는 수단이 필요하다. 그 일환으로서, WORD2VEC 알고리즘이 개발되었다. WORD2VEC은 단어들을 어떠한 벡터 값으로 변환하되, 그 단어 의 동시 등장 또는 동시 인접 등장 빈도수를 기반으로 단어의 유사성을 표현한다. 그러나, 이러한 WORD2VEC은 문장이 아닌 텍스트가 구조화된 의미론적 지식을 전체적으로 추출하는 것, 즉, 문장 의 의도(INTENTION) 및 맥락(CONTEXT)을 고려하여 단어들의 유사성을 추출하는 것에는 적절한 결과를 보여주지 못하고 있다. 선행기술문헌 비특허문헌 (비특허문헌 0001) 비특허문헌 1: Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in neural information processing systems, pages 2787-2795. (비특허문헌 0002) 비특허문헌 2: Yun-Nung Chen, Dilek Hakanni-T¨ur, Gokhan Tur, Asli Celikyilmaz, Jianfeng Guo, and Li Deng. 2016. Syntax or semantics? knowledge-guided joint semantic frame parsing. In Spoken Language Technology Workshop (SLT), 2016 IEEE, pages 348-355. IEEE. (비특허문헌 0003) 비특허문헌 3: Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a similarity metric discriminatively, with application to face verification. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 539-546. IEEE. (비특허문헌 0004) 비특허문헌 4: Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493-2537. (비특허문헌 0005) 비특허문헌 5: Thomas Dietterich. 2002. Machine learning for sequential data: A review. Structural, syntactic, and statistical pattern recognition, pages 227-246. (비특허문헌 0006) 비특허문헌 6: Kien Do, Truyen Tran, and Svetha Venkatesh. 2018. Knowledge graph embedding with multiple relation projections. arXiv preprint arXiv:1801.08641. (비특허문헌 0007) 비특허문헌 7: Dilek Hakkani-Tur, G¨okhan Tur, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-Yi Wang. 2016. Multi-domain joint semantic frame parsing using bi- directional rnn-lstm. In Interspeech, pages 715-719. (비특허문헌 0008) 비특허문헌 8: Yulan He and Steve Young. 2005. Semantic processing using the hidden vector state model. Computer speech & language, 19:85-106. (비특허문헌 0009) 비특허문헌 9: Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. science, 313:504-507. (비특허문헌 0010) 비특허문헌 10: Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9:1735-1780. (비특허문헌 0011) 비특허문헌 11: Minwoo Jeong and Gary Geunbae Lee. 2006. Jointly predicting dialog act and named entity for spoken language understanding. In Spoken Language Technology Workshop, 2006. IEEE, pages 66-69. IEEE. (비특허문헌 0012) 비특허문헌 12: Joo-Kyung Kim, Gokhan Tur, Asli Celikyilmaz, Bin Cao, and Ye-YiWang. 2016. Intent detection using semantically enriched word embeddings. In Spoken Language Technology Workshop (SLT), 2016 IEEE, pages 414-419. IEEE. (비특허문헌 0013) 비특허문헌 13: Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882. (비특허문헌 0014) 비특허문헌 14: Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International Conference on Machine Learning, pages 1188-1196. (비특허문헌 0015) 비특허문헌 15: Changki Lee. 2017. Lstm-crf models for named entity recognition. IEICE Transactions on Information and Systems, 100:882-887. (비특허문헌 0016) 비특허문헌 16: Bing Liu and Ian Lane. 2016. Attention-based recurrent neural network models for joint intent detection and slot filling. arXiv preprint arXiv:1609.01454. (비특허문헌 0017) 비특허문헌 17: Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using tsne. Journal of Machine Learning Research, 9(Nov):2579-2605. (비특허문헌 0018) 비특허문헌 18: Gregoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xiaodong He, Larry Heck, Gokhan Tur, Dong Yu, et al. 2015. Using recurrent neural networks for slot filling in spoken language understanding. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 23:530-539. (비특허문헌 0019) 비특허문헌 19: Gregoire Mesnil, Xiaodong He, Li Deng, and Yoshua Bengio. 2013. Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding. In Interspeech, pages 3771-3775. (비특허문헌 0020) 비특허문헌 20: Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. (비특허문헌 0021) 비특허문헌 21: Jonas Mueller and Aditya Thyagarajan. 2016. Siamese recurrent architectures for learning sentence similarity. In AAAI, pages 2786-2792. (비특허문헌 0022) 비특허문헌 22: Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543. (비특허문헌 0023) 비특허문헌 23: Anne Preller. 2014. From logical to distributional models. arXiv preprint arXiv:1412.8527. (비특허문헌 0024) 비특허문헌 24: Patti J Price. 1990. Evaluation of spoken language systems: The atis domain. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990. (비특허문헌 0025) 비특허문헌 25: Suman V Ravuri and Andreas Stolcke. 2015. Recurrent neural network and lstm models for lexical utterance classification. In INTERSPEECH, pages 135-139. (비특허문헌 0026) 비특허문헌 26: Richard Schwartz, Scott Miller, David Stallard, and John Makhoul. 1997. Hidden understanding models for statistical sentence understanding. In Acoustics, Speech, and Signal Processing, 1997. ICASSP-97., 1997 IEEE International Conference on, volume 2, pages 1479-1482. IEEE. (비특허문헌 0027) 비특허문헌 27: Puyang Xu and Ruhi Sarikaya. 2013. Convolutional neural network based triangular crf for joint intent detection and slot filling. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 78-83. IEEE."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "이에, 본 발명이 해결하고자 하는 과제는 텍스트와 이에 대응하는 구조화된 의미론적 지식 간의 관계를 분석하 고 이해할 수 있는 학습 프레임워크 및 이를 사용하는 의미론적 벡터 학습에 기반한 자연어 처리 시스템을 제공 하는 것이다. 또한, 본 발명이 해결하고자 하는 다른 과제는 시각화, 의미론적 검색 및 자연어 이해 유닛에 대한 평가가 편리 하게 이루어질 수 있는 자연어 처리 시스템을 제공하는 것이다. 본 발명이 해결하고자 하는 과제들은 이상에서 언급한 과제들로 제한되지 않으며, 언급되지 않은 또 다른 과제 들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 과제를 해결하기 위하여 본 발명의 일 측면에 따른 자연어 처리 시스템은 의미론적 벡터 학습에 기반하며, 이 자연어 처리 시스템은, 원본 텍스트를 기초로 제1 시멘틱 벡터를 생성하는 텍스트 리더부; 상기 원본 텍스트 와 상이한 형식을 갖는 원본 시멘틱 프레임을 기초로 제2 시멘틱 벡터를 생성하는 프레임 리더부; 및 상기 제1 시멘틱 벡터 또는 상기 제2 시멘틱 벡터 중 하나를 기초로 상기 원본 시멘틱 프레임과 동일하거나 축소된 형식 을 갖는 재구성 시멘틱 프레임을 생성하는 프레임 생성부를 포함한다. 또한, 본 학습 프레임워크에서, 상기 제1 시멘틱 벡터와 상기 제2 시멘틱 벡터의 차이 및 상기 원본 프레임과 상기 재구성 시멘틱 프레임의 차이 중 적어도 하나가 작아지도록 학습된다. 또한, 상기 제1 시멘틱 벡터 및 상기 제2 시멘틱 벡터의 차이에 상응하는 거리 손실 및 상기 원본 프레임에 대 한 상기 재구성 시멘틱 프레임의 재구성 손실에 대응하는 콘텐츠 손실이 작아지도록 학습된다. 또한, 상기 원본 텍스트 및 상기 원본 시멘틱 프레임은 동일한 의미를 가지나 상이한 구조로 표현되는 대응쌍들 을 포함한다. 또한, 상기 원본 텍스트는 순차적으로 나열된 문자열로 구성되고, 상기 원본 시멘틱 프레임은 의도 태그, 슬롯 태그 및 슬롯 값으로 구성된다. 또한, 상기 텍스트 리더부는 RNN 셀로 구성된다. 또한, 상기 프레임 리더부는 원본 시멘틱 프레임의 의도 태그를 분산 표현된 벡터로 반환하도록 구현된 의도 리 더부 및 RNN 셀로 구성되며 슬롯 태그 및 슬롯 값들의 시퀀스를 판독하여 분산 표현된 벡터로 반환하도록 구현 된 슬롯 리더부를 포함한다. 또한, 상기 프레임 생성부는 재구성 의도 태그를 생성하는 의도 생성부 및 재구성 슬롯 태그를 생성하는 슬롯 태그 생성부를 포함하고, 상기 프레임 생성부는 상기 재구성 의도 태그 및 상기 재구성 슬롯 태그로 구성된 축 소된 시멘틱 프레임을 생성하고, 상기 축소된 시멘틱 프레임을 기초로 콘텐츠 손실이 계산된다. 한편, 상기 과제를 해결하기 위하여, 본 발명의 다른 측면에 따른 자연어 처리 시스템은, 입력된 원본 텍스트를 기초로 제1 시멘틱 벡터를 생성하는 텍스트 리더부 및 하나 이상의 자연어 이해 유닛에서 생성된 시멘틱 프레임 을 기초로 하나 이상의 제2 시멘틱 벡터를 생성하는 하나 이상의 프레임 리더부를 포함하며, 생성된 제1 시멘틱 벡터 및 하나 이상의 제2 시멘틱 벡터를 기초로 상기 하나 이상의 자연어 이해 유닛의 성능을 계산하는 평가부 를 포함한다. 또한, 상기 평가부는 제1 시멘틱 벡터 및 상기 하나 이상의 제2 시멘틱 벡터 사이의 거리를 계산하고 계산된 거 리를 기초로 상기 하나 이상의 자연어 이해 유닛의 성능을 평가한다. 또한, 상기 평가부는 제1 시멘틱 벡터에 대한 복수의 자연어 이해 유닛에서 생성된 제2 시멘틱 벡터들의 거리를 계산하고 계산된 거리가 가장 짧은 시멘틱 벡터에 대응하는 시멘틱 프레임을 생성하는 자연어 이해 유닛을 최우 선 순위로 결정한다.한편, 상기와 같은 과제를 해결하기 위하여, 본 발명의 또 다른 측면에 따른 자연어 처리 시스템의 학습 방법은 원본 텍스트를 제1 시멘틱 벡터로 인코딩하는 단계; 원본 시멘틱 프레임을 제2 시멘틱 벡 터로 인코딩하는 단계; 상기 제1 시멘틱 프레임을 기초로 재구성 시멘틱 프레임을 생성하는 단계; 상기 제1 시 멘틱 벡터와 상기 제2 시멘틱 벡터의 차이 및 상기 원본 시멘틱 프레임과 상기 재구성 시멘틱 프레임 사이의 차 이 중 하나 이상이 감소되도록 학습되는 학습단계를 포함한다.또한, 상기 학습단계는, 상기 제1 시멘틱 벡터 및 상기 제2 시멘틱 벡터 사이의 거리 손실을 계산하는 단계; 상 기 원본 시멘틱 프레임과 상기 재구성 시멘틱 프레임 사이의 컨텐츠 손실을 계산하는 단계; 및 상기 거리 손실 및 상기 컨텐츠 손실이 감소되도록 학습되는 단계를 포함한다. 한편, 상기와 같은 과제를 해결하기 위하여, 본 발명의 또 다른 측면에 따른 컴퓨터 판독 가능한 기록매체는 전 술한 자연어 처리 시스템의 학습 방법을 구현하기 위한 프로그램을 기록한다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 일부 실시예들을 예시적인 도면을 통해 상세하게 설명한다. 각 도면의 구성 요소들에 참조 부 호를 부가함에 있어서, 동일한 구성 요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 부 호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 발명을 설명함에 있어, 관련된 공지 구성 또는 기능에 대 한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략한다. 본 명세서에서 사용되는 바와 같이, 용어 \"유닛\" 및 \"시스템\" 등은 컴퓨터 관련 개체, 즉 하드웨어, 하드웨어와 소프트웨어의 조합, 소프트웨어 또는 실행 중인 소프트웨어 중 어느 하나를 지칭하기 위한 것이다. 예를 들어, 유닛은 프로세서 상에서 실행되는 프로세스, 프로세서, 객체, 실행 파일, 실행 쓰레드, 프로그램 및/또는 컴퓨 터 및 이를 포함하는 전자 기기일 수 있지만, 이에 한정되지 않는다. 예로서, 컴퓨터 상에서 실행되는 애플리케 이션 및 컴퓨터 둘 다가 유닛일 수 있다. 하나 이상의 유닛이 프로세스 및/또는 실행 쓰레드 내에 존재할 수 있 으며, 유닛은 하나의 컴퓨터 상에 로컬화 되거나 또는 2 개 이상의 컴퓨터, 예를 들어, 휴대용 단말기기 및 원 격 서버 간에 분산되어 있을 수 있다. 도 1은 본 발명의 일 측면에 따른 자연어 처리 시스템을 나타내는 블록도이다. 도 1을 참조하면, 본 발명은 어플리케이션 및 자연어 처리 유닛을 포함하는 자연어 처리 시스템으로 구현 될 수 있다. 자연어 처리 유닛은 그 사용 목적에 따라 자연어 이해 유닛, 자연어 생성 유닛 또는 양자를 모두 포 함할 수 있다. 자연어 이해 유닛은 자연어에서 의미를 추출하여 그 의도 및 문맥적 의미들을 판단하는 유닛일 수 있다. 자 연어 이해 유닛은 일반적으로 자연어로 표현된 발화자 또는 사용자의 의도를 식별하고 그 의도에 상응하는정보들을 추출하는 작업을 수행할 수 있다. 의도에 상응하는 정보들은 구조화된 텍스트의 형태로 처리될 수 있 다. 이 구조화된 텍스트는 그 의도를 어떻게 수행하는 지에 관한 자연어 문장의 구조인 슬롯들 및 대응하는 슬 롯들에 채워진 값들의 형태로 정의될 수 있다. 자연어 이해 유닛이 자연어로부터 구조화된 텍스트를 생성하 는 작업은 소위 슬롯 채움(Mesnil 외, 2013, Jeong and Lee, 2006, Kim et al., 2016)이라고 불린다. 본 발명의 일 실시예에서, 자연어 처리 유닛은 자연어 입력으로부터 추출한 의도 및 채워진 슬롯들이 구조화 된 데이터를 생성하며 이 구조화된 데이터는 이하에서 시멘틱 프레임(semantic frame, \"의미론적 프레임(틀)\") 이라 지칭된다. 자연어 생성 유닛은 의도 및 채워진 슬롯들로 이루어진 구조화된 데이터, 즉 시멘틱 프레임을 기초로 이에 대응하는 문맥적 의도 및 정보들을 표현하는 자연어, 예를 들어, 문자열을 생성할 수 있다. 어플리케이션은 임의의 방식으로 자연어 입력(예를 들어, 필기 텍스트, 테블릿 입력, 음성 및 타이핑된 텍스 트)을 수신하고, 수신된 자연어 입력을 이용하여 필요한 처리를 수행하는 응용 프로그램 또는 이를 포함하는 디 바이스일 수 있다. 어플리케이션은 소프트웨어, 하드웨어 및 소프트웨어 및 하드웨어 모두의 형태로 구성될 수 있다. 어플리케이션은 자연어 입력을 가공하여 질의(query)를 생성할 수 있다. 질의는 간단한 텍스트 문자열인 소 위 텍스트 토큰으로 이루어질 수 있다. 어플리케이션은 생성된 질의를 자연어 처리 유닛의 자연어 이해 유닛에 제공할 수 있다. 제공된 질의는 자연어 이해 유닛에서 의미 분석될 수 있다. 자연어 이해 유닛 은 앞서 설명한 바와 같이, 텍스트 토큰 형태의 질의를 기초로 시멘틱 프레임을 생성할 수 있다. 또한, 어플리케이션은 시멘틱 프레임, 즉, 의도 및 그에 대응하는 구조화된 텍스트 정보들로 이루어진 질의 를 생성할 수 있다. 어플리케이션은 생성된 질의를 자연어 처리 유닛의 자연어 생성 유닛에 제공할 수 있다. 자연어 생성 유닛은 앞서 설명한 바와 같이, 시멘틱 프레임 형태의 질의를 기초로 이에 대응하는 문맥적 의도 및 정보들을 표현하는 자연어, 예를 들어, 문자열을 생성할 수 있다. 자연어 처리 유닛은 그 성능을 향상시키기 위해 소위 딥러닝 방식으로 학습 또는 훈련될 수 있다. 이를 위해, 자연어 처리 유닛은 심층 신경망(Deep Neural Network, \"DNN\") 구조로 이루어질 수 있다. 또한, 본 발명의 다른 측면에서, 본 발명은 DNN 구조를 갖는 자연어 처리 유닛이 자연어와 그에 상응하는 구 조화된 의미론적 지식, 즉, 텍스트와 시멘틱 프레임 사이의 의미적 일치성(Semantic Correspondence)을 판별하 여 그 일치성을 향상시킬 수 있는 학습 프레임워크를 개시한다. 도 2는 본 발명의 다른 측면에 따른 자연어 처리 시스템의 학습 프레임워크를 나타내는 블록도이다. 도 2를 참조하면, 본 발명의 일 실시예에 따른 학습 프레임워크는 텍스트 리더부, 프레임 생성부, 및 프레임 리더부를 포함한다. 여기서, 텍스트 리더부 및 그에 연관된 프레임 생성부는 자연어 이해 유닛의 구성으로 이해될 수 있다. 또한, 텍스트 리더부, 프레임 리더부 및 프레임 생성부는 DNN 구조로 이루어질 수 있다. 텍스트 리더부는 입력된 원본 텍스트 또는 원본 텍스트 토큰을 기초로 제1 시멘틱 벡터(vt)를 생성한다. 원본 텍스트 토큰은 순차적으로 나열된 문자열일 수 있다. 원본 텍스트 토큰은 예를 들어, 말뭉치(corpus) 내의 단어들에 대응하는 값 또는 저차원 벡터들이 순차적으로 나열된 것으로 이해될 수 있다. 텍스트 리더부는, 의미론적으로 유사한 텍스트들이 공유 벡터 공간 내에서 근거리에 위치되도록, 입력 된 원본 텍스트 토큰을 제1 시멘틱 벡터(vt)로 인코딩한다. 즉, 제1 시멘틱 벡터(vt) 내에서 원본 텍스트 토 큰은 의미론적으로 동일한 텍스트들은 동일한 위치(동일한 벡터 값을 가지며) 의미론적으로 상이한 텍스트 들은 상이한 정도에 따라 먼 거리를 갖도록 분산될 수 있다. 프레임 리더부는 입력된 원본 시멘틱 프레임을 기초로 제2 시멘틱 벡터(vs)를 생성한다. 원본 시멘틱 프레임은 의도 및 그에 대응하는 구조화된 텍스트 정보들로 이루어질 수 있다. 원본 시멘틱 프레임은 원본 텍스트 토큰에 대응하여 미리 구성된 정보들일 수 있다. 예를 들어, 원본 시멘틱 프레임은 사람이 원본 텍스트 토큰에 대응하는 값을 슬롯 채우기 방식으로 미리 기록한 구조화된 데이터일 수 있다. 프레임 리더부는, 의미론적으로 유사한 텍스트들이 공유 벡터 공간 내에서 근거리에 위치되도록, 입력 된 원본 시멘틱 프레임을 제2 시멘틱 벡터(vs)로 인코딩한다. 즉, 제2 시멘틱 벡터(vs) 내에서 원본 시멘틱프레임은 의미론적으로 동일한 텍스트들이 동일한 위치(동일한 벡터 값을 가지며) 의미론적으로 상이한 텍 스트들은 상이한 정도에 따라 먼 거리를 갖도록 분산될 수 있다. 프레임 생성부는 제1 시멘틱 벡터(vt)를 기초로 재구성 시멘틱 프레임을 생성할 수 있다. 재구성 시멘 틱 프레임은 원본 시멘틱 프레임과 동일하거나 축소되도록 구조화된 텍스트 정보들로 이루어질 수 있다. 프레임 생성부는 벡터로 표현된 텍스트들의 시퀀스들로부터 그 문맥적 의미를 추출하여 시멘틱 프레임의 형 태로 변환 또는 디코딩한다. 자연어 처리 유닛의 학습 프레임워크에서, 원본 텍스트 토큰은 자연어 이해 유닛에 의해 재구성 시 멘틱 프레임으로 재구성된다. 원본 시멘틱 프레임에 대한 재구성 시멘틱 프레임의 재구성 손실 또 는 컨텐츠 손실(Lcontent)이 계산될 수 있다. 컨텐츠 손실(Lcontent)은 원본 시멘틱 프레임과 재구성 시멘틱 프 레임의 차이로 이해될 수 있다. 또한, 공유 벡터 공간 내에서 제1 시멘틱 벡터(vt) 및 제2 시멘틱 프레임은 벡터 거리 측정에 의해 양자의 차이가 거리 손실(Ldist)로서 계산될 수 있다. 본 학습 프레임워크에서, 컨텐츠 손실(Lcontent) 및 거리 손실(Ldist)은 손실 함수의 인자가 된다. 텍스트 리더부 , 프레임 리더부 및 프레임 생성부는 역전파(Back Propagation) 방식에 의해 손실 함수의 손실들이 작아지도록 학습될 수 있다. 즉, 텍스트 리더부, 프레임 리더부 및 프레임 생성부는, 제1 시멘틱 벡터(vt) 및 제2 시멘틱 벡터(vs)의 차이에 상응하는 거리 손실(Ldist) 및 원본 프레임에 대한 재구성 시멘틱 프 레임의 재구성 손실에 대응하는 콘텐츠 손실(Lcontent)이 작아지도록 학습된다. 도시된 실시예에서, 프레임 생성부는 제1 시멘틱 벡터(vt)를 기초로 재구성 시멘틱 프레임을 생성하는 것으로 예시되었으나, 다른 실시예에서, 프레임 생성부는 제2 시멘틱 벡터(vs)를 기초로 재구성 시멘틱 프 레임을 생성할 수 있다. 같은 벡터 공간 내에서 분산 표현된 제1 시멘틱 벡터(vt) 및 제2 시멘틱 벡터 (vs)가 서로 일치한다면, 즉, 거리 손실(Ldist)이 없다면, 제1 시멘틱 프레임 및 제2 시멘틱 프레임 각각을 기초 로 생성된 두 재구성 시멘틱 프레임 역시 동일할 것이다. 따라서, 프레임 생성부는 제1 시멘틱 벡터 (vt) 또는 제2 시멘틱 벡터(vs) 둘 중 어느 것을 기초로 재구성 시멘틱 프레임을 생성해도 무방하다. 앞서 도 2에 도시된 실시예를 자연어 처리 유닛의 학습 프레임워크로 서술하였으나, 본 발명은 서로 다른 형 태의 입력 사이의 의미론적 유사성을 양호하게 보존할 수 있는 의미론적 공유 벡터 학습 프레임워크로 이해될 수도 있다. 즉, 텍스트 리더부 및 프레임 리더부는, 서로 다른 형태인 원본 텍스트 토큰(문장) 및 시멘틱 프레 임(슬롯 채움)를 독출하여 이들 사이에 공통으로 내재된 의미를 임베딩 벡터 형태로 적절히 변환할 수 시멘틱 벡터 임베딩 변환 함수로 이해될 수 있다. 또한, 도 2에 도시된 실시예는 텍스트 리더부 및 프레임 리더부 에 대한 학습 프레임워크로 이해될 수 있다. 학습된 텍스트 리더부 및 프레임 리더부는 서로 다른 형태의 입력들에 내재된 의미론적 유사성이 적절 히 반영된 임베딩 벡터들을 각각 생성할 수 있고, 이는 자연어 이해 유닛뿐만 아니라 자연어 처리 유닛 의 다른 다양한 응용들에 활용될 수 있다. 또한, 도시된 자연어 처리 유닛의 학습 프레임 워크는 그 자체로서 자연어 이해 유닛으로 기능할 수도 있다. 즉, 학습 프레임 워크로 구성되어 학습된 텍스트 리더부 및 프레임 생성부는 자연어 처리 시스템 또는 유닛의 자연어 이해 유닛으로 기능할 수 있다. 학습된 자연어 이해 유닛은 원본 텍스트 토큰을 시멘틱 프레임으로 재구성할 수 있다. 구체적으로, 자 연어 이해 유닛은 학습된 텍스트 리더부 및 학습된 프레임 리더부를 포함하고, 학습된 텍스트 리더 부는 입력된 원본 텍스트 토크을 제1 시멘틱 벡터(vt)로 변환하고 학습된 프레임 생성부는 입력된 제1 시멘틱 벡터(vt)를 재구성 시멘틱 프레임으로 변환할 수 있다.도 3은 도 2에 도시된 학습 프레임워크를 이용한 자연어 처리 시스템의 학습 방법을 나타내는 순서도이다. 본 발명의 또 다른 측면에 따른 자연어 이해 유닛의 학습 방법은 다음과 같은 단계들로 구성된다. 도 3에 도시된 실시예에서, 설명의 편의를 위하여 각 단계들은 논리적 또는 시계열적 선후 관계를 같도록 예시되었으나 본 발명은 이해 한정되지 않는다. 전 단계의 출력을 기초로 하는 몇몇 단계들을 제외하고 각각의 단계들은 그 선후 관계가 바뀌어도 무방하다. 도 2 및 도 3을 함께 참조하면, 우선, 텍스트 리더부가 원본 텍스트 토큰을 제1 시멘틱 벡터(vt)로 인 코딩하는 단계가 수행된다(S31). 이와 독립적으로 프레임 리더부가 원본 시멘틱 프레임을 제2 시멘틱 벡터(vs)로 인코딩하는 단계가 수행된다(S32). 원본 텍스트 토큰 및 원본 시멘틱 프레임은 서로 그 내용이 상응하는 많은 수의 미리 준비된 데이터 세 트로 구성될 수 있다. 이어, 프레임 생성부가 제1 시멘틱 프레임을 기초로 재구성 시멘틱 프레임을 생성하는 단계가 수행된다 (S33). 다만, 앞서 설명한 바와 같이, 자연어 처리 유닛에 대한 학습 프레임워크에서, 프레임 생성부는 제2 시멘틱 프레임을 기초로 재구성 시멘틱 프레임을 생성할 수도 있다. 이어, 제1 시멘틱 벡터와 제2 시멘틱 벡ㅌ터의 차이 및 원본 시멘틱 프레임과 재구성 시멘틱 프레임 사이의 차 이 중 하나 이상이 감소되도록 학습되는 학습단계(S34, S35, S36)가 수행된다. 이 학습 단계(S34, S35, S36)에서, 제1 시멘틱 벡터(vt) 및 제2 시멘틱 벡터(vs) 사이의 거리 손실을 계산하는 단계가 수행된다(S34). 이와 독립적으로 원본 시멘틱 프레임과 재구성 시멘틱 프레임 사이의 컨텐츠 손 실을 계산하는 단계가 수행된다(S35). 이어, 거리 손실 및 컨텐츠 손실이 모두 감소하도록 텍스트 리더부, 프레임 리더부 및 프레임 생성부가 학습되는 단계가 수행된다(S36). 이 때, 텍스트 리더부, 프레임 리더부 및 프레임 생성부는 역전파 방식에 의해 거리 손실 및 컨텐츠 손실을 인자로 갖는 손실 함수가 감속되도록 학습될 수 있다. 이와 같은 학습은 미리 준비된 데이터 세트 모두에 대해 다수회 반복적으로 이루어질 수 있다. 전술한 본 발명의 또 다른 측면에 따른 자연어 이해 유닛의 학습방법은 프로그램으로 구현되어 컴퓨터 판독 가 능한 기록매체에 기록될 수 있다. 본 기록매체는 컴퓨터 시스템에 의하여 판독될 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 이러한 컴퓨터가 읽을 수 있는 기록매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 데이터 저장장치 등이 있으며, 또한 캐리어 웨이브(예를 들어, 인터넷을 통한 전송) 의 형태로 구현되는 것도 포함한다. 또한 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템 에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수도 있다. 또한, 본 발명의 일 실 시예를 구현하기 위한 기능적인(Functional) 프로그램, 코드 및 코드 세그먼트들은 본 발명의 일 실시예가 속하"}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "는 기술분야의 프로그래머들에 의해 용이하게 추론될 수 있을 것이다. 도 4는 도 2에 도시된 본 발명에 따른 시멘틱 벡터 학습 프레임워크 구조를 전체적으로(generally) 나타내는 개 략도(schematic diagram)이다. 도 5는 자연어 처리 유닛에 대한 학습 프레임워크에 사용되는 원본 텍스트 및 원본 시멘틱의 일 예를 나타내 는 테이블이다. 도 6은 시멘틱 벡터 공간 내의 텍스트들의 거리 측정에 대한 응용예들을 나타내는 개략도이다. 도 4 내지 도 6을 참조하면, 본 발명에 따른 학습 프레임워크는 아래와 같은 2 개의 속성을 만족할 것을 가정하 여 안출되었다. 제1 속성 - 임베딩 일치성: 자연어 또는 텍스트 토큰의 텍스트들이 의미론적으로 분산된 벡터 표현인 시멘틱 벡 터는 시멘틱 프레임의 텍스트들의 시멘틱 벡터와 동일할 것이다. 즉, 자연어 또는 텍스트 토큰 및 그에 대응하는 시멘틱 벡터들이 포함하는 텍스트 시퀀스들은 서로 동일한 문맥 적 의미를 가질 것이기 때문에, 이 둘이 갖는 공통의 특질들은 동일한 시멘틱 벡터로 임베딩될 수 있다. 원본 텍스트의 도메인(XT) 및 시멘틱 프레임의 도메인(Xs)에서 동일한 의미를 가지나 상이한 구조로 표현되는 한 쌍의 텍스트 및 시멘틱 프레임 대응쌍(t, s)이 있다고 할 때, 이 대응쌍(t, s)은 각각 텍스트 리더부의 변환 함수(Rt,) 및 프레임 리더부의 변환 함수(Rs)를 통해 같은 임베딩 시멘틱 벡터 공간(Z)으로 인코딩될 수 있다. 도 5의 테이블의 왼쪽에 일 원본 텍스트를 예시하였고 그 오른쪽에 대응하는 원본 시멘틱 프레임을 예시하 였다. 양자를 대비하면, 두 텍스트들은 표현 방식 또는 구조가 상이하나 텍스트의 전체적인 문맥적 의미는 같은 것으 로 이해될 수 있다. 따라서, 양자를 같은 벡터 공간 내에 분산하되, 그 벡터 공간 내에서 서로 대응하는 텍스트들이 동일하게 표현 되도록, 즉, 대응쌍(t, s)의 거리가 줄어들도록 학습될 수 있고, 이로써, 서로 다른 도메인의 두 텍스트들의 공 통의 특질인 의미적 정보를 추출하는 것이 촉진될 수 있다. 제2 속성 - 재구성: 재구성 시멘틱 프레임이 학습된 시멘틱 벡터로부터 재구성될 수 있어야 한다. 만일, 시멘틱 벡터가 제1 속성인 임베딩 일치성만을 만족하도록 학습한다면, 학습이 누적될수록, 텍스트 리더부 및 프레임 리더부는 각각 시멘틱 벡터가 점점 단순화되는 \"?戮막* 학습될 것이다. 즉, 시멘틱 벡터는 원본 텍스트 및 원본 시멘틱 프레임의 의미적 정보들을 삭제하거나 단순화하는 방식으로 그 대응쌍의 거리 가 작아지도록 학습될 것이다. 극단적으로 텍스트 리더부 및 프레임 리더부는 시멘틱 벡터의 모든 값이 0이 되도록 학습될 수도 있다. 그러나, 시멘틱 벡터는 본질적으로 자연어 처리 유닛에 의해 시멘틱 프레임으로(또는 텍스트로) 재구성 또는 복원될 수 있어야 한다. 즉, 학습 프레임워크에서, 임베딩된 시멘틱 벡터는 원본 텍스트 및 원본 시멘틱 프레임들의 의미론적 정보 들을 가능한 손실 없이 보유할 수 있어야 하고 보유된 정보들은 프레임 생성부의 변환 함수(W)를 통해 디코 딩 될 수 있어야 한다. 따라서, 본 발명에 따른 학습 프레임워크에서, 제1 속성을 만족하도록 학습하는 것 및 제2 속성을 만족하도록 학습하는 것은 서로 상보적(complementarily) 및 대립적으로(adversarial) 작용하여 원본 텍스트 및 원본 시멘 틱 프레임의 의미적 정보들(공통의 특질들)을 잘 추출하면서도 시멘틱 벡터 내 의미적 정보들의 손실을 최 소화한다. 본 의미론적 공유 벡터 학습 프레임워크에 의해, 상기 2 가지 속성을 만족하는 시멘틱 벡터를 생성할 수 있는 2 개의 변환 함수들이 학습될 수 있다. 이 2 개의 변환 함수는 도 2에 도시된 텍스트 리더부 및 프레임 리더 부로 이해될 수 있다. 우선, 도 6의 (a)에 도시된 바와 같이, 동일한 형태(텍스트 토큰)의 두 단어에 대한 시멘틱 벡터들(vt, vt') 사 이의 거리 측정을 통해 두 단어 간의 의미적 유사성을 판단할 수 있다. 또한, 도 6의 (b)에 도시된 바와 같이, 서로 다른 형태의 두 단어에 대한 시멘틱 벡터들(vt, vs)사이의 거리 측 정을 통해 두 단어(텍스트 대 슬롯 값)의 의미적 유사성을 판단할 수 있다. 또한, 도 6의 (c)에 도시된 바와 같이, 같은 형태(시멘틱 프레임)의 두 단어(두 슬롯 값)에 대한 시멘틱 벡터들 (vs, vs') 사이의 단어 간의 의미적 유사성을 판단할 수 있다. 또한, 도 6의 (d)에 도시된 바와 같이, 서로 다른 형태의 여러 단어에 대한 임베딩 벡터들(vt, vs1, vs2, vs3) 거 리 측정을 통해 각 단어의 의미적 유사성을 판단할 수 있다. 또한, 도 6의 (e)에 도시된 바와 같이, 임베딩 벡터 내 단어들의 의미적 유사성을 기초로 유사 단어들을 클러스 터링할 수 있다. 클러스터링된 유사 의미군의 단어들은 시각적으로 표현될 수 있다. 도 7은 도 2에 도시된 학습 프레임워크의 더 상세한 구현예이다. 도 7을 참조하면, 본 발명에 따른 학습 프레임워크는 텍스트 리더부, 프레임 리더부 및 프레임 생성부 의 신경망 아키텍쳐(neural architecture)로 구현될 수 있다. 여기서, EX는 입력 텍스트 토큰 x에 대한 임베딩 함수이다. EI, ES 및 EV는 각각 의도 태그, 슬롯 태그 및 슬롯 값에 대한 임베딩 함수이다. 벡터 연결(concatenation) 연산자이고; 크로스 엔트로피 연산자이고; 는평균 계산 연산자이고; 는 거리 계산 연산자이다. 는 참조 의도 태그 벡터이고 은 시간 m에서 참조 슬롯 태그 벡터이다. M은 문장의 슬롯 수이다(도 7의 예에서 M = 3임). 텍스트 리더부는 신경 문장 인코더(neural sentence encoder)로 구현될 수 있다. 이는 입력 토큰들의 시퀀 스를 판독하고(read) 각각을 벡터로 인코딩한다. 본 구현예에서, 텍스트 리더부는 입력 시퀀스를 인코딩하 기 위해 LSTM (Long Short-Term Memory) (Hochreiter and Schmidhuber, 1997)을 사용하여 구현되었다. 인코딩 과정은 다음과 같이 정의할 수 있다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, 여기서 s = {1, 2, ..., S} 및 → 는 시간 s에서의 입력 시퀀스에 대한 포워드 히든 상태(forward hidden state)이다. 는 RNN 셀(cell)이다. 는 시간 s에서 토큰 x의 분포 벡터 표현을 반환하는 토큰 임베딩 함수이다. 최종 RNN 출력 는 가 되며 이는 텍스트로부터 유래된 시멘틱 벡터이다. 도 5를 함께 참조하면, 시멘틱 프레임은 의도, 슬롯 태그 및 슬롯 값과 같은 구조화된 태그로 구성된다. 본 구 현예에서, 의도 태그는 심볼로 처리되고 슬롯 태그와 슬롯 값은 심볼들의 시퀀스로 처리된다. 예를 들어, \"달라스에서 필라델피아까지 모든 항공편을 월요일에 나열하십시오.\"라는 문장은 다음과 같이 처리 된다. - 의도 태그: atis_flight - 슬롯 태그 시퀀스: [fromloc.city_name, toloc.city_name, depart.date.day_name] - 슬롯 값 시퀀스: [Dallas, Philadelphia, Monday] 프레임 리더부는 의도 리더부 및 슬롯 리더부를 포함할 수 있다. 의도 리더부는 간단한 임베딩 함수인 의 형태로 구현될 수 있고, 이는 시멘틱 프레임의 의도 태그 i를 분산 표현된 벡터로 반환하도록 구현될 수 있다. 슬롯 리더부는 적층 LSTM 레이어(Stacked LSTM layer)를 사용하여 구현될 수 있다. 슬롯 리더부는 슬롯 태그들 및 슬롯 값들의 시퀀스를 판독할(read)하여 분산 표현된 벡터로 반환하도록 구성될 수 있다. 는 o를 토큰 으로 하는 슬롯 태그 임베딩 함수이다. 는 a를 토큰으로 포함하는 임베딩 함수이다. 임베딩 결과 및 는 시간 단계 m에서 연결되며(concatenated), 병합된 벡터는 각 타임 스텝(timestep)에 대해 스택된 레이어로 공급된다. 슬롯 태그 및 값들의 시퀀스의 판독 결과인 는 시간 M에서 RNN의 최종 출력으로부터 얻을 수 있다. 마지막으로, 의도, 슬롯 태그 및 슬롯 값이 인코딩된 벡터들은 병합되어 분산 된 시멘틱 프레임 표현으로 구성되며, 이는 다음과 같이 표현된다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서 [;]는 벡터 연결 연산자(vector concatenation operator)를 나타낸다. vs의 차원은 vt와 동일하다. 모든 임베딩 가중치(embedding weights)는 랜덤으로 초기화되고 학습 프로세스를 통해 학습된다. 프레임 생성부는 시멘틱 벡터로 임베딩된 텍스트들의 시퀀스들로부터 그 문맥적 의미를 추출하여 시멘틱 프 레임의 형태로 변환 또는 디코딩한다. 본 발명에 따른 학습 프레임워크의 목적 중 하나는 텍스트 및 관련 시멘틱 프레임의 합리적인 벡터 표현을 의미 론적으로 학습하는 것이다. 따라서, 학습 프레임워크는 바람직한 의미론적 벡터의 특성을 설정하고, 손실 함수 는 그 특성을 만족시키도록 정의되어야 한다.이러한 관점에서, \"임베딩 일치성(embedding correspondence)\"에 대한 손실 특성, 즉, 거리 손실은 시멘틱 벡터 공간에서 텍스트 리더부에서 인코딩된 시멘틱 벡터와 프레임 리더부에서 인코딩된 시멘틱 벡터 사이의 차이를 측정한다. 이 손실은 다음과 같이 정의된다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서 dist 함수는 임의의 벡터 거리 측정 방식을 사용할 수 있다. 다만, 본 구현예에서, dist 함수는 유클리 드 거리 및 코사인 거리 (= 1.0 코사인 유사도)를 사용하여 구현되었다. 또한, \"재구성\"에 대한 손실 특성, 즉, 컨텐츠 손실은 시멘틱 프레임 벡터에 포함된 의미 정보의 양을 측정한다. 콘텐츠가 손실되지 않으면 vt와 vs는 빠르게 0 벡터로 수렴될 것이다. 이 경우, 학습 프레임워크는 충 분한 의미론적 표현을 학습하지 못한다. 이에, 본 구현예에서, 프레임 생성부는, 컨텐츠가 유지되는 정도를 측정하기 위하여, 시멘틱 벡터로부터 상 징적 시멘틱 프레임을 생성하여 원본 시멘틱 프레임과 생성된 시멘틱 프레임 사이의 차이를 계산할 수 있도 록 구현된다. 특히, 본 구현예에서, 프레임 생성부는 의도 생성부 및 슬롯 태그 생성부만을 포함하여 축소된 시멘틱 프레 임을 재구성하는 형태로 구현될 수 있다. 시멘틱 프레임의 슬롯 값들은 어휘들의 크기가 매우 크기 때문에 슬롯 값들을 생성 또는 재구성하는 데 큰 학습 부하(load)를 유발할 수 있다. 이에, 본 구현예에서, 이와 같은 어려움을 완화하기 위하여, 프레임 생성부 는 축소된 시멘틱 프레임을 생성하도록 구현되었다. 축소된 시멘틱 프레임은 대응하는 시멘틱 프레임으로부터 단지 슬롯 값을 단순히 드롭함으로써 생성될 수 있다. 예를 들어, 축소된 시멘틱 프레임을 생성하기 위해 슬롯 값 [Dallas, Philadelphia, Monday]을 제거하고, 이 축 소된 시멘틱 프레임에 대해 콘텐츠 손실 계산이 수행된다. 축소된 시멘틱 프레임을 사용하는 또 다른 이점은 학습된 시멘틱 벡터가 어휘에 덜 민감하기 때문에 학습된 분 산된 시멘틱 벡터가 보다 큰 의미 추상화 능력(abstract power)을 갖는다는 것이다. 콘텐츠 손실에 대해서, 재구성 의도 태그 및 재구성 슬롯 태그를 생성하는 것에 대한 품질이 측정된다. 의도 생 성부는 선형 투영법을 사용하여 다음과 같이 간단하게 구현될 수 있다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서 v는 시멘틱 벡터이고 yintent는 출력 벡터이다. 슬롯 태그를 생성하는 네트워크는 다음과 같이 정의될 수 있다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "여기서 RG는 RNN 셀이다. 시멘틱 벡터 v가 복사되어 각 RNN 입력으로 반복적으로 공급된다. RNN의 출력은 W'S로 슬롯 태그 공간에 투영된다. 도 7은 의도 및 슬롯 태그를 생성하는 네트워크와 그 대응하는 손실 계산 방법을 도시한다. 생성 손실은 다음과 같이 생성된 태그 벡터 및 참조 태그 벡터 사이의 크로스 엔트로피로 정의될 수 있다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서 M은 문장 내에 있는 슬롯의 개수이다. 의도 및 슬롯 태그의 손실을 조합하여 시멘틱 벡터 v로부터 시멘틱 프레임을 재구성할 때의 손실인 콘텐츠 손실 (Lcontent)이 다음과 같이 정의될 수 있다."}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "마지막으로, 시멘틱 프레임 표현을 학습하기 위한 전체 손실 값(L)은 거리 손실 및 콘텐츠 손실로서 다음과 같 이 정의될 수 있다:"}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "제안된 모델의 하이퍼 파라미터는 아래의 표 1에 요약되어 있다. <표 1>"}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "응용예들 멀티-도메인 간 거리 측정 학습된 텍스트 리더부 및 프레임 리더부를 사용하여, 동일한 도메인(텍스트 또는 시멘틱 프레임)의 인 스턴스들뿐만 아니라 다른 도메인의 인스턴스들도 그 의미적 거리가 측정될 수 있다. 텍스트를 t, 시멘틱 프레임을 s라 하고, 텍스트 리더부 및 프레임 리더부는 각각 RT 및 RS라 할 때, 상 이한 도메인들(응용 분야들)에서 의미적 거리 측정은 다음과 같이 수행될 수 있다:"}
{"patent_id": "10-2019-0029736", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "예를 들어, 비행 스케쥴 예약과 관련된 하나의 도메인(응용예)에서 텍스트 및 그에 대응하는 시멘틱 프레임들에 대한 학습을 하고, 기차 스케쥴 예약과 관련된 다른 하나의 도메인(또는 전혀 무관한 다른 응용예)에서 텍스트 및 그에 대응하는 시멘틱 프레임들에 대한 학습이 수행될 수 있다.여기서, 다른 두 도메인들의 시멘틱 프레임들이 규정된 형식을 사용한다면 서로 다른 도메인들의 텍스트의 의미 적 거리 역시 측정될 수 있다. 또한, 도메인과 무관하게, 서로 다른 양식(form), 즉, 텍스트 토큰이 임베딩된 시멘틱 벡터와 시멘틱 프레임이 임베딩된 시멘틱 벡터 사이의 의미적 거리 역시 측정될 수 있다. 이어, 도 8 및 도 9를 참조하여, 본 발명에 따른 자연어처리 유닛에서, 다른 양식을 갖는 인스턴스들, 텍스트 문장과 시멘틱 프레임을 각각 입력 쿼리로 했을 때 적절한 응답 결과가 생성되는 것을 설명한다. 도 8은 텍스트 시퀀스(문장)이 자연어 처리 시스템의 쿼리로 입력되었을 때 텍스트 및 시멘틱 프레임 형식으로 각각 생성된 응답 결과를 나타낸다. 도 8에서 텍스트 쿼리와 가장 유사한 3개의 텍스트 응답 결과가 좌측에 도시되며 3 개의 시멘틱 프레임 응답 결 과가 우측에 도시된다. 도시된 바와 같이, 텍스트 쿼리에 대해 텍스트 응답을 반환하는 것에 대해 자연어 처리 시스템이 적절히 수행되 는 것을 알 수 있다. 응답 결과의 문장 패턴은 텍스트 쿼리와 유사하며 사용된 어휘 역시 유사함을 알 수 있다. 반면에, 텍스트 쿼리에 대해 시멘틱 프레임 응답을 반환하는 것은 문장 패턴(의도 및 슬롯 태그)는 적절하게 이 루어졌으나, 슬롯 값은 다소 차이가 있음을 알 수 있다. 이는 본 발명에 따른 학습 프레임워크를 도 7과 같이 구현함으로써, 슬롯 값이 재구성 특성에 대한 콘텐츠 손실 에서 배제되었기 때문이다. 큰 데이터를 가진 재구성 손실에 슬롯 값 생성을 포함시킬 수 있다면 이러한 문제가 어렵지 않게 극복될 수 있을 것이다. 도 9는 시멘틱 프레임이 자연어 처리 시스템의 쿼리로 입력되었을 때 텍스트 및 시멘틱 프레임 형식으로 각각 생성된 응답 결과를 나타낸다. 도 9에서 시멘틱 프레임 쿼리와 가장 유사한 3개의 텍스트 응답 결과가 좌측에 도시되며 3 개의 시멘틱 프레임 응답 결과가 우측에 도시된다. 도시된 바와 같이, 시멘틱 프레임 쿼리에 대해 텍스트 응답 및 시멘틱 프레임 응답을 반환하는 것에 대해 자연 어 처리 시스템은 쿼리의 의도 및 문맥에 적절한 응답을 출력하는 것을 알 수 있다. 다만, 응답 결과 중 슬롯 값, 특히, 모드(flight_mod) 및 날짜(depart_date.day_name)에 상응하는 값에서 다소 차이가 있다. 시각화 벡터 시멘틱 표현을 통해, 인스턴스들(문장들)은 보다 쉽고 자연스러운 방식으로 시각화될 수 있다. 심볼형 (symbolic) 텍스트 또는 시멘틱 프레임이 벡터로 변환, 즉, 벡터 임베딩되면 t-sne (Maaten and Hinton, 200 8)과 같은 벡터 시각화 방법을 이용하여 인스턴스 간의 관계 또는 전체 말뭉치의 분포를 직관적으로 확인할 수 있다. 도 10은 본 발명의 일 실시예에 따른 학습 프레임워크를 이용해 학습된 시멘틱 벡터의 시각화를 도시한다. 도시된 지점들은 t-sne 처리가 수행될 때 텍스트 리더부가 텍스트 문장들을 변환한 시멘틱벡터(vt)들을 나 타낸다. 상이한 색들 및 형상들을 조합하여 상이한 의도 태그들이 표현되었다. 도 10을 참조하면, 의미론적 벡터 학습이 적절히 수행되면 의미론적으로 유사한 인스턴스들이 그룹화 또는 클러 스터링 되는 것을 확인할 수 있다. 시멘틱 벡터들을 나타내는 지점들은 학습이 진행됨에 따라 의미론적으로 유 사한 인스턴스들이 서로 더 가깝게 분포되는 것을 확인할 수 있다. 또한, 본 발명에 따른 학습 프레임워크는 의 도 태그를 기반으로 문장들을 적절히 그룹화한다는 것을 알 수 있다. 리랭킹 또는 평가(Re-ranking or evaluation) 도 11은 본 발명의 일 실시예에 따른 자연어 처리 시스템의 평가부를 표현하는 블록도이다. 도 11을 참조하면, 본 발명의 일 실시예에 따른 자연어 처리 시스템의 자연어 처리 유닛은 그 범용성 및 신 뢰성을 향상시키기 위해 서로 다른 특성을 갖는 복수의 자연어 이해 유닛을 포함할 수 있다. 다만 본 발명은 이에 한정되지 않으면, 자연어 이해 유닛은 자연어 처리 유닛과 구분되는 별개의 구성으로서 존재할 수도 있으며, 이 경우, 자연어 처리 유닛은은 외부의 자연어 이해 유닛으로부터 자연어를 이해한 정보, 예를 들어, 시멘틱 프레임을 전달 받을 수 있다.또한, 본 발명의 일 실시예에 따른 자연어 처리 시스템의 자연 어 처리 유닛은 한 쌍 이상의 시멘틱 벡터의 거리를 기초로 하나 이상의 자연어 이해 유닛들의 성능을 평가 또는 계산하고 계산된 성능을 기초로 복수의 자연어 이해 유닛들 중 하나 이상을 선택할 수 있는 평가 부(또는 리랭킹부)를 포함할 수 있다. 구체적으로, 평가부 또는 리랭킹부는 입력된 텍스트 토큰을 기초로 제1 시멘틱 벡터(vt)를 생성하는 텍스트 리더 부 및 하나 이상의 자연어 이해 유닛에서 생성된 시멘틱 프레임을 기초로 하나 이상의 제2 시멘틱 벡터 (vs)를 생성하는 하나 이상의 프레임 리더부를 포함할 수 있고, 생성된 제1 시멘틱 벡터 및 복수의 제2 시 멘틱 벡터를 기초로 상기 하나 이상의 자연어 이해 유닛들의 성능 또는 우선 순위를 계산할 수 있다. 도시된 실시예에서, 각각의 자연어 이해 유닛에 대응하여 프레임 리더부들이 복수 개가 배치된 것으로 예시되었으나, 본 발명은 이에 한정되지 않는다. 프레임 리더부는 자연어 이해 유닛 보다 적은 수가 사 용될 수 있고, 또는 하나의 프레임 리더부만 사용될 수도 있다. 또한, 텍스트 리더부 및 프레임 리더부는 앞서 설명한 바와 같은 시멘틱 벡터 학습 프레임워크에 의해 학습될 수 있다. 평가부(또는 리랭킹부)는 제1 시멘틱 벡터(vt)에 대한 복수의 제2 시멘틱 벡터(vs)들 사이의 거리를 계산할 수 있다. 또한, 평가부(또는 리랭킹부)는 계산된 거리를 기초로 각각의 계산된 거리들에 대응하는 자연어 이해 유 닛의 우선 순위를 결정할 수 있다. 예를 들어, 평가부(또는 리랭킹부)는 계산된 거리가 가장 짧은 시멘틱 벡터에 대응하는 시멘틱 프레임을 생성하 는 자연어 이해 유닛을 최우선 순위로 결정할 수 있다. 즉, 본 발명에 따른 학습 프레임워크를 이용해 학습된 텍스트 리더부 및 프레임 리더부는 서로 대응하 는 인스턴스가 동일하거나 같은 거리를 갖는 시멘틱 벡터로 볼 수 있기 때문에, 서로 다른 복수의 자연어 처리 유닛이 동일한 텍스트 토큰을 기초로 생성한 시멘틱 프레임들 중 그 텍스트 토큰의 의도 및 문맥적 의미를 가장 잘 보조하는 것이 텍스트 리더부에 의해 변환된 제1 시멘틱 벡터(vt)와 가장 가까운 거리를 갖는 제2 시멘틱 벡터(vs)로 프레임 리더부에 의해 변환될 것이다. 이로써, 앙상블 학습 등의 복잡한 평가 알고리즘을 사용 및 추가 학습할 필요 없이도 간단하고 빠르게 다수의 자연어 이해 유닛들의 순위를 판별할 수 있다. 또한, 이와 같은 평가 방식은 어떠한 형태의 자연어 이해 유 닛에도 범용적으로 사용될 수 있음은 물론이고 완전 규칙 기반 시스템(purely ruled-based system)과 완전 통계 시스템(purely statistical system)을 비교하는 것에도 적절히 사용될 수 있다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11"}
{"patent_id": "10-2019-0029736", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 측면에 따른 자연어 처리 시스템을 나타내는 블록도이다. 도 2는 본 발명의 다른 측면에 따른 자연어 처리 시스템을 학습하기 위해 사용가능한 학습 프레임워크를 나타내 는 블록도이다. 도 3은 도 2에 도시된 학습 프레임워크를 이용한 자연어 처리 시스템의 학습 방법을 나타내는 순서도이다. 도 4는 도 2에 도시된 본 발명에 따른 시멘틱 벡터 학습 프레임워크 구조를 전체적으로(generally) 나타내는 개 략도(schematic diagram)이다. 도 5는 자연어 처리 유닛에 대한 학습 프레임워크에 사용되는 원본 텍스트 및 원본 시멘틱 프레임의 일 예를 나 타내는 테이블이다. 도 6은 시멘틱 벡터 공간 내의 텍스트들의 거리 측정에 대한 응용예들을 나타내는 개략도이다. 도 7은 도 2에 도시된 학습 프레임워크의 더 상세한 구현예이다. 도 8은 텍스트 시퀀스(문장)이 자연어 처리 시스템의 쿼리로 입력되었을 때 텍스트 및 시멘틱 프레임 형식으로 각각 생성된 응답 결과를 나타낸다. 도 9는 시멘틱 프레임이 자연어 처리 시스템의 쿼리로 입력되었을 때 텍스트 및 시멘틱 프레임 형식으로 각각 생성된 응답 결과를 나타낸다. 도 10은 본 발명의 일 실시예에 따른 학습 프레임워크를 이용해 학습된 시멘틱 벡터의 시각화를 도시한다. 도 11은 본 발명의 일 실시예에 따른 자연어 처리 시스템의 평가부를 표현하는 블록도이다."}
