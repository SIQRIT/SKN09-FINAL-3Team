{"patent_id": "10-2019-0058048", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0132468", "출원번호": "10-2019-0058048", "발명의 명칭": "첨단 운전자 지원 장치 및 이의 객체를 검출하는 방법", "출원인": "삼성전자주식회사", "발명자": "고상수"}}
{"patent_id": "10-2019-0058048", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "프로세싱 회로; 및상기 프로세싱 회로에 의하여 실행가능한 명령어들을 저장하는 메모리를 포함하고,상기 프로세싱 회로는 상기 명령어들을 실행함으로써,차량의 주행 중 촬영된 복수의 프레임들(상기 복수의 프레임들 각각은 제1 시점 이미지 및 제2 시점 이미지를포함하는 스테레오 이미지에 해당함)을 포함하는 비디오 시퀀스를 획득하고, 상기 제1 시점 이미지와 상기 제2 시점 이미지에 기초하여 상기 스테레오 이미지의 디스패리티 정보 산출하고, 상기 차량의 주행 중 수신된 반사파들에 기초하여 상기 스테레오 이미지에 포함된 적어도 하나의 객체에 대한깊이 정보를 획득하고, 상기 스테레오 이미지, 상기 깊이 정보 및 상기 디스패리티 정보에 기초하여 상기 깊이 정보와 상기 디스패리티정보 사이의 상관 관계 정보를 산출하고, 상기 디스패리티 정보 및 상기 상관 관계 정보에 기초하여 상기 스테레오 이미지의 깊이 값을 교정하여 상기 스테레오 이미지에 대한 깊이 이미지를 생성하는 첨단 운전자 지원 장치."}
{"patent_id": "10-2019-0058048", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 프로세싱 회로는 상기 제1 시점 이미지와 상기 제2 시점 이미지 각각에서 대응하는 픽셀들 사이의 픽셀 값의 차이에 기초하여 상기 디스패리티 정보를 산출하고,상기 제1 시점 이미지와 상기 제2 시점 이미지 각각에서 상기 대응하는 픽셀들에 대하여 스테레오 매칭을 수행하여 상기 디스패리티 정보를 산출하는 첨단 운전자 지원 장치."}
{"patent_id": "10-2019-0058048", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 프로세싱 회로는상기 반사파들에 기초하여 상기 적어도 하나의 객체까지에 대한 거리 정보를 포함하는 객체 트래킹 리스트를 생성하고, 상기 제1 시점 이미지와 상기 제2 시점 이미지 중 적어도 하나에서 상기 적어도 하나의 객체를 분리하여 적어도하나의 마스크를 추출하고, 상기 적어도 하나의 마스크 및 상기 객체 트래킹 리스트에 대한 매칭을 수행하고,상기 매칭의 결과와 상기 디스패리티 정보에 기초하여 상기 상관 관계 정보를 산출하고,상기 적어도 하나의 마스크를 구분하고,상기 적어도 하나의 마스크 및 상기 객체 트래킹 리스트에 상기 매칭을 수행하여 상기 적어도 하나의 객체까지의 거리에 기초하여 상기 적어도 하나의 마스크를 구분하는 첨단 운전자 지원 장치."}
{"patent_id": "10-2019-0058048", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 프로세싱 회로는 상기 깊이 이미지에 대하여 픽셀 레벨의 깊이를 표현하고,상기 깊이 이미지에서 상기 적어도 하나의 객체를 검출하고,공개특허 10-2020-0132468-3-학습 차량의 주행 중에 촬영된 복수의 학습 프레임들을 포함하는 비디오 시퀀스에서 학습 객체를 검출하는 결과에 기초하여 제1 학습 모델을 획득하고,상기 획득한 제1 학습 모델을 이용하여 상기 스테레오 이미지에 포함된 상기 적어도 하나의 객체를 인식하는 첨단 운전자 지원 장치."}
{"patent_id": "10-2019-0058048", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 프로세싱 회로는,상기 복수의 프레임들 내에서의 객체를 나타내는 바운딩 박스를 시계열적으로 트래킹하여 상기 객체의 이동 속도 및 이동 방향을 결정하고, 상기 객체의 이동 속도 및 이동 방향에 기초하여, 상기 차량의 주행에 관련된 이벤트의 종류와 주행의 위험도를 결정하고,상기 이벤트의 종류 및 상기 주행의 주행의 위험도에 기초하여 상기 이벤트를 알리는 알림 메시지를 출력하는첨단 운전자 지원 장치."}
{"patent_id": "10-2019-0058048", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 프로세싱 회로는,인공 신경망을 이용하여, 복수의 학습 프레임들 내에서의 학습 객체를 나타내는 바운딩 박스를 시계열적으로 트래킹하여 학습 차량의 주행에 관련된 이벤트가 발생하였는지를 학습한 결과에 기초하여 제 2 학습 모델을 획득하고,상기 획득한 제 2 학습 모델을 이용하여, 상기 객체와 관련된 상기 차량의 주행에 관련된 이벤트 발생 여부를결정하는 첨단 운전자 지원 장치."}
{"patent_id": "10-2019-0058048", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 프로세싱 회로는 상기 스테레오 이미지를 전처리하여 전처리된 제1 시점 이미지와 전처리된 제2 시점 이미지를 포함하는 전처리된 스테레오 이미지를 출력하는 이미지 프리프로세서;상기 전처리된 제1 시점 이미지 및 상기 전처리된 제2 시점 이미지에 기초하여 상기 디스패리티 정보를 포함하는 디스패리티 이미지를 생성하는 디스패리티 추정 엔진;상기 반사파들에 기초하여 상기 적어도 하나의 객체에 대한 깊이 정보를 포함하는 객체 트래킹 리스트 데이터를제공하는 객체 트래킹 엔진;상기 스테레오 이미지, 상기 객체 트래킹 리스트 데이터 및 상기 디스패리티 정보에 기초하여 상기 상관 관계를산출하는 상관 산출 모듈;상기 상관 관계에 기초하여 상기 디스패리티 이미지의 깊이 값을 교정하여 상기 깊이 이미지를 생성하는 깊이이미지 생성 엔진; 및상기 깊이 이미지 및 상기 전처리된 스테레오 이미지에 기초하여 상기 전처리된 스테레오 이미지에서 상기 적어도 하나의 객체를 검출하고 상기 검출된 객체를 포함하는 최종 이미지를 출력하거나 상기 검출된 객체를 나타내는 바운딩 박스 출력하는 객체 검출 엔진을 포함하는 첨단 운전자 지원 장치."}
{"patent_id": "10-2019-0058048", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 상관 산출 모듈은 상기 전처리된 제1 시점 이미지 및 상기 전처리된 제2 시점 이미지 중 적어도 하나에서 상기 적어도 하나의 객체를 분리하여 상기 적어도 하나의 마스크를 추출하는 장면 분리 엔진;상기 적어도 하나의 마스크 및 상기 객체 트래킹 리스트 데이터에 대한 매칭을 수행하여 매칭 결과를 출력하는매칭 엔진; 및상기 매칭 결과와 상기 디스패리티 정보에 기초하여 상기 상관 관계를 산출하는 상관 정보 산출 엔진을 포함하공개특허 10-2020-0132468-4-고,상기 객체 검출 엔진은상기 전처리된 제1 시점 이미지 및 상기 전처리된 제2 시점 이미지 중 적어도 하나의 특징을 추출하여 제1 특징벡터를 제공하는 제1 특징 추출기;상기 깊이 이미지의 특징을 추출하여 제2 특징 벡터를 제공하는 제2 특징 추출기;상기 제1 특징 벡터와 상기 제2 특징 벡터를 융합하여 융합된 특징 벡터를 제공하는 센서 융합 엔진; 및상기 융합된 특징 벡터에서 상기 검출된 객체를 포함하는 상기 최종 이미지를 출력하거나 상기 바운딩 박스로출력하는 박스 예측기를 포함하고,상기 박스 예측기는 3차원의 상기 최종 이미지를 2차원의 조감도로 변환하고, 상기 2차원의 조감도에서 상기 객체를 상기 바운딩 박스로 표시하여 출력하는 첨단 운전자 지원 장치."}
{"patent_id": "10-2019-0058048", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "프로세싱 회로; 및상기 프로세싱 회로에 의하여 실행가능한 명령어들을 포함하는 메모리를 포함하고,상기 프로세싱 회로는 상기 명령어들을 실행함으로써,차량의 주행 중 촬영된 복수의 프레임들(상기 복수의 프레임들 각각은 제1 시점 이미지 및 제2 시점 이미지를포함하는 스테레오 이미지에 해당함)을 포함하는 비디오 시퀀스를 획득하고, 상기 제1 시점 이미지와 상기 제2 시점 이미지에 기초하여 상기 스테레오 이미지의 디스패리티 정보를산출하고,상기 차량의 주행 중 수신된 반사파들에 기초하여 상기 스테레오 이미지에 포함된 적어도 하나의 객체에 대한깊이 정보를 획득하고,상기 차량의 주행 중 수신된 반사광들에 기초하여 상기 적어도 하나의 객체에 대한 포인트 클라우드 정보를 획득하고, 상기 스테레오 이미지, 상기 깊이 정보, 상기 포인트 클라우드 정보 및 상기 디스패리티 정보에 기초하여 상기깊이 정보와 상기 디스패리티 정보 사이의 상관 관계를 산출하고, 상기 상관 관계에 기초하여 상기 스테레오 이미지의 깊이 정보를 교정하여 상기 스테레오 이미지에 대한 깊이이미지를 생성하는 첨단 운전자 지원 장치."}
{"patent_id": "10-2019-0058048", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "첨단 운전자 지원 장치에서 객체를 검출하는 방법으로서, 상기 첨단 운전자 지원 장치가 설치되는 차량의 주행 중 제1 센서에 의하여 촬영된 복수의 프레임들(상기 복수의 프레임들 각각은 제1 시점 이미지 및 제2 시점 이미지를 포함하는 스테레오 이미지에 해당함)을 포함하는 비디오 시퀀스를 획득하는 단계;상기 제1 시점 이미지와 상기 제2 시점 이미지에 기초하여 상기 스테레오 이미지의 디스패리티 정보를 산출하는단계;상기 차량의 주행 중 제2 센서에 의하여 수신된 반사파들에 기초하여 상기 스테레오 이미지에 포함된 적어도 하나의 객체에 대한 깊이 정보를 획득하는 단계;상기 스테레오 이미지, 상기 깊이 정보 및 상기 디스패리티 정보에 기초하여 상기 깊이 정보와 상기 디스패리티정보 사이의 상관 관계를 산출하는 단계; 및상기 상관 관계에 기초하여 상기 스테레오 이미지의 깊이 정보를 교정하여 상기 스테레오 이미지에 대한 깊이이미지를 생성하는 단계를 포함하는 첨단 운전자 지원 장치에서 객체를 검출하는 방법.공개특허 10-2020-0132468-5-"}
{"patent_id": "10-2019-0058048", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "첨단 운전자 지원 장치는 프로세싱 회로 및 메모리를 포함한다. 상기 메모리는 상기 프로세싱 회로에 의하여 실 행가능한 명령어들을 저장한다. 상기 프로세싱 회로는 상기 명령어들을 실행함으로써, 차량의 주행 중 촬영된 복 수의 프레임들(상기 복수의 프레임들 각각은 제1 시점 이미지 및 제2 시점 이미지를 포함하는 스테레오 이미지에 (뒷면에 계속)"}
{"patent_id": "10-2019-0058048", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 객체 검출에 관한 것으로, 보다 상세하게는 차량의 주행 중 객체를 검출하는 첨단 운전자 지원 장치 및 이의 객체를 인식하는 방법에 관한 것이다."}
{"patent_id": "10-2019-0058048", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "ADAS(Advanced Driver Assistance system, 첨단 운전자 지원 시스템)은 운전자의 운전을 보조 또는 지원하는 수단이다. ADAS는 차선유지제어, 사각지대 경보장치), 자동긴급제동시스템 등을 포함할 수 있다. 영상을 이용한 객체 검출 및 장면분할은 ADAS의 기반이 되는 기술이다. 이 기술은 차후 자율 주행(Automatic Driving)에서도 활용될 것으로 예상된다. 차량(vehicle)에 적용되는 기술들이 발전함에 따라, 차량의 주행에 관련된 이벤트가 발생하였는지를 인식하는 다양한 방법들이 개발되고 있다."}
{"patent_id": "10-2019-0058048", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 목적은 깊이 이미지의 품질을 향상시킬 수 있는, 객체를 검출하는 첨단 운전자 지원 장치를 제공 하는데 있다. 본 발명의 일 목적은 깊이 이미지의 품질을 향상시킬 수 있는, 첨단 운전자 지원 장치에서 객체를 검출하는 방 법을 제공하는데 있다."}
{"patent_id": "10-2019-0058048", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시예들에 따른 첨단 운전자 지원 장치는 프로세싱 회로 및 메모리를 포함한다. 상기 메모리는 상기 프로세싱 회로에 의하여 실행가능한 명령어들을 저장한다. 상기 프로세싱 회로는 상기 명령어들을 실행함으로써, 차량의 주행 중 촬영된 복수의 프레임들(상기 복수의 프레임들 각각은 제1 시점 이미지 및 제2 시점 이미지를 포함하는 스테레오 이미지에 해당함)을 포함하는 비디오 시퀀스를 획득하고, 상기 제1 시점 이미 지와 상기 제2 시점 이미지에 기초하여 상기 스테레오 이미지의 디스패리티 정보 산출하고, 상기 차량의 주행 중 수신된 반사파들에 기초하여 상기 스테레오 이미지에 포함된 적어도 하나의 객체에 대한 깊이 정보를 획득하 고, 상기 스테레오 이미지, 상기 깊이 정보 및 상기 디스패리티 정보에 기초하여 상기 깊이 정보와 상기 디스패 리티 정보 사이의 상관 관계 정보를 산출하고, 상기 디스패리티 정보 및 상기 상관 관계 정보에 기초하여 상기 스테레오 이미지의 깊이 값을 교정하여 상기 스테레오 이미지에 대한 깊이 이미지를 생성한다. 본 발명의 실시예들에 따른 첨단 운전자 지원 장치에서 객체를 검출하는 방법에서는 상기 첨단 운전자 지원 장 치가 설치되는 차량의 주행 중 제1 센서에 의하여 촬영된 복수의 프레임들(상기 복수의 프레임들 각각은 제1 시 점 이미지 및 제2 시점 이미지를 포함하는 스테레오 이미지에 해당함)을 포함하는 비디오 시퀀스를 획득하고, 상기 제1 시점 이미지와 상기 제2 시점 이미지에 기초하여 상기 스테레오 이미지의 디스패리티 정보를 산출하고, 상기 차량의 주행 중 제2 센서에 의하여 수신된 반사파들에 기초하여 상기 스테레오 이미지에 포함된 적어도 하나의 객체에 대한 깊이 정보를 획득하고, 상기 스테레오 이미지, 상기 깊이 정보 및 상기 디스패리티 정보에 기초하여 상기 깊이 정보와 상기 디스패리티 정보 사이의 상관 관계를 산출하고, 상기 상관 관계에 기초 하여 상기 스테레오 이미지의 깊이 정보를 교정하여 상기 스테레오 이미지에 대한 깊이 이미지를 생성한다."}
{"patent_id": "10-2019-0058048", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 실시예들에 따르면, 스테레오 이미지의 디스패리티를 산출하고, 스테레오 이미지에 포함된 적어도 하 나의 객체에 대한 깊이 정보를 획득하고, 깊이 정보와 상기 디스패리티 정보 사이의 상관 관계를 산출하고, 상 관 관계에 기초하여 상기 스테레오 이미지의 깊이 정보를 교정하여 깊이 이미지의 품질을 향상시켜 첨단 운전자 지원 장치에서 객체 인식의 효율을 높일 수 있다."}
{"patent_id": "10-2019-0058048", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부한 도면들을 참조하여, 본 발명의 바람직한 실시예를 보다 상세하게 설명하고자 한다. 도면상의 동일 한 구성요소에 대해서는 동일한 참조부호를 사용하고 동일한 구성요소에 대해서 중복된 설명은 생략한다. 도 1은 본 발명의 실시예들에 따른 첨단 운전자 지원 장치가 차량 전방의 객체를 검출하여 이벤트 발생 여부를 결정하는 동작의 예시를 나타낸다. 도 1을 참조하면, 첨단 운전자 지원 장치는 차량에 설치된 장치일 수 있으며, 첨단 운전자 지원 장치 는 차량에 설치된 카메라로부터 스테레오 이미지를 포함하는 비디오 시퀀스 및 반사파 또는 반사광을 수신하여, 다양한 이벤트의 발생을 결정할 수 있다. 상기 다양한 이벤트는 객체 검출(object detection), 객체 추적(object tracking) 및 장면 분리(scene segmentation)을 포함할 수 있다. 나아가, 전자 장치는 이벤 트의 발생에 따른 알림 메시지를 사용자에게 제공할 수 있다. 앞서, 첨단 운전자 지원 장치는 차량의 전방에 설치된 카메라로부터 비디오 시퀀스를 수신한다고 설 명했는데 이에 제한되지 않으며, 차량의 주변을 촬영할 수 있는 카메라로부터 비디오 시퀀스를 수신할 수 도 있다. 차량의 주변은, 예를 들어, 차량의 전방, 측방, 후방을 포함할 수 있다. 실시예들에 있어서, 첨단 운전자 지원 장치는 객체를 지정하는 바운딩 박스를 트래킹하여 객체의 위치에 따른 이벤트를 검출함으로써, 같은 종류의 객체일지라도 위치에 따른 중요도를 다르게 인식하여 위치에 다른 객 체의 이벤트 발생 여부를 결정할 수 있다. 실시예들에 있어서, 첨단 운전자 지원 장치는 복수의 비디오 시퀀스들로부터 객체가 포함된 적어도 하나 의 비디오 시퀀스(또는 스테레오 이미지)를 검출하고 레이다 반사파들 또는 반사광들(미도시)을 획득할 수 있다. 첨단 운전자 지원 장치는 적어도 하나의 비디오 시퀀스를 분석하여 고정적인 패턴을 포함하는 도로 및 시간에 따라 이동중인 다른 차량을 검출할 수 있다. 첨단 운전자 지원 장치는 비디오 시퀀스 내에서의 다른 차량의 좌표 분석을 통해, 다른 차량의 위치를 분석함으로써 다른 차량에 의하여 야기될 이벤트 발생 여부를 결정할 수 있다. 첨단 운전자 지원 장치는, 차량 내의 헤드유닛, 임베디드 보드.일 수 있다. 또한 첨단 운전자 지원 장 치는 카메라로부터 이미지(예를 들어, 동영상(video) 및 정지 영상(still image))를 획득하고, 획득된 이 미지에 기초하여 사용자에게 알림 메시지를 제공할 수 있는 기기를 포함할 수 있다. 실시예에 있어서, 첨단 운전자 지원 장치는 차량에 설치된 모듈로서, 차량의 동작을 제어할 수 있으며, 소정의 네트워크를 통하여 차량에 설치된 다른 모듈과 통신할 수 있다. 실시예에 있어서, 차량(vehicle, 100)은, 통신 기능, 데이터 프로세싱 기능, 및 운송 기능을 구비한 자동차, 버 스, 트럭, 기차, 자전거, 오토바이 등의 교통 수단일 수 있다. 또한, 첨단 운전자 지원 장치는 비디오 시퀀스와 반사파 또는 반사광을 수신하고, 알림 메시지를 전송하 고, 다른 전자 장치(미도시)의 동작을 제어하기 위한 명령을 전송하기 위하여, 소정의 네트워크를 통하여 서버 (미도시) 및 다른 전자 장치(미도시)와 통신할 수 있다. 이 경우, 네트워크는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN), 부가가치 통신망(Value Added Network; VAN), 이동 통 신망(mobile radio communication network), 위성 통신망 및 이들의 상호 조합을 포함하며, 각 네트워크 구성 주체가 서로 원활하게 통신을 할 수 있도록 하는 포괄적인 의미의 데이터 통신망이며, 유선 인터넷, 무선 인터 넷 및 모바일 무선 통신망을 포함할 수 있다. 무선 통신은 예를 들어, 무선 랜(Wi-Fi), 블루투스, 블루투스 저 에너지(Bluetooth low energy), 지그비, WFD(Wi-Fi Direct), UWB(ultra wideband), 적외선 통신(IrDA, infrared Data Association), NFC(Near Field Communication) 등이 있을 수 있으나, 이에 한정되는 것은 아니 다. 도 2는 본 발명의 실시예들에 따른 첨단 운전자 지원 장치의 예를 나타내는 블록도이다. 도 2를 참조하면, 전자 장치(900a)는 프로세싱 회로(1000a) 및 메모리를 포함할 수 있다. 도 2에서는 설명의 편의를 위하여 차량에 설치되는 제1 센서 및 제2 센서를 함께 도시한다. 실 시예에 있어서, 제1 센서는 스테레오 카메라일 수 있고, 제1 카메라 및 제2 카메라를 포함할 수 있다. 제2 센서는 거리 또는 깊이 정보를 생성하는 레이다(radar) 또는 라이다(LiDAR(light detection and arranging)일 수 있다. 도 2에서 제2 센서는 레이다인 것으로 가정한다. 프로세싱 회로(1000a)는 적 어도 하나의 프로세서를 포함할 수 있다.스테레오 카메라는 차량의 전방을 촬영하여 복수의 프레임들을 포함하는 비디오 시퀀스를 프로세싱 회로(1000a)에 제공한다. 복수의 프레임들 각각은 제1 시점 이미지(IMG1) 및 제2 시점 이미지(IMG2)를 포함하는 스테레오 이미지(SIMG)에 해당할 수 있다. 레이다는 무선 주파수(radio frequency)를 방사하고 객체로부터 반사되는 레이다 반사파들을 수신하여 제1 센싱 데이터(SD1)로서 프로세싱 회로(1000a)에 제공한다. 메모리는 프로세싱 회로(1000a)가 실행 가능한 명령어들을 저장하고, 프로세싱 회로(1000a)는 상기 명령 어들을 실행함으로써, 차량의 주행 중에 촬영된 스테레오 이미지(SIMG)를 획득하고, 스테레오 이미지(SIMG)에 포함된 제1 시점 이미지(IMG1) 및 제2 시점 이미지(IMG2)에 스테레오 매칭을 수행하여 제1 시점 이미지(IMG1) 및 제2 시점 이미지(IMG2) 사이의 디스패리티 정보를 산출하고, 제1 센싱 데이터(SD1)를 획득하여 스테레오 이 미지(SIMG)에 포함된 적어도 하나의 객체와 매칭되는 객체 트래킹 리스트를 생성하고, 스테레오 이미지(SIMG)에 서 적어도 하나의 객체를 분리하여 적어도 하나의 마스크를 추출하고, 적어도 하나의 마스크와 객체 트래킹 리 스트를 매칭한다. 프로세싱 회로(1000a)는 상기 매칭의 결과와 상기 디스패리티 정보에 기초하여 스테레오 이미지(SIMG)의 디스패 리티 정보와 깊이 정보 사이의 상관 관계 정보를 산출하고, 상기 산출된 상관 관계 정보에 기초하여 상기 스테 레오 이미지(SIMG)의 깊이 정보를 교정하여 상기 스테레오 이미지에 대한 깊이 이미지를 생성하고, 상기 깊이 이미지에서 상기 객체를 검출하고 상기 검출된 객체의 종류를 판단하고, 상기 검출된 객체를 바운딩 박스로 표 시할 수 있다. 도 3a 및 도 3b는 도 2에서 제1 카메라) 및 제2 카메라의 위치에 따른 제1 시점 이미지와 제2 시점 이미지에서 객체들을 나타낸다. 도 3a는 제1 카메라 및 제2 카메라가 각각 원래 위치에 있을 때의 제1 시점 이미지(IMG1)와 제2 시점 이미지(IMG2)를 나타낸다. 도 3a에서와 같이 제1 카메라 및 제2 카메라에 대한 정확한 캘리브레이션 (calibration) 정보가 있으면, 제1 시점 이미지(IMG1)와 제2 시점 이미지(IMG2) 사이의 디스패리티 정보를 이용 하여 객체들(OB1, OB2, OB3)에 대한 정확한 깊이 정보를 생성할 수 있다. 도 3b는 차량의 운행 중에 제1 카메라의 물리적 위치가 변경된 것을 나타낸다. 제1 카메라의 물 리적 위치가 변경되면, 제1 시점 이미지(IMG1)는 제1 시점 이미지(IMG’)로 변경되고, 제1 시점 이미지(IMG1 ’)에서 객체들(OB1’, OB2’, OB3’)의 위치도 변경된다. 따라서 프로세싱 회로(1000a)가 산출하는 제1 시점 이미지(IMG1’)와 제2 시점 이미지(IMG2) 사이의 디스패리티 정보가 변경되고 따라서 이용하여 객체들(OB1, OB2, OB3)에 대한 깊이 정보의 정확도는 감소된다. 제1 시점 이미지(IMG’)에서 점선들은 제1 카메라의 물 리적 위치가 변경되기 전의 객체들(OB1, OB2, OB3)을 나타내고, 실선들은 제1 카메라의 물리적 위치가 변 경된 후의 객체들(OB1, OB2, OB3)을 나타낸다. 도 4는 도 2의 제2 센서에서 획득환 제1 센싱 데이터에 기초하여 생성되는 객체 트래킹 리스트(object tracking list)를 나타낸다. 도 4를 참조하면, 객체 트래킹 리스트(object tracking list;OTL)는 제2 센서(즉, 레이다)가 획득한 제1 센싱 데이터(SD1)에 기초하여 레이다로부터 객체들(OB1, OB2, OB3) 각각에 대한 거리와 속도를 나타낼 수 있다. 즉, 객체 트래킹 리스트(OTL)는 객체들(OB1, OB2, OB3) 각각에 대한 깊이 정보를 나타낼 수 있다. 도 5는 본 발명의 실시예들에 따른 도 2의 전자 장치에서 프로세싱 회로의 구성을 나타내는 블록도이다. 도 5를 참조하면, 프로세싱 회로(1000a)는 이미지 프리-프로세서, 디스패리티 추정 엔진, 객체 트래 킹 엔진, 상관 산출 모듈(300a), 깊이 이미지 생성 엔진(250a), 동기 신호 생성기 및 객체 검출 엔진 을 포함할 수 있다. 이미지 프리-프로세서는 스테레오 이미지(SIMG)를 전처리하여 전처리된 제1 시점 이미지(PIMG1)와 전처리 된 제2 시점 이미지(PIMG1)를 포함하는 전처리된 스테레오 이미지(PSIMG)를 출력할 수 있다. 이미지 프리-프로 세서는 스테레오 이미지(SIMG)에 대한 노이즈 리덕션(noise reduction), 렉티피케이션(rectification), 캘리브레이션(calibration), 색상 강화(color enhancement), 색상 공간 변환(color space conversion;CSC), 인 터폴레이션(interpolation), 카메라 게인 컨트롤(camera gain control) 등을 수행할 수 있다. 따라서, 이미지 프리프로세싱 회로는 스테레오 이미지(SIMG)보다 선명한 전처리된 스테레오 이미지(PSIMG)를 출력할 수 있 다. 실시예에 따라서, 프로세싱 회로(1000a)는 이미지 프리-프로세서를 포함하지 않을 수 있고, 이 경우에, 디 스패리티 추정 엔진과 장면 분리 엔진에는 제1 시점 이미지(IMG1)와 제2 시점 이미지(IMG2) 중 적어 도 하나를 포함하는 스테레오 이미지(SIMG)가 제공될 수 있다. 디스패리티 추정 엔진은 전처리된 제1 시점 이미지(PIMG1)와 전처리된 제2 시점 이미지(PIMG1)에 기초하여 전처리된 스테레오 이미지(PSIMG)의 디스패리티 정보를 포함하는 디스패리티 이미지(DPIMG)를 출력할 수 있다. 디스패리티 추정 엔진은 전처리된 제1 시점 이미지(PIMG1)와 전처리된 제2 시점 이미지(PIMG1)에 대하여 스테레오 매칭을 수행하여 디스패리티 정보를 포함하는 디스패리티 이미지(DPIMG)를 출력할 수 있다. 디스패리 티 추정 엔진은 전처리된 제1 시점 이미지(PIMG1)와 전처리된 제2 시점 이미지(PIMG1) 각각에서 대응하는 픽셀들 사이의 픽셀 값 차이에 기초하여 디스패리티 이미지(DPIMG)를 출력할 수 있다. 객체 트래킹 엔진은 적어도 하나의 객체로부터의 반사파들에 해당하는 제1 센싱 데이터(SD1)에 기초하여 적어도 하나에 대한 깊이 정보를 포함하는 객체 트래킹 리스트 데이터(OTLD)를 제공한다. 상관 산출 모듈(300a)은 전처리된 스테레오 이미지(PSIMG), 객체 트래킹 리스트 데이터(OTLD) 및 디스패리티 정 보를 포함하는 디스패리티 이미지(DPIMG)에 기초하여 깊이 정보와 디스패리티 정보 사이의 상관 관계 정보 (CORR1)를 산출하고 상관 관계 정보(CRRI1)를 깊이 이미지 생성 엔진(250a)에 제공한다. 상관 산출 모듈(300a)은 장면 분리 엔진, 매칭 엔진 및 상관 관계 산출 엔진을 포함할 수 있다. 장면 분리 엔진은 전처리된 제1 시점 이미지(PIMG1)와 전처리된 제2 시점 이미지(PIMG1) 중 적어도 하나에 서 적어도 하나의 객체를 분리하여 적어도 하나의 마스크(MSK)를 추출할 수 있다. 매칭 엔진은 적어도 하 나의 마스크(MSK) 및 객체 트래킹 리스트 데이터(OTLD)에 매칭을 수행하여 매칭 결과들(MMKS, MOTLD)을 상관 관 계 산출 엔진에 제공할 수 있다. 매칭 결과들(MMKS, MOTLD)은 마스크(MKS)에 대한 제1 매칭 결과(MMKS)와 객체 트래킹 리스트 데이터(OLTD)에 대한 제2 매칭 결과(MOLTD)를 포함할 수 있다. 상관 관계 산출 엔진은 디스패리티 정보를 포함하는 디스패리티 이미지(DPIMG)를 수신하고, 디스패리티 정보와 매칭 결과들(MMKS, MOTLD)에 기초하여 깊이 정보와 디스패리티 정보 사이의 상관 관계 정보(CRRI1)를 산 출하고 상관 관계 정보(CRRI1)를 깊이 이미지 생성 엔진(250a)에 제공할 수 있다. 깊이 이미지 생성 엔진(250a)은 디스패리티 이미지(DPIMG)에 포함되는 디스패티리 정보 및 상관 관계 정보 (CRRI1)에 기초하여 전처리된 스테레오 이미지(PSIMG)의 깊이 값을 교정하여 전처리된 스테레오 이미지(PSIMG) 에 대한 깊이 이미지(DPIMG1)을 생성하고, 깊이 이미지(DPIMG1)을 객체 검출 엔진에 제공할 수 있다. 예를 들어, 깊이 이미지 생성 엔진(250a)은 다음의 수학식 1에 기초하여 전처리된 스테레오 이미지(PSIMG)의 깊 이 값을 교정할 수 있다. [수학식 1]"}
{"patent_id": "10-2019-0058048", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, Z는 깊이 값이고, B는 제1 카메라와 제2 카메라 간의 거리인 베이스라인이고, f는 제1 카메 라와 제2 카메라의 초점 거리를 나타내고, d는 디스패리티를 나타낸다. s는 상관 관계 정보(CRRI1)에 해당한다. 객체 인식 엔진은 깊이 이미지(DPIMG1) 및 전처리된 스테레오 이미지(PSIMG)에 기초하여 전처리된 스테레 오 이미지(PSIMG)에서 적어도 하나의 객체를 검출하고, 상기 검출된 객체를 포함하는 최종 이미지(FIMG)또는 상 기 최종 이미지(FIMG)에서 인식된 객체를 표시하는 바운딩 박스(BB)를 출력할 수 있다. 동기 신호 생성기는 프레임 정보(FRMI1)에 기초하여 동기 신호(SYNS)를 생성할 수 있다. 프레임 정보 (FRMI1)는 스테레오 이미지(SIMG)에 대한 제1 FPS(frame per second) 및 객체 트래킹 리스트 데이터(OLTD)에 대한 제2 FPS에 관한 정보를 포함할 수 있다. 프로세싱 회로(1000a)는 상기 검출된 객체를 포함하는 최종 이미지 (FIMG) 또는 검출된 객체를 나타내는 바운딩 박스(BB)를 운행 중인 차량의 디스플레이 장치 또는 HUD(head-up display)에 표시할 수 있다. 실시예에 있어서, 스테레오 이미지(SIMG)의 초당 프레임이 객체 트래킹 리스트 데이터(OTLD)의 초당 프레임보다 빠르기 때문에, 상관 산출 모듈(300a)은 이미지 프리프로세싱 회로, 디스패리티 추정 엔진 및 깊이이미지 생성 엔진(250a)보다 느리게 갱신될 수 있다. 도 6은 본 발명의 실시예들에 따른 도 5의 프로세싱 회로에서 객체 검출 엔진을 나타내는 블록도이다. 도 6을 참조하면, 객체 검출 엔진은 제1 특징 추출기, 제2 특징 추출기, 센서 융합 엔진 및 박스 예측기를 포함할 수 있다. 제1 특징 추출기는 전처리된 제1 시점 이미지(PIMG1) 및 전처리된 제2 시점 이미지(PIMG2) 중 적어도 하나 의 특징을 추출하여 제1 특징 벡터(FV1)를 제공한다. 제2 특징 추출기는 깊이 이미지(DTIMG)의 특징을 추 출하여 제2 특징 벡터(FV2)를 제공한다. 센서 융합 엔진은 컨벌류션신경망(CNN) 기반으로 제1 특징 벡터 (FV1)와 제2 특징 벡터(FV2)를 융합하여 융합된 특징 벡터(FFV)를 제공한다. 박스 예측기는 신경망을 기반으로 융합된 특징 벡터(FFV)에서 객체를 검출하고 상기 검출된 객체를 포함하 는 최종 이미지(FIMG) 또는 검출된 객체를 나타내는 바운딩 박스(BB)를 출력한다. 실시예에 있어서, 박스 예측 기는 3차원의 3D 바운딩 박스를 출력하거나 3차원의 3D 바운딩 박스를 2차원 최종 이미지로 변환하고 상기 2차원 최종 이미지에서 상기 잭체를 상기 바운딩 박스로 표시하여 출력할 수 있다. 도 7은 도 5의 프로세싱 회로에서 디스패리티 이미지를 나타낸다. 도 5 및 도 7을 참조하면, 디스패리티 추정 엔진은 전처리된 제1 시점 이미지(PIMG1)와 전처리된 제2 시점 이미지(PIMG1) 각각에서 대응하는 픽셀들 사이의 픽셀 값 차이를 나타내는 디스패리티 이미지(DPIMG)를 출력한 다. 도 8은 도 5의 프로세싱 회로에서 마스크를 나타낸다. 도 5 및 도 8을 참조하면, 장면 분리 엔진은 전처리된 제1 시점 이미지(PIMG1)와 전처리된 제2 시점 이미 지(PIMG1) 중 적어도 하나에서 적어도 하나의 객체를 분리하여 객체들(OB1, OB2, OB3)에 대한 마스크들(MSK)을 추출할 수 있다. 마스크들(MSK)은 거리에 관계없이 동일한 컬러로 표현될 수 있다. 도 9는 도 5의 프로세싱 회로의 상관 관계 산출 엔진이 디스패리티 이미지와 마스크를 결합한 것을 나타낸다. 도 5 및 도 9를 참조하면, 상관 관계 산출 엔진(330a)은 디스패리티 이미지(DPIMG)와 마스크를 결합하여 객체들 (OB1, OB2, OB3)의 거리에 따라 마스크를 서로 구분되게 서로 다른 식별자를 사용하여 나타낼 수 있다. 도 10은 도 5의 프로세싱 회로의 깊이 이미지 생성 엔진에서 제공되는 깊이 이미지를 나타낸다. 도 5 및 도 10을 참조하면, 깊이 이미지 생성 엔진(250a)은 디스패리티 이미지(DPIMG)에 포함되는 디스패티리 정보 및 상관 관계 정보(CRRI1)에 기초하여 전처리된 스테레오 이미지(PSIMG)의 깊이 값을 교정하여 전처리된 스테레오 이미지(PSIMG)에 대한 깊이 이미지(DPIMG1)를 제공하는 것을 알 수 있다. 도 11 및 도 12는 도 5의 프로세싱 회로의 객체 인식 모듈이 제공하는 최종 이미지를 나타낸다. 도 11을 참조하면, 박스 예측기는 상기 최종 이미지(FIMG)에서 객체들(OB1, OB2, OB3) 각각을 바운딩 박스 들(BB1, BB2, BB3)로 표시하여 출력할 수 있다. 도 12를 참조하면, 박스 예측기는 상기 3차원의 바운딩 박스(BB)를 2차원의 조감도(bird-eye-view)로 변환 하고, 상기 2차원의 조감도에서 객체들(OB1, OB2, OB3) 각각을 바운딩 박스들(BB1, BB2, BB3)로 표시하여 출력 할 수 있다. 실시예에 있어서, 프로세싱 회로(1000a)는 바운딩 박스들(BB1, BB2, BB3)을 시계열적으로 트래킹하여 차량의 주 행과 관련된 이벤트를 검출하고, 검출된 이벤트를 알리는 알림 메시지를 사용자에게 제공할 수 있다. 도 13은 도 6의 객체 검출 엔진의 동작을 나타낸다. 도 13을 참조하면, 제1 특징 추출기는 전처리된 제1 시점 이미지(PIMG1)의 특징을 추출하여 제1 특징 벡터 들(FV11, FV12, FV13)를 제공하고, 제2 특징 추출기는 깊이 이미지(DTIMG)의 특징을 추출하여 제2 특징 벡 터들(FV21, FV22, FV23)를 제공한다. 센서 융합 엔진은 CNN 기반으로 제1 특징 벡터들(FV11, FV12, FV1 3)과 제2 특징 벡터들(FV21, FV22, FV23)을 융합하여 융합된 특징 벡터들(FFV11, FFV12, FFV13)을 박스 예측기 에 제공한다. 박스 예측기는 융합된 특징 벡터들(FFV11, FFV12, FFV13)에서 객체를 인식하고 인식된 객체를 바운딩 박스로 나타내는 최종 이미지(FIMG)를 출력한다. 도 14는 도 13의 객체 검출 엔진에서 박스 예측기를 나타낸다. 도 14를 참조하면, 박스 예측기는 특징 피라미드 망 및 검출기를 포함할 수 있다. 특징 피라미드 망은 융합된 특징 벡터들(FFV11, FFV12, FFV13)을 기초로 고해상도 특징 맵들(FM11, FM12, FM13)을 생성하고 고해상도 특징 맵들(FM11, FM12, FM13)을 검출기에 제공한다. 검출기는 고해상도 특징 맵들(FM11, FM12, FM13)에서 적어도 하나의 객체를 검출하고, 검출된 객체를 바운딩 박스로 나타내는 최종 이미지(FIMG)를 출력한다. 도 15a는 도 14의 박스 예측기에서 특징 피라미드 망을 나타낸다. 도 15를 참조하면, 특징 파리미드 망은 융합된 특징 벡터들(FFV11, FFV12, FFV13)을 기초로 고해상도 특징 맵들(FM11, FM12, FM13)을 생성한다. 특징 피라미드 망은 복수의 레이어들(451, 452, 453), 복수의 병합 블록(457, 458) 및 복수의 컨볼루션 커널들(454, 455, 456)을 포함할 수 있다. 도 15a에서는 세 개의 레이어들 및 컨볼루션 커널들을 도시하였으나 레이어들 및 컨벌루션 커널들의 수는 이에 제한되지 않는다. 레이어는 융합된 특징 벡터(FFV1)을 업샘플링하고, 컨볼루션 커널은 레이어의 출력에 대하여 컨 볼루션 커널 변환을 적용하여 특징 맵(FM1)으로 출력한다. 병합 블록은 레이어의 출력과 융합된 특징 벡터(FFV2)를 병합하여 출력한다. 레이어는 병합 블록의 출력을 업샘플링하고 컨볼루션 커널은 레이어의 출력에 대하여 컨볼 루션 커널 변환을 적용하여 특징 맵(FM2)으로 출력한다. 병합 블록은 레이어의 출력과 융합된 특징 벡터(FFV3)를 병합하여 출력한다. 레이어는 병합 블 록의 출력을 업샘플링하고, 컨볼루션 커널은 레이어의 출력에 대하여 컨볼루션 커널 변환을 적 용하여 특징 맵(FM3)으로 출력한다. 도 15b는 본 발명의 실시예들에 따른 도 15a의 특징 피라미드 망에서 병합 블록을 나타낸다. 도 15b를 참조하면, 병합 블록은 업-샘플러(457a), 컨볼루션 레이어(커널)(457b) 및 합산기(457c)를 포함 할 수 있다. 업-샘플러(457a)는 레이어의 출력을 업-샘플링하고 업-샘플링된 출력을 합산기(457c)에 제공한다. 업-샘플 러(457a)는 컨볼루션 레이어(CONV)로 구성될 수 있다. 컨볼루션 레이어(CONV)는 융합된 특징 벡터(FFV2)에 대하 여 컨볼루션 변환을 적용한 뒤 합산기(457c)에 제공한다. 합산기(457c)는 업-샘플러(457a)의 출력과 컨볼루션 레이어(457b)의 출력을 합산하고 합산된 결과를 레이어에 제공한다. 도 16은 본 발명의 실시예들에 따른 도 2의 첨단 운전자 지원 장치의 동작을 나타내는 흐름도이다. 도 2 내지 도 16을 참조하면, 프로세싱 회로(1000a)는 스테레오 카메라로부터 스테레오 이미지(SIMG)를 획 득하고(S110), 제1 센서로부터 레이다 반사파들을 획득하고, 레이다 반사파들에 기초하여 적어도 하나의 객체까지의 거리 정보(깊이 정보)를 나타내는 객체 트래킹 리스트 데이터(OTLD)를 생성한다(S120). 프로세싱 회로(1000a)는 스테레오 이미지(SIMG)의 제1 시점 이미지(IMG1) 및 제2 시점 이미지(IMG2) 사이의 디 스패리티 정보를 나타내는 디스패리티 이미지(DPIMG)를 생성하고(S130), 스테레오 이미지(SIMG)에 기초하여 적 어도 하나의 객체를 구분하는 마스크(MKS)를 생성한다(S140). 프로세싱 회로(1000a)는 마스크(MKS)와 객체 트래킹 리스트 데이터(OTLD)에 기초하여 스테레오 이미지(SIMG)의 깊이 정보와 디스패리티 정보 사이의 상관 관계 정보(CORRI1)를 산출한다(S150). 프로세싱 회로(1000a)는 디스 패리티 정보와 상관 관계 정보(CORRI1) 정보에 기초하여 스테레오 이미지(SIMG)의 깊이 값을 교정하여 스테레오 이미지(SIMG)에 대한 깊이 이미지(DTIMG)를 생헝한다(S160). 프로세싱 회로(1000a)는 신경 망을 기반으로 스테레오 이미지(SIMG)와 깊이 이미지 이미지(DTIMG)를 이용하여 스테레오 이미지(SIMG)에서 객체를 검출하고 검출된 객체를 포함하는 최종 이미지(FIMG)를 제공하거나 검출된 객체를 나타내는 바운딩 박스(BB)를 제공한다(S170). 도 17은 본 발명의 실시예들에 따른 첨단 운전자 지원 장치의 다른 예를 나타내는 블록도이다. 도 17을 참조하면, 전자 장치(900b)는 프로세싱 회로(1000b) 및 메모리를 포함할 수 있다. 도 17에서는 설명의 편의를 위하여 차량에 설치되는 제1 센서, 제2 센서 및 제3 센서를 함 께 도시한다. 실시예에 있어서, 제1 센서는 스테레오 카메라일 수 있고, 제1 카메라 및 제2 카메라를 포함할 수 있다. 제2 센서는 레이다(radar)일 수 있고, 제3 센서는 라이다(LiDAR)일 수 있 다. 도 17의 프로세싱 회로(1000b)는 라이다인 제3 센서에서 제공되는 제2 센싱 데이터(SD2)를 더 수신한다는 점이 도 2의 프로세싱 회로(1000b)와 차이가 있다. 라이다는 광을 방사하고 객체로부터의 반사광을 수신하여 제2 센싱 데이터(SD2)로서 프로세싱 회로(1000 b)에 제공한다. 메모리는 프로세싱 회로(1000b)가 실행 가능한 명령어들을 저장하고, 프로세싱 회로(1000a)는 상기 명령 어들을 실행함으로써, 차량의 주행 중에 촬영된 스테레오 이미지(SIMG)를 획득하고, 스테레오 이미지(SIMG)에 포함된 제1 시점 이미지(IMG1) 및 제2 시점 이미지(IMG2)에 스테레오 매칭을 수행하여 제1 시점 이미지(IMG1) 및 제2 시점 이미지(IMG2) 사이의 디스패리티 정보를 산출하고, 제1 센싱 데이터(SD1)를 획득하여 스테레오 이 미지(SIMG)에 포함된 적어도 하나의 객체와 매칭되는 객체 트래킹 리스트를 생성하고, 제2 센싱 데이터(SD2)를 획득하여 적어도 하나의 객체에 대한 공간적 포인트 클라우드 데이터를 생성하고, 스테레오 이미지(SIMG)에서 적어도 하나의 객체를 분리하여 적어도 하나의 마스크를 추출하고, 적어도 하나의 마스크, 객체 트래킹 리스트 및 공간적 포인트 클라우드 데이터를 매칭한다. 프로세싱 회로(1000b)는 상기 매칭의 결과와 상기 디스패리티 정보에 기초하여 스테레오 이미지(SIMG)의 디스패리티 정보와 깊이 정보 사이의 상관 관계를 산출하고, 상기 산 출된 상관 관계에 기초하여 상기 스테레오 이미지(SIMG)의 깊이 정보를 교정하여 상기 스테레오 이미지에 대한 깊이 이미지를 생성하고, 상기 깊이 이미지에서 상기 객체를 검출하고 상 상기 검출된 객체를 바운딩 박스로 표 시할 수 있다. 도 18은 본 발명의 실시예들에 따른 도 17의 전자 장치에서 프로세싱 회로의 구성을 나타내는 블록도이다. 도 18을 참조하면, 프로세싱 회로(1000b)는 이미지 프리-프로세서, 디스패리티 추정 엔진, 객체 트래 킹 엔진, 공간적 포인트 클라우드 엔진, 상관 산출 모듈(300b), 깊이 이미지 생성 엔진(250b) 및 객 체 인식 엔진을 포함할 수 있다. 도 18의 프로세싱 회로(1000b)는 공간적 포인트 클라우드 엔진을 더 포함하고, 상관 산출 모듈(300b) 및 깊이 이미지 생성 엔진(250b)에서 도 5의 프로세싱 회로(1000a)와 차이가 있으므로, 공간적 포인트 클라우드 엔 진, 상관 산출 모듈(300b) 및 깊이 이미지 생성 엔진(250b)에 대하여 상세히 설명한다. 공간적 포인트 클라우드 엔진은 반사광인 제2 센싱 데이터(SD2)를 수신하여 적어도 하나의 객체에 대한 공 간적 깊이 정보를 포함하는 공간적 포인트 클라우드 데이터(SPCD)를 상관 산출 모듈(300b)에 제공한다. 상관 산출 모듈(300b)은 전처리된 스테레오 이미지(PSIMG), 객체 트래킹 리스트 데이터(OTLD), 공간적 포인트 클라우드 데이터(SPCD) 및 디스패리티 정보를 포함하는 디스패리티 이미지(DPIMG)에 기초하여 깊이 정보와 디스 패리티 정보 사이의 상관 관계 정보(CORR2)를 산출하고 상관 관계 정보(CRRI1)를 깊이 이미지 생성 엔진(250b) 에 제공한다. 깊이 이미지 생성 엔진(250b)은 디스패리티 이미지(DPIMG)에 포함되는 디스패티리 정보 및 상관 관계 정보 (CRRI2)에 기초하여 전처리된 스테레오 이미지(PSIMG)의 깊이 값을 교정하여 전처리된 스테레오 이미지(PSIMG) 에 대한 깊이 이미지(DPIMG2)을 생성하고, 깊이 이미지(DPIMG2)을 객체 검출 엔진에 제공할 수 있다. 도 19는 본 발명의 실시예들에 따른 상관 산출 모듈을 나타내는 블록도이다. 도 19를 참조하면, 상관 산출 모듈(300b)은 장면 분리 엔진, 매칭 엔진(320b) 및 상관 관계 산출 엔진 (330b)을 포함할 수 있다. 장면 분리 엔진은 전처리된 제1 시점 이미지(PIMG1)와 전처리된 제2 시점 이미지(PIMG1) 중 적어도 하나에 서 적어도 하나의 객체를 분리하여 적어도 하나의 마스크(MSK)를 추출할 수 있다. 매칭 엔진(320b)은 적어도 하 나의 마스크(MSK), 객체 트래킹 리스트 데이터(OLTD) 및 공간적 포인트 클라우드 데이터(SPCD)에 매칭을 수행하 여 매칭 결과들(MMKS, MOTLD, MSPCD)를 상관 관계 산출 엔진에 제공할 수 있다. 매칭 결과들(MMKS, MOTLD, MSPCD)은 마스크(MKS)에 대한 제1 매칭 결과(MMKS)와 객체 트래킹 리스트 데이터(OLTD)에 대한 제2 매칭 결과(MOLTD) 및 공간적 포인트 클라우드 데이터(SPCD)에 대한 제3 매칭 결과(MSPCD)를 포함할 수 있다. 상관 관계 산출 엔진(330b)은 디스패리티 정보를 포함하는 디스패리티 이미지(DPIMG)를 수신하고, 디스패리티 정보와 매칭 결과들(MMKS, MOTLD, MSTI)에 기초하여 깊이 정보와 디스패리티 정보 사이의 상관 관계 정보(CRRI2)를 산출하고 상관 관계 정보(CRRI2)를 깊이 이미지 생성 엔진(250b)에 제공할 수 있다. 도 20은 도 17의 제2 센서에서 제공되는 포인트 클라우드 이미지를 나타낸다. 도 20을 참조하면, 라이다에세 제공되는 포인트 클라우드 이미지는 객체들을 포인트들의 집합으로 나타낼 수 있고, 거리에 따라 다른 컬러로 나타낼 수 있다. 도 21은 도 19의 상관 관계 산출 엔진이 디스패리티 이미지, 마스크 및 공간적 포인트 클라우드 데이터를 결합 한 것을 나타낸다. 도 21을 참조하면, 상관 관계 산출 엔진(330b)은 디스패리티 이미지(DPIMG), 마스크(MKS) 및 매칭 결과(MSDI)에 포함되는 포인트 클라우드 정보를 결합하여 객체들(OB1, OB2, OB3)의 거리에 따라 마스크를 서로 구별되도록 하 고, 객체들(OB1, OB2, OB3) 내부에 포인트들을 표시할 수 있다. 도 22는 본 발명의 실시예들에 따른 프로세싱 회로가 스테레오 이미지, 제1 센싱 데이터 및 제2 센싱 데이터를 동기시키는 것을 나타낸다. 도 22를 참조하면, 제1 센서는 스테레오 이미지(SIMG)를 제1 FPS(frame per second)로 출력하고, 제2 센 서는 객체 트래킹 리스트(OTL)를 제2 FPS로 출력하고, 제3 센서는 포인트 클라우드 이미지(SPCD)를 제3 FPS로 출력할 수 있다. 제1 FPS는 제2 FPS보다 크고, 제2 FPS는 제3 FPS보다 클 수 있다. 서로 다른 FPS를 가지는 이미지들의 동기를 맞추기 위하여 프로세싱 회로(1000b)의 동기 신호 생성기에서 생성되는 동기 신호(SYNS)에 의하여 서로 다른 FPS를 가지는 이미지들을 동기시킬 수 있다. 도 23은 본 발명의 실시예들에 따른 첨단 운전자 지원 장치가 이벤트 발생 여부를 결정하는 방법을 나타내는 흐 름도이다. 블록 S200에서 첨단 운전자 지원 장치는 차량에 설치된 카메라로부터 복수의 프레임(frame)들을 포함하는 비디오 시퀀스를 획득함과 차량에 설치된 레이다로부터 레이다 반사파들을 획득할 수 있다. 일 실시예에서, 전 자 장치는 소정의 네트워크를 통하여 차량에 설치된 카메라와 통신함으로써, 비디오 시퀀스를 수신하고, 차량에 설치된 레이다와 통신함으로써 레이다 반사파들을 획득할 수 있다. 예를 들어, 비디오 시퀀스는 차량의 블랙박스(blackbox) 영상일수 있고, 또는 차량의 스테레오 카메라로부터 수신한 스테레오 이미지일 수 있다. 일 실시예에서, 첨단 운전자 지원 장치는 카메라를 구비하여, 전자 장치에 포함된 카메라로부터 비디오 시퀀스를 획득할 수도 있다. 비디오 시퀀스는 일련의 정지 영상(still image)들로 구성될 수 있다. 정지 영상들 각각은 픽처(picture) 또는 프레임(frame)을 의미할 수 있다. 블록 S300에서 첨단 운전자 지원 장치는 복수의 프레임들에 포함된 객체를 검출하고 검출된 객체를 바운 딩 박스로 표시할 수 있다. 일 실시예에서, 첨단 운전자 지원 장치는 비디오 시퀀스에 포함된 하나의 프레 임으로부터 하나 이상의 객체를 검출할 수 있다. 하나의 프레임으로부터 검출된 하나 이상의 객체는, 동일한 비 디오 시퀀스에 포함된 다른 프레임에서도 검출될 수 있다. 또는, 하나의 프레임으로부터 검출된 하나 이상의 객 체는, 동일한 비디오 시퀀스에 포함된 다른 프레임에서는 검출되지 않을 수 있다. 첨단 운전자 지원 장치 는 복수의 프레임들에 포함된 객체를 검출함에 있어, 스테레오 이미지에 포함되는 제1 시점 이미지와 제2 시점 이미지에 기초하여 디스패리티 정보를 획득하고, 상기 레이다 반사파들로부터 객체까지의 깊이 정보를 획득하고, 상기 디스패리티 정보와 상기 깊이 정보에 기초하여 상기 디스패리티 정보와 상기 깊이 정보 사이의 상관 관계를 획득하고, 상기 상관 관계에 기초하여 상기 객체의 깊이 정보를 보정하여 상기 객체의 깊이에 대한 정확도를 향상시킬 수 있다. 실시예에 있어서, 첨단 운전자 지원 장치는 인공지능학습 모델을 이용하여 객체의 위치를 인식할 수 있다. 예를 들어, 제 1 프레임에서의 제 1 자동차의 바운딩 박스부터 제 1 자동차가 프레임 상에서 어느 위치에 있는 지 인식할 수 있다. 또한, 제 1 프레임에서의 제 1 자동차의 바운딩 박스 및 제 3 자동차의 바운딩 박스를 이용 하여 제 1 자동차와 제 3 자동차의 간격을 인식할 수 있다. 또한, 제 3 프레임에서의 제 1 자동차의 바운딩 박 스 및 제 3 자동차의 바운딩 박스를 이용하여 제 3 프레임에서 제 1 자동차와 제 3 자동차 사이의 간격의 변화 량을 인식할 수 있다. 실시예에 있어서, 첨단 운전자 지원 장치는 객체의 종류를 결정하고, 객체가 인식된 위치에서 객체의 종류 가 가질 수 있는 크기 정보에 기초하여, 객체가 노이즈인지 여부를 결정할 수 있다. 실시예에 있어서, 첨단 운전자 지원 장치는 객체의 종류(type)를 판단할 수 있다. 실시예에 있어서, 첨단 운전자 지원 장치는 프레임 내에 포함된 객체를 검출하기 위해, 제1 학습 모델을 이용할 수 있다. 일 개시에 의하여, 제1 학습 모델은 차량의 주행 중 촬영된 복수의 프레임들을 포함하는 비디 오 시퀀스에서 객체를 검출하고 검출된 객체를 바운딩 박스로 표시하는 학습의 결과에 의하여 획득될 수 있다. 따라서, 비디오 시퀀스로부터 획득된 프레임들과 레이다 반사파들이 제1 학습 모델에 입력되면, 프레임들로부터 검출된 객체를 지정하는 바운딩 박스가 제1 학습 모델로부터 출력될 수 있다. 블록 S400에서, 첨단 운전자 지원 장치는 복수의 프레임들 내의 객체의 바운딩 박스를 시계열적으로 트래 킹함으로써, 차량의 주행에 관련된 이벤트 발생 여부를 결정할 수 있다. 실시예에 있어서, 첨단 운전자 지원 장치는 비디오 시퀀스의 재생 순서(display order)에 따라, 이전 프레 임에서 다음프레임으로의 바운딩 박스의 위치 변화를 분석할 수 있다. 예를 들어, 전자 장치는 재생 순서 가 앞인 제 1 프레임에 포함된 객체의 바운딩 박스와 재생 순서가 뒤인 제 2 프레임에 포함된 동일한 객체의 바 운딩 박스의 위치를 비교함으로써, 객체의 위치 변화를 분석할 수 있다. 예를 들어, 첨단 운전자 지원 장치 는 시간의 흐름에 따라, 복수의 객체들 각각의 위치 변화를 분석함으로써, 이벤트가 발생하였는지를 결정 할 수 있다. 실시예에 있어서, 첨단 운전자 지원 장치는 복수의 프레임들 내의 객체의 바운딩 박스를 시계열적으로 트 래킹함으로써, 이벤트의 종류(type)를 결정할 수 있다. 실시예에 있어서, 첨단 운전자 지원 장치는 복수의 프레임들 내의 객체의 바운딩 박스를 시계열적으로 트래킹함으로써, 주행의 위험도를 결정할 수 있다. 실시예에 있어서, 첨단 운전자 지원 장치는 이벤트가 발생하였는지를 결정하기 위해, 제 2 학습 모델을 이 용할 수 있다. 제 2 학습 모델에 제 1 학습모델에서 출력된 객체에 관련된 출력값을 입력하면, 이벤트의 발생 여부가 출력될 수 있다. 실시예에 있어서, 객체를 검출하는 동작, 객체를 바운딩 박스로 표시하는 동작, 이벤트가 발생하였는지를 결정 하는 동작은 복수의 학습 모델들을 이용하여 수행될 수 있다. 도 24는 본 발명의 실시예들에 따른 차량의 주행에 관련된 이벤트 발생 여부를 결정하는 학습모델의 생성을 설 명하기 위한 도면이다. 실시예들에 따르면, 첨단 운전자 지원 장치는 전처리된 스테레오 이미지(PSIMG)에 포함되는 전처리된 제1 시점 이미지(PIMG1) 및 전처리된 제2 시점 이미지(PIMG2) 중 적어도 하나를 이용하여 제1 학습 모델 및 제 2 학습모델를 포함하는 학습 모델을 학습시킴으로써 객체의 위치에 따른 주행에 관련된 이벤트를 검 출할 수 있는 이벤트 검출 모델을 생성할 수 있다. 실시예들에 따르면, 제1 학습 모델은, FCN(Fully Convolutional Network)을 이용하여 객체의 종류(type) 를 판단하는 기준 및 복수의 프레임들 각각에 대해 객체를 지시하는 바운딩 박스의 복수의 프레임들 내 위치들 을 판단하는 기준을 학습함으로써 생성될 수 있다. 실시예들에 따르면, 첨단 운전자 지원 장치는 는 RGB 채널로 이루어진 프레임을 포함하는 전처리된 제1 시 점 이미지(PIMG1) 및 전처리된 제2 시점 이미지(PIMG2) 중 적어도 하나를 제1 학습 모델에 입력할 수 있다. 제1 학습 모델은 도 4의 객체 인식 엔진을 채용하여 전처리된 제1 시점 이미지(PIMG1) 및 전처 리된 제2 시점 이미지(PIMG2) 중 적어도 하나에 포함되는 객체를 검출하고, 검출된 객체를 바운딩 박스로 표시 하도록 학습될 수 있다. 첨단 운전자 지원 장치는 제1 학습 모델을 이용하여, 복수의 프레임들 내에 포함된 객체를 검출하고, 검출된 객체를 바운딩 박스로 표시할 수 있다. 예를 들어, 첨단 운전자 지원 장치는 제1 학습 모델을 이용하여, 하나의 프레임으로부터 복수의 객체들을 검출하고, 복수의 객체들 각각의 종류가 무엇인지 판단할 수 있다. 제2 학습 모델은, 다양한 신경망들 중 적어도 하나를 이용하여, 복수의 프레임들 내의 객체를 지정하는 바 운딩 박스를 시계열적으로 트래킹함으로써, 차량의 주행에 관련된 이벤트가 발생하였는지를 결정하는 기준을 학 습함으로써 생성되는 것일 수 있다. 제1 학습 모델의 출력은 제 2 학습 모델의 입력으로 이용될 수 있다. 다른 실시예에서, 전자 장치는 이벤트가 발생하였는지를 결정하기 위한 제2 학습 모델의 연산량을 감소시키기 위해, 제1 학습 모델에서 출력된 매트릭스의 차원을 감소시킨 매트릭스를 제 2 학습 모델의 입력으 로 이용할 수 있다. 예를 들어, 매트릭스의 차원을 감소시키기 위해, 확장 컨볼루션(dilated convolution)을 이용할 수 있다. 첨단 운전자 지원 장치는 전처리된 스테레오 이미지(PSIMG) 이외에 레이다 반사파(미도시)가 입력된 제1 학습 모델을 이용하여, 객체를 보다 정확하게 인식할 수 있다. 따라서, 첨단 운전자 지원 장치 는 제 2 학습 모델을 이용하여, 차량의 주행에 관련된 이벤트의 발생을 결정할 수 있다. 실시예에 있어서, 제2 학습 모델은 제1 학습 모델의 출력과 객체 트래킹 리스트 데이터(OTLD)를 병합하여 객체 검출 모델 을 생성할 수 있다. 실시예에 있어서, 프로세싱 회로(1000a, 1000b)는 학습 차량의 주행 중에 촬영된 복수의 학습 프레임들을 포함 하는 비디오 시퀀스에서 학습 객체를 검출하는 결과에 기초하여 제1 학습 모델을 획득하고, 상기 획득한 제1 학 습 모델을 이용하여 상기 스테레오 이미지에 포함된 상기 적어도 하나의 객체를 인식하고 인식된 객체를 바운딩 박스로 표시할 수 있다. 도 25는 본 발명의 실시예들에 따른 제1 학습 모델을 이용하여 객체를 검출하는 예시를 설명하기 위한 도면이다. 도 25를 참조하면, 첨단 운전자 지원 장치는 차량의 주행 중 획득한 복수의 프레임을 포함하는 전처리된 스테레오 이미지(PSIMG)를 입력값으로 학습된 제 1 학습 모델을 이용하여 프레임 내에서 객체를 검출하고, 검출된 객체를 바운딩 박스로 표시할 수 있다. 실시예에 있어서, 제 1 학습 모델은 FCN와을 이용할 수 있기 때문에, 첨단 운전자 지원 장치는 제1 학습 모델에 전처리된 스테레오 이미지(PSIMG)를 입력하면, 객체의 종류 및 바운딩 박스를 출력할 수 있다. 실시예에 있어서, 제1 학습 모델에서 출력된 일련의 매트릭스를 이미지화하면, 비디오 시퀀스에 포함 된 객체의 종류 별로 다른 색으로 표현된 비디오 시퀀스가 획득될 수 있다. 예를 들어, 일정한 패턴을 형 성하는 도로와 이동하는 객체인 자동차는 다른 색상으로 표현될 수 있다. 일 실시예에 있어서, 첨단 운전자 지원 장치는 객체의 종류 및 정확도를 검출할 수 있다. 예를 들어, 첨단 운전자 지원 장치는 제1 학습모델에서 출력된 비디오 시퀀스 내의 제1 객체 및 제2 객체 의 종류 및 위치를 결정할 수 있다. 첨단 운전자 지원 장치는 제1 객체의 형태 및 위치 정보를 이용하여 제1 객체가 75%의 정확도로 버스임을 인식할 수 있으며, 제2 객체의 형태 및 위치 정보를 이용하여 제2 객체가 97%의 정확도로 승용차임을 인식할 수 있다. 도 26은 본 발명의 실시예들에 따라 제 2 학습 모델을 이용하여 객체의 시계열적 이동에 따른 이벤트 발생 여부 를 결정하는 것을 나타낸다. 일 실시예에서, 제 1 학습모델에서 출력된 객체를 나타내는 바운딩 박스가 포함된 프레임들을 제 2 학습 모델 에 입력하면, 객체에 관련된 이벤트 발생 여부를 결정할 수 있다. 실시예에 있어서, 제 2 학습 모델은, RNN을 이용한 것인데, 서로 다른 시간 구간에서 노드들 간에 재귀적 (recurrent)인 연결이 있는 신경망을 RNN(Recurrent Neural Network)이라고 한다. 실시예에 있어서 RNN은, 순 차적 데이터(sequential data)를 인식할 수 있다. 학습 데이터와 그에 대응하는 출력 데이터를 함께 신경망에 입력하고, 학습 데이터에 대응하는 출력 데이터가 출력되도록 연결선들의 연결 가중치를 업데이트하는 지도 학습(supervised learning)을 통해 RNN을 학습시킬 수 있다. 예를 들어, RNN은 델타 규칙(delta rule)과 오류 역전파 학습(backpropagation learning) 등을 통해 뉴 런들 사이의 연결 가중치를 업데이트할 수 있다. 예를 들어, 제 2 학습 모델은 이전 프레임에서보다 다음 프레임에서 주행 차량에 가깝게 위치한 객체(80 1)를 나타내는 바운딩 박스를 인식함으로써, 객체와 주행 차량 간에 충돌이 발생할 수 있다고 결정할 수 있다. 실시예에 있어서, 제 2 학습 모델은 바운딩 박스를 시계열적으로 트래킹함으로써(바운딩 박스에 대한 시계 열적 변화를 분석함으로써), 객체로부터 야기될 수 있는 이벤트 발생 확률을 예측할 수 있다. 예를 들어, 제 2 학습 모델은 객체의 위치로부터 판단한 차량과의 거리에 따라 사고발생확률을 결정할 수 있다. 객체 와 차량과의 거리가 멀다고 판단한 경우, 블록 802에서와 같이 사고발생확률이 10%라고 예측할 수 있다. 제 2 학습 모델은 시간의 흐름에 따라 차량 및 객체가 이동하여 차량과 객체의 거리가 가까워졌 다고 판단한 경우, 블록에서와 같이 사고발생확률이 64%라고 판단할 수 있다. 일 개시에 의하여, 시간의흐름에 따른 차량 및 객체의 이동으로 발생하는 사고발생확률은 제 2 학습모델에 의하여 학습될 수 있다. 실시예에 있어서, 프로세싱 회로(1000a, 1000b)는 RNN(Recurrent Neural Network)을 이용하여, 복수의 학습 프 레임들 내에서의 학습 객체를 나타내는 바운딩 박스를 시계열적으로 트래킹하여 학습 차량의 주행에 관련된 이 벤트가 발생하였는지를 학습한 결과에 기초하여 제 2 학습 모델을 획득하고, 상기 획득한 제 2 학습 모델을 이 용하여, 상기 객체와 관련된 상기 차량의 주행에 관련된 이벤트 발생 여부를 결정할 수 있다. 도 27은 본 발명의 실시예들에 따른 전자 장치의 구성을 나타내는 블록도이다. 도 27을 참조하면, 전자 장치는 프로세싱 회로, 통신부, 및 메모리를 포함할 수 있다. 전자 장치는 사용자 입력부, 출력부, 센싱부, 및 A/V 입력부를 더 포함할 수도 있다. 사용자 입력부는, 차량에 설치된 모듈의 동작을 제어하는 사용자 입력하기 위한 사용자 입력을 수신할 수 있다. 출력부는, 오디오 신호 또는 비디오 신호 또는 진동 신호를 출력할 수 있으며, 출력부는 디스플레 이부, 음향 출력부, 및 진동 모터를 포함할 수 있다. 일 실시예에서, 출력부는 오디오, 비디오, 및/또는 진동 형태로 알림 메시지를 출력할 수 있다. 디스플레이부는 전자 장치에서 처리되는 정보를 표시 출력한다. 예를 들어, 디스플레이부는, 차량의 HUD(Head up display)에 알림 메시지를 디스플레이할 수 있다. 음향 출력부는 통신부로부터 수신되거나 메모리에 저장된 오디오 데이터를 출력한다. 또한, 음향 출력부는 전자 장치에서 수행되는 기능(예를 들어, 호신호 수신음, 메시지 수신음, 알림음)과 관련된 음향 신호를 출력한다. 예를 들어, 음향 출력부는 이벤트가 발생하였음을 알리기 위한 경보음을 출력할 수 있다. 프로세싱 회로는, 통상적으로 전자 장치의 전반적인 동작을 제어한다. 예를 들어, 프로세싱 회로 , 메모리에 저장된 프로그램들을 실행함으로써, 사용자 입력부, 출력부, 센싱부 , 통신부, A/V 입력부 등을 전반적으로 제어할 수 있다. 또한, 프로세싱 회로는 메모 리에 저장된 프로그램들을 실행함으로써, 전자 장치의 기능을 수행할 수 있다. 프로세싱 회로(100 0)는 적어도 하나의 프로세서를 구비할 수 있다. 프로세싱 회로는 그 기능 및 역할에 따라, 복수의 프로 세서들을 포함하거나, 통합된 형태의 하나의 프로세서를 포함할 수 있다. 일 실시예에서, 프로세싱 회로 는 메모리에 저장된 적어도 하나의 프로그램을 실행함으로써 알림 메시지를 제공하도록 하는 적어도 하나 의 프로세서를 포함할 수 있다. 프로세싱 회로는 통신부를 통하여 차량에 설치된 카메라로부터 복 수의 프레임(frame)들을 포함하는 비디오 시퀀스를 획득할 수 있다. 일 실시예에서, 프로세싱 회로는 통 신부를 통하여 이벤트의 종류 및 주행의 위험도에 기초하여, 차량에 설치된 모듈의 동작을 제어하기 위한 명령을 차량에 설치된 모듈에 게 전송할 수 있다. 센싱부는, 전자 장치의 상태, 사용자의 상태 또는 전자 장치 주변의 상태를 감지하고, 감지된 정보를 프로세싱 회로로 전달할 수 있다. 센싱부는, 지자기 센서(Magnetic sensor), 가속도 센서(Acceleration sensor), 온/습도 센 서, 적외선 센서, 자이로스코프 센서, 위치 센서(예컨대, GPS), 기압 센서, 근 접 센서, 및 RGB 센서(RGB sensor) 중 적어도 하나를 포함할 수 있다. 통신부는, 전자 장치가 다른 전자 장치(미도시) 및 서버(미도시)와 통신을 하게 하는 하나 이상의 구성요소를 포함할 수 있다. 다른 전자 장치(미도시)는 컴퓨팅 장치이거나, 센싱 장치일 수 있으나, 이에 제한 되지 않는다. 또한, 예를 들어, 다른 전자 장치는, 전자 장치와 같이 차량에 포함된 모듈일 수 있다. 예를 들어, 통신부는, 근거리 통신부, 이동 통신부, 방송 수신부를 포함할 수 있다. 근거리 통신부(short-range wireless communication unit)는, 블루투스 통신부, BLE(Bluetooth Low Energy) 통신부, 근거리 무선 통신부(Near Field Communication unit), WLAN(와이파이) 통신부, 지그비 (Zigbee) 통신부, 적외선(IrDA, infrared Data Association) 통신부, WFD(Wi-Fi Direct) 통신부, UWB(ultra wideband) 통신부, Ant+ 통신부 등을 포함할 수 있다. 일 실시예에서, 통신부는 차량에 설치된 카메라로부터 복수의 프레임(frame)들을 포함하는 비디오 시퀀스 를 수신할 수 있다. 일 실시예에서, 통신부는 차량에 설치된 모듈의 동작을 제어하기 위한 명령을 차량에설치된 모듈에게 전송할 수 있다. A/V(Audio/Video) 입력부는 오디오 신호 또는 비디오 신호 입력을 위한 것으로, 이에는 카메라와 마이크로폰 등이 포함될 수 있다. 카메라는 화상 통화 모드 또는 촬영 모드에서 이미지 센서를 통 해 정지영상 또는 동영상 등의 화상 프레임을 얻을 수 있다. 이미지 센서를 통해 캡쳐된 이미지는 프로세싱 회 로 또는 별도의 이미지 처리부(미도시)를 통해 처리될 수 있다. 예를 들어, 카메라에 의해 촬영된 이미지는 이벤트가 발생하였는지를 결정하기 위한 정보로 활용될 수 있다. 마이크로폰은, 외부의 음향 신호를 입력 받아 전기적인 음성 데이터로 처리한다. 예를 들어, 마이크로폰 은 외부 전자 장치 또는 사용자로부터 음향 신호를 수신할 수 있다. 마이크로폰은 외부의 음향 신 호를 입력 받는 과정에서 발생 되는 잡음(noise)을 제거하기 위한 다양한 잡음 제거 알고리즘을 이용할 수 있다. 메모리는, 프로세싱 회로의 처리 및 제어를 위한 프로그램을 저장할 수 있고, 전자 장치로 입 력되거나 전자 장치로부터 출력되는 데이터를 저장할 수도 있다. 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 메모리 에 저장된 프로그램들은 그 기능에 따라 복수 개의 모듈들로 분류할 수 있는데, 예를 들어, UI 모듈 , 터치 스크린 모듈, 알림 모듈 등으로 분류될 수 있다. UI 모듈은, 애플리케이션 별로 전자 장치와 연동되는 특화된 UI, GUI 등을 제공할 수 있다. 터치 스 크린 모듈은 사용자의 터치 스크린 상의 터치 제스처를 감지하고, 터치 제스처에 관한 정보를 프로세싱 회로로 전달할 수 있다. 일 실시예에 따른 터치 스크린 모듈은 터치 코드를 인식하고 분석할 수 있 다. 터치 스크린 모듈은 컨트롤러를 포함하는 별도의 하드웨어로 구성될 수도 있다. 알림 모듈은 이벤트의 발생을 알리기 위한 신호를 발생할 수 있다. 알림 모듈은 디스플레이부 를 통해 비디오 신호 형태로 알림 신호를 출력할 수도 있고, 음향 출력부를 통해 오디오 신호 형태 로 알림 신호를 출력할 수도 있고, 진동 모터를 통해 진동 신호 형태로 알림 신호를 출력할 수도 있다. 이상에서 설명된 실시예들은 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨 어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치, 방법 및 구성요소는, 예를 들 어, 프로세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마 이크로컴퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령(instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터 를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로"}
{"patent_id": "10-2019-0058048", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "설명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 산업상 이용가능성 본 발명의 실시예들은 인공 신경망을 사용하여 객체를 검출하는 첨단 운전자 지원 장치에 제공될 수 있다."}
{"patent_id": "10-2019-0058048", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "상기에서는 본 발명의 실시예들을 참조하여 설명하였지만, 해당 기술분야에서 통상의 지식을 가진 자는 하기의 특허청구범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 것이다.도면 도면1 도면2 도면3a 도면3b 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15a 도면15b 도면16 도면17 도면18 도면19 도면20 도면21 도면22 도면23 도면24 도면25 도면26 도면27"}
{"patent_id": "10-2019-0058048", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예들에 따른 첨단 운전자 지원 장치가 차량 전방의 객체를 검출하여 이벤트 발생 여부를 결정하는 동작의 예시를 나타낸다. 도 2는 본 발명의 실시예들에 따른 첨단 운전자 지원 장치의 예를 나타내는 블록도이다. 도 3a 및 도 3b는 도 2에서 제1 카메라) 및 제2 카메라의 위치에 따른 제1 시점 이미지와 제2 시점 이미지에서 객체들을 나타낸다. 도 4는 도 2의 제2 센서에서 획득환 제1 센싱 데이터에 기초하여 생성되는 객체 트래킹 리스트(object tracking list)를 나타낸다. 도 5는 본 발명의 실시예들에 따른 도 2의 첨단 운전자 지원 장치에서 프로세싱 회로의 구성을 나타내는 블록도 이다. 도 6은 본 발명의 실시예들에 따른 도 5의 프로세싱 회로에서 객체 인식 엔진을 나타내는 블록도이다. 도 7은 도 5의 프로세싱 회로에서 디스패리티 이미지를 나타내고, 도 8은 도 5의 프로세싱 회로에서 마스크를 나타내고, 도 9는 도 5의 프로세싱 회로의 상관 관계 산출 엔진이 디스패리티 이미지와 마스크를 결합한 것을 나타내고, 도 10은 도 5의 깊이 이미지 생성 엔진에서 제공되는 깊이 이미지를 나타낸다. 도 11 및 도 12는 도 5의 프로세싱 회로의 객체 검출 모듈이 제공하는 최종 이미지를 나타낸다. 도 13은 도 6의 객체 검출 엔진의 동작을 나타낸다. 도 14는 도 13의 객체 검출 엔진에서 박스 예측기를 나타낸다. 도 15a는 도 14의 박스 예측기에서 특징 피라미드 망을 나타낸다. 도 15b는 본 발명의 실시예들에 따른 도 15a의 특징 피라미드 망에서 병합 블록을 나타낸다. 도 16은 본 발명의 실시예들에 따른 도 2의 첨단 운전자 지원 장치의 동작을 나타내는 흐름도이다. 도 17은 본 발명의 실시예들에 따른 첨단 운전자 지원 장치의 다른 예를 나타내는 블록도이다. 도 18은 본 발명의 실시예들에 따른 도 17의 첨단 운전자 지원 장치에서 프로세싱 회로의 구성을 나타내는 블록 도이다. 도 19는 본 발명의 실시예들에 따른 상관 산출 모듈을 나타내는 블록도이다. 도 20은 도 17의 제2 센서에서 제공되는 포인트 클라우드 이미지를 나타낸다. 도 21은 도 19의 상관 관계 산출 엔진이 디스패리티 이미지, 마스크 및 공간적 포인트 클라우드 데이터를 결합 한 것을 나타낸다. 도 22는 본 발명의 실시예들에 따른 프로세싱 회로가 스테레오 이미지, 제1 센싱 데이터 및 제2 센싱 데이터를 동기시키는 것을 나타낸다. 도 23은 본 발명의 실시예들에 따른 첨단 운전자 지원 장치가 이벤트 발생 여부를 결정하는 방법을 나타내는 흐 름도이다. 도 24는 본 발명의 실시예들에 따른 차량의 주행에 관련된 이벤트 발생 여부를 결정하는 학습모델의 생성을 설 명하기 위한 도면이다. 도 25는 본 발명의 실시예들에 따른 제1 학습 모델을 이용하여 객체를 검출하는 예시를 설명하기 위한 도면이다. 도 26은 본 발명의 실시예들에 따라 제 2 학습 모델을 이용하여 객체의 시계열적 이동에 따른 이벤트 발생 여부 를 결정하는 것을 나타낸다. 도 27은 본 발명의 실시예들에 따른 전자 장치의 구성을 나타내는 블록도이다."}
