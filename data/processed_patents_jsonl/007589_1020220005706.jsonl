{"patent_id": "10-2022-0005706", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0109897", "출원번호": "10-2022-0005706", "발명의 명칭": "딥러닝 기반 구음 장애 시각화 및 재활을 수행하는 장치, 제어 방법 및 프로그램", "출원인": "한림대학교 산학협력단", "발명자": "이은주"}}
{"patent_id": "10-2022-0005706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 특징 벡터에 대한 차원 축소 및 군집화를 수행하는 인공지능 모델이 저장된 메모리; 및상기 메모리와 연결된 프로세서;를 포함하고,상기 프로세서는,사용자의 음성에 대응되는 오디오 데이터를 획득하고,상기 오디오 데이터로부터 복수의 특징 벡터를 추출하고,상기 추출된 복수의 특징 벡터를 상기 인공지능 모델에 입력하고,상기 인공지능 모델의 출력을 기반으로 군집화된 특징 벡터들이 2차원 공간 상에 매핑(mapping)된 이미지를 획득하는, 전자 장치."}
{"patent_id": "10-2022-0005706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 인공지능 모델은,상기 복수의 특징 벡터 중 적어도 하나의 구음 장애 유형의 분류와 관련된 특징 벡터들을 선택하고,상기 선택된 특징 벡터들을 적어도 하나의 구음 장애 유형에 따라 군집화하는, 전자 장치."}
{"patent_id": "10-2022-0005706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 프로세서는,일 텍스트를 발화하는 정상인의 음성에 대응되는 제1 오디오 데이터와 상기 텍스트를 발화하는 환자의 음성에대응되는 제2 오디오 데이터를 비교하고,상기 제1 오디오 데이터와 상기 제2 오디오 데이터 각각의 복수의 특징 벡터를 비교한 결과, 임계치 이상의 차이를 갖는 적어도 하나의 특징 벡터를 식별하고,상기 식별된 특징 벡터를 바탕으로, 상기 인공지능 모델을 훈련시키는, 전자 장치."}
{"patent_id": "10-2022-0005706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 인공지능 모델은,상기 복수의 특징 벡터 중 적어도 하나를 바탕으로 적어도 하나의 구음 장애 유형의 분류와 관련된 특징 벡터들을 생성하고,상기 생성된 특징 벡터들을 군집화하는, 전자 장치."}
{"patent_id": "10-2022-0005706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 프로세서는,구음 장애 유형 별로 매핑된 상기 군집화된 특징 벡터들의 수치를 기반으로, 서로 다른 오디오 특성이 별도의영역으로 표출되도록 2차원 공간 상의 복수의 영역을 포함하는 이미지를 생성하고,공개특허 10-2023-0109897-3-상기 복수의 영역 중 상기 사용자의 음성의 오디오 특성을 나타내는 영역이 구분되도록 표시하는, 전자 장치."}
{"patent_id": "10-2022-0005706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서,상기 복수의 영역은,오디오 특성 간의 차이가 적을수록 영역 간의 거리가 가깝게 설정되고,동일한 구음 장애 유형을 나타내는 오디오 특성은 기설정된 범위 내의 영역에 위치하도록 설정되는, 전자 장치."}
{"patent_id": "10-2022-0005706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 프로세서는,복수의 재활 텍스트 각각에 대해, 상기 사용자에 대한 구음 장애 재활을 수행한 횟수를 식별하고,상기 복수의 재활 텍스트 각각에 대해, 마지막으로 수행된 재활에 따라 입력된 상기 사용자의 음성의 오디오 특성을 정상인의 오디오 특성과 비교하여 최종 일치율을 식별하고,상기 복수의 재활 텍스트 중 제1 텍스트에 대한 상기 사용자의 재활이 수행된 결과 최종 일치율이 임계치를 초과하고, 재활 횟수가 일정 횟수 이하인 경우, 상기 복수의 재활 텍스트 중 상기 제1 텍스트와 유사한 적어도 하나의 제2 텍스트를 획득하고,상기 복수의 재활 텍스트 중 제3 텍스트에 대한 상기 사용자의 재활이 수행된 결과 최종 일치율이 임계치를 초과하고, 재활 횟수가 일정 횟수를 초과하는 경우, 상기 복수의 재활 텍스트 중, 상기 제3 텍스트 와 유사한 적어도 하나의 제4 텍스트를 획득하고,상기 제2 텍스트 및 상기 제4 텍스트 중, 상기 제4 텍스트에 대한 상기 사용자의 재활을 우선적으로 수행하는,전자 장치."}
{"patent_id": "10-2022-0005706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "적어도 하나의 특징 벡터에 대한 차원 축소 및 군집화를 수행하는 인공지능 모델이 저장된 전자 장치의 제어 방법에 있어서,서버가, 사용자 단말로부터 사용자의 음성에 대응되는 오디오 데이터를 획득하는 단계;상기 서버가, 상기 오디오 데이터로부터 복수의 특징 벡터를 추출하는 단계;상기 서버가, 상기 복수의 특징 벡터를 상기 인공지능 모델에 입력하는 단계;상기 서버가, 상기 인공지능 모델의 출력을 기반으로 군집화된 특징 벡터들이 2차원 공간 상에 매핑(mapping)된이미지를 획득하는 단계; 및상기 서버가, 상기 이미지를 상기 사용자 단말을 통해 출력하는 단계;를 포함하는, 전자 장치의 제어 방법."}
{"patent_id": "10-2022-0005706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 전자 장치의 제어 방법은,상기 서버가, 일 텍스트를 발화하는 정상인의 음성에 대응되는 제1 오디오 데이터를 획득하는 단계;상기 서버가, 상기 텍스트를 발화하는 환자의 음성에 대응되는 제2 오디오 데이터를 획득하는 단계;상기 서버가, 상기 제1 오디오 데이터와 상기 제2 오디오 데이터 각각의 복수의 특징 벡터를 비교하여, 임계치이상의 차이를 갖는 적어도 하나의 특징 벡터를 식별하는 단계; 및상기 서버가, 상기 식별된 특징 벡터를 바탕으로, 상기 인공지능 모델을 훈련시키는 단계;를 포함하는, 전자 장치의 제어 방법.공개특허 10-2023-0109897-4-청구항 10 하드웨어인 컴퓨터와 결합되어, 제8 항의 방법을 수행할 수 있도록 컴퓨터에서 독출가능한 기록매체에 저장된컴퓨터 프로그램."}
{"patent_id": "10-2022-0005706", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은, 사용자의 음성에 대응되는 오디오 데이터를 획득하고, 오디오 데이터로부터 복수의 특징 벡터를 추출 하고, 추출된 복수의 특징 벡터 중 적어도 하나의 특징 벡터를 인공지능 모델에 입력하여, 인공지능 모델의 출력 을 기반으로 특징 벡터들이 2차원 공간 상에 매핑(mapping)된 이미지를 획득하는, 딥러닝 기반 구음 장애 시각화 및 재활을 수행하는 전자 장치, 전자 장치의 제어 방법, 및 전자 장치의 제어 방법을 수행하는 컴퓨터 프로그램 에 관한 것이다."}
{"patent_id": "10-2022-0005706", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 딥러닝 기반 구음 장애 시각화 및 재활을 수행하기 위해, 딥러닝을 통해 인공지능 모델을 학습시켜, 사용자의 음성이 입력되면 사용자의 구음 장애 정도를 이미지로써 시각화하여 사용자에게 제공하고, 실시간 사 용자 음성에 따른 이미지를 제공하여, 구음 장애에 대한 재활을 돕는 전자 장치, 전자 장치의 제어 방법, 및 전 자 장치의 제어 방법을 수행하는 컴퓨터 프로그램에 관한 것이다."}
{"patent_id": "10-2022-0005706", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래의 구음 장애 평가는 온라인 또는 오프라인 상에 주어지는 언어 능력 테스트 수행 및 설문지의 작성을 통해 이루어 졌으며, 이는 사용자의 스스로에 대한 평가를 통해 응답되는 것으로 객관화되지 못하여, 정확한 구음 장 애 정도를 도출하지 못하는 문제점이 있어 왔다. 또한, 특허문헌 1에서 개시하는 바와 같이, 복잡한 절차에 따른 언어 능력 테스트 수행 및 설문지의 작성은 시 간적 제약이 존재하며, 사용자가 지쳐, 도중에 구음 장애 평가를 포기하는 문제가 발생해 왔다."}
{"patent_id": "10-2022-0005706", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "한편, 상기의 배경기술로서 설명된 사항들은 본 발명의 배경에 대한 이해 증진을 위한 것일 뿐, 이 기술분야에 서 통상의 지식을 가진 자에게 이미 알려진 종래기술에 해당함을 인정하는 것으로 받아들여져서는 안 될 것이다. 선행기술문헌 특허문헌 (특허문헌 0001) 등록특허공보 제10-1921890호, 2018.11.20."}
{"patent_id": "10-2022-0005706", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는 입력된 사용자의 음성을 바탕으로 오디오 데이터를 획득하여, 인공지능 모델 을 통해 오디오 데이터에 매칭되는 구음 장애 정도를 판단하고, 이를 시각화하는 딥러닝 기반 구음 장애 시각화 및 재활 기술을 제공하는 것이다. 또한 본 발명은, 한국지능정보사회진흥원, 인공지능 학습용 데이터 구축사업의 구음장애 음성 데이터(과제고유 번호 21002131305088212100101BS)로 2021.06.01부터 2021.12.31의 기간에 연구된 기술이다. 본 발명이 해결하고자 하는 과제들은 이상에서 언급된 과제로 제한되지 않으며, 언급되지 않은 또 다른 과제들 은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0005706", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 과제를 해결하기 위한 본 발명의 일 면에 따른 적어도 하나의 특징 벡터에 대한 차원 축소 및 군집화를 수행하는 인공지능 모델이 저장된 메모리, 및 메모리와 연결된 프로세서를 포함한다. 이때, 프로세서는, 사용자의 음성에 대응되는 오디오 데이터를 획득하고, 오디오 데이터로부터 복수의 특징 벡 터를 추출하고, 추출된 복수의 특징 벡터를 인공지능 모델에 입력하고, 인공지능 모델의 출력을 기반으로 군집 화된 특징 벡터들이 2차원 공간 상에 매핑(mapping)된 이미지를 획득할 수 있다. 한편, 인공지능 모델은, 복수의 특징 벡터 중 적어도 하나의 구음 장애 유형의 분류와 관련된 특징 벡터들을 선 택하고, 선택된 특징 벡터들을 적어도 하나의 구음 장애 유형에 따라 군집화할 수 있다. 또는, 인공지능 모델은, 복수의 특징 벡터 중 적어도 하나를 바탕으로 적어도 하나의 구음 장애 유형의 분류와 관련된 특징 벡터들을 생성하고, 생성된 특징 벡터들을 군집화할 수 있다. 한편, 프로세서는, 일 텍스트를 발화하는 정상인의 음성에 대응되는 제1 오디오 데이터와 텍스트를 발화하는 환 자의 음성에 대응되는 제2 오디오 데이터를 비교하고, 제1 오디오 데이터와 제2 오디오 데이터 각각의 복수의 특징 벡터를 비교한 결과, 임계치 이상의 차이를 갖는 적어도 하나의 특징 벡터를 식별하고, 식별된 특징 벡터 를 바탕으로, 인공지능 모델을 훈련시키는 것이 가능하다. 한편, 프로세서는, 구음 장애 유형 별로 매핑된 군집화된 특징 벡터들의 수치를 기반으로, 서로 다른 오디오 특 성이 별도의 영역으로 표출되도록 2차원 공간 상의 복수의 영역을 포함하는 이미지를 생성하고, 복수의 영역 중 사용자의 음성의 오디오 특성을 나타내는 영역이 구분되도록 표시할 수 있다. 이때, 복수의 영역은, 오디오 특성 간의 차이가 적을수록 영역 간의 거리가 가깝게 설정되고, 동일한 구음 장애 유형을 나타내는 오디오 특성은 기설정된 범위 내의 영역에 위치하도록 설정된다. 한편, 프로세서는, 복수의 재활 텍스트 각각에 대해, 사용자에 대한 구음 장애 재활을 수행한 횟수를 식별하고, 복수의 재활 텍스트 각각에 대해, 마지막으로 수행된 재활에 따라 입력된 사용자의 음성의 오디오 특성을 정상 인의 오디오 특성과 비교하여 최종 일치율을 식별할 수 있다. 이때, 복수의 재활 텍스트 중 제1 텍스트에 대한 사용자의 재활이 수행된 결과 최종 일치율이 임계치를 초과하 고, 재활 횟수가 일정 횟수 이하인 경우, 프로세서는, 복수의 재활 텍스트 중 제1 텍스트와 유사한 적어도 하나 의 제2 텍스트를 획득한다. 또한, 복수의 재활 텍스트 중 제3 텍스트에 대한 사용자의 재활이 수행된 결과 최종 일치율이 임계치를 초과하 고, 재활 횟수가 일정 횟수를 초과하는 경우, 프로세서는, 복수의 재활 텍스트 중, 제3 텍스트 와 유사한 적어 도 하나의 제4 텍스트를 획득한다. 이에 따라, 프로세서는, 제2 텍스트 및 제4 텍스트 중, 제4 텍스트에 대한 사용자의 재활을 우선적으로 수행한 다. 한편, 본 발명의 이 면에 따른, 적어도 하나의 특징 벡터에 대한 차원 축소 및 군집화를 수행하는 인공지능 모 델이 저장된 전자 장치의 제어 방법에 있어서, 서버가, 사용자 단말로부터 사용자의 음성에 대응되는 오디오 데 이터를 획득하는 단계, 서버가, 오디오 데이터로부터 복수의 특징 벡터를 추출하는 단계, 서버가, 복수의 특징 벡터를 인공지능 모델에 입력하는 단계, 서버가, 인공지능 모델의 출력을 기반으로 군집화된 특징 벡터들이 2차 원 공간 상에 매핑(mapping)된 이미지를 획득하는 단계, 및 서버가, 이미지를 사용자 단말을 통해 출력하는 단 계를 포함할 수 있다. 이때, 전자 장치의 제어 방법은, 서버가, 일 텍스트를 발화하는 정상인의 음성에 대응되는 제1 오디오 데이터를 획득하는 단계, 서버가, 텍스트를 발화하는 환자의 음성에 대응되는 제2 오디오 데이터를 획득하는 단계, 서버 가, 제1 오디오 데이터와 제2 오디오 데이터 각각의 복수의 특징 벡터를 비교하여, 임계치 이상의 차이를 갖는 적어도 하나의 특징 벡터를 식별하는 단계, 및 서버가, 식별된 특징 벡터를 바탕으로, 인공지능 모델을 훈련시 키는 단계를 포함할 수 있다. 한편, 본 발명은, 하드웨어인 컴퓨터와 결합되어, 본 발명의 이 면에 따른 전자 장치의 제어 방법을 수행할 수 있도록 컴퓨터에서 독출가능한 기록매체에 저장된 컴퓨터 프로그램를 더 포함할 수 있다. 본 발명의 기타 구체적인 사항들은 상세한 설명 및 도면들에 포함되어 있다."}
{"patent_id": "10-2022-0005706", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 딥러닝 기반 구음 장애 시각화 및 재활 기술에 의하면, 사용자의 음성 입력만으로도 사용자가 갖는 구음 장애 정도를 판단하여 이미지로써 시각화하여 제공함으로써, 시간 및 공간에 따른 제약에서 자유로운 구음 장애 정도 파악 및 재활 서비스에 의한, 사용자 만족감 증대의 효과가 발생한다."}
{"patent_id": "10-2022-0005706", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과들은 이상에서 언급된 효과로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재로 부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0005706", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나, 본 발명은 이하에서 개시되는 실시예들에 제한되는 것이 아니라 서 로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하고, 본 발명이 속하는 기술 분야의 통상의 기술자에게 본 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명 세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한다 (comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성요소 외에 하나 이상의 다른 구성요소의 존재 또는 추가를 배제하지 않는다. 명세서 전체에 걸쳐 동일한 도면 부호는 동일한 구성 요소를 지칭하며, \"및/또는\"은 언급된 구성요소들의 각각 및 하나 이상의 모든 조합을 포함한다. 비록 \"제1\", \"제2\" 등이 다양한 구성요소들을 서술하기 위해서 사용되나, 이들 구성요소들은 이들 용어에 의해 제한되지 않음은 물론이다. 이들 용어들은 단 지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사용하는 것이다. 따라서, 이하에서 언급되는 제1 구성 요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있음은 물론이다. 다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어(기술 및 과학적 용어를 포함)는 본 발명이 속하는 기술 분야의 통상의 기술자에게 공통적으로 이해될 수 있는 의미로 사용될 수 있을 것이다. 또한, 일반적으로 사용되 는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 명세서에서 사용되는 \"부\" 또는 “모듈”이라는 용어는 소프트웨어, FPGA 또는 ASIC과 같은 하드웨어 구성요소 를 의미하며, \"부\" 또는 “모듈”은 어떤 역할들을 수행한다. 그렇지만 \"부\" 또는 “모듈”은 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. \"부\" 또는 “모듈”은 어드레싱할 수 있는 저장 매체에 있도록 구성될 수 도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 \"부\" 또는 “모 듈”은 소프트웨어 구성요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라 이버들, 펌웨어, 마이크로 코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들 및 변수들을 포함한다. 구성요소들과 \"부\" 또는 “모듈”들 안에서 제공되는 기능은 더 작은 수의 구성요소들 및 \"부\" 또는 “모듈”들로 결합되거나 추가적인 구성요소들과 \"부\" 또는 “모듈”들로 더 분리될 수 있다. 공간적으로 상대적인 용어인 \"아래(below)\", \"아래(beneath)\", \"하부(lower)\", \"위(above)\", \"상부(upper)\" 등 은 도면에 도시되어 있는 바와 같이 하나의 구성요소와 다른 구성요소들과의 상관관계를 용이하게 기술하기 위 해 사용될 수 있다. 공간적으로 상대적인 용어는 도면에 도시되어 있는 방향에 더하여 사용시 또는 동작시 구성 요소들의 서로 다른 방향을 포함하는 용어로 이해되어야 한다. 예를 들어, 도면에 도시되어 있는 구성요소를 뒤 집을 경우, 다른 구성요소의 \"아래(below)\"또는 \"아래(beneath)\"로 기술된 구성요소는 다른 구성요소의 \"위(above)\"에 놓일 수 있다. 따라서, 예시적인 용어인 \"아래\"는 아래와 위의 방향을 모두 포함할 수 있다. 구성요 소는 다른 방향으로도 배향될 수 있으며, 이에 따라 공간적으로 상대적인 용어들은 배향에 따라 해석될 수 있다. 본 명세서에서, 컴퓨터는 적어도 하나의 프로세서를 포함하는 모든 종류의 하드웨어 장치를 의미하는 것이고, 실시 예에 따라 해당 하드웨어 장치에서 동작하는 소프트웨어적 구성도 포괄하는 의미로서 이해될 수 있다. 예 를 들어, 컴퓨터는 스마트폰, 태블릿 PC, 데스크톱, 노트북 및 각 장치에서 구동되는 사용자 클라이언트 및 애 플리케이션을 모두 포함하는 의미로서 이해될 수 있으며, 또한 이에 제한되는 것은 아니다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예를 상세하게 설명한다. 도 1은 본 발명의 일 실시예에 따른 전자 장치의 구성도이고, 도 2는 본 발명의 일 실시예에 따른 구음 장 애 정도 시각화의 기본 흐름도이다. 일 실시예로, 전자 장치는 스마트폰(smartphone), 태블릿 PC(tablet personal computer), 이동 전화기 (mobile phone), 영상 전화기, 전자책 리더기(e-book reader), 데스크탑 PC (desktop PC), 랩탑 PC(laptop PC), 넷북 컴퓨터(netbook computer), 워크스테이션(workstation), 서버, PDA(personal digital assistant), PMP(portable multimedia player), MP3 플레이어, 모바일 의료기기, 카메라, 또는 웨어러블 장치(wearable device), 인공지능 스피커(AI speaker) 중 적어도 하나를 포함할 수 있다. 본 발명의 일 면에 따른 딥러닝 기반 구음 장애 시각화 및 재활을 위한 전자 장치는, 적어도 하나의 특징 벡터에 대한 차원 축소 및 군집화를 수행하는 인공지능 모델이 저장된 메모리, 및 메모리와 연 결된 프로세서를 포함한다. 구음 장애는, 후천적 또는 선천적인 요인에 의한 장애 및/또는 질병에 의해 발성 기관에 생긴 기능 이상으로 말 하기 어려운 상태가 된 것을 뜻하며, 구음 장애 유형으로는, 구마비, 가성구마비, 청각 장애, 후두 장애, 언어 장애, 파킨슨병, 치매 등이 포함된다. 이때, 도 2에 도시된 바와 같이, 프로세서는, 사용자의 음성에 대응되는 오디오 데이터를 획득하고, 오디 오 데이터로부터 복수의 특징 벡터를 추출하고, 추출된 복수의 특징 벡터를 인공지능 모델에 입력하고, 인 공지능 모델의 출력을 기반으로 군집화된 특징 벡터들이 2차원 공간 상에 매핑(mapping)된 이미지를 획득 할 수 있다. 이때, 음성에 대응되는 오디오 데이터란, 사용자의 음성으로부터 추출되는 디지털화된 음원이다. 한편, 복수의 특징 벡터를 추출하기 위하여, 획득된 오디오 데이터는, 프로세서에 의해, 노이즈 필터링 (noise filtering) 작업을 거치며, 프로세서는 필터링된 오디오 데이터를 복수의 구간으로 구간화하되, 인 접한 구간 간에 일정 시간만큼 중첩하여 프레이밍(framing)하고, 프레이밍된 오디오 데이터로부터 복수의 음향 (aqoustic) 특징 벡터 및 복수의 MFCC(Mel Frequency Cepstral Coefficients) 특징 벡터를 추출하며, 복수의 특징 벡터는, 복수의 음향 특징 벡터 및 복수의 MFCC 특징 벡터를 포함한다. 이때, 복수의 음향 특징 벡터는, meanF0Hz, stdevF0Hz, meanF1Hz, stdevF1Hz, meanF2Hz, stdevF2Hz, HNR, localjitter, localabsolutejitter, rapjitter, ppq5jitter, ddpjitter, localShimmer, localdbShimmer, apq3Shimmer, apq5Shimmer, apq11Shimmer, ddaShimmer, pitch_max 및 pich_min 중 적어도 하나를 포함하여, 프 레이밍된 오디오 데이터로부터 프로세서에 의해 추출될 수 있다. 이때, 복수의 MFCC 특징 벡터는, 적어도 하나의 제1 MFCC 특징 벡터, 적어도 하나의 제2 MFCC 특징 벡터, 및 적 어도 하나의 제3 MFCC 특징 벡터를 포함한다. 제1 MFCC 특징 벡터는, 프로세서가 프레이밍된 오디오 데이터에 MFCC 기법을 적용함으로써 최대 13개의 벡 터로 추출된다. 제2 MFCC 특징 벡터는, 프로세서에 의해, 적어도 하나의 제1 MFCC 특징 벡터의 값을 미분하여 추출되며, 제3 MFCC 특징 벡터는, 프로세서에 의해, 적어도 하나의 제2 MFCC 특징 벡터의 값을 미분하여 추출될 수 있다. 이에 따라, 프로세서는, 음향 특징 벡터 최대 20개, MFCC 특징 벡터 최대 78개(각 프레임 마다, 제1 MFCC 특징 벡터의 최대 개수인 13개, 1회 미분으로 인한 최대 개수인 13개, 2회 미분으로 인한 최대 개수인 13 개, 총 13*3=39개를 구한 다음 이들 각각을 모든 프레임에 대하여 구한 평균 및 표준편차를 구하여 특징 벡터로 사용, 따라서 하나의 오디오 데이터에 대하여 최대 13*3*2=78개의 MFCC 특징 벡터를 추출)를 포함하여 최대 98개의 특징 벡터를 오디오 데이터로부터 추출할 수 있다. 한편, 특징 벡터들을 군집화하는데 있어서, 인공지능 모델은 훈련 및 모델링을 실시하기 위한 분류 모델, 및 군집화 및 시각화를 실시하기 위한 군집화-시각화 모델(훈련 및 모델링)을 포함한다. 이때, 군집화-시각화 모델로는 SOM(Self-Organizing Map) 알고리즘, k-means, gaussian mixture model 등이 사 용될 수 있다. 일 실시예로, 군집화-시각화 모델이 SOM 알고리즘을 사용하는 경우, 인공지능 모델은 사용자 음성이 입력되면 특징 벡터 추출 이후 장애 유형에 대한 판별(분별)과 더불어 SOM 알고리즘을 적용하여 군집화 및 시각화, 즉, SOM 모델링을 실시할 수 있다. 이때, 인공지능 모델은, 분류에 최적인 특징 벡터 중 적어도 하나 이상의 특징 벡터를 선택하여 SOM 모델 링을 실시하는 한편, 사용자의 발화 특성을 고려하여 특정 모음, 음절, 및 단어의 조합을 선택하여 SOM 모델링 을 수행할 수 있다. 이에 따라, SOM 모델링은 개인 맞춤형으로 실시간 업데이트된다. 한편, 인공지능 모델은, SOM 모델링을 실행한 결과, 시각화된 사용자의 오디오 특성 영역이 정상인의 오디 오 특성 영역과 형성하는 거리가 최대가 될 수 있도록 하는 특징 벡터의 조합이나 특정 모음, 음절, 및 단어의 조합을 업데이트할 수 있다. 즉, 사용자에 따라 사용자의 발화 특성과 구음 장애 유형, 구음 장애 유형의 원인 을 명확히 구분할 수 있는 특정 특징 벡터의 조합과 함께 특정 모음, 음절, 및 단어의 조합에 대한 정보를 바탕 으로 실시간 SOM 모델링이 인공지능 모델에 의해 진행된다. 예컨대, 인공지능 스피커가 사용자에 의해 학 습이 되는 것과 같이, 인공지능 모델이 사용자의 오디오 특성에 맞게 학습된다. 이에 따라, 사용자 음성에 의한 오디오 데이터가 입력되면, 이미 사용자에 맞게 학습된 분류 모델에 의해 사용자의 구음장애 유형이 판별 될 수 있다. 한편, 사용자의 발화 특성과 다양한 원인에 따른 구음 장애 유형(다른 원인을 갖더라도 동일한 유형의 구음 장 애가 발생할 수 있음)을 명확히 구분할 수 있도록, 인공지능 모델에 의해 사전 추출된 특징 벡터 중, 사용 자의 발화 특성에 최적화된 특징 벡터 및 사용자의 발화 특성에 최적화된 특정 모음, 음절, 단어들의 조합을 추 출하여 업데이트 하는 과정을 선행한다. 이후, 인공지능 모델은 선행 과정에 따른 결과물을 바탕으로 SOM 모델링을 실행한다. 사음자의 구음 장애를 재활하기 위한 재활모드에서는, 재활을 위한 텍스트 및 텍스트를 사용하는 상황 중 적어 도 하나를 사용자가 선택해서 사용자 단말을 통해 입력할 수 있으며, 텍스트를 사용하는 상황을 사용자 단 말로부터 수신한 경우, 인공지능 모델은 상황에 따라 선택적으로 텍스트 리스트를 생성할 수 있다. 이때, 재활은 시각적으로 표시되고, 가이딩되는 것을 토대로 사용자가 발화를 바이오 피드백적으로 개선할 수 있도록 실시간 SOM 모델링을 통해 구현할 수 있다. 한편, SOM 모델링에 있어서, SOM은 사람이 눈으로 볼 수 있는 저차원(2차원 내 3차원) 격자에 고차원 데이터의 각 개체들이 대응하도록 인공신경망과 유사한 방식의 학습을 통해 군집을 도출해내는 기법이며, 고차원의 데이 터 원공간에서 유사한 개체들은 저차원에 인접한 격자들과 연결될 수 있다. 이때, 저차원 격자에서의 유사도는 고차원 입력 공간에서의 유사도를 최대한 보존하도록 학습된다. 도 3 및 도 4는 본 발명의 일 실시예에 따른 시스템 구성도이다. 도시된 바와 같이, 본 발명의 전자 장치는, 딥러닝 기반 구음 장애 시각화 및 재활을 수행하는데 있어서, 마이크를 포함하여 사용자의 오디오 데이터를 획득하고, 디스플레이를 포함하여 이미지를 출력할 수 있다. 또한, 도 4에 도시된 바와 같이, 본 발명의 전자 장치는, 통신부를 포함하여 사용자 단말과 통 신함으로써, 사용자 단말을 통해 오디오 데이터를 획득하고, 사용자 단말을 통해 이미지를 출력할 수 있다. 이때, 사용자 단말은, 본 발명의 전자 장치를 사용하는 사용자의 단말로, 일 실시예로, 사용자 단말 은, 스마트폰(smartphone), 태블릿 PC(tablet personal computer), 이동 전화기(mobile phone), 영상 전 화기, 전자책 리더기(e-book reader), 데스크탑 PC (desktop PC), 랩탑 PC(laptop PC), 넷북 컴퓨터(netbook computer), 워크스테이션(workstation), 서버, PDA(personal digital assistant), PMP(portable multimedia player), MP3 플레이어, 모바일 의료기기, 카메라, 또는 웨어러블 장치(wearable device), 인공지능 스피커(AIspeaker) 중 적어도 하나를 포함할 수 있다. 한편, 인공지능 모델은, 복수의 특징 벡터 중 적어도 하나의 구음 장애 유형의 분류와 관련된 특징 벡터들 을 선택하고, 선택된 특징 벡터들을 적어도 하나의 구음 장애 유형에 따라 군집화할 수 있다. 또는, 인공지능 모델은, 복수의 특징 벡터 중 적어도 하나를 바탕으로 적어도 하나의 구음 장애 유형의 분 류와 관련된 특징 벡터들을 생성하고, 생성된 특징 벡터들을 군집화할 수 있다. 또는, 인공지능 모델은, 생성된 특징 벡터들 및 복수의 특징 벡터 중 적어도 하나의 구음 장애 유형의 분 류와 관련된 특징 벡터들을 선택하고, 선택된 특징 벡터들을 적어도 하나의 구음 장애 유형에 따라 군집화할 수 있다. 도 5는 본 발명의 일 실시예에 따른 학습 및 이미지 생성에 대한 시스템의 동작 흐름도이다. 도시된 바와 같이, 프로세서가 인공지능 모델을 훈련시키기 위해, 프로세서는, 일 텍스트를 발 화하는 정상인의 음성에 대응되는 제1 오디오 데이터와 텍스트를 발화하는 환자의 음성에 대응되는 제2 오디오 데이터를 비교하고, 제1 오디오 데이터와 제2 오디오 데이터 각각의 복수의 특징 벡터를 비교한 결과, 임계치 이상의 차이를 갖는 적어도 하나의 특징 벡터를 식별하고, 식별된 특징 벡터를 바탕으로, 인공지능 모델을 훈련시킨다. 구체적으로, 프로세서는 일 텍스트에 대해 정상인의 발음을 나타내는 복수의 특징 벡터와 환자(각 구음 장 애 유형에 해당하는 환자)의 발음을 나타내는 복수의 특징 벡터를 비교하여 차이가 일정 값 이상인 적어도 하나 의 특징 벡터를 식별하고, 이를 바탕으로 인공지능 모델을 훈련시킬 수 있다. 그 결과, 인공지능 모델은 적어도 하나의 구음 장애 유형의 분류와 관련된 특징 벡터들을 선택하고, 선택 된 특징 벡터들을 적어도 하나의 구음 장애 유형에 따라 분류/선택/조합하여, 구음 장애 유형에 따라 군집화할 수 있다. 이때, 프로세서는 수집된 사용자의 음성에 대한 오디오 데이터를 학습용 데이터와 예측용 데이터를 일정 비율(ex. 학습용 8: 예측용 2)로 분류하여 학습용 데이터를 통해 인공지능 모델을 학습시키고, 예측용 데 이터를 학습된 인공지능 모델에 입력하여 출력된 분류 결과를 바탕으로 인공지능 모델의 정확도는 일 정 수치(ex. 99%) 이상을 목표로 하여, 학습이 수행될 수 있다. 한편, 인공지능 모델은 차원축소 및 군집화의 학습 과정에서, 특징 벡터의 값에 따라 여러 영역들 중 하나 에 학습용 데이터를 할당하는 작업을 반복 수행함으로써 군집화를 실시하며, 이에 따라, 학습 결과를 바탕으로 각 영역에는 다양한 학습용 데이터가 할당될 수 있다. 이후, 각 영역에 대하여 군집화된 결과, 그룹 라벨링은 각 영역에 할당된 학습용 데이터 중 가장 우세한 그룹으 로 설정한다. 예컨대, 각 영역에 할당된 데이터에 대하여 그룹(라벨)별 확률값을 구하고, 그 확률값이, 주어진 임계값 이상인 그룹으로 해당 영역의 군집 그룹 영역을 설정하여 시각화 모델을 생성할 수 있다. 인공지능 모델 은 예측용 데이터를 이용하여 시각화 모델을 검증한다. 이때, 사용자에 대해 개인 맞춤형 서비스를 제공하기 위하여, 인공지능 모델은, 학습용 데이터는 사용자의 구음 장애 패턴, 즉, 특정 자음 혹은 모음 발화에 장애가 있는 경우, 해당 자음 및/또는 모음 발화에 장애가 있 는 경우, 해당 자음 및/또는 모음이 발화되는 음소만, 구음 장애 음성과 정상인 음성을 포함하는 데이터 베이스 로부터 추출하여 최적 단음절을 추출할 수 있다. 또한, 인공지능 모델은 각 단모음 별 특징 벡터들의 분포를 비교하여, 분포의 차이에 따른 순서를 식별하 고, 분포의 차이가 가장 큰 순서부터 기 설정된 개수만큼의 특징 벡터(조합)를 최적 특징 벡터(조합)으로 추출 할 수 있다. 추가로, 인공지능 모델의 학습 과정에서, 사용자에 대한 개인 맞춤형으로 학습이 이루어지도록, 정상인 데 이터와 사용자의 구음 장애 유형에 대한 그룹에 속하는 데이터 만으로 이분 분류 시각화한 시각화 모델을 생성 할 수 있으며, 이때, 시각화 결과 정상인 영역의 영역과 사용자 구음 장애 유형에 대한 그룹 영역의 영역, 두 가지만으로 이미지 상에 표기될 수 있다. 한편, 재활에 있어서, 인공지능 모델은, 사용자의 음성에 대한 오디오 데이터가 입력되면, 오디오 데이터 로부터 획득한 특징 벡터의 값에 따라 여러 영역 중 하나에 사용자의 오디오 데이터를 할당하고, 사용자의 구음 장애 유형을 분류하여 사용자에 맞춘 재활 훈련용 단어 리스트를 추출하여 재활을 실시하도록 한다.이때, 재활 과정 중 프로세서는 사용자의 구음 장애 정도를 2차원 공간 상에 출력하여 사용자 단말을 통해 실시간으로 제공하며, 이를 통해 바이오 피드백 효과를 이용한 재활 훈련을 실시할 수 있다. 일 실시예로, 인공지능 모델의 훈련을 위해, 프로세서는 복수의 텍스트 각각에 대해 정상인의 음성의 특징 벡터들과 환자(각 구음 장애 유형에 해당하는 환자)의 음성의 특징 벡터들을 비교하여, 텍스트-특징 벡터 조합을 획득할 수도 있다. 예를 들어, 특정 단모음(ex. ㅏ, ㅓ, ㅐ 등)에 대해서는 특정 그룹의 특징 벡터들이 구음 장애에 따라 큰 차이 를 보이는 것으로 식별될 수 있다. 일 예로, 복수의 단모음 각각에 대하여 정상인의 음성과 환자의 음성이 비교될 수 있다. 이때, 제1 단모음(ex. ㅏ)에 대해서는 제1 그룹의 특징 벡터들의 차이가 크고, 제2 단모음(ex. ㅓ)에 대해서는 제2 그룹의 특징 벡터 들의 차이가 클 수 있다. 이때, 동일한 텍스트(ex. 단모음)라도, 구음 장애 유형 별로 주된 차별점을 보이는 특징 벡터가 구분될 수도 있 다. 예를 들어, 제1 단모음에 있어, 정상인의 음성과 제1 구음 장애 유형(ex. 청각 장애)에 해당하는 환자의 음성 간에는 제1-1그룹의 특징 벡터들의 차이가 클 수 있다. 또한, 제1 단모음에 있어, 정상인의 음성과 제2 구음 장 애 유형(ex. 파킨슨병)에 해당하는 환자의 음성 간에는 제1-2그룹의 특징 벡터들의 차이가 클 수 있다. 이렇듯, 구음 장애 유형 별로 다양한 텍스트-특징 벡터 조합이 식별될 수 있고, 프로세서는 다양한 텍스트 -특징 벡터 조합을 활용하여 인공지능 모델을 훈련시킬 수 있다. 그 결과, 인공지능 모델은 정상인 또는 다양한 구음 장애 유형과 관련된 특징 벡터를 선택/조합하는 군집화 방식에 대하여 점차 훈련될 수 있다. 한편, 프로세서는, 도 5, 6 및 9와 같이, 군집화된 특징 벡터들이 2차원 공간 상에 매핑된 이미지를 출력 하기 위해, 구음 장애 유형 별로 매핑된 군집화된 특징 벡터들의 수치를 기반으로, 서로 다른 오디오 특성이 별 도의 영역으로 표출되도록 2차원 공간 상의 복수의 영역을 포함하는 이미지를 생성한다. 이때, 복수의 영역 중 사용자의 음성의 오디오 특성을 나타내는 영역이 구분되도록 표시할 수 있다. 프로세서가 복수의 영역을 이미지 상에 배치하는데 있어, 프로세서는, 오디오 특성 간의 차이가 적을 수록 영역 간의 거리가 가깝게 설정하고, 동일한 구음 장애 유형을 나타내는 오디오 특성을 갖는 영역들은 기설 정된 범위 내의 영역에 위치하도록 설정된다. 오디오 특성은, 일 오디오 데이터에 대한 특징 벡터들 각각이 갖는 정량적 수치이다. 예를 들어, 정상인의 음성에 대한 기준 오디오 특성, 제1 구음 장애 유형에 대한 제1 오디오 특성, 제2 구음 장 애 유형에 대한 제2 오디오 특성, 및 제3 구음 장애 유형에 대한 제3 오디오 특성이 획득된 경우, 기준 오디오 특성은 0~10의 수치를 갖는 특징 벡터 a, 10~15의 수치를 갖는 특징 벡터 b, 및 31~33의 수치를 갖는 특징 벡터 c를 포함할 수 있다. 이때, 제1 오디오 특성은 13~14의 수치를 갖는 특징 벡터 a, 및 13~20의 수치를 갖는 특징 벡터 b를, 제2 오디 오 특성은 0~7의 수치를 갖는 특징 벡터 a, 및 30~40의 수치를 갖는 특징 벡터 b를 포함할 수 있다. 이에 따라, 제1 오디오 특성에 대한 제1 영역은, 특징 벡터 b를 기반으로 기준 오디오 특성에 대한 기준 영역과 의 거리가 산정될 수 있고, 제2 오디오 특성에 대한 제2 영역은, 특징 벡터 a를 기반으로 기준 영역과의 거리가 산정될 수 있다. 이때, 제1 영역과 제2 영역은, 각각의 특성 벡터들 간의 유사점이 존재하지 않음으로, 동일한 구음 장애 유형을 나타내지 않고 있다. 반면, 제3 오디오 특성은, 11~14의 수치를 갖는 특징 벡터 a, 20~22의 수치를 갖는 특징 벡터 b, 및 0~5의 수치 를 갖는 특징 벡터 c를 포함할 수 있다. 이때, 제3 오디오 특성에 대한 제3 영역은, 제2 영역 및 기준 영역에 대하여, 각각의 특성 벡터들 간의 유사점 이 존재하지 않으나, 제1 영역과는 특징 벡터 a 및 특징 벡터 b를 기반으로 영역 간의 거리가 산정될 수 있다. 한편, 일 영역은 군집화된 특징 벡터들을 2차원화한, 점의 분포도 또는 면의 넓이로 출력될 수 있다. 도 6은 본 발명의 일 실시예에 따른 사용자 단말을 통한 재활 수행 상태도이다. 도 6에 도시된 바와 같이, 본 발명의 프로세서는 사용자의 음성을 정상인의 음성과 비교한 차이를 새로 입 력된 음성에 대해 업데이트된 이미지를 제공할 수 있다. 구체적으로, 사용자 단말로, 일 텍스트에 대한 사용자의 음성이 입력되면, 사용자의 음성이 해당하는 적어 도 하나의 구음 장애 유형에 따라, 해당 텍스트를 정상인이 발음하였을 때의 구간과, 사용자의 발음에 따른 구 간이, 사용자 단말의 화면 상에 구분되도록 출력된다. 일 실시예로, 프로세서는 사용자 단말로부터 해당 텍스트에 대한 정상인의 음성을 출력하여 사용자가 이를 따라 음성을 입력하도록 유도함으로써 재활을 실시할 수 있으며, 프로세서는 사용자의 음성이 입력될 때 마다(ex. 1회차 재활, 2회차 재활, 3회차 재활), 사용자 단말의 화면 상에 표시된 이미지를 새로 입력 된 사용자의 음성을 바탕으로 업데이트하여 제공할 수 있다. 이를 통해, 사용자는 현재 자신의 음성이 이미지 상에 차지하는 영역과, 해당 음성에 대한 텍스트를 정상인이 발음하였을 때 위치하는 영역 간의 거리를 가시적으로 비교하며 구음 장애 재활을 수행할 수 있다. 도 7은 본 발명의 일 실시예에 따른 영역 별 복수의 노드 매칭 상태를 도시한 것이다. 각 영역을 구성하는 적어도 하나의 영역에는, 기존 인공지능 모델의 학습에 의해 획득된 특징 벡터들 중 동일한 특징 벡터에 대한 데이터가 샘플로써 할당되며, 각 영역에 해당하는 동그라미의 색인(패턴)은 특징 벡터 의 그룹(라벨)에 따른 분류에 따른다. 구체적으로, 도 7에 도시된 바와 같이, 일 영역(j)은 시각화 모델을 구성하는 복수의 출력 노드(x_1, x_2, x_3 등) n개가 매칭되며, 각각의 출력 노드는 매칭된 해당 영역(j)에 대해 적용되는 가중치를 갖는다. 이때, 시각화 모델이 해당 영역(j)을 이미지로 출력하는데 있어, 해당 영역(j)에 매칭된 복수의 출력 노드(x_1, x_2, x_3 등) 각각이 갖는 가중치(w_j1, w_j2, w_j3 등)에 의해, 구음 장애 유형에 따른 색인(패턴)이 이미지 상에 해당 영역(j)에 대해 출력될 수 있다. 예를 들어, 영역(j)은 특정한 하나의 구음 장애 유형에 매칭되도록 설정될 수 있다. 여기서, 사용자의 음성으로 부터 획득된 특징 벡터의 그룹에 따라 사용자가 해당 구음 장애 유형에 매칭될 확률이 주어진 임계치 이상인 경 우, 해당 영역(j)이 특정한 컬러로 표시될 수 있다. 그 결과, 이미지 내 어떠한 영역이 상술한 컬러로 표시되는 지가 재활 과정에서 육안으로 확인될 수 있고, 사용자는 정상에 매칭되는 영역과 컬러 표시된 본인의 영역을 비 교해가면서 본인의 구음 장애 유형 및 실시간 재활 효과를 인지할 수 있다. 도 8은 본 발명의 일 실시예에 따른 타당성 실험 결과를 도시한 것이다. 도시된 바에 따르면, 구음 장애 유형에 따라 분류된 구음장애 그룹 1, 2, 및 3은 각각의 분포에 따라 형성된 영 역이, 정상인에 대해 그룹화된 특징 벡터가 매핑된 영역과 명확하게 구분되는 영역이 추출된 것으로 나타났다. 이에 따라, 본 발명의 딥러닝 기반 구음 장애 시각화 및 재활 기술은 유효한 기술인 것으로 판단된다. 도 9 및 도 10은 본 발명의 이 실시예에 따른 사용자 단말을 통한 재활 수행 상태도이다. 본 발명의 이 실시예로, 프로세서는 사용자의 음성에 대한 구음 장애 여부를 판단하고, 구음 장애 유형이 식별되면, 사용자의 음성에 따른 복수의 재활 텍스트를 출력하여, 사용자가 출력된 복수의 재활 텍스트에 대한 재활을 실시하도록 할 수 있다. 이때, 사용자는 종래 기술인 AI 비서가 결합된 본 발명의 전자 장치를 음성으로 제어(ex. 발음 연습할 수 있게 단어 리스트 뽑아줘, 이제 00 연습해야지 등 동작 지시)하는 것이 가능하다. 한편, 프로세서는, 사용자의 음성에 대한 오디오 특성을 바탕으로 복수의 재활 텍스트를 획득하고, 획득된 복수의 재활 텍스트 각각에 대해, 사용자에 대한 구음 장애 재활을 수행한 횟수를 식별하여, 복수의 재활 텍스 트 각각에 대해 마지막으로 수행된 재활에 따라 입력된 사용자의 음성의 오디오 특성을 정상인의 오디오 특성과 비교하여 최종 일치율을 식별할 수 있다. 일 실시예에 따라, 복수의 재활 텍스트 중 제1 텍스트에 대한 사용자의 재활이 수행된 결과 최종 일치율이 임계 치를 초과하고, 재활 횟수가 일정 횟수 이하인 경우, 프로세서는 복수의 재활 텍스트 중 제1 텍스트와 유 사한 적어도 하나의 제2 텍스트를 획득한다. 즉, 프로세서는, 사용자가 제1 텍스트에 대하여 재활에 따른 발화를 적게 실행하였음에도 제1 텍스트에 대 한 재활이 완료되었으므로, 제1 텍스트에 대한 발음이 사용자에게 구음 장애를 쉽게 극복할 수 있는 발음인 것으로 판단한다. 이에 따라, 프로세서는, 제1 텍스트에 대한 정상인의 발음과 유사한 정상인의 발음을 갖는 제2 텍스트를 획득하여, 복수의 재활 텍스트에서, 제2 텍스트를 제외하거나, 또는, 복수의 재활 텍스트 중, 재활 순서를 후순 위로 배치할 수 있다. 반면, 복수의 재활 텍스트 중 제3 텍스트에 대한 사용자의 재활이 수행된 결과 최종 일치율이 임계치를 초과하 고, 재활 횟수가 일정 횟수를 초과하는 경우, 프로세서는, 복수의 재활 텍스트 중, 제3 텍스트와 유사한 적어도 하나의 제4 텍스트를 획득한다. 즉, 프로세서는, 사용자가 제3 텍스트에 대하여 재활에 따른 발화를 많이 실행하여야만 제3 텍스트에 대한 재활이 완료된 것에 대하여, 제3 텍스트에 대한 발음이 사용자에게 구음 장애를 극복하기 어려운 발음인 것으로 판단한다. 이에 따라, 프로세서는, 복수의 재활 텍스트 중, 제4 텍스트에 대한 사용자의 재활을 우선적으로 수행한다. 한편, 프로세서가 제1 텍스트와 유사한 제2 텍스트, 제3 텍스트와 유사한 제4 텍스트를 식별하기 위해, 텍 스트 간의 유사도를 판단하기 위한 기술로써, 프로세서는, 텍스트를 발화하는 정상적인 음성으로부터 추출 되는 복수의 특징 벡터를 텍스트 별로 비교할 수 있다. 예컨대, 제1 텍스트와 제2 텍스트, 제3 텍스트와 제4 텍 스트는 각각에 대한 정상적인 음성으로부터 추출된 복수의 특징 벡터 중, 일치하는 특징 벡터의 수가 임계치 이 상임에 따라, 프로세서는, 제1 텍스트의 발음과 제2 텍스트의 발음이 서로 유사하고, 제3 텍스트의 발음과 제4 텍스트의 발음이 서로 유사한 것으로 판단할 수 있다. 한편, 프로세서가 텍스트를 발화하는 음성에 대한 오디오 데이터를 획득하는데 있어서, 텍스트는, 도 5, 7, 및 9에 도시된 바와 같이, 단음절(monosyllabic word/language), 단모음(monopthong) 및 단어(word)일 수 있으며, 다양한 실시예로써, 음소(phoneme), 문장(sentence), 형태소(morpheme) 등일 수 있다. 한편, 본 발명의 이 면에 따른, 적어도 하나의 특징 벡터에 대한 차원 축소 및 군집화를 수행하는 인공지능 모 델이 저장된 전자 장치의 제어 방법에 있어서, 메모리, 통신부 및 프로세서를 포함하는 서버 에 의해 사용자 단말로부터 사용자의 음성에 대응되는 오디오 데이터를 획득(S210)하고, 획득된 오디오 데 이터로부터 복수의 특징 벡터를 추출할 수 있다. 서버는 복수의 특징 벡터를 인공지능 모델에 입력하여, 인공지능 모델의 출력을 기반으로 군집 화된 특징 벡터들이 2차원 공간 상에 매핑(mapping)된 이미지를 획득(S220)한다. 획득된 이미지는, 서버가 사용자 단말을 통해 출력할 수 있다. 이때, 서버는 오디오 데이터를 텍스트와 매칭하여 저장할 수 있다. 한편, 오디오 데이터로부터 복수의 특징 벡터를 추출하기 위해, 서버는 오디오 데이터를 노이즈 필터링 (noise filtering)하고, 필터링된 오디오 데이터를 복수의 구간으로 구간화하되, 인접한 구간 간에 일정 시간만 큼 중첩하여 프레이밍(framing)한다. 이때, 서버는 프레이밍된 오디오 데이터로부터, meanF0Hz, stdevF0Hz, meanF1Hz, stdevF1Hz, meanF2Hz, stdevF2Hz, HNR, localjitter, localabsolutejitter, rapjitter, ppq5jitter, ddpjitter, localShimmer, localdbShimmer, apq3Shimmer, apq5Shimmer, apq11Shimmer, ddaShimmer, pitch_max 및 pich_min 중 적어도 하 나를 포함하여, 복수의 음향(aqoustic) 특징 벡터를 추출할 수 있다. 또한, 서버는 프레이밍된 오디오 데이터로부터, 제1 MFCC 특징 벡터, 제2 MFCC 특징 벡터, 및 제3 MFCC 특 징 벡터를 포함하는 복수의 MFCC 특징 벡터를 추출할 수 있다. 구체적으로, 서버는 프레이밍된 오디오 데이터에 MFCC(Mel Frequency Cepstral Coefficients) 기법을 적 용하여 최대 13개의 제1 MFCC 특징 벡터를 획득할 수 있다. 이때, 서버는 적어도 하나의 제1 MFCC 특징 벡터의 값을 미분하여 적어도 하나의 제2 MFCC 특징 벡터를 획 득하고, 적어도 하나의 제2 MFCC 특징 벡터의 값을 미분하여 적어도 하나의 제3 MFCC 특징 벡터를 획득할 수 있 다.이에 따라, 서버는 음향 특징 벡터 최대 20개, MFCC 특징 벡터 최대 78개(각 프레임 마다, 제1 MFCC 특 징 벡터의 최대 개수인 13개, 1회 미분으로 인한 최대 개수인 13개, 2회 미분으로 인한 최대 개수인 13개, 총 13*3=39개를 구한 다음 이들 각각을 모든 프레임에 대하여 구한 평균 및 표준편차를 구하여 특징 벡터로 사 용, 따라서 하나의 오디오 데이터에 대하여 최대 13*3*2=78개의 MFCC 특징 벡터를 추출)를 포함하여 최대 98개의 특징 벡터를 오디오 데이터로부터 추출할 수 있다. 이하, 서버는 본 발명의 일 면에 따른 전자 장치의 프로세서의 기능을 수행한다. 한편, 인공지능 모델의 훈련을 위하여, 서버는 일 텍스트를 발화하는 정상인의 음성에 대응되는 제1 오디오 데이터, 및 해당 텍스트를 발화하는 환자의 음성에 대응되는 제2 오디오 데이터를 획득한다. 서버는 제1 오디오 데이터와 제2 오디오 데이터 각각의 복수의 특징 벡터를 비교하여, 임계치 이상의 차이 를 갖는 적어도 하나의 특징 벡터를 식별하고, 해당 특징 벡터를 바탕으로 인공지능 모델을 훈련시킬 수 있다. 한편, 본 발명은, 하드웨어인 컴퓨터와 결합되어, 본 발명의 이 면에 따른 전자 장치의 제어 방법을 수행 할 수 있도록 컴퓨터에서 독출가능한 기록매체에 저장된 컴퓨터 프로그램를 더 포함할 수 있다. 한편, 본 발명 일 면 및/또는 이 면에 따른 인공지능 모델은, 선택된 특징 벡터들을 적어도 하나의 구음 장애 유형에 따라 분류하여, 구음 장애 유형에 따라 군집화를 실시하는데 있어서, 구음 장애 유형을 분류하기 위한, 별도의 제1 모델 및 제2 모델을 더 포함할 수 있다. 구체적으로, 제1 모델은 입력된 복수의 특징 벡터 중 적어도 하나의 특징 벡터를 선택하고, 프로세서는 선 택된 특징 벡터를 제2 모델에 입력한다. 이때, 제1 모델은 ica(: 독립 성분 분석법, independent component analysis), pca(: 주성분 분석법, principal component analysis), rp(: 랜덤 투영법, random projection), 및 dae(: 딥 오토인코더, deep auto- encoder)를 포함하는 방법 중 적어도 하나의 방법으로 적어도 하나의 특징 벡터를 선택할 수 있다. 제2 모델은 입력된 특징 벡터를 분류함으로써 구음 장애 유형을 판단할 수 있으며, svm(: 서포트 벡터 머신, support vector machine), rf(: 무작위 (결정)숲, random forest), mlp(: 다층 퍼셉트론, multi-layer perceptron) 및 cnn(: 합성곱 신경망, convolutional deep neural networks)를 포함하는 분류 방법 중 적어도 하나의 방법으로 특징 벡터를 분류할 수 있다. 제1 모델이 선택하는 특징 벡터는, 적어도 하나의 구음 장애 유형에 해당하는 환자의 오디오 데이터가 정상인의 오디오 데이터와 비교된 결과에 따라 설정될 수 있다. 예를 들어, 정상인의 오디오 데이터의 특징 벡터 각각과 구음 장애를 가진 환자의 오디오 데이터의 특징 벡터 각각을 비교했을 때, 임계치 이상의 차이를 가지는 특징 벡터가 제1 모델이 선택하는 특징 벡터일 수 있다. 구체적인 예로, 복수의 정상인의 오디오 데이터에 대하여 제1 특징 벡터의 평균 값이 산출되고, 복수의 환자의 오디오 데이터에 대하여 제1 특징 벡터의 평균 값이 산출될 수 있으며, 만약 상술한 평균 값들 간의 차이가 일 정 수치 이상인 경우, 제1 특징 벡터는 제1 모델에 의해 선택되는 특징 벡터로 설정될 수 있다. 한편, 일 실시 예로, 구음 장애 유형에 따라, 서로 다른 그룹의 특징 벡터에 대한 정보가 저장될 수 있다. 예를 들어, 제1 구음 장애 유형에 대해서는 제1 그룹의 특징 벡터가 매칭되는 것으로 설정되고, 제2 구음 장애 유형에 대해서는 제2 그룹의 특징 벡터가 매칭되는 것으로 설정될 수 있다. 구체적으로, 프로세서는, 구음 장애 유형 별로, 정상인과의 차이가 임계치 이상인 특징 벡터를 식별하여, 해당 특징 벡터를 구음 장애 유형에 매칭되는 그룹으로 저장할 수 있다. 여기서, 각 구음 장애 유형에 매칭되는 복수의 (특징 벡터) 그룹에 대해, 제2 모델로 입력되는 순서가 설정될 수 있으며, 순서는, 각 구음 장애 유형의 발생 빈도에 따라 설정될 수 있다. 예를 들어, 제1 구음 장애 유형, 제2 구음 장애 유형, 제3 구음 장애 유형의 순서로 발생 빈도가 높은 경우를 가정한다. 이 경우, 프로세서는 사용자의 오디오 데이터로부터 먼저 제1 구음 장애 유형에 매칭되는 제1 특징 벡터 그룹을 추출하도록 제1 모델을 제어하고, 추출된 제1 특징 벡터 그룹을 제2 모델로 입력할 수 있다. 그 결과,사용자가 제1 구음 장애 유형에 해당하는지 여부가 출력될 수 있다. 다음으로, 프로세서는 제1 특징 벡터 그룹의 다음 순서인 제2 특징 벡터 그룹을 제2 모델로 입력하여, 그 결과, 사용자가 제2 구음 장애 유형에 해당하는지 여부가 출력될 수 있으며, 프로세서에 의해, 다음 순서 인 제3 특징 벡터 그룹이 제2 모델에 입력되어, 그 결과인, 사용자가 제3 구음 장애 유형에 해당하는지 여부가 출력된다. 추가 실시예로, 프로세서는 구음 장애 유형 간의 연관성에 따라 각 특징 벡터 그룹이 제2 모델로 입력되는 순서를 유동적으로 변경할 수 있다. 구체적으로, 프로세서는 각 구음 장애 유형의 발생 이력을 바탕으로 둘 이상의 구음 장애 유형이 동시에 발생한 빈도/확률을 식별할 수 있다. 그리고, 프로세서는 식별된 빈도/확률에 따라 구음 장애 유형 간의 매칭 여부를 설정할 수 있다. 예를 들어, 제1 구음 장애 유형과 제3 구음 장애 유형이 동시에 발생한 빈도가 일정 수 이상인 경우, 프로세서 는 제1 구음 장애 유형과 제3 구음 장애 유형이 서로 매칭되는 것으로 설정할 수 있다. 관련하여, 일 실시 예로, 제1 특징 벡터 그룹, 제2 특징 벡터 그룹, 제3 특징 벡터 그룹의 순서대로 제2 모델로 입력되는 것으로 기설정된 상황을 가정한다. 이 경우, 일반적으로, 프로세서는 제1 특징 벡터 그룹을 제2 모델로 입력하여, 사용자가 제1 구음 장애 유 형에 해당하는지 여부를 출력하고, 이어서, 제1 특징 벡터 그룹의 다음 순서인 제2 특징 벡터 그룹을 제2 모델 로 입력하여, 사용자가 제2 구음 장애 유형에 해당하는지 여부를 출력한다. 다만, 제1 특징 벡터 그룹이 입력된 결과 사용자가 제1 구음 장애 유형으로 판단되면, 프로세서는, 제1 특 징 벡터 그룹의 다음 순번인 제2 특징 벡터 그룹이 아닌, 제3 특징 벡터 그룹을 제2 모델로 입력할 수 있다. 구체적으로, 프로세서는, 사용자가 제1 구음 장애 유형으로 판단된 경우, 제1 구음 장애 유형에 매칭된 구 음 장애 유형인 제3 구음 장애 유형을 식별하고, 제3 구음 장애 유형에 매칭된 제3 특징 벡터 그룹을 제2 모델 로 입력한다. 그 결과, 사용자가 제3 구음 장애 유형에 해당하는지 여부를 출력할 수 있다. 이후, 사용자가 제3 구음 장애 유형이 아닌 것으로 판단되거나, 또는 사용자가 제3 구음 장애 유형으로 판단되 되, 제3 구음 장애 유형에 매칭된 구음 장애 유형이 존재하지 않는 경우, 프로세서는 제1 특징 벡터 그룹 의 다음 순번인 제2 특징 벡터 그룹을 제2 모델로 입력하여, 사용자가 제2 구음 장애 유형에 해당하는지 여부를 출력할 수 있다. 반면, 사용자가 제3 구음 장애 유형으로 판단되되, 제3 구음 장애 유형에 매칭된 구음 장애 유형으로 제4 구음 장애 유형이 식별된 경우, 프로세서는, 제4 구음 장애 유형에 매칭된 제4 특징 벡터 그룹을 제2 모델에 우 선 입력할 수 있다. 추가로, 제1 모델은 오디오 데이터로부터 추출된, 복수의 음향 특징 벡터 및 복수의 MFCC 특징 벡터 중, 텍스트 에 따라 적어도 하나의 특징 벡터를 선택할 수 있다. 구체적으로, 제1 모델은, 텍스트에 따라 적어도 하나의 특징 벡터를 선택하고, 제2 모델은, 선택된 특징 벡터를 바탕으로, 텍스트를 발화하는 정상인의 음성의 오디오 데이터를 오디오 데이터와 비교한다. 이를 위해, 오디오 데이터는 텍스트에 매칭되어 프로세서에 입력되며, 프로세서는 오디오 데이터를 전처리하여 복수의 특징 벡터를 텍스트와 함께 제1 모델로 입력한다. 한편, 프로세서는, 사용자의 실제 구음 장애 유형이 인공지능 모델로부터 출력된 구음 장애 유형과 일치하지 않는 경우, 텍스트에 매칭된 오차 발생 횟수를 업데이트하고, 오차 발생 횟수가 일정 값을 초과하면, 텍스트에 대하여 다른 특징 벡터를 선택하도록 제1 모델을 업데이트할 수 있다. 이때, 업데이트된 제1 모델을 통한 선택 및 기존 제2 모델에 의한 분류로 획득된 구음 장애 유형이 실게 구음 장애 유형과 일치하지 않는 경우, 프로세서는, 텍스트에 대하여 다른 방식으로 분류를 실시하도록 제2 모 델을 업데이트할 수 있다. 예를 들어, 기존 구음 장애 분류 방법으로 본 발명에 의한 제1 모델이 ica를 적용하여 98개의 특징 벡터 중 40 개의 특징 벡터(1차 선택 특징 벡터)를 선택하고, 제2 모델이 mlp(1차 분류 방식)를 적용하여 분류를 실시하는경우, 텍스트 \"낮새\"에 대한 사용자의 구음 장애 유형이 파킨슨인 반면, 제2 모델에 의해 후두로 출력되면, 프 로세서는 텍스트 \"낮새\"에 매칭된 98개의 특징 벡터 중 1차 선택 특징 벡터를 제외한 나머지 58개의 특징 벡터로부터 신규 40개의 특징 벡터(2차 선택 특징 벡터)를 선택하도록 제1 모델을 업데이트 할 수 있다. 이 경우, 인공지능 모델은, 업데이트 이후 획득된, 텍스트 \"낮새\"에 대한 오디오 데이터에 매칭된 복수의 특징 벡터가 입력되면, 기 매칭된 2차 선택 특징 벡터를 자동으로 선택하여 분류를 실시함으로써 텍스트 \"낮 새\"에 대한 구음 장애 분류의 정확도를 높일 수 있다. 이때, 텍스트 \"낮새\"에 대한 제1 모델의 업데이트에도 불구하고 제2 모델로부터의 출력이 파킨슨이 아닌 경우, 프로세서는 제2 모델의 분류 방식을 1차 분류 방식 외의 다른 방식으로 선택할 수 있으며, 다른 방식을 각 각 적용하여 후두형으로 분류되는 방식(2차 분류 방식)을 획득하고, 텍스트 \"낮새\"에 매칭된 제2 모델의 분류 방식을 2차 분류 방식으로 업데이트함으로써, 텍스트 \"낮새\"에 대한 구음 장애 분류의 정확도를 높일 수 있다. 또한, 프로세서는 텍스트에 대한 오류 발생 횟수가 일정 값을 초과하면, 일 텍스트에 대하여 선택되는 특 징 벡터의 수를 변동하도록 제1 모델을 업데이트 할 수 있다. 예를 들어, 제1 모델 및 제2 모델 각각의 업데이트 이후에도 텍스트 \"낮새\"에 대한 제2 모델의 구음 장애 분류 결과가 사용자의 구음 장애 유형과 다른 유형인 경우, 총 3회의 오류가 발생되었으므로, 프로세서는 텍스 트 \"낮새\"에 매칭된 오류 발생 횟수를 3회로 업데이트 할 수 있다. 이때, 오류 발생 횟수가 2회를 초과한 것에 대해, 프로세서는 일정 값(2회)을 초과한 것으로 판단하고, 텍 스트 \"낮새\"를 발화하는 오디오 데이터에 대해 제1 모델의 특징 벡터 선택 수를 20개로 변동하도록 업데이트 할 수 있다. 이 경우, 도 11에 도시된 바와 같이, 구음 장애 유형이 파킨슨인 경우, 특징 벡터의 수 40개에 대한 결과보다 20개에 대한 결과가 더 정확한 것에 따라, 특징 벡터의 수를 줄이는 것이 허용된다. 도 11은 본 발명의 삼 실시예에 따른 구음 장애 분류 실험 결과를 도시한 것이다. 한편, 도 11과 같은, 구음 장애 유형별 선택 방법 및 분류 방법에 따른 실험 결과를 살펴보면, 일 텍스트에 대 한 제1 모델(데이터 차원 축소)의 특징 벡터 선택 방식과 선택 수, 및 제2 모델(분류)의 선택된 특징 벡터 분류 방식에 따른 파킨슨 환자(PD)와 정상인(HC)의 분류 정확도를 수치화하고 있다. 도시된 바에 따르면, 제1 모델에서 98개의 특징 벡터 중 20개의 특징 벡터를 ica 방식으로 선택하여, 제2 모델 에서 svm 방식에 의해 분류한 결과의 정확도가 가장 높은 것으로 나타났다. 또한, 구음 장애 유형 언어, 청각, 후두 각각에 대한 제2 모델의 mlp 방식을 적용한 분류 결과, 정확도가 99%에 근사하거나 초과한 것으로 산출되었다. 이에 따라, 딥러닝 기반 구음 장애 분류는 실효성 있는 기술인 것이 증명된다. 한편, 도 12은 본 발명의 일 실시예에 다른 서버 구성도이다. 도시된 바와 같이, 서버는 메모리, 통신부 및 프로세서를 포함할 수 있다. 메모리는 서버의 동작에 필요한 각종 프로그램 및 데이터를 저장할 수 있다. 메모리는 비휘발성 메모리, 휘발성 메모리, 플래시메모리(flash-memory), 하드디스크 드라이브(HDD) 또는 솔리드 스테이트 드라이브(SSD) 등으로 구현될 수 있다. 통신부는 외부 장치와 통신을 수행할 수 있다. 특히, 통신부는 와이파이 칩, 블루투스 칩, 무선 통신 칩, NFC칩, 저전력 블루투스 침(BLE 칩) 등과 같은 다양한 통신 칩을 포함할 수 있다. 이때, 와이파이 칩, 블루 투스 칩, NFC 칩은 각각 LAN 방식, WiFi 방식, 블루투스 방식, NFC 방식으로 통신을 수행한다. 와이파이 칩이나 블루투스칩을 이용하는 경우에는 SSID 및 세션 키 등과 같은 각종 연결 정보를 먼저 송수신 하여, 이를 이용하 여 통신 연결한 후 각종 정보들을 송수신할 수 있다. 무선 통신칩은 IEEE, 지그비, 3G(3rd Generation), 3GPP(3rd Generation Partnership Project), LTE(Long Term Evolution) 등과 같은 다양한 통신 규격에 따라 통 신을 수행하는 칩을 의미한다. 프로세서는 메모리에 저장된 각종 프로그램을 이용하여 서버의 전반적인 동작을 제어할 수 있다. 프로세서는 RAM, ROM, 그래픽 처리부, 메인 CPU, 제1 내지 n 인터페이스 및 버스로 구성될 수 있다. 이때, RAM, ROM, 그래픽 처리부, 메인 CPU, 제1 내지 n 인터페이스 등은 버스를 통해 서로 연결될 수 있다.RAM은 O/S 및 어플리케이션 프로그램을 저장한다. 구체적으로, 서버가 부팅되면 O/S가 RAM에 저장되고, 사 용자가 선택한 각종 어플리케이션 데이터가 RAM에 저장될 수 있다. ROM에는 시스템 부팅을 위한 명령어 세트 등이 저장된다. 턴 온 명령이 입력되어 전원이 공급되면, 메인 CPU는 ROM에 저장된 명령어에 따라 메모리에 저장된 O/S를 RAM에 복사하고, O/S를 실행시켜 시스템을 부팅시킨다. 부팅이 완료되면, 메인 CPU는 메모리에 저장된 각종 어플리케이션 프로그램을 RAM에 복사하고, RAM에 복사된 어플리케이션 프로그램을 실행시켜 각종 동작을 수행한다. 그래픽 처리부는 연산부(미도시) 및 렌더링부(미도시)를 이용하여 아이템, 이미지, 텍스트 등과 같은 다양한 객 체를 포함하는 화면을 생성한다. 여기서, 연산부는 입력부로부터 수신된 제어 명령을 이용하여 화면의 레이아웃 에 따라 각 객체들이 표시될 좌표값, 형태, 크기, 컬러 등과 같은 속성값을 연산하는 구성일 수 있다. 그리고, 렌더링부는 연산부에서 연산한 속성값에 기초하여 객체를 포함하는 다양한 레이아웃의 화면을 생성하는 구성이 일 수 있다. 이러한 렌더링부에서 생성된 화면은 디스플레이의 디스플레이 영역 내에 표시될 수 있다. 메인 CPU는 메모리에 액세스하여, 메모리에 저장된 OS를 이용하여 부팅을 수행한다. 그리고, 메인 CPU는 메모리에 저장된 각종 프로그램, 컨텐츠, 데이터 등을 이용하여 다양한 동작을 수행한다. 제1 내지 n 인터페이스는 상술한 각종 구성요소들과 연결된다. 제1 내지 n 인터페이스 중 하나는 네트워크를 통 해 외부 장치와 연결되는 네트워크 인터페이스가 될 수도 있다. 한편, 나아가, 프로세서는 인공지능 모델을 제어할 수 있다. 이 경우, 프로세서는 인공지능 모델을 제어하기 위한 그래픽 전용 프로세서(예: GPU)를 포함할 수 있음은 물론이다. 한편, 본 발명에 따른 인공지능 모델은 교사 지도학습(supervised learning) 또는 비교사 지도학습 (unsupervised learning)기반의 모델일 수 있다. 나아가, 본 발명에 따른 인공지능 모델은 SVM(support vector machine), Decision tree, neural network 등 및 이들이 응용된 방법론을 포함할 수 있다. 일 실시예로, 본 발명에 따른 인공지능 모델은 학습데이터를 입력하여 학습된 합성곱 신경망(Convolutional deep Neural Networks, CNN) 기반의 인공지능 모델일 수 있다. 다만, 이에 한정되는 것은 아니며, 다양한 인공 지능 모델이 본 발명에 적용될 수 있음은 물론이다. 예컨대, DNN(Deep Neural Network), RNN(Recurrent Neural Network), BRDNN(Bidirectional Recurrent Deep Neural Network)과 같은 모델이 인공지능 모델로서 사용될 수 있으나, 이에 한정되지 않는다. 이때, 합성곱 신경망(Convolutional deep Neural Networks, CNN)은 최소한의 전처리(preprocess)를 사용하도록 설계된 다계층 퍼셉트론(multilayer perceptrons)의 한 종류이다. 합성곱 신경망은 하나 또는 여러개의 합성곱 계층(convolutional layer)과 그 위에 올려진 일반적인 인공신경망 계층들로 이루어져 있으며, 가중치와 통합 계층(pooling layer)들을 추가로 활용한다. 이러한 구조 덕분에 합성곱 신경망은 2차원 구조의 입력 데이터를 충분히 활용할 수 있다. 또한, 합성곱 신경망은 표준 역전달을 통해 훈련될 수 있다. 합성곱 신경망은 다른 피 드포워드 인공신경망 기법들보다 쉽게 훈련되는 편이고 적은 수의 매개변수를 사용한다는 이점이 있다. 또한, 심층 신경망(Deep Neural Networks, DNN)은 입력 계층(input layer)과 출력 계층(output layer) 사이에 복수개의 은닉 계층(hidden layer)들로 이뤄진 인공신경망(Artificial Neural Network, ANN)이다. 이때, 심층 신경망의 구조는 퍼셉트론(perceptron)으로 구성될 수 있다. 퍼셉트론은 여러 개의 입력 값(input) 과 하나의 프로세서(prosessor), 하나의 출력 값으로 구성된다. 프로세서는 여러 개의 입력 값에 각각 가중치를 곱한 후, 가중치가 곱해진 입력 값들을 모두 합한다. 그 다음 프로세서는 합해진 값을 활성화함수에 대입하여 하나의 출력 값을 출력한다. 만약 활성화함수의 출력 값으로 특정한 값이 나오기를 원하는 경우, 각 입력 값에 곱해지는 가중치를 수정하고, 수정된 가중치를 이용하여 출력 값을 다시 계산할 수 있다. 이때, 각각의 퍼셉트 론은 서로 다른 활성화함수를 사용할 수 있다. 또한 각각의 퍼셉트론은 이전 계층에서 전달된 출력들을 입력으 로 받아들인 다음, 활성화 함수를 이용해서 출력을 구한다. 구해진 출력은 다음 계층의 입력으로 전달된다. 상 술한 바와 같은 과정을 거치면 최종적으로 몇 개의 출력 값을 얻을 수 있다. 순환 신경망(Reccurent Neural Network, RNN)은 인공신경망을 구성하는 유닛 사이의 연결이 Directed cycle을 구성하는 신경망을 말한다. 순환 신경망은 앞먹임 신경망과 달리, 임의의 입력을 처리하기 위해 신경망 내부의 메모리를 활용할 수 있다. 심층 신뢰 신경망(Deep Belief Networks, DBN)이란 기계학습에서 사용되는 그래프 생성 모형(generative graphical model)으로, 딥 러닝에서는 잠재변수(latent variable)의 다중계층으로 이루어진 심층 신경망을 의미한다. 계층 간에는 연결이 있지만 계층 내의 유닛 간에는 연결이 없다는 특징이 있다. 심층 신뢰 신경망은 생성 모형이라는 특성상 선행학습에 사용될 수 있고, 선행학습을 통해 초기 가중치를 학습 한 후 역전파 혹은 다른 판별 알고리즘을 통해 가중치의 미조정을 할 수 있다. 이러한 특성은 훈련용 데이터가 적을 때 굉장히 유용한데, 이는 훈련용 데이터가 적을수록 가중치의 초기값이 결과적인 모델에 끼치는 영향이 세지기 때문이다. 선행학습된 가중치 초기값은 임의로 설정된 가중치 초기값에 비해 최적의 가중치에 가깝게 되 고 이는 미조정 단계의 성능과 속도향상을 가능케 한다. 상술한 인공지능 및 그 학습방법에 관한 내용은 예시를 위하여 서술된 것이며, 상술한 실시 예들에서 이용되는 인공지능 및 그 학습방법은 제한되지 않는다. 예를 들어, 당 업계의 통상의 기술자가 동일한 과제해결을 위하여 적용할 수 있는 모든 종류의 인공지능 기술 및 그 학습방법이 개시된 실시 예에 따른 시스템을 구현하는 데 활 용될 수 있다. 한편, 프로세서는 하나 이상의 코어(core, 미도시) 및 그래픽 처리부(미도시) 및/또는 다른 구성 요소와 신호를 송수신하는 연결 통로(예를 들어, 버스(bus) 등)를 포함할 수 있다. 일 실시예에 따른 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 본 발명과 관련하여 설명된 방법을 수행한다. 예를 들어, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써 신규 학습용 데이터 를 획득하고, 학습된 모델을 이용하여, 상기 획득된 신규 학습용 데이터에 대한 테스트를 수행하고, 상기 테스 트 결과, 라벨링된 정보가 소정의 제1 기준값 이상의 정확도로 획득되는 제1 학습용 데이터를 추출하고, 상기 추출된 제1 학습용 데이터를 상기 신규 학습용 데이터로부터 삭제하고, 상기 추출된 학습용 데이터가 삭제된 상 기 신규 학습용 데이터를 이용하여 상기 학습된 모델을 다시 학습시킬 수 있다. 한편, 프로세서는 프로세서 내부에서 처리되는 신호(또는, 데이터)를 일시적 및/또는 영구적으로 저 장하는 램(RAM: Random Access Memory, 미도시) 및 롬(ROM: Read-Only Memory, 미도시)을 더 포함할 수 있다. 또한, 프로세서는 그래픽 처리부, 램 및 롬 중 적어도 하나를 포함하는 시스템온칩(SoC: system on chip) 형태로 구현될 수 있다. 메모리에는 프로세서의 처리 및 제어를 위한 프로그램들(하나 이상의 인스트럭션들)을 저장할 수 있 다. 메모리에 저장된 프로그램들은 기능에 따라 복수 개의 모듈들로 구분될 수 있다. 본 발명의 다양한 실시 예들 중 적어도 두 개의 실시 예는 서로 양립할 수 없는 것이 아닌 한 결합되어 수행될 수 있다. 본 발명의 실시예와 관련하여 설명된 방법 또는 알고리즘의 단계들은 하드웨어로 직접 구현되거나, 하드웨어에 의해 실행되는 소프트웨어 모듈로 구현되거나, 또는 이들의 결합에 의해 구현될 수 있다. 소프트웨어 모듈은 RAM(Random Access Memory), ROM(Read Only Memory), EPROM(Erasable Programmable ROM), EEPROM(Electrically Erasable Programmable ROM), 플래시 메모리(Flash Memory), 하드 디스크, 착탈형 디스크, CD-ROM, 또는 본 발명이 속하는 기술 분야에서 잘 알려진 임의의 형태의 컴퓨터 판독가능 기록매체에 상주할 수도 있다. 본 발명의 구성 요소들은 하드웨어인 컴퓨터와 결합되어 실행되기 위해 프로그램(또는 애플리케이션)으로 구현 되어 매체에 저장될 수 있다. 본 발명의 구성 요소들은 소프트웨어 프로그래밍 또는 소프트웨어 요소들로 실행 될 수 있으며, 이와 유사하게, 실시 예는 데이터 구조, 프로세스들, 루틴들 또는 다른 프로그래밍 구성들의 조 합으로 구현되는 다양한 알고리즘을 포함하여, C, C++, 자바(Java), 어셈블러(assembler) 등과 같은 프로그래밍 또는 스크립팅 언어로 구현될 수 있다. 기능적인 측면들은 하나 이상의 프로세서들에서 실행되는 알고리즘으로 구현될 수 있다."}
{"patent_id": "10-2022-0005706", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이상, 첨부된 도면을 참조로 하여 본 발명의 실시예를 설명하였지만, 본 발명이 속하는 기술분야의 통상의 기술 자는 본 발명이 그 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 실시될 수 있다는 것을 이해할 수 있을 것이다. 그러므로, 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며, 제한적이 아닌 것으로 이해해야만 한다."}
{"patent_id": "10-2022-0005706", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 전자 장치의 구성도이다. 도 2는 본 발명의 일 실시예에 따른 구음 장애 정도 시각화의 기본 흐름도이다. 도 3 및 도 4는 본 발명의 일 실시예에 따른 시스템 구성도이다. 도 5는 본 발명의 일 실시예에 따른 학습 및 이미지 생성에 대한 시스템의 동작 흐름도이다. 도 6은 본 발명의 일 실시예에 따른 사용자 단말을 통한 재활 수행 상태도이다. 도 7은 본 발명의 일 실시예에 따른 영역 별 복수의 노드 매칭 상태를 도시한 것이다. 도 8은 본 발명의 일 실시예에 따른 타당성 실험 결과를 도시한 것이다. 도 9 및 도 10은 본 발명의 이 실시예에 따른 사용자 단말을 통한 재활 수행 상태도이다. 도 11은 본 발명의 삼 실시예에 따른 구음 장애 분류 실험 결과를 도시한 것이다. 도 12는 본 발명의 일 실시예에 다른 서버 구성도이다."}
