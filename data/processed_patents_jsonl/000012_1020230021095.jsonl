{"patent_id": "10-2023-0021095", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0128197", "출원번호": "10-2023-0021095", "발명의 명칭": "인공지능 기반의 지능형 신체 인터페이스 장치 및 방법", "출원인": "전주대학교 산학협력단", "발명자": "이영재"}}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "영상을 입력받는 단계;상기 입력된 영상에 대해, 리눅스 기반의 로봇을 사용하기 위한 초기화 루틴을 수행하는 단계; 상기 초기화 루틴이 수행된 상기 입력된 영상에 대해, 인공지능 모델을 이용하여 상기 핸드 데이터를 처리하는단계;상기 처리된 핸드 데이터에 대해 정적 핸드 제스처인지 또는 동적 핸드 제스처인지 여부를 판단하는 단계;상기 판단 결과에 따라 제스처를 구분하여 인식하는 단계; 및상기 인식된 정보를 기반으로 제어모드를 선별하여, 상기 선별된 제어모드에 대한 명령어를 생성하는 단계를 포함하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 판단 결과에 따라 정적 핸드 제스처 또는 동적 핸드 제스처를 구분하여 인식하는 단계는,상기 판단결과, 상기 정적 핸드 제스처인 경우라면, 인공지능 모델을 통해 데이터 셋을 학습하여 학습 데이터를생성하는 단계; 및상기 생성된 학습 데이터를 KNN 기법을 기반으로 인식하는 단계를 포함하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 생성된 학습 데이터를 KNN 기법을 기반으로 인식하는 단계는,파이프라인 처리를 통해 손 관절과 관련하여 21개 손가락 특징점을 참조하고 KNN을 사용해 상기 정적 핸드 제스처를 구현하는 단계를 포함하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 판단 결과에 따라 정적 핸드 제스처 또는 동적 핸드 제스처를 구분하여 인식하는 단계는,상기 판단결과, 상기 동적 핸드 제스처인 경우라면, 핸드 사인 모드와 핸드 다이나믹 모드로 구분해 처리하는단계를 포함하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 핸드 사인 모드와 핸드 다이나믹 모드로 구분해 처리하는 단계는,상기 핸드 사인 모드인 경우, 핸드 사인으로서 사용자로부터 특정 문자를 입력받아 CNN 기법을 사용해 인식하는단계; 및상기 인식된 정보를 기반으로 제어모드를 선별하여, 상기 선별된 제어모드에 대한 명령어를 생성하는 단계를 포함하는 인공지능 기반의 지능형 신체 인터페이스 방법.공개특허 10-2024-0128197-3-청구항 6 제4항에 있어서,상기 핸드 사인 모드와 핸드 다이나믹 모드로 구분해 처리하는 단계는,상기 핸드 다이나믹 모드인 경우, 시간적으로 특정 움직임을 지속적으로 하는 손의 동적 움직임을 인식하는 단계;상기 인식된 정보를 기반으로 LSTM을 사용해 사용자 제스처를 인식하는 단계; 및상기 인식된 제스처에 상응하는 제어모드를 선별하여, 상기 선별된 제어모드에 대한 명령어를 생성하는 단계를포함하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 핸드 데이터를 처리하는 단계는상기 입력된 영상으로부터 식별되는, 사용자의 손에 대한 특징점을 추출하는 단계;상기 추출된 특징점에 의한 궤적정보를 생성하는 단계;상기 생성한 궤적정보를 정규화하여 학습 데이터를 생성하는 단계;상기 생성된 학습 데이터를 이용하여 움직임 인식을 위한 인공지능 모델을 학습시키는 단계;상기 학습된 인공지능 모델을 이용하여, 상기 사용자의 움직임을 식별하는 단계를 포함하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 영상으로부터 사용자의 손에 대한 특징점을 추출하는 단계는,상기 영상을 샘플링하여, 복수의 프레임을 획득하는 단계; 및상기 복수의 프레임에 대해 사용자의 손에 대한 복수의 특징점들을 검출하는 단계를 포함하는 인공지능 기반의지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 추출된 특징점에 의한 궤적정보를 생성하는 단계는,상기 추출된 특징점 간의 상호 의존성을 기반으로, 상호 의존성이 임계값 이상인 특징점들의 연결을 통해 상기궤적정보를 생성하는 단계를 포함하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 추출된 특징점 간의 상호 의존성을 기반으로, 상호 의존성이 임계값 이상인 특징점들의 연결을 통해 상기궤적정보를 생성하는 단계는,상기 추출된 특징점을 사전 학습된 그래프 신경망 모델에 입력하여, 상기 그래프 신경망 모델에서 출력하는 상호 의존성을 획득하는 단계를 포함하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,공개특허 10-2024-0128197-4-상기 사용자의 손에 대한 특징점을 추출하는 단계는,상기 사용자의 손위치를 식별하기 위한 신체위치 특징점과, 상기 신체를 구성하는 복수의 관절위치를 식별하는관절위치 특징점을 각각 추출하는 단계;상기 추출된 신체위치 특징점과 상기 추출된 관절위치 특징점을 조합하여 상기 신체에 대한 특징점을 추출하는단계를 포함하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 신체위치 특징점은 다른 신체부위와의 상대적 위치정보를 포함하고, 상기 관절위치 특징점은 동일한 신체부위 내에 포함되는 관절들 간의 상대적 위치정보를 포함하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제7항에 있어서,생성된 학습 데이터들을 증강시키는 단계를 더 포함하고,상기 생성된 학습 데이터를 이용하여 움직임 인식을 위한 인공지능 모델을 학습시키는 단계는,상기 증강된 학습 데이터들을 이용하여, 인공지능 모델을 학습시키는 단계를 포함하는 것을 특징으로 하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 생성된 학습 데이터들을 증강시키는 단계는,상기 학습 데이터를 구성하는 정규화된 궤적 정보를 랜덤하게 변경하여, 학습 데이터를 추가로 생성하는 단계를포함하는 것을 특징으로 하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13항에 있어서,상기 생성된 학습 데이터들을 증강시키는 단계는,상기 학습 데이터를 상기 정규화된 궤적 정보를 사용자 신체의 윤곽 내에서 증강시키는 단계를 포함하는 것을특징으로 하는 인공지능 기반의 지능형 신체 인터페이스 방법."}
{"patent_id": "10-2023-0021095", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공지능 모델을 활용하여 자동으로 콘텐츠 이용이나 게임, 가상환경 등에서 활용할 수 있는 제스처 (사용자의 움직임)를 인식하기 위한 방법 및 시스템에 관한 것으로서, 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 영상을 입력받는 단계, 상기 입력된 영상에 대해, 리눅스 기반의 로봇을 사용하기 위한 초기화 루틴을 수행하는 단계, 상기 초기화 루틴이 수행된 상기 입력된 영상에 대해, 인공지능 모델을 이용하여 상기 핸드 데이터를 처리하는 단계, 상기 처리된 핸드 데이터에 대해 정적 핸드 제스처인지 또는 동적 핸드 제스 처인지 여부를 판단하는 단계, 상기 판단 결과에 따라 제스처를 구분하여 인식하는 단계, 및 상기 인식된 정보를 기반으로 제어모드를 선별하여, 상기 선별된 제어모드에 대한 명령어를 생성하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0021095", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 관련 기술에 관한 것으로, 더욱 상세하게는 인공지능 모델을 활용하여 자동으로 콘텐츠 이 용이나 게임, 가상환경 등에서 활용할 수 있는 제스처(사용자의 움직임)를 인식하기 위한 방법 및 시스템에 관 한 것이다."}
{"patent_id": "10-2023-0021095", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 인공지능, 빅데이터, 가상현실, 증강현실 등의 4차 산업혁명 기술의 발전과 사람들의 문화, 예술에 대한 수요가 높아짐에 인터랙티브 미디어 콘텐츠에 대한 관심이 고조되고 있다 특히 컴퓨터 기술의 급속한 발전에 따라 지능형 인터페이스 기술은 일상생활에서 점차 중요한 역할을 수행하고 있다. 지능형 인터페이스 기술은 사용자의 요구 사항들을 실시간으로 인공지능 기법을 사용하여 인지하여 인터랙션 (interaction) 구현할 수 있다. 인간의 얼굴과 손동작 인식 기술은 인간의 동작 행위 정보에 관해 중요하고 많은 정보들을 포함하고 있기 때문 에 자연스러운 인터랙션을 위한 인터페이스에 대한 연구와 사업화가 많이 진행되고 있으며 다양하게 응용되고 있다. 특히 사용자 중심의 경험이 강조되면서 자연스럽고 편안한 인공지능 기반의 인터페이스에 대한 연구가 활발하게 진행되고 있다. 사용자가 콘텐츠와 상호작용을 하는 과정에서 나타나는 특정한 행동 패턴을 시스템화 하여 구축 된 콘텐츠들의 문제점들을 해결하고 사용자 중심의 양질의 콘텐츠 개발이 되도록 만족도 높은 콘텐츠를 제공을 위해 필수적인 인터페이스 개발이 필요하다. 인터랙티브 콘텐츠 환경에서 몰입감을 높이고 자유로운 상호작용을 제공하기 위한 가장 자연스러운 방법은 사용 자의 손을 이용한 제스처(사용자의 움직임) 인터페이스를 제공하는 것이다. 특히, 손과 손가락의 경우 왼손과 오른손 및 각각의 손가락을 이용할 수 있으므로 다양하고 복잡한 정보까지도 세세하게 표현할 수 있다. 손을 사용한 손가락 인식이나 손의 제스처(사용자의 움직임) 인식에 관한 기존의 연구들은 특화된 센서나 장비 를 요구하거나 낮은 인식률 등의 단점이 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제10-1711736호 \"영상에서 동작 인식을 위한 특징점 추출 방법 및 골격 정보를 이용한 사용자 동작 인식 방법\" (특허문헌 0002) 한국등록특허 제10-2081854호 \"3d edm을 이용한 수어 또는 제스처(사용자의 움직임) 인식 방법 및 장치\" (특허문헌 0003) 한국공개특허 제10-2019-0050639호 \"컨볼루션 신경망에 기반한 제스처(사용자의 움직임) 분류 장치 및 방법\""}
{"patent_id": "10-2023-0021095", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 미디어파이프(mediapipe)와 인공지능 기반의 다양한 손제스처(사용자의 움직임) 인식을 통한 지능형 인터페이스 알고리즘을 제공하는 것을 목적으로 한다. 본 발명은 증강현실, 가상현실, 게임콘텐츠와 로봇 등에서 세밀한 손동작 인식에 활용하기 위한 효율적인 인터 랙션을 제공하는 것을 목적으로 한다."}
{"patent_id": "10-2023-0021095", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 영상을 입력받는 단계, 상기 입력된 영상에 대해, 리눅스 기반의 로봇을 사용하기 위한 초기화 루틴을 수행하는 단계, 상기 초기화 루틴이 수행된 상기 입 력된 영상에 대해, 인공지능 모델을 이용하여 상기 핸드 데이터를 처리하는 단계, 상기 처리된 핸드 데이터에 대해 정적 핸드 제스처인지 또는 동적 핸드 제스처인지 여부를 판단하는 단계, 상기 판단 결과에 따라 제스처를 구분하여 인식하는 단계 및 상기 인식된 정보를 기반으로 제어모드를 선별하여, 상기 선별된 제어모드에 대한 명령어를 생성하는 단계를 포함할 수 있다. 일실시예에 따른 상기 판단 결과에 따라 정적 핸드 제스처 또는 동적 핸드 제스처를 구분하여 인식하는 단계는, 상기 판단결과, 상기 정적 핸드 제스처인 경우라면, 인공지능 모델을 통해 데이터 셋을 학습하여 학습 데이터를 생성하는 단계, 및 상기 생성된 학습 데이터를 KNN 기법을 기반으로 인식하는 단계를 포함할 수 있다. 일실시예에 따른 상기 생성된 학습 데이터를 KNN 기법을 기반으로 인식하는 단계는, 파이프라인 처리를 통해 손 관절과 관련하여 21개 손가락 특징점을 참조하고 KNN을 사용해 상기 정적 핸드 제스처를 구현하는 단계를 포함할 수 있다. 일실시예에 따른 상기 판단 결과에 따라 정적 핸드 제스처 또는 동적 핸드 제스처를 구분하여 인식하는 단계는, 상기 판단결과, 상기 동적 핸드 제스처인 경우라면, 핸드 사인 모드와 핸드 다이나믹 모드로 구분해 처리하는 단계를 포함할 수 있다. 일실시예에 따른 상기 핸드 사인 모드와 핸드 다이나믹 모드로 구분해 처리하는 단계는, 상기 핸드 사인 모드인 경우, 핸드 사인으로서 사용자로부터 특정 문자를 입력받아 CNN 기법을 사용해 인식하는 단계 및 상기 인식된 정보를 기반으로 제어모드를 선별하여, 상기 선별된 제어모드에 대한 명령어를 생성하는 단계를 포함할 수 있다. 일실시예에 따른 상기 핸드 사인 모드와 핸드 다이나믹 모드로 구분해 처리하는 단계는, 상기 핸드 다이나믹 모 드인 경우, 시간적으로 특정 움직임을 지속적으로 하는 손의 동적 움직임을 인식하는 단계, 상기 인식된 정보를 기반으로 LSTM을 사용해 사용자 제스처를 인식하는 단계, 및 상기 인식된 제스처에 상응하는 제어모드를 선별하 여, 상기 선별된 제어모드에 대한 명령어를 생성하는 단계를 포함할 수 있다. 일실시예에 따른 상기 핸드 데이터를 처리하는 단계는 상기 입력된 영상으로부터 식별되는, 사용자의 손에 대한 특징점을 추출하는 단계, 상기 추출된 특징점에 의한 궤적정보를 생성하는 단계, 상기 생성한 궤적정보를 정규 화하여 학습 데이터를 생성하는 단계, 상기 생성된 학습 데이터를 이용하여 움직임 인식을 위한 인공지능 모델 을 학습시키는 단계, 상기 학습된 인공지능 모델을 이용하여, 상기 사용자의 움직임을 식별하는 단계를 포함할 수 있다. 일실시예에 따른 상기 영상으로부터 사용자의 손에 대한 특징점을 추출하는 단계는, 상기 영상을 샘플링하여, 복수의 프레임을 획득하는 단계, 및 상기 복수의 프레임에 대해 사용자의 손에 대한 복수의 특징점들을 검출하 는 단계를 포함할 수 있다. 일실시예에 따른 상기 추출된 특징점에 의한 궤적정보를 생성하는 단계는, 상기 추출된 특징점 간의 상호 의존 성을 기반으로, 상호 의존성이 임계값 이상인 특징점들의 연결을 통해 상기 궤적정보를 생성하는 단계를 포함할 수 있다. 일실시예에 따른 상기 추출된 특징점 간의 상호 의존성을 기반으로, 상호 의존성이 임계값 이상인 특징점들의 연결을 통해 상기 궤적정보를 생성하는 단계는, 상기 추출된 특징점을 사전 학습된 그래프 신경망 모델에 입력 하여, 상기 그래프 신경망 모델에서 출력하는 상호 의존성을 획득하는 단계를 포함할 수 있다. 일실시예에 따른 상기 사용자의 손에 대한 특징점을 추출하는 단계는, 상기 사용자의 손위치를 식별하기 위한 신체위치 특징점과, 상기 신체를 구성하는 복수의 관절위치를 식별하는 관절위치 특징점을 각각 추출하는 단계, 상기 추출된 신체위치 특징점과 상기 추출된 관절위치 특징점을 조합하여 상기 신체에 대한 특징점을 추출하는 단계를 포함할 수 있다. 일실시예에 따른 상기 신체위치 특징점은 다른 신체부위와의 상대적 위치정보를 포함하고, 상기 관절위치 특징 점은 동일한 신체부위 내에 포함되는 관절들 간의 상대적 위치정보를 포함할 수 있다. 일실시예에 따른 생성된 학습 데이터들을 증강시키는 단계를 더 포함하고, 상기 생성된 학습 데이터를 이용하여 움직임 인식을 위한 인공지능 모델을 학습시키는 단계는, 상기 증강된 학습 데이터들을 이용하여, 인공지능 모델을 학습시키는 단계를 포함할 수 있다. 일실시예에 따른 상기 생성된 학습 데이터들을 증강시키는 단계는, 상기 학습 데이터를 구성하는 정규화된 궤적 정보를 랜덤하게 변경하여, 학습 데이터를 추가로 생성하는 단계를 포함할 수 있다. 일실시예에 따른 상기 생성된 학습 데이터들을 증강시키는 단계는, 상기 학습 데이터를 상기 정규화된 궤적 정 보를 사용자 신체의 윤곽 내에서 증강시키는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0021095", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "일실시예에 따르면, 인공지능 기반의 다양한 손제스처(사용자의 움직임) 인식을 통한 지능형 인터페이스 알고리 즘을 제공할 수 있다. 일실시예에 따르면, 증강현실, 가상현실, 게임콘텐츠와 로봇 등에서 세밀한 손동작 인식에 활용하기 위한 효율 적인 인터랙션을 제공할 수 있다"}
{"patent_id": "10-2023-0021095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에 개시되어 있는 본 발명의 개념에 따른 실시예들에 대해서 특정한 구조적 또는 기능적 설명들은 단 지 본 발명의 개념에 따른 실시예들을 설명하기 위한 목적으로 예시된 것으로서, 본 발명의 개념에 따른 실시예 들은 다양한 형태로 실시될 수 있으며 본 명세서에 설명된 실시예들에 한정되지 않는다. 본 발명의 개념에 따른 실시예들은 다양한 변경들을 가할 수 있고 여러 가지 형태들을 가질 수 있으므로 실시예 들을 도면에 예시하고 본 명세서에 상세하게 설명하고자 한다. 그러나, 이는 본 발명의 개념에 따른 실시예들 을 특정한 개시형태들에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 변경, 균등물, 또는 대체물을 포함한다. 제1 또는 제2 등의 용어를 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들 에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만, 예를 들어 본 발명의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관계를 설명하는 표현들, 예를 들어 \"~사이에\"와 \"바로~사이에\" 또는 \"~에 직접 이웃하는\" 등도 마찬가지로 해 석되어야 한다. 본 명세서에서 사용한 용어는 단지 특정한 실시예들을 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의 도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에 서, \"포함하다\" 또는 \"가지다\" 등의 용어는 설시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조 합한 것이 존재함으로 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적 으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미를 갖는 것으로 해석되어야 하며, 본 명세서에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의 미로 해석되지 않는다. 이하, 실시예들을 첨부된 도면을 참조하여 상세하게 설명한다. 그러나, 특허출원의 범위가 이러한 실시예들에 의해 제한되거나 한정되는 것은 아니다. 각 도면에 제시된 동일한 참조 부호는 동일한 부재를 나타낸다.도 1은 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법을 설명하는 도면이다. 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 영상을 입력받아 입력 영상에 대해서 초기화 할 수 있다(단계 101). 다음으로, 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 초기화를 위해, 입력된 영상에 대 해, 리눅스 기반의 로봇을 사용하기 위한 초기화 루틴을 수행할 수 있다. 구체적으로, 본 발명은 핸드 제스처 정보를 인식하기 위해 웹캠을 통한 카메라 영상을 입력으로 사용할 수 있다. 또한 리눅스 기반의 로봇을 사용하기 위하여 초기화 루틴을 동작시킬 수 있다. 특히 PC 윈도우즈 환경에서 리눅 스 기반의 로봇에 손 제스처 정보를 전달하기 위한 방법으로 파이썬의 키모듈을 사용하므로 로봇의 초기화 동작 모드의 설정이 중요하며 이를 위하여 PC 상에서 로봇의 주소로 접근해 리눅스 기반의 노머신(nomachine)을 통해 구현할 수 있다. 초기 모드를 동작시키기 위하여 사용자 제스처 동작이 웹캠을 통해 입력되고 로봇 초기화 동작 이 이상이 없는 경우에 입력 웹캠 영상을 제안한 인공지능 기법을 사용해 왼손과 오른손을 구분하고, 구분된 왼 손과 오른손의 손가락 정보를 추론하고 인식해 정적 제스처와 동적 제스처를 구분하여 인식한다. 초기화된 입력 영상에 대한 정상 동작인 경우라고 판단되는 경우(단계 102), 초기화 루틴이 수행된 상기 입력된 영상에 대해, 인공지능 모델을 이용하여 상기 핸드 데이터를 처리할 수 있다(단계 103). 다음으로, 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 처리된 핸드 데이터에 대해 정적 핸드 제스처인지(단계 104) 또는 동적 핸드 제스처인지 여부를 판단할 수 있다(단계 105). 손짓, 신체 자세 및 표정을 통한 의사 소통을 포함하는 비언어적 의사 소통은 인간과 인간 간의 모든 의사 소통 의 약 2/3를 차지한다. 그 중에서 핸드 제스처는 의사 소통 및 상호 작용에 사용되는 가장 일반적인 신체 언어 범주 중 하나로 일반적 인 감정 정보, 뿐만 아니라 특정 언어적 내용을 표현할 수 있다. 핸드 제스처를 분류하는 방법에는 관찰 가능한 특징(feature)을 기반으로 하고 해석(interpertation)을 기반으 로 분류하는 등 여러 가지 방법이 있다. 첫 번째 범주에서 제스처는 시간적 관계에 따라 두 가지 유형 즉, 정적 제스처(static gesture)와 및 동적 제스처(dynamic gesture)로 구분된다. 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 판단 결과에 따라 정적 핸드 제스처 또는 동 적 핸드 제스처를 구분하여 인식할 수 있다. 또한, 인식된 정보를 기반으로 제어모드를 선별하여, 선별된 제어모드에 대한 명령어를 생성할 수 있다. 구체적으로, 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 판단 결과에 따라 정적 핸드 제 스처 또는 동적 핸드 제스처를 구분하여 인식하기 위해, 단계 104의 판단결과 정적 핸드 제스처인 경우라면, 인 공지능 모델을 통해 데이터 셋을 학습하여 학습 데이터를 생성하고, 상기 생성된 학습 데이터를 KNN 기법을 기 반으로 하여(단계 110), 제스처를 인식할 수 있다(단계 111). 일례로, 생성된 학습 데이터를 KNN 기법을 기반으로 인식하기 위해서는 파이프라인 처리를 통해 손 관절과 관련 하여 21개 손가락 특징점을 참조하고 KNN을 사용해 상기 정적 핸드 제스처를 구현할 수 있다. KNN기법은 최근접 알고리즘이라고 불리며 분류 알고리즘으로 새롭게 입력된 데이터에 대하여 기존에 저장된 데 이터와 정보를 비교하여 새롭게 입력된 데이터의 정보를 판단하는 알고리즘이다. n개의 특성을 가진 데이터는 유사한 특성을 가진 데이터들끼리는 거리를 구해 분류할 수 있다. 이때 분류를 알 수 없는 데이터의 경우 가장 가까운 이웃 k개의 분류를 확인하여 결정할 수 있으며 분류기의 효과를 높이기 위 해 파라미터를 조정할 수 있으며 본 발명에서는 작성한 데이터 정확도 값을 고려하여 k=3으로 설정해 알고리즘 을 구현한다. 정적 핸드 제스처(손 자세/손 포즈)는 제스처 기간 동안 손 위치가 변경되지 않는 제스처로 주로 손가락의 모양 과 굴곡 각도 정보에 의존한다. 동적 핸드 제스처에서 손 위치는 시간과 관련하여 지속적으로 변경되며 일반적 으로 준비, 스트로크 및 수축의 세 가지 동작 단계가 있으며 모양과 손가락의 각도 외에도 손 궤적과 방향 정보 에 의존한다. 두 번째 범주에서 제스처는 해석된 의미에 따라 분류하는 방법으로 예를 들어 엠블럼(emblems)은 말하는 대신 엄지손가락을 치켜세우는 것으로 말을 대체할 수 있는 제스처이며, 일러스트레이터(illustrators)는 가리키는 방법으로 방향 제공 등의 말을 설명하는 데 사용되는 제스처 같은 것이 있다. 제스처가 인식된 것으로 판단되면, 게임 & 로봇 제어모드를 위한 동작을 수행할 수 있다(단계 112). 예를 들어, 인식된 정보를 기반으로 제어모드를 선별하여, 상기 선별된 제어모드에 대한 명령어를 생성하는 과정을 수행할 수 있다. 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 단계 105의 판단 결과 동적 핸드 제스처인 경 우라면, 핸드 사인 모드와 핸드 다이나믹 모드로 구분해 처리할 수 있다. 본 발명에서는 사용자가 핸드 제스처를 통해서 특정한 동작을 하면 PC의 웹 카메라가 이를 입력영상으로 사용한 다. 입력영상은 제스처 인식을 위해 인공지능 알고리즘으로 동작을 추론하여 인식한다. 이때 사용하는 알고리즘은 핸드 제스처에 따라 KNN(K-Nearest Neighbor), LSTM(Long Short Term Memory), CNN(Convolutional Neural Network)을 사용한다. 인식된 사용자 핸드 제스처 정보는 키 이벤트 입력을 사용하여 로봇의 리눅스 기반 노머신(nomachine)상에 전달 되어 로봇 동작을 제어하게 된다. 모니터 화면은 로봇 비전카메라 영상과 자체 제작한 2D 탑뷰(top view) 레이싱(racing)게임, PC 웹캠 영상, PC 주피터 랩(Jupyter lab), 로봇 주피터 랩으로 구성되어 있다. 이때 사용되는 주피터 랩은 코딩 및 실행을 위한 개발 도구이며, PC 웹캠의 입력영상을 인공지능 방법을 적용하 여 추론과 인식 및 정보 전달을 위한 루틴으로 구성되어 있다. 로봇이 입력 받은 제스처 인식 정보를 적용하여 직진, 후진, 정지, 출발, 회전, 좌회전, 우회전, 'S'자 이동 등 다양하고 세밀한 사용자 핸드 제스처 인식 결과를 수행하는 그림이다. 핸드 사인 모드와 핸드 다이나믹 모드로 구분해 처리하기 위해, 단계 106의 판단 결과 핸드 사인 모드인 경우, 핸드 사인으로서 사용자로부터 특정 문자를 입력받아 CNN 기법을 사용해(단계 113) 제스처를 인식할 수 있다(단 계 114). 본 발명에서는 'S' 입력 시 게임상의 로봇이 'S'자 형태의 움직임을 구현하며 로봇도 'S' 형태로 자동 이동한다. 또한 다른 알파벳입력 예를 들어 'a' 형태의 인 경우도 인식해 각각 미리 정해진 루틴에 따라 게임과 로봇이 동시에 움직일 수 있다. 이때 사용자는 카메라를 통하여 로봇의 움직임과 PC 모니터 화면에 나타난 게임 의 가상객체의 움직임을 동시에 모니터링하여 동작을 확인할 수 있다. 다음으로, 제스처가 인식된 것으로 판단되면, 게임 & 로봇 제어모드를 위한 동작을 수행할 수 있다(단계 115). 핸드 사인 모드와 핸드 다이나믹 모드로 구분해 처리하기 위해, 단계 107의 판단 결과 핸드 다이나믹 모드인 경 우, 시간적으로 특정 움직임을 지속적으로 하는 손의 동적 움직임을 인식할 수 있다. 이를 위해, 인식된 정보를 기반으로 LSTM을 사용해(단계 116) 사용자 제스처를 인식할 수 있다(단계 117). 구체적으로, 핸드 다이나믹 모드인 경우엔 손을 시간적으로 특정 움직임을 지속적으로 하는 경우로 'come', 'go', 'rotate' 등의 움직임 명령을 내릴 수 있으며 LSTM을 사용해 사용자 제스처를 인식해 게임상의 객체와 로 봇이 동일한 행동을 수행한다. 한편, 다이나믹 제스처의 경우 가상세계와 실세계와의 움직임을 동시에 구현했는데 동기를 맞추기 위해서 미세 조정이 필요한 경우가 있다. 예를 들어 제작한 레이싱 게임의 메인 캐릭터는 코너를 돌 때 적절한 회전 각을 유지하며 돌 수 있으나, 실험에 사용한 3개의 옴니휠을 사용한 로봇의 경우엔 바닥의 기울어짐이나 왼쪽과 오른쪽의 회전 반경 차이로 인하여 에러가 발생할 수 있다. 이를 해결하기 위해 카운터를 사용해 회전량을 작게 나누는 루틴을 추가할 수 있다. 이 같이 사용자의 정적 제 스처, 동적 제스처 등의 핸드제스처를 인공지능 기법을 사용해 추론, 인식하고 인식된 제스처 정보를 실험을 위 해 제작한 레이싱 게임과 로봇에 적용하여 제안한 인공지능 기반 지능형 인터페이스의 성능을 확인한다. 또한, 인식된 제스처에 상응하는 제어모드를 선별하여, 선별된 제어모드에 대한 명령어를 생성할 수 있다(단계 118). 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 단계 108을 통해 전반적인 상황을 평가하여 추가적으로 새로운 제어가 필요한지 여부를 판단할 수 있다(단계 109). 인공지능에 기반한 실시간 핸드 제스처 인식 정보를 활용한 지능형 인터페이스 알고리즘을 제공할 수 있다. 이는 기능적으로 사용자 핸드 제스처의 추적 및 인식을 미디어파이프와 KNN, LSTM, CNN의 인공지능 기법을 사용 해 다양한 동작을 빠르고 지능적으로 인식되는 인터페이스 방법이다. 제안한 알고리즘 성능 평가를 위해 자체 제작한 2D 탑뷰 레이싱 게임과 로봇제어에 적용할 수 있다. 알고리즘 적용결과 게임의 가상 객체의 다양한 움직임을 세밀하고 강건하게 제어할 수 있었으며, 실세계의 로봇 제어에 적용한 결과 이동과 회전, 정지, 좌회전, 우회전 등의 움직임 제스처 정보에 따라 효과적인 제어가 가능 하다. 또한 게임의 메인 캐릭터와 실세계 로봇을 동시에 제어하여 가상과 현실의 공존공간 상황 제어를 위한 지능형 인터페이스로 최적화된 동작도 구현하다. 본 발명을 통해 동작하는 알고리즘은 신체를 활용한 자연스럽고 직관적 특성과 손가락의 미세한 움직임 인식에 따른 정교한 제어가 가능하며, 빠른 시간내에 숙련되는 장점이 있어 지능형 사용자 인터페이스 개발을 위한 기 본자료로 활용될 수 있다. 도 2는 일실시예에 따른 핸드 데이터를 처리하는 방법을 설명하는 도면이다. 본 발명은 인공지능에 기반한 실시간 핸드 제스처(사용자의 움직임) 인식 정보를 활용한 사람과 컴퓨터 간의 지 능형 인터페이스 알고리즘을 구현할 수 있다. 특히, 지능형 인터페이스를 인터랙티브 미디어 콘텐츠와 실세계 로봇 제어에 적용이 가능하며, 기능적으로 사용 자 핸드 제스처(사용자의 움직임)의 추적 및 인식을 미디어파이프와 인공지능 기법을 사용해 다양한 동작을 빠 르게 지능적으로 인식되는 인터페이스를 제공할 수 있다. 또한, 본 발명은 탑뷰 레이싱 게임과 로봇제어 등의 인터페이스에 적용될 수 있고, 게임의 가상 객체의 다양한 움직임을 세밀하고 강건하게 제어할 수 있으며, 동시에 원격으로 로봇제어에 적용해 이동과 회전, 정지, 유턴 등의 움직임을 효과적으로 제어할 수 있다. 즉, 게임 콘텐츠와 원격 로봇을 동시에 제어 가능하며 또한 가상과 현실의 공존공간 상황 제어를 위한 지능형 인터페이스로 최적화된 동작도 구현이 가능하다. 이를 위해, 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 영상으로부터 사용자의 손에 대한 특징점을 추출할 수 있다(단계 201). 영상은 사용자가 실시간을 촬영하여 스트리밍 방식으로 전송되거나, 촬영 후 저장되어 파일 형태로 전송되는 콘 텐츠를 포함할 수 있다. 또한, 복수의 정지영상(이미지)들의 형태일 수도 있다. 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 추출된 특징점에 의한 궤적정보를 생성할 수 있다(단계 202). 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 사용자의 손에 대한 특징점을 추출할 수 있다. 이를 위해, 인공지능 기반의 지능형 신체 인터페이스 방법은 신체의 특징점들의 연관관계를 고려하여 연결한 가 상의 선으로서, 궤적정보의 예시로 해석될 수 있다. 본 발명의 실시예에 따르면, Depth 정보 없이 RGB 이미지 만을 이용하며, 적은 양의 학습 데이터로 환경과 조건 에 영향을 받지 않는 강인한 고성능의 제스처(사용자의 움직임) 인식을 수행하고자, 딥러닝 기반으로 사용자 움 직임에 대한 제스처(사용자의 움직임)를 자동으로 인식할 수 있다. 본 발명의 실시예에서는 사용자 움직임에 대한 자동 인식을 상정하여 설명하는데, 제스처(사용자의 움직임)의 일예에 해당하고, 제스처(사용자의 움직임)를 인식하는 다양한 경우에도 본 발명의 기술적 사상이 적용될 수 있 다. 본 발명의 실시예에서는 입력 영상에서 사용자 인식에 필요한 주요 궤적들을 추출하고, 주요 궤적 별로 구분하 여 정규화한 후에, 궤적정보의 신뢰도(궤적정보가 주요 윤곽을 구성하는 특징점으로써의 신뢰도)를 부가하여 학 습 데이터를 생성한다. 또한, 본 발명의 실시예에 따른 학습 방법에서는, 주요 윤곽 영역(주요 윤곽을 포함하는 사각형 영역)들에 대한 특징점들을 부가하여 학습 데이터를 확장하고, 확장된 학습 데이터를 증강(Augmentation)하여 학습 데이터의 양 을 증가시킨다. 구체적으로는, 입력되는 영상에서 사용자의 손부위에 대한 윤곽을 추출하고, 추출된 신체별 윤곽에서 사용자 인 식에 필요한 주요 윤곽들을 추출한다. 주요 윤곽들의 영역들은 일부 또는 전부가 중첩될 수 있다. 예를 들어, 눈코입 윤곽은 얼굴 윤곽에 전부가 중첩 되고 손 윤곽은 눈코입 윤곽은 얼굴 윤곽에 일부가 중첩되는 형태일 수 있다. 본 발명의 실시예에 따른 학습 방법에서는, 주요 궤적별 평균좌표를 연산할 수 있다. 주요 윤곽은 다수의 궤적정보들로 구성될 수 있고, 궤적정보들은 주요 윤곽을 도출하기 위해 추출한 특징점들에 대한 정보들에 해당한다. 궤적정보는 특징점에 대한 [x 좌표, y 좌표, 신뢰도]의 형태로 구성될 수 있다. 주요 윤곽들이 추출되면, 주요 윤곽 별로 구분하여 궤적정보들의 좌표 정보들을 정규화하고, 궤적정보의 신뢰도 를 학습 데이터에 포함시킬지 여부를 결정하여, 궤적정보를 학습 데이터로 생성할 수 있다. 정규화의 경우, 눈코입 윤곽에 대한 궤적정보들만을 이용하여 눈코입 윤곽의 궤적정보들에 대한 정규화가 이루 어지고, 얼굴 윤곽에 대한 궤적정보들만을 이용하여 얼굴 윤곽의 궤적정보들에 대한 정규화가 이루어지고, 손 윤곽에 대한 궤적정보들만을 이용하여 손 윤곽의 궤적정보들에 대한 정규화가 이루어지는 형태로 구현될 수 있 다. 궤적정보를 학습 데이터로 생성하는 과정은, 궤적정보들에 대한 통계적 특성에 대해 주요 윤곽을 기준으로 분석 하고, 이를 기반으로 궤적정보들을 가공하는 전처리로, 딥러닝을 위한 학습 데이터로 변환하는 과정에 해당한다. 학습 데이터에 궤적정보의 신뢰도를 포함시킬 수도 있다. 예를 들어, 신뢰도의 포함여부는 사용자의 설정을 기초로 결정할 수 있고, 궤적정보의 신뢰도가 부가되면 학습 데이터는 [정규화된 x 좌표, 정규화된 y 좌표, 신뢰도]가 되고, 궤적정보의 신뢰도가 부가되지 않으면 학습 데 이터는 [정규화된 x 좌표, 정규화된 y 좌표]의 형태로 구현될 수 있다. 특징점 추출은 CNN(Convolutional Neural Network)을 이용하여 주요 윤곽 영역(주요 윤곽을 포함하는 사각형 영 역)들 각각에 대해 특징점들을 추출할 수 있다. 이를 위해, 본 발명에서는 주요 궤적 영역을 설정할 수 있다. 또한, 주요 궤적 영역 이미지를 크롭하고, CNN 기반의 특징벡터를 추출할 수 있다. 구체적으로, 본 발명에서는 눈코입 윤곽 영역에서 특징점들을 추출하고, 얼굴 윤곽 영역에서 특징점들을 추출하 며, 손 윤곽 영역에서 특징점들을 추출할 수 있다. 다음으로, 생성된 학습 데이터에 추출한 특징점을 부가하여, 학습 데이터를 확장할 수 있다. 특징점을 부가하 는 것은 사용자 인식 성능을 향상을 위한 것으로, 이는 환경과 편차에 강인한 사용자 인식에 도움이 된다. 학습 데이터에 부가할 특징점을 생성하기 위해, 주요 윤곽 영역들 각각에서 CNN을 이용하여 특징점(CNNs Feature)들을 추출할 수 있다. 또한, 궤적정보들을 참조하여 RGB 이미지인 입력 영상(I)에서 주요 윤곽 영역들을 설정할 수 있다. 각 주요 윤 곽들에 해당하는 좌표들에서 각각 최소 x 좌표, 최대 x 좌표, 최소 y 좌표, 최대 y 좌표를 찾아내면, 이들을 연 결하는 사각형으로 주요 윤곽 영역들을 설정할 수 있다. 다음으로, 설정된 주요 윤곽 영역들에 해당하는 이미지 영역을 Crop한 후, 이미지 처리용 딥러닝 엔진의 일종인 CNN을 이용하여 특징점인 d차원의 CNN Feature 벡터를 추출할 수 있다. 다음으로, 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 생성한 궤적정보를 정규화하여 학 습 데이터를 생성할 수 있다(단계 203).입력되는 영상에서는 사용자의 골격에 의한 특징점이 추출될 수 있다. 또한, 추출된 특징점을 서로 연결하면 궤적정보를 생성할 수 있다. 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 생성된 학습 데이터를 이용하여 움직임 인식 을 위한 인공지능 모델을 학습시킬 수 있다(단계 204). 인공지능 기반의 지능형 신체 인터페이스 방법은 생성된 궤적정보들을 정규화하여 학습 데이터들을 생성하며, 생성된 학습 데이터들을 이용하여 제스처(사용자의 움직임) 인식을 위한 인공지능 모델을 학습할 수 있다. 또한, 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 학습된 인공지능 모델을 이용하여, 사 용자의 움직임을 식별할 수 있고(단계 205), 식별된 사용자의 움직임에 대응되는 명령어를 생성할 수 있다(단계 206). 학습된 인공지능 모델에 따라, 사용자의 움직임이 식별될 수 있다. 예를 들어, 사용자의 움직임은 영상의 형태로 생성되고, 생성된 영상으로부터 특징점을 추출하고, 추출된 특징 점들을 연결하여 궤적정보를 생성할 수 있다. 이렇게 생성된 궤적정보는, 인공지능 모델의 입력을 통해 가장 유사한 패턴의 궤적에 매칭될 수 있다. 또한, 매칭된 궤적에는 적어도 하나 이상의 명령어가 매칭될 수 있어, 최종적으로 사용자의 움직임에 상응하는 명령어가 생성될 수 있다. 이를 위해, 인공지능 기반의 지능형 신체 인터페이스 방법은 영상으로부터 사용자의 손에 대한 특징점을 추출하 기 위해, 영상을 샘플링 하여 복수의 프레임을 획득할 수 있다. 또한, 복수의 프레임에 대해 사용자의 손에 대한 복수의 특징점들을 검출할 수 있다. 인공지능 기반의 지능형 신체 인터페이스 방법은 추출된 특징점에 의한 궤적정보를 생성하기 위해, 추출된 특징 점 간의 상호 의존성을 기반으로, 상호 의존성이 임계값 이상인 특징점들의 연결을 통해 궤적정보를 생성할 수 있다. 또한, 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 추출된 특징점 간의 상호 의존성을 기 반으로, 상호 의존성이 임계값 이상인 특징점들의 연결을 통해 궤적정보를 생성할 수 있다. 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 추출된 특징점을 사전 학습된 그래프 신경망 모델에 입력하여, 상기 그래프 신경망 모델에서 출력하는 상호 의존성을 획득할 수 있다. 각각의 특징점들은 인접한 특징점들 중에서 상호 의존성이 높은 특징점을 선택하여, 연결될 수 있다. 또한, 이 러한 연결이 이어져 궤적정보를 생성할 수 있다. 궤적정보들은, 신체의 윤곽 또는 지시하는 방향 등을 도출하기 위해 추출한 특징점들에 대한 정보들일 수 있다. 그리고, 특징점들에 대한 정보들은, 특징점들의 좌표 정보들을 포함할 수 있다. 또한, 학습 데이터를 생성하는 단계는 궤적 별로, 궤적을 구성하는 궤적정보들의 좌표 정보들에 대한 평균과 표 준 편차를 이용한 연산으로, 궤적정보들을 정규화할 수 있다. 그리고, 학습 데이터 생성단계는 정규화된 좌표 정보들에 윤곽 정보들의 신뢰도 정보들을 각각 포함시켜, 학습 데이터들을 생성할 수 있다. 본 발명의 실시예에 따른 학습 방법은 신체의 궤적들을 포함하는 영역들 각각에서 특징점들을 추출하고, 추출한 특징점들을 생성된 학습 데이터들에 각각 부가할 수 있다. 또한, 특징점들이 부가된 학습 데이터들을 이용하여, 인공지능 모델을 학습시킬 수 있다. 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법은 사용자의 손에 대한 특징점을 추출하고, 사용 자의 손위치를 식별하기 위한 신체위치 특징점과, 신체를 구성하는 복수의 관절위치를 식별하는 관절위치 특징 점을 각각 별도로 추출할 수 있다. 추출된 신체위치 특징점과 추출된 관절위치 특징점을 조합하여 신체에 대한 특징점을 추출할 수 있다. 예를 들어, 신체위치 특징점은 손, 발, 얼굴, 다리, 팔과 같이 신체를 구성하는 일부분을 식별하기 위한 정보이 고, 관절위치 특징점은 신체위치에서 보다 구체적인 위치를 나타내기 위한 정보이다. 즉 사용자의 움직임으로부터 손의 위치나 방향을 판별하기 위해서는, 신체위치 특징점이 이용되고, 손이 가리키 는 구체적인 방향, 손 모양 등에 대해서는 관절위치 특징점이 이용될 수 있다. 또한, 신체위치 특징점은 다른 신체부위와의 상대적 위치정보를 포함하고, 관절위치 특징점은 동일한 신체부위 내에 포함되는 관절들 간의 상대적 위치정보를 포함할 수 있다. 일례로, 인공지능 기반의 지능형 신체 인터페이스 방법은 생성된 학습 데이터들을 증강시킬 수 있다. 또한, 생성된 학습 데이터를 이용하여 움직임 인식을 위한 인공지능 모델을 학습시키기 위해서는, 증강된 학습 데이터들을 이용하여, 인공지능 모델을 학습시킬 수 있다. 또한, 인공지능 기반의 지능형 신체 인터페이스 방법은 생성된 학습 데이터들을 증강시키기 위해서, 학습 데이 터를 구성하는 정규화된 궤적 정보를 랜덤하게 변경하여, 학습 데이터를 추가로 생성할 수 있다. 이 경우, 생성된 학습 데이터들을 증강시키기 위해서는, 학습 데이터를 정규화된 궤적 정보를 사용자 신체의 윤 곽 내에서 증강시킬 수 있다. 이하에서는, 본 발명을 적용한 손 제스처 인식에 따른 다양한 실험 및 그에 따른 결과를 설명한다. 도 3는 정적 제스처 실험을 나타내는 도면이다. 도 3의 정적 제스처 실험에서는, 지능형 인터페이스 실험을 하기 위한 환경 설정과 사용자 정적 제스처 입력을 KNN과 미디어파이프를 사용해 위치 및 움직임 정보를 추론하여 제작한 게임에 적용해 메인 캐릭터인 로봇의 움 직임을 제어한다. 도 3의 정적 제스처 실험은 사용자의 직진(FORWARD), 후진(BACKWARD)의 정적 제스처를 인식하고 게임에 적용해 메인 캐릭터를 앞과 뒤로 움직이는 확인하는 실험이다. 도면부호 401은 게임의 초기상태를 나타내며 도면부호 402는 사용자가 왼손은 모두 펴고 동시에 오른손은 엄지 와 중지를 접고 나머지 손가락은 편 상태를 나타낸다. 도면부호 402를 살펴보면 인식한 결과를 왼손 아래 \"FORWARD\"로 나타내었으며 게임 메인 캐릭터의 이동 속도를 구분하여(최대 3) 표시한 것이다. 즉, 도 3의 실험은 웹캠 입력 영상에서 본 발명을 사용하여 왼손과 오른손 구분과 손가락 정적 제스처를 인식하 여 앞으로 직진(FOWARD, STRAIGHT)하는 모드를 잘 인식한 실험이다. 이 인식 결과를 게임에 적용해 메인 캐릭터의 초기 상태 위치인 도면부호 410에서 앞으로 직진모드 인식을 통해 이동한 결과를 도면부호 403에 나타내었다. 도면부호 404는 사용자의 오른손은 동일한 형태이며 왼손은 검지와 소지를 세운 모드로 변경한 제스처를 인식해 뒤로 후진(BACKWARD, STRAIGHT)으로 인식한 결과이다. 도면부호 405는 인식한 제스처 후진모드를 적용하여 메인 캐릭터를 뒤로 위치 이동을 한 결과 그림이다. 게임 속 메인 캐릭터의 위치를 확인해 보면 도면부호 403보다 뒤 위치로 이동한 내용을 확인할 수 있다. 본 실험은 본 발명에 따른 지능형 인터페이스 알고리즘으로 직진, 후진 제스처를 잘 추론, 인식하였고, 인식한 결과를 게임에 적용해 잘 이동된 결과를 확인할 수 있다. 도 4는 사용한 LSTM 모델의 정확도와 손실을 나타내는 도면이다. 시간의 흐름에 따른 지속적인 손의 움직임을 감지하고 감지한 내용을 분석, 추론해 특정한 의미를 갖는 정보로 인식하는 방법을 LSTM 기법을 사용한 알고리즘을 사용해 인식하고, 인식한 결과를 제작한 레이싱 게임에 실시간 으로 적용해 그 성능을 실험할 수 있다. 사용자의 다이나믹 핸드 제스처는 3개의 의미있는 내용 {\"GO\", \"COME\", \"ROATATE\"} 등의 3개 제어 명령어로 구 성하여 실험을 진행한다. 데이터 셋은 사용자의 3가지 제스처를 각 제스처마다 30초씩 랜드마크 점의 위치와 각도를 계속해서 녹화하여 넘파이 배열 형태로 저장한다. 저장한 데이터셋에서 트레인데이터(train_data)와 테스트데이터(test_data)를 9:1 비율로 나눈다. 모델을 제작하기 위해 텐서플로우(tensorflow.keras.models) 모듈의 시퀀셜(Sequential)클래스를 사용하고, 덴스레이어(Dense Layer)에서 3개의 클래스를 구분하기 위해 출력(output)을 3으로 설정, 소프트맥스(softmax) 활성화함수를 사용, 컴파일(compile)에서 손실(loss)을 다중 클래스 분류 함수 'categorical_crossentropy'로 설정하여 모델을 학습할 수 있다. 이 결과, 도면부호 500에서 보는 바와 같이 사용한 LSTM 모델의 정확도와 손실(Accuracy & Loss)을 나타낸다. 도 5는 다이나믹 핸드 제스처 COME 모드를 LSTM으로 인식하고 레이싱 게임에 적용한 결과 영상을 나타내는 도면 이다. 본 실험은 사용자의 다이나믹 핸드 제스처 COME 모드를 LSTM으로 인식하고 레이싱 게임에 적용하는 실험으로 도 면부호 601은 처음 시작할 때의 게임 영상으로 도면부호 601의 게임 배경 맵과 9개의 블락으로 구성되어 있고 블락 [2,2]의 위치에 메인 캐릭터인 로봇이 있다. 이때 왼손 손가락을 위로 모으고 앞뒤로 흔들면 웹캠에서 움직임(도면부호 601, 602, 604, 606 참조)을 감지하 고 이 움직임 정보를 미디어파이프를 사용해 손가락의 랜드마크의 위치를 인식해 표시하고(흰색선과 빨간색 포 인트)이 좌표정보를 분석해 주기적이고 일정한 패턴을 유지하고 있는 조건을 만족하는지를 추론해 최종 판단한 다. 이 경우엔 제어 모드 클래스에서 \"COME\"모드로 인식한다. 최종인식이 된 경우엔 사용자 왼손의 아래쪽에 \"COM E\"이라는 인식 결과를 출력하며 동시에 게임 영상에서도 블락 우측에 COME이라는 인식 결과를 출력한다(도면부 호 604, 606 참조). 추론된 최종 결과에 따라 레이싱 게임속의 로봇은 위쪽으로 이동하며 지속인(도면부호 605 참조) \"COME\" 모드가 인식되는 경우 이동을 계속한다. \"COME\" 모드 실험을 위해 다양한 여러 각도로 손을 이동 해 COME 모드를 실험한 경우 비교적 안정적으로 \"COME\" 모드를 잘 인식하였다. 도 6은 학습한 CNN 모델 정확도를 설명하는 도면이다. 손 제스처는 비언어적 의사 소통의 한 형태로 컴퓨터가 사용자의 의도를 추론해 해석하고 그에 따라 응답할 수 기술이다. 특히 핸드 사인 제스처는 손가락을 사용하여 특정한 정보를 생성해 특정 목적을 위하여 사용하는 제 스처로 정의한다. 본 실험에서는 일반적으로 이해될 수 있는 형태의 문자를 손가락으로 생성하고 생성된 문자정 보를 CNN 방법을 사용하여 추론하고 인식한다. 인식된 결과는 게임의 메인 캐릭터의 움직임에 적용하여 제안한 방법의 성능을 실험한다. CNN(Convolutional Neural Networks)은 이미지를 분석하기 위한 패턴을 찾는데 유용한 알고리즘이며, 이미지를 직접 학습하고 패턴을 사용해 이미지를 분류한다. 1차원 형태의 데이터를 사용하는 CNN은 이미지를 입력하는 과정에서 이미지의 공간적 정보의 손실로 인한 특징 추출과 학습의 비효율적이고 정확도의 한계가 발생하는 문제가 있다. 이러한 한계점을 보완한 해결책이 CNN이며, CNN의 핵심적인 개념은 이미지의 공간정보를 유지하며 학습하는 것으로 특징을 추출하는 Convolution Layer와 출력된 특징 지도를 압축하여 특정 데이터를 강조하는 Pooling Layer를 복합적으로 구성하여 알고리즘 을 만든다. CNN 모델 학습 과정으로 텐서플로우(tensorflow.keras.models) 모듈의 시퀀스(Sequential) 클래스를 사용하여 합성곱 신경망을 구성한다. Convolution Layer의 Conv2D에서 입력 이미지 크기 24x24 채널 3개, 필터 크기 3x3 32개, 패딩(padding)은 same으로 하여 출력 이미지 크기와 입력 이미지 크기가 같게 설정하고, 활성화 함수 'relu'를 사용한다. 풀링레이어(Pooling Layer)의 MaxPooling2D에서 풀 크기는 2x2로 설정한다. 풀커넥티드레 이어(Fully-connected Layer)의 덴스(Dense) 층을 추가하여 확률값을 이용해 다양한 클래스를 분류하기 위해 활 성화 함수 소프트맥스(softmax)를 사용한다. 로스 펑션(loss function)은 3개 이상의 클래스를 분류하기 위해 'categorical_crossentropy' 손실함수를 사용하고, 옵티마이저(optimizer)는 아담(adam)으로 설정한다. 모델을 학습시키는 과정에서 에폭(epochs)는 100으로 설정, steps_per_epoch는 트레인 데이터(train_data) 개수 χ 배치사이즈(batch_size) = 114로 설정, validation_steps은 5로 설정한다. 생성된 CNN 모델에 validation_data를 사용하여 평가하여 accuracy 값을 확인한다. 사용한 CNN모델의 정확도와 에러 특성은 도면부 호 700의 그래프와 같다. 도 7은 핸드 사인 제스처 입력정보 'S'를 인식하고 인식한 결과를 게임에 적용하는 실험이다. 도면부호 801은 제작한 게임의 초기 모드로 초록색 방화면에 16개의 직사각형 블락으로 구성되어 있고 입력된 정보에 따라 정해진 움직임을 하기 위한 길(도면부호 801에서는 'S')이 출력되어 있다. 사용자가 손가락으로 입 력정보 'S'를 쓰고(도면부호 802, 803, 804 참조) 도면부호 805 같은 제스처를 사용해 입력 신호를 컴퓨터로 보내면 이 정보를 추론하고 인식해 'S'라는 입력 정보를 게임에 보낸다. 게임 루틴에서는 입력 'S'를 받아 메인 캐릭터인 로봇을 도로 'S'에 맞추어 자연스럽게 이동한다.(도면부호 806~809 참조) 도 8은 정적인 핸드 제스처 인식을 위한 왼손과 오른손의 손가락 제스처의 종류를 설명하는 도면이다. 사용자의 정적인 제스처 종류와 왼손, 오른손 구분 및 손가락의 모양에 따른 전진, 후진, 정지, 좌회전, 우회전 등의 구체적인 정적 제스처 내용을 구분하여 출력한 그림이다. 이 정적 제스처를 제 안 방법으로 추론하고 인식하는 지능형 인터페이스로 활용해 로봇 제어에 적용해 그 성능을 확인할 수 있다. 도 9는 게임 속 로봇 및 실제 로봇의 속도제어를 위한 기준점을 설명하는 도면이다. 게임 속의 로봇 이동과 실 세계의 로봇이동의 정확한 속도 제어를 위하여 엄지와 중지의 끝부분 거리를 사용해 정한다. 속도를 3개의 단계로 구분해 계산하는데 사용자의 위치 및 웹캠의 위치에 따라 변화 손가락 사이의 거 리는 변화하므로 손목점과 엄지와 검지 사이의 비를 불변량으로 정의해 손가락의 크기의 변화에도 정확한 속도 값을 구할 수 있도록 프로그램을 구성할 수 있다. 결국, 본 발명을 이용하면, 실시간 처리와 지능형 인터페이스를 미디어파이프(mediapipe)와 인공지능 기반의 다 양한 손제스처(사용자의 움직임) 인식을 통한 지능형 인터페이스 알고리즘을 제공할 수 있다. 또한, 증강현실, 가상현실, 게임콘텐츠와 로봇 등에서 세밀한 손동작 인식에 활용하기 위한 효율적인 인터랙션 을 제공할 수 있다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 예를 들어, 프로 세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴 퓨터, FPA(field programmable array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상 의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설"}
{"patent_id": "10-2023-0021095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치 는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(signal wave)에 영구적으로, 또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 하나 이상의 컴퓨터 판독 가능 기록 매 체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 실시예의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다."}
{"patent_id": "10-2023-0021095", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같이 실시예들이 비록 한정된 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태로 결합 또 는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2023-0021095", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일실시예에 따른 인공지능 기반의 지능형 신체 인터페이스 방법을 설명하는 도면이다. 도 2는 일실시예에 따른 핸드 데이터를 처리하는 방법을 설명하는 도면이다. 도 3은 정적 제스처 실험을 나타내는 도면이다. 도 4는 사용한 LSTM 모델의 정확도와 손실을 나타내는 도면이다. 도 5는 다이나믹 핸드 제스처 COME 모드를 LSTM으로 인식하고 레이싱 게임에 적용한 결과 영상을 나타내는 도면 이다. 도 6은 학습한 CNN 모델 정확도를 설명하는 도면이다. 도 7은 핸드 사인 제스처 입력정보 'S'를 인식하고 인식한 결과를 게임에 적용하는 실험이다. 도 8은 정적인 핸드 제스처 인식을 위한 왼손과 오른손의 손가락 제스처의 종류를 설명하는 도면이다. 도 9는 게임 속 로봇 및 실제 로봇의 속도제어를 위한 기준점을 설명하는 도면이다."}
