{"patent_id": "10-2022-0013648", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0116565", "출원번호": "10-2022-0013648", "발명의 명칭": "전자 장치 및 전자 장치의 제어 방법", "출원인": "삼성전자주식회사", "발명자": "팜 트룽투안"}}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,디스플레이;제1 카메라; 제2 카메라; 및줌 모드로 동작하는 상기 제1 카메라를 통해 제1 이미지를 획득하고, 상기 제1 이미지를 표시하도록 상기 디스플레이를 제어하고, 일반 모드로 동작하는 제2 카메라를 통해 제2 이미지를 획득하고, 상기 줌 모드에서의 줌인 비율에 기초하여 상기 제1 이미지에 대응되는 상기 일반 모드에서의 상기 제1 카메라의 제3 이미지를 식별하고, 상기 제2 이미지에 포함된 객체를 검출하여, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 획득하고, 상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위치에 기초하여, 상기 제2 이미지로부터 획득된위치 정보에 대응되는 상기 제3 이미지에서의 상기 객체의 위치 정보를 획득하고, 상기 객체의 위치 정보에 기초하여, 상기 제1 이미지에서 상기 객체를 검출하는 프로세서;를 포함하는 전자 장치."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 위치 정보는, 상기 제2 이미지에서 검출된 객체를 포함하는 제1 바운딩 박스의 제1 좌표 값 및 제2 좌표 값을 포함하고, 상기 제1 좌표 값은, 상기 제1 바운딩 박스의 좌상단 모서리의 좌표 값이고, 상기 제2 좌표 값은 상기 제1 바운딩 박스의 우하단 모서리의 좌표 값인, 전자 장치."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 프로세서는, 상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위치에 기초하여, 상기 제1 좌표 값 및 상기 제2 좌표 값과대응하는, 상기 제3 이미지에서의 상기 객체에 대한 제3 좌표 값 및 제4 좌표 값을 각각 식별하고, 상기 줌 모드에서의 줌 인 비율 기초하여, 상기 제3 이미지 내에 상기 제1 이미지에 대응되는 프레임의 크기 및 위치를 식별하고, 상기 프레임 내 상기 제3 좌표 값 및 제4 좌표 값 중 적어도 하나가 포함되는 경우, 상기 제3 및 제4좌표 값에 기초하여, 상기 줌 모드로 동작하는 상기 제1 카메라를 통해 획득된 제1 이미지에서 상기 객체를 검출하는, 정자 장치."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 프로세서는, 상기 줌 인 비율 및 프레임의 크기 및 위치에 기초하여, 상기 제3 좌표 값 및 제4 좌표 값과 대응하는, 상기 제1 이미지에서의 제5 좌표 값 및 제6 좌표 값을 각각 식별하고, 상기 제5 좌표 값 및 제6 좌표 값을 기초로, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스를 생성하여 표시하고, 상기 제5 좌표 값은, 상기 제2 바운딩 박스의 좌상단 모서리의 좌표 값이고, 상기 제6 좌표 값은 상기 제2 바운딩 박스의 우하단 모서리의 좌표 값인, 전자 장치."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2023-0116565-3-제4항에 있어서,상기 프로세서는, 상기 제5 좌표 값 및 상기 제6 좌표 값에 기초하여, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스의 넓이를 식별하고, 상기 식별된 제2 바운딩 박스의 넓이가 기 설정된 값 이상이면, 상기 제2 바운딩박스를 표시하는, 전자 장치."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서, 상기 프로세서는, 상기 제5 좌표 값 및 상기 제6 좌표 값에 기초하여, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스의 넓이를 식별하고, 제2 바운딩 박스의 넓이가 기 설정된 값 미만이면, 상기 제1 바운딩 박스를 기초로 상기 제2 이미지 내 객체 이미지를 식별하고, 상기 식별된 객체 이미지를 상기 제1 이미지에 매칭하여 상기제1 이미지에서 상기 객체를 검출하는, 전자 장치."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4항에 있어서, 상기 프로세서는, 상기 제2 이미지에 포함된 상기 객체에 대한 특징 점을 추출하고, 상기 추출된 특징 점에 기초로 상기 객체에대한 객체 인식 정보를 획득하고, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스 및 상기객체에 대한 객체 인식 정보를 표시하는, 전자 장치."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 저장하는 메모리를 더 포함하고, 상기 프로세서는,상기 제3 이미지에서의 상기 객체 이외의 새로운 객체가 검출되면, 상기 제3 이미지에서 상기 검출된 상기 새로운 객체에 대한 위치 정보를 획득하고, 상기 위치 정보를 기초로, 상기 메모리에 저장된 위치 정보를 업데이트하는, 전자 장치."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 프로세서는, 상기 제1 카메라와 상기 제2 카메라의 상기 전자 장치에서의 배치된 이격 거리, 상기 제1 카메라의 시야 각 및상기 제2 카메라의 시야 각에 기초하여, 상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위치를 식별하고,상기 식별된 상대적인 위치에 기초하여, 상기 제2 이미지로부터 획득된 위치 정보에 대응되는 상기 제3 이미지에서의 상기 객체의 위치 정보를 획득하는, 전자 장치."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 카메라 및 제2 카메라를 포함하는, 전자 장치의 제어 방법에 있어서, 줌 모드로 동작하는 상기 제1 카메라를 통해 제1 이미지를 획득하고, 상기 제1 이미지를 표시하는 단계;일반 모드로 동작하는 제2 카메라를 통해 제2 이미지를 획득하는 단계;상기 줌 모드에서의 줌 인 비율에 기초하여 상기 제1 이미지에 대응되는 상기 일반 모드에서의 상기 제1 카메라의 제3 이미지를 식별하는 단계;상기 제2 이미지에 포함된 객체를 검출하여, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 획득하공개특허 10-2023-0116565-4-는 단계;상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위치에 기초하여, 상기 제2 이미지로부터 획득된 위치 정보에 대응되는 상기 제3 이미지에서의 상기 객체의 위치 정보를 획득하는 단계; 및상기 객체의 위치 정보에 기초하여, 상기 제1 이미지에서 상기 객체를 검출하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 상기 위치 정보는, 상기 제2 이미지에서 검출된 객체를 포함하는 제1 바운딩 박스의 제1 좌표 값 및 제2 좌표 값을 포함하고, 상기 제1 좌표 값은, 상기 제1 바운딩 박스의 좌상단 모서리의 좌표 값이고, 상기 제2 좌표 값은 상기 제1 바운딩 박스의 우하단 모서리의 좌표 값인, 방법."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서, 상기 제3 이미지에서의 상기 객체의 위치 정보를 획득하는 단계는,상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위치에 기초하여, 상기 제1 좌표 값 및 상기 제2 좌표 값과대응하는, 상기 제3 이미지에서의 상기 객체에 대한 제3 좌표 값 및 제4 좌표 값을 각각 식별하는 단계; 및상기 줌 모드에서의 줌 인 비율 기초하여, 상기 제3 이미지 내에 상기 제1 이미지에 대응되는 프레임의 크기 및위치를 식별하는 단계;를 포함하고, 제1 이미지에서 상기 객체를 검출하는 단계는, 상기 프레임 내 상기 제3 좌표 값 및 제4 좌표 값 중 적어도 하나가 포함되는 경우, 상기 제3 및 제4 좌표 값에기초하여, 상기 줌 모드로 동작하는 상기 제1 카메라를 통해 획득된 제1 이미지에서 상기 객체를 검출하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서, 상기 프레임의 크기 및 위치에 기초하여, 상기 제3 좌표 값 및 제4 좌표 값과 대응하는, 상기 제1 이미지에서의제5 좌표 값 및 제6 좌표 값을 각각 식별하는 단계; 및상기 제5 좌표 값 및 제6 좌표 값을 기초로, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스를 생성하여 표시하는 단계를 포함하고, 상기 제5 좌표 값은, 상기 제2 바운딩 박스의 좌상단 모서리의 좌표 값이고, 상기 제6 좌표 값은 상기 제2 바운딩 박스의 우하단 모서리의 좌표 값인, 방법."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서, 상기 제2 바운딩 박스를 생성하여 표시하는 단계는, 상기 제5 좌표 값 및 상기 제6 좌표 값에 기초하여, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스의 넓이를 식별하는 단계; 및상기 식별된 제2 바운딩 박스의 넓이가 기 설정된 값 이상이면, 상기 제2 바운딩 박스를 표시하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12항에 있어서, 공개특허 10-2023-0116565-5-상기 제2 바운딩 박스를 생성하여 표시하는 단계는, 상기 제5 좌표 값 및 상기 제6 좌표 값에 기초하여, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스의 넓이를 식별하는 단계; 제2 바운딩 박스의 넓이가 기 설정된 값 미만이면, 상기 제1 바운딩 박스를 기초로 상기 제2 이미지 내 객체 이미지를 식별하는 단계; 및상기 식별된 객체 이미지를 상기 제1 이미지에 매칭하여 상기 제1 이미지에서 상기 객체를 검출하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제12항에 있어서, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 획득하는 단계는, 상기 제2 이미지에 포함된 상기 객체에 대한 특징 점을 추출하고, 상기 추출된 특징 점에 기초로 상기 객체에대한 객체 인식 정보를 획득하는 단계를 더 포함하고, 상기 제2 바운딩 박스를 생성하여 표시하는 단계는, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스 및 상기 객체에 대한 객체 인식 정보를 표시하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제9항에 있어서, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 획득하는 단계는,상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 메모리에 저장하는 단계를 더 포함하고, 상기 제3 이미지에서의 상기 객체 이외의 새로운 객체가 검출되면, 상기 제3 이미지에서 상기 검출된 상기 새로운 객체에 대한 위치 정보를 획득하고, 상기 위치 정보를 기초로, 상기 메모리에 저장된 위치 정보를 업데이트하는 단계를 더 포함하는,"}
{"patent_id": "10-2022-0013648", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제9항에 있어서, 상기 제3 이미지에서의 상기 객체의 위치 정보를 획득하는 단계는, 상기 제1 카메라와 상기 제2 카메라의 상기 전자 장치에서의 배치된 이격 거리, 상기 제1 카메라의 시야 각 및상기 제2 카메라의 시야 각에 기초하여, 상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위치를 식별하는단계를 더 포함하는, 방법."}
{"patent_id": "10-2022-0013648", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 전자 장치 및 전자 장치의 제어 방법을 제공한다. 상기 전자 장치는, 디스플레이, 제1 카메라, 제2 카 메라 및 일반 모드로 동작하는 상기 제1 카메라의 모드를 줌 모드로 변경하기 위한 사용자 명령이 수신되면, 상 기 일반 모드로 동작하는 제2 카메라를 통해 제2 이미지를 획득하고, 상기 제2 이미지에 포함된 객체를 검출하여, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 획득하고, 상기 줌 모드에서의 줌 인 비율 및 상기 객체의 위치 정보에 기초하여, 상기 줌 모드로 동작하는 상기 제1 카메라를 통해 획득된 제1 이미지에서 상기 객체를 검출하는 프로세서를 포함한다."}
{"patent_id": "10-2022-0013648", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 전자 장치의 제어 방법에 관한 것이다. 더욱 상세하게는, 전자 장치에 포함된 복수의 카메라를 이용하여, 객체에 관한 위치 정보 및 객체 인식 정보를 획득하고, 이를 디스플레이에 표시하는 전자 장치 및 그 제어 방법에 관한 것이다."}
{"patent_id": "10-2022-0013648", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "모바일 산업과 함께 모바일 기기에 내장되는 카메라 모듈에 관한 기술 또한 끊임없이 발전해왔다. 특히, 전 세 계적으로 스마트폰이 본격적으로 보급된 2010년 이후부터 스마트폰 시장의 성장세가 점점 둔화되고 있으나, 카 메라에 관한 기술은 지속적으로 발전하고 있다. 이러한 이유로 스마트폰을 생산하는 각 기업에서도 스마트 폰 스펙의 차별화를 위한 마지막 단계로 스마트 폰의 카메라의 기술 개발과 이를 통한 고스펙의 카메라를 생산함으로써, 스마트 폰 시장에서의 경쟁력을 강화하고 있다. 예를 들어, 기존의 스마트 폰이 전면과 후면에 각각 하나의 카메라 만을 포함했다면, 최근 출시되는 대다수의 스마트 폰은 화각에 따라 광각 렌즈를 포함하는 카메라, 표준 렌즈를 포함하는 카메라 및 망원 렌즈를 포함하는 카메라 등의 복수의 카메라를 포함하고 있다. 한편, 카메라의 발전과 함께 카메라를 통해 획득하는 이미지를 분석하는 기술 또한 발전하였다. 예를 들어, 이 미지에 포함된 객체를 감지하고, 객체 인식을 수행하여 객체의 유형을 판단하는 것이 이에 해당한다. 다만, 이 러한 기술 발전에도 불구하고, 여전히 이미지에 포함된 객체에 관한 정보가 불충분한 경우(예를 들어, 이미지 내 객체의 전체 형상이 온전히 포함되지 않은 경우)에는, 이미지에 관한 정확한 분석이 어려우며, 궁극적으로 이미지에 포함된 객체의 유형을 정확히 식별하는 것이 불가능하다. 따라서, 이를 해결할 적절한 방안이 요구되 는 실정이다."}
{"patent_id": "10-2022-0013648", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 필요성에 의해 안출된 것으로, 본 개시의 목적은 전자 장치와 그 제어 방법을 제공함에 있다. 다만, 본 발명이 해결하고자 하는 과제는 상기된 바와 같은 과제로 한정되지 않으며, 또다른 과제들이 존재할 수 있다."}
{"patent_id": "10-2022-0013648", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 개시의 일 실시 예에 전자 장치는, 디스플레이, 제1 카메라, 제2 카메라 및 줌 모드로 동작하는 상기 제1 카메라를 통해 제1 이미지를 획득하고, 상기 제1 이미지를 표시하도록 상기 디스플레 이를 제어하고, 일반 모드로 동작하는 제2 카메라를 통해 제2 이미지를 획득하고, 상기 줌 모드에서의 줌 인 비 율에 기초하여 상기 제1 이미지에 대응되는 상기 일반 모드에서의 상기 제1 카메라의 제3 이미지를 식별하고, 상기 제2 이미지에 포함된 객체를 검출하여, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 획득하 고, 상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위치에 기초하여, 상기 제2 이미지로부터 획득된 위치 정보에 대응되는 상기 제3 이미지에서의 상기 객체의 위치 정보를 획득하고, 상기 객체의 위치 정보에 기초하여, 제1 이미지에서 상기 객체를 검출하는 프로세서를 포함한다. 또한, 본 개시의 일 실시 예에 따라, 상기 위치 정보는, 상기 제2 이미지에서 검출된 객체를 포함하는 제1 바운 딩 박스의 제1 좌표 값 및 제2 좌표 값을 포함하고, 상기 제1 좌표 값은, 상기 제1 바운딩 박스의 좌상단 모서 리의 좌표 값이고, 상기 제2 좌표 값은 상기 제1 바운딩 박스의 우하단 모서리의 좌표 값이다. 또한, 본 개시의 일 실시 예에 따라, 상기 프로세서는, 상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위 치에 기초하여, 상기 제1 좌표 값 및 상기 제2 좌표 값과 대응하는, 상기 제3 이미지에서의 상기 객체에 대한 제3 좌표 값 및 제4 좌표 값을 각각 식별하고, 상기 줌 모드에서의 줌 인 비율 기초하여, 상기 제2 이미지 내에 상기 제1 이미지에 대응되는 프레임의 크기 및 위치를 식별하고, 상기 프레임 내 상기 제3 및 제4 좌표 값 중 적어도 하나가 포함되는 경우, 상기 제3 및 제4 좌표 값에 기초하여, 상기 줌 모드로 동작하는 상기 제1 카메라 를 통해 획득된 제1 이미지에서 상기 객체를 검출한다. 또한, 본 개시의 일 실시 예에 따라, 상기 프로세서는, 상기 프레임의 크기 및 위치에 기초하여, 상기 제3 좌표 값 및 제4 좌표 값과 대응하는, 상기 제1 이미지에서의 제5 좌표 값 및 제6 좌표 값을 각각 식별하고, 상기 제5 좌표 값 및 제6 좌표 값을 기초로, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스를 생성 하여 표시하고, 상기 제5 좌표 값은, 상기 제2 바운딩 박스의 좌상단 모서리의 좌표 값이고, 상기 제6 좌표 값 은 상기 제2 바운딩 박스의 우하단 모서리의 좌표 값이다. 또한, 본 개시의 일 실시 예에 따라, 상기 프로세서는, 상기 제5 좌표 값 및 상기 제6 좌표 값에 기초하여, 상 기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스의 넓이를 식별하고, 상기 식별된 제2 바운딩 박스의 넓이가 기 설정된 값 이상이면, 상기 제2 바운딩 박스를 표시한다. 또한, 본 개시의 일 실시 예에 따라, 상기 프로세서는, 상기 제2 이미지에 포함된 상기 객체에 대한 특징 점을 추출하고, 상기 추출된 특징 점에 기초로 상기 객체에 대한 객체 인식 정보를 획득하고, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스 및 상기 객체에 대한 객체 인식 정보를 표시한다. 또한, 본 개시의 일 실시 예에 따라, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 저장하는 메모 리를 더 포함하고, 상기 프로세서는, 상기 제3 이미지에서의 상기 객체 이외의 새로운 객체가 검출되면, 상기 제3 이미지에서 상기 검출된 상기 새로운 객체에 대한 위치 정보를 획득하고, 상기 위치 정보를 기초로, 상기 메모리에 저장된 위치 정보를 업데이트 한다. 또한, 본 개시의 일 실시 예에 따라, 상기 프로세서는, 상기 제1 카메라와 상기 제2 카메라의 상기 전자 장치에 서의 배치된 이격 거리, 상기 제1 카메라의 시야 각 및 상기 제2 카메라의 시야 각에 기초하여, 상기 제2 이미 지 및 상기 제3 이미지 간의 상대적인 위치를 식별하고, 상기 식별된 상대적인 위치에 기초하여, 상기 제2 이미 지로부터 획득된 위치 정보에 대응되는 상기 제3 이미지에서의 상기 객체의 위치 정보를 획득한다. 상기 목적을 달성하기 위한 본 개시의 다른 실시 예에 따른, 제1 카메라 및 제2 카메라를 포함하는, 전자 장치 의 제어 방법에 있어서, 줌 모드로 동작하는 상기 제1 카메라를 통해 제1 이미지를 획득하고, 상기 제1 이미지 를 표시하는 단계, 일반 모드로 동작하는 제2 카메라를 통해 제2 이미지를 획득하는 단계, 상기 줌 모드에서의 줌 인 비율에 기초하여 상기 제1 이미지에 대응되는 상기 일반 모드에서의 상기 제1 카메라의 제3 이미지를 식 별하는 단계, 상기 제2 이미지에 포함된 객체를 검출하여, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 획득하는 단계, 상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위치에 기초하여, 상기 제2 이미지 로부터 획득된 위치 정보에 대응되는 상기 제3 이미지에서의 상기 객체의 위치 정보를 획득하는 단계 및 상기 객체의 위치 정보에 기초하여, 상기 제1 이미지에서 상기 객체를 검출하는 단계를 포함한다. 또한 본 개시의 일 실시 예에 따라, 상기 위치 정보는, 상기 제2 이미지에서 검출된 객체를 포함하는 제1 바운 딩 박스의 제1 좌표 값 및 제2 좌표 값을 포함하고, 상기 제1 좌표 값은, 상기 제1 바운딩 박스의 좌상단 모서 리의 좌표 값이고, 상기 제2 좌표 값은 상기 제1 바운딩 박스의 우하단 모서리의 좌표 값이다. 또한, 본 개시의 일 실시 예에 따라, 상기 제3 이미지에서의 상기 객체의 위치 정보를 획득하는 단계는, 상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위치에 기초하여, 상기 제1 좌표 값 및 상기 제2 좌표 값과 대응 하는, 상기 제3 이미지에서의 상기 객체에 대한 제3 좌표 값 및 제4 좌표 값을 각각 식별하는 단계 및 상기 줌 모드에서의 줌 인 비율 기초하여, 상기 제2 이미지 내에 상기 제1 이미지에 대응되는 프레임의 크기 및 위치를 식별하는 단계를 포함하고, 제1 이미지에서 상기 객체를 검출하는 단계는, 상기 프레임 내 상기 제3 및 제4 좌 표 값 중 적어도 하나가 포함되는 경우, 상기 제3 및 제4 좌표 값에 기초하여, 상기 줌 모드로 동작하는 상기 제1 카메라를 통해 획득된 제1 이미지에서 상기 객체를 검출하는 단계를 포함한다. 또한 본 개시의 일 실시 예에 따라, 상기 프레임의 크기 및 위치에 기초하여, 상기 제3 좌표 값 및 제4 좌표 값 과 대응하는, 상기 제1 이미지에서의 제5 좌표 값 및 제6 좌표 값을 각각 식별하는 단계 및 상기 제5 좌표 값 및 제6 좌표 값을 기초로, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스를 생성하여 표시 하는 단계를 포함하고, 상기 제5 좌표 값은, 상기 제2 바운딩 박스의 좌상단 모서리의 좌표 값이고, 상기 제6 좌표 값은 상기 제2 바운딩 박스의 우하단 모서리의 좌표 값이다. 또한 본 개시의 일 실시 예에 따라, 상기 제2 바운딩 박스를 생성하여 표시하는 단계는, 상기 제5 좌표 값 및 상기 제6 좌표 값에 기초하여, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스의 넓이를 식 별하는 단계 및 상기 식별된 제2 바운딩 박스의 넓이가 기 설정된 값 이상이면, 상기 제2 바운딩 박스를 표시하 는 단계를 포함한다. 또한 본 개시의 일 실시 예에 따라, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 획득하는 단계 는, 상기 제2 이미지에 포함된 상기 객체에 대한 특징 점을 추출하고, 상기 추출된 특징 점에 기초로 상기 객체 에 대한 객체 인식 정보를 획득하는 단계를 더 포함하고, 상기 제2 바운딩 박스를 생성하여 표시하는 단계는, 상기 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스 및 상기 객체에 대한 객체 인식 정보를 표 시하는 단계를 포함한다. 또한 본 개시의 일 실시 예에 따라, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 획득하는 단계 는, 상기 제2 이미지에서 상기 검출된 객체에 대한 위치 정보를 메모리에 저장하는 단계를 더 포함하고, 상기 제3 이미지에서의 상기 객체 이외의 새로운 객체가 검출되면, 상기 제3 이미지에서 상기 검출된 상기 새로운 객 체에 대한 위치 정보를 획득하고, 상기 위치 정보를 기초로, 상기 메모리에 저장된 위치 정보를 업데이트 하는 단계를 더 포함한다. 또한 상기 제3 이미지에서의 상기 객체의 위치 정보를 획득하는 단계는, 상기 제1 카메라와 상기 제2 카메라의 상기 전자 장치에서의 배치된 이격 거리, 상기 제1 카메라의 시야 각 및 상기 제2 카메라의 시야 각에기초하여, 상기 제2 이미지 및 상기 제3 이미지 간의 상대적인 위치를 식별하는 단계를 더 포함한다."}
{"patent_id": "10-2022-0013648", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상술한 본 개시의 실시 예에 따르면, 사용자가 동작 중인 카메라를 줌 인함으로써, 카메라를 통해 획득되는 이 미지에 객체에 관한 정보가 일부 누락되더라도, 나머지 카메라를 통해 객체에 관한 이미지를 획득하고, 이를 기 초로 객체에 관한 정보를 식별함으로써, 사용자는 카메라의 모드에 상관 없이도 객체에 관한 일관되고 정확한 정보를 제공받을 수 있을 것이다."}
{"patent_id": "10-2022-0013648", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과들은 이상에서 언급된 효과로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재로 부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0013648", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부 도면을 참조하여 본 개시를 상세히 설명한다. 본 개시의 실시 예에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 개시의 설명 부 분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 개시에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. A 또는/및 B 중 적어도 하나라는 표현은 \"A\" 또는 \"B\" 또는 \"A 및 B\" 중 어느 하나를 나타내는 것으로 이해되어 야 한다. 본 개시에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중 요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 어떤 구성요소가 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또 는 \"구성되다\" 등의 용어는 명세서 상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또 는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시에서 \"모듈\" 혹은 \"부\"는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨어로 구현되 거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\" 혹은 복수의 \"부\"는 특정한 하드 웨어로 구현될 필요가 있는 \"모듈\" 혹은 \"부\"를 제외하고는 적어도 하나의 모듈로 일체화되어 적어도 하나의 프 로세서(미도시)로 구현될 수 있다. 이하, 첨부된 도면을 참조하여 본 개시를 상세히 설명한다. 도 1은, 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 도면이다. 도 1을 참조하면, 본 개시의 일 실시 예에 따른 전자 장치는 복수의 카메라(120, 130)를 포함한다. 이때, 전자 장치는 복수의 카메라(120, 130) 중 사용자에 의해 선택된 특정 카메라를 이용하여 객체에 대한 이미 지를 획득한다. 또는 전자 장치는 카메라를 구동하는 프로그램(또는 어플리케이션)을 실행하는데 있어서, 프로그램(또는 어플리케이션)에 설정된 복수의 카메라(120, 130)의 구동 순서에 따라 설정된 특정 카메라를 이 용하여 객체에 대한 이미지를 획득한다. 그리고, 전자 장치는 전자 장치의 디스플레이에 특정 카메라를 통해 획득된 객체에 대한 이미지를 표시한다. 한편, 전자 장치는 카메라를 구동하는 동안, 카메라의 초점 거리를 조정하는 사용자의 입력(예를 들어, 사 용자의 손가락을 이용한 모션 입력 등)에 기초하여, 카메라의 초점 거리를 조정한다. 구체적으로, 전자 장치는 객체를 촬영하는 카메라의 초점 거리를 조정함으로써, 카메라를 줌 인(Zoom In)하거나 줌 아웃(Zoom Out)한다. 전자 장치는, 실제 카메라의 초점 거리의 조정 없이, 사용자 입력에 기초하여, 카메라를 통해 획득된 이미지를 크롭핑(Cropping)하여 표시함으로써, 카메라를 줌 인하거나 줌 아웃하는 기능을 제공하기도 한다. 예를 들 어, 디지털 줌(Digital Zoom)이 이에 해당한다. 이때, 객체를 촬영하는 카메라를 줌 인하는 경우, 전자 장치는 객체에 대한 확대된 이미지를 사용자에게 제공할 수 있다. 따라서, 사용자는 객체에 대한 보다 상세한 정보를 획득할 수 있다. 다만, 카메라의 줌 인으로, 객체의 일 부분이 카메라의 앵글 또는 시야(Field of View)를 벗어나는 경우에는, 사용자는 객체의 일 부분에 대한 정보는 획득하지 못한다. 도 1을 참조하면, 줌 인 된 카메라를 통해 제1 객체이 개에 관한 정 보는 온전히 획득할 수 있는 반면에, 제2 객체이 고양이에 관한 정보는 일부 누락하여 획득하게 된다. 이는, 전자 장치 또한 마찬가지이다. 예를 들어, 전자 장치가 객체에 대하여 획득한 이미지를 기초로, 이미지 내 객체의 객체 인식을 수행하는 경우를 가정한다. 이때, 제1 객체인 개와 같이, 카메라를 통해 획득한 이미지 내 객체의 전체 형상이 온전하게 포함되는 경우에는, 전자 장치는 이미지 내 제1 객체 에 대한 객체 인식을 정확히 수행할 수 있다. 구체적으로, 이미지 내 제1 객체를 검출하고, 제1 객체 에 대한 전체 특징 점을 추출할 수 있으며, 이에 기초하여 이미지 내 제1 객체에 대한 객체 인식을 정확히 수행하여 제1 객체의 유형을 식별할 수 있다. 다만, 카메라를 줌 인함으로써, 전자 장치가 객체에 대하여 획득한 이미지 내 제2 객체와 같이, 제2 객체의 일 부분이 누락된 경우에는, 전자 장치는 이미지 내 제2 객체에 대한 객체 인식을 정확히 수 행할 수 없다. 상술한 예를 들어 다시 설명하면, 전자 장치는 이미지 내 제2 객체를 검출할 수 없거 나, 또는 제2 객체에 대한 전체 특징 점 중 이미지에서 누락된 객체의 일 부분에 대응하는 특징 점을 식별 할 수도 없다. 따라서, 전자 장치는 제2 객체를 검출하지 못하여, 객체 인식 자체를 수행할 수 없거 나, 또는 제2 객체를 검출하더라도, 정확한 객체 인식 결과를 산출하지 못한다. 본 개시는 이러한 문제점을 해결하기 위하여, 구동 중인 제1 카메라에 대한 줌 인 입력을 수신하면, 제1 카메라 이외의 제2 카메라를 통해 객체의 이미지를 획득한다. 이때, 전자 장치는 제2 카메라 를 일반 모드로 구동한다. 그리고 전자 장치는 일반 모드에서 제2 카메라를 통해 획득된 객체에 대한 이미지를 기초로, 객체를 검출하고, 객체에 대한 객체 인식을 수행하여, 객체에 대한 검출 결과 및 객체 인식 결과를 획득한다. 그리고 전자 장치는, 획득한 객체의 검출 결과 및 객체 인식 결과에 기초하여, 제1 카메라(즉, 줌 인 모드 로 구동 중인 제1 카메라) 내에서의 객체를 검출하고, 객체에 대한 객체 인식을 수행한다. 이로써, 전자 장치는 제1 카메라를 통해 획득한 이미지 내 객체에 대한 전체 형상이 포함되지 않더라도, 객체(20 0)를 정확히 검출할 수 있으며, 검출된 객체에 대한 객체 인식을 정확히 수행할 수 있다. 이하, 도2 및 도 16을 참조하여, 이와 관련한 본 개시의 일 실시 예에 대하여 상세히 설명하도록 한다. 도 2는, 본 개시의 일 실시 예에 따른 전자 장치의 블록도이다. 본 개시의 일 실시 예에 따라, 전자 장치는 휴대폰, 스마트폰, 태블릿 PC, 노트북 PC 등의 디스플레이 및 복수의 카메라를 포함하는 다양한 전자 장치를 포함한다. 그러나 이에 제한되는 것은 아니며, 전자 장치는 컴퓨터, 스마트 TV 등과 같이 디스플레이를 구비하고, 복수의 카메라를 포함하거나 또는 별도의 복수의 카메라 장치와 연결되어 구동되는 다양한 전자 장치가 될 수 있다. 도 2를 참조하면, 본 개시의 일 실시 예에 따라, 전자 장치는 디스플레이, 제1 카메라, 제2 카 메라 및 프로세서를 포함한다. 디스플레이는, 프로세서의 제어에 따라 하나 이상의 이미지, 예를 들어 카메라(130, 140)를 통해 획 득된 이미지를 표시한다. 이를 위해, 디스플레이는 LCD(Liquid CrystalDisplay), PDP(Plasma Display Panel), OLED(Organic Light Emitting Diodes), TOLED(Transparent OLED) 등으로 구현될 수 있다. LCD로 구성 되는 경우, 디스플레이 내에는 a-si TFT, LTPS(low temperature poly silicon) TFT, OTFT(organic TFT) 등과 같은 형태로 구현될 수 있는 구동 회로와 백라이트 유닛 등도 함께 포함될 수 있다. 또한 디스플레이는 플렉서블(Flexible) 디스플레이 또는 폴더블(Foldable) 디스플레이로 구현될 수도 있다. 이를 위해, 디스플레이는 외부 압력에 의해 변형될 수 있는 플라스틱 기판(가령, 고분자 필름)이나 유리 박막(thin glass) 또는 금속 박막(metal foil) 등과 같이 플렉서블한 특성을 갖는 소재로 구현될 수도 있 다. 본 개시의 일 실시 예에 따라, 제1 카메라와 제2 카메라는 객체를 촬영하여 촬영 이미지를 생성 하기 위한 구성이며, 여기서 촬영 이미지는 동영상과 정지 영상 모두를 포함한다. 제1 카메라는 복수의 카메라 중 사용자에 의해 실행되는 카메라를 의미한다. 구체적으로, 제1 카메라(12 0)는 전자 장치에 포함된 복수의 카메라 중 오브젝트의 촬영을 위하여 인터페이스를 통해 사용자에 의해 선택된 후 실시간으로 오브젝트에 대한 촬영 이미지를 획득하는 카메라를 의미한다. 또는 상술한 예를 들어 설 명하면, 복수의 카메라의 구동 순서와 관련된 기 설정된 우선 순위 중 첫 번째 우선 순위의 카메라를 의미할 수 도 있다. 다만, 이에 제한되는 것은 아니다. 한편, 제2 카메라는 복수의 카메라 중 제1 카메라 이외의 나머지 카메라 중 적어도 하나의 카메라를 의미한다. 즉, 제1 카메라가 복수의 카메라 중 인터페이스를 통해 사용자에 의해 선택된 카메라에 해당한 다면, 제2 카메라는 사용자에 의해 선택되지 않은 카메라에 해당한다. 이때, 프로세서는, 제1 카메라와는 달리, 제2 카메라를 통해 외부 객체에 대한 이미지를 획득하는 동작을 전자 장치의 백그라운드 상에서 수행할 수 있다. 따라서, 프로세서는 제1 카메라 에 의해 획득된 외부 객체에 대한 이미지는 디스플레이 상에 표시하는 반면에 제2 카메라 에 의해 획득한 외부 객체에 대한 이미지는 디스플레이 상에 표시하지 않을 수 있다. 한편, 본 개시의 일 실시 예에 따라, 제1 카메라와 제2 카메라는 적어도 하나의 외부 객체에 대 한 이미지를 획득할 수 있으며, 카메라, 렌즈, 적외선 센서 등으로 구현될 수 있다. 제1 카메라와 제2 카메라는 일반 모드 및 줌 모드로 동작될 수 있다. 일반 모드는, 최초 설정된 제1 카메라의 초점 거리에 기초하여, 이미지를 획득하는 모드를 의미한다. 예를 들어, 제1 카메라에 대하 여 설정된 기본 초점 거리에서 객체에 대하여 이미지를 획득하는 모드를 의미한다. 또는 디지털 줌(Digital Zoo) 방식의 경우에는, 제1 카메라를 통해 획득한 이미지를 확대 또는 축소하지 않고, 제1 카메라에 의해 획득된 원본 이미지를 표시하는 모드를 의미한다. 한편, 줌 모드는, 일반 모드에서의 최초 설정된 카메라의 초점 거리를 변경하고, 변경된 초점 거리에 기초하여 외부 객체에 대한 이미지를 획득하는 카메라의 동작 모드를 의미한다. 또한 디지털 중 방식의 경우에는, 제1 카 메라에 의해 획득된 원본 이미지를 확대 또는 축소한 이미지를 표시하는 모드를 의미한다. 예를 들어, 제1 카메라에 의해 획득한 원본 이미지를 확대하고, 디스플레이의 크기 및 해상도에 기초하여 확대된 이미지로 부터 크롭핑(Cropping) 된 이미지를 표시하는 모드를 의미한다. 한편, 본 개시의 설명을 위하여, 일반 모드 및 줌 모드가 구분되는 것과 같이 설명하였으나, 본 개시의 일 실시 예에 따라서는, 일반 모드와 줌 모드는 명확히 구분되는 것은 아니며, 프로세서가 사용자의 줌 인, 또는 줌 아웃 입력을 수신하고, 사용자의 줌 인 또는 줌 아웃 입력에 대응하여 프로세서가 카메라 또는 카메라 로부터 획득되는 이미지를 조정하여 표시하는 경우에, 이를 줌 모드라고 지칭할 수 있다. 한편, 본 개시에 따른 제1 카메라와 제2 카메라, 각각은 이미지 센서 및 렌즈를 포함할 수 있다. 여 기에서, 렌즈들의 화각(Field of View, FOV)은 서로 상이할 수 있다. 예를 들어, 제1 카메라와 제2 카메라 는, 망원 렌즈(telephoto lens), 광각 렌즈(wide angle lens) 및 초 광각 렌즈(super wide angle lens) 중 적어도 하나를 포함할 수 있다. 다만, 제1 카메라가 망원 렌즈를 포함하는 경우, 제2 카메라는 제 1 카메라와는 다른 광각 렌즈 또는 초 광각 렌즈 중 어느 하나를 포함할 수 있다. 즉, 제1 카메라와 제2 카메라는 중복되지 않는, 서로 다른 화각을 갖는 렌즈를 각각 포함할 수 있다. 한편, 도 1에 도시된 바와 같이, 제1 카메라 및 제2 카메라는 전자 장치의 후면에 배치되어, 다 만, 본 개시의 일 실시 예에서는, 전자 장치에 제1 카메라 및 제2 카메라, 즉 두개의 카메라가 포함된 것으로 설명되었으나, 카메라의 개수와 그 유형에 특별한 제한이 있는 것은 아니다. 프로세서는 전자 장치의 전반적인 동작을 제어할 수 있다. 이를 위해, 프로세서는 RAM(Random Access Memory), ROM(Read Only Memory), CPU(central processing unit), GPU(Graphic processing unit) 및 시스템 버스 등을 포함할 수 있으며, 전자 장치에 포함된 하나 이상의 구성요소들의 제어에 관한 연산이나 데이터 처리를 실행할 수 있다. 프로세서는 스토리지에 저장된 하나 이상의 명령어를 실행시켜 전자 장치 에 포함된 하나 이상의 구성요소들을 제어하거나, 하드웨어적인 회로 내지는 칩으로서 하나 이상의 구성요 소들을 제어하거나, 또는 소프트웨어 및 하드웨어의 결합으로서 하나 이상의 구성요소들을 제어할 수 있다. 도 3은, 본 개시의 일 실시 예에 따른 제2 카메라를 이용하여 획득한 제2 이미지를 기초로, 제1 카메라의 줌 모 드에서 획득한 제1 이미지 내 객체를 검출하는 방법을 개략적으로 나타낸 순서도이다. 도 4는, 본 개시의 일 실시 예에 따른 줌 모드에서의 줌 인 비율에 기초하여 제1 이미지에 대응되는 일반 모드 에서의 제1 카메라의 제3 이미지를 식별하는 것을 나타낸 예시도이다. 도 5는, 본 개시의 일 실시 예에 따른 제2 카메라를 이용하여 획득한 제2 이미지에서 객체를 검출하여, 객체의 위치 정보를 획득하는 것을 나타낸 예시도이다. 먼저, 도 3을 참조하면, 프로세서는 줌 모드로 동작하는 제1 카메라를 통해 제1 이미지를 획득하 고, 제1 이미지를 디스플레이에 표시한다(S310). 구체적으로, 프로세서는 제1 카메라를 이용하여 외부 객체에 대한 이미지를 획득하고, 줌 모드 와 관련되어 설정된(또는 줌 모드로의 카메라 구동과 관련하여 사용자로부터 입력된) 줌 인 비율에 기초하여 획 득된 이미지를 확대하고, 디스플레이의 크기, 비율, 해상도 등에 기초하여 확대된 이미지를 크롭핑 하여 제1 이미지를 획득한다. 그리고, 프로세서는 획득된 제1 이미지를 표시하도록 디스플레이를 제어 한다. 한편, 이에 제한되는 것은 아니며, 프로세서는 제1 카메라에 포함된 렌즈의 초점 거리를 조정하여, 객체에 대한 제1 이미지를 획득할 수도 있다. 그러나, 본 개시의 설명의 편의를 위하여, 이하에서는 디 지털 줌 방식에 기초하여 프로세서가 카메라(120, 130)를 이용하여 이미지(예를 들어, 제1 내지 제2 이미 지)를 획득하는 것으로 설명하도록 한다. 한편, 프로세서는 줌 모드로 동작하기에 앞서, 일반 모드로 동작하는 제1 카메라의 모드를 줌 모드로 변경 하기 위한 사용자 명령을 수신할 수도 있다. 이때, 프로세서는 디스플레이 패널을 구비한 디스플레이 또는 입력 인터페이스를 통해 일반 모드로 동작하는 제1 카메라의 모드를 줌 모드로 변경하기 위한 사용자 명령을 수신한다. 예를 들어, 프로세서는 디스플레이를 통해 사용자의 손가락을 이용한 제1 카메라의 모드를 줌 인 모드로 변경하는 터치 입력 또는 모션 입력을 감지할 수 있다. 보다 구체적으로, 프로세서는 디스플레이를 통해 제1 터치 입력과 제2 터치 입력을 감지하고, 제1 터치 입력 위치 및 제2 터치 입력 위 치 간의 거리를 산출하고, 이후 제1 터치 입력의 위치 및 제2 터치 입력의 위치 중 적어도 하나의 위치가 변경 되어, 제1 터치 입력 및 제2 터치 입력 간의 거리가 증가되면, 제1 카메라의 모드를 줌 모드로 변경하기 위한 사용자 명령이 입력된 것으로 감지할 수 있다. 한편, 본 개시의 일 실시 예로, 줌 모드로 변경하기 위한 사용자 명령에는, 줌 인 비율(또는 배율) 및 줌 아웃 비율(또는 배율)에 관한 정보를 포함할 수도 있다. 상술한 예를 들어 다시 설명하면, 프로세서는 제1 터치 입력의 위치 및 제2 터치 입력의 위치 중 적어도 하나의 위치가 변경되어, 제1 터치 입력 및 제2 터치 입력 간 의 거리가 증가되면, 증가된 거리에 대응하여 제1 카메라의 줌 인 배율을 증가시킬 수 있다. 즉, 프로세서 는 사용자로부터 제1 카메라의 모드를 줌 모드로 변경하는 입력 뿐만 아니라, 줌 모드와 관련된 줌 인 비율 또는 줌 아웃 비율에 관한 정보도 동시에 입력 받을 수 있다. 다시 도 3을 참조하면, 프로세서는 제1 이미지를 표시한(S310) 후 일반 모드로 동작하는 제2 카메라 를 통해 제2 이미지를 획득한다(S320). 구체적으로, 프로세서는 디스플레이에 표시되는 제1 이미지를 획득하기 위하여 구동되는 제1 카메라 이외의 제2 카메라를 구동하고, 제2 카메라를 통해 객체에 대한 제2 이미지를 획득한다. 이때, 프로세서는 제2 카메라의 일반 모드에서, 객체에 대한 제2 이미지를 획득할 수 있다. 앞서 설명 한 바와 같이, 일반 모드는 제2 카메라를 통해 획득되는 이미지의 변경(예를 들어, 확대 또는 축소) 없이 객체에 대하여 획득되는 원본 이미지를 표시하는 모드를 의미한다. 또는 제2 카메라의 초점 거리 변경 없 이, 또는 제2 카메라에 대하여 기 설정된 기본 초점 거리에서 객체에 대하여 이미지를 획득하는 모드 를 의미한다. 한편, 본 개시의 일 실시 예에 따라서는, 일반 모드에서의 제1 카메라와 객체에 대한, O/I(Object & Imager Distance) 값과 일반 모드에서의 제2 카메라와 객체에 대한, O/I(Object & Imager Distance) 값은 동일할 수 있다. 그러나 이에 제한되는 것은 아니다. 다시 도 3을 참조하면, 본 개시의 일 실시 예에 따라, 프로세서는 줌 모드에서의 줌 인 비율에 기초하여 제1 이미지에 대응되는 일반 모드에서의 제1 카메라의 제3 이미지를 식별한다(S330). 제3 이미지는, 제1 카메라를 이용하여 획득 가능한 객체에 대한 원본 이미지 또는 이미지 프레임을 의 미한다. 또는, 제1 카메라의 초점 거리의 조정 없이 객체에 대하여 획득될 수 있는 이미지 또는 이미지 프레임을 의미한다. 제3 이미지는 실제 제1 카메라가 객체를 촬영하여 획득되는 것이 아니며, 줌 모드로 동작하는 제1 카메라의 줌 인 비율과 제2 이미지를 기초로, 제1 카메라의 일반 모드에서 객체에 대하여 획득될 것으로 식별되는 이미지에 해당한다는 점에서 제1 이미지와는 차이가 있다. 즉, 제3 이미지 내에는 객체에 대한 정보가 포함되어 있지 않을 수 있다. 한편, 제3 이미지를 식별하기 위하여, 프로세 서는 줌 모드에서의 줌 인 비율에 기초하여, 객체에 대하여 획득된 제1 이미지에 대응되는 일반 모드 에서의 제1 카메라의 제3 이미지를 식별할 수 있다. 예를 들어, 본 개시의 일 실시 예에 따라, 프로세 서가 줌 모드로 변경하기 위한 사용자 명령과 줌 인 비율(X2)에 관한 사용자 명령을 수신하였다고 가정한 다. 이때, 프로세서는 제1 카메라를 통해 객체에 대하여 2배 확대된 이미지를 디스플레이 에 표시할 수 있다. 그리고, 프로세서는, 줌 인 비율(X2)에 기초하여, 디스플레이에 표시되는 제1 이 미지에 대응되는 일반 모드에서의 제1 카메라의 제3 이미지를 식별할 수 있다. 도 4를 참조하면, 본 개시의 일 실시 예에 따라서 프로세서는 가상 좌표 계를 설정하고, 설정된 가상 좌표 계 상에서 제3 이미지를 식별할 수도 있다. 예를 들어, 도 4를 참조하면, 프로세서는 xy평면 상의 원점 에 제3 이미지의 좌하단의 모서리를 배치한 후 제3 이미지를 식별할 수 있다. 이때, 제3 이미지의 좌 하단의 모서리 이외의 모서리(예를 들어, 좌상단의 모서리, 우하단의 모서리 및 우상단의 모서리)의 xy 평면 상 에서의 좌표 값은, 제1 카메라의 해상도, 제1 카메라를 통해 획득되는 이미지의 크기 및 비율, 디스플레이 의 크기 등에 기초하여 설정될 수 있다. 이하, 본 개시의 설명의 편의를 위하여, 제1 카메라를 통해 획득되는 이미지(예를 들어, 제1 이미지와 제1 이미지에 대응되는 제3 이미지)의 비율은 16:12이라고 가정한다. 이때, 도 4를 참조하면, 프로세서(14 0)는 제3 이미지의 좌하단의 모서리의 좌표 값은 (0, 0)으로, 좌상단의 모서리의 좌표 값은 (0, 12)으로, 우하 단의 모서리의 좌표 값은 (16, 0)으로, 우상단의 모서리의 좌표 값은 (16, 12)으로 식별한다. 다시 도 3을 참조하면, 본 개시의 일 실시 예에 따라, 프로세서는 제2 이미지에 포함된 객체를 검 출하여, 제2 이미지에서 검출된 객체에 대한 위치 정보를 획득한다(S340). 구체적으로, 프로세서는 제2 이미지 상에서 특징 점을 식별하고, 특징 점의 집합, 또는 복수의 특징 점 을 클러스터링 하여, 객체를 검출할 수 있다. 그리고 프로세서는 제2 이미지 에서의 검출된 객체의 위치를 식별한다. 예를 들어, 제2 이미지를 xy 좌표 평면 상에 배치하고, 검출된 객체에 대한 위치, 즉 좌표 값을 식별 할 수 있다. 또는 제2 이미지 내에서의 검출된 객체에 대한 픽셀의 위치 등을 기초로 객체의 위치를 식별할 수도 있다. 한편, 이를 위해, 프로세서는 전자 장치의 메모리에 저장된 인공지능 기반의 객체 감지 모델을 이용 할 수 있다. 예를 들어, 객체 감지 모델은 CNN(Convolutional Neural Network) 모델을 포함할 수 있다. 프로세 서는, 제2 이미지를 CNN 모델에 입력하여, 제2 이미지를 구성하는 적어도 하나의 프레임에서 객체에 관한 특징(Feature)들을 추출하여, 특징 맵(Feature Map)을 생성한다. 그리고 생성된 특징 맵을 기초로, 프로세 서는 제2 이미지에서의 객체를 검출하고, 객체의 위치를 식별한다. 한편, 본 개시의 일 실시 예에 따라, 본 개시의 일 실시 예에 따라, 객체에 대한 위치 정보는, 제2 이미지 에서 검출된 객체를 포함하는 제1 바운딩 박스의 제1 좌표 값 및 제2 좌표 값을 포함하고, 제1 좌표 값 은, 제1 바운딩 박스의 좌상단 모서리의 좌표 값이고, 제2 좌표 값은 제1 바운딩 박스의 우하단 모서리 의 좌표 값일 수 있다. 구체적으로, 프로세서는, 제2 이미지에서 검출된 객체의 위치와 대응하고, 검출 된 객체를 포함하는 사각형 형태의 바운딩 박스(Bounding Box)를 식별할 수 있다. 이때, 제1 바운딩 박스는, 제2 카메라를 통해 획득된 제2 이미지에서 검출된 객체에 대한 바운딩 박스를 의미한다. 프로세서는 제2 이미지에 포함된 객체를 검출하고, 검출된 객체를 포함하는 제1 바운딩 박스를 식 별할 수 있다. 예를 들어, 프로세서는 바운딩 박스의 상부 면은 객체에 대한 복수의 특징 점(예를 들어, 특징 점 그룹 또는 클러스터링 된 복수의 특징 점) 중 y 좌표 값이 가장 큰 특징 점에 기초하여 식별되고, 바운 딩 박스의 좌측 면은 객체에 대한 특징 점 중 x 좌표 값이 가장 작은 특징 점에 기초하여 식별되고, 바운딩 박 스의 우측 면은 객체에 대한 특징 점 중 x 좌표 값이 가장 큰 특징 점에 기초하여 식별되고, 바운딩 박스의 하 부 면은 객체에 대한 특징 점 중 y 좌표 값이 가장 작은 특징 점에 기초하여 식별된다. 그리고, 프로세서는 제2 이미지에서의 객체의 위치를 바운딩 박스를 기초로 식별할 수 있다. 구체적으 로, 프로세서는 제1 바운딩 박스의 좌상단 모서리의 좌표 값인 제1 좌표 값과, 제1 바운딩 박스의 우하단 모서리의 좌표 값인 제2 좌표 값으로 제2 이미지 내에서 검출된 객체의 위치를 식별할 수 있다. 도 5를 참조하면, 프로세서는 제2 이미지에서의 제1 객체와 제2 객체을 검출한다. 그리고, 프로세서는 제2 이미지에서의 제1 객체와 제2 객체에 대응하는 제1 바운딩 박스(제1-1 바운 딩 박스와 제1-2 바운딩 박스)를 식별한다. 이때, 본 개시의 일 실시 예에 따라, 제2 카메라를 통해 획득된 제2 이미지의 크기와 비율은, 제2 카메 라의 해상도, 디스플레이 크기 등에 기초하여 설정될 수 있다. 또한, 본 개시의 일 실시 예에 따라, 제2 카메라를 통해 획득된 이미지(예를 들어, 제2 이미지)와 제1 카 메라를 통해 획득된 이미지(예를 들어, 제1 이미지 및 제1 이미지에 대응되는 제3 이미지)의 크기 및 비율을 상이할 수 있다. 그러나 이에 제한되는 것은 아니며, 전자 장치에 포함된 복수의 카메라(110, 120)로부터 획득되는 이미지의 크기 및 비율은 동일하게 설정될 수 있다. 그러나, 본 개시의 설명의 편의를 위해, 이하에서는 제1 카메라로부터 획득된 이미지(예를 들어, 제1 이미 지 및 제1 이미지에 대응되는 제3 이미지)와 제2 카메라로부터 획득된 이미지(예를 들어, 제2 이미지)의 크기 및 비율이 상이한 것으로 설명하도록 한다. 상술한 예를 들어, 다시 설명하면, 제1 카메라를 통해 획득된 제1 이미지와 제1 이미지에 대응하는 제1 카메라의 제3 이미지의 크기가 16 : 12 비율의 이미지라고 가정한다면, 제2 카메라를 통해 획득된 제2 이미지는 16 : 9의 비율의 이미지라고 가정한다. 이때, 도 5를 참조하면, 프로세서는 제2 이미지 내에서, 제1 객체의 위치를 제1 객체에 대응하는 제1-1 바운딩 박스의 제1 좌표 값인 (4, 7) 및 제1-1 바운딩 박스의 제2 좌표 값인 (8, 3)로 식별한다. 그리고 프로세서는 제2 이미지 내에서, 제2 객체의 위치를, 제2 객체에 대응하는 제1-2 바운딩 박스의 제1 좌표 값인 (9, 6) 및 제1-2 바운 딩 박스의 제2 좌표 값인 (12, 1)로 식별한다. 한편, 다시 3을 참조하면, 본 개시의 일 실시 예에 따라, 프로세서는, 제2 이미지 및 제3 이미지 간 의 상대적인 위치에 기초하여, 제2 이미지로부터 획득된 위치 정보에 대응되는 제3 이미지에서의 객체의 위 치 정보를 획득한다(S350). 프로세서는 제1 이미지에 대응하는 제1 카메라의 제3 이미지에 제2 카메라를 통해 획득된 제2 이미지를 적층(Stack)하거나 또는 제3 이미지와 제2 이미지를 정합하여 제3 이미지에서의 객체 의 위치를 식별할 수 있다. 구체적으로, 프로세서는, 제2 이미지에서 획득된 객체에 대한 위 치 정보를 기초로, 제3 이미지에서의 객체의 위치를 식별하기 위하여, 제2 이미지에서 획득된 객체 의 위치 정보를 제3 이미지 상에 동기화할 수 있다. 이를 위해, 프로세서는 제2 이미지와 제3 이미 지 간의 상대적인 위치 차이를 식별하는 과정을 선행할 수 있다. 구체적으로, 프로세서는 제3 이미지에 제2 이미지를 적층하거나 또는 제2 이미지에서 획득된 정보 (예를 들어, 제3 이미지에서의 객체의 위치 정보)를 제3 이미지 상에 동기화 시키기 위해서는, 제2 이 미지와 제3 이미지 간의 상대적인 위치 차이를 보정해야만 한다. 왜냐하면, 전자 장치에서의 제1 카메라 와 제2 카메라가 각각 배치된 위치의 차이로 인하여, 제2 이미지(제2 카메라를 통해 획득된 제2 이미 지)와 제3 이미지(제1 카메라를 통해 획득될 것으로 식별된 제3 이미지)는 동일한 일반 모드에서 획득되더라도, 각각의 이미지 내 객체의 위치는 상이할 수 있다. 이는, 제1 카메라와 제2 카메라의 시야 각에 기인 하기도 한다. 따라서, 각각의 카메라(예를 들어, 제1 카메라 및 제2 카메라)를 통해 획득된 이미지 (제2 이미지)와 획득 가능한 이미지(제3 이미지)를 정확히 동기화하기 위해서는, 프로세서는 제2 이미지 와 제3 이미지의 위치 및 위치의 차이를 식별해야 한다. 도 6을 참조하면, 프로세서는 제3 이미지를 기준으로 제2 이미지의 제3 이미지에 대한 위치 차이 를 고려하여, xy 평면 상에서의 제2 이미지를 식별한다. 보다 구체적으로, 상술한 예를 들어 다시 설명하면, 프로세서는 xy 평면 상에서 제3 이미지의 좌하단의 모서리의 좌표 값이 xy 좌표 계의 원점(0, 0)에 해당하 는 것으로 식별한다. 그리고, 프로세서는 제3 이미지의 좌상단의 모서리의 좌표 값은 (0, 12)으로, 우하단 의 모서리의 좌표 값은 (16, 0)으로, 우상단의 모서리의 좌표 값은 (16, 12)으로 식별한다. 그리고, 프로세서는 제2 이미지의 제3 이미지에 대한 상대적인 위치를 식별한다. 예를 들어, 프로세 서는 제3 이미지를 기준으로 제2 이미지의 x축 방향의 제1 변위 값(또는 제1 보정 값), 제2 변위 값(또는 제2 보정 값)을 식별하고, y축 방향의 제3 변위 값(또는 제3 보정 값), 제4 변위 값(또는 제4 보정 값)을 식별 하고, xy 평면 상에서의 제3 이미지를 기준으로 한 제2 이미지의 위치를 식별한다. 다시 도 6을 참조하면, 프로세서는 xy 평면 상에서 제2 이미지의 좌하단의 모서리의 좌표 값을(1, 1)로, 제2 이미지의 좌상단의 모서리의 좌표 값을 (1, 10)으로, 우하단의 모서리의 좌표 값은 (17, 1)으로, 우상단의 모서리의 좌표 값은 (17, 10)으로 식별한다. 이로써, 프로세서는 xy 평면 상에서의 제3 이미지 를 기준으로, 제2 이미지의 상대적인 위치를 식별할 수 있다. 한편, 상술하여 설명한 바와 같이, 프로세서는 제2 이미지로부터 획득된, 제2 이미지에서의 객체 의 위치 정보에 대응되는 제3 이미지에서의 객체의 위치 정보를 획득한다. 상술한 예를 들어 다시 설명하면, 프로세서는 제3 이미지를 기준으로 제2 이미지의 x축 방향의 제1 변위 값, 제2 변위 값을 식별하고, y축 방향의 제3 변위 값, 제4 변위 값을 식별하고, 식별된 제1 변위 값, 제2 변위 값, 제3 변위 값 및 제4 변위 값에 기초하여, 식별된 제2 이미지에서의 객체의 위치 정보를 제3 이미지에 동기화 시킬 수 있다. 이에 관해서는 도 8 및 도 9을 참조하여 자세히 설명하도록 한다. 한편, 다시 3을 참조하면, 본 개시의 일 실시 예에 따라, 줌 모드에서의 줌 인 비율 및 제3 이미지에서의 객체 의 위치 정보에 기초하여, 줌 모드로 동작하는 제1 카메라를 통해 획득된 제1 이미지에서 객체를 검출한다 (S360). 구체적으로, S350 단계에서, 프로세서는 제3 이미지에서의 객체의 좌표 정보를 식별하였다. 이때, 프 로세서는 줌 인 비율을 기초로, 제3 이미지에 대응되는 제1 이미지를 식별하고, 식별된 제3 이미지 에서의 객체의 좌표 정보를 기초로, 제1 이미지에서의 객체의 위치를 식별한다. 그리고, 프로세서 는 식별된 위치에서 객체를 검출할 수 있다. 한편, 제1 카메라를 통해 획득되는 제1 이미지와 제1 이미지에 대응하는 제1 카메라의 제3 이미지는 동일한 제1 카메라에 관한 이미지이므로, 프로세서는 제1 및 제3 이미지 간의 상대적인 위치 조정 과정을 수행하지 않 을 수 있다. 이하에서는, 도 7 및 도 8을 참조하여, 제 2 이미지와 제3 이미지의 상대적인 위치를 기초로, 제3 이미지 에서의 객체의 위치를 식별하는 본 개시의 일 실시 예에 대하여 상세히 설명하도록 한다. 도 7은 본 개시의 일 실시 예에 따른, 제 2 이미지와 제3 이미지의 상대적인 위치를 기초로, 제3 이미지에서의 객체의 위치를 식별하는 방법을 개략적으로 나타낸 순서도이다. 도 8은 본 개시의 일 실시 예에 따른, 제 2 이미지와 제3 이미지의 상대적인 위치를 기초로, 제3 이미지에서의 객체의 위치를 식별하는 것을 나타낸 예시도이다. 도 9는 본 개시의 일 실시 예에 따른, 제3 이미지에서의 제1 이미지에 대응되는 프레임의 크기 및 위치를 식별 하는 것을 나타낸 예시도이다. 도 7을 참조하면, 본 개시의 일 실시 예에 따라, 프로세서는 제2 이미지 및 제3 이미지 간의 상대적 인 위치 차이에 기초하여, 제1 좌표 값 및 제2 좌표 값과 대응하는, 제3 이미지에서의 객체에 대한 제3 좌표 값 및 제4 좌표 값을 각각 식별한다(S351). 구체적으로, 제2 카메라를 통해 획득된 제2 이미지를 제1 카메라의 제3 이미지에 적층시키기 위하여, 프로세서는 제2 이미지와 제3 이미지 간의 상대적인 위치 차이를 식별하였다. 그리고, 식별된 위 치 차이에 기초하여, xy 평면 상에서 제3 이미지를 기준으로, 제2 이미지의 xy 평면 상에서의 위치를 식별하였 다. 이때, 프로세서는 제2 이미지로부터 획득된 객체의 위치 정보인, 제1 좌표 값 및 제2 좌표 값을 각각 제3 좌표 값과 제4 좌표 값(제1 좌표 값을 제3 좌표 값으로, 제2 좌표 값을 제4 좌표 값으로 변환)으로 변 환함으로써, 제2 이미지에서의 객체의 위치 정보를 제3 이미지 상에 동기화할 수 있다. 즉, 프로세 서는, 제2 이미지와 제3 이미지 간의 상대적인 위치에 기초하여, 객체에 관한 바운딩 박스의 위치를 조정하고, 이에 따라, 바운딩 박스의 좌상단 모서리의 좌표 값인 제3 좌표 값과 바운딩 박스의 우하단 모서리의 좌표 값인 제4 좌표 값을 식별한다. 도 8을 참조하면, 프로세서는 제2 이미지 및 제3 이미지 간의 상대적인 위치에 기초하여, 제2 이미지 내에 서 제1 객체에 대응하는 제1-1 바운딩 박스의 좌상단의 모서리의 좌표 값(즉, 제1 좌표 값)을 (5, 8) 으로, 제1-2 바운딩 박스의 우하단의 모서리의 좌표 값(즉, 제2 좌표 값)을 (9, 4)으로 변환하여 식별한다. 즉, 프로세서는 제3 이미지에서의 제1 객체의 위치를 제3 좌표 값인, (5, 8) 및 제4 좌표 값인, (9, 4)으로 식별한다. 그리고, 프로세서는 제2 이미지 내에서, 제2 객체의 위치를 제3 좌표 값과 제4 좌표 값을 기초로 식별한다. 구체적으로, 프로세서는 제2 객체에 대응하는 제1-2 바운딩 박스의제3 좌표 값을 (10, 7)로, 식별하고, 제1-2 바운딩 박스의 제4 좌표 값을 (13, 2)로 식별한다. 이로써, 프로세서는 제2 카메라를 통해 획득된 제2 이미지에 기초하여, 제1 카메라의 제3 이 미지 상에서의 객체의 위치를 추정할 수 있다. 즉, 상술하여 설명한 바와 같이, 제3 이미지는 제1 이미지에 대응되는 제1 카메라의 이미지로, 제1 카메라에서 획득될 수 있는 이미지이다. 따라서, 제3 이미지 내에는, 제1 이미지와 제2 이미지와는 달리 객체에 대한 정보가 포함되지 않는다. 그러나, 프로세서는, 제2 이미지와 제2 이미지를 기초로 획득된 객체의 정보를 기초로, 제3 이미지에서의 객체의 정보, 예를 들어, 객체의 위치를 식별할 수 있다. 그리고, 다시 도 7을 참조하면, 본 개시의 일 실시 예에 따라, 프로세서는 줌 모드에서의 줌 인 비율에 기 초하여, 제3 이미지 내에 제1 이미지에 대응되는 프레임의 크기 및 위치를 식별한다(S352). 구체적으로, 도 9를 참조하면, 줌 인 비율이 2배라고 가정하였을 때, 일반 모드에서 획득되는 이미지에 비해, 줌 모드에서는 획득되는 제1 이미지는 객체에 대하여 2배 확대된 이미지이다. 이때, 프로세서는, 줌 인 비 율인 2배에 기초하여 제3 이미지 내에서의 제1 이미지에 대응되는 프레임의 크기 및 위치를 식별한다. 일 반 모드에서 획득되는 것으로 식별된 제3 이미지를 기준으로 할 때, 제1 이미지에 대응되는 프레임의 크기는, 제3 이미지의 0.5배에 해당한다. 그리고, 프로세서는 제3 이미지를 기준으로, xy 평면 상에서의 제1 이 미지에 대응되는 프레임의 위치를 식별한다. 이때, 프로세서는 제1 이미지에 대응되는 프레임의 모서리의 좌표 값으로, xy 평면 상에서의 위치를 제1 이미지에 대응되는 프레임의 위치를 식별할 수 있다. 도 9를 참조하면, 프로세서는 제1 이미지에 대응되는 프레임의 좌상단 모서리의 좌표 값을 (4, 9), 좌 하단의 모서리의 좌표 값을 (4, 3), 우상단의 모서리의 좌표 값을 (12, 9) 그리고 우하단의 모서리의 좌표 값을 (12, 3)으로 식별하여, 제3 이미지 내에서의 제1 이미지에 대응되는 프레임의 위치를 식별한다. 한편, 다시 도 7을 참조하면, 본 개시의 일 실시 예에 따라, 프로세서는 제1 이미지에 대응되는 프레임 내 제3 및 제4 좌표 값 중 적어도 하나가 포함되는 경우, 제3 및 제4 좌표 값에 기초하여, 줌 모드로 동작하는 제1 카메라를 통해 획득된 제1 이미지에서 객체를 검출한다(S361). 도 10은, 본 개시의 일 실시 예에 따른, 제1 이미지에 대응되는 프레임 포함된, 제3 및 제4 좌표 값을 기초로 객체를 식별하는 것을 나타낸 예시도이다. 구체적으로, 줌 인 비율 또는 제1 카메라의 확대된 시야 각에 따라서, 프로세서가 제1 카메라를 통해 획득한 제1 이미지 내에는 객체가 포함되지 않을 수 있다. 이때, 프로세서는, 제1 이미지 내 에서 객체를 검출하기에 앞서 또는 객체를 검출하기 위한 제1 이미지의 이미지 프로세싱(Processin g)에 앞서, 제3 및 제4 좌표 값에 기초하여 제1 이미지 상에 객체가 포함되지 않았음을 식별할 수 있다. 구체적으로, 도 10을 참조하면, 프로세서는 제1-1 바운딩 박스의 제3 좌표 값과 제4 좌표 값이 제1 이 미지에 대응되는 프레임에 포함된 것으로 식별할 수 있다. 이를 통해, 프로세서는 제1 이미지 내에 제 1-1 바운딩 박스에 대응하는 제1 객체가 포함되어 있음을 식별한다. 한편, 프로세서는 제1-2 바운딩 박스의 제3 좌표 값 및 제4 좌표 값 중 제3 좌표 값이 제1 이미지에 대응되는 프레임에 포함된 것으로 식별 할 수 있다. 이를 통해, 프로세서는 제1 이미지 내에 제1-2 바운딩 박스에 대응하는 제2 객체가 포함 되어 있음을 식별한다. 다만, 이 경우 프로세서는 제1 이미지에 대응하는 프레임에 포함되지 않은 제4 좌표 값에 기초하여, 제2 객체의 일부 형상이 제1 이미지에 포함되지 않았음을 식별할 수 있다. 도 11은, 본 개시의 일 실시 예에 따른, 제3 이미지에서의 객체의 위치 정보에 기초하여, 제1 이미지에서 객체 를 검출하지 않는 것을 나타낸 예시도이다. 도 11을 참조하면, 프로세서는 제1-3 바운딩 박스의 제3 좌표 값 및 제4 좌표 값과 제1-4 바운딩 박스 의 좌표 값 및 제4 좌표 값이 제1 이미지에 대응되는 프레임에 포함되지 않았음을 식별한다. 이때, 프로 세서는 줌 모드에서 획득되는 제1 이미지에 포함된 객체가 존재하지 않음을 식별하고, 제1 이미지 에서의 객체 검출 또는 객체 인식 과정을 수행하지 않을 수 있다. 도 12는 본 개시의 일 실시 예에 따른, 제3 이미지에서의 객체의 위치 정보에 기초하여, 제1 이미지에서 객체를 검출하여 표시하는 방법을 개략적으로 나타낸 순서도이다. 도 13은 본 개시의 일 실시 예에 따른, 제3 이미지에서의 객체의 위치 정보에 기초하여, 제1 이미지에서 객체를 검출하여 표시하는 것을 나타낸 예시도이다. 도 12를 참조하면, 본 개시의 일 실시 예에 따라, 프로세서는 제1 이미지에 대응되는 프레임의 크기 및 위치에 기초하여, 제3 좌표 값 및 제4 좌표 값과 대응하는, 제1 이미지에서의 제5 좌표 값 및 제6 좌표 값을 각 각 식별한다(S361_a). 이때, 본 개시의 일 실시 예에 따라, 제5 좌표 값은, 제1 이미지에서 검출된 객체를 포함하는 제2 바운딩 박스 의 좌상단 모서리의 좌표 값이고, 제6 좌표 값은 제2 바운딩 박스의 우하단 모서리의 좌표 값이다. 한편, 제2 바운딩 박스는, 제1 이미지에서 검출된 객체에 대한 바운딩 박스를 의미한다. 이때, 제2 바 운딩 박스는 줌 인 비율에 기초하여 제1 바운딩 박스가 변환되어 생성될 수 있다. 프로세서는 제1 이미지에 포함된 객체를 포함하는 제2 바운딩 박스를 제1 이미지에서의 객체의 위치에 대응하여 표시한다. 이를 위하여, 프로세서는 줌 인 비율 및 제1 이미지에 대응하는 프레임의 위치 및 크기에 기초하여 제3 좌표 값과 제4 좌표 값을 각각 제5 좌표 값과 제6 좌표 값을 변환한다. 구체적으로, 프로세서는 제1 이미지에 대응하는 프레임을 기초로, xy 평면상에서의 제1 이미지를 식별 하고, 제3 좌표 값을 제1 이미지에서의 제5 좌표 값으로, 제4 좌표 값을 제1 이미지에서의 제6 좌표 값을 식별 한다. 예를 들어, 도 13을 참조하면, 프로세서는 제1-1 바운딩 박스의 제3 좌표 값인, (5, 8)을 줌 인 비율 (x2) 및 프레임의 위치 및 크기에 기초하여 제5 좌표 값인, (2, 10)((구체적으로, (5-4)*2), (8-3)*2))으로 변 환한다. 그리고 제1-1 바운딩 박스의 제4 좌표 값인, (9, 4)을 줌 인 비율(x2) 및 프레임의 위치 및 크기에 기 초하여 제6 좌표 값인, (10, 2)(구체적으로, ((9-4)*2, (4-3)*2))으로 식별한다. 보다 구체적으로, 프로세서 는 제3 이미지 내에서 식별된 제1 이미지에 대응되는 프레임을, xy 평면 상에서 프레임의 좌하부 모서 리가 xy 좌표 계의 원점에 위치하도록 이동하고, 줌 인 비율에 기초하여 프레임의 좌상부 모서리, 우상부 모서 리 및 우하부 모서리의 위치를 각각 변환한다. 즉, xy 평면 상에서의 제1 이미지의 위치를 식별한다. 그리고 변환된 프레임의 크기 및 위치에 대응하여, 제3 좌표 값과 제4 좌표 값을 각각 제5 좌표 값과 제6 좌표 값을 변 환한다. 한편, 도 13을 참조하면, 프로세서는, 제1-2 바운딩 박스의 제3 좌표 값인 (10, 7)의 경우, 제1-1 바 운딩 박스의 제3 좌표 값 및 제4 좌표 값의 변환 방식과 동일하게, 제4 좌표 값인, (12, 8)(구체적으로, ((10-4)*2, (7-3)*2))로 식별한다. 반면에, 제1-2 바운딩 박스의 제4 좌표 값인 (13, 2)는 (18, -2)가 아 닌, (16, 0)으로 식별하였다. 이는, 제1-2 바운딩 박스의 제4 좌표 값의 경우 제1 이미지에 대응되는 프레 임에 포함되지 않기 때문이다. 따라서, 프로세서는 제1 이미지에 대응되는 프레임에 포함되지 않는 제4 좌 표 값의 경우, 제1 이미지 또는 변환된 프레임의 우하부 모서리의 좌표 값으로 변환한다. 즉, 프로세서는 제1-2 바운딩 박스의 제4 좌표 값을 (16, 0)의 제6 좌표 값으로 변환한다. 이와 관련하여, 본 개시의 일 실시 예에 따라, 프로세서는 제3 좌표 값에 대응하는 제5 좌표 값을 구성하 는 x 좌표 값 및 y 좌표 값을 식별하고, 제5 좌표 값의 x 좌표 값이 제1 이미지(또는 변환된 제1 이미지에 대응 하는 프레임)의 좌하부의 모서리의 x 좌표 값보다 작은 경우에는, 좌하부의 모서리의 x 좌표 값을 제5 좌표 값 의 x 좌표 값으로 식별한다. 그리고, 제5 좌표 값의 y 좌표 값이 제1 이미지(또는 변환된 제1 이미지에 대응하 는 프레임)의 좌상부의 모서리의 y 좌표 값보다 큰 경우에는 좌상부의 y 좌표 값을 제5 좌표 값의 y 좌표 값으 로 식별한다. 이와 유사하게, 프로세서는 제4 좌표 값에 대응하는 제6 좌표 값을 구성하는 x 좌표 값 및 y 좌표 값을 식별하고, 제6 좌표 값의 x 좌표 값이 제1 이미지(또는 변환된 제1 이미지에 대응하는 프레임)의 우 하부의 모서리의 x 좌표 값보다 큰 경우에는, 우하부의 모서리의 x 좌표 값을 제6 좌표 값의 x 좌표 값으로 식 별한다. 그리고, 제6 좌표 값의 y 좌표 값이 제1 이미지(또는 변환된 제1 이미지에 대응하는 프레임)의 우상부 의 모서리의 y 좌표 값보다 작은 경우에는 우후부의 y 좌표 값을 제6 좌표 값의 y 좌표 값으로 식별한다. 다시, 도 12를 참조하면, 본 개시의 일 실시 예에 따라, 프로세서는 제5 좌표 값 및 제6 좌표 값을 기초로, 제1 이미지에서 검출된 상기 객체를 포함하는 제2 바운딩 박스를 생성하여 표시한다(S361_b). 도 13을 참조하면, 프로세서는 제1-1 바운딩 박스의 제3 좌표 값 및 제4 좌표 값을 변환한 제5 좌표 값 및 제6 좌표 값에 기초하여, 제1 이미지에서의 제1 객체에 대한 제2-1 바운딩 박스를 식별한다. 그리고 프 로세서는 제1-2 바운딩 박스의 제3 좌표 값을 변환한 제5 좌표 값과 제1-2 바운딩 박스의 제4 좌표 값을 변환한 제6 좌표 값에 기초하여, 제1 이미지에서의 제2 객체에 대한 제2-2 바운딩 박스를 식별한다. 이 로써, 프로세서는 줌 모드에서 획득된 제1 이미지 내에서 객체를 검출하는 과정 없이도 제1 이미지 내에서의 객체를 추정하여 검출하고, 객체의 위치를 식별할 수 있다. 도 14는 본 개시의 일 실시 예에 따른, 제1 이미지 상에 객체를 검출하여 표시하는 전자 장치의 예시도이다. 도 14를 참조하면, 프로세서는 제2 이미지로부터 획득된 객체의 위치 정보를 기초로, 제1 이미지에서 객체를 검출하고, 각각의 객체에 대한 인식 정보를 디스플레이에 표시한다. 이로써, 사용자는 제1 카메라 를 통해 획득한 제1 이미지 내에서 정확한 객체의 위치 정보를 제공 받을 수 있을 것이다. 도 15는 본 개시의 일 실시 예에 따른, 제5 좌표 값 및 제6 좌표 값을 기초로 식별된 제2 바운딩 박스에 기초하 여 제1 이미지에서 객체를 검출하지 않는 것을 나타낸 예시도이다. 한편, 본 개시의 일 실시 예에 따라, 프로세서는, 제5 좌표 값 및 상기 제6 좌표 값에 기초하여, 상기 제1 이미지에서 검출된 객체를 포함하는 제2 바운딩 박스의 넓이를 식별하고, 식별된 제2 바운딩 박스의 넓 이가 기 설정된 값(이하, 기 설정된 제1 값) 이상이면, 제2 바운딩 박스를 표시할 수 있다. 상술한 바와 같이, 프로세서는 제5 좌표 값 및 제6 좌표 값에 기초하여 제1 이미지에 포함된 객체의 위 치를 추정하고, 추정된 객체의 위치에 대응하는 제2 바운딩 박스를 표시한다. 이때, 제5 좌표 값과 제6 좌 표 값은, 제2 이미지에서 검출된 객체 자체가 아닌, 객체를 포함하는 바운딩 박스에 관한 좌표 값이다. 그렇기 때문에, 제3 좌표 값과 제4 좌표 값이 각각 변환된 제5 좌표 값과 제6 좌표 값에 기반하여 생성된 제2 바운딩 박스에는 항상 객체가 포함되지 않을 수 있다. 구체적으로, 도 15를 참조하면, 제5 좌표 값 및 제6 좌표 값 에 기초하여 생성된 제2 바운딩 박스에 객체 또는 객체의 일 부분이 포함되지 않았다. 따라서, 이러한 경우 에 프로세서는 제5 좌표 값과 제6 좌표 값을 기초로, 제2 바운딩 박스의 넓이를 산출한다. 도 15에서 도시된 제5 좌표 값 및 제6 좌표 값에 기초하여 생성된 제2 바운딩 박스의 넓이는 4로 식별된다. 이때, 기 설정된 제1 값이 5라고 가정한다면, 프로세서는 제1 이미지 상에 객체에 대응하는 제2 바운딩 박스를 표시하지 않는다. 즉, 프로세서는 제1 이미지 상에 객체가 존재하지 않는 것으로 추정한다. 한편, 본 개시의 일 실시 예에 따라, 프로세서는, 제2 이미지 내에서의 객체의 이미지를 이용하여 제1 이미지에 서의 객체를 검출할 수도 있다. 즉, 제1 이미지에 객체의 일 부분 만이 포함된 경우, 프로세서는 제1 이미 지에서 객체를 정확히 검출하기 어려울 것이다. 따라서, 프로세서는 제2 이미지 및 제2 이미지에 포함된 객체의 이미지를 이용하여 제1 이미지에서의 객체를 검출할 수 있다. 이하, 이와 관련된 본 개시의 실시 예에 대하여 설명하도록 한다. 먼저, 프로세서는 먼저, 제5 좌표 값 및 제6 좌표 값에 기초하여, 제1 이미지에서 검출된 객체를 포함하는 제2 바운딩 박스의 넓이를 식별한다. 그리고, 제2 바운딩 박스의 넓이가 기 설정된 값(이하, 기 설정된 제2 값) 미만인지를 식별한다. 상술한 바와 같이, 프로세서는 제5 좌표 값 및 제6 좌표 값에 기초하여 제1 이미지에 포함된 객체의 위치를 추정하고, 추정된 객체의 위치에 대응하는 제2 바운딩 박스를 표시한다. 이때, 프로세서는 제2 바운딩 박스의 넓이를 산출하고, 산출된 제2 바운딩 박스의 넓이가 기 설정 된 제2 값 미만인 것으로 식별되면, 제1 이미지에 객체에 대한 정보가 온전히 포함되지 않는 것으로 식별한다. 즉, 프로세서는 제1 이미지에 객체의 일부 만이 포함된 것으로 식별한다. 이는, 프로세서가 제1 이미 지에서 객체를 정확히 검출할 수 없는 문제를 야기시키고, 사용자 또한 제1 이미지에서 객체를 정확히 식별할 수 없는 문제로 이어진다. 이를 위해, 프로세서는 제1 바운딩 박스를 기초로 제2 이미지 내 객체 이미지를 식별하고, 식별된 객 체 이미지를 상기 제1 이미지에 매칭하여 제1 이미지에서 객체를 검출한다. 구체적으로, 프로세서는 제2 이미지의 제2 바운딩 박스를 기초로, 제2 이미지 내에서의 객체 이미지를 획득한다. 예를 들어, 프로세서 는 제2 바운딩 박스를 기초로, 제2 이미지 내에서의 객체 이미지를 크로핑(Cropping)하여 객체에 대한 이미지를 획득할 수 있다. 그리고, 프로세서는 획득한 객체 이미지를 제1 이미지의 제2 바운딩 박스에 매칭한 후 제1 이미지에서 의 객체를 검출한다. 예를 들어, 프로세서는 획득한 객체 이미지의 크기를 조정하고, 제2 바운딩 박스(2 0)에 조정된 객체 이미지를 표시할 수 있다. 객체 이미지의 크기를 조정하기 위하여, 프로세서는 제1 카메 라 및 제2 카메라의 해상도, 제1 이미지와 제2 이미지의 크기 정보 등을 이용할 수 있을 것이다. 또는 프로세서 는 획득한 객체 이미지를 제1 이미지와 함께 표시할 수도 있다. 예를 들어, 프로세서는 디스플레이 에 표시된 제1 이미지 상에, 제2 바운딩 박스에 대응하는 획득한 객체 이미지를 오버 랩하여 표시할 수도 있다. 한편, 기 설정된 제2 값은, 상술하여 설명한 제1 값과 동일한 값을 설정될 수도 있을 것이다. 도 16은, 본 개시의 일 실시 예에 따른, 제2 이미지를 기초로 획득된 객체의 객체 인식 정보를 제1 이미지에 표 시하는 것을 개략적으로 나타낸 순서도이다. 도 17은, 본 개시의 일 실시 예에 따른, 제2 이미지를 기초로 획득된 객체의 객체 인식 정보를 제1 이미지에 표 시하는 것을 개략적으로 나타낸 예시도이다. 도 16을 참조하면, 본 개시의 일 실시 예에 따라, 프로세서는, 제2 이미지에 포함된 상기 객체에 대한 특징 점 을 추출하고, 추출된 특징 점에 기초로 객체에 대한 객체 인식 정보를 획득하고(S341), 제1 이미지에서 검출된 객체를 포함하는 제2 바운딩 박스 및 객체에 대한 객체 인식 정보를 표시할 수 있다(S362). 보다 구체적으로, 프로세서는 제2 이미지를 기초로, 제2 이미지에서 감지된 객체의 유형을 식별하는 객체 인식 과정을 수행한다. 이를 위해, 전자 장치의 메모리에는, 인공지능 기반의 객체 인식 모델이 저장될 수 있다. 이때, 객체 인식 모델은, 복수의 객체의 유형에 관한 이미지를 포함하는, 데이터 셋을 기초로 기 학습된 것일 수 있다. 본 개시의 일 실시 예에 따라, 인공지능 기반의 객체 인식 모델은, 콘볼루션 뉴럴 네트워크(CNN, convolutional neural network) 모델 및 순환 뉴럴 네트워크(recurrent neural network) 모델을 포함할 수 있다. 이하, 컨볼 루션 뉴럴 네트워크는 'CCN 모델'이라 지칭하고, 순환 뉴럴 네트워크는 'RNN 모델'이라 지칭하겠다. CNN 모델은 이미지의 각 영역에 대해 복수의 필터를 적용하여 특징 지도(Feature Map)를 만들어 내는 콘볼루션 층(Convolution Layer)과 특징 지도를 공간적으로 통합함으로써 위치나 회전의 변화에 불변하는 특징을 추출할 수 있도록 하는 통합층(Pooling Layer)을 번갈아 수차례 반복하는 구조로 형성될 수 있다. 이를 통해, 프로세서 는 제1 카메라와 제2 카메라로부터 획득한 이미지(제1 이미지 및 제2 이미지)내에서, 점, 선, 면 등의 낮 은 수준의 특징에서부터 복잡하고 의미 있는 높은 수준의 특징까지 다양한 수준의 특징을 추출해낼 수 있다. 콘볼루션 층은 입력 영상의 각 패치에 대하여 필터와 국지 수용장(Local Receptive Field)의 내적에 비선형 활 성 함수(Activation Function)를 취함으로 서 특징지도(Feature Map)를 구할 수 있다. 다른 네트워크 구조와 비교하여, CNN 모델은 희소한 연결성 (Sparse Connectivity)과 공유된 가중치(Shared Weights)를 가진 필터를 사용하는 특징을 가질 수 있다. 이러한 연결 구조는 학습할 모수의 개수를 줄여주고, 역전파 알고리즘을 통한 학습을 효율적으로 만들어 결과적으로 예측 성능을 향상시킬 수 있다. 통합 층(Pooling Layer 또는 Sub-sampling Layer)은 이전 콘볼루션 층에서 구해진 특징 지도의 지역 정보를 활 용하여 새로운 특징 지도를 생성할 수 있다. 일반적으로 통합 층에 의해 새로 생성된 특징 지도는 원래의 특징 지도보다 작은 크기로 줄어드는데, 대표적인 통합 방법으로는 특징 지도 내 해당 영역의 최대값을 선택하는 최 대 통합(Max Pooling)과 특징 지도 내 해당 영역의 평균값을 구하는 평균 통합(Average Pooling) 등이 있을 수 있다. 통합 층의 특징 지도는 일반적으로 이전 층의 특징 지도보다 입력 영상에 존재하는 임의의 구조나 패턴의 위치에 영향을 적게 받을 수 있다. 즉, 통합층은 입력 영상 혹은 이전 특징 지도에서의 노이즈나 왜곡과 같은 지역적 변화에 보다 강인한 특징을 추출할 수 있게 되고, 이러한 특징은 분류 성능에 중요한 역할을 할 수 있다. 또 다른 통합 층의 역할은, 깊은 구조상에서 상위의 학습 층으로 올라갈수록 더 넓은 영역의 특징을 반영 할 수 있게 하는 것으로서, 특징 추출 층이 쌓이면서, 하위 층에서는 지역적인 특징을 반영하고 상위 층으로 올 라 갈수록 보다 추상적인 전체 영상의 특징을 반영하는 특징 생성할 수 있다. 이와 같이, 콘볼루션 층과 통합 층의 반복을 통해 최종적으로 추출된 특징은 다중 신경망(MLP: Multi-Layer Perceptron)이나 서포트 벡터 머신(SVM: Support Vector Machine)과 같은 분류 모델이 완전 연결 층(Fully- connected Layer)의 형태로 결합되어 분류 모델 학습 및 예측에 사용될 수 있다. RNN 모델은 어떤 특정 부분이 반복되는 구조를 통해 순서를 학습하기에 효과적인 딥러닝 기법으로 이전 상태의 상태 값이 다음 계산의 입력으로 들어가서 결과에 영향을 미칠 수 있다(단어, 문장, 이미지를 인식할 때 앞의 단어, 글자, 프레임을 참고하여 인식할 필요가 있기 때문). 다만, 본 개시에 따른 객체 인식 모델은 CNN 모델과 RNN 모델에 한정되지 아니하고, 다양한 구조의 신경망으로 형성될 수 있다. 도 17을 참조하면, 프로세서는 제2 이미지를 기초로, 제1 객체를 강아지로 제2 객체를 고양이로 각각 인식 하였다. 그리고, 프로세서는 제2 이미지를 기초로 획득한 인식 결과 정보를 기초로, 제3 이미지에서의 제 2-1 바운딩 박스에 대응되는 제1 객체가 강아지에 해당한다는 정보를 표시한다. 이와 마찬가지로, 프로세서 는 제2 이미지를 기초로 획득한 인식 결과 정보를 기초로, 제3 이미지에서의 제2-2 바운딩 박스에 대응되는 제2 객체가 고양이에 해당한다는 정보를 표시한다. 특히, 제1 이미지 내에는 제2 객체의 일 부분만이 포함되 었지만, 프로세서는 제2 이미지를 기초로 획득한 인식 결과 정보에 기초하여, 제2-2 바운딩 박스에 대응하 는 제2 객체가 고양이에 해당한다는 정보를 사용자에게 정확히 제공할 수 있다. 한편, 본 개시의 일 실시 예에 따라서, 전자 장치는 제2 이미지를 기초로 획득된 객체에 대한 위치 정보를 저장하는 메모리를 더 포함할 수 있다. 이때, 프로세서는, 제3 이미지에서의 객체 이외의 새로운 객체가 검출되면, 제3 이미지에서 검출된 새로운 객체에 대한 위치 정보를 획득하고, 위치 정보를 기초로, 메모리에 저 장된 위치 정보를 업데이트 할 수 있다. 구체적으로, 프로세서는 제2 이미지에 포함된 객체를 검출하고, 제2 이미지에서 검출된 객체에 대한 위치 정보를 획득한 후 이와 관련된 객체에 대한 위치 정보를 메모리에 저장할 수 있다. 이는, 앞서 설명한 바와 같 이 일반 모드에서 획득된 제2 이미지를 기초로 획득된 객체의 위치 정보이다. 이때, 프로세서가 줌 모드의 제1 카메라를 이용하여 제1 이미지를 획득하는 경우에는, 프로세서는 일반 모드에서 검출되거나 감지되지 않은 새로운 오브젝트를 식별할 수 있다. 이때, 프로세서는 제1 이미지에서의 새로운 오브젝트의 위치를 식별한다. 그리고, 식별된 새로운 오브젝트의 위치, 예를 들어, 새로운 오브젝트에 대한 제7 좌표 값 및 제 8좌 표 값을 식별하고, 줌 인 비율에 기초하여, 제7 좌표 값 및 제8 좌표 값을 제1 카메라의 제3 이미지에 대응하는 제9 좌표 값 및 제10 좌표 값을 각각 변환한다. 그리고, 프로세서는 기 저장된 제2 이미지를 기초로 획득 된 객체의 위치 정보와, 제9 좌표 값 및 제10 좌표 값을 기반한 새로운 오브젝트의 위치 정보를 병합시켜, 위치 정보를 업데이트할 수 있다. 도 18은, 본 개시의 일 실시 예에 따른, 제1 카메라와 제2 카메라의 전자 장치에서의 배치된 이격 거리, 제1 카 메라의 시야 각 및 제2 카메라의 시야 각에 기초하여, 제2 이미지 및 제3 이미지 간의 상대적인 위치를 식별하 는 것을 나타낸 예시도이다. 한편, 본 개시의 일 실시 예에 따라서, 프로세서는, 제1 카메라와 제2 카메라의 전자 장치(10 0)에서의 배치된 이격 거리, 제1 카메라의 시야 각 및 제2 카메라의 시야 각에 기초하여, 제2 이미지 및 제3 이미지 간의 상대적인 위치를 식별할 수 있다. 구체적으로, 도 18을 참조하면, 프로세서는 제1 카메라와 제2 카메라 간의 이격 거리(dC)를 식 별한다. 한편, 제1 카메라와 제2 카메라 간의 이격 거리(dC)에 관한 정보는 전자 장치의 메모리 에 저장될 수 있다. 그리고, 프로세서는 제1 카메라 및 제2 카메라의 객체와의 거리를 식별한다. 예를 들어, 프로세서는 제1 카메라의 O/I(Object & Imager Distance) 및 제2 카메라의 O/I(Object & Imager Distance)를 각각 식별할 수 있다. 한편 본 개시의 일 실시 예에 따라서는 전자 장치의 후면에 제1 카메라 및 제2 카메라가 배치됨에 따라, 제1 카메라와 객체와의 거리 및 제2 카메라와 객체와의 거리 는 동일할 수 있다. 한편, 이를 위해, 전자 장치는 카메라(예를 들어, 제1 카메라 및 제2 카메라)와 객체 와의 거리를 식별할 수 있는 센서(예를 들어, ToF(Time of Flight)를 더 포함할 수 있다. 그리고, 프로세서는, 식별된 제1 카메라와 제2 카메라의 이격 거리, 제1 카메라와 객체 간의 거리, 제2 카 메라와 객체 간의 거리, 제1 카메라의 시야 각(Field of View) 및 제2 카메라의 시야 각에 기초하여, 제2 이미지와 제3 이미지 간의 상대적인 위치를 식별할 수 있다. 상술한 예를 들어 다시 설명하면, 프로세서 는 제3 이미지를 기준으로 제2 이미지의 x축 방향의 제1 변위 값, 제2 변위 값을 식별하고, y축 방향의 제 3 변위 값, 제4 변위 값을 식별할 수 있다. 도 18을 참조하면, 제1 변위 값이 w1이라고 가정한다. 이때, 프로세서는 하기의 식 1을 이용하여 제1 변위 값을 식별할 수 있다. [식 1] w1 = dW * tan(β1) - (dW * tan(α1) + dC1) 여기서, dW는 제1 카메라 및 제2 카메라와 객체 간의 거리이고, β1은 제1 카메라의 수평 시야 각의 1/2 값이고, α1은 제2 카메라의 수평 시야 각의 1/2 값이고, dc1는 제1 카메라 및 제2 카메라의 수평 이격 거리이 다. 한편, 도 18을 참조하면, 제2 변위 값이 w2이라고 가정한다. 이때, 프로세서는 하기의 식 2를 이용하여 제 2 변위 값을 식별할 수 있다. [식 2] w1 = dW * tan(β1) - (dW * tan(α1) - dC1) 한편, 도 18을 참조하면, 제3 변위 값이 h1이라고 가정한다. 이때, 프로세서는 하기의 식 3을 이용하여 제 3 변위 값을 식별할 수 있다. [식 3] h1 = dW * tan(β2) - (dW * tan(α2) - dC2) 여기서, dW는 제1 카메라 및 제2 카메라와 객체 간의 거리이고, β2는 제1 카메라의 수직 시야 각의 1/2 값이고, α2은 제2 카메라의 수직 시야 각의 1/2 값이고, dC2는 제1 카메라 및 제2 카메라의 수직 이격 거리이 다. 한편, 도 18을 참조하면, 제4 변위 값이 h2이라고 가정한다. 이때, 프로세서는 하기의 식 4를 이용하여 제 4 변위 값을 식별할 수 있다. [식 4] h2 = dW * tan(β2) - (dW * tan(α2) - dC2) 한편, 본 개시의 다른 실시 예로, 프로세서는 제1 카메라와 제2 카메라의 시야 각에 기초하여 각각의 변위 값을 식별할 수도 있다. 예를 들어, 프로세서는 제1 카메라의 시야각 정보를 기초로, 제1 카메라의 수평 시야각 거리를 산출하고, 제1 카메라의 시야각 정보를 기초로, 제2 카메라의 수평 시야각 거리를 산출한다. 그 리고 프로세서는 제1 카메라의 수평 시야각 거리에서 제2 카메라의 수평 시야각 거리와 1 카메라 및 제2 카메라의 수평 이격 거리를 뺀 값으로, 제1 변위 값인, w1을 산출할 수도 있을 것이다. 다만, 이는 제1 카메라 및 제2 카메라와 객체 간의 거리를 식별하는 라인이 각각의 카메라의 FOV의 중심을 관통하는 경우에 가능할 것 이다. 도 19는, 본 개시의 다른 실시 예에 따른, 제1 카메라와 제2 카메라의 전자 장치에서의 배치된 이격 거리, 제1 카메라의 시야 각 및 제2 카메라의 시야 각에 기초하여, 제2 이미지 및 제3 이미지 간의 상대적인 위치를 식별 하는 것을 나타낸 예시도이다. 한편, 도 19를 참조하면, 제1 카메라 및 제2 카메라와 객체 간의 거리를 식별하는 라인이 각각의 카메라의 FOV 의 중심을 관통하지 않을 수 있다. 다만, 이러한 경우에도, 프로세서는 상술하여 설명한 식 1 내지 식 4를 기초 로, 각각의 변위 값(예를 들어, 제1 변위 값 내지 제4 변위 값)을 식별할 수 있다. 보다 구체적으로, 프로세서 는 하기의 식 1을 이용하여 제1 변위 값을 식별할 수 있다. [식 1] w1 = dW * tan(β1) - (dW * tan(α1) + dC1) 여기서, dW는 제1 카메라 및 제2 카메라와 객체 간의 거리이고, β1은 제1 카메라의 수평 시야 각의 1/2 값이고, α1은 제2 카메라의 수평 시야 각의 1/2 값이고, dC1는 제1 카메라 및 제2 카메라의 수평 이격 거리이 다. 이하, 제2 내지 제4 변위 값을 식별하는 방법에 대해서는, 도 18을 참조하여 설명한 방법과 일치하므로 생 략하도록 한다. 다만, 제1 카메라 및 제2 카메라와 객체 간의 거리를 식별하는 라인이 각각의 카메라의 FOV의 중심을 관통하지 않는 경우에는 상술한 각각의 카메라의 시야 각을 이용하여 변위 값을 산출하는 방법은 적용되 지 않는다. 한편, 도면에 명확히 도시되지는 않았으나, 본 개시의 실시 예에 따라 객체에 대한 제1 카메라와 제2 카메 라의 거리가 상이할 수 있다. 즉, 제1 카메라와 객체 간의 거리와 제2 카메라와 객체 간의 거리가 상이할 수 있다. 이러한 경우, 프로세서는 각각의 카메라의 객체와의 거리에 기초하여, 제1 카메라로부터 획득 가 능한 제3 이미지와 제2 카메라로부터 획득한 제2 이미지를 동일한 평면 상에 배치한 후 상술한 방법에 기초하여 제1 내지 제4 변위 값을 식별할 수 있을 것이다. 도 20은 본 개시의 일 실시 예에 따른 전자 장치의 세부 구성도이다. 도 20을 참조하면, 본 개시의 일 실시 예에 따라 전자 장치는 디스플레이, 제1 카메라, 제2 카 메라, 프로세서, 메모리, 센서, 입출력 인터페이스 및 통신부를 포함한다. 전자 장치의 디스플레이, 제1 카메라, 제2 카메라 및 프로세서에 대해서는 상술하여 설명한 바, 생략하도록 한다. 메모리에는 전자 장치가 동작하기 위한 소프트웨어 프로그램 및 어플리케이션이 저장될 수 있으며, 프로그램 또는 애플리케이션의 실행 중에 입력되거나 설정 또는 생성되는 각종 데이터 등과 같은 다양한 정보가 저장될 수 있다. 예를 들어, 메모리는 본 개시의 일 실시 예에 따라, FPGA(Field Programmable gatearray)의 형태로 구현될 수 있다. 또한, 메모리에는 제2 이미지를 기초로 획득한 객체의 위치 정보가 저장 될 수 있다. 또한 메모리에는 제1 카메라와 제2 카메라의 이격 거리 정보, 제1 카메라의 시야 각 정보 및 제2 카메라의 시야 각 정보가 저장될 수 있다. 한편, 전자 장치는 센서를 이용하여, 전자 장치에 관한 다양한 정보를 획득한다. 예를 들어, 전 자 장치는 센서를 통해 제1 카메라와 객체 간의 거리 및 제2 카메라와 객체 간의 거리 값 을 식별할 수 있다. 이를 위해 센서는 각각의 카메라(예를 들어, 제1 카메라 및 제2 카메라)에 포함된 ToF 센서로 구현될 수 있다. 또한, 전자 장치는 입출력 인터페이스를 통해, 사용자로부터 전자 장치의 제어와 관련된 다양한 정보를 입력 받을 수 있다. 예를 들어, 전자 장치는 제1 카메라 또는 제2 카메라의 시야 각 또는 초점 거 리를 조정하는 명령을 입출력 인터페이스를 통해 입력 받을 수 있다. 보다 구체적으로, 전자 장치는 일반 모드의 제1 카메라를 줌 모드로 변경하는, 사용자의 입력을 입출력 인터페이스를 통해 입력 받는다. 이를 위해, 입출력 인터페이스는 버튼, 터치 패드, 마우스 및 키보드와 같은 장치로 구현되거나, 상술한 디스플레이 기능 및 조작 입력 기능도 함께 수행 가능한 터치 스크린, 리모컨 송 수신부 등으로 구현될 수 있다. 또한, 전자 장치는 통신부를 통해, 무선 통신 기술이나 이동 통신 기술을 이용하여 각종 외부 장치와 통신을 수행하여 객체 및 전자 장치와 관련된 다양한 정보를 송수신할 수 있다. 한편, 무선 통신 기술로는, 예 를 들어, 블루투스(Bluetooth), 저전력 블루투스(Bluetooth Low Energy), 캔(CAN) 통신, 와이 파이(Wi-Fi), 와 이파이 다이렉트(Wi-Fi Direct), 초광대역 통신(UWB, ultrawide band), 지그비(zigbee), 적외선 통신(IrDA, infrared Data Association) 또는 엔에프씨(NFC, Near Field Communication) 등이 포함될 수 있으며, 이동 통 신 기술 로는, 3GPP, 와이맥스(Wi-Max), LTE(Long Term Evolution), 5G 등이 포함될 수 있다. 한편, 이상에서 설명된 다양한 실시 예들은 소프트웨어(software), 하드웨어(hardware) 또는 이들의 조합을 이 용하여 컴퓨터(computer) 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 일부 경우 에 있어 본 명세서에서 설명되는 실시 예들이 프로세서 자체로 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들은 별도의 소프트웨어 모듈들로 구현될 수 있다. 소프트 웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 동작을 수행할 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 전자 장치의 프로세싱 동작을 수행하기 위한 컴퓨터 명령어 (computer instructions)는 비일시적 컴퓨터 판독 가능 매체(non-transitory computer-readable medium)에 저 장될 수 있다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령어는 특정 기기의 프로세서에 의해 실행되었을 때 상술한 다양한 실시 예에 따른 전자 장치에서의 처리 동작을 특정 기기가 수행하도록 한다. 비일시적 컴퓨터 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체 가 아니라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 능한 매체를 의미한다. 비일시적 컴 퓨터 판독 가능 매체의 구체적인 예로는, CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등이 있을 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2022-0013648", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해 되어져서는 안될 것이다."}
{"patent_id": "10-2022-0013648", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은, 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 도면이다. 도 2는, 본 개시의 일 실시 예에 따른 전자 장치의 블록도이다. 도 3은, 본 개시의 일 실시 예에 따른 제2 카메라를 이용하여 획득한 제2 이미지를 기초로, 제1 카메라의 줌 보 드에서 획득한 제1 이미지 내 객체를 검출하는 방법을 개략적으로 나타낸 순서도이다. 도 4는, 본 개시의 일 실시 예에 따른 줌 모드에서의 줌 인 비율에 기초하여 제1 이미지에 대응되는 일반 모드 에서의 상기 제1 카메라의 제3 이미지를 식별하는 것을 나타낸 예시도이다. 도 5는, 본 개시의 일 실시 예에 따른 제2 카메라를 이용하여 획득한 제2 이미지에서 객체를 검출하여, 객체의 위치 정보를 획득하는 것을 나타낸 예시도이다. 도 6은, 본 개시의 일 실시 예에 따른 제2 이미지와 제3 이미지의 상대적인 위치를 식별한 후 제3 이미지에 제2 이미지를 적층시키는 것을 나타낸 예시도이다. 도 7은, 본 개시의 일 실시 예에 따른, 제 2 이미지와 제3 이미지의 상대적인 위치를 기초로, 제3 이미지에서의 객체의 위치를 식별하는 방법을 개략적으로 나타낸 순서도이다. 도 8은, 본 개시의 일 실시 예에 따른, 제 2 이미지와 제3 이미지의 상대적인 위치를 기초로, 제3 이미지에서의 객체의 위치를 식별하는 것을 나타낸 예시도이다. 도 9는, 본 개시의 일 실시 예에 따른, 제3 이미지에서의 제1 이미지에 대응되는 프레임의 크기 및 위치를 식별 하는 것을 나타낸 예시도이다. 도 10은, 본 개시의 일 실시 예에 따른, 제1 이미지에 대응되는 프레임 포함된, 제3 및 제4 좌표 값을 기초로 객체를 식별하는 것을 나타낸 예시도이다. 도 11은, 본 개시의 일 실시 예에 따른, 제3 이미지에서의 객체의 위치 정보에 기초하여, 제1 이미지에서 객체 를 검출하지 않는 것을 나타낸 예시도이다. 도 12는, 본 개시의 일 실시 예에 따른, 제3 이미지에서의 객체의 위치 정보에 기초하여, 제1 이미지에서 객체 를 검출하여 표시하는 방법을 개략적으로 나타낸 순서도이다. 도 13은, 본 개시의 일 실시 예에 따른, 제3 이미지에서의 상기 객체의 위치 정보에 기초하여, 제1 이미지에서 객체를 검출하여 표시하는 것을 나타낸 예시도이다. 도 14는 본 개시의 일 실시 예에 따른, 제1 이미지 상에 객체를 검출하여 표시하는 전자 장치의 예시도이다. 도 15는 본 개시의 일 실시 예에 따른, 제5 좌표 값 및 제6 좌표 값을 기초로 식별된 제2 바운딩 박스에 기초하 여 제1 이미지에서 객체를 검출하지 않는 것을 나타낸 예시도이다. 도 16은, 본 개시의 일 실시 예에 따른, 제2 이미지를 기초로 획득된 객체의 객체 인식 정보를 제1 이미지에 표 시하는 것을 개략적으로 나타낸 순서도이다. 도 17은, 본 개시의 일 실시 예에 따른, 제2 이미지를 기초로 획득된 객체의 객체 인식 정보를 제1 이미지에 표 시하는 것을 개략적으로 나타낸 예시도이다. 도 18은, 본 개시의 일 실시 예에 따른, 제1 카메라와 제2 카메라의 전자 장치에서의 배치된 이격 거리, 제1 카메라의 시야 각 및 제2 카메라의 시야 각에 기초하여, 제2 이미지 및 제3 이미지 간의 상대적인 위치를 식별하 는 것을 나타낸 예시도이다. 도 19는, 본 개시의 다른 실시 예에 따른, 제1 카메라와 제2 카메라의 전자 장치에서의 배치된 이격 거리, 제1 카메라의 시야 각 및 제2 카메라의 시야 각에 기초하여, 제2 이미지 및 제3 이미지 간의 상대적인 위치를 식별 하는 것을 나타낸 예시도이다. 도 20은 본 개시의 일 실시 예에 따른 전자 장치의 세부 구성도이다."}
