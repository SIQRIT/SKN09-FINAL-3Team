{"patent_id": "10-2023-0015606", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0119609", "출원번호": "10-2023-0015606", "발명의 명칭": "인공지능을 이용한 조음음운장애 검사 장치 및 방법", "출원인": "가톨릭대학교 산학협력단", "발명자": "장대현"}}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자 단말와 데이터를 송수신하도록 이루어지는 통신부; 및상기 사용자 단말로부터 수신된 음성 정보 및 레퍼런스 텍스트를 기반으로 음소 에러율 및 자음 에러율을 산출하고, 상기 음소 에러율 및 상기 자음 에러율이 상기 사용자 단말에서 표시되도록 상기 음소 에러율 및 상기 자음 에러율을 상기 사용자 단말로 전송하도록 이루어지는 프로세서를 포함하고, 상기 프로세서는,상기 음성 정보를 텍스트로 변환하도록 이루어지는 음성인식 모듈; 및상기 변환된 텍스트 및 상기 레퍼런스 텍스트에 기반하여 상기 음소 에러율 및 상기 자음 에러율을 산출하도록이루어지는 조음음운검사 평가 모듈을 포함하고,상기 상기 변환된 텍스트는,자음과 모음이 분리된 텍스트 정보를 포함하는 것을 특징으로 하는 조음음운장애 검사 장치."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 조음음운검사 평가 모듈은,상기 레퍼런스 텍스트를 기준으로, 상기 음성 정보에 대한 상기 음소 에러율 및 상기 자음 에러율을 산출하는것을 특징으로 하는 조음음운장애 검사 장치."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 조음음운검사 평가 모듈은,상기 변환된 텍스트 및 상기 레퍼런스 텍스트 각각의 자음 및 모음을 분리하여 풀어쓰기 데이터를 생성하고,상기 프로세서는,상기 풀어쓰기 데이터가 상기 사용자 단말에서 표시되도록, 상기 풀어쓰기 데이터를 상기 사용자 단말로 전송하는 것을 특징으로 하는 조음음운장애 검사 장치."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 조음음운검사 평가 모듈은,상기 레퍼런스 텍스트 및 상기 변환된 텍스트 각각에 대응되는 풀어쓰기 데이터를 기반으로 상기 음소 에러율및 상기 자음 에러율을 산출하는 것을 특징으로 하는 조음음운장애 검사 장치."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 음성인식 모듈은,제1음성인식 모델 및 제2음성인식 모델을 포함하고,상기 음성 정보에 대한 상기 제1 및 제2음성인식 모델 각각의 출력값의 비교결과에 기반하여, 상기 음성 정보를공개특허 10-2023-0119609-3-텍스트로 변환하는 것을 특징으로 하는 조음음운장애 검사 장치."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 제1음성인식 모델은 KALDI 모델이고,상기 제2음성인식 모델은 e2e 모델인 것을 특징으로 하는 조음음운장애 검사 장치."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4항에 있어서,상기 음성 정보는 복수의 음성 정보를 포함하고,상기 레퍼런스 텍스트는 복수의 레퍼런스 텍스트를 포함하고,상기 조음음운검사 평가 모듈은,상기 복수의 음성 정보 중 어느 하나의 음성 정보에 대응되는 레퍼런스 텍스트를 기반으로, 상기 어느 하나의음성 정보에 대한 음소 에러율 및 자음 에러율을 산출하는 것을 특징으로 하는 조음음운장애 검사 장치."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "사용자 단말 및 서버를 포함하는 조음음운 장애 검사 방법에 있어서,상기 서버가 상기 사용자 단말로부터 음성 정보 및 레퍼런스 텍스트를 수신하는 단계;상기 음성 정보를 텍스트로 변환하는 단계;상기 서버가 상기 변환된 텍스트 및 레퍼런스 텍스트를 기반으로 음소 에러율 및 자음 에러율을 산출하는 단계;및상기 음소 에러율 및 상기 자음 에러율이 상기 사용자 단말에서 표시되도록, 상기 서버가 상기 음소 에러율 및상기 자음 에러율을 상기 사용자 단말로 전송하는 단계를 포함하고, 상기 상기 변환된 텍스트는,자음과 모음이 분리된 텍스트 정보를 포함하는 것을 특징으로 하는 조음음운장애 검사 방법."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 음소 에러율 및 자음 에러율을 산출하는 단계는,상기 레퍼런스 텍스트를 기준으로, 상기 음성 정보에 대한 상기 음소 에러율 및 상기 자음 에러율을 산출하는것을 특징으로 하는 조음음운장애 검사 방법."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 서버가 상기 변환된 텍스트 및 상기 레퍼런스 텍스트 각각의 자음 및 모음을 분리하여 풀어쓰기 데이터를생성하는 단계; 및상기 풀어쓰기 데이터가 상기 사용자 단말에서 표시되도록, 상기 서버가 상기 풀어쓰기 데이터를 상기 사용자단말로 전송하는 단계를 더 포함하는 것을 특징으로 하는 조음음운장애 검사 방법."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 음소 에러율 및 자음 에러율을 산출하는 단계는,공개특허 10-2023-0119609-4-상기 레퍼런스 텍스트 및 상기 변환된 텍스트 각각에 대응되는 풀어쓰기 데이터를 기반으로 상기 음소 에러율및 상기 자음 에러율을 산출하는 것을 특징으로 하는 조음음운장애 검사 방법."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 음성 정보를 텍스트로 변환하는 단계는,제1음성인식 모델 및 제2음성인식 모델 각각이 상기 음성 정보를 텍스트로 변환하는 단계; 및상기 음성 정보에 대한 상기 제1 및 제2음성인식 모델 각각의 출력값의 비교결과에 기반하여, 상기 음성 정보를텍스트로 변환하는 단계를 포함하는 것을 특징으로 하는 조음음운장애 검사 방법."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 제1음성인식 모델은 KALDI 모델이고,상기 제2음성인식 모델은 e2e 모델인 것을 특징으로 하는 조음음운장애 검사 방법."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 음성 정보는 복수의 음성 정보를 포함하고,상기 레퍼런스 텍스트는 복수의 레퍼런스 텍스트를 포함하고,상기 음소 에러율 및 자음 에러율을 산출하는 단계는,상기 복수의 음성 정보 중 어느 하나의 음성 정보에 대응되는 레퍼런스 텍스트를 기반으로, 상기 어느 하나의음성 정보에 대한 음소 에러율 및 자음 에러율을 산출하는 것을 특징으로 하는 조음음운장애 검사 방법."}
{"patent_id": "10-2023-0015606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "컴퓨터와 결합되어, 제8항 내지 제14항 중 어느 한 항의 조음음운장애 검사 방법을 실행시키기 위하여 컴퓨터판독 가능한 기록매체에 저장된 프로그램."}
{"patent_id": "10-2023-0015606", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 조음음운장애 검사 장치 및 방법에 관한 것이다. 본 개시에 일 측면에 따른 조음음운장애 검사 장치는, 사용자 단말와 데이터를 송수신하도록 이루어지는 통신부 및 상기 사용자 단말로부터 수신된 음성 정보 및 레퍼런스 텍스트를 기반으로 음소 에러율 및 자음 에러율을 산출하고, 상기 음소 에러율 및 상기 자음 에러율 이 상기 사용자 단말에서 표시되도록 상기 음소 에러율 및 상기 자음 에러율을 상기 사용자 단말로 전송하도록 이루어지는 프로세서를 포함하고, 상기 프로세서는 상기 음성 정보를 텍스트로 변환하도록 이루어지는 음성인식 모듈 및 상기 변환된 텍스트 및 상기 레퍼런스 텍스트에 기반하여 상기 음소 에러율 및 상기 자음 에러율을 산출 하도록 이루어지는 조음음운검사 평가 모듈을 포함하고, 상기 상기 변환된 텍스트는, 자음과 모음이 분리된 텍스 트 정보를 포함할 수 있다."}
{"patent_id": "10-2023-0015606", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 조음음운장애 검사 장치 및 방법에 관한 것이다. 보다 상세하게는, 본 개시는 인공지능을 이용한 조 음음운장애 검사 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0015606", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "조음음운 장애란 말소리 장애의 한 형태로서, 조음기관의 문제나 음소의 기능적인 오류로 인해 연령에 맞지 않 는 부정확한 발음을 산출하는 것임. 예를 들어, 30개월 연령의 아동이 “머리”를 “머이”라고 발음한 유음단 순화 오류패턴은 정상범주에 속하지만, 6세 아동이다면 비정상으로 판단된다. 조음음운 장애는 학령전 아동의 10-15%, 학령기 아동의 6%의 유병률을 가지는 비교적 흔한 질환이기 때문에 경 제적, 사회적 비용이 높은 질환이며, 일정수준 이하로 자음정확도가 저하되어 있을 때는 장애인보건법상 등록장 애인의 중증질환이 되기도 한다. 조음음운 장애 진단은 일반적으로 표준화된 조음음운 검사를 시행하여 전체적인 자음정확도를 산출하고, 오류패 턴 양상을 분석하여 이루어진다. 조음음운 장애의 평가는 훈련된 음성학자 및 언어재활사를 통해 시행되는데 검 사자의 숙련도에 따라 평가의 결과가 달라질 수 있으며, 일반적으로 발음의 오류패턴 분석은 검사 당시의 환자 음성을 녹음한 후 전사하여 시행하기 때문에 검사 이후 추가적인 분석 시간이 필요하다. 또한, 환자가 검사기관 으로 내원해야 하는 물리적인 단점이 있다.선행기술문헌 특허문헌 (특허문헌 0001) 공개특허공보 제10-2017-0036404호"}
{"patent_id": "10-2023-0015606", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시에 개시된 실시예는 비대면으로 조음음운 장애를 진단할 수 있는 장치 및 방법을 제공하는데 그 목적이 있다. 본 개시가 해결하고자 하는 과제들은 이상에서 언급된 과제로 제한되지 않으며, 언급되지 않은 또 다른 과제들 은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0015606", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 본 개시에 일 측면에 따른 조음음운장애 검사 장치는, 사용자 단말와 데이 터를 송수신하도록 이루어지는 통신부 및 상기 사용자 단말로부터 수신된 음성 정보 및 레퍼런스 텍스트를 기반 으로 음소 에러율 및 자음 에러율을 산출하고, 상기 음소 에러율 및 상기 자음 에러율이 상기 사용자 단말에서 표시되도록 상기 음소 에러율 및 상기 자음 에러율을 상기 사용자 단말로 전송하도록 이루어지는 프로세서를 포 함하고, 상기 프로세서는 상기 음성 정보를 텍스트로 변환하도록 이루어지는 음성인식 모듈 및 상기 변환된 텍 스트 및 상기 레퍼런스 텍스트에 기반하여 상기 음소 에러율 및 상기 자음 에러율을 산출하도록 이루어지는 조 음음운검사 평가 모듈을 포함하고, 상기 상기 변환된 텍스트는, 자음과 모음이 분리된 텍스트 정보를 포함할 수 있다. 일 실시 예에 있어서, 상기 조음음운검사 평가 모듈은 상기 레퍼런스 텍스트를 기준으로, 상기 음성 정보에 대 한 상기 음소 에러율 및 상기 자음 에러율을 산출할 수 있다. 일 실시 예에 있어서, 상기 조음음운검사 평가 모듈은 상기 변환된 텍스트 및 상기 레퍼런스 텍스트 각각의 자 음 및 모음을 분리하여 풀어쓰기 데이터를 생성하고, 상기 프로세서는 상기 풀어쓰기 데이터가 상기 사용자 단 말에서 표시되도록, 상기 풀어쓰기 데이터를 상기 사용자 단말로 전송할 수 있다. 일 실시 예에 있어서, 상기 조음음운검사 평가 모듈은 상기 레퍼런스 텍스트 및 상기 변환된 텍스트 각각에 대 응되는 풀어쓰기 데이터를 기반으로 상기 음소 에러율 및 상기 자음 에러율을 산출할 수 있다. 일 실시 예에 있어서, 상기 음성인식 모듈은 제1음성인식 모델 및 제2음성인식 모델을 포함하고 상기 음성 정보 에 대한 상기 제1 및 제2음성인식 모델 각각의 출력값의 비교결과에 기반하여, 상기 음성 정보를 텍스트로 변환 할 수 있다. 일 실시 예에 있어서, 상기 제1음성인식 모델은 KALDI 모델이고, 상기 제2음성인식 모델은 e2e 모델일 수 있다. 일 실시 예에 있어서, 상기 음성 정보는 복수의 음성 정보를 포함하고, 상기 레퍼런스 텍스트는 복수의 레퍼런 스 텍스트를 포함하고, 상기 조음음운검사 평가 모듈은 상기 복수의 음성 정보 중 어느 하나의 음성 정보에 대 응되는 레퍼런스 텍스트를 기반으로, 상기 어느 하나의 음성 정보에 대한 음소 에러율 및 자음 에러율을 산출할 수 있다. 또한, 본 발명에 일 측면에 따른 조음음운 장애 검사 방법은 사용자 단말 및 서버를 포함하고, 상기 서버가 상 기 사용자 단말로부터 음성 정보 및 레퍼런스 텍스트를 수신하는 단계, 상기 음성 정보를 텍스트로 변환하는 단 계, 상기 서버가 상기 변환된 텍스트 및 레퍼런스 텍스트를 기반으로 음소 에러율 및 자음 에러율을 산출하는 단계 및 상기 음소 에러율 및 상기 자음 에러율이 상기 사용자 단말에서 표시되도록, 상기 서버가 상기 음소 에 러율 및 상기 자음 에러율을 상기 사용자 단말로 전송하는 단계를 포함하고, 상기 상기 변환된 텍스트는 자음과 모음이 분리된 텍스트 정보를 포함할 수 있다. 이 외에도, 본 개시를 구현하기 위한 실행하기 위한 컴퓨터 판독 가능한 기록 매체에 저장된 컴퓨터 프로그램이 더 제공될 수 있다. 이 외에도, 본 개시를 구현하기 위한 방법을 실행하기 위한 컴퓨터 프로그램을 기록하는 컴퓨터 판독 가능한 기 록 매체가 더 제공될 수 있다."}
{"patent_id": "10-2023-0015606", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 전술한 과제 해결 수단에 의하면, 임상현장에서 비대면으로 가정에서 실행할 수 있기 때문에 조음음 운 장애를 조기에 진단하고 조기에 치료할 수 있게 된다. 본 개시의 효과들은 이상에서 언급된 효과로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재로 부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0015606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시 전체에 걸쳐 동일 참조 부호는 동일 구성요소를 지칭한다. 본 개시가 실시예들의 모든 요소들을 설명"}
{"patent_id": "10-2023-0015606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "하는 것은 아니며, 본 개시가 속하는 기술분야에서 일반적인 내용 또는 실시예들 간에 중복되는 내용은 생략한 다. 명세서에서 사용되는 '부, 모듈, 부재, 블록'이라는 용어는 소프트웨어 또는 하드웨어로 구현될 수 있으며, 실시예들에 따라 복수의 '부, 모듈, 부재, 블록'이 하나의 구성요소로 구현되거나, 하나의 '부, 모듈, 부재, 블 록'이 복수의 구성요소들을 포함하는 것도 가능하다. 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 직접적으로 연결되어 있는 경우뿐 아니라, 간접적으로 연결되어 있는 경우를 포함하고, 간접적인 연결은 무선 통신망을 통해 연결되는 것을 포함 한다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 명세서 전체에서, 어떤 부재가 다른 부재 \"상에\" 위치하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재가 존재하는 경우도 포함한다. 제 1, 제 2 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하기 위해 사용되는 것으로, 구성요소가 전술된 용어들에 의해 제한되는 것은 아니다. 단수의 표현은 문맥상 명백하게 예외가 있지 않는 한, 복수의 표현을 포함한다. 각 단계들에 있어 식별부호는 설명의 편의를 위하여 사용되는 것으로 식별부호는 각 단계들의 순서를 설명하는 것이 아니며, 각 단계들은 문맥상 명백하게 특정 순서를 기재하지 않는 이상 명기된 순서와 다르게 실시될 수 있다. 이하 첨부된 도면들을 참고하여 본 개시의 작용 원리 및 실시예들에 대해 설명한다. 본 명세서에서 '본 개시에 따른 장치'는 연산처리를 수행하여 사용자에게 결과를 제공할 수 있는 다양한 장치들 이 모두 포함된다. 예를 들어, 본 개시에 따른 장치는, 컴퓨터, 서버 장치 및 휴대용 단말기를 모두 포함하거나, 또는 어느 하나의 형태가 될 수 있다. 여기에서, 상기 컴퓨터는 예를 들어, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱(desktop), 랩톱 (laptop), 태블릿 PC, 슬레이트 PC 등을 포함할 수 있다. 상기 서버 장치는 외부 장치와 통신을 수행하여 정보를 처리하는 서버로써, 애플리케이션 서버, 컴퓨팅 서버, 데이터베이스 서버, 파일 서버, 게임 서버, 메일 서버, 프록시 서버 및 웹 서버 등을 포함할 수 있다. 상기 휴대용 단말기는 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), WiBro(Wireless Broadband Internet) 단말, 스마트 폰(Smart Phone) 등과 같은 모든 종류의 핸드헬드 (Handheld) 기반의 무선 통신 장치와 시계, 반지, 팔찌, 발찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head-mounted-device(HMD) 등과 같은 웨어러블 장치를 포함할 수 있다. 본 개시에 따른 조음운음검사 장치는 서버 및 단말기 중 적어도 하나에 의해 구현될 수 있다. 구체적으로, 본 개시에 따른 인공지능 모델 학습 장치는 서버 및 단말기 중 어느 하나에 의해 구현되거나, 서버 및 단말기 간의 데이터 송수신을 통해 시스템으로 구현될 수 있다. 이하에서는, 본 개시에 따른 조음음운장애 검사 장치에 대하여 설명한다. 도 1을 참고하면, 본 개시에 따른 조음음운장애 검사 장치는 서버 및 단말기 중 적어도 하나를 포함할 수 있다. 서버는 단말기와 네트워크로 연결되며, 단말기로부터 조음음운검사에 필요한 데이터를 수신한 후, 조음음운검사 결과를 단말기로 전송할 수 있다. 한편, 상기 단말기는 상술한 휴대용 단말기에 한정되지 않고, 프로세서가 탑재된 노트북, 데스크톱 (desktop), 랩톱(laptop), 태블릿 PC, 슬레이트 PC 등을 포함할 수 있는 것은 통상의 기술자에게 자명하다. 상술한 바와 같이, 본 개시에 따른 조음음운장애 검사 장치는 서버 및 단말기 간 데이터 송수신을 통해 구현될 수 있다. 이하에서는, 본 개시에 따른 조음음운장애 검사 장치를 구현하기 위한 서버 및 단말기 각각에 대하여 설명한다. 도 2는 본 개시의 조음음운장애 검사 장치에 포함된 서버의 블록도이다. 본 개시에 따른 서버는 통신부, 저장부 및 프로세서 중 적어도 하나를 포함할 수 있다. 통신부는 단말기, 외부 저장소(예를 들어, 데이터베이스(database, 140)), 외부 서버 및 클라우드 서버 중 적어도 하나와 통신을 수행할 수 있다. 한편, 외부 서버 또는 클라우드 서버에서는, 프로세서의 적어도 일부의 역할을 수행하도록 구성될 수 있다. 즉, 데이터 처리 또는 데이터 연산 등의 수행은 외부 서버 또는 클라우드 서버에서 이루어지는 것이 가능 하며, 본 발명에서는 이러한 방식에 대한 특별한 제한을 두지 않는다. 한편, 통신부는 통신하는 대상(예를 들어, 전자기기, 외부 서버, 디바이스 등)의 통신 규격에 따라 다양한 통신 방식을 지원할 수 있다. 예를 들어, 통신부는, WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), Wi-Fi(Wireless Fidelity) Direct, DLNA(Digital Living Network Alliance), WiBro(Wireless Broadband), WiMAX(World Interoperability for Microwave Access), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), LTE(Long Term Evolution), LTE-A(Long Term Evolution-Advanced), 5G(5th Generation Mobile Telecommunication ), 블루투스(Bluetooth™), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), UWB(Ultra-Wideband), ZigBee, NFC(Near Field Communication), Wi-Fi Direct, Wireless USB(Wireless Universal Serial Bus) 기술 중 적어도 하나를 이용하여, 통신 대상과 통신하도록 이루 어질 수 있다. 다음으로 저장부는, 본 발명과 관련된 다양한 정보를 저장하도록 이루어질 수 있다. 본 발명에서 저장부 는 본 발명에 따른 장치 자체에 구비될 수 있다. 이와 다르게, 저장부의 적어도 일부는, 데이터베이 스(database: DB, 140) 클라우드 저장소(또는 클라우드 서버) 중 적어도 하나를 의미할 수 있다. 즉, 저장부 는 본 발명에 따른 장치 및 방법을 위하여 필요한 정보가 저장되는 공간이면 충분하며, 물리적인 공간에대한 제약은 없는 것으로 이해될 수 있다. 이에, 이하에서는, 저장부, 데이터베이스, 외부 저장소, 클라우드 저장소(또는 클라우드 서버)를 별도로 구분하지 않고, 모두 저장부라고 표현하도록 한다. 다음으로, 프로세서는 본 발명과 관련된 장치의 전반적인 동작을 제어하도록 이루어질 수 있다. 프로세서 는 위에서 살펴본 구성요소들을 통해 입력 또는 출력되는 신호, 데이터, 정보 등을 처리하거나 사용자에게 적절한 정보 또는 기능을 제공 또는 처리할 수 있다. 프로세서는 적어도 하나의 CPU(Central Processing Unit, 중앙처리장치)를 포함하여, 본 발명에 따른 기능 을 수행할 수 있다. 도 2에 도시된 구성 요소들의 성능에 대응하여 적어도 하나의 구성요소가 추가되거나 삭제될 수 있다. 또한, 구 성 요소들의 상호 위치는 장치의 성능 또는 구조에 대응하여 변경될 수 있다는 것은 당해 기술 분야에서 통상의 지식을 가진 자에게 용이하게 이해될 것이다. 이하, 본 개시의 조음음운장애 검사 장치에 포함된 단말기에 대하여 구체적으로 설명한다. 도 3은 본 개시의 조음음운장애 검사 장치에 포함된 단말기의 블록도이다. 도 3을 참고하면, 본 개시에 따른 단말기는 통신부, 입력부, 표시부 및 프로세서 등 을 포함할 수 있다. 도 3에 도시된 구성요소들은 본 개시에 따른 조음음운장애 검사 장치를 구현하는데 있어서 필수적인 것은 아니어서, 본 명세서 상에서 설명되는 단말기는 위에서 열거된 구성요소들 보다 많거나, 또는 적 은 구성요소들을 가질 수 있다. 상기 구성요소들 중 통신부는 외부 장치와 통신을 가능하게 하는 하나 이상의 구성 요소를 포함할 수 있으 며, 예를 들어, 방송 수신 모듈, 유선통신 모듈, 무선통신 모듈, 근거리 통신 모듈, 위치정보 모듈 중 적어도 하나를 포함할 수 있다. 유선 통신 모듈은, 지역 통신(Local Area Network; LAN) 모듈, 광역 통신(Wide Area Network; WAN) 모듈 또는 부가가치 통신(Value Added Network; VAN) 모듈 등 다양한 유선 통신 모듈뿐만 아니라, USB(Universal Serial Bus), HDMI(High Definition Multimedia Interface), DVI(Digital Visual Interface), RS-1302(recommended standard1302), 전력선 통신, 또는 POTS(plain old telephone service) 등 다양한 케이블 통신 모듈을 포함할 수 있다. 무선 통신 모듈은 와이파이(Wifi) 모듈, 와이브로(Wireless broadband) 모듈 외에도, GSM(global System for Mobile Communication), CDMA(Code Division Multiple Access), WCDMA(Wideband Code Division Multiple Access), UMTS(universal mobile telecommunications system), TDMA(Time Division Multiple Access), LTE(Long Term Evolution), 4G, 5G, 6G 등 다양한 무선 통신 방식을 지원하는 무선 통신 모듈을 포함할 수 있 다. 입력부는 영상 정보(또는 신호), 오디오 정보(또는 신호), 데이터, 또는 사용자로부터 입력되는 정보의 입 력을 위한 것으로서, 적어도 하나의 카메라, 적어도 하나의 마이크로폰 및 사용자 입력부 중 적어도 하나를 포 함할 수 있다. 입력부에서 수집한 음성 데이터나 이미지 데이터는 분석되어 사용자의 제어명령으로 처리될 수 있다. 표시부는 시각, 청각 또는 촉각 등과 관련된 출력을 발생시키기 위한 것으로, 디스플레이부, 음향 출력부, 햅팁 모듈 및 광 출력부 중 적어도 하나를 포함할 수 있다. 디스플레이부는 터치 센서와 상호 레이어 구조를 이 루거나 일체형으로 형성됨으로써, 터치 스크린을 구현할 수 있다. 이러한 터치 스크린은, 본 장치와 사용자 사 이의 입력 인터페이스를 제공하는 사용자 입력부로써 기능함과 동시에, 본 장치와 사용자 간에 출력 인터페이스 를 제공할 수 있다. 디스플레이부는 본 장치에서 처리되는 정보를 표시(출력)한다. 예를 들어, 디스플레이부는 본 장치에서 구동되 는 응용 프로그램(일 예로, 어플리케이션)의 실행화면 정보, 또는 이러한 실행화면 정보에 따른 UI(User Interface), GUI(Graphic User Interface) 정보를 표시할 수 있다. 상술한 구성요소 외에, 상술한 단말기는 인터페이스부 및 메모리를 더 포함할 수 있다. 인터페이스부는 본 장치에 연결되는 다양한 종류의 외부 기기와의 통로 역할을 수행한다. 이러한 인터페이스부 는 유/무선 헤드셋 포트(port), 외부 충전기 포트(port), 유/무선 데이터 포트(port), 메모리 카드(memory card) 포트, 식별 모듈(SIM)이 구비된 장치를 연결하는 포트(port), 오디오 I/O(Input/Output) 포트(port), 비디오 I/O(Input/Output) 포트(port), 이어폰 포트(port) 중 적어도 하나를 포함할 수 있다. 본 장치에서는, 상 기 인터페이스부에 연결된 외부 기기와 관련된 적절한 제어를 수행할 수 있다. 메모리는 본 장치의 다양한 기능을 지원하는 데이터와, 프로세서의 동작을 위한 프로그램을 저장할 수 있고, 입 /출력되는 데이터들(예를 들어, 음악 파일, 정지영상, 동영상 등)을 저장할 있고, 본 장치에서 구동되는 다수의 응용 프로그램(application program 또는 애플리케이션(application)), 본 장치의 동작을 위한 데이터들, 명령 어들을 저장할 수 있다. 이러한 응용 프로그램 중 적어도 일부는, 무선 통신을 통해 외부 서버로부터 다운로드 될 수 있다. 이러한, 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), SSD 타입(Solid State Disk type), SDD 타입(Silicon Disk Drive type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(random access memory; RAM), SRAM(static random access memory), 롬(read-only memory; ROM), EEPROM(electrically erasable programmable read-only memory), PROM(programmable read-only memory), 자기 메모리, 자기 디스크 및 광디스 크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 또한, 메모리는 본 장치와는 분리되어 있으나, 유선 또는 무선으로 연결된 데이터베이스가 될 수도 있다. 한편, 상술한 단말기는 프로세서를 포함한다. 프로세서는 본 장치 내의 구성요소들의 동작을 제어하기 위 한 알고리즘 또는 알고리즘을 재현한 프로그램에 대한 데이터를 저장하는 메모리, 및 메모리에 저장된 데이터를 이용하여 전술한 동작을 수행하는 적어도 하나의 프로세서(미도시)로 구현될 수 있다. 이때, 메모리와 프로세서 는 각각 별개의 칩으로 구현될 수 있다. 또는, 메모리와 프로세서는 단일 칩으로 구현될 수도 있다. 한편, 도 3과 같이, 서버 및 단말기 중 적어도 하나에 포함된 프로세서는 후술할 조음음운장애 검사 장치를 구 현하기 위한 복수의 모듈을 포함할 수 있다. 구체적으로, 프로세서는 음성인식 모듈 및 조음음운검 사 평가 모듈을 포함할 수 있다. 후술하는 인공지능 모델 학습 방법은 상기 모듈들의 동작에 의해 구현되 는 것으로 서술하나, 후술하는 각 단계의 동작의 수행이 반드시 상기 모듈들에 의해 수행될 필요는 없다. 또한, 프로세서는 이하의 도면에서 설명되는 본 개시에 따른 다양한 실시 예들을 본 장치 상에서 구현하기 위하 여, 위에서 살펴본 구성요소들을 중 어느 하나 또는 복수를 조합하여 제어할 수 있다. 한편, 도 1 내지 3에 도시된 구성 요소들의 성능에 대응하여 적어도 하나의 구성요소가 추가되거나 삭제될 수 있다. 또한, 구성 요소들의 상호 위치는 장치의 성능 또는 구조에 대응하여 변경될 수 있다는 것은 당해 기술 분야에서 통상의 지식을 가진 자에게 용이하게 이해될 것이다. 이하에서는, 본 발명에서 서술되는 인공지능에 대하여 구체적으로 설명한다. 본 개시에 따른 인공지능과 관련된 기능은 상술한 서버 및 단말기에 탑재된 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래 픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미 한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도 형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경 망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의 해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep NeuralNetwork)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 본 개시의 예시적인 실시예에 따르면, 프로세서는 인공지능을 구현할 수 있다. 인공지능이란 사람의 신경세포 (biological neuron)를 모사하여 기계가 학습하도록 하는 인공신경망(Artificial Neural Network) 기반의 기계 학습법을 의미한다. 인공지능의 방법론에는 학습 방식에 따라 훈련데이터로서 입력데이터와 출력데이터가 같이 제공됨으로써 문제(입력데이터)의 해답(출력데이터)이 정해져 있는 지도학습(supervised learning), 및 출력데 이터 없이 입력데이터만 제공되어 문제(입력데이터)의 해답(출력데이터)이 정해지지 않는 비지도학습 (unsupervised learning), 및 현재의 상태(State)에서 어떤 행동(Action)을 취할 때마다 외부 환경에서 보상 (Reward)이 주어지는데, 이러한 보상을 최대화하는 방향으로 학습을 진행하는 강화학습(reinforcement learning)으로 구분될 수 있다. 또한, 인공지능의 방법론은 학습 모델의 구조인 아키텍처에 따라 구분될 수도 있는데, 널리 이용되는 딥러닝 기술의 아키텍처는, 합성곱신경망(CNN; Convolutional Neural Network), 순환신 경망(RNN; Recurrent Neural Network), 트랜스포머(Transformer), 생성적 대립 신경망(GAN; generative adversarial networks) 등으로 구분될 수 있다. 본 장치와 시스템은 인공지능 모델을 포함할 수 있다. 인공지능 모델은 하나의 인공지능 모델일 수 있고, 복수 의 인공지능 모델로 구현될 수도 있다. 인공지능 모델은 뉴럴 네트워크(또는 인공 신경망)로 구성될 수 있으며, 기계학습과 인지과학에서 생물학의 신경을 모방한 통계학적 학습 알고리즘을 포함할 수 있다. 뉴럴 네트워크는 시냅스의 결합으로 네트워크를 형성한 인공 뉴런(노드)이 학습을 통해 시냅스의 결합 세기를 변화시켜, 문제 해 결 능력을 가지는 모델 전반을 의미할 수 있다. 뉴럴 네트워크의 뉴런은 가중치 또는 바이어스의 조합을 포함할 수 있다. 뉴럴 네트워크는 하나 이상의 뉴런 또는 노드로 구성된 하나 이상의 레이어(layer)를 포함할 수 있다. 예시적으로, 장치는 input layer, hidden layer, output layer를 포함할 수 있다. 장치를 구성하는 뉴 럴 네트워크는 뉴런의 가중치를 학습을 통해 변화시킴으로써 임의의 입력(input)으로부터 예측하고자 하는 결과 (output)를 추론할 수 있다. 프로세서는 뉴럴 네트워크를 생성하거나, 뉴럴 네트워크를 훈련(train, 또는 학습(learn)하거나, 수신되는 입력 데이터를 기초로 연산을 수행하고, 수행 결과를 기초로 정보 신호(information signal)를 생성하거나, 뉴럴 네 트워크를 재훈련(retrain)할 수 있다. 뉴럴 네트워크의 모델들은 GoogleNet, AlexNet, VGG Network 등과 같은 CNN(Convolution Neural Network), R-CNN(Region with Convolution Neural Network), RPN(Region Proposal Network), RNN(Recurrent Neural Network), S-DNN(Stacking-based deep Neural Network), S-SDNN(State-Space Dynamic Neural Network), Deconvolution Network, DBN(Deep Belief Network), RBM(Restrcted Boltzman Machine), Fully Convolutional Network, LSTM(Long Short-Term Memory) Network, Classification Network 등 다양한 종류의 모델들을 포함할 수 있으나 이에 제한되지는 않는다. 프로세서는 뉴럴 네트워크의 모델들에 따른 연산을 수행하기 위한 하나 이상의 프로세서를 포함할 수 있다. 예를 들어 뉴럴 네트워크는 심층 뉴럴 네트워크 (Deep Neural Network)를 포함할 수 있다. 뉴럴 네트워크는 CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), 퍼셉트론(perceptron), 다층 퍼셉트론(multilayer perceptron), FF(Feed Forward), RBF(Radial Basis Network), DFF(Deep Feed Forward), LSTM(Long Short Term Memory), GRU(Gated Recurrent Unit), AE(Auto Encoder), VAE(Variational Auto Encoder), DAE(Denoising Auto Encoder), SAE(Sparse Auto Encoder), MC(Markov Chain), HN(Hopfield Network), BM(Boltzmann Machine), RBM(Restricted Boltzmann Machine), DBN(Depp Belief Network), DCN(Deep Convolutional Network), DN(Deconvolutional Network), DCIGN(Deep Convolutional Inverse Graphics Network), GAN(Generative Adversarial Network), LSM(Liquid State Machine), ELM(Extreme Learning Machine), ESN(Echo State Network), DRN(Deep Residual Network), DNC(Differentiable Neural Computer), NTM(Neural Turning Machine), CN(Capsule Network), KN(Kohonen Network) 및 AN(Attention Network)를 포함 할 수 있으나 이에 한정되는 것이 아닌 임의의 뉴럴 네트워크를 포함할 수 있음은 통상의 기술자가 이해할 것이다. 본 개시의 예시적인 실시예에 따르면, 프로세서는 GoogleNet, AlexNet, VGG Network 등과 같은 CNN(Convolution Neural Network), R-CNN(Region with Convolution Neural Network), RPN(Region Proposal Network), RNN(Recurrent Neural Network), S-DNN(Stacking-based deep Neural Network), S-SDNN(State-Space Dynamic Neural Network), Deconvolution Network, DBN(Deep Belief Network), RBM(Restrcted BoltzmanMachine), Fully Convolutional Network, LSTM(Long Short-Term Memory) Network, Classification Network, Generative Modeling, eXplainable AI, Continual AI, Representation Learning, AI for Material Design, 자 연어 처리를 위한 BERT, SP-BERT, MRC/QA, Text Analysis, Dialog System, GPT-3, GPT-4, 비전 처리를 위한 Visual Analytics, Visual Understanding, Video Synthesis, ResNet 데이터 지능을 위한 Anomaly Detection, Prediction, Time-Series Forecasting, Optimization, Recommendation, Data Creation 등 다양한 인공지능 구 조 및 알고리즘을 이용할 수 있으며, 이에 제한되지 않는다. 이하, 첨부된 도면을 참조하여 본 개시의 실시예를 상세하게 설명한다. 이하에서는, 상술한 구성요소를 활용한 조음음운장애 검사 방법에 대해 구체적으로 설명한다. 후술하는 조음음 운장애 검사 방법은 상술한 서버 및 단말기 간 데이터 송수신을 통해 구현되며, 상기 방법의 일부 단계는 서버 및 단말기 중 어느 하나에서 수행될 수 있다. 다만, 이에 한정되지 않고 후술하는 방법은 서버 및 단말기 중 어 느 하나에서 단독으로 수행될 수 있음은 통상의 기술자에게 자명하다. 도 5는 본 개시에 따른 조음음운장애 검사 방법의 흐름도이고, 도 6a 내지 6d는 본 개시에 따른 음성인식 모듈 을 나타내는 블록도이고, 도 7 내지 15는 본 개시에 따른 조음음운검사 프로그램의 실행화면이다. 도 5를 참조하면, 음성 정보를 입력 받는 단계가 진행된다(S110). 사용자 단말은 사용자로부터 녹음된 파일을 입력 받거나, 마이크를 통해 음성 정보를 입력 받을 수 있다. 예를 들어, 도 7을 참조하면, 사용자가 사용자 단말에서 조음음운검사 프로그램 실행 시, 사용자 단말에는 조음 음운검사 프로그램의 실행화면이 표시된다. 사용자는 상기 실행화면에서 음성 녹음 버튼 또는 파일 입력 버튼을 선택하여 음성 정보를 입력할 수 있다. 일 실시 예에 있어서, 도 8과 같이, 사용자는 음성 녹음을 활성화한 상태에서 검사대상에게 특정 텍스트를 보여 주고 발음하도록 함으로써, 마이크를 통해 검사 대상의 음성이 입력되도록 할 수 있다. 다른 일 실시 예에 있어서, 도 9와 같이, 사용자는 별도의 녹음 애플리케이션을 실행 후, 검사대상에게 특정 텍 스트를 보여주고 발음하도록 함으로써, 녹음 파일을 생성할 수 있다. 이후, 상기 녹음 파일을 조음음운검사 프 로그램에 업로드할 수 있다. 한편, 다시 도 7을 참조하면, 조음음운검사 프로그램의 실행화면은 입력된 음성 정보와 관련된 메뉴(411 내지 413)을 포함할 수 있다. 사용자는 음성 정보와 관련된 메뉴(411 내지 413)를 통해, 상기 음성 정보에 대한 음성 인식 결과를 확인하거나, 음성 정보를 청취할 수 있다. 다음으로 레퍼런스 텍스트를 입력 받는 단계가 진행된다(S120). 레퍼런스 텍스트는 검사대상이 발음해야하는 글자일 수 있다. 즉, 레퍼런스 텍스트는 조음음운검사시 정답에 해 당하는 텍스트이다. 사용자는 검사대상에게 발음하도록 한 특정 텍스트를 레퍼런스 텍스트로 입력할 수 있다. 도 7을 참조하면, 사용자는 조음음운검사 프로그램의 실행화면 중 레퍼런스 텍스트 입력 메뉴를 선택한 후 텍스트를 입력할 수 있다. 한편, 도시되지 않았지만, 사용자는 상기 레퍼런스 텍스트를 별도의 텍스트 파일로 업로드할 수 있다. 사용자가 음성 정보 및 레퍼런스 텍스트를 입력한 후 전송 버튼을 선택하면, 사용자 단말은 상기 음성 정 보 및 상기 레퍼런스 텍스트를 서버로 전송할 수 있다. 이후, 서버에 포함된 음성인식 모듈이 입력된 음성 정보를 텍스트로 변환하는 단계가 진행된다(S130). 상기 음성인식 모듈은 음성 정보를 텍스트로 변환하도록 이루어진다. 음성인식 모듈은 음향모델, 발음모델, 언어모델을 포함할 수 있다. 음성 정보가 입력되면, 음향 모델은 소리값과 가장 비슷한 음소를 탐색한다. 음향 모델은 한국인 성인 및 아동 발화를 녹음한 음성 파일을 통해 훈련될 수 있다. 한편, 발음 모델은 가장 확률이 높은 음소 연쇄를 선정할 수 있다. 이때, 발생 가능한 오류 발음 연쇄 목록이 적용될 수 있다. 한편, 언어 모델은 가장 확률이 높은 단어 연쇄를 선정할 수 있다. 이때, 조음 발달 순서를 고려하여 오류 빈도 의 위례를 나누어 가중치를 차등 부여할 수 있다. 입력된 음성 정보는 음향모델, 발음모델, 언어모델을 거쳐 텍스트로 변환된다. 상기 변환된 텍스트는 후술할 음 소 에러율 및 자음 에러율 산출에 활용된다. 한편, 음성인식 모듈은 도 6a와 같이, 세분화될 수 있다. 구체적으로, 음성인식 모듈은 발음오류사전 , 제1음성인식 모델, 제2음성인식 모델 및 평가셋을 포함할 수 있다. 발음오류사전은 아동의 연력에 따라 정상 발달 중에도 나타날 수 있는 발달적 발음 오류와 연령과 무관하 게 비발달적으로 나타나는 발음 오류의 패턴 정보를 포함할 수 있다. 일 실시 예에 있어서, 발음오류사전은 APAC/UTAP 단어에 대해, 임상을 통해 알려진 조음 장애의 유형들을 적용한 발음 오류를 목록화 한 것일 수 있다. 상기 발음오류사전에 포함된 데이터는 후술할 발음 사전, 언어모델 및 핫워드로 활용될 수 있다. 한편, 제1음성인식 모델은 KALDI 모델 일 수 있다. 제1음성인식 모델의 경우 목표 단어별로 음성인식 모델을 분리가능하도록 설계하였다. 한편, 제2음성인식 모델은 e2e 모델 일 수 있다. 음성인식 모듈은 음성 정보에 대한 상기 제1 및 제2음성인식 모델 각각의 출력값의 비교결과에 기반하여, 상기 음성 정보를 텍스트로 변환할 수 있다. 구체적으로, 음성인식 모듈에 포함된 평가셋은 각 음성 인식 모델의 출력된 값을 비교하여 둘 중 더 나은 인식 결과를 선택할 수 있다. 한편, 제1 및 제2음성인식 모델 중 어느 하나의 출력값이 상기 평가셋에 의해 선택되는 경우, 제1 및 제2 음성인식 모델 중 다른 하나는 상기 어느 하나의 출력값을 이용하여 상기 다른 하나의 가중치를 수정할 수 있다. 이를 통해, 제1 및 제2음성인식 모델은 조음음운검사 평가를 진행할수록 그 인식 정확도가 향상될 수 있 다. 한편, 도 6b와 같이, 음성인식 모듈은 DNN-HMM 방식 Speech-to-Text(STT) 모델, e2e 방식 Speech- to-Text(STT) 모델을 포함할 수 있다. 각 모델은 조음 장애 환자 음성 정보를 입력 받은 후 각각의 입력 결과(317 및 319)를 출력할 수 있다. 여기서, DNN-HMM 방식 Speech-to-Text(STT) 모델은 도 6a에서 설명한 KALDI 모델이고, e2e 방식 Speech-to-Text(STT) 모델은 도 6a에서 설명한 e2e 모델일 수 있다. DNN-HMM 방식 STT 모델의 경우, 도 6c와 같은 구조로 이루어진다. 구체적으로, 상기 모델이 음성 신호를 입력 받으면, 음향 모델, 발음사전 및 언어모델로 순차적으로 전달되어 음성인식 결과를 출력할 수 있다. 여기서 발음사전은 단어, 발음열로 구성되며, DNN-HMM 방식 STT 모델에서 인식 결과의 범위를 발음사전에 등재된 단어 목록으로 한정할 수 있다. 즉, DNN-HMM 방식 STT 모델은 “들리는대로” 인식하기 보다는 발음사전 에 등재된 단어 중 언어모델에 표현된 단어연쇄 중 높은 확률값을 가지는 결과를 출력하도록 훈련된다. 일 실시 예에 있어서, DNN-HMM 방식 STT 모델에 사용된 음향 모델은 성인 음성 6200시간, 아동 음성 3200시간이 다. 언어모델의 경우, jsgf는 발음오류사전과 개별 오류에 대한 가중치를 동일하게 설정하였고, Ngram은 단어유 형별 가중치에 차등을 두었다. 예를 들어, 실제임상관측발음 : 발달적오류발음 : 정상발음 : 비발달적오류발음 =4:3:2:1로 설정하거나, 실제전사:발달오류:정조음:비발달오류=10:5:3:1로 설정하였다. 한편, 모델그래프 검색 범위를 nbest=1, beam=1, lattice beam=5 또는 nbest=30, beam=20, lattice beam=15로 확장하였다. 이때, 실험 결과 최고 성능은 PER(Phoneme-Error-Rate) 38.65%이었다. 이때, DNN-HMM 방식 STT 모델의 발음사전에 긴장 음화와 파열음화 연쇄를 적용하였다. 예를 들어, “바지”에 대한 발음 오류 유형으로 “바띠”, “빠티”, “빠디”, “빠띠”를 적용하였다. 한편, e2e 방식 STT 모델의 경우, 도 6d와 같은 구조로 이루어진다. 구체적으로, 상기 모델이 음성 신호를 입력 받으면, 음향모델로 전달된 후, 음성인식 결과를 출력할 수 있다. 이때, 언어모델 및 핫 워드가 출력 결과에 영향을 줄 수 있다. 이때, 언어모델과 핫워드는 출력 결과에 필수적으로 반 영될 필요 없다. 즉, e2e 방식 STT 모델은 언어모델 및 핫워드에 포함된 단어 목록 외의 단어도 인식결과로 출력하도록 훈련될 수 있다. e2e 방식 STT 모델은 들리는대로 인식하도록 훈련하되, 특정 단어 혹은 연쇄에 가중치를 설정할 수 있다. 일 실시 예에 있어서, e2e 모델은 대용량 사전학습모델 (pre-trained model)에 인식 과제 맞춤형 데이터로 파인 튜닝(fine-tuning)하는 방식으로 훈련되었다. 한편, 훈련 알고리즘으로 Facebook이 개발한 wav2vec2.0이 활용되 었으며, pretraining DB로 facebook 43만6천시간(128개 국어)이 활용되었고, fine-tuning DB로 한국아동 음성 137시간이 활용되었다. 이때, e2e 방식 STT 모델의 언어모델에 긴장 음화와 파열음화 연쇄를 적용하였다. 예를 들어, “바지”에 대한 발음 오류 유형으로 “바띠”, “빠티”, “빠디”, “빠띠”를 적용하였다. 한편, 핫워드에는 실제 임상에서 발견되는 발음 오류를 적용하였다. 이때, 실험 결과 최고 성능은 PER(Phoneme-Error-Rate) 7.74%이었다. 음성인식 모듈은 변환된 텍스트를 조음음운검사 평가 모듈로 전달한다. 이후, 조음음운검사 평가 모듈이 변환된 텍스트 및 레퍼런스 텍스트를 기반으로 음소 에러율 및 자음 에러 율을 산출하고, 산출된 음소 에러율 및 자음 에러율을 표시하는 단계가 진행된다(S140). 조음음운검사 평가 모듈은 레퍼런스 텍스트를 기준으로 상기 음성 정보에 대한 음소 에러율 및 자음 에러 율을 산출할 수 있다. 여기서, 조음음운검사 평가 모듈은 상기 변환된 텍스트 및 상기 레퍼런스 텍스트 각각의 자음 및 모음을 분리하여 풀어쓰기 데이터를 생성하고, 상기 레퍼런스 텍스트 및 상기 변환된 텍스트 각각에 대응되는 풀어쓰기 데이터를 기반으로 상기 음소 에러율 및 상기 자음 에러율을 산출할 수 있다. 프로세서는 상기 음소 에러율 및 상기 자음 에러율이 상기 사용자 단말에서 표시되도록, 상기 음소 에러율 및 상기 자음 에러율을 상기 사용자 단말로 전송할 수 있다. 이에 따라, 도 10과 같이, 조음운음검사 프로그램 실행화면에는 상기 음소 에러율 및 상기 자음 에러율이 표시될 수 있다. 한편, 프로세서는 상기 풀어쓰기 데이터가 상기 사용자 단말에서 표시되도록, 상기 풀어쓰기 데이터를 상 기 사용자 단말로 전송할 수 있다. 이에 따라, 도 10과 같이, 조음운음검사 프로그램 실행화면에는 상기 레퍼런 스 텍스트 및 상기 변환된 텍스트 각각에 대응되는 풀어쓰기 데이터(522 및 523)가 표시될 수 있다. 한편, 상기 음성 정보는 복수개 입력될 수 있다. 예를 들어, 도 7에 도시된, 사용자가 조음운음검사 프로그램 실행화면에 포함된 특정 메뉴를 선택하는 경 우, 도 11과 같이, 실행화면이 변경될 수 있다. 사용자는 변경된 실행화면을 통해, 복수의 음성 정보 및 복수의 레퍼런스 텍스트를 입력할 수 있게 된다. 일 실시 예에 있어서, 도 12 및 13과 같이, 사용자는 사용자 단말에 저장된 복수의 음성 파일을 선택하여 조음음운검사 평가에 활용할 수 있다. 일 실시 예에 있어서, 도 14와 같이, 사용자는 사용자 단말에 저장된 복수의 텍스트 파일 또는 복수의 텍스트가 입력된 하나의 텍스트 파일을 선택하여 조음운음검사 평가에 활용할 수 있다. 조음음운검사 평가 모듈은 상기 복수의 음성 정보 중 어느 하나의 음성 정보에 대응되는 레퍼런스 텍스트 를 기반으로, 상기 어느 하나의 음성 정보에 대한 음소 에러율 및 자음 에러율을 산출할 수 있다. 도 15와 같이, 사용자는 조음운음검사 프로그램 실행화면에 표시된 평가 결과 리스트 중 어느 하나(A)를 선택하 여 선택된 단어에 대한 음소 에러율 및 자음 에러율, 풀어쓰기 데이터를 확인할 수 있다. 조음음운검사 평가 모듈은 음성 정보 및 레퍼런스 텍스트가 복수개 입력되는 경우, 음성 정보에 대한 음성 인식 결과를 기반으로 복수의 음성 정보 각각에 대응되는 레퍼런스 텍스트를 매칭시킨 후, 매칭된 음성 정보 및 레퍼런스 텍스트를 기반으로 음소 에러율 및 자음 에러율을 산출할 수 있다. 한편, 조음음운검사 평가 모듈은 조음음운검사 평가 결과를 서로 다른 카테고리 별로 표시할 수 있다. 일 실시 예에 있어서, 하기 표 1을 참조하면, 조음음운검사 평가 결과는 오류유형(대분류), 오류유형(중분류), 오류유형(소분류), 발달적 오류 여부, 발음 오류, 발음 오류 개수 중 적어도 하나를 포함할 수 있다. 표 1 단어 오류유형_대 분류오류유형_중분 류오류유형_소분 류발달적오류 발음오류 발음오류개수 바지왜곡오류조음위치 원순 FALSE 바쥐,봐지,봐쥐 3 바지단어단위오 류패턴생략 음생 TRUE 바,지 2 바지단어단위오 류패턴생략 두초생 FALSE 아지 1 바지단어단위오 류패턴생략 중초생 FALSE 바이 1 바지단어단위오 류패턴반복 자조 TRUE 0 바지단어단위오 류패턴첨가 음첨 FALSE 0 바지단어단위오 류패턴첨가 모첨 FALSE 0 바지단어단위오 류패턴첨가 자첨 FALSE 0 바지단어단위오 류패턴반복 음반 TRUE 0 바지단어단위오 류패턴도치및이동음절도치 FALSE 지바 1 바지단어단위오 류패턴도치및이동음소도치 FALSE 자비 1 바지분절음변화조음위치 양순 FALSE 바비 1 바지분절음변화조음위치 치경구개 FALSE 다지,자지 2 바지분절음변화조음위치 연구개 FALSE 가기,바기,가지 3 바지분절음변화조음위치 성문 FALSE 하지,바히,하히 3 바지분절음변화조음방법 파열음 TRUE 바디,바띠,바티 3 바지분절음변화조음방법 마찰음 FALSE 바시,바씨 2 바지분절음변화조음방법 비음 FALSE 마니,바니,마지 3 바지분절음변화조음방법 자활 FALSE야지,바위,와지,바 이4 바지분절음변화발성유형 긴장 TRUE 빠지,바찌,빠찌 3 바지분절음변화발성유형 기식 FALSE 파지,바치,파치 3 바지단어단위오 류패턴반복 모조 FALSE 비지,바자 2 이를 통해, 사용자는 복수의 음성 파일을 활용하여 조음음운검사 평사를 수행하고자 하는 경우, 복수의 음성 파 일 각각에 레퍼런스 텍스트를 매칭 시킬 필요가 없게 되어 용이하게 검사를 진행할 수 있게 된다. 상술한 바와 같이, 본 개시에 따르면, 임상현장에서 비대면으로 가정에서 실행할 수 있기 때문에 조음음운 장애 를 조기에 진단하고 조기에 치료할 수 있게 된다. 한편, 개시된 실시예들은 컴퓨터에 의해 실행 가능한 명령어를 저장하는 기록매체의 형태로 구현될 수 있다. 명 령어는 프로그램 코드의 형태로 저장될 수 있으며, 프로세서에 의해 실행되었을 때, 프로그램 모듈을 생성하여 개시된 실시예들의 동작을 수행할 수 있다. 기록매체는 컴퓨터로 읽을 수 있는 기록매체로 구현될 수 있다. 컴퓨터가 읽을 수 있는 기록매체로는 컴퓨터에 의하여 해독될 수 있는 명령어가 저장된 모든 종류의 기록 매체 를 포함한다. 예를 들어, ROM(Read Only Memory), RAM(Random Access Memory), 자기 테이프, 자기 디스크, 플 래쉬 메모리, 광 데이터 저장장치 등이 있을 수 있다."}
{"patent_id": "10-2023-0015606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상에서와 같이 첨부된 도면을 참조하여 개시된 실시예들을 설명하였다. 본 개시가 속하는 기술분야에서 통상 의 지식을 가진 자는 본 개시의 기술적 사상이나 필수적인 특징을 변경하지 않고도, 개시된 실시예들과 다른 형 태로 본 개시가 실시될 수 있음을 이해할 것이다. 개시된 실시예들은 예시적인 것이며, 한정적으로 해석되어서 는 안 된다.도면 도면1 도면2 도면3 도면4 도면5 도면6a 도면6b 도면6c 도면6d 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15"}
{"patent_id": "10-2023-0015606", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 전반적 시스템 도면이다. 도 2는 본 개시의 조음음운장애 검사 장치에 포함된 서버의 블록도이다. 도 3은 본 개시의 조음음운장애 검사 장치에 포함된 단말기의 블록도이다. 도 4는 본 개시의 조음음운장애 검사 장치에 포함된 프로세서의 블록도이다. 도 5는 본 개시에 따른 조음음운장애 검사 방법의 흐름도이다. 도 6a 내지 6d는 본 개시에 따른 음성인식 모듈을 나타내는 블록도이다. 도 7 내지 15는 본 개시에 따른 조음음운검사 프로그램의 실행화면이다."}
