{"patent_id": "10-2022-7002917", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0025023", "출원번호": "10-2022-7002917", "발명의 명칭": "애니메이션 처리 방법 및 장치, 컴퓨터 저장 매체 그리고 전자 디바이스", "출원인": "텐센트 테크놀로지", "발명자": "장 룽"}}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 디바이스에 적용 가능한, 애니메이션 처리 방법으로서,현재 순간(moment)의 그래픽 사용자 인터페이스에서 지형 특징(terrain feature)을 획득하고, 상기 현재 순간의애니메이션 세그먼트에서 가상 캐릭터에 대응하는 상태 정보 및 태스크(task) 정보를 획득하는 단계;상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보를 애니메이션 처리 모델에 입력하고, 상기 애니메이션 처리 모델을 사용하여 상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보에 대해 특징 추출을 수행하여, 다음순간의 상기 가상 캐릭터에 대응하는 관절 액션(joint action) 정보를 획득하는 단계;상기 관절 액션 정보에 따라 관절 토크(torque)를 결정하는 단계; 및상기 관절 토크에 기반하여 상기 현재 순간의 상기 가상 캐릭터에 대응하는 제스처 조정 정보를 획득하고, 상기제스처 조정 정보에 따라 상기 애니메이션 세그먼트를 처리하는 단계를 포함하는 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 현재 순간이 상기 애니메이션 세그먼트의 초기 순간인 경우, 상기 애니메이션 세그먼트의 초기 순간에서상기 가상 캐릭터의 제스처 정보에 따라 상기 상태 정보를 결정하는 단계; 및상기 현재 순간이 상기 애니메이션 세그먼트의 초기 순간이 아닌 경우, 이전 시점에서 상기 가상 캐릭터에 대응하는 관절 액션 정보에 따라 상기 상태 정보를 결정하는 단계를 더 포함하는 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 애니메이션 세그먼트에 기반하여 복수의 순간에서 상기 가상 캐릭터에 대응하는 제스처 조정 정보를 획득하는 단계; 및상기 복수의 순간에서의 상기 제스처 조정 정보에 따라 타깃 액션 시퀀스를 결정하는 단계를 더 포함하는 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 지형 특징은 자체 정의(self-defined) 지형의 특징 또는 실제 지형의 특징이고;상기 상태 정보는 상기 가상 캐릭터의 각 관절의 제스처, 속도 및 위상(phase)을 포함하며; 그리고상기 태스크 정보는 상기 가상 캐릭터에 대응하는 타깃 속도 방향 또는 타깃 지점 좌표를 포함하는, 애니메이션처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 애니메이션 처리 모델은 제1 제어 네트워크 및 제2 제어 네트워크를 포함하고; 그리고상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보를 애니메이션 처리 모델에 입력하고, 상기 애니메이션 처공개특허 10-2022-0025023-2-리 모델을 사용하여 상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보에 대해 특징 추출을 수행하여, 다음순간의 상기 가상 캐릭터에 대응하는 관절 액션 정보를 획득하는 단계는,상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보를 상기 제1 제어 네트워크에 입력하고, 상기 제1 제어 네트워크를 사용하여 상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보에 대해 특징 추출을 수행하여, 주요(key) 관절에 대응하는 타깃 상태 정보를 획득하는 단계 - 상기 주요 관절은 상기 지형 특징 그리고 상기 가상캐릭터의 상태 정보 및 태스크 정보에 대응함 -상기 타깃 상태 정보를 타깃 태스크 정보로 결정하는 단계; 및상기 상태 정보 및 상기 타깃 태스크 정보를 상기 제2 제어 네트워크에 입력하고, 상기 제2 제어 네트워크를 사용하여 상기 상태 정보 및 상기 타깃 태스크 정보에 대해 특징 추출을 수행하여 상기 관절 액션 정보를 획득하는 단계를 포함하는, 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 제1 제어 네트워크는 컨볼루션 유닛, 제1 완전 연결 계층, 제2 완전 연결 계층 및 제3 완전 연결 계층을포함하고; 그리고상기 제1 제어 네트워크를 사용하여 상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보에 대해 특징 추출을수행하여, 주요 관절에 대응하는 타깃 상태 정보를 획득하는 단계는, 상기 컨볼루션 유닛을 사용하여 상기 지형 특징에 대해 특징 추출을 수행하여, 지형에 대응하는 제1 특징 정보를 획득하는 단계;상기 제1 완전 연결 계층을 사용하여 상기 제1 특징 정보에 대해 특징 조합(feature combination)을 수행하여제2 특징 정보를 획득하는 단계;상기 제2 완전 연결 계층을 사용하여 상기 제2 특징 정보, 상기 상태 정보 및 상기 태스크 정보에 대해 특징 조합을 수행하여 제3 특징 정보를 획득하는 단계; 및상기 제3 완전 연결 계층을 사용하여 상기 제3 특징 정보에 대해 특징 조합을 수행하여 상기 타깃 상태 정보를획득하는 단계를 포함하는, 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서, 상기 제2 제어 네트워크는 제4 완전 연결 계층 및 제5 완전 연결 계층을 포함하고; 그리고상기 제2 제어 네트워크를 사용하여 상기 상태 정보 및 상기 타깃 태스크 정보에 대해 특징 추출을 수행하여 상기 관절 액션 정보를 획득하는 단계는, 상기 제4 완전 연결 계층을 사용하여 상기 상태 정보 및 상기 타깃 태스크 정보에 대해 특징 조합을 수행하여제4 특징 정보를 획득하는 단계; 및상기 제5 완전 연결 계층을 사용하여 상기 제4 특징 정보에 대해 특징 조합을 수행하여 상기 관절 액션 정보를획득하는 단계를 포함하는, 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 관절 액션 정보에 따라 관절 토크를 결정하는 단계는,상기 관절 액션 정보에 따라 관절의 현재 위치 및 타깃 위치를 결정하는 단계;공개특허 10-2022-0025023-3-상기 현재 위치에 따라 상기 관절의 현재 속도 및 현재 가속도를 결정하고, 상기 타깃 위치에 따라 상기 관절의타깃 속도를 결정하는 단계;상기 현재 속도 및 상기 현재 가속도에 따라 다음 제어 주기(next control period) 이후 상기 관절에 대응하는제1 위치 및 제1 속도를 결정하는 단계; 및비례 계수, 미분 이득 계수, 상기 현재 위치, 상기 타깃 위치, 상기 타깃 속도, 상기 제1 위치 및 상기 제1 속도에 따라 상기 관절 토크를 계산하는 단계를 포함하는, 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서, 상기 관절 토크에 기반하여 상기 현재 순간의 상기 가상 캐릭터에 대응하는 제스처 조정 정보를 획득하는 것은, 상기 관절 토크를 물리 엔진에 입력하고, 상기 물리 엔진을 사용하여 상기 관절 토크를 대응하는 관절에 인가하며, 렌더링을 수행하여 상기 제스처 조정 정보를 생성하는 것을 포함하는, 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제5항에 있어서, 상기 애니메이션 처리 모델을 사용하여 상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보에 대해 특징 추출을 수행하는 것 이전에, 상기 애니메이션 처리 방법은,훈련 대상(to-be-trained) 애니메이션 처리 모델을 훈련하여(training) 상기 애니메이션 처리 모델을 획득하는단계를 더 포함하는 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 훈련 대상 애니메이션 처리 모델은 훈련될 제1 제어 네트워크 및 훈련될 제2 제어 네트워크를 포함하고;그리고상기 훈련 대상 애니메이션 처리 모델을 훈련하여 상기 애니메이션 처리 모델을 획득하는 단계는, 지형 특징 샘플, 캐릭터 상태 샘플 및 태스크 정보 샘플을 획득하는 단계;상기 지형 특징 샘플, 상기 캐릭터 상태 샘플 및 상기 태스크 정보 샘플에 따라 상기 훈련될 제1 제어 네트워크를 훈련하여 상기 제1 제어 네트워크를 획득하는 단계; 및상기 가상 캐릭터의 주요 관절에 대응하는 상태 정보 샘플 및 상기 애니메이션 세그먼트 샘플의 모든 관절에 대응하는 관절 액션 정보 샘플에 따라 상기 제2 제어 네트워크를 훈련하여, 상기 제2 제어 네트워크를 획득하는단계를 포함하고,상기 훈련될 제1 제어 네트워크와 상기 훈련될 제2 제어 네트워크는 별도로 훈련되고; 상기 훈련될 제1 제어 네트워크가 훈련되는 경우, 상기 훈련될 제1 제어 네트워크는 고정된 파라미터로 상기 제2 제어 네트워크에 연결되는, 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 가상 캐릭터의 주요 관절에 대응하는 상태 정보 샘플 및 상기 애니메이션 세그먼트 샘플의 모든 관절에 대공개특허 10-2022-0025023-4-응하는 관절 액션 정보 샘플에 따라 상기 제2 제어 네트워크를 훈련하여, 상기 제2 제어 네트워크를 획득하는단계는, 복수의 애니메이션 세그먼트 샘플을 획득하는 단계;상기 가상 캐릭터의 초기 제스처에 따라 상기 복수의 애니메이션 세그먼트 샘플로부터 타깃 애니메이션 세그먼트 샘플을 결정하는 단계;상기 타깃 애니메이션 세그먼트 샘플에서 상기 주요 관절에 대응하는 상태 정보 샘플을 획득하고, 상기 상태 정보 샘플을 타깃 태스크 정보로 사용하는 단계;상기 가상 캐릭터의 모든 관절에 대응하는 상기 관절 액션 정보 샘플을 획득하는 단계; 및상기 타깃 태스크 정보 및 상기 관절 액션 정보 샘플에 따라 상기 훈련될 제2 제어 네트워크를 훈련시키는 단계를 포함하는, 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서, 상기 훈련될 제1 제어 네트워크는 제1 훈련 대상 액터(actor) 서브 네트워크 및 제1 훈련 대상 크리틱(critic)서브 네트워크를 포함하고; 그리고상기 훈련될 제2 제어 네트워크는 제2 훈련 대상 액터 서브 네트워크 및 제2 훈련 대상 크리틱 서브 네트워크를포함하며, 상기 제1 훈련 대상 액터 서브 네트워크의 구조는 상기 제1 훈련 대상 크리틱 서브 네트워크와 동일하고, 상기제2 훈련 대상 액터 서브 네트워크의 구조는 상기 제2 훈련 대상 크리틱 서브 네트워크의 구조와 동일한, 애니메이션 처리 방법."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "애니메이션 처리 장치로서,현재 순간의 그래픽 사용자 인터페이스에서 지형 특징을 획득하고 상기 현재 순간의 애니메이션 세그먼트에서가상 캐릭터에 대응하는 상태 정보 및 태스크 정보를 획득하도록 구성된 정보 획득 모듈;상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보를 애니메이션 처리 모델에 입력하고, 상기 애니메이션 처리 모델을 사용하여 상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보에 대해 특징 추출을 수행하여, 다음순간의 상기 가상 캐릭터에 대응하는 관절 액션 정보를 획득하도록 구성된 모델 처리 모듈; 및상기 관절 액션 정보에 따라 관절 토크를 결정하고, 상기 관절 토크에 기반하여 상기 현재 순간의 상기 가상 캐릭터에 대응하는 제스처 조정 정보를 획득하며, 상기 제스처 조정 정보에 따라 상기 애니메이션 세그먼트를 처리하도록 구성된 제스처 조정 모듈을 포함하는 애니메이션 처리 장치."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "전자 디바이스로서,하나 이상의 프로세서; 및하나 이상의 프로그램을 저장하도록 구성된 저장 장치를 포함하며, 상기 하나 이상의 프로그램은 상기 하나 이상의 프로세서에 의해 실행될 때, 상기 하나 이상의 프로세서가 제1항 내지 제13항 중 어느 한 항에 따른 애니메이션 처리 방법을 구현하게 하는, 전자 디바이스."}
{"patent_id": "10-2022-7002917", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "컴퓨터 프로그램을 저장하는, 컴퓨터가 판독 가능한 저장 매체로서,공개특허 10-2022-0025023-5-상기 컴퓨터 프로그램은 프로세서에 의해 실행될 때, 제1항 내지 제13항 중 어느 한 항에 따른 애니메이션 처리방법을 구현하는, 컴퓨터가 판독 가능한 저장 매체."}
{"patent_id": "10-2022-7002917", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 애니메이션 처리 방법 및 장치를 제공하며, 인공 지능 분야에 관한 것이다. 이 방법은, 현재 순간의 그래픽 사용자 인터페이스에서 지형 특징을 획득하고, 현재 순간의 애니메이션 세그먼트에서 가상 캐릭터에 대응 하는 상태 정보 및 태스크 정보를 획득하는 단계; 지형 특징, 상태 정보 및 태스크 정보를 애니메이션 처리 모델 에 입력하고, 애니메이션 처리 모델을 사용하여 지형 특징, 상태 정보 및 태스크 정보에 대해 특징 추출을 수행 하여 다음 순간의 가상 캐릭터에 대응하는 관절 액션 정보를 획득하는 단계; 및 관절 액션 정보에 따라 관절 토 크를 결정하고, 관절 토크에 기반하여 렌더링을 수행하여 현재 순간의 가상 캐릭터에 대응하는 제스처 조정 정보 를 획득하고, 제스처 조정 정보에 따라 애니메이션 세그먼트를 처리하는 단계를 포함한다."}
{"patent_id": "10-2022-7002917", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 2020년 1월 15일에 출원된 중국 특허 출원 번호 제202010043321.5호에 대한 우선권을 주장하는 바이 며, 상기 문헌의 내용은 그 전체로서 원용에 의해 포함된다. 본 개시는 인공 지능(artificial intelligence) 기술 분야에 관한 것으로, 특히 애니메이션(animation) 처리 방법, 애니메이션 처리 장치, 컴퓨터 저장 매체 및 전자 디바이스에 관한 것이다."}
{"patent_id": "10-2022-7002917", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 지능의 지속적인 발전과 함께 인공 지능 기술은 의료 분야, 금융 분야, 그래픽 디자인 분야와 같은 점점 더 많은 분야에 적용되기 시작하고 있다. 예를 들어, 게임 디자인은 원래의 2D 게임 디자인에서 현재의 3D 게임 디자인으로 점진적으로 발전한다. 현재, 게임 제작에서, 일반적으로 애니메이터(animator)에 의해 복수의 애니메이션 세그먼트(segment)가 설계되 고, 게임 엔진을 사용하여 복수의 애니메이션 세그먼트가 혼합 및 전환되어 최종적으로 게임에서의 효과 (effect)가 달성된다. 애니메이션은 캐릭터 행동의 표현이며, 완전한 애니메이션 세그먼트는 일정 기간에서의 캐릭터 객체의 액션(action)이 기록되고 재생되는 것이다. 그러나, 애니메이터가 제작한 애니메이션은 물리적 엔진(physical engine)에서 실시간으로 렌더링되는 애니메이션보다 재생 효과가 덜 자연스럽고 덜 생생하며, 플 레이어와 상호 작용할 수 없으며, 예를 들어 가변 타깃 태스크(task)를 구현할 수 없으며, 역동적인 지형 (terrain)에 적응할 수 없다. 전술한 배경에서 개시되는 정보는 단지 본 개시의 배경에 대한 이해를 강화하기 위한 것이며, 따라서 당업자에 게 공지된 관련 기술을 구성하지 않는 정보를 포함할 수 있다. 본 개시의 실시예는 애니메이션 처리 방법, 애니메이션 처리 장치, 컴퓨터 저장 매체 및 전자 디바이스를 제공 한다. 본 개시의 실시예는 전자 디바이스에 적용 가능한 애니메이션 처리 방법을 제공한다. 상기 애니메이션 처리 방 법은, 현재 순간(moment)의 그래픽 사용자 인터페이스에서 지형 특징(terrain feature)을 획득하고, 상기 현재 순간의 애니메이션 세그먼트에서 가상 캐릭터에 대응하는 상태 정보 및 태스크(task) 정보를 획득하는 단계; 상 기 지형 특징, 상기 상태 정보 및 상기 태스크 정보를 애니메이션 처리 모델에 입력하고, 상기 애니메이션 처리 모델을 사용하여 상기 지형 특징, 상기 상태 정보 및 상기 태스크 정보에 대해 특징 추출을 수행하여, 다음 순 간의 상기 가상 캐릭터에 대응하는 관절 액션(joint action) 정보를 획득하는 단계; 상기 관절 액션 정보에 따 라 관절 토크(torque)를 결정하는 단계; 및 상기 관절 토크에 기반하여 상기 현재 순간의 상기 가상 캐릭터에 대응하는 제스처 조정 정보를 획득하고, 상기 제스처 조정 정보에 따라 상기 애니메이션 세그먼트를 처리하는 단계를 포함한다. 본 개시의 실시예는 애니메이션 처리 장치를 제공하며, 상기 애니메이션 처리 장치는, 현재 순간의 그래픽 사용 자 인터페이스에서 지형 특징을 획득하고 상기 현재 순간의 애니메이션 세그먼트에서 가상 캐릭터에 대응하는 상태 정보 및 태스크 정보를 획득하도록 구성된 정보 획득 모듈; 상기 지형 특징, 상기 상태 정보 및 상기 태스 크 정보를 애니메이션 처리 모델에 입력하고, 상기 애니메이션 처리 모델을 사용하여 상기 지형 특징, 상기 상 태 정보 및 상기 태스크 정보에 대해 특징 추출을 수행하여, 다음 순간의 상기 가상 캐릭터에 대응하는 관절 액 션 정보를 획득하도록 구성된 모델 처리 모듈; 및 상기 관절 액션 정보에 따라 관절 토크를 결정하고, 상기 관 절 토크에 기반하여 상기 현재 순간의 상기 가상 캐릭터에 대응하는 제스처 조정 정보를 획득하며, 상기 제스처 조정 정보에 따라 상기 애니메이션 세그먼트를 처리하도록 구성된 제스처 조정 모듈을 포함한다. 본 개시의 실시 예는 컴퓨터 프로그램을 저장하는, 컴퓨터가 판독 가능한 저장 매체를 제공하며, 상기 컴퓨터 프로그램은 프로세서에 의해 실행될 때, 전술한 실시 예에 따른 애니메이션 처리 방법을 구현한다. 본 개시의 실시 예는 전자 디바이스를 제공하며, 상기 전자 디바이스는 하나 이상의 프로세서; 및 하나 이상의 프로그램을 저장하도록 구성된 저장 장치를 포함하며, 상기 하나 이상의 프로그램은 상기 하나 이상의 프로세서 에 의해 실행될 때, 상기 하나 이상의 프로세서가 전술한 실시 예에 따른 애니메이션 처리 방법을 구현하게 한 다. 본 개시의 실시예에서 제공되는 기술 솔루션에서, 현재 순간의 그래픽 사용자 인터페이스에서 지형 특징 그리고 현재 순간의 애니메이션 세그먼트에서 가상 캐릭터에 대응하는 상태 정보 및 태스크 정보가 먼저 획득되며; 그 런 다음, 애니메이션 처리 모델을 사용하여 지형 특징, 상태 정보 및 태스크 정보에 대해 특징 추출을 수행하여 다음 순간의 가상 캐릭터에 대응하는 관절 액션 정보를 획득하고; 마지막으로 관절 액션 정보에 따라 관절 토크 가 결정되며, 관절 토크에 기반하여 현재 순간의 가상 캐릭터에 대응하는 제스처 조정 정보가 획득되고, 제스처 조정 정보에 따라 애니메이션 세그먼트가 처리된다. 이러한 방식으로 애니메이션 세그먼트를 시뮬레이션할 수 있을 뿐만 아니라 상이한 지형 특징 및 태스크 정보에 따라 가상 캐릭터의 액션 및 제스처를 조정할 수 있다. 한편으로, 애니메이션의 충실도(fidelity)가 향상되며; 한편, 사용자와 가상 캐릭터 간의 상호 작용이 구현되고, 가상 캐릭터의 자기 적응성(self-adaptivity)이 향상된다. 위의 일반적인 설명 및 다음의 상세한 설명은 단지 예시 및 설명을 위한 것이며, 본 개시를 제한할 수 없음을 이해해야 한다."}
{"patent_id": "10-2022-7002917", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "예시적인 구현이 이제 첨부 도면을 참조하여 보다 철저하게 설명될 것이다. 그러나, 본 개시는 다양한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되는 것은 아니다. 대신, 본 개시를 보다 철저하고 완전하게 하고 예시적인 구현의 아이디어를 당업자에게 완전히 전달하기 위한 구현이 제공된다. 또한, 설명된 특징, 구조 또는 특성은 임의의 적절한 방식으로 하나 이상의 실시예에서 조합될 수 있다. 다음 설명에서, 본 개시의 실시예의 완전한 이해를 얻기 위해 많은 세부사항이 제공된다. 그러나, 당업자는 본 개시 의 기술적 솔루션이 하나 이상의 특정 세부사항 없이 구현될 수 있거나, 또는 다른 방법, 유닛, 장치 또는 단계 가 사용될 수 있음을 인지해야 한다. 다른 경우에, 잘 알려진 방법, 장치, 구현 또는 작동(operation)이 본 개 시의 측면을 모호하게 하지 않기 위해 상세하게 도시되거나 설명되지 않는다. 첨부된 도면에 도시된 블록도는 단지 기능적 엔티티일 뿐이며 물리적으로 독립된 엔티티에 반드시 대응하는 것 은 아니다. 즉, 기능적 엔티티는 소프트웨어 형태로, 또는 하나 이상의 하드웨어 모듈 또는 집적 회로로, 또는 상이한 네트워크 및/또는 프로세서 장치 및/또는 마이크로 컨트롤러 장치로 구현될 수 있다. 첨부된 도면에 도시된 흐름도는 단지 예시적인 설명일 뿐이며, 모든 내용 및 작동/단계를 포함할 필요가 없고, 설명된 순서대로 수행될 필요도 없다. 예를 들어, 일부 작동/단계는 더 분할될 수 있는 반면 일부 작동/단계는 조합되거나 부분적으로 조합될 수 있다. 따라서, 실제 실행 순서는 실제 사례에 따라 변경될 수 있다. 도 1은 본 개시의 실시예의 기술적 솔루션이 적용될 수 있는 예시적인 시스템 아키텍처의 개략도이다. 도 1에 도시된 바와 같이, 시스템 아키텍처는 단말 디바이스, 네트워크 및 서버를 포함할 수 있다. 네트워크는 단말 디바이스와 서버 사이의 통신 링크를 제공하기 위해 사용되는 매체이 다. 네트워크는 다양한 연결 유형, 예를 들어, 유선 통신 링크, 무선 통신 링크 등을 포함할 수 있다. 도 1의 단말 디바이스의 수, 네트워크의 수 및 서버의 수는 예시일 뿐임을 이해해야 한다. 실제 요건에 따라 임 의의 수의 단말 디바이스, 임의의 수의 네트워크 및 임의의 수의 서버가 있을 수 있다. 예를 들어, 서버는 복수의 서버를 포함하는 서버 클러스터일 수 있다. 단말 디바이스는 노트북, 휴대용 컴퓨터 또는 데스크탑 컴퓨터와 같이 디스플레이 화면을 포함하는 단말 디바이스일 수 있다. 본 개시의 실시예에서, 게임 애플리케이션은 단말 디바이스에서 운반되며, 게임 애플리케이션은 애니메이 션 세그먼트(animation segment)를 포함한다. 게임 애플리케이션을 실행하는 동안, 게임 애플리케이션의 관련 컨트롤을 사용하여 가상 캐릭터에 장애물이 설정될 수 있으며; 또는 단말 디바이스의 촬영 유닛을 사용하 여 실제 장면을 촬영하고, 실제 장면을 게임 화면에 통합하여 가상 캐릭터에 대한 장애물을 설정할 수 있다. 한 편, 사용자는 애니메이션 세그먼트의 장면에 따라 가상 캐릭터에 대한 태스크(task)를 설정할 수 있으며, 예를 들어 가상 캐릭터가 타깃 방향 또는 타깃 지점(point)을 향해 이동하도록 할 수 있다. 단말 디바이스는 네 트워크를 사용하여, 현재 순간의 그래픽 사용자 인터페이스에서 지형 특징(terrain feature), 그리고 현재 순간의 애니메이션 세그먼트에서 가상 캐릭터에 대응하는 태스크 정보 및 상태 정보를 서버로 전송할 수 있고, 지형 특징, 태스크 정보 및 상태 정보는 서버를 사용하여 처리되어 현재 순간의 가상 캐릭터에 대응 하는 제스처 조정(gesture adjustment) 정보를 획득한다. 이러한 방식으로 애니메이션 세그먼트를 시물레이션할 수 있을 뿐만 아니라 가상 캐릭터도 자기 적응성을 가질 수 있으며, 설정된 태스크를 완료할 수 있다. 일부 실시예에서, 애니메이션 처리 모델을 사용하여 지형 특징, 상태 정보 및 태스크 정보에 대해 특징 추출이 수행되어 다음 순간의 가상 캐릭터에 대응하는 관절 액션(joint action) 정보를 획득할 수 있고; 관절 액션 정 보에 기반하여 관절 토크(joint torque)가 결정될 수 있으며, 관절 토크는 물리적 엔진을 사용하여 대응하는 관 절에 인가되어, 렌더링(rendering)을 수행하여 현재 순간의 가상 캐릭터에 대응하는 제스처 조정 정보를 획득할 수 있다. 가상 캐릭터에 대응하는 상태 정보는 애니메이션 세그먼트의 초기 순간에서 가상 캐릭터에 대응하는 제스처 정보이거나, 또는 이전 순간의 관절 액션 정보에 기반하여 결정된 상태 정보일 수 있다. 애니메이션 세 그먼트는 일정 지속 기간(duration)을 가지며, 복수의 순간에서 가상 캐릭터에 대응하는 제스처 조정 정보는 전술한 작동을 반복함으로써 획득될 수 있으며, 타깃 액션 시퀀스(target action sequence)는 복수의 순간에서의 제스처 조정 정보에 따라 결정될 수 있고, 타깃 액션 시퀀스는 애니메이션 세그먼트를 형성할 수 있으며, 애니 메이션 세그먼트는 실행 중인 게임의 애니메이션 세그먼트와 유사하고 높은 충실도를 가지며, 차이점은 애니메 이션 세그먼트의 가상 캐릭터가 사용자가 설정한 지형에 맞게 자체 적응할 수 있으며 사용자가 설정한 태스크를 완료할 수 있다는 것이며, 즉 본 개시의 실시예의 기술적 솔루션은 사용자와 가상 캐릭터 간의 상호 작용을 개 선하고. 가상 캐릭터의 자기 적응성을 개선하여 사용자 경험을 더욱 향상시킬 수 있다. 본 개시의 실시예에서 제공되는 애니메이션 처리 방법은 서버에서 수행될 수 있고, 이에 대응하여 애니메이션 처리 장치는 서버에 배치될 수 있다. 그러나, 본 개시의 다른 실시예에서, 본 개시의 실시예에서 제공되는 애니 메이션 처리 방법은 다르게는 단말 디바이스에 의해 수행될 수 있다. 서버는 독립된 물리적 서버일 수도 있고, 또는 복수의 물리적 서버 또는 분산된 시스템을 포함하는 서버 클러스터일 수도 있으며, 또는 클라우드 서비스, 클라우드 데이터베이스, 클라우드 컴퓨팅, 클라우드 기능, 클 라우드 스토리지, 네트워크 서비스, 클라우드 통신, 미들웨어 서비스, 도메인 네임 서비스, 보안 서비스, 콘텐 츠 전송 네트워크(content delivery network, CDN), 빅데이터 및 인공 지능 플랫폼과 같은 기본적인 클라우드 컴퓨팅 서비스를 제공하는 클라우드 서버일 수도 있다. 이 분야의 관련 기술에서, 3D 게임을 예로 들 수 있으며, 3D 게임에서 캐릭터 애니메이션은 일반적으로 스킨된 애니메이션(skinned animation)을 의미한다. 도 2는 스킨된 애니메이션에서 가상 캐릭터의 구성 구조 (composition structure)를 도시한다. 도 2에 도시된 바와 같이, 스킨된 애니메이션의 가상 캐릭터는 뼈(bone), 스킨(skin), 애니메이션을 포함하며, 뼈는 관절로 구성된 가동 골격이면서 또한 움직일 수 있는 가상 주체이며, 전체 캐릭터를 움직이게 하지만 게임에서 렌더링되지 않으며; 스킨은 뼈를 감싸는 삼각형 메시(triangular mesh)이며, 메시의 각 정점은 하나 이상의 뼈에 의해 제어되고, 애니메이션은 특정 시점(time point)에서 각 뼈 의 위치나 방향이 변하는 것이며, 3차원 공간은 일반적으로 행렬로 표현된다. 일반적으로 애니메이터는 3D 애니 메이션 제작 소프트웨어를 사용하여 대량의 애니메이션 세그먼트를 미리 디자인하여 제작하고, 게임을 플레이하 는 동안 프로그램은 장면에 필요한 애니메이션 세그먼트를 적절한 시간에 재생한다. 특히 필요한 경우, 렌더링 전에 프로그램에서 애니메이션 후처리를 수행하며, 예를 들어 가상 캐릭터의 손과 발의 정확한 위치는 액션을 조정하기 위해 역운동학(inverse kinematics, IK) 방법을 사용하여 그 시간의 실제 환경에 따라 계산된다. 그러 나, 후처리의 효과는 제한적이며, 일반적으로 애니메이션의 품질은 거의 전적으로 애니메이터의 기술에 달려 있 다. 애니메이터가 직접 애니메이션을 제작한다는 것은 실제로 게임에서 직접 애니메이션을 재생하는 것으로, 실 제 세계의 물리 법칙을 물리적 엔진으로 시뮬레이션하지 않아서 캐릭터의 액션이 충분히 자연스럽지 않거나 생 생하지 않다. 현재 업계에는 물리적 애니메이션 AI를 훈련하기(train) 위한 몇 가지 머신 학습 솔루션이 있다. 그러나, 학습 효과는 일반적으로 좋지 않으며 하나의 모델은 단일 성능을 갖는 하나의 액션만 학습할 수 있다. 또한, 현대 게임 제작에서 애니메이션을 구현하는 주요 방법은 애니메이터가 제작한 애니메이션 세그먼트를 재 생하는 것으로, 실질적으로 미리 정의된 열거 가능한 장면에 적용될 수 있을 뿐이며, 환경에 대한 자체 적응 능 력이 없다. 캐릭터의 환경에 대한 자기 적응성(self-adaptivity)은 미지의(unknown) 환경에서 캐릭터 애니메이 션이 환경에 매칭하는 제스처를 제시할(present) 수 있다는 것을 의미한다. 여기서 '미지(unknown)'는 애니메이 션의 사전 제작 프로세스 동안 가정되는 환경과 관련이 있으며; 실제 환경은 애니메이션 세그먼트를 사용하는 동안 크거나 작은 변화를 가진다. 또한, 외부 간섭에 의해 충돌이 인지되어, 현실감이 강한 장면을 가지는 액션 의 편차와 수정을 제시할 수 있다. 적어도 IK 기술은 환경에 대한 자기 적응성을 실현하고 캐릭터의 사지 (extremities)가 위치 측면에서 환경 또는 타깃과 정렬될 수 있도록 해야 하고; 그리고 캐릭터의 액션의 적절한 속도와 부드러운 전환 프로세스를 계산하기 위해 환경에 대한 캐릭터의 피드백이 생생해야 하면, \"물리적\"(즉, 강체 동역학(rigid body dynamic)의 시뮬레이션)이 추가로 도입될 필요가 있다. 일반적으로 지형은 고정되어 있 고, 지형을 이동하는 캐릭터의 액션 프로세스는 애니메이션으로 제작되고, 부자연스러운 부분(unnatural part) 에 대한 적절한 수정이 이루어진다. 이 절차는 본질적으로 여전히 애니메이션을 재생하기 위한 것이며, 지형에 서 캐릭터의 이동((movement)이 부자연스럽다. 본 개시의 실시예는 관련 기술의 기존 문제점을 감안하여 애니메이션 처리 방법을 제공한다. 이 방법은 인공 지 능(artificial intelligence, AI)을 기반으로 구현된다. 인공 지능은 디지털 컴퓨터 또는 디지털 컴퓨터로 제어 되는 머신을 사용하여 인간의 지능을 시뮬레이션, 확대(extend) 및 확장(expand)하고, 환경을 인식하며, 지식을 획득하고, 지식을 사용하여 최적의 결과를 획득하는 이론, 방법, 기술 및 애플리케이션 시스템이다. 달리 말하 면, AI는 지능의 본질을 이해하고 인간의 지능과 유사하게 반응할 수 있는 새로운 유형의 지능 머신을 생산하려 는 컴퓨터 과학의 종합 기술이다. AI는 다양한 지능형 머신의 설계 원리와 구현 방법을 연구하여 머신이 인식,추론 및 의사 결정 기능을 갖도록 하는 것이다. AI 기술은 하드웨어 수준 기술과 소프트웨어 수준 기술을 모두 포함하는 광범위한 분야를 포괄하는 포괄적인 학 문이다. 기본 AI 기술에는 일반적으로 센서, 전용 AI 칩, 클라우드 컴퓨팅, 분산 스토리지, 빅 데이터 처리 기 술, 운영/상호작용 시스템, 메카트로닉스 등의 기술이 포함된다. AI 소프트웨어 기술은 주로 컴퓨터 비전 기술, 음성 처리 기술, 자연어 처리 기술, 머신 러닝/딥 러닝 등을 포함한다. 컴퓨터 비전(computer vision. CV)은 머신이 \"볼 수 있도록\" 하는 방법이며, 구체적으로 컴퓨터가 타깃을 사람 의 눈이 관찰하기에 더 적합하거나 검출용 기기로 전송되기에 더 적합한 이미지로 처리하도록, 사람의 눈을 대 신하는 카메라 및 컴퓨터를 사용하여 타깃에 대한 인식, 추적(tracking), 측정 등과 같은 머신 비전을 구현하고 추가로 그래픽 처리를 수행하는 방법을 연구하는 과학이다. CV는 과학 과목으로 관련 이론과 기술을 연구하고, 이미지나 다차원 데이터에서 정보를 획득할 수 있는 AI 시스템 구축을 시도한다. CV 기술에는 일반적으로 이미 지 처리, 이미지 인식, 이미지 의미론적 이해, 이미지 검색(retrieval), 광학 문자 인식(optical character recognition, OCR), 비디오 처리, 비디오 의미론적 이해, 비디오 콘텐츠/행동 인식, 3D 객체 재구성, 3D 기술, 가상 현실, 증강 현실, 동기 포지셔닝(synchronous positioning) 및 지도 구성이 포함되며, 일반 얼굴 인식 (common face recognition) 및 지문 인식과 같은 생체 특징 인식 기술이 더 포함된다. ML은 다방면의 학문(multi-field interdiscipline)이며, 확률 이론, 통계학, 근사 이론, 볼록 공간 분석 (convex analysis) 및 알고리즘 복잡도 이론과 같은 복수의 학문 분야에 관한 것이다. ML은 새로운 지식이나 기 술을 습득하고 기존 지식 구조를 재구성하여 컴퓨터 성능을 지속적으로 개선하기 위해 컴퓨터가 인간의 학습 액 션을 시뮬레이션하거나 구현하는 방법을 연구하는 것을 전문으로 한다. ML은 AI의 핵심이며, 컴퓨터를 지능화하 는 기본 방법으로 AI의 다양한 분야에 적용된다. ML과 딥 러닝에는 일반적으로 인공 신경망, 신뢰성 있는 네트 워크(belief network), 강화 학습(reinforcement learning), 전이 학습, 귀납적 학습 및 데모 학습(learning from demonstrations)과 같은 기술이 포함된다. 인공지능 기술의 연구와 발전으로 인공지능 기술은 일반 스마트 홈, 스마트 웨어러블 디바이스, 가상 비서, 스 마트 스피커, 스마트 마케팅, 무인 운전, 자율 주행, 무인 항공기, 로봇, 스마트 의료, 스마트 고객 서비스 등 다양한 분야에서 연구 및 적용되고 있다. 기술의 발전과 함께 AI 기술은 더 많은 분야에 적용될 것이며 점점 더 중요한 역할을 할 것으로 믿어진다. 본 개시의 실시예에서 제공하는 솔루션은 AI의 영상 처리 기술을 포함하며, 이하의 실시예를 사용하여 설명한다. 본 개시의 실시예는 먼저 애니메이션 처리 방법을 제공한다. 도 3은 본 개시의 실시예에 따른 애니메이션 처리 방법의 흐름도를 개략적으로 도시한 것이다. 애니메이션 처리 방법은 서버에서 수행될 수 있으며, 서버는 도 1 에 도시된 서버일 수 있다. 게임 애니메이션의 처리를 예로 사용한다. 도 3을 참조하면, 애니메이션 처리 방법은 적어도 단계(S310) 내지 단계(S330)를 포함한다. 단계(S310)에서: 현재 순간의 그래픽 사용자 인터페이스에서 지형 특징이 획득되고, 현재 순간의 애니메이션 세 그먼트에서 가상 캐릭터에 대응하는 상태 정보 및 태스크 정보가 획득된다. 본 개시의 실시예에서, 게임의 재미를 향상시키고 게임에서 사용자와 가상 캐릭터 간의 상호작용을 강화하기 위 해, 게임 동안 사용자는 가상 캐릭터에 대한 장애물(obstacle)을 인위적으로 설정하여, 그래픽 사용자 인터페이 스에서 새로운 지형을 설정할 수 있으며, 예를 들어 가상 캐릭터가 원래 애니메이션 세그먼트에서 평평한 도로 를 따라 똑바로 걷는 경우, 사용자는 가상 캐릭터의 이동 경로(moving path)에 로드블록(roadblock)을 설정할 수 있으며, 로드블록은 돌, 스텝(step) 또는 구덩이(pit)와 같은 장애물일 수 있고; 또는, 사용자는 하늘에 장 애물 예를 들어 가상 캐릭터의 이동 경로에 처마(eave)나 날아다니는 새와 같은 장애물을 설정할 수 있으며, 가 상 캐릭터는 이러한 장애물을 피해야 계속 앞으로 나아갈 수 있다. 본 개시의 실시예의 기술적 솔루션을 명확하 게 하기 위하여, 이하의 설명에서는 로드블록을 예로 들어 설명하며, 로드블록은 지면에서의 노치(notch), 돌기 (protuberance), 또는 스텝 등의 장애물일 수 있다. 본 개시의 실시예에서, 사용자가 설정한 로드블록은 게임 내부에 배치된 컨트롤을 사용하여 설정되거나, 실제 장면에 따라 설정될 수 있다. 일부 실시예에서, 로드블록 설정 버튼은 게임 인터랙션 인터페이스에서 설정될 수 있다. 사용자가 로드블록 설정 버튼을 트리거하는 경우, 리스트가 팝업될 수 있으며, 사용자는 리스트로부터 가 상 캐릭터에 대해 설정하고자 하는 로드블록을 선택한다. 사용자의 경정 이후에, 대응하는 로드블록이 게임 인 터페이스에 나타난다. 증강 현실 게임에서, 실제 장면은 사용자가 사용하는 단말 디바이스에 구비된 촬영 유닛을 사용하여 촬영되며, 게임 엔진은 실제 장면과 게임 장면을 통합할 수 있다. 도 4는 게임 장면과 실제 장면이 통합된 후 획득되는 장면의 개략도를 도시한다. 도 4에 도시된 바와 같이, 게임 장면에 악령(demon spirit)(V) 이 있고, 실제 장면에 복수의 스텝(step)(S)이 있으며, 최상단 스텝의 플랫폼에 복수의 전동 스쿠터(electric scooter)(M)이 배치되어 있다. 악령(V)은 게임 장면과 실제 장면을 통합하여 실제 장면의 스텝(S)에 배치될 수 있다. 본 개시의 실시예에서, 게임에서 장애물을 설정하는 것은 일반적으로 로드블록을 설정하는 것이며, 상대적으로 많은 수의 장애물을 사용하여 생성되는 지형은 조밀한 노치 지형(densely-notched terrain) 및 하이브리드 장애 물 지형(hybrid-obstacle terrain)을 포함한다. 도 5는 조밀한 노치 지형의 인터페이스의 개략도를 도시한다. 도 5에 도시된 바와 같이, 조밀한 노치 지형은 지면(G)에 연속된 복수의 노치가 존재하고, 노치의 너비가 상이 하며, 두 노치 사이에 일정한 간격(interval)이 있도록 설계된다. 도 6은 하이브리드 장애물 지형의 인터페이스 의 개략도를 도시한다. 도 6에 도시된 바와 같이, 하이브리드 장애물 지형은 일정한 길이의 지면(G)에 노치(C), 스텝(step)(D), 돌기(E)와 같은 장애물을 포함한다. 장애물의 높이와 너비가 상이하며, 두 장애물 사이에는 일 정한 간격이 있다. 본 개시의 실시예에서, 가상 캐릭터에 대한 로드블록을 설정하는 것에 더하여, 가상 캐릭터가 이동하는 동안 가 상 캐릭터에 대한 태스크가 추가로 설정될 수 있다. 예를 들어, 가상 캐릭터 앞에 축구공이 있고, 태스크는 \"축 구공 차기\"로 설정될 수 있으며, 축구공의 좌표 위치를 타깃 지점으로 사용하여, 타깃 지점에 따른 태스크 정보 를 결정할 수 있다. 특정 방향으로 이동하도록 가상 캐릭터를 구동하기 위해 타깃 속도 방향이 가상 캐릭터에 대해 추가로 설정되며, 타깃 속도 방향에 따라 태스크 정보가 결정될 수 있다. 본 개시의 실시예에서, 가상 캐릭터의 제스처 및 액션은 연속된 시간 및 공간에서 서로 연관된다. 한 걸음 내딛 는 사람 형상의(human-shaped) 가상 캐릭터를 예로 사용하며, 사람 형상의 가상 캐릭터의 오른발이 현재 순간에 들어 올려지면 다음 순간에 오른발이 착지하는 경향이 있다. 따라서, 다음 순간의 가상 캐릭터의 관절 액션 정 보의 결정은 현재 순간의 가상 캐릭터의 상태 정보 처리를 기반으로 해야 하며, 그 상태 정보는 가상 캐릭터의 각 관절의 상태를 기술하는데 사용되며, 관절의 제스처, 속도, 위상(phase)을 포함할 수 있다. 따라서, 장애물 을 피하고 태스크를 완료하기 위해 현재 순간의 가상 캐릭터의 제스처를 변경하는 방법을 결정하기 위해, 현재 순간의 그래픽 사용자 인터페이스에서의 지형 특징 그리고 현재 순간의 가상 캐릭터에 대응하는 상태 정보 및 태스크 정보가 획득될 수 있으며, 이러한 정보를 처리하여 대응하는 제스처 조정 정보를 획득할 수 있다. 본 개시의 실시예에서, 애니메이션 세그먼트를 생성할 때, 애니메이터는 애니메이션 세그먼트를 상이한 형식 (format)으로 설정할 수 있고; 애니메이션 세그먼트에서 가상 캐릭터의 상태 정보를 추출할 때, 먼저 애니메이 션 세그먼트의 형식을 일부 소프트웨어(예: MotionBuilder 또는 3ds Max)를 사용하여 FBX 형식 또는 BVH 형식의 파일로 변환하고, 그런 다음 상태 정보를 추출한다. 실제 구현에서, 현재 순간이 애니메이션 세그먼트의 초기 (initial) 순간인 경우, 애니메이션 세그먼트의 초기 순간에서 가상 캐릭터의 제스처 정보에 따라 상태 정보가 결정되고, 구현 동안 초기 순간의 제스처 정보가 상태 정보로 결정될 수 있으며; 현재 순간이 애니메이션 세그 먼트의 초기가 아닌 순간인 경우, 이전 순간의 가상 캐릭터에 대응하는 관절 액션 정보에 따라 상태 정보가 결 정되고, 구현하는 동안 이전 순간의 가상 캐릭터에 대응하는 관절 액션 정보가 상태 정보로 결정될 수 있다. 본 개시의 실시예에서, 사람 형상의 가상 캐릭터를 예로 사용하며, 사람 형상의 가상 캐릭터는 총 15개의 관절 을 가지며, 각각 루트(root) 관절, 가슴, 목, 오른쪽 다리, 왼쪽 다리, 오른쪽 무릎, 왼쪽 무릎, 오른쪽 발목, 왼쪽 발목, 오른쪽 어깨, 왼쪽 어깨, 오른쪽 팔꿈치, 왼쪽 팔꿈치, 오른손 및 왼손이며, 여기서 루트 관절은 일 반적으로 골반 위치를 나타내며 루트로 표시된다. 일반적으로 사람 형상의 가상 캐릭터의 뼈와 관절은 부모-자 식 계층 구조(parent-child hierarchical structure)로 되어 있다. 예를 들어, 어깨는 부모 관절이고, 팔꿈치 는 어깨의 자식 관절이며, 손목은 팔꿈치의 자식 관절이다. 자식 관절의 위치는 부모 관절의 위치로부터 대응하 는 트렌스레이션(translation)을 수행하는 것에 의해 획득된다. 따라서, 자식 관절의 위치 좌표를 기록할 필요 가 없으며, 최상위 루트 관절의 위치 좌표를 알고, 애니메이션을 디자인하는 동안 애니메이터에 의해 뼈 세트의 크기에 따라 트렌스레이션이 수행되면, 자식 관절의 위치 좌표를 획득할 수 있다. 액션의 경우, 캐릭터 관절의 제스처 정보를 애니메이션 세그먼트에 기록하고, 필요한 관절 각각의 위치와 회전을 알면, 가상 캐릭터의 현재 액션을 구성할 수 있다. 루트 관절의 위치 및 회전 외에도, 다른 관절의 대응하는 회전이 기록되어 가상 캐릭터 의 현재 전체 제스처를 구성한다. 도 7은 사람 형상의 캐릭터의 걷기 액션의 제1 프레임의 액션 정보를 도시한 다. 도 7에 도시된 바와 같이, 제1 프레임의 액션 정보는 세 줄로 나뉜다. 첫 번째 줄의 제1 숫자 0.0333333은 제1 프레임의 지속 기간(duration)을 초(second)로 나타내고, 그 다음 3개의 숫자(001389296, 0.8033880000000001, 0.0036694320000000002)는 3차원 공간에서 제1 프레임의 루트 관절의 좌표이며; 두 번째줄의 4개의 숫자(0.5306733251792894, -0.5324986777087051, -0.4638864011202557, -0.46865807049205305)는 제1 프레임의 루트 관절의 회전 정보이고; 세 번째 줄의 4개의 숫자(0.7517762842400346, 0.0012912812309982618, -0.0033 740637622359164, 0.6594083459744481)는 루트 관절에 대응하는 제1 자식 관 절의 회전이며, 나머지 자식 관절의 회전 정보는 도 7에서 생략된다. 회전 정보는 단위 쿼터니언(unit quaternion)으로 표시된다. 단위 쿼터니언은 3차원 공간에서 회전을 표현하기 위해 사용될 수 있으며, 흔히 사 용되는 3차원 직교 행렬 및 오일러 각도(Euler angle)와 동등하지만, 오일러 각도 표현에서 짐벌 잠금(gimbal lock) 문제를 피할 수 있다. 3차원 공간에서 한 점의 데카르트 좌표(Cartesian coordinate)가 (x, y, z)인 경 우, 그 점은 순수(pure) 쿼터니언(이는 순허수(pure imaginary number)와 유사하며, 즉, 0의 실수 컴포넌트를 갖는 쿼터니언) xi+yj+zk으로 표현된다. i, j 또는 k의 기하학적 의미는 일종의 회전으로 이해될 수 있으며, 여 기서 회전 i는 X축과 X축의 교차 평면에서 X축 양의 방향으로부터 Y축 양의 방향을 향한 회전을 나타내며, 회전 j는 Z축과 X축의 교차 평면에서 Z축 양의 방향으로부터 X축 양의 방향을 향한 회전을 나타내고, 회전 k는 Y축과 Z축의 교차 평면에서 Y축 양의 방향으로부터 Z축 양의 방향을 향한 회전을 나타내며, -i, -j, -k는 각각 회전 i, 회전 j, 회전 k의 역회전을 나타낸다. 본 개시의 실시예에서, 애니메이션 처리 모델에 입력되는 상태 정보는 197 차원 벡터(dimensional vector)일 수 있으며, 여기서 포함된 제스처는 106 차원, 포함된 속도는 90 차원, 포함된 위상은 1차원일 수 있다. 일부 실시 예에서, 제스처는 사람 형상의 캐릭터의 15개 관절의 위치 및 회전 정보를 기록하며, 여기서 위치는 3차원 좌표 로 표시되고, 회전 정보는 15*7 = 총 105 차원인 단위 쿼터니언으로 표시된다. 또한, 현재 순간에서 가상 캐릭 터의 루트 관절 좌표의 1차원 y축 값을 추가로 기록해야 하며, 이는 월드 좌표계와의 정렬에 사용된다. 속도는 각 관절의 선형 속도(linear speed)와 각속도(angular speed)를 기록하며, 각각은 x축 속도, y축 속도, z축 속 도에 대응하는 길이가 3인 벡터로 나타내므로, 15*(3+3)= 총 90 차원이 있다. 위상은 애니메이션 세그먼트의 총 시간 길이에서 현재 순간의 위치를 기록하며, 이는 총 1차원이다. 본 개시의 실시예에서, 지형 특징은 2차원 행렬일 수 있으며, 행렬의 각 엘리먼트는 대응하는 지점의 지형 높이 와 가상 캐릭터의 현재 위치의 높이 사이의 상대 높이 차이이며, 이는 가상 캐릭터 앞의 미리 설정된 범위 내 영역(region)의 높이를 커버한다(cover). 행렬의 크기와 덮는 지형(covering terrain)의 면적(area)은 실제 적 용 시나리오에 따라 조정될 수 있다. 예를 들어, 2차원 행렬의 크기는 100*100으로 설정되고, 덮는 지형의 면적 은 10m*10m로 설정된다. 이는 본 개시의 실시예에 한정되지 않는다. 도 8은 지형의 인터페이스의 개략도를 도시 한다. 도 8에 도시된 바와 같이, 지형은 직사각형 영역이고, 가상 캐릭터는 왼쪽 사이드라인(sideline)의 중간 지점에 위치되며, 화살표는 가상 캐릭터 A의 이동 방향(movement direction)을 나타낸다. 가상 캐릭터 A는 방향 전환 없이(without turning) 수평 방향으로만 전진하고 장애물 B에 평행하고 수직으로 장애물 B만큼 높기 때문 에, 지형 특징은 가상 캐릭터 앞에서 10m 이내의 지형 특징을 커버하는 100*1의 행렬로 결정될 수 있다. 현재 순간의 그래픽 사용자 인터페이스에서 지형 특징 그리고 현재 순간의 애니메이션 세그먼트에서 가상 캐릭 터에 대응하는 상태 정보 및 태스크 정보를 획득할 때, 단말이 전송한 현재 순간의 지형 특징, 상태 정보 및 태 스크 정보를 수신할 수 있거나, 그래픽 사용자 인터페이스 및 수신된 설정 정보에 따라 서버 자체에서 현재 순 간의 지형 특징, 상태 정보 및 태스크 정보를 결정할 수 있다. 단계(S320)에서: 지형 특징, 상태 정보 및 태스크 정보를 애니메이션 처리 모델에 입력하고, 애니메이션 처리 모델을 사용하여 지형 특징, 상태 정보 및 태스크 정보에 대해 특징 추출을 수행하여, 다음 순간의 가상 캐릭터 에 대응하는 관절 액션 정보를 획득한다. 본 개시의 실시예에서, 현재 순간의 지형 특징, 상태 정보 및 태스크 정보를 획득한 후, 이러한 정보가 애니메 이션 처리 모델에 입력될 수 있으며, 애니메이션 처리 모델을 사용하여 지형 특징, 상태 정보, 태스크 정보에 대해 특징 추출을 수행하여, 다음 순간의 가상 캐릭터에 대응하는 관절 액션 정보를 획득할 수 있다. 관절 액션 정보는 가상 캐릭터가 현재 순간에서 지형 특징 및 태스크 특징에 직면하는 경우 다음 순간에 각 관절이 수행할 수 있는 액션에 대한 정보이며, 관절 액션 정보는 관절의 회전 정보일 수 있으며, 이는 4차원 길이로 표현되고, 루트 관절 이외의 관절의 회전 정보를 포함하며, 이는 (15-1)*4 = 총 56차원이다. 루트 관절의 제스처는 다른 관절이 토크 효과 하에서 이동 및 회전을 수행한 후 물리적 엔진에 의해 시뮬레이션되고 획득될 수 있다. 예를 들어, 사람 형상의 가상 캐릭터가 평지에서 앞으로 걷고, 다리와 무릎과 같은 관절의 회전 정보에 의해 결정된 토크에 따라 물리적 엔진이 이동 및 회전을 수행한 후, 발에 가해지는 후방 정지 마찰력(backward static friction force)이 하지, 무릎, 허벅지, 루트 관절까지 순차적으로 전달되고, 힘의 영향으로 루트 관절이 앞으 로 밀려나며, 이에 따라 관절 액션 정보에서 루트 관절의 액션 정보가 생략될 수 있다.본 개시의 실시예에서, 특징 추출이 강화 학습 기반의 애니메이션 처리 모델을 사용하여, 획득된 지형 특징, 상 태 정보 및 태스크 정보에 대해 수행되어, 다음 순간의 가상 캐릭터에 대응하는 관절 액션 정보를 획득할 수 있 다. 도 9는 애니메이션 처리 모델의 개략적인 구조도를 도시한다. 도 9에 도시된 바와 같이, 애니메이션 처리 모델은 제1 제어 네트워크 및 제2 제어 네트워크를 포함한다. 제1 제어 네트워크는 가상 캐릭터의 주요 관절(key joint)의 액션을 가이드하도록(guide) 구성된 고수준 컨트롤러(high-level controller, HLC)일 수 있고, 주요 관절은 지형 특징, 가상 캐릭터의 상태 정보 및 태스크 정보에 대응하는 일 부 관절이다. 예를 들어, 사람 형상의 가상 캐릭터가 달리고 있는 경우, 다리의 액션이 주로 바뀌고, 허벅지는 다리와 발을 움직이게 하므로, 주요 관절이 허벅지 관절이 된다. 이와 유사하게, 사람 형상의 가상 캐릭터가 던 지고 있는 경우, 팔과 손의 액션이 주로 바뀌고 팔꿈치는 손목과 손을 움직여 움직이므로, 주요 관절은 팔꿈치 가 된다. 제2 제어 네트워크는 모든 관절에 대응하는 관절 액션 정보를 출력하도록 구성된 저수준 컨트롤 러(low-level controller, LLC)일 수 있다. 복잡한 애니메이션 장면과 복잡한 태스크에 대한 적응은 제1 제어 네트워크와 제2 제어 네트워크를 각각 설정하는 것에 의해 더 잘 구현될 수 있다. 또한, 제1 제어 네트워크는 주로 특정 액션을 가이드하도록 구성되고, 제2 제어 네트워크는 주로 캐릭터의 이동을 제어하도록 구성된다. 상 이한 특정 액션을 위한 복수의 제1 제어 네트워크는 훈련된 제2 제어 네트워크에 연결될 수 있다. 예를 들어, 훈련된 제2 제어 네트워크는 발의 타깃 상태 정보에 따라 가상 캐릭터의 발 액션의 관절 액션 정보를 출력할 수 있고, 발의 타깃 상태에 대응하는 액션은 가상 캐릭터가 발로 공을 차는 액션 또는 가상 캐릭터가 점프하는 액 션일 수 있다. 이 경우, 동일한 제2 제어 네트워크가 가상 캐릭터의 공 차기를 가이드하는 제1 제어 네트워크에 연결될 수 있고, 가상 캐릭터의 점프를 가이드하는 제1 제어 네트워크에도 연결될 수 있다. 애니메이션 세그먼 트는 제1 제어 네트워크와 제2 제어 네트워크를 포함하는 애니메이션 처리 모델을 사용하여 처리되며, 이는 액 션의 효과와 액션의 충실도를 향상시킬 수 있고, 다양한 지형에 적응할 수 있어 환경에 대한 자기 적응력을 향 상시킨다. 일부 실시예에서, 제1 제어 네트워크는 지형 특징 그리고 현재 순간의 가상 캐릭터에 대응하는 상태 정보 및 태스크 정보에 대해 특징 추출을 수행하여, 주요 관절에 대응하는 타깃 상태 정보를 획득하며; 그런 다음 타 깃 상태 정보를 타깃 태스크 정보로 결정하고, 상태 정보와 타깃 태스크 정보를 제2 제어 네트워크에 입력 한다. 제2 제어 네트워크를 사용하여 가상 캐릭터에 대응하는 상태 정보 및 타깃 태스크 정보에 대해 특징 추출을 수행하여, 가상 캐릭터의 모든 관절에 대응하는 관절 액션 정보를 획득한다. 사람 형상의 가상 캐릭터가 장애물을 넘는 경우를 예로 들 수 있다. 높이가 상이한 장애물 앞에서, 사람 형상의 가상 캐릭터가 성공적으로 넘도록 하기 위해, 사람 형상의 가상 캐릭터는 상이한 각도로 다리를 들어 올리고 상이한 크기로 걸음을 내딛는 다. 제1 제어 네트워크는 지형 특성, 태스크 정보, 및 상태 정보에 따라 캐릭터의 평면 상에서 두 허벅지 관절의 회전 및 루트 관절의 속도 방향을 출력할 수 있으며, 캐릭터의 평면 상에서 두 허벅지 관절의 회전 및 루트 관절의 속도 방향은, 주요 관절에 대응하는 타깃 상태 정보이며, 그 출력은 제2 제어 네트워크의 타 깃 태스크로 사용되어, 사람 형상의 가상 캐릭터가 다리를 들어올리도록 가이드한다. 이에 대응하여, 제1 제어 네트워크의 출력은 10차원 벡터, 즉 두 허벅지의 회전을 측정하는 단위 쿼터니언 및 길이가 2인 단위 벡터 일 수 있다. 물론, 타깃 상태 정보는 캐릭터의 평면 상의 두 허벅지 관절의 회전 및 루트 관절의 속도 방향 이 외에, 두 개의 손 관절의 추가 회전, 두 개의 어깨 관절의 회전 등일 수 있다. 타깃 상태 정보는 장애물 유형 및 태스크 정보에 따라 상이하다. 또한, 도 10은 제1 제어 네트워크의 개략적인 구조도를 도시한다. 도 10에 도시된 바와 같이, 제1 제어 네트워 크는 컨볼루션 유닛(convolution unit), 제1 완전 연결 계층(fully connected layer), 제2 완전 연결 계층 및 제3 완전 연결 계층을 포함하며, 여기서 컨볼루션 유닛은 서로 다른 크기 의 복수의 컨볼루션 계층을 포함할 수 있다. 도면에 도시된 바와 같이, 제1 컨볼루션 계층 세트의 크기는 8*8이 고, 제2 컨볼루션 계층 세트와 제3 컨볼루션 계층 세트의 크기는 모두 4*4이며, 제1 완전 연결 계층, 제2 완전 연결 계층 및 제3 완전 연결 계층의 크기는 서로 다르고, 여기서 제1 완전 연결 계층, 제2 완전 연결 계층 및 제3 완전 연결 계층에 포함되는 신경 셀(nerve cell)의 수는 각각 64, 1024, 512이다. 지형 특징 T, 태스크 정보 gH 및 상태 정보 sH가 제1 제어 네트워크에 입력된 후, 컨볼루션 유닛 을 사용하여 지형 특징 T에 대한 특징 추출이 먼저 수행되어 제1 특징 정보를 획득하고; 그런 다음, 제1 완전 연결 계층을 사용하여 제1 특징 정보에 대해 특징 조합(feature combination)을 수행하여 제2 특징 정보를 획득하며; 다음으로 제2 완전 연결 계층을 사용하여 제2 특징 정보, 상태 정보 sH 및 태스크 정보 gH에 대해 특징 조합을 수행하여 제3 특징 정보를 획득하고; 제3 완전 연결 계층을 사용하여 제3 특징 정보에대해 특징 조합을 최종적으로 수행하여 타깃 상태 정보 aH를 획득한다. 도 11은 제2 제어 네트워크의 개략적인 구조도를 도시한다. 도 11에 도시된 바와 같이, 제2 제어 네트워크(90 2)는 제4 완전 연결 계층 및 제5 완전 연결 계층을 포함하고, 제4 완전 연결 계층과 제5 완 전 연결 계층의 크기는 서로 다르다. 일부 실시예에서, 제4 완전 연결 계층은 1024개의 신경 셀을 포함할 수 있고, 제5 완전 연결 계층은 512개의 신경 셀을 포함할 수 있다. 제1 제어 네트워크가 타깃 상태 정보 aH를 출력한 후, 타깃 상태 정보 aH는 제2 제어 네트워크의 타깃 태스크 정보 gL로 간주될 수 있고, 상태 정보 sL과 함께 제2 제어 네트워크에 동시에 입력되며, 제4 완전 연결 계층을 사용하여 상태 정보 sL 및 타깃 태스크 정보 gL에 대해 특징 조합을 수행하여 제4 특징 정보를 획득하고, 이어서 제5 완전 연결 계층을 사용하여 제4 특징 정보에 대해 특징 조합을 수행하여 관절 액션 정보 aL을 획득한다. 본 개시의 실시예에서, 제1 제어 네트워크는 가상 캐릭터의 주요 관절의 액션, 즉 특정 액션을 가이드하고, 제2 제어 네트워크는 가상 캐릭터의 전체 관절의 관절 액션 정보를 출력하여, 연속적인 액션 을 형성하며, 즉 캐릭터의 이동을 제어하며, 이에 따라 제1 제어 네트워크와 제2 제어 네트워크의 호 출 주기(period)가 상이하다. 즉, 제1 제어 네트워크는 캐릭터의 액션 또는 주요 관절의 상태가 변경된 경 우에만 호출될 필요가 있고; 그리고 가상 캐릭터가 이동하는 한, 각각의 관절은 대응하는 관절 액션 정보에 대 응하며, 따라서 제2 제어 네트워크는 지속적으로 호출될 필요가 있다. 가상 캐릭터가 로드블록을 넘는 것 을 예로 들 수 있다. 제1 제어 네트워크는 가상 캐릭터가 스텝을 밟을 때만 호출되어야 하는 반면, 제2 제 어 네트워크는 지속적으로 호출되어 가상 캐릭터가 연속적으로 액션하도록 제어한다. 제1 제어 네트워크 와 제2 제어 네트워크에 대해 서로 다른 호출 주기를 설정함으로써, 시간과 자원을 절약할 수 있어, 애니메이션 처리 모델의 처리 효율을 향상시키고, 그에 따라 액션 생성의 효율을 향상시킬 수 있다. 본 개시의 실시 예에서 PD 컨트롤러에 대한 제1 제어 네트워크의 호출 주파수는 2Hz이고, 제2 제어 네트워크의 호출 주파수는 30Hz이며, 물리적 시뮬레이션 주파수는 3000Hz이다. 실제 사용시에, 현재 순간의 지형 특징, 태 스크 정보, 및 상태 정보에 따라 제1 제어 네트워크을 호출해야 하는지를 판정한다. 그러나, 다음 순간의 가상 캐릭터의 관절 액션 정보를 예측하기 위해서는 제2 제어 네트워크가 지속적으로 호출될 필요가 있다. 제1 제어 네트워크가 호출되지 않은 경우, 제2 제어 네트워크의 입력은 변경되지 않는다. 본 개시의 실시예에서, 애니메이션 처리 모델을 사용하여 지형 특징, 상태 정보 및 태스크 정보에 대해 특징 추 출을 수행하기 전에, 안정적인 애니메이션 처리 모델을 획득하기 위해 훈련 대상(to-be-trained) 애니메이션 처 리 모델을 훈련시킬 필요가 있다. 애니메이션 처리 모델을 훈련하는 동안, 일반적으로 채택되는 방법은 지형 특 성을 모델에 입력하는 것이다. 그러나, 이 방법은 효과가 보통이고 훈련이 실패하기 쉬우며, 캐릭터의 액션이 약간 뻣뻣하여, 결과적으로 이 방법은 비교적 단순한 지형에만 적응한다. 따라서, 본 개시의 실시예에서는 지형 과 액션에 대한 애니메이션 처리 모델의 민감도를 강화하고 보다 복잡한 지형으로 이행하기(migrate) 위해, 모 델에 입력된 지형 특징을 분할하여(split) 계층적 강화 학습을 적용한다. 강화 학습은 머신 러닝의 한 분야이며, 환경에 기반하여 액션하는 방법을 강조하여 기대 효과를 극대화한다. 이동 제어 문제(movement control problem)는 강화 학습의 표준 벤치마크가 되며, 딥(deep) 강화 학습 방법은 조작(manipulation) 및 이 동을 포함한 여러 태스크에 적응하는 것으로 입증되었다. 본 개시의 실시예에서, 강화 학습은 각각 환경, 지능형 에이전트, 상태, 액션, 보상(reward), 가치 함수(value function) 및 정책인 복수의 기본 개념을 포함한다. 환경은 외부 시스템이며, 지능형 에이전트는 시스템 내부에 위치하여 시스템을 인지하고 인지된 상태에 따라 특정 액션을 수행할 수 있다. 지능형 에이전트는 환경에 내장 된 시스템이며, 상태를 변경하는 태스크를 수행할 수 있다. 상태는 순간에서의 현재 환경의 상태 정보이다. 액 션은 주체가 수행하는 행동이다. 보상은 현재 액션이나 상태에 대한 환경의 보상을 나타내는 스칼라(scalar)이 다. 보상은 즉각적인 이익을 정의하는 반면, 가치 함수는 누적된 보상으로 간주될 수 있고 일반적으로 V로 표시 되는 장기 이익(long-term benefit)을 정의한다. 정책은 현재 환경 상태에서 행동으로 매핑하는 것으로, 일반적 으로 입력 상태인 π로 표시되며, 모델은 상태에서 수행되어야 하는 액션을 출력한다. 도 12는 강화 학습의 개 략적인 흐름도를 나타낸다. 도 12에 도시된 바와 같이, 순간 t에서, 현재 상태 St가 지능형 에이전트에 입력되 고, 현재 정책에 따라 지능형 에이전트는 액션 At를 출력할 수 있으며; 액션 At는 환경과 상호작용하도록 수행되 며, 타깃의 완료 조건에 따라 환경은 보상 Rt 및 다음 순간 t+1에서서의 지능형 에이전트의 상태 St+1을 피드백하 고; 지능형 에이전트는 보상에 따라 정책을 조정하여 다음 순간에서의 액션 At+1을 출력하며; 그리고 이 프로세 스를 반복하고, 계속해서 정책을 조정하며, 타깃을 완수할 수 있는 정책 π를 훈련을 통해 최종적으로 획득할수 있다. 본 개시의 실시예에서, 애니메이션 처리 모델은 AC 프레임워크에 기반하여 훈련된다. AC 프레임워크는 가치 함 수 추정 알고리즘과 정책 탐색 알고리즘을 통합한 프레임워크이며, 액터(actor) 네트워크와 크리틱(critic) 네 트워크의 2개의 네트워크를 포함한다. 액터 네트워크는 현재 정책을 훈련하며, 그리고 액션을 출력하도록 구성 되고, 크리틱 네트워크는 가치 함수를 학습하고 그리고 현재 상태 값(state value) V(s)를 출력하도록 구성되며, 값은 상태의 품질을 평가하는 데 사용된다. 도 13은 애니메이션 처리 모델의 알고리즘 프레임워크의 아키텍처 다이어그램을 나타낸다. 도 13에 도시된 바와 같이, 프레임워크는 액터 네트워크, 크리틱 네트 워크 및 환경을 포함한다. 액터 네트워크는 현재 상태 및 정책에 따른 액션을 출력하고, 환 경은 액터 네트워크에 의해 출력된 액션에 따라 보상 형태로 피드백하며, 크리틱 네트워크는 액션이 수행된 후 생성된 상태 및 환경에 의해 피드백된 보상에 따라 평가를 수행하며, 현재 상태 값을 결정하고, 현재 상태 값을 액터 네트워크에 피드백하여 액터 네트워크가 정책을 조정하게 한다. 애 니메이션 처리 모델이 안정될 때까지 이 프로세스를 반복하고 훈련을 계속하여 수행한다. 크리틱 네트워크 에 의해 출력되는 현재 상태 값의 학습 기준(learning standard)이, 환경에 의해 피드백되는 일련 의 보상을 시간차 방법(temporal difference method)을 사용하여 계산하는 것에 의해 획득되며, 그리고 크리틱 네트워크의 학습을 가이드하는 데 사용된다. 일부 실시예에서, 경로 시뮬레이션이 예로서 사용되고, 경로 상의 노드에 대응하는 보상 R1 내지 Ri가 획득될 수 있으며, i는 경로 상의 노드의 수이고; 경로 상의 노드 t에 대응 하는 상태 값 V(St)를 구하고자 하는 경우, t는 1 내지 i 범위의 값이며, St의 값 V(St)는 획득된 보상 R과 후속 상태의 상태 값의 추정된 값에 따라 업데이트된다. 업데이트가 여러 번 수행된 후, 안정적인 가치 함수가 획득 되며; 경로에 대해 샘플링이 수행된 후 가치 함수는 여러 번 업데이트될 수 있고, 사용된 평가 알고리즘은 일 수 있으며, α는 계수이다. 시간차 방법은 강화 학습의 중심 아이디 어이다. 몬테카를로(Monte Carlo) 방법과 유사하게, 시간차 방법은 몬테카를로 방법의 샘플링 방법(즉, 실험을 수행)과 동적 프로그래밍 방법(dynamic programming method)의 부트스트래핑(bootstrapping)(후속 상태의 가치 함수를 사용하여 현재 가치 함수를 추정)과 조합되며, 그리고 환경에 대한 완전한 지식 없이 경험에서 직접 알 수 있다. 시간차 방법은 동적 프로그래밍 방법과 유사하게, 전체 이벤트가 완료될 때까지 기다리지 않고 기존 추정 결과를 개선할 수 있으며, 이에 따라 학습 효율을 높일 수 있다. 본 개시의 실시예에서, 모델의 훈련 동안, 물리적 엔진은 각각 운동학적 캐릭터(kinematics character) 및 물리 적 캐릭터인 2개의 캐릭터를 포함한다. 운동학적 캐릭터는 물리적 속성을 가지지 않으며, 애니메이션 디자이너 가 디자인한 액션 세그먼트에서 액션을 수행하기 위해서만 사용되며, 운동학 방법을 사용하는 애니메이션 세그 먼트에서 운동학적 캐릭터의 관절이 참조 액션을 수행하도록 하기 위해서만 필요하다. 물리적 캐릭터는 운동학 적 캐릭터를 기준(standard)과 템플릿(template)으로 사용하여 학습하고, 물리적인 속성을 가지며, 그리고 토크 에 의해 제어될 수 있으며, 물리적 속성은 토크, 속도, 중력, 충돌 효과 등일 수 있다. 한편, 물리적 속성을 갖 는 물리적 캐릭터는 모델이 출력하는 제스처를 사용하여 각 관절의 토크를 계산하고, 물리적 엔진에서 액션 시 뮬레이션을 수행한다. 물리적 엔진은 각 액션이 수행된 후 환경의 조건을 시뮬레이션하여 실제 효과를 생성한다. 매 순간, 보상을 계산한다는 것은 두 캐릭터의 현재 제스처, 속도, 각속도 등의 차이를 측정하는 것 이며, 차이가 작을수록 보상이 크다. 복수의 보상 컴포넌트에 대해 가중 합산(weighted summation)을 수행하는 것에 의해 최종 보상이 획득되며, 필요에 따라 가중치가 조정될 수 있다. 환경은 제스처 시뮬레이션의 품질에 따라 보상을 제공하여, 캐릭터가 참조 액션의 제스처와 일치하는(consistent) 제스처를 유지하도록 자극한다 (stimulate). 두 제스처가 가까울수록 보상이 커지며, 그렇지 않으면 보상이 더 작아진다. 본 개시의 실시예에서, 보상은 수식 에 따라 결정되며, 수식 은 다음과 같다."}
{"patent_id": "10-2022-7002917", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "은 순간 t에서의 시뮬레이션의 보상 값이며, 는 순간 t에서의 타깃 태스크를 완료하는 보상 값이고, 가중 치 는 액션을 시뮬레이션하는 비율을 나타내며, 가중치 는 태스크를 완료하는 비율이고, 공학적으로(in engineering) 및 가 설정될 수 있으며, 이다. 물리적 캐릭터와 운동학적 캐릭터의 액션이 일치되도록 하기 위해, 물리적 캐릭터와 운동학적 캐릭터에 맞게 일 부 준이 설정될 수 있으며, 수식 는 다섯 부분: 제스처 보상 , 속도 보상 , 사지 관절(extremity joint) 보상 , 루트 관절 제스처 보상 및 중심 제스처(centroid gesture) 보상 을 포함하는 운동학적 측면에서 유사도를 포함한다. 제스처와 속도는 각 관절의 제스처 및 속도이다. 두 캐릭터의 액션이 일치되어야 하면, 제스처와 속도가 반드시 일치해야 하므로, 제스처 보상과 속도 보상이 설정될 수 있다. 사지 관절은 손과 발을 나타낸다. 물리적 캐릭터의 사지 관절이 운동학적 캐릭터의 사지 관절과 정렬되도록 하기 위해, 사지 관절 에 대한 사지 관절 보상이 설정된다. 루트 관절은 모든 관절의 상단(top) 관절이다. 두 캐릭터의 액션이 일치되 어야 하면, 루트 관절도 일치되어야 하며, 이에 따라 루트 관절 제스처 보상이 설정될 수 있다. 또한, 물리적 캐릭터가 보다 안정적으로 걷고 흔들리지 않도록 하기 위해, 물리적 캐릭터의 중심(centroid)이 운동학적 캐릭 터의 중심과 일치하도록 해야 하므로 중심 제스처 보상이 설정될 수 있다. 전술한 보상을 설정함으로써, 물리적 캐릭터와 운동학적 캐릭터의 액션이 최대한 일치하도록 보장될 수 있다. 보상에 대응하는 가중치는 이다. 운동학적 캐릭터 항목(item)의 오른쪽 상단 모서리는 *로 표 시된다. 제스처 컴포넌트가 예로 사용되며, 여기서 는 운동학적 캐릭터의 j번째 관절의 제스처이고, 는 시 뮬레이션 캐릭터의 j번째 관절의 제스처이다. 수식 은 다음과 같이 수식 로 변환될 수 있다."}
{"patent_id": "10-2022-7002917", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, 는 제스처 간의 유사도를 기술하며, 그리고 각 관절의 위치 및 회전과 타깃 값의 차이로 표현되고, 이며; 는 속도 사이의 유사도를 기술하고, 그리고 각 관절의 선형 속도와 타깃 값 사이 의 차이로 표현되고 이며; 는 사지 관절 제스처 사이의 유사도를 기술하고, 그리고 손 관절의 위치와 발 관절의 위치 사이의 차이로 표현되고, 이며; 는 루트 관절 사이의 유사도를 기술하며, 이고; 는 중심 속도 사이의 유사도를 기술 하며, 이다. 는 캐릭터가 타깃을 달성하는 품질을 기술하며, 일반적으로 실제 조건과 캐릭터의 이동의 타깃 사이의 차이 를 측정한다. 예를 들어, 타깃이 이동 방향 gt인 경우, 는 다음의 수식 에 보여지는 바와 같이, 지면 상 에서의 진행 방향(forward direction) vt과 타깃 gt의 각도 차이 θ로서 계산될 수 있다."}
{"patent_id": "10-2022-7002917", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "가상 캐릭터가 넘어지는 액션(action of falling down)을 학습하는데 실패한 경우, 현재 훈련 경로가 완료되며, 보상 값은 0이다. 본 개시의 실시예에서, 훈련 대상 애니메이션 처리 모델은 훈련될 제1 제어 네트워크 및 훈련될 제2 제어 네트 워크를 포함한다. 훈련 전에, 복수의 애니메이션 세그먼트 샘플을 획득할 수 있으며, 애니메이션 세그먼트 샘플 은 가상 캐릭터에 대응하는 서로 다른 지형 특징과 서로 다른 태스크 정보를 포함하고, 서로 다른 지형 특징과 서로 다른 태스크 정보에 대응하여, 가상 캐릭터는 서로 다른 제스처 및 액션을 가진다. 본 개시의 실시예에서애니메이션 처리 모델의 제1 제어 네트워크는 지형 특징, 태스크 정보 및 상태 정보에 따라 주요 관절에 대응하 는 타깃 상태 정보를 출력하며, 그런 다음 타깃 상태 정보가 타깃 태스크 정보로서 제2 제어 네트워크로 입력되 고 제2 제어 네트워크에 의해 처리되어, 관절 액션 정보를 출력하며, 이는 복합 태스크(complex task)를 처리하 는 데 사용될 수 있다. 제1 제어 네트워크와 제2 제어 네트워크가 동시에 훈련되고 제1 제어 네트워크에서 출력 되는 타깃 상태 정보에 에러(error)가 있는 경우, 에러가 있는 타깃 상태 정보를 제2 제어 네트워크에 입력하고, 애니메이션 처리 모델은 제2 제어 네트워크에 의해 출력되는 관절 액션 정보에 따라 역으로 훈련되며, 이에 따라 애니메이션 처리 모델이 불안정하여 복잡한 태스크를 효율적으로 처리하지 못한다. 따라서, 애니메이션 처리 모델이 복잡한 태스크를 처리할 수 있도록 하기 위해, 훈련될 제1 제어 네트워크와 훈 련될 제2 제어 네트워크를 훈련 중에 별도로 훈련해야 한다. 훈련될 제1 제어 네트워크의 훈련을 완료하는 것은 고정된 파라미터로 제2 제어 네트워크에 연결된, 훈련될 제1 제어 네트워크를 훈련시켜서 제1 제어 네트워크를 획득하는 것이다. 본 개시의 실시예에서, 애니메이션 처리 모델은 AC 알고리즘 프레임워크를 기반으로 학습되고, 애니메이션 처리 모델에서 훈련될 제1 제어 네트워크와 훈련될 제2 제어 네트워크는 별도로 훈련되고, 따라서, 훈련될 제1 제어 네트워크와 훈련될 제2 제어 네트워크는 각각 한 쌍의 AC 네트워크를 포함하도록 설정될 수 있으며, 즉, 훈련될 제1 제어 네트워크는 제1 훈련 대상 액터 서브 네크워크 및 제1 훈련 대상 크리틱 서브 네트워크를 포함하고, 훈련될 제2 제어 네트워크는 제2 훈련 대상 액터 서브 네크워크 및 제2 훈련 대상 크리틱 서브 네트워크를 포함 한다. 또한, 제1 훈련 대상 액터 서브 네트워크의 구조는 제1 훈련 대상 크리틱 서브 네트워크의 구조와 동일하 게 설정될 수 있으며, 제2 훈련 대상 액터 서브 네트워크의 구조는 제2 훈련 대상 크리틱 서브 네트워크의 구조 와 동일하게 설정될 수 있다. 제1 훈련 대상 액터 서브 네트워크 및 제1 훈련 대상 크리틱 서브 네트워크의 구 조에 대해서는 도 10을 참조할 수 있으며, 제2 훈련 대상 액터 서브 네트워크 및 제2 훈련 대상 크리틱 서브 네 트워크의 구조에 대해서는 도 11을 참조할 수 있으며; 차이점은 단지 입력된 정보와 출력되는 정보에 있다. 훈 련될 제1 제어 네트워크와 훈련될 제2 제어 네트워크의 훈련이 완료된 후, 제1 액터 서브 네트워크와 제2 액터 서브 네트워크를 호출하기만 하면, 제1 액터 서브 네트워크를 사용하여, 입력된 지형 특징, 태스크 정보 및 상 태 정보에 따라 주요 관절에 대응하는 타깃 상태 정보 aH가 출력되고, 추가로 제2 액터 서브 네트워크를 사용하 여 타깃 상태 정보 및 상태 정보에 따라 가상 캐릭터의 모든 관절의 관절 액션 정보 aL를 출력한다. 사람 형상의 가상 캐릭터가 장애물을 피하는 것이 예로 사용된다. 훈련될 제2 제어 네트워크의 훈련 동안, 애니 메이션 세그먼트 세트를 사용하여 평지에 대해 훈련을 수행할 수 있다. 애니메이션 세그먼트 세트는 서로 다른 높이의 장애물 앞에서 가상 캐릭터의 다리 들기(leg lifting) 및 스테핑(stepping)의 서로 다른 제스처를 커버 하는 복수의 애니메이션 세그먼트 샘플을 포함한다. 초기 액션들은 가깝고, 단 하나의 스텝만을 가진다. 예를 들어, 애니메이션 세그먼트 세트에는 총 15개의 애니메이션 세그먼트 샘플이 있으며, 각 애니메이션 세그먼트 샘플의 길이는 0.5초이다. 훈련 동안, 복수의 애니메이션 세그먼트 샘플 중에서 가장 적절한 애니메이션 세그먼 트 샘플을 선택하여 훈련할 수 있다. 애니메이션 세그먼트 세트를 획득한 후, 각 애니메이션 세그먼트 샘플에 대해 미러링(mirroring)이 수행될 수 있으며, 즉, 캐릭터의 오른쪽 다리의 스테핑이 왼쪽 다리의 스테핑으로 전환되고, 캐릭터의 왼쪽 다리의 스테핑 이 오른쪽 다리의 스테핑으로 전환될 수 있으며, 각 액션 세그먼트의 초기 제스처와 발을 착지하는(landing a foot) 제스처가 카운트된다. 제2 제어 네트워크는 제1 제어 네트워크의 출력을 타깃 태스크로 사용하기 때문에, 훈련될 제2 제어 네트워크가 훈련되는 경우, 가상 캐릭터의 초기 제스처가 미리 설정될 수 있으며, 타깃 애니메 이션 세그먼트 샘플은 초기 제스처에 따라 애니메이션 세그먼트 세트로부터 결정될 수 있고, 타깃 애니메이션 세그먼트 샘플에 따라 타깃 태스크가 결정되어, 훈련될 제2 제어 네트워크가 타깃 애니메이션 세그먼트 샘플에 따라 학습하게 한다. 가상 캐릭터가 한 스텝을 완료하고 다음 스텝을 시작할 준비가 된 경우, 전술한 작동이 반 복될 수 있으며, 초기 제스처와 동일하거나 유사한 타깃 애니메이션 세그먼트 샘플을 획득하여, 훈련될 제2 제 어 네트워크를 훈련할 수 있다. 타깃 애니메이션 세그먼트 샘플이 결정된 경우, 초기 제스처는 애니메이션 세그 먼트 세트의 각 애니메이션 세그먼트 샘플과 비교되어, 초기 제스처와 각 애니메이션 세그먼트 샘플의 가상 캐 릭터의 제스처 간의 유사도를 획득할 수 있으며, 그런 다음 복수의 유사도를 내림차순으로 순위 지정하여 시퀀 스를 형성하고, 가장 높은 유사도에 대응하는 애니메이션 세그먼트 샘플을 최종적으로 타깃 애니메이션 세그먼 트 샘플로 사용하거나; 또는 미리 설정된 수의 유사도가 시퀀스로부터 연속적으로 획득될 수 있고, 유사도에 대 응하는 애니메이션 세그먼트 샘플 중 임의의 하나가 타깃 애니메이션 세그먼트 샘플로 사용되며, 미리 설정된 수는 실제 요건에 따라 설정될 수 있으며, 예를 들어, 미리 설정된 수는 3 또는 5일 수 있다. 타깃 애니메이션 세그먼트 샘플이 결정된 후, 주요 관절에 대응하는 상태 정보 샘플을 타깃 애니메이션 세그먼트 샘플로부터 추출하고, 상태 정보 샘플을 타깃 태스크 정보로 사용하며, 가상 캐릭터의 모든 관절에 대응하는 관절 액션 정보 샘플을 동시에 획득하고; 그리고 타깃 태스크 정보를 훈련될 제2 제어 네트워크에 입력하여 훈련을 수행하며, 훈련될 제2 제어 네트워크에 의해 출력된 관절 액션 정보가 관절 액션 정보 샘플과 동일하거나 유사한 경우, 이 는 훈련될 제2 제어 네트워크의 훈련이 완료되었음을 지시한다. 관절 액션 정보 샘플은 발을 착지할 때 운동학 적 캐릭터의 두 허벅지의 회전 및 회전 그리고 평면 상에서 루트 관절의 속도 방향 일 수 있으며, 는 허벅지 관절에 대응하는 착지하는 발(landing foot)의 회전을 기술하고, 는 착지하려고 하는 비착지하는 발(non-landing foot)의 회전/허벅지 관절에 대응하는 착지하는 발의 회전을 기술하며, 현재 타깃 태스크 정보 는 두 허벅지의 회전 및 회전 그리고 평면에서 루트 관절의 속도 방향 에 따라 결 정될 수 있으며, 훈련을 수행하기 위해 훈련될 제2 제어 네트워크에 입력된다. 제2 제어 네트워크의 안정성(stability)을 확보하기 위해, 캐릭터의 복수의 달리기 경로(running path)에 따라 훈련이 수행될 수 있고, 캐릭터의 달리기 경로의 최대값은 200초(200s)와 같은 숫자 값으로 설정될 수 있다. 강 화 학습 기반으로 훈련될 제2 제어 네트워크는 가상 캐릭터가 달리기 경로를 완료한 후 가상 캐릭터의 액션 또 는 상태를 평가하고, 상태 값을 결정하며, 최대 상태 값이 획득될 때까지 상태 값에 따라 액션을 조정한다. 훈 련에 의해, 상이한 타깃 태스크 정보가 입력될 때, 제2 제어 네트워크는 대응하는 스테핑 액션을 수행할 수 있 다. 본 개시의 실시예에서, 제2 제어 네트워크의 훈련이 완료된 후, 훈련될 제1 제어 네트워크가 훈련될 수 있고, 훈련에 사용되는 달리기의 애니메이션 세그먼트 샘플은 하나뿐이며, 각 경로의 최대 길이는 200초로 제한될 수 도 있다. 각각의 경로가 시작되는 경우, 현재 순간의 지형 특징 샘플, 캐릭터 상태 샘플 및 태스크 정보 샘플이 입력될 수 있으며, 훈련될 제1 제어 네트워크를 사용하여 현재 순간의 지형 특징 샘플, 캐릭터 상태 샘플 및 태 스크 정보 샘플에 대해 특징 추출이 수행되며, 액션 정보가 출력된다. 액션 정보는 타깃 태스크로서 훈련된 제2 제어 네트워크에 입력될 수 있고, 대응하는 관절 액션 정보가 캐릭터의 액션을 제어하기 위해 제2 제어 네트워 크에 의해 출력될 수 있다. 유사하게, 가상 캐릭터가 달리기 경로를 완료한 후, 강화 학습에 기반하여 훈련될 제1 제어 네트워크는 환경에 의해 피드백되는 보상에 따라 가상 캐릭터의 상태에 대응하는 상태 값을 결정할 수 있으며, 상태 값이 미리 설정된 값 또는 최대값에 도달하는 경우, 이는 훈련될 제1 제어 네트워크의 훈련이 완 료되었음을 지시한다. 단계(S330)에서, 관절 액션 정보에 따라 관절 토크가 결정되고, 관절 토크를 기반으로 렌더링이 수행되어 가상 캐릭터에 대응하는 제스처 조정 정보를 획득하며, 제스처 조정 정보에 따라 애니메이션 세그먼트가 처리된다. 본 개시의 실시예에서, 애니메이션 처리 모델에 의해 출력되는 관절 액션 정보가 획득된 후, 관절 액션 정보에 따라 관절 토크가 결정될 수 있고, 관절 토크가 추가로, 물리적 엔진을 사용하여 강체 구조(rigid body structure)에 대응하는 관절에 적용되어 렌더링을 수행하여, 가상 캐릭터에 대응하는 제스처 조정 정보를 획득 하고, 애니메이션 세그먼트가 제스처 조정 정보에 따라 처리된다. 본 개시의 실시예에서, 모션 애니메이션에서, 캐릭터 제스처가 일반적으로 역운동학에 기반한 방법을 사용하여 제어된다. 그러나, 물리 기반의 캐릭터 제스처 제어를 위해, 실시간으로 캐릭터를 제어하기 위해 운동학적 방법 을 사용하는 경우, 실제 물리적 효과가 발생하지 않고 충돌과 같은 상호 효과가 인지되지 못하며, 따라서, 토크 가 일반적으로 캐릭터 액션을 제어하는 데 사용된다. 물리적 캐릭터를 실시간으로 제어하는 방법은 크게 3가지 가 있다: 토크 제어: 모델이 각 관절에 가해지는 토크를 직접 출력한다. 이 방법은 구현하기 쉽다. 그러나, 제어 효과가 약하고, 동적 제어가 불안정하며, 흔들림(shaking)이 발생하기 쉽고, 액션이 충분히 자연스럽지 않 다. 위치 제어: 모델이 각 관절의 타깃 위치를 제공하며, 그런 다음 PD 컨트롤러(proportional-derivative controller)를 사용하여 대응하는 위치에 있도록 캐릭터가 동적으로 제어된다. 토크 제어에 비해, 위치 제어가 더 안정적이며, 모델이 각 관절의 제스처를 출력하며, 이 방법은 분포 분산이 비교적 작고 샘플이 작으며 모델 수렴 속도(model convergence speed)가 높다. 그러나, 기존의 PD 제어는 여전히 상대적으로 큰 흔들림이 있다. 속도 제어: 모델이 각 관절의 타깃 속도를 직접 제공하며, 캐릭터는 그런 다음 PD 제어 알고리즘을 사용하 여 타깃 속도로 동적으로 제어되며, 속도 제어의 효과와 모델 수렴 속도가 위치 제어의 효과 및 모델 수렴 속도 와 실질적으로 일치한다. 그러나, 일반적으로 위치 컨트롤러가 사용되며, 이는 계층적 제어와 동등하며, 의사결정 네트워크(decision network)는 현재 캐릭터 상태를 획득하고, 다음 순간의 타깃 위치를 출력하며, 그런 다음 캐릭터가 PD 컨트롤러를 사용하여 타깃 제스처에 있도록 동적으로 제어되고, 실제 엔지니어링에서, PD의 제어 주기를 100으로 설정한 다. 이 방법은 모델 수렴 속도와 견고성(robustness) 모두에 좋은 영향을 미친다. 그러나, 일반적인 PD 컨트롤 러를 사용하는 경우, 흔들림이 상대적으로 크고 제스처가 그다지 표준적이지 않다. 본 개시의 실시예에서는 기존 제스처 제어 방식의 단점을 극복하기 위해 역운동학 기반의 안정적인 PD 제어를 제공한다. 기존의 PD 컨트롤러를 사용하여 토크를 결정하는 계산 수식은 다음 수식 와 같다."}
{"patent_id": "10-2022-7002917", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "는 토크 출력이고, q는 현재 순간에서의 가상 캐릭터의 관절의 현재 위치이며, 는 가상 캐릭터의 관절의 타깃 위치이고, 는 현재 순간에서의 관절의 속도이며, kp는 비례 계수이고, kd는 미분 이득 계수이며, n은 PD 제어의 제어 주기 수이다. 물리적 캐릭터를 제어하는 동안, 컨트롤러는 타깃 제스처와의 차이를 빠르게 줄여야 하므로, kp를 더 크게 설정 해야 한다. 이 경우, 높은 비례 이득의 안정성 문제가 발생하기 쉽다. 안정적인 PD 제어는 이 문제를 잘 해결할 수 있다. 안정적인 PD 제어는 다음 기간(time period) 이후의 위치를 사용하여 를 계산하여 획득하며, 이 는 타깃으로부터의 차이를 비교하면서 초기 상태를 고려하는 경우와 동등하며, 이에 따라 물리적 속성의 안정성 을 향상시킨다. 일부 실시예에서, 관절의 현재 위치 및 타깃 위치는 관절 액션 정보에 따라 결정되고; 관절의 현재 속도 및 현재 가속도는 현재 위치에 따라 결정되며, 관절의 타깃 속도는 타깃 위치에 따라 결정되고; 현재 속도 및 현재 가속도에 따라 다음 제어 주기 이후 관절에 대응하는 제1 위치 및 제1 속도가 결정되며; 관절 토 크는 비례 계수, 미분 이득 계수, 현재 위치, 타깃 위치, 타깃 속도, 제1 위치 및 제1 속도에 따라 계산된다. 계산 수식에 대해 수식 를 참조한다."}
{"patent_id": "10-2022-7002917", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "는 토크 출력이고, kp는 비례 계수이며, kd는 미분 이득 계수이고, 은 현재 위치이며, 은 일정 기간 후 현재 속도에서 관절의 제1 위치이고, 는 관절의 타깃 위치이며, 는 관절의 현재 속도이고, 는 일정 기간 후 현재 가속도에서 관절의 제1 속도이며, 는 관절의 타깃 속도이고, n은 컨트롤러의 제어 주기 수이다. 본 개시의 실시예에서, 하나의 관절 액션 정보에 대응하는 복수의 토크는 역운동학에 기반한 안정적인 PD 제어 를 사용하여 결정될 수 있으며, 복수의 토크는 추가로 물리적 엔진을 사용하여 대응하는 관절에 각각 적용되며, 관절의 회전축 및 앵커 포인트(anchor point)에 따라 각속도 및 최종 제스처를 계산하고, 관절 회전의 실제 조 건을 시뮬레이션하며, 즉, 현재 순간의 가상 캐릭터에 대응하는 제스처 조정 정보를 획득할 수 있으며, 제스처 조정 정보는 액션 시퀀스일 수 있다. 역운동학을 기반으로 한 안정적인 PD 제어는 계산 정확도를 향상시키고, 흔들림을 줄이며, 가상 캐릭터의 액션 효과를 향상시킬 수 있다. 본 개시의 실시예에서, 전술한 솔루션은 애니메이션 세그먼트의 마지막 프레임의 이미지의 시뮬레이션이 완료될 때까지 연속적인 기간에서 반복되므로, 매순간(each moment)에서 가상 캐릭터에 대응하는 제스처 조정 정보, 즉, 각각의 이미지 프레임에서 가상 캐릭터에 대응하는 제스처 조정 정보가 획득될 수 있고, 여기서 제스처 조 정 정보는 그래픽 사용자 인터페이스에서 새로 추가된 지형 특징 및 가상 캐릭터에 대응하는 설정 태스크 특징 에 따라 결정된 가상 캐릭터의 제스처에 관한 것이다. 타깃 액션 시퀀스가 매순간에서 가상 캐릭터에 대응하는 제스처 조정 정보에 따라 결정될 수 있다. 사용자의 관점에서 볼 때, 타깃 액션 시퀀스에 의해 제시되는 애니메 이션 효과는 원래 애니메이션 세그먼트의 효과보다 더 생생하고, 가상 캐릭터는 새로 설정된 장애물을 피하고 대응하는 태스크를 완료할 수 있으며, 애니메이션 효과는 더 생생하고, 사용자 경험이 더 좋다. 도 14의 (A) 내지 (J)는 애니메이션 처리 모델에 의해 제어되고 평지를 달리는 가상 캐릭터의 액션 시퀀스를 도 시한다. 도 14의 (A) 내지(J)에 도시된 바와 같이, 가상 캐릭터의 다리 들기, 스테핑, 발 착지, 팔 흔들기 액션이 보다 자연스럽고 생생하다. 도 15a 내지 도 15e는 조밀한 노치 지형을 달리는 사람 형상의 가상 캐릭터의 액션 시퀀스를 나타낸다. 도 15a 내지 도 15e에 도시된 바와 같이, 2개의 사람 형상의 가상 캐릭터: 흰색 사람 형상의 가상 캐릭터 W 및 검은색 사람 형상의 가상 캐릭터 B가 포함된다. 흰색 사람 형상의 가상 캐릭터 W는 원래 애니메이션 세그먼트에서 사람 형상의 가상 캐릭터이고, 검은색 사람 형상의 가상 캐릭터 B는 애니메이션 처리 모델에 의해 제어되는 사람 형 상의 가상 캐릭터이다. 도 15a 내지 도 15e로부터, 흰색 사람 형상의 가상 캐릭터 W와 검은 색 사람 형상의 가 상 캐릭터 B의 액션은 동일하고, 노치 C에서 흰색 사람 형상의 가상 캐릭터 W와 검은 색 사람 형상의 가상 캐릭 터 B 사이에 스테핑의 차이만 있음을 알 수 있다. 애니메이션 처리 모델에 의해 제어되는 검은색 사람 형상의 가상 캐릭터 B는 도 15a, 도 15b, 도 15d 및 도 15e에 도시된 바와 같이 조밀한 노치 지형 G의 전체 달리기를 성공적으로 완료할 수 있다. 도 16a 내지 도 16l은 하이브리드 장애물 지형에서 달리는 사람 형상의 가상 캐릭터의 액션 시퀀스를 도시한다. 도 16a 내지 도 16l에 도시된 바와 같이, 하이브리드 장애물 지형의 지면(G)은 노치(C), 돌기(E) 및 스텝(D)을 포함한다. 도 15와 유사하게, 도면은 원래의 애니메이션 세그먼트의 흰색 사람 형상의 가상 캐릭터 W와 애니메 이션 처리 모델에 의해 제어되는 검은색 사람 형상의 가상 캐릭터 B를 포함한다. 도 16a 내지 도 16e는 노치를 건너는 사람 형상의 가상 캐릭터의 액션 시퀀스를 도시하며, 도 16f 내지 도 16k는 돌기를 건너는 사람 형상의 가상 캐릭터의 액션 시퀀스를 도시하고, 도 16l은 사람 형상의 가상 캐릭터가 스텝을 건너는 액션 시퀀스를 나 타낸다. 검은색 사람 형상의 가상 캐릭터 B는 노치, 돌기, 스텝을 더 잘 넘을 수 있는 반면 흰색 사람 형상의 가상 캐릭터 W의 달리기 효과는 좋지 않음을 알 수 있다. 예를 들어, 흰색 사람 형상의 가상 캐릭터 W의 발은 노치 위 또는 돌기 또는 스텝 아래에 있을 수 있으며, 이는 비현실적인 애니메이션 효과를 가진다. 본 개시의 실시예에 따른 애니메이션 처리 방법은 물리적 애니메이션을 필요로 하는 모든 게임 또는 애니메이션 디자인에 적용될 수 있다. 본 개시의 실시예에 따른 애니메이션 처리 방법에 따르면, 애니메이터가 디자인한 애 니메이션 세그먼트를 시뮬레이션할 수 있다. 시뮬레이션 중에 장애물과 태스크가 가상 캐릭터에 대해 설정될 수 있다. 애니메이션 처리 모델을 사용하여 지형 특징 그리고 현재 순간의 가상 캐릭터에 대응하는 태스크 정보 및 상태 정보에 따라, 다음 순간의 가상 캐릭터에 대응하는 관절 액션 정보가 결정된다. 예를 들어, 가상 사용자가 현재 순간에 왼발을 착지하고 오른발을 들어올리고, 지형적 특징은 가상 사용자의 이동 경로에 돌기가 있는 것 이고, 태스크 정보는 속도 방향이 정방향이라는 것이므로, 애니메이션 처리 모델은 이러한 정보에 따라 다음 순 간의 가상 캐릭터의 관절 액션 정보를 출력하여, 가상 캐릭터가 복수의 순간에 액션을 수행한 후 성공적으로 돌 기를 건널 수 있도록 한다. 마지막으로, 관절 액션 정보에 따라 관절 토크를 결정하고, 물리적 엔진을 사용하여 관절 토크를 허벅지와 발에 인가하여 렌더링을 수행하여, 가상 캐릭터가 돌기를 건너는 액션을 획득한다. 본 개시의 실시예에 따른 애니메이션 처리 방법은 임의 유형의 게임 애니메이션에 적용될 수 있다. 증강 현실 게임이 예로 사용된다. 게임 장면과 실제 장면이 통합된 후 획득되는 장면의 개략도를 나타내는 도 4를 기반으 로, 게임 애니메이션에서 악령(V)은 가상 캐릭터이고, 악령이 있는 환경은 실제 장면에서 스텝(S)이며, 악령 뒤 에는 전동 스쿠터(M)이 줄지어 있다. 본 개시의 실시예에 따른 애니메이션 처리 방법에 따르면, 사용자는 악령 (V)이 스텝을 내려가거나 전동 스쿠터(M)를 우회하도록 태스크를 설정할 수 있다. 악령(V)의 상태 정보 및 태스 크 정보 및 그래픽 사용자 인터페이스의 지형 특징에 따라, 악령에 대응하는 생생한 액션 시퀀스를 획득할 수 있다. 시각 효과 면에서 악령(V)은 현재 스텝에서 다음 스텝으로 점프하거나 전동 스쿠터(M)를 성공적으로 우회 할 수 있으며, 스텝(S) 아래에 발이 나타나거나 전기 스쿠터(M)와 몸과 겹치는 경우가 일어나지 않는다. 따라서, 액션이 더욱 생생하고 환경에 대한 자기적응력이 강해진다. 본 개시의 실시예에 따른 애니메이션 처리 방법에 따르면, 애니메이션 처리 모델을 사용하여 지형 특징 그리고 매순간의 가상 캐릭터의 상태 정보 및 태스크 정보에 따라, 매순간에 인접한 다음 순간의 관절 액션 정보를 출 력하고, 관절 액션 정보에 따라 결정된 관절 토크를 물리적 엔진을 사용하여 대응하는 관절에 인가하여 렌더링 하여, 생생한 액션 시퀀스를 획득할 수 있다. 생생한 액션 시퀀스에 따라 생성되는 애니메이션의 애니메이션 효 과는 애니메이터가 디자인한 애니메이션에 비해 자연스럽고 생생하다. 또한, 처리 프로세스 동안, 서로 다른 지 형과 태스크가 추가되고, 게임에서 사용자와 가상 캐릭터 간의 상호작용이 구현되므로, 가상 캐릭터가 자기 적 응력을 가져서 가상 캐릭터가 지형을 인지하는 능력을 향상시킨다. 평지에서 가상 캐릭터가 수행하는 액션을 복 잡한 지형으로 이행하여(migrated), 게임의 재미를 향상시켜 사용자 경험을 더욱 향상시키고 게임 애니메이션의 제작 비용을 절감할 수 있다. 다음은 본 개시의 전술한 실시예의 애니메이션 처리 방법을 수행하는데 사용될 수 있는 본 개시의 장치 실시예 를 설명한다. 본 개시의 장치 실시예에서 공개되지 않은 세부 사항은 본 개시의 전술한 애니메이션 처리 방법을 참조한다. 도 17은 본 개시의 실시예에 따른 애니메이션 처리 장치의 블록도를 개략적으로 도시한 도면이다. 도 17을 참조하면, 본 개시의 실시예에 따른 애니메이션 처리 장치는 정보 획득 모듈, 모델 처리 모듈 및 제스처 조정 모듈을 포함한다. 정보 획득 모듈은 현재 순간의 그래픽 사용자 인터페이스에서 지형 특징을 획득하고, 현재 순간의 애니메 이션 세그먼트에서 가상 캐릭터에 대응하는 상태 정보 및 태스크 정보를 획득하도록 구성되며; 모델 처리 모듈 은 지형 특징, 상태 정보 및 태스크 정보를 애니메이션 처리 모델에 입력하고, 애니메이션 처리 모델을 사용하여 지형 특징, 상태 정보 및 태스크 정보에 대해 특징 추출을 수행하여, 다음 순간의 가상 캐릭터에 대응 하는 관절 액션 정보를 획득하도록 구성되고; 제스처 조정 모듈은 관절 액션 정보에 따라 관절 토크를 결 정하고, 관절 토크에 기반하여 렌더링을 수행하여, 현재 순간의 가상 캐릭터에 대응하는 제스처 조정 정보를 획 득하고, 제스처 조정 정보에 따라 애니메이션 세그먼트를 처리하도록 구성된다. 본 개시의 실시예에서, 애니메이션 처리 장치는 추가로, 현재 순간이 애니메이션 세그먼트의 초기 순간인 경우, 애니메이션 세그먼트의 초기 순간의 가상 캐릭터의 제스처 정보에 따라 상태 정보를 결정하고: 현재 순간 이 애니메이션 세그먼트의 초기 순간이 아닌 경우, 이전 순간의 가상 캐릭터에 대응하는 관절 액션 정보에 따라 상태 정보를 결정하도록 구성된다. 본 개시의 실시예에서, 애니메이션 처리 장치는 추가로, 애니메이션 세그먼트에 기반하여 복수의 순간에 가상 캐릭터에 대응하는 제스처 조정 정보를 획득하고; 그리고 복수의 순간에서 제스처 조정 정보에 따라 타깃 액션 시퀀스를 결정하도록 구성된다. 본 개시의 실시예에서, 지형 특징은 자체 정의(self-defined) 지형의 특징 또는 실제 지형의 특징이고; 상태 정 보는 가상 캐릭터의 각 관절의 제스처, 속도 및 위상을 포함하고; 태스크 정보는 가상 캐릭터에 대응하는 타깃 속도 방향 또는 타깃 지점 좌표를 포함한다. 본 개시의 실시예에서, 애니메이션 처리 모델은 제1 제어 네트워크 및 제2 제어 네트워크를 포함한다. 모델 처 리 모듈은 지형 특징, 상태 정보 및 태스크 정보를 제1 제어 네트워크에 입력하고, 제1 제어 네트워크를 사용하여 지형 특징, 상태 정보 및 태스크에 대해 특징 추출을 수행하여 주요 관절에 대응하는 타깃 상태 정보 를 획득하도록 구성된 제1 특징 추출 유닛; 및 타깃 상태 정보를 타깃 태스크 정보로 사용하고, 상태 정보 및 타깃 태스크 정보를 제2 제어 네트워크에 입력하며, 제2 제어 네트워크를 사용하여 상태 정보 및 타깃 태스크 정보에 대해 특징 추출을 수행하여 관절 액션 정보를 획득하도록 구성된 제2 특징 추출 유닛을 포함한다. 본 개시의 실시예에서, 제1 제어 네트워크는 컨볼루션 유닛, 제1 완전 연결 계층, 제2 완전 연결 계층, 및 제3 완전 연결 계층을 포함한다. 제1 특징 추출 유닛은: 컨볼루션 유닛을 사용하여 지형 특징에 대해 특징 추출을 수행하여 지형에 대응하는 제1 특징 정보를 획득하고; 제1 완전 연결 계층을 사용하여 제1 특징 정보에 대해 특 징 조합을 수행하여 제2 특징 정보를 획득하며; 제2 완전 연결 계층을 사용하여 제2 특징 정보, 상태 정보 및 태스크 정보에 대해 특징 조합을 수행하여 제3 특징 정보를 획득하고; 제3 완전 연결 계층을 사용하여 제3 특징 정보에 대해 특징 조합을 수행하여 타깃 상태 정보를 획득하도록 구성된다. 본 개시의 실시예에서, 제2 제어 네트워크는 제4 완전 연결 계층 및 제5 완전 연결 계층을 포함한다. 제2 특징 추출 유닛은: 제4 완전 연결 계층을 사용하여 상태 정보 및 타깃 태스크 정보에 대해 특징 조합을 수행하여 제4 특징 정보를 획득하고; 그리고 제5 완전 연결 계층을 사용하여 제4 특징 정보에 대해 특징 조합을 수행하여 관 절 액션 정보를 획득하도록 구성된다. 본 개시의 실시예에서, 제스처 조정 모듈은: 관절 액션 정보에 따라 관절의 현재 위치 및 타깃 위치를 결 정하고; 현재 위치에 따라 관절의 현재 속도 및 현재 가속도를 결정하며, 타깃 위치에 따라 관절의 타깃 속도를 결정하고; 현재 속도 및 현재 가속도에 따라 다음 제어 주기 후 관절에 대응하는 제1 위치 및 제1 속도를 결정 하며; 비례 계수, 미분 이득 계수, 현재 위치, 타깃 위치, 타깃 속도, 제1 위치, 및 제1 속도에 따라 관절 토크 를 계산하도록 구성된다. 본 개시의 실시예에서, 제스처 조정 모듈은 물리적 엔진에 관절 토크를 입력하고, 물리적 엔진을 사용하 여 관절 토크를 대응하는 관절에 인가하며, 렌더링을 수행하여 제스처 조정 정보를 생성하도록 구성된다. 본 개시의 실시예에서, 애니메이션 처리 장치는: 애니메이션 처리 모델을 사용하여 지형 특징, 상태 정보 및 태스크 정보에 대해 특징 추출을 수행하기 전에, 훈련 대상 애니메이션 처리 모델을 훈련하여 애니메이션 처 리 모델을 획득하도록 구성된 훈련 모듈을 더 포함한다. 본 개시의 실시예에서, 훈련 대상 애니메이션 처리 모델은 훈련될 제1 제어 네트워크 및 훈련될 제2 제어 네트 워크를 포함하고; 훈련 모듈은 지형 특징 샘플, 캐릭터 상태 샘플 및 태스크 정보 샘플을 획득하고, 지형 특징 샘플, 캐릭터 상태 샘플 및 태스크 정보 샘플에 따라 훈련될 제1 제어 네트워크를 훈련하여, 제1 제어 네트워크 를 획득하도록 구성된 제1 훈련 유닛; 및 가상 캐릭터의 주요 관절에 대응하는 상태 정보 샘플 및 애니메이션 세그먼트 샘플의 모든 관절에 대응하는 관절 액션 정보 샘플에 따라 제2 제어 네트워크를 훈련하여 제2 제어 네 트워크를 획득하도록 구성된 제2 훈련 유닛을 포함하고, 훈련될 제1 제어 네트워크와 훈련될 제2 제어 네트워크 는 별도로 훈련되며; 그리고 훈련될 제1 제어 네트워크가 훈련되는 경우, 훈련될 제1 제어 네트워크는 고정된 파라미터로 제2 제어 네트워크에 연결된다. 본 개시의 실시예에서, 제2 훈련 유닛은 복수의 애니메이션 세그먼트 샘플을 획득하고, 가상 캐릭터의 초기 제 스처에 따라 복수의 애니메이션 세그먼트 샘플로부터 타깃 애니메이션 세그먼트 샘플을 결정하고; 타깃 애니메 이션 세그먼트 샘플에서 주요 관절에 대응하는 상태 정보 샘플을 획득하고, 상태 정보 샘플을 타깃 태스크 정보 로 사용하며; 가상 캐릭터의 모든 관절에 대응하는 관절 액션 정보 샘플을 획득하고; 그리고 타깃 태스크 정보 및 관절 액션 정보 샘플에 따라 훈련될 제2 제어 네트워크를 훈련하도록 구성된다. 본 개시의 실시예에서, 훈련될 제1 제어 네트워크는 제1 훈련 대상 액터 서브 네트워크 및 제1 훈련 대상 크리 틱 서브 네트워크를 포함하고, 훈련될 제2 제어 네트워크는 제2 훈련 대상 액터 서브 네트워크와 제2 훈련 대상 크리틱 서브 네트워크를 포함하며, 여기서 제1 훈련 대상 액터 서브 네트워크의 구조는 제1 훈련 대상 크리틱 서브 네트워크의 구조와 동일하며, 제2 훈련 대상 액터 서브 네트워크의 구조는 제2 훈련 대상 크리틱 서브 네 트워크의 구조와 동일하다. 도 18은 본 개시의 실시예들을 구현하도록 적응된 전자 디바이스의 컴퓨터 시스템의 개략적인 구조도를 도시한 다. 도 18에 도시된 전자 디바이스의 컴퓨터 시스템은 예시일 뿐, 본 개시의 실시 예의 기능 및 사용 범위를 제한하지 않는다. 도 18에 도시된 바와 같이, 컴퓨터 시스템은 중앙 처리 유닛(central processing unit, CPU)을 포 함하며, 이는 읽기 전용 메모리(read-only memory, ROM)에 저장된 프로그램 또는 저장부(storage part)로부터 랜덤 액세스 메모리(random access memory, RAM)에 로딩된 프로그램에 따라 다양한 적절한 액션 또는 처리를 수행하여, 전술한 실시예의 애니메이션 처리 방법을 구현한다. RAM은 추가로, 시스템 운영에 필요한 다양한 프로그램 및 데이터를 저장한다. CPU, ROM, 및 RAM은 버스 를 통해 서로 연결된다. 입/출력(I/O) 인터페이스는 또한 버스에 연결된다. 키보드, 마우스 등을 포함하는 입력부(input part), 음극선관(cathode ray tube, CRT), 액정 디스플레이 (liquid crystal display, LCD), 스피커 등을 포함하는 출력부, 하드 디스크 등을 포함하는 저장부 , 및 근거리 통신망(local area network, LAN) 카드 또는 모뎀과 같은 네트워크 인터페이스 카드를 포함 하는 통신부를 포함하는 구성 요소가 I/O 인터페이스에 연결된다. 통신부는 인터넷 등의 네 트워크를 사용하여 통신 처리를 행한다. 드라이브(drive)는 또한 필요에 따라 I/O 인터페이스에 연 결된다. 자기 디스크, 광 디스크, 광자기 디스크, 반도체 메모리 등의 제거가능한 매체(removable medium)가 필요에 따라 드라이브에 설치되므로, 제거가능한 매체에서 읽어 들인 컴퓨터 프로그램이 필요에 따라 저장부에 설치된다. 특히, 본 개시의 실시예에 따르면, 흐름도를 참조하여 후술하는 프로세스들이 컴퓨터 소프트웨어 프로그램으로 구현될 수 있다. 예를 들어, 본 개시의 이 실시예는 컴퓨터 프로그램 제품을 포함하고, 컴퓨터 프로그램 제품은 컴퓨터가 판독 가능한 매체에서 운반되는 컴퓨터 프로그램을 포함하고, 컴퓨터 프로그램은 흐름도에 도시된 방 법을 수행하기 위해 사용되는 프로그램 코드를 포함한다. 이러한 실시예에서, 통신부를 이용함으로써, 컴 퓨터 프로그램은 네트워크로부터 다운로드 및 설치될 수 있거나 및/또는 제거가능한 매체로부터 설치될 수 있다. 컴퓨터 프로그램이 CPU에 의해 실행될 때, 본 출원의 시스템에 정의된 다양한 기능이 실행된다. 본 개시의 실시예에서 컴퓨터가 판독 가능한 매체는 컴퓨터가 판독 가능한 신호 매체 또는 컴퓨터가 판독 가능 한 저장 매체 또는 이들의 임의의 조합일 수 있다. 컴퓨터가 판독 가능한 저장 매체는 예를 들어 전기, 자기, 광학, 전자기, 적외선 또는 반도체 시스템, 장치, 또는 컴포넌트, 또는 이들의 임의의 조합일 수 있지만 이에제한되지는 않는다. 컴퓨터가 판독 가능한 저장 매체는 하나 이상의 와이어를 갖는 전기적 연결, 휴대용 컴퓨터 자기 디스크, 하드 디스크, 랜덤 액세스 메모리(random access memory, RAM), 읽기 전용 메모리(read-only memory, ROM), 지울 수 있는 프로그램 가능 읽기 전용 메모리(erasable programmable read-only memory, EPROM), 플래시 메모리, 광섬유, 컴팩트 디스크 읽기 전용 메모리(compact disk read-only memory, CD-ROM), 광학 저장 디바이스, 자기 저장 디바이스, 또는 이들의 임의의 적절한 조합을 포함할 수 있으며, 이에 한정되지 않는다. 본 개시에서, 컴퓨터가 판독 가능한 저장 매체는 프로그램을 포함하거나 저장하는 유형의 매체라면 어 떠한 것이든 될 수 있으며, 프로그램은 명령 실행 시스템, 장치 또는 디바이스에 의해 사용되거나 조합되어 사 용될 수 있다. 본 개시에서, 컴퓨터가 판독 가능한 신호 매체는 기저대역에 있거나 반송파의 일부로서 전파되는 데이터 신호를 포함할 수 있고, 데이터 신호는 컴퓨터가 판독 가능한 프로그램 코드를 운반한다. 이러한 방식으 로 전파되는 데이터 신호는 전자기 신호, 광 신호, 또는 이들의 임의의 적절한 조합을 포함하지만 이에 제한되 지 않는 복수의 형태를 취할 수 있다. 컴퓨터가 판독 가능한 신호 매체는 컴퓨터가 판독 가능한 저장 매체 외에 컴퓨터가 판독 가능한 임의의 매체를 더 포함할 수 있다. 컴퓨터가 판독 가능한 매체는 명령 실행 시스템, 장치 또는 디바이스에 의해 사용되거나 이와 함께 사용되는 프로그램을 전송, 전파 또는 전송할 수 있다. 컴퓨터가 판독 가능한 매체에 포함된 프로그램 코드는 무선 매체, 유선 등, 또는 이들의 임의의 적절한 조합을 포함하나 이에 제한되지 않는 임의의 적절한 매체를 사용하여 전송될 수 있다. 첨부 도면의 흐름도 및 블록도는 본 개시의 다양한 실시예에 따른 시스템, 방법, 및 컴퓨터 프로그램 제품에 의 해 구현될 수 있는 가능한 시스템 아키텍처, 기능 및 작동을 예시한다. 이와 관련하여 흐름도 또는 블록도의 각 박스(box)는 모듈, 프로그램 세그먼트 또는 코드의 일부를 나타낼 수 있다. 모듈, 프로그램 세그먼트 또는 코드 의 일부는 지정된 논리 기능을 구현하는 데 사용되는 하나 이상의 실행 가능한 명령을 포함한다. 대안으로 사용 되는 일부 구현에서, 박스에 주석이 달린 기능은 다르게는 첨부 도면에 주석이 달린 것과 상이한 순서로 발생할 수 있다. 예를 들어, 실제로 연속적으로 도시된 두 개의 박스는 기본적으로 병렬로 수행될 수 있고, 때로는 두 개의 박스가 역순으로 수행될 수 있다. 이것은 관련 기능에 의해 결정된다. 블록도 및/또는 흐름도의 각 박스와 블록도 및/또는 흐름도의 박스 조합은 지정된 기능 또는 작동을 수행하도록 구성된 전용 하드웨어 기반 시스템 을 사용하여 구현될 수 있거나 또는 전용 하드웨어와 컴퓨터 명령의 조합을 사용하여 구현될 수 있다. 본 개시의 실시예에서 설명되는 관련 유닛은 소프트웨어적으로 구현될 수도 있고, 하드웨어적으로 구현될 수도 있으며, 설명된 유닛은 프로세서에 설정될 수도 있다. 유닛의 이름은 특정 경우 유닛에 대한 제한을 구성하지 않는다. 다른 측면에 따르면, 본 개시는 컴퓨터가 판독 가능한 매체를 더 제공한다. 컴퓨터가 판독 가능한 매체는 전술 한 실시예에서 설명한 애니메이션 처리 장치에 포함될 수도 있고, 단독으로 존재하고 전자 디바이스에 배치되지 않을 수도 있다. 컴퓨터가 판독 가능한 매체는 하나 이상의 프로그램을 운반하며, 하나 이상의 프로그램은 전자 디바이스에 의해 실행될 때, 전자 디바이스로 하여금 전술한 실시예에서 설명된 방법을 구현하게 한다. 전술한 상세한 설명에서 액션을 수행하도록 구성된 디바이스의 복수의 모듈 또는 유닛이 논의되었지만, 그러한 구분이 필수는 아니다. 실제로, 본 개시의 구현예에 따르면, 상술한 둘 이상의 모듈 또는 유닛의 특징 및 기능 은 하나의 모듈 또는 유닛으로 구현될 수 있다. 반대로, 상술한 하나의 모듈 또는 유닛의 특징 및 기능은 더 분 할되어 복수의 모듈 또는 유닛으로 구현될 수 있다. 구현에 대한 전술한 설명에 따르면, 당업자는 여기에 설명된 예시적인 구현이 소프트웨어를 사용하여 구현될 수 있거나, 소프트웨어와 필요한 하드웨어를 결합하여 구현될 수 있음을 쉽게 이해할 수 있다. 따라서, 본 개시의 실시 예에 따른 기술 솔루션은 소프트웨어 제품의 형태로 구현될 수 있다. 소프트웨어 제품은 비휘발성 저장 매 체(CD-ROM, USB 플래시 드라이브, 이동식 하드 디스크 등일 수 있음) 또는 네트워크상에 저장되며, 컴퓨팅 디바 이스(개인 컴퓨터, 서버, 터치 단말, 네트워크 디바이스 등일 수 있음)가 본 개시의 실시 예에 따른 방법을 수 행하게 하는 여러 명령을 포함한다. 본 개시의 다른 실시예는 여기에서의 개시의 명세서 및 실시를 고려함으로써 당업자에게 명백할 것이다. 본 개 시는 본 개시의 임의의 변형, 사용 또는 적응적 변경을 포함하도록 의도된다. 이러한 변형, 사용 또는 적응적 변경은 본 개시의 일반적인 원칙을 따르며, 본 개시에서 개시하지 않은 일반적인 상식 또는 기술적인 수단을 기 술에 포함한다. 본 개시는 상기에서 설명되고 첨부된 도면에 도시된 정확한 구조에 한정되지 않으며, 본 개시의 범위를 벗어나 지 않고 다양한 수정 및 변경이 이루어질 수 있음을 이해해야 한다. 본 개시의 범위는 첨부된 청구범위에 의해 서만 제한된다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15a 도면15b 도면15c 도면15d 도면15e 도면16a 도면16b 도면16c 도면16d 도면16e 도면16f 도면16g 도면16h 도면16i 도면16j 도면16k 도면16l 도면17 도면18"}
{"patent_id": "10-2022-7002917", "section": "도면", "subsection": "도면설명", "item": 1, "content": "여기에서 첨부된 도면은 본 명세서에 통합되어 본 명세서의 일부를 구성하고, 본 개시에 부합하는 실시예를 도 시하며, 본 명세서와 함께 본 개시의 원리를 설명하기 위해 사용된다. 명백하게, 후술하는 첨부된 도면은 본 개 시의 일부 실시예에 불과하며, 본 기술 분야에서 통상의 지식을 가진 자라면 창의적인 노력 없이도 첨부된 도면 에 따라 다른 첨부된 도면을 더 획득할 수 있을 것이다. 첨부된 도면에서: 도 1은 본 개시의 실시예의 기술적 솔루션이 적용될 수 있는 예시적인 시스템 아키텍처의 개략도이다. 도 2는 종래의 스킨된 애니메이션(skinned animation)에서 가상 캐릭터의 구성 구조를 개략적으로 도시한다. 도 3은 본 개시의 실시예에 따른 애니메이션 처리 방법의 개략적인 흐름도를 개략적으로 도시한다. 도 4는 본 개시의 실시예에 따라 게임 장면과 실제 장면이 통합된 후 획득되는 장면의 개략도를 개략적으로 도 시한다. 도 5는 본 개시의 실시예에 따른 조밀한 노치 지형(densely-notched terrain)의 인터페이스의 개략도를 개략적 으로 도시한다. 도 6은 본 개시의 실시예에 따른 하이브리드 장애물 지형(hybrid-obstacle terrain)의 인터페이스의 개략도를 개략적으로 도시한다. 도 7은 본 개시의 실시예에 따른 사람 형상의 캐릭터의 걷기 액션의 제1 프레임의 액션 정보를 개략적으로 도시 한다. 도 8은 본 개시의 실시예에 따른 지형의 인터페이스의 개략도를 개략적으로 도시한다. 도 9는 본 개시의 실시예에 따른 애니메이션 처리 모델의 개략적인 구조도를 개략적으로 도시한다. 도 10은 본 개시의 실시예에 따른 제1 제어 네트워크의 개략적인 구조도를 개략적으로 도시한다. 도 11은 본 개시의 실시예에 따른 제2 제어 네트워크의 개략적인 구조도를 개략적으로 도시한다. 도 12는 본 개시의 실시예에 따른 강화 학습의 개략적인 흐름도를 개략적으로 도시한다. 도 13은 본 개시의 실시예에 따른 애니메이션 처리 모델의 알고리즘 프레임워크의 아키텍처 다이어그램을 개략 적으로 도시한다. 도 14는 본 개시의 실시예에 따른, 애니메이션 처리 모델에 의해 제어되고 평지(flat ground)를 달리는 가상 캐 릭터의 액션 시퀀스를 개략적으로 도시한다. 도 15a 내지 도 15e는 본 개시의 실시예에 따른 조밀한 노치 지형을 달리는 사람 형상의 가상 캐릭터의 액션 시 퀀스를 개략적으로 도시한다.도 16a 내지 도 16l은 본 개시의 실시예에 따른 하이브리드 장애물 지형을 달리는 사람 형상의 가상 캐릭터의 액션 시퀀스를 개략적으로 도시한다. 도 17은 본 개시의 실시예에 따른 애니메이션 처리 장치의 블록도를 개략적으로 도시한다. 도 18은 본 개시의 실시예를 구현하도록 적응된 전자 디바이스의 컴퓨터 시스템의 개략적인 구조도를 도시한다."}
