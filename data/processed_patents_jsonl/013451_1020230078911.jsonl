{"patent_id": "10-2023-0078911", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0177490", "출원번호": "10-2023-0078911", "발명의 명칭": "미학습 화자의 음성을 합성하는 방법 및 장치", "출원인": "주식회사 자이냅스", "발명자": "주동원"}}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "미학습 발화 정보를 획득하는 단계; 및상기 미학습 발화 정보를 이용하여 미학습 발화 임베딩 벡터를 생성하는 단계;를 포함하고,상기 생성하는 단계는, 전이 학습(Transfer Learning)된 제1 인코더에 의하여 수행되고,상기 제1 인코더는 사전 학습(Pre-trained)된 제2 인코더로부터 출력된 정보를 이용하여 전이 학습되는, 미학습화자의 음성을 합성하는 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 제1 인코더는, 발화 정보를 적어도 하나의 음성 프레임으로 분할하고, 상기 음성 프레임에 포함된 음성의 특징을 추출하여 상기 음성 프레임 각각에 대한 피처 벡터를 생성하고, 상기 피처 벡터를 임베딩 공간에 매핑하기 위하여 상기 피처 벡터 각각에 대한 프레임 임베딩 벡터를 생성하는 것인, 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 전이 학습은,기 설정된 학습 데이터베이스로부터 획득된 소정의 발화 정보를 상기 제1 인코더에 입력하여 상기 소정의 발화정보가 분할된 적어도 하나의 음성 프레임 각각에 대한 피처 벡터를 획득하는 단계;상기 피처 벡터를 상기 제1 인코더에 입력하여 상기 피처 벡터 각각에 대한 프레임 임베딩 벡터를 획득하는 단계; 및상기 소정의 발화 정보에 대응하여 상기 제2 인코더가 출력한 타겟 임베딩 벡터, 상기 프레임 임베딩 벡터 및상기 피처 벡터 중 적어도 하나를 이용하여, 상기 제1 인코더의 파라미터를 교정하는 단계;를 포함하여 수행되는, 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 파라미터를 교정하는 단계는,지각 손실함수(Perceptual Loss) 및 삼중항 손실함수(Triplet loss) 중 적어도 하나를 이용하여 상기 파라미터를 교정하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서,상기 지각 손실함수 및 삼중항 손실함수 중 적어도 하나를 이용하여 상기 파라미터를 교정하는 단계는,상기 타겟 임베딩 벡터, 상기 프레임 임베딩 벡터 및 상기 지각 손실함수에 기초하여 제1 손실을 계산하는단계; 및공개특허 10-2024-0177490-3-상기 계산된 제1 손실에 기초하여 상기 파라미터를 교정하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 4 항에 있어서,상기 지각 손실함수 및 삼중항 손실함수 중 적어도 하나를 이용하여 상기 파라미터를 교정하는 단계는,상기 소정의 발화 정보와 상이한 비교 발화 정보를 상기 제1 인코더에 입력하여 적어도 하나의 비교 피처 벡터를 획득하는 단계;상기 비교 피처 벡터 중 선택된 하나의 비교 피처 벡터, 상기 소정의 발화 정보에 기초하여 생성된 상기 피처벡터 중 선택된 두 개의 피처 벡터 및 삼중항 손실함수에 기초하여 제2 손실을 계산하는 단계; 및상기 계산된 제2 손실에 기초하여 상기 파라미터를 교정하는 단계;를 포함하되,상기 비교 발화 정보는 상기 데이터베이스로부터 획득된 것이고,상기 비교 발화 정보에 대응하는 제1 화자는, 상기 소정의 발화 정보에 대응하는 제2 화자와 상이한, 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 3 항에 있어서,상기 피처 벡터를 획득하는 단계는,상기 음성 프레임 중에서, 음성이 존재하는 프레임에 대응하는 상기 피처 벡터를 획득하는, 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서,상기 미학습 발화 임베딩 벡터를 생성하는 단계는,상기 미학습 발화 정보에 기초하여 상기 미학습 발화 정보가 분할된 적어도 하나의 음성 프레임 각각에 대한 피처 벡터를 생성하는 단계; 및상기 피처 벡터에 기초하여 상기 미학습 발화 임베딩 벡터를 생성하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8 항에 있어서,상기 피처 벡터에 기초하여 상기 미학습 발화 임베딩 벡터를 생성하는 단계는,상기 피처 벡터에 기초하여 상기 피처 벡터의 수와 동일한 수의 프레임 임베딩 벡터를 생성하는 단계; 및상기 프레임 임베딩 벡터에 기초하여 상기 미학습 발화 임베딩 벡터를 생성하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1 항에 있어서,상기 미학습 발화 임베딩 벡터를 생성하는 단계는,입력 발화 정보가 상기 제2 인코더의 학습 데이터베이스에 포함되는 것인지 판단하는 단계; 및상기 입력 발화 정보가 상기 데이터베이스에 포함되지 않는 경우, 상기 입력 발화 정보를 상기 미학습 발화 정보로 하여, 상기 미학습 발화 임베딩 벡터를 생성하고, 공개특허 10-2024-0177490-4-상기 입력 발화 정보가 상기 데이터베이스에 포함되는 경우, 상기 입력 발화 정보에 기초하여 학습 발화 임베딩벡터를 생성하는 단계;를 포함하되,상기 미학습 발화 임베딩 벡터를 생성하는 단계는, 상기 제1 인코더에 의해 수행되고,상기 학습 발화 임베딩 벡터를 생성하는 단계는, 상기 제2 인코더에 의해 수행되는, 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 1 항에 있어서,특정 자연 언어로 구성된 텍스트 및 상기 미학습 발화 임베딩 벡터에 기초하여 음성 신호를 생성하는 단계;를 더 포함하는, 방법."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "적어도 하나의 프로그램이 저장된 메모리; 및상기 적어도 하나의 프로그램을 실행함으로써 동작하는 프로세서;를 포함하되,상기 프로세서는,미학습 발화 정보를 획득하고, 상기 미학습 발화 정보를 이용하여 미학습 발화 임베딩 벡터를 생성하되, 상기생성은, 전이 학습(Transfer Learning)된 제1 인코더에 의하여 수행되고, 상기 제1 인코더는 사전 학습(Pre-trained)된 제2 인코더로부터 출력된 정보를 이용하여 전이 학습되는, 미학습 화자의 음성을 합성하는 장치."}
{"patent_id": "10-2023-0078911", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 1 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2023-0078911", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 미학습 화자의 음성을 합성하는 방법 및 장치를 제공한다. 본 개시의 일 실시예에 따르면, 미학습 발 화 정보를 획득하는 단계; 및 상기 미학습 발화 정보를 이용하여 미학습 발화 임베딩 벡터를 생성하는 단계;를 포함하고, 상기 생성하는 단계는, 전이 학습(Transfer Learning)된 제1 인코더에 의하여 수행되고, 상기 제1 인 코더는 사전 학습(Pre-trained)된 제2 인코더로부터 출력된 정보를 이용하여 전이 학습되는, 미학습 화자의 음성 을 합성하는 방법을 제공할 수 있다."}
{"patent_id": "10-2023-0078911", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 미학습 화자의 음성을 합성하는 방법 및 장치에 관한 것이다. 보다 자세하게는, 전이 학습된 인코더 에 의해 미학습 발화 임베딩 벡터를 생성함으로써 미학습 화자의 음성을 합성하는 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0078911", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 인공 지능 기술의 발달로 음성 신호를 활용하는 인터페이스가 보편화되고 있다. 이에 따라, 주어진 상황에 따라 합성된 음성을 발화할 수 있도록 하는 음성 합성(speech synthesis) 기술에 대한 연구가 활발히 진행되고 있다. 음성 합성 기술은 인공 지능에 기반한 음성 인식 기술과 접목하여 가상 비서, 오디오북, 자동 통번역 및 가상 성우 등의 많은 분야에 적용되고 있다. 종래의 음성 합성 방법으로는 연결 합성(Unit Selection Synthesis, USS) 및 통계 기반 파라미터 합성(HMM- based Speech Synthesis, HTS) 등의 다양한 방법이 있다. USS 방법은 음성 데이터를 음소 단위로 잘라서 저장하 고 음성 합성 시 발화에 적합한 음편을 찾아서 이어붙이는 방법이고, HTS 방법은 음성 특징에 해당하는 파라미 터들을 추출해 통계 모델을 생성하고 통계 모델에 기반하여 텍스트를 음성으로 재구성하는 방법이다. 그러나, 상술한 종래의 음성 합성 방법은 화자의 발화 스타일 또는 감정 표현 등을 반영한 자연스러운 음성을 합성하는 데 많은 한계가 있었다. 이에 따라, 최근에는 인공 신경망(Artificial Neural Network)에 기반하여 텍스트로부터 음성을 합성하는 음성 합성 방법이 주목받고 있다. 인공 신경망 기반의 다화자 음성 합성 시스템은 학습된 화자의 발화 특징을 모방하 여 음성을 합성한다.일반적인 다화자 음성 합성 시스템에서 학습데이터에 포함되지 않은 미학습 화자의 목소리로 음성을 합성하려면, 인코더 모델의 가중치를 확장하고, 미학습 화자에 대한 학습 데이터셋을 구축한 후, 파인 튜닝 (Fine-tuning)을 통한 추가 학습이 진행되어야 한다. 그러나, 모델의 가중치를 확장하고 새로운 학습 데이터셋을 구축하는 것은 많은 비용을 요하며, 파인 튜닝 단계 에서 기존에 학습된 화자의 음성 생성 품질이 하락할 가능성이 존재하는 바, 기 학습된 화자에 대한 합성 음성 품질을 유지하면서 미학습 화자의 음성을 합성하는 기술의 필요성이 존재하였다."}
{"patent_id": "10-2023-0078911", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 미학습 화자의 음성을 합성하는 방법 및 장치를 제공한다. 본 개시가 해결하고자 하는 과제는 이상에 서 언급한 과제에 한정되지 않으며, 언급되지 않은 본 개시의 다른 과제 및 장점들은 하기의 설명에 의해서 이 해될 수 있고, 본 개시의 실시 예에 의해보다 분명하게 이해될 것이다. 또한, 본 개시가 해결하고자 하는 과제 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 알 수 있을 것이다."}
{"patent_id": "10-2023-0078911", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본 개시의 제1 측면은, 미학습 발화 정보를 획득하는 단계; 및 상기 미학습 발화 정보를 이용하여 미학습 발화 임베딩 벡터를 생성하는 단계;를 포함하고, 상기 생성 하는 단계는, 전이 학습(Transfer Learning)된 제1 인코더에 의하여 수행되고, 상기 제1 인코더는 사전 학습 (Pre-trained)된 제2 인코더로부터 출력된 정보를 이용하여 전이 학습되는, 미학습 화자의 음성을 합성하는 방 법을 제공할 수 있다. 본 개시의 제2 측면은, 적어도 하나의 프로그램이 저장된 메모리; 및 상기 적어도 하나의 프로그램을 실행함으 로써 동작하는 프로세서;를 포함하고, 상기 프로세서는, 미학습 발화 정보를 획득하고, 상기 미학습 발화 정보 를 이용하여 미학습 발화 임베딩 벡터를 생성하되, 상기 생성은, 전이 학습(Transfer Learning)된 제1 인코더에 의하여 수행되고, 상기 제1 인코더는 사전 학습(Pre-trained)된 제2 인코더로부터 출력된 정보를 이용하여 전이 학습되는, 미학습 화자의 음성을 합성하는 장치를 제공할 수 있다. 본 개시의 제3 측면은, 본 개시의 제1 측면의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체를 제공할 수 있다. 전술한 것 외의 다른 측면, 특징, 이점이 이하의 도면, 특허청구범위 및 발명의 상세한 설명으로부터 명확해질 것이다."}
{"patent_id": "10-2023-0078911", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본 개시의 과제 해결 수단에 의하면, 인코더의 기존 학습데이터에 포함되지 않은 화자의 목소리로 음성 을 합성이 가능하여, 합성 가능한 화자의 수를 증가시킬 수 있다. 또한, 본 개시의 다른 과제 해결 수단에 의하면, 파인 튜닝 단계를 생략할 수 있으므로 경제적으로 합성 가능한 화자의 수를 증가시킬 수 있다. 또한, 본 개시의 다른 과제 해결 수단에 의하면, 음성 합성 시스템의 재활용성이 높아지며, 음성 합성 모델의 버전 관리가 용이해질 수 있다."}
{"patent_id": "10-2023-0078911", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시는 다양하게 변환하여 실시할 수 있고, 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 기재한다. 본 개시의 효과 및 특징, 그리고 그것들을 달성하는 방법은 도면과 함께 상 세하게 후술되어 있는 실시예들을 참조해 명확해질 것이다. 그러나 본 개시는 이하에서 개시되는 실시예들에 한정되는 것이 아니라 다양한 형태로 구현될 수 있다. 이하의 실시예에서, 제1, 제2 등의 용어는 한정적인 의미가 아니라 하나의 구성요소를 다른 구성요소와 구별하는 목적 으로 사용되었다. 예컨대, 본 개시의 권리 범위를 벗어나지 않으면서 먼저 서술한 제1 구성요소는 그 뒤에 제2 구성요소로 서술되는 경우가 있을 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 서술될 수 있다. 또한, 단 수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 또한, 포함하다 또는 가지다 등 의 용어는 명세서상에 기재된 특징 또는 구성요소가 존재함을 의미하는 것이고, 하나 이상의 다른 특징 또는 구 성요소가 부가될 가능성을 미리 배제하는 것은 아니다. 또한, 도면에서는 설명의 편의를 위하여 구성 요소들이 그 크기가 과장 또는 축소될 수 있다. 예컨대, 도면에서 나타난 각 구성의 크기 및 두께는 편의를 위하여 임의로 나타낸 것으로, 본 개시는 반드시 도시된 바에 한정되 지 않는다. 이하, 첨부된 도면을 참조하여 본 개시의 실시예들을 상세히 설명하기로 하며, 도면을 참조하여 설명할 때, 동 일하거나 대응하는 구성 요소는 동일한 도면 부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 도 1은 음성 합성 시스템의 동작을 개략적으로 나타내는 도면이다. 음성 합성(Speech Synthesis) 시스템은 텍스트를 사람의 음성으로 변환하는 시스템이다. 예를 들어, 도 1의 음성 합성 시스템은 인공 신경망(Artificial Neural Network) 기반의 음성 합성 시스 템일 수 있다. 인공 신경망은 시냅스의 결합으로 네트워크를 형성한 인공 뉴런이 학습을 통해 시냅스의 결합 세 기를 변화시켜, 문제 해결 능력을 가지는 모델 전반을 의미한다. 음성 합성 시스템은 PC(personal computer), 서버 디바이스, 모바일 디바이스, 임베디드 디바이스 등의 다 양한 종류의 디바이스들로 구현될 수 있고, 구체적인 예로서 인공 신경망을 이용하여 음성 합성을 수행하는 스 마트폰, 태블릿 디바이스, AR(Augmented Reality) 디바이스, IoT(Internet of Things) 디바이스, 자율주행 자 동차, 로보틱스, 의료기기, 전자책 단말기 및 네비게이션 등에 해당될 수 있으나, 이에 제한되지 않는다. 나아가서, 음성 합성 시스템은 위와 같은 디바이스에 탑재되는 전용 하드웨어 가속기(HW accelerator)에 해당될 수 있다. 또는, 음성 합성 시스템은 인공 신경망의 구동을 위한 전용 모듈인 NPU(neural processing unit), TPU(Tensor Processing Unit), Neural Engine 등과 같은 하드웨어 가속기일 수 있으나, 이 에 제한되지 않는다. 도 1을 참고하면, 음성 합성 시스템은 텍스트 입력과 특정 발화 정보를 수신할 수 있다. 예를 들어, 음성 합성 시스템은 텍스트 입력으로써 도 1에 도시된 바와 같이 \"Have a good day!\"를 수신할 수 있고, 발화 정보 입력으로써 \"화자 1\"을 수신할 수 있다. “화자 1\"은 기 설정된 화자 1의 발화 특징을 나타내는 음성 신호 또는 음성 샘플에 해당할 수 있다. 예를 들어, 음성 합성 시스템은 화자 1의 발화 특징을 나타내는 음성 신호 또는 음성 샘플을 “화자 1”로 수신 할 수 있다. 다른 예를 들어, 음성 합성 시스템은 화자 1의 발화 특징에 대응하는 적어도 하나의 정형 데 이터 변수를 수신하고, 수신에 대한 응답으로 데이터 베이스에 미리 저장된 발화 정보 중에서 화자 1의 발화 특 징을 나타내는 음성 신호 또는 음성 샘플을 선택할 수 있다. 일 실시예에 따르면, 발화 정보는 음성 합성 시스템에 포함된 통신부를 통해 외부 장치로부터 수신될 수 있다. 또는, 발화 정보는 음성 합성 시스템의 사용자 인터페이스를 통해 사용자로부터 입력될 수 있고, 음성 합성 시스템 의 데이터 베이스에 미리 저장된 다양한 발화 정보들 중 하나로 선택될 수도 있으나, 이에 제한되는 것은 아니다. 음성 합성 시스템은 입력으로 수신한 텍스트 입력과 특정 발화 정보에 기초하여 음성(speech)를 출력할 수 있다. 예를 들어, 음성 합성 시스템은 \"Have a good day!\" 및 \"화자 1\"을 입력으로 수신하여, 화자 1의 발 화 특징이 반영된 \"Have a good day!\"에 대한 음성을 출력할 수 있다. 화자 1의 발화 특징은 화자 1의 음성, 운 율, 음높이 및 감정 등 다양한 요소들 중 적어도 하나를 포함할 수 있다. 즉, 출력되는 음성은 화자 1이 \"Have a good day!\"를 자연스럽게 발음하는 듯한 음성일 수 있다. 하나의 발화 정보는 하나의 화자와 대응될 수 있다. 예를 들어, 제1 발화 정보는 화자 1의 음성 샘플과 대응될 수 있고, 제2 발화 정보는 제2 화자의 음성 샘플과 대응될 수 있다. 이때, 제1 발화 정보에 포함된 발화 특징은 제1 화자의 음성, 운율 등을 포함하고, 제2 발화 정보에 포함된 발화 특징은 제2 화자의 음성, 운율 등을 포함 할 수 있다. 또는, 발화 정보는 하나의 화자에 대하여 복수 개로 존재할 수 있다. 예를 들어, 제1 발화 정보는 '화남을 나타 내는 발화 톤'을 연기하는 화자 1의 제1 음성 샘플과 대응될 수 있고, 제2 발화 정보는 '기쁨을 나타내는 발화 톤'을 연기하는 화자 1의 제2 음성 샘플과 대응될 수 있다. 이때, 제1 발화 정보에 포함된 발화 특징은 '화남을 나타내는 발화 톤'을 포함할 수 있고, 제2 발화 정보에 포함된 발화 특징은 '기쁨을 나타내는 발화 톤'을 포함 할 수 있다. 도 2는 음성 합성 시스템의 일 실시예를 나타내는 도면이다. 도 2의 음성 합성 시스템은 도 1의 음성 합성 시스템과 동일할 수 있다. 도 2를 참조하면, 음성 합성 시스템은 인코더, 합성기 및 보코더를 포함할 수 있다. 한편, 도 2에 도시된 음성 합성 시스템에는 일 실시예와 관련된 구성요소들만이 도시되어 있다. 따라서, 음성 합 성 시스템에는 도 2에 도시된 구성요소들 외에 다른 범용적인 구성요소들이 더 포함될 수 있음은 당해 기 술분야의 통상의 기술자에게 자명하다. 도 2의 음성 합성 시스템은 발화 정보 및 텍스트(text)를 입력으로 수신하여 음성(speech)을 출력할 수 있 다. 예를 들어, 음성 합성 시스템의 인코더는 발화 정보를 입력으로 수신하여 발화 임베딩 벡터 (embedding vector)를 생성할 수 있다. 발화 정보는 화자의 음성 신호 또는 음성 샘플에 해당할 수 있다. 인코 더는 화자의 음성 신호 또는 음성 샘플을 수신하여, 화자의 발화 특징을 추출할 수 있으며 이를 임베딩 벡 터로 나타낼 수 있다. 한편, 음성 합성 시스템은 특정 화자 또는 특정 발화 특징을 나타내는 적어도 하나의 정형 데이터 변수를 입력으로 수신할 수 있다. 이때, 상기 정형 데이터 변수는 상기 정형 데이터 변수가 나타내는 화자 또는 발화 특징과 대응될 수 있고, 상기 화자 또는 상기 발화 특징은 음성 신호 또는 음성 샘플을 포함하는 특정 발화 정 보와 대응될 수 있다. 상기 특정 발화 정보는 음성 합성 시스템의 데이터 베이스에 미리 저장된 발화 정보 를 포함할 수 있다. 즉, 음성 합성 시스템는 특정 화자 또는 특정 발화 특징을 나타내는 적어도 하나의 정형 데이터 변수를 입 력으로 수신하고, 상기 정형 데이터 변수에 대응하는 특정 발화 정보를 획득할 수 있다. 이때, 인코더는 획득된 발화 정보를 수신하여, 화자의 발화 특징을 추출하고, 이를 임베딩 벡터로 나타낼 수 있다. 한편, 발화 정보는, 사용자 입력 신호에 기초하여 획득될 수 있다. 이때, 사용자 입력 신호는 화자 또는 발화 특징을 나타내는 적어도 하나의 정형 데이터 변수를 포함할 수 있다. 또한, 사용자 입력 신호는 화자 또는 발화 특징에 대응하는 고유한 식별값을 나타내는 신호를 포함할 수 있다. 예를 들어, 사용자는 특정 화자를 선택하는 신호를 음성 합성 시스템에 입력할 수 있다. 음성 합성 시스템은 사용자 입력 신호에 대한 응답으로, 대응되는 화자를 결정하고 결정된 화자에 대응되는 발화 정보를 데이터베이스로부터 획득할 수 있다. 화자의 발화 특징은 발화 속도, 휴지 구간, 음높이, 음색, 운율, 억양 또는 감정 등 다양한 요소들 중 적어도 하나를 포함할 수 있다. 즉, 인코더는 발화 정보에 포함된 불연속적인 데이터 값을 연속적인 숫자로 구성 된 벡터로 나타낼 수 있다. 예를 들어, 인코더는 pre-net, CBHG 모듈, DNN(Deep Neural Network), CNN(convolutional neural network), RNN(Recurrent Neural Network), LSTM(Long Short-Term Memory Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 등 다양한 인공 신경망 모델 중 적어도 하나또는 둘 이상의 조합에 기반하여 발화 임베딩 벡터를 생성할 수 있다. 음성 합성 시스템의 합성기는 인코더가 생성한 발화 임베딩 벡터에 기초하여 음성 데이터를 생 성할 수 있다. 예를 들어, 음성 합성 시스템의 합성기는 화자의 발화 특징을 나타내는 발화 임베딩 벡터 및 텍스트(text)를 입력으로 수신하여 스펙트로그램(spectrogram)을 출력할 수 있다. 도 3은 음성 합성 시스템의 합성부의 일 실시예를 나타내는 도면이다. 도 3의 합성부는 도 2의 합성기 및 보코더를 포함할 수 있다. 도 3을 참조하면, 음성 합성 시스템의 합성부는 텍스트 인코더 및 디코더를 포함할 수 있다. 한편, 합성부에는 도 3에 도시된 구성요소들 외에 다른 범용적인 구성요소들이 더 포함될 수 있음은 당해 기술분 야의 통상의 기술자에게 자명하다. 화자의 발화 특징을 나타내는 발화 임베딩 벡터는 상술한 바와 같이 인코더로부터 생성될 수 있으며, 합성 부의 인코더 또는 디코더는 인코더로부터 화자의 발화 특징을 나타내는 발화 임베딩 벡터를 수신할 수 있다. 도 4는 인공 신경망 기반의 인코더의 일 실시예를 나타내는 도면이다. 도 4의 인코더는 도 2의 인코더 와 동일할 수 있다. 도 4를 참조하면, 인코더는 발화 정보를 학습된 인공 신경망 모델에 입력하여, 화자의 음성 신호 또는 음 성 샘플과 가장 유사한 음성 데이터의 임베딩 벡터를 출력할 수 있다. 인코더는 화자의 음성 신호 또는 음 성 샘플을 발화 정보로 하여 학습된 인공 신경망 모델에 입력할 수 있다. 인코더는 발화 정보에 포함된 발 화 특징을 나타내는 발화 임베딩 벡터를 임베딩 공간에 매핑할 수 있다. 상기 인공 신경망 모델은 pre-net, CBHG 모듈, DNN, CNN, RNN, LSTM, BRDNN 등 다양한 인공 신경망 모델 중 적어도 하나일 수 있다. 인코더는 적어도 하나의 정형 데이터 변수를 입력으로 수신하고, 수신에 대한 응답으로, 한 층 이상의 가 중치 행렬로 구성된 인공 신경망을 이용하여 수신된 정형 데이터 변수에 대응하는 발화 정보를 발화 임베딩 벡 터로 변환할 수 있다. 상기 인공 신경망은 단일 임베딩 레이어, 심층 신경망(MLP, Multi-layer Perceptron) 등 을 포함할 수 있다. 일 실시예에 따르면, 발화 정보에 대응하는 발화 임베딩 벡터가 데이터베이스에 미리 저장될 수 있고, 이때 인 코더는 발화 임베딩 벡터를 특정하는 적어도 하나의 정형 데이터 변수를 입력으로 수신하여 상기 데이터베 이스로부터 상기 발화 임베딩 벡터를 획득할 수 있다. 한편, 인코더는 발화 정보에 STFT(Short-time Fourier transform)를 수행하여 제1 스펙트로그램 (spectrogram)들을 생성할 수 있다. 인코더는 학습된 인공 신경망 모델에 제1 스펙트로그램들을 입력하여 발화 임베딩 벡터를 생성할 수 있다. 스펙트로그램은 음성 신호의 스펙트럼을 시각화하여 그래프로 표현한 것이다. 스펙트로그램의 x축은 시간, y축 은 주파수를 나타내며 각 시간당 주파수가 가지는 값을 값의 크기에 따라 색으로 표현할 수 있다. 스펙트로그램 은 연속적으로 주어지는 음성 신호에 STFT(Short-time Fourier transform)를 수행한 결과물일 수 있다. STFT는 음성 신호를 일정한 길이의 구간들로 나누고 각 구간에 대하여 푸리에 변환을 적용하는 방법이다. 이 때, 음성 신호에 STFT를 수행한 결과물은 복소수 값이기 때문에, 복소수 값에 절대값을 취해 위상(phase) 정보 를 소실시키고 크기(magnitude) 정보만을 포함하는 스펙트로그램을 생성할 수 있다. 한편, 멜 스펙트로그램은 스펙트로그램의 주파수 간격을 멜 스케일(Mel Scale)로 재조정한 것이다. 사람의 청각 기관은 고주파수(high frequency) 보다 저주파수(low frequency) 대역에서 더 민감하며, 이러한 특성을 반영해 물리적인 주파수와 실제 사람이 인식하는 주파수의 관계를 표현한 것이 멜 스케일이다. 멜 스펙트로그램은 멜 스케일에 기반한 필터 뱅크(filter bank)를 스펙트로그램에 적용하여 생성될 수 있다. 도 5는 인코더에서 임베딩 벡터를 생성하기 위한 벡터 공간의 일 실시예를 나타내는 도면이다. 인코더는 다양한 음성 데이터들에 해당하는 스펙트로그램들 및 이에 대응하는 임베딩 벡터들을 벡터 공간 상에 표시할 수 있다. 인코더는 학습된 인공 신경망 모델에 화자의 음성 신호 또는 음성 샘플로부터 생성한 스펙트로그램들을 입력할 수 있다. 인코더는 학습된 인공 신경망 모델로부터 벡터 공간 상에 서 화자의 음성 신호 또는 음성 샘플과 가장 유사한 음성 데이터의 임베딩 벡터를 발화 임베딩 벡터로 출력할 수 있다. 즉, 학습된 인공 신경망 모델은 스펙트로그램들을 입력 받아 벡터 공간의 특정 포인트에 매칭되는 임베딩 벡터를 생성할 수 있다. 인코더가 생성한 발화 임베딩 벡터는, 벡터 공간 상에 매핑될 수 있다. 벡터 공간은 임베딩 공 간을 포함할 수 있다. 일반적으로, 임베딩 공간은 임베딩 벡터의 성분값의 개수를 차원의 수로 표현할 수 있다. 예를 들어, 인코더는 제1 내지 제3 발화 정보를 입력으로 수신하여 제1 내지 제3 발화 임베딩 벡터를 출력 할 수 있다. 이때, 제1 내지 제3 발화 임베딩 벡터는 벡터 공간 상에서 각각 다른 위치에 매핑될 수 있다. 즉, 제1 내지 제3 발화 임베딩 벡터는 벡터 공간상의 서로 다른 세개의 포인트(511, 512, 513)에 대응될 수 있다. 한편, 임베딩 벡터는, 임베딩 벡터가 매핑되는 벡터 공간의 차원 수에 해당하는 개수의 성분값을 가질 수 있다. 표는 벡터 공간의 특정 포인트에 대응되는 벡터 값 및 임베딩 벡터의 매핑 테이블에 해당될 수 있다. 매핑 테이블은 임베딩 벡터의 리스트와 벡터 값의 리스트로 구성될 수 있다. 다시 도 3으로 돌아와서, 합성부의 텍스트 인코더는 텍스트를 입력으로 수신하여 텍스트 임베딩 벡터를 생 성할 수 있다. 텍스트는 특정 자연 언어로 된 문자들의 시퀀스를 포함할 수 있다. 예를 들어, 문자들의 시퀀스 는 알파벳 문자들, 숫자들, 문장 부호들 또는 기타 특수 문자들을 포함할 수 있다. 텍스트 인코더는 입력된 텍스트를 자모 단위, 글자 단위 또는 음소 단위로 분리할 수 있고, 분리된 텍스트를 인 공 신경망 모델에 입력할 수 있다. 예를 들어, 텍스트 인코더는 pre-net, CBHG 모듈, DNN, CNN, RNN, LSTM, BRDNN 등 다양한 인공 신경망 모델 중 적어도 하나 또는 둘 이상의 조합에 기반하여 텍스트 임베딩 벡터를 생성 할 수 있다. 또는, 텍스트 인코더는 입력된 텍스트를 복수의 짧은 텍스트들로 분리하고, 짧은 텍스트들 각각에 대하여 복수 의 텍스트 임베딩 벡터들을 생성할 수도 있다. 합성부의 디코더는 인코더로부터 발화 임베딩 벡터 및 텍스트 임베딩 벡터를 입력으로 수신할 수 있 다. 또는, 합성부의 디코더는 인코더로부터 발화 임베딩 벡터를 입력으로 수신하고, 텍스트 인코더로 부터 텍스트 임베딩 벡터를 입력으로 수신할 수 있다. 디코더는 발화 임베딩 벡터와 텍스트 임베딩 벡터를 인공 신경망 모델에 입력하여, 입력된 텍스트에 대응되는 스펙트로그램을 생성할 수 있다. 즉, 디코더는 화자의 발화 특징이 반영된 입력 텍스트에 대한 스펙트로그램을 생성할 수 있다. 예를 들면, 스펙트로그램은 멜 스펙트로그램(mel-spectrogram)에 해당할 수 있으나, 이에 제한 되는 것은 아니다. 한편, 도 3에는 도시되어 있지 않으나, 합성부는 어텐션 얼라이먼트를 생성하기 위한 어텐션 모듈을 더 포 함할 수 있다. 어텐션 모듈은 디코더의 특정 타임 스텝(time-step)의 출력이 인코더의 모든 타임 스텝의 출력 중 어떤 출력과 가장 연관이 있는가를 학습하는 모듈이다. 어텐션 모듈을 이용하여 더 고품질의 스펙트로그램 또는 멜 스펙트로그램을 출력할 수 있다. 다시 도 2로 돌아와서, 음성 합성 시스템의 보코더는 합성기에서 출력된 스펙트로그램을 실제 음성(speech)으로 생성할 수 있다. 상술한 바와 같이 출력된 스펙트로그램은 멜 스펙트로그램일 수 있다. 일 실시예에서, 보코더는 ISFT(Inverse Short-Time Fourier Transform)를 이용하여 합성기에서 출력 된 스펙트로그램을 실제 음성 신호로 생성할 수 있다. 스펙트로그램 또는 멜 스펙트로그램은 위상 정보를 포함 하고 있지 않으므로, ISFT를 이용하여 음성 신호를 생성하는 경우 스펙트로그램 또는 멜 스펙트로그램의 위상 정보는 고려되지 않는다. 다른 실시예에서, 보코더는 그리핀-림 알고리즘(Griffin-Lim algorithm)을 사용하여 합성기에서 출력 된 스펙트로그램을 실제 음성 신호로 생성할 수 있다. 그리핀-림 알고리즘은 스펙트로그램 또는 멜 스펙트로그 램의 크기 정보에서 위상 정보 추정하는 알고리즘이다. 또는, 보코더는 예를 들어 뉴럴 보코더(neural vocoder)에 기초하여 합성기에서 출력된 스펙트로그램 을 실제 음성 신호로 생성할 수 있다. 뉴럴 보코더는 스펙트로그램 또는 멜 스펙트로그램을 입력으로 받아 음성 신호를 생성하는 인공 신경망 모델이 다. 뉴럴 보코더는 스펙트로그램 또는 멜 스펙트로그램과 음성 신호 사이의 관계를 다량의 데이터를 통해 학습 할 수 있고, 이를 통해 고품질의 실제 음성 신호를 생성할 수 있다.뉴럴 보코더는 WaveNet, Parallel WaveNet, WaveRNN, WaveGlow 또는 MelGAN 등과 같은 인공 신경망 모델에 기 반한 보코더에 해당할 수 있으나, 이에 제한되는 것은 아니다. 예를 들어, WaveNet 보코더는 여러 층의 dilated causal convolution layer들로 구성되며, 음성 샘플들 간의 순차적 특징을 이용하는 자기회귀(Autoregressive) 모델이다. WaveRNN 보코더는 WaveNet의 여러 층의 dilated causal convolution layer를 GRU(Gated Recurrent Unit)로 대체한 자기회귀 모델이다. WaveGlow 보코더는 가역 성(invertible)을 지닌 변환 함수를 이용하여 스펙트로그램 데이터셋(x)으로부터 가우시안 분포와 같이 단순한 분포가 나오도록 학습할 수 있다. WaveGlow 보코더는 학습이 끝난 후 변환 함수의 역함수를 이용하여 가우시안 분포의 샘플로부터 음성 신호를 출력할 수 있다. 한편, 음성 합성 시스템에 어떤 화자의 음성 샘플이 입력되더라도, 상기 화자의 발화 특징이 반영된 입력 텍스트에 대한 음성을 생성할 수 있는 것이 중요하다. 학습되지 않은 화자의 음성 샘플이 입력되더라도 화자의 음성 샘플과 가장 유사한 음성 데이터의 임베딩 벡터를 발화 임베딩 벡터로 출력하기 위해서, 인코더의 인 공 신경망 모델은 다양한 화자들의 음성 데이터로 학습될 필요가 있다. 예를 들어, 인코더의 인공 신경망 모델을 학습시키기 위한 학습 데이터는 화자가 특정 텍스트에 해당하는 녹음 대본에 기초하여 녹음을 수행한 녹음 데이터에 해당할 수 있다. 이에 따라, 인코더의 인공 신경망 모 델을 학습시키기 위한 녹음 데이터의 품질을 평가할 필요성이 있다. 예를 들어, 화자가 녹음 대본에 일치하게 녹음을 수행하였는지 여부와 관련하여 녹음 데이터의 품질이 평가될 수 있다. 음성 합성 시스템을 이용하 여 화자가 녹음 대본에 일치하게 녹음을 수행하였는지 여부를 평가할 수 있다. 이하에서는, 도 6 내지 도 9를 참조하여 미학습 화자의 음성을 합성하는 장치(이하, '장치'라 함)가 동작하는 일 예를 상세히 설명한다. 한편, 장치는 도 2에 도시된 음성 합성 시스템과 동일하게 구현될 수 있으나, 이에 한정되지 않는다. 도 6은 제2 인코더를 이용하여 학습된 제1 인코더를 포함하는 음성 합성 시스템의 일 실시예를 나타내는 도면이 다. 한편, 도 6에 도시된 장치와 도 2에 도시된 시스템을 비교하면, 장치는 서로 구분된 제1 인코더 및 제2 인코더를 포함할 수 있다. 제1 인코더 및 제2 인코더는 도 1 내지 도 5과 관련하 여 설명된 인코더가 수행하는 동작을 각각 수행할 수 있다. 즉, 제1 인코더 또는 제2 인코더는 도 2 에 도시된 인코더 또는 도 4에 도시된 인코더와 동일할 수 있다. 일 실시예에 따르면, 제1 인코더는 제2 인코더로부터 출력된 데이터를 이용하여 학습될 수 있다. 이 를 통해, 장치는 학습된 제1 인코더를 이용하여 미학습 화자의 음성을 합성할 수 있다. 장치는 미학습 발화 정보를 획득할 수 있다. 미학습 발화 정보는 장치가 사전에 학습한 바 없는 발화 정보를 포함할 수 있다. 일 실시예에 따르면, 장치는 사전에 학습한 발화 정보들에 대응하는 발화 임베딩 벡터들을 소정의 임베딩 공간에 매핑할 수 있다. 이때, 미학습 발화 정보는 소정의 임베딩 공간에 매핑된 발화 임베딩 벡터들이 나타내지 않는 발화 특징을 포함하는 발화 정보에 해당될 수 있다. 또는, 미학습 발화 정보는 사전에 학습된 발화 정보에 대응하는 화자들과 상이한 화자의 음성 샘플을 포함할 수 있다. 그러나, 이에 제한 되지 않는다. 종래 기술에 의하면, 사전 학습된 인코더가 학습하지 않은 미학습 화자의 음성을 합성하기 위해서, 사전 학습된 인코더를 미세조정(Fine-tuning)하는 방법을 이용할 수 있다. 미세조정은, 기 학습된 인공 신경망 모델에 기반 하여, 모델의 아키텍쳐를 새로운 목적에 맞게 변형하고, 이미 학습된 모델의 파라미터를 미세하게 조정하여 학 습시키는 방법이다. 기 학습된 인코더는 사전 학습 과정에서 획득한 지식을 미세조정 과정에서 활용할 수 있다. 다만, 미세조정은 모델의 모든 파라미터를 업데이트 하여야 하므로, 미세조정을 수행하는 과정에서 많은 자원을 요한다. 본 개시 의 일 실시예에 따르면, 미세조정을 거치지 않고, 전이 학습된 별도의 인코더를 이용함으로써 미학습 화자의 음 성을 합성할 수 있다. 장치는 미학습 발화 정보를 이용하여 미학습 발화 임베딩 벡터를 생성할 수 있다. 미학습 발화 임베딩 벡 터는, 미학습 발화 정보를 인공 신경망 모델에 입력하여 획득한 발화 임베딩 벡터에 해당될 수 있다. 즉, 미학 습 발화 임베딩 벡터는 미학습 발화 정보에 포함된 발화 특징을 음성 신호에 반영하는데 이용되는 발화 임베딩 벡터에 해당될 수 있다.미학습 발화 임베딩 벡터의 생성은, 전이 학습(Transfer Learning)된 제1 인코더에 의하여 수행될 수 있다. 상기 제1 인코더는 사전 학습(Pre-trained)된 제2 인코더로부터 출력된 정보를 이용하여 전이 학습된 것일 수 있다. 제1 인코더 및/또는 제2 인코더 모델은 인코더에 포함될 수 있다. 즉, 제 1 인코더 및/또는 제2 인코더는 발화 정보를 입력으로 수신하여 발화 임베딩 벡터를 출력하는 인공 신경망 기반의 인공지능 모델을 포함할 수 있다. 이하에서는, 제1 인코더를 전이 학습하는 과정에 관하여 상세히 설명하도록 한다. 전이 학습은 특정 문제 해결을 위해 획득한 지식을, 다른 문제 해결을 위해 사용하는 기법이다. 전이 학습은 사 전 학습(Pre-trained) 인공지능 모델의 지식을 새롭게 이용할 다른 인공지능 모델에 전달하는 방식을 이용할 수 있다. 이때, 사전 학습 모델을 교사 모델로, 다른 인공지능 모델을 학생 모델로 지칭할 수 있다. 일 실시예에 따르면, 제2 인코더는 교사 모델로 하여, 학생 모델인 제1 인코더는 전이 학습될 수 있다. 예를 들어, 전이 학습은, 장치에 의해 수행될 수 있다. 또는, 전이 학습은, 장치와 독립된 기계학습 시스템에 의해 수행될 수 있다. 그러나, 전이 학습을 수행하는 주체는 이들에 한정되지 않는다. 일 실시예에 따르면, 장치는 기 설정된 학습 데이터베이스로부터 획득된 소정의 발화 정보를 상기 제1 인 코더에 입력하여 상기 소정의 발화 정보가 분할된 적어도 하나의 음성 프레임 각각에 대한 피처 벡터를 획 득할 수 있다. 소정의 발화 정보는 제1 인코더의 전이 학습을 위해 기 설정된 학습 데이터베이스에 저장된 발화 정보에 해당될 수 있다. 기 설정된 학습 데이터베이스는 인공 신경망 모델을 학습시키는 학습데이터를 포함할 수 있다. 기 설정된 학습 데이터베이스에 포함되는 학습데이터는 발화 정보를 포함할 수 있고, 발화 정보는 음성 신호, 음성 샘플 등을 포함할 수 있으나, 이에 제한되지 않는다. 학습 데이터베이스는 개인, 회사 및 국가가 수집한 데이터를 포함할 수 있다. 음성 프레임은 소정의 발화 정보가 임의의 길이로 분할된 프레임을 포함할 수 있다. 또는, 음성 프레임은 소정 의 발화 정보가 소정의 기준에 따라 분할된 프레임을 포함할 수 있다. 또는, 음성 프레임은 소정의 발화 정보 그 자체인 1개의 프레임을 포함할 수 있다. 피처 벡터는 음성 프레임의 특징을 추출하여 표현한 벡터에 해당될 수 있다. 피처 벡터는 분할된 음성 프레임의 개수만큼 생성될 수 있다. 피처 벡터는 하나의 음성 프레임을 하나의 벡터로 표현할 수 있는 모든 기법에 기반 하여 생성될 수 있다. 예를 들어, 피처 벡터는 MFCC, Mel-Spectrogram, Linear Spectrogram 등의 신호처리 기법에 기반하여 생성될 수 있다. 또는, 인공 신경망에 기반하여 생성될 수 있다. 그러나, 이에 제한되지 않는다. 일 실시예에 따르면, 장치는 소정의 발화 정보가 분할된 적어도 하나의 음성 프레임 중에서, 음성이 존재 하는 프레임에 대응하는 피처 벡터를 획득할 수 있다. 이를 통해, 음성이 존재하지 않는 프레임에 대응하는 피 처 벡터를 학습 또는 추론에 이용하지 않을 수 있고, 합성 음성의 기대 품질을 향상시킬 수 있다. 예를 들어, 소정의 발화 정보는 제1 내지 제4 음성 프레임으로 분할될 수 있고, 제3 음성 프레임은 음성이 존재 하지 않을 수 있다. 이때, 장치는 제1 내지 제4 음성 프레임에 대응하는 제1 내지 제4 피처 벡터 중, 제3 피처 벡터를 획득하지 않고, 제1 내지 제2 피처 벡터 및 제4 피처 벡터만을 획득할 수 있다. 한편, 일 실시예에 따르면, 장치는 음성의 F0(Fundamental Frequency)를 추출함으로써 음성이 존재하는 프레임에 대응하는 피 처 벡터를 획득할 수 있다. 장치는 피처 벡터를 제1 인코더에 입력하여 피처 벡터 각각에 대한 프레임 임베딩 벡터를 획득할 수 있다. 일 실시예에 따르면, 장치는 피처 벡터를 인공 신경망 모델을 이용하는 제1 인코더에 입력하여 프레임 임베딩 벡터를 획득할 수 있다. 프레임 임베딩 벡터는 발화 임베딩 벡터에 포함될 수 있다. 예를 들어, 장치는 발화 정보의 특정 음성 프 레임에 포함된 발화 특징을 추출함으로써 피처 벡터를 생성할 수 있다. 이때, 장치는 피처 벡터를 인공 신 경망 모델에 입력함으로써, 발화 임베딩 벡터와 동일한 차원을 갖는 프레임 임베딩 벡터를 생성할 수 있다. 일 실시예에 따른 제1 인코더는, 발화 정보를 적어도 하나의 음성 프레임으로 분할하고, 음성 프레임에 포 함된 음성의 특징을 추출하여 음성 프레임 각각에 대한 피처 벡터를 생성하고, 피처 벡터를 임베딩 공간에 매핑 하기 위하여 피처 벡터 각각에 대한 프레임 임베딩 벡터를 생성할 수 있다. 일 실시예에 따르면, 피처 벡터는발화 임베딩 벡터와 차원의 수 등이 상이할 수 있으므로, 장치는 피처 벡터에 기초하여 프레임 임베딩 벡 터를 생성할 수 있다. 장치는 소정의 발화 정보에 대응하여 제2 인코더가 출력한 타겟 임베딩 벡터, 프레임 임베딩 벡터 및 피처 벡터 중 적어도 하나를 이용하여, 제1 인코더의 파라미터를 교정할 수 있다. 장치는 소정의 발화 정보를 제2 인코더에 입력하여 타겟 임베딩 벡터를 획득할 수 있다. 타겟 임베딩 벡터는 발화 임베딩 벡터에 포함될 수 있다. 예를 들어, 소정의 발화 정보가 제1 화자의 발화 특징을 포함하는 경우, 타겟 임베딩 벡터는 음성 신호에 제1 화자의 발화 특징을 부가하는데 이용되는 발화 임베딩 벡터에 해당 될 수 있다. 한편, 제1 인코더는 사전에 학습되지 않은 인공 신경망 모델을 포함할 수 있다. 제1 인코더는 제2 인 코더를 모방하도록 하는, 크기가 작은 인공 신경망 모델을 포함할 수 있다. 장치는 하나의 손실함수 또는 복수개의 손실함수항의 조합을 이용하여 상기 파라미터를 교정할 수 있다. 상기 조합은 손실함수 가중치를 이용하여 복수개의 손실함수항을 가중합함으로써 수행될 수 있다. 높은 학습 효 과를 제공하는 것으로 검증된 가중치는 하이퍼파라미터(Hyperparameter)로 설정될 수 있다. 일 실시예에 따르면, 장치는 지각 손실함수(Perceptual Loss) 및 삼중항 손실함수(Triplet loss) 중 적어도 하나를 이 용하여 상기 파라미터를 교정할 수 있다. 손실함수는 인공지능 신경망 모델을 통해 추론된 예측 값이 타겟 값 (정답 값)과 얼마나 멀고 가까운지를 비교하는 함수일 수 있다. 일반적으로 기계 학습의 방향은 손실함수로 계 산된 손실 값을 줄이는 방향으로 설정될 수 있다. 지각 손실함수는 종래 이미지 초해상화 분야에서 이용되는 지각 손실함수를 포함할 수 있다. 삼중항 손실함수는 두개의 동일한 범주의 데이터와 하나의 상이한 범주의 데이터를 이용하는 손실함수를 포함할 수 있다. 삼중항 손실함수는 동일한 범주의 데이터를 입력할 때, 출력값이 가까워지도록 학습하기 위해 이용되는 손실함수를 포 함할 수 있다. 장치는 타겟 임베딩 벡터, 프레임 임베딩 벡터 및 지각 손실함수에 기초하여 제1 손실을 계산할 수 있다. 예를 들어, 장치는 예측 값과 타겟 값을 변수로 하는 지각 손실함수에, 예측 값으로서 프레임 임베딩 벡터 를 입력하고 타겟 값으로서 타겟 임베딩 벡터를 입력하여 제1 손실을 획득할 수 있다. 장치는 제1 인코더에 소정의 발화 정보를 입력하여 프레임 임베딩 벡터를 생성할 수 있다. 제1 인코 더는 인공 신경망의 입력 레이어 파라미터들로 소정의 발화 정보를 연산하고, 모든 은닉 레이어의 파라미 터들로 연산하는 과정을 거친 후, 프레임 임베딩 벡터를 생성할 수 있다. 한편, 상기 인공 신경망의 연산은 발화 정보에 전처리를 수행한 결과물에 기초하여 수행될 수 있다. 예를 들어, 전처리는 음성 프레임 분할 또는 피처 벡터 생성에 해당될 수 있다. 그러나, 이에 제한되지 않는다. 장치는 순전파로 획득한 제1 인코더의 프레임 임베딩 벡터, 제2 인코더의 타겟 임베딩 벡터 및 지각 손실함수에 기초하여 제1 손실을 계산할 수 있다. 장치는 계산된 제1 손실에 기초하여 파라미터를 교정할 수 있다. 일 실시예에 따르면, 상기 제1 손실을 바 탕으로 역전파를 진행하여 제1 인코더의 파라미터들을 교정할 수 있다. 예를 들어, 장치는 역전파를 진행하여 손실 값을 줄이는 방향으로 제1 인코더의 파라미터들을 교정할 수 있다. 장치는 교정된 파라미터들에 기초하여, 순전파 및 역전파를 적어도 1회 이상 수행할 수 있다. 일 실시예에 따르면, 장치는 기 설정된 횟수만큼 순전파 및 역전파를 반복할 수 있다. 또는, 계산된 손실 값이 기 설정 된 오차범위보다 작아질 때까지 순전파 및 역전파를 반복할 수 있다. 장치는 소정의 발화 정보와 상이한 비교 발화 정보를 제1 인코더에 입력하여 적어도 하나의 비교 피 처 벡터를 획득할 수 있다. 비교 발화 정보는 기 설정된 학습 데이터베이스로부터 획득될 수 있다. 비교 발화 정보에 대응하는 제1 화자는, 소정의 발화 정보에 대응하는 제2 화자와 상이할 수 있다. 피처 벡터는 음성 프레임의 특징이 추출되어 생성된 것으로 이해될 수 있다. 따라서, 동일 화자에 대한 피처 벡 터는 다른 화자에 대한 피처 벡터와 벡터 공간상 멀게 생성되는 것이 바람직할 수 있다. 장치는 제1 화자의 발화 정보를 소정의 발화 정보로서 제1 인코더에 입력하여 피처 벡터를 생성할 수 있다. 장치는 제1 화자와 상이한 제2 화자의 발화 정보를 비교 발화 정보로서 제1 인코더에 입력하여 비교 피처 벡터를 획득할 수 있다.일 실시예에 따르면, 장치는 적어도 하나의 비교 피처 벡터 중 선택된 하나의 비교 피처 벡터, 상기 소정 의 발화 정보에 기초하여 생성된 복수 개의 피처 벡터 중 선택된 두 개의 피처 벡터 및 삼중항 손실함수에 기초 하여 제2 손실을 계산할 수 있다. 삼중항 손실함수는 두 개의 동일한 범주의 벡터와 하나의 상이한 범주의 벡터 를 변수로 하는 손실함수를 포함할 수 있다. 장치는 삼중항 손실함수에 제1 화자에 대응되는 두개의 피처 벡터와 제2 화자에 대응되는 하나의 비교 피처 벡터를 입력함으로써, 제2 손실을 계산할 수 있다. 장치는 계산된 제2 손실에 기초하여 제1 인코더의 파라미터를 교정할 수 있다. 장치는 같은 화 자에 대한 피처 벡터는 벡터 공간상 가깝게, 다른 화자에 대한 피처 벡터는 벡터 공간상 멀게 생성되도록, 제1 인코더의 파라미터를 교정할 수 있다. 이하에서는, 전이 학습된 제1 인코더를 이용하여 미학습 발화 임베딩 벡터를 생성하는 내용을 상세히 설명 하도록 한다. 장치는 미학습 발화 정보에 기초하여 미학습 발화 임베딩 벡터를 생성할 수 있다. 일 실시예에 따르면, 장 치는 미학습 발화 정보에 기초하여 미학습 발화 정보가 분할된 적어도 하나의 음성 프레임 각각에 대한 피 처 벡터를 생성할 수 있다. 한편, 피처 벡터의 생성은, 제1 인코더의 전이 학습이 수행될 때 피처 벡터를 생성하는 과정과 동일하게 수행될 수 있다. 장치는 생성된 피처 벡터에 기초하여 미학습 발화 임베딩 벡터 를 생성할 수 있다. 예를 들어, 장치는 피처 벡터에 기초하여 프레임 임베딩 벡터를 생성하고, 프레임 임 베딩 벡터에 기초하여 미학습 발화 임베딩 벡터를 생성할 수 있다. 장치는 피처 벡터에 기초하여 피처 벡터의 수와 동일한 수의 프레임 임베딩 벡터를 생성할 수 있다. 한편, 프레임 임베딩 벡터의 생성은, 제1 인코더의 전이 학습이 수행될 때, 프레임 임베딩 벡터를 생성하는 과정 과 동일하게 수행될 수 있다. 장치는 프레임 임베딩 벡터에 기초하여 상기 미학습 발화 임베딩 벡터를 생성할 수 있다. 장치는 하 나의 미학습 발화 정보에 대응되는 모든 프레임 임베딩 벡터를 이용하여 하나의 미학습 발화 임베딩 벡터를 생 성할 수 있다. 일 실시예에 따르면, 장치는 미학습 발화 정보에 기초하여 적어도 하나의 피처 벡터를 생성하고, 각각의 피처 벡터에 대한 프레임 임베딩 벡터를 생성하고, 모든 프레임 임베딩 벡터의 평균값 또는 중간값을 갖는 미학 습 발화 임베딩 벡터를 생성할 수 있다. 또는, 장치는 프레임 임베딩 벡터를 생성하고, 연속적 이상치 탐지 기법, 아웃라이어 제거 기법 등을 이용 하여 보다 높은 품질의 미학습 발화 임베딩 벡터를 생성할 수 있다. 장치는 입력 발화 정보가 제2 인코더의 학습 데이터베이스에 포함되는 것인지 판단할 수 있다. 예를 들어, 장치는 입력 발화 정보에 매핑된 발화 임베딩 벡터가 존재하는지 여부에 따라 입력 발화 정보의 학 습 데이터베이스 포함 여부를 판단할 수 있다. 또는, 장치는 발화 정보로서 입력된 화자 식별 정보에 기초 하여 상기 포함 여부를 판단할 수 있다. 그러나, 판단 방법은 이에 제한되지 않는다. 장치는 입력 발화 정보가 데이터베이스에 포함되지 않는 경우, 입력 발화 정보를 미학습 발화 정보로 하여, 미학습 발화 임베딩 벡터를 생성할 수 있다. 한편, 입력 발화 정보가 데이터베이스에 포함되는 경우, 입 력 발화 정보에 기초하여 학습 발화 임베딩 벡터를 생성할 수 있다. 미학습 발화 임베딩 벡터의 생성은, 제1 인 코더에 의하여 수행되고, 학습 발화 임베딩 벡터의 생성은, 상기 제2 인코더에 의하여 수행될 수 있 다. 예를 들어, 기 학습된 화자의 발화 정보가 입력된 경우, 장치는 제2 인코더를 이용하여 학습 발화 임 베딩 벡터를 생성할 수 있다. 장치는 입력된 학습 발화 정보를 제2 인코더에 입력하여 학습 발화 임 베딩 벡터를 획득할 수 있다. 또는, 장치는 입력된 학습 발화 정보에 대응하여, 제2 인코더가 사전에 생성한 학습 발화 임베딩 벡터를 획득할 수 있다. 이때 입력된 학습 발화 정보는, 학습된 화자에 대응되는 적어 도 하나의 발화 정보로서, 장치가 수신한 정형 데이터 변수에 대응하는 발화 정보 해당될 수 있으나, 이에 제한되지 않는다. 다른 예로서, 미학습 화자의 발화 정보가 입력된 경우, 장치는 제1 인코더를 이용하여 미학습 발화 임베딩 벡터를 생성할 수 있다. 장치는 입력된 미학습 발화 정보를 제1 인코더에 입력하여 미학습 발 화 임베딩 벡터를 획득할 수 있다. 이때 미학습 발화 정보는 장치가 수신한 미학습 화자의 음성 신호 또는 음성 샘플에 해당될 수 있으나, 이에 제한되지 않는다.한편, 미학습 발화 임베딩 벡터는 학습 발화 임베딩 벡터와 동일한 차원을 가질 수 있다. 즉, 제1 인코더 가 생성하는 발화 임베딩 벡터는, 제2 인코더가 생성하는 발화 임베딩 벡터와 동일한 임베딩 공간에 매핑 된 발화 임베딩 벡터를 포함할 수 있다. 장치는 미학습 발화 정보에 기초하여 미학습 발화 임베딩 벡터를 생성할 수 있다. 장치는 특정 자연 언어로 구성된 텍스트 및 미학습 발화 임베딩 벡터에 기초하여 음성 신호를 생성할 수 있다. 일 실시예에 따르 면, 장치는 텍스트에 기초하여 생성된 텍스트 임베딩 벡터 및 미학습 발화 임베딩 벡터를 인공 신경망 모 델에 입력함으로써, 스펙트로그램을 생성할 수 있다. 예를 들어, 장치는 텍스트 임베딩 벡터 및 미학습 발 화 임베딩 벡터를 합성기에 입력함으로써 스펙트로그램을 생성할 수 있다. 한편, 특정 텍스트 및 미학습 발화 임베딩 벡터에 기초하여 생성된 음성 신호는, 제2 인코더의 학습 데이 터베이스에 존재하지 않는 화자의 발화 특징이 반영된 음성 신호일 수 있다. 도 7은 발화 정보를 입력으로 수신하여 임베딩 벡터를 생성하는 제1 인코더의 일 실시예를 나타내는 도면이다. 도 7은 도 6에 도시된 제2 인코더를 이용하여 전이 학습된 제1 인코더의 동작을 상세히 설명하기 위 한 도면이다. 도 7에 도시된 제1 인코더 및 제2 인코더는 각각 도 6에 도시된 제1 인코더와 제2 인코더에 대응될 수 있다. 도 7을 참조하면, 장치는 소정의 발화 정보를 제1 인코더에 입력하여 프레임 임베딩 벡터 를 획득함으로써 제1 인코더를 전이 학습할 수 있다. 또한, 장치는 미학습 발화 정보를 제1 인 코더에 입력하여 미학습 발화 임베딩 벡터를 획득함으로써 미학습 화자의 음성 신호를 합성할 수 있 다. 한편, 전이 학습을 수행할 때, 장치는 소정의 발화 정보를 제1 인코더의 피처 추출기에 입 력하여 피처 벡터를 획득할 수 있다. 피처 추출기는 역전파에 의해 파라미터가 교정되는 인공 신경망에 기 반하는 알고리즘을 포함할 수 있다. 또한, 피처 추출기는 신호처리 기법에 기반하는 알고리즘 또는 역전파 에 의해 파라미터가 교정되지 않는 인공 신경망에 기반하는 알고리즘을 포함할 수 있다. 피처 추출기는 MFCC, Mel-Spectrogram, Linear Spectrogram 등의 신호처리 기법에 기반하는 알고리즘을 포함할 수 있으나, 이에 제한되지 않는다. 피처 추출기는 wav2vec, HuBERT 등 사전학습에 의해 파라미터가 확정된 인공 신경망에 기반하는 알고리즘을 포함할 수 있으나, 이에 제한되지 않는다. 장치는 미학습 발화 정보를 전이 학습된 제1 인코더의 피처 추출기에 입력하여 피처 벡터를 생성할 수 있다. 한편, 전이 학습을 수행할 때, 장치는 소정의 발화 정보에 대응되는 피처 벡터를 피처 변환기에 입력하여 프레임 임베딩 벡터를 획득할 수 있다. 피처 변환기는 피처 벡터를 음성 합성에 이용할 수 있도록 임베딩 공간에 매핑하는 알고리즘을 포함할 수 있다. 예를 들어, 피처 변환기는 다층 신경망 (Multi-layer Perceptron)을 포함할 수 있다. 또는, 피처 변환기는 완전연결(Fully Connected) 레이어를 포함할 수 있다. 장치는 미학습 발화 정보에 대응되는 피처 벡터를 전이 학습된 제1 인코더의 피처 변환기에 입력하여 미학습 발화 임베딩 벡터를 생성할 수 있다. 장치는 특정 텍스트 및 미학습 발화 임베딩 벡터에 기초하여 학습 데이터에 포함되지 않는 화자의 음 성 신호를 생성할 수 있다. 도 8은 일 실시예에 따른 미학습 화자의 음성을 합성하는 방법의 흐름도이다. 도 8을 참조하면, 단계 810에서, 장치는 미학습 발화 정보를 획득할 수 있다. 단계 820에서, 장치는 미학습 발화 정보를 이용하여 미학습 발화 임베딩 벡터를 생성할 수 있다. 생성은, 전이 학습(Transfer Learning)된 제1 인코더에 의하여 수행될 수 있다. 제1 인코더는 사전 학습(Pre- trained)된 제2 인코더로부터 출력된 정보를 이용하여 전이 학습된 것일 수 있다. 예를 들어, 전이 학습은, 장치에 의해 수행될 수 있다. 또는, 전이 학습은, 장치와 독립된 기계학습 시스템에 의해 수행될 수 있다. 그러나, 전이 학습을 수행하는 주체는 이들에 한정되지 않는다. 일 실시예에 따르면, 장치는 기 설정된 학습 데이터베이스로부터 획득된 소정의 발화 정보를 제1 인코더에 입력하여 소정의 발화 정보가 분할된 적어도 하나의 음성 프레임 각각에 대한 피처 벡터를 획득할 수 있다. 일 실시예에 따르면, 장치는 음성 프레임 중에서, 음성이 존재하는 프레임에 대응하는 피처 벡터를 획득할 수 있다. 일 실시예에 따르면, 장치는 피처 벡터를 제1 인코더에 입력하여 피처 벡터 각각에 대한 프레임 임베딩 벡 터를 획득할 수 있다. 장치는 소정의 발화 정보에 대응하여 제2 인코더가 출력한 타겟 임베딩 벡터, 프레임 임베딩 벡터 및 피처 벡터 중 적어도 하나를 이용하여, 제1 인코더의 파라미터를 교정할 수 있다. 제1 인코더는, 발화 정보를 적어도 하나의 음성 프레임으로 분할하고, 음성 프레임에 포함된 음성의 특징을 추 출하여 음성 프레임 각각에 대한 피처 벡터를 생성하고, 피처 벡터를 임베딩 공간에 매핑하기 위하여 피처 벡터 각각에 대한 프레임 임베딩 벡터를 생성할 수 있다. 장치는 지각 손실함수(Perceptual Loss) 및 삼중항 손실함수(Triplet loss) 중 적어도 하나를 이용하여 파라미터를 교정할 수 있다. 장치는 타겟 임베딩 벡터, 프레임 임베딩 벡터 및 지각 손실함수에 기초하여 제1 손실을 계산할 수 있다. 장치는 계산된 제1 손실에 기초하여 파라미터를 교정할 수 있다. 장치는 소정의 발화 정보와 상이한 비교 발화 정보를 제1 인코더에 입력하여 적어도 하나의 비교 피처 벡 터를 획득할 수 있다. 비교 발화 정보는 데이터베이스로부터 획득될 수 있다. 비교 발화 정보에 대응하는 제1 화자는, 소정의 발화 정보에 대응하는 제2 화자와 상이할 수 있다. 장치는 비교 피처 벡터 중 선택된 하나의 비교 피처 벡터, 소정의 발화 정보에 기초하여 생성된 피처 벡터 중 선택된 두 개의 피처 벡터 및 삼중항 손실함수에 기초하여 제2 손실을 계산할 수 있다. 장치는 계산된 제2 손실에 기초하여 파라미터를 교정할 수 있다. 장치는 미학습 발화 정보에 기초하여 미학습 발화 정보가 분할된 적어도 하나의 음성 프레임 각각에 대한 피처 벡터를 생성할 수 있다. 장치는 피처 벡터에 기초하여 미학습 발화 임베딩 벡터를 생성할 수 있다. 장치는 피처 벡터에 기초하여 피처 벡터의 수와 동일한 수의 프레임 임베딩 벡터를 생성할 수 있다. 장치는 프레임 임베딩 벡터에 기초하여 미학습 발화 임베딩 벡터를 생성할 수 있다. 장치는 입력 발화 정보가 제2 인코더의 학습 데이터베이스에 포함되는 것인지 판단할 수 있다. 장치는 입력 발화 정보가 데이터베이스에 포함되지 않는 경우, 입력 발화 정보를 미학습 발화 정보로 하여, 미학습 발화 임베딩 벡터를 생성하고, 입력 발화 정보가 데이터베이스에 포함되는 경우, 입력 발화 정보 에 기초하여 학습 발화 임베딩 벡터를 생성할 수 있다. 미학습 발화 임베딩 벡터의 생성은, 제1 인코더에 의하여 수행되고, 학습 발화 임베딩 벡터의 생성은, 제2 인코 더에 의하여 수행될 수 있다. 장치는 특정 자연 언어로 구성된 텍스트 및 미학습 발화 임베딩 벡터에 기초하여 음성 신호를 생성할 수 있다. 도 9는 일 실시예에 따른 미학습 화자의 음성을 합성하는 장치의 블록도이다. 도 9에 도시된 장치는 도 2 에 도시된 음성 합성 시스템 또는 도 6에 도시된 장치에 대응될 수 있다. 도 9를 참조하면, 장치는 프로세서 및 메모리를 포함함으로써 음성 합성 시스템 또는 장치 등을 구현할 수 있다. 도 9의 장치에는 실시예와 관련된 구성요소들 만이 도시되어 있으므로, 도 9"}
{"patent_id": "10-2023-0078911", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "에 도시된 구성요소들 외에 다른 범용적인 구성요소들이 더 포함될 수 있음을 당해 기술분야의 통상의 기술자라 면 이해할 수 있다. 프로세서는 장치의 전반적인 동작을 제어한다. 예를 들어, 프로세서는 메모리에 저장된 프 로그램들을 실행함으로써, 입력부(미도시), 디스플레이(미도시), 통신 모듈(미도시), 메모리 등을 전반적 으로 제어할 수 있고, 장치의 동작을 제어할 수 있다. 예를 들어, 프로세서는 미학습 발화 정보를 획득하고, 상기 미학습 발화 정보를 이용하여 미학습 발화 임 베딩 벡터를 생성하되, 상기 생성은, 전이 학습(Transfer Learning)된 제1 인코더에 의하여 수행되고, 상기 제1인코더는 사전 학습(Pre-trained)된 제2 인코더로부터 출력된 정보를 이용하여 전이 학습될 수 있다. 프로세서가 동작하는 구체적인 예는 도 1 내지 도 8을 참조하여 상술한 바와 동일하다. 따라서, 이하에서 는 프로세서의 동작에 대한 구체적인 설명은 생략한다. 프로세서는 ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서 (microprocessors), 기타 기능 수행을 위한 전기적 유닛 중 적어도 하나를 이용하여 구현될 수 있다. 메모리는 장치 내에서 처리되는 각종 데이터들을 저장하는 하드웨어로서, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수 있다. 메모리는 DRAM(dynamic random access memory), SRAM(static random access memory) 등과 같은 RAM(random access memory), ROM(read-only memory), EEPROM(electrically erasable programmable read-only memory), CD-ROM, 블루레이 또는 다른 광학 디스크 스토리지, HDD(hard disk drive), SSD(solid state drive), 또는 플래시 메모리를 포함할 수 있다. 또한, 본 개시에 따른 실시 예는 컴퓨터 상에서 다양한 구성요소를 통하여 실행될 수 있는 컴퓨터 프로그램의 형태로 구현될 수 있으며, 이와 같은 컴퓨터 프로그램은 컴퓨터로 판독 가능한 매체에 기록될 수 있다. 이때, 매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플 롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등과 같은, 프로그램 명령어를 저장하고 실행하도록 특별히 구성된 하드웨어 장치를 포함할 수 있다. 한편, 상기 컴퓨터 프로그램은 본 개시를 위하여 특별히 설계되고 구성된 것이거나 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수 있다. 컴퓨터 프로그램의 예에는, 컴파일러에 의하여 만들어지는 것 과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용하여 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함될 수 있다. 일 실시예에 따르면, 본 개시의 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 또는 두 개의 사용자 장치들 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 한편, 본 개시에 따른 방법을 구성하는 단계들에 대하여 명백하게 순서를 기재하거나 반하는 기재가 없다면, 상 기 단계들은 적당한 순서로 행해질 수 있다. 반드시 상기 단계들의 기재 순서에 따라 본 개시가 한정되는 것은 아니다. 본 개시에서 모든 예들 또는 예시적인 용어(예들 들어, 등등)의 사용은 단순히 본 개시를 상세히 설명 하기 위한 것으로서 특허청구범위에 의해 한정되지 않는 이상 상기 예들 또는 예시적인 용어로 인해 본 개시의 범위가 한정되는 것은 아니다. 또한, 당업자는 다양한 수정, 조합 및 변경이 부가된 특허청구범위 또는 그 균등 물의 범주 내에서 설계 조건 및 팩터에 따라 구성될 수 있음을 알 수 있다. 따라서, 본 개시의 사상은 상기 설명된 실시 예에 국한되어 정해져서는 아니 되며, 후술하는 특허청구범위 뿐만 아니라 이 특허청구범위와 균등한 또는 이로부터 등가적으로 변경된 모든 범위는 본 개시의 사상의 범주에 속한 다고 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9"}
{"patent_id": "10-2023-0078911", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 음성 합성 시스템의 동작을 개략적으로 나타내는 도면이다. 도 2는 음성 합성 시스템의 일 실시예를 나타내는 도면이다. 도 3은 음성 합성 시스템의 합성기의 일 실시예를 나타내는 도면이다. 도 4는 인공 신경망 기반의 인코더의 일 실시예를 나타내는 도면이다. 도 5는 인코더에서 임베딩 벡터를 생성하기 위한 벡터 공간의 일 실시예를 나타내는 도면이다.도 6은 제2 인코더를 이용하여 학습된 제1 인코더를 포함하는 음성 합성 시스템의 일 실시예를 나타내는 도면이 다. 도 7은 발화 정보를 입력으로 수신하여 임베딩 벡터를 생성하는 제1 인코더의 일 실시예를 나타내는 도면이다. 도 8은 일 실시예에 따른 미학습 화자의 음성을 합성하는 방법의 흐름도이다. 도 9는 일 실시예에 따른 미학습 화자의 음성을 합성하는 장치의 블록도이다."}
