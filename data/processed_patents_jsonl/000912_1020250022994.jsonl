{"patent_id": "10-2025-0022994", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0029104", "출원번호": "10-2025-0022994", "발명의 명칭": "인공지능 기반 디지털 미디어 컨텐츠 자동 제작 방법, 장치 및 시스템", "출원인": "주식회사 엘지유플러스", "발명자": "황선희"}}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능 기반의 동영상 컨텐츠 생성 방법에 있어서,입력 영상 및 타겟 스타일(target style)을 스타일 변환 인공지능 모델에 적용함에 의해 상기 입력 영상을 상기타겟 스타일로 변환하는 단계;상기 변환된 영상 내에서 적어도 하나의 전경 영역을 분할하는 단계;상기 분할된 적어도 하나의 전경 영역의 후면에 채워질 배경 영상을 생성하여 기초 배경 영상을 생성하는 단계;및상기 기초 배경 영상 위에, 상기 분할된 적어도 하나의 전경 중 일부가, 입력된 시나리오에 대응하는 움직임을갖도록 배열함에 의해, 동영상 컨텐츠를 생성하는 단계를 포함하되, 상기 적어도 하나의 전경 영역을 분할하는 단계는,상기 입력된 시나리오의 키워드를 추출하는 단계; 및상기 추출된 키워드에 대응하는 동적 객체를 상기 입력 영상 또는 상기 변환된 영상 내에서 추출하여 상기 적어도 하나의 전경 영역을 분할하는 단계를 포함하고,도메인 분류기 및 FID(Frechet Inception Distance)를 사용하여 상기 스타일 변환 인공지능 모델에 대한 품질분석을 수행하고, 상기 품질 분석의 수행 결과를 기반으로 상기 입력 영상을 상기 타겟 스타일로 변환하는, 인공지능 기반의 동영상 컨텐츠 생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 입력 영상은 사용자가 촬영하거나 수집한 영상 또는 사진을 포함하는, 인공지능 기반의 동영상 컨텐츠 생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서, 상기 타겟 스타일은, 스타일 추천 인공지능 모델을 이용하여, 상기 입력된 시나리오 및 상기 생성된 동영상 컨텐츠의 시청 대상자의 연령과 관련된 정보 중 적어도 하나를 기반으로 추천되는, 인공지능 기반의 동영상 컨텐츠 생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서, 상기 적어도 하나의 전경 영역을 분할하는 단계는,동적 객체 영역 인식 인공지능 모델을 이용하여 상기 입력 영상 또는 상기 변환된 내에서 상기 적어도 하나의전경 영역을 추출하는 단계; 및그래프 컷(graph cut) 알고리즘을 이용하여, 상기 변환된 영상 내에서 상기 추출된 적어도 하나의 전경 영역을분할하는 단계를 포함하는, 인공지능 기반의 동영상 컨텐츠 생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서, 상기 기초 배경 영상을 생성하는 단계는,인페인팅(inpainting) 기반의 배경 자동 생성 인공지능 모델을 이용하여 상기 분할된 적어도 하나의 전경 영역의 후면에 채워질 배경 영상을 생성함에 의해 상기 기초 배경 영상을 생성하는 단계를 포함하는, 인공지능 기반공개특허 10-2025-0029104-3-의 동영상 컨텐츠 생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서,상기 인페인팅 기반의 배경 자동 생성 인공지능 모델의 학습 데이터는 동적 객체 영역과 배경 영역을 포함하는학습용 영상 데이터에서 동적 객체 영역을 제외한 배경 영역을 마스킹 처리함에 의해 생성된 데이터를포함하는, 인공지능 기반의 동영상 컨텐츠 생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 5 항에 있어서,상기 인페인팅 기반의 배경 자동 생성 인공지능 모델은 적대적 생성 신경망을 통해 도메인을 구분하지 못하도록학습된 인공지능 모델인, 인공지능 기반의 동영상 컨텐츠 생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서, 상기 동영상 컨텐츠를 생성하는 단계는,상기 적어도 하나의 전경 중 적어도 일부, 상기 기초 배경 영상 및 상기 입력된 시나리오를 시나리오 기반 동작추천 인공지능 모델에 적용하여 상기 동영상 컨텐츠를 생성하는 단계를 포함하는, 인공지능 기반의 동영상 컨텐츠 생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항에 있어서, 상기 동영상 컨텐츠를 생성하는 단계는,상기 입력된 시나리오에서 동적 객체와 연관된 키워드를 추출하는 단계;동작 추천 인공지능 모델을 이용하여 상기 추출된 동적 객체와 연관된 키워드에 대응하는 움직임을 추천하는 단계:상기 적어도 하나의 전경 중 일부를 상기 추출된 동적 객체와 연관된 키워드에 연관시킴에 의해, 상기 적어도하나의 전경 중 일부가 상기 추천된 움직임을 갖도록 전경 동영상을 생성하는 단계; 및상기 생성된 전경 동영상과 상기 기초 배경 영상을 합성하는 단계를 포함하는, 인공지능 기반의 동영상 컨텐츠생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서, 상기 움직임을 추천하는 단계는,상기 입력된 시나리오에서 상기 추출된 동적 객체와 연관된 명사 키워드와 상기 명사 키워드에 대응하는 동사키워드를 추출하는 단계; 및상기 추출된 동사 키워드를 이용하여, 상기 명사 키워드와 연관된 동적 객체에 대해, 그에 대응하는 움직임을추천하는 단계를 포함하는, 인공지능 기반의 동영상 컨텐츠 생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 1 항에 있어서,상기 입력 영상은 e-book의 일부 및 그림을 포함하는 책의 캡쳐된 영상 데이터 중 적어도 하나를 포함하는, 인공지능 기반의 동영상 컨텐츠 생성 방법."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "인공지능 기반의 동영상 컨텐츠 생성 장치에 있어서,입력 영상 및 상기 동영상 컨텐츠의 시나리오에 대한 정보를 입력받는 입력부; 상기 입력 영상 및 타겟 스타일(target style)을 스타일 변환 인공지능 모델에 적용함에 의해 상기 입력 영상을공개특허 10-2025-0029104-4-상기 타겟 스타일로 변환하고, 상기 변환된 영상 내에서 적어도 하나의 전경 영역을 분할하며, 상기 분할된 적어도 하나의 전경 영역의 후면에 채워질 배경 영상을 생성하여 기초 배경 영상을 생성하고, 상기 기초 배경 영상 위에, 상기 분할된 적어도 하나의 전경 중 일부가, 입력된 시나리오에 대응하는 움직임을 갖도록 배열함에의해, 상기 동영상 컨텐츠를 생성하는 프로세서; 및 상기 생성된 동영상 컨텐츠를 출력하는 출력부를 포함하되, 상기 프로세서는, 상기 입력된 시나리오의 키워드를 추출하고, 상기 추출된 키워드에 대응하는 동적 객체를 상기 입력 영상 또는 상기 변환된 영상 내에서 추출하여 상기 적어도 하나의 전경 영역을 분할하고,상기 프로세서는, 도메인 분류기 및 FID를 사용하여 상기 스타일 변환 인공지능 모델에 대한 품질 분석을 수행하고, 상기 품질 분석의 수행 결과를 기반으로 상기 입력 영상을 상기 타겟 스타일로 변환하는, 인공지능 기반의 동영상 컨텐츠 생성 장치."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "인공지능 기반의 동영상 컨텐츠 생성 시스템에 있어서,대용량 데이터베이스와 연계하여 적어도 하나의 인공지능 모델을 학습시키는 인공지능 모델 학습 장치, 상기 적어도 하나의 인공지능 모델은 입력 영상으로부터 동영상 컨텐츠 생성과 연관된 모델을 포함함; 및상기 입력 영상 및 상기 동영상 컨텐츠의 시나리오에 대한 정보를 입력받아, 상기 입력 영상 및 타겟 스타일(target style)을 상기 인공지능 모델 학습 장치에서 학습된 스타일 변환 인공지능 모델에 적용함에 의해, 상기입력 영상을 상기 타겟 스타일로 변환하고, 상기 변환된 영상 내에서 적어도 하나의 전경 영역을 분할하며, 상기 분할된 적어도 하나의 전경 영역의 후면에 채워질 배경 영상을 생성하여 기초 배경 영상을 생성하고, 상기기초 배경 영상 위에, 상기 분할된 적어도 하나의 전경 중 일부가, 입력된 시나리오에 대응하는 움직임을 갖도록 배열함에 의해, 동영상 컨텐츠를 생성하는 컨텐츠 자동 제작 장치를 포함하되, 상기 컨텐츠 자동 제작 장치는, 상기 입력된 시나리오의 키워드를 추출하고, 상기 추출된 키워드에 대응하는 동적 객체를 상기 입력 영상 또는 상기 변환된 영상 내에서 추출하여 상기 적어도 하나의 전경 영역을 분할하고,상기 컨텐츠 자동 제작 장치는, 도메인 분류기 및 FID를 사용하여 상기 스타일 변환 인공지능 모델에 대한 품질분석을 수행하고, 상기 품질 분석의 수행 결과를 기반으로 상기 입력 영상을 상기 타겟 스타일로 변환하는, 인공지능 기반의 동영상 컨텐츠 생성 시스템."}
{"patent_id": "10-2025-0022994", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13 항에 있어서,상기 적어도 하나의 인공지능 모델 중 적어도 일부에 의해 도출된 결과 데이터는 상기 적어도 하나의 인공지능모델 중 적어도 일부에 학습 데이터로써 재사용되는, 인공지능 기반의 동영상 컨텐츠 생성 시스템."}
{"patent_id": "10-2025-0022994", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 양태는, 인공지능 기반의 동영상 컨텐츠 생성 방법을 개시하고 있다. 상기 방법은, 입력 영상 및 타겟 스타일(target style)을 스타일 변환 인공지능 모델에 적용함에 의해 상기 입력 영상을 상기 타겟 스타일로 변환하는 단계, 상기 변환된 영상 내에서 적어도 하나의 전경 영역을 분할하는 단계, 상기 분할된 적어도 하나의 (뒷면에 계속)"}
{"patent_id": "10-2025-0022994", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 컨텐츠 자동 제작 방법에 관한 것으로, 보다 상세하게는, 인공지는 기반으로 자동으로 컨텐츠를 제작 하는 방법에 관한 것이다."}
{"patent_id": "10-2025-0022994", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래의 AI기술 기반의 미디어 컨텐츠 자동제작 기술들은 주로 입력 키워드 기반의 영상 생성 또는 자연 영상을 만화 그림체로 변환하는 작업에 초점을 두며, 유아동 교육 컨텐츠를 제작하기 위한 시스템은 존재하지 않았다. 구체적으로, 입력 키워드 기반의 영상 생성 기술은 입력된 문장으로부터 각 단어에 해당되는 영상을 개별적으로 검색 및 수집하여, 해당 영상들을 결합하기 위한 기술이며, 컨텐츠 제작자가 각 영상들을 유아동을 대상으로 하 는 일러스트 형태의 그림책으로 제작하기 위해서는 드로잉 작업이 추가적으로 필요하다는 불편한이 존재한다. 또한, 자연 영상을 만화 그림체로 변경하는 기술의 경우, 단순 그림체 변환 기술로만 발전이 되고 있으며, 동영 상 형태로 영상을 제작하기 위한 기술이 존재하지 않아 이 역시 동영상 생성 기술로 활용하기에는 부족한 부분 이 있다. 더욱이, 드로잉 작업을 자동화하는 배경/전경 분할 및 배경자동생성 기술은 객체의 시멘틱 정보를 고려하지 않은 채 개발되고 있으며, 고품질의 결과를 내기 위해서는 사용자의 수작업을 추가로 요구하는 문제점이 있다. 특히, 배경 자동 생성 기술은 주로 자연 영상에 대해 작동되는 모델을 학습하여, 아트 영상에 적용시 도 메인 차이로 인한 품질 저하가 발생하는 문제점이 있다."}
{"patent_id": "10-2025-0022994", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상술한 문제점을 해결하기 위한 본 발명의 일 양태에 따른 목적은, 유아동 디지털 컨텐츠를 제작하는 작업자가 직접 촬영하거나 수집한 사진을 작가의 화풍 컨텐츠들 중 제작 대상 유아동 컨텐츠를 분석하여 추출한 맞춤형 화풍 정보로 변환하고 해당 그림체 기반의 움직이는 유아동 디지털 컨텐츠(E-book 등)를 자동으로 생성하기 위 한 컨텐츠 자동 제작 방법, 장치 및 시스템을 제공하는 것이다."}
{"patent_id": "10-2025-0022994", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명의 일 양태에 따른, 인공지능 기반의 동영상 컨텐츠 생성 방법은, 입력 영 상 및 타겟 스타일(target style)을 스타일 변환 인공지능 모델에 적용함에 의해 상기 입력 영상을 상기 타겟 스타일로 변환하는 단계, 상기 변환된 영상 내에서 적어도 하나의 전경 영역을 분할하는 단계, 상기 분할된 적 어도 하나의 전경 영역의 후면에 채워질 배경 영상을 생성하여 기초 배경 영상을 생성하는 단계 및 상기 기초 배경 영상 위에, 상기 분할된 적어도 하나의 전경 중 일부가, 입력된 시나리오에 대응하는 움직임을 갖도록 배 열함에 의해, 동영상 컨텐츠를 생성하는 단계를 포함하되, 상기 적어도 하나의 전경 영역을 분할하는 단계는, 상기 입력된 시나리오의 키워드를 추출하는 단계 및 상기 추출된 키워드에 대응하는 동적 객체를 상기 입력 영 상 또는 상기 변환된 영상 내에서 추출하여 상기 적어도 하나의 전경 영역을 분할하는 단계를 포함하고, 도메인 분류기 및 FID(Frechet Inception Distance)를 사용하여 상기 스타일 변환 인공지능 모델에 대한 품질 분석을 수행하고, 상기 품질 분석의 수행 결과를 기반으로 상기 입력 영상을 상기 타겟 스타일로 변환할 수 있다. 상기 입력 영상은 사용자가 촬영하거나 수집한 영상 또는 사진을 포함할 수 있다. 상기 타겟 스타일은, 스타일 추천 인공지능 모델을 이용하여, 상기 입력된 시나리오 및 상기 생성된 동영상 컨 텐츠의 시청 대상자의 연령과 관련된 정보 중 적어도 하나를 기반으로 추천될 수 있다. 상기 적어도 하나의 전경 영역을 분할하는 단계는, 동적 객체 영역 인식 인공지능 모델을 이용하여 상기 입력 영상 또는 상기 변환된 내에서 상기 적어도 하나의 전경 영역을 추출하는 단계 및 그래프 컷(graph cut) 알고리 즘을 이용하여, 상기 변환된 영상 내에서 상기 추출된 적어도 하나의 전경 영역을 분할하는 단계를 포함할 수 있다. 상기 기초 배경 영상을 생성하는 단계는, 인페인팅(inpainting) 기반의 배경 자동 생성 인공지능 모델을 이용하 여 상기 분할된 적어도 하나의 전경 영역의 후면에 채워질 배경 영상을 생성함에 의해 상기 기초 배경 영상을 생성하는 단계를 포함할 수 있다. 상기 인페인팅 기반의 배경 자동 생성 인공지능 모델의 학습 데이터는 동적 객체 영역과 배경 영역을 포함하는 학습용 영상 데이터에서 동적 객체 영역을 제외한 배경 영역을 마스킹 처리함에 의해 생성된 데이터를 포함할 수 있다. 상기 인페인팅 기반의 배경 자동 생성 인공지능 모델은 적대적 생성 신경망을 통해 도메인을 구분하지 못하도록 학습된 인공지능 모델일 수 있다. 상기 동영상 컨텐츠를 생성하는 단계는, 상기 적어도 하나의 전경 중 적어도 일부, 상기 기초 배경 영상 및 상 기 입력된 시나리오를 시나리오 기반 동작 추천 인공지능 모델에 적용하여 상기 동영상 컨텐츠를 생성하는 단계 를 포함할 수 있다. 상기 동영상 컨텐츠를 생성하는 단계는, 상기 입력된 시나리오에서 동적 객체와 연관된 키워드를 추출하는 단계, 동작 추천 인공지능 모델을 이용하여 상기 추출된 동적 객체와 연관된 키워드에 대응하는 움직임을 추천 하는 단계, 상기 적어도 하나의 전경 중 일부를 상기 추출된 동적 객체와 연관된 키워드에 연관시킴에 의해, 상 기 적어도 하나의 전경 중 일부가 상기 추천된 움직임을 갖도록 전경 동영상을 생성하는 단계 및 상기 생성된 전경 동영상과 상기 기초 배경 영상을 합성하는 단계를 포함할 수 있다. 상기 움직임을 추천하는 단계는, 상기 입력된 시나리오에서 상기 추출된 동적 객체와 연관된 명사 키워드와 상 기 명사 키워드에 대응하는 동사 키워드를 추출하는 단계 및 상기 추출된 동사 키워드를 이용하여, 상기 명사 키워드와 연관된 동적 객체에 대해, 그에 대응하는 움직임을 추천하는 단계를 포함할 수 있다. 상기 입력 영상은 e-book의 일부 및 그림을 포함하는 책의 캡쳐된 영상 데이터 중 적어도 하나를 포함할 수 있 다. 상기한 목적을 달성하기 위한 본 발명의 다른 양태에 따른, 인공지능 기반의 동영상 컨텐츠 생성 장치는, 입력 영상 및 상기 동영상 컨텐츠의 시나리오에 대한 정보를 입력받는 입력부, 상기 입력 영상 및 타겟 스타일 (target style)을 스타일 변환 인공지능 모델에 적용함에 의해 상기 입력 영상을 상기 타겟 스타일로 변환하고, 상기 변환된 영상 내에서 적어도 하나의 전경 영역을 분할하며, 상기 분할된 적어도 하나의 전경 영역의 후면에 채워질 배경 영상을 생성하여 기초 배경 영상을 생성하고, 상기 기초 배경 영상 위에, 상기 분할된 적어도 하나 의 전경 중 일부가, 입력된 시나리오에 대응하는 움직임을 갖도록 배열함에 의해, 상기 동영상 컨텐츠를 생성하 는 프로세서 및 상기 생성된 동영상 컨텐츠를 출력하는 출력부를 포함하되, 상기 프로세서는, 상기 입력된 시나 리오의 키워드를 추출하고, 상기 추출된 키워드에 대응하는 동적 객체를 상기 입력 영상 또는 상기 변환된 영상 내에서 추출하여 상기 적어도 하나의 전경 영역을 분할하고, 상기 프로세서는, 도메인 분류기 및 FID를 사용하 여 상기 스타일 변환 인공지능 모델에 대한 품질 분석을 수행하고, 상기 품질 분석의 수행 결과를 기반으로 상 기 입력 영상을 상기 타겟 스타일로 변환할 수 있다. 상기한 목적을 달성하기 위한 본 발명의 또 다른 양태에 따른, 인공지능 기반의 동영상 컨텐츠 생성 시스템은, 대용량 데이터베이스와 연계하여 적어도 하나의 인공지능 모델을 학습시키는 인공지능 모델 학습 장치(상기 적 어도 하나의 인공지능 모델은 입력 영상으로부터 동영상 컨텐츠 생성과 연관된 모델을 포함함) 및 상기 입력 영 상 및 상기 동영상 컨텐츠의 시나리오에 대한 정보를 입력받아, 상기 입력 영상 및 타겟 스타일(target style) 을 상기 인공지능 모델 학습 장치에서 학습된 스타일 변환 인공지능 모델에 적용함에 의해, 상기 입력 영상을 상기 타겟 스타일로 변환하고, 상기 변환된 영상 내에서 적어도 하나의 전경 영역을 분할하며, 상기 분할된 적 어도 하나의 전경 영역의 후면에 채워질 배경 영상을 생성하여 기초 배경 영상을 생성하고, 상기 기초 배경 영 상 위에, 상기 분할된 적어도 하나의 전경 중 일부가, 입력된 시나리오에 대응하는 움직임을 갖도록 배열함에 의해, 동영상 컨텐츠를 생성하는 컨텐츠 자동 제작 장치를 포함하되, 상기 컨텐츠 자동 제작 장치는, 상기 입력 된 시나리오의 키워드를 추출하고, 상기 추출된 키워드에 대응하는 동적 객체를 상기 입력 영상 또는 상기 변환 된 영상 내에서 추출하여 상기 적어도 하나의 전경 영역을 분할하고, 상기 컨텐츠 자동 제작 장치는, 도메인 분 류기 및 FID를 사용하여 상기 스타일 변환 인공지능 모델에 대한 품질 분석을 수행하고, 상기 품질 분석의 수행 결과를 기반으로 상기 입력 영상을 상기 타겟 스타일로 변환할 수 있다. 상기 적어도 하나의 인공지능 모델 중 적어도 일부에 의해 도출된 결과 데이터는 상기 적어도 하나의 인공지능 모델 중 적어도 일부에 학습 데이터로써 재사용될 수 있다."}
{"patent_id": "10-2025-0022994", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 컨텐츠 자동 제작 방법, 장치 및 시스템에 따르면, 작업자의 상당한 수작업 시간을 요구하는 드로잉 작업에 대해, 인공지능 모델 기반의 영상 일러스트화 및 배경영상 생성을 적용하여 영상제작 시간 효율성을 높 이고, 드로잉 분야 비전문가가 손쉽게 유아동 교육 컨텐츠 제작이 가능하도록 하는 효과가 있다. 특히, 드로잉 비전문가의 유아동 디지털 컨텐츠 제작 어플리케이션으로 활용하여 인공지능 모델이 드로잉작업 등 영상 편집에 필요한 작업을 자동화해 줌에 따라, 유아동 대상 교육업 종사자, 학부모, 유아동이 직접 촬영 또는 수집한 영상을 기반으로 동영상 컨텐츠를 효율적으로 제작하는 효과가 있다. 또한, 정지 화면으로 제작된 동화책, 만화책 등을 움직이는 디지털 컨텐츠 제작에 하여, 쉽게 동영상 컨텐츠를 제작하는 효과가 있다. 더욱이, 동영상 컨텐츠 제작에 사용된 영상 및 텍스트 데이터를 동영상 컨텐츠 자동 제작에 필요한 다양한 인공 지능 모델의 학습데이터로 재사용하여 인공지능 모델의 성능을 고도화시킬 수 있는 효과도 가질 수 있다."}
{"patent_id": "10-2025-0022994", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포 함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제 1, 제 2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용 된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제 1 구성요소는 제 2 구성요소로 명명될 수 있고, 유사하게 제 2 구성요소도 제 1 구성요소로 명명될 수 있다. 및/또는 이라는 용어는 복수의 관련된 기재된 항목 들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 본 출원에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조 합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미 를 가진 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의 미로 해석되지 않는다. 이하, 첨부한 도면들을 참조하여, 본 발명의 바람직한 실시예를 보다 상세하게 설명하고자 한다. 본 발명을 설 명함에 있어 전체적인 이해를 용이하게 하기 위하여 도면상의 동일한 구성요소에 대해서는 동일한 참조부호를 사용하고 동일한 구성요소에 대해서 중복된 설명은 생략한다. 도 1은 본 발명의 일 실시예에 따른 컨텐츠 자동 제작 장치의 구성을 나타낸 블록도이다. 도 1에 도시된 바와 같이, 본 발명의 일 실시예에 따른 컨텐츠 자동 제작 장치는, 입력부, 프로세서, 인공지능 모델 학습 부, 동영상 출력부를 포함할 수 있다. 도 1을 참조하면, 컨텐츠 자동 제작 장치는 인공지능 기반으로 특정 연령대(예를 들어, 유아동 연령대, 청소년 연령대, 중년 연령대, 노인 연령대 등)에 맞춤형 디지털 미디어 컨텐츠(예를 들어, E-book 등으로 활용가능한 일러스트 영상 또는 동영상을 포함하며, 이하 \"동영상 컨텐츠\" 또는 \"동영상\"이라 부를 수 있음)를 자동 제작하 는 장치이며, 이는 컴퓨팅 장치로 구현될 수 있다. 이하, 컨텐츠 자동 제작 장치를 \"장치\"라고 부를 수 있다. 입력부는 사용자로부터 디지털 미디어 컨텐츠의 기본 정보를 입력하는 구성요소이다. 이때, 상기 디지털 미디어 컨텐츠의 기본 정보는 대상 연령에 대한 정보 및 동영상의 시나리오 정보와 같은 텍스트 정보 뿐만 아니 라 사용자가 촬영 또는 수집한 영상 정보를 포함한다. 또한, 상기 컨텐츠의 타겟 스타일(화풍) 영상도 포함할 수 있다. 타겟 스타일(화풍)이란 예를 들어, 만화, 일러스트화, 풍경 스타일, 겨울/여름 스타일, 스케치 스타일, 수채화, 모노크롬, 구름, 모자이크, 뭉크, 피카소, 고흐, 세잔느, 몬드리안 등 특정 그림의 스타일 또 는 특정 작가의 스타일과 같은 영상의 화풍(스타일)을 지칭하는 것이다. 본 발명의 실시예에서, 타겟 스타일은 특정 화풍의 영상을 입력하는 방식을 이용하여 사용자가 직접 지정할 수도 있지만, 대용량 타겟 스타일 영상 후 보들을 이용하여 컨텐츠 기본 정보에 대응하는 맞춤형 화풍을 추론하는 인공지능 모델에 의해 결정될 수도 있다. 프로세서는 입력부를 통해 입력된 자연 영상을 타겟 스타일로 변환하고, 변환된 영상에서 적어도 하 나의 전경들을 분할하며, 분할된 전경들의 후면의 배경 영역을 자동 생성하여 채워줌에 의해 기초 배경 영상을 생성하고, 생성된 기초 배경 영상 상에 앞서 분할한 전경들 중 적어도 일부에 대해, 사용자에 의해 입력된 시나 리오에 대응하는 움직임을 부여하여 동영상을 생성하는 구성요소이다. 프로세서는 스타일 변환부, 전 경 분할부, 배경영역 생성부 및 동영상 생성부를 포함할 수 있다. 보다 구체적으로, 스타일 변환부는 화풍 추천 (인공지능) 모델을 활용하여 컨텐츠 기본 정보에 대응하는 최적 타겟 스타일을 추론하고, 스타일 변환 (인공지능) 모델을 활용하여 입력된 자연 영상을 상기 추론된 최적 타겟 스타일로 변환할 수 있다. 예를 들어, 자연 영상(실제 사진)을 일러스트 영상으로 변환 생성할 수 있다. 이때, 사용자가 지정한 타겟 스타일이 입력된 경우, 상기 화풍 추천 모델을 활용한 추론 과정은 필요하지 않을 수 있다. 전경 분할부는 상기 스타일 변환부를 통해 타겟 스타일로 변환된 영상에서 전경(객체를 포함함)을 추 출하여 분리한다. 이때, 동적 객체 추출 (인공지능) 모델을 사용하여 타겟 스타일의 영상에서 동적으로 움직일 수 있는 객체를 추출한 후, 해당 객체를 분할할 수 있다. 이때, 그래프컷 기반의 정교한 객체 분할 알고리즘이 적용될 수 있다. 배경 영역 생성부은 상기 전경 분할부를 통해 분할된 전경 영역의 후면에 해당되는 배경 영역을 자동 생성한다. 이때, 도메인 비제한적인 배경 영상 자동 생성 (인공지능) 모델을 이용하여 배경 영역을 생성할 수 있다. 배경 영역을 채우고 나면, 동영상의 기본 배경 영상이 완성된 것이라고 볼 수 있다. 동영상 생성부는 컨텐츠 기본 정보로 입력된 시나리오에 맞춰 전경 분할부에서 분할된 전경의 움직임 을 매칭시키고, 매칭된 움직임에 맞는 동영상을 생성한다. 동영상은 상기 배경 영역 생성부에서 생성된 기 초 배경 영상 위에 움직임을 갖는 전경 영역을 합성함에 의해 생성될 수 있다. 이때, 영상 스티칭(video stitching) 기술이 사용될 수 있다. 동영상 출력부는 동영상 생성부를 통해 생성된 동영상을 출력하는 구성이다. 이는 재생부라고 부를 수 있다. 이는 모니터, 터치스크린 등과 같은 디스플레이 수단을 포함할 수 있다. 또는 HDMI(High Definition Multimedia Interface) 및 DVI(Digital Video/Visual Interface)과 같이 타 디스플레이 장치로의 연결을 위한 인터페이스 수단을 포함할 수 있다. 인공지능 모델 학습부는 적어도 하나의 인공지능 모델들을 학습시키는 구성요소이다. 이는 프로세서 내에 구현되어 하나의 프로세서에서 상기 적어도 하나의 인공지능 모델을 학습시키는 형태로 구현될 수 있다. 또는, 하나의 장치 내에 프로세서와 다른 병렬 프로세서로 구현될 수 있다. 또 다른 예에서, 인공지능 모 델 학습부는 별도의 컴퓨팅 장치로 구현되어 컨텐츠 자동 제작 장치의 프로세서와 연동하는 방식으로 동작할 수 있다. 프로세서는 필요한 기능에 대응하는 인공지능 모델을 인공지능 모델 학습부로부터 로딩(loading)하여 실행시킴에 의해 필요한 기능을 수행할 수 있다. 한편, 인공지능 모델 학습부는 대용량 데이터베이스와 연계하여 적어도 하나의 인공지능 모델들을 학 습시킨다. 대용량 데이터베이스는 네트워크(미도시)와 연계된 검색 엔진을 포함할 수 있다. 대용량 데이터 베이스는 대량의 영상 데이터, 텍스트 데이터 등을 저장하고 있다가 제공 인공지능 모델의 학습데이터로써 제공할 수 있다. 인공지능 모델 학습부에서 학습되는 상기 적어도 하나의 인공지능 모델은 화풍 추천 인공 지능 모델, 스타일 변환 인공지능 모델, 동적 객체 영역 인식 인공지능 모델, 그래프컷 기반 영역 추출 인공지 능 모델, 인페인팅(inpainting) 인공지능 모델 및 시나리오 기반 동작 추천 인공지능 모델 중 적어도 하나를 포 함할 수 있다. 한편, 본 발명의 일 실시예에 따른 컨텐츠 자동 제작 장치는 대용량 서버 장치로써, 사용자 단말(미도시)과 연 계하는 시스템으로 동작할 수 있다. 예를 들어, 컨텐츠 자동 제작과 관련된 애플리케이션(application)(이하, \"앱\"이라 부를 수 있음) 또는 웹 사이트 등을 운영하고, 앱 또는 웹 사이트에 접속한 사용자 단말로부터 컨텐 츠 자동 제작 요청을 받아 그에 대한 응답으로 컨텐츠를 자동 제작하여 전달하는 형태로 동작할 수 있다. 이때, 사용자 단말로부터의 컨텐츠 제작 요청에는 대상 연령, 시나리오, 입력 영상 등의 컨텐츠 기본 정보가 포함되어 있을 수 있다. 이러한 앱 또는 웹을 이용하여 인공지능 모델이 드로잉 작업 등 영상 편집에 필요한 작업을 자동 화해주어, 유아동 대상 교육업 종사자, 학부모, 유아동이 직접 촬영 또는 수집한 영상을 기반으로 동영상 컨텐 츠를 제작할 수 있도록 한다. 추가적으로, 배경 및 전경 분할 기술, 배경 영상 자동 생성 기술 및 시나리오 기반 동영상 제작 기술을 이용하 여, 정지 화면으로 제작된 동화책 및/또는 만화책 등을 움직이는 디지털 컨텐츠로 제작할 수 있다. 즉, e- book의 일 화면 및/또는 그림을 포함하는 동화책 및 만화책을 캡쳐한 화면 데이터를 입력 영상으로 하여, 입력 영상 내에 포함된 텍스트를 동영상의 시나리오로 이용함에 의해, 본 발명의 일 실시예에 따른 컨텐츠 자동 제작 방법을 이용하여 디지털 컨텐츠를 제작할 수 있는 것이다. 더욱이, 스타일 변환 과정을 통해 출력된 영상을 배경 영상 자동 생성 인공지능 모델의 학습데이터로 활용하여 인공지능 모델의 성능을 고도화하는 것도 가능하다. 이러한 인공지능 모델의 결과 데이터의 재학습은 추후 보다 상세히 설명하도록 한다. 도 2a는 컨텐츠 기본 정보를 기반으로 맞춤 화풍을 추천하는 방법을 나타낸 흐름도이다. 도 2a를 참조하면, 장치(인공지능 모델 학습부)는 컨텐츠 기본정보를 입력받아 입력정보를 기반으로 화풍 추천 모델을 학습시킬 수 있다(S210). 여기서, 컨텐츠 기본 정보는 유아동 컨텐츠를 위한 기본 정보로, 대상 연령 및 시나리오를 포함할 수 있다. 다만, 반드시 유아동에 연령대를 맞출 필요는 없고, 청소년, 장년, 중년 및 노년 등 다양한 연령대를 선택하여 컨텐츠를 자동 제작을 수행해도 무방하다. 화풍 추천 모델은 대용량의 연령 및 시나리오 정보에 대응하는 화풍 정보를 하나의 학습 데이터 셋으로 하여 다 수의 학습데이터를 기반으로 학습될 수 있다. 이때, 화풍 추천 모델은 시나리오의 형태소를 분석하여 그 의미를 파싱(parsing)하고, 파싱된 의미에 따라 감정 및/또는 키워드를 추론한다. 그리고는 해당 감정 및/또는 키워드 에 대응하는 화풍을 추론할 수 있다. 예를 들어, 시나리오에 \"여행\"이라는 단어가 존재할 때, 장치는 여행과 관 련된 감정 및/또는 키워드로, \"모험\", \"호기심\", \"판타지\"라는 단어를 추론할 수 있다. 그리고는, 추론된 감정 및/또는 키워드를 기반으로 그에 대응하는 화풍을 추론할 수 있는 것이다. 장치는 테스트 데이터를 기반으로 추천 정확도를 산출하여 미리 설정된 임계치와 비교한다(S220). 임계치보다 높지 못하면, 계속하여 더 많은 학습 데이터를 기반으로 학습을 수행하고, 임계치보다 높으면, 화풍 추천 모델 에 대한 학습을 종료한다(S230). 이렇게 학습된 화풍 추천 모델은 도 2b의 단계(S265)에서 사용된다. 이는 도 2b를 참조하여 보다 상세히 설명한다. 도 2b는 도 2a의 추천 화풍과 작업자가 입력한 자연영상을 이용하여 타겟 화풍 영상을 생성하는 방법을 나타낸 흐름도이다. 도 2b를 참조하면, 장치(인공지능 모델 학습부)는 데이터베이스로부터 대용량의 자연 영상(다른 종류의 영상(그 림, 아트 영상, 컴퓨터 그래픽, 캡쳐 화면 등)이어도 무방함) 및 타겟 화풍(Artistic) 영상 후보에 대한 정보를 입력하여 스타일 변환 모델을 학습시킨다(S240). 스타일 변환 모델은 자연 영상을 타겟 화풍의 영상으로 변환시 키는 인공지능 모델이다. 이는 회화 데이터베이스를 기반으로 특정 화가의 작품 스타일을 익혀 입력 영상(예를 들어, 사진)을 화풍에 따른 그림으로 변환할 수 있다. 예를 들어, 광화문 사진을 고흐 풍의 그림으로 변환시킬 수 있다. 장치는 스타일 변환 모델을 학습하면서 테스트 학습 데이터 셋을 이용하여 품질 분석을 수행할 수 있 다. 인공지능 모델의 품질 분석에는 도메인 분류기 및 FID(Frechet Inception Distance)가 사용될 수 있다. 즉, 스타일 변환 모델은 도메인 분류기 및 FID를 이용하여 품질을 측정한다. 여기서, FID는 실제 이미지 그룹과생성 이미지 그룹의 유사도를 측정하는 지표이다. 그리고는, 측정된 품질을 미리 설정된 임계치와 비교한다 (S250). 이때, 개별 인공지능 모델에 대한 임계치는 서로 다를 수 있다. 즉, 도 2a의 화풍 추천 모델에 대한 임 계치와 스타일 변환 모델에 대한 임계치는 서로 다를 수 있다. 다만, 동일한 값을 가져도 무방하다. 학습된 스 타일 변환 모델의 품질이, 임계치보다 낮으면, 학습 단계(S240)로 돌아가 학습을 지속해 나가고, 임계치보다 높 으면, 장치(스타일 변환부)는 학습된 변환 모델을 이용하여 스타일 변환을 진행한다(S260). 이때, 사용자가 입 력한 입력 영상이 사용된다. 이는 사용자가 직접 촬영 또는 수집한 자연 영상(예를 들어, 사진)을 포함할 수 있 다. 그리고 위 입력된 자연 영상을 변환시킬 대상인 타겟 스타일(화풍) 정보가 요구되는데, 이는 사용자가 입력 한 컨텐츠 기본정보(연령 및 시나리오)에, 도 2a에서 학습된 화풍 추천 모델을 적용하여 추론된 화풍 정보를 입 력함에 의해 달성될 수 있다(S265). 만약, 사용자가 직접 화풍 영상을 입력한다면, 도 2a 및 단계(S265)의 과정 이 필요하지 않을 수 있다. 학습 완료된 스타일 변환 모델에 상기 자연 영상 및 추천 화풍 정보가 입력되면, 장치는 스타일 변환 모델을 이 용하여 자연 영상을 추천 화풍의 스타일로 변환시킨다(S260). 그리고는, 변환된 타겟 화풍 영상을 출력한다 (S270). 도 3은 도 2a 및 도 2b의 방법에 따라 자연 영상을 타겟 화풍 영상으로 생성한 도면을 예시적으로 나타낸 예시 도이다. 도 3을 참조하면, 사용자는 유아동 컨텐츠의 기본 정보로, 대상 연령을 5세로, 시나리오는 강아지가 초원 을 배경으로 여행하는 내용을 장치(입력부)에 입력할 수 있다. 사용자는 입력 영상으로 강아지가 초원에 있고, 후면에 나무들이 무성한 자연 영상을 장치(입력부)에 입력할 수 있다. 그리고, 추천 화풍으로는 모 험, 호기심 및 판타지 스타일의 일러스트 화풍이 추론 또는 지정될 수 있다. 장치(스타일 변환부)는 스타일 변 환 모델을 이용하여 상기 자연 영상을 추천 화풍으로 변환하여 변환된 영상을 생성할 수 있다. 도 4는 도 1의 전경 분할부가 배경 및 전경 영상을 분할하는 방법을 나타낸 흐름도이다. 도 4를 참조하면, 장치(인공지능 모델 학습부)는 데이터베이스로부터 대용량 영상 및 텍스트 데이터를 입력으로 받아 동적 객체 영역 인식 모델을 학습시킨다(S410). 여기서, 영상 및 텍스트 데이터에는 동적 객체가 포함되어 있는 것이 바람직하다. 이는 동영상의 움직이는 객체로 활용될 수 있기 때문이다. 동적 객체 영역 인식 모델은 사람, 동물, 나무, 오토바이, 트럭, 버스 등 움직임을 가질 수 있는 객체를 검출하는 인공지능 모델이다. 이러 한 인공지능 모델로는 CNN(convolutional neuron network)을 이용한 방법으로, R-CNN, fast R-CNN, faster R- CNN, YOLO, YOLOv2, YOLOv, SSD 등이 사용될 수 있다. 장치가 동적 객체 영역 인식 모델을 학습시키고 나면, 테스트 학습 데이터 셋을 이용해 영역 인식 정확도를 분 석하고 이를 임계치와 비교한다(S420). 학습된 동적 객체 영역 인식 모델의 정확도가, 임계치보다 낮으면, 학습 단계(S410)로 돌아가 학습을 지속해 나가고, 임계치보다 높으면, 장치(전경 분할부)는 사용자가 입력한 자연 영 상 및 컨텐츠 기본 정보(예를 들어, 시나리오)를 입력하여 학습된 동적 객체 영역 인식 모델을 이용함에 의해 그에 따른 추론을 진행한다(S430). 추론 결과, 자연 영상 내의 동적 객체 영역을 추출할 수 있다. 이때, 동적 영역은 복수 개일 수 있다. 장치(전경 분할부)는 추출된 동적 객체 영역을 영상으로부터 추출할 수 있다(S440). 이때, 그래프 컷 알고리즘 (graph cut algorithm)을 이용하여 정교하게 객체 영역을 분리하는 것이 바람직하다. 그래프 컷 알고리즘을 이 용한 객체와 배경 영역을 분리하는 과정은 다음과 같다. 입력 영상 내에서 객체와 배경으로 분리하고자 하는 영 역이 선택되면, 가장 먼저 선택된 영역 내의 히스토그램과 영역 밖의 히스토그램을 이용하여 두 히스토그램이 가장 잘 분별되는 특징값을 찾는다. 이때 히스토그램의 분별력 평가를 위한 척도로는 로그 함수의 분산율을 사 용하는 것이 바람직하다. 그리고는, 상기 특징값을 이용하여 로그-우도비(LLR: Log-Likelihood Ratio) 영상을 생성한 후 미디언(Median) 필터링을 수행한다. 마지막으로, 로그 우도비 영상으로부터 객체와 배경에 대한 히스 토그램을 생성하여 그래프 컷 알고리즘의 데이터 항 입력으로 사용함에 따라 선택된 영역을 객체와 배경으로 구 분할 수 있다. 위와 같은 그래프컷 알고리즘을 이용하여 정교하게 타겟 화풍 영상으로부터 객체 영역을 추출하 고 나면, 타겟 화풍 영상의 배경 및 전경 영역이 분할된 상태로 출력될 수 있다(S450). 본 발명의 실시예에 따르면, 도 4의 과정에서 동적 객체 영역 인식 및 추출은 입력 영상(예를 들어, 자연 영상) 및 타겟 화풍 영상 중 적어도 하나에 의해 수행될 수 있다. 두 과정 모두 자연 영상에서 수행될 수도 있고, 두 과정 모두 타겟 화풍 영상에서 수행될 수도 있다. 바람직하게는, 영역 인식 및 추출을 자연 영상에서 수행하고, 추출된 영역의 좌표를 그대로 타겟 화풍 영상에 대입하여 타겟 화풍 영상에서 전경 및 배경 영역을 추출 및 분리하는 것이 바람직할 수 있다. 도 5는 도 4의 방법에 따라 자연 영상에서 타겟 화풍 영상의 전경 및 배경을 분할한 모습을 예시적으로 나타낸 예시도이다. 도 5를 참조하면, 장치는 자연영상 및 시나리오 중 적어도 하나를 이용하여 동적 객체를 인식한다. 그리고는, 영상 내에서 동적 객체 영역을 추출하고, 이를 그래프 컷 알고리즘을 이용하여 정교하게 분리해낸다. 그리고 나 면, 결과적으로, 타겟 화풍 영상에서 전경 부분(예를 들어, 나무 및 강아지)과 이를 뺀 나머지 배경 부분이 출 력될 수 있다. 도 6은 도 1의 배경 영역 생성부가 배경 영역을 자동을 생성하는 방법을 구체적으로 나타낸 상세 흐름도이다. 도 6을 참조하면, 장치는 도 4의 과정에서와 같이, 대용량 영상 및 텍스트 데이터를 이용하여 자연 영상에서 동 적 객체 영역 추출 단계(S610) 및 그래프컷 기반의 정교한 영역 추출 단계(S620)를 진행한다. 이는 배경 영상 자동 생성 인공지능 모델을 학습시키는 학습 데이터 생성을 위한 전처리 과정이다. 그리고 나서, 장치는 위의 전처리 과정을 기반으로 배경 영상 자동 생성을 위한 실제 학습 데이터를 생성한다 (S630). 이때, 배경 영상 자동 생성을 위한 학습 데이터는 자연 영상에서 동적 객체 영역이 아닌 배경 영역 부 분에 마스킹을 수행한 영상 데이터를 활용한다. 마스킹은 배경 영역 부분에 무작위로 수행될 수 있다. 이러한 학습용 영상 데이터를 다수 개 확보한 후, 확보된 학습용 영상 데이터들을 이용하여 배경 영상 자동 생 성을 위한 인페인팅(inpainting) 모델을 학습시킨다(S640). 인페인팅 모델은 이미지 복원 알고리즘으로, 인공지 능이 지워진 부분을 주변 이미지에 어울리도록 자연스럽게 복구하는 모델이다. 한편, 인페인팅 모델은 적대적 생성 신경망(Generative Adversarial Network)을 통해 배경영역을 복구하도록 학습되는 것이 바람직하다. 적대 적 생성 신경망이란 서로 대립하는 두 시스템이 경쟁하는 방식으로 학습을 진행하는 비지도 학습 방식의 신경망 을 의미한다. 학습 데이터의 분포를 흉내내는 훈련을 반복적으로 시행하면서 진짜 데이터와 모사 샘플을 구분하 는 과정을 무한히 순환하다 보면, 진본과 모사본을 구분하지 못할 정도에 이르게 되는데, 본 발명의 일 실시예 에 따른 인페인팅 모델도 이러한 원리를 이용하여 학습시키는 것이 바람직하다. 학습 과정을 거치면서, 결과 이미지의 생성 품질을 FID 및 SSIM(structural similarity) 측면에서 산출하여 이 를 임계치와 비교한다(S650). SSIM은 인간의 지각적 유사성 판단을 예측함으로써 이미지 유사성을 정량적으로 평가하기 위한 지표이다. 임계치보다 낮으면, 학습 단계(S640)로 돌아가 학습을 지속하고, 임계치보다 높은 품질이 나오면, 도메인 구분 성능을 임계치와 비교한다(S660). 구분 성능이 미리 설정된 임계치보다 낮으면, 즉, 진본과 모사본을 구분하지 못할 정도의 성능으로 인페인팅 모델이 학습되면, 장치(배경 영역 생성부)는 인페인팅 모델의 학습을 중단하고 상기 인페인팅 모델에, 앞서 도 5에서 생성한 전경이 분할된 타겟 화풍의 배경 영상을 입력한다. 그리고는, 인 페인팅 모델을 적용하여(S670), 전경 영역 후면에 배치된 배경 영상을 생성하여 동영상의 바탕이 되는 기본 배 경 영상을 완성 및 출력한다(S680). 도 6의 상기 단계들은 도 7a 내지 도 7b를 참조하여 소년, 소녀 및 사과가 포함된 사진을 통해 인공지능 모델들 을 학습시키는 과정을 예시로 들어 보다 상세히 설명한다. 도 7a는 배경 자동 생성 인공지능 모델의 학습 데이터를 생성하는 과정을 설명하기 위한 개념도이다. 도 7a를 참조하면, 도 4 및 도 5의 방법과 같이, 시나리오를 기반으로 동적 객체 영역을 추출하고 그래프컷 기 반의 정교한 영역 추출을 진행한다. 예를 들어, 시나리오가 \"소녀가 사과를 든 소년을 바라보고 있어요\"일 때, 장치는 동적 객체로, 소년, 사과 및 소녀를 추출할 수 있고, 그래프컷 기반의 정교한 영역 추출 과정을 거쳐 해 당 부분을 정교하게 도려낼 수 있다. 장치는 동적 객체 영역은 배경 데이터 생성 모델의 학습 데이터를 학습시키기 위해 마스킹하는 것은 바람직하지 않기 때문에, 해당 영역의 위치 및 좌표를 기억하고 있다가 해당 부분을 제외한 실질 배경 영역을 대상으로 마 스킹이 수행되도록 제어한다. 예를 들어, 소년, 소녀 및 사과가 포함된 영상에서 소년, 소녀 및 사과 부분을 제 외한 부분에 임의로 마스킹 처리를 수행하여 학습 데이터를 생성한다. 본 발명의 실시예에 따르면, 사용자가 입 력한 입력 영상도 학습 데이터로 재사용할 수 있다. 예를 들어, 도 5의 실시예의 자연영상에서 강아지와 나무 부분을 제외한 나머지 부분에 마스킹 처리를 수행하여 학습데이터로 생성할 수 있다. 도 7b는 도메인 제한 없는 배경 자동 생성 인공지능 모델을 학습하는 과정을 설명하기 위한 개념도이다. 도 7b를 참조하면, 마스킹 처리가 된 학습 데이터는 적대적 생성 신경망을 통해 학습된다. 이를 위해, 생성모델 (Generator)은 학습 데이터 x의 분포를 흉내내는 트레이닝(training)을 반복적으로 시행한다. 그리고 도메인 분 류기(Discriminator)는 생성모델에서 생성한 모사 샘플과 진짜 데이터 x를 구별한다. 명확하게는, 실제 데이터 일 확률을 추정(estimate)한다. 이와 같이, 생성모델과 도메인 분류기를 이용하여 생성과 판별을 무한히 수행하 다 보면, x의 분포를 정확하게 모사하는 수준에 이르게 되고, 결국 도메인 분류기가 진본과 모사본을 구분하지 못하는 상황에 이르게 된다. 본 발명의 실시예에 따르면, 장치는 자연 영상과 아트영상을 구분하지 못하도록 적 대적으로 학습시키고, 임계값 이하의 구분 성능이 나오는 인페인팅 모델이 실제 배경 영상 자동 생성에 활용될 수 있도록 제어한다. 도 8은 도 6의 방법에 따라 타겟 화풍 영상의 자동 생성된 배경 영상의 모습을 예시적으로 나타낸 예시도이다. 도 8을 참조하면, 장치는 도 5의 전경이 분할된 타겟 화풍 영상을 학습이 완료된 인페인팅 모델에 적용하여 도 8의 우측 도면과 같이, 전경 영역 후면의 배경 영역을 자연스럽게 복구한 동영상 기초 영상을 생성한다. 이는 움직임을 갖게 되는 전경의 후면에 배열되어 결국 동영상 컨텐츠의 배경으로서 역할을 하는 기초 영상(기초 배 경 영상이라 부를 수 있음)이다. 도 9는 도 1의 동영상 생성부가 입력 시나리오를 기반으로 동영상을 생성하는 과정을 나타낸 흐름도이다. 도 9를 참조하면, 장치는 도 8의 전경, 기초 배경 영상 및 시나리오 정보를 기반으로 시나리오 기반 동작 추천 모델을 학습시킨다(S910). 시나리오 기반 동작 추천 모델은 시나리오로부터 추출된 시멘틱 정보(동적 객체 정보 추출 및 활용)를 활용한다. 보다 구체적으로, 이는 시나리오 내의 문장의 형태소를 분석하여 동적 객체에 대응 하는 키워드를 결정하고 결정된 키워드에 대응하는 동작을 추천하는 인공지능 모델이다. 이는 도 10을 통해 보 다 상세히 설명한다. 도 10은 시나리오를 분석하여 전경과 움직임을 매칭하는 과정을 설명하기 위한 개념도이다. 도 10을 참조하면, 장치(인공지능 모델 학습부)는 대용량 전경(동적 객체) 데이터, 배경 영상 데이터 및 시나리 오(텍스트) 데이터를 학습 데이터로 하여, 시나리오 기반 동작 추천 모델을 학습시킨다. 먼저, 장치는 시나리오 텍스트 내 문장의 형태소를 분석한다. 그리고는, 분석되는 형태소에 기초하여 키워드를 결정한다. 이때, 키워드 는 동적 객체에 해당하는 명사 키워드와 동적 객체의 움직임과 관련된 동사 키워드를 포함할 수 있다. 도 10의 실시예에서는, \"나무에 바람이 불어오기 시작하자, 강아지가 멀리 도망가기 시작했어요.\"라는 문장에서, 동적 객체에 해당하는 \"나무\"와 \"강아지\"라는 명사 키워드를 추출할 수 있다. 그리고, 해당 명사 키 워드와 대응하는 동사의 키워드를 결정할 수 있다. 만약, 시나리오 내에 동사 키워드가 없다면, 명사 키워드를 기반으로 이를 유추할 수 있다. \"강아지\"의 경우, \"도망간다\"라는 동사 키워드가 있기에, \"이동\"이라는 동작 키 워드를 추천할 수 있다. 다만, \"나무\"의 경우, 시나리오 내에는 나무의 움직임을 나타내는 동사 키워드가 없다. 하지만, \"나무\"는 네트워크를 통해 나무와 관련된 텍스트 등을 수집하여 나무의 동작을 나타내는 적절한 동사 키워드들을 확보할 수 있고, 이들 중 하나를 나무의 동작으로 추천할 수 있다. 특히, 도 10의 실시예에서는, \" 바람이 분다\"라는 텍스트 내용과 함께 사용되고 있기에, \"바람\"과 \"나무\"가 함께 들어간 문장들을 네트워크 내 의 검색 엔진을 이용하여 추출함에 따라 바람에 의해 흔들리는 나무를 표현하기 위해, \"나무\"의 동작으로 \"흔들 림\"이라는 동작 키워드를 추천할 수 있는 것이다. 이를 통해, \"나무-흔들림\"과 \"강아지-이동\"의 동적 객체-움직 임 쌍을 추론할 수 있다. 다시 도 9로 돌아가서, 위와 같은 방법에 의해 학습된 동작 추천 모델의 동작 추천 성능이 임계치보다 높은지 비교하여(S920), 임계치보다 낮으면, 학습 단계(S910)로 돌아가 학습을 지속시키고, 임계치보다 높으면, 장치 (동영상 생성부)는 전경, 배경 영상 및 시나리오에 대한 동작 추천을 수행하여 그에 대응하는 컨텐츠를 자동 생 성한다(S930). 이때, 나무의 흔들림과 관련된 이미지 또는 동영상은 네트워크를 통해 수집할 수 있고, 수집된 이미지 또는 동영상을 이용하여 나무의 움직임 동영상을 생성할 수 있다. 또한, 강아지의 이동과 관련된 이미지 또는 동영상을 네트워크를 통해 수집할 수 있고, 수집된 이미지 또는 동영상을 이용하여 강아지의 이동과 관련 된 동영상을 생성할 수 있다. 장치는 앞서 도 8의 실시예를 통해 생성된 기초 배경 영상 위에 상기 나무 흔들림 동영상과 강아지 이동 동영상을 배열하여 동영상 컨텐츠를 생성한다. 이때, 나무 동영상과 강아지 동영상의 최 초 배열 위치는 객체 추출시 추출되었던 위치를 기초로 결정하는 것이 바람직하다. 동영상을 생성하고 나면, 장치는 생성된 동영상을 출력할 수 있다(S940). 도 11은 전경, 추천 동작 및 배경 영상을 기반으로 생성된 동영상의 모습을 예시적으로 나타낸 예시도이다. 도 11을 참조하면, 장치(동영상 생성부)는 전경 분할부를 통해 추출된 전경인 강아지와 나무에 대한 영상, 그리 고 시나리오를 기반으로 상기 전경에 대응하는 추천 동작 정보 및 도 8의 인페인팅 모델을 기반으로 생성된 기 초 배경 영상을 입력하여 동영상 컨텐츠를 자동 생성할 수 있다. 이때, 전경의 동작과 관련된 이미지 또는 동영 상을 수집하고, 영상 스티칭 기술을 이용하여, 수집되는 이미지 또는 동영상을 문장의 순서에 따라 순차적으로 연결함에 따라 동영상을 최종적으로 생성한다. 본 발명의 실시예에 따르면, 사용자가 입력한 입력 영상, 입력 시나리오, 입력 연령 정보 등을 기반으로 도 2 내지 도 11의 실시예를 통해 인공지능 모델의 결과물로 나온 결과 데이터를 다시 해당 인공지능 모델의 학습 데 이터로 재사용할 수 있다. 예를 들어, 화풍 추천 모델의 결과물과 입력된 시나리오도 화풍 추천 모델의 학습 데 이터로 사용될 수 있고, 사용자가 입력한 자연영상과 추천된 화풍 정보를 기반으로 스타일 변환된 타겟 화풍 영 상도 스타일 변환 모델의 학습데이터로 재사용될 수 있다. 이와 같은 방법으로, 동적 객체 영역 인식 모델, 인 페인팅 모델, 동작 추천 모델 등도 사용자 입력 데이터와 그에 대한 결과 데이터를 기반으로 반복하여 재학습될 수 있다. 이를 통해 인공지능 모델의 정확도를 제고시킬 수 있다. 이상에서 설명된 시스템 또는 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 시스템, 장치 및 구성요소는, 예를 들어, 프로세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴퓨터, FPA(field programmable array), PLU(programmable logic unit), 마이크 로프로세서, 또는 명령(instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에 서 수행되는 하나 이상의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가"}
{"patent_id": "10-2025-0022994", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "사용되는 것으로 설명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리요소(processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처 리 장치는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세 서(parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처 리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(signal wave)에 영구적으로, 또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 하나 이상의 컴퓨터 판독 가능 기록 매 체에 저장될 수 있다. 실시예들에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설 계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto- optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드 뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하 드웨어 장치는 실시예의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다."}
{"patent_id": "10-2025-0022994", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2025-0022994", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 컨텐츠 자동 제작 장치의 구성을 나타낸 블록도, 도 2a는 컨텐츠 기본 정보를 기반으로 맞춤 화풍을 추천하는 방법을 나타낸 흐름도, 도 2b는 도 2a의 추천 화풍과 작업자가 입력한 자연 영상을 이용하여 타겟 화풍 영상을 생성하는 방법을 나타낸흐름도, 도 3은 도 2a 및 도 2b의 방법에 따라 자연 영상을 타겟 화풍 영상으로 생성한 도면을 예시적으로 나타낸 예시 도, 도 4는 도 1의 전경 분할부가 배경 및 전경 영상을 분할하는 방법을 나타낸 흐름도, 도 5는 도 4의 방법에 따라 자연 영상에서 타겟 화풍 영상의 전경 및 배경을 분할한 모습을 예시적으로 나타낸 예시도, 도 6은 도 1의 배경 영역 생성부가 배경 영역을 자동을 생성하는 방법을 구체적으로 나타낸 상세 흐름도, 도 7a는 배경 자동 생성 인공지능 모델의 학습 데이터를 생성하는 과정을 설명하기 위한 개념도, 도 7b는 도메인 제한 없는 배경 자동 생성 인공지능 모델을 학습하는 과정을 설명하기 위한 개념도, 도 8은 도 6의 방법에 따라 타겟 화풍 영상의 자동 생성된 배경 영상의 모습을 예시적으로 나타낸 예시도, 도 9는 도 1의 동영상 생성부가 입력 시나리오를 기반으로 동영상을 생성하는 과정을 나타낸 흐름도, 도 10는 시나리오를 분석하여 전경과 움직임을 매칭하는 과정을 설명하기 위한 개념도, 도 11은 전경, 추천 동작 및 배경 영상을 기반으로 생성된 동영상의 모습을 예시적으로 나타낸 예시도이다."}
