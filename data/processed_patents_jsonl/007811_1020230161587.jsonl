{"patent_id": "10-2023-0161587", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0074690", "출원번호": "10-2023-0161587", "발명의 명칭": "지식 증류를 통한 비전 트랜스포머 사전 훈련 방법 및 시스템, 이를 통해 사전 훈련된 비전", "출원인": "주식회사 LG 경영개발원", "발명자": "김범수"}}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "메모리와 프로세서를 포함하는 컴퓨팅 시스템이 비전-언어 데이터셋으로 비전 트랜스포머를 사전 훈련하는 방법으로서,복수의 이미지-텍스트 쌍으로 구성된 데이터셋을 획득하는 단계;상기 획득한 데이터셋에서 제n 배치의 텍스트들을 텍스트 인코더에 입력하여 텍스트 특징 표현들을 출력하는 단계;상기 획득한 데이터셋에서 제n 배치의 이미지들을 티처 이미지 인코더에 입력하여 제1 이미지 특징 표현들을 출력하는 단계;상기 획득한 데이터셋에서 제n 배치의 이미지들을 스튜던트 이미지 인코더에 입력하여 제2 이미지 특징 표현들을 출력하는 단계;상기 텍스트 특징 표현들과 상기 제1 이미지 특징 표현들에 대해 제1 정렬 매트릭을 생성하는 단계;상기 제n 배치의 이미지-텍스트 쌍의 포지티브와 네거티브 맵핑 관계에 따라서 유사성을 가지도록 상기 텍스트특징 표현들과 상기 제1 이미지 특징 표현들이 정렬되도록 상기 제1 정렬 매트릭을 학습하는 단계;상기 학습된 제1 정렬 매트릭의 출력을 예측하도록 상기 제2 정렬 매트릭에 대해 지식 증류를 수행하는 단계를포함하는지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 티처 이미지 인코더에 입력하여 제1 이미지 특징 표현들을 출력하는 단계는,상기 제n 배치의 이미지들을 패치화하는 단계와, 상기 패치화된 이미지 패치들을 복수의 셀프 어텐션 레이어와, 피드 포워트 네트워크 레이어를 통해 제1 이미지특징 표현들을 출력하는 단계를 포함하는지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 스튜던트 이미지 인코더에 입력하여 제2 이미지 특징 표현들을 출력하는 단계는,상기 패치화된 이미지 패치들을 복수의 셀프 어텐션 레이어에서의 출력 값을 기초로 토큰 희소화 레이어에 입력하여 토큰 희소화를 수행하는 단계를 포함하는지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 텍스트 특징 표현들과 상기 제1 이미지 특징 표현들이 정렬되도록 상기 제1 정렬 매트릭을 학습하는 단계는,상기 이미지-텍스트 쌍의 맵핑관계에 따라서 상기 텍스트 특징 표현들과 상기 제1 이미지 특징 표현들과의 포지공개특허 10-2024-0074690-3-티브 특징 표현 쌍과, 네가티브 특징 표현 쌍을 결정하는 단계와, 유사성 정렬을 위해 상기 포지티브 특징 표현 쌍 사이의 거리는 가까워지도록 하고 상기 네가티브 특징 표현 쌍사이의 거리는 멀어지도록 하는 손실함수에 따라 인코더들을 학습하는 단계를 포함하는지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서,상기 손실함수에 따라 인코더들을 학습하는 단계는,상기 손실함수에 따라 유사성 정렬을 위하 학습시, 상기 티처 이미지 인코더에 운동량 정지 기울기를 적용해 역전파를 차단하는 단계를 포함하는지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서,상기 제2 정렬 매트릭에 대해 지식 증류를 수행하는 단계는,상기 유사성 정렬에 따른 제1 정렬 매트릭의 출력 값을 예측하도록 지식 증류하는 단계를 포함하는지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 제2 정렬 매트릭에 대해 지식 증류를 수행하는 단계는,상기 지식 증류 중 상기 텍스트 인코더로의 역전파를 차단하는 단계를 포함하는지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서,상기 제2 정렬 매트릭에 대해 지식 증류를 수행하는 단계는,상기 제2 정렬 매트릭의 매개변수가 상기 제1 정렬 매트릭의 매개변수를 따르도록 지식 증류하는 단계를 포함하는지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 제2 정렬 매트릭의 매개변수가 상기 제1 정렬 매트릭의 매개변수를 따르도록 지식 증류하는 단계는, 상기 제2 정렬 매트릭의 매개변수를 제1 정렬 매트릭의 매개변수에 지수 이동 평균(EMA)로 업데이트하는 단계를포함하는지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항에 있어서,상기 제1 정렬 매트릭의 함수 A'와, 제2 정렬 매트릭의 함수 A 사이의 증류 손실을 KL 발산 이고,공개특허 10-2024-0074690-4-상기 KL 발산 은 하기 수학식 3에 따라 정의되는[수학식 3]여기서, σ는 소프트맥스 함수임지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10 항에 있어서,상기 제2 정렬 매트릭에 대해 지식 증류를 수행하는 단계는,전체 증류 손실(overall distillation loss) 은 행 벡터와 열 벡터에 대한 KL 손실의 평균이므로,하기 수학식 4와 같이 정의되는 것을 특징으로 하는[수학식 4]지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 스튜던트 이미지 인코더의 최종 손실 은 하기 수학식 5로 정의 되고,[수학식 5]여기서, λ는 KL 발산 손실과 InfoNCE 손실의 균형을 맞추는 매개변수로 실시예에서 지수 이동 평균(ema)에 기초하여 설정됨상기 티처 이미지 인코더와 상기 스튜던트 이미지 인코더를 포함하는 인코더의 최종 손실 은 하기 수학식 6으로 정의되는[수학식 6] 지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12 항에 있어서,상기 티처 이미지 인코더와 상기 텍스트 인코더 사이의 역전파 방지를 위해 정지 기울기를 통한 업데이트를 수공개특허 10-2024-0074690-5-행하는 단계를 더 포함하는지식 증류를 통한 비전 트랜스포머 사전 훈련 방법."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1 항에 따라 사전 훈련된 비전 트랜스포머를 포함하는 비전 테스크 수행 모델."}
{"patent_id": "10-2023-0161587", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "메모리와 프로세서를 포함하는 컴퓨팅 시스템이 사전 훈련한 비전 트랜스포머로서,제n 배치의 텍스트들을 입력받아 텍스트 특징 표현을 출력하는 텍스트 인코더;제n 배치의 이미지들을 입력 받아 제1 이미지 특징 표현들을 출력하는 티처 이미지 인코더;상기 제n 배치의 이미지들을 입력 받아 제2 이미지 특징 표현들을 출력하는 스튜던트 이미지 인코더; 및상기 텍스트 특징 표현들과 상기 제1 이미지 특징 표현들을 유사성 정렬하여 제1 정렬 매트릭이 되도록 훈련되고, 상기 텍스트 특징 표현들과 상기 제2 이미지 특징 표현들에 대한 제2 정렬 매틝이 상기 제1 정렬 매트릭에 출력값을 예측하도록 지식 증류된지식 증류를 통한 사전 훈련된 비전 트랜스포머."}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 대규모 크기의 선별되지 않은 데이터셋(large uncurated datasets)로 비전 트랜스포머(Vision Transformers)를 자기지도학습 방식으로 지식 증류 프레임워크에 따라 사전 훈련하여, 데이터처리 오버헤드를 감 소시키며 신속하게 간소화된 비전 트랜스포머를 학습시키는 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 지식 증류를 통한 비전 트랜스포머 사전 훈련 방법 및 시스템, 이를 통해 사전 훈련된 비전 트랜스포 머에 관한 것이다."}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 대규모 일반 도메인 데이터에 대하여 사전 훈련된(pre-trained) 비전-언어 모델(Vision-Language Pretraining, VLP)이 등장하면서, 인공지능 기반 컴퓨터 비전 처리 기술이 급격하게 발전하고 있다. 특히, 선행문헌 2와 같이 글로벌 셀프 어텐션(global self-attention)과 대조 언어-이미지 기반 사전훈련 (Contrastive Language-Image Pretraining) 등의 기술을 적용한 대규모 이미지-텍스트 데이터 세트로 훈련된 비전 트랜스포머(Vision Transformers)는 다양하고 까다로운 비전 작업 등에 대한 다운스트림 테스크에서 혁신 적인 진전을 나타내었다. 그러나 비전 트랜스포머에서 주로 구동되는 글로벌 셀프 언텐션을 완전히 트레이닝하기 위해서는 대규모의 데이 터 세트가 필요하며 과도한 데이터 처리 오버헤드를 가지는 문제가 있다. 한편, 선행문헌 1에서는, 비전 트랜스포머의 사전 훈련을 가속화하기 위한 방법으로 토큰 희소화(Token Sparsification)를 통한 프레임 워크를 제안하였다. 그러나 선행문헌 1에서는 사전 정의된 레이블이 있는 지도 학습 파이프라인(예를 들어, classification, detection, or dense prediction)에서만 적용 가능하며, 사전 정 의되지 않은(unexplored) 텍스트-이미지를 자기지도((self-supervision)로 사전훈련(Pretraining)하기 적합하 지 않은 문제가 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 선행문헌 1: Youwei Liang, Chongjian GE, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. EVit: Expediting vision transformers via token reorganizations. In International Conference on Learning Representations, 2022. (특허문헌 0002) 선행문헌 2: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision, on 26 Feb 2021"}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 대규모 크기의 선별되지 않은 데이터셋(large uncurated datasets)로 비전 트랜스포머(Vision Transformers)를 자기지도학습 방식으로 사전 훈련하여, 데이터처리 오버헤드를 감소시키며 신속하게 간소화된 비전 트랜스포머를 학습시키는 비전 트랜스포머 사전훈련 방법 및 시스템을 제안하고자 한다. 자세히, 본 발명의 일 실시예에 따르면, 기존 토큰 희소화(Token Sparsification) 프레임워크를 대조 이미지-텍 스트 사전훈련(contrastive language-image pretraining)에 적용할 경우 발생할 수 잇는 이미지-텍스트의 오정 렬(misalignment)을 문제를 자기지도학습 방식으로 수행하는 비전 트랜스포머 사전훈련 방법 및 시스템을 제공 할 수 있다. 좀더 자세히, 본 발명의 일 실시예에 따르면, 토큰 희소화로 인한 데이터 효율성 문제를 해결할 수 있는 대조 이미지-텍스트 사전훈련하기 위한 지식 증류 프레임워크(knowledge distillation framework)를 포함하는 비전 트랜스포머 사전훈련 방법 및 시스템을 제공할 수 있다. 또한, 본 발명의 일 실시예에 따르면, 이와 같이 사전 훈련된 비전 트랜스포머를 이용하여 다양한 비전 작업 및 비전 작업을 포함하는 애플리케이션을 제공할 수 있다."}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 비전 트랜스포머 사전훈련 방법 및 시스템은, 티처 인코더에 대한 이미지-텍스트 정렬 행렬을 스튜던트 인코더가 학습하는 방식으로 사전 훈련하는 지식 증류 프레임워크를 채택한다. 자세히, 본 발명의 일 실시예에 따른 지식 증류 프레임워크는, 텍스트 인코더와 티처 인코더에 대한 이미지 -텍스트 특징 표현(feature representation)들에 대한 정렬 행렬(Alignment matrices)과, 텍스트 인코더와 스튜던트 인코더에 대한 이미지-텍스트 특징 표현에 대한 정렬 행렬이 서로 매칭되도록 지식 증류하는 방식으로 사전 훈련하여, 티처 인코더 대비 간소화된 스튜던트 인코더로 효율적으로 지식을 추출할 수 있고, 대규모 데이 터셋에서 자연적으로 존재하는 이미지-텍스트 간 불일치를 완화할 수 있다. 이때, 본 발명의 일 실시예에 따른 스튜던트 인코더는, 토큰 희소화 레이어를 포함하여, 데이터 학습 가속화 및 이미지-텍스트 매칭 효율을 향상시킬 수 있다."}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예에 따른 지식 증류 프레임워크에 따라서 비전 트랜스포머 사전 훈련 방법 및 시스템에 따르면, 대규모 선별되지 않은 이미지-텍스트 쌍의 데이터셋을 데이터 처리 과부하를 줄이며 신속하게 사전 훈련시킬 수 있다. 또한, 본 발명의 실시예에 따른 지식 증류 프레임워크에 따라서 비전 트랜스포머 사전 훈련 방법 및 시스템에 따르면, 지식 증류를 통해 경량화된 비전 트랜스포머에 대해 사전 훈련시킬 수 있다. 또한, 본 발명의 실시예에 따른 지식 증류 프레임워크에 따라서 비전 트랜스포머 사전 훈련 방법 및 시스템에 따르면, 토큰 희소화를 통해 이미지-텍스트 간 오정렬 문제를 해소시키고, 데이터를 좀더 신속하게 처리할 수 있다. 또한, 또한, 본 발명의 실시예에 따른 지식 증류 프레임워크에 따 사전 훈련된 비전 트랜스포머는, 다양한 비전 테스크 작업에서 우수한 효과를 보일 수 있다. 특히, 본 발명의 실시예에 따른 비전 트랜스포머는, 주의도가 높 은 패치 토큰을 효율적으로 판별하여 이미지 분할 작업(segmentation)에 우수한 성능을 가질 수 있다."}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변환을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고"}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "상세한 설명에 상세하게 설명하고자 한다. 본 발명의 효과 및 특징, 그리고 그것들을 달성하는 방법은 도면과 함께 상세하게 후술되어 있는 실시예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시 예들에 한정되는 것이 아니라 다양한 형태로 구현될 수 있다. 이하의 실시예에서, 제1, 제2 등의 용어는 한정적 인 의미가 아니라 하나의 구성 요소를 다른 구성 요소와 구별하는 목적으로 사용되었다. 또한, 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 또한, 포함하다 또는 가지다 등의 용어는 명 세서상에 기재된 특징, 또는 구성요소가 존재함을 의미하는 것이고, 하나 이상의 다른 특징들 또는 구성요소가 부가될 가능성을 미리 배제하는 것은 아니다. 또한, 도면에서는 설명의 편의를 위하여 구성 요소들이 그 크기가 과장 또는 축소될 수 있다. 예컨대, 도면에서 나타난 각 구성의 크기 및 두께는 설명의 편의를 위해 임의로 나 타내었으므로, 본 발명이 반드시 도시된 바에 한정되지 않는다. 도 1은 본 발명의 실시예에 따른 지식 증류 프레임워크를 통해 비전 트랜스포머를 사전 훈련하고, 사전 훈련된 비전 트랜스포머를 포함하는 애플리케이션을 실행하는 컴퓨팅 시스템의 블록도의 예시를 도시한다. 도 1을 참조하면, 본 발명의 일 실시예에 따른 컴퓨팅 시스템은, 유저 컴퓨팅 디바이스, 트레이닝 컴퓨팅 시스템 및 서버 컴퓨팅 시스템을 포함하며, 각 디바이스 및 시스템은 네트워크를 통해 통신 가능하게 연결된다. 본 발명의 다양한 실시예에 따르면, 1) 유저 컴퓨팅 디바이스가 로컬에서 비전 트랜스포머를 사전 훈 련시키고, 학습된 비전 트랜스포머를 포함하는 애플리케이션을 실행할 수 있으며, 2) 유저 컴퓨팅 디바이 스와 통신하는 서버 컴퓨팅 시스템이 비전 트랜스포머(120 또는/및 140)를 사전 훈련시키고, 비전 트 랜스포머(120 또는/및 140) 또는/및 비전 트랜스포머(120 또는/및 140)를 포함하는 애플리케이션을 유저 컴퓨팅 디바이스에 직접 또는 웹 서비스 형태로 제공할 수 있으며, 3) 유저 컴퓨팅 디바이스와 서버 컴퓨팅 시스템이 서로 연계하여 비전 트랜스포머(120 또는/및 140)를 사전훈련 시키거나, 사전훈련된 비전 트랜스 포머(120 또는/및 140)를 실행하여 각종 애플리케이션 서비스를 제공할 수 있다. 또한, 본 발명의 다양한 실시예에 따르면, 유저 컴퓨팅 디바이스 및/또는 서버 컴퓨팅 시스템은 네트 워크를 통해 통신적으로 연결된 트레이닝 컴퓨팅 시스템과의 인터렉션을 통해 모델을 트레이닝 할 수 있다. 이때, 트레이닝 컴퓨팅 시스템은 서버 컴퓨팅 시스템과 별개이거나 서버 컴퓨팅 시스템 의 일부일 수 있다 즉, 실시예에 따른 비전 트랜스포머 사전훈련 방법은, 1) 유저 컴퓨팅 디바이스가 로컬에서 직접 비전 트 랜스포머를 사전 훈련시킬 수 있고, 2) 서버 컴퓨팅 시스템과 유저 컴퓨팅 디바이스가 네트워크 를 통해 서로 인터랙션하며 사전 훈련할 수 있고, 3) 별도의 트레이닝 컴퓨팅 시스템이 비전 트랜스포머를 다양한 트레이닝 기법과 학습 기법을 사용하여 사전 훈련시킬 수 있다. 그리고 트레이닝 컴퓨팅 시스템이 사전 훈련시킨 비전 트랜스포머(120 또는/및 140)를 네트워크를 통해 유 저 컴퓨팅 디바이스 또는/및 서버 컴퓨팅 시스템에 전송하여 제공 또는/및 업데이트 하는 방식으로 구현될 수도 있다. 일부 실시예에서, 트레이닝 컴퓨팅 시스템은, 서버 컴퓨팅 시스템의 일부이거나, 유저 컴퓨팅 디바이 스에 일부일 수 있다. 또한, 본 발명은 비전 트랜스포머 사전훈련 방법 및 시스템은, 사전 훈련된 비전 트랜스포머를 파인 튜닝하는 등의 추가 작업을 수행하여 각종 다운스트림 테스크를 수행하는 애플리케이션에 포함시킬 수 있다. 유저 컴퓨팅 디바이스는, 스마트 폰(smart phone), 휴대폰, 디지털방송용 디바이스, PDA(personal digital assistants), PMP(portable multimedia player), 데스크 탑, 웨어러블 디바이스, 임베디드 컴퓨팅 장 치 및/또는 태블릿 PC(tablet PC) 등 기타 모든 유형의 컴퓨팅 장치를 포함할 수 있다. 이러한 유저 컴퓨팅 디바이스는, 적어도 하나 이상의 프로세서 및 메모리를 포함한다. 여기서, 프로세서는, 중앙처리장치(CPU), 그래픽처리장치(GPU), ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세스(microprocessors) 및/또는 기타 기능 수행을 위한 전기적 유 닛 중 적어도 하나 또는 전기적으로 연결된 복수의 프로세서들로 구성될 수 있다. 메모리는, RAM, ROM, EEPROM, EPROM, 플래시 메모리 디바이스, 자기 디스크 등 같은 하나 이상의 비일시 적/일시적 컴퓨터 판독가능한 저장 매체 및 이들의 조합을 포함할 수 있고, 인터넷(internet) 상에서 메모리의 저장 기능을 수행하는 서버의 웹 스토리지(web storage)를 포함할 수 있다. 이러한 메모리는, 상기 적어도 하나 이상의 프로세서가 비전 트랜스포머를 사전 훈련시키거나, 사전 훈련된 비전 트랜스포머를 포함하는 애플리케이션을 실행 등의 동작을 수행하기 위하여 필요한 데이터 및 명령어들을 저장할 수 있다. 일 실시예에서, 유저 컴퓨팅 디바이스는, 적어도 하나 이상의 머신 러닝 모델들(예들 들어, 비전 트랜스포 머)을 저장할 수 있다. 자세히, 일 실시예의 비전 트랜스포머는, 복수의 뉴럴 네트워크(예를 들어, Deep neural network)와 같은 다양한 머신 러닝 모델들 또는 비선형 모델 및/또는 선형 모델을 포함하는 다른 유형의 머신 러닝 모델들일 수 있으며, 이들의 조합으로 구성할 수 있다. 그리고 뉴럴 네트워크는 피드-포워드 뉴럴 네트워크들(feed-forward neural networks), 순환 뉴럴 네트워크(예 를 들어, 장단기 메모리 순환 신경 네트워크들), 컨벌루션 신경 네트워크 또는/및 다른 형태의 신경 네트워크들 중 적어도 하나 이상을 포함할 수 있다. 일 실시예에서, 유저 컴퓨팅 디바이스는, 네트워크를 통해서 서버 컴퓨팅 시스템으로부터 적어 도 하나 이상의 비전 트랜스포머를 수신하고, 메모리에 저장한 후, 저장된 비전 트랜스포머를 프로세 서에 의해 실행하여, 다양한 비전 기반 테스크를 가지는 애플리케이션을 동작할 수 있다. 다른 실시예에서, 서버 컴퓨팅 시스템은, 적어도 하나 이상의 머신 러닝 모델(예를 들어, 비전 트랜스포머 )를 포함하여 비전 트랜스포머를 통한 동작을 수행하며, 유저 컴퓨팅 디바이스와 이와 관련된 데이터를 통신하는 방식으로 유저 컴퓨팅 디바이스와 연동해 비전 트랜스포머를 이용한 다운스트림 테스크를 유저에게 제공할 수 있다. 예를 들어, 유저 컴퓨팅 디바이스는, 웹을 통해 서버 컴퓨팅 시스템 이 비전 트랜스포머를 이용하여 유저의 입력에 대한 출력을 제공하는 방식으로 비전 트랜스포머(14 0)를 포함한 다운스트림 테스크가 수행될 수 있다. 또한, 비전 트랜스포머들(120 또는/및 140) 중 적어도 일부 는 유저 컴퓨팅 디바이스에서 실행되고, 나머지는 서버 컴퓨팅 시스템에서 실행되는 방식으로도 비전 트랜스포머(120 또는/및 140)를 구현될 수 있다. 또한, 유저 컴퓨팅 디바이스는, 유저의 입력을 감지하는 적어도 하나 이상의 입력 컴포넌트를 포함할 수 있다. 예를 들어, 유저 입력 컴포넌트는, 유저의 입력 매체(예를 들어, 손가락 또는 스타일러스)의 터치를 감지 하는 터치 센서(예를 들어, 터치 스크린 또는/및 터치 패드 등), 유저의 모션 입력을 감지하는 이미지 센서, 유 저 음성 입력을 감지하는 마이크로폰, 버튼, 마우스 및/또는 키보드 등을 포함할 수 있다. 또한, 유저 입력 컴 포넌트는, 인터페이스를 통해 외부 컨트롤러(예컨대, 마우스, 키보드 등)에 대한 입력을 수신할 경우에, 인터페 이스와 외부 컨트롤러가 포함될 수 있다. 서버 컴퓨팅 시스템은 적어도 하나 이상의 프로세서와 메모리를 포함한다. 여기서, 프로세서 는, 중앙처리장치(CPU), 그래픽처리장치(GPU), ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro- controllers), 마이크로 프로세스(microprocessors) 및/또는 기타 기능 수행을 위한 전기적 유닛 중 적어도 하 나 또는 전기적으로 연결된 복수의 프로세서들로 구성될 수 있다. 그리고 메모리는, RAM, ROM, EEPROM, EPROM, 플래시 메모리 디바이스, 자기 디스크 등 같은 하나 이상의 비일시적/일시적 컴퓨터 판독가능한 저장 매체 및 이들의 조합을 포함할 수 있다. 이러한 메모리는, 프로 세서가 비전 트랜스포머를 사전 훈련하거나, 비전 트랜스포머를 이용한 다양한 비전 테스크(예 를 들어, image detection, classification, segmentation 등) 동작을 수행하기 위하여 필요한 데이터 및 명령어들을 저장할 수 있다. 일 실시예에서, 서버 컴퓨팅 시스템은, 적어도 하나 이상의 컴퓨팅 디바이스를 포함하여 구현될 수 있다. 예를 들어, 서버 컴퓨팅 시스템은, 복수의 컴퓨팅 디바이스를 순차적 컴퓨팅 아키텍처, 병렬 컴퓨팅 아키 텍처 또는 이들의 조합에 따라 동작하도록 구현될 수 있다. 또한, 서버 컴퓨팅 시스템은, 네트워크로 연결 된 복수의 컴퓨팅 디바이스를 포함할 수 있다. 또한, 서버 컴퓨팅 시스템은 적어도 하나 이상의 비전 트랜스포머를 저장할 수 있다. 예를 들어, 서 버 컴퓨팅 시스템은, 비전 트랜스포머로 뉴럴 네트워크 또는/및 기타 멀티 레이어 비선형 모델을 포 함할 수 있다. 예시적 신경 네트워크는 피드 포워드 뉴럴 네트워크, 딥 뉴럴 네트워크, 순환 뉴럴 네트워크 및 컨벌루션 뉴럴 네트워크를 포함할 수 있다. 트레이닝 컴퓨팅 시스템은 적어도 하나 이상의 프로세서와 메모리를 포함한다. 여기서, 프로세 서는, 중앙처리장치(CPU), 그래픽처리장치(GPU), ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro- controllers), 마이크로 프로세스(microprocessors) 및/또는 기타 기능 수행을 위한 전기적 유닛 중 적어도 하 나 또는 전기적으로 연결된 복수의 프로세서들로 구성될 수 있다. 그리고 메모리는, RAM, ROM, EEPROM, EPROM, 플래시 메모리 디바이스, 자기 디스크 등 같은 하나 이상의 비일시적/일시적 컴퓨터 판독가능한 저장 매체 및 이들의 조합을 포함할 수 있다. 이러한 메모리는, 프로 세서가 비전 트랜스포머를 학습을 수행하기 위하여 필요한 데이터 및 명령어들을 저장할 수 있 다. 예를 들어, 트레이닝 컴퓨팅 시스템은, 에러의 역방향 전파와 같은 다양한 트레이닝 또는 학습 기법을 사 용하여(도 5에 도시된 프레임워크에 따라), 유저 컴퓨팅 디바이스 또는/및 서버 컴퓨팅 시스템에 저 장된 비전 트랜스포머(120 또는/및 140)를 사전 훈련(training)하는 모델 트레이너를 포함할 수 있다. 예를 들어, 모델 트레이너는, 정의된 손실 함수에 기초하여 비전 트랜스포머(120 또는/및 140)의 하나 이 상의 파라미터를 업데이트를 역전파 방식으로 수행할 수 있다. 일부 구현예에서, 에러의 역방향 전파를 수행하는 것은 시간을 통한 잘린 역 전파(truncated backpropagation through time)를 수행하는 것을 포함할 수 있다. 모델 트레이너는 트레이닝되는 비전 트랜스포머(120 또는 /및 140)의 일반화 능력을 향상시키기 위해 다수의 일반화 기법들(예를 들어, 가중치 감소, 드롭 아웃, 지식 증 류 등)을 수행할 수 있다. 특히, 모델 트레이너는 일련의 트레이닝 데이터에 기초하여 비전 트랜스포머(120 또는/및 140)를 트레이닝 할 수 있다. 트레이닝 데이터는 예를 들어, 이미지, 오디오 샘플, 텍스트 등과 같은 상이한 복수의 양식(multi-modal)의 데이터를 포함할 수 있다. 사용될 수 있는 이미지 유형의 예는 비디오 프레임, LiDAR 포인트 클라우드, X선 이미지, 컴퓨터 단층 촬영 스캔, 초분광 이미지 및/또는 다양한 기타 형태의 이미지를 포함할 수 있다. 이러한 트레이너 데이터와, 다운스트림 테스크를 위한 입력 데이터는, 유저 컴퓨팅 디바이스 또는/및 서버 컴퓨팅 시스템에 의해 제공될 수 있다. 유저 컴퓨팅 디바이스의 특정 데이터에 대해 트레이닝 컴퓨팅 디바이스가 비전 트랜스포머를 학습시킬 경우, 비전 트랜스포머는 개인화된 모델로 특성화될 수 있다. 그리고 모델 트레이너는 원하는 기능을 제공하기 위해 활용되는 컴퓨터 로직을 포함한다. 모델 트레이너 는 범용 프로세서를 제어하는 하드웨어, 펌웨어 및/또는 소프트웨어로 구현될 수 있다. 예를 들어, 일 실 시예에서, 모델 트레이너는 저장 디바이스에 저장된 프로그램 파일을 포함하고, 메모리에 로딩되고 하나 이상의 프로세서에 의해 실행될 수 있다. 다른 구현예에서, 모델 트레이너는 RAM 하드 디스크 또는 광학 또는 자기적 매체와 같은 유형적 컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 실행가능 명령어들의 하나 이상의 세트들을 포함한다. 네트워크는 3GPP(3rd Generation Partnership Project) 네트워크, LTE(Long Term Evolution) 네트워크, WIMAX(World Interoperability for Microwave Access) 네트워크, 인터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), 블루투스(Bluetooth) 네트워크, 위성 방송 네트워크, 아날로그 방송 네트워크 및/또는 DMB(Digital Multimedia Broadcasting) 네트워크 등이 포함되나 이에 한정되지는 않는다. 일반적으로, 네트워크를 통한 통신은 임의의 유형의 유선 및/또는 무선 연결을 사용하여, 다양한 통신 프 로토콜들(예를 들어, TCP/IP, HTTP, SMTP, FTP), 인코딩 또는 포맷들(예를 들어, HTML, XML), 및/또는 보호 스 키마(예를 들어, VPN, 시큐어 HTTP, SSL)를 통해 수행될 수 있다. 도 2는 본 발명의 실시예에 따른 따른 지식 증류 프레임워크에 따라서 비전 트랜스포머 사전 훈련하고, 사전 훈 련된 비전 트랜스포머를 실행하는 컴퓨팅 디바이스의 블록도의 예시를 도시한다. 도 2를 포함하면, 유저 컴퓨팅 디바이스, 서버 컴퓨팅 시스템 및 트레이닝 컴퓨팅 시스템에 포 함되는 컴퓨팅 디바이스는, 다수의 애플리케이션(예를 들어, 애플리케이션 1 내지 애플리케이션 N)을 포함 한다. 각 애플리케이션은 머신 러닝 라이브러리 및 하나 이상의 비전 트랜스포머를 포함할 수 있다. 예를 들어, 애플리케이션은 이미지 처리(예를 들어, detection, classification, segmentation 등) 애플리케이션, 텍스트 메시징 애플리케이션, 이메일 애플리케이션, 받아쓰기 애플리케이션, 가상 키보드 애플리케이션, 브라우저 애플 리케이션, 쳇-봇(chat-bot) 애플리케이션 등을 포함할 수 있다. 실시예에서, 컴퓨팅 디바이스는, 비전 트랜스포머를 사전 훈련시키기 위한 모델 트레이너를 포함할 수 있고, 상기 사전 훈련된 비전 트랜스포머를 저장하고 동작시켜 입력 데이터에 대해 비전 트랜스포머를 이용 한 각종 비전 테스크를 수행할 수 있다. 컴퓨팅 디바이스의 각 애플리케이션은 예를 들어, 하나 이상의 센서, 컨텍스트 관리자, 디바이스 상태 컴 포넌트 및/또는 추가 컴포넌트들과 같은 컴퓨팅 디바이스의 다수의 다른 컴포넌트들과 통신할 수 있다. 일 실시예에서, 각 애플리케이션은 API(예를 들어, 퍼블릭 API)를 사용하여 각 디바이스 컴포넌트와 통신할 수 있 다. 일 실시예에서, 각 애플리케이션에 의해 사용되는 API는 해당 애플리케이션에 대해 특정적일 수 있다. 도 3은 본 발명의 실시예에 따른 지식 증류 프레임워크를 통해 비전 트랜스포머 사전 훈련하고, 사전 훈련된 비 전 트랜스포머를 실행하는 컴퓨팅 디바이스에 대한 다른 측면에서의 블록도의 예시를 도시한다. 도 3을 참조하면, 컴퓨팅 디바이스는 다수의 애플리케이션(예를 들어, 애플리케이션 1 내지 애플리케이션 N)을 포함한다. 각 애플리케이션은 중앙 인텔리전스 레이어와 통신할 수 있다. 예를 들어, 애플리케이션은 이미 지 처리 애플리에키션, 문자 메시지 애플리케이션, 이메일 애플리케이션, 받아쓰기 애플리케이션, 가상 키보드 애플리케이션, 브라우저 애플리케이션 등을 포함할 수 있다. 일 실시예에서, 각 애플리케이션은 API(예: 모든 애플리케이션에 걸쳐 공통 API)를 사용하여 중앙 인텔리전스 레이어(및 그 안에 저장된 모델)과 통신할 수 있다. 그리고 중앙 인텔리전스 레이어는 다수의 비전 트랜스포머들을 포함할 수 있다. 예를 들어, 도 3에 도시된 바와 같이, 각각의 비전 트랜스포머 중 적어도 일부가 각 애플리케이션에 대해 제공될 수 있고, 중앙 인텔리전스 레 이어에 의해 관리될 수 있다. 다른 구현예에서, 2개 이상의 애플리케이션들은 단일의 비전 트랜스포머를 공유할 수 있다. 예를 들어, 일부 구현예에서, 중앙 인텔리전스 레이어는 모든 애플리케이션에 대해 단일 모델을 제공 할 수 있다. 일부 구현예에서, 중앙 인텔리전스 레이어는 컴퓨팅 디바이스의 운영 체제 내에 포함되거나 이와 다르게 구현될 수 있다. 중앙 인텔리전스 레이어는 중앙 디바이스 데이터 레이어와 통신할 수 있다. 중앙 디바이스 데이터 레이어는 컴 퓨팅 디바이스에 대한 중앙 집중식 데이터 저장소일 수 있다. 도 3에 도시된 바와 같이, 중앙 디바이스 데 이터 레이어는 예를 들어, 하나 이상의 센서, 컨텍스트 관리자, 디바이스 상태 컴포넌트 및/또는 추가 컴포넌트 들과 같은 컴퓨팅 디바이스의 다수의 다른 컴포넌트들과 통신할 수 있다. 일부 구현예에서, 중앙 디바이스 데이터 레이어는 API(예를 들어, 사설 API)를 사용하여 각 디바이스 컴포넌트와 통신할 수 있다. 본 명세서에서 설명한 기술은 서버, 데이터베이스, 소프트웨어 애플리케이션들 및 다른 컴퓨터 기반 시스템뿐만 아니라 취해진 액션들 및 상기 시스템으로 전송되거나 그로부터 전송된 정보를 참조할 수 있다. 컴퓨터 기반 시 스템들의 내재적 유연성은 광범위한 가능한 구성들, 조합들 및 작업의 분할 및 컴포넌트들 간의 및 그로부터의 기능성을 허용함을 인식할 것이다. 예를 들어, 본 명세서에서 기술한 프로세스들은 단일의 디바이스 또는 컴포 넌트 또는 조합으로 작동하는 다수의 디바이스들 또는 컴포넌트들을 사용하여 구현될 수 있다. 데이터베이스 및 애플리케이션들은 단일 시스템 또는 다수의 시스템들에 걸쳐 분산된 시스템에서 구현될 수 있다. 분산 컴포넌트 들은 순차적으로 또는 병렬로 동작할 수 있다. 이하, 이러한 컴퓨팅 시스템이, 지식 증류 프레임워크를 통해 비전 트랜스포머를 사전 훈련하는 과정을 도 4 내지 도 6을 참조하여 상세히 설명한다. 본 발명에서 설명하는 비전 트랜스포머는, 비전-언어(이미지-텍스트) 쌍의 두가지 이종 형식의 데이터 간 공동 표현을 대규모 스케일의 데이터셋으로 사전 훈련된 비전-언어 모델(VLP)을 의미한다. 이러한 실시예에 따른 비전 트랜스포머는, 이미지와 텍스트가 결합된 입력 데이터를 변환하는 싱글-스트림 모델 과, 이미지-텍스트를 별도의 인코더를 통해 처리하는 듀얼(멀티)-스트림 모델을 포함할 수 있다. 이하 실시예에서, 작업의 편의를 위해 이미지-텍스트가 매칭되어 있는 데이터셋에 대해 대조 목표로 사전 훈련 하는 듀얼 스트림 아키텍처를 가지는 비전 트랜스포머를 기준으로 설명한다. 실시예에 따른 비전 트랜스포머 사전 훈련방법은, 자기 증류된 인코더를 사용하여 대조 이미지-텍스트 쌍의 데 이터셋의 사전 훈련을 촉진할 수 있다. 도 4 내지 5를 참조하면, 실시예에 따른 비전 트랜스포머 사전 훈련 아키텍처는, 텍스트 인코더, 티처 이미 지 인코더 및 스튜던트 이미지 인코더를 포함한다. 그리고 스튜던트 이미지 인코더와 티처 이미지 인코더는, 멀티-헤드 셀프-어텐션 레이어와, 피드-포워드 네트워크를 포함할 수 있다. 그리고 스튜던트 이 미지 인코더는, 토큰 희소화 레이어를 더 포함할 수 있다. 여기서, 사전 훈련을 위한 이미지-텍스트 데이터셋은 선별되지 않은(uncurated) 데이터셋으로, 예를 들어, 라벨 링 작업이다 캡셔닝 작업 등이 수행되지 않은 데이터를 의미한다. 실시예에서는, 본 실시예에 따른 비전 트랜스포머 사전 훈련방법의 효율성을 명확히 검증하기 위해서 사전 훈련 을 위한 데이터셋으로 대규모 오픈 소스 데이터셋인 CC (Conceptual Captions) 3M, YFCC (Yahoo Flickr Creative Commons) 15M 및 YFCC15M, 88M 중 적어도 하나 이상의 데이터셋을 포함할 수 있다. 또한, 본 실시예에 따라서 사전 훈련된 비전 트랜스포머의 성능을 확인하기 위한 다운스트림 데이터셋으로, Flickr30K 또는/및 MS-COCO에서 제로샷 이미지-텍스트를 포함할 수 있다. 이후, 컴퓨팅 시스템은, 사전 훈련 데이터셋의 이미지-텍스트 쌍을 데이터들을 배치(batch) 사이즈에 따 라 분류하고, 배치 사이즈 내에서 기 매칭된 이미지-텍스트 쌍을 포지티브 쌍으로 맵핑하고, 다른 이미지의 포 지티브로 매칭된 텍스트에 대해서는 네가티브 쌍으로 매칭할 수 있다. 그리고 컴퓨팅 시스템은, 배치 별로 텍스트들을 텍스트 인코더에 입력하여 텍스트 특징 표현(T)들을 출력할 수 있다. 또한, 컴퓨팅 시스템은, 배치 별 이미지들을 패치화(patchify)하고, 이미지 패치들을 티처 이미지 인코더 에 입력하여 제1 이미지 특징 표현(I')들을 출력할 수 있다. 오버바(overbar)에 대한 표기는 '로 대체하여 표기하기로 한다. 다음으로, 컴퓨팅 시스템은, 텍스트 특징 표현(T)들과, 제1 이미지 특징 표현(I')들을 기 매칭된 포지티 브 쌍과, 네가티브 쌍에 따라서 맵핑하여 제1 정렬 매트릭(A')를 생성할 수 있다. 그리고 컴퓨팅 시스템은, 출력된 텍스트 특징 표현(T)들과, 이미지 특징 표현들이 기 맵핑된 포지티브/네 거티브 기준에 따라 제1 정렬 매트릭(A')을 유사성 대조 정렬하는 방식(예를 들어, InfoNCE loss(포지티브 쌍의 유사성이 최대화되고, 네거티브 쌍의 유사성을 최소화하도록 훈련하는 방식)으로 훈련하며, 하드 얼라인먼트 레 이블이 있는 데이터 쌍으로 학습을 진행할 수 있다. 이때, 컴퓨팅 시스템은, 제1 이미지 특징 표현(I')들로 구성된 제1 정렬 매트릭(A')을 유사성 정렬을 위 해 훈련하는 과정에서 티처 이미지 인코더는 정지 기울기가 있는 운동량 티처 모델(Momentum Teacher with Stop Gradient)일 수 있으며, 따라서, 유사성 정렬 중 티처 이미지 인코더로 역전파(sg)되는 것을 차단할 수 있다. 자세히, 유사성은, 이미지 특징 표현과 텍스트 특징 표현(T) 간의 내적(dot product)을 의미할 수 있다. 이후, 컴퓨팅 시스템은, 유사성 정렬을 위해 포지티브 특징 표현들 사이의 공간 상 거리가 가까워지도록, 네가티브 특징 표현들 사이의 공간 상 거리가 멀어지도록 손실함수에 따라 학습시킬 수 있다. 즉, 이 되도록 손실함수를 정의하여 대조 학습(contrastive learning)할 수 있다. 예를 들어 전술한 바와 같이, 컴퓨팅 시스템은, 유사성 메트릭에 손실함수인 InfoNCE Loss를 적용하여 학 습시킬 수 있다. 그리고 컴퓨팅 시스템은, 배치별 이미지 패치들을 스튜던트 이미지 인코더에 입력하여 제2 이미지 특 징 표현(I)들을 출력할 수 있다. 이때, 스튜던트 이미지 인코더는 토큰 희소화 레이어를 포함하여 패치 토큰을 재구성함으로써, 사전 훈련을 가속화할 수 있다. 자세히, 스튜던트 이미지 인코더는, 이미지 패치들 사이의 주의 가치(value)를 산출(self-attention)하고, 산출된 각 이미지 패치들 사이의 주의 가치에 따라서 기 정해진 기준 이하의 토큰을 폐기할 수 있다. 예를 들어, 스튜던트 이미지 인코더는, 셀프 어텐셜 레이어 중 4, 7, 10번째 변환기 레이어의 각 패치 사이 의 주의 가치에 따라 부주의한 토큰을 고정 비율(1-κ)에 따라서 폐기할 수 있다. 여기서 κ는 토큰 유지율이다. 그리고 컴퓨팅 시스템은, 텍스트 특징 표현(T)들과, 토큰 희소화를 거친 제2 이미지 특징 표현(I)들을 기 매칭된 포지티브 쌍과, 네가티브 쌍에 따라서 맵핑하여 제2 정렬 매트릭(Alignment matrices)를 생성할 수 있다. 다음으로, 컴퓨팅 시스템은, 기존 지식 증류 방식과 상이하게 제2 정렬 매트릭(A)이 유사성 맵핑에 따라 정렬된 제1 정렬 매트릭(A')의 출력 값을 예측하도록 지식 증류할 수 있다. 즉, 컴퓨팅 시스템은, 제1 정렬 매트릭(A')에 따라서 소프트 얼라인먼트로 제2 정렬 매트릭(A)과 매칭되 도록 스튜던트 이미지 인코더를 훈련하는 방식으로 지식 증류(distillation)할 수 있다. 이때, 텍스트 인코 더는, 지식 증류 중 텍스트 인코더로 역전파(sg)되는 것을 차단하는 정지 기울기가 있는 운동량 티처 (Momentum Teacher with Stop Gradient) 모델 일 수 있다. 자세히, 컴퓨팅 시스템은, 제2 정렬 매트릭(A)이 제1 정렬 매트릭(A')의 매개변수를 따르도록 지식 증류 할 수 있다. 구체적으로, 컴퓨팅 시스템은, 제2 정렬 매트릭(A)이 제1 정렬 매트릭(A')의 매개변수를 지수 이동 평균 (EMA)로 업데이트할 수 있다. 그리고 컴퓨팅 시스템은, 상기 티처 이미지 인코더에 대한 훈련과, 상기 스튜던트 이미지 인코더(3 0)에 훈련과, 매개변수 업데이트 과정을 n회(예를 들어, 1~3회) 반복 수행할 수 있다. 그리고 반복 수행되는 과 정에서 텍스트 인코더와, 티처 이미지 인코더는 정지 기울기를 통해 역전파(sg)를 방지하여 서로 간의 충돌(collapsing)을 예방할 수 있다. 이하에서는, 구체적인 수학식을 통해서 위 사전 훈련을 위한 계산 과정을 면밀하게 설명한다. 구체적으로, 정지 기울기를 가진 운동량 티처 이미지 인코더에 대한 함수 와, 정지 기울기를 가진 운동 량 텍스트 인코더에 대한 함수 와, 스튜던트 인코더에 대한 함수 에 대한 제1 정렬 매트릭(A')을 나타내는 함수 과, 제2 정렬 매트릭(A)을 나타내는 함수 는 하기 수학식 1과 같이 정의될 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, sg는 정지 기울기이고, 와 는 각각 티처 이미지 인코더 와 스튜던트 이미지 인코더를 사용하여 j번째 이미지에 대한 이미지 특징 표현이고, 로 i번째 텍스트에 대한 텍스트 특징 표현(T)이다. 또한, 는, 이미지 특징 표현과 텍스트 특징 표현에 대한 정렬 매트릭이며, N은 이미지-텍스 트 쌍의 배치 사이즈이고, sim은 코사인 유사도에 대한 함수를 의미한다. 그리고 전술한 바와 같이, InfoNCE 손실(하기 수학식 2)을 사용하여 제1 정렬 매트릭(A')에 대한 손실을 얻을 수 있다. [수학식 2]"}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "은 InfoNCE loss로서, 이하에서는 수학식 2에 따라서 수학식 1의 에 대 한 InfoNCE loss를 으로로 정의한다. 다음으로, 전술한 바와 같이 컴퓨팅 시스템은, 제2 정렬 매트릭(A)이 제1 정렬 매트릭(A')과 일치하도록 예측하는 지식 증류를 수행한다. 자세히, 제1 정렬 매트릭(A')과, 제2 정렬 매트릭(A) 사이의 각 행과 열에 대한 KL 발산으로 증류 손실을 정의 할 때, σ를 소프트맥스 함수라고 하면 제1 정렬 매트릭(A')과, 제2 정렬 매트릭(A) 사이의 KL 발산인 을 하기 수학식 3 같이 나타내어, 수학식 3에 따라 증류 손실을 산출할 수 있다. [수학식 3]"}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, 전체 증류 손실(overall distillation loss)인 은, 제1 정렬 매트릭의 매개변수와 제2 정렬 매트릭 매개변수의 각각의 행 벡터와 열 벡터에 대한 KL 손실의 평균이므로, 하기 수학식 4와 같이 정의할 수 있다. [수학식 4]"}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "그리고 스튜던트 이미지 인코더의 훈련을 가속화하기 위하여, 지식 증류 훈련과, 티처 이미지 인코더 훈련 사이의 균형을 맞추면 스튜던트 이미지 인코더의 최종 손실 은 하기 수학식 5에 따라 정의되고 따라서 최종 손실 는 수학식 6에 따라서 정의된다. [수학식 5]"}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "[수학식 6]"}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, λ는 KL 발산 손실과 InfoNCE 손실의 균형을 맞추는 매개변수로 실시예에서 지수 이동 평균(ema)에 기 초하여 산출된다. 그리고 전술한 바와 같이, 티처 이미지 인코더와 텍스트 인코더에서 역전파 방지를 위해 정지 기울기를 통한 업데이트를 수행할 수 있다. 자세히, 와 는 각각 학생 인코더와, 운동량 티처 이미지 인코더의 매개변수를 의미하고, t번째 단계에서 를 업데이트는 하기 수학식 7에 따라 수행될 수 있다. [수학식 7]"}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "실험결과, m은 0.994일 때 가장 효율적인 훈련이 수행될 수 있었다. 즉, 상기 수학식 1 내지 7에 과정을 통해, 비전 트랜스포머의 인코더들이 대조 지도 및 지식 증류를 통해 사전 훈련될 수 있다. 이하에서는, 본 발명의 실시예에 따른 지식 증류를 통한 비전 트랜스포머 사전 훈련 효과를 기존 기술과 비교하 기 위한 설명을 기술한다. 비교를 위해, 토큰 희소화를 기존 대조 언어-이미지 사전 훈련에 그대로 적용한 기존 사전 훈련 방식(EViT)과, 본 발명에 따른 지식 증류를 통한 비전 트랜스포머 사전 훈련 방법(ECLIPSE)를 비교한 그래프인 도 6을 참조하 면 하기와 같다. 본 발명의 실시예에 따른 비전 트랜스포머 사전 훈련 방법(ECLIPSE)은, 기존 사전 훈련 방법(EViT) 대비 101% 만큼 더 빠른 처리량으로 더 간소화된 비전 트랜스포머를 학습시켰으며, 이와 같이 사전 훈련된 비전 트랜스포 머의 성능 또한 제로샷 이미지의 정확도가 상대적으로 우수함을 확인할 수 있다. 또한, 토큰 희소화를 적용하지 않은, 대표 대조 언어-이미지 사전 훈련 모델인 CLIP(Contrastive Language-Image Pretraining)에 대비해서도 사전 훈련 속도, 비전 트랜스포머 용량, 제로샷 정확도에서 우수한 성능을 보이는 것을 확인할 수 있다. 야기서, 각 모델 성능을 비교하기 위해 사용한 백본은 ViT-B/16이다. [표 1]"}
{"patent_id": "10-2023-0161587", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "좀더 자세히, 상기 표 1은 CLIP 모델과, 본 발명의 ECLIPSE 모델의 ImageNet-1k Top-1에 대한 제로샷 정확도를 나타내는 표이다. CLIP 모델 대비 ECLIPSE 모델의 정확도가 월등함을 확인할 수 있다. 특히, 도 7을 참조하면, CLIP 모델과, 본 발명의 ECLIPSE 모델의 각 패치 토큰 별 주의도를 산출한 것으로, ECLIPSE 모델이 이미지에서 주의 깊게 관찰해야할 객체(사람)에 관련된 패치들을 더욱 정확하게 판별하고 있는 것을 확인할 수 있다. 즉, ECLIPSE 모델이 토큰 희소화를 통해 이미지 분할 테스크에 우수한 성능을 보이는 것 을 확인할 수 있다. 따라서, 컴퓨팅 시스템은, 본 발명의 비전 트랜스포머를 이용하여 다양한 비전 테스크 업무를 기존 모델 대비 효율적이고 더욱 정확한 성능으로 수행할 수 있다. 예를 들어, 본 발명의 비전 트랜스포머는, 이미지 분류(Image Classification), 분할(segmentation), 객체 검 출(object detetion), 이미지 생성, 자동캡션생성, 이미지 검색, 이미지 설명 등의 비전 테스크 작업을 수행할 수 있다. 또한, 컴퓨팅 시스템은, 이러한 비전 테스크에 대해 우수한 성능을 가지는 비전 트랜스포머를 포함한 다 양한 애플리케이션을 실행하여, 각종 인공지능 업무를 수행할 수 있다. 그리고 이러한 대조 언어-이미지 사전 훈련에 토큰 희소화와 지식 증류를 활용한 프레임워크는 오디오 등의 추 가적인 양식에 대한 사전 훈련에도 통상의 기술자에 수준에서 확장되어 적용 가능할 것이다. 이상 설명된 본 발명에 따른 실시예는 다양한 컴퓨터 구성요소를 통하여 실행될 수 있는 프로그램 명령어의 형 태로 구현되어 컴퓨터 판독 가능한 기록 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능한 기록 매체는 프로그 램 명령어, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터 판독 가능한 기록 매체에 기록되는 프로그램 명령어는 본 발명을 위하여 특별히 설계되고 구성된 것이거나 컴퓨터 소프트웨 어 분야의 당업자에게 공지되어 사용 가능한 것일 수 있다. 컴퓨터 판독 가능한 기록 매체의 예에는, 하드 디 스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플롭티컬 디스크 (floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등과 같은, 프 로그램 명령어를 저장하고 실행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령어의 예에는, 컴파일러에 의하여 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용하여 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함된다. 하드웨어 장치는 본 발명에 따른 처리를 수행하기 위하여 하나 이 상의 소프트웨어 모듈로 변경될 수 있으며, 그 역도 마찬가지이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7"}
{"patent_id": "10-2023-0161587", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 지식 증류 프레임워크에 따라서 비전 트랜스포머 사전 훈련(ECLIPS: Expediting Contrastive Language-Image Pretrainin)하고, 사전 훈련된 비전 트랜스포머를 실행하는 컴퓨팅 시 스템의 블록도의 예시를 도시한다. 도 2는 본 발명의 실시예에 따른 따른 지식 증류 프레임워크에 따라서 비전 트랜스포머 사전 훈련하고, 사전 훈 련된 비전 트랜스포머를 실행하는 컴퓨팅 디바이스의 블록도의 예시를 도시한다. 도 3은 본 발명의 실시예에 따른 지식 증류 프레임워크를 통해 비전 트랜스포머 사전 훈련하고, 사전 훈련된 비 전 트랜스포머를 실행하는 컴퓨팅 디바이스에 대한 다른 측면에서의 블록도의 예시를 도시한다. 도 4는 본 발명의 실시예에 따른 지식 증류 프레임워크를 통해 비전 트랜스포머를 사전 훈련하는 방법에 대한 개념도이다. 도 5는 본 발명의 실시예에 따른 지식 증류를 통해 비전 트랜스포머를 사전 훈련하기 위한 프레임워크의 메타- 아키텍처(meta-architecture)를 나타낸다. 도 6은, 본 발명의 실시예에 따라 사전 훈련 방법 및 사전 훈련된 비전 트랜스포머의 효과를 나타내기 위해 기 존 사전 훈련 방법과 모델들과 비교한 그래프이다. 도 7은 본 발명의 실시예에 따라 사전 훈련된 비전 트랜스포머와 기존 비전 트랜스포머의 각각의 심층 레이어에 서의 어텐션 토큰을 비교한 도면이다."}
