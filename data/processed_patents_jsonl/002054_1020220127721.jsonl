{"patent_id": "10-2022-0127721", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0048207", "출원번호": "10-2022-0127721", "발명의 명칭": "사용자의 상황 정보 예측 기반 확장현실 디바이스의 영상 스트리밍 방법 및 장치", "출원인": "한국전자기술연구원", "발명자": "박우출"}}
{"patent_id": "10-2022-0127721", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "확장현실 디바이스로부터 현재 시점의 사용자 위치 정보와 포즈 정보를 포함하는 상황 정보를 수신하는 단계;상기 현재 시점의 상황 정보와 미리 설정된 이전 시점의 상황 정보를 입력으로 하는 미리 학습된 인공지능을 이용하여 미리 설정된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측하는 단계;상기 예측된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화에 기초하여 영상의 이미지 텍스처를 렌더링하는단계; 및상기 다음 시점에 상기 이미지 텍스처가 렌더링된 영상 데이터를 상기 확장현실 디바이스로 전송하는 단계를 포함하는, 확장현실 디바이스의 영상 스트리밍 방법."}
{"patent_id": "10-2022-0127721", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 수신하는 단계는,상기 확장현실 디바이스로부터 IMU(Inertial Measurement Unit) 정보를 상기 포즈 정보로 수신하는, 확장현실디바이스의 영상 스트리밍 방법."}
{"patent_id": "10-2022-0127721", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 수신하는 단계는,상기 확장현실 디바이스의 카메라에 의해 촬영된 영상을 수신하고, 상기 수신된 영상의 분석을 통해 상기 포즈정보를 획득하는, 확장현실 디바이스의 영상 스트리밍 방법."}
{"patent_id": "10-2022-0127721", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 예측하는 단계는,상기 인공지능에서 상기 현재 시점과 이전 시점 간의 사용자 위치 정보와 포즈 정보의 차이에 기초하여 상기 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측하는, 확장현실 디바이스의 영상 스트리밍 방법."}
{"patent_id": "10-2022-0127721", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 인공지능은,상기 상황 정보에 포함되는 연속적인 프레임 별 6자유도 정보를 기반으로 다음 시점의 사용자 위치 정보와 포즈정보의 변화를 예측하는, 확장현실 디바이스의 영상 스트리밍 방법.공개특허 10-2024-0048207-3-청구항 6 확장현실 디바이스로부터 현재 시점의 사용자 위치 정보와 포즈 정보를 포함하는 상황 정보를 수신하는 수신부;상기 현재 시점의 상황 정보와 미리 설정된 이전 시점의 상황 정보를 입력으로 하는 미리 학습된 인공지능을 이용하여 미리 설정된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측하는 예측부;상기 예측된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화에 기초하여 영상의 이미지 텍스처를 렌더링하는렌더링부; 및상기 다음 시점에 상기 이미지 텍스처가 렌더링된 영상 데이터를 상기 확장현실 디바이스로 전송하는 전송부를 포함하는, 확장현실 디바이스의 영상 스트리밍 장치."}
{"patent_id": "10-2022-0127721", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 수신부는,상기 확장현실 디바이스의 카메라에 의해 촬영된 영상을 수신하고, 상기 수신된 영상의 분석을 통해 상기 포즈정보를 획득하는, 확장현실 디바이스의 영상 스트리밍 장치."}
{"patent_id": "10-2022-0127721", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 예측부는,상기 인공지능에서 상기 현재 시점과 이전 시점 간의 사용자 위치 정보와 포즈 정보의 차이에 기초하여 상기 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측하는, 확장현실 디바이스의 영상 스트리밍 장치."}
{"patent_id": "10-2022-0127721", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서,상기 인공지능은,상기 상황 정보에 포함되는 연속적인 프레임 별 6자유도 정보를 기반으로 다음 시점의 사용자 위치 정보와 포즈정보의 변화를 예측하는, 확장현실 디바이스의 영상 스트리밍 장치."}
{"patent_id": "10-2022-0127721", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "렌더링 서버; 및확장현실 디바이스를 포함하고,상기 확장현실 디바이스는,현재 시점의 상기 확장현실 디바이스의 사용자 위치 정보와 포즈 정보를 포함하는 상황 정보를 획득하여 상기렌더링 서버로 전송하며,상기 렌더링 서버는,상기 확장현실 디바이스로부터 상기 현재 시점의 상황 정보를 수신하고,상기 현재 시점의 상황 정보와 미리 설정된 이전 시점의 상황 정보를 입력으로 하는 미리 학습된 인공지능을 이공개특허 10-2024-0048207-4-용하여 미리 설정된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측하며,상기 예측된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화에 기초하여 영상의 이미지 텍스처를렌더링하고,상기 다음 시점에 상기 이미지 텍스처가 렌더링된 영상 데이터를 상기 확장현실 디바이스로 전송하는, 영상 스트리밍 시스템."}
{"patent_id": "10-2022-0127721", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 실시예에 따른 확장현실 디바이스의 영상 스트리밍 방법은, 확장현실 디바이스로부터 현재 시점의 사용자 위치 정보와 포즈 정보를 포함하는 상황 정보를 수신하는 단계; 상기 현재 시점의 상황 정보와 미리 설정 된 이전 시점의 상황 정보를 입력으로 하는 미리 학습된 인공지능을 이용하여 미리 설정된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측하는 단계; 상기 예측된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화 에 기초하여 영상의 이미지 텍스처를 렌더링하는 단계; 및 상기 다음 시점에 상기 이미지 텍스처가 렌더링된 영 상 데이터를 상기 확장현실 디바이스로 전송하는 단계를 포함한다."}
{"patent_id": "10-2022-0127721", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 확장현실 디바이스의 영상 스트리밍 방법 및 장치에 관한 것이며, 보다 구체적으로 확장현실 예를 들 어, 가상현실 또는 증강현실 디바이스로부터 수신된 사용자의 시선과 움직임을 포함하는 상황 정보를 이용하여 다음 시점의 상황 정보를 예측함으로써, 확장현실 디바이스의 배터리 소모를 절약할 수 있는 확장현실 디바이스 의 영상 스트리밍 방법 및 장치에 대한 것이다."}
{"patent_id": "10-2022-0127721", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "VR/AR 서비스를 제공할 때 사용자는 디바이스 착용이 필수적이며, 이러한 디바이스는 착용에 따른 불편함이 따 른다. 저전력에 고품질의 실감형 3D 객체 등 영상을 재생하기 위해서는 컴퓨팅 파워가 많이 필요하다. 많은 컴 퓨팅 자원의 요구는 디바이스의 배터리 소모와 무게를 증가시켜서, 사용자가 머리 착용에 따른 불편함이 증가된 다. 이러한 문제점을 해결하기 위해서 클라우드 컴퓨팅의 자원을 활용하여, 대부분의 컴퓨팅 자원을 필요로 하는 렌 더링 작업 등을 처리하고, VR/AR 디바이스는 최소한의 작업을 수행하게 하는 기술 개발이 매우 중요하다. 즉, 클라우드 서버에서 사용자의 상황 정보 예를 들어, 시선, 움직임 등의 예측을 컴퓨팅하여 VR/AR 디바이스에 스트리밍을 하는 기술이 매우 중요하다. 이렇게 함으로써, VR/AR 디바이스 배터리 소모를 줄일 수 있고, 컴퓨팅 자원의 경량화를 달성할 수 있어서, 매우 필요한 기술이다."}
{"patent_id": "10-2022-0127721", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 기술적 과제는, 확장현실 예를 들어, 가상현실 또는 증강현실 디바이스로부터 수신된 사용자의 시선 과 움직임을 포함하는 상황 정보를 이용하여 다음 시점의 상황 정보를 예측함으로써, 확장현실 디바이스의 배터 리 소모를 절약할 수 있는 확장현실 디바이스의 영상 스트리밍 방법 및 장치를 제공하는데 그 목적이 있다. 본 개시에서 이루고자 하는 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급하지 않은"}
{"patent_id": "10-2022-0127721", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "또 다른 기술적 과제들은 아래의 기재로부터 본 개시가 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하 게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0127721", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따른 확장현실 디바이스의 영상 스트리밍 방법은, 확장현실 디바이스로부터 현재 시점의 사용자 위치 정보와 포즈 정보를 포함하는 상황 정보를 수신하는 단계; 상기 현재 시점의 상황 정보와 미리 설 정된 이전 시점의 상황 정보를 입력으로 하는 미리 학습된 인공지능을 이용하여 미리 설정된 다음 시점의 사용 자 위치 정보와 포즈 정보의 변화를 예측하는 단계; 상기 예측된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화에 기초하여 영상의 이미지 텍스처를 렌더링하는 단계; 및 상기 다음 시점에 상기 이미지 텍스처가 렌더링 된 영상 데이터를 상기 확장현실 디바이스로 전송하는 단계를 포함한다.이때, 상기 수신하는 단계는, 상기 확장현실 디바이스로부터 IMU(Inertial Measurement Unit) 정보를 상기 포즈 정보로 수신할 수 있다. 이때, 상기 수신하는 단계는, 상기 확장현실 디바이스의 카메라에 의해 촬영된 영상을 수신하고, 상기 수신된 영상의 분석을 통해 상기 포즈 정보를 획득할 수 있다. 이때, 상기 예측하는 단계는, 상기 인공지능에서 상기 현재 시점과 이전 시점 간의 사용자 위치 정보와 포즈 정 보의 차이에 기초하여 상기 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측할 수 있다. 이때, 상기 인공지능은, 상기 상황 정보에 포함되는 연속적인 프레임 별 6자유도 정보를 기반으로 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측할 수 있다. 본 개시의 다른 실시예에 따른 확장현실 디바이스의 영상 스트리밍 장치는, 확장현실 디바이스로부터 현재 시점 의 사용자 위치 정보와 포즈 정보를 포함하는 상황 정보를 수신하는 수신부; 상기 현재 시점의 상황 정보와 미 리 설정된 이전 시점의 상황 정보를 입력으로 하는 미리 학습된 인공지능을 이용하여 미리 설정된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측하는 예측부; 상기 예측된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화에 기초하여 영상의 이미지 텍스처를 렌더링하는 렌더링부; 및 상기 다음 시점에 상기 이미지 텍스 처가 렌더링된 영상 데이터를 상기 확장현실 디바이스로 전송하는 전송부를 포함한다. 본 개시의 다른 실시예에 따른 영상 스트리밍 시스템은, 렌더링 서버; 및 확장현실 디바이스를 포함하고, 상기 확장현실 디바이스는, 현재 시점의 상기 확장현실 디바이스의 사용자 위치 정보와 포즈 정보를 포함하는 상황 정보를 획득하여 상기 렌더링 서버로 전송하며, 상기 렌더링 서버는, 상기 확장현실 디바이스로부터 상기 현재 시점의 상황 정보를 수신하고, 상기 현재 시점의 상황 정보와 미리 설정된 이전 시점의 상황 정보를 입력으로 하는 미리 학습된 인공지능을 이용하여 미리 설정된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측 하며, 상기 예측된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화에 기초하여 영상의 이미지 텍스처를 렌더 링하고, 상기 다음 시점에 상기 이미지 텍스처가 렌더링된 영상 데이터를 상기 확장현실 디바이스로 전송하는 것을 특징으로 한다."}
{"patent_id": "10-2022-0127721", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "본 개시에 대하여 위에서 간략하게 요약된 특징들은 후술하는 본 개시의 상세한 설명의 예시적인 양상일 뿐이며, 본 개시의 범위를 제한하는 것은 아니다."}
{"patent_id": "10-2022-0127721", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 확장현실 예를 들어, 가상현실 또는 증강현실 디바이스로부터 수신된 사용자의 시선과 움직 임을 포함하는 상황 정보를 이용하여 다음 시점의 상황 정보를 예측함으로써, 확장현실 디바이스의 배터리 소모 를 절약할 수 있는 확장현실 디바이스의 영상 스트리밍 방법 및 장치를 제공할 수 있다. 본 개시에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은 아래의 기재로부터 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0127721", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참고로 하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나, 본 개시는 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 본 개시의 실시예를 설명함에 있어서 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그에 대한 상세한 설명은 생략한다. 그리고, 도면에서 본 개시에 대한 설명과 관계없 는 부분은 생략하였으며, 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 개시에 있어서, 어떤 구성요소가 다른 구성요소와 \"연결\", \"결합\" 또는 \"접속\"되어 있다고 할 때, 이는 직접 적인 연결 관계 뿐만 아니라, 그 중간에 또 다른 구성요소가 존재하는 간접적인 연결관계도 포함할 수 있다. 또 한 어떤 구성요소가 다른 구성요소를 \"포함한다\" 또는 \"가진다\"고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 배제하는 것이 아니라 또 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 본 개시에 있어서, 제1, 제2 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용되 며, 특별히 언급되지 않는 한 구성요소들 간의 순서 또는 중요도 등을 한정하지 않는다. 따라서, 본 개시의 범 위 내에서 일 실시예에서의 제1 구성요소는 다른 실시예에서 제2 구성요소라고 칭할 수도 있고, 마찬가지로 일 실시예에서의 제2 구성요소를 다른 실시예에서 제1 구성요소라고 칭할 수도 있다. 본 개시에 있어서, 서로 구별되는 구성요소들은 각각의 특징을 명확하게 설명하기 위한 것일 뿐, 구성요소들이 반드시 분리되는 것을 의미하지는 않는다. 즉, 복수의 구성요소가 통합되어 하나의 하드웨어 또는 소프트웨어 단위로 이루어질 수도 있고, 하나의 구성요소가 분산되어 복수의 하드웨어 또는 소프트웨어 단위로 이루어질 수 도 있다. 따라서, 별도로 언급하지 않더라도 이와 같이 통합된 또는 분산된 실시예도 본 개시의 범위에 포함된 다. 본 개시에 있어서, 다양한 실시예에서 설명하는 구성요소들이 반드시 필수적인 구성요소들은 의미하는 것은 아 니며, 일부는 선택적인 구성요소일 수 있다. 따라서, 일 실시예에서 설명하는 구성요소들의 부분집합으로 구성 되는 실시예도 본 개시의 범위에 포함된다. 또한, 다양한 실시예에서 설명하는 구성요소들에 추가적으로 다른 구성요소를 포함하는 실시예도 본 개시의 범위에 포함된다. 본 개시에 있어서, 본 명세서에 사용되는 위치 관계의 표현, 예컨대 상부, 하부, 좌측, 우측 등은 설명의 편의 를 위해 기재된 것이고, 본 명세서에 도시된 도면을 역으로 보는 경우에는, 명세서에 기재된 위치 관계는 반대 로 해석될 수도 있다. 본 개시에 있어서, \"A 또는 B\", \"A 및 B 중 적어도 하나\", \"A 또는 B 중 적어도 하나\", \"A, B 또는 C\", \"A, B 및 C 중 적어도 하나\", 및 \"A, B, 또는 C 중 적어도 하나\"와 같은 문구들 각각은 그 문구들 중 해당하는 문구에 함께 나열된 항목들 중 어느 하나, 또는 그들의 모든 가능한 조합을 포함할 수 있다. 콘텐츠 재현 공간이라 함은 크게 콘텐츠가 정합되어 보여지는 공간을 말하며 각각의 XR(VR, AR, VR, MR)에 따라 다른 특징 및 한계점을 가지고 있다. 현재 메타버스 공간으로 거론되는 공간들은 모두 VR(Virtual Reality) 환경으로, 고정된 크기의 세계를 가지게 되며, 모든 환경이 인위적으로 제작된 공간을 말한다. VR 콘텐츠를 즐길 때, 사용자는 시각적으로 현실 세계와 완전히 단절되어 있으며, 외부 세계의 변화가 재생되는 콘텐츠에 영향을 주지 않는다. AR(Augmented Reality)은 가상으로 만들어진 객체 기반의 콘텐츠를 사용자가 바라보는 시점에 '덮어 씌우는' 형 태로 가상의 콘텐츠를 재생하여 현실 세계에 겹쳐서 재생한다, 간단한 사물 인식 정도를 통해 콘텐츠가 재생될 위치 정도가 변화하나, 현실 세계와 동화되지는 않는다. MR(Mixed Reality)은 가상으로 만들어진 객체 콘텐츠가 재생되는 점은 동일하나, 사용자의 시점에 보이는 현실 세계와 조화를 이루어 콘텐츠를 재생한다. MR 콘텐츠가 재생될 때에는 먼저, 사용자가 바라보는 시점의 현실 세계를 독자적인 좌표계를 갖는 투명한 가상 공간이 만들어지게 된다. 가상 공간이 만들어지면 가상 콘텐츠가 배 치되게 되며 현실 세계의 환경에 따라 해당 콘텐츠가 보여지는 모습이 달라져 현실 세계와 동화(mixed)되게 된 다. 확장현실(XR)은 VR과 AR을 아우르는 MR 기술을 망라하는 것으로, VR/AR 기술의 개별 활용 또는 혼합 활용을 자 유롭게 선택하며, 확장된 현실을 창조한다. 본 개시의 실시예들은, 사용자 XR 디바이스의 배터리 절약 및 경량화를 위하여, 렌더링 서버에서 사용자 XR 디 바이스로부터 수신되는 현재 시점의 상황 정보 예를 들어, 사용자 위치 정보, 카메라에 의해 촬영된 영상 정보 또는 포즈 정보를 기반으로 인공지능을 이용하여 다음 시점의 상황 정보(예를 들어, 사용자의 시선, 움직임 변 화)를 예측하고, 예측된 상황 정보를 기반으로 다음 시점의 영상의 이미지 텍스처를 미리 렌더링하여 다음 시점 에 이미지 텍스처가 렌더링된 영상 데이터를 사용자 XR 디바이스로 실시간으로 전송하는 것을 그 요지로 한다. 이때, 인공지능은, 상황 정보에 포함되는 연속적인 프레임 별 6자유도 정보를 기반으로 다음 시점의 사용자 위 치 정보와 포즈 정보의 변화를 예측할 수 있는 것으로, 이러한 인공지능을 학습하기 위하여 학습 데이터를 수집 하고, 수집된 학습 데이터를 기반으로 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측하기 위한 학습 모델을 학습시킬 수 있다. 본 개시의 실시예에서의 영상 예를 들어, VR/AR 콘텐츠는 포인트 클라우드 기반의 콘텐츠일 수 있으며, 렌더링 서버는 포인트 클라우드 기반의 콘텐츠를 재생할 수 있도록 3D 엔진을 활용하여 포인트 클라우드 콘텐츠를 가상 공간 안에서 실시간으로 렌더링하고 이를 XR 디바이스로 전송할 수 있다. 즉, 본 개시의 실시예들은, 고성능의 컴퓨팅 자원을 요구하는 포인트 클라우드 기반의 영상을 기존에 널리 보급 된 단말이나 경량형 사용자 장치(또는 사용자 단말)에서도 시청할 수 있다. 이러한 본 개시의 실시예들에 따른 포인트 클라우드 기반의 스트리밍은 객체를 중심으로 콘텐츠를 재현하기 때 문에 모든 XR 영역에서 활용이 가능하다. 포인트 클라우드에 대하여 간략하게 설명하면, 포인트 클라우드(point cloud)는, Lidar 센서, RGB-D센서 등으로 수집되는 데이터를 의미한다. 이러한 센서들은 물체에 빛/신호를 보내서 돌아오는 시간을 기록하여 각 빛/신호 당 거리 정보를 계산하고, 하나의 포인트를 생성한다. 포인트 클라우드는 3차원 공간상에 퍼져 있는 여러 포인 트(Point)의 집합(set cloud)를 의미한다. 포인트 클라우드(Point Cloud)는 2D 이미지와 다르게 깊이(z축) 정보를 가지고 있기 때문에, 기본적으로 N X 3 Numpy 배열로 표현된다. 여기서, 각 N 줄은 하나의 포인트와 맵핑이 되며, 3(x, y, z) 정보를 가지고 있다. 이러한 포인트 클라우드 기반 콘텐츠 예를 들어, 영상 콘텐츠를 경량형 XR 디바이스에서 적은 컴퓨팅 파워로도 재생하거나 즐길 수 있으며, 이를 통해 배터리 소모를 절약할 수 있는 기술에 대하여 설명하면 다음과 같다. 도 1은 본 개시의 일 실시예에 따른 영상 스트리밍 시스템의 구성을 나타낸 것이다. 도 1을 참조하면, 본 개시의 일 실시예에 따른 영상 스트리밍 시스템은, 포인트 클라우드 획득 장치, 포인 트 클라우드 전송 장치, 렌더링 서버와 XR 디바이스를 포함한다. 렌더링 서버에서 제공할 포인트 클라우드 영상을 렌더링 서버 내 로컬 파일로부터 재생하는 경우, 시스템 구성에 포인트 클라우드 획득 장치 및 포인트 클라우드 전송 장치가 필요하지 않다. 이 경우, 렌더링 서버와 사용자 단말만 있어도 서비스가 가능하다. 물론, 렌더링 서버는 포인트 클라우드 영상을 용량 등의 문제로 이미 압축되어 있는 것을 저장하여 사용할 수 있다. 포인트 클라우드 획득 장치는, XR 디바이스에서 재생될 포인트 클라우드 콘텐츠의 로(raw) 데이터를 수집하는 장치를 말한다. 이때, 포인트 클라우드 획득 장치는, 포인트 클라우드를 획득하는 장치 예를 들어, Microsoft사의 Azure Kinect를 이용하여 포인트 클라우드를 획득할 수도 있고, RGB 카메라를 이용하여 실사 객체로부터 포인트 클라 우드를 획득할 수도 있다. 포인트 클라우드는 3D 엔진을 통해 가상의 객체로부터 얻을 수도 있으며, 최종적으로는 촬영 대상이 실사든 CG 로 만들어진 가상의 객체든 포인트 클라우드 영상 형태로 나오게 된다. 다만, 실사의 경우 모든 면을 촬영하는 것을 보통으로 하기 때문에 1대 이상의 카메라를 사용하여 촬영하게 된다. 포인트 클라우드 획득 장치는 로 포 맷(raw format)으로 영상을 획득하기 때문에 출력물의 용량이 큰 편이다. 포인트 클라우드 전송 장치는, 포인트 클라우드 획득 장치에 의해 획득된 포인트 클라우드 영상 데이 터를 렌더링 서버로 전송하는 장치로, 네트워크 장치를 통해 압축된 포인트 클라우드 영상 데이터를 렌더 링 서버로 전송한다. 이때, 포인트 클라우드 전송 장치는, 하나의 서버나 PC일 수 있다. 즉, 포인트 클라우드 전송 장치는, 로 포맷이 포인트 클라우드 영상 데이터를 입력으로 수신할 수 있으며, 압축된 포인트 클라우드 영상을 렌더링 서버로 출력할 수 있다. 포인트 클라우드 영상의 압축은 상당히 고난도의 기술 및 높은 사양의 시스템을 요구한다. 포인트 클라우드 영 상의 압축 방법과 기술 등은 이 기술 분야에 종사하는 당업자에게 있어서 자명하기에 그 상세한 설명은 생략한 다. 실시예에 따라, 포인트 클라우드 전송 장치는, 포인트 클라우드 데이터가 여러 포인트 클라우드 획득 장치 에 의해 획득되는 경우 여러 포인트 클라우드 획득 장치에 의해 획득된 데이터를 동기화한 후에 하나의 압 축된 포인트 클라우드 영상을 만들어낼 수 있다. 렌더링 서버는, 본 개시의 실시예에 따른 영상 스트리밍 장치에 해당하는 장치로, 압축된 포인트 클라우드 영상을 렌더링하여 가상 공간 내에 클라우드 포인트 영상을 재생하고, XR 디바이스로부터 현재 시점의 사 용자 위치 정보와 포즈 정보를 포함하는 상황 정보를 수신하고, 현재 시점의 상황 정보와 미리 설정된 이전 시 점의 상황 정보를 입력으로 하는 미리 학습된 인공지능을 이용하여 미리 설정된 다음 시점의 사용자 위치 정보 와 포즈 정보의 변화를 예측하며, 예측된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화에 기초하여 영상의 이미지 텍스처를 렌더링하고, 다음 시점에 이미지 텍스처가 렌더링된 영상 데이터를 XR 디바이스로 전송한 다. 이때, 렌더링 서버는, XR 디바이스의 포즈 정보를 XR 디바이스에 의해 측정된 IMU(Inertial Measurement Unit) 정보로 수신할 수도 있고, XR 디바이스로부터 카메라에 의해 촬영된 영상 정보가 수신 되는 경우 영상 분석을 통해 포즈 정보를 추정할 수도 있다. 상황에 따라, 렌더링 서버는, XR 디바이스 로부터 카메라 내/외부 파라미터 값도 함께 수신할 수도 있으며, IMU 데이터 값은 가속도, 회전 속도 그리 고 자력계 등을 포함할 수 있다. 실시예에 따라, 렌더링 서버는, 입력 데이터로 압축된 포인트 클라우드 영상 3종(color geometry, occupancy)과 XR 디바이스로부터 현재 시점의 사용자 위치 정보와 포즈 정보를 수신하고, 다음 시점의 사 용자 위치 정보와 포즈 정보의 변화를 예측함으로써, 다음 시점의 2차원 영상을 미리 렌더링함으로써, 다음 시 점의 사용자 위치 정보와 포즈 정보를 수신될 때, 미리 렌더링된 2차원 영상을 XR 디바이스로 실시간으로 전송할 수 있다. 물론, 렌더링 서버는, XR 디바이스로부터 현재 시점의 상황 정보를 수신하는 시점에, 이전 시점에서 이미 렌더링된 현재 시점의 2차원 영상을 XR 디바이스로 실시간으로 전송할 수 있 다. 여기서, 렌더링 서버는, 압축된 포인트 클라우드 영상을 로컬 파일 형태로 저장하고 있거나 포인트 클라우 드 전송 장치로부터 압축된 포인트 클라우드 영상을 수신할 수도 있다. 이러한 렌더링 서버는, 인공지능을 이용하여 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측하 기 위하여, 인공지능을 사전에 미리 학습 예를 들어, CNN 또는 RNN 학습할 수 있으며, 인공지능을 학습시키기 위한 학습 데이터를 수집할 수 있다. 예를 들어, 렌더링 서버는, 복수의 모바일 디바이스들로부터 사용자 의 머리 움직임에 대한 데이터와 머리 움직임에 대한 영상 정보 또는 IMU 값을 수집할 수 있다. 이때, 모바일 디바이스는 데이터를 수집하기 위한 소프트웨어가 탑재될 수 있으며, 해당 소프트웨어를 이용하여, 사용자의 머 리 움직임에 대한 데이터 수집, 카메라 예를 들어, ARCore 카메라로 촬영 및 영상 기반 포즈 분석 그리고 영상 의 각 프레임 단위로 카메라 포즈(6DOF) 정보를 기록하고, 이러한 데이터를 특정 형태의 파일로 저장할 수 있다. 실시예에 따라, 모바일 디바이스는 수집된 데이터를 csv 파일로 저장할 수 있으며, 해당 파일에는 영상 해상도, Center pixel, Focal length, 4 × 4 pose matrix 등의 정보가 저장될 수 있다. 렌더링 서버는, 다양한 상황 및 환경 속에서 사용자의 자연스러운 모션 데이터 즉, 학습 데이터를 모바일 디바이스로부터 수집할 수 있으며, 이렇게 수집된 학습 데이터를 이용하여 이전 시점과 현재 시점의 입력 데이 터(예를 들어, 사용자의 위치 정보와 포즈 정보를 포함하는 상황 정보)에 대한 다음 시점에서의 사용자의 위치 정보와 포즈 정보의 변화를 예측하기 위한 인공지능을 학습할 수 있다. 실시예에 따라, 인공지능은, CSV 파일의 연속적인 프레임 별 6자유도 정보를 기반으로 사용자의 위치 및 자세변화를 예측할 수 있으며, 연속된 영상 프 레임 간 가중치 정보 및 자세 변화 예측 정보를 획득하기 위하여 컨볼루션(convolution) 기법을 적용할 수 있다. 예를 들어, 연산량을 경량화 하면서, 정보 손실이 일어나지 않으며 유의미한 정보만을 추출하기 위한 dilated 컨볼루션(또는 atrous 컨볼루션) 기법을 적용하여 인공지능을 학습시킬 수 있으며, 이렇게 학습된 인공 지능은, 컨볼루션을 통해 프레임 별 정보 기반으로 회전과 이동 값을 예측하여 결과를 도출할 수 있다. 물론, 본 개시의 실시예에서 인공지능을 학습하는 과정이 상술한 내용으로 제한되거나 한정되지 않으며, CSV 파 일의 연속적인 프레임 별 6자유도 정보를 기반으로 사용자의 위치 및 자세변화를 예측하기 위한 모든 학습 과정 이 적용될 수 있다. XR 디바이스는, XR 디바이스를 착용한 사용자의 위치 정보와 포즈 정보 또는 영상 정보를 측정 또는 획득하여 렌더링 서버로 전송하고, 사전 예측을 통해 미리 렌더링된 현재 시점의 영상 데이터를 렌더링 서 버로부터 수신하여 디스플레이한다. 이때, XR 디바이스는, IMU 센서를 이용하여 사용자 단말의 IMU 정보를 측정하고, 이렇게 측정된 IMU 정보 를 렌더링 서버로 전송할 수도 있고, 카메라에 의해 촬영된 영상 정보를 전송하여 렌더링 서버에서 영상 기반 분석을 통하여 포즈 정보를 획득할 수도 있으며, 카메라에 의해 촬영된 영상 정보의 분석을 통하여 포즈 정보를 XR 디바이스에서 획득하여 렌더링 서버로 전송할 수도 있다. 이러한 XR 디바이스는, 안경이나, 헤드셋 또는 스마트 폰 형태 뿐만 아니라 본 개시의 실시예에 따른 기술 이 적용 가능한 모든 단말을 포함할 수 있으며, 2D 영상을 빠르게 복호화 할 수 있는 하드웨어 디코더, 영상을 보여줄 수 있는 디스플레이, 영상을 촬영할 수 있는 촬영 수단(예를 들어, 카메라 등), 디바이스의 포즈에 대한 로(raw) 데이터를 획득할 수 있는 IMU 센서, 그리고 IMU 정보를 전송할 수 있는 네트워크 장치 등을 보유할 수 있다. 여기서, 네트워크 장치는, 렌더링 서버와 통신을 수행할 수 있는 모든 네트워크를 포함할 수 있으며, 예를 들어, 셀룰러 네트워크(LTE, 5G)에 접속하기 위한 장치, Wi-Fi 등에 접속하기 위한 장치 등을 포함할 수 있다. 물론, 네트워크 장치는 상기 기능의 장치 뿐만 아니라 본 기술에 적용 가능한 모든 네트워크 장치를 포함할 수 있다. 실시예에 따라, XR 디바이스는, XR 디바이스에 내장된 IMU 센서를 통해 XR 디바이스의 위치 및 회전 매트릭스의 값을 획득할 수도 있다. 여기서, 좌표 시스템은 해당 데이터를 가공해주는 시스템에 의존할 수 있으며, 예를 들어, 안드로이드 스마트폰의 경우 OpenGL의 좌표 시스템에 의존할 수 있다. XR 디바이스는, IMU 센서에 의해 획득된 XR 디바이스의 위치와 회전 매트릭스를 하나의 4 × 4 매트 릭스로 구성할 수 있으며, 아래 <수학식 1>과 같이 나타낼 수 있다. [수학식 1]"}
{"patent_id": "10-2022-0127721", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, R11 ~ R33 은, XR 디바이스의 회전 매트릭스를 의미하고, T1 ~ T3은 사용자 단말의 3차원 공간 상의 위치를 나타내는 좌표를 의미할 수 있다. 각 매트릭스의 요소들은 각각 float 형태의 4바이트 크기의 데이터이며 하나의 매트릭스 당 총 64바이트의 크기 를 가질 수 있다. XR 디바이스는, 상황 정보 데이터를 시스템에 정의된 전송 방식에 따라 렌더링 서버에 전송한다. 여 기서, XR 디바이스는, 빠른 전송을 위해 TCP 방식에서는 로 소켓(raw socket)을 이용하여 상황 정보 데이 터를 전송할 수 있으며, UDP에서는 QUIC 프로토콜 전송 방법을 사용하여 상황 정보 데이터를 전송할 수 있다. 도 2는 도 1에 도시된 렌더링 서버에 대한 일 실시예의 구성을 나타낸 것으로, 본 개시의 실시예에 따른 영상 스트리밍 장치의 구성을 나타낸 것이다. 도 2를 참조하면, 렌더링 서버는, 수신부, 예측부, 렌더링부와 전송부를 포함한다. 수신부는, XR 디바이스로부터 상황 정보를 수신하고, 포인트 클라우드 전송 장치로부터 포인트 클라 우드 영상 데이터가 전송되는 경우 포인트 클라우드 영상 데이터를 수신한다. 여기서, 수신부는, XR 디바이스로부터 사용자 위치 정보와 포즈 정보를 수신할 수 있으며, 포즈 정보 는 영상 정보 또는 IMU 정보(IMU data) 중 적어도 하나를 포함할 수 있다. 예측부는, 현재 시점에 수신되는 상황 정보와 미리 설정된 이전 시점에 수신된 상황 정보를 입력으로 하는 미리 학습된 인공지능을 이용하여 미리 설정된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측한다. 즉, 예측부는, XR 디바이스를 착용하고 있는 사용자의 움직임 변화를 예측한다. 이때, 예측부는, 인공지능에서 현재 시점과 이전 시점 간의 사용자 위치 정보와 포즈 정보의 차이에 기초 하여 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측할 수 있다. 실시예에 따라, 예측부는, 수신부를 통해 영상 정보가 수신되는 경우 해당 시점의 영상 정보를 분석 함으로써, 해당 시점의 포즈 정보를 획득할 수도 있다. 렌더링부는, 예측부에 의해 예측된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화에 기초하여 영 상의 이미지 텍스처를 렌더링한다. 나아가, 렌더링부는, 포인트 클라우드 전송 장치로부터 포인트 클라우드 영상에 대응하는 클라우드 포인트 데이터가 수신되는 경우, 클라우드 포인트 데이터에 포함되는 채널들 예를 들어, color data 채널, geometry data 채널과 occupancy data 채널 각각의 영상을 복호화하여 렌더링함으로써, XR 서비스를 제공하는 공간 예를 들어, 가상 공간에 클라우드 포인트 영상(VR/AR 콘텐츠)을 재생할 수 있다. 전송부는, 렌더링부에 의해 이미지 텍스처가 렌더링된 영상 데이터 예를 들어, 2D 영상을 다음 시점 에 XR 디바이스로 전송한다. 이러한 구성을 포함하는 렌더링 서버의 동작에 대하여 조금 더 설명하면 다음과 같다. 렌더링 서버는, 다 음 시점의 영상을 미리 렌더링하기 위하여, XR 디바이스로부터 전달된 상황 정보와 이전 시점의 상황 정보 를 이용하여 다음 시점에서 사용자 위치 정보와 포즈 정보가 어떻게 변화하였는지 예측하여야 한다. 즉, 렌더링 서버는, 다음 시점에서 사용자가 어느 부분을 보고 있는지 미리 예측하여야 한다. 예를 들어, 도 3에 도시된 바와 같이, 렌더링 서버는 현재 시점이 A라는 시점으로 바라보고 있다 가정한 상태에서 현재 시점의 상황 정보가 수신되면 다음 시점에서 B라는 시점을 예측함으로써, 다음 시점의 영상의 텍 스처 이미지를 렌더링할 수 있다. 실시예에 따라, VR 콘텐츠의 경우, 가상 공간 내에 가상 카메라를 통해 사용 자의 시점을 반영한다 가정하면, A 시점으로 이미 배치된 가상 카메라를 B 시점으로 변경함으로써, 다음 시점인 B 시점에서 영상의 텍스처 이미지를 미리 렌더링할 수 있다. 그리고, 포인트 클라우드 영상은 각 포인트 별로 color, geometry, 그리고 occupancy로 구성되어 있기 때문에 총 3채널의 영상을 동시에 재생해야 한다. 렌더링 서버가 네트워크를 통해 3채널의 영상을 수신하는 경우 각각 의 영상을 모두 받아 복호화 후 렌더링하게 되며, 렌더링되는 포인트 클라우드 영상은 3차원 가상 공간 내에 재 생된다. 예를 들어, 도 4에 도시된 바와 같이, 렌더링 서버는 포인트 클라우드 영상을 수신하여 각 채널의 영상을 렌더링함으로써, 가상 공간 내에 포인트 클라우드 영상을 재생할 수 있다. 렌더링 서버는, 다음 시점에 대하여 예측된 사용자 위치 정보와 포즈 정보의 변화에 의해 보여지는 이미지 텍스처를 렌더링 하도록 GPU에 명령을 내릴 수 있다. GPU로부터 얻어진 이미지 텍스처는 H.264, HEVC 등의 코덱을 통해 압축(또는 인코딩)되며, 압축된 영상은 다시 적절한 파일 포맷에 담겨(muxing) 최종적으로 영상 데이터가 생성된다. 이렇게 생성된 2D 영상 데이터는 통신 인터페이스를 통해 다음 시점에 XR 디바이스로 전달됨으로써, XR 디바이스는 다음 시점에 대한 2D 영상을 빠르게 수신하여 디스플레이할 수 있다. 예를 들어, 렌더링 서버는, 도 4에 도시된 바와 같이, 다음 시점에 해당하는 영상의 이미지 텍스처를 렌더링함으로써, 도 5에 도시된 바와 같이, 다음 시점 텍스처 획 득 영역에 해당하는 2D 영상을 생성할 수 있으며, 이렇게 생성된 도 5의 다음 시점 텍스처 획득 영역(61 0)의 2D 영상을 다음 시점에 XR 디바이스로 전달하게 된다. 이러한 과정이 실시간으로 반복된다. 이와 같이, 본 개시의 실시예에 따른 영상 스트리밍 장치는, 고성능의 컴퓨팅 자원을 요구하는 VR/AR 콘텐츠 등 을 기존에 널리 보급된 단말이나 경량형 사용자 장치에서도 시청할 수 있고, 영상 콘텐츠를 경량형 XR 디바이스 에서 적은 컴퓨팅 파워로도 재생하거나 즐길 수 있으며, 이를 통해 배터리 소모를 절약할 수 있다. 또한, 본 개시의 실시예에 따른 영상 스트리밍 장치는, XR 디바이스의 사용자 움직임 변화 등을 렌더링 서버에 서 예측하고, 예측된 사용자 움직임 변화에 따라 영상을 미리 렌더링함으로써, 네트워크 과부하 및 그에 따른 XR 디바이스 즉, 사용자 단말의 연산량 및 배터리 소모량을 줄일 수 있다. 또한, 본 개시의 실시예에 따른 영상 스트리밍 시스템은, 복잡하거나 변수가 많은 연산을 렌더링 서버에서 수행 하므로, XR 디바이스에 탑재되는 소프트웨어가 간단해지고 호환성을 높일 수 있다. 도 6은 본 개시의 다른 실시예에 따른 영상 스트리밍 방법의 동작 흐름도를 나타낸 것으로, 도 1 내지 도 5의 장치 또는 시스템에서 수행되는 동작 흐름도를 나타낸 것이다. 도 6을 참조하면, 본 개시의 다른 실시예에 따른 영상 스트리밍 방법은, XR 디바이스로부터 현재 시점의 사용자 위치 정보와 포즈 정보를 포함하는 상황 정보를 수신하고, 현재 시점의 상황 정보와 미리 설정된 이전 시점의 상황 정보를 입력으로 하는 미리 학습된 인공지능을 이용하여 미리 설정된 다음 시점의 사용자 위치 정보와 포 즈 정보의 변화를 예측한다(S610, S620). 여기서, 단계 S620은, 인공지능에서 현재 시점과 이전 시점 간의 사용자 위치 정보와 포즈 정보의 차이에 기초 하여 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 예측할 수 있다. 실시예에 따라, 인공지능은, 상황 정보에 포함되는 연속적인 프레임 별 6자유도 정보를 기반으로 다음 시점의 사용자 위치 정보와 포즈 정보의 변 화를 예측할 수도 있다. 단계 S620에 의해 다음 시점의 사용자 위치 정보와 포즈 정보의 변화가 예측되면, 예측된 다음 시점의 사용자 위치 정보와 포즈 정보의 변화에 기초하여 영상의 이미지 텍스처를 렌더링하고, 다음 시점에 이미지 텍스처가 렌더링된 영상 데이터 예를 들어, 2D 영상을 XR 디바이스로 전송한다(S630, S640). 비록, 도 6의 방법에서 그 설명이 생략되더라도, 본 개시의 실시예에 따른 방법은 도 1 내지 도 5의 장치 또는 시스템에서 설명한 모든 내용을 포함할 수 있으며, 이는 해당 기술 분야에 종사하는 당업자에게 있어서 자명하 다. 도 7은 본 개시의 다른 실시예에 따른 영상 스트리밍 장치가 적용되는 디바이스의 구성도를 나타낸 것이다. 예를 들어, 도 2의 본 개시의 다른 실시예에 따른 영상 스트리밍 장치는 도 7의 디바이스가 될 수 있다. 도 7을 참조하면, 디바이스는 메모리, 프로세서, 송수신부 및 주변 장치를 포함 할 수 있다. 또한, 일 예로, 디바이스는 다른 구성을 더 포함할 수 있으며, 상술한 실시예로 한정되지 않 는다. 이때, 상기 디바이스는 예를 들어 이동 가능한 사용자 단말기(예를 들어, 스마트 폰, 노트북, 웨어 러블 기기 등) 이거나 고정된 관리 장치(예를 들어, 서버, PC 등) 일 수 있다. 보다 상세하게는, 도 7의 디바이스는 콘텐츠 제공 서버, 확장 영상 서비스 서버, 클라우드 포인트 영상 제공 서버 등과 같은 예시적인 하드웨어/소프트웨어 아키텍처일 수 있다. 이때, 일 예로, 메모리는 비이 동식 메모리 또는 이동식 메모리일 수 있다. 또한, 일 예로, 주변 장치는 디스플레이, GPS 또는 다른 주 변기기들을 포함할 수 있으며, 상술한 실시예로 한정되지 않는다. 또한, 일 예로, 상술한 디바이스는 상기 송수신부와 같이 통신 회로를 포함할 수 있으며, 이에 기 초하여 외부 디바이스와 통신을 수행할 수 있다.또한, 일 예로, 프로세서는 범용 프로세서, DSP(digital signal processor), DSP 코어, 제어기, 마이크 로제어기, ASIC들(Application Specific Integrated Circuits), FPGA(Field Programmable Gate Array) 회로들, 임의의 다른 유형의 IC(integrated circuit) 및 상태 머신과 관련되는 하나 이상의 마이크로프로세서 중 적어도 하나 이상일 수 있다. 즉, 상술한 디바이스를 제어하기 위한 제어 역할을 수행하는 하드웨어적 /소프트웨어적 구성일 수 있다. 또한 상기 프로세서는 전술한 도 2의 예측부와 렌더링부의 기 능을 모듈화하여 수행할 수 있다. 이때, 프로세서는 영상 스트리밍 장치의 다양한 필수 기능들을 수행하기 위해 메모리에 저장된 컴 퓨터 실행가능한 명령어들을 실행할 수 있다. 일 예로, 프로세서는 신호 코딩, 데이터 처리, 전력 제어, 입출력 처리 및 통신 동작 중 적어도 어느 하나를 제어할 수 있다. 또한, 프로세서는 물리 계층, MAC 계 층, 어플리케이션 계층들을 제어할 수 있다. 또한, 일 예로, 프로세서는 액세스 계층 및/또는 어플리케이 션 계층 등에서 인증 및 보안 절차를 수행할 수 있으며, 상술한 실시예로 한정되지 않는다. 일 예로, 프로세서는 송수신부를 통해 다른 장치들과 통신을 수행할 수 있다. 일 예로, 프로세서 는 컴퓨터 실행가능한 명령어들의 실행을 통해 영상 스트리밍 장치가 네트워크를 통해 다른 장치들과 통 신을 수행하게 제어할 수 있다. 즉, 본 개시에서 수행되는 통신이 제어될 수 있다. 일 예로, 송수신부는 안테나를 통해 RF 신호를 전송할 수 있으며, 다양한 통신망에 기초하여 신호를 전송할 수 있다. 또한, 일 예로, 안테나 기술로서 MIMO 기술, 빔포밍 등이 적용될 수 있으며, 상술한 실시예로 한정되지 않는다. 또한, 송수신부를 통해 송수신한 신호는 변조 및 복조되어 프로세서에 의해 제어될 수 있으며, 상 술한 실시예로 한정되지 않는다. 본 개시의 예시적인 방법들은 설명의 명확성을 위해서 동작의 시리즈로 표현되어 있지만, 이는 단계가 수행되는 순서를 제한하기 위한 것은 아니며, 필요한 경우에는 각각의 단계가 동시에 또는 상이한 순서로 수행될 수도 있 다. 본 개시에 따른 방법을 구현하기 위해서, 예시하는 단계에 추가적으로 다른 단계를 포함하거나, 일부의 단 계를 제외하고 나머지 단계를 포함하거나, 또는 일부의 단계를 제외하고 추가적인 다른 단계를 포함할 수도 있 다. 본 개시의 다양한 실시예는 모든 가능한 조합을 나열한 것이 아니고 본 개시의 대표적인 양상을 설명하기 위한 것이며, 다양한 실시예에서 설명하는 사항들은 독립적으로 적용되거나 또는 둘 이상의 조합으로 적용될 수도 있 다. 또한, 본 개시의 다양한 실시예는 하드웨어, 펌웨어(firmware), 소프트웨어, 또는 그들의 결합 등에 의해 구현 될 수 있다. 하드웨어에 의한 구현의 경우, 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 범용 프로세서(general processor), 컨트롤러, 마이크로 컨트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 본 개시의 범위는 다양한 실시예의 방법에 따른 동작이 장치 또는 컴퓨터 상에서 실행되도록 하는 소프트웨어 또는 머신-실행가능한 명령들(예를 들어, 운영체제, 애플리케이션, 펌웨어(firmware), 프로그램 등), 및 이러한 소프트웨어 또는 명령 등이 저장되어 장치 또는 컴퓨터 상에서 실행 가능한 비-일시적 컴퓨터-판독가능 매체 (non-transitory computer-readable medium)를 포함한다."}
{"patent_id": "10-2022-0127721", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 영상 스트리밍 시스템의 구성을 나타낸 것이다. 도 2는 도 1에 도시된 렌더링 서버에 대한 일 실시예의 구성을 나타낸 것이다. 도 3은 확장현실 디바이스의 현재 상황에 따라 다음 시점의 사용자 위치 정보와 포즈 정보의 변화를 설명하기 위한 일 예시도를 나타낸 것이다. 도 4는 VR/AR 콘텐츠에 대한 일 예시도를 나타낸 것이다.도 5은 다음 시점에 대한 렌더링된 텍스처 영상 데이터에 대한 일 예시도를 나타낸 것이다. 도 6은 본 개시의 다른 실시예에 따른 영상 스트리밍 방법의 동작 흐름도를 나타낸 것이다. 도 7은 본 개시의 다른 실시예에 따른 영상 스트리밍 장치가 적용되는 디바이스의 구성도를 나타낸 것이다."}
