{"patent_id": "10-2018-0064572", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0138238", "출원번호": "10-2018-0064572", "발명의 명칭": "딥 블라인드 전의 학습", "출원인": "삼성전자주식회사", "발명자": "강유"}}
{"patent_id": "10-2018-0064572", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능 신경망 모델의 딥 블라인드 전의 학습(Deep Blind Transfer Learning)방법에 있어서,라벨링되지 않은(unlabeled) 소스 데이터(source data)를 통해 인코더(encoder)를 학습하는 단계;라벨링된 소스 데이터를 통해 분류자(classifier) 및 상기 인코더(encoder)를 학습하는 단계;라벨링되지 않은 타겟 데이터(target data)를 통해 상기 인코더를 미세조정(fine-tune)하는 단계;상기 라벨링되지 않은 타겟 데이터를 통해 정렬자(aligener)를 학습하는 단계; 및라벨링된 타겟 데이터를 통해 상기 분류자, 정렬자, 인코더의 전체 스택을 학습하는 단계;를 포함하는 학습 방법."}
{"patent_id": "10-2018-0064572", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 소스 데이터(source data)를 통해 인코더(encoder)를 학습하는 단계는,자동인코더(autoencoder)의 재구성 손실(reconstruction loss)을 바탕으로 라벨링되지 않은 소스 데이터를 통해인코더를 학습하는 것을 특징으로 하는 학습 방법."}
{"patent_id": "10-2018-0064572", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 분류자는 상기 라벨링된 데이터를 바탕으로 일반적인 감독방식(common supervised fashion)으로 학습되는것을 특징으로 하는 학습 방법."}
{"patent_id": "10-2018-0064572", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 정렬자를 학습하는 단계는,타겟 특징 공간(target feature space)을 소스 특징 공간(sorce feature space)으로 맵핑(mapping)하여 학습하는 단계인 것을 특징으로 하는 학습 방법."}
{"patent_id": "10-2018-0064572", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 정렬자가 학습되는 동안, 상기 인코더 및 분류자는 학습되지 않는 것을 특징으로 하는 학습 방법."}
{"patent_id": "10-2018-0064572", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,각 학습 단계에서 상기 소스 데이터 및 타겟 데이터는 동시에 사용되지 않는 것을 특징으로 하는 학습 방법."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "우리는 어떻게 소스 도메인에서 훈련된 준감독 (semi-supervised) 모델을 소스 데이터 세트 내의 어떤 인스턴스 도 전이시키지 않은 채로 타겟 도메인에 전의할 수 있을까? 전통적인 전이 학습(transfer learning)에서는 소스 와 타겟 데이터 활용에 대한 제한이 없었다. 그러나 소스 데이터 세트의 크기가 메모리 안에 들어가기에는 너무 크거나 개인적인 데이터 세트인 경우, 우리는 타겟 데이터를 훈련시키면서 소스 데이터를 활용할 수 없다. 이 논 문에서, 우리는 해당 모델에 완전히 블라인드 제제(completely blind constraint)를 더했으며, 이것은 소스나 타겟 데이터가 타겟 혹은 소스 모델 각각을 훈련하면서 사용될 수 없다는 것을 의미한다. 우리는 소스 도메인과 정렬 신경망에서 학습된 모델이 타겟 도메인에서 더 나은 모델을 학습할 수 있도록 활용하는 간단하고 참신한 준 감독 전이 학습 방법을 제시한다. 정렬 신경망(aligning neural network)은 타겟 특징 공간을 소스 특징 공간으 로 매핑하여 소스 도메인의 분류된 데이터로부터 학습된 감독 모델이 타겟 도메인에서 용이하게 사용될 수 있도 록 한다. 우리는 부분 감독 전이 학습에 있어 최첨단의 성과를 달성하였다. 공개특허10-2019-0138238 발명자 김재덕 경기도 안산시 상록구 감골로 59, 710동 802호(사 동, 상록수타운월드아파트) 김정환 서울특별시 관악구 관악로 1, 301동 519호(신림동, 서울대학교)허혜문 서울특별시 관악구 관악로 1, 301동 519호(신림동, 서울대학교)명 세 서 청구범위 청구항 1 인공지능 신경망 모델의 딥 블라인드 전의 학습(Deep Blind Transfer Learning)방법에 있어서, 라벨링되지 않은(unlabeled) 소스 데이터(source data)를 통해 인코더(encoder)를 학습하는 단계; 라벨링된 소스 데이터를 통해 분류자(classifier) 및 상기 인코더(encoder)를 학습하는 단계; 라벨링되지 않은 타겟 데이터(target data)를 통해 상기 인코더를 미세조정(fine-tune)하는 단계; 상기 라벨링되지 않은 타겟 데이터를 통해 정렬자(aligener)를 학습하는 단계; 및 라벨링된 타겟 데이터를 통해 상기 분류자, 정렬자, 인코더의 전체 스택을 학습하는 단계; 를 포함하는 학습 방법. 청구항 2 제1항에 있어서, 상기 소스 데이터(source data)를 통해 인코더(encoder)를 학습하는 단계는, 자동인코더(autoencoder)의 재구성 손실(reconstruction loss)을 바탕으로 라벨링되지 않은 소스 데이터를 통해 인코더를 학습하는 것을 특징으로 하는 학습 방법. 청구항 3 제1항에 있어서, 상기 분류자는 상기 라벨링된 데이터를 바탕으로 일반적인 감독방식(common supervised fashion)으로 학습되는 것을 특징으로 하는 학습 방법. 청구항 4 제1항에 있어서, 상기 정렬자를 학습하는 단계는, 타겟 특징 공간(target feature space)을 소스 특징 공간(sorce feature space)으로 맵핑(mapping)하여 학습하 는 단계인 것을 특징으로 하는 학습 방법. 청구항 5 제1항에 있어서, 상기 정렬자가 학습되는 동안, 상기 인코더 및 분류자는 학습되지 않는 것을 특징으로 하는 학습 방법. 청구항 6 제1항에 있어서, 각 학습 단계에서 상기 소스 데이터 및 타겟 데이터는 동시에 사용되지 않는 것을 특징으로 하는 학습 방법. 발명의 설명"}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간 지능을 실현하는 인공 지능 시스템은 현재 다양한 기술분야에서 이용되고 있다. 일반적으로 인공 지능 시 스템은 기존의 규칙 기반 스마트 시스템과 달리 기계가 자체적으로 학습하고 결정하며 더 똑똑해 지는 특징을 가진다. 사용자가 인공 지능 시스템을 사용함에 따라 인공 지능 시스템의 인식률이 향상되고, 인공지능 시스템 은 사용자의 기호 또는 관심사를 보다 잘 이해 할 수 있다. 따라서 기존의 규칙 기반 스마트 시스템이 심층 학 습 기반 AI 시스템으로 대체되고 있다. 인공 지능 기술에는 기계 학습(예: 딥 러닝(deep learning)) 및 기계 학습을 사용하는 요소 기술이 포함된다. 기계 학습이란 기계가 입력 데이터의 특성을 분류 및 학습하는 알고리즘 기술을 의미한다. 요소 기술은 딥 러닝 과 같은 기계 학습 알고리즘을 사용하여 인간 두뇌의 인지 또는 결정 기능을 시뮬레이션 하는 기술을 말하며, 언어 이해, 시각적 이해, 추론, 예측, 지식 표현의 분야로 나눌 수 있다. 인공 지능 기술(AI)은 다양한 분야에 적용될 수 있다. 언어 이해는 인간의 구두(verbal)/서면(written) 언어를 인식, 적용 및 처리하는 기술을 말하며 자연 언어 처리, 기계 번역, 대화 시스템, 질의 응답 및 음성 인식, 음 성 합성을 포함한다. 시각적 이해는 인간의 관점에서 물체를 인식하고 처리하는 기술을 말하며, 물체 인식, 물 체 추적, 이미지 검색, 인간 인식, 장면 이해, 공간 이해 및 이미지 개선 기술을 포함한다. 추론/예측은 정보를 결정하고 논리적 추론 및 예측을 실행하는 기술을 말하며 지식/확률 기반 추론, 최적화 예측, 선호도 기반 계획 및 추천을 포함한다. 지식 표현은 인간 경험 정보를 자동화된 지식 데이터로 처리하는 기술을 말하며 지식 구성 (생성/분류 데이터)과 지식 관리(데이터 활용)을 포함한다. 동작 제어는 차량의 자동 구동 및 로봇의 동작을 제어하는 기술을 말하며, 동작 제어(네비게이션, 충돌, 주행) 및 조작 제어(동작 제어)를 포함한다. 참조문헌"}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "[1] S. J. pan and Q. Yang, \"A survey on transfer learning,\" IEEE Transactions on knowledge and data engineering, vol,22, no. 10, pp. 1345-1359, 2010"}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "[2] K. Weiss, T. M Khoshgoftaar, and D. Wang, \"A survey of transfer learning,\" Journal of Big Data, vol. 3, no. 1, p.9, 2016."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 4, "content": "[3] J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence, Dataset shift in machine learning. The MIT Press, 2009."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 5, "content": "[4] Y. Aytar and A. Zisserman, \"Tabula rasa: Model transfer for object category detection,\" in Computer Vision (ICCV), 2011 IEEE International Congerence on. IEEE 2011, pp. 2252-2259."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 6, "content": "[5] R.Girshick, J. Donahue, T. Darrell, and J. Malik, \"Rich feature hierarchies for accurate object detection and semantic segmentation,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 580-587."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 7, "content": "[6] J. Long. E Shelhamer, and T. Darrell, \"Fully convolutional networks for semantic segmentation,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3431-3440."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 8, "content": "[7] S. Ren, K. He, R. Girshick, and J. Sun, \"Faster r-cnn: Towards real-time object detection with region proposal networks,\" in Advances in neural information processing systems, 2015, pp. 91-99."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 9, "content": "[8] R. Collobert, J. Weston, L. Bottou, M. Karen, K. Kavukcuoglu, and P. Kuksa, \"Natural language processing (almost) from scratch,\" Journal of machine Learning Reserch, Vol. 12, no. Aug, pp. 2493- 2537, 2011."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 10, "content": "[9] X. Glorot, A. Bordes, and Y. Bengio, \"Domain adaptation for large-scale sentiment classification: A deep learning approach,\" in Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 513-520."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 11, "content": "[10] M. Chen, Z. Xu, K. Q. Weinberger, and F. Sha, \"Marginalized denoising autoencoders for domain adaptation,\" in Proceedings of the 29th International Coference on International Conference on Machine Learning. Omnipress, 2012, pp. 1627-1634."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 12, "content": "[11] M Xiao and Y. Guo, \"Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model,\" in Proceedings of the 30th International Conference on Machine Learning Volume 28. JMLR. org, 2013, pp. 1-293."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 13, "content": "[12] E. Tzeng, J. Hoffman, N. Zhang, K, Saenko, and T. Darrell, \"Deep domain confusion: Maximizing for domain invariance,\" arXiv preprint arXiv: 1412.3474, 2014."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 14, "content": "[13] E. Tzeng, J. Hoffman, T. Darrel, and K, Saenko, \"Simultaneous deep transfer across domains and tasks,\" in Computer Vision (ICCV), 2015 IEEE International Conference on. IEEE 2015, pp. 4068-4076."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 15, "content": "[14] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrel, \"Adversarial discriminative domain adaptation,\" in Computer Vision and Pattern Recognition (CVPR), vol. 1, no. 2, 2017, p.4"}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 16, "content": "[15] M. Long, Y. Cao, J. Wang, and M. Jordan, \"Learning transferable features with deep adaptation networks.\" International Conference on Machine Learning, 2015, pp. 97-105."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 17, "content": "[16] M. Long, Y. Cao, J. Wang, and M. Jordan, \"Unsupervised domain adaptation with residual transfer networks,\" in Advances in neural information processing systems, 2016, pp. 136-144."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 18, "content": "[17]-,\"Deep transfer learning with joint adaptation networks,\" International Conference on Machine Learning, 2017, pp. 2208-2217."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 19, "content": "[18] Y. Ganin and V. Lempitsky, \"Unsupervised domain adaptation by backpropagation,\" International Conference on Machine Learning, 2015, pp. 1180-1189."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 20, "content": "[19] Y. Ganin, E. Ustinova, H. Marchand, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky, \"Domain-adversarial training of neural networks,\" The Joural of Machine Learning Reserach, vol. 17, no. 1, pp. 2096-2030, 2016."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 21, "content": "[20] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan, \"Domain separation networks,\" in Advances in neural information processing systems, 2016, pp. 343-351."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 22, "content": "[21] X. Zhang, F. X. Yu. S.-F. Chang and S, Wang, \"Deep transfer networks: Unsupervised domain adaptation,\" arXiv preprint arXiv: 1503.00591, 2015."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 23, "content": "[22] F. Ma, D. Meng, Q. Xie, Z. Li, and X, Dong, \"Self-paced co-training,\" in International Conference on Machine Learning, 2017, pp. 2275-2284."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 24, "content": "[23] K. Saito, Y. Ushiku, and T. Harada, \"Asymmetric tri-training for unsupervised domain adaptation,\" arXiv preprint arXiv: 1702.08400, 2017."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 25, "content": "[24] A. Kumar, P. Sattigeri, and T. Fletcher, \"Semi-supervised learning with in Advances in neural information processing systems,"}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 26, "content": "[25] Z. Dai, Z. Yang, F. Yang, W. W. Cohen, and R. R. Salakhutdinov, \"Good semi-supervised learning that requires a bad gan,\" in Advances in neural information processing systems, 2017, pp. 6513-6523."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 27, "content": "[26] T. Miyato, S-i. Maeda, M. Koyama, and S. Ishii, \"Virtual adversarial training: a regularization method for supervised and semi-supervised learning: arXiv preprint arXiv: 1704.03976, 2017."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 28, "content": "[27] C. N. d. Santos, K. Wadhawan, and B. Zhou, \"Learning loss functions for semi-supervised learning via discriminative adversarial network,\" arXiv preprint arXiv: 1707.02198, 2017."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 29, "content": "[28] L. Duan, I. W. Tsang, D. Xu, and T.-S. Chua, \"Domain adaptation from multiple sources via auxiliary classifiers,\" in Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 289-296."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 30, "content": "[29] H. Daume III, A. Kurnar, and A. Saha, \"Frustratingly easy semi-supervised domain adaptation,\" in Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing. Association for Computational Linguistics, 2010, pp. 53-59"}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 31, "content": "[30] J. Donahue, J. Hoffman, E. Rodner, K. Saenko, and T. Darrell, \"semi-supervised domain adaptation with instance constraints,\" in Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conferenceon. IEEE, 2013, pp. 668-675"}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 32, "content": "[31] T. Yao, Y. Pan, C.-W. Ngo H. Li, and T. Mei, \"Semi-supervised domain adaptation with subspace learning for visual recognition,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 2142-2150."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 33, "content": "[32] N. Papernot, M. Abadi U. Erlingsson, I. Goodfellow, and K. Talwar, \"Semi-supervised knowledge transfer for deep learning form private training data.\" arXiv preprint arXiv: 1610.05755, 2016."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 34, "content": "[33] S. Ao, X. Li, and C. X. Ling, \"Fast generalized distillation for semi-supervised domain adaptation,\" in AAAL, 2017, pp. 1719-1725."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 35, "content": "[34] M. Chen, K. Q. Weinberger, and J. Blitzer, \"Co-training for domain adaptation,\" in Advances in neural information processing systems, 2011, pp. 2456-2464."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 36, "content": "[35] M. Rohrbach, S. Ebert, and B. Schiele, \"Transfer learning in a transductive setting,\" in Advances in neural information processing systems, 2013, pp. 46-54."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 37, "content": "[36] D.-H. Lee, \"Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks,\" in Workshop on Challenges in Representation Learning, ICML, vol. 3, 2013, p.2."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 38, "content": "[37] S. Khamis and C. H. Lampert, \"Coconut: Co-classification with output space regularization.\" in BMVC, 2014."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 39, "content": "[38] O. Sener, H. O. Song, A. Saxena, and S. Savarese, \"Learning transferrable representations for unsupervised domain adaptation,\" in Advances in Neural Information Processing Systems, 2016, pp. 2110- 2118."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 40, "content": "[39] N. N. Pise and P. Kulkarni, \"A survey of semi-supervised learning methods,\" in Computational Intelligence and Security, 2008. CIS'08. International Conference on, vol. 2. IEEE, 2008, pp. 30-34."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 41, "content": "[40] V. J. Prakash and D. L. Nithya, \"A survey on semi-supervised learning techniques,\" arXiv preprint arXiv: 1402.4645, 2014. 본 개시는 소스 및 타겟 특징 공간들을 정렬함으로써 특징 분포 시프트 문제를 해결하는 딥 블라인드 전의 학습 (Deep Blind Transfer Learning: DBTL)에 관한 것이다."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "I. 도입 데이터 가시성(visibility)의 제약하에서 어떻게 학습된 심층 학습 모델을 소스에서 타겟으로 전의할 수 있는가? 심층 순환 신경망(deep neural network, DNN)의 최근 향상된 성능을 나타내기 위해서는 대량의 라벨링 된 데이터를 필요로 한다. 다만, 대부분의 경우 라벨링되지 않은 데이터가 라벨링 된 데이터보다 많으며, 라벨 링되지 않은 데이터를 라벨링하는데 비용이 많이 들게 된다. 따라서, 본 개시에서는 적은양의 라벨링된 데이터 를 사용하면서 약간 다른 분포를 가지지만 양은 충분한 관련 데이터를 추가적으로 사용할 수 있다. 라벨링되지 않은 데이터의 획득은 비교적 비용이 적게 들기 때문에 본 개시에서는 심층 순환 신경망(DNN)을 학습시키기 위 해 라벨링되지 않은 데이터를 활용할 수 있다.다른 소스 도메인에 대한 풍부한 라벨링된 데이터의 정보를 활용하는 전의 학습 방법[1, 2]은 기계 학습(머신 러닝) 분야에서 많은 관심을 받았다. 다른 학습 방법과는 달리 전의 학습은 소스 도메인으로부터 다르지만 관련 된 타겟 도메인으로 지식 학습을 적용한다. 소스 도메인과 타겟 도메인이 다른 분포를 가지거나 다른 역할을 수 행할 수 있기 때문에 타겟 도메인에서 예측에 도움이 되는 적절한 정보를 소스 도메인으로 전의하는 것은 어렵 다 [3]. 실용적인 순방향 딥 전의 학습 설정(straight-forward deep transfer) [4, 5, 6, 7, 8, 9, 10, 11]에서는 소스 기반 피드-포워드(feedforward) 네트워크의 일부 저 수준(low-level) 매개변수가 복사되고 대상 데이터에 맞게 미세 조정 될 수 있다. 소스 도메인의 학습 과정은 타겟 도메인의 학습 과정과 엄격히 구분된다. 이와 대조적으 로 최근의 딥 전의(deep transfer) 학습 방법 [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]은 소스와 타겟 도메 인 간의 피쳐 분포 불일치를 최소화하기 위해 소스와 타겟 데이터를 함께 학습 시킨다. 그러나, 일부 전의 학습 응용 프로그램에서는 비 효과 적인 소스 데이터의 대용량 첨부 또는 소스 데이터의 프 라이버스에 민감한 특성 때문에 소스 데이터 집합을 타겟 데이터 프로세스에 활용할 수 없다. 본 개시에서는 전의 학습에 있어서 블라인드 전의 학습 프로그램을 제안한다. 블라인드 전의 학습 프로그램은 소스 데이터 및 타겟 데이터 세트에 대한 학습 과정이 엄격히 분리된다. 블라인드 제약하에서 소스 도메인으로부터의 지식은 소 스 도메인으로 단 한번만 전의될 수 있다. 타겟 데이터는 소스 데이터의 학습 중에 활용될 수 없으며, 그 반대 의 경우도 마찬가지이다. 블라인드 제약 조건은 명확하지 않으며 이전 연구에서 연구되지 않았다. 블라인드 제 약하 학습과정에서 경험적으로 분포의 불일치를 계산하거나 최적화하는 것은 불가능하며, 데이터 분포의 변화를 줄이는 것이 더욱 어려워 진다. 본 개시에서는 소스 도메인 및 타겟 도메인 모두에 라벨링이 된 인스턴스와 라벨링이 없는 인스턴스가 포함되어 있는지 여부에 따라, 전체 및 부분적인 준 감독 전의 학습(semi-supervised transfer learning)으로 구분된다. 전체 준 감독 전의 학습의 경우 학습 과정은 소스/타겟 데이터 세트에서 감독/감독되지 않은 모델을 순차적으로 미세조정 함으로써 휠씬 복잡해 질 수 있다. 서로 다른 데이터 또는 오브젝트를 통한 복수의 미세 조정은 이전 학습을 지우고 정보의 손실을 가져올 수 있다. 게다가, 소스 및 타겟 특징 공간은 구별될 수 있으며, 소스 분류 자(classifier)는 타겟 특징 공간과 양립 할 수 없다. 순차적인 미세조정 접근법은 타겟 도메인의 최종 모델에 서 좋은 성능을 보장할 수 없다. 본 개시에서는 소스 및 타겟 특징 공간을 정렬하여 특징 분포 시프트 문제를 해결하는 DBTL(Deep Blind Transfer Learning)을 제안한다. DBTL은 간단한 미세조정 접근법을 순환신경망의 간단한 정렬과 함께 확장하여 타겟 특징을 소스 특징 공간으로 매핑한다. 구체적으로 소스 및 타겟 특징의 매핑은 각각의 도메인에서 감독되 지 않은 방식으로 개별적으로 학습될 수 있다. 그 후, 타겟 대상 특징을 소스 특징으로 맵핑하여 타겟이 라벨 링되지 않은 소스 특징을 타겟 특징에 매핑함으로 다른 순환신경망을 학습한다. 특징 공간 정렬로 인해 분류자 (classifier)는 타겟 데이터가 적용된 소스 도메인에서 학습된다(learned in). 타겟 라벨링된 데이터 세트에서 전체 구조를 미세조정하면 최종 모델이 생성된다. 이 과정에서 복수의 데이터 또는 목적(objective) 함수에 대 한 단일 구조의 순차 미세조정은 이전의 학습을 제거하기 않고 블라인드 제약을 충족시킬 수 있다. 소스/ 타겟 라벨링/타겟 라벨링되지 않은 데이터의 모든 정보가 보존되어 최종 모델이 향상 될 수 있다. 이러한 본 개시의 방법에 관한 실험 결과는 우수한 성능을 보여준다. II. 서설 자동 인코딩은 인코더(encoder)와 디코더(decoder) 두 부분[?]으로 구성된다. 인코더는 저 차원 공간에서 새로 운 표현(representation)을 얻기 위해 입력 데이터를 은닉 레이어에 매핑(mapping)하는 것을 목표로 한다. 디코 더는 원래의 입력과 재구성된 출력간의 오차를 최소화 함으로 은닉 표현을 사용하여 원래의 입력을 재구성하려 고 시도한다. 본 개시에서는 를 m개의 샘플, 차원에 대한 입력 데이터 셋으로 정의한다. 인코더와 디코더는 다음 과 같이 형성된다."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "은 은닉 표현이고, 는 원래의 특징 에 대한 재구성된 출력이다. 모델 매개변수 는 가중치 행렬 , 및 편향 , 을 포함한다. 자동 인코더의 목표는 주어진 데이터 집합 X와 목적 함수에 대한 재구성 오차를 최소화 하여 모델 매개변수 를 학습하는 것이다."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 R(W, W')는 정규자(regularization)로 사용된다. III. 제안하는 방법 이 절에서는 본 개시의 블라인드 완전 준 감독 전이 학습방법인, 딥 블라인드 전이 학습(Deep Blind Transfer Learning)방법을 자세히 설명한다. A. Overview 완전 준 감독(full semi-supervised) 전의 학습방법에 있어서, (라벨링된/라벨링되지 않은, 소스/ 타겟) 네 가 지의 데이터가 있다. 소스 도메인에 대한 라벨링되지 않은 샘플을 로 정의하고, 타겟 도메인에 대한 라벨링되지 않은 샘플을 로 정의한다. 소스 도메인에 대한 라벨링된 샘 플을 로 정의하고, 타겟 도메인에 대한 라벨링된 샘플을 로 정의한다. 블라인드 완전 준 감독 전이 학습 (full semi-supervised transfer learning) 과정은 다음과 같이 모델 학습 과정을 제약(constrain)한다. 위 모델은 과 에 대한 접근 없이 과 를 바탕으로 처음 학습된다 그 후, 학습된 모델은 전의되고, 과 에 대한 접근 없이 과 를 통해 학습된다. 블라인드 제약하에서는 두 가 지 과제(challenge)가 있다. 첫 번째로 소스 및 타겟 데이터가 학습과정에서 함께 사용되지 않으므로, 샘플링 접근으로부터 특징 분포 차이를 계산하기 힘들다. 두 번째로 소스 데이터에 대해 학습된 특징 표현과 분류자를 모두 활용하는 것은 학습된 특징 표현이 소스 및 타겟 도메인간에서 다르기 때문에 어렵다. 본 개시는 이러한 어려움을 해결하기 위해 딥 블라인드 전의 학습 모델을 제안한다. 소스와 타겟 특징 공간 간의 불일치를 최소화 하는 대신 위 모델에서는 타겟 특징 공간을 소스 특징 공간과 정렬하는 방법을 학습한다. 타겟 도메인에 대해서 만 맵핑을 학습 함으로써, 블라인드 제약이 만족된다. 또한, 타겟 자동인코더의 맨 위에 타겟에서 소스 특징으 로의 맵핑은 소스 특징 공간에서 학습된 분류자가 타겟 데이터를 계속 예측할 수 있도록 한다. B. DBTL 본 개시에 따른 모델은 도1의 가장 왼쪽에 보이는 것과 같이 3개의 신경망 구조로 구성되어 있다. 본 개시에 따 른 모델을 이용하여 특징 공간의 구조를 학습하기 위해 라벨링 없는 데이터를 통해 첫번째 신경망을 자동인코더 기로 학습시킨다. 두 번째 신경망은 타겟 자동인코더의 출력을 소스 자동인코더의 출력에 매핑함으로 학습된다. 세 번째 신경망은 라벨링된 데이터와 함께 일반적인 감독 방식으로 학습된다. 첫 번째, 두 번째, 세 번째 신경 망을 각각 인코더(encoder), 정렬자(aligner) 및 분류자(classifier)로 정의한다. 블라인드 제약 조건을 만족시 키기 위해 다음 5단계에서 라벨링되지 않은 소스/타겟 데이터 및 라벨링된 소스/타겟 데이터를 순차적으로 학습 시킨다. 1단계: 인코더는 자동인코더의 재구성 손실(autoencoder reconstruction loss)을 사용하여 라벨링되지 않은 소 스 데이터에 대해 학습시킨다. 2단계: 미세 조정된 라벨링된 타겟 데이터를 이용해 사전 인코더 위의 분류자(classifier on top of the previous encoder)를 학습시킨다. 3단계: 인코더는 표준 자동인코더의 재구성 손실로 라벨링되지 않은 타겟 데이터에서 다시 미세조정 된다. 이 단계에서 미세조정된 파라미터를 가진 인코더를 타겟 인코더로 부른다. 4단계: 정렬자는 타겟 인코딩을 라벨링되지 않은 타겟 데이터의 소스 인코딩에 매핑하도록 학습한다. 이 단계 에서 정렬자(녹색 부분)만 학습시킨다. 5단계: 인코더, 정렬자 및 분류자의 스택(stack)은 타겟 라벨링된 데이터에서 미세조정 된다. 소스 및 타겟 인코더는 각각 소스 및 타겟 인코더에 의해 인코딩된 특징 공간으로 정의된다. 소스 특징 공간과 함께 타겟 특징 공간을 정렬하기 위해 정렬자를 학습하는 것을 목표로 한다. C. 모델 특성 블라인드 구속, 모든 학습 단계에서 소스 데이터 집합이나 타겟 데이터 집합만 사용한다. 학습단계에서 1단계와 2단계에서 모델은 소스 데이터 집합에 대해 학습되며, 타겟 데이터 집합은 필요하지 않는다. 대조적으로 3단계 와 4단계 및 5단계는 소스 데이터집합에 엑세스 하지 않고 타겟 데이터 집합을 활용한다. 2단계와 3단계 사이에 전달되는 유일한 정보는 소스 자동 인코딩 및 소스 분류자이다. 학습과정에서의 이러한 명확한 분리는 블라인드 제약을 만족시킨다. 성능의 예측. 3단계의 타겟 자동 인코딩은 2단계에서 미세 조정된 소스 자동 인코딩을 통해 매핑된 다른 특징 (feature)을 제공한다. 이러한 차이점 때문에 2단계에서 학습된 소스 분류자는 타겟 자동 인코딩으로 부터의 특 징에 대해 일관된 성능을 제공하지 않는다. 3단계에서는 타겟 데이터 집합의 구조를 학습하는 동안 소스 분류자 의 예측 성능이 저하된다. 따라서, 4단계에서 정렬자를 도입한다. 정렬자를 학습하여 소스 자동 인코더의 인코 딩을 타겟 자동인코더의 인코딩에 매핑한다. 타겟 자동인코더 및 정렬 신경 네트워크의 스택은 타겟 도메인의 데이터를 소스 특징 공간으로 매핑한다. 그 후 소스 분류자는 정렬된 특징 공간에서 좋은 예측 성능을 제공한다. IV. 실험 결과 이 섹션에서는 광범위한 실험을 통해 DBTL의 성능을 검증한다. 본 개시는 다음 질문에 답하고자 한다. Q1. (성능의 예측) DBTL은 블라인드 준 감독 전의(semi-supervised transfer) 학습을 어떻게 수행하는가? Q2. (준 감독 학습에서 라벨링되지 않은 데이터 이용) 라벨링된 데이터가 희박할 때 라벨링이 되지 않은 데이터 를 효과적으로 이용하는가? Q3. (정렬자 학습의 효과) 정렬자의 이점은 더 많은 계층을 배치하는 것보다 특징을 정렬하는 것에서 비롯되는 가? A. 실험설정 데이터세트. 본 개시에서는 HEPMASS, HIGGS, SUSY, Sensorless Gas와 같은 다차원 구조의 5가지 실제 데이터를 통해 실험을 수행한다. 세 개의 데이터 세트(HEPMASS, HIGGS and SUSY)는 입자 충돌 실험의 센서 관측이다. 여 기서 라벨링은 실재 관심세대와 이진(Binary)이다. 다른 데이터 세트(Sensorless Gas)는 라벨링은 모터 상태상 태의 진단 및 화학반응에서의 가스 유형에 관한 센서 관측이다. 데이터의 통계는 표 1에 나와있다. 모든 데이터 세트는 UCL 머신 러닝(기계 학습) 저장소에서 사용할 수 있다. 표 1 데이터 세트의 통계에 따르면, 모든 데이터 세트는 UCL 머신러닝 자료실에서 사용할 수 있다. URL:HTTP:ARCHIVE.ICS.UCI.EDU/ML"}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "데이터 처리. 완전 준 감독 전이(Full semi-supervised transfer) 실험을 수행하기 위해, 소스 / 타겟, 라벨링 된 / 라벨링되지 않은 인스턴스가 있는 데이터 세트가 필요하다. 위의 데이터를 소스 / 타겟으로 분할하고 다음 과 같이 라벨링을 지정하거나 라벨링을 지정하지 않는다. 1단계: 먼저 특징범위를 정규화 한다. 특징이 음의 값을 포함하면, -1 ~ 1의 범위로 정규화 될 수 있다. 음의 값을 포함하지 않는 경우 0 ~ 1의 범위로 정규화 될 수 있다. 2단계: K를 10으로 설정한 K-means 클러스터링 알고리즘을 이용하여 전체 데이터의 10%를 타겟 데이터로 선택한 다. 타겟 데이터는 충분한 크기의 클러스터에서 샘플링 된다. 3단계: 전체 데이터의 1%를 소스 데이터로 샘플링하고 해당 파티션으로부터 타겟 테스트 데이터를 샘플링한다. 본 개시에서는 set을 학습 데이터로 설정한다. 4단계: 본 개시에서는 무작위로 소스 및 타겟 학습데이터의 90%를 선택하고, 학습데이터로 라벨링되지 않은 데 이터를 얻기위해 라벨링을 지운다. K-means 클러스터링에서 각 클러스터의 인스턴스는 다른 클러스터의 인스턴스에 비해 서로 더 가깝다. 타겟 데 이터 인스턴스는 동일한 클러스터에서 샘플링 되므로 소스 및 타겟 데이터는 의도된 다른 특징 분포를 가진다. 최종 데이터 비율은 라벨링된 소스: 라벨링되지 않은 소스: 테스트 소스: 라벨링된 타겟: 라벨링되지 않은 타겟 = 80.1:8.9:1:8.1:0.9:1. 이다. B. 성능의 예측 본 개시의 일 실시 예에 따르면 DBTL의 예측성능을 블라인드 전체 준 감독 전의(blind full semi-supervised transfer) 학습에 대한 성능으로 설정한다. 본 개시에 따른 방법은 네 가지 유형의 학습 데이터를 모두 사용한 다. 데이터의 일부만 사용하는 4가지의 다른 기준 방법과 본 개시에 따른 방법을 비교한다. 1) T-L: 하나의 신경망이 라벨링된 타겟 데이터세트로만 학습된다. 2) ST-L: 단일 신경망은 라벨링된 소스 데이터 세트로 사전 학습을 받았고, 라벨링된 타겟 데이터 세트에 미세 조정 되었다. 이는 라벨링된 소스 데이터 세트를 활용하는 간단한 전의 학습 접근법이다. 3) T-LU: 인코더와 신경망 분류자의 스택(STACK)으로 인코더가 자동 인코더로 라벨링되지 않은 타겟 데이터 세 트에 대해 학습되고 전체 네트워크가 라벨링된 타겟 데이터 세트에 대해 추가로 학습된다. 이는 라벨링없는 타 겟 데이터 세트를 활용하는 간단한 준-감독(semi-supervised) 학습 접근법이다. 4) ST-LU: 인코더와 신경망 분류자의 스택(stack)은 정렬자가 없는 것만 제외하고는 DBTL방법과 같이 학습된다. 이는 준-감독 학습과 전의를 결합하는 자연스러운 접근법이다. 동일한 지면(ground)에서 비교하기 위해 인코더, 정렬자 및 분류자의 구조를 각각 80_40_20_10, 100_50_100 및 00_300_300_300_300으로 고정한다. 각 DBTL단계의 최적화 과정에서 학습률 0.001, 배치 크기 100의 adam 최적 화(optimizer)를 사용한다. 학습은 테스트 데이터 집합의 성능이 수렴될 때까지 계속된다. 레이어의 크기 증가 의 잠재적인 이점은 IV_D파트에서 분석된다."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "타겟 테스트 세트에 대한 정확도 예측은 표2에 요약되어 있다. 최종 성능은 멀티-클레스 데이터 세트(Gas, Sensorless) 및 이진 라벨링된 데이터 세트(HEPMASS, HIGGS, SUSY)의 정확도로 보고된다. 모든 데이터 세트에 대해 DBTL은 ST-LU의 자연스러운 접근 방식에서 가장 좋은 예측성능을 나타내었다. 자연스러운(naive) 감독학습 과 비교할 때 DBTL은 멀티-클레스 데이터세트에서 6%, 이진 라벨링된 데이터 세트에서 3~12%의 성능향상이 나타 났다. 게다가, 전의 및 준-감독 학습학습의 적용은 자연스러운 학습에 비해 예측성능을 향상시켰다. 전의 학습 모델은 1~8%의 성능 증가를 보인 반면, 준-감독 학습 모델은 1~3%의 성능 증가를 보였다. 표 2 DBTL의 예측 성과. 블라인드 준-감독 전의 학습문제에서 네 가지 기본 방법과 비교되는 DBTL의 성능을 나타내었 다. 본 개시에 따른 방법 DBTL은 모든 5가지 데이터베이스에 대해 최상의 예측 성능을 달성하였다. C. 준-감독 학습의 라벨링되지 않은 데이터 활용 본 개시에서는 DBTL에서 준-감독 학습의 효과를 실험하였다. D. 정렬 학습의 효과 본 개시에서는 DBTL의 정렬자 학습의 효과를 실험하였다. V. 관련 연구 딥 신경망 네트워크(DNN)을 학습하려면 엄청난 양의 데이터 세트가 필요하다. 그러나 작업이 정해진 라벨링된 데이터 세트는 일반적으로 부족하다. 딥 신경망 네트워크를 학습하려면 엄청난 양의 데이터 집합이 필요하지만 다른 도메인으로부터 라벨링된 데이터 세트 또는 라벨링되지 않은 데이터 세트의 집합도 필요하다. 전의 학습[1, 2]은 학습된 데이터로부터 다른 분포를 갖는 다른 도메인으로 머신러닝 모델을 전의하는 것을 목 표로 한다. 소스 도메인에서 습득한 일반화 가능한 지식을 활용하여 대상 도메인에서 더 나은 모델을 학습한다. 컴퓨터 비전[4, 5, 6, 7]과 자연 언어 처리[8, 9, 10, 11]영역에서 최첨단 성능을 얻기 위해 전의 학습을 적용 하는 연구가 많이 있었다. 딥 전의(deep transfer) 학습에 대한 기존의 방법은 소스와 타겟 도메인 간의 특징 분포 불일치를 자주 (frequently) 최소화 한다 [12, 13, 14, 15, 16, 17, 18, 19, 20]. 이러한 방법은 학습과정에서 소스 및 타겟 데이터를 함께 사용하여 특징 분포 불일치를 계산하고 최적화 한다. 전의 학습에 대한 다른 연구는 라벨링되지 않은 데이터와 타겟 데이터에 의해 유사 라벨링(pseudo-lables)을 각각 학습하고 할당한다 [34, 35, 36, 37, 38, 22, 23]. 이러한 방법은 소스 및 타겟 데이터에서 모델을 반복적으로 미세 조정하여 더 나은 유사-라벨링을 제공한다. 준-감독 학습[39, 40]은 라벨링 예측에 대한 더 나은 성능을 위해 라벨링된 데이터와 라벨링되지 않은 데이터 를 모두 활용하려고 한다. 라벨링되지 않은 데이터는 특징 공간 구조에서 구조를 발견하거나 작은 라벨링된 데 이터에서 초과 피팅(over-fitting)으로 모델을 정규화 하는데 사용된다. 가장 최근에 제안된 연구 [24, 25, 26, 27]는 adversarial 학습을 이용하여 특징 공간에서의 구조를 학습하였다. 일부 연구는 준-감독 영역 적응의 문 제를 연구하였다 [28, 29, 30, 31]. 또한 이러한 연구는 소스 및 타겟 데이터를 모두 사용하여 데이터 분포의 불일치를 측정하였다. 본 개시에서는 학습과정에서 소스 및 타겟 데이터 세트의 가시성(visibility)을 명시적으로 제한하였다. 우리가 알고 있는 바에 따르면, 준-감독 학습의 학습과정에서 타겟 데이터로부터 소스 테이터의 격리(isolation)는 최 근 [32, 33]에서만 고려되었다. 이 연구에서는 소스 도메인에서 학습한 훈련(teacher) 분류자가 학습(student) 분류자가 학습할 수 있도록 타겟 도메인의 유사 라벨링을 제공한다. 그러나 앙상블(ensembling) [32]이나 증류 (distillation) [33]가 있더라도 이러한 모델은 잘못된 라벨이 붙은 데이터로 학습될 수 있다. 본 개시의 작업 은 최종 모델의 학습에서 모든 소스 / 타겟, 라벨링된 / 라벨링되지 않은 데이터를 활용하고 블라인드 제약 조 건을 충족시키기 위한 원칙적은 접근 방법을 제공한다. 게다가, 본 개시의 작업은 준-감독 전의 학습의 문제를"}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "가장 먼저 다루는 것이다. 문제 설정 간의 차이점은 표3에 요약되어 있다."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "표 3 문제 설정간의 차이점 요약. 본 개시에 따른 방법은 모든 다른 유형의 데이터 베이스를 활용하고, 블라인드 제 약을 충족한다."}
{"patent_id": "10-2018-0064572", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "VI. 결론 본 개시는 타겟 학습동안 소스 데이터에 대한 가시성 제약 조건을 가진 완전 준-감독 설정하에서 새로운 딥 전 의 학습 방법(deep transfer learning method)을 제안한다. 본 개시는 신중하게 모든 데이터의 활용도를 최대화 하고, 블라인드 제약 조건을 충족시키기 위해 학습 과정을 설정한다. 블라인드 제약으로 인해 유사-라벨링에 대 한 불일치를 최소화 하거나 반복적으로 학습하는 이전의 접근법은 적용할 수 없다. 소스 도메인에서 학습한 분 류자를 활용하기 위해 자동 인코딩을 통해 타겟 특징을 소스 특징 영역에 매핑한다. 이 방법으로 타겟 데이터에 대한 감독되지 않은 학습으로부터의 정보와 소스 데이터에 대한 감독된 학습으로부터의 정보 모두를 활용한다. 블라인드 제약하 실제 세계 데이터에 대한 완전한 준-감독 전의 학습에서 최첨단의 성능을 가진 효과적인 접근 법을 보여준다."}
{"patent_id": "10-2018-0064572", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 DBTL의 구조 및 학습에 관한 도면이다. 1단계에서, 라벨링되지 않은 소스 인스턴스를 통해 자동인코더 (auto encoder)를 학습시킨다. 2단계에서, 라벨링된 소스 인스턴스를 통해 분류자 및 인코더의 스택(stack)을 학습시킨다. 그 후, 위 모델을 타겟 도메인으로 전이시킨 후, 3단계에서, 라벨링되지 않은 타겟 인스턴스를 통 해 자동인코더를 미세 조정(fine-tune) 시킨다. 4단계에서, 라벨링되지 않은 인스턴스를 사용해 타겟 특징 공간 을 소스 특징 공간으로 맵핑 시키며 정렬자(aligner)를 학습시킨다. 마지막으로 5단계에서, 라벨링된 타겟 인스 턴스에 인코더, 분류자, 정렬자의 전체 스택(stack)을 학습시킨다."}
