{"patent_id": "10-2023-0134879", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0051941", "출원번호": "10-2023-0134879", "발명의 명칭": "위상이 동적으로 조정된 클럭 신호에 따라 구동되는 신경 프로세싱 유닛", "출원인": "주식회사 딥엑스", "발명자": "김녹원"}}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트(PE)를 포함하는 복수의 PE 그룹들을 위해 배치된, 제1 회로;상기 제1 회로에 복수의 클럭 신호들을 출력하도록 배치된, 제2 회로; 적어도 상기 제1 회로의 피크 파워와 평균 파워의 비율을 측정하도록 설정된, 3 회로; 및상기 제3 회로에서 측정된 피크 파워와 평균 파워의 비율에 기초하여, 상기 제2 회로의 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 교정(calibrate)하도록 배치된, 제4 회로;를 포함하는,신경 프로세싱 유닛."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 제3 회로에서 측정한 피크 파워 값이 임계 파워 값보다 높을 경우,상기 제4 회로는,상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 조정(adjust)하도록 구성된, 신경 프로세싱 유닛."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 복수의 클럭 신호들의 적어도 하나의 위상은 상기 복수의 PE 그룹들의 이용율에 기초하여 기 설정된 임계파워 값에 기초하여 조정되는, 신경 프로세싱 유닛."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 제3 회로는 상기 제1 회로의 제1 피크 파워 값을 측정하고,상기 제3 회로는 상기 제1 피크 파워 값을 상기 제4 회로에 제공하고,상기 제4 회로는 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하고,이후, 상기 제3 회로는 상기 제1 회로의 제2 피크 파워 값을 다시 측정하고,상기 제4 회로는 상기 제2 피크 파워 값이 상기 제1 피크 파워 값보다 더 작은지를 판단하도록 구성된, 신경 프로세싱 유닛."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서, 상기 제3 회로는 제1 시점의 제1 피크 파워 값을 측정하고,상기 제4 회로는 상기 제1 시점의 제1 피크 파워 값과 기 설정된 임계 파워 값을 비교하여 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하도록 구성된, 공개특허 10-2025-0051941-3-신경 프로세싱 유닛."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서, 상기 하나 또는 복수의 클럭 신호들은:상기 복수의 PE 그룹들 중에서 제1 그룹의 PE들이 동작하는 제1 클럭 신호; 및상기 복수의 PE 그룹들 중에서 제2 그룹의 PE들이 동작하는 제2 클럭 신호를 포함하는, 신경 프로세싱 유닛."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서, 상기 복수의 클럭 신호들은:제1 위상을 갖는 제1 클럭 신호; 및상기 제1 클럭 신호의 제1 위상 보다 늦은 제2 위상을 갖는 제2 클럭 신호를 포함하는, 신경 프로세싱 유닛."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항에 있어서, 상기복수의 클럭 신호들은:원 클럭 신호에 기초하여 생성되는 제1 클럭 신호; 및상기 원 클럭 신호를 쉬프팅 또는 드리프팅함으로써 생성되는 제2 클럭 신호를 포함하는, 신경 프로세싱 유닛."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 항에 있어서, 상기 제2 회로는:원 클럭 신호 소스(source);체인 형태로 연결되는 하나 또는 복수의 위상 변환기 - 상기 하나 또는 복수의 위상 변환기 중 제1 위상 변환기는 상기 원 클럭 신호 소스에 연결됨-; 및상기 원 클럭 신호 소스의 출력 및 상기 하나 또는 복수의 위상 변환기들의 출력들에 연결되어, 상기 원 클럭신호 소스의 출력 그리고 상기 하나 또는 복수의 위상 변환기들의 출력들 중에서 적어도 하나의 출력을 선택하는 선택기;를 포함하는, 신경 프로세싱 유닛."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "반도체(semi-conductor) 기판;ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트(PE)를 포함하는 복수의 PE 그룹들을 위해 상기 반도체 기판 상에 배치된, 제1 회로;상기 제1 회로에 복수의 클럭 신호들을 출력하도록 상기 반도체 기판 상에 배치된, 제2 회로; 적어도 상기 제1 회로의 피크 파워와 평균 파워의 비율을 측정하도록 상기 반도체 기판 상에 배치된, 3 회로;공개특허 10-2025-0051941-4-및상기 제3 회로에서 측정된 피크 파워와 평균 파워의 비율에 기초하여, 상기 제2 회로의 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 교정(calibrate)하도록 상기 반도체 기판 상에 배치된, 제4 회로;를 포함하는,시스템-온-칩."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10 항에 있어서,상기 제3 회로에서 측정한 피크 파워 값이 임계 파워 값보다 높을 경우,상기 제4 회로는,상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 조정(adjust)하도록 구성된, 시스템-온-칩."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10 항에 있어서,상기 복수의 클럭 신호들의 적어도 하나의 위상은 상기 복수의 PE 그룹들의 이용율에 기초하여 기 설정된 임계파워 값에 기초하여 조정되는, 시스템-온-칩."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10 항에 있어서,상기 제3 회로는 상기 제1 회로의 제1 피크 파워 값을 측정하고,상기 제3 회로는 상기 제1 피크 파워 값을 상기 제4 회로에 제공하고,상기 제4 회로는 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하고,이후, 상기 제3 회로는 상기 제1 회로의 제2 피크 파워 값을 다시 측정하고,상기 제4 회로는 상기 제2 피크 파워 값이 상기 제1 피크 파워 값보다 더 작은지를 판단하도록 구성된, 시스템-온-칩."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제10 항에 있어서, 상기 제3 회로는 제1 시점의 제1 피크 파워 값을 측정하고,상기 제4 회로는 상기 제1 시점의 제1 피크 파워 값과 기 설정된 임계 파워 값을 비교하여 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하도록 구성된, 시스템-온-칩."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제10 항에 있어서, 상기 하나 또는 복수의 클럭 신호들은:상기 복수의 PE 그룹들 중에서 제1 그룹의 PE들이 동작하는 제1 클럭 신호; 및상기 복수의 PE 그룹들 중에서 제2 그룹의 PE들이 동작하는 제2 클럭 신호를 포함하는, 공개특허 10-2025-0051941-5-시스템-온-칩."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "인쇄 회로 기판;ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트(PE)를 포함하는 복수의 PE 그룹들을 위해 상기 인쇄 회로 기판 상에 배치된, 제1 회로;상기 제1 회로에 복수의 클럭 신호들을 출력하도록 상기 인쇄 회로 기판 상에 배치된, 제2 회로; 적어도 상기 제1 회로의 피크 파워와 평균 파워의 비율을 측정하도록 상기 인쇄 회로 기판 상에 배치된, 3회로; 및상기 제3 회로에서 측정된 피크 파워와 평균 파워의 비율에 기초하여, 상기 제2 회로의 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 교정(calibrate)하도록 상기 인쇄 회로 기판 상에 배치된, 제4 회로;를 포함하는,전자 장치."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 제3 회로에서 측정한 피크 파워 값이 임계 파워 값보다 높을 경우,상기 제4 회로는,상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 조정(adjust)하도록 구성된, 전자 장치."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항에 있어서,상기 복수의 클럭 신호들의 적어도 하나의 위상은 상기 복수의 PE 그룹들의 이용율에 기초하여 기 설정된 임계파워 값에 기초하여 조정되는, 전자 장치."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제16항에 있어서,상기 제3 회로는 상기 제1 회로의 제1 피크 파워 값을 측정하고,상기 제3 회로는 상기 제1 피크 파워 값을 상기 제4 회로에 제공하고,상기 제4 회로는 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하고,이후, 상기 제3 회로는 상기 제1 회로의 제2 피크 파워 값을 다시 측정하고,상기 제4 회로는 상기 제2 피크 파워 값이 상기 제1 피크 파워 값보다 더 작은지를 판단하도록 구성된, 전자 장치."}
{"patent_id": "10-2023-0134879", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제16항에 있어서,상기 제3 회로는 제1 시점의 제1 피크 파워 값을 측정하고,상기 제4 회로는 상기 제1 시점의 제1 피크 파워 값과 기 설정된 임계 파워 값을 비교하여 상기 복수의 클럭 신공개특허 10-2025-0051941-6-호들 중 적어도 하나의 위상을 교정하도록 구성된, 전자 장치."}
{"patent_id": "10-2023-0134879", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 예시는 신경 프로세싱 유닛을 제공한다. 신경 프로세싱 유닛은 ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 PE 그룹들 을 위해 배치된, 제1 회로; 상기 제1 회로에 복수의 클럭 신호들을 출력하도록 배치된, 제2 회로; 적어도 상기 제1 회로의 피크 파워와 평균 파워의 비율을 측정하도록 설정된, 3 회로; 및 상기 제3 회로에서 측정된 피크 파 워와 평균 파워의 비율에 기초하여, 상기 제2 회로의 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 교정(calibrate)하도록 배치된, 제4 회로;를 포함할 수 있다."}
{"patent_id": "10-2023-0134879", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 신경 프로세싱 유닛의 순간 소모 전력을 낮추는 기술에 관한 한 것이다."}
{"patent_id": "10-2023-0134879", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(artificial intelligence: AI)도 점차 발전하고 있다. AI는 인간의 지능, 즉 인식(Recognition), 분 류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정(Control/Decision making) 등을 할 수 있는 지능을 인공적으로 모방하는 것을 의미한다. 또한, 최근에는 인공지능(AI)을 위한 연산 속도를 가속하기 위하여, 신경 프로세싱 유닛 (Neural processing unit; NPU)가 개발되고 있다. 인공지능 서비스의 목적에 따라 여러 타입의 인공신경망(ANN) 모델이 존재할 수 있다. 예를 들어, 입력되는 데 이터가 이미지 또는 영상인 경우, 인공지능 서비스를 위하여 이미지/영상 내의 객체 분류, 객체 검출, 객체 추 적 등을 위한 CNN 타입의 인공신경망모델이 사용될 수 있다. 일반적으로, 인공신경망모델은 레이어 마다 연산량이 다를 수 있다. 특히, 특정 레이어에서 연산량이 크게 증가하면, 전력 소모량이 순간적으로 증가할 수 있다."}
{"patent_id": "10-2023-0134879", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "인공신경망 연산은 데이터 인텐시브(intensive) 한 특성을 가진다. 특히, 인공신경망 연산은 병렬 처리 연산이 필요하다. 즉, 인공신경망 연산은 동시에 방대한 데이터를 빠른 속도로 병렬로 처리하지 못하면 처리 속도가 저 하되는 특성이 있다. 이에, 본 개시의 발명자들은 인공신경망 연산에 특화된 신경 프로세싱 유닛을 개발하였다. 본 개시의 발명자들 은 신경 프로세싱 유닛의 복수의 프로세싱 엘리먼트의 개수를 증가시켜서 신경 프로세싱 유닛의 병렬 처리 성능 을 향상시키고자 하였다. 또한 본 개시의 발명자들은 저전력 동작이 가능한 신경 프로세싱 유닛을 개발하고자 하였다. 한편, 엣지 디바이스에 설치되는 저전력 동작에 특화된 신경 프로세싱 유닛의 전원 공급부는 서버나 개인용 컴 퓨터(PC)에서 동작하는 그래픽 처리 장치(GPU)의 전원 공급부에 비해서 상대적으로 전원 공급 능력이 낮을 수 있다. 또한, 엣지 디바이스에 설치되는 저전력 동작에 특화된 신경 프로세싱 유닛의 전원 공급 부의 커패시턴스 의 용량이 순간적인 전원 공급을 감당하기에 부족할 수 있다. 하지만, 저전력 동작에 특화된 신경 프로세싱 유닛의 프로세싱 엘리먼트의 개수가 증가할수록, 순간적으로 공급 전압이 흔들리는 정도가 증가할 수 있다는 사실을 본 개시의 발명자들은 인식하였다. 부연 설명하면, 요구되는 신경 프로세싱 유닛의 순간 전원 공급량은 동작하는 프로세싱 엘리먼트의 개수에 비례할 수 있다. 또한, 인공신경망모델의 연산량은 각 레이어별로 편차가 상당하다. 따라서 인공신경망모델의 레이어의 연산량에 따라서 병렬로 동작하는 프로세싱 엘리먼트의 개수는 상이할 수 있다. 즉, 동시에 많은 프로세싱 엘리먼트들이 동작할 경우, 순간적으로 신경 프로세싱 유닛의 전원 공급부의 전압이 흔들리거나 또는 강하될 수 있다. 또한, 인공신경망모델의 특정 레이어의 연산량은 매우 작을 수 있다. 이러한 경우 신경 프로세싱 유닛의 구동 주파수를 증가시키더라도, 신경 프로세싱 유닛의 공급 전압 안정성이 보장될 수 있다는 것을 본 개시의 발명자 들이 인식하게 되었다. 또한, 공급 전압이 순간적으로 흔들리거나 또는 강하될 경우, 시스템 안전성을 위해서 공급 전압(VDD)을 올려야 할 경우가 발생될 수도 있다. 따라서 시스템 안전성을 확보하지 못하면 불필요하게 공급 전압이 상승되는 문제 가 발생될 수 있다. 공급 전압이 상승될 경우, 신경 프로세싱 유닛의 소비 전력이 급격하게 상승될 수 있는 문 제가 발생될 수 있다. 이러한 경우, 신경 프로세싱 유닛의 구동 주파수를 저감 시켜 신경 프로세시 유닛의 공급전원부의 전압을 안정시킬 수 있다는 사실을 본 개시의 발명자들이 인식하였다. 이와 반대로, 공급 전압이 안정될수록, 공급 전압(VDD)을 저감 시킬 수 있다. 따라서 신경 프로세싱 유닛의 공 급 전압의 안전성을 확보하면 공급 전압을 저감 시킬 수 있으며, 결과적으로 신경 프로세싱 유닛의 소비 전력을 저감할 수 있다. 이에, 본 개시의 발명자들은 프로세싱 엘리먼트의 개수가 증가하여 피크파워가 증가하더라도 인공신경망 연산에 특화된 신경 프로세싱 유닛의 공급 전압의 안정성을 개선시키는 것이 필요하다고 인식하였다. 따라서, 본 개시의 개시들은 특정 연산 단계에서 과도한 파워에 기인한 공급 전원 부의 공급 전압의 떨림을 안 정화시키는 기술적 방안들을 제시하는 것을 목적으로 한다."}
{"patent_id": "10-2023-0134879", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "전술한 목적을 달성하기 위하여, 본 개시의 일 예시는 신경 프로세싱 유닛을 제공한다. 신경 프로세싱 유닛은 ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 PE 그룹들을 위해 배치된, 제1 회로; 상기 제1 회로에 복수의 클럭 신호들을 출력하도 록 배치된, 제2 회로; 적어도 상기 제1 회로의 피크 파워와 평균 파워의 비율을 측정하도록 설정된, 3 회로; 및 상기 제3 회로에서 측정된 피크 파워와 평균 파워의 비율에 기초하여, 상기 제2 회로의 상기 복수의 클럭 신호 들의 적어도 하나의 위상을 동적으로 교정(calibrate)하도록 배치된, 제4 회로;를 포함할 수 있다. 상기 제3 회로에서 측정한 파워 값이 임계 파워 값보다 높을 경우, 상기 제4 회로는 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 조정(adjust)하도록 구성될 수 있다. 상기 복수의 클럭 신호들의 적어도 하나의 위상은 상기 복수의 PE 그룹들의 이용율에 기초하여 기 설정된 임계 파워 값에 기초하여 조정될 수 있다. 상기 제3 회로는 상기 제1 회로의 제1 파워 값을 측정하고, 상기 제3 회로는 상기 제1 파워 값을 상기 제4 회로 에 제공하고, 상기 제4 회로는 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하고, 이후, 상기 제3 회 로는 상기 제1 회로의 제2 파워 값을 다시 측정하고, 상기 제4 회로는 상기 제2 파워 값이 상기 제1 파워 값보 다 더 작은지를 판단하도록 구성될 수 있다. 상기 제3 회로는 제1 시점의 제1 파워 값을 측정하고, 상기 제4 회로는 상기 제1 시점의 제1 파워 값과 기 설정 된 임계 파워 값을 비교하여 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하도록 구성될 수 있다. 상기 하나 또는 복수의 클럭 신호들은 상기 복수의 PE 그룹들 중에서 제1 그룹의 PE들이 동작하는 제1 클럭 신 호; 및 상기 복수의 PE 그룹들 중에서 제2 그룹의 PE들이 동작하는 제2 클럭 신호를 포함할 수 있다. 상기 복수의 클럭 신호들은 제1 위상을 갖는 제1 클럭 신호; 및 상기 제1 클럭 신호의 제1 위상 보다 늦은 제2 위상을 갖는 제2 클럭 신호를 포함할 수 있다. 상기복수의 클럭 신호들은 원 클럭 신호에 기초하여 생성되는 제1 클럭 신호; 및 상기 원 클럭 신호를 쉬프팅 또는 드리프팅함으로써 생성되는 제2 클럭 신호를 포함할 수 있다. 상기 제2 회로는 원 클럭 신호 소스(source); 체인 형태로 연결되는 하나 또는 복수의 위상 변환기 - 상기 하나 또는 복수의 위상 변환기 중 제1 위상 변환기는 상기 원 클럭 신호 소스에 연결됨-; 및 상기 원 클럭 신호 소스 의 출력 및 상기 하나 또는 복수의 위상 변환기들의 출력들에 연결되어, 상기 원 클럭 신호 소스의 출력 그리고 상기 하나 또는 복수의 위상 변환기들의 출력들 중에서 적어도 하나의 출력을 선택하는 선택기;를 포함할 수 있 다. 전술한 목적을 달성하기 위하여, 본 개시의 일 예시는 시스템-온-칩을 제공한다. 시스템-온-칩은 반도체(semi- conductor) 기판; ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 PE 그룹들을 위해 상기 반도체 기판 상에 배치된, 제1 회로; 상기 제1 회로에 복수의 클럭 신호들을 출력하도록 상기 반도체 기판 상에 배치된, 제2 회로; 적어도 상기 제1 회로 의 피크 파워와 평균 파워의 비율을 측정하도록 상기 반도체 기판 상에 배치된, 3 회로; 및 상기 제3 회로에서 측정된 피크 파워와 평균 파워의 비율에 기초하여, 상기 제2 회로의 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 교정(calibrate)하도록 상기 반도체 기판 상에 배치된, 제4 회로;를 포함할 수 있다. 상기 제3 회로에서 측정한 파워 값이 임계 파워 값보다 높을 경우, 상기 제4 회로는 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 조정(adjust)하도록 구성될 수 있다. 상기 복수의 클럭 신호들의 적어도 하나의 위상은 상기 복수의 PE 그룹들의 이용율에 기초하여 기 설정된 임계 파워 값에 기초하여 조정될 수 있다. 상기 제3 회로는 상기 제1 회로의 제1 파워 값을 측정하고, 상기 제3 회로는 상기 제1 파워 값을 상기 제4 회로 에 제공하고, 상기 제4 회로는 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하고, 이후 상기 제3 회 로는 상기 제1 회로의 제2 파워 값을 다시 측정하고, 상기 제4 회로는 상기 제2 파워 값이 상기 제1 파워 값보 다 더 작은지를 판단하도록 구성될 수 있다. 상기 제3 회로는 제1 시점의 제1 파워 값을 측정하고, 상기 제4 회로는 상기 제1 시점의 제1 파워 값과 기 설정 된 임계 파워 값을 비교하여 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하도록 구성될 수 있다. 상기 하나 또는 복수의 클럭 신호들은 상기 복수의 PE 그룹들 중에서 제1 그룹의 PE들이 동작하는 제1 클럭 신 호; 및 상기 복수의 PE 그룹들 중에서 제2 그룹의 PE들이 동작하는 제2 클럭 신호를 포함할 수 있다. 전술한 목적을 달성하기 위하여, 본 개시의 일 예시는 전자 장치를 제공한다. 전자 장치는 인쇄 회로 기판; ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 PE 그룹들을 위해 상기 인쇄 회로 기판 상에 배치된, 제1 회로; 상기 제1 회로에 복수 의 클럭 신호들을 출력하도록 상기 인쇄 회로 기판 상에 배치된, 제2 회로; 적어도 상기 제1 회로의 피크 파워 와 평균 파워의 비율을 측정하도록 상기 인쇄 회로 기판 상에 배치된, 3 회로; 및 상기 제3 회로에서 측정된 피 크 파워와 평균 파워의 비율에 기초하여, 상기 제2 회로의 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동 적으로 교정(calibrate)하도록 상기 인쇄 회로 기판 상에 배치된, 제4 회로;를 포함할 수 있다. 상기 제3 회로에서 측정한 파워 값이 임계 파워 값보다 높을 경우, 상기 제4 회로는 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 조정(adjust)하도록 구성될 수 있다. 상기 복수의 클럭 신호들의 적어도 하나의 위상은 상기 복수의 PE 그룹들의 이용율에 기초하여 기 설정된 임계 파워 값에 기초하여 조정될 수 있다. 상기 제3 회로는 상기 제1 회로의 제1 파워 값을 측정하고, 상기 제3 회로는 상기 제1 파워 값을 상기 제4 회로 에 제공하고, 상기 제4 회로는 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하고, 이후 상기 제3 회 로는 상기 제1 회로의 제2 파워 값을 다시 측정하고, 상기 제4 회로는 상기 제2 파워 값이 상기 제1 파워 값보 다 더 작은지를 판단하도록 구성될 수 있다. 상기 제3 회로는 제1 시점의 제1 파워 값을 측정하고, 상기 제4 회로는 상기 제1 시점의 제1 파워 값과 기 설정 된 임계 파워 값을 비교하여 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하도록 구성될 수 있다."}
{"patent_id": "10-2023-0134879", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 예시들에 따르면, 인공신경망 연산이 복수의 클럭 신호에 따라 분산 동작되게 되어, 특정 연산 단계 의 파워를 낮출 수 있는 효과가 있다. 본 개시의 예시들에 따르면, 인공신경망 연산이 복수의 클럭 신호에 따라 분산 동작되게 되어, 신경 프로세싱 유닛에 공급되는 공급 전압의 안전성을 개선할 수 있는 효과가 있다. 본 개시의 예시들에 따르면, 인공신경망 연산이 복수의 클럭 신호에 따라 분산 동작되게 되어, 신경 프로세싱 유닛에 공급되는 공급 전압을 저감하여 신경 프로세싱 유닛의 전력 소비량을 상당히 저감할 수 있는 효과가 있 다."}
{"patent_id": "10-2023-0134879", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시의 예시들의 특정한 구조적 내지 단계적 설명들은 단지 본 개시의 개념에 따른 예시를 설명하기 위한 것 이다. 따라서 본 개시의 개념에 따른 예시들은 다양한 형태로 실시될 수 있다. 본 개시의 개념에 따른 예시들은 다양한 형태로 실시될 수 있다. 본 개시는 본 개시의 예시들에 한정되는 것으로 해석되어서는 아니 된다. 본 개시의 개념에 따른 예시에 다양한 변경을 가할 수 있고 여러 가지 형태를 가질 수 있다. 이에, 특정 예시들 을 도면에 예시하고 본 개시 또는 출원에 대해서 상세하게 설명하고자 한다. 그러나, 이는 본 개시의 개념에 따 른 예시를 특정한 개시 형태에 대해 한정하려는 것이 아니다. 본 개시의 개념에 따른 여시는 본 개시의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1 및/또는 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만 사용될 수 있다. 상기 용어들은 본 개시의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제1 구성요소 는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있다. 하지만 복수의 구성요소들 중간에 다른 구성요소가 존재할 수도 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거 나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃하는\"과 \"~ 에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 본 개시에서 사용한 용어는 단지 특정한 예시를 설명하기 위해 사용된 것으로, 본 개시를 한정하려는 의도가 아 니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"포함하 다\" 또는 \"가지다\" 등의 용어는 서술된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존 재함을 지정하려는 것이다. 따라서 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미 를 가지는 것으로 해석되어야 한다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 본 개시에 서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다.예시를 설명함에 있어서 본 개시가 속하는 기술 분야에 익히 알려져 있고 본 개시와 직접적으로 관련이 없는 기 술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 개시의 요지를 흐리지 않고 더욱 명확히 전달하기 위함이다. <용어의 정의> 이하, 본 개시에서 제시되는 개시들의 이해를 돕고자, 본 개시에서 사용되는 용어들에 대하여 간략하게 정리하 기로 한다. NPU: 신경 프로세싱 유닛(Neural Processing Unit)의 약어로서, CPU(Central processing unit)과 별개로 인공 신경망모델의 연산을 위해 특화된 프로세서를 의미할 수 있다. ANN: 인공신경망(artificial neural network)의 약어로서, 인간의 지능을 모방하기 위하여, 인간 뇌 속의 뉴런 들(Neurons)이 시냅스(Synapse)를 통하여 연결되는 것을 모방하여, 노드들을 레이어(Layer: 계층) 구조로 연결 시킨, 네트워크를 의미할 수 있다. DNN: 심층 신경망(Deep Neural Network)의 약어로서, 보다 높은 인공 지능을 구현하기 위하여, 인공신경망의 은 닉 레이어의 개수를 늘린 것을 의미할 수 있다. CNN: 컨볼루션 신경망(Convolutional Neural Network)의 약어로서, 인간 뇌의 시각 피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망이다. 컨볼루션 신경망은 영상처리에 적합한 것으로 알려져 있으며, 입력 데이 터의 특징들을 추출하고, 특징들의 패턴을 파악하기에 용이한 것으로 알려져 있다. 이하, 첨부한 도면을 참조하여 본 개시의 바람직한 예시를 설명함으로써, 본 개시를 상세히 설명한다. 이하, 본 개시의 예시를 첨부된 도면을 참조하여 상세하게 설명한다. <인공지능> 인간은 인식(Recognition), 분류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정 (Control/Decision making) 등을 할 수 있는 지능을 갖추고 있다. 인공지능(artificial intelligence: AI)은 인간의 지능을 인공적으로 모방하는 것을 의미한다. 인간의 뇌는 뉴런(Neuron)이라는 수많은 신경세포로 이루어져 있다. 각각의 뉴런은 시냅스(Synapse)라고 불리는 연결부위를 통해 수백에서 수천 개의 다른 뉴런들과 연결되어 있다. 인간의 지능을 모방하기 위하여, 생물학적 뉴런의 동작원리와 뉴런 간의 연결 관계를 모델링한 것을, 인공신경망모델이라고 한다. 즉, 인공신경망은 뉴런 들을 모방한 노드들을 레이어(Layer: 계층) 구조로 연결시킨, 시스템이다. 이러한 인공신경망모델은 레이어 수에 따라 '단층 신경망'과 '다층 신경망'으로 구분한다. 일반적인 다층신경망 은 입력 레이어와 은닉 레이어, 출력 레이어로 구성된다. 입력 레이어(input layer)은 외부의 자료들을 받 아들이는 레이어로서, 입력 레이어의 뉴런 수는 입력되는 변수의 수와 동일하다. 은닉 레이어(hidden layer)은 입력 레이어와 출력 레이어 사이에 위치하며 입력 레이어로부터 신호를 받아 특성을 추출하여 출력층 으로 전달한다. 출력 레이어(output layer)은 은닉 레이어로부터 신호를 받아 외부로 출력한다. 뉴런 간의 입력신호는 0에서 1 사이의 값을 갖는 각각의 연결강도와 곱해진 후 합산된다. 이 합이 뉴런의 임계치보다 크면 뉴런이 활성화되어 활성화 함수를 통하여 출력 값으로 구현된다. 한편, 보다 높은 인공 지능을 구현하기 위하여, 인공신경망의 은닉 레이어의 개수를 늘린 것을 심층 신경망 (Deep Neural Network, DNN)이라고 한다. DNN은 다양한 구조로 개발되고 있다. 예를 들면, DNN의 일 예시인 합성곱 신경망(convolutional neural network, CNN)은 입력 값 (영상 또는 이미지)의 특징들을 추출하고, 추출된 출력 값의 패턴을 파악하기에 용이 한 것으로 알려져 있다. CNN은 합성곱 연산, 활성화 함수 연산, 풀링(pooling) 연산 등이 특정 순서로 처리되는 형태로 구성될 수 있다. 예를 들면, DNN의 레이어 각각에서, 파라미터(i.e., 입력 값, 출력 값, 가중치 또는 커널 등)는 복수의 채널로 구성된 행렬일 수 있다. 파라미터는 합성곱 또는 행렬 곱셈으로 NPU에서 처리될 수 있다. 각 레이어에서 연산이 처리된 출력 값이 생성된다. 예를 들면, 트랜스포머(transformer)는 어텐션(attention) 기술에 기반한 DNN이다. 트랜스포머는 행렬 곱셈 (matrix multiplication) 연산을 다수 활용한다. 트랜스포머는 입력 값과 쿼리(query; Q), 키(key; K), 및 값 (value; V) 등의 파라미터를 사용하여 출력 값인 어텐션(Q,K,V)를 획득할 수 있다. 트랜스포머는 출력 값 (즉, 어텐션(Q,K,V))에 기초하여 다양한 추론 연산을 처리할 수 있다. 트랜스포머는 CNN 보다 더 우수한 추론 성능을 보여주는 경향이 있다. 도 1은 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 이하 신경 프로세싱 유닛에서 작동될 수 있는 예시적인 인공신경망모델(110a)의 연산에 대하여 설명한다. 도 1의 예시적인 인공신경망모델(110a)은 객체 인식, 음성 인식 등 다양한 추론 기능을 수행하도록 학습된 인공 신경망일 수 있다. 인공신경망모델(110a)은 심층 신경망(DNN, Deep Neural Network)일 수 있다. 단, 본 개시의 예시들에 따른 인공신경망모델(110a)은 심층 신경망에 제한되지 않는다. 예를 들어, 인공신경망모델(110a)은 DaViT, MobileViT, Swin-Transformer, Transformer, YOLO, CNN, PIDNet, BiseNet, RCNN, VGG, VGG16, DenseNet, SegNet, DeconvNet, DeepLAB V3+, U-net, SqueezeNet, Alexnet, ResNet18, MobileNet-v2, GoogLeNet, Resnet-v2, Resnet50, Resnet101, Inception-v3 등의 모델로 구현될 수 있다. 단, 본 개시는 상술한 모델들에 제한되지 않는다. 또한 인공신경망모델(110a)은 적어도 두 개의 서로 다 른 모델들에 기초한 앙상블 모델일 수도 있다. 이하 예시적인 인공신경망모델(110a)에 의해서 수행되는 추론 과정에 대해서 설명하기로 한다. 인공신경망모델(110a)은 입력 레이어(110a-1), 제1 연결망(110a-2), 제1 은닉 레이어(110a-3), 제2 연결망 (110a-4), 제2 은닉 레이어(110a-5), 제3 연결망(110a-6), 및 출력 레이어(110a-7)을 포함하는 예시적인 심층 신경망 모델이다. 단, 본 개시는 도 1에 도시된 인공신경망모델에만 제한되는 것은 아니다. 제1 은닉 레이어 (110a-3) 및 제2 은닉 레이어(110a-5)는 복수의 은닉 레이어로 지칭되는 것도 가능하다. 입력 레이어(110a-1)는 예시적으로, x1 및 x2 입력 노드를 포함할 수 있다. 즉, 입력 레이어(110a-1)는 2개의 입력 값에 대한 정보를 포함할 수 있다. 제1 연결망(110a-2)은 예시적으로, 입력 레이어(110a-1)의 각각의 노드를 제1 은닉 레이어(110a-3)의 각각의 노 드로 연결시키기 위한 6개의 가중치 값에 대한 정보를 포함할 수 있다. 각각의 가중치 값은 입력 노드 값과 곱 해지고, 곱해진 값들의 누산된 값이 제1 은닉 레이어(110a-3)에 저장된다. 가중치 값과 입력 노드 값은 인공신 경망모델의 파라미터로 지칭될 수 있다. 제1 은닉 레이어(110a-3)는 예시적으로 a1, a2, 및 a3 노드를 포함할 수 있다. 즉, 제1 은닉 레이어(110a-3)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 도 1의 제1 프로세싱 엘리먼트(PE1)는 a1 노드의 연산을 처리할 수 있다. 도 1의 제2 프로세싱 엘리먼트(PE2)는 a2 노드의 연산을 처리할 수 있다. 도 1의 제3 프로세싱 엘리먼트(PE3)는 a3 노드의 연산을 처리할 수 있다. 제2 연결망(110a-4)은 예시적으로, 제 1 은닉 레이어(110a-3)의 각각의 노드를 제2 은닉 레이어(110a-5)의 각각의 노드로 연결시키기 위한 9개의 가중 치 값에 대한 정보를 포함할 수 있다. 제2 연결망(110a-4)의 가중치 값은 제1 은닉 레이어(110a-3)로부터 입력 되는 노드 값과 각각 곱해지고, 곱해진 값들의 누산된 값이 제2 은닉 레이어(110a-5)에 저장된다. 제2 은닉 레이어(110a-5)는 예시적으로 b1, b2, 및 b3 노드를 포함할 수 있다. 즉, 제2 은닉 레이어(110a-5)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 도 1의 제4 프로세싱 엘리먼트(PE4)는 b1 노드의 연산을 처리할 수 있다. 도 1의 제5 프로세싱 엘리먼트(PE5)는 b2 노드의 연산을 처리할 수 있다. 도 1의 제6 프로세싱 엘리먼트(PE6)는 b3 노드의 연산을 처리할 수 있다. 제3 연결망(110a-6)은 예시적으로, 제2 은닉 레이어(110a-5)의 각각의 노드와 출력 레이어(110a-7)의 각각의 노 드를 연결하는 6개의 가중치 값에 대한 정보를 포함할 수 있다. 제3 연결망(110a-6)의 가중치 값은 제2 은닉 레이어(110a-5)로부터 입력되는 노드 값과 각각 곱해지고, 곱해진 값들의 누산된 값이 출력 레이어(110a-7)에 저 장된다. 출력 레이어(110a-7)는 예시적으로 y1, 및 y2 노드를 포함할 수 있다. 즉, 출력 레이어(110a-7)는 2개의 노드 값에 대한 정보를 포함할 수 있다. 도 1의 제7 프로세싱 엘리먼트(PE7)는 y1 노드의 연산을 처리할 수 있다. 도 1의 제8 프로세싱 엘리먼트(PE8)는 y2 노드의 연산을 처리할 수 있다. 각각의 노드는 특징 값에 대응될 수 있으며, 특징 값은 특징맵에 대응될 수 있다. 도 2a은 컨볼루션 신경망(CNN)의 기본 구조를 설명하기 위한 도면이다. 도 2a을 참조하면, 입력 이미지는 특정 사이즈(size)의 행과 특정 사이즈의 열로 구성된 2차원적 행렬로 표시될 수 있다. 입력 이미지는 복수의 채널을 가질 수 있는데, 여기서 채널은 입력 데이터 이미지의 컬러 성분의 수를 나타낼 수 있다. 컨볼루션 과정은 입력 이미지를 지정된 간격으로 순회하면서 커널과 합성곱 연산을 수행하는 것을 의미한다. 컨볼루션 신경망은 현재 레이어의 출력 값(합성곱 또는 행렬 곱셈)을 다음 레이어의 입력 값으로 전달하는 구조 를 가질 수 있다. 예를 들면, 합성곱(컨볼루션)은, 두 개의 주요 파라미터(입력 특징맵 및 커널)에 의해 정의된다. 파라미터는 입 력 특징맵, 출력 특징맵, 활성화 맵, 가중치, 커널, 및 어텐션(Q,K,V) 등을 포함할 수 있다, 합성곱(컨볼루션)은 입력 특징맵 위로 커널 윈도우를 슬라이딩 한다. 커널이 입력 특징맵을 슬라이딩 하는 단차 사이즈를 보폭(stride)이라고 한다. 합성곱 이후에는 풀링(pooling)이 적용될 수 있다. 또한, 컨볼루션 신경망의 끝단에는 FC (fully-connected)레 이어가 배치될 수 있다. 도 2b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 2b을 참조하면, 예시적으로 입력 이미지가 6 x 6 크기를 갖는 2차원적 행렬인 것으로 나타나 있다. 또한, 도 2b에는 예시적으로 3개의 노드, 즉 채널 1, 채널 2, 채널 3이 사용되는 것으로 나타내었다. 먼저, 합성곱 동작에 대해서 설명하기로 한다. 입력 이미지(도 2b에서는 예시적으로 6 x 6 크기인 것으로 나타내어짐)는 첫 번째 노드에서 채널 1을 위한 커널 1(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)과 합성곱되고, 그 결과로서 특징맵1(도 2b에서는 예시적으로 4 x 4 크기인 것으로 나타내어짐)이 출력된다. 또한, 상기 입력 이미지(도 2b에서는 예시적으로 6 x 6 크기인 것으로 나타내어짐)는 두 번째 노드에서 채널 2를 위한 커널 2(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)와 합성곱되고 그 결과로서 특징맵 2(도 2b에서는 예시적으로 4 x 4 크기인 것으로 나타내어 짐)가 출력된다. 또한, 상기 입력 이미지는 세 번째 노드에서 채널 3을 위한 커널 3(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)과 합성곱되고, 그 결과로서 특징맵3(도 2b에서는 예시적으로 4 x 4 크기인 것으 로 나타내어짐)이 출력된다. 각각의 합성곱을 처리하기 위해서 신경 프로세싱 유닛의 프로세싱 엘리먼트들(PE1 to PE12)은 MAC 연산을 수행하도록 구성된다. 다음으로, 활성화 함수의 동작에 대해서 설명하기로 한다. 합성곱 동작으로부터 출력되는 특징맵1, 특징맵2 그리고 특징맵3(도 2b에서는 각각의 크기는 예시적으로 4 x 4 인 것으로 나타내어짐)에 대해서 활성화 함수가 적용될 수 있다. 활성화 함수가 적용되고 난 이후의 출력은 예 시적으로 4 x 4의 크기일 수 있다. 다음으로, 폴링(pooling) 동작에 대해서 설명하기로 한다. 상기 활성화 함수로부터 출력되는 특징맵1, 특징맵2, 특징맵3(도 2b에서는 각각의 크기는 예시적으로 4 x 4인 것으로 나타내어짐)은 3개의 노드로 입력된다. 활성화 함수로부터 출력되는 특징맵들을 입력으로 받아서 폴링 (pooling)을 수행할 수 있다. 상기 폴링이라 함은 크기를 줄이거나 행렬 내의 특정 값을 강조할 수 있다. 폴링 방식으로는 최대값 폴링과 평균 폴링, 최소값 폴링이 있다. 최대값 폴링은 행렬의 특정 영역 안에 값의 최댓값 을 모으기 위해서 사용되고, 평균 폴링은 특정 영역내의 평균을 구하기 위해서 사용될 수 있다. 도 2b의 예시에서는 4 x 4 크기의 특징맵이 폴링에 의하여 2 x 2 크기로 줄어지는 것으로 나타내었다. 구체적으로, 첫 번째 노드는 채널 1을 위한 특징맵1을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출력한다. 두 번째 노드는 채널 2을 위한 특징맵2을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출 력한다. 세 번째 노드는 채널 3을 위한 특징맵3을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출력 한다. 전술한 합성곱, 활성화 함수과 폴링이 반복되고 최종적으로는, 도 8a과 같이 fully connected로 출력될 수 있다. 해당 출력은 다시 이미지 인식을 위한 인공신경망으로 입력될 수 있다. 단, 본 개시는 특징맵, 커널의 크 기에 제한되지 않는다. 지금까지 설명한 CNN은 다양한 심층신경망(DNN) 방법 중에서도 컴퓨터 비전(Vision) 분야에서 가장 많이 쓰이는 방법이다. 특히, CNN은 이미지 분류(image classification) 및 객체 검출(objection detection)과 같은 다양한 작업을 수행하는 다양한 연구 영역에서 놀라운 성능을 보였다. <ANN의 연산을 위해 필요한 하드웨어 자원> 도 3은 본 개시의 일 예시에 따른 신경 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 3에 도시된 신경 프로세싱 유닛(neural processing unit, NPU)은 인공신경망을 위한 동작을 수행하도록 특화된 프로세서이다. 인공신경망은 여러 입력 또는 자극이 들어오면 각각 가중치를 곱해 더해주고, 추가적으로 편차를 더한 값을 활 성화 함수를 통해 변형하여 전달하는 인공 뉴런들이 모인 네트워크를 의미한다. 이렇게 학습된 인공신경망은 입 력 데이터로부터 추론(inference) 결과를 출력하는데 사용될 수 있다. 신경 프로세싱 유닛는 전기/전자 회로로 구현된 반도체일 수 있다. 전기/전자 회로라 함은 수많은 전자 소자, (예컨대 트렌지스터, 커패시터)를 포함하는 것을 의미할 수 있다. Transformer 및/또는 CNN 기반의 인공신경망모델인 경우, 신경 프로세싱 유닛는 행렬 곱셈 연산, 합성곱 연산, 등을 인공신경망의 구조(architecture)에 따라 선별하여, 처리할 수 있다. 예를 들어, 합성곱 신경망(CNN)의 레이어 각각에서, 입력 데이터에 해당하는 입력 특징맵(Input feature map)과 가중치(Weight)에 해당하는 커널(kernel)은 복수의 채널로 구성된 텐서(Tensor) 또는 행렬일 수 있다. 입력 특 징맵과 커널의 합성곱 연산이 수행되며, 각 채널에서 합성곱 연산과 풀링 출력 특징맵(output feature map)이 생성된다. 출력 특징맵에 활성화 함수를 적용하여 해당 채널의 활성화맵(activation map)이 생성된다. 이후, 활 성화맵에 대한 풀링이 적용될 수 있다. 여기서 포괄적으로 활성화맵은 출력 특징맵으로 지칭될 수 있다. 이하 설명의 편의를 위해 활성화맵은 출력 특징맵으로 지칭하여 설명한다. 단, 본 개시의 예시들은 이에 제한되지 않으며, 출력 특징맵은 행렬 곱셈 연산 또는 합성곱 연산등이 적용된 것 을 의미한다. 부연 설명하면, 본 개시의 예시들에 따른 출력 특징맵은 포괄적인 의미로 해석되어야 한다. 예를 들면, 출력 특 징맵은 행렬 곱셈 연산 또는 합성곱 연산 결과값일 수 있다. 이에, 복수의 프로세싱 엘리먼트는 추가 알고 리즘을 위한 처리 회로부를 더 포함하도록 변형 실시되는 것도 가능하다. 즉, 후술할 SFU의 일부 회로 유 닛들이 복수의 프로세싱 엘리먼트에 포함되도록 구성되는 것도 가능하다. 신경 프로세싱 유닛는 상술한 인공신경망 연산에 필요한 합성곱 및 행렬 곱셈을 처리하기 위한 복수의 프 로세싱 엘리먼트를 포함하도록 구성될 수 있다. 신경 프로세싱 유닛는 상술한 인공신경망 연산에 필요한 행렬 곱셈 연산, 합성곱 연산, 활성화 함수 연산, 풀링 연산, 스트라이드 연산, 배치 정규화 연산, 스킵 커넥션 연산, 접합 연산, 양자화 연산, 클리핑 연산, 패 딩 연산에 최적화된 각각의 처리 회로를 포함하도록 구성될 수 있다.예를 들면, 신경 프로세싱 유닛는 상술한 알고리즘들 중 활성화 함수 연산, 풀링 연산, 스트라이드 연산, 배치 정규화 연산, 스킵 커넥션 연산, 접합 연산, 양자화 연산, 클리핑 연산, 패딩 연산 중 적어도 하나를 처리 하기 위한 SFU를 포함하도록 구성될 수 있다. 구체적으로, 신경 프로세싱 유닛는 복수의 프로세싱 엘리먼트(processing element: PE) , SFU, NPU 내부 메모리, NPU 컨트롤러, 및 NPU 인터페이스를 포함할 수 있다. 복수의 프로세싱 엘리먼 트, SFU, NPU 내부 메모리, NPU 컨트롤러, 및 NPU 인터페이스 각각은 수많은 트렌지 스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있 고, 동작에 의해서만 식별될 수 있다. 예컨대, 임의 회로는 복수의 프로세싱 엘리먼트으로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. NPU 컨트롤러는 신경 프로세싱 유닛의 인공신경망 추론 동작을 제어하도록 구성된 제어부 의 기능을 수행하도록 구성될 수 있다. 신경 프로세싱 유닛는 복수의 프로세싱 엘리먼트 및 SFU에서 추론될 수 있는 인공신경망모델의 파라미터를 저장하도록 구성된 NPU 내부 메모리, 및 복수의 프로세싱 엘리먼트, SFU, 및 NPU 내 부 메모리의 연산 스케줄을 제어하도록 구성된 스케줄러를 포함하는 NPU 컨트롤러를 포함할 수 있다. 신경 프로세싱 유닛는 SVC(scalable video coding) 또는 SFC(scalable feature-map coding)를 이용한 인 코딩 및 디코딩 방식에 대응되어 특징맵을 처리하도록 구성될 수 있다. 상기 방식들은 통신 채널 또는 통신 버 스의 실효 대역폭 및 신호대잡음비(signal to noise ratio; SNR)에 따라 가변적으로 데이터 전송량을 가변 하는 기술이다. 즉, 신경 프로세싱 유닛은 인코더 및 디코더를 더 포함하도록 구성되는 것도 가능하다. 복수의 프로세싱 엘리먼트는 인공신경망을 위한 동작의 일부를 수행할 수 있다. SFU는 인공신경망을 위한 동작의 다른 일부를 수행할 수 있다. 신경 프로세싱 유닛는 복수의 프로세싱 엘리먼트와 SFU를 사용하여 인공신경망모델의 연산을 하 드웨어적으로 가속하도록 구성될 수 있다. NPU 인터페이스는 시스템 버스를 통해서 신경 프로세싱 유닛와 연결된 다양한 구성요소들, 예컨대 메 모리와 통신할 수 있다. NPU 컨트롤러는 신경 프로세싱 유닛의 추론 연산을 위한 복수의 프로세싱 엘리먼트의 연산, SFU의 연산 및 NPU 내부 메모리의 읽기 및 쓰기 순서를 제어하도록 구성된 스케줄러를 포함할 수 있 다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 복수의 프로세싱 엘리먼트, SFU, 및 NPU 내부 메모리를 제어하도록 구성될 수 있다. NPU 컨트롤러 내의 스케줄러는 복수의 프로세싱 엘리먼트 및 SFU에서 작동할 인공신경망모델의 구조를 분석하거나 또는 이미 분석된 정보를 제공받을 수 있다. 분석된 정보는 컴파일러에 의해서 생성된 정보 일 수 있다. 예를 들면, 인공신경망모델이 포함할 수 있는 인공신경망의 데이터는 각각의 레이어의 노드 데이터 (즉, 특징맵), 레이어들의 배치 데이터, 지역성 정보 또는 구조에 대한 정보, 각각의 레이어의 노드를 연결하는 연결망 각각의 가중치 데이터 (즉, 가중치 커널) 중 적어도 일부를 포함할 수 있다. 인공신경망의 데이터는 NPU 컨트롤러 내부에 제공되는 메모리 또는 NPU 내부 메모리에 저장될 수 있다. 단, 이에 제한되지 않으 며 인공신경망의 데이터는 NPU 또는 NPU를 포함하는 SoC에 구비된 별도의 캐시 메모리 또는 레지스터 파일에 저 장될 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 신경 프로세싱 유닛이 수행할 인공신경망모델의 연산 순서를 스케줄링 할 수 있다. NPU 컨트롤러 내의 스케줄러는 컴파일 된 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 신경 프로세싱 유닛이 수행할 인공신경망모델의 연산 순서의 스케줄링 정보를 제공받을 수 있다. 예를 들면, 상기 스케줄링 정보는 컴파일러에서 생성된 정보일 수 있다. 컴파일러에서 생성된 스케줄링 정보는 머신 코드(machine code) 또는 이진화 코드(binary code) 등으로 지칭될 수 있다. 즉 NPU 컨트롤러에서 활용되는 스케줄링 정보는 인공신경망모델의 데이터 지역성 정보 또는 구조에 기초하 여 컴파일러에 의해서 생성된 정보일 수 있다. 부연 설명하면, 컴파일러는 인공신경망모델이 가지는 고유한 특성인 인공신경망 데이터 지역성을 얼마나 잘 이 해하고 재구성하는지에 따라 NPU의 스케줄링을 효율적으로 할 수 있다. 부연 설명하면, 컴파일러는 신경 프로세싱 유닛의 하드웨어 구조와 성능을 얼마나 잘 이해하는지에 따라 NPU의 스케줄링을 효율적으로 할 수 있다. 부연 설명하면, 컴파일러에 의해서 인공신경망모델이 신경 프로세싱 유닛에서 실행되도록 컴파일 될 때, 인공신경망 데이터 지역성이 재구성될 수 있다. 인공신경망 데이터 지역성은 인공신경망모델에 적용된 알고리즘 들, 및 프로세서의 동작 특성에 따라서 재구성될 수 있다. 부연 설명하면, 인공신경망 데이터 지역성은 신경 프로세싱 유닛이 해당 인공신경망모델을 처리하는 방식, 예를 들면, 특징맵 타일링, 프로세싱 엘리먼트의 스테이셔너리(Stationary) 방식에 따라 재구성될 수 있다. 부연 설명하면, 인공신경망 데이터 지역성은 신경 프로세싱 유닛의 프로세싱 엘리먼트의 개수, 내부 메모 리의 용량에 따라서 재구성될 수 있다. 부연 설명하면, 인공신경망 데이터 지역성은 신경 프로세싱 유닛과 통신하는 메모리의 대역폭에 따라서 재 구성될 수 있다. 왜냐하면, 상술한 각 요인들에 의해서 동일한 인공신경망모델을 연산 처리하더라도 신경 프로세싱 유닛이 클럭 단위로 매 순간 필요한 데이터의 순서를 상이하게 결정할 수 있기 때문이다. 컴파일러는 인공신경망모델의 연산에 필요한 데이터의 순서는 인공신경망의 레이어, 단위 합성곱 및/또는 행렬 곱의 연산 순서에 기초하여 데이터 지역성을 결정하고, 컴파일 된 머신 코드를 생성할 수 있다. 스케줄러는 머신 코드에 포함된 스케줄링 정보를 활용하도록 구성될 수 있다. NPU 컨트롤러 내의 스케줄러는 스케줄링 정보에 기초하여 인공신경망모델의 레이어의 특징맵 및 가중치 데 이터가 저장된 메모리 어드레스 값을 획득할 수 있다. 예를 들면, NPU 컨트롤러 내의 스케줄러는 메모리에 저장된 인공신경망모델의 레이어의 특징맵 및 가중치 데이터가 저장된 메모리 어드레스 값을 획득할 수 있다. 따라서 NPU 컨트롤러 내의 스케줄러는 구동할 인 공신경망모델의 레이어의 특징맵 및 가중치 데이터를 메인 메모리에서 가져와서 NPU 내부 메모리에 저장할 수 있다. 각각의 레이어의 특징맵은 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. 각각의 가중치 데이터는 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보, 예를 들면, 인공신경망모델의 인공신경망의 레이어들의 배치 데이터, 지역성 정보 또는 구조에 대한 정보에 기초해서 복수 의 프로세싱 엘리먼트의 연산 순서에 대한 스케줄링 정보를 제공받을 수 있다. 스케줄링 정보는 컴파일 단 계에서 생성될 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 스케줄링 된 정보를 기초로 동작 하기 때문에, 일반적인 CPU의 스케줄링 개념과 다르게 동작할 수 있다. 일반적 인 CPU의 스케줄링은 공평성, 효율성, 안정성, 반응 시간 등을 고려하여, 최상의 효율을 낼 수 있도록 동작한다. 즉, 우선 순위, 연산 시간 등을 고려해서 동일 시간내에 가장 많은 프로세싱을 수행하도록 스케줄링 한다. 종래의 CPU는 각 프로세싱의 우선 순서, 연산 처리 시간 등의 데이터를 고려하여 작업을 스케줄링 하는 알고리 즘을 사용하였다. 이와 다르게 NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 에 기초하여 결정된 신경 프로세싱 유닛의 프로세싱 순서대로 신경 프로세싱 유닛를 제어할 수 있다. 더 나아가면, NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및/또는 사용하려는 신경 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 결정된 프로세싱 순서대로 NPU를 구동할 수 있다. 단, 본 개시는 신경 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보에 제한되지 않는다. NPU 컨트롤러 내의 스케줄러는 인공신경망의 데이터 지역성 정보 또는 구조에 대한 정보를 저장하도록 구 성될 수 있다. 즉, NPU 컨트롤러 내의 스케줄러는 적어도 인공신경망모델의 인공신경망의 데이터 지역성 정보 또는 구조 에 대한 정보만 활용하더라도 프로세싱 순서를 결정할 수 있다. 더 나아가서, NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및 신경 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보를 고려하여 신경 프로세싱 유닛(10 0)의 프로세싱 순서를 결정할 수 있다. 또한, 결정된 프로세싱 순서대로 신경 프로세싱 유닛의 프로세싱 최적화도 가능하다. 즉, NPU 컨트롤러는 컴파일러로부터 컴파일 된 머신 코드에 기초하여 동작하도록 구성될 수 있으나, 다른 예에서는 NPU 컨트롤러는 임베디드 컴파일러를 내장하도록 구성되는 것도 가능하다. 상술한 구성에 따르면, 신경 프로세싱 유닛은 다양한 AI 소프트웨어의 프레임워크의 형식의 파일을 입력받아 머신 코드를 생성하도록 구성될 수 있다. 예를 들면, AI 소프트웨어의 프레임워크는 TensorFlow, PyTorch, Keras, XGBoost, mxnet, DARKNET, ONNX 등이 있다. 복수의 프로세싱 엘리먼트는 인공신경망의 특징맵과 가중치 데이터를 연산하도록 구성된 복수의 프로세싱 엘리먼트들(PE1 to PE12)이 배치된 구성을 의미한다. 각각의 프로세싱 엘리먼트는 MAC (multiply and accumulate) 연산기 및/또는 ALU (Arithmetic Logic Unit) 연산기를 포함할 수 있다. 단, 본 개시에 따른 예시 들은 이에 제한되지 않는다. 각각의 프로세싱 엘리먼트는 추가적인 특수 기능을 처리하기 위해 추가적인 특수 기능 유닛을 선택적으로 더 포 함하도록 구성될 수 있다. 예를 들면, 프로세싱 엘리먼트(PE)는 배치-정규화 유닛, 활성화 함수 유닛, 인터폴레이션 유닛 등을 더 포함하 도록 변형 실시되는 것도 가능하다. SFU는 활성화 함수 연산, 풀링(pooling) 연산, 스트라이드(stride) 연산, 배치 정규화(batch- normalization) 연산, 스킵 커넥션(skip-connection) 연산, 접합(concatenation) 연산, 양자화(quantization) 연산, 클리핑(clipping) 연산, 패딩(padding) 연산 등을 인공신경망의 구조(architecture)에 따라 선별하여, 처리하도록 구성된 회로부를 포함할 수 있다. 즉, SFU는 복수의 특수 기능 연산 처리 회로 유닛들을 포함 할 수 있다. 도 3에서는 예시적으로 복수의 프로세싱 엘리먼트들이 도시되었지만, 하나의 프로세싱 엘리먼트 내부에 MAC을 대체하여, 복수의 곱셈기(multiplier) 및 가산기 트리(adder tree)로 구현된 연산기들이 병렬로 배치되어 구성 되는 것도 가능하다. 이러한 경우, 복수의 프로세싱 엘리먼트는 복수의 연산기를 포함하는 적어도 하나의 프로세싱 엘리먼트로 지칭되는 것도 가능하다. 복수의 프로세싱 엘리먼트는 복수의 프로세싱 엘리먼트들(PE1 to PE12)을 포함하도록 구성된다. 도 3에 도 시된 복수의 프로세싱 엘리먼트들(PE1 to PE12)은 단지 설명의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼 트들(PE1 to PE12)의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12)의 개수에 의해서 복수 의 프로세싱 엘리먼트의 사이즈 또는 개수가 결정될 수 있다. 복수의 프로세싱 엘리먼트의 사이즈는 N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 복수의 프로세싱 엘리먼트는 N x M 개의 프로세싱 엘리먼트를 포함할 수 있다. 즉, 프로세싱 엘리먼트는 1개 이상일 수 있다. 복수의 프로세싱 엘리먼트의 사이즈는 신경 프로세싱 유닛이 작동하는 인공신경망모델의 특성을 고려 하여 설계할 수 있다. 복수의 프로세싱 엘리먼트는 인공신경망 연산에 필요한 덧셈, 곱셈, 누산 등의 기능을 수행하도록 구성된 다. 다르게 설명하면, 복수의 프로세싱 엘리먼트는 MAC(multiplication and accumulation) 연산을 수행하 도록 구성될 수 있다. 이하 복수의 프로세싱 엘리먼트 중 제1 프로세싱 엘리먼트(PE1)를 예를 들어 설명한다. 도 4a는 본 개시의 일 예시에 적용될 수 있는 복수의 프로세싱 엘리먼트 중 하나의 프로세싱 엘리먼트를 설명하 는 개략적인 개념도이다. 본 개시의 일 예시에 따른 신경 프로세싱 유닛은 복수의 프로세싱 엘리먼트, 복수의 프로세싱 엘리먼 트에서 추론될 수 있는 인공신경망모델을 저장하도록 구성된 NPU 내부 메모리 및 복수의 프로세싱 엘 리먼트 및 NPU 내부 메모리를 제어하도록 구성된 NPU 컨트롤러를 포함하고, 복수의 프로세싱 엘 리먼트는 MAC 연산을 수행하도록 구성되고, 복수의 프로세싱 엘리먼트는 MAC 연산 결과를 양자화해서 출력하도록 구성될 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. NPU 내부 메모리은 메모리 사이즈와 인공신경망모델의 데이터 사이즈에 따라 인공신경망모델의 전부 또는 일부를 저장할 수 있다. 제1 프로세싱 엘리먼트(PE1)는 곱셈기, 가산기, 누산기, 및 비트 양자화 유닛을 포함할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않으며, 복수의 프로세싱 엘리먼트는 인공신경망의 연 산 특성을 고려하여 변형 실시될 수도 있다. 곱셈기는 입력 받은 (N)bit 데이터와 (M)bit 데이터를 곱한다. 곱셈기의 연산 값은 (N+M)bit 데이터 로 출력된다. 곱셈기는 하나의 변수와 하나의 상수를 입력 받도록 구성될 수 있다. 누산기는 (L)loops 횟수만큼 가산기를 사용하여 곱셈기의 연산 값과 누산기의 연산 값을 누산 한다. 따라서 누산기의 출력부와 입력부의 데이터의 비트 폭은 (N+M+log2(L))bit로 출력될 수 있다. 여기서 L은 0보다 큰 정수이다. 누산기는 누산이 종료되면, 초기화 신호(initialization reset)를 입력 받아서 누산기 내부에 저장된 데이터를 0으로 초기화 할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 비트 양자화 유닛은 누산기에서 출력되는 데이터의 비트 폭을 저감할 수 있다. 비트 양자화 유닛 은 NPU 컨트롤러에 의해서 제어될 수 있다. 양자화된 데이터의 비트 폭은 (X)bit로 출력될 수 있다. 여기서 X는 0보다 큰 정수이다. 상술한 구성에 따르면, 복수의 프로세싱 엘리먼트는 MAC 연산을 수행하도 록 구성되고, 복수의 프로세싱 엘리먼트는 MAC 연산 결과를 양자화해서 출력할 수 있는 효과가 있다. 특히 이러한 양자화는 (L)loops가 증가할수록 소비 전력을 더 절감할 수 있는 효과가 있다. 또한 소비 전력이 저감되 면 발열도 저감할 수 있는 효과가 있다. 특히 발열을 저감하면 신경 프로세싱 유닛의 고온에 의한 오동작 발생 가능성을 저감할 수 있는 효과가 있다. 비트 양자화 유닛의 출력 데이터(X)bit은 다음 레이어의 노드 데이터 또는 합성곱의 입력 데이터가 될 수 있다. 만약 인공신경망모델이 양자화되었다면, 비트 양자화 유닛은 양자화된 정보를 인공신경망모델에서 제공받도록 구성될 수 있다. 단, 이에 제한되지 않으며, NPU 컨트롤러는 인공신경망모델을 분석하여 양자 화된 정보를 추출하도록 구성되는 것도 가능하다. 따라서 양자화된 데이터 사이즈에 대응되도록, 출력 데이터 (X)bit를 양자화 된 비트 폭으로 변환하여 출력될 수 있다. 비트 양자화 유닛의 출력 데이터(X)bit는 양자 화된 비트 폭으로 NPU 내부 메모리에 저장될 수 있다. 본 개시의 일 예시에 따른 신경 프로세싱 유닛의 복수의 프로세싱 엘리먼트는 곱셈기, 가산기 , 및 누산기를 포함한다. 비트 양자화 유닛은 양자화 적용 여부에 따라 취사 선택될 수 있다. 다른 예시에서는, 비트 양자화 유닛은 SFU에 포함되도록 구성되는 것도 가능하다. 도 4b는 본 개시의 일 예시에 적용될 수 있는 SFU를 설명하는 개략적인 개념도이다. 도 4b를 참고하면 SFU는 여러 기능 유닛을 포함한다. 각각의 기능 유닛은 선택적으로 동작될 수 있다. 각 각의 기능 유닛은 선택적으로 턴-온되거나 턴-오프될 수 있다. 즉, 각각의 기능 유닛은 설정이 가능하다. 다시 말해서, SFU는 인공신경망 추론 연산에 필요한 다양한 회로 유닛들을 포함할 수 있다. 예를 들면, SFU의 회로 유닛들은 건너뛰고 연결하기(skip-connection) 동작을 위한 기능 유닛, 활성화 함 수(activation function) 동작을 위한 기능 유닛, 풀링(pooling) 동작을 위한 기능 유닛, 양자화 (quantization) 동작을 위한 기능 유닛, NMS(non-maximum suppression) 동작을 위한 기능 유닛, 정수 및 부동소수점 변환(INT to FP32) 동작을 위한 기능 유닛, 배치 정규화(batch-normalization) 동작을 위한 기능 유닛, 보간법(interpolation) 동작을 위한 기능 유닛, 연접(concatenation) 동작을 위한 기능 유닛, 및 바이아스 (bias) 동작을 위한 기능 유닛 등을 포함할 수 있다. SFU의 기능 유닛들은 인공신경망모델의 데이터 지역성 정보에 의해서 선택적으로 턴-온되거나 혹은 턴-오 프될 수 있다. 인공신경망모델의 데이터 지역성 정보는 특정 레이어를 위한 연산이 수행될 때, 해당 기능 유닛 의 턴-오프 혹은 턴-오프와 관련된 제어 정보를 포함할 수 있다. SFU의 기능 유닛들 중 활성화된 유닛은 턴-온 될 수 있다. 이와 같이 SFU의 일부 기능 유닛을 선택적 으로 턴-오프하는 경우, 신경 프로세싱 유닛의 소비 전력을 절감할 수 있다. 한편, 일부 기능 유닛을 턴- 오프하기 위하여, 파워 게이팅(power gating)을 이용할 수 있다. 또는, 일부 기능 유닛을 턴-오프하기 위하여, 클럭 게이팅(clock gating)을 수행할 수도 있다. 도 5는 도 3에 도시된 신경 프로세싱 유닛의 변형예를 나타낸 예시도이다. 도 5에 도시된 신경 프로세싱 유닛은 도 3에 예시적으로 도시된 프로세싱 유닛과 비교하면, 복수의 프로세싱 엘리먼트를 제외하곤 실질적으로 동일하기 때문에, 이하 단지 설명의 편의를 위해서 중복 설명은 생략할 수 있다. 도 5에 예시적으로 도시된 복수의 프로세싱 엘리먼트는 복수의 프로세싱 엘리먼트들(PE1 to PE12) 외에, 각각의 프로세싱 엘리먼트들(PE1 to PE12)에 대응되는 각각의 레지스터 파일들(RF1 to RF12)을 더 포함할 수 있 다. 도 5에 도시된 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12)은 단지 설명 의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12) 의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12)의 개수에 의해서 복수의 프 로세싱 엘리먼트의 사이즈 또는 개수가 결정될 수 있다. 복수의 프로세싱 엘리먼트 및 복수의 레지스 터 파일들(RF1 to RF12)의 사이즈는 N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 복수의 프로세싱 엘리먼트의 어레이 사이즈는 신경 프로세싱 유닛이 작동하는 인공신경망모델의 특 성을 고려하여 설계할 수 있다. 부연 설명하면, 레지스터 파일의 메모리 사이즈는 작동할 인공신경망모델의 데 이터 사이즈, 요구되는 동작 속도, 요구되는 소비 전력 등을 고려하여 결정될 수 있다. 신경 프로세싱 유닛의 레지스터 파일들(RF1 to RF12)은 프로세싱 엘리먼트들(PE1 to PE12)과 직접 연결된 정적 메모리 유닛이다. 레지스터 파일들(RF1 to RF12)은 예를 들면, 플립플롭, 및/또는 래치 등으로 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 대응되는 프로세싱 엘리먼트들(PE1 to PE12)의 MAC 연산 값을 저장하도 록 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 NPU 내부 메모리와 가중치 데이터 및/또는 노드 데이 터를 제공하거나 제공받도록 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 MAC 연산 시 누산기의 임시 메모리의 기능을 수행하도록 구성되는 것도 가능하 다. <본 개시의 발명자에 의해 찾아진 기술적 난관> 인공지능 서비스의 목적에 따라 여러 타입의 인공신경망(ANN) 모델이 존재할 수 있다. 예를 들어, 입력되는 데 이터가 이미지 또는 영상인 경우, 인공지능 서비스를 위하여 이미지/영상 내의 객체 분류, 객체 검출, 객체 추 적 등을 위한 CNN 타입의 인공신경망모델이 사용될 수 있다. 일반적으로, 인공신경망모델은 레이어 마다 연산량이 다를 수 있다. 이를 도 6을 참조하여 설명하기로 한다. 도 6a은 예시적인 인공신경망모델 내의 각 레이어 별 데이터의 크기를 나타낸 예시도이다. 도 6b는 도 6a에 도 시된 예시적 인공신경망모델에서 각 레이어 별 데이터 사이즈 등을 나타낸 예시적인 테이블이다.도 6a에 도시된 예시적인 인공신경망모델은 Mobilenet V1인 것으로 나타나 있다. 도 6a에 도시된 가로축은 예시 적인 인공신경망모델에서 레이어들을 순차적으로 나타내고, 세로축은 데이터의 크기를 나타낸다. 도 6a에 도시된 레이어 1을 참고하면, 입력 특징맵의 크기(IFMAP_SIZE) 보다 출력 특징맵의 크기(OFMAP_SIZE)가 더 큰 것을 알 수 있다. 레이어 1의 출력 특징맵은 레이어 2로 전달되어, 상기 레이어 2의 입력 특징맵이 된다. 상기 레이어 2의 연산이 마쳐지면, 출력 특징맵이 출력된다. 상기 레이어 2의 출력 특징맵은 다시 레이어 3로 전달되어, 상기 레이어 3 의 입력 특징맵이 된다. 이처럼 각 레이어에 입력되는 입력 데이터의 크기와 각 레이어에서 출력되는 출력 특징맵의 크기는 상이할 수 있다. 이에 따라 임의 레이어에서는 연산량이 작을 수도 있지만 다른 레이어에서는 연산량이 매우 클 수 있다. 이처럼 레이어 별 연산량이 큰 폭으로 변화됨에 따라, 전력 제어가 쉽지 않게 되는 문제가 발생한다. 각 레이어는 신경 프로세싱 유닛의 프로세싱 엘리먼트의 개수 및 NPU 내부 메모리의 용량 한계에 의해서 복수의 연산 단계로 나누어져서 처리될 수 있다. 이에, 신경 프로세싱 유닛은 각 레이어를 복수의 타일로 나누어 복수의 연산 단계를 처리하도록 스케줄링 할 수 있다. 상기 스케줄링은 컴파일러에서 수행될 수 있다. 예를 들면, 하나의 레이어는 4개의 타일로 분할될 수 있다. 각각의 타일은 신경 프로세싱 유닛에서 순차적으로 처리될 수 있다. 컴파일 된 인공신경망모델은 컴파일 될 때 결정된 연산 순서 정보를 스케줄러에 저장할 수 있 다. 이때 각 연산 단계 마다 PE 이용률이 정보가 제공될 수 있다 각 레이어의 연산량은 MAC으로 알 수 있다. 각 레이어별 로 연산량은 최대 227배 차이가 나는 것을 알 수 있다. 신경 프로세싱 유닛은 각 레이어의 연산 단계마다 MAC 연산량에 비례하여 복수의 프로세싱 엘리먼트 중 이용되는 프로세싱 엘리먼트의 개수를 결정할 수 있다. 그리고 이용되는 프로세싱 엘리먼트의 개수에 비례하 여 전력 소비량이 증가할 수 있다. 여기서, 이용되는 프로세싱 엘리먼트의 개수와 프로세싱 엘리먼트의 총 개수를 알면 신경 프로세싱 유닛의 이용률(%)을 계산할 수 있다. 따라서, 신경 프로세싱 유닛에서 처리되는 인공신경망모델의 각 레이어의 각 연산 단계마다 PE 이용률이 계산될 수 있다. 또한 신경 프로세싱 유닛이 처리하는 인공신경망모델의 구조가 변경되지 않는 한, 각 연 산 단계 별 PE 이용률은 반복적으로 정확히 예측될 수 있다. 즉, 신경 프로세싱 유닛은 특정 인공신경망모 델을 반복적으로 추론할 수 있다. 이러한 경우, 신경 프로세싱 유닛은 동일한 가중치 파라미터와 동일한 네트워크 레이어 구조를 반복 사용할 수 있다는 것을 본 개시의 발명자들이 인식하였다. 또한, 인접한 레이어들의 MAC 연산량 차이가 클수록, 인접한 레이어들 사이의 피크 파워의 편차가 증가할 수 있 다. 그리고 인접한 레이어들의 피크 파워의 편차가 클수록 공급 전압(VDD)이 더 흔들릴 수 있다. 이때, 공급 전 압(VDD)의 안정성을 고려하여 구동 주파수를 가변 할 수 있다는 사실을 본 개시의 발명자들이 인식하였다. 특히, 특정 레이어의 연산 단계에서 연산량이 크게 증가하면, 순간 전력 소모량이 증가함으로써, 시스템 안정성 이 저하되는 문제가 발생할 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. 예를 들어, 특정 레이어의 연산 단계의 연산을 위해서 동시에 많은 프로세싱 엘리먼트들이 가동될 수 있다. 각 각의 프로세싱 엘리먼트 구동을 위해서 소정의 전원이 필요하며, 상당한 개수의 프로세싱 엘리먼트들이 동시에 구동을 하면 순간적으로 필요한 파워가 급증할 수 있다. 만약 신경 프로세싱 유닛이 저전력 동작에 특화되어 설 계될 경우, 서버용 신경 프로세싱 유닛보다 파워 공급 능력이 상대적으로 부족할 수 있다. 따라서, 이러한 엣지 디바이스용 신경 프로세싱 유닛은 순간 파워 공급 이슈에 상대적으로 더 취약할 수 있으며, 파워 공급량이 폭증 할 경우, 공급 전압(VDD)이 흔들릴 수 있다. 특히 공급 전압(VDD)이 트랜지스터의 임계 전압 이하로 떨어질 경 우, 트랜지스터에 저장된 데이터가 손실될 수 있다. 다르게 설명하면, 공급 전압(VDD)이 낮아지는 경우 트랜지 스터의 동작 속도가 저하되어 setup/hold violation 문제가 발생해 오동작이 일어날 수 있다. 특히 이러한 현상 은 반도체 파운더리의 공정이 3nm, 4nm, 5nm, 7nm 와 같이 낮아질수록 더 심화될 수 있다. 다른 예를 들어, 인공신경망을 위한 연산, 즉 예컨대 가산(add), 곱셈(multiply), 누산(accumulate)을 수행하는 PE들이 전력 소모를 순간적으로 많이 사용함으로써, 신경 프로세싱 유닛 내의 다른 컴포넌트, 예컨대 내부 메모리에는 충분한 전력이 공급되지 못할 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. 구체적 으로 내부 메모리에 충분한 전력이 공급되지 못하면, 저장되어 있는 데이터 비트가 손상(compromise)될 가능성도 배제할 수 없는 문제점이 있을 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. 더 나아가서, 특정 연산 단계에서 PE들의 이용률이 낮을 경우, 신경 프로세싱 유닛의 구동 주파수를 증가시키더 라도, 신경 프로세싱 유닛의 전압 안정성이 보장될 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. <본 특허의 개시들> 전술한 문제점은, 인공신경망모델의 연산이 하드웨어적으로 하나의 클럭 신호를 기준으로 수행되기 때문에 발생 하는 것임을, 본 개시의 발명자들이 인식하게 되었다. 따라서, 본 특허의 발명자는, NPU 내의 상기 복수의 PE들은 제1 그룹의 PE들과 제2 그룹의 PE들로 분할한 후, 서로 다른 클럭 신호에 따라 동작되도록 함으로써, 피크 파워를 낮추는 기법을 발명하게 되었다. 본 개시의 발명자들이 발명한 기법이 구현되는 여러 예시들을 이하 도면을 참고하여 설명하기로 한다. 도 7a는 본 개시의 일 예시에 따른 NPU의 구조를 나타낸 예시도이다. 도 7a에서는 NPU가 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 내부 메모리, SFU, 클럭 소 스, 파워 측정 통합 회로(power management integrated circuit, PMIC) 그리고 위상 결정 회로(19 0)를 포함하는 것으로 도시되어 있다. 몇몇 예시에서는, 상기 NPU는 도 3 또는 도 5에 도시된 바와 같이 NPU 컨트롤러 그리고 NPU 인터페이 스를 더 포함할 수 있다. 복수의 PE들은 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2)으로 구분될 수 있다. 상기 복수의 PE들은 NPU 코어(core), NPU 엔진(engine), NPU 쓰레드(thread) 등으로 지칭되는 것도 가능하다. 상기 NPU 코어, NPU 엔진, NPU 쓰레드 등은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 상기 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-02), 상기 내부 메모리, NPU 컨트롤러, 상기 NPU 인 터페이스, SFU, 상기 클럭 소스, 상기 PMIC 그리고 상기 위상 결정 회로 각각은 수 많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어 려울 수 있고, 동작에 의해서만 식별될 수 있다. 상기 클럭 소스는 도 8a에 도시된 바와 같이 클럭 신호 생성 회로와 위상 변환기를 포함할 수 있고 이에 대해서는 후술하기로 한다. 예를 들어, 제1 그룹의 PE(110-1) 및 제2 그룹의 PE(110-2)들은 제1 회로로 지칭될 수 있다. 예를 들어, 위상 변환기를 포함하는 클럭 소스는 제2 회로로 지칭될 수 있다. 예를 들어, PMIC는 제3 회로로 지칭될 수 있다. 예를 들어, 위상 결정 회로는 제4 회로로 지칭될 수 있다. 단 본 개시의 예시들은 이에 제한되지 않으며, 임의 회로는 복수의 PE들으로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. 도 7a에 도시된 제1 그룹의 PE(110-1)와 제2 그룹의 PE(110-2)는 도 3 또는 도 5에 도시된 복수의 PE들이 나뉜 것으로 이해될 수 있다. 각 그룹에 속한 PE들의 개수는 도 7a에서는 예시적으로 8개인 것으로 나타나 있지 만, 각 그룹에 속한 PE들의 개수는 달라질 수 있는 것은 자명할 것이다. 또한, 각 그룹에 속한 PE들의 개수는 반-고정적으로 혹은 동적으로 변경될 수 있다. 예를 들어, 제1 그룹의 PE들(110-1)은 10개의 PE들을 포함하고, 제2 그룹의 PE들(110-2)은 6개의 PE들을 포함하도록 설정될 수 있다. 이러한 변경은 NPU 컨트롤러의 제어 에 따라 수행될 수 있다. 상기 PMIC는 상기 NPU의 적어도 하나의 회로의 파워를 측정하도록 설정된 회로일 수 있다. 예를 들면, PMIC는 제1 그룹의 PE(110-1)의 파워를 런타임동안 측정하도록 구성될 수 있다. 예를 들면, PMIC는 제2 그룹의 PE(110-1)의 파워를 런타임동안 측정하도록 구성될 수 있다. 예를 들면, PMIC는 내부 메모리의 파워를 런타임동안 측정하도록 구성될 수 있다. 예를 들면, PMIC는 SFU의 파워를 런타임동안 측정하도록 구성될 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않으며, 다양한 회로의 파워를측정하도록 구성될 수 있다. PMIC는 실시간으로 순간 파워를 측정하도록 구성될 수 있다. 이때 최대 값을 가지는 순간 파워는 피크 파 워로 정의될 수 있다. PMIC는 특정 기간 동안의 평균 파워를 측정하도록 구성될 수 있다. 부연 설명하면, PMIC는 피크 파워 및 평균 파워를 측정하도록 구성될 수 있다. PMIC는 짧은 기간에 발생되는 피크 파워 값을 저장할 수 있다. 또한 일정 기간동안 측정된 파워의 적분 값을 기초로 평균 파워를 측 정할 수 있다. PMIC는 시간 축을 따라서 그래프 형태의 파워 값을 런타임동안 제공할 수 있다. 여기서, 특정 기간동안 평균 파워가 동일하더라도, 피크 파워가 상대적으로 더 클 경우, 공급 전압(VDD)의 안 정성은 상대적으로 더 불안해질 수 있다. 이에, 위상 결정 회로는 특정 기간 동안 PMIC에서 측정된 평균 파워와 해당 기간 동안의 피크 파워의 평균 파워의 비율을 계산하도록 설계된 회로일 수 있다. 단, 본 개시의 예시들이 이에 제한되지 않으며, 상기 비율은 PMIC 또는 다른 회로에서 수행되는 것도 가능하다. 만약 피크 파워가 1W이고, 평균 파워가 1W이면 피크 파워와 평균 파워의 비율은 1:1일 수 있다. 즉, 특정 기간 동안 피크 파워와 평균 파워의 비율은 100%일 수 있다. 만약 피크 파워가 1W이고, 평균 파워가 2W이면 피크 파워와 평균 파워의 비율은 2:1일 수 있다. 즉, 특정 기간 동안 피크 파워와 평균 파워의 비율은 200%일 수 있다. 만약 피크 파워가 1W이고, 평균 파워가 3W이면 피크 파워와 평균 파워의 비율은 3:1일 수 있다. 즉, 특정 기간 동안 피크 파워와 평균 파워의 비율은 300%일 수 있다. 즉, 위상 결정 회로는 피크 파워와 평균 파워의 비율이 증가할수록, 공급 전압(VDD)의 안정성이 불안해진 다고 판단할 수 있다. 즉, PMIC는 특정 기간 동안의 평균 파워와 피크 파워의 비율 데이터를 NPU의 런타임 동안 위상 결정 회로에게 제공할 수 있다. PMIC에서 측정된 평균 파워 값 및 피크 파워 값은 특정 메모리, 레지스터 파일 등에 저장될 수 있다. 예를 들면, 측정된 평균 파워 값, 피크 파워 값, 및/또는 피크 파워와 평균 파워의 비율 값은 위상 결정 회로 에 저장될 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. 몇몇 예시들에서는, 복수의 PE들의 파워는 복수의 PE들의 이용율에 따라 증가될 수 있다는 점을 활용할 수 있다. 부연 설명하면, PE들의 파워는 PE들의 이용률에 비례하여 증가할 수 있다. 이러한 특성은 도 11 내지 도 12에 자세히 설명되어 있다. 따라서, 위상 결정 회로는 PMIC에서 측정된 파워를 기초로 복수의 PE들 의 이용률을 추론하는 것도 가능하다. 또한, PE들의 개수 및 각 PE의 파워 소비 정보를 제공받을 경우, 위상 결정 회로는 PMIC에서 측정된 파워를 기초로 PE들의 이용률을 보다 정확이 예측할 수 있다. 따라서 위상 결정 회로는 PE 그룹들의 이용률이 증가될수록 각 그룹의 PE들에 공급되는 복수의 클럭 신호들 중 적 어도 하나의 위상을 조정하여 피크 파워를 저감 시킬 수 있다. 예를 들면, PMIC는 복수의 PE 그룹들(110-1, 110-2)의 제1 피크 파워 값을 측정하고, PMIC는 제1 피크 파워 값을 위상 결정 회로에 제공할 수 있다. 이에, 위상 결정 회로는 복수의 클럭 신호들 중 적어도 하나의 위상을 교정할 수 있다. 이후, PMIC는 복수의 PE 그룹들(110-1, 110-2)의 제2 피크 파워 값 을 다시 측정하고, 위상 결정 회로는 상기 제2 피크 파워 값이 상기 제1 피크 파워 값보다 더 작은지를 판 단하도록 구성될 수 있다. 몇몇 예시들에서는, 위상 결정 회로는 상기 복수의 PE 그룹들(110-1, 110-2)에서 처리되는 특정 ANN 모델 의 특정 연산 단계에 대한 연산량 또는 복수의 PE 그룹들(110-1, 110-2)의 이용률에 대한 정보를 획득할 수 있 다. 여기서, 위상 결정 회로는 상기 복수의 PE 그룹들(110-1, 110-2)에서 처리되는 특정 ANN 모델의 특정 연산 단계에 대한 연산량 또는 복수의 PE 그룹들(110-1, 110-2)의 이용률에 대한 정보를 기초로 제어 신호를 생 성하도록 구성될 수 있다. 즉, 특정 인공신경망모델의 특정 연산 단계의 복수의 PE 그룹들(110-1, 110-2)의 이 용률을 기초로 IR-drop margin이 예측될 수 있다. 따라서, IR-drop margin이 임계값 이하로 떨어질 것이 예상될 경우, 복수의 PE 그룹들(110-1, 110-2)의 이용률과 IR-drop margin을 기초로 제어 신호를 결정하여 IR-drop margin을 확보할 수 있다. 몇몇 예시들에서는, 위상 결정 회로가 복수의 클럭 신호들 중 적어도 하나의 위상을 동적으로 조정할 수 있다. 몇몇 예시에서는, 위상 결정 회로가 기 설정됨 임계 파워 값에 기초하여 기 설정된 위상 차이를 가지도록 복수의 클럭 신호들 중 적어도 하나의 위상을 조정할 수 있다. 임계 파워 값은 PE의 이용률에 따른 피크 파워 특성과, 위상 차이에 따른 피크 파워 저감 특성을 고려하여 설정될 수 있다. 예를 들면, PMIC에서 측정된 피크 파워 값이 임계 파워 값보다 높을 경우, 위상 결정 회로는, 복수의 클럭 신호들의 적어도 하나의 위 상을 동적으로 조정(adjust)하도록 구성될 수 있다. 예를 들면, 복수의 클럭 신호들의 적어도 하나의 위상은 상 기 복수의 PE 그룹들의 이용율에 기초하여 기 설정된 임계 파워 값에 기초하여 조정될 수 있다. 상기 클럭 소스는 하나 또는 복수의 클럭 신호들을 출력하도록 설정된 회로일 수 있다. 상기 위상 결정 회로는 상기 NPU의 피크 파워를 감소시키기 위하여, 피크 파워와 평균 파워의 비율에 기초하여, 상기 클럭 소스에 의해서 출력될 상기 하나 또는 복수의 클럭 신호들의 적어도 하나의 위상을 교정(calibrate) 혹은 조정(adjust)하기 위한 제어 신호를 생성할 수 있다. 상기 제어 신호는 상기 위상 결정 회로에 입력될 수 있다. 후술될 도 11 및 도 12에서는 평균 파워가 동일하더라도, 복수의 클럭 신호들의 적어도 하나의 위상을 교정하여 피크 파워를 저감하는 본개시의 일 예시 및 비교예가 설명된다. 그러면, 상기 클럭 소스는 위상 결정 회로의 제어 신호에 대응하여, 상기 복수의 클럭 신호들 중 적 어도 하나의 위상을 동적으로 조정(adjust)하여 출력할 수 있다. 상기 복수의 클럭 신호들의 적어도 하나의 위 상은 상기 복수의 PE들의 피크 파워와 평균 파워의 비율에 기초하여 조정될 수 있다. 예를 들어, 피크 파워와 평균 파워의 비율이 이전 보다 높게 측정된 경우, 상기 피크 파워를 감소시키기 위하여, 상기 위상 결정 회로로부터의 제어 신호에 따라 상기 클럭 소스는 상기 원 클럭 신호의 위상 을 쉬프트(shift) 또는 드리프트(drift)한, 위상 지연 클럭 신호를 출력할 수 있다. 상기 위상을 지연 클럭 신호 출력 후 위상 결정 회로는 저장된 피크 파워 값과 위상 교정 후 측정된 피크 파워 값을 비교할 수 있다. 만약 위상 교정 후 측정된 피크 파워 값이 기존의 피크 파워 값 보다 저감될 경우, 위상 지연 결정 회로는 위상 교정이 성공적이었다고 판단할 수 있다. 따라서, 위상 결정 회로는 교정된 위상 차이를 계속 유지할 수 있다. 만약 위상 교정 후 측정된 피크 파워 값이 기존의 피크 파워 값 보다 높을 경우, 위상 지연 결정 회로는 위상 교정이 실패였다고 판단할 수 있다. 따라서, 위상 결정 회로는 이전 위상으로 원복 할 수 있다. 위상 교정 단위는 한번의 추론 단위로 수행될 수 있다. 구체적으로 설명하면, 특정 인공신경망모델을 1회 추론 할 경우, 복수의 PE 그룹들에 입력되는 복수의 클럭들의 위상이 같거나 상이하더라도 측정된 평균 파워는 동일 할 수 있다. 다만, 피크 파워는 위상에 따라 상이해질 수 있다. 따라서, 위상 결정 회로는 다양한 위상 차 를 발생시키는 제어 신호들을 클럭 소스에 입력하여, 가장 낮은 값의 피크 파워를 가지는 위상 차를 도출 할 수 있다. 여기서, 1회 추론은 인공신경망모델의 입력 레이어에서 출력 레이어까지의 모든 연산을 처리하는 것을 의미할 수 있다. 즉, 1회 추론은 1개의 이미지 프레임의 추론으로 지칭될 수 있다. 따라서 위상 교정은 적어도 한 개의 이미지 프레임 단위 또는 1회 추론 단위로 수행될 수 있다. 예를 들면, 10단계의 서로 다른 위상을 가지는 10개의 제어 신호를 이미지 프레임 별로 클럭 소스에 입력 할 수 있다. 따라서 위상 결정 회로는 각 위상 별 피크 파워를 10 프레임 동안 측정할 수 있다. 결과적으 로, 위상 결정 회로는 10 프레임 동안 측정된 결과를 기초로 복수의 클럭 신호들의 최적의 위상 차를 도출 할 수 있다. 이에 최적의 위상 차 결과 값은 위상 결정 회로에 저장될 수 있다. 위상 결정 회로는 NPU가 처리하는 인공신경망모델이 교체되지 전까지 상기 위상 차를 유지할 수 있다. 만약, 피크 파워가 증가할 경우, 위상 결정 회로는 복수의 클럭 신호 중 적어도 하나의 클럭의 위상 교정을 다시 수행하는 것도 가능하다. 만약 인공신경망모델이 교체될 경우, 위상 결정 회로는 위상 교정 단계를 다시 수행하도록 구성될 수 있다. 상기 클럭 소스는 도 8a에 도시된 바와 같이 클럭 신호 생성 회로와 위상 변환기를 포함할 수 있다. 상기 클럭 신호 생성 회로는 오실레이터(Oscillator)를 포함할 수 있다. 또한, 상기 오실레이터는PWM(Pulse Width Modulator)를 포함할 수 있다. 상기 PWM은 클럭의 사이클을 Duty Cycle를 변경함으로써 high 신호와 low 신호의 비율을 조정할 수 있다. 상기 클럭 소스 내의 클럭 신호 생성 회로가 원 클럭 신호를 생성하여 출력하면, 상기 위상 결정 회 로로부터의 상기 제어 신호에 따라 상기 클럭 소스 내의 상기 위상 변환기는 원 클럭 신호를 그 대로(bypass)하여 출력하기도 하고 또는 상기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)하여 출력 할 수 있다. 예를 들어, 상기 위상 결정 회로로부터의 상기 제어 신호에 따라, 상기 클럭 소스 내의 상기 위상 변 환기가 동작할 수 있다. 위상 변환기는 상기 클럭 신호 생성 회로가 생성한 상기 원 클럭 신호 를 그대로(bypass) 상기 제1 그룹의 PE들(110-1)에게 전달할 수 있다. 또는 상기 위상 변환기는 상기 클럭 신호 생성 회로가 상기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)시킨 제1 지연 클럭 신호를 상기 제1 그룹의 PE들(110-1)에게 전달할 수 있다. 또한, 상기 위상 결정 회로로부터의 상기 제어 신호에 따라 상기 위상 변환기는 상기 클럭 신호 생성 회로가 생성한 상기 원 클럭 신호의 위상을 지연 (delay) 또는 쉬프트(shift)시킨 제2 지연 클럭 신호를 제2 그룹의 PE들(110-2)에게 전달할 수 있다. 이상에서 설명한 바와 같이, 본 개시의 일 예시는 도 3 또는 도 5에 도시된 복수의 PE들을 도 7a에 도시된 바와 같이 제1 그룹의 PE(110-1)와 제2 그룹의 PE(110-2)로 나눈 후, 상기 제1 그룹의 PE(110-1)와 상기 제2 그룹의 PE(110-2)들을 서로 다른 클럭 신호에 따라 분산 동작하게 함으로써, 복수의 PE 그룹들(110-1, 110-2)의 피크 파워를 낮출 수 있도록 한다. 또한, 본 개시의 일 예시는 위상이 서로 다른 클럭 신호들을 복수개로 제공하기 위해서, 클럭 소스를 복수 개로 두지 않고, 단순히 위상 변환기만을 추가함으로써, 제조 단가를 낮출 수 있도록 한다. 도 7b는 본 개시의 다른 예시에 따른 NPU의 구조를 타낸 예시도이다. 도 7b에서는 NPU가 제1 그룹의 PE(110-1)와 제2 그룹의 PE(110-2)와 내부 메모리 그리고 SFU를 포함하는 것으로 도시되어 있으나, 상기 NPU는 그 외에도 도 3 또는 도 5에 도시된 바와 같이 NPU 컨트롤 러 그리고 NPU 인터페이스를 더 포함할 수 있다. 상기 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-02), 상기 내부 메모리, NPU 컨트롤러, 상기 NPU 인 터페이스 그리고 SFU 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 도 7b를 참조하면, 상기 클럭 소스, 상기 PMIC 그리고 상기 위상 결정 회로는 NPU 내부가 아니라 외부에 위치하는 것으로 도시되어 있다. 그 외에는 도 7a에 도시된 것과 동일하므로, 도 7b에 대해서는 별도로 설명하지 않기로 하고, 도 7a를 참조하여 설명한 내용을 그대로 원용하기로 한다. 한편, 도 7a 및 도 7b에 도시된 상기 내부 메모리와 상기 SFU도 상기 위상 변환기로부터 출력되 는 클럭 신호(예를 들어, 원 클럭 신호 또는 위상 지연된 클럭 신호)에 따라 동작할 수 있다. 다른 한편, 도 7a 및 도 7b에 도시된 상기 제1 그룹의 PE들(110-1)과 상기 제2 그룹의 PE들(110-2)이 인공신경 망모델의 임의 레이어에 대한 연산을 수행 완료하면, 상기 위상 변환기로부터 각기 제공받은 클럭 신호에 따라 상기 제1 그룹의 PE들(110-1)과 상기 제2 그룹의 PE들(110-2)은 각기 출력 특징맵을 출력할 수 있다. 구체 적으로, 상기 제1 그룹의 PE들(110-1)은 제1 출력 특징맵을 상기 위상 변환기로부터 제공받은 임의의 클럭 신호(예를 들어 상기 원 클럭 신호 또는 상기 제1 지연 클럭 신호)에 따라 출력할 수 있다. 마찬가지로 상기 제 2 그룹의 PE들(110-2)은 제2 출력 특징맵을 상기 위상 변환기로부터 제공받은 임의의 클럭 신호(예를 들어 제2 지연 클럭 신호)에 따라 출력할 수 있다. 상기 제1 그룹의 PE들(110-1)에 의해서 출력되는 제1 출력 특징맵과 상기 제2 그룹의 PE들(110-2)에 의해서 출 력되는 제2 출력 특징맵이 서로 의존적이지 않고 독립적인 경우, 상기 SFU는 FIFO(First Input First Output) 방식으로 먼저 전달받은 출력 특징맵을 먼저 처리할 수 있다. 한편, 도 7a 및 도 7b에서 도시되지는 않았으나, 상기 SFU는 상기 NPU 내부 메모리에 직접 연결될 수 있다. 도 7a 및 도 7b에 도시된 상기 NPU 내의 각 엘리먼트에는 공급 전압(VDD)이 입력될 수 있다. 도 7a 및 도 7b에서는 공통된 공급 전압(VDD)이 입력되는 것으로 도시 되었으나, 본 개시의 예시들은 이에 제한 되지 않는다. 몇몇 예시들에서는, 상기 NPU 내의 각 엘리먼트에는 독립되거나 혹은 분리된 공급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 다른 예시들에서는, 상기 NPU 내의 각 엘리먼트에는 독립되거나 혹은 분리된 공급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 또 다른 예시들에서는, 상기 NPU 내의 특정 엘리먼트에 제1 공급 전압과 상이한 제2 공급 전압이 입력되도 록 구성되는 것도 가능하다. 한편, 도 7a 및 도 7b에 도시된 상기 제1 그룹의 PE들(110-1)과 상기 제2 그룹의 PE들(110-2)이 인공신경망모델 의 임의 레이어에 대한 연산을 수행 완료하면, 상기 제1 그룹의 PE들(110-1)과 상기 제2 그룹의 PE들(110-2)은 연산 결과, 즉 출력 특징맵을 상기 클럭의 제1 부분 및 제2 부분 중에서 어느 하나의 부분에 맞추어서 출력할 수 있다. 이를 위하여, 상기 제1 그룹의 PE들(110-1)과 상기 제2 그룹의 PE들(110-2) 중 적어도 하나 이상은 시 간 지연 버퍼(예컨대, 쉬프트 레지스터)를 더 포함할 수 있다. 상기 시간 지연 버퍼(예컨대, 쉬프트 레지스터) 는 상기 출력이 상기 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 더 늦은 부분에 맞추어서 수행되게끔, 시 간 지연을 수행할 수 있다. 예를 들어 상기 제2 그룹의 PE들(110-2)의 출력 포트는 시간 지연 버퍼(예컨대, 쉬프트 레지스터)에 연결될 수 있다. 대안적으로, 도 7a 및 도 7b에 도시된 상기 제1 그룹의 PE들(110-1)은 제1 출력 특징맵을 상기 클럭의 제1 부분 에 맞추어서 출력하고, 상기 제2 그룹의 PE들(110-2)은 제2 출력 특징맵을 상기 클럭의 제2 부분에 맞추어서 출 력할 수 있다. 이 경우, 상기 SFU가 시간 지연 버퍼(예컨대, 쉬프트 레지스터)를 포함할 수 있다. 상기 SFU는 상기 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 더 빠른 부분에 맞추어서 전달받은 출력 특징 맵을 시간 지연시킴으로써, 상기 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 늦은 부분에 맞추어서 전달받 게 될 출력 특징맵과 시간이 동기 되도록 할 수 있다. 대안적으로, 상기 제1 그룹의 PE들(110-1)에 의해서 출력되는 제1 출력 특징맵과 상기 제2 그룹의 PE들(110- 2)에 의해서 출력되는 제2 출력 특징맵이 서로 의존적이지 않고 독립적인 경우, 상기 SFU는 상기 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 더 빠른 부분에 맞추어서 전달받은 출력 특징맵을 먼저 처리할 수 있 다. 즉, 상기 제1 그룹의 PE들(110-1)로부터의 제1 출력과, 상기 제2 그룹의 PE들(110-2)로부터의 제2 출력은 상기 클럭의 제1 부분을 기준으로 전달될 수 있다. 상기 제2 그룹의 PE들(110-2)을 위한 기준 위상은 상기 클럭의 제2 부분에서 제1 부분으로 변환될 수 있다. 한편, 도 7a 및 도 7b에서 도시되지는 않았으나, 상기 SFU는 상기 NPU 내부 메모리에 직접 연결될 수 있다. 다른 한편, 도 7a 및 도 7b에서는 NPU를 설명하였으나, 상기 NPU는 SoC 형태로 구현될 수도 있다. 도 8a는 도 7a 또는 도 7b에 도시된 클럭 소스를 일 예시에 따라 상세하게 나타낸 예시도이고, 도 8b는 클 럭 소스에 의해서 출력되는 복수의 클럭 신호들을 나타낸 예시도이다. 도 8a에 도시된 바와 같이 클럭 소스는 클럭 신호 생성 회로와 위상 변환기를 포함할 수 있다. 상기 위상 변환기는 복수개의 쉬프터(S)와 다중화기/선택기(MUX/Selector)(175-5)를 포함할 수 있다. 도 8a에서는 예시적으로 쉬프터(S)가 4개(175-1, 175-2, 175-3, 175-4)인 것으로 도시되어 있다. 이하 도 8a 및 도 8b를 함께 참고하여 설명하기로 한다. 상기 클럭 신호 생성 회로로부터 출력되는 원 클럭 신호는 상기 위상 변환기 내에서 그대로(bypass) 되어 상기 다중화기/선택기(MUX/Selector)(175-5)를 거쳐서 출력될 수 있다. 또한, 상기 클럭 신호 생성 회로로부터 출력되는 원 클럭 신호는 상기 위상 변환기 내의 제1 쉬프터 (175-1)를 통해 위상이 한번 지연됨으로써 제1 지연 클럭 신호로 변환된 후, 상기 다중화기/선택기(MUX/Selector)(175-5)를 거쳐서 출력될 수 있다. 그리고, 상기 클럭 신호 생성 회로로부터 출력되는 원 클럭 신호는 상기 위상 변환기 내의 제1 쉬프 터(175-1)와 제2 쉬프터(175-2)를 통해 위상이 두번 지연됨으로써 제2 지연 클럭 신호로 변환된 후, 상기 다중 화기/선택기(MUX/Selector)(175-5)를 거쳐서 출력될 수 있다. 그리고, 상기 클럭 신호 생성 회로로부터 출력되는 원 클럭 신호는 상기 위상 변환기 내의 제1 쉬프 터(175-1), 제2 쉬프터(175-2) 그리고 제3 쉬프터(175-3)를 통해 위상이 세번 지연됨으로써 제3 지연 클럭 신호 로 변환된 후, 상기 다중화기/선택기(MUX/Selector)(175-5)를 거쳐서 출력될 수 있다. 마찬가지로, 상기 클럭 신호 생성 회로로부터 출력되는 원 클럭 신호는 상기 위상 변환기 내의 제1 쉬프터(175-1), 제2 쉬프터(175-2), 제3 쉬프터(175-3) 그리고 제4 쉬프터(175-4)를 통해 위상이 네 번 지연됨 으로써 제4 지연 클럭 신호로 변환된 후, 상기 다중화기/선택기(MUX/Selector)(175-5)를 거쳐서 출력될 수 있다. 상기 위상 결정 회로로부터의 상기 제어 신호에 따라, 상기 다중화기/선택기(MUX/Selector)(175-5)는 도 8b에 나타난 바와 같이, 복수의 클럭 신호들(즉, 상기 원 클럭 신호, 상기 제1 지연 클럭 신호, 상기 제2 지연 클럭 신호, 상기 제3 지연 클럭 신호, 상기 제4 지연 클럭 신호)을 다중화한 후, 상기 NPU 내의 각 부분에 선택 적으로 하나의 클럭 신호를 제공할 수 있다. 예를 들어, 상기 위상 결정 회로로부터의 상기 제어 신호에 따라, 상기 다중화기/선택기 (MUX/Selector)(175-5)는 상기 원 클럭 신호를 상기 내부 메모리에 제공하고, 상기 제1 지연 클럭 신호는 상기 제1 그룹의 PE들(110-1)에게 제공하고, 상기 제2 지연 클럭 신호는 상기 제2 그룹의 PE들(110-2)에게 제공 하고, 상기 제3 지연 클럭 신호는 상기 SFU에 제공할 수 있다. 이와 같이, 상기 NPU 내의 각 부분들이 서 로 위상이 다른 여러 클럭 신호에 따라 동작됨으로써, 피크 파워가 갑자기 상승되는 것이 억제될 수 있다. 본 개시의 몇몇 예시들에서는, 클럭 소스는 클럭 스킵퍼(skipper) 회로, 클럭 디바이더(divider) 회로, 클 럭 게이팅(gating), 오실레이터(oscillator), 위상 루프 (PLL) 회로, 선택기 중 적어도 하나를 포함한 회로일 수 있다. 클럭 소스는 클럭 스킵퍼(skipper) 회로, 클럭 디바이더(divider) 회로, 클럭 게이팅(gating) 회로, 오실레이터(oscillator) 회로, 위상 루프(PLL) 회로, 선택기 중 적어도 두개가 병합된 회로일 수 있다. 단, 본 개시의 예시는 이에 제한되지 않는다. 도 9는 본 개시의 일 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 9를 참조하면, 예시적인 SoC은 복수의 NPU들과 복수의 CPU(central processing unit)들, 복수의 메모 리들을 포함할 수 있다. 상기 복수의 NPU들은 예컨대 제1 NPU(100-1)과 제2 NPU(100-2)를 포함할 수 있다. 그리 고, 상기 복수의 CPU들은 예컨대 제1 CPU(200-1)과 제2 CPU(200-2)를 포함할 수 있다. 상기 복수의 메모리들은 제1 메모리(300-1)와 제2 메모리(300-2)를 포함할 수 있다. 상기 복수의 NPU들 각각(예컨대 제1 NPU(100-1)과 제2 NPU(100-2))은 도 7a 및 도 7b에 도시된 바와 같이 제1 그룹의 PE들과 제2 그룹의 PE들을 포함할 수 있다. 도 9에서는 상기 복수의 NPU들, 복수의 CPU들 그리고 복수의 메모리들의 개수가 각기 2개인 것으로 도시되었지 만, 이에 한정되는 것은 아니고 개수는 4개, 6개, 8개 등으로 다양하게 변형될 수 있다. 상기 예시적인 SoC는 메모리 컨트롤러와, 클럭 소스와, 위상 변환기와, 시스템 버스(50 0)와 그리고 I/O(input output) 인터페이스를 포함할 수 있다. 상기 시스템 버스는 반도체 다이(die) 위에 형성된 전기 전도성 패턴(electrically conductive pattern) 에 의해서 구현될 수 있다. 상기 시스템 버스는 고속 통신을 가능하게 한다. 예를 들어, 상기 복수의 NPU들, 상 기 복수의 CPU들, 상기 복수의 메모리들 그리고 상기 메모리 컨트롤러은 상기 시스템 버스를 통하여 서로 통신할 수 있다. 상기 복수의 NPU들과 그리고 상기 복수의 CPU들은 상기 시스템 버스를 통하여 상기 메모리 컨트롤러 에 요청한다. 이에 메모리 컨트롤러는 상기 복수의 메모리들 중 적어도 하나 이상으로부터 데이터를 읽어 내거나 혹은 기록할 수 있다.도 9에 도시된 상기 클럭 소스와 상기 PMIC 그리고 상기 위상 결정 회로는 도 7a, 도 7b, 그리 고 도 8a에 도시된 것과 동일하다. 따라서, 상기 위상 결정 회로로부터의 제어 신호에 따라 상기 클럭 소스는 상기 원 클럭 신호를 상기 제1 NPU(100-1) 내의 제1 그룹의 PE들에 제공할 수 있고, 아울러 상기 원 클럭 신호의 위상이 한번 지연된 제1 위상 지연 클럭 신호를 상기 제1 NPU(100-1) 내의 제2 그룹의 PE들에게 제공할 수 있다. 또는, 상기 위상 결정 회로로부터의 제어 신호에 따라 상기 클럭 소스는 상기 클럭 소스로부터의 상기 원 클럭 신호의 위상을 특정 위상 차로 지연시킨 제1 위상 지연 클럭 신호를 상기 제1 NPU(100-1) 내의 제1 그룹의 PE들에 제공 할 수 있고, 아울러 상기 원 클럭 신호의 위상이 두 번 지연된 제2 위상 지연 클럭 신호를 상기 제1 NPU(100-1) 내의 제2 그룹의 PE들에게 제공할 수 있다. 마찬가지로, 상기 위상 결정 회로로부터의 제어 신호에 따라 상기 클럭 소스는 상기 원 클럭 신호를 상기 제2 NPU(100-2) 내의 제1 그룹의 PE들에 제공할 수 있고, 아울러 상기 원 클럭 신호의 위상이 한번 지연된 제2 위상 지연 클럭 신호를 상기 제2 NPU(100-2) 내의 제2 그룹의 PE들에게 제공할 수 있다. 또는, 상기 위상 결정 회로로부터의 제어 신호에 따라 상기 클럭 소스는 상기 원 클럭 신호의 위상을 한번 지연시킨 제1 위상 지연 클럭 신호를 상기 제2 NPU(100-2) 내의 제1 그룹의 PE들에 제공할 수 있고, 아울러 상기 원 클럭 신호의 위상이 두 번 지연된 제2 위상 지연 클럭 신호를 상기 제2 NPU(100-2) 내의 제2 그룹의 PE들에게 제공할 수 있다. 도 9에 도시된 상기 SoC 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에는 공급 전압(VDD)이 입력될 수 있 다. 도 9에서는 공통된 공급 전압(VDD)이 입력되는 것으로 도시 되었으나, 본 개시의 예시들은 이에 제한되지 않는 다. 몇몇 예시들에서는, 상기 SoC 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 공급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 다른 예시들에서는, 상기 SoC 내의 일부 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 공 급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 또 다른 예시들에서는, 상기 SoC 내의 특정 엘리먼트에 제1 공급 전압과 상이한 제2 공급 전압이 입력되 도록 구성되는 것도 가능하다. 도 10은 본 개시의 일 예시에 따른 NPU의 동작 방법을 나타낸 예시도이다. 도 10을 참조하면, 복수의 PE들을 구동하기 위한 복수의 클럭 신호가 생성될 수 있다(S1010). 상기 복수의 클럭 신호는 원 클럭 신호와 상기 원 클럭 신호로부터 위상 지연된 복수의 위상 지연 클럭 신호를 포함할 수 있다. 상기 위상 지연을 결정하기 위하여, NPU의 파워가 측정될 수 있다. 상기 파워는 복수의 PE들의 이용율에 따라 변화될 수 있다. 상기 NPU의 피크 파워를 감소시키기 위하여, 상기 측정된 파워에 기초하여 상기 원 클럭 신호 의 위상은 교정(calibrate) 또는 조정될 수 있다. 그리고, 상기 복수의 PE들은 제1 그룹의 PE들과 제2 그룹의 PE들로 분할될 수 있다(S1020). 각 그룹에 속한 PE 들의 개수는 달라질 수 있는 것은 자명할 것이다. 또한, 각 그룹에 속한 PE들의 개수는 반-고정적으로 혹은 동 적으로 변경될 수 있다. 이러한 변경은 NPU 컨트롤러의 제어에 따라 수행될 수 있다. 이어서, 제1 클럭 신호를 기준으로 상기 제1 그룹의 PE들이 구동될 수 있다(S1030). 또한, 제2 클럭 신호를 기 준으로 상기 제2 그룹의 PE들이 구동될 수 있다(S1040). 본 개시의 일 개시에 의하면, NPU(neural processing unit)를 구동하는 방법이 제공된다. 상기 NPU 구동 방법은 상기 NPU의 피크 파워를 감소시키기 위하여, 측정된 파워에 기초하여 하나 또는 복수의 클럭 신호들의 적어도 하나의 위상을 교정(calibrate)하는 단계와; 상기 하나 또는 복수의 클럭 신호들을 출력하는 단계를 포함할 수 있다. 상기 하나 또는 복수의 클럭 신호들은 제1 클럭 신호와 제2 클럭 신호를 포함할 수 있다. 상기 NPU 구동 방법은 상기 제1 클럭 신호에 기초하여 제1 그룹의 PE들(processing elements)을 동작시키는 단계와; 그리고 상 기 제2 클럭 신호에 기초하여 제2 그룹의 PE들을 동작시키는 단계를 포함할 수 있다.도 11은 서로 다른 위상의 클럭 신호를 NPU의 복수의 PE들에게 입력하지 않는 예시를 도시한다. 도 11을 참조하면, 제1 클럭(CLK0)은 제1 그룹의 PE(110-1) 및 제2 그룹의 PE(110-2)에 입력될 수 있다. 여기 서 각각의 그룹의 PE들(110-1, 110-2)은 동일한 위상의 클럭 신호를 입력 받도록 구성된다. 도 11의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 11의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 11의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 11을 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워가 증가하고, 공급 전압(VDD)이 저하되는 경향이 도시되어 있다. 부연 설명하면, 각 클럭 사이클 마다 동작하는 PE들의 개수는 인공신경망모델을 컴파일하는 컴파일러의 성능에 따라 달라질 수 있다. 즉, 컴파일러의 성능이 우수할수록 단위 클럭마다 더 많은 PE들을 동작 시킬 수 있다. 여 기서 전체 PE들 중 동작하는 PE의 비율을 PE의 이용률(utilization rate %)로 지칭할 수 있다. 반대로, 컴파일 러의 성능이 우수할수록 NPU의 피크 파워가 더 증가할 수 있다. 즉, 피크 파워는 PE의 이용률과 정비례할 수 있 다. 따라서 컴파일러의 알고리즘이 정교해짐에 따라, NPU의 피크 파워는 더 증가될 수 있는 문제가 있다. 다시 도 11을 참조하면, 제1 클럭(CLK0)을 기준으로 동작하는 제1 그룹의 PE(110-1) 및 제2 그룹의 PE(110-2)의 피크 파워는 각 클럭당 동작하는 PE의 개수에 따라서 달라질 수 있다. 즉, 하나의 클럭에 동작하는 PE의 개수에 비례하여 피크 파워가 증가될 수 있다. 부연 설명하면, 저전력으로 개발된 NPU는 엣지 디바이스에 적용될 때 M.2 인터페이스 또는 PCI Express 인터페 이스에 대응되도록 개발될 수 있다. 특히 M.2 인터페이스의 경우 PCI Express 인터페이스에 비해서 최대 파워가 상대적으로 더 낮을 수 있다. 예를 들면 M.2 인터페이스의 경우 3A의 전류와 10W의 파워 제한을 가질 수 있다. 만약 비교예의 NPU의 피크 파워가 특정 클럭에서 10W를 초과할 경우, NPU에 공급되는 공급 전압(VDD)이 흔들릴 수 있다. 이때, PE의 이용률이 올라갈수록 NPU의 피크 파워는 더 증가되고, 피크 파워가 파워 제한을 초과할수 록 공급 전압(VDD)의 저하 정도는 초과한 정보에 비례하여 더 커질 수 있다. 만약 공급 전압(VDD)이 IR-drop margin 이하로 떨어질 경우, NPU가 연산 중인 데이터에 오류가 발생할 수 있다. 부연 설명하면, 금속 상호 연결의 저항(R)과 이를 통해 흐르는 전류(I)로 인해 발생하며 옴의 법칙에 따라 전압 강하(V=IR)가 발생될 때 IR-drop이 발생할 수 있다. NPU의 복수의 PE와 관련된 많은 수의 회로가 동시에 스위칭 하면 NPU가 상당한 양의 전류를 소모할 수 있다. 이 높은 전류는 NPU에서 더 큰 전압 강하를 발생시켜 SoC의 다 른 부분에서 공급 전압을 감소시킬 수 있다. 도 11의 비교예를 정리하면, NPU의 안정적인 작동을 보장하고 잠재적인 오류를 방지하려면 충분한 IR-drop margin을 확보해야 한다. IR-drop margin은 공급 전압(VDD)과 피크 파워 조건 동안 NPU의 모든 지점에서 허용 가능한 최저 전압 간의 차이를 의미할 수 있다. 도 12는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 NPU의 복수의 PE들에게 입력하는 예시를 도시한다. 프로세싱 엘리먼트의 이용률이 증가할수록 파워는 비례하여 증가할 수 있다는 점을 주목하여야 한다. 이에, 각 연산 단계별 PE들의 이용률 정보와 구동 주파수 정보를 기초로 각 연산 단계별 파워를 계산할 수 있다. 상기 파 워 계산은 특정 인공신경망모델의 스케줄링 정보를 기초로 계산될 수 있다. 여기서 파워(Watt)는 해당 연산 단 계에 소비되는 에너지를 단위 시간으로 나눈 값(i.e., 1 watt (W) = 1 joule per second (J/s))으로 계산될 수 있다. 따라서 각 연산 단계의 파워를 스케줄링 정보에 기초하여 계산할 수 있다. 도 12를 참조하면, 제1 클럭(CLK1)은 제1 그룹의 PE(110-1)에 입력되고, 제2 클럭(CLK2)은 제2 그룹의 PE(110- 2)에 입력될 수 있다. 여기서 각 그룹의 PE들(110-1, 110-2)은 서로 상이한 위상의 클럭 신호를 입력 받도록 구성된다. 도 12의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 12의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 12의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 12를 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워가 증가하고, 공급 전압(VDD)이 저하되는 경향이 도시되어 있다. 다만, 위상이 서로 다른 신호 클럭이 각 그룹의 PE들에 입력됨에 따라 전체 피크 파워가 저감되고 및 공급 전압(VDD)의 출렁임도 저감되는 경향이 도시되어 있 다. 여기서, 도 12의 일 예시에 따른 평균 파워와 도 11의 비교예에 따른 평균 파워는 실질적으로 동일하다. 하지만, 도 12의 일 예시에 따른 피크 파워는 도 11의 비교예에 따른 평균 파워보다 작게 된다. 따라서, 도 12 의 일 예시에 따르면 도 11의 비교예보다, 피크 파워와 평균 파워의 비율이 개선될 수 있다. 부연 설명하면, 각 클럭 마다 동작하는 PE들의 개수는 인공신경망모델을 컴파일하는 컴파일러의 성능에 따라 달 라질 수 있다. 즉, 컴파일러의 성능이 우수할수록 단위 클럭마다 더 많은 PE들을 동작 시킬 수 있다. 여기서 전 체 PE들 중 동작하는 PE의 비율을 PE의 이용률(utilization rate %)로 지칭할 수 있다. 반대로, 컴파일러의 성 능이 우수할수록 NPU의 피크 파워가 더 증가할 수 있다. 즉, 피크 파워는 PE의 이용률과 정비례할 수 있다. 따 라서 컴파일러의 알고리즘이 정교해짐에 따라, NPU의 피크 파워는 더 증가될 수 있는 문제가 있다. 다시 도 12를 참조하면, 제1 클럭(CLK1)에 대응하는 제1 그룹의 PE(110-1) 및 제2 그룹의 PE(110-2)의 피크 파 워는 각 클럭당 동작하는 PE의 개수에 따라서 달라질 수 있다. 즉, 하나의 클럭에 동작하는 PE의 개수에 비례하 여 피크 파워가 증가될 수 있다. 부연 설명하면, 저전력으로 개발된 NPU는 엣지 디바이스에 적용될 때 M.2 인터페이스 또는 PCI Express 인터페 이스에 대응되도록 개발될 수 있다. 특히 M.2 인터페이스의 경우 PCI Express 인터페이스에 비해서 최대 파워가 상대적으로 더 낮을 수 있다. 예를 들면 M.2 인터페이스의 경우 3A의 전류와 10W의 파워 제한을 가질 수 있다. 만약 비교예의 NPU의 피크 파워가 특정 클럭에서 10W를 초과할 경우, NPU에 공급되는 공급 전압(VDD)이 흔들릴 수 있다. 이때, PE의 이용률이 올라갈수록 NPU의 피크 파워는 더 증가되고, 피크 파워가 파워 제한을 증가할수 록 공급 전압(VDD)의 저하 정도는 더 커질 수 있다. 만약 공급 전압(VDD)이 IR-drop margin 이하로 떨어질 경우, NPU가 연산 중인 데이터에 오류가 발생될 수 있다. 한편, 제1 그룹의 PE(110-1)에는 제1 클럭(CLK1)이 인가되고, 제2 그룹의 PE(110-2)에는 제1 클럭(CLK1)과 위 상이 다른 제2 클럭(CLK2)이 인가될 수 있다. 따라서 각각의 그룹의 PE들의 피크 파워는 분산될 수 있으며, NPU 의 피크 파워 관점에서 피크 파워가 도 11의 비교예 대비 반으로 저감될 수 있다. 이러한 경우 공급 전압(VDD) 의 전압 안정성이 도 11에 도시된 비교예와 비교할 때 상대적으로 더 안정적일 수 있다. 즉, 본 개시의 일 예시에 따르면, NPU의 피크 파워가 저감될 수 있으며, NPU를 포함하는 SoC의 피크 파워 또한 저감될 수 있다. 또한 M.2와 같은 저전력 인터페이스의 경우, 파워 제한이 낮을 수 있기 때문에, 이러한 저전력 인터페이스에서 더욱 더 효과적일 수 있다. 하지만, 본 개시는 특정 인터페이스에 제한되지 않는다. <본 개시의 예시들의 간략 정리> 지금까지 설명한 본 개시의 내용들을 정리하여 설명하면 다음과 같다. 본 개시의 일 예시는 신경 프로세싱 유닛을 제공한다. 신경 프로세싱 유닛은 ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 PE 그룹 들을 위해 배치된, 제1 회로; 상기 제1 회로에 복수의 클럭 신호들을 출력하도록 배치된, 제2 회로; 적어도 상 기 제1 회로의 피크 파워와 평균 파워의 비율을 측정하도록 설정된, 3 회로; 및 상기 제3 회로에서 측정된 피크파워와 평균 파워의 비율에 기초하여, 상기 제2 회로의 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적 으로 교정(calibrate)하도록 배치된, 제4 회로;를 포함할 수 있다. 상기 제3 회로에서 측정한 파워 값이 임계 파워 값보다 높을 경우, 상기 제4 회로는 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 조정(adjust)하도록 구성될 수 있다. 상기 복수의 클럭 신호들의 적어도 하나의 위상은 상기 복수의 PE 그룹들의 이용율에 기초하여 기 설정된 임계 파워 값에 기초하여 조정될 수 있다. 상기 제3 회로는 상기 제1 회로의 제1 파워 값을 측정하고, 상기 제3 회로는 상기 제1 파워 값을 상기 제4 회로 에 제공하고, 상기 제4 회로는 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하고, 이후, 상기 제3 회 로는 상기 제1 회로의 제2 파워 값을 다시 측정하고, 상기 제4 회로는 상기 제2 파워 값이 상기 제1 파워 값보 다 더 작은지를 판단하도록 구성될 수 있다. 상기 제3 회로는 제1 시점의 제1 파워 값을 측정하고, 상기 제4 회로는 상기 제1 시점의 제1 파워 값과 기 설정 된 임계 파워 값을 비교하여 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하도록 구성될 수 있다. 상기 하나 또는 복수의 클럭 신호들은 상기 복수의 PE 그룹들 중에서 제1 그룹의 PE들이 동작하는 제1 클럭 신 호; 및 상기 복수의 PE 그룹들 중에서 제2 그룹의 PE들이 동작하는 제2 클럭 신호를 포함할 수 있다. 상기 복수의 클럭 신호들은 제1 위상을 갖는 제1 클럭 신호; 및 상기 제1 클럭 신호의 제1 위상 보다 늦은 제2 위상을 갖는 제2 클럭 신호를 포함할 수 있다. 상기복수의 클럭 신호들은 원 클럭 신호에 기초하여 생성되는 제1 클럭 신호; 및 상기 원 클럭 신호를 쉬프팅 또는 드리프팅함으로써 생성되는 제2 클럭 신호를 포함할 수 있다. 상기 제2 회로는 원 클럭 신호 소스(source); 체인 형태로 연결되는 하나 또는 복수의 위상 변환기 - 상기 하나 또는 복수의 위상 변환기 중 제1 위상 변환기는 상기 원 클럭 신호 소스에 연결됨-; 및 상기 원 클럭 신호 소스 의 출력 및 상기 하나 또는 복수의 위상 변환기들의 출력들에 연결되어, 상기 원 클럭 신호 소스의 출력 그리고 상기 하나 또는 복수의 위상 변환기들의 출력들 중에서 적어도 하나의 출력을 선택하는 선택기;를 포함할 수 있 다. 본 개시의 일 예시는 시스템-온-칩을 제공한다. 시스템-온-칩은 반도체(semi-conductor) 기판; ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 PE 그룹들을 위해 상기 반도체 기판 상에 배치된, 제1 회로; 상기 제1 회로에 복수의 클럭 신호들을 출력하도록 상기 반도체 기판 상에 배치된, 제2 회로; 적어도 상기 제1 회로의 피크 파워와 평균 파워의 비율을 측정하도록 상기 반도체 기판 상에 배치된, 3 회로; 및 상기 제3 회로에서 측정된 피크 파워와 평균 파워의 비율에 기초하여, 상기 제2 회로의 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 교 정(calibrate)하도록 상기 반도체 기판 상에 배치된, 제4 회로;를 포함할 수 있다. 상기 제3 회로에서 측정한 파워 값이 임계 파워 값보다 높을 경우, 상기 제4 회로는 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 조정(adjust)하도록 구성될 수 있다. 상기 복수의 클럭 신호들의 적어도 하나의 위상은 상기 복수의 PE 그룹들의 이용율에 기초하여 기 설정된 임계 파워 값에 기초하여 조정될 수 있다. 상기 제3 회로는 상기 제1 회로의 제1 파워 값을 측정하고, 상기 제3 회로는 상기 제1 파워 값을 상기 제4 회로 에 제공하고, 상기 제4 회로는 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하고, 이후 상기 제3 회 로는 상기 제1 회로의 제2 파워 값을 다시 측정하고, 상기 제4 회로는 상기 제2 파워 값이 상기 제1 파워 값보 다 더 작은지를 판단하도록 구성될 수 있다. 상기 제3 회로는 제1 시점의 제1 파워 값을 측정하고, 상기 제4 회로는 상기 제1 시점의 제1 파워 값과 기 설정 된 임계 파워 값을 비교하여 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하도록 구성될 수 있다. 상기 하나 또는 복수의 클럭 신호들은 상기 복수의 PE 그룹들 중에서 제1 그룹의 PE들이 동작하는 제1 클럭 신 호; 및 상기 복수의 PE 그룹들 중에서 제2 그룹의 PE들이 동작하는 제2 클럭 신호를 포함할 수 있다. 본 개시의 일 예시는 전자 장치를 제공한다. 전자 장치는 인쇄 회로 기판; ANN(artificial neural network) 모 델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 PE 그룹들을 위해 상기 인쇄 회로 기판 상에 배치된, 제1 회로; 상기 제1 회로에 복수의 클럭 신호들을 출력하도록 상기 인쇄 회로 기판 상에 배치된, 제2 회로; 적어도 상기 제1 회로의 피크 파워와 평균 파워의 비율을 측정하도록 상기 인쇄 회로 기판 상에 배치된, 3 회로; 및 상기 제3 회로에서 측정된 피크 파워와 평균 파워의 비율에 기초 하여, 상기 제2 회로의 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 교정(calibrate)하도록 상기 인쇄 회로 기판 상에 배치된, 제4 회로;를 포함할 수 있다. 상기 제3 회로에서 측정한 파워 값이 임계 파워 값보다 높을 경우, 상기 제4 회로는 상기 복수의 클럭 신호들의 적어도 하나의 위상을 동적으로 조정(adjust)하도록 구성될 수 있다. 상기 복수의 클럭 신호들의 적어도 하나의 위상은 상기 복수의 PE 그룹들의 이용율에 기초하여 기 설정된 임계 파워 값에 기초하여 조정될 수 있다. 상기 제3 회로는 상기 제1 회로의 제1 파워 값을 측정하고, 상기 제3 회로는 상기 제1 파워 값을 상기 제4 회로 에 제공하고, 상기 제4 회로는 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하고, 이후 상기 제3 회 로는 상기 제1 회로의 제2 파워 값을 다시 측정하고, 상기 제4 회로는 상기 제2 파워 값이 상기 제1 파워 값보 다 더 작은지를 판단하도록 구성될 수 있다. 상기 제3 회로는 제1 시점의 제1 파워 값을 측정하고, 상기 제4 회로는 상기 제1 시점의 제1 파워 값과 기 설정 된 임계 파워 값을 비교하여 상기 복수의 클럭 신호들 중 적어도 하나의 위상을 교정하도록 구성될 수 있다. 본 명세서와 도면에 나타난 본 개시의 예시들은 본 개시의 기술 내용을 쉽게 설명하고 본 개시의 이해를 돕기 위해 특정 예를 제시한 것뿐이며, 본 명의 범위를 한정하고자 하는 것은 아니다. 지금까지 설명한 예시들 이외 에도 다른 변형 예들이 실시 가능하다는 것은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 자명 한 것이다."}
{"patent_id": "10-2023-0134879", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 도 2a은 컨볼루션 신경망(CNN)의 기본 구조를 설명하기 위한 도면이다. 도 2b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 3은 본 개시의 일 예시에 따른 신경 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 4a는 본 개시의 일 예시에 적용될 수 있는 복수의 프로세싱 엘리먼트 중 하나의 프로세싱 엘리먼트를 설명하는 개략적인 개념도이다. 도 4b는 본 개시의 일 예시에 적용될 수 있는 SFU를 설명하는 개략적인 개념도이다. 도 5는 도 3에 도시된 신경 프로세싱 유닛의 변형예를 나타낸 예시도이다. 도 6a은 예시적인 인공신경망모델 내의 각 레이어 별 데이터의 크기를 나타낸 예시도이고, 도 6b는 도 6a에 도 시된 예시적 인공신경망모델에서 각 레이어 별 데이터 사이즈 등을 나타낸 예시적인 테이블이다. 도 7a는 본 개시의 일 예시에 따른 NPU의 구조를 나타낸 예시도이다. 도 7b는 본 개시의 다른 예시에 따른 NPU의 구조를 타낸 예시도이다. 도 8a는 도 7a 또는 도 7b에 도시된 클럭 소스를 일 예시에 따라 상세하게 나타낸 예시도이다. 도 8b는 클럭 소스에 의해서 출력되는 복수의 클럭 신호들을 나타낸 예시도이다. 도 9는 본 개시의 일 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 10은 본 개시의 다른 일 예시에 따른 NPU의 동작 방법을 나타낸 예시도이다. 도 11은 서로 다른 위상의 클럭 신호를 NPU의 복수의 PE들에게 입력하지 않는 예시를 도시한다. 도 12는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 NPU의 복수의 PE들에게 입력하는 예시를 도시한다."}
