{"patent_id": "10-2019-0085920", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0009194", "출원번호": "10-2019-0085920", "발명의 명칭": "다중 디코더를 이용한 심화 신경망 기반의 비-자동회귀 음성 합성 방법 및 시스템", "출원인": "한양대학교 산학협력단", "발명자": "장준혁"}}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "다중 디코더 기반의 비재귀적 심화 신경망을 구성하고, 음성의 시간적 정보가 포함된 템플릿(Template)으로부터상기 다중 디코더를 통하여 단계적으로 음성 특징 벡터열을 생성하는 음성 특징 벡터열 합성부; 및 상기 음성 특징 벡터열을 음성 데이터로 변환하는 음성 재구성부를 포함하고, 상기 음성 특징 벡터열 합성부는, 상기 템플릿(Template) 입력을 생성하고, 생성된 상기 템플릿(Template) 입력에 어텐션 메커니즘(attentionmechanism)을 통해 정제된 문장 데이터를 추가하여 음성 특징 벡터열을 생성하는 것을 특징으로 하는, 심화 신경망 기반의 비-자동회귀 음성 합성 시스템."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,문장 데이터를 분석하여 정제된 문장 데이터를 출력하는 문장 데이터 분석부를 더 포함하고, 상기 문장 데이터 분석부는, 상기 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성한 후, 임베딩하여 문장 특징 벡터열형태의 임베딩된 문장 데이터를 형성하고, 상기 임베딩된 문장 데이터를 콘볼루션(convolution) 인공 신경망을이용하여 정제하여 상기 정제된 문장 데이터를 형성하는 것를 포함하는, 심화 신경망 기반의 비-자동회귀 음성 합성 시스템."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 문장 데이터 분석부는, 상기 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성하고, 상기 자모 단위 입력을 색인하여숫자 데이터로 매핑하여, 상기 숫자 데이터를 원-핫 인코딩(One-hot encoding)하고, 원-핫 인코딩(One-hotencoding)된 벡터열을 문장 임베딩 매트릭스와 곱하여 연속된 특성을 가지는 벡터열로 이루어진 상기 임베딩된문장 데이터를 생성하는 것을 특징으로 하는, 심화 신경망 기반의 비-자동회귀 음성 합성 시스템."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 음성 특징 벡터열 합성부는, 상기 템플릿(Template) 입력을 생성하고, 상기 템플릿(Template) 입력에 어텐션 메커니즘(attentionmechanism)을 이용하여 상기 정제된 문장 데이터를 추가하여 인코딩된 템플릿(Template)을 생성한 후, 상기 인코딩된 템플릿(Template)을 디코딩을 통해 멜 필터 뱅크 음성 특징 벡터열을 합성하고, 상기 멜 필터 뱅크 음성특징 벡터열에서 로그 파워 스펙트럼 음성 특징 벡터열을 합성하는 것을 특징으로 하는, 심화 신경망 기반의 비-자동회귀 음성 합성 시스템.공개특허 10-2021-0009194-3-청구항 5 제4항에 있어서,상기 음성 특징 벡터열 합성부는, 상기 정제된 문장 데이터와 상기 템플릿(Template)을 입력으로 받아 상기 어텐션 메커니즘이 로그 파워 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성하고, 다중 디코더를 통해 상기 템플릿(Template)으로부터 단계적으로 로그 파워 스펙트럼을 추정하며, 상기 다중 디코더 중 적어도 어느 하나 이상의 단일 디코더에서 콘볼루션 네트워크와 상기 어텐션 메커니즘을 반복하여 정확한 정보를 담은 상기 템플릿(Template) 데이터를 인코딩하는 것을 특징으로 하는, 심화 신경망 기반의 비-자동회귀 음성 합성 시스템."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 음성 재구성부는, 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 크기(magnitude) 정보를 갖는 상기 음성 특징 벡터열로부터 위상(phase) 정보를 생성하여 상기 음성 데이터로 변환하는 것을 특징으로 하는, 심화 신경망 기반의 비-자동회귀 음성 합성 시스템."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "다중 디코더 기반의 비재귀적 심화 신경망을 구성하고, 음성의 시간적 정보가 포함된 템플릿(Template)으로부터상기 다중 디코더를 통하여 단계적으로 음성 특징 벡터열을 생성하는 음성 특징 벡터열 합성 단계; 및 상기 음성 특징 벡터열을 음성 데이터로 변환하는 음성 재구성 단계를 포함하고, 상기 음성 특징 벡터열 합성 단계는, 상기 템플릿(Template) 입력을 생성하고, 생성된 상기 템플릿(Template) 입력에 어텐션 메커니즘(attentionmechanism)을 통해 정제된 문장 데이터를 추가하여 음성 특징 벡터열을 생성하는 것을 특징으로 하는, 심화 신경망 기반의 비-자동회귀 음성 합성 방법."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,문장 데이터를 분석하여 정제된 문장 데이터를 출력하는 문장 데이터 분석 단계를 더 포함하고, 상기 문장 데이터 분석 단계는, 상기 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성한 후, 임베딩하여 문장 특징 벡터열형태의 임베딩된 문장 데이터를 형성하는 단계; 및 상기 임베딩된 문장 데이터를 콘볼루션(convolution) 인공 신경망을 이용하여 정제하여 상기 정제된 문장 데이터를 형성하는 단계를 포함하는, 심화 신경망 기반의 비-자동회귀 음성 합성 방법."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 문장 특징 벡터열 형태의 임베딩된 문장 데이터를 형성하는 단계는, 공개특허 10-2021-0009194-4-상기 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성하는 단계; 상기 자모 단위 입력을 색인하여 숫자 데이터로 매핑하는 단계; 상기 숫자 데이터를 원-핫 인코딩(One-hot encoding)하는 단계; 및 원-핫 인코딩(One-hot encoding)된 벡터열을 문장 임베딩 매트릭스와 곱하여 연속된 특성을 가지는 벡터열로 이루어진 상기 임베딩된 문장 데이터를 생성하는 단계를 포함하는, 심화 신경망 기반의 비-자동회귀 음성 합성 방법."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서,상기 음성 특징 벡터열 합성부의 입력인 상기 템플릿(Template)은, 절대적인 위치의 인코딩(absolute positional encoding) 데이터와 상대적인 위치의 인코딩(relativepositional encoding) 데이터로 이루어지는 것을 특징으로 하는, 심화 신경망 기반의 비-자동회귀 음성 합성 방법."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제7항에 있어서, 상기 음성 특징 벡터열 합성 단계는, 상기 템플릿(Template) 입력을 생성하는 단계; 상기 템플릿(Template) 입력에 어텐션 메커니즘(attention mechanism)을 이용하여 상기 정제된 문장 데이터를추가하여 인코딩된 템플릿(Template)을 생성하는 음성 데이터 인코딩 단계; 상기 인코딩된 템플릿(Template)을 디코딩을 통해 멜 필터 뱅크 음성 특징 벡터열을 합성하는 음성 데이터 디코딩 단계; 및 상기 멜 필터 뱅크 음성 특징 벡터열에서 로그 파워 스펙트럼 음성 특징 벡터열을 합성하는 단계를 포함하는, 심화 신경망 기반의 비-자동회귀 음성 합성 방법."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 상기 템플릿(Template) 입력을 생성하는 단계는, 절대적인 위치의 인코딩 데이터를 생성하는 단계; 상대적인 위치의 인코딩 데이터를 생성하는 단계; 및생성된 상기 절대적인 위치의 인코딩 데이터와 상기 상대적인 위치의 인코딩 데이터를 병합(concatenate)하여상기 템플릿(Template)을 생성하는 단계를 포함하는, 심화 신경망 기반의 비-자동회귀 음성 합성 방법."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 음성 데이터 인코딩 단계는, 상기 정제된 문장 데이터와 상기 템플릿(Template)을 입력으로 받아 상기 어텐션 메커니즘이 로그 파워 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성하는 단계; 다중 디코더를 통해 상기 템플릿(Template)으로부터 단계적으로 로그 파워 스펙트럼을 추정하는 단계; 및 상기 다중 디코더 중 적어도 어느 하나 이상의 단일 디코더에서 콘볼루션 네트워크와 상기 어텐션 메커니즘을공개특허 10-2021-0009194-5-반복하여 정확한 정보를 담은 상기 템플릿(Template) 데이터를 인코딩하는 단계를 포함하는, 심화 신경망 기반의 비-자동회귀 음성 합성 방법."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 음성 데이터 디코딩 단계는, 음성 데이터 디코딩 인공 신경망을 통해 상기 인코딩된 템플릿(Template)으로부터 멜 필터 뱅크 음성 특징 벡터열을 합성하는 단계를 포함하는, 심화 신경망 기반의 비-자동회귀 음성 합성 방법."}
{"patent_id": "10-2019-0085920", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제7항에 있어서,상기 음성 재구성 단계는, 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 크기(magnitude) 정보를 갖는 상기 음성 특징 벡터열로부터 위상(phase) 정보를 생성하여 상기 음성 데이터로 변환하는 것을 특징으로 하는, 심화 신경망 기반의 비-자동회귀 음성 합성 방법."}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "심화 신경망 기반의 비-자동회귀 음성 합성 방법 및 시스템이 제시된다. 일 실시예에 따른 심화 신경망 기반의 비-자동회귀 음성 합성 시스템은, 다중 디코더 기반의 비재귀적 심화 신경망을 구성하고, 음성의 시간적 정보가 포함된 템플릿(Template)으로부터 상기 다중 디코더를 통하여 단계적으로 음성 특징 벡터열을 생성하는 음성 특 징 벡터열 합성부; 및 상기 음성 특징 벡터열을 음성 데이터로 변환하는 음성 재구성부를 포함하고, 상기 음성 특징 벡터열 합성부는, 상기 템플릿(Template) 입력을 생성하고, 생성된 상기 템플릿(Template) 입력에 어텐션 메커니즘(attention mechanism)을 통해 정제된 문장 데이터를 추가하여 음성 특징 벡터열을 생성할 수 있다."}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명의 실시예들은 다중 디코더를 이용한 심화 신경망 기반의 비-자동회귀 음성 합성 방법 및 시스템에 관한 것으로, 더욱 상세하게는 다중 디코더 기반의 비재귀적 심화 신경망을 구성하여 음성의 시간적 정보가 담긴 입 력으로부터 음성 특징 벡터를 생성하는 음성 합성 모델 생성 방법 및 시스템에 관한 것이다. 이 발명은 2017년 도 정부(과학기술정보통신부)의 재원으로 정보통신기술진흥센터의 지원을 받아 수행된 연구임(No.2017-0-00474, AI스피커 음성비서를 위한 지능형 음성신호처리 기술개발)."}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "심화 신경망(Deep Neural Network, DNN) 기반의 음성 합성 기술은 심화 신경망을 이용하여 문장 데이터에서 음 성 데이터를 생성해내는 기술로, 일반적으로 문장 데이터를 분석하는 문장 데이터 분석부와 음성 특징 벡터를 생성하는 음성 특징 벡터열 합성부가 하나의 네트워크로 구성된다. 첫 번째 단계인 문장 데이터 분석부는 문장 데이터를 자모로 분리하고 분리된 자모를 신경망 입력으로 유효한 벡터열로 바꾸는 문장 데이터 임베딩(character embedding) 부분, 임베딩된 벡터열에서 콘볼루션(convolution) 신경망과 순환 신경망으로 구성된 네트워크가 음성 특징 벡터 생성에 필요한 정보를 정제하는 부분으로 나뉜다. 두 번째 단계인 음성 특징 벡터열 합성부 또한 두 개의 단계로 이루어지며, 첫 번째 단계에서는 어텐션 메커니 즘(attention mechanism)으로 정제된 문장 데이터 벡터열에서 음성 데이터에 맞는 정보를 선택적으로 취합한 후, 취합된 정보를 바탕으로 순환 신경망이 멜 필터 뱅크(Mel-Filterbank) 음성 특징 벡터를 생성한다. 이때 순 환 신경망의 입력은 이전 단계에서 생성된 출력 멜 필터 뱅크 음성 데이터를 자동회귀(autoregressive) 방식에 따라 입력으로 구성하게 된다. 두 번째 단계에서는 멜 필터 뱅크 음성 데이터를 로그 파워 스펙트로그램(log- power spectrogram)으로 매핑한다. 일반적으로 음성 합성의 품질은 주관적 평가 방법인 M.O.S(Mean Opinion Score)를 이용하여 측정한다. 일부 보 코더(vocoder) 모델의 경우에는 음성의 왜곡 정도를 측정하지만, 심화 신경망(DNN) 기반의 엔드투엔드(end to end) 모델의 경우에는 M.O.S만을 사용한다. 심화 신경망(DNN) 기반의 엔드투엔드(end to end) 음성 합성 기술은 하나의 심화 신경망(DNN) 모델이 문장 데이 터를 분석하여 스펙트로그램 기반의 음성 특징 또는 음성 신호를 생성해내는 기술을 말한다. 문장 데이터와 음 성 데이터의 샘플링 비율(sampling rate)이 서로 다르기 때문에 이를 해결하기 위해 seq2seq(sequence-to- sequence) 네트워크와 어텐션 메커니즘(attention mechanism)이 사용된다. seq2seq 네트워크는 인코더(encoder)와 디코더(decoder)로 구성되어 있다. 인코더는 문장 데이터를 정제하는 역할을 하며, 디코더는 인코 더에 의해 정제된 정보를 바탕으로 스펙트로그램 기반의 음성 특징을 생성해낸다. 디코더는 생성된 출력물이 다 음 시간의 입력이 되는 자동회귀 플로우(autoregressive flow)를 바탕으로 순차적으로 출력을 생성해낸다. 그러 나, 자동회귀 플로우(autoregressive flow)는 효율적인 정보의 전달을 가능하게 하지만 시간 순서에 따라 순차 적으로 출력을 생성해야 하기 때문에 속도가 느리다는 단점이 있다. 이와 같이, 기존의 신경망 기반 음성 합성 기술은 자동회귀(autoregressive) 방식으로 이전 단계에서 생성된 멜 필터 뱅크 음성 특징 벡터를 다시 현재 단계의 입력으로 사용하는 방식으로 음성 특징 벡터를 합성한다. 학습 단계에서는 이미 주어진 타겟 음성 특징 벡터열을 시프팅하여 사용하는 방식으로 입력을 구성하기 때문에 이전 단계 출력을 기다릴 필요가 없지만, 테스트 단계에서는 단계적으로 출력을 생성해야 되기 때문에 생성 속도가 느린 단점이 있다. 자동회귀(autoregressive) 음성 합성 모델의 경우 출력이 입력으로 다시 들어가기 때문에 순환 신경망 특성상 뒤쪽으로 갈수록 음성의 소리 크기가 작아지는 현상이 빈번하다. 또한 음성 생성시에 음성의 끝을 따로 찾는 구 조가 필요하며, 끝부분을 제대로 찾지 못하면 중간에 음성의 생성을 멈춰버리는 현상이 나타난다. 선행기술문헌 비특허문헌 (비특허문헌 0001) Wang Y, Skerry-Ryan RJ, Stanton D, Wu Y, Weiss RJ, Jaitly N, Yang Z, Xiao Y, Chen Z, Bengio S, Le Q. Tacotron: A fully end-to-end text-to-speech synthesis model. arXiv preprint arXiv:1703.10135. 2017 Mar 29."}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 실시예들은 심화 신경망 기반의 비-자동회귀 음성 합성 방법 및 시스템에 관하여 기술하며, 보다 구 체적으로 음성 특징 벡터의 길이를 추정하고 빈 입력을 생성하여 비재귀적으로 음성 벡터를 생성하는 음성 합성 모델 생성 기술을 제공한다. 본 발명의 실시예들은 기존의 자동회귀(autoregressive) 음성 합성 기술이 가지는 문제를 해결하기 위하여, 기 존의 자동회귀(autoregressive) 음성 합성 기술의 자동회귀 플로우(autoregressive flow)를 제거하고, 템플릿 (Template)으로 불리는 새로운 입력을 구성하여 테스트 단계에서도 학습 단계와 같은 방식으로 음성을 생성할 수 있도록 하는 심화 신경망 기반의 비-자동회귀 음성 합성 방법 및 시스템을 제공할 수 있다. 본 발명은 템플릿(Template)이라는 새로운 입력을 이용하여 음성 합성 모델이 단계적으로 음성 특징 벡터를 생 성해야 하는 문제를 해결할 수 있는 방법을 제공하는 것을 그 목적으로 한다. 하지만, 템플릿(Template)은 빈 입력에 음성의 오직 시간적 정보만 포함된 입력으로, 템플릿(Template)으로부터 멜 필터 뱅크 음성특징을 추정하는 것은 기존의 자동회귀(autoregressive) 방식에서 이전 단계에서 생성된 멜 필터 뱅크 음성특징으로부터 현재 단계의 멜 필터 뱅크 음성특징을 추정하는 것과 달리, 입출력의 도메인의 차 이가 커서 기존의 음성합성 방법만으로는 한계가 있다. 본 발명은 템플릿(Template)으로부터 음성 합성 모델이 다중 디코더를 통하여 단계적으로 음성 특징 벡터를 생성하도록 구현하여 비-자동회귀(non-autoregressive) 방 식에서 음질이 크게 저하되는 문제를 해결할 수 있는 방법을 제공하는 것을 그 목적으로 한다."}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 심화 신경망 기반의 비-자동회귀 음성 합성 시스템은, 다중 디코더 기반의 비재귀적 심화 신 경망을 구성하고, 음성의 시간적 정보가 포함된 템플릿(Template)으로부터 상기 다중 디코더를 통하여 단계적으 로 음성 특징 벡터열을 생성하는 음성 특징 벡터열 합성부; 및 상기 음성 특징 벡터열을 음성 데이터로 변환하 는 음성 재구성부를 포함하고, 상기 음성 특징 벡터열 합성부는, 상기 템플릿(Template) 입력을 생성하고, 생성 된 상기 템플릿(Template) 입력에 어텐션 메커니즘(attention mechanism)을 통해 정제된 문장 데이터를 추가하 여 음성 특징 벡터열을 생성할 수 있다. 문장 데이터를 분석하여 정제된 문장 데이터를 출력하는 문장 데이터 분석부를 더 포함하고, 상기 문장 데이터 분석부는, 상기 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성한 후, 임베딩하여 문장 특 징 벡터열 형태의 임베딩된 문장 데이터를 형성하고, 상기 임베딩된 문장 데이터를 콘볼루션(convolution) 인공 신경망을 이용하여 정제하여 상기 정제된 문장 데이터를 형성할 수 있다. 상기 문장 데이터 분석부는, 상기 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성하고, 상 기 자모 단위 입력을 색인하여 숫자 데이터로 매핑하여, 상기 숫자 데이터를 원-핫 인코딩(One-hot encoding)하 고, 원-핫 인코딩(One-hot encoding)된 벡터열을 문장 임베딩 매트릭스와 곱하여 연속된 특성을 가지는 벡터열 로 이루어진 상기 임베딩된 문장 데이터를 생성할 수 있다. 상기 음성 특징 벡터열 합성부는, 상기 템플릿(Template) 입력을 생성하고, 상기 템플릿(Template) 입력에 어텐 션 메커니즘(attention mechanism)을 이용하여 상기 정제된 문장 데이터를 추가하여 인코딩된 템플릿(Templat e)을 생성한 후, 상기 인코딩된 템플릿(Template)을 디코딩을 통해 멜 필터 뱅크 음성 특징 벡터열을 합성하고, 상기 멜 필터 뱅크 음성 특징 벡터열에서 로그 파워 스펙트럼 음성 특징 벡터열을 합성할 수 있다. 상기 음성 특징 벡터열 합성부는, 상기 정제된 문장 데이터와 상기 템플릿(Template)을 입력으로 받아 상기 어 텐션 메커니즘이 로그 파워 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성하고, 다중 디 코더를 통해 상기 템플릿(Template)으로부터 단계적으로 로그 파워 스펙트럼을 추정하며, 상기 다중 디코더 중 적어도 어느 하나 이상의 단일 디코더에서 콘볼루션 네트워크와 상기 어텐션 메커니즘을 반복하여 정확한 정보 를 담은 상기 템플릿(Template) 데이터를 인코딩할 수 있다. 상기 음성 재구성부는, 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 크기(magnitude) 정보를 갖는 상기 음성 특징 벡터열로부터 위상(phase) 정보를 생성하여 상기 음성 데이터로 변환할 수 있다. 다른 실시예에 따른 심화 신경망 기반의 비-자동회귀 음성 합성 방법은, 다중 디코더 기반의 비재귀적 심화 신 경망을 구성하고, 음성의 시간적 정보가 포함된 템플릿(Template)으로부터 상기 다중 디코더를 통하여 단계적으 로 음성 특징 벡터열을 생성하는 음성 특징 벡터열 합성 단계; 및 상기 음성 특징 벡터열을 음성 데이터로 변환 하는 음성 재구성 단계를 포함하고, 상기 음성 특징 벡터열 합성 단계는, 상기 템플릿(Template) 입력을 생성하 고, 생성된 상기 템플릿(Template) 입력에 어텐션 메커니즘(attention mechanism)을 통해 정제된 문장 데이터를 추가하여 음성 특징 벡터열을 생성할 수 있다. 문장 데이터를 분석하여 정제된 문장 데이터를 출력하는 문장 데이터 분석 단계를 더 포함하고, 상기 문장 데이 터 분석 단계는, 상기 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성한 후, 임베딩하여 문 장 특징 벡터열 형태의 임베딩된 문장 데이터를 형성하는 단계; 및 상기 임베딩된 문장 데이터를 콘볼루션 (convolution) 인공 신경망을 이용하여 정제하여 상기 정제된 문장 데이터를 형성하는 단계를 포함할 수 있다. 상기 문장 특징 벡터열 형태의 임베딩된 문장 데이터를 형성하는 단계는, 상기 문장 데이터를 한글의 자모 단위 로 분해하여 자모 단위 입력을 생성하는 단계; 상기 자모 단위 입력을 색인하여 숫자 데이터로 매핑하는 단계; 상기 숫자 데이터를 원-핫 인코딩(One-hot encoding)하는 단계; 및 원-핫 인코딩(One-hot encoding)된 벡터열 을 문장 임베딩 매트릭스와 곱하여 연속된 특성을 가지는 벡터열로 이루어진 상기 임베딩된 문장 데이터를 생성 하는 단계를 포함할 수 있다. 여기서, 상기 음성 특징 벡터열 합성부의 입력인 상기 템플릿(Template)은 절대적인 위치의 인코딩(absolute positional encoding) 데이터와 상대적인 위치의 인코딩(relative positional encoding) 데이터로 이루어질 수 있다. 상기 음성 특징 벡터열 합성 단계는, 상기 템플릿(Template) 입력을 생성하는 단계; 상기 템플릿(Template) 입 력에 어텐션 메커니즘(attention mechanism)을 이용하여 상기 정제된 문장 데이터를 추가하여 인코딩된 템플릿 (Template)을 생성하는 음성 데이터 인코딩 단계; 상기 인코딩된 템플릿(Template)을 디코딩을 통해 멜 필터 뱅 크 음성 특징 벡터열을 합성하는 음성 데이터 디코딩 단계; 및 상기 멜 필터 뱅크 음성 특징 벡터열에서 로그 파워 스펙트럼 음성 특징 벡터열을 합성하는 단계를 포함할 수 있다. 또한, 상기 템플릿(Template) 입력을 생성하는 단계는, 절대적인 위치의 인코딩 데이터를 생성하는 단계; 상대 적인 위치의 인코딩 데이터를 생성하는 단계; 및 생성된 상기 절대적인 위치의 인코딩 데이터와 상기 상대적인 위치의 인코딩 데이터를 병합(concatenate)하여 상기 템플릿(Template)을 생성하는 단계를 포함할 수 있다. 상기 음성 데이터 인코딩 단계는, 상기 정제된 문장 데이터와 상기 템플릿(Template)을 입력으로 받아 상기 어 텐션 메커니즘이 로그 파워 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성하는 단계; 다 중 디코더를 통해 상기 템플릿(Template)으로부터 단계적으로 로그 파워 스펙트럼을 추정하는 단계; 및 상기 다 중 디코더 중 적어도 어느 하나 이상의 단일 디코더에서 콘볼루션 네트워크와 상기 어텐션 메커니즘을 반복하여 정확한 정보를 담은 상기 템플릿(Template) 데이터를 인코딩하는 단계를 포함할 수 있다. 상기 음성 데이터 디코딩 단계는, 음성 데이터 디코딩 인공 신경망을 통해 상기 인코딩된 템플릿(Template)으로 부터 멜 필터 뱅크 음성 특징 벡터열을 합성하는 단계를 포함할 수 있다. 상기 음성 재구성 단계는, 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 크기(magnitude) 정보를 갖 는 상기 음성 특징 벡터열로부터 위상(phase) 정보를 생성하여 상기 음성 데이터로 변환할 수 있다."}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들에 따르면 비-자동회귀(non-autoregressive) 방식으로 음성 특징 벡터열을 한번에 합성하기 때문에 자동회귀(autoregressive) 음성 합성 방식에 비해 빠른 속도로 음성을 합성할 수 있는 심화 신경망 기반 의 비-자동회귀 음성 합성 방법 및 시스템을 제공할 수 있다. 또한, 본 발명의 실시예들에 따르면 자동회귀(autoregressive) 기반 모델 회귀(regression) 모델에서 출력의 크 기가 점점 감소하는 현상이 나타나지 않아 음성의 크기가 문장 전체에서 일정하게 유지되는 이점이 있다. 또한, 본 발명의 실시예들에 따르면 음성 특징벡터를 프레임별로 순차적으로 생성하지 않고, 비-자동회귀(non- autoregressive) 방식으로 전체 프레임의 음성 특징벡터를 한번에 합성하기 때문에 기존의 자동회귀 (autoregressive) 방식보다 빠른 속도로 음성을 합성할 수 있다."}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 실시예들을 설명한다. 그러나, 기술되는 실시예들은 여러 가지 다른 형태로 변 형될 수 있으며, 본 발명의 범위가 이하 설명되는 실시예들에 의하여 한정되는 것은 아니다. 또한, 여러 실시"}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "예들은 당해 기술분야에서 평균적인 지식을 가진 자에게 본 발명을 더욱 완전하게 설명하기 위해서 제공되는 것 이다. 도면에서 요소들의 형상 및 크기 등은 보다 명확한 설명을 위해 과장될 수 있다. 아래의 본 발명의 실시예들은 음성 특징 벡터의 길이를 추정하고 빈 입력을 생성하여 비재귀적으로 음성 벡터를 생성하는 심화 신경망 기반의 비-자동회귀 음성 합성 방법 및 시스템에 관한 것이다. 실시예들은 기존의 자동회 귀(autoregressive) 음성 합성 시스템의 자동회귀 플로우(autoregressive flow)를 제거하고, 템플릿(Templat e)으로 불리는 새로운 입력을 구성하여 테스트 단계에서도 학습 단계와 같은 방식으로 음성을 생성할 수 있도록 한다. 실시예들에 따르면 비-자동회귀(non-autoregressive) 방식으로 음성 특징 벡터열을 한번에 합성함으로써 자동회 귀(autoregressive) 음성 합성 방식에 비해 빠른 속도로 음성을 합성할 수 있다. 도 1은 일 실시예에 따른 비-자동회귀 음성 합성 시스템의 전체적인 구성을 나타내는 도면이다. 도 1을 참조하면, 일 실시예에 따른 심화 신경망 기반의 비-자동회귀(non-autoregressive) 음성 합성 시스템 은 문장 데이터를 분석하는 문장 데이터 분석부, 음성 특징 벡터열을 합성해내는 음성 특 징 벡터열 합성부, 그리고 음성 특징 벡터열을 음성으로 변환하는 음성 재구성부를 포함하여 이 루어질 수 있다. 문장 데이터 분석부는 문장 데이터를 분석하여 정제된 문장 데이터를 출력할 수 있다. 이러한 문장 데이터 분석부는 한글 자모 단위로 들어오는 문장 입력 데이터를 심화 신경망의 입력으로 변경하는 문장 데이터 임베딩(character embedding) 부분과, 임베딩된 데이터를 정제하는 인공 신경망 부분으로 나뉘어질수 있다. 보다 구체적으로, 문장 데이터 임베딩 부분은 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성한 후, 임베딩하여 문장 특징 벡터열 형태의 임베딩된 문장 데이터를 형성하고, 인공 신경망 부분은 임베딩된 문장 데이터를 콘볼루션(convolution) 인공 신경망을 이용하여 정제하여 정제된 문장 데이터 를 형성할 수 있다. 특히, 문장 데이터 분석부는 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성하고, 자모 단위 입력을 색인하여 숫자 데이터로 매핑하여, 숫자 데이터를 원-핫 인코딩(One-hot encodin g)하고, 원-핫 인코딩(One-hot encoding)된 벡터열을 문장 임베딩 매트릭스와 곱하여 연속된 특성을 가지는 벡 터열로 이루어진 임베딩된 문장 데이터를 생성할 수 있다. 음성 특징 벡터열 합성부는 다중 디코더 기반의 비재귀적 심화 신경망을 구성하고, 음성의 시간적 정보가 포함된 템플릿(Template, 103)으로부터 다중 디코더를 통하여 단계적으로 음성 특징 벡터열을 생성할 수 있다. 보다 구체적으로, 음성 특징 벡터열 합성부는 템플릿(Template, 103) 입력을 생성하고, 생성된 템플 릿(Template, 103)에 어텐션 메커니즘(attention mechanism)을 이용하여 정제된 문장 데이터를 추가하여 음성 특징 벡터열을 생성할 수 있다. 여기서, 음성 특징 벡터열 합성부의 입력인 템플릿(Template, 103)은 절대적인 위치의 인코딩(absolute positional encoding) 데이터와 상대적인 위치의 인코딩(relative positional encoding) 데이터로 이루어질 수 있다. 또한, 음성 특징 벡터열 합성부는 음성 데이터 인코딩부와 음성 데이터 디코딩부 구성될 수 있다. 음성 특징 벡터열 합성부는 템플릿(Template, 103) 입력을 생성하고, 음성 데이터 인코딩부를 통해 템플릿 (Template, 103) 입력에 어텐션 메커니즘(attention mechanism)을 이용하여 정제된 문장 데이터를 추가하 여 인코딩된 템플릿(Template)을 생성할 수 있다. 이 후, 음성 데이터 디코딩부는 인코딩된 템플릿(Template)을 디코딩을 통해 멜 필터 뱅크 음성 특징 벡터열을 합성할 수 있다. 그리고, 멜 필터 뱅크 음성 특징 벡터열에서 로그 파워 스펙트럼 음성 특징 벡터열을 합성할 수 있다. 음성 특징 벡터열 합성부는 정제된 문장 데이터와 템플릿(Template, 103)을 입력으로 받아 어텐션 메 커니즘이 로그 파워 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성할 수 있다. 이 후, 음 성 특징 벡터열 합성부는 다중 디코더를 통해 템플릿(Template, 103)으로부터 단계적으로 로그 파워 스펙 트럼을 추정하며, 다중 디코더 중 적어도 어느 하나 이상의 단일 디코더에서 콘볼루션 네트워크와 어텐션 메커 니즘을 반복하여 정확한 정보를 담은 템플릿(Template, 103) 데이터를 인코딩할 수 있다. 음성 재구성부는 음성 특징 벡터열을 음성 데이터로 변환할 수 있다. 보다 구체적으로, 음성 재 구성부는 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 크기(magnitude) 정보를 갖는 음성 특 징 벡터열로부터 위상(phase) 정보를 생성하여 음성 데이터로 변환할 수 있다. 도 2 내지 도 4는 일 실시예에 따른 비-자동회귀 음성 합성 방법을 나타내는 흐름도이다. 도 2를 참조하면, 일 실시예에 따른 심화 신경망 기반의 비-자동회귀(non-autoregressive) 음성 합성 방법은, 문장 데이터를 분석하여 정제된 문장 데이터를 출력하는 문장 데이터 분석 단계(S110), 다중 디코더 기반의 비 재귀적 심화 신경망을 구성하고, 음성의 시간적 정보가 포함된 템플릿(Template)으로부터 다중 디코더를 통하여 단계적으로 음성 특징 벡터열을 생성하는 음성 특징 벡터열 합성 단계(S120), 및 음성 특징 벡터열을 음성 데이 터로 변환하는 음성 재구성 단계(S130)를 포함하여 이루어질 수 있다. 도 3을 참조하면, 문장 데이터 분석 단계(S110)는 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력 을 생성한 후, 임베딩하여 문장 특징 벡터열 형태의 임베딩된 문장 데이터를 형성하는 단계(S111)와, 임베딩된 문장 데이터를 콘볼루션(convolution) 인공 신경망을 이용하여 정제하여 정제된 문장 데이터를 형성하는 단계 (S112)를 포함할 수 있다. 도 4를 참조하면, 음성 특징 벡터열 합성 단계(S120)는, 템플릿(Template) 입력을 생성하는 단계(S121), 템플릿 (Template) 입력에 어텐션 메커니즘(attention mechanism)을 이용하여 정제된 문장 데이터를 추가하여 인코딩된 템플릿(Template)을 생성하는 음성 데이터 인코딩 단계(S122), 인코딩된 템플릿(Template)을 디코딩을 통해 멜 필터 뱅크 음성 특징 벡터열을 합성하는 음성 데이터 디코딩 단계(S123), 및 멜 필터 뱅크 음성 특징 벡터열에 서 로그 파워 스펙트럼 음성 특징 벡터열을 합성하는 단계(S124)를 포함할 수 있다. 아래에서 일 실시예에 따른 비-자동회귀 음성 합성 방법의 각 단계를 보다 상세히 설명하기로 한다. 일 실시예에 따른 심화 신경망 기반의 비-자동회귀 음성 합성 방법은 도 1에서 설명한 일 실시예에 따른 심화 신경망 기반의 비-자동회귀 음성 합성 시스템을 예를 들어 보다 상세히 설명할 수 있다. 여기서, 일 실시예에 따른 심화 신경망 기반의 비-자동회귀 음성 합성 시스템은 문장 데이터 분석부, 음성 특징 벡터열 합성부 및 음 성 재구성부를 포함하여 이루어질 수 있다. 문장 데이터 분석 단계(S110)에서, 문장 데이터 분석부는 문장 데이터를 분석하여 정제된 문장 데이터를 출력할 수 있다. 여기서, 문장 데이터 분석 단계(S110)는 하나의 인공 신경망으로 정제하여 정제된 문장 데이터를 생성 할 수 있다. 이때 인공 신경망을 문장 데이터 정제 인공 신경망이라 할 수 있으며, 이를 학습시킬 수 있다. 이러한 문장 데이터 분석 단계(S110)는 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성한 후, 임베딩하여 문장 특징 벡터열 형태의 임베딩된 문장 데이터를 형성하는 단계(S111)와, 임베딩된 문장 데이 터를 콘볼루션(convolution) 인공 신경망을 이용하여 정제하여 정제된 문장 데이터를 형성하는 단계(S112)를 포 함할 수 있다. 여기서, 임베딩은 문장 등의 불연속적인 기호 데이터를 연속적이고 다양한 특성을 가지는 특징 벡터로 변환하기 위한 과정이다. 임베딩된 문장 데이터를 형성하는 단계(S111)는, 문장 데이터를 한글의 자모 단위로 분해하여 자모 단위 입력을 생성하는 단계(문장 분해 단계), 자모 단위 입력을 색인하여 숫자 데이터로 매핑하는 단계(색인 단계), 숫자 데 이터를 원-핫 인코딩(One-hot encoding)하는 단계 및 원-핫 인코딩(One-hot encoding)된 벡터열을 문장 임베딩 매트릭스와 곱하여 연속된 특성을 가지는 벡터열로 이루어진 임베딩된 문장 데이터를 생성하는 단계(특징 벡터 변환 단계)를 포함할 수 있다. 문장 분해 단계는 한글 문장을 자모로 쪼개는 단계이고, 색인 단계는 쪼개진 자모에 각각 번호를 매기는 단계로 각 자모별 번호가 일대일 대응되는 색인표를 구성하고 색인표에 따라 번호를 매기게 된다. 이러한 두 단계를 거 치면 문장 데이터가 숫자 데이터로 변하게 된다. 생성된 문장 특징 벡터열(즉, 임베딩된 문장 데이터)은 다음 식과 같이 나타낼 수 있다. [수학식 1]"}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "특징 벡터 변환 단계에서는 생성된 숫자 데이터를 원-핫 인코딩(One-hot encoding)한 후, 생성된 원-핫(One- hot) 벡터( )를 수학식 2와 같이 임베딩 매트릭스( )와 각각 곱하여 특징 벡터로 변환할 수 있다. 이에 따라 벡터열로 이루어진 임베딩된 문장 데이터를 생성할 수 있다. [수학식 2]"}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "음성 특징 벡터열 합성 단계(S120)에서, 음성 특징 벡터열 합성부는 다중 디코더 기반의 비재귀적 심화 신경망 을 구성하고, 음성의 시간적 정보가 포함된 템플릿(Template)으로부터 다중 디코더를 통하여 단계적으로 음성 특징 벡터열을 생성할 수 있다. 보다 구체적으로, 음성 특징 벡터열 합성부는 템플릿(Template) 입력을 생성하 고, 생성된 템플릿(Template)에 어텐션 메커니즘(attention mechanism)을 이용하여 정제된 문장 데이터를 추가 하여 음성 특징 벡터열을 생성할 수 있다. 음성 특징 벡터열 합성 단계(S120)는, 템플릿(Template) 입력을 생성하는 단계(S121), 템플릿(Template) 입력에 어텐션 메커니즘(attention mechanism)을 이용하여 정제된 문장 데이터를 추가하여 인코딩된 템플릿(Template) 을 생성하는 음성 데이터 인코딩 단계(S122), 인코딩된 템플릿(Template)을 디코딩을 통해 멜 필터 뱅크 음성 특징 벡터열을 합성하는 음성 데이터 디코딩 단계(S123), 및 멜 필터 뱅크 음성 특징 벡터열에서 로그 파워 스 펙트럼 음성 특징 벡터열을 합성하는 단계(S124)를 포함할 수 있다. 또한, 템플릿(Template) 입력을 생성하는 단계(S121)는 절대적인 위치의 인코딩 데이터를 생성하는 단계, 상대 적인 위치의 인코딩 데이터를 생성하는 단계, 및 생성된 절대적인 위치의 인코딩 데이터와 상대적인 위치의 인 코딩 데이터를 병합(concatenate)하여 템플릿(Template)을 생성하는 단계를 포함할 수 있다. 그리고, 음성 데이터 인코딩 단계(S122)는 정제된 문장 데이터와 템플릿(Template)을 입력으로 받아 어텐션 메 커니즘이 로그 파워 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성하는 단계와, 정제된 문장 데이터와 템플릿(Template)을 입력으로 받아 어텐션 메커니즘이 로그 파워 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성하는 단계와, 다중 디코더를 통해 템플릿(Template)으로부터 단계적으로 로 그 파워 스펙트럼을 추정하는 단계, 그리고 다중 디코더 중 적어도 어느 하나 이상의 단일 디코더에서 콘볼루션 네트워크와 어텐션 메커니즘을 반복하여 정확한 정보를 담은 템플릿(Template) 데이터를 인코딩하는 단계를 포 함할 수 있다. 이때 음성 데이터 인코딩 인공 신경망을 학습시킬 수 있으며, 음성 데이터 인코딩 인공 신경망은 콘볼루션(convolution) 인공 신경망이 될 수 있다. 또한, 음성 데이터 디코딩 단계(S123)는 음성 데이터 디코딩 인공 신경망을 통해 인코딩된 템플릿(Template)으 로부터 멜 필터 뱅크 음성 특징 벡터열을 합성하는 단계를 포함할 수 있다. 이때 음성 데이터 디코딩 인공 신경 망을 학습시킬 수 있다. 앞에서 언급한 바와 같이, 음성 특징 벡터열 합성부는 전체 모델의 인코더에 해당하며, 음성 특징 벡터열 합성 부의 입력인 템플릿(Template)(또는 템플릿(Template) 데이터)은 상대적인 위치의 인코딩(relative positional encoding)과 절대적인 위치의 인코딩(absolute positional encoding)으로 구성될 수 있다. 각 위치의 인코딩 (positional encoding)은 멜 필터 뱅크 음성 특징 벡터열과 같은 차원으로 구성될 수 있다. 즉, [시간, 주파수 빈]의 크기를 가지는 행렬이다. 절대적인 위치의 인코딩의 구성을 다음 식과 같이 나타낼 수 있다. [수학식 3] ), ) 절대적인 위치의 인코딩은 세 개의 파라미터를 가지는 sin, cos 파형으로 구성될 수 있다. 이때, pos는 데이터 내의 시간 순서에 따라 1부터 커지는 자연수이며, 는 데이터 내의 주파수 빈의 순서에 따라 1부터 커지는 자연수이다. 은 멜 필터 뱅크의 주파수 빈 개수와 같은 80이다. 시간에 따라 변하면서도 중복되지 않는 sin 파와 cos 파는 데이터에 시간 정보를 더해주며, 순환 신경망과 달리 순서 정보의 학습이 제한적인 콘볼루션 신경망이 순서, 시간 정보를 결과물에 반영할 수 있게 도와준다. 상대적인 위치의 인코딩의 구성은 다음 식과 같이 나타낼 수 있다. [수학식 4] ), ) 상대적인 위치의 인코딩은 절대적인 위치의 인코딩과 유사하지만, 파라미터 대신에 전체 음성 데이터의 길이인 파라미터가 포함될 수 있다. 파라미터가 위치의 인코딩의 구성에 포함되기 때문 에 절대적인 위치의 인코딩과 다르게 전체 길이에 대한 상대적인 시간 정보를 표현할 수 있게 된다. 각 위치의 인코딩은 병합(concatenate)되어 템플릿(Template)을 구성하게 된다. 따라서 전체 템플릿(Template) 은 [시간, 주파수 빈*2]의 크기를 가지는 행렬이다. 도 5는 일 실시예에 따른 멜 필터 뱅크 음성 특징 벡터열 생성 과정을 설명하기 위한 도면이다. 도 5를 참조하면, 심화 신경망 기반의 음성 합성 시스템의 음성 특징 벡터열 합성부의 멜 필터 뱅크 음성 특징 벡터열 생성 과정을 확인할 수 있다. 음성 특징 벡터열 합성부는 전체 모델의 디코더에 해당하며, 이는 음성 데이터 인코딩부와 음성 데이 터 디코딩부로 구성될 수 있다. 음성 데이터 인코딩부는 다수의 콘볼루션 네트워크와 어텐션 메커니 즘으로 구성될 수 있다. 각 어텐션 메커니즘을 거치며 템플릿(Template, 202)은 멜 필터 뱅크 음성 특징 벡터열 을 생성하기 위한 문장 정보를 적절하게 담고 있는 새로운 정보로 인코딩될 수 있다. 일반적으로 음성 특징 벡터열과 문장 데이터 사이에는 샘플링 비율의 차이가 존재한다. 문장 데이터에 비해 음 성 특징 벡터열의 샘플링 비율이 일반적으로 훨씬 크다. 템플릿(Template, 202)에 문장 데이터에 대한 정보를추가해줄 때 이러한 샘플링 비율의 차이로 인해 템플릿(Template, 202)의 각 시간당 어떤 문장 데이터를 추가해 줄 것인가에 관한 문제가 생기게 된다. 어텐션 메커니즘은 일반적인 자동회귀(autoregressive) 음성 합성 시스 템에서 이러한 문제를 해결하는데 있어 큰 효과를 보여왔다. 본 발명 또한 어텐션 메커니즘을 이용하여 정 제된 문장 데이터를 템플릿(Template, 202)에 적절하게 추가해준다. 하지만 자동회귀(autoregressive) 모델의 입력인 멜 필터 뱅크 음성 특징 벡터열에 이전 시간의 문장 정보 및 음성 정보가 담겨있는 반면, 템플릿(Template, 202)에는 오로지 시간 정보만이 담겨있다. 따라서 템플릿 (Template, 202) 정보 부족을 해결하기 위해 본 발명은 도 5의 구조와 같이 다중 디코더를 통해 점진적으 로 정확도를 높여가는 학습을 제안할 수 있다. 다중 디코더는 동일한 구조의 단일 디코더들이 여러 개 존재하는 구조로, 이전 단계의 디코더 출력이 다음 단계의 디코더의 입력으로 들어가는 구조이다. 이 때, 각 디코더들은 모두 음성의 멜 필터 뱅크 특징 벡터(203, 204)를 추정할 수 있다. 다음 식은 단일 디코더의 목적함수를 나타낸다. [수학식 5]"}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이러한 구조를 통해 각 디코더의 입력에는 시간적 정보만이 포함되어 있던 템플릿(Template, 202)으로부터 점점 많은 정보가 추가된 입력이 들어가게 된다. 이것은 하나의 디코더에 가중되는 짐을 다중 디코더(21 0)로 분산시킴으로써 각 디코더의 학습 효율을 높이고 전체적으로 정확도가 높은 멜 필터 뱅크 음성 특징 벡터를 생성할 수 있게 도와준다. 또한, 다중 디코더로 구성된 음성 특징 벡터열 합성부는 하나의 목적 함 수(objective function)을 통해 학습될 수 있다. 다음 식은 M 개의 단일 디코더를 사용하는 다중 디코더의 목적함수를 나타낸다. [수학식 6]"}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "또한, 각 디코더에서는 총 3 번의 어텐션 메커니즘과 콘볼루션 네트워크를 이용해 템플릿(Template, 202) 데이터에 정제된 문장 데이터 정보를 적절하게 추가할 수 있다. 도 6은 일 실시예에 따른 단일 디코더에서 템플릿 데이터에 정제된 문장 데이터 정보를 추가하는 과정을 설명하 기 위한 도면이다. 도 6을 참조하면, 단일 디코더에서 어텐션 메커니즘(312, 314, 316)과 콘볼루션 네크워크(311, 313, 315)를 반 복하여 템플릿(Template, 302) 데이터에 정제된 문장 데이터 정보를 적절하게 추가하는 과정을 나타내는 과정이 다. 앞에서 설명한 바와 같이, 음성 특징 벡터열 합성부는 전체 모델의 디코더에 해당하며, 이는 음성 데 이터 인코딩부와 음성 데이터 디코딩부로 구성될 수 있다. 음성 데이터 인코딩부는 다수의 콘볼 루션 네트워크(311, 313, 315)와 어텐션 메커니즘(312, 314, 316)으로 구성될 수 있다. 각 어텐션 메커니즘 (312, 314, 316)을 거치며 템플릿(Template, 302)은 멜 필터 뱅크 음성 특징 벡터열을 생성하기 위한 문 장 정보를 적절하게 담고 있는 새로운 정보로 인코딩될 수 있다. 음성 데이터 인코딩부를 거치고 나면 템플릿(Template, 302)은 멜 필터 뱅크 음성 특징 벡터열을 합 성하기 위한 문장 정보를 포함하는 인코딩된 템플릿(Template, 303)으로 매핑된다. 각 어텐션 메커니즘(312, 314, 316)의 과정은 다음과 같다. 먼저, 정제된 문장 데이터를 V 행렬과 K 행렬로 분리한다. 이때 V, K 행렬의 차원은 [문장 시간, 채널]로 동일하다. 콘볼루션 네트워크(311, 313, 315)를 거친 템플릿(Template)을 Q 행렬이라 하고 크기는 [음성 시간, 채널]로 구성된다. 이때 문장 데이터와 음성 데이터의 채널 값은 동일하다. [수학식 7]"}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "[수학식 7]은 어텐션 메커니즘(312, 314, 316)에서 두 입력의 매칭되는 정도를 구하는 방법에 관한 식이다. 두 행렬의 매칭되는 정도를 나타내는 행렬인 는 와 의 스칼라 곱으로 구해진다. 이때, 행렬 의 크기는 [음성시간, 문장 시간]이다. 행렬 의 원소를 로 나타낼 수 있다. [수학식 8]"}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "[수학식 8]은 행렬의 데이터를 소프트맥스(soft-max) 함수를 이용해 확률의 의미를 가지는 로 변환할 수 있 다. 로 이루어진 행렬을 A로 나타내고 얼라인먼트(alignment) 행렬이라 부른다. 어텐션 메커니즘의 최종 결과물인 행렬을 구하는 과정을 다음 식과 같이 나타낼 수 있다. [수학식 9]"}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "C 행렬은 문장 데이터에 대한 정보를 담고 있으며 콘텍스트(context) 데이터라 한다. 이 행렬은 행렬과 동 일한 차원 [음성 시간, 채널]을 가지기 때문에 와 를 병합(concatenate)하여 네트워크의 다음 입력으로 사용 할 수 있다. 어텐션 메커니즘(312, 314, 316)은 총 세 차례에 걸쳐 적용되며, 순차적으로 더 정확한 얼라인먼트 행렬을 형성 할 수 있다. 상기된 내용과 같은 과정으로 생성된 인코딩된 템플릿(Template, 303)은 음성 데이터 디코딩부 를 거쳐 멜 필터 뱅크 음성 특징 벡터열로 매핑될 수 있다. 위 과정을 거쳐 합성된 멜 필터 뱅크 음성 특징 벡터열은 포스트 프로세싱 인공 신경망을 거쳐 로그 파워 스펙트럼 음성 특징 벡터열로 매핑될 수 있다. 문장 데이터 분석부와 음성 특징 벡터열 합성부에서 사용되는 인 공 신경망은 합성된 두 특징 벡터열과 실제 음성 특징 벡터 데이터 사이의 오차를 통해 학습될 수 있다. 즉, 음 성 특징 벡터열 합성 인공 신경망을 학습시킬 수 있다. 음성 재구성 단계(S130)에서, 음성 재구성부는 음성 특징 벡터열을 음성 데이터로 변환할 수 있다. 음성 재구성부에서는 이전 단계에서 최종 합성된 로그 파워 스펙트럼 음성 특징 벡터열을 이용하여 음성 데이터 를 복구할 수 있다. 심화 신경망이 합성한 로그 파워 스펙트럼 음성 특징 벡터열은 위상(phase) 정보가 없이 크 기(magnitude) 정보만을 가지고 있기 때문에 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 새로운 위 상 정보를 생성하여 재구성해야 한다. 실시예들에 따르면 비-자동회귀(non-autoregressive) 방식으로 음성 특징 벡터열을 한번에 합성하기 때문에 자 동회귀(autoregressive) 음성 합성 방식에 비해 빠른 속도로 음성을 합성할 수 있다. 또한, 자동회귀 (autoregressive) 기반 모델 회귀(regression) 모델에서 출력의 크기가 점점 감소하는 현상이 나타나지 않아 음 성의 크기가 문장 전체에서 일정하게 유지될 수 있다. 또한, 실시예들에 따르면 템플릿(Template)으로부터 음성 합성 모델이 다중 디코더를 통하여 단계적으로 음성 특징 벡터를 생성하도록 구현하여 비-자동회귀(non-autoregressive) 방식에서 음질이 크게 저하되는 문제를 해 결할 수 있다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 예를 들어, 프로 세서, 컨트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴 퓨터, FPA(field programmable array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상 의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설"}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 컨트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치에 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터 는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2019-0085920", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2019-0085920", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 비-자동회귀 음성 합성 시스템의 전체적인 구성을 나타내는 도면이다. 도 2 내지 도 4는 일 실시예에 따른 비-자동회귀 음성 합성 방법을 나타내는 흐름도이다. 도 5는 일 실시예에 따른 멜 필터 뱅크 음성 특징 벡터열 생성 과정을 설명하기 위한 도면이다."}
