{"patent_id": "10-2024-7027326", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0145474", "출원번호": "10-2024-7027326", "발명의 명칭": "비디오 스트림의 생성을 위한 시스템 및 방법", "출원인": "라이브아레나 테크놀로지스 에이비", "발명자": "비에르크만, 안드레아스"}}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "공유 디지털 비디오 스트림을 제공하는 방법에 있어서, 상기 방법은:수집 단계(S2)에서, 제1 디지털 비디오 소스로부터 제1 디지털 비디오 스트림을 및 제2 디지털 비디오 소스로부터 제2 디지털 스트림을 수집하는 단계;제1 생성 단계(S4)에서, 상기 제1 디지털 비디오 소스의 이미지 정보가 상기 공유 디지털 비디오 스트림에서 보이지만, 상기 제2 디지털 비디오 소스의 이미지 정보는 상기 공유 디지털 비디오 스트림에서 보이지 않도록 상기 제1 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로 생성하는 단계;지연 단계(A3)에서, 상기 제1 및 제2 디지털 비디오 스트림에 대해 대기 시간을 의도적으로 도입하는 단계;트리거 감지 단계(S5)에서, 적어도 하나의 트리거를 감지하기 위해 상기 제1 및/또는 제2 디지털 비디오 스트림을 디지털 방식으로 분석하는 단계 - 상기 분석은 미리 결정된 유형의 이미지 및/또는 오디오 패턴의 상기 자동감지에 기초하고, 상기 트리거 감지 단계(S5)는 상기 대기 시간을 도입하기 전에 상기 제1 및/또는 제2 디지털비디오 스트림에 기초하여 수행되고, 상기 패턴의 존재는 상기 트리거를 구성하고, 상기 트리거는 미리 정해진생성 규칙에 따라, 상기 공유 디지털 비디오 스트림의 상기 생성 모드를 변경하도록 지시함 - ;상기 트리거의 상기 감지에 응답하여 개시되는 제2 생성 단계(S6,S7)에서, 상기 제2 디지털 비디오 소스의 이미지 정보가 상기 공유 디지털 비디오 스트림에서 보이도록 상기 제2 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로서 생성하고 및/또는상기 제1 디지털 비디오 소스의 연속적으로 고려되는 프레임에 기초하지만 상기 제1 생성 단계(S4)와 비교하여상기 제1 디지털 비디오 스트림의 다른 크로핑, 다른 줌잉, 다른 패닝 또는 다른 초점 평면 선택 중 적어도 하나를 사용하여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로 생성하는 단계; 및게시 단계(S8)에서, 상기 출력 디지털 비디오 스트림을 상기 공유 디지털 비디오 스트림의 소비자에게 지속적으로 제공하는 단계 - 상기 대기 시간은 상기 공유 디지털 비디오 스트림에 존재함 -를 포함하는, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 제1 디지털 비디오 스트림은 제1 디지털 카메라(410)에 의해 연속적으로 캡처되고, 상기 제2 디지털 비디오 스트림은 제2 디지털 카메라(420)에 의해 연속적으로 캡처되는, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 미리 결정된 이미지 및/또는 오디오 패턴은 상기 제2 디지털 비디오 스트림에 묘사된 객체(440)를 응시하는 상기 제1 디지털 비디오 스트림에 묘사된 참가자(430)를 포함하고, 상기 미리 결정된 이미지 및/또는 오디오 패턴은 상기 제1 카메라(410), 상기 참가자(430) 및 상기 객체(440)의 상대적인 방향에 대한정보에 기초하여 감지되고, 상기 미리 결정된 이미지 및/또는 오디오 패턴은 상기 제1 디지털 비디오 스트림을기반으로 해당 상기 참가자(430)의 신체 방향, 머리 방향 및 시선 방향 중 적어도 하나에 대한 디지털 이미지기반 결정에 기초하여 추가로 감지되는, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "선행 항들 중 어느 한 항에 있어서, 상기 미리 결정된 이미지 및/또는 오디오 패턴은 미리 결정된 제스처를 취하는 상기 제1 디지털 비디오 스트림에 묘사된 참가자(430)를 포함하는, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2024-0145474-3-제4항에 있어서, 상기 제스처는 상기 객체(440)를 가리키는 것과 같은, 상기 제2 디지털 비디오 스트림에 묘사된 객체(440)에 대한 제스처이고, 상기 미리 결정된 이미지 및/또는 오디오 패턴은 상기 제1 카메라(410), 상기참가자(430) 및 상기 객체(440)의 상대적인 방향에 대한 정보에 기초하여 감지되고, 상기 미리 결정된 이미지및/또는 오디오 패턴은 상기 제1 디지털 비디오 스트림에 기초하는 해당 상기 참가자(430)의 제스처 방향에 대한 디지털 이미지 기반 결정에 기초하여 추가로 감지되는, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "선행 항들 중 어느 한 항에 있어서, 상기 미리 결정된 이미지 및/또는 오디오 패턴은 상기 제2 디지털 비디오스트림을 연속적으로 캡처하는 제2 카메라(420)를 응시하는 상기 제2 디지털 비디오 스트림에 묘사된 참가자(430)를 포함하는, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "선행 항들 중 어느 한 항에 있어서, 상기 미리 결정된 이미지 및/또는 오디오 패턴은 상기 제1 및/또는 제2 디지털 비디오 스트림의 상대적인 움직임 변화를 포함하는, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "선행 항들 중 어느 한 항에 있어서, 상기 미리 결정된 이미지 및/또는 오디오 패턴은 특정 사운드 캡처 마이크의 상대적인 마이크 볼륨에 의해 결정되는 주파수 및/또는 진폭 및/또는 진폭 시간 미분 및/또는 절대 진폭 변화 및/또는 사운드 위치를 특징으로 하는 미리 결정된 사운드 패턴을 포함하는, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 미리 결정된 사운드 패턴은 적어도 하나의 구두로 발언된 단어를 포함하는 미리 결정된문구를 포함하는, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "선행 항들 중 어느 한 항에 있어서, 상기 대기 시간은 적어도 3개의 비디오 프레임, 적어도 5개의 비디오 프레임, 적어도 10개의 비디오 프레임과 같은, 적어도 몇 개의 비디오 프레임인, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "선행 항들 중 어느 한 항에 있어서, 상기 대기 시간은 최대 5초, 최대 1초, 최대 0.5초와 같은, 최대 30초인,방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "선행 항들 중 어느 한 항에 있어서, 상기 이미지 및/또는 오디오 패턴의 상기 감지는 제1 및/또는 제2 기본 디지털 비디오 스트림의 특정 정보를 고려하는 단계를 포함하고, 상기 특정 정보는 상기 출력 디지털 비디오 스트림의 상기 생성에 아직 사용되지 않은 프레임보다 나중 프레임에 존재하는, 방법."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "공유 디지털 비디오 스트림을 제공하기 위한 컴퓨터 소프트웨어 제품에 있어서, 상기 컴퓨터 소프트웨어 제품은실행시:제1 디지털 비디오 스트림이 제1 디지털 비디오 소스로부터 및 제2 디지털 스트림이 제2 디지털 비디오 소스로부터 수집되는, 수집 단계(S2);상기 제1 디지털 비디오 소스의 이미지 정보가 상기 공유 디지털 비디오 스트림에서 보이지만, 상기 제2 디지털비디오 소스의 이미지 정보는 상기 공유 디지털 비디오 스트림에서 보이지 않도록 상기 제1 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 상기 공유 디지털 비디오 스트림이 출력 디지털 비디오 스트림으로 생성되는, 제1 생성 단계(S4); 대기 시간이 상기 제1 및 제2 디지털 비디오 스트림에 대해 의도적으로 도입되는, 지연 단계(A3);상기 제1 및/또는 제2 디지털 비디오 스트림이 적어도 하나의 트리거를 감지하기 위해 디지털 방식으로 분석되공개특허 10-2024-0145474-4-는, 트리거 감지 단계(S5) - 상기 분석은 미리 결정된 유형의 이미지 및/또는 오디오 패턴의 상기 자동 감지에기초하고, 상기 트리거 감지 단계(S5)는 상기 대기 시간을 도입하기 전에 상기 제1 및/또는 제2 디지털 비디오스트림에 기초하여 수행되고, 상기 패턴의 존재는 상기 트리거를 구성하고, 상기 트리거는 미리 정해진 생성 규칙에 따라, 상기 공유 디지털 비디오 스트림의 상기 생성 모드를 변경하도록 지시함 - ;상기 트리거의 상기 감지에 응답하여 개시되고, 상기 제2 디지털 비디오 소스의 이미지 정보가 상기 공유 디지털 비디오 스트림에서 보이도록 상기 제2 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 상기공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로서 생성하고 및/또는 상기 제1 디지털 비디오 소스의 연속적으로 고려되는 프레임에 기초하지만 상기 제1 생성 단계(S4)와 비교하여 상기 제1 디지털 비디오 스트림의 다른 크로핑, 다른 줌잉, 다른 패닝 또는 다른 초점 평면 선택 중 적어도 하나를 사용하여 상기 공유 디지털 비디오 스트림이 출력 디지털 비디오 스트림으로 생성되는, 제2 생성 단계(S6,S7); 및상기 출력 디지털 비디오 스트림이 상기 공유 디지털 비디오 스트림의 소비자에게 지속적으로 제공되는, 게시단계(S8) - 상기 대기 시간은 상기 공유 디지털 비디오 스트림에 존재함 -를 수행하도록 구성되는, 제품."}
{"patent_id": "10-2024-7027326", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "공유 디지털 비디오 스트림을 제공하기 위한 시스템(100)에 있어서, 상기 시스템(100)은 중앙 서버(130)을 포함하고 상기 시스템은:제1 디지털 비디오 소스로부터 제1 디지털 비디오 스트림을 및 제2 디지털 비디오 소스로부터 제2 디지털 스트림을 수집하도록 구성되는, 수집 기능;상기 제1 디지털 비디오 소스의 이미지 정보가 상기 공유 디지털 비디오 스트림에서 보이지만, 상기 제2 디지털비디오 소스의 이미지 정보는 상기 공유 디지털 비디오 스트림에서 보이지 않도록 상기 제1 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로 생성하도록 구성되는, 수집 기능(135, 135', 135\",135\"');상기 제1 및 제2 디지털 비디오 스트림에 대해 대기 시간을 의도적으로 도입하도록 구성되는, 지연 기능;적어도 하나의 트리거를 감지하기 위해 상기 제1 및/또는 제2 디지털 비디오 스트림을 디지털 방식으로 분석하도록 구성되는, 트리거 감지 기능 - 상기 분석은 미리 결정된 유형의 이미지 및/또는 오디오 패턴의 상기 자동감지에 기초하고, 상기 분석은 상기 대기 시간을 도입하기 전에 상기 제1 및/또는 제2 디지털 비디오 스트림에기초하여 수행되고, 상기 패턴의 존재는 상기 트리거를 구성하고, 상기 트리거는 미리 정해진 생성 규칙에따라, 상기 공유 디지털 비디오 스트림의 상기 생성 모드를 변경하도록 지시함 - ;상기 트리거의 상기 감지에 응답하여 개시되고, 상기 제2 디지털 비디오 소스의 이미지 정보가 상기 공유 디지털 비디오 스트림에서 보이도록 상기 제2 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 상기공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로서 생성하고 및/또는 상기 제1 디지털 비디오 소스의 연속적으로 고려되는 프레임에 기초하지만 상기 제1 생성 기능(135, 135', 135\", 135\"')과 비교하여 상기 제1 디지털 비디오 스트림의 다른 크로핑, 다른 줌잉, 다른 패닝 또는 다른 초점 평면 선택 중 적어도 하나를 사용하여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로 생성하도록 구성되는, 제2 생성 기능(135, 135', 135\", 135\"'); 및상기 출력 디지털 비디오 스트림을 상기 공유 디지털 비디오 스트림의 소비자에게 지속적으로 제공하도록 구성되는, 게시 기능 - 상기 대기 시간은 상기 공유 디지털 비디오 스트림에 존재함 - 을 포함하는, 시스템."}
{"patent_id": "10-2024-7027326", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 공유 디지털 비디오 스트림을 제공하는 방법은 제1 및 제2 디지털 비디오 스트림을 수집하는 수집 단 계(S2); 상기 제1 소스가 공유 스트림에 표시되지만, 상기 제2 소스가 공유 스트림에 표시되지 않도록 상기 제1 스트림에 기초하여 상기 공유 스트림을 생성하는 제1 생성 단계(S4); 제1 및 제2 디지털 비디오 스트림에 대해 의도적으로 대기 시간을 도입하는 지연 단계(A3); 미리 결정된 이미지 및/또는 오디오 패턴의 감지에 기초하여, 트리거를 감지하기 위해 상기 제1 및/또는 제2 스트림을 분석하는 트리거 감지 단계(S5); 상기 제2 스트림에 기 초하여 및/또는 상기 제1 소스에 기초하지만, 다른 크로핑, 다른 줌잉, 다른 패닝 또는 다른 초점 평면 선택 중 적어도 하나를 사용하여 공유 스트림을 생성하는 제2 생성 단계(S6,S7); 및 상기 출력 스트림을 소비자에게 제공 하는, 게시 단계(S8)를 포함한다. 본 발명은 또한 컴퓨터 소프트웨어 제품 및 시스템에 관한 것이다."}
{"patent_id": "10-2024-7027326", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 디지털 비디오 스트림을 생성하기 위한 시스템, 컴퓨터 소프트웨어 제품 및 방법에 관한 것으로, 특 히 둘 이상의 서로 다른 디지털 입력 비디오 스트림을 기반으로 디지털 비디오 스트림을 생성하기 위한 방법에 관한 것이다. 바람직한 실시예에서, 디지털 비디오 스트림은 특히 복수의 서로 다른 동시 사용자를 포함하는 디지털 비디오 회의 또는 디지털 비디오 회의 또는 회의 시스템의 컨텍스트에서 생성된다. 생성된 디지털 비디오 스트림은 외부로 공개되거나 디지털 비디오 회의 또는 디지털 비디오 회의 시스템 내에서 공개될 수 있다."}
{"patent_id": "10-2024-7027326", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "다른 실시예에서, 본 발명은 디지털 화상 회의는 아니지만, 여러 디지털 비디오 입력 스트림이 동시에 처리되어 생성된 디지털 비디오 스트림에 결합되는 컨텍스트에서 적용된다. 예를 들어, 이러한 컨텍스트는 교육적이거나 지시적일 수 있다. 두 명 이상의 참가자가 현장에서 녹화된 디지털 비디오 및 오디오를 사용하여 가상으로 회의하고 모든 참가자에 게 방송하여 실제 회의와 같은 느낌을 받을 수 있도록 하는, Microsoft® Teams®, Zoom® 및 Google® Meet®와 같이 많은 알려진 디지털 화상 회의 시스템이 있다. 특히, 누구에게 언제 어떤 배포 채널을 통해 무엇을 보여줄지와 같이, 시청되는 콘텐츠의 생성과 관련하여 이러 한 디지털 화상 회의 솔루션을 개선해야 할 필요성이 일반적으로 대두되고 있다. 예를 들어, 일부 시스템은 현재 대화 중인 참가자를 자동으로 감지하고, 대화 중인 참가자의 해당 비디오 피드 를 다른 참가자에게 보여준다. 많은 시스템에서 현재 표시되는 화면, 보기 창 또는 디지털 프레젠테이션과 같은 그래픽을 공유할 가능성이 있다. 그러나 가상 회의가 더 복잡해짐에 따라 서비스에서 각 시점에서 각 참가자에 게 현재 사용 가능한 모든 정보 중에서 무엇을 보여줄지를 알기가 더 어려워진다. 다른 예에서 발표하는 참가자는 디지털 프레젠테이션에서 슬라이드에 대해 이야기하는 동안 무대를 돌아다닌다. 그런 다음 시스템은 프레젠테이션, 발표자 또는 둘 다 보여줄지 또는 둘 사이를 전환할지를 결정해야 한다. 자동 생성 프로세스를 통해 여러 입력 디지털 비디오 스트림을 기반으로 하나 또는 여러 개의 출력 디지털 비디 오 스트림을 생성하고, 이렇게 생성된 디지털 비디오 스트림을 하나 또는 여러 개의 소비 개체에 제공하는 것이 바람직할 수 있다. 그러나 많은 경우, 이러한 디지털 비디오 컨퍼런스 시스템이 직면한 여러 가지 기술적 어려움으로 인해, 동적 컨퍼런스 화면 레이아웃 관리자 또는 기타 자동화된 생성 기능이 어떤 정보를 표시할지 선택하기가 어렵다. 첫째, 디지털 비디오 회의는 실시간 측면이 있으므로 대기 시간이 낮아야 한다. 이는 서로 다른 하드웨어를 사 용하여 참여하는 서로 다른 참가자와 같이 서로 다른 수신 디지털 비디오 스트림이 서로 다른 대기 시간, 프레 임 속도, 종횡비 또는 해상도와 연관되어 있는 경우 문제가 된다. 이러한 수신 디지털 비디오 스트림은 잘 구성 된 사용자 경험을 위해 처리해야 할 필요가 있다. 둘째, 시간 동기화에 문제가 있다. 외부 디지털 비디오 스트림이나 참가자가 제공한 디지털 비디오 스트림과 같 은 다양한 입력 디지털 비디오 스트림은 일반적으로 중앙 서버 또는 유사한 서버에 공급되므로, 이러한 각 디지 털 비디오 피드를 동기화할 절대적 시간이 없다. 대기 시간이 너무 긴 경우와 마찬가지로, 동기화되지 않은 디 지털 비디오 피드는 결과적으로 사용자 경험이 좋지 않게 된다. 셋째, 다자간 디지털 비디오 회의에는 서로 다른 인코딩 또는 형식을 가진 서로 다른 디지털 비디오 스트림이 포함될 수 있으며, 이를 디코딩하고 다시 인코딩해야 하므로 대기 시간과 동기화 측면에서 문제가 발생한다. 이 러한 인코딩은 또한 계산적으로 부담스럽고 따라서 하드웨어 요구 사항 측면에서 비용이 많이 든다. 넷째, 다른 디지털 비디오 소스가 서로 다른 프레임 속도, 종횡비 및 해상도와 연관될 수 있다는 사실로 인해, 결과적으로 메모리 할당 요구 사항이 예측 불가능하게이 변하게 되므로 지속적인 밸런싱을 필요로 한다. 이로 인해 추가 지연 및 동기화 문제가 발생할 수 있다. 결과적으로 버퍼 요구 사항이 커진다. 다섯째, 참가자는 가변적인 연결성, 이탈/재연결 등의 측면에서 다양한 문제점에 직면하게 되므로, 잘 구성된 사용자 경험을 자동으로 생성하는 데 추가적인 과제를 제기한다. 이러한 문제는 예를 들어, 많은 참가자가 참여하는 경우; 참가자가 서로 다른 하드웨어 및/또는 소프트웨어를 사용하여 연결하는 경우; 외부에서 제공되는 디지털 비디오 스트림; 화면 공유; 또는 여러 호스트와 같은 더 복 잡한 회의 상황에서 증폭된다. 해당 문제는 예를 들어 교육 및 지시를 위한 디지털 비디오 생성 시스템에서와 같이, 여러 입력 디지털 비디오 스트림을 기반으로 출력 디지털 비디오 스트림을 생성해야 하는 다른 컨텍스트에서 발생한다. .본 출원의 발효일에 공개되지 않은 스웨덴 출원 SE 2151267-8은 상술된 문제에 대한 다양한 솔루션을 공개한다. 현재 출원의 유효일에는 공개되지 않은 스웨덴 출원 2151461-7은 예를 들어, 다른 참가자 그룹이 다른 일반적인 대기 시간과 연관되는 경우와 같이, 다중 참가자 디지털 비디오 환경에서 대기 시간을 처리하는 데 특화된 다양 한 솔루션을 공개한다. 상술된 유형의 회의를 자동화된 방식으로 생성하는 데는 여전히 문제가 있다. 특히, 이러한 자동화된 생성에 여 러 대의 카메라를 사용할 수 있는 경우, 회의 참가자에게 자연스럽고 직관적으로 느껴지는 방식으로 이러한 카 메라의 해당 이미지 출력을 사용하는 것이 어려운 것으로 입증되었다."}
{"patent_id": "10-2024-7027326", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상술된 문제 중 하나 이상을 해결한다."}
{"patent_id": "10-2024-7027326", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "따라서, 본 발명은 공유 디지털 비디오 스트림을 제공하는 방법에 관한 것으로, 상기 방법은 수집 단계에서, 제 1 디지털 비디오 소스로부터 제1 디지털 비디오 스트림을 및 제2 디지털 비디오 소스로부터 제2 디지털 스트림 을 수집하는 단계; 제1 생성 단계에서, 상기 제1 디지털 비디오 소스의 이미지 정보가 상기 공유 디지털 비디오 스트림에서 보이지만, 상기 제2 디지털 비디오 소스의 이미지 정보는 상기 공유 디지털 비디오 스트림에서 보이 지 않도록 상기 제1 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로 생성하는 단계; 지연 단계에서, 상기 제1 및 제2 디지털 비디오 스트 림에 대해 대기 시간을 의도적으로 도입하는 단계; 트리거 감지 단계에서, 적어도 하나의 트리거를 감지하기 위 해 상기 제1 및/또는 제2 디지털 비디오 스트림을 디지털 방식으로 분석하는 단계 - 상기 분석은 미리 결정된 유형의 이미지 및/또는 오디오 패턴의 상기 자동 감지에 기초하고, 상기 트리거 감지 단계는 상기 대기 시간을 도입하기 전에 상기 제1 및/또는 제2 디지털 비디오 스트림에 기초하여 수행되고, 상기 패턴의 존재는 상기 트 리거를 구성하고, 상기 트리거는 미리 정해진 생성 규칙에 따라, 상기 공유 디지털 비디오 스트림의 상기 생성 모드를 변경하도록 지시함 - ; 상기 트리거의 상기 감지에 응답하여 개시되는 제2 생성 단계에서, 상기 제2 디 지털 비디오 소스의 이미지 정보가 상기 공유 디지털 비디오 스트림에서 보이도록 상기 제2 디지털 비디오 스트 림의 연속적으로 고려되는 프레임에 기초하여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으 로서 생성하고 및/또는 상기 제1 디지털 비디오 소스의 연속적으로 고려되는 프레임에 기초하지만 상기 제1 생 성 단계와 비교하여 상기 제1 디지털 비디오 스트림의 다른 크로핑, 다른 줌잉, 다른 패닝 또는 다른 초점 평면 선택 중 적어도 하나를 사용하여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로 생성하는 단계; 및 게시 단계에서, 상기 출력 디지털 비디오 스트림을 상기 공유 디지털 비디오 스트림의 소비자에게 지 속적으로 제공하는 단계 - 상기 대기 시간은 상기 공유 디지털 비디오 스트림에 존재함 - 를 포함한다. 또한, 본 발명은 공유 디지털 비디오 스트림을 제공하기 위한 컴퓨터 소프트웨어 제품에 관한 것으로, 상기 컴 퓨터 소프트웨어 기능은 실행시: 제1 디지털 비디오 스트림이 제1 디지털 비디오 소스로부터 및 제2 디지털 스 트림이 제2 디지털 비디오 소스로부터 수집되는, 수집 단계; 상기 제1 디지털 비디오 소스의 이미지 정보가 상 기 공유 디지털 비디오 스트림에서 보이지만, 상기 제2 디지털 비디오 소스의 이미지 정보는 상기 공유 디지털 비디오 스트림에서 보이지 않도록 상기 제1 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 상 기 공유 디지털 비디오 스트림이 출력 디지털 비디오 스트림으로 생성되는, 제1 생성 단계; 대기 시간이 상기 제1 및 제2 디지털 비디오 스트림에 대해 의도적으로 도입되는, 지연 단계; 상기 제1 및/또는 제2 디지털 비디 오 스트림이 적어도 하나의 트리거를 감지하기 위해 디지털 방식으로 분석되는, 트리거 감지 단계 - 상기 분석 은 미리 결정된 유형의 이미지 및/또는 오디오 패턴의 상기 자동 감지에 기초하고, 상기 트리거 감지 단계는 상 기 대기 시간을 도입하기 전에 상기 제1 및/또는 제2 디지털 비디오 스트림에 기초하여 수행되고, 상기 패턴의 존재는 상기 트리거를 구성하고, 상기 트리거는 미리 정해진 생성 규칙에 따라, 상기 공유 디지털 비디오 스트 림의 상기 생성 모드를 변경하도록 지시함 - ; 상기 트리거의 상기 감지에 응답하여 개시되고, 상기 제2 디지털 비디오 소스의 이미지 정보가 상기 공유 디지털 비디오 스트림에서 보이도록 상기 제2 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로서 생성하고 및/또는 상기 제1 디지털 비디오 소스의 연속적으로 고려되는 프레임에 기초하지만 상기 제1 생성 단 계와 비교하여 상기 제1 디지털 비디오 스트림의 다른 크로핑, 다른 줌잉, 다른 패닝 또는 다른 초점 평면 선택중 적어도 하나를 사용하여 상기 공유 디지털 비디오 스트림이 출력 디지털 비디오 스트림으로 생성되는, 제2 생성 단계; 및 상기 출력 디지털 비디오 스트림이 상기 공유 디지털 비디오 스트림의 소비자에게 지속적으로 제 공되는, 게시 단계 - 상기 대기 시간은 상기 공유 디지털 비디오 스트림에 존재함 - 를 수행하도록 구성된다. 또한, 본 발명은 공유 디지털 비디오 스트림을 제공하기 위한 시스템에 관한 것으로, 상기 시스템은 중앙 서버 를 포함하고 상기 시스템은 제1 디지털 비디오 소스로부터 제1 디지털 비디오 스트림을 및 제2 디지털 비디오 소스로부터 제2 디지털 스트림을 수집하도록 구성되는, 수집 기능; 상기 제1 디지털 비디오 소스의 이미지 정보 가 상기 공유 디지털 비디오 스트림에서 보이지만, 상기 제2 디지털 비디오 소스의 이미지 정보는 상기 공유 디 지털 비디오 스트림에서 보이지 않도록 상기 제1 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하 여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로 생성하도록 구성되는, 수집 기능; 상기 제1 및 제2 디지털 비디오 스트림에 대해 대기 시간을 의도적으로 도입하도록 구성되는, 지연 기능; 적어도 하 나의 트리거를 감지하기 위해 상기 제1 및/또는 제2 디지털 비디오 스트림을 디지털 방식으로 분석하도록 구성 되는, 트리거 감지 기능 - 상기 분석은 미리 결정된 유형의 이미지 및/또는 오디오 패턴의 상기 자동 감지에 기 초하고, 상기 분석은 상기 대기 시간을 도입하기 전에 상기 제1 및/또는 제2 디지털 비디오 스트림에 기초하여 수행되고, 상기 패턴의 존재는 상기 트리거를 구성하고, 상기 트리거는 미리 정해진 생성 규칙에 따라, 상기 공 유 디지털 비디오 스트림의 상기 생성 모드를 변경하도록 지시함 - ; 상기 트리거의 상기 감지에 응답하여 개시 되고, 상기 제2 디지털 비디오 소스의 이미지 정보가 상기 공유 디지털 비디오 스트림에서 보이도록 상기 제2 디지털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 상기 공유 디지털 비디오 스트림을 출력 디지 털 비디오 스트림으로서 생성하고 및/또는 상기 제1 디지털 비디오 소스의 연속적으로 고려되는 프레임에 기초 하지만 상기 제1 생성 기능과 비교하여 상기 제1 디지털 비디오 스트림의 다른 크로핑, 다른 줌잉, 다른 패닝 또는 다른 초점 평면 선택 중 적어도 하나를 사용하여 상기 공유 디지털 비디오 스트림을 출력 디지털 비디오 스트림으로 생성하도록 구성되는, 제2 생성 기능; 및 상기 출력 디지털 비디오 스트림을 상기 공유 디지털 비디 오 스트림의 소비자에게 지속적으로 제공하도록 구성되는, 게시 기능 - 상기 대기 시간은 상기 공유 디지털 비 디오 스트림에 존재함 - 을 포함한다."}
{"patent_id": "10-2024-7027326", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "모든 도면은 동일하거나 대응하는 부분에 대한 참조 번호를 공유한다. 도 1은 본 발명에 따른 시스템을 도시하며, 디지털 비디오 스트림, 예를 들어 공유 디지털 비디오 스트림 을 제공하기 위한 발명에 따른 방법을 수행하도록 배열되어 있다. 시스템은 비디오 통신 서비스을 포함할 수 있지만, 비디오 통신 서비스은 일부 실시예에서 시스 템 외부에 있을 수도 있다. 설명되는 바와 같이, 하나 이상의 비디오 통신 서비스이 있을 수 있다. 시스템은 하나 이상의 참여자 클라이언트을 포함할 수 있지만, 하나, 일부 또는 모든 참여자 클라이 언트는 일부 실시예에서 시스템 외부에 있을 수도 있다. 시스템은 중앙 서버을 포함할 수 있다. 본 명세서에서 사용되는 바와 같이, \"중앙 서버\"는 예를 들어 잘 정의된 API(애플리케이션 프로그래밍 인터페이 스)를 통해, 논리적으로 중앙화된 방식으로 액세스되도록 구성된 컴퓨터 구현 기능이다. 이러한 중앙 서버의 기 능은 순수하게 컴퓨터 소프트웨어로 구현되거나 소프트웨어와 가상 및/또는 물리적 하드웨어의 조합으로 구현될 수 있다. 이것은 독립형 물리적 또는 가상 서버 컴퓨터에서 구현되거나 여러 개의 상호 연결된 물리적 및/또는 가상 서버 컴퓨터에 분산될 수 있다. 중앙 서버가 실행하는, 물리적 또는 가상 하드웨어, 즉 중앙 서버의 기능을 정의하는 컴퓨터 소프트 웨어는 그 자체로 기존의 CPU, 기존의 GPU, 기존의 RAM/ROM 메모리, 기존의 컴퓨터 버스, 인터넷 연결과 같은 기존의 외부 통신 기능을 포함할 수 있다. 각 비디오 통신 서비스는 사용되는 범위 내에서, 상기 의미에서 또한 중앙 서버이며, 중앙 서버와 다 른 중앙 서버이거나 중앙 서버의 일부일 수 있다. 이에 따라, 각 참여자 클라이언트는 상기 의미에서 해당 해석으로 중앙 서버일 수 있으며, 각 참여자 클라 이언트가 실행되는 물리적 또는 가상 하드웨어, 즉 참여자 클라이언트의 기능을 정의하는 컴퓨터 소 프트웨어는 또한 그 자체로 기존 CPU/GPU, 그 자체로 기존 RAM/ROM 메모리, 그 자체로 기존 컴퓨터 버스 및 인 터넷 연결과 같은 그 자체로 기존 외부 통신 기능을 포함할 수 있다. 각 참여자 클라이언트는 또한 일반적으로 진행 중인 비디오 통신의 일부로 참여자 클라이언트에 제공 된 비디오 콘텐츠를 표시하도록 배열된 컴퓨터 화면; 상기 비디오 통신의 일부로 참여자 클라이언트에 제 공된 사운드 콘텐츠를 방출하도록 배열된 라우드스피커; 비디오 카메라; 및 해당 비디오 통신에 인간 참가자 에게 로컬로 소리를 녹음하도록 배열된 마이크를 포함하거나 이와 통신하고, 참가자는 해당 참여자 클라이언트을 사용하여 해당 비디오 통신에 참여한다. 다시 말해서, 각 참여자 클라이언트의 각각의 인간-기계 인터페이스는 각각의 참여자가 비디오 통신 에서, 다른 참여자 및/또는 다양한 소스에서 제공하는 오디오/비디오 스트림으로, 해당 클라이언트과 상호 작용할 수 있도록 한다. 일반적으로, 각 참여자 클라이언트는 해당 비디오 카메라, 마이크, 키보드; 컴퓨터 마우스 또는 트랙패드; 및/또는 디지털 비디오 스트림, 디지털 오디오 스트림 및/또는 기타 디지털 데이터를 수신하기 위한 API를 포함 할 수 있는 해당 입력 수단을 포함한다. 입력 수단은 비디오 통신 서비스 및/또는 중앙 서버 과 같은 중앙 서버로부터 비디오 스트림 및/또는 오디오 스트림을 수신하도록 특별히 배열되어 있으며, 이 러한 비디오 스트림 및/또는 오디오 스트림은 비디오 통신의 일부로 제공되며, 바람직하게는 이러한 디지털 데 이터 입력 스트림의 적어도 두 개의 소스, 예를 들어 참여자 클라이언트 및/또는 외부 소스(아래 참조)로 부터 상기 중앙 서버에 제공된 해당 디지털 데이터 입력 스트림을 기반으로 생성된다. 또한 일반적으로, 각 참여자 클라이언트은 상기 컴퓨터 화면을 포함할 수 있는 각각의 출력 수단; 상 기 라우드스피커; 및 디지털 비디오 및/또는 오디오 스트림을 방출하는 API를 포함하고, 이러한 스트림은 해당 참여자 클라이언트을 사용하여 참여자에게 현장에서 캡처된 비디오 및/또는 오디오를 나타낸다. 실제로, 각 참여자 클라이언트은 화면, 라우드스피커, 마이크 및 인터넷 연결이 배치된 모바일 폰과 같은 모바일 장치일 수 있으며, 모바일 장치는 현장에서 컴퓨터 소프트웨어를 실행하거나 원격으로 실행되는 컴퓨터 소프트웨어에 액세스하여 해당 참여자 클라이언트의 기능을 수행한다. 이에 상응하여, 참여자 클라이언트 는 또한 현장에 설치된 애플리케이션을 실행하고, 웹 브라우저를 통해 원격으로 액세스되는 기능을 사용하 는 등 경우에 따라 두껍거나 얇은 노트북 또는 고정 컴퓨터일 수 있다. 현재 유형의 동일한 비디오 통신에 사용되는 참여자 클라이언트는 하나 이상, 적어도 3명 또는 심지어 적 어도 4명일 수 있다. 참여자 클라이언트의 그룹의 수는 적어도 2개 이상일 수 있다. 각 참여자 클라이언트는 해당 그룹에 할당될 수 있다. 그룹은 참여자 클라이언트의 다른 역할, 참여자 클라이언트의 다른 가상 또는 물리적 위치 및/또는 참여 자 클라이언트의 다른 상호 작용 권한을 반영할 수 있다. 사용 가능한 이러한 역할은 예를 들어 \"리더\" 또는 \"컨퍼런시어\", \"발표자\", \"패널 참여자\", \"상호 작용하는 청 중\" 또는 \"원격 청취자\"일 수 있다. 사용 가능한 이러한 물리적 위치는 예를 들어 \"무대 위\", \"패널\", \"물리적으로 존재하는 청중\" 또는 \"물리적으 로 멀리 떨어진 청중\"일 수 있다. 가상 위치는 물리적 위치의 관점에서 정의될 수 있지만, 상기 물리적 위치와 부분적으로 겹칠 수 있는 가상 그 룹화를 포함할 수도 있다. 예를 들어, 물리적으로 현장에 있는 청중을 제1 가상 그룹과 제2 가상 그룹으로 나눌 수 있으며, 물리적으로 현장에 있는 청중 참가자 중 일부는 물리적으로 멀리 떨어져 있는 청중 참가자와 함께 하나의 동일한 가상 그룹으로 그룹화될 수 있다. 사용 가능한 이러한 다양한 상호작용 권한은 예를 들어 \"전체 상호작용\"(제한 없음), \"말할 수 있지만 마이크를 요청한 후에만 가능\"(예를 들어, 화상 회의 서비스에서 가상적으로 손을 드는 것), \"말할 수 없지만 일반 채팅 에서 쓸 수 있음\" 또는 \"보기/듣기 전용\"일 수 있다. 어떤 경우에, 정의된 각 역할 및/또는 물리적/가상 위치가 특정 사전 결정된 상호작용 권한에 따라 정의될 수 있다. 다른 경우에는 동일한 상호작용 권한을 가진 모든 참가자가 그룹을 형성한다. 따라서 정의된 역할, 위치 및/또는 상호작용 권한은 다양한 그룹 할당을 반영할 수 있으며, 경우에 따라 다른 그룹은 분리되거나 겹칠 수 있다. 이것은 아래에서 예시된다. 비디오 통신은 비디오 통신 서비스에 의해 적어도 부분적으로 제공될 수 있고, 중앙 서버에 의해 적 어도 부분적으로 제공될 수 있으며, 이는 본 명세서에서 설명되고 예시될 것이다. 본 명세서에서 사용되는 용어에 따르면, \"비디오 통신\"은 하나 또는 여러 개의 혼합 또는 공동 디지털 비디오/ 오디오 스트림을 생성하는 데 사용되며, 다음에 비디오 및/또는 오디오를 통해 비디오 통신에 기여할 수도 있고 그렇지 않을 수도 있는 하나 또는 여러 소비자(예를 들어, 논의된 유형의 참여자 클라이언트)에 의해 소비되는 적어도 두 개, 바람직하게는 적어도 세 개 또는 심지어 적어도 네 개의 비디오 스트림 및 바람직하게는 또한 일 치하는 오디오 스트림을 포함하는 대화형 디지털 통신 세션이다. 이러한 비디오 통신은 특정 대기 시간 또는 지 연이 있거나 없는 실시간이다. 이러한 비디오 통신에 대한 적어도 한 명, 바람직하게는 적어도 두 명 또는 심지 어 적어도 네 명의 참여자가 비디오 통신에 대화형 방식으로 참여하여, 비디오/오디오 정보를 제공하고 소 비한다. 참여자 클라이언트 중 적어도 하나 또는 모든 참여자 클라이언트은 아래에서 더 자세히 설명되는 로 컬 동기화 소프트웨어 기능을 포함할 수 있다. 비디오 통신 서비스은 이하 더 자세히 설명되는 바와 같이, 공통 시간 참조를 포함하거나 이에 액세스할 수 있다. 적어도 하나의 중앙 서버 각각은 해당 중앙 서버 외부의 개체와 디지털로 통신하기 위한 해당 API를 포함할 수 있다. 이러한 통신은 입력과 출력을 모두 포함할 수 있다. 시스템, 예를 들어 상기 중앙 서버는, 또한 예를 들어 외부에서 제공된 비디오 스트림과 같은 외부 정보 소스으로부터 오디오 및/또는 비디오 스트림 데이터와 같은 디지털 정보를 디지털 방식으로 통신하도 록, 특히 수신하도록 구성될 수 있다. 정보 소스이 \"외부\"라는 것은 중앙 서버에서 제공되거나 중앙 서버의 일부로 제공되지 않는다는 것을 의미한다. 바람직하게는, 외부 정보 소스이 제공하는 디지털 데이터는 중앙 서버과 독립적이며, 중앙 서버은 그 정보 내용에 영향을 미칠 수 없다. 예를 들어, 외 부 정보 소스는 예를 들어, 공개 스포츠 이벤트 또는 진행 중인 뉴스 이벤트 또는 보고의 라이브 캡처 비 디오 및/또는 오디오일 수 있다. 외부 정보 소스는 웹 카메라 또는 유사한 장비에 의해 캡처될 수 있지만, 참여자 클라이언트 중 어느 누구에 의해서도 캡처할 수 없다. 따라서 이러한 캡처된 비디오는 참여자 클라 이언트 중 어느 것과 동일한 로컬티를 묘사할 수 있지만, 참여자 클라이언트 자체의 활동의 일부로 캡처될 수는 없다. 외부에서 제공되는 정보 소스과 내부에서 제공되는 정보 소스 간의 한 가지 가능 한 차이점은 내부적으로 제공되는 정보 소스는 위에서 정의된 유형의 비디오 통신에 참여자로서, 그리고 참여자 로서의 역량으로 제공될 수 있는 반면, 외부에서 제공되는 정보 소스는 그렇지 않고, 대신 해당 비디오 컨 퍼런스 외부의 컨텍스트의 일부로 제공된다는 것이다. 또한 오디오 및/또는 비디오 스트림과 같은 해당 유형의 디지털 정보를 중앙 서버에 병렬로 제공하는 여러 외부 정보 소스가 있을 수 있다. 도 1에 도시된 바와 같이, 각 참여자 클라이언트는 설명된 바와 같이 해당 참여자 클라이언트에 의해 비디오 통신 서비스에 제공되는, 각각의정보(비디오 및/또는 오디오) 스트림의 소스를 구성할 수 있 다. 중앙 서버과 같은 시스템은 외부 소비자과 디지털로 통신하고, 특히 외부 소비자에게 디지 털 정보를 방출하도록 추가로 구성될 수 있다. 예를 들어, 중앙 서버에 의해 생성된 디지털 비디오 및/또 는 오디오 스트림은 해당 API를 통해 하나 또는 여러 외부 소비자에게 실시간 또는 거의 실시간으로 지속적으로 제공될 수 있다. 다시 말하지만, 소비자이 \"외부적\"이라는 것은 소비자가 중앙 서버(13 0)의 일부로 제공되지 않고/또는 해당 비디오 통신의 당사자가 아니라는 것을 의미한다. 달리 명시되지 않는 한, 본 명세서의 모든 기능과 통신은 디지털 및 전자적으로 제공되며, 적절한 컴퓨터 하드 웨어에 의해 실행되는 컴퓨터 소프트웨어에 의해 수행되고 인터넷과 같은 디지털 통신 네트워크 또는 채널을 통 해 전달된다. 따라서 도 1에 도시된 시스템 구성에서, 다수의 참여자 클라이언트는 비디오 통신 서비스이 제 공하는 디지털 비디오 통신에 참여한다. 따라서 각 참여자 클라이언트는 비디오 통신 서비스에 대한 지속적인 로그인, 세션 또는 이와 유사한 것을 가질 수 있으며, 비디오 통신 서비스이 제공하는 동일한 하 나의 지속적인 비디오 통신에 참여할 수 있다. 다시 말해, 비디오 통신은 참여자 클라이언트 간에 \"공유\" 되고 따라서 해당 인간 참여자에 의해서도 공유된다. 도 1에서, 중앙 서버는 자동 참여자 클라이언트를 포함하는데, 이는 참여자 클라이언트에 대응 하지만 인간 참여자와 연관되지 않은 자동 클라이언트이다. 대신, 자동 참여자 클라이언트은 참여자 클라이언트과 동일한 공유 비디오 통신에 참여하기 위해 비디오 통신 서비스에 참여자 클라이언트로 추가된다. 이러한 참여자 클라이언트로서, 자동 참여자 클라이언트은 비디오 통신 서비스에 의해 진 행 중인 비디오 통신의 일부로 제공되는 지속적으로 생성된 디지털 비디오 및/또는 오디오 스트림에 대한 액세 스가 부여되고, 자동 참여자 클라이언트을 통해 중앙 서버에서 소비될 수 있다. 바람직하게, 자동 참 여자 클라이언트는 비디오 통신 서비스으로부터, 각 참여자 클라이언트에 배포되거나 배포될 수 있는 공통 비디오 및/또는 오디오 스트림; 각 참여자 클라이언트 중 하나 또는 여러 개에서 비디오 통신 서비스에 제공되고 비디오 통신 서비스에 의해 모든 또는 요청하는 참여자 클라이언트에 원시 또는 수정된 형태로 중계되는 각각의 비디오 및/또는 오디오 스트림; 및/또는 공통 시간 참조를 수신한다. 중앙 서버는 아래에 설명된 대로 처리하기 위해 자동 참여자 클라이언트, 및 가능하게는 외부 정보 소스으로부터 상기 유형의 비디오 및/또는 오디오 스트림을 수신한 다음에, API을 통해 생성된, 예를 들어 공유된 비디오 스트림을 제공하도록 구성된 수집 기능을 포함할 수 있다. 예를 들어, 이 생성된 비디 오 스트림은 외부 소비자 및/또는 비디오 통신 서비스에 의해 소비되어 다음에 비디오 통신 서비스 에 의해 참여자 클라이언트의 모두 또는 요청한 것에 배포될 수 있다. 도 2는 도 1과 유사하지만 자동 클라이언트 참여자를 사용하는 대신에, 중앙 서버는 비디오 통신 서 비스의 API를 통해 진행 중인 비디오 통신에서 비디오 및/또는 오디오 스트림 데이터를 수신한다. 도 3은 또한 도 1과 유사하지만 비디오 통신 서비스은 표시하지 않는다. 이 경우, 참여자 클라이언트(12 1)은 중앙 서버의 API와 직접 통신하여, 예를 들어 중앙 서버에 비디오 및/또는 오디오 스트림 데이터를 제공하고/하거나 중앙 서버에서 비디오 및/또는 오디오 스트림 데이터를 수신한다. 그런 다음 생 성된 공유 스트림은 외부 소비자 및/또는 클라이언트 참여자 중 하나 또는 여러 개에 제공될 수 있다. 도 4는 중앙 서버을 보다 자세히 설명한다. 설명된 바와 같이, 상기 수집 기능은 하나 또는 바람직하 게는 여러 개의 형식별 수집 기능(131a)을 포함할 수 있다. 상기 포맷별 수집 기능(131a) 각각은 미리 정해진 포맷, 예를 들어 미리 정해진 바이너리 인코딩 포맷 및/또는 미리 정해진 스트림 데이터 컨테이너를 갖는 비디 오 및/또는 오디오 스트림을 수신하도록 구성될 수 있으며, 상기 포맷의 바이너리 비디오 및/또는 오디오 데이 터를 개별 비디오 프레임, 비디오 프레임 시퀀스 및/또는 시간 슬롯으로 구문 분석하도록 특별히 배열될 수 있 다. 중앙 서버는 또한 수집 기능로부터 비디오 및/또는 오디오 스트림 데이터(예를 들어, 바이너리 스트 림 데이터)를 수신하고 수신된 각 개별 데이터 스트림에 대해 해당 이벤트 감지를 수행하도록 배열되는 이벤트감지 기능를 포함할 수 있다. 이벤트 감지 기능는 해당 이벤트 감지를 수행하기 위한 AI(인공지능) 구성 요소(132a)를 포함할 수 있다. 이벤트 감지는 수집된 개별 스트림을 먼저 시간 동기화하지 않고도 수행될 수 있다. 중앙 서버는 수집 기능에서 제공하고 이벤트 감지 기능에 의해 처리되었을 수 있는 데이터 스트 림을 시간 동기화하도록 배열된 동기화 기능을 더 포함한다. 동기화 기능은 해당 시간 동기화를 수행 하기 위한 AI 구성 요소(133a)를 포함할 수 있다. 중앙 서버는 수신된 데이터 스트림 중 적어도 하나, 그러나 많은 경우 적어도 두 개, 예를 들어 적어도 세 개 또는 심지어 적어도 네 개, 예를 들어 모든 데이터 스트림의 조합을 기반으로 패턴 감지를 수행하도록 배열 되는 패턴 감지 기능을 더 포함할 수 있다. 패턴 감지는 이벤트 감지 기능에 의해 상기 데이터 스트 림 중 각각에 대해 감지된 하나 또는 일부 경우 적어도 두 개 이상의 이벤트를 기반으로 할 수 있다. 패턴 감지 기능에 의해 고려된 이러한 감지된 이벤트는 수집된 각 개별 스트림에 대해 시간에 따라 분포될 수 있다. 패턴 감지 기능은 상기 패턴 감지를 수행하기 위한 AI 구성 요소(134a)를 포함할 수 있다. 패턴 감지는 또 한 상기 논의된 그룹화를 기반으로 할 수 있으며, 특히 하나의 그룹에 대해서만; 일부 그룹에 대해서만; 모든 그룹에 대해서만 발생하는 특정 패턴을 감지하도록 배열될 수 있다. 중앙 서버은 수집 기능에서 제공된 데이터 스트림을 기반으로 하며, 또한 감지된 이벤트 및/또는 패 턴을 기반으로 하여 공유 디지털 비디오 스트림과 같은 생성된 디지털 비디오 스트림을 생성하도록 구성된 생성 기능를 더 포함한다. 이러한 생성된 비디오 스트림은 적어도 수집 기능에서 제공된 하나 이상의 비디 오 스트림을 포함하도록 생성된 비디오 스트림을 포함할 수 있으며, 원시, 재포맷 또는 변환된 비디오 스트림을 포함할 수 있으며, 또한 해당 오디오 스트림 데이터를 포함할 수 있다. 아래에서 예시하는 바와 같이, 여러 개 의 생성된 비디오 스트림이 있을 수 있으며, 여기서 이러한 생성된 비디오 스트림 중 하나는 위에서 논의한 방 식으로 생성될 수 있지만, 이미 생성된 다른 비디오 스트림을 기반으로 할 수 있다. 모든 생성된 비디오 스트림은 바람직하게는 연속적으로, 바람직하게는 거의 실시간으로(아래에서 논의되는 유형 의 대기 시간과 지연을 할인한 후) 생성된다. 중앙 서버는 상술된 바와 같이, 예를 들어 API을 통해 해당 생성생성된 디지털 비디오 스트림을 게시 하도록 배열된 게시 기능을 포함할 수 있다. 도 1, 2 및 3은 중앙 서버을 사용하여 본 명세서에 설명된 원리를 구현하고 특히 본 발명에 따른 방법을 제공하는 방법에 대한 세 가지 다른 예를 설명하지만, 하나 또는 여러 개의 비디오 통신 서비스을 사용하 거나 사용하지 않는 다른 구성도 가능한다. 따라서 도 5는 상기 생성된 디지털 비디오 스트림을 제공하는 방법을 설명한다. 도 6a-6f는 도 5에 설명된 방법 단계에서 발생하는 다양한 디지털 비디오/오디오 데이터 스트림 상태를 설명한다. 제1 단계에서 이 방법이 시작된다. 후속 수집 단계에서, 각각의 기본 디지털 비디오 스트림(210, 301)은 상기 수집 기능에 의해 상기 디지털 비디오 소스 120, 300 중 적어도 두 개에서 수집된다. 이러한 각 기본 데이터 스트림(210, 301)은 오디오 부분 및/또는 비디오 부분를 포함할 수 있다. 이 컨텍스트에서 \"비디오\"는 이러한 데이터 스트림의 움직 이는 이미지 및/또는 정지 이미지 콘텐츠를 의미한다. 각 기본 데이터 스트림(210, 301)은 (해당 기본 스트림 (210, 301)을 제공하는 개체에서 사용하는 해당 코덱을 사용하는) 모든 비디오/오디오 인코딩 사양에 따라 인코 딩될 수 있다. 인코딩 형식은 동일한 비디오 통신에서 동시에 사용되는 다른 기본 스트림(210, 301)에서 다를 수 있다. 기본 데이터 스트림(210, 301) 중 적어도 하나, 예를 들어 모든 것이 바이너리 데이터 스트림으로 제 공되는 것이 바람직하며, 이는 본래의 기존 데이터 컨테이너 데이터 구조로 제공될 수 있다. 기본 데이터 스트 림(210, 301) 중 적어도 하나, 예를 들어 적어도 두 개, 또는 모든 것이 각각의 라이브 비디오 녹화로 제공되는 것이 바람직하다. 기본 스트림(210, 301)은 수집 기능에서 수신될 때 시간 측면에서 동기화되지 않을 수 있다는 것에 유의한 다. 이는 서로에 대해 서로 다른 대기 시간 또는 지연과 연관되어 있음을 의미할 수 있다. 예를 들어, 두 개의 기본 비디오 스트림(210, 301)이 라이브 녹화인 경우, 이것은 수집 기능에서 수신될 때 녹화 시간과 관련 하여 서로 다른 대기 시간과 연관되어 있음을 의미할 수 있다. 또한 기본 스트림(210, 301) 자체가 웹 카메라의 해당 라이브 카메라 피드; 현재 공유되는 화면 또는 프레젠테 이션; 시청된 필름 클립 또는 이와 유사한 것; 또는 이러한 것들을 하나의 동일한 화면에 다양한 방식으로 배열 한 것의 조합일 수 있다는 것에 유의한다. 수집 단계는 도 6a 및 도 6b에 도시되어 있다. 도 6b에서, 수집 기능이 각 기본 비디오 스트림(210, 301) 을 묶음 오디오/비디오 정보 또는 연관된 비디오 스트림 데이터와 분리된 오디오 스트림 데이터로 저장할 수 있 는 방법도 도시되어 있다. 도 6b는 기본 비디오 스트림(210, 301) 데이터가 개별 프레임 또는 프레임의 컬 렉션/클러스터로 저장되는 방법을 도시하고, 여기서 \"프레임\"은 이미지 데이터 및/또는 연관된 오디오 데이터의 시간 제한 부분을 의미하며, 예를 들어 각 프레임은 개별 정지 이미지 또는 연속된 이미지 시리즈(예를 들어, 최대 1초의 움직이는 이미지를 구성하는 이러한 시리즈)가 함께 움직이는 이미지 비디오 콘텐츠를 형성한다. 이벤트 감지 기능에 의해 수행되는 후속 이벤트 감지 단계에서, 상기 기본 디지털 비디오 스트림(210, 301)은 상기 이벤트 감지 기능 및 특히 상기 AI 구성 요소(132a)에 의해 분석되어, 제1 이벤트 세트에서 선택된 적어도 하나의 이벤트을 감지한다. 이는 도 6c에 설명되어 있다. 이 이벤트 감지 단계는 적어도 하나, 예를 들어 적어도 두 개, 예를 들어 모든 기본 비디오 스트림(210, 301)에 대해 수행될 수 있으며, 이러한 기본 비디오 스트림(210, 301) 각각에 대해 개별적으로 수행될 수 있는 것이 바 람직하다. 다시 말해, 이벤트 감지 단계는 바람직하게 해당 특정 기본 비디오 스트림(210, 301)의 일부로 포함 된 정보만을 고려하고, 특히 다른 기본 비디오 스트림의 일부로 포함된 정보는 고려하지 않고, 해당 개별 기본 비디오 스트림(210, 301)에 대해 발생한다. 또한, 이벤트 감지는 바람직하게 여러 기본 비디오 스트림(210, 301)과 관련된 공통 시간 참조을 고려하지 않고 발생한다. 반면에, 이벤트 감지는 바람직하게는 0초, 예를 들어 적어도 0.1초, 예를 들어 적어도 1초보다 긴, 특정 시간 간격, 예를 들어 기본 비디오 스트림의 과거 시간 간격에 걸쳐 해당 개별 분석된 기본 비디오 스트림의 일부로 포함된 정보를 고려한다. 이벤트 감지는 해당 기본 비디오 스트림(210, 301)의 일부로 포함된 오디오 및/또는 비디오 데이터에 포함된 정 보를 고려할 수 있다. 제1 이벤트 집합은 여러 유형의 이벤트, 예를 들어, 해당 기본 비디오 스트림(210, 301)을 구성하거나 해당 기 본 비디오 스트림의 일부인 슬라이드 프레젠테이션의 슬라이드 변경; 해당 기본 비디오 스트림(210, 301)을 제 공하는 소스(120, 300)의 연결 품질 변경, 이에 의해 이미지 품질이 변경되거나 이미지 데이터가 손실되거나 이 미지 데이터가 다시 확보됨; 및 해당 기본 비디오 스트림(210, 301)에서 감지된 움직임 물리적 이벤트, 예를 들 어, 비디오에서 사람이나 물체의 움직임, 비디오의 조명 변경, 오디오의 갑자기 날카로운 노이즈 또는 오디오 품질 변경을 포함할 수 있다. 이는 전체 목록인 것으로 의도하지는 않았고, 이러한 예는 현재 설명된 원리의 적 용 가능성을 이해하기 위해 제공한 것이다. 동기화 기능에 의해 수행되는 후속 동기화 단계에서, 기본 디지털 비디오 스트림은 시간 동기화된다. 이 시간 동기화는 공통 시간 참조에 대한 것일 수 있다. 도 6d에 도시된 바와 같이, 시간 동기화는 예를 들어, 공통 시간 참조을 사용하여 기본 비디오 스트림(210, 301)을 서로에 대해 정렬하는 것을 포함할 수 있으므로, 이들을 결합하여 시간 동기화된 컨텍스트를 형성할 수 있다. 공통 시간 참조는 데이터 스트림, 하트비트 신호 또는 기타 펄스 데이터 또는 각 개별 기본 비디오 스트림(210, 301)에 적용 가능한 시간 앵커일 수 있다. 공통 시간 참조는 해당 기본 비디오 스트림(210, 301)의 정보적 내용이 공통 시간 축에 대해 공통 시 간 참조와 명확하게 관련될 수 있는 방식으로 각 개별 기본 비디오 스트림(210, 301)에 적용될 수 있다. 즉, 공 통 시간 참조는 시간 이동을 통해 기본 비디오 스트림(210, 301)이 정렬되어 현재 의미에서 시간 동기화되도록 할 수 있다. 다른 실시예에서, 시간 동기화는 해당 기본 비디오 스트림(210, 301) 간의 시간 차이에 대해 알려 진 정보에 기반, 예를 들어 측정에 기반할 수 있다. 도 6d에 도시된 바와 같이, 시간 동기화는 각 기본 비디오 스트림(210, 301)에 대해, 하나 또는 여러 개의 타임 스탬프을, 예를 들어, 공통 시간 참조과 관련하여 또는 각 비디오 스트림(210, 301)에 대해 다른 비 디오 스트림(210, 301) 또는 다른 비디오 스트림(210, 301)과 관련하여 결정하는 것을 포함할 수 있다. 패턴 감지 기능에 의해 수행되는 후속 패턴 감지 단계에서, 이렇게 시간 동기화된 기본 디지털 비디오 스 트림(210, 301)은 제1 패턴 세트에서 선택된 적어도 하나의 패턴를 감지하기 위해 분석된다. 이것은 도 6e 에 도시되어 있다. 이벤트 감지 단계와 대조적으로, 패턴 감지 단계는 공동으로 고려되는 시간 동기화된 기본 비디오 스트림(210, 301) 중 적어도 두 개의 일부로 포함된 비디오 및/또는 오디오 정보를 기반으로 수행하는 것이 바람직할 수 있다. 상기 제1 패턴 세트는 여러 유형의 패턴, 예를 들어, 몇 명의 참가자가 번갈아가며 또는 동시에 말하는 것; 또 는 다른 참가자가 말하는 것과 같이 프레젠테이션 슬라이드 변경이 다른 이벤트로 동시에 발생하는 것을 포함할 수 있다. 이 목록은 포괄적인 것은 아니고, 설명을 위한 것이다. 대체 실시예에서, 감지된 패턴는 여러 개의 상기 기본 비디오 스트림(210, 301)에 포함된 정보가 아니라 상기 기본 비디오 스트림(210, 301) 중 하나에만 포함된 정보와 관련될 수 있다. 이러한 경우, 이러한 패턴 는 적어도 두 개의 감지된 이벤트, 예를 들어 두 개 이상의 연속 감지된 프레젠테이션 슬라이드 변경 또는 연결 품질 변경에 걸쳐 있는 해당 단일 기본 비디오 스트림(210, 301)에 포함된 비디오 및/또는 오디오 정 보를 기반으로 감지되는 것이 바람직하다. 예를 들어, 시간이 지남에 따라 서로 빠르게 이어지는 여러 개의 연 속적인 슬라이드 변경이 감지된 각각의 슬라이드 변경 이벤트에 대한 하나의 개별적인 슬라이드 변경 패턴과 대 조적으로, 하나의 단일 슬라이드 변경 패턴으로 감지될 수 있다. 제1 이벤트 집합과 제1 패턴 집합은 매개변수 집합과 매개변수 간격의 각각의 집합을 사용하여 정의된 미리 정 해진 유형의 이벤트/패턴을 포함할 수 있다는 것을 알 수 있다. 아래에서 설명하는 바와 같이, 해당 집합의 이 벤트/패턴은 다양한 AI 도구를 사용하여 정의되고 감지될 수도 있다. 생성 기능에 의해 수행되는 후속 생성 단계에서, 공유 디지털 비디오 스트림은 시간 동기화된 기본 디지털 비디오 스트림(210, 301)의 연속적으로 고려된 프레임과 상기 감지된 패턴를 기반으로 출력 디지털 비디오 스트림으로 생성된다. 이하에서 설명하고 자세히 설명하겠지만, 본 발명은 출력 디지털 비디오 스트림과 같은 비디오 스트림의 완전 자동적 생성을 허용한다. 예를 들어, 이러한 생성은 어떤 기본 비디오 스트림(210, 301)에서 어떤 비디오 및/또는 오디오 정보를 선택하 여 이러한 출력 비디오 스트림에서 어느 정도 사용할 것인지; 출력 비디오 스트림의 비디오 화면 레 이아웃; 시간에 따른 이러한 다양한 용도 또는 레이아웃 간의 전환 패턴; 등을 포함할 수 있다. 이는 또한 (예를 들어, 상기 공통 시간 참조에) 시간 동기화되어 시간 동기화된 기본 비디오 스트림(210, 301)과 함께 출력 비디오 스트림의 생성에 사용될 수 있는 추가의 디지털 비디오 정보 스트림과 같은, (공 통 시간 참조과 관련될 수 있는) 하나 이상의 추가 시간 관련 디지털 비디오 정보를 또한 도시하는 도 6f에 도시되어 있다. 예를 들어, 추가 스트림은 감지된 패턴을 기반으로 동적으로 사용할 수 있는 비디 오 및/또는 오디오 특수 효과; 비디오 통신을 위한 계획된 시간 일정 등과 관련된 정보를 포함할 수 있다. 게시 기능에 의해 수행되는 후속 게시 단계에서, 생성된 출력 디지털 비디오 스트림은 상술된 바와 같이 생성된 디지털 비디오 스트림의 소비자(110, 150)에게 지속적으로 제공된다. 생성된 디지털 비디오 스트림 은 비디오 통신 서비스 등을 통해 하나 이상의 참여자 클라이언트에게 제공될 수 있다. 후속 단계에서, 방법은 종료된다. 그러나 먼저 이 방법은 출력 비디오 스트림을 지속적으로 제공되는 스트 림으로 생성하기 위해서, 도 5에 도시된 바와 같이 어떤 회수라도 반복할 수 있다. 바람직하게는, 출력 비디오 스트림은 실시간 또는 거의 실시간으로 (모든 단계에서 추가된 총 대기 시간을 고려함), 및 지속적으로(추 가 정보가 제공될 때 즉시 게시되지만 아래에 설명된 의도적으로 추가된 대기 시간이나 지연은 계산하지 않음) 소비되도록 생성된다. 이런 방식으로, 출력 비디오 스트림은 대화형 방식으로 소비될 수 있으므로, 출력 비디오 스트림은 비디오 통신 서비스으로 또는 수집 기능에 다시 공급되는 기본 비디오 스트림 을 생성하기 위한 기반을 형성하는 다른 컨텍스트로 다시 피드백되어 닫힌 피드백 루프를 형성할 수 있거 나; 출력 비디오 스트림이 다른(시스템 외부 또는 적어도 중앙 서버 외부) 컨텍스트에서 소비될 수 있지만, 이를 통해 실시간 대화형 비디오 통신의 기반을 형성할 수 있다. 상술한 바와 같이, 일부 실시예에서는 상기 기본 디지털 비디오 스트림(210, 301) 중 적어도 두 개, 예를 들어 적어도 세 개, 예를 들어 적어도 네 개, 또는 적어도 다섯 개가 상기 비디오 통신 서비스에서 제공하는 것 과 같은 공유 디지털 비디오 통신의 일부로 제공되고, 해당 비디오 통신에는 해당 기본 디지털 비디오 스트림 을 제공하는 각각의 원격으로 연결된 참여자 클라이언트이 포함된다. 이러한 경우, 수집 단계는 예를 들어, 해당 비디오 통신 서비스 내부로부터 비디오 및/또는 오디오 스트림 데이터에 대한 액세스가 부여되 는 자동 참여자 클라이언트을 통해 및/또는 비디오 통신 서비스의 API를 통해, 공유 디지털 비 디오 통신 서비스 자체로부터 적어도 하나의 상기 기본 디지털 비디오 스트림을 수집하는 것을 포함할 수 있다. 또한, 이 경우 및 그 외 경우에서, 수집 단계는 상기 기본 디지털 비디오 스트림(210, 301) 중 적어도 하나를 공유 디지털 비디오 통신 서비스 외부에 있는 정보 소스에서 수집된 각각의 외부 디지털 비디오 스트 림로 수집하는 것을 포함할 수 있다. 사용된 이러한 외부 비디오 소스 중 하나 또는 여러 개가 중앙 서버 외부에 있을 수도 있다는 점에 유의한다. 일부 실시예에서, 기본 비디오 스트림(210, 301)은 동일한 방식으로 포맷되지 않는다. 이러한 서로 다른 포맷은 서로 다른 유형의 데이터 컨테이너(예를 들어, AVI 또는 MPEG)의 수집 기능에 전달되는 형태일 수 있지만, 바람직한 실시 예에서 기본 비디오 스트림(210, 301) 중 적어도 하나는 (해당 기본 비디오 스트림(210, 301) 중 적어도 하나와 비교했을 때) 편차 기본 디지털 비디오 스트림(210, 301)이 편차 비디오 인코딩, 편차 고정 또는 가변 프레임 속도, 편차 종횡비, 편차 비디오 해상도 및/또는 편차 오디오 샘플 속도를 갖는 측면에서 편차 포 맷에 따라 포맷된다. 수집 기능은 모든 수집된 기본 비디오 스트림(210, 301)에서 발생하는 모든 인코딩 형식, 컨테이너 표준 등을 읽고 해석하도록 미리 구성되는 것이 바람직하다. 이렇게 하면 본 명세서에서 설명한 대로 처리를 수행할 수 있으며, 프로세스의 비교적 후반부까지 (예를 들어, 해당 기본 스트림이 해당 버퍼에 들어간 후, 이벤트 감 지 단계 후, 심지어 이벤트 감지 단계 후까지) 디코딩이 필요하지 않다. 그러나 기본 비디오 피드(210, 301) 중 하나 이상이 수집 기능이 디코딩 없이는 해석할 수 없는 코덱을 사용하여 인코딩되는 드문 경우, 수집 기 능은 이러한 기본 비디오 스트림(210, 301)의 디코딩 및 분석을 수행하고, 이어서 예를 들어 이벤트 감지 기능에서 처리할 수 있는 형식으로의 변환을 수행하도록 배열될 수 있다. 이 경우에도, 이 단계에서는 재인코딩 을 수행하지 않는 것이 바람직한 것에 유의한다. 예를 들어, 비디오 통신 서비스에 의해 제공되는 것과 같이, 다자간 비디오 이벤트로부터 기본 비디오 스 트림을 가져오면 일반적으로 낮은 대기 시간에 대한 요구 사항을 가지므로 참가자가 효과적인 통신을 할 수 있도록 하는 가변 프레임 속도 및 가변 픽셀 해상도와 일반적으로 연관된다. 다시 말해, 전체 비디오 및 오디오 품질은 낮은 대기 시간을 위해 필요에 따라 감소된다. 반면 외부 비디오 피드는 일반적으로 더 안정적인 프레임 속도, 더 높은 품질을 갖지만 따라서 더 높은 대 기 시간을 가질 수 있다. 따라서 비디오 통신 서비스은 각 시점에서 외부 비디오 소스와는 다른 인코딩 및/또는 컨테이너를 사 용할 수 있다. 따라서 이 경우 본 명세서에서 설명된 분석 및 비디오 생성 프로세스는 결합된 경험을 위해 서로 다른 형식의 스트림(210, 301)을 새로운 스트림에 결합해야 한다. 상술된 바와 같이, 수집 기능은 형식별 수집 기능(131a)의 집합을 포함할 수 있으며, 각각은 특정 유형의 형식의 기본 비디오 스트림(210, 301)을 처리하도록 배열되어 있다. 예를 들어, 이러한 형식별 수집 기능(131a) 각각은 Windows® Media® 또는 DivX®와 같은 다른 비디오 각각의 인코딩 방법/코덱을 사용하여 인코딩된 기본 비디오 스트림(210, 301)을 처리하도록 배열될 수 있다. 그러나 바람직한 실시예에서, 수집 단계는 기본 디지털 비디오 스트림(210, 301) 중 적어도 두 개, 예를 들어 모든 것을 공통 프로토콜으로 변환하는 단계를 포함한다. 이 컨텍스트에서 사용되는 용어 \"프로토콜\"은 디지털 비디오/오디오 스트림에 포함된 정보를 저장하는 방법을 지정하는 정보 구조화 표준 또는 데이터 구조를 말한다. 그러나 공통 프로토콜은 디지털 비디오 및/또는 오디오 정보를 예를 들어 바이너리 수준(즉, 사운드와 이미지 자체를 지시하는 인코딩/압축된 데이터)으로 저장하는 방 법을 지정하지 않고, 대신 이러한 데이터를 저장하기 위한 미리 결정된 형식의 구조를 형성한다. 다시 말해, 공 통 프로토콜은 이러한 저장과 관련하여 디지털 비디오 디코딩이나 디지털 비디오 인코딩을 수행하지 않고 원시 바이너리 형태로 디지털 비디오 데이터를 저장하도록 규정한다. 기존 바이너리 형태를 전혀 수정하지 않고 바이 너리 형태 바이트 시퀀스를 연결 및/또는 분할할 수 있다. 대신에, 이 원시 바이너리 데이터를 프로토콜에 의해 정의된 데이터 구조로 다시 패키징하면서, 해당 기본 비디오 스트림(210, 301)의 원시 (인코딩/압축된) 바이너 리 데이터 내용은 유지된다. 일부 구현예에서, 공통 프로토콜은 비디오 파일 컨테이너 형식을 정의한다. 도 7은 예를 들어, 도 6a에 도시된 기본 비디오 스트림(210, 301)을 설명하는데, 이는 해당 형식별 수집 기능 (131a)에 의해 재구성되고 상기 공통 프로토콜을 사용한다. 따라서, 공통 프로토콜은 디지털 비디오 및/또는 오디오 데이터를, 바람직하게는 해당 기본 비디오 스트림 (210, 301)에 관한 시간선을 따라 이산적이고 연속적인 데이터 세트로 나뉘는, 데이터 세트에 저장하도록 규정한다. 각각의 이러한 데이터 세트는 하나 이상의 비디오 프레임과 연관된 오디오 데이터를 포함할 수 있다. 공통 프로토콜은 또한 저장된 디지털 비디오 및/또는 오디오 데이터 세트와 관련하여 지정된 시간 지 점과 연관된 메타데이터를 저장하도록 규정할 수 있다. 메타데이터는 예를 들어, 해당 원시 바이너리 데이터; 비디오 데이터의 해상도, 비디오 프레임 속도, 프레 임 속도 가변성 플래그, 비디오 해상도, 비디오 종횡비, 오디오 압축 알고리즘 또는 오디오 샘플링 속도를 생성 하는 데 사용된 디지털 비디오 인코딩 방법 또는 코덱에 관하여, 해당 기본 디지털 비디오 스트림의 원시 바이너리 형식에 대한 정보를 포함할 수 있다. 메타데이터는 또한 해당 기본 비디오 스트림(210, 301)의 시간 참조와 관련하여 또는 상술된 바와 같은 다른 비디오 스트림과 관련하여, 저장된 데이터의 타임스탬프에 대한 정보를 포함할 수 있다. 해당 공통 프로토콜과 함께 해당 형식별 수집 기능(131a)를 사용하면 수신된 비디오/오디오 데이터를 디코 딩/재인코딩하여 대기 시간을 추가하지 않고 기본 비디오 스트림(210, 301)의 정보 콘텐츠를 빠르게 수집할 수 있다. 따라서, 수집 단계는 해당 기본 비디오 스트림(210, 301)을 구문 분석하고 구문 분석된 원시 및 이진 데이터를 공통 프로토콜을 사용하여 데이터 구조에 저장하고 관련 메타데이터를 함께 저장하기 위해서, 서로 다른 이진 비디오 및/또는 오디오 인코딩 형식을 사용하여 인코딩된 기본 디지털 비디오 스트림(210, 301)을 수집하기 위 한 상기 형식별 수집 기능(131a) 중 서로 다른 기능을 사용하는 것을 포함할 수 있다. 당연히, 어떤 기본 비디 오 스트림(210, 301)에 어떤 형식별 수집 기능(131a)를 사용할지 결정하는 것은 해당 기본 비디오 스트림(210, 301) 각각의 미리 결정되거나 동적으로 감지된 속성에 기반하여 수집 기능에 의해 수행될 수 있다. 따라서 수집된 각 기본 비디오 스트림(210, 301)은 중앙 서버의 RAM 메모리 버퍼와 같은 자체 별도 메모리 버퍼에 저장될 수 있다. 따라서 각 형식별 수집 기능(131a)에 의해 수행되는 기본 비디오 스트림(210, 301)의 변환은 이렇게 변환된 각 기본 디지털 비디오 스트림(210, 301)의 원시 이진 데이터를 정렬된 더 작은 데이터 세트로 분할하는 것을 포함할 수 있다. 또한, 변환은 예를 들어, 공통 시간 참조과 관련하여, 상기 적은 세트의 각각 (또는 해당 기본 스트 림(210, 301)의 각각의 시간선을 따라 규칙적으로 분포된 부분 집합과 같은 부분 집합)을 공유 시간선을 따라 각각의 시간과 연관시키는 단계를 포함할 수 있다. 이 연관은 후술되는 주요 방법 중 하나 또는 다른 방법으로 원시 바이너리 비디오 및/또는 오디오 데이터를 분석하여 수행할 수 있으며, 기본 비디오 스트림(210, 301)의 후속 시간 동기화를 수행할 수 있기 위해 수행될 수 있다. 사용된 공통 시간 참조의 유형에 따라, 각 데이터 세 트의 이 연관의 적어도 일부는 동기화 기능에 의해 또한 또는 대신에 수행될 수 있다. 후자의 경우, 수집 단계는 대신 더 작은 세트 각각이나 하위 집합을 해당 기본 스트림(210, 301)에 대해 특정적인 타임 라인의 각각의 시간과 연관시키는 단계를 포함할 수 있다. 일부 실시예에서, 수집 단계는 또한 기본 비디오 스트림(210, 301)에서 수집된 원시 바이너리 비디오 및/또는 오디오 데이터를 균일한 품질 및/또는 업데이트 주파수로 변환하는 것을 포함한다. 이것은 필요에 따라 기본 디 지털 비디오 스트림(210, 301)의 해당 원시 바이너리 디지털 비디오 및/또는 오디오 데이터를 공통 비디오 프레 임 속도, 공통 비디오 해상도 또는 공통 오디오 샘플링 속도로 다운샘플링 또는 업샘플링하는 것을 포함할 수 있다. 이러한 재샘플링은 해당 형식별 수집 기능(131a)이 올바른 바이너리 인코딩 대상 형식에 따라 원시 바이 너리 데이터를 직접 처리할 수 있기 때문에 전체 디코딩/재인코딩을 수행하지 않고 또는 전혀 디코딩을 수행하 지 않고도 수행될 수 있다는 점에 유의한다. 상기 기본 디지털 비디오 스트림(210, 301) 각각은 개별 프레임 또는 상기 설명된 프레임의 시퀀스로 서 개별 데이터 저장 버퍼에 저장될 수 있으며, 또한 각각은 상기 공통 시간 참조과 차례로 연관된 해당 타임 스탬프와 연관될 수 있다. 이러한 원리를 설명하기 위해 제공되는 구체적인 예에서, 비디오 통신 서비스는 동시 참가자를 포함 하는 화상 회의를 실행하는 Microsoft® Teams®이다. 자동 참여자 클라이언트은 Teems® 회의에서 회의 참 가자로 등록된다. 그런 다음, 기본 비디오 입력 신호는 자동 참여자 클라이언트을 통해 수집 기능에 사용할 수 있 고 이에 의해 수집된다. 이들은 H264 형식의 원시 신호이며 모든 비디오 프레임에 대한 타임스탬프 정보를 포함 한다. 관련 형식별 수집 기능(131a)은 구성 가능한 사전 정의된 TCP 포트에서 IP(클라우드 내 LAN 네트워크)를 통해 원시 데이터를 수집한다. 모든 Teems® 회의 참가자와 관련 오디오 데이터는 별도의 포트와 연결된다. 그런 다음 수집 기능은 오디오 신호(50Hz)의 타임스탬프를 사용하고 비디오 스트림을 해당 개별 버퍼에 저 장하기 전에 비디오 데이터를 25Hz의 고정 출력 신호로 다운샘플링한다. 언급한 바와 같이, 공통 프로토콜은 원시 바이너리 형태로 데이터를 저장할 수 있다. 매우 낮은 수준으로 설계되어 비디오/오디오 데이터의 원시 비트와 바이트를 처리할 수 있다. 바람직한 실시예에서 데이터는 공통 프로토콜에 간단한 바이트 배열 또는 해당 데이터 구조(예를 들어, 슬라이스)로 저장된다. 이는 데이터를 기존 비디오 컨테이너에 넣을 필요가 전혀 없다는 것을 의미한다(이 컨텍스트에서 해당 공통 프로토콜은 기존 컨테이너를 구성하지 않음). 또한, 비디오 인코딩 및 디코딩은 계산량이 많고, 이는 지연이 발생하고 값비 싼 하드웨어를 필요로 하는 것을 의미한다. 게다가 이 문제는 참여자 수에 따라 증가한다. 공통 프로토콜을 사용하면, 각 Teams® 회의 참가자와 관련된 기본 비디오 스트림 및 모든 외부 비디오 소스에 대해 메모리를 수집 기능에서 예약한 다음에, 프로세스 중에 할당된 메모리 양을 즉시 변경하는 것이 가능하다. 이렇게 하면 입력 스트림의 수를 변경하고 결과적으로 각 버퍼를 효과적으로 유지하는 것이 가능하게 된다. 예를 들어 해상도, 프레임 속도 등의 정보는 가변적일 수 있지만 공통 프로토콜에 메 타데이터로 저장되기 때문에, 이 정보를 사용하여 필요에 따라 각 버퍼의 크기를 빠르게 조정할 수 있다. 다음은 현재 유형의 공통 프로토콜 사양의 예이다. 바이트 예제 설명 1바이트 1 0=비디오; 1=오디오 4바이트 1234567 버퍼 길이(정수) 8바이트 424234234 수신오디오/비디오버퍼의 타임스탬프, 틱 단위로 측정, 1틱 = 100ns. (긴 정수) 1바이트 0 VideoColorFormat { NV12 = 0, Rgb24 = 1, Yuy2 = 2, H264 = 3 } 4바이트 720 비디오 프레임 픽셀 높이(정수) 4바이트 640 비디오 프레임 픽셀 너비(정수) 4바이트 25.0 비디오 프레임 속도 초당 프레임 수(부동소수점) 1바이트 0 오디오가 없는가? 1 = true; 0 = false 1바이트 0 AudioFormat { 0 = Pcm16K 1 = Pcm44KStereo } 1바이트 0 있는 경우 감지된 이벤트 0 = 이벤트 없음 1, 2, 3 등=지정된유형의 이벤트 감지 30바이트 향후 사용을 위해 예약 8바이트 1000000 바이트 단위의 이진 데이터 길이(long int) 변수 0x87A879… 이 프레임(들)의 원시 이진 비디오/오디오 데이터 4바이트 1234567 우세한 화자 포트 4바이트 1234567 활동 화자 상기 \"있는 경우 감지된 이벤트\" 데이터는 공통 프로토콜 사양의 일부로 포함된다. 그러나 일부 실시예에 서, (감지된 이벤트에 관한) 이 정보는 대신 별도의 메모리 버퍼에 저장될 수 있다. 일부 실시예에서, 오버레이 또는 효과일 수 있는 적어도 하나의 추가 디지털 비디오 정보은 또한 해당 개 별 버퍼에, 해당 공통 시간 참조과 차례로 연관된 해당 시간 스탬프와 각각 연관되는 개별적 프레임 또는 프레임 시퀀스로 저장된다. 위에서 예시한 바와 같이, 이벤트 감지 단계는 해당 이벤트이 감지된 기본 디지털 비디오 스트림(210, 301)과 연관된 감지된 이벤트을 설명하는 메타데이터를 상기 공통 프로토콜을 사용하여 저장하 는 것을 포함할 수 있다. 이벤트 감지는 다양한 방식으로 수행될 수 있다. AI 구성 요소(132a)에 의해 수행되는 일부 실시예에서, 이벤트 감지 단계는 제1 훈련된 신경망 또는 기타 머신 러닝 구성 요소가 상기 이벤트 중 하나를 자동으로 감지하 기 위해 상기 기본 디지털 비디오 스트림(210, 301) 중 하나 이상, 예를 들어 여러 개 또는 심지어 전부를 개별 적으로 분석하는 것을 포함한다. 이것은 AI 구성 요소(132a)가 관리된 분류에서 기본 비디오 스트림(210, 301) 데이터를 미리 정의된 이벤트 집합으로 분류하거나, 관리되지 않은 분류에서 동적으로 결정된 이벤트 집합으로 분류하는 단계를 포함할 수 있다. 일부 실시예에서, 감지된 이벤트은 프레젠테이션의 프레젠테이션 슬라이드의 변경이 해당 기본 비디오 스 트림(210, 301)에 있거나 이에 포함되는 것이다. 예를 들어, 프레젠테이션의 발표자가 그 당시 청중에게 제공하고 있는 프레젠테이션의 슬라이드를 변경하기로 결정하면, 이것은 주어진 시청자에게 흥미로운 것이 변경될 수 있음을 의미한다. 새로 표시된 슬라이드는 소위 \"나비\" 모드에서 간략하게 가장 잘 볼 수 있는 높은 수준의 그림일 수 있다(예를 들어, 출력 비디오 스트림 에서 발표자의 비디오와 나란히 슬라이드를 표시하는 것). 또는, 슬라이드는 많은 세부 정보, 작은 글꼴 크기의 텍스트 등을 포함할 수 있다. 후자의 경우, 슬라이드는 대신 전체 화면으로 표시되어야 하며, 일반적으 로 발생하는 것보다 다소 긴 시간 동안 표시되어야 한다. 나비 모드는 적절하지 않을 수 있는데, 이 경우 슬라 이드가 발표자의 얼굴보다 프레젠테이션을 보는 사람에게 더 흥미로울 수 있기 때문이다. 실제로, 이벤트 감지 단계는 다음 중 하나 이상을 포함할 수 있다. 첫째, 이벤트은 감지된 슬라이드의 제1 이미지와 감지된 슬라이드의 후속 제2 이미지 간의 차이에 대한 이 미지 분석을 기반으로 감지될 수 있다. 슬라이드를 보여주는 주 비디오 스트림(220, 301)의 특성은 예를 들어 광학 문자 인식(OCR)과 함께 동작 감지를 사용하여, 기존의 디지털 이미지 처리를 사용하여 자동으로 결정될 수 있다. 이것은 자동 컴퓨터 이미지 처리 기술을 사용하여 감지된 슬라이드가 실제로 슬라이드 변경으로 분류할 만큼 상 당히 변경되었는지 확인하는 단계를 포함할 수 있다. 이것은 RGB 색상 값과 관련하여 현재 슬라이드와 이전 슬라이드 간의 델타를 확인하여 수행될 수 있다. 예를 들어, 해당 슬라이드가 덮은 화면 영역에서 RGB 값이 전역 적으로 얼마나 변경되었는지 평가하고, 함께 속하고 함께 변경되는 픽셀 그룹을 찾는 것이 가능한지 확인할 수 있다. 이런 식으로 관련 슬라이드 변경 사항을 감지하는 동시에 화면에서 표시된 컴퓨터 마우스 움직임과 같은 무관한 변경 사항은 필터링할 수 있다. 이 접근 방식은 또한 완전한 구성 가능성을 허용하며, 예를 들어 발표자 가 여러가지를 가리키면서 컴퓨터 마우스를 사용하여 무언가를 자세히 발표하려는 경우, 컴퓨터 마우스 움직임 을 캡처할 수 있는 것이 바람직한 경우가 있다. 둘째, 이벤트는 해당 제2 이미지 자체의 정보적 복잡성에 대한 이미지 분석을 기반으로 감지하여 이벤트 유형을 보다 구체적으로 결정할 수 있다. 이것은 예를 들어, 해당 슬라이드의 총 텍스트 정보, 뿐만 아니라 관련 글꼴 크기를 평가하는 것을 포함할 수 있다. 이것은 딥 러닝 기반 문자 인식 기술과 같은 기존 OCR 방법을 사용하여 수행할 수 있다. 평가된 비디오 스트림(210, 301)의 원시 바이너리 형식이 알려져 있기 때문에, 이것은 비디오 데이터를 먼저 디코딩하거나 재인코딩하지 않고 바이너리 도메인에서 직접 수행될 수 있다는 것에 유의한다. 예를 들어, 이벤 트 감지 기능는 이미지 해석 서비스에 대한 관련 형식별 수집 기능을 호출할 수 있거나, 이벤트 감지 기능 자체가 여러 지원되는 원시 바이너리 비디오 데이터 형식에 대한 개별 픽셀 수준과 같은 이미지 정보를 평가하는 기능을 포함할 수 있다. 다른 예에서, 감지된 이벤트는 참여자 클라이언트과 디지털 비디오 통신 서비스의 통신 연결이 끊어진 것이다. 그런 다음, 감지 단계는 해당 참여자 클라이언트에 해당하는 기본 디지털 비디오 스트림 의 일련의 후속 비디오 프레임에 대한 이미지 분석에 근거하여, 해당 참여자 클라이언트이 통신 연결이 끊어졌음을 감지하는 것을 포함할 수 있다. 참여자 클라이언트는 서로 다른 물리적 위치와 서로 다른 인터넷 연결과 연관되어 있기 때문에, 누군가가 비디오 통신 서비스 또는 중앙 서버과의 연결이 끊어질 수 있다. 그러한 상황에서, 생성된 출력 비디 오 스트림에 검은색 또는 빈 화면을 표시하는 것은 피하는 것이 좋다. 대신, 이러한 연결 손실은 예를 들어, 사용된 2개의 클래스가 연결/연결되지 않은(데이터 없음) 2-클래스 분류 알고리즘을 적용하여, 이벤트 감지 기능에 의해 이벤트로 감지될 수 있다. 이 경우, \"데이터 없음\"은 발표 자가 의도적으로 검은색 화면을 보내는 것과 다르다는 것이 이해되어야 한다. 1개 또는 2개 프레임과 같은 짧은 검은색 화면은 최종 프로덕션 스트림에서 눈에 띄지 않을 수 있으므로, 시간에 따라 상기 2-클래스 분류 알고리즘을 적용하여 시계열을 생성할 수 있다. 그런 다음 연결 중단에 대한 최소 길이를 지정하는 임계값을 사 용하여 연결이 끊어졌는지 여부를 결정할 수 있다. 이하 설명하는 바와 같이, 이러한 예시된 유형의 감지된 이벤트는 패턴 감지 기능에 의해 적합하고 원하는 대로 다양한 조치를 취하는 데 사용될 수 있다. 언급된 바와 같이, 개별 기본 비디오 스트림(210, 301)은 각각 공통 시간 참조 또는 시간 영역에서 서로 관련될 수 있으며, 이에 의해 동기화 기능이 서로에 대해 시간 동기화할 수 있다. 일부 실시예에서, 공통 시간 참조은 공통 오디오 신호를 기반으로 하거나 이를 포함하며(도 1-3 참조), 공통 오디오 신호는 위에서 설명한 바와 같이 적어도 두 개의 원격으로 연결된 참여자 클라이언트 을 포함하는 공유 디지털 비디오 통신 서비스에 공통이며, 각각은 해당 기본 디지털 비디오 스트림 중 각각의 것을 제공한다. 위에서 논의한 Microsoft® Teams®의 예에서, 공통 오디오 신호는 자동 참여자 클라이언트 및/또는 API를 통해 중앙 서버에 의해 생성되어 캡처될 수 있다. 이 예와 다른 예에서, 이러한 공통 오디오 신호는 하트비트 신호로 사용되어 이 하트비트 신호를 기반으로 각각을 특정 시간 지점에 바인딩하여 개별 기본 비디오 스트림을 시간 동기화할 수 있다. 이러한 공통 오디오 신호는 (각 다른 기본 비디오 스트림과 관련하여) 별도의 신호로 제공될 수 있으며, 이로 인해 해당 다른 기본 비디오 스트림에 포함된 오디오를 기반으로 하거나 심지어 여기에 포함된 이미지 정보를 기반으로 하여 다른 기본 비디오 스트림은 각각 공 통 오디오 신호와 개별적으로 시간 상관될 수 있다(예를 들어, 자동 이미지 처리 기반 립싱크 기술 사용하여). 다시 말해서, 개별 기본 비디오 스트림과 관련된 모든 변수 및/또는 다른 대기 시간을 처리하고, 결합된 비디오 출력 스트림에 대한 시간 동기화를 달성하기 위해서, 이러한 공통 오디오 신호는 중앙 서버의 모든 기본 비디오 스트림에 대한 하트비트로 사용된다(하지만 외부 기본 비디오 스트림은 아닐 수 있음). 다시 말해서, 다른 모든 신호는 이 공통 오디오 시간 하트비트에 매핑되어 모든 것이 시간 동기화되는 것 을 확실하게 한다. 다른 예에서, 시간 동기화는 출력 디지털 비디오 스트림에 도입되고 참여자 클라이언트 중 하나 또는 여러 개의 개별적인 것의 일부로 제공되는 해당 로컬 시간 동기화 소프트웨어 기능에 의해 감지되는 시간 동기화 요소을 사용하여 달성되며, 로컬 소프트웨어 기능는 출력 비디오 스트림에서 시간 동기 화 요소의 도착 시간을 감지하도록 배열된다. 이해되는 바와 같이, 이러한 구현예에서 출력 비디오 스트림 은 비디오 통신 서비스으로 다시 공급되거나 다르게는 각 참여자 클라이언트 및 해당 로컬 소프 트웨어 기능에 이용하게 된다. 예를 들어, 시간 동기화 요소는 시각적 마커, 예를 들어, 정기적인 시간 간격으로 출력 비디오에 배 치되거나 업데이트되는 미리 정해진 순서 또는 방식으로 색상을 변경하는 픽셀; 출력 비디오에서 업데이트 되고 표시되는 시각적 시계; (예를 들어, 충분히 낮은 진폭 및/또는 충분히 높은 주파수를 갖는 방식으로 참가 자에게 들리지 않도록 설계될 수 있는) 출력 비디오 스트림의 오디오 형성 부분에 추가되는 사운드 신호일 수 있다. 로컬 소프트웨어 기능는 적절한 이미지 및/또는 오디오 처리를 사용하여 각각의 시간 동 기화 요소의 각각의 도착 시간을 자동으로 감지하도록 배열된다. 그런 다음, 공통 시간 참조는 적어도 상기 감지된 도착 시간을 기준으로 결정될 수 있다. 예를 들어, 각 로컬 소프트웨어 기능는 상기 감지된 도착 시간을 나타내는 각각의 정보를 중앙 서버에 전달할 수 있 다. 이러한 통신은 해당 참여자 클라이언트과 중앙 서버 간의 직접 통신 링크를 통해 이루어질 수 있다. 그러나, 통신은 또한 해당 참여자 클라이언트와 연관된 기본 비디오 스트림을 통해 이루어질 수도 있 다. 예를 들어, 참여자 클라이언트는 중앙 서버에 의한 자동 감지를 위해 해당 참여자 클라이언트 에 의해 생성되고 공통 시간 참조을 결정하는 데 사용되는 기본 비디오 스트림에 상술된 유형과 같은 시각적 또는 청각적 코드를 도입할 수 있다. 또 다른 예에서, 각 참여자 클라이언트은 상술된 것과 일치하는 방식으로, 모든 참여자 클라이언트에 의해 비디오 통신 서비스에서 볼 수 있는 공통 비디오 스트림에서 이미지 감지를 수행하고 이러한 이미지 감지의 결과를 중앙 서버에 중계할 수 있으므로, 시간이 지남에 따라 각 참여자 클라이언트의 서로에 대한 각각의 오프셋을 결정하는 데 사용된다. 이런 식으로 공통 시간 참조가 개별 상대 오프셋 세트로 결 정될 수 있다. 예를 들어, 일반적으로 사용 가능한 비디오 스트림의 선택된 참조 픽셀은 여러 또는 모든 참여자 클라이언트(예를 들어, 로컬 소프트웨어 기능)에 의해 모니터링될 수 있으며, 해당 픽셀의 현재 색상 은 중앙 서버에 전달될 수 있다. 중앙 서버는 다수의 (또는 모든) 참여자 클라이언트 각각으로 부터 연속적으로 수신된 이러한 색상 값을 기반으로 각각의 시간 시리즈를 계산하고, 상호 상관을 수행하여 서 로 다른 참여자 클라이언트에 대한 상대적 시간 오프셋의 추정 세트를 생성한다. 실제로, 비디오 통신 서비스에 공급되는 출력 비디오 스트림은 해당 비디오 통신의 모든 참여자 클라 이언트의 공유 화면의 일부로서 포함될 수 있고, 이에 따라 참여자 클라이언트와 관련된 시간 오프셋을 평 가하는 데 사용될 수 있다. 특히, 비디오 통신 서비스에 공급되는 출력 비디오 스트림은 자동 참여자 클라이언트 및/또는 API를 통해 중앙 서버에 다시 이용 가능해질 수 있다. 일부 실시예에서, 공통 시간 기준은 상기 1차 디지털 비디오 스트림(210, 301) 중 제1 스트림의 오디오 부 분과 상기 1차 디지털 비디오 스트림(210, 301)의 이미지 부분 사이의 감지된 불일치에 적어도 부분 적으로 기초하여 결정될 수 있다. 이러한 불일치는 예를 들어 해당 상기 제1 기본 디지털 비디오 스트림(210, 301)에서 보이는 말하는 참가자의 디지털 립싱크 비디오 이미지 분석에 기초할 수 있다. 이러한 립싱크 분 석은 그 자체로 기존 방식이며, 예를 들어 훈련된 신경망을 사용할 수 있다. 분석은 이용 가능한 공통 오디오 정보와 관련하여 각 기본 비디오 스트림(210, 301)에 대해 동기화 기능에 의해 수행될 수 있고, 개별 기본 비디오 스트림(210, 301)에 걸친 상대적인 오프셋은 이 정보에 기초하여 결정될 수 있다. 일부 실시예에서, 동기화 단계는 의도적으로 최대 30초, 예를 들어 최대 5초, 최대 1초, 최대 0.5초이지만, 0초 보다 긴 지연(이 컨텍스트에서 용어 \"지연\"과 \"대기 시간\"은 동일한 의미임)을 도입하는 것으로 구성되므로, 출 력 디지털 비디오 스트림에는 적어도 상기 지연이 제공된다. 여하튼, 의도적으로 도입된 지연은 수집 단계 에서 리샘플링 후에 저장된 프레임(또는 개별 이미지)의 수와 같이, 적어도 몇개의 비디오 프레임, 예를 들어, 적어도 3개, 심지어는 적어도 5개 또는 심지어 10개의 비디오 프레임이다. 본 명세서에 사용된 바와 같이, \"의도적으로\"라는 용어는 동기화 문제 등을 기반으로 이러한 지연을 도입할 필요성과 관계없이 지연이 도입되는 것 을 의미한다. 다시 말해서, 의도적으로 도입된 지연은 하나를 다른 것에 대해 시간 동기화하기 위해서 기본 비 디오 스트림(210, 301)의 동기화의 일부로 도입된 지연에 추가로 도입된다. 의도적으로 도입된 지연은 공통 시 간 기준과 관련하여 미리 결정되거나 고정되거나 가변적일 수 있다. 지연 시간은 기본 비디오 스트림(210, 301) 중 최소 잠재 스트림과 관련하여 측정될 수 있으므로, 상기 시간 동기화의 결과로 이들 스트림(210, 301) 중 더 많은 잠재적인 것이 상대적으로 더 작은 의도적으로 추가된 지연과 연관된다. 일부 실시예에서, 0.5초 이하와 같이 상대적으로 작은 지연이 발생한다. 이러한 지연은 출력 비디오 스트림 을 사용하는 비디오 통신 서비스에 대한 참가자가 거의 인지할 수 없을 것이다. 다른 실시예에서, 예 를 들어 출력 비디오 스트림이 대화형 컨텍스트에서 사용되지 않고 대신 외부 소비자에 대한 단방향 통신으로 게시되는 경우, 더 큰 지연이 도입될 수 있다. 이러한 의도적으로 도입된 지연은 동기화 기능이 수집된 개별 기본 스트림(210, 301) 비디오 프레임을 올 바른 공통 시간 기준 타임스탬프에 매핑하는 데 충분한 시간을 달성하기에 충분할 수 있다. 또한 손 실된 기본 스트림(210, 301) 신호, 슬라이드 변경, 해상도 변경 등을 감지하기 위해서, 상술된 이벤트 감지를 수행하는 데 충분한 시간을 허용하기에 충분할 수 있다. 더욱이, 의도적으로 상기 지연을 도입하는 것은 다음에 설명되는 바와 같이 개선된 패턴 감지 기능을 허용하기에 충분할 수 있다. 상기 지연의 도입은 해당 버퍼링된 프레임을 사용하여 출력 비디오 스트림을 게시하기 전에 수집되고 시간 동기화된 기본 비디오 스트림(210, 301) 각각을 버퍼링하는 것을 포함할 수 있다는 것이 인식된다. 다시 말해서, 기본 비디오 스트림(210, 301) 중 적어도 하나, 여러 개 또는 심지어 모두의 비디오 및/또는 오디 오 데이터는 캐시와 마찬가지로 버퍼링된 방식으로 중앙 서버에 존재할 수 있지만 다양한 대역폭 상황을 처리할 수 있는 의도로 (기존 캐시 버퍼와 같이) 사용되지는 않고 상기 이유로 특히 패턴 감지 기능에 의 해 사용된다. 따라서 일부 실시예에서 상기 패턴 감지 단계는 기본 디지털 비디오 스트림(210, 301) 중 적어도 하나, 예를 들 어 여러 개, 적어도 4개 또는 심지어 전부의 특정 정보를 고려하는 것을 포함하고, 특정 정보는 아직 출력 디지 털 비디오 스트림의 생성에 사용될 시간 동기화된 기본 디지털 비디오 스트림의 프레임보다 나중 프 레임에 존재한다. 따라서 새로 추가된 프레임은 출력 비디오 스트림의 일부(또는 기초)를 형성 하기 전에 특정 대기 시간 동안 해당 버퍼에 존재할 것이다. 이 기간 동안, 해당 프레임에 있는 정보 는 출력 비디오 스트림의 현재 프레임을 생성하기 위해 현재 사용되는 프레임과 관련된 \"미래\"의 정보를 구성할 것이다. 출력 비디오 스트림 타임라인이 해당 프레임에 도달하면, 이는 출력 비디오 스트림 의 해당 프레임을 생성하는 데 사용되고 그 후에 폐기될 수 있다. 다시 말해서, 패턴 감지 기능은 출력 비디오 스트림을 생성하기 위해 아직 사용되지 않은 비디오/오 디오 프레임 세트를 처리할 수 있고, 이 데이터를 사용하여 상기 패턴을 감지할 수 있다. 패턴 감지는 다양한 방식으로 수행될 수 있다. AI 구성요소(134a)에 의해 수행되는 일부 실시예에서, 패턴 감지 단계는 상기 패턴을 자동으로 감지하기 위해서 제2 훈련된 신경망 또는 상기 기본 디지털 비디오 스트림 (120, 301) 중 적어도 두 개, 예를 들어, 적어도 3개, 적어도 4개 또는 심지어 모두를 분석하는 다른 기계 학습 구성요소를 포함한다. 일부 실시예에서, 감지된 패턴은 각각의 참여자 클라이언트와 각각 연관된 적어도 2명, 예를 들어 적 어도 3명, 적어도 4명의 서로 다른 발언 참가자를 공유 비디오 통신 서비스에 연루시키는 말하기 패 턴을 포함하고, 상기 발언 참가자 각각은 상기 주요 디지털 비디오 스트림(210, 301) 중 각각의 것에서 시 각적으로 보여진다. 생성 단계는 바람직하게는 출력 비디오 스트림의 현재 생성 상태를 결정하고, 추적하고, 업데이트하는 것 을 포함한다. 예를 들어, 이러한 상태는 참가자가 출력 비디오 스트림에서 볼 수 있는 것과 화면의 어디에 있는지; 외부 비디오 스트림이 출력 비디오 스트림에서 볼 수 있는 것과 화면의 어디에 있는 지; 슬라이드나 공유 화면이 전체 화면 모드로 라이브 비디오 스트림과 함께 표시되는지 등을 지시할 수 있다. 그러므로, 생성 함수는 생성된 출력 비디오 스트림에 관한 상태 머신으로 볼 수 있다. 예를 들어 최종 소비자가 볼 수 있는 결합된 비디오 경험으로 출력 비디오 스트림을 생성하려면, 개 별 기본 비디오 스트림(210, 301)과 관련된 개별 이벤트를 단순히 감지하는 것보다 더 깊은 수준에서 일어나는 것을 중앙 서버가 이해할 수 있는 것이 유리하다.제1 예에서, 발표하는 참여자 클라이언트는 현재 보고 있는 슬라이드를 변경하고 있다. 이 슬라이드 변화 는 전술한 바와 같이 이벤트 감지 기능에 의해 감지되고, 메타데이터는 슬라이드 변화를 나타내는 해 당 프레임에 추가된다. 이는 프레젠테이션 참여자 클라이언트가 빠르게 연속해서 앞으로 여러 슬라이드를 건너뛰는 것으로 밝혀졌기 때문에 여러 번 발생하고, 결과적으로 일련의 \"슬라이드 변경\" 이벤트가 이벤트 감지 기능에 의해 감지되고 해당 기본 비디오 스트림에 대한 개별 버퍼에 해당 메타데이터와 함 께 저장된다. 실제로 이렇게 빠르게 앞으로 건너뛴 슬라이드는 단 몇 분의 1초 동안만 표시될 수 있다. 이러한 감지된 슬라이드 변화 중 몇 개에 걸쳐 있는, 해당 버퍼에 있는 정보를 보는 패턴 감지 기능 은 숫자 또는 빠르게 수행되는 슬라이드 변경보다는, 하나의 단일 슬라이드 변경에 해당하는 패턴을 감지하는 것이다(즉, 앞으로 건너뛰기의 마지막 슬라이드까지, 빠른 건너뛰기가 끝나면 슬라이드가 다시 표시된다). 다시 말해서, 패턴 감지 기능은 예를 들어 매우 짧은 시간 내에 10개의 슬라이드 변경이 있음을 알 수 있으며, 이들은 단일 슬라이드 변경을 나타내는 감지된 패턴으로 처리된다. 결과적으로, 패턴 감지 기능에 의해 감 지된 패턴에 액세스하는 생성 기능은 몇 초 동안 출력 비디오 스트림에서 전체 화면 모드로 최종 슬 라이드를 표시하도록 선택할 수 있는데, 왜냐하면 이 슬라이드가 상기 상태 머신에서 잠재적으로 중요하다고 판 단하기 때문이다. 또한 출력 스트림에서 중간에 본 슬라이드를 전혀 표시하지 않도록 선택할 수도 있다. 여러 개의 빠르게 변화하는 슬라이드의 패턴 탐지는 간단한 규칙 기반 알고리즘에 의해 감지될 수 있지만, 분류 에 따라 움직이는 이미지에서 이러한 패턴을 감지하도록 설계되어 훈련된 훈련 신경망을 사용하여 대안적으로 감지할 수 있다. 비디오 통신이 토크쇼, 패널 토론 또는 이와 유사한 경우에 유용할 수 있는 다른 예에서, 조용하고 부드러운 출 력 비디오 스트림을 생성하고 게시함으로써 현재 화자 사이와 여전히 소비자에게 적절한 시청 경험을 제공하는 것 사이에서 시각적 주의를 빠르게 전환하는 것이 바람직할 수 있다. 이 경우, 이벤트 감지 기능(13 2)은 특정 기본 비디오 스트림(210, 301)에서 보여지는 사람이 현재 말하고 있는지를 항상 결정하기 위해서 각 기본 비디오 스트림(210, 301)을 지속적으로 분석할 수 있다. 이는 예를 들어, 그 자체로 종래의 이미지 처리 도구를 사용하여, 상술된 바와 같이 수행될 수 있다. 다음에, 패턴 감지 기능은 상기 주요 비디오 스트림 (210, 301) 중 몇몇을 포함하여, 특정 전체 패턴을 감지하도록 동작 가능할 수 있고, 상기 패턴은 원활한 출력 비디오 스트림을 생성하는 데 유용하다. 예를 들어, 패턴 감지 기능은 현재 화자 사이의 매우 빈번한 전환 패턴 및/또는 여러 동시 화자와 관련된 패턴을 감지할 수 있다. 그러면, 생성 기능은 예를 들어 0.5초 동안만 말하고 다시 침묵하는 화자에게 시각적 초점을 자동으로 전 환하지 않거나, 두 사람이 번갈아 가면서 또는 동시에 말하는 경우 일정 시간 동안 여러 명의 화자가 나란히 표 시되는 상태로 전환하는 것으로, 상기 생성 상태와 관련하여 자동화된 결정을 내릴 때 이러한 감지된 패턴을 고 려할 수 있다. 이러한 상태 결정 프로세스는 시계열 패턴 인식 기술을 사용하거나 훈련된 신경망을 사용하여 자 체적으로 수행될 수 있다. 그러나 미리 결정된 규칙 세트에 적어도 부분적으로 기초할 수도 있다. 일부 실시예에서, 병렬로 감지되어 생성 기능 상태 머신에 대한 입력을 형성하는 여러 패턴이 있을 수 있 다. 이러한 다중 패턴은 생성 기능에 의해 다양한 AI 구성 요소, 컴퓨터 비전 감지 알고리즘 등에 의해 사 용될 수 있다. 일 예로서, 일부 참여자 클라이언트의 불안정한 연결을 동시에 감지하는 동시에 영구적인 슬라이드 변경을 감지할 수 있는 반면, 다른 패턴은 현재 주요 발언 참가자를 감지한다. 이러한 사용 가능 한 모든 패턴 데이터를 사용하여, 그러한 패턴 데이터의 시계열 분석을 위해, 분류기 신경망이 훈련될 수 있고/ 있거나 규칙 세트가 개발될 수 있다. 이러한 분류는 적어도 부분적으로, 예를 들어 완전히 감독되어 결과적으로 결정된 원하는 상태 변경이 상기 생성에 사용된다. 예를 들어, 다양하고 다른 생성 스타일 및 요구에 따라 출력 비디오 스트림을 자동으로 생성하도록 구체적으로 배열된 서로 다른 미리 결정된 분류기가 생성될 수 있다. 훈련은 원하는 출력으로 알려진 생성 상태 변경 시퀀스와 훈련 데이터로 알려진 패턴 시계열 데이터를 기 반으로 할 수 있다. 일부 실시예에서, 베이지안 모델을 사용하여 이러한 분류기를 생성할 수 있다. 구체적인 예 로서, 정보는 숙련된 생성자로부터 선험적으로 수집되어 \"토크쇼에서 나는 화자 A에서 화자 B로 바로 전환하지 않고 다른 화자가 매우 우세하여 큰 소리로 말하지 않는 한 항상 다른 화자에 집중하기 전에 먼저 개요를 보여 준다.\"과 같은 입력을 제공할 수 있다. 이 생성 논리는 일반 형식 \"X가 참이면 | Y가 참이라는 사실을 고려하여 | Z를 수행한다\"의 베이지안 모델로 표현된다. 실제 감지(누군가 큰 소리로 말하고 있는지 등)는 분류기 또는 임계값 기반 규칙을 사용하여 수행될 수 있다. (패턴 시계열 데이터의) 대규모 데이터 세트의 경우, 딥 러닝 방법을 사용하면 비디오 스트림의 자동화된 생성"}
{"patent_id": "10-2024-7027326", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "에 사용할 정확하고 매력적인 생성 형식을 개발할 수 있다. 요약하자면, 개별적 기본 비디오 스트림(210, 301); 의도적으로 도입된 지연; 여러 시간 동기화된 기본 비디오 스트림(210, 301) 및 감지된 이벤트에 기초한 패턴 감지; 감지된 패턴을 기반으로 생성 공정에 기초한 이벤트 감지의 조합을 사용하게 되면 다양한 취향과 스타일 선택에 따라 출력 디지털 비디오 스트림의 자동화된 생성을 달성하는 것을 가능하게 한다. 이 결과는 이벤트 감지 기능, 패턴 감지 기능 및 생성 기능 에 의해 사용되는 광범위한 가능한 신경망 및/또는 규칙 기반 분석 기술에 걸쳐 유효하다. 뿐만 아니라, 이는 제1 생성된 비디오 스트림이 제2 생성된 비디오 스트림의 자동 생성에 사용되고, 다양한 참여자 클라이언 트 그룹에 대해 의도적으로 추가된 다양한 지연을 특징으로 하는 후술되는 실시예에서 유효하다. 특히, 감지된 트리거가 생성된 출력 비디오 스트림에서 어떤 비디오 스트림이 사용되는지에 대한 전환을 초래하거나 출력 비 디오 스트림에서 사용된 비디오 스트림의 자동 크롭 또는 줌을 초래하는 후술되는 실시예에서도 유효하다. 상기 예시한 바와 같이, 생성 단계는 상기 출력 디지털 비디오 스트림에 있는 상기 기본 디지털 비디오 스 트림(210, 301) 중 개별적인 가시성에 관한 사전 결정된 및/또는 동적으로 가변적인 매개변수 세트; 시각적 및/ 또는 청각적 비디오 콘텐츠 배열; 시각 또는 청각 효과 사용; 및/또는 출력 디지털 비디오 스트림의 출력 모드에 기초하여 출력 디지털 비디오 스트림을 생성하는 것을 포함할 수 있다. 이러한 매개변수는 상기 생 성 기능 상태 기계에 의해 자동으로 결정될 수 있고 및/또는 생성을 제어하는 조작자에 의해 설정될 수 있 고(반자동으로 만듬) 및/또는 특정한 선험적 구성 요구(예를 들어 출력 비디오 스트림 레이아웃 변경 또는 위에서 예시된 유형의 상태 변경 사이의 최단 시간)에 기초하여 미리 결정될 수 있다. 실제적인 예에서는, 상태 머신은 출력 비디오 스트림에 적용될 수 있는 미리 결정된 표준 레이아웃 세트, 예를 들어, 전체 화면 발표자 보기(현재 말하고 있는 참가자를 전체 화면으로 표시함); 슬라이드 보기(현 재 공유된 프레젠테이션 슬라이드를 전체 화면으로 표시함); 현재 말하고 있는 참가자와 현재 공유된 프리 젠테이션 슬라이드를 나란히 표시하는 \"나비 보기\"; 참가자의 전체 또는 선택된 하위 집합을 나란히 또는 매트릭스 레이아웃으로 보여주는 다중 화자 보기; 등을 지원할 수 있다. 다양한 이용 가능한 생성 포맷은 이용 가능한 상태 세트(에를 들어 상기 표준 레이아웃 세트)와 함께 (위에 예시된 바와 같이) 상태 머신 상태 변경 규칙 세트에 의해 정의될 수 있다. 예를 들어, 이러한 생성 형식 중 하나는 \"패널 토론\", 또 다른 \"발표\" 등일 수 있다. GUI 또는 중앙 서버에 대한 다른 인터페이스를 통해 특정 생성 형식을 선택함으로써, 시스템 의 운영자는 미리 정의된 생성 형식 세트 중 하나를 신속하게 선택한 다음에, 중앙 서버가 위에서 설 명된 이용 가능한 정보에 기초하여 해당 생성 포맷에 따라 출력 비디오 스트림을 완전 자동으로 생성하도 록 허용한다. 게다가 생성 과정에서, 상술한 바와 같이, 각각의 미팅 참여자 클라이언트 또는 외부 비디오 소스에 대해 각각의 인메모리 버퍼가 생성되고 유지된다. 이러한 버퍼는 즉시 쉽게 제거, 추가 및 변경할 수 있다. 그 러면 중앙 서버는 추가/탈퇴된 참여자 클라이언트 및 연설 예정인 참가자; 프레젠테이션의 계획 된 또는 예상치 못한 일시 중지/재개; 현재 사용되는 생성 형식에 대한 원하는 변경 사항 등과 관련하여, 출력 비디오 스트림을 생성하는 동안 정보를 수신하도록 배열될 수 있다. 이러한 정보는 예를 들어, 전술한 바 와 같이 운영자 GUI 또는 인터페이스를 통해 중앙 서버에 공급될 수 있다. 위에 예시된 바와 같이, 일부 실시예에서 주요 디지털 비디오 스트림(210, 301) 중 적어도 하나가 디지털 비디 오 통신 서비스에 제공되고, 게시 단계는 상기 출력 디지털 비디오 스트림을 동일한 통신 서비스 에 제공하는 단계를 포함할 수 있다. 예를 들어, 출력 비디오 스트림은 비디오 통신 서비스의 참여자 클라이언트에 제공될 수 있거나, API를 통해 외부 비디오 스트림으로서 비디오 통신 서비스 에 제공될 수 있다. 이러한 방식으로, 출력 비디오 스트림은 현재 비디오 통신 서비스에 의해 달성되고 있는 비디오 통신 이벤트에 대한 일부 또는 모든 참가자에게 이용 가능하게 될 수 있다. 또한 상술된 바와 같이, 추가적으로 또는 대안적으로 출력 비디오 스트림은 하나 또는 여러 외부 소비자 에게 제공될 수 있다. 일반적으로, 생성 단계는 중앙 서버에 의해 수행될 수 있으며, API를 통해 라이브 비디오 스트림으로 서 하나 이상의 동시 소비자에게 상기 출력 디지털 비디오 스트림을 제공할 수 있다. 상술된 바와 같이, 참여자 클라이언트는 둘 이상의 참여자 클라이언트 그룹으로 구성될 수 있다. 도 8은 그러한 그룹이 있는 경우에 출력 비디오 스트림의 자동 생성을 수행하는 구성의 시스템의 단순화된 도 면이다. 도 8에서, 중앙 서버는 상술된 바와 같을 수 있는 수집 기능을 포함한다. 중앙 서버는 또한 제1 생성 기능(135'), 제2 생성 기능(135\") 및 제3 생성 기능(135\"')을 포함한다. 각각 의 이러한 생성 기능(135', 135\", 135\"')은 생성 기능에 대응하고, 생성 기능과 관련하여 위에서 언 급한 내용은 생성 기능(135', 135\", 135\"')에도 동일하게 적용된다. 생성 기능(135', 135\", 135\"')은 중앙서버 의 세부 구성에 따라 여러 기능을 갖는 하나의 단일 논리적 기능으로 구별되거나 공동 배열될 수 있으며, 세 개 이상의 생성 기능이 있을 수도 있다. 생성 기능(135', 135\", 135\"')은 경우에 따라 하나의 동일한 생성 기능의 서로 다른 기능적 측면일 수 있다. 생성 기능(135', 135\", 135\"')과 다른 개체 간의 다양한 통신은 적절한 API를 통해 이루어질 수 있다. 각각의 생성 기능(135', 135\", 135\"') 또는 그러한 생성 기능의 그룹에 대해 별도의 수집 기능이 있을 수 있고 세부 구성에 따라 각각의 수집 기능을 갖춘 논리적으로 분리된 여러 개의 중앙 서버가 있을 수 있다는 것이 또한 인식된다. 게다가, 중앙 서버는 제1 게시 기능(136'), 제2 게시 기능(136\") 및 제3 게시 기능(136\"')을 포함한다. 각 각의 이러한 게시 기능(136', 136\", 136\"')은 게시 기능에 대응하고, 게시 기능과 관련하여 위에서 언급한 내용은 게시 기능(136', 136\", 136\"')에도 동일하게 적용된다. 게시 기능(136', 136\", 136\"')은 중앙서 버의 세부 구성에 따라 여러 기능을 가진 하나의 단일 논리적 기능으로 구별되거나 공동 배열될 수 있으며, 세 개 이상의 게시 기능이 있을 수도 있다. 게시 기능(136', 136\", 136\"')은 경우에 따라 하나의 동일 한 게시 기능의 다른 기능적 측면일 수 있다. 도 8에서, 설명을 위해 세 세트 또는 그룹의 참여자 클라이언트가 도시되어 있으며, 각각은 상술된 참여자 클라 이언트에 해당한다. 따라서, 그러한 참여자 클라이언트(121')의 제1 그룹; 참여자 클라이언트의 제2 그룹 (121\"); 및 참여자 클라이언트의 제3 그룹(121\"')이 있다. 이들 그룹 각각은 하나 또는 바람직하게는 적어도 두 명의 참여자 클라이언트로 구성될 수 있다. 세부 구성에 따라 이러한 그룹은 2개만 있을 수도 있고 3개 이상 있 을 수도 있다. 그룹(121', 121\", 121\"') 간의 할당은 각 참여자 클라이언트가 최대 하나의 그룹(121', 121\", 121\"')에 할당된다는 점에서 배타적일 수 있다. 대체 구성에서, 적어도 하나의 참여자 클라이언트는 동시에 하나 이상의 그룹(121', 121\", 121\"')에 할당될 수 있다. 도 8은 또한 외부 소비자를 도시하며, 전술한 바와 같이 외부 소비자가 둘 이상 있을 수 있다는 것이 인식된다. 도 8은 단순성 때문에, 비디오 통신 서비스를 도시하지는 않았지만, 전술한 방식으로 중앙서버를 이 용하여 각 참여자 클라이언트에게 공유 영상통신 서비스를 제공하는 등, 전술한 일반적인 형태의 영상통신 서비스를 중앙서버와 함께 이용할 수도 있음이 실현된다. 각각의 주요 비디오 스트림은 수집 기능에 의해 상기 그룹(121', 121\", 121\"')의 참여자 클라이언트와 같 은 각각의 참여자 클라이언트로부터 수집될 수 있다. 제공된 기본 비디오 스트림에 기초하여 생성 기능 (135', 135\", 135\"')은 각각의 디지털 비디오 출력 스트림을 생성할 수 있다. 도 8에 도시된 바와 같이, 이러한 생성된 하나 또는 여러 개의 출력 스트림은 하나 또는 여러 개의 각각의 생성 기능(135', 135\"')으로부터 다른 생성 기능(135\")에 각각의 입력 디지털 비디오 스트림으로서 제공될 수 있고, 다음에 게시 기능(136\")에 의한 게시를 위해 2차 디지털 출력 비디오 스트림을 생성할 수 있고, 따라서 보조 디 지털 출력 비디오 스트림은 하나 또는 여러 개의 입력 기본 디지털 비디오 스트림뿐만 아니라 하나 또는 여러 개의 미리 생성된 디지털 입력 디지털 비디오 스트림을 기반으로 생성된다. 2개 이상의 서로 다른 생성 단계(135', 135\", 135\"')는 각각의 시간 지연의 도입을 포함할 수 있다. 일부 실시 예에서, 이러한 생성 단계(135', 135\", 135\"')로부터 각각 생성된 출력 디지털 비디오 스트림 중 하나 또는 여 러 개는 상기 시간 지연 도입으로 인해, 상기 게시 단계에서 다른 참여자 클라이언트에게 제공될 수 있는 비디 오 스트림의 어느 것과도 시간 동기화되지 않을 수 있다. 이러한 시간 지연은 본 명세서에서 설명된 임의의 방 식으로 의도적으로 추가되거나 해당 생성된 디지털 비디오 스트림 생성의 직접적인 결과일 수 있다. 결과적으로, 상기 시간이 동기화되지 않은 생성된 출력 디지털 비디오 스트림을 소비하는 모든 참여자 클라이언 트는 상기 다른 참여자 클라이언트의 비디오 스트림 소비 \"시간대\"와 관련하여 약간 (시간 기준으로) 오프셋된 \"시간대\"에서 그렇게 할 것이다. 예를 들어, 참여자 클라이언트의 그룹(121', 121\", 121\"') 중 하나는 제1 \"시간대\"에서 각각 생성된 비디 오 스트림을 소비할 수 있는 반면, 상기 그룹(121', 121\", 121\"') 중 다른 하나의 참여자 클라이언트는 \" 시간대\"와 같은 초 단위로 각각 생성된 비디오 스트림을 소비할 수 있다. 이러한 각각의 생성된 비디오 스트림은 모두 동일한 기본 비디오 스트림에 적어도 부분적으로 기초하여 생성될 수 있으므로, 이러한 모든 참여자 클 라이언트는 동일한 비디오 통신에서 활성화되지만 서로에 대해 서로 다른 \"시간대\"에서 활성화된다. 즉, 생성된 비디오 스트림 소비에 대한 각각의 타임라인은 서로 다른 그룹(121', 121\", 121\"') 사이에서 시간 기준 으로 오프셋될 수 있다. 예를 들어, 일부 생성 단계(예를 들어, 135', 135\"')는 (의도적으로 도입된 시간 지연을 사용하지 않고) 직접적 일 수 있고 및/또는 게시 준비 전에 계산적으로 상대적으로 가벼운 처리만 포함할 수 있는 반면, 다른 생성 단 계(예를 들어, 135\")는 의도적으로 도입된 시간 지연 및/또는 상대적으로 무거운 처리를 포함할 수 있고 이는 해당 생성된 디지털 비디오 스트림이 이전 생성 단계(135', 135\"')의 각각의 디지털 비디오 스트림의 게시를 위 해 가장 빠른 지연과 관련한 지연 시 가장 빠른 게시를 위해 생성된다. 따라서, 상기 그룹(121', 121\", 121\"') 중 하나 또는 여러 그룹의 각 참여자 클라이언트는 동일한 인지된 시간 지연으로 서로 상호작용할 수 있다. 동시에, 그룹이 더 큰 각각의 시간 지연과 관련된다는 것은 시간 지연 이 더 큰 그룹이 더 나중 \"시간대\"에서 보는 출력 비디오 스트림을 생성할 때 시간 지연이 적은 그룹에서 생성 된 비디오 스트림을 사용할 입력 비디오 스트림으로 사용할 수 있다. 따라서 제1 더 큰 시간 지연 생성(단계 135\")에 의한 결과는 예를 들어 해당 주요 비디오 스트림 중 하나 또는 여러 개를 처리되거나 처리되지 않은 형태로 하위 부분으로 시각적으로 포함할 수 있는, 상술된 유형의 생성된 디지털 비디오 스트림이다. 해당 생성된 비디오 스트림은 중앙 서버에 의해 생성된 비디오 출력 스트림과 관련하여 위에서 일반적으로 설명한 바와 같이, 실시간으로 캡처된 비디오 스트림, 슬라이드, 외부에서 제공되 는 비디오 또는 이미지 등으로 구성될 수 있다. 해당 생성된 비디오 스트림은 상술된 일반적인 방식으로 참여자 클라이언트에 의해 제공되는 고의적으로 지연되거나 실시간 입력된 기본 비디오 스트림의 감지된 이벤트 및/또는 패턴을 기반으로 생성될 수도 있다. 예시적인 예에서, 제1 그룹(121') 참여자 클라이언트는 토론 패널의 일부이며 상대적으로 낮은 대기 시간으로 비디오 통신 서비스를 사용하여 통신하고, 이들 참여자 클라이언트 각각은 게시 단계(136')로부터 생성된 비디오 스트림(또는 서로의 각각의 기본 비디오 스트림)을 지속적으로 공급받는다. 토론 패널의 청중은 제2 그 룹(121\") 참여자 클라이언트로 구성되며 생성 단계(135\")에서 생성된 비디오 스트림을 지속적으로 공급받으며, 이는 결과적으로 약간 더 높은 대기 시간과 관련된다. 생성 단계(135\")에서 생성된 비디오 스트림은 상술된 일 반적인 방식으로 자동으로 생성될 수 있어, 개별 토론 패널 발표자의 보기(제1 그룹(121')에 할당된 참여자 클 라이언트, 이러한 뷰는 수집 기능에서 직접 제공됨)와 모든 토론 패널 화자를 보여주는 생성된 보기(이 보 기는 처음으로 생성된 비디오 스트림임) 사이를 자동으로 전환한다. 따라서 청중은 잘 만들어진 경험을 받을 수 있고 패널 화자는 최소한의 대기 시간으로 서로 상호 작용할 수 있다. 생성 단계(136\")에서 사용된 각각의 주요 비디오 스트림에 의도적으로 추가된 지연은 적어도 0.1초, 예를 들어 적어도 0.2초, 적어도 0.5초일 수 있으며; 최대 5초, 예를 들어 최대 2초, 최대 1초일 수 있다. 이것은 또한 사 용된 각 기본 비디오 스트림과 생성 단계(135\")에서 생성 단계(135\")로 들어오는 생성된 비디오 스트림 사이의 완전한 시간 동기화를 달성하기 위해서, 각 기본 비디오 스트림과 관련된 상속 대기 시간에 따라 달라질 수도 있다. 해당 기본 비디오 스트림, 뿐만 아니라 생성 단계(135')에서 생성된 비디오 스트림이 상술된 일반적인 방식으로 제2 생성 기능(135\")에서 사용하기 위한 패턴 감지를 개선하기 위해 모두 추가로 의도적으로 지연될 수 있다는 것이 이해된다. 또한, 도 8은 중앙 서버에 의해 생성된 다양한 생성 비디오 스트림을 게시하는 다수의 대안적 또는 동시적 방식을 도시한다. 일반적으로, 제1 생성 기능(135')으로부터 처음으로 생성된 비디오 스트림을 수신하도록 배열된 제1 게시 기능 (136')에 의해 수행되는 게시 단계에서, 상기 제1 생성 비디오 스트림은 제1 참여자 클라이언트 및 제2 참 여자 클라이언트 중 적어도 하나에게 지속적으로 제공될 수 있다. 예를 들어, 이 제1 참여자 클라이언트는 각각의 주요 디지털 비디오 스트림을 제1 생성 기능(135')에 제공하는 그룹(121')으로부터의 참여자 클라이언트 일 수 있다. 일부 실시예에서, 그룹(121')의 참여자 클라이언트 중 하나 또는 여러 개는 또한 제2 게시 기능(136\")에 의해 제2 생성 비디오 스트림을 수신할 수 있고, 이어서 제2 생성 기능(135\")으로부터 제2 생성된 비디오 스트림을 수신하도록 배열된다. 따라서, 제1 그룹(121')에 할당되는 각 기본 비디오 스트림 제공 참여자 클라이언트는, 서로의 기본 디지털 비 디오 스트림을 직접 제공받지 않는 경우, 제1 생성된 비디오 스트림이 제공될 수 있어, 상술된 바와 같이, 상기 기본 비디오 스트림 사이의 동기화로 인한 특정 지연 또는 대기 시간 및 또한 이벤트 및/또는 패턴 감지를 위한 충분한 시간을 허용하기 위해 의도적으로 추가된 지연 또는 대기 시간을 수반한다. 이에 따라, 제2 그룹(121\")에 할당된 각각의 참여자 클라이언트에는 제2 생성 단계와 관련하여 의도적으로 추가 된 지연을 포함하고, 제1 생성된 비디오 스트림을 제1 및 제2 기본 비디오 스트림과 시간 동기화할 목적으로 추 가된, 제2 생성 비디오 스트림이 제공될 수 있다. 이러한 추가 지연은 제2 그룹(121\")의 참여자 클라이언트 사 이에 통신 문제를 일으킬 수도 있고 그렇지 않을 수도 있는데, 이는 예를 들어 이들이 제1 그룹(121\") 참가자와 는 다른 방식으로 비디오 통신 서비스와 상호 작용하기 때문이다. 따라서, 제1 그룹(121') 참여자 클라이언트는 제1 또는 제2 생성된 비디오 스트림와 같은 생성된 비디오 스트림 을 지속적으로 제공받는 대신, 참여자 클라이언트보다 약간 앞선(예를 들어, 1~3초 앞선) \"시간대\"에서 해당 서 비스에 존재하고 이를 사용하는, 해당 영상 통신 서비스에 현재 참여하고 있는 모든 참여자 클라이언트 의 하위 그룹을 형성한다. 그럼에도 불구하고, (제1 그룹(121')에 할당되지 않고 제2 그룹(121\")에 할당된) 다른 참여자 클라이언트는 제2 생성된 비디오 스트림이 계속해서 제공되고, 이것은 제2 생성된 비디오 스트림이 생설될 때 기반으로 하는 기본 비디오 스트림 중 적어도 하나에 기반하지만, 이 때 약간 더 늦은 \"시 간대\"에서 생성된다(각 시점에서 둘 중 하나 또는 둘 다를 포함함). 제1 생성된 비디오 스트림은 기본 비디오 스트림 자체를 기반으로 이미 생성된 비디오 스트림에 시간을 동기화하기 위해 추가되는 지연이나 대기 시간 없 이, 이들 적어도 하나의 기본 비디오 스트림을 기반으로 직접 생성되기 때문에, 보다 직접적이고 대기 시간이 짧은 비디오 통신 서비스 경험이 이들 참여자 클라이언트에게 이용 가능하게 된다. 다시, 이것은 또한 제1 그룹(121')에 할당된 참여자 클라이언트에게 제2 생성된 비디오 스트림에 대한 액 세스가 제공되지 않는다는 것을 의미할 수도 있다. 도 8에 도시된 바와 같이, 제2 생성된 비디오 스트림은 제3 생성 단계(135\"')의 생성된 출력 비디오(제3 생성된 출력 디지털 비디오 스트림)에 추가적으로 기초하여 디지털 비디오 스트림으로 생성될 수도 있다. 따라서 각각 의 생성 단계(135', 135\", 135\"')는 적어도 부분적으로 겹치는 1차 입력 디지털 비디오 스트림을 기반으로 하지 만, 상기 기본 비디오 스트림과 관련하여 의도적으로 추가된 서로 다른 대기 시간(\"시간대\")을 사용하여 생성되 어 참여자 클라이언트의 서로 다른 각 그룹(121', 121\", 121\"')에 제공된다. 제3 그룹(121\")에 할당된 참여자 클라이언트는 제1 그룹(121\")에 할당된 참여자 클라이언트보다 덜 엄격한 대기 시간 요구사항을 가질 수 있다. 예를 들어, 제1 그룹(121') 참여자 클라이언트는 (실시간으로 서로 상호 작용하므로 짧은 대기 시간으로 필요로 하는) 상술된 토론 패널의 구성원일 수 있는 반면, 제3 그룹 (121\") 참여자 클라이언트는 패널과 보다 구조화된 방식(예를 들어, 명확한 질문/답변 사용)으로 상호 작 용하는 전문가 또는 유사한 패널을 구성할 수 있으므로, 제1 그룹(121')보다 높은 대기 시간을 견딜 수 있다. 경우에 따라, 제1 생성된 비디오 스트림과 제3 생성된 비디오 스트림 모두 제2 생성된 비디오 스트림의 생성을 기반으로 하는 제2 생성 기능(135\")에 공급될 수 있다. 따라서, 제1 생성 단계(135')는 제1 및 제2 기본 비디오 스트림의 동기화의 일부로 유도된 지연 외에 추가로 도 입되고, 예를 들어 효율적인 이벤트 및/또는 패턴 감지를 수행하는 데 충분한 시간을 확보하기 위해 도입되는, 상술된 유형의 의도적인 지연 또는 대기 시간을 도입하는 것을 포함할 수 있다. 이러한 의도적인 지연 또는 대 기시간의 도입은 상기 동기화 기능(간단함을 이유로 도 8에는 도시되지 않음)에 의해 수행되는 동기화의 일부로서 발생할 수 있다. 이는 제3 생성 단계(135\"')에 해당될 수 있지만, 제1 생성 단계(135')에 대해 의도적 으로 도입된 지연 또는 대기 시간과 다른 의도적인 지연 또는 대기 시간을 도입한다. 특히, 의도적으로 도입된 지연 또는 대기 시간으로 인해 제1 생성된 비디오 스트림과 제3 생성된 비디오 스트림 간의 시간 비동기화가 발생한다. 이는 제1 및 제3 생성된 비디오 스트림이 각 개별 프레임 생성 시 즉각적이고 지속적으로 게시되는 경우 공통 타임라인을 따르지 않눈 것음을 의미한다. 따라서 세 개의 개별 생성된 비디오 스트림은 동시에 하지만 서로 다른 \"시간대\"에 생성되고 소비/게시될 수 있 다. 이들은 적어도 부분적으로 동일한 기본 비디오 자료를 기반으로 하지만, 생성된 비디오 스트림은 서로 다른 대기 시간으로 게시된다. 가장 낮은 대기 시간을 요구하는 제1 그룹(121')은 제1 생성된 비디오 스트림을 사용 하여 상호 작용할 수 있으며 매우 낮은 대기 시간을 제공하고; 약간 더 큰 대기 시간을 기꺼이 수용하는 제3 그 룹(121\"')은 제2 생성된 비디오 스트림을 사용하여 상호 작용하여, 더 많은 대기 시간을 제공하지만 다른 한편으로는 본 명세서 다른 부분에 설명된 대로 더 나은 자동 생성을 달성하기 위해 의도적으로 추가된 지연 측면에 서 더 큰 유연성을 제공하고; 반면 대기 시간에 그다지 민감하지 않는 제2 그룹(121\")은 제1 그룹(121') 및 제3 그룹(121\"')의 자료를 통합할 수 있는 제2 생성된 비디오 스트림을 사용하여 상호 작용을 즐길 수 있고 매우 유 연한 방식으로 자동 생성될 수도 있다. 이들 모든 참여 사용자 그룹(121', 121\", 121\"')은 다양한 대기 시간을 사용하고 그에 따라 다른 \"시간대\"에서 작동함에도 불구하고, 상기 영상통화 서비스를 이용하여 서로 상호 작용하는 것에 특히 유의한다. 그러나 각 생성 기능에서 개별적 입력 비디오 스트림의 동기화로 인해, 참가자 사용자는 각자의 관점에서 다른 대기 시간을 인식하지 못할 것이다. 상술된 바와 같이, 상기 그룹(121', 121\", 121\"') 각각에 할당된 각각의 참여자 클라이언트는 제2 생성된 비디오 스트림이 지속적으로 공개되는 하나의 동일한 비디오 통신 서비스에 참여할 수 있다. 다음에, 상기 그룹(121', 121\", 121\"') 중 다른 그룹은 상기 비디오 통신 서비스의 다른 참가자 상호 작용 권한과 연관될 수 있다. 이들 및 다른 실시예에서, 상기 그룹(121', 121\", 121\"') 중 서로 다른 그룹은 해당 그 룹(121', 121\", 121\"')에 할당된 참여자 클라이언트에게 게시된 각각의 생성된 비디오 스트림을 생성하는 데 사용되는 서로 다른 최대 시간 지연(대기 시간)과 연관될 수도 있다. 예를 들어, 패널 토론 참여자 클라이언트의 제1 그룹(121')은 완전한 상호 작용 권한과 연관될 수 있으며 이들 이 원할 때마다 말할 수 있다. 참여자 클라이언트의 제3 그룹(121\"')은 예를 들어 이들이 마이크 음소거를 해제 하는 비디오 통신 서비스에 의해 말할 수 있기 전에 발언권을 요청할 필요가 있는 약간 제한된 상호 작용 권한과 연관될 수 있다. 청중 참여 사용자의 제2 그룹(121\")은 예를 들어, 공용 채팅방에서는 글로만 질문을 할 수 있지만, 말을 할 수는 없는 것과 같은, 훨씬 더 제한된 상호작용 권리와 연관될 수 있다. 따라서, 다양한 그룹의 참가자 사용자는 서로 다른 상호 작용 권한 및 그들에게 게시되는 각각의 생성된 비디오 스트림에 대한 서로 다른 대기 시간과 연관될 수 있으며, 이러한 방식으로 대기 시간은 상호 작용 권한을 감소 시키는 증가 함수이다. 해당 참가자 사용자가 비디오 통신 서비스에 의해 다른 사용자와 상호 작용하 는 것이 더 자유롭게 허용될수록 허용되는 대기 시간은 낮다. 허용되는 대기 시간이 낮을수록 해당 자동 생성 기능이 감지된 이벤트나 패턴 등을 고려할 가능성이 작아진다. 대기 시간이 가장 큰 그룹은 영상통화 서비스에 수동적 참여를 제외하고는 상호작용 권한이 없는 시청자 전용 그룹일 수 있다. 특히, 상기 그룹(121', 121\", 121\"') 각각에 대한 각각의 최대 시간 지연(대기 시간)은 모든 기본 비디오 스트 림과 해당 그룹에 참여하는 클라이언트에게 지속적으로 게시되는 생성된 비디오 스트림 간의 가장 큰 대기 시간 차이로 결정될 수 있다. 이 합계에는 상술된 바와 같이 이벤트 및/또는 패턴을 감지할 목적으로 의도적으로 추 가된 추가 시간 지연이 추가될 수 있다. 본 명세서에 사용된 바와 같이, \"생성\" 및 \"생성된 디지털 비디오 스트림\"이라는 용어는 다양한 유형의 생성을 의미할 수 있다. 한 가지 예에서, 단일의 잘 정의된 디지털 비디오 스트림은 해당 생성된 디지털 비디오 스트림 을 소비할 특정 세트의 참여자 클라이언트 각각에 대한 제공 및 게시를 위해, 해당 생성된 디지털 비디오 스트림을 형성하기 위해 중앙 서버와 같은 중앙 개체에 의해 생성된다. 다른 경우, 참여자 클라이언트 와 같은 다른 개인은 해당 생성된 디지털 비디오 스트림의 약간 다른 버전을 볼 수 있다. 예를 들어, 생성 된 디지털 비디오 스트림은 참여자 클라이언트 로컬 소프트웨어 기능이 해당 사용자가 전환하고; 화면에 배열하고; 다른 방식으로 구성하거나 처리하도록 할 수 있는 여러 개별 또는 결합된 디 지털 비디오 스트림을 포함할 수 있다. 많은 경우, 중요한 것은 시간 동기화된 하위 구성 요소를 포함하여 생성 된 디지털 비디오 스트림이 제공되는 \"시간대\"(즉, 대기 시간)이다. 따라서, 도 8과 관련하여 위에서 설명된 바 와 같이, 제1 그룹(121')의 서로 다른 참여자 클라이언트가 서로의 기본 비디오 스트림을 제공받는 경우는 제1 생성 디지털 비디오 스트림이 이들 참여자 클라이언트에게 제공되는 것으로 볼 수 있다(시간 동기화된 원시 또는 처리된 제1 및 제2 기본 디지털 비디오 스트림 세트가 제1 및 제2 참여자 클라이언트 모두에게 제공된다는 의미에서). 위에서 설명된 참여자 클라이언트 그룹(121', 121\", 121\"')의 사용을 더욱 명확하게 하고, 세 가지 동시 \"시간 대\"를 포함하는 화상 통신 서비스 회의의 형태로 다음 예가 제공된다. 참여자 클라이언트(121')의 제1 그룹은 (피할 수 없는 하드웨어 및 소프트웨어 대기 시간에 따라) 실시간으로 또는 적어도 거의 실시간으로 서로 상호 작용을 경험하고 있다. 이러한 참여자 클라이언트는 해당 사용자 간의 상호 작용 및 통신을 달성하기 위해 소리를 포함하여 비디오를 서로 제공받는다. 제1 그룹(121')은 회의의핵심에서 사용자에게 서비스를 제공할 수 있으며, 그 상호작용은 다른 (비-제1 그룹(121')) 참여자 클라이 언트가 참여할 수 있는 관심 사항이 될 수 있다. 이러한 다른 참여자 클라이언트(121\")의 제2 그룹은 동일한 회의에 참여하지만 다른 \"시간대\"에 있어 제1 그룹 참여자 클라이언트(121')보다 실시간에서 더 멀리 떨어져 있다. 예를 들어, 제2 그룹(121\")은 제1 그룹(121')에 게 질문을 할 수 있는 등의 상호작용 특권을 가진 청중일 수 있다. 제2 그룹(121\")의 \"시간대:는 제1 그룹 (121\")의 \"시간대\"에 비해 지연을 가지므로, 제기된 질문과 답변이 눈에 띄지만 짧은 지연과 연관될 수 있다. 반면에, 약간 더 큰 지연을 통해 제2 그룹(121\") 참여자 클라이언트는 더 복잡한 방식으로 자동으로 생성된 생 성된 디지털 비디오 스트림을 경험할 수 있어, 보다 기분 좋은 사용자 경험을 제공할 수 있다. 그러한 다른 참여자 클라이언트(121\"')의 제3 그룹도 동일한 회의에 참여하지만 오직 시청자로만 참여한다. 이 제3 그룹(121\"')은 더욱 정교하고 복잡한 방식으로 자동 생성될 수 있는 생성된 디지털 비디오 스트림을 소비하 며, 이는 제2 \"시간대\"보다 훨씬 더 지연되는 제3 \"시간대\"에서 소비된다. 그러나, 제3 그룹(121\"')은 제1 그룹 (121') 및 제2 그룹(121\")에 영향을 미칠 정도로 통신 서비스에 입력을 제공할 수 없으므로, 제3 그룹(121\"')은 \"실시간\"으로 진행되는 미팅을 기분 좋은 연출로 경험하게 된다. 물론 본 명세서에서 설명된 원칙을 사용하여 점점 더 큰 시간 지연과 생성 복잡성이 증가하는 각 회의 \"시간 대\"와 연관되는 3개 이상의 참여자 클라이언트 그룹이 있을 수 있다. 도 9는 본 발명에 따른 방법을 도시하며, 이 방법은 공유된 디지털 비디오 스트림을 제공하는 것이다. 제1 단계 S1에서, 방법이 시작된다. 이어지는 수집 단계(S2)에서, 제1 디지털 비디오 스트림은 제1 디지털 비디오 소스로부터 수집되고, 제2 디지털 스트림은 위에서 일반적으로 설명한 방식으로 제2 디지털 비디오 소스로부터 수집된다. 따라서, 제1 및/또는 제 2 디지털 비디오 스트림은 각자의 참여자 클라이언트 또는 외부 소스로부터 각각 수집될 수 있으며, 중앙 서버의 수집 기능에 의해 수행될 수 있다. 후속하는 제1 생성 단계(S4)에서, 상기 공유 디지털 비디오 스트림은 출력 디지털 비디오 스트림으로서 생성된 다. 이러한 생성은 예를 들어 생성 단계(135, 135', 135\", 135\"')에 의해, 일반적으로 상술된 바와 같이 수행될 수 있다. 제1 생성 단계(S4)에서, 공유 디지털 비디오 스트림은 상기 제1 디지털 비디오 스트림의 연속적으로 고려되는 프레임을 기반으로 생성되므로, 상기 제1 디지털 비디오 소스로부터의 이미지 정보가 공유된 디지털 비디오 스 트림에서 볼 수 있지만, 상기 제2 디지털 비디오 소스의 이미지 정보는 공유된 디지털 비디오 스트림에서 볼 수 없다. 즉, 공유 비디오 스트림은 적어도 어느 정도 제1 디지털 비디오 스트림에서 발생한 시각적 자료를 포함하 지만, 제2 디지털 비디오 스트림에서 발생한 시각적 자료는 포함하지 않는다. 후속하는 트리거 감지 단계(S5)에서, 상기 제1 및/또는 제2 디지털 비디오 스트림은 적어도 하나의 트리거를 감 지하기 위해 디지털 방식으로 분석된다. 이러한 분석 및 탐지는 해당 공유 비디오 스트림을 생성하는 동일한 생성 단계(135, 135', 135\", 135\"')에 의해 수행될 수 있으며, 미리 결정된 유형의 이미지 및/또는 오디오 패턴의 자동 감지를 기반으로 한다. 트리거는 위에서 설명하고 예시한 유형의 이벤트 또는 패턴일 수 있고(각각 이벤트 감지 기능 및 패턴 감 지 기능에 의해 수행됨). 그 감지는 일반적으로 해당 디지털 비디오 스트림(들)에 포함된 오디오 및/또는 이미지/비디오 데이터의 자동 디지털 처리를 사용하여 발생한다. 예를 들어, 상술된 바와 같이, 훈련된 신경망 이나 기타 기계 학습 도구를 사용하는 등의 자동 이미지 처리 알고리즘은 상기 제1 및/또는 제2 비디오 피드에 포함된 이미지를 기반으로 특정 트리거의 존재를 자동으로 감지하는 데 사용될 수 있다. 이에 따라, 해당 유형 의 기존 자동 오디오 처리 알고리즘 자체를 사용하여 상기 제1 및/또는 제2 비디오 피드에 포함된 오디오를 기 반으로 특정 트리거의 존재를 감지할 수 있다. 이미지 및/또는 오디오 패턴이 \"미리 결정된 유형\"이라는 것은 해당 패턴이해당 감지에 앞서 정의된 하나 이상 의 절대 또는 상대 매개변수 값 세트로 특성화될 수 있는 것을 의미한다. 이에 대한 예는 아래에 설명되어 있다. 일반적으로, 상기 오디오 또는 이미지 패턴의 존재는 해당 트리거를 구성한다. 게다가, 트리거는 해당 트리거가 감지되면 공유 디지털 비디오 스트림의 생성 모드(규칙)를 변경하기 위해서, 미리 결정된 생성 규칙에 따라 공유 비디오 스트림을 생성하는 자동 생성 단계(135, 135', 135\", 135\"')에 대해 유익하도록 특별히 미리 결정된 다. 따라서, 생성 단계(135, 135', 135\", 135\"')는 순간적으로 또는 시간에 걸쳐 해당 이미지 및/또는 오디오 패턴 을 특성화하는 해당 매개변수 값의 관점에서 하나 또는 여러 트리거를 정의하는 데이터베이스를 포함하거나 이 에 액세스할 수 있다. 상기 트리거의 감지에 응답하여 개시되는 후속 제2 생성 단계(S6 또는 S7)에서, 공유된 디지털 비디오 스트림은 동일한 (또는 다른) 생성 단계(135, 135', 135\", 135\"')에 의해 다시 생성되지만, 제1 생성 단계(S4)와 동일한 방식은 아니다. 제1 대안적인 제2 생성 단계(S6)에서, 공유 디지털 비디오 스트림은 상기 제2 디지털 비디오 스트림의 연속적으 로 고려되는 프레임에 기초하여 출력 디지털 비디오 스트림으로 생성되므로, 상기 제2 디지털 비디오 소스의 이 미지 정보가 공유 디지털 비디오 스트림에서 보이게 된다. 이 경우 출력 디지털 비디오 스트림은 또한 제1 디지 털 비디오 스트림의 연속적으로 고려되는 프레임에 기초하여 생성되거나 기초하지 않고 생성될 수 있으므로, 제 1 디지털 비디오 소스의 이미지 정보가 공유 디지털 비디오 스트림에서 보이게 되는 것에 유의한다. 다시 말해 서, 제1 생성 단계(S4)에서 제2 생성 단계(S6)로 전환할 때, 공유 비디오 스트림은 제1 비디오 스트림의 콘텐츠 를 표시하지만 제2 비디오 스트림은 표시하지 않거나, 제2 비디오 스트림의 콘텐츠를 표시하지만 제1 비디오 스 트림은 표시하지 않거나, 제1 및 제2 비디오 스트림 모두의 콘텐츠를 표시할 수 있다. 제2 대안적인 제2 생성단계(S7)에서는, 공유 디지털 비디오 스트림은 상기 제1 디지털 비디오 소스의 연속적으 로 고려되는 프레임에 기반하지만 제1 생성 단계 S4와 비교하여, 상기 제1 디지털 비디오 스트림의 다른 크로핑, 다른 줌잉, 다른 패닝 및 다른 초점 평면 선택 중 적어도 하나로 출력 디지털 비디오 스트림으로 생성 된다. 다시 말해서, 공유 비디오 스트림에 표시되는 비디오 스트림의 콘텐츠는 크롭되고, 디크롭(de-cropped)되 고, 줌인되고, 줌아웃되고, 수직 및/또는 수평 패팅되고, 및/또는 해당의 비디오 스트림의 초점 평면은 제1 생 성 단계(S4)에서 사용된 비디오 스트림의 현재 크롭/줌/팬/초점 평면 상태와 관련하여 이동된다. 이러한 크롭/팬/줌/포커스 평면은 (픽셀 수준에서 서로 다른 이미지 정보를 가진 여러 개의 가능한 초점 평면을 자체적으로 포함할 수 있는) 기존 비디오 스트림을 기반으로 해당 생성 단계(135, 135\", 135\"') 및/또는 해당 비디오 스트림을 캡처하는 비디오 소스(예를 들어, 디지털 비디오 카메라)에 명령을 전달하여 이에 따라 해당 캡처 매개변수를 수정하는 생성 단계( 135, 135\", 135\", 135\"')에 의해 수행될 수 있다. 예를 들어, 이것은 생 성 단계(135, 135', 135\", 135\"')에 의해 제공되는 명령에 따라, 해당 카메라가 줌잉, 패닝 및/또는 초점 평면 이동한 해당 비디오 스트림을 캡처하는 것을 수반할 수 있다. 후속 게시 단계(S8)에서, 상기 출력 디지털 비디오 스트림은 상술된 일반적인 방식으로, 공유 디지털 비디오 스 트림의 소비자에게, 예를 들어 참여자 클라이언트 및/또는 외부 소비자에게 지속적으로 제공된다. 그 후, 이 방법은 도 9에 도시된 바와 같이 단계 S2로 다시 반복할 수 있다. 후속 단계(S9)에서, 방법은 종료된다. 제1 디지털 비디오 스트림은 제1 디지털 카메라에 의해 연속적으로 캡처될 수 있으며, 제2 디지털 비디오 스트 림은 제2 다른 디지털 카메라에 의해 연속적으로 캡처될 수 있다(따라서 본 명세서에서 이전에 사용된 용어를 사용하여 기본 비디오 스트림을 구성함). 대안적으로, 상술된 서로 다른 각각의 대기 시간(\"시간대\")과 연관되 는 참여자 클라이언트의 여러 다른 그룹(121', 121\", 121\"')을 사용하는 경우, 제1 및/또는 제2 디지털 비 디오 스트림은 각각 이전에 생성된 디지털 비디오 스트림을 구성할 수 있으며, 현재 생성된 공유 비디오 스트림 과 비교하여 다른 대기 시간(\"시간대\")으로 생성될 수 있다(\"시간대\"에 관한 자세한 내용은 위를 참조). 제1 영상 스트림이 이미 생성된 영상 스트림인 경우, 업스트림 카메라에 크롭/줌/팬 설정을 수정하도록 지시하 는 것과는 대조적으로, (필수는 아니지만) 이미 존재하는 제1 비디오 스트림을 기반으로 크로핑/줌잉/패닝을 수 행하는 것이 바람직하다. 본 방법을 이용하면, 생성된 공유 영상의 소비자에게 보다 직관적이고 자연스러운 경험을 제공할 수 있는 공유 영상 스트림의 자동 생성이 가능한데, 이는 각각의 비디오 스트림의 실제 오디오/비디오 콘텐츠는 자동 생성이 하나의 자동 생성 형식에서 다른 형식으로 이동하는 결과를 가져오는 트리거를 감지하는 데 사용되기 때문이다. 트리거는 다양한 요구 사항을 충족하기 위해서, 적절한 매개변수를 사용하여 미리 정의될 수 있다. 예를 들어, 제1 및/또는 제2 비디오 스트림에 묘사된 개인의 행동은 그러한 트리거와 관련하여 자동으로 평가될 수 있으며그러한 행동이 수행되는 방식에 따라 생성 형식을 변경할 수 있다. 다른 예에서, 특정 미리 정의된 트리거는 진 행 중인 생성 중에 해당 생성 형식을 즉석에서 변경하기 위해, 제1 및/또는 제2 비디오 스트림에 묘사된 인간에 의해 전달되는 수동 큐로 사용될 수 있다. 이하, 이러한 트리거 및 해당 생성 형식 변경의 여러 예가 설명된다. 도 10a 및 10b에 도시된 제1 예에서, 미리 결정된 패턴은 제1 디지털 비디오 카메라에 의해 캡처된 것으로 도시된 도 10a에서, 상기 제1 디지털 비디오 스트림에 묘사된 제1 (인간 또는 예를 들어 기계 기계) 참가자 를 포함한다. 제1 참가자는 객체, 도 10a에 도시된 예에서는 제2 (인간 또는 기계) 참가자를 응 시하고, 제2 참가자는 차례로 제2 디지털 비디오 스트림에 묘사된다. 제2 디지털 비디오 스트림은 이 예에 서 제2 디지털 비디오 카메라에 의해 캡처된다. 제2 객체는 인간 참가자 그룹이나 진행 중인 통신에 대한 일반적인 관심의 물리적 객체와 같은 다른 것일 수 있다. 예를 들어, 공유된 비디오 스트림은 의료 절차에 대한 문서일 수 있고, 이에 의해 객체는 환자의 일부일 수 있다. 다른 예에서, 객체는 교육 세션 또는 판매 프리젠테이션의 객체일 수 있다. 객체는 예를 들어 화이트보드 또는 슬라이드 프리젠테이션 화면일 수도 있다. 제1 및 제2 비디오 스트림은 상술된 임의의 유형일 수 있다. 도 10a 및 도 10b에 예시된 경우, 상기 제1 카메라, 상기 참가자 및 상기 객체의 상대적인 방향 에 대한 정보에 기초하여 미리 결정된 이미지 및/또는 오디오 패턴이 감지된다. 이 정보는 예를 들어, 설정/구 성 중에 이전에 제공되고 및/또는 진행 중인 생성 중에 자동으로 감지되는 바와 같이, 시스템(특히 중앙 서버)에 존재할 수 있다. 예를 들어, 제1 카메라 및 제2 카메라의 각각의 위치는 가속도계 및 자이로를 포함하는 MEMS 회로와 같은 측정 수단, 또는 일부 적절한 기하학적 기준에 기초하여 해당 카메라(410, 420)의 현재 위치를 연속적으로 또는 간헐적으로 측정하도록 구성된 스테핑 모터와 같은 그 자체로 기존 위치 파악 수단을 포함하는 카메라에 의해, 각각의 카메라(410, 420)로부터 중앙 서버로 공급될 수 있다. 다른 예에서, 해당 카메라(410, 420) 의 방향은 적절한 자동 디지털 이미지 처리 알고리즘을 사용하여 제3 카메라(도시되지 않음)에 의해 감지될 수 있고, 제3 카메라는 카메라(410, 420) 중 적어도 하나를 이미지로 캡처하고 디지털 이미지 처리를 사용하여 이 캡처된 이미지 정보를 기반으로 해당 상대적인 방향을 결정한다. 이러한 컨텍스트에서, \"배향\"은 위치 및 배향 구성요소 모두를 포함할 수 있다는 점에 유의한다. (예를 들어, 제1 카메라 및/또는 제2 카메라와 관련된) 일부 적절한 기준 시스템과 관련된 제1 참가 자 및 객체의 위치는 제1 카메라 및/또는 제2 카메라에 의해 캡처된 비디오 스트림(들)에 기초한 디지털 이미지 처리를 사용하여 결정될 수 있다. 도 10a에 도시된 바와 같이, 제1 참가자는 도면에서 제2 참가자를 향하지 않고 아래쪽을 응시한다. 이 예에서, 미리 결정된 이미지 및/또는 오디오 패턴은 제1 디지털 비디오 스트림에 기초하여, 제1 참가자(43 0)의 신체 방향, 머리 방향 및 시선 방향 중 적어도 하나에 대한 디지털 이미지 기반 결정에 기초하여 추가로 감지된다. 도 10b에 도시된 바와 같이, 제1 참가자는 제2 참가자를 향해 몸을 돌려, 제2 참가자를 바라보 고(시선을 돌리고) 있다. 제1 참가자의 신체 및 머리 방향은 제1 카메라를 통해 촬영된 제1 비디오 스트림의 디지털 영상 처리 에 기초하여 결정될 수 있다. 이러한 알고리즘은 다음과 같은 관례로, 예를 들어, 다양한 방향으로 회전할 때 제1 카메라에 의해 캡처된 비디오 스트림에서 제1 참가자의 예상 모양에 대한 사전 지식을 사용할 수 있다. 이는 훈련된 신경망이나 기타 기계 학습 구성 요소를 사용하여 수행될 수 있다. 제1 카메라의 상대 적인 방향, 제1 참가자와 객체의 상대적인 위치, 결정된 제1 참가자의 몸 또는 머리 방향과 함 께, 중앙서버는 제1 참가자가 해당 객체를 향해 (머리 또는 몸이) 회전하는지 여부를 결정할 수 있다. 제1 참가자의 시선 방향은 예를 들어 제1 비디오 카메라에 의해 촬영된 이미지를 기반으로 하여 유사 한 방식으로 달성될 수 있다. 이러한 시선 추적 기술은 그 자체로 잘 알려져 있으며, 예를 들어, 제1 참가자 의 눈에 보이는 동공의 위치와 빛의 반사를 식별하는 것에 기초할 수 있다. 해당 트리거는 전환 패턴, 예를 들어, 제1 참가자가 객체를 향하지 않거나 응시하지 않는 상태에서 실제로 제1 참가자가 객체을 사실상 향하고/하거나 응시하는 상태로의 전환의 감지로 정의될 수 있다. 따라서, 중앙 서버는 적절하게 설정된 해당 절대 또는 상대 매개변수 값에 기초하여 이러한 전환을 지속적으로 모니터링할 수 있으며, 일단 이러한 전환이 발생하면 트리거가 감지될 수 있다. 본 실시예 및 다른 실시예에서, 여러 개의 미리 결정된 이미지 및/또는 오디오 패턴을 동시에 모니터링할 수 있 고, 그런 다음 감지된 미리 결정된 이미지 및/또는 오디오 패턴이 감지된 해당 트리거를 구성하고, 이는 다음에 미리 결정된 매개변수화된 생성 논리에 따라 자동 생성 전환이 서로 다른 각각의 해당 모드로 전환한다. 도 10c는 도 10b에 도시된 것과 유사한 제2 예를 도시하지만, 해당 트리거에 대응하는 미리 결정된 이미지 및/ 또는 오디오 패턴은 제1 참가자가 객체로 돌거나 응시하는 것이 아니다. 대신에, 미리 결정된 이미지 및/또는 오디오 패턴은 미리 결정된 제스처를 취하는 (예를 들어 제1 카메라에 의한) 제1 디지털 비디오 스트림에 묘사된 참가자(예를 들어 제1 참가자)를 포함한다. 제스처는 상기 제2 디지털 비디오 스트림에 묘사된 상기 객체에 대한 기하학적 관계의 제스처와 같이 미리 결정되고 매개변수화된 유형의 임의의 제스처일 수 있다. 구체적으로, 제스처는 제1 참가자가 상기 객체 쪽을 가리키는 것일 수 있다(도 10c에 도시된 바와 같이 제1 참가자의 팔이 객체를 향 함). 그러나 제스처는 예를 들어, 제1 참가자의 손이나 손가락에만 기초할 수도 있다. 이 제스처 유형의 사전 결정된 패턴(및 특히 공간에서의 그 방향)은 상기 제1 카메라, 상기 제1 참가자 및 상기 객체의 상대적인 방향에 대한 정보에 기초하여, 도 10b와 관련하여 설명된 상황에 대응하는 방식으로 감지될 수 있고, 또한 제1 디지털 비디오 스트림에 기초한 제1 참가자의 제스처 방향의 디지털 이미지 기반 결정에 기초하여 감지된다. 도 10b에 예시된 경우와 도 10c에 예시된 두 경우에 대해, 제2 카메라의 방향이 감지되어 트리거가 감지되 는지 여부를 결정하는 데에 사용될 수 있다. 예를 들어, 이것은 객체가 제2 카메라에서 표시되는지 확인하는 데 사용될 수 있고, 이는 트리거가 해당 감지된 트리거시 제2 카메라로의 전환과 연관되는 경우 감지되기 위한 조건을 구성할 수 있다. 일부 실시예에서, 제2 생성 단계(S6)는 현재 객체를 보여주고 있는 하나의 제2 카메라(여러 개의 가능한 제2 카메라 중에서)를 결정하고, 그 제2 카메라를 제2 생성 단 계(S6)에서 해당 제2 비디오 스트림을 제공하는 것으로 선택하는 것을 포함할 수 있다. 도 10d에 도시된 예에서, 단 하나의 카메라, 즉 제1 카메라가 있다(다양한 실시예에는 더 많은 카메라와 기타 비디오 스트림 생성 구성 요소가 물론 존재할 수 있다는 것이 이해되지만). 제1 카메라는 제1 참가자 와 객체를 모두 촬영한다. 제1 참가자가 물체를 향해 몸, 머리, 시선을 돌리면, 이 감지된 이미지 패턴이 감지된 트리거를 구성한다. 이 경우, 제2 생성 단계는 해당 생성된 출력 비디오 스트림의 시청자 의 주의를 객체에 집중시키기 위해서, 생성된 출력 비디오 스트림에서 패닝 및/또는 줌잉 및/또는 크롭되 는 제1 카메라에 의해 캡처된 비디오 스트림을 포함할 수 있다. 실제적인 예에서, 제1 참가자의 감지된 주의(위에서 예시된 바와 같이 제1 참가자의 신체, 머리 또는 시선 방향으로 구현됨)는 해당 객체에게 증가된 시각적 초점을 제공하기 위해서, 기존 비디오 스트림을 기 반으로 하거나 해당 기본 비디오 스트림을 생성하는 장치에 지시함으로써, (상기 제2 생성 단계에서) 자동 생성 이 어떤 방식으로든 하이라이트되거나 촛점 이동하게 한다. 도 11a 및 11b에 도시된 또 다른 예에서, 미리 결정된 이미지 및/또는 오디오 패턴은 제2 디지털 비디오 스트림 을 연속적으로 캡처하는 제2 카메라를 응시하는 상기 제2 디지털 비디오 스트림에 묘사된 제1 참가자(43 0)를 포함한다. 구체적으로 도 11a에서, 제1 참가자는 제2 카메라를 응시하지 않는 반면, 도 11a의 제1 참가자는 제2 카메라를 응시한다. 따라서 이러한 감지된 시선 방향의 전환은 해당 대응 트리거의 감지를 구성할 수 있다. 다른 예에서, 미리 결정된 이미지 및/또는 오디오 패턴은 상기 제1 디지털 비디오 스트림 및/또는 상기 제2 디 지털 비디오 스트림의 상대적 움직임 변화를 포함한다. 예를 들어, 제1 비디오 스트림에 나타난 일반적인 움직 임의 양은 매개변수화될 수 있고, 제1 비디오 스트림 및/또는 제2 비디오 스트림의 줌은 일반 움직임을 감소시 키는 함수로서 증가될 수 있거나 그 반대일 수 있으며; 또는 제2 생성 단계는 증가된 일반적인 움직임이 감지된 경우 제2 이미지 스트림으로 전환할 수 있고, 제2 비디오 스트림은 제2 카메라에 의해 캡처되어 제1 카메 라에 의해 묘사된 장면의 더 넓은 각도 또는 더 먼 뷰를 보여준다. 이에 따라, 참가자 음성의 녹음 사운드는 이러한 줌인/줌아웃/카메라 전환 성능을 결정하는 데 사용될 수 있다. 예를 들어, 참가자가 더크게 말하는 것으로 녹화된 경우, 출력되는 공유 비디오 스트림에서 제1 비디오 스트림이 줌아웃될 수 있고 그 반대도 가능하다. 또 다른 예에서, 미리 결정된 이미지 및/또는 오디오 패턴은 결과적으로 주파수 및/또는 진폭 및/또는 진폭 시 간 미분 및/또는 절대 진폭 변화 및/또는 예를 들어 특정 사운드 캡처 마이크의 상대적인 마이크 볼륨에 의해 결정되는 사운드 위치를 특징으로 하는 미리 결정된 사운드 패턴을 포함한다. 이러한 마이크는, 예를 들어, 제1 카메라의 일부일 수도 있고, 독립형 마이크일 수도 있다. 그런 다음 이러한 마이크는 제1 비디오 스트림에 표시되는 장면에서 발생하거나 장면과 직접 관련하여 발생하는 사운드를 녹음하도록 배열된다. 예를 들어, 미리 결정된 사운드 패턴은 적어도 하나의 구두로 발언된 단어를 포함하는 미리 결정된 문구를 포함 할 수 있다. 사운드는 제1 생성 단계에 공급되는 비디오 스트림의 일부로 녹음되어 제공될 수 있고, 상기 미리 정해진 패턴은 중앙서버에 의해 감지될 수 있고, 그런 다음 해당 트리거가 감지되면 생성이 제2 생성 단계 로 전환될 수 있다. 오디오 분석은 미리 결정된 사운드 패턴이 감지되는지 여부를 결정하기 위해 다양한 사운드 정보(예를 들어, 피치, 진폭 및 패턴 매칭)를 사용하는 규칙 기반 결정 엔진; 훈련된 신경망 등과 같은 적합한 디지털 오디오 처리 알고리즘이 사용될 수 있다. 위에서 자세히 설명한 바와 같이, 본 방법은 또한 지연 단계를 포함할 수 있고(도 9 참조), 여기서 적어도 제1 및 제2 디지털 비디오 스트림에 대해 의도적으로 대기 시간이 도입되고, 대기 시간은 공유 디지털 비디오 스트 림에 존재한다. 다음에, 트리거 감지 단계는 상기 대기 시간을 도입하기 전에 제1 및/또는 제2 디지털 비디오 스트림에 기초하여 수행될 수 있다. 대기 시간은 최대 30초, 예를 들어, 최대 5초, 최대 1초, 최대 0.5초일 수 있다. 의도적으로 추가된 지연을 사용하여, 자동 생성은 예를 들어 감지된 트리거(예를 들어, 제2 카메라를 들여 다보는 참가자)를 기반으로 제1 카메라에서 제2 카메라로 자동 전환을 계획할 수 있고, 이 계획 은 이벤트가 생성된 공유 비디오 스트림에 참여하기 전 특정 시간(예를 들어, 0.5초)에 이루어진다. 이러한 방 식으로, 이러한 전환은 실제로 트리거가 발생하는 시간에 정확하게 수행될 수 있거나, 해당 참가자의 음성 리듬에 맞춰지는 것과 같은 다른 매개변수화된 생성 매개변수에 최적으로 맞는 시간에 수행될 수 있다. 상기 유 형의 몇 그룹(121', 121\", 121\"')이 여러개 있을 경우, 이러한 계획은 그룹(121', 121\", 121\"') 중 서로 다른 그룹의 각 참여자 클라이언트(에 의한 소비)와 관련하여 생성된 생성된 출력 비디오 스트림과 관련하여 서 로 다른 시간 범위에서 이루어질 수 있다. 미리 결정된 이미지 및/또는 오디오 패턴은 이벤트 감지 기능 및 패턴 감지 기능과 관련하여 위에서 설명한 유형의 \"이벤트\" 및/또는 \"패턴\"을 구성(및 결정)할 수 있다. 이하는 본 발명이 실제로 어떻게 구현될 수 있는지에 대한 다수의 예이다: 멀티캠 프리젠테이션, 토크쇼 또는 패널 토론에서, 카메라마다 발표자의 카메라 각도가 다를 수 있다. 이 때 자 동 생성 기능이 발표자가 보고 있는 카메라에 따라 다양한 카메라 각도 중에서 자동으로 선택하거나 제스처나 오디오 신호에 의해 트리거되도록 구성될 수 있다. 비디오 팟캐스트나 토킹헤드 비디오에서, 자동 생성 기능은 현재 발표자가 어떤 카메라를 바라보고 있는지에 따 라 여러 다른 카메라 간에 자동으로 전환되도록 구성할 수 있다. 타운홀 미팅에서, 자동 생성 기능은 참석자의 입력이나 질문을 추가하기 위해 청중을 향한 카메라로 전환하도록 구성될 수 있다. 이는 해당 카메라와 관련된 오디오 피드를 모니터링하고 특정 수준에 도달하면 카메라가 라이 브로 전환되거나 음성 명령, 예를 들어, \"청중의 질문\"에 의해 트리거될 수 있다. 제품 프리젠테이션 또는 리뷰에서, 자동 생성 기능은 해당 소스에서 동작을 감지할 때, 또는 다른 트리거를 기 반으로 제품을 가리키는 카메라로 자동 전환하도록 구성할 수 있다. 로봇 수술에서, 수술을 캡처하는 로봇 카메라에 의해 캡처된 비디오 스트림은 미리 결정된 제스처, 오디오 신호 를 감지하면, 또는 외과의사가 외과의사 콘솔을 사용하거나 보고 있지 않다는 것을 인식하면 일반적인 정보 표 시로 대체될 수 있다. 교육 컨텍스트에서, 카메라는 일반 화이트보드나 칠판을 가리키도록 구성할 수 있고, 자동 생성 기능은 교사가 칠판을 향해 제스처를 취하거나 음성 명령을 하면 해당 카메라로 전환되도록 구성될 수 있다. 콘서트나 기타 문화 행사에서, 자동 생성 기능은 가수, 밴드 멤버 또는 오케스트라를 가리키는 카메라 간에 전 환하도록 구성될 수 있다. 이는 제스처를 통해 수행되거나 배우가 보고 있는 카메라에 의해 트리거될 수 있다. 연극 공연에서, 자동 생성 기능은 얼굴 추적, 오디오 신호, 제스처 또는 사전 결정된 일정(런다운)을 따라 말하 는 사람에 따라 다양한 카메라 각도 사이를 전환하도록 구성될 수 있다. 따라서 상술된 유형의 트리거 감지 외에도, 자동 생성은 미리 결정된 일정(런다운)에 따라 한 형식(생성 규칙) 에서 다른 형식으로 전환될 수도 있으며, 물론 경우에 따라 수동 오버라이드도 가능하다. 본 발명은 또한 상술된 내용에 따라 공유 디지털 비디오 스트림을 제공하기 위한 컴퓨터 소프트웨어 기능에 관 한 것이다. 그러한 컴퓨터 소프트웨어 기능은 실행 시 위에서 설명한 수집, 지연, 제1 생성, 트리거 감지, 제2 생성 및 게시 단계 중 적어도 일부를 수행하도록 배열될 수 있다. 컴퓨터 소프트웨어 기능은 전술한 바와 같이 중앙 서버의 물리적 또는 가상 하드웨어에서 실행되도록 배열될 수 있다. 본 발명은 또한 공유 디지털 비디오 스트림을 제공하고 중앙 서버를 포함하는 시스템과 같은 시스템 에 관한 것이다. 중앙 서버는 상술된 수집, 지연, 제1 생성, 트리거 감지, 제2 생성 및 게시 단계 중 적어 도 일부를 수행하도록 배열될 수 있다. 예를 들어, 이들 단계는 상술된 바와 같이 상기 단계를 수행하기 위해 상기 컴퓨터 소프트웨어 기능을 실행하는 중앙 서버에 의해 수행될 수 있다. 수집은 수집 기능에 의 해 수행될 수 있다. 전술한 미리 결정된 이미지 및/또는 오디오 패턴 및 트리거의 감지는 중앙 서버의 이 벤트 및 패턴 감지 기능(132, 134) 또는 생성 기능에 의해 수행될 수 있다. 의도적인 지연은 예를 들어 수 집 기능에 의해, 일반적으로 상술된 방식으로 수행될 수 있다. 게시는 게시 기능에 의해 수행될 수 있다. 입력 비디오 스트림의 시간 동기화, 이벤트 및/또는 패턴 감지, 트리거 감지 등을 포함하는, 상술된 사용 가능 한 입력 비디오 스트림 세트를 기반으로 하는 자동 생성 원칙이 동시에 다른 수준으로 적용될 수 있다는 것이 이해된다. 따라서, 이러한 자동 생성된 비디오 스트림 중 하나는 다운스트림 자동 생성 기능에 대한 사용 가능 한 입력 비디오 스트림을 형성하고 차례로 비디오 스트림을 생성할 수 있다. 중앙 서버는 개별 참여자 클라이언트에 대한 그룹(121', 121\", 121\"') 할당을 제어하도록 배열될 수 있다. 예를 들어, 라이브 비디오 통신 서비스 세션 중에 특정 참여자 클라이언트에 대한 그룹 할당을 동적으로 변경하는 것은 중앙 서버에 의한 상기 비디오 통신 서비스의 자동 생성의 일부일 수 있다. 이러한 재할당 은 예를 들어 개별 참여자 클라이언트 사용자의 요청(해당 클라이언트을 통해 제공됨)에 따라, 미리 결정된 시간표에 기초하여 또는 동적으로, 예를 들어 시간이 지남에 따라 동적으로 변경될 수 있는 매개변수 데 이터의 함수로 트리거될 수 있다. 이에 따라, 중앙 서버는 미리 결정된 시간 슬롯 동안 (예를 들어, 계획된 패널 토론 동안) 예를 들어 특정 그룹만을 사용하여 비디오 통신 서비스 과정 동안 그룹 구조를 동적으로 변경하도록 배열될 수 있다. 미리 결정된 방식으로 그룹 할당을 변경하는 것은 도 9-11b와 관련하여 상술된 바와 같은 것에 대응하는 방식으 로 특정 트리거가 감지된 자동 결과일 수 있다. 상술된 모든 측면에서, 본 발명은 제1 그룹의 적어도 하나의 참여자 클라이언트(제1 그룹은 제1 지연과 연관 됨)가 동일한 제1 그룹 또는 제2 그룹의 적어도 하나의 참여자 클라이언트와 두 방향(양 방향) 방식으로 상호작 용하는, 상호작용 단계를 더 포함할 수 있고, 제2 그룹은 제2 대기 시간과 연관되며, 제2 대기 시간은 제1 대기 시간과 다르다. 이러한 참여자 클라이언트는 모두 위에서 설명한 유형의 동일한 통신 서비스에 대한 참여자가 될 수 있다는 것이 이해된다. 이상, 바람직한 실시예가 설명되었다. 그러나, 본 발명의 기본 사상에서 벗어나지 않고 개시된 실시예에 많은 수정이 이루어질 수 있다는 것이 당업자에게 명백하다. 예를 들어, 많은 추가 기능이 본 명세서에 기술된 시스템의 일부로서 제공될 수 있으며, 이는 본 명세서에 기술되지 않는다. 일반적으로, 현재 설명된 솔루션은 비디오 데이터 스트림이 통신에 사용되는 다양한 구체적인 애플리케이션을 충족하기 위해 세부 기능을 구축할 수 있는 프레임워크를 제공한다. 일반적으로, 현재 방법과 관련하여 언급된 모든 내용은 현재 시스템 및 컴퓨터 소프트웨어 제품에 적용 가능하 며, 그 반대의 경우도 마찬가지이다. 따라서, 본 발명은 설명된 실시예에 제한되지 않고 첨부된 청구범위의 범 위 내에서 변경될 수 있다.도면 도면1 도면2 도면3 도면4 도면5 도면6a 도면6b 도면6c 도면6d 도면6e 도면6f 도면7 도면8 도면9 도면10a 도면10b 도면10c 도면11a 도면11b"}
{"patent_id": "10-2024-7027326", "section": "도면", "subsection": "도면설명", "item": 1, "content": "이하에서, 본 발명은 본 발명의 예시의 실시예와 관련하여 첨부된 도면을 참조하여 자세히 설명한다: 도 1은 제1 예시의 시스템을 도시한다; 도 2는 제2 예시의 시스템을 도시한다; 도 3은 제3 예시의 시스템을 도시한다; 도 4는 중앙 서버를 도시한다; 도 5는 제1 방법을 도시한다; 도 6a-6f는 도 5에 도시된 방법의 다른 방법 단계와 관련하는 후속 상태를 도시한다; 도 7은 개념적으로 공통 프로토콜을 도시한다; 도 8은 제4 예시의 시스템을 도시한다; 도 9는 제2 방법을 도시한다; 도 10a-11b는 상기 및 다양한 구성의, 카메라와 참가자를 도시한다."}
