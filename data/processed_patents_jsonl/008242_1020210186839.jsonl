{"patent_id": "10-2021-0186839", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0097389", "출원번호": "10-2021-0186839", "발명의 명칭": "얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델 및 이를 포함하는 음성 인식 장", "출원인": "한양대학교 산학협력단", "발명자": "장준혁"}}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 정보를 입력 정보로 하고, 상기 음성 정보에 대응되는 얼라이먼트(alignment) 정보를 출력 정보로 하는 제1인공신경망 모듈을 포함하는 제1음향 모델(Acoustic Model); 및상기 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대응되는 텍스트 정보를 출력 정보로 하는 제2인공신경망 모듈을 포함하는 제2음향 모델;을 포함하고,상기 제2음향 모델은,상기 얼라이먼트 정보를 기초로 상기 제2음향 모델의 학습을 수행하는데 필요한 레퍼런스 데이터를 생성한 후,상기 레퍼런스 데이터 및 상기 텍스트 정보를 기초로 생성된 손실함수를 이용하여 상기 제2음향 모델의 파라미터를 보정하는 방법으로 학습을 수행하는 것을 특징으로 하는,얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 제2음향 모델은,상기 얼라이먼트 정보를 기초로 어텐션 스코어 메트릭스(attention score matrix) 정보를 생성한 후, 상기 어텐션 스코어 메트릭스 정보를 기초로 학습을 수행하는 것을 특징으로 하는,얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 제2음향 모델은,상기 음성 정보 및 상기 얼라이먼트 정보 사이에 대응되는 부분을 구분하여 복수 개의 구간으로 분리한 후, 각각 대응되는 구간들 사이의 유사도(similarity) 정보를 기초로, 학습을 수행하는 것을 특징으로 하는, 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3항에 있어서,상기 제2음향 모델은,상기 음성 정보 및 상기 얼라이먼트 정보를 사일런스(silence) 구간과 스피치(speech) 구간으로 분리한 후, 학습을 수행하는 것을 특징으로 하는, 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4항에 있어서,상기 제2음향 모델은,상기 음성 정보의 사일런스 구간의 학습과, 상기 음성 정보의 스피치 구간은 각각 별도로 학습이 이루어지는 것을 특징으로 하는, 공개특허 10-2023-0097389-3-얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,상기 제2음향 모델은,상기 음성 정보에서의 사일런스 구간과 상기 얼라이먼트 정보에서의 사일런스 구간이 서로 대응되는 영역을 제1영역으로 구분하고, 상기 음성 정보에서의 스피치 구간과 상기 얼라이먼트 정보에서의 스피치 구간이 서로 대응되는 영역을 제2영역으로 구분한 후,상기 제1영역과 상기 제2영역은 서로 다른 방법으로 학습을 수행하는 것을 특징으로 하는, 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 제2음향 모델은,상기 제1영역의 유사도 값이 상기 제2영역의 유사도 값보다 더 큰 값을 가지도록 학습을 수행하는 것을 특징으로 하는,얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 제2음향 모델은,상기 제1영역의 유사도 값은 1에 수렴하도록 학습을 수행하고, 상기 제2영역의 유사도 값은 0에 수렴하도록 학습을 수행하는 것을 특징으로 하는,얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 4항에 있어서,상기 제2음향 모델은,상기 음성 정보의 사일런스 구간과 상기 음성 정보의 사일런스 구간에 대응되는 상기 얼라이먼트 정보의 사일런스 구간 사이의 제1-1유사도 값; 및상기 음성 정보의 사일런스 구간과 상기 음성 정보의 사일런스 구간에 대응되는 상기 얼라이먼트 정보의 스피치구간 사이의 제2-1유사도 값;을 기초로 학습을 수행하는 것을 특징으로 하는, 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 제2음향 모델은,상기 제1-1유사도 값이 상기 제2-1유사도 값보다 더 큰 값을 가지도록 학습을 수행하는 것을 특징으로 하는,얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2023-0097389-4-제 4항에 있어서,상기 제2음향 모델은,상기 음성 정보의 스피치 구간과 상기 음성 정보의 스피치 구간에 대응되는 상기 얼라이먼트 정보의 스피치 구간 사이의 제1-2유사도 값; 및상기 음성 정보의 스피치 구간과 상기 음성 정보의 스피치 구간에 대응되는 상기 얼라이먼트 정보의 사일런스구간 사이의 제2-2유사도 값;을 기초로 학습을 수행하는 것을 특징으로 하는, 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 제2음향 모델은,상기 제1-2유사도 값이 상기 제2-2유사도 값보다 더 큰 값을 가지도록 학습을 수행하는 것을 특징으로 하는,얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 얼라이먼트(alignment) 정보를 출력 정보로 하는 제1인공신경망 모듈을 포함하는 제1음향 모델(Acoustic Model)을 이용하여, 상기 얼라이먼트 정보를 생성하는 단계; 및상기 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 텍스트 정보를 출력 정보로 하는 제2인공신경망 모듈을 포함하는 제2음향 모델을 이용하여 상기 텍스트 정보를 출력하는 단계;를 포함하고,상기 텍스트 정보를 출력하는 단계는,상기 얼라이먼트 정보를 기초로 상기 제2음향 모델의 학습을 수행하는데 필요한 레퍼런스 데이터를 생성한 후,상기 레퍼런스 데이터 및 상기 텍스트 정보를 기초로 생성된 손실함수를 이용하여 상기 제2음향 모델의 파라미터를 보정하는 방법으로 학습을 수행하는 단계를 포함하는 것을 특징으로 하는,얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델 학습 방법."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 얼라이먼트(alignment) 정보를 출력 정보로 하는 제1인공신경망 모듈을 포함하는 제1음향 모델(Acoustic Model); 및상기 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 텍스트 정보를 출력 정보로 하는 제2인공신경망 모듈을 포함하는 제2음향 모델;을 포함하고,상기 제2음향 모델은,상기 얼라이먼트 정보를 기초로 상기 제2음향 모델의 학습을 수행하는데 필요한 레퍼런스 데이터를 생성한 후,상기 레퍼런스 데이터 및 상기 텍스트 정보를 기초로 생성된 손실함수를 이용하여 상기 제2음향 모델의 파라미터를 보정하는 방법으로 학습을 수행하는 것을 특징으로 하는,얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델을 포함하는 음성 인식 장치."}
{"patent_id": "10-2021-0186839", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14항에 있어서,상기 제2음향 모델은,상기 얼라이먼트 정보를 기초로 어텐션 스코어 메트릭스(attention score matrix) 정보를 생성한 후, 생성된 어텐션 스코어 메트릭스 정보를 기초로 학습을 수행하는 것을 특징으로 하는,공개특허 10-2023-0097389-5-얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델을 포함하는 음성 인식 장치."}
{"patent_id": "10-2021-0186839", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델은 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대응되는 얼라이먼트 (alignment) 정보를 출력 정보로 하는 제1인공신경망 모듈을 포함 하는 제1음향 모델(Acoustic Model) 및 상기 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대응되는 텍스트 정보를 출력 정보로 하는 제2인공신경망 모듈을 포함하는 제2음향 모델;을 포함하고, 상기 제2음향 모델은, 상기 얼라이먼트 정보를 기초로 상기 제2음향 모델의 학습을 수행하는데 필요한 레퍼런스 데이터를 생성한 후, 상기 레퍼런스 데이터 및 상기 텍스트 정보를 기초로 생성된 손실함수를 이용하여 상기 제2음향 모델의 파라미터를 보 정하는 방법으로 학습을 수행할 수 있다."}
{"patent_id": "10-2021-0186839", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델 및 이를 포함하는 음성 인식 장치에 관 한 발명으로서, 보다 상세하게는 얼라이먼트 정보를 기초로 생성된 어텐션 정보를 이용하여 음성 구간과 노이즈 구간을 분리하여 음향 모델에 대해 학습을 수행하는 기술에 관한 발명이다."}
{"patent_id": "10-2021-0186839", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "음성은 인간의 가장 자연스러운 의사 소통 수단이면서 정보 전달 수단이자, 언어를 구현하는 수단으로서 인간이 내는 의미 있는 소리이다. 기술이 발전함에 따라 인간과 기계 사이의 음성을 통한 통신 구현에 대한 연구가 계속 이루어지고 있다. 더욱이 최근 음성 정보를 효과적으로 처리하기 위한 음성 정보 처리 기술(speech information technology;SIT) 분야가 괄목할 만한 발전을 이룩함에 따라 실생활에서도 적용이 되고 있다. 이러한 음성 정보 처리 기술을 크게 분류하면, 음성 인식(speech recognition), 음성 합성(speech synthesis), 화자 인증(speaker identification and verification), 음성 코딩(speech coding) 등의 카테고리로 분류될 수 있다. 음성 인식은 발화된 음성을 인식하여 문자열로 변환하는 기술이고, 음성 합성은 문자열을 음성 분석에서 얻어진 데이 터나 파라미터를 이용하여 원래의 음성으로 변환하는 기술이며, 화자 인증은 발화된 음성을 통하여 발화자 를 추정하 거나 인증하는 기술이며 음성 코딩은 음성 신호를 효과적으로 압축하여 부호화하는 기술이며, 음성합 성 기술은 실제 응용방식에 따라 크게 두 가지로 구분될 수 있다. 제한된 어휘 개수와 구문구조의 문장만을 합 성하는 제한 어휘합성 또는 자동음성응답시스템(ARS; Automatic Response System)과 임의의 문장을 입력받아 음 성 합성하는 무제한 어휘합성 또는 텍스트-음성 변환(TTS; Text-to-Speech) 시스템이 있다. 그 중, 텍스트-음성 변환(TTS) 시스템은 작은 합성 단위음성과 언어 처리를 이용하여 임의의 문장에 대한 음성 을 생성한다. 언어 처리를 이용하여 입력된 문장을 적당한 합성 단위의 조합으로 대응시키고, 문장으로부터 적 당한 억양과 지속시간을 추출하여 합성음의 운율을 결정한다. 언어의 기본 단위인 음소, 음절 등의 조합에 의해 음성을 합성해 내므로 합성 대상어휘에 제한이 없으며 주로 TTS(Text-to-Speech) 장치 및 CTS(Context-to- Speech) 장치 등에 적용된다. 특히, 인공 지능 기술이 발달함에 따라 인공 신경망 기반 알고리즘은 기존 모델 대비 큰 성능 향상을 보여주고 있다. 일반적으로 인공 신경망을 이용한 음성 합성 모델은 음향 모델 부분을 인공 신경망으로 대신하여, 인공 신경망이 분석된 문장 데이터를 기반으로 음성 파라미터를 합성할 수 있다. 대표적인 음향 모델로는 트랜스포머(transformer) 네트워크를 활용한 음향 모델이 많이 활용되고 있는데, 트랜 스포머에 기초한 음향 모델의 경우 많은 파라미터로 인해서 학습을 수행하는데 쉽게 오버피팅(overfitting) 되 는 문제점을 가지고 있다. 또한, 학습을 수행함에 있어서 가용한 데이터가 제한적인 상황인 경우 또는 발화 구 간 대비 침묵 구간이 상대적으로 긴 데이터를 기초로 학습을 수행하는 경우 쉽게 오버피팅이 되는 문제점이 존 재한다 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제10-1871604호 (2018.06.25. 공개) - '심화 신경망을 이용한 다채널 마이크 기 반의 잔향시간 추정 방법 및 장치' (특허문헌 0002) 한국등록특허 제10-1988504호 (2019.06.05.) - '딥러닝에 의해 생성된 가상환경을 이용한 강화"}
{"patent_id": "10-2021-0186839", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "학습 방법' 발명의 내용"}
{"patent_id": "10-2021-0186839", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "따라서, 일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델 및 이를 포함하는 음성 인식 장치는 앞서 설명한 문제점을 해결하기 위해 고안된 발명으로서, 음향 모델이 학습을 수행함에 있어서 제 약적인 상황에서 쉽게 오버 피팅 되는 것을 방지하여, 음성 인식의 효율성을 높일 수 있는 기술을 제공하는데 목적이 있다. 보다 구체적으로는 다른 음향 모델에서 출력된 얼라이먼트 정보를 기초로 생성된 어텐션 정보를 이용하여 텍스 트 정보를 출력하는 음향 모델에 대해 음성 구간과 노이즈 구간에 대해 각각 별도의 학습의 수행하여, 음향 모 델이 오버 피팅이 되는 것을 방지하는 것에 목적이 있다."}
{"patent_id": "10-2021-0186839", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델은 음성 정보를 입력 정보로 하 고, 상기 음성 정보에 대응되는 얼라이먼트(alignment) 정보를 출력 정보로 하는 제1인공신경망 모듈을 포함하 는 제1음향 모델(Acoustic Model) 및 상기 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대응되는 텍스트 정보를 출력 정보로 하는 제2인공신경망 모듈을 포함하는 제2음향 모델;을 포함하고, 상기 제2음향 모델은, 상 기 얼라이먼트 정보를 기초로 상기 제2음향 모델의 학습을 수행하는데 필요한 레퍼런스 데이터를 생성한 후, 상 기 레퍼런스 데이터 및 상기 텍스트 정보를 기초로 생성된 손실함수를 이용하여 상기 제2음향 모델의 파라미터 를 보정하는 방법으로 학습을 수행할 수 있다. 상기 제2음향 모델은, 상기 얼라이먼트 정보를 기초로 어텐션 스코어 메트릭스(attention score matrix) 정보를 생성한 후, 상기 어텐션 스코어 메트릭스 정보를 기초로 학습을 수행할 수 있다. 상기 제2음향 모델은, 상기 음성 정보 및 상기 얼라이먼트 정보 사이에 대응되는 부분을 구분하여 복수 개의 구 간으로 분리한 후, 각각 대응되는 구간들 사이의 유사도(similarity) 정보를 기초로, 학습을 수행할 수 있다. 상기 제2음향 모델은, 상기 음성 정보 및 상기 얼라이먼트 정보를 사일런스(silence) 구간과 스피치(speech) 구 간으로 분리한 후, 학습을 수행할 수 있다. 상기 제2음향 모델은, 상기 음성 정보의 사일런스 구간의 학습과, 상기 음성 정보의 스피치 구간은 각각 별도로 학습이 이루어질 수 있다. 상기 제2음향 모델은, 상기 음성 정보에서의 사일런스 구간과 상기 얼라이먼트 정보에서의 사일런스 구간이 서 로 대응되는 영역을 제1영역으로 구분하고, 상기 음성 정보에서의 스피치 구간과 상기 얼라이먼트 정보에서의 스피치 구간이 서로 대응되는 영역을 제2영역으로 구분한 후, 상기 제1영역과 상기 제2영역은 서로 다른 방법으 로 학습을 수행할 수 있다. 상기 제2음향 모델은, 상기 제1영역의 유사도 값이 상기 제2영역의 유사도 값보다 더 큰 값을 가지도록 학습을 수행할 수 있다. 상기 제2음향 모델은, 상기 제1영역의 유사도 값은 1에 수렴하도록 학습을 수행하고, 상기 제2영역의 유사도 값 은 0에 수렴하도록 학습을 수행할 수 있다. 상기 제2음향 모델은, 상기 음성 정보의 사일런스 구간과 상기 음성 정보의 사일런스 구간에 대응되는 상기 얼 라이먼트 정보의 사일런스 구간 사이의 제1-1유사도 값 및 상기 음성 정보의 사일런스 구간과 상기 음성 정보의 사일런스 구간에 대응되는 상기 얼라이먼트 정보의 스피치 구간 사이의 제2-1유사도 값;을 기초로 학습을 수행 할 수 있다. 상기 제2음향 모델은, 상기 제1-1유사도 값이 상기 제2-1유사도 값보다 더 큰 값을 가지도록 학습을 수행할 수 있다. 상기 제2음향 모델은, 상기 음성 정보의 스피치 구간과 상기 음성 정보의 스피치 구간에 대응되는 상기 얼라이 먼트 정보의 스피치 구간 사이의 제1-2유사도 값 및 상기 음성 정보의 스피치 구간과 상기 음성 정보의 스피치 구간에 대응되는 상기 얼라이먼트 정보의 사일런스 구간 사이의 제2-2유사도 값;을 기초로 학습을 수행할 수 있 다.상기 제2음향 모델은, 상기 제1-2유사도 값이 상기 제2-2유사도 값보다 더 큰 값을 가지도록 학습을 수행할 수 있다. 일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델 학습 방법은, 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 얼라이먼트(alignment) 정보를 출력 정보로 하는 제1인공신경망 모듈을 포함하는 제1음향 모델(Acoustic Model)을 이용하여, 상기 얼라이먼트 정보를 생성하는 단계 및 상기 음 성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 텍스트 정보를 출력 정보로 하는 제2인공신경망 모듈을 포함하는 제2음향 모델을 이용하여 상기 텍스트 정보를 출력하는 단계를 포함하고, 상기 텍스트 정보를 출력하는 단계는, 상기 얼라이먼트 정보를 기초로 상기 제2음향 모델의 학습을 수행하는데 필요한 레퍼런스 데 이터를 생성한 후, 상기 레퍼런스 데이터 및 상기 텍스트 정보를 기초로 생성된 손실함수를 이용하여 상기 제2 음향 모델의 파라미터를 보정하는 방법으로 학습을 수행하는 단계를 포함할 수 있다. 일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델을 포함하는 음성 인식 장치는, 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 얼라이먼트(alignment) 정보를 출력 정보로 하 는 제1인공신경망 모듈을 포함하는 제1음향 모델(Acoustic Model) 및 상기 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 텍스트 정보를 출력 정보로 하는 제2인공신경망 모듈을 포함하는 제2음향 모델을 포 함하고, 상기 제2음향 모델은, 상기 얼라이먼트 정보를 기초로 상기 제2음향 모델의 학습을 수행하는데 필요한 레퍼런스 데이터를 생성한 후, 상기 레퍼런스 데이터 및 상기 텍스트 정보를 기초로 생성된 손실함수를 이용하 여 상기 제2음향 모델의 파라미터를 보정하는 방법으로 학습을 수행할 수 있다. 상기 제2음향 모델은, 상기 얼라이먼트 정보를 기초로 어텐션 스코어 메트릭스(attention score matrix) 정보를 생성한 후, 생성된 어텐션 스코어 메트릭스 정보를 기초로 학습을 수행할 수 있다."}
{"patent_id": "10-2021-0186839", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델 및 이를 포함하는 음성 인식 장 치는 얼라이먼트 정보를 기초로 생성된 어텐션 정보를 이용하여 음성 구간과 노이즈 구간에 각각 별도의 학습을 수행하므로 음향 모델이 과도하게 오버 피팅이 되는 것을 방지할 수 있는 효과가 존재한다. 또한, 음향 모델의 얼라이먼트가 명확하게 구분되어지므로, 음향 모델의 어텐션 헤드 간의 다양성을 증진시킬 수 있어, 보다 정확한 정보를 출력할 수 있는 장점이 존재한다."}
{"patent_id": "10-2021-0186839", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명에 따른 실시 예들은 첨부된 도면들을 참조하여 설명한다. 각 도면의 구성요소들에 참조 부호를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 부호를 가 지도록 하고 있음에 유의해야 한다. 또한, 본 발명의 실시 예를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 발명의 실시예에 대한 이해를 방해한다고 판단되는 경우에는 그 상세한 설명은 생략한 다. 또한, 이하에서 본 발명의 실시 예들을 설명할 것이나, 본 발명의 기술적 사상은 이에 한정되거나 제한되지않고 당업자에 의해 변형되어 다양하게 실시될 수 있다. 또한, 본 명세서에서 사용한 용어는 실시 예를 설명하기 위해 사용된 것으로, 개시된 발명을 제한 및/또는 한정 하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\", \"구비하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들 이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않 는다. 또한, 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"간접적으로 연결\"되어 있는 경우도 포함하며, 본 명세서에 서 사용한 \"제 1\", \"제 2\" 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지 만, 상기 구성 요소들은 상기 용어들에 의해 한정되지는 않는다. 아래에서는 첨부한 도면을 참고하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략한다. 한편 본 명세서에 따른 발명의 명칭은 '얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델 및 이를 포함하는 음성 인식 장치'로 기재하였으나, 이하 설명의 편의를 위해 '얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델 및 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델을 포함하는 음성 인식 장 치'는 각각 '음향 모델'과 '음성 인식 장치'로 축약하여 설명하도록 한다. 도 1은 일 실시예에 따른 음성 인식 시스템의 일부 구성 요소를 도시한 블럭도이다. 도 1에 도시된 바와 같이, 인공 신경망을 이용한 음성 인식 시스템은, 음성 인식 장치와 서버를 포 함할 수 있으며, 각각의 구성요소는 네트워크에 의해 서로 통신 연결되어 있을 수 있다. 일 예로, 음성 인식 장치와 서버는 5G 통신 환경에서 서로 연결될 수 있으며, 도 1에 도시된 기기들 이 외에 가정 또는 사무실에서 사용되는 다양한 전자 기기들이 사물 인터넷 환경 하에서 서로 연결되어 동작할 수 있다. 음성 인식 장치는 입력되는 음성 데이터에 대해 대응되는 텍스트 정보 또는 음성 정보를 출력하는 장치로서, 출력 정보를 효율적으로 출력할 수 있도록 하는 각종 인공지능 알고리즘을 수행하는데 필요한 장치 또는 프로그램 코들이 마련되어 있으며, 인공 신경망을 작동시키고 학습시키는데 필요한 데이터 또한 저장되어 있을 수 있다. 음성 인식 장치는 인공 신경망 모듈에 의한 학습과 추론 및 입력된 음성 데이터에 대응되는 텍스트 정보 또는 음성 정보를 출력할 수 있는 장치로, 서버(Server), PC, 태블릿 PC, 스마트 폰(smart phobne), 스마트와치 (smart watch), 스마트 글라스(smart glass), 웨어러블 기기(wearable device) 등과 같은 장치로 구현될 수 있 으며, 특정 어플리케이션이나 프로그램으로 구현될 수 도 있다. 음성 인식 장치가 사용자 단말기 형태의 장치로 구현되는 경우, 음성 인식 장치에는 음성 인식 어플리 케이션 또는 음성 인식 사이트에 접속할 수 있는 프로그램이 설치되어 있으며, 사용자는 인증 과정을 거친 후 음성 인식 서비스를 제공받을 수 있다. 서버는 각종 인공지능 알고리즘을 적용하는데 필요한 빅데이터 및 음성 인식 모델 생성 장치를 동작시 키는 데이터를 제공하는 데이터베이스 서버일 수 있다. 그 밖에 서버는 음성 인식 장치에 설치된 음성 합성 어플리케이션 또는 음성 합성 웹 브라우저를 이용하여 음성 인식 장치의 동작을 원격에서 제어할 수 있도록 하는 웹 서버 또는 애플리케이션 서버를 포함할 수 있다. 구체적으로 서버는 각종 데이터 베이스를 기초로 음향 모델을 생성하거나, 트랜스포머 네트워크 기반의 음 향 모델을 생성하고 학습시킬 수 있으며, 이렇게 생성된 모델은 서버에 저장되어 음성 인식 서비스를 사용 자에게 제공하거나, 음성 인식 장치에 내재되어 사용자에게 음성 인식 서비스를 제공해줄 수도 있다. 여기서 인공 지능(artificial intelligence, AI)은, 인간의 지능으로 할 수 있는 사고, 학습, 자기계발 등을 컴 퓨터가 할 수 있도록 하는 방법을 연구하는 컴퓨터 공학 및 정보기술의 한 분야로, 컴퓨터가 인간의 지능적인 행동을 모방할 수 있도록 하는 것을 의미할 수 있다. 또한, 인공지능은 그 자체로 존재하는 것이 아니라, 컴퓨터 과학의 다른 분야와 직간접으로 많은 관련을 맺고 있다. 특히 현대에는 정보기술의 여러 분야에서 인공지능적 요소를 도입하여, 그 분야의 문제 풀이에 활용하려 는 시도가 매우 활발하게 이루어지고 있다. 머신 러닝(machine learning)은 인공지능의 한 분야로, 컴퓨터에 명시적인 프로그램 없이 배울 수 있는 능력을 부여하는 연구 분야를 포함할 수 있다. 구체적으로 머신 러닝은, 경험적 데이터를 기반으로 학습을 하고 예측을 수행하고 스스로의 성능을 향상시키는 시스템과 이를 위한 알고리즘을 연구하고 구축하는 기술이라 할 수 있다. 머신 러닝의 알고리즘들은 엄격하게 정해진 정적인 프로그램 명령들을 수행하는 것이라기보다, 입력 데이터를 기반으로 예측이나 결정을 이끌어내기 위해 특정한 모델을 구축하는 방식을 취할 수도 있다. 네트워크는 인공 신경망을 이용한 다화자 음성 인식 장치와, 서버를 연결하는 역할을 수행할 수 있다. 이러한 네트워크는 예컨대 LANs(local area networks), WANs(wide area networks), MANs(metropolitan area networks), ISDNs(integrated service digital networks) 등의 유선 네트워크나, 무선 LANs, CDMA, 블루투스, 위성 통신 등의 무선 네트워크를 망라할 수 있으나, 본 발명의 범위가 이에 한정되는 것은 아니다. 또한 네트워 크는 근거리 통신 및/또는 원거리 통신을 이용하여 정보를 송수신할 수 있다. 여기서 근거리 통신은 블루 투스(Bluetooth), RFID(radio frequency identification), 적외선 통신(IrDA, infrared data association), UWB(ultra-wideband), ZigBee, Wi-Fi (wireless fidelity) 기술을 포함할 수 있고, 원거리 통신은 CDMA(code division multiple access), FDMA(frequency division multiple access), TDMA(time division multiple access), OFDMA(orthogonal frequency division multiple access), SC-FDMA(single carrier frequency division multiple access) 기술을 포함할 수 있다. 네트워크는 허브, 브리지, 라우터, 스위치 및 게이트웨이와 같은 네트워크 요소들의 연결을 포함할 수 있다. 네 트워크는 인터넷과 같은 공용 네트워크 및 안전한 기업 사설 네트워크와 같은 사설 네트워크를 비롯한 하나 이 상의 연결된 네트워크들, 예컨대 다중 네트워크 환경을 포함할 수 있다. 네트워크에의 액세스는 하나 이상의 유 선 또는 무선 액세스 네트워크들을 통해 제공될 수 있다. 더 나아가 네트워크는 사물 등 분산된 구성 요소들 간 에 정보를 주고 받아 처리하는 IoT(Internet of Things, 사물인터넷) 망 및/또는 5G 통신을 지원할 수 있다. 도 1에서는 사용자에게 음성 인식 서비스를 제공하는 장치는 음성 인식 장치를 기준으로 설명하고, 음성 인 식 장치에 포함되는 음향 모델을 생성하는 장치는 서버를 기준으로 설명하였으나, 본 발명의 실시예가 이러한 실시예로 한정되는 것은 아니고, 음성 인식 장치가 스스로 음향 모델을 생성하고, 이를 기초로 음성 인식 서비스를 사용자에게 제공해 줄 수 도 있다. 지금까지 본 발명에 따른 음성 인식 시스템에 대해 알아보았다. 이하 본 발명의 특징에 해당하는 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델에 대해 알아본다. 도 2은 본 발명의 일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델의 일부 구성 요소를 도시한 블록도이다. 도 2를 참조하면, 일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델은, 입력 되는 음성 정보를 기초로 얼라이먼트 정보를 출력하는 제1음향 모델과, 음성 정보 및 얼라이 먼트 정보를 기초로 음성 정보에 대응되는 텍스트 정보를 출력하는 제2음향 모델을 포함할 수 있다. 구체적으로, 제1음향 모델은 음향 모델에 입력되는 음성 정보를 입력 정보로 하고, 음성 정보(2 0)에 대응되는 얼라이먼트(alignment) 정보를 출력 정보로 하는 기 학습된 제1인경신경망 모듈을 포함하는 모델 을 의미할 수 있다. 따라서, 제1인공신경망 모듈은 입력 정보인 음성 정보를 기초로 얼라이먼트 정보를 출력 정보로 출력하는 추론 세션(미도시)과, 출력 정보와 레퍼런스 정보를 기초로 제1음향 모델에 대해 학습을 수행하는 학습 세션(미도시)을 구비할 수 있다. 제1음향 모델에 따른 음향 모델(Acoustic model)은 기 공지되어 있는 모델을 활용할 수 있다. 일 예로, 제 1음향 모델은 DNN(Deep Neural Network) 또는 HMM(Hidden Markow Model)에 기반한 음향 모델이 차용될 수 있으 며, 얼라이먼트 정보를 출력할 수 있는 인공신경망 네트워크이면 본 발명의 실시예에 포함될 수 있다. 제1음향 모델에서 출력하는 얼라이먼트 정보는 음성 인식 분야에서 흔히 쓰는 alignment에 대한 정보를 의 미하는 것으로서, 구체적으로는 입력되는 음성과 이에 대응되는 텍스트와의 시간적 정렬에 대한 정보를 의미하 며, 제1음향 모델에서 출력되는 얼라이먼트 정보는 제2음향 모델이 학습을 수행하는데 필요한 레퍼런 스 데이터가 될 수 있다. 제2음향 모델은 음향 모델에 입력되는 음성 정보를 입력 정보로 하고, 음성 정보에 대응되는 텍스트 정보를 출력 정보로 하는 기 학습된 제2인경신경망 모듈을 포함하는 모델을 의미한다. 따라서, 제2 인공신경망 모듈은 입력 정보인 음성 정보를 기초로 텍스트 정보를 출력 정보로 출력하는 추론 세션(미 도시)과, 출력 정보와 이에 대한 레퍼런스 정보를 기초로 제2음향 모델에 대해 학습을 수행하는 학습 세션(미도 시)을 구비할 수 있으며, 일 예로 제2음향 모델은 RNN을 사용하지 않고, 인코더-디코더 구조를 기초로 어 텐션(attention) 기법만을 활용하여 구현한 트랜스포머(Transformer) 모델이 차용될 수 있다. 구체적으로 제2음향 모델은 얼라이먼트 정보를 기초로 제2음향 모델이 학습을 수행하는데 필요한 레퍼런스 데이터를 생성한 후, 생성된 레퍼런스 데이터와 제2음향 모델이 출력하는 텍스트 정보를 기초로 손실함수를 생성하고, 생성된 손실함수를 이용하여 제2음향 모델의 파라미터를 보정 하는 방법으로 학습을 수행 할 수 있다. 제2음향 모델은 학습을 수행함에 있어서, 얼라이먼트 정보를 기초로 어텐션 스코어 메트릭스 (attention score matrix) 정보를 생성한 후, 생성된 어텐션 스코어 메트릭스 정보를 기초로 학습을 수행할 수 있는데, 구체적으로 제2음향 모델은 음성 정보 및 얼라이먼트 정보 사이에 대응되는 부분을 구분 하여 복수 개로 구간으로 분리한 후, 각각 대응되는 구간들 사이의 유사도(similarity) 정보를 기초로, 학습을 수행할 수 있다. 또한, 제2음향 모델은 음성 정보 및 얼라이먼트 정보에 포함되어 있는 정보를 음성이 존재하지 않 는 사일런스(silence) 구간과 음성이 존재하는 스피치(speech)구간으로 분리한 후, 학습을 수행할 수 있다. 이 에 대한 자세한 설명은 후술하도록 한다. 도 3은 본 발명에 따른 제2음향 모델의 일부 구성 요소를 도시한 블럭도이고, 도 4는 본 발명에 따른 제2음향 모델의 임베딩 모듈에 입력되는 정보와 출력되는 정보를 도시한 도면이다. 도 3을 참조하면, 본 발명의 일 실시예에 따른 제2음향 모델은 임베딩부, 인코더부 및 디코더부 를 포함할 수 있으며, 임베딩부는 제1임베딩부와 제2임베딩부를 포함할 수 있다. 한편, 본 발명에 따른 제2음향 모델은 순환 신경망(Recurrent neural network) 구조는 아니나, seq2seq처럼 인코더 부에서 입력 시퀀스를 입력 받고, 디코더부에서 출력 시퀀스를 출력하는 인코더-디코더 구조를 가질 수 있다. 구체적으로, 제1임베딩부는 인코더부에 입력되는 단위 텍스트에 대해 임베딩을 한 정보를 인코더부 의 입력 정보로 입력 할 수 있다. 보다 구체적으로 도 3에 도시된 바와 같이 인코더부의 제1인코더 로 입력 정보로 송신할 수 있다. 제2임베딩부는 인코더부에 입력되는 단위 텍스트에 대한 레퍼런스 정보에 해당하는 텍스트를 디코더부의 제1디코더의 입력 정보로 송신할 수 있다. 인코더부와 디코더부는 도 3에 도시된 바와 같이 각각 적어도 하나의 인코더와 디코더를 포함할 수 있다. 본 발명에 따른 인코더부와 디코더부에 포함되어 있는 인코더와 디코더의 개수는 특정 개수로 한정되는 것은 아니고 사용 환경 및 목적에 따라 1개부터 N개까지 다양하게 설정될 수 있으며, 디코더부는 기존의 seq2seq 구조처럼 시작 심볼 <sos>를 입력으로 받아 종료 심볼 <eos>가 나올 때까지 연산을 진행할 수 있다. 본 발명에 따른 임베딩부는 제1단위 텍스트를 입력 받아 총 3개의 임배딩 정보를 가지는 벡터를 출력 할 수 있다. 구체적으로, 제1임베딩부는 제1단위 텍스트를 입력 받고, 제1단위 텍스트에 대한 워드 임베딩, 위치 임베딩 및 구별(segment) 임베딩을 출력할 수 있으며, 제2임베딩부 제2단위 텍스 트를 입력 받고, 제2단위 텍스트에 대한 워드 임베딩, 위치 임베딩 및 구별(segment) 임베딩 을 출력할 수 있다. 워드 임베딩은 단어에 대한 임베딩으로서 실질적인 입력이 되는 임베딩에 해당한다. 단어보다 더 작은 단위 로 쪼개는 서브워드 토크나이저를 이용하여 서브워드들을 병합해가는 방식으로 최종 단위 집합인 단어 (vocabulary)를 만들어 갈 수 있다. 따라서, 임베딩 벡터의 종류는 단어 집합의 크기와 동일할 수 있다. 본 발명에 따른 제2음향 모델은 RNN처럼 자연어 처리를 위해 단어를 순차적으로 입력 받는 방식이 아니기 때문에, 단어의 위치 정보를 위치 임베딩을 이용하여 표현할 수 있다. 위치 임베딩은 위치 정보를 표현 하는 임베딩으로서, 해당 워드 임베딩이 벡터가 몇 번째 위치한 단어인지에 대한 위치 정보를 포함하고 있으며, 학습을 통해서 위치 임베딩에 대한 정보를 획득할 수 있다. 위치 임베딩의 경우 워드 임베딩처럼 임베딩 층(Embedding layer)을 하나 더 사용하여 표현된다. 따라 서, 도 4에 도시된 바와 같이 입력되는 문장의 길이가 4라면(총 단어가 4개라면) 4개의 위치 임베딩 벡터를 학 습시키며, 워드 임베딩이 입력 될 때마다 위치 임베딩 벡터가 더해지는 프로세스로 진행될 수 있다. 즉, 첫번째 단어의 임베딩 벡터는0번 위치 임베딩 벡터가 더해지고, 두번째 단어의 임베딩 벡터는 1번 위치 임 베딩 벡터가 더해지고, 세번째 단어의 임베딩 벡터는 2번 포지션 임베딩 벡터가 더해지고, 네번째 단어의 임베 딩 벡터는 3번 포지션 임베딩 벡터가 더해질 수 있다. 즉, 이와 같은 방식으로 인해 순서가 정해질 수 있으므로, 제2음향 모델에 입력되는 벡터는 순서 정보가 고려된 임베딩 벡터가 입력된다고 볼 수 있다. 또한, 제2음향 모델에 입력되는 임베딩 벡터는 구별 임베딩이 워드 임베딩과 위치 임베딩과 결합되어 같이 입력될 수 있는데, 구별 임베딩은 입력되는 2개의 문장을 구별하기 위한 임베딩을 의미한다. 따라서, 첫번째 문장의 경우 첫번째 문장에 해당하는 모든 단어에 대해 0 임베딩 벡터가 부여되고, 두번째 문장 의 경우 두번째 문장에 해당하는 모든 단어에 대해 1임베딩 벡터가 부여될 수 있다. 제2임베딩부에 입력되는 제2단위 텍스트 또한 제2임베딩부를 통과하여 제2단위 텍스트에 대한 워드 임베딩, 위치 임베딩 및 구별(segment) 임베딩을 출력할 수 있다. 각각의 임베딩이 출력되는 과정 은 제1임베딩부와 동일한바 생략하도록 한다. 본 발명에 따른 제2음향 모델은 다중 셀프 어텐션 방법을 사용할 수 있다. 셀프 어텐션(self-attention)이 란 음향 모델이 각각의 프로세서를 병렬적으로 수행하는 과정을 의미하는데, 일반적으로 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구하고, 구해낸 이 유사도를 가중치로 하여 키와 맵 핑되어있는 각각의 '값(Value)'에 반영하며, 유사도가 반영된 '값(Value)'을 모두 가중합하여 리턴합을 하는 것 을 특징으로 한다. 셀프 어텐션 함수는 이러한 셀프 어텐션을 자기 자신에게 수행한다는 것을 의미한다. 구체적으로, 어텐션 함수 에서 Q(Query)는 t 시점의 디코더 셀에서의 은닉 상태를 의미하고, K(Keys)는 모든 시점의 인코더 셀의 은닉 상태들을 의미하며, V(Values)는 모든 시점의 인코더 셀의 은닉 상태들을 의미한다. 여기서 t 시점이라는 것은 계속 변화하면서 반복적으로 쿼리를 수행하므로 결국 전체 시점에 대해서 일반화를 할 수 있으므로, 어텐션 함 수의 경우 셀의 은닉 상태가 Q이고 인코더 셀의 은닉 상태가 K라는 점에서 Q와 K가 서로 다른 값을 가지고 있는 데, 셀프 어텐션 함수는 Q, K, V가 전부 동일한 특징을 가지고 있다. 따라서, 셀프 어텐션의 경우 특정 단어와 입력 문장 내의 단어들끼리의 유사도를 구하는 방법으로 특정 단어가 입력 문장 내의 단어들과 어떠한 관계를 가지고 있는지 알 수 있는 장점이 존재한다. 다중 셀프 어텐션의 경우 다양한 기준에 따라 셀프 어텐션을 여러 번 수행하는 것을 의미하며, 구체적으로 scaled dot-product 어텐션을 병렬 처리하기 위해 셀프 어텐션을 여러 번 수행하는 것을 의미한다. 다중 셀프 어텐션을 수행하는 경우 셀프 어텐션을 다양한 기준에 따라 여러 번 수행하는 것을 의미하므로, 하나의 기준에 의해 셀프 어텐션을 수행하는 경우보다 더 정확한 결과값을 도출해 낼 수 있다. 다중 셀프 어텐션은 다양한 기준에 따라 셀프 어텐션을 여러 번 수행하기에 때문에 어텐션 헤드(Attention- head)는 그 수 만큼 여러 개 생성될 수 있다. 본 발명의 실시예가 특정한 개수로 한정되는 것은 아니나, 일 예 로 어텐션 헤드는 총 4개 내지 8개가 생성될 수 있으며, 이렇게 생성된 어텐센 헤드는 후술할 학습 방법에 의해 학습이 되어 변형될 수 있다. 이에 대한 자세한 설명은 후술하도록 한다. 도 5는 음향 모델에 주로 사용되는 두 종류의 뉴럴 네트워크인CNN (Convolutional neural network)와 트랜스포 머(transformer)를 비교하여 도시한 도면이다. 도 5를 참조하면, CNN의 경우 상대적으로 짧은 구간의 스피치 시그널(speech signal)만을 참조하여 음소 정보를 추론하는 반면, 본 발명의 제2음향 모델이 차용할 수 있는 트랜스포머 모델은 짧은 프레임의 음소 정보를추론할 때 전체 스피치 구간을 참조하여 추론을 하기 때문에, 문맥의 전반적인 정보를 활용할 수 있어 종래의 CNN에 따른 모델보다 비교적 정확한 정보를 출력할 수 있다. 그러나, 트랜스포머 모델의 경우, 음성 정보가 없는 사일런스(silence) 구간 내에 노이즈(noise)가 존재하는 경 우 문맥 전체의 정보를 고려하다 보니 음성 정보를 추론함에 있어서 이러한 노이즈의 영향을 크게 받을 수 있다. 따라서, 이를 적절하게 제어하지 않는다면, 상대적으로 긴 사일런스 구간 또는 사일런스 구간 내에 환경 소음(ambient noise)이 존재하는 경우에 음향 모델의 성능을 크게 저하시킬 수 있는 문제점이 존재한다. 따라서, 일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델 및 이를 포함하는 음성 인식 장치는 이러한 문제점을 해결하기 위해 고안된 발명으로서, 얼라이먼트 정보를 기초로 생성된 어텐션 정보 를 이용하여 음성 구간과 노이즈 구간을 분리하여 음향 모델에 대해 학습을 수행함으로써 보다 정확한 정보를 출력할 수 있게 하는데 그 목적이 있다. 이하 도면을 통해 음향 모델의 학습 방법에 대해 자세히 알아본다. 도 6은 본 발명의 일 실시예에 따른 제2음향 모델이 구간별로 학습을 수행하는 방법을 설명하기 위한 도면으로 서, 도 6의 표는 얼라이먼트 정보를 기초로 어텐션 스코어 메트릭스(attention score matrix) 정보를 의미한다. 어텐션 스코어 메트릭스는 입력(음성)과 출력(텍스트) 사이의 유사도(similarity)를 표현한다고 이해하여도 무 방하며 각 원소들은 표에 도시된 바와 같이0~1 사이의 확률값으로 normalize되어 있다. 구체적으로, 도 6의 표의 가로축은 음성 입력되는 음성 정보에 대해 시간에 따라 구간별로 사일런스 구간과 스 피치 구간을 나누어 표시한 축이고, 도 6의 세로축은 얼라이언먼트 정보를 시간에 따라 구간별로 사일런스 구간 과 스피치 구간을 나누어 표시한 축이며, 표 안에 기재되어 있는 각각의 숫자는 음성 정보와 얼러이먼트 정보 사이의 구간별 유사도(similarity) 값을 일 예로 나타낸 것이다. 또한, 도 6의 표에서 빗금치지 않은 영역은 제1 영역으로 지칭될 수 있으며, 반대로 도 6의 표에서 빗금친 영역 은 제2영역으로 지칭될 수 있다. 구체적으로, 제1영역은 음성 정보에서의 사일런스 구간과 얼라이먼트 정보에서의 사일런스 구간이 서로 대응되 는 영역(제1-1영역)과 음성 정보에서의 스피치 구간과 얼라이먼트 정보에서의 스피치 구간이 서로 대응되는 영 역(제1-2영역)으로 나뉘어지며, 제1-1영역의 유사도 값을 제1-1 유사도 값으로 지칭할 수 있고, 제1-2영역의 유 사도 값을 제1-2 유사도 값으로 지칭할 수 있고, 이와 반대로, 제2영역은 음성 정보에서의 사일런스 구간과 얼라이먼트 정보에서의 스피치 구간이 서로 대응되는 영역(2-1영역)과 음성 정보에서의 스피치 구간과 얼라이먼트 정보에서의 사일런스 구간이 서로 대응되는 영역 (2-2영역)으로 나뉘어지며, 제2-1영역의 유사도 값을 제2-1 유사도 값으로 지칭할 수 있고, 제2-2영역의 유사도 값을 제2-2 유사도 값으로 지칭할 수 있고, 제2음향 모델을 통해 텍스트 정보가 정확히 출력되기 위해서는 도 5의 표의 가로축에 해당하는 음성 정보 와 도 5의 표의 세로축에 해당하는 얼라이먼트 정보 사이의 유사도 값이 서로 대응되는 구간 사이에서는 높고 그렇지 않은 구간에서는 낮아야 한다. 즉, 제1영역의 경우 유사도 값이 상대적으로 높고, 제2영역의 경유 유사 도 값이 상대적으로 낮아야 음향 모델이 출력하는 정보가 정확도가 높다고 말할 수 있다. 따라서, 제2음향 모델은 학습을 수행함에 있어서, 제1영역의 유사도 값이 제2영역의 유사도 값보다 높은 값을 가지도록 학습을 수행할 수 있다. 구체적으로 제2음향 모델은 사일런스 구간을 추론함에 있어서 1-1영역과 1-2영역에서의 정보를 이용할 수 있는데, 1-1영역에서의 유사도 값이 2-1영역에서의 유사도 값보다 큰 값을 가지도록 학습을 수행(패널티를 부여)할 수 있다. 이와 반대로, 제2음향 모델은 스피치 구간을 추론함에 있어서 1-2영역과 2-2영역에서의 정보를 이용할 수 있는데, 1-2영역에서의 유사도 값이 2-2영역에서의 유사도 값보다 큰 값을 가지도록 학습을 수행(패널티를 부여)할 수 있다. 제2음향 모델이 학습을 수행하는 방법은 크게 2가지 방법으로 학습을 수행할 수 있는데, 제1손실함수를 생 성한 후 이를 기초로 학습을 수행하는 방법과, 제2손실함수를 생성하여 이를 기초로 학습을 수행하는 방법으로 나누어 질 수 있다. 구체적으로, 제1손실함수는 제2영역의 유사도 값을 합산한 값을 손실함수(lossL2)로 하여 이를 최소화하는 방법 으로 학습을 수행하는 방법이며, 구체적으로 아래 수학식 과 같이 표현될 수 있다. 수학식"}
{"patent_id": "10-2021-0186839", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 에서 는 target vector이며, A는 TxT 사이즈의 2차원 매트릭스이며, T는 시간을 의미한다. 그리고 aij는 attention score matrix(A)의 원소이며, K는 class의 개수를 의미한다. 예를 들어 설명하면, 음성 정보가 총 5초로 이루어져 있는 신호라 하고(이 경우 T=5이다) 3~4초 구간에서는 음 성 신호가 포함되어 있는 스피치 구간이고, 나머지 구간은 음선 신호가 존재하지 않는 사일런스 구간이라 가정 한다. 그리고 음성을 1초 단위로 나누는 경우 음성은 총 5초이므로 총 다섯 개 구간으로 나누어 질 수 있으며, 이 때 스피치 구간을 1로 표기하고 사일런스 구간을 0으로 표기할 수 있다. 이 경우 수학식 에서 Y^=[0,0,1,1,0]로 표현될 수 있으며, 수학식 에서 Ⅱ(Y^i,Y^j) 함수는 두개의 입력 이 동일할 경우 0을 두개의 입력이 다를 경우 1을 출력하게 되므로, 이러한 값들을 이용하여 제1손실함수를 생 성할 수 있다. 제2손실함수(lossrank)는 제1손실함수를 변형한 식으로 아래와 같이 수학식 로 표현할 수 있으며, 제2손실함 수는 cross attend의 경우가 그렇기 않은 경우보다 낮은 유사도 값을 유지하도록 하는데 목적을 가진다. 수학식"}
{"patent_id": "10-2021-0186839", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 에서 max 함수는 두 개의 값 중 큰 값을 고르는 함수이며 m은 마진(margin)을 의미하고 일반적으로 0 이 사용된다 따라서, max 함수의 오른쪽 식이 양수가 되는 경우 패널티가 부여될 수 있다. 수학식 에서 두번 째 식은 제1영역의 경우 -1이 출력되고 제2영역의 경우 1이 출력되는 식을 의미한다. 제2손실함수를 기준으로 학습을 수행하는 방법은 lossrank는 하나의 기준 값(i)과 두개의 샘플링된 값을 비교한다. 두개의 샘플(j,k)은 각각 스피치 혹은 사일런스일 수 있으므로 총 여덟 가지 경우의 수가 가능하다. 예를 들어, i=스피치, j=스피치, k=스피치인 경우, =1이며 = -1인 된다. 따라서 이 경우의 loss 값은 0이 된다. 다른 예로, I=스피치, j=스피치, k=사일런스인 경우 =1, =0이 되며 이 된다. 따라서 loss 값 은 2*max(0, aik-aij)이 된다. 따라서 스피치-사일런스 어탠딩(attending 구간(aik)의 유사도 값이 스피치-스피 치 어탠딩 구간(aij)의 값보다 큰 경우 패널티(aik-aij)가 부여될 수 있으며, 작은 경우에는 max(a,b)함수에 따라 0값이 부여될 수 있다. 한편, 제2음향 모델은 이렇게 산출된 제1손실함수와 제2손실함수는 각각의 함수에 서로 다른 가중치가 적 용된 후에 합산 된 제3손실함수를 생성한 후, 제3손실함수를 기초로 학습을 수행할 수 도 있다. 도 7의 (a)는 종래 기술에 따른 음향 모델과 본 발명에 따른 음향 모델을 비교하기 위한 도면으로서, 도 7의 (b)는 다수의 어텐션 헤드 중 한 개의 어텐션 헤드만 제어하는 경우 발생하는 효과를 설명하기 위한 도면이다. 도 7의 (a)를 참조하면, 기존의 트랜스포머를 통해서 학습하면 음성 구간과 노이즈 구간 사이에 정확한 얼라이 먼트가 잡히지 않기 떄문에, 음향 모델이 음성, 노이즈 구간을 정확하게 구별하지 못하는 단점이 존재하는 것을 알 수 있다. 그러나 본 발명에 따른 음향 모델의 경우 트랜스포머의 어텐션 스코어 메트릭스에 직접적인 제약을 가하는 방법 으로 학습을 수행하므로 도 7의 (a)의 가장 왼쪽에 표현된 바와 같이 어텐션 스코어 메트릭스의 얼라이먼트가 정확하게 잡히는 것을 확인할 수 있으며, 스피치 구간과 사일런스 구간이 정확하게 구분되는 것을 확인할 수 있 다. 제2헤드, 제3헤드 및 제4헤드의 경우 별도의 제약을 가하지 않았으므로 출력 결과는 별 차이가 존재하지 않 는 것을 알 수 있다. 한편, 제2음향 모델의 트랜스포머를 학습함에 있어서 앞서 설명한 바와 같이 다수의 어텐션 헤드를 사용하게 되 는데, 각 헤드 간에 유사도가 높은 경우 다양성이 상실되어 attention collapse 문제가 발생할 수 있다. 따라서, 본 발명의 경우 모든 어텐션 헤드에 제약을 가하는(학습을 수행하여 파라미터를 조정하는) 방법을 사용 하지 않고, 학습 효율을 가장 높이기 위해 복수 개의 어텐션 헤드 중 하나의 헤드만에 제약을 가하는 방법으로 제2음향 모델에 대해 학습을 수행할 수 있다. 본 발명과 같이 한 개의 어텐션 헤드에만 제약을 가하는 경우 도 7의 (b)에 도시된 바와 같이 한 개의 어텐션 헤드만을 규제하는 것이 가장 높은 성능의 증진을 보이는 것을 알 수 있다. 이는 한 개의 어텐션 헤드만을 규제 하는 것이 헤드 간 다양성(diversity)을 높여 음향 모델의 표현력을 증진시키기 때문이다. 도 8은 종래 기술에 따른 음향 모델과 본 발명에 따른 음향 모델의 성능을 비교하기 위한 도면으로서, 도 8의 (a)는 종래 기술에 따른 음향 모델의 성능을, 도 8의 (b)는 본 발명에 따른 음향 모델을 성능을 표시한 도면이 다. 도 8에서 빨간색 선은 감지된 스피치 구간을, 검은색 선은 감지된 사일런스 구간을 도시한 것이다. 종래 기술에 따른 음향 모델의 경우 도 8의 (a)에 도시된 바와 같이 음성의 끝점을 제대로 잡아내지 못해서 음 성 신호 내 사일런스 구간을 제대로 분리 해내지 못하는 모습을 보여주고 있다. 그러나 본 발명의 경우 도 8의 (b)에 도시된 바와 같이 음성 신호의 끝점을 잘 찾아내어서, 음성 신호 내 사일런스 구간을 잘 찾아내는 것을 알 수 있다. 지금까지 도면을 통해 본 발명에 따른 일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음 향 모델 및 이를 포함하는 음성 인식 장치에 대해 자세하 알아보았다. 일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델 및 이를 포함하는 음성 인식 장 치는 얼라이먼트 정보를 기초로 생성된 어텐션 정보를 이용하여 음성 구간과 노이즈 구간에 각각 별도의 학습을 수행하므로 음향 모델이 과도하게 오버 피팅이 되는 것을 방지할 수 있는 효과가 존재한다. 또한, 음향 모델의 얼라이먼트가 명확하게 구분되어지므로, 음향 모델의 어텐션 헤드 간의 다양성을 증진시킬 수 있어, 보다 정확한 정보를 출력할 수 있는 장점이 존재한다. 한편, 본 명세서에 기재된 \"~부\"로 기재된 구성요소들, 유닛들, 모듈들, 컴포넌트들 등은 함께 또는 개별적이지 만 상호 운용 가능한 로직 디바이스들로서 개별적으로 구현될 수 있다. 모듈들, 유닛들 등에 대한 서로 다른 특 징들의 묘사는 서로 다른 기능적 실시예들을 강조하기 위해 의도된 것이며, 이들이 개별 하드웨어 또는 소프트 웨어 컴포넌트들에 의해 실현되어야만 함을 필수적으로 의미하지 않는다. 오히려, 하나 이상의 모듈들 또는 유 닛들과 관련된 기능은 개별 하드웨어 또는 소프트웨어 컴포넌트들에 의해 수행되거나 또는 공통의 또는 개별의 하드웨어 또는 소프트웨어 컴포넌트들 내에 통합될 수 있다. 특정한 순서로 작동들이 도면에 도시되어 있지만, 이러한 작동들이 원하는 결과를 달성하기 위해 도시된 특정한 순서, 또는 순차적인 순서로 수행되거나, 또는 모든 도시된 작동이 수행되어야 할 필요가 있는 것으로 이해되지 말아야 한다. 임의의 환경에서는, 멀티태스킹 및 병렬 프로세싱이 유리할 수 있다. 더욱이, 상술한 실시예에서 다양한 구성요소들의 구분은 모든 실시예에서 이러한 구분을 필요로 하는 것으로 이해되어서는 안되며, 기술된 구성요소들이 일반적으로 단일 소프트웨어 제품으로 함께 통합되거나 다수의 소프트웨어 제품으로 패키징될 수 있다는 것이 이해되어야 한다. 컴퓨터 프로그램(프로그램, 소프트웨어, 소프트웨어 어플리케이션, 스크립트 또는 코드로도 알려져 있음)은 컴 파일되거나 해석된 언어나 선험적 또는 절차적 언어를 포함하는 프로그래밍 언어의 어떠한 형태로도 작성될 수 있으며, 독립형 프로그램이나 모듈, 컴포넌트, 서브루틴 또는 컴퓨터 환경에서 사용하기에 적합한 다른 유닛을포함하여 어떠한 형태로도 전개될 수 있다. 부가적으로, 본 특허문헌에서 기술하는 논리 흐름과 구조적인 블럭도는 개시된 구조적인 수단의 지원을 받는 대 응하는 기능과 단계의 지원을 받는 대응하는 행위 및/또는 특정한 방법을 기술하는 것으로, 대응하는 소프트웨 어 구조와 알고리즘과 그 등가물을 구축하는 데에도 사용 가능하다. 본 명세서에서 기술하는 프로세스와 논리 흐름은 입력 데이터 상에서 작동하고 출력을 생성함으로써 기능을 수 행하기 위하여 하나 이상이 컴퓨터 프로그램을 실행하는 하나 이상이 프로그래머블 프로세서에 의하여 수행 가 능하다. 본 기술한 설명은 본 발명의 최상의 모드를 제시하고 있으며, 본 발명을 설명하기 위하여, 그리고 당업자가 본 발명을 제작 및 이용할 수 있도록 하기 위한 예를 제공하고 있다. 이렇게 작성된 명세서는 그 제시된 구체적인 용어에 본 발명을 제한하는 것이 아니다. 이상에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자 또는 해당 기술 분야에 통상의 지식을 갖는 자라면, 후술될 특허청구범위에 기재된 본 발명의 사상 및 기술 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다. 따라서, 본 발명의 기술적 범위는 명세서의 상세한 설명에 기재된 내용으로 한정되는 것이 아니라 특허청구범위 에 의해 정해져야 할 것이다."}
{"patent_id": "10-2021-0186839", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 음성 인식 시스템의 일부 구성 요소를 도시한 블럭도이다. 도 2은 본 발명의 일 실시예에 따른 얼라이먼트 정보를 기초로 구간별 학습을 수행하는 음향 모델의 일부 구성 요소를 도시한 블록도이다. 도 3은 본 발명에 따른 제2음향 모델의 일부 구성 요소를 도시한 블럭도이고, 도 4는 본 발명에 따른 제2음향 모델의 임베딩 모듈에 입력되는 정보와 출력되는 정보를 도시한 도면이다. 도 5는 음향 모델에 주로 사용되는 두 종류의 뉴럴 네트워크인CNN (Convolutional neural network)와 트랜스포 머(transformer)를 비교하여 도시한 도면이다. 도 6은 본 발명의 일 실시예에 따른 제2음향 모델이 구간별로 학습을 수행하는 방법을 설명하기 위한 도면이다. 도 7의 (a)는 종래 기술에 따른 음향 모델과 본 발명에 따른 음향 모델을 비교하기 위한 도면으로서, 도 7의 (b)는 다수의 어텐션 헤드 중 한 개의 어텐션 헤드만 제어하는 경우 발생하는 효과를 설명하기 위한 도면이다. 도 8은 종래 기술에 따른 음향 모델과 본 발명에 따른 음향 모델의 성능을 비교하기 위한 도면이다."}
