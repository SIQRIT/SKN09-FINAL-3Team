{"patent_id": "10-2010-0026970", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2011-0107692", "출원번호": "10-2010-0026970", "출원인": "동서대학교산학협력단", "발명자": "이동훈"}}
{"patent_id": "10-2010-0026970", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템에 있어서,영상을 디스플레이하는 스크린과;상기 스크린의 전방 상단에 설치되며 상기 스크린의 전방에 적외선을 수직으로 발생하여 인터랙티브 막을 생성하는 적외선 LED 어레이 바와;상기 스크린의 후방에 설치되며 상기 스크린에 영상을 투영하는 2개의 3차원 입체 프로젝터와;상기 스크린 전방에서 상기 인터랙티브 막에 인체나 물체가 접촉되었을 때 반사되는 적외선 빛을 촬영하는 적외선 카메라와;상기 적외선 카메라로부터 획득한 정보로 영상처리를 통하여 상기 인체나 물체의 인터랙션 위치의 좌표값을 추출하는 인식 컴퓨터; 및상기 인식 컴퓨터로부터 추출된 인터랙션 위치의 좌표값을 입체 콘텐츠와의 인터랙션을 통해 렌더링 정보를 생성하여 상기 3차원 입체 프로젝터를 통해 3D 입체 영상을 생성하는 렌더링 컴퓨터;를 포함하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템."}
{"patent_id": "10-2010-0026970", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "에 기재된 발명은, 「제 1 항에 있어서, 상기 인식 컴퓨터는: 상기 적외선 카메라로부터 획득한 정보를 [0023]영상 이진화를 통하여 LED 빛이 반사된 영역과 그렇지 않은 영역으로 분리한 후 블랍 및 라벨링을 통해 상기 반사된 영역을 묶어서 각각 번호를 지정한 다음, 호모그래피 행렬을 이용하여 신호 왜곡을 보정하고 윈도우 API을통해 좌표를 보정 한 후, 상기 인터랙션 막에 접촉된 인체나 물체의 좌표값을 추출하는 것을 특징으로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템.」을 제공한다."}
{"patent_id": "10-2010-0026970", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "에 기재된 발명은, 「제 1 항에 있어서, 상기 렌더링 컴퓨터는: 오우거 3D(Ogre 3D)를 이용하여 가상 [0024]공간상에서 스테레오 카메라를 생성하여 각각의 카메라 뷰를 렌더링하고, 상기 렌더링 된 정보를 상기 3차원 입공개특허 10-2011-0107692-7-체 프로젝터를 통해 각각의 좌, 우 영상을 투영하여 3D 입체 영상을 생성하는 것을 특징으로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템.」을 제공한다."}
{"patent_id": "10-2010-0026970", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "에 기재된 발명은, 「제 1 항에 있어서, 상기 적외선 LED 어레이 바는: 복수 개의 적외선 LED(IR- [0025]LEDs)를 일정 간격으로 배열하여 구성되고, 상기 적외선 LED 양쪽에 격벽을 형성한 것을 특징으로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템.」을 제공한다."}
{"patent_id": "10-2010-0026970", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "에 기재된 발명은, 「제 1 항에 있어서, 상기 적외선 카메라는: CCD 카메라, USB CCD 카메라, 웹 카메 [0026]라를 포함한 카메라 중 하나이며, 상기 카메라의 렌즈 앞에 적외선 투과 필터(IR-Filter)를 장착한 것을 특징으로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템.」을 제공한다.또한, 전술한 기술적 과제를 해결하기 위한 수단으로서, 청구항 6에 기재된 발명은, 「공간 멀티 인터랙션 기반 [0027]3차원 입체 인터랙티브 비전 방법에 있어서, (a) 스크린 앞쪽에 적외선을 발생하여 인터랙티브 막을 생성하는단계와; (b) 상기 인터랙티브 막에 인체나 물체가 접촉되었을 때 반사되는 적외선 빛을 적외선 카메라로 촬영하는 단계와; (c) 상기 적외선 카메라로부터 획득한 정보로 영상처리를 통하여 상기 인체나 물체의 인터랙션 위치의 좌표값을 추출하는 단계와; (d) 상기 추출된 좌표값을 입체 콘텐츠와의 인터랙션을 통해 렌더링 정보를 생성하는 단계; 및 (e) 상기 생성된 렌더링 정보를 2대의 3차원 입체 프로젝터를 통해 각각의 좌우 영상을 투영하여3D 입체 영상을 생성하는 단계;를 포함하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법.」을 제공한다."}
{"patent_id": "10-2010-0026970", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법에 있어서,(a) 스크린 앞쪽에 적외선을 발생하여 인터랙티브 막을 생성하는 단계와;(b) 상기 인터랙티브 막에 인체나 물체가 접촉되었을 때 반사되는 적외선 빛을 적외선 카메라로 촬영하는 단계와;(c) 상기 적외선 카메라로부터 획득한 정보로 영상처리를 통하여 상기 인체나 물체의 인터랙션 위치의 좌표값을추출하는 단계와;(d) 상기 추출된 좌표값을 입체 콘텐츠와의 인터랙션을 통해 렌더링 정보를 생성하는 단계; 및(e) 상기 생성된 렌더링 정보를 2대의 3차원 입체 프로젝터를 통해 각각의 좌우 영상을 투영하여 3D 입체 영상을 생성하는 단계;를 포함하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법."}
{"patent_id": "10-2010-0026970", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "에 기재된 발명은, 「제 6 항에 있어서, 상기 (c)단계에서 인터랙션 위치정보를 추출하는 방법은: (c1) [0028]상기 적외선 카메라로부터 획득한 정보를 영상 이진화를 통하여 LED 빛이 반사된 영역과 그렇지 않은 영역으로분리하는 이진화 단계와; (c2) 상기 반사된 영역을 묶어주는 블랍 단계와; (c3) 상기 블랍을 통해 묶인 영역들에 대해 번호를 지정하는 라벨링 단계와; (c4) 상기 라벨링 이후 호모그래피 행렬을 이용하여 신호 왜곡을 보정하는 호모그래피 단계와; (c5) 상기 적외선 카메라가 보는 화면에서 영상의 각 모서리 4군데의 좌표를 입력하여4개의 점으로 이루어진 사각형 내의 이미지를 새로운 4개의 점으로 이루어진 사각형으로 변환하여 왜곡을 보정하는 윈도우 API 좌표보정 단계; 및 (c6) 상기 인체나 물체의 인터랙션 위치를 파악하여 좌표값을 추출하는 단계;를 포함하는 것을 특징으로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법.」을 제공한다."}
{"patent_id": "10-2010-0026970", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "에 기재된 발명은, 「제 6 항 또는 제 7 항에 있어서, 상기 좌표값은: 상기 스크린의 좌측 상단(0,0)을 [0029]기준으로 하는 윈도우 좌표계의 형태로 변경하여 좌표 데이터를 추출하는 것을 특징으로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법.」을 제공한다."}
{"patent_id": "10-2010-0026970", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "에 기재된 발명은, 「제 6 항에 있어서, 상기 (d)단계에서 좌표값을 입체 콘텐츠와 인터랙션 하는 방법 [0030]은: 상기 좌표값을 이용하여 광선추적(ray tracing) 기법으로 오브젝트들이 카메라를 기준으로 거리별로 정렬되게 한 후, 가장 가까이 있는 오브젝트가 인터렉션이 일어나도록 처리하는 것을 특징으로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법.」을 제공한다."}
{"patent_id": "10-2010-0026970", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "에 기재된 발명은, 「제 6 항에 있어서, 상기 (e)단계에서 3D 입체 영상을 생성하는 방법은: 오우거 [0031]3D(Ogre 3D)를 이용하여 가상 공간상에서 스테레오 카메라를 생성하여 각각의 카메라 뷰를 렌더링하고, 상기 렌더링 된 정보를 상기 3차원 입체 프로젝터를 통해 각각의 좌, 우 영상을 투영하여 3D 입체 영상을 생성하는 것을 특징으로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법.」을 제공한다.발명의 효과본 발명에 따르면, 사용자가 스크린에서 멀리 떨어져 있으면 디스플레이 기능만 하고 사용자가 스크린의 인터랙 [0032]티브 영역에 들어오면 사용자에게 콘텐츠와 인터랙션을 할 수 있는 기능을 제공하며, 한 명 또는 다수의 인원이동시에 참여할 수 있도록 프로젝션 방식을 통해 대규모 인터랙티브 인터페이스를 제공하여 사용자에게 재미와몰입감 있는 인터랙션 콘텐츠를 제공한다.또한, 다양한 인터랙션 정보들을 전처리 과정을 통해 인식하게 되며 스크린의 크기가 커지면 다수의 사용자가 [0033]함께 사용이 가능하며, 작은 공간에서 대규모 공간까지 아무런 제약 없이 운용 가능한 확장성을 가지고 있다.또한, 적외선 인터랙션 막을 통해 사용자가 제한된 스크린 면이 아닌 공간에서 자연스럽게 손동작으로 인터랙션 [0034]을 함으로써 사용자가 직접 다양한 콘텐츠를 직접 몸으로 느끼고 체험할 수 있다.공개특허 10-2011-0107692-8-또한, 사용자의 손동작에 따라 입체 영상의 물체가 반응하여 움직임으로써, 사용자가 영상 속의 개체들과 직접 [0035]인터랙션을 할 수 있다.본 발명의 효과는 이상에서 언급된 것들에 한정되지 않으며, 언급되지 아니한 다른 효과들은 아래의 기재로부터 [0036]당업자에게 명확하게 이해되어 질 수 있을 것이다.도면의 간단한 설명도 1은 종래 기술에 따른 실시간 가상 현실 스포츠 플랫폼 장치의 구성도 [0037]도 2는 본 발명의 바람직한 실시 예에 의한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템을 개략적으로 나타낸 구성도도 3은 적외선 LED 어레이 바(IR-LEDs Array Bar)의 제품 사진도 4는 그래픽 랜더링 및 에니메이션을 처리하는 OGRE 엔진 코어 오브젝트(Engine Core Objects)의 기능별 구성도도 5는 3차원 입체 영상을 생성하기 위해 카메라를 사용하여 영상을 획득하는 방법을 설명하기 위한 도면도 6은 시차의 종류를 설명하기 위한 도면도 7은 인터랙션 막에서의 손 영역을 추출하는 원리를 설명하기 위한 도면도 8은 영상 왜곡 보정을 위한 호모그래픽(Homography) 알고리즘 과정을 보여주는 예시도도 9 내지 도 16은 본 발명의 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템을 구현한 예의 캡처화면으로서,도 9은 후면 스크린의 정면 모습의 캡처 화면이고,도 10 내지 도 12는 한 명이 인터랙션 하는 모습을 나타낸 캡처 화면이고,도 13 내지 도 16은 두 명이 동시에 인터랙션 하는 모습을 나타낸 캡처 화면이다.발명을 실시하기 위한 구체적인 내용아래에서는 첨부한 도면을 참조하여 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자가 용이하게 실시할 [0038]수 있도록 본 발명의 실시 예를 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며여기에서 설명되는 실시 예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙여 설명하기로 한다.이하, 본 발명에서 실시하고자 하는 구체적인 기술내용에 대해 첨부도면을 참조하여 상세하게 설명하기로 한다. [0039]공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템의 실시 예 [0040]도 2는 본 발명의 바람직한 실시 예에 의한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템을 개 [0041]략적으로 나타낸 구성도이고, 도 3은 적외선 LED 어레이 바(IR-LEDs Array Bar)의 제품 사진이다.본 발명의 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템은 도 2에 도시된 바와 같이, 후면 스크 [0042]린(Rear Screen) 또는 벽면형 디스플레이 장치(100), 적외선 LED 어레이 바(IR-LEDs Array Bar: 110), 3차원입체 프로젝터(projector: 120), 적외선 카메라(Infrared Camera: 130), 인식 컴퓨터(140), 전원부(150), 렌더링(rendering) 컴퓨터(160), 제어 컴퓨터(170)를 포함하여 구성한다.여기서, 상기 스크린(Screen: 100)은 상기 프로젝터(120)의 영상을 투영할 수 있도록 영상을 맺히도록 한다. 상 [0043]기 적외선 LED 어레이 바(110)는 복수 개의 적외선 LED(IR-LEDs: 111)를 일정 간격으로 배열한 적외선 발생장치로서, 상기 스크린(100) 전면의 천장(10)에 설치되며, 상기 스크린(100) 앞쪽에 적외선을 수직으로 발생하여 사공개특허 10-2011-0107692-9-용자가 콘텐츠와 인터랙션 할 수 있는 인터랙티브 막(Interactive Surface: 160)을 생성한다. 상기 3차원 입체프로젝터(120)는 2대의 프로젝터를 이용하여 상기 스크린(100)에 3차원 입체 영상을 투영하는 기능을 하며, 상기 적외선 카메라(130)는 일반 카메라(예를 들어, USB CCD 카메라 등)의 렌즈 앞에 적외선 투과 필터(IR-Filter: 131)를 장착한 카메라로서, 상기 인터랙티브 막(160)에 사용자의 손이 닿을 때 사용자의 손(손등)에서반사되는 적외선 빛을 촬영하여 상기 서버(140)로 촬영한 영상신호를 전송한다. 상기 인식 컴퓨터(140)는 상기적외선 카메라(130)로부터 획득한 정보로 영상처리를 통하여 상기 인체나 물체의 인터랙션 위치의 좌표값을 추출한다. 상기 전원부(150)는 상기 적외선 LED 어레이 바(110)에 전원을 공급해 주는 역할을 한다. 상기 렌더링컴퓨터(160)는 상기 인식 컴퓨터(140)로부터 추출된 인터랙션 위치의 좌표값을 입체 콘텐츠와의 인터랙션을 통해 렌더링 정보를 생성하여 상기 3차원 입체 프로젝터(150)를 통해 3D 입체 영상을 생성한다. 상기 제어 컴퓨터(170)는 상기 렌더링 컴퓨터(160)를 모니터링 하고 제어하는 역할을 한다.본 발명에서는 상기 적외선 LED 어레이 바(IR-LEDs Array Bar: 110)를 이용하여 상기 스크린(100) 전면에 인터 [0044]랙티브 막(160)을 생성함으로써 다양한 인터랙션(Interaction) 기법을 제공한다. 이하, 상기 인터랙티브 막(160)에 사람의 손을 통해 터치 되는 영역들에 대해서는 인터랙티브 영역(Interactive Zone)이라 지칭하고 그외에 터치가 되지 않는 영역을 앰비언트 영역(Ambient Zone)이라고 정의하기로 한다. 상기 적외선 LED 어레이 바(IR-LEDs Array Bar: 110)는 도 3의 사진과 같이, 복수 개의 적외선 LED(IR-LEDs)를 [0045]일정 간격(예를 들어, 3cm 간격)을 두고 설치한다. 이때, 상기 적외선 LED(IR-LEDs: 111)를 일정한 간격으로 배치하는 이유는 모든 영역에 대해서 적외선 빛의 양을 균일하게 하기 위해서이다. 그리고, 상기 적외선 LED(IR-LEDs)의 간격을 3cm로 하는 이유는 각 LED의 발열량을 고려하면서 충분한 적외선의 양을 공급받을 수 있는 거리를 시험한 결과, 3cm의 간격이 적당한 것으로 밝혀졌다. 하지만, 이값에 반드시 국한된 것은 아니며, LED 소자나 기타 다른 환경에 따라 변할 수도 있다.상기 적외선 LED(IR-LEDs: 111)는 일반적으로 45도 각도로 빛이 발산된다. 따라서, 본 발명의 적외선 LED 어레 [0046]이 바(IR-LEDs Array Bar: 110)에서는 LED의 빛이 45°로 발산하는 것을 막기 위해 상기 적외선 LED(IR-LEDs:111)의 양쪽을 벽으로 차단함으로써 수직으로만 빛이 나갈 수 있도록 구성하여 빛의 직진성을 향상시켰다.상기 적외선 LED 어레이 바(IR-LEDs Array Bar: 110)는 상기 스크린(100) 전면의 상단 부분(천장 또는 측벽 상 [0047]부)에 위치시킨다. 이는 사용자의 인터랙션을 포착하기 위해서 일정 간격을 두고(예를 들어, 3cm 정도) 전면부에 부착을 하며, 사용자의 인터랙션 영역을 구성하는 인터랙션 막(160)을 생성하게 된다. 이는 사용자가 손을뻗었을 때 상기 인터랙션 막(160)에 사용자의 손이 닿음으로써, 사용자가 인터랙션을 자유롭게 할 수 있는 인터랙션 영역에 진입했다는 의미이다. 도 2 및 도 3을 참조하면, 상기 3차원 입체 프로젝터(120)와 상기 적외선 카메라(130)는 사용자의 인터랙션을 [0048]하기 위한 최적의 장소를 찾아 위치시킨다. 이때, 상기 적외선 카메라(130)는 밴드 패스 필터(Band passfilter)의 사용 없이 화면 자체를 얻어오게 되면 빔 프로젝터에서 영사되는 모든 영상을 다 받아오기 때문에 밴드 패스 필터(Band pass filter; 예를 들어, 850nm 이하를 차단하는 필터)를 이용하여 상기 스크린(100)에서 나오는 영상을 차단하고 적외선 빛을 받을 수 있도록 하였다. 그 결과, 사용자가 콘텐츠와의 인터랙션을 위해 상기 스크린(100)에 손을 뻗게 되면, 상기 스크린(100) 위에 위 [0049]치한 상기 적외선 LED 어레이 바(IR-LEDs Array Bar: 110)에서 내려오는 적외선이 손(손등)을 통과하지 못하고막히게 된다. 이때 상기 적외선 카메라(130)는 손(손등)에서 반사되는 적외선 빛을 촬영하여 상기 서버(140)에촬영된 영상 신호를 전송하게 된다. 상기 인식 컴퓨터(140)에서는 상기 적외선 카메라(130)에서 전송된 신호를영상처리를 통해 손 영역의 좌표(x,y) 값을 계산하게 된다. 여기서, 손 영역의 좌표(x,y) 값을 계산하는 방법은 다음과 같다. [0050]영상 이진화를 통하여 LED가 반사되는 영역과 그렇지 않은 영역을 나눈 뒤에 반사되는 영역에 한해서 블랍/라벨 [0051]링을 거쳐 호모그래피 행렬을 이용하여 왜곡을 보정한 뒤 좌표의 최상위점을 인식하고 있는 좌표로 계산하여서작동한다. 여기서, 상기 블랍은 영상 이진화를 하였을 때, LED가 반사된 영역들을 묶는 역할을 한다. 즉, 사람의 손이 하나의 픽셀(점)로 빛을 반사시키는 게 아니라 다수의 픽셀(면)로 빛을 반사시키기 때문에, 연속으로LED가 반사되는 영역을 묶어주는 역할을 한다. 상기 라벨링은 상기 블랍을 통해 묶인 영역들에 대해 이름 그대로 번호를 지정하는 작업이다. 각 영역당 번호를 지정함으로써, 영역이 생겨서부터 사라질 때까지의 좌표값을계산할 수 있다. 이때, 영상은 3차원이 아닌 2차원이기 때문에 (x,y)좌표로만 계산하여 (x,y)의 좌표정보 값만계산된다.공개특허 10-2011-0107692-10-이와 같이, 상기 인식 컴퓨터(140)는 상기 적외선 카메라(130)로부터 획득한 정보를 영상 이진화를 통하여 LED [0052]빛이 반사된 영역과 그렇지 않은 영역으로 분리한 후 블랍 및 라벨링을 통해 상기 반사된 영역을 묶어서 각각번호를 지정한 다음, 호모그래피 행렬을 이용하여 신호 왜곡을 보정하고 윈도우 API을 통해 좌표를 보정 한 후,상기 인터랙션 막에 접촉된 인체나 물체의 좌표값을 추출한다. 그리고, 상기 렌더링 컴퓨터(160)는 상기 좌표값을 통해 입체 콘텐츠와의 인터랙션을 하게 된다. 이러한 입체 [0053]콘텐츠는 오픈 게임 엔진인 오우거 3D(Ogre 3D)를 통해 랜더링하게 된다. 이때, 엔진에서는 가상 공간상에서 스테레오 카메라를 생성하여 각각의 카메라 뷰를 렌더링하고, 상기 렌더링 된 정보를 상기 3차원 입체 프로젝터를통해 각각의 좌, 우 영상을 투영하여 3D 입체 영상을 생성한다. 또한, 인터랙션은 외부 스크립트 언어인Luatinker를 사용하여 인터랙션 이벤트 요소를 추가한다.3차원 입체 인터랙티브 비전 시스템의 구체적인 동작 원리 [0054]본 발명에서는 적외선 LED를 이용한 적외선 LED 어레이 바(110)와 적외선 카메라(130)를 통해 사용자의 행동에 [0055]대한 감지를 하게 된다. 이는 기존의 접촉식 센서와는 달리 인체의 유무와 통과 여부를 2차원 좌표로 출력하여사용자의 위치를 감지한다. 이렇게 감지된 좌표정보 값은 컨트롤러에 전송되고 컨트롤러의 CPU 프로그램에 의해x축과 y축의 입력포트의 위치를 연산 처리하여 컴퓨터로 전송하게 된다. 이러한 위치정보 값은 3차원 그래픽 영상과 연동하여 사용자가 실제로 3차원 입체 공간상에 있는 것처럼 느끼게 된다. 즉, 입체 영상 위에 사용자의행동에 따라 물체가 행동패턴에 맞춰 반응하여 영상 속의 개체들과 직접 인터랙션 할 수 있다. 이러한 내용들을구체적으로 살펴보면 다음과 같다.OGRE 엔진을 이용한 입체 영상 생성 [0056]OGRE 엔진은 Object-oriented Graphic Rendering Engine의 약자로 그래픽 렌더링 및 애니메이션을 처리해주는 [0057]엔진이다. 일반적으로 우리가 아는 Unrial이나 Gamebrio와 같은 게임엔진은 그래픽 렌더링 엔진에 인공지능 엔진과 물리엔진 등을 포함하는 풀(Full) 3D 게임엔진(Game Engine)이기 때문에 OGRE와 같은 그래픽 엔진과는 차이가 있다. 하지만 객체지향인터페이스 방식의 설계를 통한 깔끔하고 간결한 엔진 클래스 및 인터페이스와 확장성 때문에, 공개 엔진 중에서 가장 많이 이용되는 엔진이 OGRE 엔진이기도 한다.상기 OGRE는 도 4와 같이, 크게 장면 관리(Scene Management: 310), 리소스 관리(Resource Management: 320), [0058]렌더링(Rendering: 330)의 3개 그룹으로 나뉘어 있고, 기타 플러그인들을 붙여서 사용을 하게 된다.먼저, 상기 장면 관리(Scene Management: 310)는 화면상에 보이는 모든 개체들을 종합적으로 관리하는 역할을 [0059]하는 부분이다. 카메라, 광원, 평면과 같은 것들도 장면 관리자의 관리 대상인데, 이것들은 장면 위에 직접 배치가 불가능하기 때문에 간접적인 방법을 이용하여 관리하게 된다. 그 방법이 바로 장면노드(Scene Node)를 이용하는 것이다.다음, 상기 리소스 관리(Resource Management: 320)는 상기 렌더링(Rendering: 330)에 필요한 geometry, [0060]texture, font와 같은 모든 리소스 자원을 관리하는 역할을 한다. 주요 역할은 리소스 불러오기, 재사용, 해제등의 관리이다.끝으로, 상기 렌더링(Rendering: 330) 파트는 화면에 보여주는 역할을 한다. 렌더링을 위해 거치는 과정은 [0061]월드, 카메라, 투영행렬을 거쳐 3차원의 정점이 2차원의 점으로 변환되는데, 이것을 묘사한 것이 렌더링 파이프라인이다. 이 파이프라인의 하위 단계에서 모니터에 보여주기까지의 처리과정이 렌더링(Rendering) 단계에서 관리된다.따라서, 본 발명에서는 상기 장면 관리(Scene Management: 310)에서 양안의 카메라를 생성하여 3차원 입체 영상 [0062]을 생성하게 된다. 생성된 양안의 카메라 뷰(view)를 상기 렌더링(Rendering: 330) 파트에서 각각의 좌, 우 카메라 뷰(view)를 렌더링하게 된다.먼저, 3차원 영상의 획득이라 함은 사람의 양안 시차에 관한 원리를 말한다. 즉, 인간은 두 눈을 통해 영상의 [0063]입체감, 깊이 감을 느끼듯이 카메라 두 대를 사용하여 좌측 카메라로 획득한 영상은 좌측 눈에, 우측 카메라로획득한 영상은 우측 눈에 보여주는 것이다.양안 입체영상의 품질은 인간의 시각 기능과 최대한 가까운 조건으로 설정되었을 때 인간이 보는 듯한 현장감이 [0064]공개특허 10-2011-0107692-11-있는 양질의 3차원 입체 영상을 획득할 수 있다. 인간의 두 눈의 기능과 가까운 양안 입체 영상 획득을 위해서는 양안 입체 영상 카메라가 필요하다.일반적으로, 좌, 우 영상을 획득하는 양안 입체 카메라는 두 대의 카메라와 카메라의 거리 조절에 따라 크게 세 [0065]가지로 평행방식(Parallel Camera)과 교차방식(Toed-in Camera), 그리고 수평이동방식(Horizontal-movingCamera)으로 나눌 수 있다. 도 5는 3차원 입체 영상을 생성하기 위해 카메라를 사용하여 영상을 획득하는 방법을 나타낸 것이다.양안 입체영상 획득 방식 중 가장 단순한 형태로 도 5의 (a)에 나타낸 바와 같이, 피사체에 대해 평행하게 일정 [0066]간격으로 고정한 후, 좌·우 영상을 획득하는 방식이다. 도 5의 (b)는 양안 입체 영상 획득 방식 중 가장 많이사용되는 방식으로 피사체의 거리 이동에 따른 양안 시차의 조절을 위해 좌·우 카메라의 광축을 회전시키는 방식이다. 이때 이동거리를 선정함으로써 양안을 얻게 되는데 보통 이동거리는 동공간격(예를 들어, 65㎜)으로 하고 있다.본 발명에서는 다양한 입체 카메라 생성 방법 중에서 사람의 눈과 흡사한 방식인 교차식 방식(도 5의 (b))을 사 [0067]용하여 카메라를 생성한다. 이 방식은 카메라 광축 간격의 거리를 조절함으로써 사용자가 입체감이 가장 높은거리를 설정하는 방식으로 피사체의 이동에 따른 시차 변화의 보정이 가능하다는 장점을 가지고 있기 때문이다.일반적으로, 사람의 양안은 65㎜ 간격을 가지고 있다. 이로 인해 사람이 어떤 사물을 바라보게 되면 양안의 시 [0068]차에 의해 사물을 입체로 인지하게 되며, 깊이 판단에 중요한 실마리가 된다. 또한, 사물의 깊이 정보를 판단하는 기준인 영 교차점은 그 위치에 따라 입체감이 달라진다. 영 교차점은 그 위치에 따라 양의 시차, 영 시차,음의 시차가 있다. 양의 시차(도 6의 (a))는 영 교차점이 영상의 뒤에 위치해서 사물이 영상 뒤에 맺히며 영 시차(도 6의 (b))는 영상과 같은 위치에, 음의 시차(도 6의 (c))는 영상의 앞에 사물이 보이게 된다.이렇듯 3차원 입체 영상을 생성하기 위해서는 양안 시차와 영 교차점은 중요한 요소들이다. 따라서 양안 시차와 [0069]영 교차점을 적절히 조절함으로써 각각의 3차원 입체 콘텐츠 제작을 위한 각 장면들의 입체 영상을 생성한다.양안 시차와 영 교차점을 설정하는 것은 사람마다 느끼는 입체감이 다르기 때문에 설정하기가 어렵다. 왜냐하면양안 시차와 영 교차점을 조절하여 사용자로 하여금 몰입감이 높은 결과물로 인식되기까지는 반복적이고 많은시간이 필요하다. 그리고 기존의 방법들은 실시간으로 양안 시차와 영 교차점을 조절하면서 확인할 수 있는 방법을 제공하지 않고 있다. 이러한 문제를 해결하기 위해서 본 발명에서는 사용자가 직관적으로 각 장면들의 양안 시차와 영 교차점을 실시 [0070]간으로 조절하여 입체 영상의 입체 정도를 확인할 수 있도록 대화식 입체 영상 미리 보기 기능을 제안한다. 실제로, 사용자가 높은 몰입감을 느낄 수 있는 입체 영상 시점을 결정하기 위해 편광 필터 안경을 착용한 상태에서 입체 영상을 실시간으로 확인하며 카메라의 양안 시차와 영 교차점을 조절하고 입체 정도를 확인하면서 사용자가 느낄 수 있는 가장 적절한 입체감을 결정한 후 3차원 입체 영상을 생성한다.적외선 LED 어레이 바(IR-LEDs Array Bar)를 이용한 인터랙션 방법 [0071]적외선 LED 어레이 바(IR-LEDs Array Bar: 110)를 3차원 입체 영상 디스플레이(100)의 상단 전면부분에 설치한 [0072]다. 사용자의 인터랙션을 포착하기 위해 전면부 천정에 부착을 하며, 이때 사용자의 인터랙티브 영역을 구성하는 인터랙션 막이 생성된다. 사용자의 손이 인터랙션 막에 닿았을 때 사용자가 자유롭게 인터랙션 할 수 있는인터랙션 영역에 진입했다는 의미이다.이때, 인터랙션 막에서 발생되는 적외선 빛은 \"빛의 직진성\" 원리에 의해 직선으로 빛이 퍼져나간다. 사용자 손 [0073]을 트래킹하기 위해서 적외선 빛이 상단에서 하단으로 수직으로 발산한다. 그리고 앞에 물체가 가로막고 있으면물체 반대편에 그림자가 생기게 된다. 이러한 원리를 이용하여 대규모 디스플레이에 인터랙션하기 위해 적외선빛은 손등에서 남아 있게 되고 아래쪽에는 그림자가 생긴다. 이때 3차원 입체 영상 디스플레이를 바라보는 적외선 카메라는 적외선 빛을 수집하게 된다.즉, 사용자가 상기 스크린(100)에 인터랙션하기 위해 손으로 터치할 경우 상기 스크린(100) 전면에 형성된 상기 [0074]인터랙션 막(180)에 닿게 된다. 상기 인터랙션 막(180)은 앞에서 설명한 바와 같이, 상기 적외선 LED 어레이 바(110)에서 발생 된 적외선 빛에 의해 형성된 인터랙션 영역이다.도 7의 그림과 같이, 상기 인터랙션 막(180)에 손이 닿으면, 손 아래쪽에는 상기 적외선 LED(IR-LEDs)의 빛이 [0075]차단되어 그림자가 생기게 되고, 손 위쪽에는 상기 적외선 LED(IR-LEDs)의 빛이 손등에서 반사되게 된다. 이때,공개특허 10-2011-0107692-12-상기 손등에서 반사되는 적외선 빛은 상기 인터랙션 영역을 촬영하는 상기 적외선 카메라(130)에 촬영되어 수집된다. 상기 적외선 카메라(130)는 촬영된 영상 신호를 상기 인식 컴퓨터(140)에 전송하고, 상기 인식 컴퓨터(140)에서는 상기 적외선 카메라(130)에서 전송된 신호를 영상처리를 통해 사용자의 손 영역을 검출함으로써,손의 좌표(x,y) 값을 추출하게 된다. 이때, 사용자가 바라보는 뷰 영역의 스크린과 상기 적외선 카메라(130)가 바라보는 뷰 영역의 스크린의 영상의 [0076]모양은 차이가 있다. 사용자가 바라보는 뷰 영역은 편의를 위해 직사각형의 영역으로 맞추지만(도 8의 a). 상기적외선 카메라(130)로 바라보는 뷰 영역은 공간적 제약에 따른 사다리꼴의 영상왜곡이 생긴다(도 8의 b). 왜곡된 영상의 손가락(또는 물체) 좌표를 보정 없이 윈도우 시스템이 좌표로 적용하는 것은 원하는 위치의 좌표로반환할 수 없다. 이러한 문제를 해결하기 위한 방법으로 호모그래피 행렬(Homography Matrix)을 적용한다. 상기 호모그래피 행렬은 어느 한 평면에 대하여 그 평면 위에 있는 점과 다른 한 평면의 대응점은 3×3 행렬로 [0077]표현이 가능하다. 여기에서 3×3 행렬은 두 평면 사이의 사용 변환 관계를 나타내는데, 이것을 호모그래피(Homography) 라고 한다. 호모그래피 행렬을 사전식으로 배열한 9×1 벡터를 평면 위에 있는 점과 다른 한 평면의 대응점을 조합한 n×9 행렬에 곱하면 0이 된다. 이 과정에서 호모그래피 행렬을 사전식으로 배열한 9×1 벡터는 SVD(Singular value Decomposition)를 통해 구한다. 이와 같이, 호모그래피 행렬을 이용하여 영상의 왜곡을 보정한다. 보정된 좌표는 상기 적외선 카메라(130)의 뷰 [0078]영역에서의 좌표일 뿐 실제 윈도우 좌표에 적용되는 좌표는 아니다. 만약, 상기 적외선 카메라(130)의 전체 뷰영역이 640×480이고, 윈도우의 해상도가 1024×768이라면 위치의 오차는 많이 날 수밖에 없다. 이러한 위치를줄이기 위해 마우스의 범위를 API레벨에서 계산하여 제어 가능하도록 하였다. 마우스의 실제 좌표는 상하좌우의모든 영역은 해상도와 관계없이 0~65535의 영역을 가지기 때문에 현 해상도(width, height)를 나눠준 후 호모그래피의 계산 결과로 나온 좌표를 곱하면 해상도에 맞는 좌표가 계산된다(도 8의 c). 다음으로, 상기 적외선 카메라(130)가 바라보는 해상도의 크기는 640×480이지만, 실제 모니터(1024×768) 또는 [0079]프로젝터(120)에 보이는 화면의 크기가 달라 호모그래피 행렬을 사용하여 좌표보정을 하여도 올바르지 않다. 이를 위해, WindowsAPI 레벨에서 좌표 보정을 하게 되면 올바르게 트래킹이 된다.이때, 상기 윈도우(Windows) API 레벨에서 좌표 보정하는 방법과 트래킹 방법은 다음과 같다. OpenCV 라이브러 [0080]리에서 제공하는 cvWarpPerspective와 cvGetPerspectiveTransform을 통하여 상기 적외선 카메라(130)가 보는화면에서 영상의 각 모서리 4군데의 좌표를 입력하여 4개의 점으로 이루어진 사각형 내의 이미지를 새로운 4개의 점으로 이루어진 사각형으로 변환하여 왜곡을 보정 한다. 상기 트래킹은 사람의 손이나 물체의 위치를 파악하여 얻은 좌표값을 네트워크로 좌표값을 콘텐츠에 보내서 해당되는 지점에 인터랙션을 발생하게 해준다.이렇게 추적된 손이 좌표값을 통해 입체 콘텐츠와의 인터랙션을 하게 된다. 상기 3차원 입체 인터렉티브 비전 [0081]시스템은 서버-클라이언트를 기반으로 한다. 손의 좌표를 계산해서 추출하는 것은 상기 인식 컴퓨터(140)에서이루어 계산되고, 추출된 손의 좌표는 상기 스크린(100)의 좌측 상단(0, 0)을 기준으로 하는 윈도우 좌표계의형태로 변경하여 클라이언트에 초당 30∼50개 정도의 좌표 데이터를 전송하게 된다. 상기 렌더링 컴퓨터(160)에서는 월드좌표 내 카메라를 기준으로 전달받은 좌표를 이용해 광선추적(ray tracing) 기법을 이용해서 오브젝트들이 카메라를 기준으로 거리별로 정렬되게 한 후, 가장 가까이 있는 오브젝트가 인터랙션이 일어나도록 처리된다.상기 입체 콘텐츠는 오픈 게임 엔진인 오우거 3D(Ogre 3D)를 통해 렌더링하게 된다. 상기 오우거 3D(Ogre 3D)에 [0082]서는 입체 콘텐츠가 실행된 후, 초기화 과정에서 똑같은 해상도의 렌더링 윈도우(최종 이미지를 출력하는 창)를좌, 우 각각 1개씩 생성한다. 생성된 각각의 윈도우에 3D 카메라를 연결하여 각각의 카메라 시점에서 보이는 오브젝트들이 렌더링 된 결과물이 연결된 윈도우를 통해서 렌더링 된다.여기서, 본 발명에서는 좌, 우 카메라의 양안 차 조절을 다음과 같이 실시간 제어하였다. 즉, 양안 차를 조절하 [0083]기 위해 키보드의 ‘←’, ‘→’ 방향키를 이용하여 카메라 간의 간격을 조절할 수 있는 기능을 추가하였다.초기 셋-업 할 때의 카메라 위치는 같지만 ‘←’ 키를 이용하면 좌측에 위치한 렌더링 윈도우와 연결된 카메라는 좌측으로 이동하고, 우측에 위치한 렌더링 윈도우와 연결된 카메라는 우측으로 이동하면서 좌우 간격이 벌어지게 되고, ‘→’ 키를 이용하면 좌, 우 카메라의 간격을 좁힐 수 있다. 셋업된 카메라 정보는 별도의 파일에저장되어 이후 실행될 때마다 설정된 값을 로드해서 사용하게 된다.그 다음, 상기 오우거 3D(Ogre 3D)를 통해 가상 공간상에서 스테레오(좌우) 카메라를 생성하여 각각의 카메라 [0084]뷰(view)를 렌더링한다. 초기화 과정에서 좌, 우 각각의 렌더링 윈도우를 생성해서 두 개의 창이 활성화된다.공개특허 10-2011-0107692-13-각 창에 연결된 카메라는 각각 초당 30프레임 이상의 속도로 각 카메라 시점에서 보이는 월드(다양한 3차원 오브젝트들로 구성)를 렌더링하게 된다. 이때, 렌더링 절차는 오브젝트의 월드변환, 각 카메라 정보를 이용한 뷰(카메라) 변환, 각 렌더링 윈도우에 맞는 프로젝션 변환, 레스터라이즈, 화면 출력 순으로 진행된다.상기 렌더링 된 정보는 3D 입체 프로젝터를 통해 각각의 좌, 우 영상을 투영함으로써 3D 입체 영상을 생성하게 [0085]된다. 3D 입체 영상을 생성하기 위해 2개의 렌더링 윈도우에 다시 2개의 빔프로젝터를 연결한다. 렌더링 된 이미지는 빔프로젝터를 통해서 좌, 우 영상을 싱크에 맞게 제공해주고, 양안 카메라의 거리를 조절하게 해줌으로써 입체감이 살아날 수 있게 한다. 입체를 위해 사용된 빔프로젝터의 출력부위에 편광유리를 두고, 두 개의 상이 한곳에 모이게끔 합성하여, 이용자는 편광안경을 통해 입체감을 체감할 수 있게 된다.또한, 인터랙션은 외부 스크립트 언어인 Luatinker를 사용하여 인터랙션 이벤트 요소를 추가한다. 인터랙션 연 [0086]동을 위해 사용된 외부 스크립트는 루아 스크립트이고, 오브젝트별로 테이블을 지정해 놓고, 여기에 오브젝트의이름, 타입, 사용 메쉬, 초기좌표, 크기, 회전 값 등을 넣을 수 있게 해놓았다. 이렇게 형식에 맞게 채워진 값들은 엔진의 초기화 부분에서 이용되며, 오브젝트의 생성에 활용된다.이벤트에 대한 처리를 위해서는 오브젝트명.lua 파일에 따로 지정을 해놓았는데, lua파일에는 이동 및 돌출 같 [0087]은 이벤트를 함수로 지정해놓고, 인터렉션에 따라 엔진에서 루아에 있는 함수들을 호출하는 형태로 되어있다.이렇게 이벤트와 이동에 대한 정의를 루아스크립트로 빼놓으면서, 프로그램이 실행되는 중에도 실시간으로 수정이 가능하며 스크립트 다시 읽기 기능을 넣어, 바로 갱신된 화면을 확인할 수 있다.공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템의 구현 예 [0088]도 9 내지 도 16은 본 발명의 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템을 구현한 예의 캡처 [0089]화면으로서, 도 9은 후면 스크린의 정면 모습의 캡처 화면이고, 도 10 내지 도 12는 한 명이 인터랙션 하는 모습을 나타낸 캡처 화면이고, 도 13 내지 도 16은 두 명이 동시에 인터랙션 하는 모습을 나타낸 캡처 화면이다.이와 같이 구성된 본 발명에 의한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법은 [0090]스크린 전면의 천장 또는 바닥에 적외선 LED 어레이 바(IR-LEDs Array Bar)를 설치하여 사용자가 인터랙션할 수있는 인터랙티브 막(interactive surface)을 생성하여 사용자가 손동작으로 콘텐츠와 인터랙션(interaction)을자유롭게 할 수 있고, 사용자의 손동작에 따라 입체 영상의 물체가 반응하여 움직임으로써 영상 속의 개체들과직접 인터랙션을 할 수 있다. 또한, 한 명 또는 다수의 인원이 동시에 참여할 수 있도록 프로젝션 방식을 통해대규모 인터랙티브 인터페이스를 제공하여 사용자에게 재미와 몰입감 있는 인터랙션 콘텐츠를 제공함으로써, 본발명의 기술적 과제를 해결할 수가 있다.이상에서 설명한 본 발명의 바람직한 실시 예들은 기술적 과제를 해결하기 위해 개시된 것으로, 본 발명이 속하 [0091]는 기술분야에서 통상의 지식을 가진 자(당업자)라면 본 발명의 사상과 범위 안에서 다양한 수정, 변경, 부가등이 가능할 것이며, 이러한 수정 변경 등은 이하의 특허청구범위에 속하는 것으로 보아야 할 것이다.산업상 이용가능성본 발명에 의한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법은 개인 도는 대규모의 [0092]군중이 다양하게 콘텐츠를 즐길 수 있는 인터랙티브 앰비언트 디스플레이 환경으로 제공할 수 있으며, 멀티 카메라, 멀티 컴퓨터 등의 연동을 통한 패러럴 컴퓨팅 방식으로 확장을 할 수 있다.부호의 설명10 : 천장 20 : 바닥 [0093]100 : 후면 스크린(Rear Screen) 또는 벽면형 디스플레이 장치110 : 적외선 LED 어레이 바(IR-LEDs Array Bar)공개특허 10-2011-0107692-14-111 : 적외선 LED(IR-LEDs)120 : 3차원 입체 프로젝터(projector)130 : 적외선 카메라(Infrared Camera)140 : 인식 컴퓨터 150 : 전원부160 : 렌더링(rendering) 컴퓨터 170 : 제어 컴퓨터180 : 인터랙티브 영역(interactive Zone) 또는 인터랙티브 막(interactive surface)도면도면1공개특허 10-2011-0107692-15-도면2도면3공개특허 10-2011-0107692-16-도면4공개특허 10-2011-0107692-17-도면5도면6도면7공개특허 10-2011-0107692-18-도면8도면9도면10도면11공개특허 10-2011-0107692-19-도면12도면13도면14공개특허 10-2011-0107692-20-도면15도면16공개특허 10-2011-0107692-21-"}
{"patent_id": "10-2010-0026970", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법에 관한 것으로, 스크린의 전 면에 인터랙티브 막(interactive surface)을 제공하여 사용자가 손동작으로 콘텐츠와 인터랙션(interaction)을 자유롭게 할 수 있으며, 사용자의 손동작에 따라 입체 영상의 물체가 반응하여 움직임으로써 영상 속의 개체들과 직접 인터랙션을 할 수 있다. 또한, 한 명 또는 다수의 인원이 동시에 참여할 수 있도록 프로젝션 방식을 통해 대규모 인터랙티브 인터페이스를 제공함으로써 사용자에게 재미와 몰입감 있는 인터랙션 콘텐츠를 제공한다. 본 발명에 따른 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템은, 영상을 디스플레이하는 스크린 과; 상기 스크린의 전방 상단에 설치되며 상기 스크린의 전방에 적외선을 수직으로 발생하여 인터랙티브 막을 생 성하는 적외선 LED 어레이 바와; 상기 스크린의 후방에 설치되며 상기 스크린에 영상을 투영하는 2개의 3차원 입 체 프로젝터와; 상기 스크린 전방에서 상기 인터랙티브 막에 인체나 물체가 접촉되었을 때 반사되는 적외선 빛을 촬영하는 적외선 카메라와; 상기 적외선 카메라로부터 획득한 정보로 영상처리를 통하여 상기 인체나 물체의 인 터랙션 위치의 좌표값을 추출하는 인식 컴퓨터; 및 상기 인식 컴퓨터로부터 추출된 인터랙션 위치의 좌표값을 입 체 콘텐츠와의 인터랙션을 통해 렌더링 정보를 생성하여 상기 3차원 입체 프로젝터를 통해 3D 입체 영상을 생성 하는 렌더링 컴퓨터;를 포함하여 구성한다."}
{"patent_id": "10-2010-0026970", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템(3D Stereo interactive Vision System) 및 그 방법에 관한 것으로, 보다 상세하게는 사용자가 스크린에서 멀리 떨어져 있으면 디스플레이 기능 만 하고, 사용자가 스크린의 인터랙티브 영역(interactive Zone)에 들어오면 사용자에게 콘텐츠와 인터랙션 (interaction)을 할 수 있는 기능을 제공하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법에 관한 것이다."}
{"patent_id": "10-2010-0026970", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 디지털 영상 미디어의 발전으로 인해 3차원 입체 콘텐츠에 대한 관심이 높아지고 있다. 이로 인해 3차원 입체 콘텐츠 생성에 대한 많은 기술이 연구 개발되고 있다. 3차원 입체 콘텐츠 제작 기술은 기존의 2차원 평면 영상과는 달리 사람이 보고 느끼는 실제 영상과 유사하여 시 각정보의 질적 수준을 한층 높여주는 새로운 개념의 실감형 영상미디어로서 차세대 디지털 영상 문화를 주도하 고 있다. 이러한 3차원 입체 콘텐츠 제작 기술은 차세대 영상 가시화 분야 개발의 핵심기술로 자리매김을 하고 있다. 일반적인 3차원 입체 콘텐츠 제작은 미리 알려진 3차원 모델 정보 또는 2차원 실사 영상에 대해 양안에 해당하 는 일정한 시차 정보를 가진 두 대의 카메라 영상을 획득하여 양안 영상을 정합함으로써 3차원 입체 영상을 생 성 표현한다. 하지만, 입체 콘텐츠 생성 표현에 대한 많은 시도들이 되고 있지만 정작 콘텐츠를 활용한 새로운 시도들은 부족한 실정이다. 즉, 콘텐츠를 활용한 사용자들의 참여를 높일 수 있는 3차원 입체 영상 콘텐츠가 종 래에는 시도되지 못하였다. 종래의 3차원 입체 영상 콘텐츠와 관련된 선행기술들은 다음과 같다. 국내 공개특허 제2009-0131499호(이하, \"선행기술 1\"이라 한다)는 3차원 입체 영상 장치를 이용한 3차원 콘텐츠 개발 시스템에 관한 것으로, 사물을 입체적으로 촬영하는 3D 실사 카메라와; 피사체의 상을 입체 상으로 표현하 여 출력하는 3D 현미경과; 3D 콘텐츠를 출력하기 위한 3D 모니터 및 3D 프로젝터와; 상기 3D 실사 카메라에서 촬영된 3D 실사 카메라 영상에 적합한 3D 라이브러리와, 상기 3D 현미경에서 촬영된 3D 현미경 영상에 적합한 3D 라이브러리 및, 상기 3D 실사 카메라 및 3D 현미경으로부터의 3D 영상을 3D 모델링하기 위한 3D 모델링 데이 터를 저장하는 3D 데이터 저장부와; 3D 콘텐츠를 표시하기 위하여 처리하는 그래픽 엔진과; 상기 3D 실사 카메 라 또는 상기 3D 현미경으로부터의 3D 영상을 상기 3D 데이터 저장부에 저장된 3D 라이브러리 및 3D 모델링 데 이터를 이용하여 상기 3D 모니터 및/또는 3D 프로젝터로 출력하기에 적합한 형태로 영상 처리를 하여 3D 콘텐츠 를 생성하는 3D 영상 처리모듈을 구비하고 있다. 상기 선행기술 1은 3차원 입체 영상장치를 활용하여 고도의 현실감을 느낄 수 있는 3D 콘텐츠를 비교적 짧은 시 간에 그리고 비교적 저비용으로 개발할 수 있는 3차원 입체 영상장치를 이용한 3차원 콘텐츠 개발 시스템에 대 한 것으로, 사용자가 직접 3D 입체 영상에 참여하여 즐길 수 있는 방법에 대해서는 개시되어 있지 않다. 국내 등록특허 제0913173호(이하, \"선행기술 2\"라 한다)는 3Ｄ 그래픽 처리장치 및 이를 이용한 입체영상 표시 장치에 관한 것으로, 3D 그래픽 데이터 및 동기신호를 입력받아, 제어신호를 생성하여 상기 3D 그래픽 데이터와 함께 출력하는 제어부; 3D 활성화 신호의 입력에 기초하여, 복수 시점의 3D 입체영상 데이터를 생성하기 위한복수의 입체 매트릭스를 생성하고, 상기 복수의 입체 매트릭스를 이용하여 상기 제어부로부터 입력된 상기 3D 그래픽 데이터를 상기 3D 입체영상 데이터로 변환하는 3D 그래픽 처리부; 상기 3D 그래픽 처리부로부터 출력된 영상 데이터 및 상기 제어부로부터 출력된 제어신호에 기초하여 구동신호를 생성하는 구동부; 및 상기 구동신호 에 기초하여 상기 영상 데이터에 대응하는 영상을 표시하는 표시부를 포함하여 구성하고 있다. 상기 선행기술 2는 2D 영상 컨텐츠인 3D 그래픽을 이용하여 3D 입체영상을 실시간으로 표시할 수 있는 입체영상 표시장치에 대한 것으로, 사용자가 직접 3D 입체 영상에 참여하여 즐길 수 있는 방법에 대해서는 개시되어 있지 않다. 국내 등록특허 제0573983호(이하, \"선행기술 3\"이라 한다)는 3차원 가상현실 콘텐츠 구현 시스템 및 구현 방법 에 관한 것으로, 통상의 3차원 데이터 저작도구로 구현된 원시 데이터를 VRML 파일로 변환하고, 카메라로부터 획득한 영상 데이터를 회전시켜 VR 데이터 파일로 변환하는 데이터 변환부; 상기 데이터 변환부로부터 변환된 상기 VRML 파일로부터 오브젝트 콘텐츠와 공간 콘텐츠를 생성하는 VRML 생성부와, 상기 VR 데이터로부터 파노라 마 VR 콘텐츠와 포토 오브젝트 VR 콘텐츠를 생성하는 실사 콘텐츠 생성부를 갖는 가상현실 콘텐츠 생성부; 상기 가상현실 콘텐츠 생성부로부터 생성된 상기 오브젝트 콘텐츠, 공간 콘텐츠, 파노라마 VR 콘텐츠 및 포토 오브젝 트 VR 콘텐츠를 파일로 저장하여 로컬 또는 웹상에서 제공되도록 관리하는 가상현실 콘텐츠 관리부; 및 상기 가 상현실 콘텐츠 관리부로부터 제공되는 상기 오브젝트 콘텐츠, 공간 콘텐츠, 파노라마 VR 콘텐츠 및 포토 오브젝 트 VR 콘텐츠가 디스플레이 되도록 뷰어프로그램을 갖는 사용자 단말기를 포함하여 구성하고 있다. 상기 선행기술 3은 다양한 가상현실 구현기술을 통합적으로 구현 및 서비스할 수 있는 솔루션을 제공함으로써 3 차원 가상현실 콘텐츠 서비스의 제공자나 3차원 가상현실 콘텐츠를 사용하는 이용자가 용이하게 3차원 가상현실 콘텐츠를 사용할 수 있는 3차원 가상현실 콘텐츠 구현 시스템에 대한 것으로, 사용자가 직접 3D 입체 영상에 참 여하여 즐길 수 있는 방법에 대해서는 개시되어 있지 않다. 국내 공개특허 제2009-0122875호(이하, \"선행기술 4\"라 한다)는 실시간 가상 현실 스포츠 플랫폼 장치에 관한 것으로, 도 1에 도시된 바와 같이 3차원 운동 경로 정보를 포함하는 3차원 게임 또는 가상 현실 프로그램이 설 치되어 구동되는 주 제어 수단; 상기 주 제어 수단과 연결되며, 상기 주제어수단에서 전달된 3 차원 운동 경로 정보와 가상 공간에서의 사용자와 운동 장치의 3차원 위치 및 방향 속도, 가속도, 이동 거 리, 일률, 소모 에너지, 등의 물리량 정보에 따라 가상현실상에서 사용자가 실제로 느낄 수 있는 부하량과 같도 록 사용자가 구동하는 피드백 구동수단에 가해지는 부하량을 제어하여 사용자가 느끼는 부하량을 조절하는 피드백 제어 수단 및 상기 동력 피드백 구동 수단에 사용자가 가하는 운동 부하를 측정하는 구동 센싱 수 단으로 구성되는 하나 이상의 동력 피드백 수단; 상기 주 제어 수단과 연결되며, 상기 주제어수 단에서 전달된 3차원 운동 경로 정보와 물리량 정보에 따라 가상현실상에서 사용자가 실제로 느낄 수 있는 자세와 진동으로 운동 체감 재현 장치의 자세와 진동을 제어하는 모션 제어 수단으로 구성되는 모션 재현 수단; 상기 주 제어 수단과 연결되며, 상기 주제어 수단에 사용자의 조작 입력을 받아 전달하는 조작 입력 수단; 상기 주 제어 수단과 연결되며, 상기 주제어 수단에서 전달된 3차원 운동 경로 정보를 이용하여 3차원 영상 정보를 발생시키는 3차원 그래픽 엔진과, 상기 3차원 그래픽 엔진의 영상 정 보를 표시하는 하나 이상의 이차원 또는 3차원 디스플레이로 구성되는 영상재현 수단; 상기 주 제어 수단과 연결되며, 상기 주제어 수단에서 전달된 음향 정보를 표현하는 음향재현 수단;을 포함하 여 구성하고 있다. 상기 선행기술 4는 실시간 가상 현실 스포츠 플랫폼 장치를 제공하여, 눈과 귀의 3D 그래픽을 중심으로 한 실시 간 가상현실 기술에 인간의 자세와 가속 운동의 체감 효과를 느끼게 하는 4D 가상현실을 제공하여 가상의 세계 에 자신을 빠트리는 완벽한 그래픽 및 체감 효과를 줄 수 있고, 손발의 운동이 가상현실 속의 다양한 콘텐츠의 물체를 움직이는 스포츠 게임과 같은 사이버 공간을 만들 수 있어 운동의 몰입도 및 효과를 높일 수 있도록 하 고 있다. 하지만, 상기 선행기술 4는 사용자가 운동을 3D 게임과 같이 즐기면서 할 수 있도록 구현한 것으로, 사용자의 자세와 핸들의 방향 및 발의 움직임 속도에 따라 전방에 설치된 3D 화면이 바뀌는 단순한 형태로 되어 있다. 또한, 상기 선행기술 4는 도 1과 같은 스포츠 플랫폼 장치를 구비하고 있기 때문에 구성이 복잡할 뿐만 아니라 제작 비용이 많이 소요되는 단점이 있다."}
{"patent_id": "10-2010-0026970", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2010-0026970", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "전술한 문제점을 해결하기 위하여 본 발명이 이루고자 하는 기술적 과제는, 기존에 동적인 입체영상 감상에서 벗어나 사용자가 직접 입체 영상에 참여하여 즐길 수 있는 새로운 개념의 공간 멀티 인터랙션 센서 바를 기반으 로 한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법을 제시하는 데 있다. 또한, 본 발명이 이루고자 하는 다른 기술적 과제는 스크린의 전면에 인터랙티브 막(interactive surface)을 제 공하여 사용자가 손동작으로 콘텐츠와 인터랙션(interaction)을 자유롭게 할 수 있도록 한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법을 제시하는 데 있다. 또한, 본 발명이 이루고자 하는 또 다른 기술적 과제는 사용자의 손동작에 따라 입체 영상의 물체가 반응하여 움직임으로써, 영상 속의 개체들과 직접 인터랙션을 할 수 있는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티 브 비전 시스템 및 그 방법을 제시하는 데 있다. 또한, 본 발명이 이루고자 하는 또 다른 기술적 과제는 한 명 또는 다수의 인원이 동시에 참여할 수 있도록 프 로젝션 방식을 통해 대규모 인터랙티브 인터페이스를 제공함으로써, 사용자에게 재미와 몰입감 있는 인터랙션 콘텐츠를 제공하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법을 제시하는 데 있 다. 또한, 본 발명이 이루고자 하는 또 다른 기술적 과제는 사용자가 스크린에서 멀리 떨어져 있으면 디스플레이 기 능만 하고, 사용자가 스크린의 인터랙티브 영역에 들어오면 사용자에게 콘텐츠와 인터랙션을 할 수 있는 기능을 제공하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법을 제시하는 데 있다. 또한, 본 발명이 이루고자 하는 또 다른 기술적 과제는 스크린 전면의 천장 또는 바닥에 적외선 LED 어레이 바 (IR-LEDs Array Bar)를 설치하여 사용자가 인터랙션할 수 있는 인터랙티브 막(interactive surface)을 생성한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법을 제시하는 데 있다. 또한, 본 발명이 이루고자 하는 또 다른 기술적 과제는 유비쿼터스 엠비언트 환경에서 사용자에게 다양한 멀티 터치를 제공함으로써, 인식을 위한 디바이스 장치의 도움없이 사용자의 단순한 손동작만으로 인터랙션 할 수 있 는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법을 제시하는 데 있다. 본 발명의 해결과제는 이상에서 언급된 것들에 한정되지 않으며, 언급되지 아니한 다른 해결과제들은 아래의 기 재로부터 당업자에게 명확하게 이해되어 질 수 있을 것이다."}
{"patent_id": "10-2010-0026970", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "전술한 기술적 과제를 해결하기 위한 수단으로서, 청구항 1에 기재된 발명은, 「공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템에 있어서, 영상을 디스플레이하는 스크린과; 상기 스크린의 전방 상단에 설치되며 상기 스크린의 전방에 적외선을 수직으로 발생하여 인터랙티브 막을 생성하는 적외선 LED 어레이 바와; 상기 스 크린의 후방에 설치되며 상기 스크린에 영상을 투영하는 2개의 3차원 입체 프로젝터와; 상기 스크린 전방에서 상기 인터랙티브 막에 인체나 물체가 접촉되었을 때 반사되는 적외선 빛을 촬영하는 적외선 카메라와; 상기 적 외선 카메라로부터 획득한 정보로 영상처리를 통하여 상기 인체나 물체의 인터랙션 위치의 좌표값을 추출하는 인식 컴퓨터; 및 상기 인식 컴퓨터로부터 추출된 인터랙션 위치의 좌표값을 입체 콘텐츠와의 인터랙션을 통해 렌더링 정보를 생성하여 상기 3차원 입체 프로젝터를 통해 3D 입체 영상을 생성하는 렌더링 컴퓨터;를 포함하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템.」을 제공한다. 청구항 2에 기재된 발명은, 「제 1 항에 있어서, 상기 인식 컴퓨터는: 상기 적외선 카메라로부터 획득한 정보를 영상 이진화를 통하여 LED 빛이 반사된 영역과 그렇지 않은 영역으로 분리한 후 블랍 및 라벨링을 통해 상기 반 사된 영역을 묶어서 각각 번호를 지정한 다음, 호모그래피 행렬을 이용하여 신호 왜곡을 보정하고 윈도우 API을 통해 좌표를 보정 한 후, 상기 인터랙션 막에 접촉된 인체나 물체의 좌표값을 추출하는 것을 특징으로 하는 공 간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템.」을 제공한다. 청구항 3에 기재된 발명은, 「제 1 항에 있어서, 상기 렌더링 컴퓨터는: 오우거 3D(Ogre 3D)를 이용하여 가상 공간상에서 스테레오 카메라를 생성하여 각각의 카메라 뷰를 렌더링하고, 상기 렌더링 된 정보를 상기 3차원 입체 프로젝터를 통해 각각의 좌, 우 영상을 투영하여 3D 입체 영상을 생성하는 것을 특징으로 하는 공간 멀티 인 터랙션 기반 3차원 입체 인터랙티브 비전 시스템.」을 제공한다. 청구항 4에 기재된 발명은, 「제 1 항에 있어서, 상기 적외선 LED 어레이 바는: 복수 개의 적외선 LED(IR- LEDs)를 일정 간격으로 배열하여 구성되고, 상기 적외선 LED 양쪽에 격벽을 형성한 것을 특징으로 하는 공간 멀 티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템.」을 제공한다. 청구항 5에 기재된 발명은, 「제 1 항에 있어서, 상기 적외선 카메라는: CCD 카메라, USB CCD 카메라, 웹 카메 라를 포함한 카메라 중 하나이며, 상기 카메라의 렌즈 앞에 적외선 투과 필터(IR-Filter)를 장착한 것을 특징으 로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템.」을 제공한다. 또한, 전술한 기술적 과제를 해결하기 위한 수단으로서, 청구항 6에 기재된 발명은, 「공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법에 있어서, (a) 스크린 앞쪽에 적외선을 발생하여 인터랙티브 막을 생성하는 단계와; (b) 상기 인터랙티브 막에 인체나 물체가 접촉되었을 때 반사되는 적외선 빛을 적외선 카메라로 촬영하 는 단계와; (c) 상기 적외선 카메라로부터 획득한 정보로 영상처리를 통하여 상기 인체나 물체의 인터랙션 위치 의 좌표값을 추출하는 단계와; (d) 상기 추출된 좌표값을 입체 콘텐츠와의 인터랙션을 통해 렌더링 정보를 생성 하는 단계; 및 (e) 상기 생성된 렌더링 정보를 2대의 3차원 입체 프로젝터를 통해 각각의 좌우 영상을 투영하여 3D 입체 영상을 생성하는 단계;를 포함하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법.」을 제 공한다. 청구항 7에 기재된 발명은, 「제 6 항에 있어서, 상기 (c)단계에서 인터랙션 위치정보를 추출하는 방법은: (c1) 상기 적외선 카메라로부터 획득한 정보를 영상 이진화를 통하여 LED 빛이 반사된 영역과 그렇지 않은 영역으로 분리하는 이진화 단계와; (c2) 상기 반사된 영역을 묶어주는 블랍 단계와; (c3) 상기 블랍을 통해 묶인 영역들 에 대해 번호를 지정하는 라벨링 단계와; (c4) 상기 라벨링 이후 호모그래피 행렬을 이용하여 신호 왜곡을 보정 하는 호모그래피 단계와; (c5) 상기 적외선 카메라가 보는 화면에서 영상의 각 모서리 4군데의 좌표를 입력하여 4개의 점으로 이루어진 사각형 내의 이미지를 새로운 4개의 점으로 이루어진 사각형으로 변환하여 왜곡을 보정 하는 윈도우 API 좌표보정 단계; 및 (c6) 상기 인체나 물체의 인터랙션 위치를 파악하여 좌표값을 추출하는 단 계;를 포함하는 것을 특징으로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법.」을 제공한다. 청구항 8에 기재된 발명은, 「제 6 항 또는 제 7 항에 있어서, 상기 좌표값은: 상기 스크린의 좌측 상단(0,0)을 기준으로 하는 윈도우 좌표계의 형태로 변경하여 좌표 데이터를 추출하는 것을 특징으로 하는 공간 멀티 인터랙 션 기반 3차원 입체 인터랙티브 비전 방법.」을 제공한다. 청구항 9에 기재된 발명은, 「제 6 항에 있어서, 상기 (d)단계에서 좌표값을 입체 콘텐츠와 인터랙션 하는 방법 은: 상기 좌표값을 이용하여 광선추적(ray tracing) 기법으로 오브젝트들이 카메라를 기준으로 거리별로 정렬되 게 한 후, 가장 가까이 있는 오브젝트가 인터렉션이 일어나도록 처리하는 것을 특징으로 하는 공간 멀티 인터랙 션 기반 3차원 입체 인터랙티브 비전 방법.」을 제공한다. 청구항 10에 기재된 발명은, 「제 6 항에 있어서, 상기 (e)단계에서 3D 입체 영상을 생성하는 방법은: 오우거 3D(Ogre 3D)를 이용하여 가상 공간상에서 스테레오 카메라를 생성하여 각각의 카메라 뷰를 렌더링하고, 상기 렌 더링 된 정보를 상기 3차원 입체 프로젝터를 통해 각각의 좌, 우 영상을 투영하여 3D 입체 영상을 생성하는 것 을 특징으로 하는 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 방법.」을 제공한다."}
{"patent_id": "10-2010-0026970", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 사용자가 스크린에서 멀리 떨어져 있으면 디스플레이 기능만 하고 사용자가 스크린의 인터랙 티브 영역에 들어오면 사용자에게 콘텐츠와 인터랙션을 할 수 있는 기능을 제공하며, 한 명 또는 다수의 인원이 동시에 참여할 수 있도록 프로젝션 방식을 통해 대규모 인터랙티브 인터페이스를 제공하여 사용자에게 재미와 몰입감 있는 인터랙션 콘텐츠를 제공한다. 또한, 다양한 인터랙션 정보들을 전처리 과정을 통해 인식하게 되며 스크린의 크기가 커지면 다수의 사용자가 함께 사용이 가능하며, 작은 공간에서 대규모 공간까지 아무런 제약 없이 운용 가능한 확장성을 가지고 있다. 또한, 적외선 인터랙션 막을 통해 사용자가 제한된 스크린 면이 아닌 공간에서 자연스럽게 손동작으로 인터랙션 을 함으로써 사용자가 직접 다양한 콘텐츠를 직접 몸으로 느끼고 체험할 수 있다.또한, 사용자의 손동작에 따라 입체 영상의 물체가 반응하여 움직임으로써, 사용자가 영상 속의 개체들과 직접 인터랙션을 할 수 있다."}
{"patent_id": "10-2010-0026970", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 이상에서 언급된 것들에 한정되지 않으며, 언급되지 아니한 다른 효과들은 아래의 기재로부터 당업자에게 명확하게 이해되어 질 수 있을 것이다."}
{"patent_id": "10-2010-0026970", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 발명의 실시 예를 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명되는 실시 예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙여 설명하 기로 한다. 이하, 본 발명에서 실시하고자 하는 구체적인 기술내용에 대해 첨부도면을 참조하여 상세하게 설명하기로 한다. 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템의 실시 예 도 2는 본 발명의 바람직한 실시 예에 의한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템을 개 략적으로 나타낸 구성도이고, 도 3은 적외선 LED 어레이 바(IR-LEDs Array Bar)의 제품 사진이다. 본 발명의 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템은 도 2에 도시된 바와 같이, 후면 스크 린(Rear Screen) 또는 벽면형 디스플레이 장치, 적외선 LED 어레이 바(IR-LEDs Array Bar: 110), 3차원 입체 프로젝터(projector: 120), 적외선 카메라(Infrared Camera: 130), 인식 컴퓨터, 전원부, 렌더 링(rendering) 컴퓨터, 제어 컴퓨터를 포함하여 구성한다. 여기서, 상기 스크린(Screen: 100)은 상기 프로젝터의 영상을 투영할 수 있도록 영상을 맺히도록 한다. 상 기 적외선 LED 어레이 바는 복수 개의 적외선 LED(IR-LEDs: 111)를 일정 간격으로 배열한 적외선 발생장치 로서, 상기 스크린 전면의 천장에 설치되며, 상기 스크린 앞쪽에 적외선을 수직으로 발생하여 사용자가 콘텐츠와 인터랙션 할 수 있는 인터랙티브 막(Interactive Surface: 160)을 생성한다. 상기 3차원 입체 프로젝터는 2대의 프로젝터를 이용하여 상기 스크린에 3차원 입체 영상을 투영하는 기능을 하며, 상 기 적외선 카메라는 일반 카메라(예를 들어, USB CCD 카메라 등)의 렌즈 앞에 적외선 투과 필터(IR- Filter: 131)를 장착한 카메라로서, 상기 인터랙티브 막에 사용자의 손이 닿을 때 사용자의 손(손등)에서 반사되는 적외선 빛을 촬영하여 상기 서버로 촬영한 영상신호를 전송한다. 상기 인식 컴퓨터는 상기 적외선 카메라로부터 획득한 정보로 영상처리를 통하여 상기 인체나 물체의 인터랙션 위치의 좌표값을 추 출한다. 상기 전원부는 상기 적외선 LED 어레이 바에 전원을 공급해 주는 역할을 한다. 상기 렌더링 컴퓨터는 상기 인식 컴퓨터로부터 추출된 인터랙션 위치의 좌표값을 입체 콘텐츠와의 인터랙션을 통 해 렌더링 정보를 생성하여 상기 3차원 입체 프로젝터를 통해 3D 입체 영상을 생성한다. 상기 제어 컴퓨터 는 상기 렌더링 컴퓨터를 모니터링 하고 제어하는 역할을 한다. 본 발명에서는 상기 적외선 LED 어레이 바(IR-LEDs Array Bar: 110)를 이용하여 상기 스크린 전면에 인터 랙티브 막을 생성함으로써 다양한 인터랙션(Interaction) 기법을 제공한다. 이하, 상기 인터랙티브 막 에 사람의 손을 통해 터치 되는 영역들에 대해서는 인터랙티브 영역(Interactive Zone)이라 지칭하고 그 외에 터치가 되지 않는 영역을 앰비언트 영역(Ambient Zone)이라고 정의하기로 한다. 상기 적외선 LED 어레이 바(IR-LEDs Array Bar: 110)는 도 3의 사진과 같이, 복수 개의 적외선 LED(IR-LEDs)를 일정 간격(예를 들어, 3cm 간격)을 두고 설치한다. 이때, 상기 적외선 LED(IR-LEDs: 111)를 일정한 간격으로 배 치하는 이유는 모든 영역에 대해서 적외선 빛의 양을 균일하게 하기 위해서이다. 그리고, 상기 적외선 LED(IR- LEDs)의 간격을 3cm로 하는 이유는 각 LED의 발열량을 고려하면서 충분한 적외선의 양을 공급받을 수 있는 거리 를 시험한 결과, 3cm의 간격이 적당한 것으로 밝혀졌다. 하지만, 이값에 반드시 국한된 것은 아니며, LED 소자 나 기타 다른 환경에 따라 변할 수도 있다. 상기 적외선 LED(IR-LEDs: 111)는 일반적으로 45도 각도로 빛이 발산된다. 따라서, 본 발명의 적외선 LED 어레 이 바(IR-LEDs Array Bar: 110)에서는 LED의 빛이 45°로 발산하는 것을 막기 위해 상기 적외선 LED(IR-LEDs: 111)의 양쪽을 벽으로 차단함으로써 수직으로만 빛이 나갈 수 있도록 구성하여 빛의 직진성을 향상시켰다. 상기 적외선 LED 어레이 바(IR-LEDs Array Bar: 110)는 상기 스크린 전면의 상단 부분(천장 또는 측벽 상 부)에 위치시킨다. 이는 사용자의 인터랙션을 포착하기 위해서 일정 간격을 두고(예를 들어, 3cm 정도) 전면부 에 부착을 하며, 사용자의 인터랙션 영역을 구성하는 인터랙션 막을 생성하게 된다. 이는 사용자가 손을 뻗었을 때 상기 인터랙션 막에 사용자의 손이 닿음으로써, 사용자가 인터랙션을 자유롭게 할 수 있는 인터 랙션 영역에 진입했다는 의미이다. 도 2 및 도 3을 참조하면, 상기 3차원 입체 프로젝터와 상기 적외선 카메라는 사용자의 인터랙션을 하기 위한 최적의 장소를 찾아 위치시킨다. 이때, 상기 적외선 카메라는 밴드 패스 필터(Band pass filter)의 사용 없이 화면 자체를 얻어오게 되면 빔 프로젝터에서 영사되는 모든 영상을 다 받아오기 때문에 밴 드 패스 필터(Band pass filter; 예를 들어, 850nm 이하를 차단하는 필터)를 이용하여 상기 스크린에서 나 오는 영상을 차단하고 적외선 빛을 받을 수 있도록 하였다. 그 결과, 사용자가 콘텐츠와의 인터랙션을 위해 상기 스크린에 손을 뻗게 되면, 상기 스크린 위에 위 치한 상기 적외선 LED 어레이 바(IR-LEDs Array Bar: 110)에서 내려오는 적외선이 손(손등)을 통과하지 못하고 막히게 된다. 이때 상기 적외선 카메라는 손(손등)에서 반사되는 적외선 빛을 촬영하여 상기 서버에 촬영된 영상 신호를 전송하게 된다. 상기 인식 컴퓨터에서는 상기 적외선 카메라에서 전송된 신호를 영상처리를 통해 손 영역의 좌표(x,y) 값을 계산하게 된다. 여기서, 손 영역의 좌표(x,y) 값을 계산하는 방법은 다음과 같다. 영상 이진화를 통하여 LED가 반사되는 영역과 그렇지 않은 영역을 나눈 뒤에 반사되는 영역에 한해서 블랍/라벨 링을 거쳐 호모그래피 행렬을 이용하여 왜곡을 보정한 뒤 좌표의 최상위점을 인식하고 있는 좌표로 계산하여서 작동한다. 여기서, 상기 블랍은 영상 이진화를 하였을 때, LED가 반사된 영역들을 묶는 역할을 한다. 즉, 사람 의 손이 하나의 픽셀(점)로 빛을 반사시키는 게 아니라 다수의 픽셀(면)로 빛을 반사시키기 때문에, 연속으로 LED가 반사되는 영역을 묶어주는 역할을 한다. 상기 라벨링은 상기 블랍을 통해 묶인 영역들에 대해 이름 그대 로 번호를 지정하는 작업이다. 각 영역당 번호를 지정함으로써, 영역이 생겨서부터 사라질 때까지의 좌표값을 계산할 수 있다. 이때, 영상은 3차원이 아닌 2차원이기 때문에 (x,y)좌표로만 계산하여 (x,y)의 좌표정보 값만 계산된다.이와 같이, 상기 인식 컴퓨터는 상기 적외선 카메라로부터 획득한 정보를 영상 이진화를 통하여 LED 빛이 반사된 영역과 그렇지 않은 영역으로 분리한 후 블랍 및 라벨링을 통해 상기 반사된 영역을 묶어서 각각 번호를 지정한 다음, 호모그래피 행렬을 이용하여 신호 왜곡을 보정하고 윈도우 API을 통해 좌표를 보정 한 후, 상기 인터랙션 막에 접촉된 인체나 물체의 좌표값을 추출한다. 그리고, 상기 렌더링 컴퓨터는 상기 좌표값을 통해 입체 콘텐츠와의 인터랙션을 하게 된다. 이러한 입체 콘텐츠는 오픈 게임 엔진인 오우거 3D(Ogre 3D)를 통해 랜더링하게 된다. 이때, 엔진에서는 가상 공간상에서 스 테레오 카메라를 생성하여 각각의 카메라 뷰를 렌더링하고, 상기 렌더링 된 정보를 상기 3차원 입체 프로젝터를 통해 각각의 좌, 우 영상을 투영하여 3D 입체 영상을 생성한다. 또한, 인터랙션은 외부 스크립트 언어인 Luatinker를 사용하여 인터랙션 이벤트 요소를 추가한다. 3차원 입체 인터랙티브 비전 시스템의 구체적인 동작 원리 본 발명에서는 적외선 LED를 이용한 적외선 LED 어레이 바와 적외선 카메라를 통해 사용자의 행동에 대한 감지를 하게 된다. 이는 기존의 접촉식 센서와는 달리 인체의 유무와 통과 여부를 2차원 좌표로 출력하여 사용자의 위치를 감지한다. 이렇게 감지된 좌표정보 값은 컨트롤러에 전송되고 컨트롤러의 CPU 프로그램에 의해 x축과 y축의 입력포트의 위치를 연산 처리하여 컴퓨터로 전송하게 된다. 이러한 위치정보 값은 3차원 그래픽 영 상과 연동하여 사용자가 실제로 3차원 입체 공간상에 있는 것처럼 느끼게 된다. 즉, 입체 영상 위에 사용자의 행동에 따라 물체가 행동패턴에 맞춰 반응하여 영상 속의 개체들과 직접 인터랙션 할 수 있다. 이러한 내용들을 구체적으로 살펴보면 다음과 같다. OGRE 엔진을 이용한 입체 영상 생성 OGRE 엔진은 Object-oriented Graphic Rendering Engine의 약자로 그래픽 렌더링 및 애니메이션을 처리해주는 엔진이다. 일반적으로 우리가 아는 Unrial이나 Gamebrio와 같은 게임엔진은 그래픽 렌더링 엔진에 인공지능 엔 진과 물리엔진 등을 포함하는 풀(Full) 3D 게임엔진(Game Engine)이기 때문에 OGRE와 같은 그래픽 엔진과는 차 이가 있다. 하지만 객체지향인터페이스 방식의 설계를 통한 깔끔하고 간결한 엔진 클래스 및 인터페이스와 확장 성 때문에, 공개 엔진 중에서 가장 많이 이용되는 엔진이 OGRE 엔진이기도 한다. 상기 OGRE는 도 4와 같이, 크게 장면 관리(Scene Management: 310), 리소스 관리(Resource Management: 320), 렌더링(Rendering: 330)의 3개 그룹으로 나뉘어 있고, 기타 플러그인들을 붙여서 사용을 하게 된다. 먼저, 상기 장면 관리(Scene Management: 310)는 화면상에 보이는 모든 개체들을 종합적으로 관리하는 역할을 하는 부분이다. 카메라, 광원, 평면과 같은 것들도 장면 관리자의 관리 대상인데, 이것들은 장면 위에 직접 배 치가 불가능하기 때문에 간접적인 방법을 이용하여 관리하게 된다. 그 방법이 바로 장면노드(Scene Node)를 이 용하는 것이다. 다음, 상기 리소스 관리(Resource Management: 320)는 상기 렌더링(Rendering: 330)에 필요한 geometry, texture, font와 같은 모든 리소스 자원을 관리하는 역할을 한다. 주요 역할은 리소스 불러오기, 재사용, 해제 등의 관리이다. 끝으로, 상기 렌더링(Rendering: 330) 파트는 화면에 보여주는 역할을 한다. 렌더링을 위해 거치는 과정은 월드, 카메라, 투영행렬을 거쳐 3차원의 정점이 2차원의 점으로 변환되는데, 이것을 묘사한 것이 렌더링 파이프 라인이다. 이 파이프라인의 하위 단계에서 모니터에 보여주기까지의 처리과정이 렌더링(Rendering) 단계에서 관 리된다. 따라서, 본 발명에서는 상기 장면 관리(Scene Management: 310)에서 양안의 카메라를 생성하여 3차원 입체 영상 을 생성하게 된다. 생성된 양안의 카메라 뷰(view)를 상기 렌더링(Rendering: 330) 파트에서 각각의 좌, 우 카 메라 뷰(view)를 렌더링하게 된다. 먼저, 3차원 영상의 획득이라 함은 사람의 양안 시차에 관한 원리를 말한다. 즉, 인간은 두 눈을 통해 영상의 입체감, 깊이 감을 느끼듯이 카메라 두 대를 사용하여 좌측 카메라로 획득한 영상은 좌측 눈에, 우측 카메라로 획득한 영상은 우측 눈에 보여주는 것이다. 양안 입체영상의 품질은 인간의 시각 기능과 최대한 가까운 조건으로 설정되었을 때 인간이 보는 듯한 현장감이 있는 양질의 3차원 입체 영상을 획득할 수 있다. 인간의 두 눈의 기능과 가까운 양안 입체 영상 획득을 위해서 는 양안 입체 영상 카메라가 필요하다. 일반적으로, 좌, 우 영상을 획득하는 양안 입체 카메라는 두 대의 카메라와 카메라의 거리 조절에 따라 크게 세 가지로 평행방식(Parallel Camera)과 교차방식(Toed-in Camera), 그리고 수평이동방식(Horizontal-moving Camera)으로 나눌 수 있다. 도 5는 3차원 입체 영상을 생성하기 위해 카메라를 사용하여 영상을 획득하는 방법 을 나타낸 것이다. 양안 입체영상 획득 방식 중 가장 단순한 형태로 도 5의 (a)에 나타낸 바와 같이, 피사체에 대해 평행하게 일정 간격으로 고정한 후, 좌·우 영상을 획득하는 방식이다. 도 5의 (b)는 양안 입체 영상 획득 방식 중 가장 많이 사용되는 방식으로 피사체의 거리 이동에 따른 양안 시차의 조절을 위해 좌·우 카메라의 광축을 회전시키는 방 식이다. 이때 이동거리를 선정함으로써 양안을 얻게 되는데 보통 이동거리는 동공간격(예를 들어, 65㎜)으로 하 고 있다. 본 발명에서는 다양한 입체 카메라 생성 방법 중에서 사람의 눈과 흡사한 방식인 교차식 방식(도 5의 (b))을 사 용하여 카메라를 생성한다. 이 방식은 카메라 광축 간격의 거리를 조절함으로써 사용자가 입체감이 가장 높은 거리를 설정하는 방식으로 피사체의 이동에 따른 시차 변화의 보정이 가능하다는 장점을 가지고 있기 때문이다. 일반적으로, 사람의 양안은 65㎜ 간격을 가지고 있다. 이로 인해 사람이 어떤 사물을 바라보게 되면 양안의 시 차에 의해 사물을 입체로 인지하게 되며, 깊이 판단에 중요한 실마리가 된다. 또한, 사물의 깊이 정보를 판단하 는 기준인 영 교차점은 그 위치에 따라 입체감이 달라진다. 영 교차점은 그 위치에 따라 양의 시차, 영 시차, 음의 시차가 있다. 양의 시차(도 6의 (a))는 영 교차점이 영상의 뒤에 위치해서 사물이 영상 뒤에 맺히며 영 시 차(도 6의 (b))는 영상과 같은 위치에, 음의 시차(도 6의 (c))는 영상의 앞에 사물이 보이게 된다. 이렇듯 3차원 입체 영상을 생성하기 위해서는 양안 시차와 영 교차점은 중요한 요소들이다. 따라서 양안 시차와 영 교차점을 적절히 조절함으로써 각각의 3차원 입체 콘텐츠 제작을 위한 각 장면들의 입체 영상을 생성한다. 양안 시차와 영 교차점을 설정하는 것은 사람마다 느끼는 입체감이 다르기 때문에 설정하기가 어렵다. 왜냐하면 양안 시차와 영 교차점을 조절하여 사용자로 하여금 몰입감이 높은 결과물로 인식되기까지는 반복적이고 많은 시간이 필요하다. 그리고 기존의 방법들은 실시간으로 양안 시차와 영 교차점을 조절하면서 확인할 수 있는 방 법을 제공하지 않고 있다. 이러한 문제를 해결하기 위해서 본 발명에서는 사용자가 직관적으로 각 장면들의 양안 시차와 영 교차점을 실시 간으로 조절하여 입체 영상의 입체 정도를 확인할 수 있도록 대화식 입체 영상 미리 보기 기능을 제안한다. 실 제로, 사용자가 높은 몰입감을 느낄 수 있는 입체 영상 시점을 결정하기 위해 편광 필터 안경을 착용한 상태에 서 입체 영상을 실시간으로 확인하며 카메라의 양안 시차와 영 교차점을 조절하고 입체 정도를 확인하면서 사용 자가 느낄 수 있는 가장 적절한 입체감을 결정한 후 3차원 입체 영상을 생성한다. 적외선 LED 어레이 바(IR-LEDs Array Bar)를 이용한 인터랙션 방법 적외선 LED 어레이 바(IR-LEDs Array Bar: 110)를 3차원 입체 영상 디스플레이의 상단 전면부분에 설치한 다. 사용자의 인터랙션을 포착하기 위해 전면부 천정에 부착을 하며, 이때 사용자의 인터랙티브 영역을 구성하 는 인터랙션 막이 생성된다. 사용자의 손이 인터랙션 막에 닿았을 때 사용자가 자유롭게 인터랙션 할 수 있는 인터랙션 영역에 진입했다는 의미이다. 이때, 인터랙션 막에서 발생되는 적외선 빛은 \"빛의 직진성\" 원리에 의해 직선으로 빛이 퍼져나간다. 사용자 손 을 트래킹하기 위해서 적외선 빛이 상단에서 하단으로 수직으로 발산한다. 그리고 앞에 물체가 가로막고 있으면 물체 반대편에 그림자가 생기게 된다. 이러한 원리를 이용하여 대규모 디스플레이에 인터랙션하기 위해 적외선 빛은 손등에서 남아 있게 되고 아래쪽에는 그림자가 생긴다. 이때 3차원 입체 영상 디스플레이를 바라보는 적외 선 카메라는 적외선 빛을 수집하게 된다. 즉, 사용자가 상기 스크린에 인터랙션하기 위해 손으로 터치할 경우 상기 스크린 전면에 형성된 상기 인터랙션 막에 닿게 된다. 상기 인터랙션 막은 앞에서 설명한 바와 같이, 상기 적외선 LED 어레이 바 에서 발생 된 적외선 빛에 의해 형성된 인터랙션 영역이다. 도 7의 그림과 같이, 상기 인터랙션 막에 손이 닿으면, 손 아래쪽에는 상기 적외선 LED(IR-LEDs)의 빛이 차단되어 그림자가 생기게 되고, 손 위쪽에는 상기 적외선 LED(IR-LEDs)의 빛이 손등에서 반사되게 된다. 이때,상기 손등에서 반사되는 적외선 빛은 상기 인터랙션 영역을 촬영하는 상기 적외선 카메라에 촬영되어 수집 된다. 상기 적외선 카메라는 촬영된 영상 신호를 상기 인식 컴퓨터에 전송하고, 상기 인식 컴퓨터 에서는 상기 적외선 카메라에서 전송된 신호를 영상처리를 통해 사용자의 손 영역을 검출함으로써, 손의 좌표(x,y) 값을 추출하게 된다. 이때, 사용자가 바라보는 뷰 영역의 스크린과 상기 적외선 카메라가 바라보는 뷰 영역의 스크린의 영상의 모양은 차이가 있다. 사용자가 바라보는 뷰 영역은 편의를 위해 직사각형의 영역으로 맞추지만(도 8의 a). 상기 적외선 카메라로 바라보는 뷰 영역은 공간적 제약에 따른 사다리꼴의 영상왜곡이 생긴다(도 8의 b). 왜곡 된 영상의 손가락(또는 물체) 좌표를 보정 없이 윈도우 시스템이 좌표로 적용하는 것은 원하는 위치의 좌표로 반환할 수 없다. 이러한 문제를 해결하기 위한 방법으로 호모그래피 행렬(Homography Matrix)을 적용한다. 상기 호모그래피 행렬은 어느 한 평면에 대하여 그 평면 위에 있는 점과 다른 한 평면의 대응점은 3×3 행렬로 표현이 가능하다. 여기에서 3×3 행렬은 두 평면 사이의 사용 변환 관계를 나타내는데, 이것을 호모그래피 (Homography) 라고 한다. 호모그래피 행렬을 사전식으로 배열한 9×1 벡터를 평면 위에 있는 점과 다른 한 평면 의 대응점을 조합한 n×9 행렬에 곱하면 0이 된다. 이 과정에서 호모그래피 행렬을 사전식으로 배열한 9×1 벡 터는 SVD(Singular value Decomposition)를 통해 구한다. 이와 같이, 호모그래피 행렬을 이용하여 영상의 왜곡을 보정한다. 보정된 좌표는 상기 적외선 카메라의 뷰 영역에서의 좌표일 뿐 실제 윈도우 좌표에 적용되는 좌표는 아니다. 만약, 상기 적외선 카메라의 전체 뷰 영역이 640×480이고, 윈도우의 해상도가 1024×768이라면 위치의 오차는 많이 날 수밖에 없다. 이러한 위치를 줄이기 위해 마우스의 범위를 API레벨에서 계산하여 제어 가능하도록 하였다. 마우스의 실제 좌표는 상하좌우의 모든 영역은 해상도와 관계없이 0~65535의 영역을 가지기 때문에 현 해상도(width, height)를 나눠준 후 호모그 래피의 계산 결과로 나온 좌표를 곱하면 해상도에 맞는 좌표가 계산된다(도 8의 c). 다음으로, 상기 적외선 카메라가 바라보는 해상도의 크기는 640×480이지만, 실제 모니터(1024×768) 또는 프로젝터에 보이는 화면의 크기가 달라 호모그래피 행렬을 사용하여 좌표보정을 하여도 올바르지 않다. 이 를 위해, WindowsAPI 레벨에서 좌표 보정을 하게 되면 올바르게 트래킹이 된다. 이때, 상기 윈도우(Windows) API 레벨에서 좌표 보정하는 방법과 트래킹 방법은 다음과 같다. OpenCV 라이브러 리에서 제공하는 cvWarpPerspective와 cvGetPerspectiveTransform을 통하여 상기 적외선 카메라가 보는 화면에서 영상의 각 모서리 4군데의 좌표를 입력하여 4개의 점으로 이루어진 사각형 내의 이미지를 새로운 4개 의 점으로 이루어진 사각형으로 변환하여 왜곡을 보정 한다. 상기 트래킹은 사람의 손이나 물체의 위치를 파악 하여 얻은 좌표값을 네트워크로 좌표값을 콘텐츠에 보내서 해당되는 지점에 인터랙션을 발생하게 해준다. 이렇게 추적된 손이 좌표값을 통해 입체 콘텐츠와의 인터랙션을 하게 된다. 상기 3차원 입체 인터렉티브 비전 시스템은 서버-클라이언트를 기반으로 한다. 손의 좌표를 계산해서 추출하는 것은 상기 인식 컴퓨터에서 이루어 계산되고, 추출된 손의 좌표는 상기 스크린의 좌측 상단(0, 0)을 기준으로 하는 윈도우 좌표계의 형태로 변경하여 클라이언트에 초당 30∼50개 정도의 좌표 데이터를 전송하게 된다. 상기 렌더링 컴퓨터에 서는 월드좌표 내 카메라를 기준으로 전달받은 좌표를 이용해 광선추적(ray tracing) 기법을 이용해서 오브젝트 들이 카메라를 기준으로 거리별로 정렬되게 한 후, 가장 가까이 있는 오브젝트가 인터랙션이 일어나도록 처리된 다. 상기 입체 콘텐츠는 오픈 게임 엔진인 오우거 3D(Ogre 3D)를 통해 렌더링하게 된다. 상기 오우거 3D(Ogre 3D)에 서는 입체 콘텐츠가 실행된 후, 초기화 과정에서 똑같은 해상도의 렌더링 윈도우(최종 이미지를 출력하는 창)를 좌, 우 각각 1개씩 생성한다. 생성된 각각의 윈도우에 3D 카메라를 연결하여 각각의 카메라 시점에서 보이는 오 브젝트들이 렌더링 된 결과물이 연결된 윈도우를 통해서 렌더링 된다. 여기서, 본 발명에서는 좌, 우 카메라의 양안 차 조절을 다음과 같이 실시간 제어하였다. 즉, 양안 차를 조절하 기 위해 키보드의 ‘←’, ‘→’ 방향키를 이용하여 카메라 간의 간격을 조절할 수 있는 기능을 추가하였다. 초기 셋-업 할 때의 카메라 위치는 같지만 ‘←’ 키를 이용하면 좌측에 위치한 렌더링 윈도우와 연결된 카메라 는 좌측으로 이동하고, 우측에 위치한 렌더링 윈도우와 연결된 카메라는 우측으로 이동하면서 좌우 간격이 벌어 지게 되고, ‘→’ 키를 이용하면 좌, 우 카메라의 간격을 좁힐 수 있다. 셋업된 카메라 정보는 별도의 파일에 저장되어 이후 실행될 때마다 설정된 값을 로드해서 사용하게 된다. 그 다음, 상기 오우거 3D(Ogre 3D)를 통해 가상 공간상에서 스테레오(좌우) 카메라를 생성하여 각각의 카메라 뷰(view)를 렌더링한다. 초기화 과정에서 좌, 우 각각의 렌더링 윈도우를 생성해서 두 개의 창이 활성화된다.각 창에 연결된 카메라는 각각 초당 30프레임 이상의 속도로 각 카메라 시점에서 보이는 월드(다양한 3차원 오 브젝트들로 구성)를 렌더링하게 된다. 이때, 렌더링 절차는 오브젝트의 월드변환, 각 카메라 정보를 이용한 뷰 (카메라) 변환, 각 렌더링 윈도우에 맞는 프로젝션 변환, 레스터라이즈, 화면 출력 순으로 진행된다. 상기 렌더링 된 정보는 3D 입체 프로젝터를 통해 각각의 좌, 우 영상을 투영함으로써 3D 입체 영상을 생성하게 된다. 3D 입체 영상을 생성하기 위해 2개의 렌더링 윈도우에 다시 2개의 빔프로젝터를 연결한다. 렌더링 된 이 미지는 빔프로젝터를 통해서 좌, 우 영상을 싱크에 맞게 제공해주고, 양안 카메라의 거리를 조절하게 해줌으로 써 입체감이 살아날 수 있게 한다. 입체를 위해 사용된 빔프로젝터의 출력부위에 편광유리를 두고, 두 개의 상 이 한곳에 모이게끔 합성하여, 이용자는 편광안경을 통해 입체감을 체감할 수 있게 된다. 또한, 인터랙션은 외부 스크립트 언어인 Luatinker를 사용하여 인터랙션 이벤트 요소를 추가한다. 인터랙션 연 동을 위해 사용된 외부 스크립트는 루아 스크립트이고, 오브젝트별로 테이블을 지정해 놓고, 여기에 오브젝트의 이름, 타입, 사용 메쉬, 초기좌표, 크기, 회전 값 등을 넣을 수 있게 해놓았다. 이렇게 형식에 맞게 채워진 값 들은 엔진의 초기화 부분에서 이용되며, 오브젝트의 생성에 활용된다. 이벤트에 대한 처리를 위해서는 오브젝트명.lua 파일에 따로 지정을 해놓았는데, lua파일에는 이동 및 돌출 같 은 이벤트를 함수로 지정해놓고, 인터렉션에 따라 엔진에서 루아에 있는 함수들을 호출하는 형태로 되어있다. 이렇게 이벤트와 이동에 대한 정의를 루아스크립트로 빼놓으면서, 프로그램이 실행되는 중에도 실시간으로 수정 이 가능하며 스크립트 다시 읽기 기능을 넣어, 바로 갱신된 화면을 확인할 수 있다. 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템의 구현 예 도 9 내지 도 16은 본 발명의 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템을 구현한 예의 캡처 화면으로서, 도 9은 후면 스크린의 정면 모습의 캡처 화면이고, 도 10 내지 도 12는 한 명이 인터랙션 하는 모 습을 나타낸 캡처 화면이고, 도 13 내지 도 16은 두 명이 동시에 인터랙션 하는 모습을 나타낸 캡처 화면이다. 이와 같이 구성된 본 발명에 의한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법은 스크린 전면의 천장 또는 바닥에 적외선 LED 어레이 바(IR-LEDs Array Bar)를 설치하여 사용자가 인터랙션할 수 있는 인터랙티브 막(interactive surface)을 생성하여 사용자가 손동작으로 콘텐츠와 인터랙션(interaction)을 자유롭게 할 수 있고, 사용자의 손동작에 따라 입체 영상의 물체가 반응하여 움직임으로써 영상 속의 개체들과 직접 인터랙션을 할 수 있다. 또한, 한 명 또는 다수의 인원이 동시에 참여할 수 있도록 프로젝션 방식을 통해 대규모 인터랙티브 인터페이스를 제공하여 사용자에게 재미와 몰입감 있는 인터랙션 콘텐츠를 제공함으로써, 본 발명의 기술적 과제를 해결할 수가 있다. 이상에서 설명한 본 발명의 바람직한 실시 예들은 기술적 과제를 해결하기 위해 개시된 것으로, 본 발명이 속하"}
{"patent_id": "10-2010-0026970", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "는 기술분야에서 통상의 지식을 가진 자(당업자)라면 본 발명의 사상과 범위 안에서 다양한 수정, 변경, 부가 등이 가능할 것이며, 이러한 수정 변경 등은 이하의 특허청구범위에 속하는 것으로 보아야 할 것이다. 산업상 이용가능성 본 발명에 의한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템 및 그 방법은 개인 도는 대규모의 군중이 다양하게 콘텐츠를 즐길 수 있는 인터랙티브 앰비언트 디스플레이 환경으로 제공할 수 있으며, 멀티 카 메라, 멀티 컴퓨터 등의 연동을 통한 패러럴 컴퓨팅 방식으로 확장을 할 수 있다."}
{"patent_id": "10-2010-0026970", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 종래 기술에 따른 실시간 가상 현실 스포츠 플랫폼 장치의 구성도 도 2는 본 발명의 바람직한 실시 예에 의한 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템을 개 략적으로 나타낸 구성도 도 3은 적외선 LED 어레이 바(IR-LEDs Array Bar)의 제품 사진 도 4는 그래픽 랜더링 및 에니메이션을 처리하는 OGRE 엔진 코어 오브젝트(Engine Core Objects)의 기능별 구성 도 도 5는 3차원 입체 영상을 생성하기 위해 카메라를 사용하여 영상을 획득하는 방법을 설명하기 위한 도면 도 6은 시차의 종류를 설명하기 위한 도면 도 7은 인터랙션 막에서의 손 영역을 추출하는 원리를 설명하기 위한 도면 도 8은 영상 왜곡 보정을 위한 호모그래픽(Homography) 알고리즘 과정을 보여주는 예시도 도 9 내지 도 16은 본 발명의 공간 멀티 인터랙션 기반 3차원 입체 인터랙티브 비전 시스템을 구현한 예의 캡처 화면으로서, 도 9은 후면 스크린의 정면 모습의 캡처 화면이고, 도 10 내지 도 12는 한 명이 인터랙션 하는 모습을 나타낸 캡처 화면이고, 도 13 내지 도 16은 두 명이 동시에 인터랙션 하는 모습을 나타낸 캡처 화면이다."}
