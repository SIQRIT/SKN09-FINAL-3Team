{"patent_id": "10-2023-0084078", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0001624", "출원번호": "10-2023-0084078", "발명의 명칭": "문체 관점에서 언어 모델을 학습시키는 방법과 장치, 그리고 이를 기록한 기록 매체", "출원인": "성균관대학교산학협력단", "발명자": "이지형"}}
{"patent_id": "10-2023-0084078", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "제1 훈련 데이터 셋을 가지고 비지도 학습 방식으로 언어 모델을 사전 학습시키는 제1 단계;문체가 구분된 제2 훈련 데이터 셋을 가지고 사전 학습된 상기 언어 모델을 재학습시키는 제2 단계; 그리고도메인에 따라 분류된 제3 훈련 데이터 셋을 가지고 지도 학습 방식으로 재학습된 상기 언어 모델을 미세 조정하는 제3 단계;를 포함하는, 한국어 기계 독해를 위한 문체 관점에서 언어 모델을 학습시키는 방법."}
{"patent_id": "10-2023-0084078", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 트레이닝 데이터 셋은 상기 도메인 및 상기 문체와 관계없이 만들어진 대량의 말뭉치인, 한국어 기계독해를 위한 문체 관점에서 언어 모델을 학습시키는 방법."}
{"patent_id": "10-2023-0084078", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제2 단계에서 상기 사전 학습된 언어 모델은 비지도 방식으로 재학습되는, 한국어 기계 독해를 위한 문체관점에서 언어 모델을 학습시키는 방법."}
{"patent_id": "10-2023-0084078", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 제2 훈련 데이터 셋은 상기 제3 훈련 데이터 셋의 도메인과 동일한 도메인을 가지는, 한국어 기계 독해를위한 문체 관점에서 언어 모델을 학습시키는 방법."}
{"patent_id": "10-2023-0084078", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 제2 훈련 데이터 셋은 상기 제1 훈련 데이터 셋과 다른 데이터 셋이며, 상기 제3 훈련 데이터 셋과는 도메인이 동일한, 한국어 기계 독해를 위한 문체 관점에서 언어 모델을 학습시키는 방법."}
{"patent_id": "10-2023-0084078", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제5항 중 어느 한 항에 기재된 한국어 기계 독해를 위한 문체 관점에서 언어 모델을 학습시키는 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2023-0084078", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "공개특허 10-2025-0001624-3-기계 독해를 위한 언어 모델을 저장하는 메모리; 및상기 언 모델을 실행해 입력으로부터 결과를 추론하는 프로세서;를 포함하고,상기 언어 모델은,제1 훈련 데이터 셋을 가지고 비지도 학습 방식으로 사전 학습되고,문체가 구분된 제2 훈련 데이터 셋에 기반해 비지도 학습 방식으로 재학습되고,도메인에 따라 분류된 제3 훈련 데이터 셋에 기반해 지도 학습 방식으로 미세 조정된,연산 장치."}
{"patent_id": "10-2023-0084078", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 제1 트레이닝 데이터 셋은 상기 도메인 및 상기 문체와 관계없이 만들어진 대량의 말뭉치인, 연산 장치."}
{"patent_id": "10-2023-0084078", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 제2 훈련 데이터 셋은 상기 제3 훈련 데이터 셋의 도메인과 동일한 도메인을 가지는, 연산 장치."}
{"patent_id": "10-2023-0084078", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서,상기 제2 훈련 데이터 셋은 상기 제1 훈련 데이터 셋과 다른 데이터 셋이며, 상기 제3 훈련 데이터 셋과는 도메인이 동일한, 연산 장치."}
{"patent_id": "10-2023-0084078", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예는 제1 훈련 데이터 셋을 가지고 비지도 학습 방식으로 언어 모델을 사전 학습시키는 제1 단 계, 문체가 구분된 제2 훈련 데이터 셋을 가지고 사전 학습된 상기 언어 모델을 재학습시키는 제2 단계, 도메인 에 따라 분류된 제3 훈련 데이터 셋을 가지고 지도 학습 방식으로 재학습된 상기 언어 모델을 미세 조정하는 제3 단계를 포함하는, 한국어 기계 독해를 위한 문체 관점에서 언어 모델을 학습시키는 방법에 관한 것이다."}
{"patent_id": "10-2023-0084078", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 기계 독해에서 동사 중심 언어인 한국어의 언어적 특성을 고려해 언어 모델을 학습시켜 언어 모델의 성능을 개선한 학습 방법과 장치, 그리고 이를 기록한 기록 매체에 관한 것이다."}
{"patent_id": "10-2023-0084078", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "기계 독해(Machine Reading Comprehension)는 인공지능(AI) 알고리즘이 스스로 문제를 분석하고 질문에 최적화 된 답안을 찾아내는 기술을 말한다. 이 같은 기계 독해를 위한 언어 모델을 학습시키는 일반적 방법은 대량의 말뭉치를 이용해 비지도 방식으로 언어 모델을 사전 학습시키는 과정과 도메인에 따라 분류된 데이터 셋을 가지 고 언어 모델을 지도 학습 방식으로 훈련시키는 과정을 포함한다. 그런데, 사전학습된 언어 모델의 미세조정에 사용되는 라벨링된 데이터는 라벨링에 많은 시간과 비용이 든다. 과업이 도메인에 특화되어 있으면 데이터는 더욱 구하기 어려워진다. 또한, 명사 중심 언어인 영어와 달리 한국어는 동사 중심 언어로 사회적 관계를 분명하게 반영하는 특별한 존칭 어휘, 문장 종결어미, 토씨 등을 사용한다. 이러한 특징으로 한국어는 단순한 격식성의 차원이 아닌 종결어미의 구분체계에 근거하여 문체를 구분한다. 이에 한국어 언어모델에 대한 적응적 사전학습은 문체 관점의 연구가 필"}
{"patent_id": "10-2023-0084078", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "요하다.발명의 내용"}
{"patent_id": "10-2023-0084078", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 이 같은 기술적 배경에 창안된 것으로, 영어와 다른 한국어의 특성을 고려하여 문체 관점의 적응적 사전학습을 언어 모델에 적용해서 언어 모델의 성능을 개선하고자 한다."}
{"patent_id": "10-2023-0084078", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 학습 방법은, 문체 관점의 적응적 사전학습을 수행하기 위해 3 단계를 거쳐 학습을 진행한다. 본 발명의 일 실시예에 따른 학습 방법은, 도 1 예시와 같이 제1 훈련 데이터 셋을 가지고 비지도 학 습 방식으로 언어 모델을 사전 학습시키는 제1 단계(S10)와, 문체가 구분된 제2 훈련 데이터 셋을 가지고 사전 학습된 상기 언어 모델을 재학습시키는 제2 단계(S20)와, 도메인에 따라 분류된 제3 훈련 데이터 셋을 가지고 지도 학습 방식으로 재학습된 상기 언어 모델을 미세 조정하는 제3 단계(S30)를 포함할 수 있다. 여기서, 상기 제1 훈련 데이터 셋은 상기 도메인 및 상기 문체와 관계없이 만들어진 대량의 말뭉치이다. 상기 제2 단계에서 상기 사전 학습된 언어 모델은 비지도 방식으로 재학습될 수 있다. 상기 제2 훈련 데이터 셋은 상기 제3 훈련 데이터 셋의 도메인과 동일한 도메인을 가질 수 있다. 상기 제2 훈련 데이터 셋은 상기 제1 훈련 데이터 셋과 다른 데이터 셋이며, 상기 제3 훈련 데이터 셋과는 도메 인이 동일하다. 또한 본 발명의 다른 실시예들에서는 상기와 같은 학습 방법을 구현하는 연산 장치 및 기록매체를 개시한다."}
{"patent_id": "10-2023-0084078", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 독해과정이 문체를 포함한 텍스트적 이해가 필요함에 따라 문체 관점의 적응적 사전학습을 기계 독해에 적용함으로써 언어모델이 문체에 적응적으로 대응하여 효과적인 기계 독해를 할 수가 있다. 또한 라벨링이 필요한 미세조정 데이터와 달리 라벨링이 없는 데이터를 사전학습 단계에서 추가적으로 이용함으로써 시간 비용적으로 효율적인 결과를 창출한다. 본 발명을 통해 기존에 미세조정 데이터와 도메인을 맞춰주는 것에 중심을 두었던 적응적 사전학습에서 문체 도 같이 맞춰주며 더 큰 성능 향상을 도모할 수 있다."}
{"patent_id": "10-2023-0084078", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 도면을 참조하여 본 발명의 실시예들을 구체적으로 설명하도록 한다. 다만, 하기의 설명 및 첨부된 도면에서 본 발명의 요지를 흐릴 수 있는 공지 기능 또는 구성에 대한 상세한 설명은 생략한다. 덧붙여, 명세서 전체에서, 어떤 구성 요소를 '포함'한다는 것은, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것 이 아니라, 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 제 1, 제 2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로 사용될 수 있다. 예를 들어, 본 발명의 권리 범위로부터 이탈되지 않은 채 제 1 구성 요소는 제 2 구성 요소로 명명될 수 있고, 유사하게 제 2 구성 요소도 제 1 구성 요소로 명명될 수 있다. 본 발명에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"구비하다\" 등의 용어는 설시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또 는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 특별히 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본"}
{"patent_id": "10-2023-0084078", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "발명이 속하는 기술분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미이다. 일반적 으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미인 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해 석되지 않는다. 사전학습된 언어 모델의 미세조정(fine-tuning)에 사용되는 라벨링된 데이터는 라벨링에 많은 시간과 비용이 든 다. 과업(task)이 도메인에 특화되어 있으면 데이터는 더욱 구하기 어려워진다. 이에 본 발명에서는 비지도학습 으로 사전 학습된 언어모델을 미세 조정하기 전, 추가적으로 재학습시켜 미세조정 성능을 높이고자 한다. 이 같 은 적응적 사전학습(Adaptive Pretraining)은 '미세조정에 이용되는(task-)' 혹은 '미세조정 데이터셋과 도메인 이 유사한(domain-)' 라벨링 없는 데이터셋을 추가로 사전 학습하여, 언어모델이 미세조정 데이터에 적응적일 수 있게 한다. 또한, 본 발명에서는 명사 중심 언어인 영어와 달리 한국어는 동사중심 언어로 사회적 관계를 분명하게 반영하 는 특별한 존칭어휘, 문장 종결어미, 토씨 등을 사용한다. 이러한 특징으로 한국어는 단순한 격식성의 차원이 아닌 종결어미의 구분체계에 근거하여 문체를 구분한다. 이에 한국어 언어모델에 대한 적응적 사전학습은 문체 관점의 연구가 필요하다. 특히 독해 과정이 문체를 포함한 텍스트적 이해가 요구되는 것을 고려하여, 본 발명은 기계독해(Machine Reading Comprehension)에서 한국어 문체 관점의 적응적 사전학습(Style-Adaptive Pretraining)과 관련된다. 이 같은 점 들을 고려한 본 발명의 일 실시예에 따른 학습 방법은, 문체 관점의 적응적 사전학습을 수행하기 위 해 3 단계를 거쳐 학습을 진행한다. 본 발명의 일 실시예에 따른 학습 방법은, 도 1 예시와 같이 제1 훈련 데이 터 셋을 가지고 비지도 학습 방식으로 언어 모델을 사전 학습시키는 제1 단계(S10)와, 문체가 구분된 제2 훈련 데이터 셋을 가지고 사전 학습된 상기 언어 모델을 재학습시키는 제2 단계(S20)와, 도메인에 따라 분류된 제3 훈련 데이터 셋을 가지고 지도 학습 방식으로 재학습된 상기 언어 모델을 미세 조정하는 제3 단계(S30)를 포함 할 수 있다. 여기서, 상기 제1 훈련 데이터 셋은 상기 도메인 및 상기 문체와 관계없이 만들어진 대량의 말뭉치이다. 상기 제2 단계에서 상기 사전 학습된 언어 모델은 비지도 방식으로 재학습될 수 있다. 상기 제2 훈련 데이터 셋은 상기 제3 훈련 데이터 셋의 도메인과 동일한 도메인을 가질 수 있다. 상기 제2 훈련 데이터 셋은 상기 제1 훈련 데이터 셋과 다른 데이터 셋이며, 상기 제3 훈련 데이터 셋과는 도메 인이 동일하다. 이상과 같은 본 발명의 구성은 아래에서 설명되는 실험 및 그 실험 결과를 통해 보다 구체적으로 인식될 수 있 다. 본 발명의 학습 방법에서, 제1 단계는 사전학습된 언어모델을 사용한다. 문체 관점의 적응적 사전학습이 한국어 의 특성을 반영하여 수행하는 것이므로 한국어 언어모델을 사용한다. 제2 단계는 미세 조정(fine-tuning)에 사 용하는 데이터와 문체가 동일한 데이터를 이용하여 사전 학습된 언어 모델의 재학습을 진행한다. 이때 적응적 사전학습인 제2 단계는 레이블이 없는 텍스트 데이터를 이용한다. 제3 단계에서 레이블된 데이터 셋으로 재학습 된 언어 모델에 미세 조정(fine-tuning)을 수행한다.한국어 문체 관점에서의 적응적 사전학습을 진행하기 위해 일 실시예에서는 3가지 문체를 사용하였다. 첫 번째 는 행정문서 데이터의 문어체(이하 행정체)이다. 행정문서는 문서작성법에 따라 특수기호를 포함한 항목 기호, 항목에 따른 계층적 띄우기와 함께 문서 내용이 간결하고 명확하게 표현되어야 하므로 명사형 종결어미와 명사 가 많이 사용된다. 두 번째는 뉴스 기사 데이터의 문어체(이하 뉴스체)이다. 뉴스체의 경우 평서형 종결어미가 사용되어 평서문 형태로 구성되어있다. 세 번째는 영상 댓글 데이터(이하 온라인 구어체)이다. 온라인 구어체는 비격식적 구어체 형태로 구성되어 있다. 예시는 표 1과 같다. 표 1 문체 예시 문장 행정체 반도체는 전년 동기 수출 호조에 따른 기저효과로 감소하였으나, 단가상승 및 수 요 증가에 힘입어 금년 중 월간 최대 실적(57억불) 기록 디스플레이는 OLED 수요 확대 및 패널 단가 회복 등으로 감소폭 완화 뉴스체 2006년 개통 이후 15년 무사고 운행으로 국내 최고 수준의 안전성을 자랑하고 있 으며, 지난해에도 재난관리평가 행정안전부 장관상을 수상한 바 있다. 올해는 과학도시 대전의 명성에 걸맞게 '인공지능(AI) 스테이션'을 조성한다는 계획이다. 온라인 구어체 개인적으로 실사화하는데 그래픽이 너무 좋와져서 컴퓨터 걱정함 이왕 이렇게 된 거 이번에 스타워즈 세계에서도 독특한 인공 지능 캐릭터가 여럿 필요한데 거기도 가보겠나 수냐 그러죠 뭐. 이하, 이 같은 본 발명의 일 실시예에 따른 학습 방법이 실질적으로 어떤 효과가 있는지에 대해 실험해 보고, 실험 결과에 대해 설명한다. 실험에 사용된 적응적 사전학습, 실험 설계, 데이터 셋, 모델에 대해 도 2를 가지고 설명하면 다음과 같다. 문체 관점의 적응적 사전학습(제2 단계) 기존의 기계독해는 도 1의 과 같이 뉴스, 위키, 나무위키 데이터로 학습된 한국어 사전학습 모델 \"KoELECTRA\"에 행정체를 이용하여 기계독해 모델인 \"Retrospective Reader\" 로 미세 조정하였다. 문체 관점의 적응적 사전학습(제2 단계)은 그림 1의 과 같이 미세조정에 사용된 데이터와 동일한 문체의 데 이터로 추가적 사전학습(또는 재학습)을 진행하였다. 미세조정 데이터와 도메인은 다르게 설정하여 도메인에 대 한 효과를 배제하고 문체 관점의 적응적 사전학습 효과를 검증하고자 한다. 도 1의 는 에 대한 비교군으 로 미세조정 데이터와 문체를 달리하여 확인한다. 실험의 설계 문체 관점의 적응적 사전학습의 중요성을 분석하기 위해 첫 번째 실험으로 도 1과 같이 실험을 진행한다. 본 실 험은 도메인의 영향을 줄이고 문체 관점의 적응적 사전학습의 효과를 확인하기 위해 도 1의 B 도메인으로 각 문 체에서 도메인의 특정화를 줄일 수 있는 도메인을 사용하였다. 행정체는 B 도메인으로 공공행정 도메인을 사용 하고, 도 1의 에 사용된 뉴스체는 정치, 경제를 포함한 4가지 도메인을 섞어 사용하였다. 도 1의 의 A 도 메인은 과학 도메인을 이용하여 미세조정에 사용된 도메인과는 다르도록 설정하였다. 추가로 문체 관점의 적응 적 사전학습을 문체별로 실험하였다. 적응적 사전학습 단계(제2 단계)의 도메인을 과학 도메인으로 설정하여 행 정체, 뉴스체, 온라인 구어체에 대한 실험을 진행하였다. 데이터셋 행정문서는 AI HUB의 행정문서 대상 기계독해 데이터를 이용했다. 해당 데이터에서 과학기술과 공공행정 도메인 을 이용하였고, 미세조정에 사용된 기계독해 질문 유형은 정답경계 추출형, 절차(방법)형, 응답불가형을 사용하 였다. 뉴스는 AI HUB의 뉴스 기사 기계독해 데이터의 IT과학 도메인을 이용하였다. 온라인 구어체는 AI HUB의 온라인 구어체 말뭉치 데이터의 과학 도메인을 이용했다. 모든 실험에서 미세조정에 사용된 행정체 데이터는 32,976개의 질의응답 쌍이 사용되었다. 첫 번째 실험의 적응적 사전학습에는 뉴스와 행정문서 각각 50,000 문장이 사용되었고, 추가로 진행한 두 번째 실험에서의 적응적 사전학습에는 뉴스와 행정문서 각각 5,000 문장과 문 장 길이를 고려하여 온라인 구어체는 9,000 문장이 사용되었다. 언어 모델 한국어 사전학습 언어모델은 \"KoELECTRA-small-v2\"을 사용하였다. 행정문서가 사전학습에 사용되지 않는 실험 설정을 위해 뉴스, 위키, 나무위키 데이터 약 14G를 사전학습한 v2를 이용하였다. 본 실험에서 사용한 기계독해 는 문단(paragraph)이 있고 그에 대한 질문을 하면 답을 문단 내에서 찾아주는 span extraction과 질문의 답변 가능 여부를 가리는 unanswerable 유형을 사용하였다. 기계독해 모델은 \"Retospective Reader\"를 이용하였다. Retro-Reader는 인간의 독해방식을 차용한 모델로 ELECTRA 모델을 사용하였을 때 가장 좋은 성능을 보였다. 성 능 평가는 질의에 대한 답변을 정확히 맞췄는가에 대한 EM과 토큰 단위의 정답 여부를 보는 F1 score를 이용하 였다. 실험 결과를 설명하면 다음과 같다. 첫 번째 실험의 결과는 표 2와 같다. 3가지 실험(도 1의 , , ) 중 본 발명에 따른 문체 관점의 적응적 사전학습(제2 단계)을 적용한 표2의 실험군인 이 가장 높은 성능을 보였다. 비교군인 와 비교해보았을 때 문체에 대한 적응적 사전학습이 기계독해 성능을 향상시키는 것을 확인할 수 있었다. 또한 라벨링된 데이터로 미세조정한 종래 기술에 따른 과 비교했을 때 큰 성능 차이를 보여 문체가 유사한 레이블 없는 데이터로 기 계독해 성능을 크게 향상시킬 수 있음을 확인했다. 표 2 실험(도메인 - 문체) EM F1 제2 단계 제3 단계 - 공공-행정체 63.32 73.07 일반-뉴스체 공공-행정체 65.94 74.22 과학-행정체 공공-행정체 70.67 76.80 적응적 사전학습(제2 단계)을 문체별로 수행한 두 번째 실험의 결과는 표 3과 같다. 미세조정 데이터와 문체를 맞춰준 적응적 사전학습인 표 3의 이 가장 좋은 성능을 보였다. 또한, 의 온라인 구어체에 대한 적응적 사전학습이 의 뉴스체보다 더 낮은 성능을 보였는데, 뉴스체와 행정체가 모두 문어체임을 고려하면, 한국어 문체 간 유사성에 따라 적응적 사전학습의 성능이 크게 달라질 수 있음을 보였다. 표 3 실험(도메인 - 문체) EM F1 제2 단계 제3 단계 과학-구어체 공공-행정체 51.20 61.86 과학-뉴스체 공공-행정체 65.03 74.02 과학-행정체 공공-행정체 66.30 75.69 본 발명에서는 한국어의 특성을 고려하여 문체 관점의 적응적 사전학습을 기계독해에서 진행하였다. 기계독해 데이터와 동일한 문체의 데이터를 추가 사전학습 시킨 실험에서 큰 성능 향상을 보여 문체 관점의 적응적 사전 학습이 언어 모델의 성능을 향상시킴을 알 수 있었다. 도 3은 적응적 사전 학습이 적용된 언어 모델을 학습시키는 연산 장치를 도시한 블록도로, 상술한 일련의 구성 을 하드웨어 구성의 관점에서 재구성한 것이다. 따라서, 여기서는 설명의 중복을 피하고자 각 구성의 기능 및 동작을 중심으로 그 개요만을 약술하도록 한다 연산 장치는 기계 독해를 위한 언어 모델을 저장하는 메모리, 상기 언어 모델을 실행해 입력으 로부터 결과를 추론하는 프로세서를 포함한다. 언어 모델은 제1 훈련 데이터 셋을 가지고 비지도 학습 방식으로 언어 모델을 사전 학습되고, 문체가 구분 된 제2 훈련 데이터 셋을 가지고 비지도 학습 방식으로 재학습되고, 도메인에 따라 분류된 제3 훈련 데이터 셋 을 가지고 지도 학습 방식으로 미세 조정된다. 제1 트레이닝 데이터 셋은 상기 도메인 및 상기 문체와 관계없이 만들어진 대량의 말뭉치일 수 있다. 제2 훈련 데이터 셋은 상기 제3 훈련 데이터 셋의 도메인과 동일한 도메인을 가거나, 또는 셋은 상기 제1 훈련 데이터 셋과 다른 데이터 셋이며, 상기 제3 훈련 데이터 셋과는 도메인이 동일하다. 한편, 상술한 본 발명은 컴퓨터로 읽을 수 있는 기록 매체에 컴퓨터가 읽을 수 있는 코드로 구현하는 것이 가능 하다. 컴퓨터가 읽을 수 있는 기록 매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종 류의 기록 장치를 포함한다. 컴퓨터가 읽을 수 있는 기록 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 데이터 저장장치 등을 포함한다. 또한, 컴퓨터가 읽을 수 있는 기록 매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산 방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 그리고 본 발명을 구현하기 위한 기능적인 (functional) 프로그램, 코드 및 코드 세그먼트들은 본 발명이 속하는 기술 분야의 프로그래머들에 의하여 용이 하게 추론될 수 있다. 이상에서 본 발명에 대하여 그 다양한 실시예들을 중심으로 살펴보았다. 본 발명에 속하는 기술 분야에서 통상 의 지식을 가진 자는 본 발명이 본 발명의 본질적인 특성에서 벗어나지 않는 범위에서 변형된 형태로 구현될 수 있음을 이해할 수 있을 것이다. 그러므로 개시된 실시예들은 한정적인 관점이 아니라 설명적인 관점에서 고려되 어야 한다. 본 발명의 범위는 전술한 설명이 아니라 특허청구범위에 나타나 있으며, 그와 동등한 범위 내에 있 는 모든 차이점은 본 발명에 포함된 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2023-0084078", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 학습 방법을 설명하는 흐름도이다. 도 2는 적응적 사전 학습의 효과를 알아보기 위한 실험을 설명하는 도면이다. 도 3은 본 발명의 일 실시예에 따른 연산 장치의 블록도이다."}
