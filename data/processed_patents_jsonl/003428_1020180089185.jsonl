{"patent_id": "10-2018-0089185", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0013935", "출원번호": "10-2018-0089185", "발명의 명칭": "인공지능 기반 게임 전략 유도 시스템 및 방법", "출원인": "한국과학기술원", "발명자": "이상완"}}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능 기반의 전략 유도 시스템에서 수행되는 전략 유도 방법에 있어서, 사용자(User)와 상호작용하는 환경을 조성하는 단계; 및 상기 조성된 환경에서 사용자의 의사 결정이 관측됨에 따라 상기 조성된 환경을 변화시켜 기 설정된 다중 목적에 기반한 사용자의 전략을 제어하는 단계 를 포함하는 전략 유도 방법."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 사용자의 전략을 제어하는 단계는,상기 조성된 환경에서 사용자의 의사 결정에 따라 환경의 변화 여부를 판단하고, 상기 사용자의 목표를 성취하기 위한 예측 에러를 조작할 수 있도록 상기 환경을 변경시키는 단계를 포함하는 전략 유도 방법."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 사용자의 전략을 제어하는 단계는,상기 사용자의 뇌 기저핵에서 관찰되는 강화학습 신호와 특성에 대한 목적 함수로 설정하여 사용자의 보상 예측에러 및 상기 사용자의 상태 예측 에러를 포함하는 예측 에러를 예측하는 단계 를 포함하는 전략 유도 방법."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 사용자의 전략을 제어하는 단계는,상기 조성된 환경에서 사용자의 의사 결정에 기반한 환경 변화를 감지하는 단계 를 포함하는 전략 유도 방법."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 사용자의 전략을 제어하는 단계는,상기 환경 변화가 감지됨에 따라 변경된 환경에서의 상기 사용자의 목표를 성취하기 위한 전략이 수행되고, 상기 수행된 전략에 따른 예측 에러가 기 설정된 목적 함수를 만족하는지 여부를 판단하는 단계를 포함하는 전략 유도 방법."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 사용자의 전략을 제어하는 단계는,공개특허 10-2020-0013935-3-상기 사용자의 예측 에러가 상기 기 설정된 목적 함수를 만족하면, 프로세스를 종료하고, 상기 사용자의 예측에러가 상기 기 설정된 목적 함수를 만족시키지 못할 경우, 상기 사용자의 예측 에러 상태 및 현재 환경 상태를확인하고, 상기 확인된 현재 환경 상태에서 요구되는 전략을 유도하기 위한 환경 설정을 변경하는 단계를 포함하는 전략 유도 방법."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서, 상기 사용자의 전략을 제어하는 단계는,상기 환경 변화가 감지되지 않음에 따라 사용자의 의사 결정이 상기 목적 함수를 만족하는지 여부를 판단하고,상기 사용자의 의사 결정이 상기 목적 함수를 만족하면, 프로세스를 종료하고, 상기 사용자의 의사 결정이 상기목적 함수를 만족시키지 못할 경우, 상기 조성된 환경에서 요구되는 전략을 유도하기 위한 환경 설정을 변경하는 단계 를 포함하는 전략 유도 방법."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "인공지능 기반의 전략 유도 시스템에 있어서, 사용자(User)와 상호작용하는 환경을 조성하는 환경 조성부; 및 상기 조성된 환경에서 사용자의 의사 결정이 관측됨에 따라 상기 조성된 환경을 변화시켜 기 설정된 다중 목적에 기반한 사용자의 전략을 제어하는 전략 제어부 를 포함하는 전략 유도 시스템."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 전략 제어부는, 상기 조성된 환경에서 사용자의 의사 결정에 따라 환경의 변화 여부를 판단하고, 상기 사용자의 목표를 성취하기 위한 예측 에러를 조작할 수 있도록 상기 환경을 변경시키는 것을 특징으로 하는 전략 유도 시스템."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 전략 제어부는, 상기 사용자의 뇌 기저핵에서 관찰되는 강화학습 신호와 특성에 대한 목적 함수로 설정하여 사용자의 보상 예측에러 및 상기 사용자의 상태 예측 에러를 포함하는 예측 에러를 예측하는것을 특징으로 하는 전략 유도 시스템."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 전략 제어부는, 상기 조성된 환경에서 사용자의 의사 결정에 기반한 환경 변화를 감지하는 것을 특징으로 하는 전략 유도 시스템."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 공개특허 10-2020-0013935-4-상기 전략 제어부는, 상기 환경 변화가 감지됨에 따라 변경된 환경에서의 상기 사용자의 목표를 성취하기 위한 전략이 수행되고, 상기 수행된 전략에 따른 예측 에러가 기 설정된 목적 함수를 만족하는지 여부를 판단하는것을 특징으로 하는 전략 유도 시스템."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 전략 제어부는, 상기 사용자의 예측 에러가 상기 기 설정된 목적 함수를 만족하면, 프로세스를 종료하고, 상기 사용자의 예측에러가 상기 기 설정된 목적 함수를 만족시키지 못할 경우, 상기 사용자의 예측 에러 상태 및 현재 환경 상태를확인하고, 상기 확인된 현재 환경 상태에서 요구되는 전략을 유도하기 위한 환경 설정을 변경하는 것을 특징으로 하는 전략 유도 시스템."}
{"patent_id": "10-2018-0089185", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서, 상기 전략 제어부는, 상기 환경 변화가 감지되지 않음에 따라 사용자의 의사 결정이 상기 목적 함수를 만족하는지 여부를 판단하고,상기 사용자의 의사 결정이 상기 목적 함수를 만족하면, 프로세스를 종료하고, 상기 사용자의 의사 결정이 상기목적 함수를 만족시키지 못할 경우, 상기 조성된 환경에서 요구되는 전략을 유도하기 위한 환경 설정을 변경하는 것을 특징으로 하는 전략 유도 시스템."}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "일 실시예에 따른 인공지능 기반의 게임 전략 유도 시스템에서 수행되는 게임 전략 유도 방법은, 사용자(User)와 상호작용하는 환경을 조성하는 단계; 및 상기 조성된 환경에서 사용자의 의사 결정이 관측됨에 따라 상기 조성된 환경을 변화시켜 기 설정된 다중 목적에 기반한 사용자의 전략을 제어하는 단계를 포함할 수 있다."}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "아래의 설명은 인간(사용자)과 컴퓨터가 상호작용하는 상황에서, 인공지능에 기반한 전략 유도 시스템 및 방법 에 관한 것이다."}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 인공지능 기술의 적용 범위는 전통적인 로봇 제어나 전략 탐색 문제를 넘어서서, 인간의 동반자로서 적응 형 서비스를 제공하는 문제까지 확장되고 있다. 이와 같은 문제에서는, 일반적으로 컴퓨터는 인공지능 에이전 트에 해당하고 사용자는 인공지능 에이전트가 상호작용하는 환경이라는 설정을 사용하여, 컴퓨터가 사용자의 만 족도에 근거한 보상치를 극대화시키 것을 목표로 한다. 컴퓨터 중심 논리에 따르면, 컴퓨터(인공지능 에이전트)는 사용자(환경)을 관찰하고, 사용자가 가장 만족할 수 있는 정보/서비스를 추론, 계획 및 최종 결정하여 사용자에게 제공한다. 이 때, 컴퓨터가 제공한 정보 또는 서 비스에 의해 사용자의 상태 및 만족도는 변화된다. 위와 같은 절차를 통해 컴퓨터는 사용자의 상태 및 만족도 가 최대화 되었을 때 최대의 보상을 받는 구조이다. 여기서 컴퓨터는 단일 목적을 가지고 사용자에게 편리를 제공하는 구조로, 단일 목적을 실현하기 위해 사용자를 관찰하고, 사용자의 상태가 컴퓨터의 단일 목적에 도달 하면 임무가 종료된다. 게임 상황을 예로 든다면, 컴퓨터 중심 논리에 기반한 시스템은 사용자의 게임 내 성취 도를 환경 상태의 일부로 관찰하고, 이에 따라 컴퓨터가 사용자 성취 최대화라는 단일 목적을 성취하기 위하여 적절한 형태의 게임 전략을 조언하거나, 구동 환경(layout, user interface 등)을 동적으로 재구성하는 시스템 들로 볼 수 있다. 위에서 확인한 바와 같이, 컴퓨터 중심 논리는 사용자 만족도를 최대화시킬 수 있는 가능성을 보이고 있다. 컴 퓨터가 인공지능(혹은 autonomous contents manager)으로서 행동 레벨에서 사용자의 행동 변화를 위해 직접 intervention을 수행하여 사용자 능률 개선을 도모하고, 사용자는 인공지능이 제어하는 환경의 일부로서 인공지능이 제공하는 일련의 intervention 과정을 수동적으로 따르게 함으로써 특정 수준의 만족도를 성취할 수 있다. 다만 이 접근은 사용자의 게임 성취도 및 만족감을 궁극적으로 향상시키지 못한다. 빈번하게 발생하는 사용자 의 심경 변화, 태도 변화, 상황 변화들을 통해 효과가 좌우되는 경우도 발생할 수 있기 때문이다. 컴퓨터는 게 임 성과를 기준으로 환경에 반응하기 때문에, 여러 가지 변화가 이미 진행된 사용자의 상황을 이해하지 못한 상 태에서 지속적으로 intervention을 주면서 사용자의 게임 학습에 오히려 방해를 유발하기도 한다. 이는 컴퓨터 의 목적을 이루지 못한 것일 뿐만 아니라, 사용자의 게임 경험도 오히려 퇴보하는 결과를 낳을 수 있다."}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "사용자와 게임 환경 제어 에이전트가 상호작용하는 환경에서, 게임 환경 제어 에이전트가 환경을 학습하는 사용 자의 경험을 제어하는 인공지능 기반의 전략 유도 방법 및 시스템을 제공할 수 있다. 사용자의 판단이 이전 경험에 따른 인간 뇌 기저핵 레벨에서 관찰되는 보상 예측 에러, 상태 예측 에러를 통해 결정된다는 점에 착안, 사용자의 경험을 원하는 방향으로 최적화하기 위해 컴퓨터가 환경을 조작하여, 다양한 목적에 부합하는 방향으로 사용자의 예측 에러를 뇌 기저핵 레벨에서 조종하고, 결과적으로 사용자의 특정 행동 을 유도하는 인공지능 기반의 전략 유도 방법 및 시스템을 제공할 수 있다."}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "인공지능 기반의 전략 유도 시스템에서 수행되는 전략 유도 방법은, 사용자(User)와 상호작용하는 환경을 조성 하는 단계; 및 상기 조성된 환경에서 사용자의 의사 결정이 관측됨에 따라 상기 조성된 환경을 변화시켜 기 설 정된 다중 목적에 기반한 사용자의 전략을 제어하는 단계를 포함할 수 있다. 상기 사용자의 전략을 제어하는 단계는, 상기 조성된 환경에서 사용자의 의사 결정에 따라 환경의 변화 여부를 판단하고, 상기 사용자의 목표를 성취하기 위한 예측 에러를 조작할 수 있도록 상기 환경을 변경시키는 단계를 포함할 수 있다. 상기 사용자의 전략을 제어하는 단계는, 상기 사용자의 뇌 기저핵에서 관찰되는 강화학습 신호와 특성에 대한 목적 함수로 설정하여 사용자의 보상 예측 에러 및 상기 사용자의 상태 예측 에러를 포함하는 예측 에러를 예측 하는 단계를 포함할 수 있다. 상기 사용자의 전략을 제어하는 단계는, 상기 조성된 환경에서 사용자의 의사 결정에 기반한 환경 변화를 감지 하는 단계를 포함할 수 있다. 상기 사용자의 전략을 제어하는 단계는, 상기 환경 변화가 감지됨에 따라 변경된 환경에서의 상기 사용자의 목 표를 성취하기 위한 전략이 수행되고, 상기 수행된 전략에 따른 예측 에러가 기 설정된 목적 함수를 만족하는지 여부를 판단하는 단계를 포함할 수 있다. 상기 사용자의 전략을 제어하는 단계는, 상기 사용자의 예측 에러가 상기 기 설정된 목적 함수를 만족하면, 프 로세스를 종료하고, 상기 사용자의 예측 에러가 상기 기 설정된 목적 함수를 만족시키지 못할 경우, 상기 사용 자의 예측 에러 상태 및 현재 환경 상태를 확인하고, 상기 확인된 현재 환경 상태에서 요구되는 전략을 유도하 기 위한 환경 설정을 변경하는 단계를 포함할 수 있다. 상기 사용자의 전략을 제어하는 단계는, 상기 환경 변화가 감지되지 않음에 따라 사용자의 의사 결정이 상기 목 적 함수를 만족하는지 여부를 판단하고, 상기 사용자의 의사 결정이 상기 목적 함수를 만족하면, 프로세스를 종 료하고, 상기 사용자의 의사 결정이 상기 목적 함수를 만족시키지 못할 경우, 상기 조성된 환경에서 요구되는 전략을 유도하기 위한 환경 설정을 변경하는 단계를 포함할 수 있다. 인공지능 기반의 전략 유도 시스템은, 사용자(User)와 상호작용하는 환경을 조성하는 환경 조성부; 및 상기 조 성된 환경에서 사용자의 의사 결정이 관측됨에 따라 상기 조성된 환경을 변화시켜 기 설정된 다중 목적에 기반 한 사용자의 전략을 제어하는 전략 제어부를 포함할 수 있다. 상기 전략 제어부는, 상기 조성된 환경에서 사용자의 의사 결정에 따라 환경의 변화 여부를 판단하고, 상기 사 용자의 목표를 성취하기 위한 예측 에러를 조작할 수 있도록 상기 환경을 변경시킬 수 있다. 상기 전략 제어부는, 상기 사용자의 뇌 기저핵에서 관찰되는 강화학습 신호와 특성에 대한 목적 함수로 설정하 여 사용자의 보상 예측 에러 및 상기 사용자의 상태 예측 에러를 포함하는 예측 에러를 예측할 수 있다. 상기 전략 제어부는, 상기 조성된 환경에서 사용자의 의사 결정에 기반한 환경 변화를 감지할 수 있다. 상기 전략 제어부는, 상기 환경 변화가 감지됨에 따라 변경된 환경에서의 상기 사용자의 목표를 성취하기 위한 전략이 수행되고, 상기 수행된 전략에 따른 예측 에러가 기 설정된 목적 함수를 만족하는지 여부를 판단할 수 있다. 상기 전략 제어부는, 상기 사용자의 예측 에러가 상기 기 설정된 목적 함수를 만족하면, 프로세스를 종료하고, 상기 사용자의 예측 에러가 상기 기 설정된 목적 함수를 만족시키지 못할 경우, 상기 사용자의 예측 에러 상태 및 현재 환경 상태를 확인하고, 상기 확인된 현재 환경 상태에서 요구되는 전략을 유도하기 위한 환경 설정을 변경할 수 있다. 상기 전략 제어부는, 상기 환경 변화가 감지되지 않음에 따라 사용자의 의사 결정이 상기 목적 함수를 만족하는 지 여부를 판단하고, 상기 사용자의 의사 결정이 상기 목적 함수를 만족하면, 프로세스를 종료하고, 상기 사용 자의 의사 결정이 상기 목적 함수를 만족시키지 못할 경우, 상기 조성된 환경에서 요구되는 전략을 유도하기 위 한 환경 설정을 변경할 수 있다."}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "일 실시예에 따른 전략 유도 시스템은 인공 지능에 기반하여 사용자와 컴퓨터가 상호작용하는 학습 상황에 탑재 되어 사용자의 퍼포먼스, 과거 행동, 현재의 의도를 고려하여 컴퓨터에 설정된 목적을 달성하도록 환경을 제어 할 수 있다. 일 실시예에 따른 전략 유도 시스템은 특정 환경(예를 들면, 온라인 게임, 성인용 슬롯 머신, 대규모 군사 작전 등)에 적용하여, 사용자의 퍼포먼스, 과거 행동, 의도에 따라 환경을 제어하여 게임 내 상황과 목적에 적합하게 사용자의 게임 전략과 퍼포먼스를 유도하도록 제어할 수 있다."}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 실시예를 첨부한 도면을 참조하여 상세히 설명한다. 도 1은 일 실시예에 따른 전략 유도 시스템의 개괄적인 동작을 설명하기 위한 도면이다. 전략 유도 시스템은 환경 및 환경의 일부를 관측하며, 환경과 인터랙션(상호작용)하는 사 용자, 사용자와 환경의 상호작용 상태를 관측하며 환경을 제어하는 환경 제어 에이전트로 구성될 수 있다. 이때, 환경 제어 에이전트는 추정된 사용자의 강화학습 은닉신호(보상예측에러, 상태예측에러)를 보상치로 정량화하여 최적의 환경 제어 전략을 학습할 수 있다. 이때, 전략 유도 시스템은 예를 들면, 사용자 와 상호작용하는 각 컴퓨터 자체에서 동작할 수 있으며, 별도의 서버 시스템 형태로 동작할 수도 있다. 전략 유도 시스템은 사용자와 환경 제어 에이전트가 상호작용하는 환경에서, 환경 제어 에 이전트가 환경을 학습하는 사용자의 경험 자체를 제어한다는 순환 논리로 동작하는 인공지능 기반의 전략 유도 시스템을 의미할 수 있다. 전략 유도 시스템은 사용자 경험의 최적화를 위한 인공지능 기반의 전략 유도를 수행할 수 있다. 사용자 는 환경, 환경 관찰 정보를 관측할 수 있다. 환경 제어 에이전트는 환경의 모든 상태를 관측할 수 있다. 이때, 환경 제어 에이전트가 사용자의 상태로 표현되는 사용자의 경험을 원하는 시나리오대로 제어할 수 있다. 사용자와 환경 제어 에이전트가 상호작용하는 상태에 따라 환경 제어 에이전트가 환경을 제어하 여 사용자에게 직접적인 개입을 하지 않고도 사용자의 경험이 유도될 수 있다. 이에 따라, 전략 유도 시스템 은 환경 제어 에이전트를 통해 환경을 학습하는 사용자의 경험을 제어할 수 있고, 사용자의 경험을 다양하게 유도할 수 있는 다중 목적을 위한 메커니즘으로, 양방향성 공진화 기술을 제공할 수 있다. 사용자는 환경으로부터 직접적인 개입을 받지 않았음에도 환경 제어 에이전트의 제어로 인하여 변경된 환경과 상호작용을 지속할 수 있다. 사용자는 태스크(Task)를 수행하기 위하여 환경과 상호작용을 하게 되며, 환경 제어 에이전트에 의하여 설정된 환경 하에서, 목표를 성취하기 위한 행동을 수행하면서 환경 내에서 성취해야 할 목적을 지속적으로 추구할 수 있다. 사용자는 환경이 요구하는 행동을 수 행함으로써 환경에 의해 행동 전략이 도출될 수 있다. 환경 제어 에이전트는 사용자와 환경의 상호작용을 사용자의 학습 상태, 전략 메커니즘 상태 및 지속적인 환경 상태 변화 측면에서 관찰하면서, 사용자의 만족도와 성취도에 따라 현재 환경에서 요구되는 전략을 유도할 수 있도록 환경 상태를 변경할 수 있다. 환경 제어 에이전트에 의하여 환경이 변경되면, 사용자는 변경된 환경 상태에서도 지속적으로 목표를 달성하기 위하여 현재 상황에서 가장 적합한 행동을 취할 수 있다. 이때, 사용자가 현재 상황에서 액션 을 취하는 행동은 환경 제어 에이전트에 의하여 의도된 전략일 수 있다. 이러한 행동은 환경 제어 에이전 트가 유도하려고 한 전략인지 관찰을 통하여 확인될 수 있다. 환경 제어 에이전트는 사용자의 행동이 유도된 전략이 아닐 경우, 더욱 강하게 유도할 수 있는 환경 상태 를 설정하거나, 우회적으로 유도할 수 있는 환경 상태를 설정할 수 있다. 또는, 사용자의 행동이 유도된 전략일 경우, 환경 제어 에이전트가 목표로 하는 상태를 극대화할 수 있는 방향으로 환경 상태를 변경할 수 있다. 일반적으로, 사용자는 사용자의 만족도를 극대화하는 목표를 가지고 있고, 환경 제어 에이전트는 다 중 목적을 가지고 행동할 수 있다. 예를 들면, 환경 제어 에이전트는 사용자의 특정 행동을 극대화하도록 환경을 변화시킬 수 있고, 특정 전략을 소거하기 위한 환경을 설정할 수 있다. 또는, 다중 목표를 동시에 성취 하도록 환경을 변화시킬 수 있다. 환경 제어 에이전트는 이러한 다양한 행동들 중에서 특정 타입의 행동들은 강화시키고, 동시에 다른 행동들을 소거하는 목적을 가지고 환경을 설정할 수 있다. 이러한 과정에서 사용자는 시시각각 변하는 환경에서 만족도를 최적화할 수 있고, 최고의 행동(전략)을 발 전시켜 나갈 수 있으며, 동시에 환경 제어 에이전트는 특정 행동을 유도하는 환경을 설정하기 위한 최고의 행동을 발전시켜 나갈 수 있으므로 인간과 인공지능 간의 공진화를 실현시킬 수 있다. 특정 시간 t에서, 환경 제어 에이전트는 정책 에 따라 환경을 조작하는 행동을 수행하여 새로운 환경 상태 를 생성할 수 있다. 사용자는 정책에 따라 조성된 환경에 행동 을 수행하여 새로운 환경 상태 를 생성하고, 그에 따른 보상 을 부여받을 수 있다. 환경 제어 에이전트는 사용자가 행동을 수행한 후에 보상 을 받을 수 있다. 이때, 환경 제어 에 이전트가 획득하는 보상(치) 이(가) 사용자의 상태에 의존하며, 환경 제어 에이전트가 환경 제어 에이전트의 보상 을 극대화하는 과정에서 기 설정된 시나리오대로 사용자의 학습 상태가 유도될 수 있다. 이러한 상호작용들은 t+1에서도 반복될 수 있다. 이와 같이, 전략 유도 시스템은 뇌 기반 의사 결정 시스템의 메커니즘에 기반한다. 최근 신경과학 연구에 따르 면 인간의 판단은 Model-Free 및 Model-Based 강화학습의 혼합이며, 인간은 보상 예측 에러(Reward Prediction Error, RPE)와 상태 예측 에러(State Prediction Error, SPE)를 고려하여 판단을 내린다고 알려져 있다. Model-Free 강화학습은 수많은 경험을 통해 행동(전략)을 학습하는 것으로, 어떤 행동을 취했을 때 그에 따르는 보상을 고려하여 학습하는 것을 의미한다. 여기서, 발생하는 신호가 보상 예측 에러(Reward Prediction Error, RPE)이다. 보상이 가장 큰 행동을 지향하게 되어 습관적인 행동을 결정하는데 이는 중요한 역할을 한다. Model-Free 강화학습에 의해 학습된 행동(전략)은 동일한 상황이 발생하였을 때 아주 빠르게 대처할 수 있으나, 상황이 급격하게 변했을 때 습관적인 행동(전략)이 더 이상 유효하지 않기 때문에 적절히 대처하기 어렵다. Model-based 강화학습은 일단 에이전트가 처한 환경이 어떠한 지 적어도 한 번 이상의 시도를 통해 빨리 습득한 후 현재 환경에서 가장 빨리 보상을 최대화할 수 있는 전략을 수립하여 행동하는 목표 지향적 방법을 의미한다. 여기서 발생하는 신호가 상태 예측 에러(State Prediction Error, SPE)이다. 다소 많은 인지 부하를 요구하는 인간 뇌 내부 학습 전략이지만, 빠르게 변화하는 환경에서 적은 시간 동안 강인하게 대처할 수 있는 효과적인 학습 전략이다. 전략 유도 시스템은 사용자의 판단이 이전에 경험한 보상 예측 에러, 상태 예측 에러를 통해 결정된다는 점에 착안하여 사용자의 경험을 원하는 방향으로 최적화하기 위해 컴퓨터가 환경을 조작하여 다양한 목적에 부합하는 방향으로 예측 에러를 유도할 수 있다. 인공지능의 목적 함수는 현재 환경에서 유도하고자 하는 사용자의 행동에 부합하는 예측 에러에 근접시키는 것 을 의미할 수 있다. 인공지능의 목적 함수가 아래와 같이 정의될 수 있다. 사용자의 의사 결정은 강화 학습의 과정을 따른다. 이 과정은 일반적으로 아래 Bellman's principle of optimality로 정의된다."}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "상기 정리에 따르면, 사용자의 의사 결정은 가장 높은 가치(Q*(s, a) 부분)을 획득할 수 있는 행동을 선택하는 경향을 보인다. 이러한 가치는 현재 인간이 받을 수 있는 보상에 비례한다. 이 보상을 극대화 하기 위해서는 인간은 크게 두 가지 정보를 정확하게 파악하고 있어야 한다. 현재 첫째는 보 상에 대한 정보 (reward (“TD target”) 부분)는 보상의 최대치는 무엇인지, 보상을 유발하는 행동/결정이 무 엇인지 그 관계를 파악하는데 주요한 역할을 한다. 둘째로 현재 사용자가 처한 환경의 상태(state 부분)는 최 대 보상을 획득하기 위해서 환경을 어떻게 탐색하여 목표를 성취할 지에 대한 중요한 정보를 제공한다. 이에 따라 보상을 예측하고, 현재 환경 상태를 예측하는 것은 사용자의 의사 결정에 있어 매우 중요한 역할을 차지한다. 상기 두 예측에 대한 신호는 뇌 과학에서도 발견된다. 보상 예측 에러(RPE)는 model-free RL에 공 헌하는 신호로써 도파민 시스템에서 발견되고 있고, 상태 예측 에러(SPE)는 Lateral prefrontal cortex에서 발 견되고 있다. 실시예에서는 사용자의 보상 예측 에러, 상태 예측 에러를 조절하는 것을 목적 함수로 사용할 수 있다. 다시 말해, 사용자가 현재 상호작용하는 (게임) 환경을 조작함으로써 현재 사용자의 보상 예측 에러, 상태 예측 에러 를 유도하는 것을 목적으로 한다. 보상 예측 에러를 정의하는 모델은 아래와 같이 나타낼 수 있다. 수학식 1:"}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 s, s'은 각각 현재 상태와 다음 상태, a, a'은 각각 현재 상태에서의 사용자 행동과 다음 상태에서의 행 동을 의미한다. r(s')은 다음 상태 s'에서 사용자가 획득한 보상의 크기이고, 는 discounted factor로써 사용자의 행동에 따른 보상이 시간차를 두고 올 때 시간차에 따라 보상의 (체감) 크기가 감소하는 것을 결정한 다. 와 는 수학식 1에서 밝힌 가치 정보로써, 각각 다음 상태 s' 에서 행 동 a'을 했을 때 획득할 수 있는 가치 값과 현재 상태 s에서 행동 a를 했을 때 획득할 수 있는 가치 값을 말한 다. 이에 따라 보상 예측 에러에 대한 목적 함수는 아래와 같이 정의될 수 있다. ·인간 사용자의 model-free RL을 활성화하려면 → 보상 예측 에러 극소화 →"}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "·인간 사용자의 model-free RL을 억제하려면 → 보상 예측 에러 극대화 →"}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "또한, 상태 예측 에러를 정의하는 모델은 아래와 같이 나타낼 수 있다."}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서 T(s, a, s') 는 상태 천이 행렬로써 상태 천이 확률을 포함하고 있다. 다시 말하면 사용자가 상태 s에 서 행동 a를 했을 때 다음 상태 s'으로 갈 수 있는 확률을 의미한다. 상태 예측 에러에 대한 목적 함수는 아래 와 같이 정의될 수 있다. ·인간 사용자의 Model-based RL을 활성화하려면 → 상태 예측 에러 극소화 →"}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "·인간 사용자의 Model-based RL을 억제하려면 → 상태 예측 에러 극대화 →"}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "보상 예측 에러, 상태 예측 에러는 모두 예측 에러로써, 기 설정된 기준값 이하의 작은 값을 가지는 것이 의사 결정의 정확도가 더 높은 것을 의미한다. 기 설정된 기준값 이상의 큰 값을 가질수록 전략에 대한 의사 결정 정확도가 떨어지는 것을 말한다. 예를 들어 상태 예측 에러의 경우, 상태 예측 에러값이 0인 것은 모든 전략에 대한 의사 결정이 항상 정확하게 맞는 것을 의미하며, 반대로 상태 예측 에러값이 1인 경우는 어떤 의사 결정을 내리더라도 그 결정이 항상 틀린 것을 의미한다. 전략 유도 시스템은 사용자-컴퓨터 상호작용 상황에서 사용자의 상태, 행동 전략, 목표를 확인하기 위해서 인간 뇌 기저핵에서 관찰되는 신호 및 특징들을 사용하고, 컴퓨터가 사용자의 특정 행동/전략을 유도하기 위한 기준 으로 사용자로부터 관찰되는 동일한 신호 및 특징들을 사용하여 사용자 경험 자체를 제어하여 전략을 유도할 수 있다. 다시 말해서, 전략 유도 시스템은 사용자가 게임 환경에 있을 때, 환경을 조작함으로써 사용자의 전략을 유도할 수 있고, 결국 사용자의 보상 예측 에러 및 상태 예측 에러를 포함하는 예측 에러를 뇌 레벨에서 제어할 수 있다. 도 2는 일 실시예에 따른 전략 유도 시스템의 구성을 설명하기 위한 블록도이고, 도 3은 일 실시예에 따른 전략 유도 시스템의 게임 전략 유도 방법을 설명하기 위한 흐름도이다. 전략 유도 시스템의 프로세서는 환경 조성부 및 전략 제어부를 포함할 수 있다. 이러한 구성요 소들은 전략 유도 시스템에 저장된 프로그램 코드가 제공하는 제어 명령에 따라 프로세서에 의해 수행되는 서로 다른 기능들(different functions)의 표현들일 수 있다. 구성요소들은 도 3의 전략 유도 방법이 포함하는 단계들(310 내지 320)을 수행하도록 전략 유도 시스템을 제어할 수 있다. 이때, 구성요소들은 메모리가 포함하는 운영체제의 코드와 적어도 하나의 프로그램의 코드에 따른 명령(instruction)을 실행하도록 구현될 수 있다. 프로세서는 전략 유도 방법을 위한 프로그램의 파일에 저장된 프로그램 코드를 메모리에 로딩할 수 있다. 예를 들면, 전략 유도 시스템에서 프로그램이 실행되면, 프로세서는 운영체제의 제어에 따라 프로그램의 파일로 부터 프로그램 코드를 메모리에 로딩하도록 전략 유도 시스템을 제어할 수 있다. 이때, 프로세서 및 프로세서 가 포함하는 환경 조성부 및 전략 제어부 각각은 메모리에 로딩된 프로그램 코드 중 대응하는 부분의명령을 실행하여 이후 단계들(310 내지 320)을 실행하기 위한 프로세서의 서로 다른 기능적 표현들일 수 있다. 단계에서 환경 조성부는 사용자와 상호작용하는 환경을 조성할 수 있다. 단계에서 전략 제어부는 조성된 환경에서 사용자의 의사 결정이 학습됨에 따라 조성된 환경을 변화시 켜 기 설정된 다중 목적에 기반한 사용자의 전략을 제어할 수 있다. 이때, 사용자의 의사 결정은 사용자에 의 해 액션이 수행된 행동을 포함할 수 있다. 도 4를 참고하면, 사용자의 전략을 제어하기 위한 방법을 설명하기 위한 흐름도이다. 전략 유도 시스템은 환경 제어 에이전트에서 설정된 환경 설정에 따른 전략을 수행할 수 있 다. 전략 유도 시스템은 환경 제어 에이전트의 환경 설정에 기초하여 환경이 조성될 수 있다. 조성된 환 경의 환경 상태를 사용자가 감지할 경우, 조성된 환경에서 사용자가 특정 태스크를 수행하기 위해 목표 성 취 전략을 수행할 수 있다. 사용자는 상기 환경에서 목표를 성취하기 위한 행동을 수행하면서 환경 내에서 성 취해야 할 목적을 지속적으로 추구할 수 있다. 이와 같이, 사용자의 여러 가지 행동은 환경의 상태를 변화시킬 수 있다. 전략 유도 시스템은 환경의 변화 여부를 판단할 수 있다. 사용자가 목표를 성취하기 위한 행동 (전략)을 수행할 수 있다. 환경 변화가 감지될 경우, 사용자가 목표를 성취하기 위한 행동을 수행함에 따라 사용자의 예측 에러(PE)를 도 출할 수 있다. 이때, 전략 유도 시스템은 사용자의 예측 에러가 환경 제어 에이전트의 목적 함수를 만족 하는지 여부를 판단할 수 있다. 사용자는 환경이 변경되면, 변경된 환경에서도 지속적으로 목표를 달성하 기 위해, 현재 상황에서 가장 적합한 행동을 취할 수 있다. 이때, 사용자의 행동이 환경 제어 에이전트가 유도 하려고 한 전략인지 환경을 통하여 관찰할 수 있다. 만약, 사용자의 예측 에러가 환경 제어 에이전트의 목적 함수를 만족하지 못할 경우, 환경 제어 에이전트는 사용자의 예측 에러 상태 및 환경 상태를 확인하고, 현재 환 경에서 요구되는 전략을 사용자로부터 유도될 수 있도록 환경 상태를 변경시킬 수 있다. 예를 들면, 환경 제어 에이전트는 현재 환경에서 요구되는 전략이 사용자로부터 유도되지 않았으면, 더욱 강하게 유도될 수 있도록 환 경 상태를 설정하거나, 우회적으로 유도할 수 있는 환경 상태를 설정할 수 있고, 현재 환경에서 요구되는 전략 이 사용자로부터 유도되었다면, 기 설정된 목표로 하는 상태를 극대화할 수 있는 방향으로 환경 상태를 변경시 킬 수 있다. 또한, 환경 변화가 감지되지 않음에 따라 사용자의 의사 결정이 목적 함수를 만족하는지 여부를 판단하고, 사용 자의 의사 결정이 목적 함수를 만족하면, 프로세스를 종료하고, 사용자의 의사 결정이 상기 목적 함수를 만족시 키지 못할 경우, 조성된 환경에서 요구되는 전략을 유도하기 위한 환경 설정을 변경할 수 있다. 도 5는 일 실시예에 따른 전략 유도 시스템에서 보상 예측 에러, 상태 예측 에러 각각을 따로 조작하는 시나리 오에 따른 보상 예측 에러값, 상태 예측 에러값의 변화 추이를 나타낸 그래프이고, 도 6은 일 실시예에 따른 전 략 유도 시스템에서 보상 예측 에러, 상태 예측 에러 각각을 동시에 조작하는 시나리오에 따른 보상 예측 에러 값, 상태 예측 에러값의 변화 추이를 나타낸 그래프이다. 일례로, 전략 유도 시스템에서 사용자와 환경 제어 에이전트가 상호작용하는 환경으로 2단계 마르코 프 결정 환경이 설정될 수 있다. 예를 들면, 해당 환경은 (Daw et al., 2011)에서 사용된 순차적 결정 과정을 기반으로 하고, 한 에피소드는 사용자가 200번의 행동을 수행함에 따라 완료될 수 있다. 사용자의 입장에서, 해당 환경은 총 9개의 관측 가능한 상태와 행할 수 있는 두 가지 행동을 제공할 수 있 다. 이때, 환경은 사용자의 행동을 통해 전이 확률에 따라 다른 상태로 변화될 수 있다. 사용자는 두 번의 행 동 후에 보상을 획득할 수 있다. 환경 제어 에이전트의 입장에서, 해당 환경은 4가지 항목으로 이루어진 환경 상태를 제공할 수 있다. 예 를 들면, 사용자에게 보여지는 상태, 사용자에게 주어진 보상, 사용자에게 주어진 보상 중 기본값 이외에 환경 제어 에이전트에 의해 추가로 더 주어진 값 및 사용자에게 보여지는 상태 값의 전이 확률 등의 항목을 제공할 수 있다. 또한, 환경 제어 에이전트는 해당 환경을 제어하는 6가지 행동을 수행할 수 있다. 예를 들면, 사용자에게 주어지는 보상 중 추가로 주어지는 값을 증감하는 행동, 사용자에게 보상이 주어지는 상태와 그렇지 않는 상태 간의 연결구조를 바꾸는 행동, 사용자에게 보여지는 상태 간의 전이 확률을 결정론적 혹은 확률적으로 바꾸는 행동 및 환경 제어를 하지 않는 행동으로 구성될 수 있다. 이에, 총 8가지의 시나리오에 대하여 사용자와 환경 제어 에이전트의 상호작용이 진행될 수 있다. 도 5를 참고하면, 상태 예측 에러와 보상 예측 에러를 동시에 극대화하거나 극소화하는 경우, 상태 예측 에러는극대화 하면서 보상 예측 에러는 극소화 하는 경우, 상태 예측 에러는 극소화 하면서 보상 예측 에러는 극대화 하는 경우 등의 4가지 종류로 구성할 수 있다. 각 시나리오에서 상태 예측 에러와 보상 예측 에러의 추이는 도 5 및 도 6에 도시된 바와 같이 의도된 방향으로 변화함을 확인할 수 있다. 환경 제어 에이전트를 통하여 사용자(예를 들면, 가상의 인간 에이전트)의 학 습 상태를 원하는 방향으로 제어할 수 있음을 확인할 수 있다. 이에 따라, 환경을 학습하는 사용자의 경험 자체를 컴퓨터가 제어하는 환경 제어 프레임워크를 제안하고, 그 타 당성을 시뮬레이션 실험을 통하여 검증할 수 있다. 이러한 프레임워크가 게임, 학습 등의 다방면에 적용되어 사용자와 협업하며 공진화하는 차세대 인공지능 패러다임으로서 응용될 수 있다. 도 7은 일 실시예에 따른 전략 유도 시스템에서 조성된 게임 환경에서 사용자의 전략을 제어하기 위한 동작을 설명하기 위한 도면이다. 인공지능 기반의 게임 전략 유도 시스템은 사용자와 상호작용하는 모든 환경에 적용할 수 있다. 예 를 들면, 온라인 게임, 성인용 슬롯 머신, 대규모 군사 작전, 온라인 학습, 의료정보 시스템, 비상 매뉴얼, 판 례학습 시스템 등의 환경에 적용할 수 있고, 이외에도 다양한 환경에 적용시킬 수 있다. 도 7에서는 온라인 게 임, 카지노 게임의 환경을 예를 들어 설명하기로 한다. 온라인 게임 환경에서 사용자와 상호작용하는 게임 엔 진을 다루는 도메인에 적용될 수 있다. 일례로, 사용자는 현재 처해진 환경을 탐색 및 관찰하고 목표를 성취하기 위하여 의사 결정을 내릴 수 있 고, 의사 결정에 따른 행동을 수행할 수 있다. 온라인 게임의 경우, 해당 퀘스트(Quest)를 성취하기 위하여 아 이템을 취득하거나 상대방을 공격하여 점수와 아이템을 획득할 수 있다. 또는, 카지노 게임의 경우, 더 많은 보상(Reward)를 받도록 패턴을 맞출 수 있다. 환경 제어 에이전트는 사용자가 목표에 따라 의사 결정 후 수행하는 행동, 행동으로 인하여 변경된 환경 상태를 파악하고 사용자의 목표 성취를 위한 행동 예측 에러를 조작할 수 있도록 환경을 변경시킬 수 있다. 예를 들면, 온라인 게임의 경우, 사용자는 퀘스트를 성취하기 위해 아이템을 취득하거나 상대방을 공격 하여 점수와 아이템을 획득하려고 할 것이다. 이러한 행동으로 인하여 환경이 변화되면, 환경 제어 에이전트 는 사용자가 획득한 점수와 환경 변화 상태를 확인할 수 있고, 사용자의 다음 행동을 유도하도 록 환경을 변경시킬 수 있다. 예를 들면, 게임 환경에서 사용자가 빠져나갈 수 없도록, 사용자가 예측한 것보다 약간 더 좋은 결과를 가지도록 환경을 제어하여 계속적으로 목표를 수행하도록 게임을 유지하게 할 수 있고, 사용자의 예측이 항상 틀리도록 환경을 제어하여 사용자가 게임에 흥미를 느끼지 못하게 하고 이탈하도록 제어할 수도 있다. 또는, 사용자들의 성향에 따라 모험을 즐기는 부류에는 사용자의 예측 에러를 극대화시키는 방향으로 환경을 변화시켜서 게임에서 얻는 재미를 극대화할 수 있다. 또는, 스트레스를 덜 받는 환경에서 게 임을 즐기는 것을 좋아하는 부류의 사용자에게 사용자의 예측 에러를 극소화시키는 방향으로 환경을 변화시켜서 게임에서 획득되는 재미를 극대화시키고 게임을 계속 사용하도록 유도할 수도 있다. 또한, 카지노 게임의 경우, 사용자가 더 많은 보상을 받도록 사용자는 게임의 패턴을 더 정확하게 맞추려는 행 동을 수행할 수 있다. 사용자는 게임에 더 많은 금액을 투자하고, 게임에서 이탈하지 못하게 하려면, 환 경 제어 에이전트는 사용자가 예측한 상태를 기준으로 사용자가 예측한 상태보다 더 나은 상태로 조작하여 사용자의 예측보다 약간 더 나은 보상을 받을 수 있도록 환경을 제어할 수 있다. 이 경우, 사용자는 사용 자의 결정이 나쁘지 않다고 여기게 되어, 게임을 하는 행위를 계속 지속하고, 결과적으로 게임으로부터 이탈하 지 못하도록 할 수 있다. 또는, 사용자를 즉시 게임에서 이탈시키기 위하여 환경 제어 에이전트는 사용자가 예측한 상태보다 훨씬 더 좋지 않은 환경 상태로 조작하여, 사용자의 예측 에러보다 더 높은 예 측 에러를 갖도록 유도할 수 있다. 이 경우, 사용자는 사용자의 예측보다 더 좋지 않은 상황을 직면하게 되기 때문에 스트레스에 강하고 도전을 좋아하는 사용자라면, 이러한 상황을 극복할 더 나은 전략을 세워 끝없 이 도전하거나, 아니면 게임을 플레이하는 것을 포기하고 게임을 그만둘 수도 있다. 환경 제어 에이전트는 사용자의 행동과 그로 인한 환경 변화를 관찰하고, 환경 제어 에이전트의 목표 (예를 들면, 예측 에러 극대화, 극소화 혹은 특정 구간의 예측 에러 범위)에 기반하여 환경을 조작하여 사 용자가 환경에 대응하는 행동을 할 수 있도록 간접적으로 유도할 수 있다. 일 실시예에 따른 게임 전략 유도 시스템은 인공지능 모델 형태로 구현되어, 사용자와 컴퓨터가 상호작용하는 학습 상황에 탑재될 수 있다. 사용자의 퍼포먼스, 과거 행동, 현재 의도를 고려하여, 컴퓨터가 목적을 이루도 록 환경을 제어하는 시스템에 적용 가능하다. 일 실시예에 따른 게임 전략 유도 시스템은 온라인 게임에 적용이 가능하다. 사용자의 퍼포먼스, 과거 행동, 의도에 따라 컴퓨터 환경을 제어하여, 게임 내 상황과 목적에 맞게 인간 사용자의 게임 전략과 퍼포먼스를 유도 하는 서버 시스템에 구현될 가능성도 있다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 예를 들어, 프로 세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴 퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상 의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설"}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치 는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치에 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터 는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2018-0089185", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7"}
{"patent_id": "10-2018-0089185", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 전략 유도 시스템의 개괄적인 동작을 설명하기 위한 도면이다. 도 2는 일 실시예에 따른 전략 유도 시스템의 구성을 설명하기 위한 블록도이다. 도 3은 일 실시예에 따른 전략 유도 시스템의 게임 전략 유도 방법을 설명하기 위한 흐름도이다. 도 4는 일 실시예에 따른 전략 유도 시스템에서 사용자의 전략을 제어하기 위한 방법을 설명하기 위한 흐름도이 다. 도 5는 일 실시예에 따른 전략 유도 시스템에서 보상 예측 에러, 상태 예측 에러 각각을 따로 조작하는 시나리 오에 따른 보상 예측 에러값, 상태 예측 에러값의 변화 추이를 나타낸 그래프이다. 도 6은 일 실시예에 따른 전략 유도 시스템에서 보상 예측 에러, 상태 예측 에러 각각을 동시에 조작하는 시나 리오에 따른 보상 예측 에러값, 상태 예측 에러값의 변화 추이를 나타낸 그래프이다. 도 7은 일 실시예에 따른 전략 유도 시스템에서 조성된 게임 환경에서 사용자의 전략을 제어하기 위한 동작을 설명하기 위한 도면이다."}
