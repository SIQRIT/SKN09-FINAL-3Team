{"patent_id": "10-2023-0005160", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0113074", "출원번호": "10-2023-0005160", "발명의 명칭": "주행 가능 영역 기반으로 이동객체를 추종하는 로봇장치 및 방법", "출원인": "한국로봇융합연구원", "발명자": "곽정훈"}}
{"patent_id": "10-2023-0005160", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "심층학습을 통해 학습된 분류모델에 전방이 촬영된 영상정보를 적용하여 주행이 가능한 영역인 주행 가능 영역과 주행이 불가능한 영역인 주행 불가능 영역을 분류하고, 상기 분류된 결과를 이용하여 상기 주행 가능 영역에대한 지도를 생성하는 영역 인식부;심층학습을 통해 학습된 인식모델에 상기 영상정보를 적용하여 기 설정된 이동객체를 인식하고, 상기 이동객체의 위치를 추정하는 객체 인식부; 및상기 지도에 상기 이동객체의 위치를 매핑하여 상기 이동객체가 주행 가능 영역에 위치하는지 여부를 판단하고,상기 판단된 결과를 기반으로 상기 이동객체의 추종을 제어하는 주행 제어부;를 포함하는 로봇장치."}
{"patent_id": "10-2023-0005160", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 영역 인식부는,상기 영상정보에 포함된 바닥면의 텍스처(texture)를 이용하여 상기 주행 가능 영역과 상기 주행 불가능 영역을분류하는 것을 특징으로 하는 로봇장치."}
{"patent_id": "10-2023-0005160", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서,상기 영역 인식부는,상기 주행 가능 영역 중 텍스처가 변경되는 영역을 상기 지도에 표시하는 것을 특징으로 하는 로봇장치."}
{"patent_id": "10-2023-0005160", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3항에 있어서,상기 주행 제어부는,상기 텍스처가 변경되는 영역으로 주행하는 경우, 현재 주행속도를 상기 변경되는 텍스처에 설정된 속도로 변경하는 것을 특징으로 하는 로봇장치."}
{"patent_id": "10-2023-0005160", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1항에 있어서,상기 객체 인식부는,상기 영상정보에 포함된 이동객체의 신체조건, 헤어스타일, 옷차림 및 액세사리 착용유무 중 적어도 하나를 이용하여 상기 이동객체를 인식하는 것을 특징으로 하는 로봇장치."}
{"patent_id": "10-2023-0005160", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1항에 있어서,상기 주행 제어부는,상기 이동객체가 주행 불가능 영역에 위치하는 경우, 상기 이동객체의 현재 위치에서 가장 가까운 주행 이동 영역으로 이동한 다음 주행을 정지하여 주행 대기 상태를 유지시키고, 상기 주행 대기 상태에서 알림 메시지가 출력되도록 제어하는 것을 특징으로 하는 로봇장치.공개특허 10-2024-0113074-3-청구항 7 제 6항에 있어서,상기 주행 제어부는,상기 주행 대기 상태에서 상기 이동객체가 주행 가능 영역에 다시 위치하는 경우, 상기 이동객체를 추종하는 것을 특징으로 하는 로봇장치."}
{"patent_id": "10-2023-0005160", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "로봇장치가 심층학습을 통해 학습된 분류모델에 전방이 촬영된 영상정보를 적용하여 주행이 가능한 영역인 주행가능 영역과 주행이 불가능한 영역인 주행 불가능 영역을 분류하고, 상기 분류된 결과를 이용하여 상기 주행 가능 영역에 대한 지도를 생성하는 단계;상기 로봇장치가 심층학습을 통해 학습된 인식모델에 상기 영상정보를 적용하여 기 설정된 이동객체를인식하고, 상기 이동객체의 위치를 추정하는 단계; 및상기 로봇장치가 상기 지도에 상기 이동객체의 위치를 매핑하여 상기 이동객체가 주행 가능 영역에 위치하는지여부를 판단하고, 상기 판단된 결과를 기반으로 상기 이동객체를 추종하는 단계;를 포함하는 이동객체 추종방법."}
{"patent_id": "10-2023-0005160", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "주행 가능 영역 기반으로 이동객체를 추종하는 로봇장치 및 방법이 개시된다. 로봇장치는 심층학습을 통해 학습 된 분류모델에 전방이 촬영된 영상정보를 적용하여 주행이 가능한 영역인 주행 가능 영역과 주행이 불가능한 영 역인 주행 불가능 영역을 분류하고, 분류된 결과를 이용하여 주행 가능 영역에 대한 지도를 생성하는 영역 인식 부, 심층학습을 통해 학습된 인식모델에 영상정보를 적용하여 기 설정된 이동객체를 인식하고, 이동객체의 위치 를 추정하는 객체 인식부 및 지도에 이동객체의 위치를 매핑하여 이동객체가 주행 가능 영역에 위치하는지 여부 를 판단하고, 판단된 결과를 기반으로 이동객체의 추종을 제어하는 주행 제어부를 포함한다."}
{"patent_id": "10-2023-0005160", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 이동객체를 추종하는 기술에 관한 것으로, 더욱 상세하게는 영상정보에서 주행 가능 영역을 인식하고, 인식된 주행 가능 영역으로 이동객체를 추종하는 주행 가능 영역 기반으로 이동객체를 추종하는 로봇 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0005160", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일반적으로 로봇이 이동객체를 추종하는 과정에서 해당 로봇은 이동객체가 이동한 경로를 따라 추종을 한다. 하 지만 이동객체를 추종하는 과정에서 로봇은 모든 영역을 주행하지 못하는 문제점이 있다. 예를 들어 이동객체가 로봇이 넘어가지 못하는 배수로를 넘어갈 경우 로봇은 이동객체를 계속 추종하여 고장이 발생되거나, 이동불능 상태가 될 수 있다. 따라서 보다 안정적으로 이동객체를 추종할 수 있는 연구가 필요한 실정이다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허공보 제10-1486308호(2015.01.20.)"}
{"patent_id": "10-2023-0005160", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는, 실시간으로 촬영된 영상정보를 기반으로 주행 가능 영역을 인식하고, 인식 된 주행 가능 영역으로 이동객체를 추종하는 주행 가능 영역 기반으로 이동객체를 추종하는 로봇장치 및 방법을 제공하는 것이다."}
{"patent_id": "10-2023-0005160", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 과제를 해결하기 위해 본 발명에 따른 로봇장치는 심층학습을 통해 학습된 분류모델에 전방이 촬영된 영상 정보를 적용하여 주행이 가능한 영역인 주행 가능 영역과 주행이 불가능한 영역인 주행 불가능 영역을분류하고, 상기 분류된 결과를 이용하여 상기 주행 가능 영역에 대한 지도를 생성하는 영역 인식부, 심층학습을 통해 학습된 인식모델에 상기 영상정보를 적용하여 기 설정된 이동객체를 인식하고, 상기 이동객체의 위치를 추 정하는 객체 인식부 및 상기 지도에 상기 이동객체의 위치를 매핑하여 상기 이동객체가 주행 가능 영역에 위치 하는지 여부를 판단하고, 상기 판단된 결과를 기반으로 상기 이동객체의 추종을 제어하는 주행 제어부를 포함한 다. 또한 상기 영역 인식부는, 상기 영상정보에 포함된 바닥면의 텍스처(texture)를 이용하여 상기 주행 가능 영역 과 상기 주행 불가능 영역을 분류하는 것을 특징으로 한다. 또한 상기 영역 인식부는, 상기 주행 가능 영역 중 텍스처가 변경되는 영역을 상기 지도에 표시하는 것을 특징 으로 한다. 또한 상기 주행 제어부는, 상기 텍스처가 변경되는 영역으로 주행하는 경우, 현재 주행속도를 상기 변경되는 텍 스처에 설정된 속도로 변경하는 것을 특징으로 한다. 또한 상기 객체 인식부는, 상기 영상정보에 포함된 이동객체의 신체조건, 헤어스타일, 옷차림 및 액세사리 착용 유무 중 적어도 하나를 이용하여 상기 이동객체를 인식하는 것을 특징으로 한다. 또한 상기 주행 제어부는, 상기 이동객체가 주행 불가능 영역에 위치하는 경우, 상기 이동객체의 현재 위치에서 가장 가까운 주행 이동 영역으로 이동한 다음 주행을 정지하여 주행 대기 상태를 유지시키고, 상기 주행 대기 상태에서 알림 메시지가 출력되도록 제어하는 것을 특징으로 한다. 또한 상기 주행 제어부는, 상기 주행 대기 상태에서 상기 이동객체가 주행 가능 영역에 다시 위치하는 경우, 상 기 이동객체를 추종하는 것을 특징으로 한다. 본 발명에 따른 이동객체 추종방법은 로봇장치가 심층학습을 통해 학습된 분류모델에 전방이 촬영된 영상정보를 적용하여 주행이 가능한 영역인 주행 가능 영역과 주행이 불가능한 영역인 주행 불가능 영역을 분류하고, 상기 분류된 결과를 이용하여 상기 주행 가능 영역에 대한 지도를 생성하는 단계, 상기 로봇장치가 심층학습을 통해 학습된 인식모델에 상기 영상정보를 적용하여 기 설정된 이동객체를 인식하고, 상기 이동객체의 위치를 추정하 는 단계 및 상기 로봇장치가 상기 지도에 상기 이동객체의 위치를 매핑하여 상기 이동객체가 주행 가능 영역에 위치하는지 여부를 판단하고, 상기 판단된 결과를 기반으로 상기 이동객체를 추종하는 단계를 포함한다."}
{"patent_id": "10-2023-0005160", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예에 따르면, 실시간으로 촬영된 영상정보를 기반으로 주행 가능 영역과 주행 불가능 영역을 구 분하고, 주행 가능 영역을 통해 이동객체를 추종함으로써, 안정적으로 이동객체 추종을 수행할 수 있다. 또한 이동객체가 주행 불가능 영역으로 이동한 경우, 이동객체가 다시 주행 가능 영역으로 이동할 때까지 대기 하여 이동중 고장 또는 이동불능 상태를 미연에 차단할 수 있다."}
{"patent_id": "10-2023-0005160", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위 해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 명세서 및 도면(이하 '본 명세서')에서, 동일한 구성요소에 대해서 중복된 설명은 생략한다. 또한 본 명세서에서, 어떤 구성요소가 다른 구성요소에 '연결되어' 있다거나 '접속되어' 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존 재할 수도 있다고 이해되어야 할 것이다. 반면에 본 명세서에서, 어떤 구성요소가 다른 구성요소에 '직접 연결 되어' 있다거나 '직접 접속되어' 있다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되 어야 할 것이다. 또한, 본 명세서에서 사용되는 용어는 단지 특정한 실시예를 설명하기 위해 사용되는 것으로써, 본 발명을 한정 하려는 의도로 사용되는 것이 아니다. 또한 본 명세서에서, 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 또한 본 명세서에서, '포함하다' 또는 '가지다' 등의 용어는 명세서에 기재된 특징, 숫자, 단계, 동작, 구성요 소, 부품, 또는 이들을 조합한 것이 존재함을 지정하려는 것일 뿐, 하나 또는 그 이상의 다른 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이 해되어야 할 것이다. 또한 본 명세서에서, '및/또는' 이라는 용어는 복수의 기재된 항목들의 조합 또는 복수의 기재된 항목들 중의 어느 항목을 포함한다. 본 명세서에서, 'A 또는 B'는, 'A', 'B', 또는 'A와 B 모두'를 포함할 수 있다. 또한 본 명세서에서, 본 발명의 요지를 흐리게 할 수 있는 공지 기능 및 구성에 대한 상세한 설명은 생략될 것 이다. 도 1은 본 발명의 실시예에 따른 이동객체 추정 시스템을 설명하기 위한 구성도이다. 도 1을 참조하면, 이동객체 추정 시스템은 실시간으로 촬영된 영상정보를 기반으로 주행 가능 영역을 인식 하고, 인식된 주행 가능 영역으로 이동객체를 추종한다. 여기서 이동객체는 이동하는 객체로써, 바람직하게는 사람일 수 있다. 이동객체 추정 시스템은 적어도 하나의 로봇장치 및 사용자 단말을 포함한다. 로봇장치는 이동객체를 추종하는 로봇으로써, 물건을 적재한 상태에서 안정적으로 이동객체를 추종할 수 있다. 즉 로봇장치는 다양한 분야에서 적용될 수 있다. 예를 들면 로봇장치는 택배회사에서 택배 물 건 이동을 도와주기, 백화점에서 상품 이동을 도와주기, 식당에서 음식 서핑을 도와주기 등을 수행할 수 있다. 이를 위해 로봇장치는 전방을 촬영한 영상정보에 포함된 영역 중 로봇장치가 주행할 수 있는 영역인 주행 가능 영역을 인식한다. 이때 로봇장치는 심층학습(Deep Learning)을 통해 학습된 분류모델을 이용하 여 주행 가능 영역을 인식할 수 있다. 로봇장치는 인식된 주행 가능 영역에 대한 지도를 생성한다. 또한 로봇장치는 전방을 촬영한 영상정보에 포함된 이동객체를 인식하고, 이동객체의 위치를 추정한다. 이때 로 봇장치는 심층학습을 통해 학습된 인식모델을 이용하여 이동객체를 인식할 수 있다. 로봇장치는 생성 된 지도에 추정된 이동객체의 위치를 매핑하여 이동객체가 주행 가능 영역에 위치하는지 여부를 판단한다. 로봇 장치는 판단된 결과를 기반으로 이동객체를 추종하고, 추종하는 과정에 대한 모니터링 정보를 생성한다. 로봇장치는 모니터링 정보를 사용자 단말로 전송할 수 있다. 사용자 단말은 사용자(관리자)가 사용하는 단말로써, 적어도 하나의 로봇장치로부터 모니터링 정보를 수신하고, 수신된 모니터링 정보를 분석한다. 예를 들어 로봇장치가 복수개인 경우, 사용자 단말은 복수의 로봇장치로부터 모니터링 정보를 각각 수신하고, 수신된 모니터링 정보를 분석하여 각 로봇장치 의 이동경로, 구동효율, 추종 정확도 등을 산출할 수 있다. 사용자 단말은 분석된 모니터링 정보를 출력한다. 이를 통해 사용자 단말은 사용자가 관리하는 모든 로봇장치의 상태를 직관적으로 파악할수 있도록 지원한다. 한편 이동객체 추정 시스템은 적어도 하나의 로봇장치와 사용자 단말 사이에 통신망을 구 축하여 서로 간의 통신이 이루어지도록 지원한다. 통신망은 백본망과 가입자망으로 구성될 수 있다. 백본 망은 X.25 망, Frame Relay 망, ATM망, MPLS(Multi-Protocol Label Switching) 망 및 GMPLS(Generalized Multi-Protocol Label Switching) 망 등 중에 하나 또는 복수의 통합된 망으로 구성될 수 있다. 가입자망은 FTTH(Fiber To The Home), ADSL(Asymmetric Digital Subscriber Line), 케이블망, 지그비(zigbee), 블루투스 (bluetooth), Wireless LAN(IEEE 802.11b, IEEE 802.11a, IEEE 802.11g, IEEE 802.11n), Wireless Hart(ISO/IEC62591-1), ISA100.11a(ISO/IEC 62734), CoAP(Constrained Application Protocol), MQTT(Message Queuing Telemetry Transport), WIBro(Wireless Broadband), Wimax, 3G, HSDPA(High Speed Downlink Packet Access), 4G, 5G 및 6G 등일 수 있다. 일부 실시예로, 통신망은 인터넷망일 수 있고, 이동 통신망일 수 있 다. 또한 통신망은 기타 널리 공지되었거나 향후 개발될 모든 무선통신 또는 유선통신 방식을 포함할 수 있다. 도 2는 본 발명의 실시예에 따른 로봇장치를 설명하기 위한 블록도이고, 도 3은 본 발명의 실시예에 따른 주행 가능 영역 인식하는 과정을 설명하기 위한 도면이며, 도 4는 본 발명의 실시예에 따른 이동객체 인식하는 과정 을 설명하기 위한 도면이고, 도 5는 본 발명의 실시예에 따른 로봇장치가 주행 가능 영역으로 이동하는 이동객 체를 추종하는 과정을 설명하기 위한 도면이며, 도 6은 본 발명의 실시예에 따른 로봇장치가 주행 불가능 영역 으로 이동하는 이동객체를 추종하는 과정을 설명하기 위한 도면이다. 도 1 내지 도 6을 참조하면, 로봇장치는 본체, 통신부, 카메라부, 센서부, 제어부, 입출력부 및 저장부를 포함한다. 본체는 로봇장치가 주행을 하기 위한 프레임으로써, 이송수단(미도시), 적재수단(미도시)을 포함한다. 이송수단은 로봇장치가 이동을 하기 위한 수단으로써, 복수의 바퀴, 바퀴를 회전시키는 모터 등을 포함한 다. 적재수단은 물건을 적재하는 공간으로써, 바닥면이 평판 구조를 이루고 테두리에 가이드가 구비되어 적재된 물건이 이탈되지 않도록 형성될 수 있다. 통신부는 본체에 구비되고, 사용자 단말과의 통신을 수행한다. 통신부는 이동객체의 추종에 대한 모니터링 정보를 사용자 단말로 전송한다. 이때 통신부는 기 설정된 주기마다 모니터링 정보를 전송하거나, 실시간으로 모니터링 정보를 전송할 수 있다. 카메라부는 본체에 구비되고, 적어도 하나의 카메라를 통해 전방에 대한 영상을 촬영하여 영상정보를 생성한다. 이때 카메라는 주행 중 발생되는 흔들림을 보정해주는 기능을 포함할 수 있으며, 이로 인해 흔들림이 최소화된 영상정보를 생성할 수 있다. 여기서 영상정보는 이동하는 이동객체가 포함된 영상일 수 있다. 또한 영 상정보는 로봇장치가 이동하는 바닥면이 포함된 영상일 수 있다. 센서부는 본체에 구비되고, 전방에 위치한 이동객체, 장애물 등에 대한 거리를 측정하여 측정정보를 생 성한다. 이를 위해 센서부는 거리센서, 라이다(LiDar) 등을 포함할 수 있다. 제어부는 본체에 구비되고, 로봇장치의 전반적인 구동을 제어한다. 제어부는 영역 인식부 , 객체 인식부 및 주행 제어부를 포함하고, 학습부를 더 포함할 수 있다. 학습부는 주행 가능 영역과 주행 불가능 영역을 분류하는 분류모델과 이동객체를 인식하는 인식모델에 대한 심층학습(deep learning)을 각각 수행한다. 여기서 분류모델과 인식모델 각각은 인코더(EN) 및 디코더(DE)를 포 함할 수 있으나, 이에 한정하지 않고 다양한 인공지능 기술이 적용될 수 있다. 인코더 및 디코더를 포함하는 분 류모델과 인식모델은 가중치가 적용되는 복수의 연산을 수행하는 복수의 계층을 포함한다. 복수의 계층은 컨볼 루션(Convolution) 연산을 수행하는 컨볼루션 계층(Convolution layer), 다운 샘플링(Down sampling) 연산을 수행하는 풀링 계층(Pooling layer), 업 샘플링(Up sampling) 연산을 수행하는 언풀링 계층(Unpooling layer) 및 디컨볼루션(Deconvolution) 연산을 수행하는 디컨볼루션 계층(Deconvolution layer) 각각을 하나 이상 포함 한다. 이때 컨볼루션, 다운 샘플링, 업 샘플링 및 디컨볼루션 연산 각각은 소정의 행렬로 이루어진 필터(커널) 을 이용하며, 이러한 행렬의 원소의 값들이 가중치가 된다. 분류모델은 학습용 영상정보가 입력되면 입력된 학습용 영상정보에 대해 복수의 계층 간 가중치가 적용되는 복 수의 연산을 수행하여 학습용 영상정보에 포함된 바닥면의 텍스처(texture)와 기 설정된 텍스처를 비교하여 해당 바닥면에 대한 텍스처 분류값을 출력한다. 학습부는 텍스처 분류값이 학습용 영상정보에 대응하는 바닥 면의 텍스처 정답값(미리 알고 있음)의 차이가 가까워지도록 학습을 수행한다. 학습부는 이러한 학습을 기 설정된 횟수만큼 반복 수행할 수 있으며, 기 설정된 횟수는 사용자가 원하는 평가 지표의 정확도에 도달할 때까 지의 횟수를 의미한다. 인식모델은 학습용 영상정보가 입력되면 입력된 학습용 영상정보에 대해 복수의 계층 간 가중치가 적용되는 복 수의 연산을 수행하여 학습용 영상정보에 포함된 이동객체와 기 설정된 이동객체를 비교하여 해당 이동객체에 대한 객체 식별값을 출력한다. 학습부는 객체 식별값이 학습용 영상정보에 대응하는 이동객체의 객체 정답 값(미리 알고 있음)의 차이가 가까워지도록 학습을 수행한다. 학습부는 이러한 학습을 기 설정된 횟수만큼 반복 수행할 수 있으며, 기 설정된 횟수는 사용자가 원하는 평가 지표의 정확도에 도달할 때까지의 횟수를 의미 한다. 영상 인식부는 학습부로부터 기 학습된 분류모델에 카메라부로부터 생성된 영상정보를 적용하여 주 행이 가능한 영역인 주행 가능 영역과 주행이 불가능한 영역인 주행 불가능 영역을 분류한다. 이때 영상 인식부 는 영상정보에 포함된 바닥면의 텍스처를 이용하여 주행 가능 영역과 주행 불가능 영역을 분류한다. 예를 들어 영상 인식부는 영상정보(도 3의 (b))가 입력되면 주행 가능 영역에서 사용되는 바닥면의 텍스처 들(도 3의 (a))에 대한 학습이 완료된 분류모델(도 3의 (c))를 이용하여 영상정보에 포함된 바닥면의 텍스처를 분류한다. 즉 분류모델은 바닥면의 텍스처가 기 설정된 텍스처들 중 어느 하나와 동일 질감을 가지는지 분류한 다. 이때 분류모델은 동일 질감이 가지는 텍스처별로 분류하여 현재 도로가 어떤 형태의 도로인지 구분할 수 있 도록 지원할 수 있다. 영상 인식부는 영상정보에 기 설정된 텍스처와 동일 질감을 가지는 텍스처를 주행 가 능 영역(P)로 인식할 수 있다(도 3의 (d)). 영상 인식부는 분류된 결과를 이용하여 주행 가능 영역에 대한 지도를 생성한다. 이때 영상 인식부는 주행 가능 영역을 탑뷰(top view) 형태로 변환하여 지도를 생성할 수 있다. 또한 영상 인식부는 주행 가능 영역 중 텍스처가 변경되는 영역을 지도에 별도로 표시하여 도로 형태가 변화되었음을 나타낼 수 있다. 예를 들어 로봇장치가 각 구역별로 서로 다른 바닥면으로 구분된 택배회사 사업장에서 택배 물건 이동하는 경우, 영상 인식부는 주행 가능 영역에서 바닥면의 텍스처가 변경되는 부분을 인식하고, 인식된 부분에 대 해 지도에 표시하여 주행 속도, 주행 패턴 등과 같은 변화를 주기 위한 정보를 제공할 수 있다. 객체 인식부는 학습부로부터 기 학습된 인식모델에 카메라부로부터 생성된 영상정보를 적용하여 기 설정된 이동객체를 인식한다. 이때 객체 인식부는 영상정보에 포함된 이동객체의 후방 영상을 기반으로 객 체 인식을 수행해야 함으로, 이동객체의 신체조건(키, 체격 등), 헤어스타일, 옷차림 및 액세서라 착용유무 중 적어도 하나를 이용하여 이동객체를 인식할 수 있다. 예를 들어 객체 인식부는 영상정보(도 4의 (b))가 입력되면 이동객체(도 4의 (a)) 인식에 대한 학습이 완료 된 인식모델(도 4의 (c))를 이용하여 영상정보에 포함된 이동객체를 인식한다. 즉 인식모델은 이동객체의 후방 에 대한 특징을 감지하여 이동객체를 인식할 수 있다(도 4의 (d)). 이때 인식모델은 이동객체를 박스 형태(B)로 인식할 수 있다 객체 인식부는 인식된 결과를 이용하여 이동객체의 위치를 추정한다. 이때 객체 인식부는 이동객체와 로봇장치의 상대적인 위치를 추정할 수 있다. 이를 위해 객체 인식부는 영상정보 및 센서부로부터 측정된 측정정보를 이용하여 이동객체와의 위치를 산출하고, 산출된 위치를 기반으로 상대적인 위치를 추정할 수 있다. 주행 제어부는 영역 인식부로부터 생성된 지도에 객체 인식부로부터 인식된 이동객체의 위치를 매 핑하여 이동객체가 주행 가능 영역에 위치하는지 여부를 판단한다. 주행 제어부는 판단된 결과를 기반으로 이동객체의 추종을 제어한다. 주행 제어부는 이동객체의 추종을 제어하는 과정에 대한 모니터링 정보를 생 성한다. 주행 제어부는 생성된 모니터링 정보를 사용자 단말로 전송되도록 제어한다. 상세하게는 이동객체가 주행 가능 영역으로 이동하는 경우(도 5), 주행 제어부는 본체가 이동객체 의 이동경로를 따라 추종하도록 주행을 제어한다. 또한 이동객체가 주행 불가능 영역에 위치하는 경 우(도 6), 주행 제어부는 본체가 이동객체의 현재 위치에서 가장 가까운 주행 이동 영역으로 이동 되도록 제어한다. 주행 제어부는 본체가 해당 영역으로 이동되면 주행을 정지시키고, 정지된 상태에서 알림 메시지가 출력되도록 제어한다. 여기서 알림 메시지는 로봇장치의 현재 상태를 알려주는 메시지로써, 이동객체가 주행 불가능 영역에 위치함을 알리는 동시에 로봇장치의 현재 상태가 주행 대기 상태에있음을 알리는 메시지일 수 있다. 주행 제어부는 주행 대기 상태에서 이동객체가 주행 가능 영역에 다 시 위치하는 경우, 이동객체를 다시 추종하도록 주행을 제어한다. 여기서 주행 제어부는 지도에서 텍스처가 변경되는 영역으로 주행하는 경우, 현재 주행속도를 기 설정된 속 도로 변경할 수 있다. 여기서 기 설정된 속도는 변경되는 텍스처에 설정된 속도일 수 있다. 예를 들어 주행 제 어부는 주행하는 바닥면이 제1 텍스처에서 제2 텍스처로 변경되는 경우, 제1 텍스처에 설정된 제1 속도에서 제2 텍스처에 설정된 제2 속도로 속도 변경을 할 수 있다. 입출력부는 본체에 구비되고, 사용자 입력을 입력받고, 알림 메시지를 출력한다. 입출력부는 이동 객체와 관련된 사용자 입력을 입력받는다. 즉 사용자 입력은 로봇장치가 추종하려는 이동객체를 설정하는 입력일 수 있다. 입출력부는 로봇장치의 현재 상태를 알려주는 메시지를 시각적/청각적으로 출력할 수 있다. 바람직하게는 입출력부는 터치스크린 기능을 포함할 수 있다. 저장부는 본체에 구비되고, 로봇장치가 구동하기 위한 프로그램 또는 알고리즘이 저장된다. 저장 부는 카메라로부터 생성된 영상정보와, 센서부로부터 생성된 측정정보가 저장된다. 저장부는 기 학습된 분류모델과, 인식모델이 저장되고, 이동객체를 추종하는 과정에서 생성된 정보가 저장된다. 저장부 는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 미디어 카드 마이크로 타 입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(Random Access Memory, RAM), SRAM(Static Random Access Memory), 롬(Read-Only Memory, ROM), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기메모리, 자기 디스크 및 광디스크 중 적어도 하나의 저장매체를 포함할 수 있다. 도 7은 본 발명의 실시예에 따른 이동객체 추종방법을 설명하기 위한 순서도이다. 도 1 및 도 7을 참조하면, 이동객체 추종방법은 실시간으로 촬영된 영상정보를 기반으로 주행 가능 영역과 주행 불가능 영역을 구분하고, 주행 가능 영역을 통해 이동객체를 추종함으로써, 안정적으로 이동객체 추종을 수행할 수 있다. S110 단계에서, 로봇장치는 영상을 촬영한다. 로봇장치는 전방에 대한 영상을 촬영하여 영상정보를 생성한다. 이때 로봇장치는 바닥면과 적어도 하나의 이동객체가 포함되도록 영상을 촬영할 수 있다. S120 단계에서, 로봇장치는 추종할 이동객체를 설정한다. 로봇장치는 생성된 영상정보에서 추종할 이 동객체를 설정할 수 있다. 예를 들어 복수의 이동객체가 영상정보에 포함된 경우, 로봇장치는 사용자로부 터 추종할 이동객체를 선택하는 사용자 입력을 입력받고, 입력된 사용자 입력에 따라 추종할 이동객체를 설정할 수 있다. S130 단계에서, 로봇장치는 주행 가능 영역을 인식한다. 로봇장치는 심층학습을 통해 학습된 분류모 델에 영상정보를 적용하여 주행이 가능한 영역인 주행 가능 영역과 주행이 불가능한 영역인 주행 불가능 영역을 분류하고, 분류된 결과를 이용하여 주행 가능 영역을 인식한다. 이때 로봇장치는 영상정보에 포함된 바닥 면의 텍스처를 이용하여 주행 가능 영역과 주행 불가능 영역을 분류할 수 있다. S140 단계에서, 로봇장치는 주행 가능 영역에 대한 지도를 생성한다. 로봇장치는 주행 가능 영역을 탑뷰 형태로 변환하여 지도를 생성할 수 있다. 이때 로봇장치는 주행 가능 영역 중 텍스처가 변경되는 영 역을 지도에 별도로 표시하여 도로 형태가 변화되었음을 나타낼 수 있다. S150 단계에서, 로봇장치는 이동객체를 인식한다. 로봇장치는 심층학습을 통해 학습된 인식모델에 영 상정보를 적용하여 기 설정된 이동객체를 인식한다. 로봇장치는 영상정보에 포함된 이동객체의 신체조건, 헤어스타일, 옷차림 및 액세사리 착용유무 중 적어도 하나를 이용하여 이동객체를 인식할 수 있다. S160 단계에서, 로봇장치는 이동객체의 위치를 추정한다. 로봇장치는 이동객체와 로봇장치의 상 대적인 위치를 추정할 수 있다. 이를 위해 로봇장치는 영상정보 및 측정정보를 이용하여 이동객체와의 위 치를 산출하고, 산출된 위치를 기반으로 상대적인 위치를 추정할 수 있다. S170 단계에서, 로봇장치는 이동객체를 추종한다. 로봇장치는 생성된 지도에 인식된 이동객체의 위치 를 매핑하여 이동객체가 주행 가능 영역에 위치하는지 여부를 판단한다. 로봇장치는 판단된 결과를 기반으 로 이동객체를 추종하면서 주행한다. 도 8은 도 7의 S170 단계를 상세하게 설명하기 위한 순서도이다. 도 7 및 도 8을 참조하면, 이동객체 추종방법은 이동객체가 주행 불가능 영역으로 이동한 경우, 이동객체가 다 시 주행 가능 영역으로 이동할 때까지 대기하여 이동중 고장 또는 이동불능 상태를 미연에 차단할 수 있다. S210 단계에서, 로봇장치는 이동객체가 주행 가능 영역으로 이동하는지 여부를 판단한다. 로봇장치는 이동객체가 주행 가능 영역으로 이동하면 S220 단계를 수행하고, 이동객체가 주행 가능 영역으로 이동하지 않으 면 S230 단계를 수행한다. S220 단계에서, 로봇장치는 이동객체를 추종한다. 로봇장치는 이동객체의 이동경로를 따라 추종하도 록 주행을 제어한다. S230 단계에서, 로봇장치는 주행 대기 상태로 변환한다. 로봇장치는 이동객체의 현재 위치에서 가장 가까운 주행 이동 영역으로 이동되도록 제어한다. 로봇장치는 해당 영역으로 이동되면 주행을 정지시키고, 정지된 상태에서 알림 메시지가 출력되도록 제어한다. 여기서 알림 메시지는 로봇장치의 현재 상태를 알려 주는 메시지로써, 이동객체가 주행 불가능 영역에 위치함을 알리는 동시에 로봇장치의 현재 상태가 주행 대기 상태에 있음을 알리는 메시지일 수 있다. S240 단계에서, 로봇장치는 이동객체의 추종이 종료되었는지 판단한다. 로봇장치는 이동객체의 추종 이 종료되면 시스템을 종료하고, 이동객체의 추종이 종료되지 않으면 S210 단계를 재수행한다. 도 9는 본 발명의 실시예에 따른 컴퓨팅 장치를 설명하기 위한 블록도이다. 도 9를 참조하면, 컴퓨팅 장치(TN100)는 본 명세서에서 기술된 장치(예를 들면 로봇장치, 사용자 단말 등) 일 수 있다. 컴퓨팅 장치(TN100)는 적어도 하나의 프로세서(TN110), 송수신 장치(TN120), 및 메모리(TN130)를 포함할 수 있 다. 또한, 컴퓨팅 장치(TN100)는 저장 장치(TN140), 입력 인터페이스 장치(TN150), 출력 인터페이스 장치 (TN160) 등을 더 포함할 수 있다. 컴퓨팅 장치(TN100)에 포함된 구성 요소들은 버스(bus)(TN170)에 의해 연결 되어 서로 통신을 수행할 수 있다. 프로세서(TN110)는 메모리(TN130) 및 저장 장치(TN140) 중에서 적어도 하나에 저장된 프로그램 명령(program command)을 실행할 수 있다. 프로세서(TN110)는 중앙 처리 장치(CPU: central processing unit), 그래픽 처리 장치(GPU: graphics processing unit), 또는 본 발명의 실시예에 따른 방법들이 수행되는 전용의 프로세서를 의 미할 수 있다. 프로세서(TN110)는 본 발명의 실시예와 관련하여 기술된 절차, 기능, 및 방법 등을 구현하도록 구성될 수 있다. 프로세서(TN110)는 컴퓨팅 장치(TN100)의 각 구성 요소를 제어할 수 있다. 메모리(TN130) 및 저장 장치(TN140) 각각은 프로세서(TN110)의 동작과 관련된 다양한 정보를 저장할 수 있다. 메모리(TN130) 및 저장 장치(TN140) 각각은 휘발성 저장 매체 및 비휘발성 저장 매체 중에서 적어도 하나로 구 성될 수 있다. 예를 들어, 메모리(TN130)는 읽기 전용 메모리(ROM: read only memory) 및 랜덤 액세스 메모리 (RAM: random access memory) 중에서 적어도 하나로 구성될 수 있다. 송수신 장치(TN120)는 유선 신호 또는 무선 신호를 송신 또는 수신할 수 있다. 송수신 장치(TN120)는 네트워크 에 연결되어 통신을 수행할 수 있다. 한편, 본 발명의 실시예는 지금까지 설명한 장치 및/또는 방법을 통해서만 구현되는 것은 아니며, 본 발명의 실 시예의 구성에 대응하는 기능을 실현하는 프로그램 또는 그 프로그램이 기록된 기록 매체를 통해 구현될 수도 있으며, 이러한 구현은 상술한 실시예의 기재로부터 본 발명이 속하는 기술 분야의 통상의 기술자라면 쉽게 구 현할 수 있는 것이다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 통상의 기술자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속하는 것이다."}
{"patent_id": "10-2023-0005160", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 이동객체 추정 시스템을 설명하기 위한 구성도이다. 도 2는 본 발명의 실시예에 따른 로봇장치를 설명하기 위한 블록도이다. 도 3은 본 발명의 실시예에 따른 주행 가능 영역 인식하는 과정을 설명하기 위한 도면이다. 도 4는 본 발명의 실시예에 따른 이동객체 인식하는 과정을 설명하기 위한 도면이다. 도 5는 본 발명의 실시예에 따른 로봇장치가 주행 가능 영역으로 이동하는 이동객체를 추종하는 과정을 설명하 기 위한 도면이다. 도 6은 본 발명의 실시예에 따른 로봇장치가 주행 불가능 영역으로 이동하는 이동객체를 추종하는 과정을 설명 하기 위한 도면이다. 도 7은 본 발명의 실시예에 따른 이동객체 추종방법을 설명하기 위한 순서도이다. 도 8은 도 7의 S170 단계를 상세하게 설명하기 위한 순서도이다. 도 9는 본 발명의 실시예에 따른 컴퓨팅 장치를 설명하기 위한 블록도이다."}
