{"patent_id": "10-2023-0025288", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0070367", "출원번호": "10-2023-0025288", "발명의 명칭": "카메라를 포함하는 X선 영상 장치 및 그 동작 방법", "출원인": "삼성전자주식회사", "발명자": "권재현"}}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "대상체의 움직임을 검출하는 X선 영상 장치(100)에 있어서, X선을 발생하고, 상기 대상체에 X선을 조사하는 X선 조사부(120); 상기 X선 조사부(120)에 의해 조사되고, 상기 대상체를 투과하는 X선을 검출하는 X선 디텍터(130); 상기 X선 디텍터(130) 앞에 포지셔닝된 대상체를 촬영함으로써 대상체 이미지를 획득하는 카메라(110); 디스플레이부(172); 및 인공지능 모델을 이용하여 상기 대상체 이미지를 분석하여, 상기 대상체 이미지로부터 상기 대상체의 움직임을검출하고, 상기 대상체의 움직임 검출 결과를 사용자에게 알리는 알림 신호를 상기 디스플레이부(172) 상에 출력하는 적어도 하나의 프로세서(140); 를 포함하는, X선 영상 장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 적어도 하나의 프로세서(140)는,상기 카메라(110)를 이용하여, 상기 X선 디텍터(130) 앞에 포지셔닝 완료된 상기 대상체를 촬영하여 기준 이미지(reference image)를 획득하고, 상기 기준 이미지를 획득한 이후 상기 대상체를 후속 촬영하여 이미지 프레임을 획득하고, 상기 인공지능 모델을 이용하는 분석을 통해 상기 기준 이미지로부터 인식된 상기 대상체와 상기 이미지 프레임으로부터 인식된 상기 대상체를 비교함으로써, 상기 대상체의 움직임을 검출하는, X선 영상 장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 적어도 하나의 프로세서(140)는, 상기 인공지능 모델 중 자기 조직화 지도(Self-Organizing Map)를 이용하여 상기 기준 이미지로부터 인식된 상기 대상체를 나타내는 픽셀들에 대한 가중치(weights)를 획득하고, 상기 가중치를 이용하여 상기 이미지 프레임으로부터 인식된 상기 대상체와 상기 기준 이미지로부터 인식된 상기 대상체를 비교하여 상기 대상체의 움직임을 검출하고, 검출 결과를 이용하여, 상기 기준 이미지 및 상기 가중치를 업데이트하는, X선 영상 장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2 항에 있어서,상기 적어도 하나의 프로세서(140)는, 상기 인공지능 모델 중 학습된 심층 신경망 모델(trained deep neural network)을 이용하는 추론(inferencing)을 통해 상기 기준 이미지로부터 상기 대상체의 주요 부위(landmark)에 관한 복수의 제1 키 포인트를 추출하공개특허 10-2024-0070367-3-고,상기 추출된 복수의 제1 키 포인트를 상기 이미지 프레임으로부터 추출된 상기 대상체의 복수의 제2 키 포인트와 비교하여 키 포인트들 간의 차이값을 산출하고, 상기 산출된 차이값을 기 설정된 임계치와 비교하여, 상기 대상체의 움직임을 검출하는, X선 영상 장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서,상기 심층 신경망 모델은, 기 획득된 복수의 이미지를 입력 데이터로 적용하고, 신체 주요 부위의 키 포인트들의 위치 좌표값들을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식을 통해 학습된(trained) 모델인, X선 영상장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항 내지 제5 항 중 어느 하나의 항에 있어서,환자 포지셔닝 완료 후 움직임 검출 모드를 선택하는 사용자 입력을 수신하는 사용자 입력 인터페이스(160);를 더 포함하고, 상기 적어도 하나의 프로세서(140)는, 상기 수신된 사용자 입력에 기초하여 상기 움직임 검출 모드를 실행하고, 상기 움직임 검출 모드가 실행됨에 응답하여 상기 대상체의 움직임을 검출하는, X선 영상 장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항 내지 제5 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(140)는, 환자 포지셔닝 완료 후 기 설정된 시간이 경과함에 따라 움직임 검출 모드를 실행하고, 상기 움직임 검출 모드가 실행됨에 응답하여 상기 대상체의 움직임을 검출하는, X선 영상 장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항 내지 제7 항 중 어느 하나의 항에 있어서,스테레오 카메라(stereo-type camera), ToF 카메라(time of flight camera), 및 레이저 거리 측정기 중 적어도하나를 포함하는 깊이 측정 장치(180);를 더 포함하고, 상기 적어도 하나의 프로세서(140)는,상기 깊이 측정 장치(180)를 이용하여 상기 X선 조사부(120)와 상기 대상체 간의 거리를 측정함으로써 환자 포지셔닝을 검출하는, X선 영상 장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "공개특허 10-2024-0070367-4-제1 항 내지 제8 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(140)는, 상기 대상체와 상기 X선 조사부(120) 간의 거리인 SID(Source to Image Distance), 상기 대상체의 크기, 형태,및 촬영 프로토콜 중 적어도 하나에 기초하여 움직임 검출 민감도를 설정하는, X선 영상 장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항 내지 제9 항 중 어느 하나의 항에 있어서,상기 디스플레이부(172)는 상기 대상체의 움직임을 나타내는 기 설정된 컬러를 갖는 그래픽 UI(graphic userinterface)를 디스플레이하는, X선 영상 장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1 항 내지 제9 항 중 어느 하나의 항에 있어서,상기 대상체의 움직임에 관한 정보를 상기 사용자에게 알리는 음성 및 알림음 중 적어도 하나의 음향 신호를 출력하는 스피커(174);를 더 포함하는, X선 영상 장치(100)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "X선 영상 장치(100)의 동작 방법에 있어서, 카메라(110)를 이용하여 대상체를 촬영함으로써 상기 대상체의 이미지 데이터를 획득하는 단계(S610); 인공지능 모델을 이용하여 상기 이미지 데이터를 분석하여, 상기 이미지 데이터로부터 상기 대상체의 움직임을검출하는 단계(S620); 및상기 대상체의 움직임 검출 결과를 사용자에게 알리는 알림 신호를 출력하는 단계(S630); 를 포함하는, 방법."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12 항에 있어서,상기 이미지 데이터를 획득하는 단계(S610)는, 상기 카메라(110)를 이용하여, X선 디텍터(130) 앞에 포지셔닝 완료된 상기 대상체를 촬영하여 기준 이미지(reference image)를 획득하는 단계; 및 상기 기준 이미지를 획득한 이후 상기 대상체를 후속 촬영하여 이미지 프레임을 획득하는 단계;를 포함하고, 상기 대상체의 움직임을 검출하는 단계는, 상기 인공지능 모델을 이용하는 분석을 통해 상기 기준 이미지로부터 인식된 상기 대상체와 상기 이미지 프레임으로부터 인식된 상기 대상체를 비교함으로써, 상기 대상체의 움직임을 검출하는, 방법."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "공개특허 10-2024-0070367-5-제13 항에 있어서,상기 대상체의 움직임을 검출하는 단계(S620)는, 자기 조직화 지도(Self-Organizing Map)를 이용하여 상기 기준 이미지로부터 가중치(weights)를 획득하는 단계(810); 상기 가중치를 이용하여 상기 이미지 프레임으로부터 인식된 상기 대상체와 상기 기준 이미지로부터 인식된 상기 대상체를 비교하여 상기 대상체의 움직임을 검출하는 단계(S820); 및검출 결과를 이용하여, 상기 기준 이미지 및 상기 가중치를 업데이트하는 단계(S830);를 포함하는, 방법."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13 항에 있어서,상기 대상체의 움직임을 검출하는 단계(S620)는, 학습된 심층 신경망 모델(trained deep neural network)을 이용하는 추론(inferencing)을 통해 상기 기준 이미지로부터 상기 대상체의 주요 부위(landmark)에 관한 복수의 제1 키 포인트를 추출하는 단계(S910); 상기 추출된 복수의 제1 키 포인트를 상기 이미지 프레임으로부터 추출된 상기 대상체의 복수의 제2 키 포인트와 비교하여 키 포인트들 간의 차이값을 산출하는 단계(S920); 및 상기 산출된 차이값을 기 설정된 임계치와 비교하여, 상기 대상체의 움직임을 검출하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제12 항 내지 제15 항 중 어느 하나의 항에 있어서,환자 포지셔닝 완료 후 움직임 검출 모드를 선택하는 사용자 입력을 수신하는 단계;를 더 포함하고, 상기 대상체의 움직임을 검출하는 단계(S620)는, 상기 사용자 입력에 기초하여 상기 움직임 검출 모드를 실행하는 단계; 및 상기 움직임 검출 모드가 실행됨에 응답하여 상기 대상체의 움직임을 검출하는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제12 항 내지 제15 항 중 어느 하나의 항에 있어서,환자 포지셔닝 완료 후 기 설정된 시간이 경과함에 따라 움직임 검출 모드를 실행하는 단계;를 더 포함하고, 상기 대상체의 움직임을 검출하는 단계(S620)는, 상기 움직임 검출 모드가 실행됨에 응답하여 상기 대상체의 움직임을 검출하는, 방법."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "공개특허 10-2024-0070367-6-제12 항 내지 제17 항 중 어느 하나의 항에 있어서,상기 대상체와 X선 조사부(120) 간의 거리인 SID(Source to Image Distance), 상기 대상체의 크기, 형태, 및촬영 프로토콜 중 적어도 하나에 기초하여 움직임 검출 민감도를 설정하는 단계;를 더 포함하는, 방법."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제12 항 내지 제18 항 중 어느 하나의 항에 있어서,상기 알림 신호를 출력하는 단계(S630)는,상기 대상체의 움직임을 나타내는 기 설정된 컬러를 갖는 그래픽 UI(graphic user interface)를디스플레이하는, 방법."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제12 항 내지 제19 항 중 어느 하나의 항에 있어서,상기 알림 신호를 출력하는 단계(S630)는,상기 대상체의 움직임에 관한 정보를 상기 사용자에게 알리는 음성 및 알림음 중 적어도 하나의 음향 신호를 출력하는, 방법."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "스티칭 X선 촬영을 수행하는 X선 영상 장치(300)에 있어서, X선 조사부(320)에 의해 조사되고, 상기 대상체를 투과하는 X선을 검출하는 X선 디텍터(330); 상기 X선 디텍터(330) 앞에 포지셔닝된 대상체를 촬영함으로써 대상체 이미지를 획득하는 카메라(310); 디스플레이부(370); 및상기 획득된 대상체 이미지를 학습된(trained) 인공지능 모델에 입력하고, 상기 인공지능 모델을 이용하는 추론(inferencing)을 통해 상기 대상체의 스티칭 X선 촬영을 위한 복수의 분할 촬영 영역을 획득하고, 상기 복수의분할 촬영 영역 각각의 상단, 하단, 및 좌우의 경계면을 나타내는 복수의 가이드라인을 상기 디스플레이부(370)상에 디스플레이하는 적어도 하나의 프로세서(340); 를 포함하는, X선 영상 장치(300)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제21 항에 있어서, 상기 인공지능 모델은, 기 획득된 복수의 이미지를 입력 데이터로 적용하고, 촬영 프로토콜에 따라 스티칭된 분할 촬영 영역을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식으로 학습된(trained) 심층 신경망 모델(deep neural network)인, X선 영상 장치(300)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제21 항 또는 제22 항에 있어서,공개특허 10-2024-0070367-7-상기 적어도 하나의 프로세서(340)는, 촬영 프로토콜에 기초하여 상기 대상체 이미지로부터 촬영 대상 부위를 인식하고, 상기 인식된 촬영 대상 부위에 관한 정보를 상기 대상체 이미지와 함께 상기 인공지능 모델에 입력하고, 상기 인공지능 모델을 이용하는 추론을 통해 상기 복수의 분할 촬영 영역을 획득하는, X선 영상 장치(300)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제21 항 내지 제23 항 중 어느 하나의 항에 있어서, 상기 적어도 하나의 프로세서(340)는, 상기 복수의 분할 촬영 영역의 크기를 상기 X선 디텍터(330)의 크기 보다 작은 크기로 조절하는, X선 영상 장치(300)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제21 항 내지 제24 항 중 어느 하나의 항에 있어서, 상기 복수의 가이드라인 중 적어도 하나의 위치를 조정(adjust)하는 사용자 입력을 수신하는 사용자 입력 인터페이스(360); 를 더 포함하고, 상기 적어도 하나의 프로세서(340)는, 상기 수신된 사용자 입력에 기초하여 상기 복수의 가이드라인 중 적어도 하나의 위치를 조정함으로써, 상기 복수의 분할 촬영 영역의 위치, 크기, 및 형태 중 적어도 하나를 변경하는, X선 영상 장치(300)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제21 항 내지 제25 항 중 어느 하나의 항에 있어서, 상기 적어도 하나의 프로세서(340)는, 사용자 입력에 의해 설정된 마진(margin) 정보에 기초하여 상기 복수의 분할 촬영 영역의 상하좌우 마진 크기를결정하는, X선 영상 장치(300)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제21 항 내지 제26 항 중 어느 하나의 항에 있어서, 상기 적어도 하나의 프로세서(340)는, 상기 대상체 이미지 상에 상기 복수의 분할 촬영 영역을 나타내는 그래픽 사용자 UI를 오버레이하여 디스플레이하도록 상기 디스플레이부(370)를 제어하는, X선 영상 장치(300)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "제21 항 내지 제27 항 중 어느 하나의 항에 있어서, 상기 복수의 분할 촬영 영역의 설정 완료에 관한 정보를 음성 또는 알림음으로 출력하는 스피커(speaker);를 더 포함하는, X선 영상 장치(300). 공개특허 10-2024-0070367-8-청구항 29 제21 항 내지 제28 항 중 어느 하나의 항에 있어서, 스테레오 타입 카메라(stereo-type camera), ToF(time of flight) 카메라, 및 레이저 거리 측정기 중 적어도하나를 포함하는 깊이 측정 장치(380);를 더 포함하고, 상기 적어도 하나의 프로세서(340)는, 상기 깊이값 측정 장치(380)를 이용하여 상기 X선 조사부(320)와 상기 대상체 간의 거리를 측정함으로써 상기 X선 디텍터(330) 앞의 대상체 포지셔닝을 검출하는, X선 영상 장치(300)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_30", "content": "제29 항에 있어서, 상기 적어도 하나의 프로세서(340)는, 상기 깊이값 측정 장치(380)를 이용하여 측정된 상기 X선 조사부(320)와 상기 대상체 간의 거리에 기초하여 상기 대상체의 움직임을 검출하는, X선 영상 장치(300)."}
{"patent_id": "10-2023-0025288", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_31", "content": "제21 항 내지 제30 항 중 어느 하나의 항에 있어서, 상기 적어도 하나의 프로세서(340)는, 상기 복수의 분할 촬영 영역을 X선 촬영함으로써 복수의 X선 분할 촬영 이미지를 획득하고, 상기 획득된 복수의X선 분할 촬영 이미지를 스티칭함으로써 X선 촬영 대상 영역에 관한 X선 이미지를 획득하는, X선 영상 장치(300)."}
{"patent_id": "10-2023-0025288", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "카메라를 포함하는 X선 영상 장치 및 그 동작 방법을 제공한다. 본 개시의 일 실시예에 따른 X선 영상 장치는 카 메라를 이용하여 대상체를 촬영함으로써 대상체 이미지를 획득하고, 인공지능 모델을 이용하여 대상체 이미지를 분석함으로써 대상체의 움직임을 검출하고, 대상체의 움직임 검출 결과를 사용자에게 알리는 알림 신호를 출력할 수 있다. 본 개시의 일 실시예에 따른 X선 영상 장치는 카메라를 이용하여 대상체를 촬영함으로써 대상체 이미지 를 획득하고, 인공지능 모델을 이용하여 X선 촬영 전에 스티칭 촬영을 위한 복수의 분할 촬영 영역을 획득하고, 복수의 분할 촬영 영역의 상단, 하단 및 좌우의 경계면을 나타내는 복수의 가이드라인을 디스플레이할 수 있다."}
{"patent_id": "10-2023-0025288", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 카메라를 포함하는 X선 영상 장치 및 그 동작 방법에 관한 것이다. 구체적으로, 본 개시는 카메라를 이용하여 대상체를 촬영함으로써 획득된 이미지를 이용하여 대상체의 움직임을 검출하는 X선 영상 장치에 관한 것이다. 본 개시는 카메라를 통해 촬영된 대상체의 이미지를 이용하여 스티칭 X선 촬영을 수행하는 X선 영상 장 치에 관한 것이다."}
{"patent_id": "10-2023-0025288", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근에는 카메라를 장착하여, 카메라를 통해 획득된 대상체(예를 들어, 환자)의 이미지 데이터를 이용하여 X선 영상 장치의 설정, 이동, 환자 위치 및 상태 확인 등을 자동화하는 X선 영상 장치가 보급되고, 사용되고 있다. 구체적으로, 카메라를 포함하는 X선 영상 장치는 카메라를 통해 획득된 이미지로부터 환자의 위치, 자세 정보, X선 검출 활성 영역(active area), 또는 AEC(automatic exposure control) 챔버의 위치 등을 인식할 수 있다. 카메라를 포함하는 X선 영상 장치는 카메라가 없는 종래의 X선 영상 장치와 비교할 때, 사용자의 조작 시간을 단축할 수 있는 기술적 효과가 있다. 카메라를 포함하는 X선 영상 장치는 대상체(예를 들어, 환자)의 이미지를 이용하여 환자의 움직임을 감지할 수 있다. 종래의 X선 영상 장치는 환자의 특정 신체 부위에 기준 요소(Fiducial Element)를 부착하고, 기준 요소를 부착한 환자를 촬영하여 획득된 이미지로부터 기준 요소의 변위를 분석하는 방식을 통해 환자의 움직임을 검출 하였다. 종래 기술은 환자의 특정 위치에 부착하는 기준 요소가 별도로 구비되지 않으면 동작이 불가능한 기술이며, 연속적인 X선 이미지를 획득하는 과정에서만 환자의 움직임을 검출할 수 있다. 따라서, 종래 기술은 X선 이미지의 촬영 전 미리 환자의 움직임을 검출하여 비정상적인 이미지의 획득을 방지할 수 없는 기술적 한계점이 있다. 최근에서 X선 영상 장치에 인공지능(artificial intelligence, AI) 기술을 도입하여 환자 인식을 더욱 정확하고 신속하게 할 수 있는 기술이 보급되고 있다. 인공지능 시스템은 인간 수준의 지능을 구현하는 컴퓨터 시스템으 로서 기계가 스스로 학습하고 판단하며, 사용할수록 인식률이 향상되는 시스템이다. 인공지능 기술은 입력 데이 터들의 특징을 스스로 분류/학습하는 알고리즘을 이용하는 기계학습 기술 또는 딥 러닝 알고리즘을 활용하여 인 간 두뇌의 인지, 판단 등의 기능을 모사하는 요소 기술들로 구성된다."}
{"patent_id": "10-2023-0025288", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면(aspect)은 대상체의 움직임을 검출하는 X선 영상 장치를 제공한다. 본 개시의 일 실시예에 따른 X선 영상 장치는 X선을 발생하고, 대상체에 X선을 조사하는 X선 조사부, X선 조사부에 의해 조사되고, 대 상체를 투과하는 X선을 검출하는 X선 디텍터, X선 디텍터 앞에 포지셔닝된 대상체를 촬영함으로써 대상체 이미 지를 획득하는 카메라, 디스플레이부, 및 적어도 하나의 프로세서를 포함할 수 있다. 상기 적어도 하나의 프로 세서는 인공지능 모델을 이용하여 상기 대상체 이미지를 분석하여, 대상체 이미지로부터 대상체의 움직임을 검 출할 수 있다. 상기 적어도 하나의 프로세서는 대상체의 움직임 검출 결과를 사용자에게 알리는 알림 신호를 디 스플레이부 상에 출력할 수 있다. 본 개시의 다른 측면(another aspect)은 X선 영상 장치의 동작 방법을 제공한다. 본 개시의 일 실시예에 따른 X 선 영상 장치의 동작 방법은 카메라를 이용하여 대상체를 촬영함으로써 대상체의 이미지 데이터를 획득하는 단 계를 포함할 수 있다. 본 개시의 일 실시예에 따른 X선 영상 장치의 동작 방법은 인공지능 모델을 이용하여 이 미지 데이터를 분석하여, 이미지 데이터로부터 대상체의 움직임을 검출하는 단계를 포함할 수 있다. 본 개시의 일 실시예에 따른 X선 영상 장치의 동작 방법은 대상체의 움직임 검출 결과를 사용자에게 알리는 알림 신호를 출력하는 단계를 포함할 수 있다. 본 개시의 또 다른 측면은 스티칭 X선 촬영을 수행하는 X선 영상 장치를 제공한다. 본 개시의 일 실시예에 따른 X선 영상 장치는 X선 조사부에 의해 조사되고, 대상체를 투과하는 X선을 검출하는 X선 디텍터, X선 디텍터 앞에 포지셔닝된 대상체를 촬영함으로써 대상체 이미지를 획득하는 카메라, 디스플레이부, 및 적어도 하나의 프로세 서를 포함할 수 있다. 상기 적어도 하나의 프로세서는 대상체 이미지를 학습된(trained) 인공지능 모델에 입력 하고, 인공지능 모델을 이용하는 추론(inferencing)을 통해 대상체의 스티칭 X선 촬영을 위한 복수의 분할 촬영 영역을 획득할 수 있다. 상기 적어도 하나의 프로세서는 복수의 분할 촬영 영역 각각의 상단, 하단, 및 좌우의 경계면을 나타내는 복수의 가이드라인을 디스플레이부 상에 디스플레이할 수 있다."}
{"patent_id": "10-2023-0025288", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서의 실시예들에서 사용되는 용어는 본 개시의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 실시예의 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 명세서에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 개시 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 본 명세서에 기재된 \"...부\", \"...모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 본 개시에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for)\", \"~하는 능력을 가지는(having the capacity to)\", \"~하도록 설계된(designed to)\", \"~하도 록 변경된(adapted to)\", \"~하도록 만들어진(made to)\", 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 시스템\"이라는 표현은, 그 시 스템이 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수 행하도록 구성된(또는 설정된) 프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메모리에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 또한, 본 개시에서 일 구성요소가 다른 구성요소와 \"연결된다\" 거나 \"접속된다\" 등으로 언급된 때에는, 상기 일 구성요소가 상기 다른 구성요소와 직접 연결되거나 또는 직접 접속될 수도 있지만, 특별히 반대되는 기재가 존 재하지 않는 이상, 중간에 또 다른 구성요소를 매개하여 연결되거나 또는 접속될 수도 있다고 이해되어야 할 것 이다. 본 개시에서 '대상체(object)'는 촬영의 대상이 되는 것으로서, 사람, 동물, 또는 그 일부를 포함할 수 있다. 예를 들어, 대상체는 환자, 환자 신체의 일부(장기 또는 기관 등; organ) 또는 팬텀(phantom) 등을 포함할 수 있다. 본 개시에서 'X선(X-ray)'이란 0.01 ~ 100 옴스트롬(Å)의 파장을 갖는 전자기파로서, 물체를 투과하는 성질을 가지고 있어서 생체 내부를 촬영하는 의료장비나 일반산업의 비파괴검사장비 등에 일반적으로 널리 사용될 수 있다. 본 개시에서, 'X선 영상 장치'는 X선을 대상체(예를 들어, 환자의 신체)에 투과시켜 대상체의 내부 구조를 X선 이미지로 획득하는 의료 영상 장치이다. X선 장치는 MRI 장치, CT 장치 등을 포함하는 다른 의료 영상 장치에 비해 간편하고, 짧은 시간 내에 대상체의 의료 이미지를 획득할 수 있다는 장점이 있다. 따라서, X선 장치는 단 순 흉부 촬영, 단순 복부 촬영, 단순 골격 촬영, 단순 부비동 촬영, 단순 경부 연조직(neck soft tissue) 촬영 및 유방 촬영 등에 널리 이용되고 있다. 본 개시에서, '이미지' 또는 '대상체 이미지'는 이산적인 이미지 요소들(예를 들어, 2차원 이미지에 있어서의 픽셀들)로 구성된 데이터를 의미할 수 있다. 본 개시에서 '이미지' 또는 '대상체 이미지'는 일반적인 이미지 센 서(예를 들어, CMOS 또는 CCD)를 갖는 카메라를 이용하여 촬영된 이미지를 의미한다. 본 개시에서, '이미지' 또 는 '대상체 이미지'는 대상체를 투과한 X선을 X선 디텍터를 통해 검출한 이후 전기적 신호로 변환하는 이미지 처리를 통해 획득한 'X선 이미지'와는 다른 대상을 지칭한다. 본 개시에서, '인공지능(Artificial Intelligence)'과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로 세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저 장된 기 정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미 한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도 형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 본 개시에서, '인공지능 모델'은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의연산을 통해 신경망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망 모델은 심층 신경 망(DNN: Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), RBM(Restricted Boltzmann Machine), DBN(Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크(Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 이하에서는 도면을 참조하여 본 개시의 실시예들을 상세하게 설명한다. 도 1은 일 실시예에 따른 X선 시스템의 구성을 도시하는 외관도이다. 도 1에서는 룸 X선 촬영 장치(Room DR)를 예로 들어 설명한다. 도 1을 참조하면, X선 시스템은 X선 영상 장치 및 워크스테이션을 포함할 수 있다. X선 영상 장치는 대상체를 촬영하여 대상체 이미지를 획득하는 카메라, X선을 발생시키고, X선을 대상체 에 조사하는 X선 조사부, 대상체를 투과한 X선을 검출하는 X선 디텍터, 및 사용자 입력 인터페이 스를 포함할 수 있다. 도 1에는 X선 영상 장치의 동작을 설명하기 위한 필수적인 구성 요소만이 도시 되었고, 본 개시의 X선 영상 장치의 구성 요소가 도 1에 도시된 바와 같이 한정되는 것은 아니다. 워크스 테이션은 X선 영상 장치와 데이터 통신을 수행하고, 사용자로부터 명령을 입력받고 사용자에게 정보 를 제공할 수 있다. 또한, X선 시스템은 워크스테이션을 통해 입력된 명령에 따라 X선 시스템를 제 어하는 제어부 및 외부 기기와 통신하는 통신 인터페이스를 더 포함할 수 있다. 통신 인터페이스 및 제어부의 구성 요소 중 일부 또는 전부는 워크스테이션에 포함되거나, 또는 워크스테이션 과 별도로 마련될 수 있다. X선 조사부는 X선을 발생시키는 X선 소스 및 X선 소스에서 발생되는 X선의 조사영역을 조절하는 콜리메이 터(collimator)를 구비할 수 있다. X선 시스템이 배치되는 검사실 천장에는 가이드 레일이 설치될 수 있고, 가이드 레일을 따라 이 동하는 이동 캐리지에 X선 조사부를 연결하여 대상체에 대응되는 위치로 X선 조사부를 이동 시킬 수 있고, 이동 캐리지와 X선 조사부는 절첩 가능한 포스트 프레임을 통해 연결되어 X선 조사 부의 높이를 조절할 수 있다. 워크스테이션에는 사용자의 명령을 입력 받는 입력 인터페이스 및 정보를 표시하는 출력 인터페이스 가 마련될 수 있다. 입력 인터페이스는 촬영 프로토콜, 촬영 조건, 촬영 타이밍, X선 조사부의 위치 제어 등을 위한 명령 을 입력 받을 수 있다. 본 개시의 일 실시예에서, 입력 인터페이스는 키보드, 마우스, 터치스크린, 음성 인식기, 등을 포함할 수 있다. 출력 인터페이스는 사용자의 입력을 가이드하기 위한 화면, X선 영상, X선 시스템의 상태를 나타내 는 화면 등을 표시할 수 있다. 본 개시의 일 실시예에서, 출력 인터페이스는 디스플레이를 포함할 수 있다. 제어부는 사용자로부터 입력된 명령에 따라 X선 조사부의 촬영 타이밍, 촬영 조건 등을 제어할 수 있 고, X선 디텍터로부터 수신된 이미지 데이터를 이용하여 X선 이미지를 생성할 수 있다. 또한, 제어부(22 0)는 촬영 프로토콜 및 대상체의 위치에 따라 X선 조사부나 X선 디텍터가 장착된 장착부(14, 2 4)의 위치 또는 자세를 제어할 수도 있다. 제어부는 전술한 동작 및 후술하는 동작을 수행하는 프로그램이 저장된 메모리 및 저장된 프로그램을 실행 하는 프로세서를 포함할 수 있다. 제어부는 단일 프로세서를 포함할 수도 있고, 복수의 프로세서를 포함할 수도 있는바, 후자의 경우에는 복수의 프로세서가 하나의 칩 상에 집적될 수도 있고, 물리적으로 분리될 수도 있다. X선 시스템은 통신 인터페이스를 통해 외부 장치(예를 들어, 외부의 서버, 의료 장치 및 휴대용 단말(예를 들어, 스마트 폰, 태블릿 PC, 웨어러블 기기 등)와 연결되어 데이터를 송신하거나 수신할 수 있다. 통신 인터페이스는 외부 장치와 통신을 가능하게 하는 하나 이상의 구성 요소를 포함할 수 있으며, 예를 들어 근거리 통신 모듈, 유선 통신 모듈 및 무선 통신 모듈 중 적어도 하나를 포함할 수 있다. 또한, 통신 인터페이스가 외부 장치로부터 제어 신호를 수신하고, 수신된 제어 신호를 제어부에 전달 하여 제어부로 하여금 수신된 제어 신호에 따라 X선 시스템을 제어하도록 하는 것도 가능하다. 또한, 제어부는 통신 인터페이스를 통해 외부 장치에 제어 신호를 송신함으로써, 외부 장치를 제어부 의 제어 신호에 따라 제어하는 것도 가능하다. 예를 들어, 외부 장치는 통신 인터페이스를 통해 수 신된 제어부의 제어 신호에 따라 외부 장치의 데이터를 처리할 수 있다. 또한, 통신 인터페이스는 X선 시스템의 구성요소들 간에 통신을 가능하게 하는 내부 통신 모듈을 더 포함할 수도 있다. 외부 장치에는 X선 시스템를 제어할 수 있는 프로그램이 설치될 수 있는 바, 이 프로 그램은 제어부의 동작 중 일부 또는 전부를 수행하는 명령어를 포함할 수 있다. 프로그램은 휴대용 단말에 미리 설치될 수도 있고, 휴대용 단말의 사용자가 애플리케이션 (application)을 제공하는 서버로부터 프로그램을 다운로딩하여 설치하는 것도 가능하다. 애플리케이션을 제공 하는 서버에는 해당 프로그램이 저장된 기록매체가 포함될 수 있다. 한편, X선 디텍터는 스탠드나 테이블에 고정된 고정형 X선 디텍터(130-1)로 구현될 수도 있고, 장 착부(14, 24)에 착탈 가능하게 장착되거나, 또는 임의의 위치에서 사용 가능한 모바일 X선 디텍터(mobile x-ray detector)(130-2) 또는 휴대용 X선 디텍터(portable x-ray detector)로 구현될 수도 있다. 모바일 X선 디텍터 (130-2) 또는 휴대용 X선 디텍터는 데이터 전송 방식과 전원 공급 방식에 따라 유선 타입 또는 무선 타입으로 구현될 수 있다. X선 디텍터는 X선 시스템의 구성 요소로 포함될 수도 있고, 포함되지 않을 수도 있다. 후자의 경우, X선 디텍터는 사용자에 의해 X선 시스템에 등록될 수 있다. 또한, 두 경우 모두 X선 디텍터는 통신 인터페이스를 통해 제어부와 연결되어 제어 신호를 수신하거나 이미지 데이터를 송신할 수 있다. X선 조사부의 일 측면에는 사용자에게 정보를 제공하고 사용자로부터 명령을 입력 받는 사용자 입력 인터 페이스가 마련될 수 있다. 사용자 입력 인터페이스는 워크스테이션의 입력 인터페이스 및 출력 인터페이스가 수행하는 기능 중 일부 또는 전부를 수행하는 서브 유저 인터페이스(sub user interface)일 수 있다. 통신 인터페이스 및 제어부의 구성 요소 중 전부 또는 일부가 워크스테이션과 별도로 마련되는 경우에는 X선 조사부에 마련된 사용자 입력 인터페이스에 포함될 수 있다. 도 1에 도시된 X선 시스템은 검사실의 천장에 연결된 룸 X선 촬영 장치이지만, X선 시스템은 C-암 (arm) 타입 X선 장치, 모바일 X선 장치 등 당업자에게 자명한 범위 내에서 다양한 구조의 X선 장치를 포함할 수 있다. 도 2는 X선 디텍터의 외관도이다. 도 2를 참조하면, X선 디텍터는 모바일 X선 디텍터로 구현될 수 있다. 이 경우, X선 디텍터는 전원을 공급하는 배터리를 포함하여 무선으로 동작할 수도 있고, 도 2에 도시된 바와 같이, 충전 포트가 별도의 전원 공급부와 케이블(C)에 의해 연결되어 동작할 수도 있다. X선 디텍터의 외관을 형성하는 케이스의 내부에는 X선을 검출하여 이미지 데이터로 변환하는 검출 소 자, 이미지 데이터를 일시적 또는 비일시적으로 저장하는 메모리, X선 시스템로부터 제어 신호를 수신하 거나 X선 시스템에 이미지 데이터를 송신하는 통신 모듈과, 배터리가 마련될 수 있다. 또한, 메모리에는 디텍터의 이미지 보정 정보 및 X선 디텍터의 고유의 식별 정보가 저장될 수 있고, X선 시스템와 통 신할 때에 저장된 식별 정보를 함께 전송할 수 있다. 도 3은 본 개시의 일 실시예에 따른 모바일 X선 디텍터를 포함하는 X선 영상 장치를 도시한 도면이다. 도 3을 참조하면, X선 영상 장치는 모바일 X선 디텍터를 포함할 수 있다. 모바일 X선 디텍터는 촬영 장소에 구애받지 않고, X선 촬영을 수행할 수 있도록 이동 또는 휴대가 가능한 형태의 X선 디텍터이다. 도 3에 도시된 X선 영상 장치는 도 1에 도시된 X선 영상 장치의 일 실시예일 수 있다. 도 3에 도시된 X 선 영상 장치에 포함되는 구성 요소들 중 도 1과 동일한 구성 요소는 도 1과 동일한 도면 부호를 사용하고, 중복되는 설명은 생략한다. 도 3에 도시된 X선 영상 장치는, X선 영상 장치의 전반적인 동작을 제어하는 프로세서를 포함하 는 메인부, X선 영상 장치의 이동을 위한 휠이 마련되는 이동부, 테이블, X선을 발생시키 고, 대상체에 조사하는 X선 조사부, X선 조사부에 의해 대상체에 조사되고, 대상체를 투과한 X선을 검출하는 X선 디텍터, 사용자의 입력을 수신하는 사용자 입력 인터페이스, 및 디스플레이부를 포함할 수 있다. 메인부는 X선 영상 장치의 조작을 위한 사용자 인터페이스(User Interface)를 제공하는 조작부를 더 포함할 수 있다. 도 3에서는 조작부가 메인부에 포함되어 있는 것으로 도시되어 있으나, 이에 한정되는 것 은 아니다. 예를 들어, 도 1에서와 같이, X선 시스템의 입력 인터페이스 및 출력 인터페이스는 워크스테이션(200, 도 1 참조)의 일 측면에 마련될 수도 있다. X선 조사부는 X선을 발생시키는 X선 소스 및 X선 소스에 의해 발생되어 조사되는 X선의 경로를 안내하여 X선의 조사 영역을 조절하는 콜리메이터를 포함할 수 있다. 메인부에는 X선 소스에 인 가되는 고전압을 발생시키는 고전압 발생부가 포함될 수 있다. X선 디텍터, 프로세서, 사용자 입력 인터페이스, 및 디스플레이부의 구체적인 기능 및/또 는 동작에 대해서는 도 5에서 상세하게 설명하기로 한다. X선 시스템은 전술한 실링 타입뿐만 아니라 모바일 타입으로도 구현 가능하다. 도 3에서의 X선 디텍터 는 테이블 상에 배치되는 테이블 타입으로 도시되어 있으나, 모바일 타입 또는 휴대용 타입으로써 스 탠드 타입으로도 구현될 수 있음이 자명하다. 도 4는 본 개시의 일 실시예에 따른 X선 영상 장치가 카메라를 통해 획득된 이미지로부터 대상체(1 0)의 움직임을 검출하는 동작을 설명하기 위한 개념도이다. 도 4에서, X선 영상 장치는 실링 타입(ceiling type)으로 도시되었으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예에서, X선 영상 장치는 모바일 타입으로도 구현될 수 있다. 도 4를 참조하면, X선 영상 장치는 카메라, X선 조사부, X선 디텍터, 사용자 입력 인터페 이스, 및 디스플레이부를 포함할 수 있다. 도 4에서는 X선 영상 장치의 기능 및/또는 동작을 설 명하기 위한 최소한의 구성 요소만이 도시되어 있고, X선 영상 장치에 포함된 구성 요소가 도 4에 도시된 바와 같이 한정되는 것은 아니다. X선 영상 장치의 구성 요소에 대해서는 도 5에서 상세하게 설명하기로 한다. X선 영상 장치는 카메라를 이용하여 대상체를 촬영함으로써, 대상체 이미지를 획득한다(동 작 ①). X선 영상 장치는 X선 디텍터 앞에 환자 포지셔닝(positioning)이 완료됨에 따라 카메라(11 0)를 통해 대상체를 촬영할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 움직임 검출 모드의 실행을 위한 버튼 UI를 디스플레이부 상에 디스플레이하고, 버튼 UI를 터치하는 사용자의 터치 입력을 수신할 수 있다. X선 영상 장치는 사용자의 터치 입력을 수신함에 따라 움직임 검출 모드를 실행하 고, 카메라를 통해 대상체를 촬영하여 대상체 이미지를 획득할 수 있다. 그러나, 이에 한정되는 것은 아니고, X선 영상 장치는 자동으로 움직임 검출 모드를 실행할 수도 있다. 본 개시의 일 실시예에서, X선 영상 장치는 X선 디텍터 앞에 환자 포지셔닝이 완료된 후 기 설정된 시간이 경과함에 따라 자동 으로 움직임 검출 모드를 실행하고, 카메라를 통해 대상체를 촬영하여 대상체 이미지를 획득할 수 있 다. 여기서, 획득된 '대상체 이미지'는 일반적인 이미지 센서(예를 들어, CMOS 또는 CCD)를 갖는 카메라 를 통해 획득된 2차원 이미지로서, 대상체를 투과한 X선을 X선 디텍터를 통해 수광하여 이미지 처리를 통해 획득된 X선 이미지와는 다르다. 본 개시의 일 실시예에서, X선 영상 장치는 대상체 이미지를 디 스플레이부 상에 디스플레이할 수 있다. X선 영상 장치는 인공지능 모델을 이용하여 대상체의 움직임을 검출한다 (동작 ②). X선 영상 장치 는 획득된 대상체 이미지를 인공지능 모델을 이용하여 분석함으로써, 대상체의 움직임을 검출할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 환자 포지셔닝 완료 후 대상체를 촬영하여 획득 한 첫번째 이미지 프레임을 기준 이미지(reference image)로 결정하고, 인공지능 모델을 이용하여 기준 이 미지와 후속 촬영을 통해 획득된 이미지 프레임을 비교하여, 대상체의 움직임을 검출할 수 있다. 인공지능 모델 은 기계 학습(machine learning) 알고리즘 및 심층 신경망 모델(deep neural network) 중 적어도 하나를 포함할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 기계 학습 모델 중 자기 조직화 지도(Self-Organizing Map)를 이용하여 기준 이미지 및 후속 이미지 프레임에서 대상체와 배경(background)의 픽셀들을 클러스터링 (clustering)하고, 대상체를 나타내는 픽셀들에 가중치(weights)를 부여함으로써, 배경의 노이즈 영향을 감 소시키는 방식으로 대상체의 움직임 검출 정확도를 향상시킬 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 학습된 심층 신경망 모델(trained deep neural network)에 대 상체 이미지를 입력하고, 심층 신경망 모델을 이용하는 추론(inferencing)을 수행함으로써, 대상체의 움직 임을 검출할 수 있다. X선 영상 장치는 심층 신경망 모델을 이용하는 추론을 수행함으로써, 기준 이미지 및 후속 이미지 프레임으로부터 각각 대상체의 주요 부위(landmark)에 관한 키 포인트들을 추출할 수 있다. 여기서, '심층 신경망 모델'은 기 획득된 복수의 이미지를 입력 데이터로 적용하고, 신체 주요 부위의 키 포인 트들의 위치 좌표값들을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식을 통해 학습 된(trained) 모델일 수 있다. 심층 신경망 모델은 예를 들어, 컨볼루션 신경망 모델(Convolutional Neural Network; CNN)일 수 있으나, 이에 한정되는 것은 아니다. X선 영상 장치는 기준 이미지로부터 추출된 키 포인트들과 후속 이미지 프레임으로부터 추출된 키 포인트들 간의 차이값을 산출하고, 산출된 차이값을 임계치 와 비교함으로써 대상체의 움직임을 검출할 수 있다. X선 영상 장치는 대상체의 움직임 검출 결과를 나타내는 알림 신호를 출력한다 (동작 ③). 본 개시의 일 실시예에서, X선 영상 장치는 대상체의 움직임을 나타내는 기 설정된 컬러를 갖는 그래픽 UI(graphic user interface)를 디스플레이부 상에 디스플레이할 수 있다. 그래픽 UI는 예를 들 어, 주행색(또는 빨간색) 컬러를 갖고, 움직이는 사람을 형상화한 아이콘일 수 있다. 도 4에는 도시되지 않았지 만, X선 영상 장치는 음향 신호를 출력하는 스피커를 더 포함하고, 스피커를 통해 대상체의 움직임에 관한 정보를 사용자에게 알리는 음성 및 알림음 중 적어도 하나의 음향 신호를 출력할 수 있다. 종래의 X선 영상 장치는 대상체(예를 들어, 환자)의 특정 신체 부위에 기준 요소(Fiducial Element)를 부착 하고, 기준 요소를 부착한 대상체를 촬영하여 획득된 이미지로부터 기준 요소의 변위를 분석하는 방식을 통 해 대상체의 움직임을 검출하였다. 종래 기술은 대상체의 특정 신체 부위에 부착하는 기준 요소가 별도로 구비되지 않으면 동작이 불가능한 기술이며, 연속적인 X선 이미지를 획득하는 과정에서만 대상체의 움직임 을 검출할 수 있다. 따라서, 종래 기술은 X선 이미지의 촬영 전 미리 대상체의 움직임을 검출하여 비정상적 인 이미지의 획득을 방지할 수 없는 기술적 한계점이 있다. 본 개시는 카메라를 이용하여 대상체를 촬영함으로써 대상체 이미지를 획득하고, 인공지능 모델 을 이용하여 대상체 이미지를 분석함으로써 대상체의 움직임을 검출하는 X선 영상 장치 및 그 동작 방법을 제공하는 것을 목적으로 한다. 도 4에 도시된 실시예에 따른 X선 영상 장치는 종래 기술과는 달리 환자 포지셔닝 완료 후 실제 X선 촬영 이 이루어지기 전에 호흡 등 통상적인 대상체(예를 들어, 환자)의 움직임 보다 큰 움직임이 발생된 경우, 대상체의 움직임을 나타내는 그래픽 UI 또는 음향 신호를 출력함으로써, 사용자가 효율적이고 정확한 X선 촬영을 할 수 있도록 지원할 수 있고, 이를 통해 사용자 편의성을 향상시킬 수 있다. 본 개시의 일 실시예 에 따른 X선 영상 장치는 환자 모니터링을 자동화할 수 있고, 사용자(예를 들어, 방사선사)가 의도한 정확 한 자세를 유지하는 상태에서 대상체의 X선 이미지 획득이 가능하며, 이를 통해 대상체의 움직임으로 인한 X선 이미지의 화질 저하를 방지할 수 있다. 또한, 본 개시의 일 실시예에 따른 X선 영상 장치는 대상 체의 움직임으로 인한 재촬영(retake) 시 발생되는 촬영 시간의 증가와 추가적인 방사선 노출의 위험을 사 전에 예방할 수 있는 기술적 효과를 제공한다. 도 5는 본 개시의 일 실시예에 따른 X선 영상 장치의 구성 요소를 도시한 블록도이다. 도 5에 도시된 X선 영상 장치는 모바일 X선 디텍터를 포함하는 모바일 타입의 장치일 수 있다. 그러 나, 이에 한정되는 것은 아니고, X선 영상 장치는 실링 타입으로도 구현될 수 있다. 실링 타입의 X선 영상장치는 도 12에서 상세하게 설명하기로 한다. 도 5를 참조하면, X선 영상 장치는 카메라, X선 조사부, X선 디텍터, 프로세서, 메 모리, 사용자 입력 인터페이스, 및 출력 인터페이스를 포함할 수 있다. 카메라, X선 조사 부, X선 디텍터, 프로세서, 메모리, 사용자 입력 인터페이스, 및 출력 인터페이스 는 각각 전기적 및/또는 물리적으로 서로 연결될 수 있다. 도 5에는 X선 영상 장치의 동작을 설명하 기 위한 필수적 구성 요소만이 도시되었고, X선 영상 장치가 포함하는 구성 요소가 도 5에 도시된 바와 같 이 한정되는 것은 아니다. 본 개시의 일 실시예에서, X선 영상 장치는 워크스테이션(200, 도 12 참조), 서 버(2000, 도 1 참조), 타 의료 장치(3000, 도 1 참조), 또는 외부 휴대용 단말(4000, 도 1 참조)와 데이터 통신 을 수행하기 위한 통신 인터페이스(190, 도 12 참조)를 더 포함할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 X선 소스에 인가되는 고전압을 발생시키는 고전압 발생부(126, 도 3 참조)를 더 포함할 수 있 다. 본 개시의 다른 실시예에서, X선 영상 장치의 출력 인터페이스는 스피커를 포함하지 않을 수도 있다. 카메라는 X선 디텍터 앞에 포지셔닝된 대상체(예를 들어, 환자)를 촬영함으로써, 대상체 이미지를 획 득하도록 구성된다. 본 개시의 일 실시예에서, 카메라는 렌즈 모듈, 이미지 센서, 및 이미지 프로세싱 모 듈을 포함할 수 있다. 카메라는 이미지 센서(예를 들어, CMOS 또는 CCD)에 의해 대상체에 관한 정지 이미 지(still image) 또는 동영상(video)을 획득할 수 있다. 동영상은 카메라를 통해 대상체를 촬영함으로써 실시간으로 획득되는 복수의 이미지 프레임을 포함할 수 있다. 이미지 프로세싱 모듈은 이미지 센서를 통해 획 득된 단일 이미지 프레임으로 구성된 정지 이미지 또는 복수의 이미지 프레임으로 구성된 동영상 데이터를 인코 딩하여 프로세서에 전달할 수 있다. 본 개시의 일 실시예에서, 카메라는 X선 영상 장치의 사용자 입력 인터페이스의 일측에 장착될 수 있도록 소형 폼 팩터(form factor)로 구현되고, 저전력을 소비하는 경량 RGB 카메라일 수 있다. 그러나, 이 에 한정되는 것은 아니고, 본 개시의 다른 실시예에서 카메라는 깊이 추정 기능을 포함하는 RGB-depth 카 메라, 스테레오 어안 카메라, 그레이스케일 카메라, 또는 적외선 카메라 등 공지의 모든 종류의 카메라로 구현 될 수 있다. X선 조사부는 X선을 발생시키고, 대상체에 X선을 조사하도록 구성된다. X선 조사부는 고전압 발생부 (126, 도 3 참조)에서 발생된 고전압을 인가받아 X선을 발생시키고 조사하는 X선 소스 및 X선 소스에 서 조사되는 X선의 경로를 안내하여 X선의 조사 영역을 조절하는 콜리메이터(collimator)를 포함할 수 있 다. X선 소스는 X선관(X-ray tube)을 포함하며, X선관은 양극과 음극으로 된 2극 진공관으로 구현될 수 있다. X선관 내부를 약 10mmHg 정도의 고진공 상태로 만들고 음극의 필라멘트를 고온으로 가열하여 열전자를 발생시킨 다. 필라멘트로는 텅스텐 필라멘트를 사용할 수 있고 필라멘트에 연결된 전기도선에 10V의 전압과 3-5A 정도의 전류를 가하여 필라멘트를 가열할 수 있다. 음극과 양극 사이에 10-300kVp 정도의 고전압을 걸어주면 열전자가 가속되어 양극의 타겟 물질에 충돌하면서 X선이 발생된다. 발생된 X선은 윈도우를 통해 외부로 조사되며, 윈도 우의 재료로는 베륨 박막을 사용할 수 있다. 이 때, 타겟 물질에 충돌하는 전자의 에너지 중 대부분은 열로 소 비되며 열로 소비되고 남은 나머지 에너지가 X선으로 변환된다. 양극은 주로 구리로 구성되고, 음극과 마주보는 쪽에 타겟 물질이 배치되며, 타겟 물질로는 Cr, Fe, Co, Ni, W, Mo 등의 고저항 재료들이 사용될 수 있다. 타겟 물질은 회전자계에 의해 회전할 수 있으며, 타겟 물질이 회전하 게 되면 전자 충격 면적이 증대되고 고정된 경우에 비해 열 축적율이 단위 면적당 10배 이상 증대될 수 있다. X선관의 음극과 양극 사이에 가해지는 전압을 관전압이라 하며, 이는 고전압 발생부에서 인가되고, 그 크 기는 파고치 kVp로 표시할 수 있다. 관전압이 증가하면 열전자의 속도가 증가되고 결과적으로 타겟 물질에 충돌 하여 발생되는 X선의 에너지(광자의 에너지)가 증가된다. X선관에 흐르는 전류는 관전류라 하며 평균치 mA로 표 시할 수 있고, 관전류가 증가하면 필라멘트에서 방출되는 열전자의 수가 증가하고 결과적으로 타겟 물질에 충돌 하여 발생되는 X선의 선량(X선 광자의 수)이 증가된다. 따라서, 관전압에 의해 X선의 에너지가 제어될 수 있고, 관전류 및 X선 노출 시간에 의해 X선의 세기 또는 선량이 제어될 수 있다. X선 디텍터는 X선 조사부에 의해 조사되어 대상체를 투과한 X선을 검출하도록 구성된다. 본 개시의 일 실시예에서, X선 디텍터는 TFT(thin film transistor)로 구현되거나, 또는 CCD(charge coupled device)로 구현된 디지털 검출부일 수 있다. 도 5에서 X선 디텍터는 X선 영상 장치에 포함되는 구성요소로 도시되어 있으나, X선 디텍터는 X선 영상 장치에 연결 및 분리 가능한 별개의 장치일 수 있다. 프로세서는 메모리에 저장된 프로그램의 하나 이상의 명령어들(instructions)을 실행할 수 있다. 프 로세서는 산술, 로직 및 입출력 연산과 이미지 프로세싱을 수행하는 하드웨어 구성 요소로 구성될 수 있다. 도 5에는 프로세서가 하나의 엘리먼트로 도시되었으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예에서, 프로세서는 하나 이상의 복수 개의 엘리먼트들로 구성될 수 있다. 프로세서는 CPU(Central Processing Unit), AP(Application Processor), DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU(Graphic Processing Unit), VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU(Neural Processing Unit)와 같은 인공지능 전용 프로세서일 수 있다. 프로세서는, 기 정의된 동작 규 칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어할 수 있다. 또는, 프로세서가 인공지능 전 용 프로세서인 경우, 인공지능 전용 프로세서는 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 메모리는 예를 들어, 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미 디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 또는 광 디스크 중 적어도 하나의 타입의 저장매체로 구성될 수 있다. 메모리에는 X선 영상 장치가 카메라에 의해 획득된 대상체 이미지로부터 대상체의 움직임을 검 출하는 기능 및/또는 동작들과 관련된 명령어들(instructions)이 저장될 수 있다. 본 개시의 일 실시예에서, 메 모리에는 프로세서가 판독할 수 있는 명령어들, 알고리즘(algorithm), 데이터 구조, 프로그램 코드 (program code), 및 애플리케이션 프로그램(application program) 중 적어도 하나가 저장될 수 있다. 메모리 에 저장되는 명령어들, 알고리즘, 데이터 구조, 및 프로그램 코드는 예를 들어, C, C++, 자바(Java), 어 셈블러(assembler) 등과 같은 프로그래밍 또는 스크립팅 언어로 구현될 수 있다. 이하의 실시예에서, 프로세서는 메모리에 저장된 명령어들 또는 프로그램 코드들을 실행함으로써 구 현될 수 있다. 프로세서는 카메라로부터 대상체를 촬영한 대상체 이미지의 이미지 데이터를 획득할 수 있다. 프로세 서는 X선 디텍터 앞에 환자 포지녀싱(positioning)이 완료됨에 따라 카메라를 제어하여 대상체 를 촬여하고, 대상체의 이미지 데이터를 획득할 수 있다. 본 개시의 일 실시예에서, 프로세서는 사용자 입 력 인터페이스를 통해 움직임 검출 모드를 실행하기 위한 사용자 입력이 수신되는 경우 카메라를 통 해 대상체를 촬영하도록 카메라를 제어할 수 있다. 사용자 입력 인터페이스는 디스플레이부 상 에 디스플레이되는 움직임 검출 모드의 실행을 위한 버튼 UI(404, 도 4 참조)를 선택하는 사용자의 터치 입력을 수신하고, 터치 입력이 수신됨에 따라 프로세서는 움직임 검출 모드를 실행할 수 있다 (수동 모드). 본 개 시의 일 실시예에서, '움직임 검출 모드를 실행시키는 사용자 입력'은 터치 입력으로 한정되는 것은 아니고, 키 패드(key pad), 하드웨어 버튼, 조그 스위치 등을 누르는 입력일 수도 잇다. 그러나, 이에 한정되는 것은 아니고, 프로세서는 자동으로 대상체를 촬영하도록 움직임 검출 모드를 실행 할 수 있다. 본 개시의 일 실시예에서, 프로세서는 X선 디텍터 앞에 환자 포지셔닝이 완료된 후 기 설정된 시간이 경과함에 따라 자동으로 움직임 검출 모드를 실행할 수 있다 (자동 모드). 본 개시의 일 실시예에서, 프로세서는 카메라에 의해 실시간으로 촬영된 복수의 이미지 프레임으로 구성된 동영상 데이터를 획득할 수 있다. 프로세서는 인공지능 모델(Artificial Intelligent model, AI model)을 이용하여 이미지 데이터를 분석하여, 이미지 데이터로부터 대상체의 움직임을 검출할 수 있다. 인공지능 모델은 기계 학습(machine learning) 알고리즘 및 심층 신경망 모델(deep neural network) 중 적어도 하나를 포함할 수 있다. 본 개시의 일 실시예에서, 인공지능 모델은 메모리에 저장되어 있는 명령어들(instructions), 프로그램 코드 (program code), 또는 알고리즘으로 구현될 수 있으나, 이에 한정되는 것은 아니다. 본 개시의 다른 실시예에서, 인공지능 모델은 X선 영상 장치에 포함되지 않을 수 있다. 이 경우, 인공지능 모델(232, 도 12 참조)은 워크스테이션(200, 도 12 참조)에 포함될 수도 있다. 프로세서는 환자 포지셔닝 완료 후 움직임 검출 모드가 실행됨에 따라, 카메라를 이용하여 대상체를 촬영하여 획득된 첫번째 이미지 프레임을 기준 이미지(reference image)로 결정하고, 인공지능 모델을 이 용하여 기준 이미지와 후속 촬영을 통해 획득된 이미지 프레임을 비교하여, 대상체의 움직임을 검출할 수 있다. 본 개시의 일 실시예에서, 프로세서는 인공지능 모델 중 기계 학습 알고리즘인 자기 조직화 지도 (Self-Organizing Map)를 이용하여 기준 이미지 및 후속 이미지 프레임 각각에서 대상체 및 배경(background)의 픽셀들을 각각 클러스터링(clustering)하고, 대상체를 나타내는 픽셀들에 가중치(weights)를 부여함으로써, 대 상체의 움직임을 검출할 수 있다. 자기 조직화 지도를 이용함으로써, 배경의 노이즈 영향이 감소되는 바, 대상 체의 움직임 검출 정확도가 향상될 수 있다. 프로세서가 자기 조직화 지도를 이용하여 대상체의 움직임을 검출하는 구체적인 실시예에 대해서는 도 8에서 상세하게 설명하기로 한다. 본 개시의 일 실시예에서, 프로세서는 인공지능 모델 중 학습된 심층 신경망 모델(trained deep neural network)에 대상체 이미지를 입력하고, 심층 신경망 모델을 이용하는 추론(inferencing)을 수행함으로써, 대상체의 움직임을 검출할 수 있다. 프로세서는 심층 신경망 모델을 이용하는 추론을 수행 함으로써, 기준 이미지 및 후속 이미지 프레임으로부터 각각 대상체의 주요 부위(landmark)에 관한 키 포인트들 을 추출할 수 있다. 여기서, '심층 신경망 모델'은 기 획득된 복수의 이미지를 입력 데이터로 적용하고, 신체 주요 부위의 키 포인트들의 위치 좌표값들을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식을 통해 학습된(trained) 모델일 수 있다. 심층 신경망 모델은 예를 들어, 컨볼루션 신경망 모델 (Convolutional Neural Network; CNN)일 수 있으나, 이에 한정되는 것은 아니다. 프로세서는 기준 이미지 로부터 추출된 키 포인트들과 후속 이미지 프레임으로부터 추출된 키 포인트들 간의 차이값을 산출하고, 산출된 차이값을 임계치와 비교함으로써 대상체의 움직임을 검출할 수 있다. 프로세서가 심층 신경망 모델을 이용 하여 대상체의 움직임을 검출하는 구체적인 실시예에 대해서는 도 9 및 도 10에서 상세하게 설명하기로 한다. 프로세서는 대상체의 움직임 검출 결과를 사용자에게 알리는 알림 신호를 출력하도록 출력 인터페이스 를 제어할 수 있다. 본 개시의 일 실시예에서, 프로세서는 대상체의 움직임을 나타내는 기 설정된 컬 러를 갖는 그래픽 UI(graphic user interface)를 디스플레이하도록 디스플레이부를 제어할 수 있다. 그래 픽 UI는 예를 들어, 주행색(또는 빨간색) 컬러를 갖고, 움직이는 사람을 형상화한 아이콘일 수 있다. 본 개시의 일 실시예에서, 프로세서는 대상체의 움직임에 관한 정보를 사용자에게 알리는 음성 및 알림음 중 적어도 하나의 음향 신호를 출력하도록 스피커를 제어할 수 있다. 프로세서는 움직임 검출의 정도를 조절하기 위한 움직임 검출 민감도를 설정할 수 있다. 여기서, '움직임 검출 민감도'는 대상체의 움직임 정도에 따라 움직임 검출 및 알림 신호를 제공할 지 여부에 관한 정도를 나타 낸다. 움직임 검출 민감도가 크면 클수록 대상체의 작은 움직임에도 움직임이 검출되어 알림 신호가 출력되고, 움직임 검출 민감도가 작으면 작을수록 대상체의 상대적으로 큰 움직임에만 움직임이 검출될 수 있다. 본 개시 의 일 실시예에서, 프로세서는 대상체와 X선 조사부 간의 거리인 SID(Source to Image Distance), 대상체의 크기, 형태, 및 촬영 프로토콜 중 적어도 하나에 기초하여 움직임 검출 민감도를 설정할 수 있다. 그 러나, 이에 한정되는 것은 아니고, 프로세서는 사용자 입력에 의해 움직임 검출 민감도를 설정할 수 있다. 이 경우, 사용자 입력 인터페이스는 움직임 검출 민감도를 설정 또는 조절하는 사용자 입력을 수신하고, 프로세서는 수신된 사용자 입력에 기초하여 움직임 검출 민감도를 설정할 수 있다. 사용자 입력 인터페이스는 X선 영상 장치의 조작을 위한 인터페이스를 제공하도록 구성된다. 사용자 입력 인터페이스는 예를 들어, 키 패드(key pad), 마우스, 트랙볼, 조그 다이얼(jog dial), 조그 스위치 (jog switch), 또는 터치 패드 등 하드웨어적 요소를 포함하는 컨트롤 패널로 구성될 수 있으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예에서, 사용자 입력 인터페이스는 터치 입력을 수신하고, 그래픽 사용자 인터페이스(Graphical User Interface, GUI)를 표시하는 터치스크린으로 구성될 수도 있다. 사용자 입력 인터페이스는 사용자로부터 X선 영상 장치의 조작을 위한 명령(command) 및 X선 촬영에 관한 각종 정보를 입력받을 수 있다. 사용자 입력 인터페이스는 예를 들어, 움직임 검출 민감도 설정, 움 직임 검출 모드 실행을 위한 명령(수동 모드) 등 사용자 입력을 수신할 수 있다. 출력 인터페이스는 프로세서의 제어에 의해 대상체의 움직임 검출 결과를 출력하도록 구성된다. 출력 인터페이스는 디스플레이부 및 스피커를 포함할 수 있다. 디스플레이부는 대상체의 움직임 검출 결과를 나타내는 그래픽 사용자 인터페이스를 표시할 수 있다. 디스 플레이부는 예를 들어, CRT 디스플레이, LCD 디스플레이, PDP 디스플레이, OLED 디스플레이, FED 디스플레 이, LED 디스플레이, VFD 디스플레이, DLP(Digital Light Processing) 디스플레이, 평판 디스플레이(Flat Panel Display), 3D 디스플레이, 및 투명 디스플레이 중 적어도 하나를 포함하는 하드웨어 장치로 구성될 수 있으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예에서, 디스플레이부는 터치 인터페이스를 포함하는 터치스크린으로 구성될 수도 있다. 디스플레이부가 터치스크린으로 구성되는 경우, 디스플레이부는 터치 패널로 구성되는 사용자 입력 인터페이스와 통합되는 구성 요소일 수 있다. 도 6은 본 개시의 일 실시예에 따른 X선 영상 장치가 카메라를 통해 획득된 이미지로부터 대상체의 움직임 을 검출하는 방법을 도시한 흐름도이다. 단계 S610에서, X선 영상 장치는 카메라를 이용하여 대상체를 촬영함으로써 대상체의 이미지 데이터를 획 득한다. X선 영상 장치는 카메라를 이용하여 X선 디텍터(130, 도 4 참조) 앞에 포지셔닝된 대상체(예를 들 어, 환자)를 촬영함으로써 이미지 데이터를 획득할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 환자 포지셔닝 완료 후 움직임 검출 모드의 실행을 위한 버튼 UI를 선택하는 사용자 입력을 수신하고, 수신된 사용자 입력에 기초하여 움직임 검출 모드를 실행할 수 있다. X선 영상 장치는 움직임 검출 모드가 실행됨 에 따라 대상체의 움직임을 검출하도록 카메라를 통해 대상체의 이미지 데이터를 획득할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 X선 디텍터 앞에 환자 포지셔닝이 완료된 후 기 설정된 시간이 경과함에 따라 자동으로 움직임 검출 모드를 실행할 수 있다. X선 영상 장치는 움직임 검출 모드가 실행됨에 따라 카메라를 통해 대상체를 촬영하여 이미지 데이터를 획득할 수 있다. X선 영상 장치는 X선 디텍터 앞에 환자 포지셔닝이 완료되면, 대상체를 촬영하여 기준 이미지 (reference image)를 획득하고, 기준 이미지를 획득한 이후 대상체를 후속 촬영하여 이미지 프레임을 획득할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 기준 이미지 획득 이후, 카메라를 이용하여 실시간으로 대상체를 촬영하여 복수의 이미지 프레임을 획득할 수 있다. 단계 S620에서, X선 영상 장치는 인공지능 모델을 이용하여 이미지 데이터를 분석하여, 이미지 데이터로부 터 대상체의 움직임을 검출한다. X선 영상 장치는 인공지능 모델을 이용하는 분석을 통해 기준 이미지로부 터 인식된 대상체와 후속 촬영된 이미지 프레임으로부터 인식된 대상체를 비교함으로써, 대상체의 움직임을 검 출할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 인공지능 모델 중 기계 학습(machine learning) 알고리즘인 자 기 조직화 지도(Self-Organizing Map)를 이용하여 기준 이미지 및 후속 이미지 프레임 각각에서 대상체를 인식 하고, 인식된 대상체 및 배경(background)을 나타내는 픽셀들을 각각 클러스터링(clustering)하며, 대상체를 나 타내는 픽셀들에 가중치(weights)를 부여함으로써 대상체의 움직임을 검출할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 인공지능 모델 중 학습된 심층 신경망 모델(trained deep neural network)에 이미지 데이터를 입력하고, 심층 신경망 모델을 이용하는 추론(inferencing)을 수행함으로써 대상체의 움직임을 검출할 수 있다. X선 영상 장치는 심층 신경망 모델을 이용하는 추론을 수행함으로써, 기준 이미지 및 후속 이미지 프레임으로부터 각각 대상체의 주요 부위(landmark)에 관한 키 포인트들을 추출할 수 있다. X선 영상 장치는 기준 이미지로부터 추출된 키 포인트들과 후속 이미지 프레임으로부터 추출된 키 포인트들 간의 차이값을 산출하고, 산출된 차이값을 임계치와 비교함으로써 대상체의 움직임을 검출할 수 있 다. 단계 S630에서, X선 영상 장치는 대상체의 움직임 검출 결과를 사용자에게 알리는 알림 신호를 출력한다. 본 개시의 일 실시예에서, X선 영상 장치는 대상체의 움직임을 나타내는 기 설정된 컬러를 갖는 그래픽 UI(graphic user interface)를 디스플레이할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 대상체 의 움직임에 관한 정보를 사용자에게 알리는 음성 및 알림음 중 적어도 하나의 음향 신호를 출력할 수 있다. 도 7은 본 개시의 일 실시예에 따른 X선 영상 장치가 기준 이미지(iR)와 후속 이미지 프레임(i1, i2, i3, ...)을 비교하여 대상체의 움직임을 검출하는 동작을 설명하기 위한 도면이다. 도 7을 참조하면, t0 시점에 대상체가 X선 디텍터(130, 도 4 및 도 5 참조) 앞에 위치하는 환자 포지셔닝이 완 료된다. X선 영상 장치는 환자 포지셔닝이 완료된 후 카메라를 이용하여 대상체를 촬영함으로써 복수의 이 미지 프레임(iR, i1, i2, i3, ...)을 획득할 수 있다. X선 영상 장치는 환자 포지셔닝 이후 제1 시점(t1)에 서 획득된 이미지 프레임을 기준 이미지(reference image)(iR)로 결정할 수 있다. X선 영상 장치는 기준 이미지(iR)를 메모리(150, 도 5 참조) 내의 저장 공간에 저장할 수 있다. X선 영상 장치는 기준 이미지 (iR)를 획득한 이후 대상체를 후속 촬영하여 복수의 이미지 프레임(i1, i2, i3, ...)을 획득할 수 있다. 예를 들어, X선 영상 장치는 제2 시점(t2)에서 제1 이미지 프레임(i1)을 획득하고, 제3 시점(t3)에서 제2 이미지 프레임(i2)을 획득하며, 제4 시점(t4)에서 제3 이미지 프레임(i3)을 획득할 수 있다. X선 영상 장치는 인공지능 모델(152, 도 4 및 도 5 참조)을 이용하는 분석을 통해 기준 이미지(iR)로부터 대상체를 인식하고, 후속 촬영을 통해 획득된 복수의 이미지 프레임(i1, i2, i3, ...) 각각으로부터 인식된 대상체(701, 702, 703)를 비교함으로써, 대상체의 움직임을 검출할 수 있다. 예를 들어, X선 영상 장치는 인공지능 모델을 이용하여 기준 이미지(iR)로부터 대상체를 인식하고, 후속 촬영된 제1 이미지 프레 임(i1)로부터 대상체를 인식하며, 기준 이미지(iR)와 제1 이미지 프레임(i1) 각각으로부터 인식된 대상체를 비교함으로써 대상체의 움직임을 검출할 수 있다. 마찬가지로, X선 영상 장치는 기준 이미지(iR)로부터 인 식된 대상체를 후속 촬영된 제2 이미지 프레임(i2)으로부터 인식된 대상체, 제3 이미지 프레임(i3)으 로부터 인식된 대상체와 각각 비교하여, 대상체의 움직임을 검출할 수 있다. 도 8은 본 개시의 일 실시예에 따른 X선 영상 장치가 기계 학습(machine learning) 알고리즘을 이용하여 대상체의 움직임을 검출하는 방법을 도시한 흐름도이다. 도 8에 도시된 단계 S810 내지 S830은 도 6에 도시된 단계 S620을 구체화한 단계들이다. 도 8의 단계 S810은 도 6에 도시된 단계 S610이 수행된 이후에 수행될 수 있다. 도 8의 단계 S830이 수행된 이후에는 도 6에 도시된 단 계 S630이 수행될 수 있다. 단계 S810에서, X선 영상 장치는 자기 조직화 지도(Self-Organizing Map)를 이용하여 기준 이미지로부터 가중치(weights)를 획득한다. X선 영상 장치의 프로세서(140, 도 5 참조)는 기계 학습 알고리즘 중 자기 조직화 지도를 이용하여 기준 이미지로부터 대상체를 인식하고, 인식된 대상체를 나타내는 픽셀들에 가중치를 부여할 수 있다. 본 개시의 일 실시예에서, 프로세서는 기준 이미지의 이미지 데이터 및 가중치를 메모리 (150, 도 5 참조) 내의 저장 공간에 저장할 수 있다. 본 개시의 일 실시예에서, 프로세서는 배경 (background)을 나타내는 픽셀들에는 가중치를 부여하지 않거나, 또는 낮은 가중치를 부여할 수 있다. 단계 S820에서, X선 영상 장치는 가중치를 이용하여, 후속 촬영된 이미지 프레임으로부터 인식된 대상체와 기준 이미지로부터 인식된 대상체를 비교하여 대상체의 움직임을 검출한다. 본 개시의 일 실시예에서, X선 영상 장치의 프로세서는 기준 이미지를 획득한 이후에 후속 촬영을 통해 획득된 이미지 프레임으로부터 대 상체를 인식하고, 인식된 대상체를 기준 이미지로부터 인식된 대상체와 비교함으로써, 인식된 대상체들 간의 이 미지 픽셀값의 차이값을 산출할 수 있다. 프로세서는 산출된 차이값이 기 설정된 임계치를 초과하는 경우 대상체의 움직임을 인식할 수 있다. 본 개시의 일 실시예에서, 프로세서는 기 설정된 시간 간격에 따라 주 기적으로 후속 이미지 프레임으로부터 대상체를 인식하고, 인식된 대상체를 기준 이미지로부터 인식된 대상체와 비교하여 대상체의 움직임을 검출할 수 있다. 단계 S830에서, X선 영상 장치는 검출 결과를 이용하여, 기준 이미지 및 가중치를 업데이트한다. X선 영상 장치의 프로세서는 기준 이미지와 후속 촬영된 이미지 프레임 각각에서의 인식된 대상체의 픽셀들의 정보를 이용하여 기준 이미지를 업데이트할 수 있다. 또한, 프로세서는 검출된 대상체의 픽셀들의 정보를 이용하여 가중치를 업데이트할 수 있다. 프로세서는 후속 촬영된 복수의 이미지 프레임에 대해서도 S820 및 S830을 반복적으로 수행하여, 기준 이미지 및 가중치를 업데이트할 수 있다. 자기 조직화 지도를 이용하여 대상체를 인식하는 경우, 이미지 내의 대상체에는 높은 가중치를 부여하고, 배경 (background)에는 가중치를 부여하지 않거나 또는 상대적으로 낮은 가중치를 부여할 수 있다. 도 8에 도시된 흐 름도에 따르면, X선 영상 장치는 자기 조직화 지도를 이용하여 대상체를 인식하고, 기준 이미지 및 가중치 를 업데이트함으로써, 환자의 체형 또는 촬영거리 차이에 의한 모션 검출 정도와 저조도 환경 등 외부 환경에 의한 배경(background)의 노이즈(random noise) 영향을 최소화할 수 있고, 이를 통해 대상체의 움직임 검출의 정확도를 향상시킬 수 있다. 본 개시의 다른 실시예에서, X선 영상 장치는 공지의 기계 학습 알고리즘을 이용하여 기준 이미지와 후속 이미지 프레임을 비교함으로써, 대상체의 움직임을 검출할 수 있다. 예를 들어, X선 영상 장치는 SVM(Support Vector Machine), 선형 회귀(linear regression), 로지스틱 회귀(logistic regression), 나이브 베이즈 분류(Naive Bayes), 랜덤 포레스트(random forest), decision tree, 또는 k-nearest neighbor algorithm 중 적어도 하나를 이용하여 기준 이미지 및 후속 이미지 프레임을 분석함으로써, 대상체 움직임을 검출할 수도 있다. 도 9는 본 개시의 일 실시예에 따른 X선 영상 장치가 학습된 심층 신경망 모델(trained deep neural network)을 이용하여 대상체의 움직임을 검출하는 방법을 도시한 흐름도이다. 도 9에 도시된 단계 S910 내지 S950은 도 6에 도시된 단계 S620을 구체화한 단계들이다. 도 9의 단계 S910은 도 6에 도시된 단계 S610이 수행된 이후에 수행될 수 있다. 도 9의 단계 S940이 수행된 이후에는 도 6에 도시된 단 계 S630이 수행될 수 있다. 도 10은 본 개시의 일 실시예에 따른 X선 영상 장치가 학습된 심층 신경망 모델을 이용하여 대상체의 움직 임을 검출하는 동작을 설명하기 위한 개념도이다. 이하에서는, 도 9와 도10을 함께 참조하여, X선 영상 장치가 대상체의 움직임을 검출하는 기능 및/또는 동 작을 설명하기로 한다. 단계 S910에서, X선 영상 장치는 학습된 심층 신경망 모델을 이용하는 추론(inferencing)을 통해 기준 이 미지로부터 대상체의 주요 부위(landmark)에 관한 복수의 제1 키 포인트를 추출한다. 도 10을 함께 참조하면, X 선 영상 장치는 카메라를 통해 기준 이미지(iR)를 획득할 수 있다. X선 영상 장치의 프로세서 는 인공지능 모델에 기준 이미지(iR)를 입력하고, 인공지능 모델을 이용하는 추론을 통해 기준 이미지(iR)로부터 대상체의 주요 부위에 관한 복수의 제1 키 포인트(PR_1, PR_2, ... , PR_n)를 추출할 수 있다. 본 개시의 일 실시예에서, '대상체의 주요 부위'의 위치 및 개수는 촬영 프로토콜에 따라 결정될 수 있다. 예를 들어, 대상체(예를 들어, 환자)의 전신을 촬영하는 스티칭(stitching) 프로토콜의 경우, 신체 부위 중 머리, 어깨, 팔꿈치, 손, 허리, 무릎, 및 발 등이 주요 부위로 결정되고, 프로세서는 주요 부위의 키 포인트를 추출할 수 있다. 다른 예를 들어, 홀 스파인(Whole Spine) 프로토콜의 경우, 신체 부위 중 귀에서부터 골반 아래까지 부위 중 머리, 어깨, 팔꿈치, 손, 허리 등이 주요 부위로 결정되고, 프로세서는 주요 부위 의 키 포인트를 추출할 수 있다. 머리(Skull), 손, 또는 발과 같은 익스트리미티 프로토콜(Extremity protoco l)의 경우, 얼굴, 손, 또는 발 등 신체 고유의 특성을 갖는 부위가 주요 부위로서 결정되고, 프로세서는 주요 부위의 키 포인트를 추출할 수 있다. 도 10은 설명의 편의를 위하여 홀 스파인 프로토콜의 경우 추출되는 복수의 제1 키 포인트(PR_1, PR_2, ... , PR_n)를 예시적으로 도시한 도면이다. 복수의 제1 키 포인트(PR_1, PR_2, ... , PR_n)는 도 10에 도시된 바와 같이 한정되는 것은 아니고, 촬영 프로토콜 및 요구되는 정확도에 따라 변경 이 가능하다. 본 개시의 일 실시예에서, 프로세서는 복수의 제1 키 포인트(PR_1, PR_2, ... , PR_n)를 데이터 세트(dataset)로 구축할 수 있다. 프로세서는 데이터 세트를 메모리(150, 도 5 참조)에 저장할 수 있다. 본 개시의 일 실시예에서, 인공지능 모델은 심층 신경망 모델(deep neural network)일 수 있다. 심층 신경 망 모델은 기 획득된 복수의 이미지를 입력 데이터로 적용하고, 신체 주요 부위의 키 포인트들의 위치 좌표값들 을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식을 통해 학습된(trained) 모델일 수 있다. 키 포인트의 추출 정확도 향상을 위하여, 심층 신경망 모델은 일부 변형된 형태로 학습될 수도 있다. 학습을 위한 입력 데이터(예를 들어, 복수의 이미지) 및 정답값 데이터(예를 들어, 키 포인트들의 위치 좌표 값)가 부족한 경우, 증강(augmentation)을 통해 데이터를 가공하여 데이터 양을 증가시키거나, 학습된 모델을 일부 수정하는 파인 튜닝(Fine Tuning) 방식을 적용하는 것도 가능하다. 심층 신경망 모델은 컨볼루션 신경망 모델(Convolutional Neural Network; CNN)로 구현될 수 있다. 심층 신경 망 모델은 예를 들어, U-Net일 수 있다. 그러나, 이에 한정되는 것은 아니고, 심층 신경망 모델은 공지된 모든 자세 예측 모델(pose estimation model)로 구현될 수 있다. 다시 도 9를 참조하면, 단계 S920에서 X선 영상 장치는 기준 이미지로부터 추출된 복수의 제1 키 포인트를 후속 촬영된 이미지 프레임으로부터 추출된 복수의 제2 키 포인트와 비교하여 차이값을 산출한다. 도 10을 함께 참조하면, X선 영상 장치의 프로세서는 카메라를 이용하여 대상체를 촬영함으로써, 후속 이미지 프레임(i1)을 획득하고, 인공지능 모델을 이용하는 추론을 통해 후속 이미지 프레임(i1)으로부터 대상체 의 주요 부위에 관한 복수의 제2 키 포인트(P1, P2, ... , Pn)를 추출할 수 있다. 프로세서는 기준 이미지(iR)로부터 추출된 복수의 제1 키 포인트(PR_1, PR_2, ... , PR_n)를 후속 이미지 프레임(i1)으로부터 추출된 복수의 제2 키 포인트(P1, P2, ... , Pn)와 비교하여 차이값을 산출할 수 있다. 다시 도 9를 참조하면, 단계 S930에서 X선 영상 장치는 산출된 차이값을 기 설정된 임계치(α)와 비교할 수 있다. 비교 결과, 차이값이 임계치(α)를 초과하는 경우(단계 S940), X선 영상 장치는 대상체의 움직임을 검출한 다. X선 영상 장치의 프로세서는 차이값이 임계치(α)를 초과하는 경우 대상체가 움직였다고 판단할 수 있다. 비교 결과, 차이값이 임계치(α) 이하인 경우(단계 S950), X선 영상 장치는 대상체의 움직임을 검출하지 않는다. 움직임이 검출되지 않는 경우, X선 영상 장치는 후속 촬영을 통해 이미지 프레임(예를 들어, 제2 이미지 프레임)을 획득하고, 단계 S920으로 돌아가서 이미지 프레임(예를 들어, 제2 이미지 프레임) 으로부터 복수의 제3 키 포인트를 추출하고, 추출된 복수의 제3 키 포인트를 복수의 제1 키 포인트(PR_1, PR_2, ... , PR_ n)와 비교함으로써 차이값을 산출하는 동작을 수행할 수 있다. 도 11은 본 개시의 일 실시예에 따른 X선 영상 장치가 깊이 측정 장치를 이용하여 대상체의 포지 셔닝(positioning)을 검출하는 동작을 도시한 도면이다. 도 11을 참조하면, X선 영상 장치는 카메라, X선 조사부, X선 디텍터, 및 깊이 측정 장치 를 포함할 수 있다. 도 11에 도시된 카메라, X선 조사부, 및 X선 디텍터는 도 4에서 설명 한 구성들과 동일하므로, 중복되는 설명은 생략한다. 깊이 측정 장치는 X선 조사부와 대상체 간의 거리를 측정하도록 구성된다. 본 개시의 일 실시예 에서, 깊이 측정 장치는 스테레오 카메라(stereo-type camera), ToF 카메라(time of flight camera), 및 레이저 거리 측정기 중 적어도 하나를 포함할 수 있다. X선 영상 장치의 프로세서(140, 도 5 참조)는 깊이 측정 장치를 이용하여 X선 조사부와 대상체 간의 거리를 측정함으로써 환자 포지셔닝을 검출할 수 있다. 프로세서는 환자 포지셔닝을 검출한 이후, 사용자 입력을 수신하거나(수동 모드) 또는 기 설정된 시간이 경과 후에(자동 모드) 움직임 검출 모드를 실행할 수 있다. 도 12는 본 개시의 일 실시예에 따른 X선 영상 장치 및 워크스테이션의 구성 요소를 도시한 블록도이 다. 도 12에 도시된 X선 영상 장치는 실링 타입으로 구현될 수 있다. 도 12를 참조하면, X선 영상 장치는 카메라, X선 조사부, X선 디텍터, 프로세서, 사용자 입력 인터페이스, 출력 인터페이 스, 및 통신 인터페이스를 포함할 수 있다. 도 12에 도시된 X선 영상 장치는 메모리(150, 도 5 참조)를 포함하지 않고, 통신 인터페이스를 더 포함한다는 점을 제외하고는 도 5에 도시된 X선 영상 장치 (100, 도 5 참조)와 동일하므로, 중복되는 설명은 생략한다. 통신 인터페이스는 유선 또는 무선 통신 네트워크를 통해 워크스테이션과 데이터를 송수신하며, 데이 터를 처리할 수 있다. 통신 인터페이스는 예를 들어, 유선 랜, 무선 랜(Wireless LAN), 와이파이(Wi-Fi), 블루투스(Bluetooth), 지그비(zigbee), WFD(Wi-Fi Direct), 적외선 통신(IrDA, infrared Data Association), BLE(Bluetooth Low Energy), NFC(Near Field Communication), 와이브로(Wireless Broadband Internet, Wibro), 와이맥스(World Interoperability for Microwave Access, WiMAX), SWAP(Shared Wireless Access Protocol), 와이기그(Wireless Gigabit Allicance, WiGig) 및 RF 통신을 포함하는 데이터 통신 방식 중 적어도 하나를 이용하여 워크스테이션과 데이터 통신을 수행할 수 있다. 본 개시의 일 실시예에서, 통신 인터페이스는 프로세서의 제어에 의해, 카메라를 통해 대상체를 촬영하여 획득된 대상체 이미지를 워크스테이션에 전송하고, 워크스테이션으로부터 대상체의 움직임 검출 결과를 수신할 수 있다. X선 영상 장치는 수신된 대상체의 움직임 검출 결과를 나타내는 알림 신호 (예를 들어, 그래픽 UI)를 디스플레이부를 통해 표시하거나, 또는 스피커를 통해 음향 신호로서 출력 할 수 있다. 워크스테이션은 X선 영상 장치와 통신하는 통신 인터페이스, 적어도 하나의 명령어들 (instructions) 또는 프로그램 코드를 저장하는 메모리, 및 메모리에 저장된 명령어들 또는 프로그램 코드를 실행하도록 구성되는 프로세서를 포함할 수 있다. 프로세서는 도 1에 도시된 제어부(220, 도 1 참조)를 구성하는 하드웨어 장치일 수 있다. 워크스테이션의 메모리에는 인공지능 모델이 저장되어 있을 수 있다. 워크스테이션에 저장 된 인공지능 모델은 도 4 및 도 5에 도시되고, 설명된 인공지능 모델(152, 도 4 및 도 5 참조)와는 저장 위치를 제외하고는 모두 동일하므로, 중복되는 설명은 생략한다. 워크스테이션은 통신 인터페이스를 통해 X선 영상 장치로부터 대상체 이미지의 이미지 데이터를 수신할 수 있다. 본 개시의 일 실시예에서, 워크스테이션으로 전송되는 이미지 데이터는 기준 이미지 및 후속 이미지 프레임을 포함할 수 있다. 워크 스테이션의 프로세서는 인공지능 모델을 이용하여 기준 이미지와 후속 이미지 프레임을 비교하 여, 대상체의 움직임을 검출할 수 있다. 본 개시의 일 실시예에서, 프로세서는 인공지능 모델 중 기계 학습 알고리즘인 자기 조직화 지도 (Self-Organizing Map)를 이용하여 기준 이미지 및 후속 이미지 프레임 각각에서 대상체 및 배경(background)을 나타내는 픽셀들을 각각 클러스터링(clustering)하고, 대상체를 나타내는 픽셀들에 가중치(weights)를 부여함으 로써, 대상체의 움직임을 검출할 수 있다. 본 개시의 일 실시예에서, 프로세서는 인공지능 모델 중 학습된 심층 신경망 모델(trained deep neural network)에 기준 이미지 및 후속 이미지 프레임을 입력하고, 심 층 신경망 모델을 이용하는 추론(inferencing)을 수행함으로써 기준 이미지 및 후속 이미지 프레임으로부터 대 상체의 주요 부위(landmark)에 관한 키 포인트들을 추출하고, 추출된 키 포인트들을 비교함으로써 대상체의 움 직임을 검출할 수 있다. 프로세서가 자기 조직화 지도 또는 심층 신경망 모델을 이용하여 대상체의 움직임 을 검출하는 구체적인 방법은 도 8 내지 도 10에서 설명한 X선 영상 장치의 프로세서의 동작 방법과 동일하므로, 중복되는 설명은 생략한다. 워크스테이션의 프로세서는 통신 인터페이스를 제어하여, 움직임 검출 결과 데이터를 X선 영상 장치에 전송할 수 있다. 일반적으로, X선 영상 장치의 메모리(150, 도 5 참조)의 저장 용량, 프로세서의 연산 처리 속도 등은 워크스테이션에 비하여 제한적일 수 있다. 따라서, 워크스테이션은 대용량 데이터의 저장 및 대용량 의 연산량이 필요한 동작(예를 들어, 인공지능 모델을 이용하는 추론을 통해 대상체의 움직임 검출)을 수 행한 이후, 통신 네트워크를 통하여 필요한 데이터(예를 들어, 대상체의 움직임 검출 결과 데이터)를 X선 영상 장치에 전송할 수 있다. 이와 같은 방식으로, X선 영상 장치는 대용량의 메모리 및 빠른 연산 능력을 갖는 프로세서 없이도 워크스테이션으로부터 대상체의 움직임 검출 결과 데이터를 수신하고, 대상체의 움 직임 검출 결과를 나타내는 알림 신호를 출력함으로써, 대상체의 움직임을 검출하는데 소요되는 프로세싱 타임 (processing time)을 단축시키고, 움직임 검출 결과의 정확도를 향상시킬 수 있다. 도 13은 본 개시의 일 실시예에 따른 X선 영상 장치가 카메라를 통해 획득된 이미지 상에 스티칭 (stitching) X선 촬영을 위한 분할 촬영 영역을 디스플레이하는 동작을 설명하기 위한 개념도이다. 도 13에서, X선 영상 장치는 실링 타입(ceiling type)으로 도시되었으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예에서, X선 영상 장치는 모바일 타입으로도 구현될 수 있다. 도 13을 참조하면, X선 영상 장치는 카메라, X선 조사부, X선 디텍터, 사용자 입력 인터페 이스, 및 디스플레이부를 포함할 수 있다. 도 13에서는 X선 영상 장치의 기능 및/또는 동작을 설명하기 위한 최소한의 구성 요소만이 도시되어 있고, X선 영상 장치에 포함된 구성 요소가 도 13에 도시 된 바와 같이 한정되는 것은 아니다. X선 영상 장치의 구성 요소에 대해서는 도 14에서 상세하게 설명하기 로 한다. X선 영상 장치는 카메라를 이용하여 대상체를 촬영함으로써, 대상체 이미지를 획득한다(동 작 ①). X선 영상 장치는 X선 디텍터 앞에 환자 포지셔닝(positioning)이 완료됨에 따라 카메라(31 0)를 통해 대상체를 촬영할 수 있다. X선 영상 장치는 X선 디텍터 앞에 대상체(10, 예를 들어, 환자)가 포지셔닝된 것을 자동으로 인식하고, 환자 포지셔닝이 인식됨에 따라 카메라를 이용하여 대상체 를 촬영할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 스테레오 카메라(stereo-type camera), ToF 카메라(time of flight camera), 및 레이저 거리 측정기 중 적어도 하나로 구현되는 깊이 측정 장치(380, 도 19 참조)를 더 포함하고, 깊이 측정 장치를 이용하여 X선 조사부와 대상체 간의 거 리를 측정함으로써 환자 포지셔닝을 검출할 수 있다. 동작 ①에서 획득된 '대상체 이미지'는 일반적인 이미지 센서(예를 들어, CMOS 또는 CCD)를 갖는 카메라 를 통해 획득된 2차원 이미지로서, 대상체를 투과한 X선을 X선 디텍터를 통해 수광하여 이미지 처리를 통해 획득된 X선 이미지와는 다르다. 본 개시의 일 실시예에서, X선 영상 장치는 대상체 이미지 를 디스플레이부 상에 디스플레이할 수 있다. X선 영상 장치는 인공지능 모델을 이용하여 대상체의 스티칭(stitching) X선 촬영을 위한 복수의 분할 촬영 영역(1310-1 내지 1310-3)을 획득한다(동작 ②). X선 영상 장치는 대상체 이미지를 학습 된(trained) 인공지능 모델에 입력하고, 인공지능 모델을 이용하는 추론(inferencing)을 통해 대상체 의 스티칭 X선 촬영을 위한 복수의 분할 촬영 영역(1310-1 내지 1310-3)을 획득할 수 있다. 본 개시의 일 실시예에서, 인공지능 모델은 복수의 이미지를 입력 데이터로 적용하고, 촬영 프로토콜에 따라 스티칭된 분할 촬영 영역을 나타내는 위치 좌표값을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식으로 학습된(trained) 심층 신경망 모델(deep neural network)일 수 있다. 심층 신경망 모델은 컨볼루션 신 경망 모델(Convolutional Neural Network; CNN)일 수 있으나, 이에 한정되는 것은 아니다. 심층 신경망 모델은 예를 들어, CenterNet으로 구현될 수 있으나, 이에 한정되는 것은 아니다. 본 개시에서, '스티칭(stitching)'은 복수의 X선 분할 촬영 영역(1310-1 내지 1310-3)에 관한 복수의 X선 이미 지들을 연결하여 하나의 X선 이미지를 획득하는 영상 처리(imaging processing)를 의미한다. 스티칭은 복수의 분할 촬영 영역(1310-1 내지 1310-3) 각각에 관하여 획득된 X선 이미지 중 서로 중첩되는 부분을 검출하고, 검 출된 중첩 부분을 연결하는 영상 처리를 포함할 수 있다. 도 13에서, 복수의 분할 촬영 영역(1310-1 내지 1310- 3)은 대상체의 촬영 대상 영역을 총 3개로 분할한 것으로 도시되어 있으나 이는 예시적인 것일 뿐이고, 도 시된 바와 같이 한정되는 것은 아니다. 분할 촬영 영역(1310-1 내지 1310-3)의 개수 및 분할촬영의 횟수는 X선 촬영 대상의 부위, 촬영 프로토콜, 대상체의 크기(예를 들어, 키), 또는 형태(예를 들어, 체형) 중 적어도 하나에 기초하여 결정되고, 2개 이상의 복수 개로 결정될 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 사용자 입력 인터페이스를 통해 오토 스티칭 플래닝 UI(auto stitching planning user interface)를 선택하는 사용자 입력을 수신하고, 사용자 입력이 수신 됨에 따라 카메라를 통해 대상체 이미지로부터 스티칭 X선 촬영을 위한 복수의 분할 촬영 영역 (1310-1 내지 1310-3)을 획득할 수 있다. 본 개시의 일 실시예에서, 오토 스티칭 플래닝 UI는 디스플레이 부 상에 표시하는 그래픽 UI(graphic user interface)일 수 있다. 이 경우, 사용자 입력 인터페이스(36 0)와 디스플레이부는 터치스크린 형태로 통합되는 구성일 수 있다. X선 영상 장치는 복수의 분할 촬영 영역(1310-1 내지 1310-3)을 나타내는 그래픽 UI를 디스플레이한다 (동 작 ③). X선 영상 장치는 복수의 분할 촬영 영역(1310-1 내지 1310-3) 각각의 상단 및 하단을 나타내는 복 수의 가이드라인(1320S, 1320-1, 1320-2, 1320-3)을 디스플레이부 상에 디스플레이할 수 있다. 본 개시의 일 실시예에서, 복수의 가이드라인(1320S, 1320-1, 1320-2, 1320-3)은 복수의 분할 촬영 영역(1310-1 내지 1310-3)의 상단 및 하단 뿐만 아니라, 좌우의 경계면을 나타낼 수도 있다. X선 영상 장치는 대상체 이미지 상의 복수의 분할 촬영 영역(1310-1 내지 1310-3) 상에 복수의 가이드라인(1320S, 1320-1, 1320-2, 1320-3)을 나타내는 그래픽 UI를 오버레이(overlay)하여 디스플레이할 수 있다. 디스플레이부 상에 디스플 레이되는 복수의 가이드라인(1320S, 1320-1, 1320-2, 1320-3) 중 상단 인디케이터(1320S)는 제1 분할 촬영 영 역(1310-1)의 상단을 나타내는 그래픽 UI일 수 있다. 제1 가이드라인(1320-1)은 제1 분할 촬영 영역(1310-1)의 하단을 나타내고, 제2 분할 촬영 영역(1310-2)의 상단을 나타내는 그래픽 UI이고, 제2 가이드라인(1320-2)은 제 2 분할 촬영 영역(1310-2)의 하단을 나타내고, 제3 분할 촬영 영역(1310-3)의 상단을 나타내는 그래픽 UI이며, 제3 가이드라인(1320-3)은 제3 분할 촬영 영역(1310-3)의 하단을 나타내는 그래픽 UI일 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 복수의 분할 촬영 영역(1310-1 내지 1310-3)에 따른 분할 촬영 의 횟수를 나타내는 분할 촬영 횟수 UI를 디스플레이할 수 있다. 도 13에서, 분할 촬영 횟수 UI은 분할 촬영의 횟수를 숫자(예를 들어, 1, 2, 3, ...)로 디스플레이할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 스티칭 아이콘, 재설정 아이콘, 및 설정 아이콘 을 포함하는 그래픽 UI를 디스플레이부 상에 디스플레이할 수 있다. 스티칭 아이콘은 복수의 가이드라인(1320S, 1320-1, 1320-2, 1320-3)을 대상체 이미지 상에 오버레이하여 표시하도록 하는 사용자 입력을 수신하기 위한 그래픽 UI이다. 재설정 아이콘은 복수의 가이드라인(1320S, 1320-1, 1320-2, 1320-3)의 위치를 변경하여 복수의 분할 촬영 영역(1310-1 내지 1310-3)의 위치, 크기, 및 형태 중 적어도 하나 를 변경하는 재설정 모드로 진입하기 위한 사용자 입력을 수신하는 그래픽 UI이다. 설정 아이콘은 디스플 레이된 복수의 분할 촬영 영역(1310-1 내지 1310-3)을 결정하여 스티칭 X선 촬영을 하기 위한 사용자 입력을 수 신하는 그래픽 UI이다. 도 13에 도시된 실시예에 따른 X선 영상 장치는 인공지능 모델을 이용하여 X선 촬영 전에 스티칭 촬 영을 위한 복수의 분할 촬영 영역(1310-1 내지 1310-3)을 획득하고, 복수의 분할 촬영 영역(1310-1 내지 1310-3)의 상단 및 하단을 나타내는 복수의 가이드라인(1320S, 1320-1, 1320-2, 1320-3)을 디스플레이하는 바, 사용 자가 효율적이고, 정확한 분할 촬영 영역을 횟수를 편리하고, 직관적으로 파악하도록 할 수 있다. 본 개시의 일 실시예에 따른 X선 영상 장치는 스티칭의 전체 과정을 자동화할 수 있어, 효율적인 X선 이미지의 획득이 가능하며, 스티칭 촬영 준비 시간을 단축시킬 수 있다. 또한, 본 개시의 일 실시예에 따른 X선 영상 장치 는 부정확한 촬영 영역 설정으로 인하여 재촬영 시 발생되는 촬영 시간의 증가를 방지하고, 환자에 관한 추가적 인 방사선 노출 또는 방사선 과조사의 위험을 사전에 방지할 수 있는 기술적 효과를 제공한다. 도 14는 본 개시의 일 실시예에 따른 X선 영상 장치의 구성 요소를 도시한 블록도이다. 도 14에 도시된 X선 영상 장치는 모바일 X선 디텍터를 포함하는 모바일 타입의 장치일 수 있다. 그러 나, 이에 한정되는 것은 아니고, X선 영상 장치는 실링 타입으로도 구현될 수 있다. 실링 타입의 X선 영상 장치는 도 20에서 상세하게 설명하기로 한다. 도 14를 참조하면, X선 영상 장치는 카메라, X선 조사부, X선 디텍터, 프로세서, 메 모리, 사용자 입력 인터페이스, 및 디스플레이부를 포함할 수 있다. 카메라, X선 조사부 , X선 디텍터, 프로세서, 메모리, 사용자 입력 인터페이스, 및 디스플레이부는 각각 전기적 및/또는 물리적으로 서로 연결될 수 있다. 도 14에는 X선 영상 장치의 동작을 설명하기 위한 필수적 구성 요소만이 도시되었고, X선 영상 장치가 포함하는 구성 요소가 도 14에 도시된 바와 같이 한정 되는 것은 아니다. 본 개시의 일 실시예에서, X선 영상 장치는 워크스테이션(400, 도 20 참조), 서버 (2000, 도 1 참조), 타 의료 장치(3000, 도 1 참조), 또는 외부 휴대용 단말(4000, 도 1 참조)와 데이터 통신을 수행하기 위한 통신 인터페이스(390, 도 20 참조)를 더 포함할 수 있다. 카메라, X선 조사부, 및 X선 디텍터는 도 5에 도시되고, 설명된 카메라(110, 도 5 참조), X선 조사부(120, 도 5 참조), 및 X선 디텍터(130, 도 5 참조)와 동일한 구성 요소이고, 카메라, X선 조사부 , 및 X선 디텍터의 기능 및/또는 동작을 동일하게 수행하는 바 중복되는 설명은 생략한다. 프로세서는 메모리에 저장된 프로그램의 하나 이상의 명령어들(instructions)을 실행할 수 있다. 프 로세서는 산술, 로직 및 입출력 연산과 이미지 프로세싱을 수행하는 하드웨어 구성 요소로 구성될 수 있다. 도 14에는 프로세서가 하나의 엘리먼트로 도시되었으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예에서, 프로세서는 하나 이상의 복수 개의 엘리먼트들로 구성될 수 있다. 프로세서는 CPU(Central Processing Unit), AP(Application Processor), DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU(Graphic Processing Unit), VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU(Neural Processing Unit)와 같은 인공지능 전용 프로세서일 수 있다. 프로세서는, 기 정의된 동작 규 칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어할 수 있다. 또는, 프로세서가 인공지능 전 용 프로세서인 경우, 인공지능 전용 프로세서는 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 메모리는 예를 들어, 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미 디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 또는 광 디스크 중 적어도 하나의 타입의 저장매체로 구성될 수 있다. 메모리에는 X선 영상 장치가 카메라에 의해 획득된 대상체 이미지로부터 스티칭 X선 촬영을 위 한 분할 촬영 영역을 획득하고, 분할 촬영 영역 각각의 상단, 하단, 및 좌우의 경계면을 나타내는 복수의 가이 드라인 UI를 디스플레이하는 기능 및/또는 동작들과 관련된 명령어들(instructions)이 저장될 수 있다. 본 개시 의 일 실시예에서, 메모리에는 프로세서가 판독할 수 있는 명령어들, 알고리즘(algorithm), 데이터 구조, 프로그램 코드(program code), 및 애플리케이션 프로그램(application program) 중 적어도 하나가 저장 될 수 있다. 메모리에 저장되는 명령어들, 알고리즘, 데이터 구조, 및 프로그램 코드는 예를 들어, C, C++, 자바(Java), 어셈블러(assembler) 등과 같은 프로그래밍 또는 스크립팅 언어로 구현될 수 있다. 이하의 실시예에서, 프로세서는 메모리에 저장된 명령어들 또는 프로그램 코드들을 실행함으로써 구 현될 수 있다. 프로세서는 카메라로부터 대상체를 촬영한 대상체 이미지의 이미지 데이터를 획득할 수 있다. 프로세 서는 X선 디텍터 앞에 환자 포지녀싱(positioning)이 완료됨에 따라 카메라를 제어하여 대상체를 촬영하고, 대상체의 이미지 데이터를 획득할 수 있다. 본 개시의 일 실시예에서, 프로세서는 사용자 입 력 인터페이스를 통해 자동 스티칭 촬영 모드를 실행하기 위한 버튼 UI를 선택하는 사용자의 터치 입력을 수신하고, 터치 입력이 수신됨에 따라 프로세서는 스티칭 촬영을 위하여 대상체를 촬영하도록 카메라(31 0)를 제어할 수 있다. '자동 스티칭 촬영 모드를 실행시키는 사용자 입력'은 터치 입력으로 한정되는 것은 아니 고, 키 패드(key pad), 하드웨어 버튼, 조그 스위치 등을 누르는 입력일 수도 잇다. 그러나, 이에 한정되는 것은 아니고, X선 영상 장치는 깊이 측정 장치(380, 도 19 참조)를 더 포함하고, 프로세서는 깊이 측정 장치를 이용하여 X선 디텍터 앞에 포지셔닝된 대상체를 인식하고, 대상체 가 인식됨에 따라 자동으로 대상체를 촬영하도록 자동 스티칭 촬영 모드를 실행할 수 있다. 프로세서가 깊 이 측정 장치를 이용하여 대상체의 포지셔닝을 인식하는 구체적인 실시예에 대해서는 도 19에서 상세하게 설명하기로 한다. 프로세서는 인공지능 모델(Artificial Intelligent model, AI model)을 이용하여 대상체 이미지를 분석하는 추론을 통해 스티칭 X선 촬영을 위한 복수의 분할 촬영 영역을 획득할 수 있다. 본 개시의 일 실시예 에서, 인공지능 모델은 메모리에 저장되어 있는 명령어들(instructions), 프로그램 코드(program code), 또는 알고리즘으로 구현될 수 있으나, 이에 한정되는 것은 아니다. 본 개시의 다른 실시예에서, 인공지 능 모델은 X선 영상 장치에 포함되지 않을 수 있다. 이 경우, 인공지능 모델(432, 도 20 참조)은 워 크스테이션(400, 도 20 참조)에 포함될 수도 있다. 프로세서는 인공지능 모델에 대상체 이미지를 입력하고, 인공지능 모델을 이용하는 추론 (inferencing)을 통해 스티칭 X선 촬영을 위한 복수의 분할 촬영 영역을 획득할 수 있다. 본 개시의 일 실시예 에서, 인공지능 모델은 기 획득된 대상체에 관한 복수의 이미지를 입력 데이터로 적용하고, 촬영 프로토콜 에 따라 스티칭된 분할 촬영 영역을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식 으로 학습된(trained) 심층 신경망 모델(deep neural network)일 수 있다. 분할 촬영 영역의 정답값은 촬영 프 로토콜에 따라 다르게 결정될 수 있다. 분할 촬영 영역의 정확도 향상을 위하여, 심층 신경망 모델은 일부 변형 된 형태로 학습될 수도 있다. 학습을 위한 입력 데이터(예를 들어, 복수의 이미지) 및 정답값 데이터(예를 들어, 분할 촬영 영역의 위치 좌표값)가 부족한 경우, 증강(augmentation)을 통해 데이터를 가공하여 데이터 양 을 증가시키거나, 학습된 모델을 일부 수정하는 파인 튜닝(Fine Tuning) 방식을 적용하는 것도 가능하다. 본 개시의 일 실시예에서, 심층 신경망 모델은 컨볼루션 신경망 모델(Convolutional Neural Network; CNN)일 수 있다. 심층 신경망 모델은 예를 들어, CenterNet으로 구현될 수 있다. 그러나, 이에 한정되는 것은 아니고, 심층 신경망 모델은 예를 들어, 순환 신경망 모델(Recurrent Neural Network), 제한된 볼츠만 머신(Restricted Boltzmann Machine), 심층 신뢰 신경망 모델(Deep Belief Network), 양방향 순환 신경망 모델(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크(Deep Q-Networks) 등으로 구현될 수도 있다. 인공지능 모델을 통해 획득된 복수의 분할 촬영 영역의 크기는 X선 디텍터에 의해 촬영 가능한 영역 의 크기 보다 클 수 있다. 본 개시의 일 실시예에서, 프로세서는 인공지능 모델을 통해 획득된 복수 의 분할 촬영 영역의 크기를 X선 디텍터의 크기 보다 작은 크기로 조절할 수 있다. 프로세서는 촬영 프로토콜에 기초하여 대상체 이미지로부터 촬영 대상 부위를 인식하고, 인식된 촬영 대상 부위에 관한 정보를 대상체 이미지와 함께 인공지능 모델에 입력하고, 인공지능 모델을 이용하는 추 론을 통해 복수의 분할 촬영 영역을 획득할 수 있다. 프로세서가 촬영 대상 부위에 기초하여 복수의 분할 촬영 영역을 획득하는 구체적인 실시예에 대해서는 도 16에서 상세하게 설명하기로 한다. 프로세서는 대상체 이미지를 디스플레이부를 통해 디스플레이할 수 있다. 본 개시의 일 실시예에서, 프로세서는 대상체 이미지 상에 복수의 분할 촬영 영역을 나타내는 그래픽 UI를 오버레이하여 디스플레이 하도록 디스플레이부를 제어할 수 있다. 사용자 입력 인터페이스는 복수의 분할 촬영 영역의 상단, 하단, 및 좌우의 경계면을 나타내는 복수의 가 이드라인 중 적어도 하나의 위치를 조정(adjust)하는 사용자 입력을 수신할 수 있다. 본 개시의 일 실시예에서, 사용자 입력 인터페이스는 복수의 가이드라인 중 적어도 하나의 위치를 조정하는 사용자의 터치 입력을 수 신할 수 있다. 프로세서는 사용자 입력 인터페이스를 통해 수신된 사용자 입력에 기초하여 복수의 가 이드라인 중 적어도 하나의 위치를 조정함으로써, 복수의 분할 촬영 영역의 위치, 크기, 및 형태 중 적어도 하 나를 변경할 수 있다. 프로세서가 사용자 입력에 기초하여 복수의 분할 촬영 영역의 위치, 크기, 및 형태 중 적어도 하나를 변경하는 구체적인 실시예에 대해서는 도 17a 및 도 17b에서 상세하게 설명하기로 한다.사용자 입력 인터페이스는 대상체의 촬영 대상 영역의 크기를 조절하여 X선 촬영 영역과 촬영 대상 영역 간의 마진(margin)의 크기를 조절하는 사용자 입력을 수신할 수 있다. 프로세서는 사용자 입력 인터페이스 를 통해 수신된 사용자 입력에 기초하여 복수의 분할 촬영 영역의 상하좌우 마진 크기를 결정할 수 있다. 프로세서가 사용자 입력에 기초하여 X선 촬영 영역의 마진 크기를 결정 또는 조절하는 구체적인 실시예에 대해서는 도 18에서 상세하게 설명하기로 한다. 프로세서는 복수의 분할 촬영 영역을 X선 촬영함으로써 적어도 하나의 X선 분할 촬영 이미지를 획득할 수 있다. 프로세서는 X선 조사부를 제어하여 대상체에 X선을 조사하고, 대상체를 투과한 X선을 X선 디텍 터를 통해 수광하며, 수광된 X선을 전기적 신호로 변환하여 복수의 X선 분할 촬영 이미지를 획득할 수 있 다. 프로세서는 복수의 X선 분할 촬영 이미지를 스티칭함으로써 X선 촬영 대상 영역에 관한 X선 이미지를 획득할 수 있다. 사용자 입력 인터페이스는 X선 영상 장치의 조작을 위한 인터페이스를 제공하도록 구성된다. 사용자 입력 인터페이스는 예를 들어, 키 패드(key pad), 마우스, 트랙볼, 조그 다이얼(jog dial), 조그 스위치 (jog switch), 또는 터치 패드 등 하드웨어적 요소를 포함하는 컨트롤 패널로 구성될 수 있으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예에서, 사용자 입력 인터페이스는 터치 입력을 수신하고, 그래픽 UI를 표 시하는 터치스크린으로 구성될 수도 있다. 디스플레이부는 프로세서의 제어에 의해 대상체 이미지를 디스플레이할 수 있다. 본 개시의 일 실시 예에서, 디스플레이부는 프로세서의 제어에 의해 대상체 이미지 상에 복수의 분할 촬영 영역을 나타 내는 그래픽 UI를 오버레이하여 디스플레이할 수 있다. 디스플레이부는 예를 들어, CRT 디스플레이, LCD 디스플레이, PDP 디스플레이, OLED 디스플레이, FED 디스플레이, LED 디스플레이, VFD 디스플레이, DLP(Digital Light Processing) 디스플레이, 평판 디스플레이(Flat Panel Display), 3D 디스플레이, 및 투명 디스플레이 중 적어도 하나를 포함하는 하드웨어 장치로 구성될 수 있으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예 에서, 디스플레이부는 터치 인터페이스를 포함하는 터치스크린으로 구성될 수도 있다. 디스플레이부 가 터치스크린으로 구성되는 경우, 디스플레이부는 터치 패널로 구성되는 사용자 입력 인터페이스와 통합되는 구성 요소일 수 있다. 도 14에는 도시되지 않았지만, X선 영상 장치는 음향 신호를 출력하는 스피커를 더 포함할 수 있다. 본 개 시의 일 실시예에서, 프로세서는 복수의 분할 촬영 영역의 설정 완료에 관한 정보를 음성 또는 알림음으로 출력하도록 스피커를 제어할 수 있다. 도 15는 본 개시의 일 실시예에 따른 X선 영상 장치가 카메라를 통해 획득된 이미지 상에 스티칭 X선 촬영 을 위한 분할 촬영 영역을 획득하고, 분할 촬영 영역을 나타내는 그래픽 UI(Graphic User Interface)를 디스플 레이하는 방법을 도시한 흐름도이다. 단계 S1510에서, X선 영상 장치는 X선 디텍터 앞에 포지셔닝된 대상체를 촬영하여 대상체 이미지를 획득한 다. X선 영상 장치는 카메라를 이용하여 X선 디텍터(330, 도 13 및 도 14 참조) 앞에 포지셔닝된 대상체 (예를 들어, 환자)를 촬영함으로써 이미지 데이터를 획득할 수 있다. 단계 S1510에서 획득된 '대상체 이미지'는 일반적인 이미지 센서(예를 들어, CMOS 또는 CCD)를 갖는 카메라를 통해 획득된 2차원 이미지로서, 대상체를 투 과한 X선을 X선 디텍터를 통해 수광하여 이미지 처리를 통해 획득된 X선 이미지와는 다르다. 단계 S1520에서, X선 영상 장치는 대상체 이미지를 학습된(trained) 인공지능 모델에 입력하고, 인공지능 모델을 이용하는 추론(inferencing)을 통해 스티칭 X선 촬영을 위한 복수의 분할 촬영 영역을 획득한다. 본 개 시의 일 실시예에서, 인공지능 모델은 기 획득된 대상체에 관한 복수의 이미지를 입력 데이터로 적용하고, 촬영 프로토콜에 따라 스티칭된 분할 촬영 영역을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식으로 학습된(trained) 심층 신경망 모델(deep neural network)일 수 있다. 심층 신경망 모델은 컨볼루션 신경망 모델(Convolutional Neural Network; CNN)일 수 있다. 심층 신경망 모델은 예를 들어, CenterNet으로 구현될 수 있다. 그러나, 이에 한정되는 것은 아니고, 심층 신경망 모델은 예를 들어, 순환 신경 망 모델(Recurrent Neural Network), 제한된 볼츠만 머신(Restricted Boltzmann Machine), 심층 신뢰 신경망 모델(Deep Belief Network), 양방향 순환 신경망 모델(Bidirectional Recurrent Deep Neural Network) 또는 심 층 Q-네트워크(Deep Q-Networks) 등으로 구현될 수도 있다. X선 영상 장치는 촬영 프로토콜에 기초하여 대상체 이미지로부터 촬영 대상 부위를 인식하고, 인식된 촬영 대상 부위에 관한 정보를 대상체 이미지와 함께 인공지능 모델에 입력하는 추론을 통해 복수의 분할 촬영 영역을 획득할 수 있다. 단계 S1530에서, X선 영상 장치는 복수의 분할 촬영 영역의 상단, 하단, 및 좌우의 경계면을 나타내는 그 래픽 UI를 디스플레이한다. 본 개시의 일 실시예에서, X선 영상 장치는 대상체 이미지를 디스플레이하고, 대상체 이미지 상에 복수의 분할 촬영 영역의 상단, 하단, 및 좌우의 경계면을 나타내는 복수의 가이드라인을 오버레이하여 디스플레이할 수 있다. 본 개시의 일 실시예에서, X선 영상 장치는 복수의 분할 촬영 영역의 설정 완료에 관한 정보를 사용자에게 제공하는 음성 또는 알림음을 출력할 수 있다. X선 영상 장치는 복수의 분할 촬영 영역을 X선 촬영함으로써 복수의 X선 분할 촬영 이미지를 획득하고, 획 득된 복수의 X선 분할 촬영 이미지를 스티칭함으로써 X선 촬영 대상 영역에 관한 X선 이미지를 획득할 수 있다. 도 16은 본 개시의 일 실시예에 따른 X선 영상 장치가 촬영 프로토콜에 따라 분할 촬영 영역을 결정하고, 결정된 분할 촬영 영역을 나타내는 그래픽 UI를 디스플레이하는 동작을 도시한 도면이다. X선 영상 장치의 프로세서는 대상체 이미지로부터 촬영 프로토콜을 인식할 수 있다. 촬영 프로토콜은 예를 들어, 홀 스파인(Whole Spine) 프로토콜, 롱 본(Long Bone) 프로토콜, 또는 익스트리미티(Extremity) 프 로토콜을 포함할 수 있으나, 이에 한정되는 것은 아니다. 프로세서는 촬영 프로토콜에 기초하여 대상체 이 미지로부터 촬영 대상 부위를 인식할 수 있다. 예를 들어, 홀 스파인 프로토콜의 경우 프로세서는 대상체 이미지 중 귀에서부터 골반 아래까지의 부위, 예를 들어, 머리, 어깨, 팔꿈치, 손, 허리 등을 촬영 대상 부위로 인식할 수 있다. 예를 들어, 롱 본 프로토콜의 경우 프로세서는 허리에서부터 발끝까지의 부위를 촬영 대 상 부위로 인식하고, 익스트리미티 프로토콜의 경우 프로세서는 얼굴, 손, 또는 발 등의 부위를 촬영 대상 부위로 인식할 수 있다. 프로세서는 인식된 촬영 대상 부위에 관한 정보를 대상체 이미지와 함께 인공지능 모델에 입력하고, 인공지능 모델을 이용하는 추론(inferencing)을 수행할 수 있다. 인공지능 모델은 대상체에 관한 복 수의 이미지를 입력 데이터로 적용하고, 촬영 프로토콜에 따라 스티칭된 분할 촬영 영역을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식으로 학습된(trained) 심층 신경망 모델(deep neural network)일 수 있다. 분할 촬영 영역의 정답값은 촬영 프로토콜에 따라 다르게 결정될 수 있다. 예를 들 어, 촬영 프로토콜 중 홀 스파인(Whole Spine) 프로토콜의 경우 신체 부위 중 귀에서부터 골반 아래까지 부위 중 머리, 어깨, 팔꿈치, 손, 허리 등이 부위를 분할한 영역으로 결정되고, 롱 본(Long Bone) 프로토콜의 경우 허리에서부터 발끝까지의 부위를 분할한 영역으로 결정될 수 있다. 다른 예를 들어, 머리(Skull), 손, 또는 발 과 같은 익스트리미티 프로토콜(Extremity protocol)의 경우, 얼굴, 손, 또는 발 등 신체 고유의 특성을 갖는 부위를 분할한 영역으로 결정될 수 있다. 프로세서는 심층 신경망 모델을 이용하는 추론을 통해 대상체 이 미지로부터 촬영 대상 부위를 인식하고, 분할 촬영 영역을 표시할 수 있다. 심층 신경망 모델을 이용하는 추론 과정에서, 심층 신경망 모델의 데이터가 암호화되어 있는 경우 프로세서 는 암호화되어 데이터를 복호화(decoding)할 수 있다. 심층 신경망 모델의 추론 결과, 촬영 대상 영역이 복수의 후보들로 출력되는 경우 프로세서는 복수의 후보들 중 가장 높은 신뢰도(confidence)를 갖는 후보 를 선택하여 최종 촬영 대상 영역으로 결정할 수 있다. 도 16에 도시된 실시예를 참조하면, 프로세서는 제1 대상체 이미지로부터 제1 프로토콜을 인식할 수 있다. 프로세서는 제1 대상체 이미지 및 인식된 제1 프로토콜을 인공지능 모델에 입력하고, 인 공지능 모델을 이용하는 추론을 통해 제1 촬영 대상 영역을 인식하고, 제1 촬영 대상 영역를 복수의 분할 촬영 영역으로 구분하는 복수의 가이드라인(1610-1 내지 1610-3)을 획득할 수 있다. 예를 들어, 제 1 프로토콜은 홀 스파인 프로토콜이고, 제1 촬영 대상 영역은 신체 부위 중 귀에서부터 골반 아래까지의 부위를 포함할 수 있다. 복수의 가이드라인(1610-1 내지 1610-3)은 귀에서부터 골반 아래까지의 부위 중 머리, 어깨, 팔꿈치, 허리 등의 부위를 분할한 복수의 영역 각각의 상단 및 하단을 지시하는 그래픽 UI일 수 있다. 프 로세서는 디스플레이부 상에 제1 대상체 이미지를 디스플레이하고, 제1 대상체 이미지 상에 제1 촬영 대상 영역 및 복수의 가이드라인(1610-1 내지 1610-3)을 오버레이하여 디스플레이할 수 있 다. 마찬가지 방식으로, 프로세서는 제2 대상체 이미지로부터 제2 프로토콜을 인식하고, 인공지능 모델 을 이용하는 추론을 통해 제2 대상체 이미지로부터 제2 프로토콜에 대응되는 제2 촬영 대상 영역 을 인식하며, 제2 촬영 대상 영역을 복수의 분할 촬영 영역으로 구분하는 복수의 가이드라인(1612-1 내지 1612-4)을 획득할 수 있다. 예를 들어, 제2 프로토콜은 롱 본 프로토콜이고, 제2 촬영 대상 영역 은 신체 부위 중 허리에서부터 발끝까지의 부위를 포함할 수 있다. 프로세서는 디스플레이부 상에 디 스플레이된 제2 대상체 이미지 상에 제2 촬영 대상 영역 및 복수의 가이드라인(1612-1 내지 1612- 4)을 오버레이하여 디스플레이할 수 있다. 프로세서는 제3 대상체 이미지로부터 제3 프로토콜을 인 식하고, 인공지능 모델을 이용하는 추론을 통해 제3 대상체 이미지로부터 제3 프로토콜에 대응되는 제3 촬영 대상 영역을 인식하며, 제3 촬영 대상 영역을 복수의 분할 촬영 영역으로 구분하는 복수 의 가이드라인(1614-1 내지 1614-5)을 획득할 수 있다. 예를 들어, 제3 프로토콜은 익스트리미티 프로토콜이고, 제3 촬영 대상 영역은 신체 부위 중 어깨에서부터 손 끝까지의 부위를 포함할 수 있다. 프로세서는 디스플레이부 상에 디스플레이된 제3 대상체 이미지 상에 제3 촬영 대상 영역 및 복수의 가이 드라인(1614-1 내지 1614-5)을 오버레이하여 디스플레이할 수 있다. 도 16에 도시된 실시예에서, X선 영상 장치는 촬영 프로토콜 별로 촬영 대상 부위를 인식하고, 인공지능 모델을 이용하는 추론을 통해 촬영 프로토콜 별로 촬영 대상 영역(1620, 1622, 1624)을 인식하는 바, 촬영 프로토콜에 따른 스티칭 X선 촬영을 자동화하고, 정확하고 안정적인 촬영 대상 영역(1620, 1622, 1624)을 검출 할 수 있다. 따라서, 사용자 편의성이 향상되고, 스티칭 X선 촬영의 시간이 단축될 수 있다. 도 17a는 본 개시의 일 실시예에 따른 X선 영상 장치가 사용자 입력에 기초하여 분할 촬영 영역의 위치, 크기, 및 형태 중 적어도 하나를 변경하는 동작을 도시한 도면이다. 도 17a를 참조하면, 디스플레이부 상에는 대상체 이미지(1700a)가 디스플레이되고, 복수의 분할 촬영 영역 의 상단 및 하단을 나타내는 복수의 가이드라인(1710S, 1710-1 내지 1710-4)이 대상체 이미지(1700a) 상에 디스 플레이될 수 있다. 사용자는 복수의 가이드라인(1710S, 1710-1 내지 1710-4) 중 적어도 하나의 가이드라인의 위 치를 조정할 수 있다. X선 영상 장치의 사용자 입력 인터페이스(360, 도 14 참조)는 복수의 가이드라인 (1710S, 1710-1 내지 1710-4) 중 적어도 하나의 가이드라인의 위치를 조정하는 사용자 입력을 수신할 수 있다. 본 개시의 일 실시예에서, 사용자 입력 인터페이스는 터치패드를 포함하는 터치스크린으로 구성되고, 이 경우 사용자 입력 인터페이스는 디스플레이부와 통합되는 구성 요소일 수 있다. 사용자 입력 인터페 이스는 복수의 가이드라인(1710S, 1710-1 내지 1710-4) 중 적어도 하나의 가이드라인의 위치를 조정하는 사용자의 터치 입력을 수신할 수 있다. 그러나, 이에 한정되는 것은 아니고, 사용자 입력 인터페이스는 키 패드(key pad), 하드웨어 버튼, 마우스, 조그 스위치, 또는 조그 다이얼 등을 통해 복수의 가이드라인(1710S, 1710-1 내지 1710-4) 중 적어도 하나의 가이드라인의 위치를 조정하는 사용자 입력을 수신할 수 있다. 도 17a에 도시된 실시예에서, 사용자 입력 인터페이스는 복수의 가이드라인(1710S, 1710-1 내지 1710-4) 중 제1 분 할 촬영 영역의 상단을 나타내는 상단 인디케이터(1710S)의 위치를 아래 방향으로 조정하는 사용자 입력을 수신 할 수 있다. 사용자 입력이 수신됨에 따라 X선 영상 장치의 프로세서(340, 도 14 참조)는 적어도 하나의 가이드라인의 위치를 변경하고, 변경된 적어도 하나의 가이드라인에 기초하여 복수의 분할 촬영 영역의 위치, 크기, 및 형태 중 적어도 하나를 변경할 수 있다. 도 17a에 도시된 실시예에서, 프로세서는 사용자 입력에 의해 위치가 조정된 상단 인디케이터(1710a)에 기초하여 복수의 분할 초라영 영역의 크기 및 형태를 변경할 수 있다. 프로세 서는 위치가 조정된 상단 인디케이터(1710a)와 제4 가이드 라인(1710-4) 사이의 영역을 균등하게 분할함으 로써, 복수의 분할 촬영 영역의 크기 및 위치를 변경할 수 있다. 프로세서는 변경된 상단 인디케이터 (1710a) 및 변경된 복수의 분할 촬영 영역을 디스플레이부 상에 디스플레이할 수 있다. 본 개시의 일 실시예에서, 사용자 입력에 의해 적어도 하나의 가이드라인의 위치가 조정됨에 따라 프로세서 는 분할 촬영의 횟수를 변경할 수 있다. 예를 들어, 상단 인디케이터(1710S)의 위치를 아래 방향으로 조정 하는 사용자 입력이 수신되는 경우 프로세서는 분할 촬영의 횟수를 감소시킬 수 있다. 다른 예를 들어, 상 단 인디케이터(1710S)의 위치를 위 방향으로 조정하거나 또는 제4 가이드라인(1710-4)의 위치를 아래 방향으로 조정하는 사용자 입력이 수신되는 경우 프로세서는 분할 촬영의 횟수를 증가시킬 수 있다. 예를 들어, 프 로세서는 분할 촬영 횟수를 4에서 3으로 감소시키고, 사용자 입력에 의해 위치가 조정된 상단 인디케이터 (1710a)와 제4 가이드라인(1710-4) 사이의 영역을 3개의 영역으로 균등하게 분할할 수 있다. 그러나, 이에 한정되는 것은 아니고, 본 개시의 다른 실시예에서 프로세서는 상단 인디케이터(1710a)와 제 4 가이드라인(1710-4) 사이의 영역을 균등하지 않게 분할할 수도 있다. 예를 들어, 상단 인디케이터(1710a)의 위치가 조정됨에 따라 프로세서는 제1 분할 촬영 영역의 크기 및 형태만 변경할 수도 있다. 도 17b는 본 개시의 일 실시예에 따른 X선 영상 장치가 사용자 입력에 기초하여 분할 촬영 영역의 위치, 크기, 및 형태 중 적어도 하나를 변경하는 동작을 도시한 도면이다. 도 17b는 복수의 가이드라인(1710S, 1710-1 내지 1710-4) 중 사용자 입력에 의해 조정되는 가이드라인이 제3 가 이드라인(1710-3)이고, 사용자 입력에 의해 제3 가이드라인(1710-3)의 위치가 위 방향으로 조정된다는 점을 제 외하고는 도 17a와 동일하므로, 중복되는 설명은 생략한다. 도 17b를 참조하면, X선 영상 장치의 사용자 입력 인터페이스(360, 도 14 참조)는 복수의 가이드라인 (1710S, 1710-1 내지 1710-4) 중 제4 분할 촬영 영역의 하단을 나타내는 제4 가이드라인(1710-4)의 위치를 위 방향으로 조정하는 사용자 입력을 수신할 수 있다. 사용자 입력이 수신됨에 따라 X선 영상 장치의 프로세 서(340, 도 14 참조)는 사용자 입력에 의해 위치가 조정된 제4 가이드라인(1710b)에 기초하여 복수의 분할 촬영 영역의 크기 및 형태를 변경할 수 있다. 프로세서는 위치가 조정된 제4 가이드라인(1710b)에 기초하여 제1 가이드라인(1710-1)과 제4 가이드라인(1710b) 사이의 영역을 균등하게 분할함으로써, 복수의 분할 촬영 영역의 크기 및 위치를 변경할 수 있다. 그러나, 이에 한정되는 것은 아니고, 프로세서는 제1 가이드라인(1710-1)과 제4 가이드라인(1710b) 사이의 영역을 균등하지 않게 분할할 수도 있다. 예를 들어, 프로세서는 제3 가이드라인(1710b)의 위치가 위 방 향으로 조정됨에 따라 제4 분할 촬영 영역의 크기 및 형태만 변경할 수 있다. 도 17a 및 도 17b에 도시된 실시예에 따른 X선 영상 장치는 사용자 입력에 의해 복수의 가이드라인(1710S, 1710-1 내지 1710-4)의 위치를 조정함으로써, 인공지능 모델(352, 도 13 및 도 14 참조)에 의해 획득된 복수의 분할 촬영 영역의 위치, 크기, 및 형태 중 적어도 하나를 변경할 수 있다. 이를 통해, 본 개시의 일 실시예에 따른 X선 영상 장치는 인공지능 모델에 의해 정확하지 않은 분할 촬영 영역이 획득되거나 분할 촬영 영역을 사용자가 직접 변경 또는 조정하여야 하는 특수한 경우, 사용자가 수동으로 복수의 분할 촬영 영역의 위 치, 크기, 및 형태를 조정할 수 있도록 함으로써 사용자 편의성을 향상시키고, 정확한 X선 이미지 촬영을 가능 하게 할 수 있다. 도 18은 본 개시의 일 실시예에 따른 X선 영상 장치가 사용자 입력에 기초하여 분할 촬영 영역의 마진 (margin)을 결정하는 동작을 도시한 도면이다. 도 18을 참조하면, X선 영상 장치는 디스플레이부 상에 대상체 이미지를 디스플레이하고, 대상 체 이미지 상에 X선 이미지 촬영을 나타내는 그래픽 UI인 촬영 대상 영역을 오버레이하여 디스플레 이할 수 있다. X선 영상 장치의 사용자 입력 인터페이스(360, 도 14 참조)는 상하좌우 방향에 따른 복수의 마진(dm1 내지 dm4) 중 어느 하나의 마진을 조절하여 X선 촬영 영역을 결정하는 사용자 입력을 수신할 수 있다. 본 개시의 일 실시예에서, 사용자 입력 인터페이스는 터치패드를 포함하는 터치스크린으로 구성되고, 이 경우 사용자 입력 인터페이스는 디스플레이부와 통합되는 구성 요소일 수 있다. 사용자 입력 인터페이스는 터치스크린 상에 디스플레이된 그래픽 UI인 촬영 대상 영역의 상하좌우 방향에 따른 복수의 마진(dm1 내지 dm4) 중 어느 하나의 마진을 조절하는 사용자의 터치 입력을 수신할 수 있다. 그러나, 이에 한정되는 것은 아니고, 사용자 입력 인터페이스는 키 패드(key pad), 하드웨어 버튼, 마우스, 조그 스위치, 또는 조그 다이얼 등을 통해 상하좌우 방향에 따른 복수의 마진(dm1 내지 dm4) 중 어느 하나의 마진을 조 절하는 사용자 입력을 수신할 수도 있다. X선 영상 장치의 프로세서(340, 도 14 참조)는 사용자 입력 인터페이스를 통해 수신된 사용자 입력에 기초하여 상하좌우 방향에 따른 복수의 마진(dm1 내지 dm4) 중 어느 하나의 마진의 크기를 조절할 수 있다. 프로 세서는 크기가 조절된 마진에 기초하여 X선 촬영 영역을 설정할 수 있다. 도 19는 본 개시의 일 실시예에 따른 X선 영상 장치가 깊이 측정 장치를 이용하여 대상체의 포지 셔닝(positioning)을 검출하는 동작을 도시한 도면이다. 도 19를 참조하면, X선 영상 장치는 카메라, X선 조사부, X선 디텍터, 및 깊이 측정 장치 를 포함할 수 있다. 도 19에 도시된 카메라, X선 조사부, 및 X선 디텍터는 도 4에서 설명 한 카메라(110, 도 4 참조), X선 조사부(120, 도 4 참조), 및 X선 디텍터(130, 도 4 참조)와 동일하므로, 중복 되는 설명은 생략한다. 깊이 측정 장치는 X선 조사부와 대상체 간의 거리를 측정하도록 구성된다. 본 개시의 일 실시예 에서, 깊이 측정 장치는 스테레오 카메라(stereo-type camera), ToF 카메라(time of flight camera), 및 레이저 거리 측정기 중 적어도 하나를 포함할 수 있다. X선 영상 장치의 프로세서(340, 도 14 참조)는 깊이 측정 장치를 이용하여 X선 조사부와 대상체 간의 거리를 측정함으로써 환자 포지셔닝을 검출 할 수 있다. 프로세서는 환자 포지셔닝을 검출한 이후, 카메라를 통해 대상체를 촬영하여 대상체 이미지를 획득하고, 인공지능 모델(352, 도 13 및 도 14 참조)을 이용하여 대상체 이미지를 분석함으로써 복수 의 분할 촬영 영역을 획득할 수 있다. 본 개시의 일 실시예에서, 깊이 측정 장치를 통해 획득된 X선 조사부와 대상체 간의 거리, 즉 SID(source to image distance)가 기 설정된 범위를 벗어나는 경우, 프로세서는 대상체가 촬영 위치 에 정상적으로 포지셔닝되어 잇지 않다고 판단할 수 있다. 이 경우, 프로세서는 촬영 프로토콜과는 관계없 이 디폴트(default)로 설정된 분할 촬영 영역을 디스플레이부(370, 도 13 및 도 14 참조) 상에 출력할 수 있다. 도 20은 본 개시의 일 실시예에 따른 X선 영상 장치 및 워크스테이션의 구성 요소를 도시한 블록도이 다. 도 20에 도시된 X선 영상 장치는 실링 타입으로 구현될 수 있다. 도 20을 참조하면, X선 영상 장치는 카메라, X선 조사부, X선 디텍터, 프로세서, 사용자 입력 인터페이스, 디스플레이부 , 및 통신 인터페이스를 포함할 수 있다. 도 20에 도시된 X선 영상 장치는 메모리(350, 도 14 참조)를 포함하지 않고, 통신 인터페이스를 더 포함한다는 점을 제외하고는 도 14에 도시된 X선 영상 장치 (300, 도 14 참조)와 동일하므로, 중복되는 설명은 생략한다. 통신 인터페이스는 유선 또는 무선 통신 네트워크를 통해 워크스테이션과 데이터를 송수신하며, 데이 터를 처리할 수 있다. 통신 인터페이스는 예를 들어, 유선 랜, 무선 랜(Wireless LAN), 와이파이(Wi-Fi), 블루투스(Bluetooth), 지그비(zigbee), WFD(Wi-Fi Direct), 적외선 통신(IrDA, infrared Data Association), BLE(Bluetooth Low Energy), NFC(Near Field Communication), 와이브로(Wireless Broadband Internet, Wibro), 와이맥스(World Interoperability for Microwave Access, WiMAX), SWAP(Shared Wireless Access Protocol), 와이기그(Wireless Gigabit Allicance, WiGig) 및 RF 통신을 포함하는 데이터 통신 방식 중 적어도 하나를 이용하여 워크스테이션과 데이터 통신을 수행할 수 있다. 본 개시의 일 실시예에서, 통신 인터페이스는 프로세서의 제어에 의해, 카메라를 통해 대상체를 촬영하여 획득된 대상체 이미지를 워크스테이션에 전송하고, 워크스테이션으로부터 스티칭 X선 촬영 을 위한 분할 촬영 영역에 관한 데이터를 수신할 수 있다. X선 영상 장치는 수신된 분할 촬영 영역에 관한 데이터에 기초하여, 대상체 이미지 상에 분할 촬영 영역을 나타내는 복수의 가이드라인을 디스플레이부를 통해 디스플레이할 수 있다. 워크스테이션은 X선 영상 장치와 통신하는 통신 인터페이스, 적어도 하나의 명령어들 (instructions) 또는 프로그램 코드를 저장하는 메모리, 및 메모리에 저장된 명령어들 또는 프로그램 코드를 실행하도록 구성되는 프로세서를 포함할 수 있다. 워크스테이션의 메모리에는 인공지능 모델이 저장되어 있을 수 있다. 워크스테이션에 저장 된 인공지능 모델은 도 13 및 도 14에 도시되고, 설명된 인공지능 모델(352, 도 13 및 도 14 참조)와는 저 장 위치를 제외하고는 모두 동일하므로, 중복되는 설명은 생략한다. 워크스테이션은 통신 인터페이스(41 0)를 통해 X선 영상 장치로부터 대상체 이미지의 이미지 데이터를 수신할 수 있다. 워크스테이션의 프로세서는 인공지능 모델에 대상체 이미지를 입력하고, 인공지능 모델을 이용하는 추론 (inferencing)을 통해 스티칭 X선 촬영을 위한 복수의 분할 촬영 영역을 획득할 수 있다. 본 개시의 일 실시예 에서, 워크스테이션은 통신 인터페이스를 통해 X선 영상 장치로부터 촬영 프로토콜에 관한 정보 를 수신하거나, 또는 입력 인터페이스를 통해 촬영 프로토콜을 설정하는 사용자 입력을 수신할 수 있다. 프로세서는 촬영 프로토콜에 기초하여 대상체 이미지로부터 촬영 대상 부위를 인식하고, 인식된 촬영 대상 부위에 관한 정보를 대상체 이미지와 함께 인공지능 모델에 입력하고, 인공지능 모델을 이용하는 추 론을 통해 복수의 분할 촬영 영역을 획득할 수 있다. 워크스테이션의 프로세서는 통신 인터페이스를 제어하여, 분할 촬영 영역에 관한 데이터를 X선 영상 장치에 전송할 수 있다. 일반적으로, X선 영상 장치의 메모리(350, 도 14 참조)의 저장 용량, 프로세서의 연산 처리 속도 등 은 워크스테이션에 비하여 제한적일 수 있다. 따라서, 워크스테이션은 대용량 데이터의 저장 및 대용 량의 연산량이 필요한 동작(예를 들어, 인공지능 모델을 이용하는 추론을 통해 분할 촬영 영역을 획득)을 수행한 이후, 통신 네트워크를 통하여 필요한 데이터(예를 들어, 분할 촬영 영역에 관한 데이터)를 X선 영상 장치에 전송할 수 있다. 이와 같은 방식으로, X선 영상 장치는 대용량의 메모리 및 빠른 연산 능력을 갖는 프로세서 없이도 워크스테이션으로부터 분할 촬영 영역에 관한 데이터를 수신하고, 분할 촬영 영역을 나타내는 복수의 가이드라인을 디스플레이함으로써, 분할 촬영 영역을 획득하는데 소요되는 프로세싱 타임 (processing time)을 단축시키고, 분할 촬영 영역의 정확도를 향상시킬 수 있다. 본 개시는 대상체의 움직임을 검출하는 X선 영상 장치를 제공한다. 본 개시의 일 실시예에 따른 X선 영상 장치는 X선을 발생하고, 대상체에 X선을 조사하는 X선 조사부, X선 조사부에 의해 조사되고, 대 상체를 투과하는 X선을 검출하는 X선 디텍터, X선 디텍터 앞에 포지셔닝된 대상체를 촬영함으로써 대 상체 이미지를 획득하는 카메라, 디스플레이부, 및 적어도 하나의 프로세서를 포함할 수 있다. 상기 적어도 하나의 프로세서는 인공지능 모델을 이용하여 상기 대상체 이미지를 분석하여, 대상체 이미지 로부터 대상체의 움직임을 검출할 수 있다. 상기 적어도 하나의 프로세서는 대상체의 움직임 검출 결과를 사용자에게 알리는 알림 신호를 디스플레이부 상에 출력할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 카메라를 이용하여 X선 디텍터 앞에 포지셔닝 완료된 대상체를 촬영하여 기준 이미지(reference image)를 획득하고, 기준 이미지를 획득한 이후 대 상체를 후속 촬영하여 이미지 프레임을 획득할 수 있다. 상기 적어도 하나의 프로세서는 인공지능 모델을 이용하는 분석을 통해 기준 이미지로부터 인식된 대상체와 이미지 프레임으로부터 인식된 대상체를 비교함으로 써, 대상체의 움직임을 검출할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 인공지능 모델 중 자기 조직화 지도(Self- Organizing Map)를 이용하여 기준 이미지로부터 인식된 대상체를 나타내는 픽셀들에 대한 가중치(weights)를 획 득할 수 있다. 상기 적어도 하나의 프로세서는 가중치를 이용하여 이미지 프레임으로부터 인식된 대상체와 기준 이미지로부터 인식된 대상체를 비교하여 대상체의 움직임을 검출할 수 있다. 상기 적어도 하나의 프로세서 는 검출 결과를 이용하여, 기준 이미지 및 가중치를 업데이트할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 인공지능 모델 중 학습된 심층 신경망 모델 (trained deep neural network)을 이용하는 추론(inferencing)을 통해 기준 이미지로부터 대상체의 주요 부위 (landmark)에 관한 복수의 제1 키 포인트를 추출할 수 있다. 상기 적어도 하나의 프로세서는 추출된 복수 의 제1 키 포인트를 이미지 프레임으로부터 추출된 대상체의 복수의 제2 키 포인트와 비교하여 키 포인트들 간 의 차이값을 산출할 수 있다. 상기 적어도 하나의 프로세서는 산출된 차이값을 기 설정된 임계치와 비교하 여, 대상체의 움직임을 검출할 수 있다. 본 개시의 일 실시예에서, 상기 심층 신경망 모델은 기 획득된 복수의 이미지를 입력 데이터로 적용하고, 신체 주요 부위의 키 포인트들의 위치 좌표값들을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식을 통해 학습된(trained) 모델일 수 있다. 본 개시의 일 실시예에서, 상기 X선 영상 장치는 환자 포지셔닝 완료 후 움직임 검출 모드를 선택하는 사 용자 입력을 수신하는 사용자 입력 인터페이스를 더 포함할 수 있다. 상기 적어도 하나의 프로세서는 수신 된 사용자 입력에 기초하여 움직임 검출 모드를 실행하고, 움직임 검출 모드가 실행됨에 응답하여 대상체의 움 직임을 검출할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 환자 포지셔닝 완료 후 기 설정된 시간이 경과 함에 따라 움직임 검출 모드를 실행할 수 있다. 상기 적어도 하나의 프로세서는 움직임 검출 모드가 실행 됨에 응답하여 대상체의 움직임을 검출할 수 있다. 본 개시의 일 실시예에서, 상기 X선 영상 장치는 스테레오 카메라(stereo-type camera), ToF 카메라(time of flight camera), 및 레이저 거리 측정기 중 적어도 하나를 포함하는 깊이 측정 장치를 더 포함할 수 있 다. 상기 적어도 하나의 프로세서는 깊이 측정 장치를 이용하여 X선 조사부와 대상체 간의 거리 를 측정함으로써 환자 포지셔닝을 검출할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 대상체와 X선 조사부 간의 거리인 SID(Source to Image Distance), 대상체의 크기, 형태, 및 촬영 프로토콜 중 적어도 하나에 기초하여 움직임 검 출 민감도를 설정할 수 있다. 본 개시의 일 실시예에서, 상기 디스플레이부는 대상체의 움직임을 나타내는 기 설정된 컬러를 갖는 그래 픽 UI(graphic user interface)를 디스플레이할 수 있다. 본 개시의 일 실시예에서, 상기 X선 영상 장치는 대상체의 움직임에 관한 정보를 사용자에게 알리는 음성 및 알림음 중 적어도 하나의 음향 신호를 출력하는 스피커를 더 포함할 수 있다. 본 개시는 X선 영상 장치의 동작 방법을 제공한다. 본 개시의 일 실시예에 따른 X선 영상 장치의 동 작 방법은 카메라를 이용하여 대상체를 촬영함으로써 대상체의 이미지 데이터를 획득하는 단계(S610)를 포 함할 수 있다. 본 개시의 일 실시예에 따른 X선 영상 장치의 동작 방법은 인공지능 모델을 이용하여 이미 지 데이터를 분석하여, 이미지 데이터로부터 대상체의 움직임을 검출하는 단계(S620)를 포함할 수 있다. 본 개 시의 일 실시예에 따른 X선 영상 장치의 동작 방법은 대상체의 움직임 검출 결과를 사용자에게 알리는 알 림 신호를 출력하는 단계(S630)를 포함할 수 있다. 본 개시의 일 실시예에서, 상기 이미지 데이터를 획득하는 단계(S610)는 카메라를 이용하여, X선 디텍터 앞에 포지셔닝 완료된 대상체를 촬영하여 기준 이미지(reference image)를 획득하는 단계, 및 기준 이미 지를 획득한 이후 대상체를 후속 촬영하여 이미지 프레임을 획득하는 단계를 포함할 수 있다. 상기 대상체의 움 직임을 검출하는 단계(S620)에서, 상기 X선 영상 장치는 인공지능 모델을 이용하는 분석을 통해 기준 이미 지로부터 인식된 대상체와 이미지 프레임으로부터 인식된 대상체를 비교함으로써, 대상체의 움직임을 검출할 수 있다. 본 개시의 일 실시예에서, 상기 대상체의 움직임을 검출하는 단계(S620)는 자기 조직화 지도(Self-Organizing Map)를 이용하여 기준 이미지로부터 가중치(weights)를 획득하는 단계, 가중치를 이용하여 이미지 프레임 으로부터 인식된 대상체와 기준 이미지로부터 인식된 대상체를 비교하여 대상체의 움직임을 검출하는 단계 (S820)를 포함할 수 있다. 상기 대상체의 움직임을 검출하는 단계(S620)는 검출 결과를 이용하여, 기준 이미지 및 가중치를 업데이트하는 단계(S830)를 포함할 수 있다. 본 개시의 일 실시예에서, 상기 대상체의 움직임을 검출하는 단계(S620)는 학습된 심층 신경망 모델(trained deep neural network)을 이용하는 추론(inferencing)을 통해 기준 이미지로부터 대상체의 주요 부위(landmar k)에 관한 복수의 제1 키 포인트를 추출하는 단계(S910), 추출된 복수의 제1 키 포인트를 이미지 프레임으로부 터 추출된 대상체의 복수의 제2 키 포인트와 비교하여 키 포인트들 간의 차이값을 산출하는 단계(S920), 및 산 출된 차이값을 기 설정된 임계치와 비교하여 대상체의 움직임을 검출하는 단계를 포함할 수 있다. 본 개시의 일 실시예에서, 상기 X선 영상 장치의 동작 방법은 환자 포지셔닝 완료 후 움직임 검출 모드를 선택하는 사용자 입력을 수신하는 단계를 더 포함할 수 있다. 상기 대상체의 움직임을 검출하는 단계(S620)는 사용자 입력에 기초하여 움직임 검출 모드를 실행하는 단계, 및 움직임 검출 모드가 실행됨에 응답하여 대상체 의 움직임을 검출하는 단계를 포함할 수 있다. 본 개시의 일 실시예에서, 상기 X선 영상 장치의 동작 방법은 환자 포지셔닝 완료 후 기 설정된 시간이 경 과함에 따라 움직임 검출 모드를 실행하는 단계를 더 포함할 수 있다. 상기 대상체의 움직임을 검출하는 단계 (S620)에서, 상기 X선 영상 장치는 움직임 검출 모드가 실행됨에 응답하여 대상체의 움직임을 검출할 수 있다. 본 개시의 일 실시예에서, 상기 X선 영상 장치의 동작 방법은 대상체와 X선 조사부 간의 거리인 SID(Source to Image Distance), 대상체의 크기, 형태, 및 촬영 프로토콜 중 적어도 하나에 기초하여 움직임 검 출 민감도를 설정하는 단계를 더 포함할 수 있다. 본 개시의 일 실시예에서, 상기 알림 신호를 출력하는 단계(S630)는 대상체의 움직임을 나타내는 기 설정된 컬 러를 갖는 그래픽 UI(graphic user interface)를 디스플레이하는 단계를 포함할 수 있다. 본 개시의 일 실시예에서, 상기 알림 신호를 출력하는 단계(S630)는 대상체의 움직임에 관한 정보를 사용자에게 알리는 음성 및 알림음 중 적어도 하나의 음향 신호를 출력하는 단계를 포함할 수 있다. 본 개시는 스티칭 X선 촬영을 수행하는 X선 영상 장치를 제공한다. 본 개시의 일 실시예에 따른 X선 영상 장치는 X선 조사부에 의해 조사되고, 대상체를 투과하는 X선을 검출하는 X선 디텍터, X선 디텍 터 앞에 포지셔닝된 대상체를 촬영함으로써 대상체 이미지를 획득하는 카메라, 디스플레이부, 및 적어도 하나의 프로세서를 포함할 수 있다. 상기 적어도 하나의 프로세서는 대상체 이미지를 학습 된(trained) 인공지능 모델에 입력하고, 인공지능 모델을 이용하는 추론(inferencing)을 통해 대상체의 스티칭 X선 촬영을 위한 복수의 분할 촬영 영역을 획득할 수 있다. 상기 적어도 하나의 프로세서는 복수의 분할 촬영 영역 각각의 상단, 하단, 및 좌우의 경계면을 나타내는 복수의 가이드라인을 디스플레이부 상에 디스플레이할 수 있다. 본 개시의 일 실시예에서, 상기 인공지능 모델은 기 획득된 복수의 이미지를 입력 데이터로 적용하고, 촬영 프 로토콜에 따라 스티칭된 분할 촬영 영역을 정답값(ground truth)으로 적용하는 지도 학습(supervised learning) 방식으로 학습된(trained) 심층 신경망 모델(deep neural network)일 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 촬영 프로토콜에 기초하여 상기 대상체 이미지 로부터 촬영 대상 부위를 인식할 수 있다. 상기 적어도 하나의 프로세서는 인식된 촬영 대상 부위에 관한 정보를 대상체 이미지와 함께 인공지능 모델에 입력하고, 인공지능 모델을 이용하는 추론을 통해 복수의 분할 촬영 영역을 획득할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 복수의 분할 촬영 영역의 크기를 X선 디텍터 의 크기 보다 작은 크기로 조절할 수 있다. 본 개시의 일 실시예에서, 상기 X선 영상 장치는 복수의 가이드라인 중 적어도 하나의 위치를 조정 (adjust)하는 사용자 입력을 수신하는 사용자 입력 인터페이스를 더 포함할 수 있다. 상기 적어도 하나의 프로세서는 수신된 사용자 입력에 기초하여 복수의 가이드라인 중 적어도 하나의 위치를 조정함으로써, 복 수의 분할 촬영 영역의 위치, 크기, 및 형태 중 적어도 하나를 변경할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 사용자 입력에 의해 설정된 마진(margin) 정보 에 기초하여 복수의 분할 촬영 영역의 상하좌우 마진 크기를 결정할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 대상체 이미지 상에 복수의 분할 촬영 영역을 나타내는 그래픽 사용자 UI를 오버레이하여 디스플레이하도록 디스플레이부를 제어할 수 있다. 본 개시의 일 실시예에서, 상기 X선 영상 장치는 복수의 분할 촬영 영역의 설정 완료에 관한 정보를 음성 또는 알림음으로 출력하는 스피커(speaker)를 더 포함할 수 있다. 본 개시의 일 실시예에서, 상기 X선 영상 장치는 스테레오 타입 카메라(stereo-type camera), ToF(time of flight) 카메라, 및 레이저 거리 측정기 중 적어도 하나를 포함하는 깊이 측정 장치를 더 포함할 수 있 다. 상기 적어도 하나의 프로세서는 깊이값 측정 장치를 이용하여 X선 조사부와 대상체 간의 거 리를 측정함으로써 X선 디텍터 앞의 대상체 포지셔닝을 검출할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 복수의 분할 촬영 영역을 X선 촬영함으로써 복 수의 X선 분할 촬영 이미지를 획득할 수 있다. 상기 적어도 하나의 프로세서는 획득된 복수의 X선 분할 촬 영 이미지를 스티칭함으로써 X선 촬영 대상 영역에 관한 X선 이미지를 획득할 수 있다. 본 개시는 컴퓨터로 읽을 수 있는 저장 매체를 포함하는 컴퓨터 프로그램 제품(Computer Program Product)를 제 공한다. 상기 저장 매체는 카메라를 이용하여 대상체를 촬영함으로써 대상체 이미지를 획득하는 동작; 인공지능 모델을 이용하여 대상체 이미지를 분석하여 대상체 이미지로부터 대상체의 움직임을 검출하는 동작; 및 대상체 의 움직임 검출 결과를 사용자에게 알리는 알림 신호를 출력하는 동작을 X선 영상 장치가 수행하기 위하여, X선 영상 장치에 의해 판독 가능한 명령어들(instructions)을 포함할 수 있다. 본 개시에서 설명된 X선 영상 장치에 의해 실행되는 프로그램은 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 프로그램은 컴퓨터로 읽을 수 있는 명령어들을 수행할 수 있는 모든 시스템에 의해 수행될 수 있다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령어(instruction), 또는 이들 중 하나 이상 의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어는, 컴퓨터로 읽을 수 있는 저장 매체(computer-readable storage media)에 저장된 명령어를 포함하 는 컴퓨터 프로그램으로 구현될 수 있다. 컴퓨터가 읽을 수 있는 기록 매체로는, 예를 들어 마그네틱 저장 매체 (예컨대, ROM(read-only memory), RAM(random-access memory), 플로피 디스크, 하드 디스크 등) 및 광학적 판 독 매체(예컨대, 시디롬(CD-ROM), 디브이디(DVD: Digital Versatile Disc)) 등이 있다. 컴퓨터가 읽을 수 있 는 기록 매체는 네트워크로 연결된 컴퓨터 시스템들에 분산되어, 분산 방식으로 컴퓨터가 판독 가능한 코드가 저장되고 실행될 수 있다. 매체는 컴퓨터에 의해 판독 가능하며, 메모리에 저장되고, 프로세서에서 실행될 수 있다. 컴퓨터로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비 일시적'은 저장매체가 신호(signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매 체에 반영구적 또는 임시적으로 저장되는 경우를 구분하지 않는다. 예를 들어, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 또한, 본 명세서에 개시된 실시예들에 따른 프로그램은 컴퓨터 프로그램 제품(computer program product)에 포 함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 소프트웨어 프로그램, 소프트웨어 프로그램이 저장된 컴퓨터로 읽을 수 있는 저장 매체 를 포함할 수 있다. 예를 들어, 컴퓨터 프로그램 제품은 X선 영상 장치의 제조사 또는 전자 마켓(예를 들 어, 삼성 갤럭시 스토어)을 통해 전자적으로 배포되는 소프트웨어 프로그램 형태의 상품(예를 들어, 다운로드 가능한 애플리케이션(downloadable application))을 포함할 수 있다. 전자적 배포를 위하여, 소프트웨어 프로그 램의 적어도 일부는 저장 매체에 저장되거나, 임시적으로 생성될 수 있다. 이 경우, 저장 매체는 X선 영상 장치 의 제조사의 서버, 전자 마켓의 서버, 또는 소프트웨어 프로그램을 임시적으로 저장하는 중계 서버의 저장 매체가 될 수 있다. 컴퓨터 프로그램 제품은, X선 영상 장치 및/또는 서버로 구성되는 시스템에서, 서버의 저장매체 또는 X선 영상 장치의 저장매체를 포함할 수 있다. 또는, X선 영상 장치와 통신 연결되는 제3 장치(예를 들어, 워크스테이션(200, 도 12 참조))가 존재하는 경우, 컴퓨터 프로그램 제품은 제3 장치의 저장매체를 포함할 수 있다. 또는, 컴퓨터 프로그램 제품은 X선 영상 장치로부터 제3 장치로 전송되거나, 제3 장치로부터 전자 장치로 전송되는 소프트웨어 프로그램 자체를 포함할 수 있다. 이 경우, X선 영상 장치 또는 제3 장치(예를 들어, 워크스테이션(200, 도 12 참조)) 중 하나가 컴퓨터 프 로그램 제품을 실행하여 개시된 실시예들에 따른 방법을 수행할 수 있다. 또는, X선 영상 장치 및 제3 장 치 중 적어도 하나 이상이 컴퓨터 프로그램 제품을 실행하여 개시된 실시예들에 따른 방법을 분산하여 실시할 수 있다. 예를 들면, X선 영상 장치가 메모리(150, 도 5 참조)에 저장된 컴퓨터 프로그램 제품을 실행하여, X선 영 상 장치와 통신 연결된 타 전자 장치가 개시된 실시예들에 따른 방법을 수행하도록 제어할 수 있다. 또 다른 예로, 제3 장치가 컴퓨터 프로그램 제품을 실행하여, 제3 장치와 통신 연결된 전자 장치가 개시된 실시 예에 따른 방법을 수행하도록 제어할 수 있다. 제3 장치가 컴퓨터 프로그램 제품을 실행하는 경우, 제3 장치는 X선 영상 장치로부터 컴퓨터 프로그램 제 품을 다운로드하고, 다운로드된 컴퓨터 프로그램 제품을 실행할 수 있다. 또는, 제3 장치는 프리로드(pre- load)된 상태로 제공된 컴퓨터 프로그램 제품을 실행하여 개시된 실시예들에 따른 방법을 수행할 수도 있다."}
{"patent_id": "10-2023-0025288", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 컴퓨터 시스템 또는 모듈 등의 구성요소들이 설명된 방법과 다른 형태로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17a 도면17b 도면18 도면19 도면20"}
{"patent_id": "10-2023-0025288", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 개시는, 다음의 자세한 설명과 그에 수반되는 도면들의 결합으로 쉽게 이해될 수 있으며, 참조 번호 (reference numerals)들은 구조적 구성요소(structural elements)를 의미한다. 도 1은 본 개시의 일 실시예에 따른 X선 영상 장치의 구성을 도시하는 외관도이다. 도 2는 본 개시의 일 실시예에 따른 X선 디텍터의 사시도이다. 도 3은 본 개시의 일 실시예에 따른 모바일 X선 디텍터를 포함하는 X선 영상 장치를 도시한 도면이다. 도 4는 본 개시의 일 실시예에 따른 X선 영상 장치가 카메라를 통해 획득된 이미지로부터 대상체의 움직임을 검 출하는 동작을 설명하기 위한 개념도이다. 도 5는 본 개시의 일 실시예에 따른 X선 영상 장치의 구성 요소를 도시한 블록도이다. 도 6은 본 개시의 일 실시예에 따른 X선 영상 장치가 카메라를 통해 획득된 이미지로부터 대상체의 움직임을 검 출하는 방법을 도시한 흐름도이다. 도 7은 본 개시의 일 실시예에 따른 X선 영상 장치가 기준 이미지와 후속 이미지 프레임을 비교하여 대상체의 움직임을 검출하는 동작을 설명하기 위한 도면이다. 도 8은 본 개시의 일 실시예에 따른 X선 영상 장치가 기계 학습(machine learning) 알고리즘을 이용하여 대상체 의 움직임을 검출하는 방법을 도시한 흐름도이다. 도 9는 본 개시의 일 실시예에 따른 X선 영상 장치가 학습된 심층 신경망 모델(pre-trained deep neural network)을 이용하여 대상체의 움직임을 검출하는 방법을 도시한 흐름도이다. 도 10은 본 개시의 일 실시예에 따른 X선 영상 장치가 학습된 심층 신경망 모델(pre-trained deep neural network)을 이용하여 대상체의 움직임을 검출하는 동작을 설명하기 위한 개념도이다. 도 11은 본 개시의 일 실시예에 따른 X선 영상 장치가 깊이 측정 장치를 이용하여 대상체의 포지셔닝 (positioning)을 검출하는 동작을 도시한 도면이다. 도 12는 본 개시의 일 실시예에 따른 X선 영상 장치 및 워크스테이션의 구성 요소를 도시한 블록도이다. 도 13은 본 개시의 일 실시예에 따른 X선 영상 장치가 카메라를 통해 획득된 이미지 상에 스티칭(stitching) X 선 촬영을 위한 분할 촬영 영역을 디스플레이하는 동작을 설명하기 위한 개념도이다. 도 14는 본 개시의 일 실시예에 따른 X선 영상 장치의 구성 요소를 도시한 블록도이다. 도 15는 본 개시의 일 실시예에 따른 X선 영상 장치가 카메라를 통해 획득된 이미지 상에 스티칭 X선 촬영을 위 한 분할 촬영 영역을 획득하고, 분할 촬영 영역을 나타내는 그래픽 UI(Graphic User Interface)를 디스플레이하 는 방법을 도시한 흐름도이다. 도 16은 본 개시의 일 실시예에 따른 X선 영상 장치가 촬영 프로토콜에 따라 분할 촬영 영역을 결정하고, 결정 된 분할 촬영 영역을 나타내는 그래픽 UI를 디스플레이하는 동작을 도시한 도면이다. 도 17a는 본 개시의 일 실시예에 따른 X선 영상 장치가 사용자 입력에 기초하여 분할 촬영 영역의 위치, 크기, 및 형태 중 적어도 하나를 변경하는 동작을 도시한 도면이다. 도 17b는 본 개시의 일 실시예에 따른 X선 영상 장치가 사용자 입력에 기초하여 분할 촬영 영역의 위치, 크기, 및 형태 중 적어도 하나를 변경하는 동작을 도시한 도면이다. 도 18은 본 개시의 일 실시예에 따른 X선 영상 장치가 사용자 입력에 기초하여 분할 촬영 영역의 마진(margin) 을 결정하는 동작을 도시한 도면이다. 도 19는 본 개시의 일 실시예에 따른 X선 영상 장치가 깊이 측정 장치를 이용하여 대상체의 포지셔닝 (positioning)을 검출하는 동작을 도시한 도면이다. 도 20은 본 개시의 일 실시예에 따른 X선 영상 장치 및 워크스테이션의 구성 요소를 도시한 블록도이다."}
