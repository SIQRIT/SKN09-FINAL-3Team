{"patent_id": "10-2019-0079332", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0003491", "출원번호": "10-2019-0079332", "발명의 명칭": "로봇 및 그의 구동 방법", "출원인": "엘지전자 주식회사", "발명자": "박지환"}}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇의 구동 방법으로서,상기 로봇의 감지 범위 내 위치한 기준 음원(Base Sound Source)에서 발생된 사운드를 복수의 마이크를 통해 입력받는 단계;상기 기준 음원 및 상기 로봇 사이의 측정된 거리 정보에 대응되는 기준 CDR(Coherent to Diffuse PowerRatio) 정보를 산출하는 단계; 및산출된 상기 기준 CDR 정보에 기초하여, 상기 로봇과의 거리에 대응되는 사운드의 CDR 정보를 추정하는 단계를포함하는, 로봇의 구동 방법."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 로봇의 촬영범위를 벗어난 소정 음원에서 발생된 사운드를 복수의 마이크를 통해 입력받는 단계;입력된 사운드의 CDR 정보를 산출하는 단계;상기 산출된 CDR 정보와 상기 추정된 CDR 정보에 기초하여, 상기 소정 음원의 위치를 추정하는 단계; 및상기 추정된 소정 음원의 위치가 상기 로봇과 인터랙션 가능한 범위 내인 경우, 특정 인터랙션 구동을 수행하는단계를 더 포함하는, 로봇의 구동 방법."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 특정 인터랙션 구동을 수행하는 단계는,상기 소정 음원이 상기 로봇의 촬영범위 내로 진입하도록 상기 로봇의 촬영 방향을 변경하는 단계를 포함하는,로봇의 구동 방법."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 특정 인터랙션 구동을 수행하는 단계는,상기 소정 음원의 위치가 상기 로봇의 촬영범위 내로 진입하는 경우, 상기 소정 음원을 응대하는 사운드 또는영상을 출력하는 단계를 포함하는, 로봇의 구동 방법."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 로봇과의 거리 정보에 기초하여, 음원과 인터랙션을 수행하는 NFA(Near Field Area), 음원의 사운드를 트래킹하는 STA(Sound Tracking Area) 및 FFA(Far Field Area) 중 적어도 하나의 영역(Area)을 상기 로봇을 중심으로 설정하는 단계를 더 포함하는, 로봇의 구동 방법."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,음원에서 발생된 사운드의 CDR 정보에 기초하여, 음원이 상기 영역들 중 하나에 위치하는 것으로 추정되는경우, 영역들 각각에 대응되는 사운드 출력 세기를 결정하는 단계를 더 포함하는, 로봇의 구동 방법.공개특허 10-2021-0003491-3-청구항 7 제5항에 있어서,음원에서 발생된 사운드의 CDR 정보에 기초하여, 음원이 상기 FFA에 위치한 것으로 추정되는 경우, 음원의 사운드를 트래킹하는 구동을 대기시키는 단계를 더 포함하는, 로봇의 구동 방법."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 잇어서,음원이 상기 FFA에서 상기 STA로 이동한 것으로 추정되는 경우, 음원의 사운드를 트래킹하는 단계를 더 포함하는, 로봇의 구동 방법."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 이어서,음원이 상기 STA에서 상기 NFA으로 이동한 것으로 추정되는 경우, 음원과 인터랙션하기 위한 구동을 수행하는단계를 더 포함하는, 로봇의 구동 방법."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,상기 로봇과의 거리 정보에 기초하여, 하나 이상의 음원과의 거리를 반영한 사운드 맵을 생성하는 단계; 및 상기 로봇의 위치가 변경되는 경우, 변경된 위치에 기초하여 상기 사운드 맵을 갱신하는 단계를 더 포함하는,로봇의 구동 방법."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "로봇의 구동 방법에 있어서,상기 로봇의 감지 범위 내 제1 위치에서 기준 음원(Base Sound Source)의 사운드를 복수의 마이크를 통해 입력받는 단계;상기 제1 위치의 기준 음원 및 상기 로봇 사이의 측정된 거리 정보에 대응되는 제1 기준 CDR(Coherent toDiffuse Power Ratio) 정보를 산출하는 단계;상기 기준 음원이 제2 위치로 이동하는 경우, 상기 제2 위치에서 상기 기준 음원의 사운드를 복수의 마이크를통해 입력받는 단계;상기 제2 위치의 기준 음원 및 상기 로봇 사이의 측정된 거리 정보에 대응되는 제2 기준 CDR(Coherent toDiffuse Power Ratio) 정보를 산출하는 단계; 및산출된 상기 제1 및 제2 기준 CDR 정보에 기초하여, 상기 로봇과의 거리에 대응되는 사운드의 CDR 정보를 추정하는 단계를 포함하는, 로봇의 구동 방법."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "로봇으로서,거리 감지 센서;복수의 마이크를 구비하고 오디오 신호가 입력되는 입력부;디스플레이를 포함하는 출력부; 및상기 거리 감지 센서의 감지 범위 내에 배치된 기준 음원의 사운드를 상기 복수의 마이크를 통해 획득하고 처리하는 프로세서를 포함하며,상기 프로세서는,상기 로봇 및 상기 기준 음원 사이의 거리를 상기 거리 감지 센서를 통해 측정하며, 측정된 거리 정보에 대응되공개특허 10-2021-0003491-4-는 기준 CDR 정보를 산출하고, 산출된 상기 기준 CDR 정보에 기초하여 상기 로봇과의 거리에 대응되는 사운드의CDR 정보를 추정하는, 로봇."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 프로세서는,상기 로봇의 촬영범위를 벗어난 소정 음원에서 발생된 사운드를 복수의 마이크를 통해 획득하고, 획득된 사운드의 CDR 정보를 산출하며, 상기 산출된 CDR 정보 및 상기 추정된 CDR 정보에 기초하여 상기 소정 음원의 위치를추정하고, 추정된 소정 음원의 위치가 상기 로봇과 인터랙션 가능한 범위 내인 경우 특정 인터랙션 구동을 수행하는, 로봇."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 입력부는 카메라를 포함하고,상기 프로세서는,상기 소정 음원이 상기 로봇의 촬영범위 내로 진입하도록 상기 카메라의 촬영 방향을 변경하는, 로봇."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 출력부는 스피커를 포함하며,상기 프로세서는,상기 소정 음원의 위치가 상기 카메라의 촬영 범위로 진입하면, 상기 소정 음원을 응대하는 사운드를 상기 스피커를 통해 출력하는, 로봇."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제13항에 있어서,상기 디스플레이는 가상의 사람 얼굴 형상을 나타내는 아이템을 표시하며,상기 프로세서는,상기 상기 아이템을 이용하여 상기 소정 음원에 특정 인터랙션을 수행하도록 상기 디스플레이를 제어하는,로봇."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제12항에 있어서,상기 프로세서는,상기 로봇과의 거리 정보에 기초하여, 하나 이상의 음원과의 거리를 반영한 사운드 맵을 생성하고, 상기 로봇의위치가 변경되는 경우 변경된 위치에 기초하여 상기 사운드 맵을 갱신하는, 로봇."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제12항에 있어서,상기 입력부는,상기 로봇의 감지 범위 내의 제1 위치에서 기준 음원에서 발생된 사운드를 복수의 마이크를 통해 획득하고, 상기 로봇이 상기 제1 위치에서 제2 위치로 이동하는 경우, 상기 기준 음원에서 발생된 사운드를 복수의 마이크를통해 획득하며,공개특허 10-2021-0003491-5-상기 프로세서는,상기 제1 위치 및 상기 제2 위치에 배치된 기준 음원과 상기 로봇 사이의 거리가 각각 측정된 경우, 측정된 거리 정보에 대응되는 기준 CDR 정보들을 산출하고, 산출된 기준 CDR 정보들에 기초하여, 상기 로봇과의 거리에대응되는 사운드의 CDR 정보를 추정하는, 로봇."}
{"patent_id": "10-2019-0079332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제12항에 있어서,상기 프로세서는,복수의 음원이 상기 로봇에 대해 사운드를 발생 시킨 경우, 최근접 거리로 추정된 음원에서 발생된 사운드를 선택하고, 상기 선택된 사운드에서 노이즈를 제거하는, 로봇."}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "탑재된 인공지능(artificial intelligence, AI) 알고리즘 및/또는 기계학습(machine learning) 알고리즘을 실행 하며, 5G 통신 환경에서 다른 전자 기기들 및 외부 서버와 통신할 수 있는 로봇 및 그 구동방법이 개시된다. 본 로봇은 거리 감지 센서, 복수의 마이크를 구비하고 오디오 신호의 입력을 위한 입력부, 디스플레이를 포함하는 출력부 및 거리 감지 센서의 감지 범위 내에 배치된 기준 음원의 사운드를 복수의 마이크를 통해 획득하여 처리 하는 프로세서를 포함하며, 프로세서는 로봇 및 기준 음원과의 거리를 거리 감지 센서를 통해 측정하며, 측정된 거리 정보에 대응되는 기준 CDR 정보를 산출하고, 산출된 기준 CDR 정보에 기초하여 상기 로봇과의 거리에 대응 되는 사운드의 CDR 정보를 추정한다."}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 음원과의 거리를 추정하는 로봇 및 그의 구동 방법에 관한 것이다."}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "로봇(Robot)은 스스로 보유한 능력에 의해 일을 자동으로 처리하는 기기이며, 최근에는 로봇을 응용한 분야가 더욱 확대되어, 의료용 로봇, 안내 로봇, 우주 항공 로봇 등이 개발되고 있으며, 일반 가정에서 적용될 수 있는 홈 로봇도 활발하게 개발되고 있는 실정이다. 종래 기술 1(KR1020070050283A)에 개시된 서비스 로봇은 서버와 네트워크로 연결되어 음성 데이터를 서버로 전 송하며, 복수 개의 음성 인식 마이크를 이용하여 음원 방향을 추정하고 추정된 음원 방향에 대한 정보를 서버로 전송한다. 다만, 상기 서비스 로봇은 음원이 발생된 방향을 서비스 로봇 중심으로 추정하나, 사용자에게 다양한 서비스를 제공하기 위한 음원 인식을 수행하지 못하는 한계가 있다. 종래 기술 2(KR1020180079824A)에 개시된 홈 로봇은 복수의 마이크를 통해 음성 신호를 수신하고, 수신된 음성 신호에 대응되는 음원 발생 위치를 추정한다. 다만, 종래 기술 2에는 복수의 마이크를 이용하여 홈 로봇과 음원 발생자의 위치를 추정하는 내용이 단순히 언 급되기는 하나, 이를 위한 구체적인 구현 방식이 개시되지 못하는 한계가 있다."}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는 보이지 않는 곳에서 다가오는 음원(사용자)과 로봇 간의 거리를 정확도 높게 추정함으로써 보다 신속하게 사용자에게 다양한 서비스를 제공할 수 있는 로봇을 제공하는데 있다. 본 발명의 또 다른 과제는 거리 센서를 이용하지 않고도 입력된 사운드만을 이용하여 로봇과의 거리를 추정하는 방법을 제공하는데 있다. 본 발명의 또 다른 과제는 촬영 범위를 벗어난 음원이 소정 거리로 로봇에 근접하면 음원과 인터랙션을 수행하 는 방법을 제공하는데 있다. 본 발명의 또 다른 과제는 복수의 음원에서 동시에 사운드가 발생되는 경우, 최근접한 음원의 사운드를 획득하 는 방법을 제공하는데 있다. 본 발명에서 이루고자 하는 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급하지 않은"}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "또 다른 기술적 과제들은 아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하 게 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 과제를 달성하기 위하여, 본 발명의 일 실시 예에 따른 로봇은 기준 CDR(Coherent to Diffuse Power Ratio) 정보만 산출되면 복수의 마이크만을 이용하여 음원과의 거리를 추정할 수 있다. 구체적으로, 상기 로봇은 거리 감지 센서, 복수의 마이크를 구비하고 오디오 신호의 입력을 위한 입력부, 디스 플레이를 포함하는 출력부 및 거리 감지 센서의 감지 범위 내에 배치된 기준 음원의 사운드를 복수의 마이크를 통해 입력받게 하는 프로세서를 포함하며, 상기 프로세서는 로봇 및 기준 음원과의 거리를 거리 감지 센서를 통 해 측정하며, 측정된 거리 정보에 대응되는 기준 CDR 정보를 산출하고, 산출된 기준 CDR 정보에 기초하여 로봇 과의 거리에 대응되는 사운드의 CDR 정보를 추정할 수 있다. 상기 프로세서는 로봇의 촬영범위를 벗어난 소정 음원에서 발생된 사운드를 복수의 마이크를 통해 입력받고, 입 력된 사운드의 CDR 정보를 산출하며, 산출된 CDR 정보 및 추정된 CDR 정보에 기초하여 소정 음원의 위치를 추정 하고, 추정된 소정 음원의 위치가 로봇의 인터랙션 범위 내인 경우 특정 인터랙션 구동을 수행할 수 있다. 여기서, 상기 입력부는 카메라를 포함하고, 상기 프로세서는 소정 음원이 상기 로봇의 촬영범위 내로 진입하도 록 상기 카메라의 촬영 방향을 변경할 수 있다. 상기 과제를 달성하기 위하여, 본 발명의 일 실시 예에 따른 로봇의 구동 방법은 상기 로봇의 감지 범위 내 위 치한 기준 음원(Base Sound Source)에서 발생된 사운드를 복수의 마이크를 통해 입력받는 단계, 기준 음원 및 로봇과의 거리가 측정된 경우, 측정된 거리 정보에 대응되는 기준 CDR(Coherent to Diffuse Power Ratio) 정보 를 산출하는 단계, 산출된 상기 기준 CDR 정보에 기초하여, 상기 로봇과의 거리에 대응되는 사운드의 CDR 정보 를 추정하는 단계를 포함할 수 있다. 상기 구동 방법은 로봇의 촬영범위를 벗어난 소정 음원에서 발생된 사운드를 복수의 마이크를 통해 입력받는 단 계, 입력된 사운드의 CDR 정보를 산출하는 단계, 산출된 CDR 정보 및 추정된 CDR 정보에 기초하여, 소정 음원의 위치를 추정하는 단계 및 추정된 소정 음원의 위치가 로봇의 인터랙션 범위 내인 경우, 특정 인터랙션 구동을 수행하는 단계를 더 포함할 수 있다. 상기 특정 인터랙션 구동을 수행하는 단계는, 소정 음원이 상기 로봇의 촬영범위 내로 진입하도록 로봇의 촬영 방향을 변경하는 단계를 포함할 수 있다. 상기 특정 인터랙션 구동을 수행하는 단계는, 음원이 촬영범위 내로 진입하는 경우, 소정 음원을 응대하는 사운 드 또는 영상을 출력하는 단계를 포함할 수 있다. 상기 구동 방법은 로봇과의 거리 정보에 기초하여, 음원과 인터랙션을 수행하는 NFA(Near Field Area), 음원의 사운드를 트래킹하는 STA(Sound Tracking Area) 및 FFA(Far Field Area) 중 적어도 하나의 영역(Area)을 상기 로봇을 중심으로 설정하는 단계를 포함할 수 있다. 상기 구동 방법은 음원에서 발생된 사운드의 CDR 정보에 기초하여, 음원이 영역들 중 하나에 위치하는 것으로 추정되는 경우, 영역들 각각에 대응되는 사운드 출력 세기를 결정하는 단계를 더 포함할 수 있다. 상기 구동 방법은 음원에서 발생된 사운드의 CDR 정보에 기초하여, 음원이 FFA에 위치한 것으로 추정되는 경우, 음원의 사운드를 트래킹하는 구동을 대기하는 단계를 더 포함할 수 있다. 상기 구동 방법은 음원이 FFA에서 STA로 이동한 것으로 추정되는 경우, 음원의 사운드를 트래킹하는 단계를 더 포함할 수 있다. 상기 구동 방법은 음원이 상기 STA에서 상기 NFA으로 이동한 것으로 추정되는 경우, 음원과 인터랙션하기 위한 구동을 수행하는 단계를 더 포함할 수 있다. 상기 구동 방법은 로봇과의 거리 정보에 기초하여, 하나 이상의 음원과의 거리를 반영한 사운드 맵을 생성하는 단계 및 로봇의 위치가 변경되는 경우, 변경된 위치에 기초하여 사운드 맵을 갱신하는 단계를 더 포함할 수 있 다. 상기 과제를 달성하기 위하여, 본 발명의 일 실시 예에 따른 로봇의 구동 방법은 로봇의 감지 범위 내 제1 위치 에서 기준 음원(Base Sound Source)의 사운드를 복수의 마이크를 통해 입력받는 단계, 제1 위치의 기준 음원 및 로봇 사이의 측정된 거리 정보에 대응되는 제1 기준 CDR(Coherent to Diffuse Power Ratio) 정보를 산출하는 단계, 기준 음원이 제2 위치로 이동하는 경우, 제2 위치에서 기준 음원의 사운드를 복수의 마이크를 통해 입력받는 단계, 제2 위치의 기준 음원 및 로봇 사이의 측정된 거리 정보에 대응되는 제2 기준 CDR(Coherent to Diffuse Power Ratio) 정보를 산출하는 단계 및 산출된 상기 제1 및 제2 기준 CDR 정보에 기초하여, 로봇과의 거리에 대응되는 사운드의 CDR 정보를 추정하는 단계를 포함할 수 있다."}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 다양한 실시 예에 따르면 아래와 같은 효과가 도출될 수 있다. 첫째로, 음원과의 거리를 복수의 마이크만을 이용하여 정확하게 추정하는 로봇이 제공됨으로써, 보이지 않는 곳 에서 다가오는 음원과의 거리가 정확도 높게 추정될 수 있으며, 보다 신속하게 다양한 서비스가 사용자에게 제 공될 수 있다. 이에 따라, 사용자 편의가 향상될 수 있다. 둘째로, 촬영 범위를 벗어난 음원이 로봇에 접근하는 경우, 적절한 인터랙션이 로봇에 의해 수행될 수 있어서 사용자 편의가 제고될 수 있다. 셋째로, 복수의 음원에서 동시에 사운드가 발생되는 경우, 사운드의 유실이 방지될 수 있으므로 프로세싱 정확 도가 향상될 수 있으며, 사용자 편의가 제고될 수 있다."}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 동일하거나 유사한 구성요소에 는 동일 유사한 도면 부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐 릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 도 1은 본 발명의 일 실시 예에 따른 로봇의 외관을 설명하기 위한 도면이다. 로봇은 외부 다양한 기기들과 통신할 수 있는 기기로, 일정 공간(예를 들면, 가정, 병원, 회사 등)에 배치 될 수 있다. 로봇은 바디(Bo)를 포함할 수 있으며, 바디(Bo)는 바디(Bo)의 상부를 형성하는 상부 바디 (UBo) 및 하부를 형성하는 하부 바디(LBo)를 포함할 수 있다. 바디(Bo)는 좌우 방향으로 기울임 동작을 수행하 거나 전후 방향으로 기울임 동작을 수행할 수 있다. 상부 바디(UBo)는 디스플레이를 구비하여, 다양한 콘텐츠를 표시하거나 영상 통화 제공을 위한 인터페이스 를 표시할 수 있다. 또한, 상기 디스플레이는 사용자와 인터랙션을 위한 구동을 수행할 수 있다. 예를 들 면, 디스플레이는 가상적으로 사용자의 눈 모양과 비슷한 타원 또는 원형의 아이템(193a, 193b)을 표시하 고, 윙크, 깜빡임 등의 인터랙션 구동을 수행할 수 있다. 이에 따라, 사용자에게 보다 친화적인 구동이 상기 로 봇을 통해 수행될 수 있다.상기 디스플레이의 일 영역에는 카메라가 배치될 수 있으며, 카메라는 사용자를 촬영하거나 사 용자를 인식하는데 사용될 수 있다. 카메라는 상부 바디(UB)가 회전하도록 구현될 수 있어서 전후좌우 모 든 방향에 배치된 오브젝트를 촬영 및 인식할 수 있다. 실시 예에 따르면, 카메라는 자체적으로 거리를 감지하는 거리 센서를 구비하여 카메라의 촬영 방향 에 배치된 오브젝트와의 거리를 측정할 수 있다. 상기 로봇은 소정의 영역에 고정되어 배치될 수 있다. 선택적 실시 예로 상기 로봇은 이동 모듈을 구 비하여 원하는 방향 또는 입력된 방향으로 이동할 수 있다. 도 2는 본 발명의 일 실시 예에 따른 복수의 마이크가 상기 로봇의 소정 영역에 배치된 것을 상부에서 바 라본 평면도이다. 로봇은 복수의 마이크(123a~123d)를 상부 바디(UBo) 또는 하부 바디(LBo)의 소정 영역에 포함할 수 있다. 복수의 마이크(123a~123d)는 동서남북 방향에 각각 배치될 수 있다. 복수의 마이크(123a~123d)를 마이크 어레이 로 표현될 수 있다. 선택적 실시 예로 상기 복수의 마이크는 두 개, 세 개, 다섯 개 이상을 포함할 수 있다. 로봇은 복수의 마이크(123a~123d)를 이용하여 음원 로컬리제이션(Sound Source Localization)을 수행할 수 있다. 음원 로컬리제이션은 음원의 방향을 예측하는 것으로, 복수의 마이크(123a~123d)로 입력되는 사운드의 시간차를 이용하여 로봇 중심으로 음원의 방향을 예측할 수 있다. 여기서, 음원은 사운드를 발생시키는 오 브젝트(Object)로 전자 기기, 사람, 동물 등을 포함할 수 있다. 이하에서는 도 3을 참고하여, 로봇의 각 구성들을 설명하기로 한다. 로봇은 통신부, 입력부 , 센싱부, 출력부, 저장부, 전원공급부 및 프로세서를 포함할 수 있다. 도 3에 도시된 구성요소들은 로봇을 구현하는데 있어서 필수적인 것은 아니어서, 본 명세서 상에서 설명되는 로봇 은 위에서 열거된 구성요소들 보다 많거나, 또는 적은 구성요소들을 가질 수 있다. 먼저, 통신부는 로봇과 하나 이상의 통신기기들과의 통신을 수행하기 위한 모듈이다. 만약, 로봇 이 일반적인 가정에 배치된 경우, 로봇은 통신 기기(가령, 냉장고, 세탁기, IPTV(Internet Protocol TeleVision), 블루투스 스피커, AI(Artificial Intellingence) 스피커, 이동 단말 등) 등과 홈네트워크를 구성 할 수 있다. 상기 통신부는 이동 통신 모듈 및 근거리 통신 모듈을 포함할 수 있다. 먼저, 이동 통신 모듈은 이동통신을 위한 기술표준들 또는 통신방식(예를 들어, GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), CDMA2000(Code Division Multi Access 2000), EV- DO(Enhanced Voice-Data Optimized or Enhanced Voice-Data Only), WCDMA(Wideband CDMA), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), LTE(Long Term Evolution), LTE-A(Long Term Evolution-Advanced), 5G(Generation) 등)에 따라 구축된 이동 통신망 상에서 기지국, 외부의 단말, 서버 중 적어도 하나와 무선 신호를 송수신한다. 통신부는 5G 통신을 지원하는 이동 통신 모듈을 구비하여, 100Mbps 내지 20Gbps 속도로 데이터를 전송할 수 있어서 대용량의 동영상을 다양한 기기로 전송할 수 있으며, 저전력으로 구동되어 전력 소비를 최소화할 수 있다. 또한, 통신부는 근거리 통신 모듈을 포함할 수 있다. 여기서, 근거리 통신 모듈은 근거리 통신(Short range communication)을 위한 것으로서, 블루투스(Bluetooth™), RFID(Radio Frequency Identification), 적외 선 통신(Infrared Data Association; IrDA), UWB(Ultra Wideband), ZigBee, NFC(Near Field Communication), Wi-Fi(Wireless-Fidelity), Wi-Fi Direct, Wireless USB(Wireless Universal Serial Bus) 기술 중 적어도 하 나를 이용하여 근거리 통신을 수행할 수 있다. 또한, 통신부는 각종 사물 지능 통신(IoT(Internet of Things), IoE(Internet of Everything), IoST(Internet of Small Things) 등)을 지원할 수 있으며, 통신부는 M2M(Machine to Machine) 통신, V2X(Vehicle to Everything Communication) 통신, D2D(Device to Device) 통신 등을 지원할 수 있다. 입력부는 영상 신호 입력을 위한 카메라 또는 영상 입력부, 오디오 신호 입력을 위한 마이크로폰 (microphone, 123), 또는 오디오 입력부, 사용자로부터 정보를 입력받기 위한 사용자 입력부(예를 들어, 터치키 (touch key), 푸시키(mechanical key) 등)를 포함할 수 있다. 상기 입력부는 상기 카메라 및 마이크를 복수로 포함할 수 있으며, 특히, 마이크는 세 개 이상 포함될 수 있다. 본 명세서 상에서는 마이 크가 네 개인 것으로 설명하나 실시 예가 이에 국한되는 것은 아니다. 센싱부는 로봇 내 정보, 로봇을 둘러싼 주변 환경 정보 및 사용자 정보 중 적어도 하나를 센싱 하기 위한 하나 이상의 센서를 포함할 수 있다. 예를 들어, 센싱부는 거리 감지 센서(131, 가령, 근접센서 (proximity sensor), PIR(Passive Infrared) 센서, 라이다(Lidar sensor) 센서 등), 무게 감지 센서, 조도 센 서(illumination sensor), 터치 센서(touch sensor), 가속도 센서(133, acceleration sensor), 자기 센서 (magnetic sensor), 중력 센서(G-sensor), 자이로스코프 센서(135, gyroscope sensor), 모션 센서(motion sensor), RGB 센서, 적외선 센서(IR 센서: infrared sensor), 지문인식 센서(finger scan sensor), 초음파 센 서(ultrasonic sensor), 광 센서(optical sensor, 예를 들어, 카메라(121 참조)), 마이크로폰(microphone, 123 참조), 배터리 게이지(battery gauge), 환경 센서(예를 들어, 기압계, 습도계, 온도계, 방사능 감지 센서, 열 감지 센서, 가스 감지 센서 등), 화학 센서(예를 들어, 전자 코, 헬스케어 센서, 생체 인식 센서 등) 중 적어도 하나를 포함할 수 있다. 한편, 본 명세서에 개시된 로봇은, 이러한 센서들 중 적어도 둘 이상의 센서에서 센싱되는 정보들을 조합하여 활용할 수 있다. 여기서, 센싱부는 IMU(Inertial Measurement Unit) 센서를 포함할 수 있으며, IMU(Inertial Measurement Unit) 센서는 가속도 센서, 자이로 센서, 각속도 센서, 지자기 센서, 고도 센서 등을 구비하여 이동 오브젝트의 속도와 방향, 중력, 가속도 등을 측정할 수 있다. 상기 IMU 센서는 로봇의 이동을 감지할 수 있다. 출력부는 시각, 청각 또는 촉각 등과 관련된 출력을 발생시키기 위한 것으로, 디스플레이(141, 복수 개 적 용 가능), 하나 이상의 발광 소자, 음향 출력부 및 햅팁 모듈 중에서 적어도 하나를 포함할 수 있다. 디스플레 이는 터치 센서와 상호 레이어 구조를 이루거나 일체형으로 형성됨으로써, 터치 스크린으로 구현될 수 있 다. 이러한 터치 스크린은, 로봇과 사용자 사이의 입력 인터페이스를 제공하는 사용자 입력부로써 기능함 과 동시에, 로봇과 사용자 사이의 출력 인터페이스를 제공할 수 있다. 저장부는 로봇의 다양한 기능을 지원하는 데이터를 저장한다. 저장부는 로봇에서 구동되는 다수의 응용 프로그램(application program 또는 애플리케이션(application)), 로봇의 동작을 위한 데이 터들, 명령어들을 저장할 수 있다. 이러한 응용 프로그램 중 적어도 일부는, 무선 통신을 통해 외부 서버로부터 다운로드될 수 있다. 또한, 저장부는 로봇과 인터랙션을 수행하려는 사용자 정보를 저장할 수 있다. 상기 사용자 정보는 인식된 사용자가 누구인지 식별하는데 사용될 수 있다. 아울러, 저장부는 후술할 인공 지능, 머신 러닝, 인공 신경망을 이용하여 연산을 수행하는데 필요한 정보 를 저장할 수 있다. 전원공급부는 프로세서의 제어 하에서, 외부의 전원, 내부의 전원을 인가 받아 로봇의 각 구성 요소들에 전원을 공급한다. 이러한 전원공급부는 배터리를 포함하며, 상기 배터리는 내장형 배터리 또는 교체가능한 형태의 배터리가 될 수 있다. 상기 배터리는 유선 또는 무선 충전 방식으로 충전될 수 있는데, 무선 충전 방식은 자기 유도 방식 또는 자기 공진 방식을 포함할 수 있다. 프로세서는 로봇의 구성들을 컨트롤하는 모듈로, 로봇과 소정 거리를 둔 음원(Sound Source)의 사운드를 복수의 마이크를 통해 획득할 수 있다. 여기서, 음원은 사운드를 발생시키는 오브젝트로, 상기 오브젝트는 기기, 사람, 동물 등을 포함할 수 있다. 음원이 사람이라고 가정하면, 프로세서는 거리 감지 센서가 인식 가능한 거리에 있는 사용자의 음성 을 복수의 마이크를 통해 입력받을 수 있다. 여기서, 거리 감지 센서는 근접 거리의 사용자를 인식할 수 있으나, 선택적 실시 예로 원 거리의 사용자를 인식할 수 있다. 거리 감지 센서의 감지 범위 내의 음원 을 기준 음원(Base Sound Source)이라 할 수 있으나, 구현 예에 따라서는 기준 음원이 상기 범위에 한정되는 것 은 아니다. 프로세서는 로봇 및 기준 음원(Base Sounce Source)과의 거리를 거리 감지 센서를 통해 측정하 며 측정된 거리 정보에 대응되는 기준 CDR(Coherent to Diffuse Power Ratio) 정보를 산출할 있다. 기준 음원 의 경우, 프로세서가 음원과의 거리 정보 및 CDR 정보와의 상관 관계를 확인하기 위해 기준이 되는 오브젝 트로 기기, 사람, 동물 등이 될 수 있다. 즉, 기준 음원의 CDR 정보를 기초하여 다른 음원의 CDR 정보 및 다른 음원과 로봇과의 거리를 추정할 수 있다. 이하에서는 프로세서가 음원의 사운드에 대해 CDR(Coherent to Diffuse Power Ratio) 정보를 산출하는 것 을 설명하기로 한다. 여기서, CDR 정보는 디퓨즈(Diffuse) 성분 대비 코히런트(Coherent) 성분의 파워 비율로 dB로 표현될 수 있다. 직관적으로, CDR 정보는 음원에서 반사되지 않고 바로 마이크로 입력되는 신호와 공간 내부의 장애물에 의해 반 사되어 마이크로 입력되는 신호 간의 파워 비(Rate)로 표현될 수 있다. 로봇은 CDR 정보에 기초하여 로봇 및 음원(오브젝트) 사이의 상대적인 거리를 결정할 수 있다. 이하에서는 CDR 정보를 구체적으로 설명하되, 마이크가 복수(two)인 것으로 가정하여 설명하기로 한다. 복수의 마이크 각각은 반향(Reverberant) 신호와 노이즈 신호를 입력받을 수 있으며, 복수의 마이크 각각이 입 력받는 신호들이 아래 [식 1]에 의해 모델링될 수 있다. [식 1]"}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, i는 마이크의 인덱스(가령, 1 및 2)이며, 각각의 마이크가 입력받는 신호는 코히런트 성분과 디퓨즈 성 분의 합으로 모델링될 수 있다. 코히런트 성분은 기대되는(Desired) 스피치 성분이며, 디퓨즈 성분은 기대치 않 는(Undesired) 성분으로 확산하는 반향 신호 및 노이즈 신호를 포함할 수 있다. 원필드(Far-Field) 및 프리필드(Free-Field) 상황에서 손실없는 Propagation를 가정하면, X2,coh(t)는 X1,coh(t) 의 단순 타임 쉬프트에 의해 도출될 수 있다. 즉, 코히런트 성분은 점음원(Point Source)에서 방향성을 띈 파면 (Wave Front)이 각각의 마이크에 도달하기 때문에, 마이크 채널 각각에 입력되는 코히런트 성분은 시간차를 가 지고 입력되는 동일한 신호로 모델링이 가능하다. 이를 표현하면 아래의 식 2와 같이 도출될 수 있다. [식 2]"}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, τ12는 두 마이크 사이의 기대되는 사운드의 TDOA(Time Differenece Of Arrival)를 나타내고, 상기 [식 2]에 따르면, 두 마이크 사이의 기대되는 스피치 성분 사이의 공간 코히런스(Spatial Coherence)는 아래 [식 3]에 의해 표현될 수 있다. 아래의 [식 3]에 의해, 복수의 마이크 채널로 입력받은 코히런트 성분이 하나의 매 트릭스로 표현 가능하며, τ12 만큼의 시간차를 갖는 모델로 구현될 수 있다. 여기서, f는 주파수이다. [식 3]"}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "또한, 디퓨즈 성분은 동일 마이크의 경우 상관 관계가 높아서 상관 계수가 1이나, 서로 다른 마이크의 경우 상 관관계가 낮게 나타난다. 이를 반영한 복수의 마이크 채널로 입력받은 디퓨즈 성분이 하나의 매트릭스로 구현 가능하며, [식 4]와 같다. 여기서, d는 마이크 사이의 거리이며 c는 음속에 해당된다. [식 4]"}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, 코히런트 성분의 단기간의 파워 스펙트라(Spectra)인 는 마이크 각각에서의 값이 동일하며 아 래 [식 5]에 표시된 식이 성립한다. 이는 디퓨즈 성분의 경우에도 동일하다. [식 5]"}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "상기 코히런트 성분의 단기간의 파워 스펙트라인 및 상기 디퓨즈 성분의 단기간의 파워 스펙트라인 에서 k는 k번째 DFT(Discrete Fourier Transform) bin 을 나타내며, l은 l번째 타임 프레임을 나타낸 다. 여기서, CDR 정보는 아래의 [식 6]에 의해 도출될 수 있다. 즉, CDR 정보는 마이크 각각이 동일한 값을 가지며, 그 값은 디퓨즈 성분 대비 코히런트 성분의 파워 비로 성립될 수 있으며, 단위는 dB로 표시될 수 있다. CDR 정 보는 유용하게 사용될 수 있다. [식 6]"}
{"patent_id": "10-2019-0079332", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "기 설명한 바와 같이, 프로세서는 로봇 및 기준 음원 사이의 거리를 거리 감지 센서를 통해 측 정할 수 있다. 프로세서는 측정된 거리 정보에 대응되는 기준 CDR 정보를 산출하고, 산출된 상기 기준 CDR 정보에 기초하 여 로봇과의 거리에 대응되는 사운드의 CDR 정보를 추정(Estimation)할 수 있다. 프로세서는 거리를 정확하게 인식하고 있는 기준 음원과의 거리에 대응되는 기준 CDR 정보를 산출한 후, 기준 음원 또는 다른 음원의 사운드를 입력받으면, 입력된 사운드가 로봇과 얼만큼 거리를 두는지 추정할 수 있다. 이에 따라, 프로세서는 사운드의 CDR 정보를 추정하는데 있어 기준 CDR 정보를 오토 캘리브레이 션에 사용할 수 있다. 이하에서는 도 4 내지 도 13의 도면들을 참고하여 로봇의 다양한 구동 방법을 설명하기로 한다. 도 4는 본 발명의 일 실시 예에 따른 감지 범위 내의 기준 음원(BSS, Base Sound Souce)에서 발생된 사운드의 CDR 정보를 산출하는 방법을 설명하기 위한 도면이다. 프로세서는 로봇(R)을 중심에 두고 외부 오브젝트들을 가상적으로 표시할 수 있다. 상기 외부 오브젝 트들은 상기 디스플레이에 표시될 수도 있다. 프로세서는 로봇을 중심에 두고 제1 라인, 제2 라인, 제3 라인, 제4 라인 등을 가상적으로 표시할 수 있다. 상기 라인들(410~440)은 동심 원의 점선 모양으로 구현될 수 있으나, 실시 예가 이에 국한되는 것은 아니다. 여기서, 프로세서는 로봇과의 거리 정보에 기초하여, 기준 음원(BSS)의 사운드(가령, \"로봇아\")를 복 수의 마이크를 통해 입력받을 수 있다. 기준 음원(BSS)은 기준이 되는 음원에 해당된다. 프로세서는 거리 감지 센서를 통해 기준 음원(BSS)을 감지할 수 있으며, 기준 음원(BSS)과 인터랙션 수행 가능한 NFA(Near Field Area)를 설정할 수 있다. NFA는 근거리 필드 영역으로 제2 라인 내부의 영역 을 포함할 수 있으나, 상기 영역의 사이즈는 실시 예에 따라 다르게 구현될 수 있다. 프로세서는 제3 라인의 내부 영역이면서 NFA의 외부인 STA(Sound Tracking Area)를 설정할 수 있다. STA는 외부 오브젝트의 사운드를 트래킹하는 영역에 해당되나, 상기 영역도 실시 예에 따라 다르게 구현될 수 있다. 프로세서는 STA 외부 영역을 FFA(Far Field Area)로 설정할 수 있으며, FFA는 원거리 필드 영역으로 인터 랙션을 수행하지 않고, 사운드 트래킹을 대기하는 영역일 수 있다. 구현 예에 따라서는 FFA 인 경우에 로봇 이 다양한 구동을 수행할 수도 있으며, 로봇이 FFA1 및 FFA2에서 서로 다른 구동을 수행할 수도 있다. 여기서, 로봇과 각 라인들의 거리는 다양하게 설정될 수 있다. 일 예로, d1이 50cm 인 경우, d2는 d1의 두 배인 1m일 수 있으며, d3는 d2의 두 배인 2m 일 수 있으며, d4는 d2의 1.5 배인 3m 일 수 있다. 로봇은 d1 지점에서의 CDR 정보를 10dB로 산출할 수 있으며, 이에 근거하여 d2 지점에서의 CDR 정보를 4dB, d3 지점에 서의 CDR 정보를 -2dB, d4 지점에서의 CDR 정보를 -5.5dB로 각각 추정할 수 있다. 다만, 상기 음압의 감소범위 는 공간의 구조, 장애물, 매질, 발화 지점 등에 따라 다르게 설정될 수 있으나, 로봇과의 거리 대비 기준 CDR 정보가 산출됨으로써, 로봇과의 거리에 대응되는 CDR 정보가 추정될 수 있다. 프로세서는 기준 음원(BSS)과의 거리를 정확하게 측정한 상태에서 소정의 음원이 로봇의 주변에 배치 된 경우 발생된 사운드의 CDR 정보를 추정할 수 있다. 프로세서는 거리를 정확하게 측정하기 어렵고 카메 라가 촬영하는 범위가 아닌 영역에 음원이 배치되더라도 해당 음원과의 거리를 추정할 수 있다. 프로세서는 음원(가령, 기준 음원 또는 소정의 음원)에서 발생된 사운드의 CDR 정보에 기초하여 음원이 FFA에 위치한 것으로 추정되는 경우, 음원의 사운드를 트래킹하는 구동을 수행할 수 있다. 프로세서는 음원이 FFA에서 STA로 이동하는 것으로 추정되는 경우, 음원의 사운드를 트래킹하는 구동을 수 행할 수 있다. 실시 예에 따르면, 프로세서는 STA 에서 바로 음원에 인터랙션을 수행하는 구동을 수행할 수 있는데, NFA보다는 보다 소극적인 인터랙션 구동을 수행할 수 있다. 가령, 카메라는 움직이지 않고 사 운드만 출력할 수 있다. 프로세서는 음원이 STA에서 NFA로 이동한 것으로 추정되는 경우, 음원과 인터랙션을 위한 구동을 수행할 수 있다. 가령, 프로세서는 카메라를 음원 방향으로 돌리고 디스플레이 상의 아이템들이 반응하 게 할 수 있다. 또한, 프로세서는 CDR 정보를 보다 더 정확하게 추정하기 위해서 기준 음원의 사운드를 복수의 지점에서 입력받고, 입력된 사운드에 기초하여 복수의 기준 CDR 정보를 산출할 수 있다. 구체적으로, 프로세서는 거리 감지 센서의 감지 범위 내의 제1 위치 및 제2 위치에서 기준 음원의 사 운드를 복수의 마이크(123a~123d)를 통해 입력받고, 상기 제1 위치 및 상기 제2 위치의 기준 음원 및 상기 로봇 과의 측정된 거리 정보에 대응되는 기준 CDR 정보들을 산출하고, 산출된 기준 CDR 정보들에 기초하여, 상기 로 봇과의 거리에 대응되는 사운드의 CDR 정보를 추정할 수 있다. 이런 경우, 보다 정확하게 소정 사운드의 CDR 정보 및 로봇과의 거리가 정확하게 추정될 수 있다. 여기서, 기준 음원은 단수 또는 복수로 구현될 수 있다. 기준 음원이 유일한 경우, 기준 음원이 제1 위치에서 제2 위치로 이동할 수 있으며, 기준 음원이 복수인 경우, 제1 위치 및 제2 위치 각각에 배치된 기준 음원에서 CDR 정보가 산출될 수 있다. 도 5는 본 발명의 일 실시 예에 따른 기준 CDR 에 기초하여 사운드의 CDR 정보를 추출하는 방법을 나타내는 시 퀀스도이며, 상기 시퀀스도는 상술한 내용들을 정리한 시간의 흐름도에 해당된다. 먼저, 단계 S510에서 로봇은 기준 음원에서 발생된 사운드를 복수의 마이크를 통해 입력받는다. S520 단계에서, 로봇은 입력된 사운드의 기준 CDR 정보를 산출한다. S530 단계에서, 로봇은 거리 감지 센서를 통해 로봇과 기준 음원 사이의 거리를 측정한다. S540 단계에서, 산출된 기준 CDR 정보에 기초하여 로봇과의 거리에 대응되는 사운드의 CDR 정보를 추정한 다. 도 6 및 도 7은 본 발명의 일 실시 예에 따른 음원의 위치에 기초한 로봇의 구동 방법을 나타내는 시퀀스도들이 며, 도 4를 함께 참고하여 내용을 설명하기로 한다. 도 6 및 도 7의 공통된 단계로, 로봇은 음원의 사운드를 복수의 마이크를 통해 입력받고(S610, S710), 기 준 사운드의 CDR 정보 산출하고 음원과 로봇 간의 거리를 추정한다(S620, S720). 먼저, 도 6에서, 로봇은 소정의 음원이 FFA에 위치하면(S630) 대기할 수 있다(S640). 구현 예에 따라서는 로봇이 사운드 트래킹을 위해 필요한 구동들을 수행할 수 있다. 다음으로, 음원이 FFA에서 STA로 이동한 경우(S630 및 S645, 음원이 STA에 위치한 경우) 음원의 사운드 트래킹 을 시도한다(S650). 구현 예에 따라서는 로봇이 소극적인 인터랙션 구동을 수행할 수 있다. 다음으로, 음원이 STA에서 NFA로 이동한 경우(S645), 인터랙션 구동을 수행한다(S660). 도 7을 참고하면, 로봇은 음원의 위치에 따라 영역들에 대응되는 사운드 출력 세기를 결정하여 출력할 수 있다. 가령, 로봇은 음원이 FFA에 배치된 경우(S730), 제1 모드로 사운드 출력한다(S740). 여기서, 로봇은 원거리에 배치된 음원과 소통하기 위해 상대적으로 가장 쎈 출력(가령, 볼륨 6dB 상승, 피 치 10% 상승)을 설정할 수 있다. 다음으로, 음원이 FFA에서 STA로 이동한 경우(S730 및 S745, 음원이 STA에 위치한 경우), 제2 모드로 사운드 출 력한다(S750). 여기서, 제2 모드는 제1 모드보다는 출력이 다소 약한(가령, 볼륨 3dB 상승 및 피치 5% 상승) 설 정일 수 있다. 다음으로, 음원이 STA에서 NFA로 이동한 경우(S745), 제3 사운드 모드로 사운드를 출력할 수 있다(S760). 도 8 내지 도 10은 본 발명의 일 실시 예에 따른 로봇의 위치가 변경되는 경우, 사운드 맵을 갱신하는 과정을 설명하기 위한 도면들이다.도 8을 참고하면, 로봇은 로봇을 중심에 두고 사운드 맵을 생성할 수 있다. 로봇은 로봇을 중심으로 1 사분면에 OB2가 배치되고, 2사분면에 OB1이 배치될 수 있다. 일 예로, OB2의 CDR 정보는 3dB일 수 있으며, DOA는 45도일 수 있으며, OB1의 CDR 정보는 10dB이고 DOA는 110도 일 수 있다. 로봇은 기준 CDR 에 기초한 로봇과의 거리 정보를 추정할 수 있으므로, 로봇을 기준으로 음원인 오브젝트들(OB1, OB2)의 위치를 특정할 수 있다. 도 9와 같이, 로봇이 1 사분면의 일 지점으로 이동하는 경우, 로봇은 오브젝트들과의 방향 및 거리를 추정할 수 있다. 일 예로, 로봇은 OB2의 CDR 정보는 9dB이고, DOA는 0도일 수 있으며, OB1의 CDR 정보는 3dB이고 DOA는 210도 일 수 있다. 여기서, 로봇은 이동된 지점을 중심으로 삼아 오브젝트들(OB1, OB2)과의 관계를 갱신할 수 있다. 이는, 오 브젝트들의 방향만 예측하는 것보다 수월하게 사운드 맵을 갱신할 수 있다. 도 10을 참고하면, 로봇은 오브젝트들(OB1, OB2)와 로봇 중심적으로 사운드 맵을 갱신할 수 있다. 여기서, 프로세서는 IMU 센서의 센싱을 민감하지 않게 조절하여 로봇의 이동에 따라 미스 센싱하는 문제를 해결할 수 있다. 구체적으로, 프로세서는 센싱값들의 제곱의 평균값을 이용하여 급작스럽게 바뀐 센싱값이 바로 적용되지 않게 할 수 있다. 즉, 급격하게 가속도 센서, 자이로 센서의 값이 변경되더 라도 급격한 변화를 갖는 값만 고려하지 않고 이전의 여러 개의 값들을 함께 고려(가령, 제곱 평균값 사용)하여 미스 센싱이 방지될 수 있다. 도 11 및 도 12는 본 발명의 일 실시 예에 따른 촬영 범위를 벗어난 음원이 로봇으로 근접하는 경우 로봇의 구 동을 설명하기 위한 도면들이다. 로봇은 인터랙션 영역인 NFA(420 내부)에 배치되고 제1 음원(SS1)과 인터랙션을 수행하고 있다. 로봇(10 0)은 로봇의 카메라의 촬영 범위르 벗어난 STA 에 배치된 제2 음원(SS2)의 사운드(\"로봇아 안녕\")를 복수의 마 이크를 이용하여 입력받을 수 있다. 그러면, 로봇은 입력된 사운드의 CDR 정보를 산출한다. 로봇은 추정된 CDR 정보에 기초하여 제2 음원 (SS2)의 위치를 추정할 수 있다. 로봇은 제2 음원(SS2)의 위치가 STA 로 추정되므로 사운드 트래킹을 수행 할 수 있다. 로봇은 제2 음원(SS2)의 사운드를 소정 타임 구간 동안 모니터링하며, 제2 음원(SS2)이 도 12 와 같이 인터랙션 가능한 NFA에 진입하면, 사운드 트래킹 구동을 펜딩하고 제2 음원(SS2)과 인터랙션(가령, \"반 가워\"라고 사운드를 발함)을 수행할 수 있다. 구체적으로, 로봇은 제2 음원(SS2)가 NFA로 진입하면 카메라의 촬영 방향이 제2 음원(SS2)을 향하도록 설 정할 수 있다. 또한, 로봇은 제2 음원(SS2)이 촬영 범위 내로 진입하는 경우, 제2 음원(SS2)을 응대하는 사운드 또는 영상을 출력할 수 있다. 도 13은 복수의 사운드 중에서 근거리의 사운드를 선택하여 이용하는 로봇의 구동을 설명하기 위한 도면이다. 도 13을 설명하는데 있어서 도 11 및 도 12를 함께 참고하여 설명하기로 한다. 참고로, 프로세서는 추정된 CDR 정보에 기초하여 입력되는 사운드의 CDR 정보가 NFA 를 벗어난 것으로 판 단되는 경우, 해당 사운드를 원거리 필드 모델 소스 로컬리제이션(Far Field Model Source Localization)을 수 행할 수 있으며, NFA를 내로 판단하는 경우, 해당 사운드를 근거리 필드 모델 소스 로컬리제이션(Near Field Model Source Localization)을 수행할 수 있다. 여기서, 원거리 필드는 로봇과 음원과의 거리가 상당하고, 음파가 평면파이며, 모델링이 용이한 장점을 가 지며, 음원의 거리를 추정하기 어려운 필드에 해당될 수 있다. 근거리 필드는 로봇과 음원과의 거리가 마 이크 사이의 거리 정도이며, 음파의 곡선을 무시하기 어려우며, 음원의 거리 추정이 용이하며 모델링이 복잡한 특징을 가진다. 즉, 로봇은 음원이 NFA 내부인지 외부인지에 따라서 사운드의 로컬리제이션을 다르게 수행할 수 있으며, 로봇은 음원들 간의 거리를 모두 추정할 수 있으므로, 음원들의 배치 각도, 위치 등을 특정할 수 있다. 이 에, 로봇은 원거리 필드 모델 소스 로컬리제이션 및 근거리 필드 모델 소스 로컬리제이션을 재귀적 (Recursive) 방법을 이용하거나 선택적/중첩적으로 이용할 수 있다. 특히, 로봇은 사운드 트래킹을 수행할 때, 본 방법을 적용할 수 있다. 이는, 음원과의 거리를 측정하기 어려워 하나의 필드 모델 소스 로컬리제이션을 명시적으로 선택하는 문제를 해소한 것이다. 선택적 실시 예로 상기 사운드 로컬리제이션을 구분하는 기준 영역은 NFA가 아닌 다른 영역이 될 수 있다. 도 13을 참고하면, 로봇은 복수의 마이크를 통해 복수의 사운드를 입력받는다(S1310). 로봇은 NFA에 배치된 제1 음원(SS1)의 사운드도 입력받을 수 있으며, STA에 배치된 제2 음원(SS2)의 사운 드도 입력받을 수 있다. 로봇은 제1 음원(SS1)을 촬영하고 있다. S1320 단계에서, 로봇은 산출된 CDR 정보에 기초하여 음원과 로봇과의 거리를 추정한다. 로봇은 제1 음원(SS1)이 NFA에 제2 음원(SS2)가 STA에 배치된 것으로 추정할 수 있다. 여기서, 로봇 은 제1 음원(SS1)의 사운드를 대해서는 근거리 필드 모델 소스로 설정하고, STA 에 배치된 제2 음원(SS2)의 사 운드에 대해서는 원거리 필드 모델 소스로 설정할 수 있다. 그러면, 로봇은 각 음원들(SS1, SS2)의 사운드 방향을 예측하고, NFA 에 위치한 음원(SS1)의 사운드만 선 택할 수 있다(S1330). 로봇은 선택된 사운드의 노이즈만 제거할 수 있다(S1340). 그 후에, 로봇은 노이즈가 제거된 사운드가 음성인 경우, 음성 인식을 수행한다(S1350). 상술한 바와 같이, 로봇은 거리 정보에 기초하여 원거리의 사운드를 배제시킬 수 있어서, 선택된 사운드를 보다 정확하고 정밀하게 검출할 수 있다. 선택적 실시 예로 로봇은 원거리의 사운드만 선택할 수 있으며, 원거리 및 근거리의 사운드를 모두 선택한 후, 선택적으로 사용할 수 있다. 실시 예에 의하면, 로봇은 도 12와 같이 NFA에 제1 음원(SS1) 및 제2 음원(SS2)이 동시에 배치된 경우, 소 정의 조건 하에서 음원을 선택할 수 있다. 이에 따라, 로봇이 복수의 음원에서 동시에 발생되는 사운드를 인식하지 못하는 종래 기술의 문제점을 해결할 수 있다. 선택적 실시 예로 로봇은 복수의 NFA에 배치된 음 원들의 사운드를 동시에 입력받을 수 있으며, 동시에 입력받은 사운드에서 잡음/잔향을 제거할 수 있다. 로봇은 인공 지능에 관련된 모듈을 추가로 탑재할 수 있다. 상기 인공 지능 모듈은 CDR 정보를 추정하고 음성을 인식할 때, 자체적인 사고를 통해 추정 및 인식 정확도를 높힐 수 있다. 인공 지능(artificial intelligence, AI)은 인간의 지능으로 할 수 있는 사고, 학습, 자기계발 등을 컴퓨터가 할 수 있도록 하는 방법을 연구하는 컴퓨터 공학 및 정보기술의 한 분야로, 컴퓨터가 인간의 지능적인 행동을 모방할 수 있도록 하는 것을 의미한다. 또한, 인공지능은 그 자체로 존재하는 것이 아니라, 컴퓨터 과학의 다른 분야와 직간접으로 많은 관련을 맺고 있다. 특히 현대에는 정보기술의 여러 분야에서 인공지능적 요소를 도입하여, 그 분야의 문제 풀이에 활용하려 는 시도가 매우 활발하게 이루어지고 있다. 로봇은 머신 러닝(machine learning)을 통해서 음원들을 파악할 수 있으며, 음원 사운드의 CDR 정보를 추 정할 수 있으며, 로봇 및 사운드의 거리를 추정할 수 있다. 또한, 로봇은 사운드를 인식하여 누구의 사운드인지 학습 및 검출할 수 있다. 여기서, 머신 러닝은 인공지능의 한 분야로, 컴퓨터에 명시적인 프로그램 없이 배울 수 있는 능력을 부여하는 연구 분야이다. 구체적으로 머신 러닝은, 경험적 데이터를 기반으로 학습을 하고 예측을 수행하고 스스로의 성 능을 향상시키는 시스템과 이를 위한 알고리즘을 연구하고 구축하는 기술이라 할 수 있다. 머신 러닝의 알고리 즘들은 엄격하게 정해진 정적인 프로그램 명령들을 수행하는 것이라기보다, 입력 데이터를 기반으로 예측이나 결정을 이끌어내기 위해 특정한 모델을 구축하는 방식을 취한다. 용어 \"머신 러닝\"은 용어 \"기계 학습\"과 혼용 되어 사용될 수 있다. 기계 학습에서 데이터를 어떻게 분류할 것인가를 놓고, 많은 기계 학습 알고리즘이 개발되었다. 의사결정나무 (Decision Tree)나 베이지안 망(Bayesian network), 서포트벡터머신(SVM: support vector machine), 그리고 인 공 신경망(ANN: Artificial Neural Network) 등이 대표적이다. 의사결정나무는 의사결정규칙(Decision Rule)을 나무구조로 도표화하여 분류와 예측을 수행하는 분석방법이다. 베이지안 망은 다수의 변수들 사이의 확률적 관계(조건부독립성: conditional independence)를 그래프 구조로 표현하는 모델이다. 베이지안 망은 비지도 학습(unsupervised learning)을 통한 데이터마이닝(data mining)에 적합하다. 서포트벡터머신은 패턴인식과 자료분석을 위한 지도 학습(supervised learning)의 모델이며, 주로 분류와 회귀 분석을 위해 사용한다. 인공신경망은 생물학적 뉴런의 동작원리와 뉴런간의 연결 관계를 모델링한 것으로 노드(node) 또는 처리 요소 (processing element)라고 하는 다수의 뉴런들이 레이어(layer) 구조의 형태로 연결된 정보처리 시스템이다. 인공 신경망은 기계 학습에서 사용되는 모델로써, 기계학습과 인지과학에서 생물학의 신경망(동물의 중추신경계 중 특히 뇌)에서 영감을 얻은 통계학적 학습 알고리즘이다. 구체적으로 인공신경망은 시냅스(synapse)의 결합으로 네트워크를 형성한 인공 뉴런(노드)이 학습을 통해 시냅 스의 결합 세기를 변화시켜, 문제 해결 능력을 가지는 모델 전반을 의미할 수 있다. 용어 인공신경망은 용어 뉴 럴 네트워크(Neural Network)와 혼용되어 사용될 수 있다. 인공신경망은 복수의 레이어(layer)를 포함할 수 있고, 레이어들 각각은 복수의 뉴런(neuron)을 포함할 수 있다. 또한 인공신경망은 뉴런과 뉴런을 연결하는 시냅스를 포함할 수 있다. 인공 신경망은 일반적으로 다음의 세가지 인자, 즉 다른 레이어의 뉴런들 사이의 연결 패턴 연결의 가 중치를 갱신하는 학습 과정 이전 레이어로부터 수신되는 입력에 대한 가중 합으로부터 출력값을 생성하는 활성화 함수에 의해 정의될 수 있다. 인공 신경망은, DNN(Deep Neural Network), RNN(Recurrent Neural Network), BRDNN(Bidirectional Recurrent Deep Neural Network), MLP(Multilayer Perceptron), CNN(Convolutional Neural Network)와 같은 방식의 네트 워크 모델들을 포함할 수 있으나, 이에 한정되지 않는다. 본 명세서에서 용어 \"레이어\"는 용어 \"계층\"과 혼용되어 사용될 수 있다. 인공신경망은 계층 수에 따라 단층 신경망(Single-Layer Neural Networks)과 다층 신경망(Multi-Layer Neural Networks)으로 구분된다. 일반적인 단층 신경망은, 입력층과 출력층으로 구성된다. 또한 일반적인 다층 신경망 은 입력층(Input Layer)과 하나 이상의 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성된다. 입력층은 외부의 자료들을 받아들이는 층으로서, 입력층의 뉴런 수는 입력되는 변수의 수와 동일하며, 은닉층은 입력층과 출력층 사이에 위치하며 입력층으로부터 신호를 받아 특성을 추출하여 출력층으로 전달한다. 출력층은 은닉층으로부터 신호를 받고, 수신한 신호에 기반한 출력 값을 출력한다. 뉴런간의 입력신호는 각각의 연결강도 (가중치)와 곱해진 후 합산되며 이 합이 뉴런의 임계치보다 크면 뉴런이 활성화되어 활성화 함수를 통하여 획득 한 출력값을 출력한다. 한편 입력층과 출력 층 사이에 복수의 은닉층을 포함하는 심층 신경망은, 기계 학습 기술의 한 종류인 딥 러닝 을 구현하는 대표적인 인공 신경망일 수 있다. 한편 용어 \"딥 러닝\"은 용어 \"심층 학습\"과 혼용되어 사용될 수 있다. 인공 신경망은 훈련 데이터(training data)를 이용하여 학습(training)될 수 있다. 여기서 학습이란, 입력 데이 터를 분류(classification)하거나 회귀분석(regression)하거나 군집화 (clustering) 하는 등의 목적을 달성하 기 위하여, 학습 데이터를 이용하여 인공 신경망의 파라미터(parameter)를 결정하는 과정을 의미할 수 있다. 인 공 신경망의 파라미터의 대표적인 예시로써, 시냅스에 부여되는 가중치(weight)나 뉴런에 적용되는 편향(bias) 을 들 수 있다. 훈련 데이터에 의하여 학습된 인공 신경망은, 입력 데이터를 입력 데이터가 가지는 패턴에 따라 분류하거나 군 집화 할 수 있다. 훈련 데이터를 이용하여 학습된 인공 신경망을, 본 명세서에서는 학습 모델(a trained mode l)이라 명칭 할 수 있다. 다음은 인공 신경망의 학습 방식에 대하여 설명한다. 인공 신경망의 학습 방식은 크게, 지도 학습, 비 지도 학 습, 준 지도 학습(Semi-Supervised Learning), 강화 학습(Reinforcement Learning)으로 분류될 수 있다. 지도 학습은 훈련 데이터로부터 하나의 함수를 유추해내기 위한 기계 학습의 한 방법이다. 그리고 이렇게 유추 되는 함수 중, 연속 적인 값을 출력하는 것을 회귀분석(Regression)이라 하고, 입력 벡터의 클래스(class)를 예 측하여 출력하는 것을 분류(Classification)라고 할 수 있다. 지도 학습에서는, 훈련 데이터에 대한 레이블 (label)이 주어진 상태에서 인공 신경망을 학습시킨다. 여기서 레이블이란, 훈련 데이터가 인공 신경망에 입력되는 경우 인공 신경망이 추론해 내야 하는 정답(또는 결 과 값)을 의미할 수 있다. 훈련 데이터가 입력되는 경우 인공 신경망이 추론해 내야 하는 정답(또는 결과값)을 레이블 또는 레이블링 데이터(labeling data)이라 명칭 한다. 또한 본 명세서에서는, 인공 신경망의 학습을 위 하여 훈련 데이터에 레이블을 설정하는 것을, 훈련 데이터에 레이블링 데이터를 레이블링(labeling) 한다고 명 칭 한다. 이 경우 훈련 데이터와 훈련 데이터에 대응하는 레이블)은 하나의 트레이닝 셋(training set)을 구성 하고, 인공 신경망에는 트레이닝 셋의 형태로 입력될 수 있다. 한편 훈련 데이터는 복수의 특징(feature)을 나타내고, 훈련 데이터에 레이블이 레이블링 된다는 것은 훈련 데 이터가 나타내는 특징에 레이블이 달린다는 것을 의미할 수 있다. 이 경우 훈련 데이터는 입력 객체의 특징을 벡터 형태로 나타낼 수 있다. 인공 신경망은 훈련 데이터와 레이블링 데이터를 이용하여, 훈련 데이터와 레이블링 데이터의 연관 관계에 대한 함수를 유추할 수 있다. 그리고, 인공 신경망에서 유추된 함수에 대한 평가를 통해 인공 신경망의 파라미터가 결정(최적화)될 수 있다. 비 지도 학습은 기계 학습의 일종으로, 훈련 데이터에 대한 레이블이 주어지지 않는다. 구체적으로, 비 지도 학습은, 훈련 데이터 및 훈련 데이터에 대응하는 레이블의 연관 관계 보다는, 훈련 데이터 자체에서 패턴을 찾아 분류하도록 인공 신경망을 학습시키는 학습 방법일 수 있다. 비 지도 학습의 예로는, 군 집화 또는 독립 성분 분석(Independent Component Analysis)을 들 수 있다. 본 명세서에서 용어 \"군집화\"는 용 어 \"클러스터링\"과 혼용되어 사용될 수 있다. 비지도 학습을 이용하는 인공 신경망의 일례로 생성적 적대 신경 망(GAN: Generative Adversarial Network), 오토 인코더(AE: Autoencoder)를 들 수 있다. 생성적 적대 신경망이란, 생성기(generator)와 판별기(discriminator), 두 개의 서로 다른 인공지능이 경쟁하며 성능을 개선하는 머신 러닝 방법이다. 이 경우 생성기는 새로운 데이터를 창조하는 모형으로, 원본 데이터를 기 반으로 새로운 데이터를 생성할 수 있다. 또한 판별기는 데이터의 패턴을 인식하는 모형으로, 입력된 데이터가 원본 데이터인지 또는 생성기에서 생성한 새로운 데이터인지 여부를 감별하는 역할을 수행할 수 있다. 그리고 생성기는 판별기를 속이지 못한 데이터를 입력 받아 학습하며, 판별기는 생성기로부터 속은 데이터를 입력 받아 학습할 수 있다. 이에 따라 생성기는 판 별기를 최대한 잘 속이도록 진화할 수 있고, 판별기는 원본 데이터와 생성기에 의해 생성된 데이터를 잘 구분하 도록 진화할 수 있다. 전술한 본 발명은, 프로그램이 기록된 매체에 컴퓨터가 읽을 수 있는 코드로서 구현하는 것이 가능하다. 컴퓨터 가 읽을 수 있는 매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 컴퓨터가 읽을 수 있는 매체의 예로는, HDD(Hard Disk Drive), SSD(Solid State Disk), SDD(Silicon Disk Drive), ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있으며, 또한, 상기 컴퓨터는 로봇의 프로세서을 포함할 수도 있다. 앞에서, 본 발명의 특정한 실시예가 설명되고 도시되었지만 본 발명은 기재된 실시예에 한정되는 것이 아니고, 이 기술 분야에서 통상의 지식을 가진 자는 본 발명의 사상 및 범위를 벗어나지 않고서 다른 구체적인 실시예로 다양하게 수정 및 변형할 수 있음을 이해할 수 있을 것이다. 따라서, 본 발명의 범위는 설명된 실시예에 의하여 정하여 질 것이 아니고 청구범위에 기재된 기술적 사상에 의해 정하여져야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13"}
{"patent_id": "10-2019-0079332", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시 예에 따른 로봇의 외관을 나타내는 도면, 도 2는 본 발명의 일 실시 예에 따른 로봇에 배치된 복수의 마이크를 가상적으로 상부에서 바라본 도면, 도 3은 본 발명의 일 실시 예에 따른 로봇의 구성을 나타내는 블록도, 도 4는 본 발명의 일 실시 예에 따른 감지 범위 내의 기준 음원에서 발생된 사운드의 CDR 정보를 산출하는 방법 을 설명하기 위한 도면, 도 5는 본 발명의 일 실시 예에 따른 기준 CDR 에 기초하여 사운드의 CDR 정보를 추출하는 방법을 나타내는 시 퀀스도, 도 6 및 도 7은 본 발명의 일 실시 예에 따른 음원의 위치에 기초한 로봇의 구동 방법을 나타내는 시퀀스도들, 도 8 내지 도 10은 본 발명의 일 실시 예에 따른 로봇의 위치가 변경되는 경우, 사운드 맵을 갱신하는 과정을 설명하기 위한 도면들, 도 11 및 도 12는 본 발명의 일 실시 예에 따른 촬영 범위를 벗어난 음원이 로봇으로 근접하는 경우 로봇의 구 동을 설명하기 위한 도면들, 그리고, 도 13은 복수의 사운드 중에서 근거리의 사운드를 선택하여 이용하는 로봇의 구동을 설명하기 위한 도면이다."}
