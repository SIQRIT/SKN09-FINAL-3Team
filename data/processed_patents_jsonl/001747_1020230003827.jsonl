{"patent_id": "10-2023-0003827", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0114195", "출원번호": "10-2023-0003827", "발명의 명칭": "감정 분석 결과 제공 장치 및 감정 분석 결과 제공 시스템", "출원인": "주식회사 허니엠앤비", "발명자": "김석원"}}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "내담자 단말로부터 실시간 수신한 동영상데이터로부터 추출된 제1 음성데이터 및 제1 영상데이터 중 적어도 하나에 기초해, 내담자의 감정을 실시간 분석하여 복수의 제1 분석 결과를 획득하는 제1 분석결과획득부; 및상기 실시간 수신한 동영상데이터가 소정 시간 동안 누적된 누적 동영상데이터로부터 추출된 제2 음성데이터 및제2 영상데이터 중 적어도 하나에 기초해, 상기 내담자의 감정을 상기 소정 시간 경과 후 분석하여 복수의 제2분석 결과를 획득하는 제2 분석결과획득부;를 포함하는,감정 분석 결과 제공 장치."}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 복수의 제1 분석 결과는,복수의 일반 감정 상태 각각의 확률값, 대표 감정 상태, 심박수와 스트레스 지수, 키워드별 빈도수, 토픽별로구성된 키워드들 각각에 대한 빈도수, 음성 파라미터, 시선 위치 정보, 및 얼굴 움직임 정보 중 적어도 둘 이상을 포함하는,감정 분석 결과 제공 장치."}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서, 상기 복수의 제2 분석 결과는,복수의 일반 감정 상태 각각의 확률값, 복수의 치환 감정 상태 각각의 확률값, 심박수와 스트레스 지수, 키워드별 빈도수, 토픽별로 구성된 키워드들 각각에 대한 빈도수, 음성 파라미터, 시선 위치 정보, 및 얼굴 움직임 정보 중 적어도 둘 이상을 포함하는, 감정 분석 결과 제공 장치."}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3항에 있어서,상기 복수의 치환 감정 상태는 상기 복수의 일반 감정 상태 각각의 상위 개념으로 정의되는 감정 상태의조합인,감정 분석 결과 제공 장치."}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 3항에 있어서,상기 제2 분석결과획득부는,상기 제2 영상데이터, 상기 제2 음성데이터, 및 상기 제2 음성데이터가 변환된 텍스트데이터에 기초해 앙상블공개특허 10-2023-0114195-3-학습에 기반한 복수의 모델을 이용해 복수의 일반 감정 상태 각각의 확률값을 획득하는,감정 분석 결과 제공 장치."}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 3항에 있어서,상기 제2 분석결과획득부는,상기 제2 음성데이터가 변환된 텍스트데이터에 대해 잠재 디리클레 할당(LDA, Latent Dirichlet Allocation) 토픽 모델링을 수행하여 상기 토픽별로 구성된 키워드들 각각에 대한 빈도수를 획득하는,감정 분석 결과 제공 장치."}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 3항에 있어서,상기 제2 분석결과획득부는,상기 제2 영상데이터에 기초해 내담자의 얼굴을 인식하고, 상기 인식된 얼굴의 특징점을 검출하며, 상기 얼굴의특징점을 기초로 지정된 관심 영역에서 동공 중심 위치를 검출하고, 상기 검출된 동공 중심 위치의 시간에 따른변화 정도인 상기 시선 위치 정보를 유클리디안 거리의 변화에 기초해 산출하는,감정 분석 결과 제공 장치."}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "내담자 단말, 상담자 단말, 및 감정 분석 결과 제공 장치를 포함한 감정 분석 결과 제공 시스템에 있어서,상기 감정 분석 결과 제공 장치는,내담자 단말로부터 실시간 수신한 동영상데이터로부터 추출된 제1 음성데이터 및 제1 영상데이터 중 적어도 하나에 기초해, 내담자의 감정을 실시간 분석하여 복수의 제1 분석 결과를 획득하는 제1 분석결과획득부;상기 실시간 수신한 동영상데이터가 소정 시간 동안 누적된 누적 동영상데이터로부터 추출된 제2 음성데이터 및제2 영상데이터 중 적어도 하나에 기초해, 상기 내담자의 감정을 상기 소정 시간 경과 후 분석하여 복수의 제2분석 결과를 획득하는 제2 분석결과획득부; 및상기 복수의 제1 분석 결과 전체를 포함한 정보 및 상기 복수의 제2 분석 결과 전체를 포함한 정보를 상기 상담자 단말로 전송하는 전송부;를 포함하고,상기 상담자 단말은, 상기 감정 분석 결과 제공 장치로부터 수신한 상기 복수의 제1 분석 결과 전체를 포함한 정보를 하나의 사용자인터페이스 화면을 통해 제공하는 제1 분석결과제공부; 및상기 감정 분석 결과 제공 장치로부터 수신한 상기 복수의 제2 분석 결과 전체 중 사용자에 의해 선택된 제2 분석 결과를 포함한 정보를 상기 하나의 사용자 인터페이스 화면을 통해 제공하는 제2 분석결과제공부;을 포함하는,감정 분석 결과 제공 시스템."}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8항에 있어서, 공개특허 10-2023-0114195-4-상기 하나의 사용자 인터페이스 화면은 상기 복수의 제2 분석 결과를 나타내는 복수의 메뉴를 포함하고, 상기 제2 분석결과제공부는, 상기 복수의 메뉴 중 사용자에 의해 선택된 메뉴에 대응되는 상기 제2 분석 결과를 포함한 정보를 상기 하나의사용자 인터페이스 화면을 통해 제공하는,감정 분석 결과 제공 시스템."}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 8항에 있어서, 상기 복수의 제1 분석 결과는,복수의 일반 감정 상태 각각의 확률값, 대표 감정 상태, 심박수와 스트레스 지수, 키워드별 빈도수, 토픽별로구성된 키워드들 각각에 대한 빈도수, 음성 파라미터, 시선 위치 정보, 및 얼굴 움직임 정보 중 적어도 둘 이상을 포함하는,감정 분석 결과 제공 시스템."}
{"patent_id": "10-2023-0003827", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 8항에 있어서, 상기 복수의 제2 분석 결과는,복수의 일반 감정 상태 각각의 확률값, 복수의 치환 감정 상태 각각의 확률값, 심박수와 스트레스 지수, 키워드별 빈도수, 토픽별로 구성된 키워드들 각각에 대한 빈도수, 음성 파라미터, 시선 위치 정보, 및 얼굴 움직임 정보 중 적어도 둘 이상을 포함하는, 감정 분석 결과 제공 시스템."}
{"patent_id": "10-2023-0003827", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "실시예에 따른 감정 분석 결과 제공 장치는 내담자 단말로부터 실시간 수신한 동영상데이터로부터 추출된 제1 음 성데이터 및 제1 영상데이터 중 적어도 하나에 기초해, 내담자의 감정을 실시간 분석하여 복수의 제1 분석 결과 를 획득하는 제1 분석결과획득부; 및 상기 실시간 수신한 동영상데이터가 소정 시간 동안 누적된 누적 동영상데이터로부터 추출된 제2 음성데이터 및 제2 영상데이터 중 적어도 하나에 기초해, 상기 내담자의 감정을 상기 소정 시간 경과 후 분석하여 복수의 제2 분석 결과를 획득하는 제2 분석결과획득부;를 포함할 수 있다."}
{"patent_id": "10-2023-0003827", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 감정 분석 결과 제공 장치 및 감정 분석 결과 제공 시스템에 관한 것으로, 보다 구체적으로, 심리 상 담 시의 동영상데이터를 기초로 실시간 내담자의 감정을 분석하면서도 누적된 동영상데이터를 기초로 또한 내담 자의 감정을 분석하도록 함으로써, 내담자의 감정을 다각도로 분석 및 통합된 결과를 제공하기 위한, 감정 분석 결과 제공 장치 및 감정 분석 결과 제공 시스템에 관한 것이다."}
{"patent_id": "10-2023-0003827", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "심리 상담은 전문 지식을 갖춘 상담사가 심리적 문제를 지난 내담자와의 관계에서 공감적 이해, 무조건적 긍정 적 존중, 진실성을 기본으로 상담심리의 여러 이론들, 정신분석, 행동주의, 인본주의, 인지주의, 형태주의, 현 실요법, 교류분석, 가족치료 등의 내용을 이용하여 그들의 문제 해결을 돕는 치료 방법으로서 내담자가 인간의 사고, 감정, 행동, 대인관계에 대해 탐색하도록 안내하여 다양한 자신의 문제들을 이해하고 변화하도록 돕는 것 을 말한다. 이러한 심리 상담은 주로, 주어진 상담실에서 면대면으로 행해지던 전통적인 상담 방식으로 이루어졌는데, 대인 관계에서의 문제를 지니고 있거나 사회 불안이 심한 내담자나 환자들은 상담자와 직접 마주해야 하는 면대면 상 담 및 심리치료가 부담되거나 꺼려져 심리치료가 지속되지 못하고 중단되기도 한다. 그래서 면대면 상담의 대안으로 온라인 상에서 심리를 상담하고 치료하는 온라인 심리치료 프로그램이 제시되고 있다. 선행기술문헌 특허문헌 (특허문헌 0001) KR 2022-0005945 A"}
{"patent_id": "10-2023-0003827", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 온라인 심리 치료 프로그램을 진행 시의 동영상데이터를 기초로 실시간 내담자의 감정을 분석하면서 도 누적된 동영상데이터를 기초로 또한 내담자의 감정을 분석하도록 함으로써, 내담자의 감정을 다각도로 분석 및 통합된 결과를 제공하고자 하는 데에 그 목적이 있다."}
{"patent_id": "10-2023-0003827", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "실시예에 따른 감정 분석 결과 제공 장치는 내담자 단말로부터 실시간 수신한 동영상데이터로부터 추출된 제1 음성데이터 및 제1 영상데이터 중 적어도 하나에 기초해, 내담자의 감정을 실시간 분석하여 복수의 제1 분석 결 과를 획득하는 제1 분석결과획득부; 및 상기 실시간 수신한 동영상데이터가 소정 시간 동안 누적된 누적 동영상데이터로부터 추출된 제2 음성데이터 및 제2 영상데이터 중 적어도 하나에 기초해, 상기 내담자의 감정을 상기 소정 시간 경과 후 분석하여 복수의 제2 분석 결과를 획득하는 제2 분석결과획득부;를 포함할 수 있다. 상기 복수의 제1 분석 결과는, 복수의 일반 감정 상태 각각의 확률값, 대표 감정 상태, 심박수와 스트레스 지수, 키워드별 빈도수, 토픽별로 구성된 키워드들 각각에 대한 빈도수, 음성 파라미터, 시선 위치 정보, 및 얼굴 움직임 정보 중 적어도 둘 이상 을 포함할 수 있다. 상기 복수의 제2 분석 결과는, 복수의 일반 감정 상태 각각의 확률값, 복수의 치환 감정 상태 각각의 확률값, 심박수와 스트레스 지수, 키워드 별 빈도수, 토픽별로 구성된 키워드들 각각에 대한 빈도수, 음성 파라미터, 시선 위치 정보, 및 얼굴 움직임 정 보 중 적어도 둘 이상을 포함할 수 있다. 상기 복수의 치환 감정 상태는 상기 복수의 일반 감정 상태 각각의 상위 개념으로 정의되는 감정 상태의 조합일 수 있다. 상기 제2 분석결과획득부는, 상기 제2 영상데이터, 상기 제2 음성데이터, 및 상기 제2 음성데이터가 변환된 텍스트데이터에 기초해 앙상블 학습에 기반한 복수의 모델을 이용해 복수의 일반 감정 상태 각각의 확률값을 획득할 수 있다. 상기 제2 분석결과획득부는, 상기 제2 음성데이터가 변환된 텍스트데이터에 대해 잠재 디리클레 할당(LDA, Latent Dirichlet Allocation) 토 픽 모델링을 수행하여 상기 토픽별로 구성된 키워드들 각각에 대한 빈도수를 획득할 수 있다. 상기 제2 분석결과획득부는, 상기 제2 영상데이터에 기초해 내담자의 얼굴을 인식하고, 상기 인식된 얼굴의 특징점을 검출하며, 상기 얼굴의 특징점을 기초로 지정된 관심 영역에서 동공 중심 위치를 검출하고, 상기 검출된 동공 중심 위치의 시간에 따른 변화 정도인 상기 시선 위치 정보를 유클리디안 거리의 변화에 기초해 산출할 수 있다.실시예에 따른 감정 분석 결과 제공 시스템은 내담자 단말, 상담자 단말, 및 감정 분석 결과 제공 장치를 포함 한 감정 분석 결과 제공 시스템에 있어서, 상기 감정 분석 결과 제공 장치는, 내담자 단말로부터 실시간 수신한 동영상데이터로부터 추출된 제1 음성데이터 및 제1 영상데이터 중 적어도 하 나에 기초해, 내담자의 감정을 실시간 분석하여 복수의 제1 분석 결과를 획득하는 제1 분석결과획득부; 상기 실시간 수신한 동영상데이터가 소정 시간 동안 누적된 누적 동영상데이터로부터 추출된 제2 음성데이터 및 제2 영상데이터 중 적어도 하나에 기초해, 상기 내담자의 감정을 상기 소정 시간 경과 후 분석하여 복수의 제2 분석 결과를 획득하는 제2 분석결과획득부; 및 상기 복수의 제1 분석 결과 전체를 포함한 정보 및 상기 복수의 제2 분석 결과 전체를 포함한 정보를 상기 상담 자 단말로 전송하는 전송부;를 포함하고, 상기 상담자 단말은, 상기 감정 분석 결과 제공 장치로부터 수신한 상기 복수의 제1 분석 결과 전체를 포함한 정보를 하나의 사용자 인터페이스 화면을 통해 제공하는 제1 분석결과제공부; 및 상기 감정 분석 결과 제공 장치로부터 수신한 상기 복수의 제2 분석 결과 전체 중 사용자에 의해 선택된 제2 분 석 결과를 포함한 정보를 상기 하나의 사용자 인터페이스 화면을 통해 제공하는 제2 분석결과제공부;을 포함할 수 있다. 상기 하나의 사용자 인터페이스 화면은 상기 복수의 제2 분석 결과를 나타내는 복수의 메뉴를 포함하고, 상기 제2 분석결과제공부는, 상기 복수의 메뉴 중 사용자에 의해 선택된 메뉴에 대응되는 상기 제2 분석 결과를 포함한 정보를 상기 하나의 사용자 인터페이스 화면을 통해 제공할 수 있다."}
{"patent_id": "10-2023-0003827", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 제1 분석결과획득부를 통해 실시간 분석으로 제1 분석 결과를 획득하지만, 제2 분석결과획득 부를 통해 소정 시간 경과 후 분석으로 제2 분석 결과를 획득하도록 함으로써, 동일한 내담자와의 실시간 상담 내역을 실시간 분석을 통해 실시간 제공받을 수 있도록 하면서도, 상담이 종료된 후 전체 상담 내역을 통합 분 석을 통해 제공받을 수 있도록 함으로써, 하나의 상담 내역에 대해 다양한 형태의 분석 결과를 제공받을 수 있 게 되어 보다 다양화되고 체계화된 상담 내역의 관리가 가능해질 수 있게 된다. 또한, 상담 진행 중일 때의 사용자 인터페이스 화면과 상담이 종료된 후의 분석 결과를 제공하는 사용자 인터페 이스 화면을 서로 상이하게 구현하여 상담자의 상담 내역 관리 및 확인이 보다 용이해지게 된다. 예를 들어, 상 담이 진행중인 경우, 실시간 분석 결과를 제공받도록 하면서도 전체 분석 결과를 하나의 사용자 인터페이스부의 화면을 통해 제공받도록 함으로써, 상담자가 실시간 상담 내역 확인, 분석, 및/또는 판단이 보다 신속하고 용이 해질 수 있으며, 상담이 종료된 경우, 누적된 전체 상담 내역에 대해 하나의(선택된/개별적인) 분석 결과를 하 나의 사용자 인터페이스부의 화면을 통해 제공받도록 함으로써, 전체 상담 내역을 사용자가 통합적으로 확인할 수 있게 된다."}
{"patent_id": "10-2023-0003827", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "후술하는 본 발명에 대한 상세한 설명은, 본 발명이 실시될 수 있는 특정 실시예를 예시로서 도시하는 첨부 도 면을 참조한다. 이들 실시예는 당업자가 본 발명을 실시할 수 있기에 충분하도록 상세히 설명된다. 본 발명의 다양한 실시예는 서로 다르지만 상호 배타적일 필요는 없음이 이해되어야 한다. 예를 들어, 여기에 기재되어 있는 특정 형상, 구조 및 특성은 일 실시예에 관련하여 본 발명의 정신 및 범위를 벗어나지 않으면서 다른 실시 예로 구현될 수 있다. 또한, 각각의 개시된 실시예 내의 개별 구성요소의 위치 또는 배치는 본 발명의 정신 및 범위를 벗어나지 않으면서 변경될 수 있음이 이해되어야 한다. 따라서, 후술하는 상세한 설명은 한정적인 의미 로서 취하려는 것이 아니며, 본 발명의 범위는, 적절하게 설명된다면, 그 청구항들이 주장하는 것과 균등한 모 든 범위와 더불어 첨부된 청구항에 의해서만 한정된다. 도면에서 유사한 참조부호는 여러 측면에 걸쳐서 동일하 거나 유사한 기능을 지칭한다. 참고로, 본 발명의 각 순서도에 있어서, 각 단계는 일예이며, 각 순서를 다르게 변경 및/또는 조합한 경우에도 본 발명이 동일/유사하게 적용될 수 있다. 도 1은 실시예에 따른 감정 분석 결과 제공 시스템의 시스템도이다. 내담자 단말은 텍스트/음성/영상 상담을 진행하기 위한 어플리케이션을 구비하여 감정 분석 결과 제공 장치 에 연결될 수 있고, 이를 통해 상담자 단말과 텍스트/음성/영상 상담을 진행할 수 있다. 내담자 단말 은 상담자 모습이 출력되는 화면과, 내담자의 모습을 촬영하여 상담자 단말로 전송할 수 있는 카메라 (미도시)와 음성을 수집하여 상담자와 대화를 주고받을 수 있도록 하는 마이크(미도시) 및 스피커(미도시)가 적 어도 구비되어야 한다. 내담자 단말은 카메라(미도시)를 이용해 내담자의 얼굴이 촬영되어 획득된 영상데이터와 마이크(미도시)를 이용해 수집된 내담자의 음성데이터를 포함한 동영상데이터를 실시간 감정 분석 결과 제공 장치로 전송할 수 있다. 실시예에 따라, 내담자 단말은 감정 분석 결과 제공 장치로부터 수신한 감정 분석 결과를 사용자 인터페 이스부(미도시)의 화면을 통해 출력하여 내담자에게 제공할 수 있다. 상담자 단말은 텍스트/음성/영상 상담을 진행하기 위한 어플리케이션을 구비하여 감정 분석 결과 제공 장치 에 연결될 수 있고, 이를 통해 내담자 단말과 텍스트/음성/영상 상담을 진행할 수 있다. 상담자 단말 은 내담자 모습이 출력되는 화면과, 상담자의 모습을 촬영하여 내담자 단말로 전송할 수 있는 카메라와 음성을 수집하여 내담자와 대화를 주고받을 수 있도록 하는 마이크 및 스피커가 적어도 구비되어야 한다. 상담자 단말은 카메라(미도시)를 이용해 상담자의 상담 얼굴이 촬영되어 획득된 영상데이터와 마이크(미도 시)를 이용해 수집된 상담자의 음성데이터를 포함한 동영상데이터를 실시간 감정 분석 결과 제공 장치로 전 송할 수 있다. 상담자 단말은 감정 분석 결과 제공 장치로부터 수신한 감정 분석 결과를 사용자 인터페이스부의 화면을 통해 출력하여 상담자 또는 관리자(사용자)에게 제공할 수 있다. 감정 분석 결과 제공 장치는 내담자 단말과 상담자 단말의 화상 상담을 서로 중개하고, 화상 상담 에 따른 내담자의 감정 분석 결과를 내담자 단말 및/또는 상담자 단말로 전송할 수 있다. 도 2 및 도 3은 실시예에 따른 감정 분석 결과 제공 장치의 동작을 설명하는 순서도이다. 실시예에 따라, 제1 분석결과획득부(21a)는 내담자 단말로부터 실시간 동영상데이터를 수신할 수 있다 (s11). 동영상데이터는 내담자의 얼굴이 촬영된 제1 영상데이터 및 제1 음성데이터를 포함할 수 있다. 제1 분석결과획득부(21a)는 동영상데이터로부터 제1 음성데이터 및 제1 영상데이터를 추출할 수 있다(s12). 실시예에 따라, 제1 영상데이터는 실시간 수신한 단일 영상프레임이고, 제1 음성데이터는 실시간 수신한 단일 음성프레임일 수 있다. 실시예에 따라, 동영상데이터로부터 제1 영상데이터 및 제1 음성데이터를 각각 추출하는 것은 공지의 다양한 알 고리즘을 통해 구현될 수 있다. 제1 분석결과획득부(21a)는 제1 음성데이터 및 제1 영상데이터 중 적어도 하나에 기초해, 내담자의 감정을 실시 간 분석하여 복수의 제1 분석 결과를 획득할 수 있다(s13). 실시예에 따라, 제1 분석결과획득부(21a)는 음성데이터 및 영상데이터 중 적어도 하나에 기초해, 동일 카테고리 에 해당되는 분석 결과를 얻기 위해, 제2 분석결과획득부(21b)와 동일/유사한 분석 방법 또는 공지의 다양한 알 고리즘을 이용한 실시간 분석을 통해 복수의 제1 분석 결과를 실시간 획득할 수 있다. 이에, 복수의 제1 분석 결과를 획득하기 위한 분석에 대해서는 자세한 설명을 생략한다. 예를 들어, 제2 분석결과획득부(21b)가 복수의 일반 감정 상태 각각의 확률값을 획득하기 위한 분석 방법은 제1 분석결과획득부(21a)가 복수의 일반 감정 상태 각각의 확률값을 획득하는데 동일/유사하게 이용될 수 있다. 실시예에 따라, 복수의 제1 분석 결과는, 복수의 일반 감정 상태 각각의 확률값, 대표 감정 상태, 심박수와 스 트레스 정도, 키워드별 빈도수, 토픽별로 구성된 키워드들 각각에 대한 빈도수, 음성 파라미터, 시선 위치 정보, 및 얼굴 움직임 정보 중 적어도 둘 이상을 포함할 수 있다. 실시예에 따라, 복수의 일반 감정 상태는 해당 시각의 기쁨, 분노, 경멸, 놀람, 두려움, 평온 및 슬픔 중 적어 도 둘 이상을 포함할 수 있으나, 본 발명은 이에 한정되지 않는다. 실시예에 따라, 제1 분석결과획득부(21a)는 제1 영상데이터와 제1 음성데이터에 기초해 복수의 일반 감정 상태 각각의 확률값을 획득할 수 있으며, 이는, 제2 분석결과획득부(21b)가 제2 영상데이터와 제2 음성데이터에 기초 해 복수의 일반 감정 상태 각각의 확률값을 획득하는 것과 동일/유사하게 적용될 수 있다. 실시예에 따라, 대표 감정 상태는 해당 시각의 복수의 일반 감정 상태 중 내담자의 감정으로 대표될 수 있는 최 종 감정 상태일 수 있다. 예를 들어, 제1 분석결과획득부(21a)는 복수의 각 일반 감정 상태의 확률값 중 가장 높은 수치의 확률값에 대응되는 일반 감정 상태를 해당 시각의 대표 감정 상태로 결정할 수 있다. 실시예에 따라, 심박수는 RPPG 수치로 획득되며, 스트레스 정도는 높음/중간/낮음 등의 레벨로 획득될 수 있다. 실시예에 따라, 제1 분석결과획득부(21a)는 제1 영상데이터에 기초해 심박수와 스트레스 지수를 획득할 수 있다. 이는, 제2 분석결과획득부(21b)의 제2 영상데이터에 기초해 심박수와 스트레스 지수를 획득하는 것과 동 일/유사하게 적용될 수 있다. 실시예에 따라, 키워드별 빈도수는 기 설정된 키워드 중 내담자에 의해 발화된 키워드별 빈도수로 정의될 수 있 다. 실시예에 따라, 기 설정된 키워드는 실시간 상담자 또는 관리자 등의 사용자에 의해 변경되어 설정될 수 있 다. 실시예에 따라, 제1 분석결과획득부(21a)는 제1 음성데이터가 변환된 텍스트데이터에 대한 분석을 수행해 기 설 정된 키워드별 빈도수를 획득할 수 있다. 이는, 제2 분석결과획득부(21b)가 제2 음성데이터가 변환된 텍스트데이터에 대한 분석을 수행해 기 설정된 키워 드별 빈도수를 획득하는 것과 동일/유사하게 적용될 수 있다. 실시예에 따라, 제1 분석결과획득부(21a)는 제1 음성데이터가 변환된 텍스트데이터에 대해 잠재 디리클레 할당 (LDA, Latent Dirichlet Allocation) 토픽 모델링을 수행하여 상기 토픽별로 구성된 키워드들 각각에 대한 빈도 수를 획득할 수 있으며, 이는 제2 분석결과획득부(21b)가 제2 음성데이터가 변환된 텍스트데이터에 대해 잠재 디리클레 할당(LDA, Latent Dirichlet Allocation) 토픽 모델링을 수행하여 상기 토픽별로 구성된 키워드들 각 각에 대한 빈도수를 획득하는 것과 동일/유사하게 적용될 수 있다. 실시예에 따라, 제1 분석결과획득부(21a)는 제1 음성데이터를 기초로 분석하여 음성 파라미터를 획득할 수 있으 며, 이는, 제2 분석결과획득부(21b)가 제2 음성데이터를 기초로 분석하여 음성 파라미터를 획득하는 것과 동일/ 유사하게 적용될 수 있다. 실시예에 따라 음성 파라미터는 파형, 피치(pitch), 포즈(pause) 구간, 발화 속도, 및/또는 망설임 구간을 나타 내는 지표를 포함할 수 있다. 실시예에 따라, 제1 분석결과획득부(21a)는 제1 영상데이터를 기초로 분석하여 시선 위치 정보 및 얼굴 움직임 정보를 획득할 수 있으며, 이는, 제2 분석결과획득부(21b)가 제2 영상데이터를 기초로 분석하여 시선 위치 정보 및 얼굴 움직임 정보를 획득하는 것과 동일/유사하게 적용될 수 있다.실시예에 따라, 제1 분석결과획득부(21a)는 복수의 일반 감정 상태를 막대 그래프 형태로 생성하고, 대표 감정 상태를 아이콘 형태 및/또는 방사형 그래프에 적용해 생성하며, 기 설정된 키워드별 빈도수를 원의 크기에 반영 해 생성하며, 토픽별로 구성된 키워드들 각각에 대한 빈도수를 테이블 형태로 생성할 수 있다. 실시예에 따라, 기 설정된 키워드별 빈도수의 경우, 빈도수에 대응되는 레벨(예> 1 내지 5레벨)에 따라 원의 크기에 반영되도록 하여 생성할 수 있다. 예를 들어, 1레벨은 1번 내담자에 의해 발화된 것을 나타내어 상대적으로 작은 크기의 원 으로 표현되고, 5레벨은 5번 이상 내담자에 의해 발화된 것을 나타내어 상대적으로 큰 크기의 원으로 표현될 수 있다. 다만, 이는 실시예이며, 복수의 제1 분석 결과 각각에 대해서 막대 그래프 형태, 아이콘 형태, 방사형 그래프 형태, 원형 형태 등 다양하게 적용하여 생성할 수 있다. 전송부(21c)는 복수의 제1 분석 결과 전체를 포함한 정보를 상담자 단말로 전송할 수 있다(s14). 실시예에 따라 복수의 제1 분석 결과 전체를 포함한 정보는, 복수의 제1 분석 결과 전체, 내담자 단말로부 터 실시간 수신한 동영상데이터, 및 상기 실시간 수신한 동영상데이터로부터 분리된 내담자의 제1 음성데이터가 변환된 텍스트데이터와 상담자의 음성데이터가 변환된 텍스트데이터의 컨텐츠를 포함할 수 있다. 도 4는 상담자 단말의 제1 분석결과제공부의 동작에 따른 사용자 인터페이스부의 화면을 예시한다. 상담자 단말의 제1 분석결과제공부는 감정 분석 결과 제공 장치로부터 수신한 복수의 제1 분석 결 과 전체를 포함한 정보와 상담자가 실시간 기록한 상담 노트를 하나의 사용자 인터페이스부의 화면을 통해 출력함으로써 관리자 또는 상담자등의 사용자에게 제공할 수 있다. 실시예에 따라 복수의 제1 분석 결과 전체를 포함한 정보는, 복수의 제1 분석 결과 전체, 내담자가 촬영된 동영 상데이터, 및 내담자가 촬영된 동영상데이터로부터 분리된 내담자의 음성데이터가 변환된 텍스트데이터와 상담 자의 음성데이터가 변환된 텍스트데이터에 대응되는 컨텐츠를 포함할 수 있다. 실시예에 따라, 각 컨텐츠는 사용자 인터페이스부의 화면 상의 분리된 영역에 각각 출력될 수 있다. 예를 들어, 도 4는 사용자 인터페이스부의 화면에 대한 예시로, 복수의 제1 분석 결과(①), 내담자 단말 로부터 실시간 수신한 동영상데이터(②), 내담자의 음성데이터가 변환된 텍스트데이터와 상담자의 음성데이 터가 변환된 텍스트데이터(④), 상담자가 실시간 기록한 상담 노트(③)를 포함할 수 있다. (1-1)은 복수의 제1 분석 결과(①) 중 복수의 일반 감정 상태를 나타내며, (1-2)는 대표 감정 상태를, (1-3)은 심박수와 스트레스 지수를, (1-4)는 키워드별 빈도수를, (1-5)는 토픽별로 구성된 키워드들 각각에 대한 빈도수 를 나타낸다. 실시예에 따라 제2 분석결과획득부(21b)는 실간 수신한 동영상데이터가 소정 시간 동안 누적된 누적 동영상데이 터를 생성할 수 있다(s21). 실시예에 따라, 제2 분석결과획득부(21b)는 내담자 단말로부터 동영상데이터의 수신을 시작한 시점과 내담 자 단말로부터 동영상데이터의 수신을 종료한 시점 사이의 구간 내의 수신 동영상데이터을 누적 동영상데이 터로 획득할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 내담자 단말로부터 수신한 동영상데이터의 기록이 시작된 시점 과 기록이 종료된 시점 사이의 구간 내의 동영상데이터를 누적 동영상데이터로 획득할 수 있다. 실시예에 따라, 상기 소정 시간 동안 누적된 동영상데이터를 분석 대상으로 정의할 것에 대한 명령어가 미리 프 로그래밍되어 있고, 제2 분석결과획득부(21b)는 해당 명령어들을 참조해 분석 대상을 정의할 수 있다. 실시예에 따라, 상기 소정 시간은 내담자와 상담자 사이의 상담이 시작될 때부터 종료될 때까지의 시간 구간에 대응될 수 있다. 실시예에 따라 제2 분석결과획득부(21b)는 누적 동영상데이터로부터 제2 음성데이터 및 제2 영상데이터를 추출 할 수 있다(s22). 실시예에 따라, 제1 영상데이터는 실시간 수신한 단일 영상프레임인 반면, 제2 영상데이터는 소정 시간 동안 누 적된 복수의 영상프레임을 포함할 수 있다. 실시예에 따라, 제1 음성데이터는 실시간 수신한 단일 음성프레임인 반면, 제2 음성데이터는 소정 시간 동안 누 적된 복수의 음성프레임을 포함할 수 있다. 실시예에 따라, 누적 동영상데이터로부터 제2 영상데이터와 제2 음성데이터를 각각 추출하는 것은 공지의 다양 한 알고리즘을 통해 구현될 수 있다. 제2 분석결과획득부(21b)는 제2 음성데이터 및 제2 영상데이터 중 적어도 하나에 기초해, 상기 내담자의 감정을 상기 소정 시간 경과 후 분석하여 복수의 제2 분석 결과를 획득할 수 있다(s23). 실시예에 따라, 소정 시간 경과 후 바로(또는 임의의 시간이 경과된 이후) 분석하거나, 사용자에 의해 설정된 시각에 분석할 것에 대한 명령어가 미리 프로그래밍되어 있을 수 있고, 제2 분석결과획득부(21b)는 해당 명령어 를 참조해 분석을 수행할 수 있다. 실시예에 따라, 복수의 제2 분석 결과는 복수의 일반 감정 상태 각각의 확률값, 복수의 치환 감정 상태 각각의 확률값, 심박수와 스트레스 지수, 키워드별 빈도수, 토픽별로 구성된 키워드들 각각에 대한 빈도수, 음성 파라 미터, 시선 위치 정보, 및 얼굴 움직임 정보 중 적어도 둘 이상을 포함할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 복수의 제2 분석 결과 중 적어도 일부를 그래프화하여 전송할 수 있 다. 실시예에 따라, 복수의 일반 감정 상태는 해당 시각(또는 시간)의 기쁨, 분노, 경멸, 놀람, 두려움, 평온 및 슬 픔 중 적어도 둘 이상을 포함할 수 있으나, 본 발명은 이에 한정되지 않는다. 실시예에 따라, 복수의 일반 감정 상태 각각의 확률값은 소정의 시간 구간 동안 획득된 일반 감정 상태의 확률 값을 평균화하여 획득될 수도 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 도 5와 같이 시간대별 복수의 일반 감정 상태 각각의 확률값을 그래 프화하여 생성 및 전송할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 제2 영상데이터와 제2 음성데이터에 기초해 복수의 일반 감정 상태 각각의 확률값을 획득할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 제2 영상데이터, 제2 음성데이터, 및 제2 음성데이터가 변환된 텍스 트데이터에 기초해 앙상블 학습에 기반한 복수의 모델을 이용해 복수의 일반 감정 상태 각각의 확률값을 획득할 수 있다. 구체적으로, 제2 분석결과획득부(21b)는 제2 영상데이터를 구성하는 복수의 영상프레임, 제2 음성데이터를 구성 하는 복수의 음성프레임, 복수의 음성프레임이 변환된 복수의 텍스트데이터(STT기반)를 이용해 앙상블 학습에 기반한 복수의 모델을 이용해 복수의 일반 감정 상태 각각의 확률값을 획득할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 학습용 영상프레임에 기초하여 학습된 제1 모델을 생성하고, 학습용 음성프레임에 기초하여 학습된 제2 모델을 생성하며, 학습용 텍스트데이터에 기초하여 학습된 제3 모델을 생성 하고, 제1 모델의 제1 가중치, 제2 모델의 제2 가중치, 및 제3 모델의 제3 가중치를 참조해 복수의 일반 감정 상태 각각의 확률값을 획득할 수 있다. 구체적으로, 복수의 영상프레임 중 적어도 하나의 영상프레임을 기초로 제1 모델을 통해 제1 분류값을 획득하 고, 복수의 영상프레임 중 적어도 하나의 음성프레임을 기초로 제2 모델을 통해 제2 분류값을 획득하며, 복수 의 텍스트데이터 중 적어도 하나의 텍스트데이터를 기초로 제3 모델을 통해 제3 분류값을 획득하고, 제1 분류값 에 제1 가중치를 적용하고, 2 분류값에 상기 제2 가중치를 적용하며, 상기 제3 분류값에 상기 제3 가중치를 적 용함으로써, 상기 영상프레임, 상기 음성프레임, 및 상기 텍스트데이터 각각을 기초로 한 복수의 일반 감정 상태 각각의 확률값을 획득할 수 있다. 그리고, 해당 과정을 상기 복수의 영상프레임과 상기 복수의 음성프레임, 그리고 상기 복수의 텍스트데이터에 수행하여 복수의 일반 감정 상태 각각의 확률값을 전체적으로 획득할 수 있다. 실시예에 따라, 상기 제1 모델은 MobileNet에 기반한 모델일 수 있다. 실시예에 따라, 상기 제2 모델은 SVM에 기반한 모델일 수 있다. 실시예에 따라, 상기 제3 모델은 Bert에 기반한 모델일 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 내담자의 영상프레임을 기초로 제1 모델을 통해 내담자의 일반 감정 상태를 나타내는 제1 분류값을 획득할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 영상프레임을 기초로 MobileNet 모델을 통해 내담자의 일반 감정 상 태를 나타내는 제1 분류값을 획득할 수 있다. 실시예에 따라, 제1 분류값은 복수의 일반 감정 상태 각각의 제1 예비확률값을 포함할 수 있다. 제2 분석결과획득부(21b)는 내담자의 음성프레임을 기초로 제2 모델을 통해 내담자의 일반 감정 상태를 나타내 는 제2 분류값을 획득할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 음성프레임을 기초로 SVM 알고리즘 기반의 제2 모델을 통해 통해 내 담자의 일반 감정 상태를 나타내는 제2 분류값을 획득할 수 있다. 실시예에 따라, 제2 분류값은 복수의 일반 감정 상태 각각의 제2 예비확률값을 포함할 수 있다. 제2 분석결과획득부(21b)는 텍스트데이터를 기초로 제3 모델을 통해 내담자의 일반 감정 상태를 나타내는 제3 분류값을 획득할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 텍스트데이터를 기초로 BERT 모델을 통해 제3 분류값을 획득할 수 있다. 실시예에 따라, 제3 분류값은 복수의 일반 감정 상태 각각의 제3 예비확률값을 포함할 수 있다. 구체적으로, 제2 분석결과획득부(21b)는 도 13을 참조하면, 복수의 일반 감정 상태 각각의 제1 예비확률값에 제 1 가중치를 각각 적용한 복수의 제1 결과, 상기 복수의 일반 감정 상태 각각의 제2 예비확률값에 제2 가중치를 각각 적용한 복수의 제2 결과, 및 상기 복수의 일반 감정 상태 각각의 제3 예비확률값에 제3 가중치를 각각 적 용한 복수의 제3 결과를 각각 획득하고, 상기 복수의 일반 감정 상태별로 제1 결과, 제2 결과 및 제3 결과를 각 각 합산하여 복수의 일반 일반 감정 상태 각각의 확률값을 획득할 수 있다. 예를 들어, 기쁨의 제1 예비확률값과 분노의 제1 예비확률값이 각각 [0.7,0.3]이고, 기쁨의 제2 예비확률값과 분노의 제2 예비확률값이 각각 [0.8,0.2]이며, 기쁨의 제3 예비확률값과 분노의 제3 예비확률값이 각각 [0.2,0.8]이며, 제1 모델의 가중치(a)가 0.3, 제2 모델의 가중치(b)가 0.5, 제3 모델의 가중치(c)가 0.2인 경 우, 기쁨에 대해 합산된 확률값은 0.7*0.3 + 0.8*0.5 + 0.2*0.2 = 0.65이고, 분노에 대해 합산된 확률값은 0.3*0.3 + 0.2*0.5 + 0.8*0.2 = 0.35으로 산출할 수 있다. 이하, 각 모델의 학습 과정에 대해서는 도 13을 참조하여 후술한다. 실시예에 따라, 복수의 치환 감정 상태는 복수의 일반 감정 상태 각각의 상위 개념으로 정의되는 감정 상태의 조합일 수 있다. 즉, 복수의 치환 감정 상태 각각은 치환 감정 상태의 하위 개념으로 정의되는 일반 감정 상태 가 하나 또는 복수 개 매핑되도록 정의될 수 있다. 예를 들어, 일반 감정 상태가 7가지로 표현될 수 있다면, 치 환 감정 상태는 4가지(긍정: 기쁨, 부정: 분노/경멸/놀람/두려움, 중립: 평온, 기타:슬픔)로 표현될 수 있다. 실시예에 따라, 복수의 치환 감정 상태 각각의 확률값은 상기 복수의 치환 감정 상태 각각에 대응되는 하나 또 는 복수의 일반 감정 상태의 확률값들을 합산하여 획득될 수 있다. 즉, 제2 분석결과획득부(21b)는 복수의 일반 감정 상태 각각의 확률값을 산출한 다음, 복수의 일반 감정 상태 각각 중 동일 그룹으로 분류되는 일반 감정 상 태의 확률값들을 합산하여 이를 치환 감정 상태의 확률값으로 산출할 수 있다. 예를 들어, 특정 시각(또는 시간)의 기쁨의 확률값이 20%, 분노의 확률값이 10%, 경멸의 확률값이 15%, 놀람의 확률값이 5%, 두려움의 확률값이 20%, 평온의 확률값이 10%, 슬픔의 확률값이 20%인 경우, 긍정의 확률값이 20% 이며, 부정의 확률값은 50%이며, 중립의 확률값은 10%이고, 기타의 확률값은 20%인 것으로 산출될 수 있다. 실시예에 따라, 복수의 치환 감정 상태 각각의 확률값은 소정의 시간 구간 동안 획득된 치환 감정 상태의 확률 값을 평균화하여 획득될 수도 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 도 6과 같이 시간대별 복수의 치환 감정 상태 각각의 확률값을 그래 프화하여 생성 및 전송할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 제2 영상데이터에 기초해 심박수와 스트레스 지수를 획득할 수 있다. 구체적으로, 제2 분석결과획득부(21b)는 제2 영상데이터를 구성하는 각각의 영상프레임을 이용해 rPPG(Remote photoplethysmography, 원격 광혈류측정) 신호를 연산하고, 연산된 rPPG 신호를 이용해 심박수를 추출할 수 있 다. 실시예에 따라, 유사하르방식(Haar cascade)알고리즘을 사용해 영상프레임으로부터 얼굴을 검출하고(ROI), 노이 즈제거 및 주파수 변환 후 피부색의 변화를 RGB 값의 채널로 변환한 후, 각 채널에서 평균 픽셀값의 변화를 측 정하며, 각 채널에서 추출한 값을 이용해 심박수를 계산할 수 있다. 그리고, 연산된 rPPG 신호로부터 추출된 심박수를 기초로 한 심박변이도 지표와 스트레스에 의한 자율신경계 활 성화의 정적 상관관계를 이용해 스트레스 지수를 산출할 수 있다. 실시예에 따라, 심박변이도 지표는 SDNN(standard deviation of all RR interval) 값, RMSSD(square root of the mean squared differences of successive normal sinus intervals) 값, SDNN/RMSSD비율, NN(normal to normal interval), NN50(연속된 NN 간격들의 변이가 50 ms보다 큰 경우의 수), pNN50(모든 NN 간격 중의 NN50 의 비율), LF(low frequency) 값, HF(high frequency) 값, 또는 LF/HF 값 등을 포함할 수 있다. 예를 들면, SDNN 값에서 간격이 넓을수록 변화도가 크다는 뜻이며, 일반적으로 변화도가 클수록 건강하다는 것 을 이용하여 육체의 피로도를 연산할 수 있고, RMSSD 값의 수치가 높으면 심장이 안정도가 높다는 것을 이용하 여, 부교감 신경의 활동을 알 수 있다. LF 값의 경우 교감신경계 활성도를 반영하는데, 정신 스트레스와 피로감 에 영향을 받으며, 특히 급성 스트레스 정도를 평가할 수 있고, 우울 또는 분노와 상관관계가 있으며, HF 값의 경우 부교감신경계 활성도를 반영하는데, 호흡 활동과 밀접한 관련이 있으며, 장기적인 스트레스, 불안, 또는 공포와 상관관계가 있다. LF/HF 값은 자율신경계의 전체적인 균형 정도를 반영하는데, 일반적으로 건강한 사람 은 깨어 있는 동안에 LF 값이 HF 값보다 높다. 따라서, rPPG 신호로부터 추출된 심박변이도 지표와 스트레스에 의한 자율신경계 활성화의 정적 상관관계를 이용함으로써, 스트레스 지수를 산출할 수 있다. 실시예에 따라, 상담이 시작된 후 5분 동안 획득된 각 심박변이도 지표를 이용해 평균값을 산출하고, 이를 기준 값으로 설정해 5분 이후의 심박변이도 지표를 산출하여 기준값과 비교하면서 분석할 수 있다. 다만, 이는 일예이며, 공지의 다양한 방식에 의해 심박수, 심박변이도, 및 스트레스 지수를 산출할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 도 7과 같이 시간대별 심박수와 스트레스 지수를 그래프화하여 생성 및 전송할 수 있다. 실시예에 따라, 2 분석결과획득부(21b)는 시간대별 심박수와 스트레스 지수를 하나의 그래 프 상에 나타낼 수 있다. 실시예에 따라, 2 분석결과획득부(21b)는 심박수와 스트레스 지수를, 측정 수치 자체 및/또는 측정 수치에 대응되는 분류 정보(정상/주의/경계 등) (정상스트레스/보통스트레스/심한스트레스 등)로 표현된 그래프를 작성하여 전송할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 제2 음성데이터가 변환된 텍스트데이터에 대한 분석을 수행해 기 설 정된 키워드별 빈도수를 획득할 수 있다. 구체적으로, 제2 분석결과획득부(21b)는 제2 음성데이터를 구성하는 각각의 음성프레임이 변환된 텍스트데이터 (STT를 통한) 전체에 대한 분석을 수행해 상담자에 의해 기 설정된 키워드별 빈도수를 획득할 수 있다. 이는, 텍스트데이터와 기 설정된 키워드를 매핑하는 분석을 통해 이루어질 수 있다. 실시예에 따라, 기 설정된 키워드는 상담자에 의해 실시간 변경 설정되도록 구현될 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 도 8과 같이, 키워드별 빈도수를 나타내는 그래프를 생성 및 전송할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 키워드별 빈도수를 내림차순 또는 오름차순으로 정렬하여 제공할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 키워드별 빈도수를 원의 크기로 표현하여 제공할 수도 있다. 예를 들어, 많은 빈도수로 발화된 키워드는 상대적으로 큰 크기의 원으로 표현하여 그래프를 생성할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 상담자 단말로부터 설정 키워드를 전송받거나, 감정 분석 결과 제공 장치가 별도의 사용자 인터페이스부(미도시)를 통한 사용자 입력을 통해 설정 키워드를 직접 입력받을 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 제2 음성데이터가 변환된 텍스트데이터에 대해 잠재 디리클레 할당 (LDA, Latent Dirichlet Allocation) 토픽 모델링을 수행하여 상기 토픽별로 구성된 키워드들 각각에 대한 빈도수를 획득할 수 있다. 구체적으로, 제2 분석결과획득부(21b)는 제2 음성데이터를 구성하는 각각의 음성프레임이 변환된 텍스트데이터 (STT를 통한) 전체에 대한 잠재 디리클레 할당(LDA, Latent Dirichlet Allocation) 토픽 모델링을 수행하여 상 기 토픽별로 구성된 키워드들 각각에 대한 빈도수를 획득할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 텍스트데이터 전체를 구성하는 복수의 문서들 각각을 전처리하고, 전처리된 복수의 문서들로부터 적어도 하나의 토픽을 추출하며, 토픽별로 토픽을 구성하는 키워드들 각각에 대 한 단어 빈도수를 산출할 수 있다. 실시예에 따라, 전처리 과정은 특수 문자 제거(또는 숫자 제거), 형태소 분석, 불용어 제거, 유의어(또는 유사 어) 처리 중 적어도 하나 이상을 포함할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 전처리된 복수의 문서들을 이용하여 적어도 하나의 토픽을 추출하고, 전처리된 복수의 문서들 각각을 추출된 적어도 하나의 토픽 각각과 매칭할 수 있다. 구체적으로, 상 기 전처리된 복수의 문서들로부터 키워드를 추출하고, 추출된 키워드의 빈도 분석을 수행한다. 또한, 제2 분석 결과획득부(21b)는 토픽 모델링을 위해 단어-문서 행렬(Term-Document Matrix)을 생성할 수 있다. 제2 분석결과 획득부(21b)는 LDA(Latent Dirichlet Allocation) 기법을 통해 문서를 이루고 있는 키워드를 통해 적어도 하나 의 토픽(주제)을 추출하고, 각 문서를 토픽 별로 분류할 수 있다. 이때, 하나의 문서는 복수 개의 토픽에 포함 될 수도 있다. 즉, 문서와 토픽은 1:N(N은 1 이상의 자연수)의 대응 관계를 갖을 수 있다. LDA 모형(또는 LDA 모델)은 사전에 계산된 주제별 단어의 분포를 바탕으로 주어진 문서의 단어를 분석함으로써 해당 문서가 어떤 주제를 다루고 있는지 예측하는 모형이다. 토픽을 나누는 기준은 클러스터 분석 기법의 하나로 실루엣 분석을 통해 K의 군집 수를 결정하고 K 개의 주제에 따라 상위 키워드를 분석한다. 실시예에 따라, 제2 분석결과획득부(21b)는 토픽별로 토픽을 구성하는 키워드들 각각에 대한 단어 빈도수를 도 9a 및 도 9b(도 9a의 그래프 확대)와 같이 복수의 그래프로 그래프화할 수 있다. 실시예에 따라, 임의의 그래프 상에는, 토픽수에 대응되는 원들을 표기하고, 특히, 원의 크기가 토픽에 대응되 는 키워드의 빈도수에 비례하도록 그래프화할 수 있다. 또한, 원 사이의 거리는 토픽 사이의 유사도에 기초해 계산되어 그래프화될 수 있다. 그리고, 다른 그래프 상에는, 각 원의 토픽에 대응되는 키워드와 해당 키워드별 빈도수 정보를 나타내도록 작성될 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 제2 음성데이터를 기초로 분석하여 음성 파라미터를 획득할 수 있다. 본 발명에 따르면, 제2 음성데이터에 대한 음성프레임들을 이용해 각 음성프레임들에 대한 각 음성 파라미터를 획득하기 위하여 공지의 다양한 기술을 적용할 수 있고, 이에 대한 설명은 생략한다. 실시예에 따라 음성 파라미터는 파형, 피치(pitch), 포즈(pause) 구간, 발화 속도, 및/또는 망설임 구간을 나타 내는 지표를 포함할 수 있다. 피치는, 음성의 주파수와 주기를 기초로 결정될 수 있다. 포즈 구간은 파형이 발생하지 않거나 파형이 끊기는 구간으로 정의될 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 포즈 구간이 소정 시간(예> 2초) 이상인 경우, 이를 내담자가 망설 이는 구간으로 결정 및 이를 이용해 망설임 구간을 나타내는 지표(예>망설임 횟수, 망설임 평균 시간)를 생성할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 도 10a 및 도 10b와 같이 시간대별 음성 파라미터를 그래프화하여 생성할 수 있다. 예를 들어, 도 10a은 시간대별 피치 변화를 나타내며, 10b는 시간대별 포즈 변화를 나타낸다. 실시예에 따라, 제2 분석결과획득부(21b)는 제2 영상데이터를 기초로 분석하여 시선 위치 정보 및 얼굴 움직임 정보를 획득할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 제2 영상데이터를 구성하는 각각의 영상프레임으로부터 내담자의 얼 굴을 인식하고, 상기 인식된 얼굴의 특징점들 검출하며, 상기 얼굴의 특징점들을 기초로 관심 영역을 지정하고, 지정된 관심 영역에서 동공 중심 위치를 검출하고, 상기 검출된 동공 중심 위치의 시간에 따른 변화 정도를 유클리디안 거리의 변화에 기초해 산출함으로써 시선 위치 정보를 획득할 수 있다. 영상프레임으로부터 내담자의 얼굴을 인식하고, 인식된 얼굴의 특징점들을 검출하는 것은 공지의 알고리즘을 적 용해 수행할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 얼굴 특징점들 중 특히, 내담자의 좌안과 우안에 각각 대응되는 특 징점들로 구성되는 각 영역을 관심 영역으로 지정하고, 좌안 영역과 우안 영역 각 영역에서 동공 중심 위치를 검출할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 미리 생성된 특징점 탬플릿(관심 영역 포함)과 인식된 얼굴의 특징 점들을 비교해 관심 영역을 지정할 수 있다. 예를 들어, 도 12를 참조하면, 제2 분석결과획득부(21b)는 좌안 영역(및 우안 영역 각각)의 가로축을 기준으로 한 제1 양끝점들과 세로축을 기준으로 한 제2 양끝점들 각각의 좌표를 기준으로 상대적 좌표(제1 양끝점들 사이 의 중심 좌표, 제2 양끝점들 사이의 중심 좌표)를 산출하여, 해당 상대적 좌표를 동공 중심 위치로 검출할 수 있다. 그리고, 시간 경과에 따른 내담자의 시선 변화에 따라 산출된 해당 상대적 좌표의 기준 좌표와의 유클리디안 거 리를 산출하고, 좌안 영역 및 우안 영역 각각으로부터 산출된 유클리디안 거리의 평균값을 이용해 시선 위치 정 보를 획득할 수 있다. 실시예에 따라, 기준 좌표는, 소정의 시간(예> 상담이 시작된 후 약 1분) 동안 실시간 측정된 유클리디안 거리 의 평균값으로 산출될 수 있다. 실시예에 따라, 도 11과 같이, 제2 분석결과획득부(21b)은 시간변화에 따른 상기 유클리디안 거리의 평균값(기 준시간당 변위, 초당 움직임)을 그래프화하거나, 시간변화에 따른 유클리디안 거리의 평균값의 누적값(기준시간 당 누적변위, 움직임 누적)를 그래프화하여 작성 및 전송할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 제2 영상데이터를 구성하는 각각의 영상프레임으로부터 내담자의 얼 굴을 인식하고, 상기 인식된 얼굴의 특징점을 검출하고, 상기 검출된 특징점 위치의 시간에 따른 변화 정도를 유클리디안 거리의 변화에 기초해 산출함으로써 얼굴 움직임 정보를 획득할 수 있다. 영상프레임으로부터 내담자의 얼굴을 인식하고, 인식된 얼굴의 특징점을 검출하는 것은 공지의 알고리즘을 적용 해 수행할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 얼굴의 코끝점을 특히 얼굴의 움직임 위치를 검출하는 데 이용할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 미리 생성된 특징점 탬플릿(코끝이 포함된 얼굴 특징점)과 인식된 얼굴의 특징점을 비교해 코끝점을 대상 특징점으로 검출할 수 있다. 실시예에 따라, 코끝점의 좌표값은 영상프레임 전체를 기준으로 한 절대 좌표값을 이용할 수 있다. 실시예에 따라, 영상프레임을 구성하는 픽셀 단위를 참조해 절대 좌표값을 산출할 수 있다. 그리고, 시간 경과에 따른 내담자의 얼굴 움직임 변화에 따라 산출된 현 시점의 코끝점 좌표와 이전 시점의 코 끝점 좌표와의 유클리디안 거리를 산출하여 얼굴 움직임 위치 정보를 획득할 수 있다. 실시예에 따라, 도 11과 같이, 제2 분석결과획득부(21b)은 시간변화에 따른 상기 유클리디안 거리의 평균값(기 준시간당 변위, 초당 움직임)을 그래프화하거나, 시간변화에 따른 유클리디안 거리의 평균값의 누적값(기준시간 당 누적변위, 움직임 누적)를 그래프화하여 작성 및 전송할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 도 11과 같이 시간대별 시선 위치 정보와 얼굴 움직임 정보를 각각 그래프화하여 작성 및 전송할 수 있다. 전송부(21c)는 복수의 제2 분석 결과 전체를 포함한 정보를 상담자 단말로 전송할 수 있다(s24). 실시예에 따라 복수의 제2 분석 결과 전체를 포함한 정보는, 복수의 제2 분석 결과 전체, 내담자 단말로부 터 실시간 수신한 동영상데이터, 및 상기 실시간 수신한 동영상데이터로부터 분리된 내담자의 음성데이터가 변 환된 텍스트데이터와 상담자의 음성데이터가 변환된 텍스트데이터의 컨텐츠를 포함할 수 있다. 도 5 내지 도 11은 상담자 단말의 제2 분석결과제공부의 동작에 따른 사용자 인터페이스부의 화면 을 예시한다. 상담자 단말의 제2 분석결과제공부는 감정 분석 결과 제공 장치로부터 수신한 복수의 제2 분석 결 과 전체 중 사용자에 의해 선택된 제2 분석 결과를 포함한 정보와 상담자가 실시간 기록한 상담 노트를 하나의 사용자 인터페이스부의 화면을 통해 출력함으로써 관리자 또는 상담자등의 사용자에게 제공할 수 있다. 실시예에 따라, 복수의 제2 분석 결과 전체 중 사용자에 의해 선택된 제2 분석 결과를 포함한 정보는 복수의 제 2 분석 결과 전체 중 사용자에 의해 선택된 제2 분석 결과, 선택된 제2 분석 결과와 관련된 보조 분석 결과, 내 담자가 촬영된 동영상데이터, 및 내담자가 촬영된 동영상데이터로부터 분리된 내담자의 음성데이터가 변환된 텍 스트데이터와 상담자의 음성데이터가 변환된 텍스트데이터에 대응되는 컨텐츠를 포함할 수 있다. 실시예에 따라, 제2 분석결과제공부는 사용자 인터페이스부의 화면을 통해 복수의 제2 분석 결과 중 선 택된 제2 분석 결과를 출력하기 위한 사용자 입력을 수신할 수 있다. 실시예에 따라, 사용자 인터페이스부의 화면은 복수의 제2 분석 결과를 나타내는 복수의 메뉴(M)를 포함하 고, 제2 분석결과제공부는 사용자에 의해 선택 입력된 메뉴에 대응되는 제2 분석 결과(①)를 사용자 인터페 이스부의 화면을 통해 출력하여 사용자에게 제공할 수 있다. 예를 들어, 도 5는 복수의 일반 감정 상태를 제2 분석 결과로 출력하기 위한 메뉴에 대한 사용자의 선택 입력으 로 출력된 화면을 예시하고, 도 6은 복수의 치환 감정 상태를 제2 분석 결과로 출력하기 위한 메뉴에 대한 사용 자의 선택 입력으로 출력된 화면을 예시하고, 도 7은 심박수와 스트레스 측정값을 제 제2 분석 결과로 출력하기 위한 메뉴에 대한 사용자의 선택 입력으로 출력된 화면을 예시하고, 도 8은 키워드별 빈도수를 제2 분석 결과로 출력하기 위한 메뉴에 대한 사용자의 선택 입력으로 출력된 화면을 예시하고, 도 9a 및 도 9b는 토픽별로 구성 된 키워드들 각각에 대한 빈도수를 제2 분석 결과로 출력하기 위한 메뉴에 대한 사용자의 선택 입력으로 출력된 화면을 예시하고, 도 10a 및 도 10b는 음성 파라미터를 제2 분석 결과로 출력하기 위한 메뉴에 대한 사용자의 선택 입력으로 출력된 화면을 예시하고, 도 11은 시선 위치 정보 및/ 얼굴 움직임 정보를 제2 분석 결과로 출력 하기 위한 메뉴에 대한 사용자의 선택 입력으로 출력된 화면을 예시한다. 실시예에 따라, 각 컨텐츠는 사용자 인터페이스부의 화면 상의 분리된 영역에 각각 출력될 수 있다. 예를 들어, 도 5 내지 도 11의 사용자 인터페이스부의 화면에 대한 예시로, 제2 분석 결과(①), 제2 분석 결과와 관련된 보조 분석 결과(②), 내담자 가 촬영된 동영상데이터(⑤), 내담자의 음성데이터가 변환된 텍스트 데이터와 상담자의 음성데이터가 변환된 텍스트데이터(④), 상담자가 실시간 기록한 상담 노트(③)를 포함할 수 있다. 도 5 내지 도 7, 및 도 10a, 도 10b, 도 11의 사용자 인터페이스부의 화면에서, 실시예에 따라, 시간대별 제2 분석 결과가 그래프 형태로 구현되어 제공될 수 있다. 구체적으로, 상기 하나의 사용자 인터페이스 화면은 복수 개의 영역으로 구분되고, 상기 제2 분석결과제공 부는, 상기 복수 개의 영역 중 어느 하나의 영역에 상기 사용자에 의해 선택된 상기 제2 분석 결과가 시간 대별로 나타난 그래프를 출력할 수 있다. 도 5 내지 도 11의 사용자 인터페이스부의 화면에서, 실시예에 따라, 내담자의 음성데이터가 변환된 텍스트 데이터와 상담자의 음성데이터가 변환된 텍스트데이터(④)는 사용자에 의해 편집되거나 파일 형태로 제공되어 사용자에게 제공될 수 있다. 도 5 내지 도 11의 사용자 인터페이스부의 화면에서, 실시예에 따라, 내담자 정보가 리스트화되어 사용자에 의해 선택된 내담자 정보가 검색될 수 있다. 실시예에 따라, 도 5 내지 도 11의 각 사용자 인터페이스부의 화면 상의 각 영역에서, 제2 분석 결과(①)와 제1 분석 결과와 관련된 보조 분석 결과(②)에 대응되는 컨텐츠만이 상이하게 구현되고, 나머지 컨텐츠(③④ ⑤)는 동일하게 구현될 수 있다. 도 5를 참조하면, 시간대별 복수의 일반 감정 상태 각각의 확률값을 나타내는 그래프가 제2 분석 결과(①)를 나 타내는 영역에 제공될 수 있다. 실시예에 따라, 제2 분석 결과와 관련된 보조 분석 결과(②)는 복수의 일반 감 정 상태 각각에 대한 항목 정보를 포함할 수 있고, 실시예에 따라 사용자에 의해 선택된 일반 감정 상태 항목에 대한 확률값만이 그래프 상에 출력되도록 설정될 수도 있다. 실시예에 따라, 그래프 상의 동일 시점에 나타난 복수의 일반 감정 상태의 확률값 중, 확률값이 가장 큰 수치를 나타내는 일반 감정 상태가 해당 시점의 대표 감정 상태인 것으로 판단될 수 있다. 본 발명에 따르면, 도 5의 제2 분석 결과를 통해, 사용자는 전체 상담 구간 내에서 시간대별(시간의 흐름에 따 른) 내담자의 감정 변화를 전체적으로 확인할 수 있게 된다. 이 때, 사용자는 복수의 일반 감정 상태 각각의 확 률값을 통합적으로, 또는, 이 중 선택된 일반 감정 상태의 확률값을 개별적으로 확인할 수 있게 된다. 이에 따 라, 사용자는 내담자의 감정 변화의 맥락을 전체적으로 이해함으로써, 추후 상담의 방향을 모색할 수 있게 된다. 도 6을 참조하면, 시간대별 복수의 치환 감정 상태 각각의 확률값을 나타내는 그래프가 제2 분석 결과(①)를 나 타내는 영역에 제공될 수 있다. 실시예에 따라, 제2 분석 결과와 관련된 보조 분석 결과(②)는 긍정, 부정, 중 립, 기타 각각에 대응되는 빈도수 및/또는 확률 정보와, 이를 도표화한 정보를 포함할 수 있다. 실시예에 따라 사용자에 의해 선택된 치환 감정 상태에 대응되는 정보만이 출력되도록 설정될 수도 있다. 본 발명에 따르면, 도 6의 제2 분석 결과를 통해, 사용자는 전체 상담 구간 내에서 시간대별(시간의 흐름에 따 른) 내담자의 감정 변화를 전체적으로 확인할 수 있게 된다. 이 때, 사용자는 복수의 치환 감정 상태 각각의 확 률값을 통합적으로, 또는, 이 중 선택된 치환 감정 상태의 확률값을 개별적으로 확인할 수 있게 된다. 이에 따 라, 사용자는 내담자의 감정 변화의 맥락을 전체적으로 이해함으로써, 추후 상담의 방향을 모색할 수 있게 된다. 특히, 본 발명에 따르면, 도 5의 제2 분석 결과를 통해, 사용자는 보다 상세하게 분류된(카테고리화된) 감정 상 태 정보를 확인할 수 있는 반면, 도 6의 제2 분석 결과를 통해, 사용자는 보다 큰 범위로 분류된(카테고리화된) 감정 상태 정보를 확인할 수 있어, 사용자의 필요 및 선택에 따라 다양하게 감정 상태 정보를 확인할 수 있게 된다. 도 7을 참조하면, 시간대별 심박수와 스트레스 지수를 함께 나타내는 그래프가 제2 분석 결과(①)를 나타내는 영역에 제공될 수 있다. 실시예에 따라, 제2 분석 결과와 관련된 보조 분석 결과는 평균심박수, 평균스트레스레벨 등의 정보를 포함할 수 있다. 실시예에 따라 심박수와 스트레스 레벨 중 사용자에 의해 선택된 정보만이 출력되도록 설정될 수도 있 다. 본 발명에 따르면, 도 7의 제2 분석 결과를 통해, 사용자는 전체 상담 구간 내에서 시간대별(시간의 흐름에 따 른) 내담자의 스트레스 정도를 전체적으로 확인할 수 있게 된다. 이에 따라, 사용자는 내담자의 스트레스 정도 나 변화를 포함한 맥락을 전체적으로 이해함으로써, 추후 상담의 방향을 모색할 수 있게 된다. 도 8을 참조하면, 키워드별 빈도수를 나타내는 그래프가 제2 분석 결과(①)를 나타내는 영역에 제공될 수 있다. 실시예에 따라, 제2 분석 결과와 관련된 보조 분석 결과는 상담자에 의해 기 설정된 키워드 리스트 정보를 포함 할 수 있다. 본 발명에 따르면, 상담을 진행하는 중에 내담자가 발화하는 단어들 중에서, 심리 상담 측면 및 상담사가 중요 하게 생각하는 주요 키워드들이 얼마나 나타났는지를 확인할 수 있게 된다. 예를 들어, 자살, 우울 과 같은 주 요 단어들은 내담자의 심리 및 처한 상황을 극명하게 드러내는 키워드이므로 이러한 키워드들이 얼마나 많이 나 타나는 가를 확인하는 것은 매우 중요하다. 또한, 관리자에 의해 미리 등록된 설정 단어 뿐 아니라, 해당 상담 을 진행하는 상담사에 의해서도 실시간 단어를 등록하여 확인할 수 있으며, 분석에서 제외하고자 하는 단어는 제외할 수 있도록 설정하여, 설정한 단어 뿐 아니라, 내담자가 발화한 모든 단어를 대상으로 분석하고자 할 때 는 전체 언어를 대상으로 분석할 수 있다. 도 9a를 참조하면, 토픽별로 구성된 키워드들 각각에 대한 빈도수가 복수의 형태로 그래프화되어 제2 분석 결과 (①)를 나타내는 영역에 제공될 수 있다. 실시예에 따라, 제2 분석 결과와 관련된 보조 분석 결과는 토픽별로 구성된 키워드들 각각에 대한 빈도수와 순 위를 나타내는 리스트 정보를 포함할 수 있다. 본 발명에 따르면, 도 9a의 제2 분석 결과를 통해, 상담자가 상담을 하는 내용에 대하여, 내담자가 주로 관심을 가지고 있는 주제 분야를 확인할 수 있으며, 다른 주제들과의 관계성을 함께 살펴 볼 수 있게 된다. 이에 따라, 상담자는 내담자의 주된 관심 분야를 전체적으로 이해함으로써, 추후 상담의 방향을 모색할 수 있게 된다. 도 10a 및 도 10b를 참조하면, 시간대별 음성 파라미터가 그래프화되어 제2 분석 결과(①)를 나타내는 영역에 제공될 수 있다. 실시예에 따라, 제2 분석 결과와 관련된 보조 분석 결과는 피치평균값, 포즈평균값, 소정시간당 평균 발화수, 망설임 횟수, 망설임 평균수치 등의 정보를 포함할 수 있다. 실시예에 따라 이 중 사용자에 의해 선택된 정보만 이 출력되도록 설정될 수도 있다. 본 발명에 따르면, 상담자는 내담자가 언제 언성을 높이는지, 언제 조용히 말하는지, 언제 느리게 말하고, 빠르 게 말하는지 등을 그래프를 통해 확인할 수 있고, 이로서, 상담자는 내담자의 감정의 변화, 말의 내용 변화에 따른 음성 파형의 변화를 전체적으로 확인함으로써 시간대별 내담자의 미세한 감정 변화를 판단할 수 있다. 도 11을 참조하면, 시간대별 시선 위치 정보와 얼굴 움직임 정보가 각각 그래프화되어 제2 분석 결과(①)를 나 타내는 영역에 제공될 수 있다. 실시예에 따라, 제2 분석 결과와 관련된 보조 분석 결과는 평균 시선 위치 변화값, 누적된 평균 시선 위치 변화 값, 평균 얼굴 움직임 변화값, 누적된 평균 얼굴 움직임 변화값 등을 포함할 수 있다. 실시예에 따라 이 중 사 용자에 의해 선택된 정보만이 출력되도록 설정될 수도 있다. 본 발명에 따르면, 도 11의 분석 결과를 통해, 시선이나 얼굴을 움직이는 것은 내담자의 감정 상태가 몸으로 드 러나는 증거가 될 수 있으므로, 해당 결과를 통해 상담사는 내담자의 무의식적인 행동을 확인하고, 이를 통해 내담자의 감정 상태를 정확히 판단할 수 있으며, 그래프 상의 변위가 크게 발생한 시점을 찾아서 해당 시간대의 상담 내용을 확인하면서 내담자의 감정을 더욱 정확하게 파악할 수 있게 된다. 본 발명에 따르면, 제1 분석결과획득부(21a)를 통해 실시간 분석으로 제1 분석 결과를 획득하지만, 제2 분석결 과획득부(21b)를 통해 소정 시간 경과 후 분석으로 제2 분석 결과를 획득하도록 함으로써, 동일한 내담자와의 실시간 상담 내역을 실시간 분석을 통해 실시간 제공받을 수 있도록 하면서도, 상담이 종료된 후 전체 상담 내 역을 통합 분석을 통해 제공받을 수 있도록 함으로써, 하나의 상담 내역에 대해 다양한 형태의 분석 결과를 제 공받을 수 있게 되어 보다 다양화되고 체계화된 상담 내역의 관리가 가능해질 수 있게 된다. 또한, 상담 진행 중일 때의 사용자 인터페이스 화면과 상담이 종료된 후의 분석 결과를 제공하는 사용자 인터페 이스 화면을 서로 상이하게 구현하여 상담자의 상담 내역 관리 및 확인이 보다 용이해지게 된다. 예를 들어, 상 담이 진행중인 경우, 실시간 분석 결과를 제공받도록 하면서도 전체 분석 결과를 하나의 사용자 인터페이스부 의 화면을 통해 제공받도록 함으로써, 상담자가 실시간 상담 내역 확인, 분석, 및/또는 판단이 보다 신속하 고 용이해질 수 있으며, 상담이 종료된 경우, 누적된 전체 상담 내역에 대해 하나의(선택된) 분석 결과를 하나 의 사용자 인터페이스부의 화면을 통해 제공받도록 함으로써, 전체 상담 내역을 사용자가 통합적으로 확인 할 수 있게 된다. 도 13은 실시예에 따른 앙상블 모델을 생성하기 위한 학습 과정을 설명하기 위해 참조되는 도면이다. 실시예에 따라, 제2 분석결과획득부(21b)는 복수의 모델을 각각 미리 생성하여 저장부에 저장할 수 있다. 실시예에 따라 복수의 모델은 학습용 영상프레임, 학습용 음성프레임, 및 학습용 텍스트데이터 각각을 학습에 적용한 앙상블 기반의 모델들로 생성될 수 있다. 실시예에 따라, 복수의 모델은 기계학습과 딥러닝이 앙상블되어 생성될 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 학습용 영상프레임을 기초로 학습된 제1 모델을 획득하여 메모리 에 저장할 수 있다. 실시예에 따라, 제1 모델은 CNN(Convolutional Neural Network) 알고리즘 기반의 MobileNet 모델일 수 있으나, DNN(Deep Neural Network), DCNN(DeepConvolution Neural Network), RNN (Recurrent Neural Network), KNN(K- Nearest Neighbor), SVM(Support Vector Machine), Random Forest, Decision Tree 등의 알고리즘에 기반한 학 습으로 생성될 수도 있다. MobileNet 아키텍처는 처리 동작들(즉, 부동 소수점 연산들, 곱셈들 및/또는 덧셈들 등)을 최소화하기 위해 뎁스와이즈 분리가능 컨볼루션들(인수분해된 컨볼루션들의 형태)을 채택한다. 뎁스와이 즈 분리가능 컨볼루션들은, 요구되는 동작들의 수를 감소 또는 최소화함으로써 처리를 더 고속화하기 위한 관점 에서, 표준 컨볼루션을 뎁스와이즈 컨볼루션 및 1 x 1 컨볼루션(또한 \"포인트와이즈 컨볼루션\"으로 지칭됨)으로 인수분해한다(예를 들어, 그것의 함수들을 토해 낸다). 뎁스와이즈 컨볼루션은 각각의 입력 채널에 단일 필터를 적용한다. 이어서, 포인트와이즈 컨볼루션은 뎁스와이즈 컨볼루션의 출력들을 조합하기 위해 1 x 1 컨볼루션을 적용하고, 필터링 및 조합 기능들/동작들을, 표준 컨볼루션들에 의해 수행되는 단일 필터링 및 조합 동작이 아니라 2개의 단계로 분리한다. 따라서 MobileNet 아키텍처에서의 구조들은, \"계층 그룹\"을 정의하거나 예시하기 위해 구조 당 2개의 컨볼루션 계층, 1개의 뎁스와이즈 컨볼루션 계층 및 1개의 포인트와이즈 컨볼루션 계층을 포함할 수 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 학습용 영상프레임을 CNN을 통한 신경망 학습을 통해 학습용 영상프 레임에 대한 감정이 분류되도록 함으로써, 제1 모델을 생성할 수 있다. 제2 분석결과획득부(21b)는 학습용 음성프레임을 기초로 학습된 제2 모델을 획득하여 저장부에 저장할 수 있다. 실시예에 따라, 제2 모델은 SVM(Support Vector Machine) 알고리즘 기반의 모델일 수 있으나, CNN, DNN(Deep Neural Network), DCNN(DeepConvolution Neural Network), RNN (Recurrent Neural Network), KNN(K-Nearest Neighbor), Random Forest, Decision Tree 등의 알고리즘에 기반한 학습으로 생성될 수도 있다.SVM(Support Vector Machine) 알고리즘은 분류와 회귀 분석을 위해 사용되며, 두 카테고리 중 어느 하나에 속한 데이터의 집 합이 주어졌을 때, SVM 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어느 카테고리에 속할 지 판단하는 비확률적 이진 선형 분류 모델을 만든다. 실시예에 따라, 제2 분석결과획득부(21b)는 학습용 음성프레임의 주파수 분석을 기반으로 음성 특징 벡터를 추 출하고, 추출된 음성 특징 벡터를 SVM(Support Vector Machine) 알고리즘을 통해 학습하며, 학습 과정에서 음성 특징 벡터들이 감정별로 분류되도록 함으로써, 제2 모델을 생성할 수 있다. 제2 분석결과획득부(21b)는 학습용 텍스트데이터를 기초로 학습된 제3 모델(22c)를 획득하여 저장부에 저장 할 수 있다. 실시예에 따라, 제3 모델은 BERT(Bidirectional Encoder Representations from Transformers) 모델일 수 있으 나, DNN(Deep Neural Network), DCNN(DeepConvolution Neural Network), CNN, RNN (Recurrent Neural Network), KNN(K-Nearest Neighbor), SVM(Support Vector Machine), Random Forest, Decision Tree 등의 알 고리즘에 기반한 학습으로 생성될 수도 있다.BERT는 인코더-디코더 구조의 트랜스포머(Transformer) 아키텍쳐를 기반으로 한 인공지능 모델로서, 입력의 심층 표현(Representation)을 위해 복수의 트랜스포머 계층을 쌓고, 토 큰 시퀀스인 마스킹 언어 모델(Masking Language Model)에 마스킹 과정을 적용하는 것을 특징으로 한다. BERT 모델은 파인 튜닝 과정을 거침으로써 적은 양의 데이터에서도 높은 정확도를 나타내며, 특정 벡터에 주목하게 만들어 성능을 향상시키는 어텐션 기반 모델로 문장이 길어져도 성능이 떨어지지 않아 긴 문장에서도 정확도를 유지할 수 있다는 장점이 있다. 실시예에 따라, 제2 분석결과획득부(21b)는 학습용 텍스트데이터를 기초로 컨텍스트 기반 임베딩값을 획득하기 위한 신경망 학습을 통해 학습용 텍스트데이터에 대한 감정이 분류되도록 함으로써, 제3 모델을 생성할 수 있다. 이상 설명된 실시 형태는 다양한 컴퓨터 구성요소를 통하여 실행될 수 있는 프로그램 명령어의 형태로 구현되어 컴퓨터로 판독가능한 기록매체에 기록될 수 있다. 상기 컴퓨터로 판독가능한 기록매체는 프로그램 명령어, 데이 터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터로 판독가능한 기록매체에 기록되는 프로그램 명령어는 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터로 판독가능한 기록매체의 예에는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD- ROM, DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 ROM, RAM, 플래시 메모리 등과 같은 프로그램 명령어를 저장하고 실행하도록 특별히 구성된 하드웨 어 장치가 포함된다. 프로그램 명령어의 예에는, 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니 라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함된다. 상기 하드웨어 장치 는 본 발명에 따른 처리를 실행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 본 명세서의 양상들은 전체적으로 하드웨어, 전체적으로 소프트웨어 (펌웨어, 상주 소프트웨어, 마이크로 코드 등을 포함 함) 또는 컴퓨터 판독 가능 프로그램 코드가 구현 된 하나 이상의 컴퓨터 판독 가능 매체에 구현 된 컴퓨터 프로그램 제품의 형태를 취할 수 있다. 이상에서 실시예들에 설명된 특징, 구조, 효과 등은 본 발명의 하나의 실시예에 포함되며, 반드시 하나의 실시 예에만 한정되는 것은 아니다. 나아가, 각 실시예에서 예시된 특징, 구조, 효과 등은 실시예들이 속하는 분야의 통상의 지식을 가지는 자에 의해 다른 실시예들에 대해서도 조합 또는 변형되어 실시 가능하다. 따라서 이러한 조합과 변형에 관계된 내용들은 본 발명의 범위에 포함되는 것으로 해석되어야 할 것이다. 또한, 이상에서 실시예를 중심으로 설명하였으나 이는 단지 예시일 뿐 본 발명을 한정하는 것이 아니며, 본 발 명이 속하는 분야의 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성을 벗어나지 않는 범위에서 이상에 예시되지 않은 여러 가지의 변형과 응용이 가능함을 알 수 있을 것이다. 예를 들어, 실시예에 구체적으로 나타 난 각 구성 요소는 변형하여 실시할 수 있는 것이다. 그리고 이러한 변형과 응용에 관계된 차이점들은 첨부된 청구 범위에서 규정하는 본 발명의 범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2023-0003827", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 실시예에 따른 감정 분석 결과 제공 시스템의 시스템도이다. 도 2 및 도 3은 실시예에 따른 감정 분석 결과 제공 장치의 동작을 설명하는 순서도이다. 도 4는 상담자 단말의 제1 분석결과제공부의 동작에 따른 사용자 인터페이스부의 화면을 예시한다. 도 5 내지 도 11은 상담자 단말의 제2 분석결과제공부의 동작에 따른 사용자 인터페이스부의 화면 을 예시한다. 도 12 내지 도 13은 감정 분석 결과 제공 장치의 동작을 설명하기 위해 참조되는 도면이다."}
