{"patent_id": "10-2020-0047034", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0128838", "출원번호": "10-2020-0047034", "발명의 명칭": "이미지 처리 장치 및 이미지 처리 방법", "출원인": "엘지이노텍 주식회사", "발명자": "김대훈"}}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "제1 이미지 및 제2 이미지를 수신하는 수신기;상기 제1 이미지 및 상기 제2 이미지 중 적어도 하나를 이용하여 제3 이미지를 출력하는 제1 프로세서;상기 제1 이미지 및 상기 제2 이미지 중 적어도 하나를 바이패스하는 신호 라인;상기 수신기로부터 출력된 신호를 상기 제1 프로세서 또는 상기 신호라인으로 연결하는 스위치; 및상기 제1 내지 제3 이미지 중 적어도 두 개의 이미지를 송신하는 송신기를 포함하는 이미지 처리 장치."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 제1 프로세서는 콘볼루션 신경망을 포함하는 이미지 처리 장치."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 스위치는,제어신호에 따라 상기 제1 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결하는 제1 스위치; 및상기 제어신호에 따라 상기 제2 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결하는 제2 스위치를 포함하는 이미지 처리 장치."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 스위치는 상기 제1 이미지 및 상기 제2 이미지 중 하나를 상기 제1 프로세서에 연결하고,다른 하나를 상기 신호라인으로 연결하는 이미지 처리 장치."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 스위치는 상기 제1 이미지 및 상기 제2 이미지를 상기 신호라인으로 연결하는 이미지 처리장치."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 신호라인은 제1 신호라인 및 제2 신호라인을 포함하고,상기 스위치에 의해 상기 제1 이미지는 상기 제1 신호라인에 연결되고,상기 스위치에 의해 상기 제2 이미지는 상기 제2 신호라인에 연결되는 이미지 처리 장치."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 제1 신호라인은 제1 동기화부를 포함하고,상기 제2 신호라인은 제2 동기화부를 포함하는 이미지 처리 장치.공개특허 10-2021-0128838-2-청구항 8 제1항에 있어서,상기 송신기는 상기 적어도 두 개의 이미지를 제2 프로세서로 송신하는 이미지 처리 장치."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 이미지 및 제2 이미지를 수신하는 단계;상기 제1 이미지 및 상기 제2 이미지를 제1 프로세서 또는 신호라인으로 연결하는 단계; 및상기 제1 프로세서에서 출력되는 제3 이미지, 상기 신호라인을 통해 바이패스된 상기 제1 이미지 또는 상기 제2이미지 중 적어도 두 개의 이미지를 송신하는 단계를 포함하고,상기 제1 프로세서는 상기 제1 이미지 및 제2 이미지 중 적어도 하나를 이용하여 제3 이미지를 출력하고,상기 신호라인은 상기 제1 이미지 및 상기 제2 이미지 중 적어도 하나를 바이패스하는 이미지 처리 방법."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 제1 프로세서는 콘볼루션 신경망을 포함하는 이미지 처리 방법."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 상기 제1 이미지 및 상기 제2 이미지를 제1 프로세서 또는 신호라인으로 연결하는 단계는,제어신호에 따라 제1 스위치를 이용하여 상기 제1 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결하고,상기 제어신호에 따라 제2 스위치를 이용하여 상기 제2 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결하는 이미지 처리 방법."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,상기 제1 이미지 및 상기 제2 이미지를 제1 프로세서 또는 신호라인으로 연결하는 단계는,상기 제1 이미지 및 상기 제2 이미지 중 하나를 상기 제1 프로세서에 연결하고 다른 하나를 상기 신호라인으로연결하거나, 상기 제1 이미지 및 상기 제2 이미지를 상기 신호라인으로 연결하는 이미지 처리 방법."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,상기 신호라인은 제1 신호라인 및 제2 신호라인을 포함하고,상기 제1 이미지 또는 상기 제2 이미지를 상기 신호라인으로 연결 시, 상기 제1 이미지는 상기 제1 신호라인에 연결하고, 상기 제2 이미지는 상기 제2 신호라인에 연결하는 이미지 처리 방법."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제9항에 있어서,상기 신호라인에 연결된 제1 이미지 또는 제2 이미지를 상기 제3 이미지와 동기화하는 단계를 포함하는 이미지처리 방법."}
{"patent_id": "10-2020-0047034", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서,공개특허 10-2021-0128838-3-상기 적어도 두 개의 이미지를 송신하는 단계는,상기 적어도 두 개의 이미지를 제2 프로세서로 송신하는 이미지 처리 방법."}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 이미지 처리 장치는 제1 이미지 및 제2 이미지를 수신하는 수신기, 제1 이미지 및 제2 이미지 중 적어도 하나를 이용하여 제3 이미지를 출력하는 제1 프로세서, 제1 이미지 및 제2 이미지 중 적어 도 하나를 바이패스하는 신호 라인, 수신기로부터 출력된 신호를 제1 프로세서 또는 신호라인으로 연결하는 스위 치, 및 제1 내지 제3 이미지 중 적어도 두 개의 이미지를 송신하는 송신기를 포함한다."}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 이미지 처리 장치에 관한 것으로, 보다 구체적으로 복수의 이미지를 수신하여 이미지 처리하거나 바 이패스하여 송신하는 이미지 처리 장치 및 이미지 처리 방법에 관한 발명이다."}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "스마트폰의 필수 모듈인 카메라는 점점 고해상도의 센서를 요구하고 다양한 기능들을 추가하고자 고가의 AP를 사용하고 그러한 AP에 딥러닝 기반의 기술들을 접목시키고 있다. 하지만, 아무리 고사양의 AP라고 할지라도 많 은 데이터를 처리하는 전처리 기반의 딥러닝 기술들을 적용하기엔 무리가 있어서 추가적인 칩을 사용하여 AP의 resource 부하를 덜어주기 위한 시도를 하고 있다. 딥러닝 기반의 영상 처리 기술이 발달함에 따라 스마트폰에서도 여러 가지 방법으로 딥러닝 네트워크를 도입하 고 있다. 딥러닝을 처리할 수 있는 고가의 AP를 앞다투어 개발하고 있고, 각 AP 개발 업체마다 뉴럴 네트워크에 맞는 제품들을 출시하고 있다. G사는 TPU(Tensor Processing Unit), I사는 EyeQ, Loihi 등 ASIC 형태의 프로세서들을 개발하고, A사는 자체적 으로 A11 Bionic, A12 Bionic 과 같이 인공지능 연산에 최적화된 회로를 구성하였고, 모바일 AP의 대표적인 Q사 도 Snapdragon 845, 855 등 NPU(Neural Processing Unit), NPE(Neural Processing Engine) 이 들어간 칩들을 개발하여 내놓고 있다. 하지만, 이러한 프로세서들을 특정 기능에 특화되었다기 보다는 일반적인 인공지능 서비스에 대응하기 위한 것으로써 카메라와 같이 특정 제품에 최적화되어 있지도 않고, 매우 고가의 AP이므로 스마트폰 중에서도 프리미엄 급 모델에만 적용 가능하다. 프리미엄 급 외의 모델에 적용하기 위해서는 저가의 AP를 사용해야 하고, 그에 따른 SW 처리도 단순화 되어야 하기 때문에 아무리 좋은 카메라를 사용해도 AP에서 이 고사양의 카메라 데이터를 받아서 다양한 처리를 하기는 어렵다. AP의 이러한 제약을 극복하기 위해서 AP에서 해야 하는 카메라 전처리 기능들을 카메라 모듈단에서 미리 처리하 여 전달해준다면 AP의 소비전력을 줄여주고, SW resource도 여유가 있을 수 있다. 더불어 최근 스마트폰 카메라 모듈의 채용 형태가 Dual/Tripple~Penta의 개념으로 그 기능적 확장을 위해 다수 개의 카메라 입력영상의 Fusion을 통해 그 활용성을 높여가는 추세이다. 근래 2 개 이상의 카메라가 스마트폰에 주류로 채용되면서 멀티 카메라의 활용을 위한 AP 단에서의 Interfacing Ready가 이슈가 되면서 중저가 AP에서의 Multi-Camera 채용 대응을 위해 별도의 MIPI Switch IC등을 이용한 방 법이 있다. 이는 중저가 폰에 탑재된 저가 AP가 다수개의 MIPI Channel을 가지지 못한 문제를 해결하기 위한 방 법이다. 그러나, 2개의 카메라 영상을 동시에 AP에 전달해 주지는 못하는 구조로 하나의 카메라만 선택해서 제 공하는 Mux구조의 스위칭 기능만 제공하여 최근의 멀티 카메라의 영상을 퓨전(Fusion)하여 카메라 품질을 향상 시키는 방법들을 적용하지 못하여 기능적 제약의 한계가 있다."}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 기술적 과제는, 복수의 이미지를 수신하여 이미지 처리하거나 바이패스하여 송신하 는 이미지 처리 장치 및 이미지 처리 방법을 제공하는 것이다."}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 기술적 과제를 해결하기 위하여, 본 발명의 일 실시예에 따른 이미지 처리 장치는 제1 이미지 및 제2 이미 지를 수신하는 수신기; 상기 제1 이미지 및 상기 제2 이미지 중 적어도 하나를 이용하여 제3 이미지를 출력하는제1 프로세서; 상기 제1 이미지 및 상기 제2 이미지 중 적어도 하나를 바이패스하는 신호 라인; 상기 수신기로 부터 출력된 신호를 상기 제1 프로세서 또는 상기 신호라인으로 연결하는 스위치; 및 상기 제1 내지 제3 이미지 중 적어도 두 개의 이미지를 송신하는 송신기를 포함한다. 또한, 상기 제1 프로세서는 콘볼루션 신경망을 포함할 수 있다. 또한, 상기 스위치는, 제어신호에 따라 상기 제1 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결하는 제1 스위치; 및 상기 제어신호에 따라 상기 제2 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결하는 제 2 스위치를 포함할 수 있다. 또한, 상기 스위치는 상기 제1 이미지 및 상기 제2 이미지 중 하나를 상기 제1 프로세서에 연결하고, 다른 하나 를 상기 신호라인으로 연결할 수 있다. 또한, 상기 스위치는 상기 제1 이미지 및 상기 제2 이미지를 상기 신호라인으로 연결할 수 있다. 또한, 상기 신호라인은 제1 신호라인 및 제2 신호라인을 포함하고, 상기 스위치에 의해 상기 제1 이미지는 상기 제1 신호라인에 연결되고, 상기 스위치에 의해 상기 제2 이미지는 상기 제2 신호라인에 연결될 수 있다. 또한, 상기 제1 신호라인은 제1 동기화부를 포함하고, 상기 제2 신호라인은 제2 동기화부를 포함할 수 있다. 또한, 상기 송신기는 상기 적어도 두 개의 이미지를 제2 프로세서로 송신 할 수 있다. 상기 기술적 과제를 해결하기 위하여, 본 발명의 일 실시예에 따른 이미지 처리 방법은 제1 이미지 및 제2 이미 지를 수신하는 단계; 상기 제1 이미지 및 상기 제2 이미지를 제1 프로세서 또는 신호라인으로 연결하는 단계; 및 상기 제1 프로세서에서 출력되는 제3 이미지, 상기 신호라인을 통해 바이패스된 상기 제1 이미지 또는 상기 제2 이미지 중 적어도 두 개의 이미지를 송신하는 단계를 포함하고, 상기 제1 프로세서는 상기 제1 이미지 및 제2 이미지 중 적어도 하나를 이용하여 제3 이미지를 출력하고, 상기 신호라인은 상기 제1 이미지 및 상기 제2 이미지 중 적어도 하나를 바이패스한다. 또한, 상기 제1 프로세서는 콘볼루션 신경망을 포함할 수 있다. 또한, 상기 제1 이미지 및 상기 제2 이미지를 제1 프로세서 또는 신호라인으로 연결하는 단계는, 제어신호에 따 라 제1 스위치를 이용하여 상기 제1 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결하고, 상기 제어신 호에 따라 제2 스위치를 이용하여 상기 제2 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결할 수 있다. 또한, 상기 제1 이미지 및 상기 제2 이미지를 제1 프로세서 또는 신호라인으로 연결하는 단계는, 상기 제1 이미 지 및 상기 제2 이미지 중 하나를 상기 제1 프로세서에 연결하고 다른 하나를 상기 신호라인으로 연결하거나, 상기 제1 이미지 및 상기 제2 이미지를 상기 신호라인으로 연결할 수 있다. 또한, 상기 신호라인은 제1 신호라인 및 제2 신호라인을 포함하고, 상기 제1 이미지 또는 상기 제2 이미지를 상 기 신호라인으로 연결 시, 상기 제1 이미지는 상기 제1 신호라인에 연결하고, 상기 제2 이미지는 상기 제2 신호 라인에 연결할 수 있다. 또한, 상기 신호라인에 연결된 제1 이미지 또는 제2 이미지를 상기 제3 이미지와 동기화하는 단계를 포함할 수 있다. 또한, 상기 적어도 두 개의 이미지를 송신하는 단계는, 상기 적어도 두 개의 이미지를 제2 프로세서로 송신할 수 있다."}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들에 따르면, 사용자가 영상촬영관련 다양한 시나리오 모드에서 다수 개의 카메라를 통해 입력 되는 멀티 프레임 이미지(Multi-Frame Image)기술을 효과적으로 이용할 수 있도록 호환성을 제공하고 해당 시나 리오에 맞춰 저조도(De-Noise)기술과 슈퍼 레졸루션에 따른 화질개선 기능적용이 가능하다. 고해상도의 이미지를 생성함에 있어서, RGB 이미지가 아닌 로우(Raw) 데이터인 베이어 데이터의 해상도를 높여 디지털 줌을 수행하는 바, RGB 이미지에 대해 해상도를 높이는 경우보다, 정보량이 많아 화질이 높은 고해상도 이미지를 획득할 수 있다.또한, ToF IR 이미지의 해상도를 높여 RGB 이미지와 병합하여 RGB 저조도를 개선하는 효과를 높일 수 있다. 추 가 구성을 부가할 필요가 없고, 연산량을 크게 증가시키지 않으면서도, 저조도 환경에서 화질이 우수한 RGB 이 미지를 얻을 수 있다. 나아가, RGB 이미지의 해상도를 높임과 동시에 화질이 개선된 RGB 이미지를 생성할 수 있다."}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명의 바람직한 실시예를 상세히 설명한다. 다만, 본 발명의 기술 사상은 설명되는 일부 실시 예에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있고, 본 발명의 기술 사상 범위 내에서라면, 실시 예들간 그 구성 요소들 중 하나 이상을 선택적으로 결합 또는 치환하여 사용할 수 있다. 또한, 본 발명의 실시예에서 사용되는 용어(기술 및 과학적 용어를 포함)는, 명백하게 특별히 정의되어 기술되"}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "지 않는 한, 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 일반적으로 이해될 수 있는 의미로 해 석될 수 있으며, 사전에 정의된 용어와 같이 일반적으로 사용되는 용어들은 관련 기술의 문맥상의 의미를 고려 하여 그 의미를 해석할 수 있을 것이다. 또한, 본 발명의 실시예에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함할 수 있고, \"A 및(와) B, C 중 적어 도 하나(또는 한 개 이상)\"로 기재되는 경우 A, B, C로 조합할 수 있는 모든 조합 중 하나 이상을 포함할 수 있 다. 또한, 본 발명의 실시 예의 구성 요소를 설명하는데 있어서, 제1, 제2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이러한 용어는 그 구성 요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성요소 의 본질이나 차례 또는 순서 등으로 한정되지 않는다. 그리고, 어떤 구성 요소가 다른 구성 요소에 '연결', '결합', 또는 '접속'된다고 기재된 경우, 그 구성 요소는 그 다른 구성 요소에 직접적으로 '연결', '결합', 또는 '접속'되는 경우뿐만 아니라, 그 구성 요소와 그 다른 구성 요소 사이에 있는 또 다른 구성 요소로 인해 '연결', '결합', 또는 '접속'되는 경우도 포함할 수 있다. 또한, 각 구성 요소의 \"상(위)\" 또는 \"하(아래)\"에 형성 또는 배치되는 것으로 기재되는 경우, \"상(위)\" 또는 \"하(아래)\"는 두 개의 구성 요소들이 서로 직접 접촉되는 경우뿐만 아니라, 하나 이상의 또 다른 구성 요소가 두 개의 구성 요소들 사이에 형성 또는 배치되는 경우도 포함한다. 또한, \"상(위)\" 또는 \"하(아래)\"로 표현되는 경우 하나의 구성 요소를 기준으로 위쪽 방향뿐만 아니라 아래쪽 방향의 의미도 포함될 수 있다. 도 1은 본 발명의 일 실시예에 따른 이미지 처리 장치의 블록도이다. 본 발명의 일 실시예에 따른 이미지 처리 장치는 수신기, 제1 프로세서, 신호 라인, 스위 치, 및 송신기로 구성되고, 동기화부 또는 메모리를 더 포함할 수 있다. 수신기는 제1 이미지 및 제2 이미지를 수신한다. 보다 구체적으로, 수신기는 복수의 이미지 센서의 각 이미지 센서에서 생성되어 출력되는 복수의 이미지를 각각 수신할 수 있다. 예를 들어, 두 개의 이미지 센서로부터 각각 제1 이미지 및 제2 이미지를 수신한다. 세개 이상의 이미지 센서로부터 각각 이미지를 수신할 수도 있다. 여기서, 제1 이미지 또는 제2 이미지는 이미지 센서가 생성하여 출력한 로우(Raw) 데이터로, 베이어 데이터 또는 IR 데이터일 수 있다. 베이어 데이터는 이미 지 처리를 수행하여 생성되는 RGB 이미지 데이터에 비해 많은 정보를 포함하고 있다. 복수의 이미지 센서의 각 이미지 센서는 서로 다른 기능을 수행하는 이미지 센서일 수 있다. 이미지 센서는 카 메라 모듈의 렌즈를 통해 들어오는 빛을 전기 신호로 변화하는 CMOS(Complementary Metal Oxide Semiconductor) 또는 CCD(Charge Coupled Device)와 같은 이미지 센서를 포함할 수 있다. 이미지 센서는 획득 한 이미지를 컬러 필터를 통해 베이어 패턴(Bayer Pattern)의 정보를 포함하는 베이어 데이터를 생성할 수 있다. 베이어 데이터는 이미지 센서의 스펙 또는 해당 이미지를 생성시 설정된 줌 배율에 따라 제1 해상도를 가 질 수 있다. 또는, IR 데이터를 생성하는 ToF(Time of Flight) 이미지 센서일 수 있다. 복수의 이미지 센서는 장착되는 렌즈에 따라 생성되는 이미지가 달라질 수 있다. 예를 들어, 망원 렌즈가 장착되는 경우, 줌이 적용된 데이터가 생성될 수 있다. 이외에도 수신기는 다양한 이미지 센서로부터 다양한 이미지를 수신할 수 있다. 제1 이미지와 제2 이미지는 동일 시간에 생성된 이미지일 수 있다. 동일 시간에 생성되는 이미지인 바, 제1 이미지와 제2 이미지는 동기화하여 이동 내지 처리될 수 있다. 제1 프로세서는 제1 이미지 및 제2 이미지 중 적어도 하나를 이용하여 제3 이미지를 출력한다. 보다 구체적으로, 제1 프로세서는 수신기가 수신한 제1 이미지 및 제2 이미지 중 적어도 하나를 입력 받으면, 입력 받은 이미지로부터 제3 이미지를 출력한다. 여기서, 제3 이미지는 입력 받은 이미지 의 해상도보다 높은 해상도를 가지는 이미지일 수 있다. 제1 프로세서는 학습된 콘볼루션 신경망을 이용하 여 제1 해상도를 가지는 제1 이미지 또는 제2 이미지로부터 제2 해상도를 가지는 제3 이미지를 생성할 수 있다. 여기서, 제1 해상도는 제2 해상도와 상이한 해상도이고, 제1 해상도보다 제2 해상도가 높을 수 있다. 제1 해상 도는 해당 이미지 센서가 출력하는 데이터의 해상도일 수 있고, 제2 해상도는 사용자의 설정에 따라 달라지거나, 미리 설정된 해상도일 수 있다. 프로세서는 학습된 콘볼루션 신경망을 이용하여 제1 해상도를 가지는 제1 이미지 또는 제2 이미지 중 적어 도 하나의 이미지로부터 제2 해상도를 가지는 제3 이미지를 생성하는 슈퍼 레졸루션(SR, Super Resolution)을 수행할 수 있다. 여기서, 슈퍼 레졸루션은 저해상도 이미지에 기초하여 고해상도 이미지를 생성하는 과정으로, 물리적인 광학 줌이 아닌 이미지 처리를 통해 저해상도 이미지로부터 고해상도 이미지를 생성하는 디지털 줌의 기능을 한다. 슈퍼 레졸루션은 압축 또는 다운 샘플링된 이미지의 품질을 개선하는 데 이용되거나, 장치적 한계 에 따른 해상도를 가지는 이미지의 품질을 향상시키는데 이용될 수 있다. 이외에도 다양한 분야에서 이미지의 해상도를 높이는 데 이용될 수 있다. 슈퍼 레졸루션과 같이, 해상도를 높이는 과정을 수행함에 있어서, RGB 이미지가 아닌 베이어 데이터인 제1 이미 지 또는 제2 이미지를 이용하여 해상도를 높이는 과정을 수행함으로써, 해상도를 높인 결과의 품질을 높일 수 있다. 프로세서는 ToF 이미지 센서에서 생성되어 입력받은 IR 데이터의 해상도를 높일 수 있다. 이후 설명할 제2 프로세서에서 IR 데이터인 제3 이미지로부터 생성되는 IR 이미지를 이용하여 다른 이미지 센서로부터 출력 되는 베이어 데이터로부터 생성되는 제1 RGB 이미지의 화질을 개선하기 위해서, IR 이미지와 제1 RGB 이미지의 크기, 즉 해상도가 동일하도록 제3 이미지를 생성할 수 있다. 프로세서가 콘볼루션 신경망을 이용하여 제3 이미지를 출력하는 과정은 이후에 도 9 내지 도 11을 참조하 여 자세히 설명하도록 한다. 신호 라인은 상기 제1 이미지 및 상기 제2 이미지 중 적어도 하나를 바이패스하는 라인이다. 보다 구체적으로, 신호 라인은 제1 프로세서로 입력되지 않은 제1 이미지 또는 제2 이미지(12 0)를 바이패스한다. 신호 라인은 수신기와 송신기를 연결하는 신호라인으로, 별도의 처리과정없 이 수신기에서 수신된 이미지를 송신기로 전달한다. 신호 라인은 제1 이미지 및 제2 이미 지 모두 제1 프로세서에 입력되지 않는 경우, 즉, 제1 프로세서를 통한 이미지 처리없이 모두 제1 프로세서를 바이패스할 수 있다. 또는, 제1 이미지 및 제2 이미지 모두 제1 프로세서 에 입력되는 경우, 각 이미지를 바이패스하지 않을 수도 있다. 스위치는 수신기로부터 출력된 신호를 제1 프로세서 또는 신호 라인으로 연결한다. 보다 구체적으로, 스위치는 수신기로부터 출력되는 제1 이미지 또는 제2 이미지를 각각 제1 프로세서 또는 신호 라인으로 연결한다. 제1 이미지 또는 제2 이미지 중 제1 프로세서를 통해 제3 이미 지를 생성하고자 하는 제어신호를 수신하는 경우, 해당 이미지를 제1 프로세서로 연결하고, 이외의 이미지 를 신호 라인으로 연결하여 제1 프로세서를 바이패스하도록 한다. 여기서, 제어신호는 특정 카메라의 데이터를 전처리 프로세서를 거치도록 하는 모드 지시일 수 있다. 스위치는 상기 제1 이미지 및 상기 제2 이미지 중 하나를 제1 프로세서에 연결하고, 다른 하나를 신 호 라인으로 연결할 수 있다. 제1 이미지 및 제2 이미지 중 하나를 이용하여 제3 이미지를 생성하는 제어 신호에 따라 제3 이미지를 생성하기 위한 제1 이미지 또는 제2 이미지를 제1 프로세서에 연결하고, 다른 이미지는 신호 라인에 연결하여 제1 프로세서를 바이패스할 수 있다. 또한, 스위치는 상기 제1 이미지 및 상기 제2 이미지를 상기 신호 라인으로 연결할 수 있다. 제3 이 미지를 생성하고자 하는 제1 이미지 또는 제2 이미지에 대한 제어신호를 수신하지 않은 경우, 제1 이미지 및 제 2 이미지 모두 신호 라인에 연결하여 제1 프로세서를 바이패스할 수 있다. 스위치는 도 2와 같이, 제어신호에 따라 상기 제1 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결 하는 제1 스위치 및 상기 제어신호에 따라 상기 제2 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연 결하는 제2 스위치를 포함할 수 있다. 수신기는 제1 이미지를 수신하여 출력하는 제1 수신채널(131- 1) 및 제2 이미지를 수신하여 출력하는 제2 수신채널(131-2)을 포함할 수 있고, 제1 스위치 및 제2 스위치 는 각각 제1 수신채널(131-1) 및 제2 수신채널(131-2)로부터 출력되는 신호를 신호 라인 또는 제1 프 로세서로 연결한다. 이때, 신호 라인은 도 2와 같이, 제1 신호 라인 및 제2 신호 라인을 포함할 수 있다. 제1 신호 라인은 제1 이미지를 바이패스하는 경로이고, 제2 신호 라인은 제2 이미지를 바이패스하는 경로일 수 있다. 두 이미지 모두 바이패스하는 경우가 있는바, 각각 신호라인이 형성될 수 있다. 제1 이미지는 제1 스위치에 의해 제1 프로세서 또는 제1 신호 라인에 연결되고, 제2 이미지는 제2 스위치에 의해 제1 프로세서 또는 제2 신호 라인에 연결될 수 있다. 스위치는 제1 이미지 또는 제2 이미지의 신호 라인 또는 제1 프로세서를 경유하는 경로를 연결 하는 역할을 하며, 수신기와 신호 라인 또는 제1 프로세서를 연결하거나, 신호 라인 또는 제1 프로세서와 송신기를 연결할 수 있다. 스위치는 멀티플렉서 또는 디멀티플렉서로 구현되거 나, FET 등의 일반 스위치를 이용할 수도 있다. 송신기는 상기 제1 내지 제3 이미지 중 적어도 두 개의 이미지를 송신한다. 보다 구체적으로, 송신기는 제1 프로세서 또는 신호 라인으로부터 이미지를 수신한다. 제1 프로 세서로부터 제3 이미지를 수신하고, 신호 라인으로부터 제1 이미지 또는 제2 이미지 중 적어도 하나 를 수신한다. 즉, 제1 이미지 및 제3 이미지, 제2 이미지 및 제3 이미지, 또는 제1 이미지 및 제2 이미지를 수 신할 수 있고, 수신된 두 개의 이미지를 외부로 송신한다. 송신기는 두 개의 이미지를 각각 제1 송신채널(135-1) 및 제2 송신채널(135-2)을 통해 송신할 수 있다. 제 1 송신채널(135-1)은 제1 이미지 또는 제3 이미지를 송신하고, 제2 송신채널(135-2)은 제2 이미지 또는 제3 이 미지를 송신할 수 있다. 송신기는 제2 프로세서로 상기 제1 내지 제3 이미지 중 적어도 두 개의 이미지를 송신할 수 있다. 제 2 프로세서는 수신한 2 개의 이미지에 대해 RGB 이미지를 생성하기 위해 필요한 추가적인 이미지 처리를 수행하여 RGB 이미지를 생성할 수 있다. 송신기가 상기 제1 내지 제3 이미지 중 적어도 두 개의 이미지를 송신시, 두 개의 이미지를 동시에 송신하 여야 할 수 있고, 이를 위하여, 도 2와 같이, 각 신호라인(133-1, 133-2)은 동기화부(136-1, 136-2)를 포함할 수 있다. 제1 신호 라인은 제1 동기화부(136-1)를 포함하고, 제2 신호 라인은 제2 동기화부(136-2)를 포함할 수 있다. 앞서 설명한 바와 같이, 제1 이미지 및 제2 이미지는 동일 시간에 생성되는 이미지인바, 동기화하여 전송되거나 처리되어야 할 필요가 있다. 하지만, 제1 프로세서 또는 신호 라인의 서로 다른 경로를 따르는 경우, 전달 시간에서 차이가 발생할 수 있고, 이로 인해 송신기에서 송신 시간이 달라질 수 있다.따라서, 송신기에서의 송신시간을 동기화하기 위하여, 각 신호라인에는 제1 프로세서를 통해 출력되는 제3 이미지와 각 신호라인을 통해 제1 프로세서를 바이패스한 이미지를 동기화시키는 동기화부를 포함한다. 예를 들어, 제1 이미지는 제1 스위치에 의해 제1 신호 라인으로 연결되고, 제2 이미지는 제2 스위치 에 의해 제1 프로세서로 연결되는 경우, 제1 프로세서에서 제2 이미지로부터 제3 이미지가 생성 하는 시간이 소요되어, 송신기가 제1 이미지와 제3 이미지를 송신하는 시간이 달라질 수 있다. 이를 동기 화하기 위하여, 제1 동기화부(136-1)가 제1 이미지가 송신기로 출력되는 시간을 제1 프로세서에서 제 3 이미지가 출력되는 시간에 동기화할 수 있다. 송신기는 동시에 수신한 제1 이미지와 제3 이미지를 동시 에 제2 프로세서로 송신할 수 있다. 제2 프로세서는 동기화하여 수신한 제1 이미지와 제3 이미지를 동기화하여 처리할 수 있다. 도 3은 본 발명의 실시예에 따른 이미지 처리 장치의 실제 구현예로, 두 개의 카메라(340, 350)에 형성되 는 두 개의 이미지 센서로부터 입력되는 제1 데이터에 대해 하나의 프로세서를 이용하여 이미지를 처리하 여, AP로 출력한다. 수신기와 송신기는 각각 카메라(110, 120)와 AP와 MIPI를 통해 데이터를 입출력할 수 있다. 여 기서, MIPI(Mobile Industry Processor Interface)는 모바일 및 사물인터넷 장치에서 재사용 및 호환성을 강화 하기 위해 AP를 비롯한 프로세서와 주변 기기들에 대한 인터페이스를 의미한다. AP는 해상도 변환 제어신호를 12C 인터페이스를 통해 두 개의 카메라(340, 350) 및 제1 프로세서 에 전달하여, 해상도 변환 동작을 제어할 수 있다. 사용자가 모바일 단말의 카메라 애플리케이션을 구동시 I2C 인터페이스(Interface)는 AP를 통하여 카메라 모듈(340, 350)과 이미지 처리 장치(AI Chip, 132)의 제어신호 및 초기 각 모듈 내지 장치들의 초기 설정 값 (initial Setup Value) 및 이미지 처리 장치(AI Chip)의 파라미터 데이터(Parameter Data)를 자동으로 전송할 수 있다. 또한, AP는 PMIC를 제어하여, 파워 네트워크를 통해, 카메라(340, 350) 및 이미지 처 리 장치에 대한 전력 전송을 제어할 수 있다. 즉, AP가 카메라(340, 350) 및 이미지 처리 장치 를 제어할 수 있다. 따라서, 사용자는 AP에 명령을 입력함으로써 카메라(340, 350) 및 이미지 처리 장치 를 제어할 수 있다. 수신기의 제1 수신채널(131-1) 및 제2 수신채널(131-2)로 입력된 제1 이미지 또는 제2 이미지를 제1 프로 세서 또는 신호라인(133-1, 133-2)로 인가될 수 있도록 동작하는 수신기 측 스위치는 도 3과 같이, 디멀티플렉서 및 멀티플렉서로 구현될 수 있다. 수신기에서 제1 프로세서 또는 신호 라인으로부터 경로는 복수의 제1 디멀티플렉서 (211, 212) 및 제1 멀티플렉서에 의해 설정될 수 있다. 여기서, 멀티플렉서 및 디멀티플렉서는 신호 경로를 스위칭하 는 구성으로써 신호의 분리, 조합기능이 없는 스위치일 수 있다. 또한, 특정 예시에서는 두 경로의 신호를 조합 /분리하는 기능을 포함하는 스위치 일 수도 있다. 제1 디멀티플렉서(211,212)는 각 수신채널(231, 232)로 입력되는 이미지가 프로세서 또는 신호라인으 로 연결되도록 동작한다. 디멀티플렉서(Demultiplexer)는 역다중화기로, 하나의 입력으로부터 복수 개를 출력하 는 회로로, 분배기라고도 한다. 도 3과 같이, 하나의 입력을 두 개의 출력 중 하나로 출력할 수 있다. 즉, 제1 이미지 또는 제2 이미지에 대해 프로세서로의 경로와 신호라인으로의 경로 중 하나와 연결되도록 동 작한다. 제1 멀티플렉서는 복수의 제1 디멀티플렉서(211, 212) 중 제1 이미지 또는 제2 이미지가 제1 프로세서 로 연결되도록 동작하는 제1 디멀티플렉서로부터 출력되는 이미지가 제1 프로세서로 연결되도록 동작 한다. 멀티플렉서(Multiplexer)는 복수의 입력 중 하나를 선택하여 출력하는 회로로, 도 3과 같이, 두 개의 입 력으로부터 하나를 출력할 수 있다. 제1 멀티플렉서로 인가되는 입력라인은 복수의 제1 디멀티플렉서 (211,212)의 수에 따라 달라질 수 있고, 복수의 입력 중 이미지가 연결되어 입력되는 경우, 입력된 이미지를 제 1 프로세서와 연결되도록 동작한다. 수신기 및 제1 프로세서 또는 신호 라인의 경로를 설정하는 것과 대응되어, 송신기 및 제1 프로세서 또는 신호 라인의 경로를 설정하기 위한 스위치가 포함할 수 있다. 송신기 측 스위치는 제1 프로세서에서 출력된 제3 이미지가 해당 송신채널에 연결되도록 동작하며, 제1 프로세서를 바이패스한 이미지가 해당 송신채널에 연결되도록 동작한다. 송신기 측 스위치의 동작은 수신기 측 스위치의 동작과 반대로 동작하며, 송신기 측 스위치 역시 해상도 변환 제어신호에 따라 동작할 수 있다. 송신기 측 스위치는 제1 프로세서에서 출력된 제3 이미지가 복수의 송신채널(135-1, 135-2) 중 하나 로 인가되도록 동작하는 적어도 하나의 제2 디멀티플렉서를 포함할 수 있고, 제2 디멀티플렉서에서 출력되는 제3 이미지 또는 제1 프로세서를 바이패스한 이미지가 복수의 송신채널 중 해당 송신채널과 연결 되도록 동작하는 제2 멀티플렉서(222,223)를 포함할 수 있다. 이와 같이, 구현되는 이미지 처리 장치는 두 개의 카메라(340, 350)에 대해 각각의 바이패스 경로 및 제1 프로세서를 통하는 해상도 변환 경로를 형성할 수 있다. 카메라1(CAM1, 340)에 대해서, 제1 수신채널(CH1, 131-1) -> 제1 디멀티플렉서 -> 제1 신호라인(Bypass1, 133-1) -> 제1 동기화부(Sync. Buffer, 136-1) -> 제2 멀티플렉서 -> 제1 송신채널(135-1)을 경유하는 바이패스 경로와 제1 수신채널(CH1, 131-1) -> 제1 디 멀티플렉서 -> 제1 멀티플렉서 -> 제1 프로세서(AI engine(Pre-Processor), 132) -> 제2 디멀티플렉 서 -> 제2 멀티플렉서 -> 제1 송신채널(135-1)을 경유하는 해상도 변환 경로를 형성한다. 또한, 카메라2(CAM2, 350)에 대해서, 제2 수신채널(CH2, 131-2) -> 제1 디멀티플렉서 -> 제2 신호라인 (Bypass2, 133-2) -> 제2 동기화부(Sync. Buffer, 136-2) -> 제2 멀티플렉서 -> 제1 송신채널(135-2)을 경유하는 바이패스 경로와 제2 수신채널(CH2, 131-2) -> 제1 디멀티플렉서 -> 제1 멀티플렉서 -> 제1 프로세서 -> 제2 디멀티플렉서 -> 제2 멀티플렉서 -> 제1 송신채널(135-2)을 경유하는 해상도 변환 경로를 형성한다. 바이패스 경로 또는 해상도 변환 경로는 사용자의 입력에 따른 모드 선택에 따라 동작할 수 있다. 선택가능한 모드는 바이패스 모드(AI-SR Bypass mode), 해상도 변환 모드(AI-SR Zoom mode), 슬립모드(Sleep mode)를 포함 할 수 있고, 이외에도 다양한 설정이 가능할 수 있다. 사용자가 카메라 애플리케이션을 시작하는 경우, AP가 카메라(340, 350)에 ＂Streaming ON＂을 요청하게 되고, 합니다. 바이패스 모드(AI-SR Bypass mode)는 카메라1 및 카메라2의 각각의 이미지 센서의 출 력을 제1 프로세서를 통한 변경없이 AP에 전송 함을 의미한다. 이 경우, 도 4와 같이, 경로가 형성된 다. 이때, 카메라 CAM1, CAM2, 제1 디멀티플렉서 DMW1, DMW2, 제1 멀티플렉서 MW1, 제2 디멀티플렉서 DMW3, 제2 멀티플렉서 MW2, MW3에 인가되는 제어신호는 다음과 같다. 표 1 CAM1 CAM2 DMW1 DMW2 MW1 DMW3 MW2 MW3 BypassBypass 0 0 Nop. Nop. 0 0 상기와 같이, 제어신호가 입력되는 경우, 각 이미지 센서로부터 입력된 각 제1 이미지 및 제2 이미지는 도 4와 같이, 굵게 표시된 경로를 따라 바이패스 경로로 전송된다. 해상도 변환 모드(AI-SR Zoom mode)는 AP가 카메라(340, 350)에 “X배 확대\"기능을 요청할 때 작동하며, 이 경우 제1 프로세서는 이미지 품질 향상을 위해 동작모드로 진입한다. 이 경우 제1 프로세서의 슈 러 레졸루션(Zoom SR)과 저조도 개선(De-noise) 기능 활성화로 제1 프로세서를 통해 제3 이미지인 전처리 된 카메라의 프레임(F＇) 출력은 제1 이미지 또는 제2 이미지인 원본 프레임(F)에 비하여 개선된 데이터가 출력 된다. 이때, 모드 선택(Mode Selection)은 도 5 및 도 6과 같이, CAM1/CAM2 각각의 활성화 모드가 있을 수 있 다. 카메라1(CAM1, 340)에 대한 해상도 변환 모드가 선택되는 경우, 인가되는 제어신호는 다음과 같다. 표 2 CAM1 CAM2 DMW1 DMW2 MW1 DMW3 MW2 MW3 SR Bypass 1 0 0 0 1 0 상기와 같이, 제어신호가 입력되는 경우, 도 5와 같이, 카메라1로부터 입력된 제1 이미지는 굵게 표시된 경로를 따라 해상도 변환 경로를 통해 제1 프로세서를 거쳐 전송되고, 카메라2로부터 입력된 제2 이 미지는 굵게 표시된 경로를 따라 바이패스 경로로 전송된다.카메라2(CAM2, 350)에 대한 해상도 변환 모드가 선택되는 경우, 인가되는 제어신호는 다음과 같다. 표 3 CAM1 CAM2 DMW1 DMW2 MW1 DMW3 MW2 MW3 Bypass SR 0 1 1 1 0 1 상기와 같이, 제어신호가 입력되는 경우, 도 6과 같이, 카메라1로부터 입력된 제1 이미지는 굵게 표시된 경로를 따라 바이패스 경로로 전송되고, 카메라2로부터 입력된 제2 이미지는 굵게 표시된 경로를 따라 해 상도 변환 경로를 통해 제1 프로세서를 거쳐 전송된다. 카메라1(CAM1, 340) 및 카메라2(CAM2, 350)에 대한 해상도 변환 모드가 선택되는 경우, 인가되는 제어신호는 다 음과 같다. 표 4 CAM1 CAM2 DMW1 DMW2 MW1 DMW3 MW2 MW3 Resv. Resv. 1 1 0 and 10 and 1 1 1 상기와 같이, 제어신호가 입력되는 경우, 도 7과 같이, 카메라1로부터 입력된 제1 이미지 및 카메라2(35 0)로부터 입력된 제2 이미지는 굵게 표시된 경로를 따라 해상도 변환 경로를 통해 제1 프로세서로 연결된 다. 제1 프로세서는 입력 받은 제1 이미지 및 제2 이미지를 각각 해상도 변환을 수행하거나, 각각의 제3 이미지를 생성할 수 있다. 이때, 제1 프로세서는 제1 이미지 및 제2 이미지에 대해 순차적으로, 또는 병렬 처리를 통해 동시에 해상도 변환을 수행할 수 있다. 또는 제1 프로세서는 입력 받은 제1 이미지 및 제2 이 미지를 결합(merge)하여 제3 이미지를 출력할 수 있다. 두 개의 이미지 센서 내지 두 개의 카메라에서 생성되는 이미지를 결합하는 다양한 형태의 제3 이미지를 생성할 수 있다. 이때, 결합 모드(merge mode)라 할 수 있다. 슬립모드(Sleep mode)는 CAM1과 CAM2 카메라 모두 입력되는 영상이 없을 때 프로세서는 슬립모드 상태에 들어가 게 되고, 바이패스 모드와 동일한 제어신호가 인가되어 다음 제어신호를 대기하게 된다. 표 5 CAM1 CAM2 DMW1 DMW2 MW1 DMW3 MW2 MW3 Resv. Resv. 0 0 Nop. Nop. 0 0 상기와 같이, 제어신호가 입력되는 경우, 이미지 센서로부터 제1 이미지 또는 제2 이미지가 입력되지 않아, 도 8과 같이, 아무런 이미지가 전송되지 않는다. 해상도 변환 제어신호에 따라 제1 프로세서에 제1 해상도를 가지는 제1 이미지 또는 제2 이미지 중 적어도 하나의 이미지가 입력되는 경우, 제1 프로세서는 도 9와 같이, 학습된 콘볼루션 신경망을 이용하여 제1 이 미지 또는 제2 이미지 중 적어도 하나의 이미지로부터 제3 이미지를 출력한다. 제1 프로세서는 제1 해상도를 가지는 이미지로부터 제2 해상도를 가지는 이미지를 생성하도록 학습된 콘볼 루션 신경망 (CNN, Convolution Neural Network)를 포함하고, 제1 해상도를 가지는 이미지 데이터로부터 제2 해상도를 가지는 이미지 데이터를 생성하는데 이용되는 파라미터를 저장할 수 있다. 파라미터는 메모리 상에 저 장되어 있을 수 있다. 제1 프로세서는 칩(chip)의 형태로 구현될 수 있다. 제1 프로세서는 입력되는 데이터의 종류에 따른 파라미터를 이용하여 콘볼루션을 수행할 수 있다. 이를 위 하여, 입력될 수 있는 입력 데이터의 종류에 대응하는 파라미터를 저장하고, 특정 데이터가 입력시, 해당 데이 터에 대응되는 파라미터를 이용하여 콘볼루션을 수행할 수 있다. 앞서 설명한 바와 같이, 제1 프로세서에 인가되는 제1 이미지 또는 제2 이미지는 베이어 데이터 또는 IR 데이터일 수 있다. 제1 프로세서에 인가되는 데이터가 베이어 데이터인 경우, 제1 프로세서는 학습된 콘볼루션 신경망을 이용하여 제1 이미지 또는 제2 이미지로 입력받는 제1 베이어 데이터로부터 제2 베이어 데이터를 생성할 수 있다. 이외의 해상도를 높이는 알고리즘을 이용하여 제1 베이어 데이터로부터 제2 베이어 데이터를 생성할 수도 있다. 슈퍼 레졸루션(SR)에 이용되는 다양한 알고리즘을 이용할 수 있음은 당연하다. 제1 프로세서이 이용하는 콘볼루션 신경망(모델)은, 입력된 이미지 데이터의 해상도보다 더 높은 해상도를 가지는 이미지 데이터를 생성하는 모델로서, 트레이닝(training)을 통해 학습된다. 제1 프로세서는 심층 신경망(Deep Neural Network, DNN) 알고리즘이 적용되어 학습된 모델을 이용할 수 있 다. 심층 신경망(DNN)은 입력층(input layer)과 출력층(output layer) 사이에 다중의 은닉층(hidden layer)이 존재하는 심층(deep) 신경망, 동물의 시각 피질의 구조와 유사하게 뉴런 사이의 연결 패턴을 형성하는 콘볼루션 (convolutional) 신경망, 시간에 따라 매 순간마다 신경망을 쌓아 올리는 재귀(recurrent) 신경망으로 구체화될 수 있다. 구체적으로 콘볼루션 신경망은 콘볼루션(Convolution)과 서브 샘플링(Sub-Sampling)을 반복하여 데이 터의 양을 줄이고, 왜곡시켜 신경망을 분류한다. 즉, 콘볼루션 신경망은 특징 추출과 분류 행위를 통해 분류 결 과를 출력하는데, 주로 이미지를 분석하는데 사용되며, 콘볼루션은 영상 필터링을 의미한다. 제1 프로세서는 제1 해상도를 가지는 데이터를 기초로 배율을 높이고자 하는 영역에 대해 콘볼루션과 서브 샘플링(Sub-Sampling)을 수행할 수 있다. 서브 샘플링은 이미지의 크기를 줄이는 과정을 의미한다. 이때, 서브 샘플링은 맥스-풀(Max Pool) 방법 등을 사용할 수 있다. 맥스-풀은 해당 영역에서 최대치를 선택하는 기법인데 뉴런이 가장 큰 신호에 반응하는 것과 유사하다. 서브 샘플링은 노이즈를 감소시키고, 학습의 속도를 증가시킬 수 있는 장점이 존재한다. 콘볼루션과 서브 샘플링이 수행되면, 복수 개의 데이터가 출력될 수 있다. 여기서, 복수 개의 데이터은 특징 맵 일 수 있다. 그 후 복수 개의 이미지 데이터들을 기초로 업 스케일(Up Scale) 방식으로 이용하여 서로 다른 특 징을 가지는 복수 개의 이미지 데이터를 출력시킬 수 있다. 업 스케일 방식은 서로 다른 r^2개의 필터를 이용하 여 이미지를 r*r 배로 스케일을 높이는 것을 의미한다. 업 스케일에 따라 복수 개의 데이터가 출력되면, 제1 프로세서는 이러한 이미지 데이터들을 기초로 재조합 을 하여 최종적으로 제2 해상도를 가지는 데이터를 출력할 수 있다. 콘볼루션 신경망에 대한 트레이닝 과정은 도 10과 같이 반복적인 학습을 통해 수행될 수 있다. 서로 상이한 해 상도를 갖는 제1 샘플 데이터(X)와 제2 샘플 데이터(Z)를 입력 받은 후 이를 기초로 딥 러닝 트레이닝을 수행할 수 있다. 구체적으로, 제1 샘플 데이터(X)를 입력 데이터로 하여 딥 러닝 트레이닝을 수행한 제1 출력 데이터(Y)와 제2샘 플 데이터(Z)를 비교하고 분석하여 생성된 파라미터를 기초로 더 높은 해상도를 가지는 베이어 데이터를 생성하 는 알고리즘을 생성할 수 있다. 여기서 제1 출력 데이터(Y)는 실제 딥 러닝을 수행하여 출력된 데이터이고, 제2 샘플 데이터(Z)는 사용자가 입 력하는 데이터로서, 제1 샘플 데이터(X)를 알고리즘에 입력하였을 때, 가장 이상적으로 출력될 수 있는 데이터 를 의미할 수 있다. 여기서, 제1 샘플 데이터(X)는 제2 샘플 데이터(Z)를 다운 샘플링하여 해상도를 낮춘 데이 터일 수 있다. 이때, 다운 샘플링 정도는, 딥 러닝을 통해 확대할 비율, 즉 디지털 줌을 수행할 줌 비율에 따라 달라질 수 있다. 예를 들어, 딥 러닝을 통해 수행될 줌 비율이 3 배이고, 제2 샘플 데이터(Z)의 해상도가 9MP(Mega Pixel)인 경우, 제1 샘플링 데이터(X)의 해상도가 1MP이어야, 딥 러닝을 수행하여 해상도가 3 배 커진 제1 출력 데이터(Y)의 해상도 9MP이 되는 바, 9M의 제2 샘플 데이터(Z)를 1/9로 다운 샘플링하여 1MP의 제1 샘 플 데이터(Y)를 생성할 수 있다. 제1 샘플 데이터(X)의 입력에 따른 딥 러닝 수행을 통해 출력되는 제1 출력 데이터(Y)와 제2 샘플 데이터(Z)를 비교하고 분석하여 두 데이터 간 차이를 산출하고, 두 데이터 간 차이를 줄이는 방향으로 딥 러닝 모델의 파라 미터에 피드백을 줄 수 있다. 이때, 두 데이터 간 차이는 손실 함수 중 하나인 평균 제곱 오차인 MSE(Mean Squared Error) 방식을 통해 산출될 수 있다. 이외에 CEE(Cross Entropy Error) 등 다양한 손실함수를 이용할 수 있다. 구체적으로, 출력 데이터에 영향을 주는 파라미터를 분석한 후, 파리미터를 변경하거나 삭제 또는 새로운 파라 미터를 생성하는 방식으로 피드백을 주어 제2 샘플 데이터(Z)와 실제 출력 데이터인 제1 출력 데이터(Y)의 차이 가 없도록 할 수 있다. 도 10에 도시된 바와 같이 알고리즘에 영향을 주는 레이어가 총 3개이고(L1, L2, L3) 각각의 레이어에 총 8개의 파라미터(P11, P12, P13, P21, P22, P31, P32)가 존재한다고 가정할 수 있다. 이러한 경우, P22 파라미터의 값을 증가시키는 방향으로 파라미터를 변경하였더니 제1 출력 데이터(Y)와 제2 샘플 데이터(Z)의 차이가 증가한다 면, 피드백은 P22 파라미터를 감소시키는 방향으로 알고리즘을 변경할 수 있다. 이와 반대로, P33 파라미터의 값을 증가시키는 방향으로 파라미터를 변경하였더니 제1 출력 데이터(Y)와 제2 샘플 데이터(Z)의 차이가 감소하 였다면, 피드백은 P33 파라미터를 증가시키는 방향으로 알고리즘을 변경할 수 있다. 즉, 이러한 방법을 통해 딥 러닝이 적용된 알고리즘은 제1 출력 데이터(Y)가 제2 샘플 데이터(Z)와 유사하게 출 력되도록 파라미터를 도출할 수 있다. 이때, 제2 샘플 데이터(Z)의 해상도는 제1 출력 데이터(Y)의 해상도와 동 일하거나 높을 수 있으며, 제2 샘플 데이터(Z)의 해상도는 제1 출력 데이터(Y)의 해상도와 동일할 수 있다. 딥 러닝 트레이닝은 도 10과 같이, 출력 결과와 비교 대상이 존재하고, 비교 대상과의 비교를 통해 학습을 수행 하는 경우뿐만 아니라, 보상치를 이용하여 트레이닝을 수행할 수도 있다. 이경우, 먼저 주변 환경을 인지하고 현재 환경 상태를 딥 러닝 트레이닝을 수행하는 프로세서에 전달할 수 있다. 프로세서는 이에 맞는 행동 (Action)을 수행하고 환경은 다시 그 행동에 따른 보상치를 프로세서에게 알려준다. 그리고 프로세서는 보상치 를 최대로 하는 행동을 택하게 된다. 이러한 과정에 통해 학습을 반복적으로 진행함으로써 트레이닝을 수행할 수 있다. 이외에도 다양한 딥 러닝 트레이닝 방법을 이용하여 딥 러닝 트레이닝을 수행할 수 있다. 도 11은 학습된 콘볼루션 신경망을 이용하여 이미지 처리를 수행하는 이미지 처리 장치에 구현되는 파이프 라인 프로세서를 도시한 것으로, 파이프라인 프로세서는 수신기, 블록 추출기, 콘볼루션 신경망 , 블록버퍼, 및 송신기로 구성될 수 있다. 도 11의 파이프라인 프로세서는 학습된 콘볼루션 신 경망을 이용하여 해상도 변환 과정을 수행할 수 있다. 수신기가 제1 이미지 또는 제2 이미지를 입력받을 때, 다양한 포맷의 데이터가 입력될 수 있다. 예를 들어, 1xN, NxN, NxM의 블록크기를 가지는 포맷으로 입력될 수 있다. 블록 추출기는 효율적이고 빠른 처리를 위하여, 수신된 데이터의 포맷을 1xN 형태의 블록으로 추출할 수 있다. 노이즈 레벨 및 수신된 데이터로부터 제3 이미지를 출력하도록 학습된 콘볼루션 신경망은 블록 추출 기에서 추출된 블록을 입력 받아, 수신된 데이터로부터 제3 이미지의 데이터를 출력할 수 있다. 콘볼루션 신경망에서 입력된 1xN 블록 이미지가 고해상도, 즉 사이즈가 커진 sxsN 블록으로 출력되며, 블록 버퍼 에서 sxsN 블록을 저장하여, 하나의 제3 이미지를 생성하고, 송신기에서 제3 이미지를 출력할 수 있 다. 제1 프로세서에 인가되는 제1 이미지 또는 제2 이미지가 IR 데이터인 경우, 제1 프로세서는 학습된 콘볼루션 신경망을 통해 제1 이미지 또는 제2 이미지로 입력받는 제1 IR 데이터로부터 제2 IR 데이터를 생성할 수 있다. IR 데이터는 ToF 이미지 센서로부터 입력될 수 있다. ToF 이미지 센서는 깊이 정보를 획득할 수 있은 장치 중 하나로, ToF 방식에 따르면, 비행 시간, 즉 빛을 쏘아 서 반사되어 오는 시간을 측정함으로써 물체와의 거리를 계산한다. ToF 이미지 센서는 출력광 신호를 생성한 후 객체에 조사한다. ToF 이미지 센서는 직접(direct) 방식 또는 간접(indirect) 방식 중 적어도 하나 이상의 방식을 이용할 수 있다. 간접 방식의 경우, 펄스파(pulse wave)의 형태나 지속파(continuous wave)의 형태로 출력광 신호를 생성 하여 출력할 수 있다. 지속파는 사인파(sinusoid wave)나 사각파(squared wave)의 형태일 수 있다. 출력광 신호 를 펄스파나 지속파 형태로 생성함으로써, ToF 이미지 센서는 출력된 출력광 신호와 객체로부터 반사된 후 ToF 이미지 센서로 입력된 입력광 신호 사이의 위상 차를 검출할 수 있다. 직접 방식은 물체를 향해 보낸 출력광 신호가 수신부에 되돌아오는 시간을 측정해 거리를 유추하는 방식이고, 간접 방식은 물체를 향해 보낸 사인파가 수신부에 되돌아올 때 위상차를 사용해 거리를 간접 측정하는 방식이다. 주파수가 동일한 두 파형의 마루(최대값) 또는 골(최소값) 사이 차이를 활용한다. 간접 방식은 펄스 폭이 큰 빛이 있어야 측정거리가 늘어나는데, 측정거리를 늘리면 정밀도가 떨어지고, 반대로 정밀도를 높이면 측정거리가 줄어드는 특성이 있다. 직접 방식은 간접 방식보다 먼 거리 측정에 유리하다. ToF 이미지 센서는 입력광 신호로부터 전기 신호를 생성한다. 생성된 전기 신호를 이용하여 출력광과 입력광 사 이의 위상차를 계산하고, 위상차를 이용하여 객체와 ToF 이미지 센서 사이의 거리를 산출한다. 구체적으로, 전 기신호의 전하량 정보를 이용하여 출력광과 입력광 사이의 위상차를 계산할 수 있다. 출력광 신호의 주파수마다 전기신호는 4개가 생성될 수 있다. 따라서, ToF 이미지 센서는 아래의 수학식 1을 이용하여 출력광 신호와 입력 광 신호 사이의 위상차(td)를 계산할 수 있다. 수학식 1"}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, Q1 내지 Q4는 4개의 전기 신호 각각의 전하 충전량이다. Q1은 출력광 신호와 동일한 위상의 기준신호에 대응하는 전기신호의 전하량이다. Q2는 출력광 신호보다 위상이 180도 느린 기준신호에 대응하는 전기신호의 전 하량이다. Q3는 출력광 신호보다 위상이 90도 느린 기준신호에 대응하는 전기신호의 전하량이다. Q4는 출력광 신호보다 위상이 270도 느린 기준신호에 대응하는 전기신호의 전하량이다. 그러면, 출력광 신호와 입력광 신호 의 위상차를 이용하여 객체와 ToF 이미지 센서 사이의 거리를 계산할 수 있다. 이때, 아래의 수학식 2를 이용하여 객체와 ToF 이미지 센서 사이의 거리(d)를 계산할 수 있다. 수학식 2"}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, c는 빛의 속도이고, f는 출력광의 주파수이다. ToF 이미지 센서는 출력광, 입력광을 이용하여 IR 데이터를 생성한다. 이때, ToF 이미지 센서는 4개의 위상에 대한 IR 데이터인 로우(raw) 데이터를 생성할 수 있다. 여기서, 4개의 위상은 0°, 90°, 180°, 270°일 수 있 으며, 각 위상에 대한 IR 데이터는 위상 별로 디지털화된 픽셀 값으로 이루어진 데이터일 수 있다. IR 데이터는 위상 데이터(영상), 위상 IR 데이터(영상) 등과 혼용될 수 있다. 제1 프로세서는 제1 해상도를 가지는 제1 IR 데이터로부터 제2 해상도를 가지는 제2 IR 데이터를 생성하는 데 이용되는 파라미터인 IR 데이터 파라미터를 메모리에 저장할 수 있다. IR 데이터에 적용되는 콘볼루션 신경망 (모델)은, 이용되는 파라미터가 상이할 뿐, 앞서 설명한 베이어 데이터 에 대한 콘볼루션 신경망에 대응된다. IR 데이터에 대한 파라미터는 앞서 설명한 바와 같이, 별도의 트레이닝을 통해 도출될 수 있다. 상기와 같이, 제1 프로세서가 출력되는 제2 해상도를 가지는 제3 이미지는 제1 프로세서를 바이패스 한 다른 이미지와 함께 송신기에서 제2 프로세서로 출력된다. 제1 내지 제3 이미지 중 적어도 두 개 의 이미지를 수신한 제2 프로세서는 수신한 적어도 두 개의 이미지 중 적어도 하나의 이미지를 이미지 처 리하여 이미지를 생성할 수 있다. 해당 이미지가 베이어 데이터인 경우, RGB 이미지를 생성할 수 있고, 해당 이 미지가 IR 데이터인 경우, IR 이미지 또는 깊이 이미지를 생성할 수 있다. 또한, 제2 프로세서는 RGB 이미 지와 IR 이미지를 연산하여, 저조도가 개선된 RGB 이미지를 생성할 수도 있다. 제2 프로세서는 베이어 데이터에 대한 이미지 처리를 통해 제1 RGB 이미지를 생성할 수 있다. 제2 프로세 서의 이미지 처리과정은 감마값 보정(gamma correction), 색채 보정(color correction), 자동 노출(auto exposure correction), 자동 화이트값 보정(auto white balance) 과정 중에서 하나 이상을 포함할 수 있다. 제 2 프로세서는 ISP(Image Signal Processor) 일 수 있고, AP 상에 형성될 수 있다. 또는, ISP와 별도로 구 성되는 처리부일 수 있다. 또한, 제2 프로세서는 IR 데이터를 이용하여 앰플리튜드(amplitude) 이미지 또는 인텐시티(intensity) 이 미지인 IR 데이터를 생성할 수 있다. ToF 이미지 센서가 간접(indirect) 방식인 경우, ToF 이미지 센서로부터 출력되는 4 개의 서로 다른 위상을 가 지는 4 개의 IR 데이터를 이용하여 수학식 3과 같이 연산하면, ToF IR 이미지인 앰플리튜드 이미지(amplitude image)를 얻을 수 있다.수학식 3"}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, Raw(x0)은 phase 0°에서 ToF 이미지 센서가 받아들이는 픽셀 별 데이터 값이고, Raw(x90)은 phase 90 °에서 센서가 받아들이는 픽셀 별 데이터 값이며, Raw(x180)은 phase 180°에서 센서가 받아들이는 픽셀 별 데 이터 값이고, Raw(x270)은 phase 270°에서 센서가 받아들이는 픽셀 별 데이터 값일 수 있다. 또는, 4개의 IR 데이터를 이용하여 수학식 4와 같이 연산하면, 다른 ToF IR 이미지인 인텐시티 이미지 (intensity image)를 얻을 수도 있다. 수학식 4"}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "상기와 같이, ToF IR 이미지는 4 개의 위상 IR 데이터 중 2개씩 서로 빼주는 과정을 통하여 생성되는 이미지로, 이러한 과정에서 외부 광(background light)이 제거될 수 있다. 이에 따라, ToF IR 이미지에는 광원이 출력한 파장대의 신호만 남게 되어, 객체에 대한 IR 감도가 높아지고, 노이즈가 현저하게 줄어들 수 있다. 제2 프로세서에서 생성되는 IR 이미지는 앰플리튜드(amplitude) 이미지 또는 인텐시티(intensity) 이미지 를 의미할 수 있으며, 인텐시티 이미지는 컨피던스(confidence) 이미지과 혼용될 수 있다. IR 이미지는 그레이 이미지일 수 있다. 한편, 4 개의 위상 IR 데이터를 이용하여 수학식 5 및 수학식 6과 같이 연산하면, 깊이(Depth) 이미지도 얻을 수 있다. 수학식 5"}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "수학식 6"}
{"patent_id": "10-2020-0047034", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "제2 프로세서는 상기 생성된 IR 이미지를 이용하여 제1 RGB 이미지로부터 화질이 개선된 제2 RGB 이미지를 생성할 수 있다. 보다 구체적으로, 제2 프로세서는 상기 제1 RGB 이미지의 반사 성분과 상기 IR 이미지를 연산하여 산출되 는 결과 값과 상기 제1 RGB 이미지의 색조 성분 및 채도 성분을 이용하여 상기 제2 RGB 이미지를 생성할 수 있 다. 상기와 같이 생성된 IR 이미지를 이용하여 낮은 조도 환경에서 이미지 센서에서 촬영되어 생성된 RGB 이미지의 화질을 개선할 수 있다. 제2 프로세서는 베이어 데이터인 이미지로부터 제1 RGB 이미지를 생성한다. 이후, 색상 채널 변환을 통하여 제1 RGB 이미지를 제1 HSV 이미지로 변환한다. 여기서, RGB 이미지는 빨강(Red), 초록 (Green), 파랑(Blue)의 세 가지 성분의 조합으로 나타낸 데이터를 의미하고, HSV 이미지는 색조(Hue), 채도 (Saturation), 명도(Value)의 세 가지 성분의 조합으로 나타낸 데이터를 의미할 수 있다. 여기서, 색조(Hue)와채도(Saturation)는 색상 정보를 가지고 있고, 명도(Value)는 밝기 정보를 가질 수 있다. 이후, 제1 HSV 이미지 의 색조 성분(H), 채도 성분(S), 및 명도 성분(V) 중 명도 성분(V)을 반사 성분과 조명 성분으로 분리하여 반사 성분을 추출한다. 여기서, 반사 성분은 고주파 성분을 포함할 수 있고, 조명 성분은 저주파 성분을 포함할 수 있으며, 이하 반사 성분을 추출하기 위하여 명도 성분(V)을 저주파 성분과 고주파 성분으로 분리한 후 이로부터 고주파 성분을 분 리하는 것을 예로 들어 설명하나, 이로 제한되는 것은 아니다. 반사 성분, 예를 들어 고주파 성분은 이미지의 그래디언트(gradient) 정보 또는 에지(edge) 정보를 포함하고, 조명 성분, 예를 들어 저주파 성분은 이미지의 밝기 정보를 포함할 수 있다. 이를 위하여, 제1 HSV 이미지의 명도 성분(V)에 대하여 저주파 통과 필터링을 수행하여, 저주파 성분(L)을 획득 할 수 있다. 제1 HSV 이미지의 명도 성분(V)에 대하여 저주파 통과 필터링을 수행하면, 블러(blur)되어 그래디 언트 정보 또는 에지 정보가 손실될 수 있다. 저주파 성분(L)을 제거하는 연산을 통해 제1 HSV 이미지의 명도 성분에 대한 고주파 성분(R)을 획득한다. 이를 위하여, 제1 HSV 이미지의 명도 성분(V)과 저주파 성분(L)이 연 산될 수 있다. 예를 들어, 제1 HSV 이미지의 명도 성분(V)에서 저주파 성분(L)을 빼는 연산이 수행될 수 있다. 제2 프로세서는 IR 데이터 이미지로부터 IR 이미지를 생성한다. 여기서, ToF IR 이미지는 0°, 90°, 180 °및 270°의 4개의 위상에 대한 IR 데이터로부터 생성된 앰플리튜드(amplitude) 이미지 또는 인텐시티 (Intensity) 이미지일 수 있다. 이때, 제2 프로세서는 상기 제1 RGB 이미지와의 연산을 수행하기 전에 상기 IR 이미지를 보정할 수 있다. 예를 들어, ToF IR 이미지는 제1 RGB 이미지와 크기가 상이할 수 있으며, 일반적으로는 ToF IR 이미지는 제1 RGB 이미지보다 작을 수 있다. 따라서, ToF IR 이미지에 보간(interpolation)을 수행하여 ToF IR 이미지의 크기 를 제1 RGB 이미지의 크기로 확대할 수 있다. 이와 같은 보간 과정에서 이미지가 왜곡될 수 있으므로, ToF IR 이미지의 밝기를 보정할 수 있다. 이후, 제1 HSV 이미지의 명도 성분에 대한 조명 성분을 획득함과 동시에, 제1 HSV 이미지의 명도 성분에 대한 반사 성분, 예를 들어 고주파 성분과 ToF IR 이미지을 이용하여 제2 HSV 이미지 의 명도 성분(V')을 획득한다. 구체적으로, 제1 HSV 이미지의 명도 성분에 대한 반사 성분, 예를 들어 고주파 성분과 ToF IR 이미지를 정합할 수 있다. 여기서, ToF IR 이미지를 이용하여 모델링된 조명 성분과 반사 성분을 병합하여 밝기가 개선된 이미지를 획득하기 위한 연산이 이용될 수 있으며, 이는 제1 HSV 이미지의 명도 성분으 로부터 저주파 성분(L)을 제거하기 위하여 사용된 연산과 반대되는 연산일 수 있다. 예를 들어, 제1 HSV 이미지 의 명도 성분에 대한 반사 성분, 예를 들어 고주파 성분과 ToF IR 이미지가 더해지는 연산이 수행될 수 있다. 이와 같이, 제1 HSV 이미지의 명도 성분에 대한 조명 성분, 예를 들어 저주파 성분을 제거한 후, 제1 HSV 이미 지의 명도 성분에 대한 반사 성분, 예를 들어 고주파 성분과 ToF IR 이미지를 연산하면, 저조도 환경에서 촬영 된 RGB 이미지의 밝기가 개선될 수 있다. 이후, 연산을 통해 획득된 명도 성분(V')과 색상 채널 변환을 통해 획득된 색조 성분(H) 및 채도 성분(S)을 이 용하여 색상 채널 변환을 통하여 제2 RGB 이미지를 생성한다. HSV 이미지 중 색조 성분(H)과 채도 성분(S)은 색 상 정보를 가질 수 있고, 명도 성분(V)은 밝기 정보를 가질 수 있다. 명도 성분(V)의 반사 성분만을 ToF IR 이 미지와 연산한 값(V')과, 색조 성분(H) 및 채도 성분(S)은 기존 획득된 대로 이용하면, 색상의 왜곡 없이 저조 도 환경에서의 밝기만 개선할 수 있다. 입력 이미지는 반사 성분과 조명 성분의 곱으로 이루어질 수 있으며, 반 사 성분은 고주파 성분으로 이루어져 있고, 조명 성분은 저주파 성분으로 이루어질 수 있으며, 이미지의 밝기는 조명 성분에 의하여 영향을 받을 수 있다. 다만, 저조도 환경에서 촬영된 RGB 이미지로부터 조명 성분, 즉 저주 파 성분을 제거할 경우 RGB 이미지의 밝기 값이 지나치게 높아질 수 있다. 이러한 점을 보완하기 위하여 조명 성분, 즉 저주파 성분이 제거된 RGB 이미지의 명도 성분에 ToF IR 이미지를 정합함으로써 결과적으로 저조도 환 경에서 화질이 개선된 RGB 이미지를 얻을 수 있다. 앞서 설명한 바와 같이, 제2 프로세서에서 생성되는 IR 이미지는 서로 다른 4개의 위상에 따른 제2 IR 데 이터로부터 생성되는 앰플리튜드(amplitude) 이미지 또는 인텐시티(intensity) 이미지일 수 있는데, 서로 다른 4개의 위상에 따른 IR 데이터를 이용하는 간접 방식의 ToF 이미지 센서의 경우, 하나의 IR 이미지를 생성하기 위해선 ToF 이미지 센서의 1주기의 시간이 필요한 바, 이미지 센서가 제1 데이터를 생성하는 시간보다 ToF 이미 지 센서에서 제1 IR 데이터를 생성하는 시간보다 길 수가 있다. 따라서, 화질이 개선된 RGB 이미지를 생성함에 있어서, 시간지연이 발생할 수 있다. 이러한 시간지연을 방지하기 위하여, ToF 이미지 센서의 시간당 프레임 속도(fps)는 RGB 이미지 센서의 시간당 프레임 속도보다 빠를 수 있다. ToF 이미지 센서가 하나의 IR 이미지를 생성하기 위하여 서로 다른 4개의 위상에 따른 IR 데이터를 생성해야 하고, 이를 위하여, 각 위상에 따른 IR 데이터인 서브프레임을 촬영하는 ToF 이 미지 센서의 시간당 프레임 속도를 RGB 이미지 센서의 시간당 프레임 속도보다 빠르게 제어하여 시간지연을 방 지할 수 있다. ToF 이미지 센서의 시간당 프레임 속도는 RGB 이미지 센서의 시간당 프레임 속도에 따라 설정될 수 있다. ToF 이미지 센서가 하나의 위상에 따른 IR 데이터인 서브프레임을 촬영하는 속도는 RGB 이미지 센서가 하나의 베이어 데이터를 생성하기 위해 촬영하는 속도보다 빠를 수 있다. 또한, 작업 환경이나, 줌 배율이나 ToF 이미지 센서 또는 RGB 이미지 센서의 스펙에 따라 시간당 프레임 속도는 달라질 수 있다. 따라서, ToF 이미 지 센서가 하나의 IR 이미지를 생성하기 위하여 서로 다른 4개의 위상에 따른 IR 데이터를 생성하는 시간과 RGB 이미지 센서가 하나의 베이어 데이터를 생성하기 위해 촬영하는 시간을 고려하여, ToF 이미지 센서의 시간당 프 레임 속도를 다르게 설정할 수 있다. 상기 시간당 프레임 속도는 각 센서의 셔터 속도일 수 있다. 또한, 제2 프로세서는 제2 RGB 이미지뿐만 아니라, ToF 이미지 센서의 IR 데이터로부터 생성된 IR 이미지 및 깊이 이미지를 RGB 이미지에 정합하고 렌더링하여 컬러 정보 및 깊이 정보를 모두 포함하는 3차원 컬러 이미 지를 생성할 수도 있다. 도 12는 본 발명의 일 실시예에 따른 이미지 처리 방법의 흐름도이고, 도 13은 본 발명의 다른 실시예에 따른 이미지 처리 방법의 흐름도이다. 도 12 내지 도 13의 각 단계에 대한 상세한 설명은 도 1 내지 도 11의 이미지 처리 장치에 대한 상세한 설명에 대응되는바, 이하 중복된 설명은 생략하도록 한다. 복수의 이미지에 대해 하나의 프로세서를 이용하여 이미지를 처리하기 위하여, 먼저, S11 단계에서 제1 이미지 및 제2 이미지를 수신하고, S12 단계에서 상기 제1 이미지 및 상기 제2 이미지를 제1 프로세서 또는 신호라인으 로 연결한다. 이때, 제1 이미지 또는 제2 이미지 중 적어도 하나를 이용하여 제3 이미지를 생성하거나, 제3 이 미지를 생성하지 않을 수 있고, 이를 제어하는 제어신호에 따라 제1 스위치를 이용하여 상기 제1 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결하고, 상기 제어신호에 따라 제2 스위치를 이용하여 상기 제2 이미지를 상기 신호라인 또는 상기 제1 프로세서로 연결할 수 있다. 제1 이미지 및 제2 이미지 중 하나를 이용하여 제3 이미지를 생성하고자 하는 경우, 상기 제1 이미지 및 상기 제2 이미지 중 하나를 상기 제1 프로세서에 연결하고 다른 하나를 상기 신호라인으로 연결하거나, 제1 이미지 및 제2 이미지 모두를 이용하여 제3 이미지를 생성하고 자 하는 경우, 상기 제1 이미지 및 상기 제2 이미지를 상기 제1 프로세서로 연결할 수 있다. 또는 제1 이미지 및 제2 이미지로부터 제3 이미지를 생성하지 않는 경우, 상기 제1 이미지 및 상기 제2 이미지를 상기 신호라인 으로 연결할 수 있다. 상기 신호라인은 제1 신호라인 및 제2 신호라인을 포함할 수 있고, 상기 제1 이미지 또는 상기 제2 이미지를 상 기 신호라인으로 연결 시, 상기 제1 이미지는 상기 제1 신호라인에 연결하고, 상기 제2 이미지는 상기 제2 신호 라인에 연결할 수 있다. 제1 이미지 및 제2 이미지 중 적어도 하나가 제1 프로세서와 연결된 경우, S13 단계에서 상기 제1 프로세서가 상기 제1 이미지 및 제2 이미지 중 적어도 하나를 이용하여 제3 이미지를 출력한다. 제1 프로세서는 콘볼루션 신경망을 포함하고, 학습된 콘볼루션 신경망을 이용하여 상기 제1 이미지 및 제2 이미지 중 적어도 하나를 이용 하여 제3 이미지를 출력할 수 있다. S13 단계에서 제1 이미지 및 제2 이미지 중 적어도 하나를 이용하여 제3 이미지를 출력함과 동시에 S14 단계에 서 상기 신호라인이 상기 제1 이미지 및 상기 제2 이미지 중 적어도 하나를 바이패스시킨다. 상기 신호라인에 연결된 제1 이미지 또는 제2 이미지는 S21 단계에서 상기 제3 이미지와 동기화할 수 있다. 이후, S15 단계에서 상기 제1 프로세서에서 출력되는 제3 이미지, 상기 신호라인을 통해 바이패스된 상기 제1 이미지 또는 상기 제2 이미지 중 적어도 두 개의 이미지를 송신한다. 이때, 제2 프로세서로 송신할 수 있다. 한편, 본 발명의 실시예들은 컴퓨터로 읽을 수 있는 기록 매체에 컴퓨터가 읽을 수 있는 코드로 구현하는 것이 가능하다. 컴퓨터가 읽을 수 있는 기록 매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 컴퓨터가 읽을 수 있는 기록 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 데이터 저장장치 등이 있으며, 또한, 컴퓨터가 읽을 수 있는 기록 매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산 방 식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 그리고 본 발명을 구현하기 위한 기능적인 (functional) 프로그램, 코드 및 코드 세그먼트들은 본 발명이 속하는 기술 분야의 프로그래머들에 의하여 용이 하게 추론될 수 있다. 이상과 같이 본 발명에서는 구체적인 구성 요소 등과 같은 특정 사항들과 한정된 실시예 및 도면에 의해 설명되 었으나 이는 본 발명의 보다 전반적인 이해를 돕기 위해서 제공된 것일 뿐, 본 발명은 상기의 실시예에 한정되 는 것은 아니며, 본 발명이 속하는 분야에서 통상적인 지식을 가진 자라면 이러한 기재로부터 다양한 수정 및 변형이 가능하다. 따라서, 본 발명의 사상은 설명된 실시예에 국한되어 정해져서는 아니되며, 후술하는 특허청구범위뿐 아니라 이 특허청구범위와 균등하거나 등가적 변형이 있는 모든 것들은 본 발명 사상의 범주에 속한다고 할 것이다."}
{"patent_id": "10-2020-0047034", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 이미지 처리 장치의 블록도이다. 도 2는 본 발명의 실시예에 따른 이미지 처리 장치의 블록도이다. 도 3 내지 도 8은 이미지 처리 장치에서의 이미지 처리 경로를 설명하기 위한 도면이다. 도 9 내지 도 11은 이미지 처리 과정을 설명하기 위한 도면이다. 도 12는 본 발명의 일 실시예에 따른 이미지 처리 방법의 흐름도이다. 도 13은 본 발명의 다른 실시예에 따른 이미지 처리 방법의 흐름도이다."}
