{"patent_id": "10-2023-0003828", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0114196", "출원번호": "10-2023-0003828", "발명의 명칭": "감정 분석 방법 및 감정 분석 장치", "출원인": "주식회사 허니엠앤비", "발명자": "김석원"}}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "(a) 내담자 단말로부터 동영상데이터를 수신하는 단계;(b) 상기 동영상데이터로부터 영상데이터와 음성데이터를 추출하는 단계;(c) 상기 음성데이터를 기초로 텍스트데이터를 생성하는 단계; 및(d) 상기 영상데이터, 상기 음성데이터, 및 상기 텍스트데이터에 기초하여 앙상블 학습에 기반한 복수의 모델을이용해 내담자의 최종 감정 상태를 판단하는 단계;를 포함하는,감정 분석 방법."}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서, 상기 (a) 이전에,(e) 상기 복수의 모델을 생성하는 단계;를 더 포함하는,감정 분석 방법."}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서,상기 복수의 모델은 제1 모델, 제2 모델, 및 제3 모델을 포함하고,상기 (e)는,학습용 영상데이터에 기초하여 학습된 상기 제1 모델을 생성하는 단계; 학습용 음성데이터에 기초하여 학습된 상기 제2 모델을 생성하는 단계; 및학습용 텍스트데이터에 기초하여 학습된 상기 제3 모델을 생성하는 단계;를 포함하는,감정 분석 방법."}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3항에 있어서, 상기 (d)는, 상기 제1 모델의 제1 가중치, 상기 제2 모델의 제2 가중치, 및 상기 제3 모델의 제3 가중치를 참조해 상기 내담자의 최종 감정 상태를 결정하는 단계;를 포함하는, 감정 분석 방법."}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4항에 있어서,공개특허 10-2023-0114196-3-상기 (d)는,상기 영상데이터를 기초로 상기 제1 모델을 통해 제1 분류값을 획득하는 단계;상기 음성데이터를 기초로 상기 제2 모델을 통해 제2 분류값을 획득하는 단계; 상기 텍스트데이터를 기초로 상기 제3 모델을 통해 제3 분류값을 획득하는 단계; 및상기 제1 분류값에 상기 제1 가중치를 적용하고, 상기 제2 분류값에 상기 제2 가중치를 적용하며, 상기 제3 분류값에 상기 제3 가중치를 적용함으로써, 상기 내담자의 최종 감정 상태를 판단하는 단계;를 포함하는,감정 분석 방법."}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 3항에 있어서, 상기 제1 모델은 MobileNet에 기반한 모델이고,상기 제2 모델은 SVM에 기반한 모델이고,상기 제3 모델은 Bert에 기반한 모델인,감정 분석 방법."}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 2항에 있어서, 상기 복수의 모델은 제1 모델, 제2 모델, 제3 모델, 및 제4 모델을 포함하고,상기 (e)는,,학습용 영상데이터에 기초하여 학습된 상기 제1 모델을 생성하는 단계; 상기 학습용 영상데이터에 기초하여 학습된 상기 제2 모델을 생성하는 단계;학습용 음성데이터에 기초하여 학습된 상기 제3 모델을 생성하는 단계; 및학습용 텍스트데이터에 기초하여 학습된 상기 제4 모델을 생성하는 단계;를 포함하는,감정 분석 방법."}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7항에 있어서, 상기 (d)는, 상기 제1 모델의 제1 가중치, 상기 제2 모델의 제2 가중치, 상기 제3 모델의 제3 가중치, 및 상기 제4 모델의제4 가중치를 참조해 상기 내담자의 최종 감정 상태를 판단하는 단계;를 포함하는, 감정 분석 방법."}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8항에 있어서, 상기 (d)는,공개특허 10-2023-0114196-4-상기 영상데이터를 기초로 상기 제1 모델을 통해 제1 분류값을 획득하는 단계;상기 영상데이터를 기초로 상기 제2 모델을 통해 제2 분류값을 획득하는 단계;상기 음성데이터를 기초로 상기 제3 모델을 통해 제3 분류값을 획득하는 단계; 상기 텍스트데이터를 기초로 상기 제4 모델을 통해 제4 분류값을 획득하는 단계; 및상기 제1 분류값에 상기 제1 가중치를 적용하고, 상기 제2 분류값에 상기 제2 가중치를 적용하며, 상기 제3 분류값에 상기 제3 가중치를 적용하며, 상기 제4 분류값에 상기 제4 가중치를 적용함으로써, 상기 내담자의 최종감정 상태를 판단하는 단계;를 포함하는,감정 분석 방법."}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 7항에 있어서, 상기 제1 모델은 MobileNet에 기반한 모델이고,상기 제2 모델은 KNN(K-Nearest Neighbor)에 기반한 모델이고,상기 제3 모델은 SVM에 기반한 모델이고,상기 제4 모델은 Bert에 기반한 모델인,감정 분석 방법."}
{"patent_id": "10-2023-0003828", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "프로세서; 및상기 프로세서에 의해 실행 가능한 명령어들을 저장하는 메모리;를 포함하고, 상기 프로세서는, 상기 명령어들을 실행함으로써, 내담자 단말로부터 동영상데이터를 수신하고,상기 동영상데이터로부터 영상데이터와 음성데이터를 추출하며,상기 음성데이터를 기초로 텍스트데이터를 생성하고,상기 영상데이터, 상기 음성데이터, 및 상기 텍스트데이터에 기초하여 복수의 모델을 이용해 내담자의 최종 감정 상태를 판단하는,감정 분석 장치."}
{"patent_id": "10-2023-0003828", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "실시예에 따른 감정 분석 방법은, (a) 내담자 단말로부터 동영상데이터를 수신하는 단계; (b) 상기 동영상데이터로부터 영상데이터와 음성데이터를 추출하는 단계; (c) 상기 음성데이터를 기초로 텍스트데이터를 생성하는 단계; 및 (d) 상기 영상데이터, 상기 음성데이터, 및 상기 텍스트데이터에 기초하여 앙상블 학습에 기반한 복수의 모델을 이용해 내담자의 최종 감정 상태를 판단하는 단계;를 포함할 수 있다."}
{"patent_id": "10-2023-0003828", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 감정 분석 방법 및 감정 분석 장치에 관한 것으로, 보다 구체적으로, 사람의 감정과 연관된 복수의 전체 요소들-텍스트, 음성, 영상 모두에 기초하여 앙상블 학습에 기반한 인공지능모델을 통해 사람의 감정을 보 다 정확하게 분석할 수 있도록 하는 방법을 제공하고자 하는, 감정 분석 방법 및 감정 분석 장치에 관한 것이다."}
{"patent_id": "10-2023-0003828", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "심리 상담은 전문 지식을 갖춘 상담사가 심리적 문제를 지난 내담자와의 관계에서 공감적 이해, 무조건적 긍정 적 존중, 진실성을 기본으로 상담심리의 여러 이론들, 정신분석, 행동주의, 인본주의, 인지주의, 형태주의, 현 실요법, 교류분석, 가족치료 등의 내용을 이용하여 그들의 문제 해결을 돕는 치료 방법으로서 내담자가 인간의 사고, 감정, 행동, 대인관계에 대해 탐색하도록 안내하여 다양한 자신의 문제들을 이해하고 변화하도록 돕는 것 을 말한다. 이러한 심리 상담은 주로, 주어진 상담실에서 면대면으로 행해지던 전통적인 상담 방식으로 이루어졌는데, 대인 관계에서의 문제를 지니고 있거나 사회 불안이 심한 내담자나 환자들은 상담자와 직접 마주해야 하는 면대면 상 담 및 심리치료가 부담되거나 꺼려져 심리치료가 지속되지 못하고 중단되기도 한다. 그래서 면대면 상담의 대안으로 온라인 상에서 심리를 상담하고 치료하는 온라인 심리치료 프로그램이 제시되고 있다. 이에 기반하여 감정 평가 시스템이 등장하였으며, 예를 들어, 사람의 감정과 연관된 단일 요소에 기초하여, 사 람의 감정을 분석하도록 구성된 단일 모델이 등장하였다. 그러나, 종래의 단일 요소에 기초한 감정 분석 모델은 복잡한 사람의 감정을 정확하게 분석하고 평가하는 것에 여전히 한계가 있었다. 선행기술문헌 특허문헌 (특허문헌 0001) KR 2022-0005945 A"}
{"patent_id": "10-2023-0003828", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 사람의 감정과 연관된 복수의 전체 요소들-텍스트, 음성, 영상 모두에 기초하여 앙상블 학습에 기반 한 인공지능모델을 통해 사람의 감정을 보다 정확하게 분석할 수 있도록 하는 방법을 제공하고자 하는 데에 그 목적이 있다."}
{"patent_id": "10-2023-0003828", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "실시예에 따른 감정 분석 방법은, (a) 내담자 단말로부터 동영상데이터를 수신하는 단계; (b) 상기 동영상데이터로부터 영상데이터와 음성데이터를 추출하는 단계; (c) 상기 음성데이터를 기초로 텍스트데이터를 생성하는 단계; 및 (d) 상기 영상데이터, 상기 음성데이터, 및 상기 텍스트데이터에 기초하여 앙상블 학습에 기반한 복수의 모델을 이용해 내담자의 최종 감정 상태를 판단하는 단계;를 포함할 수 있다. 상기 (a) 이전에, (e) 상기 복수의 모델을 생성하는 단계;를 더 포함할 수 있다. 상기 복수의 모델은 제1 모델, 제2 모델, 및 제3 모델을 포함하고, (e) 상기 복수의 모델을 생성하는 단계는, 학습용 영상데이터에 기초하여 학습된 상기 제1 모델을 생성하는 단계; 학습용 음성데이터에 기초하여 학습된 상기 제2 모델을 생성하는 단계; 및 학습용 텍스트데이터에 기초하여 학습된 상기 제3 모델을 생성하는 단계;를 포함할 수 있다. 상기 (d)는, 상기 제1 모델의 제1 가중치, 상기 제2 모델의 제2 가중치, 및 상기 제3 모델의 제3 가중치를 참조해 상기 내담 자의 최종 감정 상태를 결정하는 단계;를 포함할 수 있다. 상기 (d)는, 상기 영상데이터를 기초로 상기 제1 모델을 통해 제1 분류값을 획득하는 단계; 상기 음성데이터를 기초로 상기 제2 모델을 통해 제2 분류값을 획득하는 단계; 상기 텍스트데이터를 기초로 상기 제3 모델을 통해 제3 분류값을 획득하는 단계; 및 상기 제1 분류값에 상기 제1 가중치를 적용하고, 상기 제2 분류값에 상기 제2 가중치를 적용하며, 상기 제3 분 류값에 상기 제3 가중치를 적용함으로써, 상기 내담자의 최종 감정 상태를 판단하는 단계;를 포함할 수 있다. 상기 제1 모델은 MobileNet에 기반한 모델이고, 상기 제2 모델은 SVM에 기반한 모델이고, 상기 제3 모델은 Bert에 기반한 모델일 수 있다. 실시예에 따라, 상기 복수의 모델은 제1 모델, 제2 모델, 제3 모델, 및 제4 모델을 포함하고, 학습용 영상데이터에 기초하여 학습된 상기 제1 모델을 생성하는 단계; 상기 학습용 영상데이터에 기초하여 학습된 상기 제2 모델을 생성하는 단계; 학습용 음성데이터에 기초하여 학습된 상기 제3 모델을 생성하는 단계; 및 학습용 텍스트데이터에 기초하여 학습된 상기 제4 모델을 생성하는 단계;를 포함할 수 있다. 상기 (d)는, 상기 제1 모델의 제1 가중치, 상기 제2 모델의 제2 가중치, 상기 제3 모델의 제3 가중치, 및 상기 제4 모델의 제4 가중치를 참조해 상기 내담자의 최종 감정 상태를 판단하는 단계;를 포함할 수 있다. 상기 (d)는, 상기 영상데이터를 기초로 상기 제1 모델을 통해 제1 분류값을 획득하는 단계; 상기 영상데이터를 기초로 상기 제2 모델을 통해 제2 분류값을 획득하는 단계; 상기 음성데이터를 기초로 상기 제3 모델을 통해 제3 분류값을 획득하는 단계; 상기 텍스트데이터를 기초로 상기 제4 모델을 통해 제4 분류값을 획득하는 단계; 및 상기 제1 분류값에 상기 제1 가중치를 적용하고, 상기 제2 분류값에 상기 제2 가중치를 적용하며, 상기 제3 분 류값에 상기 제3 가중치를 적용하며, 상기 제4 분류값에 상기 제4 가중치를 적용함으로써, 상기 내담자의 최종 감정 상태를 판단하는 단계;를 포함할 수 있다. 상기 제1 모델은 MobileNet에 기반한 모델이고, 상기 제2 모델은 KNN(K-Nearest Neighbor)에 기반한 모델이고, 상기 제3 모델은 SVM에 기반한 모델이고, 상기 제4 모델은 Bert에 기반한 모델일 수 있다. 실시예에 따른 감정 분석 장치는, 프로세서; 및 상기 프로세서에 의해 실행 가능한 명령어들을 저장하는 메모리;를 포함하고, 상기 프로세서는, 상기 명령어들을 실행함으로써, 내담자 단말로부터 동영상데이터를 수신하고, 상기 동영상데이터로부터 영상데이터와 음성데이터를 추출하며, 상기 음성데이터를 기초로 텍스트데이터를 생성하고, 상기 영상데이터, 상기 음성데이터, 및 상기 텍스트데이터에 기초하여 복수의 모델을 이용해 내담자의 최종 감 정 상태를 판단할 수 있다."}
{"patent_id": "10-2023-0003828", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 복수의 모델 각각을 통해 출력된 결과치를 활용함으로써 보다 정확하고 정밀하게 대상체의 감정을 분석하고 평가할 수 있다. 특히, 복수의 모델 각각을 통해 출력된 결과치에 대해 각 모델의 가중치를 적용한 결과를 활용함으로써, 각 모 델의 정확도 및 성능까지 감안된 보다 우수한 성능의 대상체의 감정 분석 결과 제공 시스템이 제공될 수 있다. 본 발명에 따르면, 사람의 감정과 연관된 복수의 전체 요소들-텍스트, 음성, 및 영상 모두가 함께 연관되어 분 석됨으로써, 보다 정밀도가 높은 분석 결과를 획득할 수 있다."}
{"patent_id": "10-2023-0003828", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "후술하는 본 발명에 대한 상세한 설명은, 본 발명이 실시될 수 있는 특정 실시예를 예시로서 도시하는 첨부 도 면을 참조한다. 이들 실시예는 당업자가 본 발명을 실시할 수 있기에 충분하도록 상세히 설명된다. 본 발명의 다양한 실시예는 서로 다르지만 상호 배타적일 필요는 없음이 이해되어야 한다. 예를 들어, 여기에 기재되어 있는 특정 형상, 구조 및 특성은 일 실시예에 관련하여 본 발명의 정신 및 범위를 벗어나지 않으면서 다른 실시 예로 구현될 수 있다. 또한, 각각의 개시된 실시예 내의 개별 구성요소의 위치 또는 배치는 본 발명의 정신 및 범위를 벗어나지 않으면서 변경될 수 있음이 이해되어야 한다. 따라서, 후술하는 상세한 설명은 한정적인 의미 로서 취하려는 것이 아니며, 본 발명의 범위는, 적절하게 설명된다면, 그 청구항들이 주장하는 것과 균등한 모 든 범위와 더불어 첨부된 청구항에 의해서만 한정된다. 도면에서 유사한 참조부호는 여러 측면에 걸쳐서 동일하 거나 유사한 기능을 지칭한다. 도 1은 실시예에 따른 감정 분석 결과 제공 시스템의 시스템도이다. 실시예에 따른 감정 분석 결과 제공 시스템은 내담자 단말, 감정 분석 결과 제공 장치, 상담자 단말 사이의 데이터 송수신으로 구성될 수 있다. 내담자 단말은 텍스트/음성/영상 상담을 진행하기 위한 어플리케이션을 구비하여 감정 분석 결과 제공 장치 에 연결될 수 있고, 이를 통해 상담자 단말과 텍스트/음성/영상 상담을 진행할 수 있다. 내담자 단말 은 상담자 모습이 출력되는 화면과, 내담자의 모습을 촬영하여 상담자 단말로 전송할 수 있는 카메라 (미도시)와 음성을 수집하여 상담자와 대화를 주고받을 수 있도록 하는 마이크(미도시) 및 스피커(미도시)가 적어도 구비되어야 한다. 내담자 단말은 카메라(미도시)를 이용해 내담자의 얼굴이 촬영되어 획득된 영상데이터와 마이크(미도시)를 이용해 수집된 내담자의 음성데이터를 포함한 동영상데이터를 실시간 감정 분석 결과 제공 장치로 전송할 수 있다. 실시예에 따라, 내담자 단말은 감정 분석 결과 제공 장치로부터 수신한 감정 분석 결과를 사용자 인터페 이스부(미도시)의 화면을 통해 출력하여 내담자에게 제공할 수 있다. 상담자 단말은 텍스트/음성/영상 상담을 진행하기 위한 어플리케이션을 구비하여 감정 분석 결과 제공 장치 에 연결될 수 있고, 이를 통해 내담자 단말과 텍스트/음성/영상 상담을 진행할 수 있다. 상담자 단말 은 내담자 모습이 출력되는 화면과, 상담자의 모습을 촬영하여 내담자 단말로 전송할 수 있는 카메라와 음성을 수집하여 내담자와 대화를 주고받을 수 있도록 하는 마이크 및 스피커가 적어도 구비되어야 한다. 상담자 단말은 카메라(미도시)를 이용해 상담자의 상담 얼굴이 촬영되어 획득된 영상데이터와 마이크(미도 시)를 이용해 수집된 상담자의 음성데이터를 포함한 동영상데이터를 실시간 감정 분석 결과 제공 장치로 전 송할 수 있다. 상담자 단말은 감정 분석 결과 제공 장치로부터 수신한 감정 분석 결과를 사용자 인터페이스부(미도 시)의 화면을 통해 출력하여 상담자 또는 관리자(사용자)에게 제공할 수 있다. 감정 분석 결과 제공 장치는 내담자 단말과 상담자 단말의 화상 상담을 서로 중개하고, 화상 상담 에 따른 내담자의 감정 분석 결과를 내담자 단말 및/또는 상담자 단말로 전송할 수 있다. 감정 분석 결과 제공 장치는 내담자 단말로부터 동영상데이터를 수신하여, 동영상데이터로부터 영상데 이터와 음성데이터를 추출하고, 상기 음성데이터를 기초로 텍스트데이터를 생성하며, 상기 영상데이터, 상기 음 성데이터, 및 상기 텍스트데이터에 기초하여 앙상블 학습에 기반한 복수의 모델을 이용해 내담자의 최종 감정 상태를 판단함으로써 감정 분석 결과로 획득하여 내담자 단말 및/또는 상담자 단말로 전송할 수 있다. 도 3은 실시 형태 1 및 실시 형태 2에 적용되는 감정 분석 결과 제공 장치의 동작을 설명하는 순서도이다. 참고로, 본 발명의 각 순서도에 있어서, 각 단계는 일예이며, 각 순서를 다르게 변경 및/또는 조합한 경우에도 본 발명이 동일/유사하게 적용될 수 있다. 프로세서는 내담자 단말로부터 동영상데이터를 수신할 수 있다(s11). 동영상데이터는 내담자의 얼굴이 촬영된 영상데이터 및 음성데이터를 포함할 수 있다. 프로세서는 동영상데이터로부터 영상데이터와 음성데이터를 추출할 수 있다(s12). 프로세서는 실시간 수신된 동영상데이터로부터 영상데이터와 음성데이터를 각각 추출하여 실시간 영상데이 터(영상프레임 또는 이미지프레임)와 실시간 음성데이터를 획득할 수 있다. 실시예에 따라, 동영상데이터로부터 영상데이터 및 음성데이터를 각각 추출하는 것은 공지의 다양한 알고리즘을 통해 구현될 수 있다. 프로세서는 내담자의 음성데이터를 기초로 텍스트데이터를 생성할 수 있다(s13). 프로세서는 음성데이터에 대해 전처리 단계로 잡음을 제거하고, 너무 크거나 작은 신호에 대해 적절한 크기 로 조절하고, 연속된 발화 중에서 음성 구간만을 검출하며, 최종 검출된 음성 구간의 음성을 STT(speech to text) 기술을 이용해 텍스트데이터로 변환할 수 있다. 프로세서는 내담자의 영상데이터, 음성데이터, 및 텍스트데이터에 기초하여 복수의 모델을 이용해 내담자의 최종 감정 상태를 판단할 수 있다(s14). 실시예에 따라, 프로세서는 내담자의 영상데이터, 음성데이터, 및 텍스트데이터 각각에 대응되는 각 모델을 통해 내담자의 감정 상태를 나타내는 각 분류값을 획득할 수 있고, 각 분류값에 소정의 가중치를 적용한 결과를 기초로 내담자의 최종 감정 상태를 판단할 수 있다. [실시 형태 1] 도 2는 실시 형태 1에 따른 감정 분석 결과 제공 장치의 블록도이다. 실시예에 따르면, 감정 분석 결과 제공 장치는 프로세서 및 메모리를 포함할 수 있다. 프로세서는 감정 분석 결과 제공 장치의 전반적인 동작을 제어할 수 있다. 메모리는 감정 분석 결과 제공 장치의 동작에 필요한 각종 프로그램 및 데이터 등을 저장할 수 있다. 메모리는 학습용 영상데이터에 대한 학습으로 생성된 제1 모델(22a), 학습용 음성데이터에 대한 학습으로 생성된 제2 모델(22b), 및 학습용 텍스트데이터에 대한 학습으로 생성된 제3 모델(22c)을 저장할 수 있다. 도 4 내지 도 5는 실시 형태 1에 따른 감정 분석 결과 제공 장치의 동작을 설명하는 데 참조되는 도면이다. 프로세서는 내담자의 영상데이터를 기초로 제1 모델을 통해 내담자의 감정 상태를 나타내는 제1 분류값을 획득할 수 있다. 실시예에 따라, 프로세서는 영상데이터를 기초로 MobileNet 모델을 통해 내담자의 감정 상태를 나타내는 제 1 분류값을 획득할 수 있다. 실시예에 따라, 제1 분류값은 복수의 감정 상태 각각의 제1 확률값을 포함할 수 있다. 실시예에 따라, 복수의 감정 상태는 기쁨, 분노, 경멸, 놀람, 두려움, 평온 및 슬픔 중 적어도 둘 이상을 포함 할 수 있으나, 본 발명은 이에 한정되지 않는다. 프로세서는 내담자의 음성데이터를 기초로 제2 모델을 통해 내담자의 감정 상태를 나타내는 제2 분류값을 획득할 수 있다. 실시예에 따라, 프로세서는 음성데이터를 기초로 SVM 알고리즘 기반의 제2 모델을 통해 통해 내담자의 감정 상태를 나타내는 제2 분류값을 획득할 수 있다. 실시예에 따라, 제2 분류값은 복수의 감정 상태 각각의 제2 확률값을 포함할 수 있다. 실시예에 따라, 복수의 감정 상태는 기쁨, 분노, 경멸, 놀람, 두려움, 평온 및 슬픔 중 적어도 둘 이상을 포함 할 수 있으나, 본 발명은 이에 한정되지 않는다. 프로세서는 텍스트데이터를 기초로 제3 모델을 통해 내담자의 감정 상태를 나타내는 제3 분류값을 획득할 수 있다. 실시예에 따라, 프로세서는 텍스트데이터를 기초로 BERT 모델을 통해 제3 분류값을 획득할 수 있다. 실시예에 따라, 제3 분류값은 복수의 감정 상태 각각의 제3 확률값을 포함할 수 있다. 실시예에 따라, 복수의 감정 상태는 기쁨, 분노, 경멸, 놀람, 두려움, 평온 및 슬픔 중 적어도 둘 이상을 포함 할 수 있으나, 본 발명은 이에 한정되지 않는다. 실시예에 따라, 프로세서는 내담자의 영상데이터, 음성데이터, 및 텍스트데이터에 기초하여 복수의 모델 각 각의 가중치를 이용해 내담자의 최종 감정 상태를 판단할 수 있다. 실시예에 따라, 도 5를 참조하면, 프로세서는 내담자의 영상데이터를 기초로 제1 모델을 통해 획득된 제1 분류값에 제1 가중치(a)를 적용하고, 음성데이터를 기초로 제2 모델을 통해 획득된 제2 분류값에 제2 가중치 (b)를 적용하며, 텍스트데이터를 기초로 제3 모델을 통해 획득된 제3 분류값에 제3 가중치(c)를 적용함으로써, 상기 내담자의 최종 감정 상태를 결정할 수 있다. 구체적으로, 프로세서는 복수의 감정 상태 각각의 제1 확률값에 제1 가중치를 각각 적용한 복수의 제1 결과, 상기 복수의 감정 상태 각각의 제2 확률값에 제2 가중치를 각각 적용한 복수의 제2 결과, 및 상기 복수의 감정 상태 각각의 제3 확률값에 제3 가중치를 각각 적용한 복수의 제3 결과를 각각 획득하고, 상기 복수의 감정 상태별로 제1 결과, 제2 결과 및 제3 결과를 각각 합산하고, 합산 결과가 가장 큰 수치를 나타내는 감정 상태를 상기 내담자의 최종 감정 상태로 판단할 수 있다. 예를 들어, 기쁨의 제1 확률값과 분노의 제1 확률값이 각각 [0.7,0.3]이고, 기쁨의 제2 확률값과 분노의 제2 확 률값이 각각 [0.8,0.2]이며, 기쁨의 제3 확률값과 분노의 제3 확률값이 각각 [0.2,0.8]이며, 제1 모델의 가중치 (a)가 0.3, 제2 모델의 가중치(b)가 0.5, 제3 모델의 가중치(c)가 0.2인 경우, 기쁨에 대해 합산된 확률값은 0.7*0.3 + 0.8*0.5 + 0.2*0.2 = 0.65이고, 분노에 대해 합산된 확률값은 0.3*0.3 + 0.2*0.5 + 0.8*0.2 = 0.35인 상태에서, 0.65가 0.35보다 크므로, 내담자의 최종 감정 상태는 기쁨인 것으로 결정될 수 있다. 도 4는 실시예에 따른 감정 분석 결과 제공 장치의 학습된 모델의 생성 과정을 설명하기 위한 순서도이다. 프로세서는 복수의 모델을 각각 미리 생성하여 메모리에 저장할 수 있다(s15). 실시예에 따라 복수의 모델은 학습용 영상데이터, 학습용 음성데이터, 및 학습용 텍스트데이터 각각을 학습에 적용한 앙상블 기반의 모델들로 생성될 수 있다. 실시예에 따라, 복수의 모델은 기계학습과 딥러닝이 앙상블되어 생성될 수 있다. 실시예에 따라, 앙상블 기반의 모델들은, 학습용 영상데이터에 대한 학습으로 생성된 제1 모델, 학습용 음성데 이터에 대한 학습으로 생성된 제2 모델, 및 학습용 텍스트데이터에 대한 학습으로 생성된 제3 모델 각각으로부 터 출력된 예측값(분류값)을 가공함으로써 최종예측을 수행하도록 구현될 수 있다. 구체적으로, 프로세서는 학습용 영상데이터를 기초로 학습된 제1 모델(22a)를 획득하여 메모리에 저장 할 수 있다(s151). 실시예에 따라 제1 모델(22a)은 CNN(Convolutional Neural Network) 알고리즘 기반의 MobileNet 모델일 수 있 으나, DNN(Deep Neural Network), DCNN(DeepConvolution Neural Network), RNN (Recurrent Neural Network), KNN(K-Nearest Neighbor), SVM(Support Vector Machine), Random Forest, Decision Tree 등의 알고리즘에 기반한 학습으로 생성될 수도 있다. MobileNet 아키텍처는 처리 동작들(즉, 부동 소수점 연산들, 곱셈들 및/또는 덧셈들 등)을 최소화하기 위해 뎁 스와이즈 분리가능 컨볼루션들(인수분해된 컨볼루션들의 형태)을 채택한다. 뎁스와이즈 분리가능 컨볼루션들은, 요구되는 동작들의 수를 감소 또는 최소화함으로써 처리를 더 고속화하기 위한 관점에서, 표준 컨볼루션을 뎁스 와이즈 컨볼루션 및 1 x 1 컨볼루션(또한 \"포인트와이즈 컨볼루션\"으로 지칭됨)으로 인수분해한다(예를 들어, 그것의 함수들을 토해 낸다). 뎁스와이즈 컨볼루션은 각각의 입력 채널에 단일 필터를 적용한다. 이어서, 포인 트와이즈 컨볼루션은 뎁스와이즈 컨볼루션의 출력들을 조합하기 위해 1 x 1 컨볼루션을 적용하고, 필터링 및 조 합 기능들/동작들을, 표준 컨볼루션들에 의해 수행되는 단일 필터링 및 조합 동작이 아니라 2개의 단계로 분리 한다. 따라서 MobileNet 아키텍처에서의 구조들은, \"계층 그룹\"을 정의하거나 예시하기 위해 구조 당 2개의 컨 볼루션 계층, 1개의 뎁스와이즈 컨볼루션 계층 및 1개의 포인트와이즈 컨볼루션 계층을 포함할 수 있다. 실시예에 따라, 프로세서는 학습용 영상데이터를 CNN을 통한 신경망 학습을 통해 학습용 영상데이터에 대한 감정이 분류되도록 함으로써, 제1 모델(22a)을 생성할 수 있다. 프로세서는 학습용 음성데이터를 기초로 학습된 제2 모델(22b)를 획득하여 메모리에 저장할 수 있다 (s152). 실시예에 따라 제2 모델(22b)은 SVM(Support Vector Machine) 알고리즘 기반의 모델일 수 있으나, CNN, DNN(Deep Neural Network), DCNN(DeepConvolution Neural Network), RNN (Recurrent Neural Network), KNN(K-Nearest Neighbor), Random Forest, Decision Tree 등의 알고리즘에 기반한 학습으로 생성될 수도 있 다. SVM(Support Vector Machine) 알고리즘은 분류와 회귀 분석을 위해 사용되며, 두 카테고리 중 어느 하나에 속한 데이터의 집합이 주어졌을 때, SVM 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어느 카테 고리에 속할지 판단하는 비확률적 이진 선형 분류 모델을 만든다. 실시예에 따라, 프로세서는 학습용 음성데이터의 주파수 분석을 기반으로 음성 특징 벡터를 추출하고, 추출 된 음성 특징 벡터를 SVM(Support Vector Machine) 알고리즘을 통해 학습하며, 학습 과정에서 음성 특징 벡터들 이 감정별로 분류되도록 함으로써, 제2 모델(22b)을 생성할 수 있다. 프로세서는 학습용 텍스트데이터를 기초로 학습된 제3 모델(22c)를 획득하여 메모리에 저장할 수 있다 (s153). 실시예에 따라, 제3 모델(22c)은 BERT(Bidirectional Encoder Representations from Transformers) 모델일 수 있으나, DNN(Deep Neural Network), DCNN(DeepConvolution Neural Network), CNN, RNN (Recurrent Neural Network), KNN(K-Nearest Neighbor), SVM(Support Vector Machine), Random Forest, Decision Tree 등의 알고리즘에 기반한 학습으로 생성될 수도 있다.BERT는 인코더-디코더 구조의 트랜스포머(Transformer) 아키텍쳐를 기반으로 한 인공지능 모델로서, 입력의 심 층 표현(Representation)을 위해 복수의 트랜스포머 계층을 쌓고, 토큰 시퀀스인 마스킹 언어 모델(Masking Language Model)에 마스킹 과정을 적용하는 것을 특징으로 한다. BERT 모델은 파인 튜닝 과정을 거침으로써 적 은 양의 데이터에서도 높은 정확도를 나타내며, 특정 벡터에 주목하게 만들어 성능을 향상시키는 어텐션 기반 모델로 문장이 길어져도 성능이 떨어지지 않아 긴 문장에서도 정확도를 유지할 수 있다는 장점이 있다. 실시예에 따라, 프로세서는 학습용 텍스트데이터를 기초로 컨텍스트 기반 임베딩값을 획득하기 위한 신경망 학습을 통해 학습용 텍스트데이터에 대한 감정이 분류되도록 함으로써, 제3 모델(22c)을 생성할 수 있다. [실시 형태2] 실시 형태 1에 기재된 내용은 실시 형태 2에 동일/유사하게 적용될 수 있다. 도 6은 실시예에 따른 감정 분석 결과 제공 장치의 블록도이다. 실시예에 따르면, 감정 분석 결과 제공 장치는 프로세서 및 메모리를 포함할 수 있다. 프로세서는 감정 분석 결과 제공 장치의 전반적인 동작을 제어할 수 있다. 메모리는 감정 분석 결과 제공 장치의 동작에 필요한 각종 프로그램 및 데이터 등을 저장할 수 있다. 메모리는 학습용 영상데이터에 대한 학습으로 생성된 제1 모델(22a), 상기 학습용 영상데이터에 대한 학습 으로 생성된 제2 모델(22b), 학습용 음성데이터에 대한 학습으로 생성된 제3 모델(22c), 학습용 텍스트데이터에 대한 학습으로 생성된 제4 모델(22d)을 저장할 수 있다. 도 7 내지 도 8은 실시 형태 2에 따른 감정 분석 결과 제공 장치의 동작을 설명하는 데 참조되는 도면이다. 프로세서는 내담자의 영상데이터를 기초로 제1 모델을 통해 내담자의 감정 상태를 나타내는 제1 분류값을 획득할 수 있다. 실시예에 따라, 프로세서는 제1 영상데이터를 기초로 MobileNet 모델을 통해 내담자의 감정 상태를 나타내 는 제1 분류값을 획득할 수 있다. 실시예에 따라, 제1 분류값은 복수의 감정 상태 각각의 제1 확률값을 포함할 수 있다. 실시예에 따라, 복수의 감정 상태는 기쁨, 분노, 경멸, 놀람, 두려움, 평온 및 슬픔 중 적어도 둘 이상을 포함 할 수 있으나, 본 발명은 이에 한정되지 않는다. 프로세서는 상기 내담자의 영상데이터를 기초로 제2 모델을 통해 내담자의 감정 상태를 나타내는 제2 분류 값을 획득할 수 있다. 실시예에 따라, 프로세서는 영상데이터를 기초로 KNN 모델을 통해 내담자의 감정 상태를 나타내는 제2 분류 값을 획득할 수 있다. 실시예에 따라, 제2 분류값은 복수의 감정 상태 각각의 제2 확률값을 포함할 수 있다. 실시예에 따라, 복수의 감정 상태는 기쁨, 분노, 경멸, 놀람, 두려움, 평온 및 슬픔 중 적어도 둘 이상을 포함 할 수 있으나, 본 발명은 이에 한정되지 않는다. 프로세서는 내담자의 음성데이터를 기초로 제3 모델을 통해 내담자의 감정 상태를 나타내는 제3 분류값을 획득할 수 있다. 실시예에 따라, 프로세서는 음성데이터를 기초로 SVM 알고리즘 기반의 제3 모델을 통해 통해 내담자의 감정 상태를 나타내는 제3 분류값을 획득할 수 있다. 실시예에 따라, 제3 분류값은 복수의 감정 상태 각각의 제3 확률값을 포함할 수 있다. 실시예에 따라, 복수의 감정 상태는 기쁨, 분노, 경멸, 놀람, 두려움, 평온 및 슬픔 중 적어도 둘 이상을 포함 할 수 있으나, 본 발명은 이에 한정되지 않는다. 프로세서는 텍스트데이터를 기초로 제4 모델을 통해 내담자의 감정 상태를 나타내는 제4 분류값을 획득할 수 있다.실시예에 따라, 프로세서는 텍스트데이터를 기초로 BERT 모델을 통해 제4 분류값을 획득할 수 있다. 실시예에 따라, 제4 분류값은 복수의 감정 상태 각각의 제4 확률값을 포함할 수 있다. 실시예에 따라, 복수의 감정 상태는 기쁨, 분노, 경멸, 놀람, 두려움, 평온 및 슬픔 중 적어도 둘 이상을 포함 할 수 있으나, 본 발명은 이에 한정되지 않는다. 실시예에 따라, 프로세서는 내담자의 영상데이터, 음성데이터, 및 텍스트데이터에 기초하여 복수의 모델 각 각의 가중치를 이용해 내담자의 최종 감정 상태를 판단할 수 있다. 실시예에 따라, 도 8을 참조하면, 프로세서는 내담자의 영상데이터를 기초로 제1 모델을 통해 획득된 제1 분류값에 제1 가중치(a)를 적용하고, 상기 동일한 영상데이터를 기초로 제2 모델을 통해 획득된 제2 분류값에 제2 가중치(b)를 적용하고, 음성데이터를 기초로 제3 모델을 통해 획득된 제3 분류값에 제3 가중치(c)를 적용하 며, 텍스트데이터를 기초로 제4 모델을 통해 획득된 제4 분류값에 제4 가중치(d)를 적용함으로써, 상기 내담자 의 최종 감정 상태를 결정할 수 있다. 구체적으로, 프로세서는 복수의 감정 상태 각각의 제1 확률값에 제1 가중치를 각각 적용한 복수의 제1 결과, 복수의 감정 상태 각각의 제2 확률값에 제2 가중치를 각각 적용한 복수의 제2 결과, 상기 복수의 감정 상 태 각각의 제3 확률값에 제3 가중치를 각각 적용한 복수의 제3 결과, 및 상기 복수의 감정 상태 각각의 제4 확 률값에 제4 가중치를 각각 적용한 복수의 제4 결과를 각각 획득하고, 상기 복수의 감정 상태별로 제1 결과, 제2 결과, 제3 결과 및 제4 결과를 각각 합산하고, 합산 결과가 가장 큰 수치를 나타내는 감정 상태를 상기 내담자 의 최종 감정 상태로 판단할 수 있다. 예를 들어, 기쁨의 제1 확률값과 분노의 제1 확률값이 각각 [0.7,0.3]이고, 기쁨의 제2 확률값과 분노의 제2 확 률값이 각각 [0.8,0.2]이고, 기쁨의 제3 확률값과 분노의 제3 확률값이 각각 [0.2,0.8]이며, 기쁨의 제4 확률값 과 분노의 제4 확률값이 각각 [0.9,0.1]이며, 제1 모델의 가중치(a)가 0.3, 제2 모델의 가중치(b)가 0.4, 제3 모델의 가중치(c)가 0.2, 제4 모델의 가중치(d)가 0.1인 경우, 기쁨에 대해 합산된 확률값은 0.7*0.3 + 0.8*0.4 + 0.2*0.2 + 0.9*0.1 = 0.66이고, 분노에 대해 합산된 확률값은 0.3*0.3 + 0.2*0.4 + 0.8*0.2 + 0.1*0.1= 0.34 인 상태에서, 0.66이 0.34보다 크므로, 내담자의 최종 감정 상태는 기쁨인 것으로 결정될 수 있다. 도 7은 실시예에 따른 감정 분석 결과 제공 장치의 학습된 모델의 생성 과정을 설명하기 위한 순서도이다. 프로세서는 복수의 모델을 각각 미리 생성하여 메모리에 저장할 수 있다(s25). 실시예에 따라 복수의 모델은 학습용 영상데이터, 학습용 음성데이터, 및 학습용 텍스트데이터 각각을 학습에 적용한 앙상블 기반의 모델들로 생성될 수 있다. 실시예에 따라, 복수의 모델은 기계학습과 딥러닝이 앙상블되어 생성될 수 있다. 실시예에 따라, 앙상블 기반의 모델들은, 학습용 영상데이터에 대한 학습으로 생성된 제1 모델, 상기 학습용 영 상데이터에 대한 학습으로 생성된 제2 모델, 학습용 음성데이터에 대한 학습으로 생성된 제3 모델, 및 학습용 텍스트데이터에 대한 학습으로 생성된 제4 모델 각각으로부터 출력된 예측값(분류값)을 가공함으로써 최종예측 을 수행하도록 구현될 수 있다. 구체적으로, 프로세서는 학습용 영상데이터를 기초로 학습된 제1 모델(22a)를 획득하여 메모리에 저장 할 수 있다(s251). 실시예에 따라 제1 모델(22a)은 CNN(Convolutional Neural Network) 알고리즘 기반의 MobileNet 모델일 수 있 으나, DNN(Deep Neural Network), DCNN(DeepConvolution Neural Network), RNN (Recurrent Neural Network), KNN(K-Nearest Neighbor), SVM(Support Vector Machine), Random Forest, Decision Tree 등의 알고리즘에 기반한 학습으로 생성될 수도 있다. MobileNet 아키텍처는 처리 동작들(즉, 부동 소수점 연산들, 곱셈들 및/또는 덧셈들 등)을 최소화하기 위해 뎁 스와이즈 분리가능 컨볼루션들(인수분해된 컨볼루션들의 형태)을 채택한다. 뎁스와이즈 분리가능 컨볼루션들은, 요구되는 동작들의 수를 감소 또는 최소화함으로써 처리를 더 고속화하기 위한 관점에서, 표준 컨볼루션을 뎁스 와이즈 컨볼루션 및 1 x 1 컨볼루션(또한 \"포인트와이즈 컨볼루션\"으로 지칭됨)으로 인수분해한다(예를 들어, 그것의 함수들을 토해 낸다). 뎁스와이즈 컨볼루션은 각각의 입력 채널에 단일 필터를 적용한다. 이어서, 포인 트와이즈 컨볼루션은 뎁스와이즈 컨볼루션의 출력들을 조합하기 위해 1 x 1 컨볼루션을 적용하고, 필터링 및 조 합 기능들/동작들을, 표준 컨볼루션들에 의해 수행되는 단일 필터링 및 조합 동작이 아니라 2개의 단계로 분리한다. 따라서 MobileNet 아키텍처에서의 구조들은, \"계층 그룹\"을 정의하거나 예시하기 위해 구조 당 2개의 컨 볼루션 계층, 1개의 뎁스와이즈 컨볼루션 계층 및 1개의 포인트와이즈 컨볼루션 계층을 포함할 수 있다. 실시예에 따라, 프로세서는 학습용 영상데이터를 CNN을 통한 신경망 학습을 통해 학습용 영상데이터에 대한 감정이 분류되도록 함으로써, 제2 모델(22a)을 생성할 수 있다. 프로세서는 동일한 학습용 영상데이터를 기초로 학습된 제2 모델(22b)를 획득하여 메모리에 저장할 수 있다(s252). 실시예에 따라, 제2 모델(22b)은 KNN(K-Nearest Neighbor) 알고리즘 기반의 모델일 수 있으나, DNN(Deep Neural Network), DCNN(DeepConvolution Neural Network), RNN (Recurrent Neural Network), KNN(K-Nearest Neighbor), SVM(Support Vector Machine), Random Forest, Decision Tree 등의 알고리즘에 기반한 학습으로 생성될 수도 있다. 실시예에 따라, 프로세서는 학습용 영상데이터로부터 픽셀 정보를 포함하는 특징 데이터를 추출하고, 추출 된 특징 데이터를 분류하기 위한 문자의 유니코드가 저장되어 있는 분류 데이터를 상기 특징 데이터와 함께 하 나의 파일로 저장하며, 상기 특징 데이터 및 분류 데이터에 기초하여 픽셀 비교 알고리즘인 KNN(K-Nearest Neighbor) 알고리즘을 학습시킴으로써 제2 모델(22b)을 생성할 수 있다. 실시예에 따라, 프로세서는 사용자가 지정한 K 상수에서 가장 가까운 분류명을 할당함으로써 추출된 특징 데이터를 분류할 수 있다. 프로세서는 학습용 음성데이터를 기초로 학습된 제3 모델(22b)를 획득하여 메모리에 저장할 수 있다 (s253). 실시예에 따라, 제3 모델(22c)은 SVM(Support Vector Machine) 알고리즘 기반의 모델일 수 있으나, CNN, DNN(Deep Neural Network), DCNN(DeepConvolution Neural Network), RNN (Recurrent Neural Network), KNN(K-Nearest Neighbor), Random Forest, Decision Tree 등의 알고리즘에 기반한 학습으로 생성될 수도 있 다. SVM(Support Vector Machine) 알고리즘은 분류와 회귀 분석을 위해 사용되며, 두 카테고리 중 어느 하나에 속한 데이터의 집합이 주어졌을 때, SVM 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어느 카테 고리에 속할지 판단하는 비확률적 이진 선형 분류 모델을 만든다. 실시예에 따라, 프로세서는 학습용 음성데이터의 주파수 분석을 기반으로 음성 특징 벡터를 추출하고, 추출 된 음성 특징 벡터를 SVM(Support Vector Machine) 알고리즘을 통해 학습하며, 학습 과정에서 음성 특징 벡터들 이 감정별로 분류되도록 함으로써, 제3 모델(22c)을 생성할 수 있다. 프로세서는 학습용 텍스트데이터를 기초로 학습된 제4 모델(22d)를 획득하여 메모리에 저장할 수 있다 (s254). 실시예에 따라, 제4 모델(22d)은 BERT(Bidirectional Encoder Representations from Transformers) 모델일 수 있으나, DNN(Deep Neural Network), DCNN(DeepConvolution Neural Network), CNN, RNN (Recurrent Neural Network), KNN(K-Nearest Neighbor), SVM(Support Vector Machine), Random Forest, Decision Tree 등의 알고리즘에 기반한 학습으로 생성될 수도 있다. BERT는 인코더-디코더 구조의 트랜스포머(Transformer) 아키텍쳐를 기반으로 한 인공지능 모델로서, 입력의 심 층 표현(Representation)을 위해 복수의 트랜스포머 계층을 쌓고, 토큰 시퀀스인 마스킹 언어 모델(Masking Language Model)에 마스킹 과정을 적용하는 것을 특징으로 한다. BERT 모델은 파인 튜닝 과정을 거침으로써 적 은 양의 데이터에서도 높은 정확도를 나타내며, 특정 벡터에 주목하게 만들어 성능을 향상시키는 어텐션 기반 모델로 문장이 길어져도 성능이 떨어지지 않아 긴 문장에서도 정확도를 유지할 수 있다는 장점이 있다. 실시예에 따라, 프로세서는 학습용 텍스트데이터를 기초로 컨텍스트 기반 임베딩값을 획득하기 위한 신경망 학습을 통해 학습용 텍스트데이터에 대한 감정이 분류되도록 함으로써, 제4 모델(22d)을 생성할 수 있다. 제2 실시 형태에 따르면, 동일한 학습용 영상데이터를 서로 다른 모델을 통한 학습을 수행함으로써 과적합 (overfitting)을 보다 줄일 수 있으며, 복수의 서로 다른 모델을 통해 결과를 획득할 수 있도록 함으로써, 전체 적으로 보다 정확한 결과값을 획득할 수 있게 된다. 특히, 하나의 동일한 학습용 영상데이터에 대해 머신 러닝 기반의 모델과 딥 러닝 기반의 모델을 앙상블 적용하여 감정을 분석할 수 있도록 함으로써, 머신 러닝의 장점및 딥 러닝의 장점을 함께 적용하여 결과를 획득할 수 있게 된다. 예를 들어, KNN을 통해 하이퍼파라미터의 수 를 적게 하여 구현이 보다 용이하면서도, Mobilenet을 통해 영상데이터 처리에 있어서 연산량을 줄이면서도 예 측 정확도를 보다 향상시킬 수 있게 된다. [가중치 관련 - 실시 형태 1 및 실시 형태 2 적용] 실시예에 따라, 각 모델마다 대응되는 소정의 가중치는 각 모델의 테스트 성능을 기초로 최초 결정될 수 있다. 실시예에 따라, 각 모델의 테스트 성능은 각 모델로부터 출력된 각 분류값의 레이블(감정 상태) 과의 유사도에 기초해 결정될 수 있다. 즉, 임의의 모델로부터 출력된 분류값이 레이블(감정 상태)과 유사도가 높은 값일 수록 해당 모델의 정확도가 높아(즉, 정확한 예측을 한 것으로 판단되어) 성능이 좋은 것으로 판단될 수 있다. 이 경 우, 각 모델의 성능은 상기 유사도에 기초해 상대적 수치로 산출될 수 있다. 예를 들어, 실시 형태 2의 예에서, 제1 모델이 80%의 정확도를 나타내는 성능을 보유하고, 제2 모델이 85%의 정 확도를 나타내는 성능을 보유하고, 제3 모델이 30%의 정확도를 나타내는 성능을 보유하며, 제4 모델이 20%의 정 확도를 나타내는 성능을 보유할 경우, 상대적 수치인 0.372/0.395/0.140/0.093을 각 모델의 초기 가중치로 결정 할 수 있다. 실시예에 따라, 각 모델의 변경된 가중치에 의해, 프로세서는 정확도를 재산출하고, 정확도가 가장 높게 나 타내는 시점의 가중치를 앙상블 모델에 적용할 수 있다. 또한, 가중치 변화의 폭은 각 모델의 정확도 범위를 기 준으로 산출될 수 있다. 예를 들어, 제1 모델의 정확도가 75% 내지 85% 사이이고, 나머지 정확도가 위에서 제시 한 정확도인 경우, 가중치 변화의 폭은, 제1모델: 0.357, 제2모델: 0.405, 제3모델: 0.143, 제4모델: 0.095 또 는, 제1모델: 0.386, 제2모델: 0.386, 제3모델: 0.136, 제4모델: 0.091 과 같이 산출될 수 있다. 한편, 위에서는 4개의 모델을 기준으로 설명하였으나, 3개의 모델인 경우에도 동일/유사하게 적용될 수 있다. 이상 설명된 실시 형태는 다양한 컴퓨터 구성요소를 통하여 실행될 수 있는 프로그램 명령어의 형태로 구현되어 컴퓨터로 판독가능한 기록매체에 기록될 수 있다. 상기 컴퓨터로 판독가능한 기록매체는 프로그램 명령어, 데이 터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터로 판독가능한 기록매체에 기록되는 프로그램 명령어는 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터로 판독가능한 기록매체의 예에는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD- ROM, DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 ROM, RAM, 플래시 메모리 등과 같은 프로그램 명령어를 저장하고 실행하도록 특별히 구성된 하드웨 어 장치가 포함된다. 프로그램 명령어의 예에는, 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니 라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함된다. 상기 하드웨어 장치 는 본 발명에 따른 처리를 실행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 본 명세서의 양상들은 전체적으로 하드웨어, 전체적으로 소프트웨어 (펌웨어, 상주 소프트웨어, 마이크로 코드 등을 포함 함) 또는 컴퓨터 판독 가능 프로그램 코드가 구현 된 하나 이상의 컴퓨터 판독 가능 매체에 구현 된 컴퓨터 프로그램 제품의 형태를 취할 수 있다. 이상에서 실시예들에 설명된 특징, 구조, 효과 등은 본 발명의 하나의 실시예에 포함되며, 반드시 하나의 실시 예에만 한정되는 것은 아니다. 나아가, 각 실시예에서 예시된 특징, 구조, 효과 등은 실시예들이 속하는 분야의 통상의 지식을 가지는 자에 의해 다른 실시예들에 대해서도 조합 또는 변형되어 실시 가능하다. 따라서 이러한 조합과 변형에 관계된 내용들은 본 발명의 범위에 포함되는 것으로 해석되어야 할 것이다. 또한, 이상에서 실시예를 중심으로 설명하였으나 이는 단지 예시일 뿐 본 발명을 한정하는 것이 아니며, 본 발 명이 속하는 분야의 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성을 벗어나지 않는 범위에서 이상에 예시되지 않은 여러 가지의 변형과 응용이 가능함을 알 수 있을 것이다. 예를 들어, 실시예에 구체적으로 나타 난 각 구성 요소는 변형하여 실시할 수 있는 것이다. 그리고 이러한 변형과 응용에 관계된 차이점들은 첨부된 청구 범위에서 규정하는 본 발명의 범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8"}
{"patent_id": "10-2023-0003828", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 실시예에 따른 감정 분석 결과 제공 시스템의 시스템도이다. 도 2는 실시예에 따른 감정 분석 결과 제공 장치의 블록도이다. 도 3은 실시예에 따른 감정 분석 결과 제공 장치의 동작을 설명하는 순서도이며, 도 5는 실시예에 따른 감 정 분석 결과 제공 장치의 동작을 설명하는 데 참조되는 도면이다. 도 4는 실시예에 따른 감정 분석 결과 제공 장치의 학습된 모델의 생성 과정을 설명하기 위한 순서도이다. 도 6은 실시예에 따른 감정 분석 결과 제공 장치의 블록도이다. 도 7은 실시예에 따른 감정 분석 결과 제공 장치의 학습된 모델의 생성 과정을 설명하기 위한 순서도이다. 도 8은 실시예에 따른 감정 분석 결과 제공 장치의 동작을 설명하는 데 참조되는 도면이다."}
