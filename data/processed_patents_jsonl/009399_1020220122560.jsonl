{"patent_id": "10-2022-0122560", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0043448", "출원번호": "10-2022-0122560", "발명의 명칭": "인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법, 이를 수행하는 장치 및 컴퓨터", "출원인": "연세대학교 산학협력단", "발명자": "김동현"}}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "복수개의 2D 슬라이스 영상을 포함하는 보정 대상 3D 영상인 환자의 3D 뇌 자기공명영상을 획득하는 단계; 및상기 보정 대상 3D 영상을 기반으로, 2D 적층형 U-Net(2D stacked U-net) 기반의 미리 학습되어 저장된 보정 모델을 이용하여, 상기 보정 대상 3D 영상에서 상기 환자의 움직임으로 인해 발생된 모션 아티팩트(motionartifact)를 보정한 모션 아티팩트-프리(motion artifact-free) 3D 영상을 획득하는 단계;를 포함하는 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서,상기 모션 아티팩트-프리 3D 영상 획득 단계는,상기 보정 대상 3D 영상의 하나의 2D 슬라이스 영상인 제1 2D 슬라이스 영상에 대해, 상기 제1 2D 슬라이스 영상, 상기 제1 2D 슬라이스 영상을 기준으로 제1 방향에 인접한 제2 2D 슬라이스 영상 및 상기 제1 2D 슬라이스영상을 기준으로 상기 제1 방향과 상이한 제2 방향에 인접한 제3 2D 슬라이스 영상을 상기 보정 모델에 입력하고, 상기 보정 모델의 출력을 통해 상기 제1 2D 슬라이스 영상에 대응되는 모션 아티팩트-프리 2D 슬라이스 영상을 획득하는 과정을, 상기 보정 대상 3D 영상의 각 2D 슬라이스 영상에 대해 수행하여 상기 모션 아티팩트-프리 3D 영상을 획득하는 것으로 이루어지는,인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에서,상기 보정 모델은,상기 제1 2D 슬라이스 영상, 상기 제2 2D 슬라이스 영상 및 상기 제3 2D 슬라이스 영상을 입력받고,상기 제1 2D 슬라이스 영상, 상기 제2 2D 슬라이스 영상 및 상기 제3 2D 슬라이스 영상 각각에서 특징 추출기(feature extractor)를 통해 특징을 추출하여 제1 특징, 제2 특징 및 제3 특징을 획득하며,상기 제1 특징, 상기 제2 특징 및 상기 제3 특징을 기반으로 인코더(encoder)와 디코더(decoder)를 포함하는 제1 U-Net을 통해 초기 예측 결과를 획득하고,상기 제1 특징, 상기 제2 특징, 상기 제3 특징 및 상기 초기 예측 결과를 기반으로 인코더와 디코더를 포함하는제2 U-Net을 통해 상기 모션 아티팩트-프리 2D 슬라이스 영상을 획득하며,상기 모션 아티팩트-프리 2D 슬라이스 영상을 출력하는,인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에서,상기 보정 모델은,상기 제1 특징, 상기 제2 특징 및 상기 제3 특징을 연결(concatenation)한 값을 기반으로 상기 제1 U-Net을 통해 상기 초기 예측 결과를 획득하고,상기 제1 특징, 상기 제2 특징, 상기 제3 특징 및 상기 초기 예측 결과를 연결한 값을 기반으로 상기 제2 U-Net을 통해 상기 모션 아티팩트-프리 2D 슬라이스 영상을 획득하는,공개특허 10-2024-0043448-3-인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에서,상기 제1 U-Net과 상기 제2 U-Net은,컨볼루션 블럭 어텐션 모듈(convolutional block attention module, CBAM)이 적용되는,인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에서,상기 컨볼루션 블럭 어텐션 모듈(CBAM)은,채널 방향(channel direction)과 공간 방향(spatial direction)을 따라 2개의 순차적 어텐션 맵(sequentialattention maps)을 통합하는,인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제3항에서,상기 보정 모델은,복수개의 모션 아티팩트 2D 슬라이스 영상을 포함하는 모션 아티팩트 3D 뇌 자기공명영상 및 복수개의 상기 모션 아티팩트 2D 슬라이스 영상 각각에 대응되는 복수개의 모션 아티팩트-프리 2D 슬라이스 영상을 포함하는 모션 아티팩트-프리 3D 뇌 자기공명영상을 포함하는 학습 데이터를 이용하여 학습되는,인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에서,상기 보정 모델은,상기 모션 아티팩트 2D 슬라이스 영상으로부터 상기 보정 모델을 통해 획득된 모션 아티팩트-보정 2D 슬라이스영상 및 상기 모션 아티팩트-프리 2D 슬라이스 영상을 기반으로, 구조적 유사도 인덱스 맵(structuralsimilarity index map, SSIM)을 손실 함수(loss function)로 이용하여 구조적 유사도 인덱스 맵(SSIM)이 최소화되도록 학습되는,인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제8항 중 어느 한 항에 기재된 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법을 컴퓨터에서 실행시키기 위하여 컴퓨터 판독 가능한 저장 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "인공신경망을 기반으로 뇌 자기공명영상의 모션 아티팩트(motion artifact)를 보정하는 보정 장치로서,인공신경망을 기반으로 뇌 자기공명영상의 모션 아티팩트를 보정하기 위한 하나 이상의 프로그램을 저장하는 메모리; 및상기 메모리에 저장된 상기 하나 이상의 프로그램에 따라 인공신경망을 기반으로 뇌 자기공명영상의 모션 아티팩트를 보정하기 위한 동작을 수행하는 하나 이상의 프로세서;를 포함하며,공개특허 10-2024-0043448-4-상기 프로세서는,복수개의 2D 슬라이스 영상을 포함하는 보정 대상 3D 영상인 환자의 3D 뇌 자기공명영상을 획득하고,상기 보정 대상 3D 영상을 기반으로, 2D 적층형 U-Net(2D stacked U-net) 기반의 미리 학습되어 저장된 보정 모델을 이용하여, 상기 보정 대상 3D 영상에서 상기 환자의 움직임으로 인해 발생된 모션 아티팩트를 보정한 모션아티팩트-프리(motion artifact-free) 3D 영상을 획득하는,인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 장치."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에서,상기 프로세서는,상기 보정 대상 3D 영상의 하나의 2D 슬라이스 영상인 제1 2D 슬라이스 영상에 대해, 상기 제1 2D 슬라이스 영상, 상기 제1 2D 슬라이스 영상을 기준으로 제1 방향에 인접한 제2 2D 슬라이스 영상 및 상기 제1 2D 슬라이스영상을 기준으로 상기 제1 방향과 상이한 제2 방향에 인접한 제3 2D 슬라이스 영상을 상기 보정 모델에 입력하고, 상기 보정 모델의 출력을 통해 상기 제1 2D 슬라이스 영상에 대응되는 모션 아티팩트-프리 2D 슬라이스 영상을 획득하는 과정을, 상기 보정 대상 3D 영상의 각 2D 슬라이스 영상에 대해 수행하여 상기 모션 아티팩트-프리 3D 영상을 획득하는,인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 장치."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에서,상기 보정 모델은,상기 제1 2D 슬라이스 영상, 상기 제2 2D 슬라이스 영상 및 상기 제3 2D 슬라이스 영상을 입력받고,상기 제1 2D 슬라이스 영상, 상기 제2 2D 슬라이스 영상 및 상기 제3 2D 슬라이스 영상 각각에서 특징 추출기(feature extractor)를 통해 특징을 추출하여 제1 특징, 제2 특징 및 제3 특징을 획득하며,상기 제1 특징, 상기 제2 특징 및 상기 제3 특징을 기반으로 인코더(encoder)와 디코더(decoder)를 포함하는 제1 U-Net을 통해 초기 예측 결과를 획득하고,상기 제1 특징, 상기 제2 특징, 상기 제3 특징 및 상기 초기 예측 결과를 기반으로 인코더와 디코더를 포함하는제2 U-Net을 통해 상기 모션 아티팩트-프리 2D 슬라이스 영상을 획득하며,상기 모션 아티팩트-프리 2D 슬라이스 영상을 출력하는,인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 장치."}
{"patent_id": "10-2022-0122560", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에서,상기 보정 모델은,상기 제1 특징, 상기 제2 특징 및 상기 제3 특징을 연결(concatenation)한 값을 기반으로 상기 제1 U-Net을 통해 상기 초기 예측 결과를 획득하고,상기 제1 특징, 상기 제2 특징, 상기 제3 특징 및 상기 초기 예측 결과를 연결한 값을 기반으로 상기 제2 U-Net을 통해 상기 모션 아티팩트-프리 2D 슬라이스 영상을 획득하는,인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 장치."}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법, 이를 수행하는 장치 및 컴퓨터 프로그램은, 인공신경망을 기반으로 뇌 자기공명영상에서 환자의 움직임으로 인해 발생되는 모션 아티팩트(motion artifact)를 보정함으로써, 촬영 대상의 움직임으로 인한 모션 아티팩트의 보정을 위해 딥러닝 알고리즘을 적용하기 위한 유용한 지침을 제공할 수 있고, 다중 슬라이스에서 얻을 수 있는 사전 지식 정보를 통 해 추가적인 데이터 획득과 높은 계산 비용이 필요한 다중 대비 영상을 사용하지 않고도 움직임에 의한 모션 아 티팩트를 보정할 수 있으며, 움직임에 의한 모션 아티팩트를 개선하는데 있어서 가장 중요한 구조적인 정보들을 보존하여 신뢰성 있는 모션 아티팩트 보정 방법을 제공할 수 있다."}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법, 이를 수행하는 장치 및 컴퓨터 프로그 램에 관한 것으로서, 더욱 상세하게는 뇌 자기공명영상의 아티팩트(artifact)를 보정하는, 방법, 장치 및 컴퓨 터 프로그램에 관한 것이다."}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "3D 뇌 자기공명영상(magnetic resonance imaging, MRI) 획득 시 환자의 움직임으로 인해 3D 뇌 자기공명영상에 모션 아티팩트(motion artifact)가 발생되게 된다."}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 이루고자 하는 목적은, 인공신경망을 기반으로 뇌 자기공명영상에서 환자의 움직임으로 인해 발생되 는 모션 아티팩트(motion artifact)를 보정하는, 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법, 이를 수행하는 장치 및 컴퓨터 프로그램을 제공하는 데 있다. 본 발명의 명시되지 않은 또 다른 목적들은 하기의 상세한 설명 및 그 효과로부터 용이하게 추론할 수 있는 범 위 내에서 추가적으로 고려될 수 있다."}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기의 기술적 과제를 달성하기 위한 본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모 션 아티팩트 보정 방법은, 복수개의 2D 슬라이스 영상을 포함하는 보정 대상 3D 영상인 환자의 3D 뇌 자기공명 영상을 획득하는 단계; 및 상기 보정 대상 3D 영상을 기반으로, 2D 적층형 U-Net(2D stacked U-net) 기반의 미 리 학습되어 저장된 보정 모델을 이용하여, 상기 보정 대상 3D 영상에서 상기 환자의 움직임으로 인해 발생된 모션 아티팩트(motion artifact)를 보정한 모션 아티팩트-프리(motion artifact-free) 3D 영상을 획득하는 단 계;를 포함한다. 여기서, 상기 모션 아티팩트-프리 3D 영상 획득 단계는, 상기 보정 대상 3D 영상의 하나의 2D 슬라이스 영상인 제1 2D 슬라이스 영상에 대해, 상기 제1 2D 슬라이스 영상, 상기 제1 2D 슬라이스 영상을 기준으로 제1 방향에 인접한 제2 2D 슬라이스 영상 및 상기 제1 2D 슬라이스 영상을 기준으로 상기 제1 방향과 상이한 제2 방향에 인 접한 제3 2D 슬라이스 영상을 상기 보정 모델에 입력하고, 상기 보정 모델의 출력을 통해 상기 제1 2D 슬라이스 영상에 대응되는 모션 아티팩트-프리 2D 슬라이스 영상을 획득하는 과정을, 상기 보정 대상 3D 영상의 각 2D 슬 라이스 영상에 대해 수행하여 상기 모션 아티팩트-프리 3D 영상을 획득하는 것으로 이루어질 수 있다. 여기서, 상기 보정 모델은, 상기 제1 2D 슬라이스 영상, 상기 제2 2D 슬라이스 영상 및 상기 제3 2D 슬라이스 영상을 입력받고, 상기 제1 2D 슬라이스 영상, 상기 제2 2D 슬라이스 영상 및 상기 제3 2D 슬라이스 영상 각각 에서 특징 추출기(feature extractor)를 통해 특징을 추출하여 제1 특징, 제2 특징 및 제3 특징을 획득하며, 상 기 제1 특징, 상기 제2 특징 및 상기 제3 특징을 기반으로 인코더(encoder)와 디코더(decoder)를 포함하는 제1 U-Net을 통해 초기 예측 결과를 획득하고, 상기 제1 특징, 상기 제2 특징, 상기 제3 특징 및 상기 초기 예측 결 과를 기반으로 인코더와 디코더를 포함하는 제2 U-Net을 통해 상기 모션 아티팩트-프리 2D 슬라이스 영상을 획 득하며, 상기 모션 아티팩트-프리 2D 슬라이스 영상을 출력할 수 있다. 여기서, 상기 보정 모델은, 상기 제1 특징, 상기 제2 특징 및 상기 제3 특징을 연결(concatenation)한 값을 기 반으로 상기 제1 U-Net을 통해 상기 초기 예측 결과를 획득하고, 상기 제1 특징, 상기 제2 특징, 상기 제3 특징 및 상기 초기 예측 결과를 연결한 값을 기반으로 상기 제2 U-Net을 통해 상기 모션 아티팩트-프리 2D 슬라이스 영상을 획득할 수 있다. 여기서, 상기 제1 U-Net과 상기 제2 U-Net은, 컨볼루션 블럭 어텐션 모듈(convolutional block attention module, CBAM)이 적용될 수 있다. 여기서, 상기 컨볼루션 블럭 어텐션 모듈(CBAM)은, 채널 방향(channel direction)과 공간 방향(spatial direction)을 따라 2개의 순차적 어텐션 맵(sequential attention maps)을 통합할 수 있다. 여기서, 상기 보정 모델은, 복수개의 모션 아티팩트 2D 슬라이스 영상을 포함하는 모션 아티팩트 3D 뇌 자기공 명영상 및 복수개의 상기 모션 아티팩트 2D 슬라이스 영상 각각에 대응되는 복수개의 모션 아티팩트-프리 2D 슬 라이스 영상을 포함하는 모션 아티팩트-프리 3D 뇌 자기공명영상을 포함하는 학습 데이터를 이용하여 학습될 수 있다. 여기서, 상기 보정 모델은, 상기 모션 아티팩트 2D 슬라이스 영상으로부터 상기 보정 모델을 통해 획득된 모션 아티팩트-보정 2D 슬라이스 영상 및 상기 모션 아티팩트-프리 2D 슬라이스 영상을 기반으로, 구조적 유사도 인 덱스 맵(structural similarity index map, SSIM)을 손실 함수(loss function)로 이용하여 구조적 유사도 인덱 스 맵(SSIM)이 최소화되도록 학습될 수 있다. 상기의 기술적 과제를 달성하기 위한 본 발명의 바람직한 실시예에 따른 컴퓨터 프로그램은 컴퓨터 판독 가능한 저장 매체에 저장되어 상기한 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법 중 어느 하나를 컴 퓨터에서 실행시킨다. 상기의 기술적 과제를 달성하기 위한 본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모 션 아티팩트 보정 장치는, 인공신경망을 기반으로 뇌 자기공명영상의 모션 아티팩트(motion artifact)를 보정하 는 보정 장치로서, 인공신경망을 기반으로 뇌 자기공명영상의 모션 아티팩트를 보정하기 위한 하나 이상의 프로 그램을 저장하는 메모리; 및 상기 메모리에 저장된 상기 하나 이상의 프로그램에 따라 인공신경망을 기반으로 뇌 자기공명영상의 모션 아티팩트를 보정하기 위한 동작을 수행하는 하나 이상의 프로세서;를 포함하며, 상기 프로세서는, 복수개의 2D 슬라이스 영상을 포함하는 보정 대상 3D 영상인 환자의 3D 뇌 자기공명영상을 획득하 고, 상기 보정 대상 3D 영상을 기반으로, 2D 적층형 U-Net(2D stacked U-net) 기반의 미리 학습되어 저장된 보 정 모델을 이용하여, 상기 보정 대상 3D 영상에서 상기 환자의 움직임으로 인해 발생된 모션 아티팩트를 보정한 모션 아티팩트-프리(motion artifact-free) 3D 영상을 획득한다. 여기서, 상기 프로세서는, 상기 보정 대상 3D 영상의 하나의 2D 슬라이스 영상인 제1 2D 슬라이스 영상에 대해, 상기 제1 2D 슬라이스 영상, 상기 제1 2D 슬라이스 영상을 기준으로 제1 방향에 인접한 제2 2D 슬라이스 영상 및 상기 제1 2D 슬라이스 영상을 기준으로 상기 제1 방향과 상이한 제2 방향에 인접한 제3 2D 슬라이스 영상을 상기 보정 모델에 입력하고, 상기 보정 모델의 출력을 통해 상기 제1 2D 슬라이스 영상에 대응되는 모션 아티팩 트-프리 2D 슬라이스 영상을 획득하는 과정을, 상기 보정 대상 3D 영상의 각 2D 슬라이스 영상에 대해 수행하여 상기 모션 아티팩트-프리 3D 영상을 획득할 수 있다. 여기서, 상기 보정 모델은, 상기 제1 2D 슬라이스 영상, 상기 제2 2D 슬라이스 영상 및 상기 제3 2D 슬라이스 영상을 입력받고, 상기 제1 2D 슬라이스 영상, 상기 제2 2D 슬라이스 영상 및 상기 제3 2D 슬라이스 영상 각각 에서 특징 추출기(feature extractor)를 통해 특징을 추출하여 제1 특징, 제2 특징 및 제3 특징을 획득하며, 상 기 제1 특징, 상기 제2 특징 및 상기 제3 특징을 기반으로 인코더(encoder)와 디코더(decoder)를 포함하는 제1 U-Net을 통해 초기 예측 결과를 획득하고, 상기 제1 특징, 상기 제2 특징, 상기 제3 특징 및 상기 초기 예측 결 과를 기반으로 인코더와 디코더를 포함하는 제2 U-Net을 통해 상기 모션 아티팩트-프리 2D 슬라이스 영상을 획 득하며, 상기 모션 아티팩트-프리 2D 슬라이스 영상을 출력할 수 있다. 여기서, 상기 보정 모델은, 상기 제1 특징, 상기 제2 특징 및 상기 제3 특징을 연결(concatenation)한 값을 기 반으로 상기 제1 U-Net을 통해 상기 초기 예측 결과를 획득하고, 상기 제1 특징, 상기 제2 특징, 상기 제3 특징 및 상기 초기 예측 결과를 연결한 값을 기반으로 상기 제2 U-Net을 통해 상기 모션 아티팩트-프리 2D 슬라이스 영상을 획득할 수 있다."}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법, 이를 수행하 는 장치 및 컴퓨터 프로그램에 의하면, 인공신경망을 기반으로 뇌 자기공명영상에서 환자의 움직임으로 인해 발 생되는 모션 아티팩트(motion artifact)를 보정함으로써, 촬영 대상의 움직임으로 인한 모션 아티팩트의 보정을 위해 딥러닝 알고리즘을 적용하기 위한 유용한 지침을 제공할 수 있다. 또한, 본 발명은 다중 슬라이스에서 얻을 수 있는 사전 지식 정보를 통해 추가적인 데이터 획득과 높은 계산 비 용이 필요한 다중 대비 영상을 사용하지 않고도 움직임에 의한 모션 아티팩트를 보정할 수 있고, 움직임에 의한 모션 아티팩트를 개선하는데 있어서 가장 중요한 구조적인 정보들을 보존하여 신뢰성 있는 모션 아티팩트 보정방법을 제공할 수 있다."}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과들은 이상에서 언급한 효과들로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재 로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명의 실시예를 상세히 설명한다. 본 발명의 이점 및 특징, 그리고 그것들 을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시예들을 참조하면 명확해질 것이다. 그 러나, 본 발명은 이하에서 게시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있으 며, 단지 본 실시예들은 본 발명의 게시가 완전하도록 하고, 본 발명이 속하는 기술 분야에서 통상의 지식을 가 진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐 이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어(기술 및 과학적 용어를 포함)는 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 사용될 수 있을 것이다. 또한, 일반적 으로 사용되는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하 게 해석되지 않는다. 본 명세서에서 \"제1\", \"제2\" 등의 용어는 하나의 구성 요소를 다른 구성 요소로부터 구별하기 위한 것으로, 이 들 용어들에 의해 권리범위가 한정되어서는 아니 된다. 예컨대, 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소도 제1 구성 요소로 명명될 수 있다. 본 명세서에서 각 단계들에 있어 식별부호(예컨대, a, b, c 등)는 설명의 편의를 위하여 사용되는 것으로 식별 부호는 각 단계들의 순서를 설명하는 것이 아니며, 각 단계들은 문맥상 명백하게 특정 순서를 기재하지 않는 이 상 명기된 순서와 다르게 일어날 수 있다. 즉, 각 단계들은 명기된 순서와 동일하게 일어날 수도 있고 실질적 으로 동시에 수행될 수도 있으며 반대의 순서대로 수행될 수도 있다. 본 명세서에서, \"가진다\", \"가질 수 있다\", \"포함한다\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예컨대, 수치, 기능, 동작, 또는 부품 등의 구성 요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. 이하에서 첨부한 도면을 참조하여 본 발명에 따른 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법, 이를 수행하는 장치 및 컴퓨터 프로그램의 바람직한 실시예에 대해 상세하게 설명한다. 먼저, 도 1을 참조하여 본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 장치에 대하여 설명한다. 도 1은 본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 장치를 설명 하기 위한 블록도이다. 도 1을 참조하면, 본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 장치(이하 '보정 장치'라 한다)는 인공신경망을 기반으로 뇌 자기공명영상(magnetic resonance imaging, MRI)에서 환자의 움직임으로 인해 발생되는 모션 아티팩트(motion artifact)를 보정할 수 있다. 이에 따라, 본 발명은 촬영 대상의 움직임으로 인한 모션 아티팩트의 보정을 위해 딥러닝 알고리즘을 적용하기 위한 유용한 지침을 제공할 수 있다. 또한, 본 발명은 다중 슬라이스(slice)에서 얻을 수 있는 사전 지식 정보 (self-assisted priors)를 통해 추가적인 데이터 획득과 높은 계산 비용이 필요한 다중 대비 영상을 사용하지 않고도 움직임에 의한 모션 아티팩트를 보정할 수 있고, 움직임에 의한 모션 아티팩트를 개선하는데 있어서 가 장 중요한 구조적인 정보들을 보존하여 신뢰성 있는 모션 아티팩트 보정 방법을 제공할 수 있다. 이를 위해, 보정 장치는 하나 이상의 프로세서, 컴퓨터 판독 가능한 저장 매체 및 통신 버스 를 포함할 수 있다. 프로세서는 보정 장치가 동작하도록 제어할 수 있다. 예컨대, 프로세서는 컴퓨터 판독 가능한 저장 매체에 저장된 하나 이상의 프로그램을 실행할 수 있다. 하나 이상의 프로그램은 하나 이 상의 컴퓨터 실행 가능 명령어를 포함할 수 있으며, 컴퓨터 실행 가능 명령어는 프로세서에 의해 실행되는 경우 보정 장치로 하여금 인공신경망을 기반으로 뇌 자기공명영상의 모션 아티팩트를 보정하기 위한 동작 을 수행하도록 구성될 수 있다. 컴퓨터 판독 가능한 저장 매체는 인공신경망을 기반으로 뇌 자기공명영상의 모션 아티팩트를 보정하기 위 한 컴퓨터 실행 가능 명령어 내지 프로그램 코드, 프로그램 데이터 및/또는 다른 적합한 형태의 정보를 저장하 도록 구성된다. 컴퓨터 판독 가능한 저장 매체에 저장된 프로그램은 프로세서에 의해 실행 가 능한 명령어의 집합을 포함한다. 일 실시예에서, 컴퓨터 판독 가능한 저장 매체는 메모리(랜덤 액세스 메 모리와 같은 휘발성 메모리, 비휘발성 메모리, 또는 이들의 적절한 조합), 하나 이상의 자기 디스크 저장 디바 이스들, 광학 디스크 저장 디바이스들, 플래시 메모리 디바이스들, 그 밖에 보정 장치에 의해 액세스되고 원하는 정보를 저장할 수 있는 다른 형태의 저장 매체, 또는 이들의 적합한 조합일 수 있다. 통신 버스는 프로세서, 컴퓨터 판독 가능한 저장 매체를 포함하여 보정 장치의 다른 다양 한 컴포넌트들을 상호 연결한다. 보정 장치는 또한 하나 이상의 입출력 장치를 위한 인터페이스를 제공하는 하나 이상의 입출력 인터페이스 및 하나 이상의 통신 인터페이스를 포함할 수 있다. 입출력 인터페이스 및 통신 인터페이스 는 통신 버스에 연결된다. 입출력 장치(도시하지 않음)는 입출력 인터페이스를 통해 보정 장치 의 다른 컴포넌트들에 연결될 수 있다. 그러면, 도 2 내지 도 5를 참조하여 본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모 션 아티팩트 보정 방법에 대하여 설명한다.도 2는 본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법을 설명 하기 위한 흐름도이고, 도 3은 본 발명의 바람직한 실시예에 따른 모션 아티팩트의 보정 과정을 설명하기 위한 도면이며, 도 4는 본 발명의 바람직한 실시예에 따른 보정 모델을 이용한 모션 아티팩트의 보정 과정을 보다 자 세히 설명하기 위한 도면이고, 도 5는 본 발명의 바람직한 실시예에 따른 보정 모델의 학습 과정을 설명하기 위 한 도면이다. 도 2를 참조하면, 보정 장치의 프로세서는 보정 대상 3D 영상인 환자의 3D 뇌 자기공명영상을 획득할 수 있다(S110). 여기서, 보정 대상 3D 영상은 복수개의 2D 슬라이스 영상을 포함할 수 있다. 이후, 프로세서는 보정 대상 3D 영상을 기반으로, 보정 모델을 이용하여, 보정 대상 3D 영상에서 환자의 움직임으로 인해 발생된 모션 아티팩트를 보정한 모션 아티팩트-프리 3D 영상을 획득할 수 있다(S120). 즉, 프로세서는 보정 대상 3D 영상을 기반으로, 2D 적층형 U-Net(2D stacked U-net) 기반의 미리 학습되 어 저장된 보정 모델을 이용하여, 보정 대상 3D 영상에서 환자의 움직임으로 인해 발생된 모션 아티팩트를 보정 한 모션 아티팩트-프리(motion artifact-free) 3D 영상을 획득할 수 있다. 도 3을 참조하여 보다 자세하게 설명하면, 프로세서는 보정 대상 3D 영상의 하나의 2D 슬라이스 영상인 제 1 2D 슬라이스 영상에 대해, 제1 2D 슬라이스 영상, 제1 2D 슬라이스 영상을 기준으로 제1 방향에 인접한 제2 2D 슬라이스 영상 및 제1 2D 슬라이스 영상을 기준으로 제1 방향과 상이한 제2 방향에 인접한 제3 2D 슬라이스 영상을 보정 모델에 입력하고, 보정 모델의 출력을 통해 제1 2D 슬라이스 영상에 대응되는 모션 아티 팩트-프리 2D 슬라이스 영상을 획득하는 과정을, 보정 대상 3D 영상의 각 2D 슬라이스 영상에 대해 수행하여 모 션 아티팩트-프리 3D 영상을 획득할 수 있다. 여기서, 보정 모델은 도 4에 도시된 바와 같이, 제1 2D 슬라이스 영상, 제2 2D 슬라이스 영상 및 제3 2D 슬라이스 영상을 입력받을 수 있다. 그리고, 보정 모델은 제1 2D 슬라이스 영상, 제2 2D 슬라이스 영상 및 제3 2D 슬라이스 영상 각각에서 특 징 추출기(feature extractor)를 통해 특징을 추출하여 제1 특징, 제2 특징 및 제3 특징을 획득할 수 있다. 그리고, 보정 모델은 제1 특징, 제2 특징 및 제3 특징을 기반으로 인코더(encoder)와 디코더(decoder)를 포함하는 제1 U-Net을 통해 초기 예측 결과를 획득할 수 있다. 이때, 보정 모델은 제1 특징, 제2 특징 및 제3 특징을 연결(concatenation)한 값을 기반으로 제1 U-Net을 통해 초기 예측 결과를 획득할 수 있다. 여기서, 제1 U-Net은 컨볼루션 블럭 어텐션 모듈(convolutional block attention module, CBAM)이 적용될 수 있다. 컨볼루션 블럭 어텐션 모듈(CBAM)은 채널 방향(channel direction)과 공간 방향(spatial direction)을 따라 2개의 순차적 어텐션 맵(sequential attention maps)을 통합할 수 있다. 그리고, 보정 모델은 제1 특징, 제2 특징, 제3 특징 및 제1 U-Net을 통해 획득된 초기 예측 결과를 기반으 로 인코더와 디코더를 포함하는 제2 U-Net을 통해 모션 아티팩트-프리 2D 슬라이스 영상을 획득할 수 있다. 이 때, 보정 모델은 제1 특징, 제2 특징, 제3 특징 및 초기 예측 결과를 연결한 값을 기반으로 제2 U-Net을 통해 모션 아티팩트-프리 2D 슬라이스 영상을 획득할 수 있다. 여기서, 제2 U-Net은 컨볼루션 블럭 어텐션 모 듈(CBAM)이 적용될 수 있다. 컨볼루션 블럭 어텐션 모듈(CBAM)은 채널 방향과 공간 방향을 따라 2개의 순차적 어텐션 맵을 통합할 수 있다. 그리고, 보정 모델은 모션 아티팩트-프리 2D 슬라이스 영상을 출력할 수 있다. 또한, 보정 모델은 도 5에 도시된 바와 같이, 학습 데이터를 이용하여 학습될 수 있다. 여기서, 학습 데 이터는 복수개의 모션 아티팩트 2D 슬라이스 영상을 포함하는 모션 아티팩트 3D 뇌 자기공명영상 및 복수개의 모션 아티팩트 2D 슬라이스 영상 각각에 대응되는 복수개의 모션 아티팩트-프리 2D 슬라이스 영상을 포함하는 모션 아티팩트-프리 3D 뇌 자기공명영상을 포함할 수 있다. 즉, 보정 모델은 모션 아티팩트 2D 슬라이스 영상으로부터 보정 모델을 통해 획득된 모션 아티팩트- 보정 2D 슬라이스 영상 및 모션 아티팩트-프리 2D 슬라이스 영상을 기반으로, 구조적 유사도 인덱스 맵 (structural similarity index map, SSIM)을 손실 함수(loss function)로 이용하여 구조적 유사도 인덱스 맵 (SSIM)이 최소화되도록 학습될 수 있다.그러면, 도 6 내지 도 11을 참조하여 본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모 션 아티팩트 보정 방법에 대하여 보다 자세히 설명한다. 도 6은 본 발명의 바람직한 실시예에 따른 보정 모델의 일례를 설명하기 위한 도면이고, 도 7은 본 발명의 바람 직한 실시예에 따른 손상된 2D 슬라이스 영상과 인접한 2D 슬라이스 영상의 일례를 나타내는 도면이며, 도 8은 본 발명의 바람직한 실시예에 따른 SSIM 측면에서의 다양한 모션 심각도 레벨(motion severity level) 간의 관 계를 나타내는 도면이고, 도 9는 본 발명의 바람직한 실시예에 따른 MSE 측면에서의 다양한 모션 심각도 레벨 (motion severity level) 간의 관계를 나타내는 도면이며, 도 10은 본 발명의 바람직한 실시예에 따른 PSNR 측 면에서의 다양한 모션 심각도 레벨(motion severity level) 간의 관계를 나타내는 도면이고, 도 11은 본 발명의 바람직한 실시예에 따른 PerceptualVGG 측면에서의 다양한 모션 심각도 레벨(motion severity level) 간의 관 계를 나타내는 도면이다. 본 발명은 3D 뇌 자기공명영상에서 환자의 움직임으로 인해 발생한 모션 아티팩트 문제를 해결하기 위해, 추가 적인 데이터의 획득 없이 인접한 슬라이스에서 얻은 사전 지식 정보를 활용하여 학습된 적층 인공신경망(2D stacked U-Net)을 이용하는 기술에 대한 것이다. 인간의 뇌 구조는 인접한 영역에서 높은 상관 관계를 가지므로, 연속된 슬라이스에서 서로 해부학적인 세부 정 보를 공유할 수 있다. 따라서, 움직임으로 인한 영상 훼손이 발생한 슬라이스의 해부학적 정보를 인접한 위치 의 영상 손상이 덜한 슬라이스에서 전달받을 수 있다. 결국 이는 본 발명에 따른 적층 인공신경망에서 연속된 슬라이스를 사용하여 사전 지식 정보를 학습할 수 있도록 함으로써, 움직임으로 인해 손상된 해부학적 구조를 보정하도록 활용될 수 있다. 더욱 구체적으로 본 발명은 연속된 슬라이스의 영상들이 다중 입력으로 사용되고, 움직임에 의한 모션 아티팩트가 보정된 단일 영상을 생성할 수 있다. 특히, 본 발명에서 사용되는 3D 뇌 자기 공명영상은 2D 영상과 달리 인접한 슬라이스를 건너뛰지 않고 모두 획득하기 때문에 본 발명에 따른 방법을 적 용하여 구조적 세부 사항을 복원할 수 있다. 이와 관련하여, 최근 다른 인공지능 기반의 연구에서도 더 좋은 성능을 위해 같이 슬라이스의 다중 대비 영상들 혹은 인접한 슬라이스를 다중 입력으로 사용하고 있다. 하지만, 다중 대비 영상을 얻기 위해서는 추가적인 촬 영과 높은 계산 비용이 필요하다는 단점이 존재한다. 또한, 인접한 슬라이스를 사용하는 방법은 일부 다른 의 료 영상 분석 문제에서 유용하게 활용되고 있다. 그러나, 움직임에 의한 모션 아티팩트 문제를 해결하기 위해, 다중 슬라이스를 이용하여 얻은 구조적 사전 지식 정보를 사용하는 것은 본 발명이 처음 시도하는 것이다. 따 라서, 본 발명은 추가적인 데이터의 획득 없이 신뢰할 수 있는 모션 아티팩트 보정 방법을 제안할 수 있다. 또한, 본 발명은 예측 사이의 일부 공간 관계를 포착하여 초기 추정치를 재평가할 수 있는 2개의 캐스케이드 인 코더-디코더(cascaded encoder-decoder) 구조로 구성된 새로운 인공신경망 모델을 제안하고 있다. 보다 구체적 으로, 첫 번째 인코더-디코더 모델의 초기 예측 결과는 입력된 모션-아티팩트 영상과 함께 두 번째 인코더-디코 더 모델로 전달될 수 있다. 이 연결을 통해 두 번째 네트워크는 두 영상 사이의 더 많은 공간적 차이를 포착할 수 있으므로, 더 좋은 성능의 모션 아티팩트가 보정된 결과 영상을 생성할 수 있다. 결국, 2개의 인코더-디코 더 모델 간의 추가적인 구조 정보 공유의 이점을 가진 본 발명에 따른 모델은 픽셀 대 픽셀 의존성(pixel-to- pixel dependency)을 개선하고, 공간 정보 손실을 최소화(spatial location preservation)하여 결과 영상을 개 선시킬 수 있다. 정리하면, 본 발명의 주요 내용은 다음과 같다. - 본 발명은 훼손으로 인한 누락된 구조적 정보를 보완하기 위해 다중 입력을 통해 인접한 슬라이스로부터 해부 학적 정보를 추가적으로 학습하는 새로운 모션 아티팩트 보정 방법을 제안한다. 특히, 추가적인 데이터의 획득 없이 입력 영상 자체만을 사용하여 신뢰할 수 있는 모션 아티팩트 보정 방법을 제시할 수 있다. - 본 발명에서는 위에서 설명된 사전 지식이 포함되어 종단 간 방식으로 학습되는 적층 인공신경망을 제안한다. 이는 예측 사이의 픽셀 단위 매칭 및 공간적 위치 보존을 통해 픽셀 대 픽셀 의존성을 더욱 향상시킬 수 있다. A. 본 발명에 따른 보정 모델 A-1. 원본 데이터세트(original dataset) 모션 아티팩트 보정 작업을 수행하기 위해, 서울대학교병원(SNUH)에서 임상 뇌 자기공명영상 데이터세트를 수집 하였다. 본 발명에 따른 데이터세트는 네트워크 훈련을 수행하기 위해 시뮬레이션된 모션 데이터를 생성하는데 사용된 83명의 모션-프리(motion-free) 대상(subject)을 포함한다. 또한, 생체-내 평가(in-vivo assessment) 를 수행하기 위해 모션이 왜곡된 별도의 24명의 임상 대상을 획득한다. 이러한 실제 임상 대상은 데이터 획득 도중에 모션을 유도하도록 지시받지 않았다. BRAVO(BRAin VOlume)라고 하는 3D T1-가중 기울기 에코(3D T1- weighted gradient echoes)는 3.0 Tesla MRI scanner(SIGNA Premier, GE Healthcare, United States)에서 다 음과 같은 이미징 파라미터를 사용하여 획득하였다. - 에코 시간(echo time, TE) : 2.77ms - 반복 시간(repetition time, TR) ; 6.86ms - 플립 각도(flip angle) : 10° - 픽셀 대역폭(pixel bandwidth : 244Hz/pixel 시상 이미지 행렬(sagittal image matrix)은 0.90×0.90mm2의 평면-내(in-plane) 해상도(resolution)와 1mm의 슬라이스 간격(slice spacing)으로 256×256에서 512×512 픽셀까지 다양하며, 3차원은 144에서 380 사이이다. 또한, 83명의 모션-프리 임상 환자 중 38명의 대상이 추가적으로 CE(Contrast-Enhanced) 스캔을 받았다. 이 CE 데이터는 원본 비CE 데이터와 동일한 이미징 파라미터 및 FOV를 가진다. 이 작업에서, 본 발명에 따른 네트워 크에 대한 입력으로 실제-값 크기 데이터(real-valued magnitude data)를 사용하였다. A-2. 모션 아티팩트의 시뮬레이션 다양한 기준 모션-프리 및 타겟 모션-손상(motion-corrupted) 데이터세트를 획득하는 데 한계가 있기 때문에, 네트워크 훈련을 수행하기 위해 자기공명영상 모션 아티팩트의 시뮬레이션이 불가피하다. 시뮬레이션은 83개의 원본 모션-프리 데이터에 모션-손상 이미지를 합성한다. 이 작업에는 모션-손상 데이터를 생성하기 위한 3D 갑 작스러운 뇌 모션과 지속적인 뇌 모션의 시뮬레이션을 포함한다. 본 발명에 따른 모션 아티팩트 보정 방법의 타당성과 신뢰성을 얻기 위해서는 실제 모션 아티팩트를 고도로 모방한 모션 데이터를 생성하는 것이 중요하다. 이를 위해, 벌크 강체 모션(bulk rigid motion)이 회전 모션(rotation motion) 및 병진 모션(translation motion)의 조합이라고 가정한다. 모션 아티팩트는 모든 3개의 축에서 [-7°, +7°] 범위의 산발적인 회전 모션 을 적용하고, 모든 3개의 평면에서 -7mm와 +7mm 사이의 병진 모션을 적용하여 생성된다. 모션 시뮬레이션은 평 면-내 모션 및 평면-관통(through-plane) 모션을 전부 채택한다. 이는 3D 시퀀스(3D sequence)로 수행되며, 병진 및 회전(anterior-posteior(AP), right-left(RL), superior-inferior(SI), yaw, pitch, 및 roll)의 각 축을 고려한다. A-3. 이론 자기공명영상 모션 아티팩트의 보정 방법의 목표는 데이터 획득 중 환자의 움직임으로 인해 발생한 모션-손상 이미지 xm에서 모션-프리 이미지 x를 복원하는 것이다. 이 움직임은 k-공간 지점(k-space points)을 획득할 때 열화(degradation)를 야기한다. 문제식을 단순화하기 위해, 본 발명은 공간 도메인(spatial domain)에서의 이 미지 열화를 아래의 [수학식 1]과 같이 정의한다. 수학식 1"}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, Tr은 로 정의되는 강체 모션 변환(rigid motion transformation)이다. Tθ 및 R θ는 각각 이미지 도메인(image domain)에서의 병진 연산자(translation operator) 및 회전 연산자(rotationoperator)이고, θ는 이들의 모션 파라미터 (tx, ty, tz) 및 (ρx, ρy, ρz)를 나타낸다. F, M, 및 F-1은 각각 이산 푸리에 변환(Discrete Fourier Transform, DFT), k-공간의 샘플링 연산자, 및 인버스 DFT이다. 기본적으로, 이 수학식은 많은 미지수(즉, 모션 파라미터)를 포함할 수 있기 때문에 선형으로 풀기가 어렵다. 그러나, 이 작업은 비-선형 딥 러닝 네트워크(non-linear deep learning networks)를 사용하여 보정된 이미지 를 얻는 인버스 변환 맵(inverse transformation map)을 유도하는 최적화 문제로 형성될 수 있다. 본 발명 은 netW를 아래의 [수학식 2]와 같이 모션 변환 Tr을 역으로 매핑하는 네트워크 가중치의 최적화로 정의한다. 수학식 2"}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "뇌의 백질과 회백질의 구조적 세부 사항은 인접한 슬라이스 내에서, 특히 3D 이미징 도중에 유사하다고 가정한 다. 따라서, 동일한 손상된 대상의 인접한 슬라이스에서 추가적인 사전 지식(prior knowledge)을 포함하면 이 문제를 해결하는데 도움이 될 수 있다. 위의 식은 아래의 [수학식 3]과 같이 표현될 수 있다. 수학식 3"}
{"patent_id": "10-2022-0122560", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, xm[i-1] 및 xm[i+1]은 현재 모션-손상 이미지 xm[i]의 인접한 슬라이스로부터 파생된 이미지 사전(image prior)이다. Concat<·>은 모든 슬라이스의 추출된 특징의 연결 함수(concatenation function)를 나타낸다. w1, w2 및 w3은 모든 네트워크 가중치 netW의 일부인 특징 추출기 FEW(·)의 가중치이다. 이용된 이미지 사전이 손상 데이터라는 사실에도 불구하고, 현재 i번째 이미지에서 누락될 수 있는 일부 세부 정보를 공유할 수 있다. 이는 다중-입력 단일-출력 프로세스로, 매번 단일 모션 이미지에 대한 모션 아티팩트를 보정하게 된다. 따라서, 본 발명에 따른 사전 지식 정보(self-assisted priors)의 개념을 적용하여 추가적인 구조적 정보를 학 습해야 하는 요구 사항을 해결할 수 있다. A-4. 보정 모델인 모션 아티팩트 보정 네트워크(motion artifact correction network) 처음에 이미지 분할을 위해 설계되고 노이즈 제거 및 재구성 작업에 성공적으로 적용된 U-Net(Ronneberger et al., 2015)에서 영감을 받아, 본 발명은 자기공명영상 모션 아티팩트의 이미지 보정을 위한 사전 지식 정보를 가지는 2D 적층형 U-Net을 제안한다. 본 발명에 따른 모션 보정 네트워크는 도 6에 도시된 바와 같다. 이러한 딥 러닝 네트워크를 설계하는 주요 목표는 모션-프리 이미지를 검색하고 아티팩트와 같은 장애물을 포함하지 않 는 관심 구조에서 환자를 정확하게 진단할 수 있는 독자(reader)의 능력을 높이는 것이다. 본 발명에 따른 아 키텍처(architrecture)의 필수적 기여는 사전 지식 정보 및 스택형 개선 네트워크(stacked refinement network)를 모두 포함한다. 아래에서는 이러한 개념에 대해 보다 자세히 설명한다. A-4-1. 사전 지식 정보(self-assisted priors, SAP) 본 발명에 따른 네트워크는 타겟 손상 이미지의 인접한 슬라이스로부터 이미지 사전을 통합하여 시작한다. 이 는 단일 입력 대신 다중 입력을 가져오고 전달할 수 있도록 원본 U-Net을 재설계하여 달성될 수 있다. 고정된 이미지 크기 256×256을 가진 사전 지식 정보 xm[i-1]과 xm[i+1]와 함께 그레이스케일(grayscale) 모션-손상 이미지xm[i]가 특징 추출기 인코더(feature extractor encoders) FEW(·)에 독립적으로 전달되어, 각 입력에 대해 개별 특징 맵(feature maps)을 생성한다. 그런 다음, 도 6에 도시된 바와 같이, 모든 추출된 특징들을 연결하여 본 발명에 따른 네트워크에 대한 입력으로 사용한다. 이 프로세스는 다른 사전이 없기 때문에 반복된 첫 번째 및 마지막 슬라이스를 제외하고, 훈련 및 테스트 대상 내의 모든 이미지 슬라이스에 적용할 수 있다. 이러한 이미 지 사전을 포함하면 타겟 모션-손상 이미지 xm[i]에서 놓칠 수 있는 일부 구조적 세부 정보를 공유하는데 도움이 되어, 모션-프리 이미지를 복원하는 성능이 향상된다. 이는 특히 이 작업의 경우와 같이 3D 이미징 전략에서 데이터 획득이 수행된 경우(즉, 슬라이스 간격의 영향이 없는 경우), 뇌 구조가 인접한 위치에서 높은 균질성을 가지고 있기 때문이다. 다시 말해, 3D 이미징은 더 높은 공간 해상도(spatial resolution)를 생성하므로, 인접 한 슬라이스의 추가 사전 지식을 사용하여 누락된 부분을 검색하고, 더 나은 모션 보정을 달성하는데 매우 중요 하다. 이것은 다중-입력 단일-출력 프로세스이며, 이는 네트워크가 대응하는 정답(ground-truth) 모션-프리 이 미지 x[i]의 손실을 계산하여 모션-손상 이미지 xm[i]를 보정하는 방법을 학습함을 의미한다. 각 훈련 반복에서, 사전 xm[i-1]과 xm[i+1]의 역할은 동일한 손상된 대상의 추가적인 유용한 패턴을 네트워크로 공합하는 것이다. 그 러나, 현재 모션-손상 이미지 xm[i]의 보정 도중에는 사전의 보정이 이루어지지 않는다. (Lee et al., 2021, Chatterjee et al., 2020)와 비교하여, 본 발명에 따른 사전 지식 정보 접근법은 다음과 같은 장점이 있다. - (i) 이미지 사전으로 사용하기 위해 추가적인 MRI 스캔이 필요하지 않는다. - (ii) 이미지 등록 및 정렬과 같은 추가적인 이미지 전처리가 필요하지 않기 때문에 계산 비용도 절감된다. 그럼에도 불구하고, 다른 대비 데이터(contrast data)를 사용할 수 있는 경우, 동일한 대비의 사전 지식 정보 외에 추가적인 이미지 사전으로 사용할 수 있다. 본 발명에서는 사전 지식 정보와 다른 대비 데이터의 사전을 모두 포함하여, 다양한 유형의 사전 지식 정보를 테스트한다. 도 7은 현재 손상된 슬라이스와 관련된 SSIM 점 수가 있는 인접한 슬라이스의 몇 가지 예를 나타낸다. 도 7에서 더 가까운 인접한 슬라이스(xm[i-1] 및 xm[i+1])의 상대적으로 높은 SSIM 점수는 일부 누락된 구조적 세부 정보를 공유하는 것을 통해 더 나은 결과를 위해 사전 지식 정보를 사용한다는 가정의 이점을 예시적으로 나타낸다. 멀리 있는 슬라이스(xm[i-2] 및 xm[i+2])가 현재 모 션-손상 이미지와 더 적은 유사성을 갖는 것으로 관찰된다. 따라서, 본 발명은 더 높은 유사성을 포함하기 때 문에 더 가까운 인접한 슬라이스의 사전 지식 정보를 사용하여 이 작업을 수행한다. A-4-2. 2D 적층형 U-Net(2D stacked refinement U-Nets) 적층형 네트워크의 아이디어는 의미론적 분할(semantic segmentation)(Shah et al., 2018, Jha et al., 2020) 에 활용되어 분할된 타겟을 교정(refine)하고 더 나은 성능을 달성할 수 있다. 이 작업에서, 본 발명은 2개의 캐스케이드 인코더-디코더(cascaded encoder-decoder) U-Net으로 구성된 새로운 아키텍처를 제안한다. 모든 네 트워크에서 이미지 사전을 재사용하여 2D 적층형 U-Net을 개발하였다. 보다 구체적으로, 첫 번째 인코더-디코 더 U-Net의 초기 예측은 이미지 사전 및 해당 모션-손상 이미지와 연결되어 두 번째 인코더-디코더 U-Net으로 전달된다. 이 연결을 통해 두 번째 U-Net은 2개의 예측 간의 더 많은 공간적 차이를 캡처할 수 있으므로 손상 된 이미지를 더 잘 교정(refine)할 수 있다. 본 발명에 따른 적층형 U-Net은 end-to-end 방식으로 훈련된다. 따라서, 초기 추정치는 훈련 과정에서 재평가된다. 결국, 추가 지식을 공유하는 이점이 있는 본 발명에 따른 적층형 U-Net은 픽셀 대 픽셀 의존성을 개선하고 공간 정보의 상당한 보존과 함께 모션-프리 이미지를 복원할 수 있다. A-4-3. 어텐션 모듈(attention module) 본 발명에 따른 적층형 네트워크의 모든 해상도 레벨에 컨볼루션 블럭 어텐션 모듈(CBAM)을 적용한다. 컨볼루 션 블럭 어텐션 모듈(CBAM)은 채널 방향과 공간 방향을 따라 2개의 순차적 어텐션 맵을 통합하여 달성될 수 있 다. 채널 어텐션 모듈(AttC)은 특징 맵의 채널 간 관계로부터 '무엇'이 가장 의미있는 맵(즉, 특징 디텍터)인지 알려준다. 이 어텐션 모듈은 먼저 최대-풀링(max-pooling) 및 평균-풀링(average-pooling) 연산을 이용하여 주어진 특징 맵 F를 2개의 독립적인 벡터로 압축하여 유도될 수 있다. 그런 다음, 이 두개의 설명자 (descriptors)는 단일 히든 레이어(single hidden layer)의 다중-레이어 퍼셉트론(multi-layer perceptron, MLP)에 독립적으로 전달된다. 다중-레이어 퍼셉트론(MLP)으로부터 생성된 특징 벡터는 모두 요소별 합산 (element-wise summation)을 이용하여 집계되어 채널 어텐션 맵을 생성한다. 채널 어텐션을 보완하는, 공간 어텐션 모듈(AttS)은 주어진 특징 맵의 공간 간 관계를 활용하여 가장 효과적인 정보 부분이 '어디'인지를 표시(highlight)할 수 있다. 이 어텐션 모듈은 특징 채널 방향을 따라 최대-풀링 및 평균-풀링을 적용하여 시작된ㄴ다. 그런 다음, 컨볼루션 레이어(convolutional layer)가 연결된 특징 설명자에 적용되어 공간 어텐션 맵을 생성한다. 결국, AttC 및 AttS 각각은 적응적 특징 교정(adaptive feature refinement)을 위해 및 과 같이 입력 특징 맵에 순차적으로 곱해진다. 이 프로세스는 관심 표현을 개선하고 추가 적인 보정이 필요할 수 있는 영역에 더 집중할 수 있다. A-4-4. 구현 상세 내용(implementation details) 본 발명에 따른 네트워크는 시뮬레이션된 모션-손상 이미지 및 이에 대응하는 정답(ground-truth) 모션-프리 데 이터를 이용하여 슈퍼바이즈 방식(supervised manner)으로 훈련된다. 이용된 U-Net은 4개의 인코더-디코더 레 벨을 포함하고 있다. 모든 네트워크 전체의 컨볼루션 커널(convolution kernels)은 인코더 경로(encoder path)에 32, 64, 128, 및 256의 특징 맵이 있는 3×3 크기를 가지는 반면, 인버스 특징 맵은 디코더 경로 (decoder path)에 사용된 후 최종 출력 예측 맵(final output prediction map)에 사용된다. 본 발명은 모든 컨볼루션 레이저에 대해 ReLU(Rectified Linear Unit)의 배치 정규화(batch normalization) 및 활성화 (activation) 함수를 순차적으로 적용한다. 본 발명은 필터 크기가 2×2이고 보폭(stride)이 2인 평균 풀링을 사용한다. 인코더 경로의 각 해상도 레벨의 특징 맵이 디코더 경로의 동일한 해상도 레벨의 해당 맵과 연결된 다는 점은 주목할 만하다. 이를 통해 초기 및 후기 컨볼루션 레이어 간에 전역 및 로컬 표현을 모두 융합할 수 있다. 네트워크 훈련 중에, 본 발명은 경사 하강(gradient descent)을 위해 10개 샘플의 배치 크기(batch size)로 아담 최적화기(Adam optimizer)를 활용한다. 본 발명은 초기에 학습률(learning rate)를 0.001로 설 정하고, 50 에포크(epochs) 동안 10을 팩터(factor)로 지수적으로 감소한다. 본 발명에 따른 네트워크는 401만 파라미터에 대한 훈련을 완료하는데 약 5.8시간이 소요된다. 본 발명의 구현은 128GB 램이 장착된 NVIDIA GeForce RTX 3080의 Cuda-지원 GPU가 장착된 PC에서 수행하였다. 이 작업은 Ubuntu 18.04 운영체제에서 Python 3.7.10 및 Keras 라이브러리를 갖는 Tensorflow를 사용하여 구현 되었다. A-5. 손실 측정(loss measures) 이미지 변환에 대한 이전 연구에서는 SSIM, MSE(Mean Square Error), PSNR(Peak Signal-to-Noise Ratio), 또는 Perceptual 손실과 같은 손실 함수(loss function)를 사용하였다. 그러나, 모션 보정 방법을 훈련하는데 어떤 측정이 가장 민감한지는 확실하지 않다. MSE는 많은 이미지 노이즈 제거 및 재구성 어플리케이션에서 손실 함 수로 활용되지만, 모션 보정 작업에는 SSIM을 포함한 손실 함수가 혼합되어 사용된다(Lee et al., 2021). 이는 모션 아티팩트가 이미지 구조를 크게 저하시키고 SSIM 손실이 기본적으로 참조 이미지와 타겟 이미지 간의 구조 적 유사성 관계를 계산하기 때문이다. 이 실험의 주요 동기는 다양한 유형의 모션 아티팩트에 더 민감한 솔실을 보여주는 것이다. 따라서, 가장 효과 적인 측정은 네트워크 최적화에 활용될 더 나은 손실 함수를 나타낼 수 있다. 이상적으로, 손실 함수 값이 모 션의 레벨에 비레하여야 한다. 이 실험을 수행하기 위해 동일한 환자 데이터(Kecskemeti and Alexander, 202 0)에 대해, 마일드(mild), 중간(moderate), 및 심각(severe)의 3개의 유형에 대한 모션을 생성하였다. 분석은 ImageNet에서 사전-훈련된 VGGNet(Visual Geometry Group Network)으로부터의, SSIM, MSE, PSNR, 및 Perceptual에 집중하였고, 이는 일반적으로 노이즈 제거 작업에 사용되기 때문이다. 그런 다음, 원본 비-모션 데이터에 해당하는 각 모션 심각도 레벨에 대한 모든 측정값을 계산하였다. 도 8 내지 도 11은 각 손실 인덱스에 대한 다양한 유형의 모션 아티팩트 간의 관계를 보여준다. 도 8 내지 도 11의 데이터 포인트는 특정 모션 유형의 단일 이미지의 손실 측정을 나타내는 반면, 프로젝션 플롯(projection plot)은 3개의 모션 아티팩트 간 의 관계를 보여준다. 또한, 모션 아티팩트 쌍(즉, mild vs moderate, mild vs severe, 및 moderate vs severe)에 대한 회귀선(regression line)의 R-제곱값(R-squared value)을 보여준다. 각 손실 측정에 대한 3D 프로젝션 플롯은 데이터 포인트 분포의 직관(intuition)을 제공하고 동일한 측정 내에서 3개의 병합된 모션의 트렌드(trend)를 간단히 표시할 수 있다. 이 실험은 SSIM 인덱스가 다른 평가 척도에 비해 모든 유형의 모션 아티팩트 쌍에 대해 더 높은 R-제곱값을 가짐을 보여준다. 따라서, SSIM 인덱스는 MSE, PSNR, 및 PerceptualVGG와 비교하여 다양한 모션 아티팩트 강도(즉, mild, moderate, 및 severe 모션)를 구별하는데 가 장 좋은 지표를 제공한다. PerceptualVGG는 모션-손상 이미지와 이의 참조 모션-프리 이미지를 VGGNet과 같은 고정된 사전-훈련된 모델에 전달한 다음 네트워크 내의 특정 레이어에서 이들 간의 특징 차이를 정량화하여 계 산된다. 이 작업에서 perceptual 손실은 'block3-conv3'이라는 VGGNet의 7번째 컨볼루션 레이어에서 계산된다. 서로 다른 모션 쌍 간의 상관 관게 측면에서, SSIM 인덱스는 mild vs moderate에서 0.9243, mild vs severe에 서 0.9667, moderate vs severe에서 0.9191의 매우 높은 R-제곱값을 가지는 가장 좋은 상관 관계를 보여준다. 또한, PerceptualVGG는 MSE 및 PSNR에 비해 다양한 모션 유형에서 더 나은 상관 관계를 보여준다. 따라서, 이 조사는 SSIM을 손실 함수로 사용하는 것에 대한 정당성을 제공한다. B. 본 발명의 성능 본 발명의 성능을 비교하기 위한 각 모델들은 다양한 대조도로 얻은 자기공명영상으로 다양한 정도의 움직임에 의한 모션 아티팩트를 시뮬레이션하여 생성된 데이터를 입력으로 사용하였고, 학습 데이터와 테스트 데이터의 양도 다르게 활용되었다. 공통적으로 모든 방법들은 시뮬레이션된 모션 아티팩트가 포함된 영상에서 모션 아티 팩트를 비교적 잘 제거하는 양상을 보였다. 비교 모델들은 각각 시각적 화질 차이를 통해 영상의 유사성을 계 산하는 SSIM 인덱스, 혹은 화질 손실 정도를 계산하는 PSNR 인덱스를 통해 수치적으로 평가되었다. 성능 평가 비교군인 Lee 연구진이 발표한 MC2-Net 모델은 다중 대비 영상을 사전 지식 정보로써 사용한 모델로 입력으로 모션 아티팩트가 포함된 영상을 사용하며, 모션 아티팩트가 없는 영상을 출력한다. 이 모델에서 사용 한 다중 대비 영상은 추가적인 영상 획득이 필요하며, 구조적 정보를 참고할 다른 영상 역시 움직임으로 인해 손상되었기 때문에 온전한 구조적 정보를 찾을 수 없다는 한계가 존재한다. 이는 완전한 정보의 손실을 의미하 고, 결국 이 사전 지식 정보로는 온전히 모션 아티팩트를 제거하며 구조적인 정보를 보존할 수 없다. 또한, 정 도가 약한 움직임으로 시뮬레이션된 움직임 데이터로 학습하고 평가되어 비교적 영상 훼손 정도가 작았기 때문 에, 다른 비교군 모델들에 비하여 실험 데이터로 쓰인 T1 강조 영상, T2 강조 영상, FLAIR 영상에서 각각 0.94%, 6.96%, 4.96%의 낮은 SSIM 향상을 얻었다. 학습 데이터나 테스트 데이터 모두 SSIM이 90% 이상인 데이 터를 입력 데이터로써 사용했기 때문에 움직임 손상 정도가 심한 영상에 대해서는 좋은 성능을 기대하기 어렵고, 손상 정도가 미약한 데이터에 한해서만 사용될 수 있다. 그 밖의 다른 비교군으로써 Liu 연구진이 제시한 DRN-DCMB 모델은 3개의 적층 구조를 이용한 모델로 입력으로 모션 아티팩트가 포함된 영상을 사용하며, 모션 아티팩트만을 출력한다. 결론적으로, 입력 데이터에서 출력 데 이터의 차이를 구함으로써 모션 아티팩트를 제거한 깨끗한 영상을 얻어내는 형식이다. 모델의 적층 구조는 서 로의 특징 지도를 공유함으로써 사용되었고, 이 모델은 9.8%의 SSIM 향상을 보여주었다. 이에 반해, 본 발명에서 제안된 모델은 실험 데이터에서 23.37%의 SSIM 향상, 17.27%의 PSNR 향상, 70.02%의 MSE 하락으로 앞선 모델들보다 좋은 성능을 보였다. 특히, 본 발명에 따른 모델은 다양한 정도의 움직임 시뮬 레이션을 통하여 학습 데이터를 생성했기 때문에 다양한 정도의 모션 아티팩트에 대해 적용할 수 있다는 장점을 지녔으며, 수치적인 평가를 바탕으로 구조적인 정보도 비교 모델들보다 잘 보존되었음을 알 수 있다. 또한, MC2-Net 모델을 참고하여 다중 대비 영상에 대한 사전 지식 정보를 이용하는 모델에 대해서도 실험해보았을 때, 24.71%의 SSIM 향상, 12.66%의 PSNR 향상, 55.77%의 MSE 하락을 보였다. 이는 다중 슬라이스를 이용한 모델과 성능이 비슷하고, 이를 위해서는 추가적인 영상 촬영이 필요하다는 단점이 있기 때문에 본 발명에서는 다중 슬 라이스를 이용한 사전 지식 정보 활용 방법을 채택하고 있다. 본 실시예들에 따른 동작은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨 터 판독 가능한 저장 매체에 기록될 수 있다. 컴퓨터 판독 가능한 저장 매체는 실행을 위해 프로세서에 명령어 를 제공하는데 참여한 임의의 매체를 나타낸다. 컴퓨터 판독 가능한 저장 매체는 프로그램 명령, 데이터 파일, 데이터 구조 또는 이들의 조합을 포함할 수 있다. 예컨대, 자기 매체, 광기록 매체, 메모리 등이 있을 수 있다. 컴퓨터 프로그램은 네트워크로 연결된 컴퓨터 시스템 상에 분산되어 분산 방식으로 컴퓨터가 읽을 수 있 는 코드가 저장되고 실행될 수도 있다. 본 실시예를 구현하기 위한 기능적인(Functional) 프로그램, 코드, 및 코드 세그먼트들은 본 실시예가 속하는 기술 분야의 프로그래머들에 의해 용이하게 추론될 수 있을 것이다. 본 실시예들은 본 실시예의 기술 사상을 설명하기 위한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상 의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2022-0122560", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 장치를 설명 하기 위한 블록도이다. 도 2는 본 발명의 바람직한 실시예에 따른 인공신경망 기반 뇌 자기공명영상의 모션 아티팩트 보정 방법을 설명 하기 위한 흐름도이다. 도 3은 본 발명의 바람직한 실시예에 따른 모션 아티팩트의 보정 과정을 설명하기 위한 도면이다. 도 4는 본 발명의 바람직한 실시예에 따른 보정 모델을 이용한 모션 아티팩트의 보정 과정을 보다 자세히 설명 하기 위한 도면이다. 도 5는 본 발명의 바람직한 실시예에 따른 보정 모델의 학습 과정을 설명하기 위한 도면이다. 도 6은 본 발명의 바람직한 실시예에 따른 보정 모델의 일례를 설명하기 위한 도면이다. 도 7은 본 발명의 바람직한 실시예에 따른 손상된 2D 슬라이스 영상과 인접한 2D 슬라이스 영상의 일례를 나타 내는 도면이다. 도 8은 본 발명의 바람직한 실시예에 따른 SSIM 측면에서의 다양한 모션 심각도 레벨(motion severity level) 간의 관계를 나타내는 도면이다. 도 9는 본 발명의 바람직한 실시예에 따른 MSE 측면에서의 다양한 모션 심각도 레벨(motion severity level) 간 의 관계를 나타내는 도면이다. 도 10은 본 발명의 바람직한 실시예에 따른 PSNR 측면에서의 다양한 모션 심각도 레벨(motion severity level) 간의 관계를 나타내는 도면이다. 도 11은 본 발명의 바람직한 실시예에 따른 PerceptualVGG 측면에서의 다양한 모션 심각도 레벨(motion severity level) 간의 관계를 나타내는 도면이다."}
