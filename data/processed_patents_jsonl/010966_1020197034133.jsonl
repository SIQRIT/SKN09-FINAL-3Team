{"patent_id": "10-2019-7034133", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0139991", "출원번호": "10-2019-7034133", "발명의 명칭": "복수의 동일한 다이를 갖는 단일 칩 패키지를 사용하여 신경망 태스크를 처리하기 위한 장치", "출원인": "구글 엘엘씨", "발명자": "다사리 우데이 쿠마르"}}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공 지능(artificial intelligence, AI) 프로세싱 유닛으로서, 복수의 동일한 인공 지능 프로세싱 다이들를 포함하며, 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 각각의 인공 지능 프로세싱 다이는 적어도 하나의 인터-다이입력 블록 및 적어도 하나의 인터-다이 출력 블록을 포함하며,복수의 동일한 인공 지능 프로세싱 다이들 중 각각의 인공 지능 프로세싱 다이는 인공 지능 프로세싱 다이의 상기 적어도 하나의 인터-다이 출력 블록으로부터 인공 지능 프로세싱 다이의 상기 적어도 하나의 인터-다이 입력블록으로의 하나 이상의 통신 경로들에 의해, 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 다른 인공 지능 프로세싱 다이와 통신 가능하게 연결되며, 복수의 동일한 인공 지능 프로세싱 다이들 중 각각의 인공 지능 프로세싱 다이는 신경망의 적어도 하나의 계층에 대응하는 것을 특징으로 하는 인공 지능 프로세싱 유닛."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 하나 이상의 통신 경로들은 길이가 동일한 것을 특징으로 하는 인공 지능 프로세싱 유닛."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 제1 인공 지능 프로세싱 다이는 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 제2 인공 지능 프로세싱 다이에 인접하여 위치되고, 상기 제2 인공 지능 프로세싱 다이의 방향(orientation)은 상기 제1 인공 지능 프로세싱 다이의 방향으로부터 180도 오프셋되는 것을 특징으로 하는 인공 지능 프로세싱 유닛."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 제1 인공 지능 프로세싱 다이는 복수의 동일한 인공 지능 프로세싱 다이들 중 제 인공 지능 프로세싱 다이에 인접하여 위치되고, 상기 제2 인공 지능 프로세싱 다이의 방향은 상기 제1 인공 지능 프로세싱 다이의 방향과 동일한 것을 특징으로 하는 인공 지능 프로세싱 유닛."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 복수의 인공 지능 프로세싱 다이들은 시퀀스로 배열되고 그리고 적어도 하나의 인공 지능 프로세싱 다이는상기 적어도 하나의 인공 지능 프로세싱 다이 보다 상기 시퀀스의 초기 위치에 배열된 다른 인공 지능 프로세싱다이에 입력으로서 데이터를 전송하도록 구성되는 것을 특징으로 하는 인공 지능 프로세싱 유닛."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 각각의 인공 지능 프로세싱 다이는 데이터를 수신하고 그리고 상기 수신된 데이터를 사용하여 AI 연산을 수행하도록 구성되는 것을 특징으로 하는 인공 지능 프로세싱 유닛. 공개특허 10-2019-0139991-3-청구항 7 제6항에 있어서, 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 각각의 인공 지능 프로세싱 다이는 시스톨릭 어레이(systolic array)로 구성되고, 상기 시스톨릭 어레이를 사용하여 상기 AI 연산을 수행하는 것을 특징으로 하는인공 지능 프로세싱 유닛."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 각각의 인공 지능 프로세싱 다이는 상기 인터-다이 입력 블록과는 다른 적어도 하나의 호스트-인터페이스 입력 블록 및 상기 인터-다이 출력 블록과는 다른 적어도 하나의호스트-인터페이스 출력 블록을 포함하는 것을 특징으로 하는 인공 지능 프로세싱 유닛."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 각각의 인공 지능 프로세싱 다이는 상기 호스트-인터페이스입력 블록을 통해 코-프로세싱 유닛으로부터 데이터를 수신하도록 구성되고 그리고 상기 호스트-인터페이스 출력 블록을 통해 코-프로세싱 유닛에 데이터를 전송하도록 구성되는 것을 특징으로 하는 인공 지능 프로세싱 유닛."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서, 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 각각의 인공 지능 프로세싱 다이는 적어도 하나의MAC(multiplier-accumulator) 유닛을 포함하는 것을 특징으로 하는 인공 지능 프로세싱 유닛."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서, 상기 복수의 동일한 인공 지능 프로세싱 다이들 중 각각의 인공 지능 프로세싱 다이는 적어도 메모리를 포함하는 것을 특징으로 하는 인공 지능 프로세싱 유닛."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "방법으로서, 인공 지능(artificial intelligence, AI) 프로세싱 유닛의 제1 인공 지능 프로세싱 다이에서, 신경망과 관련된제1 데이터 세트를 수신하는 단계 -상기 제1 인공 지능 프로세싱 다이는 상기 신경망의 계층과 관련됨-;상기 제1 인공 지능 프로세싱 다이에서, 상기 신경망과 관련된 상기 제1 데이터 세트를 사용하여 상기 제1 인공지능 프로세싱 다이와 관련된 상기 신경망의 계층과 관련된 제1 AI 연산 세트를 수행하는 단계; 상기 인공 지능 프로세싱 유닛의 제2 인공 지능 프로세싱 다이로, 상기 제1 인공 지능 프로세싱 다이에서 수행된 상기 제1 AI 연산 세트로부터의 결과 데이터를 전송하는 단계를 포함하며, 상기 제2 인공 지능 프로세싱 다이는 상기 제1 인공 지능 프로세싱 다이와는 다른 신경망의 계층과 관련되는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 제1 인공 지능 프로세싱 다이는 상기 신경망의 입력 계층과 관련되는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서, 공개특허 10-2019-0139991-4-상기 제2 인공 지능 프로세싱 다이에서, 상기 제1 인공 지능 프로세싱 다이에서 수행된 연산으로부터의 결과 데이터를 이용하여 상기 제2 인공 지능 프로세싱 다이와 연관된 상기 신경망의 계층과 관련된 AI 연산을 수행하는단계;상기 제2 인공 지능 프로세싱 다이에서 수행된 AI 연산으로부터의 결과 데이터를 상기 제1 인공 지능 프로세싱다이에 피드백으로서 전송하는 단계를 더 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 상기 제1 인공 지능 프로세싱 다이 및 상기 제2 인공 지능 프로세싱 다이는 시퀀스로 배열되고, 상기 제1 인공지능 프로세싱 다이는 상기 제2 인공 지능 프로세싱 다이보다 상기 시퀀스의 초기 위치에 배열되는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서, 상기 제1 인공 지능 프로세싱 다이에서, 상기 제2 인공 지능 프로세싱 다이로부터의 피드백으로서 수신된 결과데이터 및 상기 신경망과 관련된 제1 데이터 세트를 이용하여 상기 제1 인공 지능 프로세싱 다이와 관련된 신경망의 계층과 관련된 제2 AI 연산 세트를 수행하는 단계;상기 제2 AI 연산 세트로부터의 결과 데이터를 상기 제2 인공 지능 프로세싱 다이로 전송하는 단계를 더 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제12항에 있어서, 상기 제2 인공 지능 프로세싱 다이는 상기 신경망의 출력 계층과 관련되는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항에 있어서, 상기 제2 인공 지능 프로세싱 다이에서, 상기 제1 인공 지능 프로세싱 다이에서 수행된 연산으로부터의 결과 데이터를 이용하여 상기 신경망의 상기 출력 계층에 관련된 AI 연산을 수행하는 단계;상기 제2 인공 지능 프로세싱 다이에서 수행된 상기 AI 연산으로부터의 결과 데이터를 상기 인공 지능 프로세싱유닛과 통신 가능하게 연결된 코-프로세싱 유닛으로 전송하는 단계를 더 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제12항에 있어서, 상기 제1 인공 지능 프로세싱 다이 및 상기 제2 인공 지능 프로세싱 다이는 적어도 하나의 MAC(multiplier-accumulator) 유닛을 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2019-7034133", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제12항에 있어서, 상기 제1 인공 지능 프로세싱 다이 및 상기 제2 인공 지능 프로세싱 다이는 메모리를 포함하는 것을 특징으로하는 방법."}
{"patent_id": "10-2019-7034133", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "신경망 모델을 처리하기 위한 장치 및 방법이 제공된다. 장치는 복수의 동일한 인공 지능 프로세싱 다이를 포함 한다. 복수의 동일한 인공 지능 프로세싱 다이 중 각각의 인공 지능 프로세싱 다이는 하나 이상의 인터-다이 입 력 블록 및 하나 이상의 인터-다이 출력 블록을 포함한다. 복수의 동일한 인공 지능 프로세싱 다이 중 각각의 인 공 지능 프로세싱 다이는 인공 지능 프로세싱 다이의 적어도 하나의 인터-다이 출력 블록으로부터 인공 지능 프 로세싱 다이의 적어도 하나의 인터-다이 입력 블록으로의 하나 이상의 통신 경로를 통해, 복수의 동일한 인공 지 능 프로세싱 다이들 중 다른 인공 지능 프로세싱 다이와 통신 가능하게 연결된다. 복수의 동일한 인공 지능 프로 세싱 다이들 중 각각의 인공 지능 프로세싱 다이는 신경망의 적어도 하나의 계층에 대응한다."}
{"patent_id": "10-2019-7034133", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 명세서는 복수의 동일한 다이를 갖는 단일 칩 패키지를 사용하여 신경망 태스크를 처리하기 위한 장치 및 메 커니즘에 관한 것이다."}
{"patent_id": "10-2019-7034133", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 지능 컴퓨팅 분야에서 신경망의 사용은 지난 몇 년 동안 빠르게 성장했다. 보다 최근에, ASIC(application specific integrated circuit)과 같은 특정 목적 컴퓨터의 사용이 신경망 처리에 사용되어 왔다. 그러나 ASIC의 사용은 몇 가지 과제를 제기한다. 이러한 과제 중 일부는 긴 디자인 시간 및 무시할 수 없는 비-반복 엔지니어링 비용이다. 신경망의 인기가 높아지고 신경망이 사용되는 태스크의 범위가 증가함에 따라, 긴 디자인 시간과 무시할 수 없는 비-반복적인 엔지니어링 비용이 악화될 것이다. 적어도 하나의 양태는 인공 지능 프로세싱(처리) 유닛에 관한 것이다. 인공 지능 프로세싱 유닛은 복수의 동일 한 인공 지능 프로세싱 다이를 포함한다. 복수의 동일한 인공 지능 프로세싱 다이 중 각각의 인공 지능 프로세 싱 다이는 하나 이상의 인터-다이 입력 블록 및 하나 이상의 인터-다이 출력 블록을 포함한다. 복수의 동일한 인공 지능 프로세싱 다이 중 각각의 인공 지능 프로세싱 다이는 인공 지능 프로세싱 다이의 적어도 하나의 인터 -다이 출력 블록으로부터 인공 지능 프로세싱 다이의 적어도 하나의 인터-다이 입력 블록으로의 하나 이상의 통 신 경로를 통해 복수의 동일한 인공 지능 프로세싱 다이 중 다른 인공 지능 프로세싱 다이와 통신 가능하게 연 결된다. 복수의 동일한 인공 지능 프로세싱 다이 중 각각의 인공 지능 프로세싱 다이는 신경망의 적어도 하나의 계층에 대응한다. 일부 구현들에서, 하나 이상의 통신 경로들은 길이가 동일하다. 일부 구현에서, 복수의 동일한 인공 지능 프로세싱 다이 중 제1 인공 지능 프로세싱 다이는 복수의 동일한 인공 지능 프로세싱 다이 중 제 2 인공 지능 프로세싱 다이에 인접하여 위치하고, 제2 인공 지능 프로세싱 다이의 방 향은 제1 인공 지능 프로세싱 다이의 방향으로부터 180도 오프셋된다. 일부 구현에서, 복수의 동일한 인공 지능 프로세싱 다이 중 제 1 인공 지능 프로세싱 다이는 복수의 동일한 인 공 지능 프로세싱 다이들 중 제 2 인공 지능 프로세싱 다이에 인접하여 위치하고, 제 2 인공 지능 프로세싱 다 이의 방향은 제 1 인공 지능 프로세싱 다이의 방향과 동일하다. 일부 구현에서, 복수의 인공 지능 프로세싱 다이는 순서대로 배열되고 그리고 적어도 하나의 인공 지능 프로세 싱 다이는 적어도 하나의 인공 지능 프로세싱 다이보다 시퀀스의 초기 위치에 배열된 다른 인공 지능 프로세싱 다이에 입력으로서 데이터를 전송하도록 구성된다. 일부 구현에서, 복수의 동일한 인공 지능 프로세싱 다이 중 각각의 인공 지능 프로세싱 다이는 수신된 데이터를 사용하여 데이터를 수신하고 AI 계산(연산)을 수행하도록 구성된다. 일부 구현에서, 복수의 동일한 인공 지능 프로세싱 다이 중 각각의 인공 지능 프로세싱 다이는 시스톨릭 어레이 (systolic array)로 구성되고, 시스톨릭 어레이를 사용하여 AI 계산을 수행한다. 일부 구현에서, 복수의 동일한 인공 지능 프로세싱 다이 중 각각의 인공 지능 프로세싱 다이는 인터-다이 입력 블록과는 다른 적어도 하나의 호스트-인터페이스 입력 블록 및 인터-다이 출력 블록과는 다른 적어도 하나의 호 스트-인터페이스 출력 블록을 포함한다. 일부 구현에서, 복수의 동일한 인공 지능 프로세싱 다이 중 각각의 인공 지능 프로세싱 다이는 적어도 하나의 MAC(multiplier-accumulator) 유닛을 포함한다. 일부 구현에서, 복수의 동일한 인공 지능 프로세싱 다이 중 각각의 인공 지능 프로세싱 다이는 적어도 메모리를 포함한다. 적어도 하나의 양태는 신경망 모델을 처리하는 방법에 관한 것이다. 이 방법은 인공 처리 유닛의 제1 인공 지능 프로세싱 다이에서 네트워크와 관련된 제1 데이터 세트를 수신하는 단계를 포함한다. 제1 인공 지능 프로세싱 다이는 신경망의 계층과 관련된다. 이 방법은 제1 인공 지능 프로세싱 다이에서, 신경망과 관련된 제1 데이터 세트를 사용하여 제1 인공 지능 프로세싱 다이와 관련된 신경망의 계층과 관련된 제1 AI 계산 세트를 수행하는 단계를 포함한다. 이 방법은 인공 지능 프로세싱 유닛의 제2 인공 지능 프로세싱 다이로, 제1 인공 지능 프로세 싱 다이에서 수행된 제1 AI 계산 세트로부터의 결과 데이터를 전송하는 단계를 포함한다. 제2 인공 지능 프로세 싱 다이는 제1 인공 지능 프로세싱 다이와는 다른 신경망의 계층과 관련된다. 일부 구현들에서, 제1 인공 지능 프로세싱(프로세싱) 다이는 신경망의 입력 계층과 관련된다. 일부 구현에서, 이 방법은 제2 인공 지능 프로세싱 다이에서, 제1 인공 지능 프로세싱 다이에서 수행된 계산들 로부터의 결과 데이터를 이용하여 제2 인공 지능 프로세싱 다이와 연관된 신경망의 계층과 관련된 AI 계산을 수 행하는 단계를 포함한다. 이 방법은 제2 인공 지능 프로세싱 다이에서 수행된 AI 계산으로부터의 결과 데이터를 제1 인공 지능 프로세싱 다이에 피드백으로서 전송하는 단계를 포함한다. 일부 구현에서, 제1 인공 지능 프로세싱 다이 및 제2 인공 지능 프로세싱 다이는 순서대로 배열되고, 제1 인공 지능 프로세싱 다이는 제2 인공 지능 프로세싱 다이보다 시퀀스의 초기 위치에 배열된다. 일부 구현에서, 이 방법은, 제1 인공 지능 프로세싱 다이에서, 상기 제2 인공 지능 프로세싱 다이로부터의 피드 백으로서 수신된 결과 데이터 및 상기 신경망과 관련된 제1 데이터 세트를 이용하여 상기 제1 인공 지능 프로세 싱 다이와 관련된 신경망의 계층과 관련된 제2 AI 계산 세트를 수행하는 단계를 포함한다. 이 방법은 제2 AI 계 산 세트로부터의 결과 데이터를 제2 인공 지능 프로세싱 다이로 전송하는 단계를 포함한다. 일부 구현에서, 제2 인공 지능 프로세싱 다이는 신경망의 출력 계층과 관련된다. 일부 구현에서, 이 방법은, 제2 인공 지능 프로세싱 다이에서, 제1 인공 지능 프로세싱 다이에서 수행된 계산으 로부터의 결과 데이터를 이용하여 신경망의 출력 계층에 관련된 AI 계산을 수행하는 단계를 포함한다. 이 방법 은 제2 인공 지능 프로세싱 다이에서 수행된 AI 계산으로부터의 결과 데이터를 인공 지능 프로세싱 유닛과 통신 가능하게 연결된 코-프로세싱 유닛으로 전송하는 단계를 포함한다. 일부 구현에서, 제1 인공 지능 프로세싱 다이 및 제2 인공 지능 프로세싱 다이는 적어도 하나의 MAC 유닛을 포 함한다. 일부 구현들에서, 제1 인공 지능 프로세싱 다이 및 제2 인공 지능 프로세싱 다이는 메모리를 포함한다. 이들 및 다른 양태 및 구현은 아래에서 상세하게 설명된다. 전술한 정보 및 다음의 상세한 설명은 다양한 양태 및 구현의 예시적인 예를 포함하고, 청구된 양태 및 구현의 특성 및 특성을 이해하기 위한 개요 또는 프레임워 크를 제공한다. 도면은 다양한 양태 및 구현에 대한 예시 및 추가 이해를 제공하며, 본 명세서에 포함되어 본 명세서의 일부를 구성한다."}
{"patent_id": "10-2019-7034133", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시는 일반적으로 신경망의 워크로드를 처리하기 위한 장치, 시스템 및 메커니즘에 관한 것이다. 신경망의 효율적인 처리는 주문형 ASIC(application specific integrated circuit)을 이용한다. 그러나 맞춤형 ASIC을 설계하는 데는 긴 설계 시간과 상당한 비-반복 엔지니어링 비용을 포함하여 여러 가지 문제가 있지만 ASIC이 소 량으로 생산될 때 악화되는 비용이 발생한다. 신경망 태스크를 처리하고 하나의 ASIC 칩 패키지로 동일한 다이를 서로 연결하도록 구성된 표준 다이를 설계함 으로써 맞춤형 ASIC 사용의 어려움을 극복할 수 있다. 단일 칩 패키지에서 상호 연결된 다이의 수는 호스트 컴 퓨팅 장치에 의해 처리되는 신경망의 복잡성 또는 계층 수에 따라 달라진다. 복수의 동일한 다이를 갖는 패키지에서, 상이한 다이는 신경망의 상이한 계층과 관련되어, 신경망 관련 태스크를 처리하는 효율을 향상시킨다. 예 상되는 신경망 태스크 수행 빈도에 따라 단일 패키지의 다이 수를 늘리거나 줄임으로써 표준 다이를 여러 제품 에 사용할 수 있으므로 긴 설계 시간 그리고 무시할 수 없는 비-반복 엔지니어링 비용을 보다 효율적으로 상각 할 수 있다. 도 1a는 예시적인 구현에 따른, 신경망의 계산 태스크를 처리하기 위한 시스템을 도시한다. 시스템은 메인 처리 유닛 및 인공 지능 프로세싱(프로세싱) 유닛(AIPU)을 포함한다. 시스템은 호스트 컴 퓨팅 장치(미도시) 내에 수용된다. 호스트 컴퓨팅 장치의 예는 서버 및 사물 인터넷(IoT) 장치를 포함하지만 이 에 제한되지는 않는다. AIPU는 메인 처리 유닛의 코-프로세싱 유닛이다. 메인 처리 유닛은 버스 와 같은 통신 시스템의 일부인 통신 경로(104a, 104b)와 같은 하나 이상의 통신 경로를 통해 AIPU에 통신 가능하게 연결된다. 메인 처리 유닛은 제어기 및 메모리를 포함한다. 메모리는 메인 프로 세싱 유닛의 서브 프로세싱(처리) 유닛 및 메인 처리 유닛에 연결된 코 프로세싱 유닛과 관련된 구성 데이터를 저장한다. 예를 들어, 메모리는 AIPU와 관련된 구성 데이터를 저장할 수 있다. 메인 프로세 싱 유닛 제어기는 메모리에 통신 가능하게 연결되고 메모리로부터 구성 데이터를 선택하고 그리 고 메인 프로세싱 유닛에 연결된 코-프로세싱 유닛 또는 메인 프로세싱 유닛의 서브 프로세싱 유닛으 로 구성 데이터를 전송하도록 구성된다. 메인 프로세싱 유닛 제어기에 의한 구성 데이터의 선택 및 전송에 대한 추가 세부 사항은 도 3을 참조하여 아래에 설명된다. AIPU는 신경망의 계산 태스크를 처리하도록 구성된다. AIPU는 본 명세서에서 AIPD로 총칭되는 복수의 인공 지능 프로세싱 다이(AIPD) (103a, 103b, 103c, 103d, 103e, 103f)를 포함한다. AIPD는 서로 동일하다. 본 명세서에 기술된 바와 같이, 각 AIPD가 동일한 다이 설계를 사용하여 제조되고 각 AIPD(10 3)상의 하드웨어 유닛의 구현이 다른 AIPD와 동일한 경우, AIPD는 다른 AIPD와 \"동일\"하다. 따 라서, 본 개시에서, 2개의 AIPD는 다이의 설계 및 2개의 AIPD의 하드웨어 유닛의 구현이 동일하다면 여전히 신경망의 상이한 계층을 처리하도록 구성될 수 있다. AIPU에 포함된 AIPD의 수는 호스트 컴퓨 팅 장치에 의해 처리된 신경망 모델의 계층 수에 기초하여 변할 수 있다. 예를 들어, 호스트 컴퓨팅 장치가 스 마트 서모스탯(thermostat)과 같은 사물 인터넷(IoT) 장치인 경우, 스마트 서모스탯의 AIPU에 의해 처리되 는 신경망 모델의 계층 수는 데이터 센터의 서버와 같은 데이터 센터에서 호스트 컴퓨팅 장치의 AIPU에 의 해 처리된 신경망 모델의 계층 수보다 적을 것이다. 심플 신경망 모델을 처리하는 호스트 컴퓨팅 장치에서, 단일 AIPD는 호스트 컴퓨팅 장치의 신경망 관련 태 스크를 효율적으로 처리할 수 있다. 더 복잡한 신경망 모델 또는 다중 계층을 가진 신경망 모델을 처리하는 호 스트 컴퓨팅 장치에서, 복수의 동일한 AIPD는 신경망 관련 태스크를 효율적으로 처리하는데 유용 할 수 있 다. 따라서, 일부 구현들에서, AIPU는 단일 AIPD를 포함하는 반면, 다른 구현들에서, AIPU는 복 수의 동일한 AIPD들을 포함한다. AIPU가 도 1a에 도시된 것과 같은 복수의 동일한 AIPD를 포함하는 구현에서, 각각의 동일한 AIPD는 다른 동일한 AIPD에 연결된다. 또한, 각각의 AIPD는 AIPU에 의해 처리되는 신경망 의 적어도 하나의 계층과 관련된다. AIPD의 추가 세부 사항 및 AIPU 내에서 복수의 동일한 AIPD(10 3)의 배열은 도 1b, 2a, 2b를 참조하여 아래에 설명된다. 도 1b를 참조하면, AIPD의 구현의 기능 로직이 도시되어 있다. 명확한 예를 제공하기 위해, AIPD(103a)의 기능적 로직만이 도 1b에 도시되어 있지만, 각각의 AIPD는 서로 동일하므로, 당업자는 AIPD(103b, 103c, 103d, 103e, 103f)의 기능 로직이 AIPD(103a)의 기능 로직과 동일하다는 것을 이해할 것이다. AIPD(103a)는 호 스트 인터페이스 유닛, 버퍼, 제어기, 버퍼, 계산 유닛, 인터-다이 입력 블록(109a, 109b) 및 인터-다이 출력 블록(111a, 111b)을 포함한다. 호스트 인터페이스 유닛은 적어도 하나의 입력/출력(I/O) 블록(도시되지 않음)을 포함한다. I/O 블록에는 여러 개의 I/O 핀(미도시)이 포함되어 있다. 호스트 인터페이스 유닛의 I/O 블록의 I/O 핀은 양방향으로 구성되어 I/O 블록은 소스 유닛으로부터 데이터를 수신하고 목적지 유닛으로 데이터를 전송할 수 있다. 소스 및 목적지 유닛의 예는 메모리 유닛, 메인 프로세싱 유닛의 코-프로세서, 또는 데이터를 송신 또는 수신하도 록 구성된 다른 집적 회로 컴포넌트를 포함하지만 이에 제한되지는 않는다. 호스트 인터페이스 유닛은 호 스트 인터페이스 유닛의 I/O 핀을 통해 메인 프로세싱 유닛 제어기로부터 데이터를 수신하고 그리고 데이터를 메인 프로세싱 유닛 제어기, 메인 프로세싱 유닛 자체 또는 호스트 인터페이스 유닛의 I/O 핀을 통해 메모리에 직접 전송하도록 구성된다. 호스트 인터페이스 유닛은 메인 프로세싱 유닛제어기로부터 수신된 데이터를 버퍼에 저장한다. 버퍼는 데이터 저장을 위한 레지스터, 동적 랜덤 액세스 메모리(DRAM), 정적 랜덤 액세스 메모리(SRAM), 또는 다른 유형의 집적 회로 메모리와 같은 메모리를 포함한다. AIPD 제어기는 버퍼로부터 데이터를 검색하고 버퍼에 데이터를 저장하도록 구성된다. AIPD 제어기는 메인 프로세싱 유닛 제어기로부 터 전송된 데이터에 부분적으로 기초하여 동작하도록 구성된다. 메인 프로세싱 유닛 제어기로부터 전송된 데이터가 구성 데이터이면, 구성 데이터에 기초하여, AIPD 제어기는 AIPD(103a)와 다른 AIPD 사이의 통신에 사용될 인터-다이 입력 및 출력 블록을 선택하도록 구성된다. AIPD들 사이의 통신에 대한 추가 세 부 사항은 도 2a, 2b 및 2c를 참조하여 아래에 설명된다. 메인 프로세싱 유닛 제어기로부터 전송된 데이터 가 신경망 태스크를 수행하기 위한 명령(명령어들)이면, AIPD 제어기는 신경망과 관련된 데이터를 버퍼 에 저장하고, 버퍼에 저장된 입력 데이터 및 계산 유닛을 사용하여 신경망 태스크를 수행하도록 구성된다. 버퍼는 데이터 저장을 위한 레지스터, DRAM, SRAM 또는 다른 유형의 집적 회로 메모리와 같은 메모리를 포함한다. 계산 유닛은 MAC들(도시되지 않음), 다중 산술 로직 유닛(Arithmetic Logic Unit, ALU)들(도시되지 않음), 다중 시프트 레지스터(도시되지 않음)들 등을 포함한다. 버퍼들의 레지스터들 중 일부는 계산 유닛의 복수의 ALU에 결합되어, 입력 값이 한 번 판독되고 후속 연산에서 입력으로서 사용하 기 전에 결과를 저장하지 않고 복수의 상이한 연산에 사용될 수 있는 시스톨릭 어레이를 설정한다. 이러한 시스 톨릭 어레이의 예시적인 배열이 도 1c에 도시되어 있다. 도 1c에서, 레지스터는 버퍼에 포함되고 레지스터로부터의 데이터는 ALU(132a)에서의 제1 연산 을 위한 입력이다. ALU(132a)의 결과는 ALU(132b)로의 입력이고, ALU(132b)의 결과는 ALU(132c)로의 입력이고, ALU(132c)의 결과는 ALU(132d)로의 입력이다. 이러한 배열 및 구성은 AIPD를 범용 컴퓨터와 구별하는데, 이는 일반적으로 그 결과 데이터를 다시 사용하기 전에 하나의 ALU로부터의 결과 데이터를 저장 유닛에 다시 저 장한다. 도 1c에 도시된 배열은 또한 컨벌루션, 행렬 곱셈, 풀링, 요소별 벡터 연산 등과 같은 인공 지능 태스 크(본 명세서에서 \"AI 계산(연산)\"으로 지칭됨) 수행과 관련된 계산을 위해 AIPD를 최적화한다. 또한, 도 1c에 도시된 배열을 구현함으로써, AIPD는 AI 계산을 수행할 때 전력 소비 및 크기에 대해 더욱 최적화되 어 AIPU의 비용을 감소시킨다. 도 1b를 다시 참조하면, 계산 유닛은 신경망에 대해 선택되고 가중치 메모리 유닛(도시되지 않음)으로부터 전송된 가중치 및 입력 데이터를 사용하여 AI 계산을 수행한다. 일부 구현들에서, 계산 유닛은 활성화 유 닛을 포함한다. 활성화 유닛은 복수의 ALU 및 복수의 시프트 레지스터를 포함할 수 있고, AI 계산 결 과에 활성화 함수 및 비선형 함수를 적용하도록 구성될 수 있다. 활성화 유닛에 의해 적용되는 활성화 함 수 및 비선형 함수는 하드웨어, 펌웨어, 소프트웨어 또는 이들의 조합으로 구현될 수 있다. 계산 유닛은 활성화 함수 및/또는 다른 비선형 함수를 적용한 후 생성된 데이터를 버퍼에 전송하여 그 데이터를 저장한 다. AIPD 제어기는 인터-다이 통신을 위해 구성된 인터-다이 출력 블록을 사용하여 버퍼에 저장된 계 산 유닛으로부터의 출력 데이터를 AIPD(103a)에 통신적으로 연결된 AIPD로 전송한다. 메인 프로세싱 유닛 제어기로부터 수신된 구성 데이터는 두 AIPD들 사이의 인터-다이(inter-die) 통신 경로를 결정 한다. 예를 들어, AIPD(103a)에서 수신된 구성 데이터가 (도 1b에 도시된 바와 같이) 인터-다이 출력 블록 (111a)이 인터-다이 통신을 위해 사용되어야 한다는 것을 나타내는 경우, AIPD 제어기는 인터-다이 출력 블록(111a)을 사용하여 다른 AIPD로 데이터를 전송한다. 유사하게, 구성 데이터가 (도 1b에 도시된 바와 같이) 입력 블록(109b)이 인터-다이 통신에 사용될 것임을 나타내면, 그 후, AIPD 제어기는 다른 AIPD로부터 데이터를 수신하기 위한 인터-다이 입력 블록으로서 입력 블록(109b)을 선택하고, 입력 블록 (109b)에서 수신된 데이터를 판독 및 처리한다. AIPD의 각 인터-다이 입력 및 출력 블록은 복수의 핀을 포함한다. AIPD의 인터-다이 출력 블록의 핀 은 전기적 상호 접속에 의해 다른 AIPD의 인터-다이 입력 블록의 해당 핀에 연결될 수 있다. 예를 들어, 도 2a에 도시된 바와 같이, AIPD(103a)의 출력 블록(111a)의 핀은 전기적 상호 연결에 의해 AIPD(103b)의 입력 블록에 연결된다. 인터-다이 출력 블록의 핀과 다른 AIPD 의 입력 블록 사이의 전기적 상호 연결은 길이가 동일하다. 하나의 AIPD의 인터-다이 출력 블록과 다른 AIPD의 인터-다이 입력 블록 사이의 연결은 전기적 상호 연결에 의해 연결되지만, AIPD의 특정 인터-다이 출력 블록의 선택 및 인터-다이 출력 블록의 특정 핀으로 의 특정 신호 또는 데이터의 전송은 메인 프로세싱 제어기로부터 AIPD에 의해 수신된 구성 데이터에 기초하여 프로그램 가능하거나 수정될 수 있다. AIPD의 다른(상이한) 출력 블록의 선택을 통해, AIPU(10 2)는 신경망의 상이한 계층들 사이의 피드백 루프를 포함하지만 이에 제한되지 않는 상이한 신경망의 상이한 요구사항(요구조건)을 구현하도록 구성될 수 있다. 따라서, 동일한 AIPU를 사용하여 다양한 신경망 세트를 실행할 수 있어, 설계 시간 비용이 감소하고 비-반복(non-recurring) 엔지니어링 비용이 효율적으로 상각된다. AIPD 및 AIPU의 구성에 대한 추가 세부 사항은 도 2a, 도 2b 및 도 3을 참조하여 아래에 설명된다. 전술한 바와 같이, 복수의 AIPD 중 각각의 AIPD는 AIPU가 처리하도록 구성된 신경망의 적어도 하나의 계층과 관련된다. 메인 프로세싱 유닛은 AIPU 및 AIPU, 예컨대 AIPU를 구성하기 위한 구 성 데이터(configuration data)를 포함한다. 구성 데이터는 AIPU에 의해 처리되도록 선택된 신경망 모델과 연관 되어 있다. 구성 데이터는 AIPU에 의해 처리되는 신경망의 계층과 AIPD 간의 연관성을 특정한다. AIPU에 의해 처리되는 신경망과 관련된 구성 데이터에 기초하여, 메인 프로세싱 유닛 제어기는 AIPD를 신경 망의 계층과 연관시킨다. 일부 구현들에서, 메인 프로세싱 유닛 제어기는 AIPD와 신경망의 계층 간의 연관성(association)을 메모리(도 1a에 도시됨)와 같은 저장 장치에 저장한다. 메인 프로세싱 유닛 제어기 는 AIPD와 관련된 구성 데이터를 해당 AIPD로 전송한다. AIPD와 신경망의 계층과의 연관성 (관련성)은 AIPU에 의해 처리되는 신경망 모델의 요구사항에 부분적으로 기초한다. 예를 들어, 신경망이 신경망의 2개의 계층들 사이에 피드백 루프를 포함한다면, 이들 2개의 계층과 관련된 AIPD는 제1 AIPD의 인터-다이 출력 블록과 제2 AIPD의 인터-다이 입력 블록이 전기적으로 상호 연결되어 있는지 에 기초하여 선택될 수 있다. 복수의 AIPD의 이러한 구성의 예는 도 2a를 참조하여 아래에 설명된다. 도 2a는 AIPU와 같은 AIPU 내의 복수의 AIPD의 예시적인 배열을 도시한다. 도 2a에서, AIPU는 6개의 AIPD(AIPD(103a, 103b, 103c, 103d, 103e, 103f))를 포함하고 신경망의 제1 계층과 마지막 계층 사 이의 피드백 루프를 포함하는 6개의 계층으로 신경망을 처리하고 있다. AIPD(103a)는 인터-다이 입력 블록 (109a, 109b), 인터-다이 출력 블록(111a, 111b) 및 호스트 인터페이스 유닛을 포함한다. AIPD(103b)는 인터-다이 입력 블록(221a, 221b), 인터-다이 출력 블록(223a, 223b) 및 호스트 인터페이스 유닛을 포함한 다. AIPD(103c)는 인터-다이 입력 블록(225a, 225b), 인터-다이 출력 블록(227a, 227b) 및 호스트 인터페이스 유닛을 포함한다. AIPD(103d)는 인터-다이(inter-die) 입력 블록(229a, 229b), 인터-다이 출력 블록 (231a, 231b) 및 호스트 인터페이스 유닛을 포함한다. AIPD(103e)는 인터-다이 입력 블록(233a, 233b), 인터-다이 출력 블록(235a, 235b) 및 호스트 인터페이스 유닛을 포함한다. AIPD(103f)는 인터-다이 입력 블록(237a, 237b), 인터-다이 출력 블록(239a, 239b) 및 호스트 인터페이스 유닛을 포함한다. 각각의 AIPD는 신경망의 특정 계층과 관련되고, 상술된 바와 같이, 신경망의 계층과 AIPD의 연관성은 부분적으로 신경망의 계층과 관련된 특징에 기초한다. 도 2a의 신경망은 신경망의 제1(첫 번째) 계층과 마지막 계층 사이에 피드백 루프가 필요하기 때문에, 신경망의 마지막 계층 및 제1 계층은 AIPD와 연관되어야 하 고, 상기 신경망의 마지막 계층과 관련된 AIPD의 인터-다이 출력 블록은 신경망의 제1 계층과 연관된 AIPD의 인터-다이 입력 블록과 전기적으로 상호 연결된다. 도 2a에 도시된 바와 같이, AIPD(103d)의 인터- 다이 출력 블록(231a)이 AIPD(103a)의 인터-다이 입력 블록(109b)에 전기적으로 상호 연결되어 있기 때문에, 이 러한 구성은 AIPD(103a)를 제1 계층과 관련(연관)시키고 AIPD(103d)를 제6 계층과 관련(연관)시킴으로써 달성될 수 있다. 따라서, AIPD(103b, 103c, 103f, 103e)는 각각 신경망의 제2, 제3, 제4 및 제5 계층과 관련된다. 도 2a에서 AIPD의 배열 시퀀스는 AIPD(103a)가 그 시퀀스의 제1 위치에 있고, AIPD(103b)는 제2 위치에 있고, AIPD(103c)는 제3 위치에 있고, AIPD(103f)는 제4 위치에 있으며, AIPD(103e)는 제5 위치에 있고, AIPD(103d)는 제6 위치에 있고, 이어서 AIPD(103a)는 제7 위치에 있다는 것이다. 201a, 201b, 201c, 201d, 201e, 201f에 의해 지시된 바와 같이, AIPD들 사이의 신경망 관련 데이터의 통신 시퀀스는 신경망의 제1 계층과 제6 계층 사이에 피드백 계층을 포함시키기 위해 103a에서 103b로, 이어서 103c, 103f, 103e, 103d로, 그리고 다시 103a로 다시 시작한다. 본 명세서에 설명된 바와 같이, \"신경망 관련 데이터\"는 계산 유닛의 출력, 파라미터 가중치 데이터 및 다른 신경망 파라미터 관련 데이터와 같은 계산 결과 데이터를 포함하지만, 이에 제한되지는 않는다. 신경망의 출력 계층과 연관된 AIPD의 AIPD 제어기는 결과 데이터를 출력 계층으로부터 메인 프로세싱 유닛 으로 전송하도록 구성된다. 예를 들어, 출력 계층과 연관된 AIPD가 103d이면, AIPD 제어기는 결과 데 이터를 AIPD(103d)로부터 메인 프로세싱 유닛으로 전송하도록 구성된다. 일부 구현들에서, 단일 AIPD(10 3)는 메인 프로세싱 유닛으로부터 신경망의 초기 입력 데이터를 수신하고 그리고 그 결과 데이터를 신경망 의 마지막 계층으로부터 메인 프로세싱 유닛으로 전송하도록 구성된다. 예를 들어, 도 2a에서, AIPD(103 a)가 메인 프로세싱 유닛으로부터 신경망의 초기 입력 데이터 및 AIPD(103d)로부터의 결과 데이터를 수신 하면, AIPD는 신경망의 마지막 계층과 관련되고, AIPD(103a)의 AIPD 제어기는 인터-다이 입력 블록(111b) 에서 수신된 AIPD(103d)로부터의 결과 데이터를 메인 프로세싱 유닛으로 전송하도록 구성될 수 있다. 전술한 동일한 AIPD를 사용하여도 2a를 참조하여 설명된 신경망과 다른 신경망을 처리할 수 있다. 예를 들 어, 신경망이 상기 신경망의 제6 계층과 제3 계층 사이에 피드백 루프를 갖는 경우, 제6 계층과 제3 계층은 AIPD와 관련되어야 하며, 신경망의 제6 계층과 연관된 AIPD의 인터-다이 출력 블록은 신경망의 제3 계층과 관련된 AIPD의 인터-다이 입력 블록과 전기적으로 상호 연결된다. 부가적으로, 신경망의 상이한 계 층들과 연관된 각각의 AIPD는 신경망의 후속 계층과 연관된 다른 AIPD의 하나 이상의 인터-다이 입력 블록과 전기적으로 상호 연결된 하나 이상의 인터-다이 출력 블록을 가져야 한다. 예를 들어, 제1 계층과 연관 된 AIPD는 신경망의 제2 계층과 연관된 AIPD의 인터-다이 입력 블록과 전기적으로 상호 연결된 인터- 다이 출력 블록을 가져야하고; 제2 계층과 연관된 AIPD는 신경망의 제3 계층과 연관된 AIPD의 인터- 다이 입력 블록과 전기적으로 상호 연결된 인터-다이 출력 블록을 가져야 하고; 제3 계층과 관련된 AIPD는 신경망의 제4 계층과 관련된 AIPD의 인터-다이 입력 블록과 전기적으로 상호 연결된 인터-다이 출력 블록 을 가져야 하고; 제4 계층과 연관된 AIPD는 신경망의 제5 계층과 연관된 AIPD의 인터-다이 입력 블록 과 전기적으로 상호 연결된 인터-다이 출력 블록을 가져야 하고; 그리고 제5 계층과 연관된 AIPD는 신경 망의 제6 계층과 연관된 AIPD의 인터-다이 입력 블록과 전기적으로 상호 연결된 인터-다이 출력 블록을 가 져야 한다. 이러한 신경망의 처리는 도 2b의 AIPD의 배열(구성)을 사용하여 달성될 수 있다. 도 2b는 AIPU 내의 AIPD의 다른 예시적인 배열을 도시한다. 도 2b에서, AIPU는 AIPD(103a, 103b, 103c, 103d, 103e, 103f)를 포함한다. AIPU 내에서, AIPD(103a)의 인터-다이 출력 블록(111a)은 AIPD(103b)의 인터-다이 입력 블록(221a)과 전기적으로 상호 연결되고 그리고 AIPD(103a)의 인터-다이 출력 블 록(111b)은 AIPD(103d)의 인터-다이 입력 블록(229a)과 전기적으로 상호 연결되고; AIPD(103b)의 인터-다이 출 력 블록(223b)은 AIPD(103a)의 인터-다이 입력 블록(109b)과 전기적으로 상호 연결되고; AIPD(103b)의 인터-다 이 출력 블록(223a)은 AIPD (103c)의 인터-다이 입력 블록 (225a)에 전기적으로 상호 연결되고; AIPD (103c)의 인터-다이 출력 블록(227b)은 AIPD(103f)의 인터-다이 입력 블록(237a)에 전기적으로 상호 연결되고; AIPD(103f)의 인터-다이 출력 블록(239a)은 AIPD(103c)의 인터-다이 입력 블록(225b)과 전기적으로 상호 연결되 고 그리고 AIPD(103f)의 인터-다이 출력 블록(239b)은 AIPD(103e)의 인터-다이 입력 블록(233b)에 전기적으로 상호 연결되고; AIPD(103e)의 인터-다이 출력 블록(235a)은 AIPD (103b)의 인터-다이 입력 블록(221b)과 전기 적으로 상호 연결되고 그리고 AIPD(103e)의 인터-다이 출력 블록(235b)은 AIPD(103d)의 인터-다이 입력 블록 (229b)과 전기적으로 상호 연결되고; AIPD(103d)의 인터-다이 출력 블록(231a)은 AIPD(103e)의 인터-다이 입력 블록(233a)과 전기적으로 상호 연결된다. 도 2b에서, AIPD(103f)는 신경망의 제6 계층과 관련되고 그리고 AIPD(103e)는 신경망의 제3 계층과 관련된다. AIPD들(103a, 103d, 103b, 103c)은 각각 신경망의 제1, 제2, 제4 및 제5 계층과 관련된다. AIPD 제어기는 AIPD(103d)의 인터-다이 입력 블록(229a)에 전기적으로 상호 연결된 AIPD(103a)의 인터-다이 출력 블록(111b)을 이용하여, AIPD(103a)에서의 계산으로부터의 결과 데이터를 신경망의 제2 계층과 관련된 AIPD인 AIPD(103d)로 전송하도록 구성된다. AIPD(103d)의 AIPD 제어기는 AIPD(103e)의 인터-다이 입력 블록 (233a)에 전기적으로 상호 연결된 인터-다이 출력 블록(231a)을 사용하여, AIPD(103d)로부터의 결과 데이터를 신경망의 제3 계층과 관련된 AIPD(103인 AIPD(103e)로 전송하도록 구성된다. AIPD(103e)의 AIPD 제어기는 AIPD(103b)의 인터-다이 입력 블록(221b)에 전기적으로 상호 연결된 AIPD(103e)의 인터-다이 출력 블록(235a)을 사용하여, AIPD(103e)로부터 신경망의 제4 계층과 관련된 AIPD인 AIPD(103b)로 결과 데이터를 전송하도록 구성된다. AIPD(103b)의 AIPD 제어기는 AIPD(103c)의 인터-다이 입력 블록(225a)에 전기적으로 상호 연결 된 인터-다이 출력 블록(223a)을 사용하여 AIPD(103b)로부터 신경망의 제5 계층과 관련된 AIPD인 AIPD(103c)로 결과 데이터를 전송하도록 구성된다. AIPD(103c)의 AIPD 제어기는 AIPD(103f)의 인터-다이 입력 블록(237a)에 전기적으로 상호 연결된 AIPD(103c)의 인터-다이 출력 블록(227b)을 사용하여, AIPD(103c)로 부터 신경망의 제6 계층과 관련된 AIPD인 AIPD(103f)로 결과 데이터를 전송하도록 구성된다. AIPD 제어기 는, AIPD(103e)의 인터-다이 입력 블록(233b)에 전기적으로 상호 연결된 AIPD(103f)의 인터-다이 출력 블 록(239b)을 사용하여, AIPD(103f)로부터 신경망의 제3 계층과 관련된 AIPD인 AIPD(103e)로 피드백 데이터 를 전송하도록 구성된다. AIPD(103f)의 AIPD 제어기는 AIPD(103f)가 신경망의 출력층과 연관되어있는 경우 결과 데이터를 AIPD(103f)로부터 메인 프로세싱 유닛으로 전송하도록 추가로 구성된다. 도 2b에서 AIPD의 배열 시퀀스는 AIPD(103a)가 시퀀스의 제1 위치에 있고, AIPD(103d)는 제2 위치에 있으며, AIPD(103e)는 제3 위치에 있으며, AIPD(103b)는 제4 위치에 있으며, AIPD(103c)는 제5 위치에 있으며, AIPD(103f)는 제6 위치에 있고, AIPD(103e)는 제7 위치에 있다는 것이다. 202a, 202b, 202c, 202d, 202e, 202f 에 의해 지시된 바와 같이, AIPD들 사이의 도 2b의 신경망 관련 데이터의 통신 시퀀스는 103a부터 103d, 103e, 103b, 103c, 103f로 시작한 다음 피드백 데이터를 103e로 시작한다.따라서 동일한 AIPD를 사용하여 다른 신경망 요구사항을 갖는 다른 신경망을 처리할 수 있다. 따라서, 단일 인 공 지능 프로세싱 다이(AIPD)의 설계는 상이한 요구사항을 갖는 상이한 신경망의 처리 및 실행에 이용될 수 있 어서, 설계 시간 관련 비용의 감소 및 비-반복 엔지니어링 비용의 효율적인 상각을 초래한다. 또한, AIPU와 관련된 구성 데이터 및/또는 그 AIPU의 AIPD와 관련된 구성 데이터를 수정함으로써, 단일 AIPU는 상이한 신경망을 처리하기 위해 이용될 수 있다. 예를 들어, 도 2b에서, 4개의 계층을 갖는 신경망이 AIPU(25 0)에 의해 처리(프로세싱)된다면, AIPU와 관련된 구성 데이터 및/또는 AIPU의 AIPD와 관련된 구 성 데이터는 AIPD(103a)를 신경망의 제1 계층과 연관시키고, AIPD(103b)를 신경망의 제2 계층과 연관시키고, AIPD(103c)를 신경망의 제3 계층과 연관시키고, AIPD(103f)를 신경망의 제4 계층과 연관시키도록 수정될 수 있 다. 이들 AIPD의 인터-다이 입력 블록과 인터-다이 출력 블록 사이의 전기적 상호 연결은 위에서 설명되었 다. AIPU과 AIPU의 AIPD가 재구성되면, 메인 프로세싱 유닛 제어기는 신경망과 관련된 입 력 데이터를 신경망의 제1 계층인 AIPD(103a)와 연관된 AIPD로 전송한다. AIPD(103a)와 관련된 수정된 구성 데 이터 및 신경망으로의 입력 데이터에 기초하여, AIPD(103a)는 AI 계산을 포함하여 새로운 신경망의 제1 계층과 관련된 계산(연산)을 수행하고, 그 결과 데이터를 인터-다이 출력 블록(111a)을 사용하여 AIPD(103b)에 전송한 다. 본 명세서에 설명된 바와 같이, \"신경망의 계층과 관련된 계산\"은 신경망의 그 계층과 관련된 AI 계산을 포 함한다. AIPD(103b)는 인터-다이 입력 블록(221a)에서 AIPD(103a)로부터 수신된 결과 데이터 및 AIPD(103b)와 관련된 수정된 구성 데이터에 기초하여, AI 계산을 포함하는 신경망의 제2 계층에 관련된 계산을 수행한다. AIPD(103b)는 인터-다이 출력 블록(223a)을 이용하여 결과 데이터를 AIPD(103c)로 전송한다. AIPD(103c)는 인 터-다이 입력 블록(225a)에서 AIPD(103b)로부터 수신된 결과 데이터 및 AIPD(103c)와 관련된 수정된 구성 데이 터에 기초하여, AI 계산을 포함하는 신경망의 제3 계층에 관련된 계산을 수행하고, 인터-다이 출력 블록(227b) 을 사용하여 결과 데이터를 AIPD(103f)에 전송한다. AIPD(103f)는 인터-다이 입력 블록(237a)에서 AIPD(103c) 로부터 수신된 결과 데이터 및 AIPD(103f)와 연관된 수정된 구성 데이터에 기초하여, AI 계산을 포함하는 신경 망의 제4 계층에 관련된 계산을 수행한다. 신경망의 마지막 계층과 관련된 AIPD인 AIPD(103f)는 AIPD(103f)로부터의 결과 데이터를 메인 프로세싱 유닛으로 전송하도록 구성된다. 따라서, AIPU와 관련된 구성 데이터 및/또는 AIPU의 AIPD의 구성 데이터를 수정함으로써, 단일 신경 AIPU가 다른 신경망을 처리하도록 재프로그램될 수 있다. 따라서, 커스텀 ASIC의 사용과 관련된 무시할 수 없는 비-반복 엔지니어링 비용을 보다 효율적으로 상쇄하고, 이 특정 신경망의 태스크를 처리하기 위한 커스텀 ASIC의 설계와 관련된 설계 시간 비용 을 추가로 감소시킨다. 일부 구현들에서, 적어도 하나의 인터-다이 입력 블록 및 적어도 하나의 인터-다이 출력 블록은 AIPD의 일 에지에 배치되고, 적어도 하나의 인터-다이 출력 블록 및 적어도 하나의 인터-다이 입력 블록은 AIPD 103의 다 른 에지에 위치된다. 예를 들어, 도 2a와 같이, 하나의 인터-다이 입력 블록 및 하나의 인터-다이 출력 블록은 AIPD의 상부 에지에 위치하고, 그리고 다른 하나의 인터-다이 출력 블록 및 인터-다이 입력 블록은 AIPD의 하부 에지에 위치된다. 일부 구현들에서, 모든 인터-다이 입력 블록들은 AIPD의 일 에지에 위 치하고 모든 인터-다이 출력 블록들은 AIPD의 다른 에지에 위치된다. 인터-다이 입력 및 출력 블록의 이러 한 구성의 예가 도 2c에 도시되어 있다. 도 2C에서, 모든 인터-다이 입력 블록은 AIPD의 상부 에지(top edge)에 위치하고 모든 인터-다이 출력 블 록은 AIPD의 하부 에지(bottom edge)에 위치한다. 일부 구현에서, 도 2c에 도시된 바와 같이, AIPD들 사이에 동일한 길이의 전기 상호 연결을 구현하고 AIPD들을 포함하는 AIPU에 대해보다 효율적인 크 기를 달성하기 위해, AIPD들의 일부의 방향(배향)은 다른 AIPD들의 방향에 대해 특정 거리 또는 도 (degree)만큼 오프셋된다. 예를 들어, 도 2c에 도시된 바와 같이, AIPD(103b, 103e)는 AIPD(103a, 103d, 103c, 103f)의 방향에 대해 180도 회전된다. AIPD를 180도로 회전시킴으로써, AIPD(103b, 103e)의 인터-다이 입 력 및 출력 블록은 AIPD(103a, 103d, 103c, 103f)의 인터-다이 출력 및 입력 블록에 인접하여 위치하며, 이는 모든 AIPD들 사이의 전기적 상호 연결(접속) 길이가 동일한 길이가 되도록하고 그리고 AIPD들(103b 또는 103e)의 인터-다이 입력 및 출력 블록과 임의의 인접한 AIPD 사이의 전기적 상호 연결을 위한 추가 영역이 필요하지 않다. 도 2c에서, 도 2c에 도시된 AIPD들의 배열(구성)을 갖는 AIPU는 상술 한 AIPU와 유사한 신경망을 처리할 수 있다. 예를 들어, AIPD(103a)를 제1 계층과 연관시키고, AIPD(103d)를 제2 계층과 연관시키고, AIPD(103e) 를 제3 계층과 연관시키고, AIPD(103b)를 제4 계층과 연관시키고, AIPD(103c)를 제5 계층과 연관시키고, AIPD(103f)를 제6 계층과 연관시킴으로써, 6개의 계층을 갖고 그리고 계층들 사이에 피드백 루프가 없는 신경망 이 도 2c에 도시된 AIPD들의 배열(구성)에 의해 처리될 수 있다. 도 2c에서 AIPD의 배열 시퀀스는AIPD(103a)가 시퀀스의 제1 위치에 있고, AIPD(103d)가 제2 위치에 있고, AIPD(103e)가 제3 위치에 있고, AIPD(103b)가 제4 위치에 있고, AIPD(103c)가 제5 위치에 있고, AIPD(103f)가 제6 위치에 있다는 것이다. AIPD 들 사이의 통신 시퀀스는 AIPD(103a)에서 시작하여 AIPD 103d, 103e, 103b, 103c, 103f로 시작한다. 본원에 기술된 AIPD의 설계 및 구현의 이점 중 하나는 복수의 AIPD가 단일 AIPU 패키지 내에 포함될 수 있다는 것이다. 단일 AIPU 패키지 내의 AIPD 수는 AIPU 패키지의 크기에 의해서만 제한되며 AIPD의 다이 크기는 아니다. 따라서, 단일 AIPU 패키지에서, 도 2d에서 AIPD 11 내지 AIPD NN의 배열에 의해 도시된 바와 같이, AIPD의 N × N 배열이 포함될 수 있다. 도 2d의 AIPD 11 내지 AIPD NN과 유사하게 설계되고 전술한 바와 같이 AIPD들로 구성된다. 메인 프로세싱 유닛 제어기는 신경망의 초기 입력 데이터를 AIPD의 호스트 인터페이스 유닛을 통해 제1 계층(입력 계층)과 연관된 AIPD로 전송하도록 구성된다. 예를 들어, 도 2a, 2b 및 2c에 도시된 바와 같이, 신경망의 제1 계층과 관련된 AIPD는 AIPD(103a)이고, 메인 프로세싱 유닛 제어기는 호스트 인 터페이스 유닛을 통해 초기 입력 데이터를 AIPD(103a)에 전송한다. 일부 구현에서, 통신 시퀀스의 마지막 AIPD는 AIPD의 호스트 인터페이스 유닛을 사용하여 결과 데이터를 메인 프로세싱 유닛으로 다시 전송 하도록 구성된다. 일부 구현에서, 신경망의 마지막 계층과 관련된 AIPD는 결과 데이터를 메인 프로세싱 유 닛으로 다시 전송하도록 구성된다. 예를 들어, 위에서 설명한 것처럼, 도 2a에서, 통신 시퀀스에서의 마지 막 AIPD는 AIPD(103a)이며, 일부 구현들에서, AIPD(103a)의 AIPD 제어기는 결과 데이터를 호스트 인 터페이스 유닛을 사용하여 메인 프롯세싱 유닛에 전송하도록 구성된다. 마찬가지로, 도 2b에서, AIPD(103f)는 신경망의 마지막 계층과 관련된 AIPD이며, 일부 구현에서, AIPD(103f)의 AIPD 제어기 는 AIPD(103f)의 호스트 인터페이스 유닛을 사용하여 결과 데이터를 메인 프로세싱 유닛에 전송하도록 구 성된다. 신경망 프로세싱을 위해 AIPD를 구성하기 위한 예시적인 방법이 도 3을 참조하여 아래에 설명된다. 도 3은 신경망 모델을 처리하기 위해 AIPU를 구성하는 예시적인 방법의 흐름도이다. 메인 프로세서에서, 방법은 AIPU를 구성하기 위한 입력을 수신하는 단계(단계 302)를 포함한다. 방법은 AIPU 구성 데이터 를 선택하는 단계 (단계 304)를 포함한다. 방법은 구성 데이터를 AIPU의 AIPD로 전송하는 단계(단계 306)를 포함한다. 각각의 AIPD에서, 방법은 구성 데이터를 수신하는 단계(단계 308)를 포함한다. 방 법은 구성 데이터에 기초하여 AIPD를 구성하는 단계(단계 310)를 포함한다. 방법은 확인 (acknowledgment)을 메인 프로세싱 유닛에 전송하는 단계(단계 312)를 포함한다. 방법은 메인 프로세싱 유닛에서 AIPU를 구성하기 위한 입력을 수신하는 단계(단계 302)를 포함한다. AIPU를 구성하기 위한 입력을 수신하는 것에 응답하여, 방법은 AIPU 내의 각 AIPD 103)에 대한 AIPU 구성 데이터를 선택하는 단계(단계 304)를 포함한다. 메인 프로세서의 메인 프로세싱 유닛 제어기는 AIPU 와 관련된 구성 데이터를 선택한다. AIPU와 관련된 구성 데이터를 선택함에 있어서, 메인 프로세싱 유닛 제어기 는 AIPU의 각 AIPD와 관련된 구성 데이터를 선택한다. 상이한 구성 데이터는, 관련 AIPD와 다른 AIPD 사이의 신경망 관련 데이터의 송수신을 위해 구성되는 관련 AIPD의 인터-다이 출력 및 입력 블 록; 인터-다이 출력 블록의 핀에 출력 데이터를 매핑; 및 파라미터, 파라미터 가중치 데이터, 파라미터 수 와 같은 신경망 관련 데이터를 포함하는(이에 한정되지 않음) 신경망 처리를 위해 AIPD를 구성하도록 하는 상이한 값을 특정할 수 있다. 상기 구성 데이터에 의해 특정된 값은 해당 AIPD와 관련된 신경망의 계층에 기초한다. 따라서, 하나의 AIPD와 관련된 구성 데이터의 값은 다른 AIPD와 관련된 구성 데이터의 값 과 상이할 수 있다. 예를 들어, AIPU에 의해 처리되는 신경망의 제1 계층이 신경망의 제1 계층의 계산 태스크에 사용될 제1 가중치 값 세트를 요구하고 그리고 신경망의 제2 계층이 제2 계층의 계산 태스크 동안 적용될, 제1 가중치 값 세트와는 다른 제2 가중치 값 세트를 요구하는 경우, 신경망의 제1 계층과 연관된 AIPD와 연관 된 구성 데이터는 제1 가중치 값 세트에 대응하는 가중치를 특정할 것이고, 신경망의 제2 계층과 연관된 AIPD와 관련된 구성 데이터는 제2 가중치 값 세트에 대응하는 가중치 값을 특정할 것이다. 신경망 관련 데이터를 신경망의 다음 계층과 관련된 AIPD로 전송하기 위한 구성 데이터에 특정된 AIPD의 인터-다이 출력 블록은 신경망의 다음 계층과 관련된 AIPD에 대한 AIPD의 위치에 부분적 으로 기초한다. 예를 들어, AIPD(103a)가 신경망의 제1 계층과 연관되고 AIPD(103b)가 신경망의 다음 계층과 연 관된 경우, AIPD(103a)에 대한 구성 데이터에 특정된 AIPD(103a)의 인터-다이 출력 블록은 AIPD(103b)의 인터 -다이 입력 블록에 전기적으로 상호 연결된 인터-다이 출력 블록일 것이며, 이는 도 2a, 도 2b 및 도 2c에 도시 된 바와 같이 인터-다이 출력 블록(111a)이다. 유사하게, AIPD(103d)가 AIPD(103a)와 연관된 계층 이후에 다음 계층과 연관된다면, 신경망 관련 데이터를 전송하기 위해 선택되고 AIPD(103a)의 구성 데이터에 특정된 인터-다이 출력 블록은 AIPD(103d)의 인터-다이 입력 블록에 전기적으로 상호 연결된 인터-다이 출력 블록이고, 이는 도 2a, 도 2b 및 도 2c에 도시된 바와 같이 인터-다이 출력 블록(111b)이다. 각각의 AIPD는 고유 식별자와 관련되고, 일부 구현들에서, AIPD의 구성 데이터는 그 AIPD의 고 유 식별자와 관련되고, 그리고 메인 프로세싱 유닛 제어기는 AIPD와 관련된 고유 식별자에 기초하여 AIPD의 구성 데이터를 선택하도록 구성된다. 방법은 선택된 구성 데이터를 AIPD로 전송하는 단계(단계 306)를 포함한다. 전술한 바와 같이, 메인 프로세싱 유닛 제어기는 AIPD(103a)의 호스트 인터페이스 유닛과 같은 AIPD의 호스트 인터페이 스 유닛을 통해 구성 데이터를 AIPD에 전송한다. 일부 구현에서, 메인 프로세싱 유닛 제어기는 임의 의 AIPD에 대한 구성 데이터가 업데이트되었는지 여부를 주기적으로 확인하도록 구성되고 그리고 AIPD의 구성 데이터가 업데이트되면, 메인 프로세싱 유닛 제어기는 업데이트된 구성 데이터를 특정 AIPD로 전송한다. 일부 구현들에서, 메인 프로세싱 유닛 제어기는 수신된 구성 데이터에 기초하여 AIPD를 구성하도록 명령(명령어)들을 AIPD들에 전송한다. 일부 구현들에서, 상기 구성 데이터는 호스 트 컴퓨팅 장치 메모리에 저장되고 그리고 AIPD는 호스트 컴퓨팅 장치의 메모리에 저장된 데이터를 판독하 도록 구성된다. 이러한 구현들에서, 메인 프로세싱 유닛 제어기는 호스트 컴퓨팅 장치 메모리로부터 구성 데이터를 판독하고 그리고 그 구성 데이터에 기초하여 AIPD를 구성하기 위한 명령들을 AIPD들에 전송 한다. 방법은, 각각의 AIPD에서, 구성 데이터를 수신하고(단계 308), 그 수신된 구성 데이터에 기초하여 AIPD를 구성하는 단계(단계 310)를 포함한다. 상술한 바와 같이, AIPD(103a)의 AIPD 제어기와 같은 AIPD의 AIPD 제어기는 인터-다이 입력 및 출력 블록을 선택하고, 수신된 구성 데이터에 기초하여 다른 AIPD로부터 데이터를 수신하고 다른 AIPD로 데이터를 전송하도록 구성된다. AIPD의 AIPD 제어기 는 또한 수신된 구성 데이터에 기초하여, 계산 유닛으로부터의 출력과 같은 AIPD의 특정 출력 데이터 를 신경망 관련 데이터를 다른 AIPD로 전송하기 위해 선택된 상기 인터-다이 출력 블록의 특정 핀으로 전 송하도록 구성된다. AIPD의 AIPD 제어기는 파라미터 가중치 데이터와 같은 신경망 관련 데이터를 버퍼 와 같은 저장 장치에 저장하고, AIPD와 연관된 신경망의 계층과 관련된 계산 동안에 신경망 관련 데 이터를 이용하도록 더 구성된다. 방법은 각각의 AIPD에서, 확인(확인응답)(acknowledgment) 신호를 메인 프로세서에 전송하는 단계(단계 312)를 포함한다. AIPD는 AIPD(103a)의 호스트 인터페이스 유닛과 같은 호스트 인터페이스 유닛을 사용하여 확인 신호를 메인 프로세서에 전송한다. 메인 프로세서에 전송된 확인은 AIPD 의 구성이 성공적임을 메인 프로세서에 표시한다. 일부 구현에서, AIPD의 구성 동안 에러가 발생하면, AIPD는 호스트 인터페이스 유닛을 사용하여 에러 메시지를 메인 프로세서에 전송한다. 필요한 AIPD를 성공적으로 구성한 후, AIPU는 신경망 관련 태스크를 처리하기 위해 대기한다. 메인 프로세싱 유닛 제어기는 신경망 태스크의 실행을 위해 신경망 태스크를 AIPU로 전송한다. AIPU에 의해 신경망 태스크를 처리하기 위한 예시적인 방법이 도 4를 참조하여 아래에 설명된다. 도 4는 AIPU에 의해 신경망 관련 태스크를 처리하는 예시적인 방법의 흐름도이다. 메인 프로세서에서, 방법은 신경망 태스크를 식별하는 단계(단계 402)를 포함한다. 방법은 신경망과 관련된 초기 데이터 또는 입력 데이터를 AIPU에 전송하는 단계(단계 404)를 포함한다. AIPU에서, 방법은 신경망의 입력 계층과 연관된 제1 AIPD에서 신경망과 관련된 초기 데이터를 수신하는 단계(단계 406)를 포 함한다. 방법은, 제1 AIPD에서, 초기 데이터 및 제1 AIPD의 구성 데이터와 함께 수신된 임의의 신경망 관련 데이터를 이용하여 제1 AIPD와 관련된 신경망의 계층과 관련된 계산을 수행하는 단계(단계 408)를 포함한다. 방법은 상기 계산의 결과를 제2 AIPD로 전송하는 단계(단계 410)를 포함한다. 방법(40 0)은 제2 AIPD에서, 제1 AIPD로부터 수신된 결과 데이터를 이용하여 제2 AIPD와 관련된 신경망의 계 층과 관련된 계산을 수행하는 단계(단계 412)를 포함한다. 방법은 일부 구현에서, 제2 AIPD에서의 계 산으로부터의 결과를 피드백으로서 제1 AIPD에 전송하는 단계(단계 414)를 포함한다. 방법은 AIPU로 부터의 신경망의 결과를 메인 프로세서로 전송하는 단계(단계 416)를 포함한다. 방법은, 메인 프로세서에 서, 신경망 결과를 사용자에게 전송하는 단계(단계 418)를 포함한다. 방법은 메인 프로세서에서, 신경망 태스크를 식별하는 단계(단계 402)를 포함한다. 메인 프로세싱 유 닛 제어기는 요청된 태스크가 신경망 관련 태스크인지를 식별하도록 구성된다. 일부 구현에서, 요청된 태 스크에 대한 요청 메시지 또는 데이터는 메시지의 특정 필드에서 높은 비트 또는 낮은 비트와 같은 특정 인디케이터(indicator)를 운반하며, 이는 요청된 태스크가 신경망 관련 태스크임을 나타내며, 메인 프로세싱 유닛 제 어기는 요청된 태스크가 특정 인디케이터에 기초하여 신경망 태스크인지를 결정하도록 구성된다. 방법은 메인 프로세서에서, 신경망의 입력 데이터를 AIPU로 전송하는 단계(단계 404)를 포함한다. 메 인 프로세싱 유닛의 메인 프로세싱 유닛 제어기는 호스트 컴퓨팅 장치의 메모리로부터 입력 데이터를 검색하여 이를 AIPU에 의해 프로세싱되는 신경망의 초기 또는 입력 계층과 연관된 AIPD로 전송한다. 메인 프로세싱 유닛 제어기는 각각의 AIPD와 연관된 구성 데이터에 기초하여 신경망의 입력 계층과 연관된 AIPD를 식별한다. 일부 구현들에서, 신경망의 입력 계층과 연관된 AIPD의 식별자는 메모리 또는 레지 스터 또는 버퍼와 같은 저장 유닛에 저장되고, 그리고 메인 프로세싱 유닛 제어기는 메모리 또는 저장 유 닛에 저장된 식별자에 기초하여 입력 계층과 연관된 AIPD를 결정한다. AIPD가 호스트 컴퓨팅 장치의 메모리에 저장된 데이터를 판독하도록 구성된 구현에서, 메인 프로세싱 유닛 제어기는 신경망의 입력 계층 과 관련된 AIPD에 명령을 전송하여 호스트 컴퓨팅 장치의 메모리로부터 신경망에 대한 입력 데이터를 검색 한다. 방법은, 도 2a, 2b 및 2c를 참조하여 전술한 AIPD(103a)와 같은, 신경망의 입력 계층과 연관된 제1 AIPD에서 신경망과 관련된 입력 데이터를 수신하는 단계를 포함한다. 방법은 제1 AIPD에서, 제1 AIPD에서 수신된 초기 데이터 및 제1 AIPD의 구성 동안 수신된 임의의 다른 신경 망 관련 데이터를 이용하여 제1 AIPD와 관련된 신경망의 계층과 관련된 계산을 수행하는 단계(단계 408)를 포함한다. 제1 AIPD의 제어기는 연관된 신경망 계층에 기초하여 수행될 계산(연산)을 결정한다. 예를 들어, 신경망의 제1 계층이 입력 데이터에 가중치 행렬(matrix of weights)을 적용하여 행렬 곱셈을 수행하는 경우, AIPD의 구성 동안, 가중치 행렬은 제1 AIPD로 전송되고 AIPD의 버퍼에 저장될 것이다. 제 1 AIPD의 AIPD 제어기는 가중치 행렬 및 입력 데이터를 사용하여 행렬 곱셈을 수행하기 위해 가중치 행렬 을 제1 AIPD의 계산 유닛으로 전송하도록 구성된다. 일부 구현에서, 수행될 계산은 제1 AIPD에 의해 수신된 구성 데이터에서 특정되며, 특정된 계산에 기초하여, 제1 AIPD의 제어기는 AIPD(103a)의 계산 유닛 과 같은 AIPD의 적절한 계산 유닛으로 데이터를 전송한다. 방법은, 제1 AIPD에서, 제1 AIPD에서의 계산 결과를 제2 AIPD로 전송하는 단계(단계 41 0)를 포함한다. 제2 AIPD는 제1 AIPD와는 다른 신경망의 계층과 관련된다. 방법은 제2 AIPD에 서, 제1 AIPD로부터 수신된 결과 데이터 및 임의의 다른 신경망 관련 데이터를 이용하여 제2 AIPD와 연관된 신경망의 계층과 관련된 계산을 수행하는 단계(단계 412)를 포함한다. 일부 구현들에서, 계산을 수행하 는 AIPD의 제어기는 AI 계산에 사용하기 위한 파라미터 가중치 데이터와 같은 계산을 위한 추가 데이터를 호스트 컴퓨팅 장치의 메모리로부터 검색할 수 있다. AIPU에 의해 처리되는 신경망 모델이 신경망의 2개 이상의 계층들 사이에 피드백 루프를 포함하고, 제2 AIPD 및 제1 AIPD가 피드백 루프가 포함되는 신경망의 계층들과 연관된 구현들에서, 방법은 제2 AIPD에서, 제1 AIPD 에 대한 피드백으로서 제2 AIPD에서의 계산으로부터 결과 데이터를 전송하 는 단계(단계 414)를 포함한다. 제2 AIPD 및 제1 AIPD와 연관된 계층들 사이에 피드백 루프가 존재하 지 않으면, 방법은 신경망의 결과를 AIPU로부터 메인 프로세싱 유닛으로 전송하는 단계를 포함한다 (단계 416). 신경망의 출력 계층과 연관된 AIPD의 제어기는 AIPD(103a)의 호스트 인터페이스 유닛과 같은 호스트 인터페이스를 사용하여 신경망의 결과를 메인 프로세서에 전송할 것이다. 예를 들어, 도 2a에 서, AIPD(103a)는 도 2a의 신경망의 출력 계층과 관련된 AIPD이므로, AIPD(103a)의 AIPD 제어기는 결과 데이터를 호스트 인터페이스 유닛을 사용하여 메인 프로세서로 전송한다. 유사하게, 도 2b에서, AIPD(103f)는 도 2b에서 신경망의 출력 계층과 관련되고, AIPD(103f)의 AIPD 제어기는 AIPD(103f)의 호스트 인 터페이스 유닛을 이용하여 결과 데이터를 메인 프로세서로 전송한다. 방법은 메인 프로세싱 유닛에서, AIPU로부터 수신된 신경망 결과를 신경망 태스크 요청자에게 전송하 는 단계(단계 418)를 포함한다. 본 명세서에서 사용되는 \"신경망 태스크 요청자\"는 호스트 컴퓨팅 장치 또는 호 스트 컴퓨팅 장치의 최종 사용자 내의 다른 프로세스일 수 있다. 명확성을 유지하고 명확한 예를 설명하기 위해, 2개의 AIPD만이 도 4에 기술되어 있지만, 신경망 태스크를 실행하는데 이용되는 AIPD의 수는 호스트 컴퓨팅 장치에 의해 수행될 것으로 예상되는 신경망 태스크의 볼륨에 적어도 부분적으로 의존한다. 도 5는 예시적인 실시 예에 따라 여기에 설명되고 도시된 시스템 및 방법의 구성요소를 구현하기 위해 사용될 수 있는 컴퓨터 시스템의 일반적인 아키텍처를 나타내는 블록도이다. 컴퓨팅 시스템은 전술한 호스트 컴퓨팅 장치를 구현하는데 사용될 수 있다. 컴퓨팅 시스템은 도 3 및 도 4에 도시된 AIPU 방법을 사용하여 AIPU 방법의 구성 및 프로세싱 신경망 태스크를 구현하는데 이용될 수 있다. 넓은 개요에서, 컴퓨팅 시스템은 명령에 따라 동작을 수행하기 위한 적어도 하나의 프로세서 및 명령 및 데이터를 저장하기 위한 하나 이상의 메모리 장치(570 또는 575)를 포함한다. 도시된 예시적인 컴퓨팅 시스 템은, 네트워크(도시되지 않음)에 연결된 하나 이상의 네트워크 인터페이스 포트를 갖는 하나 이상의 네트워크 인터페이스 제어기와 버스를 통해 통신하는 하나 이상의 프로세서; AIPU; 메모리 ; 및 임의의 다른 컴포넌트(580( 예를 들어, 입/출력(I/O) 인터페이스)를 포함한다. 일반적으로, 프 로세서는 메모리로부터 수신된 명령을 실행할 것이다. 도시된 프로세서는 캐시 메모리를 포함하 거나 또는 이에 직접 연결된다. 보다 상세하게, 프로세서는 명령, 예를 들어 메모리 또는 캐시로부터 인출된 명령을 처리하는 임의의 논리 회로일 수 있다. 많은 실시 예들에서, 프로세서는 마이크로프로세서 유닛 또는 특수 목적 프 로세서이다. 컴퓨팅 장치는 본 명세서에 기술된 바와 같이 동작할 수 있는 임의의 프로세서 또는 프로세서 세트에 기초할 수 있다. 일부 구현들에서, 프로세서는 단계들(302, 304, 306)과 같은 도 3에 도시된 방법 의 특정 단계들 및 단계들(402, 404, 418)과 같은 도 4에 도시된 방법의 특정 단계들을 실행할 수 있 다. 프로세서는 단일 코어 또는 다중 코어 프로세서일 수 있다. 프로세서는 다중 프로세서일 수 있다. 일부 구현들에서, 프로세서는 멀티-스레드 동작들을 실행하도록 구성될 수 있다. 일부 구현들에서, 프로세서는 가상 머신들 또는 컨테이너들의 동작을 관리하기 위한 하이퍼바이저 또는 컨테이너 관리자와 함께 하나 이상의 가상 머신들 또는 컨테이너들을 호스팅할 수 있다. 이러한 구현에서, 도 3에 도시된 방법 및 도 4에 도시된 방법은 프로세서 상에 제공된 가상화 또는 컨테이너화된 환경 내에서 구현될 수 있다. 메모리는 컴퓨터 판독 가능 데이터를 저장하기에 적합한 임의의 장치 일 수 있다. 메모리는 고정 저 장소를 갖는 장치 또는 이동가능한 저장 매체를 판독하기 위한 장치일 수 있다. 예에는 모든 형태의 비휘발성 메모리, 미디어 및 메모리 장치, 반도체 메모리 장치(예: EPROM, EEPROM, SDRAM 및 플래시 메모리 장치), 자기 디스크, 광 자기 디스크 및 광학 디스크(예: CD ROM, DVD-ROM, 블루레이® 디스크)가 포함된다. 컴퓨팅 시스템 은 임의의 수의 메모리 장치를 가질 수 있다. 일부 구현들에서, 메모리는 도 3에 도시된 방법 및 도 4에 도시된 방법에 대응하는 명령(명령어)들을 포함할 수 있다. 일부 구현들에서, 메모리 는 컴퓨팅 시스템에 의해 제공되는 가상 머신 또는 컨테이너 실행 환경에 의해 액세스 가능한 가상화 또는 컨테이너화된 메모리를 지원한다. 캐시 메모리는 일반적으로 빠른 판독 시간을 위해 프로세서에 근접하여 배치된 컴퓨터 메모리의 형태 이다. 일부 구현들에서, 캐시 메모리는 프로세서의 일부이거나 동일한 칩 상에 있다. 일부 구현들에 서, 다중 레벨의 캐시, 예를 들어 L2 및 L3 캐시 계층이 존재한다. 네트워크 인터페이스 제어기는 네트워크 인터페이스(또한 네트워크 인터페이스 포트라고도 함)를 통 해 데이터 교환을 관리한다. 네트워크 인터페이스 제어기는 네트워크 통신을 위해 OSI 모델의 물리 및 데 이터 링크 계층을 처리한다. 일부 구현들에서, 네트워크 인터페이스 제어기의 태스크들 중 일부는 프로세서 에 의해 처리된다. 일부 구현들에서, 네트워크 인터페이스 제어기는 프로세서의 일부이다. 일부 구현들에서, 컴퓨팅 시스템은 복수의 네트워크 인터페이스 제어기들을 갖는다. 네트워크 인터페이스 는 물리적 네트워크 링크를 위한 연결점이다. 일부 구현들에서, 네트워크 인터페이스 제어기는 무선 네트워크 연결을 지원하고, 인터페이스 포트는 무선 수신기/송신기이다. 일반적으로, 컴퓨팅 장치는 물리적 또는 무선 링크를 통해 네트워크 인터페이스로 다른 컴퓨팅 장치와 데이터를 교환한다. 네트워크 인터페이스는 다른 장치에 직접 링크되거나 또는 중간 장치, 예를 들어 허브, 브리지, 스위치 또는 라우터 와 같은 네트워크 장치를 통해 컴퓨팅 장치를 인터넷과 같은 네트워크에 연결하는 중간 장치를 통해 링크 될 수 있다. 일부 구현들에서, 네트워크 인터페이스 제어기는 이더넷과 같은 네트워크 프로토콜을 구현한 다. 다른 컴포넌트들은 I/O 인터페이스, 외부 직렬 장치 포트 및 임의의 추가 코-프로세서(co- processors)를 포함할 수 있다. 예를 들어, 컴퓨팅 시스템은 입력 장치(예를 들어, 키보드, 마이크로폰, 마우스 또는 다른 포인팅 장치), 출력 장치(예를 들어, 비디오 디스플레이, 스피커 또는 프린터), 또는 추가 메 모리 장치(예를 들어, 휴대용 플래시 드라이브 또는 외부 미디어 드라이브)를 연결하기 위한 인터페이스(예를 들어, 범용 직렬 버스(USB) 인터페이스)를 포함할 수 있다. 일부 구현들에서, 다른 컴포넌트들은 프로세서 를 고정밀 또는 복잡한 계산으로 보조할 수 있는 매스(math) 코-프로세서와 같은 추가 코-프로세서를 포함한다. 본 명세서에 기술된 요지 및 동작의 구현은 디지털 전자 회로, 또는 본 명세서에 개시된 구조 및 구조적 등가물 을 포함하는 유형의 매체, 펌웨어 또는 하드웨어에 구현된 컴퓨터 소프트웨어 또는 이들 중 하나 이상의 조합으 로 구현될 수 있다. 본 명세서에 기술된 요지의 구현은 유형의 매체상에 구현된 하나 이상의 컴퓨터 프로그램, 즉, 데이터 처리 장치에 의해 실행되거나 동작을 제어하기 위해 하나 이상의 컴퓨터 저장 매체에 인코딩된, 하 나 이상의 컴퓨터 프로그램 명령의 모듈로서 구현될 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독 가능 저장 장치, 컴퓨터 판독 가능 저장 기판, 랜덤 또는 직렬 액세스 메모리 어레이 또는 장치, 또는 이들 중 하나 이상의 조합 일 수 있거나 이에 포함될 수 있다. 컴퓨터 저장 매체는 또한 하나 이상의 개별 컴포넌트 또는 매체(예를 들어, 복수의 CD, 디스크 또는 다른 저장 장치)일 수 있거나 그에 포함될 수 있다. 컴퓨터 저장 매체는 유형적이고 비 -일시적일 수 있다. 본 명세서에서 설명되는 동작은 하나 이상의 컴퓨터 판독 가능 저장 장치에 저장되거나 다른 소스로부터 수신된 데이터에 대해 데이터 처리 장치에 의해 수행되는 동작으로서 구현될 수 있다. 동작들은 데이터 처리 장치의 고 유 환경 내에서 또는 데이터 처리 장치에 의해 호스팅되는 하나 이상의 가상 머신 또는 컨테이너 내에서 실행될 수 있다. 컴퓨터 프로그램(프로그램, 소프트웨어, 소프트웨어 애플리케이션, 스크립트 또는 코드라고도 함)은 컴파일 또 는 해석된 언어, 선언적 또는 절차적 언어를 포함한 모든 형태의 프로그래밍 언어로 작성될 수 있으며, 독립형 프로그램 또는 컴퓨팅 환경에서 사용하기에 적합한 모듈, 컴포넌트, 서브루틴, 오브젝트 또는 기타 장치를 포함 하여 모든 형태로 배포될 수 있다. 컴퓨터 프로그램은 파일 시스템의 파일에 해당할 수 있지만 반드시 그런 것 은 아니다. 프로그램은 다른 프로그램 또는 데이터(예를 들어, 마크업 언어 문서에 저장된 하나 이상의 스크립 트)를 보유하는 파일의 일부, 해당 프로그램 전용의 단일 파일 또는 여러 개의 조정된 파일(예: 하나 이상의 모 듈, 서브 프로그램 또는 코드의 일부를 저장하는 파일)에 저장될 수 있다. 컴퓨터 프로그램은 하나의 컴퓨터 또 는 여러 컴퓨터 또는 하나의 사이트에 위치하거나 여러 사이트에 분산되어 있고 통신 네트워크에 의해 상호 연 결된 하나 이상의 가상 머신 또는 컨테이너에서 실행되도록 배포될 수 있다. 통신 네트워크의 예는 근거리 통신 망(\"LAN\") 및 광역 통신망(\"WAN\"), 인터-네트워크(예를 들어, 인터넷) 및 피어 투 피어 네트워크(예를 들어, 애 드혹 피어 투 피어 네트워크)를 포함한다. 본 명세서에 기술된 프로세스 및 로직 흐름은 입력 데이터에 대해 동작하고 출력을 생성함으로써 동작을 수행하 기 위해 하나 이상의 컴퓨터 프로그램을 실행하는 하나 이상의 프로그램 가능 프로세서에 의해 수행될 수 있다. 프로세스 및 로직 흐름은 또한 FPGA(field programmable gate array) 또는 ASIC(application specific integrated circuit)과 같은 특수 목적 로직 회로로서 수행될 수 있으며 장치는 또한 특수 목적 로직 회로로서 구현될 수 있다. 본 명세서는 많은 특정 구현 세부 사항을 포함하지만, 이들은 임의의 발명의 범위 또는 청구될 수 있는 것에 대 한 제한으로 해석되어서는 안되고, 오히려 특정 발명의 특정 구현에 특유한 특징의 설명으로 해석되어야 한다. 별도의 구현과 관련하여 본 명세서에서 설명된 특정 특징은 단일 구현으로 조합하여 구현될 수도 있다. 반대로, 단일 구현의 컨텍스트에서 설명된 다양한 특징들은 또한 개별적으로 또는 임의의 적절한 하위 조합으로 다중 구 현으로 구현될 수 있다. 더욱이, 특징들이 특정 조합으로 작용하는 것으로서 설명되고 심지어 초기에 그렇게 주 장되었지만, 청구된 조합으로부터의 하나 이상의 특징은 일부 경우에 조합으로부터 제외될 수 있고, 청구된 조 합은 하위 조합 또는 하위 조합의 변형에 관한 것일 수 있다. 유사하게, 동작들이 특정 순서로 도면에 도시되어 있지만, 이는 바람직한 결과를 달성하기 위해 그러한 동작들 이 도시된 또는 순차적인 순서로 특정 동작으로 수행되거나 모든 예시된 동작들이 수행되는 것을 요구하는 것으 로 이해되어서는 안된다. 특정 상황에서, 멀티 태스킹 및 병렬 처리가 유리할 수 있다. 더욱이, 위에서 설명된 구현들에서 다양한 시스템 컴포넌트들의 분리는 모든 구현들에서 그러한 분리를 요구하는 것으로 이해되어서는 안되며, 설명된 프로그램 컴포넌트 및 시스템은 일반적으로 단일 소프트웨어 제품에 함께 통합되거나 복수의 소 프트웨어 제품으로 패키지될 수 있음을 이해해야 한다. \"또는\"을 언급하는 것은 \"또는\"을 사용하여 기술된 임의의 용어가 설명된 단일 용어, 하나 이상, 및 모든 용어 를 나타낼 수 있도록 포괄적인 것으로 해석될 수 있다. \"제1\", \"제2\", \"제3\" 등의 레이블은 반드시 순서를 나타 내기 위한 것은 아닐 수 있으며 일반적으로 단지 유사하거나 유사한 아이템 또는 요소를 구별하기 위해 사용될 수 있다. 본 개시에서 설명된 구현들에 대한 다양한 수정들이 당업자에게 명백할 수도 있고, 본 명세서에서 정의된 일반 적인 원리들은 본 개시의 사상 또는 범위를 벗어나지 않고 다른 구현들에 적용될 수도 있다. 따라서, 청구 범위 는 본 명세서에 도시된 구현으로 제한되는 것이 아니라 본 명세서, 본 명세서에 개시된 원리 및 신규 한 특징과 일치하는 가장 넓은 범위에 따라야 한다."}
{"patent_id": "10-2019-7034133", "section": "도면", "subsection": "도면설명", "item": 1, "content": "첨부된 도면은 실제 크기대로 도시된 것이 아니다. 다양한 도면에서 유사한 참조 번호 및 명칭은 유사한 구성요 소를 나타낸다. 명확성을 위해, 모든 도면에 모든 컴포넌트가 표시되는 것은 아니다. 도 1a는 예시적인 구현에 따른 신경망 관련 태스크를 처리하기 위한 시스템을 도시한다. 도 1b는 예시적인 구현에 따른, 인공 지능 프로세싱 유닛의 인공 지능 프로세싱 다이의 기능 로직을 도시한다. 도 1c는 예시적인 구현에 따른 인공 지능 프로세싱 다이 상의 시스톨릭 어레이의 예시적인 배열을 도시한다. 도 2a, 2b, 2c 및 2d는 예시적인 구현에 따른, 인공 지능 프로세싱 유닛의 인공 지능 프로세싱 다이의 예시적인 배열을 도시한다. 도 3은 예시적인 구현에 따른 인공 지능 프로세싱 다이를 구성하는 예시적인 방법의 흐름도이다. 도 4는 예시적인 구현에 따라, 신경망 모델에 기초하여 신경망 태스크를 처리하는 예시적인 방법의 흐름도이다. 도 5는 예시적인 구현에 따라, 여기에 설명되고 도시된 시스템 및 방법의 구성요소를 구현하기 위해 사용될 수 있는 컴퓨터 시스템에 대한 일반적인 아키텍처를 나타내는 블록도이다."}
