{"patent_id": "10-2023-0042179", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0146954", "출원번호": "10-2023-0042179", "발명의 명칭": "트랜스포머 뉴럴 네트워크의 경량화를 위한 지식증류 방법 및 장치", "출원인": "광주과학기술원", "발명자": "정희준"}}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "트랜스포머 뉴럴 네트워크(transformer neural network)를 적어도 하나 이상을 포함하는 교사(teacher) 네트워크에서 지식증류(knowledge distillation)를 이용하여 트랜스포머 뉴럴 네트워크를 적어도 하나 이상을 포함하는 학생(student) 네트워크를 학습시키는 방법에 있어서,상기 교사 네트워크를 학습 데이터에 의해 미리 학습시키고, 상기 학습된 교사 네트워크를 미세 조정(finetuning)하는 단계;상기 교사 네트워크의 하단 레이어의 웨이트 파라미터를 상기 학생 네트워크에 복사하는 단계; 및상기 미세 조정된 교사 네트워크를 통해 상기 학생 네트워크에 상기 지식증류를 진행하는 단계;를 포함하고,상기 지식증류를 진행하는 단계는,상기 미세 조정된 상기 교사 네트워크의 레이어의 결과값에서 표상 구조(Feature Structure)를 추출하는 단계;상기 학생 네트워크의 레이어의 결과값에서 표상 구조를 추출하는 단계; 및상기 추출된 교사 네트워크의 표상 구조에 기초하여 상기 추출된 학생 네트워크의 표상 구조를 조정하는 단계;를 포함하는 방법."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 지식증류를 진행하는 단계는,CKA(Centered Kernel Alignment) 메트릭스에 기초하여 상기 표상 구조를 표현되는 것을 특징으로 하는 방법."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 지식증류를 진행하는 단계는,상기 레이어의 결과값을 문장 내에 있는 단어 단위로 히든 스테이트를 나누고, 상기 단어 단위로 나눈 히든 스테이트를 비교한 결과에 기초하여 상기 교사 네트워크의 표상 구조와 상기 학생네트워크의 표상 구조를 조정하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1항에 있어서,상기 지식증류를 진행하는 단계는,미니 배치(Mini-Batch) 내에 존재하는 문장의 히든 스테이트를 나누고,상기 문장의 히든 스테이트를 비교한 결과에 기초하여 상기 교사 네트워크의 표상 구조와 상기 학생 네트워크의표상 구조를 조정하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1항에 있어서,상기 지식증류를 진행하는 단계는,상기 교사 네트워크의 미니 배치 내에 존재하는 각 문장의 히든 스테이트들을 분류(clustering)하고,공개특허 10-2024-0146954-3-상기 분류된 히든 스테이트를 대표하는 대표 값(centeroid)을 정의하고,상기 정의된 대표 값에 기초하여 상기 학생 네트워크를 동작시키는 메모리의 표상 구조를 상기 교사 네트워크를동작시키는 메모리의 표상 구조로 조정하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5항에 있어서,상기 지식증류를 진행하는 단계는,상기 각 문장의 히든 스테이트를 비교한 결과 및 상기 메모리의 표상 구조를 비교한 결과에 기초하여 상기 교사네트워크의 표상 구조와 상기 학생 네트워크의 표상 구조를 조정하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "트랜스포머 뉴럴 네트워크를 적어도 하나 이상을 포함하는 교사 네트워크에서 지식증류를 이용하여 트랜스포머뉴럴 네트워크를 적어도 하나 이상을 포함하는 학생 네트워크를 학습시키는 장치에 있어서,지식증류를 수행하는 프로그램이 저장되는 저장부; 및적어도 하나 이상의 프로세서를 포함하는 제어부;를 포함하며,상기 제어부는,상기 교사 네트워크를 학습 데이터에 의해 미리 학습시키고, 상기 학습된 교사 네트워크를 미세 조정(finetunning)하고,상기 교사 네트워크의 하단 레이어의 웨이트 파라미터를 상기 학생 네트워크에 복사하고,상기 미세 조정된 교사 네트워크의 레이어의 결과값에서 표상 구조(Feature Structure)를 추출하고, 상기 학생 네트워크의 레이어의 결과값에서 표상 구조를 추출하고, 상기 추출된 교사 네트워크의 표상 구조에 기초하여 상기 추출된 학생 네트워크의 표상 구조를 조정하고,상기 표상 구조를 조정함으로써, 상기 학습된 교사 네트워크를 통해 상기 학습된 학생 네트워크에 상기 지식증류를 진행하는 장치."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7항에 있어서,상기 제어부는,CKA(Centered Kernel Alignment) 메트릭스에 기초하여 상기 표상 구조를 표현하는 것을 특징으로 하는 장치."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 7항에 있어서,상기 제어부는,상기 레이어의 결과값을 문장 내에 있는 단어 단위로 히든 스테이트를 나누고, 상기 단어 단위로 나눈 히든 스테이트를 비교한 결과에 기초하여 상기 교사 네트워크의 표상 구조와 상기 학생네트워크의 표상 구조를 조정하는 장치."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 7항에 있어서,상기 제어부는,미니 배치(Mini-Batch) 내에 존재하는 문장의 히든 스테이트를 나누고,상기 문장의 히든 스테이트를 비교한 결과에 기초하여 상기 교사 네트워크의 표상 구조와 상기 학생 네트워크의공개특허 10-2024-0146954-4-표상 구조를 조정하는 장치."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 7항에 있어서,상기 제어부는,상기 교사 네트워크의 미니 배치 내에 존재하는 각 문장의 히든 스테이트들을 분류(clustering)하고,상기 분류된 히스 스테이트를 대표하는 대표 값(centeroid)을 정의하고,상기 정의된 대표 값에 기초하여 상기 학생 네트워크를 동작시키는 메모리의 표상 구조를 상기 교사 네트워크를동작시키는 메모리의 표상 구조로 조정하는 장치."}
{"patent_id": "10-2023-0042179", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11항에 있어서,상기 제어부는,상기 각 문장의 히든 스테이트를 비교한 결과 및 상기 메모리의 표상 구조를 비교한 결과에 기초하여 상기 교사네트워크의 표상 구조와 상기 학생 네트워크의 표상 구조를 조정하는 장치."}
{"patent_id": "10-2023-0042179", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "개시된 일 실시예에 따른 트랜스포머 뉴럴 네트워크(transformer neural network)를 적어도 하나 이상을 포함하 는 교사(teacher) 네트워크에서 지식증류(knowledge distillation)를 이용하여 트랜스포머 뉴럴 네트워크를 적 어도 하나 이상을 포함하는 학생(student) 네트워크를 학습시키는 방법은 상기 교사 네트워크를 학습 데이터에 (뒷면에 계속)"}
{"patent_id": "10-2023-0042179", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "개시된 실시예는 트랜스포머 기반의 뉴럴 네트워크의 경량화를 위한 지식증류 방법 및 이를 실행하는 장치에 관 한 것으로, 교사 네트워크의 지식을 학생 네트워크로 전이하는 방법에 관한 것이다."}
{"patent_id": "10-2023-0042179", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "버트(Bidirectional Encoder Representations from Transformers, BERT)가 인공지능 분야에 등장하면서, 자연 어 처리분야에서는 BERT 를 이용한 대용량의 모델들이 등장하기 시작하였다. 버트(BERT)는 트랜스포머 (Transformer) 기반의 모델로, 자연어 처리에서도 컴퓨터 비전과 마찬가지로 거대한 모델의 사전학습(pre- training) 및 재학습(fine-tuning)이 가능해졌고, 다양한 문제들에서 뛰어난 성능을 보여주었다. 그러나 버트(BERT)의 단점은 모델의 용량이 매우 크고, BERT-base는 약 1.1억개의 파라미터를 사용하기 때문에 버트(BERT)를 사용하기 위해서는 매우 많은 양의 메모리가 필요하다는 문제가 있었다. 이를 해결하기 위해서 최근에는 BERT의 성능을 유지하면서, 모델을 경량화하는 다양한 연구가 시도되고 있다. 그 중 하나인 지식증류(Knowledge Distillation, KD) 방법은 많은 데이터 처리가 가능한 교사 네트워크 (teacher network)의 일반화 능력(generalization ability)을 교사 네트워크보다 작은 학생 네트워크(student network)로 전이시켜 네트워크를 학습시키고, 교사 네트워크만큼의 성능을 낼 수 있는 방법이다. 이러한 지식 증류 방법과 관련하여, 이미 공개된 기술은 아래와 같다. 먼저, 선행기술 1은 텍스트 기반 사전 학습 모델을 활용한 종단형 음성 언어 이해를 위한 방법에 관한 것으로, 교사 네트워크에서 학생 네트워크로 지식 증류를 위해 최상위 레이어에 있는 SLU (Spoken Language Understanding) 모델과 BERT 최상위 계층의 히든 레이어가 같아지는 손실 함수를 이용한다. 구체적으로 선행기 술 1은 학생의 소프트맥스 함수와 참(True) 레벨 사이의 크로스엔트로피 손실(Cross-Entropy Loss)을 이용한다. 선행기술 2는 지식 증류를 위해서 교사 네트워크와 학생 네트워크의 소프트맥스 함수를 조절하는 온도 값을 이 용한다. 또한, 선행기술 2는 각 네트워크의 n 번째 레이어의 결과값을 유클리언 디스턴스(Euclidean Distance) 기반의 평가치를 통해 두 결과값의 사이 거리를 좁히는 방식으로 지식 증류를 진행한다. 이러한 선행기술들은 모두 학생 네트워크가 교사 네트워크의 최종 결과값 결과로만 지식 증류가 학습되며, 특히 선행기술 2는 레이어의 결과값을 이용하나, 단순히 거리를 좁히기는 점에 국한되어 있다. (선행기술 1) 국내등록번호 10-2368064 B1 (선행기술 2) 국내공개번호 10-2022-0069225 A"}
{"patent_id": "10-2023-0042179", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "개시된 일 실시예에 따르면, 단어, 문장 및 메모리 간의 표상 관계(Feature Structure)를 추출하고, 추출된 표 상 관계에 기초하여 지식 증류를 실행함으로써, 용량이 작은 모델이 용량이 큰 모델과 동등하거나 높은 수준의 성능으로 동작하게 하는 트랜스포머 뉴럴 네트워크의 경량화를 위한 지식증류 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0042179", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "개시된 일 실시예에 따른 트랜스포머 뉴럴 네트워크(transformer neural network)를 적어도 하나 이상을 포함하 는 교사(teacher) 네트워크에서 지식증류(knowledge distillation)를 이용하여 트랜스포머 뉴럴 네트워크를 적 어도 하나 이상을 포함하는 학생(student) 네트워크를 학습시키는 방법은 상기 교사 네트워크를 학습 데이터에 의해 미리 학습시키고, 상기 학습된 교사 네트워크를 미세 조정(fine tuning)하는 단계; 상기 교사 네트워크의 하단 레이어의 웨이트 파라미터를 상기 학생 네트워크에 복사하는 단계; 및 상기 미세 조정된 교사 네트워크를 통해 상기 학생 네트워크에 상기 지식증류를 진행하는 단계;를 포함하고, 상기 지식증류를 진행하는 단계는, 상 기 미세 조정된 상기 교사 네트워크의 레이어의 결과값에서 표상 구조(Feature Structure)를 추출하는 단계; 상 기 학생 네트워크의 레이어의 결과값에서 표상 구조를 추출하는 단계; 및 상기 추출된 교사 네트워크의 표상 구 조에 기초하여 상기 추출된 학생 네트워크의 표상 구조를 조정하는 단계;를 포함한다. 상기 지식증류를 진행하는 단계는, CKA(Centered Kernel Alignment) 메트릭스에 기초하여 상기 표상 구조를 표 현되는 것을 특징으로 할 수 있다. 상기 지식증류를 진행하는 단계는, 상기 레이어의 결과값을 문장 내에 있는 단어 단위로 히든 스테이트를 나누 고, 상기 단어 단위로 나눈 히든 스테이트를 비교한 결과에 기초하여 상기 교사 네트워크의 표상 구조와 상기 학생 네트워크의 표상 구조를 조정하는 것을 특징으로 할 수 있다. 상기 지식증류를 진행하는 단계는, 미니 배치(Mini-Batch) 내에 존재하는 문장의 히든 스테이트를 나누고, 상기 문장의 히든 스테이트를 비교한 결과에 기초하여 상기 교사 네트워크의 표상 구조와 상기 학생 네트워크의 표상 구조를 조정하는 것을 특징으로 할 수 있다. 상기 지식증류를 진행하는 단계는, 상기 교사 네트워크의 미니 배치 내에 존재하는 각 문장의 히든 스테이트들 을 분류(clustering)하고, 상기 분류된 히든 스테이트를 대표하는 대표 값(centeroid)을 정의하고, 상기 정의된 대표 값에 기초하여 상기 학생 네트워크를 동작시키는 메모리의 표상 구조를 상기 교사 네트워크를 동작시키는 메모리의 표상 구조로 조정하는 것을 특징으로 할 수 있다. 상기 지식증류를 진행하는 단계는, 상기 각 문장의 히든 스테이트를 비교한 결과 및 상기 메모리의 표상 구조를 비교한 결과에 기초하여 상기 교사 네트워크의 표상 구조와 상기 학생 네트워크의 표상 구조를 조정하는 것을 특징으로 할 수 있다. 개시된 다른 실시예에 따른 트랜스포머 뉴럴 네트워크를 적어도 하나 이상을 포함하는 교사 네트워크에서 지식 증류를 이용하여 트랜스포머 뉴럴 네트워크를 적어도 하나 이상을 포함하는 학생 네트워크를 학습시키는 장치는, 지식증류를 수행하는 프로그램이 저장되는 저장부; 및 적어도 하나 이상의 프로세서를 포함하는 제어부;를 포함하며, 상기 제어부는, 상기 교사 네트워크를 학습 데이터에 의해 미리 학습시키고, 상기 학습된 교사 네트워크를 미세 조정하고, 상기 교사 네트워크의 하단 레이어의 웨이트 파라미터를 상기 학생 네트워크에 복사하고, 상기 미세 조정된 교사 네트워크의 레이어의 결과값에서 표상 구조(Feature Structure)를 추출하고, 상기 학생 네트워크의 레이어의 결과값에서 표상 구조를 추출하고, 상기 추출된 교사 네트워크의 표상 구조에 기초하여 상기 추출된 학생 네트워크의 표상 구조를 조정하고, 상기 표상 구조를 조정함으로써, 상기 학습된 교 사 네트워크를 통해 상기 학습된 학생 네트워크에 상기 지식증류를 진행한다. 상기 제어부는, CKA(Centered Kernel Alignment) 메트릭스에 기초하여 상기 표상 구조를 표현하는 것을 특징으 로 할 수 있다. 상기 제어부는, 상기 레이어의 결과값을 문장 내에 있는 단어 단위로 히든 스테이트를 나누고, 상기 단어 단위 로 나눈 히든 스테이트를 비교한 결과에 기초하여 상기 교사 네트워크의 표상 구조와 상기 학생 네트워크의 표상 구조를 조정할 수 있다. 상기 제어부는, 미니 배치(Mini-Batch) 내에 존재하는 문장의 히든 스테이트를 나누고, 상기 문장의 히든 스테 이트를 비교한 결과에 기초하여 상기 교사 네트워크의 표상 구조와 상기 학생 네트워크의 표상 구조를 조정할 수 있다. 상기 제어부는, 상기 교사 네트워크의 미니 배치 내에 존재하는 각 문장의 히든 스테이트들을 분류(clusterin g)하고, 상기 분류된 히스 스테이트를 대표하는 대표 값(centeroid)을 정의하고, 상기 정의된 대표 값에 기초하 여 상기 학생 네트워크를 동작시키는 메모리의 표상 구조를 상기 교사 네트워크를 동작시키는 메모리의 표상 구 조로 조정할 수 있다. 상기 제어부는, 상기 각 문장의 히든 스테이트를 비교한 결과 및 상기 메모리의 표상 구조를 비교한 결과에 기 초하여 상기 교사 네트워크의 표상 구조와 상기 학생 네트워크의 표상 구조를 조정할 수 있다."}
{"patent_id": "10-2023-0042179", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "개시된 일 실시예에 따른 트랜스포머 뉴럴 네트워크의 경량화를 위한 지식증류 방법 및 장치는 용량이 작은 학 생 네트워크가 용량이 큰 선생 네트워크와 동등하거나 높은 수준의 성능으로 동작하게 한다. 또한, 개시된 일 실시예에 따른 트랜스포머 뉴럴 네트워크의 경량화를 위한 지식증류 방법 및 장치는 학생 네트 워크가 메모리의 용량이 작은 휴대용 사용자 단말에서도 동작하게 하고, 지연 시간 감소 및 학습과 추론 시간의 감소에도 효과적이다."}
{"patent_id": "10-2023-0042179", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "명세서 전체에 걸쳐 동일 참조 부호는 동일 구성요소를 지칭한다. 본 명세서가 실시예들의 모든 요소들을 설명"}
{"patent_id": "10-2023-0042179", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "하는 것은 아니며, 본 발명이 속하는 기술분야에서 일반적인 내용 또는 실시예들 간에 중복되는 내용은 생략한 다. 명세서에서 사용되는 '부, 모듈, 부재, 블록'이라는 용어는 소프트웨어 또는 하드웨어로 구현될 수 있으며, 실시예들에 따라 복수의 '부, 모듈, 부재, 블록'이 하나의 구성요소로 구현되거나, 하나의 '부, 모듈, 부재, 블 록'이 복수의 구성요소들을 포함하는 것도 가능하다. 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 직접적으로 연결되어 있는 경우뿐 아니라, 간접적으로 연결되어 있는 경우를 포함하고, 간접적인 연결은 무선 통신망을 통해 연결되는 것을 포함 한다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 명세서 전체에서, 어떤 부재가 다른 부재 \"상에\" 위치하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재가 존재하는 경우도 포함한다. 제 1, 제 2 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하기 위해 사용되는 것으로, 구성요소가 전술된 용어들에 의해 제한되는 것은 아니다. 단수의 표현은 문맥상 명백하게 예외가 있지 않는 한, 복수의 표현을 포함한다. 각 단계들에 있어 식별부호는 설명의 편의를 위하여 사용되는 것으로 식별부호는 각 단계들의 순서를 설명하는 것이 아니며, 각 단계들은 문맥상 명백하게 특정 순서를 기재하지 않는 이상 명기된 순서와 다르게 실시될 수 있다. 이하 첨부된 도면들을 참고하여 본 발명의 작용 원리 및 실시예들에 대해 설명한다. 도 1은 개시된 실시예를 개략적으로 설명하기 위한 도면이다. 도 1을 참조하면, 본 발명의 실시예는 DNN(Deep Neural network)를 학습시키기 위해서 지식증류(Knowledge Distillation, KD)를 이용한다. 지식증류는 데이터 처리 및 학습하는 데이터의 양과 용량 등에서 비교적으로 더 큰 네트워크, 이하 교사 네트워크(teacher network, 20)의 일반화 능력(generalization ability)을 비교적으로 작은 네트워크, 이하 학생 네트워크(student network, 30)로 전송하여 학생 네트워크를 학습시키는 방식을 말한다. 교사 네트워크는 하드 타겟(hard-target) 정보와 소프트 타겟(soft-target) 정보를 학생 네트워크 에 제공하여, 학생 네트워크가 교사 네트워크와 유사하게 일반화하는 것을 학습할 수 있도록 한다. 본 발명에서, 지식증류의 방식을 사용하여 학습시키는 뉴럴 네트워크는 트랜스포머(transformer) 기반의 뉴럴 네트워크이며, 일실시예에 따르면, 버트(Bidirectional Encoder Representations from Transformers, BERT)일 수 있다. 이때, 트랜스포머는 seq2seq의 구조인 인코더-디코더를 따르며, RNN을 사용하지 않고 어텐션 (Attention)만으로 구현한 모델일 수 있다. 지식증류를 실행하기 전, 교사 네트워크와 학생 네트워크는 모두 학습 데이터를 이용하여 사전 학 습(Pre-trained)를 진행한다. 교사 네트워크와 학생 네트워크는 사전 학습을 통해서 각 레이어를 데이 터로 채우는 결과값(21, 31)을 가지게 된다. 개시된 일 실시예에 따른 지식증류 방법은 교사 네트워크 및 학생 네트워크의 각 레이어(또는 메모리)마다 추출될 수 있는 결과값을 표상 구조(Feature Structure, 22 및 32)로 분류하고, 학생 네트워크의 표상 구조를 교사 네트워크의 표상 구조로 조정하는 과정을 통해 학생 네트워크가 교사 네트워크와 동일 또는 유사해지도록, 지식증류를 수행한다. 여기서 표상 구조는 관계의 집합(a set of relations)으로 정의하고, Centered Kernel Alignment(이하 CKA) metric를 이용하여 표상 구조를 표현한다. 또한, 개시된 지식증류 방법은, 지역내 표상구조(Local intra feature structure), 지역간 표상구조(Local inter feature structure) 및 전역간 표상구조(Global inter feature structure)로 분류할 수 있다. 구체적으로 지역내 표상구조는 단어 간의 관계(relations between word tokens)로, 지역간 표상구조는 문장 간의 관계(relations between sentence representations)로, 전역간 표상 구조는 문장 및 메모리 간의 관계(relations between sentence representations and memory)로 정의한다. 도 2는 개시된 지식증류 장치에 관한 제어 블록도이다. 도 2를 참조하면, 지식증류 장치는 입출력부, 저장부 및 제어부를 포함할 수 있다. 일 실시예에 따른 입출력부는 사용자로부터 입력을 수신하기 위한 입력장치와, 작업의 수행 결과 또는 지 식증류 장치의 상태 등의 정보를 표시하기 위한 출력장치를 포함할 수 있다. 입출력부의 입력장치는 사용자의 입력 명령을 수신하기 위해 각종 버튼이나 스위치, 페달(pedal), 키보드, 마우스, 트랙볼(track-ball), 각종 레버(lever), 핸들(handle)이나 스틱(stick) 등과 같은 하드웨어적인 장치 를 포함할 수 있다. 입출력부의 출력장치는 학습 및 출력되는 이미지를 표시하기 위한 디스플레이 및 소리를 출력하는 스피커 등의 하드웨어적인 장치를 포함할 수 있으며, 디스플레이는 디지털 광원 처리(Digital Light Processing: DLP) 패널, 플라즈마 디스플레이 패널(Plasma Display Penal), 액정 디스플레이(Liquid Crystal Display: LCD) 패널, 전기 발광(Electro Luminescence: EL) 패널, 전기영동 디스플레이(Electrophoretic Display: EPD) 패널, 전기변색 디스플레이(Electrochromic Display: ECD) 패널, 발광 다이오드(Light Emitting Diode: LED) 패널 또 는 유기 발광 다이오드(Organic Light Emitting Diode: OLED) 패널 등으로 마련될 수 있다. 입출력부는 입력장치 대신, 사용자의 입력 명령을 위해 터치 패드(touch pad) 등과 같은 GUI(Graphical User interface), 즉 소프트웨어인 장치를 포함할 수도 있다. 터치 패드는 터치 스크린 패널(Touch Screen Panel: TSP)로 구현되어 디스플레이와 상호 레이어 구조를 이룰 수 있다. 저장부는 지식증류를 수행하여 학생 네트워크를 학습시키기 위한 데이터를 저장할 수 있다. 가령, 저 장부는 뉴럴 네트워크(교사 네트워크, 학생 네트워크)를 학습시키기 위한 학습 데이터를 저장 할 수 있고, 학생 네트워크를 학습시키기 위한 지식증류에 필요한 교사 네트워크의 결과값을 저장할 수 있다. 또한, 학생 네트워크를 학습시키기 위한 지식증류에 필요한 각종 데이터나 프로그램들을 저장할 수 있다. 저장부는 ROM(Read Only Memory), PROM(Programmable ROM), EPROM(Erasable Programmable ROM), EEPROM(Electrically Erasable Programmable ROM) 및 플래쉬 메모리(Flash memory)와 같은 비휘발성 메모리 소 자 또는 RAM(Random Access Memory)과 같은 휘발성 메모리 소자 또는 하드디스크 드라이브(HDD, Hard Disk Drive), CD-ROM과 같은 저장 매체 중 적어도 하나로 구현될 수 있으나 이에 한정되지는 않는다. 제어부는 지식증류 장치의 전체적인 동작을 제어하며, 지식증류 장치 내 구성요소들의 동작을 제어하기 위한 알고리즘 또는 알고리즘을 재현한 프로그램에 대한 데이터를 저장하는 메모리(미도시), 및 메모 리에 저장된 데이터를 이용하여 전술한 동작을 수행하는 프로세서(미도시)로 구현될 수 있다. 이때, 메모리와 프로세서는 각각 별개의 칩으로 구현될 수 있다. 또는, 메모리와 프로세서는 단일 칩으로 구현될 수도 있다. 특히, 제어부는 저장부에 저장된 프로그램을 실행하거나 데이터를 읽어 학생 네트워크를 학습시 키기 위한 지식증류를 수행할 수 있다. 제어부가 학생 네트워크를 학습시키기 위한 지식증류를 수행하 는 구체적인 방법에 대해서는 아래에서 다른 도면들을 참조하여 자세하게 설명한다. 저장부는 제어부와 관련하여 전술한 프로세서와 별개의 칩으로 구현된 메모리일 수 있고, 프로세서와 단일 칩으로 구현될 수도 있다. 도 3은 지식증류를 수행하는 구체적인 방법을 설명하기 위한 순서도이다. 도 3을 참조하면, 제어부는 교사 네트워크를 미리 학습시킨다. 교사 네트워크를 사전 학습하는 방법은 다양할 수 있다. 교사 네트워크의 사전 학습 단계는 트랜스포머 뉴럴 네트워크가 자연어 학습을 수행하기 위해서 각 시퀀스가 입력될 수 있는 전처리 과정이 포함되며, 입력 시 퀀스의 각 단어는 임베딩(Embedding) 과정을 거쳐 밀집 벡터 형태로 변환될 수 있다. 임베딩은 단어를 고차원 벡터 공간으로 매핑하는 과정으로, 이를 통해 모델은 단어 간의 유사도를 계산할 수 있다. 교사 네트워크가 포함하는 인코더에서는 입력 시퀀스를 처리하며, 다수의 어텐션 레이어(이하 레어어)는 입 력 시퀀스에서 단어 생성에 필요한 정보를 추출한다. 이 과정에서 교사 네트워크는 손실 함수(Loss function)를 최소화하는 방향으로 미세 조정(fine tuning)을 진행할 수 있다. 사전 학습된 교사 네트워크의 하단 레이어의 웨이트 파라미터를 학생 네트워크에 복사한다. 여기서 교사 네트워크의 하단 레이어는 1에서 6번째 레이어에 포함된 웨이트 파라미터일 수 있다. 다만, 하 단 레이어가 반드시 1에서 6번째 레이어에 해당하는 것은 아니고, 다양한 변형례가 있을 수 있다.교사 네트워크 의 미세 조정이 완료되면, 제어부는 지식증류를 진행한다. 도 4는 지식증류를 진행하는 과정을 구체적으로 설명하기 위한 순서도이다. 도 4를 참조하면, 제어부는 미리 학습된 상기 교사 네트워크의 레이어의 결과값에서 표상 구조를 추출하고 , 교사 네트워크의 하단 레이어의 웨이트 파라미터가 복사된 상기 학생 네트워크의 레이어의 결과값에서 표상 구조를 추출한다. 전술한 바와 같이, 표상 구조는 CKA 메트릭스를 기초로 표현된다. 여기서 CKA 메트릭스는 머신러닝에서 사용되 는 두 개의 임베딩(Embedding) 행렬이 유사한 공간을 공유하는지 측정하는 메트릭스이다. 두 개의 임베딩 행렬 을 각각 X와 Y라고 할 때, CKA 메트릭스는 먼저 두 행렬의 내적값을 계산한 후, X와 Y 각각의 내적값을 계산하 여 이 값을 정규화한다. CKA 메트릭스는 두 개의 정규화된 내적값 간의 코사인 유사도를 계산하여 두 임베딩 행 렬 간의 유사도를 측정하여 표시한다. CKA 메트릭스는 두 개의 임베딩 행렬이 유사한 공간을 공유하는 정도를 0 과 1 사이의 값으로 표현하며, 값이 1에 가까울수록 두 행렬이 공유하는 공간이 많아지며, 값이 0에 가까울수록두 행렬이 서로 다른 공간에 위치한다는 것을 의미한다. 제어부는 학생 네트워크에서 추출된 표상 구조를 교사 네트워크의 표상 구조로 조정한다. 제어부는 교사 네트워크 및 학생 네트워크에서 각각 추출된 CKA 메트릭스 값의 차이를 비교하고, 비교 결과에 기초하여 학생 네트워크의 표상 구조를 교사 네트워크의 표상 구조로 조정함으로써, 지식 증류를 수행한다. 한편, 개시된 지식증류 방법은, 표상 구조를 단어 간의 관계를 나타내는 지역 내 표상구조, 문장 간의 관계를 나타내는 지역 간 표상 구조 및 각 네트워크의 메모리 간 또는 메모리와 문장 간의 관계를 나타내는 전역간 표 상구조를 분류하여 지식증류를 진행한다. 이에 대한 구체적인 설명은 이하 도면을 통해 후술한다. 도 5는 지역 내 표상 구조를 조정하는 방법을 설명하기 위한 도면이다. 도 5를 참조하면, 제어부는 교사 네트워크내의 레이어 중 k번째 레이어(HT)의 결과값에 포함된 문장 중, 단어(I, am, a, student) 단위로 히든 스테이트를 나눈다. 제어부는 학생 네트워크내의 레이어 중 k번째 레이어(HS)의 결과값에 포함된 문장 중, 단어 단위 (Representation)로 히든 스테이트를 나눈다. 제어부는 CKA 메트릭스를 통해 교사 네트워크 및 학생 네트워크의 표상 구조의 차이가 가까워지도록 학생 네트워크를 학습시킨다. 즉, 도 5와 같이, 제어부 는 단어 단위로 구성된 표상 구조가 서로 유사해지도록 학생 네트워크의 지식증류를 진행한다. 도 6은 지역 간 표상 구조를 조정하는 방법을 설명하기 위한 도면이다. 도 6을 참조하면, 제어부는 교사 네트워크내의 미니 배치(Mini-Batch) 내에 존재하는 문장(I am a student, I have a pen, welcome, Thanks to watch my video) 단위의 히든 스테이트를 구분한다. 마찬가지로 제어부는 학생 네트워크내의 미니 배치 내에 존재하는 문장의 히든 스테이트를 구분한다. 제어부는 구분된 문장 단위의 히든 스테이트를 CKA 메트릭스를 통해 비교한 후, CKA 메트릭스의 값이 유사 해지도록 학생 네트워크를 조정한다. 즉, 도 6와 같이, 제어부는 문장 단위(Representation)로 구성된 표상 구조가 서로 유사해지도록 학생 네트워크의 지식증류를 진행한다. 도 7은 전역 간 표상구조를 조정하는 방법을 설명하기 위한 도면이다. 도 7을 참조하면, 제어부는 교사 네트워크 및 학생 네트워크의 미니 배치 내에 존재하는 문장의 히든 스테이트간의 분류(clustering)를 진행한다. 제어부는 분류된 히든 스테이트를 대표하는 대표 값 (centroid)를 정의하여 학습 네트워크가 교사 네트워크의 메모리를 학습하도록 제어한다. 즉, 제어부 는 메모리 간 또는 메모리와 문장 간의 표상 구조를 분류된 히든 스테이트를 대표하는 대표 값으로 표상 구조를 정의한 후, 대표 값이 맞춰지는 방향으로 표상 구조를 조정함으로써, 학생 네트워크의 지식증류를 진행한다. 도 7에서 도시된 바와 같이, 제어부는 교사 네트워크의 메모리 및 교사 네트워크를 통해 얻은 히 든 스테이트 간의 관계와 학생 네트워크의 메모리와 학생 네트워크의 히든 스테이트 간의 관계를 비교 하여, 각 표상 구조가 조정될 수 있도록 한다. 종전 선행기술은 문장 간의 거리 또는 단어 간의 거리를 좁히는 지식증류 방법을 제안하였지만, 개시된 지식증 류 방법은 문장 내에서 발생할 수 있는 다양한 관계를 비교 학습하여 지식증류 성능 개선에 도움을 줄 수 있으 며, 세분화된 방법 간의 상보완적인 역할을 통해 이를 지식증류의 성능을 개선할 수 있다. 도 8 및 도 9는 개시된 지식증류 방법의 성능을 비교하기 위한 표이다. 구체적으로 도 8 및 도 9의 표는 GLUE benchmark를 통한 각 인공신경망의 성능을 비교한 것이다. GLUE(General Language Understanding Evaluation) benchmark는 자연어 이해(Natural Language Understanding, NLU) 모델의 성능을 평가하기 위한 벤치마크이다. 개시된 지식증류 방법은 FSD로 표시되며, 교사 네트워크는 HF*과 BERT를 베이스로 뉴럴 네트워크이다. 특히 *은 huggingface BERT로부터 얻어진 결과값을 의미한다. 학생 네트워크는 VKD(Vanilla Knowledge Distillation), PKD(Patient Knowledge Distillation), RKD(Relational Knowledge Distillation) 및 MiniLM+을 비교 모델로 사용하였다.한편, +는 VKD, PKD 및 FSD의 일관성을 위해 재현된 모델을 의미한다. 각각의 평가 메트릭스의 Mean과 Standard deviation은 각각의 네트워크를 6번 학습시킨 결과를 수치로 나타낸 것으로, 도 8의 WNLI 및 RTE는 accuracy을, STS-B는 Pearon/Spearman correlation을, CoLA는 Matthew's correlation을, MRPC는 F1/accuracy를 의미한다. 도 8에서 확인할 수 있듯이, 개시된 지식증류 방법(FSD)는 각 지표에서 비교대상인 네트워크보다 우수한 것을 확인할 수 있으며, CoLA의 경우, RKD와 거의 근접한 수치로 평가된다. 도 9의 SST-2, QNLI 및 MNLIs는 accurarcy를 QQP 는 accuracy/F1을 의미한다. 도 9에서도 확인할 수 있듯이, 개시된 지식증류 방법(FSD)는 각 지표에서 비교대상인 네트워크보다 동등 또는 우수한 것을 확인할 수 있다. 도 10은 도 8 및 도 9에서 언급한 수치를 그래프로 표현한 도면이다. 도 10도 GLUE benchmark를 통한 수치를 그래프화한 것이며, 학생 네트워크가 교사 네트워크의 결과값 (Predictions)의 복원률(Restoration rate)를 나타낸다. 검은색 막대 그래프는 VKD를, 빨간색 막대 그래프는 PKD를, 파란색 막대 그래프가 개시된 지식증류 방법(FSD)를 나타낸다. 도 10의 그래프에서도 확인할 수 있듯이, 각 지표마다 FSD는 비교되는 다른 네트워크보다 동등 또는 우수한 것을 확인할 수 있다. 도 11은 CKA 히트 맵을 도시화한 도면이고, 도 12는 이를 수치화한 표이다. 도 11의 CoLA 데이터 세트(data set) 결과는 GLUE 벤치마크를 대표하며, x축, y축은 동일한 데이터 세트에 대한 미니 배치의 지표이다. 각각의 픽셀은 교사 네트워크와 학생 네트워크 사이의 CKA 유사도를 나타내는 것이다. 대각선 라인(Diagonal lines)에서 각각의 샘플은 동일한 샘플에 대해 교사 네트워크와 학생 네트워크 사이의 CKA 유사도이며,, 각 그림의 오른쪽 바(bar)는 각각의 모델에서 관측된 CKA 유사도의 표준화된 범위(normalized range)이다. A)는 CoLA이고, B)는 SST-2 case이다. 도 11에서 확인할 수 있듯이, 개시된 지식증류 방법(T-FSD)의 히트 맵은 T-T 히트맨과 매우 유사하며, 이 결과 는 개시된 지식증류 방법에 의한 학생 네트워크가 교사 네트워크와 가장 유사하게 학습한 것을 의미한 다. 도 12의 각 표에서 나타난 수치는 각 네트워크마다 교사 네트워크와 학생 네트워크의 쌍에 대한 diagonal value 의 평균이며, 도 12에서 볼 수 있듯이, T-FSD는 T-VKD 및 T-PKD에 비해서 CKA 유사도 값(similarity value)이 가장 높다(0.958). 도 13은 각각의 지식증류 방법을 비교하기 위한 또 다른 도면이다. 도 13의 표는 VKD, PKD, RKD 및 개시된 지식증류 방법(FSD)의 Relation Difference를 Average Rank로 표현한 것이며, 마지막 difference value를 랭킹에 사용한 것이다. 낮은 수치일수록 좋은 성능을 의미한다. 도 13의 표 및 이를 도시화한 그래프에서도 알 수 있듯이, 개시된 지식증류 방법(FSD)가 CKA 유사성에서 낮은 수치를 나타낸다 하더라도, Euclidean Distance 및 cosine similarity의 순위는 타 네트워크에 비해 우수하다. 즉, 개시된 트랜스포머 뉴럴 네트워크의 경량화를 위한 지식증류 방법 및 장치는 용량이 작은 학생 네트워크가 용량이 큰 선생 네트워크와 동등하거나 높은 수준의 성능으로 동작하게 하며, 학생 네트워크가 메모리의 용량이 작은 휴대용 사용자 단말에서도 동작하게 하고, 지연 시간 감소 및 학습과 추론 시간의 감소에도 효과적이다."}
{"patent_id": "10-2023-0042179", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 개시된 실시예를 개략적으로 설명하기 위한 도면이다. 도 2는 개시된 지식증류 장치에 관한 제어 블록도이다. 도 2는 개시된 지식증류 장치에 관한 제어 블록도이다. 도 3은 지식증류를 수행하는 구체적인 방법을 설명하기 위한 순서도이다. 도 4는 지식증류를 진행하는 과정을 구체적으로 설명하기 위한 순서도이다. 도 5는 지역 내 표상 구조를 조정하는 방법을 설명하기 위한 도면이다. 도 6은 지역 간 표상 구조를 조정하는 방법을 설명하기 위한 도면이다. 도 7은 전역 간 표상구조를 조정하는 방법을 설명하기 위한 도면이다. 도 8 및 도 9는 개시된 지식증류 방법의 성능을 비교하기 위한 표이다. 도 10은 도 8 및 도 9에서 언급한 수치를 그래프로 표현한 도면이다. 도 11은 CKA 히트 맵을 도시화한 도면이고, 도 12는 이를 수치화한 표이다. 도 13은 각각의 지식증류 방법을 비교하기 위한 또 다른 도면이다."}
