{"patent_id": "10-2021-0099728", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0018048", "출원번호": "10-2021-0099728", "발명의 명칭": "테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법, 이를 수행하기 위한 기록 매체 및", "출원인": "경북대학교 산학협력단", "발명자": "강보영"}}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇의 카메라를 통해 테이블 상태 이미지를 촬영하여 Deep Q-Network(이하, DQN)에 전달하는 단계;DQN의 이미지 분석을 통해 예측한 테이블 균형맞춤 동작을 구동하는 단계;사람으로부터 수신한 구동 동작에 대한 평가적 음성 피드백(evaluative feedback)으로부터 로봇의 테이블 균형맞춤 동작에 대한 긍정 및 부정의 정도를 평가하는 단계;음성 피드백의 긍정 및 부정의 정도에 따라 로봇의 테이블 균형맞춤 동작에 대한 보상 값을 계산하는 단계; 및다수의 에피소드를 학습시킨 음성 피드백 도입 DQN을 사용하여 테이블 상태 이미지 인식 및 모든 상태에서 테이블 균형맞춤 동작을 출력하는 단계;를 포함하는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 긍정 및 부정의 정도를 평가하는 단계는,자동음성인식 기술을 통해 마이크로 입력받은 음성 피드백 신호를 문자열로 변환하는 단계; 및감성 분석 기술을 통해 변환된 음성 피드백 어구의 긍정 및 부정의 정도를 감성 분석 값으로 도출하는 단계;를포함하는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 음성 피드백 신호를 문자열로 변환하는 단계는,학습의 초기 단계에 연속적으로 음성 피드백을 수신하거나 또는 일정한 간격을 두고 학습 전반에 걸쳐 음성 피드백을 제공받는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서, 상기 긍정 및 부정의 정도를 감성 분석 값으로 도출하는 단계는,감성 분석 값을 -1에서 1 사이의 실수값으로 설정하고, 긍정적 피드백일수록 1에 가까운 수치로, 부정적 피드백일수록 -1에 가까운 수치로 변환하는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 테이블 균형맞춤 동작을 출력하는 단계는,환경 보상과 사람 음성 피드백의 합이 최대화되는 정책을 학습하는 단계;를 포함하는, 테이블 균형맞춤 로봇을위한 인터액티브 강화학습 방법."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 테이블 균형맞춤 동작을 출력하는 단계는,환경 보상과 음성 피드백으로부터 보상 함수를 생성하는 단계;를 더 포함하는, 테이블 균형맞춤 로봇을 위한 인공개특허 10-2023-0018048-3-터액티브 강화학습 방법."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 테이블 균형맞춤 동작을 출력하는 단계는,테이블 이동 방향 및 정도에 따라 많이 올리기(aupup), 올리기(aup), 유지하기(a0), 내리기(adown) 및 많이 내리기(adowndown) 중 하나의 동작을 출력하는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항 내지 제7항 중 어느 하나의 항에 따른 상기 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법을 수행하기 위한 컴퓨터 프로그램이 기록된 컴퓨터로 판독 가능한 저장 매체."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "로봇의 카메라를 통해 테이블 상태 이미지를 촬영하여 Deep Q-Network(이하, DQN)에 전달하는 이미지 인식부;DQN의 이미지 분석을 통해 예측한 테이블 균형맞춤 동작을 구동하는 동작 구동부;사람으로부터 수신한 구동 동작에 대한 평가적 음성 피드백(evaluative feedback)으로부터 로봇의 테이블 균형맞춤 동작에 대한 긍정 및 부정의 정도를 평가하는 음성 피드백 평가부;음성 피드백의 긍정 및 부정의 정도에 따라 로봇의 테이블 균형맞춤 동작에 대한 보상 값을 계산하는 보상부;및다수의 에피소드를 학습시킨 음성 피드백 도입 DQN을 사용하여 테이블 상태 이미지 인식 및 모든 상태에서 테이블 균형맞춤 동작을 출력하는 DQN부;를 포함하는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 장치."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 음성 피드백 평가부는,자동음성인식 기술을 통해 마이크로 입력받은 음성 피드백 신호를 문자열로 변환하는 음성 피드백 변환부; 및감성 분석 기술을 통해 변환된 음성 피드백 어구의 긍정 및 부정의 정도를 감성 분석 값으로 도출하는 음성 피드백 도출부;를 포함하는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 장치."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 음성 피드백 변환부는,학습의 초기 단계에 연속적으로 음성 피드백을 수신하거나 또는 일정한 간격을 두고 학습 전반에 걸쳐 음성 피드백을 제공받는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 장치."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서, 상기 음성 피드백 도출부는,감성 분석 값을 -1에서 1 사이의 실수값으로 설정하고, 긍정적 피드백일수록 1에 가까운 수치로, 부정적 피드백일수록 -1에 가까운 수치로 변환하는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 장치.공개특허 10-2023-0018048-4-청구항 13 제9항에 있어서, 상기 DQN부는,환경 보상과 사람 음성 피드백의 합이 최대화되는 정책을 학습하는, 테이블 균형맞춤 로봇을 위한 인터액티브강화학습 장치."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 DQN부는,환경 보상과 음성 피드백으로부터 보상 함수를 생성하는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 장치."}
{"patent_id": "10-2021-0099728", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서, 상기 DQN부는,테이블 이동 방향 및 정도에 따라 많이 올리기(aupup), 올리기(aup), 유지하기(a0), 내리기(adown) 및 많이 내리기(adowndown) 중 하나의 동작을 출력하는, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 장치."}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법은, 로봇의 카메라를 통해 테이블 상태 이미지를 촬영하 여 Deep Q-Network(이하, DQN)에 전달하는 단계; DQN의 이미지 분석을 통해 예측한 테이블 균형맞춤 동작을 구동 하는 단계; 사람으로부터 수신한 구동 동작에 대한 평가적 음성 피드백(evaluative feedback)으로부터 로봇의 테 이블 균형맞춤 동작에 대한 긍정 및 부정의 정도를 평가하는 단계; 음성 피드백의 긍정 및 부정의 정도에 따라 로봇의 테이블 균형맞춤 동작에 대한 보상 값을 계산하는 단계; 및 다수의 에피소드를 학습시킨 음성 피드백 도 입 DQN을 사용하여 테이블 상태 이미지 인식 및 모든 상태에서 테이블 균형맞춤 동작을 출력하는 단계;를 포함한 다. 이에 따라, 사람의 음성피드백을 통한 자연스러운 방식으로 로봇이 사람과 협력하여 물체 균형맞춤 작업을 학습할 수 있다."}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법, 이를 수행하기 위한 기록 매체 및 장치에 관한 것으로서, 더욱 상세하게는 사람의 음성피드백을 통해 직관적이고 빠르게 테이블 균형맞춤 작업을 학습하 는 협력로봇 학습 기술에 관한 것이다."}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근, 로봇 기술 발달로 인공지능 기술을 탑재한 서비스 로봇이 증가하고 있다. 그 예로 박물관 전시안내 로봇 이나 카페 서빙 로봇, 물체 운반 로봇 등이 있다. 로봇은 일상 속 다양한 환경에서 사람의 업무를 대신 또는 함 께 수행하며, 이를 위해 사람과 협동하는 로봇에 대한 연구가 활발히 이루어져 왔다. 로봇 학습 기술 중 하나인 강화학습은 에이전트 로봇이 시행착오를 통해 최대 보상을 얻는 동작을 학습하는 방 법이다. 강화학습에서 보상은 일반적으로 상태별 에이전트 동작에 따라 주어지며, 사람-에이전트 간 실시간 상 호작용을 통해 보상이 주어질 경우 이를 인터액티브 강화학습이라고 한다. 인터액티브 강화학습에서 상호작용 방법 중 하나인 리워드 쉐이핑(reward shaping)은 사람 트레이너가 강화학습 에이전트의 동작에 긍정적 또는 부정적 피드백을 제공하여 보상 함수를 수정하는 기술이다. 그러나, 인터액티브 강화학습 선행 연구에서 상호작용에 적은 수의 어구 사용으로 피드백의 종류가 한정적이며, 에이전트의 동작 선택을 위한 모델링에 전문적 공학 지식이 요구된다. 이에, 일상의 로봇 증가에 맞추어, 로봇 활용도 향상을 위해서는 프로그래밍 지식이 없는 사람도 로봇 훈련이 가능하도록 다양한 피드백을 통한 학습 시스템의 필요성이 제기된다. 선행기술문헌특허문헌 (특허문헌 0001) KR 10-2233739 B1 (특허문헌 0002) KR 10-2020-0025532 A (특허문헌 0003) KR 10-2016-0142949 A"}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "이에, 본 발명의 기술적 과제는 이러한 점에서 착안된 것으로 본 발명의 목적은 사람의 음성 피드백을 통해 테 이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법을 제공하는 것이다. 본 발명의 다른 목적은 상기 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법을 수행하기 위한 컴퓨터 프로그램이 기록된 기록 매체를 제공하는 것이다. 본 발명의 또 다른 목적은 상기 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법을 수행하기 위한 장치 를 제공하는 것이다."}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 본 발명의 목적을 실현하기 위한 일 실시예에 따른 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법은, 로봇의 카메라를 통해 테이블 상태 이미지를 촬영하여 Deep Q-Network(이하, DQN)에 전달하는 단계; DQN의 이미지 분석을 통해 예측한 테이블 균형맞춤 동작을 구동하는 단계; 사람으로부터 수신한 구동 동작에 대 한 평가적 음성 피드백(evaluative feedback)으로부터 로봇의 테이블 균형맞춤 동작에 대한 긍정 및 부정의 정 도를 평가하는 단계; 음성 피드백의 긍정 및 부정의 정도에 따라 로봇의 테이블 균형맞춤 동작에 대한 보상 값 을 계산하는 단계; 및 다수의 에피소드를 학습시킨 음성 피드백 도입 DQN을 사용하여 테이블 상태 이미지 인식 및 모든 상태에서 테이블 균형맞춤 동작을 출력하는 단계;를 포함한다. 본 발명의 실시예에서, 상기 긍정 및 부정의 정도를 평가하는 단계는, 자동음성인식 기술을 통해 마이크로 입력 받은 음성 피드백 신호를 문자열로 변환하는 단계; 및 감성 분석 기술을 통해 변환된 음성 피드백 어구의 긍정 및 부정의 정도를 감성 분석 값으로 도출하는 단계;를 포함할 수 있다. 본 발명의 실시예에서, 상기 음성 피드백 신호를 문자열로 변환하는 단계는, 학습의 초기 단계에 연속적으로 음 성 피드백을 수신하거나 또는 일정한 간격을 두고 학습 전반에 걸쳐 음성 피드백을 제공받을 수 있다. 본 발명의 실시예에서, 상기 긍정 및 부정의 정도를 감성 분석 값으로 도출하는 단계는, 감성 분석 값을 -1에서 1 사이의 실수값으로 설정하고, 긍정적 피드백일수록 1에 가까운 수치로, 부정적 피드백일수록 -1에 가까운 수 치로 변환할 수 있다. 본 발명의 실시예에서, 상기 테이블 균형맞춤 동작을 출력하는 단계는, 환경 보상과 사람 음성 피드백의 합이 최대화되는 정책을 학습하는 단계;를 포함할 수 있다. 본 발명의 실시예에서, 상기 테이블 균형맞춤 동작을 출력하는 단계는, 환경 보상과 음성 피드백으로부터 보상 함수를 생성하는 단계;를 더 포함할 수 있다. 본 발명의 실시예에서, 상기 테이블 균형맞춤 동작을 출력하는 단계는, 테이블 이동 방향 및 정도에 따라 많이 올리기(aupup), 올리기(aup), 유지하기(a0), 내리기(adown) 및 많이 내리기(adowndown) 중 하나의 동작을 출력할 수 있 다. 상기한 본 발명의 다른 목적을 실현하기 위한 일 실시예에 따른 컴퓨터로 판독 가능한 저장 매체에는, 상기 테 이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법을 수행하기 위한 컴퓨터 프로그램이 기록되어 있다. 상기한 본 발명의 또 다른 목적을 실현하기 위한 일 실시예에 따른 테이블 균형맞춤 로봇을 위한 인터액티브 강 화학습 장치는, 로봇의 카메라를 통해 테이블 상태 이미지를 촬영하여 Deep Q-Network(이하, DQN)에 전달하는 이미지 인식부; DQN의 이미지 분석을 통해 예측한 테이블 균형맞춤 동작을 구동하는 동작 구동부; 사람으로부터 수신한 구동 동작에 대한 평가적 음성 피드백(evaluative feedback)으로부터 로봇의 테이블 균형맞춤 동작에 대 한 긍정 및 부정의 정도를 평가하는 음성 피드백 평가부; 음성 피드백의 긍정 및 부정의 정도에 따라 로봇의 테 이블 균형맞춤 동작에 대한 보상 값을 계산하는 보상부; 및 다수의 에피소드를 학습시킨 음성 피드백 도입 DQN 을 사용하여 테이블 상태 이미지 인식 및 모든 상태에서 테이블 균형맞춤 동작을 출력하는 DQN부;를 포함한다. 본 발명의 실시예에서, 상기 음성 피드백 평가부는, 자동음성인식 기술을 통해 마이크로 입력받은 음성 피드백 신호를 문자열로 변환하는 음성 피드백 변환부; 및 감성 분석 기술을 통해 변환된 음성 피드백 어구의 긍정 및 부정의 정도를 감성 분석 값으로 도출하는 음성 피드백 도출부;를 포함할 수 있다. 본 발명의 실시예에서, 상기 음성 피드백 변환부는, 학습의 초기 단계에 연속적으로 음성 피드백을 수신하거나 또는 일정한 간격을 두고 학습 전반에 걸쳐 음성 피드백을 제공받을 수 있다. 본 발명의 실시예에서, 상기 음성 피드백 도출부는, 감성 분석 값을 -1에서 1 사이의 실수값으로 설정하고, 긍 정적 피드백일수록 1에 가까운 수치로, 부정적 피드백일수록 -1에 가까운 수치로 변환할 수 있다. 본 발명의 실시예에서, 상기 DQN부는, 환경 보상과 사람 음성 피드백의 합이 최대화되는 정책을 학습할 수 있다. 본 발명의 실시예에서, 상기 DQN부는, 환경 보상과 음성 피드백으로부터 보상 함수를 생성할 수 있다. 본 발명의 실시예에서, 상기 DQN부는, 테이블 이동 방향 및 정도에 따라 많이 올리기(aupup), 올리기(aup), 유지 하기(a0), 내리기(adown) 및 많이 내리기(adowndown) 중 하나의 동작을 출력할 수 있다."}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이와 같은 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법에 따르면, 사람의 음성피드백을 통한 자연스 러운 방식으로 로봇이 사람과 협력하여 물체 균형맞춤 작업을 학습할 수 있게 한다."}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "후술하는 본 발명에 대한 상세한 설명은, 본 발명이 실시될 수 있는 특정 실시예를 예시로서 도시하는 첨부 도 면을 참조한다. 이들 실시예는 당업자가 본 발명을 실시할 수 있기에 충분하도록 상세히 설명된다. 본 발명의 다양한 실시예는 서로 다르지만 상호 배타적일 필요는 없음이 이해되어야 한다. 예를 들어, 여기에 기재되어 있 는 특정 형상, 구조 및 특성은 일 실시예에 관련하여 본 발명의 정신 및 범위를 벗어나지 않으면서 다른 실시예 로 구현될 수 있다. 또한, 각각의 개시된 실시예 내의 개별 구성요소의 위치 또는 배치는 본 발명의 정신 및 범 위를 벗어나지 않으면서 변경될 수 있음이 이해되어야 한다. 따라서, 후술하는 상세한 설명은 한정적인 의미로서 취하려는 것이 아니며, 본 발명의 범위는, 적절하게 설명된다면, 그 청구항들이 주장하는 것과 균등한 모든 범위와 더불어 첨부된 청구항에 의해서만 한정된다. 도면에서 유사한 참조부호는 여러 측면에 걸쳐서 동일하거 나 유사한 기능을 지칭한다. 이하, 도면들을 참조하여 본 발명의 바람직한 실시예들을 보다 상세하게 설명하기로 한다. 도 1은 본 발명의 일 실시예에 따른 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 장치의 블록도이다. 본 발명에 따른 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 장치(10, 이하 장치)는 사람의 음성피드백을 통해 테이블 균형맞춤 작업을 학습하는 협력로봇 학습 기술을 구현한다. 도 1을 참조하면, 본 발명에 따른 장치는 이미지 인식부, 동작 구동부, 음성 피드백 평가부 , 보상부 및 DQN부를 포함한다. 본 발명의 상기 장치는 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습을 수행하기 위한 소프트웨어(애 플리케이션)가 설치되어 실행될 수 있으며, 상기 이미지 인식부, 상기 동작 구동부, 상기 음성 피드 백 평가부, 상기 보상부 및 상기 DQN부의 구성은 상기 장치에서 실행되는 상기 테이블 균형 맞춤 로봇을 위한 인터액티브 강화학습을 수행하기 위한 소프트웨어에 의해 제어될 수 있다. 상기 장치는 별도의 단말이거나 또는 단말의 일부 모듈일 수 있다. 또한, 상기 이미지 인식부, 상기 동작 구동부, 상기 음성 피드백 평가부, 상기 보상부 및 상기 DQN부의 구성은 통합 모듈로 형성되거나, 하나 이상의 모듈로 이루어 질 수 있다. 그러나, 이와 반대로 각 구성은 별도의 모듈로 이루어질 수도 있다. 상기 장치는 이동성을 갖거나 고정될 수 있다. 상기 장치는, 서버(server) 또는 엔진(engine) 형태일 수 있으며, 디바이스(device), 기구(apparatus), 단말(terminal), UE(user equipment), MS(mobile station), 무선기기(wireless device), 휴대기기(handheld device) 등 다른 용어로 불릴 수 있다. 상기 장치는 운영체제(Operation System; OS), 즉 시스템을 기반으로 다양한 소프트웨어를 실행하거나 제작 할 수 있다. 상기 운영체제는 소프트웨어가 장치의 하드웨어를 사용할 수 있도록 하기 위한 시스템 프로그램으 로서, 안드로이드 OS, iOS, 윈도우 모바일 OS, 바다 OS, 심비안 OS, 블랙베리 OS 등 모바일 컴퓨터 운영체제 및 윈도우 계열, 리눅스 계열, 유닉스 계열, MAC, AIX, HP-UX 등 컴퓨터 운영체제를 모두 포함할 수 있다. 강화학습에서 학습 속도 향상 기술 중 하나는 사람이 트레이너(Humamn trainer)로서 에이전트를 지도하는 것이 며, 대표적인 예로 모방을 통한 학습, 시연을 통한 학습, 피드백을 제공하는 학습이 있다. 그 중 피드백 제공 학습을 살펴보면 마우스, 리모콘 등을 통해 피드백을 제공하는 인터액티브 학습 플랫폼 디자 인 연구, 인터액티브 강화학습 알고리즘 연구, 자연어 음성 피드백을 통한 인터액티브 강화학습 연구가 있다. 이러한 연구들의 공통점은 로봇 또는 컴퓨터가 사람과 상호작용하는 리워드 쉐이핑 학습을 통해 강화학습의 학 습 시간을 감소시켜 수렴 속도를 개선하고 에이전트의 목표 동작 학습 성능 향상이다. 그러나, 이러한 인터액티브 강화학습플랫폼 디자인 선행연구에서는 사람 피드백 제공에 마우스, 리모콘과 같은 입력 하드웨어가 요구되며, 이는 사람과 자연스러운 상호작용이 이루어진다고 보기 어렵다. 또한, 인터액티브 강화학습에 피드백 학습 알고리즘 개발에 초점을 맞춘 접근법들은 적절한 정적 보상 함수를 설계하거나 보상 함수 자체를 학습시키기 위해서 복잡한 도메인 지식을 요구하며, 보상 함수의 학습 자체에도 시간이 소요된다. 반면, 본 발명에서 제안한 시스템에서는 자연어 음성 피드백으로부터 직접 보상을 형성하기 때문에 학습 방법이 자연스러우며, 로봇 학습에 인공지능 도메인 지식 또는 사람의 노력을 요구하는 정도가 적다. 또한, 본 발명에서 제안한 시스템은 미리 학습된 감성 분석 모듈을 사용하여 입력된 음성 피드백에 대한 긍정 및 부정의 정도를 분석해 이를 보상값으로 변환한다. 따라서, 어떤 피드백 어구가 입력되어도 음성 피드백을 안 정적으로 보상값으로 변환해 DQN 강화학습에 사용할 수 있다. 일련의 선행연구를 통해 인터액티브 강화학습은 학습성능 향상에 도움을 주나, 사람과 자연스러운 상호작용 방 식을 사용하지 않으며 로봇 학습 모델링에 전문 심화 지식을 요구한다는 한계를 가진다. 본 발명에서는 이러한 한계점 해소를 위해 음성 피드백과 학습된 음성인식 및 감성 분석 기술을 사용하여 자연 스러운 학습을 위한 인터액티브 강화학습 시스템을 디자인하였다. 이하에서는, 먼저 음성 피드백을 기반으로 하는 테이블 균형맞춤 로봇을 위해 본 발명에서 제안된 딥 RL 프레임 워크를 설명한다. 로봇이 학습하고자 하는 과제는 사람과 협력하여 테이블을 들어올릴 때 균형을 유지하는 것이다. 음성 피드백을 통한 인간과 로봇의 상호작용 절차는 도 2와 같으며, 도 3은 본 발명이 제안하는 시스템의 전체 작업도이다. 도 2 및 도 3을 참조하면, 상기 이미지 인식부는 로봇의 카메라를 통해 테이블 상태 이미지를 촬영하여 Deep Q-Network(이하, DQN)에 전달한다. 이후 로봇은 상기 동작 구동부를 통해 DQN의 이미지 분석을 통해 예측한 테이블 균형맞춤 동작을 구동한다. 로봇은 동작 구동 후 사람으로부터 구동 동작에 대한 평가적 음성 피드백(evaluative feedback)을 받 는다. 예를 들어, 음성 피드백은 로봇의 마이크를 통해 입력될 수 있으며, 상기 음성 피드백 평가부를 거쳐 수치 값으로 변환된 후 DQN 알고리즘의 환경 보상에 통합된다. 상기 보상부는 음성 피드백의 긍정 및 부정의 정도에 따라 로봇의 테이블 균형맞춤 동작에 대한 보상 값을 계산한다. 위 과정의 반복을 통해 로봇은 상기 DQN부를 통해 환경 보상과 사람 음성 피드백의 합이 최대화되는 정책 을 학습하며, 학습의 결과로 협력 테이블 균형맞춤 동작을 수행할 수 있게 된다. 본 발명의 일 구현예에서, 테이블 균형맞춤 작업을 학습할 로봇은 Softbank의 NAO 로봇이며, 사용된 테이블은 가로 31cm, 세로 23cm, 높이 6cm의 직육면체 모양의 테이블이다. 또한, 학습에 사용할 테이블 상태 이미지는 NAO 로봇에 탑재된 하부 카메라를 사용하여 촬영하였다. 본 발명에서 제안된 시스템의 로봇은 음성 피드백 도입 DQN을 사용하여 테이블 상태 이미지 인식 및 테이블 균 형맞춤 동작을 출력한다. DQN은 Q-러닝 기반 강화학습에 딥 합성곱 신경망을 결합한 기술이며, 입력 이미지와 동작이 주어졌을 때 상태-동작 가치 함수(Q함수)를 추정하는 알고리즘이다. 도 4의 알고리즘은 음성 피드백 기반 인터액티브 DQN의 훈련 과정을 나타낸다. 본 훈련 과정은 DQN 훈련과정과 같으며 로봇 동작 후 음성피드백 기반 인터액티브 과정이 추가되었다. 입력 상태 s는 테이블 이미지 xt이며, xt 는 로봇 카메라로 촬영한 테이블 균형 상태를 나타내는 128Ｘ170 사이즈의 RGB 이미지이다. 환경은 훈련 데이터셋에서 테이블 상태 이미지를 선택한 후 DQN 에이전트인 로봇에게 전달한다. 로봇은 탐험을 위해 의 확률로 랜덤 동작을 선택하는 -greedy 정책에 따라 현재 동작을 결정한다. 랜덤 동작을 선택하지 않을 경우, 에이전트는 상태-동작 가치 함수인 Q함수의 값을 최대화하는 동작을 선택하며, DQN이 예측하고자 하는 Q 함수는 아래의 수학식 1과 같다. [수학식 1]"}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서 s는 상태, a는 로봇 동작, r은 현재 상태에서 동작을 수행하여 다음 상태로 갈 때 로봇이 받는 보 상을 의미한다. Q함수는 상태 s에서 동작 a를 실행할 때 받는 누적 보상의 기댓값으로 표현되며, γ는 감가율로 미래 상태의 Q값의 영향력을 감소시킨다. 도 5를 참조하면, 본 발명의 제안 시스템에서 테이블의 기울기 상태에 따라 사람 동작 상태는 총 5개로 올리기 (sup), 유지하기(s0), 내리기(sdown)로 나누어지며, 올리고 내린 정도에 따라 많이 올리기(supup), 많이 내리기 (sdowndown)로 구성되며, s의 아래 첨자는 사람의 동작을 나타낸다. 로봇은 무릎 관절 구동 값을 조정하여 테이블 균형맞춤 동작 a를 구동한다. 테이블 이동 방향 및 정도에 따라 aupup, aup, a0, adown, adowndown의 5가지 동작이 정의된다.동작 구동 후 에이전트는 사람의 음성 피드백과 환경 보상을 받는다. 강화학습 시스템의 환경 보상은 아래의 표 1과 같이 정의된다. [표 1]"}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "환경은 로봇이 목표 상태인 균형 유지 상태(s0)에 도달할 경우 +0.5의 보상을, 목표가 아닌 상태에 도달하는 동 작을 출력할 경우 -0.3의 보상을 제공한다. 또한, 사람 동작 상태를 supup로 인식한 상태에서 adown 동작을 반환하 는 것과 같이 균형맞춤 작업 모델 외 동작을 출력할 경우 -0.5의 보상이 제공된다. 인터액티브 음성 피드백은 로봇의 동작에 대한 사람의 음성 평가이다. 사람은 로봇의 동작에 따라 변화한 테이 블 균형 상태를 확인한 후 로봇이 목표 상태에 도달하였을 경우에는 긍정적 음성 피드백을, 그렇지 않을 경우에 는 부정적 음성 피드백을 제공한다. 제공된 음성 피드백은 음성 피드백 평가부을 통해 수치값으로 변환된 후, 스케일 조정 상수 k와 곱해져 강 화학습 환경 보상에 합산된다. 사람이 음성 피드백을 제공할 때 로봇은 피드백과 환경 보상을 사용하며, 피드백 이 없을 때는 환경 보상만을 사용한다. 음성 피드백 평가부에 대해서는 아래에서 더 자세히 설명한다. θ는 신경망의 파라미터를 의미한다. DQN에서는 yt를 타겟으로 보고 yt와 신경망에 의한 추정치인 Q(st,at)의 오 차를 줄이는 방향으로 학습을 진행한다. 따라서 DQN 모델 업데이트는 평균제곱오차를 계산한 손실 함수 L(θ)를 통해 매 에피소드마다 이루어진다. L(θ)를 최소화하는 방향으로 θ를 반복적으로 업데이트하면 Q함수는 점점 최적의 상태-동작 가치함수에 가까워지며 에이전트는 최적의 행동을 학습한다. 이러한 과정을 통해 로봇은 테이블 균형맞춤을 위해 사람의 음성피드백을 적용한 인터액티브 DQN을 학습할 수 있으며, DQN 프레임워크에서 음성 피드백을 활용하기 위해 본 발명에서는 음성 피드백 평가부을 통해 환경 보상과 음성 피드백으로부터 보상 함수를 구현하였다. 상기 음성 피드백 평가부는 사람으로부터 수신한 구동 동작에 대한 평가적 음성 피드백(evaluative feedback)으로부터 로봇의 테이블 균형맞춤 동작에 대한 긍정 및 부정의 정도를 평가한다. 도 3과 같이 음성 피드백 평가부는 자동음성인식(ASR)과 감성 분석(sentiment analysis)의 두 과정으로 구 성된다. 먼저, 로봇은 ASR을 통해 마이크로 입력받은 자연어 음성 피드백 신호를 입력받은 후 신호를 문자열로 변환하여 (transcribe) 출력한다. 본 발명에서 사용하는 ASR 시스템은 예를 들어, Google cloud의 Speech-to-text로 실 시간으로 입력된 음성과 상응하는 텍스트 표기를 지원하는 클라우드 기반 서비스일 수 있다. ASR 시스템은 온라인 및 오프라인 음성 오디오 처리를 지원한다. 따라서, 인터넷 사용 가능 여부에 관계없이 에 이전트의 학습 환경에 맞는 유동적인 음성 입력 처리방법을 위해 Google cloud의 ASR 시스템을 채택할 수 있다. 자동 음성 인식을 통해 얻은 문장의 문자열을 이용해, 감성분석은 음성 피드백 어구의 긍정 및 부정의 정도를 파악한다. 음성 피드백의 긍정, 부정 정도인 감성 분석 값은 -1에서 1 사이의 실수값이며, 긍정적 피드백일수록 1에 가까운 수치로, 부정적 피드백일수록 -1에 가까운 수치로 변환된다. 일 실시예에서, 감성 분석에는 구글 자연어 API(Google Natural Language API)를 사용할 수 있다. 해당 서비스 는 구현상의 과정에서 감성 분석 결과의 가공 및 수정이 용이하기 때문에 본 발명 시스템의 구현에 사용될 수 있다. 이하에서는, 본 발명에서 제안한 인터액티브 딥강화학습 모델의 성능 검증을 위한 음성 피드백 어구 데이터셋 구축 및 인식 모듈 평가, 인터액티브 DQN 모델 실험결과에 대해 설명한다. 먼저 실험에 사용할 음성 피드백 어구 데이터셋을 말뭉치(corpora)를 통해 구축하였다. 데이터셋 구축을 위해 사용한 말뭉치는 Sentiment lexicon from Bing Liu and collaborators, AFINN from Finn rup Nielsen, Teacher's words from Classroom English from Hankookmunhwasa이다. 말뭉치로부터 추출한 피드백 데이터셋 어구는 총 100개이며 긍정적 피드백 어구 50개, 부정적 피드백 50개를 구 축하였다. 피드백 어구는 주로 동작 평가에 대한 짧은 문장 또는 단어이며 표 2는 데이터셋의 일부 피드백 어구 와 해당 어구의 음성 인식 및 변환을 거친 감성분석 값의 예시이다. [표 2]"}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "본 발명에서 사용한 ASR인 Google Cloud의 speech-to-text에서 3회에 걸쳐 구축된 피드백 어구 데이터셋 인식 정확도를 테스트한 결과, 평균 86%의 문장 인식율을 보였다. 문자열로 변환된 음성 피드백의 감성 분석을 위한 Google Natural Language API에서 마찬가지로 100개의 피드백 어구 데이터셋을 3회 테스트한 결과 문장 인식율 이 평균 96%의 정확도를 보였다. 100% 이하의 정확도는 음성 피드백 평가부의 오작동으로 에이전트가 잘못 된 보상을 받을 수 있음을 의미한다. 본 발명에서는 잘못된 보상이 주어지는 경우를 모두 고려하였으며, 이와 같은 오류에도 불구하고 인터액티브 음 성 피드백의 사용이 에이전트의 목표 작업 학습을 촉진할 수 있음을 실험을 통해 확인하였다. 본 발명에서 제안한 시스템을 테스트하기 위한 DQN 실험 셋팅은 크게 두 가지로 구현되었으며, 음성 피드백 모 델 실험과 학습률 및 음성 피드백 스케일 조정실험이다. 첫째, 음성 피드백 모델 실험에서 연속적 제공 음성 피 드백(Consec-VF)과 간헐적 제공 음성 피드백(Prdc-VF)이다. 도 6을 참조하면, 학습 과정에서 사람은 학습의 초기 단계에 연속적으로 음성 피드백을 제공하거나(Consec- VF) 또는 일정한 간격을 두고 학습 전반에 걸쳐 음성 피드백을 제공(Prdc-VF)할 수 있다. Consec-VF에서는 학습 초기에 100회의 피드백을 연속적으로 제공하며, Prdc-VF는 2,000회의 에피소드마다 10회의 피드백을 제공 하여 강화학습 보상 함수에 반영한다. 실험을 통해 두 피드백 제공 모델과 베이스라인인 음성 피드백 미사용 DQN의 학습 결과를 비교하였으며, 추가적 으로 Consec-VF에 5가지의 옵티마이저 비교 실험을 진행하였다. 둘째, 학습률 및 음성피드백 스케일 조정 실험 은 Consec-VF 모델에 학습률과 음성 피드백의 스케일 조정을 적용한 후 그에 따른 인터액티브 DQN 모델 성능 변 화를 관찰하였다. 위 두 가지 실험에서 모델 설정별로 30회씩 실험을 진행하였으며, 훈련이 끝난 후 최적 정책 수렴 비율을 계산 하여 성능을 평가하였다. DQN 훈련을 위한 하이퍼 파라미터 설정은 아래의 표 3에 나타나 있다. [표 3]"}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "음성 피드백 횟수를 제외한 모든 하이퍼 파라미터 설정은 제안한 인터액티브 딥강화학습 모델과 베이스라인 DQN 모두에 동일하게 적용된다. 이번 실험에서는 인터액티브 음성 피드백의 두 가지 제공 방식에 따른 모델 성능의 차이를 분석하며, 두 음성 피드백 모델은 Consec-VF과 Prdc-VF이다. 두 음성 피드백 모델의 총 음성피드백 제공 횟수는 표 3과 같이 20,000번 중 100번으로 동일하며, 이 외 에피소드는 표 1의 환경 보상만을 사용하였다. Consec-VF 모델은 학습 초기에 집중적으로 음성 피드백을 전달하여 초기 학습 방향을 잡기 위한 모델이며, Prdc-VF 모델은 음성피드백을 전반적으로 배치하여 사람의 피드백이 학습과정에 꾸준히 반영되도록 설계한 모델 이다. 두 모델과 베이스라인 DQN에 표 3의 하이퍼파라미터 설정을 적용하여 세 가지 옵티마이저로 실험한 결과는 아래 의 표 4에 나타나 있다. [표 4]"}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "먼저 Consec-VF 모델의 경우, 옵티마이저로 SGD와 Adam을 사용한 경우 최적 정책 수렴율이 86%, 96%로 음성 피 드백 미사용 DQN의 최적 정책 수렴율인 80%, 73%보다 더 높은 성능을 보인다. 특히 Adam을 옵티마이저로 사용하 여 학습한 결과에서 30회 실험 중 29회가 최적의 정책을 학습한 96%의 수렴율은 연속적 음성 피드백의 도입이 DQN의 성능을 크게 향상시켰다고 관찰된다. 반면 Prdc-VF 모델은 Consec-VF 모델뿐만 아니라 음성 피드백 미사용 DQN보다도 더 저조한 성능을 보이며, 이는 도 7을 통해 성능이 저조한 원인을 분석할 수 있다. 도 7은 베이스라인과 Consec-VF모델, Prdc-VF 모델의 학습 손실 그래프이다. 학습 손실 그래프인 도 7(a) 및 도 7(b)를 살펴보면, Consec-VF 학습모델과 음성 피드백 미사 용 DQN에서는 손실이 안정적으로 0에 수렴하고 있으나, 도 7(c)인 Prdc-VF 학습 모델에서는 학습 과정에서 손실 이 불안정하게 치솟는 현상을 확인할 수 있다. 이를 통해 음성 피드백의 간헐적 개입이 학습 과정에서 손실의 수렴을 방해하여 해당 모델이 다른 두 모델에 비 해 낮은 성능을 보였다고 분석하였다. 실험을 통해 Consec-VF 모델이 베이스라인과 Prdc-VF 모델에 비해 최적 정책을 더 잘 학습함이 관찰되었다. 위 실험의 심화 실험으로 연속적 음성 피드백의 사용이 일관적으로 모델 학습 성능을 이끌어내는지를 확인하기 위해 Consec-VF 모델에 Adagrad, Adadelta 옵티마이저를 추가하여 실험을 진행한 결과를 살펴본다. Consec-VF 모델과 베이스라인 DQN에 대해 30회의 실험을 진행한 후의 최적 정책 수렴율은 아래의 표 5에 나타나 있다. [표 5]"}
{"patent_id": "10-2021-0099728", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "Consec-VF 학습 모델의 경우 Momentum 옵티마이저를 제외한 모든 실험에서 30회의 실험 중 17회 이상(56%) 최적 정책으로 수렴하였으며, 베이스라인 DQN에 비해 향상된 최적 정책 수렴율을 보였다. 이와 같은 실험 결과를 통 해 테이블 밸런싱 작업을 위한 DQN에 인터액티브 음성 피드백 도입이 대부분의 옵티마이저 셋팅에서 모델 학습 성능을 향상시키는 경향성을 관찰할 수 있었다. 본 발명은 사람-로봇 협동 테이블 균형맞춤 작업을 위한 음성피드백 기반 인터액티브 딥강화학습 모델을 제안하 였다. 본 발명에서 제안한 기술은 ASR과 감성분석 기술을 사용하여 음성 피드백 평가부을 구현 및 DQN에 적용하였으며, 이를 통해 음성 피드백 제공이라는 가장 자연스러운 방식으로 로봇 학습이 이루어지는 시스템을 구현하였다. 실험 결과, 학습 초기에 연속적 음성 피드백을 제공하는 Consec-VF 모델이 대부분의 설정에서 음성 피드백 미사 용 DQN보다 높은 최적 정책 수렴 비율을 달성하여 향상된 성능을 보였다. 또한, 학습 성능이 저조하였던 Momentum 옵티마이저의 피드백 스케일 조정 상수 도입과 학습율 조정을 통해 성능 향상을 확인하고, 제안 시스 템을 통한 성능 향상의 일관성을 확보하였다. 도 8은 본 발명의 일 실시예에 따른 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법의 흐름도이다. 본 실시예에 따른 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법은, 도 8의 장치와 실질적으로 동 일한 구성에서 진행될 수 있다. 따라서, 도 8의 장치와 동일한 구성요소는 동일한 도면부호를 부여하고, 반 복되는 설명은 생략한다. 또한, 본 실시예에 따른 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법은 테이블 균형맞춤 로봇을 위 한 인터액티브 강화학습을 수행하기 위한 소프트웨어(애플리케이션)에 의해 실행될 수 있다. 본 발명에서 로봇은 Deep Q-Network(DQN)과 사람이 실시간으로 전달하는 음성 피드백을 통해 사람과 협동이 필 요한 테이블 균형맞춤 과제를 학습하며, 로봇은 사람 음성피드백 이해를 위해 자동음성인식과 자연어 감성분석 기술을 이용한다. 도 8을 참조하면, 본 실시예에 따른 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법은, 로봇의 카메라 를 통해 테이블 상태 이미지를 촬영하여 Deep Q-Network(이하, DQN)에 전달한다(단계 S10). DQN의 이미지 분석을 통해 예측한 테이블 균형맞춤 동작을 구동한다(단계 S20). 사람으로부터 수신한 구동 동작에 대한 평가적 음성 피드백(evaluative feedback)으로부터 로봇의 테이블 균형 맞춤 동작에 대한 긍정 및 부정의 정도를 평가한다(단계 S30). 긍정 및 부정의 정도를 평가하는 단계(단계 S30)는, 먼저, 자동음성인식 기술을 통해 마이크로 입력받은 음성 피드백 신호를 문자열로 변환한다. 일 실시예에서, 학습의 초기 단계에 연속적으로 음성 피드백을 수신할 수 있 으나, 다른 실시예로서 일정한 간격을 두고 학습 전반에 걸쳐 음성 피드백을 제공받을 수 있다.이후, 감성 분석 기술을 통해 변환된 음성 피드백 어구의 긍정 및 부정의 정도를 감성 분석 값으로 도출한다. 일 실시예에서, 감성 분석 값을 -1에서 1 사이의 실수값으로 설정하고, 긍정적 피드백일수록 1에 가까운 수치로, 부정적 피드백일수록 -1에 가까운 수치로 변환할 수 있다. 음성 피드백의 긍정 및 부정의 정도에 따라 로봇의 테이블 균형맞춤 동작에 대한 보상 값을 계산한다(단계 S40). 다수의 에피소드를 학습시킨 음성 피드백 도입 DQN을 사용하여 테이블 상태 이미지 인식 및 모든 상태에서 테이 블 균형맞춤 동작을 출력한다(단계 S50). 일 실시예로서, 테이블 이동 방향 및 정도에 따라 많이 올리기(aupup), 올리기(aup), 유지하기(a0), 내리기(adown) 및 많이 내리기(adowndown) 중 하나의 동작을 출력할 수 있다. 이 단계에서, 환경 보상과 음성 피드백으로부터 보상 함수를 생성하고, 환경 보상과 사람 음성 피드백의 합이 최대화되는 정책을 학습할 수 있다. 본 발명에서는 로봇 활용도 향상을 위한 음성피드백 기반 인터액티브 딥강화학습 모델을 제안한다. 제안된 시스 템에서 로봇은 Deep Q-Network(DQN)을 사용하여 사람과 협동이 필요한 테이블 균형맞춤 과제를 수행하며, 리워 드 쉐이핑을 통해 과제 수행 정책을 학습한다. 또한, 리워드 쉐이핑에 사용할 기술은 사람 음성 피드백이다. 리워드 쉐이핑을 사용하여 작업 수행 방법을 아는 사람이 음성으로 로봇 동작에 대해 실시간으로 긍정 및 부정의 피드백을 제공한다. 따라서, 피드백이 없는 기존 의 시스템보다 직관적이고 빠르게 에이전트는 최적의 정책을 학습할 수 있다. 실험 결과, 본 발명에서 제안한 음성피드백 기반 인터액티브 딥강화학습 모델은 최고 96%의 최적 정책 수렴율을 보였으며, 학습 성능이 저조한 모델에 음성 피드백 스케일 조정을 적용함으로써 최종적으로 모든 음성 피드백 기반 모델에서 성능이 향상되는 결과를 얻었다. 이와 같은, 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법은 애플리케이션으로 구현되거나 다양한 컴 퓨터 구성요소를 통하여 수행될 수 있는 프로그램 명령어의 형태로 구현되어 컴퓨터 판독 가능한 기록 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능한 기록 매체는 프로그램 명령어, 데이터 파일, 데이터 구조 등을 단독 으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터 판독 가능한 기록 매체에 기록되는 프로그램 명령어는 본 발명을 위하여 특별히 설계되고 구성된 것들이거니와 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능한 기록 매체의 예에는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD- ROM, DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 ROM, RAM, 플래시 메모리 등과 같은 프로그램 명령어를 저장하고 수행하도록 특별히 구성된 하드웨 어 장치가 포함된다. 프로그램 명령어의 예에는, 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사 용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함된다. 상기 하드웨어 장치는 본 발명에 따른 처 리를 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 이상에서는 실시예들을 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다. 산업상 이용가능성 로봇 기술 발달로 인공지능 기술을 탑재한 서비스 로봇이 증가하고 있다. 카페 서빙 로봇, 물체 운반 로봇과 같 이 일상 속 다양한 환경에서 로봇은 사람의 업무를 대신 또는 함께 수행하며, 이를 위해 사람과 협동하며 학습 하는 로봇의 필요성이 증가하고 있다. 본 발명은 이러한 경향에 따라 로봇 공학 지식이 없는 사람도 로봇 훈련 이 가능하도록 다양한 피드백을 통한 학습 시스템을 제공하므로, 로봇 활용도 향상에 유용하게 활용될 것으로 예상된다.부호의 설명 10: 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 장치 110: 이미지 인식부 130: 동작 구동부 150: 음성 피드백 평가부 170: 보상부 190: DQN부"}
{"patent_id": "10-2021-0099728", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 장치의 블록도이다. 도 2는 도 1의 장치의 음성 피드백을 통한 인간과 로봇의 상호작용 절차의 개념도이다. 도 3은 도 1의 장치를 포함하는 시스템의 전체 작업도이다. 도 4는 음성 피드백 기반 인터액티브 DQN의 훈련 과정을 나타내는 알고리즘이다. 도 5는 본 발명의 사람의 동작 상태 및 테이블 균형맞춤 로봇의 동작을 보여주는 도면이다. 도 6은 학습 Consec-VF 또는 Prdc-VF 방식으로 음성 피드백을 제공하는 것을 설명하기 위한 도면이다. 도 7은 본 발명의 성능을 검증하기 위한 실험의 결과로서 베이스라인과 Consec-VF모델, Prdc-VF 모델의 학습 손 실 그래프이다. 도 8은 본 발명의 일 실시예에 따른 테이블 균형맞춤 로봇을 위한 인터액티브 강화학습 방법의 흐름도이다."}
