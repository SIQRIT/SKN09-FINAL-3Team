{"patent_id": "10-2022-0185944", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0100683", "출원번호": "10-2022-0185944", "발명의 명칭": "준마코프 사후상태 액터-크리틱을 활용한 전력망 자동 운영 방법", "출원인": "한국과학기술원", "발명자": "김기응"}}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "상기 컴퓨터 장치의 프로세서에 의해 실행될 수 있는 컴퓨터 프로그램을 이용하여 수행되는 방법으로서,현재 시간 단계에서의 전력망 토폴로지 구성 정보와 전력망 구성요소들의 전력량 정보를 포함하는 전력망의 상태 정보를 바탕으로 그래프 구조 데이터로 표현되는 이상적인 전력망 연결 상태인 전력망의 목표 토폴로지를 생성하는 단계; 및 생성된 상기 전력망의 목표 토폴로지에 기반하여 전력선의 연결과 변전소의 버스바 할당이 이루어지도록 제어하는 단계를 포함하는 것을 특징으로 하는 전력망 자동 운영 방법."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 이상적인 전력망 연결 상태를 생성하는 단계는,상기 전력망의 상태 정보를 각 변전소에 연결된 발전소, 전력 부하, 전력선을 노드로 삼는 그래프 구조 데이터로 변환하는 단계;상기 그래프 구조 데이터에서 모든 전력선의 연결 상태를 확인하여 끊어진 전력선이 있다면 연결되게 하는단계; 및상기 모든 전력선 중에서 흐르는 전력의 양이 소정의 문턱값을 초과하는 전력선이 존재하는 경우, 그래프 신경망(Graph neural network)으로 구성된 상위레벨 정책에 의해 상기 이상적인 전력망 연결 상태인 전력망 목표 토폴로지를 생성하는 단계를 포함하는 것을 특징으로 하는 전력망 자동 운영 방법."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항 또는 제2항에 있어서, 상기 전력망 토폴로지 구성 정보는 전력망의 각 전력선의 연결성 정보와 각 변전소의 버스바 할당 상태 정보를 포함하고, 상기 전력망 구성요소들의 전력량 정보는 적어도 상기 전력망의 각 발전소에서 생산하여 제공하는 전력량, 상기 전력망의 각 부하에서 요구하는 전력량, 각 전력선이 운반하는 전력량, 각 전력선에 과부하가 일어난 기간에 관한 정보를 포함하는 것을 특징으로 하는 전력망 자동 운영 방법."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서, 상기 전력망 목표 토폴로지를 생성하는 단계는, 상위 및 하위레벨 정책이 전력망 환경과 상호작용하며 학습 데이터를 수집하는 단계;수집된 데이터에서, 전력망의 각 발전소로부터 생산되는 전력량, 각 부하에서 요구하는 전력량, 각 전력선이 운반하는 전력량, 각 전선의 연결 상태, 각 전선에 과부하가 일어난 기간의 정보를 그래프 구조 데이터로 변환시키고 이를 상기 상위레벨 정책이 생성한 상기 전력망 목표 토폴로지와 결합하여 사후 상태를 만드는 단계;상기 사후 상태가 얼마나 좋은지를 측정하는 그래프 신경망 기반의 가치 함수(Value function)를 반복 학습시키는 단계; 및반복 학습 도중 가치 함수를 기반으로 상기 상위레벨 정책을 반복 학습시키는 단계를 포함하는 것을 특징으로하는 전력망 운영 방법."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 제어하는 단계는, 하위레벨 정책에 의하여 제어되며, 변전소 제어 순서는 변전소에 연결된 전력선, 발전소 및 부하가 많을수록 우선적으로 제어되게 하는 순서인 것을 특징으로 하는 전력망 자동 운영방법."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2023-0100683-3-제2항에 있어서, 상기 소정의 문턱값을 초과하는 전력량이 흐르는 전력선이 존재하지 않거나, 또는 상기 전력망목표 토폴로지가 정해지면, 상기 전력망 목표 토폴로지와 실행 규칙에 따라 전력망에 실제로 반영되는 하위레벨행동을 수행하는 단계를 더 포함하는 것을 특징으로 하는 전력망 자동 운영 방법."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 하위레벨 행동을 수행하는 단계는, 전력망의 현재의 상태와 상기 상위레벨 정책이 만든상기 전력망 목표 토폴로지를 비교하는 단계; 하위레벨 정책이 상기 목표 토폴로지에 도달하기 위해 필요로 하는 하위레벨의 행동들을 어떠한 순서로 실제 환경에 적용할지를 결정하는 단계; 그리고 상기 하위레벨 정책이상기 전력망 목표 토폴로지를 달성했는지를 확인하여 상기 전력망 목표 토폴로지가 달성될 때까지 상기 하위레벨 행동들을 계속해서 실행하는 단계를 포함하는 것을 특징으로 하는 전력망 자동 운영 방법."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 전력망 목표 토폴로지에 도달하면 에이전트의 학습에 필요한 전이 데이터를 저장하는 단계; 종료조건이 만족되는지 판별하는 단계; 및 종료 조건을 만족시키면 종료하는 단계를 더 포함하며, 상기 종료 조건이 만족된 경우는 전력망 운영을 성공하여 정해진 시간 단계에 도달한 경우와 전력망 운영을 실패하여전력망에 고장이 발생한 경우를 포함하는 것을 특징으로 하는 전력망 자동 운영 방법."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 에이전트가 현재 정책을 학습하는 중인지 여부를 판별하는 단계; 및 현재 정책을 학습시키는중인 경우에 신경망으로 된 액터 레이어와 크리틱 레이어의 모델 파라미터를 업데이트하는 단계를 더 포함하며,상기 크리틱 레이어는 사후 상태를 입력으로 받아 그 사후 상태가 얼마만큼 가치가 있는지를 학습하며, 상기 액터 레이어는 현재 상태를 입력으로 받아 그 현재 상태에서는 어떠한 목표를 만들어야 하는지를 학습하는 것을특징으로 하는 전력망 자동 운영 방법."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 에이전트가 학습 중인 경우엔 추가적으로 전이 데이터를 모으고, 인공신경망의 가중치(weight)인 정책 파라미터 및 가치 모델 파라미터의 업데이트를 경사 하강법에 따라 수행하는 단계를 더 포함하는 것을 특징으로 하는 전력망 운영 방법."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항 내지 제10항 중 어느 한 항에 기재된 전력망 운영 방법을 수행하기 위하여 컴퓨터 판독 가능한 기록 매체에 저장된 컴퓨터 실행가능 프로그램."}
{"patent_id": "10-2022-0185944", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항 내지 제10항 중 어느 한 항에 기재된 전력망 운영 방법을 수행하기 위한 컴퓨터 프로그램이 기록된 컴퓨터 판독 가능한 기록 매체."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "준마코프 사후상태 액터-크리틱을 활용한 전력망 자동 운영 방법이 개시된다. 이 방법은 현재 시간 단계에서의 전력망 토폴로지 구성 정보(전력망의 각 전력선의 연결성 정보와 각 변전소의 버스바 할당 상태 정보)와 전력망 구성요소들의 전력량 정보(전력망의 각 발전소에서 생산하여 제공하는 전력량, 상기 전력망의 각 부하에서 요구 하는 전력량, 각 전력선이 운반하는 전력량, 각 전력선에 과부하가 일어난 기간에 관한 정보)를 포함하는 전력망 의 상태 정보를 바탕으로 그래프 구조 데이터로 표현되는 이상적인 전력망 연결 상태인 전력망의 목표 토폴로지 를 생성한다. 이를 위해, 전력망의 상태 정보를 각 변전소에 연결된 발전소, 전력 부하, 전력선을 노드로 삼는 그래프 구조 데이터로 변환할 수 있다. 생성된 목표 토폴로지에 기반하여 전력선의 연결과 변전소의 버스바 할당 이 이루어지도록 제어한다."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 전력망의 관리와 운영 기술 분야에 관한 것으로, 보다 상세하게는 인공지능 기술을 이용한 전력망의 관리 및 운영 기술에 관한 것이다."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "생산자로부터 소비자에게 전력을 공급하기 위한 상호 연결된 네트워크인 전력망은 현대 사회의 필수구성 요소가 되었다. 대규모 정전 사태 방지 및 안정적인 전력 공급을 위한 전력망 관리/운영은 전력을 기반으로 돌아가는 현대 사회에서 필수적이다. 전력망에서 안전하고 신뢰할 수 있는 전력 전송은 현대 사회에서 매우 중요하므로, 전력망은 제어실의 인간 전문가가 지속적으로 모니터링하고 관리한다. 이에 전력망의 운영의 자동 제어에 관한 관심이 높아지고 있다. 또한, 최근에는 태양, 풍력, 수력과 같은 지속 가능한 전원으로 전환함에 따라 전력망의 구조가 점점 더 커지고 복잡해져 가고 있다. 그에 따라 상태 및 작업 공간의 규모가 엄청나게 크기 때문에 실제 규모의 전력망을 관리 하는 것은 인간 전문가의 영역을 넘어서는 매우 복잡하고 어려운 작업이 되고 있다. 오늘날 전력망은 제어실에 상시 상주하는 사람 전문가에 의해 관리/운영이 되고 있는데, 사람 전문가가 전력망을 안전하고 효율적으로 관 리/운영하기가 점점 힘들어지고 있다. 이에 따라 인공지능기반의 전력망 자동 제어 시스템 개발의 필요성이 대 두되고 있다. 그러나 대규모 전력망의 자동 제어는 복잡하면서도 신뢰할 수 있는 의사 결정이 필요하기 때문에 어려운 작업이 다. 대부분의 접근 방식은 발전 또는 전력 부하 제어에 중점을 두었지만, 토폴로지 제어를 통한 전력망 관리(변 전소의 전력선 연결 및 버스 할당 변경)가 궁극적인 목표가 될 것이다. 토폴로지 제어는 전력망의 토폴로지를 재구성함으로써 전력의 흐름을 재조정할 수 있어 생산자로부터 소비자에게 효율적으로 전력을 전송하여 잉여 생 산을 방지할 수 있다. 전력망 토폴로지 제어에 대한 간단한 연구들이 있지만, 대규모 조합 및 비선형 특성으로 인해 이러한 방법은 실제 환경에 적용할 만한 실용적인 해결책을 제공하지 못하고 있다. 선행기술문헌 비특허문헌 (비특허문헌 0001) 1. A. N. Venkat, I. A. Hiskens, J. B. Rawlings, and S. J. Wright. Distributed mpc strategies with application to power system automatic generation control. IEEE Transactions on Control Systems Technology, 16:1192­1206, 2008. (비특허문헌 0002) 2. C. Zhao, U. Topcu, N. Li, and S. Low. Design and stability of load-side primary frequency control in power systems. IEEE Transactions on Automatic Control, 59:1177­1189, 2014. (비특허문헌 0003) 3. Q. Huang, R. Huang, W. Hao, J. Tan, R. Fan, and Z. Huang. Adaptive power system emergency control using deep reinforcement learning. IEEE Transactions on Smart Grid, 11:1171­ 1182, 2020. doi: 10.1109/TSG.2019.2933191. (비특허문헌 0004) 4. E. B. Fisher, R. P. O'Neill, and M. C. Ferris. Optimal transmission switching. IEEE Transactions on Power Systems, 23:1346­1355, 2008 (비특허문헌 0005) 5. A. Khodaei and M. Shahidehpour. Transmission switching in security-constrained unit commitment. IEEE Transactions on Power Systems, 25:1937­1945, 2010."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "심층 강화학습(Deep Reinforcement Learning)은 바둑 및 아케이드 비디오 게임과 같은 복잡한 순차적 의사 결정 작업에서 효과적인 결과를 보여주었다. 이에 따라 강화학습은 전력망 관리 문제를 해결할 수 있는 유망한 방법 론 중의 하나로 인식되고 있다. 따라서 본 발명의 일 목적은 전력망 운영의 문제를 효과적으로 해결할 수 있는 강화학습 알고리즘인 준마코프 사후상태 액터-크리틱(Semi-Markov Afterstate Actor-Critic: SMAAC)을 활용하여 사람 전문가의 개입 없이 전 력망을 온전히 운영할 수 있는 전력망 자동 운영 방법을 제공하는 것이다. 본 발명이 해결하고자 하는 과제는 상술한 과제들에 한정되는 것이 아니며, 본 발명의 사상 및 영역으로부터 벗 어나지 않는 범위에서 다양하게 확장될 수 있을 것이다."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 본 발명의 일 목적을 실현하기 위한 실시예들에 따른 전력망 자동 운영 방법은, 상기 컴퓨터 장치의 프로 세서에 의해 실행될 수 있는 컴퓨터 프로그램을 이용하여 수행되는 방법으로서, 현재 시간 단계에서의 전력망 토폴로지 구성 정보와 전력망 구성요소들의 전력량 정보를 포함하는 전력망의 상태 정보를 바탕으로 그래프 구 조 데이터로 표현되는 이상적인 전력망 연결 상태인 전력망의 목표 토폴로지를 생성하는 단계; 및 생성된 상기전력망의 목표 토폴로지에 기반하여 전력선의 연결과 변전소의 버스바 할당이 이루어지도록 제어하는 단계를 포 함한다. 예시적인 실시예에 있어서, 상기 이상적인 전력망 연결 상태를 생성하는 단계는, 상기 전력망의 상태 정보를 각 변전소에 연결된 발전소, 전력 부하, 전력선을 노드로 삼는 그래프 구조 데이터로 변환하는 단계; 상기 그래프 구조 데이터에서 모든 전력선의 연결 상태를 확인하여 끊어진 전력선이 있다면 연결되게 하는 단계; 및 상기 모 든 전력선 중에서 흐르는 전력의 양이 소정의 문턱값을 초과하는 전력선이 존재하는 경우, 그래프 신경망(Graph neural network)으로 구성된 상위레벨 정책에 의해 상기 이상적인 전력망 연결 상태인 전력망 목표 토폴로지를 생성하는 단계를 포함할 수 있다. 예시적인 실시예에 있어서, 상기 전력망 토폴로지 구성 정보는 전력망의 각 전력선의 연결성 정보와 각 변전소 의 버스바 할당 상태 정보를 포함하고, 상기 전력망 구성요소들의 전력량 정보는 적어도 상기 전력망의 각 발전 소에서 생산하여 제공하는 전력량, 상기 전력망의 각 부하에서 요구하는 전력량, 각 전력선이 운반하는 전력량, 각 전력선에 과부하가 일어난 기간에 관한 정보를 포함할 수 있다. 예시적인 실시예에 있어서, 상기 전력망 목표 토폴로지를 생성하는 단계는, 상위 및 하위레벨 정책이 전력망 환 경과 상호작용하며 학습 데이터를 수집하는 단계; 수집된 데이터에서, 전력망의 각 발전소로부터 생산되는 전력 량, 각 부하에서 요구하는 전력량, 각 전력선이 운반하는 전력량, 각 전선의 연결 상태, 각 전선에 과부하가 일 어난 기간의 정보를 그래프 구조 데이터로 변환시키고 이를 상기 상위레벨 정책이 생성한 상기 전력망 목표 토 폴로지와 결합하여 사후 상태를 만드는 단계; 상기 사후 상태가 얼마나 좋은지를 측정하는 그래프 신경망 기반 의 가치 함수(Value function)를 반복 학습시키는 단계; 및 반복 학습 도중 가치 함수를 기반으로 상기 상위레 벨 정책을 반복 학습시키는 단계를 포함할 수 있다. 예시적인 실시예에 있어서, 상기 제어하는 단계는, 하위레벨 정책에 의하여 제어되며, 변전소 제어 순서는 변전 소에 연결된 전력선, 발전소 및 부하가 많을수록 우선적으로 제어되게 하는 순서일 수 있다. 예시적인 실시예에 있어서, 상기 전력망 자동 운영 방법은 상기 소정의 문턱값을 초과하는 전력량이 흐르는 전 력선이 존재하지 않거나, 또는 상기 전력망 목표 토폴로지가 정해지면, 상기 전력망 목표 토폴로지와 실행 규칙 에 따라 전력망에 실제로 반영되는 하위레벨 행동을 수행하는 단계를 더 포함할 수 있다. 예시적인 실시예에 있어서, 상기 하위레벨 행동을 수행하는 단계는, 전력망의 현재의 상태와 상기 상위레벨 정 책이 만든 상기 전력망 목표 토폴로지를 비교하는 단계; 하위레벨 정책이 상기 목표 토폴로지에 도달하기 위해 필요로 하는 하위레벨의 행동들을 어떠한 순서로 실제 환경에 적용할지를 결정하는 단계; 그리고 상기 하위레벨 정책이 상기 전력망 목표 토폴로지를 달성했는지를 확인하여 상기 전력망 목표 토폴로지가 달성될 때까지 상기 하위레벨 행동들을 계속해서 실행하는 단계를 포함할 수 있다. 예시적인 실시예에 있어서, 상기 전력망 목표 토폴로지에 도달하면 에이전트의 학습에 필요한 전이 데이터를 저 장하는 단계; 종료조건이 만족되는지 판별하는 단계; 및 종료 조건을 만족시키면 종료하는 단계를 더 포함할 수 있다. 여기서, 상기 종료 조건이 만족된 경우는 전력망 운영을 성공하여 정해진 시간 단계에 도달한 경우와 전 력망 운영을 실패하여 전력망에 고장이 발생한 경우를 포함할 수 있다. 예시적인 실시예에 있어서, 상기 전력망 자동 운영 방법은 에이전트가 현재 정책을 학습하는 중인지 여부를 판 별하는 단계; 및 현재 정책을 학습시키는 중인 경우에 신경망으로 된 액터 레이어와 크리틱 레이어의 모델 파라 미터를 업데이트하는 단계를 더 포함할 수 있다. 여기서, 상기 크리틱 레이어는 사후 상태를 입력으로 받아 그 사후 상태가 얼마만큼 가치가 있는지를 학습하며, 상기 액터 레이어는 현재 상태를 입력으로 받아 그 현재 상태 에서는 어떠한 목표를 만들어야 하는지를 학습할 수 있다. 예시적인 실시예에 있어서, 상기 전력망 자동 운영 방법은 에이전트가 학습 중인 경우엔 추가적으로 전이 데이 터를 모으고, 인공신경망의 가중치(weight)인 정책 파라미터 및 가치 모델 파라미터의 업데이트를 경사 하강법 에 따라 수행하는 단계를 더 포함할 수 있다. 예시적인 실시예에 있어서, 위에 기재된 전력망 운영 방법을 수행하기 위하여 컴퓨터 판독 가능한 기록 매체에 저장된 컴퓨터 실행가능 프로그램이 제공될 수 있다. 예시적인 실시예에 있어서, 위에 기재된 전력망 운영 방법을 수행하기 위한 컴퓨터 프로그램이 기록된 컴퓨터 판독 가능한 기록 매체가 제공될 수 있다."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예에 따른 전력망 자동 운영 방법은 계층적 의사결정 모델과 사후 상태 표현의 이점을 효과적으 로 결합하여 높은 복잡도를 갖는 전력망 최적화 문제도상당히 빠르게 효율적으로 학습할 수 있게 하고, 전력망 자동 운영에 있어서 전력망 그리드의 크기에 상관없이 종래의 다른 모든 비교 알고리즘들의 성능을 능가하는 최 고 수준의 운영 효율성과 안전성을 보여준다. 본 발명을 이용하면 전문가의 도움 없이 전력망을 자동으로 운영할 수 있는 지능형 에이전트를 개발할 수 있다."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부한 도면들을 참조하여, 본 발명의 바람직한 실시예를 보다 상세하게 설명하고자 한다. 도면상의 동일 한 구성요소에 대해서는 동일한 참조부호를 사용하고 동일한 구성요소에 대해서 중복된 설명은 생략한다. 도 1은 본 발명의 일 실시예에 따른 강화학습 시스템의 기본 개념을 도식적으로 나타낸다. 도 1을 참조하면, 강화학습 시스템은 에이전트가 환경(Environment)과 상호작용을 하며 주어진 일 을 수행할 수 있는 행동(Action)을 찾는 학습론인 강화학습을 위한 시스템이다. 강화학습 시스템에 있어서, 에이전트는 매 순간 환경으로부터 상태(state) St를 받아 인식하고 행동(action) At을 취하게 된다. 이 때 그 행동 At을 취한 결과로 보상(Reward) Rt을 받게 된다. 에이전트는 주어진 시간동안 얻게 되는 보상들 의 합을 최대화 할 수 있는 행동을 찾는 것을 목표로 한다. 먼저, 전력망 운영 자동제어 플랫폼(Grid2Op) 환경을 설명한다. 전력망은 기본적으로 변전소에 해당하는 노드로 구성된 그래프이며, 변전소에는 부하, 발전기 및 전력선이 연결되어 있다. 발전기는 전기를 생산하고, 부하는 전기를 소비하며, 전력선은 변전소 사이에서 전력을 전송한다. 변전소는 네트워크에서 라우터로 간주되어 전력 을 어디로 전송할지 결정한다. 전력망 운영 프로그램으로 구현되는 전력망 운영 자동제어 플랫폼(Grid2Op)에서 는 각 변전소에 2개의 도체가 있는 이중 버스바 시스템을 따른다. 변전소에 연결된 요소, 즉 부하, 발전기 및 전력선을 두 개의 버스바 중 하나에 할당할 수 있고 전력은 동일한 버스바(bus bar)에 연결되어 있는 요소들끼 리 전달된다. 따라서 각 변전소는 두 개의 노드로 분할된 것으로 간주할 수도 있다. 전력망 운영 자동제어 플랫폼(Grid2Op)의 에이전트는 전력망을 운영하기 위하여 변전소 및 전력선에 행동을 취 할 수 있다. 버스 할당이라고 하는 변전소에 대한 행동은 변전소의 요소(전력선, 발전기, 부하)를 버스바에 할 당한다. 회선 스위치라고 하는 회선에 대한 행동은 전력선의 연결을 끊거나(전력선의 양쪽 끝이 버스에 할당되 지 않음) 연결이 끊긴 전력선을 다시 연결한다. 에이전트는 주어진 단위 시간 당 하나의 회선 스위치 또는 하나 의 버스 할당 작업을 수행할 수 있으며, 동일한 회선이나 변전소에 연속적으로 행동을 취할 수 없다. 전력망 운영의 실패는 (i) 부하에 필요한 전력량이 전달되지 않거나 (이는 연결이 끊긴 전력선이 너무 많은 경 우 발생할 수 있음) (ii) 행동을 취했을 때 연결이 끊긴 하위 그래프가 형성되었을 경우이다. 이러한 전력망 운 영 실패는 에이전트의 성능을 측정할 때 반영될 수 있다. 또 다른 중요한 성능 지표는 저항 손실로 인해 전송중에 사라진 전력의 양으로 측정되는 전력 손실 패널티이다. 따라서 에이전트의 목표는 운영 실패 패널티와 전 력 손실 패널티를 최소화하여 전력망을 안전하고 효율적으로 운영하는 것이다. 예시적인 일 실시예에서는, 전력망 관리의 문제를 효과적으로 해결하는 강화학습 알고리즘인 준마코프 사후상태 액터-크리틱(Semi-Markov Afterstate Actor-Critic: SMAAC) 알고리즘을 제공한다. 강화학습을 활용한 실제 규 모의 전력망 관리의 주요 어려움 중 하나는 대규모 상태 및 행동 공간에 있다. 이를 다루기 위해 사후 상태 (after state) 표현과 목표기반 계층적 정책(goal-conditioned hierarchical policy)을 활용할 수 있다. 전력망 운영 자동제어 플랫폼(Grid2Op)은 전력망 운영에 강화학습을 사용하기 위한 자연스러운 프레임워크를 제 공한다: 강화학습 문제를 표현하기 위해 (S, A, p, r, γ)로 정의된 마르코프 결정 과정(Markov Decision Process: MDP)을 가정한다. 여기서 S는 상태 공간, A는 행동 공간, p(st+1|st, at)는 (알 수 없는) 상태 전이 확 률, rt = r(st, at) ∈ R은 즉각적인 보상이고, γ ∈ (0, 1)는 감가율이다. 상태를 조건으로 하는 행동에 대한 확률분포인 확률적 정책 π(at|st)을 학습한다고 가정한다. π 에 대한 상태 및 행동 가치 함수는 각각 및 이다. 도 2는 전력망에서 사후 상태 표현의 일예로서, 버스 할당 작업을 예로 하여 행동이 전력망 상태에 어떻게 영향 을 미치는지 보여준다. 예시된 전력망은 4개의 변전소, 2개의 발전기(gen1, gen2), 2개의 부하(load1, load2), 3 개의 변전소(원으로 표시됨), 그리고 5개의 전력선이 있는 전력망이다. 맨 왼쪽 도면에서부터 버스바 할당 행동 at는 전력망을 재구성하고, 다음 상태인 st+1은 부하의 전력 수요 변화와 같은 외부 요소 et+1에 의해 결정된다. 대각선 방향의 전력선은 오버플로를 겪고 있었으나, 버스바 할당 행동 at에 의해 오버플로가 해소된다. 전력 손 실 또한 15 에서 13으로 줄어들게 된다. 도 2에서 볼 수 있듯이, 전력망 운영 자동제어 플랫폼(Grid2Op)의 전이는 두 가지 단계로 구성될 수 있다. 즉, 현재 상태 st에서 행동 at에 의해 직접적으로 토폴로지 변화가 일어난 후, 외인적인 사건 et+1에 의해 나머지 상 태 변화가 일어나 st+1이 된다. 이것은 사후 상태를 사용하기에 좋은 동기가 된다. 도 2의 중간 도면인 사후 상 태 는 근사 동적 프로그래밍(ADP)에서 행동 이후 상태라고 불리기도 하며, 에이전트가 결정 at을 내린 직후 인 동시에 새로운 정보 et+1가 도착하기 직전의 가상의 상태를 말한다. 상태와 행동을 효율적으로 표현하기 위해 사용되는 사후 상태(after state) 개념을 활용하기 위하여 상태 S를 (T, X)로 정의할 수 있다. 여기서, T는 행동에 의해 결정적으로 변경되는 부분이고, X는 행동과 무관하거나 행 동에 간접적인 영향을 받는 부분이다. 전이는 fA와 fE 두 부분으로 분해될 수 있다."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 st+1의 결정론적인 부분인 τt+1은 함수 fA(st, at)에 의해 주어지고, 확률론적인 부분인 xt+1은 함수 fE(sta, et+1)에 의해 주어진다. 여기서 et+1은 어떠한 알 수 없는 분포 pE에서 추출된 전이 속 무작위성의 원천이 다. et+1 자체는 xt+1의 일부로 포함될 수 있다는 점에 주의해야 한다. 사후 상태를 사용하면 많은 이점이 있다. 예를 들면 상태와 행동 공간이 매우 크지만 고유한 사후 상태의 집합 이 상대적으로 작은 경우, 사후 상태의 가치 함수를 배우는 것이 훨씬 더 효율적이다. 정책 π에 대한 사후 상 태 sa의 가치 함수는 이고 재귀 형식은 다음과 같이 표현될 수 있다."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "식 에 의한 정책 평가와 정책 개선을 교대로 반복하여 최적의 사후 상태 가치 함수와 최적 정책을 얻을 수 있다."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "개별 전력망 조작을 행동으로 사용할 때 토폴로지에 고유한 변경이 발생하기 때문에 사후 상태 표현에서 많은 것을 얻을 수 없다. 그러나 일련의 전력망 조작들을 행동으로 고려하고 그들의 순열이 최종 토폴로지에 동일한 변화를 초래할 때 사후 상태가 매우 효과적임을 알 수 있다. 위에서 설명한 것처럼 상태-행동 쌍은 사후 상태로 표현될 수 있다. 사후 상태란 에이전트가 결정을 내린 후 환 경이 응답하기 전의 상태를 말한다. 이를 이용하면, 대규모 상태-공간을 효율적으로 처리할 수 있다. 여러 상태 -행동 쌍이 동일한 사후 상태로 이어지는 경우, 사후 상태 표현은 상태-행동을 훨씬 더 간결하게 표현할 수 있 다. 예를 들어, 전력망의 토폴로지를 제어하는 경우, 토폴로지 제어에 의해 토폴로지가 결정적으로 변화하기에 현재 토폴로지(상태)와 토폴로지 제어(행동)는 재구성된 토폴로지, 즉 사후 상태로 표현될 수 있다. 이 방법론 을 계층적 프레임워크로 확장할 수 있다. 상위레벨 정책(high-level policy)은 현재 상황에서 전력망의 목표 토 폴로지를 생성하고, 하위레벨 정책(low-level policy)은 토폴로지 제어 행동을 통해 전력망 토폴로지를 직접적 으로 재구성한다. 사후 상태와 계층적 프레임워크가 함께 결합하면 학습 중에 우수한 토폴로지를 효과적으로 탐 색할 수 있다. 예시적인 실시예에 따른 전력망의 관리/운영을 위한 자동 제어 방법(SMAAC)은 상태 공간, 행동 공간, 보상 함수 를 각각 다음과 같이 모델링할 수 있다. (i) 상태: Grid2Op 환경에서 상태 S를 (T, X)로 정의한다. 여기서 T는 (행동에 의해 결정적으로 변경되는) 토폴 로지 구성의 집합이고, X는 (행동과 무관한) 전력 수요 및 공급, (행동에 간접적인 영향을 받는) 각 전력선에서 전송되는 전력 등의 다양한 정보를 뜻한다. (T, X 모두 그래프로 표현되어 있음). (ii) 행동(action): 본 실시예에서는 에이전트에서 버스바 할당 작업만 고려하도록 한다. 전력 공급을 위한 경 로가 많을 때 오버플로 가 발생할 가능성이 적기 때문에, 가능한 한 많은 전력선을 연결하는 것이 바람직하다고 가정한다. 따라서 회선 스위치 작업의 경우 오버플로로 인해 전력선이 끊어질 때마다 다시 연결하는 규칙을 따 를 수 있다. 변전소의 번호를 Nsub로 정의하고 i번째 변전소의 요소를 Sub(i)로 정의한다. 변전소의 전력선, 발 전기 및 부하의 각 끝은 두 개의 버스바 중 하나에 할당할 수 있으므로 총 행동의 수는 이다. (즉, 각 행동은 변전소 중 하나를 선택하고 그 안에서 버스 할당을 수행하는 것이다). 에이전트가 위험한 상황 에 서만 행동(개입)하도록 할 수 있다. 위험 조건은 전력 흐름이 임계값 하이퍼-파라미터보다 큰 라인의 존재에 의해 결정된다. 이것은 자연스럽게 강화 학습에 대한 semi-MDP 설정을 불러온다. (iii) 보상: 중간 시간 단계의 보상은 전력망의 효율성으로 정의한다. 여기서 효율성은 총생산에 대한 총 부하 의 비율인 loadt/prodt로 정의된다. 이 비율이 1보다 크면 생산이 수요를 충족시키지 못하기 때문에 에피소드 실 패에 대한 큰 패널티와 함께 에피소드가 종료된다. 본 발명의 예시적인 실시예에 있어서, 사후 상태를 사용한 액터-크리틱 알고리즘을 이용할 수 있다. Grid2Op 환 경의 주요 문제는 대규모 상태 및 행동 공간을 가진다는 점이다. 본 실시예에서는 이 문제를 정책과 가치 함수 가 함수 근사기로 표현되는 액터-크리틱 구조를 적용하여 해결한다. 또한 도 2에 표시된 전이 구조를 활용하여 동일한 사후 상태로 이어지는 많은 상태-행동 쌍을 포착하기 위해 사후 상태 표현을 사용한다. 표기의 단순성을 위해 이하의 모든 수식 전개는 MDP를 가정하고, 이하에서 semi-MDP 설정으로 확장할 수 있다. ψ 및 θ에 의해 각각 매개변수화 된 사후상태 가치함수 및 정책 πθ(at|st)에 대해 함수 근사기를 사 용한다. 액터는 Jπ를 최대화하고 크리틱은 LV를 최소화하도록 훈련된다."}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서 리플레이 버퍼 D는 오프폴리시 학습을 위한 전이 튜플 을 저장한다. 액터와 크 리틱은 소프트 액터-크리틱 (SAC; Soft Actor-Critic) 알고리즘을 사용하여 학습될 수 있다. 상태-행동 쌍이 아 니라 더 간결한 표현인 재구성된 토폴로지에 의한 사후 상태에 대해 가치 함수를 학습한다는 점에 유의해야 한 다. 비록 위의 수식이 상태 가치 크리틱을 정의하고 있지만, 이는 본질적으로 행동 가치 크리틱(사후 상태는 상 태와 행동으로 정의됨)이기 때문에 여전히 오프-폴리시(off-policy) 학습을 할 수 있다. 또한, 우도비 기울기 추정기(likelihood ratio gradient estimator)보다 분산이 낮아서 안정적인 학습이 가능 하기 때문에, 재매개변수화 트릭을 통해 기울기 추정기를 적용하는 것을 목표로 한다. 재매개변수화 트릭을 통해 액터를 업데이트하려면 전이 함수 fA를 미분 가능해야 하지만, 버스 할당 행동에서 토폴로지 구성으로 매핑하 는 fA를 미분 가능한 식으로 정의하는 것은 간단하지 않다. 하지만 작업 공간을 재정의하여 목표기반 계층적 프레임워크로의 확장을 통해 이 문제를 완화할 수 있다. 구체 적으로 설명하면, Grid2Op 환경에서 탐험적 행동을 취하는 것은 매우 어려운 일이다: 에이전트가 무작위 행동을 취하면 전력망은 짧은 시간 단계 이내에 실패한다. 예를 들어, 무작위 정책을 가진 에이전트는 대부분 10 단계 미만 에서 실패한다. 반면, no-op 정책(시간 단계 전반에 걸쳐 초기 토폴로지를 유지)을 사용하는 에이전트는 평균 약 500 단계 동안 생존한다. 그러므로 에이전트가 기존의 토폴로지와 크게 다른 다양한 토폴로지 구성을 탐험하는 것은 매우 어렵고, 무작위 탐험 정책(예: ■-greedy)은 종종 한두 가지 행동만 실행하는 잘못된 로컬 최적점(local optima)에 빠지기 쉽다. 따라서 더욱 구조화된 탐험이 성공적인 훈련의 열쇠다. 이를 위해, 예시적인 실시예에서는 목표 토폴로지 구성을 상위레벨 행동(high-level action)으로 정의하는 사후 상태 액터-크리틱 알고리즘을 2단계 계층적 의사 결정 모델로 확장할 수 있다. 특히, 상위레벨 행동을 일 때 목표 토폴로지 구성 g ∈ {0, 1}n로 정의하고, 이는 상위레벨의 정책 πh에 의해 학습된 다. 이것은 로 주어지는 시간적으로 확장된 사후 상태 표현으로 유도된 다. 여기서, t는 위험이 발생한 시간을 나타내고, d는 다음 위험이 발생할 때까지의 시간 간격을 나타낸다. 동 일한 토폴로지로 이어지는 매우 다양한 기본 행동 (개별 버스 할당 행동) 시퀀스들의 동일성이 목표 토폴로지 구성에 의해 포착되기 때문에, 이제 사후 상태 표현의 효과를 최대한 발휘할 수 있다. 또한, 전력망의 목표 토폴로지를 사용한 탐험은 기본 행동을 사용한 것보다 효과적이다. 정책은 어디로 가는지, 즉 현재 상황에서 바람직한 토폴로지에만 초점을 맞추면 되므로 어떻게 가는지에 대해 신경 쓸 필요가 없다. 즉, 적절한 하위레벨 정책의 도움으로 목표 토폴로지로 귀결되는 적절한 기본 행동 시퀀스를 알아낼 수 있다. 마지막으로 fA의 결과는 gt 행동과 같기 때문에 이제 액터 업데이트에 대한 재매개변수화 트릭을 간단하게 사용 할 수 있다. 리플레이 버퍼 D는 전이 튜플 를 저장한다. 여기서 이다. 상위레 벨의 정책은 다음의 액터 레이어와 크리틱 레이어의 목적 함수를 통해 학습할 수 있다:"}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "하위레벨 정책의 경우 전력망의 현재 토폴로지를 목표 토폴로지로 변경하는 작업 순서를 찾는 것은 비교적 간단 하다. 버스 할당 변경이 필요한 변전소 집합을 식별하고 거기에서 적절한 재할당만 하면 된다. 일 실시예에서, 하위레벨의 정책에 대해 규칙 기반 방식 을 취한다. 여기서 rule은 버스 할당 행동을 실행할 변전소의 순서를 결정한다. 예를 들어 용량이 가장 적은 변전소가 가장 개입이 시급하기 때문에 용량에 따라 버 스 재할당을 먼저 하도록 변전소에 우선순위를 부여할 수 있다. 도 3은 본 발명의 예시적인 실시예에 따른 강화학습 에이전트를 이루고 있는 심층 신경망의 구조를 예시한다. 도 3을 참조하면, 심층 신경망 구조는 전력망 상태 정보 입력부, 그래프 모델링부, 공유 레이어 (shared layer), 액터 레이어(actor layer), 그리고 크리틱 레이어(critic layer)를 포함할 수 있 다. 심층 신경망 구조에서는 총 4단계의 처리가 수행될 수 있다. 첫 번째 단계의 처리는 전력망 상태정보 입력 부에서 전력망 상태 정보를 입력받는다. n개의 변전소가 있는 전력망에서 벡터로 주어지는 전력망의 현재 상태 st= [τt, xt]의 정보가 입력으로 주어질 수 있다. 두 번째 단계의 처리는 그래프 모델링부에서 수행되며, 입력 받은 전력망 상태 정보를 그래프 모델링부(6 0)는 발전소, 전력 부하, 전력선을 노드로 삼는 그래프 구조 (x, m)로 변환하여 신경망의 입력으로 사용하기 위 한 전처리를 한다. 전력망의 상호연결 구조를 활용하기 위해 그래프 신경망(Graph Neural Network)을 적용할 수 있다. 그래프 신경망은 그래프 데이터 구조로 표현된 데이터를 처리하기 위한 신경망이다. 구체적으로, 그래프모델링부는 입력으로 제공되는 전력망의 현재 상태 st= [τt, xt]에서 xt를 로 재구성(reshape)할 수 있다. 여기서, 은 인접행렬이고, 인접행렬 M은 노드들의 어텐션 가중치를 마스킹하는 데 사용될 수 있다. 은 k 길이의 특징(features)을 가지는 노드들의 행렬이다. 재구성된 는 공유 레이어에 제공될 수 있다. 세 번째 단계의 처리는 공유 레이어에서 수행되며, 전 처리된 그래프 데이터가 입력으로 주어졌을 때, 각 노드를 표현하는 벡터들을 인접한 노드들에 대한 어텐션(attention) 메커니즘을 통해 가공한다. 어텐션 메커니 즘은 출력을 예측할 때 인코더에서의 어떤 값에 집중해야 하는지를 찾는 방법으로, 인코더의 모든 시점에서의 출력들과 현재의 상태를 비교하여 가장 유사한 값에 대해 가중치를 부여한다. 공유 레이어는 xt를 엔코딩할 수 있다. 공유 레이어의 일 실시예에서 그래프 신경망(GNN) 블록으로 트랜스포머(transformer)를 채용할 수 있다. 네 번째 단계의 처리는 정책 네트워크에 해당하는 액터 레이어와 가치 네트워크에 해당하는 크리틱 레이어 에서 수행된다. 액터 레이어는 현재 상태 st = [τt, xt]를 입력받아 목표 토폴로지 gt를 출력할 수 있 다. 즉, 액터 레이어는 공유 레이어에서 가공된 결과 xt를 입력으로 받음과 동시에 현재 토폴로지 τt 정보를 추가로 더 제공받아 새로운 목표 토폴로지 gt를 생성하여 출력할 수 있다. 크리틱 레이어는 사후상 태 를 입력받아 사후상태의 값을 출력할 수 있다. 즉, 크리틱 레이어는 공유 레이어에서 가공된 결과 xt를 입력으로 받음과 동시에 목표 토폴로지 gt를 추가로 더 제공받아 새로운 가치 함수 를 생성하여 출력할 수 있다. 액터 레이어와 크리틱 레이어는 하위 레이어들을 공유하며, 그 하위 레이어는 그래프 신경망 블록들과 선형 레이어들로 구성될 수 있다. 추가적으로, SAC 방식을 따라서 액터 레이어와 크리틱 레이어의 목표 함수(objective function)에 정책의 엔트로피를 부가할 수 있다. 도 4는 본 발명의 예시적인 실시예에 따른 전력망의 관리/운영을 위한 자동 제어 방법(SMAAC)의 실행 절차를 나 타내는 흐름도이다. 도 4의 흐름도는 전력망을 운영할 때와 학습할 때를 함께 표현한 것이다. 전력망의 관리/운영을 위한 자동 제어 방법은 전력망 운영 문제를 위해 효율적인 심층 강화학습 알고리즘을 통해 토폴로지 제어 방법으로 운영할 수 있는 자동 운영 에이전트(agent) 형태로 구현될 수 있다. 토폴로지 제어 방법은 각 발전소, 전력 부하, 전력선 들을 변전소에 있는 2개의 버스바(bus bar) 중 하나로 할당하여 연결 상태를 설정하는 것을 말한다. 이 자동 운 영 에이전트는 컴퓨터에서 실행될 수 있는 프로그램(이하, '전력망 운영 프로그램'이라 함)으로 구현될 수 있다. 이 전력망 운영 프로그램은 프로세서와 이의 처리공간을 제공해주는 메모리와 같은 하드웨어를 이용하여 실행될 수 있다. 전력망 운영 프로그램은 프로세서로 하여금 다음과 같은 처리를 수행하게 할 수 있다. 전력망 운영 프로그램은 크게 현재 시간 단계에서의 전력망 토폴로지 구성 정보와 전력망 구성요소들의 전력량 정보를 포함하는 전력망의 상태 정보를 바탕으로 그래프 구조 데이터로 표현되는 이상적인 전력망 연결 상태인 전력망의 목표 토폴로지를 생성하는 단계(S10-S22 단계)와, 생성된 상기 전력망의 목표 토폴로지에 기반하여 전 력선의 연결과 변전소의 버스바 할당이 이루어지도록 제어하는 단계(S24-S30 단계)를 포함할 수 있다. 도 4를 참조하여 좀 더 상세히 설명하면, 매 시간 단계마다 전력망의 상태 정보가 전력망 운영 프로그램의 입력 으로 주어질 수 있다(S10 단계). 전력망 상태 정보 입력부는 환경으로부터 매 시간 단계마다 전력망의 현재 상태의 정보를 받을 수 있다. 전력망의 현재 상태 정보 st= [τt, xt]는 토폴로지 구성 정보와 전력량 정보로 표현될 수 있다. 토폴로지 구성 정보는 전력망을 구성하는 각 전력선의 연결성 정보 및 전력망에 존재하는 각 변전소의 버스 할당 정보를 포함할 수 있다. 전력량 정보는 전력망에 존재하는 각 발전기가 생산하여 제공하는 전력량, 전력망에 존재하는 각 부하에 필요한 전력량, 전력망의 전선을 통해 전송되는 전력량 정보를 포함할 수 있다. 전력량 정보는 전력망에 존재하는 각 전선에 과부하가 일어난 기간에 관한 정보를 포함할 수도 있다. 발 전기에서 공급하는 전력량과 부하에서 요구하는 전력량은 시간이 지남에 따라 변할 수 있으며, 전력선에 전송되 는 전력량도 현재 토폴로지 구성에 따라 수급과 함께 변할 수 있다. 또한, 각 전선은 자체적으로 전력을 전송할 수 있는 용량이 있으며, 전력이 넘칠 경우 자동으로 차단될 수 있다. 상태 정보에 포함되는 각 상태는 숫자로표현될 수 있으며, 상태는 이러한 숫자들이 단순히 나열되어 있는 형태 즉, 벡터로 표현될 수 있다. 이러한 전력망의 상태 정보가 입력되면 그 정보를 그래프 구조로 표현될 수 있는 그래프 구조 데이터로 변환할 수 있다(S12 단계). 그래프 모델링부는 벡터로 표현된 전력망의 현재 상태 정보 st= [τt, xt]를 그래프의 형태로 변환할 수 있다. 즉, 입력된 전력망의 상태 정보를 각 변전소에 연결된 발전소, 전력 부하, 전력선을 노 드로 삼는 그래프 구조의 데이터로 변환할 수 있다. 각 노드의 연결 관계를 반영하여 메시지 패싱(message passing) 메커니즘을 기반으로 하는 그래프 어텐션 네트워크 (graph attention network)를 통해 각 노드에 해당 하는 노드 표현 (node representation)을 출력할 수 있다. 이 노드 표현들의 집합을 전력망 환경을 표현하는 상 태로 간주하고, 상태가 주어졌을 때 최적의 행동을 예측하는 정책 네트워크 (policy network)와 상태와 행동이 주어졌을 때 이에 대한 가치를 예측하는 가치 네트워크 (value network)를 학습하고 전력망 환경에 적용할 수 있다. 그런 다음, 그래프 구조 데이터로 변환된 전력망의 상태를 이용하여 전력망의 안전성을 위해 모든 전력선이 연 결됐는지 확인한다(S14 단계). 그래프 구조 표현에서, 각 전력선의 연결과 끊어짐은 1과 0으로 표현될 수 있다. 끊어진 전력선 즉, 0으로 표현된 전력선이 있다면 시스템 상에서 해당 전선이 연결되어 있지 않다고 알려줄 수 있다. 만약 끊어진 전력선이 존재한다면 전력망의 그래프 구조 데이터에서 그 끊어진 전력선을 연결할 수 있다(S16 단 계). 그런 다음, 소정의 시간(예컨대, 약 5분) 동안 대기하면서 다음 행동을 결정하도록 한다(S18 단계). 소정 시간 을 대기하는 이유는 실제 환경 상에서 물리적인 제약으로 인해 짧은 시간 동안 여러 가지의 행동을 동시에 할 수 없기 때문이다. 그리고 단계 14에서 모든 전력선이 연결된 경우, 그 모든 전력선들 중에서 흐르는 전력량이 소정의 문턱값을 초 과하는 즉, 전력량이 임계점에 가까운 전력선이 존재하는지를 체크한다(S20 단계). 현재 전력망이 적절한 행동 을 필요로 하는 위험한 상태인지를 확인하는 단계이다. 전력선이 위험한 상태인지 여부는 전력선에 흐르는 전력 의 양이 사전에 정의된 문턱값을 넘었는지 즉, 임계점에 가까운 상태인지 여부를 기준으로 판별할 수 있다. 만약 임계점 근처의 전력선이 존재하면, 전력선의 연쇄적 과부화 및 비활성화를 방지하기 위해, 에이전트의 상 위 레벨 정책 모델이 이상적인 전력망 연결 상태인 목표 토폴로지를 결정한다(S22 단계). 그래프 신경망으로 구 성된 액터 레이어의 상위레벨 정책(high-level policy)은 그래프로 표현된 현재의 전력망 상태를 입력으로 받아 행동을 결정할 수 있다. 결정된 행동은 하위레벨 정책(low-level policy)에게 목표(goal) G가 된다. 즉 상 위레벨 정책은 현재의 전력망의 상태를 보고 이상적인 전력망의 연결 상태를 생성하고, 하위레벨 정책은 이상적 인 전력망의 연결 상태 즉, 전력망의 목표 토폴로지를 달성하기 위해 하위레벨 행동(low-level action)을 수행 한다. 여기서, 이상적인 전력망의 연결 상태란 발전소로부터 생산된 전력이 각 부하에 효율적으로 배분되고 있 고(전력은 전력선을 타고가면서 저항 열로 인해 손실되는데, 전력 전송 손실을 최소화하면서 발전소로부터 부하 에 보내는 것을 효율적이라고 볼 수 있음), 각 전력선에 과부하가 일어나지 않는 상태를 말한다. 그러므로 전력 망의 현재 상태(각 전력선에 흐르는 전력의 양, 각 발전소에서 만드는 전력의 양, 각 부하에서 필요로 하는 전 력의 양)에 따라서 이상적인 전력망의 연결 상태는 달라질 수 있다. 하위레벨 행동은 환경 즉, 전력망에 실제로 반영이 되는 행동들을 의미한다. 예를 들어 전력선의 연결을 연결하거나 끊는 회선 스위치 행동 또는 변전소의 연결 상태를 변화시키는 버스할당 행동 등이 하위레벨 행동에 해당된다. 만약 임계점 근처의 전력선이 존재하지 않거나, S22 단계를 통해 목표 토폴로지가 정해지면, 설정된 목표와 실 행 규칙에 따라 하위레벨 행동을 수행한다(S24 단계). 즉, 하위 레벨 정책이 목표 토폴로지에 도달할 때까지 여 러 시간 단계에 걸쳐서 실행 규칙에 따라 하위레벨 행동을 결정하고 실행한다. 달리 말하면, 현재의 상태와 상 위레벨 정책이 만든 목표를 비교하여, 하위레벨 정책이 목표를 달성했는지를 확인하며, 달성하지 않았다면 하위 레벨 정책은 하위레벨 행동을 달성할 때까지 계속해서 수행한다. 여기서, 실행 규칙은 하위레벨 행동이 수행되 는 순서를 의미한다. 상위레벨 정책이 목표 토폴로지(이상적인 전력망의 연결 상태)를 만든 후, 하위레벨 정책 은 현재 전력망의 연결 상태와 목표 토폴로지를 비교하여 그 목표 토폴로지에 도달하기 위해 필요로 하는 하위 레벨의 행동들을 어떠한 순서로 실제 환경에 적용할지를 결정해야 한다. 이는 위에도 언급하였듯이 물리적 제약 으로 인해 소정 시간(예: 약 5분)에 1개의 하위레벨 행동만을 수행할 수 있기 때문이다. 즉 실행 규칙의 예들 중 하나로는 변전소에 연결되어 있는 요소(전력선, 발전소, 부하)가 많을수록 우선순위에 두기 및 직접 순서 또 한 학습하기 등을 들 수 있다.단계 S26에서 하위레벨 행동의 수행을 통해 목표 토폴로지에 도달한 것으로 판단되면, 전이 데이터를 저장한다 (S28 단계). 전이 데이터는 학습에 필요한 데이터이다. 강화학습 에이전트를 학습시키기 위해서는 (현재 상태, 행동, 다음 상태, 보상)으로 구성되는 학습 데이터가 필요하다. 에이전트가 환경에서 행동을 하면 보상이 나오 고 다음 상태로 넘어가게 되는 바, 에이전트는 환경과 상호작용(전력망 운영)을 하며 전이 데이터를 얻을 수 있 다. 전이 데이터 저장 후, 종료조건이 만족되었는지 판별한다(S30 단계). 종료조건은 전력망 운영을 성공하여 주어 진 시간이 모두 경과한 경우와, 전력망 운영을 실패하여 전력망에 고장이 발생한 경우가 해당될 수 있다. 이러 한 종료조건이 만족된 것으로 확인되면, 종료한다. 종료 조건이 만족되지 않은 것으로 판별되는 경우, 에이전트(모델)가 학습 중인지를 판별한다(S32 단계). 에이 전트(모델)가 학습 중인지 여부의 판단 결과에 따라서 학습을 하거나 학습을 하지 않고 전력망의 운영만 할 수 있다. 강화학습은 주어진 태스크를 직접 수행을 해보며 실패와 성공을 반복하며 스스로 학습을 하는 학습론이므 로, 직접 전력망을 운영해가면서 학습을 진행할 수 있다. 전력망 최적화 문제의 높은 복잡도를 고려하여 효율적인 학습을 위해, 사후 상태(after-state)와 계층적 의사 결정 모델을 활용할 수 있다. 사후 상태는, 에이전트가 행동을 결정한 직후 환경으로부터의 피드백이 오기 전의 상태를 의미하며, 전력망 환경에서는 에이전트가 결정한 토폴로지 설정과 나머지 전력망 정보(노드들의 전력량, 사용률 등)로 이루어질 수 있다. 단일 사후 상태는 여러 개의 상태-행동 쌍들을 포착할 수 있기 때문에 에이전 트의 탐색 공간을 크게 줄여 효율적인 학습을 가능하게 한다. 그리고 정책 모델을 2단계의 상위-하위 계층으로 나누어, 상위 정책 모델은 장기간의 전체 전력망의 목표 토폴 로지를 결정하고, 하위 정책 모델은 실행 규칙에 따라 현재 전력망 토폴로지가 목표 토폴로지에 도달할 수 있는 행동 시퀀스를 결정하도록 한다. 위와 같은 구조를 바탕으로 주어진 상태에서 에이전트가 어떤 행동을 결정했을 때 이에 대한 보상을 전력 효율 성(총 전력 소비량 / 총 전력 생산량)으로 정의하여 전력망 운영 환경 전이 데이터를 모으고, 이를 통해 정책 네트워크는 정책 경사(policy gradient), 가치 네트워크는 벨만 오차 (bellman error)를 손실 함수로 삼아 경사 하강법(gradient descent)으로 학습시킨다. 위처럼 학습된 전력망 자동 운영 에이전트는 학습 후에 동일한 방식으로 그래프 전 처리된 입력이 주어졌을 때, 학습된 정책 네트워크의 출력에 의해 적절한 토폴로지 제어를 결정하여 전력망을 운영한다. 현재 정책을 학습시키는 중이라면 신경망으로 된 액터 레이어와 크리틱 레이어의 모델 파라미터를 업데 이트할 수 있다(S34 단계). 이때 크리틱 레이어는 사후 상태인 (t, g)를 입력으로 받아 사후 상태가 얼마만 큼 가치가 있는지(숫자로 표현됨)를 학습하며, 액터 레이어는 현재 상태인 (t, x)를 입력으로 받아 현재 상 태에서는 어떠한 목표를 만들어야 하는지를 학습한다. 현재 강화 학습 상태인지 아니면 전력망의 운영만을 수행 하는 상태 인지는 프로그램 사용자가 지시할 수 있다. 필요에 따라 단순히 지시 코드(예컨대, 0=학습지시 또는 1=운영 지시)를 입력하면 학습을 해라 또는 운영만을 해라를 정할 수 있다. 에이전트가 학습 중인 경우엔 추가적으로 전이 데이터를 모으는 단계와 경사 하강법에 따른 정책 파라미터 및 가치 모델 파라미터의 업데이트를 수행한다(S34 단계). 가치 모델 파라미터의 업데이트와 관련하여, 액터 레이 어와 크리틱 레이어는 인공 신경망(neural network)으로 구성되며, 파라미터는 인공신경망의 가중치 (weight)를 의미한다. 업데이트는 이러한 가중치 값을 변화시키는 것으로, 가중치 값은 식 과 에 의해서 업데이트될 수 있다. 도 5는 본 발명의 예시적인 실시예에 따라 전력망의 그래프 구조 표현에서 버스바 할당을 변경하는 예를 도시한 다. 도 5를 참조하면, 그림 (A)는 전력선 4-5(발전소 4와 5사이)를 통해 현재 임계점보다 더 많은 전력이 운반되고 있는 위험한 상황을 나타낸다. 도 5의 (B)는 이런 위험 상황을 해소하기 위해 전력망 운영 시스템이 발전소 12 의 버스 할당을 바꿔 전력선 12-13을 노란색 버스바에 연결되게 한다. 이로 인해 전력선 4-5에서 운반되던 일부 의 전력이 전력선 4-3으로 운반되면서 임계점 보다 낮은 전력(예: 0.89)이 전력선 4-5에 운반된다. 도 5의 (C) 는 변전소3의 버스 할당을 바꾸면서 변전소 4로부터 흐르는 전력은 더욱 분산되면서 전체적으로 균형 잡힌 전력 분배가 된다. 이상에서 설명한 실시예에 따른 전력망 자동 운영방법의 성능을 평가하기 위한 실험을 Grid2Op에서 제공하는 IEEE-5(최소), IEEE-14 및 L2RPN WCCI 2020(최대, 챌린지에서 사용됨)의 3가지 전력 그리드에서 수행하였다. 각 그리드의 세부 정보는 표 1에 정리되어 있다. 표 1에서 Nsub, Nline, Ngen, Nload는 변전소, 전력선, 발전기, 부 하의 숫자를 각각 나타낸다. |S|는 상태의 차원을 나타내며, |A|는 단일 버스 할당 작업의 수를, n은 토폴로지 배치의 차원을 나타낸다. 각 전력 그리드에는 일련의 시나리오가 있으며 각 시나리오는 각 시간 단계에서 전력 공급 및 수요와 같은 시뮬레이션의 변화를 지정한다. 각 시나리오의 길이는 864개의 시간 단계로, 각 단계는 5 분에 해당하며 총 3일에 해당한다. 표 1"}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "Grid2Op는 비교적 새롭게 연구되고 있는 분야이므로 그리드 토폴로지 제어에 적용된 강화학습 방법은 많지 않다. 따라서 방법의 효율성을 확인하기 위해 성능 비교를 위해 다음 3가지 기준 알고리즘(baseline)을 구현하 였다. DDQN(Dueling DQN): 챌린지의 마지막 승자와 유사한 아키텍처를 가지며, 원래 행동 (primitive action) 공간의 행동 가치 함수를 학습한다. SAC: DDQN과 유사하지만 SAC 알고리즘에 따라 최대 엔트로피 탐색을 활용한다. SMAAC/AS: 사후상태 표현이 없는 SMAAC이며, 행동 -가치 크리틱을 사용한다. 따라서 DDQN 및 SAC는 원래의 행동이 있는 MDP 설정을 가정하고 SMAAC/AS는 목표조건형 세미 MDP (goal- conditioned MDP) 설정을 가정하지만 사후 상태 표현은 사용하지 않는다. 추가적으로 이 기술의 접근 방식을 L2RPN WCCI 2020 그리드의 3위 참가자와 비교하였다. YZM, 공개적으로 사용가능한 코드가 있는 유일한 에이 전트임. 이 에이전트는 A3C (Asynchronous Advantage Actor-Critic)를 사용하여 이전의 기본 작업 중 596개 작 업을 휴리스틱하게 선택하고 감소된 작업 공간으로 에이전트를 훈련시킨다. 추가적으로 YZM은 다른 행동의 또 다른 집합으로 백업 에이전트를 추가로 교육하고 시뮬레이션 기능을 사용하여 기본 에이전트가 오버플로 또는 종료를 초래할 수 있는 경우 백업 에이전트를 호출한다. 공정한 비교를 위해서, YZM을 제외한 모든 베이스라인은 동일한 GNN 아키텍처를 통해 입력 상태를 인코딩하고 에이전트는 위험한 상황에서만 활성화된다. 도 6은 본 발명의 예시적인 실시예에 따른 전력망 운영 제어 방법(SMAAC)의 성능을 종래기술들과 비교 평가하기 위한 3가지 전력망 그리드들에 대한 학습 커브를 예시한다. 평가를 위한 롤아웃들은 매 1000스텝 마다 수행되었 으며, 음영부분은 표준 오차를 나타낸다. 학습 과정 동안 10가지 검증 시나리오에 대하여 롤아웃한 전체 평균 척도 점수를 나타낸다: 점수는 [-100, 100] 범위의 값을 가지며, no-op 에이전트의 리턴은 0으로 변환되었으며, 에이전트가 안전 및 전력 효율성 측면에서 no-op 에이전트보다 전력망을 얼마나 잘 관리하는지 나타낸다. 각각 의 알고리즘들은 평균 점수 계산을 위해 3번씩 학습 및 평가를 진행하였다. 도 6에 나타낸 것처럼, 모든 알고리즘들은 가장 작은 그리드(IEEE-5)는 쉽게 해결한다. 중간 그리드(IEEE-14)와 큰 그리드(L2RPN WCCI 2020)에 대해서는, DDQN과 SAC 모두 잘 동작하지 않는다. DDQN은 중간 그리드에서는 no- op 에이전트보다 약간 더 나은 성능을 가지고, 가장 큰 그리드에서는 no-op 에이전트보다 성능이 떨어진다. 기 본 행동들에서 탐색하는 것은 대부분의 행동이 비참한 종료로 이어질 수 있기 때문에 극도로 어려우며, 따라서 초기의 그리드 이외의 그리드를 찾을 수 없다. 이는 DDQN이 잘못된 로컬 최적 값에 빠지는 것을 야기하며, no- op 에이전트보다 좋지 않다. SAC는 더 큰 그리드에서 DDQN보다 약간 더 나은 성능을 보인다. 이는 여러 다른 RL 벤치마크 태스크들에서 영향을 미치는 것으로 나타난 SAC의 정교한 최적화 방식 때문이다. 그러나 Grid2Op에서 는 DDQN이 직면한 것과 동일한 문제로 인해 성능이 no-op 에이전트와 비교했을 때 아주 조금 좋은 수준이다. 놀랍게도 SMAAC/AS의 성능은 기본 행동들을 사용하는 것보다 낫지 않지만 계층적 결정은 초기 토폴로지에서 벗 어나도록 유도한다. 사후 상태 표현이 없으면 크리틱은 방대한 상태 및 행동 공간으로 인해 좋은 행동 가치 함 수를 배울 수 없다. YZM은 유일하게 시뮬레이션 기능을 활용하여 처음부터 좋은 성능을 보여줄 수 있다. 그러나 기본 행동들을 탐색하는 것은 행동들의 집합이 줄어들더라도 여전히 어렵고, YZM 가 성능 향상에 어려움을 겪는 것을 관찰할 수 있다. 테스트 시나리오의 성능은 표 2에서 제공된다. 표 2는 10개의 테스트 시나리오에서 3개의인스턴스에 대한 조정된 점수의 평균 및 표준 오차 측정을 통해 계산된 성능을 보여준다. 표 2"}
{"patent_id": "10-2022-0185944", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "반대로, 본 발명의 실시예에 따른 방법은 계층적 의사결정 모델과 사후 상태 표현의 이점을 효과적으로 결합하 여 상당히 빠르게 학습하고 모든 비교 알고리즘들의 성능을 능가한다. 한편, 하위 단계 정책이 전체 성능에 어떤 영향을 미치는지 조사했다. FIXED는 사전에 정의된 값으로 무작 위적으로 변전소의 우선순위를 결정하고, 학습 중에는 고정시킨다. 이 하위 단계의 에이전트를 구현하여 상위 단계의 에이전트가 안 좋은 하위 단계의 에이전트에서도 전원 네트워크를 관리할 수 있는지 확인한다. CAPA 는 용량 활용도가 높은 선로가 있는 변전소에 높은 우선순위를 부여하여 긴급한 대처가 필요한 변전소에 행동을 적용한다. DESC는 큰 변전소, 즉 연결된 요소가 많은 변전소에 우선순위를 부여한다. 대형 변전소로의 변경 은 단일 행동으로 전체 토폴로지를 크게 변경하는 것으로 볼 수 있다. OPTI는 액터가 변전소의 우선순위를 나타내는 Nsub 값을 추가로 출력하게 하는 방식으로 학습을 통해 실행 순서 를 최적화한다. 모든 규칙은 FIXED를 제외하고 중첩된 신뢰 구간에서 유사한 성능을 달성하였다. 도 7은 4가지 규칙들(FIXED, CAPA, DESC, OPTI)의 성능을 비교한 그래프이다. 도 7에서 볼 수 있듯이, 특히 CAPA는 OPTI 및 DESC에 비해 빠르게 수렴한다. SMAAC는 최적이 아닌 하위 단계 규칙들에 탄력적이기 때문에 대 부분의 규칙이 유사한 최종 성능을 달성할 수 있다고 가정한다. 의도된 토폴로지 재구성의 하위 집합을 포함하 는 하위 목표를 생성함으로써 상위 레벨 정책은 최적이 아닌 하위 레벨 규칙에 적응하여 전반적으로 최적의 정 책을 형성할 수 있다. 그러나 FIXED의 결과에서 알 수 있듯이 매우 잘못 설계된 하위 레벨 정책은 불안정을 초 래하고 성능을 저하시킬 수 있다. 이상에서, 전력망 관리에 매우 효과적인 것으로 입증된 심층 강화학습 기술인 SMAAC의 실시예를 설명하였다. SMAAC는 사후 상태 표현을 계층적 결정 모델과 결합하는 액터-크리틱 알고리즘이다. 이는 Grid2Op에 의해 모델 링된, 행동들이 효과적인 탐색에 너무 원시적이며 많은 양의 행동 시퀀스의 순열이 전력망 토폴로지에서 동일한 변경으로 이어지는, 전력망 관리에 매우 중요하다. 게다가, 기본 행동들을 바탕으로 한 단순한 탐색은 전력망 관리의 고유한 특성으로 인해 즉시 실패할 수 있다. 이 기술은 실제 규모의 전력망에서 여러 비교 알고리즘들의 성능을 훨씬 능가한다는 것을 실험적으로 입증했다. 본 발명을 이용하면 전문가의 도움 없이 전력망을 자동으로 운영할 수 있는 지능형 에이전트를 개발할 수 있다. 산업상 이용가능성 본 발명은 전력망 그리드의 자동 운영 솔루션 개발 및 그 자동 운영 솔루션을 사용하여 전력망 그리드의 운영을 자동으로 수행하는 데 이용될 수 있다. 이상과 같이 실시예들이 비록 한정된 도면에 의해 설명되었으나, 해당 기술 분야의 숙련된 당업자는 하기의 특 허 청구 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다. 예를 들어, 설명된 기술들이 설명된 방법과 다른 순서로 수행되 거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태로 결합 또는 조합 되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므 로 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한다."}
{"patent_id": "10-2022-0185944", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 강화학습 시스템의 기본 개념을 도식적으로 나타낸다. 도 2는 전력망에서 사후 상태 표현의 일예로서, 버스 할당 작업을 예로 하여 행동이 전력망 상태에 어떻게 영향 을 미치는지 예시한다. 도 3은 본 발명의 예시적인 실시예에 따른 강화학습 에이전트를 이루고 있는 심층 신경망의 구조를 예시한다. 도 4는 본 발명의 예시적인 실시예에 따른 전력망의 관리/운영을 위한 자동 제어 방법(SMAAC)의 실행 절차를 나 타내는 흐름도이다. 도 5는 본 발명의 예시적인 실시예에 따라 전력망의 그래프 구조 표현에서 버스바 할당을 변경하는 예를 보여준 다. 도 6은 본 발명의 예시적인 실시예에 따른 전력망 운영 제어 방법(SMAAC)의 성능을 종래기술들과 비교 평가하기 위한 3가지 전력망 그리드들에 대한 학습 커브를 예시한다. 도 7은 4가지 규칙들(FIXED, CAPA, DESC, OPTI)의 성능을 비교한 그래프이다."}
