{"patent_id": "10-2023-0037999", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0143125", "출원번호": "10-2023-0037999", "발명의 명칭": "인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법", "출원인": "한국과학기술원", "발명자": "유회준"}}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공 계층으로 구성된 인공 신경망;상기 인공 신경망의 인공 계층을 동일한 연산 결과로 출력하는 스파이킹 신경망;상기 인공 계층 별로 인공 신경망 연산 비용과 스파이킹 신경망 연산 비용을 계산하는 주제어부;상기 인공 신경망 연산 비용과 스파이킹 신경망 연산 비용을 비교하여 연산 비용이 적은 연산 도메인을 선택하는 연산 도메인 선택부;상기 연산 도메인 선택부의 스파이킹 신경망 연산 도메인 선택에 따라 상기 인공 신경망의 인공 계층을 상기 스파이킹 신경망의 스파이킹 계층으로 변환하여 혼합 신경망을 구성하는 등가 변환부;를 포함하는 것을 특징으로하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 등가 변환부는 스파이킹 계층의 시간 단계수를 설정하고, 역치를 설정하여 양자화 오차를 줄이고 절단 오차를 제거하는 절단오차 제거부;상기 스파이킹 계층의 모든 뉴런의 초기 막전위를 상기 역치의 절반 값을 더함으로써 양자화 오차를 제거하는양자화 오차 제거부; 및 상기 스파이킹 계층의 마지막 시간 단계에서 모든 출력 스파이크를 스파이크 개수 값으로 일괄적으로 내보내 잔여 막전위 오차를 제거하는 잔여 막전위 오차 제거부;를 포함하는 것을 특징으로 하는 인공 신경망과 스파이킹신경망을 혼합한 저전력 인공지능 처리 시스템."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서,상기 등가 변환부는 고정 소수점 방식으로 표현된 입력 및 가중치 데이터를 사용하는 인공 신경망을 산술적으로 등가의 연산을 수행하는 스파이킹 계층으로 변환하는 것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 2항에 있어서,상기 등가 변환부는상기 절단 오차 제거부를 통해 정밀도 일치화 후, 상기 양자화 오차 제거부를 통해 반-역치 초기화를 순차적으로 적용하는 것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템.공개특허 10-2024-0143125-3-청구항 5 제 2항에 있어서, 상기 스파이크 개수는 전체 시간 단계수에 거쳐 누적된 막전위를 역치로 나눈 몫인 것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1항에 있어서, 상기 주제어부는 인공 계층 평균 연산 에너지(Avg(EA))를 계산하여 인공 신경망 연산 비용을 계산하는 것을 특징으로 하는 인공신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6항에 있어서상기 주제어부는 `1`에서 인공 신경망 희소성(SA)을 뺀 값에 곱셈 연산 에너지(Emul)와 덧셈 연산 에너지(Eadd)를 더하는 수학식으로 상기 인공 계층 평균 연산 에너지(Avg(EA))을 계산하는 것을 특징으로하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서, 상기 주제어부는 스파이킹 계층 평균 연산 에너지(Avg(ES))를 계산하여 스파이킹 신경망 연산 비용을 계산하는 것을 특징로 하는인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8항에 있어서상기 주제어부는 `1`에서 스파이킹 신경망 희소성(SS)을 뺀 값에 시간 단계수(T)와 덧셈 연산 에너지(Eadd)를 더하는 수학식으로 상기 스파이킹 계층 평균 연산 에너지(Avg(ES))를 계산하는 것을 특징으로 하는인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "(a) 데이터 송수신부가 복수의 입력 데이터를 수신하는 단계;(b) 주제어부가 상기 입력 데이터를 표본화하고 입력 데이터 표본에 대하여 인공 신경망 순전파를 하여 중간 활성화 값을 얻어 각 중간 활성화 값의 희소성을 분석하는 단계;공개특허 10-2024-0143125-4-(c) 상기 주제어부가 상기 희소성을 바탕으로 인공 신경망 연산 비용과 스파이킹 신경망 연산 비용을 계산하는단계;(d) 상기 연산 도메인 선택부가 상기 인공 신경망 연산 비용과 스파이킹 신경망 연산 비용을 비교하는 단계;(e) 상기 연산 도메인 선택부가 상기 (d)단계 결과에 따라 스파이킹 계층을 선택하거나, 인공 계층을 선택하는단계;(f) 상기 주제어부가 스파이킹 계층과 인공 계층이 혼합된 혼합 신경망을 구성하고 해당 혼합 신경망의 순전파단계;를 포함하는 것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10항에 있어서,상기 (b)단계에서 상기 주제어부는상기 인공 신경망에서 활성화 데이터 값의 희소성을 나타내는 인공 계층 희소성(SA)을 전체 활성화 데이터에서`0`의 값을 갖는 데이터 수를 전체 활성화 데이터 수로 나누는 수학식 로 계산하는것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 10항에 있어서,상기 (b)단계에서 상기 주제어부는스파이킹 신경망(S)에서 스파이크 회소성을 나타내는 스파이킹 계층 희소성(SS)을 활성화 데이터에 상응하는 전체 스파이크 열에서 스파이크가 발생하지 않은 시간 단계 수를 전체 스파이크 데이터 크기로 나누는 수학식로 계산하는 것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 10항에 있어서, (e-1) 상기 (e)단계에서 스파이킹 신경망 연산 비용이 인공 신경망 연산 비용보다 작아 스파이킹 계층을 선택하면, 상기 등가 변환부가 해당 인공 계층을 계층 단위 등가 변환 알고리즘을 통해 스파이킹 계층으로 변환하는단계;를 포함하는 것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13항에 있어서, 상기 (e-1)단계는 (e-1-1) 상기 등가 변환부의 절단 오차 제거부는 스파이킹 계층의 시간 단계수와 역치를 설정하는 정밀도 일치화 단계; 및(e-1-2) 상기 등가 변환부의 양자화 오차 제거부가 인공 계층의 편향에 역치의 절반을 더하여 스파이킹 계층의편향을 설정하고, 스파이킹 계층의 가중치를 인공 계층의 학습된 가중치로 그대로 설정하는 반-역치 초기화 단공개특허 10-2024-0143125-5-계;를 포함하는 것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 10항에 있어서, (e-2) 상기 스파이킹 신경망 연산 비용이 인공 신경망 연산 비용보다 큰 경우 상기 연산 도메인 선택부가 상기(e)단계에서 해당 계층을 인공 계층으로 유지하는 단계;를 포함하는 것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 10항에 있어서,상기 (f)단계는 (f-1) 상기 주제어부가 상기 혼합 신경망을 구성하는 각 계층의 종류를 파악하는 단계;(f-2) 상기 주제어부가 상기 (f-1)단계에서 계층이 스파이킹 계층일 경우 막전위의 초기값을 편향으로 설정하는막전위 초기화 단계;(f-3) 상기 주제어부가 스파이크 개수 기반의 입력으로부터 입력 스파이크를 생성하는 단계;(f-4) 상기 주제어부는 상기 입력 스파이크와 가중치로 합성곱 연산을 수행한 값을 막전위에 더하여 막전위를누적 하는 단계;(f-5) 상기 주제어부가 마지막 시간 단계인지 판단하는 단계;(f-6) 상기 (f-5)단계에서 마지막 시간 단계인 경우, 상술한 바와 같이 상기 등가 변환부의 잔여 막전위 오차제거부가 일괄적 스파이크를 생성하는 단계; 및(f-7) 상기 주제어부가 스파이킹 계층, 또는 인공 계층이 혼합 신경망의 순전파 과정의 마지막 계층인지 판단하고, 순전파 과정을 종료하거나 상기 (f-1) 이후의 단계를 반복 수행하는 단계;를 포함하는 것을 특징으로 하는인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16항에 있어서,(f-2`) 상기 주제어부가 상기 (f-1) 단계에서 해당 계층이 인공 계층일 경우, 기존의 인공 신경망 순전파 과정과 동일하게 출력 활성화 데이터를 생성하는 단계;를 포함하는 것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법."}
{"patent_id": "10-2023-0037999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 16항에 있어서,상기 (f-3)단계에서 상기 주제어부가 매 시간 단계 마다 입력 활성화 값이 역치 이상일 경우 입력 스파이크를 생성하는 것을 특징으로 하는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법."}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템은 인공 계층으로 구성된 인공 신경망; 상기 인공 신경망의 인공 계층을 동일한 연산 결과로 출력하는 스파이킹 신경망; 상기 인공 계층 별로 인공 신경망 연산 비용과 스파이킹 신경망 연산 비용을 계산하는 주제어부; 상기 인공 신경망 연산 비용과 스파이킹 신경망 연산 비용을 비교하여 연산 비용이 적은 연산 도메인을 선택하는 연산 도메인 선택부; 상기 연 산 도메인 선택부의 스파이킹 신경망 연산 도메인 선택에 따라 상기 인공 신경망의 인공 계층을 상기 스파이킹 신경망의 스파이킹 계층으로 변환하여 혼합 신경망을 구성하는 등가 변환부;를 포함하여 인공 신경망과 비교하였 을 때 정확도 손실이 전혀 발생하지 않는 효과가 있다."}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법에 관한 것으로써, 더 욱 상세하게는 인공 신경망(ANN : Artificial Neural Network)으로 구성된 각 계층의 연산 비용보다 스파이킹 신경망(SNN : Spiking Neural Network)으로 구성된 계층의 연산 비용이 더 적다면, 인공 계층을 동일한 출력을 생성하는 스파이킹 계층으로 변환하여 사용하여 신경망 전체 연산 비용을 절감할 수 있는 인공 신경망과 스파이 킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "심층 신경망 기반의 인공지능을 실제로 적용하기 위해서는, 신경망의 정확도가 높은 것은 물론이고, 신경망 처 리를 위한 연산 비용이 적어야 한다. 현존하는 신경망 종류에는 크게 인공 신경망과 스파이킹 신경망이 있는데, 두 조건을 모두 만족하기에는 각각의 문제점이 존재한다. 인공 신경망은 역전파 기반의 학습으로 높은 정확도에 달성할 수 있지만, 에너지를 소모가 많은 곱셈 연산이 주 를 이루기 때문에 연산 비용이 크다는 문제가 있다. 스파이킹 신경망은 상술한 이유에 의하여 인공 신경망보다 연산 비용이 적어질 기회가 있지만, 인공 신경망에 비하면 정확도가 낮은 문제가 발생한다. 상기 ANN-SNN 변환을 사용하더라도 크게 3가지의 변환 오차(양자화 오차, 절단 오차, 잔여 막전위 근사 오차)가 발생하여 인공 신경망과 같은 정확도를 갖는 스파이킹 신경망을 얻기 어렵다. 양자화 오차는 인공 신경망에서 고정 소수점 방식으로 양자화된 활성화 데이터와 스파이킹 신경망에서 스파이크 개수로 양자화된 활성화 데이터의 정밀도가 달라서 발생하는 오차이다. 절단 오차는 인공 신경망에서 고정 소수점 방식에 의해 제한된 활성화 데이터의 표현 범위보다 스파이킹 신경망 에서 시간 단계수에 의해 제한된 활성화 데이터의 표현 범위가 더 좁아서 발생하는 오차이다. 잔여 막전위 근사 오차는 스파이킹 신경망의 입력 스파이크 패턴에 따라서 마지막 시간 단계에 0 미만 또는 역 치 이상의 막전위가 잔류할 가능성이 있기에 출력 스파이크 개수가 예상한 값과 달라서 발생하는 오차이다. 이러한 오차를 줄이기 위해 스파이킹 신경망의 시간 단계수(즉, 활성화 데이터 표현에 사용되는 스파이크 개 수)를 늘릴 수 있지만, 스파이킹 신경망의 누적-발화 연산은 출력 스파이크 개수에 비례하는 연산 비용을 소모 하기 때문에 연산 비용을 희생해야 한다. 이 경우 오히려 스파이킹 신경망의 연산 비용이 인공 신경망보다 커질 수 있다. 따라서 인공 신경망의 높은 정 확도를 유지하면서도, 스파이킹 신경망의 적은 연산비용 특성을 활용할 수 있는 새로운 신경망 구조 및 구성 방 법이 필요한 실정이다. 선행기술문헌 비특허문헌 (비특허문헌 0001) [1] Maass, W. 1997. Networks of spiking neurons: the third generation of neural network models. Neural networks, 10: 1659-1671. (비특허문헌 0002) [2] Masquelier, T.; and Thorpe, S. J. 2007. Unsupervised learning of visual features through spike timing dependent plasticity. PLoS computational biology, 3: e31 (비특허문헌 0003) [3] Wu, Y.; Deng, L.; Li, G.; Zhu, J.; and Shi, L. 2018. Spatiotemporal backpropagation for training high-performance spiking neural networks. Frontiers in neuroscience, 12: 331. (비특허문헌 0004) [4] Diehl, P. U.; Neil, D.; Binas, J.; Cook, M.; Liu, S.-C.; and Pfeiffer, M. 2015. Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing. In 2015"}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "International joint conference on neural networks (IJCNN), 1-8. ieee. 발명의 내용"}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상술한 문제점들을 해소하고 필요를 충족시키기 위해 본 발명은 인공 신경망의 일부 계층만 스파이킹 계층으로 선택적으로 변환하여 높은 정확도와 낮은 연산 비용을 동시에 갖는 인공 신경망과 스파이킹 신경망을 혼합한 저 전력 인공지능 처리 시스템 및 방법을 제공하는데 목적이 있다. 또한, 상술한 문제점들을 해소하고 필요를 충족시키기 위해 본 발명은 인공 계층과 스파이킹 계층의 출력 활성 화 값을 완전히 일치시키는 계층 단위 등가 변환 방법을 통해 변환 전 인공 신경망의 높은 정확도를 보존할 수 있는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법을 제공하는데 목적이 있다. 또한, 상술한 문제점들을 해소하고 필요를 충족시키기 위해 본 발명은 인공 계층에서 고정 소수점 표현방식으로 나타낸 활성화와 가중치의 곱셈 연산이 스파이킹 계층에서 시간 단계 동안의 가중치 누적 연산과 등가의 연산이 되도록 스파이킹 신경망의 연산 방식을 정의하고, 이를 통해 인공 신경망을 스파이킹 신경망으로 변환할 때 출 력 활성화 사이에 발생하는 양자화 오차, 절단 오차, 또는 잔여 막전위 근사 오차를 제거할 수 있는 인공 신경 망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법을 제공하는데 목적이 있다. 또한, 또한, 상술한 문제점들을 해소하고 필요를 충족시키기 위해 본 발명은 인공 계층과 스파이킹 계층의 연산 비용을 분석하여 해당 계층을 스파이킹 신경망으로 변환할지를 결정하는 연산 도메인 선택 방법을 통해, 계층 별로 스파이크 희소성을 얻고, 이에 따라 인공 신경망 연산과 스파이킹 신경망 연산의 비용을 계산하여, 둘 중 비용이 적은 것을 선택하여, 결과적으로, 인공 계층 또는 인공 계층으로부터 등가 변환된 스파이킹 계층을 신경 망의 각 계층으로 사용하여, 인공 신경망의 정확도를 유지하면서도 연산 비용이 작은 혼합 신경망을 구성할 수 있는 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법을 제공하는데 목적이 있다."}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템은 인공 계층으로 구성된 인공 신경망; 상기 인공 신경망의 인공 계층을 동일한 연산 결과로 출력하는 스 파이킹 신경망; 상기 인공 계층 별로 인공 신경망 연산 비용과 스파이킹 신경망 연산 비용을 계산하는 주제어부; 상기 인공 신경망 연산 비용과 스파이킹 신경망 연산 비용을 비교하여 연산 비용이 적은 연산 도메인 을 선택하는 연산 도메인 선택부; 상기 연산 도메인 선택부의 스파이킹 신경망 연산 도메인 선택에 따라 상기 인공 신경망의 인공 계층을 상기 스파이킹 신경망의 스파이킹 계층으로 변환하여 혼합 신경망을 구성하는 등가 변환부;를 포함하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 등가 변환부는 스파이킹 계층의 시간 단계수를 설정하고, 역치를 설정하여 양자화 오차를 줄이고 절단 오차를 제거하는 절단 오차 제거부; 상기 스파이킹 계층의 모든 뉴런의 초기 막전위를 상기 역치의 절반 값을 더함으로써 양자화 오차를 제거하는 양자화 오차 제거부; 및 상기 스파이킹 계층의 마지막 시간 단계에서 모든 출력 스파이크를 스파이크 개수 값으로 일괄적으로 내보내 잔여 막전위 오차를 제거하는 잔여 막전위 오차 제거부;를 포함하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 등가 변환부는 고정 소수점 방식으로 표현된 입력 및 가중치 데이터를 사용하는 인공 신경망을 산술적으로 등가의 연산을 수행하는 스파이킹 계층으로 변환하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 등가 변환부는 상기 절단 오차 제거부를 통해 정밀도 일치화 후, 상기 양자화 오차 제거부를 통 해 반-역치 초기화를 순차적으로 적용하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 스파이크 개수는 전체 시간 단계수에 거쳐 누적된 막전위를 역치로 나눈 몫인 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 주제어부는 인공 계층 평균 연산 에너지(Avg(EA))를 계산하여 인공 신경망 연산 비용을 계산하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 주제어부는 `1`에서 인공 신경망 희소성(SA)을 뺀 값에 곱셈 연산 에너지(Emul)와 덧셈 연산 에너 지(Eadd)를 더하는 수학식 으로 상기 인공 계층 평균 연산 에너지(Avg(EA))을 계산하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 주제어부는 스파이킹 계층 평균 연산 에너지(Avg(ES))를 계산하여 스파이킹 신경망 연산 비용을 계산하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 주제어부는 `1`에서 스파이킹 신경망 희소성(SS)을 뺀 값에 시간 단계수(T)와 덧셈 연산 에너지 (Eadd)를 더하는 수학식 으로 상기 스파이킹 계층 평균 연산 에너지(Avg(ES))를 계산하 는 것을 특징으로 한다. 다른 실시예로써, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전 력 인공지능 처리 방법은 (a) 데이터 송수신부가 복수의 입력 데이터를 수신하는 단계; (b) 주제어부가 상기 입 력 데이터를 표본화하고 입력 데이터 표본에 대하여 인공 신경망 순전파를 하여 중간 활성화 값을 얻어 각 중간 활성화 값의 희소성을 분석하는 단계; (c) 상기 주제어부가 상기 희소성을 바탕으로 인공 신경망 연산 비용과 스파이킹 신경망 연산 비용을 계산하는 단계; (d) 상기 연산 도메인 선택부가 상기 인공 신경망 연산 비용과 스 파이킹 신경망 연산 비용을 비교하는 단계; (e) 상기 연산 도메인 선택부가 상기 (d)단계 결과에 따라 스파이킹 계층을 선택하거나, 인공 계층을 선택하는 단계; (f) 상기 주제어부가 스파이킹 계층과 인공 계층이 혼합된 혼 합 신경망을 구성하고 해당 혼합 신경망의 순전파 단계;를 포함하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법 (b)단계에서 상기 주제어부는 상기 인공 신경망에서 활성화 데이터 값의 희소성을 나타내는 인공 계 층 희소성(SA)을 전체 활성화 데이터에서 `0`의 값을 갖는 데이터 수를 전체 활성화 데이터 수로 나누는 수학식 로 계산하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법의 (b)단계에서 상기 주제어부는 스파이킹 신경망(S)에서 스파이크 회소성을 나타내는 스파이킹 계층 희소성(SS)을 활성화 데이터에 상응하는 전체 스파이크 열에서 스파이크가 발생하지 않은 시간 단계 수를 전체 스파이크 데이터 크기로 나누는 수학식 로 계산하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법은 (e-1) 상기 (e)단계에서 스파이킹 신경망 연산 비용이 인공 신경망 연산 비용보다 작아 스파이킹 계층을 선택하면, 상기 등가 변환부가 해당 인공 계층을 계층 단위 등가 변환 알고리즘을 통해 스파이킹 계층으 로 변환하는 단계;를 포함한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법의 (e-1)단계는 (e-1-1) 상기 등가 변환부의 절단 오차 제거부는 스파이킹 계층의 시간 단계수와 역치 를 설정하는 정밀도 일치화 단계; 및 (e-1-2) 상기 등가 변환부의 양자화 오차 제거부가 인공 계층의 편향에 역 치의 절반을 더하여 스파이킹 계층의 편향을 설정하고, 스파이킹 계층의 가중치를 인공 계층의 학습된 가중치로 그대로 설정하는 반-역치 초기화 단계;를 포함하는 것을 특징으로 한다.또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법은 (e-2) 상기 스파이킹 신경망 연산 비용이 인공 신경망 연산 비용보다 큰 경우 상기 연산 도메인 선 택부가 상기 (e)단계에서 해당 계층을 인공 계층으로 유지하는 단계;를 포함하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법의 (f)단계는 (f-1) 상기 주제어부가 상기 혼합 신경망을 구성하는 각 계층의 종류를 파악하는 단계; (f-2) 상기 주제어부가 상기 (f-1)단계에서 계층이 스파이킹 계층일 경우 막전위의 초기값을 편향으로 설정하는 막전위 초기화 단계; (f-3) 상기 주제어부가 스파이크 개수 기반의 입력으로부터 입력 스파이크를 생성하는 단 계; (f-4) 상기 주제어부는 상기 입력 스파이크와 가중치로 합성곱 연산을 수행한 값을 막전위에 더하여 막전위 를 누적 하는 단계; (f-5) 상기 주제어부가 마지막 시간 단계인지 판단하는 단계; (f-6) 상기 (f-5)단계에서 마 지막 시간 단계인 경우, 상술한 바와 같이 상기 등가 변환부의 잔여 막전위 오차 제거부가 일괄적 스파이크를 생성하는 단계; 및 (f-7) 상기 주제어부가 스파이킹 계층, 또는 인공 계층이 혼합 신경망의 순전파 과정의 마지 막 계층인지 판단하고, 순전파 과정을 종료하거나 상기 (f-1) 이후의 단계를 반복 수행하는 단계;를 포함하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법은 (f-2`) 상기 주제어부가 상기 (f-1) 단계에서 해당 계층이 인공 계층일 경우, 기존의 인공 신경망 순전파 과정과 동일하게 출력 활성화 데이터를 생성하는 단계;를 포함하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위해 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법은 (f-3)단계에서 상기 주제어부가 매 시간 단계 마다 입력 활성화 값이 역치 이상일 경우 입력 스파 이크를 생성하는 것을 특징으로 한다."}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법은 인공 신경망 과 비교하였을 때 정확도 손실이 전혀 발생하지 않는 효과가 있다. 실제로, 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법은 대표 적인 이미지 분류 데이터셋인 CIFAR10, CIFAR100, ImageNet에서 정확도를 측정하였을 때, 기존 인공 신경망 정 확도와 정확히 일치하는 94.13%, 72.78%, 72.03%의 정확도를 보였다. 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법은 인공 계층 또 는 스파이킹 계층만 존재하는 신경망보다 연산 비용을 절감할 수 있는 효과가 있다. 실제로, 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법은 CIFAR10, CIFAR100, ImageNet 데이터셋에서 측정했을 때 인공 신경망보다 최대 47.8% 연산 비용을 절감할 수 있 는 효과가 있고, 스파이킹 신경망보다 최대 35.1% 작은 연산 비용을 절감할 수 있는 효과가 있다."}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서 및 청구범위에 사용된 용어나 단어는 통상적이거나 사전적인 의미로 한정하여 해석되어서는 아니 되 며, 발명자는 그 자신의 발명을 가장 최선의 방법으로 설명하기 위해 용어의 개념을 적절하게 정의할 수 있다는 원칙에 입각하여, 본 발명의 기술적 사상에 부합하는 의미와 개념으로 해석되어야만 한다. 따라서, 본 명세서에 기재된 실시예와 도면에 도시된 구성은 본 발명의 가 장 바람직한 일 실시예에 불과할 뿐 이고 본 발명의 기술적 사상을 모두 대변하는 것은 아니므로, 본 출원시점에 있어서 이들을 대체할 수 있는 다 양한 균등물과 변형예들이 있을 수 있음을 이해하여야 한다. 이하, 첨부된 도면을 참조하여 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템 및 방법에 대해 상세하게 설명한다. 먼저, 도 1에 도시된 바와 같이 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템은 인공 신경망, 등가 변환부, 연산 도메인 선택부, 혼합 신경망, 및 주제어부(50 0)를 포함하고, 도 1에 도시하지는 않았지만 스파이킹 신경망(S)이 더 포함될 수도 있다. 상기 인공 신경망은 생물학적인 뉴런의 연결성을 모델링하는 것에서 시작하여 인공지능 분야의 기초가 되 었다. 상기 인공 신경망의 대표적인 예시인 합성곱 신경망(CNN : Convolutional Neural Network)과 순환 신경망(RNN : Recurrent Neural Network)은 각각 컴퓨터 비전과 자연어 처리 분야에서 뛰어난 성능을 보인다. 하지만, 인공지능의 적용 분야가 고도화되고 인공 신경망의 연산 비용도 함께 증가함에 따라, 해당 연산 비용을 절감하기 위한 접근이 요구되고 있는 실정이다. 상기 등가 변환부는 상기 인공 신경망의 인공 계층을 동일한 연산 결과를 출력하는 스파이킹 신경망 (S)의 스파이킹 계층(SL)으로 변환한다. 상기 등가 변환부는 고정 소수점 방식으로 표현된 입력 및 가중치 데이터를 사용하는 인공 신경망을 이와 산술적으로 등가의 연산을 수행하는 스파이킹 계층으로 변환한다. 상기 인공 계층을 스파이킹 계층으로 변환할 때 발생하는 양자화 오차, 절단 오차, 잔여 막전위 근사 오차를 모 두 제거하여 결과적인 혼합 신경망이 상기 인공 신경망에 비해 정확도 하락이 없도록 한다. 참고로, 상기 스파이킹 신경망(S)은 생물학적인 뉴런의 연결성뿐만 아니라 동작성까지 모방하여 인공지능을 적 은 연산 비용으로 구현할 수 있는 방법으로 제안되었다. 상기 스파이킹 신경망(S)의 각 뉴런은 누적-발화(IF : ntegrate-and-Fire)모델을 사용하여 신경망 연산을 수행 한다. 상기 누적-발화 모델은 이전 계층에서 온 입력 스파이크(0 또는 1)에 가중치를 곱하여 막전위로 누적하다가, 상 기 막전위의 값이 역치를 넘으면 출력 스파이크를 생성하는 방법이다. 이러한 연산 방식은 두 가지 이유로 인공 신경망의 뉴런보다 연산 비용을 감축할 기회를 갖는다. 첫째, 입 력 스파이크가 발생할 때만 연산이 유발되므로 입력 스파이크의 희소성이 높을 때 연산 비용이 감소한다. 둘째, 상기 인공 신경망에서의 입력과 가중치의 곱셈 연산이 상기 스파이킹 신경망(S)에서는 가중치의 덧셈 연산 으로 치환되며 더 적은 연산 비용을 가질 수 있다. 상술한 스파이킹 신경망(S)에 필요한 파라미터(가중치, 편향 등)를 얻는 방법은 크게 두 가지로 발전되어 왔다. 첫 번째 방법은 직접 학습으로, 스파이킹 신경망(S)을 처음부터 학습시키는 방법이며, STDP(Spike-Time-Dependent Plasticity)와 BPTT (Back-Propagation Through Time)방식으로 다시 나뉜다. 상기 STDP는 시냅스의 가소성을 모방한 방식으로, 정확도가 매우 낮아 패턴 인식과 같은 간단한 어플리케이션에 만 적용될 수 있다는 문제가 있다. 상기 BPTT는 인공 신경망의 역전파(BP : Back-Propagation) 방식을 시간 축으로 확장하여 근사한 방식으로, STDP보다는 정확도가 높지만, 여전히 인공 신경망의 역전파 방식보다는 정확도가 낮다. 두 번째 방법은 인공 신경망(ANN : Artificial Neural Network)-스파이킹 신경망(SNN : Spiking Neural Network)변환으로, 상기 인공 신경망에서 역전파로 훈련시킨 파라미터를 스파이킹 신경망(S)으로 변환하여 사용하는 방식이다. 해당 방식은 직접 학습보다 파라미터를 얻기 간편할 뿐만 아니라 정확도도 현존하는 스파이킹 신경망(S) 중에 가장 높아서, 최근에는 ANN-SNN 변환 방식이 스파이킹 신경망 파라미터를 얻는 방식의 주를 이룬다. 상기 인공 신경망(ANN)의 스파이킹 신경망(SNN)으로 변환의 기본적인 원리는 다음과 같다. 일반적인 인공 신경망의 계층 하나는 <합성곱(또는 완전 연결)계층 - 배치 정규화 - ReLU (Rectified Linear Unit) 활성함수>로 구성된다. 가장 첫 계층의 입력은 일반적으로 비율 코딩(Rate Coding)을 통해 입력 데이터 값에 비례하는 개수의 입력 스파이크를 띄움으로써 구현된다. 상기 합성곱 계층 연산은 정해진 시간 단계 동안 입력 스파이크가 발생했을 때 가중치를 누적함으로써 곱셈 및 누적(MAC : Multiply-and-Accumulate)연산이 구현된다. 상기 배치 정규화는 배치 정규화 파라미터에 따라 가중치와 편향 값을 조정함으로써 구현된다. 상기 ReLU 활성함수는 누적된 막전위가 음수일 때는 출력 스파이크가 발생하지 않는 특성에 의해 저절로 구현된 다. 최종적으로 출력 스파이크는 상기 인공 신경망의 출력 활성화 값에 대략적으로 비례하여 나옴으로써 한 계층의 변환이 진행된다. 상기 연산 도메인 선택부는 각 계층 별로 인공 신경망 연산과 스파이킹 신경망 연산의 비용을 연산 에너지 측면에서 분석하여 더 적은 연산 비용을 갖는 연산 도메인을 선택한다. 상기 인공 신경망의 연산 비용은 데이터 값에 따라 일정하지만 상기 스파이킹 신경망(S)의 연산 비용은 데 이터 희소성에 따라 달라진다. 즉, 도 2에서 희소성이 높은 경우 스파이킹 계층의 연산 비용이 낮아지고, 희소성이 낮은 경우 스파이킹 계층의 연산 비용이 높아진다. 신경망의 각 계층에 상술한 상기 연산 도메인 선택부의 선택 방법을 적용하여 상기 스파이킹 신경망(S)의 연산 비용이 더 적은 계층의 경우 기준 인공 계층을 상기 등가 변환부를 통해 스파이킹 계층으로 변환하여 최종적으로 혼합 신경망을 생성한다. 상기 등가 변환부는 절단 오차 제거부, 양자화 오차 제거부, 및 잔여 막전위 오차 제거부 를 포함하는데, 도 3을 참고하여 상기 절단 오차 제거부와 양자화 오차 제거부에 의한 절단 오차 제 거와 양자화 오차 제거를 설명한다. 도 3은 상기 등가 변환부에서 정밀도 일치화 방법에 의한 절단 오차 제거 효과와 반-역치 초기화 방법에 의한 양자화 오차 제거 효과를 ReLU 활성함수 그래프를 통해 보여준다. 도 3에서 상기 인공 신경망의 경우 X축은 합성곱(또는 완전 연결) 계층과 배치 정규화를 거치고 활성함수 를 거치기 전 단계의 값이며, Y축은 ReLU 활성함수를 거친 최종 출력 값이다. 상기 스파이킹 신경망(S)의 경우 X축은 입력 스파이크와 가중치의 Integrate 연산을 통해 전체 시간 단계수에 거쳐 누적된 전체 막전위 값이며, Y축은 Fire 연산을 통해 생성된 스파이크의 개수에 해당 계층의 역치를 곱한 값이다. 참고로, 상기 역치를 곱하는 것은 인공 계층의 활성화와 같은 단위로 설정하기 위해서이다. 상기 등가 변환부의 절단 오차 제거부는 상기 정밀도 일치화 방법 을 통해 인공 계층의 입력 데이터 가 정수부 길이 I 비트, 소수부 길이 F 비트의 고정 소수점 방식으로 표현되었을 때, 이를 변환한 스파이킹 계 층의 시간 단계수를 2I+F-1로 설정하고, 역치를 2-F으로 설정한다. 상기 설정은 ANN-SNN 변환시 양자화 오차를 줄이고 절단 오차를 완전히 제거하는 효과가 있다. 도 3a의 도시된 바와 같이 스파이킹 계층의 정밀도가 인공 계층의 정밀도보다 작은 경우 인공 계층의 활성함수 출력과 스파이킹 계층의 활성함수 출력 사이에 오차가 발생하며 정밀도 오차 및 절단 오차가 모두 크게 발생한 다. 또한, 도 3b에 도시된 바와 같이 스파이킹 계층의 정밀도가 인공 계층의 정밀도보다 큰 경우에도 마찬가지로 활 성함수 출력 사이에 오차가 발생하며 정밀도 오차 및 절단 오차가 모두 발생할 수 있다. 여기서 도 3c에 도시된 바와 같이 정밀도 일치화 방법을 적용하면, 인공 계층의 출력과 스파이킹 계층의 출력이 가질 수 있는 값의 경우가 일치하여, 양자화 오차가 줄어들고 절단 오차는 완전히 제거된다. 하지만, 상기 인공 신경망의 활성함수는 출력 값 양자화 시 반올림 함수를 적용하며, 상기 스파이킹 신경 망(S)의 활성함수는 출력 스파이크 양자화 시 누적-발화 모델의 규칙에 의해 내림 함수를 적용하기 때문에 활성 함수 입력에 따라 인공 신경망과 스파이킹 신경망의 출력이 다른 범위가 존재한다. 이 때문에 정밀도 일치화 방법을 적용하더라도 양자화 오차가 아직 남아있으며, 상기 등가 변환부의 양자 화 오차 제거부는 반-역치 초기화 방법을 통해 해당 양자화 오차를 제거한다. 보다 구체적으로, 상기 등가 변환부의 양자화 오차 제거부는 반-역치 초기화 방법을 통해 변환된 스 파이킹 계층의 모든 뉴런의 초기 막전위를 상기 정밀도 일치화 방법에서 결정한 역치의 절반 값, 즉 2-F/2을 더 함으로써, ANN-SNN 변환의 양자화 오차를 완전히 제거한다. 상술한 바와 같이 상기 등가 변화부는 정밀도 일치화 방법과 반-역치 초기화 방법을 함께 적용함으로써, 절단 오차와 양자화 오차를 왼전히 제거할 수 있다. 도 3c에서 도 3d로의 변화가 나타내는 바와 같이 상기 양자화 오차 제거부가 사용하는 반-역치 초기화 방 법은 스파이킹 신경망(S)의 활성함수를 X축의 음의 방향으로 2-F/2 만큼 평행이동하는 효과가 있다. 상기 인공 신경망의 출력 양자화에 사용되는 반올림 함수 또한 스파이킹 신경망(S)의 누적-발화 모델에 적 용되는 내림 함수를 X축의 음의 방향으로 정밀도의 절반만큼 평행이동한 함수와 같으므로, 결과적으로 도 3d에 도시된 바와 같이 인공 계층과 스파이킹 계층의 활성함수 그래프가 완전히 일치하게 된다. 따라서 도 3d에 도시된 바와 같이 상기 등가 변환부는 정밀도 일치화 방법 및 반-역치 초기화 방법을 통하 여 ANN-SNN 변환의 양자화 오차와 절단 오차를 완전히 제거한다. 도 4는 상기 등가 변환부의 잔여 막전위 오차 제거부에 의해 상기 계층 단위 등가 변환 방법 중 잔여 막전위 근사 오차를 제거하기 위한 일괄적 스파이크 생성 방법을 두 가지 예시를 통해 보여준다. 상기 잔여 막전위 근사 오차는 도 4a에 도시된 오버플로우 잔여 막전위 근사 오차와 도 4b에 도시된 언더플로우 잔여 막전위 근사 오차로 나뉜다. 도 4a에서 입력 스파이크(a-1) 및 오버플로우 잔여 막전위 근사 오차(a-2)는 스파이킹 계층의 잔여 막전위가 역 치 이상으로 남은 경우이며, 예상보다 적은 개수의 출력 스파이크가 생성된다. 도 4b에서 입력 스파이크(b-1) 및 언더플로우 잔여 막전위 근사 오차(b-2)는 스파이킹 신경망의 잔여 막전위가 역치 미만으로 남은 경우이며, 예상보다 많은 개수의 출력 스파이크가 생성된다. 상기 일괄적 스파이크 생성 방법은 스파이킹 계층의 마지막 시간 단계에서 모든 출력 스파이크를 스파이크 개수 값으로 일괄적으로 내보내는 방법이다. 이때, 스파이크 개수는 전체 시간 단계수에 거쳐 누적된 막전위를 역치로 나눈 몫으로 구한다. 해당 나눗셈 연산의 나머지는 반드시 0 이상 역치 미만이므로, 잔여 막전위가 0 이상 역치 미만이라는 것을 의 미하며, 이에 따라 잔여 막전위 근사 오차가 발생하지 않는다. 도 4의 그래프(a-3, b-3)는 기존의 누적-발화 방 식에서 오버플로우 또는 언더플로우 잔여 막전위 근사 오차가 발생하는 경우에 일괄적 스파이크 생성 방법을 적용하면 잔여 막전위 근사 오차가 발생하지 않음을 도시한 것이다. 이하에서 다른 실시예로써, 상술한 구성을 갖는 본 발명에 따른 공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템에 의한 처리 방법에 대해 설명한다. 본 발명에 따른 공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 데이터 송수신부는 도 5에 도시된 바와 같이 복수의(N개)의 입력 데이터를 수신하는 단계를 수행한다(S100). 이후, 상기 주제어부는 데이터 송수신부가 수신한 입력 데이터에 대해 표본화하고, 입력 데이터 표본 에 대하여 인공 신경망 순전파를 하여 중간 활성화 값을 얻어 각 중간 활성화 값의 희소성을 분석하는 단계를 수행한다(S200). 상기 주제어부는 인공 신경망에서 활성화 데이터 값의 희소성을 나타내는 인공 계층 희소성(SA)을 아 래의 [수학식 1]을 통해 계산한다. 수학식 1"}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "상기 [수학식 1]에서 a, l, N, M, n은 각각 입력 활성화 벡터, 해당 계층의 인덱스, 입력 데이터 표본의 크기, 입력 활성화 벡터의 크기, 입력 데이터 표본의 인덱스이다. 상기 [수학식 1]을 통해 알 수 있는 바와 같이 상기 인공 계층 희소성(SA)은 전체 활성화 데이터에서 `0`의 값 을 갖는 데이터 수를 전체 활성화 데이터 수로 나누어 구한다. 또한, 상기 주제어부는 스파이킹 신경망(S)에서 스파이크 회소성을 나타내는 스파이킹 계층 희소성(SS)을 아래의 [수학식 2]를 통해 계산한다. 수학식 2"}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "상기 수학식에서 a, l, T, N, M, n, θ은 각각 입력 활성화 벡터, 해당 계층의 인덱스, 시간 단계수, 입력 데이 터 표본의 크기, 입력 활성화 벡터의 크기, 입력 데이터 표본의 인덱스, 역치 값이다. 상기 [수학식 2]를 통해 알 수 있는 바와 같이 상기 스파이킹 계층 희소성(SS)은 활성화 데이터에 상응하는 전 체 스파이크 열에서 스파이크가 발생하지 않은 시간 단계 수를 전체 스파이크 데이터 크기로 나누어 구한다. 이후, 상기 주제어부는 분석한 희소성을 바탕으로 인공 계층과 스파이킹 계층의 연산 비용을 계산하는 단 계를 수행한다(S300). 보다 구체적으로, 상기 주제어부는 인공 신경망의 연산 비용에 대응되는 인공 계층 평균 연산 에너지 (Avg(EA))를 아래의 [수학식 3]을 이용해 계산한다. 수학식 3 상기 [수학식 3]를 통해 알 수 있는 바와 같이, 상기 주제어부는 `1`에서 인공 신경망 희소성(SA)을 뺀 값 에 곱셈 연산 에너지(Emul)와 덧셈 연산 에너지(Eadd)를 더하여 인공 계층 평균 연산 에너지(Avg(EA))를 계산한다. 이는 인공 신경망 연산에서 0이 아닌 값 하나 당 1회의 곱셈 및 1회의 덧셈 연산을 요구하기 때문이다. 다음으로, 상기 주제어부는 스파이킹 신경망의 연산 비용에 대응되는 스파이킹 계층 평균 연산 에너지 (Avg(ES))를 아래의 [수학식 4]를 이용해 계산한다. 수학식 4"}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "상기 [수학식 4]를 통해 알 수 있는 바와 같이, 상기 주제어부는 `1`에서 스파이킹 신경망 희소성(SS)을 뺀 값에 시간 단계수(T)와 덧셈 연산 에너지(Eadd)를 더하여 스파이킹 계층 평균 연산 에너지(Avg(ES))를 계산한 다. 이는 스파이킹 신경망(S) 연산에서 스파이크 하나 당 1회의 덧셈 연산을 요구하기 때문이다. 상술한 바와 같이 상기 주제어부가 [수학식 3]과 [수학식 4]를 통해 각각 인공 계층 평균 연산 에너지 (Avg(EA))와 스파이킹 계층 평균 연산 에너지(Avg(ES))을 계산함으로써, 각 계층에서 인공 신경망 연산 비용과 스파이킹 신경망 연산 비용을 계산하면, 상기 연산 도메인 선택부는 이를 비교하는 단계를 수행한다 (S400). 상기 연산 도메인 선택부는 상기 S400에서의 비교를 통해 스파이킹 신경망 연산 비용과 인공 신경망 연산 비용에 따라 스파이킹 계층을 선택하거나, 인공 계층을 선택하는 단계를 수행한다(S500). 즉, 상기 연산 도메인 선택부가 스파이킹 신경망 연산 비용이 인공 신경망 연산 비용보다 작아 스파이킹 계층을 선택하면, 상기 등가 변환부는 해당 인공 계층을 계층 단위 등가 변환 알고리즘을 통해 스파이킹 계층으로 변환하는 단계를 수행한다(S510). 보다 구체적으로, 도 6에 도시된 바와 같이 상기 등가 변환부의 절단 오차 제거부는 스파이킹 계층의 시간 단계수(T)를 2I+F-1, 역치(θl)를 2-F로 설정하여, 정밀도 일치화 단계를 수행한다(S511). 상기 등가 변환부의 양자화 오차 제거부는 인공 계층의 편향에 역치의 절반을 더하여 스파이킹 계층 의 편향을 설정하고(bl S=bl A+θl/2), 스파이킹 계층의 가중치(Wl S)를 인공 계층의 학습된 가중치(Wl A)로 그대로 설 정하여 반-역치 초기화 단계를 수행한다(S512). 참고로, 상기 편향은 첫 번째 시간 단계에서 더해지기 때문에, 이는 막전위의 초기값에 역치의 절반을 더하는 것과 마찬가지이다. 상기 연산 도메인 선택부는 스파이킹 신경망 연산 비용이 인공 신경망 연산 비용보다 큰 경우 해당 계층을 인공 계층으로 유지하는 단계를 수행한다(S520). 참고로, 상기 등가 변환부 또는 연산 도메인 선택부에서 수행되는 계산 등 각종 기능은 상기 주제어 부에서도 수행될 수도 있다. 상술한 연산 도메인 선택 방법을 모든 계층에 대해 적용하여, 결과적으로 상기 주제어부는 스파이킹 계층 과 인공 계층이 혼합되어 가장 적은 연산 비용을 소모하는 혼합 신경망을 구성하고 해당 혼합 신경망의 순전파 단계를 수행한다(S600)."}
{"patent_id": "10-2023-0037999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 5, "content": "도 7은 상기 도 5에서 도시한 연산 도메인 선택 알고리즘을 통해 구성한 혼합 신경망의 순전파 과정을 요약한 흐름도이다. L개의 계층에 대해 다음의 계층 단위 순전파 과정을 반복한다. 먼저, 상기 주제어부는 상기 혼합 신경망을 구성하는 각 계층의 종류를 파악하는 단계를 수행한다(S610). 상기 주제어부는 상기 S610 단계에서 해당 계층이 인공 계층일 경우, 기존의 인공 신경망 순전파 과정과 동일하게 출력 활성화 데이터를 생성하는 단계를 수행한다(S670). 상기 주제어부는 상기 S610 단계에서 해당 계층이 스파이킹 계층일 경우 인공 계층에서 등가 변환된 스파 이킹 계층의 순전파 과정(S630~S660)을 수행한다. 보다 구체적으로, 상기 주제어부는 막전위의 초기값을 편향으로 설정하는 막전위 초기화 단계(vl[0]=bl s)를 수행한다(S620). 그리고 상기 주제어부는 전체 시간 단계수에 대해 다음과 같은 누적-발화 연산을 반복한다. 상기 주제어부는 스파이크 개수 기반의 입력으로부터 입력 스파이크를 생성하는 단계를 수행한다(S630). 보다 구체적으로, 상기 주제어부는 매 시간 단계 마다 입력 활성화 값이 역치 이상일 경우 입력 스파이크 를 생성(st-1[t]=int(al-1≥θl-1))한다. 이때, 상기 S630 단계에서 상기 주제어부는 상기 입력 활성화 값에서는 역치 값을 뺀다(at-1=al-1-θl-1st- 1[t]). 상기 주제어부는 상기 입력 스파이크와 가중치로 합성곱(또는 완전 연결) 연산을 수행한 값을 막전위에 더 하여 막전위를 누적(vl[t]=vl[t-1]+Integrate(sl-1[t], Wl s))하는 단계를 수행한다(S640). 이후, 상기 주제어부는 마지막 시간 단계인지 판단하는 단계를 수행한다(S650). 상기 S650 단계에서 마지막 시간 단계가 아닌 경우 상기 S630단계를 반복 수행하고, 상기 S650 단계에서 마지막 시간 단계인 경우, 상술한 바와 같이 상기 등가 변환부의 잔여 막전위 오차 제거부는 일괄적 스파이 크를 생성하는 단계를 수행한다(S660). 즉, 상기 등가 변환부의 잔여 막전위 오차 제거부는 상기 일괄적 스파이크 생성 방법을 적용하여, 스 파이크 개수 형태로 계층의 출력을 생성하고(cl=[vl[T]/θl]), 인공 신경망의 출력과 범위를 통일하기 위하여 스 파이크 개수에 해당 계층의 역치를 곱하여(al=θlcl) 내보내는 방식으로 일괄적 스파이크를 생성한다. 상기 주제어부는 스파이킹 계층, 또는 인공 계층이 혼합 신경망의 순전파 과정의 마지막 계층인지 판단하 고, 마지막 계층인 경우 순전파 과정을 종료하고, 마지막 계층이 아닌 경우 상기 S610 단계를 반복 수행한다. 도 8은 본 발명에서 제안하는 혼합 신경망의 구성 예시이다. 해당 예시는 하나의 신경망에서 스파이킹 계층과 인공 계층이 번갈아 가며 구성될 수 있음을 보여준다. 그리고 인공 계층의 입력 활성화에 대해서는 도 7의 인공 신경망 순전파 알고리즘을 적용하고, 스파이킹 계층의 입력 활성화에 대해서는 도 7의 스파이킹 신경망 순전파 알고리즘을 적용한다. 도 9는 실제 신경망 아키텍처 및 벤치마크에 본 발명의 혼합 신경망을 적용했을 때의 계층 구성 및 연산 에너지 를 도시한다. VGG-16 신경망와 CIFAR10 벤치마크를 사용한 경우, ResNet-18 신경망와 CIFAR100 벤치마크를 사용한 경우, 및 MobileNet-V2 신경망와 ImageNet 벤치마크를 사용한 경우의 결과를 각각 도 9a, 도 9b, 및 도 9c에 대표적으로 나타내었다. 인공 신경망, 스파이킹 신경망, 혼합 신경망에 대하여, X축은 각 계층의 인덱스, Y축은 각 계층의 에너지 소모 를 나타내었다. 상기 연산 도메인 선택 방법을 적용했을 때 혼합 신경망의 구성은 X축 하단의 박스 색깔로 도시 하였다. (마젠타: 인공 계층, 시안: 스파이킹 계층) 그 결과, 혼합 신경망은 신경망 및 벤치마크에 관계 없이 모든 계층에 대하여 최소의 연산 비용을 소모하였다. 전체 신경망 연산 에너지로 비교했을 때, 혼합 신경망은 도 9a에 도시된 바와 같이 인공 신경망의 75.4%, 스파 이킹 신경망의 90.0%의 에너지, 도 9b에 도시된 바와 같이 인공 신경망의 88.4%, 스파이킹 신경망의 71.5%의 에 너지, 도 9c에 도시된 바와 같이 인공 신경망의 76.3%의 에너지, 스파이킹 신경망의 69.5%의 에너지를 소모하였다. 해당 결과에서 혼합 신경망은 인공 신경망과 스파이킹 신경망 중 연산 비용이 작은 계층을 따라가며 둘과 비교 했을 때 최소한의 연산 비용을 소모한다는 것을 알 수 있다. 이상에서는 본 발명에 대한 기술사상을 첨부 도면과 함께 서술하였지만 이는 본 발명의 바람직한 실시 예를 예 시적으로 설명한 것이지 본 발명을 한정하는 것은 아니다. 또한 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자라면 누구나 본 발명의 기술적 사상의 범주를 이탈하지 않는 범위 내에서 다양한 변형 및 모방이 가능함 은 명백한 사실이다."}
{"patent_id": "10-2023-0037999", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 블록도 이다. 도 2는 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 메커니즘을 도 식화한 구성도 이다. 도 3은 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 양자화 오차와 절단 오차를 없애는 방법 및 원리를 설명하기 위한 도면이다. 도 4는 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 등가 변환 방 법 중 잔여 막전위 근사 오차를 없애는 방법 및 원리를 설명하기 위한 도면이다. 도 5는 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법의 플로우차트이다. 도 6은 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법에서 연산 도메인선택 방법의 플로우차트이다. 도 7은 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법에서 혼합 신경망의 순전파 알고리즘의 플로우차트이다. 도 8은 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 방법에서 인공 계층과 스파이킹 계층이 서로 활성화를 주고받으며 연속적으로 배치된 상태를 도시한 도면이다. 도 9는 본 발명에 따른 인공 신경망과 스파이킹 신경망을 혼합한 저전력 인공지능 처리 시스템의 혼합 신경망을 실제 데이터셋에 적용했을 때 인공 신경망 또는 스파이킹 신경망보다 더 적은 연산 비용이 소모됨을 설명하기 위한 도면이다."}
