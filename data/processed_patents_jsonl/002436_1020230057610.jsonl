{"patent_id": "10-2023-0057610", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0160823", "출원번호": "10-2023-0057610", "발명의 명칭": "인공지능을 이용한 오디오 분류를 수행하기 위한 방법 및 그 장치", "출원인": "한양대학교 산학협력단", "발명자": "장준혁"}}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능을 이용하여 전자 장치에 의해 수행되는 방법에 있어서,음성 데이터를 수신하는 단계;상기 음성 데이터를 기반으로 제1 데이터 및 제2 데이터를 생성하는 단계;상기 제1 데이터 및 상기 제2 데이터를 입력 정보로 하여 제1 인공지능 네트워크에서 제3 데이터를 출력하는 단계;상기 제1 데이터를 입력 정보로 하여 제2 인공지능 네트워크에서 제4 데이터를 출력하는 단계; 및상기 제3 데이터 및 상기 제4 데이터를 기반으로 상기 음성 데이터를 분류하는 단계를 포함하는 방법."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 데이터는 global view이고, 및 상기 제2 데이터는 local view인 방법."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 음성 데이터를 기반으로 제1 데이터 및 제2 데이터를 생성하는 단계는:상기 음성 데이터에 RFN(residual frequency normalization) 연산, self-copy 연산, mixup 연산 중 적어도 하나의 연산을 수행하는 단계; 및상기 연산된 음성 데이터에 RRC(random resize crop)를 수행하여 상기 제1 데이터 및 상기 제2 데이터를 생성하는 단계를 포함하는 방법."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 음성 데이터에 RFN 연산, self-copy 연산, mixup 연산 중 적어도 하나의 연산을 수행하는 단계는:미리 설정된 확률을 기반으로 상기 음성 데이터에 상기 RFN 연산, 상기 self-copy 연산, 상기 mixup 연산 중 적어도 하나를 선택하여 연산을 수행하는 단계; 또는상기 음성 데이터에 포함된 미리 설정된 파라미터 값의 크기를 기반으로 상기 음성 데이터에 상기 RFN 연산, 상기 self-copy 연산, 상기 mixup 연산 중 적어도 하나를 선택하여 연산을 수행하는 단계를 포함하는 방법."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 음성 데이터는 레이블을 포함하지 않고,공개특허 10-2024-0160823-3-상기 제1 인공지능 네트워크는 상기 제3 데이터 및 상기 제4 데이터를 기반으로 학습되는 방법."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 제1 인공지능 네트워크 및 상기 제2 인공지능 네트워크는 동일한 헤더 및 인코더로 구성되고,상기 인코더는 BC-Res2Net인 방법."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 제1 인공지능 네트워크는 상기 제2 인공지능 네트워크로부터 지식 증류 프로세스를 통하여 학습되고,상기 제1 인공지능 네트워크는 학생(student) 네트워크이고, 및상기 제2 인공지능 네트워크는 교사(teacher) 네트워크인 방법."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "전자 장치에 있어서,메모리;모뎀; 및상기 모뎀 및 상기 메모리에 연결되는 프로세서를 포함하고,상기 프로세서는:음성 데이터를 수신하고, 상기 음성 데이터를 기반으로 제1 데이터 및 제2 데이터를 생성하고,상기 제1 데이터 및 상기 제2 데이터를 입력 정보로 하여 제1 인공지능 네트워크에서 제3 데이터를 출력하고,상기 제1 데이터를 입력 정보로 하여 제2 인공지능 네트워크에서 제4 데이터를 출력하고, 그리고상기 제3 데이터 및 상기 제4 데이터를 기반으로 상기 음성 데이터를 분류하도록 구성되는 전자 장치."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 제1 데이터는 global view이고, 및 상기 제2 데이터는 local view인 전자 장치."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서, 상기 프로세서는:상기 음성 데이터에 RFN(residual frequency normalization) 연산, self-copy 연산, mixup 연산 중 적어도 하나의 연산을 수행하고, 그리고상기 연산된 음성 데이터에 RRC(random resize crop)를 수행하여 상기 제1 데이터 및 상기 제2 데이터를 생성하공개특허 10-2024-0160823-4-도록 구성되는 전자 장치."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 프로세서는:미리 설정된 확률을 기반으로 상기 음성 데이터에 상기 RFN 연산, 상기 self-copy 연산, 상기 mixup 연산 중 적어도 하나를 선택하여 연산을 수행하고, 또는상기 음성 데이터에 포함된 미리 설정된 파라미터 값의 크기를 기반으로 상기 음성 데이터에 상기 RFN 연산, 상기 self-copy 연산, 상기 mixup 연산 중 적어도 하나를 선택하여 연산을 수행하도록 구성되는 방법."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항에 있어서, 상기 음성 데이터는 레이블을 포함하지 않고,상기 제1 인공지능 네트워크는 상기 제3 데이터 및 상기 제4 데이터를 기반으로 학습되는 전자 장치."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8항에 있어서,상기 제1 인공지능 네트워크 및 상기 제2 인공지능 네트워크는 동일한 헤더 및 인코더로 구성되고,상기 인코더는 BC-Res2Net인 전자 장치."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8항에 있어서,상기 제1 인공지능 네트워크는 상기 제2 인공지능 네트워크로부터 지식 증류 프로세스를 통하여 학습되고,상기 제1 인공지능 네트워크는 학생(student) 네트워크이고, 및상기 제2 인공지능 네트워크는 교사(teacher) 네트워크인 전자 장치."}
{"patent_id": "10-2023-0057610", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "프로세서(processor)에 의해 실행 가능한 인공지능 알고리즘을 통해 방향 추정 방법을 수행하기 위한 매체에 저장된 프로그램으로서,음성 데이터를 수신하는 단계;상기 음성 데이터를 기반으로 제1 데이터 및 제2 데이터를 생성하는 단계;상기 제1 데이터 및 상기 제2 데이터를 입력 정보로 하여 제1 인공지능 네트워크에서 제3 데이터를 출력하는 단계;상기 제1 데이터를 입력 정보로 하여 제2 인공지능 네트워크에서 제4 데이터를 출력하는 단계; 및상기 제3 데이터 및 상기 제4 데이터를 기반으로 상기 음성 데이터를 분류하는 단계를 수행하는 프로그램."}
{"patent_id": "10-2023-0057610", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 실시 예에 따른 인공지능을 이용하여 전자 장치에 의해 수행되는 방법에 있어서, 음성 데이터를 수신 하는 단계; 상기 음성 데이터를 기반으로 제1 데이터 및 제2 데이터를 생성하는 단계; 상기 제1 데이터 및 상기 제2 데이터를 입력 정보로 하여 제1 인공지능 네트워크에서 제3 데이터를 출력하는 단계; 상기 제1 데이터를 입 력 정보로 하여 제2 인공지능 네트워크에서 제4 데이터를 출력하는 단계; 및 상기 제3 데이터 및 상기 제4 데이 터를 기반으로 상기 음성 데이터를 분류하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0057610", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공지능을 이용하여 오디오 분류를 수행하기 위한 방법 및 그 장치 나타낸다. 구체적으로, 인공지능 에 오디오 표현을 학습시켜 오디오 분류를 수행하는 방법을 나타낸다."}
{"patent_id": "10-2023-0057610", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "본 개시는 음성 신호를 분석하여 오디오 분류를 수행하는 장치 및 방법에 관한 것이다. 오디오 분류 기술은 다 양한 오디오 신호를 분석하여 특정 카테고리 또는 클래스로 분류하는 기술을 나타낸다. 오디오 분류 기술에는 음향 장면 분류, 음향 이벤트 분류 등이 포함될 수 있다. 오디오 분류 기술의 경우 오디오 신호에 대하여 전처리를 통해 특징(feature)을 추출하고, 해당 특징을 인공지 능 모듈에 학습시켜 새로운 오디오 데이터가 입력을 수신하였을 때 해당 오디오 데이터가 어떠한 카테고리 또는 클래스 인지를 결정할 수 있다. 오디오 분류 시스템은 음성 인식, 음악 장르 분류, 음향 사건 감지, 감정 인식 등에 사용될 수 있다. 인공지능을 활용한 오디오 분류 기술에서는 적합한 일반적인 오디오 표현 학습(general audio representation) 이 적용될 수 있는데, 일반적인 오디오 표현 학습이란 다운스트림 작업별 개별적인 모델을 지도 학습 방식으로 학습하는 것이 아니라 대규모 데이터 셋에 인코더 네트워크를 학습시켜 미세 조정, 단순 분류 알고리즘만으로 작업을 수행하는 방식이다. 대규모 데이터 셋에서 도메인의 특징을 학습하기 위해 큰 네트워크 인코더를 사용하 는 것이 보편적이다. 하지만, 서비스 배포, 디바이스에 해당 네트워크를 탑재하기 위해선 지식 증류, 네트워크 가중치 가지치기, 양자화와 같은 후처리 작업이 필수적인 상황이다. 후처리 과정을 추가하는 것은 학습 파이프 라인이 복잡해질 뿐만 아니라 경량화된 모델에서는 사전 학습된 모델에 비해 성능이 저하되는 단점이 있다. 따라서, 인공지능을 활용한 오디오 분류를 수행하기 위하여 오디오 표현 학습에서 경량 모델의 성능을 키우는 방식이 요구되고 있다. 종래에는 실제 서비스화를 고려하여 네트워크의 연산량 및 파라미터의 수가 매우 작도록 설계하였지만 특정 작업을 수행하기 위하여 작업마다 네트워크를 처음부터 학습시켜야 하는 단점이 있었다. 이 러한 경우 과적합 현상이 발생할 수도 있어, 데이터 라벨링 과정이 필수적인 상황이다."}
{"patent_id": "10-2023-0057610", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시에서는 데이터 라벨링 없이 오디오 표현을 학습시켜 딥러닝 경량 모델의 성능을 향상시키는 방법 및 그 장치를 제공하고자 한다. 본 개시에서는 데이터 변형 모듈에서 다양한 변수를 통해 데이터를 증강시켜 음향 분류 기능을 향상시키는 방법 을 제공하고자 한다."}
{"patent_id": "10-2023-0057610", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시 예에 따른 인공지능을 이용하여 전자 장치에 의해 수행되는 방법에 있어서, 음성 데이터를 수신 하는 단계; 상기 음성 데이터를 기반으로 제1 데이터 및 제2 데이터를 생성하는 단계; 상기 제1 데이터 및 상기 제2 데이터를 입력 정보로 하여 제1 인공지능 네트워크에서 제3 데이터를 출력하는 단계; 상기 제1 데이터를 입 력 정보로 하여 제2 인공지능 네트워크에서 제4 데이터를 출력하는 단계; 및 상기 제3 데이터 및 상기 제4 데이 터를 기반으로 상기 음성 데이터를 분류하는 단계를 포함할 수 있다. 일 실시예에서, 상기 제1 데이터는 global view이고, 및 상기 제2 데이터는 local view일 수 있다. 일 실시예에서, 상기 음성 데이터를 기반으로 제1 데이터 및 제2 데이터를 생성하는 단계는: 상기 음성 데이터 에 RFN(residual frequency normalization) 연산, self-copy 연산, mixup 연산 중 적어도 하나의 연산을 수행 하는 단계; 및 상기 연산된 음성 데이터에 RRC(random resize crop)를 수행하여 상기 제1 데이터 및 상기 제2데이터를 생성하는 단계를 포함할 수 있다. 일 실시예에서, 상기 음성 데이터에 RFN 연산, self-copy 연산, mixup 연산 중 적어도 하나의 연산을 수행하는 단계는: 미리 설정된 확률을 기반으로 상기 음성 데이터에 상기 RFN 연산, 상기 self-copy 연산, 상기 mixup 연 산 중 적어도 하나를 선택하여 연산을 수행하는 단계; 또는 상기 음성 데이터에 포함된 미리 설정된 파라미터 값의 크기를 기반으로 상기 음성 데이터에 상기 RFN 연산, 상기 self-copy 연산, 상기 mixup 연산 중 적어도 하 나를 선택하여 연산을 수행하는 단계를 포함할 수 있다. 일 실시예에서, 상기 음성 데이터는 레이블을 포함하지 않고, 상기 제1 인공지능 네트워크는 상기 제3 데이터 및 상기 제4 데이터를 기반으로 학습될 수 있다. 일 실시예에서, 상기 제1 인공지능 네트워크 및 상기 제2 인공지능 네트워크는 동일한 헤더 및 인코더로 구성되 고, 상기 인코더는 BC-Res2Net일 수 있다. 일 실시예에서, 상기 제1 인공지능 네트워크는 상기 제2 인공지능 네트워크로부터 지식 증류 프로세스를 통하여 학습되고, 상기 제1 인공지능 네트워크는 학생(student) 네트워크이고, 및 상기 제2 인공지능 네트워크는 교사 (teacher) 네트워크일 수 있다. 본 발명의 실시 예에 따른 전자 장치에 있어서, 메모리; 모뎀; 및 상기 모뎀 및 상기 메모리에 연결되는 프로세 서를 포함하고, 상기 프로세서는: 음성 데이터를 수신하고, 상기 음성 데이터를 기반으로 제1 데이터 및 제2 데 이터를 생성하고, 상기 제1 데이터 및 상기 제2 데이터를 입력 정보로 하여 제1 인공지능 네트워크에서 제3 데 이터를 출력하고, 상기 제1 데이터를 입력 정보로 하여 제2 인공지능 네트워크에서 제4 데이터를 출력하고, 그 리고 상기 제3 데이터 및 상기 제4 데이터를 기반으로 상기 음성 데이터를 분류하도록 구성될 수 있다. 본 발명의 실시 예에 따른 프로세서(processor)에 의해 실행 가능한 인공지능 알고리즘을 통해 방향 추정 방법 을 수행하기 위한 매체에 저장된 프로그램으로서, 음성 데이터를 수신하는 단계; 상기 음성 데이터를 기반으로 제1 데이터 및 제2 데이터를 생성하는 단계; 상기 제1 데이터 및 상기 제2 데이터를 입력 정보로 하여 제1 인공 지능 네트워크에서 제3 데이터를 출력하는 단계; 상기 제1 데이터를 입력 정보로 하여 제2 인공지능 네트워크에 서 제4 데이터를 출력하는 단계; 및 상기 제3 데이터 및 상기 제4 데이터를 기반으로 상기 음성 데이터를 분류 하는 단계를 수행할 수 있다."}
{"patent_id": "10-2023-0057610", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 일 실시예에 따르면, 오디오 표현 학습을 통해 효율적으로 오디오 분류 기능을 향상시킬 수 있다."}
{"patent_id": "10-2023-0057610", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 기술적 사상은 다양한 변경을 가할 수 있고 여러 가지 실시 예를 가질 수 있는 바, 특정 실시 예들을 도면에 예시하고 이를 상세한 설명을 통해 상세히 설명하고자 한다. 그러나, 이는 본 발명의 기술적 사상을 특 정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 기술적 사상의 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 본 발명의 기술적 사상을 설명함에 있어서, 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요 하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 본 명세서의 설명 과정에서 이용되는 숫자(예를 들어, 제1, 제2 등)는 하나의 구성요소를 다른 구성요소와 구분하기 위한 식별기호에 불과하다. 또한, 본 명세서에서, 일 구성요소가 다른 구성요소와 \"연결된다\" 거나 \"접속된다\" 등으로 언급된 때에는, 상기 일 구성요소가 상기 다른 구성요소와 직접 연결되거나 또는 직접 접속될 수도 있지만, 특별히 반대되는 기재가 존재하지 않는 이상, 중간에 또 다른 구성요소를 매개하여 연결되거나 또는 접속될 수도 있다고 이해되어야 할 것이다. 또한, 본 명세서에 기재된 \"~부\", \"~기\", \"~자\", \"~모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 프로세서(Processor), 마이크로 프로세서(Micro Processer), 마이크로 컨트롤러(Micro Controller), CPU(Central Processing Unit), GPU(Graphics Processing Unit), APU(Accelerate Processor Unit), DSP(Drive Signal Processor), ASIC(Application Specific Integrated Circuit), FPGA(Field Programmable Gate Array) 등과 같은 하드웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있으며, 적어도 하나의 기능이나 동작의 처리에 필요한 데이터를 저장하는 메모리(memory)와 결합되는 형태 로 구현될 수도 있다. 그리고 본 명세서에서의 구성부들에 대한 구분은 각 구성부가 담당하는 주기능 별로 구분한 것에 불과함을 명확 히 하고자 한다. 즉, 이하에서 설명할 2개 이상의 구성부가 하나의 구성부로 합쳐지거나 또는 하나의 구성부가 보다 세분화된 기능별로 2개 이상으로 분화되어 구비될 수도 있다. 그리고 이하에서 설명할 구성부 각각은 자 신이 담당하는 주기능 이외에도 다른 구성부가 담당하는 기능 중 일부 또는 전부의 기능을 추가적으로 수행할 수도 있으며, 구성부 각각이 담당하는 주기능 중 일부 기능이 다른 구성부에 의해 전담되어 수행될 수도 있음은 물론이다. 본 개시의 실시예들을 설명함에 있어서 관련된 기능 혹은 구성에 대한 구체적인 설명이 본 개시의 요지를 불필 요하게 흐릴 수 있다고 판단된 경우 그 상세한 설명은 생략한다. 그리고 후술되는 용어들은 본 개시에서의 기능 을 고려하여 정의된 용어들로서 이는 사용자, 운용자의 의도 또는 관례 등에 따라 달라질 수 있다. 그러므로 그 정의는 본 명세서 전반에 걸친 내용을 토대로 내려져야 할 것이다. 마찬가지 이유로 첨부 도면에 있어서 일부 구성요소는 과장되거나 생략되거나 개략적으로 도시될 수 있다. 또한, 각 구성요소의 크기는 실제 크기를 전적으로 반영하는 것이 아니다. 각 도면에서 동일한 또는 대응하는 구성요소에는 동일한 참조 번호를 부여하였다. 본 개시의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 개시는 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있으며, 단지 실시예들은 본 개시의 설명이 완전하도록 하고, 본 개시의 실시예"}
{"patent_id": "10-2023-0057610", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "들이 속하는 기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 개시의 청구하고자 하는 범위는 청구항의 범주에 의해 정의될 뿐이다. 이때, 처리 흐름도를 보이는 도면들의 각 블록과 처리 흐름도 도면들의 조합들은 컴퓨터 프로그램 인스트럭션들 에 의해 수행될 수 있음을 이해할 수 있을 것이다. 이들 컴퓨터 프로그램 인스트럭션들은 범용 컴퓨터, 특수용 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서에 탑재될 수 있으므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서를 통해 수행되는 그 인스트럭션들이 흐름도 블록(들)에서 설 명된 기능들을 수행하는 수단을 생성하게 된다. 이들 컴퓨터 프로그램 인스트럭션들은 특정 방식으로 기능을 구 현하기 위해 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 지향할 수 있는 컴퓨터 이용 가능 또는 컴퓨터 판독 가능 메모리에 저장되는 것도 가능하므로, 그 컴퓨터 이용가능 또는 컴퓨터 판독 가능 메모리에 저 장된 인스트럭션들은 흐름도 블록(들)에서 설명된 기능을 수행하는 인스트럭션 수단을 내포하는 제조 품목을 생산하는 것도 가능하다. 컴퓨터 프로그램 인스트럭션들은 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장 비 상에 탑재되는 것도 가능하므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에서 일련의 동 작 단계들이 수행되어 컴퓨터로 실행되는 프로세스를 생성해서 컴퓨터 또는 기타 프로그램 가능한 데이터 프로 세싱 장비를 수행하는 인스트럭션들은 흐름도 블록(들)에서 설명된 기능들을 실행하기 위한 단계들을 제공하는 것도 가능하다. 또한, 각 블록은 특정된 논리적 기능(들)을 실행하기 위한 하나 이상의 실행 가능한 인스트럭션들을 포함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있다. 또, 몇 가지 대체 실행 예들에서는 블록들에서 언급된 기 능들이 순서를 벗어나서 발생하는 것도 가능함을 주목해야 한다. 예컨대, 잇달아 도시되어 있는 두 개의 블록들 은 사실 실질적으로 동시에 수행되는 것도 가능하고 또는 그 블록들이 때때로 해당하는 기능에 따라 역순으로 수행되는 것도 가능하다. 본 개시에서 사용되는 '~부(unit or part)'라는 용어는 소프트웨어 또는 FPGA(field-Programmable Gate Array) 또는 ASIC(Application Specific Integrated Circuit)과 같은 하드웨어 구성요소를 의미하며, '~부'는 특정한 역할들을 수행하도록 구성될 수 있다. 그렇지만 '~부'는 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. '~부'는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 실행시키 도록 구성될 수도 있다. 따라서, 일 예로서 '~부'는 소프트웨어 구성요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브 루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들, 및 변수들을 포함한다. 구성요소들과 '~부'들 안에서 제공되는 기능은 더 작은 수 의 구성요소들 및 '~부'들로 결합되거나 추가적인 구성요소들과 '~부'들로 더 분리될 수 있다. 뿐만 아니라, 구 성요소들 및 '~부'들은 디바이스 또는 보안 멀티미디어카드 내의 하나 또는 그 이상의 CPU들을 재생시키도록 구 현될 수도 있다. 또한 실시예에서 '~부'는 하나 이상의 프로세서 및/또는 장치를 포함할 수 있다. 이하, 본 발명의 기술적 사상에 따른 실시 예들을 차례로 상세히 설명한다. 도 1은 본 개시의 일 실시 예에 따른 인공지능 구조의 기본적인 원리를 나타낸 개념도이다. 도 1을 참조하면, 인공지능 구조에서 학습이 수행되는 기본적인 원리를 나타낸다. 인공지능 기술은 학습, 문제 해결, 인식 등과 같이 주로 인간 지능과 연결된 인지 문제를 해결하기 위한 기술을 나타낸다. 인공지능은 Machine learning(ML)이라고 불리는 기계 학습 방식과 Deep learning(DL)이라고 불리는 딥 러닝 방식을 통해 학습될 수 있다. 머신 러닝은 패턴 인식 및 학습에 사용되는 기법에 주로 사용되며 기록된 데이터를 학습하여 이를 기반으로 이후의 데이터를 예측하는 알고리즘을 나타낸다. 사전에 정의된 규칙이나 패 턴을 기반으로 하지 않고 데이터로부터 스스로 학습하는 기술을 나타낸다. 반면에 딥 러닝은 머신 러닝의 한 분 야로 인공 신경망(Artificial Neural Network: ANN)을 기반으로 하여 데이터를 처리하는 차이점이 있다. 딥 러 닝은 인공 신경망을 이용하기 때문에 머신 러닝보다 더욱 복잡하고 정교한 연산을 처리할 수 있다. 딥 러닝을 위한 알고리즘 종류로는 합성곱 신경망(Convolution neural network: CNN), DNN(deep neural network), 인공 신경망(ANN), 순환 신경망(Recurrent Neural Network: RNN)등을 포함할 수 있다. 도 1을 참고하면, 인공지능 구조는 인공지능 모듈로 나타낼 수 있다. 인공지능 모듈은 소정의 입력 데이터를 수신하여 모듈에서 미리 정해진 방식을 통해 학습을 수행하고, 학습 결과에 대한 출력 데이터 를 출력하게 된다. 일 실시예에 따르면, 입력 데이터에는 소정의 데이터(ex 그림, 소리 등), 음성 신 호, 입력 시퀀스를 포함할 수 있다. 출력 데이터에는 출력 시퀀스, 향상된 음성 신호, 오디오 분류 정보 (예를 들어, 오디오 카테고리, 오디오 클래스 등) 등이 포함될 수 있다. 도 2는 본 개시의 일 실시예에 따른 인공지능 네트워크의 구조를 나타낸 도면이다. 도 2에서 사용되는 인공지능 알고리즘은 도 1의 인공지능 모듈의 종류 중 하나일 수 있다. 도 2를 참조하면, 인공지능 시스템의 구조는 음성 데이터와 인공지능 알고리즘 구조를 포함할 수 있다. 일 실시예에 따르면, 음성 데이터는 음성 데이터 셋이거나 로그-멜 변환된 스펙트로그램의 형태 일 수 있 다. 여기서, 스펙트로그램은 오디오 신호를 분석하는 데 사용되는 것으로 주파수 성분을 시각적으로 나타낸 2차 원 그래프이다. 특히, 로그-멜 스펙트로그램은 시간-주파수 영역에서 스펙트로그램을 멜 스케일로 변환하여 로 그처리한 것이다. 일 실시예에 따르면, 인공지능 알고리즘 구조는 데이터 전처리, 인공지능 학습, 데이터 출력으로 단계가 나뉠 수 있다. 본 개시에서의 데이터 전처리는 증강 모듈(Augmentation module)에서 수행될 수 있다. 증강 모듈 의 구조에 대해서는 이후 도 3에서 자세히 설명하도록 한다. 일 실시예에 따르면, 증강 모듈에 입력된 음성 데이터는 증강 모듈의 연산 절차에 따라 증강되 어 Global view 및 local view로 변형될 수 있다. 생성된 Global view 및 local view는 각각 학생 네트워크 및 교사 네트워크로 입력될 수 있다. 예를 들어, 교사 네트워크에는 global view만 입력되고, 학생 네트워크에는 global view 및 local view가 모두 입력될 수 있다. Global view의 경우 학습이 더 용이하기 때문에 연산량이 적은 학생 네트워크에 입력되어도 학습이 용이할 수 있다. 동일한 구조의 모델 에서 입력을 다르게 하여 학습을 다양화할 수 있다. 도 2의 인공지능 시스템은 DINO(DIstillation of knowledge with No labels for Open set recognition)의 구조 를 활용하였다. DINO는 인공지능 분야의 비지도학습 방법 중 하나로 대규모 데이터셋에서 효과적인 표현을 학습 하기 위한 방법으로 자기지도학습번 중 하나이다. 기존의 대규모 모델(교사 모델)에서 작은 모델(학생 모 델)로 지식을 전달하는 방식인 지식 증류 기법에 대하여 비지도 방식으로 확장한 것으로 레이블이 없이 학 습이 진행되는 것이 특징이다. DINO의 핵심 내용은 교사 모델과 학생 모델 간의 상호 작용을 통해 이미지(또는 오디오)의 유용한 특징을 추출한다. 모델은 일련의 데이터를 입력으로 받아 \"교사\"와 \"학생\"이 서로 다른 시점 에서 같은 데이터를 처리하도록 한다. 이 과정에서 교사 모델은 학생 모델에게 이미지에 대한 정보를 전달하며, 학생 모델은 이 정보를 사용하여 자신의 표현을 개선할 수 있다. DINO는 일반적으로 이미지 처리에 사용되지만 오디오 영역에 적용하기 인코더 구조 및 증강 모듈 설계를 변경하였다. 도 2의 인공지능 시스템은 학생(student) 네트워크와 매 학습 단계마다 갱신되는 학생 네트워크 파라미터 의 지수 이동 평균(Exponential Moving Average: EMA)으로 이루어진 교사(teacher) 네트워크로 구성될 수 있다. 학생 네트워크 및 교사 네트워크는 모두 DINO head(220a, 225a)와 인코더(220b, 225b)를 포함 할 수 있다. DINO head(220a, 225a)는 사전 훈련에만 사용되며, 인코더(220b, 225b)는 사전 훈련 및 미세 조정 에 사용될 수 있다. 일 실시예에 따르면, 사전 학습 단계에서 자기 지도학습의 인코더(220b, 225b)는 학습이 필요한 오디오 표현(즉, 데이터 임베딩)을 출력하며, DINO head(220a, 225a)는 인코더에서 출력한 오디오 표현을 입력으로 하 여 증류할 분포를 출력할 수 있다. 즉, 네트워크 출력 분포의 상호 일관성을 최소화하여 오디오 표현을 학습할 수 있다. 미세 조정 전에 사전 훈련된 DINO head(220a, 225a)를 전체 모델에서 제거하고 다운스트림 작업에 적 합한 새로운 분류기를 인코더와 결합할 수 있다. 즉, 인공지능 모듈은 사전 학습 후 다운스트림 작업에서 미세 조정을 할 때 DINO head(220a, 225a)가 생략될 수 있기 때문에 헤드는 무겁게, 인코더는 가볍게 설계할 수 있다. 이러한 점을 활용하여 대형 모델을 통해 사전 학습을 수행하고, 인코더만 분리해낸 경량 모델로 미세 조 정하여 경량 모델이 다양한 오디오 분류 작업에서 높은 성능을 달성할 수 있다. 예를 들어, 인코더(220b, 225 b)는 낮은 계산 복잡도를 가지나 음향 장면 분류에서 뛰어난 성능을 보이는 BC-Res2Net 모듈이 사용될 수 있다. DINO head(220a, 225a)는 MLP(multi layer perception)이 사용될 수 있다. 일 실시예에 따르면, 인코더(220b, 225b)로 BC-Res2Net의 채널을 120으로 설정하였으며 인코더 매개변수의 수는 397K일 수 있다. DINO head(220a,225a)의 MLP는 3개의 선형 레이어와 그 사이의 GELU 활성화로 구성되었으며 모든 레이어의 크기는 2048로 설정될 수 있다. 크기가 65336인 마지막 레이어의 가중치는 훈련 중에 정규화되며 헤드의 매개변수의 수는 17.3M일 수 있다. 일 실시예에 따르면, 학생 네트워크와 교사 네트워크의 구조 및 성능이 동일하게 설정될 수 있다. 일 반적으로, 교사 네트워크가 학생 네트워크보다 더 성능이 뛰어나거나 무거운 모델을 사용하나 본 개 시의 일 실시예에 따르면, 교사 네트워크와 학생 네트워크의 모델을 동일하게 설정할 수 있다. 도 2를 참조하면, 교사 네트워크의 학습된 내용과 학생 네트워크의 학습된 모듈을 활용하여 증강된 데이터가 입력되었을 때 연산을 통해 손실을 계산할 수 있고, 손실을 줄여 나가는 방식으로 파라미터 (예를 들어, gradient)를 조절할 수 있다. 이 경우 손실을 줄일 수 있도록 학생 네트워크는 추가 학 습될 수 있다. 음성 데이터가 학생 네트워크 및 교사 네트워크에 입력되어 출력된 데이터를 기반으로 음성 분류를 수행할 수 있다. 본 개시의 실시예에 따라, 학생 네트워크는 가벼운 모델로 교사 네트워크와 유사한 성능을 가지는 인 공지능 네트워크로 학습될 수 있다. 본 개시에 따른 인공지능 알고리즘을 통해 음성 데이터에서 다양한 오디오 카테고리 또는 오디오 클래스를 결정하고, 해당 카테고리 및 클래스로 분류할 수 있다. 도 3은 본 개시의 일 실시예에 따른 데이터 증강 시스템의 구조를 나타낸 도면이다. 도 3에서 사용되는 데이터 증강 시스템은 도 2의 증강 모듈과 동일한 것일 수 있다. 일 실시예에 따르면, 데이터 증강 시스템으로 입력되는 음성 데이터는 음성 데이터 셋이거나 로그-멜 변환된 스펙트로그램의 형태 일 수 있다. 도 3을 참조하면, 도 3의 증강 시스템은 RFN(residual frequency normalization) 연산 부, self- copy 연산 부, mixup 연산 부, global RRC(random resize crop) 연산 부, local RRC 연산 부 , RFN 연산 부를 포함할 수 있다. 여기서 증강 시스템의 구조는 반드시 모든 연산 부가 포함되 어야 하는 것은 아니며, 선택에 따라 포함되거나 제외될 수 있다. 증강 시스템의 입력 정보로 도 2에서와 같이 음성 데이터가 사용될 수 있다. 일 실시예에 따르면, RFN 연산 부(310,350)는 데이터 증강 시스템의 양쪽 끝에 배치될 수 있다. RFN 연산 부는 오디오 특징에 기록된 장치 관련 정보가 주파수 통계에서 지배적이어서 뛰어난 도메인 일반화 성능을 보이 는 특징이 있다. RFN 은 오디오 데이터에서 주파수 영역의 특성을 보다 정확하게 분석하기 위하여 주파수 대역 별로 일정한 에너지를 유지하는 방식으로 오디오 스펙트럼을 정규화하는데 사용될 수 있다. 오디오 데이터는 일 반적으로 다양한 에너지 분포를 가져서 특징(feature)을 추출하기 전에 데이터를 일정한 기준으로 맞추는 정규 화 작업이 필요하기 때문에 RFN이 활용될 수 있다. 일 실시예에 따르면, 데이터는 self-copy 연산 부로 입력될 수 있다. Self-copy는 데이터를 복사 또는 변 형하여 학습 데이터를 확장하는 방식을 나타낼 수 있다. Self-copy를 통해 원본 데이터를 복사하고 이후 변형하 여 사용할 수 있다. 원하는 출력의 개수만큼 self-copy를 통해 복사를 수행할 수 있다. 일 실시예에 따르면, 데이터는 mixup 연산 부로 입력될 수 있다. Mixup은 데이터 증강 기법 중 하나로 데 이터 샘플을 선형적으로 결합하여 새로운 학습 샘플을 생성하는 기법이다. 데이터 셋에서 무작위로 데이터 샘플 을 선택하여 결합 가중치를 통해 선형 결합을 수행할 수 있다. 복수의 데이터를 포함하는 데이터 셋에서 결합을 통해 새로운 데이터를 생성할 수 있다. 일 실시예에 따르면, 데이터는 전역(global) RRC(random resize crop) 연산 부 또는 local RRC 연산 부 로 입력될 수 있다. RRC는 이미지 데이터에서 주로 사용되는 기법으로 무작위로 크롭(crop) 및 리사이즈하 여 데이터 셋을 확장하는 기법이다. 원본 이미지의 크기를 무작위로 변경하고, 변경된 크기의 이미지에서 무작 위로 부분 영역을 추출하여 사용함으로써 무작위성을 높일 수 있다. Global RRC는 리사이징 및 크롭을 통해 global view를 생성하기 위한 모듈이고 local RRC는 리사이징 및 크롭을 통해 local view를 생성할 수 있다. 본 개시의 일 실시예는, 입력 오디오 스펙트로그램에 대하여 2개의 global view와 4개의 local view가 생성되도록 RRC 작업이 수행될 수 있다. 상기 6개의 뷰에 대해서는 오디오의 본질이 훼손되지 않는 범위 내에서 증강이 이루어질 수 있다. 2개의 global view와 4개의 local view를 생성하는 과정에 대하여는 도 4a 내지 도 4c에서 자세히 설명한다. 도 3에서 설명한 다양한 데이터 증강 연산 부들은 증강 모듈에 모두 포함되어야 하는 것은 아니며, 일부만으로 구성될 수 있다. 또한, 증강 모듈에 데이터 증강 연산 부들이 포함된다고 하더라도 입력 데이터 셋에 따라 각각 다른 증강 연산 부들이 수행될 수 있다. 예를 들어, 제1 데이터의 경우 RFN 연산 이후 곧바로 global RRC를 진 행할 수 있다. 또한, 제2 데이터의 경우 RFN 연산 없이 self-copy를 수행하고 mixup 연산을 수행하여 local RRC 를 진행할 수도 있다. 상기와 같이, 데이터에 포함된 소정의 파라미터 값의 크기가 임계값을 넘는지 여부 또는 사전에 미리 설정해 놓은 확률에 따라 각각의 연산 부가 데이터 각각에 대하여 적용될지를 결정할 수 있다. 각 각의 연산 부들이 데이터에 임의로 적용됨에 따라 다양한 데이터 증강이 이루어질 수 있다. 도 4a 내지 도 4c는 본 개시의 일 실시예에 따라 증강된 데이터를 나타낸 것이다. 도 4a는 증강을 위한 원본 오디오 멜 스펙트로그램일 수 있고, 도 4b는 global view이고, 도 4c는 local view일 수 있다. 도 4a를 참조하면, 도 4a의 오디오 스펙트로그램은 도 2 내지 도 3의 음성 데이터와 동일한 것일 수 있다. 인공지능 알고리즘에 입력된 오디오 데이터 셋은 로그-멜 스펙트로그램 변환방식으로 오디오 스펙트로그 램으로 변환될 수 있다. 도 4b를 참조하면, 제1 global view(420a) 및 제2 global view(420b)는 도 3에서 global RRC 연산 부를 통해 생성된 것일 수 있다. Global view(420a, 420b)는 음성 데이터에 대하여 임의의 연산(예를 들어, 도 3의 RFN, self-coly, mixup 등)을 수행한 뒤 리사이징을 수행하고 Global view를 생성하기 위 한 크기로 크롭되어 생성될 수 있다. 예를 들어, Global view 연산의 부분 비율은 0.9로 설정될 수 있고 크기 조정 후 6초로 잘라질 수 있다. 도 4c를 참조하면, 제1 local view(430a), 제2 local view(430b), 제3 local view(430c), 제4 local view(430d)는 도 3에서 local RRC 연산 부를 통해 생성된 것일 수 있다. Local view(430a, 430b, 430c, 430d)는 음성 데이터에 대하여 임의의 연산(예를 들어, 도 3의 RFN, self-coly, mixup 등)을 수행한 뒤 리사이징을 수행하고 Global view를 생성하기 위한 크기로 크롭되어 생성될 수 있다. 예를 들 어, local view 연산의 부분 비율은 0.7 내지 1로 설정될 수 있고 크기 조정 후 4초로 잘라질 수 있다. 다만, 이에 한정되지 않고 설정에 따라 다양한 크기로 잘라질 수 있다. Local view(430a, 430b, 430c, 430d)는 각 뷰 에 주파수 마스킹이 임의로 적용될 수 있으며, 마스킹 범위는 고정될 수 있다. (예를 들어, 40으로 고정될 수 있다.) 도 5는 본 개시의 일 실시예에 따른 인공지능 알고리즘과 다양한 알고리즘의 음향 분류 작업 성능을 나타낸 표 이다. 본 실험을 위하여 인공지능 알고리즘을 사전 학습을 시키기 위하여 음성 데이터는 대규모 데이터 셋으로 32000Hz의 10초 길이의 오디오 약 200만개로 구성된 오디오 셋이 사용될 수 있다. 또한, 오디오 셋은 다중 레이 블 오디오 분류를 목표로 하여 음악 장르, 바람 소리, 음성, 개 짖는 소리 등 527개의 음향 및 음성 이벤트 중 해당하는 모든 클래스로 레이블이 지정될 수 있다. 오디오 클립들에 대하여 16kHz로 다운 샘플링한 후 128ms마 다 해닝 윈도우를 64ms만큼 겹치게 적용하여 푸리에 변환을 한 뒤 256 차원의 로그 스케일의 멜스펙트로그램을 추출할 수 있다. 이 때, 배치 크기는 32로 설정하고 4e-3의 모멘텀 감쇠를 사용할 수 있다. 옵티마이저로 Adam 을 채택해 학습에 사용하였으며 학습률은 10 에포크 동안 25e-4까지 워밍업된 다음 나머지 70 에포크 동안 10e- 6까지 선형 어닐링 되었다. 사전학습이 완료된 뒤 인코더의 성능 평가 및 다른 오디오 표현 모델과 비교하기 위 해 다양한 오디오 분류 작업과 관련된 데이터 셋을 다운스트림 작업으로 선정하여 비교 실험을 진행하였다. 비 교 실험 시 본 개시의 일 실시예에 따른 DINO 프레임워크로 학습된 오디오 표현의 다운스트림 작업에 대한 성능 평가를 위하여 미세 조정 방식을 채택하였다. 미세 조정은 사전 학습된 모델의 가중치를 학습 초기 가중치로 설 정한 뒤 각 다운스트림 작업에 대해 지도 학습 방식으로 학습을 진행하는 방식으로 모델이 국부 최적화에 빠지 는 것을 방지하며 수렴 속도가 빠르다는 장점이 존재한다. 평가를 위하여 3가지 데이터 셋을 활용하였다. AS-20K는 Audioset의 하위 데이터 셋이며 각 레이블이 포함하는 오디오 클립 개수가 같은 레이블별 균형을 이루는 데이터 셋으로 다중 레이블 사운드 분류를 위한 데이터 셋이다. 총 20,785개의 10초 길이의 오디오 클립 으로 구성되어 있으며, 18,886개의 오디오 클립이 포함된 AS 평가 세트를 모델 평가에 사용하였다. ESC-50은 50개의 환경 음으로 분류된 5초 길이의 오디오 클립 2,000개로 구성된다. 각 레이블 별 40개의 오디오 클립을 포함하고 있으며 전체 데이터는 5개의 폴더로 분할된다. 5-fold 교차 검증을 통해 모델의 성능을 평가했다. DCASE19는 DCASE Challenge 2019 Task 1a에 대해 단일 녹음 장치를 사용하여 수집된 TAU Urban Acoustic Scenes 2019 모바일 개발 데이터 셋을 의미한다. 이 데이터 셋은 단일 레이블 음향 장면 분류이며 음향을 녹음 한 장소를 의미하는 공항, 쇼핑몰, 공원 등의 장면 총 10곳으로 레이블이 구성되어 있다. 교육 및 평가 세트에 는 각각 9,185개 및 4,185개의 10초 길이의 오디오 클립이 포함되어 있다. 도 5를 참조하면, 모델 사전 학습에 오디오 데이터 셋만을 사용하는 In-domain과 이미지 데이터 셋을 함께 사용 한 연구인 out-of-domain으로 분류되고, 학습 방식에 따라 지도 학습의 경우 SL, 자기지도학습은 SSL로 표시하였다. 다중 레이블 분류 작업인 AS-20K의 경우 Transformer 구조의 모델을 이미지와 오디오 데이터 셋에 사 전학 습을 진행한 AST가 가장 우수한 성능을 보였다. 본 개시의 일 실시예에 따른 BC-Res2Net는 AS-20K의 상위 데이 터 셋인 Audioset에서 사전학습을 진행하였음에도 불구하고 대조군대비 낮은 성능을 보인다. 하지만 스크래치 방식에 비해 제안한 DINO 프레임워크를 사용한 학습 후 월등한 성능 향상을 보인다. 이는 대조군에 비해 매우 작은 크기의 BC-Res2Net이 효과적으로 작동하기엔 AS-20K의 크기가 클 뿐만 아니라 다중 레이블 분류가 다소 어 려운 작업이었을 것이라 예측된다. 그에 반해 단일 레이블 분류 작업의 경우 BC-Res2Net은 두 교육 체계 모두 더 적은 매개 변수로 다른 자기지도학습 모델에 비해 가장 우수한 결과를 보인다. ESC-50에서는 AS 데이터 셋을 사용하여 트랜스포머 모델을 지도학습으로 사전 훈련한 PaSST가 가장 우수한 성능을 보였으나 자기 감독을 사용하는 모델 중에서는 제안한 BC-Res2Net이 다른 모델보다 우수한 성능을 보였다. 특히 DINO 프레임워크로 사 전학습을 진행한 뒤 자기 지도학습 대조군과 유의미한 성능차이를 보이며 높은 성능을 기록했다. 스크래치 훈련 된 BC-Res2Net을 사용하여 얻은 DCASE19의 결과는 이전의 모든 지도학습, 자기지도학습 모델보다 훨씬 우 수했으며 DINO 프레임워크로 사전 훈련을 한 뒤 성능이 더욱 향상되었다. 도 6은 본 개시의 일 실시예에 따른 인공지능 알고리즘과 다양한 알고리즘의 음성 분류 작업 성능을 나타낸 표 이다. 도 6에서는 도 5에서와 똑같은 인공지능 알고리즘에 대하여 성능 비교를 위한 테스트를 진행한 것이다. 테스트 를 위하여 2개의 데이터 셋을 사용하였다. SPCV2은 Speech Commands V2 (SPCV2)는 키워드 발견을 위한 데이터 셋이다. 1초 길이의 오디오 클립으로 35개의 키워드를 인식하는 것이 주요 목적이다. 훈련 및 평가 세트에는 84,843개 및 11,005개의 오디오 클립이 포함되어 있다. VOX1는 Voxceleb-1(VOX1)은 화자 식별 및 화자 분류에 보편적으로 사용되는 데이터 셋으로 유튜브에서 추 출된 오디오 클립으로 구성된다. 가변적 길이의 오디오 클립으로 구성되며 훈련 및 평가 세트는 1,251명의 화자 로부터 각각 145,265개 및 8,251개의 발화를 추출하였다. SPCV2와 VOX1은 음성 분류 작업임에도 불구하고 대조군에 비해 BC-Res2Net의 스크래치 훈련 결과가 열등하지 않았다. 오히려 대부분 대조군에 비해 매우 작은 모델 크기를 가짐에도 불구하고 우세한 성능을 기록 했다. 이는 BC-Res2Net이 오디오 및 음성 모두에 보편적으로 사용될 수 있는 모델임을 나타낸다. 하지만 음향 오디오 분류와 다르게 음성 영역에서는 DINO 프레임워크를 통한 학습의 효과가 미미했다. 특히 화자 식별을 위 한 VOX1에 대한 성능은 DINO 프레임워크를 사용한 사전학습 후 하락하는 결과를 보였다. 위 결과를 기반으로 DINO 접근 방식은 음성이 거의 또는 전혀 포함되지 않은 오디오 데이터 셋으로 사전 훈련을 수행했기 때문에 음 성보다 오디오에 더 적합한 매개변수를 생성한 것과 주파수와 시간 축 모두에 변형을 가하는 등 데이터 증강이 음향 오디오 특성에 적합한 방법들로 구성된 것을 원인으로 분석된다. 도 7은 본 개시의 일 실시예에 따른, 녹음 장치 종류에 따라 달라지는 성능 차이를 나타낸 표이다. 도 8은 본 개시의 일 실시예에 따른 인공지능 알고리즘의 학습 여부에 따라 성능 격차를 나타낸 도면이다. 도 7 및 도 8에서 사용된 인공지능 알고리즘은 본 개시의 도 2 내지 도 6에서 설명한 인공지능 알고리즘일 수 있다. 예를 들어, 학습된 BC-Res2Net일 수 있다. 도 7은 동일한 인공지능 알고리즘에 대하여 다양한 녹음 기기들을 사용했을 때의 정확도를 나타낸 것이다. x축 은 녹음 장치들의 종류를 나타내며, y축은 정확도를 나타낼 수 있다. 여기서 실험을 위하여 DCASE 22 데이터 셋 을 사용하였다. DCASE22는 TAU Urban Acoustic Scenes 2022 Mobile 개발 데이터 셋으로 DCASE Challenge 2022 Task 1에서 사용되어 편의상 DCASE22로 표시할 수 있다. 오디오 클립은 DCASE19 데이터 셋과 같게 10가지 유형 의 음향 장면으로 레이블이 지정되며 10개 도시의 9개 장치에서 녹음되었다. 모든 오디오 클립은 1초 세그먼트 로 분할된다. 교육 및 평가 세트는 각각 139,970 및 29,680 오디오 클립으로 구성된다. 동일한 인공지능에서 Scratch 모드와 DINO를 이용하여 학습된 fine tuning으로 구분하여 실험을 진행 하였다. 여기서, 장치 A는 Soundman OKM II Klassik/studio A3, electret binaural microphone and a Zoom F8 audio recorder를 포함하며 장치 B는 Samsung Galaxy S7, 장치 C는 아이폰 SE를 이용한 것이다. 나머지 S1~ S6 은 장치 A로 녹음된 오디오 샘플의 임펄스 응답 및 추가적인 동적 범위 압출을 사용해 시뮬레이션 된 오디오 샘플일 수 있다. 도 7을 참조하면, 모든 장치들에서 스크래치 모드에서의 정확도보다 fine tuning을 사용하였을 때 성 능이 높은 것을 볼 수 있다. 따라서, DINO를 기반으로 제안한 학습 방식이 유효함을 확인할 수 있다. 도 8을 참조하면, 학습 방식 별 녹음 장비에 따른 정확도 분포를 나타낸 상자 그림을 나타낸 것이다. 스크래치 모드로 학습된 경우 정확도가 0.45에서 0.73정도까지 넓은 범위로 나타났으며, finetuning의 경우 정 확도가 0.53에서 0.74 정도까지 상대적으로 높은 수치에서 좁은 범위로 나타남을 확인할 수 있다. 이를 통해 여 러 장치에서 오디오 분류를 진행하였을 때 장치들 간의 성능 격차가 줄어들었음을 확인할 수 있다. 즉, 여러 장 치에서 기록된 레이블이 지정되지 않은 대규모 데이터 셋에 대하여 본 개시의 인공지능 알고리즘을 통해 장치들 을 일반화하는데 효과적임을 확인할 수 있다. 도 9는 본 개시의 일 실시예에 따른, 음성 분류 시스템을 위한 전자 장치에 대한 블록 구성도이다. 도 9를 참조하면, 전자 장치는 모뎀(MODEM, 920), 메모리(MEMORY, 940) 및 프로세서(PROCESSOR, 930)를 포함할 수 있다. 모뎀은 다른 전자 장치들과 전기적으로 연결되어 상호 통신이 이뤄지도록 하는 통신 모뎀일 수 있다. 특히 모뎀은 데이터 입력을 수신하여 프로세서로 전송할 수 있고, 프로세서는 입력된 데이터 값을 메 모리에 저장할 수 있다. 또한, 시스템에서 학습된 인공지능 알고리즘에 의해 출력된 데이터 값을 다른 전 자 장치로 전송할 수 있다. 메모리는 전자 장치의 동작을 위한 각종 정보 및 프로그램 명령어들이 저장되는 구성으로서, 하드 디 스크(Hard Disk), SSD(Solid State Drive) 등과 같은 기억장치일 수 있다. 특히, 메모리는 프로세서(93 0)의 제어에 의해 모뎀에서 입력되는 하나 이상의 데이터 입력 값을 저장할 수 있다. 또한, 메모리는 프로세서에 의해 실행 가능한 음성 분류를 위한 인공지능 알고리즘과 같은 프로그램 명령어들을 저장할 수 있다. 또한, 메모리는 본 개시에서 설명한 DINO, BC-Res2Net을 통해 학습된 인공지능 알고리즘과 같은 프 로그램 (또는 프로그램 명령어)를 저장할 수 있다. 프로세서는 적어도 하나의 프로세서로 구성되며, 메모리에 저장된 데이터 및 프로그램 명령어들을 이 용하여 음성 사건 검출과 관련된 인공지능 알고리즘을 학습하고 이를 활용하여 데이터를 계산할 수 있다. 프로 세서는 도 1 내지 도 8에서 설명한 모든 인공지능 알고리즘을 제어하고 계산할 수 있다. 프로세서는 이후 도 10에서 설명하는 방법에 대한 동작을 수행할 수 있다. 도 10은 본 발명의 일 실시예에 따른 인공지능을 이용한 음성 분류 방법을 설명하기 위한 순서도이다. 이하 도 10을 참조하여, 도 1 내지 도 9을 참조하여 설명한 전자 장치의 인공지능 알고리즘의 학습 동작 및 음 성 분류 방법에 대해 정리하여 설명한다. 각 동작들은 일련의 과정에서 필수적으로 포함되어야 하는 동작들은 아니며 상황에 따라 일부만이 구성되어 동작할 수 있다. 단계 S1010에서, 인공지능을 통한 음성 분류를 수행하기 위하여 음성 데이터를 수신할 수 있다. 음성 데이터는 입력 정보로 활용되어 인공지능을 학습시키는데 사용될 수 있다. 상기 음성 데이터는 레이블을 포함하지 않을 수 있다. 단계 S1020에서, 수신된 음성 데이터를 기반으로 제1 데이터 및 제2 데이터를 생성할 수 있다. 상기 제1 데이터 는 음성 데이터를 증강하여 생성한 global view이고, 상기 제2 데이터는 음성 데이터를 증강하여 생성한 local view일 수 있다. 상기 제1 데이터 및 상기 제2 데이터는 상기 음성 데이터를 기반으로 RFN(residual frequency normalization) 연산, self-copy 연산, mixup 연산 중 적어도 하나의 연산을 수행하고, 상기 연산된 음성 데이 터에 RRC(random resize crop)를 수행하여 생성될 수 있다. 상기 제1 데이터 및 상기 제2 데이터를 생성할 때, 미리 설정된 확률을 기반으로 상기 음성 데이터에 상기 RFN 연산, 상기 self-copy 연산, 상기 mixup 연산 중 적 어도 하나를 선택하여 연산을 수행하거나 또는 상기 음성 데이터에 포함된 미리 설정된 파라미터 값의 크기를 기반으로 상기 음성 데이터에 상기 RFN 연산, 상기 self-copy 연산, 상기 mixup 연산 중 적어도 하나를 선택하 여 연산을 수행할 수 있다. 단계 S1030에서, 상기 제1 데이터 및 상기 제2 데이터를 입력 정보로 하여 제1 인공지능 네트워크에서 제3 데이 터를 출력할 수 있다. 상기 제1 인공지능 네트워크는 제2 인공지능 네트워크를 기반으로 기 학습된 네트워크일수 있다. 제1 인공지능 네트워크는 헤더와 인코더로 구성될 수 있다. 상기 인코더는 BC-Res2Net일 수 있다. 여 기서 제1 인공지능 네트워크는 지식 증류 기법에서 사용되는 학생 네트워크 일 수 있다. 단계 S1040에서, 상기 제1 데이터를 입력 정보로 하여 제2 인공지능 네트워크에서 제4 데이터를 출력할 수 있다. 상기 제2 인공지능 네트워크는 헤더와 인코더로 구성될 수 있다. 상기 인코더는 BC-Res2Net일 수 있다. 여기서 제2 인공지능 네트워크는 지식 증류 기법에서 사용되는 교사 네트워크 일 수 있다. 상기 제1 인공지능 네트워크와 상기 제2 인공지능 네트워크는 동일한 구조로 구성될 수 있다. 단계 S1050에서, 상기 제3 데이터 및 상기 제4 데이터를 기반으로 상기 음성 데이터를 분류할 수 있다. 상기 제 3 데이터 및 상기 제4 데이터를 기반으로 손실을 결정하고, 상기 손실을 기반으로 상기 제1 네트워크를 학습할 수 있다. 이상, 본 발명의 기술적 사상을 다양한 실시 예들을 들어 상세하게 설명하였으나, 본 발명의 기술적 사상은 상 기 실시 예들에 한정되지 않고, 본 발명의 기술적 사상의 범위 내에서 당 분야에서 통상의 지식을 가진 자에 의 하여 여러가지 변형 및 변경이 가능하다."}
{"patent_id": "10-2023-0057610", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 발명의 상세한 설명에서 인용되는 도면을 보다 충분히 이해하기 위하여 각 도면의 간단한 설명이 제공된다. 도 1은 본 개시의 일 실시 예에 따른 인공지능 구조의 기본적인 원리를 나타낸 개념도이다. 도 2는 본 개시의 일 실시예에 따른 인공지능 네트워크의 구조를 나타낸 도면이다. 도 3은 본 개시의 일 실시예에 따른 데이터 증강 시스템의 구조를 나타낸 도면이다. 도 4a 내지 도 4c는 본 개시의 일 실시예에 따라 증강된 데이터를 나타낸 것이다. 도 5는 본 개시의 일 실시예에 따른 인공지능 알고리즘과 다양한 알고리즘의 음향 분류 작업 성능을 나타낸 표 이다. 도 6은 본 개시의 일 실시예에 따른 인공지능 알고리즘과 다양한 알고리즘의 음성 분류 작업 성능을 나타낸 표 이다. 도 7은 본 개시의 일 실시예에 따른, 녹음 장치 종류에 따라 달라지는 성능 차이를 나타낸 표이다. 도 8은 본 개시의 일 실시예에 따른 인공지능 알고리즘의 학습 여부에 따라 성능 격차를 나타낸 도면이다. 도 9는 본 개시의 일 실시예에 따른, 음성 분류 시스템을 위한 전자 장치에 대한 블록 구성도이다.도 10은 본 발명의 일 실시예에 따른 인공지능을 이용한 음성 분류 방법을 설명하기 위한 순서도이다."}
