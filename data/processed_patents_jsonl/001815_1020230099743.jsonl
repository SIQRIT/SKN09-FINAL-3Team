{"patent_id": "10-2023-0099743", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0018739", "출원번호": "10-2023-0099743", "발명의 명칭": "아바타 생성방법 및 장치", "출원인": "에스케이텔레콤 주식회사", "발명자": "김연수"}}
{"patent_id": "10-2023-0099743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "아바타 생성장치가 아바타를 생성하기 위한 컴퓨터 구현방법으로서,영상을 입력받는 과정;상기 영상을 이용하여 특징점들 및 신뢰도를 얼굴의 부위마다 각각 검출하는 과정;상기 특징점들을 이용하여 제1 표정정보를 얼굴의 부위마다 각각 생성하는 과정; 및표정정보를 생성하려는 타겟부위가 입이 아니며, 상기 타겟부위의 상기 신뢰도가 제1 임계값보다 작거나 같은경우, 상기 타겟부위와 좌우대칭인 부위(이하, “대칭부위”)의 제1 표정정보를 이용하여 상기 타겟부위의 제1최종 표정정보를 생성하는 과정을 포함하는 방법."}
{"patent_id": "10-2023-0099743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 타겟부위의 제1 최종 표정정보를 생성하는 과정은,상기 대칭부위의 신뢰도가 상기 제1 임계값보다 큰 경우, 상기 대칭부위의 제1 표정정보 및 상기 타겟부위의 제1 표정정보를 이용하여 상기 타겟부위의 제1 최종 표정정보를 생성하는 과정을 포함하는 방법."}
{"patent_id": "10-2023-0099743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 제1 최종 표정정보를 생성하는 과정은,상기 타겟부위의 신뢰도가 제2 임계값보다 작거나 같은 경우,상기 대칭부위의 제1 표정정보만을 이용하여 상기 제1 최종 표정정보를 생성하는 과정을 더 포함하는 방법."}
{"patent_id": "10-2023-0099743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 제1 최종 표정정보를 생성하는 과정은,상기 대칭부위의 신뢰도가 상기 제1 임계값보다 작거나 같은 경우,상기 타겟부위의 제1 표정정보 및 기 입력된 표정 애니메이션을 이용하여 상기 제1 최종 표정정보를 생성하는과정을 포함하는 방법."}
{"patent_id": "10-2023-0099743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서,상기 제1 최종 표정정보를 생성하는 과정은,상기 타겟부위의 신뢰도가 제3 임계값보다 작거나 같은 경우,상기 표정 애니메이션만을 이용하여 상기 제1 최종 표정정보를 생성하는 과정을 더 포함하는 방법."}
{"patent_id": "10-2023-0099743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,공개특허 10-2025-0018739-3-음성 데이터를 입력받는 과정을 더 포함하되,상기 타겟부위가 입이며, 상기 타겟부위의 신뢰도가 상기 제1 임계값보다 작거나 같은 경우, 상기 음성 데이터의 진폭을 이용하여 입 크기를 결정하는 과정; 및상기 음성 데이터의 자음 및 모음을 이용하여 입 모양을 결정하는 과정을 포함하는 제2 표정정보를 생성하는 과정; 및상기 타겟부위의 제1 표정정보 및 상기 타겟부위의 제2 표정정보를 이용하여 제2 최종 표정정보를 생성하는 과정을 더 포함하는 방법."}
{"patent_id": "10-2023-0099743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 제2 최종 표정정보를 생성하는 과정은,상기 타겟부위의 신뢰도를 제4 임계값과 비교하고,상기 타겟부위의 신뢰도가 상기 제4 임계값보다 작거나 같은 경우에는 상기 타겟부위의 제2 표정정보만을 이용하여 상기 제2 최종 표정정보를 생성하는 과정을 더 포함하는 방법."}
{"patent_id": "10-2023-0099743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항 내지 제 7항 중 어느 한 항에 있어서,상기 제1 최종 표정정보 및 상기 제2 최종 표정정보를 이용하여 아바타를 생성하는 과정을 더 포함하는 방법."}
{"patent_id": "10-2023-0099743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "명령어들을 저장하는 메모리; 및 적어도 하나의 프로세서를 포함하되,상기 적어도 하나의 프로세서는 상기 명령어들을 실행함으로써,영상 및 음성 데이터를 입력받고,상기 영상을 이용하여 복수의 특징점들 및 신뢰도를 얼굴의 부위마다 각각 검출하고,상기 얼굴의 부위 각각에 대응되는 상기 복수의 특징점들을 이용하여 제1 표정정보를 상기 얼굴의 부위마다 각각 생성하고,상기 음성 데이터를 이용하여 제2 표정정보를 생성하고,상기 신뢰도가 제1 임계값보다 낮은 부위의 표정정보는 대칭부위의 제1 표정정보 및 기 입력된 표정 애니메이션을 이용하여 생성하며,생성한 복수의 표정정보들을 이용하여 아바타를 생성하는 장치."}
{"patent_id": "10-2023-0099743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "명령어가 저장된, 컴퓨터로 읽을 수 있는 기록매체로서, 상기 명령어는 상기 컴퓨터에 의해 실행될 때 상기 컴퓨터로 하여금, 제1 항 내지 제7 항 중 어느 한 항에 따른 방법이 포함하는 각 과정을 실행하도록 하는, 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2023-0099743", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "아바타 생성방법 및 장치를 개시한다. 본 개시의 일 측면에 의하면, 아바타 생성장치가 아바타를 생성하기 위한 컴퓨터 구현방법으로서, 영상을 입력받 는 과정; 영상을 이용하여 특징점들 및 신뢰도를 얼굴의 부위마다 각각 검출하는 과정; 특징점들을 이용하여 제1 표정정보를 얼굴의 부위마다 각각 생성하는 과정; 및 표정정보를 생성하려는 타겟부위가 입이 아니며, 타겟부위 의 신뢰도가 제1 임계값보다 작거나 같은 경우, 타겟부위와 좌우대칭인 부위(이하, “대칭부위”)의 제1 표정정 보를 이용하여 타겟부위의 제1 최종 표정정보를 생성하는 과정을 포함하는 방법을 제공한다."}
{"patent_id": "10-2023-0099743", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 아바타 생성방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0099743", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이하에 기술되는 내용은 단순히 본 실시예와 관련되는 배경 정보만을 제공할 뿐 종래기술을 구성하는 것이 아니 다. 사용자의 얼굴 표정을 영상 등을 이용하여 인식하고, 이를 기반으로 아바타의 표정을 만드는 것을 리타게팅 (retargeting)이라 한다. 종래의 리타게팅 방법은, 다양한 블렌드쉐입(blendshape)을 미리 만들어 저장해 두고, 사용자의 얼굴 표정을 실시간으로 인식하여 이에 맞는 복수의 블렌드쉐입들을 불러와 합성함으로써 아바타의 표 정을 만드는 것이다. 블렌드쉐입을 이용한 리타게팅 방법은 얼굴의 각 부위마다 복수의 블렌드쉐입들을 미리 만 들어 저장함으로써 얼굴 각 부위의 움직임에 따른 표정 변화를 묘사할 수 있다. 그러나 종래의 방법으로는, 얼굴의 부위 일부가 가려진 경우에, 가려진 부위의 표정을 인식할 수 없다. 따라서, 가려진 부위의 표정에 부합하는 블렌드쉐입을 불러올 수 없다. 도 1은 얼굴의 부위 일부가 가려진 예를 도시한 것이다. 도 1을 참조하면, 다양한 이유 때문에 얼굴의 부위 일부가 촬영되지 않은 영상이 표정검출장치에 입력될 수 있 다. 예컨대, 선글라스를 착용한 경우 양쪽 눈 부위의 표정을 검출할 수 없다. 또 다른 예로는, 영상을 촬 영하는 카메라의 각도 또는 사용자의 복장 때문에 입 및 입꼬리와 같은 부분이 영상에 나타나지 않을 수 있다 . 또 다른 예로는, 카메라를 너무 근접하여 영상을 촬영하는 경우, 오른쪽 눈과 같은 일부 부위가 영상에 나타나지 않아, 해당 부위의 표정을 검출하지 못할 수 있다. 최근의 리타게팅 기술은 사용자의 변화하는 표정을 실시간으로 아바타에 반영한다. 그러나, 사용자가 핸드폰 카 메라 등의 영상촬영장치를 항상 고정된 장소에 위치시키지 않는다. 또한, 사용자 자신의 위치 역시 고정되어 있 지 않다. 리타게팅 기술이 다양한 분야에 적용됨으로써 다양한 장소, 다양한 복장의 사용자의 표정을 인식하고 아바타를 생성할 필요성이 있다. 따라서, 얼굴의 일부 부위가 가려지더라도 표정을 인식하고 아바타를 생성하는 기술이 요구된다."}
{"patent_id": "10-2023-0099743", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는, 얼굴의 일부 부위가 가려지거나 표정이 인식되지 않더라도 이를 보정하여 아바타를 생성할 수 있는 방법 및 장치를 제공하는 데 주된 목적이 있다. 본 발명이 해결하고자 하는 과제들은 이상에서 언급한 과제들로 제한되지 않으며, 언급되지 않은 또 다른 과제 들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0099743", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면에 의하면, 아바타 생성장치가 아바타를 생성하기 위한 컴퓨터 구현방법으로서, 영상을 입력 받는 과정; 영상을 이용하여 특징점들 및 신뢰도를 얼굴의 부위마다 각각 검출하는 과정; 특징점들을 이용하여 제1 표정정보를 얼굴의 부위마다 각각 생성하는 과정; 및 표정정보를 생성하려는 타겟부위가 입이 아니며, 타겟 부위의 신뢰도가 제1 임계값보다 작거나 같은 경우, 타겟부위와 좌우대칭인 부위(이하, “대칭부위”)의 제1 표 정정보를 이용하여 타겟부위의 제1 최종 표정정보를 생성하는 과정을 포함하는 방법을 제공한다."}
{"patent_id": "10-2023-0099743", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 실시예에 의하면, 음성 데이터 및 대칭되는 부위의 표정을 이용하여, 얼굴의 일부 부위가 가려지거나 표정이 인식되지 않더라도 이를 보정하여 아바타를 생성할 수 있는 방법 및 장치를 제공할 수 있다는 효과가 있 다. 본 개시의 효과들은 이상에서 언급한 효과들로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재 로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0099743", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 일부 실시예들을 예시적인 도면을 이용해 상세하게 설명한다. 각 도면의 구성 요소들에 참조 부호를 부가함에 있어서, 동일한 구성 요소들에 대해서는 비록 다른 도면 상에 표시되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 개시를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략한다. 본 개시에 따른 실시예의 구성요소를 설명하는 데 있어서, 제1, 제2, i), ii), a), b) 등의 부호를 사용할 수 있다. 이러한 부호는 그 구성요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 부호에 의해 해당 구성요소의 본질 또는 차례나 순서 등이 한정되지 않는다. 명세서에서 어떤 부분이 어떤 구성요소를 '포함' 또는 '구비'한 다고 할 때, 이는 명시적으로 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 첨부된 도면과 함께 이하에 개시될 상세한 설명은 본 개시의 예시적인 실시형태를 설명하고자 하는 것이며, 본 개시가 실시될 수 있는 유일한 실시형태를 나타내고자 하는 것이 아니다. 이하에 사용된 단수형의 용어는 달리 명시되지 않는 한 복수형을 포함할 수 있다. 도 2는 본 개시의 일 실시예에 따른, 아바타 생성장치를 개략적으로 도시한 블록구성도이다. 한편, 도 2에 도시 된 구성요소들은 기능적으로 구분되는 요소들을 나타낸 것으로서, 적어도 하나의 구성요소가 실제 물리적 환경 에서는 서로 통합되는 형태로 구현될 수도 있다. 도 2를 참조하면, 아바타 생성장치(avatar generator, 20)는 입력부(input unit, 200), 검출부(detecting unit, 210), 생성부(generating unit, 220) 및 아바타부(avatar unit, 230)를 전부 또는 일부 포함한다. 입력부는 영상 및 음성 데이터를 입력받는다. 예를 들면, 입력부는 사용자 단말에 포함된 카메라를 이용하여 영상 데이터를 입력받을 수 있다. 입력부는 사용자 단말에 포함된 마이크를 이용하여 사용자의 음성 데이터를 입력받을 수 있다. 입력부가 영상 및 음성 데이터를 입력받기 위한 수단은 다양할 수 있으 며, 반드시 예시로서 설명된 수단만이 사용되는 것은 아니다. 검출부는 입력부가 입력받은 사용자의 얼굴로부터 특징점(feature point)을 검출한다. 검출부는 기계학습(machine learning) 모델을 이용하여 사용자의 얼굴로부터 특징점을 검출할 수 있다. 검출부가 이 용하는 기계학습 모델은 딥 러닝(deep learning) 모델을 포함할 수 있다. 검출부가 이용하는 기계학습 모 델은 CNN(convolutional neural network) 또는 R-CNN(regions with convolutional neural network)과 같은 인 공신경망(artificial neural network)을 포함할 수 있다. 검출부는 얼굴의 각 부위에 대응하는 특징점을 검출할 수 있다. 예컨대, 검출부는 오른쪽 눈(right eye), 왼쪽 눈(left eye), 오른쪽 눈썹(right eyebrow), 왼쪽 눈썹(left eyebrow), 코(nose), 콧구멍 (nostril), 입(mouth), 입술(lips), 오른쪽 입꼬리(right corner of mouth) 및 왼쪽 입꼬리(left corner of mouth) 중 선택된 적어도 하나 이상의 부위에 대응하는 특징점을 검출할 수 있다. 얼굴의 각 부위에 대응하는 특징점은 복수 개일 수 있다. 예컨대, 9 개의 특징점이 오른쪽 눈에 대응할 수 있고, 12 개의 특징점이 입술의 모양에 대응할 수 있다. 한편, 각 부위에 대응하는 특징점의 개수는 설명을 위한 예시일 뿐 반드시 이에 국한되 는 것은 아니다. 검출부는 특징점마다 각각 신뢰도(reliability)를 산출할 수 있다. 신뢰도는 0 내지 1 사이(between 0 and 1)의 수치로서 표현된다. 검출부는 얼굴 각 부위에 대응하는 적어도 하나 이상의 특징점을 이용하여 얼굴 각 부위의 신뢰도를 산출할 수 있다. 예컨대, 오른쪽 눈의 9 개의 특징점의 각 신뢰도를 이용하여 오른쪽눈의 신뢰도를 산출할 수 있다. 오른쪽 눈의 신뢰도는 수학식 1과 같이 계산할 수 있다. 수학식 1"}
{"patent_id": "10-2023-0099743", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, 은 오른쪽 눈의 신뢰도를, 는 오른쪽 눈에 대응하는 특징점들 각각의 신뢰도를, 은 오른쪽 눈동자에 대응하는 특징점의 신뢰도를 의미한다. 생성부는 표정정보(facial expression information)를 생성한다. 표정정보는 얼굴의 각 부위(each part of face)가 기본표정(default facial expression)에 비해 변경된 정도를 포함하는 정보이다. 여기서, 기본표정 은 양쪽 눈을 뜬 무표정한 얼굴(default face)을 말한다. 예컨대, 사용자가 오른쪽 눈만을 감은 경우, 표정정보 는 기본 표정에 비해 오른쪽 눈이 완전히 감겼다는 정보를 포함한다. 표정정보는 각 부위에 대응하는 특징점을 이용하여 표현될 수 있다. 표정정보는 각 특징점이 기본 표정에 비해 이동한 정도를 나타내는 정보일 수 있다. 도 3은 본 개시의 일 실시예에 따른 표정정보를 나타내는 예시적인 방법을 도시한 것이다. 도 3에 도시된 것과 같이, 표정정보는 얼굴 각 부위마다 어떤 표정을 짓고 있는지를 0 내지 1 사이(between 0 and 1)의 수치로 나타낸 정보일 수 있다. 예를 들어, 사용자가 오른쪽 눈을 완전히 감은 경우, '오른쪽 눈 감기' 표정의 수치가 1로서 나타날 수 있다. 사용자가 왼쪽 입꼬리를 올리며 웃는 경우, '왼쪽 입꼬리 올 림' 표정의 수치가 1로서 나타날 수 있다. 생성부는 제1 생성부(first generating unit, 222), 제2 생성부(second generating unit, 224), 및 제3 생성부(third generating unit, 226) 를 전부 또는 일부 포함할 수 있다. 제1 생성부는 검출부가 검출한 특징점을 이용하여 제1 표정정보(first facial expression information)를 생성한다. 제1 표정정보는 검출부가 검출한 특징점을 이용하여 생성한 표정정보를 의미한 다. 다시 말해, 제1 표정정보는 입력부가 입력한 영상 데이터를 이용하여 생성된 표정정보이다. 제1 생성 부는 얼굴 각 부위의 특징점을 이용하여, 얼굴의 각 부위마다 제1 표정정보를 생성한다. 예컨대, 제1 표정 정보는 제1 생성부가 특징점을 이용하여 생성한 입의 표정정보, 오른쪽 눈의 표정정보 및 왼쪽 눈의 표정 정보를 포함할 수 있다. 제1 생성부는 특징점을 기계학습 모델에 입력하여 표정을 인식함으로써 제1 표정 정보를 생성할 수 있다. 제1 생성부는 얼굴 각 부위의 신뢰도를 제1 임계값(first threshold)과 각각 비교할 수 있다. 여기서 제1 임계값은, 해당 부위의 제1 표정정보를 신뢰할 수 있을지 판단하기 위한 임계값이다. 표정정보를 생성하려는 얼굴의 부위(이하, “타겟부위”, target part)의 신뢰도가 제1 임계값보다 크다면, 제1 생성부는 타겟부위의 제1 표정정보를 신뢰할 수 있다고 판단한다. 제1 생성부는 제1 표정정보를 최종 표정정보(final facial expression information)로서 생성한다. 타겟부위의 신뢰도가 제1 임계값보다 낮다면, 제1 생성부는 제1 표정정보를 제2 생성부 및 제3 생성부 에게 전달한다. 제1 생성부는 타겟부위가 입인지 여부를 판단한다. 타겟부위가 입이 아닌 경우, 제1 생성부는 제1 표 정정보를 제2 생성부에게 전달한다. 타겟부위가 입인 경우, 제1 생성부는 제1 표정정보를 제3 생성부 에게 전달한다. 제1 생성부는 신뢰도가 제1 임계값보다 큰 얼굴 부위만을 이용하여 사용자의 감정을 추출할 수 있다. 제1 생성부는 기계학습 모델을 이용하여 사용자의 감정을 추출할 수 있다. 제1 생성부는 얼굴 각 부위의 특징점 및 제1 표정정보를 입력받아 사용자의 감정을 추출하도록 기 훈련된 인공지능 모델을 포함할 수 있다. 타겟부위가 입이 아닌 경우, 제2 생성부는 신뢰도가 제1 임계값보다 작거나 같은 타겟부위의 최종 표정정 보를 생성할 수 있다. 제2 생성부는 얼굴이 좌우 대칭인 점을 이용하여 최종 표정정보를 생성할 수 있다. 일 실시예에 따르면, 사용자의 얼굴의 오른쪽 눈이 카메라가 촬영한 영상에 나타나지 않을 수 있다. 이 경우, 카메라가 촬영한 영상에 나타나지 않은 오른쪽 눈의 신뢰도가 낮아, 영상으로부터 표정정보를 생성하는 것이 어렵다. 제2 생성부 는 신뢰도가 낮은 부위와 대칭되는 부위의 표정정보를 이용하여 신뢰도가 낮은 부위의 표정정보를 생성할 수 있다. 제2 생성부는 타겟부위와 대칭인 부위(이하, “대칭부위”, symmetrical part)의 신뢰도를 제1 임계값과 비교할 수 있다. 예컨대, 오른쪽 눈의 표정정보를 생성하기 위해 왼쪽 눈의 신뢰도를 제1 임계값과 비교할 수 있다. 대칭부위의 신뢰도가 제1 임계값보다 큰 경우, 제2 생성부는 타겟부위의 신뢰도를 제2 임계값(second threshold)과 비교한다. 여기서 제2 임계값은, 타겟부위의 표정정보와 대칭부위의 표정정보를 혼합하여 최종 표 정정보를 생성할지 판단하기 위한 임계값이다. 타겟부위의 신뢰도가 제2 임계값보다 작거나 같은 경우, 제2 생성부는 대칭부위의 제1 표정정보를 최종 표 정정보로서 생성한다. 즉, 타겟부위의 표정정보를 타겟부위의 최종 표정정보에 반영하지 않는다. 타겟부위의 신뢰도가 제2 임계값보다 큰 경우, 제2 생성부는 대칭부위의 제1 표정정보 및 타겟부위의 제1 표정정보를 이용하여 최종 표정정보를 생성한다. 즉, 타겟부위 및 대칭부위의 표정정보를 둘 다 반영하여 타겟 부위의 최종 표정정보를 생성한다. 대칭부위의 제1 표정정보 및 타겟부위의 제1 표정정보를 이용하여 최종 표정 정보를 생성하는 과정은 수학식 2와 같다. 수학식 2"}
{"patent_id": "10-2023-0099743", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, 여기서 는 타겟부위의 최종 표정정보를, 은 타겟부위의 제1 표정정보를, 은 대칭부위의 제 1 표정정보를 의미한다. 및 는 각각 타겟부위 및 대칭부위의 신뢰도를 의미한다. 대칭부위의 신뢰도가 제1 임계값보다 작거나 같은 경우, 즉 대칭부위의 표정정보를 신뢰할 수 없는 경우, 제2 생성부는 기 입력된 표정 애니메이션 및 타겟부위의 제1 표정정보를 이용하여 타겟부위의 최종 표정정보를 생성한다. 표정 애니메이션은 감정 상태마다 표정을 미리 정의하고, 정의된 표정들을 애니메이션으로 생성한 것 이다. 다시 말해, 정의된 감정 상태마다 대응하는 표정 애니메이션이 존재할 수 있다. 제2 생성부는 제1 생성부가 추출한 사용자의 감정에 대응하는 표정 애니메이션을 선택할 수 있다. 제2 생성부는 선택된 표정 애니메이션의 얼굴 부위들 전부 또는 일부를 이용하여 타겟부위의 최종 표정정보를 생성할 수 있다. 제2 생성부는 타겟부위의 신뢰도를 제3 임계값(third threshold)과 비교할 수 있다. 여기서, 제3 임계값이 란 타겟부위의 표정정보를 표정 애니메이션과 혼합할지 여부를 판단하기 위한 임계값이다. 타겟부위의 신뢰도가 제3 임계값보다 작거나 같은 경우, 제2 생성부는 타겟부위의 표정 애니메이션만을 이 용하여 최종 표정정보를 생성한다. 즉, 타겟부위의 표정정보를 반영하지 않는다. 타겟부위의 신뢰도가 제3 임계값보다 큰 경우, 제2 생성부는 타겟부위의 제1 표정정보 및 표정 애니메이션 을 이용하여 최종 표정정보를 생성한다. 타겟부위의 신뢰도가 제3 임계값보다 큰 경우 최종 표정정보를 생성하 는 과정은 수학식 3과 같다. 수학식 3 여기서, 및 는 각각 타겟부위의 제1 표정정보 및 타겟부위의 최종 표정정보를 의미한다. 는 선택된 얼굴 부위의 신뢰도를 의미한다. 는 선택된 표정 애니메이션을 의미한다. 정리하자면, 타겟부위의 신뢰도가 제1 임계값보다 큰 경우, 타겟부위의 제1 표정정보를 타겟부위의 최종 표정정 보로서 생성한다. 타겟부위의 신뢰도가 제1 임계값보다 작거나 같고 제2 임계값보다 크고, 대칭부위의 신뢰도가 제1 임계값보다 큰 경우, 대칭부위 및 타겟부위의 제1 표정정보를 이용하여 타겟부위의 최종 표정정보를 생성한다. 타겟부위의 신뢰도가 제1 임계값보다 작거나 같고 제2 임계값보다 작거나 같으며, 대칭부위의 신뢰도가 제1 임 계값보다 큰 경우, 대칭부위의 제1 표정정보를 타겟부위의 최종 표정정보로서 생성한다. 타겟부위의 신뢰도가 제1 임계값보다 작거나 같고 제3 임계값보다 크고, 대칭부위의 신뢰도가 제1 임계값보다 낮은 경우, 타겟부위의 제1 표정정보 및 기 입력된 표정 애니메이션을 이용하여 타겟부위의 최종 표정정보를 생 성한다. 타겟부위의 신뢰도가 제1 임계값보다 작거나 같고 제3 임계값보다 작거나 같으며, 대칭부위의 신뢰도가 제1 임 계값보다 낮은 경우, 기 입력된 표정 애니메이션을 이용하여 타겟부위의 최종 표정정보를 생성한다. 제3 생성부는 타겟부위가 입이고, 타겟부위의 신뢰도가 제1 임계값보다 작거나 같은 경우 최종 표정정보를 생성한다. 제3 생성부는 음성 데이터를 이용하여 제2 표정정보(second facial expression information)를 생성한다. 제2 표정정보란, 음성 데이터만을 사용하여 생성된, 타겟부위가 입인 표정정보이다. 제2 표정정보는 입의 제1 표정정보와 다를 수 있다. 제3 생성부는 음성 데이터를 분석할 수 있다. 제3 생성부는 음성 데이터를 분석하여 음성의 주파수, 음성의 진폭, 및 음성의 자음 및 모음을 각각 출력한다. 제3 생성부는 음성의 자음 및 모음을 이용하여 제 2 표정정보를 생성한다. 제3 생성부는 음성의 진폭을 이용하여 제2 표정정보를 생성한다. 예컨대, 제3 생 성부는 음성의 진폭과 입의 크기가 비례하도록 제2 표정정보를 생성할 수 있다. 제3 생성부는 예컨대, 립싱크(lipsync) 기능을 사용하여 제2 표정정보를 생성할 수 있다. 립싱크 기능은 음성을 입력하면 음성에 맞는 입 모양을 생성하는 기능이다. 립싱크 기능은 공지된 것이므로, 이에 대해 자세한 설명은 생략한다. 제3 생성부는 제4 임계값(fourth threshold)과 입 부위의 신뢰도를 비교한다. 여기서 제4 임계값은, 입 부 위의 제1 표정정보를 최종 표정정보에 반영할지 여부를 판단하기 위한 임계값이다. 입 부위의 신뢰도가 제4 임계값보다 작거나 같은 경우, 제3 생성부는 제2 표정정보를 최종 표정정보로서 생성한다. 입 부위의 신뢰도가 제4 임계값보다 큰 경우, 제3 생성부는 제1 표정정보 및 제2 표정정보를 이용하여 최 종 표정정보를 생성한다. 제1 표정정보 및 제2 표정정보를 이용하여 최종 표정정보를 생성하는 과정은 수학식 4 와 같다. 수학식 4"}
{"patent_id": "10-2023-0099743", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, , , 은 각각 제1 표정정보, 제2 표정정보 및 최종 표정정보를 의미한다. 은 입 부위의 신뢰도를 의미한다. 제3 생성부는 최종 표정정보를 아바타부에게 전송한다. 제1 임계값, 제2 임계값, 제3 임계값 및 제4 임계값은 사용자가 임의로 입력한 값일 수 있다. 제1 임계값 내지 제4 임계값 중 전부 또는 일부가 서로 동일할 수 있다. 제1 임계값 내지 제4 임계값 중 선택된 어느 임계값들이서로 동일하다면, 일부 표정정보를 생성하기 위한 과정이 수행되지 않을 수 있다. 제1 임계값 내지 제4 임계값은 서로 다른 값일 수 있다. 제1 임계값 내지 제4 임계값은 0 내지 1 사이(between 0 and 1)의 값을 갖는다. 다시 말해, 제1 임계값 내지 제4 임계값은 0 이상 1 이하의 값을 가질 수 있다. 제1 임계값은 제2 임계값, 제3 임계값 및 제4 임계값보다 큰 값일 수 있다. 다시 말해, 제1 임계값은 임계값 중 가장 큰 값일 수 있다. 도 4는 본 개시의 일 실시예에 따른 아바타부가 아바타를 생성하는 과정을 도시한 것이다. 본 개시에 따른 아바타는 예컨대, 3D 아바타(three-dimensional avatar)를 포함할 수 있다. 도 4를 참조하면, 아바타부는 얼굴의 모든 부위의 최종 표정정보를 각각 입력받아 아바타를 생성한다. 아 바타부는 블렌드쉐입을 이용하여 아바타를 생성할 수 있다. 예컨대, 아바타부는 각 부위의 타겟 블렌 드쉐입을 결합하여 아바타를 생성할 수 있다. 여기서, 타겟 블렌드쉐입이란, 최종 표정정보와 가장 근접한 표정 정보를 가지는 블렌드쉐입을 말한다. 아바타부는 입력받은 표정정보를 이용하여 각 부위마다 타겟 블렌드쉐입을 선택한다. 사용자가 활짝 웃으면서 오른쪽 눈을 윙크하고 있다는 정보가 최종 표정정보들에 포함되어 있다면, 아바타부는 왼쪽 입꼬 리를 올린 타겟 블렌드쉐입, 오른쪽 입꼬리를 올린 타겟 블렌드쉐입, 입을 벌린 타겟 블렌드쉐입 및 오른쪽 눈을 감은 타겟 블렌드쉐입을 선택할 수 있다. 아바타부는 선택한 타겟 블렌드쉐입 들(410, 420, 430 및 440)을 결합하여 아바타를 생성한다. 아바타부는 무표정한 얼굴의 아바타를 생성하고, 미리 생성되어 저장된 블렌드쉐입을 불러와 생성한 아바 타와 합성함으로써 아바타의 표정을 생성할 수 있다. 아바타부는 서버(미도시) 또는 메모리(미도시)에 저 장된 블렌드쉐입을 불러와 아바타와 합성할 수 있다. 또는, 아바타부는 무표정한 얼굴의 아바타를 생성하고, 최종 표정정보를 이용하여 아바타의 표정을 변형하 는 방법으로 아바타의 표정을 생성할 수 있다. 도 5는 본 개시의 일 실시예에 따른, 타겟부위가 입이 아닌 경우, 영상을 이용하여 아바타를 생성하는 과정을 도시한 순서도이다. 도 5를 참조하면, 입력부는 영상을 입력받는다. 검출부는 입력부로부터 수신한 영상 데이터로부터 얼굴 각 부위를 인식하고, 특징점들을 검출한다. 여기서, 특징점들은 얼굴 각 부위에 각각 대응될 수 있다. 검출부는 검출한 특징점들의 신뢰도를 각각 구 할 수 있다. 검출부는 각 특징점의 신뢰도를 이용하여, 얼굴 각 부위의 신뢰도를 계산할 수 있다. 제1 생성부는 타겟부위를 선택한다(S500). 타겟부위는 예컨대, 오른쪽 눈썹, 왼쪽 눈썹, 오른쪽 눈, 왼쪽 눈, 오른쪽 콧구멍, 왼쪽 콧구멍, 오른쪽 입꼬리, 및 왼쪽 입꼬리 중 선택된 하나의 부위일 수 있으나 반드시 이에 한하지 않는다. 제1 생성부는 검출한 특징점을 이용하여, 타겟부위의 제1 표정정보를 생성한다 (S500). 제1 생성부는 타겟부위의 신뢰도를 제1 임계값과 비교한다(S510). 타겟부위의 신뢰도가 제1 임계값보다 크 다면, 제1 생성부는 표정정보 생성을 완료한다. 즉, 타겟부위의 제1 표정정보를 최종 표정정보로서 생성한 다(S512). 타겟부위의 신뢰도가 제1 임계값보다 작거나 같은 경우, 제2 생성부는 대칭부위의 신뢰도를 제1 임계값과 비교한다(S520). 대칭부위의 신뢰도가 제1 임계값보다 큰 경우, 제2 생성부는 타겟부위의 신뢰도를 제2 임계값과 비교한다 (S530). 타겟부위의 신뢰도가 제2 임계값보다 큰 경우, 제2 생성부는 타겟부위의 제1 표정정보 및 대칭부위의 제1 표정정보를 이용하여 최종 표정정보를 생성한다(S532). 타겟부위의 신뢰도가 제2 임계값보다 작거나 같은 경우, 제2 생성부는 타겟부위의 제2 표정정보를 생성한 다(S534). 대칭부위의 신뢰도가 제1 임계값보다 작거나 같은 경우, 제2 생성부는 타겟부위의 신뢰도를 제3 임계값과 비교한다(S540). 타겟부위의 신뢰도가 제3 임계값보다 큰 경우, 제2 생성부는 표정 애니메이션과 타겟부위의 제1 표정정보 를 이용하여 최종 표정정보를 생성한다(S542). 타겟부위의 신뢰도가 제3 임계값보다 작거나 같은 경우, 제2 생성부는 표정 애니메이션을 이용하여 최종 표정정보를 생성한다(S544). 아바타부는 각 부위마다 생성된 최종 표정정보를 전달받는다. 아바타부는 모든 부위의 최종 표정정보 가 생성되고, 전달되었는지 확인한다(S550). 아바타부는 최종 표정정보가 생성되지 않은 부위가 있다면, 최종 표정정보가 생성되지 않은 부위를 타겟부위로 선택하여 표정정보를 생성하도록 제1 생성부에게 명령 할 수 있다. 다시 말해, 모든 부위의 최종 표정정보가 생성될 때까지 S500 내지 S550 과정을 반복한다. 여기서, 아바타부는 도 6에 도시된, 입 부위의 최종 표정정보를 함께 전달받고, 이를 이용하여 아바타를 생성할 수 있다. 모든 부위의 최종 표정정보가 생성되었다면, 아바타부는 전달받은 최종 표정정보를 이용하여 아바타를 생 성한다(S560). 다시 말해, 모든 부위의 최종 표정정보를 이용하여 각 부위의 표정을 만들고, 모든 부위의 표정 을 결합하여 아바타를 생성한다. 도 6은 본 개시의 일 실시예에 따른, 타겟부위가 입인 경우 영상 및 음성 데이터를 이용하여 아바타를 생성하는 과정을 도시한 순서도이다. 도 6을 참조하면, 입력부는 영상 및 음성 데이터를 입력받는다. 검출부는 입력부로부터 수신한 영상 데이터로부터 입 부위를 인식하고, 특징점들을 검출한다. 검출부 는 검출한 특징점들의 신뢰도를 각각 구할 수 있다. 검출부는 각 특징점의 신뢰도를 이용하여, 입 부 위의 신뢰도를 계산할 수 있다. 제1 생성부는 검출한 특징점을 이용하여, 입 부위의 제1 표정정보를 생성한다(S600). 제1 생성부는 입 부위의 신뢰도를 제1 임계값과 비교한다(S610). 입 부위의 신뢰도가 제1 임계값보다 크다면, 제1 생성부는 표정정보 생성을 완료한다. 즉, 제1 표정정보를 입 부위의 최종 표정정보로서 생성한다(S612). 입 부위의 신뢰도가 제1 임계값보다 작거나 같다면, 제1 생성부는 제1 표정정보를 제3 생성부에게 전 달한다. 제3 생성부는 음성 데이터를 이용하여 제2 표정정보를 생성한다(S620). 제3 생성부는 검출부가 계산한 입 부위의 신뢰도를 제4 임계값과 비교한다(S630). 입 부위의 신뢰도가 제4 임계값보다 크다면, 제3 생성부는 제1 표정정보 및 제2 표정정보를 이용하여 최종 표정정보를 생성한다(S632). 입 부위의 신뢰도가 제4 임계값보다 작거나 같다면, 제3 생성부는 제2 표정정보를 이용하여 최종 표정정보 를 생성한다(S634). 아바타부는 입 부위의 최종 표정정보를 전달받는다. 본 발명에 따른 장치 또는 방법의 각 구성요소는 하드웨어 또는 소프트웨어로 구현되거나, 하드웨어 및 소프트 웨어의 결합으로 구현될 수 있다. 또한, 각 구성요소의 기능이 소프트웨어로 구현되고 마이크로프로세서가 각 구성요소에 대응하는 소프트웨어의 기능을 실행하도록 구현될 수도 있다. 본 명세서에 설명되는 시스템들 및 기법들의 다양한 구현예들은, 디지털 전자 회로, 집적회로, FPGA(field programmable gate array), ASIC(application specific integrated circuit), 컴퓨터 하드웨어, 펌웨어, 소프 트웨어, 및/또는 이들의 조합으로 실현될 수 있다. 이러한 다양한 구현예들은 프로그래밍가능 시스템 상에서 실 행 가능한 하나 이상의 컴퓨터 프로그램들로 구현되는 것을 포함할 수 있다. 프로그래밍가능 시스템은, 저장 시 스템, 적어도 하나의 입력 디바이스, 그리고 적어도 하나의 출력 디바이스로부터 데이터 및 명령들을 수신하고 이들에게 데이터 및 명령들을 전송하도록 결합되는 적어도 하나의 프로그래밍가능 프로세서(이것은 특수 목적 프로세서일 수 있거나 혹은 범용 프로세서일 수 있음)를 포함한다. 컴퓨터 프로그램들(이것은 또한 프로그램들, 소프트웨어, 소프트웨어 애플리케이션들 혹은 코드로서 알려져 있음)은 프로그래밍가능 프로세서에 대한 명령어들을 포함하며 \"컴퓨터가 읽을 수 있는 기록매체\"에 저장된다. 컴퓨터가 읽을 수 있는 기록매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기 록장치를 포함한다. 이러한 컴퓨터가 읽을 수 있는 기록매체는 ROM, CD-ROM, 자기 테이프, 플로피디스크, 메모 리 카드, 하드 디스크, 광자기 디스크, 스토리지 디바이스 등의 비휘발성(non-volatile) 또는 비일시적인(non- transitory) 매체일 수 있으며, 또한 데이터 전송 매체(data transmission medium)와 같은 일시적인 (transitory) 매체를 더 포함할 수도 있다. 또한, 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수도 있다. 본 명세서의 흐름도/타이밍도에서는 각 과정들을 순차적으로 실행하는 것으로 기재하고 있으나, 이는 본 개시의 일 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것이다. 다시 말해, 본 개시의 일 실시예가 속하는 기 술 분야에서 통상의 지식을 가진 자라면 본 개시의 일 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 흐 름도/타이밍도에 기재된 순서를 변경하여 실행하거나 각 과정들 중 하나 이상의 과정을 병렬적으로 실행하는 것으로 다양하게 수정 및 변형하여 적용 가능할 것이므로, 흐름도/타이밍도는 시계열적인 순서로 한정되는 것은 아니다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2023-0099743", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 얼굴의 부위 일부가 가려진 예를 도시한 것이다. 도 2는 본 개시의 일 실시예에 따른 아바타 생성장치를 개략적으로 도시한 블록구성도이다. 도 3은 본 개시의 일 실시예에 따른 표정정보를 나타내는 예시적인 방법을 도시한 것이다. 도 4는 본 개시의 일 실시예에 따른 아바타부가 아바타를 생성하는 과정을 도시한 것이다. 도 5는 본 개시의 일 실시예에 따른, 타겟부위가 입이 아닌 경우, 영상을 이용하여 아바타를 생성하는 과정을 도시한 순서도이다. 도 6은 본 개시의 일 실시예에 따른, 타겟부위가 입인 경우 영상 및 음성 데이터를 이용하여 아바타를 생성하는 과정을 도시한 순서도이다."}
