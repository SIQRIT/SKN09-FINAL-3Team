{"patent_id": "10-2019-0090417", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0012528", "출원번호": "10-2019-0090417", "발명의 명칭": "감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치 및 방법", "출원인": "주식회사 모두커뮤니케이션", "발명자": "이효진"}}
{"patent_id": "10-2019-0090417", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자의 안면을 촬영한 사용자 영상을 수신하고, 안면 형태에 따라 복수의 서로 다른 감정 상태를 구분하도록학습된 상태의 딥러닝 알고리즘에 상기 사용자 영상을 적용하여 하나 이상의 감정 상태를 식별하는 감정판단부;상기 사용자 영상과 이모티콘을 합성하여 상기 사용자 영상에 나타나는 상기 사용자의 안면 구조 및 안면 변화가 반영되어 동적으로 움직이는 이모티콘인 객체에 대한 객체 영상을 생성하는 합성부;사용자 입력에 따른 텍스트를 수신하여 상기 텍스트를 음성으로 변환하는 변환부; 및상기 합성부로부터 수신한 객체 영상에 대응되어 상기 변환부로부터 음성을 수신하고, 상기 사용자 영상에 대응되어 상기 감정 판단부를 통해 식별된 감정 상태별로 미리 설정된 음성 패턴에 따라 상기 음성을 조절한 후 상기 객체 영상에 합성하여 객체 정보를 생성하는 제어부를 포함하는 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치."}
{"patent_id": "10-2019-0090417", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 제어부는 상기 음성 패턴에 따라 상기 음성의 톤 또는 주파수를 조절하는 것을 특징으로 하는 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치."}
{"patent_id": "10-2019-0090417", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서,상기 제어부는 상기 객체 영상에 상기 음성을 합성하기 이전에 상기 객체 영상과 음성을 상호 동기화하며, 상기감정 판단부와 연동하여 상기 동기화에 따른 상기 객체 영상 및 음성 각각에서 상기 감정 판단부에 의해 식별된특정 감정 상태에 매칭되는 특정 시간 구간을 확인하고, 상기 음성에서 상기 특정 시간 구간에 해당하는 영역에상기 특정 감정 상태에 대응되어 미리 설정된 음성 패턴을 적용하여 상기 음성을 조절하는 것을 특징으로 하는감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치."}
{"patent_id": "10-2019-0090417", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 3에 있어서,상기 제어부는 상기 감정 판단부에서 상기 사용자 영상에 대응되어 복수의 서로 다른 감정 상태를 식별한 경우상기 식별된 감정 상태별로 상기 음성에서 매칭되는 시간 구간의 영역을 확인하고, 상기 확인에 따른 상기 음성을 구성하는 복수의 서로 다른 영역별로 영역에 매칭되는 감정 상태에 대응되어 미리 설정된 음성 패턴을 적용하여 상기 음성을 조절하는 것을 특징으로 하는 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치."}
{"patent_id": "10-2019-0090417", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 3에 있어서,상기 복수의 서로 다른 감정 상태 중 미리 설정된 고유 감정 상태에서 고유 이벤트를 실행하도록 설정된 이모티공개특허 10-2021-0012528-3-콘이 하나 이상 복수의 서로 다른 감정 상태별로 매칭되어 저장된 저장부를 더 포함하고,상기 제어부는 상기 감정 판단부에 의해 식별된 감정 상태에 대응되는 하나 이상의 이모티콘 중 특정 이모티콘을 랜덤 선택하거나 사용자 입력에 따라 선택하고, 상기 합성부를 통해 상기 특정 이모티콘을 상기 사용자 영상과 합성시켜 상기 특정 이모티콘에 설정된 감정 상태에 대응되는 시간 구간에서 상기 특정 이벤트가 실행되는상기 객체에 대한 객체 영상을 생성하는 것을 특징으로 하는 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치."}
{"patent_id": "10-2019-0090417", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치의 서비스 제공 방법에 있어서,사용자의 안면을 촬영한 사용자 영상을 수신하고, 안면 형태에 따라 복수의 서로 다른 감정 상태를 구분하도록학습된 상태의 딥러닝 알고리즘에 상기 사용자 영상을 적용하여 하나 이상의 감정 상태를 식별하는 감정 판단단계;상기 사용자 영상과 이모티콘을 합성하여 상기 사용자 영상에 나타나는 상기 사용자의 안면 구조 및 안면 변화가 반영되어 동적으로 움직이는 이모티콘인 객체에 대한 객체 영상을 생성하는 합성 단계;사용자 입력에 따른 텍스트를 수신하여 상기 텍스트를 음성으로 변환하는 변환 단계; 및상기 사용자 영상에 대응되어 상기 감정 식별 단계를 통해 식별된 감정 상태별로 미리 설정된 음성 패턴에 따라상기 음성을 조절한 후 상기 객체 영상에 합성하여 객체 정보를 생성하는 객체 생성 단계를 포함하는 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 방법."}
{"patent_id": "10-2019-0090417", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치 및 방법에 관한 것으로서, 더욱 상세히는 사용자의 안면 인식을 통한 영상과 이모티콘을 합성하여 사용자의 얼굴에 나타나는 사용자의 다양한 감정을 표출 하기 위한 객체를 생성하고, 상기 영상에 대한 분석을 통해 식별된 사용자의 감정 상태에 따라 조절된 음성을 상 (뒷면에 계속)"}
{"patent_id": "10-2019-0090417", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치 및 방법에 관한 것으로서, 더욱 상세히는 사용자의 안면 인식을 통한 영상과 이모티콘을 합성하여 사용자의 얼굴에 나타나는 사용자의 다양한 감정을 표 출하기 위한 객체를 생성하고, 상기 영상에 대한 분석을 통해 식별된 사용자의 감정 상태에 따라 조절된 음성을 상기 객체 영상과 합성하여 개인화된 객체를 제공할 수 있는 감정이 반영된 개인화 객체 생성을 위한 서비스 제 공 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2019-0090417", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "현재 딥러닝 기반의 인공지능 발달과 더불어 사용자를 촬영한 영상에서 사용자를 인식하고, 사용자의 영상을 딥 러닝 알고리즘에 학습시켜 사용자의 영상으로부터 다양한 감정 상태를 구분 및 식별할 수 있는 장치가 제공되고 있다. 그러나, 기존의 장치는 감정 상태의 구분에 그치고 이를 활용한 사례가 존재하지 않아 사용자의 영상에 기반하 여 감정 상태를 구분하는 알고리즘에 대한 활용성이 떨어지는 문제가 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국공개특허 제10-2015-0092591호"}
{"patent_id": "10-2019-0090417", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상술한 문제를 해결하기 위해, 본 발명은 사용자의 안면을 촬영한 사용자 영상과 이모티콘을 합성하여 사용자의 감정 상태에 대한 사용자의 안면 구조 및 안면 변화가 반영된 이모티콘인 객체 관련 객체 영상을 생성한 후 사용자 입력에 따른 텍스트를 음성으로 변환하여 상기 객체 영상과 합성하되 상기 사용자 영상으로부터 사용자의 감정 상태를 식별하여 사용자의 감정 상태에 대응되는 음성 패턴으로 상기 음성을 조절한 후 상기 객체 영상과 합성함으로써 사용자의 감정 상태가 영상과 음성으로 반영되며 사용자의 아바타로 사용 가능한 객체를 생성하여 제공하는데 그 목적이 있다."}
{"patent_id": "10-2019-0090417", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시예에 따른 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치는, 사용자의 안면을 촬영한 사용자 영상을 수신하고, 안면 형태에 따라 복수의 서로 다른 감정 상태를 구분하도록 학습된 상태의 딥러닝 알 고리즘에 상기 사용자 영상을 적용하여 하나 이상의 감정 상태를 식별하는 감정 판단부와, 상기 사용자 영상과 이모티콘을 합성하여 상기 사용자 영상에 나타나는 상기 사용자의 안면 구조 및 안면 변화가 반영되어 동적으로 움직이는 이모티콘인 객체에 대한 객체 영상을 생성하는 합성부와, 사용자 입력에 따른 텍스트를 수신하여 상기 텍스트를 음성으로 변환하는 변환부 및 상기 합성부로부터 수신한 객체 영상에 대응되어 상기 변환부로부터 음 성을 수신하고, 상기 사용자 영상에 대응되어 상기 감정 판단부를 통해 식별된 감정 상태별로 미리 설정된 음성 패턴에 따라 상기 음성을 조절한 후 상기 객체 영상에 합성하여 객체 정보를 생성하는 제어부를 포함할 수 있다. 본 발명과 관련된 일 예로서, 상기 제어부는 상기 음성 패턴에 따라 상기 음성의 톤 또는 주파수를 조절하는 것 을 특징으로 할 수 있다. 본 발명과 관련된 일 예로서, 상기 제어부는 상기 객체 영상에 상기 음성을 합성하기 이전에 상기 객체 영상과 음성을 상호 동기화하며, 상기 감정 판단부와 연동하여 상기 동기화에 따른 상기 객체 영상 및 음성 각각에서 상기 감정 판단부에 의해 식별된 특정 감정 상태에 매칭되는 특정 시간 구간을 확인하고, 상기 음성에서 상기 특정 시간 구간에 해당하는 영역에 상기 특정 감정 상태에 대응되어 미리 설정된 음성 패턴을 적용하여 상기 음 성을 조절하는 것을 특징으로 할 수 있다. 본 발명과 관련된 일 예로서, 상기 제어부는 상기 감정 판단부에서 상기 사용자 영상에 대응되어 복수의 서로 다른 감정 상태를 식별한 경우 상기 식별된 감정 상태별로 상기 음성에서 매칭되는 시간 구간의 영역을 확인하 고, 상기 확인에 따른 상기 음성을 구성하는 복수의 서로 다른 영역별로 영역에 매칭되는 감정 상태에 대응되어 미리 설정된 음성 패턴을 적용하여 상기 음성을 조절하는 것을 특징으로 할 수 있다. 본 발명과 관련된 일 예로서, 상기 복수의 서로 다른 감정 상태 중 미리 설정된 고유 감정 상태에서 고유 이벤 트를 실행하도록 설정된 이모티콘이 하나 이상 복수의 서로 다른 감정 상태별로 매칭되어 저장된 저장부를 더 포함하고, 상기 제어부는 상기 감정 판단부에 의해 식별된 감정 상태에 대응되는 하나 이상의 이모티콘 중 특정 이모티콘을 랜덤 선택하거나 사용자 입력에 따라 선택하고, 상기 합성부를 통해 상기 특정 이모티콘을 상기 사 용자 영상과 합성시켜 상기 특정 이모티콘에 설정된 감정 상태에 대응되는 시간 구간에서 상기 특정 이벤트가 실행되는 상기 객체에 대한 객체 영상을 생성하는 것을 특징으로 할 수 있다. 본 발명의 실시예에 따른 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치의 서비스 제공 방법은, 사 용자의 안면을 촬영한 사용자 영상을 수신하고, 안면 형태에 따라 복수의 서로 다른 감정 상태를 구분하도록 학 습된 상태의 딥러닝 알고리즘에 상기 사용자 영상을 적용하여 하나 이상의 감정 상태를 식별하는 감정 판단 단 계와, 상기 사용자 영상과 이모티콘을 합성하여 상기 사용자 영상에 나타나는 상기 사용자의 안면 구조 및 안면 변화가 반영되어 동적으로 움직이는 이모티콘인 객체에 대한 객체 영상을 생성하는 합성 단계와, 사용자 입력에 따른 텍스트를 수신하여 상기 텍스트를 음성으로 변환하는 변환 단계 및 상기 사용자 영상에 대응되어 상기 감 정 식별 단계를 통해 식별된 감정 상태별로 미리 설정된 음성 패턴에 따라 상기 음성을 조절한 후 상기 객체 영 상에 합성하여 객체 정보를 생성하는 객체 생성 단계를 포함할 수 있다."}
{"patent_id": "10-2019-0090417", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 사용자의 안면을 촬영한 사용자 영상과 이모티콘을 합성하여 상기 사용자 영상에 나타나는 사용자의 얼굴 구조와 얼굴 변화가 반영된 이모티콘에 대한 객체 관련 객체 영상을 생성하고, 사용자 입력에 따른 텍스트 를 기반으로 생성된 음성을 상기 사용자 영상에 대한 분석을 통해 사용자의 안면(얼굴)을 기초로 식별된 사용자 의 감정 상태에 대응되는 음성 패턴에 따라 조절한 후 상기 객체 영상과 합성하여 사용자의 감정 상태를 객체를 통해 영상과 음성으로 표현할 수 있도록 지원하면서 상기 객체를 사용자의 아바타로서 사용할 수 있도록 지원하 는 효과가 있다.또한, 본 발명은 메신저 어플리케이션이나 게시판과 같은 웹 페이지를 통해 상기 객체 정보를 등록하거나 상기 객체 정보를 타 사용자 단말로 전송하여 타 사용자에게 사용자의 감정 상태를 영상과 음성으로 상기 객체를 통 해 표현할 수 있도록 지원하는 효과가 있다."}
{"patent_id": "10-2019-0090417", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참고하여 본 발명의 상세 실시예를 설명한다. 도 1은 본 발명의 실시예에 따른 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치(이하, 서비스 제공 장치)에 대한 구성도이며, 도 2는 본 발명의 실시예에 따른 서비스 제공 장치의 동작 순서도이다. 도시된 바와 같이, 상기 서비스 제공 장치는 감정 판단부, 합성부, 변환부, 저장부 및 제어부를 포함하여 구성될 수 있다. 이때, 상기 서비스 제공 장치는 통신망을 통해 사용자 단말과 통신하는 서버로 구성될 수 있으며, 이를 위 해 상기 서비스 제공 장치는 통신망을 통해 다른 장치 및 단말과 통신하는 통신부를 포함할 수 있다. 여기서, 상기 통신망은 널리 알려진 다양한 유무선 통신방식이 적용될 수 있다. 또는, 상기 서비스 제공 장치는 상기 사용자 단말의 제어 모듈에 의해 실행되는 어플리케이션 (application)과 같은 소프트웨어 형태로 구성되거나 상기 제어 모듈로 구성될 수도 있다. 우선, 상기 감정 판단부는 사용자의 안면을 촬영한 사용자 영상을 수신할 수 있다(S1). 이때, 상기 감정 판단부는 상기 서비스 제공 장치가 서버로 구성되는 경우 상기 사용자 단말로부터 통신망을 통해 상기 사용자 영상을 수신할 수 있으며, 상기 서비스 제공 장치가 사용자 단말에 구성되는 경우 상기 사용자 단말에 구성된 카메라부로부터 상기 사용자 영상을 수신할 수 있다. 또한, 상기 감정 판단부에는 딥러닝(Deep learning) 알고리즘이 미리 설정될 수 있으며, 상기 딥러닝 알고 리즘은 하나 이상의 신경망 모델로 구성될 수 있다. 이때, 상기 신경망 모델(또는 신경망)은 입력층(Input Layer), 하나 이상의 은닉층(Hidden Layers) 및 출력층 (Output Layer)으로 구성될 수 있으며, 상기 신경망 모델에는 DNN(Deep Neural Network), RNN(Recurrent Neural Network), CNN(Convolutional Neural Network) 등과 같은 다양한 종류의 신경망이 적용될 수 있다. 또한, 상기 감정 판단부는 사용자의 안면을 촬영한 영상을 상기 딥러닝 알고리즘에 학습시켜 상기 영상에 나타나는 안면 형태에 따라 복수의 서로 다른 감정 상태를 구분하도록 학습시킬 수 있다. 이때, 상기 딥러닝 알고리즘은 상기 영상에서 사용자의 안면을 인식하고, 상기 안면에서 복수의 특징점을 추출 하여 상기 복수의 특징점의 분포 패턴(또는 배치 패턴)에 따라 복수의 서로 다른 감정 상태를 구분하고 특정 감 정 상태를 식별하도록 학습될 수 있다. 또한, 상기 감정 판단부는 안면 형태에 따라 복수의 서로 다른 감정 상태를 구분하도록 학습 완료된 상기 딥러닝 알고리즘에 사용자의 안면을 촬영한 사용자 영상을 적용하여 하나 이상의 감정 상태를 식별할 수 있다 (S2). 이러한 감정 상태의 일례로, 중립, 행복, 슬픔, 분노, 놀람, 역겨움 등을 포함할 수 있다. 한편, 상기 저장부는 이모티콘이 저장될 수 있으며, 복수의 서로 다른 감정 상태별로 하나 이상의 이모티 콘이 매칭되어 저장될 수 있다. 이때, 상기 이모티콘은 이모티콘 정보일 수 있으며, 상기 이모티콘 정보는 이모 티콘의 명칭, 이모티콘의 종류, 이모티콘과 매칭되는 하나 이상의 감정 상태에 대한 정보, 사용자 영상에서 식 별된 얼굴 영역과의 합성 대상인 상기 이모티콘의 위치(또는 영역) 등을 포함할 수 있다. 이때, 상기 이모티콘은 이미지, 플래시(Flash), 동영상 클립(clip) 등과 같은 다양한 형태로 구성될 수 있다. 또한, 상기 합성부는 상기 사용자 영상과 이모티콘을 미리 설정된 합성 알고리즘에 따라 합성하여 상기 사 용자 영상에 나타나는 상기 사용자의 안면 구조 및 안면 변화가 반영되어 동적으로 움직이는 이모티콘인 객체에 대한 객체 영상을 생성할 수 있다(S3). 일례로, 상기 합성부는 상기 사용자 영상에서 사용자의 안면에 해당하는 얼굴 영역을 인식하고 상기 얼굴 영역에서 상기 합성 알고리즘에 따라 추출한 복수의 특징점의 분포 상태(배치 상태)에 대한 안면 구조를 상기 이모티콘에서 상기 이모티콘 정보를 기초로 식별된 안면에 해당하는 영역에 적용하여 합성할 수 있으며, 상기 사용자 영상의 재생에 따라 변화되는 상기 특징점의 분포 상태 변화(배치 상태 변화)인 상기 안면 변화를 상기 이모티콘에 적용 및 합성하여 상기 사용자의 안면 구조 및 안면 변화가 반영되어 동적으로 움직이는 이모티콘으 로 구성된 객체에 대한 객체 영상을 생성할 수 있다. 이때, 상기 합성부는 상기 사용자 영상을 구성하는 복수의 프레임별로 상기 이모티콘과 상술한 바와 같이 합성하여 상기 사용자의 안면 변화를 나타내는 복수의 연속된 상기 객체 관련 프레임을 생성하고, 상기 복수의 연속된 객체 관련 프레임을 기초로 상기 객체 영상을 생성할 수 있다. 또한, 상기 변환부는 상기 사용자 영상에 대응되어 사용자 입력에 따른 텍스트를 수신할 수 있다. 일례로, 상기 변환부는 상기 서비스 제공 장치가 서버로 구성된 경우 상기 사용자 단말로부터 상기 사용자 입력에 따라 생성된 텍스트를 수신할 수 있으며, 상기 서비스 제공 장치가 상기 사용자 단말에 구 성된 경우 상기 사용자 단말에 구성되어 사용자 입력을 수신하는 사용자 입력부를 통해 수신된 입력 정보를 기 초로 상기 제어 모듈에 의해 생성된 상기 텍스트를 상기 제어 모듈로부터 수신할 수 있다. 이때, 상기 서비스 제공 장치가 상기 제어 모듈인 경우 상기 변환부는 상기 사용자 입력부로부터 수 신된 입력정보를 기초로 상기 텍스트를 생성할 수도 있다. 또한, 상기 변환부는 상기 텍스트를 음성으로 변환할 수 있으며, 이를 위해 상기 변환부는 TTS(Text To Speech) 관련 알고리즘이 미리 설정되어 상기 TTS 관련 알고리즘을 통해 상기 텍스트를 음성으로 변환할 수 있으며, 해당 음성을 상기 제어부에 제공할 수 있다(S4). 한편, 상기 제어부는 상기 서비스 제공 장치에 구성된 각 구성부를 제어하여 상기 서비스 제공 장치 의 전반적인 제어 기능을 수행할 수 있으며, 상기 제어부는 RAM, ROM, CPU, GPU, 버스를 포함할 수 있으며, RAM, ROM, CPU, GPU 등은 버스를 통해 서로 연결될 수 있다. 이때, 상기 서비스 제공 장치에 구성되는 구성부들 중 적어도 하나가 다른 하나에 포함되어 구성될 수 있 다. 또한, 상기 제어부는 상기 합성부로부터 상기 사용자 영상에 대응되어 생성된 상기 객체 영상을 수신 할 수 있다. 또한, 상기 제어부는 상기 변환부로부터 상기 사용자 영상에 대응되어 상기 텍스트 기반으로 생성된 음성을 수신할 수 있다. 또한, 상기 제어부는 상기 사용자 영상에 대응되어 상기 감정 판단부를 통해 식별된 하나 이상의 감 정 상태별로 미리 설정된 음성 패턴에 따라 상기 음성을 조절할 수 있다(S5). 또한, 상기 제어부는 상기 음성 조절 이후 상기 객체 영상에 음성 조절된 상기 음성을 미리 설정된 알고리 즘에 따라 합성하여 객체 정보를 생성할 수 있으며, 상기 객체 정보를 제공할 수 있다(S6). 일례로, 상기 제어부는 상기 객체 정보를 상기 사용자 단말로 통신망을 통해 전송하거나 상기 사용자 단말 에 구성되어 각종 정보를 표시하는 표시부를 통해 상기 객체 정보를 출력하여 제공할 수 있다. 또한, 상기 제어부는 상기 음성 패턴에 따라 상기 음성의 톤(tone) 또는 주파수를 조절할 수 있으며, 이를 통해 상기 감정 판단부에서 식별된 감정 상태에 따른 감정이 상기 음성에 반영되도록 하여 상기 객체 영상 과 함께 상기 음성 출력시 상기 감정이 상기 객체 영상의 객체에 나타나는 표정과 음성을 통해 표출되도록 할 수 있다. 상술한 구성을 통해, 상기 제어부는 사용자의 안면을 촬영한 사용자 영상과 이모티콘을 합성하여 상기 사 용자 영상에 나타나는 사용자의 얼굴 구조와 얼굴 변화가 반영된 이모티콘에 대한 객체 관련 객체 영상을 생성 하고, 사용자 입력에 따른 텍스트를 기반으로 생성된 음성을 상기 사용자 영상에 대한 분석을 통해 사용자의 안면(얼굴)을 기초로 식별된 사용자의 감정 상태에 대응되는 음성 패턴에 따라 조절한 후 상기 객체 영상과 합성 하여 사용자의 감정 상태를 객체를 통해 영상과 음성으로 표현할 수 있도록 지원하면서 상기 객체를 사용자의 아바타(avatar)로서 사용할 수 있도록 지원할 수 있다. 또한, 상기 제어부는 메신저 어플리케이션이나 게시판과 같은 웹 페이지를 통해 상기 객체 정보를 등록하 거나 상기 객체 정보를 타 사용자 단말로 전송하여 타 사용자에게 사용자의 감정 상태를 영상과 음성으로 상기 객체를 통해 표현할 수 있도록 지원할 수 있다. 도 3은 상술한 구성에 따른 서비스 제공 장치의 동작 예시도로서, 도시된 바와 같이, 상기 제어부는 상기 객체 영상에 상기 음성을 합성하기 이전에 상기 객체 영상과 음성을 상호 동기화할 수 있다. 이때, 상기 객체 영상은 상기 사용자의 안면을 촬영한 사용자 영상과 동일한 재생 시간을 가질 수 있으며, 상기 객체 영상과 사용자 영상 사이의 재생 시간이 서로 상이한 경우 상기 제어부는 상기 객체 영상과 사용자 영상을 서로 매칭되는 프레임을 기준으로 상호 동기화시킬 수도 있다. 또한, 상기 제어부는 상기 감정 판단부와 연동하여 상기 동기화에 따른 상기 객체 영상 및 음성 각각 에서 상기 감정 판단부에 의해 식별된 특정 감정 상태에 매칭되는 특정 시간 구간을 확인할 수 있다. 또한, 상기 제어부는 상기 음성에서 상기 특정 시간 구간에 해당하는(매칭되는) 영역(신호 영역 또는 신호 구간)에 상기 특정 감정 상태에 대응되어 미리 설정된 음성 패턴을 적용하여 상기 음성을 조절할 수 있다. 이때, 상기 제어부는 상기 음성에서 상기 특정 시간 구간에 해당되는 신호 영역에 속한 신호들을 상기 음 성 패턴에 따라 조절함으로써 상기 음성을 조절할 수 있다. 일례로, 도시된 바와 같이, 상기 제어부는 상기 객체 영상과 음성이 상호 동기화된 상태에서 상기 감정 판 단부에 의해 상기 사용자 영상에서 식별된 '분노'에 해당되는 특정 감정 상태가 나타나는 특정 시간 구간 과 동일한 시간 구간을 상기 음성에서 확인하고, 상기 음성에서 상기 특정 시간 구간과 동일한 시간 구간에 대 응되는 특정 영역(특정 신호 영역 또는 특정 신호 구간)에 상기 '분노'의 감정 상태에 대응되어 미리 설정된 음 성 패턴을 적용하여 상기 특정 영역의 음성 패턴을 상기 음성의 다른 영역과 상이하게 조절할 수 있다. 이때, 본 발명에서 설명하는 음성 패턴은 가중치일 수도 있으며, 복수의 서로 다른 감정 상태 상호 간 서로 다 른 음성 패턴(또는 가중치)이 상기 제어부에 미리 설정될 수 있다. 일례로, 상기 제어부는 상기 음성에서 상기 '분노'의 감정 상태에 대응되는 음성 패턴에 따라 상기 음성의 특정 영역을 조절하여 상기 특정 영역에 속한 음성 신호의 톤을 상승시킬 수 있으며, 상기 음성 재생시 상기 특 정 영역에 해당되는 음성 부분이 강조되어 출력되도록 할 수 있다. 상술한 바와 마찬가지로, 상기 제어부는 상기 감정 판단부에서 상기 사용자 영상에 대응되어 복수의 서로 다른 감정 상태를 식별한 경우 상기 식별된 감정 상태별로 상기 음성에서 매칭되는 시간 구간의 영역을 확 인하고, 상기 확인에 따른 상기 음성을 구성하는 복수의 서로 다른 영역(신호 영역 또는 신호 구간)별로 영역에 매칭되는 감정 상태에 대응되어 미리 설정된 음성 패턴을 적용하여 상기 음성을 조절할 수 있다. 이를 통해, 상기 제어부는 음성의 시간 구간별로 서로 상이한 감정 상태의 음성 패턴을 적용하여 음성을 통해 다양한 감정 상태가 표출되도록 지원할 수 있다. 한편, 상기 저장부에는 상기 복수의 서로 다른 감정 상태 중 미리 설정된 고유 감정 상태에서 고유 이벤트 를 실행하도록 설정된 이모티콘이 하나 이상 복수의 서로 다른 감정 상태별로 매칭되어 저장될 수 있다. 이때, 상기 고유 이벤트의 일례로서, 이모티콘의 특정 동작과 같은 애니메이션 효과 등을 포함할 수 있다. 또한, 상기 제어부는 상기 감정 판단부에 의해 식별된 감정 상태에 대응되는 하나 이상의 이모티콘 중 특정 이모티콘을 랜덤 선택하거나 사용자 입력에 따라 선택하고, 상기 합성부를 통해 상기 특정 이모티 콘을 상기 사용자 영상과 합성시켜 상기 특정 이모티콘에 설정된 감정 상태에 대응되는 시간 구간에서 상기 특 정 이벤트가 실행되는 상기 객체에 대한 객체 영상을 생성할 수 있으며, 상기 객체 영상을 기초로 상기 객체 정 보를 생성할 수 있다. 본 명세서에 기술된 다양한 장치 및 구성부는 하드웨어 회로(예를 들어, CMOS 기반 로직 회로), 펌웨어, 소프트 웨어 또는 이들의 조합에 의해 구현될 수 있다. 예를 들어, 다양한 전기적 구조의 형태로 트랜지스터, 로직게이 트 및 전자회로를 활용하여 구현될 수 있다. 전술된 내용은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자라면 본 발명의 본질적인 특성에서 벗어 나지 않는 범위에서 수정 및 변형이 가능할 것이다. 따라서, 본 발명에 개시된 실시예들은 본 발명의 기술 사상 을 한정하기 위한 것이 아니라 설명하기 위한 것이고, 이러한 실시예에 의하여 본 발명의 기술 사상의 범위가 한정되는 것은 아니다. 본 발명의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 발명의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2019-0090417", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치에 대한 구성도. 도 2는 본 발명의 실시예에 따른 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치의 동작 순서도. 도 3은 본 발명의 실시예에 따른 감정이 반영된 개인화 객체 생성을 위한 서비스 제공 장치의 동작 예시도."}
