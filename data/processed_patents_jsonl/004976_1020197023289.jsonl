{"patent_id": "10-2019-7023289", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0150624", "출원번호": "10-2019-7023289", "발명의 명칭": "다중 센서 및 인공지능에 기반하여 맵을 생성하고 맵을 이용하여 주행하는 로봇", "출원인": "엘지전자 주식회사", "발명자": "어규호"}}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇을 이동시키는 이동부; 상기 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 라이다 프레임을 생성하는 라이다 센서;상기 로봇의 외부에 배치된 사물을 촬영하여 비주얼 프레임을 생성하는 카메라 센서; 상기 하나 이상의 라이다 프레임을 포함하는 라이다 브랜치와 상기 하나 이상의 비주얼 프레임을 포함하는 비주얼 브랜치와 상기 라이다 프레임 또는 상기 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상포함하는 백본으로 구성된 포즈 그래프를 생성하며, 상기 프레임 노드 사이를 상기 로봇이 이동하여 생성한 오도메트리 정보를 생성하는 제어부; 및상기 라이다 브랜치, 상기 비주얼 브랜치, 상기 백본, 상기 프레임 노드 사이의 오도메트리 정보 및 상기 포즈그래프를 저장하는 맵 저장부를 포함하는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 제어부는 이전 프레임 노드와 현재 위치 사이의 거리 또는 각도에 기반하여 새로운 프레임 노드를 생성하며, 상기 제어부는 현재 위치에서 등록 가능한 상기 비주얼 프레임 또는 상기 라이다 프레임 중 어느 하나 이상을상기 맵 저장부에서 검색하여 상기 생성한 새로운 프레임 노드에 등록시키는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 프레임 노드에 등록된 비주얼 프레임은 상기 제어부가 다수의 비주얼 프레임들 중에서 인접한 비주얼 프레임들과 거리, 각도 및 비주얼 프레임 내의 특징점의 수에 따라 선택한 비주얼 키프레임이며,상기 비주얼 브랜치는 상기 다수의 비주얼 프레임 및 상기 비주얼 프레임 중 선택된 하나 이상의 비주얼 키프레임을 포함하는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 프레임 노드에 등록된 라이다 프레임은상기 제어부가 다수의 라이다 프레임들 중에서 제1라이다 키프레임과 오버랩하는 크기에 따라 선택한 제2라이다키프레임이며, 상기 라이다 브랜치는 상기 다수의 라이다 프레임 및 상기 라이다 프레임 중 선택된 하나 이상의 라이다 키프레임을 포함하는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 로봇은 사용자로부터 정보를 입력받는 인터페이스부를 더 포함하며, 상기 인터페이스부가 프레임 노드의 추가를 입력받으면,상기 제어부가 현재 위치에서 프레임 노드를 생성하고 상기 생성한 프레임 노드에 등록 가능한 비주얼 프레임공개특허 10-2021-0150624-3-또는 라이다 프레임을 검색하여 상기 프레임 노드에 등록시키는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 제어부는 상기 포즈 그래프에서 상기 백본을 구성하는 프레임 노드에 등록된 라이다 프레임을 제거한 비주얼 전용 포즈 그래프를 생성하는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 제어부는 상기 포즈 그래프에서 상기 백본을 구성하는 프레임 노드에 등록된 비주얼 프레임을 제거한 라이다 전용 포즈 그래프를 생성하는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "로봇을 이동시키는 이동부; 상기 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 제1라이다 프레임을 생성하는 라이다 센서;상기 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성하는 카메라 센서; 상기 제1라이다 프레임과 비교 가능한 다수의 라이다 프레임들을 포함하는 라이다 브랜치를 저장하고, 상기 제1비주얼 프레임과 비교 가능한 다수의 비주얼 프레임들을 포함하는 비주얼 브랜치를 저장하며, 상기 저장된 라이다 프레임 또는 상기 저장된 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프를 저장하며, 상기 프레임 노드 사이의 오도메트리 정보를 저장하는 맵 저장부; 상기 포즈 그래프의 프레임 노드에 등록된 프레임과 상기 제1라이다 프레임 또는 상기 제1비주얼 프레임을 비교하거나 또는 상기 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산하는 제어부를 포함하는, 다중 센서 및인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 제어부는 상기 백본에서 현재 위치에 대응하는 제1프레임 노드를 검색하여 상기 제1프레임 노드에 등록된제2라이다 프레임을 추출하며, 상기 제어부가 상기 제1라이다 프레임과 상기 제2라이다 프레임을 비교한 결과 상기 두 프레임이 상이한 경우,상기 제어부는 상기 라이다 브랜치에서 상기 제2라이다 프레임에 인접한 제3라이다 프레임을 추출하여 상기 제1라이다 프레임과 상기 제3라이다 프레임을 비교하여 로봇의 현재 위치를 계산하는, 다중 센서 및 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 제어부는 상기 제1라이다 프레임과 상기 제3라이다 프레임을 비교한 결과 상기 두 프레임이 상이한 경우, 상기 제어부는 상기 제1프레임 노드에 등록된 제2비주얼 프레임을 추출하며, 상기 제어부가 상기 제1비주얼 프레임과 상기 제2비주얼 프레임을 비교하여 로봇의 현재 위치를 계산하는, 다중센서 및 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항에 있어서, 상기 제어부는 상기 백본에서 현재 위치에 대응하는 제1프레임 노드를 검색하여 상기 제1프레임 노드에 등록된제2비주얼 프레임을 추출하며, 공개특허 10-2021-0150624-4-상기 제어부가 상기 제1비주얼 프레임과 상기 제2비주얼 프레임을 비교한 결과 상기 두 프레임이 상이한 경우,상기 제어부는 상기 비주얼 브랜치에서 상기 제2비주얼 프레임에 인접한 제3비주얼 프레임을 추출하여 상기 제1비주얼 프레임과 상기 제3비주얼 프레임을 비교하여 로봇의 현재 위치를 계산하는, 다중 센서 및 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 제어부는 상기 제1비주얼 프레임과 상기 제3비주얼 프레임을 비교한 결과 상기 두 프레임이 상이한 경우, 상기 제어부는 상기 제1프레임 노드에 등록된 제2라이다 프레임을 추출하며, 상기 제어부가 상기 제1라이다 프레임과 상기 제2라이다 프레임을 비교하여 로봇의 현재 위치를 계산하는, 다중센서 및 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서, 상기 포즈 그래프를 구성하는 프레임 노드 중 제1프레임 노드는 라이다 프레임 또는 비주얼 프레임이 등록되지않은 프레임 노드이며, 상기 제어부는 상기 로봇의 현재 위치를 상기 제1프레임 노드를 기준으로 추정하며, 상기 제어부는 로봇이 이동했던 경로 중 라이다 프레임 또는 비주얼 프레임이 등록된 제2프레임 노드을 추출하여 상기 제2프레임 노드로부터 상기 제1프레임 노드까지의 오도메트리 정보를 이용하여 상기 로봇의 현재 위치를 계산하는, 다중 센서 및 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "로봇을 이동시키는 이동부; 상기 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 제1라이다 프레임을 생성하는 라이다 센서;상기 제1라이다 프레임과 비교 가능한 다수의 라이다 프레임들을 포함하는 라이다 브랜치를 저장하고, 상기 저장된 라이다 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프를 저장하며, 상기 프레임 노드 사이의 오도메트리 정보를 저장하는 맵 저장부; 상기 포즈 그래프의 프레임 노드에 등록된 프레임과 상기 제1라이다 프레임을 비교하거나 또는 상기 오도메트리정보를 이용하여 로봇의 현재 위치를 계산하는 제어부를 포함하는, 인공지능에 기반하여 생성된 맵을 이용하여주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 상기 제어부는 상기 백본에서 현재 위치에 대응하는 제1프레임 노드를 검색하여 상기 제1프레임 노드에 등록된제2라이다 프레임을 추출하며, 상기 제어부가 상기 제1라이다 프레임과 상기 제2라이다 프레임을 비교한 결과 상기 두 프레임이 상이한 경우,상기 제어부는 상기 라이다 브랜치에서 상기 제2라이다 프레임에 인접한 제3라이다 프레임을 추출하여 상기 제1라이다 프레임과 상기 제3라이다 프레임을 비교하여 로봇의 현재 위치를 계산하는, 인공지능에 기반하여 생성된맵을 이용하여 주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서, 상기 포즈 그래프를 구성하는 프레임 노드 중 제1프레임 노드는 라이다 프레임이 등록되지 않은 프레임 노드이며, 공개특허 10-2021-0150624-5-상기 제어부는 상기 로봇의 현재 위치를 상기 제1프레임 노드를 기준으로 추정하며, 상기 제어부는 로봇이 이동했던 경로 중 라이다 프레임이 등록된 제2프레임 노드을 추출하여 상기 제2프레임 노드로부터 상기 제1프레임 노드까지의 오도메트리 정보를 이용하여 상기 로봇의 현재 위치를 계산하는, 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "로봇을 이동시키는 이동부; 상기 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성하는 카메라 센서; 상기 제1비주얼 프레임과 비교 가능한 다수의 비주얼 프레임들을 포함하는 비주얼 브랜치를 저장하고, 상기 저장된 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프를 저장하며, 상기 프레임 노드 사이의 오도메트리 정보를 저장하는 맵 저장부; 상기 포즈 그래프의 프레임 노드에 등록된 프레임과 상기 제1비주얼 프레임을 비교하거나 또는 상기 오도메트리정보를 이용하여 로봇의 현재 위치를 계산하는 제어부를 포함하는, 인공지능에 기반하여 생성된 맵을 이용하여주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서, 상기 제어부는 상기 백본에서 현재 위치에 대응하는 제1프레임 노드를 검색하여 상기 제1프레임 노드에 등록된제2비주얼 프레임을 추출하며, 상기 제어부가 상기 제1비주얼 프레임과 상기 제2비주얼 프레임을 비교한 결과 상기 두 프레임이 상이한 경우,상기 제어부는 상기 비주얼 브랜치에서 상기 제2비주얼 프레임에 인접한 제3비주얼 프레임을 추출하여 상기 제1비주얼 프레임과 상기 제3비주얼 프레임을 비교하여 로봇의 현재 위치를 계산하는, 인공지능에 기반하여 생성된맵을 이용하여 주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제17항에 있어서, 상기 포즈 그래프를 구성하는 프레임 노드 중 제1프레임 노드는 비주얼 프레임이 등록되지 않은 프레임 노드이며, 상기 제어부는 상기 로봇의 현재 위치를 상기 제1프레임 노드를 기준으로 추정하며, 상기 제어부는 로봇이 이동했던 경로 중 비주얼 프레임이 등록된 제2프레임 노드을 추출하여 상기 제2프레임 노드로부터 상기 제1프레임 노드까지의 오도메트리 정보를 이용하여 상기 로봇의 현재 위치를 계산하는, 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇."}
{"patent_id": "10-2019-7023289", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 다중 센서 및 인공지능에 기반하여 맵을 생성하고 맵을 이용하여 주행하는 로봇에 관한 것으로, 발명 의 일 실시예에 의한 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇은 하나 이상의 라이다 프레임을 포 함하는 라이다 브랜치와 상기 하나 이상의 비주얼 프레임을 포함하는 비주얼 브랜치와 상기 라이다 프레임 또는 상기 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프 를 생성하며, 상기 프레임 노드 사이를 상기 로봇이 이동하여 생성한 오도메트리 정보를 생성하는 제어부를 포함 한다. 공개특허10-2021-0150624 CPC특허분류 B25J 19/023 (2013.01) B25J 9/1602 (2013.01) B25J 9/1664 (2013.01) B25J 9/1679 (2013.01) 발명자 김형록 서울특별시 서초구 양재대로11길 19 LG전자 특허 센터 노동기 서울특별시 서초구 양재대로11길 19 LG전자 특허 센터박중태 서울특별시 서초구 양재대로11길 19 LG전자 특허 센터명 세 서 청구범위 청구항 1 로봇을 이동시키는 이동부; 상기 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 라이다 프레임을 생성하는 라이다 센서; 상기 로봇의 외부에 배치된 사물을 촬영하여 비주얼 프레임을 생성하는 카메라 센서; 상기 하나 이상의 라이다 프레임을 포함하는 라이다 브랜치와 상기 하나 이상의 비주얼 프레임을 포함하는 비주 얼 브랜치와 상기 라이다 프레임 또는 상기 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프를 생성하며, 상기 프레임 노드 사이를 상기 로봇이 이동하여 생성한 오 도메트리 정보를 생성하는 제어부; 및 상기 라이다 브랜치, 상기 비주얼 브랜치, 상기 백본, 상기 프레임 노드 사이의 오도메트리 정보 및 상기 포즈 그래프를 저장하는 맵 저장부를 포함하는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇. 청구항 2 제1항에 있어서, 상기 제어부는 이전 프레임 노드와 현재 위치 사이의 거리 또는 각도에 기반하여 새로운 프레임 노드를 생성하 며, 상기 제어부는 현재 위치에서 등록 가능한 상기 비주얼 프레임 또는 상기 라이다 프레임 중 어느 하나 이상을 상기 맵 저장부에서 검색하여 상기 생성한 새로운 프레임 노드에 등록시키는, 다중 센서 및 인공지능에 기반하 여 맵을 생성하는 로봇. 청구항 3 제1항에 있어서, 상기 프레임 노드에 등록된 비주얼 프레임은 상기 제어부가 다수의 비주얼 프레임들 중에서 인접한 비주얼 프레임들과 거리, 각도 및 비주얼 프레임 내의 특 징점의 수에 따라 선택한 비주얼 키프레임이며, 상기 비주얼 브랜치는 상기 다수의 비주얼 프레임 및 상기 비주얼 프레임 중 선택된 하나 이상의 비주얼 키프레 임을 포함하는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇. 청구항 4 제1항에 있어서, 상기 프레임 노드에 등록된 라이다 프레임은 상기 제어부가 다수의 라이다 프레임들 중에서 제1라이다 키프레임과 오버랩하는 크기에 따라 선택한 제2라이다 키프레임이며, 상기 라이다 브랜치는 상기 다수의 라이다 프레임 및 상기 라이다 프레임 중 선택된 하나 이상의 라이다 키프레 임을 포함하는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇. 청구항 5 제1항에 있어서, 로봇은 사용자로부터 정보를 입력받는 인터페이스부를 더 포함하며, 상기 인터페이스부가 프레임 노드의 추가를 입력받으면, 상기 제어부가 현재 위치에서 프레임 노드를 생성하고 상기 생성한 프레임 노드에 등록 가능한 비주얼 프레임또는 라이다 프레임을 검색하여 상기 프레임 노드에 등록시키는, 다중 센서 및 인공지능에 기반하여 맵을 생성 하는 로봇. 청구항 6 제1항에 있어서, 상기 제어부는 상기 포즈 그래프에서 상기 백본을 구성하는 프레임 노드에 등록된 라이다 프레임을 제거한 비주 얼 전용 포즈 그래프를 생성하는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇. 청구항 7 제1항에 있어서, 상기 제어부는 상기 포즈 그래프에서 상기 백본을 구성하는 프레임 노드에 등록된 비주얼 프레임을 제거한 라이 다 전용 포즈 그래프를 생성하는, 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇. 청구항 8 로봇을 이동시키는 이동부; 상기 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 제1라이다 프레임을 생성하는 라이다 센서; 상기 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성하는 카메라 센서; 상기 제1라이다 프레임과 비교 가능한 다수의 라이다 프레임들을 포함하는 라이다 브랜치를 저장하고, 상기 제1 비주얼 프레임과 비교 가능한 다수의 비주얼 프레임들을 포함하는 비주얼 브랜치를 저장하며, 상기 저장된 라이 다 프레임 또는 상기 저장된 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본 으로 구성된 포즈 그래프를 저장하며, 상기 프레임 노드 사이의 오도메트리 정보를 저장하는 맵 저장부; 상기 포즈 그래프의 프레임 노드에 등록된 프레임과 상기 제1라이다 프레임 또는 상기 제1비주얼 프레임을 비교 하거나 또는 상기 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산하는 제어부를 포함하는, 다중 센서 및 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 9 제8항에 있어서, 상기 제어부는 상기 백본에서 현재 위치에 대응하는 제1프레임 노드를 검색하여 상기 제1프레임 노드에 등록된 제2라이다 프레임을 추출하며, 상기 제어부가 상기 제1라이다 프레임과 상기 제2라이다 프레임을 비교한 결과 상기 두 프레임이 상이한 경우, 상기 제어부는 상기 라이다 브랜치에서 상기 제2라이다 프레임에 인접한 제3라이다 프레임을 추출하여 상기 제1 라이다 프레임과 상기 제3라이다 프레임을 비교하여 로봇의 현재 위치를 계산하는, 다중 센서 및 인공지능에 기 반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 10 제9항에 있어서, 상기 제어부는 상기 제1라이다 프레임과 상기 제3라이다 프레임을 비교한 결과 상기 두 프레임이 상이한 경우, 상기 제어부는 상기 제1프레임 노드에 등록된 제2비주얼 프레임을 추출하며, 상기 제어부가 상기 제1비주얼 프레임과 상기 제2비주얼 프레임을 비교하여 로봇의 현재 위치를 계산하는, 다중 센서 및 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 11 제8항에 있어서, 상기 제어부는 상기 백본에서 현재 위치에 대응하는 제1프레임 노드를 검색하여 상기 제1프레임 노드에 등록된 제2비주얼 프레임을 추출하며, 상기 제어부가 상기 제1비주얼 프레임과 상기 제2비주얼 프레임을 비교한 결과 상기 두 프레임이 상이한 경우, 상기 제어부는 상기 비주얼 브랜치에서 상기 제2비주얼 프레임에 인접한 제3비주얼 프레임을 추출하여 상기 제1 비주얼 프레임과 상기 제3비주얼 프레임을 비교하여 로봇의 현재 위치를 계산하는, 다중 센서 및 인공지능에 기 반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 12 제11항에 있어서, 상기 제어부는 상기 제1비주얼 프레임과 상기 제3비주얼 프레임을 비교한 결과 상기 두 프레임이 상이한 경우, 상기 제어부는 상기 제1프레임 노드에 등록된 제2라이다 프레임을 추출하며, 상기 제어부가 상기 제1라이다 프레임과 상기 제2라이다 프레임을 비교하여 로봇의 현재 위치를 계산하는, 다중 센서 및 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 13 제9항에 있어서, 상기 포즈 그래프를 구성하는 프레임 노드 중 제1프레임 노드는 라이다 프레임 또는 비주얼 프레임이 등록되지 않은 프레임 노드이며, 상기 제어부는 상기 로봇의 현재 위치를 상기 제1프레임 노드를 기준으로 추정하며, 상기 제어부는 로봇이 이동했던 경로 중 라이다 프레임 또는 비주얼 프레임이 등록된 제2프레임 노드을 추출하 여 상기 제2프레임 노드로부터 상기 제1프레임 노드까지의 오도메트리 정보를 이용하여 상기 로봇의 현재 위치 를 계산하는, 다중 센서 및 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 14 로봇을 이동시키는 이동부; 상기 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 제1라이다 프레임을 생성하는 라이다 센서; 상기 제1라이다 프레임과 비교 가능한 다수의 라이다 프레임들을 포함하는 라이다 브랜치를 저장하고, 상기 저 장된 라이다 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프 를 저장하며, 상기 프레임 노드 사이의 오도메트리 정보를 저장하는 맵 저장부; 상기 포즈 그래프의 프레임 노드에 등록된 프레임과 상기 제1라이다 프레임을 비교하거나 또는 상기 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산하는 제어부를 포함하는, 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 15 제14항에 있어서, 상기 제어부는 상기 백본에서 현재 위치에 대응하는 제1프레임 노드를 검색하여 상기 제1프레임 노드에 등록된 제2라이다 프레임을 추출하며, 상기 제어부가 상기 제1라이다 프레임과 상기 제2라이다 프레임을 비교한 결과 상기 두 프레임이 상이한 경우, 상기 제어부는 상기 라이다 브랜치에서 상기 제2라이다 프레임에 인접한 제3라이다 프레임을 추출하여 상기 제1 라이다 프레임과 상기 제3라이다 프레임을 비교하여 로봇의 현재 위치를 계산하는, 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 16 제14항에 있어서, 상기 포즈 그래프를 구성하는 프레임 노드 중 제1프레임 노드는 라이다 프레임이 등록되지 않은 프레임 노드이 며, 상기 제어부는 상기 로봇의 현재 위치를 상기 제1프레임 노드를 기준으로 추정하며, 상기 제어부는 로봇이 이동했던 경로 중 라이다 프레임이 등록된 제2프레임 노드을 추출하여 상기 제2프레임 노 드로부터 상기 제1프레임 노드까지의 오도메트리 정보를 이용하여 상기 로봇의 현재 위치를 계산하는, 인공지능 에 기반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 17 로봇을 이동시키는 이동부; 상기 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성하는 카메라 센서; 상기 제1비주얼 프레임과 비교 가능한 다수의 비주얼 프레임들을 포함하는 비주얼 브랜치를 저장하고, 상기 저 장된 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프 를 저장하며, 상기 프레임 노드 사이의 오도메트리 정보를 저장하는 맵 저장부; 상기 포즈 그래프의 프레임 노드에 등록된 프레임과 상기 제1비주얼 프레임을 비교하거나 또는 상기 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산하는 제어부를 포함하는, 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 18 제17항에 있어서, 상기 제어부는 상기 백본에서 현재 위치에 대응하는 제1프레임 노드를 검색하여 상기 제1프레임 노드에 등록된 제2비주얼 프레임을 추출하며, 상기 제어부가 상기 제1비주얼 프레임과 상기 제2비주얼 프레임을 비교한 결과 상기 두 프레임이 상이한 경우, 상기 제어부는 상기 비주얼 브랜치에서 상기 제2비주얼 프레임에 인접한 제3비주얼 프레임을 추출하여 상기 제1 비주얼 프레임과 상기 제3비주얼 프레임을 비교하여 로봇의 현재 위치를 계산하는, 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇. 청구항 19 제17항에 있어서, 상기 포즈 그래프를 구성하는 프레임 노드 중 제1프레임 노드는 비주얼 프레임이 등록되지 않은 프레임 노드이 며, 상기 제어부는 상기 로봇의 현재 위치를 상기 제1프레임 노드를 기준으로 추정하며, 상기 제어부는 로봇이 이동했던 경로 중 비주얼 프레임이 등록된 제2프레임 노드을 추출하여 상기 제2프레임 노 드로부터 상기 제1프레임 노드까지의 오도메트리 정보를 이용하여 상기 로봇의 현재 위치를 계산하는, 인공지능 에 기반하여 생성된 맵을 이용하여 주행하는 로봇. 발명의 설명"}
{"patent_id": "10-2019-7023289", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 다중 센서 및 인공지능에 기반하여 맵을 생성하고 맵을 이용하여 주행하는 로봇에 관한 기술이다."}
{"patent_id": "10-2019-7023289", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "대형 마트, 백화점, 공항, 골프장 등 인적, 물적 교류가 활발하게 발생하는 공간에는 사람들에게 정보를 제공하 기 위해, 또는 사람들에게 편의를 제공하기 위해 로봇이 배치될 수 있다. 전술한 로봇의 종류로는 안내로봇, 보안로봇, 청소로봇 등이 있으며, 이들 다양한 로봇들은 공간 내에서 자신의 위치를 확인하며 이동한다. 한편, 로봇들이 자신의 위치를 확인하고 장애물을 회피하며 이동하기 위해서는 공간에 대한 정보와 로봇의 현재 위치 또는 이전에 로봇이 이동한 경로 등에 대한 정보를 로봇이 유지해야 한다. 로봇이 공간을 확인하고 이동하기 위해 로봇은 맵을 보유할 수 있다. 그런데, 맵을 생성하기 위해서 로봇은 다 양한 센서들을 이용하여 맵을 작성할 수 있으며, 맵 내의 정보의 다양한 정보들을 일치시켜 저장하는 것이 필요 하다. 그러나 센서들은 각각의 특성과 로봇이 이동하는 과정에서 발생하는 오차가 있으므로 이를 반영하여 맵을 생성 하는 기술이 필요하다."}
{"patent_id": "10-2019-7023289", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 명세서에서는 전술한 문제점을 해결하기 위한 것으로, 로봇이 다양한 센서를 이용하여 공간에 대한 이종의 맵을 생성하고자 한다. 또한, 본 명세서에서는 다양한 센서를 이용하여 공간에서의 로봇의 위치를 식별하는 퓨전-슬램(Fusion SLAM)을 구현하고자 한다. 또한, 본 명세서에서는 로봇이 한 종류의 센서만을 이용할 수 있도록 이종의 맵을 분리하여 제공하고자 한다. 본 발명의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 발명의 다른 목적 및 장점들 은 하기의 설명에 의해서 이해될 수 있고, 본 발명의 실시예에 의해 보다 분명하게 이해될 것이다. 또한, 본 발 명의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것 이다."}
{"patent_id": "10-2019-7023289", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "발명의 일 실시예에 의한 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇은 상기 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 라이다 프레임을 생성하는 라이다 센서와 상기 로봇의 외부에 배치된 사물 을 촬영하여 비주얼 프레임을 생성하는 카메라 센서를 포함한다. 발명의 일 실시예에 의한 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇은 하나 이상의 라이다 프레임 을 포함하는 라이다 브랜치와 상기 하나 이상의 비주얼 프레임을 포함하는 비주얼 브랜치와 상기 라이다 프레임 또는 상기 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프를 생성하며, 상기 프레임 노드 사이를 상기 로봇이 이동하여 생성한 오도메트리 정보를 생성하는 제어부 를 포함한다. 발명의 일 실시예에 의한 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇은 상기 라이다 브랜치, 상기 비주얼 브랜치, 상기 백본, 상기 프레임 노드 사이의 오도메트리 정보 및 상기 포즈 그래프를 저장하는 맵 저장 부를 포함한다. 발명의 일 실시예에 의한 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇은 이전 프레임 노드와 현재 위 치 사이의 거리 또는 각도에 기반하여 새로운 프레임 노드를 생성하며, 현재 위치에서 등록 가능한 상기 비주얼 프레임 또는 상기 라이다 프레임 중 어느 하나 이상을 상기 맵 저장부에서 검색하여 상기 생성한 새로운 프레임 노드에 등록시킨다. 발명의 일 실시예에 의한 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇은 상기 포즈 그래프에서 상기 백본을 구성하는 프레임 노드에 등록된 라이다 프레임을 제거한 비주얼 전용 포즈 그래프를 생성한다. 발명의 일 실시예에 의한 다중 센서 및 인공지능에 기반하여 맵을 생성하는 로봇은 상기 포즈 그래프에서 상기 백본을 구성하는 프레임 노드에 등록된 비주얼 프레임을 제거한 라이다 전용 포즈 그래프를 생성한다. 발명의 일 실시예에 의한 다중 센서 및 인공지능에 기반하여 생성된 맵을 이용하여 주행하는 로봇은 상기 포즈 그래프의 프레임 노드에 등록된 프레임과 라이다 센서가 생성한 제1라이다 프레임 또는 카메라 센서가 생성한 제1비주얼 프레임을 비교하거나 또는 상기 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산하는 제어부를 포함한다."}
{"patent_id": "10-2019-7023289", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들을 적용할 경우, 로봇이 다양한 센서를 이용하여 공간에 대한 이종의 맵을 생성할 수 있다. 또한, 본 발명의 실시예들을 적용할 경우, 다양한 센서를 이용하여 공간에서 로봇이 퓨전-슬램(Fusion SLAM)을 수행할 수 있다. 또한, 본 발명의 실시예들을 적용할 경우, 다양한 센서들로 구성된 맵에서 일부 맵을 추출하여 로봇에 적합하게 SLAM을 수행할 수 있다."}
{"patent_id": "10-2019-7023289", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 전술한 효과에 한정되지 않으며, 본 발명의 당업자들은 본 발명의 구성에서 본 발명의 다양 한 효과를 쉽게 도출할 수 있다."}
{"patent_id": "10-2019-7023289", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 동일 또는 유사한 구성요소에 대해서는 동일한 참조 부호를 붙이도록 한다. 또한, 본 발명의 일부 실시예들을 예시적인 도 면을 참조하여 상세하게 설명한다. 각 도면의 구성요소들에 참조부호를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 부호를 가질 수 있다. 또한, 본 발명을 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우에 는 그 상세한 설명은 생략할 수 있다. 본 발명의 구성 요소를 설명하는 데 있어서, 제 1, 제 2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이러한 용어는 그 구성 요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성 요소의 본질, 차례, 순서 또는 개수 등이 한정되지 않는다. 어떤 구성 요소가 다른 구성요소에 \"연결\", \"결합\" 또는 \"접속\"된 다고 기재된 경우, 그 구성 요소는 그 다른 구성요소에 직접적으로 연결되거나 또는 접속될 수 있지만, 각 구성 요소 사이에 다른 구성 요소가 \"개재\"되거나, 각 구성 요소가 다른 구성 요소를 통해 \"연결\", \"결합\" 또는 \"접 속\"될 수도 있다고 이해되어야 할 것이다. 또한, 본 발명을 구현함에 있어서 설명의 편의를 위하여 구성요소를 세분화하여 설명할 수 있으나, 이들 구성요 소가 하나의 장치 또는 모듈 내에 구현될 수도 있고, 혹은 하나의 구성요소가 다수의 장치 또는 모듈들에 나뉘 어져서 구현될 수도 있다.이하, 본 명세서에서 로봇은 특정한 목적(청소, 보안, 모니터링, 안내 등)을 가지거나 혹은 로봇이 이동하는 공 간의 특성에 따른 기능을 제공하며 이동하는 장치를 포함한다. 따라서, 본 명세서에서의 로봇은 소정의 정보와 센서를 이용하여 이동할 수 있는 이동수단을 보유하며 소정의 기능을 제공하는 장치를 통칭한다. 본 명세서에서 로봇은 맵을 보유하면서 이동할 수 있다. 맵은 공간에서 이동하지 않는 것으로 확인된 고정된 벽, 계단 등 고정 객체에 대한 정보를 의미한다. 또한, 주기적으로 배치되는 이동 장애물, 즉 동적인 객체들에 대한 정보도 맵 상에 저장될 수 있다. 일 실시예로 로봇의 진행 방향을 기준으로 일정한 범위 내에 배치된 장애물들에 대한 정보도 맵 상에 저장될 수 있다. 이 경우, 전술한 고정 객체가 저장되는 맵과 달리 임시적으로 장애물들의 정보가 맵에 등록되고 이후 로 봇이 이동한 후 맵에서 제거될 수 있다. 또한, 본 명세서에서 로봇은 다양한 센서들을 이용하여 외부의 동적 객체를 확인할 수 있다. 외부의 동적 객체 를 확인하면, 보행자로 붐비는 환경에서 로봇이 목적지까지 주행할 때, 목적지까지 거쳐가야 하는 경유 지점 (Waypoint)의 장애물에 의한 점유 상황을 확인할 수 있다. 또한 로봇은 경유 지점의 방향 변경 정도에 따라 유연하게 경유 지점을 도착한 것으로 판단하고 다음 경유 지점 으로 넘어 가도록 하여 목적지까지 성공적으로 주행할 수 있다. 도 1은 본 발명의 일 실시예에 의한 로봇의 외관을 보여준다. 도 1은 예시적인 외관에 해당하며, 도 1의 외관 외에도 다양한 외관으로 본 발명의 로봇을 구현할 수 있다. 특히, 각각의 구성요소는 로봇의 형상에 따라 상하 좌우 전후 등에서 다른 위치에 배치될 수 있다. 본체는 상하 방향으로 길이가 길게 형성되며, 전체적으로 하부에서 상부 방향으로 올라갈수록 슬림해지는 오뚝이 형상을 가질 수 있다. 본체는 로봇의 외관을 형성하는 케이스를 포함할 수 있다. 케이스는 상측에 배치되는 탑 커버 , 탑 커버의 하측에 배치되는 제1 미들 커버, 제1 미들 커버의 하측에 배치되는 제2 미들 커버 및 제2 미들 커버의 하측에 배치되는 바텀 커버를 포함할 수 있다. 여기서 제1 미들 커버와 제2 미들 커버는 하나의 미들 커버로 이루어질 수 있다. 탑 커버는 로봇의 최상단에 위치되며, 반구 또는 돔 형상을 가질 수 있다. 탑 커버는 사용자로부터 명령을 용이하게 입력 받기 위하여 성인의 키보다 낮은 높이에 위치될 수 있다. 그리고 탑 커버는 소정각도 회전 가능하도록 구성될 수 있다. 한편, 로봇은 그 내부에 제어모듈을 더 포함할 수 있다. 제어모듈은 일종의 컴퓨터 또는 프로세서 와 같이 로봇을 제어한다. 따라서 제어모듈은 로봇내에 배치되어 메인 프로세서와 유사한 기능을 수 행하며, 사용자와의 인터랙션(interaction)을 담당할 수 있다. 로봇의 이동과 주변의 사물을 감지하여 로봇을 제어하기 위해 제어모듈이 로봇 내부에 탑재된다. 로봇 제어모듈은 소프트웨어 모듈 또는 이를 하드웨어로 구현한 칩 등으로 구현 가능하다. 탑 커버는 전면 일측에 사용자로부터 명령을 입력받거나 정보를 출력하는 디스플레이부(31a)과 카메라 (31b), 마이크(31c)를 일 실시예로 하는 센서가 배치될 수 있다. 또한, 탑 커버의 디스플레이부(31a) 외에도 미들 커버의 일측에도 디스플레이부가 배치된다. 로봇의 기능에 따라 두 개의 디스플레이부(31a, 20) 모두 정보를 출력하거나 혹은 어느 한쪽에서만 정보가 출력 될 수 있다. 한편, 로봇의 일측면 또는 하단부 전체에는 35a, 35b와 같이 다양한 장애물 센서(도 2의 220)들이 배치된다. 장애물 센서들은 TOF(Time of Flight) 센서, 초음파 센서, 적외선 센서, 뎁스 센서, 레이저 센서, 라이다 센서 등을 일 실시예로 한다. 센서들은 다양한 방식으로 로봇 외부의 장애물을 감지한다. 또한, 도 1의 로봇은 하단부에 로봇을 이동시키는 구성요소인 이동부를 더 포함한다. 이동부는 일종의 바퀴와 같이 로봇을 이동시키는 구성요소이다. 도 1의 로봇의 형상은 예시적이며, 본 발명이 이에 한정되는 것은 아니다. 또한, 로봇의 다양한 카메라들과 센 서들 역시 로봇의 다양한 위치에 배치될 수 있다. 도 1의 로봇은 사용자에게 정보를 안내하고 특정 지점까지 이동하여 사용자를 안내하는 안내 로봇을 일 실시예로 한다. 이외에도 청소, 보안 또는 기능을 제공하는 로봇 역시 도 1의 로봇의 범위에 포함된다. 다양한 기능을 제공할 수 있으나, 본 명세서에서는 설명의 편의를 위해 안내 로봇을 중심으로 설명한다. 도 1를 일 실시예로 하는 로봇이 서비스 공간 내에 다수 배치된 상태에서 로봇이 특정한 기능(안내, 청소, 보안 등)을 수행한다. 이 과정에서 로봇은 자신의 위치를 저장하며, 로봇은 전체 공간에서 자신의 현재 위치를 확인하고, 목표 지점으로 이동하는데 필요한 경로를 생성할 수 있다. 도 2는 본 발명의 일 실시예에 의한 로봇의 제어모듈의 구성 요소를 보여준다. 라이다 센서(LiDAR Sensor)는 2차원 또는 3차원으로 주변의 사물들을 센싱할 수 있다. 2차원 라이다 센서 의 경우 로봇을 중심으로 360도 범위의 사물의 위치를 센싱할 수 있다. 특정 위치에서 센싱한 라이다 정보는 하 나의 라이다 프레임을 구성할 수 있다. 즉, 라이다 센서는 로봇의 외부에 배치된 사물과 로봇 사이의 거리 를 센싱하여 라이다 프레임을 생성한다. 카메라 센서는 일반 카메라를 일 실시예로 한다. 시야각의 제약을 해결하기 위해 둘 이상의 카메라 센서 를 사용할 수 있다. 특정 위치에서 촬영한 영상은 비전 정보를 구성한다. 즉, 카메라 센서는 로봇의 외부에 배치된 사물을 촬영하여 비전 정보를 포함하는 비주얼 프레임을 생성한다. 이하 본 발명을 적용하는 로봇은 라이다 센서와 카메라 센서를 이용한 퓨젼-SLAM(Fusion- simultaneous localization and mapping)을 수행한다. 퓨전 SLAM은 라이다 정보와 비전 정보를 결합하여 사용할 수도 있다. 이들 라이다 정보와 비전 정보는 맵으로 구성할 수 있다. 로봇이 퓨전 SLAM을 사용할 경우, 하나의 센서만을 사용하는 방식(LiDAR-only SLAM, visual-only SLAM)와 비교 하여 위치 추정의 정확도가 높다. 즉, 라이다 정보와 비전 정보를 결합하여 퓨전 SLAM을 수행하면 맵 품질(map quality) 측면에서 더 좋은 맵을 획득할 수 있다. 여기서, 맵 품질이란 비전 정보들로 구성된 비전 맵(vision map)과 라이다 정보로 구성된 라이다 맵(lidar map) 양쪽에 모두 해당하는 기준이다. 퓨전 SLAM 시 각각의 맵 품질이 좋아지는데 이는 각각의 센서가 획득하지 못하 거나 부족한 정보를 센서들이 서로 이용할 수 있기 때문이다. 또한, 하나의 맵에서 라이다 정보 또는 비전 정보만을 추출하여 사용할 수 있다. 예를 들어, 로봇이 보유하는 메모리의 양이나 연산 프로세서의 연산 능력 등에 적합하게 라이다 정보만을 이용하거나, 비전 정보만을 이용하 거나 또는 두 정보 모두를 로봇의 위치 추정(localization)에 적용할 수 있다. 인터페이스부는 사용자로부터 정보를 입력받는다. 터치 입력, 음성 입력 등 다양한 정보를 사용자로부터 입력받고, 이에 대한 결과를 출력한다. 또한 인터페이스부는 로봇이 저장하는 맵을 출력하거나, 로봇이 이동하는 과정을 맵과 오버랩 하여 출력할 수 있다. 또한, 인터페이스부는 사용자에게 소정의 정보를 제공할 수 있다. 제어부는 후술할 도 4와 같은 맵을 생성하고 이 맵을 기반으로 로봇이 이동하는 과정에서 로봇의 위치를 추정한다. 통신부는 로봇이 다른 로봇 또는 외부의 서버와 통신하여 정보를 송수신할 수 있도록 한다. 로봇은 각각의 센서들(라이다 센서, 카메라 센서)을 이용하여 각각의 맵을 생성할 수 있다. 또는 로봇은 이들 센서들을 이용하여 하나의 맵을 만든 후 맵에서 다시 이들로부터 특정 센서에 해당하는 내용만을 추출하는 맵을 생성할 수 있다. 또한, 본 발명의 맵은 바퀴의 회전에 기반한 오도메트리(odometry) 정보를 포함할 수 있다. 오도메트리 정보는 로봇의 바퀴 회전 횟수나 양 바퀴의 회전 횟수의 차이 등을 이용하여 로봇이 이동한 거리를 산출한 정보이다. 센서들을 이용한 정보 외에도 오도메트리 정보를 이용하여 로봇이 어디까지 이동했는지 로봇이 계산할 수 있다. 도 2의 제어부는 인공지능 작업 및 처리를 위한 인공지능부를 더 포함할 수 있다. 로봇의 라이다 센서 및 카메라 센서는 외부의 물체를 식별하기 위해 로봇의 외부에 다수 배치할 수 있다. 도 2에 제시된 라이다 센서 및 카메라 센서 외에도 로봇의 외부에는 다양한 종류의 센서들(라이다 센서, 적외선 센서, 초음파 센서, 뎁스 센서, 이미지 센서, 마이크 등)이 배치된다. 제어부는 센서들이 센 싱한 정보를 취합 및 처리한다. 인공지능부는 라이다 센서 및 카메라 센서, 그리고 그 외의 센서들이 처리한 정보 또는 로봇 이 이동 과정에서 누적 저장한 정보 등을 입력하여 제어부가 외부 상황을 판단하거나, 정보를 처리하거 나, 이동 경로를 생성하는데 필요한 결과물을 출력할 수 있다. 일 실시예로, 로봇은 로봇이 이동하는 공간에 배치된 다양한 사물들의 위치 정보를 맵으로 저장할 수 있다. 사물들은 벽, 문 등의 고정 사물들과 화분, 책상 등 이동 가능한 사물들을 포함한다. 인공지능부는 맵 정 보와 라이다 센서 및 카메라 센서, 그리고 그 외의 센서들이 제공하는 정보들을 이용하여 로봇이 이 동할 경로, 혹은 로봇이 작업시 커버해야 할 범위 등에 대한 데이터를 출력할 수 있다. 또한 인공지능부는 라이다 센서 및 카메라 센서, 그리고 그 외의 센서들이 제공하는 정보들을 이용하여 로봇 주변에 배치된 사물을 인식할 수 있다. 인공지능부는 이미지를 입력받아 이미지에 관한 메 타 정보를 출력할 수 있다. 메타 정보란 이미지 내의 사물의 명칭, 사물과 로봇과의 거리, 사물의 종류, 사물이 맵 상에 위치하는지 여부 등을 포함한다. 라이다 센서 및 카메라 센서, 그리고 그 외의 센서들이 제공하는 정보는 인공지능부의 딥러닝 네트워크의 입력 노드로 입력된 후, 인공지능부의 딥러닝 네트워크의 히든 레이어의 정보 처리를 통해 인 공지능부의 출력 노드에서 결과가 출력된다. 제어부는 인공지능부가 산출한 데이터 또는 다양한 센서들이 처리한 데이터를 이용하여 로봇의 이동 경로를 산출할 수 있다. 도 3은 로봇이 공간에서 이동하는 과정을 보여준다. 공간 내에서 로봇은 41이 지시하는 선을 따라 이동하며 라이다 센서를 이용하여 특정 지점에서 라이다 센서가 센싱한 정보를 맵 저장부에 저장할 수 있다. 공간의 기초 형상은 로컬 맵(local map)으로 저장될 수 있다. 마찬가지로 로봇은 공간을 이동하면서 카메라 센서를 이용하여 특정 지점에서 카메라 센서가 센싱한 정보를 맵 저장부에 저장할 수 있다. 도 4는 본 발명의 일 실시예에 의한 맵의 다중 구조를 보여준다. 도 4는 백본(backbone)을 제1레이어(first layer)로 하며 라이다 브랜치(LiDAR branch) 및 비주얼 브랜치(Visual branch)를 각각 제2레이어(second layer)로 하는 2중층의 구성을 보여준다. 도 4와 같은 구조를 구조적으로 탄력적인 포즈 그래프 기반 슬램 (structurally elastic pose graph-based SLAM)이라 명명한다. 백본은 로봇의 궤적(trajectory)을 추적한 정보이다. 또한 백본은 궤적에 대응하는 하나 이상의 프레임 노드들 을 포함한다. 그리고 이들 프레임 노드들은 다른 프레임 노드와의 관계에서 제약 정보(constraint)를 더 포함한 다. 노드 사이의 에지는 제약 정보를 나타낸다. 에지는 오도메트리 제약 정보(odometry constraint) 또는 루프 제약 정보(loop constraint)를 의미한다. 또한, 제2레이어의 라이다 브랜치는 라이다 프레임(LiDAR Frame)들로 구성된다. 라이드 프레임은 로봇의 이동 과정에서 센싱한 라이다 센싱 값을 포함한다. 이들 라이다 프레임들 중에서 적어도 하나 이상은 라이다 키프레 임(LiDAR Keyframe)으로 설정된다. 라이다 키프레임은 백본의 노드와 대응관계를 가진다. 도 4에서 백본의 노드들 v1 내지 v5 중에서 v1, v2, v4, v5가 라이다 키프레임을 지시한다. 마찬가지로, 제2레이어의 비주얼 브랜치는 비주얼 키 프레임(Visual Keyframe)들로 구성된다. 비주얼 키프레임 은 로봇의 이동 과정에서 센싱한 카메라 센싱 값(즉, 카메라로 촬영한 영상)인 비주얼 피쳐 노드(visual feature node)들을 하나 이상 지시한다. 로봇에 배치된 카메라 센서의 수에 따라 로봇은 다수의 비주얼 피쳐 노 드를 생성할 수 있다. 즉, 도 4의 맵 구조에서는 백본의 프레임 노드에 라이다 키프레임이 연결되거나 또는 비주얼 키프레임이 연결되 는 구성이다. 물론 라이다/비주얼 키프레임 모두 하나의 프레임 노드에 연결될 수 있다(v1, v4, v5). 각 프레임 노드와 연결된 라이다 또는 비주얼 키프레임의 로봇의 포즈는 같다. 다만, 라이다 센서 또는 카메라 센서가 로봇에 부착된 위치에 따라 외부 파라미터(extrinsic parameter)가 키프레임 별로 추가될 수 있다. 외부 파라미터란 로봇 중심으로부터의 센서가 부착된 상대적인 위치 정보를 의미한다. 비주얼 키프레임은 백본의 노드와 대응관계를 가진다. 도 4에서 백본의 노드들 v1 내지 v5 중에서 v1, v3, v4, v5가 비주얼 키프레임을 지시한다. 도 2에서 비주얼 피쳐 노드(비주얼 프레임)들은 두 개가 한 쌍으로 구성되는 데, 이는 로봇이 두 개의 카메라 센서를 포함하여 영상을 촬영함을 의미한다. 카메라 센서의 증감 에 따라 각각의 위치에서 비주얼 피쳐 노드의 수 역시 증감한다. 제1레이어의 백본을 구성하는 노드들(v1~v5) 사이에는 에지(edge)가 표시되어 있다. e12, e23, e34, e45는 인접 노드들 사이의 에지이며, e13, e35, e25는 인접하지 않은 노드들 사이의 에지이다. 오도메트리 제약 정보 또는 줄여서 오도메트리 정보는 e12, e23, e34, e45와 같이 인접한 프레임 노드 사이의 제약 조건을 의미한다. 루프 제약 정보 또는 줄여서 루프 정보는 e13, e25, e35와 같이 인접하지 않은 프레임 사이의 제약 조건을 의미한다. 백본은 다수의 키프레임들로 구성된다. 다수의 키프레임을 백본으로 추가하기 위해 제어부는 초기 매핑 과 정(initial mapping process)를 수행할 수 있다. 초기 매핑은 키프레임 기반으로 라이다 키프레임과 비주얼 키 프레임을 추가한다. 도 4의 구조를 정리하면 다음과 같다. 라이다 브랜치는 하나 이상의 라이다 프레임을 포함한다. 비주얼 브랜치 는 하나 이상의 비주얼 프레임을 포함한다. 그리고 백본은 라이다 프레임 또는 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함한다. 이때, 프레임 노드에 등록되는 라이다 프레임 또는 비주얼 프레임을 키프레임이라 지칭한다. 그리고 포즈 그래프는 이들 라이다 브랜치, 비주얼 브랜치, 백본을 포함한다. 뿐만 아니라, 포즈 그래프는 프레임 노드 사이의 오도메트리 정보 및 루프 정보 등을 포함한다. 오도메트리 정 보는 로봇이 프레임 노드 사이를 이동하여 생성한 휠의 회전이나 방향 등을 포함한다. 루프 정보는 라이다 센서 의 최대 센싱 거리 내에서 특정한 프레임 노드를 중심으로 비주얼 키프레임 사이에 특정한 제약 조건으로 연결된 프레임 노드 셋에 기반한다. 도 4의 포즈 그래프는 제어부가 생성한다. 제어부는 라이다 브랜치, 비주얼 브랜치, 백본, 프레임 노 드 사이의 오도메트리 정보 및 이를 포함하는 포즈 그래프를 맵 저장부에 저장한다. 도 5는 본 발명의 일 실시예에 의한 초기 매핑 과정을 보여준다. 로봇은 로컬 공간을 주행하여 도 4의 라이다 프레임, 비주얼 피쳐 노드(비주얼 프레임)를 확보한 상태이다. 또는 로봇은 로컬 공간을 주행하며 실시간으로 도 4의 라이다 프레임, 비주얼 피쳐 노드를 확보하며 동시에 도 5의 프로세스를 수행할 수 있다. 먼저 제어부는 제1레이어의 백본을 구성하는 프레임 노드를 생성한다(S45). 프레임 노드는 노드 간의 일정 한 거리와 각도를 기반으로 한다. 또한, 해당 위치에서 라이다 정보(라이다 프레임) 또는 비전 정보(비주얼 프 레임) 중 어느 하나라도 존재할 경우 프레임 노드를 생성한다. 또한, 특정한 목적지(약국, 안내데스크 등)를 생성하기 위해 추가로 노드를 생성할 수 있다. 이는 사용자가 미 리 셋팅한 경우 또는 프레임 노드 생성 과정에서 해당 위치를 노드로 미리 지정한 경우에 가능하다. 프레임 노드의 생성에서 노드 간의 거리와 각도를 반영하는 것은 로봇이 정지한 경우에 중복된 노드가 생성되는 것을 방지한다. 다음으로 비주얼 키프레임을 등록하는 과정(S46)은 기존의 프레임과의 각도와 로봇이 이루는 각도 차이가 클 경 우에 등록한다. 이 역시 중복된 키프레임이 생성되는 것을 방지하기 위함이다. 또한 제어부는 촬영된 영상에서 특징점 개수를 계산하여 해당 영상을 비주얼 키프레임으로 등록할 정도로 영상이 제공하는 정보가 많은 지를 확인한다. 만약 영상 전체가 동일한 색상이거나(벽의 경우) 사람들이 촬영된 경우(특징점 없음)에는 제어부는 비주얼 키프레임으로 등록하지 않는다. 또한 라이다 키프레임을 등록하는 과정은 로봇이 이동하면서 센싱한 라이다 정보에서 서로 중첩되는 영역이 줄 어들면 키프레임을 생성한다. 근접한 거리 내에서는 라이다 정보들이 동일할 수 있다. 따라서 일정한 거리 차이 가 있어서 라이다 정보들이 상이할 경우에만 키프레임으로 등록한다. S45 내지 S47은 로봇이 이동하면서 실시간으로 라이다 정보와 비전 정보를 취합하여 실시간으로 제어부가 백본의 프레임 노드 및 비주얼 키프레임, 라이다 키프레임을 등록할 수 있다. 또는 로봇이 1차적으로 이동하면서 미리 라이다 정보와 비전 정보를 취합 및 저장한 후, 제어부가 백본의 프레임 노드 및 비주얼 키프레임, 라이다 키프레임을 등록할 수 있다. 또한, 제어부는 백본의 프레임 노드를 생성하며 각 노드들 사이의 오도메트리 제약 정보와 루프 제약 정보 를 에지로 추가할 수 있다. 도 5를 정리하면 다음과 같다. 제어부는 이전 프레임 노드와 현재 위치 사이의 거리 또는 각도에 기반하여 새로운 프레임 노드를 생성한다(S45). 그리고 제어부는 현재 위치에서 등록 가능한 비주얼 프레임 또는 라 이다 프레임 중 어느 하나 이상을 맵 저장부에서 검색하여 S45에서 생성한 새로운 프레임 노드에 등록시킨 다(S46, S47). 물론, 새로운 프레임 노드를 기준으로 비주얼 프레임/라이다 프레임이 전혀 없을 경우, 라이다 센서 또는 카메라 센서를 이용하여 제어부는 새롭게 라이다 프레임/비주얼 프레임을 생성할 수도 있다. 도 6은 본 발명의 일 실시예에 의한 비주얼 키프레임을 생성하는 과정을 보여준다. 제어부는 입력된 비전 정보들 중에서 SLAM에 적합한 비주얼 키프레임을 선택한다. 제어부는 초기화를 수행한다(S51). 초기화는 제어부가 이웃 노드들(Vneigbor)의 집합을 공집합(Ф)으로, 그리고 이웃 노드들의 수(nneighbor)를 0으로 설정하는 것을 의미한다. 그리고 제어부는 후보 비주얼 키프레임(candidate visual keyframe, vv candidate)에 일정 거리 이내로 위치한 노드들 중에서 후보 영역의 최대 각도(Rmax) 이내에 위치한 노드들을 이웃 노드들(Vneigbor)로 포함시킨다(S52). 여 기서 Rmax는 중첩된 비주얼 키프레임을 식별하는데 필요한 최대 검색 범위를 의미한다. 예를 들어, Rmax의 크기가 작으면 맵은 매우 밀집도가 높게 작성될 수 있다. 이웃 노드들의 집합이 셋팅되면, 비주얼 키프레임을 기준으로 θreg이하의 방향 차이를 가지는 이웃 노드가 있으 며, 이들 이웃 노드들과의 에지가 포즈 그래프(pose graph)와 일치한다면 이는 위치추정에 중요한 키프레임으로 판단할 수 있다(S53). 다음 단계로, 후보 비주얼 키프레임이 충분히 3D 포인트를 가지고 있는지 확인한다. 만약, 충분히 3D 포인트를 보유한다면, 이러한 후보 비주얼 키프레임은 키프레임으로 도 4의 백본을 포함하는 포즈 그래프에 추가될 수 있 다(S54). 3D 포인트에 대한 기준은 Nmin3D_point은 키프레임으로 등록하기 위한 최소한의 3D 포인트를 의미한다. 도 6의 과정은 카메라 센서가 이미지를 취합하면 반복 실시할 수 있다. 도 6의 과정을 예시적인 유사코드 (pseudo code)로 구현하면 다음과 같다. S51은 하기의 라인 1에 해당하며, S52는 하기 라인 2에 해당한다. S53 은 하기 라인 3-8에 해당한다. S54는 하기 라인 9~14에 해당한다. 도 6을 정리하면, 프레임 노드에 등록될 수 있는 비주얼 키프레임은 비주얼 프레임들 중에서 선택된다. 제어부 는 다수의 비주얼 프레임들 중에서 인접한 비주얼 프레임들과 거리, 각도 및 비주얼 프레임 내의 특징점의 수에 따라 비주얼 키프레임을 선택한다. 그리고 도 4에 제시된 바와 같이 비주얼 브랜치는 다수의 비주얼 프레임 및 하나 이상의 비주얼 키프레임을 포 함한다. 비주얼 키프레임은 비주얼 프레임들 중에서 선택된 비주얼 프레임이다. 도 7은 본 발명의 일 실시예에 의한 라이다 키 프레임을 생성하는 과정을 보여준다. 제어부는 이동하는 과정에서 특정 위치의 라이다 프레임을 생성할 수 있다. 그리고 첫번째 라이다 프레임 을 라이다 키프레임으로 등록(S56)하고 로봇의 이동 과정에서 새로운 라이다 프레임을 생성한다(S57). 제어 부는 새로운 라이다 프레임을 선택한다(S58). 제어부는 선택한 라이다 프레임과 이전에 등록된 키프레임을 비교하여 오버랩하는 크기가 일정 기준 이하 인 경우에 키프레임으로 등록한다(S59). S57 내지 S59는 로봇이 이동하는 과정에서 반복될 수 있다. 도 7의 과정을 적용하면 라이다 프레임들 중에서 이전 공간과 변화의 크기가 큰 라이다 프레임이 키프레임으로 등록될 수 있다. 도 7을 정리하면 다음과 같다. 제어부는 다수의 라이다 프레임들 중에서 제1라이다 키프레임과 오버랩하는 크기에 따라 제2라이다 키프레임을 선택한다. 제1라이다 키프레임은 이미 키프레임으로 선택된 키프레임이며, 제2라이다 키프레임은 새롭게 선택된 키프레임이다. 도 4에 제시된 바와 같이 라이다 브랜치는 다수의 라이다 프레임 및 하나 이상의 라이다 키프레임을 포함한다. 라이다 키프레임은 다수의 라이다 프레임 중 선택된 라이다 프레임이다. 도 6 및 도 7의 비주얼 키프레임의 생성과 라이다 키프레임의 생성은 독립적으로 이루어질 수 있다. 도 6 및 도 7과 같이 생성된 키프레임들 중에서 제어부는 백본의 프레임 노드로 등록한다. 도 8은 본 발명의 일 실시예에 의한 백본의 프레임 노드를 등록하는 과정을 보여준다. 본 발명의 일 실시예에 의한 제어부는 앞서 도 6 및 도 7의 과정에서 살펴본 비주얼 키프레임 및 라이다 키프레임들이 생성된 이후 프레임 노드를 생성할 수 있다. 또는 제어부는 사용자들이 특정하게 지시한 위 치에서 별도의 비주얼 키프레임과 라이다 키프레임을 생성하고 해당 위치에서 프레임 노드를 생성하여 프레임 노드와 비주얼 키프레임/라이다 키프레임을 등록할 수 있다. 보다 상세히, 사용자의 수동 등록 상태인 경우(S61), 사용자가 POI(Point of Interest)를 입력하는 모드로 전환 한다. 즉, 인터페이스부가 프레임 노드의 추가를 입력받을 수 있다. 프레임 노드의 추가와 함께 현재 위치에 대 한 메타데이터(meta data)를 입력 받을 수 있다. 사용자의 입력에 대응하여 제어부는 현재 위치에서 프레임 노드를 생성하고 생성한 프레임 노드에 등록 가 능한 비주얼 프레임 또는 라이다 프레임을 검색하여 상기 프레임 노드에 등록시킨다. 또는 제어부가 라이 다 센서 또는 카메라 센서를 제어하여 새로운 비주얼 프레임/라이다 프레임을 생성하여 생성된 프레 임들을 프레임 노드에 등록시킬 수 있다. 제어부는 현재 로봇의 위치에서 카메라 센서를 제어하여 비전 정보를 생성하고 라이다 센서 를 제어하여 라이다 정보를 생성한다. 그리고 제어부는 생성된 비전 정보를 이용하여 비주얼 키프레임을 생성하고, 생성된 라이다 정보를 이용하 여 라이다 키프레임을 생성한다(S62). 이후 제어부는 해당 위치에서 프레임 노드를 생성하고(S63), 생성한 프레임 노드에 비주얼 키프레임 및 라이다 키프레임을 등록(혹은 결합, associate)시킨다(S64). S61에서 사용자의 수동 등록 상태가 아닌 경우, 제어부는 이전 프레임 노드(vf previous)와 로봇의 현재 위치 (pcurrent)간의 차이에 기반하여 현재 위치를 프레임 노드로 생성할 것인지를 판단한다(S65). 예를 들어, 제어부는 이전 프레임 노드와 새로운 프레임 노드 사이에서 최소한 만족시켜야 하는 거리(Tdis t)와 각도(Tangle)라는 기준을 이용한다. 보다 상세히, 제어부는 이전 프레임 노드의 위치(p(vf previous))와 현재 로봇의 위치(pcurrent)의 차이가 Tdist 보다 크거나 또는 이전 프레임 노드의 위치(p(vf previous))와 현재 로봇의 위치(pcurrent) 사이의 각도가 Tangle 보다 큰지 확인한다. 그리고 S65의 조건을 충족할 경우, 제어부는 해당 위치에서 등록 가능한 비주얼 키프레임 또는 라이다 키프레임이 존재할 경우(S66). 프레임 노드를 생성한다(S67). 그리고 프레임 노드가 생성되면 해당 위치에서 등록 가능한 비주얼 키프레임과 라이다 키프레임을 프레임 노드 에 등록한다(S68). 제어부는 비주얼 키프레임만 프레임 노드에 등록할 수 있다. 또는 제어부는 라이 다 키프레임만 프레임 노드에 등록할 수 있다, 혹은 제어부는 비주얼 키프레임/라이다 키프레임 모두 프레 임 노드에 등록할 수 있다. 도 8의 과정은 로봇이 이동하는 과정에서 반복 실시할 수 있다. 도 8의 과정을 예시적인 유사코드(pseudo cod e)로 구현하면 다음과 같다. S61은 하기의 라인 1에 해당하며, S62~S64는 하기 라인 2~4에 해당한다. S65~S68은 하기 라인 5-18에 해당한다. 보다 상세히, S65는 라인 5~6에, S66은 라인 7에 S67은 라인 8에 해당한다. S68은 라인 12 내지 17에 해당한다. 전술한 과정을 적용하면 로봇은 두 종류의 맵(비전 맵, 라이다 맵)을 각각 생성하고 이들을 백본을 중심으로 조합할 수 있다. 또는 이미 하나의 통합된 백본을 포함한 포즈 그래프에서 필요한 맵(비전 맵 또는 라이다 맵) 을 추출하여 로봇의 SLAM에 적용할 수 있다. 이는 로봇이 어느 하나의 센서만 장착하였거나(220 또는 230) 어느 하나의 센서를 중심으로 SLAM을 수행할 경우 적용가능하다. 따라서, 전술한 맵 생성은 라이다 센서 및 카메라 센서를 모두 포함하는 로봇이 두 종류의 맵(비 전 맵과 라이다 맵)을 생성하여 이들 사이의 키프레임들을 이용하여 백본을 포함한 포즈 그래프를 생성한다. 이후 라이다 센서만을 장착한 로봇에게는 라이다 맵을 설치하고, 카메라 센서만을 장착한 로봇에게는 비전 맵을 설치하여 각각의 상이한 센서를 가진 로봇들이 위치 추정을 수행할 수 있도록 한다. 물론 맵의 특징 에 따라 로봇이 사용할 수 있는 위치추정 시나리오는 다양하게 적용될 수 있다. 도 9는 본 발명의 일 실시예에 의한 포즈 그래프에서 라이다 프레임을 제거한 비주얼 전용 포즈 그래프를 보여 준다. 프레임 노드와 오도메트리 정보, 그리고 비주얼 프레임 및 비주얼 키프레임이 G_Visual이라는 포즈 그래 프를 구성한다. 도 10은 본 발명의 일 실시예에 의한 포즈 그래프에서 비주얼 프레임을 제거한 비주얼 전용 포즈 그래프를 보여 준다. 프레임 노드와 오도메트리 정보, 그리고 라이다 프레임 및 라이다 키프레임이 G_LiDAR 라는 포즈 그래프 를 구성한다. 도 11은 본 발명의 일 실시예에 의한 포즈 그래프를 일부 삭제하는 과정을 보여준다. 제어부는 프레임 노드를 포함하는 포즈 그래프를 메모리에 로딩한다(S71). 그리고 비주얼 전용 포즈 그래 프의 생성인지 아닌지에 따라 분기한다(S72). 비주얼 전용 포즈 그래프의 생성인 경우, 제어부는 로딩한 포즈 그래프에서 백본을 구성하는 프레임 노드에 등록된 라이다 프레임을 제거한다(S73). 그리고 제어부는 포즈 그래프에서 라이다 브랜치를 제거한 비주얼 전용 포즈 그래프를 생성한다(S74). 그 결과는 도 9와 같다. 라이다 센서를 포함하지 않는 로봇의 SLAM 과정에서 제어부가 도 9의 맵을 이용하여 로봇의 위치를 추정할 수 있따. 한편 S71에서 라이다 전용 포즈 그래프의 생성인 경우 제어부는 포즈 그래프에서 백본을 구성하는 프레임 노드에 등록된 비주얼 프레임을 제거한다(S75). 그리고 제어부는 포즈 그래프에서 비주얼 브랜치를 제거한 라이다 전용 포즈 그래프를 생성한다(S76). 그 결과는 도 10과 같다. 카메라 센서를 포함하지 않는 로봇의 SLAM 과정에서 제어부가 도 10의 맵을 이용하여 로봇의 위치를 추정할 수 있다. 도 12는 본 발명의 이용하여 라이다 센서에 기반한 위치추정 과정을 보여준다. 로봇은 카메라 센서를 더 포함할 수 있다. 라이다 센서가 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 제1라이다 프레임을 생성한다 (S81). 그리고 제어부는 맵 저장부에 저장된 프레임 노드를 선택한다(S82). 프레임 노드의 선택은 이 전에 확인된 프레임 노드를 기준으로 로봇이 이동한 오도메트리 정보를 이용할 수 있다. 제어부는 생성한 제1라이다 프레임과 프레임 노드에 등록된 제2라이다 프레임을 비교한다(S83). 비교 결과 에 따라 위치 추정이 가능할 경우(S84) 제어부는 로봇의 현재 위치 추정을 수행한다(S86). 반면 S84에서 위치추정이 불가능한 경우, 제어부는 맵 저장부의 백본에서 새로운 프레임 노드를 검색 하거나 혹은 카메라 센서가 생성한 제1비주얼 프레임을 이용하여 위치 추정을 수행한다(S85). 맵 저장부는 전술한 포즈 그래프를 저장한다. 맵 저장부는 제1라이다 프레임과 비교 가능한 다수의 라이다 프레임들을 포함하는 라이다 브랜치를 저장한다. 또한 맵 저장부는 제1비주얼 프레임과 비교 가능 한 다수의 비주얼 프레임들을 포함하는 비주얼 브랜치를 저장한다. 맵 저장부는 저장된 라이다 프레임 또는 저장된 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프를 저장한다. 또한 맵 저장부는 프레임 노드 사이의 오도메 트리 정보를 저장한다. 로봇이 제공하는 센서의 종류에 따라 맵저장부는 라이다 브랜치를 제거한 포즈 그래프를 저장하거나(도 9), 또는 맵저장부는 비주얼 브랜치를 제거한 포즈 그래프를 저장할 수 있다(도 10). 그리고 제어부는 포즈 그래프의 프레임 노드에 등록된 프레임과 제1라이다 프레임 또는 상기 제1비주얼 프 레임을 비교하여 위치를 추정할 수 있다. 또는 제어부는 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산할 수 있다. 특히, 라이다 센서로 위치 추정이 안될 경우 새로운 라이다 프레임을 추출한다. 예를 들어, 제어부 백본에 서 현재 위치에 대응하는 제1프레임 노드를 검색하여 상기 제1프레임 노드에 등록된 제2라이다 프레임을 추출한 다. 그리고 제어부는 제1라이다 프레임과 제2라이다 프레임을 비교한 결과 상기 두 프레임이 상이한 것으로 판 단할 수 있다. 제어부는 라이다 브랜치에서 제2라이다 프레임에 인접한 제3라이다 프레임을 추출하여 상기 제1라이다 프레임과 제3라이다 프레임을 비교하여 로봇의 현재 위치를 계산할 수 있다. 그럼에도 여전히 라이다 센서를 이용하여 위치 추정에 실패하면, 로봇은 비주얼 프레임을 사용할 수 있다. 즉, 제어부는 제1라이다 프레임과 제3라이다 프레임을 비교한 결과 두 프레임이 상이한 경우, 제어부는 제1프레임 노드에 등록된 제2비주얼 프레임을 추출한다. 그리고 제어부는 제1비주얼 프레임과 제2비주얼 프레임을 비교하여 로봇의 현재 위치를 계산한다. 현재 확인된 프레임 노드를 기준으로 라이다 프레임 또는 비주얼 프레임을 이용한 위치 추정에 실패할 경우, 제 어부는 새로운 프레임 노드를 검색할 수 있다. 예를 들어, 도 4의 구성에서 v2로 판단하였으나 위치 추정 에 실패한 경우 제어부는 인접한 v1 또는 v3를 새로운 프레임 노드로 검색할 수 있다. 또는 제어부는 현재 프레임 노드에 등록된 키프레임 외의 비주얼 프레임 또는 라이다 프레임을 맵 저장부 에서 추출하여 현재 위치에서 획득한 제1라이다 프레임/제1비주얼 프레임과 비교할 수 있다. 도 13은 본 발명의 이용하여 카메라 센서에 기반한 위치추정 과정을 보여준다. 로봇은 라이다 센서를 더 포함할 수 있다. 카메라 센서가 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성한다(S91). 그리고 제어부는 맵 저장부에 저장된 프레임 노드를 선택한다(S92). 프레임 노드의 선택은 이전에 확 인된 프레임 노드를 기준으로 로봇이 이동한 오도메트리 정보를 이용할 수 있다. 제어부는 생성한 제1비주얼 프레임과 프레임 노드에 등록된 제2비주얼 프레임을 비교한다(S93). 비교 결과 에 따라 위치 추정이 가능할 경우(S94) 제어부는 로봇의 현재 위치 추정을 수행한다(S96). 반면 S94에서 위치추정이 불가능한 경우, 제어부는 맵 저장부의 백본에서 새로운 프레임 노드를 검색 하거나 혹은 라이다 센서가 생성한 제1라이다 프레임을 이용하여 위치 추정을 수행한다(S95) 특히, 카메라 센서로 위치 추정이 안될 경우 새로운 비주얼 프레임을 추출한다. 예를 들어, 제어부 백본에 서 현재 위치에 대응하는 제1프레임 노드를 검색하여 상기 제1프레임 노드에 등록된 제2비주얼 프레임을 추출한 다. 그리고 제어부는 제1비주얼 프레임과 제2비주얼 프레임을 비교한 결과 상기 두 프레임이 상이한 것으로 판 단할 수 있다. 제어부는 비주얼 브랜치에서 제2비주얼 프레임에 인접한 제3비주얼 프레임을 추출하여 상기 제1비주얼 프레임과 제3비주얼 프레임을 비교하여 로봇의 현재 위치를 계산할 수 있다. 그럼에도 여전히 카메라 센서를 이용하여 위치 추정에 실패하면, 로봇은 라이다 프레임을 사용할 수 있다. 즉, 제어부는 제1비주얼 프레임과 제3비주얼 프레임을 비교한 결과 두 프레임이 상이한 경우, 제어부는 제1프레임 노드에 등록된 제2라이다 프레임을 추출한다. 그리고 제어부는 제1라이다 프레임과 제2라이다 프레임을 비교하여 로봇의 현재 위치를 계산한다. 현재 확인된 프레임 노드를 기준으로 라이다 프레임 또는 비주얼 프레임을 이용한 위치 추정에 실패할 경우, 제 어부는 새로운 프레임 노드를 검색할 수 있다. 예를 들어, 도 4의 구성에서 v2로 판단하였으나 위치 추정 에 실패한 경우 제어부는 인접한 v1 또는 v3를 새로운 프레임 노드로 검색할 수 있다. 또는 제어부는 현재 프레임 노드에 등록된 키프레임 외의 비주얼 프레임 또는 라이다 프레임을 맵 저장부 에서 추출하여 현재 위치에서 획득한 제1라이다 프레임/제1비주얼 프레임과 비교할 수 있다. 도 12 및 도 13에서 프레임 노드 중에 라이다 프레임 또는 비주얼 프레임이 등록되지 않았거나 위치 추정에 실 패한 경우에, 제어부는 로봇의 현재 위치를 처음 검색한 제1프레임 노드를 기준으로 추정한다. 그리고 제어부는 로봇이 이동했던 경로 중 라이다 프레임 또는 비주얼 프레임이 등록된 제2프레임 노드을 추출하여 제2프레임 노드로부터 제1프레임 노드까지의 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산할 수 있다. 전술한 실시예들을 적용할 경우, 제어부는 비전 정보를 생성하는 카메라 센서와 라이다 정보를 생성하는 라이다 센서를 융합하여 하나의 맵을 생성할 수 있다. 그리고 제어부는 이들 센서들이 센싱한 정보에 기반 하여 도 4와 같이 맵을 생성할 수 있다. 또한 제어부는 맵을 별도로 분리하여 비주얼 프레임에 기반한 맵, 라이다 프레임에 기반한 맵 등으로 분리할 수 있으며, 로봇이 보유한 센서들의 종류에 따라 분리된 맵을 각 각 독립적으로 사용하여 위치추정을 수행할 수 있다. 그 결과 서로 상이한 센서에 기반한 프레임 및 키프레임을 이용하여 고품질의 상호 호완적인 맵을 생성하여 위 치 추정에 적용할 수 있다. 본 발명의 실시예를 구성하는 모든 구성 요소들이 하나로 결합되거나 결합되어 동작하는 것으로 설명되었다고 해서, 본 발명이 반드시 이러한 실시예에 한정되는 것은 아니며, 본 발명의 목적 범위 내에서 모든 구성 요소들 이 하나 이상으로 선택적으로 결합하여 동작할 수도 있다. 또한, 그 모든 구성 요소들이 각각 하나의 독립적인 하드웨어로 구현될 수 있지만, 각 구성 요소들의 그 일부 또는 전부가 선택적으로 조합되어 하나 또는 복수 개 의 하드웨어에서 조합된 일부 또는 전부의 기능을 수행하는 프로그램 모듈을 갖는 컴퓨터 프로그램으로서 구현 될 수도 있다. 그 컴퓨터 프로그램을 구성하는 코드들 및 코드 세그먼트들은 본 발명의 기술 분야의 당업자에 의해 용이하게 추론될 수 있을 것이다. 이러한 컴퓨터 프로그램은 컴퓨터가 읽을 수 있는 저장매체(Computer Readable Media)에 저장되어 컴퓨터에 의하여 읽혀지고 실행됨으로써, 본 발명의 실시예를 구현할 수 있다. 컴 퓨터 프로그램의 저장매체로서는 자기 기록매체, 광 기록매체, 반도체 기록소자를 포함하는 저장매체를 포함한 다. 또한 본 발명의 실시예를 구현하는 컴퓨터 프로그램은 외부의 장치를 통하여 실시간으로 전송되는 프로그램 모듈을 포함한다. 이상에서는 본 발명의 실시예를 중심으로 설명하였지만, 통상의 기술자의 수준에서 다양한 변경이나 변형을 가 할 수 있다. 따라서, 이러한 변경과 변형이 본 발명의 범위를 벗어나지 않는 한 본 발명의 범주 내에 포함되는 것으로 이해할 수 있을 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13"}
{"patent_id": "10-2019-7023289", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 의한 로봇의 외관을 보여준다. 도 2는 본 발명의 일 실시예에 의한 로봇의 제어모듈의 구성 요소를 보여준다. 도 3은 로봇이 공간에서 이동하는 과정을 보여준다. 도 4는 본 발명의 일 실시예에 의한 맵의 다중 구조를 보여준다. 도 5는 본 발명의 일 실시예에 의한 초기 매핑 과정을 보여준다. 도 6은 본 발명의 일 실시예에 의한 비주얼 키프레임을 생성하는 과정을 보여준다. 도 7은 본 발명의 일 실시예에 의한 라이다 키 프레임을 생성하는 과정을 보여준다. 도 8은 본 발명의 일 실시예에 의한 백본의 프레임 노드를 등록하는 과정을 보여준다. 도 9는 본 발명의 일 실시예에 의한 포즈 그래프에서 라이다 프레임을 제거한 비주얼 전용 포즈 그래프를 보여 준다. 도 10은 본 발명의 일 실시예에 의한 포즈 그래프에서 비주얼 프레임을 제거한 비주얼 전용 포즈 그래프를 보여 준다. 도 11은 본 발명의 일 실시예에 의한 포즈 그래프를 일부 삭제하는 과정을 보여준다. 도 12는 본 발명의 이용하여 라이다 센서에 기반한 위치추정 과정을 보여준다. 도 13은 본 발명의 이용하여 카메라 센서에 기반한 위치추정 과정을 보여준다."}
