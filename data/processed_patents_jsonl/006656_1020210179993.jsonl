{"patent_id": "10-2021-0179993", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0090862", "출원번호": "10-2021-0179993", "발명의 명칭": "인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법 및 장치", "출원인": "주식회사 딜리셔스게임즈", "발명자": "김정윤"}}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치가 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법에 있어서,에이전트 별로 서로 다른 게임 레벨에 따른 적어도 하나의 게임 프로젝트를 생성하는 단계;상기 적어도 하나의 게임 프로젝트를 하나의 씬으로 생성하는 단계;상기 하나의 씬 내 적어도 하나의 게임 프로젝트들로부터 획득된 데이터에 기초하여 학습 데이터를 생성하는 단계; 및상기 생성된 학습 데이터에 기초하여 상기 게임 프로젝트를 플레이하는 인공 지능 모델을 복수의 강화 학습 알고리즘들에 따라 학습시키는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 방법은상기 학습된 인공 지능 모델이 게임 프로젝트를 학습하기에 적합한지 여부에 대한 성능 점수를 결정하는 단계;및상기 결정된 성능 점수에 기초하여 상기 복수의 강화 학습 알고리즘들 중 하나의 강화 학습 알고리즘에 따라 학습된 인공 지능 모델을 식별하는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 방법은상기 학습된 인공 지능 모델을 이용하여 상기 적어도 하나의 게임 프로젝트를 실행하는 단계;상기 게임 프로젝트의 실행 결과에 기초하여 상기 적어도 하나의 게임 프로젝트의 스테이지 별 난이도 밸런스를결정하는 단계; 및상기 학습된 인공 지능 모델을 이용하여, 상기 결정된 스테이지 별 난이도 밸런스를 가지는 적어도 하나의 게임프로젝트를 재 실행하는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 적어도 하나의 게임 프로젝트는 매치 3 게임 프로젝트를 포함하는 것을 특징으로 하는,방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 매치 3 게임 프로젝트는일반 블록 타입 또는 특수 블록 타입 중 하나의 타입으로 분류될 수 있는 서로 다른 블록들을 포함하고, 상기특수 블록 타입은 상기 일반 블록 타입 보다 더 높은 보상 값을 가지며, 상기 블록들은 2차원 필드에서 생성,이동 및 삭제되는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 적어도 하나의 게임 프로젝트를 생성하는 단계는상기 전자 장치에 대한 제1 사용자 입력에 기초하여 블록들을 생성 하는 단계;상기 생성된 블록들 중 적어도 하나의 블록을 선택하는 제2 사용자 입력을 식별하는 단계;상기 식별된 제2 사용자 입력에 기초하여 스와이프 움직임 모션을 식별하는 단계;공개특허 10-2023-0090862-3-상기 식별된 스와이프 움직임 모션이 유효한지 여부를 결정하는 단계;상기 스와이프 움직임 모션이 유효한 것으로 식별되면, 상기 선택된 적어도 하나의 블록의 타입이 동일한지 여부를 결정하는 단계;상기 선택된 적어도 하나의 블록의 타입이 동일한 타입으로 식별되면, 상기 선택된 적어도 하나의 블록을 삭제하는 단계; 및상기 삭제된 적어도 하나의 블록의 상부 방향으로 인접한 블록들 중 일부를 상기 삭제된 적어도 하나의 블록들의 위치로 이동시키는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 방법은상기 스와이프 움직임 모션이 유효하지 않은 것으로 식별되면, 상기 생성된 블록들 중 적어도 하나의 블록을 선택하는 사용자 입력을 다시 획득하는 단계; 를 더 포함하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서, 상기 방법은상기 선택된 적어도 하나의 블록의 타입이 동일하지 않은 것으로 식별되면, 상기 생성된 블록들 중 적어도 하나의 블록을 선택하는 사용자 입력을 다시 획득하는 단계; 를 더 포함하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제2항에 있어서, 상기 학습 시키는 단계는적어도 하나의 클래스를 통해 상기 생성된 학습 데이터를 상기 인공 지능 모델로 전달하는 단계;상기 인공 지능 모델로 전달된 학습 데이터를 강화 학습함으로써, 상기 인공 지능 모델을 학습시키는 단계; 를포함하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 학습 시키는 단계는상기 학습 데이터에 기초하여, 휴리스틱(heuristic) 학습 방식 또는 관측(observation) 학습 방식 중 적어도 하나에 따라 상기 인공 지능 모델을 강화 학습 시키는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 관측 학습 방식은 벡터 관측(VECTOR OBSERVATION) 또는 시각 관측(VISUAL OBSERVATION) 중 적어도 하나를포함하고,상기 휴리스틱 학습 방식은 심플 휴리스틱(SIMPLE HEURISTICE) 또는 그리디 휴리스틱(GREEDY HEURISTICE) 중적어도 하나를 포함하는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제2항에 있어서, 상기 인공 지능 모델은 유니티 3D 엔진(Unity 3D Engine) 에서 이용 가능한 유니티 머신러닝 에이전트(Unity ML Agent) 모델인 것을특징으로 하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제2항에 있어서, 상기 성능 점수를 결정하는 단계는상기 학습된 인공 지능 모델의 학습 효율에 관한 누적 보상 값을 결정하는 단계;공개특허 10-2023-0090862-4-상기 학습된 인공 지능 모델의 학습 안정도에 관한 엔트로피 수치를 결정하는 단계; 및상기 결정된 누적 보상 값 및 엔트로피 수치에 기초하여 상기 성능 점수를 결정하는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 전자 장치에 있어서,하나 이상의 인스트럭션을 저장하는 메모리; 및상기 하나 이상의 인스트럭션을 실행하는 적어도 하나의 프로세서; 를 포함하고,상기 적어도 하나의 프로세서는 상기 하나 이상의 인스트럭션을 실행함으로써,에이전트 별로 서로 다른 게임 레벨에 따른 적어도 하나의 게임 프로젝트를 생성하고,상기 적어도 하나의 게임 프로젝트를 하나의 씬으로 생성하고,상기 하나의 씬 내 적어도 하나의 게임 프로젝트들로부터 획득된 데이터에 기초하여 학습 데이터를 생성하고,상기 생성된 학습 데이터에 기초하여 상기 게임 프로젝트를 플레이하는 인공 지능 모델을 복수의 강화 학습 알고리즘들에 따라 학습시키는, 전자 장치."}
{"patent_id": "10-2021-0179993", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "전자 장치가 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법에 있어서,에이전트 별로 서로 다른 게임 레벨에 따른 적어도 하나의 게임 프로젝트를 생성하는 단계;상기 적어도 하나의 게임 프로젝트를 하나의 씬으로 생성하는 단계;상기 하나의 씬 내 적어도 하나의 게임 프로젝트들로부터 획득된 데이터에 기초하여 학습 데이터를 생성하는 단계; 및상기 생성된 학습 데이터에 기초하여 상기 게임 프로젝트를 플레이하는 인공 지능 모델을 복수의 강화 학습 알고리즘들에 따라 학습시키는 단계; 를 포함하는, 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록 매체."}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법 및 이를 수행하는 전자 장치에 관 한 것이다. 일 실시 예에 따른 전자 장치가 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법은 에이전트 별로 서로 다른 게임 레벨에 따른 적어도 하나의 게임 프로젝트를 생성하는 단계; 상기 적어도 하나의 게임 프로젝트를 하나의 씬으로 생성하는 단계; 상기 하나의 씬 내 적어도 하나의 게임 프로젝트들로부터 획득된 데이터에 기초하여 학습 데이터를 생성하는 단계; 및 상기 생성된 학습 데이터에 기초하여 상기 게임 프로젝트를 플레이하는 인공 지능 모델을 복수의 강화 학습 알고리즘들에 따라 학습시키는 단계; 를 포함할 수 있다."}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법 및 장치에 관한 것이다. 보다 상 세하게는 게임 학습 에이전트를 위한 강화 학습 알고리즘과 이에 기초하여 학습된 인공 지능 모델을 기반으로 게임 컨텐츠를 관리하는 방법 및 이를 수행하는 전자 장치에 관한 것이다."}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "스마트폰이 보급되기 시작하면서 많은 사람이 쉽게 게임을 접하게 되었으며 인식 또한 상당 부분 개선되었다. 그 중 모바일 퍼즐 게임 장르는 쉽게 게임을 이해할 수 있는 게임 규칙을 가지며, 플레이에 대한 시간 부담이 적어 남녀노소 즐길 수 있는 게임으로 대중적인 인기를 확보하고 있다. 모바일 퍼즐 게임 장르를 대표하는 게임 들 중, 매치3 게임은 같은 색 블록을 3개 이상 맞추어 점수 및 게임 목표를 향해 진행하는 비교적 간단한 규칙 을 가진 게임이다. 매치 3게임은 타일매칭 게임 장르 중 하나로 국내 게임 종합 순위 100위 안에 6개의 매치3 게임이 포함되어 있고, 해외 역시 매치 3 게임은 높은 순위를 차지하고 있다. 매치 3 게임은 인접한 두 블록의 위치를 교환하여 같은 형태의 블록이 3개 이상 이어지게 맞춰 없애는 방식으로 진행되는 퍼즐 게임이며, 미국 시장에서 높은 인앱 구매 수익으로 타 장르를 넘어 상위권을 차지하고 있다. 매 치3 퍼즐 게임은 스테이지 클리어라는 단 한가지의 목적만 가지는 게임이기 때문에 플레이 하는 유저의 유입과 유지를 위해서 정교하게 레벨 디자인된 스테이지를 빠르게 업데이트 하는 것이 중요하다. 다량의 스테이지 제작 도 보다 더 중요한 것은 정교하게 구성된 레벨 디자인을 가진 스테이지 단계이며 여러 단계의 스테이지를 빠르 고 정교하게 레벨 디자인하는 것은 매치 3 퍼즐 게임을 개발하고 있는 많은 개발사의 개발 목표이다.게임 산업에서 인공 지능 분야는 가장 중요도가 높은 분야이며 자동 플레이 자동 플레이, NPC(Non-Player Character)의 인공 지능, 데이터 분석, 자동 테스트 등 다방면으로 연구되고 있다. 특히 NPC의 인공 지능과 레 벨 디자인을 위한 자동 플레이 인공 지능은 게임의 몰입감에 직접적인 영향을 주기 때문에 게임 산업에서 주목 하고 있으며, 어떻게 하면 인공 지능이 정확하고 효율적인 상황 판단을 선택 할 수 있게 만들 수 있는지에 대한 연구가 꾸준히 진행되어오고 있다 매치 3 게임 장르에서도 정교한 레벨 디자인을 도와줄 수 있는 자동 플레이 인공 지능을 개발하기 위한 연구들 이 개발되고 있으나, 게임 장르를 학습하는데 적합한 알고리즘에 대한 기준이 불분명하며, 적합한 성능의 알고 리즘과 이에 기초하여 학습된 인공 지능 모델의 성능은 아직 한계가 있다. 따라서, 인공 지능 모델을 이용하여 게임 컨텐츠의 난이도를 효과적으로 관리하기 위한 기술 개발이 요구되고 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제2030942호"}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "일 실시 예에 따르면, 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법 및 이를 수행하는 전 자 장치가 제공될 수 있다. 일 실시 예에 의하면 정교한 레벨 디자인에 사용되기 위해 게임 컨텐츠를 자동으로 플레이하는 인공 지능 모델 및 이를 이용한 게임 컨텐츠의 난이도를 관리하는 방법이 제공될 수 있다."}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 본 개시의 일 실시 예에 따라, 전자 장치가 인공 지능 모델을 기반으로 게 임 컨텐츠의 난이도를 관리하는 방법에 있어서, 에이전트 별로 서로 다른 게임 레벨에 따른 적어도 하나의 게임 프로젝트를 생성하는 단계; 상기 적어도 하나의 게임 프로젝트를 하나의 씬으로 생성하는 단계; 상기 하나의 씬 내 적어도 하나의 게임 프로젝트들로부터 획득된 데이터에 기초하여 학습 데이터를 생성하는 단계; 및 상기 생 성된 학습 데이터에 기초하여 상기 게임 프로젝트를 플레이하는 인공 지능 모델을 복수의 강화 학습 알고리즘들 에 따라 학습시키는 단계; 를 포함하는, 방법이 제공될 수 있다. 상술한 기술적 과제를 달성하기 위한 또 다른 실시 예에 의하면, 인공 지능 모델을 기반으로 게임 컨텐츠의 난 이도를 관리하는 전자 장치에 있어서, 하나 이상의 인스트럭션을 저장하는 메모리; 및 상기 하나 이상의 인스트 럭션을 실행하는 적어도 하나의 프로세서; 를 포함하고, 상기 적어도 하나의 프로세서는 상기 하나 이상의 인스 트럭션을 실행함으로써, 에이전트 별로 서로 다른 게임 레벨에 따른 적어도 하나의 게임 프로젝트를 생성하고, 상기 적어도 하나의 게임 프로젝트를 하나의 씬으로 생성하고, 상기 하나의 씬 내 적어도 하나의 게임 프로젝트 들로부터 획득된 데이터에 기초하여 학습 데이터를 생성하고, 상기 생성된 학습 데이터에 기초하여 상기 게임 프로젝트를 플레이하는 인공 지능 모델을 복수의 강화 학습 알고리즘들에 따라 학습시키는, 전자 장치가 제공될 수 있다. 상술한 기술적 과제를 달성하기 위한 또 다른 실시 예에 의하면, 전자 장치가 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법에 있어서, 에이전트 별로 서로 다른 게임 레벨에 따른 적어도 하나의 게임 프 로젝트를 생성하는 단계; 상기 적어도 하나의 게임 프로젝트를 하나의 씬으로 생성하는 단계; 상기 하나의 씬 내 적어도 하나의 게임 프로젝트들로부터 획득된 데이터에 기초하여 학습 데이터를 생성하는 단계; 및 상기 생 성된 학습 데이터에 기초하여 상기 게임 프로젝트를 플레이하는 인공 지능 모델을 복수의 강화 학습 알고리즘들 에 따라 학습시키는 단계; 를 포함하는, 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록 매체가 제공될 수 있다."}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "일 실시 예에 의하면, 게임 컨텐츠에 대한 레벨 밸런싱 테스트를 효과적으로 수행할 수 있다. 일 실시 예에 의하면, 에이전트 구현에 가장 최적화된 강화 학습 알고리즘에 기초하여 학습된 인공 지능 모델을 이용함으로써 최적의 게임 난이도를 설정할 수 있다."}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 개시에 대해 구체적으로 설명하기로 한다. 본 개시에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상세 히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지 는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"...부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였 다. 도 1은 일 실시 예에 따른 전자 장치가 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 개략적인 과정을 나타내는 도면이다. 일 실시 예에 의하면, 전자 장치는 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리할 수 있다. 예를 들어, 전자 장치는 인공 지능 모델을 활용하여 자동으로 대상 게임 컨텐츠들에 대한 레벨 밸런싱 테 스트를 수행할 수 있다. 일 실시 예에 의하면, 전자 장치는 하나 이상의 인스트럭션을 저장하는 메모리 및 하나 이상의 인스트럭션을 실행하는 적어도 하나의 프로세서를 포함하고, 프로세서는 상기 하나 이상의 인스트럭션을 실행함으로써, 메모리에 저장된 인공 지능 모델을 학습시키고, 학습된 인공 지능 모 델을 이용하여 대상 게임 컨텐츠의 난이도를 관리할 수 있다. 머신러닝 기술이 발달하면서 다양한 게임 장르들이 인공 지능 영역으로 들어와 강화 학습을 활용한 개발을 시작 하는 예들이 개시되고 있으며, 최근 매치 3 퍼즐 게임 장르에 대해 MCTS(Monte Carlo Tree Search) 알고리즘 또 는 MCTS 알고리즘 및 DNN(Deep Neural Network)를 함께 사용함으로써 매치 3 퍼즐 게임을 강화 학습을 하는 예 들이 개시되고 있다. 본 개시에 따른 전자 장치는 게임 프로젝트 데이터를 획득하고, 획득된 게임 프로젝트 데이터 를 인공 지능 모델을 통하여 강화 학습함으로써 소정의 게임 컨텐츠(예컨대 매치 3 퍼즐 게임)를 위한 강화 학 습 에이전트를 생성하며, 소정의 강화 학습 알고리즘의 비교 분석 결과에 기초하여 강화 학습 효율이 가장 높은 강화 학습 에이전트를 결정할 수 있다. 일 실시 예에 의하면, 전자 장치가 이용하는 인공 지능 모델은 게임 엔진인 유니티 3D 엔진(Unity 3D Engine)에서 이용 가능한 유니티 ML 에이전트(Unity ML Agents)로 마련 될 수 있으나, 이에 한정되는 것은 아니다. 일 실시 예에 의하면 유니티 3D 엔진(Unity 3D Engine)은 2D 게임, 3D 게임 등에 국한되지 않고, 애니메이션, 시뮬레이션, 건축 등 여러 분야의 개발이 가능하게 해주는 게임 엔진으로, 모바일, PC 콘솔, 가상현실, 증강현 실 등 다양한 플랫폼에서의 플레이를 위한 멀티 플랫폼을 제공할 수 있다. 또한, 일 실시 예에 의하면, 유니티 ML 에이전트(Unity ML Agents)는 유니티 3D 엔진(Unity 3D Engine) 기반의 프로젝트에서 구현 가능한 머신러닝 SDK로써, 유니티 머신러닝 SKL를 활용하여 머신러닝을 구성하면 다른 머신 러닝 SDK 보다, 유니티 엔진으로 제 작한 게임이나 시뮬레이션에 쉽게 심층 강화 학습을 사용할 수 있게 하는 효과가 있다. 일 실시 예에 의하면, 전자 장치는 PPO 알고리즘을 활용한 강화 학습 방식 중 두가지 휴리스틱 (Heuristic) 학습 방식과 두가지의 관측(Observation) 학습 방식을 이용하여 인공 지능 모델을 학습시킬 수 있 으나, 이에 한정되는 것은 아니다. 본 개시에 따른 전자 장치는 소정의 학습 알고리즘에 기초하여 생성되 는 인공 지능 모델들에 대한 성능 점수를 결정하고, 성능 점수에 기초하여 하나의 인공 지능 모델 을 식별할 수 있다. 예를 들어, 전자 장치는 인공 지능 모델의 성능을 누적 보상값 또는 엔트로피 수치로 식별할 수 있으나, 이에 한정되는 것은 아니며, 기타 인공 지능 모델의 성능을 평가하기 위한 기타 지표 들을 활용할 수 있다. 또한, 전자 장치는 선택된 하나의 인공 지능 모델을 이용하여 소정의 게임 컨텐츠를 실행하고, 게임 컨텐 츠의 스테이지를 진행함에 따른 난이도 밸런스 정보 및 게임 플레이 결과에 대한 정보를 획득할 수 있다. 전자 장치는 인공 지능 모델 성능 점수, 난이도 밸런스 정보 및 게임 플레이 결과 를 전자 장치와 연결된 외부 디바이스로 전송할 수도 있다. 전자 장치는 도출한 인공 지능 모델 성능 점 수, 난이도 밸런스 정보 또는 게임 플레이 결과에 대한 정보 중 적어도 하나에 기초하여, 대상 게임 컨텐츠의 레벨 밸런스를 최적화할 수 있다. 본 개시에 따른 전자 장치는 학습 결과를 토대로 추후 매치 3 게임을 학습함으로써 종래 방식 보다 향상 된 속도로 스테이지를 검증할 수 있고, 기존의 매치 3 게임의 QA(Quality Assurance) 인건비를 줄일 수 있으며, 시간적 비용을 줄임으로써 경제적 효과를 향상시키고, 더 높은 수준의 맵 퀄리티 및 게임 컨텐츠를 제공할 수 있도록 한다. 도 2는 일 실시 예에 따른 전자 장치가 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법의 흐 름도이다. S210에서, 전자 장치는 에이전트 별로 서로 다른 게임 레벨에 따른 적어도 하나의 게임 프로젝트를 생성 할 수 있다. 일 실시 예에 의하면, 전자 장치가 생성하는 적어도 하나의 게임 프로젝트는 균형 유지 예제 게임, 점프 캐치 게임, 오브젝트 획득 게임, 타일 매칭 게임, 길찾기 게임, 테니스 라켓 게임, 매치 3 게임 중 적어도 하나를 포함할 수 있으나, 이에 한정되는 것은 아니며, 전자 장치가 인공 지능 모델을 기반으로 테스트할 수 있는 기타 게임 컨텐츠들을 포함할 수 있음은 물론이다. 일 실시 예에 의하면, 전자 장치는 인공 지능 모델을 학습시키기 위한 구성에 맞추어 게임 프로젝트를 구현할 수 있다. 일 실시 예에 의하면, 게임 프로젝트가 매치 3 게임 프로젝트를 포함하는 경우, 매치 3 게임프로젝트는 복수의 블록들을 매칭함으로써 진행되는 게임일 수 있다. 일 실시 예에 의하면, 매치 3 게임 프로젝 트는 일반 블록 타입 또는 특수 블록 타입 중 하나의 타입으로 분류될 수 있는 서로 다른 블록들을 포함하고, 상기 특수 블록 타입은 상기 일반 블록 타입 보다 더 높은 보상 값을 가지며, 상기 블록들은 2차원 필드(예컨대 X*Y 필드)에서 생성, 이동 및 삭제될 수 있다. 예를 들어, 블록들은 사라진 빈 블록들을 위에서 아래로 채워지 는 형태로 이동할 수 있다. 또한, 일 실시 예에 의하면, 전자 장치는 전자 장치에 대한 제1 사용자 입력에 기초하여 매치 3 게임 프 로젝트에 사용되는 복수의 블록들을 생성할 수 있다. 또한, 전자 장치는 생성된 블록들 중 적어도 하나의 블록을 선택하는 제2 사용자 입력을 식별할 수 있다. 예를 들어, 전자 장치는 블록들을 전자 장치의 화면 상에 표시하고, 화면상에서 표시된 블록들을 터치하는 제2 사용자 입력을 식별할 수 있다. 전자 장치는 제2 사용자 입력에 기초하여, 스와이프 움직임 모션을 식별할 수 있다. 예를 들어, 전자 장 치는 생성된 블록들을 선택하는 사용자 터치 입력이 식별된 위치에 기초하여, 전자 장치 화면상에서 이동 하는 사용자의 스와이프 움직임 모션을 식별할 수 있다. 전자 장치는 스와이프 움직임 모션이 유효한지 여부를 결정하고, 스와이프 움직임 모션이 유효한 것으로 식별되면, 선택된 적어도 하나의 블록의 타입이 동일 한지 여부를 식별할 수 있다. 전자 장치는 선택된 적어도 하나의 블록의 타입이 동일한 타입으로 식별되면, 선택된 적어도 하나의 블록 을 삭제할 수 있다. 전자 장치는 삭제된 적어도 하나의 블록의 상부 방향으로 인접한 블록들 중 일부를 상기 삭제된 적어도 하나의 블록들의 위치로 이동시킬 수 있다. 일 실시 예에 의하면, 전자 장치는 스와 이프 움직임 모션이 유효하지 않은 것으로 식별되면 생성된 블록들 중 적어도 하나의 블록을 선택하는 사용자 입력을 다시 획득할 수도 있다. 또한, 일 실시 예에 의하면, 전자 장치는 선택된 적어도 하나의 블록의 타입이 동일하지 않은 것으로 식 별되면, 생성된 블록들 중 적어도 하나의 블록을 선택하는 사용자 입력을 다시 획득할 수도 있다. 본 개시에 따 른 전자 장치는 상술한 방법에 따라 제공 가능한 매치 3 게임 프로젝트를 복수개 생성할 수 있다. S220에서, 전자 장치는 적어도 하나의 게임 프로젝트를 하나의 씬으로 생성할 수 있다. 예를 들어, 도 1 에 도시된 바와 같이, 전자 장치는 적어도 하나의 게임 프로젝트를 하나의 씬으로 생성함으로써, 인공 지 능 모델이 동시에 많은 에이전트들로 학습될 수 있도록 할 수 있다. S230에서, 전자 장치는 하나의 씬 내 적어도 하나의 게임 프로젝트들로부터 획득된 데이터에 기초하여 학 습 데이터를 생성할 수 있다. 예를 들어, 전자 장치는 소정의 매치 3 퍼즐 게임 클래스들을 생성하고, 생 성된 클래스들을 통하여 적어도 하나의 게임 프로젝트들로부터 데이터를 획득하고, 획득된 데이터에 기초하여 학습 데이터를 생성할 수 있다. 전자 장치가 이용하는 매치 3 퍼즐 게임 클래스는 기초 클래스, 중추가 되는 클래스를 상속받는 클래스를 포함할 수 있으며 매치 3 퍼즐 게임 클래스를 통하여 인공 지능 모델과 게임 프로젝트들을 연결하며, 학습 데이터를 전달할 수 있다. S240에서, 전자 장치는 학습 데이터에 기초하여 게임 프로젝트를 플레이 하는 인공 지능 모델을 복수의 강화 학습 알고리즘들에 따라 학습시킬 수 있다. 예를 들어, 전자 장치는 적어도 하나의 클래스를 통해 생성된 학습 데이터를 인공 지능 모델로 전달할 수 있고, 전달된 학습 데이터를 강화 학습함으로써 인공 지능 모델을 학습시킬 수 있다. 일 실시 예에 의하면, 전자 장치는 SAC 알고리즘 또는 PPO 알고리즘 중 적어도 하나에 기초하여 인공 지능 모델을 강화학습 시킬 수 있다. 또한, 일 실시 예에 의하면, 전자 장치가 이 용하는 강화 학습 알고리즘은 휴리스틱(Heuristic) 학습 방식 또는 관측(Observation) 학습 방식 중 적어도 하 나를 포함할 수 있고, 관측 학습 방식은 벡터 관측(VECTOR OBSERVATION) 또는 시각 관측(VISUAL OBSERVATION) 중 적어도 하나를 포함하며, 휴리스틱 학습 방식은 심플 휴리스틱(SIMPLE HEURISTICE) 또는 그리디 휴리스틱 (GREEDY HEURISTICE) 중 적어도 하나를 포함할 수 있다. 일 실시 예에 의하면, 본 개시에 따른 전자 장치는 SAC 알고리즘과 PPO 알고리즘에서 4가지 학습 방식(관 측 학습 방식인 Vector Observation 방식과 Visual Observation 방식 2가지와 모방 학습 방식인 Simple Heuristic 방식과 Greedy Heuristic 방식 2가지)에 기초하여 각각 50,0000회씩 10회 학습을 진행함으로써 복수 의 강화 학습된 인공 지능 모델을 생성할 수 있다. 도 3은 또 다른 실시 예에 따른 전자 장치가 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법 의 흐름도이다. S310 내지 S340은 도 2에 도시된 S210 내지 S240에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S350 에서, 전자 장치는 학습된 인공 지능 모델이 게임 프로젝트를 학습하기에 적합한지 여부에 대한 성능 점 수를 결정할 수 있다. 예를 들어, 전자 장치는 복수의 강화 학습 알고리즘에 기초하여 인공 지능 모델을 학습시킴으로써 복수의 강화 학습된 인공 지능 모델을 생성할 수 있고, 생성된 인공 지능 모델들로부터 획득되 는 결과 데이터로부터 횟수와 시간을 기준으로 누적 보상 수치 또는 엔트로피 값을 획득할 수 있다. 전자 장치 는 누적 보상 수치 또는 엔트로피 값 중 적어도 하나에 기초하여 인공 지능 모델 별 성능 점수를 결정하 고, 성능 점수에 기초하여 인공 지능 모델에 대한 성능을 평가할 수 있다. 보다 상세하게는, 전자 장치는 학습된 인공 지능 모델의 학습 효율에 관한 누적 보상 값을 결정하고, 학 습된 인공 지능 모델의 학습 안정도에 관한 엔트로피 수치를 결정할 수 있다. 전자 장치는 누적 보상 값 및 엔트로피 수치에 기초하여 인공 지능 모델 별 성능 지표에 관한 성능 점수를 결정할 수 있다. S360에서, 전자 장치는 성능 점수에 기초하여 복수의 강화 학습 알고리즘들 중 하나의 강화 학습 알고리 즘에 따라 학습된 인공 지능 모델을 식별할 수 있다. 일 실시 예에 의하면, 전자 장치는 가장 높은 성능 점수를 나타내는 인공 지능 모델을 식별할 수 있고, 식별된 인공 지능 모델을 이용하여 대상 게임 컨텐츠에 대 한 밸런스 테스트를 진행할 수 있다. 도 4는 일 실시 예에 따른 전자 장치가 학습된 인공 지능 모델을 이용하여 게임 프로젝트의 스테이지별 난이도 밸런스를 결정하는 과정을 설명하기 위한 도면이다. S410에서, 전자 장치는 학습된 인공 지능 모델을 이용하여 적어도 하나의 게임 프로젝트를 실행할 수 있 다. S420에서, 전자 장치는 게임 프로젝트의 실행 결과에 기초하여 적어도 하나의 게임 프로젝트의 스테 이지 별 난이도 밸런스를 결정할 수 있다. S430에서, 전자 장치는 학습된 인공 지능 모델을 이용하여, 결 정된 스테이지 별 난이도 밸런스를 가지는 적어도 하나의 게임 프로젝트를 재 실행할 수 있다. 전자 장치(100 0)는 대상 게임 컨텐츠 내 스테이지 별 난이도 밸런스가 소정의 목표 값에 도달할 때까지 S410 내지 S430 과정 을 반복함으로써 게임 내 스테이지의 난이도 밸런스를 최적화할 수 있다. 도 5는 일 실시 예에 따른 전자 장치가 이용하는 게임의 클래스 다이어그램을 나타내는 도면이다. 도 5에 도시된 그림 및 그림 을 참조하면 매치 3 게임 프로젝트에 대한 게임 클래스 다이어그램이 도시된다. 전자 장치는 도 5에 도시된 게임 클래스 다이어그램을 이용하여 매치 3 게임 프로젝트와 인공 지능 모델을 연결하고 학습 데이터를 전달할 수 있다. 일 실시 예에 의하면, 그림 의 AbstactBoard 클래스는 ML-Agents와 매치3 게임 사이의 다리 역할을 수행하 며, 전자 장치 내 인공 지능 모델은, 도 5에 도시된 게임 클래스 다이어그램을 통하여 전달된 학습 데이 터에 기초하여 다음과 같은 작업을 수행할 수 있다. 예를 들어, 전자 장치는 인공지능 모델이 Cell의 타 입 값과 특수 블록인지 확인하고 이동 가능한 상태인지 확인하도록 한다. 또한, 이동 가능 상태의 Cell을 이동 하도록 프로젝트에 요청할 수 있다. 이와 같은 작업은 GetCellType(), IsMoveValid(), MakeMove() 함수를 재정 의함으로써 구현될 수 있다. 인공 지능 모델은 AbstractBoard 클래스를 이용하여, 보드가 보유한 행, 열 및 잠 재적인 Cell 타입의 수를 추적할 수 있다. 각 함수에 대한 상세 기능은 다음과 같다. GetCellType()은 지정된 행과 열의 Cell 타입을 반환하고, 반환되는 값은 0에서 NumCellTypes ?? 1을 포함하는 값 사이의 값일 수 있으며, 값의 실제 순서는 크게 중요하지 않다. GetSpecialType()는 지정된 행과 열의 Cell 타입이 특수 블록 타입을 반환하며, 반환되는 값은 0에서 NumSpecialType을 포함하는 값 사이일 수 있고, 값의 실제 순서 또한 크게 중요하지 않다. IsMoveValid()는 특 정 Move가 게임에서 이동 가능한지 여부를 확인하고, 실제 결과는 게임의 규칙에 따라 다르게 정의될 수 있지만 특별한 부분이나 움직일 수 없는 부분을 사용하지 않는 기본적인 매치3 퍼즐 게임 규칙을 처리하는 SimpleMoveValid() 함수를 이용할 수도 있다. MakeMove(): 주어진 이동을 실행하도록 하는 함수이다. 이동이 제 대로 실행됐을 경우 true 값을 반환할 수 있다. 학습 중 유효하지 않은 이동이 계속 요구되지 않도록 유의하여 작성되어야 하며, 이런 상황이 발생했을 경우 아무것도 하지 않고 다른 이동을 요구해도 안전하도록 구현될 수 있다. Move 구조체는 2개의 인접 Cell의 이동을 캡슐화하는 구조체이며, Move.NumPotentialMoves(NumRows, NumColumns)를 사용하여 특정 크기의 보드의 잠재적 Move 수를 얻을 수 있다. 새로운 Move를 만들기 위한 2개의 함수는 다음과 같이 구현될 수 있다. 예를 들어, FromMoveIndex()는 0에서 Move.NumPotentialMoves() 함수로 반복함으로써 보드의 모든 잠재적인 이동을 반복하기 위해 사용될 수 있고, FromPositionAndDirection()는 행, 열, 방향 및 보드 크기를 통한 이동을 담당할 수 있다.Match3Sensor 클래스는 AbstractBoard 인터페이스를 사용하여 상태에 대한 관찰지를 생성할 수 있고, Vector Observation 방식과 Visual Observation 방식 중 어느 방식을 사용할지 선택할 수 있다. 이론적으로는 Visual Observation 방식은 매치3 퍼즐 게임의 보드가 2차원이기 때문에 향상된 성능을 보여줄 수 있다. Match3SensorComponent 클래스는 실행 시 Match3Sensor 클래스 변수를 생성하는 함수이며. 이 클래스 스크립트 는 구현된 에이전트 스크립트와 같은 GameObject에 추가함으로써 해당 기능들을 사용할 수 있게 구현될 수 있다. Match3Actuator 클래스는 학습 또는 추론을 통한 행동을 AbstarctBoard.MakeMove() 함수로 전달하는 인자값인 Move로 변환시키는 기능을 담당하고, 잠재적인 이동마다 AbstractBoard.IsMoveValid() 함수를 체크하고 이를 사 용하여 에이전트의 행동 마스크를 설정할 수 있다. Match3ActuatorComponent 클래스는 실행 시 Match3Actuator 클래스 변수를 생성하는 함수이며, 이 클래스 스크 립트는 구현된 에이전트 스크립트와 같은 GameObject에 추가함으로써 해당 기능들을 사용할 수 있게 구현될 수 있다. 결론적으로 전자 장치는 AbstractBoard 클래스를 구현 사항을 상속받는 Match3Board 클래스를 작성 하여 게임 학습 환경에 통합되도록 할 수 있으며, 에이전트가 복수의 블록을 연속해서 매치하거나 특수 블록의 타입을 가진 블록을 매치하는 것과 같은 특정 행동에 보상 값을 얻을 수 있도록 설정할 수 있다. Agent와 AbstractBoard 클래스를 상속하는 클래스 스크립트와 Match3SensorComponent, Match3ActuatorComponent 클래스 스크립트를 같은 GameObject에 추가하여 구성될 수 있고, 에이전트가 다음 아카데미 단계로 넘어갈 준비가 되면 Agent.RequestDecision() 함수를 호출하고 다음 아카데미 단계에서는 보드의 MakeMove() 함수를 호출하게 마련 될 수 있다. 도 6은 일 실시 예에 따른 전자 장치가 이용하는 SAC 알고리즘과 PPO 알고리즘을 설명하기 위한 도면이다. 전자 장치는 SAC 알고리즘 또는 PPO 알고리즘 중 적어도 하나에 기초하여 인공 지능 모델을 강화 학습할 수 있다. 일 실시 예에 의하면 SAC 알고리즘은 Soft Actor Critic 알고리즘으로서, 보상에 정책(Policy)에 대한 엔트로피 측정(Entropy measure)을 추가하여 에이전트가 효율적인 탐구(Exploration)를 할 수 있게 만든 기법이 다. SAC 알고리즘은 가능한 무작위로 행동하며 Task를 잘 수행하도록 학습하는 정책을 세워야 하며, 이 기법은 최대 엔트로피 강화 학습 환경을 정책 외 Actor-Critic 모델에 추가한 알고리즘 기법이다. Soft Q-Learning 알 고리즘으로 진행되었던 연구가 더 발전하여 지금의 형태를 나타나게 된 것이며, Q-Learning은 Multimodal Q- Function을 기본으로 하고 있으며, 그림 에 도시된 바와 같다. SAC 알고리즘의 3가지 구성 요소는 다음과 같다. 가치 기능 가치 기능 네트워크와 정책 기능 네트워크가 분리된 구성으로 이루어진 Actor-Critic 구조, 이전 수집 데이터를 재사용이 가능하도록 만들어진 보상 정책 외의 공식, 안정성을 유지하며 조사를 진행하도록 하는 엔트로피 최대화이다. 정책은 목적 함수(Objective function)를 통해 엔트로피와 예상 수익을 동시에 최대화하는 방향으로 학습될 수 있따. 여기서 정책은 최적의 정책을 뜻하고, 시간적 횟수를 보상하는 함수와 할인율 그리고 상태를 뜻한다. 하 기 수학식 1에서 t는 시간 단계의 동작을 수식화한 것이고 정책으로 인해 유도된 궤적 분포를 나타낸 것이다. 수학식 1"}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "엔트로피 항에서 보상과 비교하여 상대적 중요성을 결정하고 파라미터는 상태정책의 엔트로피로 계산된다. 최대 엔트로피 프레임 워크 내에서 소프트 정책을 사용해 정책 평가와 정책 개선을 번갈아 사용하여 목표를 최대화한 다. 정책의 값을 계산하는 과정이 정책 평가 단계에 포함된다. 이를 위해 소프트 상태 값 기능을 다음 하기의 수학식 2와 같이 정의한다. 그 후 테이블의 형식 설정을 임의로 초기화한 함수에서 시작하여 하기 수학식 2와 같이 수정된 Bellman Backup 연산자를 반복적으로 적용하면 Soft Q-Function을 얻을 수 있다는 것을 증명할 수 있다.수학식 2"}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기에서 주어진 현재 상태와 동작을 이용하여 다음 상태의 분포를 제공한다. 연속적인 상태의 설정에서 파라미 터를 포함한 네트워크를 사용하여 Soft Q-Function을 파라미터 화한다. Soft Q-Function을 학습하여 Soft Bellman의 나머지를 최소화하는 하기의 수학식 3을 도출할 수 있다. 수학식 3"}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "Q에 대한 목표 네트워크와 과거 경험의 재생 버퍼인 D로부터 경험을 표본화하고 상기 수학식 3의 Monte-Carlo를 사용하여 추정한다. 그 후 획득 가능한 보상을 극대화하는 방향으로 정책을 개선하는 과정이 정책 개선 단계이 다. 이를 위해 계산된 Soft Q-Function을 사용하여 정책 변경을 정책 평가 단계에서 안내한다. 특히, 이 단계에 서 새로운 Soft Q-Function의 지수에 대한 정책을 최신화한다. 그리고 가능한 정책을 매개 변수가 존재하는 분 포 제품군으로 제한하여 정책을 다루기 쉽게 한다. 이를 위해 Soft Q-Function의 지수 정책을 개선한 후 정책을 수용 가능한 정책 공간으로 Kullback-Leibler 분기로 정의된 정보 프로젝션을 사용하여 다시 투영한다. 그렇게 하여 전반적인 정책 개선 단계는 다음 하기의 수학식 4와 같이 도출될 수 있다. 수학식 4"}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "파티션 함수는 다루기 어렵지만 새 정책과 관련하여 기울기에 영향을 주지 않는 방식으로 무시할 수 있다. 연속 상태 설정에서 매개 변수를 갖는 네트워크를 사용하여 정책을 매개 변수화시킨다. 이 매개 변수는 가우스 정책 을 정의하는 데 사용되는 평균과 공분산을 출력한다. 정책 매개 변수는 온도 매개 변수 α를 곱한 후 기울기에 영향을 미치지 않기 때문에 분할 함수를 무시하고 예상 KL-분산 수학식 4를 최소화하여 학습된다. 이 수식에는 정책의 출력 분포에 대한 기대치가 포함되는데, 정상적인 방식으로 오류를 역 전파할 수 없다는 것 을 의미한다. 이를 처리하기 위해 재 파라미터 화 기법을 사용한다. 확률적 행동 분포를 직접 형성하지 않도록 정책 네트워크의 출력을 사용하여 구형 Gaussian에서 입력 잡음 벡터를 추출하여 결합한다. 예를 들어, 1차원의 경우 평균 m과 표준 편차 s를 네트워크에서 출력할 수 있다. 직접 무작위 행동을 표본화하는 방법도 있지만, 오 류를 역 전파할 수는 없다. 결국, 일반적인 방법으로 역 전파될 수 있게 수행해야 한다. 앞서 설명한 방식으로 정책을 다시 매개 변수화하고 있는 상태를 나타내기 위해 하기 수학식 5와 같이 작성될 수 있다. 수학식 5"}
{"patent_id": "10-2021-0179993", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "그리하여 새로운 정책 목표를 세우고 적용한 알고리즘은 도 6에 도시된 그림 과 같이 표현될 수 있다. 전자 장치가 이용하는 PPO(Proximal Policy Optimization) 알고리즘은 TRPO(Trust Region Policy Optimization) 알고리즘에서 발전된 형태의 알고리즘일 수 있다. 두 알고리즘의 목적은 Target Network, Experience Replay를 사용하여, 네트워크 가중치(Network Weights)를 업데이트하는 동안 공격적으로 업데이트를 수행하지 않도록 하여 안정성을 높이는 것이다. Gradient에 적절한 하이퍼 파라미터(Hyper Parameters)를 선택 하거나 조절하여 가중치를 업데이트하는 방법이 사용되기도 한다. Conservative Policy Iteration Update를 사 용하여 기존 가중치와 새로운 가중치의 비례조합을 이용한 업데이트를 수행하는데 사용될 수 있다. TRPO 알고리즘은 앞서 언급한 경험적 기법에서 벗어난, 이론 기반의 업데이트 규칙을 제안한 알고리즘이다. TRPO 알고리즘에서는 최적화 목적 함수(Optimization objective function)에 강제성을 적용한 형식으로 Kullback-Liebler 분기를 사용하여 최적화된 문제를 구성한다. PPO 알고리즘에 Kullback-Liebler 분기를 시도 한 연구 논문도 확인할 수 있었지만, 다른 연구 논문들을 통해 결론적으로 클리핑(Clipping) 기법이 더 좋은 결 과를 나타내는 경향을 보이며, PPO 알고리즘과 TRPO 알고리즘에는 'Surrogate Objective Function' 개념이 존 재한다. 이 개념은 총 누적 보상의 최대화 문제를 정책 기울기 법칙(Policy Gradient Theorem)에 따라 'Expectation'으로 변경한 후 한 번 더 Expectation으로 적용한 것이다. PPO 알고리즘에서는 클리핑 기법을 채 용하여 'Surrogate Objective Function'의 강제성을 부여하였고 도 6에 도시된 그림 과 같이 나타날 수 있다. 도 6의 그림 에 도시된 바와 같이, 'Surrogate Objective Function'이 커지는 방향으로 너무 공격적인 가 중치 업데이트가 되지 않도록 주의해야 하며, 상술한 내용을 종합하면 다음 그림 에 도시된 바와 같이 PPO 알고리즘의 Actor-Critic 스타일의 알고리즘을 정리할 수 있다. 도 7은 일 실시 예에 따른 전자 장치가 이용하는 게임 엔진, 상기 게임 엔진에서 구현 가능한 유니티 ML 에이전 트 및 유니티 ML 에이전트의 훈련 과정을 설명하기 위한 도면이다. Unity 3D Engine은 개발자가 2D 게임, 3D 게임 등에 국한되지 않고 애니메이션, 시뮬레이션, 건축 등 여러 분야 의 개발이 가능하게 해주는 게임 엔진이다. Unity 3D Engine에서는 모바일, PC, 콘솔, 가상현실, 증강현실 등 다양한 플랫폼에서의 플레이를 위한 멀티 플랫폼을 지원한다. 다양한 게임이 Unity 3D Engine을 통해서 개발되 고, 출시되고 있으며 모바일 게임 시장에서 높은 점유율을 보여주는 게임 엔진이다. Unity 3D Engine은 단순한 조작성을 가지는 게임부터 복잡한 전략 게임, 물리 엔진을 기반으로 하는 퍼즐 또는 다수 플레이어 경쟁 게임에 이르는 다양한 장르의 게임들을 구현할 수 있는 유연성을 가지고 있다. Unity 3D Engine을 사용하면 단지 게임에 국한되지 않는 시뮬레이션 환경까지 신속하게 개발할 수 있다. 또 앞서 설명한 멀티 플랫폼뿐만 아니라 더 많은 분야에서 활용이 가능한 기본 엔진 기능들이 있다. 그림 은 Unity 3D Engine에서 제공하고 있는 기능을 한눈에 도시하는 예이다. Unity ML-Agents는 Unity 3D Engine 기반의 프로젝트에서 구현 가능한 Machine Learning(ML) SDK이고, Unity ML SDK를 활용하여 머신 러닝을 구성하면 다른 머신 러닝 SDK보다 Unity Engine으로 제작한 게임이나 시뮬레이 션에 쉽게 심층 강화 학습(Deep Reinforcement Learning)을 사용할 수 있게 된다. 그리고 지능형 에이전트의 훈 련 환경으로 빠르게 구성할 수 있는 Python API를 활용할 수 있게 된다. 도 7에 도시된 그림 은 Unity ML- Agents의 구조를 나타낸 것이다. 그림 을 참조하면, Unity ML-Agents의 훈련이 어떤 방식으로 진행되는지에 대한 흐름이 도시된다. 에이전 트가 선택한 액션에 맞춰 환경은 보상의 증가 값 또는 감소 값, 학습 환경의 상태 값을 에이전트에 같이 반환해 주는 구조이며, Unity ML-Agents는 구조 특성상 특정 액션으로 유도하기 위해 먼저 보상을 지급하게 되면 에이 전트의 학습 효율이 상승할 수도 있게 된다. Unity ML-Agents의 학습 환경은 다음과 같이 정의할 수 있다. Scene 내에 Game Object와 Component를 배치하여 학습 환경을 구성한다. 그 후 마르코프 의사결정 과정(Markov Decision Process) 또는 부분적 관찰 가능 마르코 프 의사결정 과정 (Partially Observable Markov Decision Process)를 생성하면 강화 학습 작업의 기초를 구성 할 수 있게 된다. 관찰과 행동 그리고 보상 기능은 정의를 통해 이루어지게 된다. 이들은 에이전트의 구성 요소 에서 정의할 수도 있다. 또한, 에이전트의 구성 요소에 액세스할 수 있는 모든 구조는 에이전트의 보상 기능을 조작하고 에이전트의 터미널 상태 도달 여부를 정의할 수 있다. 관찰이란 환경 상태에 관한 내용을 에이전트에게 제공하는 것이다. 에이전트는 참조가 포함된 Scene 내부에서 부동 소수점 자료형 벡터와 렌더링으로 출력되는 카메라 수의 형태를 포함할 수 있다. 따라서 에이전트를 구성 하는 요소에서 모든 숫자의 관찰에 해당하는 정보를 정의할 수 있다.개발자는 환경에서 고정 또는 동적 간격을 정의할 수 있으며, 에이전트는 정의된 간격으로 브레인의 결정을 요 청할 수 있다. 보상 기능은 에이전트에게 학습 신호를 제공하는 데 사용되는 기능으로 시뮬레이션 중 Unity Script System을 사용하여 정의, 수정이 가능하다. 또한, 시뮬레이션은 각 에이전트의 개별 수준 또는 전체 환 경에서 완료된 상태로 전환할 수 있으며, Unity Script System을 통해 호출되거나 사전에 정의된 최대 학습 단 계 수에 도달하면 발생한다. 환경을 재설정할 때 Python API에서 접근 및 조작할 수 있는 재설정 환경 매개 변 수들을 정의할 수도 있다 도 8은 일 실시 예에 따른 전자 장치가 SAC 알고리즘 및 PPO 알고리즘에 따라 인공 지능 모델을 학습시킨 결과 를 비교 설명하기 위한 도면이다. 일 실시 예에 의하면, 전자 장치는 다음과 같이 학습 방식을 설정하고, 설정된 학습 방식에 따라 세분화 된 학습 알고리즘을 이용하여 인공 지능 모델을 학습시킬 수 있다. 먼저 우선 Observation 학습 방식은 무작위 로 행동하는 에이전트를 관측하여 학습하는 방식이다. Vector 기준으로 관측하는 Vector Observation 학습 방식 과 색과 같은 Visual 적인 요소를 기준으로 관측하는 Visual Observation 방식으로 나누어진다. 다음으로 Heuristic 학습 방식은 에이전트를 랜덤하게 행동시키지 않고 어느 정도 인간의 직관을 반영한 코드로 에이전트 가 움직이게 하는 모방 학습 방식이다. 여기서 더 나아가 인간의 의사가 더 정확하고 세세하게 적용되어 의미 없는 행동을 최대한 줄여가며 학습하는 방식을 Greedy Heuristic 방식이라 하고, 앞서 서술한 방식을 Simple Heuristic 방식이라 한다. 매치3 게임 프로젝트의 경우 무작위로 행동하는 에이전트가 보상을 얻기 매우 어려운 게임이므로 Observation 방식으로는 에이전트의 적당한 행동을 근사하기가 어렵다. 따라서 전자 장치는 Heuristic 방식을 같이 사용하여 인공 지능 모델을 학습시킬 수 있으며, 각각의 방식에서 나누어진 두 가지 방 식, 총 4가지 방식으로 매치3 퍼즐 게임을 학습하여 더 세분화한 비교 결과를 얻을 수 있게 된다. 전자 장치는 상술한 알고리즘들 및 각각의 학습 방식을 이용하여 인공 지능 모델이 매치3 퍼즐 게임을 학 습하도록 하고, 텐서보드를 통해 학습 결과 값들을 획득할 수 있다. 결과 값들은 누적 보상 값 및 엔트로피 수 치를 포함할 수 있다. 일 실시 예에 의하면 높은 누적 보상 수치는 인공 지능 모델의 학습 성능이 높음을 나타 낼 수 있고, 낮은 엔트로피 수치는 해당 학습 결과가 안정적임을 나타낼 수 있다. 그림 을 참조하면, 전자 장치가 인공 지능 모델을 학습시킨 결과 데이터를 횟수별로 누적 보상과 엔 트로피, 학습 시간을 각 알고리즘과 각 학습 방식으로 구분한 결과가 도시된다. 1,000,000회와 5,000,000회를 횟수 기준으로 삼고 학습 초기와 후기로 비교 도시되며, 상술한 바와 같이 누적 보상이 높을수록 에이전트의 학 습 효율이 높다는 것을 의미하며 엔트로피가 낮을수록 학습 결과가 안정적이라는 것을 의미한다. 5,000,000회를 학습하는데 가장 오래 걸린 시간은 하루하고 12시간으로 SAC 알고리즘의 Heuristic 학습 방식이 가장 느린 학습 속도를 보여주었다. 가장 짧은 시간은 8시간 36분으로 PPO 알고리즘의 Observation 학습 방식이 가장 빠른 학습 속도를 보여주었다. 동일 학습 횟수와 동일 학습 방식을 기준으로 비교했을 때 학습 속도는 대 략 두 배보다도 더 큰 학습 속도 차이를 보여주었다. 5,000,000회 학습하여 얻어진 최종 누적 보상 값은 SAC 알고리즘의 Visual Observation 학습 방식에서 40.01로 가장 높았고 가장 낮은 값은 SAC 알고리즘의 Simple Heuristic 학습 방식에서 33.9로 가장 낮았다. 하지만 누적 보상은 SAC 알고리즘의 Observation 학습 방식은 둘 다 40 정도의 수치로 수렴하여 작은 편차로 크지 않은 차이 를 보였기 때문에 2가지 Observation 학습 방식 모두 누적 보상 값이 최고 수치라고 분석하였다. Heuristic 학 습 방식은 SAC 알고리즘과 PPO 알고리즘에서 모두 큰 차이를 보이지 않고 Simple Heuristic 학습 방식은 34 정 도의 값을 유지하고 Greedy Heuristic 학습 방식은 37 정도의 값을 계속해서 유지하였기 때문에 반복 학습을 통 한 성능 향상을 기대하기 힘들다고 분석하였다. 최종 엔트로피 수치는 PPO 알고리즘의 Vector Observation 학습 방식이 0.58로 가장 낮은 수치를 보여주었고 PPO 알고리즘의 Simple Heuristic 학습 방식이 2.11로 가장 높은 수치를 보여주었다. SAC 알고리즘의 학습 결과 는 대부분 1에 가까운 수치로 보이고 편차가 크지 않아 적정하게 안정한 학습 결과를 보여주었다. PPO 알고리즘 의 Observation 학습 결과의 경우 느린 속도록 안정화되는 모습을 보이며 Observation 학습 방식은 0.58이라는 수치로 가장 안정적인 모습을 보였지만, Heuristic 학습 방식의 값 편차가 심하게 나타나 불안정한 모습을 보여 주었다. 전체적으로 분석하였을 때 빠르고 안정적인 학습 결과의 도출을 원할 때는 PPO 알고리즘에 Vector Observation 학습 방식을 활용하여 학습하는 것이 적합함을 알 수 있다. 반대로 학습 속도가 느리지만 가장 최고의 학습 성 능을 도출하기 위해서는 SAC 알고리즘에 Visual Observation 학습 방식을 활용하여 학습하는 것이 적합하다. 다만, 매치3 퍼즐 게임의 레벨 밸런싱을 위한 에이전트를 구현하는 목적에 가장 부합한 학습 결과는 최고의 학습 성능을 보여주는 것이다. 따라서 SAC 알고리즘에 Visual Observation 학습 방식을 활용하여 학습한 에이전트가 매치3 퍼즐 게임의 레벨 밸런싱 테스트 플레이에 가장 적합할 것이라는 결과로 분석할 수 있다. 도 9는 일 실시 예에 따른 전자 장치의 블록도이다. 일 실시 예에 의하면, 전자 장치는 디스플레이, 프로세서, 네트워크 인터페이스 및 메 모리를 포함할 수 있다. 그러나, 상술한 예에 한정되는 것은 아니며, 전자 장치는 게임 컨텐츠의 난이도를 관리하기 위한 기타 구성들을 더 포함할 수 있음은 물론이며, 도 9에 도시된 구성 요소 보다 더 적은 구성 요소를 포함할 수도 있다. 프로세서는, 통상적으로 전자 장치의 전반적인 동작을 제어한다. 예를 들어, 프로세서는, 메 모리에 저장된 프로그램들을 실행함으로써, 디스플레이, 네트워크 인터페이스를 전반적을 제 어할 수 있다. 일 실시 예에 의하면, 프로세서는 하나 이상의 인스트럭션을 수행함으로써, 에이전트 별로 서로 다른 게 임 레벨에 따른 적어도 하나의 게임 프로젝트를 생성하고, 상기 적어도 하나의 게임 프로젝트를 하나의 씬으로 생성하고, 상기 하나의 씬 내 적어도 하나의 게임 프로젝트들로부터 획득된 데이터에 기초하여 학습 데이터를 생성하고, 상기 생성된 학습 데이터에 기초하여 상기 게임 프로젝트를 플레이하는 인공 지능 모델을 복수의 강 화 학습 알고리즘들에 따라 학습시킬 수 있다. 또한, 일 실시 예에 의하면, 프로세서는 상기 학습된 인공 지능 모델이 게임 프로젝트를 학습하기에 적합 한지 여부에 대한 성능 점수를 결정하고, 상기 결정된 성능 점수에 기초하여 상기 복수의 강화 학습 알고리즘들 중 하나의 강화 학습 알고리즘에 따라 학습된 인공 지능 모델을 식별할 수 있다. 또한, 프로세서는 상기 학습된 인공 지능 모델을 이용하여 상기 적어도 하나의 게임 프로젝트를 실행하고, 상기 게임 프로젝트의 실행 결과에 기초하여 상기 적어도 하나의 게임 프로젝트의 스테이지 별 난이 도 밸런스를 결정하며, 상기 학습된 인공 지능 모델을 이용하여, 상기 결정된 스테이지 별 난이도 밸런스를 가 지는 적어도 하나의 게임 프로젝트를 재 실행할 수 있다. 또한, 프로세서는 도 1 내지 8에 도시된 전자 장치가 게임 컨텐츠의 난이도를 관리하기 위한 방법을 수행 할 수 있다. 네트워크 인터페이스는 프로세서의 제어에 의해 전자 장치와 연결된 외부 디바이스로부터 게임 프로젝트 에 대한 데이터들을 수신할 수 있다. 또한, 네트워크 인터페이스는 전자 장치와 연결된 외부 디바이스로 부터 인공 지능 모델에 대한 정보, 게임 프로젝트를 동작시키기 위한 게임 엔진들에 대한 정보를 더 수신할 수 도 있다. 또한, 네트워크 인터페이스는 전자 장치가 학습시킨 인공 지능 모델에 대한 정보, 인공 지능 모 델을 이용하여 게임을 학습함으로써 생성되는 결과 데이터에 대한 정보를 전자 장치와 연결된 외부 디바 이스로 전송할 수도 있다. 디스플레이는 게임 프로젝트들에 대한 데이터, 매치 3 게임 프로젝트들을 수행함으로써 표시될 수 있는 게임 컨텐츠들, 인공 지능 모델에 대한 성능 정보, 인공 지능 모델에 대한 정보들을 표시할 수 있다. 메모리는, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수 있고, 전자 장치로 입력되 거나 전자 장치로부터 출력되는 데이터를 저장할 수도 있다. 또한, 메모리는 전자 장치가 이용하는 인공 지능 모델에 대한 정보를 저장할 수 있다. 또한, 메모리는 난이도 조절 대상인 게임 프로젝트들에 대한 정보, 게임 프로젝트들을 자동으로 수행하기 위한 인공 지능 모델에 대한 정보, 인공 지능 모델을 학습시키기 위한 강화 학습 알고리즘에 대한 정보, 게임 엔진 정보, 에이전트들에 대한 정보를 더 저장할 수도 있다. 일 실시 예에 의하면, 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 도 10은 일 실시 예에 따른 전자 장치가 실행하는 게임 진행 과정을 나타내는 도면이다. 도 10을 참조하면 전자 장치가 실행하는 매치 3 게임 프로젝트의 진행 과정이 도시된다. S1002에서, 전자 장치는 메모리에 저장된 매치 3 게임 프로젝트에 대한 인스트럭션을 실행함으로써 게임을 시작할 수 있다. S1004에서, 전자 장치는 복수의 블록들(예컨대 복수 셀들)을 생성할 수 있다. S1006에서, 전자 장 치는 전자 장치의 화면상에 표시된 블록들 중 적어도 하나의 블록을 선택하는 사용자 입력에 기초하여, 소정의 블록들을 선택할 수 있다. S1008에서, 전자 장치는 블록들을 선택하는 사용자 입력에 기초하여 스와이프 움직임 모션을 식별할 수 있다. S1010에서, 전자 장치는 식별된 스와이프 움직임 모션이 유효한지 여부를 식별할 수 있다. 예를 들 어, 전자 장치는 스와이프 움직임 모션이 유효하지 않은 것으로 식별되면, 블록들 중 적어도 하나의 블록 을 선택하는 사용자 입력을 다시 획득할 수 있다. S1012에서, 전자 장치는 스와이프 움직임 모션이 유효 한 것으로 식별되면, 선택된 블록들을 마킹할 수 있다. S1014에서, 전자 장치는 마킹된 셀들이 전부 같은 타입인지 여부를 확인할 수 있다. 예를 들어, 전자 장치는 마킹된 셀들이 전부 같은 타입이 아닌 것으로 식별되면, 다시 S1006으로 진입하여, 복수의 블록들 중 적어도 하나의 블록을 선택하는 사용자 입력을 다시 획 득할 수 있다. S1016에서, 전자 장치는 마킹된 블록들이 전부 같은 타입으로 식별되는 경우, 마킹된 블록 들을 모두 삭제할 수 있다. S1018에서, 전자 장치는 블록들이 삭제된 빈 위치에 새로운 블록들을 구성할 수 있다. 전자 장치는 도 10에 도시된 방법을 반복하여 수행함으로써 매치 3 게임 프로젝트를 실행할 수 있다. 도 11은 일 실시 예에 따른 전자 장치가 실행하는 게임 프로젝트들에 대한 예시를 나타내는 도면이다. 도 11에 도시된 그림 (1101, 1102, 1103)을 참조하면 복수 타입의 블록들을 이용하여 전자 장치가 제공하는 매 치 3 게임 프로젝트들에 대한 예시 화면이 도시된다. 전자 장치는 상술한 방법에 따라 학습된 인공 지능 모델을 이용하여 매치 3 게임 프로젝트들을 강화학습하고, 학습 결과에 기초하여 매치 3 게임 프로젝트들의 스 테이지 별 레벨을 효과적으로 최적화할 수 있다. 일 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 개시를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 또한, 상기 일 실시 예에 다른 방법을 수행하도록 하는 프로그램이 저장된 기록매체를 포함하는 컴퓨터 프로그 램 장치가 제공될 수 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프 와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크 (floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같 은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴 파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행 될 수 있는 고급 언어 코드를 포함한다. 이상에서 본 개시의 실시예에 대하여 상세하게 설명하였지만 본 개시의 권리범위는 이에 한정되는 것은 아니고 다음에서 정의하고 있는 본 개시의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 개시의 권리범위에 속한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11"}
{"patent_id": "10-2021-0179993", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시 예에 따른 전자 장치가 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 개략적인 과정을 나타내는 도면이다. 도 2는 일 실시 예에 따른 전자 장치가 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법의 흐 름도이다. 도 3은 또 다른 실시 예에 따른 전자 장치가 인공 지능 모델을 기반으로 게임 컨텐츠의 난이도를 관리하는 방법 의 흐름도이다. 도 4는 일 실시 예에 따른 전자 장치가 학습된 인공 지능 모델을 이용하여 게임 프로젝트의 스테이지별 난이도 밸런스를 결정하는 과정을 설명하기 위한 도면이다. 도 5는 일 실시 예에 따른 전자 장치가 이용하는 게임의 클래스 다이어그램을 나타내는 도면이다. 도 6은 일 실시 예에 따른 전자 장치가 이용하는 SAC 알고리즘과 PPO 알고리즘을 설명하기 위한 도면이다. 도 7은 일 실시 예에 따른 전자 장치가 이용하는 게임 엔진, 상기 게임 엔진에서 구현 가능한 유니티 ML 에이전 트 및 유니티 ML 에이전트의 훈련 과정을 설명하기 위한 도면이다. 도 8은 일 실시 예에 따른 전자 장치가 SAC 알고리즘 및 PPO 알고리즘에 따라 인공 지능 모델을 학습시킨 결과 를 비교 설명하기 위한 도면이다. 도 9는 일 실시 예에 따른 전자 장치의 블록도이다. 도 10은 일 실시 예에 따른 전자 장치가 실행하는 게임 진행 과정을 나타내는 도면이다. 도 11은 일 실시 예에 따른 전자 장치가 실행하는 게임 프로젝트들에 대한 예시를 나타내는 도면이다."}
