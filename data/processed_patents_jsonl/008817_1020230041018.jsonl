{"patent_id": "10-2023-0041018", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0146268", "출원번호": "10-2023-0041018", "발명의 명칭": "다중 모델의 동적 교체를 위해 모델을 관리하는 방법, 컴퓨터 장치, 및 컴퓨터 프로그램", "출원인": "라인플러스 주식회사", "발명자": "장혁재"}}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터 장치에서 실행되는 모델 관리 방법에 있어서,상기 컴퓨터 장치는 메모리에 포함된 컴퓨터 판독가능한 명령들을 실행하도록 구성된 적어도 하나의 프로세서를포함하고,상기 모델 관리 방법은,상기 적어도 하나의 프로세서에 의해, 어플리케이션이 설치된 클라이언트 단의 플랫폼을 통해 상기 어플리케이션에 포함된 기능에 대해 각 기능과 관련된 인공지능 모델을 통합 관리하는 단계를 포함하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 관리하는 단계는,상기 기능의 활성화 여부 및 상기 기능과 상기 인공지능 모델 간의 관계를 기초로 해당 모델 파일의 다운로드와삭제를 수행하는 단계를 포함하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 관리하는 단계는,상기 컴퓨터 장치에 해당되는 디바이스 정보를 기초로 각 기능에 대해 적어도 하나의 인공지능 모델을 다운로드하는 단계를 포함하고,상기 디바이스 정보는 디바이스 유형, 디바이스 스펙, 소프트웨어 플랫폼, 및 국가 정보 중 적어도 하나를 포함하는 것을 특징으로 하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 모델 관리 방법은,상기 적어도 하나의 프로세서에 의해, 상기 플랫폼을 통해 상기 인공지능 모델에 대해 클라이언트 환경에서의모델 성능을 측정하는 단계를 더 포함하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 측정하는 단계는,상기 인공지능 모델 별로 결과 정확도(accuracy), 메모리 사용량(memory usage), 모델 크기(model file size),공개특허 10-2024-0146268-2-초기 모델 로딩 속도(initialize latency), 및 결과 처리 속도(inference latency)를 측정하는 단계를 포함하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,상기 관리하는 단계는,상기 인공지능 모델 별 성능 측정 결과를 기초로 각 기능에 대해 적어도 하나의 인공지능 모델을 다운로드하는단계를 포함하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 모델 관리 방법은,상기 적어도 하나의 프로세서에 의해, 상기 기능에 대해 클라이언트 환경에 따라 상기 인공지능 모델을 동적으로 제공하는 단계를 더 포함하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 제공하는 단계는,상기 기능이 사용하는 인공지능 모델을 상기 컴퓨터 장치의 자원 상황 또는 해당 기능의 사용 패턴에 따라 교체하는 단계를 포함하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 제공하는 단계는,복수 개의 인공지능 모델을 사용하는 기능의 경우 상기 클라이언트 환경에 따라 상기 복수 개의 인공지능 모델에 대한 스케줄링(scheduling) 또는 플래닝(planning)을 설정하는 단계를 포함하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서,상기 제공하는 단계는,상기 클라이언트 환경에 대한 조건 별로 사용 모델, 모델 스케줄링, 및 모델 플래닝 중 적어도 하나의 프로필을정의하는 단계를 포함하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 모델 관리 방법은,상기 적어도 하나의 프로세서에 의해, 상기 플랫폼을 통해 상기 인공지능 모델에 대해 클라이언트 환경에서의공개특허 10-2024-0146268-3-모델 성능을 측정하는 단계를 더 포함하고,상기 정의하는 단계는,상기 인공지능 모델 별 성능 측정 결과를 기초로 상기 프로필을 결정하는 단계를 포함하는 모델 관리 방법."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항 내지 제11항 중 어느 한 항의 모델 관리 방법을 상기 컴퓨터 장치에 실행시키기 위해 비-일시적인 컴퓨터판독가능한 기록 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "컴퓨터 장치에 있어서,메모리에 포함된 컴퓨터 판독가능한 명령들을 실행하도록 구성된 적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는,어플리케이션이 설치된 클라이언트 단의 플랫폼을 통해 상기 어플리케이션에 포함된 기능에 대해 각 기능과 관련된 인공지능 모델을 통합 관리하는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 적어도 하나의 프로세서는,상기 기능의 활성화 여부 및 상기 기능과 상기 인공지능 모델 간의 관계를 기초로 해당 모델 파일의 다운로드와삭제를 수행하는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13항에 있어서,상기 적어도 하나의 프로세서는,상기 컴퓨터 장치에 해당되는 디바이스 정보를 기초로 각 기능에 대해 적어도 하나의 인공지능 모델을 다운로드하고,상기 디바이스 정보는 디바이스 유형, 디바이스 스펙, 소프트웨어 플랫폼, 및 국가 정보 중 적어도 하나를 포함하는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제13항에 있어서,상기 적어도 하나의 프로세서는,상기 플랫폼을 통해 상기 인공지능 모델에 대해 클라이언트 환경에서의 모델 성능을 측정하는 것으로,상기 인공지능 모델 별로 결과 정확도, 메모리 사용량, 모델 크기, 초기 모델 로딩 속도, 및 결과 처리 속도를측정하는 것공개특허 10-2024-0146268-4-을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 적어도 하나의 프로세서는,상기 인공지능 모델 별 성능 측정 결과를 기초로 각 기능에 대해 적어도 하나의 인공지능 모델을 다운로드하는것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제13항에 있어서,상기 적어도 하나의 프로세서는,상기 기능에 대해 클라이언트 환경에 따라 상기 인공지능 모델을 동적으로 제공하는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 적어도 하나의 프로세서는,상기 기능이 사용하는 인공지능 모델을 상기 컴퓨터 장치의 자원 상황 또는 해당 기능의 사용 패턴에 따라 교체하는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2023-0041018", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제18항에 있어서,상기 적어도 하나의 프로세서는,복수 개의 인공지능 모델을 사용하는 기능의 경우 상기 클라이언트 환경에 따라 상기 복수 개의 인공지능 모델에 대한 스케줄링 또는 플래닝을 설정하는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2023-0041018", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "다중 모델의 동적 교체를 위해 모델을 관리하는 방법, 컴퓨터 장치, 및 컴퓨터 프로그램이 개시된다. 모델을 관 리하는 방법은, 어플리케이션이 설치된 클라이언트 단의 플랫폼을 통해 상기 어플리케이션에 포함된 기능에 대해 각 기능과 관련된 인공지능 모델을 통합 관리하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0041018", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "아래의 설명은 인공지능 모델(AI model)을 관리하는 기술에 관한 것이다."}
{"patent_id": "10-2023-0041018", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "기계학습(machine learning)과 같은 인공지능 모델은 객체 검출(object detection), 이미지 태깅(image tagging), 이미지 분류(image classification), 문자 인식(OCR: optical character recognition), 의미 분할 (semantic segmentation), 비디오 분석(video analysis) 등 다양한 컴퓨터 비전 기술에서 유망한 성과를 보여 주고 있다. 최근 사용자 디바이스에서 제공하는 서비스 내 기능 중 기계학습 모델을 사용하는 기능이 많아지고 있다. 일례로, 한국 공개특허공보 제10-2021-0006098호(공개일 2021년 01월 18일)에는 문서 검색 기능에 기계학습 모 델을 이용하는 기술이 개시되어 있다."}
{"patent_id": "10-2023-0041018", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2023-0041018", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "클라이언트 플랫폼 내부에서 여러 기능에서 사용되는 복수 개의 기계학습 모델을 관리할 수 있다. 클라이언트에서 사용되는 모델 리스트에 대해 플랫폼에서 각 모델의 성능을 측정할 수 있다. 서비스 제공 환경에 따라 동일 기능에서 사용되는 모델을 동적으로 교체할 수 있다."}
{"patent_id": "10-2023-0041018", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "컴퓨터 장치에서 실행되는 모델 관리 방법에 있어서, 상기 컴퓨터 장치는 메모리에 포함된 컴퓨터 판독가능한 명령들을 실행하도록 구성된 적어도 하나의 프로세서를 포함하고, 상기 모델 관리 방법은, 상기 적어도 하나의 프로세서에 의해, 어플리케이션이 설치된 클라이언트 단의 플랫폼을 통해 상기 어플리케이션에 포함된 기능에 대해 각 기능과 관련된 인공지능 모델을 통합 관리하는 단계를 포함하는 모델 관리 방법을 제공할 수 있다. 일 측면에 따르면, 상기 관리하는 단계는, 상기 기능의 활성화 여부 및 상기 기능과 상기 인공지능 모델 간의 관계를 기초로 해당 모델 파일의 다운로드와 삭제를 수행하는 단계를 포함할 수 있다. 다른 측면에 따르면, 상기 관리하는 단계는, 상기 컴퓨터 장치에 해당되는 디바이스 정보를 기초로 각 기능에 대해 적어도 하나의 인공지능 모델을 다운로드하는 단계를 포함하고, 상기 디바이스 정보는 디바이스 유형, 디 바이스 스펙, 소프트웨어 플랫폼, 및 국가 정보 중 적어도 하나를 포함할 수 있다. 또 다른 측면에 따르면, 상기 모델 관리 방법은, 상기 적어도 하나의 프로세서에 의해, 상기 플랫폼을 통해 상 기 인공지능 모델에 대해 클라이언트 환경에서의 모델 성능을 측정하는 단계를 더 포함할 수 있다. 또 다른 측면에 따르면, 상기 측정하는 단계는, 상기 인공지능 모델 별로 결과 정확도(accuracy), 메모리 사용 량(memory usage), 모델 크기(model file size), 초기 모델 로딩 속도(initialize latency), 및 결과 처리 속 도(inference latency)를 측정하는 단계를 포함할 수 있다. 또 다른 측면에 따르면, 상기 관리하는 단계는, 상기 인공지능 모델 별 성능 측정 결과를 기초로 각 기능에 대 해 적어도 하나의 인공지능 모델을 다운로드하는 단계를 포함할 수 있다. 또 다른 측면에 따르면, 상기 모델 관리 방법은, 상기 적어도 하나의 프로세서에 의해, 상기 기능에 대해 클라 이언트 환경에 따라 상기 인공지능 모델을 동적으로 제공하는 단계를 더 포함할 수 있다. 또 다른 측면에 따르면, 상기 제공하는 단계는, 상기 기능이 사용하는 인공지능 모델을 상기 컴퓨터 장치의 자 원 상황 또는 해당 기능의 사용 패턴에 따라 교체하는 단계를 포함할 수 있다. 또 다른 측면에 따르면, 상기 제공하는 단계는, 복수 개의 인공지능 모델을 사용하는 기능의 경우 상기 클라이 언트 환경에 따라 상기 복수 개의 인공지능 모델에 대한 스케줄링(scheduling) 또는 플래닝(planning)을 설정하 는 단계를 포함할 수 있다. 또 다른 측면에 따르면, 상기 제공하는 단계는, 상기 클라이언트 환경에 대한 조건 별로 사용 모델, 모델 스케 줄링, 및 모델 플래닝 중 적어도 하나의 프로필을 정의하는 단계를 포함할 수 있다. 또 다른 측면에 따르면, 상기 모델 관리 방법은, 상기 적어도 하나의 프로세서에 의해, 상기 플랫폼을 통해 상 기 인공지능 모델에 대해 클라이언트 환경에서의 모델 성능을 측정하는 단계를 더 포함하고, 상기 정의하는 단 계는, 상기 인공지능 모델 별 성능 측정 결과를 기초로 상기 프로필을 결정하는 단계를 포함할 수 있다. 상기 모델 관리 방법을 상기 컴퓨터 장치에 실행시키기 위해 비-일시적인 컴퓨터 판독가능한 기록 매체에 저장 된 컴퓨터 프로그램을 제공할 수 있다. 컴퓨터 장치에 있어서, 메모리에 포함된 컴퓨터 판독가능한 명령들을 실행하도록 구성된 적어도 하나의 프로세 서를 포함하고, 상기 적어도 하나의 프로세서는, 어플리케이션이 설치된 클라이언트 단의 플랫폼을 통해 상기 어플리케이션에 포함된 기능에 대해 각 기능과 관련된 인공지능 모델을 통합 관리하는 것을 특징으로 하는 컴퓨 터 장치를 제공할 수 있다."}
{"patent_id": "10-2023-0041018", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 실시예를 첨부된 도면을 참조하여 상세하게 설명한다. 본 발명의 실시예들은 인공지능 모델을 관리하는 기술에 관한 것이다. 본 명세서에서 구체적으로 개시되는 것들을 포함하는 실시예들은 서비스 제공을 위해 사용되는 복수 개의 기계 학습 모델을 관리하는 역할, 관리 대상인 모델 리스트에 포함된 각 모델의 성능 측정을 자동화하는 역할, 및 서 비스 제공 환경에 맞는 성능의 모델을 동적으로 제공하는 역할의 플랫폼을 클라이언트 단에 탑재할 수 있다. 본 발명의 실시예들에 따른 모델 관리 시스템은 적어도 하나의 컴퓨터 장치에 의해 구현될 수 있으며, 본 발명 의 실시예들에 따른 모델 관리 방법은 모델 관리 시스템에 포함되는 적어도 하나의 컴퓨터 장치를 통해 수행될 수 있다. 이때, 컴퓨터 장치에는 본 발명의 일실시예에 따른 컴퓨터 프로그램이 설치 및 구동될 수 있고, 컴퓨 터 장치는 구동된 컴퓨터 프로그램의 제어에 따라 본 발명의 실시예들에 따른 모델 관리 방법을 수행할 수 있다. 상술한 컴퓨터 프로그램은 컴퓨터 장치와 결합되어 모델 관리 방법을 컴퓨터에 실행시키기 위해 컴퓨터 판독 가능한 기록매체에 저장될 수 있다. 도 1은 본 발명의 일실시예에 따른 네트워크 환경의 예를 도시한 도면이다. 도 1의 네트워크 환경은 복수의 전 자 기기들(110, 120, 130, 140), 복수의 서버들(150, 160) 및 네트워크를 포함하는 예를 나타내고 있다. 이러한 도 1은 발명의 설명을 위한 일례로 전자 기기의 수나 서버의 수가 도 1과 같이 한정되는 것은 아니다. 또한, 도 1의 네트워크 환경은 본 실시예들에 적용 가능한 환경들 중 하나의 예를 설명하는 것일 뿐, 본 실시예 들에 적용 가능한 환경이 도 1의 네트워크 환경으로 한정되는 것은 아니다. 복수의 전자 기기들(110, 120, 130, 140)은 컴퓨터 장치로 구현되는 고정형 단말이거나 이동형 단말일 수 있다. 복수의 전자 기기들(110, 120, 130, 140)의 예를 들면, 스마트폰(smart phone), 휴대폰, 내비게이션, 컴퓨터, 노트북, 디지털방송용 단말, PDA(Personal Digital Assistants), PMP(Portable Multimedia Player), 태블릿 PC 등이 있다. 일례로 도 1에서는 전자 기기의 예로 스마트폰의 형상을 나타내고 있으나, 본 발명의 실시예 들에서 전자 기기는 실질적으로 무선 또는 유선 통신 방식을 이용하여 네트워크를 통해 다른 전자 기 기들(120, 130, 140) 및/또는 서버(150, 160)와 통신할 수 있는 다양한 물리적인 컴퓨터 장치들 중 하나를 의미 할 수 있다. 통신 방식은 제한되지 않으며, 네트워크가 포함할 수 있는 통신망(일례로, 이동통신망, 유선 인터넷, 무선 인터넷, 방송망)을 활용하는 통신 방식뿐만 아니라 기기들 간의 근거리 무선 통신 역시 포함될 수 있다. 예를 들어, 네트워크는, PAN(personal area network), LAN(local area network), CAN(campus area network), MAN(metropolitan area network), WAN(wide area network), BBN(broadband network), 인터넷 등의 네트워크 중 하나 이상의 임의의 네트워크를 포함할 수 있다. 또한, 네트워크는 버스 네트워크, 스타 네트워크, 링 네트워크, 메쉬 네트워크, 스타-버스 네트워크, 트리 또는 계층적(hierarchical) 네트워크 등을 포함하는 네트 워크 토폴로지 중 임의의 하나 이상을 포함할 수 있으나, 이에 제한되지 않는다. 서버(150, 160) 각각은 복수의 전자 기기들(110, 120, 130, 140)과 네트워크를 통해 통신하여 명령, 코드, 파일, 컨텐츠, 서비스 등을 제공하는 컴퓨터 장치 또는 복수의 컴퓨터 장치들로 구현될 수 있다. 예를 들어, 서버는 네트워크를 통해 접속한 복수의 전자 기기들(110, 120, 130, 140)로 서비스(일례로, 메 신저 서비스, 통합검색 서비스, 컨텐츠 추천 서비스 등)를 제공하는 시스템일 수 있다.도 2는 본 발명의 일실시예에 따른 컴퓨터 장치의 예를 도시한 블록도이다. 앞서 설명한 복수의 전자 기기들 (110, 120, 130, 140) 각각이나 서버들(150, 160) 각각은 도 2를 통해 도시된 컴퓨터 장치에 의해 구현될 수 있다. 이러한 컴퓨터 장치는 도 2에 도시된 바와 같이, 메모리, 프로세서, 통신 인터페이스 그리 고 입출력 인터페이스를 포함할 수 있다. 메모리는 컴퓨터에서 판독 가능한 기록매체로서, RAM(random access memory), ROM(read only memory) 및 디스크 드라이브와 같은 비소멸성 대용량 기록장치 (permanent mass storage device)를 포함할 수 있다. 여기서 ROM과 디스크 드라이브와 같은 비소멸성 대용량 기록장치는 메모리와는 구분되는 별도의 영구 저장 장치로서 컴퓨터 장치에 포함될 수도 있다. 또한, 메모리에는 운영체제와 적어도 하나의 프로그램 코드가 저장될 수 있다. 이러한 소프트웨어 구성요 소들은 메모리와는 별도의 컴퓨터에서 판독 가능한 기록매체로부터 메모리로 로딩될 수 있다. 이러 한 별도의 컴퓨터에서 판독 가능한 기록매체는 플로피 드라이브, 디스크, 테이프, DVD/CD-ROM 드라이브, 메모리 카드 등의 컴퓨터에서 판독 가능한 기록매체를 포함할 수 있다. 다른 실시예에서 소프트웨어 구성요소들은 컴 퓨터에서 판독 가능한 기록매체가 아닌 통신 인터페이스를 통해 메모리에 로딩될 수도 있다. 예를 들어, 소프트웨어 구성요소들은 네트워크를 통해 수신되는 파일들에 의해 설치되는 컴퓨터 프로그램에 기 반하여 컴퓨터 장치의 메모리에 로딩될 수 있다. 프로세서는 기본적인 산술, 로직 및 입출력 연산을 수행함으로써, 컴퓨터 프로그램의 명령을 처리하도록 구성될 수 있다. 명령은 메모리 또는 통신 인터페이스에 의해 프로세서로 제공될 수 있다. 예 를 들어 프로세서는 메모리와 같은 기록 장치에 저장된 프로그램 코드에 따라 수신되는 명령을 실행 하도록 구성될 수 있다. 통신 인터페이스는 네트워크를 통해 컴퓨터 장치가 다른 장치(일례로, 앞서 설명한 저장 장치들)와 서로 통신하기 위한 기능을 제공할 수 있다. 일례로, 컴퓨터 장치의 프로세서가 메모리 와 같은 기록 장치에 저장된 프로그램 코드에 따라 생성한 요청이나 명령, 데이터, 파일 등이 통신 인터페 이스의 제어에 따라 네트워크를 통해 다른 장치들로 전달될 수 있다. 역으로, 다른 장치로부터의 신 호나 명령, 데이터, 파일 등이 네트워크를 거쳐 컴퓨터 장치의 통신 인터페이스를 통해 컴퓨터 장치로 수신될 수 있다. 통신 인터페이스를 통해 수신된 신호나 명령, 데이터 등은 프로세서나 메모리로 전달될 수 있고, 파일 등은 컴퓨터 장치가 더 포함할 수 있는 저장 매체(상술한 영구 저장 장치)로 저장될 수 있다. 입출력 인터페이스는 입출력 장치와의 인터페이스를 위한 수단일 수 있다. 예를 들어, 입력 장치는 마이크, 키보드 또는 마우스 등의 장치를, 그리고 출력 장치는 디스플레이, 스피커와 같은 장치를 포함할 수 있 다. 다른 예로 입출력 인터페이스는 터치스크린과 같이 입력과 출력을 위한 기능이 하나로 통합된 장치와 의 인터페이스를 위한 수단일 수도 있다. 입출력 장치는 컴퓨터 장치와 하나의 장치로 구성될 수도 있다. 또한, 다른 실시예들에서 컴퓨터 장치는 도 2의 구성요소들보다 더 적은 혹은 더 많은 구성요소들을 포함 할 수도 있다. 그러나, 대부분의 종래기술적 구성요소들을 명확하게 도시할 필요성은 없다. 예를 들어, 컴퓨 터 장치는 상술한 입출력 장치 중 적어도 일부를 포함하도록 구현되거나 또는 트랜시버 (transceiver), 데이터베이스 등과 같은 다른 구성요소들을 더 포함할 수도 있다. 이하에서는 다중 모델의 동적 교체를 위해 모델을 관리하는 방법 및 장치의 구체적인 실시예를 설명하기로 한다. 본 실시예에서는 복수 개의 기계학습 모델을 관리 및 제공하는 역할의 플랫폼이 클라이언트 단에 탑재될 수 있 다. 본 명세서에서 클라이언트는 컴퓨터 장치로 구현되는 전자 기기로, 어플리케이션이 설치되는 모바일 기기 나 PC 등 사용자 측 단말에 해당되는 디바이스를 의미할 수 있다. 도 3을 참조하면, 사용자 디바이스에서 기계학습 모델을 사용하기 위해서는 우선 목적하는 기능에 대해 사용 가 능한 모델을 조사하는 과정(model survey)(S31), 디바이스 상의 플랫폼에 맞게 모델을 변환하는 과정(model converting)(S32), 모델을 경량화하는 과정(model quantization)(S33), 및 서비스 환경에서의 모델 성능을 확 인하는 과정(performance check)(S34) 등을 거치게 된다.최근 디바이스에서 제공하는 서비스 내 기능 중 기계학습 모델을 사용하는 기능이 증가하고 있다. 하나의 기능 이 하나의 모델을 사용할 수 있으며, 하나의 모델을 여러 기능이 사용하거나 하나의 기능이 여러 모델을 사용할 수도 있다. 도 4를 참조하면, 하나의 기능이 하나의 모델을 사용하는 1:1의 경우, 예를 들어 대화방 검색(chatroom search) 기능이 YOLOv3을 사용하는 경우(도 4의 (A) 도면) 메신저 서비스 내 랩스(labs)나 설정 환경에서 대화방 검색 기능을 활성화(on)하면 YOLOv3의 모델 파일이 다운로드되고 대화방 검색 기능을 비활성화(off)하면 해당 모델 파일이 삭제된다. 그러나, 메신저 서비스의 기능 중 대화방 검색 기능과 이미지 태깅(image tagging) 기능이 공통으로 YOLOv3을 사용하는 경우(도 4의 (B) 도면) 대화방 검색 기능이 비활성화되더라도 이미지 태깅 기능 때문에 YOLOv3의 모델 파일을 삭제할 수 없다. 이와 같이, 기능과 기계학습 모델 간의 관계가 1:N, N:1, N:N과 같이 복잡해지는 경우 각 기능에서 모델을 관리 하기 어렵게 된다. 그리고, 동일한 기계학습 모델이라 하더라도 다양한 변형(variation) 모델이 존재한다. 예를 들어, 이미지 내 객체 검출을 위한 YOLO(you only look once) 모델의 경우 YOLOv3, YOLOv4, YOLOv5, YOLOv3FP16, YOLOv3Int8LUT 등 수십 개의 변형 모델이 존재한다. 각 모델마다 입력 이미지 조건, 사용 메모리, 처리 속도, 정확도 등이 모두 다르기 때문에 제공하고자 하는 기 능과 디바이스 환경 조건에 맞는 기계학습 모델을 선택하는 기술이 필요하다. 기계학습 모델의 성능 측정은 정해진 환경 하에서 사전에 수집된 데이터셋에 기반하여 정확도 위주로 이루어질 뿐, 실제 서비스 환경에서 동적으로 변화하는 정확도의 요구사항, 기기와 실행 환경에 따라 마찬가지로 동적으 로 변화하는 처리 속도나 메모리 사용량 등은 고려되고 있지 않다. 샘플링된 일부 모델들을 기반으로 테스트를 진행한 후 테스트 결과 가장 적절한 모델을 탑재하여 서비스를 제공하고 있는 실정이다. 본 실시예에서는 복수 개의 기계학습 모델을 관리하는 역할, 관리 대상인 모델 리스트에 포함된 각 모델의 성능 측정을 자동화하는 역할, 및 서비스 제공 환경에 따라 모델을 동적으로 교체하는 역할을 하는 플랫폼(이하, '모 델 지원 플랫폼'이라 칭함)을 클라이언트 단에 탑재할 수 있다. 예를 들어, 도 5를 참조하면 본 발명의 일실시예에 따른 컴퓨터 장치는 클라이언트 상에 설치된 메신저 어 플리케이션에 대해 메신저 서비스 내 기능이 사용하는 복수 개의 기계학습 모델을 관리하기 위한 모델 지원 플랫폼을 클라이언트 단의 플랫폼으로 구성할 수 있다. 예를 들어, 모델 지원 플랫폼은 메신저 서비 스에서 제공하는 기능 중 채팅 감정 분석, 채팅 리액션 제안 등과 관련된 기능에 대해 메시지 감정 강도 분석을 위한 기계학습 모델을 제공하고, 채팅 이미지 검색, 이미지 태깅 등과 관련된 기능에 대해 이미지 객체 인식을 위한 기계학습 모델을 제공할 수 있다. 컴퓨터 장치의 프로세서는 이하의 모델 관리 방법을 수행하기 위한 구성요소로 구현될 수 있다. 실 시예에 따라 프로세서의 구성요소들은 선택적으로 프로세서에 포함되거나 제외될 수도 있다. 또한, 실시예에 따라 프로세서의 구성요소들은 프로세서의 기능의 표현을 위해 분리 또는 병합될 수도 있다. 이러한 프로세서 및 프로세서의 구성요소들은 이하의 모델 관리 방법이 포함하는 단계들을 수행하도 록 컴퓨터 장치를 제어할 수 있다. 예를 들어, 프로세서 및 프로세서의 구성요소들은 메모리 가 포함하는 운영체제의 코드와 적어도 하나의 프로그램의 코드에 따른 명령(instruction)을 실행하도록 구현될 수 있다. 여기서, 프로세서의 구성요소들은 컴퓨터 장치에 저장된 프로그램 코드가 제공하는 명령에 따라 프로 세서에 의해 수행되는 서로 다른 기능들(different functions)의 표현들일 수 있다. 프로세서는 컴퓨터 장치의 제어와 관련된 명령이 로딩된 메모리로부터 필요한 명령을 읽어들일 수 있다. 이 경우, 상기 읽어들인 명령은 프로세서가 이후 설명될 단계들을 실행하도록 제어하기 위한 명 령을 포함할 수 있다. 이후 설명될 모델 관리 방법이 포함하는 단계들은 도시된 순서와 다른 순서로 수행될 수 있으며, 단계들 중 일 부가 생략되거나 추가의 과정이 더 포함될 수 있다.모델 관리 방법이 포함하는 단계들은 클라이언트에서 수행될 수 있으며, 실시예에 따라서는 단계들 중 적어도 일부가 서버에서 수행되는 것 또한 가능하다. 도 6은 본 발명의 일실시예에 따른 컴퓨터 장치가 수행할 수 있는 방법의 일례를 도시한 흐름도이다. 도 6을 참조하면, 단계(S610)에서 프로세서는 클라이언트에서 제공하고자 하는 기능에 대해 모델 지원 플 랫폼 내부에서 해당 기능이 사용하는 기계학습 모델을 관리할 수 있다. 프로세서는 클라이언트에 탑 재된 플랫폼 내부에서 클라이언트 상의 서비스 어플리케이션에 포함된 기능들의 모델 사용 여부에 따라 해당 모 델을 다운로드하거나 삭제할 수 있다. 어플리케이션에 포함된 기능 중 활성화된 기능이 사용하는 모델의 경우 해당 모델 파일을 다운로드하고 비활성화된 기능이 사용하는 모델의 경우 해당 모델 파일을 삭제할 수 있다. 이때, 프로세서는 사용자 디바이스 정보를 기초로 각 기능에서 사용하는 최적 모델을 결정하여 다운로드할 수 있다. 동일한 기계학습 모델이라 하더라도 다양한 변형 모델이 존재한다. 특정 기능에 대해 모든 클라이언 트를 대상으로 공통된 모델을 제공하는 것이 아니라, 클라이언트 환경에 따라 다른 모델을 제공할 수 있다. 사 용자 디바이스 정보는 서비스 제공 환경을 나타내는 디바이스 유형, 디바이스 스펙, 소프트웨어 플랫폼(예를 들 어, 안드로이드, iOS 등), 국가 정보(언어 정보를 포함) 등을 포함할 수 있으며, 모델 지원 플랫폼은 각 기능에 대해 사용자 디바이스 정보에 적합한 모델을 다운로드하여 관리할 수 있다. 기능과 모델이 1:1은 물론 이고, 1:N, N:1, N:N으로 연결되어 있는 복잡한 사용 환경에서도 모델 지원 플랫폼을 통해 다중 모델을 용 이하게 관리할 수 있다. 프로세서는 모델 지원 플랫폼을 통해 클라이언트 어플리케이션에서 제공하 는 모든 기능에 대해 기능과 모델 간의 관계를 기반으로 각 기능이 사용하는 모델들을 통합 관리할 수 있다. 단계(S620)에서 프로세서는 클라이언트에서 사용 가능한 기계학습 모델에 대해 모델 지원 플랫폼 내 부에서 모델 성능을 측정할 수 있다. 프로세서는 같은 기능에 사용할 수 있는 모델이 복수 개인 경우 서 비스 측면에서의 성능 측정을 수행할 수 있다. 모델 리스트를 모델 지원 플랫폼에 제공하고 모델 지원 플 랫폼에서 모델 리스트에 포함된 각 모델 별로 테스트를 통해 모델 성능을 측정할 수 있다. 예를 들어, 모 델 지원 플랫폼은 메신저 내 기능인 스티커 추천 기능에 대해 기능 상의 코드 변경은 없이 추천을 위한 기 계학습 모델을 하나씩 서버에서 다운로드 받아 서비스 입장에서의 성능 항목들을 측정할 수 있다. 성능 항목은 결과 정확도(accuracy)뿐만 아니라, 메모리 사용량(memory usage)(또는 CPU 점유율), 모델 크기(model file size)(또는 저장공간 사용량), 초기 모델 로딩 속도(initialize latency), 결과 처리 속도(inference latency) 등을 포함할 수 있다. 이러한 기계학습 모델의 성능 지표는 단순히 CPU의 클럭 속도, RAM 용량과 같 이 일반적인 성능 기준에 의해 선형적으로 결정되지 않는 특징이 있다. 예를 들어, 기계학습 모델의 처리 속도 는 내부 구조에 따라서 GPU, TPU를 사용한 연산 가속, ARM 계열, x86계열과 같은 CPU 타입, 처리에 사용되는 thread 개수 등과 같은 다양한 변수에 의해서 복잡하게 성능이 결정된다. 따라서, 단순히 CPU 클럭이 빠르다고 처리 속도가 빠른 결과를 내지 않고 각 기기마다 기기의 스펙이나 사용 환경 등에 따라 모델의 성능이 달라질 수 있다. 어플리케이션에 포함된 각 기능에 대해 모델 지원 플랫폼 내부에서의 모델 성능 측정을 자동화 하여 모델 성능 측정 결과를 모델 관리 기능(S610)에서 활용할 수 있다. 예를 들어, 스티커 추천 기능이 사용 할 수 있는 복수 개의 모델 중에서 테스트를 통한 성능들이 좋은 모델을 다운로드할 수 있다. 단계(S630)에서 프로세서는 클라이언트에서의 서비스 제공 환경에 따라 동적으로 기계학습 모델을 교체할 수 있다. 다시 말해, 프로세서는 같은 기능의 기계학습 모델의 여러 버전(변형 모델)을 서비스 제공 환경 에 맞게 선택 제어하여 해당 서비스를 제공할 수 있다. 일례로, 프로세서는 사용자 디바이스의 자원 상황 (예를 들어, CPU, 메모리, 저장 공간 등)을 기초로 각 기능에 사용되는 모델을 교체할 수 있다. 예를 들어, 사 용자 디바이스에서 메모리는 부족하고 저장 공간이 여유 있는 경우 메모리 상황에 맞게 메모리 사용량이 작은 모델을 사용할 수 있다. 다른 예로, 프로세서는 기능에 대한 사용 패턴을 고려하여 해당 기능에 사용되는 모델을 교체할 수 있다. 예를 들어, 사용자가 반복적으로 자주 사용하는 기능의 경우, 메모리 사용량이 작은 모델을 이용하여 빠른 결과 처리를 제공할 수 있다. 또한, 사용자가 입력 데이터가 큰 기능을 간헐적으로 사용 하는 경우, 자원 소모가 많더라도 정확도가 높은 모델을 사용할 수 있다. 또 다른 예로, 프로세서는 기능 의 모델 사용 상황을 고려하여 모델의 스케줄링(scheduling) 또는 플래닝(planning)을 수행할 수 있다. 복수 개의 모델을 복합적으로 사용하는 기능의 경우 다른 모델과 함께 사용되는 복합 상황에 적합한 모델을 사용할 수 있으며, 이때 모델 별로 스케줄링을 설정하거나 해당 기능의 전체 제한 시간(time limit)을 위한 플래닝을 제공할 수 있다. 예를 들어, 연쇄적으로 모델을 사용하는 기능의 경우 빠른 추론을 위해 전체 처리 속도가 가 장 짧은 스케줄링 또는 플래닝을 설정할 수 있다. 또한, 사용자 디바이스의 자원에 따라 동시에 둘 이상의 추 론이 불가능한 상황이라면 모델마다 적절한 시간을 분배하거나 해당 자원 상황에 맞는 버전의 모델을 사용할 수 있다. 또한, 특정 기기가 CPU 성능은 낮지만 고성능의 TPU가 탑재되어 있는 경우, 해당 TPU 연산에 최적화되어있는 모델로 교체하여 빠른 처리 결과를 제공할 수 있다. 본 실시예에서는 기계학습 모델의 성능을 복합적으로 고려하여 각 기능에 사용되는 모델로서 서비스 제공 환경에 최적화된 성능의 기계학습 모델을 선택하여 사용할 수 있다. 도 7은 본 발명의 일실시예에 있어서 기계학습 모델을 관리하는 과정의 예시 도면이다. 프로세서는 모델 지원 플랫폼 내부에서 어플리케이션의 각 기능 별 기계학습 모델을 관리할 수 있다. 도 7을 참조하면, 모델 지원 플랫폼은 어플리케이션에 포함된 기능 중 기능 A와 기능 B가 활성화된 경우 활성화된 기능 A와 B가 사용하는 모델을 서버로부터 다운로드받아 관리할 수 있다. 예를 들어, 기능 A가 사용하는 모델로 모델 Ⅰ과 모델 Ⅱ를 다운로드받고, 기능 B가 사용하는 모델로 모델 Ⅰ, 모델 Ⅲ, 모델 Ⅳ를 관리할 수 있다. 이후, 기능 A가 비활성화되는 경우 기능 A가 사용하는 모델들을 삭제하게 되는데, 이때 기능 B에서도 사용되는 모델 Ⅰ는 삭제하지 않고 기능 A에서만 사용되는 모델 Ⅱ만을 삭제한다. 모델 지원 플랫폼은 어플리케이 션에서 제공하는 각 기능의 활성화 여부, 그리고 기능과 모델 간의 관계를 기반으로 클라이언트에서 사용되는 모든 모델들을 통합 관리할 수 있다. 도 8은 본 발명의 일실시예에 있어서 기계학습 모델 성능을 측정하는 과정의 예시 도면이다. 프로세서는 모델 지원 플랫폼 내부에서 관리 대상인 모델 리스트에 포함된 각 모델의 성능 측정을 자 동화할 수 있다. 도 8을 참조하면, 모델 지원 플랫폼은 기능 A와 기능 B가 사용하는 모델 Ⅰ, 모델 Ⅱ, 모델 Ⅲ, 모델 Ⅳ 각각에 대하여 테스트를 통해 클라이언트 환경에서의 실제 성능을 측정할 수 있다. 모델 지원 플랫폼은 모델 성능 지표로서 결과 정확도, 메모리 사용량, 모델 크기, 초기 모델 로딩 속도, 결과 처리 속도 등을 측정 할 수 있다. 모델 지원 플랫폼은 모델 관리 과정에서 같은 기능에 사용할 수 있는 변형 모델을 대상으로 성능 측정을 수행하여 성능 측정 결과를 기초로 모델을 다운로드하는 것 또한 가능하다. 예를 들어, 기능 A에서 사용하는 특정 모델이 10개 버전의 변형 모델을 가지는 경우 전체 변형 모델 풀(pool)에 대해 버전 별로 클라이언트 환경 에서의 테스트를 통해 성능을 측정한 후 해당 클라이언트에서 가장 성능 좋은 버전의 변형 모델을 기능 A를 위 한 모델로 선정할 수 있다. 도 9는 본 발명의 일실시예에 있어서 기계학습 모델을 동적으로 교체하는 과정의 예시 도면이다. 프로세서는 모델 지원 플랫폼 내부에서 클라이언트에서의 서비스 제공 환경에 따라 기계학습 모델을 변경할 수 있다. 도 9를 참조하면, 기능 A를 제공하기 위해 모델 Ⅰ과 모델 Ⅱ 중 어느 하나의 모델을 선택적으로 사용한다고 가 정한다. 이때, 모델 지원 플랫폼은 기능 A에 대해 클라이언트에서의 서비스 제공 환경이 제1 조건에 부합 하는 경우 모델 Ⅱ를 사용하고, 제2 조건에 부합하는 경우 모델 Ⅰ을 사용할 수 있다. 다시 말해, 기능 A를 제 공하기 위해 모델 Ⅰ과 모델 Ⅱ를 다운받아 놓고 제1 조건의 상황에서는 모델 Ⅱ를 사용하고, 제2 조건의 상황 에서는 모델 Ⅰ을 사용한다. 예를 들어, 번역 기능에서 입력이 되는 원문이 일정 길이 미만의 단문인 경우 모 델 Ⅱ를 사용하고, 원문이 일정 길이 이상의 장문인 경우 모델 Ⅰ을 사용한다. 또한, 메신저 내 스티커 추천 기능에서 입력 메시지의 언어가 영어인 경우 영어 기반 스티커 추천을 위한 모델 Ⅱ를 사용하고, 입력 메시지의 언어가 한글인 경우 한글 기반 스티커 추천을 위한 모델 Ⅰ을 사용한다. 그리고, 기능 B를 제공하기 위해 모델 Ⅰ, 모델 Ⅲ, 모델 Ⅳ를 복합적으로 사용한다고 가정한다. 예를 들어, 메신저 내 검색 기능은 대화방에 포함된 텍스트는 물론이고, 음성 파일과 이미지 파일을 검색 범위로 하여 텍스 트 검색을 위한 모델 Ⅰ, 음성 검색을 위한 모델 Ⅲ, 이미지 검색을 위한 모델 Ⅳ를 사용한다. 이때, 모델 지 원 플랫폼은 검색 기능에 대해 클라이언트에서의 서비스 제공 환경이 제1 조건에 부합하는 경우 모델 Ⅰ, 모델 Ⅲ, 모델 Ⅳ를 같이 이용하여 텍스트 검색과 음성 검색 및 이미지 검색을 동시에 수행할 수 있다. 제2 조 건의 서비스 제공 환경에서는 모델 별 스케줄링에 따라 텍스트 검색과 음성 검색을 먼저 수행한 후(모델 Ⅰ&Ⅲ) 다음 순서로 이미지 검색을 수행할 수 있다(모델 Ⅳ). 제3 조건의 서비스 제공 환경에서는 모델 별 스케줄링에 따라 이미지 검색을 먼저 수행한 후(모델 Ⅳ) 다음 순서로 텍스트 검색과 음성 검색을 수행할 수 있다(모델 Ⅰ& Ⅲ).실시예에 따라서는 기능 B를 위한 처리 시간을 제한하여 클라이언트의 서비스 제공 환경에 따라 모델 별 처리 시간을 차등 설정할 수 있다. 예를 들어, 모델 Ⅰ, 모델 Ⅲ, 모델 Ⅳ의 처리 시간을 제1 조건의 서비스 제공 환경에서는 1:1:1로 분배하고, 제2 조건의 서비스 제공 환경에서는 3:3:4로 분배하고, 제3 조건의 서비스 제공 환경에서는 2:2:6으로 분배할 수 있다. 사용자 디바이스(즉, 클라이언트)의 서비스 제공 환경에 대한 조건들은 물론이고, 조건 별 사용 모델, 모델 스 케줄링 규칙, 모델 플래닝 규칙 등의 프로필은 사전에 정의될 수 있다. 모델 지원 플랫폼에서 기계학습 모델 각각에 대하여 서비스 제공 환경에서의 실제 성능을 측정하게 되는데, 이러한 모델 성능 측정 결과가 서비 스 제공 환경에 대한 조건과 조건 별 모델 프로필을 정의하기 위한 근거 자료로 활용될 수 있다. 다시 말해, 모델 지원 플랫폼은 클라이언트에서 사용되는 모든 모델 리스트에 대해 모델 별 성능 측정 결과를 측정할 수 있고, 서비스 제공 환경에 따른 동적 모델 교체를 위해 모델 별 성능 측정 결과를 이용하여 서비스 제공 환 경에 대한 조건 별 사용 모델, 모델 스케줄링 규칙, 모델 플래닝 규칙 등의 프로필을 결정할 수 있다. 이처럼 본 발명의 실시예들에 따르면, 클라이언트 플랫폼 내부에서 여러 기능에서 사용되는 복수 개의 기계학습 모델을 관리할 수 있고, 클라이언트에서 사용되는 모델 리스트에 대해 플랫폼에서 각 모델의 성능을 측정할 수 있으며, 서비스 제공 환경에 따라 동일 기능에서 사용되는 모델을 동적으로 교체할 수 있다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 프로세서, 콘트롤 러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상 의 소프트웨어 어플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설"}
{"patent_id": "10-2023-0041018", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치 는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 컴퓨 터 저장 매체 또는 장치에 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분 산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 이때, 매체는 컴퓨터로 실행 가능한 프로그램을 계속 저장하거나, 실행 또는 다운로드를 위해 임시 저장하는 것일 수도 있다. 또한, 매체는 단일 또는 수 개의 하드웨어가 결합된 형태의 다양한 기록수단 또는 저장수단일 수 있는데, 어떤 컴퓨터 시스템에 직접 접속되는 매체에 한정되지 않고, 네트 워크 상에 분산 존재하는 것일 수도 있다. 매체의 예시로는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같 은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등을 포함하여 프로그램 명령어가 저장되도록 구성된 것이 있을 수 있다. 또한, 다른 매체의 예시로, 어플리케이션을 유통하는 앱 스토어나 기타 다양한 소프트웨어 를 공급 내지 유통하는 사이트, 서버 등에서 관리하는 기록매체 내지 저장매체도 들 수 있다."}
{"patent_id": "10-2023-0041018", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2023-0041018", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른 네트워크 환경의 예를 도시한 도면이다. 도 2는 본 발명의 일실시예에 따른 컴퓨터 장치의 예를 도시한 블록도이다. 도 3은 본 발명의 일실시예에 있어서 기계학습 모델 도입 과정의 예시를 도시한 것이다. 도 4는 본 발명의 일실시예에 있어서 기능과 모델 간의 관계 예시를 도시한 것이다. 도 5는 본 발명의 일실시예에 있어서 클라이언트 단에 탑재된 모델 지원 플랫폼 예시를 도시한 것이다. 도 6은 본 발명의 일실시예에 따른 컴퓨터 장치가 수행할 수 있는 방법의 일례를 도시한 순서도이다. 도 7은 본 발명의 일실시예에 있어서 기계학습 모델을 관리하는 과정의 예시 도면이다. 도 8은 본 발명의 일실시예에 있어서 기계학습 모델 성능을 측정하는 과정의 예시 도면이다. 도 9는 본 발명의 일실시예에 있어서 기계학습 모델을 동적으로 교체하는 과정의 예시 도면이다."}
