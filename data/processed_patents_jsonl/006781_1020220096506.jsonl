{"patent_id": "10-2022-0096506", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0143087", "출원번호": "10-2022-0096506", "발명의 명칭": "연합 학습 방법 및 장치", "출원인": "한국전자통신연구원", "발명자": "석진욱"}}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "클라이언트 단에서 추출된 특징 벡터 및 상기 특징 벡터에 대응되는 라벨 데이터를 수신하는 단계;상기 특징 벡터를 자기 조직화 특성 지도(SOFM)의 입력으로 하여 상기 특징 벡터에 위상 정보가 보존된 특징 벡터를 출력하는 단계; 및상기 위상 정보가 보존된 특징 벡터 및 상기 라벨 데이터를 신경망 모델의 입력으로 하여 학습시키는 단계;를 포함하는 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 클라이언트 단은 입력 데이터를 부분 연결 네트워크의 입력으로 하여 상기 특징 벡터를 추출하고, 상기 특징 벡터를 전 연결 네트워크의 입력으로 하여 상기 특징 벡터를 분류하는 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 신경망 모델의 가중치 변화율을 상기 클라이언트 단의 전 연결 네트워크로 전송하는 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 신경망 모델의 구조는 상기 클라이언트 단의 전 연결 네트워크의 구조와 대응되는 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 신경망 모델부의 손실함수 평균 변화율을 기초로 학습 시간을 가변하여 상기 자기 조직화 특성 지도의 학습을 수행하는 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 손실함수 평균 변화율은 상기 출력 벡터와 상기 라벨 데이터를 기초로 계산되는 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 자기 조직화 특성 지도는 SOFM 학습 계수를 기초로 학습 시간을 가변하여 상기 특징 벡터를 학습시키는 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 신경망 모델은 전 연결 네트워크인 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "복수의 클라이언트 단에서 추출된 특징 벡터열 및 상기 특징 벡터열에 대응되는 라벨 데이터를 수신하는 단계;공개특허 10-2023-0143087-3-및상기 특징 벡터열의 위상 정보를 보존하고, 상기 위상 정보가 보존된 상기 특징 벡터열을 신경망 모델부의 입력으로 하여 학습하는 단계;를 포함하는 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 출력 벡터열을 입력으로 하여 출력된 출력값과 상기 라벨 데이터를 기초로 서버 단 신경망 모델의 손실 함수를 계산하고, 상기 손실 함수를 기초로 경사 도함수를 계산하여 서버 단 신경망 모델부에 역전파하는 단계를더 포함하는 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 상기 출력 벡터를 출력하는 단계는 상기 특징 벡터열을 자기 조직화 특성 지도(SOFM)의 입력으로 하여 상기 특징 벡터열에 위상 정보가 보존된 출력 벡터열을 출력하는 단계, 상기 출력 벡터열을 전 연결 네트워크의 입력으로 하여 학습을 수행하여 출력 벡터를 출력하는 단계를 포함하는 연합 학습 방법."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "연합 학습을 수행하기 위한 제어 프로그램이 저장된 메모리; 및상기 메모리에 저장된 제어 프로그램을 실행하는 프로세서를 포함하고,상기 프로세서는,클라이언트 단에서 추출된 특징 벡터 및 상기 특징 벡터에 대응되는 라벨 데이터를 수신하고, 상기 특징 벡터를자기 조직화 특성 지도(SOFM)의 입력으로 하여 상기 특징 벡터에 위상 정보가 보존된 특징 벡터를 출력하고, 상기 위상 정보가 보존된 특징 벡터 및 상기 라벨 데이터를 신경망 모델의 입력으로 하여 학습시키는 연합 학습장치."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 클라이언트 단은 입력 데이터를 부분 연결 네트워크의 입력으로 하여 상기 특징 벡터를 추출하고, 상기 특징 벡터를 전 연결 네트워크의 입력으로 하여 상기 특징 벡터를 분류하는 연합 학습 장치."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 프로세서는,상기 신경망 모델의 가중치 변화율을 상기 클라이언트 단의 전 연결 네트워크로 전송하도록 제어하는 연합 학습장치."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13항에 있어서,상기 신경망 모델의 구조는 상기 클라이언트 단의 전 연결 네트워크의 구조와 대응되는 연합 학습 장치."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제12항에 있어서,상기 프로세서는,공개특허 10-2023-0143087-4-상기 신경망 모델부의 손실함수 평균 변화율을 기초로 학습 시간을 가변하여 상기 자기 조직화 특성 지도의 학습을 수행하도록 제어하는 연합 학습 장치."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 프로세서는,상기 손실함수 평균 변화율은 상기 출력 벡터와 상기 라벨 데이터를 기초로 계산되도록 제어하는 연합 학습 장치."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제12항에 있어서,상기 자기 조직화 특성 지도는 SOFM 학습 계수를 기초로 학습 시간을 가변하여 상기 특징 벡터를 학습시키는 연합 학습 장치."}
{"patent_id": "10-2022-0096506", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제12항에 있어서,상기 신경망 모델은 전 연결 네트워크인 연합 학습 장치."}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일실시예에 따른 연합 학습 방법은 클라이언트 단에서 추출된 특징 벡터 및 상기 특징 벡터에 대응되 는 라벨 데이터를 수신하는 단계와, 상기 특징 벡터를 자기 조직화 특성 지도(SOFM)의 입력으로 하여 상기 특징 벡터에 위상 정보가 보존된 특징 벡터를 출력하는 단계와, 상기 위상 정보가 보존된 특징 벡터 및 상기 라벨 데 이터를 신경망 모델의 입력으로 하여 학습시키는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 클라이언트 단과 서버 단 사이의 연합 학습을 위한 연합 학습 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근, 인공지능 기술이 발전함에 따라 영상 데이터에 대한 분류 및 의미 부여에 관한 기술들이 많은 분야에 적 용되고 있다. 인공지능 기술의 적용 대상 중 하나인 의료 분야에서는 의료 데이터의 개인 보안 문제와 맞물려 병원마다 개별 적으로 데이터를 처리하고 있다. 하지만, 개별 병원들은 보유하고 있는 진단 기기의 특성이 서로 다르기 때문에 개별 병원마다 서로 다른 진단 결과를 출력하게 된다. 이러한 문제점을 해결하기 위해, 연합 학습 등의 방법론이 제시되고 있으나, 연합 학습에 참가하는 모든 학습 네트워크가 동일해야 하는 문제와, 많은 양의 데이터가 통신을 통해 상호 전달되는 과정에서 시간 지연 및 비효 율적인 학습이 진행되는 문제가 발생된다."}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 서로 다른 특징을 가지는 네트워크에서 추출된 특징 벡터를 이용하여 데이터의 특성을 정확하 게 분류하기 위한 연합 학습 방법 및 장치를 제공하는 것이다. 또한, 본 발명의 목적은 특징 벡터의 분포를 위상 공간으로 사용하여 특징 벡터의 분류를 정확하게 수행하기 위 한 연합 학습 방법 및 장치를 제공하는 것이다."}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명에 따른 연합 학습 방법은 클라이언트 단에서 추출된 특징 벡터 및 상기 특징 벡터에 대응되는 라벨 데이터를 수신하는 단계와, 상기 특징 벡터를 자기 조직화 특성 지도(SOFM)의 입력 으로 하여 상기 특징 벡터에 위상 정보가 보존된 특징 벡터를 출력하는 단계와, 상기 위상 정보가 보존된 특징 벡터 및 상기 라벨 데이터를 신경망 모델의 입력으로 하여 학습시키는 단계를 포함할 수 있다.상기 클라이언트 단은 입력 데이터를 부분 연결 네트워크의 입력으로 하여 상기 특징 벡터를 추출하고, 상기 특 징 벡터를 전 연결 네트워크의 입력으로 하여 상기 특징 벡터를 분류할 수 있다. 상기 신경망 모델의 가중치 변화율을 상기 클라이언트 단의 전 연결 네트워크로 전송할 수 있다. 상기 신경망 모델의 구조는 상기 클라이언트 단의 전 연결 네트워크의 구조와 대응될 수 있다. 상기 신경망 모델부의 손실함수 평균 변화율을 기초로 학습 시간을 가변하여 상기 자기 조직화 특성 지도의 학 습을 수행할 수 있다. 상기 손실함수 평균 변화율은 상기 출력 벡터와 상기 라벨 데이터를 기초로 계산될 수 있다. 상기 자기 조직화 특성 지도는 SOFM 학습 계수를 기초로 학습 시간을 가변하여 상기 특징 벡터를 학습시킬 수 있다. 상기 신경망 모델은 전 연결 네트워크일 수 있다. 또한, 실시예에 따른 연합 학습 방법은 복수의 클라이언트 단에서 추출된 특징 벡터열 및 상기 특징 벡터열에 대응되는 라벨 데이터를 수신하는 단계와, 상기 특징 벡터열의 위상 정보를 보존하고, 상기 위상 정보가 보존된 상기 특징 벡터열을 신경망 모델부의 입력으로 하여 학습시킬 수 있다. 상기 출력 벡터열을 입력으로 하여 출력된 출력값과 상기 라벨 데이터를 기초로 서버 단 신경망 모델의 손실 함 수를 계산하고, 상기 손실 함수를 기초로 경사 도함수를 계산하여 서버 단 신경망 모델부에 역전파하는 단계를 더 포함할 수 있다. 상기 출력 벡터를 출력하는 단계는 상기 특징 벡터열을 자기 조직화 특성 지도(SOFM)의 입력으로 하여 상기 특 징 벡터열에 위상 정보가 보존된 출력 벡터열을 출력하는 단계, 상기 출력 벡터열을 전 연결 네트워크의 입력으 로 하여 학습을 수행하여 출력 벡터를 출력하는 단계를 포함할 수 있다. 또한, 실시예에 따른 연합 학습 장치는 연합 학습을 수행하기 위한 제어 프로그램이 저장된 메모리; 및 상기 메 모리에 저장된 제어 프로그램을 실행하는 프로세서를 포함하고, 상기 프로세서는, 클라이언트 단에서 추출된 특 징 벡터 및 상기 특징 벡터에 대응되는 라벨 데이터를 수신하고, 상기 특징 벡터를 자기 조직화 특성 지도 (SOFM)의 입력으로 하여 상기 특징 벡터에 위상 정보가 보존된 특징 벡터를 출력하고, 상기 위상 정보가 보존된 특징 벡터 및 상기 라벨 데이터를 신경망 모델의 입력으로 하여 학습시킬 수 있다. 상기 클라이언트 단은 입력 데이터를 부분 연결 네트워크의 입력으로 하여 상기 특징 벡터를 추출하고, 상기 특 징 벡터를 전 연결 네트워크의 입력으로 하여 상기 특징 벡터를 분류할 수 있다. 상기 프로세서는, 상기 신경망 모델의 가중치 변화율을 상기 클라이언트 단의 전 연결 네트워크로 전송하도록 제어할 수 있다. 상기 신경망 모델의 구조는 상기 클라이언트 단의 전 연결 네트워크의 구조와 대응될 수 있다. 상기 프로세서는, 상기 신경망 모델부의 손실함수 평균 변화율을 기초로 학습 시간을 가변하여 상기 자기 조직 화 특성 지도의 학습을 수행하도록 제어할 수 있다. 상기 프로세서는, 상기 손실함수 평균 변화율은 상기 출력 벡터와 상기 라벨 데이터를 기초로 계산되도록 제어 할 수 있다. 상기 자기 조직화 특성 지도는 SOFM 학습 계수를 기초로 학습 시간을 가변하여 상기 특징 벡터를 학습시킬 수 있다. 상기 신경망 모델은 전 연결 네트워크일 수 있다."}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 클라이언트 단에서 수집된 특징 벡터의 위상을 보존시켜 학습을 수행시킴으로써, 분류 성능을 향상시 킬 수 있다. 또한, 본 발명은 복수의 클라이언트와 서버 단의 연합 학습 장치를 통해 연합 학습을 수행함으로써, 데이터 분 류의 연산 속도를 향상시킬 수 있다.또한, 본 발명은 클라이언트 단의 특징 벡터만을 이용함으로써, 클라이언트 단의 특성은 보존하면서 데이터 처 리의 이질성을 회피할 수 있다. 또한, 본 발명은 최소한의 특징 벡터와 출력 데이터만을 이용함으로써, 보안을 강화시킬 수 있다."}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 비록 \"제1\" 또는 \"제2\" 등이 다양한 구성요소를 서술하기 위해서 사용되나, 이러한 구성요소는 상기와 같은 용 어에 의해 제한되지 않는다. 상기와 같은 용어는 단지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사 용될 수 있다. 따라서, 이하에서 언급되는 제1 구성요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있다. 본 명세서에서 사용된 용어는 실시예를 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세 서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한다 (comprises)\" 또는 \"포함하는(comprising)\"은 언급된 구성요소 또는 단계가 하나 이상의 다른 구성요소 또는 단 계의 존재 또는 추가를 배제하지 않는다는 의미를 내포한다."}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어는 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 해석될 수 있다. 또한, 일반적으로 사용되는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 본 문서에서 “A 또는 B, “A 및 B 중 적어도 하나, “A 또는 B 중 적어도 하나”, “A,B 또는 C 중 적어도 하 나”, 및 “A,B, 또는 C 중 적어도 하나”와 같은 문구들 각각은 그 문구들 중 해당하는 문구와 함께 나열된 항 목들 중 어느 하나, 또는 그들의 모든 가능한 조합을 포함할 수 있다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예들을 상세히 설명하기로 하며, 도면을 참조하여 설명할 때 동일 하거나 대응하는 구성 요소는 동일한 도면 부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 도 1은 제1 실시예에 따른 클라이언트 단의 신경망 모델을 나타낸 블록도이다. 여기서, 클라이언트 단의 신경망 모델은 개별 병원에 구비된 진단 장치일 수 있다. 도 1을 참조하면, 제1 실시예에 따른 클라이언트 단의 신경망 모델은 부분 연결 네트워크 (Partially Connected Network, 150)과 전 연결 네트워크(Fully Connected Network, 170)을 포함할 수 있다. 여기서, 부분 연결 네크워크는 네트워크를 이루는 노드들이 부분적으로 연결된 구조를 의미하고, 전 연결 네트워크는 네트워크를 이루는 노드들이 전체 연결된 구조를 의미할 수 있다.부분 연결 네트워크에 입력되는 입력 데이터는 입력 영상일 수 있다. 입력 데이터는 3개의 채널 Ci=3을 가지는 Ni X Mi 해상도의 영상일 수 있다. 여기서, 영상의 특징에 따라 Ci는 1이 될 수 있다. 라벨 데이터는 입력 데이터에 대응하는 데이터로서 클라이언트 단 신경망 모델은 지도 학습을 수행할 수 있다. 부분 연결 네트워크는 입력 데이터을 입력으로 하여 특징 벡터 를 출력할 수 있다. 전 연결 네트워크는 특징 벡터들을 분류하는 기능을 수행할 수 있다. 전 연결 네트워크에는 특징 벡 터가 입력되며 그의 출력 벡터인 를 출력할 수 있다. 여기서, R+는 0을 포함하는 양의 실수 집합 를 의미할 수 있다. 은 0을 제외한 양의 실수 집합 을 의미할 수 있다. 마찬가지로, 정수 집합 Z와 유리수의 집합 Q에 대해서도 동일한 포함 집합 ( )을 정의할 수 있다. 도 2는 제1 실시예에 따른 클라이언트 단과 연합 학습을 수행하기 위한 연합 학습 장치를 나타낸 블록도이다. 도 2에 도시된 바와 같이, 제1 실시예에 따른 연합 학습 장치는 서버 단에 배치될 수 있다. 연합 학습 장 치는 자기 조직화 특성 지도(Self-Organization Feature Map, SOFM, 230)와, 서버 단 신경망 모델을 포함할 수 있다. SOFM은 클라이언트 단의 신경망 모델로부터 추출된 특징 벡터를 입력으로 할 수 있다. SOFM은 임의의 위상 공간 상에 유사한 특징 벡터들끼리 매핑하도록 학습을 수행할 수 있다. i번째 클라이언트 단 신경망 모델에서 추출된 특징 벡터 는 SOFM를 통해 위상 정보가 보존 된 특징 벡터 를 출력할 수 있다. 서버 단 신경망 모델은 전 연결 네트워크를 포함할 수 있다. 서버 단 신경망 모델은 위상 정보가 보 존된 특징 벡터와 클라이언트 단의 신경망 모델로부터 추출된 라벨 데이터인 를 입력으로 하여 학습이 수행될 수 있다. 연합 학습 장치에는 라벨 데이터를 저장할 수 있는 메모리가 포함될 수 있다. 도 3은 제2 실시예에 따른 클라이언트 단의 신경망 모델을 나타낸 블록도이다. 도 3에 도시된 바와 같이, 제2 실시예에 따른 클라이언트 단의 신경망 모델은 복수개의 클라이언트를 포함 할 수 있다. 여기서, 복수개의 클라이언트에 각각 장착된 신경망 모델(310, 330, 350)은 서로 다른 성능을 가질 수 있다. 여기서, 복수개의 신경망 모델(310, 330, 350)의 개수는 한정되지 않는다. 복수개의 신경망 모델(310, 330, 350)은 각각 고성능 신경망, 중성능 신경망 및 저성능 신경망으로 이루어질 수 있다. 여기서, 복수개의 신경망 모델(310, 330, 350)은 서로 상이한 네트워크로 이루어지나, 특징 벡터 와 출력 벡터 의 차원은 동일하다고 가정할 수 있다. 도 4는 제2 실시예에 따른 클라이언트 단과 연합 학습을 수행하기 위한 연합 학습 장치를 나타낸 블록도이다. 도 4에 도시된 바와 같이, 제2 실시예에 따른 연합 학습 장치는 복수개의 클라이언트 단 신경망 모델들 (310, 330, 350)에서 수신된 특징 벡터를 입력으로 하여 출력 벡터를 출력할 수 있다. 연합 학습 장치에는 클라이언트 단의 신경망 모델에 대한 지표 에 대응하는 특징 벡터열 및 라벨 데이터 가 서버 단 신경망 모델부에 입력될 수 있다. 연합 학습 장치의 신경망 모델부는 특징 벡터열을 입력으로 하여 출력 를 출력할 수 있다. 연합 학습 장치의 신경망 모델부는 출력 와 라벨 데이터 를 이용하여 연합 학습 장치의 손 실 함수 를 계산할 수 있다. 신경망 모델부는 손실 함수의 경사 도함수(Gradient), 자연 경사 도함수(Natural Gradient) 등을 계산하여 연합 학습 장치에 역전파할 수 있다. 동기 학습의 경우 신경망 모델부의 가중치 변화율 은 클라이언트 단 신경망 모델(310, 330, 350)로 보내져 클라이언트 단 전 연결 신경망의 가중치 텐서의 갱신(update)에 이용될 수 있다. 도 5는 제2 실시예에 따른 연합 학습 장치의 세부 구성을 나타낸 블록도이다. 도 5에 도시된 바와 같이, 제2 실시예에 따른 연합 학습 장치의 신경망 모델부는 서버 단 SOFM과 서 버 단 전 연결 네트워크를 포함할 수 있다. 서버 단 SOFM은 특징 벡터의 위상을 보존하기 위한 것으로, 특징 벡터 를 서버 단 전 연결 네트워크 로 인가할 수 있다. 서버 단 전 연결 네트워크는 서버 단 SOFM에서 출력된 위상 정보가 보존된 특징 벡터를 입력으로 하 여 학습을 수행할 수 있다. 도 6은 동기 학습에서 SOFM의 업데이트 방식을 시간과 서버 단 손실 함수 변화에 따라 설명하기 위한 도면이다. 연합 학습 장치에서 학습이 이루어지는 초기 상태에서는 서버 단 연합 학습 장치의 손실 함수의 변화가 상대적 으로 크기 때문에 서버 단 연합 학습 장치의 손실 함수의 경사 도함수의 크기 이 상대적으로 크고, 이에 따라 SOFM의 학습은 SOFM 좌표 상에서 넓은 범위에 걸쳐 일어나게 된다. 이에 따라 SOFM의 가중치 벡터는 상대적으로 입력 값에 가깝게 갱신될 수 있다. 반면, 연합 학습 장치의 손실 함수의 변화가 작으면 경사 도함수의 크기가 작으므로, SOFM 의 학습은 SOFM 좌표 상에서 좁은 범위에 걸쳐 일어나게 되며 SOFM의 가중치 벡터는 상대적으로 평균 특징 벡터 값에 가깝게 갱신될 수 있다. 추가로, 도 2, 도 4, 도 5 및 도 6에 기재된 라벨 데이터를 일반적인 연합학습 알고리즘과 융합하기 위해 라벨 데이터가 아닌 각 클라이언트 단에서 추론 결과로 만들어진 손실 함수의 경사 도함수를 사용할 수 있다. 이 경 우 로 정의할 수 있다. 도 7은 제1 실시예에 따른 연합 학습 장치에서 수행되는 연합 학습 방법을 나타낸 순서도이다. 도 7에 도시된 바와 같이, 연합 학습 장치는 클라이언트 단에서 추출된 특징 벡터 및 상기 특징 벡터에 대응되 는 라벨 데이터를 수신할 수 있다(S100). 연합 학습 장치는 특징 벡터를 SOFM을 이용하여 특징 벡터에 위상 정보를 보존할 수 있다(S110). 연합 학습 장치는 위상 정보가 보존된 특징 벡터를 신경망 모델인 전 연결 네트워크의 입력으로 하여 학습을 수 행할 수 있다. 연합 학습 장치는 출력 벡터를 출력할 수 있다(S120). 도 8은 제2 실시예에 따른 연합 학습 장치에서 수행되는 연합 학습 방법을 나타낸 순서도이다. 도 8에 도시된 바와 같이, 연합 학습 장치는 복수의 클라이언트 단에서 추출된 특정 벡터열과 특정 벡터열에 대 응하는 라벨 데이터를 수신할 수 있다(S200). 연합 학습 장치는 특징 벡터열을 신경망 모델부의 입력으로 하여 학습을 수행할 수 있으며, 출력 벡터를 출력할 수 있다(S210). 여기서, 신경망 모델부는 SOFM과 전 연결 네트워크를 포함할 수 있다. 연합 학습 장치는 출력 벡터열을 입력으로 하여 출력된 출력 값과 라벨 데이터를 기초로 신경망 모델부의 손실 함수를 계산할 수 있다. 연합 학습 장치는 손실 함수를 기초로 경사 도함수를 계산하여 신경망 모델부에 역전파할 수 있다. 연합 학습 장치는 신경망 모델부의 가중치 텐서를 클라이언트 단의 신경망 모델부에 제공할 수 있다. 이하에서는 비동기 학습 및 동기 학습 과정에 대해 보다 상세히 설명한다. 비동기 학습을 위한 가정 도 2에 따른 클라이언트 단과 서버 단의 연합 학습 장치 사이의 비동기 학습 방법은 다음과 같다. 도 2에서 연합 학습 장치의 전 연결 네트워크와 클라이언트 단 신경망 모델의 전 연결 네트워크는 동 일한 구조를 가질 수 있다. 클라이언트 단의 부분 연결 네트워크는 입력 데이터에 대해 1차적으로 학습이 끝났다고 가정할 수 있다. 1차 학습이 끝난 후 임의의 데이터에 대한 특징 벡터 와 대응하는 라벨 데이터 를 서버 단 연 합 학습 장치로 전송될 수 있다. 서버 단의 연합 학습 장치에 수신된 특징 벡터는 SOFM의 입력으로 하여 자기학습을 수행하게 된다. SOFM 학습이 충분히 지속되어 특징 벡터의 위상 보존을 위한 SOFM이 생성되면 SOFM의 출력 에 대하여 전 연결 네트워크에서 라벨 데이터 와 함께 학습될 수 있다. 도 2의 클라이언트 단의 신경망 모델과 연합 학습 장치를 통해 다음과 같은 비동기 학습을 위한 시스 템 입출력이 이루어진다고 가정할 수 있다. 입력 데이터 가 존재하고, 이에 대응하는 특징 벡터를 이라 정의할 수 있다. 입력 데이터와 특징 데이터는 각각 입력 데이터 집합 와 특징 벡터 데이터 집합 그리고 입력 데이터 집합에 대응하는 라벨 데이터 집합이 존재하며 각각은 , 그리고 일 수 있다. 여기서, k는 데이터의 순서 지표이나, 본 실시예에서는 입력 데이터의 지표 와 특징 벡터의 지표가 동일하므로 k를 생략할 수 있다. 클라이언트 단의 출력 벡터의 한 컴포넌트 값인 가 특징 벡터가 특정 집합 C에 속하는 확률을 수학식 1로 나타낼 수 있다. [수학식 1]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수학식 1에서 sgn(x)는 시그모이드 함수 값으로 x가 참이면 1, 거짓이면 0의 값을 가질 수 있다. 특징 집합 C는 Fi에 대응하는 이진 라벨 데이터 Ti에 의해 지정될 수 있다. 예를 들어, 3차원 라벨 데이터 이 주어진다면 표시 가능한 최대의 특정 집합은 8개가 되며, 이진수 시스템에 따라 {0,1,0}을 2라고 표시하게 된다 면 특정 집합 C의 이름은 n(C)라고 하고 이진 라벨 데이터에 의해 수학식 2의 관계를 가질 수 있다. [수학식 2]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수학식 2에서 서버 단 연합 학습 장치의 전 연결 네트워크와 클라이언트 단 전 연결 네트워크는 동일한 구조를 가지고 있으므로, 서버 단이 클라이언트 단에서와 같은 특징 벡터와 라벨 데이터를 이용하여 SOFM을 통과한 일 반화 과정을 거친다 하더라도 서버 단의 출력 벡터에서의 동일 컴포넌트는 동일한 특정 집합에 속할 확률이 수 학식 3과 같을 수 있다. [수학식 3] 클라이언트 단과 서버 단의 전 연결 네트워크 구조가 동일하다고 가정할 수 있다. 클라이언트 단의 전 연결 네트워크의 가중치 텐서를 , 서버 단 연합 학습 장치의 전 연결 신경망 네트워크 의 가중치 텐서를 라고 할 수 있다. 따라서, 클라이언트 단 및 서버 단의 전 연결 네트워크의 가중치 텐 서가 형성하는 공간은 동일( )하며, 수학식 4와 같이 서버 단 연합 학습 장치의 전 연결 네트워 크의 출력이 정의될 수 있다. [수학식 4]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "상기 도시된 가정들에 따라, 전 연결 네트워크의 손실 함수를 정의할 수 있다. 수학식 4에 따라 가장 단순한 형태로 서버 단에서의 손실 함수( )는 수학식 5로 정의할 수 있다. [수학식 5]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "서버 단 전 연결 네트워크의 가중치 텐서는 확률 변수로 볼 수 있으므로, 전 연결 네트워크의 출력도 수학식 4 와 같이 확률 변수로 볼 수 있다. 따라서, 서버 단 전 연결 네트워크의 손실 함수는 수학식 6과 같이 전 연결 네트워크 출력에 대한 공분산으로 정의될 수 있다. [수학식 6]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "수학식 6의 마지막 근사 값은 실험적 기대값(Empirical Expectation) 또는 단순 산술 평균 값(Numerical Average)일 수 있다. 비동기 학습 실시예에 따른 연합 학습 장치의 비동기 학습 방법은 클라이언트 단의 학습 결과를 그대로 두고 클라이언트 단 의 학습 결과로 나온 특징 벡터의 자기 조직화 결과만을 서버 단에서 학습하여 클라이언트 단의 독자성과 서버 단의 일반성을 모두 확보하기 위한 방법일 수 있다. 순수 비동기 학습 방법에서는 클라이언트 단에서 서버 단으로 특징 벡터 집합 와 이에 특징 벡터 집합과 대응하는 라벨 데이터 만이 전송되며, 특징 벡터와 라벨 데이터가 서버 단의 메모리에 저장 되므로 특징 벡터 집합이 모두 전송되면 더 이상의 데이터 전송은 일어나지 않게 된다. 따라서, 순수 비동기 학습 방법은 클라이언트 단의 학습의 독자성을 유지하면서 일반적인 학습 결과가 필요한 분야에서 사용될 수 있다. 클라이언트 단의 분류결과와 서버 단에서 추론한 일반 결과가 서로 상이할 시 분류 정확도 확률을 상호 비교하여 분류 결과를 추론할 수 있다. 예를 들어, 클라이언트 단의 추론 결과의 가중치를 0.4라 하고, 서버 단 추론 결과를 0.6이라 하면, 입력 데이 터 Xk에 대한 클라이언트 단의 추론 확률 가 0.6이고 서버 단의 추론 결과 가 0.4라 하면 0.4 X 0.6 + 0.6 X 0.4 = 0.48이며, 추론 결과가 0.5에 미달하므로 기각이라고 판단할 수 있다. 다시 말해, SOFM을 통과한 일반화 특징 벡터의 분류 확률이 서버 단 출력 벡터에서의 각 컴포넌트의 값이 될 수 있다. 따라서, 사용자는 임의의 클라이언트 단에서 들어온 데이터의 분류 결과를 클라이언트 단의 독자적 학습 결과와 서버 단의 일반적 학습 결과와 비교하여 판단할 수 있게 된다. 물론, 비교 및 판단하는 과정은 컴퓨팅 장치에 의해 프로그램되어 수행될 수도 있다. 동기 학습을 위한 가정 도 2에 따른 클라이언트 단과 서버 단의 연합 학습 장치 사이의 동기 학습 방법은 다음과 같다. 먼저, 순수 동기 학습을 정의하기에 앞서 다음의 가정을 둘 수 있다. 클라이언트 단의 전 연결 네트워크 신경망과 서버 단 연합 장치의 전 연결 네트워크는 동일한 값으로 초기화되 어 있다고 가정할 수 있다. 동기 학습이므로 입력 데이터 가 존재하고, 이에 대한 특징 벡터를 라 할 수 있 다. 또한, 특징 벡터에 대응하는 라벨 데이터를 라 할 수 있다. 여기서, 는 클라이언트 단 에 입력되는 입력 데이터에 대한 순서 지표(index)로서 비동기 학습에서 k로 지정된 파라미터와 동일한 것으로 간주할 수 있다. 는 시간에 대한 파라미터로 보아도 무방하나, 순서 지표의 극대 값(Supremum)이 n일 경우 순서 지표 k는 t를 n으로 나누었을 때 나머지 값인 일 수 있다. 동기 학습이므로 비동기 학습의 경우와는 다르게, 시간에 대한 파라미터를 전 연결 네트워크의 가중치 텐서에 부가하여 각 데이터 및 학습 반복에 대한 지표가 있도록 하여야 한다. 따라서, 클라이언트 단의 전 연결 네트워 크 가중치 텐서를 , 서버 단의 전 연결 네트워크 가중치는 로 표시할 수 있다. 비동기 학습과 마찬가지로 동기 학습에서도 전 연결 네트워크 구조는 같다고 가정하면 전 연결 네트워크 가중치 는 모두 전 연결 네트워크 가중치 텐서 공간 W와 의 관계를 가질 수 있다. 라벨 데이터를 사용하지 않는 경우, 는 서버 단 전 연결 네트워크의 가장 마지막 계층에서 계산된 손 실 함수에 대한 경사 도함수로 놓을 수 있다. 만일 출력 계층의 차원이 k이고 출력 계층의 바로 전 계층 l의 차 원이 n이라 하면 는 으로 나타낼 수 있다. 동기 학습을 위한 SOFM의 학습 방법 먼저, 연속 시간 파라미터 를 정의하고 이에 대응되는 이산 시간에 대한 파라미터를 라 할 수 있다. 는 의 양자화된 값으로 로 정의할 수 있다. 연속 시간 파라미터는 수학식 7과 같이 갱신된다고 정의할 수 있다. [수학식 7]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "수학식 7에서, sech(x)는 쌍곡 씨컨트(Hyperbolic secant) 함수이며, 는 입력 x에 대한 비례 상수이다. 수학식 7은 수학식 8과 같이 변형되어 사용될 수 있다.[수학식 8]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "수학식 8에서 는 손실 함수 에 대한 피셔 정보 행렬(Fisher Information Matrix) 이며 은 행렬의 대각합(Trace)이다. 수학식 7에서 손실 함수의 평균 경사 도함수를 구하는 것은 단순 산술 평균 으로 구할 수도 있으나, 실시예에서는 모든 시간 지표에 대하여 경사 도함수 값을 모두 저장하게 하기 때문에 실용성이 떨어지게 된다. 대신 평균 경사 도함수를 로 놓으면 수학식 9와 같이 가중치 평균값을 얻을 수 있다. [수학식 9]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "수학식 9에서 값이 1에 가까우면 값에 민감하게 값이 가변하고, 값이 0에 가까 우면 변화에 둔감하게 가변할 수 있다. 통상의 경우 값은 0.125로 정할 수 있으나, 입력 데이터에 성질에 따라 가변할 수 있다. SOFM은 특징 벡터의 위상적 특성을 보존하기 위해 SOFM의 좌표 가 존재할 수 있다. 본 발명에서는 2차 원 자기 조직화 특성 지도임을 가정할 수 있으며, SOFM의 좌표 공간을 S라 하고 로 정의할 수 있다. 일반적으로 2차원 자기 조직화 특성 지도를 사용할 경우, 횡으로 개, 종으로 개의 점이 있 다고 하면 SOFM의 좌표 공간을 S라하고, S는 로 정의할 수 있다. SOFM 상의 좌표 상의 가중치 벡터를 라 가정할 수 있다. 수학식 8 및 수학식 9로 정의된 연속 시간 함수에 대하여 연속 시간 함수와 대응하는 이산 시간 파라미터 를 사용하여 입력으로 들어오는 특징 벡터 에 대하여 수학식 10과 같이 SOFM을 학습시킬 수 있다. [수학식 10]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "수학식 10에서 는 SOFM의 학습 계수로서 시간에 대한 단조 감소 함수이고 수학식 11의 조건을 만족할 수 있다.[수학식 11]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "수학식 11을 만족하는 SOFM의 학습 계수의 한 형식은 수학식 12와 같다. [수학식 12]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "수학식 12에서 은 인접 함수 형식을 결정하기 위한 파라미터로서 실험적으로 결정할 수 있다. 여기서, 는 1보다 큰 범위에서 작은 값을 사용하여야 하며, 는 가장 작은 값을 사용할 수 있다. 도 6을 참조하면, 서버 단 손실 함수의 경사 도함수 크기의 평균값 이 크면 수학식 7 및 수학식 8에 의해 의 변화가 작아지므로 수학식 11에 의해 학습 계수는 이전 시간의 학습 계수의 크기와 유사해져 학습 초기에는 입력 특징 벡터와 유사하게 가중치 벡터가 갱신되고, 학습 중간의 경우에는 이전 학습 계수의 크기에 비례하여 가중치 벡터가 갱신된다. 또한, 인접 함수의 크기도 학습 초기에는 넓게 정의되고 학습 중간의 경우에는 이전 인접 함수의 크기에 가깝게 가중치 벡터가 갱신될 수 있다. 반면, 서버 단 손실 함수의 경사 도함수의 크기의 평균값 이 작으면, 의 변화 가 커지므로 학습 계수는 상대적으로 감소하기 시작하고 인접 함수의 크기도 작아져 SOFM 의 개별 좌표의 가중 치 벡터는 입력 특징 벡터가 형성하는 부분 평균값으로 수렴할 수 있다. 동기 학습 동기 학습 방법은 클라이언트 단과 서버 단의 전 연결 네트워크의 가중치 텐서 를 동일한 값으 로 초기화시키고 시간 지표 t에서의 클라이언트 단 특징 벡터 를 서버 단의 SOFM에 인가시켜 SOFM 가중치 를 갱신한 후, 그 결과 값 을 서버 단의 특징 벡터로 하여 서버 단의 전 연결 네트워크의 가중치 텐서 를 클라이언트 단 특징 벡터 에 대응하는 라벨 데이터 를 사용하여 업데이트 한 후 가중치 텐서의 갱신 값 만을 다시 클라이언트 단으로 보내 클 라이언트 단의 전 연결 네트워크의 가중치 텐서 를 업데이트 하는 방법이다. 이때, 클라이언트 단의 부분 연결 네트워크의 가중치 텐서는 전 연결 네트워크 상의 첫번째 레이어에서의 가중 치 갱신 값을 기반으로 역전파 알고리즘을 통해 갱신할 수 있다. 동기 학습은 클라이언트 단 네트워크와 서버 단 네트워크의 특성을 유사하게 유지시켜 특징 벡터와 추론 결과가 클라이언트 단과 서버 단 모두 유사한 분포를 가지도록 하는 게 목적이 있다. 만일, 도 4에 도시된 바와 같이, 클라이언트 단 서로 다른 추론 엔진을 가지고 있으나, 전 연결 네트워크의 구 조가 동일한 경우, 시간 지표 t에서의 각 클라이언트 단의 특징 벡터 와 대응하는 라벨 데이 터를 입력 받은 후, SOFM의 가중치를 갱신하고 그 결과값를 라벨 데이터 를 사용하여 가중치 텐서 를 갱신할 수 있다. 그리고, 가중치 텐서의 갱신 값 을 클라이언트 단의 수 만큼 복사하여 이를 로 하여 각 클라이 언트 단에 보내는 방식이다. 종래 연합 학습에서는 각 클라이언트 단의 가중치 갱신값을 모두 받은 다음, 이의 산술 평균을 내어 서버 단과 클라이언트 단 모두 갱신하는 방식이나, 본 발명은 SOFM의 존재로 인해 서버 단에서 산술 평균을 내지 않고 서 버 단 전 연결 네트워크의 가중치 텐서 갱신 값을 그래도 클라이언트 단에 보내는 방식이다. 이를 통해 각 클라이언트 단은 서버 단에서 SOFM으로 일반화된 특징 벡터 기반의 전 연결 네트워크의 가중치 텐 서가 형성되므로 클라이언트 단의 모든 클라이언트는 부분 연결 네트워크가 서로 다르다 하더라도 전 연결 네트 워크의 통일성에 의해 유사한 추론 결과로 수렴하게 된다. 클라이언트 부분 동기/ 비동기 학습 클라이언트 단의 부분 동기/비동기 학습은 전체 시스템이 학습 완료된 상태에서 일부 시스템의 영향으로 서버 단의 일반화 성능이 떨어지거나, 일부 시스템의 추론 경향이 전체 시스템 대비 상이점이 농후할 때, 해당 시스 템을 가급적 서버 단 시스템의 분류 특성에 유사하게 교정하기 위한 방법이다. 먼저 비동기 학습을 통해 서버 단의 일반화 추론 시스템을 확립하고 각 시스템별로 시험을 거쳐 서버 단의 추론 능력 대비 국소 추론 특성이 높아 상이점이 큰 클라이언트에 대해 동기 학습을 수행할 수 있다. 이때, 서버 단의 일반화 능력을 떨어뜨리지 않기 위해, 서버 단의 전 연결 네트워크의 가중치 텐서값으로 클라 이언트 단의 전 연결 네트워크의 가중치 텐서값을 초기화시키고 클라이언트 단을 학습할 수 있다. 이때, 두가지 학습이 가능하다. 첫번째 비동기 학습 방법으로 서버 단은 학습을 진행하지 않고 클라이언트 단만 학습을 진행하는 비동기 학습으 로 서버 단의 일반화 능력을 그대로 유지하면서 클라이언트 단의 특성만 서버 단에 유사하게 교정하는 것이다. 두번째 동기 학습 방법으로 서버 단은 일반화 능력을 유지하기 위해 SOFM의 학습 파라미터 및 전 연결 네트워크 의 학습 파라미터를 작은 값으로 설정한 반면, 클라이언트 단의 학습 파라미터는 기존과 같이 가져갈 수 있다. 이를 통해 서버 단은 문제가 되는 클라이언트 단에 교정 결과가 일부 반영되고 클라이언트 단은 서버 단의 일반 화 능력을 바탕으로 클라이언트 단 고유의 학습 결과를 가지게 된다. 라벨 데이터를 사용하지 않는 동기 학습 실시예는 정보 보안을 더욱 강화하기 위해 라벨 데이터 대신 손실 함수의 경사 도함수를 이용하여 동기 학습을 진행할 수 있다. 이때, 라벨 데이터 대신 각 클라이언트 단의 손실 함수에 대하여 k차원을 가진 출력 계층과 m 차원을 가진 바로 직전 계층 사이의 가중치 텐서 에 대한 경사 도함수를 라 하면, 라벨 데이터 대신 로 설정할 수 있다. 이때, SOFM의 학습 및 갱신은 수학 식 7 및 수학식 13에서 정의된 방식대로 수행하며, 전 연결 네트워크의 갱신은 전송된 클라이언트 단의 손실함 수의 경사 도함수 텐서열에 대한 단순 산술 평균과 시간 t에서 서버 단 손실 함수의 출력 계층과 바로 직전 계 층 사이의 가중치 텐서 에 대한 경사 도함수로 설정할 수 있다. 경사 도함수는 수학식 14와 같다. [수학식 14]"}
{"patent_id": "10-2022-0096506", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "수학식 14로 정의된 경사 도함수를 서버 단의 전 연결 네트워크에서 가중치 텐서 갱신을 위한 기본 경사 도함수 로 사용하여 학습할 수 있다.실시예에 따른 연합 학습 장치는 컴퓨터 판독 가능한 기록매체와 같은 컴퓨터 시스템에서 구현될 수 있다. 도 9는 실시예에 따른 컴퓨터 시스템의 구성을 나타낸 블록도이다. 도 9를 참조하면, 실시예에 따른 컴퓨터 시스템은 버스를 통하여 서로 통신하는 하나 이상의 프로 세서, 메모리, 사용자 인터페이스 입력 장치, 사용자 인터페이스 출력 장치 및 스토리 지를 포함할 수 있다. 또한, 컴퓨터 시스템은 네트워크에 연결되는 네트워크 인터페이스를 더 포함할 수 있다. 프로세서는 중앙 처리 장치 또는 메모리나 스토리지에 저장된 프로그램 또는 프로세싱 인스트럭션들을 실 행하는 반도체 장치일 수 있다. 프로세서는 일종의 중앙처리장치로서 연합 학습 장치의 전체 동작을 제어 할 수 있다. 프로세서는 데이터를 처리할 수 있는 모든 종류의 장치를 포함할 수 있다. 여기서, '프로세서 (processor)'는 예를 들어 프로그램 내에 포함된 코드 또는 명령으로 표현된 기능을 수행하기 위해 물리적으로 구조화된 회로를 갖는, 하드웨어에 내장된 데이터 처리 장치를 의미할 수 있다. 이와 같이 하드웨어에 내장된 데이터 처리 장치의 일 예로써, 마이크로프로세서(microporcessor), 중앙처리장치(central processing unit: CPU), 프로세서 코어(processor core), 멀티프로세서(multiprocessor), ASIC(application-specific integrated circuit), FPGA(field programmable gate array) 등의 처리 장치를 망라할 수 있으나, 이에 한정되는 것은 아 니다. 메모리는 실시예에 따른 연합 학습 방법을 수행하기 위한 제어 프로그램 등 전반적인 동작을 위한 다양한 데이터가 저장될 수 있다. 구체적으로, 메모리에는 연합 학습 장치에서 구동되는 다수의 응용 프로그램, 연합 학습 장치의 동작을 위한 데이터 및 명령어가 저장될 수 있다. 메모리 및 스토리지는 휘발성 매체, 비휘발성 매체, 분리형 매체, 비분리형 매체, 통신 매체, 또는 정보 전달 매체 중에서 적어도 하나 이상을 포함하는 저장 매체일 수 있다. 예를 들어, 메모리는 ROM이나 RAM을 포함할 수 있다. 본 발명에서 설명하는 특정 실행들은 실시예들로서, 어떠한 방법으로도 본 발명의 범위를 한정하는 것은 아니다. 명세서의 간결함을 위하여, 종래 전자적인 구성들, 제어시스템들, 소프트웨어, 상기 시스템들의 다른 기능적인 측면들의 기재는 생략될 수 있다. 또한, 도면에 도시된 구성 요소들 간의 선들의 연결 또는 연결 부재 들은 기능적인 연결 및/또는 물리적 또는 회로적 연결들을 예시적으로 나타낸 것으로서, 실제 장치에서는 대체 가능하거나 추가의 다양한 기능적인 연결, 물리적인 연결, 또는 회로 연결들로서 나타내어질 수 있다. 또한, \" 필수적인\", \"중요하게\" 등과 같은 구체적인 언급이 없다면 본 발명의 적용을 위하여 반드시 필요한 구성 요소가 아닐 수 있다. 따라서, 본 발명의 사상은 상기 설명된 실시예에 국한되어 정해져서는 아니되며, 후술하는 특허청구범위뿐만 아 니라 이 특허청구범위와 균등한 또는 이로부터 등가적으로 변경된 모든 범위는 본 발명의 사상의 범주에 속한다 고 할 것이다."}
{"patent_id": "10-2022-0096506", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 제1 실시예에 따른 클라이언트 단의 신경망 모델을 나타낸 블록도이다. 도 2는 제1 실시예에 따른 클라이언트 단과 연합 학습을 수행하기 위한 연합 학습 장치를 나타낸 블록도이다. 도 3은 제2 실시예에 따른 클라이언트 단의 신경망 모델을 나타낸 블록도이다. 도 4는 제2 실시예에 따른 클라이언트 단과 연합 학습을 수행하기 위한 연합 학습 장치를 나타낸 블록도이다. 도 5는 제2 실시예에 따른 연합 학습 장치의 세부 구성을 나타낸 블록도이다. 도 6은 동기 학습에서 SOFM의 업데이트 방식을 시간과 서버단 손실 함수 변화에 따라 설명하기 위한 도면이다. 도 7은 제1 실시예에 따른 연합 학습 장치에서 수행되는 연합 학습 방법을 나타낸 순서도이다. 도 8은 제2 실시예에 따른 연합 학습 장치에서 수행되는 연합 학습 방법을 나타낸 순서도이다. 도 9는 실시예에 따른 컴퓨터 시스템의 구성을 나타낸 블록도이다."}
