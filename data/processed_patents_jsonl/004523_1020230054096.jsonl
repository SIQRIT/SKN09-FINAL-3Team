{"patent_id": "10-2023-0054096", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0157375", "출원번호": "10-2023-0054096", "발명의 명칭": "심층 강화학습 기반의 공칭 제어 보강을 수행하는 드론 제어 방법 및 그 장치", "출원인": "한국과학기술원", "발명자": "명현"}}
{"patent_id": "10-2023-0054096", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "멀티로터의 구동을 제어하는 하이브리드 제어 장치의 동작 방법으로서,PID(Proportional-Integral-Differential) 제어부를 이용하여, 레퍼런스 입력에 대한 드론의 구동을 위한 PID제어 데이터를 생성하는 단계,입력 데이터로부터 드론 구동을 위한 제어 데이터를 출력하도록 학습된 강화학습 정책을 이용하여, 상기 레퍼런스 입력에 대한 상기 드론의 구동을 위한 강화학습 제어 데이터를 생성하는 단계, 그리고두 확률 분포의 차이를 계산하는 함수를 이용하여, 상기 PID 제어 데이터와 상기 강화학습 제어 데이터 간의 확률 분포 차이가 되는 비율로 상기 PID 제어 데이터와 상기 강화학습 제어 데이터를 합산하고, 합산한 제어 데이터를 상기 드론의 구동을 위한 최종 제어 데이터로 출력하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0054096", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서,상기 출력하는 단계 이후,상기 최종 제어 데이터로 구동되는 드론의 동작을 관찰한 데이터로부터 상기 드론의 상태 정보를 추정하고, 추정한 상태 정보를 입력으로 사용하여 상기 PID 제어 데이터 및 상기 강화학습 제어 데이터를 생성하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-0054096", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에서,상기 출력하는 단계는,쿨백-라이블러 발산(Kullback-Leibler divergence, DKL)을 이용하여 상기 비율을 계산하는, 방법."}
{"patent_id": "10-2023-0054096", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에서,상기 출력하는 단계는,상기 비율을 상기 PID 제어 데이터와 상기 강화학습 제어 데이터의 가중치로 사용하여 상기 합산을 수행하는,방법."}
{"patent_id": "10-2023-0054096", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에서,상기 강화학습 정책은,도메인 무작위화(Domain Randomization)를 통해 강화학습되는, 방법."}
{"patent_id": "10-2023-0054096", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "레퍼런스 입력에 대한 드론의 구동을 위한 PID(Proportional-Integral-Differential) 제어 데이터를 생성하는PID 제어부,입력 데이터로부터 드론 구동을 위한 제어 데이터를 출력하도록 학습 강화학습 제어 정책을 이용하여, 상기 레공개특허 10-2024-0157375-3-퍼런스 입력에 대한 상기 드론의 구동을 위한 강화학습 제어 데이터를 생성하는 강화학습 제어부, 그리고두 확률 분포의 차이를 계산하는 함수를 이용하여, 상기 PID 제어 데이터와 상기 강화학습 제어 데이터 간의 확률 분포 차이가 되는 비율로 상기 PID 제어 데이터와 상기 강화학습 제어 데이터를 합산하고, 합산한 제어 데이터를 상기 드론의 구동을 위한 최종 제어 데이터로 출력하는 불확실성 인식 믹서를 포함하는, 하이브리드 제어 장치."}
{"patent_id": "10-2023-0054096", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에서,상기 최종 제어 데이터로 구동되는 드론의 동작을 관찰한 데이터로부터 상기 드론의 상태 정보를 추정하고, 추정한 상태 정보를 입력으로 사용하여 상기 PID 제어 데이터 및 상기 강화학습 제어 데이터를 생성하는 상태 추정부를 더 포함하는, 하이브리드 제어 장치."}
{"patent_id": "10-2023-0054096", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에서,상기 강화학습 제어부는,쿨백-라이블러 발산(Kullback-Leibler divergence, DKL)을 이용하여 불확실성 정도인 상기 비율을 계산하는, 하이브리드 제어 장치."}
{"patent_id": "10-2023-0054096", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에서,상기 불확실성 인식 믹서는,상기 비율을 상기 PID 제어 데이터와 상기 강화학습 제어 데이터의 가중치를 산출하는데 사용하여 상기 합산을수행하는, 하이브리드 제어 장치."}
{"patent_id": "10-2023-0054096", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제6항에서,상기 강화학습 제어부는,도메인 무작위화(Domain Randomization)를 통해 강화 학습된 보조 정책을 이용하여 상기 강화학습 제어 데이터를 생성하는, 하이브리드 제어 장치."}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "드론을 제어하기 위한 하이브리드 제어 장치는 PID(Proportional-Integral-Differential) 제어기를 이용하여, 레퍼런스 입력에 대한 드론의 구동을 위한 PID 제어 데이터를 생성하는 단계, 입력 데이터로부터 드론 구동을 위 한 제어 데이터를 출력하도록 학습된 강화학습 모듈을 이용하여, 상기 레퍼런스 입력에 대한 상기 드론의 구동을 위한 강화학습 제어 데이터를 생성하는 단계, 그리고 두 확률 분포의 차이를 계산하는 함수를 이용하여, 상기 PID 제어 데이터와 상기 강화학습 제어 데이터 간의 확률 분포 차이가 되는 비율로 상기 PID 제어 데이터와 상기 강화학습 제어 데이터를 합산하고, 합산한 제어 데이터를 상기 드론의 구동을 위한 최종 제어 데이터로 출력하는 단계를 포함하는 동작을 수행할 수 있다."}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 심층 강화학습 기반의 공칭 제어 보강을 수행하는 드론 제어 방법 및 그 장치에 관한 것이다."}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "CAROS-Q와 같은 틸팅로터(TILTING-ROTOR) 드론(Drone)은 도시 지역에서 잠재적인 어플리케이션들 때문에 최근 몇 년간 상당한 관심을 받고 있다. 이러한 어플리케이션의 예시로서, 구조적인 검사(structural inspection), 접촉 기반 벽 청소(contact-based wall-cleaning), 6-DoF 조작(manipulation)이 있다. 이러한 비정형 (atypical) 구성들은 전방향성(omnidirectionality), 벽 근접 검사(close-wall inspection)를 위한 벽과 프로 펠러들 사이에 충돌 방지의 측면에서 이점을 제공하지만, 틸팅로터 드론의 컨트롤러 설계에는 상당한 문제를 야 기하게 된다. 멀티로터(multirotor) 플랫폼을 제어하기 위한 기본 솔루션은 공칭 컨트롤러(nominal controller)라고 하는 개 별 로터 제어를 위한 동적 할당 매트릭스를 계산하는 세심한 시스템 모델링을 통한 것이다. 그러나 공칭 컨트롤러는 예컨대, 벽 효과(wall effect)로 알려진 반작용 기류(counteracting airflow)로 인한 교란 (disturbances), 그리고 구조물과의 상호 작용(interaction with a structure)과 같은 실제 불확실성(real- world uncertainties)으로 인해 어려움을 겪을 수 있다. 따라서, 월 클라이밍(wall-climbing)과 같은 보다 복잡한 시나리오에서 성능을 향상시키기 위해서는 강력한 성 능을 갖춘 제어 전략이 필요한 실정이다."}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는, 안정적인 공칭 컨트롤러를 강력한 성능을 가진 강화학습 정책을 이용하는 심층 강화학습 컨트롤러로 보강하는 하이브리드 제어 구조를 통해 틸팅로터 드론을 위한 제어 데이터를 생성하는 방법 및 그 장치를 제공 하는 것이다."}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "한 특징에 따르면, PID(Proportional-Integral-Differential) 제어부를 이용하여, 레퍼런스 입력에 대한 드론 의 구동을 위한 PID 제어 데이터를 생성하는 단계, 입력 데이터로부터 드론 구동을 위한 제어 데이터를 출력하 도록 학습된 강화학습 정책을 이용하여, 상기 레퍼런스 입력에 대한 상기 드론의 구동을 위한 강화학습 제어 데 이터를 생성하는 단계, 그리고 두 확률 분포의 차이를 계산하는 함수를 이용하여, 상기 PID 제어 데이터와 상기 강화학습 제어 데이터 간의 확률 분포 차이가 되는 비율로 상기 PID 제어 데이터와 상기 강화학습 제어 데이터 를 합산하고, 합산한 제어 데이터를 상기 드론의 구동을 위한 최종 제어 데이터로 출력하는 단계를 포함한다. 상기 출력하는 단계 이후, 상기 최종 제어 데이터로 구동되는 드론의 동작을 관찰한 데이터로부터 상기 드론의 상태 정보를 추정하고, 추정한 상태 정보를 입력으로 사용하여 상기 PID 제어 데이터 및 상기 강화학습 제어 데 이터를 생성하는 단계를 더 포함할 수 있다. 상기 출력하는 단계는, 쿨백-라이블러 발산(Kullback-Leibler divergence, DKL)을 이용하여 상기 비율을 계산할 수 있다. 상기 출력하는 단계는, 상기 비율을 상기 PID 제어 데이터와 상기 강화학습 제어 데이터의 가중치로 사용하여 상기 합산을 수행할 수 있다. 상기 강화학습 정책은, 도메인 무작위화(Domain Randomization)를 통해 강화 학습 될 수 있다. 다른 특징에 따르면, 하이브리드 제어 장치는 레퍼런스 입력에 대한 드론의 구동을 위한 PID(Proportional- Integral-Differential) 제어 데이터를 생성하는 PID 제어부, 입력 데이터로부터 드론 구동을 위한 제어 데이터 를 출력하도록 학습 강화학습 제어 정책을 이용하여, 상기 레퍼런스 입력에 대한 상기 드론의 구동을 위한 강화 학습 제어 데이터를 생성하는 강화학습 제어부, 그리고 두 확률 분포의 차이를 계산하는 함수를 이용하여, 상기 PID 제어 데이터와 상기 강화학습 제어 데이터 간의 확률 분포 차이가 되는 비율로 상기 PID 제어 데이터와 상 기 강화학습 제어 데이터를 합산하고, 합산한 제어 데이터를 상기 드론의 구동을 위한 최종 제어 데이터로 출력 하는 불확실성 인식 믹서를 포함한다. 상기 하이브리드 제어 장치는 상기 최종 제어 데이터로 구동되는 드론의 동작을 관찰한 데이터로부터 상기 드론 의 상태 정보를 추정하고, 추정한 상태 정보를 입력으로 사용하여 상기 PID 제어 데이터 및 상기 강화학습 제어 데이터를 생성하는 상태 추정부를 더 포함할 수 있다. 상기 강화학습 제어부는, 쿨백-라이블러 발산(Kullback-Leibler divergence, DKL)을 이용하여 불확실성 정도인 상기 비율을 계산할 수 있다. 상기 불확실성 인식 믹서는, 상기 비율을 상기 PID 제어 데이터와 상기 강화학습 제어 데이터의 가중치를 산출 하는데 사용하여 상기 합산을 수행할 수 있다. 상기 강화학습 제어부는, 도메인 무작위화(Domain Randomization)를 통해 강화 학습된 보조 정책을 이용하여 상 기 강화학습 제어 데이터를 생성할 수 있다."}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 불확실성을 기반으로 심층 강화학습 컨트롤러와 공칭 컨트롤러를 적절히 융합하여 공칭 컨트 롤러가지는 안정성과 학습 기반의 컨트롤러가 가지는 강인성을 모두 확보할 수 있다."}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였 다. 설명에서, 도면 부호 및 이름은 설명의 편의를 위해 붙인 것으로서, 장치들이 반드시 도면 부호나 이름으로 한 정되는 것은 아니다. 설명에서, 어떤 부분이 어떤 구성요소를 \"포함\" 한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 명세서에 기재된 \"…부\", \"…기\", \"…모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 본 개시의 장치는 적어도 하나의 프로세서가 명령어들(instructions)을 실행함으로써, 본 개시의 동작을 수행할 수 있도록 구성 및 연결된 컴퓨팅 장치이다. 컴퓨터 프로그램은 프로세서가 본 개시의 동작을 실행하도록 기술 된 명령어들(instructions)을 포함하고, 비일시적-컴퓨터 판독가능 저장매체(non-transitory computer readable storage medium)에 저장될 수 있다. 컴퓨터 프로그램은 네트워크를 통해 다운로드 되거나, 제품 형태 로 판매될 수 있다. 본 개시의 인공지능 모델(Artificial Intelligence model, AI model)은 적어도 하나의 태스크(task)를 학습하 는 기계학습모델로서, 프로세서에 의해 실행되는 컴퓨터 프로그램으로 구현될 수 있다. 인공지능 모델이 학습하 는 태스크란, 기계 학습을 통해 해결하고자 하는 과제 또는 기계 학습을 통해 수행하고자 하는 작업을 지칭할 수 있다. 인공지능 모델은 컴퓨팅 장치에서 실행되는 컴퓨터 프로그램으로 구현될 수 있고, 네트워크를 통해 다 운로드 되거나, 제품 형태로 판매될 수 있다. 또는 인공지능 모델은 네트워크를 통해 다양한 장치들과 연동할 수 있다. 도 1은 실시예에 따른 하이브리드 제어 장치의 구성도이고, 도 2는 도 1의 심층(Deep) 강화학습(Reinforcing Learning, RL) 컨트롤러(Controller)를 자세히 나타낸 구성도이고, 도 3은 실시예에 따른 드론(Drone)의 하드웨 어 파라미터들을 나타낸 도면이다. 도 1을 참조하면, 하이브리드 제어 장치는 멀티로터이며, 틸팅로터-헥사로터(tilting-rotor hexarotor)의 한 예시인 CAROS-Q(이하, '드론'으로 통칭함)의 구동을 제어한다. 하이브리드 제어 장치는 공칭 컨트롤러(Nominal Controller), 심층 강화학습(Deep RL) 컨트롤러 , 불확실성 인식 믹서(uncertainty aware mixer), 상태 추정부 및 모션 플래너(Motion Planner)를 포함할 수 있다.하이브리드 제어 장치는 심층 강화학습 컨트롤러를 통해 공칭 컨트롤러를 강화하는 장치로서, 레트로(Retro)-RL 시스템이라 호칭할 수 있다. CAROS-Q와 같은 틸팅로터 드론은 복잡한 공기역학적인 영향들이 작용되고, 공칭 컨트롤러에서는 이러 한 작용 영향들을 수학적으로 모델링하여야 하나, 이러한 모델링은 쉽지 않다. 공칭 컨트롤러와 달리 학습 기반 접근 방식은 실제 불확실성에 대한 강력한 성능과 시뮬레이션에서 모델 없는 훈련을 이용할 수 있다는 장점이 있으며, 특히, 심층 강화학습 알고리즘은 드론을 제어하는데 효과적 이다. 하지만, 심층 강화학습 알고리즘은 실제 세계에서 실행 가능성을 보장하기 위한 안정성은 부족한 한계가 있다. 즉, 공칭 컨트롤러는 환경 변수를 정해놓고 모델링하므로 안정적인 제어가 가능하지만, 다양한 변수가 존 재하는 환경을 반영하지 못한다. 반면, 심층 강화학습은 불확실성이 높은 환경(예, 외란)에서도 잘 작동하지만, out-of-distribution과 같은 상황에서 쉽게 실패하므로 안정성을 보장할 수 없다. 따라서, 하이브리드 제어 장 치는 공칭 컨트롤러를 심층 강화학습으로 보강하는 구조를 가진다. 하이브리드 제어 장치는 종래 기술과 달리, 불확실성을 기반으로 심층 강화학습 컨트롤러와 공칭 컨 트롤러를 적절히 융합하여 공칭 컨트롤러의 안정성과 심층 강화학습 컨트롤러의 강인성을 확보 하고, 하이브리드 제어 장치의 안정성을 보장하였음을 랴푸노프 안정성(Lyapunov stability) 이론을 통하 여 입증한다. 공칭 컨트롤러의 공칭 정책(nominal policy)은 로 정의되고, 는 공칭 컨트롤러에서 구축된 결 정론적(deterministic) 제어 정책이다. 공칭 컨트롤러는 동역학 모델(dynamics model)에 기초하여 틸팅로터 드론(이하, '드론'으로 통칭함)을 구 동하기 위한 공칭 제어 데이터를 생성하여 출력한다. 공칭 컨트롤러는 일반적인 PID(Proportional-Integral-Differential) 컨트롤러를 일컫는다. 드론 제어를 위한 PID 컨트롤러는 널리 알려진 기술이므로, PID 컨트롤러 자체의 구성 및 동작에 대한 자세한 설명은 생략한 다. 공칭 컨트롤러는 위치(Position) 컨트롤러, 속도(Velocity) 컨트롤러, 자세(Attitude) 컨트롤 러, 레이트(Rate) 컨트롤러 및 할당 매트릭스(Allocation Matrix)를 포함할 수 있다. 모션 플래너가 출력하는 드론의 위치(pG)는 공칭 컨트롤러의 위치 컨트롤러와 심층 강화학 습 컨트롤러에게 입력 데이터로 출력된다. 위치 컨트롤러는 모션 플래너로부터 드론의 위치(pG)를 와 자세 쿼터니언(quaternion)인 qG를 산출한다. 위치 컨트롤러가 산출한 qG는 속도 컨트롤러와 심층 강화학습 컨트롤러의 입력 데이 터로 출력된다. 속도 컨트롤러는 위치 컨트롤러로부터 pG, qG를 입력 받고, 목표 프레임에 표시되는 선형 속도(linear velocity)인 vG를 산출한다. 속도 컨트롤러가 산출한 vG는 자세 컨트롤러와 심층 강화학습 컨트롤러 의 입력 데이터로 출력된다. 자세 컨트롤러는 속도 컨트롤러로부터 pG, qG, vG를 입력 받고 프레임에 표시되는 각속도(angular velocity)인 를 산출한다. 자세 컨트롤러가 산출한 는 레이트 컨트롤러와 심층 강화학습 컨 트롤러의 입력 데이터로 출력된다. 레이트 컨트롤러는 자세 컨트롤러로부터 pG, qG, vG, 를 입력받아 할당 매트릭스 계산부로 출력한다. 할당 매트릭스 계산부는 pG, qG, vG, 를 이용하여, 은 공칭 제어 정책을 계산하여 출력한다. 공칭 컨트롤러는 공칭 정책( )에 기초하여, 입력 데이터에 대한 드론을 구동하기 위한 공칭 제어 데이터를 생성한다. 공칭 제어 데이터는 공칭 정책( )에 대해 도출된 제어 액션( )을 말한다. 공칭 정책( )은 의사-디커플링(quasi-decoupling) 컨트롤러인 공칭 컨트롤러로부터 파생된다. 의사-디커 플링 컨트롤러는 PID 제어기를 CAROS 형태에 맞추어 변형된 것이다. 의사-디커플링 컨트롤러는 MPC(model predictive control), MRAC(model reference adaptive control) 및 DOB(disturbance observer)를 포함하는 공 지된 제어 양식들에 비해 단순성이 있어, 본 발명의 실시예에서 채택되었으며, 배포 중에 온라인 최적화가 필요 하지 않다. 의사-디커플링 컨트롤러는 이미 공지된 기술이므로, 자세한 설명은 생략한다. 심층 강화학습 컨트롤러의 보조 정책(auxiliary policy)은 로 정의되고, 는 심층 강화학습 알고 리즘으로 훈련된 확률적 학습 기반 제어(probabilistic learning-based controller) 정책이다. 심층 강화학습 컨트롤러는 드론의 목표 프레임에 표시되는 상태 정보를 입력 받는다. 심층 강화학습 컨트롤러는 보조 정책( )에 기초하여, 입력 데이터, 즉, 드론의 상태 정보에 대한 드론을 구동하기 위한 강화학습 제어 데이터를 생성한다. 강화학습 제어 데이터는 보조 정책( )에 대해 도출된 제어 액션( )을 말한다. 무한-수평(infinite-horizon) MDP(Markov decision process)로서 환경은 튜플(tuple), 즉, M =(S, A, d0, p, r)로 정의된다. 환경 중에서, 상태 공간은 s ∈ S 이고, 행동 공간은 a ∈ A로 정의되며, 연속적이다. 환경 중에서, d0은 초기 상태 분포 d0(s0)이다. p는 시스템 작동 방식을 설명하는 형식, 즉, 의 상태 전이 확률이다. 은 보상 함수이다. 드론의 멀티로터의 상태를 완전히 관찰할 수 있다고 가정하면, 드론의 목표 프레임에 멀티로터의 자 세를 표현하여 관성 프레임에서 목표 프레임의 위치를 변경하는 것만으로 드론을 제어할 수 있다. 드론의 관찰 벡터(14×1 벡터)는 이다. 여기서, 여기서, pG는 목표 프레임에 표시되는 드론의 위치이다. qG는 목표 프레임에 표시되는 자세 쿼터니언(quaternion)이다. vG는 목표 프레임에 표시되는 선형 속도(linear velocity)이다. 는 목표 프레임에 표시되는 각속도(angular velocity)이다. 는 드론 의 바디 프레임의 프론트 로터의 틸팅 각도 w.r.t를 의미하며 설정된 값이다. 모션 플래너는 드론의 목표 프레임에 표시되는 드론의 위치(pG)를 공칭 컨트롤러와 심층 강화 학습 컨트롤러에게 출력한다. 공칭 컨트롤러는 qG, vG, 를 산출하며, 산출한 qG, vG, 를 심층 강화 학습 컨트롤러에게 출력 한다. 공칭 컨트롤러는 pG, qG, vG, 를 이용하여, 드론을 구동하기 위한 제어 데이터, 즉, 를 생성한 다. 심층 강화학습 컨트롤러에 입력되는 드론의 상태 정보는 st=[pG, qG, vG, , ]로 구성된다. 도 2를 참조하면, 심층 강화학습 컨트롤러는 가우시안 분포를 따르는 섭동 함수(perturbation function)를 이용하여 상태 정보(st)를 훼손시켜 배치 사이즈가 N인 적대적 상태들의 배치(a batch of adversarial states) 인 를 생성한다. 심층 강화학습 컨트롤러는 보조 정책( )에 기초하여, 훼손된 상태 정보, 즉, 주어진 적대적 상태 ( )에 대한 드론 구동을 위한 강화학습 제어 데이터( )를 생성한다. 이때, 심층 강화학습 컨트롤러는 도 1의 시뮬레이션 장치를 통해 강화 학습을 통해 생성된 보조 정책 ( )을 이용한다. 보조 정책( ) 학습을 위한 알고리즘은 다음과 같다. 보조 정책( )은 공칭 컨트롤러를 지원하는 강한 성능을 제공하도록 설계된다. 따라서, 보조 정책 ( )은 도메인 무작위화로 시뮬레이션 장치에서 훈련되어 광범위한 불확실성에 대한 일반화를 가져올 수 있다. 도메인 무작위화를 통한 정책 학습은 메타 학습, 다중 작업 학습 또는 병렬 탐색과 같은 여러 방법을 통해 수행 할 수 있다. 도메인 무작위화(Domain Randomization)에 대해 설명하면 다음과 같다. 도메인 무작위화는 시뮬레이션 장치에서 훈련된 보조 정책( )이 광범위한 불확실성을 학습할 수 있도록 하여 실제 환경에 배포될 때 보조 정책( )의 견고성을 향상시킨다. 따라서, 이 작업은 학습된 보조 정책 ( )에 파라메트릭 하드웨어 불확실성(parametric hardware uncertainties)을 캡처하기 위해 도메인 무작위 화를 활용한다. 도메인 무작위화는 시뮬레이션 장치의 모든 재설정 환경에 고정 빈도로 적용된다. 정책이 호버링 및 포즈 변경 결합 작업을 학습하도록 훈련된 종래 기술과 달리 호버링 작업에 대해서만 보조 정책( )이 훈련된다. 그 후, 에이전트는 기울기 각도 를 환경의 변수로 인식한다. 도메인 무작위화에 포즈 변경 작업을 포함하여 실현된다. 는 각 도메인 랜덤화에 대해 세트 에서 랜덤하게 선택된 값으로 설정된다. 따라서, 보조 정책 ( )은 주어진 에 대한 호버링 상태를 추가로 탐색할 수 있다. 또한, 환경의 절반만 공칭 호버링 능력을 보존하기 위해 무작위로 지정되고 나머지 절반은 에서 호버링 하는 방법을 계속 학습한다. 도메인 무작위화는 다른 파라미터들, 즉, 질량, 공칭 CoG 위치 및 바디 프레임에서 로터까지의 거리에도 적용되 었다. 이러한 파라미터들은 드론의 역학이 비선형적으로 파라미터들에 의존하기 때문에 무작위화 되었다. 그러므로, 보조 정책( )은 이러한 파라메트릭 불확실성을 학습하도록 훈련되었다."}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이러한 매개변수의 무작위화 범위에 대한 요약은 표 1에 나와 있으며, 공칭 값은 실제 드론의 구성 요소와 3DCAD(Computer-Aided Drawing) 소프트웨어에서 측정되었다. 표 1 HARDWARE PARAMETERS Parameter Value Randomization range Units Weight 3.475 [3.2, 3.6] Kg Max.thrust of each rotor [29] 19.94 - N Stall torque of servo [30] 10.6 - Nm 85.55 [80.0, 90.0] mm 182.0 [170.0, 190.0] mm 287.0 [270.0, 300.0] mm Norminal CoG (x) 55.2 [40.0, 65.0] mm Norminal CoG (y) 0.0 [-10.0, 10.0] mmNorminal CoG (z) 8.2 [3.0, 15.0] mm 메타 학습의 빠른 적응 능력은 플랫폼에 덜 유리한 계산 부담을 야기할 온라인 기울기 업데이트가 필요하기 때 문에 목표가 되지 않는다. 또한, 제안된 하이브리드 제어 장치가 드론의 여러 파라메트릭 불확실성만 무작위화하고 여러가지 작업을 무작위화 하지 않는다는 점을 고려할 때 다중 작업 학습도 적합하지 않다. 따라 서, 실시예에서는 병행 탐사 방법을 선택하였다. 즉, 시뮬레이션 장치는 A2C(Advantage Actor-Critic)라는 동기식 버전에 기초한 강화학습을 수행하여 보조 정책( )을 생성하고, 지속적인 액터-크리틱(actor-critic) 구조를 활용하여 PPO(proximal policy optimization) 알고리즘을 통해 보조 정책( )을 최적화할 수 있다. 이때, A2C와 PPO는 이미 알려진 기술이므로, 설명은 생략한다. 이와 같이, 공칭 컨트롤러와 심층 강화학습 컨트롤러는 드론을 제어하기 위한 정책들, 즉, , 을 기초로, 주어진 상태( )에 대해 서로 다른 두가지 제어 액션들( , )을 생성한다. 불확실성-인식 믹서는 두 제어 액션들, 즉, 을 이용한 제어 액션( )과 을 이용한 제어 액션 ( )을 통계적으로 결합한 하이브리드 제어 액션( ) 즉, 하이브리드 제어 데이터를 최종적으로 드론 을 제어하기 위한 데이터로 출력한다. 상태 추정부는 하이브리드 제어 액션( )을 입력 받아 동작하는 드론의 상태 정보를 추정하고, 추정한 상태 정보를 공칭 컨트롤러에게 출력한다. 추정한 상태 정보는 공칭 컨트롤러를 통해 심층 강 화학습 컨트롤러에게 전달되며, 이때, 전달되는 상태 정보는 st=[pG, qG, vG, , ]로 구성된다. 불확실성-인식 믹서의 주요 목적은 불확실성의 정도를 사용하여 에 의해 생성된 보조 제어 액션에 대 한 불확실성 가중치를 적용하는 것이다. 여기서, 불확실성 가중치는 주어진 상태를 위한 보조 정책의 액션에서 불확실성의 추정이다. 주어진 적대적 상태( )에서 보조 정책( )의 조치는 다음 수학식 1과 같이 샘플링 된다. [수학식 1]"}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "보조 정책( )의 불확실성 정도는 공칭 제어 및 보조 제어에 가중치를 부여하기 위해 계산된다. 상대적 엔트로피 이론(Relative entropy theory)이 불확실성의 정도를 결정하기 위해 채택되고, 상대적 엔트로 피 이론은 쿨백-라이블러 발산(Kullback-Leibler divergence, DKL)을 사용하여 구현된다. 불확실성 측정 방법은 세단계 프로세스에서 수행될 수 있다. 첫번째, 의 평균과 분산은 가우시안 분포 P로서 컴퓨팅되어 저장된다. 결과적으로, 임의의 액션, 은 타겟 분포 Q를 구성하기 위해 로부터 무작위로 샘플 링되고, 이는 분산 을 가진 에 중심을 둔 가우시안 분포이며, 다음 수학식 2와 같이 표현할 수 있다. [수학식 2]"}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, 은 가우시안 분포이다. Q의 분산은 의 불확실성이 허용되는 정도를 정의하는 조정 가능한 하이퍼 파라미터이다. 분산이 큰 경우, 의 불확실한 액션들은 더욱 자주 불확실성-인식 제어 믹서에 의해 수용될 수 있다. 반대로 작은 차이는 불 확실한 행동을 더 엄격하게 거부할 수 있다. 수학적 단순성을 위해 실험에 사용할 단위 분산을 선택했다. 마지 막으로, 불확실성은 를 계산하여 측정된다. 큰 값은 P와 Q 사이에 큰 불일치가 있음을 나타낸다. 그러면, Q가 로부터 구성되기 때문에 P와 Q 사이에 불일치는 가 적대적 액션들, 즉, 에 의해 구성된 가우시안 분포에 의해 표현될 수 없음을 나타 낸다. 따라서, 은 주어진 상태, 에 대해 높은 불확실성을 갖는다고 결론지을 수 있다. 대조적으로, 작은 값은 P와 Q가 통계적으로 유사함을 보여주고, 이 주어진 상태, 에 대해 낮은 불확실성을 갖는다고 추론할 수 있다. 마지막으로 추정된 불확실성 정도를 사용하여 하이브리드 제어 액션은 다음 수학식 3과 같이 나타낼 수 있다. [수학식 3]"}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, 은 공칭 정책, 즉, 에 대해 도출된 제어 액션이고, 은 보조 정책, 즉, 에 대해 도출된 제어 액션이다. 수학식 3에서, 이 높은 불확실성을 가질 때 제어 액션에 대한 기여도가 감소하고 그 반대의 경우도 있음을 암시한다. 주요 초점은 Y자형 동축 구성을 가진 종래 기술에서 제작된 틸팅 헥사로터 드론에 Retro-RL을 적용하는 것 이며, 이는 도 3에 나타나 있다. 도 3을 참조하면, 도 3의 (a)는 드론의 사시도이고, 도 3의 (b)는 드론의 측면도이다. 도 3은 드론 의 사시도와 측면도를 통해 하드웨어 관련 파라미터들을 나타낸다. {I}는 inertial frame를 나타낸다. {B}는 body frame을 나타낸다. {G}는 goal frame을 나타낸다. 은 로터 오 프셋 길이이다. 은 바디 중심에서 로터까지의 거리를 나타낸다. 는 바디 프레임에서 후방 로터까지의 거리 이다. 는 CoG의 x 위치이고, 는 CoG의 z위치이고, m은 드론의 질량이고, 는 중력 가속도에 해당한 다. 는 바디 프레임의 프론트 로터의 틸팅 각도 w.r.t이다. 상태 추정부의 상태 추정 동작에 대해 설명하면 다음과 같다. 드론의 몸체 프레임은 무게 중심(center of gravity, CoG)에 정의되었다. 따라서, 바디 프레임 위치는 팔- 기울이기 조작(arm-tilting maneuver)으로 인해 CoG가 변경되면 더 이상 유효하지 않다. 바디 프레임이 재정의되고 틸팅 액슬에 고정되어 추력과 모멘트 평형 방정식의 새로운 공식이 다음 수학식 4와 같이 생성될 수 있다. [수학식 4] 는 바디 프레임의 x축의 추력이고, 는 바디 프레임의 z축의 추력이다. 는 바디 프레임의 y축 방향 의 모멘트이고, F1은 프론트 로터에 의해 생성되는 추력이고, F2는 리어 로터에 의해 발생하는 추력이다. m은 드 론의 질량이고, 는 중력 가속도에 해당한다. 는 바디 프레임의 프론트 로터의 틸팅 각도 w.r.t이다. CoG 용어들, 와 는 틸팅 액슬의 바디 프레임의 정의로 인해 도입되었다. 구현 레벨, 즉, 와 는 CAD 모델 데이터로부터 획득한 방정식을 사용하여 추정된다. 는 현재 위치를 유지하면서 피치 변경이 가능한 바디 프레임의 원하는 피치 각도이다. 수학식 4를 풀면, θ는 바디 프레임의 현재 피치 각도이고, 는 바디 프레임의 프론트 로터의 틸팅 각도 w.r.t 이다. 결과 는 쿼터니언 형식(quaternion form), 즉, 에서 원하는 방향으로 자세 제어기에 대한 입력으로 사 용된다. 속도 컨트롤러 및 추력 할당을 포함한 후속 단계들은 PX4 컨트롤 스택에서 멀티로터 컨트롤을 위한 표준 절차들 을 따른다. 간단히, 멀티로터 컨트롤러 표준 절차를 설명하면, 모션 플래너로부터 위치(pG)를 입력 받고 상태 추정값 으로 를 입력받은 위치 컨트롤러가 qG를 출력하고, qG를 입력 받은 속도 컨트롤러가 qG를 출력하 고, qG를 입력받은 자세 컨트롤러가 vG를 출력하고, 레이트 컨트롤러가 vG로부터 를 출력하는 일 련의 동작으로 정의될 수 있다. 추력은 레이트 컨트롤러에서 출력되는 값이다. 공지된 기술을 이용하여, 와 의 결합들을 완전히 학습하는 대신에, 공칭 컨트롤러에 의한 의사-디커 플링(quasi-decoupling) 컨트롤은 최적의 를 발견하도록 정책을 가이드 하는 보상 함수에 채택된다. 오리엔 테이션 보상에서 타겟 피치로서 를 설정하였다. 공칭 컨트롤러에 의한 상태 표현은 다음과 같다. 앞서 언급한 바와 같이, 의사-디커플링(quasi-decoupling)은 해당 에 대한 목표 피치 각도로 θdes를 출력한다. θdes는 qG에서 목표 피치로서 채택된다. 정책 네트워크는 관찰 벡터를 수신하고 6개의 개별 로터 추력(thrust)으로 구성된 6×1 액션 벡터를 출력한다. 실제 구현에서 추력은 [6]에서와 같이 추력 매핑을 사용하여 로터 속도로 변환된다. 심층 강화학습(Deep RL) 컨트롤러에 의한 입증 가능한 안정적인 보상은 다음과 같다. 심층 강화학습(Deep RL) 컨트롤러에 의해 안정성이 저해되지 않음을 나타낸다.보상 디자인은 학습된 정책 의 동작을 형성하기 때문에 deep RL의 중요한 측면이다. 또한, 높은 안정성 요구 사항이 있는 시스템에 대한 중 요성이 증가한다. 멀티로터 드론 제어에 대한 다른 연구[13]는 포즈 및 속도 오류를 고려하고 최소한의 제어 노력으로 포즈 추적 정책을 학습하기 위해 전력 소비 기간을 포함하여 보상 기능을 구성했다. 이전 연구들은 드론의 안정성 보장이 부족했다. 보상 기능의 일부로 포즈 및 속도 추적 오류의 선택이 Retro-RL에서 제안된 하이브리드 설정에 통합될 때 공칭 시스템의 안정성(교란 없음)을 보장할 수 있음을 보여주며, 이는 Lyapunov 안정성 정리를 사용하여 증명될 수 있다. 정리(Theorem) 1: 상태 및 시스템 역학을 가지고 묘사되는 틸팅 헥사로터 드론 시스템을 수학식 5와 같이 나타 낼 수 있다. [수학식 5]"}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "틸팅 헥사로터 드론 시스템의 평형점(equilibrium point)은 원점에서 정의된다. 주어진 시스템에 대해 후보 Lyapunov 함수 V(x)가 존재하며, 이는 방사형으로 제한되지 않은 양의 정부호 함수 이고 그의 1차 도함수 V(x)는 음의 정부호이다. 평형점은 전체적으로 균일하고 점근적으로 안정적이다. 증명: 평형점의 안정성을 증명하기 위해, 우리는 방사형으로 제한되지 않고 양의 정부호인 적절한 Lyapunov 함 수를 정의하고, 이 Lyapunov 함수의 1차 도함수는 음의 정부호이다. 보상 함수를 다음과 같이 정의한다. [수학식 6]"}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "여기서, 는 위치 보상이고, 는 자세 보상이고, 는 선속도 보상이고, 는 각 속도 보상이다. 모든 이러한 보상 함수들은 양의 정부호 함수인 의 형식을 따른다. 증명의 예로, 위치 보상에 대한 후보 Lyapunov 함수를 다음 수학식 7과 같이 정의한다. [수학식 7]"}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "수학식 7은 양의 정부호 함수이며 방사형으로 제한되지 않는다다. 수학식 7의 후보 Lyapunov 함수의 1차 도함수는 다음 수학식 8과 같이 도출할 수 있다. [수학식 8]"}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기서, x, y 및 z는 목표 프레임에 대한 드론 본체 프레임의 3D 위치 좌표이고 , , 는 첫번째 도함수 (derivatives) w.r.t 시간이다. 원래 형태에서 수학식 8의 음의 한정성은 직접 결론을 내릴 수 없다. 그러나, 일반성을 잃지 않고, 이 문제는 다음 수학식 9와 같이 피드백 제어 법칙을 사용하는 Retro-RL의 하이브 리드 구조에서 가능한 위치 컨트롤러에 대한 피드백 제어 법칙을 적용하여 해결할 수 있다. [수학식 9]"}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "여기서, 제어 이득 K는 이 음수 한정이 되게 하는 양의 대각 행렬이다. 따라서, 의 음의 정부호 가 보장된다. Retro-RL 알고리즘은 공칭 컨트롤러에서 생성된 포즈 및 속도 목표를 할당하여 안정성을 강화한다. 이러한 목표 는 참조로 사용되며 추적 오류는 정책 네트워크의 입력 벡터를 구성한다. 따라서, Retro-RL에서 수학식 6의 형 태로 보상 함수를 채택하면 전역적으로 균일하며 점근적으로 안정적인 평형점이 생성된다.보상 함수의 다른 모든 항은 의 형식을 따르기 때문에 동일한 접근 방식을 사용하여 안정성을 증명할 수 있다. 수학식 9에서 피드백 제어 법칙은 단순화를 위해 간단한 비례 게인 K로 제공된다. 그러나 위치 컨트롤러에 사용되는 PID 및 선형 2차 조정기(linear quadratic regulator, LQR)와 같은 모든 부 류의 네거티브 피드백 제어 법칙은 이론적으로 제안된 프레임워크와 함께 작동할 수 있다. Reward function을 PID 제어기에 적절히 융합하여, PID 제어기가 가지는 안정성을 저해하지 않고, 이는 Lyapnov stability 이론을 통해 입증되며, 이에 대해 설명하면 다음과 같다. 또한, 교란된 시스템의 안정성 분석은 추가 교란 를 수학식 5의 에 도입하여 확장할 수 있다. 일반화를 위해 는 사라지지 않는 섭동(perturbation)이라고 가정한다. 섭동 시스템(perturbed system)에 대한 후 보 Lyapunov 함수의 1차 도함수(derivative)는 다음 수학식 10과 같이 유도된다. [수학식 10]"}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "수학식 5의 Theorem 1에 따르면, 수학식 10의 우변의 처음 두 항은 수학식 8과 같으므로, 음의 정부호이다. 수 학식 8을 수학식 10으로 대체하면, 다음 수학식 11과 같다. [수학식 11]"}
{"patent_id": "10-2023-0054096", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "의 음의 한정성을 보장하기 위해 수학식 11의 우변의 두번째 항을 충분히 작게 제한하여 가 여 전히 수학식 11의 마지막 항보다 더 음수이다. 이를 위해 를 작은 값으로 제한할 수 있으며, 이는 섭 동 속성에 대한 추가 가정 또는 지식으로 결정할 수 있다. 수학식 11의 마지막 항은 (x, y, z)의 매우 작은 값으로도 제한될 수 있지만 서로 다른 방향, 즉, 0+ 또는 0-에 서 0에 접근할 때 서로 다른 한계를 가진다. 따라서, 방정식을 단순화하고 교란된 시스템의 궁극적인 경계를 표시하기 위해 뚜렷한 경계를 얻으려면 교란의 한계 에 대한 추가 가정이 필요하다. 그러나 섭동의 유형과 자세한 속성에 대한 추가 조사는 향후 작 업을 위해 남겨둔다. 이처럼, 수학식 10과 수학식 11을 통해 예컨대, 사라지지 않는 외란과 같이 심층 강화 학습을 활용하면서도 동 시에 안정성을 확보할 수 있음을 증명할 수 있다. 도 4는 실시예에 따른 하이브리드 제어 방법을 나타낸 순서도로서, 도 1 ~ 도 3에서 설명한 하이브리드 제어 장 치의 동작을 순차적으로 설명한다. 도 4를 참조하면, 공칭 컨트롤러에게 모션 플래너로부터 드론의 위치값이 입력되고, 공칭 컨트 롤러에 의해 위치값으로부터 산출된 드론의 상태 정보가 심층 강화학습 컨트롤러에게 입력된다 (S101). 공칭 컨트롤러는 S101에서 입력된 위치값을 통해 드론의 상태 정보를 생성하고, 드론의 상태 정보로 부터 공칭 정책에 따른 공칭 제어 데이터를 생성한다(S102). 심층 강화학습 컨트롤러는 S101에서 모션 플래너로부터 입력받은 위치값, 그리고 공칭 컨트롤러(11 0)로부터 입력받은 속도, 자세, 각속도로 구성된 상태 정보에 대해 보조 정책에 따른 강화학습 제어 데이터를 생성한다(S103). 불확실성 인식 믹서는 공칭 제어 데이터와 강화학습 제어 데이터 간의 확률 분포 차이가 최소가 되는 비율, 즉, 불확실성 정도인 를 산출한다(S104). 불확실성 인식 믹서는 S104에서 산출한 를 이용하여 수학식 3에 정의된 하이브리드 제어 데이 터를 생성(S105)하고, 이러한 하이브리드 제어 데이터를 드론에게 출력한다(S106). 상태 추정부는 드론의 구동 관찰로부터 위치, 자세 쿼터니언, 선형 속도, 각속도, 틸팅 각도로 구성 된 현재 상태를 추정(S107)하고, 추정한 정보를 공칭 제어기로 출력한다. 추정한 현재 상태는 위치를 제외 한 속도, 자세, 각속도이며, 이들 정보를 이용하여 공칭 제어 데이터, 강화학습 제어 데이터가 생성된다. 도 5는 서로 다른 컨트롤러로 제어되는 드론의 포즈 추적을 비교한 그래프이다. 도 5를 참조하면, Position Error 그래프는 위치 추적을 위한 RMSE(Root Mean Squared Error)를 나타내고, Pitch Error 그래프는 절대 피치 오류를 나타낸다. 박스 플롯(box plot) 아래 숫자(값)는 비행 모드의 시간 경 과에 따른 RMSE를 나타낸다. 측정은 호버링 15초 동안 수행되었다. 컨트롤러는 비교 대상인 종래 기술로서, 의사-디커플링(Quasi-Decoupling), PPO adopted, 본 발명의 실시예 (Retro-RL)가 사용되었다. Regular 상태는 틸팅을 하지 않은 드론의 비행을 의미한다. Regular 상태에서, Position error, Pitch error는 모두 종래 기술에 비해 본 발명의 실시예가 작은 값을 가진다. Tilted 상태는 틸팅된 드론을 의미한다. Tilted 상태에서, Pitch error는 종래 기술보다 본 발명의 실시예가 큰 값을 가지지만, Position error는 본 발명의 실시예가 종래 기술보다 작은 값을 가진다. Wall 상태는 틸팅된 드론이 벽에 부착된 후를 의미한다. Wall 상태에서, Pitch error는 종래 기술보다 본 발명 의 실시예가 큰 값을 가지지만, Position error는 본 발명의 실시예가 종래 기술보다 작은 값을 가진다. 이와 같이, Position error는 모든 상태에서 종래 기술보다 본 발명의 실시예가 작은 값을 가지고, 특히, Wall 상태에서 작은 값을 가지므로, 본 발명의 성능이 종래 기술보다 우세함을 알 수 있다. 도 6은 서로 다른 컨트롤러로 제어되는 드론의 Lemniscate 궤적 추적을 비교한 그래프이다. 도 6을 참조하면, α= 10에 대해 서로 다른 컨트롤러, 즉, 의사-디커플링(Quasi-Decoupling), PPO adopted, 본 발명의 실시예(Retro-RL)로 제어되는 CAROS-Q의 Lemniscate 궤적 추적 비교한 그래프이다. Retro-RL을 사용하여 제어되는 드론은 학습된 정책에서 보장된 안정성과 강력한 성능 덕분에 Lemniscate 궤적을 성공적으로 탐색하고 다른 컨트롤러를 능가하고, 특히 Retro-RL 컨트롤러는 PPO 채택 컨트롤러에 비해 컨트롤러 의 불확실성 인식 활성화로 인해 더 안정적임을 알 수 있다. 한편, 도 7은 실시예에 따른 컴퓨팅 장치의 하드웨어 구성을 나타낸 블록도이다. 도 7을 참고하면, 도 1 ~ 도 6에서 설명한 장치(100, 300)는 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치로 구현될 수 있다. 컴퓨팅 장치는 하나 이상의 프로세서, 프로세서에 의하여 수행되는 컴퓨터 프로그램을 로드 하 는 메모리, 컴퓨터 프로그램 및 각종 데이터를 저장하는 저장 장치, 통신 인터페이스, 그리고 이들을 연결하는 버스를 포함할 수 있다. 이외에도, 컴퓨팅 장치는 다양한 구성 요소가 더 포함될 수 있다. 프로세서는 컴퓨팅 장치의 동작을 제어하는 장치로서, 컴퓨터 프로그램에 포함된 명령어들을 처리하 는 다양한 형태의 프로세서일 수 있고, 예를 들면, CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit) 또는 본 개시의 기술 분야에 잘 알려진 임의의 형태의 프로세서 중 적어도 하나를 포함하여 구성될 수 있다. 메모리는 각종 데이터, 명령 및/또는 정보를 저장한다. 메모리는 본 개시의 동작을 실행하도록 기술 된 명령어들이 프로세서에 의해 처리되도록 해당 컴퓨터 프로그램을 저장 장치로부터 로드할 수 있다. 메모리는 예를 들면, ROM(read only memory), RAM(random access memory) 등 일 수 있다. 저장 장치는 컴퓨터 프로그램, 각종 데이터를 비임시적으로 저장할 수 있다. 저장 장치는 ROM(Read Only Memory), EPROM(Erasable Programmable ROM), EEPROM(Electrically Erasable Programmable ROM), 플래시 메모리 등과 같은 비휘발성 메모리, 하드 디스크, 착탈형 디스크, 또는 본 개시가 속하는 기술 분야에서 잘 알 려진 임의의 형태의 컴퓨터로 읽을 수 있는 기록 매체를 포함하여 구성될 수 있다. 통신 장치는 유/무선 통신을 지원하는 유/무선 통신 모듈일 수 있다. 버스는 컴퓨팅 장치의 구성 요소 간 통신 기능을 제공한다. 컴퓨터 프로그램은, 프로세서에 의해 실행되는 명령어들(instructions)을 포함하고, 비일시적-컴퓨터 판독 가능 저장매체(non-transitory computer readable storage medium)에 저장되며, 명령어들은 프로세서가 본 개시의 동작을 실행하도록 만든다. 컴퓨터 프로그램은 네트워크를 통해 다운로드되거나, 제품 형태로 판매될 수 있다. 이상에서 설명한 본 발명의 실시예는 장치 및 방법을 통해서만 구현이 되는 것은 아니며, 본 발명의 실시예의 구성에 대응하는 기능을 실현하는 프로그램 또는 그 프로그램이 기록된 기록 매체를 통해 구현될 수도 있다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속하는 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7"}
{"patent_id": "10-2023-0054096", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 실시예에 따른 하이브리드 제어 시스템의 구성도이다. 도 2는 도 1의 심층 강화학습 컨트롤러를 자세히 나타낸 구성도이다. 도 3은 실시예에 따른 드론의 하드웨어 파라미터들을 나타낸 도면이다. 도 4는 실시예에 따른 하이브리드 제어 방법을 나타낸 순서도이다. 도 5는 서로 다른 컨트롤러로 제어되는 드론의 포즈 추적을 비교한 그래프이다. 도 6은 서로 다른 컨트롤러로 제어되는 드론의 Lemniscate 궤적 추적을 비교한 그래프이다. 도 7은 실시예에 따른 컴퓨팅 장치의 구성을 나타낸 블록도이다."}
