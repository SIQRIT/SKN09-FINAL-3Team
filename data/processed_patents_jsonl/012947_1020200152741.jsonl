{"patent_id": "10-2020-0152741", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0066564", "출원번호": "10-2020-0152741", "발명의 명칭": "가상 피팅 서비스를 제공하는 방법 및 이를 위한 시스템", "출원인": "씨제이올리브네트웍스 주식회사", "발명자": "손민성"}}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "중앙처리유닛 및 메모리를 포함하는 장치를 통해 가상 피팅 서비스를 제공하는 방법에 있어서,제1 의류 이미지를, 모델 이미지 내 모델의 자세에 대응되도록 변형하여 제2 의류 이미지를 생성하는 이미지 변형 단계; 및상기 제2 의류 이미지 및 가상모델 이미지 - 상기 가상모델 이미지는, 상기 모델 이미지로부터 재구성된 가상모델을 포함하는 이미지임 - 를 합성하여 가상 피팅 이미지를 생성하는 이미지 합성 단계;를 포함하는,가상 피팅 서비스를 제공하는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 이미지 변형 단계 이전, 데이터 전처리 단계를 더 포함하되,상기 데이터 전처리 단계는,원본 의류 이미지로부터 제1 의류 이미지 및 제1 마스킹 이미지 - 상기 제1 의류 이미지는, 상기 원본 의류 이미지로부터 적어도 일부의 영역이 수정된 상태의 이미지임 - 를 생성하고, 상기 모델 이미지로부터 분할 이미지 및 포즈 데이터 - 상기 분할 이미지는, 상기 모델 이미지 내 모델이 차지하는 영역을 복수 개의 영역들로 분할한 상태의 이미지이고, 상기 포즈 데이터는, 상기 모델 이미지 내 모델의자세를 정의하기 위한 값을 포함함 - 를 생성하는 단계인 것을 특징으로 하는,가상 피팅 서비스를 제공하는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 이미지 변형 단계는, 상기 제1 마스킹 이미지를, 상기 모델 이미지 내 모델의 자세에 대응되도록 변형하여 제2 마스킹 이미지를 생성하는 단계를 더 포함하는,가상 피팅 서비스를 제공하는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 제1 마스킹 이미지는, 상기 제1 의류 이미지 중 의류가 차지하는 영역과 그 외의 영역이 각각 구별된 상태의 것인, 가상 피팅 서비스를 제공하는 방법.공개특허 10-2022-0066564-3-청구항 5 제4항에 있어서,상기 이미지 변형 단계는,상기 제1 의류 이미지 및 제1 마스킹 이미지를, 상기 분할 이미지 또는 포즈 데이터 중 적어도 하나를 참고하여상기 모델의 자세에 대응되도록 변형하여 제2 의류 이미지 및 제2 마스킹 이미지를 생성하는 단계인 것을 특징으로 하는,가상 피팅 서비스를 제공하는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 제1 의류 이미지는, 상기 원본 의류 이미지로부터 넥라인(neck line) 및 배면 라인에 의해 둘러싸인 영역이 크롭(crop)된 된 이미지인 것을 특징으로 하는,가상 피팅 서비스를 제공하는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서,상기 이미지 합성 단계는,상기 제2 의류 이미지, 제2 마스킹 이미지, 분할 이미지, 및 포즈 데이터를 입력으로 하여 가상 피팅 이미지를생성하는 것을 특징으로 하는,가상 피팅 서비스를 제공하는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 가상모델은, 인공지능 알고리즘에 의해 생성된 분할 영역, 및 상기 모델 이미지로부터 차용된 영역의 조합에 의해 생성된 것을 특징으로 하는,가상 피팅 서비스를 제공하는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 이미지 합성 단계 이후, 상기 가상 피팅 이미지 내 가상모델의 피부톤을 보정하는 피부톤 보정 단계를 더포함하는 것을 특징으로 하는,가상 피팅 서비스를 제공하는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "공개특허 10-2022-0066564-4-제9항에 있어서,상기 피부톤 보정 단계는,상기 모델 이미지 내 임의 영역으로부터 상기 모델의 피부톤 정보를 획득하는 단계;상기 피부톤 정보를 참고하여, 상기 가상 피팅 이미지 내 가상모델의 적어도 일부 영역에 대해 피부톤을 보정하는 단계;를 포함하는 것을 특징으로 하는,가상 피팅 서비스를 제공하는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항에 있어서,상기 이미지 합성 단계는,상기 모델 이미지 내 임의 영역으로부터 상기 모델의 피부톤 정보를 획득하는 단계;상기 인공지능 알고리즘에 상기 피부톤 정보를 조건으로 설정하는 단계;를 더 포함하고,상기 인공지능 알고리즘은, 상기 피부톤 정보를 참고하여 가상모델의 분할 영역을 생성하는 것을 특징으로하는,가상 피팅 서비스를 제공하는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "중앙처리유닛 및 메모리를 포함하는 장치를 이용하여 이미지 변형 모델을 학습시키는 방법에 있어서,제1 의류 이미지를 임의의 변형 파라미터에 따라 왜곡시켜 제2 의류 이미지를 생성하는 의류 이미지 왜곡 단계;학습용 모델 이미지로부터, 의류 영역만 추출하는 의류 영역 추출 단계; 및상기 제2 의류 이미지와 상기 추출된 의류 영역을 비교하여 정합도를 판단하는 정합도 판단 단계;를 포함하는,이미지 변형 모델을 학습시키는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 의류 이미지 왜곡 단계 이전 데이터 전처리 단계를 더 포함하되,상기 데이터 전처리 단계는,학습용 의류 이미지로부터 제1 의류 이미지 및 제1 마스킹 이미지 - 상기 제1 의류 이미지는, 상기 학습용 의류이미지로부터 적어도 일부의 영역이 수정된 상태의 이미지임 - 를 생성하고,상기 학습용 모델 이미지로부터 분할 이미지 및 포즈 데이터 - 상기 분할 이미지는, 상기 학습용 모델 이미지내 모델이 차지하는 영역을 복수 개의 영역들로 분할한 상태의 이미지이고, 상기 포즈 데이터는, 상기 학습용모델 이미지 내 모델의 자세를 정의하기 위한 값을 포함함 - 를 생성하는 단계인 것을 특징으로 하는,공개특허 10-2022-0066564-5-이미지 변형 모델을 학습시키는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 의류 이미지 왜곡 단계는,상기 제1 마스킹 이미지를 상기 변형 파라미터에 따라 왜곡시켜 제2 마스킹 이미지를 생성하는 단계를 더 포함하는,이미지 변형 모델을 학습시키는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 변형 파라미터는, 제1 의류 이미지 내 임의 영역을 정의하기 위한 파라미터, 상기 임의 영역을 구성하는 픽셀들을 일정량만큼 횡또는 종 방향으로 시프트(shift)시키기 위한 파라미터, 임의 픽셀들의 색깔을 바꾸기 위한 파라미터 또는 임의픽셀들을 삭제하기 위한 파라미터 중 적어도 하나를 포함하는 것을 특징으로 하는,이미지 변형 모델을 학습시키는 방법."}
{"patent_id": "10-2020-0152741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "중앙처리유닛 및 메모리를 포함하는 장치를 이용하여 이미지 합성 모델을 학습시키는 방법에 있어서,가상모델 이미지 - 상기 가상모델 이미지는, 학습용 모델 이미지, 분할 이미지, 또는 포즈 데이터 중 적어도 하나로부터 재구성된 가상모델이 포함된 이미지임 - 와 제2 의류 이미지를 합성하여 가상 피팅 이미지를 생성하는이미지 합성 단계;상기 가상 피팅 이미지와 상기 학습용 모델 이미지를 비교하여 정합도를 판단하는 정합도 판단 단계;를 포함하는,이미지 합성 모델을 학습시키는 방법."}
{"patent_id": "10-2020-0152741", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 가상 피팅 서비스를 제공하는 방법 및 이를 위한 시스템에 관한 것으로, 임의의 의류를 촬영한 이미지 와 모델(사람)을 촬영한 이미지가 제공되었을 때, 상기 모델 상에 상기 의류를 가상적으로 입혀 본 상태의 가상 피팅 이미지를 제공함으로써 온라인 상에서 가상적으로 옷을 입어 보는 것과 같은 경험을 제공할 수 있도록 한 서비스에 관한 것이다."}
{"patent_id": "10-2020-0152741", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 가상 피팅 서비스를 제공하는 방법 및 이를 위한 시스템에 관한 것으로, 임의의 의류를 촬영한 이미 지와 모델(사람)을 촬영한 이미지가 제공되었을 때, 상기 모델 상에 상기 의류를 가상적으로 입혀 본 상태의 가 상 피팅 이미지를 제공함으로써 온라인 상에서 가상적으로 옷을 입어 보는 것과 같은 경험을 제공할 수 있도록 한 서비스에 관한 것이다."}
{"patent_id": "10-2020-0152741", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "소비자들이 옷을 살 때에 온라인 쇼핑보다 오프라인 쇼핑을 더 선호하는 이유는, 옷이란 직접 입어 보고 자신에 게 맞는지 여부를 보고 사야 하는 상품이라는 인식이 강하게 있기 때문이다. 이렇듯 직접 입어본 후 구매하는 것을 선호하는 소비자들의 심리 때문에 온라인에서의 의류 구매는 상대적으로 활발하게 성장하지 못한 측면이 있으며, 특히 가격이 비싼 옷일수록 이러한 구매 경향은 더욱 뚜렷하게 드러나고 있다. 한편 소비자들은 오프라인 구매에 따른 불편함을 조금이라도 덜어보고자 구매 후기, 블로그 후기 등을 검색해 참고하기도 하지만, 이러한 후기들은 어디까지나 타인의 경험담을 적은 것들이어서 정작 자신에게는 큰 도움이 되지 않는 경우가 많으며, 결국 소비자는 오프라인 매장을 직접 찾아 원하던 옷을 입어 본 후에야 최종적으로 구매를 하는 것으로 귀결되는 사례가 많다. 이에 따라 온라인 상에서 의류 구매를 할 때에 자신의 체형에 맞는지 여부를 확인할 수 있도록 한 가상 피팅 서 비스에 대한 수요가 높아지고 있으며, 이러한 수요는 특히 최근의 감염병 확산 사태 때문에도 더욱 높아지고 있 다. 최근에는 바이러스 감염병에 따른 피해가 심각해 지면서 쇼핑몰을 직접 방문하거나 특히 다른 사람이 입어 본 옷을 다시 입어 보는 행위 등이 크게 제약을 받는 상황이 되었는데, 이에 따라 소비자들은 어쩔 수 없이 옷 을 입어 보지도 못한 채 사야 하는 상황에 직면하게 되었으며, 나아가서는 옷을 오랜 기간 동안 사지 못하는 상 황까지 초래되고 있는 실정이다. 본 발명은 이와 같이 비대면 온라인 쇼핑의 활성화를 촉진하고, 소비자들의 편의성을 높이기 위해 제안된 것이다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허공보 10-1165076 B1 (2012.07.05)"}
{"patent_id": "10-2020-0152741", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 사용자들이 의류 이미지 및 모델 이미지만을 업로드 하면 해당 모델 이미지 상에 의류 이미지를 합성 하여 가상 피팅 이미지를 제공함으로써 사용자가 오프라인에서 옷을 직접 입어본 것과 유사한 경험을 할 수 있 는 환경을 제공하는 것을 목적으로 한다. 또한 본 발명은 가상 피팅 이미지를 생성하는 과정을 보다 정교화 하기 위해 인공지능 학습 방법을 제공하는 것 을 목적으로 한다. 또한 본 발명은 최종적으로 사용자에게 제공되는 가상 피팅 이미지의 품질을 높이기 위해 모델의 실제 피부톤을 기준으로 다른 신체 부위 피부톤을 표현해 낼 수 있게 하는 방법을 제공하는 것을 목적으로 한다. 한편, 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2020-0152741", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "위와 같은 문제점을 해결하기 위하여, 본 발명에 따른 가상 피팅 서비스 제공 방법은, 중앙처리유닛 및 메모리 를 포함하는 장치를 통해 제공되되, 상기 방법은 제1 의류 이미지를, 모델 이미지 내 모델의 자세에 대응되도록 변형하여 제2 의류 이미지를 생성하는 이미지 변형 단계; 및 상기 제2 의류 이미지 및 가상모델 이미지 - 상기 가상모델 이미지는, 상기 모델 이미지로부터 재구성된 가상모델을 포함하는 이미지임 - 를 합성하여 가상 피팅 이미지를 생성하는 이미지 합성 단계; 를 포함한다. 또한 상기 가상 피팅 서비스 제공 방법은, 상기 이미지 변형 단계 이전, 데이터 전처리 단계를 더 포함하되, 상 기 데이터 전처리 단계는, 원본 의류 이미지로부터 제1 의류 이미지 및 제1 마스킹 이미지 - 상기 제1 의류 이 미지는, 상기 원본 의류 이미지로부터 적어도 일부의 영역이 수정된 상태의 이미지임 - 를 생성하고, 상기 모델 이미지로부터 분할 이미지 및 포즈 데이터 - 상기 분할 이미지는, 상기 모델 이미지 내 모델이 차지하는 영역을 복수 개의 영역들로 분할한 상태의 이미지이고, 상기 포즈 데이터는, 상기 모델 이미지 내 모델의 자세를 정의 하기 위한 값을 포함함 - 를 생성하는 단계인 것을 특징으로 할 수 있다. 또한 상기 가상 피팅 서비스 제공 방법에 있어서 상기 이미지 변형 단계는, 상기 제1 마스킹 이미지를, 상기 모 델 이미지 내 모델의 자세에 대응되도록 변형하여 제2 마스킹 이미지를 생성하는 단계를 더 포함할 수 있다. 또한 상기 가상 피팅 서비스 제공 방법에 있어서 상기 제1 마스킹 이미지는, 상기 제1 의류 이미지 중 의류가 차지하는 영역과 그 외의 영역이 각각 구별된 상태의 것인 것을 특징으로 할 수 있다.또한 상기 가상 피팅 서비스 제공 방법에 있어서 상기 이미지 변형 단계는, 상기 제1 의류 이미지 및 제1 마스 킹 이미지를, 상기 분할 이미지 또는 포즈 데이터 중 적어도 하나를 참고하여 상기 모델의 자세에 대응되도록 변형하여 제2 의류 이미지 및 제2 마스킹 이미지를 생성하는 단계인 것을 특징으로 할 수 있다. 또한 상기 가상 피팅 서비스 제공 방법에 있어서 상기 제1 의류 이미지는, 상기 원본 의류 이미지로부터 넥라인 (neck line) 및 배면 라인에 의해 둘러싸인 영역이 크롭(crop)된 된 이미지인 것을 특징으로 할 수 있다. 또한 상기 가상 피팅 서비스 제공 방법에 있어서 상기 이미지 합성 단계는, 상기 제2 의류 이미지, 제2 마스킹 이미지, 분할 이미지, 및 포즈 데이터를 입력으로 하여 가상 피팅 이미지를 생성하는 것을 특징으로 할 수 있다. 또한 상기 가상 피팅 서비스 제공 방법에 있어서 상기 가상모델은, 인공지능 알고리즘에 의해 생성된 분할 영역, 및 상기 모델 이미지로부터 차용된 영역의 조합에 의해 생성된 것을 특징으로 할 수 있다. 또한 이 때 상기 이미지 합성 단계 이후, 상기 가상 피팅 이미지 내 가상모델의 피부톤을 보정하는 피부톤 보정 단계를 더 포함하는 것을 특징으로 할 수 있다. 또한, 상기 피부톤 보정 단계는, 상기 모델 이미지 내 임의 영역으로부터 상기 모델의 피부톤 정보를 획득하는 단계; 상기 피부톤 정보를 참고하여, 상기 가상 피팅 이미지 내 가상모델의 적어도 일부 영역에 대해 피부톤을 보정하는 단계; 를 포함하는 것을 특징으로 할 수 있다. 또한 상기 가상 피팅 서비스 제공 방법에 있어서 상기 이미지 합성 단계는, 상기 모델 이미지 내 임의 영역으로 부터 상기 모델의 피부톤 정보를 획득하는 단계; 상기 인공지능 알고리즘에 상기 피부톤 정보를 조건으로 설정 하는 단계;를 더 포함하고, 상기 인공지능 알고리즘은, 상기 피부톤 정보를 참고하여 가상모델의 분할 영역을 생성하는 것을 특징으로 할 수 있다. 한편, 본 발명의 또 다른 실시예에 따른 이미지 변형 모델 학습 방법은, 중앙처리유닛 및 메모리를 포함하는 장 치를 이용하여 실행되되, 상기 방법은 제1 의류 이미지를 임의의 변형 파라미터에 따라 왜곡시켜 제2 의류 이미 지를 생성하는 의류 이미지 왜곡 단계; 학습용 모델 이미지로부터, 의류 영역만 추출하는 의류 영역 추출 단계; 및 상기 제2 의류 이미지와 상기 추출된 의류 영역을 비교하여 정합도를 판단하는 정합도 판단 단계;를 포함할 수 있다. 또한, 상기 이미지 변형 모델을 학습시키는 방법은 상기 의류 이미지 왜곡 단계 이전 데이터 전처리 단계를 더 포함하되, 상기 데이터 전처리 단계는, 학습용 의류 이미지로부터 제1 의류 이미지 및 제1 마스킹 이미지 - 상 기 제1 의류 이미지는, 상기 학습용 의류 이미지로부터 적어도 일부의 영역이 수정된 상태의 이미지임 - 를 생 성하고, 상기 학습용 모델 이미지로부터 분할 이미지 및 포즈 데이터 - 상기 분할 이미지는, 상기 학습용 모델 이미지 내 모델이 차지하는 영역을 복수 개의 영역들로 분할한 상태의 이미지이고, 상기 포즈 데이터는, 상기 학습용 모델 이미지 내 모델의 자세를 정의하기 위한 값을 포함함 - 를 생성하는 단계인 것을 특징으로 할 수 있다. 또한, 상기 이미지 변형 모델을 학습시키는 방법에 있어서 상기 의류 이미지 왜곡 단계는, 상기 제1 마스킹 이 미지를 상기 변형 파라미터에 따라 왜곡시켜 제2 마스킹 이미지를 생성하는 단계를 더 포함할 수 있다. 또한, 상기 이미지 변형 모델을 학습시키는 방법에 있어서 상기 변형 파라미터는, 제1 의류 이미지 내 임의 영 역을 정의하기 위한 파라미터, 상기 임의 영역을 구성하는 픽셀들을 일정량만큼 횡 또는 종 방향으로 시프트 (shift)시키기 위한 파라미터, 임의 픽셀들의 색깔을 바꾸기 위한 파라미터 또는 임의 픽셀들을 삭제하기 위한 파라미터 중 적어도 하나를 포함하는 것을 특징으로 할 수 있다. 한편, 본 발명의 또 다른 실시예에 따른 이미지 합성 모델을 학습시키는 방법은, 중앙처리유닛 및 메모리를 포 함하는 장치를 이용하여 실행되되, 상기 방법은, 가상모델 이미지 - 상기 가상모델 이미지는, 학습용 모델 이미 지, 분할 이미지, 또는 포즈 데이터 중 적어도 하나로부터 재구성된 가상모델이 포함된 이미지임 - 와 제2 의류 이미지를 합성하여 가상 피팅 이미지를 생성하는 이미지 합성 단계; 상기 가상 피팅 이미지와 상기 학습용 모델 이미지를 비교하여 정합도를 판단하는 정합도 판단 단계;를 포함할 수 있다."}
{"patent_id": "10-2020-0152741", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 사용자가 의류 이미지 및 모델 이미지를 업로드 하는 것만으로 가상 피팅 체험을 할 수 있게 되므로 사용자의 편의성을 크게 높이는 효과가 있다.또한 본 발명에 따르면 비대면 온라인 의류 쇼핑이 실질적으로 가능해지는 효과가 있으며, 이에 따라 특히 감염 병 확산에 의해 경직된 경제 활동을 풀어줄 수 있는 효과가 있다. 또한 본 발명에 따르면 의류 이미지를 변형하는 알고리즘, 그리고 이미지를 합성하는 알고리즘이 학습 가능하게 구현되므로, 사용자들의 이용 회수가 늘어날수록 누적 학습의 회수도 많아져 전체 서비스의 품질이 향상되는 효 과를 꾀할 수 있다. 또한 본 발명에 따르면 최종적으로 사용자에게 가상 피팅 이미지를 제공할 때에 모델 이미지 내 피부톤을 활용 하여 가상 피팅 이미지 상에 표현된 가상모델의 피부톤을 구현해 내도록 함으로써 가상 피팅 이미지의 품질을 높이는 효과가 있다. 한편, 본 발명에 의한 효과는 이상에서 언급한 것들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 효과들 은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2020-0152741", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 목적과 기술적 구성 및 그에 따른 작용 효과에 관한 자세한 사항은 본 발명의 명세서에 첨부된 도면 에 의거한 이하의 상세한 설명에 의해 보다 명확하게 이해될 것이다. 첨부된 도면을 참조하여 본 발명에 따른 실시예를 상세하게 설명한다. 본 명세서에서 개시되는 실시 예들은 본 발명의 범위를 한정하는 것으로 해석되거나 이용되지 않아야 할 것이다. 이 분야의 통상의 기술자에게 본 명세서의 실시예를 포함한 설명은 다양한 응용을 갖는다는 것이 당연 하다. 따라서, 본 발명의 상세한 설명에 기재된 임의의 실시 예들은 본 발명을 보다 잘 설명하기 위한 예시적인 것이며 본 발명의 범위가 실시 예들로 한정되는 것을 의도하지 않는다. 도면에 표시되고 아래에 설명되는 기능 블록들은 가능한 구현의 예들일 뿐이다. 다른 구현들에서는 상세한 설명 의 사상 및 범위를 벗어나지 않는 범위에서 다른 기능 블록들이 사용될 수 있다. 또한, 본 발명의 하나 이상의 기능 블록이 개별 블록들로 표시되지만, 본 발명의 기능 블록들 중 하나 이상은 동일 기능을 실행하는 다양한 하드웨어 및 소프트웨어 구성들의 조합일 수 있다. 또한, 어떤 구성요소들을 포함한다는 표현은 \"개방형\"의 표현으로서 해당 구성요소들이 존재하는 것을 단순히 지칭할 뿐이며, 추가적인 구성요소들을 배제하는 것으로 이해되어서는 안 된다.나아가 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급될 때에는, 그 다른 구성 요소에 직접적으로 연결 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이해되 어야 한다. 이하에서는 도면들을 참조하여 본 발명의 각 실시 예들에 대해 살펴보기로 한다. 먼저 도 1은 본 발명에 따른 딥 러닝(Deep Learning) 기반 가상 피팅 서비스의 개요를 설명하기 위한 것으로, 본 발명의 기본 서비스 개념은 옷을 촬영한 의류 이미지, 그리고 사람을 촬영한 모델 이미지가 존재할 때, 이 두 개의 이미지를 합성하여 가상 피팅 이미지를 제공하는 것을 기본 골자로 한다. 즉, 사용자가 오프라인 상에 서 옷을 입어보는 것 대신, 온라인 상에서 사람 사진 위에 옷 사진을 합성함으로써 사용자가 긴 시간을 들여 쇼 핑을 직접 하지 않더라도 손쉽게 사고자 하는 옷을 입은 자기 모습을 확인할 수 있게 한 것이다. 더 구체적으로는, 의류 이미지를 단순히 모델 이미지 상에 덧씌워 합성을 하는 것이 아니라, 모델 이미지 내에 서 사람이 취하고 있는 포즈에 따라 의류 이미지 내 옷의 형상을 왜곡 내지 변형시키고, 이렇게 왜곡 내지 변형 된 의류 이미지를 상기 모델 이미지 상에 그대로 합성시킴으로써 마치 실제 사람이 그 옷을 입은 상태에서 사진 을 찍은 것과 같은 느낌의 가상 피팅 이미지를 제공하는 것을 서비스의 기본 골자로 하는 것이다. 본 발명에 따른 가상 피팅 서비스에서는, 도 1에도 도시가 되어 있듯 의류 이미지 내 의류가 붉은색 계통의 민 소매 옷이고, 모델 이미지 내 사람은 흰색 계통의 긴팔 옷을 입고 있었다고 가정할 때, 가상 피팅 이미지에서는 사람이 포즈는 그대로 유지한 채 붉은색 계통의 민소매 옷을 입은 모습으로 표현될 수 있다. 다만, 후술하겠지 만 이 때의 가상 피팅 이미지는, 실제로는 변형된 의류를 제외한 사람의 신체 영역들을 인공지능 알고리즘으로 하여금 생성하게 하고, 생성된 상기 사람의 신체 영역들과 상기 변형된 의류를 합성함으로써 만들어지는 것임을 이해한다. 이상 도 1을 참고하여 본 발명에 따라 제공될 수 있는 가상 피팅 서비스의 기본 개념에 대해 살펴 보았다. 도 2는 본 발명에 따른 가상 피팅 서비스가 제공되는 환경, 즉 전체 시스템을 개략적으로 도시한 것이다. 도 2 를 참고할 때, 시스템은 크게 사용자 단말기(100a, 100b), 그리고 서비스 서버로 구성될 수 있으며, 이 때 사용자 단말기를 통해서는 의류 이미지, 모델 이미지가 서비스 서버 측으로 업로드 될 수 있고, 서비스 서버에 서는 위 의류 이미지 및 모델 이미지로부터 가상 피팅 이미지를 생성하여 상기 사용자 단말기 측으로 제공할 수 있다. 이 때, 상기 사용자 단말기는 사용자가 보유하거나 소지하고 다니는 단말기를 일컫는 것으로, 여기에는 스마트 폰, PDA, 스마트워치, 태블릿PC와 같은 휴대가 가능한 단말기는 물론 데스크탑PC, stand alone 키오스크 등과 같은 설치형 단말기도 포함될 수 있다. 또한 사용자 단말기에는 스마트 장갑, 스마트 리스트와 같은 다른 종류 의 웨어러블 디바이스도 포함될 수 있으며, 이들 웨어러블 디바이스는 자이로 센서 기능을 이용하여 디바이스의 자세를 감지해 내는 방식을 기반으로 입력이 이루어지게 할 수 있다. 이러한 사용자 단말기를 장치의 측면에서 볼 때에, 각 사용자 단말기들은 중앙처리유닛(CPU)과 메모리를 구비하고 있는 것을 전제로 한다. 중앙처리유닛 은 컨트롤러(controller), 마이크로 컨트롤러(microcontroller), 마이크로 프로세서(microprocessor), 마이크 로 컴퓨터(microcomputer) 등으로도 불릴 수 있다. 또한 중앙처리유닛은 하드웨어(hardware) 또는 펌웨어 (firmware), 소프트웨어, 또는 이들의 결합에 의해 구현될 수 있는데, 하드웨어를 이용하여 구현하는 경우에는 ASIC(application specific integrated circuit) 또는 DSP(digital signal processor), DSPD(digital signal processing device), PLD(programmable logic device), FPGA(field programmable gate array) 등으로, 펌웨어 나 소프트웨어를 이용하여 구현하는 경우에는 위와 같은 기능 또는 동작들을 수행하는 모듈, 절차 또는 함수 등 을 포함하도록 펌웨어나 소프트웨어가 구성될 수 있다. 또한, 메모리는 ROM(Read Only Memory), RAM(Random Access Memory), EPROM(Erasable Programmable Read Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), 플래쉬(flash) 메모리, SRAM(Static RAM), HDD(Hard Disk Drive), SSD(Solid State Drive) 등으로 구현될 수 있다. 참고로, 본 상세한 설명에서는 발명의 이해를 돕기 위하여 사용자 단말기가 스마트폰, 또는 태블릿PC인 경우를 상정하여 설명하기로 한다. 이 경우, 상기 사용자 단말기는 디스플레이, 및 터치 감응형 표면, 및 마이크 등을 포함할 수 있으며, 나아가 부수적으로 물리적 키보드, 마우스 및/또는 조이스틱과 같은 하나 이상의 다른 물리적 사용자 입력용 수단이 더 연결될 수도 있다. 또한 상기 사용자 단말기는 소리나 음성을 감지하고 녹음할 수 있는 수단, 자이로 센서와 같이 자세를 감지하는 수단을 더 포함할 수도 있다. 한편, 사용자 단말기 상에서 실 행되는 다양한 애플리케이션들은, 옵션적으로, 터치 감응형 표면과 같은 적어도 하나의 보편적인 물리적 사용자 입력용 수단을 사용할 수 있다. 터치 감응형 표면의 하나 이상의 기능뿐만 아니라 사용자 단말기 상에 표시되는 대응하는 정보는 하나의 애플리케이션으로부터 다음 애플리케이션으로 그리고/또는 개별 애플리케이션 내에서 옵션적으로 조정되고/되거나 변경될 수 있다. 이러한 방식으로, 사용자 단말기의 (터치 감응형 표면과 같은) 보 편적인 물리적 아키텍처는, 옵션적으로, 사용자에게 직관적이고 명료한 사용자 인터페이스들을 이용하여 다양한 애플리케이션들을 지원할 수 있다. 한편, 서비스 서버는 본 발명에 따른 가상 피팅 서비스를 실제로 구현하기 위한 프로그램, 즉 명령어들의 집합을 제공하는 구성이며, 나아가 복수의 사용자 단말기들로부터 입력 및 업로드 된 이미지들을 처리하는 구성 이다. 서비스 서버의 형태는, 어느 특정 운영자가 관리하는 적어도 하나의 서버용 PC일 수 있으며, 또는 타 업체에서 제공하는 클라우드 서버의 형태, 즉 운영자가 회원가입하여 사용할 수 있는 클라우드 서버의 형태일 수도 있다. 특히 서비스 서버가 서버용 PC로 구현된 경우, 해당 서비스 서버는 중앙처리유닛 및 메모리를 포함할 수 있으며, 이에 대해서는 앞선 사용자 단말기의 설명 과정에서 자세히 언급하였으므로 여기서는 설명을 생략하기 로 한다. 참고로 본 상세한 설명에서 후술하게 될 각 단계들은 그 실행의 주체가 서비스 서버 또는 사용자 단말기일 수 있다. 더 정확하게는 상기 각 단계들은 서비스 서버 내 저장수단에 저장되어 있는 명령어(프로그램 코드)들, 또 는 사용자 단말기의 저장수단에 저장되어 있는 명령어들이 중앙처리유닛에 의해 처리 및 실행될 수 있다. 서비스 서버가 각 단계들을 직접 실행시키는 주체가 되는 경우란, 예를 들어 임의의 사용자가 자신의 스마트폰 (사용자 단말기)을 이용해 의류 이미지와 모델 이미지를 서비스 서버로 업로드 하면 서비스 서버가 이를 기초로 가상 피팅 서비스 제공을 위한 각 단계들을 실행한 후 최종적으로 생성된 가상 피팅 이미지를 다시 스마트폰으 로 전송하는 경우를 의미할 수 있다. 한편, 사용자 단말기가 각 단계들을 직접 실행시키는 주체가 되는 경우란, 예를 들어 임의의 사용자가 자신의 스마트폰(사용자 단말기)에 가상 피팅 서비스 제공을 위한 연산이 가능한 어플리케이션을 다운로드 한 후, 자신 의 스마트폰에 저장되어 있는 의류 이미지 및 모델 이미지가 상기 어플리케이션 상에서 로드되어 그 어플리케이 션 상에서 가상 피팅 서비스 제공을 위한 각 단계들이 실행되면, 그 결과물인 가상 피팅 이미지를 제공 받게 되 는 경우를 의미할 수 있다. 이 경우 서비스 서버는 상기 사용자 단말기에 의해 일부 정보들(예. 의류 이미지, 모델 이미지 또는 가상 피팅 이미지 중 적어도 하나)을 공유 받을 수 있으나, 실질적인 연산 수행은 하지 않을 수 있다. 즉, 가상 피팅 서비스 제공 방법은 그 실질적인 연산 과정이 서비스 서버에서 실행되는지, 아니면 사용자 단말 기 상에서 실행되는지에 따라 그 주체가 달라질 수 있음을 이해해야 할 것이며, 이러한 실행의 주체들을 모두 포괄하기 위해 본 상세한 설명에서는 가상 피팅 서비스 제공 방법의 실행 주체를 「중앙처리유닛 및 메모리를 포함하는 장치」로 정의하기로 한다. 이상 도 2를 참고하여 본 발명에 따른 가상 피팅 서비스가 제공되는 시스템을 살펴 보았다. 도 3은 본 발명에 따른 가상 피팅 서비스 제공 방법의 각 단계들을 순서대로 나열한 것이다. 가상 피팅 서비스 제공 방법의 가장 첫 번째 단계는 사용자로부터 원본 의류 이미지 또는/및 모델 이미지를 로 드하는 단계(S101)이다. 본 단계는 사용자가 입어보고자 하는 의류에 대한 이미지와 입혀보고자 하는 대상이 포 함된 모델 이미지를 로드하는 단계로서, 후속 데이터 처리를 위해 필연적으로 메모리 상에 로드되어 있는 상태 를 의미하는 것으로 이해될 수 있다. 이 때, 상기 원본 의류 이미지 또는 모델 이미지는 반드시 동시간대에 로 드가 되어야만 하는 것은 아니며, 필요에 따라 시간 간격을 두고 로드될 수도 있다. 상기 원본 의류 이미지와 관련하여, 이는 사용자가 시착(試着)해 보기를 원하는 의류에 대한 이미지로서 아직 장치에 의한 수정이 이루어지기 전 단계의 원본 이미지를 의미한다. 상기 원본 의류 이미지는 예를 들어, 사용 자가 직접 의류를 촬영한 이미지일 수 있으며, 또는 온라인 샵에서 미리 업로드 해 둔 것으로서 사용자가 다운 로드 할 수 있도록 한 이미지일 수도 있고, 그 밖에 의류가 포함되어 있는 임의 종류의 이미지일 수 있다. 또한 상기 모델 이미지와 관련하여, 이는 사용자가 의류를 입혀보고자 하는 대상에 대한 이미지로서 많은 경우 사용자 자신의 모습이 들어가 있는 사진일 것이나, 반드시 사용자 자신의 모습이 포함되어 있을 필요는 없으며 부모님, 친구, 그 밖의 지인 등이 대상이 될 수도 있다. 상기 모델 이미지에는 예를 들어, 사용자가 직접 자기 자신을 촬영한 셀피 이미지일 수 있으며, 또는 타인에 의해 자신의 모습이 찍힌 이미지, 자신이 타인의 모습을 찍은 이미지 등이 포함될 수 있다. 다른 한편, 상기 모델 이미지는 사용자가 미리 가상 피팅 서비스 이용을 위 한 회원 가입 시 미리 서비스 서버 측에 업로드 해 둔 것이거나, 또는 전용 어플리케이션에 저장해 둔 것일 수 있으며, 이 경우 사용자로서는 별도의 모델 이미지를 다시 준비할 필요 없이 기존에 업로드 또는 저장해 둔 것 을 그대로 활용하여 가상 피팅 서비스를 이용할 수 있다. S101단계 이후에는 상기 원본 의류 이미지 및 모델 이미지를 기초로 한 데이터 전처리 단계(S103)가 수행될 수 있다. 본 단계는 후속 단계들을 위해 필요한 몇몇 이미지 및 데이터를 미리 생성해 두는 단계로 이해될 수 있는 데, 더 구체적인 과정에 대해서는 이하에서 후술하기로 한다. 데이터 전처리 단계(S103)에서는 의류 관련 이미지가 2개 종류, 그리고 모델 관련 이미지 및 데이터가 각 1종류 씩 생성될 수 있는데, 이는 도 4를 참고하여 설명하기로 한다. 도 4를 참고할 때, 원본 의류 이미지로부터는 제1 의류 이미지가 생성(S1031)될 수 있다. 제1 의류 이미지란, 원본 의류 이미지로부터 적어도 일부의 영역이 수정된 상태의 이미지를 의미하며, 예를 들어서는 원본 의류 이 미지 중 추후 가상 피팅 이미지를 합성할 때에 굳이 필요하지 않은 영역을 식별하여 크롭핑(cropping)한 상태의 이미지일 수 있다. 제1 의류 이미지의 예로는 도 5에 도시된 것과 같이 의류의 뒷부분이 일부 크롭핑 되어 제거 된 것을 들 수 있는데, 더 정확하게는 원본 의류 이미지 중 넥라인(neck line)과 배면 라인에 의해 둘러싸인 영 역이 크롭핑 된 이미지일 수 있다. 크롭핑 대상이 되는 위 영역은 추후 가상 피팅 이미지 합성 시 모델에 의해 가려지게 되는 부분이어서 필요가 없을 뿐만 아니라, 오히려 (인공지능 알고리즘에 의해 구현된)이미지 합성 모 델에 의한 이미지 합성 시에는 오류를 초래할 위험이 있는 영역이므로 이를 데이터 전처리 단계에서 미리 제거 해 두는 것이다. 참고로, 원본 의류 이미지로부터 제1 의류 이미지를 생성해 내는 과정은 매크로 알고리즘 또는 인공지능 알고리즘에 의해 이루어질 수 있으며, 예를 들어 의류 이미지를 구성하는 픽셀들 중 명도값(또는 그 밖에 픽셀의 색깔을 구분하는 값)이 주변 명도값과 기 설정치 이상의 큰 차이를 보이는 영역을 넥라인(neck line; L1) 및 배면라인(L2)으로 둘러싸인 영역으로 인식하도록 하고, 그 영역을 크롭핑 하여 제거하게 함으로써 제1 의류 이미지 생성이 이루어지게 할 수 있다. 한편, 본 상세한 설명에서는 제1 의류 이미지의 예시로 넥라인 (L1) 및 배면라인(L2)에 의해 둘러싸인 영역을 크롭핑 한 이미지만을 들었으나, S1031단계에서의 제1 의류 이미 지 생성 과정에는 그 밖에 또 다른 불필요한 영역 크롭핑, 또는 추후 원활한 이미지 합성을 위해 필요한 변경 내지 왜곡이 더 포함될 수 있음을 이해한다. 한편, S1031단계에서 생성된 제1 의류 이미지를 기초로 하여서는 제1 마스킹 이미지가 생성(S1032)될 수 있다. 마스킹 이미지란, 후속적으로 모델의 포즈에 맞게 변형되어 의류 이미지와 모델 이미지를 합성 시킬 때에 합성 이미지 마스크 중 의류 부분의 Ground Truth로 사용되는 것으로, 그 형태는 도 4에 도시되어 있는 것과 같이 제 1 의류 이미지 중 의류가 차지하는 영역과 그 외의 바깥 영역이 서로 다른 명도값을 가지도록 구분된 형태의 이 미지일 수 있다. 참고로 도 4에서는 의류 영역이 더 밝은 명도값을 가지고 그 외의 영역이 더 어두운 명도값을 가지도록 표현되어 있으나, 제1 마스킹 이미지는 의류 영역과 그 외의 영역을 구분할 수 있다면 표현 방식에는 제한이 없다 할 것이다. 다음으로 모델 이미지를 기초로 하여서는 분할 이미지 및 포즈 데이터가 생성될 수 있다. (S1033, S1034) 분할 이미지란, 모델 이미지 내 모델 영역을 복수 개의 영역들로 나눈 이미지를 의미하며, 이 분할 이미지는 추 후 이미지 합성시 모델(사람) 신체 중 그대로 보존하여 사용할 부분과 그렇지 않은 영역을 구분하기 위해 데이 터 전처리 단계에서 생성해 두게 된다. 분할 이미지는, 도 4에 도시되어 있는 것과 같이 표면적으로 보이는 모 델의 각 부분들, 예를 들어 머리카락, 얼굴, 목, 의류, 왼쪽 팔, 오른쪽 팔, 바지, 또는 다리 등을 영역별로 구 분한 것일 수 있으며, 이 때 각 영역의 구별은 주로 RGB값의 비교에 의해 이루어질 수 있을 것이나 반드시 이에 한정되지는 않는다. 분할 이미지의 형태는 도 4에 도시되어 있는 것과 같이 각 영역들이 서로 다른 색깔 또는 명도로 표시됨으로써 모델 영역 내 복수 개의 영역들이 구별되게 할 수 있으며, 또는 상기 분할 이미지의 각 영 역들은 그 자체로 모델 이미지의 영역 별 픽셀정보를 그대로 유지하게 함으로써 마치 모델 이미지 상에서 각 영 역들이 가상의 선으로만 구분된 것처럼 정의되도록 할 수도 있다. 포즈 데이터란, 모델 이미지 내 모델(사람)의 자세를 식별하여 정의하기 위한 값들의 집합으로 이해될 수 있으 며, 예를 들어 모델 내 주요 로케이션들(얼굴 중앙, 목 중앙, 몸통 중앙, 팔꿈치, 손끝, 무릎, 발끝 등)을 선분 으로 연결한 것을 이미지 데이터 또는 텍스트 데이터로 저장한 것이 포즈 데이터의 일 예일 수 있다. 더 구체적 으로, 상기 포즈 데이터는 모델(사람)의 몸 각 부위(체크포인트)들을 JSON파일 형태로 저장한 데이터일 수 있다. 또한, JSON파일 형태의 포즈 데이터에는 숫자, 문자열, 불리언, 배열, 객체, null 값들 중 적어도 하나의 자료형이 포함될 수 있으며, 이들 자료형들은 모델(사람)의 몸 각 부위들을 식별하는 데에 사용될 수 있다. 위 와 같은 포즈 데이터는 추후 의류를 모델의 포즈에 맞게 변형시킬 때에 기준점으로 활용될 수 있다. 이상 데이터 전처리 단계(S103)에 대해 살펴보았다. S103단계 이후에는, 제1 의류 이미지를 모델 이미지 내 모델의 포즈(자세)에 대응되도록 변형하고, 나아가 제1 마스킹 이미지 역시 모델의 포즈에 따라 대응되도록 변형하는 이미지 변형 단계(S105)가 수행될 수 있다. 도 6은 이미지 변형 단계를 설명하기 위한 것으로, 이에 따르면 제1 의류 이미지 또는 제1 마스킹 이미지가 각 각 제2 의류 이미지, 제2 마스킹 이미지로 변형될 수 있는데, 이 때 본 단계는 이미지 변형 모델에 의해 이루어 질 수 있다. 또한, 상기 이미지 변형 단계(S105)에서는 비단 제1 의류 이미지 및 제1 마스킹 이미지뿐만 아니라 분할 이미지 및 포즈 데이터도 함께 활용될 수 있음을 확인할 수 있다. 쉽게 말해 이미지 변형 단계(S105)는 모 델 이미지 내 사람의 체형과 포즈에 맞게 옷의 모양을 변형시키는 단계로, 이 때 본 단계는 이미지 변형 모델이 라는 인공지능 알고리즘을 통해 구현될 수 있으며, 이 때 필요한 입력 데이터로는 제1 의류 이미지, 제1 마스킹 이미지, 분할 이미지, 및 포즈 데이터가 있을 수 있고, 출력 데이터로는 제2 의류 이미지 및 제2 마스킹 이미지 가 생성될 수 있다. 도 6을 참고할 때, 제2 의류 이미지 및 제2 마스킹 이미지는 제1 의류 이미지 및 제1 마스킹 이미지와 비교할 때 각 의류 이미지 및 마스킹 이미지는 이미지 변형 모델이라는 인공지능 알고리즘에 의해 모델 이미지 내 사람 이 입고 있는 옷과 비슷한 형상으로 변형되었음을 알 수 있는데, 이 때 이미지 변형 모델은 반복적인 학습을 통 해 모델의 포즈에 따라 이미지를 최적의 정도로 변형하도록 한 것이다. 위 이미지 변형 모델에 대해서는 뒤에서 더 자세히 소개하기로 한다. S105단계 이후에는, 앞선 단계에서 얻은 제2 의류 이미지, 제2 마스킹 이미지, 분할 이미지, 및 포즈 데이터를 입력 데이터로 활용하여 가상 피팅 이미지를 생성하는 이미지 합성 단계(S107)가 수행될 수 있다. 본 단계는, 쉽게 말해 앞선 단계에서 변형된 옷 이미지를 모델 이미지 내 사람과 합성하는 과정으로도 이해될 수 있는데, 이를 통해 최종적으로는 의류 이미지 내 옷을 모델 이미지 내 사람이 입고 있는 가상 피팅 이미지가 얻어질 수 있다. 참고로 제2 마스킹 이미지는 모델(사람)의 포즈에 맞게 변형된 옷의 마스크로서, 이미지 합성 단계(S10 7)에서 제2 의류 이미지와 가상 모델이 합성될 때에 기초(ground truth)가 되는 이미지일 수 있다. 가상 피팅 이미지와 관련하여, 상기 가상 피팅 이미지는 다양한 방식으로 생성될 수 있다. 예를 들어, 가상 피 팅 이미지는 제2 의류 이미지와 모델 이미지를 단순 합성하는 방식으로 생성될 수 있고, 또는 제2 의류 이미지 로 분할 이미지 중 의류 영역과 대응되는 부분을 대체함으로써 생성될 수 있는 등 다양한 방식이 존재할 수 있 다. 또한, 바람직하게는, 제2 의류 이미지, 그리고 인공지능 알고리즘에 의해 생성된 모델의 각 영역들(목, 팔, 얼굴, 머리, 하의 등)이 하나로 조합됨으로써 가상 피팅 이미지가 생성되도록 구현할 수도 있다. 이 때, 더 바 람직하게는, 모델의 목 영역 및 팔 영역은 알고리즘(모델을 생성하기 위한 인공지능 알고리즘)에 의해 새로이 생성되도록, 그리고 얼굴 영역, 머리 영역, 하의 영역은 원래의 모델 이미지 내 해당 부분을 그대로 유지한 채 따와 서로 합성함으로써 모델을 완성하고, 이렇게 완성된 모델에 상기 제2 의류 이미지를 합성하는 방식으로 가 상 피팅 이미지가 생성되도록 구현할 수도 있다. 도 7은 이러한 이미지 합성 단계(S107)를 간략히 도시한 것으로, 이에 따르면 본 단계는 이미지 합성 모델이라 는 알고리즘에 의해 구현됨을 확인할 수 있다. 이 때 이미지 합성 모델 역시 반복적인 학습을 통해 각 입력 이 미지들을 합성하도록 한 것으로, 위 이미지 합성 모델에 대해서도 역시 뒤에서 더 자세히 소개하기로 한다. 한편, S107단계 이후에는 사용자에게 가상 피팅 이미지가 제공되는 단계(S109)가 수행될 수 있다. 이 단계는, 예를 들어 사용자의 스마트폰 등과 같은 사용자 단말기 측으로 가상 피팅 이미지가 출력되는 방식 등으로 구현될 수 있다.이하에서는 앞서 미뤄두었던 인공지능 알고리즘들, 즉 이미지 변형 모델 및 이미지 합성 모델이 어떤 과정을 거 쳐 학습되는지에 대해 설명하기로 한다. 도 8은 이미지 변형 모델을 학습시키는 방법의 각 과정들을 순서에 따라 도시한 것이다. 도 8을 참고할 때 이미지 변형 모델을 학습시키는 방법은 학습용 의류 이미지 및 학습용 모델 이미지를 로드하 는 단계(S201)로부터 시작된다. 본 단계는 운영자(이 때 운영자는 가상 피팅 서비스를 운영하는 자 또는 가상 피팅 서비스를 제공하기 위해 필요한 소프트웨어를 관리하는 자 등 포괄적인 의미를 가짐)에 의해 수집되었거나 편집된 이미지들, 또는 임의의 알고리즘에 의해 온라인 상에서 크롤링(crawling) 된 이미지들, 그 밖에 다양한 수단으로 수집된 이미지들이 연산 능력을 갖춘 컴퓨팅 장치에 의해 로드되는 단계로 이해될 수 있다. 상기 학습 용 의류 이미지, 그리고 학습용 모델 이미지에 있어서 특기할 만한 사항은, 학습용 모델 이미지 내 모델(사람) 이 입고 있는 옷이 상기 학습용 의류 이미지 내 옷과 동일한 것이어야 한다는 점이다. 즉, 이미지 변형 모델을 학습시키기 위한 용도의 이미지들은 동일한 의류 상품에 관한 것들이어야 한다. S201단계 이후에는 상기 학습용 의류 이미지 및 학습용 모델 이미지를 기초로 한 데이터 전처리 단계(S203)가 수행될 수 있다. 본 단계는 후속 단계들을 위해 필요한 이미지 및 데이터를 생성해 두는 단계로, 본 단계에서는 앞서 살펴 본 도 3에서의 전처리 단계(S103)와 유사하게 의류 관련 이미지 2개 종류, 그리고 모델 관련 이미지 및 데이터가 각 1종류씩 생성될 수 있다. 도 9를 참고할 때, 학습용 의류 이미지로부터는 제1 의류 이미지가 생성(S2031)될 수 있고, 제1 의류 이미지로 부터는 제1 마스킹 이미지가 생성(S2032)될 수 있으며, 이 두 단계를 통해 의류 관련 이미지 2개 종류가 생성될 수 있다. 또한, 학습용 모델 이미지로부터는 분할 이미지, 그리고 포즈 데이터가 생성(S2033, S2034)될 수 있다. 참고로 S2031 단계 내지 S2034 단계는 앞서 도 4에서의 S1031 단계 내지 S1034 단계와 실질적으로 동일한 과정으로 진행되며, 따라서 중복 설명을 피하기 위해 여기서는 더 자세한 설명은 생략하기로 한다. S203단계 이후에는 의류 관련 이미지들을 왜곡(warping)시키는 단계(S205)가 수행될 수 있다. 더 구체적으로, 본 단계에서는 앞서 전처리 단계에서 생성된 제1 의류 이미지를 임의의 변형 파라미터(θ)에 따라 왜곡시켜 제2 의류 이미지를 생성하는 과정이 수행될 수 있으며, 여기에 더 나아가 제1 마스킹 이미지 역시 같은 방식으로 왜 곡시켜 제2 마스킹 이미지를 생성하는 과정도 더 수행될 수 있다. 이 때, 제1 마스킹 이미지를 왜곡하는 과정은 필요에 따라 생략될 수도 있다. 도 10의 좌측에는 제1 의류 이미지에 변형 파라미터(θ)가 적용되어 제2 의류 이미지가 생성되는 과정이 도시되어 있다. 변형 파라미터(θ)와 관련하여, 위 파라미터는 제1 의류 이미지를 왜곡시키기 위해 적용될 수 있는 다양한 파라 미터들을 포함하는 개념으로 이해될 수 있으며, 본 상세한 설명에서는 단순히 θ라는 하나의 파라미터만을 언급 하고 있으나 필요에 따라 복수 개의 파라미터들이 제1 의류 이미지 또는 제1 마스킹 이미지를 왜곡시키는 데에 활용될 수 있다. 제1 의류 이미지 또는 제1 마스킹 이미지의 왜곡을 위해 활용 가능한 파라미터의 구체적인 종 류로는, 이미지 내 임의 영역을 정의하기 위한 파라미터, 상기 임의 영역을 구성하는 픽셀들을 일정량만큼 횡 또는 종 방향으로 시프트(shift)시키기 위한 파라미터, 임의 픽셀들의 색깔을 바꾸기 위한 파라미터, 임의 픽셀 들을 삭제하기 위한 파라미터 등이 포함될 수 있다. 참고로, 본 상세한 설명에서 언급된 위 파라미터들의 종류 는 일 예시들에 불과한 것이며, 이 외에 상기 제1 의류 이미지를 다른 형태로 왜곡시킬 수 있는 한 더 다양한 왜곡 방식, 그리고 이러한 왜곡 방식을 구현해 내는 데에 필요한 파라미터들이 존재할 수 있음을 이해해야 할 것이다. 한편, S205단계 이후에는 학습용 모델 이미지 중 의류가 차지하고 있는 영역을 획득하는 단계(S207)가 수행될 수 있다. 본 단계에서는 앞서 데이터 전처리 단계에서 미리 생성해 둔 분할 이미지가 활용될 수 있으며, 예를 들어 학습용 모델 이미지로부터 상기 분할 이미지 중 옷이 차지하고 있는 영역과 대응되는 영역만 추출하도록 함으로써 의류 영역을 획득하게 할 수 있다. 다른 한편, 상기 분할 이미지가 그 자체로 상기 학습용 모델 이미 지 내 복수 개의 영역들에 대한 픽셀 정보들을 모두 가지고 있다고 가정할 때, 다시 말해 분할 이미지가 실질적 으로 상기 학습용 모델 이미지의 모델 영역을 복수 개의 영역들로 구분해 둔 것일 경우에는 단순히 분할 이미지 중 의류(옷)에 대응되는 영역만 취함으로써 의류 영역을 획득할 수 있게 될 것이다. 참고로 도 10의 우측에는 분할 이미지가 참조되어 학습용 모델 이미지로부터 의류 영역이 추출되는 실시예가 도시되어 있다. S207단계 이후에는 앞서 S205단계에서 생성된 제2 의류 이미지와 S207단계에서 획득된 의류 영역 간의 정합도를 판단하는 단계(S209)가 수행될 수 있다. 여기서 정합도란, 제2 의류 이미지와 의류 영역 간 얼마나 동일한지를가리기 위한 기준으로, 정합도를 연산하는 방식에는 다양한 것들이 존재할 수 있겠으나 바람직하게는 상기 제2 의류 이미지를 구성하는 각 픽셀과 상기 의류 영역을 구성하는 각 픽셀을 서로 비교하는 방식으로 정합도 연산 이 가능할 수 있다. 즉, 알고리즘에 의해 왜곡된 의류 이미지와 실제 모델이 입고 있는 상태의 의류 이미지를 서로 비교해 봄으로써 이미지 변형 모델로 하여금 어느 모델이 어떤 자세를 취하고 있을 때, 의류 이미지를 어 느 정도의 변형 파라미터(θ)를 적용하여 왜곡 시킬 때 최적화 될 수 있는지를 학습하게 할 수 있다. 한편, 이 미지 변형 모델의 학습을 위해서는 한 의류에 대해 상기 S205단계 및 S209단계가 반복적으로 수행될 수 있다. 즉, 이미지 변형 모델을 활용하여 제1 의류 이미지를 조금씩 형태를 변형시켜 가며 제2 의류 이미지들을 생성하 도록 하고, 그 각각의 제2 의류 이미지들을 학습용 모델 이미지 내 의류 영역과 반복 비교가 이루어지게 함으로 써 학습용 모델 이미지 내 모델의 자세에 따라 어느 정도의 변형 파라미터(θ)가 적용되어야 할 지에 대한 학습 이 이루어지게 할 수 있다. 참고로 이미지 변형 모델을 학습 시키는 과정에서는 제1 의류 이미지, 제1 마스킹 이미지, 분할 이미지, 또는 포즈 데이터 중 적어도 2 이상의 조합이 학습을 위한 데이터 세트로 활용될 수 있으며, 필요에 따라 학습용 모 델 이미지가 더 활용될 수도 있다. 다음으로 이미지 합성 모델에 대한 설명을 이어가기로 한다. 이미지 합성 모델을 학습시키는 과정은 앞서 설명 한 이미지 변형 모델에 의한 의류 이미지 변형이 이루어진 상태를 전제로 진행될 수 있다. 이미지 합성 모델은 왜곡된 의류 이미지를 모델 이미지에 합성하는 과정에 관한 것으로, 이미지 합성 모델을 학습시키기 위해서는 최종적으로 생성된 가상 피팅 이미지와 최초의 학습용 모델 이미지 간의 비교가 반복되는 과정이 필요하다. 가 상 피팅 이미지는 S107단계에서 전술한 바와 같이 다양한 방식으로 생성될 수 있는데, 본 상세한 설명에서는 대 표적으로 인공지능 알고리즘에 의해 모델의 모습이 생성되고, 이렇게 생성된 모델과 제2 의류 이미지를 합성함 으로써 최종적으로 생성된 가상 피팅 이미지를 기준으로 설명하기로 한다. 도 11에는 이미지 합성 모델의 학습 과정이 개념적으로 도시되어 있는데, 도 11에서 확인할 수 있듯 이 과정은 가상 피팅 이미지와 학습용 모델 이미지 간의 정합도를 판단하는 과정을 기본으로 하되, 이 때 가상 피팅 이미 지가 새로 생성될 때마다 이것이 학습용 모델 이미지, 즉 기준이 되는 모델 이미지와 얼마나 동일한지를 반복적 으로 확인하게 함으로써 학습이 이루어지게 하는 것을 특징으로 한다. 이 때 정합도란, 앞서 이미지 변형 모델 에 대한 설명에서와 마찬가지로, 가상 피팅 이미지와 학습용 모델 이미지 간 얼마나 동일한지를 가리기 위한 기 준으로 정의될 수 있으며, 정합도를 연산하는 방식에는 다양한 것들이 존재할 수 있겠으나 바람직하게는 상기 가상 피팅 이미지를 구성하는 각 픽셀과 상기 학습용 모델 이미지를 구성하는 각 픽셀을 서로 비교하는 방식으 로 정합도 연산이 가능할 수 있다. 한편, 이미지 합성 모델을 학습시키는 과정에는 제2 의류 이미지, 제2 마스킹 이미지, 분할 이미지 및 포즈 데 이터 중 적어도 2 이상의 조합이 학습을 위한 데이터 세트로 활용될 수 있으며, 필요에 따라 학습용 모델 이미 지 자체도 학습에 활용될 수 있다. 이상 도 8 내지 도 11을 참고하여 이미지 변형 모델, 그리고 이미지 합성 모델을 학습시키는 방법에 대해 살펴 보았다. 이하에서는 도 12 내지 도 13을 참고하여 가상 피팅 이미지를 보다 정교하게 생성하는 방법, 더 정확하게는 가 상모델을 생성할 때에 신체 부위 영역의 피부톤을 사용자의 피부톤에 맞추어 생성하는 방법들에 대해 살펴보기 로 한다. 본 발명에 따른 가상 피팅 서비스 제공 방법 중에는 인공지능 알고리즘에 의해 가상모델이 생성되는 과정이 포 함되는데, 특히 이 가상모델은 목, 팔 영역은 새로이 생성(인공지능 알고리즘에 의해 생성된 분할 영역)되도록, 그리고 얼굴, 머리, 하의 영역은 원래 모델 이미지로부터 그대로 차용하도록 함(원본의 모델 이미지로부터 차용 된 영역)으로써 완성이 될 수 있다고 전술한 바 있다. 그런데 이 때 발생되는 문제점 중 하나는, 새로이 생성되 는 목 영역 또는 팔 영역이 원래 모델의 피부톤과 맞지 않은 상태로 생성이 되는 문제, 그리고 이에 따라 최종 적으로 가상 피팅 이미지 내 모델이 신체 부위 별로 피부톤이 달리 표현되는 것이다. 본 발명에 따른 가상 피팅 서비스 제공 방법에서는 이와 같이 가상 피팅 이미지 내 가상모델의 피부톤을 원래 모델의 피부톤에 맞추고자 피부톤 보정 과정을 더 포함할 수 있는데, 여기서는 이에 대해 살펴보기로 한다. 먼저 도 12는 피부톤 보정 방법의 제1 실시예를 도시한 것으로, 제1 실시예는 크게 모델 이미지로부터 기준 피 부톤 정보를 획득하는 단계(S301), 그리고 상기 기준 피부톤 정보를 참고하여 가상모델의 피부톤을 보정하는 단 계(S303)를 포함할 수 있다. 먼저 S301단계와 관련하여, 가상 피팅 서비스 제공을 위한 처리를 위해 또는 이미지 변형이나 이미지 합성을 위 해서는 모델 이미지 내지 학습용 모델 이미지가 로드될 수 있음에 대해 전술하였는데, S301단계는 위 로드 된 모델 이미지(또는 학습용 모델 이미지)로부터 모델의 원래 피부톤을 찾는 것을 특징으로 하는 단계이다. S301단 계의 바람직한 세부 실시예를 살펴보면, 가장 먼저 모델 이미지로부터 얼굴 영역(모델 이미지로부터 그대로 유 지된 영역)만 추출하는 단계(S3011), 얼굴 영역 내 랜드마크를 추출하는 단계(S3012), 랜드마크를 제거하는 단 계(S3013), 피부톤 정보를 추출하는 단계(S3014)를 포함할 수 있다. 위 세부 실시예는 모델 이미지로부터 '얼굴 영역'을 따로 분리하여 얼굴의 피부톤 정보를 획득하고자 한 것이며, 이 때 모델 이미지로부터 참조될 수 있는 영역은 반드시 얼굴이 아닌 다른 영역이 될 수도 있다. 한편, S3012, S3013단계에서 랜드마크를 추출 및 제거하 는 것은 얼굴 고유의 피부톤 정보를 보다 정확하게 획득하기 위한 것으로, 눈, 눈썹, 입술 등과 같이 고유의 다 른 색깔을 가지는 영역, 그리고 얼굴 영역 내에서 그림자를 발생시키는 영역을 배제시킴으로써 더 정확한 피부 톤 정보를 얻기 위함이다. 또한 참고로 S3014단계에서 피부톤 정보를 추출하는 단계에서는 RGB모델 또는 HSV모 델 등과 같이 피부 색상을 추출해 내기 위한 모델들이 활용될 수 있는데, 이 중 RGB모델은 빛의 3원색인 적(R), 녹(G), 청색(B)을 성분 비율로 결합시켜 색을 표현하는 모델로, 각각의 R, G, B 값은 0-255 사이의 값을 가지며 (R, G, B)의 형태로 색 정보가 표현될 수 있다. 또한 HSV모델은 색상(Hue), 채도(Saturation), 명도(Value)의 좌표를 써서 특정한 색을 표현하는 모델로, H, S, V 값은 0-255 사이의 값을 가지며 (H, S, V)의 형태로 색 정 보가 표현될 수 있다. 또한 S3014단계에서는 랜드 마크가 제거된 상태의 얼굴 영역에 대해 각 픽셀마다의 색상 정보를 추출한 후 평균값, 중간값, 최빈값 중 적어도 하나의 값을 연산함으로써 가장 적절한 피부톤 정보를 획 득할 수 있다. 다음으로 S303단계와 관련하여, 본 단계는 가상 피팅 이미지 내 가상모델의 팔 영역을 따로 추출해 내고 이렇게 추출된 팔 영역에 앞서 획득된 피부톤 정보를 적용하는 것을 특징으로 하는데, 구체적으로는 팔 영역(인공지능 알고리즘에 의해 생성된 영역)을 추출하는 단계(S3031), 그리고 추출된 팔 영역에 피부톤 정보를 적용시키는 단 계(S3032)를 포함할 수 있다. 팔 영역을 추출하는 단계는, 모델 이미지로부터 팔 영역만 인식하여 추출하는 방 식, 또는 데이터 전처리 단계에서 미리 생성해 두었던 분할 이미지를 이용하여 팔 영역만 추출하는 방식(분할 이미지 내 팔 영역과 대응되는 부분을 모델 이미지로부터 추출하거나, 또는 분할 이미지 내 팔 영역 자체를 추 출) 등을 통해 구현될 수 있다. 또한 추출된 팔 영역에는 앞서 획득된 피부톤 정보를 그대로 반영(이 때 반영한 다는 것의 의미는 팔 영역을 구성하는 픽셀들 중 적어도 일부를 상기 획득된 피부톤 정보를 참고하여 그 값을 변경한다는 의미임)시킬 수 있다. 한편, 도 13은 피부톤 보정 방법의 제2 실시예를 도시한 것으로, 제2 실시예는 애초 인공지능 알고리즘이 가상 모델을 생성해 낼 시에 팔 영역에 대한 피부톤 조건을 설정해 줌으로써 보정이 된 상태의 팔 영역이 생성될 수 있게 하는 방식이다. 생성적 대립 신경망(Generative Adversarial Network) 알고리즘을 이용하여 가상모델을 생 성한다고 가정할 때, 위 알고리즘에서의 생성자(Generator)는 잠재변수(Latent Variable(z))를 입력 받아 학습 이미지의 분포를 반영한 실제와 비슷한 가짜 이미지를 생성하게 되는데, 이 때 잠재변수(z)에 조건(condition) 을 부여하고, 이 조건에 맞도록 가상모델을 생성할 수 있도록 가이드가 가능하다면 원하는 가상모델이 생성 가 능해 진다. 즉, 제2 실시예에서는 조건이 부여된 GAN(소위 cGAN)을 이용함으로써 피부색이 보정된 상태의 가상 모델을 생성하게 하며, 이 때 조건은 앞서 설명한 피부톤 정보(RGB모델 또는 HSV모델이 활용되어 획득된 것)일 수 있다. 도 13에는 (R, G, B)값 또는 (H, S, V)값이 인공지능 알고리즘 모델의 생성자(Generator)와 식별자 (Discriminator)에 조건(y)으로 추가된 모습이 도시되어 있으며, 이들 생성자 및 식별자의 대립적 프로세스를 통해 가상모델이 생성될 수 있음이 도시되어 있다. 이상 가상 피팅 서비스를 제공하는 방법 및 이를 위한 시스템에 대해 살펴보았다. 한편, 본 발명은 상술한 특정 의 실시예 및 응용예에 한정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 발명"}
{"patent_id": "10-2020-0152741", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형 실시들은 본 발명의 기술적 사상이나 전망으로부터 구별되어 이해되어서는 안 될 것이다. 부호의 설명100 사용자 단말기 200 서비스 서버"}
{"patent_id": "10-2020-0152741", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 가상 피팅 서비스의 기본 개념을 설명하기 위한 도면이다. 도 2는 본 발명에 따른 가상 피팅 서비스가 제공되는 환경인 전체 시스템을 도시한 것이다. 도 3은 가상 피팅 서비스가 제공되는 방법을 순서대로 도시한 것이다. 도 4는 데이터 전처리 단계에서 이루어지는 과정들을 도시한 것이다. 도 5는 원본 의류 이미지로부터 넥(neck)라인을 기준으로 한 일부 영역을 크롭핑(cropping)하는 실시예를 도시 한 것이다. 도 6은 이미지 변형 모델에 따라 제1 의류 이미지를 변형시키는 과정을 도시한 것이다. 도 7은 이미지 합성 모델에 따라 가상 피팅 이미지를 생성하는 과정을 도시한 것이다. 도 8은 이미지 변형 모델을 학습시키는 방법이 순서대로 도시된 것이다. 도 9는 이미지 변형 모델을 학습시키는 방법 중 데이터 전치리 단계에서 이루어지는 과정들을 도시한 것이다. 도 10은 의류 이미지 왜곡단계, 의류 영역 추출 단계, 및 정합도 판단 단계를 설명하기 위한 도면이다. 도 11은 가상 피팅 이미지와 학습용 모델 이미지 간을 서로 비교시킴으로써 이미지 합성 모델을 학습시키는 과 정을 도시한 것이다. 도 12는 피부톤 보정 방법의 제1 실시예를 도시한 것이다. 도 13은 피부톤 보정 방법의 제2 실시예를 도시한 것이다."}
