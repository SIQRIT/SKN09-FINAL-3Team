{"patent_id": "10-2022-7003194", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0027199", "출원번호": "10-2022-7003194", "발명의 명칭": "두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템 및 방법", "출원인": "뉴로션 테크놀로지스 아이엔씨.", "발명자": "런, 후아롱"}}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템에 있어서,사용자가 네트워크 모델을 설계하고 설명할 수 있는 인터페이스를 제공하는 데 사용되는 모델 설명 모듈;매개변수 데이터베이스의 형식으로 네트워크 모델의 다양한 매개변수 데이터를 저장하는 데 사용되는 매개변수데이터베이스;현재 네트워크 운영 환경의 구성 매개변수 및 시냅스의 시동, 뉴런의 프루닝 및 신생을 시동하기 위한 조건을설명하는 데 적용되는 구성 설명 모듈;시스템 구성 매개변수를 얻기 위해 상기 구성 설명 모듈을 읽는 데 적용되는 구성 매니저;모델 설명 모듈을 읽고, 네트워크의 토폴로지컬 구조를 분석하고, 데이터 매니저를 통해 데이터 파일을 읽고,메모리에 네트워크 모델 객체를 구축하는 데 사용되는 네트워크 빌더;네트워크 모델 객체를 구축, 에르고딕, 액세스 및 업데이트하는 데 적용되는 네트워크 매니저;모델 설명 모듈에 있는 사용자가 선언한 규칙을 읽고,스케줄러가 상기 네트워크 모델 객체의 연산을 스케줄링할때 이러한 규칙을 해석하고 규칙 간의 충돌을 중재하는 데 사용되는 룰 매니저;매개변수 데이터베이스를 읽고 분석하여, 데이터 형식 변환 및, 데이터를 직렬화하는 데 사용되는 데이터 매니저;하드웨어 자원을 할당하고 연산 과정을 스케줄링하고, 대응된 조작을 스케줄링 및 수행하는 스케줄러; 실행 중인 조작을 관리하는 데 사용되는 조작 매니저;시스템이 실행 시 생성된 로그를 기록하고 시스템의 작업 상태를 기록하고 이상상황을 제시하는 로그 매니저;사용자가 입력한 명령을 수신 및 응답하고 시스템의 실행 상태를 관리하는 데 사용되는 작동 모니터링 모듈; 네트워크 데이터를 읽고 표시하는 데 사용되는 그래픽 표시 모듈;을 포함하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 모델 설명 모듈은 네트워크 설명 유닛, 수렴 설명 유닛 및 플로 설명 유닛을 포함하며; 상기 네트워크 설명 유닛은 네트워크 컨테이너와 일반 매개변수를 설명하고, 네트워크의 매개변수 및 실행 규칙을 설명하며, 링크를 통해 1개 또는 그 이상의 수렴 설명 유닛 및 플로 설명 유닛을 가리키는 데 적용되며;상기 수렴 설명 유닛은 수렴 컨테이너, 모듈 컨테이너, 레이어 그룹 컨테이너, 노드 컨테이너, 노드 파라미터컨테이너 및 일반 파라미터 컨테이너 중 적어도 1개의 컨테이너를 설명하는 데 적용되며, 네트워크에 있는 노드의 모듈과 레이어 그룹 간의 획분 관계, 각 네트워크 모델 객체의 파라미터 및 실행 시의 규칙 및 명령을 설명하는 데 적용되며;상기 플로 설명 유닛은 플로 컨테이너, 채널 컨테이너, 에지 컨테이너, 에지 컨테이너 및 일반 매개변수 컨테이너 중 적어도 1개의 컨테이너를 설명하는 데 사용되며, 네트워크 에지 연결관계, 각 네트워크 모델 객체의 매개변수 및 실행 시 규칙과 명령을 설명하는 데 적용되는 것을 특징으로 하는, 공개특허 10-2022-0027199-3-두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 모델 설명 모듈은 생물학적 뇌 신경계의 구성 방식을 시뮬레이션하는 다단계 트리 구조의 네트워크 설명방식을 채택하며;상기 수렴 설명 유닛은 미리 설정된 레이어 그룹 및 모듈에 따라 네트워크 노드를 정렬 구성하도록 지원하며,생물학적 뇌에서 뉴런 및 관련 글리아 세포의 다단계 구성 방식을 표현하는 데 적용되며;상기 플로 설명 유닛은 토폴로지 유사성에 따라 네트워크 에지의 그룹화 및 계층적 정렬 및 구성을 지원하며,생물학적 뇌에 있는 신경 시냅스의 다양한 구성 방식 및 관련 글리아 세포 돌기의 구성 방식을 표현하는 데 적용되는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 네트워크 설명 유닛, 수렴 설명 유닛 및 플로 설명 유닛은 전부 중첩된 문법을 적용하며, XML 또는 JS0N파일 형식을 선택하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 매개변수 데이터는 초기화 매개변수 데이터와 실행 매개변수 데이터를 포함하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 매개변수 데이터베이스는 바이너리 파일 또는 텍스트 파일이며, 상기 텍스트 파일은 CSV 파일 형식 또는기타 문자 구분 데이터 파일 형식을 적용하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 네트워크 모델 객체는 컨테이너, 토폴로지 관계 및/또는 매개변수 데이터를 포함하며, 상기 네트워크 모델객체는 스케줄러에 의해 스케줄링되어 실행하는 객체인 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템.공개특허 10-2022-0027199-4-청구항 8 제7항에 있어서,상기 컨테이너는 다단계 트리 구조에서 인덱싱에 사용되는 번호 및/또는 이름이 포함되는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 컨테이너는 통계 및 제어 정보를 저장하기 위한 1개 또는 그 이상의 제어 블록을 가지는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 제어 블록은 네트워크의 에르고딕 순서 및 규칙, 에르고딕 연산 참가 횟수, 데이터가 메인 메모리에 입주여부, 데이터가 프로세서 메모리에 입주여부, 하드 디스크의 읽기 및 쓰기 빈도 등에서 적어도 1개 이상을 포함하며, 상기 룰 매니저 및 스케줄러에 의해 관리되고 업데이트 되는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항 또는 제9항에 있어서,상기 컨테이너는:트리 구조의 제1 레이어에 위치하여, 전뇌 및 행동 척도의 모델을 특성화하는 데 사용되는 네트워크 컨테이너;트리 구조의 제2 레이어에 위치하며, 뇌영역 척도의 모델을 특성화하는 데 사용되는 수렴 컨테이너;트리 구조의 제3 레이어에 위치하며, 신경핵 척도의 모델을 특성화하는 데 사용되는 모듈 컨테이너;트리 구조의 제4 레이어에 위치하며, 신경 회로 척도의 모델을 나타내는 데 사용되는 레이어 그룹 컨테이너;트리 구조의 제5 레이어에 위치하며, 뉴런 척도 또는 글리아 세포 척도 모델 및 뉴런 또는 글리아 세포 그룹을특성화하는 데 사용되는 노드 컨테이너;트리 구조의 제6 레이어에 위치하며, 분자 척도, 수용체 척도, 신경전달물질 또는 신경조절제 척도 및/또는 뉴런 또는 글리아 세포 그룹 모델의 매개변수 텐서를 특성화 하는 데 적용되는 노드 매개변수 컨테이너;트리 구조의 제2 레이어에 위치하며, 뇌영역을 연결하는 신경 섬유 다발의 척도를 특성화하는 모델에 사용하는플로 컨테이너; 트리 구조의 제3 레이어에 위치하며, 신경 핵 사이의 축삭 돌기를 연결하여 형성된 전도 번들의 모델을 특성화하는 데 사용되는 채널 컨테이너;트리 구조의 제4 레이어에 위치하며, 신경 회로의 축삭돌기로 구성된 신경 경로 모델을 특성화하는 데 사용되는연결 컨테이너;트리 구조의 제5 레이어에 위치하며, 수상돌기 척도 또는 시냅스 척도의 모델 및/또는 시냅스 또는 글리아 세포돌기 그룹을 특성화하는 데 사용되는 에지 컨테이너;트리 구조의 제6 레이어에 위치며, 분자 척도, 신경전달물질 또는 신경조절제 척도, 수용체 척도의 모델, 및 시공개특허 10-2022-0027199-5-냅스 또는 글리아 세포 돌기 그룹 모델의 매개변수 텐서를 특성화하는 데 적용되는 에지 매개변수 컨테이너; 및/또는, 텐서방식으로 매개변수 데이터를 수용하는데 적용하는 매개변수 컨테이너를 포함하며,상기 일반 파라미터 컨테이너는 보조 컨테이너에 속하며, 각 레이어에 있는 컨테이너는 1개 또는 그 이상의 일반 일반 파라미터 컨테이너를 가질 수 있는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 뉴런 모델의 발사 특성은 단발성 파이링, 라피드 파이링, 버스트 파이링, 플래토우 파이링 및/또는 위상성파이링 등을 포함하여 구축되며; 상기 뉴런 모델이 업스트림 입력 신호에 대한 응답은 상이한 신경 적응성 또는감도 곡선으로 구축되며;상기 뉴런 모델은 다운스트림 작용 메커니즘에 대하여 흥분성, 억제성, 변조성 및/또는 중립성 모델로구축되며;상기 뉴런 모델은 스파이킹 뉴런 모델 및/또는 기존 뉴런 모델로 구축되며;상기 글리아 세포 모델은 성상 글리아 세포 모델, 희돌기교세포모델, 미세아교세포모델, 슈반 세포모델 및/또는위성 세포 모델로 구축되는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 신경전달물질 또는 신경조절제 모델은 흥분성, 억제성 및/또는 변조성 모델로 구축되며;상기 수용체 모델은 이온성 및/또는 대사성 모델로 구축되며;신경전달물질 또는 신경조절제에 대한 상기 수용체 모델의 반응 효과는 흥분성, 억제성, 변조성 및/또는 중립성모델로 구축되는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 수상돌기 척도의 모델은 정점 수상돌기 모델, 기저 수상돌기 모델 또는 스파이크 모델로 구축되며;상기 시냅스 모델은 흥분성, 억제성, 변조성 및/또는 중립성으로 구축되는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 분자 척도의 모델은 세포내 분자 모델, 세포막 분자모델 및/또는 세포간질 분자 모델로 구축되는 것을 특징으로 하는, 공개특허 10-2022-0027199-6-두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항에 있어서,상기 노드 매개변수 컨테이너, 에지 매개변수 컨테이너 및 일반 매개변수 컨테이너는 텐서 형식의 매개변수를적용하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 텐서의 차원은 1차원 또는 다차원이며, 상기 텐서의 정렬과 사용방식은 사용자가 지정하는 것을 특징으로하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 텐서는 4차원으로 설정할 수 있으며, 텐서에서 각 매개변수의 위치는 좌표(x, y, z, t)로 표시되며, 여기서 x, y, z, 3개 차원은 상위 컨테이너에 표시된 각 신경조직 모델의 공간적 배열에 대응되며; t는 시간 차원을나타내며, 타이밍 정보의 캐시 및 지연을 특성화할 수 있으며, 신경조절제가 뉴런 및/또는 시냅스에 대한 장기작용 메커니즘을 시뮬레이션하는 데 사용되며;상기 텐서의 매개변수는 상위 컨테이너에 있는 신경 조직 모델의 전체 또는 일부에서 공유할 수 있으며, 타겟영역에 있는 모든 신경 조직에 대한 신경조절제의 대면적의 작용을 시뮬레이션하는 데 사용되는 것을 특징으로하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항에 있어서,상기 플로 컨테이너 및 모든 하위 컨테이너는 1개 또는 그 이상의 업스트림 컨테이너, 및. 1개 또는 그 이상의다운스트림 컨테이너에 대응되며, 상기 업스트림 컨테이너 및 상기 다운스트림 컨테이너의 번호 또는 이름을 통해 인덱싱하고 액세스하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서,상기 업스트림 컨테이너와 다운스트림 컨테이너는 모두 임의 레이어에 있는 컨테이너이며, 양자는 동일하거나다른 컨테이너인 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템.공개특허 10-2022-0027199-7-청구항 21 제11항에 있어서,상기 플로 컨테이너 및 모든 하위 컨테이너는 업스트림 및 다운스트림 컨테이너와 함께 정보 흐름 경로를 구성하며, 두 정보 소스 간의 흐름 및 정보 처리 프로세스를 특성화 하며, 네트워크 상의 다수 컨테이너 사이에서정보 흐름의 임의 토폴로지컬 구조를 구성하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제21항에 있어서,상기 정보의 흐름과 처리 프로세스는 적어도 하나의 생물학적 뇌신경 메커니즘을 구현하는 데 사용되는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제22항에 있어서,상기 생물학적 뇌신경 메커니즘은, 신경 스파이킹이 시냅스를 통해 뉴런 사이에서 전도되고, 시냅스와 시냅스 사이의 정보 교환, 뉴런과 시냅스의가소성 등에서 적어도 하나를 포함하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제21항에 있어서,상기 정보 흐름의 임의 토폴로지컬 구조는 뇌신경계에 있는 임의 신경 회로의 연결방식을 구현하는 데적용되며, 동일한 뉴런을 자체의 피드백 연결, 동일 그룹의 뉴런 상호간 연결, 상이한 그룹의 뉴런간 임의연결, 및 시냅스와 시냅스 간의 직접 연결 등에 대한 지원을 포함하며, 피드백형 연결의 횟수 제한이 없는 순환연산을 허용하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제1항에 있어서,상기 모델 설명 모듈은 임의 레이어의 모델을 데이터와 조작 2개 부분으로 분해하는 모델링 디자인 하는 방식을지원하며,상기 데이터는 노드 매개변수 컨테이너, 에지 매개변수 컨테이너 및/또는 일반 매개변수 컨테이너에 의해 수용되며, 대응된 매개변수 데이터베이스에 저장되며;상기 조작은 상기 데이터에 액세스하고 업데이트하는 데 사용되는 실행 가능한 프로그램에 작용하며, 상기 조작은 범용 CPU, ARM, DSP, GPU 및/또는 기타 프로세서에서 실행되어, 시스템이 가진 크로스 하드웨어 플랫폼의 통용성을 보장하는 것을 특징으로 하는, 공개특허 10-2022-0027199-8-두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제1항에 있어서,상기 모델 설명 모듈은 사용자가 1개 또는 그 이상의 조작을 정의하여, 동일한 노드 컨테이너 상의 각 뉴런이서로의 데이터에 액세스 및/또는 업데이트 하도록 지원하여, 신속한 정보 교환을 구현하며, 생물학적 뇌신경계에서 전기적 시냅스를 시물레션하는 데 적용하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제1항에 있어서,상기 모델 설명 모듈은 사용자가 1개 또는 그 이상의 조작을 정의하도록 지원하여, 동일한 에지 컨테이너 있는각 시냅스가 서로의 데이터에 직접 액세스하고 업데이트할 수 있게 하여, 정보의 빠른 교환을 구현하며, 션팅억제와 같은 메커니즘을 포함한 생물학적 뇌신경계에 있는 동일한 뉴런의 수상돌기 상의 다수 시냅스 사이에서정보를 상호교환하고, 로직 연산을 진행하는 상황을 시뮬레이션하는 데 사용되는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "제1항에 있어서,상기 시스템은 사전 설정된 트리거 조건 및 실행 규칙에 따라 시냅스 및/또는 뉴런의 프루닝 및 신생 기능의 자동 실행을 지원하며; 상기 트리거 조건은 사용자가 상기 구성 설명 모듈을 통해 지정하며; 상기 실행 규칙은 사용자가 상기 모델 설명 모듈을 통해 지정하며;상기 실행 규칙은 네트워크 모델 객체 및/또는 하위 네트워크 또는 특정 컨테이너에 작용하며;상기 시냅스 및/또는 뉴런의 프루닝 및 신생 과정은 스케줄러에 의해 스케줄링하여 실행되며, 네트워크 실행 중에 수행, 및/또는 일시 중지될 때에도 실행하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_29", "content": "제28항에 있어서,상기 트리거 조건은,사용자 명령: 즉 사용자가 키보드, 마우스 또는 기타 방식을 통해 본 시스템에 명령을 입력하고 시스템은 명령을 수신한 직후 또는 제1 사전 설정된 시간 후에 프루닝 또는 신생 프로세스를 실행하며;지속 실행: 네트워크 모델 또는 하위 영역이 프루닝 또는 신생 프로세스의 규칙을 충족하면, 프루닝 또는 신생프로세스를 실행하며;간격 실행: 시스템은 제1 사전 설정 시간 간격 또는 제1 사전 설정 에르고딕 주기에 따라 프루닝 또는 신생 프로세스를 자동으로 시동하는;등과 같은 조건 중 1개 또는 그 이상을 포함하는 것을 특징으로 하는, 공개특허 10-2022-0027199-9-두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_30", "content": "제28항에 있어서,상기 프루닝 과정의 실행 규칙은 시냅스 프루닝 규칙 및/또는 뉴런 프루닝 규칙을 포함하며;상기 스냅스 프루닝 규칙은: 특정 시냅스 매개변수 및 지정된 참조 시냅스 집합의 모든 시냅스 매개변수의 통계치가 제1 사전 설정값 관계에도달하면, 해당 시냅스는 프루닝 대기 중인 시냅스이며;특정 시냅스의 매개변수와 지정된 역치가 제2사전 설정 값 관계에 도달하면, 해당 시냅스는 프루닝 대기 중인시냅스이며;특정 시냅스가 제2 사전 설정 시간 또는 제2 사전 설정 에르고딕 주기가 지난 후 트리거 되지 않으면, 해당 시냅스는 프루닝 대기 시냅스이며;특정 스냅스가 프루닝 대기로 표시되면, 해당 스냅스는 프루닝 대기 스냅스인;등과 같은 사항 중 1개 또는 그 이상의 스냅스 프루닝 규칙 사항을 포함하며;상기 뉴런의 프루닝 규칙은: 특정 뉴런에 입력된 시냅스가 없으면, 해당 뉴런은 프루닝 대기 뉴런이며; 특정 뉴런에 출력한 시냅스가없으면, 해당 뉴런은 프루닝할 뉴런이며;특정 뉴런에 입력 및 출력 시냅스가 없으면, 해당 뉴런은 프루닝 대기 뉴런이며; 특정 뉴런의 매개변수 및 지정된 참조 뉴런의 집합에 있는 모든 뉴런의 매개변수 통계값이 제3사전설정값 관계에 도달하면, 해당 뉴런은 프루닝 대기 뉴런이며;특정 뉴런의 매개변수와 지정된 역치가 제4 사전 설정 값 관계에 도달하면, 해당 뉴런은 프루닝 대기 뉴런이며;특정 뉴런이 제3 사전 설정된 시간 또는 제3 사전 설정 에르고딕 주기를 초과한 후에도 발사 되지 않으면, 해당뉴런은 프루닝 대기 뉴런이며; 특정 뉴런이 프루닝 대기로 표시되면 해당 뉴런은 프루닝할 뉴런인; 등과 같은 사항 중 1개 또는 그 이상의 규칙 사항을 포함하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_31", "content": "제28항에 있어서,상기 신생 과정의 실행 규칙은 뉴런 신생 규칙 및/또는 시냅스 신생 규칙이 포함되며;상기 뉴런의 신생 규칙은: 특정 노드 컨테이너에 있는 기존 뉴런 수와 해당 노드 컨테이너의 총 용량이 제1 사전 설정 비율 또는 제5 사전설정 값 관계에 도달하면, 총 용량에 대한 제2 사전 설정 비율 또는 제1 사전 설정 수량에 따라 뉴런을 새로 생성하며; 이 중, 제1 사전 설정 비율과 제2 설정 비율은 동일하거나 상이할 수 있으며;특정 노드 컨테이너는 제1 사전 설정 비율에 따라(즉, 사전 설정된 시간 간격 또는 사전 설정된 에르고딕 주기에 따라) 총 용량의 미리 설정된 제3 비율 또는 제2 사전 설정 수량에 따라 새로운 뉴런을 생성하며;특정 노드 컨테이너가 신생 대기 뉴런으로 표시되고, 제2 사전설정 속도에 따라 뉴런을 신생하는;등과 같은 사항 중 1개 또는 그 이상의 규칙 사항을 포함하며;공개특허 10-2022-0027199-10-상기 시냅스 신생 규칙은: 특정 에지 컨테이너의 기존 시냅스 수와 해당 에지 컨테이너의 총 용량이 제4 사전 설정 비율 또는 제6 사전 설정 값 관계에 도달하면, 해당 총 용량의 제5 사전 설정 비율 또는 제3 사전 설정 수량에 따라 시냅스를 새로 생성하며; 특정 에지 컨테이너는 제3 사전설정 속도에 따라 시냅스를 새로 생성하며;특정 에지 컨테이너가 신생 대기 시냅스로 표시되고, 제4 사전설정된 속도에 따라 시냅스를 새로 생성하며; 특정 노드 컨테이너에 시냅스 입력 또는 출력이 없는 뉴런이 존재할 경우, 대응된 각 에지 컨테이너에서 각각 대응되게 입력 시냅스 또는 출력 시냅스를 새로 생성하는;등과 같은 사항 중 1개 또는 그 이상의 규칙 사항을 포함하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_32", "content": "제1항에 있어서,시스템 실행 상태는 기본 상태, 네트워크 구축 상태, 네트워크 실행 상태 및/또는 네트워크 중단 상태가 포함되는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_33", "content": "제1항에 있어서,상기 모델 설명 모델에서 사용자는 각 컨테이너를 위해 1개 또는 그 이상의 규칙을 지정해 주며, 상기 1개 또는그 이상의 규칙으로 룰 베이스를 구성하며;상기 룰 매니저는 상기 룰 베이스에 있는 규칙을 사전 설정된 우선 순위에 따라 정렬하며, 1개 컨테이너에 작용하는 다수 규칙 상호간 충돌이 발생할 경우, 우선 순위가 가장 높은 규칙만 실행되며, 컨테이너에 그 어떤 규칙도 지정되지 않은 경우, 룰 매니저는 기본 규칙을 적용하여 실행하며;상기 룰 베이스에 존재하는 규칙은 에르고딕 규칙, 메모리 사용 규칙, 데이터 I/O 규칙 및/또는 시냅스 및 뉴런프루닝 및 신생 규칙이 포함되며;상기 에르고딕 규칙은 스케줄러가 제2 사전 설정 시간 간격 또는 제4 사전설정 에르고딕 주기에 따라, 네트워크상의 전부 또는 특정 컨테이너에 에르고딕을 중복하거나 건너 뛰도록 스케줄러에 지시하여, 컴퓨팅 리소스를 밀집한 하위 네트워크를 연산하는 데 집중시켜, 데이터의 활용 효율성 향상시키며;상기 메모리 사용 규칙은 스케줄러가 메인 메모리 및/또는 보조 프로세서 메모리의 사용을 합리적으로 배정하도록 지시하는 데 사용하며;상기 데이터 I/O 규칙은 스케줄러가 메인 메모리와 보조 프로세서 메모리 사이, 및 메모리와 하드 디스크 사이에서 진행된 데이터 교환 빈도를 스케줄링 하도록 지시하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_34", "content": "제1항에 있어서,상기 스케줄러는 1개 또는 그 이상의 메인 메모리 풀, 및, 1개 또는 그 이상의 장치 메모리 풀을 관리하며;상기 메인 메모리 풀은 메인 메모리의 사용을 관리하는 데 사용되며;공개특허 10-2022-0027199-11-상기 장치 메모리 풀은 각 보조 프로세서에 대응되며. 해당 장치 메모리의 사용을 관리하는 데 사용되며;상기 메인 메모리 풀과 장치 메모리 풀의 용량 상한과 하한은 사용자가 구성 설명 모듈을 통해 지정하는 것을특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_35", "content": "제1항 또는 제34항에 있어서,상기 스케줄러는 메인 컴퓨팅 장치, 보조 프로세서, 및/또는, I/O 장치의 연산 부하를 배정하기 위해, 1개 또는그 이상의 상기 스레드 풀을 관리하여, 하위 스레드가 다중 스레드 연산에 참여하도록 동적 배정하는 데 사용하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_36", "content": "제35항에 있어서,상기 스케줄러는 1개 또는 그 이상의 노드 데이터 입력 버퍼, 1개 또는 그 이상의 노드 데이터 출력 버퍼, 1개또는 그 이상의 에지 데이터 입력 버퍼 및 1개 또는 그 이상의 에지 데이터 출력 버퍼를 관리하여, 하드 디스크또는 I/O 장치를 읽고 쓰는 데 필요한 데이터를 캐싱하는 데 사용되어, 스케줄러가 프로세서, 하드 디스크 및I/O 장치의 부하에 따라 하드 디스크, I/O 장치의 읽고 쓰기를 배정하게 하여 I/O의 막힘을 피하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_37", "content": "제36항에 있어서,상기 각 버퍼의 용량, 하드 디스크 또는 I/O 장치의 읽기 쓰기 빈도의 상한 및 하한, 하드 디스크 또는 I/O 장치의 읽기 쓰기 처리량의 상한 및 하한은 사용자가 구성 설명 모듈을 통해 지정하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템."}
{"patent_id": "10-2022-7003194", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_38", "content": "두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 방법에 있어서, 상기 방법은 제1항 내지 제37항 중 임의 항에 기재된 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템을 채택하는 것을 특징으로 하는, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 방법."}
{"patent_id": "10-2022-7003194", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템 및 방법에 있어서, 상기 시스 템은 모델 설명 모듈, 매개변수 데이터베이스, 구성 설명 모듈, 구성 매니저，룰 매니저, 데이 터 매니저, 네트워크 빌더, 네트워크 매니저, 운영 매니저, 스케줄러, 로그 매니저, 실행 모니터링 모듈 및 그래픽 디스플레이 모듈을 포함하며, 상기 시스템은 특정 조건과 규칙에 따라 시냅스 및 뉴런 프루닝 및 신생 기능을 자동으로 수행하며, 프루닝 및 신생 프로세스를 원활하게 시동하기 위한 다양한 트리거 조건 및 실행 프로세스 규칙을 제공하여 신경망 개발자가 자체로 시냅스 및 뉴런의 프루닝, 신생 프로그 램을 작성해야 하는 부담을 줄여 주어, 기존의 두뇌 모방 스파이킹 신경망 컴퓨팅 프레임 상에 존재하는 일부 문 제를 효과적으로 해결하였다."}
{"patent_id": "10-2022-7003194", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 2019년 7월 2일에 중화인민공화국 특허청에 제출된 출원번호 201910588964.5,발명 명칭이 \"두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템 및 방법\"인 특허에 대한 중화인민공화국 특허출원의 우선권을 청구하며, 해당 특허의 전부 내용은 인용의 방식으로 본 발명에 포함된다.본 발명은 두뇌 모방 스파이킹 신경망에 대한 시뮬레이션 및 고성능 컴퓨팅 기술 분야와 관련되며, 보다 상세하 게, 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2022-7003194", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "현재, 두뇌 모방 인텔리전트 및 인지 컴퓨팅은 스파이킹 신경망에 기반하여, 생물학적 뇌에 있는 다양한 신경전 달물질, 신경조절제, 수용체, 전기적 시냅스, 화학적 시냅스, 수상돌기, 뉴런 및 글리아 세포의 다양한 작업 메 커니즘과 연결시켜 전산 모델링하여, 구성된 신경회로, 신경핵, 뇌영역 및 전뇌 모델은 기억 및 학습, 감정 시 뮬레이션, 네비게이션 및 계획, 모션 제어, 두뇌 모방 시각 및 두뇌 모방 청각, 주의력, 의사결정 등과 같은 생 물학적 뇌의 많은 인지 메커니즘 및 행위를 시뮬레이션할 수 있으며, 인공지능 시스템의 개발을 위해 보다 넓은 경로를 제공하였다. 단, 기존의 두뇌 모방 스파이킹 신경망 컴퓨팅 프레임에는 다음과 같은 문제가 존재한다: 1. 통합적이고 원활한 모델링 방법을 제공하지 않으며, 원활한 네트워크 토폴로지도 지원할 수 없어 모델링 척 도와 모델링 다양성 사이의 통합성을 달성할 수 없다. 예를 들어, 모델링 척도의 차원에서 보면, 기존의 대부분 두뇌 모방 스파이킹 신경망 컴퓨팅 구조는 분자 척도, 신경전달물질 및 신경조절인자 척도, 수용체 척도, 시냅 스 척도, 수상돌기 척도, 뉴런 척도, 글리아 세포 척도, 신경 회로 척도, 신경핵 척도, 신경 섬유 다발 척도, 뇌영역 척도, 전뇌 척도 및 행동 척도를 지원 및 통합할 수 없다. 또한, 모델링 다양성 차원에서 보면, 기존의 두뇌 모방 스파이킹 신경망 컴퓨팅 프레임은 전기적인 시냅스 모델링에 대한 지원을 자주 소홀히 하거나, 신경 조절제를 시뮬레이션하는 작업 메커니즘에 대한 지원을 소홀히 하기도 하며, 수상돌기 중 다수 시냅스 사이에서 진행되는 정보 교환 및 로직 메커니즘에 대한 지원도 소홀하고 있고, 시냅스와 시냅스 사이에 직접 연결된 토폴 로지도 지원할 수 없는 등의 경우도 있다. 2. 생물학적 뇌신경계의 구성 방식을 모방한 기술 방법을 제공하지 않고 있다. 3. 충분히 다양한 내장 기능과 원활한 사용자 인터페이스가 통합되어 있지 않으며, 시냅스와 뉴런의 자동 프루 닝 및 신생의 자동 수행을 지원할 수 없는 경우, 사용자는 관련 기능을 구현하기 위해 자체로 프로그램을 작성 해야 한다. 4. 스파이킹 신경망, 전통 신경망 및 텐서를 주요 데이터 표현 방법으로 사용하는 기타 알고리즘을 효과적으로 통합시켜 혼합 연산을 수행할 수 없다. 5. CPU, ARM, GPU, DSP 등과 같은 칩 아키텍처와 호환되지 않으며, 하드웨어 리소스에 대한 활용도 대부분 최적 화되어 있지 않아, 중등 내지 대규모 두뇌 모방 신경망을 효율적으로 실행하는 데 적합하지 않다. 상기 문제점의 존재는 기존의 뇌형 스파이킹 신경망 컴퓨팅 프레임 하드웨어 플랫폼의 제한된 모델링 범위, 열 악한 호환성, 저효율 연산 및 개발 및 사용 상 불편을 초래하여, 두뇌 모방 지능 및 인지 컴퓨팅 기술을 스마트 장난감, 로봇, 드론, I0T 장치, 인텔리전트 홈 및 차량 탑재 시스템과 같은 제품에 낮은 원가로 대폭 적용하기 어려워진다. 따라서, 두뇌 모방 지능 및 인지 컴퓨팅에 고효율적으로 유연히 적용되는 스파이킹 신경망 컴퓨팅 시스템 및 방 법을 제공하는 것은 당업자가 시급히 해결해야 할 과제로 대두되고 있다. 본 발명에 기재된 실시예의 목적 중 하나로 두뇌 모방 지능 및 인지 컴퓨팅에 적용되는 스파이킹 신경망 컴퓨팅 시스템 및 방법을 제공하는 데 있으며, 이와 같은 시스템은 통합적이고 원활한 모델링 방법을 제공하고, 제공된 다단계 트리 구조를 가진 네트워크 기술 방식을 통해 생물학적 뇌신경계에 대한 전체 척도의 모델링과 원활한 네트워크 토폴로지컬 구조를 지원하여, 모델링 척도와 모델링의 다양성을 유기적으로 통합하고, 각 척도의 전부 모델을 통합 신경망으로 융합하여 연산하며, 텐서 형식으로 데이터를 표현하고 저장하도록 지원하며, 시스템이 스파이킹 신경망을 지원할 수 있게 하고, 기존 신경망(딥 러닝) 및 기타 텐서를 주요 데이터 표현 방법으로 적 용하는 기타 알고리즘과도 호환 가능하며, 특정 조건과 규칙에 따라 시냅스 및 뉴런 프루닝 및 신생 기능을 자 동으로 수행하여, 관련 기능을 자체로 구현해야 하는 신경망 개발자의 부담을 덜어주어 기존의 두뇌 모방 스파 이킹 신경망 컴퓨팅 프레임에 존재하는 상기 문제를 효과적으로 해결하였다. 위의 기술적 과제를 해결하기 위해 본 발명의 실시예에서 채택된 기술적 솔루션은 다음과 같다. 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템 및 방법에 있어서, 상기 시스템은 모델 설명 모듈, 매개변수 데이터베이스, 구성 설명 모듈, 구성 매니저, 룰 매니저, 데이터 매니 저, 네트워크 빌더, 네트워크 매니저, 운영 매니저, 스케줄러, 로그 매니저, 실행 모니터링 모듈 및 그래픽 디 스플레이 모듈을 포함하며; 상기 모델 설명 모듈은 사용자가 네트워크 모델을 설계하고 설명할 수 있는 인터페이스를 제공하는 데 사용되며; 상기 매개변수 데이터베이스는 초기화 매개변수 및 런타임 매개변수를 포함하여 네트워크의 다양한 매개변수 데 이터를 저장하는 데 사용되며; 매개변수 데이터베이스는 바이너리 파일 또는 텍스트 파일일 수 있으며; 텍스트 파일은 CSV 파일 형식 또는 기타 문자 구분 데이터 파일 형식을 적용할 수 있으며; 상기 구성 설명 모듈은 현재 네트워크 운영 환경의 구성 매개변수 및 시냅스의 시동, 뉴런의 프루닝 및 신생을 시동하기 위한 조건을 설명하는 데 적용되며; 상기 구성 매니저는 시스템 구성 매개변수를 얻기 위해 상기 구성 설명 모듈을 읽는 데 적용되며; 상기 네트워크 모델 객체는 네트워크 빌더에 의해 구성되고 메모리에 상주하며 모든 컨테이너, 토폴로지 관계 및 매개변수 데이터를 포함한 전체 네트워크를 특성화하는 데 사용되며 스케줄러에 의해 스케줄링하여 실행되는 객체이며; 상기 룰 매니저는 모델 설명 모듈에 있는 사용자가 선언한 규칙을 읽고,스케줄러가 네트워크 모델 객체의 연산 을 스케줄링할 때 이러한 규칙을 해석하고 규칙 간의 충돌을 중재하는 데 사용되며; 상기 데이터 매니저는 1개 또는 그 이상의 디코더와 인코더를 포함하며, 매개변수 데이터베이스를 읽고 분석하 고, 데이터 형식을 변환하고, 데이터 직열화를 구현하는 데 적용되며; 사용자는 사용자 정의를 통해 디코더와 인코더를 데이터 매니저에 추가하여, 사용자 정의 형식의 파일을 읽고 쓸 수 있으며; 상기 네트워크 빌더는 모델 설명 모듈을 읽고, 네트워크의 토폴로지컬 구조를 분석하고, 데이터 매니저를 통해 데이터 파일을 읽고, 메모리에 네트워크 모델 객체를 구축하는 데 사용되며; 상기 네트워크 매니저는 네트워크 모델 객체를 구축, 에르고딕, 액세스 및 업데이트하는 데 적용되며; 상기 운영 매니저는 시스템에서 실행할 수 있는 모든 조작은 관리하는 데 사용되며; 모든 조작은 조작 라이브러 리를 구성하며; 사용자는 모델 설명 모듈에서 각 컨테이너를 위해 수행할 조작을 지정해 주며, 실행 시, 스케줄 러를 통해 해당 조작을 스케줄링하고 실행하며; 상기 스케줄러는 하드웨어 리소스를 배정하고, 연산 프로세스를 스케줄링하며, 연산 효율성을 최적화하는 데 적 용되며; 상기 로그 매니저는 시스템이 실행 시 생성된 로그를 기록하고 사용자에게 시스템의 작업 상태 및 이상상황을 제시하여, 용이한 디버깅 및 유지 관리를 위해 적용되며; 상기 실행 모니터링 모듈은 사용자 입력을 수신 및 응답하고, 기본 상태, 네트워크 구축 상태, 네트워크 실행 상태 및 네트워크 중단 상태를 포함한 전체 시스템의 실행 상태를 관리하는 데 적용되며; 상기 그래픽 디스플레이 모듈은 네트워크 데이터를 읽고 이를 사용자에게 표시하여 용이한 개발, 모니터링 및 디버깅할 수 있도록 적용된다. 선택적으로, 상기 모델 설명 모듈은 네트워크 설명 유닛, 수렴 설명 유닛 및 플로 설명 유닛을 포함하며, 이들 은 전체 네트워크의 각 구성 부분과 토폴로지컬 구조를 설명하는 데 공동으로 적용되며; 이들은 텍스트 파일로 선택될 수 있으며, 중첩된 문법을 적용하며, XML 또는 JS0N 파일 형식을 선택할 수 있다. 선택적으로, 상기 모델 설명 모듈은 생물학적 뇌 신경계의 구성 방식을 시뮬레이션하는 다단계 트리 구조의 네 트워크 설명 방식을 채택하며; 상기 수렴 설명 유닛은 미리 설정된 레이어 그룹 및 모듈에 따라 네트워크 노드를 정렬 구성하도록 지원하며, 생물학적 뇌에서 뉴런 및 관련 글리아 세포의 다단계 구성 방식을 표현하는 데 적용되며(예: 핵->뇌영역->전 뇌); 상기 플로 설명 유닛은 토폴로지(연결 관계) 유사성에 따라 네트워크 에지의 그룹화 및 계층적 정렬 및 구성을 지원하며 생물학적 뇌에 있는 신경 시냅스의 다양한 구성 방식(예: 수상돌기, 신경 경로의 투사, 신경 섬유 다 발 등) 및 관련 글리아 세포 돌기의 구성 방식을 표현하는 데 적용된다. 선택적으로, 네트워크 설명 유닛은 Network 및 Param와 같은 컨테이너를 설명하고, 전체 네트워크의 매개변수 및 실행 규칙을 설명하며, 링크를 통해 1개 또는 그 이상의 수렴 설명 유닛 및 플로 설명 유닛을 지향하는 데 적용되며; 상기 수렴 설명 유닛은 Confluence, Module, Layer, Node, NodeParam, Param 등의 컨테이너를 설명하는데 사용 되며, 네트워크에 있는 노드의 모듈과 레이어 그룹 간의 획분 관계, 각 컨테이너의 파라미터, 실행 시의 규칙 및 명령을 설명하는 데 적용되며; 상기 플로 설명 유닛은 Flow, Channel, Link, Edge, EdgeParam, Param과 같은 컨테이너를 설명하는 데 사용되 며, 네트워크 에지의 연결(토폴로지) 관계, 각 컨테이너의 매개변수 및 실행 시의 룰과 명령을 설명하는 데 적 용된다. 선택적으로, 상기 Network는 트리 구조의 제1 레이어(최상위 레벨)에 위치한 네트워크 컨테이너를 나타내며, 전 뇌 및 행동 척도의 모델을 특성화하는 데 사용되며, 각 Network는 1개 또는 그 이상의 Confluence 및 Flow를 수 용할 수 있으며; 상기 Confluence는 트리 구조의 제2 레이어에 위치하며, 뇌영역 척도의 모델을 특성화하는 데 사용할 수 있는 수렴 컨테이너를 나타내며, 각 Confluence는 1개 또는 그 이상의 Module을 수용할 수 있으며; 상기 Module은 트리 구조의 제3 레이어에 위치하며, 신경핵 척도의 모델을 특성화하는 데 사용할 수 있는 모듈 컨테이너를 나타내며, 각 Module은 1개 또는 그 이상의 Layer를 수용할 수 있으며; 상기 Layer는 트리 구조의 제4 레이어에 위치한 레이어 그룹 컨테이너를 나타내며, 신경 회로 척도의 모델을 나 타내는 데 사용할 수 있으며, Layer는 1개 또는 그 이상의 Node를 수용할 수 있으며; 상기 Node는 트리 구조의 제5 레이어에 위치한 노드 컨테이너를 나타내며, 뉴런 척도 또는 글리아 세포 척도 모 델을 특성화하는 데 사용할 수 있으며 뉴런 또는 글리아 세포 그룹을 특성화하는 데 사용할 수도 있으며, 각 Node는 1개 또는 그 이상의 NodeParam을 수용할 수 있으며; 상기 Node는 입력 및 출력 노드를 특성화하는 데 사용할 수도 있으며, 카메라 입력, 오디오 입력, 센서 입력, 제어 출력 등과 같이 시스템의 I/O 장치를 읽고 쓰는 데이터는 해당 Node의 각 NodeParam을 통해 동적으로 업데 이트 되며; 상기 NodeParam은 트리 구조의 제6 레이어(최하위 레벨)에 있는 노드 매개변수 컨테이너를 나타내며, 분자 척도, 수용체 척도, 신경전달물질 또는 신경조절제 척도의 모델을 특성화하는 데 사용할 수 있으며, 뉴런 또는 글리아 세포 그룹 모델의 매개변수 텐서를 특성화 하는 데에도 적용되며; 상기 Flow는 트리 구조의 제2 레이어에 있는 플로 컨테이너를 나타내며, 뇌영역을 연결하는 신경 섬유 다발의 척도를 특성화하는 모델에 사용할 수 있으며, 각 Flow는 1개 또는 그 이상의 Channel을 수용할 수 있으며; 상기 Channel은 트리 구조의 제3 레이어에 위치한 채널 컨테이너를 나타내며, 신경 핵 사이의 축삭 돌기를 연결 하여 형성된 전도 번들의 모델을 특성화하는 데 사용할 수 있으며, 각 Channel은 1개 또는 그 이상의 Link를 수 용할 수 있으며; 상기 Link는 트리 구조의 제4 레이어에 위치하며 신경 회로의 축삭돌기로 구성된 신경 경로 모델을 특성화하는 데 사용할 수 있는 연결 컨테이너를 나타내며, 각 Link는 1개 또는 그 이상의 Edge를 수용할 수 있으며; 상기 Edge는 트리 구조의 제5 레이어에 위치한 수상돌기 척도 또는 시냅스 척도의 모델을 특성화하는 데 사용할 수 있는 에지 컨테이너를 나타내며, 시냅스 또는 글리아 세포 돌기 그룹을 특성화하는 데 사용할 수도 있으며, 각 Edge는 1개 또는 그 이상의 EdgeParam을 수용할 수 있으며; 상기 EdgeParam은 트리 구조의 제6 레이어(최하위 레벨)에 위치한 에지 매개변수 컨테이너를 나타내며, 분자 척 도, 신경전달물질 또는 신경조절제 척도, 수용체 척도의 모델을 특성화하는 데 사용할 수 있으며, 시냅스 또는 글리아 세포 돌기 그룹 모델의 매개변수 텐서를 특성화하는 데에도 적용할 수 있으며; 상기 Param은 일반 매개변수 컨테이너를 나타내며 보조 컨테이너에 속한다. 모델링의 필요성에 따라 상기 각 레 이어에 있는 컨테이너는 텐서 형식의 매개변수 데이터를 유지하는 데 사용되는 1개 또는 그 이상의 Param을 추가로 가질 수 있으며; 혹은, 상기 각 레이어의 컨테이너는 Param를 가지지 않을 수도 있으며; 상기 각 컨테이너에는 다단계 트리 구조에서 인덱싱에 사용되는 번호와 이름을 가지며; 상기 각 컨테이너에는 네트워크의 에르고딕 순서 및 규칙, 에르고딕 연산 참가 횟수, 데이터가 메인 메모리에 입주 여부, 데이터가 프로세서 메모리에 입주여부, 하드 디스크의 읽기 및 쓰기 빈도 등을 포함하는 정보의 통 계 및 제어 데이터를 저장하는 데 사용되는 1개 또는 그 이상의 제어블록(Control Block)가지며, 룰 매니저 및 스케줄러에 의해 관리되고 업데이트된다. 선택적으로, 상기 뉴런 모델의 발사 특성은 단발성 파이링, 라피드 파이링, 버스트 파이링, 플래토우 파이링 또 는 위상성 파이링 등으로 구축될 수 있으며; 상기 뉴런 모델이 업스트림 입력 신호에 대한 응답은 상이한 신경 적응성 또는 감도 곡선으로 구축 될 수 있으 며; 상기 뉴런 모델은 다운스트림 작용 메커니즘에 대하여 흥분성, 억제성, 변조성 또는 중립성 모델로 구축될 수 있으며; 상기 뉴런 모델은 스파이킹 뉴런 모델 또는 기존 뉴런 모델로 구축될 수 있으며; 상기 글리아 세포 모델은 성상 글리아 세포, 희돌기교세포, 미세아교세포, 슈반 세포 및 위성 세포 모델로 구축 될 수 있다. 선택적으로, 신경전달물질 또는 신경조절제 모델은 흥분성, 억제성 또는 변조성 모델로 구축 될 수 있으며; 상기 수용체 모델은 이온성 또는 대사성 모델로 구축될 수 있으며; 상기 신경전달물질 또는 신경조절제에 대한 수용체 모델의 반응 효과는 흥분성, 억제성, 변조성 또는 중립성 모 델로 구축될 수 있다. 선택적으로, 상기 수상돌기 척도의 모델은 정점 수상돌기 모델, 기저 수상돌기 모델 또는 스파이크 모델로 구축 될 수 있다. 상기 시냅스 모델은 흥분성, 억제성, 변조성 또는 중립성으로 구축될 수 있다. 선택적으로, 상기 분자 척도의 모델은 세포내 분자 모델, 세포막 분자모델 및 세포간질 분자 모델로 구축될 수 있다. 선택적으로, 상기 NodeParam, EdgeParam 및 Param 내부는 텐서(즉, 다차원 매트릭스) 형식의 매개변수를 수용할 수 있으며, 상기 텐서의 차원은 1차원 또는 다차원일 수 있으며, 특정 정렬 및 사용 방식은 사용자에 의해 지정되며; 상기 텐서는 4차원으로 설정할 수 있으며, 텐서에서 각 매개변수의 위치는 좌표(x, y, z, t)로 표시할 수 있으 며, 여기서 x, y, z, 3개 차원은 상위 컨테이너에 표시된 각 신경조직(예를 들어,뉴런 또는 시냅스 등) 모델의 공간적 배열에 대응되며, t는 시간 차원을 나타내며, 타이밍 정보의 캐시 및 지연을 특성화할 수 있으며, 신경 조절제가 뉴런 및 시냅스에 대한 장기 작용 메커니즘(지연성을 가짐)을 시뮬레이션하는 데 사용할 수 있으며; 상기 텐서의 매개변수는 상위 컨테이너에 있는 신경 조직(예를 들어, 뉴런 또는 시냅스) 모델의 전체 또는 일부 에서 공유할 수 있으며, 타겟 영역에 있는 모든 신경 조직에 대한 신경조절제의 대면적의 작용을 시뮬레이션하 는 데 사용할 수 있다. 선택적으로, 상기 Flow 및 모든 하위 컨테이너는 1개 또는 그 이상의 업스트림 컨테이너, 및, 1개 또는 그 이상 의 다운스트림 컨테이너에 대응될 수 있으며, 업스트림 및 다운스트림 컨테이너의 번호 또는 이름을 통해 인덱 싱하고 액세스할 수 있다. 상기 업스트림 컨테이너와 다운스트림 컨테이너는 모두 임의 레이어에 있는 컨테이너일 수 있으며, 양자는 동일 하거나 다른 컨테이너일 수 있으며; 상기 Flow 및 모든 하위 컨테이너는 업스트림 및 다운스트림 컨테이너와 함께 정보 흐름 경로를 형성할 수 있으 며, 두 정보 소스(예: 업스트림 및 다운스트림 컨테이너) 간의 (단방향 또는 양방향) 흐름 및 정보 처리 프로세 스를 특성화 하며, 네트워크 상의 다수 컨테이너 사이에서 정보 흐름의 임의 토폴로지컬 구조를 구성할 수있다. 선택적으로, 상기 정보 흐름 및 처리 과정은 신경 스파이킹이 시냅스를 통해 뉴런 사이에서 전도되고, 시냅스와 시냅스 사이의 정보 교환, 뉴런과 시냅스 가소성과 같은 다양한 생물학적 뇌 신경 메커니즘을 구현하는 데 적용 할 수 있다. 선택적으로, 상기 정보 흐름의 임의 토폴로지컬 구조는 뇌신경계에 있는 임의 신경 회로의 연결방식을 구현하는 데 적용할 수 있으며, 동일한 뉴런을 자체의 피드백 연결, 동일 그룹(레이어)의 뉴런 상호간 연결, 상이한 그룹 (레이어)의 뉴런간 임의 연결(순차/피드포워드, 교차 레이어, 피드백 등), 및 시냅스와 시냅스 간의 직접 연결 등에 대한 지원도 포함하며, 피드백형 연결의 횟수 제한이 없는 순환 연산을 허용한다. 선택적으로, 상기 시스템은 모든 레이어(또는 척도) 의 모델을 데이터(data) 및 조작(Operation) 2개 부분의 모 델링 디자인 방식을 지원하며, 상기 데이터는 NodeParam, EdgeParam 또는 Param에 의해 수용될 수 있으며, 대응된 매개변수 데이터베이스에 의 해 저장되며; 상기 조작은 위에서 언급한 데이터에 액세스하고 업데이트하는 데 사용되는 실행 가능한 프로그램(예: 함수 및 함수를 포함하는 클래스)으로, 조작은 범용 CPU, ARM, DSP, GPU 또는 기타 프로세서에서 실행되어 상기 시스템 이 가진 특정 크로스 하드웨어 플랫폼의 통용성을 보장한다. 선택적으로, 상기 시스템은 사용자가 1개 또는 그 이상의 조작을 정의하도록 지원하여 동일한 Node 상의 각 뉴 런(Edge를 거칠 필요 없이)이 서로의 데이터에 직접 액세스하고 업데이트하여 신속한 정보 교환을 구현할 수 있 어, 생물학적 뇌신경계에서 전기적 시냅스를 시뮬레이션하는 데 적용된다. 선택적으로, 상기 시스템은 사용자가 1개 또는 그 이상의 조작을 정의하도록 지원하여, 동일한 Edge에 있는 각 시냅스가 서로의 데이터에 직접 액세스하고 업데이트할 수 있게 하여, 정보의 빠른 교환을 구현하며, 션팅 억제 (shunting inhibition)와 같은 메커니즘을 포함한 생물학적 뇌신경계에 있는 동일한 뉴런의 수상돌기 상의 다수 시냅스 사이에서 정보를 상호교환하고, 로직 연산을 진행하는 상황을 시뮬레이션하는 데 사용된다. 선택적으로, 상기 시스템은 사전에 설정된 트리거 조건 및 실행 규칙에 따라 시냅스와 뉴런의 프루닝 및 신생 기능을 자동 실행하도록 지원하며, 상기 트리거 조건은 사용자가 구성 설명 모듈을 통해 지정할 수 있으며; 상기 실행 규칙은 사용자가 상기 모델 설명 모듈을 통해 지정할 수 있으며; 상기 실행 규칙은 전체 네트워크 모델 객체 또는 하위 네트워크 또는 특정 컨테이너에 작용할 수 있으며; 상기 시냅스와 뉴런의 프루닝 및 신생 과정은 스케줄러에 의해 스케줄링하여 실행되며, 네트워크가 실행 중이거 나 일시 중지될 때에도 수행할 수 있다. 선택적으로, 상기 트리거 조건은 다음 중 하나 또는 임의 다수 사항을 포함한다: 사용자 명령, 즉 사용자가 키보드, 마우스 또는 기타 방식을 통해 본 시스템에 명령을 입력하고 본 시스템은 명 령을 수신한 직후 또는 제1 사전 설정된 시간 후에 프루닝 또는 신생 프로세스를 실행하며; 지속 실행, 즉 네트워크 모델 또는 하위 영역이 프루닝 또는 신생 프로세스의 규칙을 충족하면, 프루닝 또는 신 생 프로세스를 실행하며; 간격 실행, 즉 시스템은 제1 사전 설정 시간 간격 또는 제1 사전 설정 에르고딕 주기에 따라 적시에 프루닝 또 는 신생 프로세스를 자동으로 시동한다. 선택적으로, 상기 프루닝 프로세스의 실행 규칙은 시냅스 프루닝 규칙과 뉴런 프루닝 규칙으로 구분되며; 상기 스냅스 프루닝 규칙은 다음 중 하나 또는 임의 다수 사항을 포함한다: 특정 시냅스 매개변수 및 지정된 참조 시냅스 집합의 모든 시냅스 매개변수의 통계치가 제1 사전 설정값 관계에 도달하면(예를 들어, 특정 시냅스의 가중치가 지정된 에지의 모든 시냅스 평균 가중치의 1% 미만인 경우), 해당 시냅스는 프루닝 대기 중인 시냅스이며; 특정 시냅스의 매개변수와 지정된 역치가 제2사전 설정 값 관계에 도달하면(예를 들어 시냅스의 가중치가 10.0 미만의 경우), 해당 시냅스는 프루닝 대기 중인 시냅스이며; 특정 시냅스가 제2 사전 설정 시간 또는 제2 사전 설정 에르고딕 주기가 지난 후 트리거 되지 않으면, 해당 시 냅스는 프루닝 대기 시냅스이며; 특정 시냅스가 다른 연산 과정에서 프루닝 가능함으로 표시되면 해당 시냅스는 프루닝 대기 시냅스이고; 프루닝 대기 시냅스는 프루닝이 가능하며; 상기 뉴런에 대한 프루닝 규칙은 다음 중 하나 또는 임의 다수 사항을 포함한다. 특정 뉴런에 입력된 시냅스가 없는 경우, 해당 뉴런은 프루닝 할 뉴런이며; 특정 뉴런에 출력한 시냅스가 없는 경우, 뉴런은 프루닝 할 뉴런이며; 특정 뉴런에 입력 및 출력한 시냅스가 없는 경우, 뉴런은 프루닝 대기 뉴런이며; 특정 뉴런의 매개변수와 지정된 참조 뉴런 집합에 있는 모든 뉴런 매개변수에 대한 통계값이 제3사전 설정 값 관계에 도달하면(특정 뉴런의 역치가 지정된 노드의 모든 뉴런 역치의 최대값보다 큰 경우), 해당 뉴런은 프루 닝 대기 뉴런이며; 특정 뉴런의 매개변수와 지정된 역치가 제4 사전 설정 값 관계에 도달하면(예를 들어, 특정 뉴런의 역치가 1000.0보다 큰 경우), 해당 뉴런은 프루닝 대기 뉴런이며; 특정 뉴런이 제3 사전 설정된 시간 또는 제3 사전 설정 에르고딕 주기를 초과한 후에도 발사 되지 않으면, 해당 뉴런은 프루닝 대기 뉴런이며; 특정 뉴런이 기타 연산 프로세스에 의해 프루닝 가능한 것으로 표시되면, 해당 뉴런은 프루닝 대기 뉴런이며; 프루닝 대기 뉴런은 프루닝 작업이 가능하며; 선택적으로, 상기 신생 프로세스의 수행 규칙은 뉴런 신생 규칙과 시냅스 신생 규칙으로 구분되며; 상기 뉴런의 신생 규칙은 다음 중 하나 또는 임의 다수 사항을 포함한다. 특정 노드 컨테이너에 있는 기존 뉴런 수와 해당 노드 컨테이너의 총 용량이 제1 사전 설정 비율 또는 제5 사전 설정 값 관계에 도달하면, 총 용량에 대한 제2 사전 설정 비율 또는 제1 사전 설정 수량에 따라 뉴런을 새로 생 성하며; 이 중, 제1 사전 설정 비율과 제2 설정 비율은 동일하거나 상이할 수 있으며; 특정 노드 컨테이너는 제1 사전 설정 비율에 따라(즉, 사전 설정된 시간 간격 또는 사전 설정된 에르고딕 주기 에 따라) 총 용량의 미리 설정된 제3 비율 또는 제2 사전 설정 수량에 따라 새로운 뉴런을 생성하며; 특정 노드 컨테이너는 다른 연산 프로세스에 의해 신생 뉴런이 필요한 것으로 표시되고, 제2 사전설정 속도에 따라(즉, 사전 설정 시간 간격 또는 사전 설정 에르고딕 주기별로 총량에 대한 사전 설정 비율 또는 수량) 뉴런 을 새로 생성하며; 상기 시냅스 신생 규칙은 다음 중 하나 또는 임의 다수 사항을 포함한다. 특정 에지 컨테이너의 기존 시냅스 수와 해당 에지 컨테이너의 총 용량이 제4 사전 설정 비율 또는 제6 사전 설 정 값 관계에 도달하면, 해당 총 용량의 제5 사전 설정 비율 또는 제3 사전 설정 수량에 따라 시냅스를 새로 생 성하며; 이 중, 제4 사전설정 비율과 제5 사전 설정 비율은 같거나 다를 수 있으며; 특정 에지 컨테이너는 제3 사전설정 속도(즉, 사전 설정 시간 간격 또는 사전 설정 에르고딕 주기별로 총량에 대한 사전 설정 비율 또는 수량)에 따라 새로운 시냅스를 생성하며; 특정 에지 컨테이너는 다른 연산 프로세스에 의해 필요한 신생 시냅스로 표시되고, 제4 사전 설정 속도에 따라 (즉, 사전 설정 시간 간격 또는 사전 설정 에르고딕 주기별로 총량에 대한 사전 설정 비율 또는 수량) 새로운 시냅스를 생성하며; 특정 노드 컨테이너에 시냅스 입력 또는 출력이 없는 뉴런이 존재할 경우, 대응된 각 에지 컨테이너에서 각각 대응되게 입력 시냅스 또는 출력 시냅스를 새로 생성한다. 선택적으로, 상기 모델 설명 모듈에서 사용자는 각 컨테이너를 위해 1개 또는 그 이상의 규칙을 지정할 수 있으 며, 전부의 규칙을 통해 룰 베이스를 구성하며, 상기 룰 매니저는 상기 룰 베이스에 있는 규칙을 사전 설정된 우선 순위에 따라 정렬하며, 1개 컨테이너에 작용 하는 다수 규칙 상호간 충돌이 발생할 경우, 우선 순위가 가장 높은 규칙만 실행되며, 컨테이너에 그 어떤 규칙도 지정되지 않은 경우, 룰 매니저는 기본 규칙을 적용하여 실행하며; 상기 룰 베이스에 존재하는 규칙은 에르고딕 규칙, 메모리 사용 규칙, 데이터 I/O 규칙, 시냅스 및 뉴런 프루닝 및 신생 규칙이 포함되며; 상기 에르고딕 규칙은 스케줄러가 제2 사전 설정 시간 간격 또는 제4 사전설정 에르고딕 주기에 따라, 네트워크 상의 전부 또는 특정 컨테이너에 에르고딕을 중복하거나 건너 뛰도록 스케줄러에 지시하여, 컴퓨팅 리소스를 밀 집한 하위 네트워크를 연산하는 데 집중시켜, 데이터의 활용 효율성 향상시키며; 상기 메모리 사용 규칙은 스케줄러가 메인 메모리와 보조 프로세서 메모리의 사용을 합리적으로 배정하도록 지 시하는 데 사용할 수 있으며; 상기 데이터 I/O 규칙은 스케줄러가 메인 메모리와 보조 프로세서 메모리 사이, 및 메모리와 하드 디스크 사이 에서 진행된 데이터 교환 빈도를 스케줄링 하도록 안내하여 I/O 리소스를 절감하고 전체 컴퓨팅을 향상시키는 데 사용할 수 있다. 선택적으로, 상기 스케줄러는 1개 또는 그 이상의 메인 메모리 풀, 및, 1개 또는 그 이상의 장치 메모리 풀을 관리하여, 메인 메모리 및 각 장치의 메모리에서 네트워크 모델 객체를 사용하는 상황을 합리적으로 배정하며, 상기 메인 메모리 풀은 메인 메모리의 사용을 관리하는 데 사용되며; 상기 장치 메모리 풀은 각 보조 프로세서(ARM, GPU, DSPASIC 등일 수 있음)에 대응되며. 해당 장치 메모리의 사 용을 관리하는 데 사용되며; 상기 메인 메모리 풀과 장치 메모리 풀의 용량 상한과 하한은 사용자가 구성 설명 모듈을 통해 지정한다. 선택적으로, 상기 스케줄러는 다중 스레드 작업에 참여하도록 하위 스레드를 동적으로 배열하기 위한 1개 또는 그 이상의 스레드 풀을 관리하여, 메인 컴퓨팅 유닛(CPU, ARM 등일 수 있음) 및 보조 프로세서(ARM, GPU, DSP 등일 수 있음) 및 I/O 장치(하드디스크, 카메라, 오디오 입력, 제어 출력 등)의 연산 부하를 적절하게 배정한다. 선택적으로, 상기 스케줄러는 1개 또는 그 이상의 노드 데이터 입력 버퍼, 1개 또는 그 이상의 노드 데이터 출 력 버퍼, 1개 또는 그 이상의 에지 데이터 입력 버퍼, 1개 또는 그 이상의 에지 데이터 출력 버퍼를 관리하여, 하드 디스크 또는 I/O 장치를 읽고 쓰는 데 필요한 데이터를 캐싱하는 데 사용되어, 스케줄러가 프로세서, 하드 디스크 및 I/O 장치의 부하에 따라 하드 디스크, I/O 장치의 읽고 쓰기를 적시에 배정하게 하여 I/O의 막힘을 피하며; 상기 각 버퍼의 용량, 하드 디스크 또는 I/O 장치의 읽기 쓰기 빈도의 상한 및 하한, 하드 디스크 또는 I/O 장 치의 읽기 쓰기 처리량의 상한 및 하한은 사용자가 구성 설명 모듈을 통해 지정한다. 본 발명은 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 방법도 제공하며, 해당 방 법은 상기 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템을 채택한다. 상기 기술 솔루션을 통해 알 수 있듯이, 기존 기술에 비해, 본 발명은 두뇌 모방 지능 및 인지 컴퓨팅에 적용되 는 스파이킹 신경망 컴퓨팅 시스템 및 방법을 제공하고 있으며, 이와 같은 시스템은 통합적이고 원활한 모델링 방법을 제공하고, 제공된 다단계 트리 구조를 가진 네트워크 기술 방식을 통해 생물학적 뇌신경계에 대한 전체 척도의 모델링과 원활한 네트워크 토폴로지컬 구조를 지원하여, 모델링 척도와 모델링의 다양성을 유기적으로 통합하고, 각 척도의 전부 모델을 통합 신경망으로 융합하여 연산하며, 텐서 형식으로 데이터를 표현하고 저장 하도록 지원하며, 시스템이 스파이킹 신경망을 지원할 수 있게 하고, 기존 신경망(딥 러닝) 및 기타 텐서를 주 요 데이터 표현 방법으로 적용하는 기타 알고리즘과도 호환 가능하며, 특정 조건과 규칙에 따라 시냅스 및 뉴런 프루닝 및 신생 기능을 자동으로 수행하여, 관련 기능을 자체로 구현해야 하는 신경망 개발자의 부담을 덜어주 었다. 상기 모델링 설계 방식은 임의 레이어(또는 척도)의 모델을 데이터(data)와 조작(operation)의 2개 부분으로 분 해할 수 있다. 위에서 기재한 바와 같이, 데이터는 NodeParam, EdgeParam 또는 Param에서 수용할 수 있으며 대 응된 매개변수 데이터베이스에 저장할 수 있다. 조작은 위에서 기재한 데이터에 액세스하고 업데이트할 수 있는 실행 가능한 프로그램(예: 함수 및 함수를 포함하는 클래스)이다."}
{"patent_id": "10-2022-7003194", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명이 달성하고자 하는 목적,기술방안 및 장점 등을 보다 명확하게 기재하기 위하여, 첨부된 도면 및 실시 예를 참조하여 다음과 같이 본 발명을 더욱 상세하게 설명하고자 한다. 이 부분에서 설명된 특정 실시예는 본 발명을 설명하기 위해 사용된 것으로, 본 발명을 제한하기 위함이 아님을 이해해야 한다. 본 발명의 기술 솔루션을 설명하기 위해, 다음과 같이 도면 및 실시예와 연결시켜 상세하게 설명한다. 도1를 참조하여 보면, 본 발명의 실시예를 통해 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신 경망 컴퓨팅 시스템을 명시하며, 이와 같은 시스템은 모델 설명 모듈, 매개변수 데이터베이스, 구성 설명 모듈, 구성 매니저, 룰 매니저, 데이터 매니저, 네트워크 빌더, 네트워크 매니저, 운영 매 니저, 스케줄러, 로그 매니저, 실행 모니터링 모듈 및 그래픽 디스플레이 모듈을 포함한다. 상기 모델 기술 모듈은 네트워크 기술 유닛, 수렴 설명 유닛 및 플로 설명 유닛을 포함한다. 이와 같은 유닛 을 통해 전체 네트워크 상에 있는 다양한 구성 요소와 토폴로지를 설명하고 있다. 이들은 우선적으로 중첩식 문 법을 적용하고 있으며, XML 또는 JS0N 파일 형식을 선택할 수 있다. 네트워크 설명 유닛은 Network 및 Param와 같은 컨테이너를 설명하고, 전체 네트워크의 매개변수 및 실행 규칙 을 설명하며, 링크를 통해 1개 또는 그 이상의 수렴 설명 유닛 및 플로 설명 유닛을 지향하는 데 적용된다. 수렴 설명 유닛은 Confluence, Module, Layer, Node, NodeParam, Param 등의 컨테이너를 설명하는데 사용되며, 네트워크에 있는 노드의 모듈과 레이어 그룹 간의 획분 관계, 각 컨테이너의 파라미터, 실행 시의 규칙 및 명령 을 설명하는 데 적용된다. 플로 설명 유닛은 Flow, Channel, Link, Edge, EdgeParam, Param과 같은 컨테이너를 설명하는 데 사용되며, 네 트워크 에지의 연결(토폴로지) 관계, 각 컨테이너의 매개변수 및 실행 시의 룰과 명령 설명하는 데 적용된다. 도 3을 참조하면, 모델 설명 모듈에서 지원되는 네트워크 설명 모드는 바람직하게는 생물학적 뇌 신경계의 구성 방식을 모방하기 위해 다단계 트리 구조로 표현된다. 1.상기 수렴 설명 유닛은 미리 설정된 레이어 그룹 및 모듈에 따라 네트워크 노드를 정렬 구성하도록 지원하며, 생물학적 뇌에서 뉴런 및 관련 글리아 세포의 다단계 구성 방식을 표현하는 데 적용되며(예: 핵->뇌영역->전 뇌); 2. 상기 플로 설명 유닛은 토폴로지(연결 관계) 유사성에 따라 네트워크 에지의 그룹화 및 계층적 정렬 및 구성 을 지원하며 생물학적 뇌에 있는 신경 시냅스의 다양한 구성 방식(예: 수상돌기, 신경 경로의 투사, 신경 섬유 다발 등) 및 관련 글리아 세포 돌기의 구성 방식을 표현하는 데 적용된다. 이를 통해 대규모 두뇌 모방 신경망 의 개발, 디버깅, 관리 및 스케줄링이 보다 직관적이고 편리해지게 한다. 구체적으로, 상기 다단계 트리 구조에서: Network는 트리 구조의 제1 레이어(최상위 레벨)에 위치한 네트워크 컨테이너를 나타내며, 전뇌 및 행동 척도의 모델을 특성화하는 데 사용된다. 각 Network는 1개 또는 그 이상의 Confluence 및 Flow를 수용할 수 있다. Confluence는 트리 구조의 제2 레이어에 위치하며, 뇌영역 척도의 모델을 특성화하는 데 사용할 수 있는 수렴 컨테이너를 나타낸다. 각 Confluence는 1개 또는 그 이상의 Module을 수용할 수 있다.Module은 트리 구조의 제3 레이어에 위치하며, 신경핵 척도의 모델을 특성화하는 데 사용할 수 있는 모듈 컨테 이너를 나타낸다. 각 Module은 1개 또는 그 이상의 Layer를 수용할 수 있다. Layer는 트리 구조의 제4 레이어에 위치한 레이어 그룹 컨테이너를 나타내며, 신경 회로 척도의 모델을 나타내 는 데 사용할 수 있다. 각 Layer는 1개 또는 그 이상의 Node를 수용할 수 있다. Node는 트리 구조의 제5 레이어에 위치한 노드 컨테이너를 나타내며, 뉴런 척도 또는 글리아 세포 척도 모델을 특성화하는 데 사용할 수 있으며 뉴런 또는 글리아 세포 그룹을 특성화하는 데 사용할 수도 있다. 뉴런 모델의 발사 특성은 단발성 파이링, 라피드 파이링, 버스트 파이링, 플래토우 파이링 또는 위상성 파이링 등으로 구축 될 수 있으며, 상기 뉴런 모델이 업스트림 입력 신호에 대한 응답은 상이한 신경 적응성 또는 감도 곡선으로 구 축 될 수 있으며, 상기 뉴런 모델은 다운스트림 작용 메커니즘에 대하여 흥분성, 억제성, 변조성 또는 중립성 모델로 구축될 수 있다. 글리아 세포 모델은 성상 글리아 세포, 희돌기교세포, 미세아교세포, 슈반 세포 및 위 성 세포 모델로 구축될 수 있다. 각 Node는 1개 또는 그 이상의 NodeParam을 수용할 수 있다. Node가 동일한 유 형의 뉴런 그룹을 특성화하는 데 사용될 경우, 수용하는 NodeParam의 수량은 뉴런 모델의 매개변수 유형 수에 의해 결정되며, 즉, 각 유형은 1개의 NodeParam에 대응되며, 해당 Node 상에 있는 모든 뉴런의 해당 유형 매개 변수를 텐서로 정렬하여 저장한다. Node는 입력 및 출력 노드를 특성화하는 데 사용할 수도 있으며, 카메라 입력, 오디오 입력, 센서 입력, 제어 출력 등과 같이 시스템의 I/O 장치와 도킹하는 데 사용된다. I/O 장치를 읽고 쓰는 데이터는 해당 Node의 각 NodeParam을 통해 동적으로 업데이트 된다. NodeParam은 트리 구조의 제6 레이어(최하위 레벨)에 있는 노드 매개변수 컨테이너를 나타내며, 분자 척도, 수 용체 척도, 신경전달물질 또는 신경조절제 척도의 모델을 특성화하는 데 사용할 수 있으며, 뉴런 또는 글리아 세포 그룹 모델의 매개변수 텐서를 특성화 하는 데에도 적용된다. 신경전달물질 또는 신경조절제 모델은 흥분성, 억제성 또는 변조성 모델로 구축 될 수 있다. 수용체 모델은 이온성 또는 대사성으로 구축될 수 있으며, 신경전달물질 또는 신경조절제에 대한 수용체 모델의 반응 효과는 흥분성, 억제성, 변조성 또는 중립성 모델로 구축될 수 있다. Flow는 트리 구조의 제2 레이어에 있는 플로 컨테이너를 나타내며, 뇌영역을 연결하는 신경 섬유 다발의 척도를 특성화하는 모델에 사용할 수 있다. 각 Flow는 1개 또는 그 이상의 Channel을 수용할 수 있다. Channel은 트리 구조의 제3 레이어 위치한 채널 컨테이너를 나타내며, 신경 핵 사이의 축삭 돌기를 연결하여 형 성된 전도 번들의 모델을 특성화하는 데 사용할 수 있다. 각 Channel은 1개 또는 그 이상의 Link를 수용할 수 있다. Link는 트리 구조의 제4 레이어에 위치하며 신경 회로의 축삭돌기로 구성된 신경 경로 모델을 특성화하는 데 사 용할 수 있는 연결 컨테이너를 나타낸다. 각 Link는 1개 또는 그 이상의 Edge를 수용할 수 있다. Edge는 트리 구조의 제5 레이어에 위치한 수상돌기 척도 또는 시냅스 척도의 모델을 특성화하는 데 사용할 수 있는 에지 컨테이너를 나타내며, 시냅스 또는 글리아 세포 돌기 그룹을 특성화하는 데 사용할 수도 있다. 수상 돌기 척도의 모델은 정점 수상돌기 모델, 기저 수상돌기 모델 또는 스파이크 모델로 구축될 수 있다. 시냅스 모 델은 흥분성, 억제성, 변조성 또는 중립성으로 구축될 수 있다. 각 Edge는 1개 또는 그 이상의 EdgeParam을 수 용할 수 있다. Node가 동일한 유형의 뉴런 그룹을 특성화하는 데 사용될 경우, 수용하는 NodeParam의 수량은 뉴 런 모델의 매개변수 유형 수에 의해 결정되며, 즉, 각 유형은 1개의 NodeParam에 대응되며, 해당 Node 상에 있 는 모든 뉴런의 해당 유형 매개변수를 텐서로 정렬하여 저장한다. EdgeParam은 트리 구조의 제6 레이어(최하위 레벨)에 위치한 에지 매개변수 컨테이너를 나타내며, 분자 척도, 신경전달물질 또는 신경조절제 척도, 수용체 척도의 모델을 특성화하는 데 사용할 수 있으며, 시냅스 또는 글리 아 세포 돌기 그룹 모델의 매개변수 텐서를 특성화하는 데에도 적용할 수도 있다. 분자 척도의 모델은 세포내 분자 모델, 세포막 분자모델 및 세포간질 분자 모델로 구축될 수 있다. 신경전달물질 또는 신경조절제 모델은 흥분성, 억제성 또는 변조성 모델로 구축 될 수 있다. 신경전달물질 또는 신경조절제에 대한 수용체 모델의 반 응 효과는 흥분성, 억제성, 변조성 또는 중립성 모델로 구축될 수 있다. Param은 일반 매개변수 컨테이너를 나타내며 보조 컨테이너에 속한다. 모델링의 필요성에 따라 상기 각 레이어 에 있는 컨테이너는 텐서 형식의 매개변수 데이터를 유지하는 데 사용되는 1개 또는 그 이상의 Param을 추가로 가질 수 있으며; 혹은, 상기 각 레이어의 컨테이너는 Param를 가지지 않을 수도 있으며;NodeParam, EdgeParam 및 Param 내부는 텐서(즉, 다차원 매트릭스) 형식의 매개변수를 수용할 수 있다. 텐서의 차원은 1차원 또는 다차원일 수 있으며, 특정 정렬 및 사용 방식은 사용자에 의해 지정된다. 예를 들어, 텐서는 4차원으로 설정할 수 있으며, 텐서에서 각 매개변수의 위치는 좌표(x, y, z, t)로 표시할 수 있으며, 여기서 x, y, z, 3개 차원은 상위 컨테이너에 표시된 각 신경조직(예를 들어,뉴런 또는 시냅스 등) 모델의 공간적 배열에 대응되며, t는 시간 차원을 나타내며, 타이밍 정보의 캐시 및 지연을 특성화할 수 있으며, 신경조절제가 뉴런 및 시냅스에 대한 장기 작용 메커니즘(지연성을 가짐)을 시뮬레이션하는 데 사용할 수 있으며; 또한, 텐서의 매 개변수는 상위 컨테이너에 있는 신경 조직(예를 들어, 뉴런 또는 시냅스) 모델의 전체 또는 일부에서 공유할 수 있으며, 타겟 영역에 있는 모든 신경 조직에 대한 신경조절제의 대면적의 작용을 시뮬레이션하는 데 사용할 수 있다. 상기 각 컨테이너에는 다단계 트리 구조에서 인덱싱에 사용되는 번호와 이름을 가진다. 각 컨테이너에는 네트워 크의 에르고딕 순서 및 규칙, 에르고딕 연산 참가 횟수, 데이터가 메인 메모리에 입주 여부, 데이터가 프로세서 메모리에 입주여부, 하드 디스크의 읽기 및 쓰기 빈도 등을 포함하는 정보의 통계 및 제어 데이터를 저장하는 데 사용되는 1개 또는 그 이상의 제어블록(Control Block)가지며, 룰 매니저 및 스케줄러에 의해 관리되고 업데 이트된다. Flow 및 모든 하위 컨테이너는 1개 또는 그 이상의 업스트림 컨테이너 및 1개 또는 그 이상의 다운스트림 컨테 이너에 대응될 수 있으며, 업스트림 및 다운스트림 컨테이너의 번호 또는 이름을 통해 인덱싱하고 액세스할 수 있다. 업스트림 컨테이너와 다운스트림 컨테이너는 모두 임의 레이어에 있는 컨테이너일 수 있으며, 양자는 동 일하거나 다른 컨테이너일 수 있다. 이런 이유로, Flow 및 모든 하위 컨테이너는 업스트림 및 다운스트림 컨테 이너와 함께 정보 흐름 경로를 형성할 수 있으며, 두 정보 소스(예를 들어 업스트림 및 다운스트림 컨테이너) 간의 (단방향 또는 양방향) 흐름 및 정보 처리 프로세스를 특성화한다. 네트워크 상에 있는 다수 컨테이너를 통 해 정보 흐름의 임의 토폴로지컬 구조를 구성할 수 있다. 상기 정보 흐름 및 처리 과정은 신경 스파이킹이 시냅스를 통해 뉴런 사이에서 전도되고, 시냅스와 시냅스 사이 의 정보 교환, 뉴런과 시냅스 가소성과 같은 다양한 생물학적 뇌 신경 메커니즘을 구현하는 데 적용할 수 있다. 상기 정보 흐름의 임의 토폴로지컬 구조는 뇌신경계에 있는 임의 신경 회로의 연결방식을 구현하는 데 적용할 수 있으며, 동일한 뉴런을 자체의 피드백 연결, 동일 그룹(레이어)의 뉴런 상호간 연결, 상이한 그룹(레이어)의 뉴런간 임의 연결(순차/피드포워드, 교차 레이어, 피드백 등), 및 시냅스와 시냅스 간의 직접 연결 등에 대한 지원도 포함하며, 피드백형 연결의 횟수 제한이 없는 순환 연산을 허용한다. 다음과 같이 구체적인 예를 들어 설명한다: Edge 1개를 1개 또는 그 이상의 시냅스에 적용하고, 대응된 업스트림 및 다운스트림 컨테이너 모두가 1개 또는 그 이상의 Node를 표현할 경우: 1. 만약 해당 Edge의 업스트림 및 다운스트림 컨테이너가 상이한 노드인 경우, 이들 간의 토폴로지 관계(예: Node1->Edge->Node 2)를 상이한 그룹(층)에 있는 뉴런 사이에서 시냅스를 통한 포워드/피드포워드 연결을 구현 하는 데 사용할 수 있으며; 2. 만약 해당 Edge의 업스트림 및 다운스트림 컨테이너가 동일한 Node인 경우, 이들 간의 토폴로지 관계(예: Node 1->Edge->Node1)를 적용하여 동일한 그룹(층)에 있는 뉴런 사이의 시냅스를 통한 상호 연결을 구현하는 데 사용할 수 있으며, 뉴런이 자가 시냅스(autapse)를 통해 자체의 피드백 연결에 다시 연결하는 데에도 사용할 수 있으며; 3. 만약 해당 Edge의 업스트림 및 다운스트림 컨테이너가 상이한 Layer의 Node로부터 전달된 경우, 이들 간의 토폴로지 관계는 상이한 뉴런 사이에서 시냅스를 통해 레이어 사이의 연결을 구현하는 데 적용할 수 있다. 일부 Edge가 1개 또는 그 이상의 시냅스를 표현하는 데 사용되고, 일부 Node가 1개 또는 그 이상의 뉴런을 각각 표현하는 데 사용되고, 이들이 구성한 토폴로지 관계가 Node 1->Edge 1->Node N->Edge N->Node 1과 같을 경우: 1. 각 Node가 상이한 Layer에 속해 있을 경우, 해당 토폴로지 관계는 상이한 레이어에 있는 뉴런 사이에서 피드 포워드 연결, 층간 연결 및 피드백 연결을 통해 신경회로를 구성하는 데 사용할 수 있으며; 2. 각 Node가 동일한 Layer에 속해 있을 경우, 해당 토폴로지 관계는 1개 또는 그 이상(또는 1개 또는 그 이상 의 그룹)의 상이한 뉴런으로 구성된 피드백 회로를 구현하는 데 사용할 수 있다.앞의 예에서 Edge에 있는 시냅스는 업스트림 및 다운스트림 컨테이너에 있는 뉴런에 액세스하여 해당 시냅스가 발화하는 타이밍 정보를 얻고, 자체 매개변수(예를 들어 가중치)와 연결시켜 계산을 수행하고 해당 계산 결과를 업스트림 및 다운스트림 컨테이너에 있는 뉴런으로 전달하여, 신경 스파이킹이 시냅스를 통해 뉴런 사이에서 전 도되고, Hebbian, Anti-Hebbian 및 STDP와 같은 장/단기 가소성 메커니즘을 구현할 수 있으며; Node에 있는 뉴 런은 수신한(신경전달물질 또는 신경조절제을 통해) 정보에 의해 기능 변경 또는 성형(뉴런 가소성의 일종)을 발생한다. Edge 1개를 하나 또는 그 이상의 시냅스를 특성화하는 데 사용하고, 대응된 업스트림 및 다운스트림 컨테이너 중 적어도 하나가 1개 또는 그 이상의 시냅스를 특성화하는 Edge인 경우, 이들 사이의 토폴로지 관계(예를 들어, Edge1->Edge2 ->Edge3)는 시냅스와 시냅스 사이의 직접 연결 관계 및 직접적인 정보 교환을 구현하는 데 사용할 수 있다. 상기 매개변수 데이터베이스는 네트워크의 다양한 매개변수 데이터(초기화 매개변수 및 실행 시 매개변수 포 함)를 저장하는 데 사용된다. 파라미터 데이터베이스는 바이너리 파일 또는 텍스트 파일로 선택될 수 있다. 텍 스트 파일은 CSV 파일 형식 또는 기타 문자 구분 파일 형식을 사용할 수 있다. 각 컨테이너에는 1개 또는 그 이 상의 대응된 매개변수 데이터베이스가 있을 수 있다. 예를 들어, NodeParam, EdgeParam 또는 Param에 수용된 매 개변수는 1개 또는 그 이상의 매개변수 데이터베이스에 저장하거나, 다수 NodeParam, EdgeParam 또는 Param이 1 개 또는 그 이상의 매개변수 데이터베이스를 공유하여 동일한 매개변수를 저장할 수 있다. 사용자는 네트워크에 있는 각 컨테이너의 매개변수 데이터베이스를 모델 파일 경로 상에 있는 해당된 하위 폴더에 배정할 수 있다. 본 시스템이 지원하는 모델링 설계 방식은 임의 레이어(또는 척도)의 모델을 데이터(data)와 조작(operation)의 2개 부분으로 분해할 수 있다. 위에서 기재한 바와 같이, 데이터는 NodeParam, EdgeParam 또는 Param에서 수용 할 수 있으며 대응된 매개변수 데이터베이스에 저장할 수 있다. 조작은 위에서 기재한 데이터에 액세스하고 업 데이트할 수 있는 실행 가능한 프로그램(예: 함수 및 함수를 포함하는 클래스)이다. 예를 들어, 뉴런 모델링은 기존 뉴런 모델을 사용하여, ReLU 활성화 함수를 작동으로 설계하고, 해당 역치 매개 변수를 데이터로 설계할 수 있으며; 뉴런의 모델링은 스파이킹 뉴런 모델을 적용하여 대응된 leaky integrate- and-fire model의 함수를 작동으로 설계하며, 해당 매개변수를 데이터로 설계한다. 또한,사용자는 1개 또는 그 이상의 조작을 정의하여, 동일한 Node 상의 각 뉴런(Edge를 거칠 필요 없이)이 서로 의 데이터에 직접 액세스하고 업데이트하여 신속한 정보 교환을 구현할 수 있어, 생물학적 뇌신경계에서 전기적 시냅스를 시뮬레이션하는 데 적용된다. 또한, 사용자는 1개 또는 그 이상의 조작을 정의하여, 동일한 Edge에 있는 각 시냅스가 서로의 데이터에 직접 액세스하고 업데이트할 수 있게 하여, 정보의 빠른 교환을 구현하며, 션팅 억제(shunting inhibition)와 같은 메커니즘을 포함한 생물학적 뇌신경계에 있는 동일한 뉴런의 수상돌기 상의 다수 시냅스 사이에서 정보를 상호 교환하고, 로직 연산을 진행하는 상황을 시뮬레이션하는 데 사용된다."}
{"patent_id": "10-2022-7003194", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "상기 내용을 요약해 보면, 시스템은 유연하고 통합적인 모델링 방법을 제공하며, 제공되는 다단계 트리 구조를 가진 네트워크에 대한 설명 방식은 생물학적 뇌신경계의 전체 척도에 대한 모델링과 유연한 네트워크 토폴로지 컬 구조를 지원하고 있어, 모델링 척도와 다양성을 유기적으로 통합시켜, 각 척도의 모든 모델을 통합된 신경망 으로 융합하여 운영한다. 또한 데이터는 텐서 형식으로 표현 및 저장되고 있어, 본 시스템은 스파이킹 신경망을 지원할 수 있고, 기존 신경망(딥 러닝) 및 텐서를 주요 데이터 표현 방법으로 사용하는 기타 알고리즘과도 호환 가능하다. 상기 운영 매니저는 본 시스템에서 실행할 수 있는 모든 조작을 관리하는 데 사용된다. 조작은 범용 CPU, ARM, DSP, GPU 또는 기타 프로세서에서 실행할 수 있는 프로그램(코드 세그먼트, 함수 및 클래스 포함)일 수 있 다. 모든 조작은 운영 라이브러리를 구성한다. 운영 매니저는 조작 번호 또는 명칭에 기반하여 지정된 조작 을 조회하고 호출하기 위한 프로그래밍 인터페이스를 제공한다. 사용자는 모델 설명 모듈에서 각 컨테이너를 위 해 수행할 조작을 지정해 주며, 실행 시, 스케줄러를 통해 해당 조작을 스케줄링하고 실행한다. 이를 통해 시스 템은 소정의 하드웨어 간 플랫폼 범용성을 확보하여,범용 CPU, GPU, ARM, DSP 등과 같은 하드웨어 플랫폼 상에 서 실행할 수 있다. 상기 구성 설명 모듈은 현재 네트워크 운영 환경의 구성 매개변수를 설명하는 데 적용된다. 예를 들어, 시스 템에서 사용할 수 있는 메모리 풀의 크기, 실행 방식(1차적, 다차적, 연속 실행), 하드디스크 데이터 읽기/쓰기 빈도의 상한 및 하한, 시냅스 및 뉴런의 프루닝 및 신생 프로세스 시동 조건 등이 있다. 상기 구성 매니저는 시스템 구성 매개변수를 얻기 위해 상기 구성 설명 모듈을 읽는 데 적용된다. 상기 네트워크 모델 객체는 네트워크 빌더에 의해 구성되고 메모리에 상주한다. 모든 컨테이너, 토폴로지 관 계 및 매개변수 데이터를 포함한 전체 네트워크를 특성화하며, 스케줄러에 의해 스케줄링하여 실행되는 객체이 다. 상기 룰 매니저는 모델 설명 모듈에 있는 사용자가 선언한 규칙을 읽고,스케줄러가 네트워크 모델 객체의 연산을 스케줄링할 때 이러한 규칙을 해석하는 데 사용된다. 사용자는 모델 설명 모듈에서 각 컨테이 너를 위해 1개 또는 그 이상의 규칙을 지정할 수 있다. 전부의 규칙을 통해 룰 베이스를 구성한다. 룰 매니저 는 룰 베이스에 있는 규칙을 사전 설정된 우선 순위에 따라 정렬하며, 1개 컨테이너에 작용하는 다수 규칙 상호간 충돌이 발생할 경우, 우선 순위가 가장 높은 규칙만 실행된다. 1개의 컨테이너에 그 어떤 규칙도 지정되 지 않은 경우, 룰 매니저는 기본 규칙을 적용하여 실행한다. 룰 베이스에 존재하는 규칙은 에르고딕 규칙, 메모리 사용 규칙, 데이터 I/O 규칙, 시냅스 및 뉴런 프루닝 및 신생 규칙 등이 포함(단, 이에 한정되지 않음)된다. 에르고딕 규칙은 스케줄러가 제2 사전 설정 시간 간격 또는 제4 사전설정 에르고딕 주기에 따라, 네트워크 상의 전부 또는 특정 컨테이너에 에르고딕을 중복하거나 건너 뛰 도록 스케줄러에 지시하여, 컴퓨팅 리소스를 밀집한 하위 네트워크를 연산하는 데 집중시켜, 데이터의 활용 효 율성 향상시키며; 메모리 사용 규칙은 스케줄러가 메인 메모리와 보조 프로세서 메모리의 사용을 합리적으로 배 정하도록 지시하는 데 사용할 수 있으며; 데이터 I/O 규칙은 스케줄러가 메인 메모리와 보조 프로세서 메모리 사이, 및 메모리와 하드 디스크 사이에서 진행된 데이터 교환 빈도를 스케줄링 하도록 안내하여 I/O 리소스를 절감하고 전체 컴퓨팅을 향상시키는 데 사용할 수 있다. 상기 데이터 매니저는 1개 또는 그 이상의 디코더와 인코더를 포함한다. 디코더는 사용자 지정 형식의 데이 터 파일을 읽고 분석하고 해당 내용을 컴퓨터에서 계산할 수 있는 데이터 형식으로 변환하는 데 사용된다. 인코 더는 하드 디스크에 다시 쓰기 위해 사용자가 지정한 형식으로 메모리에 있는 데이터를 직렬화하는 데 사용된다. 데이터 파일의 파일 형식은 바이너리 파일 또는 텍스트 파일(Unicode 또는 ASCII 형식)일 수 있다. 사용자는 사용자 정의를 통해 디코더와 인코더를 데이터 매니저에 추가하여, 사용자 정의 형식의 파일을 읽 고 쓸 수 있다. 상기 네트워크 빌더는 모델 설명 모듈을 읽고, 네트워크의 토폴로지컬 구조를 분석하고, 데이터 매니저 를 통해 데이터 파일을 읽고, 메모리에 네트워크 모델 객체를 구축하는 데 사용된다. 상기한 네트워크 매니저는 네트워크 모델 객체를 구성하기 위한 프로그래밍 인터페이스를 제공하고, 해당 인터페이스는 네트워크 모델 객체를 구성하기 위해 네트워크 빌더를 호출한다. 네트워크 매니저는 임의 컨테이너, 뉴런, 시냅스, 매개변수 등을 번호 또는 이름으로 조회 및 업데이트 하는 지원을 포함하여, 네트워크 모델 객체에 액세스하고, 에르고딕 및 실행하기 위한 프로그래밍 인터페이스도 제공한다. 지원되는 에르고딕 순 서는 다음(단 이에 한정되지 않음)을 포함한다: 1. 깊이 우선 에르고딕; 2. 너비 우선 에르고딕; 3. 모델 설명 모듈이 지정한 규칙을 따르는 에르고딕. 에르고딕을 구현하는 방식은 다음을 포함(단 이에 한정되지 않음)할 수 있다: 1. 순환식 에르고딕; 2. 재귀식 에르고딕. 상기 스케줄러는 하드웨어 리소스를 배정하고, 연산 프로세스를 스케줄링하여, 연산 효율성의 최적화를 보 장한다. 스케줄러는 1개 또는 그 이상의 메인 메모리 풀, 및, 1개 또는 그 이상의 장치 메모리 풀을 관리하 여, 메인 메모리 및 각 장치의 메모리에서 네트워크 모델 객체를 사용하는 상황을 합리적으로 배정한다. 메인 메모리 풀은 메인 메모리의 사용을 관리하는 데 사용되며; 각 보조 프로세서(ARM, GPU, DSPASIC일 수 있음)에는 1개 또는 그 이상의 장치 메모리 풀이 대응되어, 해당 장치 메모리의 사용을 관리하는 데 사용된다. 대응된 용 량 상한과 하한은 사용자가 구성 설명 모듈을 통해 지정한다. 상기 스케줄러는 다중 스레드 작업에 참여하도록 하위 스레드를 동적으로 배열하기 위한 1개 또는 그 이상 의 스레드 풀을 관리하여, 메인 컴퓨팅 유닛(CPU, ARM 등일 수 있음) 및 보조 프로세서(ARM, GPU, DSP 등일 수있음) 및 I/O 장치(하드디스크, 카메라, 오디오 입력, 제어 출력 등)의 연산 부하를 적절하게 배정한다. 상기 스케줄러는 1개 또는 그 이상의 노드 데이터 입력 버퍼, 1개 또는 그 이상의 노드 데이터 출력 버퍼, 1개 또는 그 이상의 에지 데이터 입력 버퍼, 1개 또는 그 이상의 에지 데이터 출력 버퍼를 관리하여, 하드 디스 크 또는 I/O 장치를 읽고 쓰는 데 필요한 데이터를 캐싱하는 데 사용된다. 이들은 우선적으로 환형 대열 데이터 구조를 채택한다. 상기 각 버퍼의 용량, 하드 디스크 또는 I/O 장치의 읽기 쓰기 빈도의 상한 및 하한, 하드 디 스크 또는 I/O 장치의 읽기 쓰기 처리량의 상한 및 하한은 사용자가 구성 설명 모듈을 통해 지정한다. 스케줄러 는 I/O의 막힘을 피하기 위해 프로세서, 하드 디스크 및 I/O 장치의 부하에 따라 적시에 하드 디스크 및 I/O 장치의 읽기/쓰기를 배정한다. 프로세서 및 보조 프로세서, 메모리, 하드 디스크 및 장치와 같은 하드웨어 자원의 사용을 합리적으로 할당 하기 위해 스케줄러를 사용하여, 본 시스템이 하드웨어 자원(예를 들어 메모리)이 상대적으로 제한된 임베 디드 장비 상에서 고효율적으로 실행할 수 있게 한다. 본 시스템은 소정의 트리거 조건 및 실행 규칙에 따라 자동으로 시냅스와 뉴런의 프루닝 및 신생을 실행하는 기 능을 제공하고 있다. 사용자는 구성 설명 모듈에서 프루닝 또는 신생 프로세스를 시동하기 위한 트리거 조건 을 지정하고, 모델 설명 모듈에서 프루닝 또는 신생 프로세스에 대한 실행 규칙을 지정할 수 있다. 실행 규 칙은 전체 네트워크 모델 객체 또는 하위 네트워크 또는 특정 컨테이너에 작용할 수 있다. 프루닝 및 신생 과정 은 스케줄러에 의해 스케줄링하여 실행되며, 네트워크가 실행 중이거나 일시 중지될 때에도 수행할 수 있다. 상기 프루닝 또는 신생 프로세스를 시동하는 트리거 조건은 다음 중 하나 또는 그 이상을 포함(단, 이에 한정되 지 않음)할 수 있다. 1.사용자 명령, 즉 사용자가 키보드, 마우스 또는 기타 방식을 통해 본 시스템에 명령을 입력하고 본 시스템은 명령을 수신한 직후 또는 제1 사전 설정된 시간 후에 프루닝 또는 신생 프로세스를 실행하며; 2.지속 실행, 즉 네트워크 모델 또는 하위 영역이 프루닝 또는 신생 프로세스의 규칙을 충족하면, 프루닝 또는 신생 프로세스를 실행하며; 3.간격 실행, 즉 시스템은 제1 사전 설정 시간 간격 또는 제1 사전 설정 에르고딕 주기에 따라 적시에 프루닝 또는 신생 프로세스를 자동으로 시동한다. 상기 프루닝 프로세스의 실행 규칙은 시냅스 프루닝 규칙과 뉴런 프루닝 규칙으로 구분된다. 시냅스 프루닝 규 칙은 다음 중 하나 또는 그 이상을 포함(단, 이에 한정되지 않음)할 수 있다; 1.특정 시냅스 매개변수 및 지정된 참조 시냅스 집합의 모든 시냅스 매개변수의 통계치가 제1 사전 설정값 관계 에 도달하면(예를 들어, 특정 시냅스의 가중치가 지정된 에지의 모든 시냅스 평균 가중치의 1% 미만인 경우), 해당 시냅스는 프루닝 대기 중인 시냅스이며; 2.특정 시냅스의 매개변수와 지정된 역치가 제2사전 설정 값 관계에 도달하면(예를 들어 시냅스의 가중치가 10.0미만의 경우), 해당 시냅스는 프루닝 대기 중인 시냅스이며; 3.특정 시냅스가 제2 사전 설정 시간 또는 제2 사전 설정 에르고딕 주기가 지난 후 트리거 되지 않으면, 해당 시냅스는 프루닝 대기 시냅스이며; 4.특정 시냅스가 다른 연산 과정에서 프루닝 가능함으로 표시되면 해당 시냅스는 프루닝 대기 시냅스이고; 프루 닝 대기 시냅스는 프루닝이 가능하다. 뉴런 프루닝 규칙은 다음 중 하나 또는 그 이상을 포함(단, 이에 한정되지 않음)할 수 있다; 1.특정 뉴런에 입력된 시냅스가 없는 경우, 해당 뉴런은 프루닝할 뉴런이며; 2.특정 뉴런에 출력한 시냅스가 없는 경우, 뉴런은 프루닝할 뉴런이며; 3. 특정 뉴런에 입력 및 출력한 시냅스가 없는 경우, 뉴런은 프루닝 대기 뉴런이며; 4.특정 뉴런의 매개변수와 지정된 참조 뉴런 집합에 있는 모든 뉴런 매개변수에 대한 통계값이 제3사전 설정 값 관계에 도달하면(특정 뉴런의 역치가 지정된 노드의 모든 뉴런 역치의 최대값보다 큰 경우 ), 해당 뉴런은 프루 닝 대기 뉴런이며;5.특정 뉴런의 매개변수와 지정된 역치가 제4 사전 설정 값 관계에 도달하면(예를 들어,특정 뉴런의 역치가 1000.0보다 큰 경우), 해당 뉴런은 프루닝 대기 뉴런이며; 6.특정 뉴런이 제3 사전 설정된 시간 또는 제3 사전 설정 에르고딕 주기를 초과한 후에도 발사 되지 않으면, 해 당 뉴런은 프루닝 대기 뉴런이며; 7.특정 뉴런이 기타 연산 프로세스에 의해 프루닝 가능한 것으로 표시되면, 해당 뉴런은 프루닝 대기 뉴런이며; 프루닝 대기 뉴런은 프루닝 작업이 가능하다. 해당 신생 프로세스의 수행 규칙은 뉴런 신생 규칙과 시냅스 신생 규칙으로 구분된다. 뉴런 신생 규칙은 다음 중 하나 또는 그 이상을 포함(단, 이에 한정되지 않음)할 수 있다: 1.특정 노드 컨테이너에 있는 기존 뉴런 수와 해당 노드 컨테이너의 총 용량이 제1 사전 설정 비율 또는 제5 사 전 설정 값 관계에 도달하면, 총 용량에 대한 제2 사전 설정 비율 또는 제1 사전 설정 수량에 따라 뉴런을 새로 생성하며; 이 중, 제1 사전 설정 비율과 제2 설정 비율은 동일하거나 상이할 수 있으며; 2.특정 노드 컨테이너는 제1 사전 설정 비율에 따라(즉, 사전 설정된 시간 간격 또는 사전 설정된 에르고딕 주 기에 따라) 총 용량의 미리 설정된 제3 비율 또는 제2 사전 설정 수량에 따라 새로운 뉴런을 생성하며; 특정 노드 컨테이너는 다른 연산 프로세스에 의해 신생 뉴런이 필요한 것으로 표시되고, 제2 사전설정 속도에 따라(즉, 사전 설정 시간 간격 또는 사전 설정 에르고딕 주기별로 총량에 대한 사전 설정 비율 또는 수량) 뉴런 을 새로 생성한다. 시냅스 신생 규칙은 다음 중 하나 또는 그 이상을 포함(단, 이에 한정되지 않음)할 수 있다: 1.특정 에지 컨테이너의 기존 시냅스 수와 해당 에지 컨테이너의 총 용량이 제4 사전 설정 비율 또는 제6 사전 설정 값 관계에 도달하면, 해당 총 용량의 제5 사전 설정 비율 또는 제3 사전 설정 수량에 따라 시냅스를 새로 생성하며; 이 중, 제4 사전설정 비율과 제5 사전 설정 비율은 같거나 다를 수 있으며; 2.특정 에지 컨테이너는 제3 사전설정 속도(즉, 사전 설정 시간 간격 또는 사전 설정 에르고딕 주기별로 총량에 대한 사전 설정 비율 또는 수량)에 따라 새로운 시냅스를 생성하며; 3.특정 에지 컨테이너는 다른 연산 프로세스에 의해 필요한 신생 시냅스로 표시되고, 제4 사전 설정 속도에 따 라(즉, 사전 설정 시간 간격 또는 사전 설정 에르고딕 주기별로 총량에 대한 사전 설정 비율 또는 수량) 새로운 시냅스를 생성하며; 4.특정 노드 컨테이너에 시냅스 입력 또는 출력이 없는 뉴런이 존재할 경우, 대응된 각 에지 컨테이너에서 각각 대응되게 입력 시냅스 또는 출력 시냅스를 새로 생성한다. 상기 스케줄러는 시냅스와 뉴런 프루닝 및 신생의 실행을 스케줄링하는 역할을 한다. 시냅스와 뉴런 프루닝 또 는 신생을 시동하는 조건이 트리거되면 스케줄러는 관리하는 스레드 풀에서 1개 또는 그 이상의 하위 스레드를 배정하며, 각 하위 스레드는 네트워크 모델 객체 중의 일부 영역 또는 특정 컨테이너를 담당한다. 하위 스레드 는 관할 영역의 각 컨테이너를 에르고딕 하고 지정된 규칙에 따라 뉴런 및/또는 시냅스에 대한 프루닝 및/또는 신생 프로세스를 실행한다. 뉴런 또는 시냅스의 재생 과정은 컨테이너에 필요한 메모리 공간을 할당하고 해당 객체(new/construct object) 를 생성하는 것일 수 있으며; 뉴런 또는 시냅스의 프루닝 과정은 컨테이너에서 해당 객체를 분해 (delete/destruct object)하고 점유된 메모리 공간을 방출하는 과정일 수 있다. 본 시스템은 특정 조건과 규칙에 따라 시냅스 및 뉴런 프루닝 및 신생 기능을 자동으로 수행하며, 프루닝 및 신생 프로세스를 원활하게 시동하기 위한 다양한 트리거 조건 및 실행 프로세스 규칙을 제공하여, 신 경망 개발자가 자체로 시냅스 및 뉴런의 프루닝, 신생 프로그램을 작성해야 하는 부담을 줄여 주어, 개발의 유 연성과 효율성을 개선하였다. 시냅스 및 뉴런의 프루닝 프로세스는 신생 프로세스와 교차적으로 사용할 수 있으 므로 신경망의 인코딩 효율성을 최적화하고 신경망의 사이즈와 필요한 저장 공간을 대폭 압축하여 메모리를 절 감과 함께 컴퓨팅 효율성을 향상시켜, 본 시스템이 하드웨어 리소스가 제한된 임베디드 장치에서 실행하기에 적 합해 지도록 한다. 본 시스템은 시냅스 및 뉴런의 프루닝 및 신생을 지원해주어, 생물학적 뇌신경계(예를 들어, 해마 시냅스 및 뉴런 자멸사 및 신생 현상)의 다양한 메커니즘을 시뮬레이션하는 데 도움이 되며, 뇌와 유사한 지능 및 인지 컴퓨팅을 보다 충분히 지원할 수 있다.상기 로그 매니저는 시스템 실행 시 생성된 로그를 기록하고, 사용자에게 시스템의 작업 상태 및 이상상황을 제시하여, 용이한 디버깅 및 유지 관리를 위해 적용된다. 로그는 일련의 문자열과 타임스탬프로 구성되며 커맨 드 라인 환경에 표시하거나 파일에 저장하여 텍스트 브라우저로 표시할 수 있다. 로그 매니저는 로그 기록 프로 그래밍 인터페이스와 로그 관리 서비스로 구성된다. 로그 기록 프로그래밍 인터페이스는 프로그램에서 사용자에 의해 호출되고 로그 데이터는 로그 관리 서비스로 전송된다. 로그 관리 서비스는 네트워크 연산이 막히는 것을 방지하기 위해 독립적인 스레드에 의해 실행되며, 수신된 로그 데이터를 타임스탬프에 따라 통합 정렬하여 메모 리에 캐싱하며, 캐시 데이터 수량이 소정 정도에 도달하면, 특정 순서에 따라 하드 디스크에 저장하고, 캐시를 삭제한다. 상기 실행 모니터링 모듈은 사용자 입력을 수신 및 응답하고, 전체 시스템의 실행 상태를 관리한다. 상기 모듈은 기본 상태, 네트워크 구성 상태, 네트워크 작동 상태 및 네트워크 중단 상태를 포함한 상태 머신 디자인 방식을 채택한다. 여기에 사용자가 입력한 명령을 수신하고 캐싱하는 메시지 대열 하나를 포함하며, 제시간에 대열에 있는 명령에 응답하기 위한 독립 스레드 하나를 포함하고 있어, 상태 머신이 상이한 상태 사이에서 전환 될 수 있게 한다. 사용자는 키보드, 마우스, 프로그래밍 인터페이스 또는 기타 방식을 통해 명령을 입력할 수 있다. 명령에는 네트워크 구축 명령, 실행 시작 명령, 실행 일시 중지 명령, 실행 종료 명령, 시냅스 및 뉴런 프루닝 명령, 시냅스 및 뉴런 신생 명령이 포함되며, 단, 이에 한정되지 않는다. 첨부된 도1과 연결시켜, 다음과 같이 시스템 실행 원리에 대하여 간략하여 설명한다. S1: 시스템 시동 및 운영 환경 초기화 진행; S2: 운영 환경이 기본 상태로 진입; S3: 구성 매니저를 통해 구성 설명 모듈을 읽어 구성 매개변수 취득; S4: 명령 입력 대기; S5: 네트워크 구축 명령의 수신 여부를 결과가 “예”일 때까지 판단하고 다음 단계로 진입; S6: 네트워크 구축 명령을 수신하면 운영 환경을 네트워크 구축 상태로 전환; S7: 네트워크 매니저 및 룰 매니저 초기화; S8: 네트워크 빌더를 통해 모델 설명 모듈을 읽고 네트워크 모델 객체를 구성하며, 데이터 매니저를 통해 매개 변수 데이터베이스를 읽음; S9: 명령 입력 대기; S10: 시작 명령이 수신되었는지 여부를 판단하여, 판단 결과가 “아니오”이면, 단계 S9로 돌아가서 명령 입력 을 다시 대기하며, 판단 결과가 “예”인 경우 다음 단계로 이동; S11: 운영 환경이 네트워크 운영 상태로 진입; S12: 스케줄링 실행; S13: 작업 중단 명령이 수신되었는지 여부를 판단하여, 판단 결과가 “예”이면 단계 S14로 이동하고, 판단 결 과가 “아니오”이면 단계 S17로 이동; S14: 운영 환경은 네트워크 일시 중지 상태 진입; S15: 명령 입력 대기; S16: 실행 시작 명령이 수신되었는지 여부를 판단하여, 판단 결과가 “예”이면 단계 S11로 돌아가고, 판단 결 과가 “아니오”이면 단계 S15로 돌아감; S17: 지정된 정지 조건에 도달했는지(동작 종료 명령 수신 등 포함) 여부를 판단하여, 판단 결과가 “아니오” 이면 단계 S12로 돌아가고, 판단 결과가 “예”이면, 동작을 종료한다. 시스템이 초기화되면, 상기 상태 머신은 기본 상태에 있고 메시지 대열을 시동하여 사용자 입력을 받기 시작하 며; 네트워크 구성 명령이 수신되면 상태 머신은 네트워크 구성 상태로 전환되고, 네트워크 모델 객체를 구축하 며; 실행 시작 명령이 수신되면, 상태 머신은 네트워크 실행 상태로 전환되고, 네트워크 연산을 수행하며; 실행 일시 중지 명령을 수신하면, 상태 머신은 네트워크 일시 중지 상태로 전환하고 네트워크 연산을 일시 중지하며;실행 종료 명령이 수신되면, 상태 머신은 네트워크 데이터를 하드 디스크에 저장한 후, 시스템은 종료 및 탈퇴 한다. 상태 머신이 네트워크 실행 상태 또는 네트워크 일시 중지 상태일 때, 메시지 대열에 시냅스 및 뉴런 프 루닝 명령이 있으면, 스케줄러는 시냅스 및 뉴런 프루닝 프로세스를 시동하며; 메시지 대열에 시냅스 및 뉴런 신생 명령이 있으면, 스케줄러를 통해 시냅스 및 뉴런 신생 프로세스를 시동한다. 본 시스템은 실행 모니터링 모듈을 사용하여 시스템의 작동 상태를 관리하기 때문에 전력 소모를 줄이고 본 시스템을 임베디드 시스템에 적 용하기 위해, 응용 환경에서 네트워크 동작이 필요하지 않은 경우 시스템을 네트워크 일시 중지 상태로 전환할 수 있다. 상기 그래픽 디스플레이 모듈은 네트워크 데이터를 읽고 이를 사용자에게 표시하여 용이한 개발, 모니터링 및 디버깅을 위해 적용된다. 이미지 표시 모듈은 메모리에 있는 네트워크 모델 객체의 데이터를 직접 읽거나 하드 디스크에 저장된 데이터를 읽을 수 있다. 그래픽 표시 모듈은 독립 스레드를 채택하여 네트워크 연산을 차단하기에, 네트워크 스케줄링 실행 중에 실시간으로 표시될 수 있으며, 네트워크 스케줄링 실행이 종료된 후 에 표시될 수도 있다. 여기서 설명해야 부분이라면, 상기 제1 사전설정 시간 내지 제3 사전설정 시간, 제1 사전설정 시간 간격 내지 제2 사전설정 시간 간격, 제1 사전설정 에르고딕 주기 내지 제4 사전설정 에르고딕 주기, 제1 사전설정 값 관계 내지 제6 사전설정 값 관계, 제1 사전설정 비율 내지 제5 사전설정 비율, 제1 사전설정 수량 내지 제3 사전설정 수량, 및 제1 사전설정 속도 내지 제4 사전설정 속도 등의 표현은, 사전설정 시간, 사전설정 시간 간격, 사전 설정 에르고딕 주기, 사전설정 값 관계, 사전설정 비율, 사전설정수 및 사전설정 속도 등을 쉽게 구분하기 위해 사용되며, 특정 값의 크기 또는 범위는 실제 필요에 따라 확정될 수 있으며, 본 발명의 실시예는 이를 제한하지 않는다. 또한, 상기 각 사전설정 시간, 사전설정 시간 간격, 사전설정 에르고딕 주기, 사전설정 값 관계, 사전 설정 비율, 사전설정수량 및 사전설정 속도 등의 크기는 동일하거나 상이할 수 있다. 예를 들어, 제1 설정 시간 내지 제3 사전설정 시간까지의 시간 길이는 완전히 같거나 다를 수 있으며; 또는, 이 중 부분적인 시간 길이가 같고 다른 부분적인 시간 길이는 다를 수 있다. 본 발명의 실시예는 이에 대하여 제한하지 않는다. 위의 내용은 본 발명의 선택적 실시예일 뿐이며, 본 발명을 한정하는 데 사용되지 않는다. 본 발명이 속하는 기 술분야에서 통상의 지식을 가진 자라면 본 발명에 대한 다양한 수정 및 변경이 가능하다. 본 발명의 요지와 원 칙을 벗어나지 않는 한, 이루어진 모든 수정, 동등한 교체 및 개선 등은 전부 본 발명의 특허청구범위에 포함되 어야 한다."}
{"patent_id": "10-2022-7003194", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 발명 실시예에 기재된 기술방안을 보다 분명하게 설명하기 위해, 다음과 같이 실시예 또는 예시적인 기술 설 명에 필요한 도면에 대하여 간단하게 설명하며, 하기 도면은 본 발명의 일부 실시예를 설명하기 위해 명시되며, 본 발명이 속하는 분야의 일반 기술자에 의해 창조적인 과정을 투입하지 않는 전제하에서 이와 같은 도면을 통 해 기타 관련 도면을 도출할 수도 있음은 자명하다. 도1은 본 발명을 통해 취급하는 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스 템의 전체적인 구조를 나타낸 개략도; 도2는 본 발명의 일 실시예를 통해 취급하는 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템의 네트워크 레이어를 나타낸 개략도; 도3은 본 발명의 일 실시예를 통해 취급하는 두뇌 모방 인텔리전트 및 인지 컴퓨팅에 적용하는 스파이킹 신경망 컴퓨팅 시스템의 실행 프로세스 흐름을 나타낸 개략도."}
