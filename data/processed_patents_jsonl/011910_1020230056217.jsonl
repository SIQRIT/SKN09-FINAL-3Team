{"patent_id": "10-2023-0056217", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0159742", "출원번호": "10-2023-0056217", "발명의 명칭": "사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법 및 장치", "출원인": "(주)디오비스튜디오", "발명자": "오제욱"}}
{"patent_id": "10-2023-0056217", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자 단말로부터 가상 화상 대화 서비스를 제공하는 페이지에 접속하는 단계;상기 사용자 단말에서 기 저장된 유명인과의 가상 화상 통화를 선택하는 경우, 상기 선택된 유명인의 실사 모델링 데이터, 음성 복제 데이터, 및 얼굴 표정 변화 데이터를 로딩하는 단계;상기 사용자 단말에서 음성이 입력된 경우, 상기 입력된 음성을 자연어 처리 알고리즘을 통하여 분석한 후, 상기 입력된 음성의 응답 텍스트와 얼굴 표정을 결정하는 단계; 및상기 결정된 응답 텍스트를 상기 얼굴 표정을 통하여 상기 실사 모델링 캐릭터가 발화하도록 제어하는 단계를 포함하는, 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법."}
{"patent_id": "10-2023-0056217", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 사용자 단말로부터 가상 화상 대화 서비스를 제공하는 페이지에 접속하는 단계 전,적어도 하나의 유명인을 촬영하여 실사 모델링을 실시하는 단계;상기 적어도 하나의 유명인이 발화한 음성 데이터 및 촬영 데이터를 음성 데이터가 텍스트 변환된 텍스트 데이터와 매핑하여 저장하는 단계; 및상기 실사 모델링이 수행된 실사 모델링 데이터에, 상기 음성 데이터, 촬영 데이터 및 텍스트 데이터를 적용하여 데이터베이스화하는 단계를 더 포함하는, 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법."}
{"patent_id": "10-2023-0056217", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 사용자 단말에서 음성이 입력된 경우, 상기 입력된 음성을 자연어 처리 알고리즘을 통하여 분석한 후, 상기 입력된 음성의 응답 텍스트와 얼굴 표정을 결정하는 단계는,상기 입력된 음성 신호로부터 감정을 인식하여 특징을 추출하고, 상기 추출된 특징을 이용하여 패턴을 인식하는단계를 포함하고,상기 감정인식의 특징은 피치, 에너지, 포만트, 및 말의 빠르기 중 어느 하나 또는 적어도 하나의 조합으로 추출되고, 상기 감정인식은 어쿠스틱 특징(acoustic feature) 중 피치의 통계치, 소리의 크기, 섹션 개수,IR(Increasing Rate), 및 CR(Crossing Rate) 중 어느 하나 또는 적어도 하나의 조합의 특징을 인공신경망에 적용하는, 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법."}
{"patent_id": "10-2023-0056217", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 사용자 단말에서 음성이 입력된 경우, 상기 입력된 음성을 자연어 처리 알고리즘을 통하여 분석한 후, 상기 입력된 음성의 응답 텍스트와 얼굴 표정을 결정하는 단계는,상기 사용자 단말의 얼굴 표정 데이터를 입력받는 경우, 상기 입력된 얼굴 표정 데이터인 다차원 특징 벡터 데이터를 이용하여 얼굴 표정 데이터로부터 감정을 인식하는 단계를 포함하고,공개특허 10-2024-0159742-3-상기 다차원 특징 벡터 데이터를 광학적 흐름 분석법, 홀리스틱 분석법, 및 국부적인 표현분석법 중 어느 하나또는 적어도 하나의 조합의 분석으로 감정을 인식하고,상기 홀리스틱 분석은 PCA 방법에 기반하여 특징을 추출하고 최소거리 분류 방법을 이용하여 감정을 인식하는방법인 것인, 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법."}
{"patent_id": "10-2023-0056217", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 결정된 응답 텍스트를 상기 얼굴 표정을 통하여 상기 실사 모델링 캐릭터가 발화하도록 제어하는 단계는,상기 결정된 얼굴 표정에 대응하는 정면 및 측면의 직교하는 복수의 2차원 또는 3차원의 기 저장된 얼굴 영상을추출하는 단계;상기 추출된 얼굴 영상에 포함된 적어도 하나의 특징점을 추출하여 상기 실사 모델링 캐릭터의 얼굴을 변형하는단계; 및상기 변형된 얼굴을 가진 실사 모델링 캐릭터의 위치, 크기, 표정 및 회전 정보를 반영하여 합성하는 단계를 포함하는, 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법."}
{"patent_id": "10-2023-0056217", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 결정된 응답 텍스트를 상기 얼굴 표정을 통하여 상기 실사 모델링 캐릭터가 발화하도록 제어하는 단계는,상기 결정된 얼굴 표정에 대응하는 표정을 메타데이터로 가진 실사 모델링 캐릭터를 검색하는 단계; 및상기 검색된 표정을 가진 실사 모델링 캐릭터가 상기 결정된 응답 텍스트를 발화하도록 합성하는 단계를 포함하는, 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법."}
{"patent_id": "10-2023-0056217", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법 및 장치가 개시된다. 본 발명의 일 실시예에 따른 사용자 응 대를 위한 맞춤형 가상 얼굴 서비스 방법은 사용자 단말로부터 가상 화상 대화 서비스를 제공하는 페이지에 접속 하는 단계, 상기 사용자 단말에서 기 저장된 유명인과의 가상 화상 통화를 선택하는 경우, 상기 선택된 유명인의 실사 모델링 데이터, 음성 복제 데이터, 및 얼굴 표정 변화 데이터를 로딩하는 단계, 상기 사용자 단말에서 음성 이 입력된 경우, 상기 입력된 음성을 자연어 처리 알고리즘을 통하여 분석한 후, 상기 입력된 음성의 응답 텍스 트와 얼굴 표정을 결정하는 단계 및 상기 결정된 응답 텍스트를 상기 얼굴 표정을 통하여 상기 실사 모델링 캐릭 터가 발화하도록 제어하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0056217", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0056217", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이하에 기술되는 내용은 단순히 본 발명에 따른 일 실시예와 관련되는 배경 정보만을 제공할 뿐 종래기술을 구 성하는 것은 아니다. 대화형 인공지능이란, 음성 혹은 문자를 통해 인간과의 대화를 시뮬레이션할 목적의 프로그램인데, 이러한 대화 형 인공지능은 지능 획득 방법 및 정보 교환 방식에 따라 Q&A 시스템, 지능형 검색, 말동무, 개인 비서 등의 종 류로 분류할 수 있다. 상용화된 대화형 인공지능들은 입력의 특정 단어나 어구를 검출하여 미리 준비된 응답을 출력하는데 초점이 맞춰져 있으며, 가장 많은 응용되는 비서형 인공지능은 최근 들어 스마트폰에 기본 사양으로 탑재되는 경우가 많다. 현재의 비서형 인공지능은 제조사에서 정한 캐릭터를 기반으로 하는 것이 대부분이다. 이 때, 대화형 인공지능은 가상 캐릭터가 가상공간에서 사용자와 커뮤니케이션하도록 하고, 사용자에게 맞는 대 화상대를 자동으로 선정하여 연결하는 방법으로 개발되었는데, 이와 관련하여, 선행기술인 한국공개특허 제 2007-0024918호(2007년03월08일 공개) 및 한국공개특허 제2018-0001155호(2018년01월04일 공개)에는 사용자의 선택에 따라 사용자 단말기에 사용자의 가상 캐릭터가 플레이하기 위한 시나리오 형태의 가상 공간을 제공하고, 사용자의 가상 캐릭터의 주변 부위에 인터페이스를 배치하고, 사용자의 화상을 해당 인터페이스에 제공하여, 다 수의 사용자간 실시간으로 화상 회의 방식으로 대화를 수행하는 구성, 온라인 채팅에서 사용자에게 맞는 대화상 대를 자동으로 선정해서 연결하는 빅데이터를 이용한 인공지능의 온라인 채팅 대화상대 자동맞춤 방법을 개시한 다. 다만, 대화형 에이전트는, 하나의 문장을 기본단위로 학습하고 처리하는 방식을 취하기 때문에 연속대화를 처리 할 수 없고, 대화 상황에 대한 이해보다는 정의된 기능 수행을 중심으로 진행되기 때문에 사전에 정의되지 않은질문에는 답변이 불가능하며, 인공지능 스스로의 감정을 생성하고 표현할 수 없다. 이에, 사람과의 깊은 소통을 위해서는 대화의 전후 상황을 이해하고, 이를 토대로 자신의 감정 상태를 변화시킬 줄 알며, 주관적인 감정과 의견을 드러낼 줄 아는 대화형 인공지능의 개발이 요구되고 있다."}
{"patent_id": "10-2023-0056217", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법을 제공하는 것을 목적으로 한다."}
{"patent_id": "10-2023-0056217", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명의 일 실시예에 따른 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법은 사용자 단말로부터 가상 화상 대화 서비스를 제공하는 페이지에 접속하는 단계, 상기 사용자 단말에서 기 저장 된 유명인과의 가상 화상 통화를 선택하는 경우, 상기 선택된 유명인의 실사 모델링 데이터, 음성 복제 데이터, 및 얼굴 표정 변화 데이터를 로딩하는 단계, 상기 사용자 단말에서 음성이 입력된 경우, 상기 입력된 음성을 자 연어 처리 알고리즘을 통하여 분석한 후, 상기 입력된 음성의 응답 텍스트와 얼굴 표정을 결정하는 단계 및 상 기 결정된 응답 텍스트를 상기 얼굴 표정을 통하여 상기 실사 모델링 캐릭터가 발화하도록 제어하는 단계를 포 함할 수 있다."}
{"patent_id": "10-2023-0056217", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법을 제공할 수 있다."}
{"patent_id": "10-2023-0056217", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명을 첨부된 도면을 참조하여 상세히 설명하면 다음과 같다. 여기서, 반복되는 설명, 본 발명의 요지를 불 필요하게 흐릴 수 있는 공지 기능, 및 구성에 대한 상세한 설명은 생략한다. 본 발명의 실시형태는 당 업계에서 평균적인 지식을 가진 자에게 본 발명을 보다 완전하게 설명하기 위해서 제공되는 것이다. 따라서, 도면에서의 요소들의 형상 및 크기 등은 보다 명확한 설명을 위해 과장될 수 있다. 명세서 전체에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성 요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 이하, 본 발명에 따른 바람직한 실시예를 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 본 발명의 일 실시예에 따른 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법을 나타낸 동작 흐름도이 다. 도 1을 참조하면, 본 발명의 일 실시예에 따른 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법은 먼저, 사용 자 단말로부터 가상 화상 대화 서비스를 제공하는 페이지에 접속할 수 있다(S110). 다음으로, 상기 사용자 단말에서 기 저장된 유명인과의 가상 화상 통화를 선택하는 경우, 상기 선택된 유명인의 실사 모델링 데이터, 음성 복제 데이터, 및 얼굴 표정 변화 데이터를 로딩할 수 있다(S120). 다음으로, 상기 사용자 단말에서 음성이 입력된 경우, 상기 입력된 음성을 자연어 처리 알고리즘을 통하여 분석 한 후, 상기 입력된 음성의 응답 텍스트와 얼굴 표정을 결정할 수 있다(S130). 다음으로, 상기 결정된 응답 텍스트를 상기 얼굴 표정을 통하여 상기 실사 모델링 캐릭터가 발화하도록 제어할 수 있다(S140). 일 실시예에 따르면, 상기 사용자 단말로부터 가상 화상 대화 서비스를 제공하는 페이지에 접속하는 단계 전, 적어도 하나의 유명인을 촬영하여 실사 모델링을 실시하는 단계, 상기 적어도 하나의 유명인이 발화한 음성 데 이터 및 촬영 데이터를 음성 데이터가 텍스트 변환된 텍스트 데이터와 매핑하여 저장하는 단계 및 상기 실사 모 델링이 수행된 실사 모델링 데이터에, 상기 음성 데이터, 촬영 데이터 및 텍스트 데이터를 적용하여 데이터베이스화하는 단계를 더 포함할 수 있다. 일 실시예에 따르면, 상기 사용자 단말에서 음성이 입력된 경우, 상기 입력된 음성을 자연어 처리 알고리즘을 통하여 분석한 후, 상기 입력된 음성의 응답 텍스트와 얼굴 표정을 결정하는 단계는 상기 입력된 음성 신호로부 터 감정을 인식하여 특징을 추출하고, 상기 추출된 특징을 이용하여 패턴을 인식하는 단계를 포함하고, 상기 감 정인식의 특징은 피치, 에너지, 포만트, 및 말의 빠르기 중 어느 하나 또는 적어도 하나의 조합으로 추출되고, 상기 감정인식은 어쿠스틱 특징(acoustic feature) 중 피치의 통계치, 소리의 크기, 섹션 개수, IR(Increasing Rate), 및 CR(Crossing Rate) 중 어느 하나 또는 적어도 하나의 조합의 특징을 인공신경망에 적용할 수 있다. 일 실시예에 따르면, 상기 사용자 단말에서 음성이 입력된 경우, 상기 입력된 음성을 자연어 처리 알고리즘을 통하여 분석한 후, 상기 입력된 음성의 응답 텍스트와 얼굴 표정을 결정하는 단계는 상기 사용자 단말의 얼굴 표정 데이터를 입력받는 경우, 상기 입력된 얼굴 표정 데이터인 다차원 특징 벡터 데이터를 이용하여 얼굴 표정 데이터로부터 감정을 인식하는 단계를 포함하고, 상기 다차원 특징 벡터 데이터를 광학적 흐름 분석법, 홀리스 틱 분석법, 및 국부적인 표현분석법 중 어느 하나 또는 적어도 하나의 조합의 분석으로 감정을 인식하고, 상기 홀리스틱 분석은 PCA 방법에 기반하여 특징을 추출하고 최소거리 분류 방법을 이용하여 감정을 인식할 수 있다. 일 실시예에 따르면, 상기 결정된 응답 텍스트를 상기 얼굴 표정을 통하여 상기 실사 모델링 캐릭터가 발화하도 록 제어하는 단계는 상기 결정된 얼굴 표정에 대응하는 정면 및 측면의 직교하는 복수의 2차원 또는 3차원의 기 저장된 얼굴 영상을 추출하는 단계, 상기 추출된 얼굴 영상에 포함된 적어도 하나의 특징점을 추출하여 상기 실 사 모델링 캐릭터의 얼굴을 변형하는 단계 및 상기 변형된 얼굴을 가진 실사 모델링 캐릭터의 위치, 크기, 표정 및 회전 정보를 반영하여 합성하는 단계를 포함할 수 있다. 일 실시예에 따르면, 상기 결정된 응답 텍스트를 상기 얼굴 표정을 통하여 상기 실사 모델링 캐릭터가 발화하도 록 제어하는 단계는 상기 결정된 얼굴 표정에 대응하는 표정을 메타데이터로 가진 실사 모델링 캐릭터를 검색하 는 단계 및 상기 검색된 표정을 가진 실사 모델링 캐릭터가 상기 결정된 응답 텍스트를 발화하도록 합성하는 단 계를 포함할 수 있다. 도 1은 본 발명의 일 실시예에 따른 가상현실 기반 대화형 인공지능을 이용한 화상 대화 서비스 제공 시스템을 설명하기 위한 도면이다. 도 1을 참조하면, 가상현실 기반 대화형 인공지능을 이용한 화상 대화 서비스 제공 시 스템은, 사용자 단말, 화상 대화 서비스 제공 서버, 및 적어도 하나의 실사 제공 서버를 포 함할 수 있다. 다만, 이러한 도 1의 가상현실 기반 대화형 인공지능을 이용한 화상 대화 서비스 제공 시스템 은, 본 발명의 일 실시예에 불과하므로, 도 1을 통하여 본 발명이 한정 해석되는 것은 아니다. 이때, 도 1의 각 구성요소들은 일반적으로 네트워크(network, 200)를 통해 연결된다. 예를 들어, 도 1에 도시된 바와 같이, 사용자 단말은 네트워크를 통하여 화상 대화 서비스 제공 서버와 연결될 수 있다. 그리고, 화상 대화 서비스 제공 서버는, 네트워크를 통하여 사용자 단말, 적어도 하나의 실사 제공 서버와 연결될 수 있다. 또한, 적어도 하나의 실사 제공 서버는, 네트워크를 통하여 화상 대화 서비스 제공 서버와 연결될 수 있다. 여기서, 네트워크는, 복수의 단말 및 서버들과 같은 각각의 노드 상호 간에 정보 교환이 가능한 연결 구조를 의 미하는 것으로, 이러한 네트워크의 일 예에는 RF, 3GPP(3rd Generation Partnership Project) 네트워크, LTE(Long Term Evolution) 네트워크, 5GPP(5rd Generation Partnership Project) 네트워크, WIMAX(World Interoperability for Microwave Access) 네트워크, 인터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), 블루투스 (Bluetooth) 네트워크, NFC 네트워크, 위성 방송 네트워크, 아날로그 방송 네트워크, DMB(Digital Multimedia Broadcasting) 네트워크 등이 포함되나 이에 한정되지는 않는다. 하기에서, 적어도 하나의 라는 용어는 단수 및 복수를 포함하는 용어로 정의되고, 적어도 하나의 라는 용어가 존재하지 않더라도 각 구성요소가 단수 또는 복수로 존재할 수 있고, 단수 또는 복수를 의미할 수 있음은 자명 하다 할 것이다. 또한, 각 구성요소가 단수 또는 복수로 구비되는 것은, 실시예에 따라 변경가능하다 할 것이다. 사용자 단말은, 가상현실 기반 대화형 인공지능을 이용한 화상 대화 서비스 관련 웹 페이지, 앱 페이지, 프로그램 또는 애플리케이션을 이용하여 유명인과 대화 또는 화상통화를 하고자 하는 사용자의 단말일 수 있다. 이때, 사용자 단말은, 유명인을 선택하고, 선택된 유명인과 화상 통화 또는 대화를 가상현실 또는 증강현 실 기반으로 진행하되, 사용자의 얼굴표정 및 음성신호를 수집하여 이에 대응하는 응답을 유명인이 대답하는 것과 같은 매커니즘을 구현할 수 있는 단말일 수 있다. 여기서, 사용자 단말의 네트워킹 자원 및 컴퓨팅 자 원이 기 설정된 자원을 만족하는 경우, 상술한 매커니즘이 사용자 단말에서 일어나고, 화상 대화 서비스 제공 서버에서 구현되지 않을 수 있고, 이 경우 화상 대화 서비스 제공 서버는 화상 대화 서비스 애 플리케이션, 프로그램, 앱 페이지 또는 웹 페이지를 제공하는 용도 또는 이를 제공하는 서버와 연동되는 역할을 제외하면 삭제될 수도 있다. 또한, 사용자 단말은, 얼굴표정, 응답 및 실사데이터가 합성된 결과를 출력하 고, 사용자 단말에서 더 이상 음성 및 화상 정보가 입력되지 않을 때까지 상술한 단계를 반복하는 단말일 수 있다. 여기서, 사용자 단말은, 네트워크를 통하여 원격지의 서버나 단말에 접속할 수 있는 컴퓨터로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 네비게이션, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱 (Desktop), 랩톱(Laptop) 등을 포함할 수 있다. 이때, 사용자 단말은, 네트워크를 통해 원격지의 서버나 단말에 접속할 수 있는 단말로 구현될 수 있다. 사용자 단말은, 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, 네비게이션, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)- 2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말, 스마트폰 (smartphone), 스마트 패드(smartpad), 타블렛 PC(Tablet PC) 등과 같은 모든 종류의 핸드헬드(Handheld) 기반 의 무선 통신 장치를 포함할 수 있다. 화상 대화 서비스 제공 서버는, 가상현실 기반 대화형 인공지능을 이용한 화상 대화 서비스 웹 페이지, 앱 페이지, 프로그램 또는 애플리케이션을 제공하는 서버일 수 있다. 그리고, 화상 대화 서비스 제공 서버는, 적어도 하나의 실사 제공 서버로부터 대화 모델의 영상, 2차원 데이터, 3차원 데이터, 음성 데이터 등을 수집하고, 이를 학습시켜 상황별 얼굴표정을 분류하여 저장하고, 응답 데이터를 얼굴 표정 및 음성별로 분류하 여 데이터베이스화하는 서버일 수 있다. 그리고, 화상 대화 서비스 제공 서버는, 사용자 단말로부터 대화 상대인 유명인을 선택하는 경우, 선택된 유명인의 얼굴표정, 음성 및 응답 데이터를 로딩하고, 사용자 단 말로부터 수집된 얼굴표정, 음성 및 음성이 변환된 텍스트를 입력값(질의, Query)으로 한 응답 데이터를 생성 또는 추출하고, 이를 사용자 단말로 전송하도록 하는 서버일 수 있다. 다만, 상술한 바와 같이, 사용 자 단말의 네트워킹 자원 및 컴퓨팅 자원이 기 설정된 기준값을 만족하는 경우에는, 화상 대화 서비스 제 공 서버는, 사용자 단말에서 상술한 과정이 수행되도록 제어할 수 있다. 그리고, 사용자별 감정패턴, 대화패턴 등을 학습하기 위하여 화상 대화 서비스 제공 서버는, 빅데이터를 적어도 하나의 정보제공 서버 및 기 저장된 히스토리 로그 데이터를 이용하여 구축하고, 수집, 전처리, 분석 등을 통하여 빅데이터를 분류 및 클러스터링한 후 학습시키는 서버일 수 있다. 또한, 화상 대화 서비스 제공 서버는, 비정형 데이 터인 영상 데이터나 이미지 데이터로부터 식별자를 추출하기 위하여, 영상 데이터 및 이미지 데이터로부터 식별 자를 태깅하기 위한 딥러닝 인공신경망 알고리즘을 이용하여 데이터 학습을 진행하는 서버일 수 있다. 그리고, 화상 대화 서비스 제공 서버는, 학습 결과에 따라 이후 입력되는 영상, 이미지 등으로부터 식별자를 태깅 하거나 추출하는 서버일 수 있다. 여기서, 화상 대화 서비스 제공 서버는, 네트워크를 통하여 원격지의 서버나 단말에 접속할 수 있는 컴퓨 터로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 네비게이션, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱(Desktop), 랩톱(Laptop) 등을 포함할 수 있다. 적어도 하나의 실사 제공 서버는, 가상현실 기반 대화형 인공지능을 이용한 화상 대화 서비스 관련 웹 페 이지, 앱 페이지, 프로그램 또는 애플리케이션을 이용하는 유명인 또는 유명인의 초상권이나 이를 배포하는 자 격을 가진 사람 또는 업체의 단말일 수 있다. 이때, 실사 데이터는 유명인이 아닐지라도 자신의 얼굴을 배포할 권리를 제공하는 사람으로부터 수집된 데이터라면 어느 데이터이든 가능하다 할 것이다. 또한, 유명인이라고 기 재하였지만, 비유명인의 데이터도 포함됨은 자명하다 할 것이다. 그리고, 적어도 하나의 실사 제공 서버는, 유명인의 2차원 또는 3차원의 영상 데이터, 모델링 데이터, 음성 데이터 등을 제공하는 서버일 수 있다. 이때, 화상 대화 서비스 제공 서버에서 직접 유명인을 촬영하여 모델링하는 경우에는 적어도 하나의 실사 서비스 제공 서버는 구비되지 않을 수 있다. 그리고, 적어도 하나의 실사 제공 서버는 유명인의 초상권 사용에 대한 대가로 수수료를 화상 대화 서비스 제공 서버로부터 제공받는 서버일 수 있다. 또한, 무단도용을 방지하기 위하여, 적어도 하나의 실사 제공 서버에서 SBC(Server Based Computing)이 구동될 수 있으나 이에 한정되지는 않고 본인을 인증할 수 있는 문서, 보호키, 생체인증 등 다양한 방법이 이용될 수 있음은 자명하다 할 것이다.여기서, 적어도 하나의 실사 제공 서버는, 네트워크를 통하여 원격지의 서버나 단말에 접속할 수 있는 컴 퓨터로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 네비게이션, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱(Desktop), 랩톱(Laptop) 등을 포함할 수 있다. 이때, 적어도 하나의 실사 제공 서버는, 네트워크 를 통해 원격지의 서버나 단말에 접속할 수 있는 단말로 구현될 수 있다. 적어도 하나의 실사 제공 서버는, 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, 네비게이션, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말, 스마트폰(smartphone), 스마트 패드(smartpad), 타블렛 PC(Tablet PC) 등과 같은 모든 종류의 핸드헬드(Handheld) 기반의 무선 통신 장치를 포함할 수 있다. 본 발명에서 설명하는 특정 실행들은 일 실시예들로서, 어떠한 방법으로도 본 발명의 범위를 한정하는 것은 아 니다. 명세서의 간결함을 위하여, 종래 전자적인 구성들, 제어 시스템들, 소프트웨어, 상기 시스템들의 다른 기 능적인 측면들의 기재는 생략될 수 있다. 또한, 도면에 도시된 구성 요소들 간의 선들의 연결 또는 연결 부재들 은 기능적인 연결 및/또는 물리적 또는 회로적 연결들을 예시적으로 나타낸 것으로서, 실제 장치에서는 대체 가 능하거나 추가의 다양한 기능적인 연결, 물리적인 연결, 또는 회로 연결들로서 나타내어질 수 있다. 또한, \"필 수적인\", \"중요하게\" 등과 같이 구체적인 언급이 없다면 본 발명의 적용을 위하여 반드시 필요한 구성 요소가 아닐 수 있다. 따라서, 본 발명의 사상은 상기 설명된 실시예에 국한되어 정해져서는 아니 되며, 후술하는 특허청구범위뿐만 아니라 이 특허청구범위와 균등한 또는 이로부터 등가적으로 변경된 모든 범위는 본 발명의 사상의 범주에 속한 다고 할 것이다."}
{"patent_id": "10-2023-0056217", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 사용자 응대를 위한 맞춤형 가상 얼굴 서비스 방법을 나타낸 동작 흐름도이 다."}
