{"patent_id": "10-2022-0157890", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0075961", "출원번호": "10-2022-0157890", "발명의 명칭": "드론영상을 이용한 사람행동 구분 방법", "출원인": "포항공과대학교 산학협력단", "발명자": "최석현"}}
{"patent_id": "10-2022-0157890", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "드론제어센터에서 기준 영상데이터셋을 이용하여 딥러닝모델을 학습시키는 단계;상기 드론제어센터에서 드론으로부터 드론영상을 수신하는 단계;상기 수신된 드론영상에서 사람객체를 인식하는 단계;상기 인식된 사람객체에 대한 개별아이디를 부여한 후 골격좌표를 추출하는 단계; 및상기 딥러닝모델을 이용하여 사람행동을 구분하는 단계;를 포함하는 드론영상을 이용한 사람행동 구분 방법."}
{"patent_id": "10-2022-0157890", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 딥러닝모델을 학습시키는 단계는,사람이 쓰러지는 영상데이터셋과 수어 구분을 위한 영상데이터셋을 포함하는 상기 기준 영상데이터셋을 이용하여 학습시키는드론영상을 이용한 사람행동 구분 방법."}
{"patent_id": "10-2022-0157890", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서,상기 딥러닝모델을 학습시키는 단계는,상기 기준 영상데이터셋을 이용하여 Bi-LSTM(bidirectional long short-term memory)모델을 학습시키는드론영상을 이용한 사람행동 구분 방법."}
{"patent_id": "10-2022-0157890", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 3에 있어서,상기 사람객체를 인식하는 단계는,상기 드론영상에서 Yolo-pose모델을 이용하여 상기 사람객체를 검출하여 경계상자(bounding box)로 표시하는드론영상을 이용한 사람행동 구분 방법."}
{"patent_id": "10-2022-0157890", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서,상기 골격좌표를 추출하는 단계는,상기 사람객체에 대해 동일한 객체를 구분하여 상기 개별아이디를 부여하는공개특허 10-2024-0075961-3-드론영상을 이용한 사람행동 구분 방법."}
{"patent_id": "10-2022-0157890", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 5에 있어서,상기 골격좌표를 추출하는 단계는,Strong sort모델을 이용한 상기 사람객체의 위치 비교를 통해 상기 개별아이디를 부여하는드론영상을 이용한 사람행동 구분 방법."}
{"patent_id": "10-2022-0157890", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서,상기 골격좌표를 추출하는 단계는,상기 사람객체에서 상기 골격좌표를 추출하여 단위벡터로 변환한 후에 시계열데이터로 변환하는드론영상을 이용한 사람행동 구분 방법."}
{"patent_id": "10-2022-0157890", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 7에 있어서상기 사람행동을 구분하는 단계는,상기 개별아이디에 따라 상기 시계열데이터를 취합하고, 학습된 상기 Bi-LSTM모델에 입력하여 구분하는드론영상을 이용한 사람행동 구분 방법."}
{"patent_id": "10-2022-0157890", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 드론영상을 이용한 사람행동 구분 방법에 관한 것으로, 드론제어센터에서 기준 영상데이터셋을 이용하 여 딥러닝모델을 학습시키는 단계; 상기 드론제어센터에서 드론으로부터 드론영상을 수신하는 단계; 상기 수신된 드론영상에서 사람객체를 인식하는 단계; 상기 인식된 사람객체에 대한 개별아이디를 부여한 후 골격좌표를 추출 하는 단계; 및 상기 딥러닝모델을 이용하여 사람행동을 구분하는 단계;를 포함함으로써, 사람의 행동에 대한 이 미지 분류 성능을 향상시킬 수 있고, 다수의 사람들에 대해 행동을 인식 및 구분하여 이상 행동에 대한 정교한 탐지를 수행할 수 있다."}
{"patent_id": "10-2022-0157890", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 기준 영상데이터셋을 이용하여 딥러닝모델인 Bi-LSTM모델을 학습시킨 후에, 드론으로부터 드론영상을 수신하고, 수신된 드론영상에서 Yolo-pose모델을 이용하여 사람객체를 인식하며, Strong sort모델을 통해 인식 된 사람객체에 대한 개별아이디를 부여한 후 골격좌표를 추출하며, 학습된 딥러닝모델을 이용하여 사람행동을 구분함으로써, 사람의 행동에 대한 이미지 분류 성능을 향상시킬 수 있고, 다수의 사람들에 대해 행동을 인식 및 구분하여 이상 행동에 대한 정교한 탐지를 수행할 수 있는 드론영상을 이용한 사람행동 구분 방법에 관한 것 이다."}
{"patent_id": "10-2022-0157890", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "잘 알려진 바와 같이, 컴퓨터 비전 분야가 급성장함에 따라 이를 활용하여 건설, 해양, 치안, 재난 안전 등의 분야에서 영상을 통한 관제에 대한 수요가 증가하고 있다. 이러한 비전 기술을 활용하여 영상 속 사람의 행동에 대해 인식 및 구분하는 연구가 활발히 진행되고 있다. 일 예로서, 건설 현장에서 위험 상황을 탐지하기 위해 카메라를 활용한 안전 장비 착용 여부, CCTV를 통한 위험 지역에서의 돌발 상황 알림 등을 제공할 수 있다. 하지만, CCTV를 활용하여 관제하는 경우, CCTV가 설치되어 촬영하는 영역 외의 지역에서의 위험 상황들을 관제 실 등에서 인식하기 어려움이 있을 뿐만 아니라, 해안가와 같이 CCTV를 설치하기 힘든 지역에서는 더욱 관제하 기 어려운 실정이다. 이와 같은 단점을 보완하고자 최근 공공기관이나, 지자체에서는 드론을 활용하여 광범위한 지역에 대해 안전관 리 및 모니터링하는 시스템을 구축하려 하고 있다. 하지만, 기존의 연구들은 하나의 객체(한 사람)에 대해 행동 인식 및 구분하는 연구이며, 다수의 객체에 대해 인식 및 구분하기에는 적합하지 않았고, 해당 연구들은 이미지 분류(image classification)를 통한 행동 구분을 하는 방식을 취하고 있기 때문에, 유사한 행동들(예를 들면, 누워있는 행위와 쓰러지는 행위, 수영과 물에 빠지 는 행위 등)을 분류하는 데 어려움이 있는데, 이는 특정 이미지 데이터에 대하여 과적합된 것이 주요 원인이다. 따라서, 사람의 행동에 대한 이미지 분류의 단점을 보완하고, 다수의 사람들에 대해 행동을 인식 및 구분하여 이상 행동에 대한 정교한 탐지를 수행할 수 있는 방안이 시급히 요구되고 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 1. 한국공개특허 제10-2020-0021841호(2020.03.02.공개)"}
{"patent_id": "10-2022-0157890", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 기준 영상데이터셋을 이용하여 딥러닝모델인 Bi-LSTM모델을 학습시킨 후에, 드론으로부터 드론영상을 수신하고, 수신된 드론영상에서 Yolo-pose모델을 이용하여 사람객체를 인식하며, Strong sort모델을 통해 인식 된 사람객체에 대한 개별아이디를 부여한 후 골격좌표를 추출하며, 학습된 딥러닝모델을 이용하여 사람행동을 구분함으로써, 사람의 행동에 대한 이미지 분류 성능을 향상시킬 수 있고, 다수의 사람들에 대해 행동을 인식 및 구분하여 이상 행동에 대한 정교한 탐지를 수행할 수 있는 드론영상을 이용한 사람행동 구분 방법을 제공하 고자 한다. 본 발명의 실시예들의 목적은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 또 다른 목적들은 아 래의 기재로부터 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0157890", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시예에 따르면, 드론제어센터에서 기준 영상데이터셋을 이용하여 딥러닝모델을 학습시키는 단계; 상기 드론제어센터에서 드론으로부터 드론영상을 수신하는 단계; 상기 수신된 드론영상에서 사람객체를 인식하 는 단계; 상기 인식된 사람객체에 대한 개별아이디를 부여한 후 골격좌표를 추출하는 단계; 및 상기 딥러닝모델 을 이용하여 사람행동을 구분하는 단계;를 포함하는 드론영상을 이용한 사람행동 구분 방법이 제공될 수 있다. 또한, 본 발명의 실시예에 따르면, 상기 딥러닝모델을 학습시키는 단계는, 사람이 쓰러지는 영상데이터셋과 수 어 구분을 위한 영상데이터셋을 포함하는 상기 기준 영상데이터셋을 이용하여 학습시키는 드론영상을 이용한 사 람행동 구분 방법이 제공될 수 있다. 또한, 본 발명의 실시예에 따르면, 상기 딥러닝모델을 학습시키는 단계는, 상기 기준 영상데이터셋을 이용하여 Bi-LSTM(bidirectional long short-term memory)모델을 학습시키는 드론영상을 이용한 사람행동 구분 방법이 제공될 수 있다. 또한, 본 발명의 실시예에 따르면, 상기 사람객체를 인식하는 단계는, 상기 드론영상에서 Yolo-pose모델을 이용 하여 상기 사람객체를 검출하여 경계상자(bounding box)로 표시하는 드론영상을 이용한 사람행동 구분 방법이 제공될 수 있다. 또한, 본 발명의 실시예에 따르면, 상기 골격좌표를 추출하는 단계는, 상기 사람객체에 대해 동일한 객체를 구 분하여 상기 개별아이디를 부여하는 드론영상을 이용한 사람행동 구분 방법이 제공될 수 있다. 또한, 본 발명의 실시예에 따르면, 상기 골격좌표를 추출하는 단계는, Strong sort모델을 이용한 상기 사람객체 의 위치 비교를 통해 상기 개별아이디를 부여하는 드론영상을 이용한 사람행동 구분 방법이 제공될 수 있다. 또한, 본 발명의 실시예에 따르면, 상기 골격좌표를 추출하는 단계는, 상기 사람객체에서 상기 골격좌표를 추출 하여 단위벡터로 변환한 후에 시계열데이터로 변환하는 드론영상을 이용한 사람행동 구분 방법이 제공될 수 있다. 또한, 본 발명의 실시예에 따르면, 상기 사람행동을 구분하는 단계는, 상기 개별아이디에 따라 상기 시계열데이 터를 취합하고, 학습된 상기 Bi-LSTM모델에 입력하여 구분하는 드론영상을 이용한 사람행동 구분 방법이 제공될 수 있다."}
{"patent_id": "10-2022-0157890", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 기준 영상데이터셋을 이용하여 딥러닝모델인 Bi-LSTM모델을 학습시킨 에, 드론으로부터 드론영상을 수신하고, 수신된 드론영상에서 Yolo-pose모델을 이용하여 사람객체를 인식하며, Strong sort모델을 통해 인식 된 사람객체에 대한 개별아이디를 부여한 후 골격좌표를 추출하며, 학습된 딥러닝모델을 이용하여 사람행동을 구분함으로써, 사람의 행동에 대한 이미지 분류 성능을 향상시킬 수 있고, 다수의 사람들에 대해 행동을 인식 및 구분하여 이상 행동에 대한 정교한 탐지를 수행할 수 있다."}
{"patent_id": "10-2022-0157890", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 실시예들에 대한 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후 술되어 있는 실시예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하"}
{"patent_id": "10-2022-0157890", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "고, 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되 는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 본 발명의 실시예들을 설명함에 있어서 공지 기능 또는 구성에 대한 구체적인 설명이 본 발명의 요지를 불필요 하게 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명을 생략할 것이다. 그리고 후술되는 용어들은 본 발명의 실시예에서의 기능을 고려하여 정의된 용어들로서 이는 사용자, 운용자의 의도 또는 관례 등에 따라 달라질 수 있다. 그러므로 그 정의는 본 명세서 전반에 걸친 내용을 토대로 내려져야 할 것이다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예를 상세히 설명하기로 한다. 도 1은 본 발명의 실시예에 따라 드론영상을 이용하여 사람행동을 구분하는 과정을 나타낸 플로우차트이고, 도 2는 본 발명의 실시예에 따른 드론영상을 이용한 사람행동 구분 방법을 수행하는 시스템을 예시한 도면이며, 도 3 내지 도 5는 본 발명의 실시예에 따른 드론영상을 이용한 사람행동 구분 방법을 설명하기 위한 도면이다. 도 1 내지 도 5를 참조하면, 드론제어센터에서 기준 영상데이터셋을 이용하여 딥러닝모델을 학습시킬 수 있다(단계110). 이러한 딥러닝모델을 학습시키는 단계에서는, 사람이 쓰러지는 영상데이터셋과 수어 구분을 위한 영상데이 터셋을 포함하는 기준 영상데이터셋을 이용하여 학습시킬 수 있는데, 사람이 쓰러지는 영상데이터셋은 예를 들 면, UR Fall Detection 등에서 제공하는 영상이미지셋을 이용할 수 있고, 수어 구분을 위한 영상데이터셋은 에 를 들면, 데이콘의 수어 구분 경진대회 등에서 제공하는 영상이미지셋을 이용할 수 있다. 또한, 딥러닝모델을 학습시키는 단계에서는, 기준 영상데이터셋을 이용하여 Bi-LSTM(bidirectional long short-term memory)모델을 학습시킬 수 있는데, Bi-LSTM(양방향 LSTM)모델은 정방향으로만 학습을 진행하는 대 신, 마지막 노드에서 뒤에서 앞으로(즉, 역방향) 실행되는 다른 LSTM을 추가하는 것으로, LSTM 대비 역방향으로 정보를 전달하는 히든레이어(hidden layer)를 추가함으로써, 정보를 보다 유연하게 처리할 수 있다. 즉, 각 시점에서 히든스테이트(hidden state)가 이전 시점과 미래 시점의 정보를 모두 갖는 효과가 있기 때문에, 모델을 전체 시계열 데이터로부터 학습할 수 있는 장점이 있다. 이러한 Bi-LSTM모델에 대해 부연하여 설명하면, LSTM은 시계열성 데이터를 분석하는 모델로, 순차적인 데이터를 학습하여 맥락을 파악할 수 있고, Bi-LSTM은 순차적인 데이터를 역순으로도 학습하는 추가과정을 통해 맥락을더욱 잘 파악할 수 있도록 수정된 LSTM계열의 모델이다. 예를 들면, 도 3은 Bi-LSTM모델의 모식도로서, 시계열성 골격 데이터를 입력받을 경우 행동을 구분하는 과정을 나타내고 있는데, Bi-LSTM은 시계열 데이터를 연산하는 부분을 나타내고, 완전연결계층(FC layer)은 LSTM의 최 종 출력물을 로짓(즉, 확률)로 변환하는 부분을 나타낸다. 이러한 Bi-LSTM모델에 시계열 데이터가 입력될 경우 LSTM에 순차적으로 한번 입력하고, 역순으로 한번 더 입력 하게 되며, 해당 과정 중 각 셀에서는 입력받은 데이터들 가운데 어떤 데이터를 버릴지, 저장할지를 결정한 후 에 다음 셀로 데이터를 전달할 수 있고, 모든 셀에서의 연산이 끝나고 Bi-LSTM의 최종적인 출력인 마지막 셀의 출력을 완전연결계층(FC layer)에 통과시켜 로짓으로 변환하여 행동을 분류할 수 있다. 또한, 히든스테이트는 RNN(recurrent neural network, 순환신경망-시계열의 기본예측모델)의 은닉층(hidden layer)을 의미하며, 이전 상태로부터 전달된 값을 나타낸다. 한편, UR Fall Detection 등에서 제공하는 영상이미지셋과 같이 골격좌표를 포함하지 않은 데이터의 경우 Yolo- pose모델을 이용하여 사람객체를 검출하고, 검출된 사람객체에 대한 골격좌표를 추출한 후에, 딥러닝모델(즉, Bi-LSTM모델)을 학습시킬 수 있다. 여기에서, Yolo-pose모델은 이미지 내의 존재하는 다수의 사람의 골격좌표를 실시간으로 예측한 후, 개개인의 사람을 경계박스(bounding box)로 구분하는 기술을 의미한다. 또한, UR Fall Detection은 폴란드의 제슈프 대학에서 사람의 쓰러짐을 인식하기 위해 제작한 데이터셋으로, 6- 10초 길이의 사람이 넘어지는 영상들로 구성되어 있으며, 데이콘의 수어 구분은 인공지능 플랫폼인 DACON에서 수신호를 구분하기 위해 제공한 오픈 데이터셋으로, 영상의 프레임마다 캡쳐본과 사람의 골격좌표가 저장되어 있다. 그리고, 드론제어센터에서 드론으로부터 드론영상을 수신할 수 있다(단계120). 여기에서, 드론은 무인비행체로서, 지정된 감시영역을 촬영하고, 촬영된 드론영상을 드론제어센터)로 통신망을 통해 전송할 수 있는데, 예를 들면, 열화상카메라, 적외성카메라, RGB카메라 등을 포함하는 적어 도 하나의 카메라를 구비하여 감시영역을 촬영할 수 있고, 탑재된 카메라는 360도 전방위 촬영을 위한 카메라짐 볼시스템과 함께 탑재될 수 있고, 비행각도에 관계없이 카메라 위치를 고정하여 흔들림이 최소화된 드론영상을 촬영할 수 있는 흔들림 보정 기능이 포함될 수 있다. 또한, 드론은 원거리 객체를 자동으로 줌인 확대하여 추적 및 촬영할 수 있도록 적어도 30배의 광학줌을 내장하는 IR 일체형 네트워크 PTZ 카메라가 추가로 탑재될 수 있을 뿐만 아니라, 흔들림을 보정하여 선명한 영 상을 취득할 수 있는 이미지 스태빌라이저, 역광에 의해 어두워진 영상의 화질을 개선할 수 있는 역광보정, 안 개 낀 영상을 강제로 보정하여 선명한 영상을 ??득할 수 있는 안개보정 등의 기능을 포함할 수 있다. 그리고, 드론제어센터는 드론과 무선통신을 수행하여 드론에 대한 원격제어를 수행하는 것으로, 드론으로부터 획득된 드론영상을 수신하여 후술하는 영상검출 및 사람행동을 구분하는 과정을 수행할 수 있다. 한편, 통신망은 무선통신방식을 지원하는 네트워크망으로, 예를 들어GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), CDMA2000(Code Division Multi Access 2000), EVDO(Enhanced Voice-Data Optimized or Enhanced Voice-Data Only), WCDMA(Wideband CDMA), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), LTE(Long Term Evolution), LTE- A(Long Term Evolution-Advanced) 등을 포함하는 다양한 무선통신망을 포함할 수 있으며, 이를 통해 드론(10 0)과 드론제어센터 간의 원활한 무선통신환경을 제공할 수 있다. 다음에, 드론제어센터에서는 수신된 드론영상에서 사람객체를 인식할 수 있다(단계130). 이러한 사람객체를 인식하는 단계에서는, 드론영상에서 Yolo-pose모델을 이용하여 사람객체를 검출하여 경 계상자(bounding box)로 표시할 수 있는데, 드론제어센터에서는 드론으로부터 드론영상이 수신될 경 우, 이 드론영상에서 Yolo-pose모델을 통해 이미지프레임 내 사람객체만을 탐지하고, 탐지된 사람객체의 경계선 에 따라 경계상자를 마킹하여 사람이 내부에 포함된 박스로 하여 사람객체를 표시할 수 있다. 여기에서, Yolo-pose모델은 객체 탐지를 기반으로 사람행동을 인식할 수 있는 모델인데, 객체 탐지는 이미지프 레임이 입력될 경우 입력값 내에 배경과 사물을 구분한 후 사물이 어떤 사물인지 판별하는 기술을 의미하고, 사람행동 인식은 사람의 이미지프레임이 입력될 경우 자세에 대한 정보(예를 들면, 골격좌표 등)를 출력하는 기술 을 의미한다. 그리고, 드론제어센터에서는 인식된 사람객체에 대한 개별아이디를 부여한 후 골격좌표를 추출할 수 있다 (단계140). 이러한 골격좌표를 추출하는 단계에서는, 사람객체에 대해 동일한 객체를 구분하여 개별아이디를 부여할 수 있고, Strong sort모델을 이용한 사람객체의 위치 비교를 통해 개별아이디를 부여할 수 있는데, Strong sort 모델을 이용하여 이전프레임과 현재프레임의 객체 위치를 비교하여 일관성 있는 아이디를 부여할 수 있는 장점 이 있다. 이러한 Strong sort모델은 연속된 프레임(영상)에서 객체들의 관계를 파악하여 객체에 ID를 부여하는 실시간 다 중객체 추적 기술로, 둘 이상의 객체가 교차하여 겹치는 상황에서도 객체들의 ID를 유지하여 다중객체 추적 분 야에서 정확성이 높은 기술로 알려져 있는데, 여러사람이 동시에 교차하는 영상에서도 좋은 성능을 나타낼 수 있고, 일관성 있는 아이디를 객체별로 부여함으로써, 각 아이디별로 데이터를 취합할 수 있는 장점이 있고, 여 러사람에 대한 자세 구분을 동시에 수행할 수 있는 장점이 있다. 한편, 골격좌표를 추출하는 단계에서는, 사람객체에서 골격좌표를 추출하여 단위벡터로 변환한 후에 시계 열데이터로 변환할 수 있는데, 경계상자 내의 사람객체에 대해 총 17개의 골격좌표(예를 들면, 양눈, 코, 양귀, 양어깨, 양팔꿈치, 양손목, 양고관절, 양무릎, 양발목 등을 포함함)를 추출하고, 추출된 골격좌표를 단위벡터로 변환할 수 있다. 하지만, 드론영상에서는 17개의 모든 골격좌표가 추출되기 어렵기 때문에, 본 발명의 실시예에서는 도 4에 도시 한 바와 같이 Strong sort모델을 통해 사람객체에 따라 아이디를 부여하면서 Yolo-pose모델을 통해 양어깨, 양 팔꿈치, 양고관절 및 양무릎을 포함하는 총 8개의 골격좌표를 추출하고, 추출된 8개의 골격좌표를 단위벡터로 변환할 수 있다. 또한, 드론제어센터에서는 다수의 객체에 대한 행동을 구분하기 위해서 사람객체에 부여된 아이디별로 단 위벡터를 취합할 수 있고, 취합된 단위벡터는 사람자세를 분석(즉, 사람행동을 구분)하기 위한 시계열데이터로 변환될 수 있다. 여기에서, 단위벡터는 영상 2-3초 길이(예를 들면, 30 프레임 기준으로 60-90장 이미지)로 취 합될 수 있다. 다음에, 드론제어센터에서는 딥러닝모델을 이용하여 사람행동을 구분할 수 있다(단계150). 이러한 사람행동을 구분하는 단계에서는, 개별아이디에 따라 시계열데이터를 취합하고, 학습된 Bi-LSTM모 델에 입력하여 구분할 수 있는데, 아이디별로 취합된 시계열데이터를 학습된 Bi-LSTM모델에 입력하여 사람의 행 위 맥락을 파악하고, 이를 통해 사람행동을 구분한 후에, 수신된 드론영상에 실시간으로 구분 결과를 표시하여 별도의 디스플레이장치를 통해 디스플레이할 수 있다. 여기에서, Bi-LSTM모델은 학습 시, 이전 시점과 미래 시점의 정보 또한 학습할 수 있기 때문에, 사고 및 범죄 발생 전에 사람행동을 효과적으로 예측할 수 있으며, 이를 통해 드론영상과 딥러닝 모델을 이용하여 해안가에서 발생하는 안전사고 및 범죄를 실시간으로 탐지할 수 있다. 상술한 바와 같이 드론제어센터에서 드론으로부터 통신망을 통해 전송되는 영상데이터(즉, 도 7 에 도시한 바와 같은 영상데이터로서 연속적인 영상프레임을 의미함)를 수신할 경우 드론제어센터에서는 수신한 영상데이터에서 골격좌표(key points)를 검출한 후 경계박스를 표시할 수 있다. 이와 같이 검출된 골격좌표는 다수의 사람들의 행동을 구분할 수 있도록 영상 속 객체에 부여된 ID별로 수집될 수 있고, 드론제어센터에서는 골격좌표를 통해 어깨-팔꿈치, 어깨-고관절 및 고관절-무릎이 이루는 벡터를 구한 후에, 학습에 용이하도록 단위벡터(즉, 유닛벡터)로 만들어 표준화할 수 있다. 여기에서, 유닛벡터로 만드는 이유에 대해 설명하면, 예를 들어 이미지 내에서 골격좌표가 사진 귀퉁이에서 추 출된 경우, 데이터의 일관성을 확보하기 힘들게 되는데, 그렇게 되면 훈련 데이터가 전체 데이터를 대표하지 못 할 수도 있고, 그렇게 되면 모델 학습에 방해가 될 수 있기 때문에, 이를 방지하기 위해 벡터를 표준화하여 유 닛벡터로 변환할 수 있다. 다음에, 드론제어센터에서는 여러 프레임을 통해 수집된 유닛벡터를 훈련된 Bi-LSTM에 입력값으로 입력시 켜, 벡터의 변화 양상을 파악함으로써 사람의 행동의 맥락을 이해하고, 최종적으로 사람의 행동을 분류한 후,영상에 골격좌표, 경계좌표 및 예측한 행동을 표시할 수 있다. 따라서, 본 발명의 실시예에 따르면, 기준 영상데이터셋을 이용하여 딥러닝모델인 Bi-LSTM모델을 학습시킨 후에, 드론으로부터 드론영상을 수신하고, 수신된 드론영상에서 Yolo-pose모델을 이용하여 사람객체를 인식하며, Strong sort모델을 통해 인식된 사람객체에 대한 개별아이디를 부여한 후 골격좌표를 추출하며, 학습 된 딥러닝모델을 이용하여 사람행동을 구분함으로써, 사람의 행동에 대한 이미지 분류 성능을 향상시킬 수 있고, 다수의 사람들에 대해 행동을 인식 및 구분하여 이상 행동에 대한 정교한 탐지를 수행할 수 있다. 이상의 설명에서는 본 발명의 다양한 실시예들을 제시하여 설명하였으나 본 발명이 반드시 이에 한정되는 것은"}
{"patent_id": "10-2022-0157890", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "아니며, 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자라면 본 발명의 기술적 사상을 벗어나지 않는 범 위 내에서 여러 가지 치환, 변형 및 변경이 가능함을 쉽게 알 수 있을 것이다."}
{"patent_id": "10-2022-0157890", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따라 드론영상을 이용하여 사람행동을 구분하는 과정을 나타낸 플로우차트이고, 도 2는 본 발명의 실시예에 따른 드론영상을 이용한 사람행동 구분 방법을 수행하는 시스템을 예시한 도면이며, 도 3 내지 도 5는 본 발명의 실시예에 따른 드론영상을 이용한 사람행동 구분 방법을 설명하기 위한 도면이다."}
