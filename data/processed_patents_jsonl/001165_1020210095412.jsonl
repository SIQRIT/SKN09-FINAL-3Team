{"patent_id": "10-2021-0095412", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0014264", "출원번호": "10-2021-0095412", "발명의 명칭": "인공지능 모델 변환 방법, 인공지능 모델 변환 장치, 인공지능 모델 구동 방법 및 인공지능", "출원인": "티에스엔랩 주식회사", "발명자": "김성민"}}
{"patent_id": "10-2021-0095412", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "아키텍처 및 파라미터를 갖는 인공지능 모델을 인공지능 모델의 구동 장치에 최적화된 인공지능 모델로 변환하기 위한 인공지능 모델 변환 방법으로서,상기 인공지능 모델의 아키텍처 및 파라미터를 메모리 상에 로딩하고, 로딩된 상기 인공지능 모델을 해석하는인공지능 모델 해석 단계;해석된 상기 인공지능 모델에 대하여 상기 구동 장치에 독립적인 최적화를 수행하는 모델 최적화 단계;해석된 상기 인공지능 모델에 대하여 상기 구동 장치에 종속적인 최적화를 수행하는 기기 최적화 단계; 및상기 구동 장치에 독립적인 최적화 및 상기 구동 장치에 종속적인 최적화가 수행된 상기 인공지능 모델을 상기구동 장치에서 해석 가능한 포맷으로 변환하는 기기 포맷 변환 단계를 포함하는 인공지능 모델 변환 방법."}
{"patent_id": "10-2021-0095412", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 모델 최적화 단계는,상기 인공지능 모델 중 머신러닝에 사용되고 추론에 사용되지 않는 아키텍처 및 파라미터를 제거하는 비추론부제거 단계;상기 인공지능 모델이 출력하는 결과물을 역으로 추적하여 결과물을 출력하는데 직접적으로 관여하지 않는 부분을 제거하는 비관여부 제거 단계; 및상기 인공지능 모델에서 복수회의 추론시 동일한 결과를 출력하는 아키텍처 및 파라미터와 관련된 계산을 미리수행하는 상수 계산 단계 중 하나 이상을 포함하는 인공지능 모델 변환 방법."}
{"patent_id": "10-2021-0095412", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 기기 최적화 단계는,상기 구동 장치의 계산 능력에 따라 상기 인공지능 모델의 계산 단위를 제1 계산 기준으로부터 제1 계산 기준보다 작은 크기를 갖는 제2 계산 기준으로 양자화하는 양자화 단계;상기 인공지능 모델에 대하여 메모리 얼라인먼트(memory alignment)를 미리 수행하고, 기기에 최적화된 계산을수행하도록 상기 인공지능 모델을 변경하는 메모리 얼라인먼트 단계; 상기 인공지능 모델의 계산시에 사용되지 않는 오퍼레이터를 검출하고, 상기 구동 장치로부터 사용되지 않는 오퍼레이터에 관한 기능을 제거하는 기능 제거 단계;상기 구동 장치의 부동 소수점(floating point) 표현 방식에 따라 상기 인공지능 모델과 관련된 데이터의 포맷을 변환하는 데이터 포맷 변환 단계;2 이상의 단일 연산으로 구성된 복합 오퍼레이터를 2 이상의 단일 연산으로 분리하는 복합 오퍼레이터 분리 단계; 및2 이상의 연속하는 단일 연산을 2 이상의 단일 연산을 한번에 수행하는 통합 오퍼레이터로 대체하는 오퍼레이터대체 단계공개특허 10-2023-0014264-3-중 하나 이상을 포함하는 인공지능 모델 변환 방법."}
{"patent_id": "10-2021-0095412", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 인공지능 모델은 ONNX(Open Neural Network eXchange) 또는 NNEF(Neural Network Exchange Format)를 포함하는 표준화된 인공지능 모델인 인공지능 모델 변환 방법."}
{"patent_id": "10-2021-0095412", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,표준화된 인공지능 모델에서 정의된 오퍼레이터와 상이한 사용자 정의 오퍼레이터를 규정하고, 사용자 정의에따른 사용자 정의 오퍼레이터를 실행시키는 단계를 더 포함하는 인공 지능 모델 변환 방법."}
{"patent_id": "10-2021-0095412", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "인공지능 모델을 구동하기 위한 구동 방법으로서,입력 데이터를 인공지능 모델에서 해석가능한 텐서로 변환하는 입력 변환 단계;저장된 실행 코드로부터 인공지능 모델의 아키텍처 및 파라미터를 메모리 상에 로딩하고, 로딩된 상기 인공지능모델을 해석하는 모델 해석 단계;상기 입력 데이터로 변환된 상기 텐서에 대하여 해석된 인공지능 모델을 구동하여 출력 데이터를 생성하는 모델구동 단계;인공지능 모델의 계산에 필요한 복수의 오퍼레이터에 의한 연산을 가속화하는 연산 가속 단계; 및상기 출력 데이터를 액추에이터에서 해석 가능한 데이터 형태로 변환하여 출력하는 출력 변환 단계를 포함하는 인공지능 모델 구동 방법."}
{"patent_id": "10-2021-0095412", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 모델 구동 단계는 인터프리터, FPGA 또는 ASIC에 의해 수행되는 인공지능 모델 구동 방법."}
{"patent_id": "10-2021-0095412", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 연산 가속 단계는 상기 복수의 오퍼레이터에 의한 연산을 동시에 병렬처리하는 인공지능 모델 구동 방법."}
{"patent_id": "10-2021-0095412", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "아키텍처 및 파라미터를 갖는 인공지능 모델을 인공지능 모델의 구동 장치에 최적화된 인공지능 모델로 변환하기 위한 인공지능 모델 변환 장치로서,적어도 하나의 프로그램이 기록된 메모리; 및프로그램을 실행하는 프로세서를 포함하며,상기 프로그램은,상기 인공지능 모델의 아키텍처 및 파라미터를 메모리 상에 로딩하고, 로딩된 상기 인공지능 모델을 해석하는인공지능 모델 해석 단계;해석된 상기 인공지능 모델에 대하여 상기 구동 장치에 독립적인 최적화를 수행하는 모델 최적화 단계;해석된 상기 인공지능 모델에 대하여 상기 구동 장치에 종속적인 최적화를 수행하는 기기 최적화 단계; 및공개특허 10-2023-0014264-4-상기 구동 장치에 독립적인 최적화 및 상기 구동 장치에 종속적인 최적화가 수행된 상기 인공지능 모델을 상기구동 장치에서 해석 가능한 포맷으로 변환하는 기기 포맷 변환 단계를 수행하는 인공지능 모델 변환 장치."}
{"patent_id": "10-2021-0095412", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "인공지능 모델을 구동하기 위한 구동 장치로서,적어도 하나의 프로그램이 기록된 메모리; 및프로그램을 실행하는 프로세서를 포함하며,상기 프로그램은,입력 데이터를 인공지능 모델에서 해석가능한 텐서로 변환하는 입력 변환 단계;저장된 실행 코드로부터 인공지능 모델의 아키텍처 및 파라미터를 메모리 상에 로딩하고, 로딩된 상기 인공지능모델을 해석하는 모델 해석 단계;상기 입력 데이터로 변환된 상기 텐서에 대하여 해석된 인공지능 모델을 구동하여 출력 데이터를 생성하는 모델구동 단계;인공지능 모델의 계산에 필요한 복수의 오퍼레이터에 의한 연산을 가속화하는 연산 가속 단계; 및상기 출력 데이터를 액추에이터에서 해석 가능한 데이터 형태로 변환하여 출력하는 출력 변환 단계를 포함하는 인공지능 모델 구동 장치."}
{"patent_id": "10-2021-0095412", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예에 의하면, 아키텍처 및 파라미터를 갖는 인공지능 모델을 인공지능 모델의 구동 장치에 최 적화된 인공지능 모델로 변환하기 위한 인공지능 모델 변환 방법으로서, 상기 인공지능 모델의 아키텍처 및 파라 미터를 메모리 상에 로딩하고, 로딩된 상기 인공지능 모델을 해석하는 인공지능 모델 해석 단계; 해석된 상기 인 공지능 모델에 대하여 상기 구동 장치에 독립적인 최적화를 수행하는 모델 최적화 단계; 해석된 상기 인공지능 모델에 대하여 상기 구동 장치에 종속적인 최적화를 수행하는 기기 최적화 단계; 및 상기 구동 장치에 독립적인 최적화 및 상기 구동 장치에 종속적인 최적화가 수행된 상기 인공지능 모델을 상기 구동 장치에서 해석 가능한 포맷으로 변환하는 기기 포맷 변환 단계를 포함하는 인공지능 모델 변환 방법이 제공될 수 있다."}
{"patent_id": "10-2021-0095412", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 저전력 인공지능 모델에 관한 것으로, 보다 상세하게는 인공지능 모델을 변환하는 방법, 인공지능 모 델을 변환하는 장치, 인공지능 모델을 구동하는 방법 및 인공지능 모델을 구동하는 방법에 관한 것이다."}
{"patent_id": "10-2021-0095412", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "공개특허공보 제10-2021-0042012호의 인공 지능 추론 장치 및 방법은 다양한 하드웨어 환경을 가지는 임베디드 시스템에서 인공지능 응용의 구현을 용이하게 하고, 딥러닝을 가속시키는 추론 엔진을 개발함에 있어서 하드웨 어 변경에 따른 추론 엔진의 변경을 최소화한다. 그러나 종래 인공지능 구현 시스템은 본 발명 저전력 인공지능 구현 시스템과 같이, 저전력 타겟에 맞게 인공지 능 모델을 간소화하지 않고, 연산 성능에 맞도록 연산 정의를 최적화하지 못함에 따라 저전력 타겟과 같은 제한 된 하드웨어 환경에서 인공지능 시스템이 원활히 동작하지 못하는 문제점이 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 공개특허공보 제10-2021-0042012호 \"인공 지능 추론 장치 및 방법\""}
{"patent_id": "10-2021-0095412", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하기 위한 과제는, 저전력 타겟에 맞도록 인공지능 모델을 간소화하고, 연산 성능에 맞도록 연산 정의를 최적화해서 저전력 타겟과 같은 제한된 하드웨어 환경에서 인공지능 시스템이 동작하는 인공지능 모델 변환 반법, 인공지능 모델 변환 장치, 인공지능 모델 구동 방법 및 인공지능 모델 구동 장치를 제공하는 것이다.또한, 본 발명이 해결하기 위한 과제는, 저전력 타겟과 같은 하드웨어 환경에서 인공지능 시스템이 동작해서 IoT 환경의 임베디드 기기가 인공지능 시스템을 이용할 수 있는 인공지능 모델 변환 반법, 인공지능 모델 변환 장치, 인공지능 모델 구동 방법 및 인공지능 모델 구동 장치를 제공하는 것이다."}
{"patent_id": "10-2021-0095412", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 인공지능 모델을 간소화하고, 연산 정의를 최적화(커스터마이징)하여 저전력 타 겟과 같은 제한된 하드웨어 환경에서 인공지능 시스템이 동작하고, IoT 환경의 임베디드 기기가 인공지능 시스 템을 이용할 수 있어 인공지능 시스템의 범용성이 높아지는 효과를 가질 수 있다. 또한, 본 발명의 일 실시예에 따르면, 인공지능 모델을 기기에 배포하기 전에 소형화된 기기에서 해석하기 쉬운 형태로 변환하여 모델을 배포하기 때문에 적은 처리량, 적은 저장량을 갖는 기기에서도 인공지능 모델을 구동할 수 있고, 소형화된 기기에 배포하기 전에 모델을 사전에 소형화된 기기의 특성에 맞추어 최적화 하기 때문에 소 형화된 모델에선 별도의 최적화 과정 없이 모델을 구동할 수 있어 모델의 빠른 실행을 가능하게 하고, 소형화된 기기에서 표준화된 인공지능 모델을 해석할 수 있는 장치의 일반화된 구조를 제안함으로써 다양한 소형화된 기 기에서 인공지능 엔진을 재활용할 수 있고, 소형화된 기기에 인공지능 엔진을 도입하는 비용을 낮출 수 있다."}
{"patent_id": "10-2021-0095412", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 발명의 바람직한 일실시예에 따른 저전력 인공지능 구현 시스템에 대하여 상세히 설 명하기로 한다. 이하에서 종래 주지된 사항에 대한 설명은 본 발명의 요지를 명확히 하기 위해 생략하거나 간단 히 한다. 본 발명의 설명에 포함된 구성은 개별 또는 복합 결합 구성되어 동작한다. 명세서 전반에 사용된 \"최적화\"는 인공지능 구동 장치에 맞게 인공지능 모델을 커스터마이징하는 것을 의미하며, \"최적화\"는 \"커스터마이징\"과 상호 대체될 수 있는 용어이다. 도 1은 인공지능 모델이 생성되는 프로세스를 설명하는 개념도이다. 도 1을 참조하면, 인공지능 모델은 온도, 습도, 고도, 지문 등을 검출하는 각종 센서, 이미지 화상, 적외선 화 상 등을 취득하는 카메라, 라이더와 같은 입력 장치로부터 수집된 시계열 데이터로부터 특징량 선택, 알고리즘 선택을 통해 인공지능 모델을 선택하고, 학습, 성능 검증 과정에 의한 반복 시행 착오를 거쳐 모델 선택을 반복 한다. 성능 검증이 종료되면 최적의 인공지능 모델이 선택된다. 도 2는 도 2는 인공지능 오픈 플랫폼을 설명하는 예시도이다. 도 2를 참조하면 인공지능 오픈 플랫폼은 각종 트레이닝 플랫폼을 통합하고, 배포 타겟인 CPU, GPU, FPGA, NPU 에 인공지능 오픈 플랫폼의 실행 코드를 적용한다. 인공지능 모델은 복잡한 수학 공식과 수학 공식에서 사용되는 참고 값으로 이루어져 있다. 복잡한 구조로 되어 있는 수학 공식을 아키텍처라 부르고, 아키텍처가 참고하는 숫자의 집합을 파라미터(Parameter)라고 부른다. 최 근 몇 년 사이 인공지능이 급격히 발전하면서 인공지능 모델을 표준화된 방법으로 저장하고 교환해야 할 필요성 이 생겼고, Linux Foundation의 ONNX(Open Neural Network eXchange)과 Khronos Group의 NNEF(Neural Network Exchange Format)가 만들어지면서 인공지능 모델을 표준화된 형태로 교환할 수 있게 되었다. 2가지 표준 중 현재 대부분의 회사들은 ONNX을 따르고 있는데, ONNX은 Google에서 만든 Protocol Buffer라는 방 법을 활용해 인공지능 모델의 아키텍처와 파라미터를 저장한다. 즉, ONNX은 인공지능 모델의 아키텍처와 파라미 터의 구조를 정의하고, Protocol Buffer를 이용해 아키텍처와 파리미터를 바이너리 형태로 저장한다. ONNX은 현재 인공지능 모델의 표준으로써 역할을 하기 때문에 많은 회사에서 ONNX을 활용해 인공지능 모델을 구 동하는 기술을 선보이고 있다. 대표적인 예가 Microsoft의 ONNX Runtime으로 ONNX Runtime은 ONNX 형태로 저장 된 인공지능 모델을 실행시키는 엔진 역할을 한다. 사용자 입장에선 ONNX Runtime을 통해 ONNX 형태로 되어있는 어떤 인공지능 모델이든 구동시킬 수 있다. ONNX은 Protocol Buffer를 사용하여 인공지능 모델을 바이너리 형태로 변환 시키는데, Protocol Buffer는 IoT(Internet of Things)와 같은 소형 기기에서 해석하기엔 지나치게 복잡한 문제가 있다. 왜냐하면 일반적인 IoT 기기는 처리 용량과 저장 용량이 적기 때문에 Protocol Buffer와 같은 복잡한 형태로 되어있는 모델을 해석 하는데 어려움이 따르기 때문이다. 또한, ONNX은 인공지능 모델을 원본 그대로 보관하는 역할을 하기 때문에 인공지능 모델이 구동되는 기기의 특 성을 고려하지 않는다. 예를 들어 32-bit floating point(실수를 표현하는 방법 중 하나)를 사용하도록 되어있 는 모델은 16-bit floating point만 지원하는 기기에서 구동시킬 수 없다. 즉, ONNX을 빠르게 실행시키기 위해 선 기기의 특성에 맞추어 최적화(커스터마이징)하는 과정이 필요한데, IoT 기기와 같은 소형화된 기기의 경우 ONNX을 해석하는 것도 문제가 있을 뿐만 아니라 기기 최적화를 하는데도 과도한 처리 용량과 저장 용량을 필요 로 한다. ONNX 등의 인공지능 오픈 플랫폼은 파편화된 각종 트레이닝 플랫폼에 표준 가이드를 제공하고, 표준 가이드에 따라 통합된 인공지능 모델, 실행 코드, 사용법을 통해 다양한 제어부를 가지는 배포 타겟에 적합한 실행 코드 를 제공할 수 있다. 본 발명의 일 실시예에 따른 저전력 인공지능 구현 시스템은 인공지능 오픈 플랫폼에서 저전력 배포 타겟에 적 합한 실행 코드를 만들기 위해 인공지능 모델 선택과 실행 코드 배포를 최적화한다. 도 3은 본 발명의 일 실시예에 따른 인공지능 모델 변환 장치 및 방법을 나타내는 블록도이다. 도 3을 참조하면, 인공지능 모델 변환 장치(100: 컴퓨터)는 표준 모델 해석부, 모델 최적화부, 기기 최적화부, 기기 포맷 변환부를 포함한다. 컴퓨터는 온도, 고도, 지문 등 각종 센서, 이미지, 적외선 등 카메라, 라이더와 같은 입력 장치로부터 수 집된 시계열 데이터인 입출력 데이터 포맷과, 배포 타겟인 CPU, GPU, FPGA, NPU를 포함하는 저전력 타겟의 타겟 성능 지표를 입력받는다. 표준 모델 해석부는 인공지능 오픈 플랫폼에서 입출력 데이터 포맷과 메모리, 연산 성능의 타겟 성능 지표 를 고려한 표준 모델을 해석할 수 있다. 모델 최적화부는 표준 모델 해석부에서 고려된 표준 모델에서 입출력 데이터 포맷에 적합한 인공지능 모델을 선택하고, 선택된 인공지능 모델에서 사용되지 않는 파라미터에 관련된 모델 부분을 삭제할 수 있다. 모델 최적화부는 모델이 출력하는 결과물을 역으로 추적해 결과물을 출력하는데 직접적으로 관여하지 않는 부분을 제거할 수 있다. 일 실시예에서, 모델 최적화부는 학습 훈련 과정에서만 사용되고, 추론시에 사용되지 않는 아키텍터 및 파 리미터 부분을 삭제할 수 있다. 예를 들면, 구글에서 만든 추론 모델의 경우에 학습 훈련 과정에서만 사용되는 부분이 존재한다. 도 4의 적색 박스로 표시한 영역은 학습 훈련 과정에서만 활용되고, 추론시에는 사용되지 않는다. 모델 최적화부는 인공지능 학습 모델에서 추론시에 사용되지 않는 부분을 제거함으로써 성능 향상 효과를 얻을 수 있다. 일 실시예에서, 모델 최적화부는 상수를 계산하는 부분의 계산을 미리 수행해서 모델의 크기를 줄이고, 계 산 시간을 단축시킬 수 있다. 예를 들면, 도 5는 마이크로소프트에서 만든 MNIST CNN 모델을 나타내며, 적색 박 스로 표시한 영역은 매번 상수를 계산하는 부분이다. 이러한 부분은 추론할 때마다 반복적으로 수행되고, 동일 한 결과를 출력한다. 따라서, 모델 최적화부는 모델의 컴팩트화 및 계산 시간의 단축을 위하여 상수를 계 산하는 부분의 계산을 미리 수행할 수 있다. 기기 최적화부는 모델 최적화부에서 선택된 인공지능 모델에 대해 타겟 성능 지표에 적합한 데이터 정의, 연산자 정의, 저전력 타겟 선택을 수행하고, 실행 코드 배포를 기기 포맷 변환부에 지시한다. 예를 들어, 연산자 정의는 데이터 정의에서 요구되는 비트 수에 따라 연산자 변수의 비트 수를 저전력 타겟의 하드웨어 성능에 맞게 정의한다. 컴퓨터의 경우, 정수 변수가 32비트일 때 저전력 타겟은 16비트일 수 있다. 또한, 기기 최적화부는 특정 모델에서 사용하지 않는 기능을 제거해서 모델 구동부의 크기를 줄일 수 있다. 기기 최적화부는 140가지 연산자가 있는데, 그 중 10개만 사용한다고 하면 10개의 연산자만 사용하 도록 함으로써 실행 코드의 크기를 1/14로 줄일 수 있다. 일 실시예에서, 기기 최적화부는 빠른 속도로 모델을 로딩하기 위해 디스크에 특정한 순서로 데이터를 저 장하거나, 특정한 간격을 유지해야 하는 경우 메모리 정렬 과정을 미리 수행함으로써 기기에 최적화된 계산을 수행할 수 있도록 모델을 변경할 수 있다. 일 실시예에서, 기기 최적화부는 구동 장치의 부동 소수점(floating point) 표현 방식에 맞도록 인공지능 모델과 관련된 데이터의 포맷을 변환할 수 있다. 실수(소수점으로 표현되는 숫자)는 다양한 방법으로 표현할 수 있는데, PC의 경우에 일반적으로 IEEE에서 정의한 754 표준을 따른다. IEEE 756 표준의 16-bit, 32-bit 또는 64-bit로 실수를 표현할 수 있을 뿐만 아니라, 그 외에도 binary floating point와 같이 빠른 계산을 위한 표기 방법이 존재하는 등 다양한 표현 방법이 존재한다. 구동 장치에 따라서 빠른 속도로 계산할 수 있는 표현 방법 이 존재하고, 기기 최적화부는 구동 장치에서 가장 빠르게 계산할 수 있는 실수 표기 방법으로 데이터의 형태를 변환시킴으로써 기기 최적화를 수행할 수 있다. 일 실시예에서, 기기 최적화부는 2 이상의 단일 연산으로 구성된 복합 오퍼레이터를 2 이상의 단일 연산으 로 분리하는 복합 오퍼레이터 분리 프로세스를 수행할 수 있다. 예를 들면, Conv(Convolution) 오퍼레이터에서 파라미터가 정해진 상황에서 Conv 오퍼레이터를 그대로 사용하지 않고, 좀 더 단순한 몇가지 오퍼레이터로 분리 하여 성능을 높일 수 있다. 또한, 일 실시예에서, 기기 최적화부는 2 이상의 연속하는 단일 연산을 2 이상의 단일 연산을 한번에 수행 하는 통합 오퍼레이터로 대체하는 오퍼레이터 대체 프로세스를 수행할 수 있다. 예를 들면, 곱셈 연산과 덧셈 연산이 연달아 있는 경우, 구동 장치에 곱셈과 덧셈을 한번에 수행하는 특화된 연산기가 존재할 때, 기기 최적 화부는 곱셈과 덧셈을 한번에 수행하는 오퍼레이터로 대체하여 모델을 단순화하고, 성능을 높일 수 있다. 도 3에는 도시되어 있지 않지만, 인공지능 모델 변환 장치는 표준화된 인공지능 모델에서 정의된 오퍼레이 터와 상이한 사용자 정의 오퍼레이터를 규정하고, 사용자 정의에 따른 사용자 정의 오퍼레이터를 실행시키는 사 용자 정의 오퍼레이터부를 더 포함할 수 있다. 인공지능 모델은 빠르게 변화하기 때문에 ONNX 등의 표준에서 수 용되지 않는 새로운 오퍼레이터를 사용해야 하는 경우가 있다. 본 발명의 일 실시예에서는 사용자 특화된 사용 자 정의 오퍼레이터를 추가하여 표준 모델에 없는 기능을 확장할 수 있다. 기기 포맷 변환부는 기기 최적화부의 지시에 따라 배포할 실행 코드를 만든다. 도 6은 인공지능 오픈 플랫폼 아키텍처를 나타내는 예시도이다. 도 6을 참조하면, 인공지능 오픈 플랫폼 아키텍처는 인공지능 오픈 플랫폼에서 요구되는 인공신경망 모델, 계산 그래프, 입출력 노드, 텐서, 오퍼레이터인 연산자, 변수를 정의한다. 본 발명의 일 실시예에 따른 인공지능 모델 변환 장치 및 방법은 인공지능 오픈 플랫폼 아키텍처인 표준을 따르 며, 표준을 준수하는 실행 코드를 만들 수 있다.도 7은 오퍼레이터인 연산자를 나타내는 예시도이다. 도 7을 참조하면, 연산자는 Convolution 연산을 수행하는 Conv, Element wise ReLU 연산을 수행하는 Relu, Element wise Normalization을 수행하는 BatchNormalization, 더하는 Add, 곱하는 Mul, 텐서 붙이는 Concat, Element wise LeakyReLU 연산을 수행하는 LeakyRelu, 텐서를 나누는 Slice, 텐서의 차원 크기를 변경하는 Reshape, Element wise 데이터 타입을 변환하는 Cast, 상수 텐서를 입력하는 Constant, MaxPool 연산을 수행하 는 MaxPool, 텐서의 차원을 늘리는 Unsqueeze, 텐서를 나누는 Div, 텐서를 회전하는 Transpose, 조건에 따라 분기하는 If, 텐서의 차원을 줄이는 Squeeze, 텐서를 복사하는 Identity, 빼는 Sub, 텐서로 표현된 이미지의 크 기를 줄이는 Downsample을 포함한다. 도 8은 본 발명의 일 실시예에 따른 인공지능 모델 구동 장치 및 방법을 나타내는 블록도이다. 도 8을 참조하면 저전력 타겟은 입력부, 입력 변환부, 출력부, 출력 변환부, 모델 구 동부, 연산 가속부, 모델 해석부, 모델 저장소를 포함한다. 저전력 타겟은 온도, 고도, 지문 등 각종 센서, 이미지, 적외선 등 카메라, 라이더와 같은 센서를 포함하 는 입력 장치와, 모터, 공압, 유압 등 액추에이터를 포함하는 출력 장치와 상호 작용한다. 저전력 타겟의 입력부는 센서로부터 시계열 데이터를 입력하고, 입력 변환부는 시계열 데 이터를 인공지능 모델에 입력할 수 있도록 텐서로 변환한다. 텐서는 행렬이 3차원 이상으로 쌓인 형태로 데이터 를 처리한다. 입력 변환부는 센서에서 입력받은 이미지나 음성을 텐서 형태로 변환시켜준다. 모델 구동부는 텐서를 인공지능 모델에 적용해서 출력 데이터를 출력 변환부에 제공하고, 출력 변환 부는 출력 데이터를 액추에이터를 구동할 수 있도록 변환하여 출력값을 출력부에 제공한다. 모 델 구동부는 소프트웨어인 경우 인터프리터 형태이고, 하드웨어인 경우 FPGA나 ASIC 형태가 될 수 있다. 출력부는 출력값에 따라 액추에이터를 구동한다. 연산 가속부는 저전력 타겟에서 요구되는 데이터 정의, 연산자 정의, 저전력 타겟에 맞는 연산 가속 을 모델 구동부에 제공하여 모델 구동부가 연산 가속부의 연산 가속에 따라 인공지능 모델의 연 산을 수행한다. 연산 가속부는 연산자 중 특정 연산자를 병렬 연산이 가능하도록 다수 개 구비할 수 있다. Add라는 연산자는 수백개의 덧셈을 해야 하는데, 일반적인 CPU가 숫자를 하나씩 더하는데 반해 특화된 하드웨어 는 수십 개 단위로 병렬 덧셈을 할 수 있으며, 이 경우 수십배 성능 개선 효과를 얻을 수 있다. 모델 해석부는 모델 저장소에 저장된 실행 코드로부터 인공지능 모델을 읽어들여 모델 구동부에 저전력 타켓에 적합한 인공지능 모델을 제공한다. 이상에서 설명된 단계 또는 프로세스는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합에 의해 실행될 수 있다. 예를 들어, 실시예들에서 설명된 단계 또는 프로세스는, 예를 들어, 프로세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴퓨터, FPA(field programmable array), PLU(programmable logic unit), 마이크 로프로세서, 또는 명령(instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 실행될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에 서 수행되는 하나 이상의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가"}
{"patent_id": "10-2021-0095412", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "사용되는 것으로 설명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소(processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처 리 장치는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세 서(parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처 리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상장치(virtual equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(signal wave)에 영구적으로, 또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 하나 이상의 컴퓨터 판독 가능 기록 매 체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계 되고 구성된 것들이거나 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에 는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬 (ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치 가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리 터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 실시예 의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이 다."}
{"patent_id": "10-2021-0095412", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2021-0095412", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 인공지능 모델이 생성되는 프로세스를 설명하는 개념도이다. 도 2는 인공지능 오픈 플랫폼을 설명하는 예시도이다. 도 3은 본 발명의 일 실시예에 따른 인공지능 모델 변환 장치 및 방법을 나타내는 블록도이다. 도 4는 구글의 추론 모델의 예에서 추론에는 사용되지 않고, 학습 훈련 과정에서만 사용되는 부분을 도시한다. 도 5는 마이크로소프트의 MNIST CNN 모델의 예에서 매번 상수를 계산하는 부분을 도시한다. 도 6는 인공지능 오픈 플랫폼 아키텍처를 나타내는 예시도이다. 도 7는 오퍼레이터인 연산자를 나타내는 예시도이다. 도 8은 본 발명의 일 실시예에 따른 인공지능 모델 구동 장치 및 방법을 나타내는 블록도이다."}
