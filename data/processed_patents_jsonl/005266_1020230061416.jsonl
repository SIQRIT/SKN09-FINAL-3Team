{"patent_id": "10-2023-0061416", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0164064", "출원번호": "10-2023-0061416", "발명의 명칭": "신경 프로세싱 유닛의 순간 소모 전력을 낮추는 기술", "출원인": "주식회사 딥엑스", "발명자": "김녹원"}}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "시스템으로서,적어도 하나의 메모리와 그리고 ANN(artificial neural network) 모델들 중 적어도 하나를 위한 연산을 수행할수 있는 복수의 PE(processing element)를 포함하는 NPU(neural processing unit)를 포함하고,상기 복수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함하고, 그리고상기 복수의 PE들은 클럭 신호의 제1 부분에서 동작하도록 설정된 제1 그룹의 PE들과 상기 클럭 신호의 제2 부분에서 동작하도록 설정된 제2 그룹의 PE들을 포함하는, 시스템."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 부분은 상기 클럭 신호의 상승 엣지(rising edge) 부분이고, 상기 제2 부분은 상기 클럭 신호의 하강엣지(falling edge) 부분인, 시스템."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 제1 그룹의 PE들로부터의 제1 출력과 그리고 상기 제2 그룹의 PE들로부터의 제2 출력은상기 클럭 신호의 제1 부분을 기준으로 전달되는, 시스템."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 제1 그룹의 PE들과 상기 제2 그룹의 PE들의 기준 위상은 상기 클럭 신호의 상승 엣지에서상기 클럭 신호의 하강 엣지로 변환되거나 혹은 상기 클럭 신호의 하강 엣지에서 상기 클럭 신호의 상승 엣지로변환되는, 시스템."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 제2 그룹의 PE들을 위한 기준 위상은 상기 클럭 신호의 제2 부분에서 제1 부분으로 변환되는, 시스템."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 제2 그룹의 PE들의 출력 포트는 지연 버퍼에 연결되고,상기 지연 버퍼는 상기 클럭 신호의 상승 엣지에서 상기 클럭 신호의 하강 엣지로의 변환을 수행하거나 상기 클럭 신호의 하강 엣지에서 상기 클럭 신호의 상승 엣지로 변환을 수행하는, 시스템."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 제2 그룹의 PE들의 출력 포트는 지연 버퍼에 연결되고, 상기 지연 버퍼는 상기 클럭 신호의 제2 부분에서 상기 클럭 신호의 제1 부분으로의 변환을 수행하거나 상기 클럭 신호의 제1 부분에서 상기 클럭 신호의 제2 부분으로 변환을 수행하는, 시스템."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 NPU는 상기 제1 및 제2 그룹의 PE들의 출력 포트와 연결되는 SFU(special function unit)을 더 포함하는, 시스템."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "공개특허 10-2024-0164064-3-제8항에 있어서, 상기 SFU는 상기 적어도 하나의 메모리에 연결되는, 시스템."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서, 상기 제1 및 제2 그룹의 PE들은 시간 가변적인, 시스템."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "SoC(system-on-chip)로서, 적어도 하나의 메모리를 위해 배치된 제1 회로와; 그리고ANN(artificial neural network) 모델들 중 적어도 하나를 위한 연산을 수행할 수 있는 복수의 PE(processingelement)를 위해 배치된 제2 회로를 포함하고,상기 복수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함하고, 그리고상기 복수의 PE들은 클럭 신호의 제1 부분에서 동작하도록 설정된 제1 그룹의 PE들과 상기 클럭 신호의 제2 부분에서 동작하도록 설정된 제2 그룹의 PE들을 포함하는, SoC."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 제1 부분은 상기 클럭 신호의 상승 엣지(rising edge) 부분이고, 상기 제2 부분은 상기 클럭 신호의 하강엣지(falling edge) 부분인, SoC."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서 상기 제1 그룹의 PE들로부터의 제1 출력과 그리고 상기 제2 그룹의 PE들로부터의 제2 출력은상기 클럭 신호의 제1 부분을 기준으로 전달되는, SoC."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서, 상기 제1 그룹의 PE들과 상기 제2 그룹의 PE들의 기준 위상은 상기 클럭 신호의 상승 엣지에서 상기 클럭 신호의 하강 엣지로 변환되거나 혹은 상기 클럭 신호의 하강 엣지에서 상기 클럭 신호의 상승 엣지로 변환되는, SoC."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서, 상기 제2 그룹의 PE들을 위한 기준 위상은 상기 클럭 신호의 제2 부분에서 제1 부분으로 변환되는, SoC"}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항에 있어서, 상기 제2 그룹의 PE들의 출력 포트는 지연 버퍼에 연결되고,상기 지연 버퍼는 상기 클럭 신호의 상승 엣지에서 상기 클럭 신호의 하강 엣지로의 변환을 수행하거나 상기 클럭 신호의 하강 엣지에서 상기 클럭 신호의 상승 엣지로 변환을 수행하는, SoC."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항에 있어서, 상기 제2 그룹의 PE들의 출력 포트는 지연 버퍼에 연결되고, 상기 지연 버퍼는 상기 클럭 신호의 제2 부분에서 상기 클럭 신호의 제1 부분으로의 변환을 수행하거나 상기 클럭 신호의 제1 부분에서 상기 클럭 신호의 제2 부분으로 변환을 수행하는, SoC.공개특허 10-2024-0164064-4-청구항 18 NPU(neural processing unit)를 구동하는 방법으로서,복수의 PE들(processing elements)을 포함하는 NPU를 구동하는 클럭 신호를 생성하는 단계와; 상기 복수의 PE들 중 제1 그룹의 PE들을 상기 클럭 신호의 제1 부분에서 동작시키는 단계와; 그리고상기 복수의 PE들 중 제2 그룹의 PE들을 상기 클럭 신호의 제2 부분에서 동작시키는 단계를 포함하고,상기 복수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함하는, NPU 구동 방법."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 제1 부분은 상기 클럭 신호의 상승 엣지(rising edge) 부분이고, 상기 제2 부분은 상기 클럭 신호의 하강엣지(falling edge) 부분인, NPU 구동 방법."}
{"patent_id": "10-2023-0061416", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제18항에 있어서, 상기 제1 그룹의 PE들로부터의 제1 출력과 그리고 상기 제2 그룹의 PE들로부터의 제2 출력은상기 클럭 신호의 제1 부분을 기준으로 전달되는, NPU 구동 방법."}
{"patent_id": "10-2023-0061416", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 명세서의 일 개시는 시스템을 제공한다. 상기 시스템은 적어도 하나의 메모리와 그리고 ANN(artificial neural network) 모델들 중 적어도 하나를 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 포함 하는 NPU(neural processing unit)를 포함할 수 있다. 상기 복수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함할 수 있다. 상기 복수의 PE들은 클럭 신호의 제1 부분에서 동작하도록 설정 된 제1 그룹의 PE들과 상기 클럭 신호의 제2 부분에서 동작하도록 설정된 제2 그룹의 PE들을 포함할 수 있다."}
{"patent_id": "10-2023-0061416", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 신경 프로세싱 유닛의 순간 소모 전력을 낮추는 기술에 관한 한 것이다."}
{"patent_id": "10-2023-0061416", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(artificial intelligence: AI)도 점차 발전하고 있다. AI는 인간의 지능, 즉 인식(Recognition), 분 류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정(Control/Decision making) 등을 할 수 있는 지능을 인공적으로 모방하는 것을 의미한다. 또한, 최근에는 인공지능(AI)을 위한 연산 속도를 가속하기 위하여, 신경 프로세싱 유닛 (Neural processing unit; NPU)가 개발되고 있다. 인공지능 서비스의 목적에 따라 여러 타입의 인공신경망(ANN) 모델이 존재할 수 있다. 예를 들어, 입력되는 데 이터가 이미지 또는 영상인 경우, 인공지능 서비스를 위하여 이미지/영상 내의 객체 분류, 객체 검출, 객체 추 적 등을 위한 CNN 타입의 인공신경망 모델이 사용될 수 있다. 일반적으로, 인공신경망 모델은 레이어 마다 연산량이 다를 수 있다. 특히, 특정 레이어에서 연산량이 크게 증가하면, 전력 소모량이 순간적으로 증가할 수 있다."}
{"patent_id": "10-2023-0061416", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "인공신경망 연산은 데이터 인텐시브(intensive) 한 특성을 가진다. 특히, 인공신경망 연산은 병렬 처리 연산이 필요하다. 즉, 인공신경망 연산은 동시에 방대한 데이터를 빠른 속도로 병렬로 처리하지 못하면 처리 속도가 저 하되는 특성이 있다. 이에, 본 개시의 발명자들은 인공신경망 연산에 특화된 신경 프로세싱 유닛을 개발하였다. 본 개시의 발명자들 은 신경 프로세싱 유닛의 복수의 프로세싱 엘리먼트의 개수를 증가시켜서 신경 프로세싱 유닛의 병렬 처리 성능 을 향상시키고자 하였다. 또한 본 개시의 발명자들은 저전력 동작이 가능한 신경 프로세싱 유닛을 개발하고자 하였다. 한편, 엣지 디바이스에 설치되는 저전력 동작에 특화된 신경 프로세싱 유닛의 전원 공급부는 서버나 개인용 컴 퓨터(PC)에서 동작하는 그래픽 처리 장치(GPU)의 전원 공급부에 비해서 상대적으로 전원 공급 능력이 낮을 수 있다. 또한, 엣지 디바이스에 설치되는 저전력 동작에 특화된 신경 프로세싱 유닛의 전원 공급 부의 커패시턴스 의 용량이 순간적인 전원 공급을 감당하기에 부족할 수 있다. 하지만, 저전력 동작에 특화된 신경 프로세싱 유닛의 프로세싱 엘리먼트의 개수가 증가할수록, 순간적으로 공급 전압이 흔들리는 정도가 증가할 수 있다는 사실을 본 개시의 발명자들은 인식하였다. 부연 설명하면, 요구되는 신경 프로세싱 유닛의 순간 전원 공급량은 동작하는 프로세싱 엘리먼트의 개수에 비례할 수 있다. 또한, 인공신 경망 모델의 연산량은 각 레이어별로 편차가 상당하다. 따라서 인공신경망 모델의 레이어의 연산량에 따라서 병 렬로 동작하는 프로세싱 엘리먼트의 개수는 상이할 수 있다. 즉, 동시에 많은 프로세싱 엘리먼트들이 동작할 경 우, 순간적으로 신경 프로세싱 유닛의 전원 공급부의 전압이 강하될 수 있다. 이에, 본 개시의 발명자들은 프로세싱 엘리먼트의 개수가 증가하더라도 인공신경망 연산에 특화된 신경 프로세 싱 유닛의 공급 전압의 안정성을 개선시키는 것이 필요하다고 인식하였다. 따라서, 본 명세서의 개시들은 공급 전원 부의 공급 전압의 떨림을 안정화시키는 기술적 방안들을 제시하는 것 을 목적으로 한다."}
{"patent_id": "10-2023-0061416", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "전술한 목적을 달성하기 위하여, 본 명세서의 일 개시는 시스템을 제공한다. 상기 시스템은 적어도 하나의 메모 리와 그리고 ANN(artificial neural network) 모델들 중 적어도 하나를 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 포함하는 NPU(neural processing unit)를 포함할 수 있다. 상기 복수의 PE들은 가산 기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함할 수 있다. 상기 복수의 PE들은 클럭 신호 의 제1 부분에서 동작하도록 설정된 제1 그룹의 PE들과 상기 클럭 신호의 제2 부분에서 동작하도록 설정된 제2 그룹의 PE들을 포함할 수 있다. 상기 제1 부분은 상기 클럭 신호의 상승 엣지(rising edge) 부분이고, 상기 제2 부분은 상기 클럭 신호의 하강 엣지(falling edge) 부분일 수 있다. 상기 제1 그룹의 PE들로부터의 제1 출력과 그리고 상기 제2 그룹의 PE들로부터의 제2 출력은 상기 클럭 신호의 제1 부분을 기준으로 전달될 수 있다. 상기 제1 그룹의 PE들과 상기 제2 그룹의 PE들의 기준 위상은 상기 클럭 신호의 상승 엣지에서 상기 클럭 신호 의 하강 엣지로 변환되거나 혹은 상기 클럭 신호의 하강 엣지에서 상기 클럭 신호의 상승 엣지로 변환될 수 있 다. 상기 제2 그룹의 PE들을 위한 기준 위상은 상기 클럭 신호의 제2 부분에서 제1 부분으로 변환될 수 있다. 상기 제2 그룹의 PE들의 출력 포트는 지연 버퍼에 연결될 수 있다. 상기 지연 버퍼는 상기 클럭 신호의 상승 엣 지에서 상기 클럭 신호의 하강 엣지로의 변환을 수행하거나 상기 클럭 신호의 하강 엣지에서 상기 클럭 신호의 상승 엣지로 변환을 수행할 수 있다. 상기 제2 그룹의 PE들의 출력 포트는 지연 버퍼에 연결될 수 있다. 상기 지연 버퍼는 상기 클럭 신호의 제2 부 분에서 상기 클럭 신호의 제1 부분으로의 변환을 수행하거나 상기 클럭 신호의 제1 부분에서 상기 클럭 신호의 제2 부분으로 변환을 수행할 수 있다. 상기 NPU는 상기 제1 및 제2 그룹의 PE들의 출력 포트와 연결되는 SFU(special function unit)을 더 포함할 수 있다. 상기 SFU는 상기 적어도 하나의 메모리에 연결될 수 있다. 상기 제1 및 제2 그룹의 PE들은 시간 가변될 수 있다. 전술한 목적을 달성하기 위하여, 본 명세서의 일 개시는 SoC(system-on-chip)를 제공한다. 상기 SoC는 적어도 하나의 메모리를 위해 배치된 제1 회로와; 그리고 ANN(artificial neural network) 모델들 중 적어도 하나를 위 한 연산을 수행할 수 있는 복수의 PE(processing element)를 위해 배치된 제2 회로를 포함할 수 있다. 상기 복 수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함할 수 있다. 상기 복수의 PE들은 클럭 신호의 제1 부분에서 동작하도록 설정된 제1 그룹의 PE들과 상기 클럭 신호의 제2 부분에서 동작하 도록 설정된 제2 그룹의 PE들을 포함할 수 있다.상기 제1 부분은 상기 클럭 신호의 상승 엣지(rising edge) 부분이고, 상기 제2 부분은 상기 클럭 신호의 하강 엣지(falling edge) 부분일 수 있다. 상기 제1 그룹의 PE들로부터의 제1 출력과 그리고 상기 제2 그룹의 PE들로부터의 제2 출력은 상기 클럭 신호의 제1 부분을 기준으로 전달될 수 있다. 상기 제1 그룹의 PE들과 상기 제2 그룹의 PE들의 기준 위상은 상기 클럭 신호의 상승 엣지에서 상기 클럭 신호 의 하강 엣지로 변환되거나 혹은 상기 클럭 신호의 하강 엣지에서 상기 클럭 신호의 상승 엣지로 변환될 수 있 다. 상기 제2 그룹의 PE들을 위한 기준 위상은 상기 클럭 신호의 제2 부분에서 제1 부분으로 변환될 수 있다. 상기 제2 그룹의 PE들의 출력 포트는 지연 버퍼에 연결될 수 있다. 상기 지연 버퍼는 상기 클럭 신호의 상승 엣 지에서 상기 클럭 신호의 하강 엣지로의 변환을 수행하거나 상기 클럭 신호의 하강 엣지에서 상기 클럭 신호의 상승 엣지로 변환을 수행할 수 있다. 상기 제2 그룹의 PE들의 출력 포트는 지연 버퍼에 연결될 수 있다. 상기 지연 버퍼는 상기 클럭 신호의 제2 부 분에서 상기 클럭 신호의 제1 부분으로의 변환을 수행하거나 상기 클럭 신호의 제1 부분에서 상기 클럭 신호의 제2 부분으로 변환을 수행할 수 있다. 전술한 목적을 달성하기 위하여, 본 명세서의 일 개시는 NPU(neural processing unit)를 구동하는 방법을 제공 한다. 상기 NPU 구동 방법은 복수의 PE들(processing elements)을 포함하는 NPU를 구동하는 클럭 신호를 생성하 는 단계와; 상기 복수의 PE들 중 제1 그룹의 PE들을 상기 클럭 신호의 제1 부분에서 동작시키는 단계와; 그리고 상기 복수의 PE들 중 제2 그룹의 PE들을 상기 클럭 신호의 제2 부분에서 동작시키는 단계를 포함할 수 있다. 상 기 복수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함할 수 있다. 상기 제1 부분은 상기 클럭 신호의 상승 엣지(rising edge) 부분이고, 상기 제2 부분은 상기 클럭 신호의 하강 엣지(falling edge) 부분일 수 있다. 상기 제1 그룹의 PE들로부터의 제1 출력과 그리고 상기 제2 그룹의 PE들로부터의 제2 출력은 상기 클럭 신호의 제1 부분을 기준으로 전달될 수 있다."}
{"patent_id": "10-2023-0061416", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 예시들에 따르면, 인공신경망 연산이 클럭의 서로 다른 부분을 기준으로 분산 동작되게 되어, 순간 소모 전력을 낮출 수 있는 장점이 있다. 본 개시의 예시들에 따르면, 인공신경망 연산이 클럭의 서로 다른 부분을 기준으로 분산 동작되게 되어, 신경 프로세싱 유닛에 공급되는 공급 전압의 안전성을 개선할 수 있는 장점이 있다."}
{"patent_id": "10-2023-0061416", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시의 예시들의 특정한 구조적 내지 단계적 설명들은 단지 본 개시의 개념에 따른 예시를 설명하기 위한 것 이다. 따라서 본 개시의 개념에 따른 예시들은 다양한 형태로 실시될 수 있다. 본 개시의 개념에 따른 예시들은 다양한 형태로 실시될 수 있다. 본 개시는 본 개시의 예시들에 한정되는 것으로 해석되어서는 아니 된다. 본 개시의 개념에 따른 예시에 다양한 변경을 가할 수 있고 여러 가지 형태를 가질 수 있다. 이에, 특정 예시들 을 도면에 예시하고 본 개시 또는 출원에 대해서 상세하게 설명하고자 한다. 그러나, 이는 본 개시의 개념에 따 른 예시를 특정한 개시 형태에 대해 한정하려는 것이 아니다. 본 개시의 개념에 따른 여시는 본 개시의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1 및/또는 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만 사용될 수 있다. 상기 용어들은 본 개시의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제1 구성요소 는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있다. 하지만 복수의 구성요소들 중간에 다른 구성요소가 존재할 수도 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거 나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃하는\"과 \"~ 에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 본 개시에서 사용한 용어는 단지 특정한 예시를 설명하기 위해 사용된 것으로, 본 개시를 한정하려는 의도가 아 니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"포함하 다\" 또는 \"가지다\" 등의 용어는 서술된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존 재함을 지정하려는 것이다. 따라서 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미 를 가지는 것으로 해석되어야 한다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 본 개시에 서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 예시를 설명함에 있어서 본 개시가 속하는 기술 분야에 익히 알려져 있고 본 개시와 직접적으로 관련이 없는 기 술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 개시의 요지를 흐리지 않고 더욱 명확히 전달하기 위함이다. <용어의 정의> 이하, 본 개시에서 제시되는 개시들의 이해를 돕고자, 본 개시에서 사용되는 용어들에 대하여 간략하게 정리하 기로 한다. NPU: 신경망 프로세싱 유닛(Neural Processing Unit)의 약어로서, CPU(Central processing unit)과 별개로 인 공신경망모델의 연산을 위해 특화된 프로세서를 의미할 수 있다. ANN: 인공신경망(artificial neural network)의 약어로서, 인간의 지능을 모방하기 위하여, 인간 뇌 속의 뉴런 들(Neurons)이 시냅스(Synapse)를 통하여 연결되는 것을 모방하여, 노드들을 레이어(Layer: 계층) 구조로 연결 시킨, 네트워크를 의미할 수 있다. DNN: 심층 신경망(Deep Neural Network)의 약어로서, 보다 높은 인공 지능을 구현하기 위하여, 인공신경망의 은 닉 레이어의 개수를 늘린 것을 의미할 수 있다. CNN: 컨볼루션 신경망(Convolutional Neural Network)의 약어로서, 인간 뇌의 시각 피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망이다. 컨볼루션 신경망은 영상처리에 적합한 것으로 알려져 있으며, 입력 데이 터의 특징들을 추출하고, 특징들의 패턴을 파악하기에 용이한 것으로 알려져 있다. 이하, 첨부한 도면을 참조하여 본 개시의 바람직한 예시를 설명함으로써, 본 개시를 상세히 설명한다. 이하, 본 개시의 예시를 첨부된 도면을 참조하여 상세하게 설명한다. <인공지능> 인간은 인식(Recognition), 분류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정 (Control/Decision making) 등을 할 수 있는 지능을 갖추고 있다. 인공지능(artificial intelligence: AI)은 인간의 지능을 인공적으로 모방하는 것을 의미한다. 인간의 뇌는 뉴런(Neuron)이라는 수많은 신경세포로 이루어져 있다. 각각의 뉴런은 시냅스(Synapse)라고 불리는 연결부위를 통해 수백에서 수천 개의 다른 뉴런들과 연결되어 있다. 인간의 지능을 모방하기 위하여, 생물학적 뉴런의 동작원리와 뉴런 간의 연결 관계를 모델링한 것을, 인공신경망모델이라고 한다. 즉, 인공신경망은 뉴런 들을 모방한 노드들을 레이어(Layer: 계층) 구조로 연결시킨, 시스템이다. 이러한 인공신경망모델은 레이어 수에 따라 '단층 신경망'과 '다층 신경망'으로 구분한다. 일반적인 다층신경망 은 입력 레이어와 은닉 레이어, 출력 레이어로 구성된다. 입력 레이어(input layer)은 외부의 자료들을 받 아들이는 레이어로서, 입력 레이어의 뉴런 수는 입력되는 변수의 수와 동일하다. 은닉 레이어(hidden layer)은 입력 레이어와 출력 레이어 사이에 위치하며 입력 레이어로부터 신호를 받아 특성을 추출하여 출력층 으로 전달한다. 출력 레이어(output layer)은 은닉 레이어로부터 신호를 받아 외부로 출력한다. 뉴런 간의 입력신호는 0에서 1 사이의 값을 갖는 각각의 연결강도와 곱해진 후 합산된다. 이 합이 뉴런의 임계치보다 크면 뉴런이 활성화되어 활성화 함수를 통하여 출력 값으로 구현된다. 한편, 보다 높은 인공 지능을 구현하기 위하여, 인공신경망의 은닉 레이어의 개수를 늘린 것을 심층 신경망 (Deep Neural Network, DNN)이라고 한다. DNN은 다양한 구조로 개발되고 있다. 예를 들면, DNN의 일 예시인 합성곱 신경망(convolutional neural network, CNN)은 입력 값 (영상 또는 이미지)의 특징들을 추출하고, 추출된 출력 값의 패턴을 파악하기에 용이 한 것으로 알려져 있다. CNN은 합성곱 연산, 활성화 함수 연산, 풀링(pooling) 연산 등이 특정 순서로 처리되는 형태로 구성될 수 있다. 예를 들면, DNN의 레이어 각각에서, 파라미터(i.e., 입력 값, 출력 값, 가중치 또는 커널 등)는 복수의 채널로 구성된 행렬일 수 있다. 파라미터는 합성곱 또는 행렬 곱셈으로 NPU에서 처리될 수 있다. 각 레이어에서 연산이 처리된 출력 값이 생성된다. 예를 들면, 트랜스포머(transformer)는 어텐션(attention) 기술에 기반한 DNN이다. 트랜스포머는 행렬 곱셈 (matrix multiplication) 연산을 다수 활용한다. 트랜스포머는 입력 값과 쿼리(query; Q), 키(key; K), 및 값 (value; V) 등의 파라미터를 사용하여 출력 값인 어텐션(Q,K,V)를 획득할 수 있다. 트랜스포머는 출력 값 (즉, 어텐션(Q,K,V))에 기초하여 다양한 추론 연산을 처리할 수 있다. 트랜스포머는 CNN 보다 더 우수한 추론 성능을 보여주는 경향이 있다. 도 1은 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 이하 신경망 프로세싱 유닛에서 작동될 수 있는 예시적인 인공신경망모델(110a)의 연산에 대하여 설명한다. 도 1의 예시적인 인공신경망모델(110a)은 객체 인식, 음성 인식 등 다양한 추론 기능을 수행하도록 학습된 인공 신경망일 수 있다.인공신경망모델(110a)은 심층 신경망(DNN, Deep Neural Network)일 수 있다. 단, 본 개시의 예시들에 따른 인공신경망모델(110a)은 심층 신경망에 제한되지 않는다. 예를 들어, 인공신경망모델(110a)은 Transformer, YOLO, CNN, PIDNet, BiseNet, RCNN, VGG, VGG16, DenseNet, SegNet, DeconvNet, DeepLAB V3+, U-net, SqueezeNet, Alexnet, ResNet18, MobileNet-v2, GoogLeNet, Resnet- v2, Resnet50, Resnet101, Inception-v3 등의 모델로 구현될 수 있다. 단, 본 개시는 상술한 모델들에 제한되 지 않는다. 또한 인공신경망모델(110a)은 적어도 두 개의 서로 다른 모델들에 기초한 앙상블 모델일 수도 있다. 이하 예시적인 인공신경망모델(110a)에 의해서 수행되는 추론 과정에 대해서 설명하기로 한다. 인공신경망모델(110a)은 입력 레이어(110a-1), 제1 연결망(110a-2), 제1 은닉 레이어(110a-3), 제2 연결망 (110a-4), 제2 은닉 레이어(110a-5), 제3 연결망(110a-6), 및 출력 레이어(110a-7)을 포함하는 예시적인 심층 신경망 모델이다. 단, 본 개시는 도 1에 도시된 인공신경망모델에만 제한되는 것은 아니다. 제1 은닉 레이어 (110a-3) 및 제2 은닉 레이어(110a-5)는 복수의 은닉 레이어로 지칭되는 것도 가능하다. 입력 레이어(110a-1)는 예시적으로, x1 및 x2 입력 노드를 포함할 수 있다. 즉, 입력 레이어(110a-1)는 2개의 입력 값에 대한 정보를 포함할 수 있다. 제1 연결망(110a-2)은 예시적으로, 입력 레이어(110a-1)의 각각의 노드를 제1 은닉 레이어(110a-3)의 각각의 노 드로 연결시키기 위한 6개의 가중치 값에 대한 정보를 포함할 수 있다. 각각의 가중치 값은 입력 노드 값과 곱 해지고, 곱해진 값들의 누산된 값이 제1 은닉 레이어(110a-3)에 저장된다. 가중치 값과 입력 노드 값은 인공신 경망모델의 파라미터로 지칭될 수 있다. 제1 은닉 레이어(110a-3)는 예시적으로 a1, a2, 및 a3 노드를 포함할 수 있다. 즉, 제1 은닉 레이어(110a-3)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 도 3의 제1 프로세싱 엘리먼트(PE1)는 a1 노드의 연산을 처리할 수 있다. 도 3의 제2 프로세싱 엘리먼트(PE2)는 a2 노드의 연산을 처리할 수 있다. 도 3의 제3 프로세싱 엘리먼트(PE3)는 a3 노드의 연산을 처리할 수 있다.제2 연결망(110a-4)은 예시적으로, 제1 은닉 레이어(110a-3)의 각각의 노드를 제2 은닉 레이어(110a-5)의 각각의 노드로 연결시키기 위한 9개의 가중치 값에 대한 정보를 포함할 수 있다. 제2 연결망(110a-4)의 가중치 값은 제1 은닉 레이어(110a-3)로부터 입력되 는 노드 값과 각각 곱해지고, 곱해진 값들의 누산된 값이 제2 은닉 레이어(110a-5)에 저장된다. 제2 은닉 레이어(110a-5)는 예시적으로 b1, b2, 및 b3 노드를 포함할 수 있다. 즉, 제2 은닉 레이어(110a-5)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 도 3의 제4 프로세싱 엘리먼트(PE4)는 b1 노드의 연산을 처리할 수 있다. 도 3의 제5 프로세싱 엘리먼트(PE5)는 b2 노드의 연산을 처리할 수 있다. 도 3의 제6 프로세싱 엘리먼트(PE6)는 b3 노드의 연산을 처리할 수 있다. 제3 연결망(110a-6)은 예시적으로, 제2 은닉 레이어(110a-5)의 각각의 노드와 출력 레이어(110a-7)의 각각의 노 드를 연결하는 6개의 가중치 값에 대한 정보를 포함할 수 있다. 제3 연결망(110a-6)의 가중치 값은 제2 은닉 레 이어(110a-5)로부터 입력되는 노드 값과 각각 곱해지고, 곱해진 값들의 누산된 값이 출력 레이어(110a-7)에 저 장된다. 출력 레이어(110a-7)는 예시적으로 y1, 및 y2 노드를 포함할 수 있다. 즉, 출력 레이어(110a-7)는 2개의 노드 값에 대한 정보를 포함할 수 있다. 도 3의 제7 프로세싱 엘리먼트(PE7)는 y1 노드의 연산을 처리할 수 있다. 도 3의 제8 프로세싱 엘리먼트(PE8)는 y2 노드의 연산을 처리할 수 있다. 도 2a은 컨볼루션 신경망(CNN)의 기본 구조를 설명하기 위한 도면이다. 도 2a을 참조하면, 입력 이미지는 특정 사이즈(size)의 행과 특정 사이즈의 열로 구성된 2차원적 행렬로 표시될 수 있다. 입력 이미지는 복수의 채널을 가질 수 있는데, 여기서 채널은 입력 데이터 이미지의 컬러 성분의 수를 나타낼 수 있다. 컨볼루션 과정은 입력 이미지를 지정된 간격으로 순회하면서 커널과 합성곱 연산을 수행하는 것을 의미한다. 컨볼루션 신경망은 현재 레이어의 출력 값(합성곱 또는 행렬 곱셈)을 다음 레이어의 입력 값으로 전달하는 구조 를 가질 수 있다. 예를 들면, 합성곱(컨볼루션)은, 두 개의 주요 파라미터(입력 특징맵 및 커널)에 의해 정의된다. 파라미터는 입 력 특징맵, 출력 특징맵, 활성화 맵, 가중치, 커널, 및 어텐션(Q,K,V) 등을 포함할 수 있다, 합성곱(컨볼루션)은 입력 특징맵 위로 커널 윈도우를 슬라이딩 한다. 커널이 입력 특징맵을 슬라이딩 하는 단차 사이즈를 보폭(stride)이라고 한다. 합성곱 이후에는 풀링(pooling)이 적용될 수 있다. 또한, 컨볼루션 신경망의 끝단에는 FC (fully-connected)레 이어가 배치될 수 있다. 도 2b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 2b을 참조하면, 예시적으로 입력 이미지가 6 x 6 크기를 갖는 2차원적 행렬인 것으로 나타나 있다. 또한, 도 2b에는 예시적으로 3개의 노드, 즉 채널 1, 채널 2, 채널 3이 사용되는 것으로 나타내었다. 먼저, 합성곱 동작에 대해서 설명하기로 한다. 입력 이미지(도 2b에서는 예시적으로 6 x 6 크기인 것으로 나타내어짐)는 첫 번째 노드에서 채널 1을 위한 커널 1(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)과 합성곱되고, 그 결과로서 특징맵1(도 2b에서는 예시적으로 4 x 4 크기인 것으로 나타내어짐)이 출력된다. 또한, 상기 입력 이미지(도 2b에서는 예시적으로 6 x 6 크기인 것으로 나타내어짐)는 두 번째 노드에서 채널 2를 위한 커널 2(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)와 합성곱되고 그 결과로서 특징맵 2(도 2b에서는 예시적으로 4 x 4 크기인 것으로 나타내어 짐)가 출력된다. 또한, 상기 입력 이미지는 세 번째 노드에서 채널 3을 위한 커널 3(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)과 합성곱되고, 그 결과로서 특징맵3(도 2b에서는 예시적으로 4 x 4 크기인 것으 로 나타내어짐)이 출력된다. 각각의 합성곱을 처리하기 위해서 NPU의 프로세싱 엘리먼트들(PE1 to PE12)은 MAC 연산을 수행하도록 구성 된다. 다음으로, 활성화 함수의 동작에 대해서 설명하기로 한다. 합성곱 동작으로부터 출력되는 특징맵1, 특징맵2 그리고 특징맵3(도 2b에서는 각각의 크기는 예시적으로 4 x 4 인 것으로 나타내어짐)에 대해서 활성화 함수가 적용될 수 있다. 활성화 함수가 적용되고 난 이후의 출력은 예 시적으로 4 x 4의 크기일 수 있다. 다음으로, 폴링(pooling) 동작에 대해서 설명하기로 한다. 상기 활성화 함수로부터 출력되는 특징맵1, 특징맵2, 특징맵3(도 2b에서는 각각의 크기는 예시적으로 4 x 4인 것으로 나타내어짐)은 3개의 노드로 입력된다. 활성화 함수로부터 출력되는 특징맵들을 입력으로 받아서 폴링 (pooling)을 수행할 수 있다. 상기 폴링이라 함은 크기를 줄이거나 행렬 내의 특정 값을 강조할 수 있다. 폴링 방식으로는 최대값 폴링과 평균 폴링, 최소값 폴링이 있다. 최대값 폴링은 행렬의 특정 영역 안에 값의 최댓값 을 모으기 위해서 사용되고, 평균 폴링은 특정 영역내의 평균을 구하기 위해서 사용될 수 있다. 도 2b의 예시에서는 4 x 4 크기의 특징맵이 폴링에 의하여 2 x 2 크기로 줄어지는 것으로 나타내었다. 구체적으로, 첫 번째 노드는 채널 1을 위한 특징맵1을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출력한다. 두 번째 노드는 채널 2을 위한 특징맵2을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출 력한다. 세 번째 노드는 채널 3을 위한 특징맵3을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출력 한다. 전술한 합성곱, 활성화 함수과 폴링이 반복되고 최종적으로는, 도 8a과 같이 fully connected로 출력될 수 있다. 해당 출력은 다시 이미지 인식을 위한 인공신경망으로 입력될 수 있다. 단, 본 개시는 특징맵, 커널의 크 기에 제한되지 않는다. 지금까지 설명한 CNN은 다양한 심층신경망(DNN) 방법 중에서도 컴퓨터 비전(Vision) 분야에서 가장 많이 쓰이는 방법이다. 특히, CNN은 이미지 분류(image classification) 및 객체 검출(objection detection)과 같은 다양한작업을 수행하는 다양한 연구 영역에서 놀라운 성능을 보였다. <ANN의 연산을 위해 필요한 하드웨어 자원> 도 3은 본 개시에 따른 신경 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 3에 도시된 신경 프로세싱 유닛(neural processing unit, NPU)은 인공신경망을 위한 동작을 수행하도록 특화된 프로세서이다. 인공신경망은 여러 입력 또는 자극이 들어오면 각각 가중치를 곱해 더해주고, 추가적으로 편차를 더한 값을 활 성화 함수를 통해 변형하여 전달하는 인공 뉴런들이 모인 네트워크를 의미한다. 이렇게 학습된 인공신경망은 입 력 데이터로부터 추론(inference) 결과를 출력하는데 사용될 수 있다. NPU는 전기/전자 회로로 구현된 반도체일 수 있다. 전기/전자 회로라 함은 수많은 전자 소자, (예컨대 트렌지스터, 커패시터)를 포함하는 것을 의미할 수 있다. Transformer 및/또는 CNN 기반의 인공신경망모델인 경우, NPU는 행렬 곱셈 연산, 합성곱 연산, 등을 인공 신경망의 구조(architecture)에 따라 선별하여, 처리할 수 있다. 예를 들어, 합성곱 신경망(CNN)의 레이어 각각에서, 입력 데이터에 해당하는 입력 특징맵(Input feature map)과 가중치(Weight)에 해당하는 커널(kernel)은 복수의 채널로 구성된 행렬일 수 있다. 입력 특징맵과 커널의 합성 곱 연산이 수행되며, 각 채널에서 합성곱 연산과 풀링 출력 특징맵(output feature map)이 생성된다. 출력 특징 맵에 활성화 함수를 적용하여 해당 채널의 활성화맵(activation map)이 생성된다. 이후, 활성화맵에 대한 풀링 이 적용될 수 있다. 여기서 포괄적으로 활성화맵은 출력 특징맵으로 지칭될 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않으며, 출력 특징맵은 행렬 곱셈 연산 또는 합성곱 연산등이 적용된 것 을 의미한다. 부연 설명하면, 본 개시의 예시들에 따른 출력 특징맵은 포괄적인 의미로 해석되어야 한다. 예를 들면, 출력 특 징맵은 행렬 곱셈 연산 또는 합성곱 연산 결과값일 수 있다. 이에, 복수의 프로세싱 엘리먼트는 추가 알고 리즘을 위한 처리 회로부를 더 포함하도록 변형 실시되는 것도 가능하다. NPU는 상술한 인공신경망 연산에 필요한 합성곱 및 행렬 곱셈을 처리하기 위한 복수의 프로세싱 엘리먼트 를 포함하도록 구성될 수 있다. NPU는 상술한 인공신경망 연산에 필요한 행렬 곱셈 연산, 합성곱 연산, 활성화 함수 연산, 풀링 연산, 스 트라이드 연산, 배치 정규화 연산, 스킵 커넥션 연산, 접합 연산, 양자화 연산, 클리핑 연산, 패딩 연산에 최적 화된 각각의 처리 회로를 포함하도록 구성될 수 있다. 예를 들면, NPU는 상술한 알고리즘들 중 활성화 함수 연산, 풀링 연산, 스트라이드 연산, 배치 정규화 연 산, 스킵 커넥션 연산, 접합 연산, 양자화 연산, 클리핑 연산, 패딩 연산 중 적어도 하나를 처리하기 위한 SFU를 포함하도록 구성될 수 있다. 구체적으로, NPU는 복수의 프로세싱 엘리먼트(processing element: PE) , SFU, NPU 내부 메모 리, NPU 컨트롤러, 및 NPU 인터페이스를 포함할 수 있다. 복수의 프로세싱 엘리먼트, SFU, NPU 내부 메모리, NPU 컨트롤러, 및 NPU 인터페이스 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 예컨대, 임의 회로는 복수의 프로세싱 엘리먼트으로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. NPU 컨트롤러는 NPU의 인공신경망 추론 동작을 제어하도록 구성된 제어부의 기능을 수행 하도록 구성될 수 있다. NPU는 복수의 프로세싱 엘리먼트 및 SFU에서 추론될 수 있는 인공신경망모델의 파라미터를 저장 하도록 구성된 NPU 내부 메모리, 및 복수의 프로세싱 엘리먼트, SFU, 및 NPU 내부 메모리(12 0)의 연산 스케줄을 제어하도록 구성된 스케줄러를 포함하는 NPU 컨트롤러를 포함할 수 있다. NPU는 SVC 또는 SFC를 이용한 인코딩 및 디코딩 방식에 대응되어 특징맵을 처리하도록 구성될 수 있다. 복수의 프로세싱 엘리먼트는 인공신경망을 위한 동작의 일부를 수행할 수 있다. SFU는 인공신경망을 위한 동작의 다른 일부를 수행할 수 있다. NPU는 복수의 프로세싱 엘리먼트와 SFU를 사용하여 인공신경망모델의 연산을 하드웨어적으로 가 속하도록 구성될 수 있다. NPU 인터페이스는 시스템 버스를 통해서 NPU와 연결된 다양한 구성요소들, 예컨대 메모리와 통신할 수 있다. NPU 컨트롤러는 신경 프로세싱 유닛의 추론 연산을 위한 복수의 프로세싱 엘리먼트의 연산, SFU의 연산 및 NPU 내부 메모리의 읽기 및 쓰기 순서를 제어하도록 구성된 스케줄러를 포함할 수 있 다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 복수의 프로세싱 엘리먼트, SFU, 및 NPU 내부 메모리를 제어하도록 구성될 수 있다. NPU 컨트롤러 내의 스케줄러는 복수의 프로세싱 엘리먼트 및 SFU에서 작동할 인공신경망모델의 구조를 분석하거나 또는 이미 분석된 정보를 제공받을 수 있다. 예를 들면, 인공신경망모델이 포함할 수 있는 인공신경망의 데이터는 각각의 레이어의 노드 데이터(즉, 특징맵), 레이어들의 배치 데이터, 지역성 정보 또는 구조에 대한 정보, 각각의 레이어의 노드를 연결하는 연결망 각각의 가중치 데이터 (즉, 가중치 커널) 중 적어 도 일부를 포함할 수 있다. 인공신경망의 데이터는 NPU 컨트롤러 내부에 제공되는 메모리 또는 NPU 내부 메모리에 저장될 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 NPU가 수행할 인공신경망모델의 연산 순서를 스케줄링 할 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 인공신경망모델의 레이어의 특징맵 및 가중치 데이터가 저장된 메모리 어드레스 값을 획득할 수 있다. 예를 들 면, NPU 컨트롤러 내의 스케줄러는 메모리에 저장된 인공신경망모델의 레이어의 특징맵 및 가중치 데이터 가 저장된 메모리 어드레스 값을 획득할 수 있다. 따라서 NPU 컨트롤러 내의 스케줄러는 구동할 인공신경 망모델의 레이어의 특징맵 및 가중치 데이터를 메인 메모리에서 가져와서 NPU 내부 메모리에 저장할 수 있 다. 각각의 레이어의 특징맵은 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. 각각의 가중치 데이터는 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보, 예를 들면, 인공신경망모델의 인공신경망의 레이어들의 배치 데이터, 지역성 정보 또는 구조에 대한 정보에 기초해서 복수 의 프로세싱 엘리먼트의 연산 순서를 스케줄링 할 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 스케줄링 하기 때문에, 일반적인 CPU의 스케줄링 개념과 다르게 동작할 수 있다. 일반적인 CPU의 스케줄링은 공 평성, 효율성, 안정성, 반응 시간 등을 고려하여, 최상의 효율을 낼 수 있도록 동작한다. 즉, 우선 순위, 연산 시간 등을 고려해서 동일 시간내에 가장 많은 프로세싱을 수행하도록 스케줄링 한다. 종래의 CPU는 각 프로세싱의 우선 순서, 연산 처리 시간 등의 데이터를 고려하여 작업을 스케줄링 하는 알고리 즘을 사용하였다. 이와 다르게 NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 에 기초하여 결정된 NPU의 프로세싱 순서대로 NPU를 제어할 수 있다. 더 나아가면, NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및/또는 사용하려는 신경 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 결정된 프로세싱 순서대로 NPU를 구동할 수 있다. 단, 본 개시는 NPU의 데이터 지역성 정보 또는 구조에 대한 정보에 제한되지 않는다. NPU 컨트롤러 내의 스케줄러는 인공신경망의 데이터 지역성 정보 또는 구조에 대한 정보를 저장하도록 구 성될 수 있다. 즉, NPU 컨트롤러 내의 스케줄러는 적어도 인공신경망모델의 인공신경망의 데이터 지역성 정보 또는 구조 에 대한 정보만 활용하더라도 프로세싱 순서를 결정할 수 있다. 더 나아가서, NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및 NPU의 데이터 지역성 정보 또는 구조에 대한 정보를 고려하여 NPU의 프로세싱 순서를 결정할 수 있다. 또한, 결정된 프로세싱 순서대로 NPU의 프로세싱 최적화도 가능하다. 복수의 프로세싱 엘리먼트는 인공신경망의 특징맵과 가중치 데이터를 연산하도록 구성된 복수의 프로세싱 엘리먼트들(PE1 to PE12)이 배치된 구성을 의미한다. 각각의 프로세싱 엘리먼트는 MAC (multiply and accumulate) 연산기 및/또는 ALU (Arithmetic Logic Unit) 연산기를 포함할 수 있다. 단, 본 개시에 따른 예시 들은 이에 제한되지 않는다. 각각의 프로세싱 엘리먼트는 추가적인 특수 기능을 처리하기 위해 추가적인 특수 기능 유닛을 선택적으로 더 포 함하도록 구성될 수 있다. 예를 들면, 프로세싱 엘리먼트(PE)는 배치-정규화 유닛, 활성화 함수 유닛, 인터폴레이션 유닛 등을 더 포함하 도록 변형 실시되는 것도 가능하다. SFU는 활성화 함수 연산, 풀링(pooling) 연산, 스트라이드(stride) 연산, 배치 정규화(batch- normalization) 연산, 스킵 커넥션(skip-connection) 연산, 접합(concatenation) 연산, 양자화(quantization) 연산, 클리핑(clipping) 연산, 패딩(padding) 연산 등을 인공신경망의 구조(architecture)에 따라 선별하여, 처리하도록 구성된 회로부를 포함할 수 있다. 즉, SFU는 복수의 특수 기능 연산 처리 회로 유닛들을 포함 할 수 있다. 도 5에서는 예시적으로 복수의 프로세싱 엘리먼트들이 도시되었지만, 하나의 프로세싱 엘리먼트 내부에 MAC을 대체하여, 복수의 곱셈기(multiplier) 및 가산기 트리(adder tree)로 구현된 연산기들이 병렬로 배치되어 구성 되는 것도 가능하다. 이러한 경우, 복수의 프로세싱 엘리먼트는 복수의 연산기를 포함하는 적어도 하나의 프로세싱 엘리먼트로 지칭되는 것도 가능하다. 복수의 프로세싱 엘리먼트는 복수의 프로세싱 엘리먼트들(PE1 to PE12)을 포함하도록 구성된다. 도 5에 도 시된 복수의 프로세싱 엘리먼트들(PE1 to PE12)은 단지 설명의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼 트들(PE1 to PE12)의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12)의 개수에 의해서 복수 의 프로세싱 엘리먼트의 사이즈 또는 개수가 결정될 수 있다. 복수의 프로세싱 엘리먼트의 사이즈는 N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 복수의 프로세싱 엘리먼트는 N x M 개의 프로세싱 엘리먼트를 포함할 수 있다. 즉, 프로세싱 엘리먼트는 1개 이상일 수 있다. 복수의 프로세싱 엘리먼트의 사이즈는 NPU이 작동하는 인공신경망모델의 특성을 고려하여 설계할 수 있다. 복수의 프로세싱 엘리먼트는 인공신경망 연산에 필요한 덧셈, 곱셈, 누산 등의 기능을 수행하도록 구성된 다. 다르게 설명하면, 복수의 프로세싱 엘리먼트는 MAC(multiplication and accumulation) 연산을 수행하 도록 구성될 수 있다. 이하 복수의 프로세싱 엘리먼트 중 제1 프로세싱 엘리먼트(PE1)를 예를 들어 설명한다. 도 4a는 본 개시에 적용될 수 있는 복수의 프로세싱 엘리먼트 중 하나의 프로세싱 엘리먼트를 설명하는 개략적 인 개념도이다. 본 개시의 일 예시에 따른 NPU는 복수의 프로세싱 엘리먼트, 복수의 프로세싱 엘리먼트에서 추 론될 수 있는 인공신경망모델을 저장하도록 구성된 NPU 내부 메모리 및 복수의 프로세싱 엘리먼트 및 NPU 내부 메모리를 제어하도록 구성된 NPU 컨트롤러를 포함하고, 복수의 프로세싱 엘리먼트는 MAC 연산을 수행하도록 구성되고, 복수의 프로세싱 엘리먼트는 MAC 연산 결과를 양자화해서 출력하도록 구 성될 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. NPU 내부 메모리은 메모리 사이즈와 인공신경망모델의 데이터 사이즈에 따라 인공신경망모델의 전부 또는 일부를 저장할 수 있다. 제1 프로세싱 엘리먼트(PE1)는 곱셈기, 가산기, 누산기, 및 비트 양자화 유닛을 포함할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않으며, 복수의 프로세싱 엘리먼트는 인공신경망의 연산 특성을 고려하여 변형 실시될 수도 있다. 곱셈기는 입력 받은 (N)bit 데이터와 (M)bit 데이터를 곱한다. 곱셈기의 연산 값은 (N+M)bit 데이터 로 출력된다. 곱셈기는 하나의 변수와 하나의 상수를 입력 받도록 구성될 수 있다. 누산기는 (L)loops 횟수만큼 가산기를 사용하여 곱셈기의 연산 값과 누산기의 연산 값을 누산 한다. 따라서 누산기의 출력부와 입력부의 데이터의 비트 폭은 (N+M+log2(L))bit로 출력될 수 있다. 여기서 L은 0보다 큰 정수이다. 누산기는 누산이 종료되면, 초기화 신호(initialization reset)를 입력 받아서 누산기 내부에 저장된 데이터를 0으로 초기화 할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 비트 양자화 유닛은 누산기에서 출력되는 데이터의 비트 폭을 저감할 수 있다. 비트 양자화 유닛 은 NPU 컨트롤러에 의해서 제어될 수 있다. 양자화된 데이터의 비트 폭은 (X)bit로 출력될 수 있다. 여기서 X는 0보다 큰 정수이다. 상술한 구성에 따르면, 복수의 프로세싱 엘리먼트는 MAC 연산을 수행하도 록 구성되고, 복수의 프로세싱 엘리먼트는 MAC 연산 결과를 양자화해서 출력할 수 있는 효과가 있다. 특히 이러한 양자화는 (L)loops가 증가할수록 소비 전력을 더 절감할 수 있는 효과가 있다. 또한 소비 전력이 저감되 면 발열도 저감할 수 있는 효과가 있다. 특히 발열을 저감하면 NPU의 고온에 의한 오동작 발생 가능성을 저감할 수 있는 효과가 있다. 비트 양자화 유닛의 출력 데이터(X)bit은 다음 레이어의 노드 데이터 또는 합성곱의 입력 데이터가 될 수 있다. 만약 인공신경망모델이 양자화되었다면, 비트 양자화 유닛은 양자화된 정보를 인공신경망모델에서 제공받도록 구성될 수 있다. 단, 이에 제한되지 않으며, NPU 컨트롤러는 인공신경망모델을 분석하여 양자 화된 정보를 추출하도록 구성되는 것도 가능하다. 따라서 양자화된 데이터 사이즈에 대응되도록, 출력 데이터 (X)bit를 양자화 된 비트 폭으로 변환하여 출력될 수 있다. 비트 양자화 유닛의 출력 데이터(X)bit는 양자 화된 비트 폭으로 NPU 내부 메모리에 저장될 수 있다. 본 개시의 일 예시에 따른 NPU의 복수의 프로세싱 엘리먼트는 곱셈기, 가산기, 및 누산기 를 포함한다. 비트 양자화 유닛은 양자화 적용 여부에 따라 취사 선택될 수 있다. 도 4b는 본 개시에 적용될 수 있는 SFU를 설명하는 개략적인 개념도이다. 도 4b를 참고하면 SFU는 여러 기능 유닛을 포함한다. 각각의 기능 유닛은 선택적으로 동작될 수 있다. 각 각의 기능 유닛은 선택적으로 턴-온되거나 턴-오프될 수 있다. 즉, 각각의 기능 유닛은 설정이 가능하다. 다시 말해서, SFU는 인공신경망 추론 연산에 필요한 다양한 회로 유닛들을 포함할 수 있다. 예를 들면, SFU의 회로 유닛들은 건너뛰고 연결하기(skip-connection) 동작을 위한 기능 유닛, 활성화 함 수(activation function) 동작을 위한 기능 유닛, 풀링(pooling) 동작을 위한 기능 유닛, 양자화 (quantization) 동작을 위한 기능 유닛, NMS(non-maximum suppression) 동작을 위한 기능 유닛, 정수 및 부동 소수점 변환(INT to FP32) 동작을 위한 기능 유닛, 배치 정규화(batch-normalization) 동작을 위한 기능 유닛, 보간법(interpolation) 동작을 위한 기능 유닛, 연접(concatenation) 동작을 위한 기능 유닛, 및 바이아스 (bias) 동작을 위한 기능 유닛 등을 포함할 수 있다. SFU의 기능 유닛들은 인공신경망모델의 데이터 지역성 정보에 의해서 선택적으로 턴-온되거나 혹은 턴-오 프될 수 있다. 인공신경망모델의 데이터 지역성 정보는 특정 레이어를 위한 연산이 수행될 때, 해당 기능 유닛 의 턴-오프 혹은 턴-오프와 관련된 제어 정보를 포함할 수 있다. SFU의 기능 유닛들 중 활성화된 유닛은 턴-온 될 수 있다. 이와 같이 SFU의 일부 기능 유닛을 선택적 으로 턴-오프하는 경우, NPU의 소비 전력을 절감할 수 있다. 한편, 일부 기능 유닛을 턴-오프하기 위하여, 파워 게이팅(power gating)을 이용할 수 있다. 또는, 일부 기능 유닛을 턴-오프하기 위하여, 클럭 게이팅(clock gating)을 수행할 수도 있다. 도 5는 도 3에 도시된 NPU의 변형예를 나타낸 예시도이다. 도 5에 도시된 NPU는 도 3에 예시적으로 도시된 프로세싱 유닛과 비교하면, 복수의 프로세싱 엘리먼 트를 제외하곤 실질적으로 동일하기 때문에, 이하 단지 설명의 편의를 위해서 중복 설명은 생략할 수있다. 도 5에 예시적으로 도시된 복수의 프로세싱 엘리먼트는 복수의 프로세싱 엘리먼트들(PE1 to PE12) 외에, 각각의 프로세싱 엘리먼트들(PE1 to PE12)에 대응되는 각각의 레지스터 파일들(RF1 to RF12)을 더 포함할 수 있 다. 도 5에 도시된 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12)은 단지 설명 의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12) 의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12)의 개수에 의해서 복수의 프 로세싱 엘리먼트의 사이즈 또는 개수가 결정될 수 있다. 복수의 프로세싱 엘리먼트 및 복수의 레지스 터 파일들(RF1 to RF12)의 사이즈는 N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 복수의 프로세싱 엘리먼트의 어레이 사이즈는 NPU이 작동하는 인공신경망모델의 특성을 고려하여 설 계할 수 있다. 부연 설명하면, 레지스터 파일의 메모리 사이즈는 작동할 인공신경망모델의 데이터 사이즈, 요구 되는 동작 속도, 요구되는 소비 전력 등을 고려하여 결정될 수 있다. NPU의 레지스터 파일들(RF1 to RF12)은 프로세싱 엘리먼트들(PE1 to PE12)과 직접 연결된 정적 메모리 유 닛이다. 레지스터 파일들(RF1 to RF12)은 예를 들면, 플립플롭, 및/또는 래치 등으로 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 대응되는 프로세싱 엘리먼트들(PE1 to PE12)의 MAC 연산 값을 저장하도록 구성될 수 있 다. 레지스터 파일들(RF1 to RF12)은 NPU 내부 메모리와 가중치 데이터 및/또는 노드 데이터를 제공하거나 제공받도록 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 MAC 연산 시 누산기의 임시 메모리의 기능을 수행하도록 구성되는 것도 가능하 다. <본 특허의 발명자에 의해 찾아진 기술적 난관> 인공지능 서비스의 목적에 따라 여러 타입의 인공신경망(ANN) 모델이 존재할 수 있다. 예를 들어, 입력되는 데 이터가 이미지 또는 영상인 경우, 인공지능 서비스를 위하여 이미지/영상 내의 객체 분류, 객체 검출, 객체 추 적 등을 위한 CNN 타입의 인공신경망 모델이 사용될 수 있다. 일반적으로, 인공신경망 모델은 레이어 마다 연산량이 다를 수 있다. 이를 도 6을 참조하여 설명하기로 한다. 도 6a은 예시적인 인공신경망 모델 내의 각 레이어 별 데이터의 크기를 나타낸 예시도이고, 도 6b는 도 6a에 도 시된 예시적 인공신경망 모델에서 각 레이어 별 데이터 사이즈 등을 나타낸 예시적인 테이블이다. 도 6a에 도시된 예시적인 인공신경망 모델은 Mobilenet V1인 것으로 나타나 있다. 도 6a에 도시된 가로축은 예 시적인 인공신경망 모델에서 레이어들을 순차적으로 나타내고, 세로축은 데이터의 크기를 나타낸다. 도 6a에 도시된 레이어 1을 참고하면, 입력 특징맵의 크기(IFMAP_SIZE) 보다 출력 특징맵의 크기(OFMAP_SIZE)가 더 큰 것을 알 수 있다. 레이어 1의 출력 특징맵은 레이어 2로 전달되어, 상기 레이어 2의 입력 특징맵이 된다. 상기 레이어 2의 연산이 마쳐지면, 출력 특징맵이 출력된다. 상기 레이어 2의 출력 특징맵은 다시 레이어 3로 전달되어, 상기 레이어 3 의 입력 특징맵이 된다. 이처럼 각 레이어에 입력되는 입력 데이터의 크기와 각 레이어에서 출력되는 출력 특징맵의 크기는 상이할 수 있다. 이에 따라 임의 레이어에서는 연산량이 작을 수도 있지만 다른 레이어에서는 연산량이 매우 클 수 있다. 이처럼 레이어 별 연산량이 큰폭으로 변화됨에 따라, 전력 제어가 쉽지 않게 되는 문제가 발생한다. 특히, 특정 레이어에서 연산량이 크게 증가하면, 순간 전력 소모량이 증가함으로써, 시스템 안정성이 저하되는 문제가 발생할 수 있다는 것을 본 특허의 발명자가 인식하게 되었다. 예를 들어, 특정 레이어의 연산을 위해서 동시에 많은 프로세싱 엘리먼트들이 가동될 수 있다. 각각의 프로세싱 엘리먼트 구동을 위해서 소정의 전원이 필요하며, 상당한 개수의 프로세싱 엘리먼트들이 동시에 구동을 하면 순 간적으로 필요한 파워가 급증할 수 있다. 만약 신경 프로세싱 유닛이 저전력 동작에 특화되어 설계될 경우, 서 버용 신경 프로세싱 유닛보다 파워 공급 능력이 상대적으로 부족할 수 있다. 따라서, 이러한 엣지 디바이스용 신경 프로세싱 유닛은 순간 파워 공급 이슈에 상대적으로 더 취약할 수 있으며, 파워 공급량이 폭증할 경우, 공급 전압이 흔들릴 수 있다. 특히 공급 전압이 트랜지스터의 임계 전압 이하로 떨어질 경우, 트랜지스터에 저장 된 데이터가 손실될 수 있다. 다르게 설명하면, 공급 전압이 낮아 지는 경우 트랜지스터의 동작 속도가 저하되 어 setup/hold violation 문제가 발생해 오동작이 일어날 수 있다 다른 예를 들어, 인공신경망을 위한 연산, 즉 예컨대 가산(add), 곱셈(multiply), 누산(accumulate)을 수행하는 PE들이 전력 소모를 순간적으로 많이 사용함으로써, NPU 내의 다른 컴포넌트, 예컨대 내부 메모리에 는 충분한 전력이 공급되지 못할 수 있다는 것을 본 특허의 발명자가 인식하게 되었다. 구체적으로 내부 메모리 에 충분한 전력이 공급되지 못하면, 저장되어 있는 데이터 비트가 손상(compromise)될 가능성도 배제할 수 없는 문제점이 있을 수 있다는 것을 본 특허의 발명자가 인식하게 되었다. <본 특허의 개시들> 전술한 문제점은, 인공신경망 모델의 연산이 하드웨어적으로 하나의 클럭 신호를 기준으로 수행되기 때문에 발 생하는 것임을, 본 특허의 발명자가 인식하게 되었다. 따라서, 본 특허의 발명자는, 클럭 신호를 여러 부분으로 구분하여 나눈 후, 인공신경망 모델의 연산들이 클럭 신호의 여러 부분들을 기준으로 분산 수행되도록 함으로써, 전술한 문제점을 해결하는 기법을 발명하게 되었다. 본 특허의 발명자가 발명한 기법이 구현되는 여러 실시예들을 이하 도면을 참고하여 설명하기로 한다. 도 7a는 본 명세서의 제1 개시의 일 예시에 따른 NPU의 구조를 나타낸 예시도이다. 도 7a에서는 NPU가 제1 그룹의 PE(110a)와 제2 그룹의 PE(110b)와 내부 메모리와 SFU 그리고 클럭 생성기를 포함하는 것으로 도시되어 있으나, 상기 NPU는 그 외에도 도 3 또는 도 5에 도시된 바 와 같이 NPU 컨트롤러 그리고 NPU 인터페이스를 더 포함할 수 있다. 상기 제1 그룹의 PE(110a), 제2 그룹의 PE(110b), 상기 내부 메모리, NPU 컨트롤러, 상기 NPU 인터페이스, SFU 그리고 클 럭 생성기 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으 로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 클럭 생성기는 클럭 신호 공급 회로로 지칭될 수 있다. 예컨대, 임의 회로는 복수의 프로세싱 엘리먼트으로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. 도 7a에 도시된 제1 그룹의 PE(110a)와 제2 그룹의 PE(110b)는 도 3 또는 도 5에 도시된 복수의 PE들이 나뉜 것으로 이해될 수 있다. 각 그룹에 속한 PE들의 개수는 도 7a에서는 예시적으로 8개인 것으로 나타나 있지 만, 각 그룹에 속한 PE들의 개수는 달라질 수 있는 것은 자명할 것이다. 또한, 각 그룹에 속한 PE들의 개수는 반-고정적으로 혹은 동적으로 변경될 수 있다. 예를 들어, 제1 그룹의 PE들은 10개의 PE들을 포함하고, 제2 그 룹의 PE들은 6개의 PE들을 포함하도록 변경될 수 있다. 이러한 변경은 NPU 컨트롤러의 제어에 따라 수행될 수 있다. 도 7a에 도시된 클럭 생성기는 오실레이터(Oscillator)를 포함할 수 있다. 또한, 상기 오실레이터는 PWM(Pulse Width Modulator)를 포함할 수 있다. 상기 PWM은 클럭의 사이클을 Duty Cycle를 변경함으로써 high 신호와 low 신호의 비율을 조정할 수 있다. 상기 클럭 생성기가 신호를 생성하여 출력하면, 제1 그룹의 PE들(110a)과 제2 그룹의 PE들(100b)로 전달되 게 된다. 이때, 상기 제1 그룹의 PE들(100a)은 상기 클럭의 제1 부분(예컨대, rising edge)를 기준으로 동작하도록 설정 되고, 상기 제2그룹의 PE들(100b)은 상기 클럭의 제2부분(예컨대, falling edge)를 기준으로 동작하도록 설정될 수 있다. 도 3 또는 도 5에 도시된 복수의 PE들이 도 7a에 도시된 바와 같이 제1 그룹의 PE(110a)와 제2 그룹의 PE(110b)로 나뉘게 되고, 상기 제1 그룹의 PE(110a)와 상기 제2 그룹의 PE(110b)이 상기 클럭의 서로 다른 부분 을 기준으로 분산 동작되게 하면, 순간 소모 전력을 낮출 수 있는 장점이 있다. 도 7b는 본 명세서의 제1 개시의 다른 예시에 따른 NPU의 구조를 나타낸 예시도이다. 도 7b에서는 NPU가 제1 그룹의 PE(110a)와 제2 그룹의 PE(110b)와 내부 메모리와 SFU를 포함하 는 것으로 도시되어 있으나, 상기 NPU는 그 외에도 도 3 또는 도 5에 도시된 바와 같이 NPU 컨트롤러그리고 NPU 인터페이스를 더 포함할 수 있다. 상기 제1 그룹의 PE(110a), 제2 그룹의 PE(110b), 상기 내부 메모리, NPU 컨트롤러, 상기 NPU 인터페이스, 그리고 SFU 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 도 7b를 참조하면, 상기 클럭 생성기는 NPU 내부가 아니라 외부에 위치하는 것으로 도시되어 있다. 그 외에는 도 7a에 도시된 것과 동일하므로, 도 7b에 대해서는 별도로 설명하지 않기로 하고, 도 7a를 참조하여 설명한 내용을 그대로 원용하기로 한다. 한편, 도 7a 및 도 7b에 도시된 상기 제1 그룹의 PE들(110a)과 상기 제2 그룹의 PE들(100b)이 인공신경망 모델 의 임의 레이어에 대한 연산을 수행 완료하면, 상기 제1 그룹의 PE들(110a)과 상기 제2 그룹의 PE들(100b)은 연 산 결과, 즉 출력 특징맵을 상기 클럭의 제1 부분 및 제2 부분 중에서 어느 하나의 부분에 맞추어서 출력할 수 있다. 이를 위하여, 상기 제1 그룹의 PE들(110a)과 상기 제2 그룹의 PE들(100b) 중 적어도 하나 이상은 시간 지 연 버퍼(예컨대, 쉬프트 레지스터)를 더 포함할 수 있다. 상기 시간 지연 버퍼(예컨대, 쉬프트 레지스터)는 상 기 출력이 상기 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 더 늦은 부분에 맞추어서 수행되게끔, 시간 지 연을 수행할 수 있다. 예를 들어 상기 제2 그룹의 PE들(110b)의 출력 포트는 시간 지연 버퍼(예컨대, 쉬프트 레지스터)에 연결될 수 있다. 이 경우, 상기 지연 버퍼는 상기 클럭의 제1 부분(예컨대 상승 엣지)에서 상기 클럭의 제2 부분(예컨대 하강 엣지)로의 변환을 수행하거나 상기 클럭의 제2 부분(예컨대 하강 엣지)에서 상기 클럭의 제1 부분(예컨대 상승 엣지)로 변환을 수행할 수 있다. 대안적으로, 도 7a 및 도 7b에 도시된 상기 제1 그룹의 PE들(110a)은 제1 출력 특징맵을 상기 클럭의 제1 부분 에 맞추어서 출력하고, 상기 제2 그룹의 PE들(110b)은 제2 출력 특징맵을 상기 클럭의 제2 부분에 맞추어서 출 력할 수 있다. 이 경우, 상기 SFU가 시간 지연 버퍼(예컨대, 쉬프트 레지스터)를 포함할 수 있다. 상기 SFU는 상기 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 더 빠른 부분에 맞추어서 전달받은 출력 특징 맵을 시간 지연시킴으로써, 상기 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 늦은 부분에 맞추어서 전달받 게 될 출력 특징맵과 시간이 동기 되도록 할 수 있다. 대안적으로, 상기 제1 그룹의 PE들(110a)에 의해서 출력되는 제1 출력 특징맵과 상기 제2 그룹의 PE들(110b)에 의해서 출력되는 제2 출력 특징맵이 서로 의존적이지 않고 독립적인 경우, 상기 SFU는 상기 클럭의 제1 부 분 및 제2 부분 중에서 시간적으로 더 빠른 부분에 맞추어서 전달받은 출력 특징맵을 먼저 처리할 수 있다. 즉, 상기 제1 그룹의 PE들(110a)로부터의 제1 출력과, 상기 제2 그룹의 PE들(110b)로부터의 제2 출력은 상기 클 럭의 제1 부분을 기준으로 전달될 수 있다. 상기 제2 그룹의 PE들(110b)을 위한 기준 위상은 상기 클럭의 제2 부분에서 제1 부분으로 변환될 수 있다. 상기 제1 그룹의 PE들(110a)과 상기 제2 그룹의 PE들(110b)의 기준 위상은 상기 클럭의 제1 부분(즉, 상승 엣지)에서 상기 클럭의 제2 부분(즉, 하강 엣지)로 변환되거나 혹은 상기 클럭의 제2 부분(즉, 하강 엣지)에서 상기 클럭의 제1 부분(즉, 상승 엣지)로 변환될 수 있다. 한편, 도 7a 및 도 7b에서 도시되지는 않았으나, 상기 SFU는 상기 NPU 내부 메모리에 직접 연결될 수 있다. 다른 한편, 도 7a 및 도 7b에서는 NPU를 설명하였으나, 상기 NPU는 SoC 형태로 구현될 수도 있다. 도 8a는 본 명세서의 제2 개시의 일 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 8a을 참조하면, 예시적인 SoC은 복수의 NPU들과 복수의 CPU(central processing unit)들, 복수의 메 모리들을 포함할 수 있다. 상기 복수의 NPU들은 예컨대 제1 NPU(100-1)과 제2 NPU(100-2)를 포함할 수 있다. 그 리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200-1)과 제2 CPU(200-2)를 포함할 수 있다. 상기 복수의 메모리들 은 제1 메모리(300-1)와 제2 메모리(300-2)를 포함할 수 있다. 도 8a에서는 상기 복수의 NPU들, 복수의 CPU들 그리고 복수의 메모리들의 개수가 각기 2개인 것으로 도시되었지 만, 이에 한정되는 것은 아니고 개수는 4개, 6개, 8개 등으로 다양하게 변형될 수 있다. 상기 예시적인 SoC는 메모리 컨트롤러와, 클럭 생성기와, 시스템 버스와 그리고 I/O(input output) 인터페이스를 포함할 수 있다.상기 시스템 버스는 반도체 다이(die) 위에 형성된 전기 전도성 패턴(electrically conductive pattern) 에 의해서 구현될 수 있다. 상기 시스템 버스는 고속 통신을 가능하게 한다. 예를 들어, 상기 복수의 NPU들, 상 기 복수의 CPU들, 상기 복수의 메모리들 그리고 상기 메모리 컨트롤러은 상기 시스템 버스를 통하여 서로 통신할 수 있다. 상기 복수의 NPU들과 그리고 상기 복수의 CPU들은 상기 시스템 버스를 통하여 상기 메모리 컨트롤러 에 요청하여, 상기 복수의 메모리들 중 적어도 하나 이상으로부터 데이터를 읽어내거나 혹은 기록할 수 있다. 도 8a에 도시된 클럭 생성기는 오실레이터(Oscillator)를 포함할 수 있다. 또한, 상기 오실레이터는 PWM(Pulse Width Modulator)를 포함할 수 있다. 상기 PWM은 클럭의 사이클을 Duty Cycle를 변경함으로써 high 신호와 low 신호의 비율을 조정할 수 있다. 상기 클럭 생성기가 신호를 생성하여 출력하면, 제1 NPU(100-1)과 제2 NPU(100-2)로 전달되게 된다. 이때, 상기 제1 NPU(100-1)은 상기 클럭의 제1 부분(예컨대, rising edge)를 기준으로 동작하도록 설정되고, 상 기 제2 NPU(100-2)은 상기 클럭의 제2부분(예컨대, falling edge)를 기준으로 동작하도록 설정될 수 있다. 이처럼 제1 NPU(100-1)와 상기 제2 NPU(100-2)이 상기 클럭의 서로 다른 부분을 기준으로 분산 동작되게 하면, 순간 소모 전력을 낮출 수 있는 장점이 있다. 도 8b는 본 명세서의 제2 개시의 다른 일 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 8b을 참조하면, 예시적인 SoC은 복수의 NPU들과 복수의 CPU(central processing unit)들, 복수의 메 모리들을 포함할 수 있다. 상기 복수의 NPU들은 예컨대 제1 NPU(100-1)과 제2 NPU(100-2)를 포함할 수 있다. 그 리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200-1)과 제2 CPU(200-2)를 포함할 수 있다. 상기 복수의 메모리들 은 제1 메모리(300-1)와 제2 메모리(300-2)를 포함할 수 있다. 도 8b에서는 상기 복수의 NPU들, 복수의 CPU들 그리고 복수의 메모리들의 개수가 각기 2개인 것으로 도시되었지 만, 이에 한정되는 것은 아니고 개수는 4개, 6개, 8개 등으로 다양하게 변형될 수 있다. 상기 예시적인 SoC는 메모리 컨트롤러와, 클럭 생성기와, 복수의 버스와 그리고 I/O(input output) 인터페이스를 포함할 수 있다. 상기 복수의 버스는 CPU를 위한 버스, 즉 CPU 버스(500-1), NPU를 위한 버스, 즉 NPU 버스(500-2)와 주변 컴포 넌트를 위한 버스, 즉 주변 버스(Peripheral Bus)(500-3)을 포함할 수 있다. 상기 CPU 버스(500-1)에는 상기 제1 CPU(200-1)과 상기 제2 CPU(200-2) 그리고 상기 제1 메모리(300-1)이 연결 될 수 있다. 상기 NPU 버스(500-2)에는 제1 NPU(100-1)과 제2 NPU(100-2)와 그리고 제2 메모리(300-2)가 연결 될 수 있다. 상기 주변 버스(Peripheral Bus)(500-3)에는 메모리 컨트롤러와, 클럭 생성기가 연결될 수 있다. 상기 클럭 생성기는 오실레이터(Oscillator)를 포함할 수 있다. 또한, 상기 오실레이터는 PWM(Pulse Width Modulator)를 포함할 수 있다. 상기 클럭 생성기가 신호를 생성하여 출력하면, 제1 NPU(100-1)과 제2 NPU(100-2)로 전달되게 된다. 이때, 상기 제1 NPU(100-1)은 상기 클럭의 제1 부분(예컨대, rising edge)를 기준으로 동작하도록 설정되고, 상 기 제2 NPU(100-2)은 상기 클럭의 제2부분(예컨대, falling edge)를 기준으로 동작하도록 설정될 수 있다. 제1 NPU(100-1)와 상기 제2 NPU(100-2)이 상기 클럭의 서로 다른 부분을 기준으로 분산 동작되게 하면, 순간 소 모 전력을 낮출 수 있는 장점이 있다. 이상에서는 SoC를 위주로 설명되지만, 본 명세서의 개시는 SoC에만 한정되는 것은 아니며, 본 개시의 내용은 SiP(System in Package) 혹은 PCB(Printed circuit board) 기반 보드 레벨 시스템에도 적용될 수 있다. 예를 들어, 각 기능 컴포넌트는 독립 반도체 칩으로 구현되고, PCB 상에 형성된 전기 전도성 패턴(electrically conductive pattern)에 의해서 구현되는 시스템 버스를 통해 서로 연결되는 형태로 구현될 수 있다. 구체적으로 는 도 9를 참조하여 설명하기로 한다. 도 9a는 본 명세서의 제3 개시의 일 예시에 따른 시스템의 구조를 나타낸 예시도이다. 도 9a를 참조하면, 예시적인 시스템은 기판에 장착되는 복수의 NPU들과 복수의 CPU(central processing unit)들, 복수의 메모리들을 포함할 수 있다. 상기 복수의 NPU들은 예컨대 제1 NPU(100-1)과 제2 NPU(100-2)를 포함할 수 있다. 그리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200-1)과 제2 CPU(200-2)를 포함 할 수 있다. 상기 복수의 메모리들은 제1 메모리(300-1)와 제2 메모리(300-2)를 포함할 수 있다. 도 9a에서는 상기 복수의 NPU들, 복수의 CPU들 그리고 복수의 메모리들의 개수가 각기 2개인 것으로 도시되었지 만, 이에 한정되는 것은 아니고 개수는 4개, 6개, 8개 등으로 다양하게 변형될 수 있다. 상기 예시적인 시스템는 메모리 컨트롤러와, 클럭 생성기와, 시스템 버스와 그리고 I/O(input output) 인터페이스를 포함할 수 있다. 상기 시스템 버스는 상기 기판 위에 형성된 전기 전도성 패턴(electrically conductive pattern)에 의해서 구현될 수 있다. 상기 시스템 버스는 고속 통신을 가능하게 한다. 예를 들어, 상기 복수의 NPU들, 상기 복수의 CPU들, 상기 복수의 메모리들 그리고 상기 메모리 컨트롤러은 상기 시스템 버스를 통하여 서 로 통신할 수 있다. 도 9a에 도시된 클럭 생성기는 오실레이터(Oscillator)를 포함할 수 있다. 또한, 상기 오실레이터는 PWM(Pulse Width Modulator)를 포함할 수 있다. 상기 PWM은 클럭의 사이클을 Duty Cycle를 변경함으로써 high 신호와 low 신호의 비율을 조정할 수 있다. 상기 클럭 생성기가 신호를 생성하여 출력하면, 제1 NPU(100-1)과 제2 NPU(100-2)로 전달되게 된다. 이때, 상기 제1 NPU(100-1)은 상기 클럭의 제1 부분(예컨대, rising edge)를 기준으로 동작하도록 설정되고, 상 기 제2 NPU(100-2)은 상기 클럭의 제2부분(예컨대, falling edge)를 기준으로 동작하도록 설정될 수 있다. 이처럼 제1 NPU(100-1)와 상기 제2 NPU(100-2)이 상기 클럭의 서로 다른 부분을 기준으로 분산 동작되게 하면, 순간 소모 전력을 낮출 수 있는 장점이 있다. 도 9b는 본 명세서의 제3 개시의 일 예시에 따른 시스템의 구조를 나타낸 예시도이다. 도 9b를 참조하면, 예시적인 시스템은 기판에 장착되는 복수의 NPU들과 복수의 CPU(central processing unit)들, 복수의 메모리들을 포함할 수 있다. 상기 복수의 NPU들은 예컨대 제1 NPU(100-1)과 제2 NPU(100-2)를 포함할 수 있다. 그리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200-1)과 제2 CPU(200-2)를 포함 할 수 있다. 상기 복수의 메모리들은 제1 메모리(300-1)와 제2 메모리(300-2)를 포함할 수 있다. 도 9b에서는 상기 복수의 NPU들, 복수의 CPU들 그리고 복수의 메모리들의 개수가 각기 2개인 것으로 도시되었지 만, 이에 한정되는 것은 아니고 개수는 4개, 6개, 8개 등으로 다양하게 변형될 수 있다. 상기 예시적인 시스템는 메모리 컨트롤러와, 클럭 생성기와, 복수의 버스와 그리고 I/O(input output) 인터페이스를 포함할 수 있다. 상기 복수의 버스는 상기 기판 위에 형성된 전기 전도성 패턴(electrically conductive pattern)에 의해 서 구현될 수 있다. 상기 복수의 버스는 CPU를 위한 버스, 즉 CPU 버스(500-1), NPU를 위한 버스, 즉 NPU 버스(500-2)와 주변 컴포 넌트를 위한 버스, 즉 주변 버스(Peripheral Bus)(500-3)을 포함할 수 있다. 상기 CPU 버스(500-1)에는 상기 제1 CPU(200-1)과 상기 제2 CPU(200-2) 그리고 상기 제1 메모리(300-1)이 연결 될 수 있다. 상기 NPU 버스(500-2)에는 제1 NPU(100-1)과 제2 NPU(100-2)와 그리고 제2 메모리(300-2)가 연결 될 수 있다. 상기 주변 버스(Peripheral Bus)(500-3)에는 메모리 컨트롤러와, 클럭 생성기가 연결될 수 있다. 상기 클럭 생성기는 오실레이터(Oscillator)를 포함할 수 있다. 또한, 상기 오실레이터는 PWM(Pulse Width Modulator)를 포함할 수 있다. 상기 클럭 생성기가 신호를 생성하여 출력하면, 제1 NPU(100-1)과 제2 NPU(100-2)로 전달되게 된다. 이때, 상기 제1 NPU(100-1)은 상기 클럭의 제1 부분(예컨대, rising edge)를 기준으로 동작하도록 설정되고, 상 기 제2 NPU(100-2)은 상기 클럭의 제2부분(예컨대, falling edge)를 기준으로 동작하도록 설정될 수 있다. 제1 NPU(100-1)와 상기 제2 NPU(100-2)이 상기 클럭의 서로 다른 부분을 기준으로 분산 동작되게 하면, 순간 소 모 전력을 낮출 수 있는 장점이 있다.도 10은 본 명세서의 개시들에 따른 NPU의 동작 장법을 나타낸 예시도이다. 도 10을 참조하면, 복수의 PE들을 구동하기 위한 구동 신호가 생성될 수 있다(S1010). 그리고, 상기 복수의 PE들은 제1 그룹의 PE들과 제2 그룹의 PE들로 분할될 수 있다(S1020). 각 그룹에 속한 PE 들의 개수는 달라질 수 있는 것은 자명할 것이다. 또한, 각 그룹에 속한 PE들의 개수는 반-고정적으로 혹은 동 적으로 변경될 수 있다. 이러한 변경은 NPU 컨트롤러의 제어에 따라 수행될 수 있다. 이어서, 상기 클럭 신호의 제1 부분을 기준으로, 상기 제1 그룹의 PE들이 구동될 수 있다(S1030). 또한, 상기 클럭 신호의 제2 부분을 기준으로, 상기 제2 그룹의 PE들이 구동될 수 있다(S1040). 도 11은 본 명세서의 개시들에 따른 NPU의 동작 장법을 나타낸 예시도이다. 도 11을 참조하면, 복수의 NPU들을 구동하기 위한 구동 신호가 생성될 수 있다(S1110). 그리고, 상기 복수의 NPU들은 제1 그룹의 NPU들과 제2 그룹의 NPU들로 분할될 수 있다(S1120). 각 그룹에 속한 NPU들의 개수는 달라질 수 있는 것은 자명할 것이다. 또한, 각 그룹에 속한 NPU들의 개수는 반-고정적으로 혹은 동적으로 변경될 수 있다. 이러한 변경은 CPU의 제어에 따라 수행될 수 있다. 이어서, 상기 클럭 신호의 제1 부분을 기준으로, 상기 제1 그룹의 NPU들이 구동될 수 있다(S1130). 또한, 상기 클럭 신호의 제2 부분을 기준으로, 상기 제2 그룹의 NPU들이 구동될 수 있다(S1140). 본 명세서의 일 개시는 시스템을 제공한다. 상기 시스템은 적어도 하나의 메모리와 그리고 ANN(artificial neural network) 모델들 중 적어도 하나를 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 포함 하는 NPU(neural processing unit)를 포함할 수 있다. 상기 복수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함할 수 있다. 상기 복수의 PE들은 클럭 신호의 제1 부분에서 동작하도록 설정 된 제1 그룹의 PE들과 상기 클럭 신호의 제2 부분에서 동작하도록 설정된 제2 그룹의 PE들을 포함할 수 있다. 본 명세서의 일 개시는 SoC(system-on-chip)를 제공한다. 상기 SoC는 적어도 하나의 메모리를 위해 배치된 제1 회로와; 그리고 ANN(artificial neural network) 모델들 중 적어도 하나를 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 위해 배치된 제2 회로를 포함할 수 있다. 상기 복수의 PE들은 가산기(adder), 곱셈 기(multiplier) 그리고 누산기(accumulator)를 포함할 수 있다. 상기 복수의 PE들은 클럭 신호의 제1 부분에서 동작하도록 설정된 제1 그룹의 PE들과 상기 클럭 신호의 제2 부분에서 동작하도록 설정된 제2 그룹의 PE들을 포 함할 수 있다. 본 명세서의 일 개시는 NPU(neural processing unit)를 구동하는 방법을 제공한다. 상기 NPU 구동 방법은 복수 의 PE들(processing elements)을 포함하는 NPU를 구동하는 클럭 신호를 생성하는 단계와; 상기 복수의 PE들 중 제1 그룹의 PE들을 상기 클럭 신호의 제1 부분에서 동작시키는 단계와; 그리고 상기 복수의 PE들 중 제2 그룹의 PE들을 상기 클럭 신호의 제2 부분에서 동작시키는 단계를 포함할 수 있다. 상기 복수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함할 수 있다. 본 명세서와 도면에 나타난 본 개시의 예시들은 본 개시의 기술 내용을 쉽게 설명하고 본 개시의 이해를 돕기 위해 특정 예를 제시한 것뿐이며, 본 명의 범위를 한정하고자 하는 것은 아니다. 지금까지 설명한 예시들 이외 에도 다른 변형 예들이 실시 가능하다는 것은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 자명 한 것이다.도면 도면1 도면2a 도면2b 도면3 도면4a 도면4b 도면5 도면6a 도면6b 도면7a 도면7b 도면8a 도면8b 도면9a 도면9b 도면10 도면11"}
{"patent_id": "10-2023-0061416", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 도 2a은 컨볼루션 신경망(CNN)의 기본 구조를 설명하기 위한 도면이다. 도 2b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 3은 본 개시에 따른 신경 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 4a는 본 개시에 적용될 수 있는 복수의 프로세싱 엘리먼트 중 하나의 프로세싱 엘리먼트를 설명하는 개략적 인 개념도이다. 도 4b는 본 개시에 적용될 수 있는 SFU를 설명하는 개략적인 개념도이다. 도 5는 도 3에 도시된 NPU의 변형예를 나타낸 예시도이다. 도 6a은 예시적인 인공신경망 모델 내의 각 레이어 별 데이터의 크기를 나타낸 예시도이고, 도 6b는 도 6a에 도 시된 예시적 인공신경망 모델에서 각 레이어 별 데이터 사이즈 등을 나타낸 예시적인 테이블이다. 도 7a는 본 명세서의 제1 개시의 일 예시에 따른 NPU의 구조를 나타낸 예시도이다.도 7b는 본 명세서의 제1 개시의 다른 예시에 따른 NPU의 구조를 나타낸 예시도이다. 도 8a는 본 명세서의 제2 개시의 일 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 8b는 본 명세서의 제2 개시의 다른 일 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 9a는 본 명세서의 제3 개시의 일 예시에 따른 시스템의 구조를 나타낸 예시도이다. 도 9b는 본 명세서의 제3 개시의 일 예시에 따른 시스템의 구조를 나타낸 예시도이다. 도 10은 본 명세서의 개시들에 따른 NPU의 동작 장법을 나타낸 예시도이다. 도 11은 본 명세서의 개시들에 따른 NPU의 동작 장법을 나타낸 예시도이다."}
