{"patent_id": "10-2021-0068564", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0160392", "출원번호": "10-2021-0068564", "발명의 명칭": "다중 실행 불가한 시뮬레이터 기반 지능형 에이전트를 위한 분산 학습 방법", "출원인": "한국과학기술원", "발명자": "최호진"}}
{"patent_id": "10-2021-0068564", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "강화 학습을 이용하여 지능형 에이전트를 학습하는 분산 학습 방법에 있어서,도커 컨테이너를 분산 학습 환경에 적용하는 적용 단계; 및상기 도커 컨테이너의 적용에 따른 통신을 정의하는 정의 단계;를 포함하는 분산 학습 방법."}
{"patent_id": "10-2021-0068564", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 적용 단계는, Actor, Learner 및 Replay가 분리되어 비동기적으로 작동하는 Actor-Learner 알고리즘에서 네트워크 통신을 이루는 객체인 상기 Actor에 상기 도커 컨테이너를 적용하는 분산 학습 방법."}
{"patent_id": "10-2021-0068564", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 정의 단계는,상기 Actor, Learner 및 Replay 간의 통신에 있어서 동일 서버에 위치하는 경우에 한하여 도커 네트워크를 사용하는 것을 특징으로 하는 분산 학습 방법."}
{"patent_id": "10-2021-0068564", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 강화 학습을 이용하여 지능형 에이전트를 학습하는 분산 학습 방법에 있어서, 도커 컨테이너를 분산 학습 환경에 적용하는 적용 단계 및 상기 도커 컨테이너의 적용에 따른 통신을 정의하는 정의 단계를 포함한다. 본 발명에 의하면 기존의 알고리즘을 크게 변형하지 않으면서 다중 실행이 불가능한 시뮬레이터 환경에서도 빠르 게 경험 데이터를 축적할 수 있게 된다."}
{"patent_id": "10-2021-0068564", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 다중 실행이 불가능한 시뮬레이터 환경에서도 빠르게 경험 데이터를 축적할 수 있는 시뮬레이터 기반 지능형 에이전트를 위한 분산 학습 방법에 관한 것이다."}
{"patent_id": "10-2021-0068564", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "강화 학습(Reinforcement learning) 이란 기계 학습의 한 분야로 행동 심리학에서 영감을 받아 발생한 기술이다. 강화 학습에는 환경이 정의되어 있으며 그 환경과 상호작용을 하는 에이전트가 정의되어 있다. 에이 전트는 주어진 환경 안에서 현재 자신의 상태를 인식하고, 상태를 기반으로 선택 가능한 행동 중 자신이 사전에 정의된 규칙에 따라 앞으로 받을 수 있는 보상을 최대화할 수 있는 행동 순서를 선택하도록 학습한다. 에이전트 가 받을 수 있는 보상은 사용자가 정의하며 이를 통해 에이전트가 어떤 행동을 학습하게 될지를 유도하게 된다. 강화 학습에서 다루는 환경은 주로 마르코프 결정 과정(Markov Decision Process; MDP)을 통해 표현할 수 있다. 마르코프 결정 과정은 어떤 상태 s가 존재한다고 했을 때 어떤 행동 a를 취할 수 있으며, 이때 행동 a를 통해 새로운 상태 s'로 전이한다. 이때 에이전트는 이에 대한 보상 Ra(s, s')를 받는다. 즉 새로운 상태 s'는 현재 상태 s와 에이전트의 행동 a에만 영향을 받으며 이전 모든 상태와 독립적인 확률 분포를 갖는다. 또한 만약 에 이전트가 환경으로부터 상태를 분적으로만 관찰한 경우를 부분 관측 마르코프 결정 과정(Partially observable Markov decision process; POMDP)이라 한다. 강화 학습에서 에이전트를 학습하기 위해서는 지도 학습(Supervised Learning)과 마찬가지로 학습 입출력 쌍이 주어질 필요가 있다. 하지만 이 값들은 일반적인 지도 학습(Supervised Learning)과 같이 데이터셋을 주어지지 않는다는 점에서 차이가 있다. 또한 학습 과정에서 기저 진실(Ground Truth)가 주어지지 않는 점에서도 사이가 난다. 강화 학습에서는 상태 s와 사전에 정의된 규칙에 따른 보상이 학습 위해 필요하다. 이러한 정보는 환경으 로부터 에이전트의 상태를 관측하고 사전에 정의된 규칙에 맞는 보상을 계산하여 제공된다. 따라서 강화 학습을 위해서는 환경에서 에이전트을 작동시킬 필요가 있으며 탐색(Explore)과 활용(Exploit)의 균형을 맞추어 학습을위한 데이터를 수집하고 에이전트를 학습할 필요가 있다. 최근 인공신경망(Artificial Neural network; ANN)의 발전은 강화 학습의 발전에 큰 영향을 주었다. 인공 신경 망은 동물의 뉴런을 모사하여 만든 통계학적 학습 알고리즘으로 임의의 목적 함수를 설정하고 이를 근사하는 기 계 학습의 한 분야이다. 강화 학습의 에이전트가 상태를 기반으로 행동을 예측하는 과정을 인공신경망으로 대체 하는 방식이 주로 연구되고 있다. 이를 통해 강화 학습을 기반으로한 지능형 에이전트는 괄목할 발전을 이뤄왔다. 바둑이나 아타리 같은 게임은 사람을 능가하는 수준의 의사결정 수준을 달성했으며, 더욱 복잡한 전략 게임 환경에서도 강화 학습을 통한 에 이전트는 사람 수준의 의사결정 능력을 보이고 있다. \"Grandmaster level in StarCraft II using multi-agent reinforcement learning\" (Vinyals et al, 2019). 이러한 변화는 강화 학습의 연구 방향에도 많은 영향을 주었 다. 그 중 한 가지는 인공 신경망의 도입과 함께 학습에 필요한 상태와 보상을 포함하는 에이전트의 경험 데이 터가 급수적으로 증가하고 있다는 점이다. 최근의 많은 에이전트가 짧게는 수 일에서 많게는 몇 달의 학습이 필 요로 하다. 이를 개선하고자 경험 데이터를 비동기적인 프로세스를 통해 빠르게 획득하고자 하는 알고리즘이 활 발하게 연구되고 있다. 또 다른 변화로 단순히 강화학습을 위해 정의된 환경 외에도 다양한 환경에 지능형 에이전트의 적용하려는 시도 가 이어지고 있다는 점이다. 기존에 만들어져 있던 시뮬레이터나 게임에 적용 시키려는 많은 연구가 이뤄지고 있으며 높은 성취를 얻고 있다. 많은 게임이나 시뮬레이터가 오랜 기간 개발되어왔고 규칙이 잘 정의되어 있고 평가 기준이 명확하여 강화 학습 에이전트를 학습시키기에 매우 용이하다. \"ViZDoom Competitions: Playing Doom From Pixels\" (Marek Wydmuch et all 2018). 하지만 많은 게임 또는 시뮬레이터 환경은 강화 학습과 관련 없이 개발되어 왔기 때문에 강화 학습에서 사용하는 알고리즘을 적용하기 어려운 부분이 있다. 특히 이런 프로 그램들은 대부분 다중 실행을 고려하지 않기 때문에 앞서 얘기한 경험 데이터를 빠르게 수집을 위한 알고리즘의 적용은 까다로운 문제가 될 수 있다."}
{"patent_id": "10-2021-0068564", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상술한 문제점을 감안하여 안출된 것으로, 본 발명의 목적은 강화 학습에서 다중 실행이 불가능한 시 뮬레이터 기반의 환경에서 프로세스 격리 기술인 도커(Docker)를 활용하여 경험 데이터를 빠르게 수집을 위해 개발된 대표적인 Actor-Learner 알고리즘에 적용하는 분산 학습 방법을 제공함에 있다."}
{"patent_id": "10-2021-0068564", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명에 따른 분산 학습 방법은, 강화 학습을 이용하여 지능형 에이전트를 학습하 는 분산 학습 방법에 있어서, 도커 컨테이너를 분산 학습 환경에 적용하는 적용 단계; 및 상기 도커 컨테이너의 적용에 따른 통신을 정의하는 정의 단계;를 포함한다. 그리고, 상기 적용 단계는, Actor, Learner 및 Replay가 분리되어 비동기적으로 작동하는 Actor-Learner 알고 리즘에서 네트워크 통신을 이루는 객체인 상기 Actor에 상기 도커 컨테이너를 적용할 수 있다. 또한, 상기 정의 단계는, 상기 Actor, Learner 및 Replay 간의 통신에 있어서 동일 서버에 위치하는 경우에 한 하여 도커 네트워크를 사용할 수 있다."}
{"patent_id": "10-2021-0068564", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의하면 기존의 알고리즘을 크게 변형하지 않으면서 다중 실행이 불가능한 시뮬레이터 환경에서도 빠 르게 경험 데이터를 축적할 수 있게 된다."}
{"patent_id": "10-2021-0068564", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요 지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되 지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되 어야 한다. 최근 인공신경망을 도입한 강화 학습은 다양한 분야에서 활용되고 있다. 강화 학습 에이전트를 학습하기 위해서 는 에이전트가 도 1에 도시된 바와 같이 행동(action)을 환경에 입력하고 환경으로부터 보상(reward)와 상태 (state)를 받는 과정을 반복하여 학습에 사용할 경험 데이터를 축적하게 된다. 따라서 에이전트를 효과적인 학 습을 위해서는 환경으로부터 경험 데이터를 빠르게 축적하는 것이 중요한 요인으로 작용한다. 만약 환경이 시간의 속도를 제어할 수 없는 시뮬레이터가 있다고 하면, 바둑이나 체스같은 게임의 경우 어떤 행 동이 일어났을 때 다음 상태와 보상을 즉각적으로 계산할 수 있는 반면 시뮬레이터는 행동이 일어나고 다음 상 태로 변이할 때까지 기다릴 필요가 있다. 이런 요인은 학습의 속도를 크게 저하시키는 요인이 될 수 있다. 설령 속도를 제어할 수 있는 환경이라 하더라 도 컴퓨팅 자원에 의해 생성할 수 있는 경험 데이터의 속도는 제한될 수 밖에 없다. 이런 문제를 해결하기 위해 전통적인 프로그래밍 기법에 근거하여 다중 프로세스로 환경을 실행시키는 방법이 있다. Volodymyr Mnih 외, \"Asynchronous Methods for Deep Reinforcement Learning\"(Volodymyr Mnih et al, 2016)에서는 다수의 에이전 트가 다수의 환경으로부터 경험을 축적하고 축적한 경험 데이터를 토대로 에이전트의 행동 정책(Policy)를 학습 하는 Asynchronous Advantage Actor-Critic(이하 A3C)를 제안했다. 다시 이 행동 정책을 받은 다수 에이전트가 경험을 축적하고 학습을 진행하는 과정을 반복하는 방법이 보고되었다. 하지만 학습과 경험 데이터의 수집이 번 갈아 일어나야 한다는 제약이 있다는 단점이 있다. 이런 단점을 해결한 방법으로 도 2에 도시된 바와 같이 학습과 경험 데이터의 축적을 모두 비동기적으로 실행시 키는 방법 또한 보고되었다. \"Distributed prioritized experience replay\" (Dan Horgan et al, 2018). 이 방 법(이하 Ape-X)은 Actor(액터)-Learner(러너)라는 방식으로 한가지로 환경과 에이전트를 Actor로 분리하고 행동 정책을 학습하는 부분을 Learner로 분리시켜 비동기적으로 작동한다. 또한 경험 데이터를 수집할 때 사용한 에 이전트의 정책과 현재 에이전트의 정책이 크게 차이가 나면 학습이 어려워 질 수 있기 때문에 우선도(Priorit y)라는 개념을 도입했다. 하지만 앞서 언급한 A3C을 포함해 Ape-X는 다중 실행이 불가능한 환경에 대해 어떻게 대처할 것인지에 대한 논의는 이루어지지 않고 있다. 도 3은 본 발명에 따른 분산 학습 방법의 설계도이다. Ape-X를 기반으로 시뮬레이터 환경을 다중 실행시키는 기 술에 대해 제공한다. 본 발명에서 정의한 환경은 OpenAi Gym에서 제공하는 환경을 기준으로 작성되었다. 강화 학습 환경으로 시뮬레이터 환경을 가정했으며 에이전트와 연동을 위해 로컬 통신을 하는 상황을 가정했다. 로컬 통신이 아닌 다른 방법으로 시뮬레이터와 환경이 통신을 하더라도 제안하는 방법은 적용될 수 있다. 이를 토대 로 시뮬레이터와 환경 그리고 에이전트의 흐름도는 도 4와 같다. 본 발명에 따른 분산 학습 방법은 환경의 다중 실행을 위한 도커(Docker)의 적용 부분과 구성 요소의 변화에 따 른 통신 방법의 정의 부분으로 두 가지로 이루어져있다. 기본적으로 시뮬레이터를 다중 실행하기 위해서는 도커를 사용했다. 도커는 프로세스 격리 기술을 통해 컨테이 너를 실행하여 운영 체제 수준의 가상화의 추상화를 제공한다. 도커 컨테이너는 일종의 소프트웨어를 소프트웨 어의 실행에 필요한 코드, 런타임, 시스템 도구, 시스템 라이브러리와 같은 모든 것을 포함하는 완전한 파일 시 스템 안에 감싼다. 따라서 도커 컨테이너는 실행 중인 환경에 관게 없이 언제나 동일하게 소프트웨어를 실행되 도록 보증한다. 도커 명령어는 도커 소켓을 통해 도커 클라이언트에게 전달되며 도커 클라이언트가 명령을 수행 하는 방식으로 이뤄진다. 본 발명에서 설계한 Actor-Learner 구조는 도 3데 도시된 바와 같다. Actor와 Learner 그리고 경험 데이터를 축 적하는 Replay가 분리되어 각각 비동기적으로 작동한다. 또한 Actor는 네트워크 통신을 통해 Learner나 Replay 와 같은 다른 프로세스와 데이터를 교환한다. Actor는 Replay에게 경험 데이터를 송신한다. Replay는 경험 데이 터를 관리하며, Learner에게 경험 데이터를 전달하는 역할을 맡는다. Learner는 이 경험 데이터를 통해 행동 정책(Policy)를 학습한다. Learner에서 학습된 행동 정책는 주기적으로 Actor에게 전달된다. Actor 내부에 정의된 환경과 시뮬레이터의 구조는 도 4에 도시된 바와 같다. 환경은 Initialize, Close와 같은 함수를 통해 시뮬레이터를 시작 및 종료할 수 있다. 또한 환경은 시뮬레이터를 초기 상태로 되돌리기 위한 Reset과 시뮬레이터를 통해 시뮬레이션을 하기 위한 행동을 제공하고 보상과 상태를 받아오는 Step이 정의되어 있다. Render는 필요에 따라 시뮬레이터의 화면을 랜더링하기 위한 함수이다. 또한 현재 상태에 대한 적절한 행 동을 얻기 위한 에이전트가 환경과 상호 작용을한다. 시뮬레이터로 받은 보상과 상태를 에이전트에 전달하면 에 이전트는 자신이 받을 수 있는 보상을 최대화하는 행동을 선택한다. 컨테이너의 적용 범위는, 도 3에 도시된 바와 같이, 시뮬레이터와 환경 객체를 포함한 Actor 전체를 한 개의 도 커 컨테이너(Docker container)에서 실행했다. 본 발명에서 컨테이너의 적용 범위를 Actor 전체로 한 이유는 두 가지이다. 먼저 Actor가 시뮬레이터를 포함하기 때문에 시뮬레이터 환경의 다중 실행을 보장할 수 있다. 두 번 째는 Actor는 다른 프로세스와 네트워크 통신을 통해서만 데이터를 교환하기 때문이다. Actor 프로세스는 행동 정책에 대한 파라미터를 받고 수집한 경험 데이터를 송신하기 위해서 다른 프로세스와 네트워크 통신을 사용한 다. 컨테이너 적용 범위를 네트워크 통신이 실행되는 객체를 단위로 묶음으로 추가적인 통신으로 인한 속도의 저하를 방지할 수 있다. 시뮬레이터를 포함하며 이를 만족하는 범위는 Actor 프로세스가 될 수 있다. 또한 이 방법을 통해 기존 Actor의 알고리즘을 변형 없이 그대로 사용 가능하다는 장점이 있다. 상기 컨테이너의 적용을 고려하여 비해 각 프로세스 별로 통신 방법에 대해 정의할 필요가 있다. Actor가 하나 의 컨테이너로 분리되었기 때문에 Actor 내부에서 시뮬레이터와 환경이 데이터를 주고 받는 방법을 자유롭게 선 택할 수 있다. 본 발명에서는 보편적인 방법으로 시뮬레이터와 환경이 로컬 네트워크를 통해 통신하는 상황을 고려했다. 컨테이너 간의 통신 환경은 도커 네트워크를 사용하여 구축할 수 있다. Learner와 Replay는 동일한 도커 컨테이너에 위치하고 있기 때문에 프로세스 간 통신(Internal Process Communication; IPC)을 통해 데이 터를 주고 받았다. 컨테이너가 도커 네트워크를 통해 통신하기 위해서는 동일한 도커 클라이언트로부터 만들어진 도커 네트워크를 사용할 필요가 있다. 이를 충족 시키기 위해 운영 체제 시스템의 도커 클라이언트로부터 Learner와 Replay를 위 한 컨테이너와 Actor를 위한 컨테이너를 생성할 필요가 있다. 이 경우 제안하는 방법은 컨테이너를 생성 시 도 커 클라이언트로부터 생성된 도커 네트워크를 제공해야하기 때문에 도커 컨테이너 내부에서는 실행할 수 없다. 이를 해결하기 위해 Docker in Docker(이하 DinD)를 적용할 수 있다. DinD는 container 생성 시 도커 소켓을 공유하여 도커 컨테이너 내부에서 시스템의 도커 클라이언트를 사용하는 방법이다. 따라서 Learner와 Replay를 위한 컨테이너를 생성할 때 시스템의 도커 소켓을 공유하면 Learner와 Replay가 속한 도커 컨테이너에서 Actor 를 위한 컨테이너를 생성할 수 있으며, 동일한 도커 네트워크를 사용할 수 있다. 본 발명에서 제안하는 프레임 워크의 속도를 측정하기 위해 Openai Gym에 정의된 아타리 환경과 자체적으로 구 현한 unreal engine 기반의 시뮬레이터 환경에서 초당 샘플을 수집하는 속도를 측정했다. 아타리의 경우 시뮬레 이터 환경과 유사한 환경을 만들기 위해 관측값을 얻을 때 0.1초의 지연을 주는 환경을 만들어 추가로 실험을 진행했다. 본 실험에서는 Deep Q-Network를 사용하여 경험 데이터를 생성했으며, e-greedy는 사용하지 않았다. 아타리 환경은 다중 실행이 가능하기 때문에 제안하는 방법을 적용한 Ape-X와 기존 Ape-X를 각각 적용하여 Replay 객체에서 경험 데이터가 생성되는 속도를 측정했다. 이에 대한 결과는 표 1과 같다. 표 1 1 2 5 10 Atari (N) 487.29 700.70 - - Atari (C) 475.69 700.89 - - Atari (ND) 9.34 18.67 48.72 79.86 Atari (CD) 9.33 18.84 48.73 86.82 각 항목은 순서대로 아타리 환경에 기존 Ape-X 적용한 경우, 제안한 방법을 적용한 경우, 그리고 지연을 추가한 아타리 환경에 대해 기존 Ape-X 적용한 경우, 제안한 방법을 적용한 경우를 의미한다. 실험의 결과 모든 경우에 대해 actor의 수가 증가함에 따라 경험 데이터의 수집 속도가 비례해서 증가하는 것을 볼 수 있다. 또한 제안한 바와 같이 아타리와 지연이 추가된 아타리 둘 다 Ape-X와 제안하는 방법의 경험 데이터의 생성 속도가 유의미한차이를 보이지 않음을 관측할 수 있다. 본 발명에서는 강화학습 에이전트 학습을 위해 시뮬레이터 기반 환경에서 Actor-Learner를 적용하기위한 방법을 제안한다. 제공하는 방법은 기존의 Actor-Learner 알고리즘의 통신 네트워크 구조를 이용하여 경험 데이터 수집 속도를 저하시키지 않도록 설계되었으며 실험을 통해 이를 보였다. 또한 기존의 알고리즘을 크게 변형하지 않는 다는 장점이 있다. 따라서, 제안하는 방법을 통해 다중 실행이 불가능한 시뮬레이터 환경에서도 빠르게 경험 데 이터를 축적할 수 있게 된다. 본 발명에 따른 분산 학습 방법을 수행하는 데이터 처리 장치는, 상기 데이터 처리 장치에서 판독 가능한 명령 을 실행하도록 구현되는 프로세서를 포함하고, 상기 프로세서는 위에서 설명한 분산 학습 방법의 각 단계를 수 행할 수 있다. 한편, 본 발명은 컴퓨터 시스템 상에서 구현될 수 있고, 하나 이상의 컴퓨터로 구성된 시스템은 소프트웨어, 펌 웨어, 하드웨어 또는 이들의 조합을 작동중 시스템이 작업들을 수행하도록 하는 시스템에 설치함으로써 위에서 설명한 방법의 각 단계를 수행하도록 구성될 수 있다. 하나 이상의 컴퓨터 프로그램은 데이터 처리 장치에 의해 실행될 때 장치로 하여금 동작들을 수행하게 하는 명령어들을 포함함으로써 특정 동작들 또는 작업들을 수행하 도록 구성될 수 있다. 한편, 위에서 설명한 분산 학습 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구 현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 컴퓨터 판독 가능 기록 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있 다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체 (magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장 하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작 동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 이상에서 실시예들에 설명된 특징, 구조, 효과 등은 본 발명의 하나 의 실시예에 포함되며, 반드시 하나의 실시 예에만 한정되는 것은 아니다. 나아가, 각 실시예에서 예시된 특징, 구조, 효과 등은 실시예들이 속하는 분야의 통상의 지식을 가지는 자에 의해 다른 실시예들에 대해서도 조합 또는 변형되어 실시 가능하다. 따라서 이러한 조합과 변형에 관계된 내용들은 본 발명의 범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4"}
{"patent_id": "10-2021-0068564", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 강화학습의 과정을 나타내는 개념도이다. 도 2는 효과적인 경험 데이터 수집을 위해 강화학습에서 사용하는 프레임워크를 도시한다. 도 3은 본 발명에 따른 분산 학습 방법의 설계도이다. 도 4는 본 발명에 따른 분산 학습 방법에 있어서, 시뮬레이터-환경-에이전트 간의 흐름도이다."}
