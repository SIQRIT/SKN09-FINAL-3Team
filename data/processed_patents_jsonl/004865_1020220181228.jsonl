{"patent_id": "10-2022-0181228", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0100506", "출원번호": "10-2022-0181228", "발명의 명칭": "인공지능을 이용한 물체 자세 추정 방법 및 시스템", "출원인": "한국기술교육대학교 산학협력단", "발명자": "김지우"}}
{"patent_id": "10-2022-0181228", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "(A) 생성부가 키넥트 카메라에서 촬영된 영상에 나타난 물체의 자세 추정에 필요한 데이터를 생성하는 단계;(B) 검증부가 상기 생성한 데이터에 대한 상기 물체의 위치를 검증하는 단계;및(C) 추론부가 상기 검증된 데이터를 바탕으로 인공지능으로 학습하여 상기 물체의 위치와 자세를 추정하는단계;를 포함하는 것을 특징으로 하는,인공지능을 이용한 물체 자세 추정 방법."}
{"patent_id": "10-2022-0181228", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 (A) 단계에서 상기 데이터는,RGB 정보, Depth 정보 및 Mask 각각의 이미지에 대한 정보 및 물체의 회전(rotation)정보, 이동(translation)정보 중 적어도 하나를 포함하는 정보와, 상기 카메라 내부 파라미터의 정보를 포함하는 것을 특징으로 하는,인공지능을 이용한 물체 자세 추정 방법."}
{"patent_id": "10-2022-0181228", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 (B) 단계는,(B1) 상기 생성한 데이터의 회전(rotation) 값과 이동(translation) 값을 3차원 점군 데이터 프로그램의 파일에존재하는 각각의 최대점에 곱하는 단계;(B2) 상기 곱한 결과에 카메라 내부 파라미터 행렬을 곱하고, 곱한 결과의 z값으로 모든 최대점을 나누는 단계;및(B3) 상기 나눈 결과를 RGB 이미지 위에 출력하여 상기 RGB 이미지상의 물체의 위치와 상기 생성된 데이터로부터 검증한 결과를 비교하는 단계;를 포함하는 것을 특징으로 하는,인공지능을 이용한 물체 자세 추정 방법."}
{"patent_id": "10-2022-0181228", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 (C) 단계는,(C1) RGB 및 Mask 이미지 정보를 세그먼테이션 네트워크(SegNet)를 학습하는 단계;(C2) 상기 RGB 정보, Depth 정보, 회전(rotation)값, 이동(transltaion)값 및 카메라 캘리브레이션(camerecalibration) 행렬 정보를 상기 세그멘테이션 네트워크(SegNet)로 추론된 Mask 이미지 정보를 이용하여 자세추정 네트워크(Masked-Fusion)로 학습하는 단계;및(C3) 상기 추론 결과를 상기 RGB 이미지 위에 출력하여 시각화를 통해 평가하는 단계;를 포함하는 것을 특징으로 하는,공개특허 10-2024-0100506-3-인공지능을 이용한 물체 자세 추정 방법."}
{"patent_id": "10-2022-0181228", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "키넥트 카메라에서 촬영된 영상에 나타난 물체의 자세 추정에 필요한 데이터를 생성하는 생성부;상기 생성한 데이터에 대한 상기 물체의 위치를 검증하는 검증부;및상기 검증된 데이터를 바탕으로 인공지능으로 학습하여 상기 물체의 위치와 자세를 추정하는 추론부;를 포함하는 것을 특징으로 하는,인공지능을 이용한 물체 자세 추정 시스템."}
{"patent_id": "10-2022-0181228", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 목적은, 인공지능을 이용한 물체 자세 추정 방법 및 시스템을 제공하는 것이다. 상기 목적을 달성하기 위해, 본 발명에 따른 (A) 생성부가 키넥트 카메라에서 촬영된 영상에 나타난 물체의 자세 추정에 필요한 데이터를 생성하는 단계; (B) 검증부가 상기 생성한 데이터에 대한 상기 물체의 위치를 검증하는 단계; 및 (C) 추론부가 상기 검증된 데이터를 바탕으로 인공지능으로 학습하여 상기 물체의 위치와 자세를 추정 하는 단계;를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2022-0181228", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능을 이용한 물체 자세 추정 방법 및 시스템에 관한 것이다. 더욱 상세하게는, 가상의 환경에서 제작한 데이터를 학습하여 실제 물체의 위치와 자세를 추정하여 VR/AR 환경 에 해당 물체를 재구성하여 표현할 수 있는 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2022-0181228", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "물체의 자세를 추정하기 위해서는 물체에 대한 데이터가 필요하다. 또한, 공장, 건설 현장 등 산업현장에 존재하는 물체도 매우 다양하며 각 물체에 따라 데이터도 달라진다. 그래서 어떤 물체의 데이터를 수집하고, 얼마나 다양한 물체의 데이터를 수집할 것인지가 중요하다. 상기의 문제점을 고려한다면, 현실에서는 다양한 물체의 데이터를 수집하는 것에 공간적이 제약이 많이 따른다. 또한, 수많은 물체를 직접 카메라로 찍어 데이터를 생성하는 것은 어려울 수 있다. 가상공간에서 생성한 물체 데이터는 현실의 물체 데이터와 완벽히 동일하지는 않는다. 가상의 물체 데이터는 현실 세게의 빛 또는 질감을 완벽하게 반영할 수 없다. 또한, 데이터를 수집하기 위해선 생성하려는 모델이 고해상도 렌더 파이프라인(HDRP)(High-Definition Render Pipeline)을 지원하는지 확인해야한다. 만약 고해상도 렌더 파이프라인(HDRP)(High-Definition Render Pipeline)을 지원한다면 모델에 대한 데이터를 생성하면 된다. 하지만, 고해상도 렌더 파이프라인(HDRP)(High-Definition Render Pipeline)만을 사용하여 데이터를 생성하기 는 쉽지 않다. 또한, 고해상도 렌더 파이프라인(HDRP)(High-Definition Render Pipeline)을 지원하지 않는다면, 모델을 직접 제작해야 했다. 또한, 가상의 물체가 현실에서도 존재하는 물체라면, 현실 물체의 자세를 추정하기 위해 가상의 물체를 이용하 여 생성한 데이터로 학습한 인공지능을 이용해 현실에서 물체의 자세를 추정하기 위해 현실에서 동일한 물체를 소유하거나 동일한 물체가 존재해야 한다. 하지만 현실에서 볼 수 있는 대부분의 물체는 모델 파일이 존재하지 않거나 있는 경우여도 고해상도 렌더 파이 프라인(HDRP)(High-Definition Render Pipeline)을 지원하지 않는다는 문제점이 있었다. 또한, 고해상도 렌더 파이프라인(HDRP)(High-Definition Render Pipeline)을 이용하여 데이터를 생성하더라도, 현실 데이터와의 상이점이 존재하므로 학습할 시 GT(Grdoun Truth)로써 신뢰도가 떨어질 수도 있다는 문제점이있었다. 선행기술문헌 특허문헌 (특허문헌 0001) 공개특허공보 제10-2022-0126622호 (2022.09.16.공개)"}
{"patent_id": "10-2022-0181228", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 인공지능을 이용한 물체 자세 추정 방법 및 시스템에 관한 것으로, 3D 애니메이션과 건축 시각화, 가 상현실 등 인터랙티브 콘텐츠 제작을 위한 통합 제작 도구인 Unity를 사용하였다. 일단, 가상의 공간에 물체를 배치한다. 이후 Unity 내부에 있는 카메라로 다양한 각도와 위치에서 RGB, Depth, Mask를 찍고 직접 관찰 및 측정에 의해 제공되는 실제 또는 사실로 알려진 정보인 GT(Ground Truth) 데이터를 생성하였다. 이렇게 생성된 데이터를 인공지능 모델에 사용하기 적합한 형식이나 구조로 변환한다. 또한, 가상의 물체 데이터와 현실의 물체 데이터 사이의 괴리를 완전히 줄이기는 어려웠다. 이를 해결하기 위해 본 발명에서는 고품질 렌더링으로 해결하려고 한다. Unity에서 지원하는 고해상도 렌더 파이프라인(HDRP)(High-Definition Render Pipeline)을 통해 사진과 같은 장면을 구성하여 데이터셋(dataset)을 생성한다. 또한, 빛의 구성이나 환경, 고품질 렌더링을 지원하는 모델들로 좀 더 현실에 가깝게 데이터를 생성하는 것을 목적으로 한다."}
{"patent_id": "10-2022-0181228", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위해, 본 발명에 따른 인공지능을 이용한 물체 자세 추정 방법은, (A) 생성부가 키넥트 카메라에서 촬영된 영상에 나타난 물체의 자세 추정에 필요한 데이터를 생성하는 단계; (B) 검증부가 상기 생성 한 데이터에 대한 상기 물체의 위치를 검증하는 단계; 및 (C) 추론부가 상기 검증된 데이터를 바탕으로 인공지 능으로 학습하여 상기 물체의 위치와 자세를 추정하는 단계;를 포함하는 것이 바람직하다. 또한, 본발명에 따른 인공지능을 이용한 물체 자세 추정 방법의 상기 (A) 단계에서 상기 데이터는, RGB 정보, Depth 정보 및 Mask 각각의 이미지에 대한 정보 및 물체의 회전(rotation)정보, 이동(translation)정보 중 적 어도 하나를 포함하는 정보와, 상기 카메라 내부 파라미터의 정보를 포함하는 것이 바람직하다. 또한, 본발명에 따른 인공지능을 이용한 물체 자세 추정 방법의 상기 (B) 단계는, (B1) 상기 생성한 데이터의 회전(rotation) 값과 이동(translation) 값을 3차원 점군 데이터 프로그램의 파일에 존재하는 각각의 최대점에 곱하는 단계; (B2) 상기 곱한 결과에 카메라 내부 파라미터 행렬을 곱하고, 곱한 결과의 z값으로 모든 최대점을 나누는 단계; 및 (B3) 상기 나눈 결과를 RGB 이미지 위에 출력하여 상기 RGB 이미지상의 물체의 위치와 상기 생 성된 데이터로부터 검증한 결과를 비교하는 단계;를 포함하는 것이 바람직하다. 또한, 본발명에 따른 인공지능을 이용한 물체 자세 추정 방법의 상기 (C) 단계는, (C1) RGB 및 Mask 이미지 정 보를 세그먼테이션 네트워크(SegNet)를 학습하는 단계; (C2) 상기 RGB 정보, Depth 정보, 회전(rotation)값, 이동(transltaion)값 및 카메라 캘리브레이션(camere calibration) 행렬 정보를 상기 세그멘테이션 네트워크 (SegNet)로 추론된 Mask 이미지 정보를 이용하여 자세추정 네트워크(Masked-Fusion)로 학습하는 단계; 및 (C3) 상기 추론 결과를 상기 RGB 이미지 위에 출력하여 시각화를 통해 평가하는 단계;를 포함하는 것이 바람직하다. 또한, 상기 목적을 달성하기 위해, 인공지능을 이용한 물체 자세 추정 시스템은, 키넥트 카메라에서 촬영된 영 상에 나타난 물체의 자세 추정에 필요한 데이터를 생성하는 생성부; 상기 생성한 데이터에 대한 상기 물체의 위 치를 검증하는 검증부; 및 상기 검증된 데이터를 바탕으로 인공지능으로 학습하여 상기 물체의 위치와 자세를 추정하는 추론부;를 포함하는 것이 바람직하다. 기타 실시 예의 구체적인 사항은 \""}
{"patent_id": "10-2022-0181228", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의하면, 가상공간에서 자신이 원하는 데이터를 생성할 수 있으므로 공간적 시간적 제약이 적고, 다양 한 데이터를 생성할 수 있으므로 현실에서 인식할 수 있는 데이터도 더 다양해진다. 또한 인공지능 네트워크를 사용하여 물체의 자세 추정 정확도를 높임으로써 가상공간에서 실제 물체에 대한 가 상물체를 더 정확하게 생성 및 배치할 수 있다. 이렇게 가상공간에서 실제 물체에 대해 가상물체를 높은 정확도로 증강시켜 공장 및 건설 현장 등 산업현장에 적용할 경우 작업 효율을 향상시킬 수 있다는 효과가 있다. 또한, 본 발명에서 제안하는 방법은, 현실 공간을 가상 공간으로 이동시키기 위한 기반의 기술이 될 수 있을 것 이다. 예를 들어, 작업 상황 등을 원격으로 공유할 수 있다. 따라서, 건설현장, 공장 등의 장소 및 현장에서 비숙련자의 교육에 활용할 수 있으며, 작엽현장 공유를 통한 안 전 감독 등 여러 상황에서 활용할 수 있다는 효과가 있다."}
{"patent_id": "10-2022-0181228", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "\" 및 첨부 \"도면\"에 포함되어 있다.s 본 발명의 이점 및/또는 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 각종 실시 예를 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 각 실시 예의 구성만으로 한정되는 것이 아니라 서로 다른 다양한 형태로 도 구현될 수도 있으며, 단지 본 명세서에서 개시한 각각의 실시 예는 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 본 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구범위의 각 청구항의 범주에 의해 정의될 뿐임을 알아야 한다."}
{"patent_id": "10-2022-0181228", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "발명의 효과 본 발명에 의하면, 가상공간에서 자신이 원하는 데이터를 생성할 수 있으므로 공간적 시간적 제약이 적고, 다양 한 데이터를 생성할 수 있으므로 현실에서 인식할 수 있는 데이터도 더 다양해진다. 또한 인공지능 네트워크를 사용하여 물체의 자세 추정 정확도를 높임으로써 가상공간에서 실제 물체에 대한 가 상물체를 더 정확하게 생성 및 배치할 수 있다. 이렇게 가상공간에서 실제 물체에 대해 가상물체를 높은 정확도로 증강시켜 공장 및 건설 현장 등 산업현장에 적용할 경우 작업 효율을 향상시킬 수 있다는 효과가 있다. 또한, 본 발명에서 제안하는 방법은, 현실 공간을 가상 공간으로 이동시키기 위한 기반의 기술이 될 수 있을 것 이다. 예를 들어, 작업 상황 등을 원격으로 공유할 수 있다. 따라서, 건설현장, 공장 등의 장소 및 현장에서 비숙련자의 교육에 활용할 수 있으며, 작엽현장 공유를 통한 안 전 감독 등 여러 상황에서 활용할 수 있다는 효과가 있다."}
{"patent_id": "10-2022-0181228", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "도면의 간단한 설명 도 1은 ape에 대한 데이터 검증 시각화를 나타내는 도면. 도 2는 duck에 대한 자세 추론 결과를 RGB이미지로 나타내는 도면. 도 3은 인공지능을 이용한 물체 자세 추정 방법의 진행방법을 나타내는 순서도. 도 4는 인공지능을 이용한 물체 자세 추정 시스템의 구성을 나타내는 도면. 도 5는 ape에 대한 데이터 검증 시각화를 확대하여 나타내는 도면. 도 6은 가상환경에서 생성된 데이터셋 샘플 중 RGB에 대해 나타내는 도면. 도 7은 가상환경에서 생성된 데이터셋 샘플 중 Depth에 대해 나타내는 도면. 도 8은 가상환경에서 생성된 데이터셋 샘플 중 Mask에 대해 나타내는 도면. 도 9는 본 발명에서 제작한 데이터로 학습한 세그먼테이션(SegNet)의 입력(input) 이미지를 RGB 카메라로 촬영 한 장면을 나타내는 도면. 도 10은 도 9의 각 모델별 추론 결과를 Mask 이미지로 나타내는 도면. 도 11은 도 9의 각 모델별 추론 결과를 Mask 이미지로 나타내는 도면. 도 12는 도 9의 각 모델별 추론 결과를 Mask 이미지로 나타내는 도면.도 13은 본 발명의 실시예로 학습한 자세추정 네트워크(Masked-Fusion)의 추론 결과를 나타내는 도면."}
{"patent_id": "10-2022-0181228", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "발명을 실시하기 위한 구체적인 내용 본 발명을 상세하게 설명하기 전에, 본 명세서에서 사용된 용어나 단어는 통상적이거나 사전적인 의미로 무조건 한정하여 해석되어서는 아니 되며, 본 발명의 발명자가 자신의 발명을 가장 최선의 방법으로 설명하기 위해서 각종 용어의 개념을 적절하게 정의하여 사용할 수 있고, 더 나아가 이들 용어나 단어는 본 발명의 기술적 사상 에 부합하는 의미와 개념으로 해석되어야 함을 알아야 한다. 즉, 본 명세서에서 사용된 용어는 본 발명의 바람직한 실시 예를 설명하기 위해서 사용되는 것일 뿐이고, 본 발 명의 내용을 구체적으로 한정하려는 의도로 사용된 것이 아니며, 이들 용어는 본 발명의 여러 가지 가능성을 고 려하여 정의된 용어임을 알아야 한다. 또한, 본 명세서에서, 단수의 표현은 문맥상 명확하게 다른 의미로 지시하지 않는 이상, 복수의 표현을 포함할 수 있으며, 유사하게 복수로 표현되어 있다고 하더라도 단수의 의미를 포함할 수 있음을 알아야 한다. 본 명세서의 전체에 걸쳐서 어떤 구성 요소가 다른 구성 요소를 \"포함\"한다고 기재하는 경우에는, 특별히 반대 되는 의미의 기재가 없는 한 임의의 다른 구성 요소를 제외하는 것이 아니라 임의의 다른 구성 요소를 더 포함 할 수도 있다는 것을 의미할 수 있다. 더 나아가서, 어떤 구성 요소가 다른 구성 요소의 \"내부에 존재하거나, 연결되어 설치된다\"라고 기재한 경우에 는, 이 구성 요소가 다른 구성 요소와 직접적으로 연결되어 있거나 접촉하여 설치되어 있을 수 있고, 일정한 거 리를 두고 이격되어 설치되어 있을 수도 있으며, 일정한 거리를 두고 이격되어 설치되어 있는 경우에 대해서는 해당 구성 요소를 다른 구성 요소에 고정 내지 연결하기 위한 제 3의 구성 요소 또는 수단이 존재할 수 있으며, 이 제 3의 구성 요소 또는 수단에 대한 설명은 생략될 수도 있음을 알아야 한다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결\"되어 있다거나, 또는 \"직접 접속\"되어 있다고 기재되는 경우에는, 제 3의 구성 요소 또는 수단이 존재하지 않는 것으로 이해하여야 한다. 마찬가지로, 각 구성 요소 간의 관계를 설명하는 다른 표현들, 즉 \" ~ 사이에\"와 \"바로 ~ 사이에\", 또는 \" ~ 에 이웃하는\"과 \" ~ 에 직접 이웃하는\" 등도 마찬가지의 취지를 가지고 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 \"일면\", \"타면\", \"일측\", \"타측\", \"제 1\", \"제 2\" 등의 용어는, 사용된다면, 하나의 구성 요소에 대해서 이 하나의 구성 요소가 다른 구성 요소로부터 명확하게 구별될 수 있도록 하기 위해서 사용되며, 이와 같은 용어에 의해서 해당 구성 요소의 의미가 제한적으로 사용되는 것은 아님을 알아야 한다. 또한, 본 명세서에서 \"상\", \"하\", \"좌\", \"우\" 등의 위치와 관련된 용어는, 사용된다면, 해당 구성 요소에 대해 서 해당 도면에서의 상대적인 위치를 나타내고 있는 것으로 이해하여야 하며, 이들의 위치에 대해서 절대적인 위치를 특정하지 않는 이상은, 이들 위치 관련 용어가 절대적인 위치를 언급하고 있는 것으로 이해하여서는 아 니된다. 또한, 본 명세서에서는 각 도면의 각 구성 요소에 대해서 그 도면 부호를 명기함에 있어서, 동일한 구성 요소에 대해서는 이 구성 요소가 비록 다른 도면에 표시되더라도 동일한 도면 부호를 가지고 있도록, 즉 명세서 전체에 걸쳐 동일한 참조 부호는 동일한 구성 요소를 지시하고 있다. 본 명세서에 첨부된 도면에서 본 발명을 구성하는 각 구성 요소의 크기, 위치, 결합 관계 등은 본 발명의 사상 을 충분히 명확하게 전달할 수 있도록 하기 위해서 또는 설명의 편의를 위해서 일부 과장 또는 축소되거나 생략 되어 기술되어 있을 수 있고, 따라서 그 비례나 축척은 엄밀하지 않을 수 있다. 또한, 이하에서, 본 발명을 설명함에 있어서, 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 구성, 예 를 들어, 종래 기술을 포함하는 공지 기술에 대해 상세한 설명은 생략될 수도 있다. 이하, 본 발명의 실시 예에 대해 관련 도면들을 참조하여 상세히 설명하기로 한다. 본 발명은 인공지능을 이용한 물체 자세 추정 방법 및 시스템에 관한 것이다. 더욱 상세하게는, 가상의 환경에서 제작한 데이터를 학습하여 실제 물체의 위치와 자세를 추정하여 VR/AR 환경 에 해당 물체를 재구성하여 표현할 수 있다.또한, 본 발명은 RGB-D 카메라를 기반으로 현실 공간의 물체를 가상 공간으로 이동하고, 가상 공간에서 옮겨진 정보를 증강현실(AR, Augmentod Reality) 디스플레이를 이용하여 원거리에 있는 사용자에게 공유할 수 있다. 또한, 본 발명에서 물체의 정보는, 물체의 형상 및 6자유도 자세(6D pose)의 정보를 포함할 수 있다. 본 발명은 물체의 정보 인식 인공지능 모델 학습을 위한 학습 데이터 및 가상공간에서 생성하는 데이터 생성 도 구 개발을 포함할 수 있다. 본 발명은 가상공간에서 사용자가 원하는 데이터를 생성함으로써, 공간적 제약을 줄일수 있다. 또한, 좀 더 다양한 물체를 생성함으로써 물체에 대해 수집할 수 있는 데이터의 다양성을 확보해야 한다. 또한 상기 정보는 물체의 종류 및 자세를 포함할 수 있다. 근래에는 자동화 공장이나 산업현장에서 사영되는 부품이나 장비의 조립 민 운용에 대한 교육을 진행하고 있다. 또한, 홀로렌즈, 바이브와 같은 혼합혈실 기술을 활용하여 산업현장에서 교육을 진행하는 방식이 늘어나고 있다. 본 발명은 가상환경에서 실제 존재하는 물체를 재구성하여 데이터를 만들고 이를 인공지능으로 학습하여 실체 물체를 인식하고 위치와 자세를 추정하는 것을 목적으로 한다. 가상의 물체 데이터를 제작할 시 최대한 유사할 수 있게 고려하여 생성하는 것이 중요하다. 다양한 물체에 대한 데이터를 수집하기 위해 데이터를 생성하기 위해선 공간적 제약이 적은 가상의 공간에서 생 성해야 한다. 따라서 본 발명에서는 3D 애니메이션과 건축 시각화, 가상현실 등을 예시로 인터랙티브 콘텐츠 제작을 위한 통 합 제작 도구인 Unity를 사용한다. 첫 번째로 가상의 공간에 물체를 배치한다. 두 번째로 Unity 내부에 있는 카메라로 다양한 각도 및 위치에서 RGB, Depth, Mask를 찍는다. 세 번째로 찍고, 관찰 및 측정에 의해 제공되는 실체 혹은 사실로 알려진 정보인 GT(Ground Truth) 데이터를 생 성한다. 결과적으로 상기와 같이 생성된 데이터는 인공지능 모델에 사용하기에 적합한 형식 또는 구조로 변환하였다. 또한, 가상의 물체 데이터와 현실의 물체 데이터 사이의 괴리를 완전히 줄이기는 쉽지 않았다. 하지만 본 발명에서는 고품질 렌더링으로 상기 문제를 개선하고자 한다. 유니티(Unity)에서 지원하는 고해상도 렌더 파이프라인(HDRP)(High-Definition Render Pipeline)을 통해 참조 된 도면과 같은 장면을 구성하여 데이터셋(dataset)을 생성한다. 또한, 빛의 구성, 환경 및 고품질 렌더링을 지원하는 모델을 사용하여 최대한 현실과 가깝게 데이터를 생성한다. 또한, 본 발명은 고해상도 렌더 파이프라인(HDRP)(High-Definition Render Pipeline)을 지원하지만 현실에 존 재하지 않거나 획득하기 번거로운 경우 3D프린팅을 할 수 있다. 또는, 모델이 공개되어 수요가 용이하지만, 고해상도 렌더 파이프라인(HDRP)(High-Definition Render Pipeline)을 지원하지 않는 경우 고해상도 렌더 파이프라인(HDRP)(High-Definition Render Pipeline)가 지원하 는 파일의 형식으로 변경할 수 있다. 또한, 본 발명은 여러 가지 효과를 기대할 수 있다. 예를 들어, 가상공간에서 사용자가 원하는 데이터를 생성할 수 있으므로 공간적 시간적 제약이 적고, 다양한 데 이터를 생성할 수 있으므로 현실에서 인식할 수 있는 데이터도 더 다양해진다. 또한 인공지능 네트워크를 사용하여 물체의 자세 추정 정확도를 높임으로써 가상공간에서 실제 물체에 대한 가 상물체를 더 정확하게 생성 및 배치할 수 있다. 상기와 같이 가상공간에서 실제 물체에 대해 가상물체를 높은 정확도로 증강시켜 공장 및 건설 현장 등 산업현 장에 적용할 경우 작업 효율을 향상시킬 수 있다는 효과가 있다. 또한, 본 발명에서 제안하는 방법은, 현실 공간을 가상 공간으로 이동시키기 위한 기반의 기술이 될 수 있을 것 이다. 예를들어, 작업 상황 등을 원격으로 공유할 수 있다. 따라서, 건설현장, 공장 등의 장소 및 현장에서 비숙련자의 교육에 활용할 수 있으며, 작엽현장 공유를 통한 안 전 감독 등 여러 상황에서 활용할 수 있는 효과가 있다. 또한, 비대면 교육에서 교사가 사용하는 도구 및 자세를 학생에게 그대로 전달함으로써, 교육 효과를 높일 수 있는 효과가 있다. 또한, 본 발명에서 실시한 학습데이터 자동 생성 도구는 다양한 분야에서 인공지능 학습을 위한 데이터 생성에 응요할 수 있다는 효과가 있다. 상기의 예시를 더 포함하여, 여러 가지 실시예를 본 발명으로 인해 실시할 수 있다. 첨부된 도면을 참조하여 본 발명을 더욱 상세하게 설명하도록 한다. 도 1은 ape에 대한 데이터 검증 시각화를 나타내는 도면이고, 도 2는 duck에 대한 자세 추론 결과를 RGB이미지로 나타내는 도면이다. 도 3은 인공지능을 이용한 물체 자세 추정 방법의 진행방법을 나타내는 순서도이다. S100 단계는 생성부가 키넥트 카메라에서 촬영된 영상에 나타난 물체의 자세 추정에 필요한 데이터를 생성하는 단계이다. S100 단계에서 데이터는, RGB 정보, Depth 정보 및 Mask 각각의 이미지에 대한 정보 및 물체의 회전(rotation)정보, 이동(translation) 정보 중 적어도 하나를 포함하는 정보와, 상기 카메라 내부 파라미터의 정보를 포함할 수 있다. S200 단계는 검증부가 생성한 데이터에 대한 물체의 위치를 검증하는 단계이다. S200 단계는 3개의 단계로 더 나뉘어질 수 있다. 첫 번째는, 생성한 데이터의 회전(rotation)값과, 이동(translation)값을 3차원 점군 데이터 프로그램의 파일에 존재하는 각각의 최대점(vertex)에 곱할 수 있다. 상기 단계에서 3차원 점군 데이터 프로그램은, ply파일 일 수 있다. ply파일은, 3차원 점군 데이터를 저장하기 위해 설계된 파일이다. 이 때 점군 데이터는, 라이다(Lidar) 센서, RGB-D 센서 등으로 수집되는 데이터를 말할 수 있다. 두 번째는, 곱한 결과에 카메라 내부 파라미터 행렬을 곱하고, 곱한 결과의 z값으로 모든 최대점을 나눌 수 있 다. 상기에서 파라미터는, 범용 프로그램의 개개의 작업에 적용할 경우 필요한 수치정보를 말할 수 있다. 세 번째는, 나눈 결과를 RGB 이미지 위에 출력하여 RGB 이미지상의 물체의 위치와 생서된 데이터로부터 검증한 결과를 비교할 수 있다. S300 단계는, 추론부가 검증된 데이터를 바탕으로 인공지능으로 학습하여 물체의 위치와 자세를 추정하는 단계 이다. S300 단계 또한 3개의 단계로 더 나뉘어질 수 있다. 첫 번째는, RGB 및 Mask 이미지 정보로 세그멘테이션 네트워크(SegNet)를 학습할 수 있다. 이 때, 세그멘테이션은 프로그램을 세그먼트 단위로 나누는 프로그래머를 말할 수 있다. 또한, 세그먼트는, 컴퓨터 프로그램에서, 대형 프로그램을 조각으로 나누어 주기억 장치에 다운로드할 때 나누 어진 한 조각을 말할 수 있다. 두 번째는, RGB 정보, Depth 정보, 회전(roatation)값, 이동(translation)값 및 카메라 캘리브레이션(camere calibtaion) 행렬 정보를 세그멘테이션 네트워크(SegNet)로 추론된 Mask 이미지 정보를 이용하여 자세추정 네트 워크(Masked-Fusion)로 학습할 수 있다. 이 때, 카메라 캘리브레이션(camere calibtaion)은 카메라 파라미터를 추정하는 과정을 말할 수 있다. 자세추정 네트워크(Masked-Fusion)는, RGB-D 데이터를 사용하여 개체의 6자유도 자세(6D pose)를 추정하는 프레 임워크 일 수 있다. 프레임워크는, 소프트웨어 개발의 뼈대라고 할 수 있다. 즉, 어떤 목적을 달성하기 위해 복잡하게 얽혀있는 문제를 해결하기 위한 구조를 말할 수 있다. 카메라 파라미터는, 카메라에대한 모든 정보를 말할 수 있다. 세 번째는, 추론 결과를 RGB 이미지 위에 출력하여 시각화를 통해 평가할 수 있다. 또한, 본 발명에서는 키넥트로 찍은 물체를 인공지능을 통해 자세를 추정하고 홀로렌즈에 증강할 수 있다. 상기의 순서는, 키넥트 중 어느 하나로 물체를 찍어서 RGB 이미지와 Depth 이미지를 인공지능 네트워크인 자세 추정 네트워크(Masked-Fusion)으로 전달할 수 있다. 자세추정 네트워크(Masked-Fusion)에서 입력으로 들어온 RGB 및 Depth 이미지에 나타난 물체에 대한 자세를 추 정할 수 있다. 이 때, 자세에는 회전(rotation) 및 이동(translation) 정보가 포함될 수 있다. 자세를 추정한 결과로 나온 회전(rotation) 및 이동(translation) 정보의 행렬 값을 홀로렌즈가 가진 물체의 ply 파일에 적용하여 현실의 물체를 가상공간에 증강시킬 수 있다. 도 4는 인공지능을 이용한 물체 자세 추정 시스템의 구성을 나타내는 도면이다. 인공지능을 이용한 물체 자세 추정 시스템은, 생성부, 검증부 및 추론부로 이루어질 수 있다. 생성부는, 키넥트 카메라에서 촬영된 영상에 나타난 물체의 자세 추정에 필요한 데이터를 생성할 수 있다. 검증부는, 생성한 데이터에 대한 물체의 위치를 검증할 수 있다. 추론부는, 검증된 데이터를 바탕으로 인공지능으로 학습하여 물체의 위치와 자세를 추정할 수 있다. 도 5는 ape에 대한 데이터 검증 시각화를 확대하여 나타내는 도면이다. 도 5는 본 발명의 실시예로 ape에 대한 데이터 검증 시각화를 확대하여 나타낸 도면이다. 도 6은 가상환경에서 생성된 데이터셋 샘플 중 RGB에 대해 나타내는 도면이고, 도 7은 가상환경에서 생성된 데이터셋 샘플 중 Depth에 대해 나타내는 도면이고. 도 8은 가상환경에서 생성된 데이터셋 샘플 중 Mask에 대해 나타내는 도면이다. 도 6 내지 도 8은 같은 이미지를 가상환형에서 데이터셋(dataset)의 샘플을 생성하여 나타낸 것이다. 도 9는 본 발명에서 제작한 데이터로 학습한 세그먼테이션(SegNet)의 입력(input) 이미지를 나타내는 도면이다. 도 9는 RGB 본 발명에서 제작한 데이터로 학습한 세그먼테이션(SegNet)의 입력(input) 이미지를 카메라로 촬영 한 장면을 나타내는 도면이다도 10은 도 9의 각 모델별 추론 결과를 Mask 이미지로 나타내는 도면이고. 도 11은 도 9의 각 모델별 추론 결과를 Mask 이미지로 나타내는 도면이고. 도 12는 도 9의 각 모델별 추론 결과를 Mask 이미지로 나타내는 도면이다. 도 10 내지 도 12는 도 9의 추론 결과를 모델 별 Mask 이미지를 나타낸 도면이다. 도 13은 본 발명의 실시예로 학습한 자세추정 네트워크(Masked-Fusion)의 추론 결과를 나타내는 도면이다. 도 13은 가상환경에서 제작한 입력(input) 이미지로 학습한 자세추정 네트워크(Musked-Fusion)의 추론 결과를 나타내는 도면이다. 또한, 각 모델의 위치와 자세를 추정하여 포인트 클라우드(Point cloud)로 추정한 장면일 수 있다. 포인트 클라우드(Point cloud)는, 라이다(Lidar)센서, RGB-D 센서 등으로 수집되는 데이터를 말한다. 상기의 센서들은 물체에 빛 및 신호를 보내서 돌아오는 시간을 기록하여 빛 및 신호 당 거리 정보를 계산하고, 하나의 점을 생성한다. 포인트 클라우드(Point cloud)는 3차원 공간상에 퍼져 있는 여러 포인트의 집합을 의미한다. 포인트 클라우드(Point cloud)는 2D 이미지와 다르게 깊이(z축) 정보를 가지고 있기 때문에, 기본적으로 N X 3 넘파이(Numpy) 배열로 표현된다. 넘파이(Numpy)는, 행렬이나 일반적으로 대규모 다차원 배열을 쉽게 처리할 수 있도록 지원하는 파이썬의 라이 브러리이다 포인트 클라우드(Point cloud)에서 각 N 줄은 하나의 점과 맵핑이 되며, 3(x, y, z) 정보를 가지고 있다. 이상, 일부 예를 들어서 본 발명의 바람직한 여러 가지 실시 예에 대해서 설명하였지만, 본 \"발명을 실시하기 위한 구체적인 내용\" 항목에 기재된 여러 가지 다양한 실시 예에 관한 설명은 예시적인 것에 불과한 것이며, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자라면 이상의 설명으로부터 본 발명을 다양하게 변형하여 실 시하거나 본 발명과 균등한 실시를 행할 수 있다는 점을 잘 이해하고 있을 것이다. 또한, 본 발명은 다른 다양한 형태로 구현될 수 있기 때문에 본 발명은 상술한 설명에 의해서 한정되는 것이 아 니며, 이상의 설명은 본 발명의 개시 내용이 완전해지도록 하기 위한 것으로 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 본 발명의 범주를 완전하게 알려주기 위해 제공되는 것일 뿐이며, 본 발명은 청구범 위의 각 청구항에 의해서 정의될 뿐임을 알아야 한다. 본 출원에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 본 출원에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성 요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫 자,단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으 로 이해되어야 한다."}
{"patent_id": "10-2022-0181228", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 ape에 대한 데이터 검증 시각화를 나타내는 도면. 도 2는 duck에 대한 자세 추론 결과를 RGB이미지로 나타내는 도면. 도 3은 인공지능을 이용한 물체 자세 추정 방법의 진행방법을 나타내는 순서도. 도 4는 인공지능을 이용한 물체 자세 추정 시스템의 구성을 나타내는 도면. 도 5는 ape에 대한 데이터 검증 시각화를 확대하여 나타내는 도면. 도 6은 가상환경에서 생성된 데이터셋 샘플 중 RGB에 대해 나타내는 도면. 도 7은 가상환경에서 생성된 데이터셋 샘플 중 Depth에 대해 나타내는 도면. 도 8은 가상환경에서 생성된 데이터셋 샘플 중 Mask에 대해 나타내는 도면. 도 9는 본 발명에서 제작한 데이터로 학습한 세그먼테이션(SegNet)의 입력(input) 이미지를 RGB 카메라로 촬영 한 장면을 나타내는 도면. 도 10은 도 9의 각 모델별 추론 결과를 Mask 이미지로 나타내는 도면. 도 11은 도 9의 각 모델별 추론 결과를 Mask 이미지로 나타내는 도면. 도 12는 도 9의 각 모델별 추론 결과를 Mask 이미지로 나타내는 도면.도 13은 본 발명의 실시예로 학습한 자세추정 네트워크(Masked-Fusion)의 추론 결과를 나타내는 도면."}
