{"patent_id": "10-2023-0031871", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0104074", "출원번호": "10-2023-0031871", "발명의 명칭": "3차원 스캔 데이터의 치아 자동 분리 방법 및 이를 컴퓨터에서 실행시키기 위한 프로그램이", "출원인": "이마고웍스 주식회사", "발명자": "천소정"}}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "3차원의 스캔 데이터를 커팅하여 커팅 데이터를 생성하는 단계;상기 커팅 데이터를 미리 정해진 공간에 맵핑하여 맵핑 데이터를 생성하는 단계;상기 맵핑 데이터를 기초로 세그멘테이션 마스크를 판단하는 단계; 및상기 세그멘테이션 마스크를 상기 스캔 데이터 또는 상기 커팅 데이터에 맵핑하는 단계를 포함하는 3차원 스캔데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 3차원의 상기 스캔 데이터를 2차원 영상으로 변환하는 단계; 및상기 2차원 영상을 기초로 3차원 랜드마크를 판단하는 단계를 더 포함하고,상기 커팅 데이터를 생성하는 단계는 상기 3차원 랜드마크를 이용하여 상기 스캔 데이터를 커팅하며,상기 스캔 데이터를 상기 2차원 영상으로 변환하는 단계는 상기 스캔 데이터 내에서 포인트들이 형성하는 주축을 분석하여, 서로 수직인 제1 주축, 제2 주축 및 제3 주축을 판단하는 주축 정규화 단계를 포함하는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 3차원의 상기 스캔 데이터를 2차원 영상으로 변환하는 단계; 및상기 2차원 영상을 기초로 3차원 랜드마크를 판단하는 단계를 더 포함하고,상기 커팅 데이터를 생성하는 단계는 상기 3차원 랜드마크를 이용하여 상기 스캔 데이터를 커팅하며,상기 3차원 랜드마크를 판단하는 단계는상기 2차원 영상으로부터 2차원 랜드마크를 판단하는 단계; 및상기 2차원 랜드마크를 상기 3차원 랜드마크로 변환하는 단계를 포함하는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 커팅 데이터를 생성하는 단계는 3차원 랜드마크를 이용하여 상기 스캔 데이터를 커팅하며,상기 3차원 랜드마크는 제1 측의 최외곽 치아 내에 배치되는 제1 점;2개의 중절치의 사이에 배치되는 제2 점; 및제2 측의 최외곽 치아 내에 배치되는 제3 점을 포함하는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2023-0104074-3-제4항에 있어서, 상기 커팅 데이터를 생성하는 단계는 상기 제1 점 및 상기 제3 점에 의해 정의되는 제1 측면 노멀 벡터, 상기 제1 점 및 상기 제3 점에 의해 정의되는 제2 측면 노멀 벡터, 상기 제1 점, 상기 제2 점 및 상기 제3 점에 의해 생성되는 하면 노멀 벡터 및 상기 제1 측면 노멀 벡터 및 상기 하면 노멀 벡터에 의해 생성되는 후면 노멀 벡터를 이용하는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 제1 점이 P1, 상기 제2 점이 P2, 상기 제3 점이 P3, 상기 제1 측면 노멀 벡터가 nside1, 상기 제2 측면 노멀 벡터가 nside2, 상기 하면 노멀 벡터가 ndown, 상기 후면 노멀 벡터가 nback일 때,, ,, 및인 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서, 상기 커팅 데이터를 생성하는 단계는상기 제1 점으로부터 상기 제1 측으로 치아의 바깥쪽을 향하여 이동한 제1 커팅 점에서 상기 제1 측면 노멀 벡터를 노멀 벡터로 갖는 제1 커팅 평면을 이용하여 상기 스캔 데이터를 커팅하는 단계를 포함하는 것을 특징으로하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제5항에 있어서, 상기 커팅 데이터를 생성하는 단계는상기 제3 점으로부터 상기 제2 측으로 치아의 바깥쪽을 향하여 이동한 제2 커팅 점에서 상기 제2 측면 노멀 벡터를 노멀 벡터로 갖는 제2 커팅 평면을 이용하여 상기 스캔 데이터를 커팅하는 단계를 포함하는 것을 특징으로하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제5항에 있어서, 상기 커팅 데이터를 생성하는 단계는상기 제1 점 및 상기 제3 점의 중점으로부터 하면을 향하여 이동한 제3 커팅 점에서 상기 하면 노멀 벡터를 노멀 벡터로 갖는 제3 커팅 평면을 이용하여 상기 스캔 데이터를 커팅하는 단계를 포함하는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제5항에 있어서, 상기 커팅 데이터를 생성하는 단계는상기 제1 점 및 상기 제3 점의 중점으로부터 후면을 향하여 이동한 제4 커팅 점에서 상기 후면 노멀 벡터를 노멀 벡터로 갖는 제4 커팅 평면을 이용하여 상기 스캔 데이터를 커팅하는 단계를 포함하는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제4항에 있어서, 상기 3차원 랜드마크 및 상기 커팅 데이터를 이용하여 앵커 포인트를 판단하는 단계를 더 포함하고,공개특허 10-2023-0104074-4-상기 앵커 포인트를 판단하는 단계는상기 제1 점, 상기 제2 점 및 상기 제3 점을 연결한 곡선을 이용하는 것을 특징으로 하는 3차원 스캔 데이터의치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 앵커 포인트를 판단하는 단계는상기 제1 점에서 상기 곡선의 기울기를 노멀 벡터로 갖는 제1 평면 및 상기 제3 점에서 상기 곡선의 기울기를노멀 벡터로 갖는 제2 평면을 이용하는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 앵커 포인트를 판단하는 단계는상기 제1 평면과 상기 커팅 데이터가 만나는 점 중 최외곽 점 2개를 제1 앵커 포인트 및 제2 앵커 포인트로 판단하고, 상기 제2 평면과 상기 커팅 데이터가 만나는 점 중 최외곽 점 2개를 제3 앵커 포인트 및 제4 앵커 포인트로 판단하는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 미리 정해진 공간은 직사각형이고, 상기 맵핑 데이터를 생성하는 단계는상기 제1 앵커 포인트, 상기 제2 앵커 포인트, 상기 제3 앵커 포인트 및 상기 제4 앵커 포인트를 상기 직사각형의 4개의 꼭지점에 대응시켜 상기 맵핑 데이터를 생성하는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1항에 있어서, 상기 맵핑 데이터를 생성하는 단계는상기 커팅 데이터를 상기 커팅 데이터 내의 각 점의 곡률값을 나타내는 곡률 데이터로 변환하는 단계를 포함하는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서, 상기 곡률 데이터는 상기 각 점의 최소 곡률값을 나타내는 것을 특징으로 하는 3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서, 상기 각 점의 상기 최소 곡률값이 크면 흰색으로 표현되고, 상기 각 점의 상기 최소 곡률값이작으면 검은색으로 표현되도록 상기 곡률 데이터의 그레이를 반전하는 단계를 더 포함하는 것을 특징으로 하는3차원 스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제1항에 있어서, 상기 스캔 데이터를 변환하여 생성된 2차원 영상을 기초로 3차원 랜드마크가 판단되고, 2차원의 상기 맵핑 데이터를 기초로 2차원의 상기 세그멘테이션 마스크가 판단되는 것을 특징으로 하는 3차원스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "3차원의 스캔 데이터를 커팅하여 커팅 데이터를 생성하는 단계;상기 커팅 데이터를 미리 정해진 공간에 맵핑하여 맵핑 데이터를 생성하는 단계;공개특허 10-2023-0104074-5-상기 맵핑 데이터를 기초로 세그멘테이션 마스크를 판단하는 단계; 및상기 세그멘테이션 마스크를 상기 스캔 데이터 또는 상기 커팅 데이터에 맵핑하는 단계를 포함하는 3차원 스캔데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서, 상기 3차원의 상기 스캔 데이터를 기초로 3차원 랜드마크가 판단되고, 상기 3차원 랜드마크를 이용하여 상기 스캔 데이터가 커팅되며,2차원의 상기 맵핑 데이터를 기초로 2차원의 상기 세그멘테이션 마스크가 판단되는 것을 특징으로 하는 3차원스캔 데이터의 치아 자동 분리 방법."}
{"patent_id": "10-2023-0031871", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제1항 내지 제20항 중 어느 한 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램이 기록된 컴퓨터로 읽을 수있는 기록 매체."}
{"patent_id": "10-2023-0031871", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "3차원 스캔 데이터의 치아 자동 분리 방법은 3차원의 스캔 데이터를 2차원 영상으로 변환하는 단계, 2차원 영상 을 입력 받는 제1 인공지능 신경망을 이용하여 3차원 랜드마크를 판단하는 단계, 3차원 랜드마크를 이용하여 스 캔 데이터를 커팅하여 커팅 데이터를 생성하는 단계, 3차원 랜드마크 및 커팅 데이터를 이용하여 앵커 포인트를 판단하는 단계, 앵커 포인트를 이용하여 커팅 데이터를 미리 정해진 공간에 맵핑하여 맵핑 데이터를 생성하는 단 계, 맵핑 데이터를 입력 받는 제2 인공지능 신경망을 이용하여 세그멘테이션 마스크를 판단하는 단계 및 세그멘 테이션 마스크를 스캔 데이터 또는 커팅 데이터에 맵핑하는 단계를 포함한다."}
{"patent_id": "10-2023-0031871", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 3차원 스캔 데이터의 치아 자동 분리 방법 및 이를 컴퓨터에서 실행시키기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록 매체에 관한 것으로, 더욱 상세하게는 메쉬 파라미터라이제이션(mesh parameterization) 및 딥러닝(deep learning)을 통해 자동으로 수행되어 스캔 데이터에서 치아를 분리하기 위한 시간과 노력을 감소시킬 수 있는 3차원 스캔 데이터의 치아 자동 분리 방법 및 이를 컴퓨터에서 실행시키기 위 한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록 매체에 관한 것이다."}
{"patent_id": "10-2023-0031871", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "치과에서 진단, 분석, 보철물 제작 등을 위해 환자의 3차원 스캔 데이터에서 치아를 분리하는 기술이 필요하다. 특히, 치과계에서 구강 스캐너를 이용한 디지털 교정이 늘어가고 있다. 교정에서는 치아의 배열, 교합 등을 예 측하고 이에 적합한 계획 수립이 중요한데 이를 위해서는 치아 분리가 필수적이다. 치아 분리를 위해 널리 사용되는 방법은 다음과 같다. 먼저, 구강 스캐너로 치아 스캔 데이터를 얻는다. 그 후 작업자가 치아의 경계를 수작업으로 지정해주고, 축 정보와 치아의 경계를 이용하여 치아 분리에 사용할 평면을 지정하며, 마지막으로 치아와 치아 사이의 분리된 면을 확인하고 필요한 부분을 수정하여 마무리한다. 그리고 이와 같은 과정을 모든 치아에 대하여 반복적으로 수행하여 분리된 치아 데이터를 획득할 수 있다. 이와 같이 작업자가 수작업을 하는 경우, 3차원 데이터에 대해 2차원 화면을 통해 육안으로 경계를 지정하여야 하므로 그 정확도가 떨어지며, 작업자의 높은 숙련도와 많은 시간을 요구하는 문제가 있다."}
{"patent_id": "10-2023-0031871", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 이루고자 하는 목적은 메쉬 파라미터라이제이션(mesh parameterization) 및 딥러닝을 통해 자동으로 수행되어 스캔 데이터에서 치아를 분리하기 위한 시간과 노력을 감소시키고 정확도를 향상시킬 수 있는 3차원 스캔 데이터의 치아 자동 분리 방법을 제공하는 것이다. 본 발명이 이루고자 하는 다른 목적은 상기 3차원 스캔 데이터의 치아 자동 분리 방법을 컴퓨터에서 실행시키기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록 매체를 제공하는 것이다."}
{"patent_id": "10-2023-0031871", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 본 발명의 목적을 실현하기 위한 일 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법은 3차원의 스캔 데이터를 2차원 영상으로 변환하는 단계, 상기 2차원 영상을 입력 받는 제1 인공지능 신경망을 이용하여 3차원 랜드마크를 판단하는 단계, 상기 3차원 랜드마크를 이용하여 상기 스캔 데이터를 커팅하여 커팅 데이터를 생성하는 단계, 상기 3차원 랜드마크 및 상기 커팅 데이터를 이용하여 앵커 포인트를 판단하는 단계, 상기 앵커 포인트를 이용하여 상기 커팅 데이터를 미리 정해진 공간에 맵핑하여 맵핑 데이터를 생성하는 단계, 상기 맵핑 데이터를 입력 받는 제2 인공지능 신경망을 이용하여 세그멘테이션 마스크를 판단하는 단계 및 상기 세그멘테이 션 마스크를 상기 스캔 데이터 또는 상기 커팅 데이터에 맵핑하는 단계를 포함한다. 본 발명의 일 실시예에 있어서, 상기 스캔 데이터를 상기 2차원 영상으로 변환하는 단계는 상기 스캔 데이터 내 에서 포인트들이 형성하는 주축을 분석하여, 서로 수직인 제1 주축, 제2 주축 및 제3 주축을 판단하는 주축 정 규화 단계를 포함할 수 있다. 본 발명의 일 실시예에 있어서, 상기 제1 인공지능 신경망을 이용하여 상기 3차원 랜드마크를 판단하는 단계는 상기 제1 인공지능 신경망을 이용하여 상기 2차원 영상으로부터 2차원 랜드마크를 판단하는 단계 및 상기 2차원 랜드마크를 상기 3차원 랜드마크로 변환하는 단계를 포함할 수 있다. 본 발명의 일 실시예에 있어서, 상기 3차원 랜드마크는 제1 측의 최외곽 치아 내에 배치되는 제1 점, 2개의 중 절치의 사이에 배치되는 제2 점 및 제2 측의 최외곽 치아 내에 배치되는 제3 점을 포함할 수 있다. 본 발명의 일 실시예에 있어서, 상기 커팅 데이터를 생성하는 단계는 상기 제1 점 및 상기 제3 점에 의해 정의 되는 제1 측면 노멀 벡터, 상기 제1 점 및 상기 제3 점에 의해 정의되는 제2 측면 노멀 벡터, 상기 제1 점, 상 기 제2 점 및 상기 제3 점에 의해 생성되는 하면 노멀 벡터 및 상기 제1 측면 노멀 벡터 및 상기 하면 노멀 벡 터에 의해 생성되는 후면 노멀 벡터를 이용할 수 있다. 본 발명의 일 실시예에 있어서, 상기 제1 점이 P1, 상기 제2 점이 P2, 상기 제3 점이 P3, 상기 제1 측면 노멀 벡 터가 nside1, 상기 제2 측면 노멀 벡터가 nside2, 상기 하면 노멀 벡터가 ndown, 상기 후면 노멀 벡터가 nback일 때, , , , 및 일 수 있다. 본 발명의 일 실시예에 있어서, 상기 커팅 데이터를 생성하는 단계는 상기 제1 점으로부터 상기 제1 측으로 치 아의 바깥쪽을 향하여 이동한 제1 커팅 점에서 상기 제1 측면 노멀 벡터를 노멀 벡터로 갖는 제1 커팅 평면을 이용하여 상기 스캔 데이터를 커팅하는 단계를 포함할 수 있다. 본 발명의 일 실시예에 있어서, 상기 커팅 데이터를 생성하는 단계는 상기 제3 점으로부터 상기 제2 측으로 치 아의 바깥쪽을 향하여 이동한 제2 커팅 점에서 상기 제2 측면 노멀 벡터를 노멀 벡터로 갖는 제2 커팅 평면을 이용하여 상기 스캔 데이터를 커팅하는 단계를 포함할 수 있다. 본 발명의 일 실시예에 있어서, 상기 커팅 데이터를 생성하는 단계는 상기 제1 점 및 상기 제3 점의 중점으로부 터 하면을 향하여 이동한 제3 커팅 점에서 상기 하면 노멀 벡터를 노멀 벡터로 갖는 제3 커팅 평면을 이용하여 상기 스캔 데이터를 커팅하는 단계를 포함할 수 있다. 본 발명의 일 실시예에 있어서, 상기 커팅 데이터를 생성하는 단계는 상기 제1 점 및 상기 제3 점의 중점으로부 터 후면을 향하여 이동한 제4 커팅 점에서 상기 후면 노멀 벡터를 노멀 벡터로 갖는 제4 커팅 평면을 이용하여 상기 스캔 데이터를 커팅하는 단계를 포함할 수 있다. 본 발명의 일 실시예에 있어서, 상기 앵커 포인트를 판단하는 단계는 상기 제1 점, 상기 제2 점 및 상기 제3 점 을 연결한 곡선을 이용할 수 있다. 본 발명의 일 실시예에 있어서, 상기 앵커 포인트를 판단하는 단계는 상기 제1 점에서 상기 곡선의 기울기를 노 멀 벡터로 갖는 제1 평면 및 상기 제3 점에서 상기 곡선의 기울기를 노멀 벡터로 갖는 제2 평면을 이용할 수 있 다. 본 발명의 일 실시예에 있어서, 상기 앵커 포인트를 판단하는 단계는 상기 제1 평면과 상기 커팅 데이터가 만나 는 점 중 최외곽 점 2개를 제1 앵커 포인트 및 제2 앵커 포인트로 판단하고, 상기 제2 평면과 상기 커팅 데이터 가 만나는 점 중 최외곽 점 2개를 제3 앵커 포인트 및 제4 앵커 포인트로 판단할 수 있다. 본 발명의 일 실시예에 있어서, 상기 미리 정해진 공간은 직사각형일 수 있다. 상기 맵핑 데이터를 생성하는 단 계는 상기 제1 앵커 포인트, 상기 제2 앵커 포인트, 상기 제3 앵커 포인트 및 상기 제4 앵커 포인트를 상기 직사각형의 4개의 꼭지점에 대응시켜 상기 맵핑 데이터를 생성할 수 있다. 본 발명의 일 실시예에 있어서, 상기 맵핑 데이터를 생성하는 단계는 상기 커팅 데이터를 상기 커팅 데이터 내 의 각 점의 곡률값을 나타내는 곡률 데이터로 변환하는 단계를 포함할 수 있다. 본 발명의 일 실시예에 있어서, 상기 곡률 데이터는 상기 각 점의 최소 곡률값을 나타낼 수 있다. 본 발명의 일 실시예에 있어서, 상기 3차원 스캔 데이터의 치아 자동 분리 방법은 상기 각 점의 상기 최소 곡률 값이 크면 흰색으로 표현되고, 상기 각 점의 상기 최소 곡률값이 작으면 검은색으로 표현되도록 상기 곡률 데이 터의 그레이를 반전하는 단계를 더 포함할 수 있다. 본 발명의 일 실시예에 있어서, 상기 제1 인공지능 신경망은 상기 스캔 데이터를 변환하여 생성된 상기 2차원 영상을 입력 받아 상기 3차원 랜드마크를 판단할 수 있다. 상기 제2 인공지능 신경망은 2차원의 상기 맵핑 데이 터를 입력 받아 2차원의 상기 세그멘테이션 마스크를 판단할 수 있다. 상기한 본 발명의 목적을 실현하기 위한 일 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법은 3차원의 스캔 데이터를 입력 받는 제1 인공지능 신경망을 이용하여 3차원 랜드마크를 판단하는 단계, 상기 3차원 랜드마 크를 이용하여 상기 스캔 데이터를 커팅하여 커팅 데이터를 생성하는 단계, 상기 3차원 랜드마크 및 상기 커팅 데이터를 이용하여 앵커 포인트를 판단하는 단계, 상기 앵커 포인트를 이용하여 상기 커팅 데이터를 미리 정해 진 공간에 맵핑하여 맵핑 데이터를 생성하는 단계, 상기 맵핑 데이터를 입력 받는 제2 인공지능 신경망을 이용 하여 세그멘테이션 마스크를 판단하는 단계 및 상기 세그멘테이션 마스크를 상기 스캔 데이터 또는 상기 커팅 데이터에 맵핑하는 단계를 포함한다. 본 발명의 일 실시예에 있어서, 상기 제1 인공지능 신경망은 상기 3차원의 상기 스캔 데이터를 입력 받아 상기 3차원 랜드마크를 판단할 수 있다. 상기 제2 인공지능 신경망은 2차원의 상기 맵핑 데이터를 입력 받아 2차원의 상기 세그멘테이션 마스크를 판단할 수 있다. 상기한 본 발명의 목적을 실현하기 위한 일 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법은 3차원의 스캔 데이터 및 상기 스캔 데이터의 3차원 랜드마크를 이용하여 앵커 포인트를 판단하는 단계, 상기 앵커 포인 트를 이용하여 상기 스캔 데이터를 미리 정해진 공간에 맵핑하여 맵핑 데이터를 생성하는 단계, 상기 맵핑 데이 터를 입력 받는 인공지능 신경망을 이용하여 세그멘테이션 마스크를 판단하는 단계 및 상기 세그멘테이션 마스 크를 상기 스캔 데이터에 맵핑하는 단계를 포함한다. 본 발명의 일 실시예에 있어서, 상기 3차원 스캔 데이터의 치아 자동 분리 방법을 컴퓨터에서 실행시키기 위한 프로그램은 컴퓨터로 읽을 수 있는 기록 매체에 기록될 수 있다."}
{"patent_id": "10-2023-0031871", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따른 3차원 스캔 데이터의 치아 자동 분리 방법에 따르면, 메쉬 파라미터라이제이션 및 딥러닝을 통 해 치아 분리가 완전 자동으로 수행되어 스캔 데이터에서 치아를 분리하기 위한 시간과 노력을 감소시킬 수 있 다. 제1 인공지능 신경망을 이용하여 3차원 랜드마크를 자동으로 판단하고, 상기 3차원 랜드마크를 이용하여 스캔 데이터를 커팅하며, 상기 3차원 랜드마크 및 커팅 데이터를 이용하여 앵커 포인트를 판단하고, 상기 앵커 포인 트를 이용하여 상기 커팅 데이터를 미리 정해진 공간에 맵핑할 수 있다. 상기 미리 정해진 공간에 맵핑된 영상 을 제2 인공지능 신경망의 입력으로 활용하여 세그멘테이션 마스크를 판단하여, 치아 자동 분리의 정확도를 더 욱 향상시킬 수 있다."}
{"patent_id": "10-2023-0031871", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본문에 개시되어 있는 본 발명의 실시예들에 대해서, 특정한 구조적 내지 기능적 설명들은 단지 본 발명의 실시 예를 설명하기 위한 목적으로 예시된 것으로, 본 발명의 실시예들은 다양한 형태로 실시될 수 있으며 본문에 설 명된 실시예들에 한정되는 것으로 해석되어서는 아니 된다. 본 발명은 다양한 변경을 가할 수 있고 여러 가지 형태를 가질 수 있는바, 특정 실시예들을 도면에 예시하고 본 문에 상세하게 설명하고자 한다. 그러나 이는 본 발명을 특정한 개시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의 해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로 사용될 수 있다. 예를 들어, 본 발명의 권리 범위로부터 이탈되지 않은 채 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관 계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃하는\"과 \"~에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다.본 출원에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 기재된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미이다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미인 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 한편, 어떤 실시예가 달리 구현 가능한 경우에 특정 블록 내에 명기된 기능 또는 동작이 순서도에 명기된 순서 와 다르게 일어날 수도 있다. 예를 들어, 연속하는 두 블록이 실제로는 실질적으로 동시에 수행될 수도 있고, 관련된 기능 또는 동작에 따라서는 상기 블록들이 거꾸로 수행될 수도 있다. 이하, 첨부한 도면들을 참조하여, 본 발명의 바람직한 실시예를 보다 상세하게 설명하고자 한다. 도면상의 동일 한 구성요소에 대해서는 동일한 참조부호를 사용하고 동일한 구성요소에 대해서 중복된 설명은 생략한다. 도 1은 본 발명의 일 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법을 나타내는 순서도이다. 도 2는 도 1의 치아 자동 분리 방법을 나타내는 개념도이다. 도 1 및 도 2를 참조하면, 본 발명에서는 메쉬 파라미터라이제이션(mesh parameterization) 및 딥러닝을 이용하 여 3차원 스캔 데이터로부터 개별 치아를 완전 자동으로 분할할 수 있다. 예를 들어, 상기 스캔 데이터는 폴리 곤메쉬(polygon mesh)로 이루어질 수 있으므로, 도 2 및 도 3에서 Mesh로 기재하였다. 일반적으로 딥러닝 네트워크는 일정한 크기의 데이터(정형 데이터)만을 입력 받을 수 있다. 상기 스캔 데이터는 3차원 폴리곤메쉬로 표현되며, 메쉬의 점이나 엣지, 셀의 개수가 데이터마다 다르다는 문제가 있다. 따라서, 상 기 스캔 데이터를 입력으로 받는 딥러닝 네트워크의 정확도를 향상시키기 위해서는 상기 스캔 데이터를 정형 데 이터로 만들어주는 과정이 필요할 수 있다. 본 발명에서는 상기 스캔 데이터를 정형 데이터로 만들어주기 위해 메쉬 파라미터라이제이션을 이용할 수 있다. 상기 메쉬 파라미터라이제이션은 3차원 메쉬를 구성하는 점들을 미리 정해진 다른 공간으로 일대일 맵핑시켜주 는 것을 의미할 수 있다. 메쉬가 존재하는 공간에서 상기 미리 정해진 다른 공간(파라미터 공간)으로 맵핑을 하 기 위해서는 먼저 상기 메쉬가 존재하는 공간의 바운더리를 상기 파라미터 공간의 바운더리로 맵핑할 수 있다. 상기 메쉬가 존재하는 공간의 바운더리를 제외한 내부 점들은 상기 파라미터 공간의 바운더리의 내부로 맵핑시 킬 수 있으며, 이 때 에너지 함수를 사용하여 메쉬의 토폴로지(topology)를 최대한 유지하는 방법을 사용할 수 있다. 상기 메쉬 파라미터라이제이션을 이용하면, 상기 스캔 데이터를 2차원 공간으로 맵핑시킬 수 있기 때문에 상기 딥러닝에 이용하는 상기 정형 데이터를 적절하게 형성할 수 있다. 상기 메쉬 파라미터라이제이션을 적용할 때 바운더리의 맵핑 순서를 일정하게 해주기 위해 인공지능 신경망을 이용하여 치아의 랜드마크(특징점)를 얻을 수 있다. 본 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법은 3차원의 스캔 데이터를 2차원 영상으로 변환하는 단계(S100), 상기 2차원 영상을 입력 받는 제1 인공지능 신경망(AI 1)을 이용하여 3차원 랜드마크를 판단하는 단계(S200), 상기 3차원 랜드마크를 이용하여 상기 스캔 데이터를 커팅하여 커팅 데이터를 생성하는 단계 (S300), 상기 3차원 랜드마크 및 상기 커팅 데이터를 이용하여 앵커 포인트를 판단하는 단계(S400), 상기 앵커 포인트를 이용하여 상기 커팅 데이터를 미리 정해진 공간에 맵핑하여 맵핑 데이터를 생성하는 단계(S500), 상기 맵핑 데이터를 입력 받는 제2 인공지능 신경망(AI 2)을 이용하여 세그멘테이션 마스크를 판단하는 단계(S600) 및 상기 세그멘테이션 마스크를 상기 스캔 데이터 또는 상기 커팅 데이터에 맵핑하는 단계(S700)를 포함할 수 있다. 본 실시예의 3차원 스캔 데이터의 치아 자동 분리 방법은 컴퓨팅 장치에 의해 수행될 수 있다. 도 3은 도 1의 스캔 데이터(Mesh)를 2차원 영상(Captured Image)으로 변환하는 단계(S100)를 나타내는 도면이다. 도 1 내지 도 3을 참조하면, 본 실시예에서, 상기 제1 인공지능 신경망(AI 1)의 입력은 2차원 영상이므로 상기 3차원의 스캔 데이터를 상기 2차원 영상으로 변환하는 단계를 포함할 수 있다. 예를 들어, 상기 2차원 영상은 레드 계조, 그린 계조 및 블루 계조를 포함하는 RGB 계조 데이터일 수 있다. 예 를 들어, 상기 2차원 영상은 흑백 계조 데이터일 수 있다. 예를 들어, 상기 2차원 영상은 뎁스 정보를 포함하는 뎁스 맵(depth map)일 수 있다. 상기 스캔 데이터를 상기 2차원 영상으로 변환하는 단계는 주축 정규화 단계를 포함할 수 있다. 상기 주축 정규 화는 주축 분석을 통해 공간상 오리엔테이션을 설정해주는 것이다. 상기 주축 정규화 단계에서는 상기 스캔 데 이터 내에서 포인트들이 형성하는 주축을 분석하여, 서로 수직인 제1 주축, 제2 주축 및 제3 주축을 판단할 수 있다. 상기 주축 분석으로 추출된 상기 제1 주축, 상기 제2 주축 및 상기 제3 주축 중 가장 긴 축은 치아의 U-shape의 좌우 방향으로 판단할 수 있다. 상기 제1 주축, 상기 제2 주축 및 상기 제3 주축 중 가장 짧은 축은 상기 U- shape의 상하 방향으로 판단할 수 있다. 상기 제1 주축, 상기 제2 주축 및 상기 제3 주축 중 두 번째로 긴 축은 상기 U-shape의 전후 방향으로 판단할 수 있다. 상기 주축 정규화 단계를 통해 치아의 교합면이 최대한 잘 보이도록 상기 스캔 데이터를 정렬할 수 있다. 도 4는 도 1의 제1 인공지능 신경망(AI 1)을 이용하여 3차원 랜드마크(3D Landmarks)를 판단하는 단계(S200)를 나타내는 도면이다. 도 1 내지 도 4를 참조하면, 상기 제1 인공지능 신경망(AI 1)은 상기 2차원 영상을 입력 받아 3차원 랜드마크를 판단할 수 있다. 예를 들어, 상기 제1 인공지능 신경망(AI 1)의 입력은 상기 S100 단계에서 생성된 상기 2차원 영상이고, 상기 제1 인공지능 신경망(AI 1)의 출력은 3차원 랜드마크의 좌표일 수 있다. 이와는 달리, 상기 제1 인공지능 신경망(AI 1)을 이용하여 3차원 랜드마크를 판단하는 단계(S200)는 상기 제1 인공지능 신경망(AI 1)을 이용하여 상기 2차원 영상으로부터 2차원 랜드마크를 판단하는 단계 및 상기 2차원 랜 드마크를 상기 3차원 랜드마크로 변환하는 단계를 포함할 수도 있다. 이 때에는 상기 제1 인공지능 신경망(AI 1)의 입력은 상기 S100 단계에서 생성된 상기 2차원 영상이고, 상기 제 1 인공지능 신경망(AI 1)의 출력은 2차원 랜드마크 좌표일 수 있다. 상기 2차원 랜드마크를 상기 3차원 랜드마크로 변환하는 단계는 상기 S100 단계에서 수행한 변환의 역 변환일 수 있다. 예를 들어, 상기 제1 인공지능 신경망(AI 1)은 컨볼루셔널 신경망(Convolutional neural network, CNN)일 수 있다. 상기 3차원 랜드마크는 적어도 3개 이상의 치아 특징점들을 포함할 수 있다. 예를 들어, 상기 3차원 랜드마크는 제1 측의 최외곽 치아 내에 배치되는 제1 점(P1(x1, y1, z1)), 2개의 중절치의 사이에 배치되는 제2 점(P2(x2, y2, z2)) 및 제2 측의 최외곽 치아 내에 배치되는 제3 점(P3(x3, y3, z3))을 포함할 수 있다. 예를 들어, 상기 제 1 점(P1(x1, y1, z1))은 상기 제1 측의 가장 안쪽의 대구치의 표면 중심점에 배치될 수 있다. 예를 들어, 상기 제3 점(P3(x3, y3, z3))은 상기 제2 측의 가장 안쪽의 대구치의 표면 중심점에 배치될 수 있다. 예를 들어, 상기 제2 점(P2(x2, y2, z2))은 상기 2개의 중절치의 중심점에 배치될 수 있다. 상기 3차원 랜드마크는 상기 스캔 데이터 내에서 필요 없는 부분을 잘라내기 위해 사용될 수 있고, 메쉬 파라미 터라이제이션을 수행할 때에 기준이 되는 앵커 포인트를 설정하기 위해 사용될 수 있다. 본 실시예에서는 상기 3차원 랜드마크가 3개의 점을 포함하는 경우를 예시하였으나, 본 발명은 이에 한정되지 않으며, 상기 3차원 랜드마크는 3개보다 많은 점을 포함할 수도 있다. 상기 3차원 랜드마크가 포함하는 점의 개 수가 많아지면, 메쉬 파라미터라이제이션의 정확도가 높아질 수 있는 반면, 연산 로드가 증가할 수 있다. 도 5는 도 1의 상기 3차원 랜드마크(3D Landmarks)를 이용하여 상기 스캔 데이터를 커팅하는 단계(S300)를 나타 내는 도면이다. 도 6은 도 1의 상기 3차원 랜드마크를 이용하여 상기 스캔 데이터를 커팅하는 단계(S300)에서 사용되는 벡터들을 나타내는 도면이다. 도 7 및 도 8은 도 1의 상기 3차원 랜드마크를 이용하여 상기 스캔 데이 터를 커팅하는 단계(S300)에서 사용되는 커팅 평면들을 나타내는 도면이다.도 1 내지 도 8을 참조하면, 상기 스캔 데이터를 커팅하여 상기 커팅 데이터(Cut Mesh)를 생성하는 단계(S300) 는 상기 제1 점(P1) 및 상기 제3 점(P3)에 의해 정의되는 제1 측면 노멀 벡터(nside1), 상기 제1 점(P1) 및 상기 제3 점(P3)에 의해 정의되는 제2 측면 노멀 벡터(nside2), 상기 제1 점(P1), 상기 제2 점(P2) 및 상기 제3 점(P3) 에 의해 생성되는 하면 노멀 벡터(ndown) 및 상기 제1 측면 노멀 벡터(nside1) 및 상기 하면 노멀 벡터(ndown)에 의 해 생성되는 후면 노멀 벡터(nback)를 이용할 수 있다. 상기 커팅 데이터를 생성하는 단계(S300)는 상기 제1 측면 노멀 벡터(nside1), 상기 제2 측면 노멀 벡터(nside2), 상기 하면 노멀 벡터(ndown) 및 상기 후면 노멀 벡터(nback)를 이용하여 생성되는 제1 내지 제4 커팅 평면(CP1, CP2, CP3, CP4)을 이용하여 상기 스캔 데이터를 커팅하여 상기 커팅 데이터를 생성할 수 있다. 예를 들어, 상기 제1 점이 P1, 상기 제2 점이 P2, 상기 제3 점이 P3, 상기 제1 측면 노멀 벡터가 nside1, 상기 제2 측면 노멀 벡터가 nside2, 상기 하면 노멀 벡터가 ndown, 상기 후면 노멀 벡터가 nback일 때, , , , 및 을 만족할 수 있 다. 예를 들어, 상기 커팅 데이터를 생성하는 단계(S300)는 상기 제1 점(P1)으로부터 상기 제1 측으로 치아의 바깥 쪽을 향하여 이동한 제1 커팅 점에서 상기 제1 측면 노멀 벡터(nside1)를 노멀 벡터로 갖는 제1 커팅 평면(CP1)을 이용하여 상기 스캔 데이터를 커팅할 수 있다. 예를 들어, 상기 커팅 데이터를 생성하는 단계(S300)는 상기 제3 점(P3)으로부터 상기 제2 측으로 치아의 바깥 쪽을 향하여 이동한 제2 커팅 점에서 상기 제2 측면 노멀 벡터(nside2)를 노멀 벡터로 갖는 제2 커팅 평면(CP2)을 이용하여 상기 스캔 데이터를 커팅할 수 있다. 예를 들어, 상기 커팅 데이터를 생성하는 단계(S300)는 상기 제1 점(P1) 및 상기 제3 점(P3)의 중점으로부터 하 면을 향하여 이동한 제3 커팅 점에서 상기 하면 노멀 벡터(ndown)를 노멀 벡터로 갖는 제3 커팅 평면(CP3)을 이 용하여 상기 스캔 데이터를 커팅할 수 있다. 예를 들어, 상기 커팅 데이터를 생성하는 단계(S300)는 상기 제1 점(P1) 및 상기 제3 점(P3)의 중점으로부터 후 면을 향하여 이동한 제4 커팅 점에서 상기 후면 노멀 벡터(nback)를 노멀 벡터로 갖는 제4 커팅 평면(CP4)을 이 용하여 상기 스캔 데이터를 커팅할 수 있다. 상기 커팅 데이터(Cut Mesh)는 도 5의 우측 하단에 도시하였다. 도 9는 도 1의 상기 3차원 랜드마크(3D Landmarks) 및 커팅 데이터(Cut Mesh)를 이용하여 앵커 포인트(3D Anchor Points)를 판단하는 단계(S400)를 나타내는 도면이다. 도 10은 도 1의 상기 3차원 랜드마크(3D Landmarks) 및 상기 커팅 데이터(Cut Mesh)를 이용하여 앵커 포인트(3D Anchor Points)를 판단하는 단계(S40 0)에서 사용되는 스플라인 커브를 나타내는 도면이다. 도 11은 도 1의 상기 3차원 랜드마크(3D Landmarks) 및 상기 커팅 데이터(Cut Mesh)를 이용하여 앵커 포인트(3D Anchor Points)를 판단하는 단계(S400)에서 사용되는 평면들을 나타내는 도면이다. 도 12는 도 1의 상기 3차원 랜드마크(3D Landmarks) 및 상기 커팅 데이터(Cut Mesh)를 이용하여 앵커 포인트(3D Anchor Points)를 판단하는 단계(S400)에서 판단된 앵커 포인트들(3D Anchor Points)을 나타내는 도면이다. 도 1 내지 도 12를 참조하면, 도 10에서 보듯이, 상기 앵커 포인트를 판단하는 단계(S400)는 상기 3차원 랜드마 크의 상기 제1 점(P1), 상기 제2 점(P2) 및 상기 제3 점(P3)을 연결한 곡선을 이용할 수 있다. 예를 들어, 상기 곡선은 스플라인 커브(spline curve)일 수 있다. 도 11에서 보듯이, 상기 앵커 포인트를 판단하는 단계(S400)는 상기 제1 점(P1)에서 상기 곡선의 기울기를 노멀 벡터로 갖는 제1 평면(AP1) 및 상기 제3 점(P3)에서 상기 곡선의 기울기를 노멀 벡터로 갖는 제2 평면(AP2)을 이용할 수 있다. 예를 들어, 상기 앵커 포인트를 판단하는 단계(S400)는 상기 제1 평면(AP1)과 상기 커팅 데이터(Cut Mesh)가 만 나는 점 중 최외곽 점 2개를 제1 앵커 포인트(도 12의 3) 및 제2 앵커 포인트(도 12의 4)로 판단하고, 상기 제2 평면(AP2)과 상기 커팅 데이터(Cut Mesh)가 만나는 점 중 최외곽 점 2개를 제3 앵커 포인트(도 12의 1) 및 제4 앵커 포인트(도 12의 2)로 판단할 수 있다. 도 13은 도 1의 상기 앵커 포인트(3D Anchor Points)를 이용하여 상기 커팅 데이터(Cut Mesh)를 미리 정해진 공 간에 맵핑하는 단계(S500)를 나타내는 도면이다. 도 14는 상기 앵커 포인트(1, 2, 3, 4)가 표시된 상기 커팅 데 이터(Cut Mesh)를 나타내는 도면이다. 도 15는 상기 커팅 데이터(Cut Mesh) 내의 각 점들의 곡률값을 나타내는 도면이다. 도 16은 도 1의 상기 앵커 포인트(3D Anchor Points)를 이용하여 상기 커팅 데이터(Cut Mesh)를 미리 정해진 공간에 맵핑하는 단계(S500)에서 사용되는 상기 미리 정해진 공간을 나타내는 도면이다. 도 17은 상기 미리 정해진 공간에 맵핑된 상기 커팅 데이터(Cut Mesh)의 상기 곡률값을 나타내는 도면이다. 도 1 내지 도 17을 참조하면, 상기 앵커 포인트(3D Anchor Points)를 이용하여 상기 커팅 데이터(Cut Mesh)를 미리 정해진 공간에 맵핑할 수 있으며, 이러한 단계를 메쉬 파라미터라이제이션이라고 할 수 있다. 도 16을 보면, 상기 미리 정해진 공간은 직사각형일 수 있다. 상기 맵핑 데이터를 생성하는 단계(S500)는 상기 제1 앵커 포인트(도 12의 3), 상기 제2 앵커 포인트(도 12의 4), 상기 제3 앵커 포인트(도 12의 1) 및 상기 제4 앵커 포인트(도 12의 2)를 상기 직사각형의 4개의 꼭지점에 각각 대응시켜 상기 맵핑 데이터를 생성할 수 있다. 본 실시예에서, 상기 맵핑 데이터를 생성하는 단계(S500)는 상기 커팅 데이터(Cut Mesh)를 상기 커팅 데이터 (Cut Mesh) 내의 각 점의 곡률값(Curvature)을 나타내는 곡률 데이터로 변환하는 단계를 포함할 수 있다. 도 14는 상기 커팅 데이터(Cut Mesh)를 도시하고, 도 15는 상기 커팅 데이터(Cut Mesh) 내의 각 점의 곡률값을 나타내는 곡률 데이터를 도시한다. 예를 들어, 상기 곡률 데이터는 최대 곡률값, 최소 곡률값, 가우시안 곡률값 및 평균 곡률값을 나타낼 수 있다. 도 15에서는 상기 곡률 데이터는 상기 각 점의 최소 곡률값을 나타낸다. 상기 치아의 상면의 경우 곡률값이 비교적 일정한 값을 가질 수 있다. 반면, 치아와 치아의 경계부에서는 상기 곡률값이 크게 변화할 수 있다. 따라서, 상기 곡률값은 치아와 잇몸이 닿는 부분을 잘 나타내는 값이며, 상기 곡률값을 이용하여 치아의 세그멘테이션을 수행하는 경우 그 정확도가 상대적으로 높을 수 있다. 종래의 곡률 데이터는 상기 곡률값이 클수록 검은색을 갖고, 작을수록 흰색을 가질 수 있다. 본 실시예에서는 상기 각 점의 상기 최소 곡률값이 크면 흰색으로 표현하고, 상기 각 점의 상기 최소 곡률값이 작으면 검은색으로 표현하도록 상기 곡률 데이터의 그레이를 반전하는 단계를 더 수행할 수 있다. 도 15는 상기 곡률 데이터의 그레이가 반전된 도면이며, 따라서, 상기 최소 곡률값이 큰 부분이 흰색으로 표현되었고, 상기 최소 곡률값이 작은 부분이 검은색으로 표현되었다. 예를 들어, 상기 곡률 데이터의 그레이를 반전하지 않는 경 우, 도 15와 반대로, 상기 최소 곡률값이 큰 경우 검은색을 나타내고, 상기 최소 곡률값이 작은 경우 흰색을 나 타내게 된다. 즉, 상기 곡률 데이터가 상기 최소 곡률값을 반전하지 않은 값을 갖는 경우, 치아와 잇몸이 닿는 부분과 같이 유의미한 부분(최소 곡률값이 큰 부분)이 검은색을 나타내게 된다. 그러나, 본 실시예에서는 상기 그레이 반전 으로 인해, 치아와 잇몸이 닿는 부분과 같이 유의미한 부분(최소 곡률값이 큰 부분)이 흰색을 나타내게 된다. 예를 들어, 스캔 데이터에 홀이 있어서 해당 부분에 포인트 자체가 존재하지 않아 곡률값이 존재하지 않는 경우, 종래의 곡률 데이터는 곡률값이 존재하지 않는 부분을 검은색으로 표시하게 된다. 만약, 상기 곡률 데이 터의 그레이 반전을 하지 않는 경우, 유의미한 부분(예컨대, 치아와 잇몸이 닿는 부분)이 검은색으로 나타나게 되는데, 이 경우에 곡률값이 존재하지 않는 부분도 마찬가지로 검은색으로 표시되어, 곡률값이 존재하지 않는 부분이 유의미한 부분으로 오인되는 문제가 있다. 반대로, 상기 곡률 데이터의 그레이 반전이 수행되는 경우, 유의미한 부분(예컨대, 치아와 잇몸이 닿는 부분)이 흰색으로 나타나게 되며, 이 경우에는 곡률값이 존재하지 않는 홀 부분이 유의미한 부분으로 인식하지 않으므로 곡률값이 존재하지 않는 부분으로 인한 오판단을 제거할 수 있다. 도 17은 도 14의 상기 커팅 데이터(Cut Mesh)를 도 15의 상기 곡률 데이터로 변환한 후에, 상기 앵커 포인트를 이용하여 도 16의 미리 정해진 직사각형의 공간(파리미터 공간)에 맵핑한 결과(parameterized image)를 나타낸 다.도 18은 도 1의 제2 인공지능 신경망(AI 2)을 이용하여 세그멘테이션 마스크를 판단하는 단계(S600)를 나타내는 도면이다. 도 19는 도 1의 상기 세그멘테이션 마스크를 상기 스캔 데이터에 맵핑하는 단계(S700)를 나타내는 도 면이다. 도 1 내지 도 19를 참조하면, 상기 맵핑 데이터(parameterized image)를 상기 제2 인공지능 신경망(AI 2)에 통 과시켜 상기 세그멘테이션 마스크를 판단할 수 있다. 예를 들어, 상기 제2 인공지능 신경망(AI 2)은 2차원의 상 기 맵핑 데이터(parameterized image)를 입력 받아 2차원의 상기 세그멘테이션 마스크를 판단할 수 있다. 배경 부분이 0의 레이블을 갖고, 1번 내지 16번 치아 부분이 각각 1 내지 16의 레이블을 갖는다고 할 때, 상기 세그멘테이션 마스크는 상기 맵핑 데이터(parameterized image)의 각 영역에 0 내지 16의 레이블을 표시한 것으 로 이해할 수 있다. 예를 들어, 상기 제2 인공지능 신경망(AI 2)은 컨볼루셔널 신경망(Convolutional neural network, CNN)일 수 있다. 2차원의 상기 세그멘테이션 마스크를 3차원의 상기 스캔 데이터에 맵핑하여 최종적으로 치아 세그멘테이션이 완 료된 스캔 데이터(Segmented Mesh)를 얻을 수 있다. 2차원의 상기 세그멘테이션 마스크를 3차원의 상기 스캔 데 이터(Mesh)에 맵핑하는 경우를 예시하였으나, 2차원의 상기 세그멘테이션 마스크를 3차원의 상기 커팅 데이터 (Cut Mesh)에 맵핑할 수도 있다. 본 실시예에 따르면, 메쉬 파라미터라이제이션 및 딥러닝을 통해 치아 분리가 완전 자동으로 수행되어 스캔 데 이터에서 치아를 분리하기 위한 시간과 노력을 감소시킬 수 있다. 상기 제1 인공지능 신경망(AI 1)을 이용하여 3차원 랜드마크(3D Landmarks)를 자동으로 판단하고, 상기 3차원 랜드마크(3D Landmarks)를 이용하여 스캔 데이터(Mesh)를 커팅하며, 상기 3차원 랜드마크(3D Landmarks) 및 커 팅 데이터(Cut Mesh)를 이용하여 앵커 포인트(3D Anchor Points)를 판단하고, 상기 앵커 포인트(3D Anchor Points)를 이용하여 상기 커팅 데이터(3D Anchor Points)를 미리 정해진 공간에 맵핑할 수 있다. 상기 미리 정 해진 공간에 맵핑된 영상(Parameterized Image)을 상기 제2 인공지능 신경망(AI 2)의 입력으로 활용하여 상기 세그멘테이션 마스크를 판단하여, 치아 자동 분리의 정확도를 더욱 향상시킬 수 있다. 도 20은 본 발명의 일 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법을 나타내는 순서도이다. 도 21 는 도 20의 치아 자동 분리 방법을 나타내는 개념도이다. 본 실시예에 따른 치아 자동 분리 방법은 제1 인공지능 신경망의 입력을 제외하면, 도 1 내지 도 19의 치아 자 동 분리 방법과 실질적으로 동일하므로, 동일 또는 유사한 구성 요소에 대해서는 동일한 참조 번호를 사용하고, 중복되는 설명은 생략한다. 도 20 및 도 21을 참조하면, 본 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법은 3차원의 스캔 데이 터를 입력 받는 제1 인공지능 신경망(AI 1)을 이용하여 3차원 랜드마크를 판단하는 단계(S250), 상기 3차원 랜 드마크를 이용하여 상기 스캔 데이터를 커팅하여 커팅 데이터를 생성하는 단계(S300), 상기 3차원 랜드마크 및 상기 커팅 데이터를 이용하여 앵커 포인트를 판단하는 단계(S400), 상기 앵커 포인트를 이용하여 상기 커팅 데 이터를 미리 정해진 공간에 맵핑하여 맵핑 데이터를 생성하는 단계(S500), 상기 맵핑 데이터를 입력 받는 제2 인공지능 신경망(AI 2)을 이용하여 세그멘테이션 마스크를 판단하는 단계(S600) 및 상기 세그멘테이션 마스크를 상기 스캔 데이터 또는 상기 커팅 데이터에 맵핑하는 단계(S700)를 포함할 수 있다. 본 실시예의 3차원 스캔 데이터의 치아 자동 분리 방법은 컴퓨팅 장치에 의해 수행될 수 있다. 도 1 내지 도 19의 치아 자동 분리 방법에서는 상기 제1 인공지능 신경망(AI 1)의 입력이 2차원 영상인 경우를 예시하였으나, 본 실시예에서 상기 제1 인공지능 신경망(AI 1)은 입력이 3차원 스캔 데이터인 경우를 예시한다. 본 실시예에서, 상기 제1 인공지능 신경망(AI 1)은 상기 3차원 스캔 데이터를 입력 받아 상기 3차원 랜드마크의 좌표를 직접 판단할 수 있다. 본 실시예에서, 상기 제2 인공지능 신경망(AI 2)은 2차원의 상기 맵핑 데이터(Parameterized Image)를 입력 받 아 2차원의 상기 세그멘테이션 마스크를 판단할 수 있다. 본 실시예에 따르면, 메쉬 파라미터라이제이션 및 딥러닝을 통해 치아 분리가 완전 자동으로 수행되어 스캔 데 이터에서 치아를 분리하기 위한 시간과 노력을 감소시킬 수 있다.상기 제1 인공지능 신경망(AI 1)을 이용하여 3차원 랜드마크(3D Landmarks)를 자동으로 판단하고, 상기 3차원 랜드마크(3D Landmarks)를 이용하여 스캔 데이터(Mesh)를 커팅하며, 상기 3차원 랜드마크(3D Landmarks) 및 커 팅 데이터(Cut Mesh)를 이용하여 앵커 포인트(3D Anchor Points)를 판단하고, 상기 앵커 포인트(3D Anchor Points)를 이용하여 상기 커팅 데이터(3D Anchor Points)를 미리 정해진 공간에 맵핑할 수 있다. 상기 미리 정 해진 공간에 맵핑된 영상(Parameterized Image)을 상기 제2 인공지능 신경망(AI 2)의 입력으로 활용하여 상기 세그멘테이션 마스크를 판단하여, 치아 자동 분리의 정확도를 더욱 향상시킬 수 있다. 도 22는 본 발명의 일 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법을 나타내는 순서도이다. 본 실시예에 따른 치아 자동 분리 방법은 3차원 랜드마크 및 스캔 데이터(또는 커팅 데이터)가 주어진 상황에서 치아 자동 분리 방법을 수행하는 경우를 예시한다. 즉, 스캔 데이터를 2차원 영상으로 변환하는 단계(S100), 제 1 인공지능 신경망을 이용하여 3차원 랜드마크를 판단하는 단계(S200) 및 3차원 랜드마크를 이용하여 스캔 데이 터를 커팅(S300)하는 단계를 포함하지 않는 것을 제외하면, 도 1 내지 도 19의 치아 자동 분리 방법과 실질적으 로 동일하므로, 동일 또는 유사한 구성 요소에 대해서는 동일한 참조 번호를 사용하고, 중복되는 설명은 생략한 다. 도 2 및 도 22를 참조하면, 본 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법은 3차원의 스캔 데이터 (Mesh) 및 상기 스캔 데이터(Mesh)의 3차원 랜드마크(3D Landmarks)를 이용하여 앵커 포인트(3D Anchor Points)를 판단하는 단계(S450), 상기 앵커 포인트(3D Anchor Points)를 이용하여 상기 스캔 데이터(Mesh)를 미리 정해진 공간에 맵핑하여 맵핑 데이터(Parameterized Mesh)를 생성하는 단계(S500), 상기 맵핑 데이터 (Parameterized Mesh)를 입력 받는 인공지능 신경망(AI 2)을 이용하여 세그멘테이션 마스크를 판단하는 단계 (S600) 및 상기 세그멘테이션 마스크를 상기 스캔 데이터(Mesh)에 맵핑하는 단계(S700)를 포함할 수 있다. 본 실시예의 3차원 스캔 데이터의 치아 자동 분리 방법은 컴퓨팅 장치에 의해 수행될 수 있다. 도 22의 치아 자동 분리 방법에서는 제1 인공지능 신경망(AI 1)을 이용하여 3차원 랜드마크를 판단하는 단계를 생략하였고, 스캔 데이터를 커팅하여 커팅 데이터를 생성하는 단계도 생략하였다. 도 22의 치아 자동 분리 방법은 앵커 포인트를 판단하는 단계(S450), 상기 앵커 포인트를 이용하여 메쉬 파라미 터라이제이션을 수행하는 단계(S500), 상기 맵핑 데이터(Parameterized Mesh)를 입력 받아 인공지능 신경망(AI 2)을 이용하여 세그멘테이션 마스크를 판단하는 단계(S600)를 중점적으로 포함한다. 본 실시예에 따르면, 메쉬 파라미터라이제이션 및 딥러닝을 통해 치아 분리가 완전 자동으로 수행되어 스캔 데 이터에서 치아를 분리하기 위한 시간과 노력을 감소시킬 수 있다. 상기 3차원 랜드마크(3D Landmarks) 및 스캔 데이터(Mesh)를 이용하여 앵커 포인트(3D Anchor Points)를 판단 하고, 상기 앵커 포인트(3D Anchor Points)를 이용하여 상기 스캔 데이터(3D Anchor Points)를 미리 정해진 공 간에 맵핑할 수 있다. 상기 미리 정해진 공간에 맵핑된 영상(Parameterized Image)을 상기 인공지능 신경망(AI 2)의 입력으로 활용하여 상기 세그멘테이션 마스크를 판단하여, 치아 자동 분리의 정확도를 더욱 향상시킬 수 있다. 본 발명의 일 실시예에 의하면, 상기 실시예들에 따른 3차원 스캔 데이터의 치아 자동 분리 방법을 컴퓨터에서 실행시키기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록 매체가 제공될 수 있다. 전술한 방법은 컴퓨터 에서 실행될 수 있는 프로그램으로 작성 가능하고, 컴퓨터 판독 가능 매체를 이용하여 상기 프로그램을 동작시 키는 범용 디지털 컴퓨터에서 구현될 수 있다. 또한, 전술한 방법에서 사용된 데이터의 구조는 컴퓨터 판독 가 능 매체에 여러 수단을 통하여 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데 이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 발명을 위 하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 분야의 통상의 기술자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자 기 매체, CD-ROM, DVD와 같은 광기록 매체, 플롭티컬 디스크와 같은 자기-광 매체 및 롬, 램, 플래시 메모리 등 과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에 는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이 상의 소프트웨어 모듈로서 작동하도록 구성될 수 있다. 또한, 전술한 3차원 스캔 데이터의 치아 자동 분리 방법은 기록 매체에 저장되는 컴퓨터에 의해 실행되는 컴퓨 터 프로그램 또는 애플리케이션의 형태로도 구현될 수 있다. 산업상 이용가능성 본 발명은 3차원 스캔 데이터의 치아 자동 분리 방법 및 이를 컴퓨터에서 실행시키기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록 매체에 대한 것으로, 치아를 분리하기 위한 시간과 노력을 감소시키고 정확도를 향 상시킬 수 있다."}
{"patent_id": "10-2023-0031871", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "상기에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술분야의 숙련된 당업자는 하기의 특 허청구범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 것이다."}
{"patent_id": "10-2023-0031871", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법을 나타내는 순서도이다. 도 2는 도 1의 치아 자동 분리 방법을 나타내는 개념도이다. 도 3은 도 1의 스캔 데이터를 2차원 영상으로 변환하는 단계를 나타내는 도면이다. 도 4는 도 1의 제1 인공지능 신경망을 이용하여 3차원 랜드마크를 판단하는 단계를 나타내는 도면이다. 도 5는 도 1의 상기 3차원 랜드마크를 이용하여 상기 스캔 데이터를 커팅하는 단계를 나타내는 도면이다.도 6은 도 1의 상기 3차원 랜드마크를 이용하여 상기 스캔 데이터를 커팅하는 단계에서 사용되는 벡터들을 나타 내는 도면이다. 도 7 및 도 8은 도 1의 상기 3차원 랜드마크를 이용하여 상기 스캔 데이터를 커팅하는 단계에서 사용되는 커팅 평면들을 나타내는 도면이다. 도 9는 도 1의 상기 3차원 랜드마크 및 커팅 데이터를 이용하여 앵커 포인트를 판단하는 단계를 나타내는 도면 이다. 도 10은 도 1의 상기 3차원 랜드마크 및 상기 커팅 데이터를 이용하여 앵커 포인트를 판단하는 단계에서 사용되 는 스플라인 커브를 나타내는 도면이다. 도 11은 도 1의 상기 3차원 랜드마크 및 상기 커팅 데이터를 이용하여 앵커 포인트를 판단하는 단계에서 사용되 는 평면들을 나타내는 도면이다. 도 12는 도 1의 상기 3차원 랜드마크 및 상기 커팅 데이터를 이용하여 앵커 포인트를 판단하는 단계에서 판단된 앵커 포인트들을 나타내는 도면이다. 도 13은 도 1의 상기 앵커 포인트를 이용하여 상기 커팅 데이터를 미리 정해진 공간에 맵핑하는 단계를 나타내 는 도면이다. 도 14는 상기 앵커 포인트가 표시된 상기 커팅 데이터를 나타내는 도면이다. 도 15는 상기 커팅 데이터 내의 각 점들의 곡률값을 나타내는 도면이다. 도 16은 도 1의 상기 앵커 포인트를 이용하여 상기 커팅 데이터를 미리 정해진 공간에 맵핑하는 단계에서 사용 되는 상기 미리 정해진 공간을 나타내는 도면이다. 도 17은 상기 미리 정해진 공간에 맵핑된 상기 커팅 데이터의 상기 곡률값을 나타내는 도면이다. 도 18은 도 1의 제2 인공지능 신경망을 이용하여 세그멘테이션 마스크를 판단하는 단계를 나타내는 도면이다. 도 19는 도 1의 상기 세그멘테이션 마스크를 상기 스캔 데이터에 맵핑하는 단계를 나타내는 도면이다. 도 20은 본 발명의 일 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법을 나타내는 순서도이다. 도 21는 도 20의 치아 자동 분리 방법을 나타내는 개념도이다. 도 22는 본 발명의 일 실시예에 따른 3차원 스캔 데이터의 치아 자동 분리 방법을 나타내는 순서도이다."}
