{"patent_id": "10-2021-0154639", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0064336", "출원번호": "10-2021-0154639", "발명의 명칭": "클러스터 연결 신경망", "출원인": "딥큐브 엘티디.", "발명자": "데이비드 엘리"}}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "클러스터 연결 신경망(cluster-connected neural network)을 사용한 훈련 또는 예측을 위한 방법으로서,복수의 클러스터들로 분할된 신경망을 저장하는 단계 ― 각각의 클러스터는 상이한 복수의 인공 뉴런들 또는 컨볼루션(convolutional) 채널들을 포함하며, 뉴런들 또는 채널들의 각각의 쌍은 가중치 또는 컨볼루션 필터에 의해 고유하게 연결됨 ―;상기 클러스터 연결 신경망의 각각의 클러스터 내에서, 클러스터 내(intra-cluster) 가중치들 또는 필터들의 국소적으로 조밀한 하위 네트워크를 생성 또는 유지하는 단계 ― 동일한 클러스터 내의 뉴런들 또는 채널들의 대부분의 쌍들이 클러스터 내 가중치들 또는 필터들에 의해 연결되어, 상기 클러스터 연결 신경망을 사용한 훈련또는 예측 동안 각각의 클러스터 내의 뉴런들 또는 채널들의 연결된 대부분의 쌍들이 활성화 블록으로서 함께공동 활성화됨 ―;상기 클러스터 연결 신경망의 각각의 클러스터 외부에서, 클러스터 간(inter-cluster) 가중치들 또는 필터들의전역적 희소 네트워크를 생성 또는 유지하는 단계 ― 상이한 클러스터들에 걸쳐 클러스터 경계에 의해 분리된뉴런들 또는 채널들의 소수의 쌍들이 클러스터 간 가중치들 또는 필터들에 의해 연결됨 ―; 및상기 클러스터 연결 신경망을 사용하여 훈련 또는 예측하는 단계를 포함하는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,테스트로부터 야기되는 고도로 링크된 뉴런들 또는 채널들의 활성화 패턴들과 가장 근접하게 닮은 최적의 클러스터 형상을 결정하기 위해 상기 클러스터 연결 신경망에서 뉴런 또는 채널 활성화 패턴들을 테스트하는 단계를포함하는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,훈련 동안 활성화 패턴들이 변함에 따라 상기 최적의 클러스터 형상을 동적으로 조정하는 단계를 포함하는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 복수의 클러스터들 중 하나 이상의 클러스터의 클러스터 경계는: 열, 행, 원형, 다각형, 불규칙한 형상,직사각형 프리즘, 원통, 구, 다면체, 및 임의의 2차원, 3차원 또는 N차원 형상으로 구성된 그룹으로부터 선택된형상을 갖는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,단절된 클러스터들로 신경망을 초기화하고 소수의 가능한 클러스터 간 가중치들 또는 필터들을 추가함으로써 상기 클러스터 연결 신경망을 훈련시키는 단계를 포함하는,공개특허 10-2022-0064336-2-클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,완전 연결 신경망(fully-connected neural network)을 초기화하고 상기 클러스터 간 가중치들 또는 필터들의 대부분을 프루닝(prune)함으로써 상기 클러스터 연결 신경망을 훈련시키는 단계를 포함하는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 프루닝은, 클러스터 내 가중치들에 유리하게 편향시키고, 클러스터 간 가중치들에 대해 편향시킴으로써 훈련 단계 동안 수행되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 프루닝은: L1 정규화, Lp 정규화, 임계화, 랜덤 제로화 및 편향 기반 프루닝으로 구성된 그룹으로부터 선택된 하나 이상의 기술들을 사용하여 수행되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 클러스터 연결 신경망은, 상기 클러스터 연결 신경망의 가중치들 또는 필터들의 강도가 상기 필터들의 가중치들에 의해 연결된 뉴런들 또는 채널들 사이의 거리에 반비례하여 편향되도록 훈련되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,진화 알고리즘 또는 강화 학습을 사용하여 상기 클러스터 연결 신경망을 훈련시키는 단계를 포함하는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,하나의 클러스터 내의 경계 뉴런들 또는 채널들은 클러스터 간 가중치들 또는 필터들에 의해 하나 이상의 상이한 클러스터들 내의 경계 뉴런들 또는 채널들에 연결되는 반면, 보딩(board)된 상기 클러스터로부터 이격된 내부 뉴런들 또는 채널들은 단지 클러스터 내 가중치들 또는 필터들에 의해 동일한 클러스터 내의 다른 뉴런들 또는 채널들에 연결되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항에 있어서,각각의 클러스터 내의 뉴런들 또는 채널들은 완전히 연결되거나 부분적으로 연결되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법.공개특허 10-2022-0064336-3-청구항 13 제1항에 있어서,상기 클러스터 연결 신경망은 클러스터 연결 구역들과 표준 비-클러스터 연결 구역들의 하이브리드인,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1항에 있어서,고유 클러스터 인덱스에 대한 연관성을 갖는 상기 클러스터 연결 신경망의 각각의 채널에 클러스터 내 가중치들또는 필터들을 저장하는 단계, 및 상기 클러스터 내의 상기 클러스터 내 가중치들을 상기 가중치들의 행렬 포지션들로 표현하는 클러스터 특정 행렬을 사용하는 단계를 포함하는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1항에 있어서,고유 인덱스에 대한 연관성을 갖는 상기 클러스터 연결 신경망의 복수의 클러스터 간 가중치들 또는 필터들 각각을 저장하는 단계를 포함하고,상기 고유 인덱스는 상기 클러스터 간 가중치 또는 필터에 의해 표현되는 연결을 갖는 인공 뉴런들 또는 채널들의 쌍을 고유하게 식별하며,상이한 클러스터들 내의 뉴런들 또는 채널들의 쌍들 사이의 연결들을 표현하는 비-제로 클러스터 간 가중치들또는 필터들만이 저장되고, 뉴런들 또는 채널들의 쌍들 사이의 연결들을 표현하지 않는 제로 클러스터 간 가중치들 또는 필터들은 저장되지 않는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,각각의 클러스터 간 가중치 또는 필터를 식별하는 값들의 트리플릿(triplet)을 저장하는 단계를 포함하며,상기 값들은:제1 클러스터 내의 상기 쌍의 제1 뉴런 또는 채널을 식별하는 상기 인덱스의 제1 값,상이한 제2 클러스터 내의 상기 쌍의 제2 뉴런 또는 채널을 식별하는 상기 인덱스의 제2 값, 및상기 클러스터 간 가중치 또는 필터의 값을 포함하는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제15항에 있어서,상기 클러스터 연결 신경망에서 비-제로 클러스터 간 가중치들 또는 필터들의 희소 분포와 연관된 인덱스들의비-순차적 패턴에 따라 메인 메모리의 비-순차적 위치들에 저장되는 클러스터 간 가중치들 또는 필터들을 상기메인 메모리로부터 페치(fetch)하는 단계; 및상기 메인 메모리 내의 비-순차적 위치들로부터 페치된 클러스터 간 가중치들 또는 필터들을 캐시 메모리 내의순차적인 위치들에 저장하는 단계를 포함하는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "공개특허 10-2022-0064336-4-제15항에 있어서,압축된 희소 행(CSR: compressed sparse row) 표현, 압축된 희소 열(CSC: compressed sparse column) 표현, 희소 텐서(tensor) 표현, 맵 표현, 리스트 표현 및 희소 벡터 표현으로 구성된 그룹으로부터 선택된 하나 이상의데이터 표현들을 사용하여 상기 클러스터 연결 신경망의 클러스터 간 가중치들 또는 필터들의 값들을 저장하는단계를 포함하는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 방법."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템으로서,복수의 클러스터들로 분할된 신경망을 저장하도록 구성된 하나 이상의 메모리들 ― 각각의 클러스터는 상이한복수의 인공 뉴런들 또는 컨볼루션 채널들을 포함하며, 뉴런들 또는 채널들의 각각의 쌍은 가중치 또는 컨볼루션 필터에 의해 고유하게 연결됨 ―; 및하나 이상의 프로세서들을 포함하며,상기 하나 이상의 프로세서들은:상기 클러스터 연결 신경망의 각각의 클러스터 내에서, 클러스터 내 가중치들 또는 필터들의 국소적으로 조밀한하위 네트워크를 생성 또는 유지하고 ― 동일한 클러스터 내의 뉴런들 또는 채널들의 대부분의 쌍들이 클러스터내 가중치들 또는 필터들에 의해 연결되어, 상기 클러스터 연결 신경망을 사용한 훈련 또는 예측 동안 각각의클러스터 내의 뉴런들 또는 채널들의 연결된 대부분의 쌍들이 활성화 블록으로서 함께 공동 활성화됨 ―,상기 클러스터 연결 신경망의 각각의 클러스터 외부에서, 클러스터 간 가중치들 또는 필터들의 전역적 희소 네트워크를 생성 또는 유지하고 ― 상이한 클러스터들에 걸쳐 클러스터 경계에 의해 분리된 뉴런들 또는 채널들의소수의 쌍들이 클러스터 간 가중치들 또는 필터들에 의해 연결됨 ―, 그리고상기 클러스터 연결 신경망을 사용하여 훈련 또는 예측하도록 구성되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서,상기 하나 이상의 프로세서들은, 테스트로부터 야기되는 고도로 링크된 뉴런들 또는 채널들의 활성화 패턴들과가장 근접하게 닮은 최적의 클러스터 형상을 결정하기 위해 상기 클러스터 연결 신경망에서 뉴런 또는 채널 활성화 패턴들을 테스트하도록 구성되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제20항에 있어서,상기 하나 이상의 프로세서들은, 훈련 동안 활성화 패턴들이 변함에 따라 상기 최적의 클러스터 형상을 동적으로 조정하도록 구성되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제19항에 있어서,상기 복수의 클러스터들 중 하나 이상의 클러스터의 클러스터 경계는: 열, 행, 원형, 다각형, 불규칙한 형상,직사각형 프리즘, 원통, 구, 다면체, 및 임의의 2차원, 3차원 또는 N차원 형상으로 구성된 그룹으로부터 선택된형상을 갖는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템.공개특허 10-2022-0064336-5-청구항 23 제19항에 있어서,상기 하나 이상의 프로세서들은, 단절된 클러스터들로 신경망을 초기화하고 소수의 가능한 클러스터 간 가중치들 또는 필터들을 추가함으로써 상기 클러스터 연결 신경망을 훈련시키도록 구성되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제19항에 있어서,상기 하나 이상의 프로세서들은, 완전 연결 신경망을 초기화하고 상기 클러스터 간 가중치들 또는 필터들의 대부분을 프루닝함으로써 상기 클러스터 연결 신경망을 훈련시키도록 구성되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제19항에 있어서,하나의 클러스터 내의 경계 뉴런들 또는 채널들은 클러스터 간 가중치들 또는 필터들에 의해 하나 이상의 상이한 클러스터들 내의 경계 뉴런들 또는 채널들에 연결되는 반면, 보딩된 상기 클러스터로부터 이격된 내부 뉴런들 또는 채널들은 단지 클러스터 내 가중치들 또는 필터들에 의해 동일한 클러스터 내의 다른 뉴런들 또는 채널들에 연결되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제19항에 있어서,상기 하나 이상의 메모리들은, 고유 클러스터 인덱스에 대한 연관성을 갖는 상기 클러스터 연결 신경망의 각각의 채널에 클러스터 내 가중치들 또는 필터들을 저장하고, 그리고 상기 클러스터 내의 상기 클러스터 내 가중치들을 상기 가중치들의 행렬 포지션들로 표현하는 클러스터 특정 행렬을 사용하도록 구성되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제19항에 있어서,상기 하나 이상의 메모리들은, 고유 인덱스에 대한 연관성을 갖는 상기 클러스터 연결 신경망의 복수의 클러스터 간 가중치들 또는 필터들 각각을 저장하도록 구성되고,상기 고유 인덱스는 상기 클러스터 간 가중치 또는 필터에 의해 표현되는 연결을 갖는 인공 뉴런들 또는 채널들의 쌍을 고유하게 식별하며,상이한 클러스터들 내의 뉴런들 또는 채널들의 쌍들 사이의 연결들을 표현하는 비-제로 클러스터 간 가중치들또는 필터들만이 저장되고, 뉴런들 또는 채널들의 쌍들 사이의 연결들을 표현하지 않는 제로 클러스터 간 가중치들 또는 필터들은 저장되지 않는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "제19항에 있어서,상기 하나 이상의 메모리들은, 각각의 클러스터 간 가중치 또는 필터를 식별하는 값들의 트리플릿을 저장하도록구성되며,상기 값들은:공개특허 10-2022-0064336-6-제1 클러스터 내의 상기 쌍의 제1 뉴런 또는 채널을 식별하는 상기 인덱스의 제1 값,상이한 제2 클러스터 내의 상기 쌍의 제2 뉴런 또는 채널을 식별하는 상기 인덱스의 제2 값, 및상기 클러스터 간 가중치 또는 필터의 값을 포함하는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_29", "content": "제19항에 있어서,상기 하나 이상의 프로세서들은:상기 클러스터 연결 신경망에서 비-제로 클러스터 간 가중치들 또는 필터들의 희소 분포와 연관된 인덱스들의비-순차적 패턴에 따라 메인 메모리의 비-순차적 위치들에 저장되는 클러스터 간 가중치들 또는 필터들을 상기메인 메모리로부터 페치하고; 그리고상기 메인 메모리 내의 비-순차적 위치들로부터 페치된 클러스터 간 가중치들 또는 필터들을 캐시 메모리 내의순차적인 위치들에 저장하도록 구성되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_30", "content": "제19항에 있어서,상기 하나 이상의 메모리들은, 압축된 희소 행(CSR) 표현, 압축된 희소 열(CSC) 표현, 희소 텐서 표현, 맵표현, 리스트 표현 및 희소 벡터 표현으로 구성된 그룹으로부터 선택된 하나 이상의 데이터 표현들을 사용하여상기 클러스터 연결 신경망의 클러스터 간 가중치들 또는 필터들의 값들을 저장하도록 구성되는,클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 시스템."}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "클러스터 연결 신경망을 사용한 훈련 또는 예측을 위한 디바이스, 시스템 및 방법이 제공된다. 클러스터 연결 신 경망은 컨볼루션 필터들에 의해 연결된 컨볼루션 채널들 또는 가중치들에 의해 연결된 인공 뉴런들의 복수의 클 러스터들로 분할될 수 있다. 각각의 클러스터 내에는, 훈련 또는 예측 동안 활성화 블록으로서 함께 공동 활성화 되는 클러스터 내 가중치들 또는 필터들에 의해 연결된 뉴런들 또는 채널들의 대부분의 쌍들을 갖는 클러스터 내 가중치들 또는 필터들의 국소적으로 조밀한 하위 네트워크가 있다. 각각의 클러스터 외부에는 클러스터 간 가중 치들 또는 필터들에 의해 연결된 상이한 클러스터들에 걸쳐 클러스터 경계에 의해 분리된 뉴런들 또는 채널들의 소수의 쌍들을 갖는 클러스터 간 가중치들 또는 필터들의 전역적 희소 네트워크가 있다. 훈련 또는 예측은 클러 스터 연결 신경망을 사용하여 수행된다."}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명의 실시예들은 기계 학습에 의한 인공 지능(AI: artificial intelligence) 분야에 관한 것이다. 특히, 본 발명의 실시예들은 신경망(neural network)들을 사용하는 딥 러닝에 관한 것이다."}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 신경망, 또는 단순히 \"신경망\"은 기계 학습에 의해 훈련되며 뉴런들의 생물학적 망을 닮은 컴퓨터 모델이 다. 종래의 신경망은 입력 계층, 다수의 중간 또는 은닉 계층(들) 및 출력 계층을 갖는다. 각각의 계층은 복수 (예컨대, 수백 내지 수천 개)의 인공 \"뉴런들\"을 갖는다. 계층(N)의 각각의 뉴런은 인공 \"시냅스(synapse)\"에 의해 이전(N-1) 계층 및 후속(N+1) 계층의 일부 또는 모든 뉴런들에 연결되어 \"부분적으로 연결된\" 또는 \"완전 히 연결된\" 신경망을 형성한다. 각각의 시냅스 연결의 강도는 가중치로 표현된다. 따라서 신경망은 망 내의 모 든 가중치들의 세트로 표현될 수 있다. 신경망(NN: neural network)은 해당 연결의 강도를 지시하는 각각의 시냅스의 가중치를 해결하거나 학습하기 위 한 학습 데이터 세트에 기초하여 훈련된다. 시냅스들의 가중치들은 일반적으로 예컨대, 랜덤하게 초기화된다. 반복적으로 신경망에 샘플 데이터 세트를 입력하고, 데이터 세트에 적용된 신경망의 결과를 출력하고, 예상(예 컨대, 타깃) 출력과 실제 출력 간의 오류들을 산출하고, 오류들을 최소화하도록 오류 정정 알고리즘(예컨대, 역 전파(backpropagation))을 사용하여 신경망 가중치들을 조정함으로써 훈련이 수행된다. 오류가 최소화되거나 수 렴할 때까지 훈련이 반복될 수 있다. 통상적으로, 훈련 세트를 통한 다수의 패스(pass)들(예컨대, 수십 또는 수 백)이 수행된다(예컨대, 각각의 샘플이 신경망에 여러 번 입력됨). 전체 훈련 세트에 대한 각각의 완전한 패스 는 하나의 \"에포크(epoch)\"로 지칭된다. 최첨단 신경망들은 통상적으로 수백만 내지 수십억 개의 가중치들을 가지며, 그 결과로 훈련 및 런타임(예측) 단계들 모두를 위한 전문 하드웨어(대개, GPU)를 필요로 한다. 이로써, 예측 모드에서도 대부분의 엔드포인트 디바이스들(예컨대, IoT 디바이스들, 모바일 디바이스들 또는 심지어, 전용 가속기 하드웨어가 없는 랩톱들 및데스크톱들) 상에서 딥 러닝 모델들을 진행(run)하는 것은 비현실적이다. 제한된 처리 속도 및/또는 제한된 메 모리 가용성을 갖는 디바이스들 상에서 딥 러닝 모델들을 효과적으로 진행하는 것은 오늘날 중요한 과제로 남아 있다. 제한된 하드웨어 용량의 문제를 해결하기 위해, 현재 대부분의 딥 러닝 예측은 원격 서버 또는 클라우드 상에서 수행된다. 예를 들어, 스마트 어시스턴트(예컨대, Alexa)가 정보(예컨대, 음성 신호)를 클라우드로 송신하고, 딥 러닝 예측이 전용 하드웨어 상의 클라우드에서 원격으로 수행되며, 응답이 다시 로컬 디바이스로 송신된다. 그러므로 이러한 엔드포인트 디바이스들은 이들이 클라우드로부터 연결이 끊어진다면, 입력 레이트가 너무 높아 클라우드와 지속적으로 통신하는 것이 실현 불가능하다면, 또는 전용 하드웨어조차 오늘날 충분히 빠르지 않은 경우에 매우 빠른 예측이 요구된다면(예컨대, 고빈도 거래를 위한 딥 러닝), 딥 러닝 기반 결과들을 제공할 수 없다. 이에 따라, 훈련 및/또는 예측 모드들에서 신경망에 대한 딥 러닝의 메모리 요건들을 감소시키고 효율을 높이는"}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "것이 당해 기술분야에 필요하다. 본 발명의 일부 실시예들에 따르면, 클러스터들 내에서는 국소적으로 완전히 또는 조밀하게 연결되고(예컨대, 로컬 클러스터 내에서 클러스터 내(intra-cluster) 가중치들을 조장(encourage)하고) 클러스터들의 외부에서는 전역적으로 희소하게 연결되는(예컨대, 클러스터 경계들에 걸쳐 클러스터 간(inter-cluster) 가중치들을 제거하 는) \"클러스터 연결(cluster-connected)\" 신경망을 사용하여 훈련 및 예측하기 위한 디바이스, 시스템 및 방법 이 제공된다. 클러스터들은 열(column)들로서 형상화되어, 입력으로부터 (예컨대, 신경망 축에 평행한) 출력 계 층을 향해 연장되는 통상적으로 우세한 뉴런 활성화 방향을 조장하고, (예컨대, 신경망 축에 직교하는) 통상적 으로 덜 우세한 측 방향 뉴런 활성화를 막을 수 있다. 일례로, 1000개의 뉴런들의 2개의 계층들을 각각 포함하 는 신경망에서, 계층들은 완전 연결 설계에서는 백만 개의 가중치들(1000 × 1, 000)로 연결되지만, 열 연결 설 계에서는 10개의 열들로 분할될 때 십만 개(100 × 100 × 10)와 몇몇 나머지 희소 열 간(inter-column) 가중치 들에 의해서만 연결된다. 이 열 연결 신경망은 이로써, 실질적으로 동일한 정확도를 갖는 완전 연결 신경망과 비교하여 런타임 예측 동안 계산 속도의 대략 10배 증가, 및 새로운 희소 인덱싱을 이용한 클러스터 간 가중치 들의 수의 1/10을 저장하기 위한 메모리의 대략 10배 감소를 제공한다. 이러한 열 연결 신경망은 또한, 실질적 으로 동일한 정확도를 갖는 완전 연결 신경망과 비교하여 훈련의 속도를 높인다. 예를 들어, 신경망이 클러스터 연결 신경망으로서 초기화될 때, 훈련 속도의 증가는 런타임 가속과 동일한 정도(예컨대, 위의 시나리오에서는 10배)로 최대화된다. 다른 예에서는, 신경망이 완전 연결 신경망으로서 초기화될 때, 점점 더 많은 시냅스 (synapse)들이 제거되거나 프루닝(prune)됨에 따라, 열 연결 신경망이 형성되고 완전한 훈련 가속(예컨대, 위의 시나리오에서는 10배)이 달성될 때까지 각각의 순차적 훈련 반복에서 훈련 속도가 높아진다. 본 발명의 일부 실시예들에 따르면, 클러스터 연결 신경망을 사용하여 훈련 또는 예측하기 위한 디바이스, 시스 템 및 방법이 제공된다. 클러스터 연결 신경망이 복수의 클러스터들로 분할될 수 있다. 각각의 클러스터는 상이 한 복수의 인공 뉴런들 또는 컨볼루션 채널들을 포함할 수 있으며, 여기서 뉴런들 또는 채널들의 각각의 쌍은 가중치 또는 컨볼루션 필터에 의해 고유하게 연결된다. 클러스터 연결 신경망의 각각의 클러스터 내에서, 클러 스터 내 가중치들 또는 필터들의 국소적으로 조밀한 하위 네트워크가 생성 또는 유지될 수 있으며, 여기서 동일 한 클러스터 내의 뉴런들 또는 채널들의 대부분의 쌍들이 클러스터 내 가중치들 또는 필터들에 의해 연결되어, 클러스터 연결 신경망을 사용한 훈련 또는 예측 동안 각각의 클러스터 내의 뉴런들 또는 채널들의 연결된 대부 분의 쌍들이 활성화 블록으로서 함께 공동 활성화된다. 클러스터 연결 신경망의 각각의 클러스터 외부에서, 클 러스터 간 가중치들 또는 필터들의 전역적 희소 네트워크가 생성 또는 유지될 수 있으며, 여기서 상이한 클러스 터들에 걸쳐 클러스터 경계에 의해 분리된 뉴런들 또는 채널들의 소수의 쌍들이 클러스터 간 가중치들 또는 필 터들에 의해 연결된다. 클러스터 연결 신경망은 훈련 및/또는 예측을 위해 실행될 수 있다."}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "개별 뉴런의 활성화는 뉴런 주변 이웃 뉴런들의 활성화 패턴들에 의존한다. 비교적 더 높은 가중치들을 갖는 뉴 런들의 클러스터에 연결되는 뉴런은, 비교적 더 낮은 가중치들을 갖는 클러스터에 연결된 경우보다 활성화될 가 능성이 더 높다. 따라서 뉴런들은 클러스터들에서 활성화된다. 본 발명의 실시예들에 따른 클러스터 기반 신경 망은, 활성화가 지배적인 각각의 클러스터 내부의 가중치들(클러스터 내 가중치들)을 강화하고, 흔히 사소한 활 성화만을 나타내는 클러스터들 외부의 가중치들(클러스터 간 가중치들)을 감소시키거나 제거한다. 지배적인 클 러스터 내 가중치들을 조장하고 약한 클러스터 간 가중치들을 제거함으로써, 본 발명의 실시예들은 클러스터 연 결 신경망을 형성하며, 여기서 뉴런들은 각각의 클러스터 내에서 조밀하게 연결되고(국소적으로 조밀함) 상이한 클러스터들에 걸쳐 희소하게 연결된다(전역적으로 희소함). 클러스터 연결 신경망은, 실질적으로 더 적은 프로 세싱 노력 및 시간으로 실질적으로 동일한 결과를 달성하기 위해, 가장 영향력 있는 클러스터 내 뉴런 활성화들 에 계산 노력을 집중시키고, 덜 결과적인 클러스터 간 뉴런 활성화들에 대한 계산 노력을 제거하거나 감소시킴 으로써, 종래의 신경망과 비교하여 효율을 개선한다. 뉴런들은 통상적으로, 신경망 축과 동일한 방향으로(예컨대, 계층 평면들에 직교하는, 입력 계층으로부터 출력 계층으로 연장되는 도 1의 배향 신경망 축에서) 우세하게 활성화된다. 신경망이 수직으로 배향되는, 도 1 에 도시된 예에서, 뉴런 활성화는 수직 클러스터들에서 우세하여, 열 형상 클러스터들을 형성한다. 도 1에 서는 열 형상 클러스터들이 사용되지만, 예컨대 고도로 연결된 뉴런 활성화 구역들의 경계들인 다른 클러스터 형상들이 사용될 수 있다. 예를 들어, 뉴런 클러스터들의 경계들의 형상들은 원들, 열들, 행들, 다각형들, 불규 칙한 형상들, 및/또는 임의의 2D 또는 3D 형상들을 포함할 수 있다. 클러스터들은 정렬되거나 오정렬될 수 있고 (예컨대, 스태거링(stagger)된 열들), 다양한 크기들(예컨대, 4x2 뉴런 열들, 3x6 뉴런 행들 등), 다양한 배향 들, 중첩 또는 비중첩 등일 수 있다. 추가로 또는 대안으로, 일부 클러스터들은 (예컨대, 근접도 대신에 또는 근접도에 추가로 가중치 값에 기초하여 선택된) 인접하지 않은 또는 이웃하지 않는 뉴런들을 표현할 수 있다. 일례로, 도 1의 신경망 배향이 수평 배향으로 회전되었다면, 행 형상 클러스터들은 열 형상 클러스터들과 동등할 수 있다. 추가로 또는 대안으로, 도 1의 도시된 배향에서, 행 클러스터들은 뉴런들이 동일한 계층의 다 른 뉴런들에 연결되는 순환 신경망들에 사용될 수 있다. 일부 실시예들에서, 예를 들어, 순환 또는 층 내 연결 들이 우세한 영역들에서 행 클러스터들을 사용하고 그리고/또는 계층 간 연결들이 우세한 영역들에서 열 클러스 터들을 사용하는 열 및 행 클러스터들의 조합이 사용될 수 있다. 추가로 또는 대안으로, 3D 클러스터들은 3D 신 경망(예컨대, 필터들의 다차원 채널들을 갖는 컨볼루션 신경망(CNN: convolutional neural network))에 대해 사용될 수 있다. 일부 실시예들에서, 클러스터 형상들의 최적의 패턴을 결정하기 위해 테스트들이 수행될 수 있다. 예를 들어, (예컨대, 뉴런들의 로컬화된 또는 랜덤 또는 준-랜덤 샘플링의 서브세트를 모두 테스트하는) 가장 높은 집합적 그룹 가중치를 갖는 그룹 뉴런들, 다수의 훈련 반복들 또는 에포크들에 걸쳐 가장 탄력 있는(예컨대, 가장 느리 게 변화하는) 가중치들을 갖는 뉴런들, 또는 뉴런 쌍 또는 그룹 가중치들의 임의의 다른 측정치를 그룹화하는 클러스터 형상들이 정의될 수 있다. 테스트 분석이 주기적으로, 각각의 에포크에 대해, 임의의 다른 규칙적인 또는 불규칙적인 시간 간격들로 한번 수행될 수 있고, 그리고/또는 임의의 다른 이벤트 또는 기준(예컨대, 임계 치를 넘는 가중치들)에 의해 트리거될 수 있다. 훈련 계산들과 독립적으로 또는 그 일부로서 테스트 통계가 계 산될 수 있다. 일부 실시예들에서, 훈련 중에 뉴런 가중치들이 변함에 따라, 클러스터들의 패턴 및 형상들은 최적의 클러스터링을 유지하도록 동적으로 조정될 수 있다. 클러스터 연결 신경망들을 훈련시키기 위해, 본 발명의 일부 실시예들은 완전 연결 신경망으로 시작하여 클러스 터 간 가중치들을 프루닝할 수 있다. 본 발명의 다른 실시예들은 단절된 클러스터들로 시작하여 선택된 클러스 터 간 가중치들을 추가할 수 있다. 가중치 훈련은 (둘 다 동일한 클러스터 내에 위치된 뉴런들을 연결하는) 클러스터 내 가중치들을 강화 또는 추 가하고 (상이한 클러스터들에 걸쳐 위치된 뉴런들을 연결하기 위해 클러스터 경계를 넘는) 클러스터 간 가중치 들을 약화시키거나 프루닝하는 것에 유리하게 편향될 수 있다. 클러스터 간 가중치들은 L1 정규화 (regularization), Lp 정규화, 임계화, 랜덤 제로화(random zero-ing), 새로운 가중치 생성, 유전 알고리즘들을 사용한 가중치들의 진화, 및/또는 편향 기반 프루닝을 사용하여 감소되거나 프루닝될 수 있다. 일부 실시예들에 서, 가중치 강도는 뉴런들 사이의 거리에 반비례하여 편향될 수 있다. 예를 들어, 가중치들은, 연결된 뉴런들이 서로 더 가까울수록 더 강하고, 연결된 뉴런들이 서로 더 멀리 위치될수록 더 약하게 편향될 수 있다. 예를 들 어, Lp 정규화는 예컨대, 로서 네트워크에서 가중치들을 0으로 푸시할 수 있으며, 여기 서 d는 가중치(wij)에 의해 연결된 i번째 뉴런과 j번째 뉴런 간의 거리를 나타낸다. 이에 따라, 뉴런 거리(d)가 더 클수록, Lp 정규화가 가중치(wij)를 0으로 드라이브하는 것이 더 빠르다. 뉴런 거리(d)는 예를 들어, 2개의 연결된 i번째 뉴런과 j번째 뉴런을 분리하는 뉴런들, 계층들, 클러스터들 등의 수에 기초한 뉴런 분리 또는 근 접도의 임의의 메트릭일 수 있다. 모든 가능한 클러스터 간 가중치들의 서브세트(예컨대, 소수)가 추가되거나 유지될 수 있다. 다양한 실시 예들에서, 임계치를 초과하는 가중치를 갖거나, 최상위 클러스터 간 가중치들 중 상위 N개(미리 결정된 수) 중 에 있거나, 가장 작은 또는 임계치 미만의 거리를 갖는 뉴런들을 연결하거나, 다른 기준들에 따른 클러스터 간 가중치들이 추가되거나 유지될 수 있다. 일부 실시예들에서, 클러스터 경계를 따라 위치된 뉴런들(그러나 내부 비-경계 뉴런들은 아님)만이 클러스터 간 가중치들을 통해 상이한 클러스터들의 뉴런들에 연결되도록 허용 된다. 일부 실시예들에서, 각각의 뉴런(또는 경계 뉴런들만)에는 미리 결정된 수의 클러스터 간 가중치들, 또는 클러스터 간 가중치들 대 클러스터 내 가중치들의 비율이 허용된다. 다양한 실시예들에서, 뉴런들은 각각의 클러스터 내에서 완전히 그리고/또는 부분적으로 연결될 수 있다. 일부 하이브리드 실시예들에서, 뉴런들/가중치들의 다양한 구역들, 계층들, 서브세트들 등은 클러스터 연결, 비-클러 스터 연결, 완전 연결, 부분 연결, 또는 다른 식으로 연결될 수 있다. 일 실시예에서, 완전 및 부분 연결된 클 러스터들의 조합이 사용될 수 있다. 예를 들어, 상이한 타입들의 클러스터들은 상이한 연결 패턴들, 이를테면 (예컨대, 더 중요한 계층 간 연결들을 표현하는) 완전히 연결된 열 클러스터들 및 (예컨대, 덜 중요한 반복적 계층 내 연결들을 표현하는) 부분적으로 연결된 행 클러스터를 사용할 수 있다. 다른 하이브리드 실시예는 구역 들의 서브세트에 대해 클러스터 연결 뉴런들을 사용할 수 있는 한편, 다른 구역들은 표준 연결들을 사용할 수 있다. 일부 실시예들은 희소 컨볼루션 신경망(CNN)을 생성할 수 있다. CNN은 입력 계층의 채널을 컨볼루션 계층의 채 널에 연결하는 복수의 필터들에 의해 표현된다. 필터는 (예컨대, N×N 픽셀 이미지 구역을 표현하는) 뉴런들의 각각의 점진 구역 상에서 동작하는 입력 채널을 스캔하고, 각각의 구역의 컨볼루션 또는 다른 변환을 컨볼루션 채널 내의 단일 뉴런에 매핑한다. 다수의 뉴런들의 전체 구역들을 각각의 단일 컨볼루션 뉴런에 연결함으로써, 필터들은 다대일(many-to-one) 뉴런 연결을 갖는 시냅스들을 형성하며, 이는 표준 NN들에서의 일대일 뉴런 연결 들과 비교하여 CNN들에서의 시냅스들의 수를 감소시킨다. 일부 실시예들은 채널들의 클러스터들을 그룹화하고 상이한 클러스터들로부터의 채널들을 연결하는 클러스터 간 필터들을 프루닝 또는 제로화함으로써 클러스터 연 결 CNN을 생성할 수 있다. CNN들에서, 필터들은 (첫 번째 계층의 각각의 단일 채널을 두 번째 계층의 단일 채널과 연결하는) 2차원(2D: two-dimensional) 또는 (두 번째 계층의 각각의 단일 채널을 첫 번째 계층의 복수의 채널들과 연결하는) 3차원 (3D: three-dimensional)일 수 있다. 예를 들어, 도 2에 도시된 클러스터 연결 CNN은 입력 및 제1 컨볼루 션 계층들을 30개의 2D 필터들 또는 10개의 3D 필터들과 연결할 수 있다. 이에 따라, CNN 클러스터들은 또 한 도 2에 도시된 바와 같이, (2D 필터들을 그룹화하는) 2D 또는 (3D 필터들 또는 2D 필터들의 다수의 계층들을 그룹화하는) 3D일 수 있다. 따라서 프루닝은 CNN에서 (도 2의) 3D 또는 2D, 또는 이들의 임의의 조합인 클러스 터 간 필터들을 삭제할 수 있다.본 발명의 실시예들은 각각의 클러스터 내에서 조밀하게 연결되고(예컨대, 국소적으로 완전히 연결된 클러스터 내 가중치들) 클러스터 경계들에 걸쳐 희소하게 연결되는(예컨대, 전역적으로 희소한 클러스터 간 가중치들) 클 러스터 연결 신경망을 사용하여 훈련 및 예측하기 위한 신규 시스템 및 방법을 제공한다. 희소화는 훈련 단계 동안 클러스터 간 가중치들을 프루닝함으로써 또는 (예컨대, 유전 알고리즘들을 사용하여 돌연변이들에 의한 클 러스터 간 가중치들을 감소시키거나 제거하도록) 신경망을 진화시킴으로써 달성될 수 있다. 이러한 실시예들은 몇 가지 중요한 개선들을 제공한다: 신경망들에서 상당한 양의 희소성을 가능하게 한다. 클러스터 간 가중치들은 대부분의 네트워크 가중치들을 설명하며, 이들 대부분은 먼 거리들에 걸쳐 있으며, 따라서 로컬 클러스터 내 가중치들만큼 중요할 가능성이 낮 다. 위의 예에서, 1000개의 뉴런들의 2개의 계층들을 각각 10개의 열 클러스터들로 분할하는 것은, 완전 연결 설계에서의 백만 개의 가중치들(1,000 × 1,000)에서 열 연결 설계에서의 십만 개(100 × 100 × 10)로 가중치 들의 수를 90% 감소시킨다. 나머지 몇 개의 클러스터 간 가중치들은 희소하고, 가중치들의 수의 작은 증가만을 설명한다. 훈련 동안의 프루닝은 나머지 가중치들이 프루닝에 의해 야기된 차이들을 상쇄시킬 수 있게 하여, 프루닝 이 전(예컨대, 완전 연결 네트워크에서) 그리고 프루닝 이후(예컨대, 클러스터 연결 네트워크에서) 실질적으로 동 일한 예측 정확도를 야기한다. 신경망에서 유도된 희소성의 양에 정비례하는 선형 가속을 갖는 예측 모드 및 훈련 모드 모두를 야기한다. 예 를 들어, (가중치들의 50% 미만 또는 소수를 유지하는) 50% 희소 클러스터 연결 신경망은 2배(또는 200%) 더 빠 른 예측 및 훈련을 야기한다. 위의 예에서, (가중치들의 10%를 유지하는) 90% 희소 클러스터 연결 신경망은 10 배(또는 1000%) 더 빠른 예측 및 훈련을 야기한다. 일반적으로, 신경망의 희소성이 더 클수록, 예측 시간 및 훈 련 시간이 더 빠르다. 클러스터 기반 인덱싱으로 메모리 사용량의 대략적으로 선형 감소를 야기한다. 국소적으로 조밀한 클러스터들 은 빠른 행렬 곱셈을 위해 각각의 클러스터 특정 행렬과 연관된 클러스터 인덱스에 의해 그들의 클러스터 내 가 중치들을 표현할 수 있다. 그러나 전역적으로 희소한 클러스터 간 가중치들의 대부분은 각각의 비-제로 클러스 터 간 가중치에 대해 독립적으로 인덱싱되어, 제로 클러스터 간 가중치들을 저장할 필요성을 제거할 수 있다. 각각의 비-제로 클러스터 간 가중치를 독립적으로 인덱싱하기 위해(예컨대, 인덱스뿐만 아니라 값을 저장하기 위해) 추가(예컨대, 2배의) 메모리를 사용하면서, 제로 클러스터 간 가중치들의 대부분을 제거하는 것은 90% 비 율로 프루닝된 클러스터 간 가중치들을 저장하기 위한 메모리 소비의 10/2 또는 5배(80%) 감소를 야기한다. 임의의 하드웨어에서 선형 가속을 야기한다. 예를 들어, 90% 희소한 클러스터 연결 신경망은, 계산 디바이스 에 관계없이, 예컨대 느린 CPU 상에서 실행되든 또는 고속 전용 GPU 상에서 실행되든, 완전 연결 신경망과 비교 하여 10배의 가속을 야기한다. 다시 말해서, 본 발명의 실시예들은 (종래의 신경망들을 효율적으로 처리 또는 저장할 수 없는) CPU 또는 메모리 제한 디바이스들 상에서 네트워크들의 딥 러닝을 가능하게 하는 효율에 대한 개선들을 제공할 수 있지만, 동일한 실시예들은 고속 하드웨어에 의해 몇 배의 크기의 저장 감소 및 가속을 야 기하도록 구현될 수 있다(이는 가장 빠른 전용 하드웨어 상에서도 딥 러닝을 사용하는 것이 실현 불가능한 실시 간 내비게이션과 같은 영역들에서 중요하다). 이 방법은 신경망의 타입에 대해 관용적(agnostic)이며 예를 들어, 완전 연결, 부분 연결, 컨볼루션, 순환 등을 포함하지만 이에 제한되지 않는 임의의 신경망 아키텍처에 적용될 수 있고, 네트워크 정확도에 악영향을 주지 않으면서 상당한 희소성을 야기한다. 행렬 표현은 (다수 또는 대부분의 활성 시냅스들을 갖는) 조밀한 신경망들에 대해 구현하기에 편리하고 효율적 이지만, (연결된 시냅스들이 거의 없거나 소수를 갖는) 희소 신경망들에 대해서는 효율적인 표현이 아니다. 신 경망 예측의 속도는 신경망의 가중치들의 수에 비례한다. 10×20개의 가중치들을 갖는 예시적인 행렬의 경우, 행렬은 가중치들의 대부분의 값들을 0으로 설정함으로써 희소 신경망을 표현할 것이다. 그러나 제로화 행렬 가 중치들은 행렬의 엔트리들의 수를 감소시키지 않으며, 따라서 신경망에 대해 수행되는 계산들의 수를 감소시키 지 않는다. 따라서 행렬 표현에서의 메모리 및 계산 요건들은 조밀한 신경망의 경우와 희소 신경망에 대해 동일 하다(행렬 곱셈에서 0이 아닌 값과 마찬가지로 0 값이 저장되고 곱해진다). 다시 말해서, 행렬 표현에서 가중치 들을 0으로 설정하는 것은 메모리로부터 그러한 가중치들을 제거하거나 연관된 계산들의 수를 감소시키지 않는 다. 이에 따라, 클러스터 연결 신경망에서 가중치들을 프루닝하는 것은 종래의 행렬 표현들을 사용하여 메모리를 감소시키지 않는다. 각각의 클러스터 간 가중치를 독립적으로 인덱싱(가중치가 어떤 시냅스를 표현하는지를 독립적으로 정의)하는, 클러스터 연결 신경망들의 새로운 콤팩트한 표현이 본 발명의 일부 실시예들에 따라 제공되며, 이는 프루닝 또 는 생략된 시냅스들의 클러스터 간 가중치들이 스킵 또는 폐기될 수 있게 한다. 종래의 행렬 표현에서, 각각의 가중치는 행렬에서의 자신의 포지션에 의해 인덱싱된다(예컨대, 행 i 열 j의 가중치는 첫 번째 계층의 i번째 뉴 런을 두 번째 계층의 j번째 뉴런에 연결하는 시냅스를 나타낸다). 계층들의 각각의 쌍에 대한 가중치들을 저장 하기 위해 추가 행렬들이 사용될 수 있다. 인덱싱은 행렬 포지션에 기반하기 때문에, 가중치들이 행렬에서 다른 가중치들의 포지션을 시프트할 것이므로 가중치들은 제거될 수 없다. 이는, 희소 신경망이 대부분 0인 엔트리들 의 희소 행렬에 의해 표현되게 하며, 이는 대부분 0인 가중치들을 저장하기 위한 메모리와 0인 가중치들을 곱하 기 위한 계산들 모두의 낭비이다. 본 발명의 실시예들에 따라 각각의 클러스터 간 가중치를 독립적으로 인덱싱 함으로써, 가중치들의 인덱스들은 서로 의존하지 않으며, 따라서 각각의 프루닝된 클러스터 간 가중치는 다른 클러스터 간 또는 클러스터 내 가중치들의 인덱싱에 영향을 미치지 않으면서 완전히 폐기될 수 있다. 이로써, 이러한 독립적인 인덱싱은 (메모리 소비를 감소시키는) 단절된 클러스터 간 시냅스들에 대한 엔트리들을 저장할 필요성을 제거하고, (처리 속도를 향상시키는) 단절된 클러스터 간 시냅스들에 기반하여 수행되는 계산들을 제 거한다. 신경망을 실행하는 속도가 그 안의 가중치들의 수에 비례하기 때문에, 클러스터 간 가중치들에 의해 연 결된 교차 클러스터 뉴런들의 일부만을 갖는 본 발명의 실시예들에 따른 희소 클러스터 연결 신경망이 조밀 또 는 완전 연결 신경망처럼 짧은 시간 내에 실행되고 훈련될 것이다. 클러스터 연결 신경망은 각각의 클러스터 내에서는 국소적으로 조밀한 수의 클러스터 내 가중치들의 어레인지먼 트를 갖지만, 각각의 클러스터 외부에서는 클러스터 간 가중치들의 전역적으로 희소한 어레인지먼트를 갖기 때 문에, 본 발명의 실시예들은 클러스터 간 인덱스들 및 클러스터 내 가중치들을 서로 다르게 인덱싱하는 하이브 리드 인덱싱 시스템을 제공한다. 전역적 희소성을 이용하기 위해, 클러스터 간 가중치들은 각각의 클러스터 간 가중치들을 고유하게 그리고 독립적으로 인덱싱하는 위의 새로운 콤팩트 인덱싱에 의해 인덱싱될 수 있고, 이로 써 제로 클러스터 간 가중치들을 로깅하는 것을 피할 수 있다. 다른 한편으로, 로컬 밀도를 이용하기 위해, 각 각의 클러스터 내의 클러스터 내 가중치들은 각각의 클러스터 내의 가중치들을 이들의 포지션으로 표현하는 조 밀한 하위 행렬과 조합하여 클러스터 인덱스에 의해 인덱싱될 수 있으며, 이는 클러스터별 고속 행렬 곱셈으로 부터 이익을 얻는다. 본 발명의 실시예들은 (예컨대, 도 4의 트리플릿(triplet) 표현을 사용하는) 각각의 시냅스 또는 가중치, 압축 된 희소 행(CSR: compressed sparse row) 표현, 압축된 희소 열(CSC: compressed sparse column) 표현, 맵 표 현, 리스트 표현, 이중 어레이 표현(하나의 어레이는 0이 아닌 엘리먼트들을 저장하고 다른 어레이는 이들의 인 덱스들을 저장함), 희소 텐서(tensor) 표현, 또는 임의의 다른 희소 신경망 또는 행렬 인덱싱을 독립적으로 인 덱싱하는 것을 포함하지만 이에 제한되지 않는, 희소 신경망을 인덱싱하는 많은 방법들을 지원한다. 본 발명의 일부 실시예들에 따라, 클러스터 연결 신경망을 개략적으로 예시하는 도 1이 참조된다. 클러스터 연결 신경망은 (도 1에서 뉴런들을 연결하는 화살표들에 의해 도시된) 복수의 시냅스 연결들에 의해 연결된 복수의 인공 뉴런들을 포함한다. 클러스터 연결 신경망은 각각의 복수의 시냅스 연결들 의 강도를 나타내는 복수의 가중치들로 표현될 수 있다. 시냅스 연결들은 (동일한 클러스터 내부의 2개의 뉴런 들 모두를 연결하는) 클러스터 내 가중치들 및 (서로 다른 클러스터들에 걸쳐 위치된 뉴런들을 연결하기 위해 클러스터 경계(파선 경계 박스들)를 넘는) 클러스터 간 가중치들에 의해 연결될 수 있다. 인공 뉴런들은 다수의 계층들의 계층 구조로 배열될 수 있다. 신경망은 입력 계층, 하나 이상의 중간 또는 은닉 계층(들)(1, 2, … N) 및 출력 계층을 포함할 수 있다. 클러스터 연결 신경망은 복수의 뉴런 클 러스터들로 분할된다. 도 1에 도시된 뉴런 클러스터들은 열 형상이지만, 다른 클러스터 형상들이 사 용될 수 있다. 각각의 클러스터들 내의 각각의 뉴런은 클러스터 내 가중치들에 의해 해당 클러스터 내부 의 인접한 계층들의 (완전히 연결된) (모든) 뉴런들에 연결된다. 그러나 각각의 클러스터들 내의 각각의 뉴런은 상이한 클러스터들 내의 대부분의(또는 모든) 뉴런들로부터 단절된다. 일부 실시예들에서, 각 각의 클러스터들 내의 뉴런들의 서브세트 또는 소수(예컨대, 클러스터 경계의 점선을 따라 포지셔닝 된 경계 뉴런들만)가 클러스터 간 가중치들에 의해 상이한 클러스터들 내의 뉴런들에 연결된다. 이에 따라, 각각의 클러스터 내부에는 완전히 상호 연결된 뉴런들의 국소적으로 조밀한 하위 네트워크가 있는 한편, 클러스터들의 외부에는 대부분 희소 단절된 뉴런들의 전역적 희소 신경망이 있다.이에 따라, 클러스터 연결 신경망은 국소적으로 \"조밀\"할 수 있으며, 여기서 각각의 클러스터 내의 뉴런들의 대부분 또는 임계 퍼센티지 이상이 (예컨대, 비-제로 연결 가중치들을 갖는) 클러스터 내 가중치 들에 의해 연결된다. 임계치는 50% 초과(대부분 연결됨) 내지 100%(\"완전 연결됨\")의 범위 내의 임의의 퍼 센티지일 수 있고, 통상적으로는 90-99% 연결된다. 도 1에 도시된 예에서, 각각의 클러스터 내의 모든 뉴 런들은 인접한 계층들의 모든 다른 뉴런들에 연결되고, 따라서 각각의 클러스터는 완전히 연결된다. 이 예에서, 4개의 뉴런들의 인접한 계층들의 각각의 쌍은 16개의 가능한 연결들을 가지며, 인접한 계층들의 2개 의 쌍들에 대해, 각각의 클러스터에는 32개의 뉴런 연결들 및 연관된 클러스터 내 가중치들이 있다. 클러스터 연결 신경망은 전역적으로 \"희소\"할 수 있으며, 여기서 전체 신경망에 걸친 그리고/또는 교 차 클러스터들의 뉴런들 사이의 뉴런들의 소수 또는 임계 퍼센티지 이하가 클러스터들 간 가중치들에 의해 연결된다(또는 교차 클러스터 뉴런들의 대부분 또는 임계 퍼센티지 초과는 연결되지 않는다). 임계치는 50% 미 만의 범위(소수 연결됨)의 임의의 퍼센티지일 수 있고, 1-10% 연결될 수 있다. 일부 실시예들에서, 클러스터들 간 가중치들의 수 또는 밀도는 예를 들어, 임계치를 초과하는 정확도를 달성하는 최소 수가 되도록 정확도 드라이브될 수 있다. 도 1에 도시된 예에서, 단지 몇 개의 희소 클러스터들 간 가중치들만이 존재한다. 일부 실시예들에서, 클러스터 연결 신경망은 조밀한 신경망으로서 훈련을 개시할 수 있고, 클러스터 간 가 중치들의 대부분 또는 임계치를 넘는 퍼센티지를 프루닝함으로써 도 1의 희소 클러스터 연결 신경망 을 생성하도록 변환될 수 있다. 가중치들은 이전에 연결된 뉴런 쌍들을 단절함으로써 프루닝될 수 있다. 클러스 터 연결 신경망은 신경망을 진화시키는 방법들, 이를테면 유전 알고리즘들, 유전 프로그래밍, 강화 학습 등을 사용하여 훈련될 수 있다. 클러스터 연결 신경망은 전역적으로 희소 표현을 갖는 다양한 타입들의 연 결들, 이를테면 예컨대, 로컬 연결들, 순환 연결들, 스킵 연결들 등의 하이브리드 혼합을 가질 수 있다. 이러한 연결의 혼합으로 신경망을 진화시키는 것은 클러스터들 간 가중치들 및/또는 클러스터들 내 가중치들(10 4)을 인덱싱하기 위해 본 발명의 실시예들에 따른 콤팩트한 독립적인 인덱싱을 사용하여 효율적으로 수행될 수 있다. 추가로 또는 대안으로, 클러스터 연결 신경망은 (프루닝 없이) 우선 희소 네트워크로서 생성 또는 수신될 수 있다. 일부 실시예들에서, 클러스터 연결 신경망은 클러스터 내 가중치들(그러나 클러스터 간 가중치들은 아님)로만 개시될 수 있고, 클러스터 간 가중치들의 희소 서브세트가 훈련 도중 추가 될 수 있다. 종래의 행렬들에서, 프루닝된 또는 생략된 가중치들은 0으로 설정되고, 연결된 가중치들과 동일하게 취급되며, 이는 프루닝에 대한 어떠한 상당한 저장 또는 처리 이점도 산출하지 않는다. 본 발명의 실시예들에 따르면, 도 3의 클러스터 연결 신경망의 복수의 클러스터 간 가중치들을 클러스터 간 가중치들의 값(열 3) 및 연관된 고유 인덱스(열 1 - 열 2)로 나타내는 새로운 데이터 구조가 도 4에 도시된 바와 같이 제공된다. 클 러스터 간 가중치들이 각각의 데이터 엔트리에서 명시적으로 인덱싱되기 때문에, 도 4의 표현에서의 데이 터 엔트리들의 순서는 더는 이들의 암시적 인덱스로서 기능하지 않으며, 가중치 엔트리들은 정보의 손실 없이 셔플(shuffle) 또는 재정렬될 수 있다. 특히, 행렬 표현들에서와 같이 인덱싱을 유지하기 위한 플레이스 홀더 (placeholder)로서 클러스터 간 가중치에 대해 0의 값을 저장할 이유가 없다. 이에 따라, 2개의 클러스터 간 뉴런들이 (프루닝에 의해) 단절되거나 먼저 연결되지 않을 때, 도 4의 데이터 구조는 단순히 해당 연결에 대 한 엔트리를 완전히 삭제하거나 생략한다(예컨대, 가중치 또는 임의의 정보의 어떠한 기록도 해당 연결에 대해 저장되지 않는다). 뉴런들의 쌍들 사이의 활성 교차 클러스터 연결들을 나타내는 비-제로 클러스터 간 가중치들만을 저 장함(그리고 뉴런들의 쌍들 사이의 단절들, 비활성 연결들 또는 연결들 없음을 표현하는 제로 클러스터 간 가중 치들을 저장하지 않음)으로써, 도 4의 데이터 구조는 클러스터 연결 신경망의 희소 클러스터 간 가중치들 을 저장하기 위한 메모리를 전체적으로 클러스터 간 가중치들 및/또는 네트워크의 희소성에 정 비례하는 양만큼 감소시킬 수 있다. 클러스터 간 가중치들의 X%가 제거되거나 생략되어 총 가중치들의 100-X%만을 남기고, 인덱스가 가중치와 동일한 수의 비트들을 사용한다면, 가중치 엔트리들은 완전 연결 신경망 에 의해 점유된 것보다 저장소의 2x(100-X)%를 점유할 수 있다(예컨대, 99% 희소성은 조밀한 표현에 사용되는 메모리의 단지 2%, 즉 50배 더 적은 메모리 사용량만을 요구하는 희소 표현을 야기한다). 일부 실시예들에서, 조밀한 클러스터 내 가중치들은 조밀한 또는 완전히 연결된 가중치들을 저장하는 데 보다 효율적인 행렬들에 의해 저장될 수 있다. 전체 클러스터 연결 신경망에 대한 하나의 전역적 행렬에 추가로 또는 그 대신에, 각각의 클러스터는 고유 클러스터 인덱스에 의해 표현되는 하위 네트워크로서 취 급될 수 있고, 그의 클러스터 내 가중치들은 대응하는 클러스터 특정 하위 행렬에 의해 표현될 수 있다.신경망을 실행하는 속도는 신경망의 가중치들의 수에 비례한다. 클러스터 연결 신경망에서 연결들을 프루 닝하거나 생략하는 것은 희소성의 양에 비례하여 직접적인 예측 가속을 야기할 수 있다(예컨대, 클러스터 간 시 냅스들의 X%가 제거되거나 생략되어 총 시냅스들의 100-X%만이 남는다면, 결과적인 클러스터 연결 신경망은 완 전 연결 신경망보다 100/(100-X)배 더 빠르게 수행될 것이다). 본 발명의 일부 실시예들에 따라, 클러스터 연결 컨볼루션 신경망을 개략적으로 예시하는 도 2가 참조된다. 컨볼루션 신경망은 입력 계층, 하나 이상의 컨볼루션 계층들(202, 203) 및 하나 이상의 출력 계층들 을 포함한다. CNN의 각각의 계층(201, 202, 203, …)은 하나 또는 복수의 채널들을 가질 수 있다. 도 2에 도시된 예에서, 입력 계층은 컬러 이미지를 나타내고 3개의 컬러 채널들(예컨대, 적색, 녹색 및 청색 채널 들)을 갖는다. 제1 컨볼루션 계층은 복수(예컨대, 10개)의 채널들(예컨대, C1-C10)을 갖고, 제2 컨볼루션 계층은 복수(예컨대, 8개)의 채널들(예컨대, C1-C8)을 갖는다. 각각의 컨볼루션 채널은 가장자리들, 선들, 원들 또는 상위 계층들의 더 복잡한 객체들, 이를테면 사과들, 망치들 등과 같은 피처의 피처 맵을 나타낼 수 있다. 이러한 피처들의 채널들은 통상적으로 신경망의 훈련 프로세스로부터 완전히 드러난다(그리고 수동으로 지정되지 않음). 완전히 연결된 CNN에서, 계층의 각각의 채널은 컨볼루션 필터에 의해 후속 계층의 각각의 채널에 연결될 수 있다. 각각의 필터는 하나의 채널의 (예컨대, N×N 픽셀 이미지 구역을 표현하는) 뉴런들의 구역들의 (인접한 또는 인접하지 않은) 컨볼루션 계층의 채널 내의 뉴런들로의 컨볼루션 또는 변환인 복수의 가중치들의 그룹을 표현한다. 예시적인 2D 컨볼루션 필터는 입력 채널(예컨대, 1a+2b+3c+…) 내의 뉴런들의 각각의 N ×N 그룹(예컨대, 1, 2, 3,…NN)을 컨볼브(convolve)하여 컨볼루션 채널에서 단일 연결 컨볼루션 뉴런과 동일하 게 하도록, 한 세트의 N×N 가중치들(예컨대, a, b, c,…)을 포함한다. N×N 가중치들의 동일한 단일 컨볼루션 필터가 입력 채널 전반에 걸쳐 뉴런들의 모든 N×N 그룹들을 컨볼브하는 데 사용된다. 일반적으로, 컨볼루 션 필터는 1차원(1D: one-dimensional)(예컨대, 뉴런들의 열 또는 행에 대해 동작하는 1×N 행 필터 또는 N×1열 필터), 2차원(2D) (예컨대, 뉴런들의 2D 그리드에 대해 동작하는 N×M 필터), 3차원(3D)(예컨대, 계층 내의 다수의 채널들에 걸쳐 그리드에 대해 동작하는 N×M×P 필터), …, 또는 (예컨대, 다수의 채널들 및 다수 의 계층들에 걸쳐 그리드에 대해 동작하는) N차원(ND: N-dimensional)을 포함하는 다양한 차원들을 가질 수 있 다. 예를 들어, 입력 계층의 각각의 컬러 채널은 제1 컨볼루션 계층의 각각의 컨볼루션 채널(C1- C10)에 연결될 수 있으며, 각각의 컨볼루션 채널(C1-C10)은 결국 제2 컨볼루션 계층의 각각의 컨볼루션 채 널(C1-C8)에 연결될 수 있다. 도 2의 예에서는, 입력 계층에 3개의 채널들이 있고, 제1 컨볼루션 계층 에 10개의 채널들(C1-C10)이 있으며, 제2 컨볼루션 계층에 8개의 채널들(C1-C8)이 있어, 완전 연결 아키텍처의 총 N = 240개의 가능한 필터들이 입력 및 컨볼루션 계층들(201, 202, 203, …)로부터의 모든 각각의 쌍의 채널들을 연결하게 된다. CNN은 통상적으로, 도시된 것보다 더 많은 컨볼루션 계층들 및/또는 도시된 것과는 다른(예컨대, 최대 풀링) 계층들을 가지며, 이는 필터들의 수가 기하급수적으로(예컨대, 수천, 수백만 또는 수십억 개의 필터들로) 증가하게 한다. 본 발명의 실시예들은. 채널들을 복수의 이산 클러스터들로 그룹화하고 상이한 클러스터들에 위치된 채널 들을 연결하기 위해 경계들을 넘는 많은 또는 대부분의 클러스터 간 필터들을 프루닝 또는 생략함으로써, 희소 클러스터 연결 CNN을 생성할 수 있다. 채널들이 상당한 수의 클러스터들(예컨대, 3개 초과, 바 람직하게는 수십 개 또는 심지어 수백 개의 클러스터들)로 분할될 때, 완전히 연결된 CNN 필터들의 대부분은 클 러스터 간 필터들이다. 클러스터 간 필터들의 희소 어레인지먼트를 거의 프루닝함으로써, 본 발명의 실시예들은 전역적으로 희소 클러스터 연결 CNN을 생성한다. 프루닝된 또는 생략된 클러스터 간 필터들 로 클러스터 연결 CNN을 동작시키는 것은 이들의 연관된 컨볼루션 동작들을 실행하는 것을 피하고, 클러스터 연결 CNN의 훈련 및/또는 예측을 가속화한다. 종래의 CNN들은 비-제로 필터들과 동일한 방식으로 제로 필터들에 대해 저장 및 동작하여, 프루닝에 상당한 저 장 또는 처리 이익을 산출하지 않는 반면, 본 발명의 실시예들에 따르면, 비-제로 클러스터 간 필터들만을 저장하는 새로운 데이터 구조가 제공된다. 새로운 데이터 구조는, 2개의 채널 인덱스들(열 1 - 열 2)이 클러스 터 간 필터들에 의해 연결된 입력/출력 채널들을 고유하게 정의하고 하나의 필터 표현(열 3)이 필터의 가 중치 값을 정의하도록, 도 4의 트리플릿 표현과 같은 콤팩트한 희소 인덱싱 방법을 사용할 수 있다. 클러스터 간 필터들이 각각의 데이터 엔트리에서 명시적으로 인덱싱되기 때문에, 데이터 엔트리들의 행렬 포지션이 더는 이들의 암시적 인덱스로서 기능하지 않으며, 가중치 간 필터 엔트리들은 정보의 손실 없이 셔플되거나, 재 정렬되거나, 삭제될 수 있다. 특히, 행렬 표현들에서와 같이 인덱싱을 유지하기 위한 플레이스 홀더로서 제로클러스터 간 필터들(모두 0인 가중치들을 갖는 필터)을 저장할 이유가 없다. 이에 따라, 뉴런들의 채널들이 (프 루닝에 의해) 단절되거나 먼저 연결되지 않을 때, 도 4의 데이터 구조는 단순히 연관된 필터에 대한 엔트리를 완전히 삭제하거나 생략한다(예컨대, 임의의 가중치 또는 임의의 정보의 어떠한 기록도 해당 필터에 대해 저장 되지 않는다). 다양한 실시예들에서, 데이터 구조는 예컨대, 미리 정의된 바와 같이 또는 완전히 제로화된 최고 차원으로서 1D, 2D, 3D, 또는 ND 필터들을 생략할 수 있다. CNN들에서, 필터들은 (첫 번째 계층의 각각의 단일 채널을 두 번째 계층의 단일 채널과 연결하는) 2차원(2D) 또는 (두 번째 계층의 각각의 단일 채널을 첫 번째 계 층의 복수의 채널들과 연결하는) 3차원(3D)일 수 있다. 예를 들어, 도 2에 도시된 클러스터 연결 CNN은 CNN을 3D 클러스터들로 분할할 수 있고, 따라서 3D 클러스터 간 필터들을 삭제할 수 있지만, 클러스 터들 및 필터들의 임의의 차원이 사용될 수 있다. 뉴런들 사이의 활성 컨볼루션들을 나타내는 비-제로 클러스터 간 필터들만을 저장함(그리고 뉴런들 사이의 컨볼루션들을 전혀 나타내지 않거나 무시할 수 있는 컨볼루션들을 나타내는 제로 필터들을 저장하지 않음)으로 써, 도 4의 데이터 구조는 희소 컨볼루션 신경망을 저장하기 위한 메모리를 CNN에서 삭제된 클러스터 간 필터들의 양에 비례하는 양만큼 감소시킬 수 있다. 컨볼루션 신경망을 실행하는 속도는 CNN의 필터들의 수에 비례한다. 클러스터 연결 CNN에서 필터들을 프루 닝 또는 생략하는 것은 CNN에서 생략된 필터들의 수에 비례하여 직접적인 예측 가속을 야기할 수 있다. 도 1 - 도 4의 데이터 구조들의 어레인지먼트는 단지 예들일 뿐이며, 뉴런들, 연결들, 필터들, 채널들, 계층들"}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "및 클러스터들의 다른 수들, 크기들, 치수들 및 구성들이 사용될 수 있다는 것이 당해 기술분야에서 통상의 지 식을 가진 자들에 의해 인식될 것이다. 추가 희소 데이터 표현들: 다음의 표현들은 도 4의 트리플릿 표현에 추가로 또는 대안으로, 비효율적인 종래의 희소 행렬 표현을 대체할 수 있다. 압축된 희소 행(CSR) 데이터 표현은 희소 행렬에 대한 저장을 감소시키는 데 사용될 수 있다. CSR은 3개의 (1차 원) 어레이들을 사용하여 행 형태의 행렬을 표현할 수 있으며, 첫 번째 어레이는 행렬의 0이 아닌 값들을 정의 하고 나머지 어레이들은 행렬 내의 클러스터 간 가중치들의 희소성 패턴을 나타낸다. 희소 컨볼루션 신경망들의 경우, 본 발명의 실시예들은 예컨대, 행렬의 상이한 차원들에 대해 4차원(또는 더 높은) 행렬 또는 CSR 기반 인 덱싱 방법, 또는 이 둘의 조합을 표현하기 위해 수정된 트리플릿들을 사용할 수 있다. 맵 표현은 뉴런 ID들(또는 필터 ID들)\"로부터(from)\" 및 뉴런 ID들(또는 필터 ID들)\"로(to)\"가 가중치(w)에 매 핑되는 맵으로 종래의 행렬을 대체할 수 있다. 이는 트리플릿 표현과 유사한 양의 저장을 요구하지만, 새로운 0 이 아닌 가중치들의 더 느린 추가의 대가로, 개별 가중치들(제로 및 비-제로 모두)에 대한 더 빠른 액세스를 허 용한다. 리스트 표현은 종래의 행렬을 쌍들의 리스트 <\"from\", inner_list>로 대체할 수 있는 한편, 내부 리스트들은 <\"to\", w> 형태의 쌍들을 포함하며, 여기서 \"to\", \"from\" 및 w는 위와 같다. 위의 변형은 예를 들어, 행렬을 행 들의 수의 크기의 리스트로서 표현하기 위해 희소 벡터들의 리스트를 유지하는 것이며, 행들의 엘리먼트들은 <j, w> 쌍들의 리스트들이다(이 인덱스의 뉴런이 연결들을 갖지 않는다면, 가능하게는 비어 있음). 리스트 표현 은 예컨대, 다음과 같이 임의의 희소 벡터 표현과 함께 사용될 수 있다. 희소 벡터 표현들은 예를 들어, 다음을 포함한다: 인덱스들에 의해 정렬된 또는 정렬되지 않은 <index, value> 쌍들의 리스트. 0이 아닌 엘리먼트의 인덱스가 엘리먼트에 매핑되는 맵 또는 사전. 누락 인덱스들은 0들로 취급될 수 있다. 2개의 어레이들, 즉 0이 아닌 모든 엘리먼트들을 보유하는 하나의 데이터 어레이, 및 원래의 벡터에서 일치하는 데이터 엘리먼트의 인덱스를 보유하는 인덱스 엘리먼트. 희소 벡터들의 희소 벡터는 가능한 희소 벡터 표현들 중 하나에서 종래의 행렬을 희소 벡터로 대체할 수 있으며, 여기서 각각의 데이터 엘리먼트는 다른 희소 벡터이다. 이는 다수의 제로 행들/열들을 갖는 행렬들에 특히 유용할 수 있다. 압축된 희소 행(일명 압축된 행 저장) 표현은 종래의 행렬을 다음의 3개의 어레이들로 대체할 수 있다: (예 컨대, 행 주요 순서로, 즉 왼쪽에서 오른쪽으로, 그 다음 위에서 아래로 정렬되는) 0이 아닌 모든 가중치들을 보유하는 제1 데이터 어레이. 제2 데이터 어레이는 증가하는 수의 엘리먼트들을 행들로 표현한다(따라서 전체 행렬에서 항상 0이 아닌 엘리먼트들의 총 수인 마지막 엘리먼트까지, 첫 번째 엘리먼트는 항상 0이고, 두 번 째는 첫 번째 행 내의 0이 아닌 엘리먼트들의 수이고, 세 번째는 처음 2개의 행들 내의 0이 아닌 엘리먼트들의 수가 되는 식이다). 제3 데이터 어레이는 각각의 0이 아닌 엘리먼트의 열 인덱스(j)(즉, 뉴런의 \"to\" 식별 자)를 포함하며, 이는 데이터 어레이에서 이들의 순서와 일치한다. 압축된 희소 열(일명 압축된 열 저장, 일명 하웰-보잉 희소 행렬(Harwell-Boeing Sparse Matrix) 표현은 종래의 행렬을 다음의 3개의 어레이들로 대체할 수 있다: 압축된 희소 행에서와 같이 (예컨대, 열 주요 순서로, 즉 위에서 아래로, 그 다음 왼쪽에서 오른쪽으로 정렬되는) 0이 아닌 모든 클러스터 간 가중치들의 제1 데이터 어 레이. 제2 데이터 어레이는 값들에 대응하는 행 인덱스들의 리스트를 나타낸다. 제3 데이터 어레이는 데이터 어레이의 인덱스들의 리스트를 포함하며, 여기서 각각의 새로운 열이 시작된다. 예를 들어, [1,2,4]는 데이터 어레이의 첫 번째 엘리먼트가 행렬의 첫 번째 열에 속하고, 두 번째 엘리먼트 및 세 번째 엘리먼트는 두 번째 열에 속하며, 네 번째 엘리먼트가 세 번째 열을 시작함을 의미한다. 수정된 압축된 희소 행: 개선된 CSR 표현은 종래의 행렬을 다음의 2개의 어레이들로 대체할 수 있다: 제1 데이터 어레이가 행 주요 순서(일반적인 CSR과 동일한 방식)로 먼저 (예컨대, 대각선 상에 임의의 값이 있다면, 0들을 포함하는) 대각선 값들, 그 다음에 나머지 0이 아닌 엘리먼트들을 보유한다. 제2 (인덱스) 데이터 어 레이는 제1 데이터 어레이와 동일한 길이를 갖는다. 제1 어레이의 대각선 엘리먼트들과 일치하는 엘리먼트들은 데이터 배열에서 해당 행의 첫 번째 엘리먼트를 가리키는 한편(따라서 첫 번째 엘리먼트는 항상 대각선의 크기 에 1을 더한 값임), 데이터의 나머지와 일치하는 엘리먼트들은 행렬의 해당 데이터 엘리먼트의 열 인덱스를 특 정한다. 예를 들어, 다음의 값들: [[1,2,0,3], [0,4,5,0], [0,0,0,6], [0,0,0,7]]을 갖는 4×4 행렬은 제1 데 이터 어레이: [1,4,0,7,2,3,5,6] 및 제2 인덱스 어레이: [4,6,7,7,1,3,2,3]이 될 것이다. 수정된 압축된 희소 열 표현은 종래의 행렬을 다음의 2개의 어레이들로 대체할 수 있다: 제1 데이터 어레이 가 열 주요 순서(일반적인 CSC와 동일한 방식)로 먼저 (대각선 상에 임의의 값이 있다면, 0들을 포함하는) 대각 선 값들, 그 다음에 나머지 0이 아닌 엘리먼트들을 보유한다. 제2 (인덱스) 어레이는 제1 데이터 어레이와 동일한 길이를 갖는다. 제1 어레이의 대각선 엘리먼트들과 일치하는 엘리먼트들은 데이터 배열에서 해당 열의 첫 번째 엘리먼트를 가리키는 한편(따라서 첫 번째 엘리먼트는 항상 대각선의 크기에 1을 더한 값임), 데이터의 나머지와 일치하는 엘리먼트들은 행렬의 해당 데이터 엘리먼트의 행 인덱스를 특정한다. 예를 들어, 다음의 값 들(위와 동일한 값들): [[1,2,0,3], [0,4,5,0], [0,0,0,6], [0,0,0,7]]을 갖는 4×4 행렬은 제1 데이터 어레이: [1,4,0,7,2,5,3,6] 및 제2 인덱스 어레이: [4,4,5,6,1,2,3,3]이 될 것이다. 희소 텐서 표현: 텐서는 벡터들 및 행렬들의 더 높은 차원으로의 일반화이다. 예를 들어, 3차원 텐서는 (행렬들 에 대해 2개, 그리고 벡터들에 대해 하나의 인덱스보다는) 3개의 인덱스들을 가지며, 엘리먼트들이 행렬들인 벡 터로서 고려될 수 있다. 희소 텐서 표현들은 다음의 2개의 카테고리들로 분할될 수 있다: 더 낮은 차원의 텐서들의 조합, 또는 특정된 방법들 중 하나의 일반화. 예를 들어, 3D 텐서는 행렬들의 벡터로서 표현될 수 있 으며, 여기서 각각의 행렬은 위의 포맷들 중 임의의 포맷을 사용하는 희소 행렬이다. 대안으로 또는 추가로, 3D 텐서는 압축된 희소 행의 일반화에 의해 표현될 수 있으며, 여기서 데이터, 인덱스 및 열 어레이들 은 이전과 같지만, 인덱스 어레이는 단지 행 인덱스들보다는 인덱스들의 쌍들을 유지한다. 클러스터 간 가중치들 또는 필터들은 다음 기술들 중 임의의 하나 이상을 사용하여 감소되거나 프루닝될 수 있 다: 훈련 동안의 희소성 유도: 훈련 동안 희소성을 유도하기 위한 몇몇 실시예들이 제공되며, 이들은: L1 정규화, Lp 정규화, 임계화, 랜덤 제로화, 새로운 가중치 생성, 유전 알고리즘들을 사용한 가중치들의 진화, 및 편향 기반 프루닝 중 하나 이상의 임의의 조합을 포함한다. L1 정규화: 본 발명의 일부 실시예들은 (예컨대, 역전파와 같은 가중치 정정 업데이트들에 추가로) 하나 이상의 반복들 각각에서 신경망 훈련 동안 L1 정규화를 사용하여 뉴런 연결들을 프루닝할 수 있다. 신경망의 가중치들 (wij)은 예를 들어, 다음과 같이 각각의 훈련 반복에서 가중치들(wij')로 업데이트될 수 있으며:"}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 d는 \"가중치 감쇠\" 파라미터(통상적으로는 매우 작은 수)이고, sgn은 부호 함수이다. 가중치 감쇠는 거 리의 함수일 수 있다. 다시 말해서, 각각의 클러스터 간 가중치 업데이트에서, 클러스터 간 가중치의 값은 점진적으로 감쇠되거나 0을 향해 드라이브된다. 위의 식에서 클러스터 간 가중치에 의해 연결된 뉴런들 사이의 거리 의 감쇠 파라미터(d)가 클수록, 클러스터 간 가중치들은 더 빠르게 0에 접근할 것이고, 교차 클러스터 뉴런들 사이의 단절(연결의 프루닝)을 나타내는 절대 0이 될 클러스터 간 가중치들의 부분이 더 커질 것이다. 일 실시예에서, 프루닝은 수정된 L1 정규화를 사용하여 수행될 수 있다: 클러스터 간 가중치가 0이 되는(또는 부 호를 변경하는) 순간, 가중치의 메모리 엔트리는 저장소로부터(트리플릿 표현 표로부터) 물리적으로 제거되거나 삭제되고, 미래에(예컨대, 임의의 향후 시점에 또는 설정된 잠금 기간 시간 기간 또는 설정된 횟수의 반복들 동 안) 다시 성장하거나 0이 아닌 값으로 재생성될 수 없다. Lp 정규화: Lp 정규화는 예컨대, 다음과 같이 네트워크에서 가중치들을 0으로 \"푸시\"하는 원하는 거동을 개선할 수 있는 L1 정규화의 확장이며:"}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서 d는 0으로의 드라이브 또는 푸시의 속도, 이를테면 클러스터 간 뉴런들(i, j) 사이의 거리를 나타내고, p는 Lp 정규화에서 정규화 팩터의 거듭제곱을 나타내며, 이는 해당 드라이브가 적용되는 값들의 분포를 효과적으 로 나타낸다(예컨대, p는 양의 값임). 이 예에서, 더 높은 p는 더 높은 가중치들 쪽으로 드라이브를 0으로 더 시프트하여, 더 낮은 가중치들에 더 적은 압력을 가한다. 컨볼루션 계층들을 정규화할 때, 전체 필터가 단위로 서 함께 정규화될 수 있으며, 이 경우 위의 Lp 정규화는 예컨대, 다음과 같이 수정될 수 있으며:"}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서 p는 0 내지 1이고, r은 커널(컨볼루션 계층의 필터)의 반경이며, 예컨대 커널은 2*r+1 크기의 행렬이다. 이러한 수정된 Lp 정규화에서는, 0 값들을 갖는 이웃 필터들이 더 많을수록, 필터에 대한 압력은 0까지 더 커진 다. Lp 정규화는 탄력적인 동적 압력을 허용하며, 여기서 p는 클러스터 간 가중치들의 도함수/노름(norm)을 0으 로 푸시하도록 예컨대, 희소성의 퍼센티지에 기반하여 동적으로 수정될 수 있다. 위의 식들은 가중치들의 값들 자체, 클러스터 간 뉴런들 사이의 거리에 기초하여, 그리고 컨볼루션 필터들의 경우에는, 동일한 필터 내의 이 웃 가중치들의 가중치들에도 또한 기초하여 클러스터 간 가중치들을 0으로 조장한다. 임계화: 클러스터 간 가중치들 및 이들의 엔트리들은, 가중치 값이 0은 아니지만 거의 0인 임계치 미만일 때 다 음과 같이 물리적으로 삭제될 수 있다:"}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "임계치는 훈련 동안 오류 정정(예컨대, 역전파)을 취소하지 않도록 충분히 낮게 균형이 맞춰질 수 있는 한편, 적정하게 빠른 레이트로 프루닝하고 그 오류 정정이 값들을 0으로부터 끌어당기는 것을 방지하기에 충분히 높다. 예시적인 임계치들은 0.1, 0.001, 0.0001, 0.00001 등을 포함하지만 이에 제한되지 않는다. 반올림: 부동 소수점 뒤의 미리 지정된 수의 자릿수들 뒤의 값들을 제거한다. 예를 들어, 5자리에서 반올림한다 면, 0.12345678 값은 0.12345로 설정된다. 반올림은 가중치 값이 반올림에 의해 허용되는 최소값보다 작을 때 가중치를 제로화할 것이다. 그렇지 않으면, 반올림이 가중치를 직접적으로 제로화하지 않을 때, 이는 역전파로 인해 가중치 업데이트들 중 일부를 방해함으로써 추가적인 전체 희소성을 야기할 수 있다. 반올림하기 위한 미 리 지정된 수의 자릿수들은 마찬가지로, 오류 정정을 취소하지 않게 충분히 적도록 균형이 맞춰질 수 있는 한편, 그 오류 정정이 값들을 0으로부터 끌어당기는 것을 방지하기에 충분히 많을 수 있다. 가중치가 반올림되 는 부동 소수점 다음의 임의의 정수 개의 자릿수들이 사용될 수 있다. 랜덤 제로화: 클러스터 간 가중치들은 고정된 작은 확률(완전 랜덤 제로화)로 또는 이들의 현재 값에 비례하는 확률(부분 랜덤 제로화)로 0으로 설정될 수 있다. 부분 랜덤 제로화의 후자의 경우, 가중치가 더 작을수록, 가 중치가 0이 될 확률이 더 크다. 일반적으로, 랜덤하게, 확률적으로(예컨대, 이들의 현재 값에 비례하는 확률로) 그리고/또는 수학적 또는 통계 적 휴리스틱스(heuristics)를 사용하는 것을 포함하여, 클러스터 간 가중치들을 0으로 설정하거나 클러스터 간 가중치들을 0에 접근하게 감쇠시키는 임의의 추가 또는 대안적인 프루닝 방법이 여기서 사용될 수 있다.새로운 가중치 생성: 클러스터 간 가중치들을 0으로 설정하고 이들을 메모리로부터 삭제(프루닝)하는 것에 추가 로 또는 대안으로, 본 발명의 일부 실시예들은 이전에 존재하지 않았던 새로운 클러스터 간 가중치들 또는 연결 들을 랜덤으로 생성(야기)할 수 있다. 새로운 클러스터 간 가중치들은 랜덤하게, 확률적으로(예컨대, 2개의 뉴 런들이 더 많이 \"함께 발화(fire)\"될수록, 이들이 연결될 확률이 더 높고 그리고/또는 그 연결의 가중치가 더 높음), 그리고/또는 수학적 또는 통계적 휴리스틱스를 사용하여 생성될 수 있다. 희소 신경망들의 진화: 유전 알고리즘(GA: genetic algorithm)들이 신경망들을 훈련시키는 데 사용될 수 있다. GA들은 예컨대, 인공 \"염색체\"로서 신경망의 가중치들의 세트를 표현하며, 여기서 각각의 염색체는 하나의 신경 망을 표현한다. 유전 알고리즘들은 (a) 각각의 염색체의 적합성 또는 정확도를 측정하는 단계(예컨대, 훈련 세 트에 대한 평균 손실이 낮을수록, 적합성이 더 양호함), (b) 번식에 더 적합한 염색체들을 선택하는 단계, (c) (예컨대, 자식을 생성하기 위해 모체들로부터 랜덤하게 가중치들을 선택하는) 모체 염색체들의 쌍들 간의 재조 합 또는 크로스오버를 수행하는 단계, 및 (d) 자식을 돌연변이가 되게 하는 단계(예컨대, 클러스터 간 가중치들 을 삭제 또는 추가)를 수행함으로써 이러한 염색체들의 집단을 진화시킬 수 있다. GA들은 일반적으로 훈련 동안 너무 많은 변동성 및 휘발성을 겪지만, 본 명세서에 개시된 희소 데이터 구조들의 콤팩트하고 빠른 표현은 신경 망들을 효율적으로 진화시키기 위한 균형을 제공할 수 있다. 대안으로 또는 추가로, 유전 프로그래밍(GP: genetic programming)이 또한 사용될 수 있다. GP는, 신경망을 염색체로서 표현하는 대신에, 신경망이 \"트리\"로 서 표현된다는 차이를 갖고, GA와 유사한 방식으로 작동한다. 따라서 신경망 아키텍처(계층들 및 이들의 연결들)는 GP 트리로서 표현되고 진화될 수 있다. GA는 통상적으로, 고정된 수의 계층들 및 뉴런들을 가정하지 만(그리고 연결들만을 진화시키지만), GP는 계층들의 수, 뉴런들의 수, 및/또는 이들의 연결들을 진화시킬 수 있다. 신경망 아키텍처를 진화시키기 위한 더 추가적인 또는 대안적인 방법으로서, 강화 학습이 또한 적용될 수 있으며, 여기서 신경망 아키텍처의 단일 인스턴스는 전체 정확도를 최대화하기 위해 확률적으로 수정된다. 편향 기반 뉴런 프루닝: 편향 유닛은 클러스터 내 가중치들에 유리하게, 뉴런의 가중치들에 각각 부스팅 또는 감소 상수 값을 추가함으로써 훈련 동안 뉴런의 클러스터 간 가중치들에 대해 \"편향\"될 수 있다. 편향 값이 충 분히 낮다면(예컨대, 큰 크기의 음의 값), 편향 유닛은 뉴런의 클러스터 간 가중치들 중 일부를 음의 값으로 시 프트할 수 있고, 그런 다음, 이는 프루닝된다. 본 발명의 일 실시예에 따른 클러스터 연결 신경망을 사용한 훈련 및 예측을 위한 시스템을 개략적으로 예 시하는 도 5가 참조된다. 시스템은 데이터 구조들을 저장 및/또는 생성할 수 있고, 도 1 - 도 4를 참조하 여 설명된 신경망들의 훈련 및 예측을 구현할 수 있다. 시스템은 하나 이상의 로컬 엔드포인트 디바이스(들) 및 네트워크 또는 컴퓨팅 클라우드를 통해 로컬 디바이스에 액세스 가능한 하나 이상의 원격 서버(들)를 포함할 수 있다. 통상적으로, 클러스터 연결 신경망은 원격 서버에 의해 훈련되고 및 하나 이상의 로컬 엔드포인트 디바이스들에서 예측을 위해 실행되지만, 원격 서버 및/또는 로컬 엔드포인트 디바이스들은 본 발명의 실시예들에 따라 클러스터 연결 신경망을 사용하여 훈련 및/또는 예측할 수 있다. 특히, 통상적으로 매우 제한된 메모리 및 처리 능력들을 갖는 로컬 엔드포인트 디바이스들이 클러스터 연결 신경망에 기반하여 훈련 및/또는 예측할 수 있게 하기 에 충분히 콤팩트한 클러스터 연결 신경망들에 대해 데이터 표현(예컨대, 도 4, CSR, 또는 다른 희소 행렬 표현)이 제공된다. 로컬 엔드포인트 디바이스들이 훈련 및 런타임 예측을 수행할 때, 원격 서버는 제 거될 수 있다. 원격 서버는 클러스터 연결 신경망을 저장하기 위한 메모리 및 클러스터 연결 신경망에 기초하여 훈 련 및/또는 예측하기 위한 프로세서를 가질 수 있다. 원격 서버는 단절된 클러스터들을 갖는 신경망 으로 초기화할 수 있고, 소수의 클러스터 간 가중치들 또는 필터들을 추가할 수 있거나, 완전 연결 신경망을 초 기화하고 클러스터 간 가중치들 또는 필터들의 대부분을 프루닝하여 클러스터 연결 신경망(예컨대, 도 1의 100 또는 도 2의 200)을 생성할 수 있다. 일부 실시예들에서, 원격 서버는 예를 들어, 조밀 또는 완전 연결 신 경망이 사용될 때, 신경망을 저장하기 위한 대형 메모리 및 특수 프로세서(예컨대, GPU)를 포함하는 특수 하드웨어를 가질 수 있다. 메모리는 훈련 데이터 세트를 포함하는 데이터 및 클러스터 연결 신 경망의 복수의 가중치들을 나타내는 데이터를 저장할 수 있다. 데이터는 또한 예컨대, 본 발명의 실시예들 에 따라 데이터의 저장 및 검색을 가능하게 할 코드(예컨대, 소프트웨어 코드) 또는 로직을 포함할 수 있 다. 로컬 엔드포인트 디바이스(들)는 각각, 본 발명의 일부 실시예들에서 제공되는 데이터 표현(예컨대, 도 4, CSR 또는 다른 희소 행렬 표현)에 따라 클러스터 연결 신경망을 저장하기 위한 하나 이상의 메모리들을 포함할 수 있다. 메모리는 고유 인덱스(예컨대, 도 4의 데이터 표현들의 열 1 및 열 2)와 함께(또는 이와 연 관된) 클러스터 연결 신경망의 복수의 가중치들(예컨대, 도 4의 데이터 표현들의 열 3) 각각을 저장할 수 있다. 고유 인덱스는 그 가중치에 의해 표현되는 연결을 갖는 인공 뉴런들의 쌍을 고유하게 식별할 수 있다. 일 실시 예에서, 각각의 클러스터 간 가중치 또는 필터는: 가중치 또는 필터에 의해 연결된 제1 또는 \"from\" 클러스 터에서 뉴런 또는 채널을 식별하는 제1 인덱스 값, 가중치 또는 필터에 의해 연결된 제2 또는 \"to\" 클러스 터에서 뉴런 또는 채널을 식별하는 제2 인덱스 값, 및 클러스터 간 가중치 또는 필터의 값을 정의하는 트리 플릿으로 표현될 수 있다. 가중치들 또는 필터들을 독립적으로 인덱싱함으로써, 메모리는 비-제로 가중치 들 또는 필터들을 갖는 연결들에 대한 엔트리들만을 저장할 수 있다(예컨대, 단절들에 대한 엔트리들을 삭제 또 는 생략하거나 제로 가중치들 또는 필터들과 연관된 연결들이 없음). 클러스터 연결 신경망을 저장하기 위한 메 모리 사용량은 완전 연결 신경망과 비교하여 X% 희소성 및 각각의 가중치 또는 필터 엔트리의 크기의 2배 에 대해, 조밀한 신경망에 사용되는 메모리의 2x(100-X)%로 감소될 수 있다(예컨대, 99% 희소성 클러스터 연결 신경망은 조밀한 표현에 사용되는 메모리의 양의 2%, 즉 50배 더 적은 메모리 사용량만을 사용한다). 로컬 엔드 포인트 디바이스(들)는 각각, 메모리에 저장된 클러스터 연결 신경망의 가중치들 또는 필터들에 기반 하여 예측을 훈련 및/또는 실행하기 위한 하나 이상의 프로세서(들)를 포함할 수 있다. 예측 동안, 클러스 터 연결 신경망은 순방향으로 한 번 진행된다. 훈련 중에, 클러스터 연결 신경망은 두 번 실행되는데, 출력을 생성하기 위해 순방향으로 한 번, 그리고 오류 정정(예컨대, 역전파)을 위해 역방향으로 한 번 진행된다. 클러 스터 연결 신경망이 실행될 때마다, 클러스터 연결 신경망의 가중치들의 수의 감소에 비례하여 계산들의 수가 감소되고 속도가 증가된다. X% 희소성을 갖는 클러스터 연결 신경망의 경우, 프로세서(들)는 (X% 더 적은 계산들로) 신경망을 100/(100-X)배 더 빠르게 실행할 수 있다. 클러스터 연결 신경망이 희소 클러스터 간 연결 들로 또는 희소 클러스터 간 연결들 없이 초기화될 때, 가속은 순간적이다. 반면, 클러스터 연결 신경망이 조밀 또는 완전 연결 신경망으로서 초기화된 다음 프루닝될 때, 100/(100-X))의 최대 가속이 달성될 때까지, 시간 경 과에 따라 가속이 증가한다. 로컬 엔드포인트 디바이스(들)는 스마트 디바이스들, 개인용 컴퓨터, 데스크톱 컴퓨터, 모바일 컴퓨터, 랩 톱 컴퓨터 및 노트북 컴퓨터, 또는 다른 적절한 디바이스, 이를테면 셀룰러 전화, 개인용 디지털 보조기기(PDA: personal digital assistant), 비디오 게임 콘솔 등을 포함할 수 있으며, 유선 또는 무선 연결들 또는 모뎀들을 포함할 수 있다. 로컬 엔드포인트 디바이스(들)는 사용자로부터의 입력(예컨대, 신경망 파라미터들, 이를 테면 뉴런들, 시냅스들 및 계층들의 수들, 크기들, 치수들 및 구성들, 정확도 또는 훈련 임계치들 등)을 수신하 기 위한 하나 이상의 입력 디바이스(들)를 포함할 수 있다. 로컬 엔드포인트 디바이스(들)는 컴퓨터 또는 원격 서버에 의해 생성된 데이터를 사용자에게 디스플레이하기 위한 하나 이상의 출력 디바이 스(들)(예컨대, 모니터 또는 화면)를 포함할 수 있다. 다양한 애플리케이션들에서, 로컬 엔드포인트 디바 이스(들)는 이미지 인식, 컴퓨터 비전, 가상 또는 증강 현실, 음성 인식, 텍스트 이해, 또는 딥 러닝의 다 른 애플리케이션들을 위한 시스템의 일부이다. 얼굴 인식의 애플리케이션에서, 디바이스는 희소 신경망을 사용 하여 얼굴 인식을 효율적으로 수행하여, 일치가 검출되면 디바이스가 자신을 또는 물리적 문을 잠금 해제하도록 트리거할 수 있다. 보안 애플리케이션에서는, 보안 카메라 시스템이 희소 신경망을 사용하여 보안 위반을 효율 적으로 검출하고 경보 또는 다른 보안 조치를 울릴 수 있다. 자율 주행 애플리케이션에서는, 차량 컴퓨터가 희 소 신경망을 사용하여, 예컨대 검출된 객체를 피하게 멀리 조향하도록 운전 동작들을 제어할 수 있다. 로컬 엔드포인트 디바이스(들)와 원격 서버를 연결하는 네트워크는 인터넷과 같은 임의의 공공"}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "또는 사설 네트워크일 수 있다. 네트워크에 대한 액세스는 유선, 지상파 무선, 위성 또는 당해 기술분야에 잘 알려진 다른 시스템들을 통해 이루어질 수 있다. 로컬 엔드포인트 디바이스(들) 및 원격 서버는 본 발명의 실시예들에 따른 동작들을 실행하기 위한, 각각 하나 이상의 제어기(들) 또는 프로세서(들)(556, 516), 그리고 프로세서(들)에 의해 실행 가능한 데이터 및/또는 명령들(예컨대, 본 발명의 실시예들에 따른 방법을 적용하기 위한 소프트웨어)를 저장하기 위한, 각각 하나 이상의 메모리 유닛(들)(558, 515)을 포함할 수 있다. 프로세서(들)(556, 516)는 예를 들어, 중앙 처 리 유닛(CPU: central processing unit), 그래픽 처리 유닛(GPU: graphical processing unit), 필드 프로그래 밍 가능 게이트 어레이(FPGA: field-programmable gate array), 주문형 집적 회로(ASIC: application-specific integrated circuit), 디지털 신호 프로세서(DSP: digital signal processor), 마이크로프로세서, 제어기, 칩, 마이크로 칩, 집적 회로(IC: integrated circuit) 또는 임의의 다른 적합한 다목적 또는 특정 프로세서 또는 제 어기를 포함할 수 있다. 메모리 유닛(들)(558, 515)은 예를 들어, 랜덤 액세스 메모리(RAM: random access memory), 동적 RAM(DRAM: dynamic RAM), 플래시 메모리, 휘발성 메모리, 비휘발성 메모리, 캐시 메모리, 버퍼,단기 메모리 유닛, 장기 메모리 유닛, 또는 다른 적절한 메모리 유닛들 또는 저장 유닛들을 포함할 수 있다. 다른 디바이스들 및 구성들이 사용될 수 있는데, 예를 들어 데이터는 메모리에 저장될 수 있으며, 별 도의 서버가 사용되지 않을 수 있다. 본 발명의 일부 실시예들에 따른, 클러스터 연결 신경망을 사용한 훈련 및 예측을 위한 방법의 흐름도인 도 6이 참조된다. 도 6의 동작들은 메모리(예컨대, 도 5의 하나 이상의 메모리 유닛(들)(515 및/또는 558))에 저장된 데이터를 사용하여 프로세서(예컨대, 도 5의 하나 이상의 프로세서(들)(516 및/또는 556))에 의해 실행될 수 있 다. 동작에서, 프로세서가 메모리에서 초기 신경망을 생성 또는 수신할 수 있다. 초기 신경망은, 후속적으로 프루닝되는 조밀 또는 완전 연결 클러스터 간 가중치들로 시작할 수 있거나, 추가되는 희소 클러스터 간 가중치 들로 또는 클러스터 간 가중치들 없이 시작할 수 있다. 동작에서, 프로세서가 초기 신경망을 복수의 클러스터들로 분할할 수 있다. 프로세서는 클러스터 분할된 신경망을 저장할 수 있으며, 여기서 각각의 클러스터는 상이한 복수의 인공 뉴런들 또는 컨볼루션 채널들을 포 함할 수 있고, 뉴런들 또는 채널들의 복수의 쌍들 각각은 가중치 또는 컨볼루션 필터에 의해 고유하게 연결된다. 동작에서, 프로세서는 클러스터 연결 신경망의 각각의 클러스터 내에서 클러스터 내 가중치들 또는 필터들 의 국소적으로 조밀한 하위 네트워크를 갖는 클러스터 연결 신경망을 생성, 훈련 또는 수신할 수 있으며, 여기 서 동일한 클러스터 내의 뉴런들 또는 채널들의 쌍들의 대부분은 (비-제로) 클러스터 내 가중치들 또는 필터들 에 의해 연결된다. 각각의 클러스터 내의 뉴런들 또는 채널들의 연결된 대부분의 쌍들은 클러스터 연결 신경망 을 사용한 훈련 또는 예측 동안 활성화 블록으로서 함께 공동 활성화될 수 있다(예컨대, 신경망의 동일한 패스 (pass) 또는 실행에서 모두 활성화됨). 각각의 클러스터 내의 뉴런들 또는 채널들은 완전히 연결되거나 부분적 으로 연결될 수 있다. 동작에서, 프로세서는 클러스터 연결 신경망의 각각의 클러스터 외부에서(또는 상이한 클러스터들 사이에 서) 클러스터 간 가중치들 또는 필터들의 전역적 희소 네트워크를 갖는 클러스터 연결 신경망을 생성, 훈련 또 는 수신할 수 있으며, 여기서 상이한 클러스터들에 걸쳐 클러스터 경계에 의해 분리된 뉴런들 또는 채널들의 소 수의 쌍들이 클러스터 간 가중치들 또는 필터들에 의해 연결된다. 클러스터 간 뉴런들 또는 채널들의 나머지 대 부분의 단절된 쌍들 각각의 뉴런들 또는 채널들은, 각각의 그러한 뉴런 또는 채널 쌍이 연결되지 않기 때문에, 훈련 또는 예측 동안 함께 공동 활성화되지 않는다. 동작의 초기 신경망이 조밀하게 또는 완전히 연결된 클러스터 간 가중치들 또는 필터들을 가질 때, 프로세 서는 클러스터 간 가중치들 또는 필터들의 대부분을 프루닝함으로써 클러스터 간 가중치들 또는 필터들의 전역 적 희소 네트워크를 훈련시킬 수 있다. 동작의 초기 신경망이 클러스터 간 가중치들 또는 필터들을 갖지 않거나 희소한 경우, 프로세서는 소수의 가능한 클러스터 간 가중치들 또는 필터들을 추가하거나 재배열함으로 써 클러스터 간 가중치들 또는 필터들의 전역적 희소 네트워크를 훈련시킬 수 있다. 프로세서는 신경망의 훈련 단계 동안 그리고/또는 이후에 기존의 클러스터 간 가중치들을 프루닝하거나 새로운 클러스터 간 가중치들을 추 가할 수 있다. 프로세서는, 클러스터 내 가중치들에 유리하게 편향시키고, 클러스터 간 가중치들에 대해 편향시 킴으로써 훈련 단계 동안 클러스터 간 가중치들을 프루닝할 수 있다. 프로세서는 L1 정규화, Lp 정규화, 임계화, 랜덤 제로화 및 편향 기반 프루닝을 사용하여 클러스터 간 가중치들을 프루닝할 수 있다. 일부 실시예들에서, 프로세서는 클러스터 연결 신경망의 가중치들 또는 필터들의 강도가 필터들의 가중치들에 의해 연결된 뉴런들 또는 채널들 사이의 거리에 반비례하여 편향되도록, 클러스터 연결 신경망을 훈련시킬 수 있다. 프로세서는 랜 덤하게, 확률적으로 그리고/또는 휴리스틱하게 가중치들을 프루닝할 수 있다. 프로세서는 연결 생성에 의해 클 러스터 연결 신경망에서 하나 이상의 새로운 클러스터 간 가중치들을 추가할 수 있다. 새로운 가중치들은 랜덤 하게, 확률적으로 그리고/또는 휴리스틱하게 생성될 수 있다. 일부 실시예들에서, 클러스터 연결 신경망은 진화 계산(유전 알고리즘들 또는 유전 프로그래밍)을 사용하여 또는 강화 학습을 사용하여 진화될 수 있다. 프로세서는, 테스트로부터 야기되는 고도로 링크된 뉴런들 또는 채널들의 활성화 패턴들과 가장 근접하게 닮은 최적의 클러스터 형상을 결정하기 위해 클러스터 연결 신경망에서 뉴런 또는 채널 활성화 패턴들을 테스트할 수 있다. 프로세서는 훈련 동안 활성화 패턴들이 변함에 따라 최적의 클러스터 형상을 동적으로 조정할 수 있다. 다양한 실시예들에서, 복수의 클러스터들 중 하나 이상의 클러스터의 클러스터 경계는 열(N×1 또는 N×M 차 원), 행(1×N 또는 M×N 차원), 원형, 다각형, 불규칙한 형상, 직사각형 프리즘, 원통, 구, 다면체, 및/또는 임의의 2차원, 3차원 또는 N차원 형상의 형상을 가질 수 있다. 상이한 형상들의 조합들이 사용될 수 있다. 일부 실시예들에서, 클러스터 연결 신경망은 클러스터 연결 구역들과 표준 비-클러스터 연결 구역들의 하이브리드이 다. 일부 실시예들에서, 클러스터 간 연결들은 경계 뉴런들만을 연결할 수 있지만 내부 뉴런들은 연결할 수 없 다. 예를 들어, 하나의 클러스터 내의 경계 뉴런들 또는 채널들은 클러스터 간 가중치들 또는 필터들에 의해 하 나 이상의 상이한 클러스터들 내의 경계 뉴런들 또는 채널들에 연결되는 반면, 보딩(board)된 클러스터로부터 이격된 내부 뉴런들 또는 채널들은 단지 클러스터 내 가중치들 또는 필터들에 의해 동일한 클러스터 내의 다른 뉴런들 또는 채널들에 연결된다. 클러스터 연결 신경망들의 예들은 도 1 및 도 2를 참조하여 설명된다. 본 발명의 실시예에 따라 다양한 인덱싱 방법들이 사용될 수 있다. 클러스터 연결 신경망의 클러스터 간 가중치 들 또는 필터들의 값들은 압축된 희소 행(CSR) 표현, 압축된 희소 열(CSC) 표현, 희소 텐서 표현, 맵 표현, 리 스트 표현 및/또는 희소 벡터 표현, 임의의 다른 희소 행렬 또는 신경망 표현을 사용하여 저장될 수 있다. 일부 실시예들에서, 메모리는 고유 클러스터 인덱스에 대한 연관성을 갖는 클러스터 연결 신경망의 각각의 채널에 클 러스터 내 가중치들 또는 필터들을 저장할 수 있고, 클러스터 내의 클러스터 내 가중치들을 이들의 행렬 포지션 들로 표현하는 클러스터 특정 행렬을 사용할 수 있다. 일부 실시예들에서, 메모리는 고유 인덱스에 대한 연관성 을 갖는 클러스터 연결 신경망의 복수의 클러스터 간 가중치들 또는 필터들 각각을 저장할 수 있다. 고유 인덱 스는 클러스터 간 가중치 또는 필터에 의해 표현되는 연결을 갖는 인공 뉴런들 또는 채널들의 쌍을 고유하게 식 별할 수 있으며, 여기서 상이한 클러스터들 내의 뉴런들 또는 채널들의 쌍들 사이의 연결들을 표현하는 비-제로 클러스터 간 가중치들 또는 필터들만이 저장되고, 뉴런들 또는 채널들의 쌍들 사이의 연결들을 표현하지 않는 제로 클러스터 간 가중치들 또는 필터들은 저장되지 않는다. 일부 실시예들에서, 메모리는 예컨대, 도 4에 도시 된 바와 같이, 각각의 클러스터 간 가중치 또는 필터를 식별하는 값들의 트리플릿을 저장할 수 있는데, 이러한 값들은: 제1 클러스터 내의 쌍의 제1 뉴런 또는 채널을 식별하는 인덱스의 제1 값(예컨대, 도 4, 열 1), 상이한 제2 클러스터 내의 쌍의 제2 뉴런 또는 채널을 식별하는 인덱스의 제2 값(예컨대, 도 4, 열 2), 및 클러스터 간 가중치 또는 필터의 값(예컨대, 도 4, 열 3)을 포함한다. 동작에서, 프로세서는 예측을 위해 동작들(600-606)에서 생성, 훈련 또는 수신된 클러스터 연결 신경망을 실행할 수 있다. 예측 모드에서, 프로세서는 메모리로부터 리트리브하고, 동작들(604, 606)에서 클러스터 연결 신경망의 비-제로 가중치들인 클러스터 간 가중치들 또는 필터들 중 소수에만 기반하여(그리고 제로 클러스터 간 가중치들 또는 필터들에는 기반하지 않고) 출력을 계산하도록 구성된 클러스터 연결 신경망을 실행할 수 있 다. 예측하기 위해, 프로세서는 클러스터 연결 신경망의 입력 계층에 소스 데이터를 입력하고, 각각의 계층의 뉴런들을 후속 계층들에 연결하는 비-제로 가중치들만으로 해당 계층의 데이터에 대해 반복적으로 동작함으로써 희소 신경망의 복수의 뉴런 또는 채널 계층들을 통해 데이터를 전파하며, 클러스터 연결 신경망의 최종 계층의 결과를 출력할 수 있다. 어떤 실시예에서, 순방향 훈련 또는 예측 패스 동안, 프로세서는 클러스터 연결 신경망에서 비-제로 클러스터 간 가중치들 또는 필터들의 희소 분포와 연관된 인덱스들의 비-순차적 패턴에 따라 메인 메모리의 비-순차적 위 치들에 저장되는 클러스터 간 가중치들 또는 필터들을 메인 메모리로부터 페치(fetch)할 수 있다. 그러한 클러 스터 간 가중치들 또는 필터들은 메인 메모리 내의 비-순차적 위치들로부터 페치된 후에, 이들은 로컬 또는 캐 시 메모리의 순차적인 메모리 위치들에 저장될 수 있다. 다른 동작들 또는 동작들의 다른 순서들이 사용될 수 있다. 예를 들어, 동작에서 초기(비-클러스터 연결) 신경망으로 시작하고 클러스터 연결 신경망을 훈련시키는 대신에, 일부 실시예들은 완전히 훈련된 클러스터 연 결 신경망을 수신하고, 동작들(600-608)을 스킵하고, 동작에서 프로세스를 시작하여 클러스터 연결 신경망 을 사용한 예측을 수행할 수 있다. 추가로, 동작(각각의 클러스터 내부에서의 훈련) 및 동작(각각의 클러스터 외부에서의 훈련)은 종종 동일한 훈련 프로세스의 일부이며, 동일한 동작의 일부로서 동시에 실행된다. 일부 실시예들에서, 클러스터들 내부에서는 어떠한 훈련도 없을 수 있는데, 예컨대 각각의 클러스터 내부에는 완전 연결 네트워크가 있으므로, 동작은 스킵될 수 있다. 결과들: 본 발명의 실시예들을 몇몇 딥 러닝 벤치마크들에 적용하는 것은, 원래 정확도의 99% 초과를 유지하면 서, 신경망 내의 가중치들의 수의 90-99%의 감소를 야기하였다. 이는 (예측 모드 동안, 뿐만 아니라 네트워크가 훈련의 각각의 반복에서 더 희소화됨에 따른 훈련 모드 동안에도) 신경망에 대한 계산 속도의 10배 내지 100배 가속, 및 메모리 사용량의 5배 내지 50배 감소에 대응한다. 따라서 딥 러닝 네트워크들은, 지금까지는 가능하지 않았던 어떤 것인, 특히 클라우드 또는 네트워크 측 서버들 (예컨대, 도 5의 원격 서버)의 하드웨어만이 아니라, 최소량의 CPU 능력 및 메모리 이용 가능성을 갖는 디바이스들(예컨대, 도 5의 로컬 엔드포인트 디바이스(들))에 대해 효율적으로 실행될 수 있다. 추가로, 가 중치들의 콤팩트(예컨대, 트리플릿) 표현은 처리 속도를 더 높이기 위해 임의의 하드웨어(CPU, GPU 등) 상에서 쉽게 병렬화될 수 있다. 희소 신경망들에 대한 콤팩트(예컨대, 트리플릿) 표현을 사용하여, 본 발명의 실시예들은 클러스터 연결 신경망 들을 진화시키기에 충분한 효율을 제공할 수 있다. 클러스터 연결 컨볼루션 NN의 훈련 및 예측을 가속하기 위해, (예컨대, 통상적으로 비교적 느리고 복잡한) 컨볼 루션 동작은 재배열되고 복제된 항들에 대해 실행되는 (예컨대, 통상적으로 컨볼루션 연산들보다 비교적 더 빠 르고 덜 복잡한) 행렬 곱셈 동작에 의해 동등하게 수행될 수 있다. 이 변환은 \"img2col\" 함수로 지칭된다. 일부 실시예들은 희소 CNN에 적응된 새롭고 더 콤팩트한 img2col 함수를 제공한다. 정규 img2col 함수에서, 각각의 행 및 열 곱셈이 컨볼루션 연산을 표현하도록, 계층에 의해 수행되는 모든 각각의 컨볼루션 연산을 표현하도록 2개의 커스텀 행렬들이 구성된다. 본 발명의 실시예들은 수정된 img2col 함수를 제공할 수 있으며, 여기서 커널 들 중 일부는 제로 아웃(zero out)되고, 연관된 행렬들은 이러한 행들 및 열들을 생략하거나 삭제하도록 수정될 수 있다. 이는 표준 img2col 연산들과 비교하여 동일한 컨볼루션 결과들을 달성하기 위해 더 적은 곱셈 연산들 과 연관된 더 콤팩트 한 행렬들을 야기한다. 신경망들의 뉴런들 및 가중치들에 관련된 본 발명의 실시예들은 컨볼루션 신경망들의 채널들 및 필터들에 각각 적용될 수 있다. 본 발명의 실시예가 클러스터 간 가중치들에 대한 희소 인덱싱을 설명하지만, 동일한 희소 인덱싱이 추가적으로 또는 대안으로 클러스터 내 가중치들에 적용될 수 있다. 대안으로, 희소 인덱싱이 사용되지 않을 수 있다. 앞서 말한 설명에서는, 본 발명의 다양한 양상들이 설명된다. 설명을 목적으로, 본 발명의 전반적인 이해를 제 공하기 위해 특정 구성들 및 세부사항들이 제시된다. 그러나 본 발명은 본 명세서에 제시된 특정 세부사항들 없"}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "이 실시될 수 있음이 당해 기술분야에서 통상의 지식을 가진 자들에게 또한 명백할 것이다. 더욱이, 잘 알려진 특징들은 본 발명을 모호하게 하지 않기 위해 생략되거나 단순화될 수 있다. 구체적으로 달리 언급되지 않는 한, 하기 논의들로부터 명백하듯이, 본 명세서 전반에서 \"처리,\" \"계산,\" \"산출,\" \"결정\" 등과 같은 용어들을 이용하는 논의들은 컴퓨팅 시스템의 레지스터들 및/또는 메모리들 내에서 물리적, 이를테면 전자적 양들로서 표현되는 데이터를 조작하고 그리고/또는 이를 컴퓨팅 시스템의 메모리들, 레지스터들 또는 다른 그러한 정보 저장, 전송 또는 디스플레이 디바이스들 내에서 물리량들로서 유사하게 표현 되는 다른 데이터로 변환하는 컴퓨터 또는 컴퓨팅 시스템, 또는 유사한 전자 컴퓨팅 디바이스의 동작 및/또는 프로세스들을 의미한다고 인식된다. 앞서 언급한 흐름도 및 블록도는 본 발명의 다양한 실시예들에 따른 시스템들 및 방법들의 가능한 구현들의 아 키텍처, 기능 및 동작을 예시한다. 이와 관련하여, 흐름도 또는 블록도들 내의 각각의 블록은 지정된 로직 기능 (들)을 구현하기 위한 하나 이상의 실행 가능 명령들을 포함할 수 있는 코드의 일부, 모듈 또는 세그먼트를 나 타낼 수 있다. 일부 대안적인 구현들에서는, 블록에서 언급된 기능들이 도면들에서 언급된 순서와 다르게 또는 다른 모듈들에 의해 발생할 수 있다. 명시적으로 언급되지 않는 한, 본 명세서에서 설명되는 방법 실시예들은 특정 순서 또는 시퀀스로 제한되지 않는다. 추가로, 설명되는 방법 실시예들 또는 그 엘리먼트들 중 일부는 동 일한 시점에 발생하거나 수행될 수 있다. 블록도들 및/또는 흐름도 예시의 각각의 블록, 및 블록도들 및/또는 흐름도 예시의 블록들의 조합들은 지정된 기능들 또는 동작들을 수행하는 특수 목적 하드웨어 기반 시스템들, 또는 특수 목적 하드웨어 및 컴퓨터 명령들의 조합들에 의해 구현될 수 있다. 본 발명의 실시예들은 프로세서 또는 제어기(예컨대, 도 5의 프로세서(516 또는 556))에 의해 실행될 때, 본 명 세서에 개시된 방법들을 실행하는 명령들, 예컨대 컴퓨터 실행 가능 명령들을 인코딩, 포함 또는 저장하는, 예 를 들어 메모리(예컨대, 도 5의 메모리 유닛들(515 또는 558)), 디스크 드라이브 또는 USB 플래시 메모리와 같 은 컴퓨터 또는 프로세서 비-일시적 저장 매체, 또는 비-일시적 컴퓨터 또는 프로세서 판독 가능 매체와 같은 물품을 포함할 수 있다. 위의 설명에서, 실시예는 본 발명들의 일례 또는 구현이다. \"일 실시예,\" \"한 실시예\" 또는 \"일부 실시예들\"의 다양한 출현들은 반드시 모두 동일한 실시예들을 의미하는 것은 아니다. 본 발명의 다양한 특징들이 단일 실시 예와 관련하여 설명될 수 있지만, 실시예들의 특징들은 또한 개별적으로 또는 임의의 적절한 조합으로 제공될 수 있다. 반대로, 본 발명은 본 명세서에서 명확성을 위해 개별 실시예들과 관련하여 설명될 수 있지만, 본 발 명은 또한 단일 실시예로 구현될 수 있다. 본 명세서에서 \"일부 실시예들\", \"한 실시예\", \"일 실시예\" 또는 \"다른 실시예들\"에 대한 언급은 실시예들과 관련하여 설명된 특정한 특징, 구조 또는 특성이 본 발명들의 반드시 모든 실시예들이 아닌, 적어도 일부 실시예들에 포함되는 것을 의미한다. 위에서 설명한 본 발명의 양상들은 본 발명의 실시예들에서 조합되거나 아니면 공존할 수 있음이 추가로 인식될 것이다. 청구항들 및 명세서에 제시된 설명들, 예들, 방법들 및 재료들은 한정으로서가 아니라 오히려 단지 예시로서 해"}
{"patent_id": "10-2021-0154639", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "석되어야 한다. 본 발명의 특정 특징들이 본 명세서에서 예시 및 설명되었지만, 당해 기술분야에서 통상의 지식 을 가진 자들에게 많은 수정들, 대체들, 변경들 및 등가물들이 발생할 수 있다. 따라서 첨부된 청구항들은 본 발명의 진의 내에 속하는 것으로서 그러한 모든 수정들 및 변경들을 커버하는 것으로 의도된다고 이해되어야 한 다. 본 발명은 한정된 수의 실시예들에 대해 설명되었지만, 이들은 본 발명의 범위에 대한 제한들로 해석되는 것이 아니라, 오히려 바람직한 실시예들 중 일부의 예시들로서 해석되어야 한다. 다른 가능한 변형들, 수정들 및 애 플리케이션이 또한 본 발명의 범위 내에 있다. 다른 실시예들이 본 명세서에 개시된다. 특정 실시예들의 특징들 은 다른 실시예들의 특징들과 조합될 수 있으며; 따라서 특정 실시예들은 다수의 실시예들의 특징들의 조합들일 수 있다."}
{"patent_id": "10-2021-0154639", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 발명으로서 간주되는 청구 대상은 본 명세서의 결론 부분에서 특별히 지적되고 명백하게 청구된다. 그러나 본 발명은 발명의 목적들, 특징들 및 이점들과 함께 동작의 구성 및 방법 모두에 대해 첨부 도면들과 함께 일독 시 다음의 상세한 설명을 참조하여 가장 잘 이해될 수 있다: 도 1은 본 발명의 일부 실시예들에 따른 클러스터 연결 신경망의 개략적인 예시이다.도 2는 본 발명의 일부 실시예들에 따른 클러스터 연결 컨볼루션 신경망의 개략적인 예시이다. 도 3은 본 발명의 일부 실시예들에 따른 클러스터 연결 신경망의 개략적인 예시이고, 도 4는 도 3의 네트워크에 서 클러스터 간 가중치들을 저장하기 위한 데이터 구조이다. 도 5는 본 발명의 일부 실시예들에 따른, 클러스터 연결 신경망을 사용한 훈련 및 예측을 위한 시스템의 개략적 인 예시이다. 도 6은 본 발명의 일부 실시예들에 따른, 클러스터 연결 신경망을 사용한 훈련 및 예측을 위한 방법의 흐름도이 다. 예시의 단순성 및 명확성을 위해, 도면들에 도시된 엘리먼트들은 반드시 실척대로 그려진 것은 아니라고 인식될 것이다. 예를 들어, 엘리먼트들 중 일부 엘리먼트들의 치수들은 명확성을 위해 다른 엘리먼트들에 비해 과장될 수 있다. 또한, 적절한 것으로 여겨지는 경우, 도면들 간에 참조 번호들이 반복되어 대응하는 또는 유사한 엘리 먼트들을 지시할 수 있다."}
