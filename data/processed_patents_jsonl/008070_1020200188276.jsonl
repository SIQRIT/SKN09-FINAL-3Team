{"patent_id": "10-2020-0188276", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0096115", "출원번호": "10-2020-0188276", "발명의 명칭": "전자 장치 및 그의 제어 방법", "출원인": "삼성전자주식회사", "발명자": "셰르비나 아르템"}}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "AR(Augmented Reality) 컨텐츠를 제공하는 전자 장치에 있어서,디스플레이;카메라; 및상기 디스플레이에 AR 컨텐츠를 표시하고, 상기 카메라를 통해 획득된 이미지 프레임들에서 사용자의 손을 검출하고, 상기 손의 사이즈에 기초하여 상기 AR 컨텐츠에 대한 상기 손의 인터렉션(interaction)을 식별하는 프로세서;를 포함하며, 상기 손의 사이즈는,상기 디스플레이를 통해 제공되는 오브젝트에 대한 상기 손의 인터렉션이 발생되면, 상기 오브젝트에 대한 정보에 기초하여 획득되는 전자 장치."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 오브젝트는, 상기 카메라를 통해 획득된 이미지 프레임에 포함된 제1 타입의 오브젝트 및 상기 디스플레이에 표시되는 AR 컨텐츠에 포함된 제2 타입의 오브젝트 중 적어도 하나를 포함하는 전자 장치."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 프로세서는,상기 검출된 손의 사이즈를 기설정된 값으로 설정하고, 상기 설정된 사이즈에 기초하여 상기 디스플레이를 통해제공되는 오브젝트에 대한 상기 손의 인터렉션이 발생되었는지를 식별하고, 상기 인터렉션이 발생된 것으로 식별되면, 상기 오브젝트에 대한 정보에 기초하여 상기 손의 사이즈를 식별하고, 상기 식별된 손의 사이즈에 기초하여 상기 AR 컨텐츠에 대한 상기 손의 인터렉션을 식별하는 전자 장치."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,레퍼런스 오브젝트의 특징 정보 및 사이즈 정보가 저장된 메모리;를 더 포함하고,상기 프로세서는,상기 카메라를 통해 획득된 이미지 프레임들에서 제1 타입의 오브젝트 및 사용자의 손이 검출되면, 상기 메모리에 저장된 특징 정보 및 상기 검출된 제1 타입의 오브젝트의 특징 정보에 기초하여 상기 제1 타입의 오브젝트가상기 레퍼런스 오브젝트인지를 식별하고, 상기 제1 타입의 오브젝트가 상기 레퍼런스 오브젝트인 것으로 식별되면, 상기 레퍼런스 오브젝트의 사이즈에 기초하여 상기 손의 사이즈를 식별하는 전자 장치."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 프로세서는,상기 메모리에 저장된 특징 정보 및 상기 검출된 제1 타입의 오브젝트의 특징 정보에 기초하여 상기 제1 타입의오브젝트가 상기 레퍼런스 오브젝트에 해당하지 않은 것으로 식별되면, 상기 카메라를 통해 획득된 연속된 이미공개특허 10-2022-0096115-3-지 프레임들을 이용하여 상기 연속된 이미지 프레임들에 포함된 상기 제1 타입의 오브젝트의 사이즈를식별하고, 상기 제1 타입의 오브젝트의 사이즈에 기초하여 상기 손의 사이즈를 식별하는 전자 장치."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제2항에 있어서,상기 프로세서는,상기 검출된 손의 사이즈를 기설정된 값으로 설정하고, 상기 설정된 사이즈에 기초하여 상기 디스플레이를 통해제공되는 제2 타입의 오브젝트에 대한 상기 손의 인터렉션이 발생되었는지를 식별하고, 상기 손의 인터렉션이식별된 제2 타입의 오브젝트의 뎁스에 기초하여 상기 손의 사이즈를 식별하는 전자 장치."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "AR(Augmented Reality) 컨텐츠를 제공하는 전자 장치의 제어 방법에 있어서,디스플레이에 AR 컨텐츠를 표시하는 단계; 카메라를 통해 획득된 이미지 프레임들에서 사용자의 손을 검출하는 단계; 상기 손의 사이즈에 기초하여 상기 AR 컨텐츠에 대한 상기 손의 인터렉션을 식별하는 단계; 를 포함하며, 상기 손의 사이즈는,상기 디스플레이를 통해 제공되는 오브젝트에 대한 상기 손의 인터렉션이 발생되면, 상기 오브젝트에 대한 정보에 기초하여 획득되는 제어 방법."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 오브젝트는, 상기 카메라를 통해 획득된 이미지 프레임에 포함된 제1 타입의 오브젝트 및 상기 디스플레이에 표시되는 AR 컨텐츠에 포함된 제2 타입의 오브젝트 중 적어도 하나를 포함하는 제어 방법."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 검출된 손의 사이즈를 기설정된 값으로 설정하는 단계; 상기 설정된 사이즈에 기초하여 상기 디스플레이를 통해 제공되는 오브젝트에 대한 상기 손의 인터렉션이 발생되었는지를 식별하는 단계; 및상기 인터렉션이 발생된 것으로 식별되면, 상기 오브젝트에 대한 정보에 기초하여 상기 손의 사이즈를 식별하는단계;를 더 포함하는 제어 방법."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서,상기 카메라를 통해 획득된 이미지 프레임들에서 제1 타입의 오브젝트 및 사용자의 손이 검출되면, 상기 전자장치에 저장된 레퍼런스 오브젝트의 특징 정보 및 상기 검출된 제1 타입의 오브젝트의 특징 정보에 기초하여 상기 제1 타입의 오브젝트가 상기 레퍼런스 오브젝트인지를 식별하는 단계; 및상기 제1 타입의 오브젝트가 상기 레퍼런스 오브젝트인 것으로 식별되면, 상기 전자 장치에 저장된 상기 레퍼런스 오브젝트의 사이즈에 기초하여 상기 손의 사이즈를 식별하는 단계;를 더 포함하는 제어 방법."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 전자 장치에 저장된 특징 정보 및 상기 검출된 제1 타입의 오브젝트의 특징 정보에 기초하여 상기 제1 타공개특허 10-2022-0096115-4-입의 오브젝트가 상기 레퍼런스 오브젝트에 해당하지 않은 것으로 식별되면, 상기 카메라를 통해 획득된 연속된이미지 프레임들을 이용하여 상기 연속된 이미지 프레임들에 포함된 상기 제1 타입의 오브젝트의 사이즈를 식별하는 단계; 및 상기 식별된 제1 타입의 오브젝트의 사이즈에 기초하여 상기 손의 사이즈를 식별하는 단계;를 더 포함하는 제어방법."}
{"patent_id": "10-2020-0188276", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항에 있어서,상기 검출된 손의 사이즈를 기설정된 값으로 설정하는 단계; 상기 설정된 사이즈에 기초하여 상기 디스플레이를 통해 제공되는 제2 타입의 오브젝트에 대한 상기 손의 인터렉션이 발생되었는지를 식별하는 단계; 및 상기 손의 인터렉션이 식별된 제2 타입의 오브젝트의 뎁스에 기초하여 상기 손의 사이즈를 식별하는 단계;를 더포함하는 제어 방법."}
{"patent_id": "10-2020-0188276", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시에서는 전자 장치 및 그 제어 방법이 제공된다. 본 개시의 전자 장치는 AR(Augmented Reality) 컨텐츠를 제공하며, 디스플레이, (monocular) 카메라 및 디스플레이에 AR 컨텐츠를 표시하고, 카메라를 통해 획득된 이미 지 프레임들에서 사용자의 손을 검출하고, 손의 사이즈에 기초하여 AR 컨텐츠에 대한 손의 인터렉션을 식별하는 프로세서를 포함하며, 손의 사이즈는, 디스플레이를 통해 제공되는 오브젝트에 대한 손의 인터렉션이 발생되면, 오브젝트에 대한 정보에 기초하여 획득될 수 있다."}
{"patent_id": "10-2020-0188276", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 그의 제어 방법에 관한 것으로, 보다 상세하게는 카메라를 이용하여 오브젝트의 거리를 추정하는 전자 장치 및 그의 제어 방법에 관한 것이다."}
{"patent_id": "10-2020-0188276", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "전자 기술의 발달로 인해 증강현실(Augmented Reality; AR) 시장이 급속도로 성장하고 있다. AR 시장은 모바일 AR, 쇼핑에 대한 AR, 네비게이션에 대한 AR, 엔터프라이즈용 AR 등의 SW(Software) 및 AR 앱 개발과 스마트 글 래스, NPU(Neural Network Processing Unit), DSP(Digital signal processing), AI(Artificial Intelligence) 기술의 등장으로 AR 영역이 크게 확장되고 있는 HW(Hardware) 개발과 같은 2개의 서브 트렌드에 집중하고 있다. 최근 AR을 제공하는 웨어러블 장치(예: AR 글래스 등) 등과 같은 전자 장치의 사이즈는 소형화되고 무게가 경량 화되는 추세이며, 이러한 소형화 및 경량화에 따라 전자 장치에서 센서와 배터리가 장착되는 공간이 줄어들고 있다. 또한, 전자 장치의 사이즈 감소 및 예비 전력의 감소로 인해, 뎁스 센서(구조광, ToF 등)는 종래의 일반 적인 카메라(예: RGB 카메라)로 전환되고 있고, 또한 소형화 및 비용 절감 등의 이유로 인해 스테레오 비전 (stereo vision)에서 모노큘러 비전(monocular vision) 방식의 카메라와 같이 전환되고 있으며, 글로벌 셔터 (global shutter)에서 롤링 셔터(rolling shutter) 방식의 카메라로 전환되고 있다. 이와 같이 모노큘러 카메라 의 중요성이 점차 커지고 있다. 일반적으로 AR 환경에서 전자 장치가 3차원(3D) 공간 상에 렌더링된 컨텐츠 및 UI(User Interface) 요소 등의 가상의 오브젝트를 디스플레이에 표시하면, 사용자는 가상의 오브젝트와 상호 작용을 수행할 수 있다. 여기서 사용자가 가상의 오브젝트와 상호 작용하는 자연스러운 방법은 사용자 자신의 손을 움직이는 제스쳐를 이용하는 것이다. 그러나 모노큘러 카메라를 통해 획득되는 2차원 이미지만을 이용해 전자 장치 및 손(특히, 움직이는 손) 사이의 거리(또는 손의 위치)를 정확히 추정하기는 어렵다. 이는 사용자가 의도한 가상의 오브젝트와는 다른 가상의 오 브젝트와의 상호 작용(즉, 부정확한 상호 작용)을 야기하거나, 또는 상호 작용 자체가 수행되지 않는 경우가 발 생한다는 문제가 있다."}
{"patent_id": "10-2020-0188276", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 필요성에 의해 안출된 것으로, 본 개시의 목적은 카메라를 이용하여 보다 정확하게 오브젝트 와의 거리를 추정하는 전자 장치 및 그의 제어 방법을 제공함에 있다."}
{"patent_id": "10-2020-0188276", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한, 본 개시의 일 실시 예에 따른 전자 장치는 AR(Augmented Reality) 컨텐츠를 제공하 며, 디스플레이, 카메라 및 디스플레이에 AR 컨텐츠를 표시하고, 카메라를 통해 획득된 이미지 프레임들에서 사 용자의 손을 검출하고, 손의 사이즈에 기초하여 AR 컨텐츠에 대한 손의 인터렉션을 식별하는 프로세서를 포함하 며, 손의 사이즈는, 디스플레이를 통해 제공되는 오브젝트에 대한 손의 인터렉션이 발생되면, 오브젝트에 대한 정보에 기초하여 획득될 수 있다. 본 개시의 일 실시 예로서, 오브젝트는, 카메라를 통해 획득된 이미지 프레임에 포함된 제1 타입의 오브젝트 및 디스플레이에 표시되는 AR 컨텐츠에 포함된 제2 타입의 오브젝트 중 적어도 하나를 포함할 수 있다. 본 개시의 일 실시 예로서, 프로세서는, 검출된 손의 사이즈를 기설정된 값으로 설정하고, 설정된 사이즈에 기 초하여 디스플레이를 통해 제공되는 오브젝트에 대한 손의 인터렉션이 발생되었는지를 식별하고, 인터렉션이 발 생된 것으로 식별되면, 오브젝트에 대한 정보에 기초하여 손의 사이즈를 식별하고, 식별된 손의 사이즈에 기초 하여 AR 컨텐츠에 대한 손의 인터렉션을 식별할 수 있다. 본 개시의 일 실시 예로서, 레퍼런스 오브젝트의 특징 정보 및 사이즈 정보가 저장된 메모리를 더 포함하고, 프 로세서는, 카메라를 통해 획득된 이미지 프레임들에서 제1 타입의 오브젝트 및 사용자의 손이 검출되면, 메모리 에 저장된 특징 정보 및 검출된 제1 타입의 오브젝트의 특징 정보에 기초하여 제1 타입의 오브젝트가 레퍼런스 오브젝트인지를 식별하고, 제1 타입의 오브젝트가 레퍼런스 오브젝트인 것으로 식별되면, 레퍼런스 오브젝트의 사이즈에 기초하여 손의 사이즈를 식별할 수 있다. 본 개시의 일 실시 예로서, 프로세서는, 메모리에 저장된 특징 정보 및 검출된 제1 타입의 오브젝트의 특징 정 보에 기초하여 제1 타입의 오브젝트가 레퍼런스 오브젝트에 해당하지 않은 것으로 식별되면, 카메라를 통해 획 득된 연속된 이미지 프레임들을 이용하여 연속된 이미지 프레임들에 포함된 제1 타입의 오브젝트의 사이즈를 식 별하고, 제1 타입의 오브젝트의 사이즈에 기초하여 손의 사이즈를 식별할 수 있다. 본 개시의 일 실시 예로서, 프로세서는, 검출된 손의 사이즈를 기설정된 값으로 설정하고, 설정된 사이즈에 기 초하여 디스플레이를 통해 제공되는 제2 타입의 오브젝트에 대한 손의 인터렉션이 발생되었는지를 식별하고, 손 의 인터렉션이 식별된 제2 타입의 오브젝트의 뎁스에 기초하여 손의 사이즈를 식별할 수 있다. 한편, 본 개시의 일 실시 예에 따른 AR(Augmented Reality) 컨텐츠를 제공하는 전자 장치의 제어 방법은, 디스 플레이에 AR 컨텐츠를 표시하는 단계, 카메라를 통해 획득된 이미지 프레임들에서 사용자의 손을 검출하는 단계, 손의 사이즈에 기초하여 AR 컨텐츠에 대한 손의 인터렉션을 식별하는 단계를 포함하며, 손의 사이즈는 디 스플레이를 통해 제공되는 오브젝트에 대한 손의 인터렉션이 발생되면, 오브젝트에 대한 정보에 기초하여 획득 될 수 있다. 본 개시의 일 실시 예로서, 오브젝트는, 카메라를 통해 획득된 이미지 프레임에 포함된 제1 타입의 오브젝트 및 디스플레이에 표시되는 AR 컨텐츠에 포함된 제2 타입의 오브젝트 중 적어도 하나를 포함할 수 있다. 본 개시의 일 실시 예로서, 검출된 손의 사이즈를 기설정된 값으로 설정하는 단계, 설정된 사이즈에 기초하여 디스플레이를 통해 제공되는 오브젝트에 대한 손의 인터렉션이 발생되었는지를 식별하는 단계 및 인터렉션이 발 생된 것으로 식별되면, 오브젝트에 대한 정보에 기초하여 손의 사이즈를 식별하는 단계를 더 포함할 수 있다. 본 개시의 일 실시 예로서, 카메라를 통해 획득된 이미지 프레임들에서 제1 타입의 오브젝트 및 사용자의 손이 검출되면, 전자 장치에 저장된 레퍼런스 오브젝트의 특징 정보 및 검출된 제1 타입의 오브젝트의 특징 정보에 기초하여 제1 타입의 오브젝트가 레퍼런스 오브젝트인지를 식별하는 단계 및 제1 타입의 오브젝트가 레퍼런스 오브젝트인 것으로 식별되면, 전자 장치에 저장된 레퍼런스 오브젝트의 사이즈에 기초하여 손의 사이즈를 식별 하는 단계를 더 포함할 수 있다. 본 개시의 일 실시 예로서, 전자 장치에 저장된 특징 정보 및 검출된 제1 타입의 오브젝트의 특징 정보에 기초 하여 제1 타입의 오브젝트가 레퍼런스 오브젝트에 해당하지 않은 것으로 식별되면, 카메라를 통해 획득된 연속 된 이미지 프레임들을 이용하여 연속된 이미지 프레임들에 포함된 제1 타입의 오브젝트의 사이즈를 식별하는 단 계 및 식별된 제1 타입의 오브젝트의 사이즈에 기초하여 손의 사이즈를 식별하는 단계를 더 포함할 수 있다. 본 개시의 일 실시 예로서, 검출된 손의 사이즈를 기설정된 값으로 설정하는 단계, 설정된 사이즈에 기초하여 디스플레이를 통해 제공되는 제2 타입의 오브젝트에 대한 손의 인터렉션이 발생되었는지를 식별하는 단계 및 손의 인터렉션이 식별된 제2 타입의 오브젝트의 뎁스에 기초하여 손의 사이즈를 식별하는 단계를 더 포함할 수 있 다."}
{"patent_id": "10-2020-0188276", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상과 같은 본 개시의 다양한 실시 예에 따르면, 카메라를 이용하여 보다 정확하게 오브젝트와의 거리를 추정 하는 전자 장치 및 그의 제어 방법을 제공할 수 있다. 본 개시의 일 실시 예에 따르면, 사용자의 손의 사이즈를 정확히 추정할 수 있고, 사용자의 손에 대한 파라미터 를 정확하게 추정할 수 있다."}
{"patent_id": "10-2020-0188276", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시를 설명함에 있어서, 관련된 공지 기능 혹은 구성에 대한 구체적인 설명이 본 개시의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그에 대한 상세한 설명은 생략한다. 덧붙여, 하기 실시 예는 여러 가지 다른 형 태로 변형될 수 있으며, 본 개시의 기술적 사상의 범위가 하기 실시 예에 한정되는 것은 아니다. 오히려, 이들 실시 예는 본 개시를 더욱 충실하고 완전하게 하고, 당업자에게 본 개시의 기술적 사상을 완전하게 전달하기 위 하여 제공되는 것이다. 본 개시에 기재된 기술을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 개시의 실시 예의 다양한 변경 (modifications), 균등물(equivalents), 및/또는 대체물(alternatives)을 포함하는 것으로 이해되어야 한다.도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조 부호가 사용될 수 있다. 본 개시에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중 요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 상기 구성요소들을 한정하지 않는다. 본 개시에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\"등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 개시에서 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \" 포함하다\" 또는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들 을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요 소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요 소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제1 구성요소)가 다른 구성 요소(예: 제2 구성요소)에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 상기 어떤 구성요 소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제 3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 개시에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for),\" \"~하는 능력을 가지는(having the capacity to),\" \"~하도록 설계된(designed to),\" \"~하도 록 변경된(adapted to),\" \"~하도록 만들어진(made to),\" 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장치가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행하도록 구성된(또는 설정된) 프로세서\"는 상기 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메 모리 장치에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 상기 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 도 1a는 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 도면이다. 도 1a의 및 를 참조하면, 전자 장치는 AR(Augmented Reality) 컨텐츠를 제공할 수 있다. 전자 장치는 사용자가 착용할 수 있는 웨어러블 장치로 구현될 수 있다. 여기서, 웨어러블 장치는 액세서 리형(예: 시계, 반지, 팔찌, 발찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head-mounted- device(HMD)), 직물 또는 의류 일체형(예: 전자 의복), 신체 부착형, 또는 생체 이식형 회로 등과 같이 다양한 타입의 장치로 구현될 수 있다. 또한, 전자 장치는 스마트폰, 태블릿 PC(personal computer), 전자책 리더 기, 랩탑 PC, 네비게이션, 차량 등으로 구현될 수 있다. 한편, 전자 장치는 휠 수 있는 플렉서블 장치로 구현되는 것 또한 가능하다. AR 컨텐츠는 AR 환경에서 제공되는 컨텐츠를 나타낼 수 있다. AR 환경은 사용자의 주변 환경에 실제로 존재하지 않는 가상의 사물을 나타내는 AR 컨텐츠를 디스플레이를 통해 주변의 환경과 함께 제공함으로써 가상의 사물이 주변 환경에 존재하는 것처럼 보이도록 할 수 있다. 나아가, AR 컨텐츠는 정보(예: 실제로 존재하는 환경에 존 재하는 사물에 대한 부가 정보, 날씨 정보 등)를 제공하기 위한 컨텐츠일 수 있다. 또한, 도 1a의 을 참조하면, 전자 장치는 카메라를 통해 이미지 프레임을 획득할 수 있다. 여기에서, 카메라는 스테레오 카메라 및 뎁스 카메라 등과는 달리, 일반적인 카메라일 수 있다. 구체적으 로, 카메라는 모노큘러 카메라를 포함할 수 있다. 모노큘러 카메라는 2차원의 위치 정보(예: 수평 방향을 나타내는 x축 및 수직 방향을 나타내는 y축 상의 위치 정보)를 포함하는 이미지 프레임을 획득할 수 있는 카메 라를 포함할 수 있다. 여기서, 모노큘러 카메라는 2개의 모노큘러 카메라를 포함하는 스테레오 카메라, 또는 뎁 스 정보를 획득할 수 있는 뎁스 카메라 등에 비해 무게가 가볍고, 소형화가 가능하며, 비용이 절감되는 효과가 있다. 한편, 이미지 프레임에는 사용자의 손이 포함될 수 있다. 구체적으로, 카메라는 사용자의 손을 촬영 함으로써 사용자의 손이 포함된 이미지 프레임을 획득할 수 있다. 예를 들어, 도 1a의 를 참조하면, 전자 장치는 AR 컨텐츠를 디스플레이에 표시할 수 있다. 이때, 전자 장치는 가상의 3차원 공간 상의 영역(또는 위치)에 디스플레이의 영역(또는 픽셀)을 대응시키고, AR 컨텐츠를 3차원의 영역에 대응되는 디스플레이의 영역에 표시할 수 있다. 즉, 디스플레이 에 표시된 AR 컨텐츠는 3차원의 위치 정보를 포함할 수 있다. 이 경우, 전자 장치는 카메라를 통해 획득된 이미지 프레임에 포함된 사용자의 손(또는, 손의 포즈(pose))을 기반으로, 디스플레이에 표시된 AR 객체와의 인터렉션(interaction)을 식별할 수 있다. 예를 들어, 전자 장치는 카메라를 통해 획득된 이미지 프레임에 포함된 사용자의 손을 검출하고, 손의 위치 정보(또는, 손의 포즈(pose)에 따라 정의되는 손의 위치 정보)와 AR 객체의 위치 정보를 비 교하여, AR 객체에 대한 인터렉션이 발생되었는지를 식별할 수 있다. 다만, 이는 일 예일 뿐이고, 전자 장치 는 종래 다양한 방법을 이용하여 손과 AR 객체의 인터렉션을 식별할 수도 있다. 여기서, 인터렉션은 사용자의 손이 AR 객체와 접촉하는 동작, 사용자의 손이 AR 객체를 가리키는 동작, 사용자 의 손이 AR 객체와 근접하는 동작 중 적어도 하나를 나타낼 수 있다. 예를 들어, 전자 장치는 AR 객체가 표시된 위치(예를 들어, xyz 좌표)에 사용자의 손이 위치하거나, AR 객체가 표시된 위치(예를 들어, xyz 좌 표)와 사용자의 손 간의 거리가 기설정된 거리 미만일 경우 디스플레이에 표시된 AR 객체와의 인터렉션 이 발생한 것으로 판단할 수 있다. 또한, 사용자의 손(또는, 손의 포즈(pose))은 손에 대한 파라미터에 기초하여 정의될 수 있다. 여기에서, 도 1b를 참조하면, 손에 대한 파라미터는 3차원 공간 상에서 손(또는 손목, 손가락 등)의 위치 (position)를 나타내는 위치 정보(즉, x축, y축 및 z축 상의 위치)를 포함할 수 있다. 또한, 손에 대한 파라 미터는 사용자의 손목의 중심 축을 기준으로 손이 회전하는 방향 및 회전하는 정도 등을 나타내는 회전 (rotation) 정보(즉, pitch, yaw 및 roll에 대한 회전)를 포함할 수 있다. 이와 같이, 손에 대한 파라미터는 x축, y축 및 z축 상의 위치 정보 또는 pitch, yaw 및 roll의 회전 정보와 같은 3 개의 운동 방향을 나타내는 3자유도 파라미터(three degrees of freedom; 3DOF)를 포함할 수 있다. 또 는, 손에 대한 x축, y축 및 z축 상의 위치 정보 및 pitch, yaw 및 roll의 회전 정보와 같이 6 개의 운동 방 향을 나타내는 6자유도 파라미터(six degrees of freedom; 6DOF)를 포함할 수 있다. 다만, 이는 일 실시 예일 뿐 손에 대한 파라미터는 손가락 길이, 손가락 굽힘 각도 등과 같이 다양한 매개 변수를 의미할 수 있다. 한편, 이러한 손에 대한 파라미터 즉, 3DOF 또는 6DOF는 통상적으로 손의 사이즈에 기초하여 산출될 수 있다 (가령, 기 공개된 논문 'Hand Pose Estimation via Latent 2.5D Heatmap Regression'(저자: Umar Iqbal, Pavlo Molchanov, Thomas Breuel, Juergen Gall, Jan Kautz)에 개시된 방법과 같이, 손의 사이즈를 통해 6DOF 가 산출될 수 있다). 이에 따라, 본 개시의 일 실시 예에 따르면, 전자 장치는 손에 대한 파라미터를 판단하기 위해, 카메라 를 통해 획득된 이미지 프레임을 이용하여 사용자의 손의 사이즈를 식별할 수 있다. 그리고, 전자 장치 는 손에 대한 파라미터를 이용하여 사용자의 손과 디스플레이에 표시된 AR 객체의 인터렉션을 식별할 수 있다. 여기에서, 도 1c를 참조하면, 손의 사이즈는 일 예로, 손의 폭(또는 너비)(WH), 손의 면적(또는 부 피), 손을 구성하는 손가락의 길이 등 중에서 적어도 하나를 포함하는 개념일 수 있다. 전자 장치는 밀 리미터 단위의 손의 사이즈를 추정할 수 있다. 이에 따라, 본 개시의 일 실시 예에 따르면, 카메라를 이용하더라도, 손의 사이즈를 기반으로 사용자의 손과 디스플레이에 표시된 AR 객체와 인터렉션을 정확히 식별할 수 있게 된다. 이하에서는 첨부된 도면들을 참조하여, 본 개시의 다양한 실시 예에 따른 손의 사이즈를 판단하는 방법에 대해 구체적으로 설명하도록 한다. 도 2는 본 개시의 일 실시 예에 따른 전자 장치의 구성을 설명하기 위한 블록도이다. 도 2를 참조하면, 전자 장치는 디스플레이, 카메라 및 프로세서를 포함할 수 있다. 디스플레이는 시각적인 정보를 제공하는 장치이다. 예를 들어, 디스플레이는 이미지 프레임을 디스플 레이 영역의 전체 또는 일부 영역에 표시할 수 있다. 디스플레이의 디스플레이 영역은 서로 다른 위치로 구분되는 복수의 픽셀을 포함할 수 있다. 디스플레이는 디스플레이의 각 픽셀 별로 이미지 프레임의 각 픽셀에 포함된 색상 및 밝기 값을 갖는 광을 방출함으로써 이미지 프레임을 표시할 수 있다. 이를 위해, 일 실시 예에 따른 디스플레이는 별도의 백라이트 유닛(예: LED(light emitting diode) 등)을 광원으로 이용하고 액정(Liquid Crystal)의 분자 배열을 제어함으로써 백라이트 유닛에서 방출된 빛이 액정을 통해 투과되는 정도(빛의 밝기 또는 빛의 세기)를 조절하는 LCD(Liquid Crystal Display)로 구현될 수 있다. 다 른 일 실시 예에 따른 디스플레이는 별도의 백라이트 유닛 또는 액정 없이 자발광 소자(예: 크기가 100- 200um인 mini LED, 크기가 100um이하인 micro LED, OLED(Organic LED), QLED(Quantum dot LED) 등)를 광원으로 이용하는 디스플레이로 구현될 수 있다. 한편, 디스플레이는 사용자의 터치 조작을 감지할 수 있는 터치스크린 형태로 구현될 수 있다. 다른 예를 들어, 디스플레이는 일정 부분이 휘거나 접히고 다시 펼 수 있는 특성을 갖는 플렉서블 디스플레이 (flexible display)의 형태로 구현되거나, 디스플레이는 디스플레이의 후방에 위치한 사물을 투과시 켜 보이게 하는 특성을 갖는 투명 디스플레이로 구현될 수 있다. 카메라는 이미지 프레임을 획득할 수 있다. 구체적으로, 카메라는 특정한 시점(Point of View; PoV) 에서 화각(Field of View; FoV) 내에 존재하는 오브젝트(즉, 피사체)를 촬영함으로써 오브젝트를 포함하는 이미 지 프레임을 획득할 수 있다. 예를 들어, 이미지 프레임에 포함되는 오브젝트는 주변 환경에 실존하는 사물, 또 는 사용자의 손을 포함할 수 있다. 이를 위해, 카메라는 적어도 하나의 렌즈(121, 도 5 참조), 이미지 센서(123, 도 5 참조) 및 이미지 프로 세서를 포함할 수 있다. 렌즈는 피사체로부터 반사된 빛을 이미지 센서로 집광 또는 분광시킬 수 있 다. 이미지 센서는 2차원 평면 상에 서로 다른 위치로 구분하여 배치되는 복수의 픽셀을 포함하며, 렌즈 로부터 투과된 광을 픽셀 단위로 구분하여 각 픽셀마다 R(Red), G(Green), B(Blue) 색상을 감지하여 전기 신호를 생성할 수 있다. 이미지 프로세서는 이미지 센서에서 감지된 전기 신호에 따라 피사체의 색상 및 밝기를 표현하는 이미지 프레임을 획득할 수 있다. 이때, 이미지 프레임은 현실의 3차원 공간이 가상의 2차원 평면으로 투영된 것이며, 즉, 이미지 프레임은 서로 다른 2차원의 위치 정보(예: x축상 위치, y축상 위치)를 갖 는 복수의 픽셀을 포함할 수 있다. 이미지 프레임의 각 픽셀에는 특정한 색상 및 밝기 값이 포함될 수 있다. 또한, 카메라는 촬영 속도(또는 촬영 주기)를 나타내는 프레임 레이트(Frame Rate)로 시간에 대해 연속적 인 촬영을 수행하여, 복수의 이미지 프레임을 순차적으로(또는 주기적으로) 획득할 수 있다. 예를 들어, 카메라 는 프레임 레이트 30fps(frame per second)로 주변 환경을 촬영하는 경우, 1초 당 30개의 이미지 프레임을 순차적으로 획득할 수 있다. 이때, 카메라를 통해 획득된 이미지 프레임에는 카메라에 의해 촬영된 프레임 레이트, 촬영 시간, 화 각 중에서 적어도 하나에 대한 정보를 포함할 수 있다. 화각은 카메라의 렌즈의 초점 길이(focal length) 및 카메라의 이미지 센서의 사이즈(예: 대각 길이) 등에 따라 결정되는 값을 나타낼 수 있다. 한편, 상술한 프레임 레이트, 시간, 화각 중에서 적어도 하나에 대한 정보는 이미지 프레임 자체에 포함 되거나, 이미지 프레임에 대응되는 메타 데이터에 포함되는 형태로 구현될 수 있다. 프로세서는 전자 장치 또는 전자 장치의 전반적인 구성을 제어할 수 있다. 이를 위해, 프로세서 는 프로세서 내부에 구비된 메모리 또는 프로세서에 외부에 존재하는 메모리(160, 도 13 참조) 에 저장된 적어도 하나의 인스트럭션을 실행함으로써 전자 장치를 제어할 수 있다. 이때, 프로세서 내부에 구비된 메모리는 ROM(예: NOR 또는 NAND형 플래시 메모리), RAM(예: DRAM(dynamic RAM), SDRAM(synchronous DRAM), DDR SDRAM(Double data rate SDRAM)), 휘발성 메모리 등을 포함할 수 있다. 한편, 프로세서는 적어도 하나의 프로세서로 구성될 수 있으며, 각 프로세서는 CPU(Central Processing Unit), AP(Application Processor) 등과 같은 범용 프로세서, GPU(Graphic Processing Unit), VPU(Vision Processing Unit) 등과 같은 그래픽 전용 프로세서, NPU(Neural Processing Unit)와 같은 인공지능 전용 프로 세서 등으로 구현될 수 있다. 한편, 복수의 프로세서는 통합된 칩셋의 형태로 구현되거나 또는 별도의 칩셋의 형태로 구현될 수 있다. 한편, GPU 및 CPU는 서로 연계하여 본 개시의 동작을 수행할 수 있다. 예를 들어, GPU는 데이터 중 이미지 프레 임 등을 처리하고, CPU는 나머지 데이터(예: 인스트럭션, 코드 등)를 처리할 수 있다. 이때, GPU는 여러 명령어또는 데이터를 동시에 처리하는 병렬 처리 방식에 특화된 수백 또는 수천 개의 코어를 갖는 구조로 구현되며, CPU는 명령어 또는 데이터가 입력된 순서대로 처리하는 직렬 처리 방식에 특화된 수개의 코어를 갖는 구조로 구 현될 수 있다. 예를 들어, GPU는 본 개시의 이미지 프레임을 처리하여 정보를 획득하고, CPU는 이미지 프레임을 통해 획득된 정보를 처리하거나 연산을 수행할 수 있다. 도 3은 본 개시의 일 실시 예에 따른 프로세서의 동작을 설명하기 위한 도면이다. 도 3을 참조하면, 프로세서는 디스플레이에 AR 컨텐츠를 표시할 수 있다(S310). 즉, 프로세서는 AR 컨텐츠를 표시하도록 디스플레이를 제어할 수 있다. 여기서, AR 컨텐츠는 이미지, 동영상, 애니메이션 효과 등 다양한 종류로 구현될 수 있다. 예를 들어, AR 컨텐 츠는 실존하는 사물(예: TV, 디지털 액자, 사운드 바, 냉장고, 세탁기, 가구, 자동차, 건물, 나무 등)이 2차원 또는 3차원으로 렌더링된 이미지일 수 있다. 다른 예를 들어, AR 컨텐츠는 텍스트, 문자, 이미지, 사진, 동영상, 문서, 대시보드(dashboard) 등과 같은 다양한 유형 중 하나의 정보일 수 있다. 한편, 프로세서는 AR 컨텐츠의 투명도를 조절하여 AR 컨텐츠를 표시하도록 디스플레이를 제어할 수 있다. 즉, AR 컨텐츠는 불투명하거나 반투명한 상태로 디스플레이에 표시될 수 있다. AR 컨텐츠는 디스플레이를 통해 가상의 3차원 공간 상의 위치(예를 들어, xyz 좌표)에 표시될 수 있다. 여 기서, 가상의 3차원 공간 상의 위치는 디스플레이의 2차원 평면(플랫(flat)한 평면 또는 커브드(curved)된 평면)에 매핑(mapping)되어 있을 수 있다. 예를 들어, 프로세서는 가상의 3차원 공간 상의 좌표 (100, 200, 50)에 대응되는 디스플레이의 좌표 (90, 180)에 위치한 픽셀에 AR 컨텐츠의 중심이 위치하도록 AR 컨텐츠를 표시할 수 있다. 그리고, 프로세서는 카메라를 통해 획득된 이미지 프레임들에서 사용자의 손을 검출할 수 있다 (S320). 이때, 실시간 컴퓨터 비전을 분석하기 위한 프로그래밍 라이브러리(예: OpenCV, Python 등), Sobel 필터 또는 Fuzzy 필터 등의 다양한 색상 필터 또는 캐니 에지 검출(Canny Edge Detection), 색채기반, 템플릿 기반, 배경 차분화 방법 등의 다양한 알고리즘 등이 이용될 수 있다. 예를 들어, 프로세서는 이미지 프레임에 포함된 각 픽셀의 색상을 이진화(binarization)하는 전처리를 수 행할 수 있다. 프로세서는 전처리가 수행된 각 픽셀의 이진화된 색상(또는 명암)에 기초하여, 동일한 색상 을 갖는 인접한 픽셀들을 묶어(또는 그룹화하여) 각각 하나의 오브젝트로 검출할 수 있다. 그리고, 프로세서 는 검출된 오브젝트 중에서 손(또는 손가락)과 유사한 형상, 비율 및 굴곡을 갖는 오브젝트를 사용자의 손 으로 검출(식별)할 수 있다. 그리고, 프로세서는 손의 사이즈에 기초하여 AR 컨텐츠에 대한 손의 인터렉션을 식별할 수 있다 (S330). 구체적으로, 손의 사이즈는 기설정된 값으로 설정될 수 있다. 또는, 손의 사이즈는 디스플레이를 통 해 제공되는 오브젝트에 대한 손의 인터렉션이 발생되면, 오브젝트에 대한 정보에 기초하여 획득되는 값일 수 있다. 여기서, 디스플레이를 통해 제공되는 오브젝트에 대한 손의 인터렉션은 상술한 AR 컨텐츠에 대한 손 의 인터렉션 보다 이전에 발생한 동작일 수 있다. 프로세서는 이미지 프레임 상에서 검출된 손의 픽셀 사이즈를 식별할 수 있다. 여기서, 픽셀 사이즈는 이미지 프레임에서 검출된 손의 폭(또는 너비), 손을 구성하는 손가락의 길이 및 손의 면적(또는 부피) 등 중에서 적어도 하나를 나타내는 영역에 포함된 픽셀들의 개수를 의미할 수 있다. 프로세서는 손의 사이즈 및 손의 픽셀 사이즈를 이용하여, 전자 장치 및 손 사이의 거리를 산 출할 수 있다. 여기서, 거리는 z축 상의 값(또는 뎁스(depth))을 의미할 수 있다. 예를 들어, 프로세서는 손의 사이즈, 손의 거리, 이미지 프레임에서 검출된 손의 픽셀 사이즈의 대응 관계가 미리 산출된 하기의 표 1에 따라, 손의 사이즈 및 손의 픽셀 사이즈에 대응되는 손의 거 리를 산출할 수 있다. 표 1 사이즈 거리 픽셀 사이즈 0.2미터 0.5미터 200 0.2미터 1미터 100 0.2미터 2미터 50 0.3미터 0.5미터 300 0.3미터 1미터 150 0.3미터 2미터 75 ... ... ... 여기서, 사이즈는 카메라의 촬영 대상이 되는 오브젝트(즉, 현실의 공간에 존재하는 오브젝트이며, 예를 들어, 손, 스마트폰 등의 오브젝트를 포함할 수 있다)를 물리적으로 측정한 길이 및 면적 중에서 적어도 하 나를 나타낼 수 있다. 거리는 전자 장치(또는 카메라) 및 오브젝트(예를 들어, 손, 스마트폰 등의 오브젝트) 사이의 길 이를 나타낼 수 있다. 거리는 z 축 상의 값으로 표현될 수 있다. 픽셀 사이즈는 가상의 공간(예를 들어, 이미지 프레임)에 존재하는 오브젝트(예를 들어, 손, 스마트폰 등의 오브젝트)의 길이 및 면적 중에서 적어도 하나를 나타내는 영역에 포함된 픽셀의 개수를 나타낼 수 있다. 한편, 프로세서는 이미지 프레임 상에서 검출된 손을 나타내는 픽셀의 위치 정보를 통해, 손의 위치 를 식별할 수 있다. 이때, 식별되는 손의 위치는 2차원의 정보(예를 들어 xy 좌표)일 수 있다. 한편, 프로세 서는 이미지 프레임에 포함된 손 또는 손가락에 기초하여, 손의 방향 또는 손가락의 각도를 추정할 수 있다. 이 경우, 프로세서는 손의 사이즈를 측정하기 위한 점들(예: 손의 관절을 나타내는 점 등)의 좌표를 추정할 수 있다. 프로세서는 손의 2차원 위치 정보와 z축 값을 결합해 3차원 위치 정보(예를 들어, xyz 좌표)를 획득할 수 있다. 그리고, 프로세서는 사용자의 손의 3차원 공간 상의 위치 및 디스플레이에 표시된 AR 컨텐츠의 3 차원 공간 상의 위치를 비교하여 사용자의 손과 AR 컨텐츠 간의 인터렉션이 발생했는지 여부를 판단할 수 있 다. 예를 들어, 프로세서는 손의 3차원 위치 정보 및 디스플레이에 표시된 AR 컨텐츠의 3차원 위 치 정보를 비교하여, 손 및 AR 컨텐츠의 거리가 기설정된 값 이하로 판단되면, AR 컨텐츠에 대한 손의 인 터렉션이 발생한 것으로 식별할 수 있다. 이하에서는 본 개시의 일 실시 예에 따라 손의 사이즈를 식별하는 방법에 대해 보다 구체적으로 설명하도록 한다. 구체적인 본 개시의 일 실시 예에 따르면, 프로세서는 검출된 손의 사이즈를 기설정된 값으로 설정할 수 있다. 이 경우, 프로세서는 기설정된 값으로 설정된 손의 사이즈에 기초하여 디스플레이를 통해 제공 되는 오브젝트에 대한 손의 인터렉션이 발생되었는지를 식별할 수 있다. 여기서, 기설정된 값은 물리적인 측정 단위(예: cm 등)의 값일 수 있다. 일 예를 들어, 기설정된 값은 학습된 인공지능 모델을 통해 출력되는 값일 수 있다. 구체적으로, 프로세서는 이미지 프레임을 학습된 인공지능 모델에 입력하여 학습된 인공지능 모델에서 출력되는 기설정된 값을 검출된 손의 사이즈로 설정할 수 있다. 여 기서, 학습된 인공지능 모델은 사용자의 손이 포함된 이미지 프레임 및 해당 이미지 프레임에 포함된 손의 사이 즈를 학습 데이터로 하여 사용자의 손이 포함된 이미지 프레임이 입력되면 이미지 프레임에 포함된 손의 사이즈 를 출력하도록 학습된 것일 수 있다. 다른 예를 들어, 기설정된 값은 사용자들의 손의 평균 사이즈를 나타내는 값일 수 있다. 이때, 사용자들은 특정 한 집단(예: 성별, 나이, 지역 등) 별로 분류된 것일 수 있다. 예를 들어, 전자 장치(또는 메모리)는 각 집단(예: 성인 남성, 성인 여성 등)에 속하는 사용자에 대해, 해당 사용자의 손 의 특징 정보 및 해당 사 용자의 손의 사이즈(예: 개별 사이즈, 평균 사이즈 등)에 대한 정보를 저장할 수 있다. 특징 정보는 손을 구성하는 손가락 길이의 비율, 손의 거칠기, 손의 주름 등 중에서 적어도 하나에 대한 정보를 포함할 수 있다. 이 경우, 프로세서는 저장된 손의 특징 정보와 이미지 프레임에서 검출된 손의 특징 정보를 비교하여, 검출된 손이 속하는 집단을 식별할 수 있다. 그리고, 프로세서는 식별된 집단에 속하는 사용자들의 손의 평균 사이즈를 기설정된 값으로 결정하고, 기설정된 값을 검출된 손의 사이즈로 설정할 수 있다. 한편, 본 개시의 일 실시 예에 따른 손의 사이즈는 디스플레이를 통해 제공되는 오브젝트에 대한 손 의 인터렉션이 발생되면, 오브젝트에 대한 정보에 기초하여 획득될 수 있다. 즉, 프로세서는 디스플레 이를 통해 제공되는 오브젝트에 대한 손의 인터렉션이 발생되면, 인터렉션이 발생한 오브젝트에 대한 정보에 기초하여 손의 사이즈를 획득할 수 있다. 즉, 프로세서는 손의 사이즈를 기설정된 값에서 획 득된 값으로 보정할 수 있다. 이 경우, 프로세서는 손의 사이즈에 기초하여 디스플레이를 통해 제 공되는 오브젝트에 대한 손의 인터렉션이 발생되었는지를 식별할 수 있다. 여기서, 오브젝트는 카메라를 통해 획득된 이미지 프레임에 포함된 제1 타입의 오브젝트 및 디스플레이 에 표시되는 AR 컨텐츠에 포함된 제2 타입의 오브젝트 중 적어도 하나를 포함할 수 있다. 즉, 디스플레이 를 통해 제공되는 오브젝트는 제1 타입의 오브젝트 또는 제2 타입의 오브젝트일 수 있다. 구체적으로, 제1 타입의 오브젝트는 카메라를 통해 촬영될 수 있는 현실의 공간에 존재하는 오브젝트(예: 스마트폰, 냉장고, 엘리베이터 버튼 등)를 의미할 수 있다. 예를 들어, 제1 타입의 오브젝트는 투명 디스플레이인 디스플레이를 통해 투과되는 형태로 사용자에게 제 공될 수 있다. 즉, 디스플레이는 외부에 존재하는 제1 타입의 오브젝트를 나타내는 빛을 투과시킬 수 있다. 이때, 제1 타입의 오브젝트는 현실의 공간에서 디스플레이를 기준으로 일 방향(예를 들어 전방)에 위치한 사용자와는 반대의 일 방향(예를 들어 후방)에 위치할 수 있다. 이 경우, 사용자는 디스플레이에 투과된 빛을 통해 제1 타입의 오브젝트를 볼 수 있다. 다른 예를 들어, 제1 타입의 오브젝트는 이미지 프레임이 디스플레이에 표시되는 형태로 사용자에게 제공 될 수 있다. 즉, 디스플레이는 카메라를 통해 획득된 이미지 프레임을 표시할 수 있다. 이때, 이미지 프레임에는 제1 타입의 오브젝트가 포함될 수 있다. 이 경우, 사용자는 이미지 프레임을 표시하는 디스플레이 에서 방출되는 빛을 통해 제1 타입의 오브젝트를 볼 수 있다. 한편, 제2 타입의 오브젝트는 디스플레이 상에 표시되는 가상의 오브젝트(예: 3차원 UI(user interface 요 소, 위젯 등)를 의미할 수 있다. 여기서, 제2 타입의 오브젝트는 상술한 AR 컨텐츠에 대한 설명이 동일하게 적 용될 수 있다. 오브젝트에 대한 정보는 오브젝트의 사이즈(예를 들어, 가로 길이, 세로 길이, 면적 등), 오브젝트 및 전자 장 치 사이의 거리, 이미지 프레임에 포함된 오브젝트의 픽셀 사이즈에 대한 정보 및 이들의 대응 관계에 대 한 정보를 포함할 수 있다. 예를 들어, 오브젝트에 대한 정보는 표 1과 같이 동일한 행에 위치한 정보들은 서로 대응되는 관계(또는 매핑되는 관계)인 것을 나타낼 수 있다. 오브젝트에 대한 손의 인터렉션은, 손 및 오브젝트 사이의 거리가 기설정된 값 이하가 되는 이벤트, 손 이 오브젝트에 접촉하는 이벤트, 손이 오브젝트를 잡는 이벤트, 손의 위치 주변에 존재하는 오브젝트 에서 디스플레이(또는 LED 등)에 표시되는 화면이 변경되는 이벤트, 손의 위치 주변에 존재하는 오브젝트의 이동이 시작되거나 이동이 중지되는 이벤트 중에서 적어도 하나를 포함할 수 있다. 본 개시의 일 실시 예로서, 전자 장치는 메모리(160, 도 13 참조)를 더 포함할 수 있다. 메모리는 레 퍼런스 오브젝트의 특징 정보 및 사이즈 정보를 저장할 수 있다. 여기서, 레퍼런스 오브젝트는 특징 정보 및 사 이즈 정보가 기저장되어 있는 오브젝트(즉, 알고 있는 오브젝트)를 의미할 수 있다. 여기서, 특징 정보는 오브 젝트를 식별하기 위한 고유의 정보를 나타내며, 예를 들어 텍스쳐, 스크래치, 형상 등 중에서 적어도 하나를 포 함할 수 있다. 이 경우, 프로세서는, 카메라를 통해 획득된 이미지 프레임들에서 제1 타입의 오브젝트 및 사용자의 손이 검출되면, 메모리에 저장된 특징 정보 및 검출된 제1 타입의 오브젝트의 특징 정보에 기초하여 제1 타입의 오브젝트가 레퍼런스 오브젝트인지를 식별할 수 있다. 이 경우, 프로세서는 제1 타입의 오브젝트가 레퍼런스 오브젝트인 것으로 식별되면(즉, 제1 타입의 오브젝 트가 알고 있는 오브젝트인 경우), 레퍼런스 오브젝트의 사이즈에 기초하여 손의 사이즈를 식별할 수 있다. 이와 다른 일 실시 예로서, 프로세서는 메모리에 저장된 특징 정보 및 검출된 제1 타입의 오브젝트의 특징 정보에 기초하여 제1 타입의 오브젝트가 레퍼런스 오브젝트에 해당하지 않은 것으로 식별되면(즉, 제1 타 입의 오브젝트가 모르는 오브젝트인 경우), 카메라를 통해 획득된 연속된 이미지 프레임들을 이용하여 연속된 이미지 프레임들에 포함된 제1 타입의 오브젝트의 사이즈를 식별할 수 있다. 이 경우, 프로세서는 제 1 타입의 오브젝트의 사이즈에 기초하여 손의 사이즈를 식별할 수 있다. 본 개시의 일 실시 예로서, 프로세서는 검출된 손의 사이즈를 기설정된 값으로 설정할 수 있다. 이 경우, 프로세서는 설정된 사이즈에 기초하여 디스플레이를 통해 제공되는 제2 타입의 오브젝트에 대한 손의 인터렉션이 발생되었는지를 식별할 수 있다. 이 경우, 프로세서는 손의 인터렉션이 식별된 제2 타입의 오 브젝트의 뎁스에 기초하여 손의 사이즈를 식별할 수 있다. 이하에서는 첨부된 도면과 함께 본 개시의 다양한 일 실시 예에 따라 손의 사이즈를 식별하는 방법에 대해 설명하도록 한다. 도 4는 본 개시의 일 실시 예에 따른 초점 길이를 통해 손의 사이즈를 식별하는 방법을 설명하기 위한 도면이다. 도 4를 참조하면, 프로세서는 카메라의 스펙 정보에 기초하여, 카메라의 초점 길이가 변경 가능 한지 여부를 식별할 수 있다. 이때, 카메라의 스펙 정보는 초점 길이의 가변 여부에 대한 정보를 포함할 수 있으며, 전자 장치에 저장되어 있을 수 있다. 프로세서는 카메라의 초점 길이가 변경 가능 가능하지 않은 것으로 판단되면(S410, No), 도 7의 단계 2의 동작을 수행할 수 있으며, 이에 대해서는 후술하여 설명하도록 한다. 프로세서는 카메라의 초점 길이가 변경 가능 가능한 것으로 판단되면(S410, Yes), 초점 길이를 변경 하여 획득된 이미지 프레임에 포함된 손의 대비(contrast)가 가장 높은 이미지 프레임을 선택할 수 있다 (S420). 프로세서는 선택된 이미지의 초점 거리를 이용해 손과의 거리를 산출할 수 있다(S430). 이에 대해서는 도 5 및 도 6을 참조하여 함께 설명하도록 한다. 도 5는 본 개시의 일 실시 예에 따른 초점 길이 및 초점 거리 간의 관계를 설명하기 위한 도면이다. 도 6은 본 개시의 일 실시 예에 따른 초점 길이를 통해 손의 사이즈를 식별하는 방법을 설명하기 위한 도면이다. 도 5를 참조하면, 본 개시의 일 실시 예에 따른 카메라는 적어도 하나의 렌즈 및 이미지 센서를 포함할 수 있다. 이 경우, 카메라는 피사체(예를 들어, 손 또는 제1 타입의 오브젝트 등)를 촬영 하여 이미지 프레임을 획득할 수 있다. 본 개시의 일 실시 예에 따른 프로세서는 도 5의 및 와 같은 식에 따라 카메라 및 피사체 간의 거리를 산출할 수 있다. 여기서, 초점 거리(focus distance, a)는 카메라 및 피사체(예: 초점이 맞춰진 피사체) 간의 거리(즉, 렌즈 및 피사체 간의 거리)이며, 초점 길이(focal length, f)는 렌즈(예: 렌즈 의 주점) 및 이미지 센서 간의 거리이다. 또한, L1은 피사체의 사이즈(예를 들어, 폭 또는 너비)를 나타내며, L2는 이미지 센서의 사이즈(예를 들어, 폭 또는 너비)를 나타낼 수 있다. 여기서, 초점 길이 (f)가 변경되면, 초점 거리(a) 또한 변경될 수 있다. 초점 길이(f)는 모터의 구동을 통한 자동 방식 또는 사용 자에 의한 수동 방식을 통해 조절될 수 있다. 도 6을 참조하면, 프로세서는 카메라의 초점 길이(f)가 변경 가능 가능한 것으로 판단되면(S410, Yes), 초점 길이(f)(또는 초점 거리(a))를 변경하여 도 6의 내지 과 같은 이미지 프레임(610, 620, 630)을 획득할 수 있다. 예를 들어, 도 6의 의 이미지 프레임은 초점 거리 10cm를 갖고, 도 6의 의 이미지 프레임은 초점 거리 18cm를 갖고, 도 6의 의 이미지 프레임은 초점 거리 23cm를 가질 수 있다. 그리고, 프로세서는 서로 다른 초점 거리(또는 초점 길이)를 갖는 이미지 프레임(610, 620, 630) 중에서 손의 대비가 가장 높은 이미지 프레임을 선택할 수 있다. 예를 들어, 프로세서는 이미지 프레임(610, 620, 630)에서 손을 나타내는 영역을 검출하고, 검출된 영역(615, 625, 635)을 전자 장치에 기저장된 손 모델과 비교하여 대비(contrast)를 판단할 수 있다. 여기서 대비는 선명도(또는 흐림의 정도)를 의미할 수 있으며, 대 비가 높을수록 손이 선명하게 촬영된 것이며, 대비가 높을수록 손에 대해 초점이 맞추어진 것을 의미할 수 있다. 여기서, 프로세서는 판단된 대비 중에서 도 6의 의 이미지 프레임에서 검출된 손을 나타 내는 영역의 대비가 가장 높은 것으로 판단할 수 있으며, 가장 대비가 높은 이미지 프레임으로 선택된 이 미지 프레임의 초점 거리 18cm를 손과의 거리로 식별할 수 있다. 다시 도 4를 참조하면, 프로세서는 손과의 거리를 이용하여 손의 사이즈를 산출할 수 있다(S440). 예를 들어, 프로세서는 현실의 측정 사이즈(예: 20cm)를 갖는 손에 대해, 손이 전자 장치와 이 격된 거리(예: 0.5미터, 1미터, ... 등) 및 해당 거리에서 촬영을 통해 획득된 이미지 프레임에 포함된 손의 사이즈(예: 픽셀 수 100개, 50개, ... 등)의 대응 관계를 전자 장치 또는 메모리에 기저장하고 있을 수 있다. 이 경우, 프로세서는 손(또는 오브젝트)의 거리를 알고 있다면, 기저장된 대응 관계에서 이미 지 프레임에 포함된 손(또는 오브젝트)의 거리를 통해 손(또는 오브젝트)의 사이즈를 산출할 수 있다. 프로세서는 산출된 손의 사이즈에 대한 정확도를 산출할 수 있다. 프로세서는 산출된 정확도가 기 설정된 값 이상인 경우(S450, Yes), 손의 사이즈 및 정확도를 업데이트할 수 있다. 이와 달리, 프로세서 는 산출된 정확도가 기설정된 값 미만인 경우(S450, No), 도 7의 단계 2의 동작을 수행할 수 있으며, 이에 대해서는 후술하여 설명하도록 한다. 도 7은 본 개시의 일 실시 예에 따른 손과 인터렉션한 오브젝트를 통해 손의 사이즈를 식별하는 방법을 설명하 기 위한 도면이다. 도 7을 참조하면, 프로세서는 디스플레이를 통해 제공되는 오브젝트 중에서 손과 인터렉션한 오브 젝트가 검출되는지 여부를 판단할 수 있다. 여기서, 디스플레이를 통해 제공되는 오브젝트는 카메라 를 통해 획득된 이미지 프레임에 포함된 제1 타입의 오브젝트 및 디스플레이에 표시되는 AR 컨텐츠에 포함 된 제2 타입의 오브젝트 중 적어도 하나를 포함할 수 있다. 인터렉션에 대해서는 도 8a 및 도 8b를 참조하여 설 명하도록 한다. 도 8a는 본 개시의 일 실시 예에 따른 손과 인터렉션한 오브젝트를 검출하는 방법을 설명하기 위한 도면이다. 도 8b는 본 개시의 일 실시 예에 따른 손과 인터렉션한 오브젝트를 검출하는 방법을 설명하기 위한 도면이다. 도 8a를 참조하면, 일 실시 예에 따른 프로세서는 카메라를 통해 순차적으로 이미지 프레임(810, 820, 830, 840)를 획득할 수 있다. 프로세서는 획득된 이미지 프레임에서 특징 정보를 검출하고, 특징 정보를 통해 사용자의 손을 나 타내는 영역(811h, 813h, 815h) 및 오브젝트를 나타내는 영역(811o, 813o, 815o)을 검출할 수 있다. 여기서, 이미지 프레임은 이미지 프레임(811, 813, 815) 중 적어도 하나를 포함할 수 있다. 이미지 프레임 은 손의 위치가 오브젝트 보다 전방에 있는 것을 나타낸 것이며(즉, 손 및 전자 장치 간의 거 리가 더 큰 경우), 이미지 프레임은 손의 위치가 오브젝트 보다 후방에 있는 것을 나타낸 것이며(즉, 손 및 전자 장치 간의 거리가 더 작은 경우), 이미지 프레임은 손(즉, 손가락)의 위치가 오브 젝트 사이에 있는 것을 나타낸 것이다(즉, 손 및 전자 장치간의 거리가 오브젝트 및 전자 장치 간 의 거리와 동일한 경우). 한편, 특징 정보는 이미지 프레임에서 손 또는 오브젝트를 식별하는데 이용되는 고 유한 특징을 나타내는 정보이며, 특징 정보는 텍스쳐, 색상, 형상 등에 대한 정보를 포함할 수 있다. 이후, 프로세서는 획득된 이미지 프레임에서 특징 정보를 검출하고, 특징 정보를 통해 사용자의 손 을 나타내는 영역(825h) 및 오브젝트를 나타내는 영역(825o)을 검출할 수 있다. 여기서, 프로세서는 순 차적으로 획득된 이미지 프레임 및 이미지 프레임을 비교하여 오브젝트가 이동을 시작하거나 이동을 중지하는지 여부를 판단할 수 있다. 이후, 프로세서는 획득된 이미지 프레임에서 특징 정보를 검출하고, 특징 정보를 통해 사용자의 손 을 나타내는 영역(835h)를 검출할 수 있다. 여기서, 프로세서는 순차적으로 획득된 이미지 프레임 및 이미지 프레임을 비교하여 오브젝트가 사용자의 손 아래(예를 들어 그림자 부분)에 위치하는지 여부 를 판단할 수 있다. 이후, 프로세서는 카메라를 통해 획득된 이미지 프레임에서 특징 정보를 검출하고, 특징 정보를 통해 사용자의 손을 나타내는 영역(845h)를 검출할 수 있다. 여기서, 프로세서는 순차적으로 획득된 이 미지 프레임 및 이미지 프레임을 비교하여 사용자의 손의 모양 변화를 통해 사용자의 손이 오 브젝트를 잡는지 여부를 판단할 수 있다. 프로세서는 사용자의 손이 오브젝트를 잡는 것으로 판단되면, 사용자의 손이 오브젝트와 인터렉션한 것으로 판단할 수 있다. 도 8b의 및 를 참조하면, 일 실시 예에 따른 프로세서는 카메라를 통해 이미지 프레임(850, 860)를 획득할 수 있다. 예를 들어, 프로세서는 도 8b의 의 이미지 프레임에서 특징 정보를 검출하고, 특징 정보를 통해 사용자의 손을 나타내는 영역 및 오브젝트(예: 엘리베이터 버튼)를 나타내는 영역(855o)을 검출할 수 있다. 그리고, 프로세서는 순차적으로 획득된 이미지 프레임을 통해 오브젝트를 나타내는 영역(855o)의 변화(예: 엘리베이터 버튼의 LED가 점등됨)가 감지되면 사용자의 손이 오브젝트와 인터렉션한 것으로 판단할 수 있다. 다른 예를 들어, 프로세서는 도 8b의 의 이미지 프레임에서 특징 정보를 검출하고, 특징 정보를 통해 사용자의 손을 나타내는 영역 및 오브젝트(예: 냉장고의 디스플레이)를 나타내는 영역(865o)을 검출할 수 있다. 그리고, 프로세서는 순차적으로 획득된 이미지 프레임을 통해 오브젝트를 나타내는 영역(865o)의 변화(예: 냉장고의 디스플레이에 표시된 이미지가 변경됨)가 감지되면 사용자의 손이 오브젝트와 인터렉션한 것으로 판단할 수 있다. 다시 도 7을 참조하면, 프로세서는 손과 인터렉션한 오브젝트가 검출되면(S710, Yes), 검출된 오브젝트 가 제1 타입의 오브젝트에 해당하는지 여부를 판단할 수 있다. 또는, 프로세서는 검출된 오브젝트가 제2 타입의 오브젝트에 해당하는지 여부를 판단할 수 있다. 여기서, 제1 타입의 오브젝트는 카메라를 통해 촬 영될 수 있는 현실의 공간에 존재하는 오브젝트(예: 스마트폰, 냉장고, 엘리베이터 버튼 등)를 의미할 수 있다. 한편, 제2 타입의 오브젝트는 디스플레이 상에 표시되는 가상의 오브젝트(예: 3차원 UI(user interface 요 소, 위젯 등)를 의미할 수 있다. 구체적인 예를 들어, 프로세서는 카메라를 통해 획득되는 이미지 프레임에서, 이미지 프레임에 포함 된 손의 위치 및 적어도 하나의 제1 타입의 오브젝트의 위치를 비교할 수 있다. 그리고, 프로세서는 적 어도 하나의 제1 타입의 오브젝트 중에서 손과 기설정된 값 이하의 거리를 갖는 제1 타입의 오브젝트를 손 과 인터렉션 한 오브젝트로 검출할 수 있다. 이 경우(S720, Yes), 프로세서는 검출된 제1 타입의 오브 젝트가 레퍼런스 오브젝트인지 여부를 판단할 수 있다. 검출된 타입의 오브젝트가 제1 타입의 오브젝트인 경우(S720, Yes), 프로세서는 검출된 제1 타입의 오브젝 트가 레퍼런스 오브젝트인지 여부를 판단할 수 있다. 여기서, 레퍼런스 오브젝트는 메모리에 특징 정보 및 사이즈 정보가 기저장된 오브젝트를 의미할 수 있다. 예를 들어, 프로세서는 메모리에 저장된 복수의 레퍼런스 오브젝트의 특징 정보 및 검출된 제1 타입 의 오브젝트의 특징 정보를 비교하여 각 레퍼런스 오브젝트 및 검출된 제1 타입의 오브젝트에 대한 유사도를 산 출할 수 있다. 그리고, 프로세서는 복수의 레퍼런스 오브젝트 중에서 기설정된 값 이상이면서 가장 높은 유사도를 갖는 레퍼런스 오브젝트인 것으로 검출할 수 있다. 여기서, 프로세서는 제1 타입의 오브젝트가 레퍼런스 오브젝트인 것으로 식별되면(즉, 제1 타입의 오브젝 트가 알고 있는 오브젝트인 경우) (S730, Yes), 레퍼런스 오브젝트의 사이즈에 기초하여 손의 사이즈를 식별 할 수 있다(S740). 예를 들어, 프로세서는 기설정된 값 이상이면서 가장 높은 유사도를 갖는 레퍼런스 오 브젝트가 검출되면, 검출된 제1 타입의 오브젝트가 검출된 레퍼런스 오브젝트인 것으로 식별할 수 있다. 이에 대한 다양한 실시 예는 도 9를 참조하여 설명하도록 한다. 도 9는 본 개시의 일 실시 예에 따른 제1 타입의 오브젝트를 통해 손의 사이즈를 식별하는 방법을 설명하기 위 한 도면이다. 도 10은 제1 타입의 오브젝트가 레퍼런스 오브젝트인 경우를 설명하기 위한 도면이다. 도 9의 및 를 참조하면, 일 실시 예에 따른 프로세서는 카메라를 통해 이미지 프레임(910, 930)을 획득할 수 있다. 도 9의 과 같이 프로세서는 획득된 이미지 프레임에서 특징 정보를 검출하고, 특징 정보를 통해 사용자의 손을 나타내는 영역(915h) 또는 제1 타입의 오브젝트를 나타내는 영역(915o)을 검출할 수 있다. 이 경우, 프로세서는 사용자의 손을 나타내는 영역(915h) 및 제1 타입의 오브젝트를 나타내는 영역 (915o) 사이의 거리가 기설정된 값 이하가 되는 것으로 판단되면, 사용자의 손이 제1 타입의 오브젝트에 대 한 인터렉션이 발생한 것으로 판단할 수 있다. 이 경우, 프로세서는 제1 타입의 오브젝트를 나타내는 영역(915o)에 포함된 특징 정보 및 메모리에 저장된 복수의 레퍼런스 오브젝트의 특징 정보를 비교하여, 제1 타입의 오브젝트가 레퍼런스 오브젝트인지 여부 를 판단할 수 있다. 도 9의 은 이미지 프레임에 포함된 제1 타입의 오브젝트가 레퍼런스 오브젝트(예: 가로 69.6mm × 세로 142.4mm의 사이즈를 갖는 갤럭시 S7)인 경우를 나타낸 것이다. 이 경우, 프로세서는 레퍼런스 오브젝트의 사이즈, 레퍼런스 오브젝트의 픽셀 사이즈(X1, Y1) 및 손의 픽셀 사이즈(915hs)에 기초하여, 손의 사이즈를 식별할 수 있다. 예를 들어, 프로세서는 하기 수학식 1 과 같은 비율 관계를 통해 손의 사이즈를 산출할 수 있다. [수학식 1] 레퍼런스 오브젝트의 사이즈 : 레퍼런스 오브젝트의 픽셀 사이즈 = 손의 사이즈 : 손의 픽셀 사이즈 도 9의 의 경우에도 상술한 방식이 동일하게 적용될 수 있다. 즉, 프로세서는 획득된 이미지 프레임 에 포함된 사용자의 손을 나타내는 영역(935h) 및 제1 타입의 오브젝트를 나타내는 영역(935o) 사이의 거리가 기설정된 값 이하가 되는 것으로 판단되면, 사용자의 손이 제1 타입의 오브젝트에 대한 인터렉션이 발생한 것으로 판단할 수 있다. 이 경우, 프로세서는 제1 타입의 오브젝트를 나타내는 영역(935o)에 포함된 특징 정보 및 메모리에 저장된 복수의 레퍼런스 오브젝트의 특징 정보를 비교하여, 제1 타입의 오브젝트가 레퍼런스 오브젝트인지 여부 를 판단할 수 있다. 도 9의 는 이미지 프레임에 포함된 제1 타입의 오브젝트가 레퍼런스 오브젝트(예: 가로 120mm × 세로 205mm의 사이즈를 갖는 냉장고의 디스플레이)인 경우를 나타낸 것이다. 이 경우, 프로세서는 레퍼런스 오브젝트의 사이즈, 레퍼런스 오브젝트의 픽셀 사이즈(X2, Y2) 및 손의 픽 셀 사이즈(935hs)에 기초하여, 손의 사이즈를 식별할 수 있다. 한편, 상술한 예는 일 실시 예일 뿐이며, 프로세서는 카메라를 통해 이미지 프레임이 획득되면, 획득 된 이미지 프레임에 포함된 적어도 하나의 제1 타입의 오브젝트(또는 레퍼런스 오브젝트)를 식별한 후, 사용자 의 손이 검출되면, 적어도 하나의 제1 타입의 오브젝트 중에서 검출된 손과 인터렉션이 발생한 오브젝트를 식별 하는 동작을 수행할 수 있다. 한편 도 7을 다시 참조하여 이와 달리, 프로세서는 메모리에 저장된 특징 정보 및 검출된 제1 타입의 오브젝트의 특징 정보에 기초하여 제1 타입의 오브젝트가 레퍼런스 오브젝트에 해당하지 않은 것으로 식별되면 (즉, 제1 타입의 오브젝트가 모르는 오브젝트인 경우)(S730, No), 카메라를 통해 획득된 연속된 이미지 프 레임들을 이용하여 연속된 이미지 프레임들에 포함된 제1 타입의 오브젝트의 사이즈를 식별할 수 있다(S750). 예를 들어, 프로세서는 기설정된 값 이상인 유사도를 갖는 레퍼런스 오브젝트가 검출되지 않으면, 제1 타 입의 오브젝트가 레퍼런스 오브젝트에 해당하지 않은 것으로 식별할 수 있다. 이 경우, 프로세서는 카메라를 통해 획득된 연속된 이미지 프레임들을 이용하여, 연속된 이미지 프레 임들에 포함된 제1 타입의 오브젝트의 사이즈를 식별할 수 있다. 그리고, 프로세서는 제1 타입의 오브젝트 의 사이즈에 기초하여 손의 사이즈를 식별할 수 있다(S755). 이에 대한 다양한 실시 예는 도 10을 참조하여 설 명하도록 한다. 도 10은 본 개시의 일 실시 예에 따른 제1 타입의 오브젝트를 통해 손의 사이즈를 식별하는 방법을 설명하기 위 한 도면이다. 도 10은 제1 타입의 오브젝트가 레퍼런스 오브젝트가 아닌 경우를 설명하기 위한 도면이다. 도 10을 참조하면, 일 실시 예에 따른 프로세서는 카메라를 통해 이미지 프레임(1010, 1020, 1030)을 획득할 수 있다. 그리고, 프로세서는 획득된 이미지 프레임(1010, 1020, 1030)에서 특징 정보를 검출하고, 특징 정보를 통해 사용자의 손을 나타내는 영역(1035h) 또는 제1 타입의 오브젝트를 나타내는 영역(1011o, 1012o, 1025o, 1035o)을 검출할 수 있다. 여기서는, 프로세서가 이미지 프레임(1010, 1020, 1030)에 포함된 제1 타입의 오브젝트가 레퍼런스 오브젝 트에 해당하지 않은 것으로 식별한 경우를 가정하도록 한다. 이 경우, 프로세서는 SLAM(Simultaneous localization and mapping) 방식을 통해 이미지 프레임(1010, 1020, 1030)에 포함된 제1 타입의 오브젝트의 사 이즈를 측정할 수 있다. 이때, 이미지 프레임 및 이미지 프레임은 촬영된 시간 간격(예: 5ms 등)이 매우 작은 연속적으로 획득된 이미지 프레임을 나타낸 것이며(즉, t-1번째 획득된 이미지 프레임 다음인 t번째 획득된 이미지 프레임의 관계), 이에 비해 복수의 이미지 프레임(1010 내지 1030)들은 촬영된 시간 간격(예: 2초 등)이 큰 이미지 프레임을 나타낸 것이다. 구체적으로, 프로세서는 카메라가 움직이는 동안(또는 회전되는 동안) 연속적으로 촬영되어 획득된 2 개의 이미지 프레임(1011, 1012)를 비교하여, 제1 타입의 오브젝트를 나타내는 영역(1011o, 1012o)의 중심 위치 가 이동한 거리를 판단할 수 있다. 또한, 이 경우, 프로세서는 전자 장치에 구비된 별도의 센서(예:가속도 센서 등)를 통해 카메라가 촬영한 시간 동안 카메라가 움직인 정도(예: 이동 거리, 또는 회전 각도)를 획득할 수 있다. 그리고, 프로세서는 카메라가 움직이는 동안(또는 회전되는 동안) 제1 타입의 오브젝트를 나타내는 영역(1011o, 1012o)의 중심 위치가 이동한 거리 및 카메라가 움직인 정도(예: 이동 거리, 또는 회전 각 도)에 대응되는 거리(오브젝트 및 전자 장치 사이의 거리)를 식별할 수 있다. 이를 위해, 전자 장치 에는 전자 장치가 회전 각도 및 중심 위치가 이동한 거리에 매칭되는 거리에 대한 정보가 기저장되어 있을 수 있다. 이는 카메라가 이동하면, 카메라를 통해 획득되는 이미지 프레임 상에서 오브젝트가 이동할 수 있는데, 이때 카메라가 동일한 거리를 이동하더라도 카메라 및 오브젝트 간의 거리에 따라, 이미 지 프레임 상에서 오브젝트의 중심 거리가 이동하는 정도가 달라질 수 있기 때문이다. 그리고, 프로세서는 이미지 프레임을 통해 카메라 및 오브젝트 간의 거리가 식별되면, 거리가 식별된 이후에 획득된 이미지 프레임에서 제1 타입의 오브젝트를 나타내는 영역(1025o)의 픽셀 사이즈(예: 가로 픽셀, 세로 픽셀 등)를 식별할 수 있다. 또한, 프로세서는 오브젝트의 거리 및 오브젝트 의 픽셀 사이즈에 대응되는 오브젝트의 사이즈(예를 들어, 가로 40mm, 세로 45mm의 실측 사이즈)를 식별할 수 있다. 이 경우, 프로세서는 오브젝트에 대한 정보(가령, 오브젝트의 사이즈, 오브젝트의 거리 및 이미지 프레임에 포함된 오브젝트의 픽셀 사이즈에 대한 정보 및 이들의 대응 관계에 대한 정보(가령, 표 1))을 이용하 여, 오브젝트의 거리 및 오브젝트의 픽셀 사이즈에 매핑된 오브젝트의 사이즈를 식별할 수 있다. 이후, 프로세서는 이미지 프레임에서 사용자의 손을 나타내는 영역(1035h) 및 제1 타입의 오브젝 트를 나타내는 영역(1035o) 사이의 거리가 기설정된 값 미만이 되면, 사용자의 손이 오브젝트에 대해 인터렉 션이 발생한 것으로 판단할 수 있다. 일 실시 예로서, 프로세서는 오브젝트의 거리(오브젝트 및 카메라 사이의 거리)를 사용자의 손의 거리(손 및 카메라 사이의 거리)로 추정할 수 있다. 이 경우, 프로세서는 표 1과 같은 대응 관계 를 통해 사용자의 손의 픽셀 사이즈 및 손의 거리에 대응되는 손의 사이즈를 식별할 수 있다. 일 실시 예로서, 프로세서는 수학식 1과 유사한 방식으로, 오브젝트의 사이즈, 오브젝트의 픽셀 사이즈 및 손의 픽셀 사이즈에 기초하여, 손의 사이즈를 식별할 수 있다. 한편, 도 7을 다시 참조하여 다른 예를 들어, 프로세서는 카메라를 통해 획득된 이미지 프레임에 포 함된 손의 위치 및 디스플레이를 통해 표시되는 적어도 하나의 제2 타입의 오브젝트의 위치를 비교할 수 있다. 프로세서는 적어도 제2 타입의 오브젝트 중에서 손과 기설정된 값 이하의 거리를 갖는 제2 타 입의 오브젝트를 손과 인터렉션 한 오브젝트로 검출할 수 있다. 이 경우(S720, No), 프로세서는 검출된 제2 타입의 오브젝트의 뎁스에 기초해 손의 사이즈를 식별할 수 있 다(S760). 구체적으로, 프로세서는 검출된 손의 사이즈를 기설정된 값으로 설정할 수 있다. 또한, 검출된 제2 타입의 오브젝트의 뎁스 값은 손의 거리 값(전자 장치 및 손 사이의 거리, 즉 z 축상의 길이)으로 간주될 수 있다. 이 경우, 프로세서는 표 1의 실시 예와 같이, 손의 거리 값 및 이미지 프레임에 포함된 복수 의 픽셀 중 손을 나타내는 픽셀의 픽셀 사이즈에 대응되는 손의 사이즈를 획득할 수 있다. 한편, 프로세 서는 이미지 프레임에 포함된 복수의 픽셀 중 손을 나타내는 픽셀의 위치 정보(즉, xy축 상의 좌표) 및 손의 거리 값(z 축상의 길이)를 결합하여, 손의 3차원 위치 정보를 획득할 수 있다. 이에 대한 다양한 실 시 예는 도 11을 참조하여 설명하도록 한다. 도 11은 본 개시의 일 실시 예에 따른 제2 타입의 오브젝트를 통해 손의 사이즈를 식별하는 방법을 설명하기 위 한 도면이다. 도 11을 참조하면, 일 실시 예에 따른 디스플레이를 통해 이미지가 제공될 수 있다. 여기서 이미지 는 카메라를 통해 획득된 이미지 프레임 및 가상의 제2 타입의 오브젝트(1125o)가 중첩되어 디스플 레이에 표시되는 상태의 이미지, 또는 외부 환경은 디스플레이에 투과되어 보이며 가상의 제2 타입의 오브젝트(1125o)가 외부 환경에 중첩되어 디스플레이에 표시되는 상태의 이미지 중 적어도 하나를 포함할 수 있다. 프로세서는 제2 타입의 오브젝트(또는 AR 컨텐츠)(1125o)를 가상의 3차원 공간 상의 특정한 위치에 표시하 도록 디스플레이를 제어할 수 있다. 여기서, 제2 타입의 오브젝트(1125o)는 3차원의 위치 정보(예를 들어,xyz 좌표)를 포함할 수 있다. 프로세서는 카메라를 통해 이미지 프레임을 획득할 수 있다. 프로세서는 획득된 이미지 프레임 에서 특징 정보를 검출하고, 특징 정보를 통해 사용자의 손을 나타내는 영역(1125h)를 검출할 수 있다. 프로 세서는 검출된 손의 사이즈를 기설정된 값(예를 들어, 사용자들의 손에 대한 평균 값)으로 설정할 수 있다. 이 경우, 프로세서는 표 1과 같은 대응 관계를 통해 사용자의 손의 사이즈 및 손의 픽셀 사이 즈에 대응되는 사용자의 손의 거리를 식별할 수 있다. 프로세서는 이미지 프레임 상의 손의 xy 좌표 및 거리를 결합하여 손의 3차원 위치 정보를 식별할 수 있다. 그리고, 프로세서는 손의 3차원 위치 및 제2 타입의 오브젝트(1125o)는 3차원의 위치를 비교하여, 손 및 제2 타입의 오브젝트(1125o) 사이의 거리가 기설정된 값 미만이면, 손의 제2 타입의 오브젝트(1125 o)에 대한 인터렉션이 발생한 것으로 판단할 수 있다. 여기서, 프로세서는 손의 제2 타입의 오브젝트(1125o)에 대한 인터렉션이 발생한 것으로 판단되면, 제2 타입의 오브젝트(1125o)의 z 축상의 거리 값을 손의 z 축상의 거리로 추정(또는 보정)할 수 있다. 즉, 손 의 거리(z 축상의 거리)가 재조정될 수 있다. 이 경우, 프로세서는 표 1과 같은 대응 관계를 통해 재조정된 손의 거리 및 손의 픽셀 사이즈에 대 응되는 손의 사이즈를 식별할 수 있다. 즉, 손의 사이즈는 기설정된 값에서 식별된 값으로 보정될 수 있 다. 한편, 본 개시의 일 실시 예에 따르면, 전자 장치는 사용자의 손의 사이즈를 사용자 계정(account) 별 로 저장하여 관리할 수 있다. 즉, 전자 장치는 특정한 사용자의 손의 사이즈를 해당 사용자의 사용자 계정에 함께 저장(또는 업데이트)할 수 있다. 이후 전자 장치는 사용자 계정에 접근하여 사용자 계정에 함 께 저장된 손의 사이즈를 로딩할 수 있다. 특정한 사용자 계정에 접근하기 위해서, 전자 장치는 인증 과정을 수행할 수 있다. 예를 들어, 전자 장치는 기 등록된 패스워드(password)(예: 문자, 숫자, 기호, 패 턴, 제스처 등)가 입력된 경우에 패스워드에 대응되는 사용자 계정에 접근할 수 있다. 또 다른 예를 들어, 전자 장치는 기 등록된 생체 정보(예: 지문, 망막, 얼굴, 손의 형상 등)가 입력된 경우에 생체 정보에 대응되는 사용자 계정에 접근할 수 있다. 도 12a는 본 개시의 일 실시 예에 따른 전자 장치의 부가적인 구성을 설명하기 위한 블록도이다. 도 12a를 참조하면, 본 개시의 일 실시 예에 따른 전자 장치는 디스플레이, 카메라 및 프로세서 외에도 입력 인터페이스, 출력 인터페이스, 메모리, 센서, 통신부, 전원부 중에서 적어도 하나를 더 포함할 수 있다. 입력 인터페이스는 다양한 사용자 명령을 수신하여 프로세서로 전달할 수 있다. 즉, 프로세서는 입력 인터페이스를 통해 사용자로부터 입력된 사용자 명령을 인지할 수 있다. 여기서, 사용자 명령은 사용 자의 터치 입력(터치 패널), 키 또는 버튼을 누르는 입력, 사용자가 발화하는 음성 입력 등 다양한 방식으로 구 현될 수 있다. 출력 인터페이스는 스피커를 더 포함할 수 있다. 스피커는 오디오 처리부(미도시)에 의해 디코딩이나 증폭, 노이즈 필터링과 같은 다양한 처리 작업이 수행된 각종 오디오 데이터뿐만 아니라 각종 알림음이나 음성 메시지를 소리로 출력할 수 있다. 메모리는 전자 장치의 구성요소들의 전반적인 동작을 제어하기 위한 운영체제(OS: Operating System) 및 전자 장치의 구성요소와 관련된 다양한 데이터를 저장하기 위한 구성이다. 이를 위해, 메모리는 데이터 또는 정보를 일시적 또는 영구적으로 저장하는 하드웨어로 구성될 수 있다. 예를 들어, 메모리는 비휘발성 메모리, 휘발성 메모리, 플래시메모리(Flash Memory), 하드디스크 드라이브 (HDD) 또는 솔리드 스테이트 드라이브(SSD), RAM, ROM 등 중에서 적어도 하나의 하드웨어로 구현될 수 있다. 센서는 카메라, 마이크, 근접 센서, 조도 센서, 모션 센서, ToF 센서, GPS 센서 등 다양한 센서로 구현될 수 있다. 예를 들어, 카메라는 빛을 픽셀 단위로 구분하고, 각 픽셀마다 R(Red), G(Green), B(Blue) 색상에 대 한 빛의 세기를 감지하여, 빛의 세기를 전기적 신호로 변환하여 객체의 색상, 형상, 명암 등을 표현하는 데이터 를 획득할 수 있다. 이때, 데이터의 타입은 복수의 픽셀 각각에 대해 R, G, B 색상 값을 갖는 이미지일 수 있다. 마이크는 사용자의 음성과 같은 음파를 감지하여, 음파를 전기적 신호로 변환하여 데이터를 획득할 수 있 다. 이때, 데이터의 타입은 다양한 포맷의 오디오 신호일 수 있다. 근접 센서(proximity sensor)는 주변 물체의존재를 감지하여, 주변 물체의 존재 여부 또는 주변 물체의 근접 여부에 대한 데이터를 획득할 수 있다. 조도 센서는 전자 장치의 주변 환경에 대한 광량(또는 밝기)을 감지하여, 조도에 대한 데이터를 획득할 수 있다. 모션 센서는 전자 장치의 이동 거리, 이동 방향, 기울기 등을 감지할 수 있다. 이를 위해, 모션 센 서는 가속도 센서, 자이로(gyro) 센서, 지자기 센서 등의 결합으로 구현될 수 있다. TOF(Time Of Flight) 센서 는 특정한 속도를 갖는 다양한 전자기파(예: 초음파, 적외선, 레이저, UWB(Ultra-Wideband) 등)를 방출한 후 되 돌아오는 비행 시간을 감지하여, 대상과의 거리(또는 위치)에 대한 데이터를 획득할 수 있다. GPS(Global Positioning System) 센서는 복수의 위성으로부터 전파 신호를 수신하고, 수신된 신호의 전달 시간을 이용하여 각 위성과의 거리를 각각 산출하고, 산출된 거리를 삼각측량을 이용하여 전자 장치의 현재 위치에 대한 데 이터를 획득할 수 있다. 다만, 상술한 센서의 구현 예는 일 실시 예일 뿐이며, 이에 제한되지 아니하고 다 양한 유형의 센서로 구현되는 것이 가능하다 할 것이다. 통신부는 다양한 유형의 통신 방식에 따라 다양한 유형의 외부 장치와 통신을 수행하여 다양한 유형의 데 이터를 송수신할 수 있다. 통신부는 다양한 방식의 무선 통신을 수행하는 회로로서 블루투스 모듈(블루투 스 또는 블루투스 저전력 방식), 와이파이 모듈(와이파이 방식), 무선 통신 모듈(3G, 4G, 5G 등의 셀룰러 방 식), NFC(Near Field Communication) 모듈(NFC 방식), 적외선 모듈(적외선 방식), Zigbee 모듈(Zigbee 방식), 초광대역 모듈(Ultra-wideband(UWB) 방식) 및 초음파 모듈(초음파 방식) 등과 유선 통신을 수행하는 이더넷 모 듈, USB 모듈, HDMI(High Definition Multimedia Interface), DP(DisplayPort), D-SUB(D-subminiature), DVI(Digital Visual Interface), 썬더볼트(Thunderbolt) 및 컴포넌트 중 적어도 하나를 포함할 수 있다. 전원부는 전자 장치의 각 구성에 대해 전원을 공급하거나 차단할 수 있다. 전원부는 전원을 공 급하기 위한 배터리를 포함할 수 있으며, 배터리는 유선 충전 방식 또는 무선 충전 방식에 따라 충전될 수 있다. 도 12b는 본 개시의 일 실시 예에 따른 전자 장치의 구현 예를 설명하기 위한 도면이다. 도 12b를 참조하면, 본 개시의 다양한 실시예에 따른 전자 장치는 안경 형태로 구현될 수 있다. 다만, 이 는 일 실시 예일 뿐이며, 전자 장치는 고글(goggles), 헬멧, 모자, 스마트폰 등의 다양한 형태의 전자 장 치로 구현될 수 있다. 다양한 실시예에서, 전자 장치는 사용자의 머리 부분에 착용되어, 사용자에게 증강현실 서비스와 관련된 영상을 제공할 수 있다. 일 실시예에 따르면, 전자 장치는 사용자의 시야각(FoV, field of view)으로 판단 되는 영역에 적어도 하나의 가상 객체가 겹쳐 보이도록 출력하는 증강 현실 서비스를 제공할 수 있다. 예를 들 어, 사용자의 시야각으로 판단되는 영역은 전자 장치를 착용한 사용자가 전자 장치를 통해 인지할 수 있다고 판단되는 영역을 의미할 수 있다. 일 실시예에 따른 전자 장치는 지지부(예: 제1 지지부, 및/또는 제2 지지부) 및 본체부로 구분될 수 있다. 전자 장치의 본체부와 지지부(101, 102)는 작동적으로 연결된 상태일 수 있다. 예를 들어, 본체부와 지지부(101, 102)는 힌지부를 통해 작동적으로 연결될 수 있다. 본체부는 사용 자의 코에 거치될 수 있고, 적어도 하나의 글래스(111, 112), 디스플레이 모듈 및 카메라를 포함할 수 있다. 지지부(101, 102)는 사용자의 귀에 거치되는 지지 부재를 포함하고, 왼쪽 귀에 거치되는 제1 지지부 및/또는 오른쪽 귀에 거치되는 제2 지지부를 포함할 수 있다. 전자 장치는 사용자의 양안(예: 좌안, 우안) 각각에 대응하는 복수의 글래스(예: 제1 글래스, 제2 글 래스)를 포함할 수 있다. 일 실시 예를 들어, 복수의 글래스(111, 112) 각각은 상술한 디스플레이로 기능할 수 있다. 이를 위해, 복 수의 글래스(111, 112) 각각은 액정 표시 장치(liquid crystal display; LCD), 디지털 미러 표시 장치(digital mirror device; DMD), 실리콘 액정 표시 장치(liquid crystal on silicon; LCoS), 유기 발광 다이오드(organic light emitting diode; OLED) 및 마이크로 엘이디(micro light emitting diode; micro LED) 중에서 적어도 하 나를 포함할 수 있다. 이 경우, 전자 장치는 영상을 표시하도록 디스플레이 패널을 구동할 수 있다. 이때, 복수의 글래스(111, 112) 각각은 하나의 투명 디스플레이로서 기능할 수 있다. 또 다른 실시 예를 들어, 디스플레이는 프로젝터 방식에 의해 영상을 표시할 수 있다. 이를 위해, 디스플 레이는 사용자의 눈으로 영상에 해당하는 빛을 제공하는 디스플레이 모듈(예: 프로젝터, 백라이트 유 닛 등)을 포함할 수 있다. 이 경우, 디스플레이 모듈는 각 글래스(111, 112)의 측면, 각 글래스(111, 11 2)의 연결부 등에 위치할 수 있으나, 이는 일 실시 예일 뿐 다양하게 변형될 수 있다. 보다 구체적으로, 일 실시예에 따르면, 전자 장치는 적어도 하나의 글래스(예: 제1 글래스 및 제2 글 래스)를 포함할 수 있다. 이때, 적어도 하나의 글래스(예: 제1 글래스 및 제2 글래스)는 집광 렌즈(미도시) 및/또는 투명 도파관(미도시)를 포함할 수 있다. 예를 들어, 투명 도파관은 글래스의 적어도 일부 에 위치할 수 있다. 일 실시예에 따르면, 디스플레이 모듈에서 방출된 광은 제1 글래스 및 제2 글래 스를 통해, 글래스의 일단으로 입광될 수 있고, 입광된 광이 글래스 내에 형성된 도파관 및/또는 광도파로 (예: waveguide)를 통해 사용자에게 전달될 수 있다. 도파관은 글래스, 플라스틱, 또는 폴리머로 제작될 수 있 으며, 내부 또는 외부의 일표면에 형성된 나노 패턴, 예를 들어, 다각형 또는 곡면 형상의 격자 구조(grating structure)를 포함할 수 있다. 일 실시예에 따르면, 입광된 광은 나노 패턴에 의해 도파관 내부에서 전파 또는 반사되어 사용자에게 제공될 수 있다. 일 실시예에 따르면, 광도파로(waveguide)는 적어도 하나의 회절 요소(예: DOE(diffractive optical element), HOE(holographic optical element)) 또는 반사 요소(예: 반사 거 울, 전반사(total internal reflection; TIR) 부재 등) 중 적어도 하나를 포함할 수 있다. 일 실시예에 따르면, 광도파로는 적어도 하나의 회절 요소 또는 반사 요소를 이용하여 광원부로부터 방출된 광을 사용자의 눈으로 유도할 수 있다. 일 실시예에 따르면, 복수의 글래스(111, 112) 각각은 투명한 성질의 소재로 구현되어 외부의 빛을 투과시킬 수 있다. 즉, 사용자는 복수의 글래스(111, 112)를 통해 실제 공간 또는 외부의 물체를 볼 수 있다. 또한, 전자 장 치는 사용자에게 실제 공간의 적어도 일부에 가상 객체가 덧붙여진 것으로 보여지도록 디스플레이(예: 복수의 글래스(111, 112) 중 적어도 하나)의 일부 영역에 가상 객체를 표시할 수 있다. 일 실시예에 따르면, 디스플레이을 통해 출력되는 가상 객체는 전자 장치에서 실행되는 어플리케이션 프로그램과 관련된 정보 및/또는 사용자의 시야각(FoV, field of view)으로 판단되는 영역에 대응하는 실제 공 간에 위치한 외부 객체와 관련된 정보를 포함할 수 있다. 예를 들어, 전자 장치는 전자 장치의 카메 라를 통해 획득한 실제 공간과 관련된 영상 정보 중 사용자의 시야각(FoV)으로 판단되는 영역에 대응하는 적어도 일부에 포함되는 외부 객체를 확인할 수 있다. 전자 장치는 적어도 일부에서 확인한 외부 객체와 관련된 가상 객체를 전자 장치의 표시 영역 중 사용자의 시야각으로 판단되는 영역을 통해 출력(또는 표시)할 수 있다. 외부 객체는 실제 공간에 존재하는 사물을 포함할 수 있다. 일 실시예에 따르면, 전자 장치는 사용자의 시야각(FoV, field of view)에 대응되는 영상을 촬영하는 카메 라 외에도, 사용자가 바라보는 시선의 방향을 확인하기 위한 시선 추적 카메라(eye tracking camera)을 더 포함할 수 있다. 예를 들어, 카메라는 전자 장치의 전면 방향을 촬영할 수 있고, 시선 추적 카메라는 카메라의 촬영 방향과 반대되는 방향(즉, 전자 장치를 착용한 사용자의 눈이 위치한 방향)을 촬영할 수 있다. 예를 들어, 시선 추적 카메라는 사용자의 양안을 촬영할 수 있다. 일 실시예에 따르면, 전자 장치는 적어도 하나의 발광 장치(illumination LED)를 포함할 수 있다. 예 를 들어, 발광 장치는 빛을 발광할 수 있다. 발광 장치는 조도가 낮은 환경에 빛을 제공함으로써 카 메라가 획득하는 이미지의 정확도를 높이기 위한 보조 수단으로 사용될 수 있다. 일 실시예에 따르면, 제1 지지부 및 제2 지지부 각각은 입력 인터페이스, 인쇄 회로 기판 (printed circuit board; PCB), 출력 인터페이스(예: 스피커 등) 및 전원부 중 적어도 하나를 포함할 수 있다. 여기서, 입력 인터페이스(예: 마이크 등)는 사용자의 음성 및 주변 소리를 수신할 수 있 다. 인쇄 회로 기판은 전자 장치의 각 구성요소에 전기적 신호를 전달할 수 있다. 출력 인터페이스 은 오디오 신호를 출력할 수 있다. 전원부는 인쇄 회로 기판 등과 같이 전자 장치의 각 구 성요소가 구동하는데 필요한 전력을 공급할 수 있다. 또한, 제1 지지부 및 제2 지지부 각각은 전자 장치의 본체부에 결합하기 위한 힌지부를 포함할 수 있다. 도 13은 본 개시의 일 실시 예에 따른 흐름도를 설명하기 위한 도면이다. 도 13을 참조하면, AR(Augmented Reality) 컨텐츠를 제공하는 전자 장치의 제어 방법은, 디스플레이(11 0)에 AR 컨텐츠를 표시하는 단계(S1310), 카메라를 통해 획득된 이미지 프레임들에서 사용자의 손을 검출 하는 단계(S1320), 손의 사이즈에 기초하여 AR 컨텐츠에 대한 손의 인터렉션을 식별하는 단계(S1330)를 포함할 수 있다. 여기서, 손의 사이즈는 디스플레이를 통해 제공되는 오브젝트에 대한 손의 인터렉션이 발생되면, 오브젝트에 대한 정보에 기초하여 획득될 수 있다. 구체적으로, 제어 방법은 디스플레이에 AR 컨텐츠를 표시할 수 있다(S1310). 그리고, 카메라를 통해 획득된 이미지 프레임들에서 사용자의 손을 검출할 수 있다(S1320). 본 개시의 일 실시 예로서, 검출된 손의 사이즈를 기설정된 값으로 설정할 수 있다. 이 경우, 디스플레이 를 통해 제공되는 오브젝트에 대한 손의 인터렉션이 발생되면, 오브젝트에 대한 정보에 기초하여 손의 사이즈를 획득할 수 있다. 본 개시의 일 실시 예로서, 검출된 손의 사이즈를 기설정된 값으로 설정할 수 있다. 이 경우, 설정된 사이즈에 기초하여 디스플레이를 통해 제공되는 오브젝트에 대한 손의 인터렉션이 발생되었는지를 식별할 수 있다. 그리고, 인터렉션이 발생된 것으로 식별되면, 오브젝트에 대한 정보에 기초하여 손의 사이즈를 식별할 수 있다. 본 개시의 일 실시 예로서, 오브젝트는, 카메라를 통해 획득된 이미지 프레임에 포함된 제1 타입의 오브젝 트 및 디스플레이에 표시되는 AR 컨텐츠에 포함된 제2 타입의 오브젝트 중 적어도 하나를 포함할 수 있다. 본 개시의 일 실시 예로서, 카메라를 통해 획득된 이미지 프레임들에서 제1 타입의 오브젝트 및 사용자의 손이 검출되면, 전자 장치에 저장된 레퍼런스 오브젝트의 특징 정보 및 검출된 제1 타입의 오브젝트의 특 징 정보에 기초하여 제1 타입의 오브젝트가 레퍼런스 오브젝트인지를 식별하는 단계 및 제1 타입의 오브젝트가 레퍼런스 오브젝트인 것으로 식별되면, 전자 장치에 저장된 레퍼런스 오브젝트의 사이즈에 기초하여 손의 사이즈를 식별하는 단계를 더 포함할 수 있다. 본 개시의 일 실시 예로서, 전자 장치에 저장된 특징 정보 및 검출된 제1 타입의 오브젝트의 특징 정보에 기초하여 제1 타입의 오브젝트가 레퍼런스 오브젝트에 해당하지 않은 것으로 식별되면, 카메라를 통해 획 득된 연속된 이미지 프레임들을 이용하여 연속된 이미지 프레임들에 포함된 제1 타입의 오브젝트의 사이즈를 식 별하는 단계 및 식별된 제1 타입의 오브젝트의 사이즈에 기초하여 손의 사이즈를 식별하는 단계를 더 포함할 수 있다. 본 개시의 일 실시 예로서, 검출된 손의 사이즈를 기설정된 값으로 설정하는 단계, 설정된 사이즈에 기초하여 디스플레이를 통해 제공되는 제2 타입의 오브젝트에 대한 손의 인터렉션이 발생되었는지를 식별하는 단계 및 손의 인터렉션이 식별된 제2 타입의 오브젝트의 뎁스에 기초하여 손의 사이즈를 식별하는 단계를 더 포함할 수 있다. 그리고, 손의 사이즈에 기초하여 AR 컨텐츠에 대한 손의 인터렉션을 식별할 수 있다(S1330). 이상과 같은 본 개시의 다양한 실시 예에 따르면, 카메라를 이용하여 보다 정확하게 오브젝트와의 거리를 추정 하는 전자 장치 및 그의 제어 방법을 제공할 수 있다. 본 개시의 일 실시 예에 따르면, 사용자의 손의 사이즈를 정확히 추정할 수 있고, 사용자의 손에 대한 파라미터 를 정확하게 추정할 수 있다. 본 개시의 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는 저장 매체로부터 저장된 명령 어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실시 예들에 따른 전자 장치(예: 전자 장치)를 포함할 수 있다. 상기 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 상기 프로세 서의 제어 하에 다른 구성요소들을 이용하여 상기 명령에 상기하는 기능을 수행할 수 있다. 명령은 컴파일러 또 는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장매체는 비일시적 (non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장매체가 신호(signal)를 포함하 지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임시적으로 저장됨을 구분 하지 않는다. 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽 을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구성될 수 있으 며, 전술한 상기 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로그램)은 하나 의 개체로 통합되어, 통합되기 이전의 각각의 상기 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시 예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다."}
{"patent_id": "10-2020-0188276", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1a는 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 도면이다. 도 1b는 본 개시의 일 실시 예에 따른 사용자의 손의 파라미터를 설명하기 위한 도면이다. 도 1c는 본 개시의 일 실시 예에 따른 사용자의 손의 사이즈를 설명하기 위한 도면이다. 도 2는 본 개시의 일 실시 예에 따른 전자 장치의 구성을 설명하기 위한 블록도이다. 도 3은 본 개시의 일 실시 예에 따른 프로세서의 동작을 설명하기 위한 도면이다. 도 4는 본 개시의 일 실시 예에 따른 초점 길이를 통해 손의 사이즈를 식별하는 방법을 설명하기 위한 도면이다. 도 5는 본 개시의 일 실시 예에 따른 초점 길이 및 초점 거리 간의 관계를 설명하기 위한 도면이다. 도 6은 본 개시의 일 실시 예에 따른 초점 길이를 통해 손의 사이즈를 식별하는 방법을 설명하기 위한 도면이다. 도 7은 본 개시의 일 실시 예에 따른 손과 인터렉션한 오브젝트를 통해 손의 사이즈를 식별하는 방법을 설명하 기 위한 도면이다. 도 8a는 본 개시의 일 실시 예에 따른 손과 인터렉션한 오브젝트를 검출하는 방법을 설명하기 위한 도면이다. 도 8b는 본 개시의 일 실시 예에 따른 손과 인터렉션한 오브젝트를 검출하는 방법을 설명하기 위한 도면이다. 도 9는 본 개시의 일 실시 예에 따른 제1 타입의 오브젝트를 통해 손의 사이즈를 식별하는 방법을 설명하기 위 한 도면이다. 도 10은 본 개시의 일 실시 예에 따른 제1 타입의 오브젝트를 통해 손의 사이즈를 식별하는 방법을 설명하기 위 한 도면이다. 도 11는 본 개시의 일 실시 예에 따른 제2 타입의 오브젝트를 통해 손의 사이즈를 식별하는 방법을 설명하기 위 한 도면이다. 도 12a는 본 개시의 일 실시 예에 따른 전자 장치의 부가적인 구성을 설명하기 위한 블록도이다. 도 12b는 본 개시의 일 실시 예에 따른 전자 장치의 구현 예를 설명하기 위한 도면이다. 도 13은 본 개시의 일 실시 예에 따른 흐름도를 설명하기 위한 도면이다."}
