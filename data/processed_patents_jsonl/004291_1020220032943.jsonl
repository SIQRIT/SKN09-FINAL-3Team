{"patent_id": "10-2022-0032943", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0135439", "출원번호": "10-2022-0032943", "발명의 명칭": "증강 현실 디바이스 및 그 동작 방법", "출원인": "삼성전자주식회사", "발명자": "자리크니 이호르"}}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "현실 공간에 관한 복수의 이미지 프레임을 획득하는 카메라; 디스플레이부;기 설정된 객체에 관한 정보를 저장하는 메모리; 및 적어도 하나의 프로세서; 를 포함하고, 상기 적어도 하나의 프로세서는, 상기 카메라를 통해 획득된 복수의 이미지 프레임 중 제1 이미지 프레임으로부터 상기 기 설정된 객체에 관한정보에 대응되는 적어도 하나의 기 설정된 객체를 인식하고, 상기 제1 이미지 프레임으로부터 상기 인식된 적어도 하나의 기 설정된 객체의 영역을 분할함으로써 마스크(mask)를 획득하고, 상기 마스크를 이용하여 상기 제1 이미지 프레임 이후 획득된 제2 이미지 프레임을 렌더링함으로써, 상기 적어도 하나의 기 설정된 객체의 영역을 제외한 영역이 블러(blur) 처리된 블러 이미지(blur image)를 획득하고, 상기 블러 이미지를 상기 디스플레이부를 통해 디스플레이하는, 증강 현실 디바이스."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 적어도 하나의 프로세서는, 객체 인식 모델을 이용하여 상기 제1 이미지 프레임으로부터 복수의 객체를 인식하고, 상기 복수의 객체 중 상기 적어도 하나의 기 설정된 객체를 식별하고, 상기 적어도 하나의 기 설정된 객체는 사용자의 작업과 관련된 객체로서 기 정의된 사용자 정의 객체(user-defined object)인, 증강 현실 디바이스."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 적어도 하나의 프로세서는, 객체 추적 알고리즘(object tracking algorithm)을 이용하여 상기 제1 이미지 프레임 보다 이전 이미지 프레임에서 인식된 적어도 하나의 객체를 트래킹함으로써, 상기 제1 이미지 프레임으로부터 상기 적어도 하나의 기 설정된 객체를 인식하는, 증강 현실 디바이스."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 인식된 적어도 하나의 기 설정된 객체 중 중요 객체를 선택하는 사용자 입력을 수신하는 사용자 입력부;공개특허 10-2023-0135439-3-를 더 포함하고, 상기 적어도 하나의 프로세서는, 상기 사용자 입력부를 통해 수신된 사용자 입력에 기초하여 선택된 상기 중요객체에 대응되는 영역을 상기 제1 이미지 프레임으로부터 분할함으로써 상기 마스크를 획득하는, 증강 현실 디바이스."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 적어도 하나의 프로세서는, 상기 제2 이미지 프레임과 상기 마스크를 컨볼루션(convolution)함으로써, 상기 제2 이미지 프레임의 전체 영역중 상기 적어도 하나의 기 설정된 객체에 대응되는 영역을 제외한 주변 영역을 블러 처리하는, 증강 현실 디바이스."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 적어도 하나의 프로세서는, 기 설정된 작업 시간 동안에만 이미지 렌더링을 수행하여, 상기 블러 이미지를 획득하고,상기 기 설정된 작업 시간 동안에만 상기 블러 이미지를 상기 디스플레이부를 통해 디스플레이하는, 증강 현실디바이스."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 적어도 하나의 프로세서는, 상기 블러 처리된 영역을 작업의 종류 또는 작업 환경에 대응되는 서로 다른 컬러와 합성하는, 증강 현실 디바이스."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항에 있어서,상기 적어도 하나의 프로세서는, 상기 블러 처리된 영역에 작업과 관련된 정보를 제공하는 가상 객체 또는 그래픽 UI(Graphic User Interface)를합성하는, 증강 현실 디바이스."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 항에 있어서,상기 블러 처리된 영역에 대한 블러 처리 정도, 컬러, 및 밝기 중 적어도 하나를 포함하는 블러 옵션(bluroption)을 결정하는 사용자 입력을 수신하는 사용자 입력부;를 더 포함하고, 상기 적어도 하나의 프로세서는, 상기 사용자 입력부를 통해 수신된 사용자 입력에 의해 결정된 블러 옵션에 기공개특허 10-2023-0135439-4-초하여 상기 영역을 블러 처리하는 이미지 렌더링을 수행하는, 증강 현실 디바이스."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항에 있어서,상기 제1 이미지 프레임으로부터 인식된 복수의 객체 중 적어도 하나의 객체를 선택하는 사용자 입력을 수신하는 사용자 입력부; 를 더 포함하고, 상기 적어도 하나의 프로세서는, 상기 사용자 입력부를 통해 수신된 사용자 입력에 의해 선택된 상기 적어도 하나의 객체를 블러 처리하는 이미지 렌더링을 수행함으로써, 상기 블러 이미지를 획득하는, 증강 현실 디바이스."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "증강 현실 디바이스의 동작 방법에 있어서, 카메라를 이용하여 현실 공간을 촬영하여 복수의 이미지 프레임을 획득하는 단계; 상기 복수의 이미지 프레임 중 제1 이미지 프레임으로부터 기 설정된 객체에 관한 정보에 대응되는 적어도 하나의 기 설정된 객체를 인식하는 단계; 상기 제1 이미지 프레임으로부터 상기 인식된 적어도 하나의 기 설정된 객체의 영역을 분할함으로써 마스크(mask)를 획득하는 단계;상기 마스크를 이용하여 상기 제1 이미지 프레임 이후 획득된 제2 이미지 프레임을 렌더링함으로써, 상기 적어도 하나의 기 설정된 객체의 영역을 제외한 영역이 블러(blur) 처리된 블러 이미지(blur image)를 획득하는 단계; 및 상기 블러 이미지를 디스플레이하는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 적어도 하나의 기 설정된 객체를 인식하는 단계는,객체 인식 모델을 이용하여 상기 제1 이미지 프레임으로부터 복수의 객체를 인식하는 단계; 및상기 복수의 객체 중 상기 적어도 하나의 기 설정된 객체를 식별하는 단계;를 포함하고,상기 적어도 하나의 기 설정된 객체는 사용자의 작업과 관련된 객체로서 기 정의된 사용자 정의 객체(user-defined object)인, 방법."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11 항에 있어서,상기 마스크를 획득하는 단계는,사용자 입력에 기초하여, 상기 인식된 적어도 하나의 기 설정된 객체 중 중요 객체를 선택하는 단계; 및공개특허 10-2023-0135439-5-상기 제1 이미지 프레임으로부터 상기 선택된 중요 객체에 대응되는 영역을 분할함으로써, 상기 마스크를 획득하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11 항에 있어서,블러 이미지를 획득하는 단계는, 상기 제2 이미지 프레임과 상기 마스크를 컨볼루션(convolution)함으로써, 상기 제2 이미지 프레임의 전체 영역중 상기 적어도 하나의 기 설정된 객체에 대응되는 영역을 제외한 주변 영역을 블러 처리하는, 방법."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11 항에 있어서,상기 블러 이미지를 획득하는 단계는, 기 설정된 작업 시간 동안에만 이미지 렌더링을 수행하여, 상기 블러 이미지를 획득하고,상기 블러 이미지를 디스플레이하는 단계는,상기 기 설정된 작업 시간 동안에만 상기 블러 이미지를 디스플레이하는, 방법."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11 항에 있어서,상기 블러 이미지를 획득하는 단계는, 상기 블러 처리된 영역을 작업의 종류 또는 작업 환경에 대응되는 서로 다른 컬러와 합성하는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11 항에 있어서,상기 블러 이미지를 획득하는 단계는, 상기 블러 처리된 영역에 작업과 관련된 정보를 제공하는 가상 객체 또는 그래픽 UI(Graphic User Interface)를합성하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11 항에 있어서,상기 블러 이미지를 획득하는 단계는, 사용자 입력에 기초하여 블러 처리 정도, 컬러, 및 밝기 중 적어도 하나를 포함하는 블러 옵션(blur option)을결정하는 단계; 및 상기 사용자 입력에 의해 결정된 블러 옵션에 기초하여 상기 영역을 블러 처리하는 이미지 렌더링을 수행하는공개특허 10-2023-0135439-6-단계;를 포함하는, 방법."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11 항에 있어서,사용자 입력에 기초하여 상기 제1 이미지 프레임으로부터 인식된 복수의 객체 중 적어도 하나의 객체를 선택하는 단계;를 더 포함하고, 상기 블러 이미지를 획득하는 단계는, 상기 사용자 입력에 의해 선택된 적어도 하나의 객체를 블러 처리하는 이미지 렌더링을 수행하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2022-0032943", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제11 항 내지 제19 항 중 어느 하나의 항에 기재된 방법을 구현하기 위한 적어도 하나의 프로그램이 기록된 컴퓨터로 판독 가능한 기록 매체."}
{"patent_id": "10-2022-0032943", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "사용자의 작업에 관한 집중력 및 몰입도를 향상시킬 수 있는 증강 현실 디바이스 및 그 동작 방법을 제공한다. 본 개시의 일 실시예에 따른 증강 현실 디바이스는 카메라를 이용하여 현실 공간을 촬영하여 복수의 이미지 프레 임을 획득하고, 복수의 이미지 프레임으로부터 기 설정된 객체를 인식하고, 복수의 이미지 프레임의 전체 영역 중 적어도 하나의 기 설정된 객체의 영역을 제외한 영역이 블러(blur) 처리된 블러 이미지(blur image)를 획득하 고, 블러 이미지를 디스플레이할 수 있다."}
{"patent_id": "10-2022-0032943", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 시야각(Field Of View, FOV) 내의 현실 공간에 관한 이미지를 수정하여 디스플레이하는 증강 현실 디 바이스 및 그 동작 방법에 관한 것이다. 구체적으로, 본 개시는 사용자의 작업 공간 내의 일부 영역을 블러 처 리하여 디스플레이하는 증강 현실 디바이스 및 그 동작 방법에 관한 것이다."}
{"patent_id": "10-2022-0032943", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "증강 현실(Augmented Reality)은 현실 세계의 물리적 환경 공간이나 현실 객체(real world object) 상에 가상 이미지를 오버레이(overlay)하여 함께 보여주는 기술로서, 증강 현실 기술을 활용한 증강 현실 디바이스(예를 들어, 스마트 글래스(Smart Glass) 가 정보 검색, 길 안내, 카메라 촬영과 같이 일상 생활에서 유용하게 사용되 고 있다. 특히, 스마트 글래스는 패션 아이템으로도 착용되고, 실외 활동에 주로 사용되고 있다. 증강 현실 디바이스는 일반적으로, 사용자에 착용된 상태에서 사용자의 눈에 가깝게 배치된 시스루 디스플레이 (see-through display)를 통해 장면을 보게 된다. 여기서, 장면은 사용자가 눈을 통해 직접 보는 물리적 환경 또는 공간 내의 하나 이상의 현실 객체를 포함한다. 사용자는 증강 현실 디바이스의 시스루 디스플레이를 통해 현실 객체와 가상 이미지를 동시에 볼 수 있다. 한편, 사용자는 작업 공간에서 다양한 객체를 이용하여 작업을 수행할 수 있다. 작업 공간에서 사용자의 시야 내에 여러 객체가 있는 경우, 복수의 객체 중 작업과 직접적으로 관련되지 않은 객체, 예를 들어 모바일 폰 또 는 태블릿 PC 등은 메시지 알림, 인터넷 검색, 인터넷 쇼핑 등을 통해 사용자의 주의를 끌 수 있다. 사용자의 시야 내의 작업과 관련없는 객체들로 인하여 사용자의 작업은 방해받을 수 있고, 작업에 관한 집중력은 저하될 수 있다. 작업과 관련 없는 객체들에 의한 집중력 저하 문제를 해결하기 위하여 증강 현실 디바이스를 착용한 상태에서 작업을 수행하는 방안을 고려할 수 있다."}
{"patent_id": "10-2022-0032943", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2022-0032943", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 사용자의 시야 내의 복수의 객체들 중 작업과 관련없는 적어도 하나의 객체가 배치된 영역을 블러 처 리함으로써, 사용자의 작업에 관한 집중력 및 몰입도를 향상시킬 수 있는 증강 현실 디바이스 및 그 동작 방법 을 제공한다. 본 개시의 일 실시예에 따른 증강 현실 디바이스는 인공지능 모델(AI 모델)을 이용하는 객체 인식 (object detection), 객체 추적(object tracking), 및 객체 분할(object segmentation)을 수행함으로써, 작업 과 관련된 적어도 하나의 객체를 제외한 주변 영역을 블러 처리하는 이미지 렌더링(image rendering)을 수행할 수 있다."}
{"patent_id": "10-2022-0032943", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 해결하기 위하여 본 개시는 일 실시예는, 현실 공간에 관한 복수의 이미지 프레임을 획득 하는 카메라, 디스플레이부, 기 설정된 객체에 관한 정보를 저장하는 메모리, 및 적어도 하나의 프로세서를 포 함하고, 상기 적어도 하나의 프로세서는 상기 카메라를 통해 획득된 복수의 이미지 프레임 중 제1 이미지 프레 임으로부터 상기 기 설정된 객체에 관한 정보에 대응되는 적어도 하나의 기 설정된 객체를 인식하고, 상기 제1 이미지 프레임으로부터 상기 인식된 적어도 하나의 기 설정된 객체 영역을 분할함으로써, 마스크(mask)를 획득 하고, 상기 마스크를 이용하여 상기 제1 이미지 프레임 이후 획득된 제2 이미지 프레임을 렌더링함으로써, 상기 적어도 하나의 기 설정된 객체 영역을 제외한 영역이 블러(blur) 처리된 블러 이미지(blur image)를 획득하고, 상기 블러 이미지를 상기 디스플레이부를 통해 디스플레이하는 증강 현실 디바이스를 제공한다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 객체 인식 모델을 이용하여 상기 제1 이미지 프레임 으로부터 복수의 객체를 인식하고, 상기 복수의 객체 중 상기 적어도 하나의 기 설정된 객체를 식별하고, 상기 적어도 하나의 기 설정된 객체는 사용자의 작업과 관련된 객체로서 기 정의된 사용자 정의 객체(user-defined object)일 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 객체 추적 알고리즘(object tracking algorithm)을 이용하여 상기 제1 이미지 프레임 보다 이전 이미지 프레임에서 인식된 적어도 하나의 객체를 트래킹함으로써, 상기 제1 이미지 프레임으로부터 상기 적어도 하나의 기 설정된 객체를 인식할 수 있다. 본 개시의 일 실시예에서, 상기 증강 현실 디바이스는 인식된 적어도 하나의 기 설정된 객체 중 중요 객체를 선 택하는 사용자 입력을 수신하는 사용자 입력부를 더 포함하고, 상기 적어도 하나의 프로세서는 상기 사용자 입 력부를 통해 수신된 사용자 입력에 기초하여 선택된 상기 중요 객체에 대응되는 영역을 상기 제1 이미지 프레임 으로부터 분할함으로써 상기 마스크를 획득할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 상기 제2 이미지 프레임과 상기 마스크를 컨볼루션 (convolution)함으로써, 상기 제2 이미지 프레임의 전체 영역 중 상기 적어도 하나의 기 설정된 객체에 대응되 는 영역을 제외한 주변 영역을 블러 처리할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 기 설정된 작업 시간 동안에만 이미지 렌더링을 수행 하여, 상기 블러 이미지를 획득하고, 상기 기 설정된 작업 시간 동안에만 상기 블러 이미지를 상기 디스플레이 부를 통해 디스플레이할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 상기 블러 처리된 영역을 작업의 종류 또는 작업 환 경에 대응되는 서로 다른 컬러와 합성할 수 있다. 본 개시의 일 실시예에서, 상기 적어도 하나의 프로세서는 상기 블러 처리된 영역에 작업과 관련된 정보를 제공 하는 가상 객체 또는 그래픽 UI(Graphic User Interface)를 합성할 수 있다. 본 개시의 일 실시예에서, 상기 증강 현실 디바이스는 상기 블러 처리된 영역에 대한 블러 처리 정도, 컬러, 및 밝기 중 적어도 하나를 포함하는 블러 옵션(blur option)을 결정하는 사용자 입력을 수신하는 사용자 입력부를 더 포함하고, 상기 적어도 하나의 프로세서는 상기 사용자 입력부를 통해 수신된 사용자 입력에 의해 결정된 블 러 옵션에 기초하여 상기 영역을 블러 처리하는 이미지 렌더링을 수행할 수 있다. 본 개시의 일 실시예에서, 상기 증강 현실 디바이스는 상기 제1 이미지 프레임으로부터 인식된 복수의 객체 중 적어도 하나의 객체를 선택하는 사용자 입력을 수신하는 사용자 입력부를 더 포함하고, 상기 적어도 하나의 프로세서는 상기 사용자 입력부를 통해 수신된 사용자 입력에 의해 선택된 상기 적어도 하나의 객체를 블러 처리 하는 이미지 렌더링을 수행함으로써, 상기 블러 이미지를 획득할 수 있다. 상술한 기술적 과제를 해결하기 위하여 본 개시의 다른 실시예는, 증강 현실 디바이스의 동작 방법을 제공한다. 본 개시의 일 실시예에 따른 증강 현실 디바이스의 동작 방법은, 카메라를 이용하여 현실 공간을 촬영하여 복수 의 이미지 프레임을 획득하는 단계, 상기 복수의 이미지 프레임 중 제1 이미지 프레임으로부터 기 설정된 객체 에 관한 정보에 대응되는 적어도 하나의 기 설정된 객체를 인식하는 단계, 상기 제1 이미지 프레임으로부터 상 기 인식된 적어도 하나의 기 설정된 객체의 영역을 분할함으로써, 마스크(mask)를 획득하는 단계, 상기 마스크 를 이용하여 상기 제1 이미지 프레임 이후 획득된 제2 이미지 프레임을 렌더링함으로써, 상기 적어도 하나의 기 설정된 객체의 영역을 제외한 영역이 블러(blur) 처리된 블러 이미지(blur image)를 획득하는 단계, 및 상기 블 러 이미지를 디스플레이하는 단계를 포함한다. 본 개시의 일 실시예에서, 상기 적어도 하나의 기 설정된 객체를 인식하는 단계는 객체 인식 모델을 이용하여 상기 제1 이미지 프레임으로부터 복수의 객체를 인식하는 단계, 및 상기 복수의 객체 중 상기 적어도 하나의 기 설정된 객체를 식별하는 단계를 포함하고, 상기 적어도 하나의 기 설정된 객체는 사용자의 작업과 관련된 객체 로서 기 정의된 사용자 정의 객체(user-defined object)일 수 있다. 본 개시의 일 실시예에서, 상기 마스크를 획득하는 단계는 사용자 입력에 기초하여, 상기 인식된 적어도 하나의 기 설정된 객체 중 중요 객체를 선택하는 단계, 및 상기 제1 이미지 프레임으로부터 상기 선택된 중요 객체를 분할함으로써, 상기 마스크를 획득하는 단계를 포함할 수 있다. 본 개시의 일 실시예에서, 블러 이미지를 획득하는 단계에서, 상기 제2 이미지 프레임과 상기 마스크를 컨볼루 션(convolution)함으로써, 상기 제2 이미지 프레임의 전체 영역 중 상기 적어도 하나의 기 설정된 객체에 대응 되는 영역을 제외한 주변 영역을 블러 처리할 수 있다. 본 개시의 일 실시예에서, 상기 블러 이미지를 획득하는 단계에서, 기 설정된 작업 시간 동안에만 이미지 렌더 링을 수행하여, 상기 블러 이미지를 획득하고, 상기 블러 이미지를 디스플레이하는 단계에서, 상기 기 설정된 작업 시간 동안에만 상기 블러 이미지를 디스플레이할 수 있다. 본 개시의 일 실시예에서, 상기 블러 이미지를 획득하는 단계는, 상기 블러 처리된 영역을 작업의 종류 또는 작 업 환경에 대응되는 서로 다른 컬러와 합성하는 단계를 포함할 수 있다. 본 개시의 일 실시예에서, 상기 블러 이미지를 획득하는 단계는 상기 블러 처리된 영역에 작업과 관련된 정보를 제공하는 가상 객체 또는 그래픽 UI(Graphic User Interface)를 합성하는 단계를 포함할 수 있다. 본 개시의 일 실시예에서, 상기 블러 이미지를 획득하는 단계는 사용자 입력에 기초하여 블러 처리 정도, 컬러, 및 밝기 중 적어도 하나를 포함하는 블러 옵션(blur option)을 결정하는 단계, 및 상기 사용자 입력에 의해 결 정된 블러 옵션에 기초하여 상기 영역을 블러 처리하는 이미지 렌더링을 수행하는 단계를 포함할 수 있다. 본 개시의 일 실시예에서, 상기 방법은 사용자 입력에 기초하여 상기 제1 이미지 프레임으로부터 인식된 복수의 객체 중 적어도 하나의 객체를 선택하는 단계를 더 포함하고, 상기 블러 이미지를 획득하는 단계는 상기 사용자 입력에 의해 선택된 적어도 하나의 객체를 블러 처리하는 이미지 렌더링을 수행하는 단계를 포함할 수 있다. 상술한 기술적 과제를 해결하기 위하여, 본 개시의 다른 실시예는 컴퓨터에서 실행시키기 위한 프로그램을 기록 한 컴퓨터로 읽을 수 있는 기록매체를 제공한다."}
{"patent_id": "10-2022-0032943", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서의 실시예들에서 사용되는 용어는 본 개시의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 실시예의 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 명세서에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 개시 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 본 명세서에 기재 된 \"...부\", \"...모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 본 개시에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for)\", \"~하는 능력을 가지는(having the capacity to)\", \"~하도록 설계된(designed to)\", \"~하도 록 변경된(adapted to)\", \"~하도록 만들어진(made to)\", 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 시스템\"이라는 표현은, 그 시 스템이 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수 행하도록 구성된(또는 설정된) 프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메모리에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다.또한, 본 개시에서 일 구성요소가 다른 구성요소와 \"연결된다\" 거나 \"접속된다\" 등으로 언급된 때에는, 상기 일 구성요소가 상기 다른 구성요소와 직접 연결되거나 또는 직접 접속될 수도 있지만, 특별히 반대되는 기재가 존 재하지 않는 이상, 중간에 또 다른 구성요소를 매개하여 연결되거나 또는 접속될 수도 있다고 이해되어야 할 것 이다. 본 개시에서, '증강 현실(Augmented Reality)'은 현실 세계(Real world)의 물리적 환경 공간 내에 가상 이미지 를 함께 보여주거나 현실 객체와 가상 이미지를 함께 보여주는 것을 의미한다. 본 개시에서, '증강 현실 디바이스'는 증강 현실을 표현할 수 있는 장치로서, 일반적으로 사용자가 안면부(顔面 部)에 착용하는 안경 형상의 증강 현실 안경 장치(Augmented Reality Glasses) 뿐만 아니라, 두부(頭部)에 착용 하는 헤드 마운트 디스플레이 장치 (HMD : Head Mounted Display Apparatus)나, 증강 현실 헬멧(Augmented Reality Helmet) 등을 포괄한다. 그러나, 이에 한정되는 것은 아니고, 증강 현실 디바이스는 모바일 디바이스, 스마트 폰(smart phone), 노트북 컴퓨터(laptop computer), 데스크 탑, 태블릿 PC, 전자책 단말기, 디지털 방 송용 단말기, PDA(Personal Digital Assistants), PMP(Portable Multimedia Player), 네비게이션, MP3 플레이 어, 캠코더, IPTV(Internet Protocol Television), DTV(Digital Television), 착용형 기기(wearable device) 등과 같은 다양한 전자 장치로 구현될 수 있다. 본 개시에서, '현실 공간(real scene)'이란 사용자가 증강 현실 디바이스를 통해서 보는 현실 세계의 장면으로 서, 현실 객체(real world object)(들)를(을) 포함할 수 있다. 본 개시에서, '가상 이미지(virtual image)'는 광학 엔진을 통해 생성되는 이미지로 정적 이미지와 동적 이미지 를 모두 포함할 수 있다. 이러한 가상 이미지는 현실 장면과 함께 관측되며, 현실 장면 속의 현실 객체에 대한 정보 또는 증강 현실 디바이스의 동작에 대한 정보 등을 나타내는 이미지일 수 있다. 본 개시에서, '가상 객체'는 가상 이미지의 일부 영역을 의미한다. 가상 객체는 현실 객체와 관련된 정보를 나 타낼 수 있다. 가상 객체는 예를 들어, 문자, 숫자, 기호, 아이콘, 이미지, 및 애니메이션 중 적어도 하나를 포 함할 수 있다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 이하에서는 도면을 참조하여 본 개시의 실시예들을 상세하게 설명한다. 도 1은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 현실 공간에 관한 블러 이미지를 디스플레 이하는 동작을 도시한 개념도이다. 도 1을 참조하면, 증강 현실 디바이스는 카메라 및 안경 렌즈를 포함할 수 있다. 증강 현실 디 바이스는 카메라를 이용하여 현실 공간을 촬영함으로써, 현실 공간에 관한 시계열적인 복수의 이미지 프레임을 획득할 수 있다. 본 개시의 일 실시예에서, 현실 공간은 사용자가 작업을 수행하는 작업 공간일 수 있 으나, 이에 한정되는 것은 아니다. 작업 공간에는 복수의 현실 객체(1 내지 9)가 배치되어 있을 수 있다. 복수 의 현실 객체(1 내지 9)는 적어도 하나의 기 설정된 객체를 포함할 수 있다. 본 개시의 일 실시예에서, 기 설정 된 객체는 사용자 정의 객체일 수 있다. 본 개시에서, '사용자 정의 객체(user-defined object)'는 사용자의 작 업과 관련된 객체로서 미리 정의되거나, 결정된 객체를 의미한다. 사용자 정의 객체는 하나 또는 그 이상일 수 있다. 사용자 정의 객체는 사용자의 선택에 의해 정의될 수 있다. 사용자 정의 객체는 예를 들어, 노트 북 PC(laptop computer), 컴퓨터 용 모니터, 키보드, 또는 마우스 등일 수 있으나, 이에 한정되는 것은 아니다. 도 1에 도시된 실시예에서, 노트북 PC, 컴퓨터 용 모니터(2, 3), 키보드, 및 마우스는 사용자에 의해 미 리 정의된 사용자 정의 객체일 수 있다. 사용자 정의 객체의 이미지에 관한 정보는 증강 현실 디바이스의 메모리(130, 도 2 참조)에 저장되어 있을 수 있다. 증강 현실 디바이스는 복수의 이미지 프레임으로부터 복수의 객체를 인식할 수 있다. 본 개시의 일 실시예 에서, 증강 현실 디바이스는 객체 인식 모델을 이용하여 복수의 이미지 프레임으로부터 복수의 객체를 인 식할 수 있다. 객체 인식 모델은 예를 들어, 컨볼루션 신경망 모델(Convolution Neural Network model)로 구성 될 수 있으나, 이에 한정되는 것은 아니다. 증강 현실 디바이스는 복수의 이미지 프레임으로부터 인식된 복수의 객체 중 적어도 하나의 사용자 정의 객체(11, 12, 13, 14, 15)를 식별할 수 있다. 증강 현실 디바이스는 복수의 이미지 프레임으로부터 식별된 적어도 하나의 사용자 정의 객체(11, 12, 13, 14, 15)에 대응되는 영역을 분할할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 컨볼루션 신 경망 모델로 구성된 객체 분할 모델을 이용하여, 복수의 이미지 프레임으로부터 적어도 하나의 사용자 정의 객 체(11, 12, 13, 14, 15)에 대응되는 영역을 분할할 수 있다. 증강 현실 디바이스는 분할 결과에 따라 적어 도 하나의 사용자 정의 객체(11, 12, 13, 14, 15)에 대응되는 영역이 제거된 이미지 프레임을 이용하여, 마스크 (mask)를 획득할 수 있다. 증강 현실 디바이스는 마스크를 이용하여 복수의 이미지 프레임을 렌더링함으로써, 블러 이미지를 획 득할 수 있다. 블러 이미지는 적어도 하나의 사용자 정의 객체(11, 12, 13, 14, 15)에 대응되는 영역을 제 외한 주변 영역이 블러 처리된 이미지일 수 있다. 증강 현실 디바이스는 블러 이미지를 안경 렌즈를 통해 디스플레이할 수 있다. 본 개시의 일 실 시예에서, 안경 렌즈는 투명한 재질로 구성되고, 작업 공간 내의 복수의 현실 객체(1 내지 9) 뿐만 아니라, 블러 이미지를 볼 수 있는 시스루 디스플레이(see through display)로 구현될 수 있다. 본 개시의 일 실시예에서, 안경 렌즈는 디스플레이 엔진(144, 도 2 참조)에 의해 투사(project)되는 블러 이미지(1 0)를 사용자를 통해 출력하고, 사용자는 안경 렌즈를 통해 블러 이미지를 볼 수 있다. 작업 공간에서 사용자의 시야 내에 여러 객체가 있는 경우, 복수의 현실 객체(1 내지 9) 중 작업과 직접적으로 관련되지 않은 객체, 예를 들어 헤드 셋, 모바일 폰 및 전등은 사용자의 주의를 끌고 집중력을 저하시 킬 수 있다. 또한, 작업 공간 내에 모바일 폰 또는 태블릿 PC가 배치된 경우, 메시지 알림, 인터넷 검색, 인 터넷 쇼핑 등을 통해 사용자의 주의를 끌 수 있고, 사용자의 작업이 방해받을 수 있다. 본 개시의 일 실시예에 따른 증강 현실 디바이스는 카메라를 통해 작업 공간을 촬영함으로써 획득된 복수의 이미지 프레임으로부터 작업과 관련된 객체로서 기 정의된 사용자 정의 객체(11, 12, 13, 14, 15)를 인 식하고, 인식된 사용자 정의 객체(11, 12, 13, 14, 15)에 대응되는 영역을 제외한 주변 영역을 블러 처리한 블러 이미지를 사용자에게 디스플레이하는 바, 작업에 대한 사용자의 집중력을 향상시킬 수 있다. 또한, 본 개시의 일 실시예에 따른 증강 현실 디바이스는 블러 이미지를 통해 작업에 관한 특별한 시각적 환경 및 사용자 경험(User eXperience, UX)을 제공할 수 있다. 도 2는 본 개시의 일 실시예에 따른 증강 현실 디바이스의 구성 요소를 도시한 블록도이다. 도 2를 참조하면, 증강 현실 디바이스는 카메라, 프로세서, 메모리, 및 디스플레이부(14 0)를 포함할 수 있다. 카메라, 프로세서, 메모리, 및 디스플레이부는 각각 전기적 및/또는 물리적으로 서로 연결될 수 있다. 도 2에 도시된 구성 요소는 본 개시의 일 실시예에 따른 것일 뿐, 증강 현실 디바이스가 포함하고 있는 구 성 요소가 도 2에 도시된 바와 같이 한정되는 것은 아니다. 증강 현실 디바이스는 도 2에 도시된 구성 요 소 중 일부를 포함하지 않을 수 있고, 도 2에 도시되지 않은 구성 요소를 더 포함할 수도 있다. 예를 들어, 증 강 현실 디바이스는 카메라, 프로세서, 메모리, 및 디스플레이부에 구동 전력을 공급 하는 배터리를 더 포함할 수 있다. 다른 예를 들어, 증강 현실 디바이스는 사용자의 시선 방향에 관한 데 이터를 획득하는 시선 추적 센서를 더 포함할 수 있다. 또 다른 예를 들어, 증강 현실 디바이스는 이미지 프레임으로부터 사용자의 손 바닥 또는 손 가락을 식별하고, 손을 통해 가리키는 영역 또는 지점을 인식하는 핸 드 트래킹 센서(hand tracking sensor)를 더 포함할 수 있다. 카메라는 현실 공간, 예를 들어 작업 공간을 촬영하여 이미지 프레임을 획득하도록 구성된다. 본 개시의 일 실시예에서, 카메라는 시간의 흐름에 따라 순차적으로 복수의 이미지 프레임을 획득할 수 있다. 카메라 는 렌즈 및 이미지 센서를 포함할 수 있다. 렌즈는 사용자가 증강 현실 디바이스를 착용하였을 때, 사용자의 얼굴 방향이 아닌 외부의 현실 공간을 향하는 방향으로 배치될 수 있다. 이미지 센서는 렌즈를 통해 현실 공간의 현실 객체에 의해 반사된 광을 수광하고, 수광된 광의 휘도 또는 세기를 전기적 신호로 변환하고, 전기적 신호를 이미지화 함으로써 복수의 이미지 프레임을 획득할 수 있다. 카메라는 복수의 이미지 프레임의 데이터를 프로세서에 제공한다. 프로세서는 메모리에 저장된 하나 이상의 명령어들(instruction) 또는 프로그램 코드를 실행하고, 명 령어들 또는 프로그램 코드에 대응되는 기능 및/또는 동작을 수행할 수 있다. 프로세서는 산술, 로직 및입출력 연산과 시그널 프로세싱을 수행하는 하드웨어 구성 요소로 구성될 수 있다. 프로세서는 예를 들어, 중앙 처리 장치(Central Processing Unit), 마이크로 프로세서(microprocessor), 그래픽 프로세서(Graphic Processing Unit), 애플리케이션 프로세서(Application Processor, AP), ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), 및 FPGAs(Field Programmable Gate Arrays) 중 적어도 하나로 구성될 수 있으나, 이에 제한되는 것은 아니다. 도 2에는 프로세서가 하나의 엘리먼트로 도시되었으나, 이에 한정되는 것은 아니다. 일 실시예에서, 프로 세서는 하나 또는 하나 이상의 복수 개로 구성될 수 있다. 일 실시예에서, 프로세서는 인공 지능(Artificial Intelligence; AI) 학습을 수행하는 전용 하드웨어 칩으 로 구성될 수도 있다. 메모리에는 프로세서가 판독할 수 있는 명령어들 및 프로그램 코드(program code)가 저장될 수 있다. 메모리는 예를 들어, 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 솔리드 스테이트 드라이브(SSD, Solid State Drive), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), 또는 PROM(Programmable Read-Only Memory), Mask ROM, Flash ROM 등) 중 적어도 하나를 포함할 수 있다. 메모리에는 증강 현실 디바이스의 기능 또는 동작들을 수행하기 위한 명령어들 또는 프로그램 코드가 저장될 수 있다. 본 개시의 일 실시예에서, 메모리에는 프로세서가 판독할 수 있는 명령어들, 알고리 즘(algorithm), 데이터 구조, 프로그램 코드(program code), 및 애플리케이션 프로그램(application program) 중 적어도 하나가 저장될 수 있다. 메모리에 저장되는 명령어들, 모델, 알고리즘, 데이터 구조, 및 프로그 램 코드는 예를 들어, C, C++, 자바(Java), 어셈블러(assembler) 등과 같은 프로그래밍 또는 스크립팅 언어로 구현될 수 있다. 메모리에는 객체 인식 모델, 객체 추적 알고리즘, 객체 분할 모델, 및 이미지 렌더링 모듈 에 관한 명령어들, 알고리즘, 데이터 구조, 또는 프로그램 코드가 저장되어 있을 수 있다. 메모리에 포함되는 '모델' 또는 '모듈'은 프로세서에 의해 수행되는 기능이나 동작을 처리하는 단위를 의미하고, 이 는 명령어들, 알고리즘, 데이터 구조, 또는 프로그램 코드와 같은 소프트웨어로 구현될 수 있다. 이하의 실시예에서, 프로세서는 메모리에 저장된 명령어들 또는 프로그램 코드들을 실행함으로써 구 현될 수 있다. 객체 인식 모델(object detection model)은 이미지 프레임으로부터 객체를 인식하는 동작 및/또는 기능에 관한 명령어들 또는 프로그램 코드로 구성된다. 본 개시의 일 실시예에서, 객체 인식 모델은 인공 신경망 모델로 구성될 수 있다. 객체 인식 모델은 수만 장 또는 수억 장의 입력 이미지로부터 객체로 인식될 수 있는 바운딩 박스(bounding box) 이미지를 입력 데이터로 적용하고, 객체 인식 결과에 관한 라벨 값(label value)을 출력 정답값(ground truth)으로 적용하여 지도 학습(supervised learning) 방식을 통해 객체를 인식 하도록 트레이닝된(trained) 심층 신경망 모델(deep neural network model)일 수 있다. 객체 인식 모델은 예를 들어, 영역 기반 컨볼루션 신경망 모델(Region-based Convolution Neural Network, R-CNN), 고속 영역 기 반 컨볼루션 신경망 모델(Faster Region-based Convolution Neural Network, Faster R-CNN), 싱글 샷 멀티박스 디텍터 모델(Single Shot multibox Detector, SSD), YOLO v4, 센터 넷(CenterNet), 또는 모바일 넷(MobileNe t)으로 구현될 수 있다. 그러나, 본 개시의 객체 인식 모델이 전술한 심층 신경망 모델로 한정되는 것은 아니다. 프로세서는 객체 인식 모델과 관련된 명령어들 또는 프로그램 코드를 실행함으로써, 복수의 이미지 프레임으로부터 복수의 객체를 인식할 수 있다. 본 개시의 일 실시예에서, 프로세서는 객체 인식 모델 을 이용하여, 복수의 이미지 프레임 중 제1 이미지 프레임으로부터 복수의 객체를 인식하고, 복수의 객체 중 적어도 하나의 기 설정된 객체를 식별할 수 있다. 프로세서는 기 설정된 객체에 관한 정보에 기초하여, 제1 이미지 프레임으로부터 인식된 복수의 객체 중 적어도 하나의 기 설정된 객체를 식별할 수 있다. 기 설정된 객체에 관한 정보는 메모리에 미리 저장되어 있을 수 있다. 본 개시의 일 실시예에서, 기 설정된 객체는 사용자 정의 객체일 수 있다. '사용자 정의 객체(user-defined object)'는 사용자의 작업과 관련된 객체로서 미리 정의되거나, 결정된 객체를 의미한다. 사용자 정의 객체는 사용자의 선택에 의해 정의될 수 있다. 사용자 정 의 객체에 관한 정보는 메모리에 미리 저장되어 있을 수 있다. 프로세서는 제1 이미지 프레임으로부 터 인식된 복수의 객체와 메모리에 기 저장된 사용자 정의 객체의 이미지를 비교함으로써, 복수의 객체 중 적어도 하나의 사용자 정의 객체를 식별할 수 있다. 본 개시의 일 실시예에서, 프로세서는 인스턴스 인식 (instance recognition) 기술을 이용하여 복수의 객체로부터 적어도 하나의 사용자 정의 객체를 식별할 수 있다. 프로세서가 객체 인식 모델을 이용하여 제1 이미지 프레임으로부터 적어도 하나의 사용자 정의 객체를 인식하는 구체적인 실시예에 대해서는 도 5 및 도 6에서 상세하게 설명하기로 한다. 객체 추적 알고리즘(object tracking algorithm)은 복수의 이미지 프레임으로부터 인식된 객체의 위치 변 화를 트래킹(tracking)하는 영상 처리와 관련된 명령어들 또는 프로그램 코드로 구성된다. 객체 추적 알고리즘 은 복수의 이미지 프레임 내의 객체의 크기, 모양, 윤곽선, 또는 컬러 등의 특징적인 정보를 이용하여 객 체의 변화를 트래킹할 수 있다. 본 개시의 일 실시예에서, 프로세서는 객체 추적 알고리즘과 관련된 명령어들 또는 프로그램 코드를 실행하여, 복수의 이미지 프레임 중 제1 이미지 프레임 보다 이전 이미지 프레 임에서 인식된 적어도 하나의 객체를 트래킹하고, 이를 통해 제1 이미지 프레임으로부터 적어도 하나의 사용자 정의 객체를 인식할 수 있다. 프로세서는 객체 추적 알고리즘을 이용하여 제1 이미지 프레임으로부터 적어도 하나의 사용자 정의 객체의 위치 변화를 트래킹할 수 있다. 객체 분할 모델(object segmentation model)은 이미지로부터 객체 영역을 분할하는 동작 및/또는 기능에 관한 명령어들 또는 프로그램 코드로 구성된다. 본 개시의 일 실시예에서, 객체 분할 모델은 심층 신경망 모델(deep neural network model)로 구성될 수 있다. 본 개시의 일 실시예에서, 객체 분할 모델은 컨볼루 션 신경망 객체 분할 모델(Convolution Neural Network object segmentation model)로 구현될 수 있다. 그러나, 이에 한정되는 것은 아니고, 객체 분할 모델은 예를 들어, 영역 기반 컨볼루션 신경망 모델 (Region-based Convolution Neural Network, R-CNN), 고속 영역 기반 컨볼루션 신경망 모델(Faster Region- based Convolution Neural Network, Fast R-CNN), 싱글 샷 멀티박스 디텍터 모델(Single Shot multibox Detector, SSD), YOLO v4, 센터 넷(CenterNet), 또는 모바일 넷(MobileNet)으로 구현될 수 있다. 도 2에서 객 체 분할 모델은 객체 인식 모델과 별개의 신경망 모델로 도시되었으나, 이에 한정되는 것은 아니다. 본 개시의 다른 실시예에서, 객체 분할 모델과 객체 인식 모델은 하나의 신경망 모델로 통합될 수 있 다. 본 개시의 일 실시예에서, 프로세서는 객체 분할 모델과 관련된 명령어들 또는 프로그램 코드를 실행 하여, 제1 이미지 프레임으로부터 적어도 하나의 사용자 정의 객체에 대응되는 영역을 분할할 수 있다. 프로세 서는 분할 결과에 따라 적어도 하나의 사용자 정의 객체에 대응되는 영역이 제거된 이미지 프레임을 획득 할 수 있다. 프로세서는 획득된 이미지 프레임을 이용하여 마스크(mask)를 획득할 수 있다. '마스크'는 이 미지 프레임 중 특정 부분을 가리거나 수정 또는 편집하기 위한 이미지를 의미한다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 적어도 하나의 사용자 정의 객체 중 중요 객체를 선택하 는 사용자 입력을 수신하는 사용자 입력부를 더 포함할 수 있다. 사용자 입력부는 예를 들어, 사용자의 손바닥 또는 손가락이 가리키는 영역 또는 객체를 인식하는 핸드 트래킹 센서(hand tracking sensor)일 수 있다. 이 경 우, 프로세서는 핸드 트래킹 센서를 이용하여 적어도 하나의 사용자 정의 객체 중 사용자의 손가락이 가리 키는 객체를 인식하고, 인식된 객체를 중요 객체로서 선택할 수 있다. 그러나, 이에 한정되는 것은 아니고, 사 용자 입력부는 사용자의 양안의 시선 방향에 관한 정보를 획득함으로써, 양얀의 시선이 수렴하는 응시점(gaze point)의 위치 좌표 정보를 획득하는 시선 추적 센서를 포함할 수 있다. 이 경우, 프로세서는 적어도 하나 의 사용자 정의 객체 중 응시점이 기 설정된 시간 이상 머무르는 객체를 인식하고, 인식된 객체를 중요 객체로 서 선택할 수 있다. 프로세서는 사용자 입력에 의해 선택된 중요 객체를 제1 이미지 프레임으로부터 분할 함으로써 마스크를 획득할 수 있다. 이미지 렌더링 모듈은 마스크를 이용하여 복수의 이미지 프레임을 렌더링함으로써, 블러 이미지(blur image)를 생성하는 동작 및/또는 기능에 관한 명령어들 또는 프로그램 코드로 구성된다. 이미지 렌더링 모듈 은 이미지 블러링 또는 이미지 스무딩(image smoothing) 기술을 이용하여 이미지를 블러 처리할 수 있다. 이미지 렌더링 모듈은 예를 들어, 평균 블러링(Average Blurring), 가우시안 블러링(Gaussian Blurring), 미디언 블러링(Median Blurring), 또는 바이레터럴 필터(Bilateral Filter) 중 적어도 하나의 기술을 이용하여 이미지 블러링을 수행할 수 있다. 그러나, 이에 한정되는 것은 아니고, 이미지 렌더링 모듈은 공지의 모든 블러링 기술을 이용하여 이미지 블러링을 수행할 수 있다. 프로세서는 이미지 렌더링 모듈과 관련된 명령어들 또는 프로그램 코드를 실행하여, 제1 이미지 프레 임 이후 획득된 제2 이미지 프레임을 마스크와 합성하고, 이를 통해 제2 이미지 프레임의 전체 영역 중 적어도 하나의 사용자 정의 객체에 대응되는 영역을 제외한 주변 영역이 블러 처리된 블러 이미지를 획득할 수 있다. 본 개시의 일 실시예에서, 프로세서는 제2 이미지 프레임과 마스크를 컨볼루션(convolution)함으로써, 제2 이미지 프레임의 전체 영역 중 적어도 하나의 사용자 정의 객체에 대응되는 영역을 제외한 주변 영역을 블러 처 리할 수 있다. 프로세서가 객체 분할 모델을 이용하여 제1 이미지 프레임으로부터 적어도 하나의 사 용자 정의 객체에 대응되는 영역을 분할하고, 이미지 렌더링 모듈을 이용하여 블러 이미지를 획득하는 구 체적인 실시예에 대해서는 도 7 및 도 8에서 상세하게 설명하기로 한다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 주변 영역에 관한 블러 처리 정도, 컬러, 및 밝기 중 적 어도 하나를 포함하는 블러 옵션(blur option)을 결정하는 사용자 입력을 수신하는 사용자 입력부를 더 포함할 수 있다. 프로세서는 사용자 입력부를 통해 수신된 사용자 입력에 기초하여 블러 처리 정도, 컬러, 및 밝 기 중 적어도 하나에 관한 블러 옵션을 결정하고, 결정된 블러 옵션에 기초하여 제2 이미지 프레임의 전체 영역 중 주변 영역을 블러 처리할 수 있다. 본 개시의 일 실시예에서, 프로세서는 작업 시간 동안에만 이미지 렌더링을 수행하여, 블러 이미지를 획득 할 수 있다. 작업 시간은 사용자에 의해 미리 설정될 수 있다. 작업 시간은 예를 들어, 45분으로 설정되고, 휴 식 시간은 15분으로 설정될 수 있다. 프로세서는 작업 시간 동안에만 블러 이미지를 디스플레이부를 통해 디스플레이할 수 있다. 프로세서가 작업 시간 동안에만 블러 이미지를 획득하고, 블러 이미지를 디스 플레이하는 구체적인 실시예에 대해서는 도 9에서 상세하게 설명하기로 한다. 본 개시의 일 실시예에서, 프로세서는 제2 이미지 프레임의 전체 영역 중 블러 처리된 주변 영역을 서로 다른 컬러와 합성함으로써, 블러 이미지를 획득할 수 있다. 프로세서는 주변 영역을 작업의 종류 또는 작 업 환경에 대응되는 서로 다른 컬러와 합성할 수 있다. 프로세서가 작업의 종류 또는 작업 환경에 따라 주 변 영역을 서로 다른 컬러로 합성하여 블러 이미지를 획득하는 구체적인 실시예에 대해서는 도 10에서 상세하게 설명하기로 한다. 본 개시의 일 실시예에서, 프로세서는 제2 이미지 프레임의 전체 영역 중 블러 처리된 주변 영역에 작업과 관련된 가상 이미지를 합성하여 블러 이미지를 획득할 수 있다. 주변 영역에 합성되는 가상 이미지는 작업과 관 련된 정보를 제공하는 가상 객체 또는 그래픽 UI(Graphic User Interface)일 수 있다. 프로세서가 주변 영 역에 가상 객체 또는 그래픽 UI를 합성하여 블러 이미지를 획득하는 구체적인 실시예에 대해서는 도 11에서 상 세하게 설명하기로 한다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 이미지로부터 인식된 복수의 객체 중 적어도 하나의 객체 를 선택하는 사용자 입력을 수신하는 사용자 입력부를 더 포함할 수 있다. 프로세서는 사용자 입력부를 통 해 수신된 사용자 입력에 기초하여 제1 이미지 프레임으로부터 인식된 복수의 객체 중 적어도 하나의 객체를 선 택하고, 선택된 적어도 하나의 객체를 블러 처리하여 블러 이미지를 획득할 수 있다. 프로세서가 사용자 입력에 의해 선택된 적어도 하나의 객체를 블러 처리하여 블러 이미지를 획득하는 구체적인 실시예에 대해서는 도 13 및 도 14에서 상세하게 설명하기로 한다. 프로세서는 블러 이미지를 디스플레이부를 통해 사용자에게 디스플레이할 수 있다. 디스플레이부는 안경 렌즈 및 디스플레이 엔진을 포함할 수 있다. 안경 렌즈는 투명한 재질로 구성되고, 작업 공간 내의 현실 객체 뿐만 아니라, 디스플레이 엔진에 의 해 투사된(projected) 블러 이미지를 볼 수 있는 시스루 디스플레이(see through display)로 구현될 수 있다. 일 실시예에서, 안경 렌즈는 디스플레이 엔진으로부터 투사된 블러 이미지의 광을 수광하고, 광을 전 파하며, 광 경로를 변경하는 웨이브 가이드(waveguide)를 포함할 수 있다. 디스플레이 엔진은 블러 이미지를 안경 렌즈의 웨이브 가이드에 투사하도록 구성된다. 디스플레이 엔 진은 프로젝터(projector)와 같은 기능을 수행할 수 있다. 디스플레이 엔진은 조명 광학계, 광경로 변환기, 화상 패널, 빔 스필리터, 및 투사 광학계를 더 포함할 수 있다. 본 개시의 일 실시예에서, 디스플레이 엔진은 프로세서로부터 블러 이미지를 구성하는 이미지 데이터를 획득하고, 획득된 이미지 데이터에 기초하여 가상 이미지를 생성하고, 가상 이미지를 광원으로부터 출력된 광과 함께 출사면을 통해 웨이브 가이드 에 투사할 수 있다. 이 경우, 프로세서는 가상 이미지를 구성하는 복수의 픽셀의 RGB 컬러 및 휘도 값을 포함하는 이미지 데이터를 디스플레이 엔진에 제공할 수 있다. 디스플레이 엔진은 복수의 픽셀 각각의 RGB 컬러 값과 휘도 값을 이용하여 이미지 프로세싱을 수행하고, 광원을 제어함으로써 가상 이미지를 웨이브 가이드에 투사할 수 있다. 도 3은 본 개시의 일 실시예에 따른 증강 현실 디바이스의 동작 방법을 도시한 흐름도이다. 도 4는 본 개시의 일 실시예에 따른 증강 현실 디바이스가 블러 이미지를 획득하는 동작을 도시한 도 면이다. 이하에서는, 도 3과 도 4를 함께 참조하여, 증강 현실 디바이스가 블러 이미지를 획득하는 동작을 설 명하기로 한다. 도 3을 참조하면, 단계 S310에서 증강 현실 디바이스는 카메라를 이용하여 현실 공간을 촬영함으로써 복수 의 이미지 프레임을 획득한다. 현실 공간은 예를 들어, 사용자가 작업을 수행하는 작업 공간일 수 있다. 그러나, 이에 한정되지 않는다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 카메라를 이용하여 시간 의 흐름에 따라 순차적으로 복수의 이미지 프레임을 획득할 수 있다. 도 4를 함께 참조하면, 증강 현실 디바이스는 복수의 이미지 프레임 중 제1 이미지 프레임을 획득할 수 있다. 제1 이미지 프레임은 복수의 객체(401 내지 408)을 포함할 수 있다. 도 3의 단계 S320을 참조하면, 증강 현실 디바이스는 복수의 이미지 프레임 중 제1 이미지 프레임으로부터 적어도 하나의 기 설정된 객체를 인식한다. 도 4를 함께 참조하면, 증강 현실 디바이스는 객체 인식 모델 (132, 도 2 참조)를 이용하여 제1 이미지 프레임으로부터 복수의 객체(401 내지 408)를 인식할 수 있다. 객체 인식 모델은 예를 들어, 영역 기반 컨볼루션 신경망 모델(Region-based Convolution Neural Network, R-CNN), 고속 영역 기반 컨볼루션 신경망 모델(Faster Region-based Convolution Neural Network, Faster R-CNN), 싱글 샷 멀티박스 디텍터 모델(Single Shot multibox Detector, SSD), YOLO v4, 센터 넷 (CenterNet), 또는 모바일 넷(MobileNet)으로 구현될 수 있다. 그러나, 본 개시의 객체 인식 모델이 전술 한 심층 신경망 모델로 한정되는 것은 아니다. 증강 현실 디바이스는 객체 인식 모델을 이용하여 제1 이미지 프레임으로부터 인식된 복수의 객 체(401 내지 409) 중 적어도 하나의 기 설정된 객체를 식별할 수 있다. 증강 현실 디바이스는 메모리(130, 도 2 참조)에 저장된 기 설정된 객체에 관한 정보를 이용하여 복수의 객체(401 내지 409) 중 적어도 하나의 기 설정된 객체를 식별할 수 있다. 본 개시의 일 실시예에서, 기 설정된 객체는 사용자 정의 객체(401, 402, 403, 404, 405)일 수 있다. 본 개시에서, '사용자 정의 객체(user-defined object)(401, 402, 403, 404, 405)'는 사 용자의 작업과 관련된 객체로서 미리 정의되거나, 결정된 객체를 의미한다. 사용자 정의 객체는 사용자의 선택 에 의해 정의될 수 있다. 사용자 정의 객체(401, 402, 403, 404, 405)에 관한 정보 및 사용자 정의 객체(401, 402, 403, 404, 405)의 이미지는 메모리(130, 도 2 참조)에 미리 저장되어 있을 수 있다. 증강 현실 디바이스 는 제1 이미지 프레임으로부터 인식된 복수의 객체(401 내지 409)와 메모리에 기 저장된 사용자 정의 객체의 이미지를 비교함으로써, 복수의 객체(401 내지 409) 중 적어도 하나의 사용자 정의 객체(401, 402, 403, 404, 405)를 식별할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 인스턴스 인식 (instance recognition) 기술을 이용하여 복수의 객체(401 내지 409)로부터 적어도 하나의 사용자 정의 객체 (401, 402, 403, 404, 405)를 식별할 수 있다. 도 4에 도시된 실시예에서, 사용자 정의 객체(401, 402, 403, 404, 405)는 노트북 PC, 컴퓨터 용 모니터 (402, 403), 키보드, 및 마우스로 도시되었으나, 이에 한정되는 것은 아니다. 또한, 도 4에서 사용자 정의 객체(401, 402, 403, 404, 405)는 5개로 도시되고, 설명되었지만, 이는 예시적인 것이고, 도면에 도시된 개수로 한정되는 것은 아니다. 본 개시의 일 실시예에서, 사용자 정의 객체(401, 402, 403, 404, 405)는 하나 또는 복수일 수 있다. 증강 현실 디바이스는 식별된 사용자 정의 객체(401, 402, 403, 404, 405)를 특정하기 위하여 사용자 정의 객체(401, 402, 403, 404, 405)를 둘러싸는 바운딩 박스(B1, B2, B3, B4, B5)를 표시할 수 있다. 도 3의 단계 S330을 참조하면, 증강 현실 디바이스는 제1 이미지 프레임으로부터 적어도 하나의 사용자 객 체를 분할함으로써, 마스크(mask)를 획득한다. 도 4를 함께 참조하면, 증강 현실 디바이스는 객체 분할 모 델(136, 도 2 참조)를 이용하여 제1 이미지 프레임 내의 바운딩 박스(B1, B2, B3, B4, B5) 내의 적어도 하나의 사용자 정의 객체(401, 402, 403, 404, 405)를 특정할 수 있다. 증강 현실 디바이스는 적어도 하나의 사용자 정의 객체(401, 402, 403, 404, 405)의 윤곽 및 경계선을 인식함으로써, 적어도 하나의 사용자 정의 객체(401, 402, 403, 404, 405)의 형태를 식별하고, 식별된 적어도 하나의 사용자 정의 객체(401, 402, 403, 404, 405)에 대응되는 영역을 제1 이미지 프레임으로부터 분할하여 제거할 수 있다. 증강 현실 디바이스 는 분할 결과에 따라 제1 이미지 프레임으로부터 적어도 하나의 사용자 정의 객체(401, 402, 403, 404, 405)에 대응되는 영역이 제거된 이미지 프레임을 획득할 수 있다. 증강 현실 디바이스는 획득된 이미 지 프레임을 이용하여 적어도 하나의 사용자 정의 객체(401, 402, 403, 404, 405)에 대응되는 영역과 주변 영역 을 분리하기 위한 마스크를 획득할 수 있다. 도 3의 단계 S340을 참조하면, 증강 현실 디바이스는 마스크를 이용하여 제2 이미지 프레임을 렌더링함으 로써, 적어도 하나의 사용자 정의 객체를 제외한 주변 영역이 블러 처리된 블러 이미지(blur image)를 획득한다. 도 4를 함께 참조하면, 증강 현실 디바이스는 제1 이미지 프레임 이후에 획득된 제2 이미 지 프레임을 마스크와 합성하고, 이를 통해 제2 이미지 프레임의 전체 영역 중 적어도 하나의 사용자 정의 객체(401, 402, 403, 404, 405)에 대응되는 영역을 제외한 주변 영역이 블러 처리된 블러 이미지를 획득할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 제2 이미지 프레임과 마스크를 컨 볼루션(convolution)함으로써, 제2 이미지 프레임의 전체 영역 중 적어도 하나의 사용자 정의 객체에 대응되는 영역을 제외한 주변 영역을 블러 처리할 수 있다. 도 3의 단계 S350을 참조하면, 증강 현실 디바이스는 블러 이미지(420, 도 4 참조)를 디스플레이한다. 증 강 현실 디바이스는 디스플레이 엔진(144, 도 2 참조)를 이용하여 블러 이미지를 광원으로부터 출력 된 광과 함께 웨이브 가이드에 투사하고, 웨이브 가이드를 통해 투사된 블러 이미지를 사용자에게 디스플 레이할 수 있다. 도 5는 본 개시의 일 실시예에 따른 증강 현실 디바이스가 제1 이미지 프레임으로부터 사용자 정의 객체 (user-defined object)를 식별하는 방법을 도시한 흐름도이다. 도 5에 도시된 단계 S510 및 S520은 도 3에 도시된 단계 S320을 구체화한 단계들이다. 도 5의 단계 S510은 도 3 의 단계 S310이 수행된 이후에 수행될 수 있다. 도 5의 단계 S520이 수행된 이후에는 도 3의 단계 S330이 수행 될 수 있다. 도 6은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 제1 이미지 프레임으로부터 사용자 정의 객체(601, 602, 603, 604, 605)를 인식하는 동작을 도시한 도면이다. 이하에서는, 도 5와 도 6을 함께 참조하여, 증강 현실 디바이스가 제1 이미지 프레임으로부터 사용자 정의 객체(601, 602, 603, 604, 605)를 인식하는 동작을 설명하기로 한다. 도 5를 참조하면, 단계 S510에서 증강 현실 디바이스는 객체 인식 모델을 이용하여 제1 이미지 프레임으로 부터 복수의 객체를 인식한다. 도 6을 함께 참조하면, 증강 현실 디바이스는 카메라(110, 도 2 참조)를 통 해 획득한 복수의 이미지 프레임 중 제1 이미지 프레임을 객체 인식 모델(132, 도 2 참조)에 입력하는 추 론을 수행함으로써, 제1 이미지 프레임으로부터 복수의 객체가 인식되는 복수의 바운딩 박스(bounding box)(B1 내지 B13)을 인식할 수 있다. 본 개시의 일 실시예에서, 객체 인식 모델은 수만 장 또는 수억 장 의 입력 이미지로부터 객체로 인식될 수 있는 바운딩 박스 이미지를 입력 데이터로 적용하고, 객체 인식 결과에 관한 라벨 값(label value)을 출력 정답값(ground truth)으로 적용하여 지도 학습(supervised learning) 방식 을 통해 객체를 인식하도록 트레이닝된(trained) 심층 신경망 모델(deep neural network model)일 수 있다. 객 체 인식 모델은 예를 들어, 영역 기반 컨볼루션 신경망 모델(Region-based Convolution Neural Network, R-CNN), 고속 영역 기반 컨볼루션 신경망 모델(Faster Region-based Convolution Neural Network, Faster R- CNN), 싱글 샷 멀티박스 디텍터 모델(Single Shot multibox Detector, SSD), YOLO v4, 센터 넷(CenterNet), 또 는 모바일 넷(MobileNet)으로 구현될 수 있다. 그러나, 본 개시의 객체 인식 모델이 전술한 심층 신경망 모델로 한정되는 것은 아니다. 증강 현실 디바이스는 미리 학습된(pre-trained) 심층 신경망 모델로 구성된 객체 인식 모델에 제1 이미지 프레임을 입력하고, 객체 인식 모델을 통한 추론 결과에 따라 복수의 객체를 포함하는 복수의 바운딩 박스(B1 내지 B13)을 인식할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 객체 추적 알고리즘(134, 도 2 참조)를 이용하여 제1 이미지 프레임의 이전에 획득된 이미지 프레임으로부터의 인식 결과를 이용하여 복수의 바운딩 박스(B1 내지 B13)을 인식할 수도 있다. 객체 추적 알고리즘은 복수의 이미지 프레임 내의 객체의 크기, 모양, 윤곽선, 또는 색 등의 특징적인 정보를 이용하여 객체의 변화를 트래킹할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 객체 추적 알고리즘을 이용하여, 복수의 이미지 프레임 중 제1 이미지 프레임 보다 이전 이미지 프레임에서 인식된 적어도 하나의 객체를 트래킹하 고, 이를 통해 제1 이미지 프레임으로부터 복수의 바운딩 박스(B1 내지 B13)을 인식할 수도 있다. 도 5의 단계 S520을 참조하면, 증강 현실 디바이스는 복수의 객체 중 사용자의 작업과 관련된 객체로서 기 정의된 적어도 하나의 사용자 정의 객체를 식별한다. 도 6을 함께 참조하면, 증강 현실 디바이스는 인식된 복수의 바운딩 박스(B1 내지 B13)에 포함된 복수의 객체를 메모리(130, 도 2 참조)에 기 저장된 사용자 정의 객 체의 이미지를 비교함으로써, 적어도 하나의 사용자 정의 객체(601, 602, 603, 604, 605)를 포함하는 바운딩 박 스(B1, B2, B3, B4, B5)를 식별할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 인스턴스 인 식(instance recognition)을 이용하여 복수의 바운딩 박스(B1 내지 B13)로부터 적어도 하나의 사용자 정의 객체 (601, 602, 603, 604, 605)를 포함하는 바운딩 박스(B1, B2, B3, B4, B5)를 식별할 수 있다. 본 개시에서, '사 용자 정의 객체(user-defined object)(601, 602, 603, 604, 605)'는 사용자의 작업과 관련된 객체로서 미리 정 의되거나, 결정된 객체를 의미한다. 도 7은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 제1 이미지 프레임으로부터 사용자 정의 객체(701, 702, 703, 704, 705)를 인식하는 동작을 도시한 도면이다. 도 7을 참조하면, 증강 현실 디바이스는 객체 분할 모델(136, 도 2 참조)를 이용하여 제1 이미지 프레임 으로부터 인식된 바운딩 박스(B1, B2, B3, B4, B5) 내의 사용자 정의 객체(701, 702, 703, 704, 705)를 정확하게 특정할 수 있다. 객체 분할 모델(object segmentation model)은 이미지로부터 객체를 분할하는 심층 신경망 모델로 구성될 수 있다. 객체 분할 모델은 예를 들어, 컨볼루션 신경망 객체 분할 모델 (Convolution Neural Network object segmentation model)로 구현될 수 있지만, 이에 한정되는 것은 아니다. 다른 예를 들어, 객체 분할 모델은 영역 기반 컨볼루션 신경망 모델(Region-based Convolution Neural Network, R-CNN), 고속 영역 기반 컨볼루션 신경망 모델(Faster Region-based Convolution Neural Network, Fast R-CNN), 싱글 샷 멀티박스 디텍터 모델(Single Shot multibox Detector, SSD), YOLO v4, 센터 넷 (CenterNet), 또는 모바일 넷(MobileNet)으로 구현될 수도 있다. 증강 현실 디바이스는 객체 분할 모델을 이용하여 제1 이미지 프레임으로부터 인식된 바운딩 박 스(B1, B2, B3, B4, B5) 내의 사용자 정의 객체(701, 702, 703, 704, 705)의 윤곽 및 경계선을 인식함으로써 사용자 정의 객체(701, 702, 703, 704, 705)의 영역을 식별할 수 있다. 증강 현실 디바이스는 사용자 정의 객체(701, 702, 703, 704, 705)의 영역을 제1 이미지 프레임으로부터 분할할 수 있다. 도 8은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 블러 이미지를 획득하는 동작을 도시한 도 면이다. 도 8을 참조하면, 증강 현실 디바이스는 객체 분할 모델(136, 도 2 참조)를 이용하여 제1 이미지 프레임 으로부터 적어도 하나의 사용자 정의 객체(801, 802, 803, 804, 805)에 대응되는 영역을 분할하여 제거할 수 있다. 증강 현실 디바이스는 분할 결과에 따라 제1 이미지 프레임으로부터 적어도 하나의 사용자 정의 객체(801, 802, 803, 804, 805)에 대응되는 영역이 제거된 이미지 프레임을 획득할 수 있다. 증강 현실 디 바이스는 획득된 이미지 프레임을 이용하여 적어도 하나의 사용자 정의 객체(801, 802, 803, 804, 805)에 대응되는 영역과 주변 영역을 분리하기 위한 마스크를 획득할 수 있다. 마스크는 이미지 프레임 중 특정 부분을 가리거나 수정 또는 편집하기 위한 이미지일 수 있다. 도 8에 도 시된 실시예를 참조하면, 마스크는 적어도 하나의 사용자 정의 객체(801, 802, 803, 804, 805)에 해당되는 영역은 투명 마스크로 처리되고, 적어도 하나의 사용자 정의 객체(801, 802, 803, 804, 805)를 제외한 주변 영 역을 검게 처리될 수 있다. 증강 현실 디바이스는 마스크를 이용하여 복수의 이미지 프레임을 렌더링함으로써, 블러 이미지(82 0)를 획득할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 제1 이미지 프레임 이후 획득 된 제2 이미지 프레임을 마스크와 합성하고, 이를 통해 제2 이미지 프레임의 전체 영역 중 적어도 하나의 사용자 정의 객체(801, 802, 803, 804, 805)에 대응되는 영역을 제외한 주변 영역이 블러 처리된 블러 이 미지를 획득할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 제2 이미지 프레임과 마스크를 컨볼루션(convolution)함으로써, 제2 이미지 프레임의 전체 영역 중 적어도 하나의 사용자 정의 객체 (801, 802, 803, 804, 805)에 대응되는 영역을 제외한 주변 영역을 블러 처리할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 이미지 블러링(image blurring) 또는 이미지 스무딩 (image smoothing) 기술을 이용하는 렌더링을 통해 복수의 이미지 프레임을 블러 처리할 수 있다. 증강 현실 디 바이스는 예를 들어, 평균 블러링(Average Blurring), 가우시안 블러링(Gaussian Blurring), 미디언 블러 링(Median Blurring), 또는 바이레터럴 필터(Bilateral Filter) 중 적어도 하나의 기술을 이용하여 이미지 블 러링을 수행할 수 있다. 도 9는 본 개시의 일 실시예에 따른 증강 현실 디바이스가 시간의 흐름에 따라 서로 다른 이미지를 디스플 레이하는 동작을 설명하기 위한 도면이다. 도 9를 참조하면, 증강 현실 디바이스는 안경 렌즈를 통해 시간의 흐름에 따라 서로 다른 이미지를 디스플레이할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 작업 시간 동안에만 이미지 렌더 링을 수행하여, 적어도 하나의 사용자 정의 객체를 제외한 주변 영역을 블러 처리한 블러 이미지를 디스플 레이할 수 있다. '작업 시간'은 사용자가 작업을 수행하도록 미리 설정된 시간을 의미한다. 작업 시간은 사용자 의 입력에 의해 결정될 수 있다. 작업 시간은 예를 들어, 45분으로 결정될 수 있으나, 이에 한정되는 것은 아니 다. 도 9의 실시예를 참조하면, 증강 현실 디바이스의 프로세서(120, 도 2 참조)는 시작 시점(t0)으로부터 제1 시점(t1) 사이의 작업 시간 동안 이미지 프레임을 렌더링함으로써, 적어도 하나의 사용자 정의 객체에 대응되는 영역을 제외한 주변 영역을 블러 처리한 블러 이미지를 획득하고, 블러 이미지를 안경 렌즈를 통해 디스플레이할 수 있다. 본 개시의 일 실시예에서, 프로세서는 블러 처리된 주변 영역을 특정한 컬러 와 합성하여 블러 이미지를 디스플레이할 수 있다. 합성되는 컬러는 사용자 입력에 의해 결정될 수 있다. 예를 들어, 프로세서는 녹색 컬러를 블러 처리된 주변 영역과 합성하여, 블러 이미지를 디스플레이할 수 있다. 프로세서는 제1 시점(t1)으로부터 제2 시점(t2) 사이의 휴식 시간 동안에는 이미지 프레임에 블러 처리 등 렌더링을 수행하지 않고, 작업 공간에 관한 이미지를 디스플레이할 수 있다. 본 개시의 일 실시예에서, 프 로세서는 카메라(110, 도 2 참조)에 의해 획득된 작업 공간에 관한 이미지를 별도의 렌더링 없이 원 본 상태로 디스플레이할 수 있다. 제1 시점(t1)으로부터 제2 시점(t2) 사이의 휴식 시간은 예를 들어, 15분일 수 있으나, 이에 한정되는 것은 아니다. 프로세서는 제2 시점(t2)으로부터 제3 시점(t3) 사이의 작업 시간 동안에는 다시 이미지 프레임을 렌더링하 여, 적어도 하나의 사용자 정의 객체에 대응되는 영역을 제외한 주변 영역을 블러 처리하고, 블러 이미지 를 디스플레이할 수 있다. 프로세서는 시작 시점(t0)으로부터 제1 시점(t1) 사이의 작업 시간과 동일하게, 제2 시점(t2)과 제3 시점(t3) 사이의 작업 시간 동안에도 블러 처리된 주변 영역에 사용자 입력에 의해 결정된 컬러를 합성하여 블러 이미지를 획득하고, 블러 이미지를 안경 렌즈를 통해 디스플레이할 수 있 다. 프로세서는 제3 시점(t3)으로부터 제4 시점(t4) 사이의 휴식 시간 동안에는 이미지 프레임에 블러 처리 등 렌더링을 수행하지 않고, 작업 공간에 관한 이미지를 디스플레이할 수 있다. 프로세서는 제4 시점 (t4)으로부터 제5 시점(t5) 사이의 작업 시간 동안에는 다시 이미지 프레임을 렌더링하여, 블러 이미지를 디스플레이할 수 있다. 도 9에 도시된 실시예에 따른 증강 현실 디바이스는 사용자에 의해 설정된 작업 시간 동안에만 적어도 하 나의 사용자 정의 객체에 대응되는 영역을 제외한 주변 영역을 블러 처리한 블러 이미지를 선택적으로 디 스플레이함으로써, 작업 시간 동안 사용자의 작업에 관한 집중력을 향상시키는 기술적 효과를 제공한다. 또한, 본 개시의 일 실시예에 따른 증강 현실 디바이스는 블러 처리된 주변 영역을 특정 컬러로 합성하여 디스플 레이함으로써, 사용자에게 작업에 관한 특별한 시각적 환경 및 특별한 사용자 경험(User eXperience, UX)을 제 공할 수 있다. 도 10은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 작업의 종류에 따라 서로 다른 블러 이미지를 디스플레이하는 동작을 설명하기 위한 도면이다. 도 10을 참조하면, 증강 현실 디바이스는 안경 렌즈를 통해 서로 다른 컬러로 합성된 블러 이미지 (1010, 1020, 1030)를 디스플레이할 수 있다. 증강 현실 디바이스의 프로세서(120, 도 2 참조)는 이미지 프레임의 전체 영역 중 블러 처리된 주변 영역을 서로 다른 컬러와 합성하여 블러 이미지(1010, 1020, 1030)를 획득하고, 획득된 블러 이미지(1010, 1020, 1030)를 안경 렌즈를 통해 디스플레이할 수 있다. 본 개시의 일 실시예에서, 프로세서는 블러 처리된 주변 영역을 작업의 종류 또는 작업 환경에 대응되는 서로 다른 컬러와 합성할 수 있다. 도 10에 도시된 실시예에서, 프로세서는 제1 작업을 수행하는 시작 시 점(t0)으로부터 제1 시점(t1)까지는 이미지 프레임의 전체 영역 중 적어도 하나의 사용자 정의 객체에 대응되는 영역을 제외한 주변 영역을 녹색과 합성하여 생성된 제1 블러 이미지를 디스플레이할 수 있다. 프로세서 는 제2 작업을 수행하는 제2 시점(t2)으로부터 제3 시점(t3)까지는 이미지 프레임의 전체 영역 중 적어도 하나의 사용자 정의 객체에 대응되는 영역을 제외한 주변 영역을 파란색과 합성하여 생성된 제2 블러 이미지 를 디스플레이하고, 제3 작업을 수행하는 제3 시점(t3)으로부터 제4 시점(t4)까지는 주변 영역을 빨간색으 로 합성하여 생성된 제3 블러 이미지를 디스플레이할 수 있다. 프로세서는 제1 작업을 수행하는 제4 시점(t4)으로부터 제5 시점(t5)까지는 제1 블러 이미지를 디스플레이할 수 있다. 본 개시의 일 실시예에서, 프로세서는 작업의 종류 또는 작업 환경에 기초하여 블러 처리되는 주변 영역을 결정할 수도 있다. 예를 들어, 제1 작업을 수행하는 동안 디스플레이되는 제1 블러 이미지에 대해서는 이 미지 프레임의 전체 영역 중 컴퓨터 용 모니터를 제외한 나머지 주변 영역을 블러 처리하고, 제2 작업을 수행하 는 동안 디스플레이되는 제2 블러 이미지에 대해서는 이미지 프레임의 전체 영역 중 노트북 PC와 1대의 컴퓨터 용 모니터를 제외한 나머지 주변 영역을 블러 처리하며, 제3 작업을 수행하는 동안 디스플레이되는 제3 블러 이미지에 대해서는 이미지 프레임의 전체 영역 중 노트북 PC를 제외한 주변 영역을 블러 처리할 수 있다. 프로세서는 작업 시간이 아닌, 휴식 시간에는 이미지 렌더링을 수행하지 않고, 작업 공간에 관한 원본 상 태의 이미지를 디스플레이할 수 있다. 사용자는 하나의 작업만을 수행하는 것이 아니고, 여러가지 작업을 수행할 수 있다. 다양한 작업은 종류가 다르 고, 작업 환경이 다르며, 작업 수행 시 사용하는 도구(예를 들어, 노트북 PC, 또는 데스크탑 PC, 태블릿 PC 등)가 다를 수 있다. 도 10에 도시된 실시예에 따른 증강 현실 디바이스는 복수의 서로 다른 작업의 종류 또는 작업 환경에 따라 서로 다른 컬러로 주변 영역을 합성하는 블러 이미지(1010, 1020, 1030)를 디스플레이하 는 바, 사용자가 다양한 작업을 수행하는 경우 작업 간 전환을 용이하게 해주고, 전환된 작업에 관한 집중력을 향상시키는 기술적 효과를 제공한다. 또한, 본 개시의 일 실시예에 따른 증강 현실 디바이스는 작업의 종 류 또는 작업의 환경에 따라 서로 다른 컬러의 블러 이미지(1010, 1020, 1030)를 디스플레이하는 바, 작업에 관 한 특별한 시각적 환경을 제공할 수 있다. 도 11은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 블러 처리된 주변 영역에 가상 객체 또는 그래 픽 UI(Graphic User Interface)를 디스플레이하는 동작을 도시한 도면이다. 도 11을 참조하면, 증강 현실 디바이스는 이미지 프레임의 전체 영역 중 적어도 하나의 사용자 정의 객체 (1101, 1102, 1103, 1104, 1105)에 대응되는 영역을 제외한 주변 영역에 가상 이미지를 합성하여 획득된 블러 이미지를 디스플레이할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스의 프로 세서(120, 도 2 참조)는 블러 이미지의 전체 영역 중 주변 영역 상에 가상 이미지가 오버레 이(overlay)되도록 가상 이미지를 주변 영역과 합성하는 렌더링을 수행할 수 있다. 본 개시의 일 실시예에서, 프로세서는 가상 이미지가 적어도 하나의 사용자 정의 객체(1101, 1102, 1103, 1104, 1105)의 영역과 겹치지 않도록 가상 이미지의 위치를 결정할 수 있다. 가상 이미지는 작업과 관련된 정보를 제공하는 가상 객체 또는 그래픽 UI(Graphic User Interface)일 수 있다. 예를 들어, 제1 가상 이미지는 작업과 관련된 텍스트 또는 코딩 명령어들을 기록한 위젯(widget)이고, 제2 가상 이미지는 캘린더 UI이고, 제3 가상 이미지은 메모 UI이며, 제4 가상 이미지는 타이머 UI일 수 있다. 도 11에 도시된 가상 이미지는 예시적인 것이고, 도시된 위젯, 캘린더 UI, 메모 UI, 및 타이머 UI로 한정되는 것은 아니다. 도 11에 도시된 실시예에 따른 증강 현실 디바이스는 적어도 하나의 사용자 정의 객체(1101, 1102, 1103, 1104, 1105)를 제외한 주변 영역을 블러 처리할 뿐만 아니라, 블러 처리된 주변 영역에 작업과 관 련된 정보를 제공하는 가상 이미지를 표시함으로써, 사용자의 작업 효율성을 향상시키는 기술적 효과를 제공한다. 도 12는 본 개시의 일 실시예에 따른 증강 현실 디바이스가 사용자 입력에 기초하여 결정된 블러 옵션 (blur option)에 따라 블러 이미지를 획득하는 방법을 도시한 흐름도이다. 도 11에 도시된 단계 S1210 및 S1220은 도 3에 도시된 단계 S340을 구체화한 단계들이다. 도 11의 단계 S1210은 도 3의 단계 S330이 수행된 이후에 수행될 수 있다. 도 11의 단계 S1220이 수행된 이후에는 도 3의 단계 S350이 수행될 수 있다. 단계 S1210에서, 증강 현실 디바이스는 사용자 입력에 기초하여 블러 처리 정도, 컬러, 및 밝기 중 적어도 하나를 포함하는 블러 옵션(blur option)을 결정한다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 블 러 옵션을 선택하기 위한 그래픽 UI(Graphic User Interface)로 구성된 가상 이미지를 디스플레이하고, 가상 이 미지를 통해 블러 처리 정도, 컬러, 및 밝기 중 적어도 하나에 관한 블러 옵션을 선택하는 사용자 입력을 수신 할 수 있다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 사용자의 손가락이 향하는 위치 또는 영역을 인식하는 핸드 트래킹 센서(hand tracking sensor)를 포함할 수 있다. 이 경우, 증강 현실 디바이스는 핸 드 트래킹 센서를 이용하여 그래픽 UI를 가리키는 손가락의 위치를 인식함으로써, 블러 옵션을 선택하는 사용자 입력을 수신할 수 있다. 증강 현실 디바이스는 사용자 입력에 기초하여, 블러 이미지의 전체 영역 중 적어 도 하나의 사용자 정의 객체를 제외한 주변 영역에 관한 블러 처리 정도, 컬러, 및 밝기 중 적어도 하나에 관한 블러 옵션을 결정할 수 있다. 단계 S1220에서, 증강 현실 디바이스는 결정된 블러 옵션에 기초하여 주변 영역을 블러 처리하는 이미지 렌더링을 수행한다. 증강 현실 디바이스는 사용자 입력에 의해 결정된 주변 영역에 관한 블러 처리 정도, 컬러, 및 밝기 중 적어도 하나에 기초하여 주변 영역에 관한 이미지 렌더링을 수행함으로써, 블러 이미지를 획 득할 수 있다. 도 13은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 사용자 입력에 의해 선택된 객체를 블러 처리 함으로써 블러 이미지를 획득하는 방법을 도시한 흐름도이다. 도 14는 본 개시의 일 실시예에 따른 증강 현실 디바이스가 사용자 입력에 의해 선택된 객체를 블러 처리 함으로써 블러 이미지를 획득하는 동작을 도시한 도면이다. 이하에서는, 도 13과 도 14를 함께 참조하여 증강 현실 디바이스의 동작에 대하여 설명하기로 한다. 도 13의 단계 S1310을 참조하면, 증강 현실 디바이스는 카메라(110, 도 2 참조)를 이용하여 현실 공간을 촬영함으로써 복수의 이미지 프레임을 획득한다. 현실 공간은 예를 들어, 사용자가 작업을 수행하는 작업 공간 일 수 있으나, 이에 한정되지 않는다. 단계 S1310은 도 3에 도시되고 설명된 단계 S310과 동일하므로, 중복되는 설명은 생략한다. 단계 S1320에서, 증강 현실 디바이스는 복수의 이미지 프레임 중 제1 이미지 프레임으로부터 복수의 객체 를 인식한다. 본 개시의 일 실시예에서, 증강 현실 디바이스의 프로세서(120, 도 2 참조)는 객체 인식 모 델(132, 도 2 참조)를 이용하여 복수의 이미지 프레임 중 제1 이미지 프레임으로부터 복수의 객체를 인식할 수 있다. 객체 인식 모델은 예를 들어, 영역 기반 컨볼루션 신경망 모델(Region-based Convolution Neural Network, R-CNN), 고속 영역 기반 컨볼루션 신경망 모델(Faster Region-based Convolution Neural Network, Faster R-CNN), 싱글 샷 멀티박스 디텍터 모델(Single Shot multibox Detector, SSD), YOLO v4, 센터 넷 (CenterNet), 또는 모바일 넷(MobileNet)으로 구현될 수 있다. 그러나, 본 개시의 객체 인식 모델이 전술 한 심층 신경망 모델로 한정되는 것은 아니다. 도 14를 함께 참조하면, 프로세서는 객체 인식 모델을 이용하여 제1 이미지 프레임으로부터 복수의 객체(1401, 1402, 1403, 1404, 1405, 1406)를 인식할 수 있다. 단계 S1330에서, 증강 현실 디바이스는 사용자 입력에 기초하여 제1 이미지 프레임으로부터 인식된 복수의 객체 중 적어도 하나의 객체를 선택한다. 본 개시의 일 실시예에서, 증강 현실 디바이스는 제1 이미지 프 레임으로부터 인식된 복수의 객체 중 적어도 하나의 객체를 선택하는 사용자 입력을 수신하는 사용자 입력부를 더 포함할 수 있다. 사용자 입력부는 예를 들어, 사용자의 손바닥 또는 손가락이 가리키는 영역 또는 객체를 인 식하는 핸드 트래킹 센서(hand tracking sensor)일 수 있다. 이 경우, 증강 현실 디바이스의 프로세서 는 핸드 트래킹 센서를 이용하여 복수의 객체 중 사용자의 손가락이 가리키는 적어도 하나의 객체를 인식 하고, 인식된 적어도 하나의 객체를 선택할 수 있다. 도 14를 함께 참조하면, 프로세서는 핸드 트래킹 센서를 통해 제1 이미지 프레임으로부터 인식된 복 수의 객체(1401, 1402, 1403, 1404, 1405) 중 제1 객체 및 제6 객체를 가리키는 사용자의 손가락 을 인식할 수 있다. 프로세서는 핸드 트래킹 센서에 의해 인식된 제1 객체 및 제6 객체를 선 택할 수 있다. 본 개시의 일 실시예에서, 프로세서는 선택된 제1 객체 및 제6 객체 각각을 포 함하는 바운딩 박스(B1, B2)를 제1 이미지 프레임 상에 표시할 수 있다. 그러나, 이에 한정되는 것은 아니고, 증강 현실 디바이스는 사용자의 양안의 시선 방향에 관한 정보를 획 득함으로써, 양얀의 시선이 수렴하는 응시점(gaze point)의 위치 좌표 정보를 획득하는 시선 추적 센서를 포함 할 수 있다. 이 경우, 프로세서는 복수의 객체(1401, 1402, 1403, 1404, 1405, 1406) 중 응시점이 기 설 정된 시간 이상 머무르는 객체를 인식하고, 인식된 객체를 선택할 수 있다. 단계 S1340에서, 증강 현실 디바이스는 사용자 입력에 의해 선택된 적어도 하나의 객체를 블러 처리하는 렌더링을 수행함으로써, 블러 이미지를 획득한다. 도 14를 함께 참조하면, 증강 현실 디바이스의 프로세서 는 객체 분할 모델(136, 도 2 참조)를 이용하여, 사용자 입력에 의해 선택된 제1 객체 및 제6 객체 를 제1 이미지 프레임으로부터 분할하고, 분할 이후 제1 객체 및 제6 객체에 대응되는 이미지 영역(1420, 1422)을 블러 처리함으로써 블러 이미지를 획득할 수 있다. 블러 이미지는 제1 객체에 대응되는 제1 영역 및 제6 객체에 대응되는 제2 영역만이 블러 처리되고, 나머 지 객체(1402, 1403, 1404, 1405) 및 주변 영역은 블러 처리되지 않은 상태일 수 있다. 단계 S1350에서, 증강 현실 디바이스는 블러 이미지(1410, 도 14 참조)를 디스플레이한다. 도 13 및 도 14에 도시된 실시예에 따른 증강 현실 디바이스는 사용자가 작업에 방해되거나, 집중력을 분 산시킬 수 있는 객체, 예를 들어 모바일 폰(도 14에서 제6 객체) 등을 직접 선택하고, 선택된 객체를 블 러 처리함으로써, 사용자에 의해 커스텀된 블러 이미지를 획득하고, 디스플레이할 수 있다. 따라서, 본 개시의 일 실시예에 따른 증강 현실 디바이스는 사용자의 작업 집중력을 향상시키고, 커스텀된 작업 환경 에 관한 사용자 경험을 제공할 수 있다. 본 개시에서 설명된 증강 현실 디바이스에 의해 실행되는 프로그램은 하드웨어 구성요소, 소프트웨어 구성 요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 프로그램은 컴퓨터로 읽 을 수 있는 명령어들을 수행할 수 있는 모든 시스템에 의해 수행될 수 있다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령어(instruction), 또는 이들 중 하나 이상 의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어는, 컴퓨터로 읽을 수 있는 저장 매체(computer-readable storage media)에 저장된 명령어를 포함하 는 컴퓨터 프로그램으로 구현될 수 있다. 컴퓨터가 읽을 수 있는 기록 매체로는, 예를 들어 마그네틱 저장 매체 (예를 들어, ROM(read-only memory), RAM(random-access memory), 플로피 디스크, 하드 디스크 등) 및 광학적 판독 매체(예컨대, 시디롬(CD-ROM), 디브이디(DVD: Digital Versatile Disc)) 등이 있다. 컴퓨터가 읽을 수 있는 기록 매체는 네트워크로 연결된 컴퓨터 시스템들에 분산되어, 분산 방식으로 컴퓨터가 판독 가능한 코드가 저장되고 실행될 수 있다. 매체는 컴퓨터에 의해 판독가능하며, 메모리에 저장되고, 프로세서에서 실행될 수 있다. 컴퓨터로 읽을 수 있는 저장매체는 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비 일시적'은 저장매체가 신호(signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임시적으로 저장되는 경우를 구분하지 않는다. 예를 들어, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 또한, 본 명세서에 개시된 실시예들에 따른 프로그램은 컴퓨터 프로그램 제품(computer program product)에 포 함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 소프트웨어 프로그램, 소프트웨어 프로그램이 저장된 컴퓨터로 읽을 수 있는 저장 매체 를 포함할 수 있다. 예를 들어, 컴퓨터 프로그램 제품은 증강 현실 디바이스의 제조사 또는 전자 마켓(예 를 들어, 삼성 갤럭시 스토어TM 또는 구글 플레이스토어TM)을 통해 전자적으로 배포되는 소프트웨어 프로그램 형 태의 상품(예를 들어, 다운로드 가능한 애플리케이션(downloadable application))을 포함할 수 있다. 전자적 배 포를 위하여, 소프트웨어 프로그램의 적어도 일부는 저장 매체에 저장되거나, 임시적으로 생성될 수 있다. 이 경우, 저장 매체는 증강 현실 디바이스의 제조사의 서버, 전자 마켓의 서버, 또는 소프트웨어 프로그램을 임시적으로 저장하는 중계 서버의 저장매체가 될 수 있다. 컴퓨터 프로그램 제품은, 증강 현실 디바이스 및/또는 서버로 구성되는 시스템에서, 서버의 저장매체 또는 증강 현실 디바이스의 저장매체를 포함할 수 있다. 또는, 증강 현실 디바이스와 통신 연결되는 제3 장치(예를 들어, 모바일 디바이스 또는 웨어러블 디바이스)가 존재하는 경우, 컴퓨터 프로그램 제품은 제3 장치 의 저장매체를 포함할 수 있다. 또는, 컴퓨터 프로그램 제품은 증강 현실 디바이스로부터 서버 또는 제3 장치로 전송되거나, 제3 장치로부터 증강 현실 디바이스로 전송되는 소프트웨어 프로그램 자체를 포함할 수 있다. 이 경우, 증강 현실 디바이스, 서버, 및 제3 장치 중 적어도 하나가 컴퓨터 프로그램 제품을 실행하여 개 시된 실시예들에 따른 방법을 수행할 수 있다. 또는, 증강 현실 디바이스, 서버, 및 제3 장치 중 어느 하 나가 컴퓨터 프로그램 제품을 실행하여 개시된 실시예들에 따른 방법을 분산하여 실시할 수 있다. 예를 들면, 증강 현실 디바이스가 메모리(130, 도 2 참조)에 저장된 컴퓨터 프로그램 제품을 실행하여, 증 강 현실 디바이스와 통신 연결된 타 전자 장치(예를 들어, 모바일 디바이스 또는 웨어러블 디바이스)가 개 시된 실시예들에 따른 방법을 수행하도록 제어할 수 있다. 또 다른 예로, 제3 장치가 컴퓨터 프로그램 제품을 실행하여, 제3 장치와 통신 연결된 전자 장치가 개시된 실시 예에 따른 방법을 수행하도록 제어할 수 있다. 제3 장치가 컴퓨터 프로그램 제품을 실행하는 경우, 제3 장치는 증강 현실 디바이스로부터 컴퓨터 프로그 램 제품을 다운로드하고, 다운로드된 컴퓨터 프로그램 제품을 실행할 수 있다. 또는, 제3 장치는 프리로드(pre- load)된 상태로 제공된 컴퓨터 프로그램 제품을 실행하여 개시된 실시예들에 따른 방법을 수행할 수도 있다."}
{"patent_id": "10-2022-0032943", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 컴퓨터 시스템 또는 모듈 등의 구성요소들이 설명된 방법과 다른 형태로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14"}
{"patent_id": "10-2022-0032943", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 개시는, 다음의 자세한 설명과 그에 수반되는 도면들의 결합으로 쉽게 이해될 수 있으며, 참조 번호 (reference numerals)들은 구조적 구성요소(structural elements)를 의미한다. 도 1은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 현실 공간에 관한 블러 이미지를 디스플레이하는 동 작을 도시한 개념도이다. 도 2는 본 개시의 일 실시예에 따른 증강 현실 디바이스의 구성 요소를 도시한 블록도이다. 도 3은 본 개시의 일 실시예에 따른 증강 현실 디바이스의 동작 방법을 도시한 흐름도이다. 도 4는 본 개시의 일 실시예에 따른 증강 현실 디바이스가 블러 이미지를 획득하는 동작을 도시한 도면이다. 도 5는 본 개시의 일 실시예에 따른 증강 현실 디바이스가 제1 이미지 프레임으로부터 사용자 정의 객체(user- defined object)를 식별하는 방법을 도시한 흐름도이다. 도 6은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 제1 이미지 프레임으로부터 사용자 정의 객체(user- defined object)를 인식하는 동작을 도시한 도면이다. 도 7은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 제1 이미지 프레임으로부터 사용자 정의 객체를 인식 하는 동작을 도시한 도면이다. 도 8은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 블러 이미지를 획득하는 동작을 도시한 도면이다. 도 9는 본 개시의 일 실시예에 따른 증강 현실 디바이스가 시간의 흐름에 따라 서로 다른 이미지를 디스플레이 하는 동작을 설명하기 위한 도면이다. 도 10은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 작업의 종류에 따라 서로 다른 블러 이미지를 디스 플레이하는 동작을 설명하기 위한 도면이다. 도 11은 본 개시의 일 실시예에 따른 증강 현실 디바이스가 블러 처리된 영역에 가상 객체 또는 그래픽 UI(Graphic User Interface)를 디스플레이하는 동작을 도시한 도면이다. 도 12는 본 개시의 일 실시예에 따른 증강 현실 디바이스가 사용자 입력에 기초하여 결정된 블러 옵션(blur option)에 따라 블러 이미지를 획득하는 방법을 도시한 흐름도이다. 도 13은 본 개시의 일 실시예에 따른 사용자 입력에 의해 선택된 객체를 블러 처리함으로써 블러 이미지를 획득 하는 방법을 도시한 흐름도이다. 도 14는 본 개시의 일 실시예에 따른 사용자 입력에 의해 선택된 객체를 블러 처리함으로써 블러 이미지를 획득 하는 동작을 도시한 도면이다."}
