{"patent_id": "10-2020-0002674", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0099966", "출원번호": "10-2020-0002674", "발명의 명칭": "명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 방법 및 장치", "출원인": "고려대학교 산학협력단", "발명자": "이홍철"}}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "원시 데이터로부터 제1 입력 데이터 및 제2 입력 데이터 획득하되, 상기 제1 입력 데이터 및 상기 제2 입력 데이터는 서로 상이한 형태의 데이터인 데이터 처리부;상기 제1 입력 데이터 및 상기 제2 입력 데이터 양자를 이용하여 학습을 수행하여 제1 출력 데이터를 획득하는제1 학습부;상기 원시 데이터를 이용하여 학습을 수행하여 제2 출력 데이터를 획득하는 제2 학습부; 및상기 제1 출력 데이터 및 상기 제2 출력 데이터를 이용하여 학습을 수행하는 조합부;를 더 포함하는 학습 장치."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 원시 데이터는, 상기 제2 입력 데이터에 대응하는 명목형 데이터를 포함하는 학습 장치."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 원시 데이터는, 상기 제1 입력 데이터에 대응하는 수치형 데이터를 더 포함하는 학습 장치."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 데이터처리부는, 상기 명목형 데이터를 미리 정의된 크기의 노드로 변환함으로써 상기 제2 입력 데이터를획득하는 학습 장치."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 제1 학습부는,상기 제1 입력 데이터 및 상기 제2 입력 데이터를 이용하여 학습을 수행하는 전체 데이터 학습부; 및상기 제2 입력 데이터만을 이용하여 학습을 수행하는 일부 데이터 학습부;를 포함하는 학습 장치."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 조합부와 연결되고 이미 학습된 모델의 정보를 기반으로 학습을 더 수행하는 전이 학습부;를 더 포함하는학습 장치.공개특허 10-2020-0099966-2-청구항 7 제1항에 있어서,상기 제1 학습부, 제2 학습부 및 조합부 중 적어도 하나는,심층 신경망(DNN), 콘볼루션 신경망(CNN), 순환 신경망(RNN), 콘볼루션 순환 신경망(CRNN), 다층 퍼셉트론(MLN), 심층 신뢰 신경망(DBN) 및 심층 Q-네트워크(Deep Q-Networks) 중 적어도 하나를 기반으로 상기 학습을수행하는 학습 장치."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "원시 데이터로부터 제1 입력 데이터 및 제2 입력 데이터 획득하되, 상기 제1 입력 데이터 및 상기 제2 입력 데이터는 서로 상이한 형태의 데이터인 단계;상기 제1 입력 데이터 및 상기 제2 입력 데이터 양자를 이용하여 학습을 수행하여 제1 출력 데이터를 획득하는단계;상기 원시 데이터를 이용하여 학습을 수행하여 제2 출력 데이터를 획득하는 단계; 및상기 제1 출력 데이터 및 상기 제2 출력 데이터를 이용하여 학습을 수행하는 단계;를 더 포함하는 학습 방법."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 원시 데이터는, 상기 제2 입력 데이터에 대응하는 명목형 데이터 및 상기 제1 입력 데이터에 대응하는 수치형 데이터를 포함하는 학습 방법."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 원시 데이터로부터 제1 입력 데이터 및 제2 입력 데이터 획득하는 단계는, 상기 명목형 데이터를 미리 정의된 크기의 노드로 변환함으로써 상기 제2 입력 데이터를 획득하는 단계;를 포함하는 학습 방법."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항에 있어서,상기 제1 입력 데이터 및 상기 제2 입력 데이터 양자를 이용하여 학습을 수행하여 제1 출력 데이터를 획득하는단계는,상기 제1 입력 데이터 및 상기 제2 입력 데이터를 이용하여 학습을 수행하는 단계; 및상기 제2 입력 데이터만을 이용하여 학습을 수행하는 단계; 중 적어도 하나를 포함하는 학습 방법."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항에 있어서,공개특허 10-2020-0099966-3-이미 학습된 모델의 정보를 기반으로 학습을 더 수행하는 단계;를 더 포함하는 학습 방법."}
{"patent_id": "10-2020-0002674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8항에 있어서,상기 학습을 수행하는 것은, 심층 신경망(DNN), 콘볼루션 신경망(CNN), 순환 신경망(RNN), 콘볼루션 순환 신경망(CRNN), 다층 퍼셉트론(MLN), 심층 신뢰 신경망(DBN) 및 심층 Q-네트워크(Deep Q-Networks) 중 적어도 하나를 기반으로 상기 학습을 수행하는 것을 포함하는 학습 방법."}
{"patent_id": "10-2020-0002674", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 방법 및 장치에 관한 것으로, 학습 장치는 원시 데이터 로부터 제1 입력 데이터 및 제2 입력 데이터 획득하되, 상기 제1 입력 데이터 및 상기 제2 입력 데이터는 서로 상이한 형태의 데이터인 데이터 처리부, 상기 제1 입력 데이터 및 상기 제2 입력 데이터 양자를 이용하여 학습을 수행하여 제1 출력 데이터를 획득하는 제1 학습부, 상기 원시 데이터를 이용하여 학습을 수행하여 제2 출력 데이 터를 획득하는 제2 학습부 및 상기 제1 출력 데이터 및 상기 제2 출력 데이터를 이용하여 학습을 수행하는 조합 부를 포함할 수 있다."}
{"patent_id": "10-2020-0002674", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2020-0002674", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "근자에 하드웨어 및 소프트웨어의 발달에 따라 컴퓨터 장치와 같은 하드웨어 자체가 스스로 학습하여 규칙을 생 성하는 기계 학습에 대한 연구가 다방면에서 이루어 지고 있다. 예를 들어, 기계 학습은 발생 가능한 다양한 상 황이나 상태 등을 예측하기 위해 이용되기도 한다. 예를 들어, 도로 상에서의 교통 사고 발생 가능성의 예측이 나, 시가지 또는 시외의 교통 흐름의 예측이나, 기계나 통신 장비의 장애 발생 여부의 예측이나, 또는 질병 발 생 가능성 여부 예측 등을 위해 기계 학습이 이용되고 있다. 그러나, 이와 같이 상황 판단을 위해 기계 학습을 이용할 때 서로 상이한 형태의 데이터를 입력 데이터로 이용하는 경우가 많아 종래의 단순한 학습 모델만으로는 적절한 학습 모델이나 예측 결과를 획득하기 어렵다. 예를 들어, 입력된 데이터 중 일부가 무의미해지는 경향이 있으며, 또한 입력 데이터의 반영이 제대로 수행되지 않아 예측 결과와 실제 결과 간에 상당한 차이가 존재하기 도 한다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 공개특허 제2017-0065898호 (특허문헌 0002) 대한민국 공개특허 제2015-0072471호"}
{"patent_id": "10-2020-0002674", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "명목형 데이터나 수치형 데이터 등과 같이 서로 상이한 형태의 데이터를 기반으로 학습을 수행하는 경우에도 보 다 우수하고 적절한 학습 결과를 획득할 수 있는 명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 방법 및 장치를 제공하는 것을 해결하고자 하는 과제로 한다."}
{"patent_id": "10-2020-0002674", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 과제를 해결하기 위하여 명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 방법 및 장치가 제공된 다. 학습 장치는 제1 입력 데이터 및 상기 제2 입력 데이터는 서로 상이한 형태의 데이터인 데이터 처리부, 상기 제 1 입력 데이터 및 상기 제2 입력 데이터 양자를 이용하여 학습을 수행하여 제1 출력 데이터를 획득하는 제1 학습부, 상기 원시 데이터를 이용하여 학습을 수행하여 제2 출력 데이터를 획득하는 제2 학습부 및 상기 제1 출력 데이터 및 상기 제2 출력 데이터를 이용하여 학습을 수행하는 조합부를 더 포함할 수 있다. 학습 방법은 원시 데이터로부터 제1 입력 데이터 및 제2 입력 데이터 획득하되, 상기 제1 입력 데이터 및 상기 제2 입력 데이터는 서로 상이한 형태의 데이터인 단계, 상기 제1 입력 데이터 및 상기 제2 입력 데이터 양자를 이용하여 학습을 수행하여 제1 출력 데이터를 획득하는 단계, 상기 원시 데이터를 이용하여 학습을 수행하여 제 2 출력 데이터를 획득하는 단계 및 상기 제1 출력 데이터 및 상기 제2 출력 데이터를 이용하여 학습을 수행하는 단계를 더 포함할 수 있다."}
{"patent_id": "10-2020-0002674", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상술한 명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 방법 및 장치에 의하면, 서로 명목형 또는 수치 형 등과 같이 서로 상이한 형태의 데이터를 기반으로 학습을 수행하는 경우에도 보다 적절하고 우수한 학습 결 과를 획득할 수 있게 되고 이에 따라 학습 모델의 성능 향상을 도모할 수 있는 효과를 얻을 수 있다. 상술한 명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 방법 및 장치에 의하면, 비대칭 클래스 구조를 보완하기 위해 이용되는 일반적인 데이터 오버 샘플링(over-sampling) 기법보다 과적합(over-fitting) 발생 위 험을 상당히 절감할 수 있는 장점도 얻을 수 있다. 상술한 명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 방법 및 장치에 의하면, 다른 학습 모델보다 성 능의 향상을 도모하면서도 모델의 안정성을 개선할 수 있는 효과도 얻을 수 있다. 상술한 명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 방법 및 장치에 의하면, 콘텐츠 추천 시스템이 나 위험 예측 시스템 등 명목형 데이터도 함께 이용하는 여러 시스템이나 서비스의 성능을 향상 및 개선하는 효 과도 얻을 수 있다."}
{"patent_id": "10-2020-0002674", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 명세서 전체에서 동일 참조 부호는 특별한 사정이 없는 한 동일 구성요소를 지칭한다. 이하에서 사용되는 '부'가 부가된 용어는, 소프트웨어 또는 하드웨어로 구현될 수 있으며, 실시예에 따라 하나의 '부'가 하나의 물 리적 또는 논리적 부품으로 구현되거나, 복수의 '부'가 하나의 물리적 또는 논리적 부품으로 구현되거나, 하나 의 '부'가 복수의 물리적 또는 논리적 부품들로 구현되는 것도 가능하다. 명세서 전체에서 어떤 부분이 다른 부분과 연결되어 있다고 할 때, 이는 어떤 부분과 다른 부분에 따라서 물리 적 연결을 의미할 수도 있고, 또는 전기적으로 연결된 것을 의미할 수도 있다. 또한, 어떤 부분이 다른 부분을 포함한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 부분 이외의 또 다른 부분을 제외하는 것이 아니 며, 설계자의 선택에 따라서 또 다른 부분을 더 포함할 수 있음을 의미한다. 제 1 이나 제 2 등의 용어는 하나의 부분을 다른 부분으로부터 구별하기 위해 사용되는 것으로, 특별한 기재가 없는 이상 이들이 순차적인 표현을 의미하는 것은 아니다. 또한 단수의 표현은 문맥상 명백하게 예외가 있지 않 는 한, 복수의 표현을 포함할 수 있다.이하 도 1 내지 도 6를 참조하여 학습 장치 및 이를 수행하기 위한 학습 처리부의 일 실시예에 대해서 설명하도 록 한다. 도 1은 학습 장치의 일 실시예에 대한 블록도이다. 도 1에 도시된 바와 같이, 학습 장치는, 입력된 적어도 하나의 데이터에 대한 학습을 수행하는 학습처리부 를 포함할 수 있으며, 필요에 따라, 저장부, 입력부 및 출력부 중 적어도 하나를 포함할 수 있다. 저장부, 입력부, 출력부 및 학습처리부 중 적어도 둘은, 케이블, 회로 및/또는 무선 통 신 장치 등을 통해 상호 데이터 송수신이 가능하게 마련된다. 저장부는 학습처리부의 동작에 필요한 적어도 하나의 데이터 및/또는 적어도 하나의 애플리케이션(소 프트웨어, 앱 또는 프로그램 등으로 지칭 가능하다)을 저장하며, 학습처리부의 요청에 따라 데이터 및/또 는 애플리케이션 등을 학습처리부로 제공할 수 있도록 마련된다. 일 실시예에 의하면, 저장부는 학습을 위해 학습처리부에 입력될 원시 데이터를 저장할 수 있다. 원시 데이터는, 적어도 하나의 데이터가 집합적으로 형성된 적어도 하나의 데이터 셋, 일례로 제1 데이터 셋 내지 제n 데이터 셋(1-1 내지 1-n, 여기서 n은 2보다 큰 자연수)을 포함할 수 있으며, 이들 데이터 셋(1-1 내지 1-n) 중 적어도 둘은 서로 상이한 형태의 데이터의 집합으로 구축된 것일 수 있다. 예를 들어, 제1 데이터 셋 (1-1)은 수치형 데이터(스칼라 형식의 데이터)의 집합을 포함하고, 제2 데이터 셋(1-2)은 명목형 데이터(질적 데이터 등으로 표현 가능함)의 집합을 포함할 수도 있다. 상황에 따라서, 이들 데이터 셋(1-1, 1-2)은 서로 분 리되어 저장부에 저장되어 있을 수도 있고, 또는 서로 혼재되어 저장부에 저장되어 있을 수도 있다. 즉, 하나의 원시 데이터에는 서로 상이한 형태의 데이터, 일례로 명목형 데이터(1-1) 및 수치형 데이터(1- 2)가 서로 혼재되어 있을 수 있다. 실시예에 따라서, 저장부는 학습처리부에 의해 학습되거나 또는 학습에 이용될 학습 모델을 더 저 장할 수도 있다. 학습 모델은 예를 들어 학습 알고리즘을 포함할 수 있다. 학습 알고리즘은, 예를 들어, 심 층 신경망(DNN), 콘볼루션 신경망(CNN), 순환 신경망(RNN), 콘볼루션 순환 신경망(CRNN), 다층 퍼셉트론(MLN), 심층 신뢰 신경망(DBN) 및/또는 심층 Q-네트워크(Deep Q-Networks) 등을 포함할 수 있으나, 이에 한정되는 건 아니다. 또한, 저장부는 학습처리부가 전이 학습을 수행하는 경우, 학습처리부의 전이 학습에 이 용될 전이학습용 매개변수를 더 포함할 수도 있다. 이외에도 저장부는 학습처리부의 동작/처리에 필요한 모델, 데이터 및/또는 애플리케이션 등을 더 저장할 수도 있다. 저장부는, 예를 들어, 주기억장치 및 보조기억장치 중 적어도 하나를 포함할 수 있다. 주기억장치는 롬 (ROM) 및/또는 램(RAM)과 같은 반도체 저장 매체를 이용하여 구현된 것일 수 있다. 롬은, 예를 들어, 통상적인 롬, 이프롬(EPROM), 이이프롬(EEPROM) 및/또는 마스크롬(MASK-ROM) 등을 포함할 수 있다. 램은 예를 들어, 디램 (DRAM) 및/또는 에스램(SRAM) 등을 포함할 수 있다. 보조기억장치는, 플래시 메모리 장치, SD(Secure Digital) 카드, 솔리드 스테이트 드라이브(SSD, Solid State Drive), 하드 디스크 드라이브(HDD, Hard Disc Drive), 자 기 드럼, 컴팩트 디스크(CD), 디브이디(DVD) 또는 레이저 디스크 등과 같은 광 기록 매체(optical media), 자기 테이프, 광자기 디스크 및/또는 플로피 디스크 등과 같이 데이터를 영구적 또는 반영구적으로 저장 가능한 적어 도 하나의 저장 매체를 이용하여 구현될 수 있다. 입력부는 외부로부터 명령/지시를 입력 받거나 및/또는 적어도 하나의 데이터를 입력받을 수 있다. 예를 들 어, 입력부는 사용자의 조작에 따라 학습 개시 명령이나 학습 결과 출력 명령 등을 입력 받거나 및/또는 원 시 데이터에 해당하는 적어도 하나의 데이터나, 전이학습용 매개변수나, 학습 모델 등을 입력 받을 수 있다. 입력부는, 예를 들어, 키보드, 마우스, 키패드, 터치스크린, 터치패드, 트랙볼, 트랙패드, 스캐너, 외장 메모리 장치(외장 하드디스크나 외장 플래쉬 메모리 장치 등) 등이 장착 가능한 데이터 단자(예를 들어, 범용 직렬 버스 단자 등), 유선 또는 무선 통신 모듈 및/또는 미리 별도로 생성된 데이터 셋(1-1 내지 1-n) 또 는 사용자의 조작에 대응하여 입력되는 데이터 등을 수신할 수 있는 적어도 하나의 장치를 포함할 수 있으나, 이에 한정되는 것은 아니다. 출력부는 학습처리부의 학습 결과, 저장부에 저장된 원시 데이터, 학습 모델 및/또는 전이 학습용 매개변수 등을 사용자의 지시에 따라 또는 미리 정의된 설정에 따라 외부로 출력할 수 있다. 출력부 는, 예를 들어, 화상 출력 장치, 프린터 장치 또는 사운드 출력 장치 등을 포함할 수 있다.학습처리부는 원시 데이터를 기반으로 학습을 수행하여, 소정의 학습 모델, 일례로 저장부에 저장 된 학습 모델을 학습시켜 학습된 모델을 획득하거나, 및/또는 소정의 입력 데이터를 학습된 모델에 적용하여 입력 데이터에 대응하는 결과 데이터를 획득할 수 있다. 학습처리부의 동작에 대한 상세한 설명은 후술한 다. 학습처리부는, 하나 또는 둘 이상의 프로세서를 이용하여 구현될 수 있다. 여기서, 프로세서는, 예를 들어, 중앙 처리 장치(CPU, Central Processing Unit), 마이크로 컨트롤러 유닛(MCU, Micro Controller Unit), 마이컴(Micom, Micro Processor), 애플리케이션 프로세서(AP, Application Processor), 전자 제어 유닛(ECU, Electronic Controlling Unit) 및/또는 각종 연산 처리 및 제어 신호의 생성이 가능한 다른 전자 장치 등을 포 함할 수 있다. 이들 장치는 예를 들어 하나 또는 둘 이상의 반도체 칩 및 관련 부품을 이용하여 구현 가능하다. 학습처리부는, 저장부에 저장된 애플리케이션(미도시)을 구동시켜 학습과 관련된 연산, 판단, 처리 및 /또는 제어 동작 등을 수행할 수도 있다. 여기서, 저장부에 저장된 애플리케이션은, 설계자에 의해 미리 작 성되거나 외부의 저장 매체(예를 들어, 외장 하드디스크 장치 등)로부터 전달되어 저장부에 저장된 것일 수 도 있고, 및/또는 유선 또는 무선 통신 네트워크를 통해 접속 가능한 전자 소프트웨어 유통망을 통하여 획득 또 는 갱신된 것일 수도 있다 상술한 학습 장치는, 데이터의 연산, 처리 및 이와 관련된 각종 제어 동작을 수행할 수 있는 정보처리장치 를 기반으로 구현 가능하다. 예를 들어, 학습 장치는 데스크톱 컴퓨터, 랩톱 컴퓨터, 서버용 컴퓨터 장치, 스마트 폰, 태블릿 피씨, 스마트 시계, 두부 장착형 디스플레이(HMD, Head Mounted Display) 장치, 내비게이션 장치, 개인용 디지털 보조기(PDA, Personal Digital Assistant), 휴대용 게임기, 디지털 텔레비전, 셋톱 박스, 가전기기(로봇 청소기, 세탁기 또는 냉장고 등), 전자 광고판, 인공지능 음향 재생 장치, 차량, 로봇, 기계 장 치 또는 이외 학습 처리를 수행할 수 있는 다양한 장치 중 적어도 하나를 포함할 수 있다. 도 2는 학습 장치의 동작의 일례를 설명하기 위한 도면이다. 도 2에 도시된 일 실시예에 따르면, 원시 데이터는 학습 처리부에 입력되고, 학습 처리부는 원시 데이터를 기반으로 학습을 수행하여 학습 결과(120a, 이하 제1 학습 결과)를 획득할 수 있다. 보다 구체적으 로 학습처리부는 원시 데이터가 입력되면, 원시 데이터를 기반으로 입력 데이터를 생성 및 획 득하고, 모든 입력 데이터(114, 일례로 명목형 데이터에 대응하는 데이터 및 수치형 데이터에 대응하는 데이 터)를 전체 데이터를 위한 학습부(121-1, 이하 전체 데이터 학습부)에 입력하여 학습을 수행하거나, 및/또는 입 력 데이터 중에서 명목형 데이터에 대응하는 데이터를 명목형 데이터를 위한 학습부(121-2, 이하 일부 데 이터 학습부)에 입력하여 학습을 수행하여 학습을 수행한 후, 전체 데이터 학습부(121-1)와 일부 데이터 학습부 (121-2)의 학습 결과를 기반으로 입력 데이터에 대응하는 학습 결과(121a, 이하 제1 출력 데이터)를 획득 할 수 있다. 또한, 학습처리부는 이와는 별도로 원시 데이터 그 자체에 대한 학습부(122, 이하 제2 학 습부)에 원시 데이터를 입력하여 제2 출력 데이터(122a)를 획득하고, 제1 출력 데이터(121a)와 제2 출력 데 이터(122a)를 조합하여 제1 학습 결과(120a)를 획득할 수 있다. 이 경우, 제1 출력 데이터(121a)와 제2 출력 데 이터(122a)의 조합은 앙상블 모델(ensemble model)을 채용하여 수행될 수 있다. 도 3은 학습 처리부의 일 실시예를 설명하기 위한 블록도이다. 보다 구체적으로 도 4에 도시된 바와 같이, 학습 처리부는, 일 실시예에 의하면, 데이터처리부, 조합 학습부 및 결과출력부를 포함할 수 있다. 데이터처리부는 원시 데이터를 획득하고, 원시 데이터 내의 데이터의 전부 또는 일부를 변환하거나 또는 변환하지 않음으로써 조합학습부에 입력될 입력 데이터를 획득할 수 있다. 이 경우, 데이터처리 부는 데이터획득부, 데이터변환부 및 입력 데이터 생성부를 포함하여 마련된 것일 수 있다. 데이터획득부는 저장부로부터 원시 데이터의 적어도 하나의 데이터 셋(1-1 내지 1-n)을 획득하고, 획득된 데이터 셋(1-1 내지 1-n)의 데이터의 형태에 따라서 데이터를 데이터변환부 또는 입력 데이터 생성 부로 전달할 수 있다. 예를 들어, 데이터획득부는 데이터 셋(1-1 내지 1-n) 중에서 수치형 데이터를 포함하는 데이터 셋, 일례로 제1 데이터 셋(1-1)은 입력 데이터 생성부로 전달하고, 명목형 데이터를 포함하는 데이터 셋, 일례로 제2 데이터 셋(1-2)은 데이터변환부로 전달하도록 설계될 수 있다. 이에 따라 수 치형 데이터, 일례로 제1 데이터 셋(1-1)은 변환 없이 단일 값 그대로 입력 데이터 생성부로 전달된다. 데이터변환부는 수신한 데이터를 변환하고, 변환된 데이터를 입력 데이터 생성부로 전달할 수 있다. 예를 들어, 데이터변환부는 명목형 데이터, 일례로 제2 데이터 셋(1-2)의 적어도 하나의 데이터를 수신하 고, 수신한 각각의 데이터를 미리 정의된 크기의 노드(node)로 변경하고, 미리 정의된 크기의 노드로 변경된 데 이터를 입력 데이터 생성부로 전달할 수 있다. 이 경우, 명목형 데이터는 미리 정의된 바에 따라 모두 동 일한 크기의 노드로 변경될 수 있다. 노드의 크기는 사용자에 의해 지정된 것을 수도 있고 또는 설계자에 의해 미리 정의된 것일 수도 있다. 도 4는 임베디드 레이어의 일례를 설명하기 위한 도면으로, 명목형 데이터를 원-핫(one-hot) 형식으로 표현한 도면이다. 입력 데이터 생성부는 데이터획득부 및/또는 데이터 변환부에서 전달된 적어도 하나의 데이터를 기반으로 조합학습부에 입력될 데이터를 생성할 수 있다. 이 경우, 입력 데이터 생성부는 데이터 획 득부에서 전달된 데이터에 대응하는 입력 데이터(114-1, 이하 제1 입력 데이터) 및 데이터변환부에서 전달된 데이터에 대응하는 입력 데이터(114-2, 이하 제2 입력 데이터)를 생성함으로써, 조합학습부에 입력 될 데이터를 생성할 수 있다. 여기서, 제1 입력 데이터(114-1)는 수치형 데이터들로 이루어지고 및/또는 제2 입 력 데이터(114-2)는 명목형 데이터들로 이루어진 것일 수 있다. 입력 데이터 생성부는 필요에 따라, 입력 데이터(114-1, 114-2)를 변환할 수도 있다. 예를 들어, 입력 데이터 생성부는 분석에 불필요한 데이터를 제거하거나, 입력 데이터(114-1, 114-2) 중 적어도 하나에 대한 변환 처리(예를 들어, 원-핫(one-hot) 인코딩 처리 등)를 수행하거나 및/또는 입력 데이터(114-1, 114-2) 중 적어도 하나에 대한 스케일링 처리(예를 들어, 데이터 범위를 0 내지 1 사이의 값으로 변경 처리 등)를 수행하는 등 학습의 신속 또는 정확성 개선 등을 위해 필요한 처리를 더 수행할 수도 있다. 이와 같은 과정에 의해 조합 학습부에 입력될 입력 데이터가 형 성될 수 있게 된다. 입력 데이터 생성부에 의해 생성된 입력 데이터는 조합학습부의 제1 학습부 로 전달되고, 제1 학습부에서 임베디드 레이어(embedded layer)로 이용될 수 있다. 임베디드 레이어 는 사용자 또는 설계자에 의해 미리 정의된 개수(k)의 노드에 완전히 연결(fully connected)되어 생성된 레이어 로, 후술하는 바와 같이 조합학습부에 입력 값(입력 층)으로 이용된다. 도 4에 도시된 예를 참조하면, 임 베디드 레이어는 명목형 데이터(원-핫 형식에 따라 feature size 내에서 1로 표현된 데이터) 및 수치형 데이터 (1 이외의 다른 값을 갖는 데이터) 등을 포함하여 생성된다. 임베디드 레이어는 사용자 또는 설계자에 의해 미 리 정의된 소정의 값(k), 일례로 노드의 크기와 특성의 크기를 곱한 값을 그 크기로 가질 수 있다. 조합학습부는, 도 3에 도시된 바와 같이 제1 학습부, 제2 학습부 및 조합부를 포함할 수 있다. 제1 학습부는 입력 데이터 생성부로부터 입력 데이터(114-1, 114-2)를 수신하고 이를 기반으로 학습 을 수행한다. 제1 학습부는, 일 실시예에 있어서, 전체데이터 학습부(121-1) 및 일부 데이터 학습부(121- 2)를 포함할 수 있다. 전체데이터 학습부(121-1)는, 임베디드 레이어의 모든 종류의 데이터, 일례로 제1 입력 데이터(114-1) 및 제2 입력 데이터(114-2) 양자를 모두 이용해서 학습을 수행할 수 있다. 다시 말해서, 전체데이터 학습부(121- 1)는, 변환되지 않은 수치형 데이터(114-1, 즉, 원시적 형태 그대로의 수치형 데이터)와, 소정 크기의 노드로 변환된 명목형 데이터(114-2)를 함께 이용하여 학습을 수행할 수 있다. 전체데이터 학습부(121-1)는, 예를 들어, 심층 신경망, 콘볼루션 신경망, 순환 신경망, 콘볼루션 순환 신경망, 다층 퍼셉트론, 심층 신뢰 신경망 및/또는 심층 Q-네트워크 등을 이용하여 학습을 수행할 수도 있다. 일 실시예에 의하면, 전체데이터 학습부 (121-1)는 적어도 하나 이상의 은닉층을 포함할 수 있으며, 구체적으로 예를 들어 4개의 은닉층을 포함할 수도 있다. 은닉층의 크기는, 예를 들어, 시행착오(trial-error)를 기반으로 결정된 것일 수 있다. 일부 데이터 학습부(121-2)는, 제1 입력 데이터(114-1) 및 제2 입력 데이터(114-2) 중에서 명목형 데이터로부터 도출된 입력 데이터, 즉 제2 입력 데이터(114-2)를 기반으로 학습을 수행할 수 있다. 따라서, 일부 데이터 학습 부(121-2)는 동일하게 소정 크기의 노드로 변환된 데이터를 기반으로 학습을 수행하게 된다. 일부 데이터 학습 부(121-2)는 제2 입력 데이터(114-2)를 선형적으로 분석할 수 있다. 필요에 따라, 일부 데이터 학습부(121-2)는 가중치를 제2 입력 데이터(114-2)에 적용하는 것도 가능하다. 일부 데이터 학습부(121-2)는, 예를 들어, 인수화모델(FM: Factorization Model) 등을 기반으로 학습을 수행할 수도 있다. 각각의 데이터(1-2)가 동일한 크기의 노드로 변환되었으므로, 명목형 데이터로 인해 발생되는 희박한 특성이 완화되고, 내적 연산의 수행이 가능해진 다. 실시예에 따라서, 제1 출력 데이터(121a)의 획득을 위해 클래스 가중치(class weight)가 더 이용될 수도 있 다. 클래스 가중치는, 각각의 클래스(class)에 할당되는 가중치로, 여기서 클래스는 기계 학습 등에서 이용되는 레이블의 범주를 포함할 수 있다. 도 5는 명목형 데이터 및 수치형 데이터 구분을 위한 프로그램 코드의 일례를 도시한 도면이다. 필요에 따라서 제1 학습부는 일부 데이터 학습부(121-2)의 동작 이전에 전체적인 입력 데이터(141, 즉 임 베디드 레이어)에서 명목형 데이터(141-2)를 분류할 수도 있다. 구체적으로 도 5의 코드에 나타난 바와 같이 제 1 학습부는 전체 임베이드 레이어 중에서 동일한 크기(예를 들어, 사용자 등에 의해 설정된 노드의 크기) 로 처리된 명목형 데이터만을 추출하고 이를 일부 데이터 학습부(121-2)로 전달하기 위해 슬라이싱 전처리를 더 수행할 수도 있다. 보다 상세하게는 제1 학습부는 전체적인 입력 데이터 내의 각각의 데이터에 대해 순차적으로 데이터의 값(특징)이 무엇인지(예를 들어, 1인지 또는 그 외의 값인지) 여부를 판단하고, 데이터의 값에 따라서 각각의 데이터에 식별을 위한 값을 부기하고, 부기된 값에 따라 각각의 데이터를 특정한 학습부 (121-1, 121-2)에 입력시킴으로써 전체적인 입력 데이터를 분류할 수도 있다. 전체데이터 학습부(121-1) 및 일부 데이터 학습부(121-2)의 각각의 학습 결과에 따라서, 도 2에 도시된 바와 같 이, 제1 출력 데이터(121a)가 획득될 수 있다. 제1 출력 데이터(121a)는, 예를 들어, 제1 학습부의 학습에 따른 예측 결과를 포함할 수 있다. 일 실시예에 의하면, 제1 학습부의 제1 출력 데이터(121a)의 획득 과정은, 심층인수화모델(DeepFM: Deep Factorization Model)이 결과 데이터를 획득하는 과정과 동일한 과정을 통해 또는 이를 일부 변형한 과정을 통 해 수행될 수 있다. 상술한 제1 학습부의 학습 과정은, 예를 들어 하기의 수학식 1으로 표현될 수도 있다. [수학식 1]"}
{"patent_id": "10-2020-0002674", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서 는 일부 데이터 학습부(121-2)에서 수행되는 학습을 의미하고, 는 전체데이터 학습부(121-1)에서 수행되는 학습을 의미한다. 여기서 가중치 ( )는 학습 과정에서 변경될 수도 있다. 는 제1 출력 데이터(121a)를 의미한다. 통상적인 학습 알고리즘에서는 불균형한 클래스의 편향 학습 완화를 위해 클래스 내의 데이터를 인위적으로 생 성하거나 또는 삭제하는 등의 데이터 전처리 과정을 필요로 한다. 그러나 이와 같은 전처리 과정은 학습에 있어 서 과적합의 위험성을 증가시킬 수 있다. 그러나, 상술한 제1 학습부는 데이터의 인위적 생성 및 제거의 과정을 필요로 하지 않고 또한 필요에 따라 클래스 가중치를 더 이용하여 학습을 수행할 수 있으므로, 종래의 다른 학습 알고리즘을 가지고 있는 과적합 문제를 해결할 수 있게 된다. 제1 학습부의 학습 결과에 따라 획득된 제1 출력 데이터(121a)는 조합부로 전달될 수 있다. 제2 학습부는, 원시 데이터(1: 1-1 내지 1-n)를 기반으로 학습을 수행하도록 마련된 것일 수 있다. 다시 말해서, 제1 학습부는 데이터처리부에서 획득한 입력 데이터(114-1, 114-2)를 기반으로 학습을 수행 하나, 제2 학습부는 입력 데이터(114-1, 114-2)가 아닌 저장부에 저장된 원시 데이터(1: 1-1 내지 1- n)을 기반으로 학습을 수행한다. 제2 학습부는, 예를 들어, 심층 신경망, 콘볼루션 신경망, 순환 신경망, 콘볼루션 순환 신경망, 다층 퍼셉트론, 심층 신뢰 신경망 및/또는 심층 Q-네트워크 등을 기반으로 학습을 수행 할 수 있다. 이 경우, 제2 학습부로 이용되는 학습 알고리즘은, 상술한 전체데이터 학습부(121-1) 등에서 이용되는 학습 알고리즘과 동일할 수도 있고 상이할 수도 있다. 제2 학습부의 학습 결과, 즉 제2 출력 데 이터(122a) 역시 조합부로 전달될 수 있다. 제2 출력 데이터(122a)는, 예를 들어, 제2 학습부의 학습에 따른 예측 결과를 포함할 수 있다. 조합부는, 제1 학습부의 학습 결과와 제2 학습부의 학습 결과를 병합하여, 도 2에 도시된 바와 같이, 제1 학습 결과(120a), 일례로 새로운 학습된 모델을 획득할 수 있다. 보다 구체적으로, 조합부는 제 1 학습부의 출력 데이터(121a) 및 제2 학습부의 출력 데이터(122a)를 수신하고, 이들 출력 데이터 (121a, 122a)를 입력 값으로 하여 학습을 더 수행할 수 있다. 다시 말해서, 제1 출력 데이터(121a) 및 제2 출력 데이터(122a)가 학습 데이터(training data)로 이용될 수 있다. 일 실시예에 의하면, 조합부에 의한 학습 모델의 생성 과정은, 앙상블 모델(ensemble model)의 스태킹(stacking) 방식을 동일하게 또는 일부 상이하게 이 용하여 수행될 수도 있다. 조합부에 의해 획득된 제1 학습 결과(120a, 예를 들어, 학습 모델이나 학습 모델에 적용되어 획득된 결과 데이터 등)는 결과 출력부로 전달될 수 있다. 결과 출력부는 수신한 학습 결과를 출력부로 전달 하여 외부로 출력되도록 하거나 및/또는 저장부로 전달하여 일시적 또는 비일시적으로 저장되도록 할 수 있 다. 상술한 두 개의 단계, 즉 제1 출력 데이터(121a) 및 제2 출력 데이터(122a)를 획득하는 단계 및 출력 결과를 조 합하여 새로운 모델을 생성하는 단계의 수행은, 단일 모델만을 이용하여 학습을 수행하는 경우에 비해 상대적으 로 성능이 향상되고 모델 성능의 안정화도 꾀할 수 있는 장점이 있다. 실시예에 따라서, 제1 학습부, 제2 학습부 및 조합부 중 적어도 하나는, 전이 학습(Transfer Learning)을 더 적용하여 학습을 수행할 수도 있다. 도 6은 전이 학습의 수행 동작의 일례를 설명하기 위한 도면이다. 일 실시예에 의하면, 도 3 및 도 6에 도시된 바와 같이 조합부와 전이 학습부는 상호 연결되어 학습 을 수행하도록 마련될 수도 있다. 보다 구체적으로 전이 학습부는, 적어도 하나의 노드를 각각 갖는 적어 도 하나의 은닉층을 포함할 수 있으며, 조합 학습부의 은닉층의 적어도 하나의 노드는 전이 학습부의 은닉층의 적어도 하나의 노드와 연결되어 전체적으로 학습 모델을 형성하게 된다. 조합부 및 전이 학습부 는 전이 학습을 기반으로 학습을 더 수행하여 학습 결과(130a, 이하 제2 학습 결과)를 획득할 수 있다. 이 경우, 전이 학습부는 기존에 이미 학습된 모델의 정보(일례로 전이학습용 매개변수)를 기반으로 학습을 수행함으로써 보다 개선된 학습 모델을 획득할 수 있다. 다시 말해서, 전이 학습부는, 학습 장치가 학 습하고자 하는 모델(예를 들어, 교통량 예측 모델)과 관련되고 사전에 학습된 적어도 하나의 모델을 이용하여 학습을 수행하되, 기학습된 모델에 속하는 적어도 하나의 은닉층 내의 적어도 하나의 노드의 매개변수를 각 각 재활용하여 학습을 수행할 수 있다. 여기서, 전이 학습의 수행을 위해 기학습된 모델의 적어도 하나의 은닉 층 중에서 모든 은닉층이 이용될 수도 있고 일부의 은닉층만이 이용될 수도 있다. 또한, 전이 학습을 위해 동일 한 은닉층 내의 모든 노드의 매개변수가 이용될 수도 있고, 또는 모든 노드 중 일부의 노드의 매개변수만 이 이용될 수도 있다. 전이 학습부에 의해 획득된 제2 학습 결과(130a)는, 대외적 출력 및/또는 내부적 저장을 위해 결과 출력부 로 전달될 수도 있다. 이와 같이 전이 학습부를 더 포함하는 경우, 제1 학습 결과(120a)는, 설계자의 선택에 따라서, 결과 출력부로 전달될 수도 있고 또는 전달되지 않을 수도 있다. 상술한 데이터처리부와, 조합학습부와, 전입학습부와, 및/또는 결과출력부와, 이들에 포함 된 각각의 요소(111, 112, 113, 121, 122, 123) 등은 물리적으로 구분되는 것일 수도 있고 및/또는 논리적으로 구분되는 것일 수도 있다. 예를 들어, 데이터처리부, 조합학습부, 전입학습부 및 결과출력부 는, 하나의 반도체 장치로 이루어진 프로세서를 통해 구현될 수도 있고, 각각 물리적으로 분리된 서로 상 이한 복수의 프로세서를 통해 구현될 수도 있다. 이외에도 설계자의 선택에 따라서 상술한 데이터처리부, 조합학습부, 전입학습부 및/또는 결과출력부 등은 다양한 방식으로 적어도 하나의 물리적 프로 세서를 이용하여 구현 가능하다. 이하, 도 7을 참조하여 학습 방법의 일 실시예에 대해서 설명한다. 도 7은 학습 방법의 일 실시예에 대한 흐름도이다. 도 7에 도시된 바에 의하면, 먼저 학습 모델의 학습에 이용될 원시 데이터가 획득된다. 원시 데이터는 명 목형 데이터를 포함할 수 있으며, 명목형 데이터 이외의 다른 데이터(예를 들어, 수치형 데이터)를 더 포함할 수 있다. 원시 데이터는, 장치 내부의 저장 매체에 미리 저장된 것일 수도 있고, 사용자나 설계자에 의해 직접 입력된 것일 수도 있고, 외부의 다른 저장 매체를 통해 입력된 것일 수도 있으며, 또는 유무선 통신 네트워크를 통해 전달된 것일 수도 있다. 원시 데이터를 기반으로 입력 데이터가 생성될 수 있다. 이 경우, 원시 데이터 중에서 명목형 데이터는 동 일한 크기의 노드로 변환하고, 수치형 데이터는 그대로 변환 없이 유지함으로써 입력 데이터를 생성할 수도 있 다. 생성된 입력 데이터는 학습 모델에 있어서 임베디드 레이어로 이용될 수 있다. 생성된 입력 데이터의 전부(즉, 명목형 데이터 및 수치형 데이터 모두)는 전체 데이터 학습을 위해 이용될 수 있다. 전체 데이터 학습은 심층 신경망, 콘볼루션 신경망, 순환 신경망, 콘볼루션 순환 신경망, 다층 퍼셉 트론, 심층 신뢰 신경망 및/또는 심층 Q-네트워크 등 소정의 기계학습 알고리즘을 기반으로 수행될 수 있다. 한편, 생성된 입력 데이터 중에서 명목형 데이터는 명목형 데이터 학습에 추가적으로 이용될 수도 있다. 필요에 따라, 명목형 데이터 학습에 선행하여 입력 데이터 중에서 명목 데이터를 별도로 추출하는 과정이 더 수 행될 수도 있다. 명목 데이터의 학습은 인수화 모델을 기반으로 수행될 수도 있다. 필요에 따라 명목형 데이터 학습을 위해 클래스 가중치를 더 이용하는 것도 가능하다. 전체 데이터 학습에 따른 결과와 명목형 데이터 학습에 따른 결과가 획득되면 이들 결과의 결합에 의해 제1 출 력 데이터가 획득된다. 한편, 입력 데이터의 생성 과정 내지 제1 출력 데이터의 획득 과정(302 내지 308)과 동시에 또는 순차적으로 제 2 학습이 더 수행될 수 있다. 제2 학습은 원시 데이터를 이용하여 수행된다. 제2 학습 역시 심층 신경망, 콘볼루션 신경망, 순환 신경망, 콘볼루션 순환 신경망, 다층 퍼셉트론, 심층 신뢰 신경망 및/또는 심층 Q-네트 워크 등의 학습 알고리즘을 기반으로 수행될 수 있다. 제2 학습에 응하여 제2 출력 데이터가 획득된다. 제1 출력 데이터 및 제2 출력 데이터가 획득되면(308, 312), 제1 출력 데이터 및 제2 출력 데이터는 조합된다 . 구체적으로 제1 출력 데이터 및 제2 출력 데이터를 입력 값으로 하여 학습이 더 수행되고 이에 따라 새 로운 학습 모델이 생성될 수 있다. 실시예에 따라서, 제1 출력 데이터 및 제2 출력 데이터의 조합 기반의 학습 모델의 생성은 스태킹 방식을 이용하여 수행되는 것일 수도 있다. 새롭게 생성된 학습 모델은, 시각적 및/또는 청각적 형태로 외부로 출력되거나 또는 저장 매체 등에 일시적 또 는 비일시적으로 저장될 수 있다. 일 실시예에 의하면, 전이 학습이 추가적으로 더 수행될 수도 있다. 즉, 조합에 의한 학습 모델의 생성은, 전이 학습을 더 적용하여 수행될 수도 있다. 전이 학습에 따라 생성된 학습 모델 역시 외부에 출력되거나 저장 매체 등에 저장될 수 있다. 상술한 실시예에 따른 학습 방법은, 컴퓨터 장치에 의해 구동될 수 있는 프로그램의 형태로 구현될 수 있다. 여 기서 프로그램은, 프로그램 명령, 데이터 파일 및 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 프로그램은 기계어 코드나 고급 언어 코드를 이용하여 설계 및 제작된 것일 수 있다. 프로그램은 상술한 방법을 구현하기 위하여 특별히 설계된 것일 수도 있고, 컴퓨터 소프트웨어 분야에서 통상의 기술자에게 기 공지되어 사용 가능한 각종 함수나 정의를 이용하여 구현된 것일 수도 있다. 또한, 여기서, 컴퓨터 장치는, 프로그램의 기능을 실현 가능하게 하는 프로세서나 메모리 등을 포함하여 구현된 것일 수 있으며, 필요에 따라 통신 장치를 더 포함할 수도 있다. 상술한 학습 방법을 구현하기 위한 프로그램은, 컴퓨터에 의해 판독 가능한 기록 매체에 기록될 수 있다. 컴퓨 터에 의해 판독 가능한 기록 매체는, 예를 들어, 하드 디스크나 플로피 디스크와 같은 자기 디스크 저장 매체, 자기 테이프, 콤팩트 디스크나 디브이디와 같은 광 기록 매체, 플롭티컬 디스크와 같은 자기-광 기록 매체 및 롬, 램 또는 플래시 메모리 등과 같은 반도체 저장 장치 등 컴퓨터 등의 호출에 따라 실행되는 특정 프로그램을 저장 가능한 다양한 종류의 하드웨어 장치를 포함할 수 있다. 상술한 학습 방법 및 장치는 다양한 분야에 적용될 수 있다. 예를 들어, 상술한 학습 방법 및 장치는, 사용자의 데이터를 기반으로 사용자에게 특정한 정보나 데이터를 제공 또는 추천하기 위한 시스템에서 이용될 수 있으며, 보다 구체적으로 예를 들어, 광고, 영화, 보험, 쇼핑(일례로 인터넷 쇼핑이나 텔레비전 기반 홈쇼핑 등) 및/또 는 콘텐츠 제공 서비스(일례로, 인터넷 프로토콜 텔레비전 서비스 등) 등 다양한 분야의 시스템, 장치 또는 서비스 등에 적용될 수 있다. 또한, 상술한 학습 방법 및 장치는, 기존의 데이터를 기반으로 현재 또는 미래의 상 황/상태를 예측하기 위한 시스템(일례로 교통사고 데이터 기반 위험 예측 시스템 등)에도 적용 가능하다. 이외 에도 상술한 학습 방법 및 장치는 명목형 데이터를 갖는 다수의 데이터 기반의 학습 시스템, 장치 및 방법(서비 스 등)에도 동일하게 또는 일부 변형된 방식을 이용하여 적용 가능하다. 이상 명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 장치 및 방법의 여러 실시예에 대해 설명하였으나, 장치 및 방법은 오직 상술한 실시예에 한정되는 것은 아니다. 해당 기술 분야에서 통상의 지식을 가진 자가 상술한 실시예를 기초로 수정 및 변형하여 구현 가능한 다양한 장치나 방법 역시 상술한 명목형 데이 터를 포함하는 데이터를 기반으로 하는 학습 장치 및 방법의 일례가 될 수 있다. 예를 들어, 설명된 기술들이 설명된 방법과 다른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성 요소들이 설명된 방법과 다른 형태로 결합 또는 조합되거나 다른 구성 요소 또는 균등물에 의하여 대치되거나 또는 치환되더라도 상술한 명목형 데이터를 포함하는 데이터를 기반으로 하는 학습 장치 및 방법의 일 실시예가 될 수 있다."}
{"patent_id": "10-2020-0002674", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 학습 장치의 일 실시예에 대한 블록도이다. 도 2는 학습 장치의 동작의 일례를 설명하기 위한 도면이다. 도 3은 학습 처리부의 일 실시예를 설명하기 위한 블록도이다. 도 4는 임베디드 레이어의 일례를 설명하기 위한 도면이다. 도 5는 명목형 데이터 및 수치형 데이터 구분을 위한 프로그램 코드의 일례를 도시한 도면이다. 도 6은 전이 학습의 수행 동작의 일례를 설명하기 위한 도면이다. 도 7은 학습 방법의 일 실시예에 대한 흐름도이다."}
