{"patent_id": "10-2023-0038744", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0131211", "출원번호": "10-2023-0038744", "발명의 명칭": "인공지능 모델을 이용하여 연산을 수행하는 전자 장치 및 전자 장치의 동작 방법", "출원인": "삼성전자주식회사", "발명자": "김정배"}}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,인공지능 모델을 저장하는 메모리; 및프로세서를 포함하고,상기 프로세서는상기 메모리에 저장된 인공지능 모델을 로딩하여 프레임워크(framework)의 런타임 엔진(runtime engine)을 실행시키고,타깃 프로세서 상에서 실행 함수(operation)를 지원하는지 확인하고,상기 타깃 프로세서 상에서 실행 함수(operation)를 지원함에 기반하여 상기 인공지능 모델 상에서 추론(inference)을 실행하기 위한 첫 번째 노드가 에러(error)없이 동작하는지 확인하고,첫 번째 노드가 에러(error)없이 동작함에 기반하여 노드를 하나 더 추가하여 마지막 노드까지 확인을반복하고,N번째 노드에서 에러가 발생하는 것으로 확인됨에 기반하여 첫번째 노드부터 확인된 N-1번째 노드까지 파티션(partition)을 생성하여 제 1 그룹을 형성하고,에러가 발생한 N번째 노드에 대해 파티션(partition)을 생성하여 제 2 그룹을 형성하는 전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 제 1 그룹에 포함된 노드들은 컴파일(compile) 및/또는 해석(interpreting)을 수행하는 장치(Xpu)들 중 어느 하나에 의해 실행되고,상기 제 2 그룹에 포함된 노드는 컴파일(compile) 및/또는 해석(interpreting)을 수행하는 장치(Xpu)들 중 상기제 1 그룹과는 다른 장치에 의해 실행되며,상기 프로세서는N+1 번째 노드부터 마지막 노드까지 상기 인공지능 모델 상에서 추론(inference)이 에러 없이 실행되는지 확인을 반복하는 전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 프로세서는상기 타깃 프로세서가 실행 함수(operation)을 지원하지 않음에 기반하여 컴파일(compile) 및/또는 해석(interpreting)을 수행하는 장치(Xpu)들 중 어느 하나를 이용하여 실행 함수(operation)를 지원하도록 제어하는전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "공개특허 10-2024-0131211-3-제 1항에 있어서,상기 프로세서는노드를 새롭게 추가한 상황에서 상기 타깃 프로세서를 이용하여 새롭게 추가된 노드까지 포함하더라도 에러 없이 추론(inference)을 실행할 수 있는지 확인을 수행하는 전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4항에 있어서,상기 프로세서는상기 타깃 프로세서를 이용하여 확인을 수행하고,새롭게 추가된 노드를 포함하면 에러 없이 추론(inference)을 실행하기 어려운 것으로 결정됨에 기반하여 이전노드들과 새롭게 추가된 노드들 사이에 파티션(partition)을 생성하는 전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1항에 있어서,상기 프로세서는상기 인공지능 모델의 컴파일이 완료됨에 기반하여 상기 타깃 프로세서에서 사용 가능하도록 컴파일 또는 해석(interpreting)된 커널을 상기 메모리에 저장하는 전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "전자 장치에 있어서,인공지능 모델을 저장하는 메모리; 및프로세서를 포함하고,상기 프로세서는상기 메모리에 저장된 인공지능 모델을 로딩하여 프레임워크(framework)의 런타임 엔진(runtime engine)을 실행시키고,상기 전자장치 상의 하드웨어에 대한 정보를 획득하고,타깃 프로세서 상에서 실행 함수(operation)을 지원하는지 확인하고,상기 타깃 프로세서 상에서 실행 함수(operation)을 지원함에 기반하여 상기 인공지능 모델 상에서 추론(inference)을 실행하기 위한 첫 번째 노드가 에러(error)없이 동작하는지 확인하고,상기 확인이 통과됨에 기반하여 노드를 하나 더 추가하여 마지막 노드까지 확인을 반복하고,N번째 노드에서 상기 확인이 통과되지 않음에 기반하여 첫번째 노드부터 확인이 통과된 N-1번째 노드까지 파티션(partition)을 생성하여 제 1 그룹을 형성하고,상기 N번째 노드가 에러(error)없이 동작하는지 확인하고,상기 N번째 노드가 에러(error)없이 동작함에 기반하여 노드를 하나 더 추가하여 마지막 노드까지 확인을 반복하고,K번째 노드에서 에러(error)가 발생함에 기반하여 N번째 노드부터 확인된 K-1번째 노드까지 파티션(partition)을 생성하여 제 2 그룹을 형성하고,공개특허 10-2024-0131211-4-마지막 노드까지 에러(error)가 발생하는지 확인을 반복하며,상기 N,K는 자연수를 의미하고, 상기 K는 N보다 상대적으로 큰 값을 갖는 전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7항에 있어서,상기 전자장치 상의 하드웨어에 대한 정보는상기 메모리의 사이즈, 동작 주파수의 속도를 포함하는 전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 7항에 있어서,상기 프로세서는상기 타깃 프로세서가 실행 함수(operation)을 지원하지 않음에 기반하여 컴파일(compile) 및/또는 해석(interpreting)을 수행하는 장치(Xpu)들 중 어느 하나를 이용하여 실행 함수(operation)를 지원하도록 제어하는전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 7항에 있어서,상기 프로세서는노드를 새롭게 추가한 상황에서 상기 타깃 프로세서를 이용하여 새롭게 추가된 노드까지 포함하더라도 에러 없이 추론(inference)을 실행할 수 있는지 확인하는 전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10항에 있어서,상기 프로세서는상기 타깃 프로세서를 이용하여 확인을 수행하고,새롭게 추가된 노드를 포함하면 에러 없이 추론(inference)을 실행하기 어려운 것으로 결정됨에 기반하여 이전노드들과 새롭게 추가된 노드들 사이에 파티션(partition)을 생성하는 전자 장치."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 7항에 있어서,상기 프로세서는상기 인공지능 모델의 컴파일이 완료됨에 기반하여 상기 타깃 프로세서에서 사용 가능하도록 컴파일된 결과물을상기 메모리에 저장하며,상기 컴파일된 결과물은커널(kernel) 또는 이미지를 포함하는 전자 장치.공개특허 10-2024-0131211-5-청구항 13 전자 장치의 동작 방법에 있어서,메모리에 저장된 인공지능 모델을 로딩하여 프레임워크(framework)의 런타임 엔진(runtime engine)을 실행하는동작;타깃 프로세서 상에서 실행 함수(operation)을 지원하는지 확인하는 동작;상기 타깃 프로세서 상에서 실행 함수(operation)을 지원함에 기반하여 상기 인공지능 모델 상에서 추론(inference)을 실행하기 위한 첫 번째 노드가 에러(error)없이 동작하는지 확인하는 동작;첫 번째 노드가 에러(error)없이 동작함에 기반하여 노드를 하나 더 추가하여 마지막 노드까지 확인을 반복하는동작;N번째 노드에서 에러가 발생함에 기반하여 첫번째 노드부터 확인된 N-1번째 노드까지 파티션(partition)을 생성하여 제 1 그룹을 형성하는 동작;및에러가 발생하는 N번째 노드에 대해 파티션(partition)을 생성하여 제 2 그룹을 형성하는 동작을 포함하는방법."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13항에 있어서,상기 전자 장치의 동작 방법은N+1 번째 노드부터 마지막 노드까지 상기 인공지능 모델 상에서 추론(inference)이 에러 없이 실행되는지 확인을 반복하는 동작을 더 포함하고,상기 제 1 그룹에 포함된 노드들은 컴파일(compile) 및/또는 해석(interpreting)을 수행하는 장치(Xpu)들 중 어느 하나에 의해 실행되고,상기 제 2 그룹에 포함된 노드는 컴파일(compile) 및/또는 해석(interpreting)을 수행하는 장치(Xpu)들 중 상기제 1 그룹과는 다른 장치에 의해 실행되는 방법."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 13항에 있어서,상기 타깃 프로세서 상에서 실행 함수(operation)을 지원하는지 확인하는 동작은 상기 타깃 프로세서가 실행 함수(operation)을 지원하지 않음에 기반하여 컴파일(compile) 및/또는 해석(interpreting)을 수행하는 장치(Xpu)들 중 어느 하나를 이용하여 실행 함수(operation)를 지원하도록 제어하는동작을 더 포함하는 방법."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 13항에 있어서,상기 전자 장치의 동작 방법은 노드를 새롭게 추가한 상황에서 상기 타깃 프로세서를 이용하여 새롭게 추가된 노드까지 포함하더라도 에러 없이 추론(inference)을 실행할 수 있는지 확인하는 동작을 더 포함하는 방법."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "공개특허 10-2024-0131211-6-제 16항에 있어서,상기 전자 장치의 동작 방법은 상기 타깃 프로세서를 이용하여 에러 발생 여부를 확인하고,새롭게 추가된 노드를 포함하면 에러 없이 추론(inference)을 실행하기 어려운 것으로 결정됨에 기반하여 이전노드들과 새롭게 추가된 노드들 사이에 파티션(partition)을 생성하는 동작을 더 포함하는 방법."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "저장 매체에 있어서,인공지능 모델을 저장하는 메모리; 및프로세서를 포함하고,상기 메모리는 실행시에 상기 프로세서가상기 메모리에 저장된 인공지능 모델을 로딩하여 프레임워크(framework)의 런타임 엔진(runtime engine)을 실행시키고,타깃 프로세서 상에서 실행 함수(operation)을 지원하는지 확인하고,상기 타깃 프로세서 상에서 실행 함수(operation)을 지원함에 기반하여 상기 인공지능 모델 상에서 추론(inference)을 실행하기 위한 첫 번째 노드가 에러(error)없이 동작하는지 확인하고,첫 번째 노드가 에러 없이 동작함에 기반하여 노드를 하나 더 추가하여 마지막 노드까지 확인을 반복하고,N번째 노드에서 에러가 발생함에 기반하여 첫번째 노드부터 확인된 N-1번째 노드까지 파티션(partition)을 생성하여 제 1 그룹을 형성하고,에러가 발생한 N번째 노드에 대해 파티션(partition)을 생성하여 제 2 그룹을 형성하고,N+1 번째 노드부터 마지막 노드까지 상기 인공지능 모델 상에서 추론(inference)이 에러 없이 실행되는지 확인을 반복하는 인스트럭션들을 저장하는 저장 매체."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 18항에 있어서,상기 제 1 그룹에 포함된 노드들은 컴파일(compile) 및/또는 해석(interpreting)을 수행하는 장치(Xpu)들 중 어느 하나에 의해 실행되고,상기 제 2 그룹에 포함된 노드는 컴파일(compile) 및/또는 해석(interpreting)을 수행하는 장치(Xpu)들 중 상기제 1 그룹과는 다른 장치에 의해 실행되는 저장 매체."}
{"patent_id": "10-2023-0038744", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 18항에 있어서,상기 메모리는, 실행 시에, 상기 프로세서가노드를 새롭게 추가한 상황에서 상기 타깃 프로세서를 이용하여 새롭게 추가된 노드까지 포함하더라도 에러 없이 추론(inference)을 실행할 수 있는지 확인하는 인스트럭션을 더 포함하는 저장 매체."}
{"patent_id": "10-2023-0038744", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치는 인공지능 모델을 저장하는 메모리 및 프로세서를 포함할 수 있다. 프로세서는 메모리에 저장된 인공 지능 모델을 로딩하여 프레임워크(framework)의 런타임 엔진(runtime engine)을 실행시키고, 타깃 프로세서 상에 서 실행 함수(operation)를 지원하는지 확인하고, 타깃 프로세서 상에서 실행 함수(operation)를 지원함에 기반 하여 인공지능 모델 상에서 추론(inference)을 실행하기 위한 첫 번째 노드가 에러(error)없이 동작하는지 확인 하고, 첫 번째 노드가 에러(error)없이 동작함에 기반하여 노드를 하나 더 추가하여 마지막 노드까지 확인을 반 복하고, N번째 노드에서 에러가 발생하는 것으로 확인됨에 기반하여 첫번째 노드부터 확인된 N-1번째 노드까지 파티션(partition)을 생성하여 제 1 그룹을 형성하고, 에러가 발생한 N번째 노드에 대해 파티션(partition)을 생 성하여 제 2 그룹을 형성할 수 있다."}
{"patent_id": "10-2023-0038744", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 문서는 전자 장치에 관한 것이며, 구체적으로는 인공지능 모델을 이용하여 연산을 수행하는 전자 장치 및 전 자 장치의 동작 방법에 관한 것이다."}
{"patent_id": "10-2023-0038744", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 신경망(artificial neural network; ANN)은 생물학적 뇌를 모델링한 컴퓨터 과학적 아키텍쳐 (computational architecture)를 참조한다. 인공 신경망에 기초하여 딥러닝(deep learning) 또는 기계 학습 (machine learning) 등이 구현될 수 있다. 인공 신경망의 예시로서 심층 신경망(deep neural network) 또는 딥 러닝(deep learning)은 복수의 레이어들을 포함하는 멀티 레이어 구조를 가질 수 있다. 시각, 음성을 분석하는 데에 인공지능 모델(AI 모델)이 다양하게 활용되고 있다. 모바일 단말에서 인공지능 모 델을 효과적으로 동작 시키기 위해 인공지능 모델과 관련된 하드웨어 기술에 대한 연구 개발이 활발하게 진행되 고 있다. 예를 들어, 딥러닝 인공지능 모델에서 수행되는 MAC 연산 (multiply-accumulate) 최적화 연구뿐만 아 니라 인공지능 모델을 고려한 하드웨어의 구조개선 기술 또한 연구, 적용 되고 있다. 또한, 데이터 프로세싱 시스템은 일반적으로 중앙 처리 장치(센트럴 프로세싱 유닛, CPU, central processing unit)으로 알려져 있는 적어도 하나의 프로세서를 포함할 수 있다. 그러한 데이터 프로세싱 시스템은 또한 다양 한 타입의 특화된 프로세싱을 위해 사용되는 적어도 하나의 다른 프로세서, 예를 들어 뉴럴 프로세싱 유닛(NPU, neural processing unit)을 포함할 수 있다. 인공지능 학습모델은 복잡한 graph형태로 구성되어 학습될 수 있다. 도 3a 및 도 3b에 도시된 것처럼 1 개의 학 습 모델은 많은 수(예: 100,000개)의 노드(node)를 가질 수도 있다. 인공지능 학습모델의 백엔드(backend)에서 많은 수(예: 100,000개)의 노드(node)를 처리하기 위해 컴파일(compile) 또는 해석(interpreting)을 할 수 있 다. 이 때 인공지능 학습모델의 백엔드(backend)에서는 필요한 메모리가 실제 전자 장치 상에서 사용 가능한 메 모리의 사이즈(size) 보다 커지게 되어 정상적으로 컴파일(compile) 또는 해석(interpreting)을 수행하기 어려 울 수 있다. 본 문서에 따른 인공지능 모델을 이용하여 연산을 수행하는 전자 장치는 단일한 파티션이 아닌 복수 개의 파티 션을 이용하여 노드를 분리하고, 인공지능 학습모델 상에서 정상적으로 컴파일(compile) 또는 해석 (interpreting)을 수행하도록 제어하고자 한다. 전자 장치는 인공지능 모델을 저장하는 메모리 및 프로세서를 포함할 수 있다. 프로세서는 메모리에 저장된 인 공지능 모델을 로딩하여 프레임워크(framework)의 런타임 엔진(runtime engine)을 실행시키고, 타깃 프로세서 상에서 실행 함수(operation)를 지원하는지 확인하고, 타깃 프로세서 상에서 실행 함수(operation)를 지원함에 기반하여 인공지능 모델 상에서 추론(inference)을 실행하기 위한 첫 번째 노드가 에러(error)없이 동작하는지 확인하고, 첫 번째 노드가 에러(error)없이 동작함에 기반하여 노드를 하나 더 추가하여 마지막 노드까지 확인 을 반복하고, N번째 노드에서 에러가 발생하는 것으로 확인됨에 기반하여 첫번째 노드부터 확인된 N-1번째 노드 까지 파티션(partition)을 생성하여 제 1 그룹을 형성하고, 에러가 발생한 N번째 노드에 대해 파티션 (partition)을 생성하여 제 2 그룹을 형성할 수 있다. 전자 장치의 동작 방법은 메모리에 저장된 인공지능 모델을 로딩하여 프레임워크(framework)의 런타임 엔진 (runtime engine)을 실행하는 동작, 타깃 프로세서 상에서 실행 함수(operation)을 지원하는지 확인하는 동작, 상기 타깃 프로세서 상에서 실행 함수(operation)을 지원함에 기반하여 상기 인공지능 모델 상에서 추론 (inference)을 실행하기 위한 첫 번째 노드가 에러(error)없이 동작하는지 확인하는 동작, 첫 번째 노드가 에러 (error)없이 동작함에 기반하여 노드를 하나 더 추가하여 마지막 노드까지 확인을 반복하는 동작, N번째 노드에 서 에러가 발생함에 기반하여 첫번째 노드부터 확인된 N-1번째 노드까지 파티션(partition)을 생성하여 제 1 그 룹을 형성하는 동작및 에러가 발생하는 N번째 노드에 대해 파티션(partition)을 생성하여 제 2 그룹을 형성하는 동작을 포함할 수 있다. 본 문서에 따른 인공지능 모델을 이용하여 연산을 수행하는 전자 장치는 단일한 파티션이 아닌 복수 개의 파티 션을 이용하여 노드를 분리할 수 있다. 이러한 동작을 통해 전자 장치는 실제 메모리 사이즈의 한계에도 불구하고 인공지능 학습모델 상에서 정상적으로 컴파일(compile) 또는 해석(interpreting)을 수행하도록 제어할 수 있 다."}
{"patent_id": "10-2023-0038744", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 1, "content": "도 1은, 다양한 실시예들에 따른, 네트워크 환경 내의 전자 장치의 블록도이다. 도 1을 참조하면, 네 트워크 환경에서 전자 장치는 제 1 네트워크(예: 근거리 무선 통신 네트워크)를 통하여 전자 장 치와 통신하거나, 또는 제 2 네트워크(예: 원거리 무선 통신 네트워크)를 통하여 전자 장치 또 는 서버 중 적어도 하나와 통신할 수 있다. 일실시예에 따르면, 전자 장치는 서버를 통하여 전 자 장치와 통신할 수 있다. 일실시예에 따르면, 전자 장치는 프로세서, 메모리, 입력 모듈 , 음향 출력 모듈, 디스플레이 모듈, 오디오 모듈, 센서 모듈, 인터페이스, 연 결 단자, 햅틱 모듈, 카메라 모듈, 전력 관리 모듈, 배터리, 통신 모듈, 가입 자 식별 모듈, 또는 안테나 모듈을 포함할 수 있다. 어떤 실시예에서는, 전자 장치에는, 이 구 성요소들 중 적어도 하나(예: 연결 단자)가 생략되거나, 하나 이상의 다른 구성요소가 추가될 수 있다. 어 떤 실시예에서는, 이 구성요소들 중 일부들(예: 센서 모듈, 카메라 모듈, 또는 안테나 모듈)은 하나의 구성요소(예: 디스플레이 모듈)로 통합될 수 있다. 프로세서는, 예를 들면, 소프트웨어(예: 프로그램)를 실행하여 프로세서에 연결된 전자 장치 의 적어도 하나의 다른 구성요소(예: 하드웨어 또는 소프트웨어 구성요소)를 제어할 수 있고, 다양한 데이 터 처리 또는 연산을 수행할 수 있다. 일실시예에 따르면, 데이터 처리 또는 연산의 적어도 일부로서, 프로세서 는 다른 구성요소(예: 센서 모듈 또는 통신 모듈)로부터 수신된 명령 또는 데이터를 휘발성 메 모리에 저장하고, 휘발성 메모리에 저장된 명령 또는 데이터를 처리하고, 결과 데이터를 비휘발성 메 모리에 저장할 수 있다. 일실시예에 따르면, 프로세서는 메인 프로세서(예: 중앙 처리 장치 또 는 어플리케이션 프로세서) 또는 이와는 독립적으로 또는 함께 운영 가능한 보조 프로세서(예: 그래픽 처 리 장치, 신경망 처리 장치(NPU: neural processing unit), 이미지 시그널 프로세서, 센서 허브 프로세서, 또는 커뮤니케이션 프로세서)를 포함할 수 있다. 예를 들어, 전자 장치가 메인 프로세서 및 보조 프로세서 를 포함하는 경우, 보조 프로세서는 메인 프로세서보다 저전력을 사용하거나, 지정된 기능에 특 화되도록 설정될 수 있다. 보조 프로세서는 메인 프로세서와 별개로, 또는 그 일부로서 구현될 수 있 다. 보조 프로세서는, 예를 들면, 메인 프로세서가 인액티브(예: 슬립) 상태에 있는 동안 메인 프로세서 를 대신하여, 또는 메인 프로세서가 액티브(예: 어플리케이션 실행) 상태에 있는 동안 메인 프로세서 와 함께, 전자 장치의 구성요소들 중 적어도 하나의 구성요소(예: 디스플레이 모듈, 센서 모듈 , 또는 통신 모듈)와 관련된 기능 또는 상태들의 적어도 일부를 제어할 수 있다. 일실시예에 따르면, 보조 프로세서(예: 이미지 시그널 프로세서 또는 커뮤니케이션 프로세서)는 기능적으로 관련 있는 다른 구 성요소(예: 카메라 모듈 또는 통신 모듈)의 일부로서 구현될 수 있다. 일실시예에 따르면, 보조 프로세서(예: 신경망 처리 장치)는 인공지능 모델의 처리에 특화된 하드웨어 구조를 포함할 수 있다. 인공지능 모델은 기계 학습을 통해 생성될 수 있다. 이러한 학습은, 예를 들어, 인공지능 모델이 수행되는 전자 장치 자체에서 수행될 수 있고, 별도의 서버(예: 서버)를 통해 수행될 수도 있다. 학습 알고리즘은, 예를 들어, 지도형 학습(supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi- supervised learning) 또는 강화 학습(reinforcement learning)을 포함할 수 있으나, 전술한 예에 한정되지 않 는다. 인공지능 모델은, 복수의 인공 신경망 레이어들을 포함할 수 있다. 인공 신경망은 심층 신경망(DNN: deep neural network), CNN(convolutional neural network), RNN(recurrent neural network), RBM(restricted boltzmann machine), DBN(deep belief network), BRDNN(bidirectional recurrent deep neural network), 심층 Q-네트워크(deep Q-networks) 또는 상기 중 둘 이상의 조합 중 하나일 수 있으나, 전술한 예에 한정되지 않는다. 인공지능 모델은 하드웨어 구조 이외에, 추가적으로 또는 대체적으로, 소프트웨어 구조를 포함할 수 있 다. 메모리는, 전자 장치의 적어도 하나의 구성요소(예: 프로세서 또는 센서 모듈)에 의해 사 용되는 다양한 데이터를 저장할 수 있다. 데이터는, 예를 들어, 소프트웨어(예: 프로그램) 및, 이와 관련 된 명령에 대한 입력 데이터 또는 출력 데이터를 포함할 수 있다. 메모리는, 휘발성 메모리 또는 비 휘발성 메모리를 포함할 수 있다. 프로그램은 메모리에 소프트웨어로서 저장될 수 있으며, 예를 들면, 운영 체제, 미들 웨어 또는 어플리케이션을 포함할 수 있다. 입력 모듈은, 전자 장치의 구성요소(예: 프로세서)에 사용될 명령 또는 데이터를 전자 장치 의 외부(예: 사용자)로부터 수신할 수 있다. 입력 모듈은, 예를 들면, 마이크, 마우스, 키보드, 키 (예: 버튼), 또는 디지털 펜(예: 스타일러스 펜)을 포함할 수 있다. 음향 출력 모듈은 음향 신호를 전자 장치의 외부로 출력할 수 있다. 음향 출력 모듈은, 예를 들 면, 스피커 또는 리시버를 포함할 수 있다. 스피커는 멀티미디어 재생 또는 녹음 재생과 같이 일반적인 용도로 사용될 수 있다. 리시버는 착신 전화를 수신하기 위해 사용될 수 있다. 일실시예에 따르면, 리시버는 스피커와 별개로, 또는 그 일부로서 구현될 수 있다. 디스플레이 모듈은 전자 장치의 외부(예: 사용자)로 정보를 시각적으로 제공할 수 있다. 디스플레이 모듈은, 예를 들면, 디스플레이, 홀로그램 장치, 또는 프로젝터 및 해당 장치를 제어하기 위한 제어 회로 를 포함할 수 있다. 일실시예에 따르면, 디스플레이 모듈은 터치를 감지하도록 설정된 터치 센서, 또는 상 기 터치에 의해 발생되는 힘의 세기를 측정하도록 설정된 압력 센서를 포함할 수 있다. 오디오 모듈은 소리를 전기 신호로 변환시키거나, 반대로 전기 신호를 소리로 변환시킬 수 있다. 일실시예 에 따르면, 오디오 모듈은, 입력 모듈을 통해 소리를 획득하거나, 음향 출력 모듈, 또는 전자 장치와 직접 또는 무선으로 연결된 외부 전자 장치(예: 전자 장치)(예: 스피커 또는 헤드폰)를 통해 소리를 출력할 수 있다. 센서 모듈은 전자 장치의 작동 상태(예: 전력 또는 온도), 또는 외부의 환경 상태(예: 사용자 상태) 를 감지하고, 감지된 상태에 대응하는 전기 신호 또는 데이터 값을 생성할 수 있다. 일실시예에 따르면, 센서 모듈은, 예를 들면, 제스처 센서, 자이로 센서, 기압 센서, 마그네틱 센서, 가속도 센서, 그립 센서, 근접 센서, 컬러 센서, IR(infrared) 센서, 생체 센서, 온도 센서, 습도 센서, 또는 조도 센서를 포함할 수 있다. 인터페이스는 전자 장치가 외부 전자 장치(예: 전자 장치)와 직접 또는 무선으로 연결되기 위해 사용될 수 있는 하나 이상의 지정된 프로토콜들을 지원할 수 있다. 일실시예에 따르면, 인터페이스는, 예 를 들면, HDMI(high definition multimedia interface), USB(universal serial bus) 인터페이스, SD카드 인터 페이스, 또는 오디오 인터페이스를 포함할 수 있다. 연결 단자는, 그를 통해서 전자 장치가 외부 전자 장치(예: 전자 장치)와 물리적으로 연결될 수 있는 커넥터를 포함할 수 있다. 일실시예에 따르면, 연결 단자는, 예를 들면, HDMI 커넥터, USB 커넥터, SD 카드 커넥터, 또는 오디오 커넥터(예: 헤드폰 커넥터)를 포함할 수 있다. 햅틱 모듈은 전기적 신호를 사용자가 촉각 또는 운동 감각을 통해서 인지할 수 있는 기계적인 자극(예: 진 동 또는 움직임) 또는 전기적인 자극으로 변환할 수 있다. 일실시예에 따르면, 햅틱 모듈은, 예를 들면, 모터, 압전 소자, 또는 전기 자극 장치를 포함할 수 있다.카메라 모듈은 정지 영상 및 동영상을 촬영할 수 있다. 일실시예에 따르면, 카메라 모듈은 하나 이상 의 렌즈들, 이미지 센서들, 이미지 시그널 프로세서들, 또는 플래시들을 포함할 수 있다. 전력 관리 모듈은 전자 장치에 공급되는 전력을 관리할 수 있다. 일실시예에 따르면, 전력 관리 모듈 은, 예를 들면, PMIC(power management integrated circuit)의 적어도 일부로서 구현될 수 있다. 배터리는 전자 장치의 적어도 하나의 구성요소에 전력을 공급할 수 있다. 일실시예에 따르면, 배터리 는, 예를 들면, 재충전 불가능한 1차 전지, 재충전 가능한 2차 전지 또는 연료 전지를 포함할 수 있다. 통신 모듈은 전자 장치와 외부 전자 장치(예: 전자 장치, 전자 장치, 또는 서버) 간 의 직접(예: 유선) 통신 채널 또는 무선 통신 채널의 수립, 및 수립된 통신 채널을 통한 통신 수행을 지원할 수 있다. 통신 모듈은 프로세서(예: 어플리케이션 프로세서)와 독립적으로 운영되고, 직접(예: 유선) 통 신 또는 무선 통신을 지원하는 하나 이상의 커뮤니케이션 프로세서를 포함할 수 있다. 일실시예에 따르면, 통신 모듈은 무선 통신 모듈(예: 셀룰러 통신 모듈, 근거리 무선 통신 모듈, 또는 GNSS(global navigation satellite system) 통신 모듈) 또는 유선 통신 모듈(예: LAN(local area network) 통신 모듈, 또는 전력선 통신 모듈)을 포함할 수 있다. 이들 통신 모듈 중 해당하는 통신 모듈은 제 1 네트워크(예: 블루투스, WiFi(wireless fidelity) direct 또는 IrDA(infrared data association)와 같은 근거리 통신 네트워 크) 또는 제 2 네트워크(예: 레거시 셀룰러 네트워크, 5G 네트워크, 차세대 통신 네트워크, 인터넷, 또는 컴퓨터 네트워크(예: LAN 또는 WAN)와 같은 원거리 통신 네트워크)를 통하여 외부의 전자 장치와 통신할 수 있다. 이런 여러 종류의 통신 모듈들은 하나의 구성요소(예: 단일 칩)로 통합되거나, 또는 서로 별도의 복수 의 구성요소들(예: 복수 칩들)로 구현될 수 있다. 무선 통신 모듈은 가입자 식별 모듈에 저장된 가입 자 정보(예: 국제 모바일 가입자 식별자(IMSI))를 이용하여 제 1 네트워크 또는 제 2 네트워크와 같 은 통신 네트워크 내에서 전자 장치를 확인 또는 인증할 수 있다. 무선 통신 모듈은 4G 네트워크 이후의 5G 네트워크 및 차세대 통신 기술, 예를 들어, NR 접속 기술(new radio access technology)을 지원할 수 있다. NR 접속 기술은 고용량 데이터의 고속 전송(eMBB(enhanced mobile broadband)), 단말 전력 최소화와 다수 단말의 접속(mMTC(massive machine type communications)), 또 는 고신뢰도와 저지연(URLLC(ultra-reliable and low-latency communications))을 지원할 수 있다. 무선 통신 모듈은, 예를 들어, 높은 데이터 전송률 달성을 위해, 고주파 대역(예: mmWave 대역)을 지원할 수 있다. 무선 통신 모듈은 고주파 대역에서의 성능 확보를 위한 다양한 기술들, 예를 들어, 빔포밍(beamforming), 거대 배열 다중 입출력(massive MIMO(multiple-input and multiple-output)), 전차원 다중입출력(FD-MIMO: full dimensional MIMO), 어레이 안테나(array antenna), 아날로그 빔형성(analog beam-forming), 또는 대규모 안테나(large scale antenna)와 같은 기술들을 지원할 수 있다. 무선 통신 모듈은 전자 장치, 외부 전자 장치(예: 전자 장치) 또는 네트워크 시스템(예: 제 2 네트워크)에 규정되는 다양한 요구사항을 지원할 수 있다. 일실시예에 따르면, 무선 통신 모듈은 eMBB 실현을 위한 Peak data rate(예: 20Gbps 이 상), mMTC 실현을 위한 손실 Coverage(예: 164dB 이하), 또는 URLLC 실현을 위한 U-plane latency(예: 다운링 크(DL) 및 업링크(UL) 각각 0.5ms 이하, 또는 라운드 트립 1ms 이하)를 지원할 수 있다. 안테나 모듈은 신호 또는 전력을 외부(예: 외부의 전자 장치)로 송신하거나 외부로부터 수신할 수 있다. 일실시예에 따르면, 안테나 모듈은 서브스트레이트(예: PCB) 위에 형성된 도전체 또는 도전성 패턴으로 이 루어진 방사체를 포함하는 안테나를 포함할 수 있다. 일실시예에 따르면, 안테나 모듈은 복수의 안테나들 (예: 어레이 안테나)을 포함할 수 있다. 이런 경우, 제 1 네트워크 또는 제 2 네트워크와 같은 통신 네트워크에서 사용되는 통신 방식에 적합한 적어도 하나의 안테나가, 예를 들면, 통신 모듈에 의하여 상기 복수의 안테나들로부터 선택될 수 있다. 신호 또는 전력은 상기 선택된 적어도 하나의 안테나를 통하여 통신 모 듈과 외부의 전자 장치 간에 송신되거나 수신될 수 있다. 어떤 실시예에 따르면, 방사체 이외에 다른 부품 (예: RFIC(radio frequency integrated circuit))이 추가로 안테나 모듈의 일부로 형성될 수 있다. 다양한 실시예에 따르면, 안테나 모듈은 mmWave 안테나 모듈을 형성할 수 있다. 일실시예에 따르면, mmWave 안테나 모듈은 인쇄 회로 기판, 상기 인쇄 회로 기판의 제 1 면(예: 아래 면)에 또는 그에 인접하여 배 치되고 지정된 고주파 대역(예: mmWave 대역)을 지원할 수 있는 RFIC, 및 상기 인쇄 회로 기판의 제 2 면(예: 윗 면 또는 측 면)에 또는 그에 인접하여 배치되고 상기 지정된 고주파 대역의 신호를 송신 또는 수신할 수 있 는 복수의 안테나들(예: 어레이 안테나)을 포함할 수 있다. 상기 구성요소들 중 적어도 일부는 주변 기기들간 통신 방식(예: 버스, GPIO(general purpose input and output), SPI(serial peripheral interface), 또는 MIPI(mobile industry processor interface))을 통해 서로연결되고 신호(예: 명령 또는 데이터)를 상호간에 교환할 수 있다. 일실시예에 따르면, 명령 또는 데이터는 제 2 네트워크에 연결된 서버를 통해서 전자 장치와 외부의 전자 장치간에 송신 또는 수신될 수 있다. 외부의 전자 장치(102, 또는 104) 각각은 전자 장치 와 동일한 또는 다른 종류의 장치일 수 있다. 일실시예에 따르면, 전자 장치에서 실행되는 동작들의 전부 또는 일부는 외부의 전자 장치들(102, 104, 또는 108) 중 하나 이상의 외부의 전자 장치들에서 실행될 수 있다. 예를 들면, 전자 장치가 어떤 기능이나 서비스를 자동으로, 또는 사용자 또는 다른 장치로부터의 요 청에 반응하여 수행해야 할 경우에, 전자 장치는 기능 또는 서비스를 자체적으로 실행시키는 대신에 또는 추가적으로, 하나 이상의 외부의 전자 장치들에게 그 기능 또는 그 서비스의 적어도 일부를 수행하라고 요청할 수 있다. 상기 요청을 수신한 하나 이상의 외부의 전자 장치들은 요청된 기능 또는 서비스의 적어도 일부, 또는 상기 요청과 관련된 추가 기능 또는 서비스를 실행하고, 그 실행의 결과를 전자 장치로 전달할 수 있다. 전자 장치는 상기 결과를, 그대로 또는 추가적으로 처리하여, 상기 요청에 대한 응답의 적어도 일부로서 제공할 수 있다. 이를 위하여, 예를 들면, 클라우드 컴퓨팅, 분산 컴퓨팅, 모바일 에지 컴퓨팅(MEC: mobile edge computing), 또는 클라이언트-서버 컴퓨팅 기술이 이용될 수 있다. 전자 장치는, 예를 들어, 분산 컴 퓨팅 또는 모바일 에지 컴퓨팅을 이용하여 초저지연 서비스를 제공할 수 있다. 다른 실시예에 있어서, 외부의 전자 장치는 IoT(internet of things) 기기를 포함할 수 있다. 서버는 기계 학습 및/또는 신경망을 이용한 지능형 서버일 수 있다. 일실시예에 따르면, 외부의 전자 장치 또는 서버는 제 2 네트워크 내에 포함될 수 있다. 전자 장치는 5G 통신 기술 및 IoT 관련 기술을 기반으로 지능형 서비스(예: 스마트 홈, 스마트 시티, 스마트 카, 또는 헬스 케어)에 적용될 수 있다. 본 문서에 개시된 다양한 실시예들에 따른 전자 장치는 다양한 형태의 장치가 될 수 있다. 전자 장치는, 예를 들면, 휴대용 통신 장치(예: 스마트폰), 컴퓨터 장치, 휴대용 멀티미디어 장치, 휴대용 의료 기기, 카메라, 웨 어러블 장치, 또는 가전 장치를 포함할 수 있다. 본 문서의 실시예에 따른 전자 장치는 전술한 기기들에 한정되 지 않는다. 본 문서의 다양한 실시예들 및 이에 사용된 용어들은 본 문서에 기재된 기술적 특징들을 특정한 실시예들로 한 정하려는 것이 아니며, 해당 실시예의 다양한 변경, 균등물, 또는 대체물을 포함하는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 또는 관련된 구성요소에 대해서는 유사한 참조 부호가 사용될 수 있다. 아이템 에 대응하는 명사의 단수 형은 관련된 문맥상 명백하게 다르게 지시하지 않는 한, 상기 아이템 한 개 또는 복수 개를 포함할 수 있다. 본 문서에서, \"A 또는 B\", \"A 및 B 중 적어도 하나\", \"A 또는 B 중 적어도 하나\", \"A, B 또는 C\", \"A, B 및 C 중 적어도 하나\", 및 \"A, B, 또는 C 중 적어도 하나\"와 같은 문구들 각각은 그 문구들 중 해당하는 문구에 함께 나열된 항목들 중 어느 하나, 또는 그들의 모든 가능한 조합을 포함할 수 있다. \"제 1\", \"제 2\", 또는 \"첫째\" 또는 \"둘째\"와 같은 용어들은 단순히 해당 구성요소를 다른 해당 구성요소와 구분하기 위 해 사용될 수 있으며, 해당 구성요소들을 다른 측면(예: 중요성 또는 순서)에서 한정하지 않는다. 어떤(예: 제 1) 구성요소가 다른(예: 제 2) 구성요소에, \"기능적으로\" 또는 \"통신적으로\"라는 용어와 함께 또는 이런 용어 없이, \"커플드\" 또는 \"커넥티드\"라고 언급된 경우, 그것은 상기 어떤 구성요소가 상기 다른 구성요소에 직접적 으로(예: 유선으로), 무선으로, 또는 제 3 구성요소를 통하여 연결될 수 있다는 것을 의미한다. 본 문서의 다양한 실시예들에서 사용된 용어 \"모듈\"은 하드웨어, 소프트웨어 또는 펌웨어로 구현된 유닛을 포함 할 수 있으며, 예를 들면, 로직, 논리 블록, 부품, 또는 회로와 같은 용어와 상호 호환적으로 사용될 수 있다. 모듈은, 일체로 구성된 부품 또는 하나 또는 그 이상의 기능을 수행하는, 상기 부품의 최소 단위 또는 그 일부 가 될 수 있다. 예를 들면, 일실시예에 따르면, 모듈은 ASIC(application-specific integrated circuit)의 형 태로 구현될 수 있다. 본 문서의 다양한 실시예들은 기기(machine)(예: 전자 장치) 의해 읽을 수 있는 저장 매체(storage medium)(예: 내장 메모리 또는 외장 메모리)에 저장된 하나 이상의 명령어들을 포함하는 소프트웨어 (예: 프로그램)로서 구현될 수 있다. 예를 들면, 기기(예: 전자 장치)의 프로세서(예: 프로세서 )는, 저장 매체로부터 저장된 하나 이상의 명령어들 중 적어도 하나의 명령을 호출하고, 그것을 실행할 수 있다. 이것은 기기가 상기 호출된 적어도 하나의 명령어에 따라 적어도 하나의 기능을 수행하도록 운영되는 것 을 가능하게 한다. 상기 하나 이상의 명령어들은 컴파일러에 의해 생성된 코드 또는 인터프리터에 의해 실행될 수 있는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장 매체는, 비일시적(non-transitory) 저장 매체의 형 태로 제공될 수 있다. 여기서, '비일시적'은 저장 매체가 실재(tangible)하는 장치이고, 신호(signal)(예: 전자 기파)를 포함하지 않는다는 것을 의미할 뿐이며, 이 용어는 데이터가 저장 매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구분하지 않는다. 일실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory(CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 또는 두 개의 사용자 장치들(예: 스마트 폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨 터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같 은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 다양한 실시예들에 따르면, 상기 기술한 구성요소들의 각각의 구성요소(예: 모듈 또는 프로그램)는 단수 또는 복수의 개체를 포함할 수 있으며, 복수의 개체 중 일부는 다른 구성요소에 분리 배치될 수도 있다. 다양한 실시 예들에 따르면, 전술한 해당 구성요소들 중 하나 이상의 구성요소들 또는 동작들이 생략되거나, 또는 하나 이상 의 다른 구성요소들 또는 동작들이 추가될 수 있다. 대체적으로 또는 추가적으로, 복수의 구성요소들(예: 모듈 또는 프로그램)은 하나의 구성요소로 통합될 수 있다. 이런 경우, 통합된 구성요소는 상기 복수의 구성요소들 각각의 구성요소의 하나 이상의 기능들을 상기 통합 이전에 상기 복수의 구성요소들 중 해당 구성요소에 의해 수행되는 것과 동일 또는 유사하게 수행할 수 있다. 다양한 실시예들에 따르면, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작들은 순차적으로, 병렬적으로, 반복적으로, 또는 휴리스틱하게 실행되거나, 상기 동 작들 중 하나 이상이 다른 순서로 실행되거나, 생략되거나, 또는 하나 이상의 다른 동작들이 추가될 수 있다. 도 2는 뉴럴 네트워크(NN, Neural network) 구조의 일 예에 관한 도면이다. 도 2와 관련한 설명에서, 모델 또는 레이어의 동작은 중앙처리 장치(CPU) 및/또는 뉴럴 프로세싱 유닛(NPU)의 동작으로 이해될 수 있다. 예시적으로, 제1 레이어(L1)는 컨볼루션 레이어이고, 제2 레이어(L2)는 샘플링 레이어일 수 있다. 인공 신경망 모델은 활성(activation) 레이어를 더 포함할 수 있으며, 다른 종류의 연산을 수행하는 레이어를 더 포함할 수 있다. 복수의 레이어들 각각은 입력되는 이미지 데이터 또는 이전 레이어에서 생성된 피처맵(feature map)을 입력 피 처맵으로서 수신하고, 입력 피처맵을 연산하여 출력 피처맵을 생성할 수 있다. 이때, 피처맵은 입력 데이터의 다양한 특징이 표현된 데이터를 의미할 수 있다. 피처맵들(FM1, FM2, FM3)은 예컨대 2차원 매트릭스 또는 3차원 매트릭스 형태를 가질 수 있다. 피처맵들(FM1 내지 FM3)은 너비(W)(또는 칼럼이라고 함), 높이(H)(또는 로우라 고 함) 및 깊이(D)를 가지며, 이는 좌표상의 x축, y축 및 z축에 각각 대응할 수 있다. 이때, 깊이(D)는 채널 수 로 지칭될 수 있다. 제1 레이어(L1)는 제1 피처맵(FM1)을 웨이트 맵(WM)과 컨볼루션하여 제2 피처맵(FM2)을 생성할 수 있다. 웨이트 맵(WM)은 제1 피처맵(FM1)을 필터링할 수 있으며, 필터 또는 커널로 지칭될 수 있다. 예컨대, 웨이트 맵(WM)의 깊이, 즉 채널 개수는 제1 피처맵(FM1)의 깊이, 예를 들어, 채널 개수와 동일하며, 웨이트 맵(WM)과 제1 피처맵 (FM1)의 동일한 채널끼리 컨볼루션될 수 있다. 웨이트 맵(WM)은 제1 피처맵(FM1)을 슬라이딩 윈도우로하여 횡단 하는 방식으로 시프트될 수 있다. 시프트되는 양은 \"스트라이드(stride) 길이\" 또는 \"스트라이드\"로 지칭될 수 있다. 각 시프트동안, 웨이트 맵(WM)에 포함되는 웨이트들 각각이 제1 피처맵(FM1)과 중첩된 영역에서의 모든 피처값과 곱해지고 더해질 수 있다. 제1 피처맵(FM1)과 웨이트 맵(WM)이 컨볼루션 됨에 따라, 제2 피처맵(FM2) 의 하나의 채널이 생성될 수 있다. 도 2a에는 하나의 웨이트 맵(WM)이 표시되었으나, 실질적으로는 복수개의 웨 이트 맵이 제1 피처맵(FM1)과 컨볼루션 되어, 제2 피처맵(FM2)의 복수개의 채널이 생성될 수 있다. 다시 말해, 제2 피처맵(FM2)의 채널의 수는 웨이트 맵의 개수에 대응할 수 있다. 제2 레이어(L2)는 제2 피처맵(FM2)의 공간적 크기(spatial size)를 변경함으로써, 제3 피처맵(FM3)을 생성할 수 있다. 일 예로, 제2 레이어(L2)는 샘플링 레이어일 수 있다. 제2 레이어(L2)는 업-샘플링 또는 다운-샘플링을 수행할 수 있으며, 제2 레이어(L2)는 제2 피처맵(FM2)에 포함된 데이터들 중 일부를 선별할 수 있다. 예컨대, 2 차원의 윈도우(WD)가 윈도우(WD)의 사이즈(예컨대, 4 * 4 행렬) 단위로 제2 피처맵(FM2) 상에서 쉬프트되고, 윈 도우(WD)와 중첩되는 영역에서 특정 위치(예컨대, 1행 1열)의 값을 선택할 수 있다. 제2 레이어(L2)는 선택된 데이터를 제3 피처맵(FM3)의 데이터로서 출력할 수 있다. 다른 예로, 제2 레이어(L2)는 풀링 레이어일 수 있다. 이 경우, 제2 레이어(L2)는 제2 피처맵(FM2)에서 윈도우(WD)와 중첩되는 영역의 피처 값들의 최대값(또는 피처값들의 평균값)이 선택될 수 있다. 제2 레이어(L2)는 선택된 데이터를 제3 피처맵(FM3)의 데이터로서 출력할 수 있다. 이에 따라, 제2 피처맵(FM2)으로부터 공간적 사이즈가 변경된 제3 피처맵(FM3)이 생성될 수 있다. 제3 피처맵 (FM3)의 채널과 제2 피처맵(FM2)의 채널 개수는 동일할 수 있다. 한편, 본 개시의 예시적인 실시예에 따르면, 풀링 레이어보다 샘플링 레이어의 연산 속도가 빠를 수 있고, 샘플링 레이어는 출력 이미지의 퀄리티(예컨대, PSNR(Peak Signal to Noise Ratio) 측면에서)를 개선할 수 있다. 예컨대, 풀링 레이어에 의한 연산은, 최대 값 또는 평균 값을 산출하여야 하므로 샘플링 레이어에 의한 연산보다 연산 시간이 더 클 수 있다. 실시예에 따라, 제2 레이어(L2)는 샘플링 레이어 또는 풀링 레이어에 한정되지 않을 수 있다. 예를 들어, 제2 레이어(L2)는 제1 레이어(L1)와 유사한 컨볼루션 레이어가 될 수 있다. 제2 레이어(L2)는 제2 피처맵(FM2)을 웨 이트 맵과 컨볼루션하여 제3 피처맵(FM3)을 생성할 수 있다. 이 경우, 제2 레이어(L2)에서 컨볼루션 연산을 수 행한 웨이트 맵은 제1 레이어(L1)에서 컨볼루션 연산을 수행한 웨이트 맵(WM)과 다를 수 있다. 제1 레이어(L1) 및 제2 레이어(L2)를 포함한 복수의 레이어들을 거쳐 제N 레이어에서 제N 피처맵을 생성할 수 있다. 제N 피처맵은 출력 데이터가 출력되는 인공 신경망 모델의 백 엔드(back end)에 위치한 복원 레이어 (reconstruction layer)에 입력될 수 있다. 복원 레이어는 제N 피처맵을 기반으로 출력 이미지를 생성할 수 있 다. 또한, 복원 레이어는 제N 피처맵 뿐만 아니라, 제1 피처맵(FM1) 및 제2 피처맵(FM2) 등 복수의 피처맵들을 수신하고, 복수의 피처맵들에 기초하여 출력 이미지를 생성할 수 있다. 예컨대, 복원 레이어는, 컨볼루션 레이어 또는 디-컨볼루션 레이어일 수 있다. 실시예에 따라서는, 피처맵으로 부터 이미지를 복원할 수 있는 다른 종류의 레이어로 구현될 수도 있다. 도 2b는, 일 실시예에 따른 전자 장치의 블록도이다. 도 2b를 참조하면, 전자 장치(예 : 도 1의 전자 장치)는 프로세서(예: 도 1의 프로세서), 메모리(예: 도 1의 메모리)를 포함할 수 있다. 도 2에 포함된 구성 요소는 전자 장치에 포함된 구성들의 일부에 대한 것이며 전자 장치는 이 밖에도 도 1에 도시된 것과 같이 다양한 구성요소를 포함할 수 있다. 메모리는 인공지능 모델과 관련된 데이터를 일시적으로 또는 비일시적으로 저장할 수 있다. 예를 들어, 메 모리는 인공지능 모델의 레이어, 가중치, 오퍼레이션과 같은 인공지능 모델과 관련된 데이터를 저장하고, 인공지능 모델의 출력 값에 기반하여 인공지능 모델과 관련된 데이터를 업데이트할 수 있다. 예를 들어, 메모리 는 인공지능 모델의 출력 값을 저장할 수 있다. 일 실시예에 따른 인공지능 모델은 지정된 언어로 작성되어, 복수의 레이어(layer) 및/또는 오퍼레이션 (operation)을 포함하는 인공 신경망 모델(neural network model)일 수 있다. 일 실시예에 따른 인공지능 모델 은 CNN(convolution neural network), R-CNN(region with convolution neural network), RPN(region proposal network), RNN(recurrent neural network), S-DNN(stacking-based deep neural network), S-SDNN(state-space dynamic neural network), Deconvolution Network, DBN(deep belief network), RBM(restricted boltzman machine), Fully Convolutional Network, LSTM(long short-term memory) Network, Classification Network와 같은다양한 종류의 네트워크 중 적어도 하나일 수 있다. 일 실시예에 따른 인공지능 모델은 지정된 데이터에 대 하여 학습될 수 있고, 입력 데이터를 획득하고, 입력 데이터를 기반으로 연산을 수행하여 출력 데이터를 생성할 수 있다. 일 실시예에 따른 인공지능 모델은 입력 계층(input layer), 은닉 계층(hidden layer) 및 출력 계층(output layer)을 포함할 수 있다. 입력 계층(input layer)은 인공지능 모델에 입력되는 입력 값과 관련될 숭 ㅣㅆ다. 은닉 계층(hidden layer)에서는 입력 값에 대하여 MAC 연산(multiply-accumulate)과 활성화 연산을 수행하여 피쳐 맵을 출력할 수 있다. 출력 계층(output layer)은, 은닉 계층에서 수행한 연산의 결과 값과 관련될 수 있 다. 일 실시예에 따르면, 인공지능 모델은 메모리에 저장되어있을 수 있다. 일 실시예에 따르면, 인공지능 모 델에 기초한 연산은 프로세서(예: 중앙 처리 장치(CPU, central processing unit) 및/또는 뉴럴 프로세싱 유닛(NPU, neural processing unit))에서 수행될 수 있다.프로세서는 메모리에 저장된 인공지능 모델에 기초하여 연산을 수행할 수 있다. 일 실시예에 따른 프로세서는 적어도 하나의 프로세서를포함할 수 있다. 예를 들어, 프로세서은 중앙 처리 장치(CPU, Central Processing unit)(예: 도 1의 메인 프로세서) 및/또는 뉴럴 프로세싱 유닛(NPU, neural processing unit)(예: 도 1의 보조 프로세서)을 포함할 수 있다. 일 실시예에 따른 프로세서는 컴파일러를 포함할 수 있다. 일 실시예에 따른 컴파일러는 특정 언어로 작성된 소스 코드를 타깃 프로그램 및/또는 타깃 하드웨어에서 처리할 수 있는 목적 코드로 컴파일(compile)할 수 있다. 예를 들어, 컴파일러는 메모리에서 인공지 능 모델을 불러오고(또는, 로드(load)하고), 인공지능 모델을 프로세서에서 사용가능한 바이너리로 컴파일 할 수 있다. 일 실시예에 따른 프로세서는 컴파일한 인공지능 모델에 포함된 활성화 함수를 확인할 수 있다. 예를 들어, 프로세서는 컴파일한 인공지능 모델의 활성화 함수에 제 1 유형 함수를 포함함에 대응하여, 제 1 기능 및/또는 제 2 기능을 활성화할 수 있다. 일 실시예에 따르면, 제 1 유형 함수는 렐루(ReLU, Rectified Linear Unit) 함수일 수 있다. 렐루 함수는 입력 값이 음수일 경우 0, 양수일 경우 입력 값을 그대로 출력하는 함수이다. 일 실시예에 따른 프로세서는 은닉 계층(hidden layer)에서 입력 값에 대하여 MAC 연산(multiply- accumulate)과 활성화 연산을 수행하여 피쳐 맵을 출력할 수 있다. MAC 연산은 입력 값과 대응되는 가중치를 각각 곱하고, 곱한 값들을 합하는 연산일 수 있다. 활성화 연산은 MAC 연산의 결과를 활성화 함수에 입력하여 결과 값을 출력하는 연산일 수 있다. 일 실시예에 따른 프로세서는 은닉 계층의 입력에 포함된 지정된 값에 대하여 연산을 생략할 수 있다. 예를 들어, 제 1 기능은 제로 스키핑(zero-skipping) 기능일 수 있다. 제로 스키핑 기능은, 입력 값(예 : 피쳐 맵, 이전 은닉 계층의 결과 값)에 포함된 0에 대하여, MAC 연산을 생략하는 기능일 수 있다. 예를 들어, 제 2 기능은 피쳐 맵 압축 기능일 수 있다. 피쳐 맵 압축 기능은, 프로세서가 은닉 계층의 출력 값 (예 : 피쳐 맵)에 대하여 0이 아닌 값을 추출하여 피쳐 맵을 압축하는 기능일 수 있다. 일 실시예에 따른 인공지능 모델(미도시)은 입력 계층(input layer), 은닉 계층(hidden layer) 및 출력 계층 (output layer)을 포함할 수 있다. 입력 계층(input layer)은 인공지능 모델에 입력되는 입력 값과 관련된 계층이다. 은닉 계층(hidden layer)에서는 입력 값에 대하여 MAC 연산(multiply-accumulate)과 활성화 연산을 수행하여 피쳐 맵을 출력할 수 있다. MAC 연산은 입력 값과 대응되는 가중치를 각각 곱하고, 곱한 값들을 합하는 연산일 수 있다. 활성화 연산은 MAC 연산의 결과를 활성화 함수에 입력하여 결과 값을 출력하는 연산일 수 있다. 활성화 함수는, 다양한 유형일 수 있다. 예를 들어, 활성화 함수는, 시그모이드 함수, 탄젠트 함수, 렐루 함수, 리키 렐루 함수, 맥스아웃 함수 및/또는 엘루 함수을 포함할 수 있으나 그 종류에 제한이 없다. 은닉 계층은 적어도 하나의 계층(layer)로 구성될 수 있다. 예를 들어, 은닉 계층이 제 1 은닉 계층 및 제 2 은 닉 계층으로 구성된 경우, 제 1 은닉 계층은 입력 계의 입력 값에 기반하여 MAC 연산 및 활성화 연산을 수행하 여 피쳐 맵을 출력하고, 제 1 은닉 계층에서의 결과 값인 피쳐 맵이 제 2 은닉 계층에서의 입력 값이 될 수 있 다. 제 2 은닉 계층은 제 1 은닉 계층의 결과 값인 피쳐 맵에 기반하여 MAC 연산 및 활성화 연산을 수행할 수 있다. 출력 계층(output layer)은, 은닉 계층에서 수행한 연산의 결과 값과 관련된 계층일 수 있다. 도 2b와 관련한 설명에서, 프로세서에 포함된 것으로 도시된 부분들 적어도 일부가 프로세서에서 실 행되는 SW모듈일 수 있고, 이 경우, 해당 부분들이 수행하는 동작은 프로세서의 동작으로 이해될 수 있다. 도 3a 및 도 3b는 비교 실시예에 따른 인공지능 학습모델에서 각 노드(node)의 실행 단위(partition)를 통해 순 차적으로 추론(inference)하는 과정을 도시한 것이다. 그림 310 에 도시된 것처럼 인공지능 학습모델(AI model)은 xpu(예: CPU,GPU)의 경우 각 노드(node)를 실행 단 위로 하여 순차적으로 하나의 연산을 수행할 수 있다. 예를 들어 그림 310에서는 제 1 xpu(310_1)에서 연산을 수행 후 결과를 제 2 xpu(310_2)로 보낼 수 있다. 제 2 xpu(310_2)는 제 1 xpu(310_1)의 연산 결과를 수신하고 이를 기반으로 연산을 수행 후 결과를 제 3 xpu(310_3)로 보낼 수 있다. 그림 310에서는 이러한 과정을 제 12 xpu(310_12)까지 반복할 수 있으며, 제 12 xpu(310_12)는 최종 연산 결과를 출력시킬 수 있다. xpu의 수는 일 예시일 뿐 이것으로 한정되는 것은 아니다. 그림 320에 도시된 것처럼 인공지능 학습모델(AI model)은 internal path(compile or interpreting)을 수행하 는 모든 장치(Xpu)(예: GPU,NPU)에서 실행되는 복수의 연산(operation)들을 동일한 그룹으로 묶을 수 있다. internal path를 갖는 xpu는 외부의 데이터를 내부 컴파일을 이용하여 해당 xpu 내 core에서 실행가능한 이미지 로 변경할 수 있다. 인공지능 학습모델(AI model)은 동일한 그룹으로 묶인 복수의 연산들을 한 번에 처리할 수 있다. 예를 들어, 그림 320에서 제 1 npu(320_1)는 4개의 연산을 하나의 묶음으로 실행할 수 있다. 이후 제 1 npu(320_1)는 연산 결과를 제 2 cpu(320_2)로 전송할 수 있다. 제 2 cpu(320_2)는 연산 후 결과를 제 3 gpu(320_3)으로 전송할 수 있다. 제 3 gpu(320_3)는 연산 후 결과를 제 4 npu(320_4)로 전송할 수 있다. 제 4 npu(320_4)는 2개의 연산들을 하나의 묶음으로 실행할 수 있다. 인공지능 학습모델(AI model)은 이러한 방식으 로 그림 310보다 상대적으로 빠르게 최종 연산 결과를 도출할 수 있다. 실행하는 연산의 수나 xpu의 수는 이것으로 제한되는 것은 아니며 설정에 따라 달라질 수 있다. 도 3b의 그림 330에 도시된 것처럼 실행해야 하는 노드의 수는 설정에 따라 달라질 수 있다. 그림 330은 노드의 수가 100,000개인 경우를 가정한 것이다. 이 경우 연산을 처리하는 하드웨어의 성능(예:메모리의 사이즈, 동작 주파수의 속도)에 한계가 있어서 연산 처리 속도가 느려지거나 연산의 실행이 불가능할 수 있다. 하드웨어의 성 능으로 인한 문제는 데이터가 늘어나고 복잡해질수록 연산의 수가 늘어나고 노드의 수가 늘어나기 때문에 더 심 각해질 수 있다. 이하에서는 이러한 하드웨어의 성능으로 인한 문제를 극복하기 위해 노드 사이를 적절하게 파 티셔닝(partitioning)하는 전자 장치의 동작 방법에 대해 설명될 것이다. 도 4는 연산을 위해 노드들 사이에 파티션(partition)을 자동으로 생성하는 전자 장치의 계층(layer)을 도시한 것이다. 도 4에 도시된 것처럼 프로세서(예: 도 2의 프로세서)는 어플리케이션의 백엔드(backend) 상에 sub- graph validation logic을 더 포함할 수 있다. 프로세서는 sub-graph validation logic을 이용 하여 연산의 실행 과정에서 발생하는 문제가 해결될 수 있는지 사전에 확인할 수 있다. 즉, 프로세서는 sub-graph validation logic을 이용하여 sub-graph를 생성한 상황을 가정하여 하드웨어의 제약 사항이 해 결되고, 연산이 정상적으로 수행될 수 있는지 사전에 확인할 수 있다. sub-graph는 구체적으로 노드(node)들을 그룹별로 묶어서 파티션(partition)으로 구분하는 과정을 의미할 수 있다. 파티션(partition)은 특정 Xpu(예: cpu, gpu, npu)에서 실행되는 노드들의 묶음을 의미할 수 있다. 예를 들어, 프로세서는 1번 노드부터 999번 노드까지 하나의 파티션으로 묶음을 생성할 수 있다. 프로세서는 여 기에 하나의 노드(예: 1000번 노드)를 추가했을 때 인공지능 학습모델의 추론(inference)이 잘 실행될 수 있는 지 결정할 수 있다. 메모리 사이즈의 한계가 있기 때문에 한 번에 실행할 수 있는 노드의 수는 제한될 수 있다. 프로세서는 이러한 메모리 사이즈의 한계를 고려하여 파티션을 이용해 노드들을 적정한 수로 분리할 수 있다. 여기서 노드의 적정한 수는 인공지능 학습모델의 연산을 수행하는데 에러 없이 작동하는 노드 의 수를 의미할 수 있다. 프로세서는 노드들을 하나씩 추가하면서 인공지능 학습모델의 연산을 수행할 때 에러 없이 작동하는지 확인할 수 있다. 여기서 확인하는 동작은 인공지능 학습모델의 연산을 수행할 때 에러 없 이 작동하는지 검증하는 동작을 의미할 수 있다. 또는 확인하는 동작은 프로세서의 연산에 근거한 예측 동 작을 의미할 수 있다. 프로세서는 연산이 실행될 수 있는지 노드(node) 단위로 미리 하나씩 시뮬레이션을 실행하여 연산이 실행되지 못하는 노드(node) 발생 시 해당 노드를 파티션(partition)의 기준으로 결정할 수 있 다. 프로세서는 연산이 실행될 수 있는 노드까지 파티션(partition)으로 묶어서 하드웨어의 제약에도 불구 하고 연산을 문제 없이 실행시킬 수 있다. 이는 도 5a 내지 7을 통해 상세히 설명될 것이다. 프로세서는 현재 사용되고 있는 xpu에서 제약 사항을 확인할 수 있다. 제약 사항은, 예를 들어, 노드 (node)의 weight 정보, quantization/dequantization weight 정보 또는 parameter information 중 적어도 어느하나를 포함할 수 있다. 노드(node)의 weight 정보는 예를들어 Conv2d와 같은 operation 수행을 위한 parameter 정보로 input data와의 연산에 반드시 필요한 정보를 의미할 수 있다. 노드(node)의 quantization/dequantization weight 정보는 예를 들어, FP32<->INT32 또는 FP32<->INT8 과 같이 operation의 inference data type을 변환하기 위한 정보를 의미 할 수 있다. 노드(node)의 parameter information은 예를 들어 operation shape 정보, operation수행을 위한 axis, padding option, bias 및 threshold 를 의미할 수 있다. 이외에 메모리 재활용을 위한 memory handle와 같은 정보를 더 포함할 수 있다. internal path 를 갖는 NPU 또는 GPU의 구동 방식은 CPU와 동일하게 task 방식일 수 있다. task 방식은 context 기반으로 동작될 수 있다. 인공지능 학습모델 상에서 partition으로 나눠진 task 상에서 이용 가능한 자원은 현재 이용중인 NPU 또는 GPU가 갖는 메모리 자원과 동일할 수 있다. 이러한 이유로 메모리 자원을 고려 하여 sub-graph를 생성하면 메모리 사이즈의 한계를 극복하고 인공지능 학습모델을 문제 없이 작동시킬 수 있다. 도 5a 내지 도 6b에서는 인공지능 학습모델을 여러 개의 파티셔닝(partitioning)을 통해 메모리 사이즈의 한계를 극복하는 과정에 대해 설명될 것이다. 도 5a는 연산을 위해 노드들 사이에 파티션(partition)을 자동으로 생성하는 제 1 실시예를 도시한 것이다. 도 5b는 제 1 실시예에 대한 과정을 순서도로 나타낸 것이다. 도 5a에서 프로세서(예: 도 2의 프로세서)는 100,000개의 노드를 갖는 학습 모델을 가동시킬 수 있다. 노드의 수는 일 예시일 뿐 설정에 따라 달라질 수 있다. 도 5a에서는 internal path(compile or interpreting)을 수행하는 모든 장치(Xpu)에서 실행되는 상황을 가정한 것이다. 프로세서는 100,000개의 노드에 대해 sub-partition(502, 504)을 이용하여 분리할 수 있다. 프로세서는 하드웨어(예: 메모리 )의 제약과 상관없이 partition(502, 504)을 이용하여 학습모델을 작동시킬 수 있다. 프로세서(22 0)는 partition(502, 504)을 형성하기 위해 internal path(compile or interpreting)을 수행하는 모든 장치 (Xpu)를 이용할 수 있다. 프로세서는 NPU 또는 GPU를 이용하여 partition(502, 504)이 형성되 면 학습모델이 에러(error)없이 잘 동작할 것인지 시뮬레이션을 진행하고, 시뮬레이션 결과에 기반하여 partition(502, 504)을 형성할 수 있다. 에러(error)는 예를 들어, 메모리 사이즈의 제한으로 인해 단일한 파티 션(partition) 형태로 구성된 그래프의 실행이 불가능한 상황을 의미할 수 있다. 단일한 파티션(partition) 형 태로 구성된 그래프는 메모리 사이즈보다 상대적으로 큰 메모리를 요구할 수 있다. 이 경우 프로세서는 정 상적으로 컴파일을 수행하기 어려울 수 있다. 이러한 에러를 방지하기 위해 프로세서는 적어도 하나 이상 의 파티션(partition)을 이용하여 복수의 노드들을 분할하고, 메모리 사이즈의 제약을 극복하고 정상적으로 컴 파일을 수행할 수 있다. 본 문서에 따른 전자 장치(예: 도 2b의 전자 장치)는 sub-partiton이 형성되기 전에 sub-partiton이 형성 된 상황을 가정하여 시뮬레이션을 진행할 수 있다. 전자 장치는 시뮬레이션 결과에 기반하여 sub-partiton 이 생성되는 위치 및 하나의 파티션(partition) 상에 포함되는 노드의 수를 다르게 결정할 수 있다. 전자 장치 는 적어도 하나의 파티션(partition)을 생성하기 전 미리 시뮬레이션을 실행하여 적어도 하나의 파티션 (partition)이 생성된 후에는 현재 하드웨어에서 학습모델이 에러(error)없이 잘 동작할 것인지 확인할 필 요가 없을 수 있다. 도 5a에서는 노드가 100,000개, sub-partiton이 12개인 상황을 가정하여 설명했지만 이는 일 예시일 뿐 설정에 따라 달라질 수 있다. 오픈 소스(open source)기반의 머신 러닝 프레임워크(framework)는 특정 xpu에서 연산이 수행 가능한지 또는 불 가능한지 여부를 판단할 수 있다. 즉, 머신 러닝 프레임워크(framework)는 수행 가능 및 수행 불가능의 2가지 상황으로만 노드들을 제어할 수 있다. 예를 들어, 머신 러닝 프레임워크(framework)는 1번째 노드 부터 1000번 째 노드까지는 연산의 수행이 가능하고, 1001번째 노드가 포함되면 연산의 수행이 불가능하다는 결과를 수신할 수 있다. 이 경우 머신 러닝 프레임워크(framework)는 1번째 노드 부터 1000번째 노드까지 연속적으로 배치하고 1개의 파티션을 이용하여 분리할 수 있으며, 1001번째 노드는 1000번째 노드로부터 분리하여 배치할 수 있다. 머신 러닝 프레임워크(framework)는 1001번째 노드를 기준으로 연속적으로 몇 개의 노드들을 더 묶어서 연산의 수행이 가능한지 결정할 수 있다. 예를 들어, 머신 러닝 프레임워크(framework)는 1001번째 노드를 기준으로 4030번째 노드까지 연산의 수행이 가능하고 4031번째 노드가 포함되면 연산의 수행이 불가능하다는 결과를 수신 할 수 있다. 이 경우 머신 러닝 프레임워크(framework)는 1001번째 노드 부터 4030번째 노드까지 연속적으로 배 치하고 1개의 파티션을 이용하여 분리할 수 있으며, 4031번째 노드는 4030번째 노드로부터 분리하여 배치할 수있다. 머신 러닝 프레임워크(framework)는 수정 없이 자체적으로 파티셔닝을 수행할 수도 있다. 이 경우 머신 러닝 프 레임워크(framework)는 수행 가능 및 수행 불가능의 2가지 상황으로만 노드들을 제어할 수 있기 때문에 도 5a처 럼 연속되어 파티션이 형성되는 것이 아니라 도 6a처럼 특정 노드들이 분리될 수 있다. 이는 도 6a에서 설명될 것이다. 도 5b 및 도 5c는 도 5a의 제 1 실시예에 대한 과정을 순서도로 나타낸 것이다. 도 5b 및 도 5c를 통하여 설명되는 동작들은 컴퓨터 기록 매체 또는 메모리(예: 도 1의 메모리)에 저장될 수 있는 인스트럭션들 을 기반으로 구현될 수 있다. 도시된 방법은 앞서 도 1 내지 도 4를 통해 설명한 전자 장 치(예: 도 1의 전자 장치)에 의해 실행될 수 있으며, 앞서 설명한 바 있는 기술적 특징은 이하에서 생략하 기로 한다. 도 5b의 각 동작의 순서가 변경될 수 있으며, 일부 동작이 생략될 수도 있고, 일부 동작들이 동시에 수행될 수도 있다. 동작 512에서, 프로세서는 인공지능 학습모델 상에서 프레임워크(framework) 런타임(runtime) 엔진 (engine)을 실행시킬 수 있다. 동작 514에서, 프로세서는 인공지능 학습모델을 실행하기 위한 하드웨어의 정보(예: 메모리 사이즈, colck 수)를 획득할 수 있다. 프로세서는 획득한 하드웨의의 정보를 기반으로 프레임워크(framework) 상에서 학 습 실행 시 파티셔닝(partitioning)을 할 수 있다. 학습은 그래프(graph)를 생성하여 진행될 수 있으며, 하드웨 어의 제약으로 모든 노드가 포함된 그래프(graph) 형태로 실행이 어려울 수 있다. 이러한 문제를 해결하기 위해 파티셔닝(partitioning)이 필요할 수 있다. 동작 520에서, 프로세서는 타깃 프로세서(Xpu(예: CPU,GPU,NPU))에서 실행 함수(operation)를 지원하는지 확인할 수 있다. 각각의 Xpu(CPU,GPU,NPU)는 지원하는 실행 함수(operation)가 다를 수 있다. 예를 들어, CPU 는 GPU 또는 NPU보다 상대적으로 많은 실행 함수(operation)를 지원할 수 있다. 이 경우 프로세서는 GPU 또는 NPU가 아닌 CPU를 이용하여 실행 함수(operation)를 실행할 수 있다. 또는 프로세서는 프레임워크에 서 실행 함수(operation)에 대한 정보를 획득할 수도 있다. 동작 520에서 타깃 프로세서(예: CPU, GPU, NPU)) 상에서 실행 함수(operation)를 지원함에 기반하여 동작 522 에서 프로세서는 첫 번째 노드가 에러없이 실행 가능한지 결정할 수 있다. 동작 522에서 프로세서는 첫 번째 노드가 에러없이 실행 불가능함에 기반하여 동작을 종료할 수 있다. 동작 5220에서, 프로세서는 첫 번째 노드가 에러없이 실행 가능함에 기반하여 동작 525에서, 새로운 노드(예: N 번째 노드)를 추가할 수 있 다. 이후 동작 530에서, 프로세서는 현재 노드(예: N번째 노드) 까지 에러(error) 없이 실행이 가능한지 확인 할 수 있다. 프로세서는 타깃 프로세서를 이용하여 현재 노드(N번째 노드) 까지 에러(error) 없이 실행이 가능한지 시뮬레이션 할 수 있다. 동작 530에서, 프로세서는 현재 노드(N번째 노드) 까지 에러(error) 없이 실행이 가능함에 기반하여 동작 540에서 노드의 수를 1개 더 증가시켜 시뮬레이션을 반복할 수 있다. 동작 530에서, 프로세서는 현재 노드(N번째 노드) 까지 에러(error) 없이 실행이 불가능함에 기반하여 현 재 노드의 이전 노드(N-1번째 노드)까지 묶어서 파티션(partition)을 생성할 수 있다. 동작 540에서, 프로세서는 현재 노드(N+1번째 노드) 까지 에러(error) 없이 실행이 불가능함에 기반하여 현재 노드의 이전 노드(N번째 노드)까지 묶어서 파티션(partition)을 생성할 수 있다. 동작 550에서, 프로세서는 마지막 노드까지 동작 530 및 동작 540을 반복하여 파티션(partition) 생성을 완료할 수 있다. 앞서 언급했던 것처럼 전자 장치는 적어도 하나의 파티션(partition)을 생성하기 전 미리 시뮬레이션을 실행하여 적어도 하나의 파티션(partition)이 생성된 후에는 현재 하드웨어에서 학습모델이 에러(error)없이 잘 동작할 것인지 확인할 필요가 없을 수 있다. 도 6a는 연산을 위해 노드들 사이에 파티션(partition)을 자동으로 생성하는 제 2 실시예를 도시한 것이다. 도 6b는 제 2 실시예에 대한 과정을 순서도로 나타낸 것이다. 도 6a에서 프로세서(예: 도 2의 프로세서)는 100,000개의 노드를 갖는 학습 모델을 가동시킬 수 있다. 노드의 수는 일 예시일 뿐 설정에 따라 달라질 수 있다. 도 6a에서는 internal path(compile or interpreting)을 수행하는 모든 장치(Xpu)에서 실행되는 상황을 가정한 것이다. 또한, 프로세서는 타깃 프 로세서로 CPU를 이용하여 internal path(compile or interpreting)을 수행하는 모든 장치(Xpu) 상에서 실행되 지 않는 노드를 실행시킬 수도 있다. 프로세서는 100,000개의 노드에 대해 sub-partition 및 fallback sub-partition을 이용하여 분리할 수 있다. 프로세서는 하드웨어(예: 메모리)의 제약 과 상관없이 partition(602, 604)을 이용하여 학습모델을 작동시킬 수 있다. 프로세서는 sub- partition을 형성하기 위해 internal path(compile or interpreting)을 수행하는 모든 장치(Xpu)(예: internal path가 포함된 NPU 또는 GPU)를 이용할 수 있다. 프로세서는 fallback sub-partition을 형 성하기 위해 CPU나 다른 NPU 또는 GPU를 이용할 수 있다. 프로세서는 NPU 또는 GPU를 이용하여 sub- partition이 형성되면 학습모델이 에러(error)없이 잘 동작할 것인지 시뮬레이션을 진행하고, 시뮬레 이션 결과에 기반하여 partition을 형성할 수 있다. sub-partition은 internal path를 갖는 npu 또 는 gpu에서 실행될 수 있는 노드들을 포함할 수 있다. fallback sub-partition는 internal path를 갖는 npu 또는 gpu에서 실행되지 않는 1개의 노드를 포함할 수 있다. sub-partition은 internal path를 갖는 npu 또는 gpu에서 실행되고, fallback sub-partition은 CPU나 앞선 sub-partition과는 다른 npu 또 는 gpu에서 실행되는 차이가 있다. fallback sub-partition에 포함된 1개의 노드는 internal path를 갖는 npu 또는 gpu가 실행 함수(operation)을 지원하지 않을 때 fallback sub-partition에 의해서 분리될 수 있다. fallback sub-partition에 포함된 1개의 노드는 internal path(compile or interpreting)을 수행하 는 모든 장치(Xpu)(예: internal path를 갖는 npu 또는 gpu)가 메모리 사이즈의 한계로 복수의 노드들을 실행할 수 없을 때 fallback sub-partition에 의해서 분리될 수 있다. 앞선 도 5a에서 설명된 것처럼 머신 러닝 프레임워크(framework)는 수정 없이 자체적으로 파티셔닝을 수행할 수 도 있다 이 경우 머신 러닝 프레임워크(framework)는 연산의 수행 가능 및 불가능의 2가지 상황으로만 노드들을 제어할 수 있다. 예를 들어, 머신 러닝 프레임워크(framework)는 1번째 노드 부터 999번째 노드까지는 연산의 수행이 가능하고, 1000번째 노드가 포함되면 연산의 수행이 불가능하다는 결과를 수신할 수 있다. 이 경우 프로 세서는 머신 러닝 프레임워크(framework)를 이용하여 1번째 노드 부터 999번째 노드까지 연속적으로 배치 하고 1개의 파티션을 이용하여 분리할 수 있으며, 1000번째 노드는 999번째 노드로부터 분리하여 배치할 수 있 다. 여기서 머신 러닝 프레임워크(framework)는 연산의 수행 가능 및 불가능의 2가지 상황으로만 노드들을 제어 할 수 있다. 그래서 프로세서는 머신 러닝 프레임워크(framework)를 이용하여 1번째 노드 부터 999번째 노 드까지 하나의 파티션으로 묶고, 1000번째 노드는 별도의 파티션으로 생성할 수 있다. 이후 1001 번째 노드에 대해서는 1번째 노드와 마찬가지로 노드들을 하나씩 추가하면서 연산의 수행이 가능한지 결정할 수 있다. 도 6b는 도 6a의 제 2 실시예에 대한 과정을 순서도로 나타낸 것이다. 도 6b를 통하여 설명되는 동작들은 컴퓨터 기록 매체 또는 메모리(예: 도 1의 메모리)에 저장될 수 있는 인스트럭션들 을 기반으로 구현될 수 있다. 도시된 방법은 앞서 도 1 내지 도 4를 통해 설명한 전자 장치(예: 도 1의 전자 장치)에 의해 실행될 수 있으며, 앞서 설명한 바 있는 기술적 특징은 이하에서 생략하기로 한 다. 도 6b의 각 동작의 순서가 변경될 수 있으며, 일부 동작이 생략될 수도 있고, 일부 동작들이 동시에 수행될 수도 있다. 동작 612에서, 프로세서는 인공지능 학습모델 상에서 프레임워크(framework) 런타임(runtime) 엔진 (engine)을 실행시킬 수 있다. 동작 620에서, 프로세서는 internal path(compile or interpreting)을 수행하는 모든 장치(Xpu)(예: internal path를 지원하는 GPU 또는 NPU)에서 실행함수(operation)를 지원하는지 결정할 수 있다. 프로세서 는 GPU 또는 NPU에서 실행함수(operation)를 지원하지 않음에 기반하여 다른 CPU를 이용하거나 다른 GPU 또는 NPU를 이용하여 실행함수(operation)를 작동시킬 수 있다. 프로세서는 동작 620에서 internal path를 지원하는 GPU 또는 NPU에서 실행함수(operation)를 지원하는 것을 확인함에 기반하여 동작 630을 실행할 수 있다. 동작 630에서, 프로세서는 internal path를 지원하는 GPU 또는 NPU에서 첫 번째 노드부터 현재 노드까지 에러 없이 실행할 수 있는지 확인할 수 있다. 프로세서(22 0)는 첫 번째 노드가 에러 없이 실행되는 것으로 확인되는 경우 동작 632에서 하나의 노드를 더 추가하여 확인 을 반복할 수 있다. 예를 들어, N-1 번째 노드까지 에러 없이 실행되다가 N번째 노드에서 에러가 발생하는 것으로 결정되는 경우 프로세서는 N번째 노드를 파티션(partition)으로 구분할 수 있다. 프로세서는 첫 번째 노드부터 N-1번째 노드까지 sub-partition으로 구분하고, N번째 노드는 fallback sub-partition으로 구분 할 수 있다. sub-partition으로 구분된 노드들은 internal path를 지원하는 GPU 또는 NPU에서 실행될 수 있다. fallback sub-partition으로 구분된 노드들은 CPU나 internal path를 지원하지 않는 다른 GPU 또는 NPU에서 실 행될 수 있다. 에러는 런타임(runtime) 에러를 의미하는 것으로 하드웨어(예: 메모리의 사이즈)의 제약으로 발 생할 수 있다. 하드웨어의 제약사항은 도 3a 및 도 3b에서 설명된 바 있다. 프로세서는 N번째 노드에서 에러가 발생하는 것으로 결정되는 경우 동작 634에서, N 번째 노드를 fallback sub-partition으로 구분할 수 있다. 이후 동작 636에서, 프로세서는 다음 N+1번째 노드에 대해 동작 630의 확인을 반복할 수 있다. 프로세서는 N+1 번째 노드, N+2 번째 노드에 대해 확인을 반복할 수 있으며, 마지 막 노드까지 확인된 경우 동작을 종료할 수 있다. 도 7은 일 실시예에 따른 전자 장치의 연산 수행 방법을 순서도로 나타낸 것이다. 도 7을 통하여 설명되는 동작들은 컴퓨터 기록 매체 또는 메모리(예: 도 1의 메모리)에 저장될 수 있는 인 스트럭션들 을 기반으로 구현될 수 있다. 도시된 방법은 앞서 도 1 내지 도 4를 통해 설명한 전자 장치(예: 도 1의 전자 장치)에 의해 실행될 수 있으며, 앞서 설명한 바 있는 기술적 특징은 이하에서 생략하기로 한다. 도 7의 각 동작의 순서가 변경될 수 있으며, 일부 동작이 생략될 수도 있고, 일부 동작들이 동시에 수행될 수도 있다. 동작 710에서, 프로세서(예: 도 2b의 프로세서)는 인공지능 모델을 로딩하여 프레임워크(framework)의 런 타임 엔진(runtime engine)을 실행시킬 수 있다. 동작 720에서, 프로세서는 타깃 프로세서 상에서 실행 함수(operation)를 지원하는지 확인할 수 있다. 타 깃 프로세서는 예를 들어, CPU, NPU 또는 GPU 중 어느 하나를 포함할 수 있다. 일 실시예에서, CPU는 GPU 또는 NPU보다 상대적으로 많은 실행 함수(operation)를 지원할 수 있다. 프로세서는 GPU 또는 NPU가 실행 함수 (operation)를 지원하지 않는 경우 CPU를 이용하여 실행 함수(operation)를 실행할 수 있다. 또는 프로세서 는 프레임워크에서 실행 함수(operation)에 대한 정보를 획득할 수도 있다. 동작 730에서, 프로세서는 인공지능 모델 상에서 추론(inference)을 실행하기 위한 첫 번째 노드가 에러 (error)없이 동작하는지 확인할 수 있다. 프로세서는 타깃 프로세서 상에서 실행 함수(operation)를 지원 함에 기반하여 인공지능 모델 상에서 추론(inference)을 실행하기 위한 첫 번째 노드가 에러(error)없이 동작하 는지 확인할 수 있다. 에러(error)는 예를 들어, 메모리 사이즈의 제한으로 인해 단일한 파티션(partition) 형 태로 구성된 그래프의 실행이 불가능한 상황을 의미할 수 있다. 단일한 파티션(partition) 형태로 구성된 그래 프는 메모리 사이즈보다 상대적으로 큰 메모리를 요구할 수 있다. 이 경우 프로세서는 정상적으로 컴파일 을 수행하기 어려울 수 있다. 이러한 에러를 방지하기 위해 프로세서는 적어도 하나 이상의 파티션 (partition)을 이용하여 복수의 노드들을 분할하고, 메모리 사이즈의 제약을 극복하고 정상적으로 컴파일을 수 행할 수 있다. 동작 740에서, 프로세서는 확인이 통과됨에 기반하여 노드를 하나 더 추가하여 확인을 수행하고, 마지막 노드까지 확인을 반복할 수 있다. 예를 들어, 프로세서는 N번째 노드에서 상기 확인이 통과되지 않음에 기 반하여 첫번째 노드부터 확인이 통과된 N-1번째 노드까지 파티션(partition)을 생성하여 제 1 그룹을 형성할 수 있다. N은 자연수를 의미할 수 있으며, 범위는 설정에 따라 달라질 수 있다. 프로세서는 확인이 통과되지 않은 N번째 노드에 대해 파티션(partition)을 생성하여 제 2 그룹을 형성할 수 있다. 프로세서는 N+1 번째 노드부터 마지막 노드까지 인공지능 모델 상에서 추론(inference)이 에러 없이 실행되는지에 대한 확인을 반복 할 수 있다. 일 실시예에 따르면, 제 1 그룹에 포함된 노드들은 internal path(compile or interpreting)을 수행하는 모든 장치(Xpu)(예: internal path를 포함하는 NPU (neural processing unit) 또는 GPU(graphics processing unit))에 의해 실행될 수 있다. 이는 도 6a의 sub partition을 의미할 수 있다. 일 실시예에 따르면, 제 2 그룹에 포함된 노드는 제 1 그룹과는 다른 에 의해 실행될 수 있다. 이는 도 6a의 fallback sub partition을 의미할 수 있다. 일 실시예에 따르면, 프로세서는 타깃 프로세서가 실행 함수(operation)을 지원하지 않음에 기반하여 다른 CPU, NPU 또는 GPU 중 어느 하나를 이용하여 실행 함수(operation)를 지원하도록 제어할 수 있다. 일 실시예에 따르면, 프로세서는 노드를 새롭게 추가한 상황에서 상기 타깃 프로세서를 이용하여 새롭게 추가된 노드까지 포함하더라도 에러 없이 추론(inference)을 실행할 수 있는지 확인을 수행할 수 있다. 일 실시예에 따르면, 프로세서는 타깃 프로세서를 이용하여 확인을 수행하고, 새롭게 추가된 노드를 포함 하면 에러 없이 추론(inference)을 실행하기 어려운 것으로 결정됨에 기반하여 이전 노드들과 새롭게 추가된 노 드들 사이에 파티션(partition)을 생성할 수 있다. 일 실시예에 따르면, 프로세서는 인공지능 모델의 컴파일이 완료됨에 기반하여 타깃 프로세서에서 사용 가 능하도록 컴파일된 결과물(예: 커널 또는 이미지)을 메모리에 저장할 수 있다. 여기서 커널은 컴파일의 결 과물을 의미할 수 있다. 컴파일은 프로세서가 벡엔드(backend)에서 특정 파일을 해석해서 프로세서의 코어(core)에서 작동시킬 수 있는 형태로 변경하는 과정을 의미할 수 있다. 본 명세서와 도면에 개시된 본 문서의 실시예는 본 문서의 실시예에 따른 기술 내용을 쉽게 설명하고 본 문서의 실시예의 이해를 돕기 위해 특정 예를 제시한 것일 뿐이며, 본 문서의 실시예의 범위를 한정하고자 하는 것은 아니다. 따라서 본 문서의 일 실시예의 범위는 여기에 개시된 실시예 이외에도 본 문서의 일 실시예의 기술적 사상을 바탕으로 도출되는 모든 변경 또는 변형된 형태가 본 문서의 일 실시예의 범위에 포함되는 것으로 해석 되어야 한다.도면 도면1 도면2a 도면2b 도면3a 도면3b 도면4 도면5a 도면5b 도면5c 도면6a 도면6b 도면7"}
{"patent_id": "10-2023-0038744", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 다양한 실시예들에 따른, 네트워크 환경 내의 전자 장치의 블록도이다. 도 2는 뉴럴 네트워크(NN, Neural network) 구조의 일 예에 관한 도면이다. 도 3a 및 도 3b는 비교 실시예에 따른 인공지능 학습모델에서 각 노드(node)의 실행 단위(partition)를 통해 순 차적으로 추론(inference)하는 과정을 도시한 것이다. 도 4는 연산을 위해 노드들 사이에 파티션(partition)을 자동으로 생성하는 전자 장치의 계층(layer)을 도시한 것이다. 도 5a는 연산을 위해 노드들 사이에 파티션(partition)을 자동으로 생성하는 제 1 실시예를 도시한 것이다. 도 5b는 제 1 실시예에 대한 과정을 순서도로 나타낸 것이다. 도 6a는 연산을 위해 노드들 사이에 파티션(partition)을 자동으로 생성하는 제 2 실시예를 도시한 것이다. 도 6b는 제 2 실시예에 대한 과정을 순서도로 나타낸 것이다. 도 7은 일 실시예에 따른 전자 장치의 연산 수행 방법을 순서도로 나타낸 것이다."}
