{"patent_id": "10-2021-0105063", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0023189", "출원번호": "10-2021-0105063", "발명의 명칭": "작업 유사성 기반의 다중 작업 수행 방법 및 장치", "출원인": "한국전자통신연구원", "발명자": "김은우"}}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "작업 유사성 기반의 다중 작업 학습 수행 방법에 있어서, 제1 작업과 제2 작업의 유사성 분석을 수행하는 단계;상기 유사성 분석의 결과를 기반으로 제2 작업을 위해 신경망을 학습하는 단계;를 포함하되,상기 신경망은,상기 제1 작업을 위해 사용된 제1 학습 데이터와 상기 제2 작업을 위해 사용된 제2 학습 데이터가 유사하다고판단되면,상기 제1 학습 데이터에 할당된 제1 파라미터를 기반으로 제2 학습 데이터에 할당된 제2 파라미터를 학습하는,다중 작업 학습 수행 방법."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 신경망은 상기 제1 학습 데이터를 이용하여 상기 제1 작업에 대해 기 학습된, 다중 작업 학습 수행 방법."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 제1 학습 데이터와 상기 제2 학습 데이터는 이미지 데이터인, 다중 작업 학습 수행 방법."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,상기 제1 학습 데이터와 상기 제2 학습 데이터는 차원 축소가 진행된 이미지 데이터인, 다중 작업 학습 수행 방법."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서, 상기 유사성 분석은 상기 제1 학습 데이터와 상기 제2 학습 데이터의 이미지 클러스터링을 통한 이미지 벡터 간 거리를 계산하여 유사도를 산출하는 것을 포함하는, 다중 작업 학습 수행 방법."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서,공개특허 10-2023-0023189-3-상기 이미지 벡터 간 거리는 기 설정된 임계 값과 비교되는, 다중 작업 학습 수행 방법."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서, 상기 유사도는 원-핫 벡터(one-hot vector)의 형태로 정의되는, 다중 작업 학습 수행 방법."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항에 있어서,상기 신경망은 완전 연결 레이어(fully connected layer)를 기반으로 하는, 다중 작업 학습 수행 방법."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "작업 유사성 기반의 다중 작업 학습 수행 장치에 있어서, 데이터를 저장하는 메모리;상기 메모리를 제어하는 프로세서;를 포함하되,상기 프로세서는, 제1 작업과 제2 작업의 유사성 분석을 수행하고, 상기 유사성 분석의 결과를 기반으로 제2 작업을 위해 신경망을 학습하되,상기 제1 작업을 위해 사용된 제1 학습 데이터와 상기 제2 작업을 위해 사용된 제2 학습 데이터가 유사하다고판단되면,상기 제1 학습 데이터에 할당된 제1 파라미터를 기반으로 제2 학습 데이터에 할당된 제2 파라미터를 학습하는,다중 작업 학습 수행 장치."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9 항에 있어서,상기 신경망은 상기 제1 학습 데이터를 이용하여 상기 제1 작업에 대해 기 학습된, 다중 작업 학습 수행 장치."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10 항에 있어서,상기 제1 학습 데이터와 상기 제2 학습 데이터는 이미지 데이터인, 다중 작업 학습 수행 장치."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 제1 학습 데이터와 상기 제2 학습 데이터는 차원 축소가 진행된 이미지 데이터인, 다중 작업 학습 수행 장치.공개특허 10-2023-0023189-4-청구항 13 제12 항에 있어서, 상기 유사성 분석은 상기 제1 학습 데이터와 상기 제2 학습 데이터의 이미지 클러스터링을 통한 이미지 벡터 간거리를 계산하여 유사도를 산출하는 것을 포함하는, 다중 작업 학습 수행 장치."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13 항에 있어서,상기 이미지 벡터 간 거리는 기 설정된 임계 값과 비교되는, 다중 작업 학습 수행 장치."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14 항에 있어서, 상기 유사도는 원-핫 벡터(one-hot vector)의 형태로 정의되는, 다중 작업 학습 수행 장치."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제10 항에 있어서,상기 신경망은 완전 연결 레이어(fully connected layer)를 기반으로 하는, 다중 작업 학습 수행 장치."}
{"patent_id": "10-2021-0105063", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "비-일시적 컴퓨터 판독 가능한 매체에 저장된 프로그램에 있어서,컴퓨터에서,제1 작업과 제2 작업의 유사성 분석을 수행하는 단계;상기 유사성 분석의 결과를 기반으로 제2 작업을 위해 신경망을 학습하는 단계;를 수행하되,상기 신경망이,상기 제1 작업을 위해 사용된 제1 학습 데이터와 상기 제2 작업을 위해 사용된 제2 학습 데이터가 유사하다고판단되면,상기 제1 학습 데이터에 할당된 제1 파라미터를 기반으로 제2 학습 데이터에 할당된 제2 파라미터를 학습하게하는,비-일시적 컴퓨터 판독 가능한 매체에 저장된 프로그램."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 인공지능을 활용한 작업 유사성 기반의 다중 작업 수행 방법 및 장치에 대한 것이다. 본 개시의 일 실 시예에 따른 작업 유사성 기반의 다중 작업 학습 수행 방법은 제1 작업과 제2 작업의 유사성 분석을 수행하는 단 계, 상기 유사성 분석의 결과를 기반으로 제2 작업을 위해 신경망을 학습하는 단계를 포함하되, 상기 신경망은, 상기 제1 작업을 위해 사용된 제1 학습 데이터와 상기 제2 작업을 위해 사용된 제2 학습 데이터가 유사하다고 판 단되면, 상기 제1 학습 데이터에 할당된 제1 파라미터를 기반으로 제2 학습 데이터에 할당된 제2 파라미터를 학 습할 수 있다."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공지능을 활용한 작업 유사성 기반의 다중 작업 수행 방법 및 장치에 대한 것이다."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "4차 산업혁명의 화두 아래 인공지능, 빅데이터, 사물 인터넷, 클라우드 컴퓨팅과 같은 기술들이 널리 사용되어 지고 있다. 여기서 인공 지능(Artificial Intelligent)은 학습, 문제 해결, 패턴 인식 등과 같이 주로 인간 지 능과 연결된 인지 문제를 해결하는 데 주력하는 기술이다. 최근 컴퓨팅 효율성 개선에 따라 AI를 기반으로 한 기계 학습 기술도 함께 발전을 거듭하게 되었다. 이와 마찬가지로 네트워크 컴퓨팅이 발전하면서 딥 러닝 기술도 더욱 빠른 속도로 발전 및 응용되고 있다. 포즈 찾기, 데이터 구분하기, 사진 복원하기 등 다양한 분야에 딥 러닝 기술이 적용될 수 있으며, 딥러닝 기술을 활용하는 기술에 관한 관심이 증가한 만큼, 딥러닝 모델의 경량 화 및 효율성에 대한 요구가 늘어나고 있다. 대표적인 딥러닝 모델 경량화 방법에는 가지치기(pruning), 증류(distillation) 등이 있는데, 그 중 네트워크의 효율성을 높이면서 딥러닝 모델을 경량화 할 수 있는 기술 중 하나로 다중작업 학습이 있다. 다중 작업 학습은 기존 딥 러닝의 기술을 다중 작업에 확장, 적용시킨 것으로, 기존의 딥 러닝은 하나의 모델 안에서 하나의 작업만 수행하는 것이 일반적이나, 반면 다중 작업 학습은 하나의 모델 안에서 여러 작업들을 동 시에 최적화하는 것이 특징이다. 다중 작업 학습 기술은 같은 계산 비용으로 더 많은 결과를 낸다는 점에서 기 존 딥 러닝 기술에 비해 효율적일 수 있다. 다중 작업 학습을 통해 제한된 자원을 효율적으로 운용하여 사용자 들에게 더 나은 서비스를 제공해 줄 수 있다. 특히 온 디바이스(On-device) 제품에서의 딥 러닝 기술 적용을 고려하면, 온 디바이스 제품들은 제한된 자원을 가지고 있고, 작업마다 용량을 내어줄 수 있을 만큼 온 디바이스 제품들의 자원은 넉넉하지 못하므로, 여러 작 업이 독립적으로 메모리를 차지하기가 현실적으로 어려울 수 있다. 때문에, 딥러닝 상용화 측면에서 다중 작업 학습 기술이 필수적이라고 볼 수 있어, 딥 러닝의 다중작업 학습 기술에 대한 연구의 중요성이 높아지는 실정이 다."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 목적은 인공지능을 활용한 작업 유사성 기반의 다중 작업 수행 방법 및 장치를 제공하는 데 있다. 본 개시의 목적은 하나의 신경망을 학습함에 있어서 연속적인 다중 작업 학습을 가능하게 하는 효율적인 학습 방법 및 장치를 제공하는 데 있다. 본 개시의 목적은 한정된 GPU 메모리를 가지고 신경망을 학습하는 데 있다. 본 개시의 다른 목적 및 장점들은 하기의 설명에 의해서 이해될 수 있으며, 본 개시의 실시예에 의해 보다 분명 하게 알게 될 것이다. 또한, 본 개시의 목적 및 장점들은 특허청구범위에 나타낸 수단 및 그 조합에 의해 실현 될 수 있음을 쉽게 알 수 있을 것이다."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따르면, 작업 유사성 기반의 다중 작업 학습 수행 방법은 제1 작업과 제2 작업의 유사성 분석을 수행하는 단계, 상기 유사성 분석의 결과를 기반으로 제2 작업을 위해 신경망을 학습하는 단계를 포함하 되, 상기 신경망은, 상기 제1 작업을 위해 사용된 제1 학습 데이터와 상기 제2 작업을 위해 사용된 제2 학습 데 이터가 유사하다고 판단되면, 상기 제1 학습 데이터에 할당된 제1 파라미터를 기반으로 제2 학습 데이터에 할당 된 제2 파라미터를 학습할 수 있다. 한편, 상기 신경망은 상기 제1 학습 데이터를 이용하여 상기 제1 작업에 대해 기 학습된 것일 수 있다. 한편, 상기 제1 학습 데이터와 상기 제2 학습 데이터는 이미지 데이터일 수 있다. 한편, 상기 제1 학습 데이터와 상기 제2 학습 데이터는 차원 축소가 진행된 이미지 데이터일 수 있다. 한편, 상기 유사성 분석은 상기 제1 학습 데이터와 상기 제2 학습 데이터의 이미지 클러스터링을 통한 이미지 벡터 간 거리를 계산하여 유사도를 산출하는 것을 포함할 수 있다. 한편, 상기 이미지 벡터 간 거리는 기 설정된 임계 값과 비교될 수 있다. 한편, 상기 유사도는 원-핫 벡터(one-hot vector)의 형태로 정의될 수 있다. 한편, 상기 신경망은 완전 연결 레이어(fully connected layer)를 기반으로 할 수 있다. 본 개시의 일 실시예에 따른 작업 유사성 기반의 다중 작업 학습 수행 장치는, 데이터를 저장하는 메모리, 상기 메모리를 제어하는 프로세서를 포함하되, 상기 프로세서는, 제1 작업과 제2 작업의 유사성 분석을 수행하고, 상 기 유사성 분석의 결과를 기반으로 제2 작업을 위해 신경망을 학습하되, 상기 제1 작업을 위해 사용된 제1 학습데이터와 상기 제2 작업을 위해 사용된 제2 학습 데이터가 유사하다고 판단되면, 상기 제1 학습 데이터에 할당 된 제1 파라미터를 기반으로 제2 학습 데이터에 할당된 제2 파라미터를 학습할 수 있다. 한편 상기 신경망은 상기 제1 학습 데이터를 이용하여 상기 제1 작업에 대해 기 학습된 것일 수 있다. 한편, 상기 제1 학습 데이터와 상기 제2 학습 데이터는 이미지 데이터일 수 있다. 한편, 상기 제1 학습 데이터와 상기 제2 학습 데이터는 차원 축소가 진행된 이미지 데이터일 수 있다. 한편, 상기 유사성 분석은 상기 제1 학습 데이터와 상기 제2 학습 데이터의 이미지 클러스터링을 통한 이미지 벡터 간 거리를 계산하여 유사도를 산출하는 것을 포함할 수 있다. 한편, 상기 이미지 벡터 간 거리는 기 설정된 임계 값과 비교될 수 있다. 한편,상기 유사도는 원-핫 벡터(one-hot vector)의 형태로 정의될 수 있다. 한편, 상기 신경망은 완전 연결 레이어(fully connected layer)를 기반으로 할 수 있다. 본 개시의 일 실시예에 따른 비-일시적 컴퓨터 판독 가능한 매체에 저장된 프로그램은, 컴퓨터에서, 제1 작업과 제2 작업의 유사성 분석을 수행하는 단계, 상기 유사성 분석의 결과를 기반으로 제2 작업을 위해 신경망을 학습 하는 단계를 수행하되, 상기 신경망은, 상기 제1 작업을 위해 사용된 제1 학습 데이터와 상기 제2 작업을 위해 사용된 제2 학습 데이터가 유사하다고 판단되면, 상기 제1 학습 데이터에 할당된 제1 파라미터를 기반으로 제2 학습 데이터에 할당된 제2 파라미터를 학습하게 할 수 있다."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면 하나의 신경망을 기반으로 다중 작업 학습을 수행할 수 있다. 본 개시에 따르면 한정된 자원을 이용하여 효율적인 다중 작업 학습을 수행할 수 있다. 본 개시에 따르면 온 디바이스(on-device)에서 딥러닝 기반의 다중 작업 학습을 수행할 수 있다. 본 개시의 실시 예들에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은 이하의 본 개시의 실시 예들에 대한 기재로부터 본 개시의 기술 구성이 적용되는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 도출되고 이해될 수 있다. 즉, 본 개시에서 서술하는 구성을 실시함에 따 른 의도하지 않은 효과들 역시 본 개시의 실시 예들로부터 당해 기술 분야의 통상의 지식을 가진 자에 의해 도 출될 수 있다."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참고로 하여 본 개시의 실시 예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나, 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 본 개시의 실시 예를 설명함에 있어서 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그에 대한 상세한 설명은 생략한다. 그리고, 도면에서 본 개시에 대한 설명과 관계없 는 부분은 생략하였으며, 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 개시에 있어서, 서로 구별되는 구성요소들은 각각의 특징을 명확하게 설명하기 위함이며, 구성요소들이 반드 시 분리되는 것을 의미하지는 않는다. 즉, 복수의 구성요소가 통합되어 하나의 하드웨어 또는 소프트웨어 단위 로 이루어질 수도 있고, 하나의 구성요소가 분산되어 복수의 하드웨어 또는 소프트웨어 단위로 이루어질 수도 있다. 따라서, 별도로 언급하지 않더라도 이와 같이 통합된 또는 분산된 실시 예도 본 개시의 범위에 포함된다. 본 개시에 있어서, 다양한 실시 예에서 설명하는 구성요소들이 반드시 필수적인 구성요소들을 의미하는 것은 아 니며, 일부는 선택적인 구성요소일 수 있다. 따라서, 일 실시 예에서 설명하는 구성요소들의 부분집합으로 구성 되는 실시 예도 본 개시의 범위에 포함된다. 또한, 다양한 실시 예에서 설명하는 구성요소들에 추가적으로 다른 구성요소를 포함하는 실시 예도 본 개시의 범위에 포함된다. 이하, 본 개시의 실시예들을 설명함에 있어서 모델, 네트워크, 신경망은 혼용될 수 있다. 이하, 도면을 참조하여 본 개시에 대해 상세히 설명한다. 도 1은 본 개시에 따른 크로스 스티치(cross-stitch) 구조 기반의 다중 작업 학습 기술을 나타낸 도면이다. 신경망 기반의 다중 작업 학습 기술은 동일한 계산량, 계산 비용으로 더 다양한 작업에 대한 결과를 도출할 수 있다는 점에서 기존의 딥 러닝 기술에 비해 효율적일 수 있다. 기존 딥 러닝의 기술을 확장시킨 다중 작업 학습을 구조적으로 접근하여 구현한 기술인 크로스 스티치 네트워 크(Cross-stitch network)는 다중작업 학습을 병렬적으로 수행하게 하는 구조로서, 개별적 네트워크 사이에 도 입되는 크로스 스티치 유닛(Cross-Stitch unit)을 기반으로 한다. 즉, 네트워크 A는 작업 A에 대해 학습되고, 네트워크 B는 작업 B에 대해 학습된다고 가정하면, 크로스 스티치 유닛은 작업 별(task-specific) 네트워크, 즉 네트워크 A 및 네트워크 B 간의 지식 공유가 가능하게끔 만들어 준다. 한편, 크로스 스티치 네트워크는 결국 네트워크 당 하나의 작업 만을 수행하며, 작업이 늘어나면 파라미터 수도 선형적으로 늘어나 계산량도 늘어날 수 있다. 도 2는 본 개시에 따른 매개 변수 할당 기반의 다중 작업 학습 기술을 나타낸 도면이다. 연속적인 다중 작업 학습에서는 학습을 진행할 때, 이전 작업을 위해 이전 학습 데이터에 의해 학습되었던 모델 의 성능을 유지시켜야만 앞서 학습한 내용 유실(Catastrophic forgetting)이 발생하지 않는다. 앞서 학습한 내 용, 즉 선행 작업에 대해 학습한 내용이 바뀌지 않게 하기 위해, 선행 작업 시 중요한 매개 변수를 모델(신경망)에 할당하는 방식이 있다. 중요한 매개 변수를 모델에 할당하게 되면 이후에 다른 데이터로 학습을 진행하여도 할당된 매개 변수는 변하지 않기 때문에 성능의 유지에 있어서 굉장히 유리한 면이 있다. 도 2의 모델은 일 실시예로서 학습 과정을 3단계(훈련->프루닝->재훈련)로 구성하였다. 먼저, 작업 I를 위한 초 기 필터(initial filter)(a)에서 파라미터를 60% 제거(프루닝, pruning)하고, 재훈련을 수행했다고 가정한다. 이후, 작업 I를 위한 최종 필터(final filter)(b)를 작업 II에 대해 학습하고, 작업 II를 위한 초기 필터 (initial filter)(c)다. 이후, 33% 프루닝 후 재학습을 수행하여, 작업 II를 위한 최종 필터를 생성하고, 다시 작업 III에 대해 학습을 수행할 수 있다. 학습이 끝난 뒤에는 중요한 매개 변수를 제외한 불필요한 매개 변수를 제거(pruning)하는 프루닝 과정은, 성능 에 큰 영향을 미치지 않은 매개 변수를 제거하게 될 수 있다. 일 실시예로서, 성능에 큰 영향을 미치지 않는 매 개 변수를 선별하는 기준은 매개 변수의 가중치를 이용할 수 있다. 재훈련 과정은, 마지막으로 남아있는 중요한 매개 변수들을 재훈련하는 과정을 포함할 수 있다. 한편 상기에서 설명한 다중 작업 학습 기술은 작업 (예를 들어, I, II, III)간의 관계를 고려하지 않고 전체 파 라미터를 추론과정에서 함께 활용하기 때문에, 작업 간 관련성이 높을 때 보다 효율적일 수 있다. 즉, 작업 간 관련성이 적거나 작업 간 관련 성에 대해 판단하기 어려운 경우(후행 작업이 결정되지 않은 경우 등) 보다는 작업 간 관련성이 높을 때 이를 활용하면 장점이 더욱 극대화될 수 있다. 이에 따라, 작업 간의 관련성이 높은 경우에는 물론, 작업 간의 관련성이 높지 않거나 후행 작업이 결정되지 않 은 경우 등에도 효율적인 본 개시에 따른 다중 작업 학습 기술을 이하 설명한다. 본 개시에 따른 작업 유사성 기반의 다중 작업 학습 기술에 의하면, 다중 작업을 연속적으로 학습함에 있어서 유사성이 있는 작업을 스스로 판단하여, 다중 작업의 모델을 학습함에 있어 연관성이 있는 작업 파라미터만을 추론에 활용하는 방식을 통해서, 보다 효율적인 연속 다중 작업 학습을 가능하게 할 수 있다. 본 개시에 따른 작업 유사성 기반의 다중 작업 학습 기술은, 앞서 설명한 매개 변수 할당에 있어서 작업 별로 서로 다른 매개 변수를 할당할 수 있고, 데이터가 한 번에 들어오지 않는 연속적 다중 작업 학습에서, 이전 데 이터에 대해 학습된 모델의 매개변수는 이후의 데이터에 대해 모델을 학습하여도 유지되어 이전 데이터와 이후 의 데이터에 대한 모델의 성능을 유지할 수 있다. 또한, 작업 간의 관계를 고려하지 않고 전체 매개 변수를 추론과정에서 함께 활용하는 경우, 즉, 이전에 학습되 어 있던 모델에 대한 매개 변수를 현재 전혀 다른 형태의 데이터에 대해 학습 시 모두 참고하여 학습을 진행한 다면, 그 매개 변수는 현재 할당된 매개 변수를 학습할 때 잡음(noise)으로 작용하게 되어 원활한 학습을 방해 하게 될 수 있다. 따라서 작업 간의 유사성 분석을 통해 어떤 데이터가 기존에 학습 되어있는 데이터의 매개 변 수 중에서 어떤 데이터에 대한 매개 변수를 이용하여 학습할 것인지 선택하여 학습하는 것이 효율적일 수 있다. 이에 따라, 본 개시에 따른 작업 유사성 기반의 다중 작업 학습 기술에서는 다중 작업을 연속적으로 학습함에 있어서 유사성이 있는 작업을 스스로 판단하여, 다중 작업의 모델을 학습함에 있어 연관성이 있는 작업 파라미 터만을 추론에 활용하는 방식을 통해서, 보다 효율적인 연속 다중 작업 학습을 가능하게 할 수 있다. 도 3은 본 개시의 일 실시예에 따른 다중 작업 학습 과정을 나타낸 도면이다. 보다 상세하게는, 도 3은 본 개시의 일 실시예에 따른 연속적인 다중 작업 학습이 가능한 시각 정보 임베딩 모 델을 학습하기 위한 과정 중 제1 작업에 대한 제1 학습 데이터에 대해 훈련, 프루닝 및 재훈련의 과정을 나타낸 것이다. 일 실시예로서, 다중 작업은 모드 시각 정보와 관련된 작업이라고 가정하며, 다중 작업 학습 과정은, 크게 두 가지 과정으로 이루어질 수 있다. 먼저 연속적으로 들어오는 작업에 대한 유사성 분석을 하는 과정과 그리고 이 유사성 분석 결과를 통해서 연속적인 작업 학습을 하는 과정으로 구성될 수 있다. 일 예로서, 먼저 시각 작업 간의 유사성 분석이 가능하게 하려면, 즉 시각 데이터에 기초한 작업의 유사성 분석 이 가능하게 하려면, 신경망 학습 시 학습 데이터의 유사도에 기반을 두어 학습을 진행할 수 있다. 이전에 학습 되어 있던 신경망 모델에 대한 매개 변수를 현재 전혀 다른 형태의 데이터가 학습 시 참고하여 학습을 진행한다 면, 그 매개 변수는 현재 할당된 매개 변수를 학습할 때 잡음(noise)으로 작용하게 되어 원활한 학습을 방해할 수 있다. 따라서 유사성 분석을 통한 현재 학습 데이터의 학습을 위해, 기존에 학습되어 있는 데이터의 매개 변 수 중에서 어떤 학습 데이터에 대한 매개 변수를 이용하여 학습할 것인지 선택하여 학습하여야 한다. 또한, 다중 시각 작업을 위한 임베딩 모델을 활용하는 방식은 학습 데이터가 한 번에 들어오지 않을 수 있기 때 문에, 한 데이터에 대해서 모델이 학습된 이후 다른 데이터가 들어와 학습하여도 학습한 데이터와 현재 학습한 데이터에 대한 모델의 성능을 유지하는 방향으로 학습이 되어야 한다. 본 개시에 따르면 순차적으로 들어오는 학습 데이터를 이용해서 학습을 진행할 수 있기 때문에, 학습하여도 이미 학습된 데이터에 대한 모델의 성능이 하락하지 않는다. 도 3에서 나타난 원들은 특정 학습 데이터를 이용하여 학습 시 할당된 매개 변수들을 의미할 수 있다. 한 학습 데이터에 대해서 학습을 하고 난 뒤에, 불필요한 매개 변수들은 제거될 수 있다. 여기서, 불필요한 매개 변수들 이란 매개 변수들의 절댓값을 기준으로 선별된 것일 수 있으며, 절댓값이 작은 것들이 주로 삭제될 수 있다. 이 때 이전에 학습된 데이터에 대한 매개 변수는 제거되지 않을 수 있다. 작업 간 유사도를 이용한 모델 학습은 현재 모델에 학습되려는 데이터가 모델에 이미 학습되어 있는 데이터와 얼마나 유사한지를 판단함에 따라 수행할 수 있다. 즉, 선행 작업인 제1 작업의 제1 학습 데이터에 따라 현재 신경망 모델이 학습되어 있는 경우, 후행 작업인 제2 작업의 제2 학습 데이터 간의 유사도를 판단할 수 있다. 일 실시예로서, 제1 학습 데이터와 제2 학습 데이터가 이미지 데이터인 경우, 유사도를 판단하기 위해 고차원의 이미지의 차원 축소가 진행될 수 있다. 이미지 차원 축소가 진행되면, 저차원으로 축소된 이미지 벡터를 클러스 터링을 진행하여 현재 데이터와 이미 학습된 데이터의 중심 값의 거리를 계산하여 유사도를 산출할 수 있다. 이는, 도 4의 이미지 클러스터링을 기반으로 할 수 있는데, 이 과정에 대하여는 하기에서 도 4를 참조하여 더욱 상세하게 설명하기로 한다. 도 4는 본 개시의 일 실시예에 따른 데이터 임베딩 시각화를 나타낸 도면이다. 도 4는 이미지 임베딩의 시각적인 확인을 위한 도면으로서, 설명의 명료함을 위해 세 가지 서로 다른 학습 데이 터의 존재를 가정한다. 일 실시예로서, 세 가지 서로 다른 학습 데이터에 대해 이미지 임베딩을 2차원으로 줄여 플롯(plot)한 결과를 나타낸 도면이다. 일 예로서, 작업 간 최종적으로 유사도는 아래의 수식과 같이 원-핫 벡 터(one-hot vector) 형태로 정의될 수 있다."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "일 실시예로서, 위 수식에 따르면, 중심 값의 거리가 임계 값(threshold) 이상이면 1이고, 임계 값 이하이면 0 로 주어질 수 있다. 일 실시예로서, 학습을 진행할 때 유사도를 먼저 판단한 뒤, 유사도를 기반으로 학습을 할 수 있다. 유사도를 기반으로 학습을 한다는 것은, 현재 모델에 학습하기 위해 들어온 학습 데이터가 이전에 학습된 데이터와 유사 하다면, 이전에 학습된 데이터에 할당된 매개 변수를 기반으로 현재 데이터에 할당된 매개 변수를 학습하는 것 을 포함할 수 있다. 일 예로서, 현재 데이터에 할당된 매개 변수는 라고 정의한다. 현재 데이터를 학습할 때 이용하는 모델 의 매개 변수들을 식으로 나타내면 아래의 식과 같이 나타낼 수 있다."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "상기 식에서, 는 현재 학습하는 데이터를 모델에 포워드 패스(forward pass)할 때 사용하게 되는 매개 변수들을 의미한다. 실질적으로 학습이 진행되는 매개 변수들은 에 포함된 매개 변수들로, 현재 데이터에 대해 할당된 매개 변수들의 값들만 업데이트하게 될 수 있다. 도 5는 본 개시의 일 실시예에 따른 데이터 유사도에 따라 다중 작업 학습 과정을 나타낸 도면이다. 일 실시예로서, 도 5는 신경망 모델에 이전에 학습되었던 데이터가 존재할 때, 다른 학습 데이터가 추가로 학습 을 위해 모델에 들어왔을 때를 나타내고 있다. 특히, 이전에 학습된 데이터와 학습하려고 하는 데이터가 유사할 때를 가정하여 나타낸 도면이다. 제3 학습 데이터(세번째 학습 데이터)가 학습을 위해 모델에 들어왔을 때, 제1 학습 데이터(첫 번째 학습된 데 이터)와 유사하고, 제2 학습 데이터(두 번째 학습된 데이터)와는 유사하지 않을 때 사용하는 매개 변수를 나타 냈다. 일 실시예로서, 제1 학습 데이터를 기반으로 한 제1 작업의 학습 및 즉, 제2 학습 데이터를 기반으로 한 제2 작 업의 학습은 상기에서 설명한 훈련->프루닝->재훈련의 과정을 거쳐 수행되었다고 가정한다. 일 실시예로서, 제3 작업과 제2 작업은 유사하지 않으므로, 제2 작업 학습과 관련된 매개 변수는 제3 작업을 위 한 학습 시 중요도가 낮게 책정되어 고려되지 않았음을 확인할 수 있다. 이에 따르면, 제2 작업에 대한 학습이 이루어질 때 매개 변수 업데이트가 학습 과정에서 유사도가 있는 작업 간의 관계만을 활용하여 학습하는 방법으 로 보다 효율적으로 다중 시각 기억을 학습할 수 있다. 도 6은 본 개시의 일 실시예에 따른 작업 유사성 기반의 다중 작업 학습 방법을 나타낸 도면이다. 일 예로서, 도 6의 다중 작업 학습 방법은 상기에서 다른 도면을 참조하여 언급한 다중 작업 학습을 기반으로 하며, 도 7의 장치를 포함한 다중 작업 학습 장치 및 다중 작업 학습 시스템 등에 의해 수행될 수 있으며, 본 개시가 이에 한정되는 것은 아니다. 일 예로서, 작업 유사성 기반의 다중 작업 학습 수행 방법은, 작업 간의 유사성 분석을 수행(S601)할 수 있다. 상기 작업에는 제1 작업 및 제2 작업을 포함한 임의의 n개의 작업이 포함될 수 있으며, 연속적인 작업에 해당할 수 있다. 또한, 각 작업을 위한 학습 데이터도 제n 학습 데이터로 명명될 수 있다. 다만, 복수 개의 작업들은 신경망을 학습하기 전에 기 정해져있거나, 정해지지 않을 수 있다. 일 실시예로서, 유사성 분석이란, 상기 제1 학습 데이터와 상기 제2 학습 데이터가 이미지 데이터인 경우, 상기 제1 학습 데이터와 상기 제2 학습 데이터는 차원 축소가 진행된 이미지 데이터일 수 있고, 이미지 클러스터링을 통한 이미지 벡터 간 거리를 계산하여 유사 도를 산출하는 것을 포함할 수 있다. 일 예로서, 상기 이미지 벡터 간 거리는 기 설정된 임계 값과 비교되어, 임계 값 이하이면 유사도가 높은 것으로 산출되고, 임계 값 이상이면 유사도가 낮은 것으로 산출될 수 있다. 또 한, 상기 유사도는 원-핫 벡터(one-hot vector)의 형태로 정의될 수 있는데, 이는 상기에서 언급한 바와 같이, 하기의 수식에 따를 수 있다."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이후, 상기 유사성 분석의 결과를 기반으로 제2 작업을 위해 신경망을 학습(S602)할 수 있다. 일 예로서, 상기 신경망은, 상기에서 언급한 바와 같이 상기 제1 작업을 위해 사용된 제1 학습 데이터와 상기 제2 작업을 위해 사용된 제2 학습 데이터가 유사하다고 판단되면, 상기 제1 학습 데이터에 할당된 제1 파라미터를 기반으로 제2 학습 데이터에 할당된 제2 파라미터를 학습할 수 있다. 이는, 상기에서 [수식 2]를 참조하여 설명한 바와 동일 할 수 있다. 일 실시예로서, 상기 신경망은 상기 제1 학습 데이터를 이용하여 상기 제1 작업에 대해 기 학습된 것일 수 있다. 일 예로서, 상기 신경망은 완전 연결 레이어(fully connected layer)를 기반으로 할 수 있다. 일 실시예로서, 도 6의 다중 작업 학습 방법은 일 실시예에 해당하므로, 각 단계의 순서가 변경되거나, 다른 단 계가 더 추가되거나 일부 단계는 삭제될 수 있다. 도 7은 본 개시의 일 실시예에 따른 작업 유사성 기반의 다중 작업 학습 장치를 나타낸 도면이다. 일 실시예로서, 다중 작업 학습 장치는 데이터를 저장하는 메모리 및 상기 메모리를 제어하는 프로세 서를 포함할 수 있다. 일 실시예로서, 다중 작업 학습 장치는 상기에서 언급한 다중 작업 학습 방법을 포함한 다중 작업 학습을 수행 할 수 있다. 일 실시예로서, 상기 프로세서는 제1 작업과 제2 작업의 유사성 분석을 수행하고, 상기 유사성 분석의 결과를 기반으로 제2 작업을 위해 신경망을 학습하되, 상기 제1 작업을 위해 사용된 제1 학습 데이터와 상기 제2 작업 을 위해 사용된 제2 학습 데이터가 유사하다고 판단되면, 상기 제1 학습 데이터에 할당된 제1 파라미터를 기반 으로 제2 학습 데이터에 할당된 제2 파라미터를 학습할 수 있다. 일 실시예로서, 상기 신경망은 상기 제1 학습 데이터를 이용하여 상기 제1 작업에 대해 기 학습된 것일 수 있다. 또한, 상기 제1 학습 데이터와 상기 제2 학습 데이터는 이미지 데이터일 수 있다. 상기 제1 학습 데이터 와 상기 제2 학습 데이터는 차원 축소가 진행된 이미지 데이터일 수 있다. 상기 유사성 분석은 상기 제1 학습 데이터와 상기 제2 학습 데이터의 이미지 클러스터링을 통한 이미지 벡터 간 거리를 계산하여 유사도를 산출하 는 것을 포함할 수 있다. 상기 이미지 벡터 간 거리는 기 설정된 임계 값과 비교될 수 있다. 상기 유사도는 원- 핫 벡터(one-hot vector)의 형태로 정의될 수 있다. 상기 신경망은 완전 연결 레이어(fully connected layer) 를 기반으로 할 수 있다. 한편, 상기에서 도면을 참조하여 설명한 다중 작업 학습 장치는 설명의 명료함을 위해 메모리 및 프로세서로 나 누어 설명한 것이며, 도면과 다르게 구성될 수 있다. 예를 들어, 프로세서는 작업 간 유사성 분석 모듈 및 유사성 분석 결과를 이용한 다중 작업 학습 모듈 등으로 구성될 수도 있으며, 이에 한정되지 않는다. 도 8은 본 개시의 일 실시예에 따른 학습 데이터를 나타낸 도면이다. 보다 상세하게는, 상기에서 설명한 작업 유사성 기반의 다중 작업 학습 기술에 사용한 학습 데이터를 나타낸 도 면으로서, 본 개시에서 설명된 다중 작업 학습을 기반으로 한 실험에 사용된 학습 데이터를 나타낸 도면이다. 일 실시예로서, 학습 데이터는 일 실시예로서, ImageNet, Stanford dogs, MNIST, CUBS, Fashion MNIST, Stanford Cars, Flowers 등일 수 있다. 하기에서는 설명의 명료함을 위하여 이미지 분류에서 일반적으로 사용되 는 ImageNet을 사용하였다고 가정하고, CUBS와 Stanford Cars와 같은 Find grained data를 추가하여 모델에 학 습한 데이터 중 유사한 데이터를 찾아 효과적으로 학습이 되었는지 확인하였다고 가정한다. 또한, ImageNet과 Find grained 데이터와 성질이 다른 MNIST와 Fashion MNIST 흑백 데이터를 추가하여 모델이 학습하는 데이터의 확장이 효과적으로 이루어졌는지 확인하였다고 가정한다. 또한, 연속 작업 학습을 수행하면서, 학습 데이터는 ImageNet -> S. Dogs -> MNIST -> CUBS -> F.MNIST -> S.Cars -> Flowers 순으로 학습을 시행하였다고 가정한다. 여기서, 사용한 데이터셋의 학습 데이터 및 클래스의 숫자는 아래와 같을 수 있다.[표 1]"}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "일 예로서, 작업 학습을 위한 작업 유사성 분석 결과는 하기의 표와 같을 수 있다. [표 2]"}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "위의 [표 2]는 연속적인 다중 작업 간의 유사도를 나타낸 결과로, 일 실시예로서, 이를 모델에 학습하는 데이터 간 유사도로서 나타낸 결과이다. 데이터 S.Dogs와 S.Cars는 각각 Stanford Dogs와 Stanford Cars를 의미하고, F.MNIST는 Fashion MNIST 데이터를 의미한다. 새로운 데이터가 모델로 들어오면 이미지의 임베딩을 PCA 차원 축 소 기법을 이용하여 도출해 낸다. 실험에서 사용한 차원은 30차원으로 사용하였다. 각 종류의 데이터마다 랜덤 하게 100개의 이미지를 골라 이미지 임베딩을 수행하는데, 예를 들어, KNN 알고리즘을 통해 데이터의 종류만큼 클러스터를 나누는 과정을 포함할 수 있다. 이후, 이전에 학습하였던 데이터에 해당하는 클러스터의 중심 값과 현재 데이터에 해당하는 클러스터의 중심 값과의 거리가 산출될 수 있다. 일 실시예로서, 거리를 구하는 메트릭 (metric)은 유클리디안(Euclidean) 거리 방식이 사용될 수 있다. 일 실시예로서, 이 거리가 기 설정된 임계 값 (threshold)보다 작다면 유사하다고 판단하였다. 일 예로서, 실험에서 사용한 임계값은 120이다. 위 [표 2]의 결과에서 O는 유사한 데이터라는 것을 의미하고, ×는 다른 데이터로 분류된 것을 의미한다. 도 9는 본 개시의 일 실시예에 따른 데이터 2D 시각화 결과를 나타낸 도면이다. 보다 상세하게는, 도 9는 상기에서 설명한 실험에서 사용된 데이터들의 임베딩을 PCA 기법을 통해 2차원으로 축 소시킨 뒤 시각화한 도면이다. 고차원의 이미지 임베딩을 2차원으로 줄여, 실제로 계산된 유사도와는 같지 않지 만, 대략적인 분포를 보기 위해 시각화하였다. MNIST 데이터와 Fashion MNIST 데이터가 2D 임베딩 공간에서 위 치한 곳은 Fine grained 데이터들과 멀리 떨어져 있는 것을 알 수 있다. 아래의 [표 3]는 7가지 데이터에 대한 본 개시에 따른 이미지 분류 성능을 나타낸다. 특히, 세번째 데이터인 MNIST부터 마지막 데이터인 Flowers 데이터까지 정확도의 차이가 발생하는 것을 알 수 있다. 학습을 진행할 때 이전에 학습하였던 데이터에 대한 매개 변수들을 모두 사용하는 것보다 유사도에 따라서 학습을 진행하는 것이연속 학습에 있어서 효과적으로 성능을 향상할 수 있음을 확인할 수 있다. [표 3]"}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "본 개시에 따르면, 한 네트워크 당 데이터를 한번 수집하고 처음부터 새로이 학습하는 것이 아니라, 추가적인 데이터와 작업을 받아서 연속 학습이 가능하므로 딥러닝 네트워크의 실용적 활용에 핵심이 되는 학습 기술이 될 수 있다. 또한, 하나의 딥러닝 네트워크로 여러 작업을 수행하는 다중 작업 학습에서도 연속적인 학습을 지원하 는 네트워크의 경우 한정된 메모리 크기를 기반으로 성능이 떨어지지 않게 할 수 있다. 본 개시의 다양한 실시 예는 모든 가능한 조합을 나열한 것이 아니고 본 개시의 대표적인 양상을 설명하기 위한 것이며, 다양한 실시 예에서 설명하는 사항들은 독립적으로 적용되거나 또는 둘 이상의 조합으로 적용될 수도 있다. 또한, 본 개시의 다양한 실시 예는 하드웨어, 펌웨어(firmware), 소프트웨어, 또는 그들의 결합 등에 의해 구현 될 수 있다. 또한, 하나의 소프트웨어가 아닌 하나 이상의 소프트웨어의 결합에 의해 구현될 수 있으며, 일 주 체가 모든 과정을 수행하지 않을 수 있다. 예를 들어, 고도의 데이터 연산 능력 및 방대한 메모리를 요구하는 기계학습 과정은 클라우드나 서버에서 이루어지고, 사용자 측은 기계학습이 완료된 신경망만을 이용하는 방식으 로 구현될 수도 있으며, 이에 한정되지 않음은 자명하다. 하드웨어에 의한 구현의 경우, 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 범용 프로세서(general processor), 컨트롤러, 마이크로 컨트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 예를 들어, 상기 범용 프로세서를 포함한 다양한 형태 를 띨 수도 있다. 하나 혹은 그 이상의 결합으로 이루어진 하드웨어로 개시될 수도 있음은 자명하다. 본 개시의 범위는 다양한 실시 예의 방법에 따른 동작이 장치 또는 컴퓨터 상에서 실행되도록 하는 소프트웨어 또는 머신-실행 가능한 명령들(예를 들어, 운영체제, 애플리케이션, 펌웨어(firmware), 프로그램 등), 및 이러 한 소프트웨어 또는 명령 등이 저장되어 장치 또는 컴퓨터 상에서 실행 가능한 비-일시적 컴퓨터-판독가능 매체 (non-transitory computer-readable medium)를 포함한다. 일 실시예로서, 본 개시의 일 실시예에 따른 비-일시적 컴퓨터-판독가능 매체에 저장된 컴퓨터 프로그램은, 컴 퓨터에서 제1 작업과 제2 작업의 유사성 분석을 수행하는 단계, 상기 유사성 분석의 결과를 기반으로 제2 작업 을 위해 신경망을 학습하는 단계를 수행하되, 상기 신경망이, 상기 제1 작업을 위해 사용된 제1 학습 데이터와 상기 제2 작업을 위해 사용된 제2 학습 데이터가 유사하다고 판단되면, 상기 제1 학습 데이터에 할당된 제1 파 라미터를 기반으로 제2 학습 데이터에 할당된 제2 파라미터를 학습하게 할 수 있다. 한편, 각 도면을 참조하여 설명한 내용은 각 도면에만 한정되는 것은 아니며, 상반되는 내용이 없는 한 상호 보 완적으로 적용될 수도 있다."}
{"patent_id": "10-2021-0105063", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "이상에서 설명한 본 개시는, 본 개시가 속하는 기술분야에서 통상의 지식을 가진 자에게 있어 본 개시의 기술적 사상을 벗어나지 않는 범위 내에서 여러 가지 치환, 변형 및 변경이 가능하므로, 본 개시의 범위는 전술한 실시 예 및 첨부된 도면에 의해 한정되는 것이 아니다."}
{"patent_id": "10-2021-0105063", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시에 따른 크로스 스티치(cross-stitch) 구조 기반의 다중 작업 학습 기술을 나타낸 도면이다. 도 2는 본 개시에 따른 매개 변수 할당 기반의 다중 작업 학습 기술을 나타낸 도면이다. 도 3은 본 개시의 일 실시예에 따른 다중 작업 학습 과정을 나타낸 도면이다. 도 4는 본 개시의 일 실시예에 따른 데이터 임베딩(imbedding) 시각화를 나타낸 도면이다. 도 5는 본 개시의 일 실시예에 따른 데이터 유사도에 따라 다중 작업 학습 과정을 나타낸 도면이다. 도 6은 본 개시의 일 실시예에 따른 작업 유사성 기반의 다중 작업 학습 방법을 나타낸 도면이다. 도 7은 본 개시의 일 실시예에 따른 작업 유사성 기반의 다중 작업 학습 장치를 나타낸 도면이다. 도 8은 본 개시의 일 실시예에 따른 학습 데이터를 나타낸 도면이다. 도 9는 본 개시의 일 실시예에 따른 데이터 2D 시각화 결과를 나타낸 도면이다."}
