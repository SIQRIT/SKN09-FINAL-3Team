{"patent_id": "10-2021-0182517", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0093683", "출원번호": "10-2021-0182517", "발명의 명칭": "영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 장치 및 방법", "출원인": "주식회사 에스제이테크놀로지", "발명자": "성인호"}}
{"patent_id": "10-2021-0182517", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "다수의 학습용 영상 데이터들을 수집하여 저장하는 영상 수집부;다수의 원본 영상 데이터들을 입력받아 저장하는 영상 입력부;영상 유형별로 다수의 장면 학습 영상 데이터들을 저장하는 장면 학습 데이터베이스;상기 영상 유형별로 다수의 편집 학습 영상 데이터들을 저장하는 편집 학습 데이터베이스;상기 학습용 영상 데이터들 중에서 사용자에 의해 입력된 특정 영상 유형에 해당하는 복수의 학습용 영상 데이터들을 선택하고, 상기 선택된 학습용 영상 데이터들과 미리 지정된 장면 학습 파라미터와 미리 지정된 편집 학습 파라미터를 이용하여 인공지능 엔진을 학습시키는 인공지능 엔진 학습부;미리 지정된 특정 소리를 기준으로 상기 원본 영상 데이터들의 기준 시점을 동기화하는 영상 동기화부;상기 장면 학습 영상 데이터들을 이용하여 상기 동기화된 원본 영상 데이터들에서 다수의 장면들을 분석하는 장면 분석부; 및상기 인공지능 엔진을 이용하는 인공지능 프로그램에 상기 편집 학습 영상 데이터들을 적용하여 상기 분석된 장면들 중에서 복수의 장면들을 선별하고, 상기 선별된 장면들을 편집하여 편집 영상 데이터를 생성하는 편집 분석부를 포함하는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 장치."}
{"patent_id": "10-2021-0182517", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 인공지능 엔진 학습부는, 상기 선택된 학습용 영상 데이터들에 상기 장면 학습 파라미터를 적용하여 상기인공지능 엔진을 학습시키고, 상기 선택된 학습용 영상 데이터들에서 상기 장면 학습 파라미터를 충족하는 상기장면 학습 영상 데이터들을 추출하여 상기 장면 학습 데이터베이스에 저장하며, 상기 선택된 학습용 영상 데이터들에 상기 편집 학습 파라미터를 적용하여 상기 인공지능 엔진을 학습시키고, 상기 선택된 학습용 영상 데이터들 중에서 상기 편집 학습 파라미터를 충족하는 상기 편집 학습 영상 데이터들을 추출하여 상기 편집 학습 데이터베이스에 저장하는 것을 특징으로 하는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로편집하는 장치."}
{"patent_id": "10-2021-0182517", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 장면 학습 파라미터는, 화자 수, 객체 움직임, 화면 구도, 카메라 움직임 및 오디오 음절 중 적어도 하나를 포함하는 것을 특징으로 하는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 장치."}
{"patent_id": "10-2021-0182517", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 편집 학습 파라미터는, 장면 전환, 오디오 전환, 대화 인식, 액션 인식, 배경음악 위치 중 적어도 하나를포함하는 것을 특징으로 하는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 장치.공개특허 10-2023-0093683-3-청구항 5 제1항에 있어서,상기 학습용 영상 데이터들 각각은, 영상 유형별로 길이가 동일하며 미리 편집된 것을 특징으로 하는 영상 편집장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 장치."}
{"patent_id": "10-2021-0182517", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "영상 수집부가, 다수의 학습용 영상 데이터들을 수집하여 저장하는 과정,인공지능 엔진 학습부가, 상기 학습용 영상 데이터들 중에서 사용자에 의해 입력된 특정 영상 유형에 해당하는복수의 학습용 영상 데이터들을 선택하는 과정,상기 인공지능 엔진 학습부가, 상기 선택된 학습용 영상 데이터들과 미리 지정된 장면 학습 파라미터와 미리 지정된 편집 학습 파라미터를 이용하여 인공지능 엔진을 학습시키는 과정,영상 입력부가, 다수의 원본 영상 데이터들을 입력받아 저장하는 과정,영상 동기화부가, 미리 지정된 특정 소리를 기준으로 상기 원본 영상 데이터들의 기준 시점을 동기화하는 과정,장면 분석부가, 상기 장면 학습 영상 데이터들을 이용하여 상기 동기화된 원본 영상 데이터들에서 다수의 장면들을 분석하는 과정,편집 분석부가, 상기 인공지능 엔진을 이용하는 인공지능 프로그램에 상기 편집 학습 영상 데이터들을 적용하여상기 분석된 장면들 중에서 복수의 장면들을 선별하는 과정, 상기 편집 분석부가, 상기 선별된 장면들을 편집하여 편집 영상 데이터를 생성하는 과정을 포함하는 영상 편집장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 방법."}
{"patent_id": "10-2021-0182517", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 인공지능 엔진을 학습시키는 과정은, 상기 선택된 학습용 영상 데이터들에 상기 장면 학습 파라미터를 적용하여 상기 인공지능 엔진을 학습시키는 과정, 상기 선택된 학습용 영상 데이터들에서 상기 장면 학습 파라미터를 충족하는 상기 장면 학습 영상 데이터들을추출하여 장면 학습 데이터베이스에 저장하는 과정, 상기 선택된 학습용 영상 데이터들에 상기 편집 학습 파라미터를 적용하여 상기 인공지능 엔진을 학습시키는 과정, 및상기 선택된 학습용 영상 데이터들 중에서 상기 편집 학습 파라미터를 충족하는 상기 편집 학습 영상 데이터들을 추출하여 편집 학습 데이터베이스에 저장하는 과정을 포함하는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 방법."}
{"patent_id": "10-2021-0182517", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 장면 학습 파라미터는, 화자 수, 객체 움직임, 화면 구도, 카메라 움직임 및 오디오 음절 중 적어도 하나를 포함하는 것을 특징으로 하는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 방법.공개특허 10-2023-0093683-4-청구항 9 제6항에 있어서,상기 편집 학습 파라미터는, 장면 전환, 오디오 전환, 대화 인식, 액션 인식, 배경음악 위치 중 적어도 하나를포함하는 것을 특징으로 하는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 방법."}
{"patent_id": "10-2021-0182517", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제6항에 있어서,상기 학습용 영상 데이터들 각각은, 영상 유형별로 길이가 동일하며 미리 편집된 것을 특징으로 하는 영상 편집장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 방법."}
{"patent_id": "10-2021-0182517", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시 예에 따른, 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 장치 및 방법은, 다수의 학습용 영상 데이터들을 수집하여 저장하는 영상 수집부; 다수의 원본 영상 데이터들을 입력 받아 저장하는 영상 입력부; 영상 유형별로 다수의 장면 학습 영상 데이터들을 저장하는 장면 학습 (뒷면에 계속)"}
{"patent_id": "10-2021-0182517", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 영상 편집 장치에 관한 것으로, 특히, 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동 으로 편집하는 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2021-0182517", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인터넷과 네트워크의 발달로 현재 미디어 생산은 대형 방송국을 통한 매스 미디어(Mass Media) 방송뿐만 아니라 1인 미디어 방송에서도 활발하게 진행되고 있다. 이러한 미디어 방송의 확장은 영상 편집의 급속한 증가를 가져 왔다. 특히, 방송이나 SNS(social network service)와 같은 매체를 통해 영상을 전파하기 위해서는 해당 영상의 목적"}
{"patent_id": "10-2021-0182517", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "에 따라 영상을 편집해야 한다. 여기서, 영상의 목적은 광고 또는 해당 방송의 하이라이트(highlight) 요약 등 을 포함한다. 그러나 이러한 영상 편집은 사용자(예를 들면, 편집자)에 의해 일일이 수동으로 이루어지고 있기 때문에 많은 시간과 노력이 소모되는 문제점이 있었다. 예를 들면, 영상 편집이 교차 편집인 경우, 숙련된 편집자일지라도 5 시간 내지 10시간 이상의 시간을 소요하여 교차 편집으로 5분 이내의 영상을 제작할 수 있다. 여기서, 교차 편 집 영상은 편집자가 임의로 아이돌 그룹의 여러 무대 영상을 편집하여 노래와 안무의 흐름이 이어지는 하나의 영상으로 만든 2차 저작물로서, 유튜브 등의 영상 제공 플랫폼에서 큰 인기를 누리고 있다. 그리고 영상 편집에 의한 결과물의 제작 품질은 편집자의 숙련도에 따라 큰 품질 차이가 발생하는 문제점이 있 었다. 따라서, 이러한 문제점을 해결하기 위한 방안의 필요성이 대두하였다."}
{"patent_id": "10-2021-0182517", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 실시 예는 영상 편집 장치에서 짧은 시간과 적은 노력을 소모하여 영상 데이터를 자동으로 편집하 는 장치 및 방법을 제안한다. 그리고 본 발명의 일 실시 예는 영상 편집 장치에서 결과물이 일정한 품질을 가지도록 영상 데이터를 자동으로 편집하는 장치 및 방법을 제안한다."}
{"patent_id": "10-2021-0182517", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시 예에 따른, 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 장치 는, 다수의 학습용 영상 데이터들을 수집하여 저장하는 영상 수집부; 다수의 원본 영상 데이터들을 입력받아 저 장하는 영상 입력부; 영상 유형별로 다수의 장면 학습 영상 데이터들을 저장하는 장면 학습 데이터베이스; 상기 영상 유형별로 다수의 편집 학습 영상 데이터들을 저장하는 편집 학습 데이터베이스; 상기 학습용 영상 데이터들 중에서 사용자에 의해 입력된 특정 영상 유형에 해당하는 복수의 학습용 영상 데이터들을 선택하고, 상기 선 택된 학습용 영상 데이터들과 미리 지정된 장면 학습 파라미터와 미리 지정된 편집 학습 파라미터를 이용하여 인공지능 엔진을 학습시키는 인공지능 엔진 학습부; 미리 지정된 특정 소리를 기준으로 상기 원본 영상 데이터 들의 기준 시점을 동기화하는 영상 동기화부; 상기 장면 학습 영상 데이터들을 이용하여 상기 동기화된 원본 영 상 데이터들에서 다수의 장면들을 분석하는 장면 분석부; 및 상기 인공지능 엔진을 이용하는 인공지능 프로그램 에 상기 편집 학습 영상 데이터들을 적용하여 상기 분석된 장면들 중에서 복수의 장면들을 선별하고, 상기 선별 된 장면들을 편집하여 편집 영상 데이터를 생성하는 편집 분석부를 포함한다. 본 발명의 일 실시 예에 따른, 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하는 방법 은, 영상 수집부가, 다수의 학습용 영상 데이터들을 수집하여 저장하는 과정, 인공지능 엔진 학습부가, 상기 학 습용 영상 데이터들 중에서 사용자에 의해 입력된 특정 영상 유형에 해당하는 복수의 학습용 영상 데이터들을 선택하는 과정, 상기 인공지능 엔진 학습부가, 상기 선택된 학습용 영상 데이터들과 미리 지정된 장면 학습 파 라미터와 미리 지정된 편집 학습 파라미터를 이용하여 인공지능 엔진을 학습시키는 과정, 영상 입력부가, 다수 의 원본 영상 데이터들을 입력받아 저장하는 과정, 영상 동기화부가, 미리 지정된 특정 소리를 기준으로 상기 원본 영상 데이터들의 기준 시점을 동기화하는 과정, 장면 분석부가, 상기 장면 학습 영상 데이터들을 이용하여 상기 동기화된 원본 영상 데이터들에서 다수의 장면들을 분석하는 과정, 편집 분석부가, 상기 인공지능 엔진을 이용하는 인공지능 프로그램에 상기 편집 학습 영상 데이터들을 적용하여 상기 분석된 장면들 중에서 복수의 장 면들을 선별하는 과정, 상기 편집 분석부가, 상기 선별된 장면들을 편집하여 편집 영상 데이터를 생성하는 과정 을 포함한다."}
{"patent_id": "10-2021-0182517", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시 예는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하여 짧은 시간 과 적은 노력으로 영상 데이터를 편집할 수 있다. 그리고 본 발명의 일 실시 예는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하여 결 과물이 일정한 품질을 가지도록 영상 데이터를 편집할 수 있다. 그 외에 본 발명의 실시 예로 인해 얻을 수 있거나 예측되는 효과에 대해서는 본 발명의 실시 예에 대한 상세한 설명에서 직접적 또는 암시적으로 개시하도록 한다. 즉, 본 발명의 실시 예에 따라 예측되는 다양한 효과에 대 해서는 후술될 상세한 설명 내에서 개시될 것이다."}
{"patent_id": "10-2021-0182517", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 발명에 대해 구체적으로 설명하기로 한다. 본 발명의 실시 예에서 사용되는 용어는 본 발명에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당하는 발명의 설명 부 분에서 상세히 그 의미를 기재할 것이다. 따라서 본 발명에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 발명의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 발명의 실시 예들은 다양한 변환을 가할 수 있고 여러 가지 실시 예를 가질 수 있는바, 특정 실시 예들을 도 면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나 이는 특정한 실시 형태에 대해 범위를 한정하려는 것이 아니며, 발명된 사상 및 기술 범위에 포함되는 모든 변환, 균등물 내지 대체물을 포함하는 것으로 이 해되어야 한다. 실시 예들을 설명함에서 관련된 공지 기술에 대한 구체적인 설명이 요지를 흐릴 수 있다고 판단 되는 경우 그 상세한 설명을 생략한다. 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 구성요소들은 용어들에 의해 한정되 어서는 안 된다. 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또 는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 발명의 실시 예에서 '모듈' 혹은 '부'는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨 어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 '모듈' 혹은 복수의 '부'는 특정한 하드웨어로 구현될 필요가 있는 '모듈' 혹은 '부'를 제외하고는 적어도 하나의 모듈로 일체화되어 적어 도 하나의 프로세서(미도시)로 구현될 수 있다. 본 발명의 실시 예에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\"되어 있는 경우도 포함한다. 또한, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 아래에서는 첨부한 도면을 참고하여 본 발명의 실시 예에 대하여 본 발명가 속하는 기술 분야에서 통상의 지식 을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해 서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 도 1은 본 발명의 일 실시 예에 따른 영상 편집 장치의 블록 구성도이다. 도 1을 참조하면, 영상 편집 장치는 영상 수집부와 인공지능(Artificial Intelligence, 이하 'AI'라 한다) 엔진 학습부와 장면 학습 데이터베이스(DataBase, 이하 'DB'라 한다)와 편집 학습 데이터베이 스와 영상 입력부와 영상 동기화부와 장면 분석부와 편집 분석부를 포함한다. 각 구성요소를 살펴보면, 영상 수집부는 외부에서 제작된 다수의 학습용 영상 데이터들을 수집하여 저장한다. 예를 들면, 학습용 영상 데이터들은 이미 편집된 영상 데이터들이며, 학습용 영상 데이터들의 유형에 따라 분류되어 저장될 수 있다. 예를 들면, 학습용 영상 데이터들의 유형은 영상 데이터의 장르에 따라 구분될 수 있다. 예를 들면, 영상 데이터의 장르는 예능 영상 데이터, 공연 영상 데이터(예를 들면, 무대 영상 데이터 등), 리뷰 영상 데이터(예를 들면, 자동차, 부동산 또는 장난감 리뷰 영상 데이터 등), 스포츠 영상 데이터(예 를 들면, 현장 또는 중계 스포츠 영상 데이터 등), 하이라이트 영상 데이터(예를 들면, 영상, 드라마, 스포츠 (예를 들면, 득점 장면 하이라이트 등),예능 또는 다큐멘터리 하이라이트 영상 데이터 등)를 포함할 수 있다. 예를 들면, 선택된 학습용 영상 데이터들은 서로 동일한 길이를 가진 학습용 영상 데이터일 수 있다. 인공지능 엔진 학습부는 다수의 학습용 영상 데이터들 중에서 사용자에 의해 선택된 분류된 영상 데 이터들을 이용하여 인공지능 엔진을 학습한다. 예를 들면, 인공지능 엔진은 인공지능 프로그램에서 두뇌 역할을 수행하는 소프트웨어일 수 있다. 예를 들면, 인공지능 프로그램은 다수의 영상 데이터들을 편집하여 하나의 영 상 데이터를 생성하는 기능을 제공하는 소프트웨어일 수 있다. 예를 들면, 인공지능 프로그램은 ResNext-101 모 델일 수 있다. 좀 더 자세히 설명하면, 인공지능 엔진 학습부는 사용자로부터 인공지능 엔진의 학습을 위한 영상 유형을 입력받는다. 그리고 인공지능 엔진 학습부는 다수의 학습용 영상 데이터들 중에서 입력된 영상 유형에 대 응하는 복수의 학습용 영상 데이터들을 선택한다. 그리고 인공지능 엔진 학습부는 선택된 학습용 영상 데이터들에 미리 지정된 장면 학습 파라미터를 적용하 여 인공지능 엔진을 학습시킨다. 예를 들면, 장면 학습 파라미터는 선택된 학습용 영상 데이터들에서 좋은 장면이 가지는 특성을 추출하기 위한 것일 수 있다. 예를 들면, 장면 학습 파라미터는 화자 수, 객체 움직임, 화면 구도, 카메라 움직임 및 오디오 음절 중 적어도 하나를 포함할 수 있다. 예를 들면, 화자 수는 화자가 1인 인 경우, 화자 1인이 나오는 장면을 선택하는 것과 화자가 2인 이상인 경우, 복수의 화자들이 포함된 장면을 선택하는 것을 포함할 수 있다. 예를 들면, 객체 움직임은 객체가 화면에서 움 직이는 속도가 변하는 구간을 단위 장면으로 구분하는 규칙, 객체가 화면 중심에서 일정 거리 이상 멀어지는 장 면을 학습하는 것과 복수의 움직이는 객체들 중에서 움직임이 다른 객체가 존재하는 장면을 학습하는 것을 포함 할 수 있다. 예를 들면, 화면 구도는 중심 객체에 따른 화면 구도와 카메라 앵글에 따른 화면 구도를 포함할 수 있다. 예를 들면, 중심 객체에 따른 화면 구도는 인물 중심의 구도와 비인물 중심의 구도로 구분될 수 있다. 예를 들면, 인 물 중심의 구도인 경우, 화면 구도는 바스트 샷, 풀 샷 또는 클로즈업(예를 들면, 얼굴 중심)을 포함할 수 잇다. 예를 들면, 비인물 중심의 구도인 경우, 화면 구도는 객체(예를 들면, 비인물)의 포함 여부에 따라 달라 지는데, 객체를 포함하는 경우, 화면 중심에 객체가 있는 풀 샷을 우선하는 것을 포함할 수 있다. 예를 들면, 카메라 앵글에 따른 화면 구도는 특이한 앵글(예를 들면, 부감 도는 조감) 여부에 따라 장면을 선택하는 것일 수 있다. 예를 들면, 카메라 움직임은 패닝(Panning) 여부와 틸팅(Tilting) 여부와 줌(Zoom) 여부를 포함할 수 있다. 예 를 들면, 패닝 여부는 카메라 각도가 좌 또는 우로 5도 이상 변화할 때, 미리 지정된 카메라 패닝 적정 시간을 고려하여 패인 시작 시점과 끝 시점에 대한 판단 기준을 결정하는 것일 수 있다. 예를 들면, 틸팅 여부는 카메 라가 각도가 상 또는 하로 2도 이상 변화할 때, 미리 지정된 카메라 틸팅 적정 시간을 고려하여 틸팅 시작 시점 과 끝 시점에 대한 판단 기준을 결정하는 것일 수 있다. 예를 들면, 줌 여부는 줌 대상 객체의 크기가 커지는 변화 정도를 학습하거나 줌 대상 객체의 크기가 작아지는 변화 정도를 학습하는 것일 수 있다. 예를 오디오 음절은 오디오 음 크기 변화, 음절이 묶음이 되는 경우, 화자가 특정 문장을 말하는 장면을 포함할 수 있다. 예를 들면, 오디오 음 크기 변화는 오디오 음 크기가 변화하거나 묵음 시간이 미리 지정된 시간 이상 인 경우를 포함할 수 있다. 예를 들면, 특정 문장은 \"한번 보실까요?\"와 \"살펴보겠습니다.\" 중 적어도 하나를 포함할 수 있다. 그리고 인공지능 엔진 학습부는 선택된 학습용 영상 데이터들 중에서 장면 학습 파라미터를 충족하는, 적 어도 하나의 장면에 해당하는 적어도 하나의 영상 데이터(이하, '장면 학습 영상 데이터'라 한다)를 생성한다. 그리고 인공지능 엔진 학습부는 생성된 장면 학습 영상 데이터를 장면 학습 데이터베이스에 저장한다. 그리고 인공지능 엔진 학습부는 선택된 학습용 영상 데이터들에 미리 지정된 편집 학습 파라미터를 적용하 여 인공지능 엔진을 학습시킨다. 예를 들면, 편집 학습 파라미터는 다수의 장면들 중에서 편집 학습 파라미터에 충족되는 복수의 장면들을 선택하고, 선택된 장면들을 편집하기 위한 것일 수 있다. 예를 들면, 편집 학습 파라미터는 장면 전환, 오디오 전환, 대화 인식, 액션 인식, 배경음악 위치 중 적어도 하 나를 포함할 수 있다. 예를 들면, 장면 전환은 이전 장면과 다음 장면의 화면 복잡도, 화면 내 움직임 정도, 화면 내 조명의 변화, 화 면 배색의 변화, 화면 내 시간대의 변화(예를 들면, 밤 또는 낮 정도), 객체(인물 또는 대상)의 움직임과 화면 내 크기 변화 및 화면 전환 기법(예를 들면, 페이드, 와이핑, 디졸브)을 포함할 수 있다. 예를 들면, 대화 인식 은 인물과 오디오 간의 매칭을 사전에 학습하는 것과 화면의 대상 중에서 화자를 인식하는 것을 포함할 수 있다. 예를 들면, 액션 인식은 화면 내에서 튀는 동작을 감지하는 것과 표정을 인식하는 것을 포함할 수 있다. 화면 내에서 튀는 동작을 감지하는 것은 연속되는 장면에서 구분이 가능한 수준의 동작 변화를 구분하는 것일 수 있 다. 표정을 인식하는 것은 웃음, 울음, 슬픔, 기쁨 등 대상 중 화자의 표정을 인식하여 구별하는 것일 수 있다. 예를 들면, 배경음악 위치는 배경음악이 삽입된 구간을 구별하는 것과 배경음악이 삽입된 구간에서 객체의 움직 임 변화를 구별하는 것과 배경음악이 삽입된 구간에서 화면의 역동성과 흐름의 차이 중 적어도 하나를 구별하는 것을 포함할 수 있다. 그리고 인공지능 엔진 학습부는 선택된 학습용 영상 데이터들 중에서 편집 학습 파라미터를 충족하는, 적 어도 하나의 편집점에 해당하는 적어도 하나의 영상 데이터(이하, '편집 학습 영상 데이터'라 한다)를 생성한다. 그리고 인공지능 엔진 학습부는 생성된 편집 학습 영상 데이터를 편집 학습 데이터베이스 에 저장한다. 장면 학습 데이터베이스는 적어도 하나의 장면 학습 영상 데이터를 저장한다. 예를 들면, 장면 학습 데이 터베이스는 영상 데이터의 유형별로 다수의 장면 학습 영상 데이터들을 저장할 수 있다. 편집 학습 데이터베이스는 적어도 하나의 편집 학습 영상 데이터를 저장한다. 예를 들면, 편집 학습 데이 터베이스는 영상 데이터의 유형별로 다수의 편집 학습 영상 데이터들을 저장할 수 있다. 영상 입력부는 사용자로부터 다수의 원본 영상 데이터들을 입력받아 저장한다. 예를 들면, 원본 영상 데이터들은 다수의 영상 촬영 카메라들에 의해 촬영된 다수의 영상 데이터들일 수 있다. 이때, 영상 입력부 는 사용자로부터 다수의 원본 영상 데이터들의 유형을 입력받을 수 있다. 영상 동기화부는 다수의 원본 영상 데이터들의 기준 시점을 동기화한다. 예를 들면, 영상 동기화부 는, 도 2에 도시된 바와 같이, 원본 영상 데이터들에 포함된 미리 지정된 소리를 기준 시점으로 지정 하고, 지정된 기준 시점의 특정 프레임(예를 들면, xxxx년 yy월 zz일 aa시 bb분 cc초 dd프레임)들을 시작 세로 시간 부호(Longitudinal Time Code, 이하 'LTC'라 한다)들로 설정하여 음향 동기화를 수행할 수 있다. 예를 들 면, 동기화 소리는 박수 소리일 수 있다. 예를 들면, 영상 데이터의 한 장면은 30 프레임으로 구성될 수 있다. 이때, 영상 동기화부는 마스터 레코딩 트랙(Master Recording Track)을 별도로 사용하거나 다수의 영상 촬 영 카메라들 중에서 특정 영상 촬영 카메라의 오디오 트랙을 마스터 오디오 트랙으로 사용할 수 있다. 그리고 영상 동기화부는 원본 영상 데이터들의 시작 세로 시간 부호들을 동일 시점으로 원본 영상 데이터들 의 프레임들(예를 들면, xxxx년 yy월 zz일 aa시 bb분 cc초 dd프레임)을 지정하여 비디오 동기화를 수행할 수 있 다. 그리고 영상 동기화부는 오디오 트랙 시작 전에는 묵음 구간을 제거하고, 오디오 트랙 종료 후에는 프 레임 마진을 부여할 수 있다. 장면 분석부는 미리 학습된 모델(pre-trained model)을 이용하여 다수의 동기화된 원본 영상 데이터들에서 다수의 장면들을 분석한다. 예를 들면, 미리 학습된 모델은 사용자가 풀고자 하는 문제와 비슷하면서 사이즈가 큰 데이터를 이용하여 미리 학습된 모델을 나타낼 수 있다. 예를 들면, 미리 학습된 모델은 장면 학습 영상 데 이터들일 수 있다. 좀 더 자세히 설명하면, 장면 분석부는 영상 동기화부로부터 다수의 동기화된 원본 영상 데이터들을 입력받는다. 그리고 장면 분석부는 미리 학습된 모델에 동기화된 원본 영상 데이터들을 통과시켜 동기화된 원본 영상 데이터들에서 다수의 장면들에 대응하는 다수의 특징들을 추출한다. 예를 들면, 미리 학습된 모델은 다수의 원본 영상 데이터들과 동일한 유형을 가진 다수의 장면 학습 영상 데이터들일 수 있다. 그리고 장면 분석부는 다수의 장면들에 대응하는 추출된 특징들 각각에 컨캣(concat) 과정을 수행하여 추 출된 특징들을 다수의 데이터 값들로 변환한다. 예를 들면, 컨캣 과정은 추출된 특징에 대한 문자열과 문자열을 결합하여 특정 데이터 값을 생성하는 과정일 수 있다. 그리고 장면 분석부는 다수의 장면들에 대응하는 변 환된 데이터 값들을 편집 분석부로 출력한다. 편집 분석부는 인공지능 프로그램에 변환된 데이터 값들을 적용하여 다수의 장면들 중에서 복수의 장면들 을 선별한다. 그리고 편집 분석부는 다수의 편집 학습 영상 데이터들을 기반으로 선별된 장면들을 자동으 로 편집하여 하나의 편집 영상 데이터와 편집 정보를 생성한다. 이후에, 편집 분석부는 편집 영 상 데이터와 편집 정보를 데이터베이스(미도시)에 저장하거나 표시부(미도시)를 통해 출력한다. 예를 들면, 편집 정보는 장면에 대한 시작과 끝 시점의 시간 정보, 편집 순서 정보 및 편집 추천 정확도 정보를 포함할 수 있다. 좀 더 자세히 설명하면, 편집 분석부는 변환된 데이터 값들을 인공지능 엔진 모델에 입력한다. 예를 들면, 인공지능 엔진 모델은 출원인에 의해 개발된 인공지능 알고리즘을 적용하여 구성된 학습 엔진일 수 있다. 그리 고 편집 분석부는 유형별 정답 비디오 클래스의 각 파라미터(예를 들면 화자수, 객체의 움직임 크 기, 카메라 앵글변화의 크기, 오디오 레벨 변화 등)을 측정하여 그 결과치를 편집 추천 정확도 정보로 생성하여 저장한다. 그리고 편집 분석부는 각 입력 영상 장면에 대한 파라미터 값이 측정된 실측자 료와 크로스-엔트로피 손실값을 산출한다. 예를 들면, 실측자료 값은 각 장면이 가지는 파라미터별 특성 값을 나타내고, 크로스엔트로피 손실은 편집 추천 정확도를 나타낼 수 있다. 예를 들면, 학습 내용은 장면의 채택 여부와 장면 구성 길이와 편집점(예를 들면, 장면 전환)을 포함할 수 있다. 예를 들면, 편집이 특정 아이돌 그룹이 출연한 음악 방송의 교차 편집인 경우, 편집점은 특정 아이돌 그 룹이 부르는 노래의 소절별로 구분하는 방식을 사용할 수 있다. 예를 들면, 학습 내용은 동작 인식으로 이루어질 수 있다. 예를 들면, 동작 인식은 인물의 말하는 동작, 움직임 인식(예를 들면, 표정, 동작, 움직임, 춤 등), 단위 장면의 시작 및/또는 끝을 인식, 목소리와 인물 간의 매칭 (단기 학습을 통해 이루어짐)을 포함할 수 있다. 예를 들면, 편집 분석부는 미리 지정된 편집 규칙을 이용하여 편집을 수행할 수 있다. 예를 들면, 미리 지 정된 편집 규칙은 영상 촬영 카메라의 대수에 따라 달라질 수 있다. 예를 들면, 특정 아이돌 그룹이 출연하는 정규 방송 프로그램의 영상을 제작하는 경우, 전체 카메라와 아이돌 그룹의 멤버별로 카메라가 정규 방송에 배정되므로, 영상 촬영 카메라의 대수는 4대 이상일 수 있다. 이러한 경 우, 편집 규칙은 전체 카메라에서 촬영된 전체 샷 우선(시작 및/또는 끝에 적용)하기, 한 장면에 두 사람 이상 의 목소리가 존재하는 경우, 전체 샷 사용하기, 및 한 장면에 한 사람의 목소리가 존재하는 경우, 해당 인물을 인식하여 해당 인물의 카메라에서 촬영된 전용 샷을 지적하여 사용하기를 포함할 수 있다. 예를 들면, 편집 분 석부는 이러한 편집 규칙을 이용하여 동기화된 원본 영상 데이터들을 편집함으로써, 도 3의 301 그래프와 같이, 편집 영상 데이터의 시작 부분과 끝 부분을 전체 샷으로 구성하고, 나머지 중간 부분을 멤버별 샷과 우측 샷으로 구성할 수 있다. 다른 예로, SNS에서 주로 활동하는 개인 또는 소규모 집단에서 영상을 제작하는 경우, 영상 촬영 카메라의 대수 는 3대 이하 일 수 있다. 이러한 경우, 편집 규칙은 침묵 컷(Silence cut)(+/- frame margin) 배제하기, LTC (절대값)를 기준으로 편집 시퀀스(sequence) 정리하기(다수의 원본 영상 데이터에서 화면을 가져다가 사용할 수 있음), 자동 자막 인식하기(타임 구간에 맞춰서), 마스터 오디오 지정하기(예를 들면, 메인 크리에이터 샷(Main Creator Shot)을 촬영하는 카메라의 오디오가 마스터 오디오로 지정될 수 있음), 편집 영상 데이터에 타이틀 클 립(Title Clip)과 엔딩 클립(Ending Clip)을 자동으로 삽입하기를 포함할 수 있다. 예를 들면, 편집 분석부 는 이러한 편집 규칙을 이용하여 동기화된 원본 영상 데이터들을 편집함으로써, 도 3의 303 그래프와 같이, 편집 영상 데이터의 시작 부분과 끝 부분을 메인 크리에이터 샷으로 구성하고, 나머지 중간 부분을 오브 젝트 줌 샷과 우측 샷으로 구성할 수 있다. 이때, 편집 분석부는 비선형 편집 시스템(Non-Linear Editing system, 이하 'NLE'라 한다) 등을 활용하여 추가 편집을 진행할 경우를 대비하여 편집 영상 데이터에 대한 XML(eXtensible Markup Language) 출력 기능을 제공한다. 이러한 구성을 통해, 본 발명의 일 실시 예는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하여 짧은 시간과 적은 노력으로 영상 데이터를 편집할 수 있다. 그리고 본 발명의 일 실시 예는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하여 결과물이 일정한 품질을 가지도록 영상 데이터 를 편집할 수 있다. 도 4는 본 발명의 일 실시 예에 따른 영상 편집 장치에서 영상 데이터를 자동으로 편집하는 흐름도이다. 도 4를 참조하면, 영상 편집 장치의 인공지능 엔진 학습부는 영상 수집부에서 수집된 다수의 학 습용 영상 데이터들을 이용하여 인공지능 엔진을 학습시킨다. 이제부터, 도 5를 참조하여, 인공지능 엔진을 학 습시키는 과정을 자세히 설명하고자 한다. 도 5는 본 발명의 일 실시 예에 따른 인공지능 엔진 학습부에서 인공지능 엔진을 학습하는 흐름도이다. 도 5를 참조하면, 인공지능 엔진 학습부는, 501 단계에서, 인공지능 엔진의 학습을 위한 영상 유형이 사용 자에 의해 입력되는지 여부를 확인한다. 예를 들면, 인공지능 엔진 학습부는 입출력부(미도시)를 통해 사 용자로부터 영상 유형을 입력받을 수 있다. 확인 결과, 영상 유형이 입력되면, 인공지능 엔진 학습부는 503 단계로 진행하고, 그렇지 않으면, 501 단 계를 반복적으로 수행한다. 503 단계에서, 인공지능 엔진 학습부는 영상 수집부에서 수집된 다수의 학습용 영상 데이터들 중에서 입력된 영상 유형에 대응하는 복수의 학습용 영상 데이터들을 선택한다. 505 단계에서, 인공지능 엔진 학습부는 선택된 학습용 영상 데이터들에 미리 지정된 장면 학습 파라미터를 적용하여 인공지능 엔진을 학습시킨다. 이때, 인공지능 엔진 학습부는 선택된 학습용 영상 데이터들 중에 서 장면 학습 파라미터를 충족하는, 적어도 하나의 장면에 대응하는 적어도 하나의 영상 데이터를 추출하고, 추 출된 영상 데이터를 적어도 하나의 장면 학습 영상 데이터로 생성한다. 그리고 인공지능 엔진 학습부는 생 성된 장면 학습 영상 데이터를 장면 학습 데이터베이스에 저장한다. 507 단계에서, 인공지능 엔진 학습부는 선택된 학습용 영상 데이터들에 미리 지정된 편집 학습 파라미터를 적용하여 인공지능 엔진을 학습시킨다. 이때, 인공지능 엔진 학습부는 선택된 학습용 영상 데이터들 중에 서 편집 학습 파라미터를 충족하는, 적어도 하나의 편집점에 대응하는 적어도 하나의 영상 데이터를 추출하고, 추출된 영상 데이터를 적어도 하나의 편집 학습 영상 데이터로 생성한다. 그리고 인공지능 엔진 학습부는 생성된 편집 학습 영상 데이터를 편집 학습 데이터베이스에 저장한다. 다시 도 4로 돌아와서, 영상 편집 장치의 영상 동기화부는, 403 단계에서, 영상 입력부로부터 입력된 다수의 원본 영상 데이터들의 기준 시점을 동기화한다. 예를 들면, 영상 동기화부는, 도 2에 도시된 바와 같이, 원본 영상 데이터들에 포함된 미리 지정된 소리를 기준 시점으로 지정하고, 지정된 기준 시점의 특정 프레임(예를 들면, xxxx년 yy월 zz일 aa시 bb분 cc초 dd프레임)들을 시작 세로 시간 부호(LTC)들로 설정하여 음향 동기화를 수행할 수 있다. 그리고 영상 동기화부 는 원본 영상 데이터들의 시작 세로 시간 부호들을 동일 시점으로 원본 영상 데이터들의 프레임들(예 를 들면, xxxx년 yy월 zz일 aa시 bb분 cc초 dd프레임)을 지정하여 비디오 동기화를 수행할 수 있다. 405 단계에서, 영상 편집 장치의 장면 분석부는 미리 학습된 모델을 이용하여 다수의 동기화된 원본 영상 데이터들에서 다수의 장면들을 분석한다. 407 단계에서, 영상 편집 장치의 편집 분석부는 인공지능 프로그램을 이용하여 다수의 장면들 중에서 복수의 장면들을 선별한다. 그리고 편집 분석부는 다수의 편집 학습 영상 데이터들을 기반으로 선별된 장 면들을 자동으로 편집하여 하나의 편집 영상 데이터와 편집 정보를 생성한다. 이러한 과정을 통해, 본 발명의 일 실시 예는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하여 짧은 시간과 적은 노력으로 영상 데이터를 편집할 수 있다. 그리고 본 발명의 일 실시 예는 영상 편집 장치에서 인공지능을 이용하여 영상 데이터를 자동으로 편집하여 결과물이 일정한 품질을 가지도록 영상 데이터 를 편집할 수 있다. 이상에서는 본 발명의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 발명은 상술한 특정의 실시 예에"}
{"patent_id": "10-2021-0182517", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 발명이 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안 될 것이다."}
{"patent_id": "10-2021-0182517", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시 예에 따른 영상 편집 장치의 블록 구성도이다. 도 2는 본 발명의 일 실시 예에 따른 영상 동기화부에서 다수의 영상 데이터들의 기준 시점을 동기화하는 동작 을 도시한 도면이다. 도 3은 본 발명의 일 실시 예에 따른 편집 분석부에서 다수의 영상 데이터들을 편집하는 동작을 도시한 도면이 다. 도 4는 본 발명의 일 실시 예에 따른 영상 편집 장치에서 영상 데이터를 자동으로 편집하는 흐름도이다. 도 5는 본 발명의 일 실시 예에 따른 인공지능 엔진 학습부에서 인공지능 엔진을 학습하는 흐름도이다."}
