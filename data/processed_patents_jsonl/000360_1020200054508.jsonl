{"patent_id": "10-2020-0054508", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0136365", "출원번호": "10-2020-0054508", "발명의 명칭": "액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법", "출원인": "(주)밸류파인더스", "발명자": "이주홍"}}
{"patent_id": "10-2020-0054508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법으로서,강화학습 시스템이 포트폴리오 자산 배분을 위한 금융 시계열을 정의하는 단계;상기 포트폴리오 자산 배분을 위한 금융 시계열에 따라 액터 네트워크의 합성곱 신경망(CNN)이 과거 수익률을입력받아 특징을 검출하여 상기 액터 네트워크의 순환 신경망 메모리(LSTM)로 출력하는 단계;크리틱 네트워크의 에이전트가 상태와 행동를 입력받아 로 정의된 q-value를 추정하는단계; 및상기 q-value에 따라 평균 제곱 오차를 최소화하여 상기 크리틱 네트워크를 학습하는 단계를 포함하는 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법."}
{"patent_id": "10-2020-0054508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 액터 네트워크의 순환 신경망 메모리(LSTM)로 출력하는 단계에서,상기 액터 네트워크는 상태(state)를 특정 행동(action)에 결정적으로 대응하는 합성곱 신경망(CNN)을 가진 순환 신경망 메모리(LSTM)를 사용하고, 훈련 알고리즘은 DPG(Deterministic Policy Gradient) 방법을 사용하는것을 특징으로 하는 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법."}
{"patent_id": "10-2020-0054508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서,상기 DPG(Deterministic Policy Gradient) 방법은 하기의 수학식여기서, 는 critic 네트워크의 출력, 는 학습률으로 정의되는 것을 특징으로 하는 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법."}
{"patent_id": "10-2020-0054508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 3에 있어서,상기 액터 네트워크는 과 조건 을 만족하기 위해 하기의 수학식여기서, 은 LSTM의 출력공개특허 10-2021-0136365-3-으로 정의되는 소프트맥스(softmax) 함수를 사용하는 것을 특징으로 하는 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법."}
{"patent_id": "10-2020-0054508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 1에 있어서,상기 q-value를 추정하는 단계에서, 타겟 q-value는 시간 에서 하기의 수학식여기서, 는 즉각적인 보상(샤프지수), 는 타겟 critic 네트워크의 출력으로 정의되는 것을 특징으로 하는 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법."}
{"patent_id": "10-2020-0054508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 1에 있어서,상기 크리틱 네트워크를 학습하는 단계에서의 학습은 하기의 수학식으로 정의되는 것을 특징으로 하는 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법."}
{"patent_id": "10-2020-0054508", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서,그래디언트(Gradient)를 사용하여 상기 크리틱 네트워크의 매개변수()를 하기의 수학식여기서, 는 학습률에 따라 업데이트하는 것을 특징으로 하는 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법."}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법에 관한 것으로, 보다 상세하게는 DPG(Deterministic Policy Gradient) 알고리즘 기반의 액터 크리틱(actor-critic) 모델을 이용한 포트폴리오 자 산배분 강화학습방법에 관한 것이다. 본 발명에 의하면, 자산 배분을 위한 DPG(Deterministic Policy Gradient) 알고리즘 기반의 액터 크리틱(actor-critic) 모델을 이용함으로써, 최적화된 포트폴리오를 찾아줄 수 있는 효과 가 있다."}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법에 관한 것으로, 보다 상세하게는 DPG(Deterministic Policy Gradient) 알고리즘 기반의 액터 크리틱(actor-critic) 모델을 이용한 포트폴리오 자산배분 강화학습방법에 관한 것이다."}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 들어, 인공지능 기술은 비약적으로 발전되고 있고, 다양한 분야에 적용되어 뛰어난 성과를 거두고 있다. 금융 분야에서도 인공지능을 적용한 산업이 빠르게 발전하고 있는데, 인공지능이 학습한 알고리즘을 이용해 투 자조언, 투자결정 및 자산운용을 할 수 있게 되었다. 인공지능이 적용되는 금융 분야의 세부적인 영역으로는 포트폴리오 최적화, 신용등급 평가, 주식투자, 자산예측 등이 있다. 그중 포트폴리오 최적화는 투자의 안정성 확보와 수익 창출이라는 목표를 위해 중요한 의사결정이필요하다. 기존의 포트폴리오 알고리즘으로는 Markowitz의 Mean-Variance 모델, 선형계획법, 비선형계획법 등이 있고, 인 공지능을 활용한 방법으로는 인공신경망, 강화학습 등의 방법들이 있는데, 그 중 회귀 강화학습 방법이 최근에 많은 관심을 받고 활발히 연구되어 오고 있다. 그러나 회귀 강화학습에 관한 기존의 연구들은 자산들의 과거데이터만 사용하기 때문에 포트폴리오의 성능향상 에 도움을 줄 수 있는 다른 요소들에 대한 적용이 부족하다. 또한, 기존의 균등배분, Markowitz, 순환형 강화학습(Recurrent Reinforcement Learning) 방법들은 수익률을 최대화하거나 위험을 최소화하고, 위험 배분(Risk Budgeting) 방법은 각 자산에 목표 리스크를 배분하여 최적의 포트폴리오를 찾는다. 그러나, 이 방법들은 기대 수익률이나 샤프지수 같은 가치(value) 함수에 따라 달라지게 되어 미래의 최적화된 포트폴리오를 잘 찾아주지 못하는 문제점이 있었다. 선행기술문헌 특허문헌 (특허문헌 0001) KR10-2017-0012702A"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 이와 같은 문제점을 해결하기 위해 창안된 것으로서, 자산 배분을 위한 DPG(Deterministic Policy Gradient) 알고리즘 기반의 액터 크리틱(actor-critic) 모델을 이용하여 미래의 최적화된 포트폴리오를 찾아줄 수 있도록 한 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법을 제공함을 목적으로 한다."}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명에 따른 액터 크리틱 모델을 이용한 포트폴리오 자산배분 강화학습방법의 일측면에 따르면, 강화학습 시스템이 포트폴리오 자산 배분을 위한 금융 시계열을 정의하는 단계; 상기 포트폴 리오 자산 배분을 위한 금융 시계열에 따라 액터 네트워크의 합성곱 신경망(CNN)이 과거 수익률을 입력받아 특 징을 검출하여 상기 액터 네트워크의 순환 신경망 메모리(LSTM)로 출력하는 단계; 크리틱 네트워크의 에이전트 가 상태 와 행동 를 입력받아 로 정의된 q-value를 추정하는 단계; 및 상기 q-value에 따 라 평균 제곱 오차를 최소화하여 상기 크리틱 네트워크를 학습하는 단계를 포함할 수 있다. 또한, 상기 액터 네트워크의 순환 신경망 메모리(LSTM)로 출력하는 단계에서, 상기 액터 네트워크는 상태 (state)를 특정 행동(action)에 결정적으로 대응하는 합성곱 신경망(CNN)을 가진 순환 신경망 메모리(LSTM)를 사용하고, 훈련 알고리즘은 DPG(Deterministic Policy Gradient) 방법을 사용할 수 있다. 또한, 상기 DPG(Deterministic Policy Gradient) 방법은 하기의 수학식으로 정의될 수 있다. 여기서, 는 critic 네트워크의 출력, 는 학습률 또한, 상기 액터 네트워크는 과 조건 을 만족하기 위해 하기의 수학식으로 정의되는 소프트맥스 (softmax) 함수를 사용할 수 있다. 여기서, 은 LSTM의 출력 또한, 상기 q-value를 추정하는 단계에서, 타겟 q-value는 시간 에서 하기의 수학식으로 정의될 수 있다. 여기서, 는 즉각적인 보상(샤프지수), 는 타 겟 critic 네트워크의 출력 또한, 상기 크리틱 네트워크를 학습하는 단계에서의 학습은 하기의 수학식으로 정의될 수 있다."}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "또한, 그래디언트(Gradient)를 사용하여 상기 크리틱 네트워크의 매개변수( )를 하기의 수학식에 따라 업데이 트할 수 있다. 여기서, 는 학습률"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의하면, 자산 배분을 위한 DPG(Deterministic Policy Gradient) 알고리즘 기반의 액터 크리틱 (actor-critic) 모델을 이용함으로써, 최적화된 포트폴리오를 찾아줄 수 있는 효과가 있다."}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 첨부된 도면을 참조로 본 발명의 바람직한 실시예를 상세히 설명하기로한다. 이에 앞서, 본 명세서 및 청 구범위에 사용된 용어나 단어는 통상적이거나 사전적인 의미로 한정해서 해석되어서는 아니되며, 발명자는 그 자신의 발명을 가장 최선의 방법으로 설명하기 위해 용어의 개념을 적절하게 정의할 수 있다는 원칙에 입각하여 본 발명의 기술적사상에 부합하는 의미와 개념으로 해석되어야만 한다. 따라서, 본 명세서에 기재된 실시예와 도면에 도시된 구성은 본 발명의 가장 바람직한 일실시예에 불과할 뿐이고 본 발명의 기술적 사상을 모두 대변 하는 것은 아니므로, 본 출원시점에 있어서 이들을 대체할 수 있는 다양한 균등물과 변형예들이 있을 수 있음을 이해하여야 한다. 도 1은 액터 크리틱 모델(actor critic model)의 프레임워크를 나타내는 도면이다. 도시된 바와 같이, 액터 크리틱 모델(actor critic model)의 일반적인 프레임워크는 액터 네트워크(actor network)와 크리틱 네트워크(critic network) 및 환경(environment)으로 구성될 수 있다. 즉, 액터 크리틱 알고리즘(actor critic algorithms)은 액터 네트워크(actor network)와 크리틱 네트워크 (critic network)라는 2개의 네트워크를 사용할 수 있다. 또한, 액터 네트워크(actor network)는 상태(state)가 주어졌을 때 행동(action)을 결정하고, 크리틱 네트워크 (critic network)는 상태(state)의 가치를 평가할 수 있으며 특정 상태에서 이득이 되는 방향으로 학습하기 위 한 Q값을 액터 네트워크(actor network)로 전달할 수 있다. 도 2는 언폴디드 순환 신경망(unfolded Recurrent Neural Networks, 이하, 'RNNs'라 칭함)의 프레임워크를 나 타내는 도면이다. 도시된 바와 같이, Recurrent Neural Networks(RNNs)의 아키텍처(architecture)는 기본적인 신경망(Neural Networks) 구조에 이전 시간(t-1)의 Hidden Layer의 출력(output)을 다음 시간(t)에 Hidden Layer로 다시 집어 넣는 경로가 추가된 형태이다. 이 구조는 \"recurrent-반복되는-\"라는 단어에서 알 수 있듯이, 현재 시간 t의 결 과가 다음 시간 t+1에 영향을 미치고, 이는 다시 다음 시간 t+2에 영향을 미치는 과정이 끊임없이 반복되는 neural network 형태이다. 특히, 도 2에서와 같이 펼친(Unfold) 형태의 RNNs에서 는 hidden state를 의미하고, 입력 는 출력 를 구하는 데 이용될 수 있으며, 하기의 수학식 1과 같이 정의될 수 있다. 수학식 1"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, 와 는 hyperbolic tangent, sigmoid 등의 활성화 함수(activation functions) 도 3은 합성곱 신경망(Convolutional Neural Network, 이하, 'CNN'이라 칭함)의 일예를 나타내는 도면이다. 도시된 바와 같이, CNN은 이미지의 특징을 추출하는 부분과 클래스를 분류하는 부분으로 구분할 수 있다. 특징 추출 영역은 Convolution Layer와 Pooling Layer를 여러 겹 쌓는 형태로 구성될 수 있으며, Convolution Layer는 입력 데이터에 필터를 적용 후 활성화 함수를 반영하는 필수 요소이다. Convolution Layer 다음에 위치하는 Pooling Layer는 선택적인 레이어이며, CNN 마지막 부분에는 이미지 분류를 위한 Fully Connected 레이어가 추가될 수 있다. 이미지의 특징을 추출하는 부분과 이미지를 분류하는 부분 사 이에 이미지 형태의 데이터를 배열 형태로 만드는 Flatten 레이어가 위치할 수 있다. CNN은 이미지 특징 추출을 위하여 입력데이터를 필터가 순회하며 합성곱을 계산하고, 그 계산 결과를 이용하여 Feature map을 만들 수 있다. Convolution Layer는 Filter 크기, Stride, Padding 적용 여부, Max Pooling 크 기에 따라서 출력 데이터의 형태(shape)가 변경될 수 있다. 한편, 포트폴리오 자산 배분이란 개인의 목표를 달성하기 위한 위험과 수익률의 적절한 균형을 맞추어서 자산을 배분하는 투자전략을 말한다. 자산의 예로는 주식, 채권, 상품 및 현금 등이 있다. 포트폴리오 이론은 마코위츠(Markowitz)에 의해서 체계화되었으며, 마코위츠 모델은 공분산 행렬 형태의 위험을 최소화하면서 포트폴리오의 기대 수익률을 최대화하는 평균-분산 최적화 방법이다. 위험 배분(Risk budgeting)은 자본이 아닌 포트폴리오의 위험에 따라 자산을 할당하는 방법으로서, 이 방법은 포트폴리오 매니저가 일련의 위험 예산(Risk Budget)을 정의한 다음 포트폴리오의 가중치를 계산할 수 있다. 균등 배분(Equally weighted)은 포트폴리오 각 자산에 동일한 가중치를 부여하는 가중치 방법이다. 일반적으로 포트폴리오를 할당한다는 것은 투자 매니저의 의사결정 프로세스를 의미할 수 있으며, 강화학습은 순차적인 의 사결정 작업을 학습하는 일반적인 프레임워크다. 또한, TD(Temporal Difference) 방법으로 매개변수를 조정하여 시스템의 행동(action)에 따른 기대보상 (expected reward)을 최대화할 수 있다. 자산 배분에서 포트폴리오 가중치는 강화학습으로 정의될 수 있으며, RRL(Recurrent Reinforcement Learning) 알고리즘을 사용하여 샤프지수(Sharpe ratio)가 최대가 되도록 할 수 있다. RRL이 포함된 자산 배분 시스템은 행동(action)의 가치(value)를 학습한 후, 측정된 행동들의 가치(value)를 기 반으로 행동(action)을 선택할 수 있다. 이러한 방법은 기대 수익률이나 샤프지수 같은 가치(value) 함수에 따라 달라지게 되는 문제점이 있는 바, 하기 에서는 이러한 문제점을 해결하기 위하여 DPG(Deterministic Policy Gradient) 알고리즘 기반의 액터 크리틱(actor-critic) 모델에 대해 구체적으로 설명하기로 한다. 먼저, 포트폴리오 자산 배분을 위한 금융 시계열을 정의할 수 있다. 자산 가격 행렬은 하기의 수학식 2에서와 같이 m 개의 자산의 t 날 자산의 가격을 열 단위로 나타낼 수 있다. 수학식 2"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "다른 방법과 마찬가지로 강화학습 시스템의 상태(state) 입력에 대한 과거의 자산 수익률을 데이터를 사용할 수 있다. 수학식 3"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "상기 수학식 3에서와 같이 시간 에서 번째 자산의 수익률은 하기의 수학식 4로 정의할 수 있다. 수학식 4"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이에 따라, 자산 수익률 행렬을 하기의 수학식 5로 정의할 수 있다. 수학식 5"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "강화학습 에이전트의 행동은 포트폴리오 가중치 에 의해 정의될 수 있고, 트레이더(trade r)는 매수 만 사용할 수 있다. 또한, 본 발명에서는 샤프지수를 보상(reward)으로 사용할 수 있다. 도 4는 본 발명의 일실시예에 따른 포트폴리오 자산 배분을 위한 액터 네트워크(actor network) 구성을 나타내 는 도면이고, 도 5는 본 발명의 일실시예에 따른 포트폴리오 자산 배분을 위한 크리틱 네트워크(critic network) 구성을 나타내는 도면이다. 도시된 바와 같이, 포트폴리오 자산 배분을 위한 액터 크리틱 모델(Actor critic model)에 있어 먼저, 전술한 합성곱신경망(CNN)을 가진 LSTM(Long Short-Term Memory models) 액터 네트워크(actor network)로 구성될 수 있다. 즉, 도 4에서와 같이 액터 네트워크(actor network)는 상태(state)를 특정 행동(action)에 결정적으로 대 응하는 CNN을 가진 LSTM를 사용할 수 있고, 훈련 알고리즘은 DPG(Deterministic Policy Gradient) 방법을 사용할 수 있다. 수학식 6"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "상기의 수학식 6에서 는 크리틱 네트워크(critic network)의 출력이고, 는 학습률이다. 액터 네트워크(actor network)가 과 조건 을 만족하기 위해 하기의 수학식 7과 같이 정의 되는 소프트맥스(softmax) 함수를 사용할 수 있다. 수학식 7"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, 은 LSTM의 출력이다. CNN은 과거 수익률을 입력으로 사용할 수 있고, 중요한 특징(important feature)을 찾아내서 출력할 수 있다. 출력값은 LSTM의 입력으로 사용될 수 있다. 또한, 을 타겟 critic 네트워크로 사용하고, 매개변수 와 experience replay로 크리틱 네트워크(critic network)를 학습할 수 있다. 즉, 도 5에서와 같이 크리틱 네트워크(critic network)의 에이전트는 상태 와 행동 를 입력으로 사 용하여 로 정의된 q-value를 추정할 수 있다. 타겟 q-value는 시간 에서 하기의 수학식 8과 같 이 정의될 수 있다. 수학식 8"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기서, 는 즉각적인 보상(샤프지수), 는 타겟 critic 네트워크의 출력일 수 있다. 또한, 평균 제곱 오차(Mean Square Error)를 최소화하여 critic 네트워크를 학습할 수 있으며, 하기의 수학식 9 와 같이 정의될 수 있다. 수학식 9"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "또한, 그래디언트(Gradient)를 사용하여 critic 네트워크의 매개변수를 업데이트할 수 있으며, 하기의 수학식 10과 같이 정의될 수 있다.수학식 10"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "여기서, 는 학습률이다. 또한, FC(fully connected) layer와 CNN을 사용하여 대략적인 Q-value 함수 를 매개변수화 할 수 있다. 하기에서는 본 발명의 Actor Critic 포트폴리오 자산 배분 시스템의 실험예에 대해 설명하기로 한다. (실험예) Actor Critic 포트폴리오 자산 배분 시스템은 일 단위 데이터로 학습하며, 제안된 모델의 특성을 평가하기 위해 3개의 포트폴리오(포트폴리오-A, 포트폴리오-B, 포트폴리오-C)를 사용한다. 각 포트폴리오는 총 10개의 자산으로 구성되어 있다. 2012년 1월부터 2019년 7월까지 총 8년의 일일 데이터를 사용하고, 2012년 1월부터 2019년 1월 사이의 데이터를 훈련 세트로 사용하고 다른 데이터는 테스트 세트로 사 용한다. 또한, 한 달에 한 번 모든 포트폴리오를 재조정하고 0.001의 학습률을 가진 Adam Optimizer을 사용한다. 포트폴리오-A는 Nikkei225, Nasdaq, Kospi200, DAX, S&P500 5개의 주가지수와 S-Oil, SKTelecom, POSCO, Samsung Electronics, Korea Electric Power 5개의 주식으로 구성된다. 포트폴리오-B는 Nikkei 225, FTSE 100, Kospi 200, DAX, S&P500 5개의 주가지수와 SK Telecom, POSCO, Samsung Electronics, Korea Electric Power, Hyundai Motors 5개의 주식으로 구성된다. 포트폴리오-C는 Nikkei 225, FTSE 100, DAX, S&P500 4개의 주가지수와 SK Telecom, POSCO, Samsung Electronics, S-Oil, Hyundai Motors 5개의 주식으로 구성되어 있다. 각 포트폴리오의 실험결과로 나온 샤프지수는 하기의 [표 1] 내지 [표 3]과 같다. [표 1]은 포트폴리오-A의 연 간 샤프지수를 나타내고, [표 2]는 포트폴리오-B의 연간 샤프지수를 나타내며, [표 3]은 포트폴리오-C의 연간 샤프지수를 나타낸다. 표 1 Models The Annualized Sharpe ratio Markowitz 0.40250.5820 Equally Weighted -0.4644-0.3112 Risk Budgeting 0.27940.3980 RRL 1.80661.8417 Actor Critic 1.96521.9624 표 2 Models The Annualized Sharpe ratio Markowitz 0.5820 Equally Weighted -0.3112 Risk Budgeting 0.3980 RRL 1.8417 Actor Critic 1.9624표 3 Models The Annualized Sharpe ratio Markowitz 0.8708 Equally Weighted 0.4386 Risk Budgeting 1.0737 RRL 1.7861 Actor Critic 1.8025 전술한 바와 같이, 본 발명에서는 자산 배분을 위한 Actor Critic 강화학습 모델에 대하여 설명하였다. 기존의 방법들의 낮은 성능의 결과를 통해 강화학습이 자산 배분 문제에 중요한 역할을 하는 것을 볼 수 있다. RRL과 Actor Critic 모델은 LSTM을 사용하여 temporal dependencies를 반영하여 성능을 개선할 수 있다. 또한, 샤프지수를 보상으로 사용한 RRL 모델은 샤프지수의 영향을 많이 받는다. 이 단점을 해결하기 위하여 본 발명에서는 q-value 함수를 근사화하는 actor critic 모델에 대하여 설명하였다. 즉, Actor critic 모델은 타겟 네트워크와 experience replay를 통해서 훈련 모델을 효과적으로 학습할 수 있으 며, 실험결과인 표 1 내지 표 3은 actor critic 모델의 우수한 성능을 나타낸다. 이상과 같이, 본 발명은 비록 한정된 실시예와 도면에 의해 설명되었으나, 본 발명은 이것에 의해 한정되지 않"}
{"patent_id": "10-2020-0054508", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "으며 본 발명이 속하는 기술분야에서 통상의 지식을 가진자에 의해 본 발명의 기술사상과 아래에 기재될 특허청 구범위의 균등범위 내에서 다양한 수정 및 변형이 가능함은 물론이다."}
{"patent_id": "10-2020-0054508", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 액터 크리틱 모델(actor critic model)의 프레임워크를 나타내는 도면이다. 도 2는 언폴디드 순환 신경망(unfolded Recurrent Neural Networks)의 프레임워크를 나타내는 도면이다. 도 3은 합성곱신경망(Convolutional Neural Network)의 일예를 나타내는 도면이다. 도 4는 본 발명의 일실시예에 따른 포트폴리오 자산 배분을 위한 액터 네트워크(actor network) 구성을 나타내 는 도면이다. 도 5는 본 발명의 일실시예에 따른 포트폴리오 자산 배분을 위한 크리틱 네트워크(critic network) 구성을 나타 내는 도면이다."}
