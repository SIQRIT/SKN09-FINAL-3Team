{"patent_id": "10-2022-0118831", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0039918", "출원번호": "10-2022-0118831", "발명의 명칭": "사용자 인터페이스 제어 장치 및 이의 제어 방법", "출원인": "현대자동차주식회사", "발명자": "박당희"}}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "모빌리티에 탑승한 승객의 정보를 획득하는 센서부;상기 모빌리티와 상기 승객의 소통 수단을 제공하는 사용자 인터페이스부; 및상기 승객의 정보를 바탕으로 상기 승객을 모니터링하여 상기 승객의 외형 또는 신체 이상 여부에 따라 상기 승객의 유형을 판단하고, 상기 승객의 유형에 따라 과업완료시간이 단축되도록 상기 사용자 인터페이스부 중에서활성화되는 사용자 인터페이스를 다르게 선택하는 프로세서;를 포함하는 사용자 인터페이스 제어 장치."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 센서부는 상기 승객의 이미지를 획득하는 카메라를 포함하고,상기 프로세서는상기 이미지에서 미리 설정된 신체 부위 또는 상기 신체 부위와 연결된 특정 객체를 인공지능 학습하여, 상기승객의 연령 또는 상기 승객의 신체 자유도 중에서 적어도 어느 하나를 판단하는 것을 특징으로 하는 사용자 인터페이스 제어 장치."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 프로세서는상기 승객의 신체 자유도가 임계 자유도 이하일 경우, 상기 사용자 인터페이스부 중에서 모션 인식부 또는 음성인식부를 활성화시키는 것을 특징으로 하는 사용자 인터페이스 제어 장치."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 2 항에 있어서,상기 프로세서는상기 승객의 연령이 제1 임계 연령 이상이거나, 제2 임계 연령 이하일 경우, 상기 사용자 인터페이스부 중에서음성 인식부를 활성화시키는 것을 특징으로 하는 사용자 인터페이스 제어 장치."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 2 항에 있어서,상기 프로세서는상기 학습 결과를 바탕으로 상기 승객의 신체 이상 여부를 판단하고,상기 승객이 시각장애 또는 팔 손실 장애가 있다고 판단될 경우, 상기 사용자 인터페이스부 중에서 음성 인식부공개특허 10-2024-0039918-3-를 활성화시키는 것을 특징으로 하는 사용자 인터페이스 제어 장치."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서,상기 프로세서는상기 승객이 청각장애라고 판단될 경우, 상기 사용자 인터페이스부 중에서 터치 스크린 또는 모션 인식부 중에서 적어도 어느 하나를 활성화시키는 것을 특징으로 하는 사용자 인터페이스 제어 장치."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 5 항에 있어서,상기 프로세서는 상기 승객의 신체 이상 여부를 판단하기 위해서, 상기 승객의 개인 정보를 확인하는 것을 특징으로 하는 사용자인터페이스 제어 장치."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서,상기 프로세서는상기 승객의 행동 패턴을 판단하여, 상기 승객의 안면 방향이 향하는 위치에 디스플레이 영상을 표시하는 것을특징으로 하는 사용자 인터페이스 제어 장치."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8 항에 있어서,상기 프로세서는상기 승객이 식사 중이라고 판단되는 경우, 상기 사용자 인터페이스부 중에서 스피커를 활성화시키는 것을 특징으로 하는 사용자 인터페이스 제어 장치."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1 항에 있어서, 상기 프로세서는 상기 사용자 인터페이스부 중에서, 상기 승객의 착석한 위치에 매칭되는 디스플레이를 활성화시키며, 운전석이위치한 1열 이외에 착석한 승객에게 음성 인식부 또는 모션 인식부를 활성화시키는 것을 특징으로 하는 사용자인터페이스 제어 장치."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "모빌리티 장치에 탑승한 승객을 모니터링하는 단계;상기 모니터링 결과를 바탕으로, 상기 승객의 외형 또는 신체 이상 여부에 따라 상기 승객의 유형을 판단하는단계; 및공개특허 10-2024-0039918-4-상기 승객의 유형에 따라, 사용자 인터페이스부를 제어하는 과업완료시간이 단축될 수 있도록 상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계;를 포함하는 사용자 인터페이스 제어 방법."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서,상기 승객을 모니터링하는 단계는, 카메라로부터 상기 승객의 이미지를 제공받는 단계를 포함하고,상기 승객의 유형을 판단하는 단계는,상기 이미지에서 미리 설정된 신체 부위 또는 상기 신체 부위와 연결된 특정 객체를 인공지능 학습하는 단계;및상기 학습 결과를 바탕으로 상기 승객의 연령 또는 상기 승객의 신체 자유도 중에서 적어도 어느 하나를 판단하는 단계;를 포함하는 것을 특징으로 하는 사용자 인터페이스 제어 방법."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12 항에 있어서,상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는, 상기 승객의 신체 자유도가 임계 자유도 이하일 경우, 상기 사용자 인터페이스부 중에서 모션 인식부 또는 음성인식부를 활성화시키는 단계를 포함하는 것을 특징으로 하는 사용자 인터페이스 제어 방법."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 12 항에 있어서,상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는,상기 승객의 연령이 제1 임계 연령 이상이거나, 제2 임계 연령 이하일 경우, 상기 사용자 인터페이스부 중에서음성 인식부를 활성화시키는 단계를 포함하는 것을 특징으로 하는 사용자 인터페이스 제어 방법."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 12 항에 있어서,상기 승객의 유형을 판단하는 단계는상기 학습 결과를 바탕으로 상기 승객의 신체 이상 여부를 판단하는 단계를 더 포함하고,상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는,상기 승객이 시각장애 또는 팔 손실 장애가 있다고 판단될 경우, 상기 사용자 인터페이스부 중에서 음성 인식부를 활성화시키는 단계를 포함하는 것을 특징으로 하는 사용자 인터페이스 제어 방법."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15 항에 있어서,상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는,상기 승객이 청각장애라고 판단될 경우, 상기 사용자 인터페이스부 중에서 터치 스크린 또는 모션 인식부 중에공개특허 10-2024-0039918-5-서 적어도 어느 하나를 활성화시키는 단계;를 포함하는 것을 특징으로 하는 사용자 인터페이스 제어 방법."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 15 항에 있어서,상기 학습 결과를 바탕으로 상기 승객의 신체 이상 여부를 판단하는 단계는,상기 승객의 개인 정보를 확인하는 단계를 더 포함하는 것을 특징으로 하는 사용자 인터페이스 제어 방법."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 11 항에 있어서,상기 승객의 유형을 판단하는 단계는 상기 승객의 행동 패턴을 판단하는 단계를 더 포함하고, 상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는,상기 승객의 안면 방향이 향하는 위치에 디스플레이 영상을 표시하는 단계를 포함하는 것을 특징으로 하는 사용자 인터페이스 제어 방법."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 18 항에 있어서, 상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는,상기 승객이 식사 중이라고 판단되는 경우, 상기 사용자 인터페이스부 중에서 스피커를 활성화시키는 단계를 포함하는 것을 특징으로 하는 사용자 인터페이스 제어 방법."}
{"patent_id": "10-2022-0118831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 11 항에 있어서,상기 승객의 유형을 판단하는 단계는 상기 승객의 착석한 위치를 판단하는 단계를 더 포함하고,상기 사용자 인터페이스부 중에서, 상기 승객의 착석한 위치에 매칭되는 디스플레이를 활성화시키는 단계; 및운전석이 위치한 1열 이외에 착석한 승객에게 음성 인식부 또는 모션 인식부를 활성화시키는 단계;를 포함하는것을 특징으로 하는 사용자 인터페이스 제어 방법."}
{"patent_id": "10-2022-0118831", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 실시 예에 의한 사용자 인터페이스 제어 장치는 모빌리티에 탑승한 승객의 정보를 획득하는 센서부, 모빌리티와 승객의 소통 수단을 제공하는 사용자 인터페이스부 및 프로세서를 포함할 수 있다. 프로세서는 승객 의 정보를 바탕으로 승객을 모니터링하여 승객의 외형 또는 신체 이상 여부에 따라 상기 승객의 유형을 판단할 수 있다. 또한 프로세서는 승객의 유형에 따라 과업완료시간이 단축되도록 사용자 인터페이스부 중에서 활성화되 는 사용자 인터페이스를 다르게 선택할 수 있다."}
{"patent_id": "10-2022-0118831", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 사용자 인터페이스 제어 장치 및 이의 제어 방법에 관한 것으로, 보다 상세하게는 승객에 따라 다양 한 사용자 인터페이스를 효과적으로 제어하는 기술에 관한 것이다."}
{"patent_id": "10-2022-0118831", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "차량에는 차량과 사용자 간의 소통을 위한 사용자 인터페이스(User Interface; UI)가 장착되어 있다. 사용자 인터페이스 장치는 물리적 사용자 인터페이스 (PUI: Physical User Interface) 장치와 그래픽 사용 자 인터페이스(GUI: Graphical User Interface) 장치를 포함한다. 물리적 사용자 인터페이스 장치(PUI)는 키 패드, 리모컨, 터치패드 등과 같이 물리적인 방법으로 사용자 명령을 입력 받는 장치이고, 그래픽 사용자 인터 페이스 장치(GUI)는 디스플레이 상에 표시되는 아이콘이나 메뉴가 선택됨으로서 사용자 명령을 입력 받는 장치이다. 사용자는 그래픽 사용자 인터페이스 장치를 통해 표시되는 메뉴, 리스트, 아이콘 등을 참조하여 커서를 이동 시키고, 커서가 위치한 항목을 선택할 수 있다. 또한, 사용자는 물리적 사용자 인터페이스 장치를 통해서 커 서를 이동시켜 선택하고자 하는 항목을 선택할 수 있다. 근래에는 차량의 기본적인 주행 기능 이외에도 차량의 부가 기능이 늘어나면서, 이를 제어하기 위한 사용자 인 터페이스도 다양해지고 복잡해지고 있다. 또한, 차량에 탑승하는 승객은 여러 유형이 있는데, 사용자 인터페이스는 모든 승객들에게 획일화되어 있기 때 문에, 승객들에 따라 사용자 인터페이스가 불편하게 느껴질 수 있다. 특히, 차량의 자율주행 기능이 증가할수록 단순히 운전을 위한 운전자 이외의 승객이 늘어나며, 이에 따라 운전 이외의 다양한 사용자 인터페이스를 활용하는 경우가 늘어날 수 있는데, 기존의 사용자 인터페이스는 승객들의 다양성에 비하여 최적화가 되어 있지 않은 실정이다."}
{"patent_id": "10-2022-0118831", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 다양한 승객에 대응하여 최적화 된 사용자 인터페이스를 제공할 수 있는 사용자 인터페이스 제어 장 치 및 제어 방법을 제공하기 위한 것이다. 또한, 본 발명은 승객들이 사용자 인터페이스를 제어하는 과업완료시간을 줄일 수 있는 사용자 인터페이스 제어 장치 및 제어 방법을 제공하기 위한 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재들로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0118831", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시 예에 의한 사용자 인터페이스 제어 장치는 모빌리티에 탑승한 승객의 정보를 획득하는 센서부, 모빌리티와 승객의 소통 수단을 제공하는 사용자 인터페이스부 및 프로세서를 포함할 수 있다. 프로세서는 승객 의 정보를 바탕으로 승객을 모니터링하여 승객의 외형 또는 신체 이상 여부에 따라 상기 승객의 유형을 판단할 수 있다. 또한 프로세서는 승객의 유형에 따라 과업완료시간이 단축되도록 사용자 인터페이스부 중에서 활성화 되는 사용자 인터페이스를 다르게 선택할 수 있다. 실시 예에 의하면, 상기 센서부는 상기 승객의 이미지를 획득하는 카메라를 포함하고, 상기 프로세서는 상기 이 미지에서 미리 설정된 신체 부위 또는 상기 신체 부위와 연결된 특정 객체를 인공지능 학습하여, 상기 승객의 연령 또는 상기 승객의 신체 자유도 중에서 적어도 어느 하나를 판단할 수 있다. 실시 예에 의하면, 상기 프로세서는 상기 승객의 신체 자유도가 임계 자유도 이하일 경우, 상기 사용자 인터페 이스부 중에서 모션 인식부 또는 음성 인식부를 활성화시킬 수 있다. 실시 예에 의하면, 상기 프로세서는 상기 승객의 연령이 제1 임계 연령 이상이거나, 제2 임계 연령 이하일 경우, 상기 사용자 인터페이스부 중에서 음성 인식부를 활성화시킬 수 있다. 실시 예에 의하면, 상기 프로세서는 상기 학습 결과를 바탕으로 상기 승객의 신체 이상 여부를 판단하고, 상기 승객이 시각장애 또는 팔 손실 장애가 있다고 판단될 경우, 상기 사용자 인터페이스부 중에서 음성 인식부를 활 성화시킬 수 있다. 실시 예에 의하면, 상기 프로세서는 상기 승객이 청각장애라고 판단될 경우, 상기 사용자 인터페이스부 중에서 터치 스크린 또는 모션 인식부 중에서 적어도 어느 하나를 활성화시킬 수 있다. 실시 예에 의하면, 상기 프로세서는 상기 승객의 신체 이상 여부를 판단하기 위해서, 상기 승객의 개인 정보를 확인할 수 있다. 실시 예에 의하면, 상기 프로세서는 상기 승객의 행동 패턴을 판단하여, 상기 승객의 안면 방향이 향하는 위치 에 디스플레이 영상을 표시할 수 있다. 실시 예에 의하면, 상기 프로세서는 상기 승객이 식사 중이라고 판단되는 경우, 상기 사용자 인터페이스부 중에 서 스피커를 활성화시킬 수 있다. 실시 예에 의하면, 상기 프로세서는 상기 사용자 인터페이스부 중에서, 상기 승객의 착석한 위치에 매칭되는 디 스플레이를 활성화시키며, 운전석이 위치한 1열 이외에 착석한 승객에게 음성 인식부 또는 모션 인식부를 활성 화시킬 수 있다. 본 발명의 실시 예에 의한 사용자 인터페이스 제어 방법은 모빌리티 장치에 탑승한 승객을 모니터링하는 단계, 상기 모니터링 결과를 바탕으로 상기 승객의 외형 또는 신체 이상 여부에 따라 상기 승객의 유형을 판단하는 단 계, 및 상기 승객의 유형에 따라 사용자 인터페이스부를 제어하는 과업완료시간이 단축될 수 있도록 상기 사용 자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계를 포함할 수 있다. 실시 예에 의하면, 상기 승객을 모니터링하는 단계는 카메라로부터 상기 승객의 이미지를 제공받는 단계를 포함 하고, 상기 승객의 유형을 판단하는 단계는 상기 이미지에서 미리 설정된 신체 부위 또는 상기 신체 부위와 연 결된 특정 객체를 인공지능 학습하는 단계, 및 상기 학습 결과를 바탕으로 상기 승객의 연령 또는 상기 승객의 신체 자유도 중에서 적어도 어느 하나를 판단하는 단계를 포함할 수 있다. 실시 예에 의하면, 상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는, 상기 승객의 신체 자유도가 임계 자유도 이하일 경우, 상기 사용자 인터페이스부 중에서 모션 인식부 또는 음성 인식부를 활성화시키는 단계를 포함할 수 있다. 실시 예에 의하면, 상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는, 상기 승객의 연령이 제1 임계 연령 이상이거나 제2 임계 연령 이하일 경우 상기 사용자 인터페이스부 중에서 음 성 인식부를 활성화시키는 단계를 포함할 수 있다. 실시 예에 의하면, 상기 승객의 유형을 판단하는 단계는 상기 학습 결과를 바탕으로 상기 승객의 신체 이상 여 부를 판단하는 단계를 더 포함하고, 상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는, 상기 승객이 시각장애 또는 팔 손실 장애가 있다고 판단될 경우, 상기 사용자 인터페이스부 중에서 음성 인식부를 활성화시키는 단계를 포함할 수 있다. 실시 예에 의하면, 상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는, 상기 승객이 청각장애라고 판단될 경우, 상기 사용자 인터페이스부 중에서 터치 스크린 또는 모션 인식부 중에 서 적어도 어느 하나를 활성화시키는 단계를 포함할 수 있다. 실시 예에 의하면, 상기 학습 결과를 바탕으로 상기 승객의 신체 이상 여부를 판단하는 단계는, 상기 승객의 개 인 정보를 확인하는 단계를 더 포함할 수 있다. 실시 예에 의하면, 상기 승객의 유형을 판단하는 단계는 상기 승객의 행동 패턴을 판단하는 단계를 더 포함하고, 상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는, 상기 승 객의 안면 방향이 향하는 위치에 디스플레이 영상을 표시하는 단계를 포함할 수 있다. 실시 예에 의하면, 상기 사용자 인터페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택하는 단계는, 상기 승객이 식사 중이라고 판단되는 경우, 상기 사용자 인터페이스부 중에서 스피커를 활성화시키는 단계를 포 함할 수 있다. 실시 예에 의하면, 상기 승객의 유형을 판단하는 단계는 상기 승객의 착석한 위치를 판단하는 단계를 더 포함하 고, 상기 사용자 인터페이스부 중에서, 상기 승객의 착석한 위치에 매칭되는 디스플레이를 활성화시키는 단계 및 운전석이 위치한 1열 이외에 착석한 승객에게 음성 인식부 또는 모션 인식부를 활성화시키는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0118831", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시 예에 의하면, 승객의 유형을 판단하고, 이에 매칭되는 사용자 인터페이스를 활성화시킴으로써, 각각의 승객에게 최적화 된 사용자 인터페이스를 제공할 수 있다. 또한, 본 발명의 실시 예에 의하면, 승객의 유형에 따라 과업완료시간을 줄일 수 있는 사용자 인터페이스를 활 성화시킴으로써, 승객들이 보다 빠르고 편하게 사용자 인터페이스를 제어할 수 있다. 이 외에, 본 문서를 통해 직접적 또는 간접적으로 파악되는 다양한 효과들이 제공될 수 있다."}
{"patent_id": "10-2022-0118831", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 일부 실시예들을 예시적인 도면을 통해 상세하게 설명한다. 각 도면의 구성요소들에 참조부호 를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 발명의 실시예를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 발명의 실시예에 대한 이해를 방해한다고 판단되는 경우에는 그 상세한 설명은 생략한 다. 본 발명의 실시예의 구성 요소를 설명하는 데 있어서, 제 1, 제 2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이러한 용어는 그 구성 요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성 요소 의 본질이나 차례 또는 순서 등이 한정되지 않는다. 또한, 다르게 정의되지 않는 한, 기술적이거나 과학적인 용 어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들 은 관련 기술의 문맥상 가지는 의미와 일치하는 의미를 가진 것으로 해석되어야 하며, 본 출원에서 명백하게 정 의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 이하, 도 1 내지 도 18을 참조하여, 본 발명의 실시 예들을 구체적으로 설명하기로 한다. 도 1은 본 발명의 일 실시 예에 의한 사용자 인터페이스 제어 장치의 구성을 나타내는 도면이다. 도 1은 사용자 인터페이스 제어 장치가 차량에 적용된 실시 예를 도시하고 있다. 이하, 본 명세서는 차량에 적용된 사용자 인 터페이스를 중심으로 설명하지만, 본 발명의 실시 예에 의한 사용자 인터페이스 제어 장치는 승객이 탑승하는 다양한 형태의 모빌리티(mobility)에 적용될 수 있다. 예를 들어, 사용자 인터페이스 제어 장치는 유인 항공체 에 적용될 수도 있다. 도 1을 참조하면, 본 발명의 일 실시 예에 의한 사용자 인터페이스 제어 장치는 센서부, 프로세서 , 메모리 및 사용자 인터페이스부를 포함할 수 있다. 센서부는 차량 내부를 모니터링하기 위한 것으로, 레이더, 라이다, 카메라, 초음파 센서 , 및 압력 센서 등을 포함할 수 있다. 레이더는 지향성의 송신 신호를 송출하고, 송신 신호의 반사파를 수신 신호로 수신할 수 있다. 레이더 는 송신 신호와 수신 신호 간의 주파수 차이를 바탕으로 객체를 판단할 수 있다. 라이다는 레이저 펄스를 송출하고, 송출된 레이저 펄스가 반사되는 시간을 측정하여 객체를 판별할 수 있 다. 라이다는 3차원 정보를 제공할 수 있다. 카메라는 이미지 센서를 통해서 획득한 전기 신호를 바탕으로 차량 내부의 영상 데이터를 획득할 수 있다. 카메라는 복수의 시트 영역들에 대한 영상을 획득하도록 배치될 수 있고, 예를 들어 각각의 시트 영역에대응하도록 복수 개의 카메라들이 설치될 수 있다. 카메라는 모노 카메라, 스테레오 카메라, AVM(Around View Monitoring) 카메라 중 적어도 어느 하나일 수 있다. 초음파 센서는 초음파 액츄에이터를 통해서 생성된 초음파를 송출하고, 객체에 의해서 반사되는 초음파를 수신한 것을 바탕으로, 객체의 거리를 산출할 수 있다. 초음파 센서는 복수의 시트 영역들에 대한 객체를 감지할 수 있도록 배치될 수 있고, 예를 들어 각각의 시트 영역에 대응하도록 복수 개의 초음파 센서들이 설치될 수 있다. 압력 센서는 시트에 내장되어, 시트에 가해지는 압력을 센싱하는 압력 센서일 수 있다. 압력 센서에 의해 획득된 압력을 승객의 착석 여부를 판단하는 과정에서 이용될 수 있다. 프로세서는 승객의 정보를 바탕으로 승객을 모니터링할 수 있다. 승객의 정보는 카메라가 획득한 이 미지일 수 있다. 프로세서는 모니터링 결과를 바탕으로 승객의 외형 또는 신체 이상 여부에 따라 승객의 유형을 판단할 수 있다. 그리고, 프로세서는 승객의 유형에 따라 과업완료시간이 단축되도록 사용자 인터 페이스부 중에서 활성화되는 사용자 인터페이스를 다르게 선택할 수 있다. 사용자 인터페이스의 일례는 후 술하기로 한다. 프로세서는 승객의 유형을 판단하기 위해서 인공지능 기반으로 이미지를 학습할 수 있다. 이를 위해서, 프 로세서는 인공지능(artificial intelligence; 이하, AI) 프로세서를 포함할 수 있다. AI 프로세서는 미리 저장된 프로그램을 이용하여 신경망을 학습할 수 있다. 이미지 학습을 위한 신경망은 인간의 뇌 구조를 컴퓨터 상에서 모의하도록 설계될 수 있으며, 인간의 신경망의 뉴런(neuron)을 모의하는, 가중치를 갖는 복수의 네트워 크 노드들을 포함할 수 있다. 복수의 네트워크 모드들은 뉴런이 시냅스(synapse)를 통해 신호를 주고 받는 뉴런 의 시냅틱 활동을 모의하도록 각각 연결 관계에 따라 데이터를 주고 받을 수 있다. 신경망은 신경망 모델에서 발전한 딥러닝 모델을 포함할 수 있다. 딥 러닝 모델에서 복수의 네트워크 노드들은 서로 다른 레이어에 위치하 면서 컨볼루션(convolution) 연결 관계에 따라 데이터를 주고 받을 수 있다. 신경망 모델의 예는 심층 신경망 (DNN, deep neural networks), 합성곱 신경망(CNN, convolutional deep neural networks), 순환 신경망(RNN, Recurrent Boltzmann Machine), 제한 볼츠만 머신(RBM, Restricted Boltzmann Machine), 심층 신뢰 신경망 (DBN, deep belief networks), 심층 Q-네트워크(Deep Q-Network)와 같은 다양한 딥 러닝 기법들을 포함할 수 있다. 메모리는 인공지능 학습 모델, 및 센서부가 획득한 정보를 해석하기 위한 알고리즘을 저장할 수 있다. 메모리는 프로세서 내에 구비 될 수 있고, 별도의 메모리가 될 수 있다. 따라서, 메모리는 하드 디스크 드라이브, 플래시 메모리, EEPROM(Electrically erasable programmable read-only memory), SRAM(Static RAM), FRAM (Ferro-electric RAM), PRAM (Phase-change RAM), MRAM(Magnetic RAM), DRAM(Dynamic Random Access Memory), SDRAM(Synchronous Dynamic Random Access Memory), DDR-SDRAM(Double Date Rate- SDRAM) 등으로 구성될 수 있다. 사용자 인터페이스부는 승객으로부터 사용자 입력을 수신하고, 사용자에게 특정 정보를 제공하기 위한 것으로, 음성 인식부, 스피커, 디스플레이부 및 햅틱부를 포함할 수 있다. 음성 인식부, 모션 인식부는 입력 인터페이스에 포함될 수 있다. 모션 인식부는 카메라가 획득한 승 객의 사용자 제스처를 기반으로 동작하는 프로세서의 알고리즘을 포함할 수 있다. 따라서, 카메라는 입력 인터페이스의 모션 인식부일 수 있으며, 본 명세서에서 모션 인식부를 활성화한다는 의미는 카메라를 활성 화한다는 의미로 해석될 수 있다. 스피커, 디스플레이부, 및 햅틱부는 출력 인터페이스에 포함될 수 있다. 디스플레이부는 사용자에게 특정 영상을 표시하는 출력 인터페이스이면서, 터치 스크린을 통해서 사용자 입력을 수신하는 입력 인터페이스의 기능을 수행할 수 있다. 음성 인식부는 음성 텍스트 변환(Speech to Text, STT) 기법을 이용하여 마이크를 통해 입력되는 음성 신 호를 문자 데이터로 변환할 수 있다. 음성 인식부는 자연어 이해(Natural Language Understanding, NLU) 기법을 이용하여 변환된 텍스트의 의미를 분석하여 음성인식 결과를 출력한다. 스피커는, 프로세서로부터 제공되는 전기 신호를 오디오 신호로 변환하여 출력할 수 있다. 디스플레이부는, 다양한 정보에 대응되는 그래픽 객체를 표시할 수 있다. 디스플레이부는 액정 디스 플레이(liquid crystal display, LCD), 유기 발광 다이오드(organic light-emitting diode, OLED), 플렉서블 디스플레이(flexible display), 프로젝션(Projection) 등을 이용할 수 있다. 또한, 디스플레이부는 HUD(Head Up Display)로 구현될 수 있다. 디스플레이부가 HUD로 구현되는 경우, 디스플레이부는 투사 모듈을 구비하여 윈드 쉴드 또는 윈도우에 투사되는 이미지를 통해 정보를 출 력할 수 있다. 디스플레이부는 투명 디스플레이를 포함할 수 있다. 투명 디스플레이는 윈드 쉴드 또는 윈 도우에 결합될 수 있지만, 배치 위치는 이에 한정되지 않는다. 투명 디스플레이는 패널의 일부 영역에 OLED 표 시부가 형성되고, 다른 일부 영역에 투명부가 형성된 구조를 이용할 수 있다. 디스플레이부는 복수의 디스플레이를 포함할 수 있다. 예를 들어, 디스플레이부는 제1 내지 제3 디스 플레이들(530a, 530b, 530c)을 포함할 수 있다. 제1 디스플레이(530a)는 도 2에서와 같이, 프로젝션 타입일 수 있고, 제2 디스플레이(530b)는 도 4 및 도 5에서와 같이, 시트에 결합된 것일 수 있으며, 제3 디스플레이(530 c)는 도 10에서와 같이, 클러스터에 결합된 것일 수 있다. 제2 및 제3 디스플레이들(530b, 530c)은 터치 스크린이 결합되어 사용자 입력을 제공받을 수 있다. 따라서, 본 명세서에서 터치 스크린을 활성화시키는 동작은 제2 디스플레이(530b) 또는 제3 디스플레이(530c) 중에서 어느 하나를 활성화시키는 절차를 의미할 수 있다. 이외에도, 디스플레이부는 스티어링 휠, 각 필러의 내측 영역, 도어의 내측 영역, 윈드 쉴드의 내측 영역, 윈도우의 내측 영역에 형성될 수 있다. 햅틱부는 촉각적인 출력을 발생시킬 수 있다. 예를 들면, 햅틱부는 스티어링 휠, 안전 벨트, 시트를 진동시켜, 특정 기능 또는 특정 장치의 동작 사실을 승객에게 인지시킬 수 있다. 도 2 및 도 3은 본 발명의 실시 예에 의한 사용자 인터페이스 제어 장치를 나타내는 도면이다. 도 1 및 도 2를 참조하면, 본 발명의 실시 예에 의한 사용자 인터페이스 제어 장치는 각 구성들이 하우징 의 내부 또는 외부에 결합되어 일체형으로 구현될 수 있다. 하우징은 사각형의 바(bar) 형태일 수 있다. 하우징은 전면이 차량(VEH) 내부에서 승객이 위치할 수 있는 영역을 향하도록 차량(VEH)에 결합될 수 있다. 카메라는 하우징의 전면에 장착될 수 있다. 마이크 및 스피커는 하우징의 일면에 장착될 수 있다. 카메라는 이동체에서 회전 가능하도록 결합되고, 이동체는 하우징에 결합될 수 있다. 이에 따라, 카메라의 화각은 차량(VEH) 내부의 전체 영역을 커버할 수 있다. 프로세서 및 메모리는 하우징 내부에 탑재될 수 있고, 그 이외의 다른 구성들도 하우징에 탑 재되거나 하우징과 결합될 수 있다. 도 3을 참조하면, 본 발명의 실시 예에 의한 사용자 인터페이스 제어 장치는 하우징은 평면이 타원형 또는 원형으로 형성될 수 있고, 차량(VEH) 내부에서 탈부착이 가능할 수 있다. 카메라는 하우징의 외측에 노출되도록 형성될 수 있다. 마이크 및 스피커는 하우징의 일면에 장착될 수 있다. 카메라는 이동체에서 회전 가능하도록 결합되고, 이동체는 하우징에 결합될 수 있다. 이에 따라, 카메라의 화각은 차량(VEH) 내부의 전체 영역을 커버할 수 있다. 프로세서 및 메모리는 하우징 내부에 탑재될 수 있고, 그 이외의 다른 구성들도 하우징에 탑 재되거나 하우징과 결합될 수 있다. 도 4 및 도 5는 사용자 인터페이스 제어 장치가 차량에 설치된 실시 예를 나타내는 도면이다. 도 4 및 도 5를 참조하면, 사용자 인터페이스 제어 장치는 차량(VEH) 내부의 전체 영역을 모니터링하기 수 월하도록 차량의 천장에서 중심부에 설치될 수 있다. 또한, 사용자 인터페이스 제어 장치는 썬루프의 개폐 장치에 결합될 수도 있다. 사용자 인터페이스 제어 장치는 하우징에 결합된 제1 디스플레이(530a) 이외에도 시트의 후면에 장착된 제2 디스플레이(530b)와 연계되어 동작할 수 있다. 즉, 제2 디스플레이(530b)는 사용자 인터페이스 제어 장치 의 제어에 따라 활성화될 수 있다. 도 6을 참조하여 본 발명의 실시 예에 의한 사용자 인터페이스 제어 장치의 동작을 설명하는 순서도이다. 도 6 에 도시된 절차들은 사용자 인터페이스부를 제어하기 위한 프로세서의 동작으로 이해될 수 있다. 도 6을 참조하여 본 발명의 실시 예에 의한 사용자 인터페이스부의 제어 방법을 살펴보면 다음과 같다. S610에서, 프로세서는 차량에 탑승한 승객을 모니터링할 수 있다. 이를 위해서, 프로세서는 센서부로부터 승객에 대한 정보를 제공받을 수 있다. 실시 예에 의하면, 프로세서는 카메라가 획득한 승객의 이미지를 제공받을 수 있다. S620에서, 프로세서는 모니터링 결과를 바탕으로, 승객의 외형 또는 신체 이상 여부에 따라, 승객의 유형 을 판단할 수 있다. 실시 예에 의하면, 승객의 유형을 판단하는 단계는 승객의 연령을 판단하는 단계를 포함할 수 있다. 또한, 실시 예에 의하면, 승객의 유형을 판단하는 단계는 승객의 신체 자유도를 판단하는 단계를 포함할 수 있 다. 신체 자유도는 승객이 활동에 제한을 주는 요인에 비례하여 낮은 크기로 산출될 수 있다. 예를 들어, 승객 이 임산부이거나 깁스 등으로 인해서 신체 움직임에 제한이 있을 경우, 신체 자유도는 낮아질 수 있다. 또한, 실시 예에 의하면, 승객의 유형을 판단하는 단계는 승객의 신체 사이즈를 판단하는 단계를 포함할 수 있 다. 또한, 실시 예에 의하면, 승객의 유형을 판단하는 단계는 승객의 신체 이상 여부를 판단하는 단계를 포함할 수 있다. 예를 들어, 프로세서는 승객이 시각장애 또는 청각장애 또는 팔손실 장애가 있는지를 판단할 수 있 다. 또한, 실시 예에 의하면, 승객의 유형을 판단하는 단계는 승객의 행동 패턴을 판단하는 단계를 포함할 수 있다. 예를 들어, 프로세서는 승객이 음식을 섭취 중인지, 승객이 누워있는지 등을 판단할 수 있다. 또한, 실시 예에 의하면, 승객의 유형을 판단하는 단계는 승객의 착석 위치를 판단하는 단계를 포함할 수 있다. 또한, 실시 예에 의하면, 프로세서는 승객의 유형을 판단하기 위해서 승객의 개인 정보를 확인할 수 있다. 프로세서는 승객에게 개인 정보 제공을 요청하기 위해서 스피커 또는 디스플레이부를 제어할 수 있다. S630에서, 프로세서는 승객의 유형에 따라, 사용자 인터페이스를 다르게 선택하여 활성화시킬 수 있다. 실 시 예에 의하면, 프로세서는 사용자 인터페이스를 제어하는 과업완료시간(Task Completion Time; TCT)이 단축될 수 있도록 사용자 인터페이스부를 활성화시킬 수 있다. 실시 예에 의하면, 프로세서는 승객의 신체 자유도가 임계 자유도 이하일 경우, 모션 인식을 위한 카메라 또는 음성 인식부를 활성화시킬 수 있다. 예를 들어, 승객이 임산부이거나 신체 상해로 인해서 거동 이 불편할 경우, 착석 위치에서 떨어져 있는 콕핏(cockpit) 모듈 또는 터치 스크린 등의 직접 조작 형태의 입력 장치를 제어하기에는, 과업완료시간이 길어질 수 있다. 따라서, 프로세서는 승객의 거동이 불편할 경우, 모션 인식부 또는 음성 인식부를 활성화시킬 수 있다. 또한, 임산부와 같이 호흡이 쉽지 않을 경우는 음성 인식도 불편할 수 있기 때문에, 모션 인식부를 활성화시킬 수 있다. 또한, 실시 예에 의하면, 프로세서는 승객의 연령이 제1 임계 연령 이상이거나, 제2 임계 연령 이하일 경 우, 음성 인식부를 활성화시킬 수 있다. 고령의 승객은 직접 조작 형태의 입력 장치를 제어하는 것에 어려 움을 느낄 수 있고, 미리 정해진 패턴을 익혀야 하는 모션 인식부를 활용하기에도 수월하지 않을 수 있다. 또한 승객이 유아일 경우, 카시트 또는 시트벨트의 구속으로 인해서 직접 조작 형태의 입력 장치를 제어하기 곤란할 수 있다. 따라서, 프로세서는 승객이 고령이거나 유아일 경우, 과업완료시간을 줄이기 위해서 음성 인식부 를 활성화시킬 수 있다. 만약, 유아로 판단된 승객이 모션 인식을 제어할 수 있을 정도의 연령이라면, 프로세서는 모션 인식부를 입력 인터페이스로 활성화시킬 수도 있다. 또한, 실시 예에 의하면, 프로세서는 승객이 시각장애 또는 팔손실 장애가 있다고 판단될 경우, 음성 인식 부를 활성화시킬 수 있다. 시각장애이거나 팔손실 장애가 있을 경우, 직접 조작 형태의 입력 장치 뿐만 아 니라 모션 인식부를 제어하기가 수월하지 않을 수 있다. 따라서, 프로세서는 승객이 시각장애 또는 팔손실 장애가 있다고 판단될 경우, 과업완료시간을 줄이기 위해서 음성 인식부를 활성화시킬 수 있다. 또한, 실시 예에 의하면, 프로세서는 승객이 청각장애라고 판단될 경우, 모션 인식을 위한 카메라 또 는 터치 스크린 중에서 적어도 어느 하나를 활성화시킬 수 있다. 승객이 청각장애일 경우, 음성 인식이 곤란하 기 때문에, 프로세서는 과업완료시간을 줄이기 위해서 카메라 또는 터치 스크린을 활성화시킬 수 있 다. 또한, 실시 예에 의하면, 프로세서는 승객의 안면 방향이 향하는 위치에 디스플레이 영상을 표시할 수 있 다. 예를 들어, 승객이 누워있을 경우, 프로세서는 차량의 천장에 제1 디스플레이(530a)를 이용하여 영상 을 맵핑할 수 있다. 또한, 실시 예에 의하면, 프로세서는 승객이 식사 중이라고 판단되는 경우, 스피커를 활성화시킬 수 있다. 또한, 실시 예에 의하면, 프로세서는 승객이 착석한 위치에 매칭되는 디스플레이를 활성화시킬 수 있다. 도 7은 승객의 유형을 판단하는 절차를 설명하는 순서도이다. 도 7을 참조하여, 승객의 유형을 판단하는 절차를 살펴보면 다음과 같다. S710에서, 프로세서는 승객 이미지에서 신체의 특정 부위를 학습할 수 있다. 또한, 프로세서는 카메 라로부터 제공받은 승객 이미지에서, 미리 설정된 신체 부위 또는 신체 부위와 연결된 특정 객체를 학습할 수 있다. 미리 설정된 신체 부위는 사용자에 따라 다양성이 큰 부위일 수 있고, 예를 들어 얼굴, 배 등이 선택될 수 있다. 또한, 사용자의 신체 이상을 판단하기 위해서 프로세서는 팔, 다리의 신체 부위를 학습할 수 있다. 신체 부위와 연결된 특정 객체는 사용자의 신체 이상을 판단하기 위한 것으로, 지팡이, 목발, 안경, 지팡이 등 일 수 있다. 프로세서는 신체의 특정 부위 및 특정 객체를 학습하여, 승객의 유형을 분류할 수 있다. 예를 들어, 프로세서는 승객의 키, 자세, 얼굴 부위를 학습하여, 유아 또는 노약자를 판단할 수 있다. 또한, 프로세서는 승객의 얼굴, 배, 자세 등을 학습하여 임산부와 같이 거동이 불편한 자를 판단할 수 있 다. 또한, 프로세서는 지팡이 등을 학습하여 시각장애를 판단할 수 있다. 또한, 프로세서는 보청기, 수화 사용 모션 등을 학습하여 청각장애를 판단할 수 있다. 또한, 프로세서는 팔을 학습하여 팔손실 장애를 판단할 수 있다. 프로세서는 노약자, 유아, 거동이 불편한 승객, 시각장애, 청각장애, 및 팔손실 장애와 같이 특정된 사용 자 유형을 제외한 승객을 일반 승객으로 판단할 수 있다. 도 8은 프로세서가 이미지를 바탕으로 승객의 유형을 분류한 실시 예를 나타내는 도면이다. 프로세서는 S710에서와 같이, 승객의 신체 특징을 바탕으로 이미지에서 승객을 분류하고 각각의 승객에 클 래스(U1~U6)를 부여할 수 있다. S720에서, 프로세서는 승객의 행동 패턴을 판단할 수 있다. 도 9는 승객의 행동 패턴을 분류한 실시 예를 설명하는 도면이다. 도 9를 참조하면, 프로세서는 제1 이미지(IMG1)에서와 같이 승객이 누워있는 행동 패턴을 판단할 수 있다. 또한, 프로세서는 제2 이미지(IMG2)에서와 같이 승객이 음식을 섭취하는 중인지를 판단할 수 있다. 또한, 프로세서는 제3 이미지(IMG3)에서와 같이 둘 이상의 승객들이 서로 대면하는지를 판단할 수 있다.S730에서, 프로세서는 승객의 착석 위치 판단할 수 있다. 도 10은 승객의 착석 위치를 분류하는 실시 예를 설명하는 도면이다. 도 10에서와 같이, 차량(VEH)의 좌석이 4개일 경우, 프로세서는 각각의 좌석을 구분할 수 있는 정보를 확 인할 수 있다. 예를 들어, 좌석들은 제1 내지 제4 시트들(S1~S4)로 구분될 수 있다. 프로세서는 이미지를 학습한 결과를 바탕으로, 승객이 착석한 위치를 확인할 수 있다. 또는, 프로세서는 레이더, 라이다, 시트에 위치한 압력 센서 등으로부터의 센싱 정보를 바 탕으로 승객의 착석 위치를 확인할 수 있다. 도 7을 바탕으로 설명된 절차에서 이미지 학습은 합성곱 신경망(convolutional Neural Network; CNN)을 이용하 는 방법일 수 있다. CNN은 시각적 영상을 분석하는 데 사용되는 다층의 피드-포워드적인 인공신경망의 한 종류 이다. 필터링 기법을 인공신경망에 적용하여 이미지를 효과적으로 처리할 수 있는 심층 신경망 기법으로 행렬로 표현된 필터의 각 요소가 데이터 처리에 적합하도록 자동으로 학습되는 과정을 통해 이미지를 분류하는 기법이 다. 프로세서는 사용자 유형, 신체 특징, 승객 위치, 행동 패턴 등을 미리 정의하고, 각각을 CNN 기반으로 학 습할 수 있다. 도 11은 이미지 학습을 위한 인공 신경망 구조를 모식화 한 도면이다. 도 11을 참조하면, 인공 신경망은 복수의 레이어들을 포함할 수 있고, 각각의 레이어들은 신경망의 뉴런에 대응 되는 하나 이상의 노드를 포함할 수 있다. 인공 신경망은 한 층의 노드와 다른 층의 노드 간을 연결하는 시냅스 를 포함할 수 있다. 인공 신경망에서 노드는 시냅스를 통해 입력되는 입력 신호들을 받고, 각 입력 신호들에 대 한 가중치 및 편향에 대한 활성 함수에 기초하여 출력 값을 생성할 수 있다. 각 노드의 출력 값은 시냅스를 통 해 다음 층의 입력 신호로 작용할 수 있다. 한 층의 모든 노드와 다음 층의 모든 노드가 시냅스를 통해 모두 연 결된 경우의 인공 신경망을 완전 연결된 인공 신경망이라 칭할 수 있다. 인공 신경망 모델의 파라미터는 학습을 통해 결정되는 파라미터를 의미하며, 시냅스 연결의 가중치와 뉴런의 편 향 등이 포함될 수 있다. 그리고, 하이퍼 파라미터는 기계 학습 알고리즘에서 학습 전에 설정되어야 하는 파라 미터를 의미하며, 학습률(Learning Rate), 반복 횟수, 미니 배치 크기, 초기화 함수 등이 포함될 수 있다. 도 12 내지 도 17은 본 발명의 사용자 인터페이스를 제어하는 구체적인 실시 예들을 나타내는 순서도이다. 도 12 내지 도 17은 도 7의 S710 이후의 절차들에 해당할 수 있다. 도 12는 S710에서 승객이 일반 성인으로 판단된 결과를 바탕으로 진행되는 절차일 수 있다. 도 12를 참조하여 실시 예에 의한 사용자 인터페이스를 제어하는 방법을 살펴보면 다음과 같다. S1201에서 승객이 앉아서 정면을 응시하는 것으로 판단될 경우, 프로세서는 승객이 착석한 위치를 판단할 수 있다. S1201, S1204, 및 S1208로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 1열에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 클러스터의 터치 스크린이 포함된 제3 디스플레이(530 c)를 포함하는 직접 조작계 입력 장치를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 제3 디스플레이(530c)를 활성화시킬 수 있다. 직접 조작계 입력 장치는 정확도가 높고 응답성이 빠르기 때문에, 승객이 직접 조작계 입력 장치를 수월하게 제어할 수 있는 일반 성인들일 경우, 프로세서는 직접 조작계 입력 장치를 활성화시킬 수 있다. S1201, S1204, S1205, 및 S1209로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 좌측에 착 석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수 있다. 또한, 프로세서는 프로젝션 타입의 제1 디스플레이(530a)를 활성화시킬 수 있고, 디스플레이 맵핑 영역은 1열 좌 측 시트의 후방으로 설정될 수 있다. 또는 프로세서는 출력 인터페이스 중에서 1열 좌측 시트에 결합된 제 2 디스플레이(530b1)를 활성화시킬 수 있다. S1201, S1204, S1205, S1206, 및 S1210로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 우측에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수 있 다. 또한, 프로세서는 프로젝션 타입의 제1 디스플레이(530a)를 활성화시킬 수 있고, 디스플레이 맵핑 영역은 1열 우측 시트의 후방으로 설정될 수 있다. 또는 프로세서는 출력 인터페이스 중에서 1열 우측 시트 에 결합된 제2 디스플레이(530b2)를 활성화시킬 수 있다. S1201, S1204, S1205, S1206, S1207 및 S1211로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 중간에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 클러스터의 제3 디스플레이(530c)를 활성화시킬 수 있다. S1202, 및 S1212에서, 승객이 누워있는 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인 식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 프로젝션 타입의 제1 디스플 레이(530a)를 활성화시킬 수 있고, 디스플레이의 맵핑 영역을 차량의 천장으로 설정할 수 있다. S1203, 및 S1213에서, 승객이 식사 중인 것으로 판단될 경우, 프로세서는 프로세서는 입력 인터페이 스 중에서 모션 인식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. 승객이 식사를 할 경우 승객은 테이블을 응시하고 있기 때문에 스피커를 출력 인터페 이스로 활용할 수 있다. 도 13은 S710에서 승객이 임산부로 판단된 결과를 바탕으로 진행되는 절차일 수 있다. 도 13을 참조하여 실시 예에 의한 사용자 인터페이스를 제어하는 방법을 살펴보면 다음과 같다. S1301에서 승객이 앉아서 정면을 응시하는 것으로 판단될 경우, 프로세서는 승객이 착석한 위치를 판단할 수 있다. S1301, S1304, 및 S1308로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 1열에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수 있다. 또한, 프로세서 는 출력 인터페이스 중에서 제3 디스플레이(530c)를 활성화시킬 수 있다. S1301, S1304, S1305, 및 S1309로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 좌측에 착 석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 프로젝션 타입의 제1 디스플레이(530a)를 활성화시킬 수 있고, 디스플 레이 맵핑 영역은 1열 좌측 시트의 후방으로 설정될 수 있다. 또는 프로세서는 출력 인터페이스 중에서 1 열 좌측 시트에 결합된 제2 디스플레이(530b1)를 활성화시킬 수 있다. S1301, S1304, S1305, S1306, 및 S1310로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 우측에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수 있 다. 또한, 프로세서는 프로젝션 타입의 제1 디스플레이(530a)를 활성화시킬 수 있고, 디스플레이 맵핑 영 역은 1열 우측 시트의 후방으로 설정될 수 있다. 또는 프로세서는 출력 인터페이스 중에서 1열 우측 시트 에 결합된 제2 디스플레이(530b2)를 활성화시킬 수 있다. S1301, S1304, S1305, S1306, S1307 및 S1311로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 중간에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 클러스터의 제3 디스플레이(530c)를 활성화시킬 수 있다. S1302, 및 S1312에서, 승객이 누워있는 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인 식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 프로젝션 타입의 제1 디스플 레이(530a)를 활성화시킬 수 있고, 디스플레이의 맵핑 영역을 차량의 천장으로 설정할 수 있다. S1303, 및 S1313에서, 승객이 식사 중인 것으로 판단될 경우, 프로세서는 프로세서는 입력 인터페이 스 중에서 모션 인식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. 도 14는 S710에서 승객이 노인으로 판단된 결과를 바탕으로 진행되는 절차일 수 있다. 도 14를 참조하여 실시 예에 의한 사용자 인터페이스를 제어하는 방법을 살펴보면 다음과 같다. S1401에서 승객이 앉아서 정면을 응시하는 것으로 판단될 경우, 프로세서는 승객이 착석한 위치를 판단할 수 있다. S1401, S1404, 및 S1408로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 1열에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1401, S1404, S1405, 및 S1409로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 좌측에 착 석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시킬 수 있다. 또한 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1401, S1404, S1405, S1406, 및 S1410로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 우측에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시킬 수 있다. 또한 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1401, S1404, S1405, S1406, S1407 및 S1411로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 중간에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시 킬 수 있다. 또한 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1402, 및 S1412에서, 승객이 누워있는 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인 식부를 활성화시킬 수 있다. 또한 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1403, 및 S1413에서, 승객이 식사 중인 것으로 판단될 경우, 프로세서는 프로세서는 입력 인터페이 스 중에서 음성 인식부를 활성화시킬 수 있다. 식사 중이라고 할지라도, 노인의 승객은 음성 인식부 를 이용하는 것이 모션 인식부를 이용하는 것보다 편할 수 있기 때문이다. 또한, 프로세서는 출력 인터페 이스 중에서 스피커를 활성화시킬 수 있다. 도 15는 S710에서 승객이 유아로 판단된 결과를 바탕으로 진행되는 절차일 수 있다. 즉, 도 15는 신체 사이즈가 작은 승객에 대한 사용자 인터페이스의 제어 방법을 설명하는 도면이다. 도 15를 참조하여 실시 예에 의한 사용자 인터페이스를 제어하는 방법을 살펴보면 다음과 같다. S1501에서 승객이 앉아서 정면을 응시하는 것으로 판단될 경우, 프로세서는 승객이 착석한 위치를 판단할 수 있다. S1501, S1504, 및 S1508로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 1열에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시킬 수 있다. 또한, 프로세 서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1501, S1504, S1505, 및 S1509로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 좌측에 착 석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시킬 수 있다. 프로세서는 출력 인터페이스 중에서 프로젝션 타입의 제1 디스플레이(530a)를 활성화시킬 수 있고, 디스플 레이 맵핑 영역은 1열 좌측 시트의 후방으로 설정될 수 있다. 또는 프로세서는 출력 인터페이스 중에서 1 열 좌측 시트에 결합된 제2 디스플레이(530b1)를 활성화시킬 수 있다. S1501, S1504, S1505, S1506, 및 S1510로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 우측에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시킬 수 있다. 프로세서는 출력 인터페이스 중에서 프로젝션 타입의 제1 디스플레이(530a)를 활성화시킬 수 있 고, 디스플레이 맵핑 영역은 1열 우측 시트의 후방으로 설정될 수 있다. 또는 프로세서는 출력 인터페이스 중에서 1열 우측 시트에 결합된 제2 디스플레이(530b2)를 활성화시킬 수 있다. S1501, S1504, S1505, S1506, S1507 및 S1511로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 중간에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시 킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 클러스터의 제3 디스플레이(530c)를 활성화시킬 수 있다. S1502, 및 S1512에서, 승객이 누워있는 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인 식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 프로젝션 타입의 제1 디스플레이 (530a)를 활성화시킬 수 있고, 디스플레이의 맵핑 영역을 차량의 천장으로 설정할 수 있다. S1503, 및 S1513에서, 승객이 식사 중인 것으로 판단될 경우, 프로세서는 프로세서는 입력 인터페이 스 중에서 모션 인식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다.도 16은 S710에서 승객이 시각장애 또는 팔 손실 장애인 것으로 판단된 결과를 바탕으로 진행되는 절차일 수 있 다. 도 16을 참조하여 실시 예에 의한 사용자 인터페이스를 제어하는 방법을 살펴보면 다음과 같다. S1601에서 승객이 앉아서 정면을 응시하는 것으로 판단될 경우, 프로세서는 승객이 착석한 위치를 판단할 수 있다. S1601, S1604, 및 S1608로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 1열에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시킬 수 있다. 또한, 프로세 서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1601, S1604, S1605, 및 S1609로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 좌측에 착 석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시킬 수 있다. 또한 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1601, S1604, S1605, S1606, 및 S1610로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 우측에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시킬 수 있다. 또한 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1601, S1604, S1605, S1606, S1607 및 S1611로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 중간에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인식부를 활성화시 킬 수 있다. 또한 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1602, 및 S1612에서, 승객이 누워있는 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 음성 인 식부를 활성화시킬 수 있다. 또한 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. S1603, 및 S1613에서, 승객이 식사 중인 것으로 판단될 경우, 프로세서는 프로세서는 입력 인터페이 스 중에서 음성 인식부를 활성화시킬 수 있다. 시각 장애이거나 팔 손실 장애일 경우, 식사 중일지라도 모션 인 식 보다 음성 인식이 과업완료시간이 줄어들 수 있기 때문에, 프로세서는 음성 인식부를 입력 인터페이스 로 결정할 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. 도 17은 S710에서 승객이 청각장애 판단된 결과를 바탕으로 진행되는 절차일 수 있다. 도 17을 참조하여 실시 예에 의한 사용자 인터페이스를 제어하는 방법을 살펴보면 다음과 같다. S1701에서 승객이 앉아서 정면을 응시하는 것으로 판단될 경우, 프로세서는 승객이 착석한 위치를 판단할 수 있다. S1701, S1704, 및 S1708로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 1열에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수 있다. 또한, 프로세서 는 출력 인터페이스 중에서 제3 디스플레이(530c)를 활성화시킬 수 있다. S1701, S1704, S1705, 및 S1709로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 좌측에 착 석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수 있다. 프로세 서는 출력 인터페이스 중에서 프로젝션 타입의 제1 디스플레이(530a)를 활성화시킬 수 있고, 디스플레이 맵핑 영역은 1열 좌측 시트의 후방으로 설정될 수 있다. 또는 프로세서는 출력 인터페이스 중에서 1열 좌 측 시트에 결합된 제2 디스플레이(530b1)를 활성화시킬 수 있다. S1701, S1704, S1705, S1706, 및 S1710로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 우측에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수 있 다. 프로세서는 출력 인터페이스 중에서 프로젝션 타입의 제1 디스플레이(530a)를 활성화시킬 수 있고, 디 스플레이 맵핑 영역은 1열 우측 시트의 후방으로 설정될 수 있다. 또는 프로세서는 출력 인터페이스 중에 서 1열 우측 시트에 결합된 제2 디스플레이(530b2)를 활성화시킬 수 있다. S1701, S1704, S1705, S1706, S1707 및 S1711로 이어지는 절차에서, 승객이 앉아서 정면을 응시하고, 사용자가 2열 중간에 착석한 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인식부를 활성화시킬 수있다. 또한, 프로세서는 출력 인터페이스 중에서 클러스터의 제3 디스플레이(530c)를 활성화시킬 수 있다. S1702, 및 S1712에서, 승객이 누워있는 것으로 판단될 경우, 프로세서는 입력 인터페이스 중에서 모션 인 식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 프로젝션 타입의 제1 디스플레이 (530a)를 활성화시킬 수 있고, 디스플레이의 맵핑 영역을 차량의 천장으로 설정할 수 있다. S1703, 및 S1713에서, 승객이 식사 중인 것으로 판단될 경우, 프로세서는 프로세서는 입력 인터페이 스 중에서 모션 인식부를 활성화시킬 수 있다. 또한, 프로세서는 출력 인터페이스 중에서 스피커를 활성화시킬 수 있다. 도 18은 본 발명의 일 실시 예에 따른 컴퓨팅 시스템을 도시한다. 도 18을 참조하면, 컴퓨팅 시스템은 버스를 통해 연결되는 적어도 하나의 프로세서, 메모리 , 사용자 인터페이스 입력 장치, 사용자 인터페이스 출력 장치, 스토리지, 및 네트워 크 인터페이스를 포함할 수 있다. 프로세서는 중앙 처리 장치(CPU) 또는 메모리 및/또는 스토리지에 저장된 명령어들에 대한 처 리를 실행하는 반도체 장치일 수 있으며, 승객의 유형에 따라 사용자 인터페이스부를 제어할 수 있다. 메모리 및 스토리지는 다양한 종류의 휘발성 또는 불휘발성 저장 매체를 포함할 수 있다. 예를 들어, 메 모리는 ROM(Read Only Memory) 및 RAM(Random Access Memory)을 포함할 수 있다. 따라서, 본 명세서에 개시된 실시 예들과 관련하여 설명된 방법 또는 알고리즘의 단계는 프로세서에 의해 실행되는 하드웨어, 소프트웨어 모듈, 또는 그 2 개의 결합으로 직접 구현될 수 있다. 소프트웨어 모듈은 RAM 메모리, 플래시 메모리, ROM 메모리, EPROM 메모리, EEPROM 메모리, 레지스터, 하드 디스크, 착탈형 디스크, CD-ROM과 같은 저장 매체(즉, 메모리 및/또는 스토리지)에 상주할 수도 있다. 예시적인 저장 매체는 프로세서에 커플링되며, 그 프로세서는 저장 매체로부터 정보를 판독할 수 있고 저장 매체에 정보를 기입할 수 있다. 다른 방법으로, 저장 매체는 프로세서와 일체형일 수도 있다. 프로세서 및 저장 매체는 주문형 집적회로(ASIC) 내에 상주할 수도 있다. ASIC는 사용자 단말기 내에 상주할 수 도 있다. 다른 방법으로, 프로세서 및 저장 매체는 사용자 단말기 내에 개별 컴포넌트로서 상주할 수도 있다. 이상의 설명은 본 발명의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 발명이 속하는 기술 분야에 서 통상의 지식을 가진 자라면 본 발명의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변형이 가 능할 것이다. 따라서, 본 발명에 개시된 실시 예들은 본 발명의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위한 것이 고, 이러한 실시 예에 의하여 본 발명의 기술 사상의 범위가 한정되는 것은 아니다. 본 발명의 보호 범위는 아 래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 발명의 권리범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17 도면18"}
{"patent_id": "10-2022-0118831", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시 예에 의한 사용자 인터페이스 제어 장치의 구성을 나타내는 도면이다. 도 2 및 도 3은 본 발명의 실시 예에 의한 사용자 인터페이스 제어 장치를 나타내는 도면이다. 도 4 및 도 5는 사용자 인터페이스 제어 장치가 차량에 설치된 실시 예를 나타내는 도면이다. 도 6을 참조하여 본 발명의 실시 예에 의한 사용자 인터페이스 제어 장치의 동작을 설명하는 순서도이다. 도 7은 승객의 유형을 판단하는 절차를 설명하는 순서도이다. 도 8은 프로세서가 이미지를 바탕으로 승객의 유형을 분류한 실시 예를 나타내는 도면이다. 도 9는 승객의 행동 패턴을 분류한 실시 예를 설명하는 도면이다. 도 10은 승객의 착석 위치를 분류하는 실시 예를 설명하는 도면이다. 도 11은 이미지 학습을 위한 인공 신경망 구조를 모식화 한 도면이다. 도 12 내지 도 17은 본 발명의 사용자 인터페이스를 제어하는 구체적인 실시 예들을 나타내는 순서도이다. 도 18은 본 발명의 일 실시 예에 따른 컴퓨팅 시스템을 나타내는 도면이다."}
