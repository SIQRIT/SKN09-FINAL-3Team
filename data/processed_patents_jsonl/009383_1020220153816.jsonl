{"patent_id": "10-2022-0153816", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0025440", "출원번호": "10-2022-0153816", "발명의 명칭": "3D 영상을 표시하기 위한 전자 장치 및 전자 장치의 동작 방법", "출원인": "삼성전자주식회사", "발명자": "이여울"}}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "베이스 패널(211);상기 베이스 패널(211) 상에 배치된 레이어 패널(212);적어도 하나의 명령어(instruction)를 저장하는 메모리(230); 및상기 메모리(230)에 저장된 상기 적어도 하나의 명령어를 실행하는 적어도 하나의 프로세서(220)를 포함하고,상기 적어도 하나의 프로세서(220)는 상기 적어도 하나의 명령어를 실행함으로써,입력 콘텐츠의 제1 프레임에서의 제1 프레임 영상 및 상기 제1 프레임의 직전 프레임인 제2 프레임에서의 제2프레임 영상을 획득하고,상기 제1 프레임 영상 및 상기 제2 프레임 영상을 영상 생성 모듈에 적용함으로써, 상기 제1 프레임에서 상기베이스 패널에 대응되는 베이스 영상 및 상기 제1 프레임에서 상기 레이어 패널에 대응되고, 상기 제1 프레임영상과 상기 제2 프레임 영상 간의 움직임에 기초한 레이어 영상을 생성하고,상기 베이스 영상 또는 상기 레이어 영상을 영상 보정 모듈에 적용함으로써, 상기 제1 프레임 영상과 상기 제2프레임 영상 간의 움직임에 기초한 보정-베이스 영상을 생성하며,상기 보정-베이스 영상을 상기 베이스 패널에 표시하고, 상기 레이어 영상을 상기 레이어 패널에 표시하는 전자장치(100)."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 전자 장치(100)는, 상기 베이스 패널(211)과 상기 레이어 패널(212) 사이에 배치된 광학층(213)을 더 포함하고,상기 베이스 영상은, 서로 다른 복수의 뷰(view)에 각각 대응되는 복수의 서브 베이스 영상들을 포함하고, 상기 적어도 하나의 프로세서(220)는,상기 베이스 영상 또는 상기 레이어 영상을 상기 영상 보정 모듈에 적용함으로써, 상기 서로 다른 복수의 뷰에따른 상기 복수의 서브 베이스 영상들 간의 차이에 기초하고, 복수의 서브 보정-베이스 영상들을 포함하는 상기보정-베이스 영상을 생성하는 전자 장치(100)."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 또는 제2 항 중 어느 하나의 항에 있어서,상기 영상 생성 모듈은, 상기 제1 프레임 영상 및 상기 제2 프레임 영상에 기초하여 상기 베이스 영상 및 상기레이어 영상을 추론하도록 훈련된 제1 인공 지능 모델을 포함하고,상기 제1 인공 지능 모델은,훈련용 입력 콘텐츠의 제1 프레임에서의 제1 훈련용 프레임 영상 및 상기 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 훈련용 프레임 영상을 획득하고,상기 제1 훈련용 프레임 영상 및 상기 제2 훈련용 프레임 영상 간의 움직임을 추출하고,상기 추출된 움직임을 이용하여, 기-생성된 상기 제2 프레임에서 상기 레이어 패널에 대응되는 비교 레이어 영상을 와핑(warping)하고,상기 제1 인공 지능 모델을 통하여 생성되는 상기 제1 프레임에서 상기 레이어 패널에 대응되는 훈련용 레이어공개특허 10-2024-0025440-3-영상과 상기 와핑된 비교 레이어 영상에 따른 제1 손실 함수에 기초하여 훈련된 인공 지능 모델인 전자 장치(100)."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,상기 제1 훈련용 프레임 영상 및 상기 제2 훈련용 프레임 영상들 각각은 서로 다른 복수의 뷰에서 획득된 복수의 훈련용 뷰 영상들을 포함하고,상기 제1 훈련용 프레임 영상 및 상기 제2 훈련용 프레임 영상 간의 움직임은, 상기 제1 훈련용 프레임 영상에포함된 상기 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된 센터 뷰 영상 및 상기 제2 훈련용 프레임 영상에포함된 상기 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된 센터 뷰 영상으로부터 추출되는 전자 장치(100)."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3 또는 제4 항 중 어느 하나의 항에 있어서,상기 제1 손실 함수는, 상기 훈련용 레이어 영상과 상기 와핑된 비교 레이어 영상의 차이에 따라 계산되며,상기 제1 인공 지능 모델은, 상기 제1 손실 함수에 기초하여, 상기 훈련용 레이어 영상과 상기 와핑된 비교 레이어 영상의 차이가 적어지도록 훈련되는 전자 장치(100)."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3 내지 제5 항 중 어느 하나의 항에 있어서,상기 제1 인공 지능 모델은,상기 제1 훈련용 프레임 영상 및 상기 제2 훈련용 프레임 영상을 상기 제1 인공 지능 모델에 적용함으로써, 상기 제1 프레임에서 상기 베이스 패널에 대응되는 훈련용 베이스 영상과, 상기 제1 프레임에서 상기 레이어 패널에 대응되는 상기 훈련용 레이어 영상을 생성하고,상기 훈련용 베이스 영상, 상기 훈련용 레이어 영상, 상기 베이스 패널의 밝기 정보 및 상기 레이어 패널의 밝기 정보를 토대로 상기 제1 프레임에서의 훈련용 출력 영상을 획득하고,상기 제1 훈련용 프레임 영상과 상기 훈련용 출력 영상 간의 차이에 따른 제2 손실 함수에 기초하여 훈련된 인공 지능 모델인 전자 장치(100)."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 또는 제2 항 중 어느 하나의 항에 있어서,상기 적어도 하나의 프로세서(220)는, 상기 적어도 하나의 명령어를 실행함으로써,상기 제1 프레임 영상 및 상기 레이어 영상을 상기 영상 보정 모듈에 적용하여 상기 보정-베이스 영상을 생성하는 전자 장치(100)."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제2 항에 있어서,상기 제1 프레임 영상은, 상기 서로 다른 복수의 뷰에서 획득된 복수의 뷰 영상들을 포함하고,상기 적어도 하나의 프로세서(220)는, 상기 적어도 하나의 명령어를 실행함으로써, 상기 제1 프레임 영상, 상기레이어 영상 및 상기 복수의 뷰에 각각 대응되는 복수의 쉬프트 값들을 상기 영상 보정 모듈에 적용하여, 상기보정-베이스 영상을 생성하며,상기 영상 보정 모듈은,상기 레이어 영상을 상기 복수의 쉬프트 값들 각각을 이용하여 쉬프트하고, 각각의 상기 복수의 뷰 영상들 및 상기 복수의 뷰 영상들에 각각 대응되는 복수의 쉬프트된 레이어 영상들을 이용하여, 상기 보정-베이스 영상을 생성하는 동작이나 기능을 수행하는 명령어를 포함하고,공개특허 10-2024-0025440-4-상기 복수의 쉬프트 값들은, 상기 베이스 패널(211)과 상기 레이어 패널(212) 간의 간격, 상기 레이어 패널(212)의 해상도 또는 상기 서로 다른 복수의 뷰 중 적어도 하나에 기초하여 결정되는 전자 장치(100)."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제2 항에 있어서,상기 영상 보정 모듈은, 상기 베이스 영상 및 기준 쉬프트 값에 기초하여, 상기 보정-베이스 영상을 추론하도록훈련된 제2 인공 지능 모델을 포함하고,상기 제2 인공 지능 모델은,상기 영상 생성 모듈을 통하여 생성되는, 상기 복수의 서로 다른 뷰에 각각 대응되는 복수의 서브 훈련용 베이스 영상들을 포함하는 훈련용 베이스 영상 및 상기 기준 쉬프트 값을 획득하고,상기 복수의 서브 훈련용 베이스 영상들 각각을 상기 기준 쉬프트 값을 이용하여 쉬프트하고,상기 제2 인공 지능 모델을 통하여 생성되는 보정-훈련용 베이스 영상과 상기 쉬프트된 훈련용 베이스 영상에따른 제3 손실 함수에 기초하여 훈련된 인공 지능 모델이며,상기 기준 쉬프트 값은, 상기 베이스 패널(211)과 상기 레이어 패널(212) 간의 간격 또는 상기 레이어 패널(212)의 해상도 중 적어도 하나에 기초하여 결정되는 전자 장치(100)."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9 항에 있어서,상기 제3 손실 함수는, 상기 보정-훈련용 베이스 영상에 포함된 복수의 서브 보정-훈련용 베이스 영상들 중 어느 하나의 제1 뷰에 대응되는 서브 보정-훈련용 베이스 영상과 상기 쉬프트된 훈련용 베이스 영상에 포함된 복수의 쉬프트된 서브 훈련용 베이스 영상들 중 상기 제1 뷰에 인접한 제2 뷰에 대응되는 서브 훈련용 베이스 영상을 쉬프트한 서브 훈련용 베이스 영상 간의 차이에 따라 계산되며,상기 제2 인공 지능 모델은, 상기 제3 손실 함수에 기초하여, 상기 서브 보정-훈련용 베이스 영상과 상기 쉬프트된 서브 훈련용 베이스 영상의 차이가 적어지도록 훈련되는 전자 장치(100)."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "베이스 패널(211) 및 상기 베이스 패널(211) 상에 배치된 레이어 패널(212)을 포함하는 전자 장치(100)의 동작방법에 있어서,입력 콘텐츠의 제1 프레임에서의 제1 프레임 영상 및 상기 제1 프레임의 직전 프레임인 제2 프레임에서의 제2프레임 영상을 획득하는 단계;상기 제1 프레임 영상 및 상기 제2 프레임 영상을 영상 생성 모듈에 적용함으로써, 상기 제1 프레임에서 상기베이스 패널(211)에 대응되는 베이스 영상 및 상기 제1 프레임에서 상기 레이어 패널(212)에 대응되고, 상기 제1 프레임 영상과 상기 제2 프레임 영상 간의 움직임에 기초한 레이어 영상을 생성하는 단계;상기 베이스 영상 또는 상기 레이어 영상을 영상 보정 모듈에 적용함으로써, 상기 제1 프레임 영상과 상기 제2프레임 영상 간의 움직임에 기초한 보정-베이스 영상을 생성하는 단계; 및상기 보정-베이스 영상을 상기 베이스 패널(211)에 표시하고, 상기 레이어 영상을 상기 레이어 패널(212)에 표시하는 단계를 포함하는 전자 장치(100)의 동작 방법."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 전자 장치(100)는 상기 베이스 패널(211)과 상기 레이어 패널(212) 사이에 배치된 광학층(213)을 더 포함하고,상기 베이스 영상은, 서로 다른 복수의 뷰(view)에 각각 대응되는 복수의 서브 베이스 영상들을 포함하고,상기 보정-베이스 영상을 생성하는 단계에서는,공개특허 10-2024-0025440-5-상기 베이스 영상 또는 상기 레이어 영상을 상기 영상 보정 모듈에 적용함으로써, 상기 서로 다른 복수의 뷰에따른 상기 복수의 서브 베이스 영상들 간의 차이에 기초하고, 복수의 서브 보정-베이스 영상들을 포함하는 상기보정-베이스 영상을 생성하는 전자 장치(100)의 동작 방법."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11 항 또는 제12 항 중 어느 하나의 항에 있어서,상기 영상 생성 모듈은, 상기 제1 프레임 영상 및 상기 제2 프레임 영상에 기초하여, 상기 베이스 영상 및 상기레이어 영상을 추론하도록 훈련된 제1 인공 지능 모델을 포함하고,상기 제1 인공 지능 모델은,훈련용 입력 콘텐츠의 제1 프레임에서의 제1 훈련용 프레임 영상 및 상기 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 훈련용 프레임 영상을 획득하는 단계;상기 제1 훈련용 프레임 영상 및 상기 제2 훈련용 프레임 영상 간의 움직임을 추출하는 단계;상기 추출된 움직임을 이용하여, 기-생성된 상기 제2 프레임에서 상기 레이어 패널(212)에 대응되는 비교 레이어 영상을 와핑(warping) 하는 단계; 및상기 제1 인공 지능 모델을 통하여 생성되는 상기 제1 프레임에서의 상기 레이어 패널(212)에 대응되는 훈련용레이어 영상과 상기 와핑된 비교 레이어 영상에 따른 제1 손실 함수에 기초하여, 상기 제1 인공 지능 모델을 훈련시키는 단계를 통하여 훈련된 인공 지능 모델인 전자 장치(100)의 동작 방법."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13 항에 있어서,상기 제1 훈련용 프레임 영상 및 상기 제2 훈련용 프레임 영상들 각각은 서로 다른 복수의 뷰에서 획득된 복수의 훈련용 뷰 영상들을 포함하고,상기 제1 훈련용 프레임 영상 및 상기 제2 훈련용 프레임 영상 간의 움직임을 추출하는 단계에서,상기 움직임은, 상기 제1 훈련용 프레임 영상에 포함된 상기 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된센터 뷰 영상 및 상기 제2 훈련 프레임 영상에 포함된 상기 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된 센터 뷰 영상으로부터 추출되는 전자 장치(100)의 동작 방법."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13 또는 제14 항 중 어느 하나의 항에 있어서,상기 제1 인공 지능 모델은,상기 제1 훈련용 프레임 영상 및 상기 제2 훈련용 프레임 영상을 상기 제1 인공 지능 모델에 적용함으로써, 상기 제1 프레임에서 상기 베이스 패널(211)에 대응되는 훈련용 베이스 영상 및 상기 제1 프레임에서 상기 레이어패널(212)에 대응되는 상기 훈련용 레이어 영상을 생성하는 단계;상기 훈련용 베이스 영상, 상기 훈련용 레이어 영상, 상기 베이스 패널(211)의 밝기 정보 및 상기 레이어 패널(212)의 밝기 정보에 기초하여 상기 제1 프레임에서의 훈련용 출력 영상을 획득하는 단계; 및상기 제1 훈련용 프레임 영상과 상기 훈련용 출력 영상의 차이에 따른 제2 손실 함수에 기초하여 상기 제1 인공지능 모델을 훈련시키는 단계를 통하여 훈련된 인공 지능 모델인 전자 장치(100)의 동작 방법."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11 또는 제12 항 중 어느 하나의 항에 있어서,상기 보정-베이스 영상을 생성하는 단계에서는,상기 제1 프레임 영상 및 상기 레이어 영상을 상기 영상 보정 모듈에 적용함으로써, 상기 보정-베이스 영상을생성하는 전자 장치(100)의 동작 방법.공개특허 10-2024-0025440-6-청구항 17 제12 항에 있어서,상기 제1 프레임 영상은, 상기 서로 다른 복수의 뷰에서 획득된 복수의 뷰 영상들을 포함하고,상기 보정-베이스 영상을 생성하는 단계에서는, 상기 제1 프레임 영상, 상기 레이어 영상 및 상기 복수의 뷰에각각 대응되는 복수의 쉬프트 값들을 상기 영상 보정 모듈에 적용함으로써, 상기 보정-베이스 영상을 생성하며,상기 영상 보정 모듈은,상기 레이어 영상을 상기 복수의 쉬프트 값들 각각을 이용하여 쉬프트하고,각각의 상기 복수의 뷰 영상들과 상기 복수의 뷰 영상들에 각각 대응되는 복수의 쉬프트된 레이어 영상들을 이용하여, 상기 보정-베이스 영상을 생성하는 동작이나 기능을 수행하는 명령어를 포함하고,상기 복수의 쉬프트 값들은, 상기 베이스 패널(211)과 상기 레이어 패널(212) 간의 간격, 상기 레이어 패널(212)의 해상도 또는 상기 서로 다른 복수의 뷰 중 적어도 하나에 기초하여 결정되는 전자 장치(100)의 동작 방법."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제12 항에 있어서,상기 영상 보정 모듈은, 상기 베이스 영상에 기초하여 상기 보정-베이스 영상을 추론하도록 훈련된 제2 인공 지능 모델을 포함하고,상기 제2 인공 지능 모델은,상기 영상 생성 모듈을 통하여 생성되는, 상기 복수의 서로 다른 뷰에 각각 대응되는 복수의 서브 훈련용 베이스 영상들을 포함하는 훈련용 베이스 영상 및 기준 쉬프트 값을 획득하는 단계;상기 복수의 훈련용 베이스 영상들 각각을 상기 기준 쉬프트 값을 이용하여 쉬프트하는 단계; 및상기 제2 인공 지능 모델을 통하여 생성되는 보정-훈련용 베이스 영상과 쉬프트된 훈련용 베이스 영상에 따른제3 손실 함수에 기초하여, 상기 제2 인공 지능 모델을 훈련시키는 단계를 통하여 훈련된 인공 지능 모델인 전자 장치(100)의 동작 방법."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18 항에 있어서,상기 제3 손실 함수에 기초하여 상기 제2 인공 지능 모델을 훈련시키는 단계는,상기 보정-훈련용 베이스 영상에 포함된 복수의 서브 보정-훈련용 베이스 영상들 중 어느 하나의 제1 뷰에 대응되는 서브 보정-훈련용 베이스 영상과 상기 쉬프트된 훈련용 베이스 영상에 포함된 복수의 쉬프트된 서브 훈련용 베이스 영상들 중 상기 제1 뷰에 인접한 제2 뷰에 대응되는 쉬프트된 서브 훈련용 베이스 영상 간의 차이에따라 상기 제3 손실 함수를 계산하는 단계; 및상기 제3 손실 함수에 기초하여, 상기 서브 보정-훈련용 베이스 영상과 상기 쉬프트된 서브 훈련용 베이스 영상의 차이가 적어지도록 상기 제2 인공 지능 모델을 훈련시키는 단계를 포함하는 전자 장치(100)의 동작 방법."}
{"patent_id": "10-2022-0153816", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제11 항 내지 제19 항 중 어느 한 항의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수있는 기록매체."}
{"patent_id": "10-2022-0153816", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 디스플레이, 적어도 하나의 명령어(instruction)를 저장하는 메모리 및 메모리에 저장된 적어도 하나 의 명령어를 실행하는 적어도 하나의 프로세서를 포함하고, 적어도 하나의 프로세서는 구성 요소를 포함하는 입 력 문서를 획득하고, 입력 문서로부터 구성 요소를 인식하고, 인식된 구성 요소의 종류에 기초하여 입력 문서의 유형을 분류하고, 분류된 입력 문서의 유형에 기초하여 식별된 순서에 따라 인식된 구성 요소를 하나의 행으로 정렬하여 보정 문서를 획득하며, 획득된 보정 문서를 디스플레이 상에 표시하는 전자 장치 및 전자 장치의 동작 방법을 포함한다."}
{"patent_id": "10-2022-0153816", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 3D 영상을 표시하기 위한 전자 장치 및 전자 장치의 동작 방법에 관한 것이다."}
{"patent_id": "10-2022-0153816", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "전자 기술의 발달에 힘입어 다양한 유형의 전자기기가 개발 및 보급 되고 있다. 영상을 표시하는 디스플레이 장 치를 포함하는 전자 장치는 최근 수년 간 급속도로 발전하고 있다. 디스플레이 장치가 발전함에 따라, 디스플레이 장치에서 표시하는 영상의 종류도 다양해졌다. 2D(two dimension) 영상뿐만 아니라 3D(three dimension) 영상까지 표시할 수 있는 디스플레이 장치가 개발되고 있다. 최근 들어, 3D 영상을 표시하기 위하여, 3D 공간에서 물체를 표시할 수 있는 볼류메트릭 디스플레이(volumetric display)를 이용하여 3D 영상을 표시하는 장치 및 방법이 제안되었다. 특히, 적층된 복수의 디스플레이 패널을 포함하고, 복수의 디스플레이 패널 각각에 영상을 표시하여 3D 영상을 제공하는 적층형 디스플레이(stacked display)가 제안되었다. 적층형 디스플레이는 서로 다른 뷰(view)에서 촬영된 라이트 필드(light field) 영상에 기초하여 획득한 복수의 영상들을 적층된 복수의 디스플레이 패널들에 각각 표시하여 3D 영상을 표시한다."}
{"patent_id": "10-2022-0153816", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따른 전자 장치는 베이스 패널, 베이스 패널 상에 배치된 레이어 패널 , 적어도 하나의 명령어(instruction)를 저장하는 메모리 및 메모리에 저장된 적어도 하나의 명 령어를 실행하는 적어도 하나의 프로세서를 포함할 수 있다. 적어도 하나의 프로세서는 적어도 하나 의 명령어를 실행함으로써, 입력 콘텐츠의 제1 프레임에서의 제1 프레임 영상 및 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 프레임 영상을 획득할 수 있다. 적어도 하나의 프로세서는 제1 프레임 영상 및 제2 프레임 영상을 영상 생성 모듈에 적용함으로써, 제1 프레임에서 베이스 패널에 대응되는 베이스 영상 및 제1 프 레임에서 레이어 패널에 대응되고, 제1 프레임 영상과 제2 프레임 영상 간의 움직임에 기초한 레이어 영상을 생 성할 수 있다. 적어도 하나의 프로세서는 베이스 영상 또는 레이어 영상을 영상 보정 모듈에 적용함으로써, 제1 프레임 영상과 제2 프레임 영상 간의 움직임에 기초한 보정-베이스 영상을 생성할 수 있다. 적어도 하나의 프로세서는 보정-베이스 영상을 베이스 패널에 표시하고, 레이어 영상을 레이어 패널에 표 시할 수 있다. 본 개시의 다른 실시예는, 베이스 패널 및 베이스 패널 상에 배치된 레이어 패널을 포함하는 전 자 장치의 동작 방법을 제공한다. 전자 장치의 동작 방법은 입력 콘텐츠의 제1 프레임에서의 제1 프 레임 영상 및 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 프레임 영상을 획득하는 단계를 포함할 수 있 다. 전자 장치의 동작 방법은 제1 프레임 영상 및 제2 프레임 영상을 영상 생성 모듈에 적용함으로써 제1 프레임에서 베이스 패널에 대응되는 베이스 영상 및 제1 프레임에서 레이어 패널에 대응되고, 제1 프 레임 영상과 제2 프레임 영상 간의 움직임에 기초한 레이어 영상을 생성하는 단계를 포함할 수 있다. 전자 장치 의 동작 방법은 베이스 영상 또는 상기 레이어 영상을 영상 보정 모듈에 적용함으로써, 상기 제1 프레임 영상과 상기 제2 프레임 영상 간의 움직임에 기초한 보정-베이스 영상을 생성하는 단계를 포함할 수 있다. 전자 장치의 동작 방법은 보정-베이스 영상을 베이스 패널에 표시하고, 레이어 영상을 레이어 패널에 표시하는 단계를 포함할 수 있다. 본 개시의 일 실시예로, 개시된 동작 방법의 실시예 중 적어도 하나의 방법을 컴퓨터에서 수행하기 위한 프로그 램이 기록된 컴퓨터로 읽을 수 있는 기록 매체를 제공할 수 있다."}
{"patent_id": "10-2022-0153816", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에서 사용되는 용어에 대해 간략히 설명하고, 본 개시의 일 실시예에 대해 구체적으로 설명하기로 한다. 본 개시에서 사용되는 용어는 본 개시의 일 실시예에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적 인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 본 개시의 실 시예의 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명 칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 개시 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 본 개시에 기재된 \"...부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 본 개시에서 사용된 표현 “~하도록 구성된(또는 설정된)(configured to)”은 상황에 따라, 예를 들면, “~에 적합한(suitable for)”, “~하는 능력을 가지는(having the capacity to)”, “~하도록 설계된(designed to) ”, “~하도록 변경된(adapted to)”, “~하도록 만들어진(made to)”, 또는 “~를 할 수 있는(capable of)” 과 바꾸어 사용될 수 있다. 용어 “~하도록 구성된(또는 설정된)”은 하드웨어적으로 “특별히 설계된 (specifically designed to)” 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, “~하도록 구성 된 시스템”이라는 표현은, 그 시스템이 다른 장치 또는 부품들과 함께 “~할 수 있는” 것을 의미할 수 있다. 예를 들면, 문구 “A, B, 및 C를 수행하도록 구성된(또는 설정된) 프로세서”는 해당 동작을 수행하기 위한 전 용 프로세서(예: 임베디드 프로세서), 또는 메모리에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 또한, 본 개시에서 일 구성요소가 다른 구성요소와 “연결된다” 거나 “접속된다” 등으로 언급된 때에는, 상 기 일 구성요소가 상기 다른 구성요소와 직접 연결되거나 또는 직접 접속될 수도 있지만, 특별히 반대되는 기재 가 존재하지 않는 이상, 중간에 또 다른 구성요소를 매개하여 연결되거나 또는 접속될 수도 있다고 이해되어야 할 것이다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시의 일 실시예는 여러 가지 상이한 형태 로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시의 일 실시예를 명 확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 본 개시 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 이하에서는 도면을 참조하여 본 개시의 실시예들을 상세하게 설명한다. 도 1은 본 개시의 일 실시예에 따른 전자 장치를 설명하기 위한 도면이다. 도 1을 참조하면, 본 개시의 일 실시예로 전자 장치는 디스플레이를 포함할 수 있다. 전자 장치(10 0)는 디스플레이를 통하여 영상을 표시할 수 있다. 일 실시예에서, 사용자의 위치에 따라 전자 장치가 디스플레이를 통해 사용자에게 제공하 는 영상은 달라질 수 있다. 일 실시예에서, 전자 장치가 제공하는 영상은 전자 장치를 사 용하는 사용자에게 입체감(three dimensional effect)을 제공할 수 있는 영상일 수 있다. 일 실시예에서, 전자 장치는 현실의 물체에서 반사되어 사용자에게 제공되는 광을 재현한다. 일 실시 예에서, 전자 장치는 현실의 물체에서 반사되어 사용자150)에게 제공되는 광과 동일한 경로를 갖는 광 을 사용자에게 제공한다. 사용자는 전자 장치에 표시되는 영상을 통하여 현실의 물체 로부터 반사되어 제공되는 광과 동일한 경로를 갖는 광을 제공받는다. 이에 따라, 사용자는 현실의 물체를 보는 것과 같이, 전자 장치에 표시되는 영상에 포함되는 물체의 입체감을 느낄 수 있다. 일 실시예에서, 전자 장치는 사용자의 위치에 따라 각각 다른 영상을 제공하여, 사용자가 영상에 포함된 물체의 입체감을 느끼게 할 수도 있다. 설명의 편의를 위하여, 영상이 도 1에 도시된 것처럼 육면체(hexahedron)의 형상을 가지는 물체를 포함하고 있는 것으로 설명한다. 일 실시예에서, 사용자가 전자 장치의 정면에 위치하는 경우에, 전자 장치는 사용자에게 물체의 정면을 포함하는 영상을 제공할 수 있다. 일 실시예에서, 사용자가 전자 장치의 정면이 아닌, 정면에 수직한 방향과 교차하는 제1 방향에 위치 하는 경우에, 전자 장치는 사용자에게 물체의 제1 측면 및 정면을 포함하는 영상을 제공할 수 있다. 일 실시예에서, 정면에 수직한 방향과 제1 방향 사이의 각도에 따라, 전자 장치가 사용자에게 제공하는 물체의 제1 측면 및 물체의 정면은 달라질 수 있다. 일 실시예에서, 정면에 수직한 방향과 제1 방향 사이의 각도에 따라, 전자 장치는 사용자에게 물체의 제1 측면만을 포함하는 영상을 제공할 수 도 있다. 일 실시예에서, 사용자가 전자 장치의 정면에 수직한 방향과 교차하고, 제1 방향과 다른 제2 방향에 위치하는 경우에, 전자 장치는 사용자에게 물체의 제1 측면과 다른 제2 측면 및 정면을 포함하는 영 상을 제공할 수 있다. 일 실시예에서, 정면에 수직한 방향과 제2 방향 사이의 각도에 따라, 전자 장치 가 사용자에게 제공하는 물체의 제2 측면 및 물체의 정면은 달라질 수 있다. 일 실시예에서, 정면에 수직한 방향과 제2 방향 사이의 각도에 따라, 전자 장치는 사용자에게 물체의 제2 측면만을 포함하는 영상160)을 제공할 수도 있다. 일 실시예에서, 물체의 제1 측면과 정면은 현실에서 사용자가 물체의 정면에 수직한 방향과 교차하는 제1 방향에서 물체를 바라볼 때 볼 수 있는 물체의 영역일 수 있다. 일 실시예에서, 물체의 제2 측면과 정면은 현실 에서 사용자가 물체의 정면에 수직한 방향과 교차하는 제2 방향에서 물체를 바라볼 때 볼 수 있는 물체의 영역일 수 있다. 일 실시예에서, 전자 장치는 영상을 통하여 사용자에게 현실에서 물체를 바라보는 것과 같은 광 을 제공할 수 있다. 따라서 사용자는 전자 장치가 표시하는 영상에 포함되는 물체의 입체 감을 느낄 수 있다. 일 실시예에서, 전자 장치는 사용자의 좌안(left eye)과 우안(right eye)에 각각 다른 영상을 제공하여, 사용자가 양안 시차(binocular disparity)를 느끼게 할 수 있다. 일 실시예에서, 제1 방향은 사 용자가 좌안을 통하여 전자 장치를 바라보는 방향일 수 있다. 제2 방향은 사용자가 우안을 통하 여 전자 장치를 바라보는 방향일 수 있다. 사용자는 우안과 좌안에 제공되는 영상이 서로 달라 양안 시차를 느낄 수 있고, 이를 통하여 물체의 입체감을 느낄 수도 있다. 일 실시예에서, 디스플레이는 복수의 패널들(110, 120)을 포함할 수 있다. 일 실시예에서, 복수의 패널들 (110, 120)은 적층되어 배치될 수 있다. 도 1에는 디스플레이가 두 개의 패널들(110, 120)을 포함하는 것으로 도시되어 있으나, 본 개시는 이에 제한되지 않는다. 디스플레이는 세 개 이상의 패널들을 포함할 수 도 있다. 또한, 디스플레이는 복수의 패널들 사이에 배치된 광학층(213, 도 3 참조)를 포함할 수도 있다. 전자 장치는 두 개의 패널들(110, 120) 각각에 후술할 베이스 영상 및 레이어 영상을 표시하여 사용자 에게 영상을 제공할 수 있다. 도 2는 본 개시의 일 실시예에 따른 전자 장치의 동작을 설명하기 위한 도면이다. 이하, 도 1에서 설명한 구성 과 동일한 구성에 대한 중복되는 설명은 생략하도록 한다. 도 2를 참조하면, 전자 장치는 디스플레이, 적어도 하나의 명령어(instruction)를 저장하는 메모리 및 적어도 하나의 프로세서를 포함할 수 있다. 적어도 하나의 프로세서는 메모리에 포함 된 적어도 하나의 명령어를 실행함으로써, 전자 장치의 동작을 제어할 수 있다. 일 실시예에서, 디스플레 이는 베이스 패널, 레이어 패널, 광학층 및 백라이트를 포함할 수 있다. 일 실시예에서, 입력 콘텐츠는 전자 장치를 통하여 사용자에게 제공하고자 하는 영상에 대 한 정보를 포함할 수 있다. 입력 콘텐츠는 복수의 프레임들(frames)에 각각 대응되는 복수의 프레임 영상 들을 포함할 수 있다. 입력 콘텐츠는 복수의 프레임 영상들이 순차적으로 나열되어 표시되는 영상에 대한 정보를 포함할 수 있다. 일 실시예에서, 입력 콘텐츠는 서로 다른 복수의 뷰(view)에서 현실의 물체를 촬영하여 획득된 정보일 수 있다. 입력 콘텐츠는 복수의 프레임 동안 서로 다른 복수의 뷰(view)에서 현실의 물체를 촬영하여 획득된 정보일 수 있다. 다만, 본 개시는 이에 제한되지 않고, 입력 콘텐츠는 사용자에게 서로 다른 복수의 뷰에서의 물체의 영상을 제공하기 위하여 생성된 정보를 포함할 수도 있다. 일 실시예에서, 복수의 프레임 동안 현실의 물체가 움직이는 경우에 입력 콘텐츠에 포함된 각각의 복수의 프레임 영상들에 포함된 물체들의 위치는 서로 다를 수 있다. 일 예로, 복수의 프레임 동안 현실의 물체가 움직 이는 경우, 각각의 복수의 프레임 영상들에 포함된 물체들의 위치는 서로 다를 수 있다. 복수의 프레임 동안 현 실의 물체의 일부가 움직이는 경우, 각각의 복수의 프레임 영상들에 포함된 물체들 중 현실의 물체가 움직이는 부분에 대응되는 부분의 위치는 서로 다를 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 입력 콘텐츠의 제1 프레임에서의 제1 프레임 영상(240_1) 및 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 프레임 영상(240_2)을 획득할 수 있다. 입력 콘텐츠(24 0)가 제1 프레임 및 제2 프레임 동안 움직이는 영상에 대한 정보를 포함하는 경우, 제1 프레임 영상 (240_1)에 포함된 물체의 위치와, 제2 프레임 영상(240_2)에 포함된 물체의 위치는 서로 다를 수 있다. 일 실시예에서, 입력 콘텐츠는 물체를 서로 다른 복수의 뷰에서 촬영하여 획득된 복수의 뷰 영상들을 포함 할 수 있다. 일 실시예에서, 복수의 뷰 영상들 각각은 서로 다른 위치에 배치되어 서로 다른 뷰에서 물체를 촬 영하는 복수의 카메라를 통하여 획득된 영상일 수 있다. 또한, 일 실시예에서, 복수의 뷰 영상들 각각은, 마이 크로 렌즈 어레이(micro lens array)를 포함하는 카메라를 통하여, 서로 다른 뷰에서 획득된 영상일 수 있다. 이하, 설명의 편의를 위하여 복수의 뷰 영상들은 서로 다른 뷰에서 물체를 촬영하는 복수의 카메라를 통하여 획 득된 영상들로 정의한다. 일 실시예에서, 입력 콘텐츠에 포함된 제1 프레임 영상(240_1) 및 제2 프레임 영상(240_2) 각각은 서로 다 른 복수의 뷰를 갖는 영상들을 포함할 수 있다. 도 2에는 사람의 얼굴을 포함하는 입력 콘텐츠가 도시되어 있다. 입력 콘텐츠는 제1 프레임 및 제2 프레임 동안 표정이 변하는 사람의 얼굴을 촬영하여 획득된 정보를 포함할 수 있다. 제1 프레임 영상(240_1)에포함된 사람의 얼굴 표정과 제2 프레임 영상(240_2)에 포함된 사람의 얼굴 표정은 서로 상이할 수 있다. 다만, 도 2의 도시는 설명의 편의를 위한 것이며, 물체는 사람의 얼굴 형상으로 제한되지 않는다. 입력 콘텐츠는 다양한 종류의 물체를 촬영하여 획득한 정보를 포함할 수 있다. 일 실시예에서, 전자 장치는 입력 콘텐츠를 제공받는다. 일 실시예에서, 전자 장치에 포함된 적 어도 하나의 프로세서는 제1 프레임 영상(240_1) 및 제2 프레임 영상(240_2)을 제공받을 수 있다. 적어도 하나의 프로세서는 메모리에 포함된 적어도 하나의 명령어(instruction)를 실행하여, 입력 콘 텐츠의 제1 프레임에서의 영상인 제1 프레임 영상(240_1) 및 입력 콘텐츠의 제2 프레임에서의 영상인 제2 프레임 영상(240_2)에 기초하여 제1 프레임에서 디스플레이에 표시할 보정-베이스 영상 및 레이어 영 상을 생성할 수 있다. 적어도 하나의 프로세서는 보정-베이스 영상을 베이스 패널에 표시하고, 레이어 영상을 레이어 패널 에 표시하여 사용자에게 촬영한 물체를 재현하는 영상을 제공할 수 있다. 이하, 적어도 하나의 프로세서가 보정-베이스 영상 및 레이어 영상을 생성하는 것에 대하여는 도 4 내지 도 12b에서 후술하도록 한다. 일 실시예에서, 도 2에는 디스플레이가 백라이트(back light, 214) 및 광학층을 포함하는 것으로 도 시되어 있다. 일 실시예에서, 백라이트는 광을 생성하여 사용자에게 제공할 수 있다. 일 실시예에서, 백라이트가 생성하는 광은 백색 광일 수 있다. 다만, 본 개시는 이에 한정되지 않고, 백라이트가 제 공하는 광은 백색이 아닌, 다른 색을 가질 수도 있다. 일 실시예에서, 베이스 패널 및 레이어 패널 각각은 복수의 픽셀들을 포함할 수 있다. 일 실시예에서, 베이스 패널 및 레이어 패널이 액정 표시(liquid crystal) 디스플레이인 경우, 베이스 패널 및 레이어 패널 각각은 복수의 컬러 필터(color filter)들을 포함하는 필터층일 수 있다. 일 실 시예에서, 복수의 픽셀들 각각은 복수의 컬러 필터들에 대응될 수 있다. 일 실시예에서, 베이스 패널 또는 레이어 패널 중 적어도 하나는 복수의 레드, 그린 및 블루 픽셀들 을 포함할 수도 있다. 일 실시예에서, 베이스 패널 또는 레이어 패널 중 적어도 하나는 복수의 레드, 그린, 블루 컬러 픽셀들 및 광을 필터링하지 않는 개구부들을 포함할 수도 있다. 일 실시예에서, 베이스 패널 또는 레이어 패널 중 적어도 하나는 복수의 옐로우, 블루 픽셀 등을 포함할 수도 있다. 다만, 본 개시는 이에 한정되지 않는다. 일 실시예에서, 백라이트에서 제공하는 광의 파장 및 백라이트 에서 제공되는 광을 이용하여 영상을 표시하기 위한 색의 조합 등에 따라 베이스 패널 및 레이 어 패널에 포함되는 복수의 픽셀들의 색은 달라질 수 있다. 일 실시예에서, 광학층은 베이스 패널 및 레이어 패널 사이에 배치될 수 있다. 광학층에 의하여 베이스 패널을 투과한 광은 굴절(refraction), 반사(reflection), 분산(dispersion) 등이 된 후 레이어 패널에 제공될 수 있다. 도 2에는 광학층이 베이스 패널과 레이어 패널 사이에 배치된 것으로 도시되었으나, 본 개시는 이에 한정되지 않는다. 일 실시예에서, 디스플레이가 세 개 이상 의 패널들을 포함할 경우, 광학층은 베이스 패널과 레이어 패널 사이가 아닌, 다른 곳에 배치될 수도 있다. 또한, 일 실시예에서, 광학층은 생략될 수도 있다. 이하, 설명의 편의를 위하여, 광학층 을 지나는 광은 굴절되는 것으로 설명한다. 일 실시예에서, 백라이트에서 제공되는 광은 베이스 패널 및 레이어 패널을 각각 투과하여 사용자에게 제공될 수 있다. 일 실시예에서, 백라이트에서 생성된 광은 베이스 패널에 포함된 어느 하나의 픽셀 및 레이어 패널에 포함된 어느 하나의 픽셀을 거쳐 사용자에게 제공될 수 있다. 백라이트에서 생성된 광 이 지나가는 베이스 패널의 픽셀 및 레이어 패널의 픽셀 각각의 색 또는 투과율 중 적어도 하나 에 따라 사용자에게 제공되는 광의 파장 및 세기가 결정된다. 백라이트에서 생성된 광이 지나가는 베이스 패널의 픽셀과 레이어 패널의 픽셀의 조합에 따라 사용자에게 제공되는 광 의 파장 및 세기가 결정된다. 이때, 사용자의 위치에 따라, 사용자에게 도달하기 위하여 백라이트에서 생성된 광이 지나 가는 베이스 패널의 픽셀과 레이어 패널의 픽셀이 달라진다. 구체적으로, 사용자의 위치에 따라, 사용자에게 도달하기 위하여 백라이트에서 생성된 광이 지나가는 베이스 패널의 픽셀 및 레이어 패널의 픽셀 각각의 색 또는 투과율 등이 달라질 수 있다. 일 실시예에서, 사용자의 위 치에 따라 사용자에 도달하기 위하여 백라이트에서 생성된 광이 지나가는 베이스 패널의 픽셀과 레이어 패널의 픽셀의 조합이 달라질 수 있다. 일 실시예에서, 디스플레이가 광학층을 포함하는 경우, 백라이트에서 제공되는 광은 베이 스 패널을 투과되고, 광학층에 의하여 굴절된 후 레이어 패널을 투과하여 사용자에게 제공 될 수 있다. 광학층에 의하여 굴절되어 경로가 변경된 광은, 변경된 경로에 대응되는 위치의 사용자 에게 제공될 수 있다. 일 실시예에서, 광학층은 적어도 하나의 렌즈를 포함할 수 있다. 도 2에는 광학층이 세 개의 렌즈를 포함하는 것으로 도시되었으나, 본 개시는 이에 제한되지 않는다. 광학층은 두 개 이하의 렌즈, 또는 네 개 이상의 렌즈를 포함할 수도 있다. 일 실시예에서, 베이스 패널에 포함된 픽셀과 광학층에 포함된 적어도 하나의 렌즈의 위치 관계, 적 어도 하나의 렌즈의 개수, 형상 및 굴절률에 따라, 베이스 패널을 지나간 광이 굴절되어 레이어 패널 을 향하는 경로가 변경될 수 있다. 광학층에 의하여 경로가 굴절된 광은, 레이어 패널을 투과한 후, 해당 경로에 대응되는 위치에서 전자 장치를 보는 사용자에게 제공될 수 있다. 따라서, 사용자 의 위치에 따라 서로 다른 베이스 패널의 픽셀과 레이어 패널의 픽셀을 지나간 광이 사용자 에게 제공되어, 사용자에게 물체의 다른 면을 포함하는 영상을 제공할 수 있다. 일 실시예에서, 디스플레이가 광학층을 포함하는 경우, 광학층을 포함하지 않는 경우와 비교할 때, 베이스 패널을 투과한 광의 경로가 전자 장치의 정면에 수직한 방향과 이루는 각도가 커질 수 있 다. 따라서 사용자가 전자 장치를 통하여 영상을 시인할 수 있는 시야각(Viewing Angle)이 커질 수 있다. 또한, 광학층에 포함된 적어도 하나의 렌즈의 개수, 형상, 굴절률 등을 조절하여, 사용자에 게 제공하는 복수의 뷰의 개수 및 인접한 뷰의 간격을 다르게 할 수도 있다. 이때, '뷰'는 사용자가 전자 장치를 통하여 표시되는 영상에 포함된 물체의 서로 다른 면을 볼 수 있는 위치에 대응될 수 있다. 전자 장치는 사용자의 위치가 복수의 뷰 중 어느 하나의 뷰에 대응 됨에 따라, 사용자에게 서로 다른 물체의 면을 포함하는 영상을 제공하여 사용자가 입체감을 느 끼도록 할 수 있다. 이하, 설명의 편의를 위하여, 베이스 패널을 투과하는 광을 베이스 패널에 표시되는 베이스 영상이라 지칭하고, 레이어 패널을 투과하는 광을 레이어 패널에 표시되는 레이어 영상이라 지칭한다. 이때, 베이스 영상은 베이스 패널을 투과하는 광에 대응되는 복수의 픽셀들의 투과율 및 색 등의 정보를 포함할 수도 있다. 레이어 영상은 레이어 패널을 투과하는 광에 대응되는 복수의 픽셀들의 투과율 및 색 등의 정 보를 포함할 수도 있다. 일 실시예에서, 베이스 패널에 포함되는 베이스 영상은 서로 다른 복수의 뷰에 각각 대응되는 복수의 서브 베이스 영상들을 포함할 수 있다. 복수의 서브 베이스 영상들 각각에서 나온 광은 광학층에 의하여 굴절된 후 레이어 패널로 제공될 수 있다. 복수의 서브 베이스 영상들 각각에서 나온 광은, 광학층에 의하여, 서로 다른 경로로 굴절되어 레이어 패널로 제공될 수 있다. 서로 다른 경로로 레이어 패널로 제공된 광은, 레이어 영상의 서로 다른 영역을 투과한 후 각각의 뷰에서 사용자에게 제공될 수 있다. 따라서, 전자 장치는 사용자의 위치 변화에 대응하여 달라지는 영상을 사용자에게 제공할 수 있다. 사용자는 사용자의 위치 변화에 대응하여 전자 장치가 제공하는 영상이 달라지는 것에 기초하여, 영상에 포함된 물체의 입체감을 느낄 수 있다. 일 실시예에서, 사용자가 전자 장치의 정면에 수직한 방향과 교차하는 제1 방향에 위치하는 경우에 전자 장치가 사용자에게 제공하는 영상과 사용자가 전자 장치의 정면에 수직한 방향 과 교차하고, 제1 방향과 다른 제2 방향에 위치하는 경우에 전자 장치가 사용자에게 제공하는 영상 은 서로 다를 수 있다. 일 실시예에서, 전자 장치는 제1 방향에 위치한 사용자에게, 제1 방향에서 물체를 바라보는 것처럼 느끼게 하는 영상을 제공할 수 있다. 일 실시예에서, 전자 장치는 제2 방향에 위치한 사용자에 게, 제2 방향에서 물체를 바라보는 것처럼 느끼게 하는 영상을 제공할 수 있다. 따라서 사용자는 전 자 장치가 표시하는 영상에 포함된 물체의 입체감을 느낄 수 있다.다만, 본 개시는 이에 한정되지 않고, 베이스 패널 또는 레이어 패널 중 적어도 하나는 유기 발광 다 이오드 디스플레이, 무기 발광 다이오드 디스플레이 등과 같이, 베이스 패널 및 레이어 패널 각각이 스스로 광을 생성하는 디스플레이일 수도 있다. 베이스 패널 및 레이어 패널이 모두 스스로 광을 생 성하는 패널인 경우, 디스플레이는 백라이트를 포함하지 않을 수 있다. 이 경우, 전자 장치는 각각의 패널에서 생성되는 광의 세기, 파장 및 각 서브 패널의 투과율 등을 조절하여 사용자의 위치에 따 라 다른 영상을 사용자에게 제공할 수도 있다. 도 3은 본 개시의 일 실시예에 따른 전자 장치의 구성을 설명하기 위한 블록도이다. 이하, 도 1 및 도 2에서 설 명한 구성과 동일한 구성에 대한 중복되는 설명은 생략하도록 한다. 도 2 및 도 3을 참조하면, 일 실시예에서, 전자 장치는 디스플레이, 메모리, 적어도 하나의 프 로세서 및 통신 인터페이스를 포함할 수 있다. 그러나, 도 3에 도시된 구성 요소가 모두 필수 구성 요소인 것은 아니다. 도 3에 도시된 구성 요소보다 많은 구성 요소에 의해 전자 장치가 구현될 수도 있고, 그보다 적은 구성 요소에 의해서도 전자 장치는 구현될 수 있다. 디스플레이, 메모리, 적어도 하나의 프로세서 및 통신 인터페이스는 각각 전기적 및/또는 물리적으로 서로 연결될 수 있다. 일 실시예에서, 디스플레이는 액정 표시(liquid crystal) 디스플레이, 플라즈마(plasma) 디스플레이, 유기 발광 다이오드(organic light emitting diodes) 디스플레이, 무기 발광 다이오드(inorganic light emitting diodes) 디스플레이 중 어느 하나의 디스플레이를 포함할 수 있다. 다만, 본 개시는 이에 제한되지 않고, 디스 플레이는 사용자에게 영상을 제공할 수 있는 다른 종류의 디스플레이를 포함할 수 있다. 일 실시예에서, 디스플레이는 베이스 패널, 레이어 패널, 광학층 및 백라이트를 포함 할 수 있다. 그러나, 디스플레이는 도 2에 도시된 구성 요소보다 많은 구성 요소를 포함할 수도 있고, 그 보다 적은 구성 요소를 포함할 수도 있다. 일 예로, 디스플레이는 베이스 패널 및 레이어 패널 만을 포함할 수도 있다. 또한, 디스플레이는 광학층, 백라이트 외에 세 개 이상의 패널들을 포 함할 수도 있다. 일 실시예에서, 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀 티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), Mask ROM, Flash ROM 등), 하드 디스크 드라이브(HDD) 또는 솔리드 스테이트 드라이브(SSD) 중 적어도 하나를 포함할 수 있다. 메모리에는 전자 장치의 기능 또는 동작들을 수행하기 위한 명령어들 또는 프로그램 코드가 저장될 수 있다. 메모리에 저장되는 명령어들, 알고리즘, 데이터 구조, 프로그램 코드 및 애플리케이션 프 로그램은 예를 들어, C, C++, 자바(Java), 어셈블러(assembler) 등과 같은 프로그래밍 또는 스크립팅 언어로 구현될 수 있다. 일 실시예에서, 메모리에는 디스플레이를 통하여 영상을 사용자에게 제공하는데 이용될 수 있는 다양한 종류의 모듈들이 저장될 수 있다. 메모리에는 입력 콘텐츠 획득 모듈, 영상 생성 모듈 및 영상 보정 모듈이 저장될 수 있다. 그러나, 도 3에 도시된 모듈 모두가 필수 모듈인 것은 아니다. 메모리에는 도 3에 도시된 모듈보다 더 많은 모듈들이 저장될 수도 있고, 그보다 적은 모듈들이 저장될 수도 있다. 일 실시예에서, 메모리에는 획득한 입력 콘텐츠를 전처리 하기 위한 모듈 등이 더 저장 될 수도 있다. 메모리에 포함되는 '모듈'은 적어도 하나의 프로세서에 의해 수행되는 기능이나 동작을 처리하는 단 위를 의미할 수 있다. 메모리에 포함되는 '모듈'은 명령어들(instructions), 알고리즘, 데이터 구조, 또는 프로그램 코드와 같은 소프트웨어로 구현될 수 있다. 일 실시예에서, 입력 콘텐츠 획득 모듈은 입력 콘텐츠를 획득하는 동작이나 기능에 관한 명령어들 또 는 프로그램 코드로 구성될 수 있다. 입력 콘텐츠 획득 모듈은 외부의 서버 또는 주변의 전자 장치들로부 터 입력 콘텐츠를 수신하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실 시예에서, 입력 콘텐츠 획득 모듈은 입력 콘텐츠의 제1 프레임에서의 제1 프레임 영상(240_1) 및 제2 프레임에서의 제2 프레임 영상(240_2)을 획득하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 영상 생성 모듈은 획득한 입력 콘텐츠에 기초하여, 제1 프레임에서 베이스 패널(21 1)에 대응되는 베이스 영상 및 제1 프레임에서 레이어 패널에 대응되는 레이어 영상을 생성하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 레이어 영상은 제1 프레임에서 레 이어 패널에 표시되는 영상일 수 있다. 일 실시예에서, 영상 생성 모듈은 팩토리제이션(factorization)을 수행하여 베이스 영상 및 레이어 영상을 생성하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 영상 생성 모 듈은 입력 콘텐츠에 기초하여, 팩토리제이션(factorization)을 수행하여 베이스 영상 및 레이어 영상 을 추론하는 제1 인공 지능 모델을 포함할 수 있다. 일 실시예에서, 영상 생성 모듈에 포함된 제1 인공 지 능 모델은 머신 러닝(machine learning) 또는 딥 러닝(deep learning) 모델을 포함할 수 있다. 일 실시예에서, 레이어 영상 생성 모듈에 포함된 제1 인공 지능 모델은 제1 프레임 영상(240_1) 및 제2 프 레임 영상(240_2)에 기초하여, 제1 프레임에서 베이스 패널에 대응되는 베이스 영상 및 제1 프레임에서 레이어 패널에 대응되는 레이어 영상을 추론하기 위하여 훈련(trained)된 인공 지능 모델일 수 있다. 일 실시예에서, 전자 장치는 영상 생성 모듈에 포함된 제1 인공 지능 모델을 훈련시킬 수 있다. 전자 장치는 영상 생성 모듈에 포함된 제1 인공 지능 모델을 훈련시키기 위하여, 사전 학습 모델을 이용한 전이 학습(transfer learning)을 할 수도 있다. 다만, 본 개시는 이에 한정되지 않고, 영상 생성 모듈은 팩토리제이션을 수행하여 제1 프레임 영상(240_1) 및 제2 프레임 영상(240_2)에 기초하여 제1 프레임에서의 베 이스 영상 및 레이어 영상을 추론하기 위해 학습된 제1 인공지능 모델을 통신 인터페이스를 통하여 외부의 서버 또는 주변의 전자 장치들로부터 수신할 수도 있다. 이하, 영상 생성 모듈의 동작 및 영상 생성 모듈 에 포함된 제1 인공 지능 모델의 훈련은 도 6 내지 도 11에서 후술하도록 한다. 일 실시예에서, 영상 보정 모듈은 베이스 영상 또는 레이어 영상에 기초하여, 제1 프레임 영상과 제2 프레 임 영상 간의 움직임에 기초한 보정-베이스 영상을 생성하는 동작이나 기능에 관한 명령어들 또는 프로그램 코 드로 구성될 수 있다. 일 실시예에서, 영상 보정 모듈은 베이스 영상 또는 레이어 영상에 기초하여, 제1 프레임 영상과 제2 프레임 영상 간의 움직임이 반영되고, 제1 프레임에서 베이스 패널에 표시되는 보정-베이스 영상을 생성할 수 있다. 일 실시예에서, 디스플레이가 광학층을 포함하고, 베이스 영상이 서로 다른 복수의 뷰에 대응되는 복 수의 서브 베이스 영상을 포함하는 경우, 영상 보정 모듈은 베이스 영상 또는 레이어 영상에 기초하여, 복 수의 서브 베이스 영상들 간의 차이에 기초하고, 복수의 서브 보정-베이스 영상들을 포함하는 보정-베이스 영상 을 생성하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 영상 보정 모듈은 베이스 영상 또는 레이어 영상에 기초하여, 복수의 서브 베이스 영상들 간의 차이가 반영되고, 제1 프레임에서 서로 다른 복수의 뷰에 대응되어 베이스 패널에 표시되는 보정-베이스 영상을 생성할 수 있다. 일 실시예에서, 영상 보정 모듈은 베이스 영상에 기초하여 보정-베이스 영상을 추론하는 제2 인공 지능 모 델을 포함할 수 있다. 일 실시예에서, 영상 보정 모듈에 포함된 제2 인공 지능 모델은 머신 러닝(machine learning) 또는 딥 러닝(deep learning) 모델을 포함할 수 있다. 일 실시예에서, 영상 보정 모듈에 포함된 제2 인공 지능 모델은 베이스 영상에 기초하여, 제1 프레임에서 베이스 패널에 표시되는 보정-베이스 영상을 추론하기 위하여 훈련(trained)된 인공 지능 모델일 수 있다. 일 실시예에서, 전자 장치는 영상 보정 모듈에 포함된 제2 인공 지능 모델을 훈련시킬 수 있다. 전자 장치는 영상 보정 모듈에 포함된 제2 인공 지능 모델을 훈련시키기 위하여, 사전 학습 모델을 이용한 전이 학습(transfer learning)을 할 수도 있다. 다만, 본 개시는 이에 한정되지 않고, 영상 보정 모듈은 베이스 영상에 기초하여 보정-베이스 영상을 추론하기 위해 학습된 제2 인공지능 모델을 통신 인터페이스 를 통하여 외부의 서버 또는 주변의 전자 장치들로부터 수신할 수도 있다. 이하, 영상 보정 모듈의 동작 및 영상 보정 모듈에 포함된 제2 인공 지능 모델의 훈련은 도 6 내지 도 9, 도 12a 및 도 12b에서 후술하 도록 한다. 일 실시예에서, 영상 생성 모듈에 포함된 제1 인공 지능 모델 및 영상 보정 모듈에 포함된 제2 인공 지능 모델은 복수의 신경망 레이어들을 포함할 수 있다. 각각의 신경망 레이어는 복수의 가중치(weight value s)을 가지고 있으며, 이전(previous) 신경망 레이어의 연산 결과와 복수의 가중치의 연산을 통해 현재(present) 신경망 레이어의 연산을 수행할 수 있다. 인공 지능 모델의 예로는, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network), 심층 Q-네트워크 (Deep Q-Networks), GAN(Generative Adversarial Networks), VAE(Variational Auto Encoder) 등이 있으며, 본 개시에 서의 제1 인공 지능 모델 및 제2 인공 지능 모델은 전술한 예에 한정되지 않는다. 일 실시예에서, 메모리에는 획득한 입력 콘텐츠를 전처리하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성된 전처리 모듈이 더 저장될 수 있다. 전처리 모듈은 재구성(wrangling), 변환 (transformation), 통합(integration), 클리닝(cleaning), 축소(reduction), 이산화(discretization) 등을 수 행하여 입력 콘텐츠를 전처리하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 영상 생성 모듈은 전처리 모듈을 통하여 전처리된 입력 콘텐츠에 기초하여 베이스 영상 및 레이어 영 상을 생성할 수도 있다. 일 실시예에서, 통신 인터페이스는 적어도 하나의 프로세서의 제어에 의해 외부의 서버와 데이터 통 신을 수행할 수 있다. 또한, 통신 인터페이스는 외부의 서버뿐 아니라, 다른 주변 전자 장치들과도 데이터 통신을 수행할 수 있다. 통신 인터페이스는 예를 들어, 유선 랜, 무선 랜(Wireless LAN), 와이파이(Wi- Fi), 블루투스(Bluetooth), 지그비(zigbee), WFD(Wi-Fi Direct), 적외선 통신(IrDA, infrared Data Association), BLE (Bluetooth Low Energy), NFC(Near Field Communication), 와이브로(Wireless Broadband Internet, Wibro), 와이맥스(World Interoperability for Microwave Access, WiMAX), SWAP(Shared Wireless Access Protocol), 와이기그(Wireless Gigabit Allicance, WiGig) 및 RF 통신을 포함하는 데이터 통신 방식 중 적어도 하나를 이용하여 서버 또는 다른 주변 전자 장치들과 데이터 통신을 수행할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 통신 인터페이스를 통하여 입력 콘텐츠를 외부의 서 버 또는 주변의 전자 장치들로부터 수신할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 통신 인터 페이스를 통하여 생성한 보정-베이스 영상 및 레이어 영상을 외부의 서버 또는 주변의 전자 장치들로 송신 할 수도 있다. 도 4는 본 개시의 일 실시예에 따른 전자 장치의 동작을 설명하기 위한 순서도이다. 도 2, 도 3 및 도 4를 참조하면, 일 실시예에서, 전자 장치의 동작 방법은, 입력 콘텐츠의 제1 프레 임에서의 제1 프레임 영상(240_1) 및 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 프레임 영상(240_2)을 획득하는 단계(S100)를 포함할 수 있다. 적어도 하나의 프로세서는 입력 콘텐츠 획득 모듈을 실행하 여, 제1 프레임 영상(240_1) 및 제2 프레임 영상(240_2)을 획득할 수 있다. 일 실시예에서, 전자 장치의 동작 방법은, 제1 프레임 영상(240_1) 및 제2 프레임 영상(240_2)을 영상 생 성 모듈에 적용함으로써, 제1 프레임에서 베이스 패널에 대응되는 베이스 영상 및 제1 프레임에서 레 이어 패널에 대응되고, 제1 프레임 영상과 제2 프레임 영상 간의 움직임에 기초한 레이어 영상을 생성하는 단계(S200)를 포함할 수 있다. 적어도 하나의 프로세서는 영상 생성 모듈을 실행하여, 제1 프레임 영 상(240_1) 및 제2 프레임 영상(240_2)에 기초하여 제1 프레임에서 베이스 패널에 대응되는 베이스 영상 및 제1 프레임에서 레이어 패널에 대응되고, 제1 프레임 영상과 제2 프레임 영상 간의 움직임에 기초한 레이 어 영상을 생성할 수 있다. 이하, 베이스 영상 및 레이어 영상은 도 6 내지 도 8에서 자세히 설명하도록 한다. 일 실시예에서, 전자 장치의 동작 방법은, 베이스 영상 또는 레이어 영상을 영상 보정 모듈에 적용함 으로써, 제1 프레임 영상과 제2 프레임 영상 간의 움직임에 기초한 보정-베이스 영상을 생성하는 단계(S300)를 포함할 수 있다. 적어도 하나의 프로세서는 영상 보정 모듈을 실행하여, 베이스 영상 또는 레이어 영 상에 기초하여 제1 프레임 영상과 제2 프레임 영상 간의 움직임에 기초한 보정-베이스 영상을 생성할 수 있다. 일 실시예에서, 보정-베이스 영상을 생성하는 단계(S300)에서, 제1 프레임 영상 및 레이어 영상을 영상 보정 모 듈에 적용함으로써 보정-베이스 영상을 생성할 수도 있다. 적어도 하나의 프로세서는 영상 보정 모듈 을 실행하여, 제1 프레임 영상 및 레이어 영상에 기초하여 보정-베이스 영상을 생성할 수도 있다. 일 실시예에서, 보정-베이스 영상을 생성하는 단계(S300)에서, 베이스 영상에 기초하여 보정-베이스 영상을 생 성할 수도 있다. 적어도 하나의 프로세서는 영상 보정 모듈을 실행하여, 베이스 영상에 기초하여 보 정-베이스 영상을 생성할 수도 있다. 이하, 보정-베이스 영상은 도 6 내지 도 8에서 자세히 설명하도록 한다. 일 실시예에서, 전자 장치의 동작 방법은, 보정-베이스 영상을 베이스 패널에 표시하고, 레이어 영상 을 레이어 패널에 표시하는 단계(S400)를 포함할 수 있다. 적어도 하나의 프로세서는 보정-베이스 영 상을 표시하도록 베이스 패널을 제어하고, 레이어 영상을 표시하도록 레이어 패널을 제어할 수 있다. 도 5는 본 개시의 일 실시예에 따른 광학층을 포함하는 전자 장치의 동작을 설명하기 위한 순서도이다. 이하, 도 4에서 설명한 단계와 동일한 단계에 대하여는, 동일한 도면 부호를 부여하고 설명은 생략하도록 한다. 도 2, 도 3 및 도 5를 참조하면, 전자 장치가 베이스 패널과 레이어 패널 사이에 배치된 광학층 을 더 포함하는 경우, 베이스 영상은 서로 다른 복수의 뷰에 각각 대응되는 복수의 서브 베이스 영상들을 포함할 수 있다. 베이스 패널을 통하여 제공되는 복수의 서브 베이스 영상들 각각은 광학층에 의하여 서로 다른 방향으로 굴절되어 레이어 패널에 제공될 수 있다. 일 실시예에서, 베이스 영상 및 레이어 영상을 생성하는 단계(S200a)에서는, 제1 프레임 영상 및 제2 프레임 영 상을 영상 생성 모듈에 적용함으로써, 제1 프레임에서 베이스 패널에 대응되고, 서로 다른 복수의 뷰 에 각각 대응되는 복수의 서브 베이스 영상들을 포함하는 베이스 영상 및 레이어 영상을 생성할 수 있다. 적어 도 하나의 프로세서는 영상 생성 모듈을 실행하여, 제1 프레임 영상 및 제2 프레임 영상에 기초하여, 제1 프레임에서 베이스 패널에 대응되고, 서로 다른 복수의 뷰에 각각 대응되는 복수의 서브 베이스 영상 들을 포함하는 베이스 영상 및 제1 프레임에서 레이어 패널에 대응되는 레이어 영상을 생성할 수 있다. 일 실시예에서, 보정-베이스 영상을 생성하는 단계(S300a)에서는, 베이스 영상 및 레이어 영상을 영상 보정 모 듈에 적용함으로써, 서로 다른 복수의 뷰에 따른 복수의 서브 베이스 영상들 간의 차이에 기초하고, 복수 의 서브 보정-베이스 영상들을 포함하는 보정-베이스 영상을 생성할 수 있다. 적어도 하나의 프로세서는 영상 보정 모듈을 실행하여, 베이스 영상 및 레이어 영상에 기초하여 서로 다른 복수의 뷰에 따른 복수의 서브 베이스 영상들 간의 차이에 기초하고, 복수의 서브 보정-베이스 영상들을 포함하는 보정-베이스 영상을 생 성할 수 있다. 이하, 전자 장치가 광학층을 포함하는 경우의 베이스 영상 및 보정-베이스 영상에 대 하여는 도 7 및 도 8에서 후술하도록 한다. 도 6은 본 개시의 일 실시예에 따른 레이어 영상 및 보정-베이스 영상을 생성하는 전자 장치의 동작을 설명하기 위한 도면이다. 도 7은 본 개시의 일 실시예에 따른, 제1 프레임 영상과 제2 프레임 영상 간의 움직임을 설명하 기 위한 도면이다. 이하 도 3에서 설명한 구성과 동일한 구성에 대한 중복되는 설명은 생략하도록 한다. 도 6은 전자 장치가 베이스 패널 및 레이어 패널을 포함하는 경우의 전자 장치의 동작을 설명하기 위한 도면이다. 이때, 전자 장치는 베이스 패널 및 레이어 패널에 광을 제공하는 백라 이트를 더 포함할 수도 있다. 도 3, 도 4 및 도 6을 참조하면, 일 실시예에서, 적어도 하나의 프로세서는 입력 콘텐츠를 획득할 수 있다. 입력 콘텐츠는 입력 콘텐츠의 제1 프레임에서의 제1 프레임 영상(630_1) 및 입력 콘텐츠 의 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 프레임 영상(630_2)을 포함할 수 있다. 제1 프레임 영상 (630_1)은 서로 다른 복수의 뷰에서 획득된 복수의 뷰 영상들을 포함할 수 있다. 제2 프레임 영상(630_2) 은 서로 다른 복수의 뷰에서 획득된 복수의 뷰 영상들을 포함할 수 있다. 일 실시예에서, 입력 콘텐츠는 복수의 프레임들 동안 움직이는 물체에 대한 정보를 포함할 수 있다. 다만, 본 개시는 이에 제한되지 않고, 입력 콘텐츠는 복수의 프레임들 동안 물체의 형상, 크기 또는 물체 중 일 부(예를 들어, 사람의 얼굴의 표현 변화 등)가 변하는 물체에 대한 정보를 포함할 수도 있다. 따라서, 입력 콘 텐츠에 포함된 복수의 프레임들 중 제1 프레임에서의 제1 프레임 영상에 포함된 정보와 및 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 프레임 영상에 포함된 정보는 상이할 수 있다. 도 6 및 도 7을 참조하면, 도 7에는 제1 프레임 영상(731_1) 및 제2 프레임 영상(732_1)이 도시되어 있다. 제1 프레임 영상(731_1) 및 제2 프레임 영상(732_1)에는 각각 제1 물체(733_1, 733_2) 및 제2 물체(734_1, 734_2) 의 정보가 포함되어 있다. 일 실시예에서, 입력 콘텐츠에 포함된 제1 물체는 복수의 프레임들 동안 움직이는 물체일 수 있다. 입력 콘텐츠에 포함된 제2 물체는 복수의 프레임들 동안 움직이지 않는 물체일 수 있다. 따라서, 제1 프레임 영상(731_1)에 포함된 제1 물체(733_1)의 위치와 제2 프레임 영상(732_1)에 포함된 제1 물체(733_2)의 위치는 서로 상이할 수 있다. 제1 프레임 영상(731_1)에 포함된 제2 물체(734_1)와 제2 프레임 영상(732_1)에 포함된 제2 물체(734_2)의 위치는 서로 동일할 수 있다. 일 실시예에서, 제1 프레임 영상(731_1)과 제2 프레임 영상(732_1)을 비교하여 제1 프레임 영상(731_1)과 제2 프레임 영상(732_1) 간의 움직임이 계산될 수 있다. 제1 프레임 영상(731_1)과 제2 프레임 영상(732_1) 간 의 움직임은 연속되는 프레임인 제1 프레임 영상(731_1)과 제2 프레임 영상(732_2)에 기초하여 계산된 광 학 흐름(Optical Flow)을 의미할 수 있다. 일 실시예에서, 제1 프레임 영상(731_1)과 제2 프레임 영상(732_1) 간의 움직임은, 제1 프레임 영상 (731_1)에 포함된 제1 물체(733_1)와 제2 프레임 영상(732_1)에 포함된 제1 물체(733_2)의 움직임에 대한 정보 를 포함할 수 있다. 제1 프레임 영상(731_1)과 제2 프레임 영상(732_1) 간의 움직임은, 제2 프레임에서 제 1 프레임동안 제1 물체가 이동한 방향 및 거리에 대한 정보를 포함할 수 있다. 이하, 제1 프레임 영상 (731_1)과 제2 프레임 영상(732_1) 간의 움직임을 계산하는 방법에 대하여는, 도 12a에서 후술하도록 한다. 일 실시예에서, 적어도 하나의 프로세서는 제1 프레임 영상(630_1) 및 제2 프레임 영상(630_2)을 영상 생 성 모듈에 적용함으로써, 디스플레이에 대응되는 패널 영상을 생성할 수 있다. 디스플레이(21 0)에 베이스 패널 및 레이어 패널이 포함된 경우, 제1 프레임 영상(630_1) 및 제2 프레임 영상 (630_2)을 영상 생성 모듈에 적용함으로써, 베이스 패널에 대응되는 베이스 영상(640_2) 및 레이어 패널에 대응되는 레이어 영상(640_1)을 생성할 수 있다. 일 실시예에서, 베이스 영상(640_2) 및 레이어 영상(640_1)은, 각각 베이스 패널 및 레이어 패널에 표시되어, 베이스 영상(640_2)과 레이어 영상(640_1)의 조합으로 입력 콘텐츠에 포함된 물체를 재현하는, 입체감 있는 영상(260, 도 2 참조)을 사용자(250, 도 2 참조)에게 제공할 수 있는 영상이다. 일 실시예에서, 베이스 패널과 레이어 패널이 모두 액정 표시 디스플레이와 같은 비-자발광 디스플레 이(non-self-emitting display)인 경우, 베이스 영상(640_2)과 레이어 영상(640_1)이 합해진 영상이 사용 자에게 제공될 수 있다. 일 실시예에서, 레이어 패널이 유기 발광 다이오드 디스플레이, 무기 발광 다이오드 디스플레이 등과 같은 자발광 디스플레이(self-emitting display)인 경우, 베이스 영상(640_2)과 레이어 영상(640_1)이 곱해진 영상 이 사용자에게 제공될 수 있다. 일 실시예에서, 영상 생성 모듈은 제1 프레임 영상(630_1) 및 제2 프레임 영상(630_2)에 기초하여, 베이스 영상(640_2) 및 레이어 영상(640_1)을 추론하도록 훈련된 제1 인공 지능 모델을 포함할 수 있다. 일 실시예에서, 제1 인공 지능 모델은 제1 프레임 영상(630_1) 및 제2 프레임 영상(630_2)에 기초하여, 제1 프 레임에서 베이스 패널에 대응되는 베이스 영상(640_2) 및 제1 프레임에서 레이어 패널에 대응되고, 제1 프레임 영상(630_1)과 제2 프레임 영상(630_2) 간의 움직임에 기초한 레이어 영상(640_1)을 추론하도 록 훈련된 인공 지능 모델일 수 있다. 일 실시예에서, 제1 인공 지능 모델은 제1 프레임 영상(630_1) 및 제2 프레임 영상(630_2)에 기초하여, 제1 프 레임 영상(630_1)과 제2 프레임 영상(630_2) 간의 움직임을 반영한 레이어 영상(640_1)을 추론하도록 훈련 된 모델일 수 있다. 일 실시예에서, 제1 프레임 영상(630_1)에 포함된 물체 및 제2 프레임 영상(630_2)에 포함된 물체는 각각 베이 스 영상 및 레이어 영상의 조합에 의하여 영상으로 재현된다. 제1 프레임 영상(630_1)에 포함된 물체의 위치와 제2 프레임 영상(630_2)에 포함된 물체의 위치가 서로 상이할 때, 제1 프레임 영상(630_1)에 기초하여 생성되는 제1 프레임에서의 베이스 영상과 제2 프레임 영상(630_2)에 기초하여 생성되는 제2 프레임에서의 베이스 영상도 서로 달라질 수 있다. 이때, 제1 프레임에서의 베이스 영상과 제2 프레임에서의 베이스 영상은 제2 프레임에서 제1 프레임으로의 움직 임이 반영되지 않고, 각각 제1 프레임 영상(630_1)과 제2 프레임 영상(630_2)에만 기초하여 생성되므로, 제1 프 레임에서의 베이스 영상과 제2 프레임에서의 베이스 영상에 각각 포함된 물체의 위치 정보뿐만 아니라, 물체의 휘도, 계조, 색상 등의 정보도 서로 상이할 수 있다. 또한, 제1 프레임 영상(630_1)에 기초하여 생성되는 제1 프레임에서의 레이어 영상과 제2 프레임 영상(630_2)에 기초하여 생성되는 제2 프레임에서의 레이어 영상도 각각 제1 프레임에서의 베이스 영상과 제2 프레임에서의 베 이스 영상에 대응되도록 생성될 수 있다. 따라서, 제1 프레임에서의 레이어 영상과 제2 프레임에서의 레이어 영 상에 각각 포함된 물체의 위치 정보뿐만 아니라, 물체의 휘도, 계조, 색상 등의 정보도 서로 상이할 수 있다. 일 실시예에서, 제1 프레임에서의 레이어 영상 및 베이스 영상에 포함된 물체의 휘도, 계조 또는 색상 중 적어 도 하나의 정보와 제2 프레임에서의 레이어 영상 및 베이스 영상에 포함된 물체의 휘도, 계조 또는 색상 중 적 어도 하나의 정보가 상이할 경우, 제2 프레임에서 제1 프레임동안 영상에 포함된 물체의 움직임 외에, 물 체의 휘도, 계조 또는 색상 중 적어도 하나가 변하여 사용자에게 플리커(flicker)가 시인될 수 있다. 따라서 사용자의 시인성 및 영상의 품질이 낮아질 수 있다. 본 개시의 영상 생성 모듈은 제1 프레임 영상(630_1)과 제2 프레임 영상(630_2) 간의 움직임을 반영 하여 제1 프레임에서의 레이어 영상(640_1)을 추론하도록 훈련된 제1 인공 지능 모델을 포함함에 따라, 제1 프 레임 영상(630_1)에 포함된 물체의 위치와 제2 프레임 영상(630_2)에 포함된 물체의 위치가 서로 상이할 때, 제 2 프레임에서의 레이어 영상과 비교하여, 물체의 위치 정보만이 상이하고, 물체의 휘도, 계조, 색상 등의 정보 는 제2 프레임에서의 레이어 영상과 차이가 적은 제1 프레임에서의 레이어 영상을 생성할 수 있다. 이하, 제1 인공 지능 모델의 훈련 방법은 도 11 내지 도 12b에서 후술하도록 한다. 일 실시예에서, 영상 보정 모듈은 제1 프레임 영상(630_1)과 레이어 영상(640_1)에 따라 보정-베이스 영상 을 생성하는 동작이나 기능을 수행하기 위한 명령어 또는 프로그램 코드를 포함할 수 있다. 일 실시예에서, 영상 보정 모듈은 베이스 패널과 레이어 패널의 디스플레이 종류에 기초하여, 제1 프레임 영상(630_1)과 레이어 영상(640_1)에 따라 보정-베이스 영상을 생성하는 동작이나 기능을 수행 하기 위한 명령어 또는 프로그램 코드를 포함할 수 있다. 일 실시예에서, 제1 프레임 영상(630_1)은 전자 장치가 베이스 패널에 표시되는 보정-베이스 영상 과 레이어 패널에 표시되는 레이어 영상(640_1)의 조합에 의하여 생성하여 사용자에게 제공하고 자 하는 영상에 대응되는 영상일 수 있다. 일 실시예에서, 베이스 패널과 레이어 패널이 모두 액정 표시 디스플레이와 같은 비-자발광 디스플레 이(non-self-emitting display)인 경우, 영상 보정 모듈은 제1 프레임 영상(630_1)과 레이어 영상(640_1) 간의 차이에 따라 보정-베이스 영상을 생성하는 동작이나 기능을 수행하기 위한 명령어 또는 프로그램 코 드를 포함할 수 있다. 일 실시예에서, 레이어 패널이 유기 발광 다이오드 디스플레이, 무기 발광 다이오드 디스플레이 등과 같은 자발광 디스플레이(self-emitting display)인 경우, 영상 보정 모듈은 제1 프레임 영상(630_1)에서 레이 어 영상(640_1)을 나누어 베이스 영상을 생성하는 동작이나 기능을 수행하기 위한 명령어 또는 프로그램 코드를 포함할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 레이어 영상(640_1) 및 제1 프레임 영상(630_1)을 영상 보정 모 듈에 적용함으로써, 제1 프레임 영상(630_1)과 제2 프레임 영상(630_2) 간의 움직임에 기초한 보정-베이스 영상을 생성할 수 있다. 보정-베이스 영상은 제1 프레임에서 베이스 패널에 대응되는 영상일 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 보정-베이스 영상을 베이스 패널에 표시하고, 레이어 영상(640_1)을 레이어 패널에 표시함으로써, 제2 프레임에서 제1 프레임동안 움직이는 물체를 포함하는 입 력 콘텐츠가 제공되더라도, 제2 프레임과 비교할 때 제1 프레임에서 물체의 위치 정보만이 상이하고, 물체 의 휘도, 계조, 색상 등은 동일한 영상을 사용자에게 제공하여, 사용자의 시인성 및 영상 의 품질을 향상시킬 수 있다. 도 8은 본 개시의 일 실시예에 따른 레이어 영상 및 복수의 서브 보정-베이스 영상들을 포함하는 보정-베이스 영상을 생성하는 전자 장치의 동작을 설명하기 위한 도면이다. 도 9는 본 개시의 일 실시예에 따른, 제2 인공 지능 모델을 포함하는 영상 보정 모듈을 통하여 보정-베이스 영상을 생성하는 전자 장치의 동작을 설명하기 위 한 도면이다. 도 10은 본 개시의 일 실시예에 따른, 쉬프트 값을 설명하기 위한 도면이다. 이하, 도 3 및 도 6 에서 설명한 구성과 동일한 구성에 대한 중복되는 설명은 생략하도록 한다. 도 2 및 도 8을 참조하면, 도 8은 전자 장치가 베이스 패널, 레이어 패널 및 베이스 패널 과 레이어 패널 사이에 배치되는 광학층을 포함하는 경우의 전자 장치의 동작을 설명하기 위한 도면이다. 이때, 전자 장치는 베이스 패널 및 레이어 패널에 광을 제공하는 백라이트를 더 포함할 수도 있다. 도 3, 도 4, 도 6 및 도 8을 참조하면, 일 실시예에서, 적어도 하나의 프로세서는 입력 콘텐츠를 획 득할 수 있다. 입력 콘텐츠는 입력 콘텐츠의 제1 프레임에서의 제1 프레임 영상(830_1) 및 입력 콘텐 츠의 제2 프레임에서의 제2 프레임 영상(830_2)을 포함할 수 있다. 제1 프레임 영상(830_1)은 서로 다른 복수의 뷰에서 획득된 복수의 뷰 영상들을 포함할 수 있다. 제2 프레임 영상(830_2)은 서로 다른 복수의 뷰에서 획득된 복수의 뷰 영상들을 포함할 수 있다.일 실시예에서, 적어도 하나의 프로세서는 제1 프레임 영상(830_1) 및 제2 프레임 영상(830_2)을 영상 생 성 모듈에 적용함으로써, 디스플레이에 대응되는 패널 영상을 생성할 수 있다. 적어도 하나의 프로세서는 제1 프레임 영상(830_1) 및 제2 프레임 영상(830_2)을 영상 생성 모듈에 적용함으로써, 베이스 패널에 대응되는 베이스 영상(840_2) 및 레이어 패널에 대응되는 레이어 영상(840_1)을 생성 할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 디스플레이에 포함된 베이스 패널, 레이어 패널 및 광학층에 대한 정보를 영상 생성 모듈에 더 적용할 수도 있다. 이 경우, 적어도 하나의 프로세서 는 베이스 패널, 레이어 패널 및 광학층에 대한 정보를 참조하여, 제1 프레임 영상(830_1) 및 제2 프레임 영상(830_2)에 기초하여 베이스 영상(840_2) 및 레이어 영상(840_1)을 생성할 수 있다. 일 실시예에서, 베이스 패널에 대한 정보는 베이스 패널의 해상도, 디스플레이 종류 또는 밝기 중 적 어도 하나에 대한 정보를 포함할 수 있다. 레이어 패널에 대한 정보는 레이어 패널의 해상도, 디스플 레이 종류 또는 밝기 중 적어도 하나에 대한 정보를 포함할 수 있다. 광학층에 대한 정보는 광학층에 포함된 적어도 하나의 렌즈의 개수, 적어도 하나의 렌즈의 크기 또는 적어도 하나의 렌즈의 굴절률 중 적어도 하나에 대한 정보를 포함할 수 있다. 일 실시예에서, 베이스 영상(840_2)은 서로 다른 복수의 뷰에 각각 대응되는 복수의 서브 베이스 영상들을 포함할 수 있다. 복수의 서브 베이스 영상들 각각은 광학층에 의하여 굴절된 후, 레이어 패널에 표시되는 레이어 영상(840_1)의 서로 다른 영역을 투과하여, 굴절된 경로에 대응되는 뷰에 위치한 사용자 에게 제공될 수 있다. 일 실시예에서, 복수의 서브 베이스 영상들 각각은 서로 다른 뷰에서 사용자에 게 물체의 다른 면을 포함하는 영상을 제공하기 위한 베이스 영상들이다. 일 실시예에서, 복수의 서브 베이스 영상들 각각은 서로 다른 뷰에서 물체의 다른 면을 포함하는 영상 을 제공하기 위한 베이스 영상들이므로, 복수의 서브 베이스 영상들 각각에 포함된 물체의 위치 정보 는 상이할 수 있다. 또한, 복수의 서브 베이스 영상들 각각에 포함된 물체의 휘도, 계조, 색상 등의 정보 도 서로 상이할 수 있다. 따라서 복수의 서브 베이스 영상들을 포함하는 베이스 영상(840_2)을 베이스 패 널에 표시할 경우, 뷰의 변화에 따라 물체의 휘도, 계조 또는 색상 중 적어도 하나가 변하여 사용자 에게 플리커(flicker)가 시인될 수 있다. 따라서 사용자의 시인성 및 영상의 품질이 낮아질 수 있다. 일 실시예에서, 영상 생성 모듈은 제1 프레임 영상(830_1) 및 제2 프레임 영상(830_2)에 기초하여, 복수의 서브 베이스 영상들을 포함하는 베이스 영상(840_2) 및 레이어 영상(840_1)을 추론하도록 훈련된 제1 인공 지능 모델을 포함할 수 있다. 일 실시예에서, 제1 인공 지능 모델은 제1 프레임 영상(830_1) 및 제2 프레임 영상(830_2)에 기초하여, 제1 프 레임에서 베이스 패널에 대응되고, 복수의 서브 베이스 영상들을 포함하는 베이스 영상(840_2) 및 제 1 프레임에서 레이어 패널에 대응되고, 제1 프레임 영상(830_1)과 제2 프레임 영상(830_2) 간의 움직임 (735, 도 7 참조)에 기초한 레이어 영상(840_1)을 추론하도록 훈련된 인공 지능 모델일 수 있다. 일 실시예에서, 영상 보정 모듈은 레이어 영상(840_1)을 복수의 쉬프트 값들 각각을 이용하여 쉬프트 하여 복수의 쉬프트 레이어 영상들을 생성하고, 제1 프레임 영상(830_1)과 생성된 복수의 쉬프트 레이어 영상들 에 따라 복수의 서브 보정-베이스 영상들을 포함하는 보정-베이스 영상을 생성하는 동작이나 기능을 수행하기 위한 명령어 또는 프로그램 코드를 포함할 수 있다. 일 실시예에서, 영상 보정 모듈은 제1 프레임 영상(830_1)에 포함된 복수의 뷰 영상들 각각과 복수의 뷰 영상들에 각각 대응되도록 쉬프트된 복수의 쉬프트 레이어 영상들에 따라 복수의 서브 보정-베이스 영 상들을 포함하는 보정-베이스 영상을 생성하는 동작이나 기능을 수행하기 위한 명령어 또는 프로그램 코드를 포함할 수 있다. 일 실시예에서, 영상 보정 모듈은 베이스 패널과 레이어 패널의 디스플레이 종류에 기초하여, 복수의 뷰 영상들과 복수의 쉬프트 레이어 영상들에 따라 보정-베이스 영상을 생성하는 동작이나 기 능을 수행하기 위한 명령어 또는 프로그램 코드를 포함할 수 있다. 일 실시예에서, 베이스 패널과 레이어 패널이 모두 액정 표시 디스플레이와 같은 비-자발광 디스플레 이(non-self-emitting display)인 경우, 영상 보정 모듈은 복수의 뷰 영상들 각각과 대응되는 쉬프 트 레이어 영상의 차이에 따라 복수의 서브 보정-베이스 영상들을 포함하는 보정-베이스 영상을 생성하는 동작이나 기능을 수행하기 위한 명령어 또는 프로그램 코드를 포함할 수 있다. 일 실시예에서, 레이어 패널이 유기 발광 다이오드 디스플레이, 무기 발광 다이오드 디스플레이 등과 같은 자발광 디스플레이(self-emitting display)인 경우, 영상 보정 모듈은 복수의 뷰 영상들 각각과, 대 응되는 쉬프트 레이어 영상을 나누어 복수의 서브 보정-베이스 영상들을 포함하는 보정-베이스 영상 을 생성하는 동작이나 기능을 수행하기 위한 명령어 또는 프로그램 코드를 포함할 수 있다. 도 10을 참조하면, 도 10에는 디스플레이에 포함된 베이스 패널, 광학층 및 레이어 패널이 도시되어 있다. 일 실시예에서, 광학층은 베이스 패널 상에 배치되어 있다. 레이어 패널은 광학 층 상에 배치되어 있다. 일 실시예에서, 디스플레이는 베이스 패널의 하단에 배치된 백라이트 를 더 포함할 수 있다. 일 실시예에서, 베이스 패널과 레이어 패널은 서로 간의 간격을 달리 하여 디스플레이에 포함될 수 있다. 일 실시예로, 레이어 패널은 베이스 패널로부터 이격되어 위치하고, 레이어 패널과 베 이스 패널 사이의 거리를 기준 거리라 정의한다. 일 실시예에서, 기준 거리는 베이스 패널 의 상면과 레이어 패널의 하면 사이의 거리일 수 있다. 일 실시예에서, 베이스 패널 및 레이어 패널은 각각 복수의 픽셀들을 포함할 수 있다. 일 실시예에서, 미도시된 백라이트에서 제공되는 광은 베이스 패널에 포함된 복수의 픽셀들 중 어느 하나의 픽셀과 레이어 패널에 포함된 복수의 픽셀들 중 어느 하나의 픽셀을 투과하여 사용자(250, 도 2 참조)에게 제공될 수 있다. 일 실시예에서, 광학층에 포함된 적어도 하나의 렌즈의 크기, 렌즈의 형상 또는 렌즈의 굴절률 중 적어도 하나에 기초하여 베이스 패널을 투과한 광이 광학층에 의하여 굴절되는 정도가 결정될 수 있다. 베이 스 패널을 투과한 광은 광학층에 의하여 굴절되어 경로가 변경된 후, 레이어 패널을 투과하여 사용자에게 제공될 수 있다. 일 실시예에서, 베이스 패널에 표시되는 베이스 영상은 제1 서브 베이스 영상(1051_1), 제2 서브 베 이스 영상(1051_2) 및 제3 서브 베이스 영상(1051_3)을 포함할 수 있다. 일 실시예에서, 제1 내지 제3 서브 베이스 영상들(1051_1, 1051_2, 1051_3) 각각은 광학층에 의하여 굴절 되어, 서로 다른 뷰에 위치한 사용자에게 제공될 수 있다. 제1 내지 제3 서브 베이스 영상들(1051_1, 1051_2, 1051_3) 각각은 서로 다른 뷰에 위치한 사용자에게 물체의 다른 면을 제공하기 위한 영상일 수 있 다. 일 실시예에서, 제2 서브 베이스 영상(1051_2) 및 제3 서브 베이스 영상(1051_3)은 제1 서브 베이스 영상 (1051_1)에 인접한 영상이다. 제1 서브 베이스 영상(1051_1)에 의한 영상이 제공되는 사용자가 위치한 뷰 는, 제2 서브 베이스 영상(1051_2)에 의한 영상이 제공되는 사용자가 위치한 뷰에 인접할 수 있다. 제1 서 브 베이스 영상(1051_1)에 의한 영상이 제공되는 사용자가 위치한 뷰는, 제3 서브 베이스 영상(1051_3)에 의한 영상이 제공되는 사용자가 위치한 뷰에 인접할 수 있다. 도 10에는 베이스 영상이 세 개의 서브 베이스 영상들을 포함하는 것으로 도시되었으나, 본 개시는 이에 제한되지 않는다. 베이스 패널의 해상도, 광학층에 포함된 적어도 하나의 렌즈의 크기, 렌즈의 형상 또는 렌즈의 굴절률 중 적어도 하나에 기초하여, 베이스 영상은 두 개의 서브 베이스 영상 또는 네 개 이 상의 서브 베이스 영상들을 포함할 수도 있다. 일 실시예에서, 제1 내지 제3 베이스 영상들(1051_1, 1051_2, 1051_3) 각각은 광학층에 의하여 굴절된 후, 레이어 영상 중 대응되는 광 경로에 있는 영역을 투과하여 사용자에게 제공될 수 있다. 일 실시예에서, 레이어 영상 중 광학층에 포함된 렌즈에 수직으로 입사하여, 굴절되지 않고, 레이어 패널의 정면으로 향하는 제1 서브 베이스 영상(1051_1)이 투과하는 영역을 제1 레이어 영역(1040_1)이라 정의할 수 있다. 레이어 영상 중 제1 서브 베이스 영상(1051_1)에 인접한 제2 서브 베이스 영상(1051_2) 이 광학층에 의하여 굴절된 후, 투과하는 영역을 제2 레이어 영역(1040_2)이라 정의할 수 있다. 레이어 영 상 중 제1 서브 베이스 영상(1051_1)에 인접한 제3 서브 베이스 영상(1051_3)이 광학층에 의하여 굴 절된 후, 투과하는 영역을 제3 레이어 영역(1040_3)이라 정의할 수 있다. 일 실시예에서, 인접한 제1 서브 베이스 영상(1051_1)과 제2 서브 베이스 영상(1051_2)이 광학층에 의하여 각각 굴절된 후 투과하는 제1 레이어 영역(1040_1)의 픽셀과 제2 레이어 영역(1040_2)의 픽셀 간의 간격을 기준쉬프트 값이라 정의할 수 있다. 일 실시예에서, 기준 쉬프트 값은 사용자가 전자 장치를 통하여 표시되는 영상에 포함된 물체의 서로 다른 면을 볼 수 있는 복수의 뷰 중 인접한 뷰의 간격, 기준 거리, 레이어 패널의 해상 도 중 적어도 하나에 기초하여 결정될 수 있다. 일 실시예에서, 기준 거리가 멀어질수록, 기준 쉬프트 값 은 커질 수 있다. 레이어 패널의 해상도가 높아질수록, 기준 쉬프트 값은 커질 수 있다. 인접 한 뷰의 간격이 커질수록, 기준 쉬프트 값은 커질 수 있다. 일 실시예에서, 복수의 쉬프트 값들(860, 도 8 참조)은 기준 쉬프트값 및 서로 다른 복수의 뷰에 기초하 여 결정될 수 있다. 사용자가 전자 장치를 통하여 표시되는 영상에 포함된 물체의 서로 다른 면 을 볼 수 있는 뷰가 개라고 할 때, 사용자의 위치가 전자 장치의 정면에서 영상을 볼 수 있는 뷰에 있는 것을 기준으로 전자 장치의 좌측면으로 개의 뷰, 전자 장치의 우측면으로 개의 뷰가 포함될 수 있다. 일 실시예에서, 기준 쉬프트 값을 라고 할 때, 서로 다른 복수의 뷰에 각각 대응되는 복수의 쉬프트 값 들은 내지 사이의 값 및 0의 값을 가질 수 있다. 이때, 은 자연수이고, 0은 사용자 의 위치가 전자 장치의 정면에서 영상을 볼 수 있는 뷰에 있을 때의 쉬프트 값일 수 있다. 일 실시예에서, 복수의 쉬프트 값들 각각은 서로 다른 복수의 뷰에 대한 정보를 포함할 수 있다. 레이어 영상(840-1)을 각각의 복수의 쉬프트 값들을 이용하여 쉬프트하여 생성된 복수의 쉬프트 레이어 영상들은 서로 다른 복수의 뷰에서 사용자에게 제공되는 레이어 영상에 대한 정보를 포함할 수 있다. 다시 도 8을 참조하면, 일 실시예에서, 적어도 하나의 프로세서는 레이어 영상(840_1), 제1 프레임 영상 (830_1) 및 복수의 쉬프트 값들을 영상 보정 모듈에 적용함으로써, 제1 프레임 영상(830_1)과 제2 프 레임 영상(830_2) 간의 움직임에 기초한 보정-베이스 영상을 생성할 수 있다. 보정-베이스 영상은 제 1 프레임에서 베이스 패널에 대응되는 영상일 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 제1 프레임 영상(830_1)에 포함된 각각의 복수의 뷰 영상들 및 복수의 쉬프트 값 중 복수의 뷰 영상들 각각의 뷰에 대응되는 쉬프트 값에 따라 쉬프트된 쉬프트 레이어 영상에 따라, 서로 다른 복수의 뷰에 따른 복수의 서브 베이스 영상들 간의 차이에 기초하 고, 복수의 서브 보정-베이스 영상들을 포함하는 보정-베이스 영상을 생성할 수 있다. 일 실시예에서, 각각의 뷰에서 사용자에게 재현되는 물체를 포함하는 복수의 뷰 영상들과 각각의 뷰 에 대응되는 쉬프트 값에 따라 쉬프트된 복수의 쉬프트 레이어 영상에 의하여 획득된 복수의 서브 보정-베이스 영상들 각각은, 인접한 서브 보정-베이스 영상들과 비교하여 서로 다른 뷰에 따른 물체의 위치 정보만이 상이하고, 물체의 휘도, 계조, 색상 등의 정보는 동일할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 보정-베이스 영상을 베이스 패널에 표시하고, 레이어 영상(840_1)을 레이어 패널에 표시함으로써, 제2 프레임에서 제1 프레임동안 움직이는 물체를 포함하는 입 력 콘텐츠가 제공되더라도, 제2 프레임과 비교할 때 제1 프레임에서 물체의 위치 정보만이 상이하고, 물체 의 휘도, 계조, 색상 등은 동일한 영상을 사용자에게 제공하여, 사용자의 시인성 및 영상 의 품질을 향상시킬 수 있다. 또한, 사용자에게 서로 다른 뷰에서 물체의 다른 면을 포함하는 영상을 제공하기 위하여, 복수의 서 브 보정-베이스 영상들을 포함하는 보정-베이스 영상을 베이스 패널에 표시하더라도, 뷰의 변화 에 따라 물체의 위치 정보만이 상이하고, 물체의 휘도, 계조, 색상 등은 동일한 영상을 사용자에게 제공하여, 사용자의 시인성 및 영상의 품질을 향상시킬 수 있다. 도 9를 참조하면, 도 9에는, 영상 보정 모듈이 베이스 영상(940_2) 및 복수의 쉬프트 값들에 기초하 여, 보정-베이스 영상을 추론하도록 훈련된 제2 인공 지능 모델을 포함하는 구성이 도시되어 있다. 이하, 도 8에서 설명한 구성과 동일한 구성에 대한 중복되는 설명은 생략하도록 한다. 일 실시예에서, 적어도 하나의 프로세서는 제1 프레임 영상(930_1) 및 제2 프레임 영상(930_2)을 영상 생 성 모듈에 적용함으로써, 베이스 패널에 대응되고, 복수의 서브 베이스 영상들을 포함하는 베이스 영상(940_2) 및 레이어 패널에 대응되는 레이어 영상(940_1)을 생성할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 복수의 서브 베이스 영상들을 포함하는 베이스 영상 (940_2) 및 복수의 쉬프트 값을 영상 보정 모듈에 적용함으로써, 복수의 서브 보정-베이스 영상들 을 포함하는 보정-베이스 영상을 생성할 수 있다. 일 실시예에서, 제2 인공 지능 모델은, 복수의 서브 베이스 영상들을 포함하는 베이스 영상(940_2) 및 복 수의 쉬프트 값들에 기초하여, 제1 프레임에서 베이스 패널에 대응되고, 서로 다른 복수의 뷰에 각각 대응되는 복수의 서브 보정-베이스 영상들을 포함하는 보정-베이스 영상을 추론하도록 훈련된 인공 지능 모델일 수 있다. 일 실시예에서, 제2 인공 지능 모델은 서로 다른 복수의 뷰에 각각 대응되는 복수의 서브 베이스 영상들 및 서로 다른 복수의 뷰에 각각 대응되는 복수의 쉬프트 값들에 따라, 서로 다른 복수의 뷰에 따른 복수의 서브 베이스 영상들 각각의 차이에 기초하는 복수의 서브 보정-베이스 영상들을 포함하는 보정-베이 스 영상을 추론하도록 훈련된 인공 지능 모델일 수 있다. 이하, 제2 인공 지능 모델의 훈련 방법은 도 12c 에서 후술하도록 한다. 일 실시예에서, 적어도 하나의 프로세서는 보정-베이스 영상을 베이스 패널에 표시하고, 레이어 영상(940_1)을 레이어 패널에 표시할 수 있다. 도 11은 본 개시의 일 실시예에 따른, 영상 생성 모듈에 포함된 제1 인공 지능 모델 및 영상 보정 모듈에 포함 된 제2 인공 지능 모델의 훈련 과정을 설명하기 위한 도면이다. 이하, 도 6, 도 8 및 도 9에서 설명한 구성과 동일한 구성에 대한 중복되는 설명은 생략하도록 한다. 도 2 및 도 11을 참조하면, 도 11은 전자 장치가 베이스 패널, 레이어 패널 및 베이스 패널 과 레이어 패널 사이에 배치되는 광학층을 포함하는 경우의 전자 장치의 동작을 설명하기 위한 도면이다. 이때, 전자 장치는 베이스 패널 및 레이어 패널에 광을 제공하는 백라이트(21 4)를 더 포함할 수도 있다. 일 실시예에서, 전자 장치에 포함된 영상 생성 모듈은 제1 인공 지능 모델을 포함하고, 영상 보정 모듈은 제2 인공 지능 모델을 포함할 수 있다. 일 실시예에서, 전자 장치가 도 11에 도시된 것과 같이 영상 생성 모듈에 포함된 제1 인공 지능 모 델 및 영상 보정 모듈에 포함된 제2 인공 지능 모델을 훈련시킬 수 있다. 다만, 본 개시는 이에 제한되지 않고, 주변의 전자 장치가 도 11에 도시된 것과 같이 인공 지능 모델을 훈련시키고, 전자 장치는 주변의 전자 장치로부터 훈련된 인공 지능 모델을 제공받을 수도 있다. 또한, 전자 장치는 주변의 전자 장치로부 터 전이 학습(transfer learning)을 위해 사전 학습 인공 지능 모델을 제공받고, 도 11에 도시된 것을 토대로 전이 학습을 할 수도 있다. 이하, 설명의 편의를 위하여 전자 장치가 영상 생성 모듈에 포함된 제1 인공 지능 모델 및 영상 보정 모듈에 포함된 제2 인공 지능 모델을 훈련시키는 경우로 설명한다. 일 실시예에서, 전자 장치는 훈련용 입력 콘텐츠를 획득할 수 있다. 전자 장치는 훈련용 입력 콘텐츠의 제1 프레임에서의 제1 훈련용 프레임 영상(1130_1) 및 제2 프레임에서의 제2 훈련용 프레임 영 상(1130_2)을 획득할 수 있다. 일 실시예에서, 영상 생성 모듈에 포함된 제1 인공 지능 모델은, 복수의 훈련용 뷰 영상들을 포함 하는 제1 훈련용 프레임 영상(1130_1) 및 복수의 훈련용 뷰 영상들을 포함하는 제2 훈련용 프레임 영상 (1130_2)에 기초하여, 제1 프레임에서 베이스 패널에 대응되고, 복수의 훈련용 서브 베이스 영상들 을 포함하는 훈련용 베이스 영상(1140_2) 및 제1 프레임에서 레이어 패널에 대응되고, 제1 훈련용 프레임 영상(1130_1)과 제2 훈련용 프레임 영상(1130_2) 간의 움직임(753, 도 7 참조)을 반영한 훈련용 레이어 영상 (1140_1)을 추론하도록 훈련될 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 제1 훈련 모듈을 실행하여, 제1 훈련용 프레임 영상 (1130_1) 및 제2 훈련용 프레임 영상(1130_2) 간의 움직임을 추출하고, 추출한 움직임에 기초하여 기 -생성된 제2 프레임에서 상기 레이어 패널에 대응되는 비교 레이어 영상을 와핑(warping)하여, 와핑된 비 교 레이어 영상과 차이가 적은 훈련용 레이어 영상(1140_1)을 추론하도록 제1 인공 지능 모델을 훈련할 수 있다. 이하, 제1 훈련 모듈에 의한 제1 인공 지능 모델의 훈련은 도 12a에서 후술하도록 한다. 일 실시예에서, 제1 훈련용 프레임 영상(1130_1)을 재현한 영상을 사용자에게 제공하기 위하여 적어 도 하나의 프로세서는 제2 훈련 모듈을 실행하여, 제1 훈련용 프레임 영상(1130_1)에 기초하여 제1 프레임에서 베이스 패널에 대응되는 훈련용 베이스 영상(1140_2) 및 제1 프레임에서 레이어 패널에 대응되는 훈련용 레이어 영상(1140_1)을 추론하도록 제1 인공 지능 모델을 훈련할 수 있다. 적어도 하나의 프로 세서는 제2 훈련 모듈을 실행하여 훈련용 베이스 영상(1140_2), 훈련용 레이어 영상(1140_1), 베이 스 패널의 밝기 정보 및 레이어 패널의 밝기 정보에 따라 전자 장치로 재현되는 영상을 시 뮬레이션하여 획득한 훈련용 출력 영상과 제1 훈련용 프레임 영상(1130_1) 간의 차이가 적어지도록 훈련용 베이 스 영상(1140_2)과 훈련용 레이어 영상(1140_1)을 추론하도록 제1 인공 지능 모델을 훈련할 수 있다. 이하, 제2 훈련 모듈에 의한 제1 인공 지능 모델의 훈련은 도 12b에서 후술하도록 한다. 일 실시예에서, 적어도 하나의 프로세서는 제3 훈련 모듈을 실행하여, 서로 다른 복수의 뷰에 대응 되는 복수의 훈련용 서브 베이스 영상들 각각을 기준 쉬프트 값을 이용하여 쉬프트하고, 복수의 쉬 프트된 서브 훈련용 베이스 영상들과 차이가 적은 복수의 서브 보정-훈련용 베이스 영상을 포함하는 보정 -훈련용 베이스 영상을 추론하도록 제2 인공 지능 모델을 훈련할 수 있다. 이하, 제3 훈련 모듈에 의한 제2 인공 지능 모델의 훈련은 도 12c에서 후술하도록 한다. 도 12a는 본 개시의 일 실시예에 따른, 제1 인공 지능 모델을 훈련시키기 위한 제1 손실 함수를 설명하기 위한 도면이다. 이하, 도 6, 도 8, 도 9 및 도 11에서 설명한 구성과 동일한 구성에 대한 중복되는 설명은 생략한다. 도 12a를 참조하면, 일 실시예에서 제1 훈련 모듈은 움직임 추출 모듈, 영상 와핑 모듈 및 제1 손실 함수 계산 모듈을 포함할 수 있다. 일 실시예에서, 움직임 추출 모듈은 제1 훈련용 프레임 영상(1230_1) 및 제2 훈련용 프레임 영상(1230_ 2)에 기초하여, 제1 훈련용 프레임 영상(1230_1)과 제2 훈련용 프레임 영상(1230_2) 간의 움직임(735, 도 7 참 조)을 추출하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 움직임 추출 모듈은 제1 훈련용 프레임 영상(1230_1)과 제2 훈련용 프레임 영상(1230_2) 에 기초하여, 훈련용 프레임 영상에 포함된 픽셀의 제2 프레임에서 제1 프레임으로의 벡터 성분을 구하고, 벡터의 크기 및 방향에 기초하여 제2 프레임에서 제1 프레임으로의 훈련용 프레임 영상에 포함된 픽셀의 움직임을 추출하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시 예에서, 훈련용 프레임 영상에 포함된 픽셀은, 훈련용 프레임 영상에 포함된 물체에 포함될 수 있 다. 일 실시예에서, 움직임 추출 모듈은 루카스-카나데(Lucas-Kanade) 알고리즘 등을 통하여 제2 프레임에서 제1 프레임으로의 훈련용 프레임 영상에 포함된 픽셀의 움직임을 추출할 수 있다. 또한, 움직임 추 출 모듈은 플로우넷(FlowNet), 라이트 플로우넷(LiteFlowNet) 등과 같은 딥 러닝 모델을 통하여 제2 프레 임에서 제1 프레임으로의 훈련용 프레임 영상에 포함된 픽셀의 움직임을 추출할 수도 있고, 어느 하 나의 동작이나 기능으로 제한되지 않는다. 일 실시예에서, 움직임 추출 모듈은 제1 훈련용 프레임 영상(1230_1)에 포함된 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된 센터 뷰 영상 및 제2 훈련용 프레임 영상(1230_2)에 포함된 복수의 훈련용 뷰 영 상들 중 센터 뷰에서 획득된 센터 뷰 영상에 기초하여, 제1 훈련용 프레임 영상(1230_1)과 제2 훈련용 프 레임 영상(1230_2) 간의 움직임을 추출하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 센터 뷰는 사용자가 전자 장치의 정면에 위치하여, 영상에 포함된 물체의 정면을 볼 수 있는 위치에 대응될 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 제1 훈련용 프레임 영상(1230_1) 및 제2 훈련용 프레임 영상 (1230_2)을 움직임 추출 모듈에 적용함으로써, 제1 훈련용 프레임 영상(1230_1)과 제2 훈련용 프레임 영 상(1230_2) 간의 움직임을 추출할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 제1 훈련용 프레임 영상(1230_1)에 포함된 복수의 훈련용 뷰 영 상들 중 센터 뷰에서 획득된 센터 뷰 영상 및 제2 훈련용 프레임 영상(1230_2)에 포함된 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된 센터 뷰 영상을 움직임 추출 모듈에 적용함으로써, 제1 훈련용 프레임 영상(1230_1)과 제2 훈련용 프레임 영상(1230_2) 간의 움직임을 추출할 수 있다. 이하, 설명의 편의를 위하여, 제1 훈련용 프레임 영상(1230_1)과 제2 훈련용 프레임 영상(1230_2) 간의 움직임 은 제1 훈련용 프레임 영상(1230_1)에 포함된 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된 센터 뷰 영상 및 제2 훈련용 프레임 영상(1230_2)에 포함된 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된센터 뷰 영상을 움직임 추출 모듈에 적용하여 추출된 움직임으로 설명한다. 일 실시예에서, 영상 와핑 모듈은 추출된 움직임 및 기-생성된 제2 프레임에서 레이어 패널에 대응되는 비교 레이어 영상을 와핑(warping)하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 비교 레이어 영상은 적어도 하나의 프로세서가 제2 프레임에서 영상 생성 모듈 을 실행하여 생성한 레이어 영상일 수 있다. 적어도 하나의 프로세서는 훈련용 입력 콘텐츠의 제2 프레임에서의 제2 훈련용 프레임 영상(1230_2) 및 제2 프레임의 직전 프레임인 제3 프레임에서의 제3 훈련 용 프레임 영상을 영상 생성 모듈에 적용함으로써, 제2 프레임에서 레이어 패널에 대응되는 비교 레 이어 영상을 생성할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 움직임 추출 모듈을 통하여 추출한 제1 훈련용 프레임 영 상(1230_1)과 제2 훈련용 프레임 영상(1230_2) 간의 움직임 및 기-생성한 비교 레이어 영상을 영상 와핑 모듈에 적용함으로써, 비교 레이어 영상을 제1 훈련용 프레임 영상(1230_1)과 제2 훈련용 프 레임 영상(1230_2) 간의 움직임만큼 와핑할 수 있다. 일 실시예에서, 제1 훈련용 프레임 영상(1230_1)과 제2 훈련용 프레임 영상(1230_2) 간의 움직임만큼 와핑 된 비교 레이어 영상은, 비교 레이어 영상과 비교하여 제2 프레임에서 제1 프레임동안 입력 콘텐츠(123 0)에 포함된 물체의 위치 변화가 반영된 영상일 수 있다. 일 실시예에서, 제1 손실 함수 계산 모듈은 적어도 하나의 프로세서가 영상 생성 모듈을 실행 하여 생성한 제1 프레임에서 레이어 패널에 대응되는 훈련용 레이어 영상(1240_1)과 와핑된 비교 레이어 영상에 따른 제1 손실 함수를 계산하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 제1 손실 함수 계산 모듈은, 훈련용 레이어 영상(1240_1)과 와핑된 비교 레이어 영상에 따라, 아래의 수학식 1에 따라 제1 손실 함수(loss function)를 계산할 수 있다. 수학식 1:"}
{"patent_id": "10-2022-0153816", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이때, 은 제1 손실 함수이고, 은 제1 마스킹 값이고, 는 제1 프레임에서의 훈련용 레이어 영상이고, 는 와핑된 제2 프레임에서의 비교 레이어 영상이며, 는 와 의 차이의 L1 노름 (norm)이다. 일 실시예에서, 제1 마스킹 값은 영상 와핑 모듈을 통하여 비교 레이어 영상을 제1 훈련용 프레임 영상(1230_1)과 제2 훈련용 프레임 영상(1230_2) 간의 움직임만큼 와핑하는 동작에서, 와핑이 제1 훈련용 프레임 영상(1230_1)과 제2 훈련용 프레임 영상(1230_2) 간의 움직임만큼 되지 않은 비교 레이어 영상이 제1 손실 함수의 값에 반영되는 것을 방지할 수 있다. 제1 손실 함수 계산 모듈은 아래의 수학식 2에 따 라 제1 마스킹 값을 계산할 수 있다. 수학식 2:"}
{"patent_id": "10-2022-0153816", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이때, 은 제1 마스킹 값이고, 는 임의의 계수이며, 는 제1 프레임에서의 훈련용 레이어 영상이고, 는 와핑된 제2 프레임에서의 비교 레이어 영상이며, 는 와 의 차이의 L2 노 름(norm)이다.일 실시예에서, 적어도 하나의 프로세서는 제1 손실 함수 계산 모듈을 통하여, 와 의 차이 가 기-설정된 차이값보다 클 경우, 비교 레이어 영상의 와핑이 제1 훈련용 프레임 영상(1230_1)과 제2 훈 련용 프레임 영상(1230_2) 간의 움직임만큼 되지 않았다고 판단하여, 마스킹 값이 0에 가까운 값을 갖도록 할 수 있다. 이 경우, 제1 손실 함수의 값도 0에 가까운 값이 되어, 해당 훈련 과정은 제1 인공 지능 모델의 훈 련에 반영되지 않을 수 있다. 반면, 적어도 하나의 프로세서는 제1 손실 함수 계산 모듈을 통하여 와 의 차이가 기-설정 된 차이값과 같거나 작을 경우, 비교 레이어 영상의 와핑이 제1 훈련용 프레임 영상(1230_1)과 제2 훈련 용 프레임 영상(1230_2) 간의 움직임만큼 되었다고 판단하여, 마스킹 값이 1에 가까운 값을 갖도록 할 수 있다. 이 경우, 해당 훈련 과정은 제1 인공 지능 모델의 훈련에 반영될 수 있다. 이때, 기-설정된 차이값의 크기는 의 크기를 조절하여 결정할 수 있다. 의 크기를 크게 할수록 기-설정된 차이값의 크기가 작아질 수 있다. 다만, 본 개시는 이에 제한되지 않고, 제1 손실 함수 계산 모듈은 제1 손실 함수 및 제1 마스킹 값을 다 른 수학식을 이용하여 계산할 수도 있다. 일 실시예에서, 제1 손실 함수의 값은 훈련용 레이어 영상(1240_1)과 와핑된 비교 레이어 영상의 차이가 적을수 록 작아질 수 있다. 적어도 하나의 프로세서는 제1 훈련 모듈을 통하여 제1 손실 함수의 값이 작아 지도록 영상 생성 모듈에 포함된 제1 인공 지능 모델을 훈련시킬 수 있다. 적어도 하나의 프로세서 는 제1 훈련용 프레임 영상(1230_1) 및 제2 훈련용 프레임 영상(1230_2)에 기초하여, 작은 제1 손실 함수 값을 얻을 수 있는 훈련용 레이어 영상(1240_1)을 추론하도록 제1 인공 지능 모델을 훈련시킬 수 있다. 제1 인공 지 능 모델에 포함된 가중치는 제1 손실 함수의 값이 작아지도록 업데이트 될 수 있다. 도 12b는 본 개시의 일 실시예에 따른, 제1 인공 지능 모델을 훈련시키기 위한 제2 손실 함수를 설명하기 위한 도면이다. 이하, 도 12a에서 설명한 구성과 동일한 구성에 대한 중복되는 설명은 생략한다. 도 12b를 참조하면, 일 실시예에서, 제2 훈련 모듈은 시뮬레이션 모듈 및 제2 손실 함수 계산 모듈 을 포함할 수 있다. 일 실시예에서, 시뮬레이션 모듈은 적어도 하나의 프로세서가 영상 생성 모듈을 통하여 생성 한 훈련용 베이스 영상(1240_2)을 베이스 패널에 표시하고, 훈련용 레이어 영상(1240_1)을 레이어 패널 에 표시할 경우, 제1 프레임에서 사용자에게 제공되는 영상인 훈련용 출력 영상을 획득하기 위 한 시뮬레이션(simulation)을 수행하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 시뮬레이션 모듈은 훈련용 베이스 영상(1240_2), 훈련용 레이어 영상(1240_1) 및 베이스 패널에 대한 정보 및 레이어 패널에 대한 정보를 포함하는 패널 정보에 기초하여, 훈련용 출력 영상을 획득하기 위한 시뮬레이션을 수행하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 패널 정보에는 베이스 패널의 디스플레이 종류, 레이어 패널의 디스플레이 종 류, 베이스 패널의 밝기 정보, 레이어 패널의 밝기 정보 및 베이스 패널과 레이어 패널 간 의 간격 등에 대한 정보가 포함될 수 있다. 일 실시예에서, 패널 정보에 같은 계조의 영상을 표시할 때의 레이어 패널의 밝기가 같은 계조의 영 상을 표시할 때의 베이스 패널의 밝기보다 m배 크다는 정보가 포함된 경우, 적어도 하나의 프로세서 는 시뮬레이션 모듈을 통하여, 해당 밝기 정보를 반영하여 베이스 패널에 훈련용 베이스 영상 (1240_2)이 표시되고, 레이어 패널에 훈련용 레이어 영상(1240_1)이 표시될 때 사용자에게 시인되는 훈련용 출력 영상을 획득하기 위한 시뮬레이션을 수행할 수 있다. 이를 통하여, 적어도 하나의 프로세서가 후술할 제2 훈련 모듈을 통하여 제1 인공 지능 모델을 훈련시킬 때, 훈련용 베이스 영상(1240_2)의 밝기 가 훈련용 레이어 영상(1240_1)의 밝기보다 m배 크도록 추론되도록 제1 인공 지능 모델을 훈련시킬 수 있다. 또한, 패널 정보에 베이스 패널 또는 레이어 패널 중 적어도 하나의 패널의 디스플레이 종류가 단일-컬러(mono-color) 패널이 적층되어 이루어진 디스플레이라는 정보를 포함된 경우, 적어도 하나의 프로세서는 시뮬레이션 모듈을 통하여, 적층된 단일-컬러 패널들 각각을 R, G, B 등의 서브-컬러 픽셀을 포 함하는 패널로 렌더링하여 베이스 패널에 훈련용 베이스 영상(1240_2)이 표시되고, 레이어 패널에 훈 련용 레이어 영상(1240_1)이 표시될 때 사용자에게 시인되는 훈련용 출력 영상을 획득하기 위한 시뮬레이 션을 수행할 수 있다. 일 실시예에서, 제2 손실 함수 계산 모듈은 적어도 하나의 프로세서가 시뮬레이션 모듈을 통 하여 획득한 훈련용 출력 영상과 제1 훈련용 프레임 영상(1230_1)에 따른 제2 손실 함수를 계산하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 제2 손실 함수 계산 모듈은 훈련용 출력 영상에 포함된 복수의 서로 다른 뷰에 각각 대응 되는 복수의 서브 훈련용 출력 영상과 제1 훈련용 프레임 영상(1230_1)에 포함되는 복수의 훈련용 뷰 영상들 의 차이에 기초하여 제2 손실 함수를 계산하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구 성될 수 있다. 일 실시예에서, 제2 손실 함수의 값은, 영상 생성 모듈을 통하여 생성된 훈련용 베이스 영상(1240_2)과 훈련용 레이어 영상(1240_1) 및 패널 정보에 기초하여 시뮬레이션 동작을 수행하여 획득한 훈련용 출력 영상과 제1 훈련용 프레임 영상(1230_1)의 차이가 적을수록 작아질 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 제2 훈련 모듈을 통하여 제2 손실 함수의 값이 작아지도 록 영상 생성 모듈에 포함된 제1 인공 지능 모델을 훈련시킬 수 있다. 적어도 하나의 프로세서는 제 1 훈련용 프레임 영상(1230_1)에 기초하여, 작은 제2 손실 함수의 값을 얻을 수 있는 훈련용 베이스 영상 (1230_1) 및 훈련용 레이어 영상(1240_1)을 추론하도록 제1 인공 지능 모델을 훈련시킬 수 있다. 제1 인공 지능 모델에 포함된 가중치는 제2 손실 함수의 값이 작아지도록 업데이트 될 수 있다. 도 12c는 본 개시의 일 실시예에 따른, 제2 인공 지능 모델을 훈련시키기 위한 제3 손실 함수를 설명하기 위한 도면이다. 이하, 도 12a 및 도 12b에서 설명한 구성과 동일한 구성에 대한 중복되는 설명은 생략한다. 도 12c를 참조하면, 일 실시예에서 제3 훈련 모듈은 영상 쉬프트 모듈 및 제3 손실 함수 계산 모듈 을 포함할 수 있다. 일 실시예에서, 영상 쉬프트 모듈은 영상 생성 모듈을 통하여 생성된 서로 다른 복수의 뷰에 각각 대응되는 복수의 서브 훈련용 베이스 영상들을 포함하는 훈련용 베이스 영상(1240_2) 및 기준 쉬프트 값 에 기초하여, 복수의 서브 훈련용 베이스 영상들 각각을 기준 쉬프트 값을 이용하여 쉬프트하는 동 작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 서로 다른 복수의 뷰가 개일 때, 사용자의 위치가 전자 장치의 정면에서 영상 을 볼 수 있는 뷰에 있는 것을 기준으로 전자 장치의 좌측면으로 개의 뷰, 전자 장치의 우측 면으로 개의 뷰가 포함될 수 있다. 도 10을 참조할 때, 기준 쉬프트 값은 인접한 두 개의 뷰 각각에 대응되는 레이어 영역의 픽셀 간의 간격에 대응될 수 있다. 일 실시예에서, 영상 쉬프트 모듈은 서로 다른 복수의 뷰에 대응되는 복수의 서브 훈련용 영상들 각각을, 인접한 뷰에 대응되도록 기준 쉬프트 값을 이용하여 쉬프트하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드를 포함할 수 있다. 일 실시예에서, 인접한 두 개의 뷰 각각에 대응되는 서브 훈련용 영상들 각각은 서로 다른 뷰에 대응됨에 따라, 서로 다른 물체의 위치 정보를 포함할 수 있다. 영상 쉬프트 모듈은 쉬프트된 서브 훈련용 영상의 물체의 위치 정보가 인접한 뷰의 쉬프트되기 전의 서브 훈련용 영상의 물체의 위치 정보와 동일하도록 인접한 두 개의 뷰 각각에 대응되는 서브 훈련용 영상들을 기준 쉬프트 값을 이용하여 쉬프트하는 동작이나 기능에 관한 명령어 들 또는 프로그램 코드를 포함할 수 있다. 일 실시예에서, 서로 다른 복수의 뷰 중 어느 하나의 뷰를 제1 뷰라고 하고, 제1 뷰에 인접한 뷰를 제2 뷰라고 할 때, 영상 쉬프트 모듈은 복수의 서브 훈련용 베이스 영상들 중 제2 뷰에 대응되는 서브 훈련용 베이스 영상을 기준 쉬프트 값을 이용하여 제1 뷰에 대응되도록 쉬프트할 수 있다. 쉬프트된 제2 뷰에 대응되는 서브 훈련용 베이스 영상에 포함된 물체의 위치 정보는 제1 뷰에 대응되는 서브 훈련용 베이스 영상에 포함된물체의 위치 정보와 동일할 수 있다. 또한, 쉬프트된 제2 뷰에 대응되는 서브 훈련용 베이스 영상에 포함된 물 체의 휘도, 계조, 색상 등의 정보는 쉬프트되기 전의 제2 뷰에 대응되는 서브 훈련용 베이스 영상에 포함된 물 체의 휘도, 계조, 색상 등의 정보와 동일할 수 있다. 일 실시예에서, 제3 손실 함수 계산 모듈은 적어도 하나의 프로세서가 영상 보정 모듈을 실행 하여 생성한 보정-훈련용 베이스 영상과 쉬프트된 훈련용 베이스 영상에 따른 제3 손실 함수를 계산하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 제3 손실 함수 계산 모듈은 보정-훈련용 베이스 영상에 포함된 복수의 서브 보정- 훈련용 베이스 영상들 중 제1 뷰에 대응되는 서브 보정-훈련용 베이스 영상과 쉬프트된 훈련용 베이스 영 상에 포함된 복수의 쉬프트된 서브 훈련용 베이스 영상들 중 제2 뷰에 대응되는 서브 훈련용 베이스 영상에 따 라 제3 손실 함수를 계산하는 동작이나 기능에 관한 명령어들 또는 프로그램 코드로 구성될 수 있다. 일 실시예에서, 제2 손실 함수 계산 모듈은, 제1 뷰에 대응되는 서브 보정-훈련용 베이스 영상과 제2 뷰 에 대응되는 서브 훈련용 베이스 영상에 따라, 아래의 수학식 3에 따라 제3 손실 함수를 계산할 수 있다. 제2 손실 함수 계산 모듈은, 개의 뷰 중, 어느 하나의 제1 뷰에 대응되는 서브 보정-훈련용 베이스 영 상과 제2 뷰에 대응되는 서브 훈련용 베이스 영상에 따라 제1 뷰에 대응되는 손실 값을 구하고, 해당 과정을 개의 뷰에 대하여 반복하는 아래의 수학식 3에 따라 제3 손실 함수를 계산할 수 있다. 수학식 3:"}
{"patent_id": "10-2022-0153816", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이때, 은 제3 손실 함수이고, 은 제2 마스킹 값이고, 는 n번째 뷰에 대응되는 서브 보정-훈련용 베 이스 영상이고, 는 쉬프트된 n-1번째 뷰에서의 서브 훈련용 베이스 영상이며, 는 와 의 차이의 L1 노름(norm)이다. 일 실시예에서, 제2 마스킹 값은 영상 쉬프트 모듈을 통하여 n-1번째 뷰에 대응되는 서브 훈련용 베이스 영상을 기준 쉬프트 값을 이용하여 n번째 뷰에 대응되도록 쉬프트하는 동작에서, 쉬프트가 기준 쉬프트 값만큼 되지 않은 서브 훈련용 베이스 영상이 제3 손실 함수의 값에 반영되는 것을 방지할 수 있다. 제3 손실 함수 계산 모듈은 아래의 수학식 4에 따라 제2 마스킹 값을 계산할 수 있다. 수학식 4:"}
{"patent_id": "10-2022-0153816", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이때, 은 제2 마스킹 값이고, 는 임의의 계수이며, 는 n번째 뷰에 대응되는 서브 보정-훈련용 베이스 영상이고, 는 쉬프트된 n-1번째 뷰에서의 서브 훈련용 베이스 영상이며, 는 와 의 차이의 L2 노름(norm)이다. 일 실시예에서, 적어도 하나의 프로세서는 제3 손실 함수 계산 모듈을 통하여, 와 의 차이 가 기-설정된 차이값보다 클 경우, 영상 쉬프트 모듈의 와핑이 기준 쉬프트 값만큼 되지 않았다고 판단하여, 개의 뷰 중 n번째 뷰에 대한 마스킹 값이 0에 가까운 값을 갖도록 할 수 있다. 이 경우, 제3 손 실 함수의 값 중 n번째 뷰의 성분이 0에 가까운 값이 되어, 해당 훈련 과정은 제2 인공 지능 모델의 훈련에 반 영되지 않을 수 있다. 반면, 적어도 하나의 프로세서는 제3 손실 함수 계산 모듈을 통하여 와 의 차이가 기-설정 된 차이값과 같거나 작을 경우, 영상 쉬프트 모듈에 의한 쉬프트 동작이 기준 쉬프트 값만큼 되었 다고 판단하여, 개의 뷰 중 n번째 뷰에 대한 마스킹 값이 1에 가까운 값을 갖도록 할 수 있다. 이 경우, n번째 뷰에 의한 훈련 과정은 제2 인공 지능 모델의 훈련에 반영될 수 있다. 이때, 기-설정된 차이값의 크기는 의 크기를 조절하여 결정할 수 있다. 의 크기를 크게 할수록 기-설정된 차이값의 크기가 작아질 수 있다. 다만, 본 개시는 이에 제한되지 않고, 제3 손실 함수 계산 모듈은 제3 손실 함수 및 제2 마스킹 값을 다 른 수학식을 이용하여 계산할 수도 있다. 일 실시예에서, 제3 손실 함수의 값은 서로 다른 복수의 뷰에 대응되는 복수의 서브 보정-훈련용 베이스 영상들 과, 쉬프트된 복수의 서브 훈련용 베이스 영상들의 차이가 적을수록 작아질 수 있다. 적어도 하나의 프로세서 는 제3 훈련 모듈을 통하여 제3 손실 함수의 값이 작아지도록 영상 보정 모듈에 포함된 제3 인공 지능 모델을 훈련할 수 있다. 적어도 하나의 프로세서는 복수의 서브 훈련용 베이스 영상들을 포함하는 훈련용 베이스 영상(1240_2)에 기초하여, 작은 제3 손실 함수 값을 얻을 수 있는 복수의 서브 보정-훈 련용 베이스 영상들을 포함하는 보정-훈련용 베이스 영상을 추론하도록 제3 인공 지능 모델을 훈련 할 수 있다. 제3 인공 지능 모델에 포함된 가중치는 제3 손실 함수의 값이 작아지도록 업데이트 될 수 있다. 상술한 기술적 과제를 해결하기 위하여, 일 실시예에서, 전자 장치는 베이스 패널, 베이스 패널 상에 배치된 레이어 패널, 적어도 하나의 명령어(instruction)를 저장하는 메모리 및 메모리에 저장된 적어도 하나의 명령어를 실행하는 적어도 하나의 프로세서를 포함할 수 있다. 적어도 하나의 프로 세서는 적어도 하나의 명령어를 실행함으로써, 입력 콘텐츠의 제1 프레임에서의 제1 프레임 영상 및 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 프레임 영상을 획득할 수 있다. 적어도 하나의 프로세서는 제1 프레임 영상 및 제2 프레임 영상을 영상 생성 모듈에 적용함으로써, 제1 프레임에서 베이스 패널에 대응되 는 베이스 영상 및 제1 프레임에서 레이어 패널에 대응되고, 제1 프레임 영상과 제2 프레임 영상 간의 움직임에 기초한 레이어 영상을 생성할 수 있다. 적어도 하나의 프로세서는 베이스 영상 또는 레이어 영상을 영상 보정 모듈에 적용함으로써, 제1 프레임 영상과 제2 프레임 영상 간의 움직임에 기초한 보정-베이스 영상을 생성 할 수 있다. 적어도 하나의 프로세서는 보정-베이스 영상을 베이스 패널에 표시하고, 레이어 영상을 레이 어 패널에 표시할 수 있다. 일 실시예에서, 전자 장치는 베이스 패널과 레이어 패널 사이에 배치된 광학층을 더 포함 할 수 있다. 베이스 영상은 서로 다른 복수의 뷰(view)에 각각 대응되는 복수의 서브 베이스 영상들을 포함할 수 있다. 적어도 하나의 프로세서는 베이스 영상 또는 레이어 영상을 영상 보정 모듈에 적용함으로써, 서 로 다른 복수의 뷰에 따른 복수의 서브 베이스 영상들 간의 차이에 기초하고, 복수의 서브 보정-베이스 영상들 을 포함하는 보정-베이스 영상을 생성할 수 있다. 일 실시예에서, 영상 생성 모듈은 제1 프레임 영상 및 제2 프레임 영상에 기초하여 베이스 영상 및 레이어 영상 을 추론하도록 훈련된 제1 인공 지능 모델을 포함할 수 있다. 제1 인공 지능 모델은 훈련용 입력 콘텐츠의 제1 프레임에서의 제1 훈련용 프레임 영상 및 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 훈련용 프레임 영 상을 획득할 수 있다. 제1 인공 지능 모델은 제1 훈련용 프레임 영상 및 제2 훈련용 프레임 영상 간의 움직임을 추출할 수 있다. 제1 인공 지능 모델은 추출된 움직임을 이용하여, 기-생성된 제2 프레임에서 레이어 패널에 대 응되는 비교 레이어 영상을 와핑(warping)할 수 있다. 제1 인공 지능 모델은, 제1 인공 지능 모델을 통하여 생 성되는 제1 프레임에서 레이어 패널에 대응되는 훈련용 레이어 영상과 와핑된 비교 레이어 영상에 따른 제1 손 실 함수에 기초하여 훈련된 인공 지능 모델일 수 있다. 일 실시예에서, 제1 훈련용 프레임 영상 및 제2 훈련용 프레임 영상들 각각은 서로 다른 복수의 뷰에서 획득된 복수의 훈련용 뷰 영상들을 포함할 수 있다. 제1 훈련용 프레임 영상 및 제2 훈련용 프레임 영상 간의 움직임은, 제1 훈련용 프레임 영상에 포함된 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된 센터 뷰 영상 및 제2 훈련용 프레임 영상에 포함된 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된 센터 뷰 영상으로부터 추출 될 수 있다. 일 실시예에서, 제1 손실 함수는 훈련용 레이어 영상과 와핑된 비교 레이어 영상의 차이를 토대로 계산될 수 있 다. 제1 인공 지능 모델은 제1 손실 함수에 기초하여 훈련용 레이어 영상과 와핑된 비교 레이어 영상의 차이가 적어지도록 훈련될 수 있다.일 실시예에서, 제1 인공 지능 모델은 제1 훈련용 프레임 영상 및 제2 훈련용 프레임 영상을 제1 인공 지능 모 델에 적용함으로써, 제1 프레임에서 베이스 패널에 대응되는 훈련용 베이스 영상과, 제1 프레임에서 레이어 패 널에 대응되는 훈련용 레이어 영상을 생성할 수 있다. 제1 인공 지능 모델은 훈련용 베이스 영상, 훈련용 레이 어 영상, 베이스 패널의 밝기 정보 및 레이어 패널의 밝기 정보를 토대로 제1 프레임에서의 훈련용 출력 영상을 획득할 수 있다. 제1 인공 지능 모델은 제1 훈련용 프레임 영상과 훈련용 출력 영상간의 차이에 따른 제2 손실 함수에 기초하여 훈련된 인공 지능 모델일 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 적어도 하나의 명령어를 실행함으로써, 제1 프레임 영상 및 레 이어 영상을 영상 보정 모듈에 적용함으로써 보정-베이스 영상을 생성할 수 있다. 일 실시예에서, 제1 프레임 영상은 서로 다른 복수의 뷰에서 획득된 복수의 뷰 영상들을 포함할 수 있다. 적어 도 하나의 프로세서는 적어도 하나의 명령어를 실행함으로써, 제1 프레임 영상, 레이어 영상 및 복수의 뷰 에 각각 대응되는 복수의 쉬프트 값들을 영상 보정 모듈에 적용하여, 보정-베이스 영상을 생성할 수 있다. 영상 보정 모듈은 레이어 영상을 복수의 쉬프트 값들 각각을 이용하여 쉬프트하는 동작이나 기능을 수행하는 명령어 를 포함할 수 있다. 영상 보정 모듈은 각각의 복수의 뷰 영상들과 복수의 뷰 영상들에 각각 대응되는 복수의 쉬 프트된 레이어 영상들 간의 차이를 이용하여, 보정-베이스 영상을 생성하는 동작이나 기능을 수행하는 명령어를 포함할 수 있다. 복수의 쉬프트 값들은 베이스 패널과 레이어 패널 간의 간격, 레이어 패널의 해상도 또는 서로 다른 복수의 뷰 중 적어도 하나에 기초하여 결정될 수 있다. 일 실시예에서, 영상 보정 모듈은 베이스 영상에 기초하여 보정-베이스 영상을 추론하도록 훈련된 제2 인공 지 능 모델을 포함할 수 있다. 제2 인공 지능 모델은 영상 생성 모듈을 통하여 생성되는 복수의 서로 다른 뷰에 각 각 대응되는 복수의 서브 훈련용 베이스 영상들을 포함하는 훈련용 베이스 영상 및 기준 쉬프트 값을 획득할 수 있다. 제2 인공 지능 모델은 복수의 서브 훈련용 베이스 영상들 각각을 기준 쉬프트 값을 이용하여 쉬프트할 수 있다. 제2 인공 지능 모델은, 제2 인공 지능 모델을 통하여 생성되는 보정-훈련용 베이스 영상과 쉬프트된 훈련 용 베이스 영상에 따른 제3 손실 함수에 기초하여 훈련된 인공 지능 모델일 수 있다. 일 실시예에서, 제3 손실 함수는 보정-훈련용 베이스 영상에 포함된 복수의 서브 보정-훈련용 베이스 영상들 중 어느 하나의 제1 뷰에 대응되는 서브 보정-훈련용 베이스 영상과 쉬프트된 훈련용 베이스 영상에 포함된 복수의 쉬프트된 서브 훈련용 베이스 영상들 중 제1 뷰에 인접한 제2 뷰에 대응되는 쉬프트된 서브 훈련용 베이스 영상 간의 차이에 따라 계산될 수 있다. 제2 인공 지능 모델은, 제3 손실 함수에 기초하여 서브 보정-훈련용 베이스 영상과 쉬프트된 서브 훈련용 베이스 영상의 차이가 적어지도록 훈련될 수 있다. 상술한 기술적 과제를 해결하기 위하여, 일 실시예에서, 베이스 패널 및 베이스 패널 상에 배치된 레 이어 패널을 포함하는 전자 장치의 동작 방법을 제공한다. 일 실시예에 따른 전자 장치의 동작 방법은 입력 콘텐츠의 제1 프레임에서의 제1 프레임 영상 및 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 프레임 영상을 획득하는 단계를 포함할 수 있다. 전자 장치의 동작 방법은 제1 프레임 영상 및 제2 프레임 영상을 영상 생성 모듈에 적용함으로써, 제1 프레임에서 베이스 패널에 대응되는 베이스 영상 및 제1 프레 임에서 레이어 패널에 대응되고, 제1 프레임 영상과 제2 프레임 영상 간의 움직임에 기초한 레이어 영상을 생성하는 단계를 포함할 수 있다. 전자 장치의 동작 방법은 베이스 영상 또는 레이어 영상을 영상 보정 모 듈에 적용함으로써, 제1 프레임 영상과 제2 프레임 영상 간의 움직임에 기초한 보정-베이스 영상을 생성하는 단 계를 포함할 수 있다. 전자 장치의 동작 방법은 보정-베이스 영상을 베이스 패널에 표시하고, 레이어 영상을 레이어 패널에 표시하는 단계를 포함할 수 있다. 일 실시예에서, 전자 장치는 베이스 패널과 레이어 패널 사이에 배치된 광학층을 더 포함 할 수 있다. 베이스 영상은 서로 다른 복수의 뷰(view)에 각각 대응되는 복수의 서브 베이스 영상들을 포함할 수 있다. 보정-베이스 영상을 생성하는 단계에서는, 베이스 영상 또는 레이어 영상을 영상 보정 모듈에 적용함 으로써 서로 다른 복수의 뷰에 따른 복수의 서브 베이스 영상들 간의 차이에 기초하고, 복수의 서브 보정-베이 스 영상들을 포함하는 보정-베이스 영상을 생성하는 단계를 포함할 수 있다. 일 실시예에서, 영상 생성 모듈은 제1 프레임 영상 및 제2 프레임 영상에 기초하여, 베이스 영상 및 레이어 영 상을 추론하도록 훈련된 제1 인공 지능 모델을 포함할 수 있다. 제1 인공 지능 모델의 훈련 방법은 훈련용 입력 콘텐츠의 제1 프레임에서의 제1 훈련용 프레임 영상 및 제1 프레임의 직전 프레임인 제2 프레임에서의 제2 훈련 용 프레임 영상을 획득하는 단계를 포함할 수 있다. 제1 인공 지능 모델의 훈련 방법은 제1 훈련용 프레임 영상 및 제2 훈련용 프레임 영상 간의 움직임을 추출하는 단계를 포함할 수 있다. 제1 인공 지능 모델의 훈련 방법은 추출된 움직임을 이용하여, 기-생성된 제2 프레임에서 레이어 패널에 대응되는 비교 레이어 영상을 와핑(warping) 하는 단계를 포함할 수 있다. 제1 인공 지능 모델의 훈련 방법은 제1 인공 지능 모델을 통하여 생성 되는 제1 프레임에서의 레이어 패널에 대응되는 훈련용 레이어 영상과 와핑된 비교 레이어 영상에 따른 제 1 손실 함수에 기초하여, 제1 인공 지능 모델을 훈련시키는 단계를 포함할 수 있다. 일 실시예에서, 제1 훈련용 프레임 영상 및 제2 훈련용 프레임 영상들 각각은 서로 다른 복수의 뷰에서 획득된 복수의 훈련용 뷰 영상들을 포함할 수 있다. 제1 훈련용 프레임 영상 및 제2 훈련용 프레임 영상 간의 움직임을 추출하는 단계에서, 제1 훈련용 프레임 영상 및 제2 훈련용 프레임 영상 간의 움직임은 제1 훈련용 프레임 영상 에 포함된 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된 센터 뷰 영상 및 제2 훈련용 프레임 영상에 포함된 복수의 훈련용 뷰 영상들 중 센터 뷰에서 획득된 센터 뷰 영상으로부터 추출될 수 있다. 일 실시예에서, 제1 인공 지능 모델의 훈련 방법은 제1 훈련용 프레임 영상 및 제2 훈련용 프레임 영상을 제1 인공 지능 모델에 적용함으로써, 제1 프레임에서 베이스 패널에 대응되는 훈련용 베이스 영상 및 제1 프레 임에서 레이어 패널에 대응되는 훈련용 레이어 영상을 생성하는 단계를 포함할 수 있다. 제1 인공 지능 모 델의 훈련 방법은 훈련용 베이스 영상, 훈련용 레이어 영상, 베이스 패널의 밝기 정보 및 레이어 패널 의 밝기 정보에 기초하여 제1 프레임에서의 훈련용 출력 영상을 획득하는 단계를 포함할 수 있다. 제1 인 공 지능 모델의 훈련 방법은 제1 훈련용 프레임 영상과 훈련용 출력 영상의 차이에 따른 제2 손실 함수에 기초 하여 제1 인공 지능 모델을 훈련시키는 단계를 포함할 수 있다. 일 실시예에서, 보정-베이스 영상을 생성하는 단계에서는 제1 프레임 영상 및 레이어 영상을 영상 보정 모듈에 적용함으로써, 보정-베이스 영상을 생성할 수 있다. 일 실시예에서, 제1 프레임 영상은 서로 다른 복수의 뷰에서 획득된 복수의 뷰 영상들을 포함할 수 있다. 보정- 베이스 영상을 생성하는 단계에서는, 제1 프레임 영상, 레이어 영상 및 복수의 뷰에 각각 대응되는 복수의 쉬프 트 값들을 영상 보정 모듈에 적용함으로써 보정-베이스 영상을 생성할 수 있다. 영상 보정 모듈은 레이어 영상 을 복수의 쉬프트 값들 각각을 이용하여 쉬프트하는 동작이나 기능을 수행하는 명령어를 포함할 수 있다. 영상 보정 모듈은 각각의 복수의 뷰 영상들과 복수의 뷰 영상들에 각각 대응되는 복수의 쉬프트된 레이어 영상들을 이용하여, 보정-베이스 영상을 생성하는 동작이나 기능을 수행하는 명령어를 포함할 수 있다. 복수의 쉬프트 값 들은 베이스 패널과 레이어 패널 간의 간격, 레이어 패널의 해상도 또는 서로 다른 복수의 뷰 중 적어도 하나에 기초하여 결정될 수 있다. 일 실시예에서, 영상 보정 모듈은 베이스 영상에 기초하여 보정-베이스 영상을 추론하도록 훈련된 제2 인공 지 능 모델을 포함할 수 있다. 제2 인공 지능 모델의 훈련 방법은, 영상 생성 모듈을 통하여 생성되는 복수의 서로 다른 뷰에 각각 대응되는 복수의 서브 훈련용 베이스 영상들을 포함하는 훈련용 베이스 영상 및 기준 쉬프트 값 을 획득하는 단계를 포함할 수 있다. 제2 인공 지능 모델의 훈련 방법은, 복수의 훈련용 베이스 영상들 각각을 기준 쉬프트 값을 이용하여 쉬프트하는 단계를 포함할 수 있다. 제2 인공 지능 모델의 훈련 방법은, 제2 인공 지능 모델을 통하여 생성되는 보정-훈련용 베이스 영상과 쉬프트된 훈련용 베이스 영상에 따른 제3 손실 함수에 기초하여 제2 인공 지능 모델을 훈련시키는 단계를 포함할 수 있다. 일 실시예에서, 제3 손실 함수에 기초하여 제2 인공 지능 모델을 훈련시키는 단계는, 보정-훈련용 베이스 영상 에 포함된 복수의 서브 보정-훈련용 베이스 영상들 중 어느 하나의 제1 뷰에 대응되는 서브 보정-훈련용 베이스 영상과 쉬프트된 훈련용 베이스 영상에 포함된 복수의 쉬프트된 서브 훈련용 베이스 영상들 중 제1 뷰에 인접한 제2 뷰에 대응되는 쉬프트된 서브 훈련용 베이스 영상 간의 차이에 따라 제3 손실 함수를 계산하는 단계를 포함 할 수 있다. 제3 손실 함수에 기초하여 제2 인공 지능 모델을 훈련시키는 단계는, 제3 손실 함수에 기초하여 서 브 보정-훈련용 베이스 영상과 쉬프트된 서브 훈련용 베이스 영상의 차이가 적어지도록 제2 인공 지능 모델을 훈련시키는 단계를 포함할 수 있다. 본 개시의 일 실시예로, 개시된 방법의 실시예 중 적어도 하나의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록 매체를 제공할 수 있다. 본 개시에서 설명된 전자 장치에 의해 실행되는 프로그램은 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 프로그램은 컴퓨터로 읽을 수 있는 명 령어들을 수행할 수 있는 모든 시스템에 의해 수행될 수 있다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령어(instruction), 또는 이들 중 하나 이상 의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다.소프트웨어는, 컴퓨터로 읽을 수 있는 저장 매체(computer-readable storage media)에 저장된 명령어를 포함하 는 컴퓨터 프로그램으로 구현될 수 있다. 컴퓨터가 읽을 수 있는 기록 매체로는, 예를 들어 마그네틱 저장 매체 (예컨대, ROM(read-only memory), RAM(random-access memory), 플로피 디스크, 하드 디스크 등) 및 광학적 판 독 매체(예컨대, 시디롬(CD-ROM), 디브이디(DVD: Digital Versatile Disc)) 등이 있다. 컴퓨터가 읽을 수 있는 기록 매체는 네트워크로 연결된 컴퓨터 시스템들에 분산되어, 분산 방식으로 컴퓨터가 판독 가능한 코드가 저장 되고 실행될 수 있다. 기록 매체는 컴퓨터에 의해 판독 가능하며, 메모리에 저장되고, 프로세서에서 실행될 수 있다. 컴퓨터로 읽을 수 있는 저장 매체는, 비일시적(non-transitory) 저장 매체의 형태로 제공될 수 있다. 여기서, ‘비일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다는 것을 의미할 뿐이며, 이 용어는 데이터가 저장 매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구 분하지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 또한, 본 명세서에 개시된 실시예들에 따른 프로그램은 컴퓨터 프로그램 제품(computer program product)에 포 함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 소프트웨어 프로그램, 소프트웨어 프로그램이 저장된 컴퓨터로 읽을 수 있는 저장 매체 를 포함할 수 있다. 예를 들어, 컴퓨터 프로그램 제품은 전자 장치의 제조사 또는 전자 마켓(예를 들어, 삼성 갤럭시 스토어)을 통해 전자적으로 배포되는 소프트웨어 프로그램 형태의 상품(예를 들어, 다운로드 가능한 애 플리케이션(downloadable application))을 포함할 수 있다. 전자적 배포를 위하여, 소프트웨어 프로그램의 적어 도 일부는 저장 매체에 저장되거나, 임시적으로 생성될 수 있다. 이 경우, 저장 매체는 전자 장치의 제조사의 서버, 전자 마켓의 서버, 또는 소프트웨어 프로그램을 임시적으로 저장하는 중계 서버의 저장 매체가 될 수 있 다."}
{"patent_id": "10-2022-0153816", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 컴퓨터 시스템 또는 모듈 등의 구성요소들이 설명된 방법과 다른 형태로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12a 도면12b 도면12c"}
{"patent_id": "10-2022-0153816", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 개시는, 다음의 자세한 설명과 그에 수반되는 도면들의 결합으로 이해될 수 있으며, 참조 번호(reference numerals)들은 구조적 구성요소(structural elements)를 의미한다. 도 1은 본 개시의 일 실시예에 따른 전자 장치를 설명하기 위한 도면이다.도 2는 본 개시의 일 실시예에 따른 전자 장치의 동작을 설명하기 위한 도면이다. 도 3은 본 개시의 일 실시예에 따른 전자 장치의 구성을 설명하기 위한 블록도이다. 도 4는 본 개시의 일 실시예에 따른 전자 장치의 동작을 설명하기 위한 순서도이다. 도 5는 본 개시의 일 실시예에 따른 광학층을 포함하는 전자 장치의 동작을 설명하기 위한 순서도이다. 도 6은 본 개시의 일 실시예에 따른 레이어 영상 및 보정-베이스 영상을 생성하는 전자 장치의 동작을 설명하기 위한 도면이다. 도 7은 본 개시의 일 실시예에 따른, 제1 프레임 영상과 제2 프레임 영상 간의 움직임을 설명하기 위한 도면이 다. 도 8은 본 개시의 일 실시예에 따른 레이어 영상 및 복수의 서브 보정-베이스 영상들을 포함하는 보정-베이스 영상을 생성하는 전자 장치의 동작을 설명하기 위한 도면이다. 도 9는 본 개시의 일 실시예에 따른, 제2 인공 지능 모델을 포함하는 영상 보정 모듈을 통하여 보정-베이스 영 상을 생성하는 전자 장치의 동작을 설명하기 위한 도면이다. 도 10은 본 개시의 일 실시예에 따른, 쉬프트 값을 설명하기 위한 도면이다. 도 11은 본 개시의 일 실시예에 따른, 영상 생성 모듈에 포함된 제1 인공 지능 모델 및 영상 보정 모듈에 포함 된 제2 인공 지능 모델의 훈련 과정을 설명하기 위한 도면이다. 도 12a는 본 개시의 일 실시예에 따른, 제1 인공 지능 모델을 훈련시키기 위한 제1 손실 함수를 설명하기 위한 도면이다. 도 12b는 본 개시의 일 실시예에 따른, 제1 인공 지능 모델을 훈련시키기 위한 제2 손실 함수를 설명하기 위한 도면이다. 도 12c는 본 개시의 일 실시예에 따른, 제2 인공 지능 모델을 훈련시키기 위한 제3 손실 함수를 설명하기 위한 도면이다."}
