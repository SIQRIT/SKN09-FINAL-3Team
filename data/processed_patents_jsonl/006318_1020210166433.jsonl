{"patent_id": "10-2021-0166433", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0079574", "출원번호": "10-2021-0166433", "발명의 명칭": "라이브 공연의 자막 동기화를 위한 인공지능 학습 방법", "출원인": "한국과학기술연구원", "발명자": "박민철"}}
{"patent_id": "10-2021-0166433", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "(a) 프로세서가 공연 촬영 정보를 소정의 시간마다 시각데이터 및 음성데이터를 생성하고, 각 시각데이터 및 음성데이터에 공연시간정보를 추가하는 단계;(b) 프로세서가 시각데이터를 입력받아 시간정보를 추론하는 제1 추론모델에 각 시각데이터에 해당하는 공연시간정보를 제1 정답값으로 설정하고, 각 시간데이터를 입력할 때 출력된 제1 추론시간값과 상기 제1 정답값 사이의 차이가 최소가 될 때까지 학습시키는 단계;(c) 프로세서가 음성데이터를 입력받아 시간정보를 추론하는 제2 추론모델에 각 음성데이터에 해당하는 공연시간정보를 제2 정답값으로 설정하고, 각 음성데이터를 입력할 때 출력된 제2 추론시간값과 상기 제2 정답값 사이의 차이가 최소가 될 때까지 학습시키는 단계; 및(d) 프로세서가 상기 제1 추론시간값과 제2 추론시간값을 입력받아 시간정보를 추론하는 제3 추론모델에 상기제1 추론시간값을 추론할 때 시각데이터와 상기 제2 추론시간값을 추론할 때 음성데이터에 동시에 해당하는 공연시간정보를 제3 정답값으로 설정하고, 상기 제1 추론시간값과 제2 추론시간값을 입력할 때 출력된 제3 추론시간값과 상기 제3 정답값 사이의 차이가 최소가 될 때까지 학습시키는 단계;를 포함하는 공연 시간 추론 모델 학습 방법."}
{"patent_id": "10-2021-0166433", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 (a) 단계에서 음성데이터를 생성할 때,프로세서가 공연 촬영 정보에서 음성신호를 스펙토그램 기반의 시각화된 데이터로 변환하여 음성데이터를 생성하는 공연 시간 추론 모델 학습 방법."}
{"patent_id": "10-2021-0166433", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서,상기 (a) 단계는, 프로세서가 시각화된 데이터의 RGB값의 변화량이 미리 설정된 기준 변화량 이상 변화하는 구간마다 시각데이터및 음성데이터를 생성하는 공연 시간 추론 모델 학습 방법."}
{"patent_id": "10-2021-0166433", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 1에 있어서,상기 (d) 단계는, 프로세서가 입력받은 제1 추론시간값과 제2 추론시간값의 차이가 미리 설정된 기준차이값 이하일 때 실행되는 단계인 공연 시간 추론 모델 학습 방법."}
{"patent_id": "10-2021-0166433", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "컴퓨터에서 청구항 1 내지 청구항 4 중 어느 한 청구항에 따른 공연 시간 추론 모델 학습 방법의 각 단계들을수행하도록 작성되어 컴퓨터로 독출 가능한 기록 매체에 기록된 컴퓨터프로그램."}
{"patent_id": "10-2021-0166433", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공연 촬영 정보를 저장한 메모리;상기 공연 촬영 정보를 소정의 시간마다 시각데이터 및 음성데이터를 생성하고, 각 시각데이터 및 음성데이터에공연시간정보를 추가하는 전처리부;공개특허 10-2023-0079574-3-시각데이터를 입력받아 시간정보를 추론하는 제1 추론모델에 각 시각데이터에 해당하는 공연시간정보를 제1 정답값으로 설정하고, 각 시간데이터를 입력할 때 출력된 제1 추론시간값과 상기 제1 정답값 사이의 차이가 최소가 될 때까지 학습시키는 제1 추론모델학습부;음성데이터를 입력받아 시간정보를 추론하는 제2 추론모델에 각 음성데이터에 해당하는 공연시간정보를 제2 정답값으로 설정하고, 각 음성데이터를 입력할 때 출력된 제2 추론시간값과 상기 제2 정답값 사이의 차이가 최소가 될 때까지 학습시키는 제2 추론모델학습부; 및상기 제1 추론시간값과 제2 추론시간값을 입력받아 시간정보를 추론하는 제3 추론모델에 상기 제1 추론시간값을추론할 때 시각데이터와 상기 제2 추론시간값을 추론할 때 음성데이터에 동시에 해당하는 공연시간정보를 제3정답값으로 설정하고, 상기 제1 추론시간값과 제2 추론시간값을 입력할 때 출력된 제3 추론시간값과 상기 제3정답값 사이의 차이가 최소가 될 때까지 학습시키는 제3 추론모델학습부;를 포함하는 공연 시간 추론 모델 학습장치."}
{"patent_id": "10-2021-0166433", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서,상기 전처리부는, 음성데이터를 생성할 때, 공연 촬영 정보에서 음성신호를 스펙토그램 기반의 시각화된 데이터로 변환하여 음성데이터를 생성하는 공연 시간 추론 모델 학습 장치."}
{"patent_id": "10-2021-0166433", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 7에 있어서,상기 전처리부는, 시각화된 데이터의 RGB값의 변화량이 미리 설정된 기준 변화량 이상 변화하는 구간마다 시각데이터 및 음성데이터를 생성하는 공연 시간 추론 모델 학습 장치."}
{"patent_id": "10-2021-0166433", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 6에 있어서,상기 제3 추론모델학습부는, 입력받은 제1 추론시간값과 제2 추론시간값의 차이가 미리 설정된 기준차이값 이하일 때 제3 추론모델의 학습을 실행하는 공연 시간 추론 모델 학습 장치."}
{"patent_id": "10-2021-0166433", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 6 내지 청구항 9 중 어느 한 청구항에 따른 공연 시간 추론 모델 학습 장치를 통해 학습된 공연 시간 추론 모델;공연시간정보에 대응하는 자막정보를 포함하는 메모리; 및상기 공연 시간 추론 모델에서 추론된 공연 시간 정보에 해당하는 자막정보를 출력하도록 제어하는 표시제어부;를 포함하는 공연 자막 동기화 장치."}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 명세서는 라이브 공연에서 공연자의 대사 또는 가사에 따라 공연 시간을 추론하는 모델을 학습시키는 방법을 제공하는 것을 목적으로 한다. 상술한 과제를 해결하기 위한 본 명세서에 따른 공연 시간 추론 모델 학습 방법은, 공연의 시각데이터를 입력받아 시간정보를 추론하는 제1 추론모델과 공연의 음성데이터를 입력받아 시간 정보를 추론하는 제2 추론모델을 각각 학습시키고, 제1 추론시간값과 제2 추론시간값을 입력받아 시간정보를 추 론하는 제3 추론모델을 학습시킬 수 있다."}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능의 학습 방법에 관한 것이며, 보다 상세하게는 라이브 공연의 자막 동기화를 위한 인공지능 의 학습 방법에 관한 것이다."}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이 부분에 기술된 내용은 단순히 본 명세서에 기재된 실시예에 대한 배경 정보를 제공할 뿐 반드시 종래 기술을 구성하는 것은 아니다. 영화나 드라마와 같은 영상 매체에서 배우의 대사를 자막으로 표시하는 경우가 있다. 이때 표시되는 자막은 자 막의 내용(배우의 대사)과 자막이 표시되는 타이밍 정보로 구성된다. 따라서, 프로그램은 영상의 시작점을 기준 으로 정해진 타이밍에 정해진 자막의 내용을 표시하는 방식으로 화면에 자막을 표시한다.최근 음악 콘서트, 연극, 뮤지컬과 같은 라이브 공연에서도 자막을 표시하는 경우가 있다. 무대의 한 쪽에 스크 린 또는 디스플레이를 설치하고, 공연자의 대사 또는 노래 가사에 맞추어 자막을 표시하는 방식이다. 또한, 최 근에는 AR 글래스를 관람객이 착용하고, AR 글래스의 디스플레이에 자막을 표시하는 기술이 개발되고 있다. 이러한 라이브 공연에서 자막의 표시는 영상 매체에서 자막을 표시하는 것과 다른 제어 기술이 필요하다. 영상 매체와 달리, 라이브 공연은 똑같은 배우의 대사 또는 노래의 가사라도 시작하는 타이밍이 변화할 수 있으며, 현장 상황에 따라 또는 공연자의 순간적인 판단에 따라 대사 또는 노래의 가사를 반복하거나 건너뛰는 상황이 발생할 수 도 있다. 이러한 현장성 및 가변성으로 인해, 현재에는 공연 현장에서 자막을 담당하는 엔지니어가 직접 눈과 귀로 상황 을 파악하고, 자막을 매번 직접 제어하고 있는 것이 현실이다. 선행기술문헌 특허문헌 (특허문헌 0001) 공개특허공보 제10-2009-0129016"}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 명세서는 라이브 공연에서 공연자의 대사 또는 가사에 따라 공연 시간을 추론하는 모델을 학습시키는 방법을 제공하는 것을 목적으로 한다. 본 명세서는 상기 언급된 과제로 제한되지 않으며, 언급되지 않은 또 다른 과제들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 과제를 해결하기 위한 본 명세서에 따른 공연 시간 추론 모델 학습 방법은, (a) 프로세서가 공연 촬영 정보를 소정의 시간마다 시각데이터 및 음성데이터를 생성하고, 각 시각데이터 및 음성데이터에 공연시간정보를 추가하는 단계; (b) 프로세서가 시각데이터를 입력받아 시간정보를 추론하는 제1 추론모델에 각 시각데이터에 해당하는 공연시간정보를 제1 정답값으로 설정하고, 각 시간데이터를 입력할 때 출력된 제1 추론시간값과 상기 제1 정답값 사이의 차이가 최소가 될 때까지 학습시키는 단계; (c) 프로세서가 음성데이터를 입력받아 시간정보 를 추론하는 제2 추론모델에 각 음성데이터에 해당하는 공연시간정보를 제2 정답값으로 설정하고, 각 음성데이 터를 입력할 때 출력된 제2 추론시간값과 상기 제2 정답값 사이의 차이가 최소가 될 때까지 학습시키는 단계; 및 (d) 프로세서가 상기 제1 추론시간값과 제2 추론시간값을 입력받아 시간정보를 추론하는 제3 추론모델에 상 기 제1 추론시간값을 추론할 때 시각데이터와 상기 제2 추론시간값을 추론할 때 음성데이터에 동시에 해당하는 공연시간정보를 제3 정답값으로 설정하고, 상기 제1 추론시간값과 제2 추론시간값을 입력할 때 출력된 제3 추론 시간값과 상기 제3 정답값 사이의 차이가 최소가 될 때까지 학습시키는 단계;를 포함할 수 있다. 본 명세서의 일 실시예에 따르면, 상기 (a) 단계에서 음성데이터를 생성할 때, 프로세서가 공연 촬영 정보에서 음성신호를 스펙토그램 기반의 시각화된 데이터로 변환하여 음성데이터를 생성할 수 있다. 본 명세서의 일 실시예에 따르면, 상기 (a) 단계는, 프로세서가 시각화된 데이터의 RGB값의 변화량이 미리 설정 된 기준 변화량 이상 변화하는 구간마다 시각데이터 및 음성데이터를 생성할 수 있다. 본 명세서의 일 실시예에 따르면, 상기 (d) 단계는, 프로세서가 입력받은 제1 추론시간값과 제2 추론시간값의 차이가 미리 설정된 기준차이값 이하일 때 실행되는 단계일 수 있다. 본 명세서에 따른 공연 시간 추론 모델 학습 방법은 컴퓨터에서 공연 시간 추론 모델 학습 방법의 각 단계들을 수행하도록 작성되어 컴퓨터로 독출 가능한 기록 매체에 기록된 컴퓨터프로그램의 형태로 구현될 수 있다. 상술한 과제를 해결하기 위한 본 명세서에 따른 공연 시간 추론 모델 학습 장치는, 공연 촬영 정보를 저장한 메 모리; 상기 공연 촬영 정보를 소정의 시간마다 시각데이터 및 음성데이터를 생성하고, 각 시각데이터 및 음성데 이터에 공연시간정보를 추가하는 전처리부; 시각데이터를 입력받아 시간정보를 추론하는 제1 추론모델에 각 시각데이터에 해당하는 공연시간정보를 제1 정답값으로 설정하고, 각 시간데이터를 입력할 때 출력된 제1 추론시 간값과 상기 제1 정답값 사이의 차이가 최소가 될 때까지 학습시키는 제1 추론모델학습부; 음성데이터를 입력받 아 시간정보를 추론하는 제2 추론모델에 각 음성데이터에 해당하는 공연시간정보를 제2 정답값으로 설정하고, 각 음성데이터를 입력할 때 출력된 제2 추론시간값과 상기 제2 정답값 사이의 차이가 최소가 될 때까지 학습시 키는 제2 추론모델학습부; 및 상기 제1 추론시간값과 제2 추론시간값을 입력받아 시간정보를 추론하는 제3 추론 모델에 상기 제1 추론시간값을 추론할 때 시각데이터와 상기 제2 추론시간값을 추론할 때 음성데이터에 동시에 해당하는 공연시간정보를 제3 정답값으로 설정하고, 상기 제1 추론시간값과 제2 추론시간값을 입력할 때 출력된 제3 추론시간값과 상기 제3 정답값 사이의 차이가 최소가 될 때까지 학습시키는 제3 추론모델학습부;를 포함할 수 있다. 본 명세서의 일 실시예에 따르면, 상기 전처리부는, 음성데이터를 생성할 때, 공연 촬영 정보에서 음성신호를 스펙토그램 기반의 시각화된 데이터로 변환하여 음성데이터를 생성할 수 있다. 본 명세서의 일 실시예에 따르면, 상기 전처리부는, 시각화된 데이터의 RGB값의 변화량이 미리 설정된 기준 변 화량 이상 변화하는 구간마다 시각데이터 및 음성데이터를 생성할 수 있다. 본 명세서의 일 실시예에 따르면, 상기 제3 추론모델학습부는, 입력받은 제1 추론시간값과 제2 추론시간값의 차 이가 미리 설정된 기준차이값 이하일 때 제3 추론모델의 학습을 실행할 수 있다. 상술한 과제를 해결하기 위한 본 명세서에 따른 공연 시간 추론 모델 학습 방법은, 본 명세서에 따른 공연 시간 추론 모델 학습 장치를 통해 학습된 공연 시간 추론 모델; 공연시간정보에 대응하는 자막정보를 포함하는 메모 리; 및 상기 공연 시간 추론 모델에서 추론된 공연 시간 정보에 해당하는 자막정보를 출력하도록 제어하는 표시 제어부;를 포함할 수 있다. 본 발명의 기타 구체적인 사항들은 상세한 설명 및 도면들에 포함되어 있다."}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 명세서에 따라 학습시킨 공연 시간 추론 모델은 라이브 공연의 촬영 정보를 이용하여 공연자의 대사 또는 가 사 정확한 타이밍에 자막을 표시할 수 있다. 이를 통해, 사람이 직접 자막의 표시 시점을 제어하는 종래 기술에 비해, 사람의 실수가 개입될 가능성이 배제되어 공연 사고를 줄일 수 있다."}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과들은 이상에서 언급된 효과로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재로 부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에 개시된 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후 술되어 있는 실시예들을 참조하면 명확해질 것이다. 그러나, 본 명세서가 이하에서 개시되는 실시예들에 제한되 는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 명세서의 개시가 완전하도 록 하고, 본 명세서가 속하는 기술 분야의 통상의 기술자(이하 '당업자')에게 본 명세서의 범주를 완전하게 알 려주기 위해 제공되는 것이며, 본 명세서의 권리 범위는 청구항의 범주에 의해 정의될 뿐이다. 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 명세서의 권리 범위를 제한하고자 하는 것은 아니다. 본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한다(comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성요소 외에 하나 이상의 다른 구성요소의 존재 또는 추가를 배제하지 않는다. 명세서 전체에 걸쳐 동일한 도면 부호는 동일한 구성 요소를 지칭하며, \"및/또는\"은 언급된 구성요소들의 각각 및 하나 이상의 모든 조합을 포함한다. 비록 \"제1\", \"제2\" 등이 다양한 구성요소들을 서술하기 위해서 사용되나, 이들 구성요소들은 이들 용어에 의해 제한되지 않음은 물론이다. 이들 용어들은 단지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사용하는 것이다. 따라서, 이하에서 언급되는 제1 구성요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있음은 물론이다. 다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어(기술 및 과학적 용어를 포함)는 본 명세서가 속하는 기 술분야의 통상의 기술자에게 공통적으로 이해될 수 있는 의미로 사용될 수 있을 것이다. 또한, 일반적으로 사용 되는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되 지 않는다. 이하, 첨부된 도면을 참조하여 본 명세서에 따른 공연 시간 추론 모델 학습 방법을 설명한다. 도 1은 본 명세서에 따른 공연 시간 추론 모델 학습 방법을 개략적으로 도시한 흐름도이다. 도 2는 본 명세서에 따른 공연 시간 추론 모델 학습 장치의 구성을 개략적으로 도시한 블럭도이다. 도 1 및 도 2를 함께 참조하면, 본 명세서에 따른 공연 시간 추론 모델 학습 방법은 크게 학습을 위한 공연 촬 영 정보 전처리 단계(S100), 시각데이터를 이용하여 공연시간을 추론하는 제1 추론모델을 학습시키는 단계 (S110), 음성데이터를 이용하여 공연시간을 추론하는 제2 추론모델을 학습시키는 단계(S120) 및 제1 추론모델이 시각데이터를 이용하여 추론한 공연시간과 제2 추론모델이 음성데이터를 이용하여 추론한 공연시간을 이용한 최 종공연시간을 추론하는 제3 추론모델을 학습시키는 단계(S130)를 포함할 수 있다. 각 단계에 대해서 보다 자세 히 설명하겠다. 먼저, 공연 촬영 정보 전처리 단계(S100)이다. 본 명세서에서 공연 촬영 정보란 무대위에서 이루어지는 공연을 촬영한 정보로서 영상 및 음성 정보를 가진 데 이터를 의미한다. 상기 공연 촬영 정보는 공연자의 리허설 무대를 촬영하여 확보할 수 있다. 또는 공연이 수차 례 반복되는 경우, 실제 공연을 촬영하여 공연 촬영 정보를 확보할 수 있다. 확보된 공연 촬영 정보는 메모리에 저장되고, 학습을 위해 전처리 과정을 거치게 된다. 본 명세서에 따른 전처리부는 시각데이터와 음성데이터로 나누어 각각 생성할 수 있다. 이때, 상기 전처리 부는 각 시각데이터 및 음성데이터에 공연시간정보를 추가할 수 있다. 본 명세서에서 공연시간정보란, 공 연이 시작한 시간을 기준으로 얼마만큼의 시간이 지났는지 나타내는 정보이다. 예를 들어, 2시간짜리 공연에서 공연자가 A라는 노래를 부르면서 춤을 추는 시각이 01시간 05분 30초 지점일 때, 해당 노래와 춤에 대한 시각데 이터 및 음성데이터에는 [010530]이라는 공연시간정보가 추가될 수 있다. 본 명세서의 일 실시예에 따르면, 상기 전처리부는 음성데이터를 생성할 때, 공연 촬영 정보에서 음성신호 를 스펙토그램 기반의 시각화된 데이터로 변환하여 음성데이터를 생성할 수 있다. 도 3은 전처리된 시각데이터 및 음성데이터의 참고도이다. 도 3을 참조하면, 공연시간정보출을 기준으로 위에는 시각데이터가 아래에는 음성데이터가 도시된 것을 확인할 수 있다. 본 명세서에 따르면, 상기 전처리부는 공연 촬영 정보의 음성파일을 데이터를 -1에서 1사이의 값들로 정규 화시킬 수 있다."}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "그리고 전처리부는 푸리에 변환을 통해 시간축으로 구성된 데이터를 주파수와 시간축으로 성분 분해해서 차원을 늘려줄 수 있다. 그리고 전처리부는 MFCC기반으로 유리한 정보 추출할 수 있다. 이 때 전처리부 는 추출하는 정보의 수를 랜덤(random)하게 설정하여 10개의 정보에 해당하는 것을 모아서 사용할 수 있다. X_bags = {M_1, M_2, …, M10} 한편, 본 명세서에 따른 제1 추론모델을 학습시키는 것과 제2 추론모델을 학습시키는 것은 서로 독립된 모델로 서 시간 순서에 상관없이 실행이 가능하다. 바람직하게, 제1 추론모델을 학습시킬 때 사용하는 시각데이터와 제 2 추론모델을 학습시킬 때 사용하는 음성데이터의 공연시각정보가 동일하다. 또한, 상기 전처리부는 시각데이터와 음성데이터를 소정의 시간 길이를 가진 데이터로 생성할 수 있다. 일 예에 따르면, 상기 전처리부는 1초, 2초, 3초 등과 같이 미리 설정된 시간 간격마다 시각데이터 및 음성데이터를 생성할 수 있다. 다른 예에 따르면, 상기 전처리부는 시각화된 데이터의 RGB값의 변화량이 미리 설 정된 기준 변화량 이상 변화하는 구간마다 시각데이터 및 음성데이터를 생성할 수 있다. 즉, 음성이 크게 변화 하는 구간마다 식별이 용이하다는 점을 이용한 것이다. 보다 구체적으로, 상기 전처리부 변환된 스펙토그 램의 주어진 시간 t동안 RGB값의 평균과 t를 n(0부터 10까지)등분을 통해 나눈 스펙토그램의 평균치가 255의 1/5이상 커지는 n을 찾아서 n-1지점을 최적의 시간으로 설정하여 지점을 기준으로 시각데이터와 음성데이터를 생성할 수 있다. 또 다른 실시예로, 상기 전처리부는 음성이 아닌, 영상이 크게 변화하는 지점마다, 시각 데이터 및 음성데이터를 생성할 수 있다. 이렇게 생성된 시각데이터는 제1 추론모델을 학습시키는데 사용되고, 음성데이터는 제2 추론모델을 학습시키는 데 사용될 수 있다. 다음, 제1 추론모델의 학습 단계(S110)이다. 본 명세서에서 제1 추론모델은 시각데이터를 입력받아 시간정보를 추론하는 인공지능모델이다. 제1 추론모델학 습부는 각 시각데이터에 해당하는 공연시간정보를 제1 정답값으로 설정활 수 있다. 제1 추론모델학습부 는 각 시간데이터를 제1 추론모델에 입력시켜서 추론된 시간정보(이하 '제1 추론시간값')를 출력시킬 수 있다. 이때, 제1 추론모델학습부는 출력된 제1 추론시간값과 상기 제1 정답값 사이의 차이가 최소가 될 때 까지 제1 추론모델을 학습시킬 수 있다. 다음, 제2 추론모델의 학습 단계(S120)이다. 본 명세서에서 제2 추론모델은 음성데이터를 입력받아 시간정보를 추론하는 인공지능모델이다. 제2 추론모델학 습부는 각 음성데이터에 해당하는 공연시간정보를 제2 정답값으로 설정활 수 있다. 제2 추론모델학습부 는 각 음성데이터를 제2 추론모델에 입력시켜서 추론된 시간정보(이하 '제2 추론시간값')를 출력시킬 수 있다. 이때, 제2 추론모델학습부는 출력된 제2 추론시간값과 상기 제2 정답값 사이의 차이가 최소가 될 때 까지 제2 추론모델을 학습시킬 수 있다. 이렇게 제1 추론모델과 제2 추론모델은 각각 시각데이타와 음성데이터를 통해 공연시간을 추론하는 모델로 학습 을 마칠 수 있다. 같은 시간정보를 가지는 시각데이터와 음성데이터에 대해서 제1 추론모델과 제2 추론모델이 서로 같은 공연시간을 추론하면 바람직하다. 그러나 같은 시간정보를 가지는 시각데이터와 음성데이터에 대해서 제1 추론모델과 제2 추론모델이 서로 다른 공연시간을 추론할 수도 있다. 예를 들어, 공연자가 의자에 앉아서 움직이지 않고 노래를 할 경우, 시각데이터가 추론한 공연시간의 정확도가 낮을 것으로 예상할 수 있다. 반면, 공연자가 노래의 1절 후렴구와 2절 후렴구를 부르는 경우, 음성데이터가 추론한 공연시간의 정확도가 낮을 것으 로 예상할 수 있다. 이처럼 추론된 2개의 공연시간이 서로 다를 경우, 최종적으로 하나의 공연시간을 추론하는 인공지능이 필요하다. 이러한 역할을 하는 것이 제3 추론모델이다. 다음, 제3 추론모델의 학습 단계(S130)이다. 본 명세서에서 제3 추론모델은 제1 추론시간값과 제2 추론시간값을 입력받아 시간정보를 추론하는 인공지능모델 이다. 제3 추론모델학습부는 상기 제1 추론시간값을 추론할 때 시각데이터와 상기 제2 추론시간값을 추론 할 때 음성데이터에 동시에 해당하는 공연시간정보를 제3 정답값으로 설정할 수 있다. 제3 추론모델학습부(13 0)는 상기 제1 추론시간값과 제2 추론시간값을 제3 추론모델에 입력시켜서 추론된 시간정보(이하 '제3 추론시간 값')를 출력시킬 수 있다. 이때, 제3 추론모델학습부는 출력된 제3 추론시간값과 상기 제3 정답값 사이의 차이가 최소가 될 때까지 제3 추론모델을 학습시킬 수 있다. 한편, 제1 추론시간값과 제2 추론시간값의 차이가 지나칠 경우가 발생할 수 있다. 예를 들어, 공연자가 앉아서 3분짜리 노래를 부를 경우, 시각데이터에 의해 3분짜리 노래의 초반 시간을 제1 추론시간값이 출력하고, 음성데 이터에 의해 3분짜리 노래의 마지막 시간을 제2 추론시간값이 출력될 수 있다. 이렇게 두 추론시간값이 일정 시 간 이상 차이날 경우, 해당 데이터는 오히려 학습에 방해가 되는 데이터가 될 수 있다. 따라서, 본 명세서에 따 른 제3 추론모델학습부는 입력받은 제1 추론시간값과 제2 추론시간값의 차이가 미리 설정된 기준차이값 이 하일 때 제3 추론모델을 학습시킬 수 있다. 상기 기준차이값은 1초, 2초, 3초, 5초 등 다양하게 설정될 수 있다. 또한, 제1 추론시간값과 제2 추론시간값의 차이에 대한 평균을 이용하여 설정될 수도 있다. 이렇게 학습된 제1 추론모델, 제2 추론모델 및 제3 추론모델은 공연 시간 추론 모델을 구성요소가 될 수 있다. 따라서, 실제 현장에서 라이브 공연에 대한 촬영 정보를 입력하면, 본 명세서에 따른 공연 시간 추론 모델은 지 금 공연 장면이 전체 공연의 어느 시점인지 추론하여 공연시간정보를 출력할 수 있다.이때, 공연시간정보에 대응하는 자막정보는 미리 메모리에 저장될 수 있다. 자막정보란, 공연의 어느 시점에 어 떠한 자막이 표시되어야 하는지 자막 내용과 시간 정보를 포함하는 데이터이다. 이때, 본 명세서에 따른 표시제 어부(도면 미도시)는 상기 공연 시간 추론 모델에서 추론된 공연 시간 정보에 해당하는 자막정보를 출력하도록 제어할 수 있다. 상기 표시제어부, 메모리 및 공연 시간 추론 모델은 공연 자막 동기화 장치의 일 구성요소가 될 수 있다. 본 명세서에 따른 공연 시간 추론 모델 학습 방법은 컴퓨터에서 공연 시간 추론 모델 학습 방법의 각 단계들을 수행하도록 작성되어 컴퓨터로 독출 가능한 기록 매체에 기록된 컴퓨터프로그램의 형태로 구현될 수 있다."}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "상기 제1 내지 제3 추론모델학습부는, 산출 및 다양한 제어 로직을 실행하기 위해 본 발명이 속한 기술분야에 알려진 프로세서, ASIC(application-specific integrated circuit), 다른 칩셋, 논리 회로, 레지스터, 통신 모 뎀, 데이터 처리 장치 등을 포함할 수 있다. 또한, 상술한 제어 로직이 소프트웨어로 구현될 때, 상기 제1 내지 제3 추론모델학습부는 프로그램 모듈의 집합으로 구현될 수 있다. 이 때, 프로그램 모듈은 상기 메모리에 저장 되고, 프로세서에 의해 실행될 수 있다. 상기 컴퓨터프로그램은, 상기 컴퓨터가 프로그램을 읽어 들여 프로그램으로 구현된 상기 방법들을 실행시키기 위하여, 상기 컴퓨터의 프로세서(CPU)가 상기 컴퓨터의 장치 인터페이스를 통해 읽힐 수 있는 C/C++, C#, JAVA, Python, 기계어 등의 컴퓨터 언어로 코드화된 코드(Code)를 포함할 수 있다. 이러한 코드는 상기 방법들을 실행 하는 필요한 기능들을 정의한 함수 등과 관련된 기능적인 코드(Functional Code)를 포함할 수 있고, 상기 기능 들을 상기 컴퓨터의 프로세서가 소정의 절차대로 실행시키는데 필요한 실행 절차 관련 제어 코드를 포함할 수 있다. 또한, 이러한 코드는 상기 기능들을 상기 컴퓨터의 프로세서가 실행시키는데 필요한 추가 정보나 미디어 가 상기 컴퓨터의 내부 또는 외부 메모리의 어느 위치(주소 번지)에서 참조되어야 하는지에 대한 메모리 참조관 련 코드를 더 포함할 수 있다. 또한, 상기 컴퓨터의 프로세서가 상기 기능들을 실행시키기 위하여 원격(Remot e)에 있는 어떠한 다른 컴퓨터나 서버 등과 통신이 필요한 경우, 코드는 상기 컴퓨터의 통신 모듈을 이용하여 원격에 있는 어떠한 다른 컴퓨터나 서버 등과 어떻게 통신해야 하는지, 통신 시 어떠한 정보나 미디어를 송수신 해야 하는지 등에 대한 통신 관련 코드를 더 포함할 수 있다. 상기 저장되는 매체는, 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체가 아니라 반 영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 구체적으로는, 상기 저 장되는 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 데이터 저장장치 등이 있지만, 이에 제한되지 않는다. 즉, 상기 프로그램은 상기 컴퓨터가 접속할 수 있는 다양한 서버 상의 다양한 기록매체 또는 사용자의 상기 컴퓨터상의 다양한 기록매체에 저장될 수 있다. 또한, 상기 매체는 네트워크로 연결된 컴퓨터 시 스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장될 수 있다."}
{"patent_id": "10-2021-0166433", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이상, 첨부된 도면을 참조로 하여 본 명세서의 실시예를 설명하였지만, 본 명세서가 속하는 기술분야의 통상의 기술자는 본 발명이 그 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 실시될 수 있 다는 것을 이해할 수 있을 것이다. 그러므로, 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며, 제한 적이 아닌 것으로 이해해야만 한다."}
{"patent_id": "10-2021-0166433", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 명세서에 따른 공연 시간 추론 모델 학습 방법을 개략적으로 도시한 흐름도이다. 도 2는 본 명세서에 따른 공연 시간 추론 모델 학습 장치의 구성을 개략적으로 도시한 블럭도이다. 도 3은 전처리된 시각데이터 및 음성데이터의 참고도이다."}
