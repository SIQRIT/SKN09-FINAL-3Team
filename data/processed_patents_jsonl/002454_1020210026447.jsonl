{"patent_id": "10-2021-0026447", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0106000", "출원번호": "10-2021-0026447", "발명의 명칭": "튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템", "출원인": "주식회사 증강지능", "발명자": "조근식"}}
{"patent_id": "10-2021-0026447", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자 명령의 의미 구조를 분석하고 사용자 요청 목록을 생성하여, 생성된 목록에 기초하여 텍스트 음성 변환API 서비스 및 사용자에 의한 음성 명령의 오디오 데이터 셋을 수집하는 데이터 셋 수집부; 수집된 음성 명령의 오디오 데이터 셋으로부터 오디오 특징을 추출하는 오디오 특징 추출부; CNN 기반 네트워크를 사용하여 추출된 오디오 특징에 대하여 학습하고 음성 명령 및 언어를 분류함으로써 요청된 음성 명령의 유형과 음성 명령이 실행된 언어를 예측하는 학습 및 예측부; 및 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 따른 작업을 수행하는 명령 훈련부를 포함하는 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템."}
{"patent_id": "10-2021-0026447", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 학습 및 예측부는, MFCC(Mel-frequency cepstral coefficients)의 오디오 특징 벡터를 입력 받아 CNN 기반 컨볼루션, 배치정규화, 최대 풀링 및 드롭아웃 레이어를 포함하는 CNN 기반 네트워크를 사용하여 음성 명령 및 언어를 분류하기 위한 학습을 수행하는 CNN 학습부; 덴스 레이어 및 덴스 레이어 내부 유닛, 드롭아웃, 소프트맥스 활성화 함수를 포함하고, 사용자가 요청한 음성명령의 유형을 분류하여 각 명령 유형에 대한 표현인 최종 출력 벡터를 제공하는 음성 명령 분류부; 및 덴스 레이어 및 덴스 레이어 내부 유닛, 드롭아웃, 소프트맥스 활성화 함수를 포함하고, 덴스 레이어의 수는 음성 명령 분류부의 덴스 레이어 보다 작고, 사용자가 요청한 음성 명령의 언어를 분류하여 각 명령 언어에 대한표현인 최종 출력 벡터를 제공하는 언어 분류부를 포함하는 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템."}
{"patent_id": "10-2021-0026447", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 명령 훈련부는, 학습 및 예측부에서 학습되고 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 대하여 과적합을 피하기위해 모델 가중치에 정규화를 적용하는 가중치 감소(Weight Decay)를 적용하고, 학습 속도를 설정하기 위해 코사인 어닐링 스케줄러(Cosine Annealing Scheduler)를 사용하고, 학습되고 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 대해 손실 가중치가 동일하도록 범주형 교차 엔트로피 손실을 적용하여 모델을 훈련하는 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템."}
{"patent_id": "10-2021-0026447", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "데이터 셋 수집부를 통해 사용자 명령의 의미 구조를 분석하고 사용자 요청 목록을 생성하여, 생성된 목록에 기초하여 텍스트 음성 변환 API 서비스 및 사용자에 의한 음성 명령의 오디오 데이터 셋을 수집하는 단계; 수집된 음성 명령의 오디오 데이터 셋으로부터 오디오 특징 추출부를 통해 오디오 특징을 추출하는 단계; 학습 및 예측부를 통해 CNN 기반 네트워크를 사용하여 추출된 오디오 특징에 대하여 학습하고 음성 명령 및 언어를 분류함으로써 요청된 음성 명령의 유형과 음성 명령이 실행된 언어를 예측하는 단계; 및 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 따라 명령 훈련부를 통해 작업을 수행하는 단계 공개특허 10-2022-0106000-3-를 포함하는 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템의 동작 방법."}
{"patent_id": "10-2021-0026447", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 학습 및 예측부를 통해 CNN 기반 네트워크를 사용하여 추출된 오디오 특징에 대하여 학습하고 음성 명령 및 언어를 분류함으로써 요청된 음성 명령의 유형과 음성 명령이 실행된 언어를 예측하는 단계는, 학습 및 예측부의 CNN 학습부에서 MFCC(Mel-frequency cepstral coefficients)의 오디오 특징 벡터를 입력 받아 CNN 기반 컨볼루션, 배치 정규화, 최대 풀링 및 드롭아웃 레이어를 포함하는 CNN 기반 네트워크를 사용하여음성 명령 및 언어를 분류하기 위한 학습을 수행하는 단계; 덴스 레이어 및 덴스 레이어 내부 유닛, 드롭아웃, 소프트맥스 활성화 함수를 포함하는 학습 및 예측부의 음성명령 분류부에서, 사용자가 요청한 음성 명령의 유형을 분류하여 각 명령 유형에 대한 표현인 최종 출력 벡터를제공하는 단계; 및 덴스 레이어(덴스 레이어의 수는 음성 명령 분류부의 덴스 레이어 보다 작음) 및 덴스 레이어 내부 유닛, 드롭아웃, 소프트맥스 활성화 함수를 포함하는 학습 및 예측부의 언어 분류부에서 사용자가 요청한 음성 명령의 언어를 분류하여 각 명령 언어에 대한 표현인 최종 출력 벡터를 제공하는 단계를 포함하는 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템의 동작 방법."}
{"patent_id": "10-2021-0026447", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템 및 그 동작 방법이 제시된다. 본 발명에서 제 안하는 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템은 사용자 명령의 의미 구조를 분석하고 사용자 요청 목록을 생성하여, 생성된 목록에 기초하여 텍스트 음성 변환 API 서비스 및 사용자에 의한 음성 명 령의 오디오 데이터 셋을 수집하는 데이터 셋 수집부, 수집된 음성 명령의 오디오 데이터 셋으로부터 오디오 특 징을 추출하는 오디오 특징 추출부, CNN 기반 네트워크를 사용하여 추출된 오디오 특징에 대하여 학습하고 음성 명령 및 언어를 분류함으로써 요청된 음성 명령의 유형과 음성 명령이 실행된 언어를 예측하는 학습 및 예측부 및 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 따른 작업을 수행하는 명령 훈련부를 포함한다."}
{"patent_id": "10-2021-0026447", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템에 관한 것이다."}
{"patent_id": "10-2021-0026447", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "혼합 현실(Mixed Reality; MR) 콘텐츠 어플리케이션이 일상 생활에 포함되기 시작한 최근에는 사용자에게 최고 의 인간-컴퓨터 상호 작용 경험을 제공하는 것이 필수적이다. 여기에는 제스처, 음성, 시선 추적 입력이 포함되 며, 어플리케이션에 따라 상호 작용 유형 중 하나가 중요한 역할을 할 수 있다. MR 콘텐츠를 기반으로 하는 스 마트 안경을 사용하는 엔지니어나 학생의 손 동작 및 동작 흐름을 개선하기 위해, 예를 들어 음성 명령은 버튼 을 클릭하거나 손동작을 사용하지 않고도 어플리케이션과 원활하게 상호 작용할 수 있는 기능을 필요로 한다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국 등록특허 제10-2128894호(2020.06.25.) (특허문헌 0002) 한국 등록특허 제10-2218207호(2021.02.16.)"}
{"patent_id": "10-2021-0026447", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 이루고자 하는 기술적 과제는 항공기 정비 직원 및 교육생을 위한 MR 콘텐츠를 기반으로 하는 스마트 안경용 AI 챗봇 시스템을 제공하는데 있다. 제안하는 챗봇 시스템을 통해 영어와 한국어로 혼합된 음성 요청을 처리할 수 있으며, 사용자 음성 요청을 처리하고 언급된 명령어 유형을 예측하고자 한다. 또한, 적은 수의 훈련 파라미터를 가지고 있고, 디스크 메모리 할당 크기를 가지고 있으며, 추론이 빠르고, 거대한 처리 자원을 필요 로 하지 않아 스마트 안경과 같은 독립 실행형 장치에 적합한 경량의 스마트 안경용 AI 챗봇 시스템을 제공한다."}
{"patent_id": "10-2021-0026447", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 측면에 있어서, 본 발명에서 제안하는 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템은 사용자 명령의 의미 구조를 분석하고 사용자 요청 목록을 생성하여, 생성된 목록에 기초하여 텍스트 음성 변환 API 서비스 및 사용자에 의한 음성 명령의 오디오 데이터 셋을 수집하는 데이터 셋 수집부, 수집된 음성 명령의 오디오 데이터 셋으로부터 오디오 특징을 추출하는 오디오 특징 추출부, CNN 기반 네트워크를 사용하여 추출된 오디오 특징에 대하여 학습하고 음성 명령 및 언어를 분류함으로써 요청된 음성 명령의 유형과 음성 명령이 실 행된 언어를 예측하는 학습 및 예측부 및 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 따른 작업을 수행하는 명령 훈련부를 포함한다. 학습 및 예측부는 MFCC(Mel-frequency cepstral coefficients)의 오디오 특징 벡터를 입력 받아 CNN 기반 컨볼 루션, 배치 정규화, 최대 풀링 및 드롭아웃 레이어를 포함하는 CNN 기반 네트워크를 사용하여 음성 명령 및 언 어를 분류하기 위한 학습을 수행하는 CNN 학습부, 덴스 레이어 및 덴스 레이어 내부 유닛, 드롭아웃, 소프트맥 스 활성화 함수를 포함하고, 사용자가 요청한 음성 명령의 유형을 분류하여 각 명령 유형에 대한 표현인 최종 출력 벡터를 제공하는 음성 명령 분류부 및 덴스 레이어 및 덴스 레이어 내부 유닛, 드롭아웃, 소프트맥스 활성 화 함수를 포함하고, 덴스 레이어의 수는 음성 명령 분류부의 덴스 레이어 보다 작고, 사용자가 요청한 음성 명 령의 언어를 분류하여 각 명령 언어에 대한 표현인 최종 출력 벡터를 제공하는 언어 분류부를 포함한다. 명령 훈련부는 학습 및 예측부에서 학습되고 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 대하여 과 적합을 피하기 위해 모델 가중치에 정규화를 적용하는 가중치 감소(Weight Decay)를 적용하고, 학습 속도를 설 정하기 위해 코사인 어닐링 스케줄러(Cosine Annealing Scheduler)를 사용하고, 학습되고 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 대해 손실 가중치가 동일하도록 범주형 교차 엔트로피 손실을 적용하여 모 델을 훈련한다. 또 다른 일 측면에 있어서, 본 발명에서 제안하는 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시 스템의 동작 방법은 데이터 셋 수집부를 통해 사용자 명령의 의미 구조를 분석하고 사용자 요청 목록을 생성하 여, 생성된 목록에 기초하여 텍스트 음성 변환 API 서비스 및 사용자에 의한 음성 명령의 오디오 데이터 셋을 수집하는 단계, 수집된 음성 명령의 오디오 데이터 셋으로부터 오디오 특징 추출부를 통해 오디오 특징을 추출 하는 단계, 학습 및 예측부를 통해 CNN 기반 네트워크를 사용하여 추출된 오디오 특징에 대하여 학습하고 음성 명령 및 언어를 분류함으로써 요청된 음성 명령의 유형과 음성 명령이 실행된 언어를 예측하는 단계 및 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 따라 명령 훈련부를 통해 작업을 수행하는 단계를 포함한다."}
{"patent_id": "10-2021-0026447", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들에 따르면 항공기 정비 직원 및 교육생을 위한 MR 콘텐츠를 기반으로 하는 스마트 안경용 AI 챗봇 시스템을 통해 영어와 한국어로 혼합된 음성 요청을 처리할 수 있으며, 사용자 음성 요청을 처리하고 언급 된 명령어 유형을 예측할 수 있다. 또한, 제안하는 챗봇 시스템은 경량으로서, 적은 수의 훈련 파라미터를 가지 고 있고, 디스크 메모리 할당 크기를 가지고 있으며, 추론이 빠르고, 거대한 처리 자원을 필요로 하지 않아 스 마트 안경과 같은 독립 실행형 장치에 적합하다."}
{"patent_id": "10-2021-0026447", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "스마트 안경에 의해 구동되는 MR(Mixed Reality)는 빠르게 전 세계에 등장하고 있으며, 엔지니어들의 작업흐름 과 학생들의 교육을 향상시키고 더 이상 도구를 들고 있지 않고 양손으로 작업할 수 있는 능력을 줌으로써 유지 보수 등 다양한 산업에서 그 응용을 찾아내고 있다. 이것은 음성 명령을 사용하는 인간과 컴퓨터의 상호 작용에 대한 엄청난 필요성을 만들어낸다. 따라서 본 발명에서는 항공기 정비 직원 및 교육생을 위한 MR 콘텐츠를 기반 으로 하는 스마트 안경용 AI 챗봇을 제안한다. 제안된 챗봇은 영어와 한국어로 혼합된 음성 요청을 처리할 수 있으며, 사용자 음성 요청을 처리하고 언급된 명령어 유형을 예측한다. 수행된 평가는 명령 유형과 언어 식별에 대해 각각 약 95%와 99%의 정확도를 보였다. 더욱이, 챗봇의 모델은 경량이다. 따라서, 적은 수의 훈련 파라미 터를 가지고 있고, 디스크 메모리 할당 크기를 가지고 있으며, 추론이 빠르고, 거대한 처리 자원을 필요로 하지 않아 스마트 안경과 같은 독립 실행형 장치에 적합하다. 이하, 본 발명의 실시 예를 첨부된 도면을 참조하여 상 세하게 설명한다. 본 발명에서는 항공 MRO(Maintenance, Repair and Overhaul)를 위한 MR 콘텐츠 기반 어리케이션과 학생 교육을 위한 AI 챗봇을 제안하였다. 어플리케이션 사용자들은 한국어와 영어의 음성 명령을 서로 교환하여 사용하므로, 언어를 미리 정의하지 않고도 명령을 이해할 수 있는 혼합 단일 모델을 제안한다. 또한 챗봇 내 네트워크는 참 조된 명령어뿐만 아니라 음성으로 된 언어까지 파악할 수 있어 각각 영어나 한국어로 피드백을 주면서 어플리케 이션이 이에 대응하게 된다. 도 1은 본 발명의 일 실시예에 따른 AI 챗봇 상호 작용 흐름을 설명하기 위한 도면이다. 도 1은 사용자(User)가 AI 챗봇에게 음성 요청(Speech Request)을 하는 상호작용 흐름을 나타낸 것으로, AI 챗 봇이 명령어(Command)와 언어 정보(Language)를 예측하여 이를 처리한 뒤 필요한 기능을 호출하는 혼합현실 콘 텐츠(Mixed Reality Content) 어플리케이션에 제공한다. 동시에, AI 챗봇은 사용자에게 상응하는 피드백 (Feedback)을 제공한다. 챗봇에서 제안된 딥 러닝 모델은 훈련 파라미터의 수, 메모리 할당 크기 및 추론 시간 측면에서 경량이며 스마트 안경의 프로세싱 소스를 많이 소모하지 않는다. 본 발명의 실시예에 따르면, 음성 상호 작용 기능을 기반으로 음성 명령 데이터셋을 수집하여 AI 챗봇의 추론 네트워크를 훈련시킨 후, 그것을 스마트 안경 Microsoft HoloLens 2로 성공적으로 불러왔다. 또한, 다양한 분류 지표와 테스트 데이터를 사용하여 챗봇을 평가한다. 실험 결과, 챗봇 네트워크는 음성 명령의 경우 약 95%, 영 어와 한국어 참여자 데이터에 대한 언어 식별의 경우 약 99%의 정확도를 달성했다. 도 2는 본 발명의 일 실시예에 따른 혼합 현실 콘텐츠 스냅 샷 및 AI 챗봇 참조 기능을 나타내는 도면이다. MR 콘텐츠용 AI 챗봇을 개발하기 위해, 우선 참조해야 할 기능을 분석해야 한다. 본 발명의 실시예에 따른 MR 콘텐츠 기반 어플리케이션은 항공 정비사와 학생들이 MRO 및 교육 절차를 수행할 수 있도록 개발되었다. 어플리 케이션의 기능을 클릭하지 않고도 제어할 수 있는 것이 주요 목표다. 도 2는 사용자의 관점에서 MR 콘텐츠를 보 여 주는 스냅샷이며, AI 챗봇과 함께 호출되도록 개발된 기능은 다음과 같다: 1. IPC Manual: IPC(Illustrated Parts Catalogue)는 모든 구성 요소를 포괄적으로 자세히 설명하는 항공기 유 형별 참조 문서이다. 이 명령이 실행될 때, 문서는 전용 영역에 표시되어야 한다(도 2 참조). 2. AMM Manual: 항공기 정비 매뉴얼(Aircraft Maintenance Manual; AMM)은 항공기에 대해 수행된 정비 작업의 단계를 상세히 기술한 문서이다. IPC Manual과 마찬가지로 AMM Manual도 특수 패널에 표시되도록 되어 있다(도 2 참조). 3. Next Instruction: 다음 단계에 필요한 내용이 표시된다. 4. Previous Instruction: 이전 단계에 필요한 내용이 표시된다. 5. Current Instruction: 현재 단계에 필요한 내용이 다시 표시된다. 6. Play: 미디어 플레이어가 참조 비디오를 실행한다. 7. Pause: 미디어 플레이어가 참조 비디오를 일시 중지한다. 8. Stop: 미디어 플레이어가 참조 비디오를 닫는다. 이러한 요구 사항을 기반으로, 제안하는 작업이 데이터셋 수집과 함께 시작되었다. 도 3은 본 발명의 일 실시예에 따른 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템의 구성을 나타내는 도면이다. 제안하는 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템은 데이터 셋 수집부, 오 디오 특징 추출부, 학습 및 예측부 및 명령 훈련부를 포함한다. 데이터 셋 수집부는 사용자 명령의 의미 구조를 분석하고 사용자 요청 목록을 생성하여, 생성된 목록에 기 초하여 텍스트 음성 변환 API 서비스 및 사용자에 의한 음성 명령의 오디오 데이터 셋을 수집한다. 오디오 특징 추출부는 수집된 음성 명령의 오디오 데이터 셋으로부터 오디오 특징을 추출한다. 본 발명의 실시예에 따른 오디오 특징 추출은 음성 명령의 오디오 데이터 셋으로부터 MFCC 특징을 추출하여 학습 및 예측부로 입력한다. 학습 및 예측부는 CNN 기반 네트워크를 사용하여 추출된 오디오 특징에 대하여 학습하고 음성 명령 및 언 어를 분류함으로써 요청된 음성 명령의 유형과 음성 명령이 실행된 언어를 예측한다. 학습 및 예측부는 CNN 학습부, 음성 명령 분류부 및 언어 분류부를 포함한다. CNN 학습부는 MFCC(Mel-frequency cepstral coefficients)의 오디오 특징 벡터를 입력 받아 CNN 기반 컨 볼루션, 배치 정규화, 최대 풀링 및 드롭아웃 레이어를 포함하는 CNN 기반 네트워크를 사용하여 음성 명령 및 언어를 분류하기 위한 학습을 수행한다. 음성 명령 분류부는 덴스 레이어 및 덴스 레이어 내부 유닛, 드롭아웃, 소프트맥스 활성화 함수를 포함하 고, 사용자가 요청한 음성 명령의 유형을 분류하여 각 명령 유형에 대한 표현인 최종 출력 벡터를 제공한다. 언어 분류부는 덴스 레이어 및 덴스 레이어 내부 유닛, 드롭아웃, 소프트맥스 활성화 함수를 포함하고, 덴 스 레이어의 수는 음성 명령 분류부의 덴스 레이어 보다 작고, 사용자가 요청한 음성 명령의 언어를 분류하여 각 명령 언어에 대한 표현인 최종 출력 벡터를 제공한다. 명령 훈련부는 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 따른 작업을 수행한다. 명령 훈련부는 학습 및 예측부에서 학습되고 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 대하 여 과적합을 피하기 위해 모델 가중치에 정규화를 적용하는 가중치 감소(Weight Decay)를 적용한다. 또한, 학습 속도를 설정하기 위해 코사인 어닐링 스케줄러(Cosine Annealing Scheduler)를 사용하고, 학습되고 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 대해 손실 가중치가 동일하도록 범주형 교차 엔트로피 손실을 적용 하여 모델을 훈련한다. 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템의 각 구성에 대하여 도 4 내지 도 6을 참조하여 더욱 상세히 설명한다. 데이터 셋 수집부의 작업에 대한 데이터 셋 생성 및 수집은 두 단계로 수행된다. 본 발명의 실시예에 따른 영어와 한국어로 된 사용자 명령의 의미 구조를 분석하고 잠재적인 사용자 요청 목록을 생성한다. 다음으로, 생 성된 목록을 기반으로, 사람들이 말하는 이러한 음성 명령과 텍스트 음성 변환 API 서비스에 대한 오디오 녹음 을 수집한다. 강력하고 유연한 AI 챗봇을 개발하기 위해서는 사용자 요청의 성격을 고려하는 것이 필수적이다. 따라서 각 명 령 유형에 대한 음성 명령의 변형과 의미론을 분석한다. 다양한 사용자가 동일한 기능을 다양한 조합으로 사용 한다(예를 들어, \"다음 지침\", \"다음 지침 표시\", \"다음 지침을 저에게 보여주실 수 있나요?\" 등). 또한 어플리 케이션의 최종 사용자는 어플리케이션과 음성 상호 작용을 하기 위해 영어와 한국어를 사용한다. 이러한 모든 요인과 목표를 고려하여, 명령 유형과 언어에 기반한 잠재적인 사용자의 음성 명령 목록을 생성한다. 도 4는 본 발명의 일 실시예에 따른 음성 명령의 분포를 나타내는 도면이다. 도 4를 참조하면, 생성된 음성 명령의 분포는 전체적으로 영어에는 541개의 명령이 있고 한국어에는 623개의 명 령이 있다. 차트를 보면 알 수 있듯이 명령의 분포는 영어와 한국어로 된 명령 유형별로 평균 약 145개의 명령 이 있을 정도로 균형을 이루고 있다. 다음으로, 생성된 모든 명령에 대한 오디오 파일을 녹음한다.오디오 데이터셋 수집은 생성된 음성 명령을 기반으로 한다. 예를 들어, 음성 오디오를 녹음하기 위해 기존의 TTS(Text-to-Speech) API 서비스를 사용하여 오디오 녹음을 생성하고, 음성 명령을 녹음하는 사람을 채용하는 두 가지 방법을 사용할 수 있다. 본 발명의 실시예에 따른 오디오 수집은 API 서비스는 구글 클라우드 TTS, 마이크로소프트 Azure, 네이버 클라 우드 TTS, 카카오 스피치 API를 사용하였다. 전체적으로, 60개, 및 23개의 영어와 한국어 음성을 각각 적용했다. 다음으로, 15명의 영어 명령 참가자와 7명의 한국어 명령 참가자로부터 음성 명령을 녹음하였다. 전체적으로 영어 명령의 경우 60개의 API와 15개의 실제 사람 음성이 있고, 이는 40,575(75*541)개의 녹음을 생 성하는 것이며, 한국어 명령의 경우 23개의 API와 7개의 실제 음성이 있고, 이는 18,690(30*623)개의 녹음을 생 성하는 것이다. 따라서 전체적으로 모든 유형의 명령에 대한 데이터 셋으로 59,265개의 오디오 녹음을 수집할 수 있다. 이러한 녹음의 지속 시간은 약 1 내지 4초이며, 이는 데이터 셋의 길이를 약 24.7시간으로 만들 수 있 다. API와 참가자들의 목소리는 억양이 다르고 미국, 호주, 캐나다, 인도, 아일랜드, 영국, 한국, 우즈베키스탄 등의 국가에서 유래했다. 따라서 데이터 셋은 동일한 구절에 대해 매우 다양한 음성 사례를 가지고 있으며, 이 는 음성 모델을 변화에 적합하게 만든다. 도 5는 본 발명의 일 실시예에 따른 오디오 데이터의 분포를 나타내는 도면이다. 도 5의 차트에서 볼 수 있듯이, API 오디오 데이터는 데이터 셋의 약 78.1%를 차지하며, 나머지 21.9%는 참가한 사람들의 실제 음성 명령이다. 한국어 데이터의 양은 영어보다 2배 적지만 API와 실제 음성 명령 데이터의 비율 은 여전히 비슷하다. 하지만, 음성 API를 합성한 오디오 녹음을 보유하는 데는 자체적인 문제가 있다. 텍스트 음성 변환 API를 통해 생성된 오디오 녹음은 참가자의 녹음과 다르기 때문에 해결해야 할 두 가지 주요 문제가 있다. 이를 위해 오디오 증강을 통해 그 문제를 해결할 수 있다. 먼저, API 생성 데이터는 배경 노이즈가 없다. 일반적으로 오디오가 녹음될 때 소위 백색 노이즈가 녹음에 항상 존재한다. 그러나, 합성 녹음의 경우 오디오가 너무 명확하여 API 데이터 배포가 실제 데이터와 다르게 된다. 이 문제를 해결하기 위해 API 생성 데이터에 다양한 유형의 배경 노이즈(예를 들어, 백색 노이즈, 거리 노이즈, 군중 노이즈 등)을 수동으로 추가하여 보다 사실적으로 만들었다. 다음으로, 텍스트 음성 변환 API 데이터는 정적 재생 시작점을 갖는다. 실제음성 녹음에서 사람이 말을 하기 시 작하는 실제 시간은 0.5초에서 1.5초까지 다양하다. 이것은 사람들의 오디오 녹음을 다양하게 만든다. 반대로 API 데이터의 경우 재생의 시작점은 정적이며 보통 0초부터 시작되는데, 이는 사람의 실제 음성 데이터와 비교 했을 때 비현실적이다. 이 문제를 처리하기 위해 각 API 녹화를 랜덤하게 전환하여 다양한 재생 시작 시간을 갖 도록 했다. 이러한 접근 방식은 API 오디오 녹음을 실제 녹음과 훨씬 더 견실하게 만들었다. 본 발명의 실시예 에 따른 데이터셋은 이와 같이 준비되었으며 이를 이용하여 모델을 생성하고 훈련할 수 있다. 제안하는 AI 챗봇 모델 행동 흐름은 두 단계로 구성된다. 먼저, 녹음된 음성 명령에서 오디오 특징을 추출한다. 본 발명의 실시예에 따르면, MFCC(Mel-frequency Cepstral coefficients)를 오디오 특징으로 사용한다. 다음으 로, 네트워크를 사용하여 요청된 명령 유형과 명령이 실행된 언어를 예측한다. 네트워크는 세 부분으로 나뉜다: CNN 학습부(특징 학습을 위한 컨볼루션 신경망 아키텍처), 음성 명령 분류부 및 언어 분류부(CNN 학습부에서 학 습된 특징에 따라 예측을 하는 분류 네트워크)이다. 명령 분류 결과에 따라 어플리케이션은 특정 작업을 수행한 다. 또한 음성 피드백은 언어 분류부에서 예측한 언어로 제공된다(즉, 사용자가 한국어로 음성 요청을 할 경우, 어플리케이션의 응답은 그에 따라 한국어로 이루어진다). 오디오 특징을 추출하는 방법은 여러 가지가 있지만, 본 발명의 실시예에 따른 오디오 특징 추출 방법은 MFCC(Mel-frequency Cepstral coefficients)를 오디오 특징으로 사용한다. MFCC는 사람의 성대에 의해 생성된 힘 스펙트럼을 표현하는 것이다. MFCC는 음성 콘텐츠에 대한 정보를 보존하고 배경 노이즈를 폐기한다. 따라서 MFCC는 자동 음성 인식에 널리 사용된다. 본 발명의 실시예에 따른 음성에서 MFCC를 추출하는 데 사용된 파라미 터는 다음과 같다: MFCC 특징 수: 13개 FFT(Fast Fourier Transform) 윈도우의 길이: 512 Hop 길이: 512 생성할 Mel 밴드 수: 40개 본 발명의 실시예에서 처리한 각 오디오 파일은 204x13차원으로 축소되었다. 여기서 13은 MFCC의 수이고 204는 hop 길이로 샘플링한 후의 프레임 수이다. 경험적으로, 언급된 파라미터는 본 발명의 실시예에서 가장 잘 작동 했다. 다음으로, 오디오 특징 입력 차원에 따라 네트워크를 구축한다. 도 6은 본 발명의 일 실시예에 따른 인공지능 챗봇 시스템의 CNN 기반 학습 및 예측부의 구성을 나타내는 도면 이다. 제안하는 AI 챗봇 네트워크의 CNN 기반 학습 및 예측부는 세 부분으로 구성된다. CNN 학습부는 입력으로 전달된 오디오 MFCC 특징 벡터를 취한다. CNN 학습부는 컨볼루션 (Conv2D), 배치 정규화(Batch Norm), 최대 풀링(Max Pooling) 및 드롭아웃(Dropout) 레이어의 스택으로 구성된 다. 도 6에 도시된 바와 같이, Conv2D는 필터 수(64개), 커널 크기(3개), 레이어 끝에 적용된 활성화 함수 (ReLU)인 3개의 파라미터를 가지고 있다. 최대 풀링의 경우 파라미터는 풀 크기와 스트라이드를 나타내며, 두 가지 모두에 대해 2이다. 또한 드롭아웃은 드롭아웃 비율이 0.3과 같다는 것을 보여준다. 마지막으로, 플랫 (Flatten) 레이어는 추가 처리를 위해 출력을 전달하기 위한 입력을 1차원으로 재구성한다. 전체적으로 CNN 학 습부는 오디오에 있는 특징을 입력으로 하여 음성 명령 분류부 및 언어 분류부로 전달한다. 음성 명령 분류부는 사용자가 요청한 명령의 유형 분류를 담당한다. 레이어 스택은 덴스(Dense) 및 그 안 에 있는 유닛 수(64 또는 8), 드롭아웃(Dropout)(0.5), 소프트맥스(Softmax) 활성화 함수로 구성되며, 1x8 형 태의 최종 출력 벡터를 제공하고 여기서 8은 각 명령 유형에 대한 표현이다. 언어 분류부는 입력 명령에서 언어를 예측하기 위해 CNN 학습부의 출력을 사용하는 언어 분류 레이어 가 사용되었다. 단, 음성 명령 분류부와 유사하지만 덴스(Dense) 레이어 수가 더 적다. 언어 분류부 의 최종 출력 벡터는 1x2 형태의 예측된 언어이며, 여기서 2는 각 언어(예를 들어, 영어와 한국어)의 점수 이다. CNN 학습부, 음성 명령 분류부 및 언어 분류부를 모두 결합하면 각각 181,066개의 훈련 가능한 파라미터와 384개의 훈련 불가능한 파라미터로 구성된 제안하는 모델의 구조가 생성된다. 생성된 모델을 이용하여 훈련하려면 데이터 셋과 전체 작업을 기반으로 한 방법을 적절히 선택해야 한다. 본 발 명의 실시예에 따른 작업에서는 영어와 한국어로 된 데이터를 다양한 음성 명령으로 사용한다. 이와 함께 다양 한 배경 노이즈와 독특한 특성을 가진 새로운 사람의 목소리를 가진 실제 환경에서 모델이 작동하길 바란다. 따 라서, 가장 본질적인 목표는 과적합을 피하는 것이다. 이 작업을 처리하기 위해 모델 가중치에 정규화를 적용하 는 가중치 감소(Weight Decay)를 적용했다. 이 접근 방식은 모델의 일반화를 개선하고 데이터셋에 없는 데이터 에서 잘 수행되도록 한다. 다음으로, 적절한 학습 속도를 설정하기 위해 코사인 어닐링 스케줄러(Cosine Annealing Scheduler)를 사용하는 것이 모델이 수렴하는 데 많은 도움이 된다는 것을 발견했다. 코사인 어닐링 스케줄러의 일반적인 작동 원리는 학습 속도를 최소값으로 비교적 빠르게 감소시킨 후 다시 빠르게 증가시키는 큰 값에서 시작하는 것이다. 매번 재시작할 때마다 스케줄러는 훈련에 가장 기여하는 가중치부터 시작하려고 노력하며, 따라서 이 기술을 \"warm restart\"라고 부르기도 한다. 손실 및 최적화 도구와 같은 파라미터에 있어서, 각 명령 및 언어 분류 부분에 대해 손실 가중치가 동일하도록 범주형 교차 엔트로피 손실을 적용했다. 아담(Adam) 최적화기는 다음과 같은 하이퍼 파라미터로 적용되었다: beta_1=0.9, beta_2=0.999, epsilon=1e-07. 도 7은 본 발명의 일 실시예에 따른 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템의 동작 방법을 설명하기 위한 흐름도이다. 제안하는 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템의 동작 방법은 데이터 셋 수집부를 통해 사용자 명령의 의미 구조를 분석하고 사용자 요청 목록을 생성하여, 생성된 목록에 기초하여 텍스트 음성변환 API 서비스 및 사용자에 의한 음성 명령의 오디오 데이터 셋을 수집하는 단계, 수집된 음성 명령의 오디오 데이터 셋으로부터 오디오 특징 추출부를 통해 오디오 특징을 추출하는 단계, 학습 및 예측부를 통 해 CNN 기반 네트워크를 사용하여 추출된 오디오 특징에 대하여 학습하고 음성 명령 및 언어를 분류함으로써 요 청된 음성 명령의 유형과 음성 명령이 실행된 언어를 예측하는 단계 및 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 따라 명령 훈련부를 통해 작업을 수행하는 단계를 포함한다. 단계에서, 데이터 셋 수집부를 통해 사용자 명령의 의미 구조를 분석하고 사용자 요청 목록을 생성하여, 생성된 목록에 기초하여 텍스트 음성 변환 API 서비스 및 사용자에 의한 음성 명령의 오디오 데이터 셋을 수집 한다. 단계에서, 수집된 음성 명령의 오디오 데이터 셋으로부터 오디오 특징 추출부를 통해 오디오 특징을 추출 한다. 본 발명의 실시예에 따른 오디오 특징 추출은 음성 명령의 오디오 데이터 셋으로부터 MFCC 특징을 추출하 여 학습 및 예측부로 입력한다. 단계에서, 학습 및 예측부를 통해 CNN 기반 네트워크를 사용하여 추출된 오디오 특징에 대하여 학습하고 음성 명령 및 언어를 분류함으로써 요청된 음성 명령의 유형과 음성 명령이 실행된 언어를 예측한다. 학습 및 예측부는 CNN 학습부, 음성 명령 분류부 및 언어 분류부를 포함한다. 단계는 학습 및 예측부의 CNN 학습부에서 MFCC(Mel-frequency cepstral coefficients)의 오디오 특징 벡 터를 입력 받아 CNN 기반 컨볼루션, 배치 정규화, 최대 풀링 및 드롭아웃 레이어를 포함하는 CNN 기반 네트워크 를 사용하여 음성 명령 및 언어를 분류하기 위한 학습을 수행하는 단계, 덴스 레이어 및 덴스 레이어 내부 유닛, 드롭아웃, 소프트맥스 활성화 함수를 포함하는 학습 및 예측부의 음성 명령 분류부에서, 사용자가 요청한 음성 명령의 유형을 분류하여 각 명령 유형에 대한 표현인 최종 출력 벡터를 제공하는 단계 및 덴스 레이어(덴 스 레이어의 수는 음성 명령 분류부의 덴스 레이어 보다 작음) 및 덴스 레이어 내부 유닛, 드롭아웃, 소프트맥 스 활성화 함수를 포함하는 학습 및 예측부의 언어 분류부에서 사용자가 요청한 음성 명령의 언어를 분류하여 각 명령 언어에 대한 표현인 최종 출력 벡터를 제공하는 단계를 포함한다. 단계에서, 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 따라 명령 훈련부를 통해 작업을 수행 한다. 명령 훈련부는 학습 및 예측부에서 학습되고 예측된 음성 명령의 유형 및 음성 명령이 실행된 언어에 대하여 과 적합을 피하기 위해 모델 가중치에 정규화를 적용하는 가중치 감소(Weight Decay)를 적용한다. 또한, 학습 속도 를 설정하기 위해 코사인 어닐링 스케줄러(Cosine Annealing Scheduler)를 사용하고, 학습되고 예측된 음성 명 령의 유형 및 음성 명령이 실행된 언어에 대해 손실 가중치가 동일하도록 범주형 교차 엔트로피 손실을 적용하 여 모델을 훈련한다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 예를 들어, 프로 세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴 퓨터, FPA(field programmable array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상 의 소프트웨어 어플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로"}
{"patent_id": "10-2021-0026447", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "설명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치 는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치에 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이 터는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2021-0026447", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형 태로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성 될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7"}
{"patent_id": "10-2021-0026447", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 AI 챗봇 상호 작용 흐름을 설명하기 위한 도면이다. 도 2는 본 발명의 일 실시예에 따른 혼합 현실 콘텐츠 스냅 샷 및 AI 챗봇 참조 기능을 나타내는 도면이다. 도 3은 본 발명의 일 실시예에 따른 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템의 구성을 나타내는 도면이다. 도 4는 본 발명의 일 실시예에 따른 음성 명령의 분포를 나타내는 도면이다. 도 5는 본 발명의 일 실시예에 따른 오디오 데이터의 분포를 나타내는 도면이다. 도 6은 본 발명의 일 실시예에 따른 인공지능 챗봇 시스템의 CNN 기반 학습 및 예측부의 구성을 나타내는 도면 이다. 도 7은 본 발명의 일 실시예에 따른 튜터링을 위해 혼합현실 컨텐츠를 제공하는 인공지능 챗봇 시스템의 동작 방법을 설명하기 위한 흐름도이다."}
