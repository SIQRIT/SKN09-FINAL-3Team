{"patent_id": "10-2022-0105734", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0027470", "출원번호": "10-2022-0105734", "발명의 명칭": "인공 신경망 모델의 동작 방법 및 장치", "출원인": "삼성전자주식회사", "발명자": "김남수"}}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공 신경망 모델의 동작 방법에 있어서,소스 데이터 및 타겟 데이터를 수신하여 제1 잠재 타겟(latent target) 데이터를 예측하는 단계;상기 소스 데이터 및 상수 데이터를 수신하여 제2 잠재 타겟 데이터를 예측하는 단계; 및상기 제1 잠재 타겟 데이터 및 상기 제2 잠재 타겟 데이터에 기초하여 상기 인공 신경망 모델을 학습하는 단계를 포함하고,상기 제1 잠재 타겟 데이터와 상기 타겟 데이터는 다대일(many-to-one) 관계를 갖는 인공 신경망 모델의 동작방법."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 인공 신경망 모델을 학습하는 단계는상기 제1 잠재 타겟 데이터와 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여, 상기 인공 신경망 모델을 학습하는 단계를 포함하는, 인공 신경망 모델의 동작 방법."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제1 잠재 타겟 데이터를 예측하는 단계는CTC(Connectionist Temporal Classification) 알고리즘에 기초하여, 상기 제1 잠재 타겟 데이터를 예측하는 단계를 포함하는, 인공 신경망 모델의 동작 방법."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 제1 잠재 타겟 데이터를 예측하는 단계는상기 타겟 데이터의 일부분을 마스킹하는 단계; 및상기 소스 데이터 및 상기 마스킹된 타겟 데이터를 수신하여, 상기 제1 잠재 타겟 데이터를 예측하는 단계를 포함하는, 인공 신경망 모델의 동작 방법."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,공개특허 10-2024-0027470-2-상기 제1 잠재 타겟 데이터를 예측하는 단계는크로스 엔트로피(cross entropy)를 손실 함수로 이용하여, 상기 제1 잠재 타겟 데이터를 예측하는 단계를 포함하는, 인공 신경망 모델의 동작 방법."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 인공 신경망 모델을 학습하는 단계상기 타겟 데이터 및 상기 제1 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제1 손실 함수, 상기 타겟 데이터 및 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제2 손실 함수 및 상기 제1 잠재 타겟 데이터 및 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제3 손실 함수 중 적어도 하나에 기초하여 상기 인공 신경망 모델을 학습하는 단계를 포함하는, 인공 신경망 모델의 동작 방법."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 인공 신경망 모델을 학습하는 단계상기 제1 손실 함수, 상기 제2 손실 함수 및 상기 제3 손실 함수 중 적어도 하나에 기초하여 결정된 최종 손실함수가 최소가 되도록 상기 인공 신경망 모델을 학습하는 단계를 포함하는, 인공 신경망 모델의 동작 방법."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 소스 데이터를 수신하여, 상기 소스 데이터에 대응하는 소스 벡터를 출력하는 단계; 및상기 타겟 데이터를 수신하여, 상기 타겟 데이터에 대응하는 타겟 벡터를 출력하는 단계를 더 포함하고,상기 제1 잠재 타겟 데이터를 예측하는 단계는상기 소스 벡터 및 상기 타겟 벡터를 수신하여 상기 제1 잠재 타겟 데이터를 예측하는 단계를 포함하고,상기 제2 잠재 타겟 데이터를 예측하는 단계는상기 소스 벡터를 수신하여 상기 제2 잠재 타겟 데이터를 예측하는 단계를 포함하는, 인공 신경망 모델의 동작 방법."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 소스 데이터 및 상기 타겟 데이터는공개특허 10-2024-0027470-3-시계열 데이터를 포함하는, 인공 신경망 모델의 동작 방법."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "하드웨어와 결합되어 제1항 내지 제9항 중 어느 하나의 항의 방법을 실행시키기 위하여 매체에 저장된 컴퓨터프로그램."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "적어도 하나의 명령어를 저장하는 메모리; 및상기 메모리에 저장된 명령어를 실행함으로써,소스 데이터 및 타겟 데이터를 수신하여 제1 잠재 타겟(latent target) 데이터를 예측하고, 상기 소스 데이터 및 상수 데이터를 수신하여 제2 잠재 타겟 데이터를 예측하고,상기 제1 잠재 타겟 데이터 및 상기 제2 잠재 타겟 데이터에 기초하여 인공 신경망 모델을 학습하는 프로세서를포함하고,상기 제1 잠재 타겟 데이터와 상기 타겟 데이터는 다대일(many-to-one) 관계를 갖는 전자 장치."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 프로세서는상기 제1 잠재 타겟 데이터와 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여, 상기 인공 신경망 모델을 학습하는, 전자 장치."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 프로세서는CTC(Connectionist Temporal Classification) 알고리즘에 기초하여, 상기 제1 잠재 타겟 데이터를 예측하는,전자 장치."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 프로세서는상기 타겟 데이터의 일부분을 마스킹하고,상기 소스 데이터 및 상기 마스킹된 타겟 데이터를 수신하여, 상기 제1 잠재 타겟 데이터를 예측하는, 전자 장치."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,공개특허 10-2024-0027470-4-상기 프로세서는크로스 엔트로피(cross entropy)를 손실 함수로 이용하여, 상기 제1 잠재 타겟 데이터를 예측하는, 전자 장치."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항에 있어서,상기 프로세서는상기 타겟 데이터 및 상기 제1 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제1 손실 함수, 상기 타겟 데이터 및 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제2 손실 함수 및 상기 제1 잠재 타겟 데이터 및 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제3 손실 함수 중 적어도 하나에 기초하여 상기 인공 신경망 모델을 학습하는, 전자 장치."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 프로세서는상기 제1 손실 함수, 상기 제2 손실 함수 및 상기 제3 손실 함수 중 적어도 하나에 기초하여 결정된 최종 손실함수가 최소가 되도록 상기 인공 신경망 모델을 학습하는, 전자 장치."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항에 있어서,상기 프로세서는상기 소스 데이터를 수신하여, 상기 소스 데이터에 대응하는 소스 벡터를 출력하고,상기 타겟 데이터를 수신하여, 상기 타겟 데이터에 대응하는 타겟 벡터를 출력하고,상기 소스 벡터 및 상기 타겟 벡터를 수신하여 상기 제1 잠재 타겟 데이터를 예측하고,상기 소스 벡터를 수신하여 상기 제2 잠재 타겟 데이터를 예측하는, 전자 장치."}
{"patent_id": "10-2022-0105734", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항에 있어서,상기 소스 데이터 및 상기 타겟 데이터는시계열 데이터를 포함하는, 전자 장치."}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공 신경망 모델의 동작 방법 및 장치가 개시된다. 일 실시예에 따른 인공 신경망 모델의 동작 방법은 소스 데 이터 및 타겟 데이터를 수신하여 제1 잠재 타겟(latent target) 데이터를 예측하는 단계; 소스 데이터 및 상수 데이터를 수신하여 제2 잠재 타겟 데이터를 예측하는 단계; 및 제1 잠재 타겟 데이터 및 제2 잠재 타겟 데이터에 기초하여 인공 신경망 모델을 학습하는 단계를 포함하고, 제1 잠재 타겟 데이터와 타겟 데이터는 다대일(many- to-one) 관계를 갖을 수 있다."}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "아래 실시예들은 인공 신경망 모델의 동작 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간이 지니고 있는 효율적인 패턴 인식 방법을 실제 컴퓨터에 적용시키려는 연구 중 하나로, 인간의 생물학적 신경 세포의 특성을 수학적 표현에 의해 모델링한 인공 뉴럴 네트워크(artificial neural network)에 대한 연구 가 있다. 입력 패턴을 특정 그룹으로 분류하는 문제를 해결하기 위해, 인공 뉴럴 네트워크는 인간이 가지고 있 는 학습이라는 능력을 모방한 알고리즘을 이용한다. 이 알고리즘을 통하여 인공 뉴럴 네트워크는 입력 패턴과 출력 패턴들 간의 사상(mapping)을 생성해낼 수 있고 학습에 이용되지 않았던 입력 패턴에 대하여도 비교적 올 바른 출력을 생성할 수 있는 일반화 능력을 가지고 있다."}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "위에서 설명한 배경기술은 발명자가 본원의 개시 내용을 도출하는 과정에서 보유하거나 습득한 것으로서, 반드 시 본 출원 전에 일반 공중에 공개된 공지기술이라고 할 수는 없다."}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 인공 신경망 모델의 동작 방법은 소스 데이터 및 타겟 데이터를 수신하여 제1 잠재 타겟 (latent target) 데이터를 예측하는 단계; 상기 소스 데이터 및 상수 데이터를 수신하여 제2 잠재 타겟 데이터 를 예측하는 단계; 및 상기 제1 잠재 타겟 데이터 및 상기 제2 잠재 타겟 데이터에 기초하여 상기 인공 신경망 모델을 학습하는 단계를 포함하고, 상기 제1 잠재 타겟 데이터와 상기 타겟 데이터는 다대일(many-to-one) 관계 를 갖을 수 있다. 상기 인공 신경망 모델을 학습하는 단계는 상기 제1 잠재 타겟 데이터와 상기 제2 잠재 타겟 데이터 사이의 차 이에 기초하여, 상기 인공 신경망 모델을 학습하는 단계를 포함할 수 있다. 상기 제1 잠재 타겟 데이터를 예측하는 단계는 CTC(Connectionist Temporal Classification) 알고리즘에 기초 하여, 상기 제1 잠재 타겟 데이터를 예측하는 단계를 포함할 수 있다. 상기 제1 잠재 타겟 데이터를 예측하는 단계는 상기 타겟 데이터의 일부분을 마스킹하는 단계; 및 상기 소스 데 이터 및 상기 마스킹된 타겟 데이터를 수신하여, 상기 제1 잠재 타겟 데이터를 예측하는 단계를 포함할 수 있다. 상기 제1 잠재 타겟 데이터를 예측하는 단계는 크로스 엔트로피(cross entropy)를 손실 함수로 이용하여, 상기 제1 잠재 타겟 데이터를 예측하는 단계를 포함할 수 있다. 상기 인공 신경망 모델을 학습하는 단계 상기 타겟 데이터 및 상기 제1 잠재 타겟 데이터 사이의 차이에 기초하 여 결정된 제1 손실 함수, 상기 타겟 데이터 및 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제2 손실 함수 및 상기 제1 잠재 타겟 데이터 및 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제3 손 실 함수 중 적어도 하나에 기초하여 상기 인공 신경망 모델을 학습하는 단계를 포함할 수 있다. 상기 인공 신경망 모델을 학습하는 단계는 상기 제1 손실 함수, 상기 제2 손실 함수 및 상기 제3 손실 함수 중 적어도 하나에 기초하여 결정된 최종 손실 함수가 최소가 되도록 상기 인공 신경망 모델을 학습하는 단계를 포 함할 수 있다. 일 실시예에 따른 인공 신경망 동작 방법은 상기 소스 데이터를 수신하여, 상기 소스 데이터에 대응하는 소스 벡터를 출력하는 단계; 및 상기 타겟 데이터를 수신하여, 상기 타겟 데이터에 대응하는 타겟 벡터를 출력하는 단계를 더 포함하고, 상기 제1 잠재 타겟 데이터를 예측하는 단계는 상기 소스 벡터 및 상기 타겟 벡터를 수신 하여 상기 제1 잠재 타겟 데이터를 예측하는 단계를 포함하고, 상기 제2 잠재 타겟 데이터를 예측하는 단계는 상기 소스 벡터를 수신하여 상기 제2 타겟 데이터를 예측하는 단계를 포함할 수 있다. 상기 소스 데이터 및 상기 타겟 데이터는 시계열 데이터를 포함할 수 있다. 일 실시예에 따른 전자 장치는 적어도 하나의 명령어를 저장하는 메모리; 및 상기 메모리에 저장된 명령어를 실 행함으로써, 소스 데이터 및 타겟 데이터를 수신하여 제1 잠재 타겟(latent target) 데이터를 예측하고, 상기 소스 데이터 및 상수 데이터를 수신하여 제2 잠재 타겟 데이터를 예측하고, 상기 제1 잠재 타겟 데이터 및 상기제2 잠재 타겟 데이터에 기초하여 인공 신경망 모델을 학습하는 프로세서를 포함하고, 상기 제1 잠재 타겟 데이 터와 상기 타겟 데이터는 다대일(many-to-one) 관계를 갖을 수 있다. 상기 프로세서는 상기 제1 잠재 타겟 데이터와 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여, 상기 인공 신경망 모델을 학습할 수 있다. 상기 프로세서는 CTC(Connectionist Temporal Classification) 알고리즘에 기초하여, 상기 제1 잠재 타겟 데이 터를 예측할 수 있다. 상기 프로세서는 상기 타겟 데이터의 일부분을 마스킹하고, 상기 소스 데이터 및 상기 마스킹된 타겟 데이터를 수신하여, 상기 제1 잠재 타겟 데이터를 예측할 수 있다. 상기 프로세서는 크로스 엔트로피(cross entropy)를 손실 함수로 이용하여, 상기 제1 잠재 타겟 데이터를 예측 할 수 있다. 상기 프로세서는 상기 타겟 데이터 및 상기 제1 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제1 손실 함 수, 상기 타겟 데이터 및 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제2 손실 함수 및 상기 제 1 잠재 타겟 데이터 및 상기 제2 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제3 손실 함수 중 적어도 하 나에 기초하여 상기 인공 신경망 모델을 학습할 수 있다. 상기 프로세서는 상기 제1 손실 함수, 상기 제2 손실 함수 및 상기 제3 손실 함수 중 적어도 하나에 기초하여 결정된 최종 손실 함수가 최소가 되도록 상기 인공 신경망 모델을 학습할 수 있다. 상기 프로세서는 상기 소스 데이터를 수신하여, 상기 소스 데이터에 대응하는 소스 벡터를 출력하고, 상기 타겟 데이터를 수신하여, 상기 타겟 데이터에 대응하는 타겟 벡터를 출력하고, 상기 소스 벡터 및 상기 타겟 벡터를 수신하여 상기 제1 잠재 타겟 데이터를 예측하고, 상기 소스 벡터를 수신하여 상기 제2 타겟 데이터를 예측할 수 있다. 상기 소스 데이터 및 상기 타겟 데이터는 시계열 데이터를 포함할 수 있다."}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서 개시되어 있는 특정한 구조적 또는 기능적 설명들은 단지 기술적 개념에 따른 실시예들을 설명하 기 위한 목적으로 예시된 것으로서, 실제로 구현된 형태는 다양한 다른 모습을 가질 수 있으며 본 명세서에 설 명된 실시예로만 한정되지 않는다. 제1 또는 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 이런 용어들은 하나의 구성요소 를 다른 구성요소로부터 구별하는 목적으로만 이해되어야 한다. 예를 들어 제1 구성요소는 제2 구성요소로 명명 될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관계를 설명하는 표현들, 예를 들어 \"~간의\"와 \"바로~간의\" 또는 \"~에 이웃하는\"과 \"~에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 실시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함 을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 해당 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적으로 사용되 는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미를 갖는 것으로 해석되어야 하며, 본 명세서에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 실시예들은 퍼스널 컴퓨터, 랩톱 컴퓨터, 태블릿 컴퓨터, 스마트 폰, 텔레비전, 스마트 가전 기기, 지능형 자동 차, 키오스크, 웨어러블 장치 등 다양한 형태의 제품으로 구현될 수 있다. 이하, 실시예들을 첨부된 도면을 참 조하여 상세하게 설명한다. 각 도면에 제시된 동일한 참조 부호는 동일한 부재를 나타낸다. 도 1은 인공 신경망을 이용한 딥러닝 연산 방법을 설명하기 위한 도면이다. 딥러닝(Deep Learning) 등을 포함하는 인공지능(AI) 알고리즘은 인공 신경망(Artificial Neural Network, AN N)에 입력 데이터를 입력시키고, 컨볼루션 등의 연산을 통해 출력 데이터를 학습하고, 학습된 인공 신경망을 이 용하여 특징을 추출할 수 있다. 인공 신경망은 생물학적 뇌를 모델링한 컴퓨터 과학적 아키텍쳐(Computational Architecture)를 의미할 수 있다. 인공 신경망 내에서, 뇌의 뉴런들에 해당되는 노드들은 서로 연결되어 있고, 입력 데이터를 처리하기 위하여 집합적으로 동작한다. 다양한 종류의 뉴럴 네트워크들을 예로 들면, 컨볼루션 뉴럴 네트워크(Convolutional Neural Network, CNN), 회귀 뉴럴 네트워크(Recurrent Neural Network, RNN), 딥 빌리프 네트워크(Deep Belief Network, DBN), 제한된 볼츠만 기계(Restricted Boltzman Machine, RBM) 방식 등 이 있으나, 이에 제한되지 않는다. 피드-포워드(feed-forward) 뉴럴 네트워크에서, 뉴럴 네트워크의 뉴런들은 다른 뉴런들과의 연결들(links)을 갖는다. 이와 같은 연결들은 뉴럴 네트워크를 통해, 한 방향으로, 예를 들어 순방향(forward direction)으로 확장될 수 있다. 도 1은 입력 데이터를 입력 받아 출력 데이터를 출력하는 인공 신경망(예를 들어, 컨볼루션 뉴럴 네트워크 (Convolution Neural Network, CNN))의 구조를 도시한다. 인공 신경망은 2개 이상의 레이어(layer)를 보유한 딥 뉴럴 네트워크(deep neural network)일 수 있다. 도 2는 종래의 지식 증류(knowledge distillation) 방법을 설명하기 위한 도면이다. 종래의 시퀀스(sequence) 모델들은 대부분 소스(source) 데이터를 입력(input)으로 받아 타겟(target) 데이터를 예측하는 지도 학습(supervised learning) 방식으로 학습을 수행하였다. 모델의 성능을 높이는 데에는 많은 방 법들이 있으며, 그 중에는 지식 증류 방법이 있다. 지식 증류 방법은 성능이 우수한 교사(teacher) 모델(예를 들어, 도 2의 model Φ)이 비교적 성능이 낮은 학생(student) 모델(예를 들어, 도 2의 model θ)로 소프트맥스 (softmax) 값, 히드 리프리젠테이션(hidden representation) 등과 같은 정보를 전달해주어 학생 모델의 학습에 활용하는 방법일 수 있다. 이 때, 교사 모델(예를 들어, 도 2의 model Φ)과 학생(student) 모델(예를 들어, 도 2의 model θ) 모두 지도 학습에 기초하여 학습될 수 있다. 예를 들어, 교사 모델(예를 들어, 도 2의 model Φ)의 경우, 소스 데이터(예 를 들어, 도 2의 x)를 수신하여 예측 데이터(예를 들어, 도 2의 값)를 출력하고, 예측 데이터와 정답 타겟 데이터(예를 들어, 도 2의 y) 사이의 차이가 최소가 되도록 교사 모델(예를 들어, 도 2의 model Φ)이 학 습될 수 있다. 마찬 가지로, 예를 들어, 학습 모델(예를 들어, 도 2의 model θ)의 경우, 소스 데이터(예를 들어, 도 2의 x)를 수신하여 예측 데이터(예를 들어, 도 2의 값)를 출력하고, 예측 데이터와 정답 타겟 데이터(예를 들어, 도 2의 y) 사이의 차이가 최소가 되도록 학습 모델(예를 들어, 도 2의 model θ)이 학습될 수 있다. 이 때, 학습 모델(예를 들어, 도 2의 model θ)은 교사 모델(예를 들어, 도 2의 model Φ)로부터 소프트맥스 (softmax) 값, 히드 리프리젠테이션(hidden representation) 등과 같은 정보를 수신하여 학습에 활용할 수 있다. 위 방법으로 학습된 학습 모델(예를 들어, 도 2의 model θ)을 사용하여, 추론(inference) 동작을 수행 할 수 있다. 다만, 종래 지식 증류 방법에 따른 교사 모델의 경우, 많은 연산 리소스(computational resource)(예를 들어, 파라미터, 학습 시간(training time) 등)가 요구될 수 있다. 아래에서 상세히 설명하겠으나, 일 실시예에 따른 지식 증류 방법에 따르면 타겟 정보를 시퀀스 모델 학습에 활 용한다는 특징이 있기 때문에 비교적 적은 연산 리소스를 사용하여 우수한 교사 모델을 생성할 수 있다. 나아 가, 종래의 지식 증류 방법과는 다르게, 일 실시예에 따른 지식 증류 방법에 따르면 교사 모델과 학생 모델이 동일한 파라미터를 공유할 수 있다. 도 3은 일 실시예에 따른 전자 장치의 블록도를 도시한 도면이다. 도 3을 참조하면, 일 실시예에 따른 전자 장치는 타겟 정보 학습 모듈, 소스 정보 학습 모듈 및 예측 모듈을 포함할 수 있다. 일 실시예에 따른 타겟 정보 학습 모듈은 타겟 데이터를 입력으로 수신하여 타겟 데이터에 대응하는 벡터 데이터(이하, 타겟 벡터 데이터로 지칭)를 출력할 수 있다. 일 실시예에 따른 소스 정보 학습 모듈은 소스 데이터를 입력으로 수신하여 소스 데이터에 대응하는 벡터 데이터(이하, 소스 벡터 데이터로 지칭)를 출력할 수 있다. 일 실시예에 따른 소스 정보 학습 모듈은 기 존 태스크(task)(예를 들어, 음성인식, 번역 등)을 처리하는 시퀀스 모델과 같은 모델 구조일 수 있다. 예를 들어, 음성인식 태스크의 경우, 소스 정보 학습 모듈은 음성 인식 모델과 같은 구조일 수 있다. 일 실시예에 따른 예측 모듈은 타겟 정보 학습 모듈의 출력인 타겟 벡터 데이터와 소스 정보 학습 모 듈의 출력인 소스 벡터 데이터 중 적어도 하나를 수신하여, 잠재 타겟(latent target) 데이터를 출력할 수 있다. 일 실시예에 따른 잠재 타겟 데이터를 이용하여, 예측 모듈을 학습할 수 있다. 일 실시예에 따른 전자 장치의 구체적인 동작 방법은 아래에서 도 4 내지 도 7을 참조하여 설명된다. 도 4는 일 실시예에 따른 지식 증류 방법을 설명하기 위한 도면이다. 도 4를 참조하면, 일 실시예에 따른 지식 증류 방법에 따르면 교사 모델과 학생 모델이 동일한 파라미터를 공유 할 수 있다. 일 실시예에 따른 지식 증류 방법은 하나의 모델(예를 들어, 도 4의 model θ)을 교사 모델 및 학 습 모델로 사용할 수 있다. 다시 말해, 일 실시예에 따른 지식 증류 방법을 수행하는 데에 있어 소스 정보 학 습 모듈, 타겟 정보 학습 모듈 및 예측 모듈은 1개씩만 존재하면 되며, 교사 모델과 학생 모델 은 이 파라미터들을 모두 공유할 수 있다. 일 실시예에 따른 교사 모델의 경우 타겟 데이터를 활용하여 더 정확한 예측(prediction)을 할 수 있으며, 이에 대한 정보를 학생 모델에게 전달해줄 수 있다. 일 실시예에 따른 교사 모델로 동작할 경우, 일 실시예에 따른 타겟 정보 학습 모듈 및 소스 정보 학습 모 듈은 각각 입력으로 타겟 데이터(예를 들어, y) 및 소스 데이터(예를 들어, x)를 수신하여, 각각에 대응하 는 타겟 벡터 및 소스 벡터를 출력할 수 있다. 일 실시예에 따른 예측 모듈은 타겟 벡터 및 소스 벡터를 수신하여, 제1 잠재 타겟 데이터(예를 들어, 도 4의 )를 예측할 수 있다. 아래에서, 일 실시예에 따 른 제1 잠재 타겟 데이터는 로 지칭될 수 있다. 일 실시예에 따른 학생 모델로 동작할 경우, 일 실시예에 따른 소스 정보 학습 모듈은 입력으로 소스 데이 터(예를 들어, x)를 수신할 수 있다. 다만, 일 실시예에 따른 학생 모델로 동작할 경우, 실제 추론 동작을 수 행 시 타겟 데이터가 없기 때문에, 일 실시예에 따른 학생 모델을 학습할 때는 타겟 데이터를 사용하지 않을 수 있다. 이에, 일 실시예에 따른 타겟 정보 학습 모듈은 타겟 데이터 대신에 상수(constant)(예를 들어,zero array)를 수신할 수 있다. 일 실시예에 따른 예측 모듈은 소스 정보 학습 모듈 및 타겟 정보 학습 모듈의 출력을 수신하여, 제2 잠재 타겟 데이터(예를 들어, 도 4의 )를 예측할 수 있다. 아 래에서, 일 실시예에 따른 제2 잠재 타겟 데이터는 로 지칭될 수 있다. 일 실시예에 따른 지식 증류 방법에 따르면, 제1 잠재 타겟 데이터 와 제2 잠재 타겟 데이터 사이의 차이 (또는, 거리)를 줄여주는 방식으로 학습이 진행될 수 있다. 다만, 일 실시예에 따른 교사 모델의 경우 단순히 타겟 데이터(예를 들어, y)를 예측 모듈의 입력으로 사 용할 경우 타겟 데이터(예를 들어, y)를 그대로 출력 데이터로 출력하는 자명해(trivial solution) 문제가 발생 할 수 있다. 이를 위해, 일 실시예에 따른 지식 증류 모델은 제1 잠재 타겟 데이터와 타겟 데이터가 다대일 (many-to-one) 관계를 갖도록 설정할 수 있다. 아래에서, 일 실시예에 따른 지식 증류 모델은 인공 신경망 모 델로 지칭될 수 있다. 다시 말해, 일 실시예에 따른 지식 증류 모델은 일(One)에 해당되는 y를 기반으로 다(many)에 해당되는 z를 예 측하는 방법을 사용하여, 자명해(trivial solution) 문제를 해결할 수 있다. 아래에서, 도 5a 내지 도b를 참조 하여 일 실시예에 따른 제1 잠재 타겟 데이터와 타겟 데이터가 다대일(many-to-one) 관계를 갖도록 설정하는 방 법이 상세히 설명된다. 도 5a는 일 실시예에 따른 CTC(Connectionist Temporal Classification) 알고리즘에 기초하여, 제1 잠재 타겟 데이터를 예측하는 방법을 설명하기 위한 도면이다. 도 5a를 참조하면, 일 실시예에 따른 지식 증류 모델은 목적 함수로 CTC를 사용할 수 있다. 일 실시예에 따른 지식 증류 모델은 CTC를 이용하여, 다대일 매핑(many-to-one mapping)을 수행할 수 있다. 보다 구체적으로, 일 실시예에 따른 지식 증류 모델은 CTC를 이용하여, 제1 잠재 타겟 데이터(z)와 타겟 데이터(y)가 다대일(many- to-one) 관계를 갖도록 설정할 수 있다. 이는 수학식 1과 같이 표현될 수 있다. 수학식 1"}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서, f()는 CTC 목적함수를 의미할 수 있다. 도 5b는 일 실시예에 따른 크로스 엔트로피(cross entropy)를 손실 함수로 이용하여, 제1 잠재 타겟 데이터를 예측하는 방법을 설명하기 위한 도면이다. 도 5b를 참조하면, 일 실시예에 따른 지식 증류 모델은 크로스 엔트로피를 손실 함수로 이용할 수 있다. 다만, 크로스 엔트로피의 경우, CTC와 달리 다대일 매핑이 정의되지 않을 수 있다. 이에, 일 실시예에 따른 지식 증 류 모델은 마스킹(masking)을 통해 일대일 관계를 다대일 관계로 변환할 수 있다. 보다 구체적으로, 일 실시예에 따른 지식 증류 모델은 타겟 데이터의 일부분을 마스킹하고, 마스킹된 타겟 데이 터를 기초로 제1 잠재 타겟 데이터를 예측할 수 있다. 예를 들어, 일 실시예에 따른 예측 모듈에 y1, y2, y3, y4에서 y2, y3가 마스킹된 타겟 데이터( )를 수신하여, 제1 잠재 타겟 데이터(z)를 예측할 수 있다. 이는 수학식 2와 같이 표현될 수 있다. 수학식 2 여기서, 마스킹 되기 전 타겟 데이터(y)(예를 들어, y1, y2, y3, y4)는 제1 잠재 타겟 데이터(z)와 동일할 수 있 다. 다시 말해, 일 실시예에 따른 제1 잠재 타겟 데이터(z)를 예측하는 것은 마스킹된 타겟 데이터( )를 이용 하여 마스킹 되기 전 타겟 데이터(y)를 예측하는 것과 동일한 의미를 갖을 수 있다. 타겟 데이터(y)를 마스킹 하여 제1 잠재 타겟 데이터(z)와 마스킹된 타겟 데이터( )를 다대일 관계로 설정할 수 있다. 이를 통해, 지식 증류 모델은 일대일 관계를 다대일 관계로 변환하여 자명해 문제를 해결할 수 있다. 다시 도 4를 참조하면, 일 실시예에 따른 CTC를 사용한 지식 증류 모델의 손실 함수는 아래 수학식 3과 같이 표 현할 수 있다. 수학식 3"}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "일 실시예에 따른 CTC를 사용한 지식 증류 모델은 수학식 3의 손실 함수가 최소가 되도록 학습될 수 있다. 수 학식 2에서, 는 학생 모델로 동작할 때, 타겟 데이터 및 제2 잠재 타겟 데이터 사이의 차 이에 기초하여 결정된 제2 손실 함수일 수 있다. 다시 말해, 일 실시예에 따른 제2 잠재 타겟 데이터와 타겟 데이터 사이의 차이가 최소가 되는 방향으로 예측 모듈이 학습될 수 있다. 수학식 3에서, 는 교사 모델로 동작할 때, 타겟 데이터 및 제1 잠재 타겟 데이터 사이 의 차이에 기초하여 결정된 제1 손실 함수일 수 있다. 다시 말해, 일 실시예에 따른 제1 잠재 타겟 데이터와 타겟 데이터 사이의 차이가 최소가 되는 방향으로 예측 모듈이 학습될 수 있다. 수학식 3에서, 는 제1 잠재 타겟 데이터와 제2 잠재 타겟 데이터 사이의 차이에 기초하여 결 정된 제3 손실 함수일 수 있다. 다시 말해, 일 실시예에 따른 제1 잠재 타겟 데이터와 제2 잠재 타겟 데이터 사이의 차이가 최소가 되는 방향으로 예측 모듈이 학습될 수 있다. 일 실시예에 따른 크로스 엔트로피를 사용한 지식 증류 모델의 손실 함수는 아래 수학식 4와 같이 표현할 수 있 다. 수학식 4"}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "일 실시예에 따른 크로스 엔트로피를 사용한 지식 증류 모델은 수학식 3의 손실 함수가 최소가 되도록 학습될 수 있다. 수학식 4에서, 는 학생 모델로 동작할 때, 타겟 데이터 및 제2 잠재 타겟 데이 터 사이의 차이에 기초하여 결정된 제2 손실 함수일 수 있다. 다시 말해, 일 실시예에 따른 제2 잠재 타겟 데 이터와 타겟 데이터 사이의 차이가 최소가 되는 방향으로 예측 모듈이 학습될 수 있다. 수학식 4에서, 는 교사 모델로 동작할 때, 타겟 데이터 및 제1 잠재 타겟 데이터 사이 의 차이에 기초하여 결정된 제1 손실 함수일 수 있다. 일 실시예에 따른 지식 증류 모델은 마스킹된 타겟 데이 터( 를 기초로 제1 잠재 타겟 데이터를 예측할 수 있다. 전술한 바와 같이, 마스킹 되기 전 타겟 데이터(y)는 제1 잠재 타겟 데이터(z)와 동일할 수 있다. 다시 말해, 일 실시예에 따른 제1 잠재 타겟 데이터(z)를 예측하는 것은 마스킹된 타겟 데이터( )를 이용하여 마스킹 되기 전 타겟 데이터(y)를 예측하는 것과 동일한 의미를 갖을 수 있다. 일 실시예에 따른 제1 잠재 타겟 데이터와 타겟 데이터 사이의 차이가 최소가 되는 방향으로 예 측 모듈이 학습될 수 있다. 수학식 4에서, 는 제1 잠재 타겟 데이터와 제2 잠재 타겟 데이터 사이의 차이에 기초하여 결정된 제3 손실 함수일 수 있다. 다시 말해, 일 실시예에 따른 제1 잠재 타겟 데이터와 제2 잠재 타겟 데이터 사이의 차이가 최소가 되는 방향으로 예측 모듈이 학습될 수 있다. 예시적으로, 본 명세서에서는 손실 함수로 CTC와 크로스 엔트로피를 사용하는 예시를 설명하였으나, 반드시 이 로 한정하는 것은 아니다. 설계에 따라, 제1 잠재 타겟 데이터와 타겟 데이터가 다대일 관계를 갖도록 설정할 수 있는 다양한 손실 함수가 채택될 수 있다. 도 6은 일 실시예에 따른 인공 신경망 모델의 동작 방법을 설명하기 위한 순서도이다. 설명의 편의를 위해, 단계들(610 내지 630)은 도 3에 도시된 전자 장치를 사용하여 수행되는 것으로 기술 된다. 그러나 이 단계들(610 내지 630)은 어떤 다른 적절한 전자 기기를 통해, 그리고 어떤 적절한 시스템 내에 서도 사용될 수 있을 것이다. 나아가, 도 6의 동작은 도시된 순서 및 방식으로 수행될 수 있지만, 도시된 실시예의 사상 및 범위를 벗어나지 않으면서 일부 동작의 순서가 변경되거나 일부 동작이 생략될 수 있다. 도 6에 도시된 다수의 동작은 병렬로 또는 동시에 수행될 수 있다. 단계에서, 일 실시예에 따른 전자 장치는 소스 데이터 및 타겟 데이터를 수신하여 제1 잠재 타겟 (latent target) 데이터를 예측할 수 있다. 일 실시예에 따른 소스 데이터 및 타겟 데이터는 시계열 데이터일 수 있다. 일 실시예에 따른 제1 잠재 타겟 데이터와 타겟 데이터는 다대일(many-to-one) 관계를 갖도록 설정될 수 있다. 일 실시예에 따른 전자 장치는 CTC 알고리즘에 기초하여, 제1 잠재 타겟 데이터를 예측할 수 있다. 또는, 일 실시예에 따른 전자 장치는 타겟 데이터의 일부분을 마스킹하고, 소스 데이터 및 마스킹된 타겟 데이터 를 수신하여, 제1 잠재 타겟 데이터를 예측할 수 있다. 단계에서, 일 실시예에 따른 전자 장치는 소스 데이터 및 상수 데이터를 수신하여 제2 잠재 타겟 데 이터를 예측할 수 있다. 단계에서, 일 실시예에 따른 전자 장치는 제1 잠재 타겟 데이터 및 상기 제2 잠재 타겟 데이터에 기 초하여 인공 신경망 모델을 학습할 수 있다. 일 실시예에 따른 전자 장치는 제1 잠재 타겟 데이터와 상기 제2 잠재 타겟 데이터 사이의 차이가 최소가 되도록 인공 신경망 모델을 학습할 수 있다. 도 7은 일 실시예에 따른 전자 장치의 구성을 도시하는 도면이다. 도 7을 참조하면, 일 실시예에 따른 전자 장치는 하나 이상의 프로세서 및 메모리를 포함할 수 있다. 일 실시예에 따른 메모리는 컴퓨터에서 읽을 수 있는 명령어들(instructions)을 저장할 수 있다. 메모리 에 저장된 명령어들이 프로세서에 의해 실행되면, 프로세서는 명령어들에 의해 정의되는 동작들 을 처리할 수 있다. 메모리는 예를 들어 RAM(random access memories), DRAM(dynamic random access memories), SRAM(static random access memories) 또는 이 기술 분야에서 알려진 다른 형태의 비휘발성 메모리 를 포함할 수 있다. 메모리는 기 학습된 인공 신경망 기반의 생성 모델을 저장할 수 있다. 일 실시예에 따른 하나 이상의 프로세서는 전자 장치의 전체적인 동작을 제어한다. 프로세서는 목적하는 동작들(desired operations)을 실행시키기 위한 물리적인 구조를 갖는 회로를 가지는 하드웨어로 구현 된 장치일 수 있다. 목적하는 동작들은 프로그램에 포함된 코드(code) 또는 명령어들을 포함할 수 있다. 하드 웨어로 구현된 장치는 마이크로프로세서(microprocessor), 중앙 처리 장치(Central Processing Unit; CPU), 그 래픽 처리 장치(Graphic Processing Unit; GPU), 프로세서 코어(processor core), 멀티-코어 프로세서(multi-core processor), 멀티프로세서(multiprocessor), ASIC(Application-Specific Integrated Circuit), FPGA(Field Programmable Gate Array), NPU(Neural Processing Unit) 등을 포함할 수 있다 일 실시예에 따른 프로세서는 전자 장치가 실행하기 위한 기능 및 명령어들을 실행함으로써 전자 장 치를 제어할 수 있다. 프로세서는 도 2 내지 도 6을 통해 전술한 적어도 하나의 동작 및/또는 기능 을 수행하도록 전자 장치를 제어할 수 있다. 일 실시예에 따른 프로세서의 제어에 의해 전자 장치는 소스 데이터 및 타겟 데이터를 수신하여 제1 잠재 타겟(latent target) 데이터를 예측하고, 소스 데이터를 수신하여 제2 잠재 타겟 데이터를 예측하고, 제1 잠재 타겟 데이터 및 제2 잠재 타겟 데이터에 기초하여 인공 신경망 모델을 학습할 수 있다. 이상에서 설명된 실시예들은 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨 어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치, 방법 및 구성요소는, 예를 들 어, 프로세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마 이크로컴퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령(instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 소프트웨어 애플리 케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처 리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설명된 경우도 있지만,"}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소(processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치는 복수 개의 프로세서 또는 하 나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서(parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들의 조합을 포함 할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로(collectively) 처 리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(signal wave)에 영구적으로, 또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독 으로 또는 조합하여 포함할 수 있다. 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성 된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2022-0105734", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "이상과 같이 실시예들이 비록 한정된 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가진 자라면 상기를 기초로 다양한 기술적 수정 및 변형을 적용할 수 있다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다.도면 도면1 도면2 도면3 도면4 도면5a 도면5b 도면6 도면7"}
{"patent_id": "10-2022-0105734", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 인공 신경망을 이용한 딥러닝 연산 방법을 설명하기 위한 도면이다. 도 2는 종래의 지식 증류(knowledge distillation) 방법을 설명하기 위한 도면이다. 도 3은 일 실시예에 따른 전자 장치의 블록도를 도시한 도면이다. 도 4는 일 실시예에 따른 지식 증류 방법을 설명하기 위한 도면이다. 도 5a는 일 실시예에 따른 CTC(Connectionist Temporal Classification) 알고리즘에 기초하여, 제1 잠재 타겟 데이터를 예측하는 방법을 설명하기 위한 도면이다. 도 5b는 일 실시예에 따른 크로스 엔트로피(cross entropy)를 손실 함수로 이용하여, 제1 잠재 타겟 데이터를 예측하는 방법을 설명하기 위한 도면이다. 도 6은 일 실시예에 따른 인공 신경망 모델의 동작 방법을 설명하기 위한 순서도이다. 도 7은 일 실시예에 따른 전자 장치의 구성을 도시하는 도면이다."}
