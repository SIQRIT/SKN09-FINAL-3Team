{"patent_id": "10-2024-0082137", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0179081", "출원번호": "10-2024-0082137", "발명의 명칭": "멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행", "출원인": "주식회사 LG 경영개발원", "발명자": "정대웅"}}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "메모리와 프로세서를 포함하는 컴퓨팅 시스템이 멀티태스킹 모델을 학습시키는 방법으로서, 복수의 도메인(Domain)에 기초한 다중 태스크(Multi-task)를 처리하는 멀티태스킹 모델(Multi-tasking model)을 초기화하는 단계; 소정의 실험 데이터를 획득하는 단계; 및 상기 획득된 실험 데이터를 기초로 상기 멀티태스킹 모델을 트레이닝(Training)하는 단계를 포함하고, 상기 멀티태스킹 모델을 트레이닝하는 단계는, 상기 실험 데이터를 기초로 하나의 통합 잠재공간(Manifold)에서 데이터 간 기하학적 정렬을 지원하는 벡터인기하학적 정렬 벡터(Geometric alignment vector)를 획득하는 단계와, 상기 획득된 기하학적 정렬 벡터를 기초로 기하학적 정렬 로스(Geometric alignment loss)를 산출하는 단계와, 상기 산출된 기하학적 정렬 로스를 기초로 상기 멀티태스킹 모델의 매개변수를 업데이트하는 단계를 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 기하학적 정렬 벡터를 획득하는 단계는, 상기 멀티태스킹 모델이 포함하는 임베딩 모듈(Embedding Module)을 기초로 상기 실험 데이터를 소정의 임베딩공간으로 투영하여 벡터 형식으로 변환한 임베딩 벡터(Embedding vector)를 획득하는 단계를 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서, 상기 기하학적 정렬 벡터를 획득하는 단계는, 상기 멀티태스킹 모델이 포함하는 펄터베이션 모듈(Perturbation Module)을 기초로 상기 임베딩 벡터를 소정의방향으로 이동시킨 복수의 펄터베이션 벡터(Perturbation vector)를 획득하는 단계를 더 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서, 상기 기하학적 정렬 벡터를 획득하는 단계는, 상기 멀티태스킹 모델이 포함하는 인코더 모듈(Encoder Module)을 기초로 상기 임베딩 벡터를 제1 태스크(Task1)의 잠재공간으로 투영한 원본 잠재 벡터(Original latent vector)를 획득하는 단계와, 상기 멀티태스킹 모델이 포함하는 인코더 모듈을 기초로 상기 펄터베이션 벡터를 제1 태스크의 잠재공간으로 투영한 펄터베이션 잠재 벡터(Perturbation latent vector)를 획득하는 단계를 더 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2024-0179081-3-제4 항에 있어서, 상기 기하학적 정렬 벡터를 획득하는 단계는, 상기 멀티태스킹 모델이 포함하는 트랜스퍼 모듈(Transfer Module)을 기초로 상기 원본 잠재 벡터를 제2 태스크(Task 2)의 잠재공간으로 매핑한 원본 트랜스퍼 벡터(Original transfer vector)를 획득하는 단계와, 상기 멀티태스킹 모델이 포함하는 트랜스퍼 모듈을 기초로 상기 펄터베이션 잠재 벡터를 제2 태스크의 잠재공간으로 매핑한 펄터베이션 트랜스퍼 벡터(Perturbation transfer vector)를 획득하는 단계를 더 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서, 상기 기하학적 정렬 벡터를 획득하는 단계는, 상기 멀티태스킹 모델이 포함하는 인버스 모듈(Inverse Transfer Module)을 기초로 상기 원본 트랜스퍼 벡터를상기 제1 태스크의 잠재공간으로 재매핑한 원본 인버스 벡터(Original inverse vector)를 획득하는 단계와, 상기 멀티태스킹 모델이 포함하는 인버스 모듈을 기초로 상기 펄터베이션 트랜스퍼 벡터를 상기 제1 태스크의잠재공간으로 재매핑한 펄터베이션 인버스 벡터(Perturbation inverse vector)를 획득하는 단계를 더 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 기하학적 정렬 벡터를 기초로 회귀 손실(Regression loss), 오토인코더 손실(Autoencoder loss), 일관성손실(Consistency loss), 매핑 손실(Mapping loss) 및 거리 손실(Distance loss)을 산출하는 단계와, 상기 산출된 회귀 손실, 오토인코더 손실, 일관성 손실, 매핑 손실 및 거리 손실을 가중 합한 통합 손실(Integrated loss)을 산출하는 단계를 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서, 상기 멀티태스킹 모델의 매개변수를 업데이트하는 단계는, 상기 통합 손실을 최소화하는 방향으로 상기 매개변수를 업데이트하는 단계를 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "재7 항에 있어서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 멀티태스킹 모델이 포함하는 리그레서 모듈(Regressor Module)을 기초로 상기 잠재 벡터에 따른 예측값을획득하는 단계와, 상기 획득된 예측값 및 상기 잠재 벡터에 대응하는 레이블(Label)값을 기초로 평균 제곱 오차(Mean SquaredError)를 계산하는 단계와, 상기 계산된 평균 제곱 오차를 기초로 상기 회귀 손실을 산출하는 단계를 더 포함하는 멀티태스킹 모델 학습 방법.공개특허 10-2024-0179081-4-청구항 10 재7 항에 있어서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 원본 잠재 벡터 및 상기 원본 인버스 벡터를 기초로 평균 제곱 오차를 계산하는 단계와, 상기 계산된 평균 제곱 오차를 기초로 상기 오토인코더 손실을 산출하는 단계를 더 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "재7 항에 있어서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 제1 태스크의 잠재공간으로부터 상기 제2 태스크의 잠재공간으로 매핑된 펄터베이션 트랜스퍼 벡터와, 상기 제2 태스크의 잠재공간으로부터 상기 제1 태스크의 잠재공간으로 매핑된 펄터베이션 트랜스퍼 벡터를 기초로평균 제곱 오차를 계산하는 단계와, 상기 계산된 평균 제곱 오차를 기초로 상기 일관성 손실을 산출하는 단계를 더 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "재7 항에 있어서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 제1 태스크에 기준한 레이블값 및 상기 제2 태스크에 기준한 원본 인버스 벡터에 따른 예측값을 기초로 평균 제곱 오차를 계산하는 단계와, 상기 계산된 평균 제곱 오차를 기초로 상기 매핑 손실을 산출하는 단계를 더 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "재7 항에 있어서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 제1 태스크에 기준한 원본 트랜스퍼 벡터 및 펄터베이션 트랜스퍼 벡터 간 거리인 제1 트랜스퍼 벡터 변위를 계산하는 단계와, 상기 제2 태스크에 기준한 원본 트랜스퍼 벡터 및 펄터베이션 트랜스퍼 벡터 간 거리인 제2 트랜스퍼 벡터 변위를 계산하는 단계와, 상기 계산된 제1 트랜스퍼 벡터 변위 및 상기 제2 트랜스퍼 벡터 변위를 기초로 평균 제곱 오차를 계산하는 단계와, 상기 계산된 평균 제곱 오차를 기초로 상기 거리 손실을 산출하는 단계를 더 포함하는 멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1 항에 있어서, 상기 실험 데이터는, 소정의 물질(物質)이 보유하는 고유한 특성을 특정하는 정보인 물질 고유특성 정보와, 소정의 물성(物性)에 대하여 소정의 물질이 가지는 데이터 값을 특정하는 정보인 물질 물성특정 정보를 포함하는 공개특허 10-2024-0179081-5-멀티태스킹 모델 학습 방법."}
{"patent_id": "10-2024-0082137", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "적어도 하나의 메모리; 및 상기 메모리에 저장된 적어도 하나의 애플리케이션을 독출하여 멀티태스킹 모델을 학습시키는 적어도 하나의 프로세서;를 포함하고, 상기 프로세서의 명령어는, 복수의 도메인(Domain)에 기초한 다중 태스크(Multi-task)를 처리하는 멀티태스킹 모델(Multi-tasking model)을 초기화하는 단계와, 소정의 실험 데이터를 획득하는 단계와, 상기 획득된 실험 데이터를 기초로 하나의 통합 잠재공간(Manifold)에서 데이터 간 기하학적 정렬을 지원하는벡터인 기하학적 정렬 벡터(Geometric alignment vector)를 획득하는 단계와, 상기 획득된 기하학적 정렬 벡터를 기초로 기하학적 정렬 로스(Geometric alignment loss)를 산출하는 단계와, 상기 산출된 기하학적 정렬 로스를 기초로 상기 멀티태스킹 모델의 매개변수를 업데이트하는 단계를 수행하는명령어를 포함하는 멀티태스킹 모델 학습 서버."}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 실시예에 따른 멀티태스킹 모델 학습 방법은, 메모리와 프로세서를 포함하는 컴퓨팅 시스템이 멀티태 스킹 모델을 학습시키는 방법으로서, 복수의 도메인(Domain)에 기초한 다중 태스크(Multi-task)를 처리하는 멀티 태스킹 모델(Multi-tasking model)을 초기화하는 단계; 소정의 실험 데이터를 획득하는 단계; 및 상기 획득된 실 험 데이터를 기초로 상기 멀티태스킹 모델을 트레이닝(Training)하는 단계를 포함하고, 상기 멀티태스킹 모델을 트레이닝하는 단계는, 상기 실험 데이터를 기초로 하나의 통합 잠재공간(Manifold)에서 데이터 간 기하학적 정렬 을 지원하는 벡터인 기하학적 정렬 벡터(Geometric alignment vector)를 획득하는 단계와, 상기 획득된 기하학적 정렬 벡터를 기초로 기하학적 정렬 로스(Geometric alignment loss)를 산출하는 단계와, 상기 산출된 기하학적 정렬 로스를 기초로 상기 멀티태스킹 모델의 매개변수를 업데이트하는 단계를 포함한다."}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법에 관한 것이다. 보다 상세하게는, 복수의 도메인(Domain)을 충족하는 통합 출력을 위한 다중 태스크(Multi- task)를 처리하기 위하여 각 태스크별 잠재공간의 지식 데이터를 하나의 통합된 잠재공간에서의 기하학적 정렬 (Geometric alignment)을 통해 상호 전이 및 학습(Transfer Learning)시키는 멀티태스킹 모델 학습 방법 및 이 를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법에 관한 것이다."}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "기계 학습과 인공지능 모델은 대규모의 데이터를 필요로 한다. 그러나 현실적으로 항상 충분한 데이터를 보유하 기엔 한계가 있다. 이는 특히 새로운 도메인이나 태스크에 모델을 적용하려 할 때 더욱 심화된다. 대표적으로 분자구조 데이터 셋은 이러한 상황의 좋은 예시이다. 화학과 약학 분야에서는 새로운 분자의 특성을 예측하기 위해 데이터가 필요하지만, 각 분자에 대한 실험 데이터는 얻기 어렵고 많은 비용이 요구된다. 따라서, 이미 학 습된 모델의 지식을 새로운 태스크에 적용하는 전이학습(Transfer Learning) 기법의 필요성이 증대되었다. 그러나, 기존의 전이학습은 주로 이미지나 텍스트 데이터와 같은 대규모 데이터 셋의 분류 문제에 집중되어 발 전해 왔다. 때문에 기존의 전이학습 기법들은 회귀 문제나 분자 데이터 셋과 같은 소규모, 복잡한 데이터 셋에 적용할 시 한계를 나타낸다. 특히, 분자구조 데이터와 같은 고차원적이며 각 구성과 결합의 관계가 각각의 물성 에 크게 영향을 주기 때문에, 하나의 물성과 분자구조 데이터 간의 관계를 학습한 후 다른 물성과 분자구조 데 이터 관계에 전이학습 기법을 적용할 경우, 기존의 유클리드 공간 기반 전이학습 기법들은 이러한 비유클리드 공간에서의 복잡한 구조를 효과적으로 처리하지 못한다. 한편, 리만 기하학(Riemannian Geometry)은 곡선 공간에서의 미적분을 가능하게 하여, 데이터의 복잡한 구조를 더 잘 표현하고 분석할 수 있게 한다. 이러한 리만 기하학적 접근법은 잠재 벡터를 곡선 매니폴드 상에 존재한 다고 가정하고 이를 통해 복잡한 소스와 타겟 태스크 간의 지오메트리를 정렬하는 데 유리하게 작용한다. 따라서, 위와 같은 배경을 바탕으로 소규모 데이터 셋에서도 높은 예측 성능과 안정성을 발휘하며, 보다 효과적 인 전이학습을 구현함과 동시에, 모델 정규화 성능을 제고하여 그 일반화 성능을 향상시키는 새로운 기술 도입 이 필요하다. 선행기술문헌특허문헌 (특허문헌 0001) KR 10-2502441 B1"}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 실시예는, 복수의 도메인(Domain)에 따른 출력을 위한 다중 태스크(Multi-task)를 처리하기 위하 여 각 태스크별 잠재공간의 지식 데이터를 하나의 통합된 잠재공간에서의 기하학적 정렬(Geometric alignment) 을 통해 상호 전이 및 학습시키는 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법을 제공하는데 그 목적이 있다. 또한, 본 발명의 일 실시예는, 이와 같이 학습된 멀티태스킹 모델을 이용하여 복수의 도메인을 각각 만족하는 통합 출력을 예측하는 통합 예측모델을 개발하는 것을 목표로 한다. 또한, 본 발명의 일 실시예는, 통합 예측모델을 복수의 물성과 물질 사이의 관계를 예측하는데 적용하여, 특정 물질에 대해 복수의 물성을 예측할 수 있고, 복수의 물성을 만족하는 특정 물질을 예측하는 통합 예측모델을 제 공하는 것을 목표로 한다. 다만, 본 발명 및 본 발명의 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되 지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시예에 따른 멀티태스킹 모델 학습 방법은, 메모리와 프로세서를 포함하는 컴퓨팅 시스템이 멀티태 스킹 모델을 학습시키는 방법으로서, 복수의 도메인(Domain)에 기초한 다중 태스크(Multi-task)를 처리하는 멀 티태스킹 모델(Multi-tasking model)을 초기화하는 단계; 소정의 실험 데이터를 획득하는 단계; 및 상기 획득된 실험 데이터를 기초로 상기 멀티태스킹 모델을 트레이닝(Training)하는 단계를 포함하고, 상기 멀티태스킹 모델 을 트레이닝하는 단계는, 상기 실험 데이터를 기초로 하나의 통합 잠재공간(Manifold)에서 데이터 간 기하학적 정렬을 지원하는 벡터인 기하학적 정렬 벡터(Geometric alignment vector)를 획득하는 단계와, 상기 획득된 기 하학적 정렬 벡터를 기초로 기하학적 정렬 로스(Geometric alignment loss)를 산출하는 단계와, 상기 산출된 기 하학적 정렬 로스를 기초로 상기 멀티태스킹 모델의 매개변수를 업데이트하는 단계를 포함한다. 다른 측면에서, 상기 기하학적 정렬 벡터를 획득하는 단계는, 상기 멀티태스킹 모델이 포함하는 임베딩 모듈 (Embedding Module)을 기초로 상기 실험 데이터를 소정의 임베딩 공간으로 투영하여 벡터 형식으로 변환한 임베 딩 벡터(Embedding vector)를 획득하는 단계를 포함한다. 다른 측면에서, 상기 기하학적 정렬 벡터를 획득하는 단계는, 상기 멀티태스킹 모델이 포함하는 펄터베이션 모 듈(Perturbation Module)을 기초로 상기 임베딩 벡터를 소정의 방향으로 이동시킨 복수의 펄터베이션 벡터 (Perturbation vector)를 획득하는 단계를 더 포함한다. 다른 측면에서, 상기 기하학적 정렬 벡터를 획득하는 단계는, 상기 멀티태스킹 모델이 포함하는 인코더 모듈 (Encoder Module)을 기초로 상기 임베딩 벡터를 제1 태스크(Task 1)의 잠재공간으로 투영한 원본 잠재 벡터 (Original latent vector)를 획득하는 단계와, 상기 멀티태스킹 모델이 포함하는 인코더 모듈을 기초로 상기 펄 터베이션 벡터를 제1 태스크의 잠재공간으로 투영한 펄터베이션 잠재 벡터(Perturbation latent vector)를 획득 하는 단계를 더 포함한다. 다른 측면에서, 상기 기하학적 정렬 벡터를 획득하는 단계는, 상기 멀티태스킹 모델이 포함하는 트랜스퍼 모듈 (Transfer Module)을 기초로 상기 원본 잠재 벡터를 제2 태스크(Task 2)의 잠재공간으로 매핑한 원본 트랜스퍼 벡터(Original transfer vector)를 획득하는 단계와, 상기 멀티태스킹 모델이 포함하는 트랜스퍼 모듈을 기초로 상기 펄터베이션 잠재 벡터를 제2 태스크의 잠재공간으로 매핑한 펄터베이션 트랜스퍼 벡터(Perturbation transfer vector)를 획득하는 단계를 더 포함한다. 다른 측면에서, 상기 기하학적 정렬 벡터를 획득하는 단계는, 상기 멀티태스킹 모델이 포함하는 인버스 모듈 (Inverse Transfer Module)을 기초로 상기 원본 트랜스퍼 벡터를 상기 제1 태스크의 잠재공간으로 재매핑한 원본 인버스 벡터(Original inverse vector)를 획득하는 단계와, 상기 멀티태스킹 모델이 포함하는 인버스 모듈을 기초로 상기 펄터베이션 트랜스퍼 벡터를 상기 제1 태스크의 잠재공간으로 재매핑한 펄터베이션 인버스 벡터 (Perturbation inverse vector)를 획득하는 단계를 더 포함한다. 다른 측면에서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 기하학적 정렬 벡터를 기초로 회귀 손실 (Regression loss), 오토인코더 손실(Autoencoder loss), 일관성 손실(Consistency loss), 매핑 손실(Mapping loss) 및 거리 손실(Distance loss)을 산출하는 단계와, 상기 산출된 회귀 손실, 오토인코더 손실, 일관성 손실, 매핑 손실 및 거리 손실을 가중 합한 통합 손실(Integrated loss)을 산출하는 단계를 포함한다. 다른 측면에서, 상기 멀티태스킹 모델의 매개변수를 업데이트하는 단계는, 상기 통합 손실을 최소화하는 방향으 로 상기 매개변수를 업데이트하는 단계를 포함한다. 다른 측면에서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 멀티태스킹 모델이 포함하는 리그레서 모듈 (Regressor Module)을 기초로 상기 잠재 벡터에 따른 예측값을 획득하는 단계와, 상기 획득된 예측값 및 상기 잠재 벡터에 대응하는 레이블(Label)값을 기초로 평균 제곱 오차(Mean Squared Error)를 계산하는 단계와, 상기 계산된 평균 제곱 오차를 기초로 상기 회귀 손실을 산출하는 단계를 더 포함한다. 다른 측면에서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 원본 잠재 벡터 및 상기 원본 인버스 벡터를 기초로 평균 제곱 오차를 계산하는 단계와, 상기 계산된 평균 제곱 오차를 기초로 상기 오토인코더 손실을 산출 하는 단계를 더 포함한다. 다른 측면에서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 제1 태스크의 잠재공간으로부터 상기 제2 태 스크의 잠재공간으로 매핑된 펄터베이션 트랜스퍼 벡터와, 상기 제2 태스크의 잠재공간으로부터 상기 제1 태스 크의 잠재공간으로 매핑된 펄터베이션 트랜스퍼 벡터를 기초로 평균 제곱 오차를 계산하는 단계와, 상기 계산된 평균 제곱 오차를 기초로 상기 일관성 손실을 산출하는 단계를 더 포함한다. 다른 측면에서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 제1 태스크에 기준한 레이블값 및 상기 제2 태스크에 기준한 원본 인버스 벡터에 따른 예측값을 기초로 평균 제곱 오차를 계산하는 단계와, 상기 계산된 평 균 제곱 오차를 기초로 상기 매핑 손실을 산출하는 단계를 더 포함한다. 다른 측면에서, 상기 기하학적 정렬 로스를 산출하는 단계는, 상기 제1 태스크에 기준한 원본 트랜스퍼 벡터 및 펄터베이션 트랜스퍼 벡터 간 거리인 제1 트랜스퍼 벡터 변위를 계산하는 단계와, 상기 제2 태스크에 기준한 원 본 트랜스퍼 벡터 및 펄터베이션 트랜스퍼 벡터 간 거리인 제2 트랜스퍼 벡터 변위를 계산하는 단계와, 상기 계 산된 제1 트랜스퍼 벡터 변위 및 상기 제2 트랜스퍼 벡터 변위를 기초로 평균 제곱 오차를 계산하는 단계와, 상 기 계산된 평균 제곱 오차를 기초로 상기 거리 손실을 산출하는 단계를 더 포함한다. 다른 측면에서, 상기 실험 데이터는, 소정의 물질(物質)이 보유하는 고유한 특성을 특정하는 정보인 물질 고유 특성 정보와, 소정의 물성(物性)에 대하여 소정의 물질이 가지는 데이터 값을 특정하는 정보인 물질 물성특정 정보를 포함한다. 한편, 본 발명의 실시예에 따른 멀티태스킹 모델 학습 서버는, 적어도 하나의 메모리; 및 상기 메모리에 저장된 적어도 하나의 애플리케이션을 독출하여 멀티태스킹 모델을 학습시키는 적어도 하나의 프로세서;를 포함하고, 상기 프로세서의 명령어는, 복수의 도메인(Domain)에 기초한 다중 태스크(Multi-task)를 처리하는 멀티태스킹 모델(Multi-tasking model)을 초기화하는 단계와, 소정의 실험 데이터를 획득하는 단계와, 상기 획득된 실험 데 이터를 기초로 하나의 통합 잠재공간(Manifold)에서 데이터 간 기하학적 정렬을 지원하는 벡터인 기하학적 정렬 벡터(Geometric alignment vector)를 획득하는 단계와, 상기 획득된 기하학적 정렬 벡터를 기초로 기하학적 정 렬 로스(Geometric alignment loss)를 산출하는 단계와, 상기 산출된 기하학적 정렬 로스를 기초로 상기 멀티태 스킹 모델의 매개변수를 업데이트하는 단계를 수행하는 명령어를 포함한다."}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태 스킹 수행 방법은, 전이학습을 통해 소스 태스크(Source task)에서 학습된 지식을 타겟 태스크(Target task)로 전이하여 데이터 부족 문제를 해소함으로써, 소규모 데이터 셋에서도 다중 태스크에 대해 높은 성능을 유지하는 멀티태스킹 모델을 제공할 수 있는 효과가 있다. 따라서 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 데이터나 도메인 지식이 부족해 머신러닝 모델을 적용하기 어려웠던 분야까지 그 활용 범위를 확장시킬 수 있는 효과가 있다. 또한, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 회귀 문제에 효과적으로 적용할 수 있는 특화된 전이학습 기법을 제공함으로써, 분자 데이터 셋과 같은 복잡한 회귀 문제에서도 높은 예측 성능을 발휘할 수 있는 효과가 있다. 또한, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 리만 기하학적 접근을 통해 소스 태스크와 타겟 태스크 간의 지식 전달을 최적화함으 로써, 태스크 간의 지오메트리(geometry)적 일관성을 유지하여 전이학습의 효율성을 향상시킬 수 있는 효과가 있다. 또한, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 다수의 손실함수를 결합하여 모델의 다양한 측면을 정규화함으로써, 모델의 일반화 성 능을 더욱 제고시킬 수 있는 효과가 있다. 또한, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 복수의 도메인을 동시에 고려한 통합 예측모델을 통해 복수의 도메인에 요구를 각각 만족하는 통합 출력을 정확하게 예측할 수 있다. 또한, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 사전 학습되지 않은 새로운 도메인에 대한 데이터 추가시 빠른 업데이트를 통해 새로 운 도메인을 만족하는 통합 예측이 가능한 통합 예측모델을 제공할 수 있다. 또한, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 통합 예측모델을 물질과 복수의 물성 사이의 관계를 예측하는 정보를 추출하는데 활용 하여, 복수의 특정 물성을 만족하는 물질을 예측하거나, 반대로 특정 물질에 대하 각각의 물성의 특성을 예측하 는데 활용할 수 있다. 따라서 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 다양한 물질(소재)에 대해 범용적으로 활용 가능한 멀티태스킹 모델을 제공해 관련 산 업 전반의 품질을 증진시킬 수 있는 효과가 있다. 다만, 본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효 과들은 아래의 기재로부터 명확하게 이해될 수 있다."}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변환을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고"}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "상세한 설명에 상세하게 설명하고자 한다. 본 발명의 효과 및 특징, 그리고 그것들을 달성하는 방법은 도면과 함께 상세하게 후술되어 있는 실시예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시 예들에 한정되는 것이 아니라 다양한 형태로 구현될 수 있다. 이하의 실시예에서, 제1, 제2 등의 용어는 한정적 인 의미가 아니라 하나의 구성 요소를 다른 구성 요소와 구별하는 목적으로 사용되었다. 또한, 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 또한, 포함하다 또는 가지다 등의 용어는 명 세서상에 기재된 특징, 또는 구성요소가 존재함을 의미하는 것이고, 하나 이상의 다른 특징들 또는 구성요소가 부가될 가능성을 미리 배제하는 것은 아니다. 또한, 도면에서는 설명의 편의를 위하여 구성 요소들이 그 크기가 과장 또는 축소될 수 있다. 예컨대, 도면에서 나타난 각 구성의 크기 및 두께는 설명의 편의를 위해 임의로 나 타내었으므로, 본 발명이 반드시 도시된 바에 한정되지 않는다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예들을 상세히 설명하기로 하며, 도면을 참조하여 설명할 때 동일 하거나 대응하는 구성 요소는 동일한 도면부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. [멀티태스킹 학습 모델 제공 서비스를 구현하는 예시적인 시스템] 이하, 복수의 도메인(Domain)에 대한 통합 출력을 위한 다중 태스크(Multi-task)를 처리하기 위하여 각 태스크 별 잠재공간의 지식 데이터를 하나의 통합된 잠재공간에서 기하학적 정렬(Geometric alignment)을 통해 상호 전 이 및 학습시키고, 이에 기초한 멀티태스킹을 수행하는 멀티태스킹 학습 모델 제공 서비스를 구현하는 예시적인 시스템을 첨부된 도면을 참조하여 상세히 설명한다. 도 1은 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델 제공 서비스를 구현하는 컴퓨팅 시스템의 블록도의 예시를 도시한다. 도 1을 참조하면, 본 발명의 멀티태스킹 학습 모델 제공 서비스를 구현하는 컴퓨팅 시스템은, 유저 컴퓨 팅 디바이스, 서버 컴퓨팅 시스템 및 트레이닝 컴퓨팅 시스템을 포함하며, 디바이스들은 네트워 크를 통해 통신 가능하다. 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태 스킹 수행 방법은, 1) 유저 컴퓨팅 디바이스가 로컬에서 구현 및 제공할 수도 있고, 2) 유저 컴퓨팅 디바 이스와 통신하는 서버 컴퓨팅 시스템이 웹 서비스 형태로 구현 및 제공할 수도 있고, 3) 유저 컴퓨팅 디바이스와 서버 컴퓨팅 시스템이 서로 연계하여 구현 및 제공할 수도 있다. 이때, 실시예에서 유저 컴퓨팅 디바이스 및/또는 서버 컴퓨팅 시스템은, 네트워크를 통해 통신 적으로 연결된 트레이닝 컴퓨팅 시스템과의 인터렉션을 통해 머신 러닝 모델(120 및/또는 140)을 학습시 킬 수 있다. 트레이닝 컴퓨팅 시스템은, 서버 컴퓨팅 시스템과 별개이거나 서버 컴퓨팅 시스템 의 일부일 수 있다. 그리고 이때, 인공지능 모델은, 1) 유저 컴퓨팅 디바이스가 로컬에서 직접 학습시킬 수 있고, 2) 서버 컴 퓨팅 시스템과 유저 컴퓨팅 디바이스가 네트워크를 통해 서로 인터랙션하며 학습시킬 수 있고, 3) 별도의 트레이닝 컴퓨팅 시스템이 다양한 트레이닝 기법과 학습 기법을 사용하여 학습시킬 수 있다. 그 리고 트레이닝 컴퓨팅 시스템이 학습시킨 인공지능 모델을 네트워크를 통해 유저 컴퓨팅 디바이스 및/또는 서버 컴퓨팅 시스템에 전송하여 제공/업데이트 하는 방식으로 구현될 수도 있다. 일부 실시예에서 트레이닝 컴퓨팅 시스템은, 서버 컴퓨팅 시스템의 일부이거나, 유저 컴퓨팅 디바이 스의 일부일 수 있다. 유저 컴퓨팅 디바이스는, 스마트 폰(smart phone), 휴대폰, 디지털방송용 디바이스, PDA(personal digital assistants), PMP(portable multimedia player), 데스크 탑, 웨어러블 디바이스, 임베디드 컴퓨팅 장 치 및/또는 태블릿 PC(tablet PC) 등 기타 모든 유형의 컴퓨팅 장치를 포함할 수 있다. 이러한 유저 컴퓨팅 디바이스는, 적어도 하나 이상의 프로세서 및 메모리를 포함한다. 여기서, 프로세서는, 중앙처리장치(CPU), 그래픽처리장치(GPU), ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세스(microprocessors) 및/또는 기타 기능 수행을 위한 전기적 유 닛 중 적어도 하나 또는 전기적으로 연결된 복수의 프로세서들로 구성될 수 있다. 메모리는, RAM, ROM, EEPROM, EPROM, 플래시 메모리 디바이스, 자기 디스크 등 같은 하나 이상의 비일시 적/일시적 컴퓨터 판독가능한 저장 매체 및 이들의 조합을 포함할 수 있고, 인터넷(internet) 상에서 메모리의 저장 기능을 수행하는 서버의 웹 스토리지(web storage)를 포함할 수 있다. 이러한 메모리는, 상기 적어도 하나 이상의 프로세서가 인공지능 모델을 학습시키거나, 인공지능 모델을 통해 멀티태스킹 학습을 실행하 는 등의 기능 동작을 수행하기 위하여 필요한 데이터 및 명령어들을 저장할 수 있다. 일 실시예에서, 유저 컴퓨팅 디바이스는, 적어도 하나 이상의 머신 러닝 모델들을 저장할 수 있다. 자세히, 머신 러닝 모델은, 복수의 뉴럴 네트워크(예를 들어, Deep neural network)와 같은 다양한 머신 러닝 모델들 또는 비선형 모델 및/또는 선형 모델을 포함하는 다른 유형의 머신 러닝 모델들일 수 있으며, 이들 의 조합으로 구성할 수 있다. 이때, 뉴럴 네트워크는, 피드-포워드 뉴럴 네트워크들(Feed-forward neural networks), 순환 뉴럴 네트워크(예 를 들어, 장단기 메모리 순환 신경 네트워크들), 컨벌루션 신경 네트워크 및/또는 다른 형태의 신경 네트워크들 중 적어도 하나 이상을 포함할 수 있다. 일 실시예에서, 유저 컴퓨팅 디바이스는, 네트워크를 통해서 서버 컴퓨팅 시스템으로부터 적어 도 하나 이상의 머신 러닝 모델을 수신하고, 메모리에 저장한 후, 저장된 머신 러닝 모델을 프 로세서에 의해 실행하여, 멀티태스킹 학습 등을 수행할 수 있다. 다른 실시예에서, 서버 컴퓨팅 시스템은, 적어도 하나 이상의 머신 러닝 모델을 포함하여 머신 러닝 모델을 통한 동작을 수행하며, 유저 컴퓨팅 디바이스와 이와 관련된 데이터를 통신하는 방식으로 유 저 컴퓨팅 디바이스와 연동해 멀티태스킹 학습 모델 제공 서비스를 유저에게 제공할 수 있다. 예를 들어, 유저 컴퓨팅 디바이스는, 웹을 통해 서버 컴퓨팅 시스템이 머신 러닝 모델을 이용하 여 유저의 입력에 대한 출력을 제공하는 방식으로 멀티태스킹 학습 모델 제공 서비스를 수행할 수 있다. 또한, 머신 러닝 모델(120 및/또는 140)들 중 적어도 일부는 유저 컴퓨팅 디바이스에서 실행되고, 나머지 는 서버 컴퓨팅 시스템에서 실행되는 방식으로도 인공지능 모델이 구현될 수 있다. 또한, 유저 컴퓨팅 디바이스는, 유저의 입력을 감지하는 적어도 하나 이상의 입력 컴포넌트를 포함할 수 있다. 예를 들어, 유저 입력 컴포넌트는, 유저의 입력 매체(예를 들어, 손가락 또는 스타일러스)의 터 치를 감지하는 터치 센서(예를 들어, 터치 스크린 및/또는 터치 패드 등), 유저의 모션 입력을 감지하는 이미지 센서, 유저 음성 입력을 감지하는 마이크로폰, 버튼, 마우스 및/또는 키보드 등을 포함할 수 있다. 또한, 유저 입력 컴포넌트는, 인터페이스를 통해 외부 컨트롤러(예컨대, 마우스 및/또는 키보드 등)에 대한 입력을 수 신할 경우에, 인터페이스와 외부 컨트롤러가 포함될 수 있다. 서버 컴퓨팅 시스템은, 적어도 하나 이상의 프로세서와 메모리를 포함한다. 여기서, 프로세서 는, 중앙처리장치(CPU), 그래픽처리장치(GPU), ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro- controllers), 마이크로 프로세스(microprocessors) 및/또는 기타 기능 수행을 위한 전기적 유닛 중 적어도 하나 또는 전기적으로 연결된 복수의 프로세서들로 구성될 수 있다. 그리고 메모리는, RAM, ROM, EEPROM, EPROM, 플래시 메모리 디바이스, 자기 디스크 등 같은 하나 이상의 비일시적/일시적 컴퓨터 판독가능한 저장 매체 및 이들의 조합을 포함할 수 있다. 이러한 메모리는, 프로 세서가 인공지능 모델을 학습시키거나, 인공지능 모델을 통해 멀티태스킹 학습을 실행하는 등의 기능 동작 을 수행하기 위하여 필요한 데이터 및 명령어들을 저장할 수 있다. 일 실시예에서, 서버 컴퓨팅 시스템은, 적어도 하나 이상의 컴퓨팅 디바이스를 포함하여 구현될 수 있다. 예를 들어, 서버 컴퓨팅 시스템은, 복수의 컴퓨팅 디바이스를 순차적 컴퓨팅 아키텍처, 병렬 컴퓨팅 아키 텍처 또는 이들의 조합에 따라 동작하도록 구현될 수 있다. 또한, 서버 컴퓨팅 시스템은, 네트워크로 연결된 복수의 컴퓨팅 디바이스를 포함할 수 있다. 또한, 서버 컴퓨팅 시스템은, 적어도 하나 이상의 머신 러닝 모델을 저장할 수 있다. 예를 들어, 서 버 컴퓨팅 시스템은, 머신 러닝 모델로 뉴럴 네트워크 및/또는 기타 멀티 레이어 비선형 모델을 포함 할 수 있다. 예시적 신경 네트워크는 피드 포워드 뉴럴 네트워크, 딥 뉴럴 네트워크, 순환 뉴럴 네트워크 및 컨 벌루션 뉴럴 네트워크를 포함할 수 있다. 트레이닝 컴퓨팅 시스템은, 적어도 하나 이상의 프로세서와 메모리를 포함한다. 여기서, 프로세 서는, 중앙처리장치(CPU), 그래픽처리장치(GPU), ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro- controllers), 마이크로 프로세스(microprocessors) 및/또는 기타 기능 수행을 위한 전기적 유닛 중 적어도 하 나 또는 전기적으로 연결된 복수의 프로세서들로 구성될 수 있다. 그리고 메모리는, RAM, ROM, EEPROM, EPROM, 플래시 메모리 디바이스, 자기 디스크 등 같은 하나 이상의 비일시적/일시적 컴퓨터 판독가능한 저장 매체 및 이들의 조합을 포함할 수 있다. 이러한 메모리는, 프로 세서가 인공지능 모델의 학습 등을 수행하기 위하여 필요한 데이터 및 명령어들을 저장할 수 있 다. 예를 들어, 트레이닝 컴퓨팅 시스템은, 에러의 역방향 전파와 같은 다양한 트레이닝 또는 학습 기법을 사 용하여(도 3에 도시된 프레임워크에 따라), 유저 컴퓨팅 디바이스 및/또는 서버 컴퓨팅 시스템에 저 장된 머신 러닝 모델(120 및/또는 140)을 학습(training)시키는 모델 트레이너를 포함할 수 있다. 예시적으로, 이러한 모델 트레이너는, 정의된 손실 함수에 기초하여 머신 러닝 모델(120 및/또는 140)의 하나 이상의 파라미터에 대한 업데이트를 역전파(Backpropagation) 방식으로 수행할 수 있다. 일부 구현예에서, 에러의 역방향 전파를 수행하는 것은 시간을 통한 잘린 역전파(truncated backpropagation through time)를 수행하는 것을 포함할 수 있다. 모델 트레이너는, 트레이닝되는 머신 러닝 모델(120 및/ 또는 140)의 일반화 능력을 향상시키기 위해 다수의 일반화 기법들(예를 들어, 가중치 감소, 드롭 아웃 및/또는 지식 증류 등)을 수행할 수 있다. 특히, 모델 트레이너는, 일련의 트레이닝 데이터에 기초하여 머신 러닝 모델(120 및/또는 140)을 트 레이닝할 수 있다. 여기서, 트레이닝 데이터는, 예를 들어 이미지, 오디오 샘플 및/또는 텍스트 등과 같은 상이한 양식의 데이터를 포함할 수 있다. 사용될 수 있는 이미지 유형의 예는 비디오 프레임, LiDAR 포인트 클 라우드, X선 이미지, 컴퓨터 단층 촬영 스캔, 초분광 이미지 및/또는 다양한 기타 형태의 이미지를 포함할 수 있다. 이러한 트레이닝 데이터는, 유저 컴퓨팅 디바이스 및/또는 서버 컴퓨팅 시스템에 의해 제공될 수 있다. 유저 컴퓨팅 디바이스의 특정 데이터에 대해 트레이닝 컴퓨팅 디바이스가 머신 러닝 모델(120 및 /또는 140)을 학습시킬 경우, 머신 러닝 모델(120 및/또는 140)은 개인화된 모델로 특성화될 수 있다. 그리고 모델 트레이너는, 원하는 기능을 제공하기 위해 활용되는 컴퓨터 로직을 포함한다. 또한, 모델 트레이너는, 범용 프로세서를 제어하는 하드웨어, 펌웨어 및/또는 소프트웨어로 구현될 수 있 다. 일 구현예에서 모델 트레이너는, 저장 디바이스에 저장된 프로그램 파일을 포함하고, 메모리에 로딩되고 하나 이상의 프로세서에 의해 실행될 수 있다. 다른 구현예에서, 모델 트레이너는, RAM 하 드 디스크 또는 광학 또는 자기적 매체와 같은 유형적 컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 실행 가능한데이터 및 명령어들의 하나 이상의 세트들을 포함한다. 네트워크는, 3GPP(3rd Generation Partnership Project) 네트워크, LTE(Long Term Evolution) 네트워크, WIMAX(World Interoperability for Microwave Access) 네트워크, 인터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), 블루투스(Bluetooth) 네트워크, 위성 방송 네트워크, 아날로그 방송 네트워크 및/또는 DMB(Digital Multimedia Broadcasting) 네트워크 등이 포함되나 이에 한정되지는 않는다. 일반적으로, 네트워크를 통한 통신은 임의의 유형의 유선 및/또는 무선 연결을 사용하여, 다양한 통신 프 로토콜들(예를 들어, TCP/IP, HTTP, SMTP 및/또는 FTP 등), 인코딩 또는 포맷들(예를 들어, HTML 및/또는 XML 등), 및/또는 보호 스키마(예를 들어, VPN, 시큐어 HTTP 및/또는 SSL 등)를 통해 수행될 수 있다. 도 2는 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델 제공 서비스를 구현하는 컴퓨팅 디바이스의 블록도의 예시를 도시한다. 도 2를 포함하면, 유저 컴퓨팅 디바이스, 서버 컴퓨팅 시스템 및 트레이닝 컴퓨팅 시스템에 포 함되는 컴퓨팅 디바이스는, 다수의 애플리케이션(예를 들어, 애플리케이션 1 내지 애플리케이션 N)을 포함 한다. 각 애플리케이션은 머신 러닝 라이브러리 및 하나 이상의 머신 러닝 모델을 포함할 수 있다. 예를 들어, 애플리케이션은 이미지 처리(예를 들어, Detection, Classification 및/또는 Segmentation 등) 애플리케이션, 텍스트 메시징 애플리케이션, 이메일 애플리케이션, 받아쓰기 애플리케이션, 가상 키보드 애플리케이션, 브라우 저 애플리케이션 및/또는 챗-봇(Chat-bot) 애플리케이션 등을 포함할 수 있다. 실시예에서, 컴퓨팅 디바이스는, 인공지능 모델을 학습시키기 위한 모델 트레이너를 포함할 수 있고, 상기 학습된 인공지능 모델을 저장하고 동작시킴으로써, 소정의 입력 데이터(실시예로, 물질 고유특성 정보 및/ 또는 물질 물성특정 정보 등)에 따른 출력 데이터를 제공할 수 있다. 컴퓨팅 디바이스의 각 애플리케이션은, 예를 들어 적어도 하나 이상의 센서, 컨텍스트 관리자, 디바이스 상태 컴포넌트 및/또는 추가 컴포넌트들과 같은 컴퓨팅 디바이스의 다수의 다른 컴포넌트들과 통신할 수 있다. 일 실시예에서, 각 애플리케이션은, API(예를 들어, 퍼블릭 API)를 사용하여 각 디바이스 컴포넌트와 통 신할 수 있다. 일 실시예에서, 각 애플리케이션에 의해 사용되는 API는 해당 애플리케이션에 대해 특정적일 수 있다. 도 3은 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델 제공 서비스를 구현하는 컴퓨팅 디바이스에 대한 다 른 측면에서의 블록도의 예시를 도시한다. 도 3을 참조하면, 컴퓨팅 디바이스는, 다수의 애플리케이션(예를 들어, 애플리케이션 1 내지 애플리케이션 N)을 포함한다. 각 애플리케이션은 중앙 인텔리전스 레이어와 통신할 수 있다. 예를 들어 애플리케이션은, 이미 지 처리 애플리에키션, 문자 메시지 애플리케이션, 이메일 애플리케이션, 받아쓰기 애플리케이션, 가상 키보드 애플리케이션 및/또는 브라우저 애플리케이션 등을 포함할 수 있다. 일 실시예에서, 각 애플리케이션은 API(예: 모든 애플리케이션에 걸쳐 공통 API)를 사용하여 중앙 인텔리전스 레이어(및 그 안에 저장된 모델)과 통신할 수 있다. 중앙 인텔리전스 레이어는 다수의 머신 러닝 모델들을 포함할 수 있다. 예를 들어, 도 3에 도시된 바와 같이, 각각의 머신 러닝 모델 중 적어도 일부가 각 애플리케이션에 대해 제공될 수 있고, 중앙 인텔리전스 레이어에 의해 관리될 수 있다. 다른 구현예에서, 2개 이상의 애플리케이션들은 단일의 머신 러닝 모델을 공유할 수 있다. 예를 들어, 일부 구현예에서, 중앙 인텔리전스 레이어는 모든 애플리케이션에 대해 단일 모델을 제공할 수 있다. 일부 구현예에서, 중앙 인텔리전스 레이어는 컴퓨팅 디바이스의 운영 체제 내에 포함되거나 이와 다르게 구현될 수 있다. 중앙 인텔리전스 레이어는 중앙 디바이스 데이터 레이어와 통신할 수 있다. 중앙 디바이스 데이터 레이어는 컴 퓨팅 디바이스에 대한 중앙 집중식 데이터 저장소일 수 있다. 도 3에 도시된 바와 같이, 중앙 디바이스 데 이터 레이어는 예를 들어, 하나 이상의 센서, 컨텍스트 관리자, 디바이스 상태 컴포넌트 및/또는 추가 컴포넌트 들과 같은 컴퓨팅 디바이스의 다수의 다른 컴포넌트들과 통신할 수 있다. 일부 구현예에서, 중앙 디바이스 데이터 레이어는 API(예를 들어, 사설 API)를 사용하여 각 디바이스 컴포넌트와 통신할 수 있다. 본 명세서에서 설명한 기술은 서버, 데이터베이스, 소프트웨어 애플리케이션들 및 다른 컴퓨터 기반 시스템들뿐 만 아니라 취해진 액션들 및 상기 시스템으로 전송되거나 그로부터 전송된 정보를 참조할 수 있다. 컴퓨터 기반시스템들의 내재적 유연성은 광범위한 가능한 구성들, 조합들 및 작업의 분할 및 컴포넌트들 간의 및 그로부터 의 기능성을 허용함을 인식할 것이다. 예를 들어, 본 명세서에서 기술한 프로세스들은 단일의 디바이스 또는 컴 포넌트 또는 조합으로 작동하는 다수의 디바이스들 또는 컴포넌트들을 사용하여 구현될 수 있다. 데이터베이스 및 애플리케이션들은 단일 시스템 또는 다수의 시스템들에 걸쳐 분산된 시스템에서 구현될 수 있다. 분산 컴포 넌트들은 순차적으로 또는 병렬로 동작할 수 있다. [멀티태스킹 학습 모델(MtLM: Multi-tasking Learning Model)] 도 4 및 도 5는 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델(MtLM)을 설명하기 위한 개념도의 예시들을 도시한다. 도 4 및 도 5를 참조하면, 본 발명의 실시예에 따른 멀티태스킹 학습 모델(MtLM)(Geometrically Aligned Transfer Encoder Model)은, 복수의 도메인(Domain)을 만족하는 통합 출력을 위한 다중 태스크(Multi-task)를 처리하기 위하여 각 태스크별 잠재공간에 파편화된 지식 데이터들(실시예로, 잠재 벡터 등)을 하나의 통합 잠재 공간(M: Manifold)에서 기하학적 전이(Geometric transfer)를 통해 상호 정렬하는 머신러닝 모델(Machine learning model)일 수 있다. 즉 실시예에 따른 멀티태스킹 학습 모델(MtLM)은, 다양한 도메인에 따른 지식 데이터들을 동시에 학습할 뿐만 아니라 여러 도메인 간 관계까지 효율적으로 학습함으로써, 학습 영역을 확장함과 동시에 각 도메인에 따른 국 소적 패턴과 복수의 도메인 간 공통적 원리의 일괄 학습을 구현하는 효과적인 멀티태스킹 학습을 수행할 수 있 다. 이에 따라 멀티태스킹 학습 모델(MtLM)은, 위와 같이 학습된 본 모델에 기반한 각종 멀티태스킹 태스크의 처리 성능 및 정확도를 직접적으로 향상시킬 수 있다. 실시예에서는, 멀티태스킹 학습 모델(MtLM)을 물질과 복수의 물성과의 관계를 예시로 설명하기로 하며, 물질에 대한 제1 물성의 특성을 예측하는 제1 태스크와, 물질에 대해 제2 물성의 특성을 예측하는 제2 태스크 등을 포 함하는 복수의 태스크를 멀티 태스킹하는 학습 모델로 이하 설명하기로 한다. 다만, 본 발명은, 물질과 복수의 물성 관계를 멀티 태스킹하기 위한 학습 방법 및 예측 방법에 한정되지 아니함은 당연하며, 물질과 복수의 물성 관계와 같이 복수의 태스크를 동시에 수행해야 하는 각종 태스크에 모두 적용될 수 있다. 실시예에서 이러한 멀티태스킹 학습 모델(MtLM)은, 소정의 실험 데이터에 기초한 사전학습(Pre-training)을 수 행할 수 있다. 여기서, 실시예에 따른 실험 데이터란, 멀티태스킹 학습 모델(MtLM)의 트레이닝에 사용되는 학습 데이터로서 소 정의 물질 고유특성 정보 및 물질 물성특정 정보를 포함하는 데이터일 수 있다. 이때, 실시예에 따른 물질 고유특성 정보란, 소정의 물질(物質)이 보유하는 고유한 특성을 특정하는 정보일 수 있다. 예를 들어, 물질 고유특성 정보는, 소정의 물질명, 분자구조식 및/또는 화학식 등 중 적어도 하나 이상을 포함 할 수 있다. 또한, 실시예에 따른 물질 물성특정 정보란, 소정의 물성(物性)에 대하여 소정의 물질이 가지는 데이터 값을 특 정하는 정보일 수 있다. 예시적으로, 물질 물성특정 정보는, 소정의 물질이 가지는 끓는점, 녹는점, 굴절율, 용해도, 점도, 표면장력, 밀도, 강도 및/또는 열전도율 등과 같은 물성(즉, 도메인) 값을 포함할 수 있다. 한편, 실시예에서 위와 같이 사전학습을 수행한 멀티태스킹 학습 모델(MtLM)은, 소정의 물질 고유특성 정보 및/ 또는 물질 물성특정 정보를 입력받고, 입력된 정보 및 학습된 지식을 기초로 예측된 데이터를 출력할 수 있다. 실시예로, 멀티태스킹 학습 모델(MtLM)은, 소정의 물질 고유특성 정보를 입력받고, 입력된 정보 및 학습된 지식 에 따라서 예측된 물질 물성특정 정보를 출력할 수 있다. 다른 실시예로, 멀티태스킹 학습 모델(MtLM)은, 소정의 물질 물성특정 정보를 입력받고, 입력된 정보 및 학습된 지식에 따라서 예측된 물질 고유특성 정보를 출력할 수 있다. 또 다른 실시예로, 멀티태스킹 학습 모델(MtLM)은, 소정의 물질 고유특성 정보 및 물질 물성특정 정보를 입력받 고, 입력된 정보들 및 학습된 지식에 따라서 예측된 최적의 물질 고유특성 정보 및 물질 물성특정 정보를 출력 하도록 역설계된 모델을 포함할 수 있다. 이하 실시예에서는, 입력 데이터로 물질 고유특성 정보를 나타내는 분자구조식 데이터를 사용하고, 출력 데이터 로 각 태스크 별 물성의 특성 값 데이터를 사용하기로 한다. 도 6은 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델(MtLM)의 내부 블록도를 도시한다. 도 6을 참조하면, 다른 측면에서 실시예에 따른 멀티태스킹 학습 모델(MtLM)은, 적어도 하나의 임베딩 모듈 (EBM: Embedding Module), 인코더 모듈(ECM: Encoder Module), 리그레서 모듈(RGM: Regressor Module), 트랜스 퍼 모듈(TFM: Transfer Module), 인버스 모듈(ITM: Inverse Transfer Module), 펄터베이션 모듈(PBM: Perturbation Module) 및 로스 캘큘레이션 모듈(LCM: Loss Calculation Module)을 포함할 수 있다. 자세히, 본 발명의 실시예에 따른 임베딩 모듈(EBM: Embedding Module)은, 소정의 입력 데이터를 임베딩 벡터 (Embedding vector)로 변환하는 프리-인코더(Pre-encoder) 모듈일 수 있다. 구체적으로, 임베딩 모듈은, 고차원 데이터인 분자구조식 데이터를 저차원의 표현인 임베딩 벡터로 압축하여, 인코더가 처리할 입력에 차원을 줄여 계산 효율성 및 학습 속도를 향상시킬 수 있으며 인코더를 사전 학습 태스 크에 대한 분자구조식에서의 중요한 특징에 집중하여 학습시킬 수 있다. 이를 통해, 소스 태스크가 학습된 모델에서의 유용한 특징들을 타겟 태스크를 학습할 모델에 용이하게 적용 가 능하며, 서로 다른 도메인 간의 중첩되는 특징들에 대해 일반화가 가능하여, 새로운 도메인/태스크에 대한 전이 학습을 효과적으로 수행할 수 있다. 즉 임베딩 모듈(EBM)은, 특정 입력 데이터를 소정의 임베딩 공간으로 투영하여 벡터 형식으로 변환하는 모듈일 수 있다. 실시예로 임베딩 모듈(EBM)로, 분자구조식 특징 추출에 적합한 그래프 신경망(Graph Neural Network, GNN)을 이 용할 수 있으며, 예를 들어 DMPNN(Directed Message Passing Neural Network) 구조에 기초하여 입력 데이터에 대한 임베딩 벡터를 제공할 수 있다. 또한, 본 발명의 실시예에 따른 인코더 모듈(ECM: Encoder Module)은, 소정의 임베딩 벡터를 입력으로 하고, 입 력된 임베딩 벡터를 해당 태스크에 대응하는 잠재공간(Latent space)으로 투영하여 잠재 벡터(Latent vector)로 변환하는 모듈일 수 있다. 즉 인코더 모듈(ECM)은, 입력된 임베딩 벡터의 주요 특징을 추출하여 대응하는 잠재공간 상에 표현하는 모듈일 수 있다. 자세히, 인코더 모듈(ECM)은, 임베딩 벡터의 특징들 중 중요한 특징을 추출할 수 있고, 불필요한 정보 나 노이즈를 제거하는 등의 데이터 압축을 데이터의 저차원 공간으로 압축해가며 수행하여, 잠재 공간에서의 표 현인 잠재 벡터로 출력할 수 있다. 실시예에서 이러한 인코더 모듈(ECM)은, 복수의 도메인 각각에 대응하는 복수의 인코더 모듈(ECM)을 포함할 수 있다. 실시예로 인코더 모듈(ECM)은, 제1 도메인(예컨대, 끓는점)에 대응하는 제1 인코더 모듈(ECM) 및 제2 도메인(예 컨대, 녹는점)에 대응하는 제2 인코더 모듈(ECM)을 포함할 수 있다. 다른 실시예로 인코더 모듈(ECM)은, 제3 도메인(예컨대, 용해도)에 대응하여 제1 용매에 대한 용해도를 예측하 는 제1 태스크를 수행하기 위한 제3 인코더 모듈과, 제2 용매에 대한 용해도를 예측하는 제2 태스크를 수행하기 위한 제4 인코더 모듈을 포함할 수 있다. 즉, 다른 실시예에서는 동일한 도메인에 대해서 서로 다른 태스크에 대해 멀티 태스킹하는 경우를 포함할 수 있다. 물론 서로 다른 도메인에 대한 멀티 태스킹과 동일한 도메인에서 서로 다른 태스크에 대한 멀티 태스킹을 통합하여 수행하는 멀티 태스킹 모델도 본 발명의 일 실시예에 포함될 수 있다. 이하에서는 서로 다른 도메인이 서로 다른 태스크인 것을 기준으로 설명하기로 한다. 이때, 실시예에서 복수의 인코더 모듈(ECM) 중 어느 하나의 인코더 모듈(ECM)은, 본 발명의 실시예에 따른 전이 학습의 소스 태스크(Source task)에 대응하는 인코더 모듈(ECM)인 소스 인코더 모듈(ECM)일 수 있다. 또한, 상기 소스 인코더 모듈(ECM)을 제외한 나머지 인코더 모듈(ECM) 중 어느 하나의 인코더 모듈(ECM)은, 본 발명의 실시예에 따른 전이학습의 타겟 태스크(Target task)에 대응하는 인코더 모듈(ECM)인 타겟 인코더 모듈(ECM)일 수 있다. 또한, 본 발명의 실시예에 따른 리그레서 모듈(RGM: Regressor Module)은, 소정의 잠재 벡터를 입력으로 하고, 입력된 잠재 벡터에 따른 최종 예측 값을 생성하는 헤드(Head) 모듈일 수 있다. 즉 실시예에서는, 헤드 모듈의 예시로 리그레서 모듈을 사용하기로 한다. 이러한 리그레서 모듈(RGM)은, 최종 출력 생성에 직접적으로 관여하여 모델의 예측 성능을 결정할 수 있다. 또한, 실시예에서 리그레서 모듈(RGM)은, 복수의 도메인 각각에 대응하는 복수의 리그레서 모듈(RGM)을 포함할 수 있다. 실시예로 리그레서 모듈(RGM)은, 제1 도메인(예컨대, 끓는점)에 대응하는 제1 리그레서 모듈(RGM) 및 제2 도메 인(예컨대, 녹는점)에 대응하는 제2 리그레서 모듈(RGM)을 포함할 수 있다. 이때, 실시예에서 복수의 리그레서 모듈(RGM) 중 어느 하나의 리그레서 모듈(RGM)은, 본 발명의 실시예에 따른 전이학습의 소스 태스크(Source task)에 대응하는 리그레서 모듈(RGM)인 소스 리그레서 모듈(RGM)일 수 있다. 또한, 상기 소스 리그레서 모듈(RGM)을 제외한 나머지 리그레서 모듈(RGM) 중 어느 하나의 리그레서 모듈(RGM) 은, 본 발명의 실시예에 따른 전이학습의 타겟 태스크(Target task)에 대응하는 리그레서 모듈(RGM)인 타겟 리 그레서 모듈(RGM)일 수 있다. 또한, 본 발명의 실시예에 따른 트랜스퍼 모듈(TFM: Transfer Module)은, 소정의 잠재 벡터를 다른 태스크의 잠 재공간으로 매핑(Mapping)하여 전이 벡터(Transfer vector)로 변환하는 모듈일 수 있다. 자세히, 실시예에서 트랜스퍼 모듈(TFM)은, 리만 지오메트리(Riemannian geometry)에 기초하여 특정 잠재 벡터 를 다른 태스크의 잠재공간으로 매핑해 전이 벡터로 변환할 수 있다. 이 과정에서 트랜스퍼 모듈(TFM)은, 본 발명의 실시예에 따라서 상기 매핑되는 각 태스크 간 기하학적 정렬을 구현할 수 있다. 이에 대한 자세한 설명은 하기 멀티태스킹 모델 학습 방법에서 후술한다. 즉, 실시예에서 트랜스퍼 모듈(TFM)은, 제1 태스크에 따른 잠재 벡터를 제2 태스크에 따른 잠재공간으로 본 발 명의 실시예에 따른 기하학적 정렬을 통해 매핑하는 방식으로 복수의 태스크 간 지식 데이터의 전이를 효과적으 로 수행할 수 있다. 이때, 실시예에서 트랜스퍼 모듈(TFM)은, 오토인코더(Autoencoder) 구조를 활용함으로써 상기 변환된 벡터(즉, 전이 벡터)의 정확성과 일관성을 제고하는 데이터 처리를 지원할 수 있다. 또한, 실시예에서 트랜스퍼 모듈(TFM)은, 복수의 도메인 각각에 대응하는 복수의 트랜스퍼 모듈(TFM)을 포함할 수 있다. 실시예로 트랜스퍼 모듈(TFM)은, 제1 도메인(예컨대, 끓는점)에 대응하는 제1 트랜스퍼 모듈(TFM) 및 제2 도메 인(예컨대, 녹는점)에 대응하는 제2 트랜스퍼 모듈(TFM)을 포함할 수 있다. 이때, 실시예에서 복수의 트랜스퍼 모듈(TFM) 중 어느 하나의 트랜스퍼 모듈(TFM)은, 본 발명의 실시예에 따른 전이학습의 소스 태스크(Source task)에 대응하는 트랜스퍼 모듈(TFM)인 소스 트랜스퍼 모듈(TFM)일 수 있다. 또한, 상기 소스 트랜스퍼 모듈(TFM)을 제외한 나머지 트랜스퍼 모듈(TFM) 중 어느 하나의 트랜스퍼 모듈(TFM) 은, 본 발명의 실시예에 따른 전이학습의 타겟 태스크(Target task)에 대응하는 트랜스퍼 모듈(TFM)인 타겟 트 랜스퍼 모듈(TFM)일 수 있다. 또한, 본 발명의 실시예에 따른 인버스 모듈(ITM: Inverse Transfer Module)은, 트랜스퍼 모듈(TFM)에 의해 다 른 태스크의 잠재공간으로 매핑 및 변환된 전이 벡터를 다시 원래의 잠재공간에 매핑되도록 재구성하는 모듈일 수 있다. 그리하여 실시예에서 인버스 모듈(ITM)은, 전이 벡터를 다시 원래대로 재구성 및 변환한 벡터(이하, 인버스 벡 터)를 생성할 수 있다. 이때, 실시예에서 인버스 모듈(ITM)은, 오토인코더 구조를 활용함으로써 상술된 재구성 프로세스의 안정성과 해 당 전이 벡터의 정확성 및 일관성을 향상시킬 수 있다. 실시예에서, 이러한 인버스 모듈(ITM)은, 복수의 도메인 각각에 대응하는 복수의 인버스 모듈(ITM)을 포함할 수 있다. 실시예로 인버스 모듈(ITM)은, 제1 도메인(예컨대, 끓는점)에 대응하는 제1 인버스 모듈(ITM) 및 제2 도메인(예 컨대, 녹는점)에 대응하는 제2 인버스 모듈(ITM)을 포함할 수 있다. 이때, 실시예에서 복수의 인버스 모듈(ITM) 중 어느 하나의 인버스 모듈(ITM)은, 본 발명의 실시예에 따른 전이 학습의 소스 태스크(Source task)에 대응하는 인버스 모듈(ITM)인 소스 인버스 모듈(ITM)일 수 있다. 또한, 상기 소스 인버스 모듈(ITM)을 제외한 나머지 인버스 모듈(ITM) 중 어느 하나의 인버스 모듈(ITM)은, 본 발명의 실시예에 따른 전이학습의 타겟 태스크(Target task)에 대응하는 인버스 모듈(ITM)인 타겟 인버스 모듈 (ITM)일 수 있다. 또한, 본 발명의 실시예에 따른 펄터베이션 모듈(PBM: Perturbation Module)은, 소정의 임베딩 벡터에 소정의 변화를 가하여 복수의 펄터베이션 벡터(Perturbation vector)를 생성하는 모듈일 수 있다. 자세히, 실시예에서 펄터베이션 모듈(PBM)은, 특정 임베딩 벡터를 소정의 방향으로 이동시키는 변화를 가함으로 써 해당 임베딩 벡터에 기준한 주변 상에 복수의 펄터베이션 벡터(즉, 펄터베이션 포인트(Perturbation point, 교란점))를 생성하는 모듈일 수 있다. 이때, 상기 생성되는 복수의 펄터베이션 벡터는, 대응하는 임베딩 벡터와 상대적인 거리를 유지하도록 설계되어 기하학적 정렬을 효과적으로 보조할 수 있다. 즉, 위와 같은 펄터베이션 모듈(PBM)은, 복수의 펄터베이션 벡터를 생성해 모델의 기하학적 정렬을 보조함으로 써 소스 태스크 및 타겟 태스크 간 좌표계를 일치시키는데 도움을 줄 수 있다. 또한, 실시예에서 펄터베이션 모듈(PBM)은, 소정의 임베딩 벡터와 이를 기초로 생성된 복수의 펄터베이션 벡터 간 거리를 산출하고, 산출된 거리를 기초로 소스 태스크와 타겟 태스크 간 변위를 일치시키도록 지원할 수 있다. 이를 통해 펄터베이션 모듈(PBM)은, 모델에 대한 잠재공간에서의 일관성을 보다 용이하게 유지시킬 수 있다. 실시예에 따라 펄터베이션 모듈(PBM)은, 소정의 임베딩 벡터와 이를 기초로 생성된 복수의 펄터베이션 벡터 간 관계를 유지하도록 강제함으로써, 모델의 과적합을 방지하고 일반화 성능을 향상시킬 수 있다. 또한, 본 발명의 실시예에 따른 로스 캘큘레이션 모듈(LCM: Loss Calculation Module)은, 멀티태스킹 학습 모델 (MtLM)을 통해 획득된 각종 벡터에 기초하여 다양한 손실함수를 계산하는 모듈일 수 있다. 실시예에서, 로스 캘큘레이션 모듈(LCM)은, 본 발명의 실시예에 따른 회귀 손실(Regression loss), 오토인코더 손실(Autoencoder loss), 일관성 손실(Consistency loss), 매핑 손실(Mapping loss), 거리 손실(Distance loss) 및/또는 통합 손실(Integrated loss) 등을 산출할 수 있다. 이에 대한 자세한 설명은 하기 멀티태스킹 모 델 학습 방법에서 후술한다. 이를 통해 로스 캘큘레이션 모듈(LCM)은, 모델의 서로 다른 부분에 대한 정규화 및 학습을 지원할 수 있으며, 모델 학습을 위한 피드백을 제공하여 모델 최적화를 구현할 수 있다. 한편, 본 발명의 실시예에서 멀티태스킹 학습 모델(MtLM)은, 상술된 모듈들과 연동한 각종 데이터 처리 프로세 스를 통하여, 모델 최적화 및 업데이트를 수행할 수 있다. 예시적으로 멀티태스킹 학습 모델(MtLM)은, AdamW 최적화 알고리즘 등을 기초로 상술된 모듈들과 연동하여 모델 최적화 및 매개변수 업데이트를 수행할 수 있다. 이와 같이 본 발명의 실시예에서 멀티태스킹 학습 모델(MtLM)은, 다양한 도메인에 따른 지식 데이터들을 동시에 학습할 뿐만 아니라 여러 도메인 간 관계까지 효율적으로 학습함으로써, 학습 영역을 확장함과 동시에 각 도메 인에 따른 국소적 패턴과 복수의 도메인 간 공통적 원리의 일괄 학습을 구현하는 효과적인 멀티태스킹 학습을 수행할 수 있다. 이에 따라 멀티태스킹 학습 모델(MtLM)은, 위와 같이 학습된 본 모델에 기반한 각종 멀티태스킹 태스크의 처리 성능 및 정확도를 직접적으로 향상시킬 수 있다. [멀티태스킹 학습 모델 제공 서비스 구현방법] 이하, 본 발명의 일 실시예에 따른 컴퓨팅 시스템이 복수의 도메인(Domain)에 따른 출력을 위한 다중 태 스크(Multi-task)를 처리하기 위하여 각 태스크별 잠재공간의 지식 데이터를 하나의 통합된 잠재공간에서 기하 학적 정렬(Geometric alignment)을 통해 상호 전이 및 학습시키고, 이에 기초한 멀티태스킹을 수행하는 멀티태 스킹 학습 모델(MtLM) 제공 서비스를 구현하는 방법을 상세히 설명한다. 일반적으로 기존의 전이학습 기법은, 주로 이미지 및/또는 언어 데이터 셋의 분류 작업에 집중되어 있으며 회귀 문제나 비유클리드 공간에서의 문제 해결에는 한계를 가지고 있다. 특히, 학습 데이터 셋이 충분하지 않은 경우 상술된 문제에 대한 예측 성능의 저하는 더욱 필연적이며, 다양한 태스크 유형을 고려하는 멀티태스킹까지 요구되면 이를 위한 학습 및 예측에 있어 그 성능의 저하는 가중된다. 또한, 대부분의 기존 방법들은 유클리드 공간에서의 데이터를 다루는 데 최적화되어 있기 때문에 복잡한 곡선 공간이나 비선형 공간에서는 효과적으로 동작하지 못하고 있다. 도 7은 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법을 설명하기 위한 개념도의 예시를 도시한다. 따라서, 도 7과 같이, 본 발명의 일 실시예에 따른 컴퓨팅 시스템은, 소규모 데이터 셋의 회귀 문제와 기 존 전이학습 기법의 한계를 극복할 수 있는 새로운 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법을 제공하고자 한다. 이하, 본 발명의 일 실시예에 따른 설명에서는, 효과적인 설명을 위하여 상술된 물질을 '분자'로 한정하고 이에 대한 도메인을 '물성'에 기준하여 설명한다. 이는, 분자 데이터 셋이 일반적으로 데이터 양이 적고, 다양한 태스크 유형을 포함하며, 회귀 문제를 주로 다룬 다는 점을 고려한 것이다. 즉, 분자 데이터 셋의 경우 수많은 물성들과 연계된 다양한 태스크 처리가 요구되나 이에 대해 주어진 데이터가 매우 제한적이며, 각각의 물성들이 상호 밀접하게 연관되거나 영향을 준다는 특성을 가진다. 이러한 점을 고려했을 때, 분자 데이터 셋은 복수의 도메인에 따른 다중 태스크 처리에 적용하기 유리한 데이터 로서, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법의 설명을 위한 바람직한 일례일 수 있다. 다만 이에 제한되는 것은 아니며, 다중 도메인에 따른 멀티 태스크의 적용이 가능한 실시예라면 본 발명의 실시 예에 포함될 수 있음이 자명하다. 이하에서는, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법에 대해 첨부된 도면들을 참조하여 좀더 상세히 설명한다. 도 8은 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법을 설명하기 위한 블록 흐름도를 도시한다. 도 8을 참조하면, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모 델을 이용한 멀티태스킹 수행 방법은, 멀티태스킹 학습 모델(MtLM)을 초기화하는 단계(S101)와, 실험 데이터를 획득하는 단계(S103)와, 획득된 실험 데이터를 기초로 멀티태스킹 학습 모델(MtLM)을 트레이닝하는 단계(S105) 와, 트레이닝된 멀티태스킹 학습 모델(MtLM)을 제공하는 단계(S107)를 포함할 수 있다. 자세히, 본 발명의 일 실시예에 따른 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM)을 초기화할 수 있다. (S101) 여기서, 다시 말하자면 본 발명의 실시예에 따른 멀티태스킹 학습 모델(MtLM)(Geometrically Aligned Transfer Encoder Model)은, 복수의 도메인에 따른 출력을 위한 다중 태스크를 처리하기 위하여 각 태스크별 잠재공간에 파편화된 지식 데이터들(실시예로, 잠재 벡터 등)을 하나의 통합 잠재공간(M)에서 기하학적 전이(Geometric transfer)를 통해 상호 정렬하는 머신러닝 모델(Machine learning model)일 수 있다. 즉 실시예에 따른 멀티태스킹 학습 모델(MtLM)은, 다양한 도메인에 따른 지식 데이터들을 동시에 학습할 뿐만 아니라 여러 도메인 간 관계까지 효율적으로 학습함으로써, 학습 영역을 확장함과 동시에 각 도메인에 따른 국 소적 패턴과 복수의 도메인 간 공통적 원리의 일괄 학습을 구현하는 효과적인 멀티태스킹 학습을 수행할 수 있 다. 자세히, 실시예에서 컴퓨팅 시스템은, 위와 같은 멀티태스킹 학습 모델(MtLM)이 포함하는 각 구성요소에 대한 초기화(Initialization)를 수행할 수 있다. 실시예로 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM) 내 임베딩 네트워크( ), 인코더 네트워크( ), 리그레서(헤드) 네트워크( ), 트랜스퍼 네트워크( ) 및/또는 인버스 네트워크( ) 등을 무 작위 매개변수( )로 초기화할 수 있다. 또한, 실시예로 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM)에 적용할 소정의 최적화 알고리즘을 설정 할 수 있다. 예시적으로 컴퓨팅 시스템은, AdamW(Decoupled Weight Decay Regularization) 알고리즘을 최적화 알고리 즘으로 설정할 수 있으며, 실시예에 따라서 가중치 감쇠(Weight decay)를 독립적으로 처리하도록 최적화 알고리 즘을 개선하여 사용할 수 있다. 또한, 본 발명의 일 실시예에 따른 컴퓨팅 시스템은, 실험 데이터를 획득할 수 있다. (S103) 여기서, 다시 말하자면 본 발명의 실시예에 따른 실험 데이터( )란, 멀티태스킹 학습 모델(MtLM)의 트레이닝에 사용되는 학습 데이터로서 소정의 물질 고유특성 정보 및 물질 물성특정 정보를 포함하는 데이터일 수 있다. 이때, 실시예에 따른 물질 고유특성 정보란, 소정의 물질(物質)이 보유하는 고유한 특성을 특정하는 정보일 수 있다. 즉 실시예에서 물질 고유특성 정보는, 소정의 분자가 보유하는 고유한 특성을 특정하는 정보일 수 있다. 예를 들어, 물질 고유특성 정보는, 소정의 물질명, 분자구조식 및/또는 화학식 등을 포함할 수 있다. 또한, 실시예에 따른 물질 물성특정 정보란, 소정의 물성(物性)에 대하여 소정의 물질이 가지는 데이터 값을 특 정하는 정보일 수 있다. 예시적으로, 물질 물성특정 정보는, 소정의 물질이 가지는 끓는점, 녹는점, 굴절율, 용해도, 점도, 표면장력, 밀도, 강도 및/또는 열전도율 등과 같은 물성(즉, 도메인) 값을 포함할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 소정의 유저 입력 및/또는 외부 서버와의 연동 등에 기초하여 상술 된 바와 같은 실험 데이터를 획득할 수 있다. 또한, 본 발명의 일 실시예에 따른 컴퓨팅 시스템은, 획득된 실험 데이터를 기초로 멀티태스킹 학습 모델 (MtLM)을 트레이닝할 수 있다. (S105) 도 9는 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델(MtLM) 트레이닝 방법을 설명하기 위한 블록 흐름도를 도시하고, 도 10은 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델(MtLM) 트레이닝 방법을 설명하기 위한 개 념도의 예시를 도시한다. 즉, 도 9 및 도 10을 참조하면, 실시예에서 컴퓨팅 시스템은, 위와 같이 획득된 실험 데이터를 기초로 멀 티태스킹 학습 모델(MtLM)에 대한 사전학습을 수행할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM)에 대한 트레이닝 루프(Training loo p)를 설정할 수 있다. (S201) 보다 상세히, 실시예에서 컴퓨팅 시스템은, 트레이닝 시의 에폭(Epoch) 반복 수, 태스크(Task) 반복 수 및/또는 배치(Batch) 반복 수를 설정할 수 있다. 실시예로 컴퓨팅 시스템은, 트레이닝 시 에폭 'i'를 '1부터 n(n>=1)까지' 반복 수행하고, 각 태스크 't' 에 대하여 반복 수행하고, 기 설정된 각 배치 'b'에 대하여 반복 수행하도록 트레이닝 루프를 설정할 수 있다. 또한, 실시예에서 컴퓨팅 시스템은, 전술된 바와 같이 획득된 실험 데이터 기반의 기하학적 정렬 벡터를 획득할 수 있다. (S203) 여기서, 본 발명의 실시예에 따른 기하학적 정렬 벡터(Geometric alignment vector)란, 멀티태스킹 학습 모델 (MtLM)을 통해 획득되는 각종 벡터를 의미할 수 있다. 실시예에서 기하학적 정렬 벡터는, 임베딩 벡터( ), 펄터베이션 벡터( ), 인코딩 벡터, 전이 벡터 및 인 버스 벡터 등을 포함할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 상기 획득된 실험 데이터를 멀티태스킹 학습 모델(MtLM)에 입력할 수 있다. 또한, 실시예에서 컴퓨팅 시스템은, 실험 데이터를 입력한 멀티태스킹 학습 모델(MtLM)을 기초로 1) 임베 딩 벡터를 획득할 수 있다. 보다 상세히, 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM)의 임베딩 모듈(EBM)과 연동하여 상기 입력된 실험 데이터를 임베딩 네트워크를 통해 임베딩 벡터로 변환할 수 있다. 이에 따라 컴퓨팅 시스템은, 실험 데이터를 소정의 임베딩 공간으로 투영하여 벡터 형식으로 변환한 임베 딩 벡터를 획득할 수 있다. 또한, 실시예에서 컴퓨팅 시스템은, 2) 획득된 임베딩 벡터 기반의 펄터베이션 벡터를 생성할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM)의 펄터베이션 모듈(PBM)과 연동하여 상기 획득된 임베딩 벡터에 기준한 소정의 주변 상에 복수의 펄터베이션 벡터(즉, 펄터베이션 포인트 (Perturbation point, 교란점))를 생성할 수 있다. 이때, 실시예에서 컴퓨팅 시스템은, 각 태스크마다 상술된 기능 동작을 반복 수행하여 각 태스크별 대응 하는 펄터베이션 벡터를 획득할 수 있다. 실시예로 컴퓨팅 시스템은, 태스크 't'에 대응하는 펄터베이션 벡터와 태스크 's'에 대응하는 펄터베이션 벡터를 획득할 수 있다. 또한, 실시예에서 컴퓨팅 시스템은, 3) 생성된 펄터베이션 벡터 및 임베딩 벡터 기반의 인코딩 벡터를 획 득할 수 있다. 여기서, 실시예에 따른 인코딩 벡터는, 소정의 펄터베이션 벡터를 기초로 생성되는 잠재 벡터인 펄터베이션 잠 재 벡터와, 상기 펄터베이션 벡터의 원본 벡터인 임베딩 벡터를 기초로 생성되는 원본 잠재 벡터를 포함할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM)의 인코더 모듈과 연동하여 상기 생성 된 펄터베이션 벡터를 해당 태스크에 대응하는 잠재공간으로 인코더 네트워크를 통해 투영하여 잠재 벡터로 변 환할 수 있다. 또한, 실시예에서 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM)의 인코더 모듈과 연동하여 상기 획득된 임베딩 벡터를 해당 태스크에 대응하는 잠재공간으로 인코더 네트워크를 통해 투영하여 잠재 벡터로 변환할 수 있다. 그리하여 실시예에서 컴퓨팅 시스템은, 펄터베이션 잠재 벡터 및 원본 잠재 벡터를 획득할 수 있다. 이때, 실시예에서 컴퓨팅 시스템은, 각 태스크마다 상술된 기능 동작을 반복 수행하여 각 태스크별 대응 하는 원본 잠재 벡터 및 펄터베이션 잠재 벡터를 획득할 수 있다. 실시예로 컴퓨팅 시스템은, 태스크 't'에 대응하는 원본 잠재 벡터( : 이하, 제t 원본 잠재 벡터) 및 태스크 't'에 대응하는 펄터베이션 잠재 벡터( : 이하, 제t 펄터베이션 잠재 벡터)를 획득할 수 있다. 또한 컴퓨팅 시스템은, 태스크 's'에 대응하는 원본 잠재 벡터( : 이하, 제s 원본 잠재 벡터) 및 태스 크 's'에 대응하는 펄터베이션 잠재 벡터( : 이하, 제s 펄터베이션 잠재 벡터)를 획득할 수 있다. 또한, 실시예에서 컴퓨팅 시스템은, 4) 획득된 인코딩 벡터 기반의 전이 벡터를 획득할 수 있다. 여기서, 실시예에 따른 전이 벡터는, 소정의 펄터베이션 잠재 벡터를 기초로 생성되는 전이 벡터인 펄터베이션 트랜스퍼 벡터와, 상기 펄터베이션 잠재 벡터에 대응하는 원본 잠재 벡터를 기초로 생성되는 전이 벡터인 원본 트랜스퍼 벡터를 포함할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM)의 트랜스퍼 모듈(TFM)과 연동하여 상 기 획득된 펄터베이션 잠재 벡터 및 원본 잠재 벡터를 다른 태스크(실시예에서, 태스크 's' 또는 태스크 't')의잠재공간으로 트랜스퍼 네트워크를 통해 매핑하여 전이 벡터로 변환할 수 있다. 그리하여 컴퓨팅 시스템은, 펄터베이션 트랜스퍼 벡터 및 원본 트랜스퍼 벡터를 획득할 수 있다. 이때, 실시예에서 컴퓨팅 시스템은, 각 태스크마다 상술된 기능 동작을 반복 수행하여 각 태스크별 대응 하는 원본 트랜스퍼 벡터 및 펄터베이션 트랜스퍼 벡터를 획득할 수 있다. 실시예로 컴퓨팅 시스템은, 태스크 't'에 대응하는 원본 트랜스퍼 벡터( : 이하, 제t 원본 트랜스퍼 벡터) 및 태스크 't'에 대응하는 펄터베이션 트랜스퍼 벡터( : 이하, 제t 펄터베이션 트랜스퍼 벡터)를 획 득할 수 있다. 또한 컴퓨팅 시스템은, 태스크 's'에 대응하는 원본 트랜스퍼 벡터( : 이하, 제s 원본 트랜스퍼 벡 터) 및 태스크 's'에 대응하는 펄터베이션 트랜스퍼 벡터( : 이하, 제s 펄터베이션 트랜스퍼 벡터)를 획득 할 수 있다. 그리하여 실시예에서 컴퓨팅 시스템은, 실험 데이터 기반의 기하학적 정렬 벡터(즉, 임베딩 벡터, 펄터베 이션 벡터, 인코딩 벡터(원본 잠재 벡터 및 펄터베이션 잠재 벡터 포함) 및 전이 벡터(원본 트랜스퍼 벡터 및 펄터베이션 트랜스퍼 벡터 포함) 등)를 획득할 수 있다. 또한, 실시예에서 컴퓨팅 시스템은, 5) 획득된 전이 벡터 기반의 인버스 벡터를 획득할 수 있다. 여기서, 실시예에 따른 인버스 벡터는, 소정의 펄터베이션 트랜스퍼 벡터를 기초로 생성되는 인버스 벡터인 펄 터베이션 인버스 벡터와, 상기 펄터베이션 트랜스퍼 벡터에 대응하는 원본 트랜스퍼 벡터를 기초로 생성되는 인 버스 벡터인 원본 인버스 벡터를 포함할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM)의 인버스 모듈(ITM)과 연동하여 상기 획득된 펄터베이션 트랜스퍼 벡터 및 원본 트랜스퍼 벡터를 다시 원래의 잠재공간에 매핑되도록 인버스 네트워 크를 통해 재구성하여 인버스 벡터로 변환할 수 있다. 그리하여 컴퓨팅 시스템은, 펄터베이션 인버스 벡터 및 원본 인버스 벡터를 획득할 수 있다. 이때, 실시예에서 컴퓨팅 시스템은, 각 태스크마다 상술된 기능 동작을 반복 수행하여 각 태스크별 대응 하는 원본 인버스 벡터 및 펄터베이션 인버스 벡터를 획득할 수 있다. 실시예로 컴퓨팅 시스템은, 태스크 't'에 대응하는 원본 인버스 벡터( : 이하, 제t 원본 인버스 벡터) 및 태스크 't'에 대응하는 펄터베이션 인버스 벡터( : 이하, 제t 펄터베이션 인버스 벡터)를 획득할 수 있다. 또한 컴퓨팅 시스템은, 태스크 's'에 대응하는 원본 인버스 벡터( : 이하, 제s 원본 인버스 벡터) 및 태스크 's'에 대응하는 펄터베이션 인버스 벡터( : 이하, 제s 펄터베이션 인버스 벡터)를 획득할 수 있다. 그리하여 실시예에서 컴퓨팅 시스템은, 실험 데이터 기반의 기하학적 정렬 벡터(즉, 임베딩 벡터, 펄터베 이션 벡터, 인코딩 벡터(원본 잠재 벡터 및 펄터베이션 잠재 벡터 포함), 전이 벡터(원본 트랜스퍼 벡터 및 펄 터베이션 트랜스퍼 벡터 포함) 및 인버스 벡터(원본 인버스 벡터 및 펄터베이션 인버스 벡터 포함) 등)를 획득 할 수 있다. 또한, 실시예에서 컴퓨팅 시스템은, 획득된 기하학적 정렬 벡터 기반의 기하학적 정렬 로스를 계산할 수 있다. (S205) 여기서, 본 발명의 실시예에 따른 기하학적 정렬 로스(Geometric alignment loss)란, 멀티태스킹 학습 모델 (MtLM)을 통해 획득된 각종 벡터(즉, 기하학적 정렬 벡터)를 기반으로 산출된 다양한 손실함수(Loss)를 의미할 수 있다. 실시예에서 기하학적 정렬 로스는, 회귀 손실( : Regression loss), 오토인코더 손실( : Autoencoder loss), 일관성 손실( : Consistency loss), 매핑 손실( : Mapping loss), 거리 손실 ( : Distance loss) 및/또는 통합 손실( : Integrated loss) 등을 포함할 수 있다. 이하의 설명에서는, 효과적인 설명을 위하여 태스크 't'를 기준으로 기하학적 정렬 로스를 계산하는 것으로 설 명한다. 도 11 및 도 12는 본 발명의 일 실시예에 따른 회귀 손실 산출 방법을 설명하기 위한 도면의 예시들을 도시한다. 자세히, 도 10 내지 도 12를 참조하면, 실시예에서 컴퓨팅 시스템은, 기하학적 정렬 벡터를 획득한 멀티 태스킹 학습 모델(MtLM)을 기초로 1) 회귀 손실(Regression loss)을 산출할 수 있다. 보다 상세히, 실시예에서 컴퓨팅 시스템은, 하기 [수학식 1]에 따라서 리그레서 모듈(RGM)을 통해 예측된 예측값( ) 및 실제값( , 즉 레이블값)에 기반한 회귀 손실을 산출할 수 있다. 여기서, [수학식 1]의 예측값 은 ' '로 표현될 수도 있다. [수학식 1]"}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "즉 컴퓨팅 시스템은, 예측값과 실제값 간의 평균 제곱 오차(Mean Squared Error, MSE)를 계산하여 회귀 손실을 산출할 수 있다. 이때, 실시예에서 각각의 태스크들은, 각 태스크에 매칭되는 인코더 모듈(ECM)과 리그레서 모듈(RGM)에 기반한 독립적인 회귀 손실을 산출하고 그에 기반한 학습을 수행함으로써 상호 간섭을 방지할 수 있다. 이처럼 컴퓨팅 시스템은, 회귀 손실을 산출하여 모델의 회귀 성능을 용이하게 평가할 수 있다. 또한, 도 10을 더 참조하면, 실시예에서 컴퓨팅 시스템은, 기하학적 정렬 벡터를 획득한 멀티태스킹 학습 모델(MtLM)을 기초로 2) 오토인코더 손실(Autoencoder loss)을 산출할 수 있다. 자세해, 실시예에서 컴퓨팅 시스템은, 하기 [수학식 2]에 따라서 원본 잠재 벡터 및 원본 인버스 벡터에 기반한 오토인코더 손실을 산출할 수 있다. [수학식 2]"}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "즉 컴퓨팅 시스템은, 잠재 벡터와 인버스 벡터 간의 평균 제곱 오차(MSE)를 계산함으로써 오토인코더 손 실을 산출할 수 있다. 실시예에서 컴퓨팅 시스템은, 위와 같이 산출된 오토인코더 손실을 통해 데이터 전이 과정에서의 정확성 을 향상시킬 수 있다. 도 13은 본 발명의 일 실시예에 따른 통합 잠재공간(M) 매핑 방법을 설명하기 위한 도면의 예시를 도시한다. 한편, 도 13을 참조하면, 실시예에서 컴퓨팅 시스템은, 각각의 태스크에 대해 공통의 통합 잠재공간(M)으 로 매핑될 수 있도록 하는 양방향의 트랜스포메이션 매트릭스(TM: Transformation Matrix)를 학습할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 양측 태스크에 대한 레이블(Label)을 모두 보유하는 지식 데이터들 을 활용하여 태스크 간 잠재공간을 연결할 수 있다. 이 과정에서 컴퓨팅 시스템은, 실시예에 따른 일관성 손실 및 매핑 손실을 산출할 수 있다. 도 14 및 도 15는 본 발명의 일 실시예에 따른 일관성 손실 산출 방법을 설명하기 위한 도면의 예시들을 도시한 다. 보다 상세히, 도 10, 도 14 및 도 15를 참조하면, 실시예에서 컴퓨팅 시스템은, 기하학적 정렬 벡터를 획 득한 멀티태스킹 학습 모델(MtLM)을 기초로 3) 일관성 손실(Consistency loss)을 산출할 수 있다. 구체적으로, 실시예에서 컴퓨팅 시스템은, 하기 [수학식 3]에 따라서 태스크 't'의 펄터베이션 트랜스퍼 벡터 및 태스크 's'의 펄터베이션 트랜스퍼 벡터에 기반한 일관성 손실을 산출할 수 있다. [수학식 3]"}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "즉 컴퓨팅 시스템은, 제t 펄터베이션 트랜스퍼 벡터 및 제s 펄터베이션 트랜스퍼 벡터 간의 평균 제곱 오 차(MSE)를 계산함으로써 일관성 손실을 산출할 수 있다. 이때, 실시예에서 컴퓨팅 시스템은, 트랜스포메이션 매트릭스(TM)로부터 공간 상의 거리를 계산하기 위한 메트릭(Metric)을 도출하고, 도출된 메트릭을 기초로 각 태스크의 잠재공간에서 거리가 동일해지도록 학습할 수 있다. 이를 통해 컴퓨팅 시스템은, 태스크 간 기하학적 정렬을 보다 효과적으로 구현할 수 있다. 도 16 및 도 17은 본 발명의 일 실시예에 따른 매핑 손실 산출 방법을 설명하기 위한 도면의 예시들을 도시한다. 또한, 도 10, 도 16 및 도 17을 참조하면, 실시예에서 컴퓨팅 시스템은, 기하학적 정렬 벡터를 획득한 멀 티태스킹 학습 모델(MtLM)을 기초로 4) 매핑 손실(Mapping loss)을 산출할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 하기 [수학식 4]에 따라서 태스크 't'에 따른 실제값 및 태스크 's'에 따른 원본 인버스 벡터를 기초로 예측된 값에 기반한 매핑 손실을 산출할 수 있다. [수학식 4]"}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "즉 컴퓨팅 시스템은, 태스크 't'의 실제값 및 태스크 's'의 원본 인버스 벡터에 따른 예측값 간의 평균 제곱 오차(MSE)를 계산함으로써 매핑 손실을 산출할 수 있다. 실시예에서 컴퓨팅 시스템은, 위와 같은 매핑 손실을 산출함으로써 일측 태스크의 잠재공간에서 타측 태 스크의 잠재공간으로 잠재 벡터를 전이하고, 전이된 벡터를 기초로 타측 태스크를 수행하도록 하는 학습을 구현 해 잠재적 특성이 상호 유사해지도록 유도할 수 있다. 이를 통해 컴퓨팅 시스템은, 다른 태스크의 잠재공간으로 전이된 벡터의 예측 성능을 평가하고 이를 향상 시키는 방향으로 학습을 유도할 수 있다. 또한, 도 10을 더 참조하면, 실시예에서 컴퓨팅 시스템은, 기하학적 정렬 벡터를 획득한 멀티태스킹 학습 모델(MtLM)을 기초로 5) 거리 손실(Distance loss)을 산출할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 하기 [수학식 5] 및 [수학식 6]에 따라서 각 태스크의 원본 트랜스 퍼 벡터 및 펄터베이션 트랜스퍼 벡터 간 거리( : 이하, 트랜스퍼 벡터 변위)를 기초로 태스크 간의 거리 손 실을 산출할 수 있다. 보다 상세히, 실시예에서 컴퓨팅 시스템은, 하기 [수학식 5의 (a)]에 따라 태스크 't'에 따른 제t 원본 트랜스퍼 벡터 및 제t 펄터베이션 트랜스퍼 벡터 간 거리( : 이하, 제t 트랜스퍼 벡터 변위)를 계산할 수 있다. 또한, 컴퓨팅 시스템은, 하기 [수학식 5의 (b)]에 따라 태스크 's'에 따른 제s 원본 트랜스퍼 벡터 및 제 s 펄터베이션 트랜스퍼 벡터 간 거리( : 이하, 제s 트랜스퍼 벡터 변위)를 계산할 수 있다. [수학식 5]"}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "또한, 실시예에서 컴퓨팅 시스템은, 하기 [수학식 6]에 따라 제t 트랜스퍼 벡터 변위 및 제s 트랜스퍼 벡 터 변위 간 평균 제곱 오차(MSE)를 계산하여 거리 손실을 산출할 수 있다. [수학식 6]"}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, [수학식 6]의 'M'은 펄터베이션 포인트의 개수를 의미한다. 이때, 실시예에서 컴퓨팅 시스템은, 제t 트랜스퍼 벡터 변위 및 제s 트랜스퍼 벡터 변위 각각을 소스 태 스크와 타겟 태스크에서의 변위로 정의할 수 있다. 그리하여 컴퓨팅 시스템은, 제t 트랜스퍼 벡터 변위와 제s 트랜스퍼 벡터 변위를 평평한 유클리드 공간에 있는 것으로 해석하여 원본 트랜스퍼 벡터와 펄터베이션 트랜스퍼 벡터 사이의 거리를 보다 용이하게 계산할 수 있다. 따라서 컴퓨팅 시스템은, 모델의 잠재공간에 대한 일관성을 더욱 온전하게 유지하도록 지원할 수 있다. 도 18은 본 발명의 일 실시예에 따른 통합 손실 산출 방법을 설명하기 위한 도면의 예시를 도시한다. 또한, 도 10 및 도 18을 참조하면, 실시예에서 컴퓨팅 시스템은, 기하학적 정렬 벡터를 획득한 멀티태스 킹 학습 모델(MtLM)을 기초로 6) 통합 손실(Integrated loss)을 산출할 수 있다. 자세히, 실시예에서 컴퓨팅 시스템은, 하기 [수학식 7]에 따라서 상술된 회귀 손실, 오토인코더 손실, 일 관성 손실, 매핑 손실 및 거리 손실을 가중 합한 통합 손실을 산출할 수 있다. [수학식 7]"}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "이때, 실시예에서 컴퓨팅 시스템은, 각 손실함수를 모델의 특정 측면에 대하여 최적화할 수 있도록 손실 함수별 가중치를 적용할 수 있다. 여기서, [수학식 7]의 ' '는 오토인코더 손실의 가중치이고, ' '는 일관성 손실의 가중치이고, ' '는 매 핑 손실의 가중치이고, ' '는 거리손실의 가중치이다. 실시예에서 컴퓨팅 시스템은, 위와 같은 가중치들을 활용해 모델의 학습 과정에서 각 가중치에 대응하는 손실함수의 중요도를 조절하여 통합 손실을 최소화하는 방향으로 매개변수를 업데이트할 수 있다. 도 9로 돌아와서, 또한 실시예에서 컴퓨팅 시스템은, 전술된 바와 같이 계산된 기하학적 정렬 로스 기반 의 모델 최적화 및 매개변수 업데이트를 수행할 수 있다. (S207) 자세히, 실시예에서 컴퓨팅 시스템은, 상술된 통합 손실에 기초하여 멀티태스킹 학습 모델(MtLM)에 대한 최적화 및 매개변수 업데이트를 수행할 수 있다. 실시예로 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM)의 각 매개변수에 대하여 통합 손실에 기준한 그 래디언트(Gradient)를 역전파(Backpropagation)를 통해 계산할 수 있다. 그리고 컴퓨팅 시스템은, 계산된 그래디언트 및 기 설정된 최적화 알고리즘(예컨대, AdamW(Decoupled Weight Decay Regularization) 알고리즘 등)을 사용하여 멀티태스킹 학습 모델(MtLM)의 매개변수 업데이트를 수 행할 수 있다. 그리하여 컴퓨팅 시스템은, 기하학적 정렬 로스(특히, 통합 손실) 기반의 멀티태스킹 학습 모델(MtLM) 최 적화를 구현할 수 있다. 이와 같이 실시예에서 컴퓨팅 시스템은, 다각적으로 계산된 여러 손실함수의 조합을 통해 멀티태스킹 학 습 모델(MtLM) 최적화 및 매개변수 업데이트 학습을 수행할 수 있다. 이때, 각각의 손실함수는 지식 데이터 매핑의 정확성, 일관성 및/또는 거리 등을 보정하여 모델의 성능 향상을 용이하게 보조할 수 있다. 이를 통해 컴퓨팅 시스템은, 소규모 데이터 셋의 회귀 문제와 기존 전이학습 기법의 한계를 극복하는 개 선된 성능을 제공함과 동시에 보다 안정적으로 동작하며 향상된 일반화 성능을 제공하는 멀티태스킹 모델을 구 현할 수 있다. 또한, 실시예에서 컴퓨팅 시스템은, 멀티태스킹 학습 모델(MtLM) 트레이닝을 종료할 수 있다. (S209) 자세히, 실시예에서 컴퓨팅 시스템은, 기 설정된 트레이닝 종료 조건을 충족하면 상술된 멀티태스킹 학습 모델(MtLM) 트레이닝 프로세스를 종료할 수 있다. 실시예로 컴퓨팅 시스템은, 설정된 트레이닝 루프를 완료하면 멀티태스킹 학습 모델(MtLM) 트레이닝을 종 료할 수 있다. 도 8로 돌아와서, 또한 본 발명의 일 실시예에 따른 컴퓨팅 시스템은, 트레이닝된 멀티태스킹 학습 모델 (MtLM)을 제공할 수 있다. (S107) 즉, 실시예에서 컴퓨팅 시스템은, 상술된 바와 같이 트레이닝된 멀티태스킹 학습 모델(MtLM)을 소정의 방 식에 따라서 제공할 수 있다. 실시예로 컴퓨팅 시스템은, 소정의 애플리케이션 서비스(예컨대, 소재 합성/평가 서비스, 재료 물성 예측 서비스 및/또는 최적 물질 추천 서비스 등)와 연동하여 본 발명의 실시예에 따라 트레이닝된 멀티태스킹 학습 모델(MtLM)을 제공할 수 있다. 그리하여 컴퓨팅 시스템은, 개선된 성능의 멀티태스킹 학습 모델(MtLM)을 이용한 각종 멀티태스킹 태스크 처리를 효과적으로 지원할 수 있다. 이처럼, 실시예에서 컴퓨팅 시스템은, 복수의 도메인에 따른 출력을 위한 다중 태스크를 처리하기 위해 각 태스크별 잠재공간의 지식 데이터를 하나의 통합된 잠재공간에서의 기하학적 정렬을 통해 상호 전이 및 학습 시킴으로써, 소규모 데이터 셋의 회귀 문제와 기존 전이학습 기법의 한계를 극복하는 개선된 성능을 제공함과 동시에 보다 안정적으로 동작하는 멀티태스킹 학습 모델(MtLM)을 제공할 수 있다. 이를 통해 컴퓨팅 시스템은, 주어진 데이터 양이 적거나, 다양한 태스크 유형을 포함하거나, 회귀 문제를 주로 다루는 등의 상황에서도 높은 일반화 성능을 보이며 안정적으로 강건하게 동작하는 전이학습 기반의 멀티 태스킹 모델을 제공할 수 있다. 다시 말해 컴퓨팅 시스템은, 복수의 도메인(실시예에서, 물성) 중 실험 데이터(학습 데이터)가 부족한 도 메인이 존재하더라도, 타 도메인과 연계되어 수행되는 기하학적 정렬 기반의 전이학습을 통해 증류된 지식을 토 대로 그 예측 성능이 향상된 멀티태스킹 학습 모델(MtLM)을 제공할 수 있다. 예를 들어 컴퓨팅 시스템은, 복수의 분자구조식 각각에 대한 제1 내지 제10 물성을 기초로 멀티태스킹 학 습 모델(MtLM)을 사전학습한 후, 제1 내지 제5 물성에 대한 데이터만을 포함하는 제1 분자구조식을 입력받으면, 제1 분자구조식에 대한 나머지 제6 내지 제10 물성에 대한 각각의 예측 값을 사전학습을 통해 전이 및 증류된 지식 데이터를 토대로 보다 정확히 예측하고 그에 기반한 출력 데이터를 생성해 제공할 수 있다. 이와 같이, 본 발명의 실시예에 따른 컴퓨팅 시스템은, 기하학적 정렬 기반의 효과적인 전이학습을 구현 하고, 높은 일반화 성능을 보장하며, 회귀 문제에 대한 예측 정확도를 제고하고, 다양한 손실함수의 결합에 따 른 정규화를 지원하며, 안정적인 학습 프로세스를 수행해 강건한 성능을 보장하는 멀티태스킹 모델을 제공할 수 있다. 이상, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 전이학습을 통해 소스 태스크에서 학습된 지식을 타겟 태스크로 전이하여 데이터 부족 문제를 해소함으로써, 소규모 데이터 셋에서도 높은 성능을 유지하는 멀티태스킹 모델을 제공할 수 있는 효과가 있다. 따라서 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 데이터나 도메인 지식이 부족해 머신러닝 모델을 적용하기 어려웠던 분야까지 그 활용 범위를 확장시킬 수 있는 효과가 있다. 또한, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 회귀 문제에 효과적으로 적용할 수 있는 특화된 전이학습 기법을 제공함으로써, 분자 데이터 셋과 같은 복잡한 회귀 문제에서도 높은 예측 성능을 발휘할 수 있는 효과가 있다. 또한, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 리만 기하학적 접근을 통해 소스 태스크와 타겟 태스크 간의 지식 전달을 최적화함으 로써, 태스크 간의 지오메트리적 일관성을 유지하여 전이학습의 효율성을 향상시킬 수 있는 효과가 있다. 또한, 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 다수의 손실함수를 결합하여 모델의 다양한 측면을 정규화함으로써, 모델의 일반화 성 능을 더욱 제고시킬 수 있는 효과가 있다. 따라서 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법 및 이를 기초로 학습된 머신러닝 모델을 이용한 멀티태스킹 수행 방법은, 다양한 물질(소재)에 대해 범용적으로 활용 가능한 멀티태스킹 모델을 제공해 관련 산 업 전반의 품질을 증진시킬 수 있는 효과가 있다. 한편, 이상에서 설명된 본 발명에 따른 실시예는 다양한 컴퓨터 구성요소를 통하여 실행될 수 있는 프로그램 명 령어의 형태로 구현되어 컴퓨터 판독 가능한 기록 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능한 기록 매체 는 프로그램 명령어, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터 판 독 가능한 기록 매체에 기록되는 프로그램 명령어는 본 발명을 위하여 특별히 설계되고 구성된 것이거나 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수 있다. 컴퓨터 판독 가능한 기록 매체의 예에는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등과 같 은, 프로그램 명령어를 저장하고 실행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령어의 예에 는, 컴파일러에 의하여 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용하여 컴퓨터에 의해 서 실행될 수 있는 고급 언어 코드도 포함된다. 하드웨어 장치는 본 발명에 따른 처리를 수행하기 위하여 하나 이상의 소프트웨어 모듈로 변경될 수 있으며, 그 역도 마찬가지이다. 본 발명에서 설명하는 특정 실행들은 일 실시 예들로서, 어떠한 방법으로도 본 발명의 범위를 한정하는 것은 아 니다. 명세서의 간결함을 위하여, 종래 전자적인 구성들, 제어 시스템들, 소프트웨어, 상기 시스템들의 다른 기 능적인 측면들의 기재는 생략될 수 있다. 또한, 도면에 도시된 구성 요소들 간의 선들의 연결 또는 연결 부재들 은 기능적인 연결 및/또는 물리적 또는 회로적 연결들을 예시적으로 나타낸 것으로서, 실제 장치에서는 대체 가 능하거나 추가의 다양한 기능적인 연결, 물리적인 연결, 또는 회로 연결들로서 나타내어질 수 있다. 또한, “필 수적인”, “중요하게” 등과 같이 구체적인 언급이 없다면 본 발명의 적용을 위하여 반드시 필요한 구성 요소 가 아닐 수 있다. 또한 설명한 본 발명의 상세한 설명에서는 본 발명의 바람직한 실시 예를 참조하여 설명하였지만, 해당 기술 분"}
{"patent_id": "10-2024-0082137", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "야의 숙련된 당업자 또는 해당 기술분야에 통상의 지식을 갖는 자라면 후술할 특허청구범위에 기재된 본 발명의 사상 및 기술 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다. 따라서, 본 발명의 기술적 범위는 명세서의 상세한 설명에 기재된 내용으로 한정되는 것이 아니라 특허청구범위에 의해 정하여져야만 할 것이다."}
{"patent_id": "10-2024-0082137", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델 제공 서비스를 구현하는 컴퓨팅 시스템의 블록도의 예시를 도시한다. 도 2는 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델 제공 서비스를 구현하는 컴퓨팅 디바이스의 블록도의 예시를 도시한다. 도 3은 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델 제공 서비스를 구현하는 컴퓨팅 디바이스에 대한 다 른 측면에서의 블록도의 예시를 도시한다. 도 4 및 도 5는 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델을 설명하기 위한 개념도의 예시들을 도시한 다. 도 6은 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델의 내부 블록도를 도시한다. 도 7은 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법을 설명하기 위한 개념도의 예시를 도시한다. 도 8은 본 발명의 일 실시예에 따른 멀티태스킹 모델 학습 방법을 설명하기 위한 블록 흐름도를 도시한다. 도 9는 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델 트레이닝 방법을 설명하기 위한 블록 흐름도를 도시 한다. 도 10은 본 발명의 일 실시예에 따른 멀티태스킹 학습 모델 트레이닝 방법을 설명하기 위한 개념도의 예시를 도 시한다. 도 11 및 도 12는 본 발명의 일 실시예에 따른 회귀 손실 산출 방법을 설명하기 위한 도면의 예시들을 도시한다. 도 13은 본 발명의 일 실시예에 따른 통합 잠재공간 매핑 방법을 설명하기 위한 도면의 예시를 도시한다. 도 14 및 도 15는 본 발명의 일 실시예에 따른 일관성 손실 산출 방법을 설명하기 위한 도면의 예시들을 도시한 다. 도 16 및 도 17은 본 발명의 일 실시예에 따른 매핑 손실 산출 방법을 설명하기 위한 도면의 예시들을 도시한다. 도 18은 본 발명의 일 실시예에 따른 통합 손실 산출 방법을 설명하기 위한 도면의 예시를 도시한다."}
