{"patent_id": "10-2021-0192818", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0102587", "출원번호": "10-2021-0192818", "발명의 명칭": "인공신경망 연산 방법 및 이를 이용한 인공신경망 연산 장치", "출원인": "국민대학교산학협력단", "발명자": "장영민"}}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 은닉층을 포함하는 인공신경망을 연산하는 방법으로서, 은닉층의 노드의 입력 데이터 및 가중치 데이터에 기반하여 상기 노드의 연산값을 연산하는 단계;상기 연산값에 기반하여 활성화 함수를 선택하는 단계; 및상기 연산값에 대하여 선택된 상기 활성화 함수의 함수값을 다음 은닉층에 대한 출력값으로 출력하는 단계를 포함하고,상기 활성화 함수를 선택하는 단계는,상기 연산값의 포지티브(positive) 영역은 선형 함수에 기반한 제1 활성화 함수를 선택하고, 상기 연산값의 네거티브(negative) 영역은 비선형 함수에 기반한 제2 활성화 함수를 선택하는 단계를 포함하는,인공신경망 연산 방법."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 연산하는 단계는,상기 입력 데이터 및 상기 가중치 데이터에 기반한 가중합을 연산하는 단계; 및상기 노드의 바이어스 값과 상기 가중합에 기반하여 상기 연산값을 결정하는 단계를 포함하는,인공신경망 연산 방법."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 포지티브 영역은 상기 연산값이 0 이상인 영역이고, 상기 네거티브 영역은 상기 연산값이 0보다 작은 영역인,인공신경망 연산 방법."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 활성화 함수는, (여기서, 으로 정의함)이고,상기 제1 활성화 함수는 이고,상기 제2 활성화 함수는 인,공개특허 10-2023-0102587-3-인공신경망 연산 방법."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 제1 활성화 함수의 제1 도함수는 이고, 상기 제2 활성화 함수의 제2 도함수는 인, 인공신경망 연산 방법."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서, 상기 활성화 함수에서 는 0.5인,인공신경망 연산 방법."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4항에 있어서, 상기 활성화 함수에서 는 상기 인공신경망의 훈련 단계에서 최적 값으로 선택되는 하이퍼 파라미터인,인공신경망 연산 방법."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제4항에 있어서, 상기 연산하는 단계 이전에,미리 수집된 훈련 데이터를 이용하여 상기 인공신경망을 훈련하는 단계를 포함하고,상기 훈련하는 단계는, 훈련 시작 시점에 상기 훈련 데이터의 타입 및 크기에 따라 상기 활성화 함수에서 의최초 설정값을 결정하는,인공신경망 연산 방법."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제4항에 있어서, 상기 연산하는 단계 이전에,미리 수집된 훈련 데이터를 이용하여 상기 인공신경망을 훈련하는 단계를 포함하고,상기 훈련하는 단계는,공개특허 10-2023-0102587-4- 값을 최소 설정값으로 설정하는 단계;상기 미리 수집된 훈련 데이터 내 제1 그룹의 데이터를 이용하여 상기 인공신경망을 훈련시키고, 훈련된 인공신경망에 대해 상기 미리 수집된 훈련 데이터 중 상기 제1 그룹의 데이터 이외의 데이터를 이용하여 훈련된 인공신경망의 정확도를 측정하고 기록하는 단계;상기 값을 제1 크기만큼 증가시키면서 상기 인공신경망의 정확도를 측정하고 기록하는 단계를 상기 값이 1이상이 될 때까지 반복하는 단계; 및상기 반복하는 단계를 통해 획득되는 정확도 중 정확도가 가장 높게 측정된 경우의 값을 상기 활성화 함수의최적 값으로 결정하고, 상기 미리 수집된 훈련 데이터를 이용하여 상기 인공신경망을 훈련시키는 단계를 포함하는,인공신경망 연산 방법."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 제1 크기는 직전 단계에서 훈련된 인공신경망의 정확도에 따라 결정되고, 직전 단계에서 훈련된 인공신경망의 정확도가 작을수록 상기 제1 크기는 크게 결정되는,인공신경망 연산 방법."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "적어도 하나의 은닉층을 포함하는 인공신경망 연산 장치로서,적어도 하나의 프로세서; 및상기 프로세서와 동작 가능하게 연결되고, 상기 프로세서에서 수행되는 적어도 하나의 코드를 저장하는 메모리를 포함하고,상기 메모리는 상기 프로세서를 통해 실행될 때, 상기 프로세서로 하여금,은닉층의 노드의 입력 데이터 및 가중치 데이터에 기반하여 상기 노드의 연산값을 연산하고, 상기 연산값에 기반하여 활성화 함수를 선택하고, 상기 연산값에 대하여 선택된 상기 활성화 함수의 함수값을 다음 은닉층에 대한 출력값으로 출력하며, 상기 활성화 함수 선택 시, 상기 연산값의 포지티브(positive) 영역은 선형 함수에 기반한 제1 활성화 함수를 선택하고, 상기 연산값의 네거티브(negative) 영역은 비선형 함수에 기반한 제2 활성화함수를 선택하기 위한 코드를 저장하는, 인공신경망 연산 장치."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 메모리는 상기 프로세서를 통해 실행될 때, 상기 프로세서로 하여금,상기 입력 데이터 및 상기 가중치 데이터에 기반한 가중합을 연산하고, 상기 노드의 바이어스값과 상기 가중합에 기반하여 상기 연산값을 결정하도록 하는 코드를 저장하는, 인공신경망 연산 장치. 공개특허 10-2023-0102587-5-청구항 13 제11항에 있어서, 상기 포지티브 영역은 상기 연산값이 0 이상인 영역이고, 상기 네거티브 영역은 상기 연산값이 0보다 작은 영역인, 인공신경망 연산 장치."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서, 상기 활성화 함수는(여기서, 으로 정의함)이고,상기 제1 활성화 함수는 이고,상기 제2 활성화 함수는 인 코드를 저장하는, 인공신경망 연산 장치."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 상기 제1 활성화 함수의 제1 도함수는 이고, 상기 제2 활성화 함수의 제2 도함수는 인, 인공신경망 연산 장치."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서, 상기 활성화 함수에서 는 0.5인,인공신경망 연산 장치."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제14항에 있어서, 상기 활성화 함수에서 는 상기 인공신경망의 훈련 단계에서 최적 값으로 선택되는 하이퍼 파라미터인,인공신경망 연산 장치. 공개특허 10-2023-0102587-6-청구항 18 제14항에 있어서, 상기 메모리는 상기 프로세서를 통해 실행될 때, 상기 프로세서로 하여금,미리 수집된 훈련 데이터를 이용하여 상기 인공신경망을 훈련 단계를 수행시키되,훈련 시작 시점에 상기 훈련 데이터의 타입 및 크기에 따라 상기 활성화 함수에서 의 최초 설정값을 결정하도록 하는 코드를 저장하는,인공신경망 연산 장치."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제14항에 있어서, 상기 메모리는 상기 프로세서를 통해 실행될 때, 상기 프로세서로 하여금,미리 수집된 훈련 데이터를 이용하여 상기 인공신경망에 대한 훈련 단계를 수행시키되,상기 훈련 단계에서, 값을 최소 설정값으로 설정하고, 상기 미리 수집된 훈련 데이터 내 제1 그룹의 데이터를 이용하여 상기 인공신경망을 훈련시키고, 훈련된 인공신경망에 대해 상기 미리 수집된 훈련 데이터 중 상기 제1 그룹의 데이터 이외의 데이터를 이용하여 훈련된 인공신경망의 정확도를 측정하고 기록하고, 상기 값을 제1 크기만큼 증가시키면서 상기 인공신경망의 정확도를 측정하고 기록하는 단계를 상기 값이 1이상이 될 때까지 반복하고, 상기 반복하는 단계를 통해 획득되는 정확도 중 정확도가 가장 높게 측정된 경우의 값을 상기 활성화 함수의 최적 값으로 결정하고, 상기 미리 수집된 훈련 데이터를 이용하여 상기 최적 값의 활성화 함수를 가지는 인공신경망을 훈련시키도록하는 코드를 포함하는,인공신경망 연산 장치."}
{"patent_id": "10-2021-0192818", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서, 상기 제1 크기는 직전 단계에서 훈련된 인공신경망의 정확도에 따라 결정되고, 직전 단계에서 훈련된 인공신경망의 정확도가 작을수록 상기 제1 크기는 크게 결정되는,인공신경망 연산 장치."}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시 예에 따른 인공신경망 연산 방법 및 이를 이용한 인공신경망 연산 장치에 따르면 딥러닝 알고 리즘에 영향을 끼치는 활성화 함수를 설계하여 Dying ReLU 문제와 계산 복잡성 문제를 동시에 해결할 수 있게 된 다."}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공신경망 연산 방법 및 이를 이용한 인공신경망 연산 장치에 관한 것으로 보다 상세하게는 딥러닝 알고리즘에 영향을 끼치는 활성화 함수를 설계하기 위한 인공신경망 연산 방법 및 이를 이용한 인공신경망 연산 장치에 관한 것이다."}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이하에서 기술되는 내용은 본 발명의 실시 예와 관련되는 배경 정보를 제공할 목적으로 기재된 것일 뿐이고, 기 술되는 내용들이 당연하게 종래기술을 구성하는 것은 아니다. 최근 다양한 분야에서 인공지능이 주목 받고 있다. 특히, 과적합 문제를 해결하고, 하드웨어의 발전과 빅데이터 의 확보가 가능해지면서 방대한 양의 데이터를 기반으로 스스로 학습하고 패턴을 찾는 딥 러닝(Deep Learning) 알고리즘이 주목 받고 있고, 이에 대한 많은 연구가 진행되고 있다. 딥러닝은 인공신경망(Neural Network)을 학습시켜 최적화하는 과정으로, 이러한 인공신경망은 사람의 뇌를 구성 하는 뉴런의 동작원리에 기초한다. 뉴런은 입력신호를 받고 연결된 다음 뉴런으로 신호를 전달하는 과정에서, 신호의 강도가 약해져서 신호가 다음 뉴런으로 전달되지 않거나 또는 의도와 다르게 신호가 강하게 전달되기도 한다. 이러한 강도는 입력값에 가중치의 곱과 편향의 합이 활성화 함수를 통과함에 의해 결정된다. 즉, 신경망 활성화 함수는 딥 러닝 모델의 출력, 정확성 및 모델 학습의 계산 효율성을 결정한다. 이러한 활성 화 함수의 수렴 능력과 속도 등에 의해 인공신경망을 생성하거나 생성된 인공신경망을 소멸하도록 할 수 있다."}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "이와 관련하여 한국공개특허 제10-2020-0097103호에서는 딥러닝 알고리즘에 영향을 끼치는 활성화 함수를 실행 하는 기술에 대해 개시하고 있다. 이와 같이 개시된 활성화 함수는 인공 신경망의 출력을 결정하는 매우 중요한 역할을 함에도 불구하고 포지티브 (positive) 영역에서 기울기가 소실되는 문제, 네거티브(negative) 영역에서의 소실되는 문제 등을 해결할 수 있는 활성화 함수가 필요한 실정이다. 한편, 전술한 선행기술은 발명자가 본 발명의 도출을 위해 보유하고 있었거나, 본 발명의 도출 과정에서 습득한 기술 정보로서, 반드시 본 발명의 출원 전에 일반 공중에게 공개된 공지기술이라 할 수는 없다."}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 과제는, 인공지능 딥러닝 모델의 포지티브(positive) 영역에서 기울기가 소실되거나 네거티브 (negative) 영역에서의 소실되는 것을 방지할 수 있도록 하는 것이다. 본 발명의 다른 과제는, 은닉층의 노드의 입력 데이터 및 가중치 데이터에 기반하여 연산된 노드의 연산값의 포 지티브 영역은 선형 함수에 기반하고, 연산값의 네거티브 영역은 비선형 함수에 기반한 활성화 함수를 선택하도 록 하여 활성화 함수 계산 복잡성을 최소화할 수 있도록 하는 것이다. 본 발명의 목적은 이상에서 언급한 과제에 한정되지 않으며, 언급되지 않은 본 발명의 다른 목적 및 장점들은 하기의 설명에 의해서 이해될 수 있고, 본 발명의 실시 예에 의해 보다 분명하게 이해될 것이다. 또한, 본 발명 의 목적 및 장점들은 청구범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 알 수 있을 것이다."}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시 예에 따른 적어도 하나의 은닉층을 포함하는 인공신경망을 연산하는 방법은, 은닉층의 노드 의 입력 데이터 및 가중치 데이터에 기반하여 상기 노드의 연산값을 연산하고, 상기 연산값에 기반하여 활성화 함수를 선택한 후, 상기 연산값에 대하여 선택된 상기 활성화 함수의 함수값을 다음 은닉층에 대한 출력값으로 출력하는 과정을 통해 구현될 수 있다. 이때 상기 활성화 함수를 선택 시, 상기 연산값의 포지티브(positive) 영역은 선형 함수에 기반한 제1 활성화 함수를 선택하고, 상기 연산값의 네거티브(negative) 영역은 비선형 함수에 기반한 제2 활성화 함수를 선택할 수 있다. 한편, 연산하는 과정에서, 상기 입력 데이터 및 상기 가중치 데이터에 기반한 가중합을 연산하고, 상기 노드의 바이어스 값과 상기 가중합에 기반하여 상기 연산값을 결정할 수 있다. 특히, 상기 포지티브 영역은 상기 연산값이 0보다 큰 영역이고, 상기 네거티브 영역은 상기 연산값이 0보다 작 거나 0인 영역일 수 있다. 한편, 활성화 함수를 선택할 때, 상기 제1 활성화 함수 및 상기 제2 활성화 함수는 (여기서, 으로 정의함)일 수 있다. 여기서, 이때, 상기 제1 활성화 함수는 이고, 상기 제2 활성화 함수는 으로 정의될 수 있다. 여기서, 상기 제1 활성화 함수의 제1 도함수는 이고, 상기 제2 활성화 함수의 제2 도함수는 으로 정의할 수 있다. 한편, 본 발명의 실시예에 따라 적어도 하나의 은닉층을 포함하는 인공신경망을 연산하는 장치는, 적어도 하나 의 프로세서 및 상기 프로세서와 동작 가능하게 연결되고, 상기 프로세서에서 수행되는 적어도 하나의 코드를 저장하는 메모리를 포함하여 구성될 수 있다. 상기 메모리가 상기 프로세서를 통해 실행될 때, 상기 프로세서로 하여금, 은닉층의 노드의 입력 데이터 및 가 중치 데이터에 기반하여 상기 노드의 연산값을 연산하고, 상기 연산값에 기반하여 활성화 함수를 선택하면, 상 기 연산값에 대하여 선택된 상기 활성화 함수의 함수값을 다음 은닉층에 대한 출력값으로 출력하며, 상기 활성 화 함수 선택 시, 상기 연산값의 포지티브(positive) 영역은 선형 함수에 기반한 제1 활성화 함수를 선택하고, 상기 연산값의 네거티브(negative) 영역은 비선형 함수에 기반한 제2 활성화 함수를 선택하기 위한 코드를 저장 할 수 있다. 전술한 것 외의 다른 측면, 특징, 및 이점이 이하의 도면, 청구범위 및 발명의 상세한 설명으로부터 명확해질 것이다."}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시 예는 인공지능 학습을 위한 활성화 함수를 선택함에 있어서, 연산값의 포지티브(양수) 영역은 선형 함수에 기반한 제1 활성화 함수를 선택할 수 있도록 하며, 연산값의 네거티브(음수) 영역은 비 선형 함수 에 기반한 제2 활성화 함수를 선택할 수 있도록 한다. 이로 인해, 포지티브 영역에서 활성화 함수를 미분하는 경우 역전파 값이 소실되어 학습이 진행할 수 없는 Dying ReLU 발생을 방지할 수 있도록 한다. 구체적으로, Dying ReLU는 다수의 ReLU 뉴런이 0의 값만 출력하는 것을 의미할 수 있다. 이는 입력값이 음(-)의 범위에 있을 때 발생할 수 있는 현상이다. 이러한 특성은 입력값이 음의 범위에서 발생할 때 전체 네트워크가 중단될 수 있다는 한계가 있다. 이 경우, 뉴 런의 출력값이 0이 되므로 역전파의 기울기는 0이 될 수 있다. 궁극적으로 네트워크가 대부분 비활성화될 수 있 으며, 이로 인해 학습 진행이 어려워질 수 있다. 따라서, 연산값의 네거티브 영역을 비 선형 함수에 기반한 제2 활성화 함수로 선택할 수 있도록 하여 입력값이 음의 범위에 있을 때 출력값이 0이 되는 것을 방지할 수 있도록 한다. 또한, 포지티브 영역에서 선형 함수인 제1 활성화 함수를 사용함으로써, 연산 복잡도를 낮춰 프로그램의 구현이 보다 쉬워질 수 있다. 즉, 은닉층의 노드의 입력 데이터 및 가중치 데이터에 기반하여 연산된 노드의 연산값의 포지티브 영역은 선형 함수에 기반하고, 연산값의 네거티브 영역은 비선형 함수에 기반한 활성화 함수를 선택하도록 하여 활성화 함수 계산 복잡성을 최소화할 수 있도록 한다."}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 이상에서 언급된 것들에 한정되지 않으며, 언급되지 아니한 다른 효과들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 도면을 참조하여 본 발명을 보다 상세하게 설명한다. 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며, 여기에서 설명하는 실시 예들에 한정되지 않는다. 이하 실시 예에서는 본 발명을 명확하게 설명하기 위 해서 설명과 직접적인 관계가 없는 부분을 생략하지만, 본 발명의 사상이 적용된 장치 또는 시스템을 구현함에 있어서, 이와 같이 생략된 구성이 불필요함을 의미하는 것은 아니다. 아울러, 명세서 전체를 통하여 동일 또는 유사한 구성요소에 대해서는 동일한 참조번호를 사용한다. 이하의 설명에서 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의해 한정되어서는 안 되며, 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목 적으로만 사용된다. 또한, 이하의 설명에서 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표 현을 포함한다. 이하의 설명에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 특징, 숫자, 단계, 동작, 구성요 소, 부분품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 이하 도면을 참고하여 본 발명을 상세히 설명하기로 한다. 도 1은 본 발명의 실시예에 따른 활성화 함수가 실행되는 인공신경망의 구성을 나타낸 개념도이다. 도 1을 참조하면, 본 발명의 일 실시예에 따른 활성화 함수가 실행되는 인공신경망은 입력층(input layer), 은 닉층(hidden layer) 및 출력층(output layer)을 포함한다. 기본적으로 은닉층은 매우 많은 개수의 노드로 구성될 수 있다. 도 1의 실시예에서는 대표적인 인공신경망 구조 인 심층 신경망을 예로 설명하고 있으나, 반드시 이에 한정될 필요는 없다.심층신경망을 학습시키는 방법으로는, 실선으로 표시된 피드 포워드(feed-forward)와 점선으로 표시된 역전파 (back propagation) 방법이 사용될 수 있는데, 피드 포워드 과정에서는 입력층에서 은닉층, 그리고 출력층 순으 로 순차적으로 학습이 진행된다. 각 층의 노드 값은 이전 층의 노드 값과 연결된 가중치의 곱을 모두 더한 뒤, 활성화 함수에 대응하여 나온 값이 될 수 있다. 그리고, 출력층에서 은닉층, 그리고 입력층 순서로 활성화 함수 의 미분을 통해 오류를 역전파함으로써 가중치를 최적화할 수 있다. 활성화 함수는 피드 포워드와 역전파 과정 에 직접적으로 관여함으로써 학습속도 및 성능에 큰 영향을 미친다. 도 2a는 시그모이드 함수(Sigmoid Function)의 도함수를 나타낸 도면이고, 도 2b는 ReLU함수(Rectified Linear Unit Function)의 도함수를 나타낸 도면이며, 도 2c는 Swish 함수의 도함수를 나타낸 도면이다. 우선 시그모이드 함수는 다음의 수학식으로 표현된다. [수학식 1]"}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "시그모이드 함수는, 0~1 사이값으로 로지스틱 회귀공식(logistic regression) 함수이며 간단한 분류문제에 사용 할 수 있다는 특징이 있다. 구체적으로 로지스틱 회귀공식(Logistic Regression)은 예측 분석을 위한 회귀분석 중에서 특히 종속 변수가 이분형일 때 수행할 수 있는 회귀 분석 기법의 한 종류로, 하나의 종속 이진 변수와 하나 이상의 숫자형, 명목형, 순서형의 독립 변수 간의 관계를 정량적으로 설명하는 데 사용되는 통계기법이다. 이러한 로지스틱 회귀 분석을 통해 하나의 종속 변수와 여러 독립 변수 간의 다변수 회귀 관계 를 조사할 수 있다. 로지스틱 회귀공식은 종속 변수와 독립 변수 사이의 관계가 비선형 관계로 식별되기 때문에 정규 분포의 가정이 독립 변수에 적용되지 않고, 명목형, 연속형, 순서형을 비롯한 다양한 독립 변수에 다양한 자료 유형을 사용할 수 있기 때문에 복잡한 현상을 설명할 수 있다는 특성이 있다. 로지스틱 회귀공식을 포함하는 시그모이드 함수는 미분 가능하고, 모든 입력값에서 음이 아닌 미분값을 가진다 는 특징이 있다. 즉, 미분된 시그모이드 함수는 도 2a와 같이 입력값이 0에 근접할수록 소정의 값을 가지고 입 력값이 0에서 멀어지는 큰 음수 및 큰 양수인 경우 출력값이 0과 근접한 값을 가진다. 즉, 시그모이드 함수는 중심 '0' 에서 멀어질수록 미분값이 작아져 역전파시 값이 소실된다는 점과, sigmoid로 인해 모든 입력값(x)이 +가 되어 모든 해가 '+' or '-' 의 같은 방향을 가져야 하는데, 만약 해가 다른 방향에 있을 경우 비효율적인 탐색(예: 지그재그 탐색)이 발생한다는 한계가 있다. 또한, 시그모이드 함수는 딥러닝 학 습에 있어서 비용이 높다는 한계가 있다. 한편, ReLU 함수는 다음의 수학식으로 표현된다. [수학식 2]"}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "ReLU 함수는 앞서 설명된 시그모이드 함수의 기울기 소실(Vanishing Gradient) 문제를 해결한 함수이다. 도 2b 에 도시된 바와 같이 ReLU 함수의 미분값은 1 또는 0만 존재하여 기울기 소실 문제를 해결하면서도 시그모이드 함수보다 미분 속도가 6배나 빠르다는 특징이 있다. 즉, ReLU 함수는 학습속도가 빠르고, 역전파일 때 기울기 소실 문제를 해결할 수 있다는 장점이 있다. 그러나, ReLU 함수는 0 이하값에 대해서는 모두 0이므로 이 경우 학습이 이루어지지 않는다는 문제가 있다. 즉, ReLU 함수는 대부분의 입력 값이 음수일 경우, 미분 값이 0으로 기울기에 의한 역전파 학습을 진행할 수 없는 Dying ReLU 문제가 발생한다. 한편, Swish 함수는 다음의 수학식으로 표현된다. [수학식 3]"}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 4, "content": "도 2c를 참고하면 Swish 함수의 미분값은 ReLU 함수와 달리 음수 부분에서 기울기가 0으로 되지 않는 특징이 있 다. 구체적으로 self-gated activation function discovered by researchers at Google에 따르면, 음수 쪽으로는 경계가 있고 양수 쪽으로는 제한이 없다는 특징과, 비 단조함수로써 음수 값이 0이 되지 않는다는 특 징이 있지만, 계산 복잡성이 높다는 한계가 있다. 설명된 각 함수의 한계를 해결하기 위해 본 발명의 실시예의 활성화 함수는 연산값인 입력값의 양수 영역인 포 지티브 영역은 선형 함수에 기반한 제1 활성화 함수로 선택하도록 하고, 음수 영역인 네거티브 영역은 비 선형 함수에 기반한 제2 활성화 함수로 선택할 수 있도록 한다. 이하 도면을 참고하여 연산값의 포지티브 영역에서는 제1 활성화 함수를 선택하고, 연산값의 네거티브 영역에서 는 제2 활성화 함수를 선택하는 특징을 아래에서 자세히 설명하기로 한다. 도 3은 본 발명의 실시예에 따라 실행되는 활성화 함수를 도시한 도면이고, 도 4는 본 발명의 실시예에 따라 실 행되는 활성화 함수의 도함수를 도시한 도면이며, 도 5는 본 발명의 실시예에 따라 실행되는 활성화 함수의 이 차 도함수를 도시한 도면이다. 앞서 설명한 도 2b 내지 도 2c에서 발생되는 Dying ReLU 문제와 계산 복잡성이 높다는 문제를 동시에 해결하기 위해 본 발명의 실시예에 따른 인공신경망을 연산하는 방법은, 은닉층 노드 입력 데이터 및 가중치 데이터에 기 반하여 연산된 연산값인 입력값이 포지티브(positive) 영역에서는 제1 활성화 함수를 선택하도록 하고, 네거티 브(negative) 영역 양수 영역에서는 제2 활성화 함수를 선택하도록 한다. 앞서 설명한 바와 같이 ReLU 함수는 앞서 설명된 시그모이드 함수의 기울기 소실(Vanishing Gradient) 문제를 해결한 함수이지만, ReLU 함수는 0 이하값에 대해서는 모두 0이므로 이 경우 학습이 이루어지지 않는 Dying ReLU이 발생한다. 즉, Dying ReLU는 다수의 ReLU 뉴런이 0의 값만 출력하는 것을 의미할 수 있다. 입력값이 네거티브인 음(-)의 범위에 있을 때 발생할 수 있는 현상이다. 설명한 바와 같이 ReLU 함수는 학습 속도가 높다는 특징이 있지만, ReLU 함수는 입력값이 네거티브일 때 출력값 이 0이므로 역전파의 기울기가 0이 될 수 있다. 이로 인해 궁극적으로 네트워크의 네거티브 영역이 비활성화 되 어 학습이 이루어지지 않는다는 한계가 있다. 이를 위해 네거티브 영역의 활성화 함수는 네거티브 영역에서도 출력값이 0이 되지 않는 비 선형 함수에 기반한 선형 제2 활성화 함수로 구성할 수 있다. 구체적으로 도 3에 도시된 바와 같이 연산값의 포지티브(positive) 영역(양수 영역)은 선형 함수에 기반한 제1 활성화 함수를 선택하고, 연산값의 네거티브(negative) 영역(음수 영역)은 비선형 함수에 기반한 제2 활성화 함 수를 선택할 수 있다. 이러한 본 개시의 활성화 함수는 Self-Gated ReLU(SGReLU)라고 지칭될 수 있다. 여기서 제1 활성화 함수는 선형함수에 기반하는 ReLU 함수를 선택하고, 제2 활성화 함수는 비 선형함수에 기반 하는 Swish 함수를 선택할 수 있다. 이러한 특징에 따라 본 발명의 실시예의 활성화 함수는 다음의 수학식을 따른다. [수학식 4] 는 제1 활성화 함수를 나타내고, f(x)=x· 는 제2 활성화 함수를 나타낼 수 있다. 이때, x는 시스템에 대한 초기 입력값이 아니며, 실제 활성화 기능이 이루어지는 입력값일 수 있다. 따라서, x 가 음수이고, 음수의 입력값을 제1 활성화 함수에 입력할 경우, 제1 활성화 함수가 ReLU 함수를 기반하므로,Dying ReLU을 야기할 수 있다. 여기서, 이를 방지하기 위해, x가 0 이상인 영역에서는 활성화 함수를 f(x)=x로 정의하며, x가 0보다 작은 음수 영역에 서는 입력값이 음수이므로 f(x)= x· 로 정의할 수 있다. 이와 같이 정의된 활성화 함수에 의하여 뉴런층은 학습이 계속 이루어질 수 있다. 이때, 음수 영역의 활성화 함수를 정의하는 값은 음수 영역에서 활성화 기능을 제어할 수 있다. 구체적으 로, 가 로 정의됨에 따라 x가 매우 큰 음수인 경우 제2 활성화 함수에서 출력되는 출력값은 0에 근접한 출력값이 될 수 있고, 이와 반대로 x가 매우 작은 음수인 경우 제2 활성화 함수에서 출력되는 출력값은 작은 값을 가질 수 있다. 따라서 입력값이 음수인 경우에도 전체 네트워크 학습이 중단되지 않을 수 있다. 여기서, 활성화 함수에서 사용되는 는 0.1 내지 1 사이의 범위에서 최적값으로 선택될 수 있으며, 최적값의 선택은 입력되는 데이터의 타입 및 데이터의 크기에 따라 정해질 수 있으며, 인공신경망의 훈련 페이즈 동안 최 적값으로 정해지는 하이퍼 파라미터의 하나로서 결정될 수 있다. 일 실시예에서 는 0.5로 정해질 수 있으며, 도 10에서 본 발명의 일 실시예에 따른 활성화 함수의 파라미터 값인 값에 따른 정확도를 나타내는 그래프에서 볼 수 있는 바와 같이 가 0.5 인 지점 근처에서 가장 높은 정확도가 나타날 수 있다. 한편, 활성화 함수에서 는 상기 인공신경망의 훈련 단계에서 최적 값으로 선택되는 하이퍼 파라미터일 수 있 다. 예를 들어, 인공신경망의 초기 설계에서 활성화 함수의 값은 상수 값으로 정해지지 않고, 인경신경망의 지도학습방식에 따른 훈련과정에서 최적값으로 정해지도록 설계될 수도 있다. 본 발명의 실시예와 같이 포지티브 영역과 네거티브 영역에서의 활성화 함수를 다르게 구현함으로써, 활성화 함 수를 미분한 경우에도 포지티브 영역과 네거티브 영역에서 출력값이 0이 되지 않는다. 구체적으로 본 발명의 실시예에 따라 실행되는 활성화 함수의 도함수는 다음의 수학식으로 표현된다. [수학식 5]"}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "("}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "수학식 2 및 수학식 5와 도 5를 참고하면, ReLU 함수의 도함수 값은 1 또는 0이다. 이러한 특징으로 네거티브 영역에서 ReLU 함수는 학습이 이루어지지 않는다. 이에 반해 본 발명의 실시예에 따른 제1 활성화 함수는 선형함수를 기반하므로, 도함수 값이 1 이 될 수 있다. 이러한 특징으로 포지티브 영역에서 미분 값이 존재하므로 역전파 학습이 진행될 수 있다. 또한, 본 발명의 실시예에 따른 제2 활성화 함수는 비 선형 함수를 기반하는 Swish 함수를 미분하는 것으로 도 함수 값이 존재한다. 즉, Swish 함수가 비 단조함수로서, 음수값이 0이 되지 않으므로 역전파 시, 소실되는 것 을 방지할 수 있는 것이다. 따라서, 네거티브 영역에서도 미분 값이 존재하여 역전파 학습이 진행될 수 있다. 이러한 특징에 따라 포지티브 영역과 네거티브 영역 모두에서 미분 값이 존재하여 역전파 학습을 진행할 수 있 게 되어 Dying ReLU 발생을 방지할 수 있다. 또한, 네거티브 영역에서 뉴런의 출력값이 생성되므로 전체 네트워크가 중단되는 것을 방지하여 네트워크 비활 성화를 방지할 수 있다. 이와 유사하게, 도 5를 참고하면, 본 발명의 실시예에 따라 실행되는 활성화 함수의 이차도함수는 다음의 수학 식으로 표현된다. [수학식 6]"}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "즉, 수학식 5에 도시된 도함수를 추가로 미분하여 이차 도함수를 생성하는 경우, 포지티브 영역의 이차 도함수 값은 0이 될 수 있고, 네거티브 영역의 이차 도함수 값은 수학식 6에 도시된 바와 같다. 따라서, 본 발명의 실시예에 따른 활성화 함수의 이차 도함수를 생성하여도 네거티브 영역에서 0으로 출력되는 것을 방지하여 활성화 함수에 작은 수를 입력하여도 전체 네트워크 학습이 중지되지 않는다는 것을 알 수 있다. 도 6은 본 개시의 실시예에 따른 활성화 함수의 다양한 실시예를 나타내는 그래프이다. 본 개시의 일 실시예에 따른 활성화 함수인 SGReLU는"}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 8, "content": "도 6은 활성화 함수의 도함수에 대한 그래프들도 도시하고 있다. SGReLU 활성화 함수는 연산값의 포지티브 영역에서는 선형 함수에 기반한 활성화 함수로서 동작하고 연산값의 네거티브 영역에서는 비선형 함수에 기반한 활성화 함수로서 동작할 수 있다. 아울러, SGReLU 활성화 함수는 연산값의 네거티브 영역의 작은 값(절대값으로서의 작은 값)에서는 음의 범프 (negative bump)를 가지고 네거티브 영역에서 값이 커질수록 0에 수렴하는 형상을 가질 수 있다. 여기서, 음의 범프의 크기와 형상은 값에 따라 정해질 수 있으며, 값이 클수록 음의 범프의 크기도 커지게 된다. 도 7은 본 발명의 실시예에 따른 활성화 함수를 실행하는 방법을 도시한 흐름도이고, 도 8은 본 발명의 실시예 에 따라 활성화 함수의 실행 과정을 도시한 도면이다. 도 7을 참고하면, 인공신경망을 연산하는 장치는 특정층(예컨대, 입력층, 은닉층 및 출력층 중 하나)의 노드로 부터 입력 데이터를 입력받을 수 있다(단계 S110). 입력된 입력 데이터 및 가중치 데이터에 기반하여 인공신경망 학습을 위한 활성화 함수를 선택하고, 선택된 활 성화 함수값을 다음 은닉층에 대한 출력값으로 출력할 수 있다. 여기서 연산값을 연산할 때, 입력 데이터 및 가중치 데이터에 기반하여 가중합을 연산한 뒤, 노드의 바이어스 값과 가중합에 기반하여 연산값을 결정할 수 있다(단계 S120). 이때, 활성화 함수 선택 시, 연산값의 포지티브(positive) 영역(양수 영역)은 선형 함수에 기반한 제1 활성화 함수를 선택하고, 연산값의 네거티브(negative) 영역(음수 영역)은 비선형 함수에 기반한 제2 활성화 함수를 적 용할 수 있다(단계 S130, 단계 S140). 구체적으로, 본 발명의 실시예에 따른 제1 활성화 함수는 선형함수에 기반하는 ReLU 함수를 선택하고, 제2 활성 화 함수는 비 선형함수에 기반하는 Swish 함수를 선택하도록 한다. 구체적으로, 포지티브 영역에서 제1 활성화 함수인 ReLU 함수 사용에 의하여 시그모이드 함수의 기울기 소실 문 제를 해결할 수 있으며, 미분 속도가 빠르기 때문에 학습속도가 빨라질 수 있다. 또한, 네거티브 영역에서는 제 2 활성화 함수인 Swish 함수를 사용함에 따라 0 이하값에 대해서는 모두 0으로 출력되어 학습이 불가능하다는 ReLU 함수의 문제점을 해결할 수 있다. 본 발명의 실시예의 활성화 함수는 로 정해질 수 있으며, 여기서 인공신경망 의 훈련 페이즈 동안 최적값으로 정해지는 하이퍼 파라미터의 하나로서 결정될 수 있다. 예를 들어, 인공신경망의 연산하는 단계 S120 이전에, 미리 수집된 훈련 데이터를 이용하여 인공신경망을 훈련 하는 단계를 포함할 수 있다. 여기서, 훈련하는 단계가 효율적으로 이루어지기 위해서는 하이퍼 파라미터들의 최초 설정 값이 적절하게 선택되는 것이 필요하다. 본 개시의 실시예에 따른 활성화 함수에서는 값이 적절하게 결정되어야 훈련이 효과적이고 효율적으로 수행될 수 있으며, 최적의 값은 훈련 데이터의 타입(이미지, 텍스트, 사운드 등)과 훈련 데이터의 크기(훈련 데이터 의 수, 데이터 양)에 따라 달라질 수 있다. 따라서, 본 개시의 실시예에 따른 인공신경망을 훈련시킬 때, 훈련 시작 시점에 훈련 데이터의 타입 및 크기에 따라 활성화 함수에서 의 최초 설정값을 결정하는 단계가 포함될 수 있다. 한편, 본 발명의 실시예에 따라 활성화 함수는 포지티브 영역과 네거티브 영역에 따라 다양한 형태로 변화가 가 능하며, 사용하는 학습 방향에 따라 그 값을 조절할 수 있다. 도 8을 참고하면 본 발명의 실시예에 따라 활성화 함수가 실행되는 과정을 살펴보면, 우선 라이브러리 함수 (library function)를 실행할 수 있다(도 8의 (a) 참고). 이후, 텐서플로(TensorFlow)를 이용하여 함수를 정의 할 수 있다(도 8의 (b) 참고). 텐서플로란, 기계학습(machine learning) 엔진 중 하나로, C++ 언어로 작성되었고, 파이선(Python) 응용 프로그래밍 인터페이스(API)를 제공한다. 또한, 텐서플로는 스마트폰 한 대에서도 운영될 수 있고, 데이터센터 에 있는 수천 대 컴퓨터에서도 동작될 수 있다. 이후, 신경 네트워크 모델에서 제안된 기능을 생성할 수 있다(도 7의 (c) 참고). 이때, 역전파 기능을 위한 제1 활성화 함수 및 제2 활성화 함수의 일차 도함수 및 이차 도함수는 모델 자체에서 자동으로 생성될 수 있다. 추가적으로 이하의 표 1을 참조하여 기존 활성화 함수와 본 발명의 실시예에 따른 활성화 함수의 기능을 살펴보 면, 본 발명의 실시예에 따른 활성화 함수의 실행 정확도가 향상되었음을 알 수 있다. [표 1]"}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "한편, 도 9는 본 발명의 실시예에 따른 활성화 함수가 실행되는 인공신경망을 연산하는 장치의 구성을 나타낸 블록도이다. 도면을 참고하면, 본 발명의 실시예에 따른 인공신경망 학습 장치는 메모리 및 프로세서를 포함 할 수 있다. 메모리는 신호라인을 통해 프로세서와 연결될 수 있다. 메모리는 모바일 프로그램 상에서 실행 되는 활성화 함수의 공식이 저장될 수 있으며, 이외에도 프로세서의 연산 동작에 관련된 프로그램을 저장 할 수 있다. 프로세서는 메모리에 저장된 실행 코드가 동작하도록 결정하는 구성 요소이다. 이를 위해 프로세서 는 메모리에 저장된 실행 코드를 제어하는 데이터를 요청, 검색, 수신 또는 활용할 수 있으며, 적어 도 하나의 실행 가능한 동작 중 예측되는 동작이나 바람직한 것으로 판단되는 동작을 실행할 수 있다. 실시예에 따른 인공신경망 학습 장치는 통신부, 입력부 및 출력부를 더 포함할 수 있다. 통신부는 외부 장치와 인공신경망 학습 장치 간의 통신 인터페이스를 제공한다. 입력부는 다른 신호라인을 통해 프로세서와 연결된다. 프로세서는 통신부 또는 입력부를 통해 활성화 함수와 연관된 연산값을 획득할 수 있다 또한, 프로세서는 통신부 또는 입력부를 통해 인공신경망의 종류 및/또는 인공신경망 내의 노드 수에 따라 변화되는 연산값을 입력받을 수 있다. 프로세서를 이용한 학습과 관련된 연산의 결과는 모바일, PC 등의 디스플레이(미도시)를 통해 표시되거나 출력부를 통해 출력될 수 있다. 도 10은 본 발명의 일 실시예에 따른 활성화 함수의 파라미터 값인 값에 따른 정확도를 나타내는 그래프이다. 는 본 발명의 일 실시예에 따른 활성화 함수의 하이퍼파라미터 값으로서. 본 개시에서 설정한 환경상의 실험에 서는 0.5 내지 0.6 사이에서 정확도가 가장 높았음을 도 10의 그래프를 통해 알 수 있다. 다만, 도 10에서의 그래프는 예시적 조건에서 획득된 활성화 함수의 값에 따른 정확도를 도시하고 있을 뿐이고, 신경망 모델의 타입, 신경망 모델에서 사용되는 데이터의 종류 등에 따라서 에 대한 최적 값은 달라 질 수 있다. 도 11은 본 발명의 일 실시예에 따른 활성화 함수의 하이퍼파라미터 결정을 위한 흐름도이다. 인공신경망의 연상을 수행하기 이전에 인공신경망을 훈련시키는 단계가 필요하다. 인공신경망을 훈련시키는 단 계는 미리 수집된 훈련 데이터를 이용하여 인공신경망을 훈련하는 단계를 포함할 수 있다. 미리 수집된 훈련 데이터는 인공신경망의 지도 학습을 위해 인공신경망의 출력 목표값과 관련한 진리값(Ground- Truth)으로 레이블링된 데이터일 수 있다. 도 11에서는 인공신경망을 훈련시키는 단계에서 활성화 함수의 최적 값을 결정하여 인공신경망의 정확도를 높 일 수 있는 방식에 대해 설명하고 있다. 최초 단계에서 값은 최소 설정값으로 설정될 수 있다(S210). 값은 0 내지 1 사이의 값일 수 있으며, 최초 단계에서의 최소 설정값은 0.1 로 결정될 수 있으며, 최소 설정값은 사용자에 의해 임의로 결정될 수 있다. 최소 설정값의 값을 가지는 활성화 함수를 포함하는 인공신경망에 대해 미리 수집된 훈련 데이터 중 제1 그룹 의 데이터를 이용하여 인공신경망을 훈련시킬 수 있다(S220). 제1 그룹의 데이터는 예를 들어, 미리 수집된 훈 련 데이터 전체 중 80%의 임의로 선정된 데이터일 수 있다. 인공신경망을 훈련시키는 장치의 프로세서(인공신경망 연상 장치의 프로세서와 동일할 수 있음)는 제1 그룹의 데이터를 이용해 훈련된 인공신경망에 대해 미리 수집된 훈련 데이터 중 제1 그룹의 데이터 이외의 데이터를 이 용하여 훈련된 인공신경망의 정확도를 측정하고 이를 기록할 수 있다(S230). 프로세서는 값을 제1 크기만큼 증가시키면서 인공신경망의 정확도를 측정하고 기록하는 단계를 값이 1 이 상이 될 때까지 반복할 수 있으며, 이를 위해 값이 1 이상에 도달하였는지 판단할 수 있다(S240). 제1 크기는 미리 정해진 고정 값(예를 들어, 0.1)일 수 있으나, 제1크기는 상황에 따라 변화하는 변수값으로 설 정될 수도 있다. 예를 들어, 제1 크기는 직전 단계에서 훈련된 인공신경망의 정확도에 따라 결정되고, 직전 단계에서 훈련된 인공신경망의 정확도가 작을수록 제1 크기는 크게 결정될 수도 있다. 직전 단계에서 훈련된 인공신경망의 정확도가 기준치보다 낮다는 것은 값이 더 증가되어야 할 필요가 있다는 것을 의미하고 직전 단계에서 훈련된 인공신경망의 정확도가 기준치보다 높다는 것은 값이 최적값에 가깝다는 의미일 수 있다. 프로세서는 값을 증가시키면서 반복하는 단계를 통해 획득되는 정확도들 중 정확도가 가장 높게 측정된 경우 의 값을 활성화 함수의 최적 값으로 결정할 수 있다(S250). 프로세서는 최적 값의 활성화 함수를 가지는 인공신경망에 대해 미리 수집된 훈련 데이터를 이용하여 훈련시키 는 단계를 수행할 수 있다(S260). 상술된 프로세스를 통해 최적의 값의 활성화 함수가 결정될 수 있고, 이는 가장 높은 정확도를 가지는 인공신 경망을 산출할 수 있도록 한다. 설명된 바와 같이 인공지능 학습을 위한 활성화 함수를 선택함에 있어서, 연산값의 포지티브(양수) 영역은 선형 함수에 기반한 제1 활성화 함수를 선택할 수 있도록 하며, 연산값의 네거티브(음수) 영역은 비 선형 함수에 기 반한 제2 활성화 함수를 선택할 수 있도록 한다. 이로 인해, 활성화 함수를 미분하는 경우 역전파 값이 소실되어 학습이 진행할 수 없는 Dying ReLU 발생을 방지 할 수 있다. 특히 네거티브 영역에서 비 선형 함수인 Swish 함수를 선택함으로써, 네거티브 영역에서 기울기가 0이 되지 않는 특징으로 역전파도 학습이 가능해질 수 있게 된다. 또한, 포지티브 영역에서 선형 함수인 제1 활성화 함수를 사용함으로써, 연산 복잡도를 낮춰 프로그램의 구현이 보다 쉬워질 수 있게 된다. 이상 설명된 본 발명에 따른 실시예는 컴퓨터 상에서 다양한 구성요소를 통하여 실행될 수 있는 컴퓨터 프로그 램의 형태로 구현될 수 있으며, 이와 같은 컴퓨터 프로그램은 컴퓨터로 판독 가능한 매체에 기록될 수 있다. 이 때, 매체는 하드 디스크, SSD(Solid State Drive), SDD(Silicon Disk Drive), 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등과 같은, 프로그램 명령어를 저장하고 실행하도록 특별히 구성된 하드웨어 장치를 포함할 수 있다. 한편, 상기 컴퓨터 프로그램은 본 발명을 위하여 특별히 설계되고 구성된 것이거나 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수 있다. 컴퓨터 프로그램의 예에는, 컴파일러에 의하여 만들어지는 것 과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용하여 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도"}
{"patent_id": "10-2021-0192818", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "포함될 수 있다.이상 설명된 본 발명의 실시 예에 대한 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야 의 통상의 지식을 가진 자는 본 발명의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태 로 쉽게 변형이 가능하다는 것을 이해할 수 있을 것이다. 그러므로 이상에서 기술한 실시 예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 발명의 범위는 상기 상세한 설명보다는 후술하는 청구범위에 의하여 나타내어지며, 청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으로 해석 되어야 한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11"}
{"patent_id": "10-2021-0192818", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 활성화 함수가 실행되는 인공신경망의 구성을 나타낸 개념도이다. 도 2a는 시그모이드 함수(Sigmoid Function)의 도함수를 나타낸 도면이고, 도 2b는 ReLU함수(Rectified Linear Unit Function)의 도함수를 나타낸 도면이며, 도 2c는 Swish 함수의 도함수를 나타낸 도면이다. 도 3은 본 발명의 실시예에 따라 실행되는 활성화 함수를 도시한 도면이다. 도 4는 본 발명의 실시예에 따라 실행되는 활성화 함수의 도함수를 도시한 도면이다. 도 5는 본 발명의 실시예에 따라 실행되는 활성화 함수의 이차 도참수를 도시한 도면이다. 도 6은 본 발명의 실시예에 따른 활성화 함수의 다양한 실시예를 나타내는 그래프이다. 도 7은 본 발명의 실시예에 따른 활성화 함수를 실행하는 방법을 도시한 흐름도이다. 도 8은 본 발명의 실시예에 따라 활성화 함수의 실행 과정을 도시한 도면이다. 도 9는 본 발명의 실시예에 따른 활성화 함수가 실행되는 인공신경망 연산 장치의 구성을 나타낸 블록도이다. 도 10은 본 발명의 일 실시예에 따른 활성화 함수의 파라미터 값인 값에 따른 정확도를 나타내는 그래프이다. 도 11은 본 발명의 일 실시예에 따른 활성화 함수의 하이퍼파라미터 결정을 위한 흐름도이다."}
