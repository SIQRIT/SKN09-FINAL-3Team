{"patent_id": "10-2024-7006947", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0063108", "출원번호": "10-2024-7006947", "발명의 명칭": "오디오 컨퍼런싱에 대한 자동 음소거 및 음소거해제", "출원인": "퀄컴 인코포레이티드", "발명자": "메흐타 우마"}}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "오디오 컨퍼런스를 제어하도록 구성되는 장치로서,상기 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하도록 구성된 메모리; 및상기 메모리와 통신하는 하나 이상의 프로세서들을 포함하고,상기 하나 이상의 프로세서들은:상기 오디오 데이터의 분석물을 생성하기 위해 상기 오디오 데이터의 스피커 또는 상기 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 상기 오디오 데이터를 분석하고; 그리고상기 오디오 데이터의 상기 분석물에 기초하여 마이크로폰을 제어하거나 또는 상기 참가자의 상기 오디오 데이터를 조정하도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 오디오 데이터의 스피커 또는 상기 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 상기 오디오 데이터를 분석하기 위해, 상기 하나 이상의 프로세서들은:상기 오디오 데이터의 상기 분석물을 생성하기 위해 하나 이상의 인공 지능 기술들을 사용하여 상기 오디오 데이터를 분석하도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서, 상기 하나 이상의 인공 지능 기술들은 뉴럴 네트워크를 포함하는, 오디오 컨퍼런스를 제어하도록 구성되는장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 2 항에 있어서, 상기 하나 이상의 인공 지능 기술들은 자연 언어 프로세싱을 포함하는, 오디오 컨퍼런스를 제어하도록 구성되는장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서,상기 오디오 데이터의 상기 스피커를 결정하도록 상기 오디오 데이터를 분석하기 위해, 상기 하나 이상의 프로세서들은 또한:스피커 분류를 결정하기 위해 상기 참가자의 음성의 등록된 버전에 대한 상기 오디오 데이터를 분류하고; 그리고상기 오디오 데이터가 상기 스피커 분류에 기초하여 상기 참가자의 상기 음성을 나타내는지를 결정하도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서, 상기 오디오 데이터의 상기 분석물에 기초하여 상기 마이크로폰을 제어하거나 상기 참가자의 상기 오디오 데이공개특허 10-2024-0063108-3-터를 조정하기 위해, 상기 하나 이상의 프로세서들은:상기 오디오 데이터가 상기 참가자의 상기 음성을 나타내지 않는다는 결정에 기초하여 상기 마이크로폰을 음소거하거나 상기 참가자의 상기 오디오 데이터를 음소거하도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 5 항에 있어서, 상기 오디오 데이터의 상기 분석물에 기초하여 상기 마이크로폰을 제어하거나 상기 참가자의 상기 오디오 데이터를 조정하기 위해, 상기 하나 이상의 프로세서들은:상기 오디오 데이터가 상기 참가자의 상기 음성을 나타낸다는 결정에 기초하여 상기 마이크로폰을 음소거하지않거나 상기 참가자의 상기 오디오 데이터를 음소거하지 않도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 5 항에 있어서, 하나 이상의 프로세서들은:상기 참가자의 상기 음성의 상기 등록된 버전을 사용하여 뉴럴 네트워크를 트레이닝하도록 구성되고, 그리고 상기 오디오 데이터를 분류하기 위해, 상기 하나 이상의 프로세서들은 상기 뉴럴 네트워크를 사용하여 상기 오디오 데이터를 분류하도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항에 있어서, 상기 오디오 데이터의 상기 컨텍스트를 결정하도록 상기 오디오 데이터를 분석하기 위해, 상기 하나 이상의 프로세서들은 또한:컨텍스트 분류를 결정하기 위해 트레이닝 데이터에 대해 상기 오디오 데이터의 컨텐츠를 분류하고;상기 오디오 데이터가 상기 컨텍스트 분류에 기초하여 상기 오디오 컨퍼런스의 컨텍스트를 나타내는지를 결정하도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서, 상기 오디오 데이터의 상기 분석물에 기초하여 상기 마이크로폰을 제어하거나 상기 참가자의 상기 오디오 데이터를 조정하기 위해, 상기 하나 이상의 프로세서들은:상기 오디오 데이터가 상기 오디오 컨퍼런스의 상기 컨텍스트를 나타내지 않는다는 결정에 기초하여 상기 마이크로폰을 음소거하거나 상기 참가자의 상기 오디오 데이터를 음소거하도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 9 항에 있어서, 상기 오디오 데이터의 상기 분석물에 기초하여 상기 마이크로폰을 제어하거나 상기 참가자의 상기 오디오 데이터를 조정하기 위해, 상기 하나 이상의 프로세서들은:상기 오디오 데이터가 상기 오디오 컨퍼런스의 상기 컨텍스트를 나타낸다는 결정에 기초하여 상기 마이크로폰을음소거하지 않거나 상기 참가자의 상기 오디오 데이터를 음소거하지 않도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "공개특허 10-2024-0063108-4-제 9 항에 있어서, 하나 이상의 프로세서들은:트레이닝 데이터를 사용하여 뉴럴 네트워크를 트레이닝하도록 구성되고, 상기 트레이닝 데이터는 오디오 컨퍼런스의 컨텍스트를 나타내는 문법을 포함하고, 상기 오디오 데이터를 분류하기 위해, 상기 하나 이상의 프로세서들은 상기 뉴럴 네트워크를 사용하여 상기 오디오 데이터를 분류하도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 1 항에 있어서, 상기 오디오 데이터의 스피커 또는 상기 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 상기 오디오 데이터를 분석하기 위해, 상기 하나 이상의 프로세서들은 또한:스피커 분류를 결정하기 위해 상기 참가자의 음성의 등록된 버전에 대한 오디오 데이터를 분류하고; 상기 오디오 데이터가 상기 스피커 분류에 기초하여 상기 참가자의 상기 음성을 나타내는지를 결정하고;컨텍스트 분류를 결정하기 위해 트레이닝 데이터에 대해 상기 오디오 데이터의 컨텐츠를 분류하고;상기 오디오 데이터가 상기 컨텍스트 분류에 기초하여 상기 오디오 컨퍼런스의 컨텍스트를 나타내는지를 결정하도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13 항에 있어서,상기 오디오 데이터의 상기 분석물에 기초하여 상기 마이크로폰을 제어하거나 상기 참가자의 상기 오디오 데이터를 조정하기 위해, 상기 하나 이상의 프로세서들은:상기 참가자의 상기 오디오 데이터가 음소거된다고 결정하고;상기 오디오 데이터가 상기 참가자의 상기 음성을 나타낸다는 결정에 기초하여 그리고 상기 오디오 데이터가 상기 오디오 컨퍼런스의 상기 컨텍스트를 나타낸다는 결정에 기초하여 상기 참가자의 상기 오디오 데이터를 음소거해제하도록 구성되는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "오디오 컨퍼런스를 제어하기 위한 방법으로서,상기 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하는 단계;상기 오디오 데이터의 분석물을 생성하기 위해 상기 오디오 데이터의 스피커 또는 상기 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 상기 오디오 데이터를 분석하는 단계;상기 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 상기 참가자의 상기 오디오 데이터를조정하는 단계를 포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15 항에 있어서, 상기 오디오 데이터의 스피커 또는 상기 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 상기 오디오 데이터를 분석하는 단계는:상기 오디오 데이터의 상기 분석물을 생성하기 위해 하나 이상의 인공 지능 기술들 또는 머신 러닝 기술들을 사용하여 상기 오디오 데이터를 분석하는 단계를 포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16 항에 있어서, 공개특허 10-2024-0063108-5-상기 하나 이상의 인공 지능 기술들 또는 머신 러닝 기술들은 뉴럴 네트워크를 포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 16 항에 있어서, 상기 하나 이상의 인공 지능 기술들 또는 머신 러닝 기술들은 자연 언어 프로세싱을 포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 15 항에 있어서, 상기 오디오 데이터의 스피커를 결정하도록 상기 오디오 데이터를 분석하는 단계는:스피커 분류를 결정하기 위해 상기 참가자의 음성의 등록된 버전에 대한 상기 오디오 데이터를 분류하는 단계;및상기 오디오 데이터가 상기 스피커 분류에 기초하여 상기 참가자의 상기 음성을 나타내는지를 결정하는 단계를포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 19 항에 있어서, 상기 오디오 데이터의 분석물에 기초하여 상기 마이크로폰을 제어하거나 또는 상기 참가자의 상기 오디오 데이터를 조정하는 단계는:상기 오디오 데이터가 상기 참가자의 상기 음성을 나타내지 않는다는 결정에 기초하여 상기 마이크로폰을 음소거하거나 상기 참가자의 상기 오디오 데이터를 음소거하는 단계를 포함하는, 오디오 컨퍼런스를 제어하기 위한방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제 19 항에 있어서, 상기 오디오 데이터의 분석물에 기초하여 상기 마이크로폰을 제어하거나 또는 상기 참가자의 상기 오디오 데이터를 조정하는 단계는:상기 오디오 데이터가 상기 참가자의 상기 음성을 나타낸다는 결정에 기초하여 상기 마이크로폰을 음소거하지않거나 상기 참가자의 상기 오디오 데이터를 음소거하지 않는 단계를 포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제 19 항에 있어서,상기 참가자의 상기 음성의 상기 등록된 버전을 사용하여 뉴럴 네트워크를 트레이닝하는 단계를 더 포함하고,그리고 상기 오디오 데이터를 분류하는 단계는 상기 뉴럴 네트워크를 사용하여 상기 오디오 데이터를 분류하는 단계를포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제 15 항에 있어서, 상기 오디오 데이터의 상기 컨텍스트를 결정하기 위해 상기 오디오 데이터를 분석하는 단계는:컨텍스트 분류를 결정하기 위해 트레이닝 데이터에 대해 상기 오디오 데이터의 컨텐츠를 분류하는 단계; 및상기 오디오 데이터가 상기 컨텍스트 분류에 기초하여 상기 오디오 컨퍼런스의 컨텍스트를 나타내는지를 결정하공개특허 10-2024-0063108-6-는 단계를 포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제 23 항에 있어서, 상기 오디오 데이터의 분석물에 기초하여 상기 마이크로폰을 제어하거나 또는 상기 참가자의 상기 오디오 데이터를 조정하는 단계는:상기 오디오 데이터가 상기 오디오 컨퍼런스의 상기 컨텍스트를 나타내지 않는다는 결정에 기초하여 상기 마이크로폰을 음소거하거나 상기 참가자의 상기 오디오 데이터를 음소거하는 단계를 포함하는, 오디오 컨퍼런스를제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제 23 항에 있어서, 상기 오디오 데이터의 분석물에 기초하여 상기 마이크로폰을 제어하거나 또는 상기 참가자의 상기 오디오 데이터를 조정하는 단계는:상기 오디오 데이터가 상기 오디오 컨퍼런스의 상기 컨텍스트를 나타낸다는 상기 결정에 기초하여 상기 마이크로폰을 음소거하지 않거나 상기 참가자의 상기 오디오 데이터를 음소거하지 않는 단계를 포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제 23 항에 있어서,상기 트레이닝 데이터를 사용하여 뉴럴 네트워크를 트레이닝하는 단계를 더 포함하고, 상기 트레이닝 데이터는상기 오디오 컨퍼런스의 컨텍스트를 나타내는 문법을 포함하고, 상기 오디오 데이터를 분류하는 단계는 상기 뉴럴 네트워크를 사용하여 상기 오디오 데이터를 분류하는 단계를포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제 15 항에 있어서, 상기 오디오 데이터의 스피커 또는 상기 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 상기 오디오 데이터를 분석하는 단계는:스피커 분류를 결정하기 위해 상기 참가자의 음성의 등록된 버전에 대한 상기 오디오 데이터를 분류하는 단계; 상기 오디오 데이터가 스피커 분류에 기초하여 상기 참가자의 상기 음성을 나타내는지를 결정하는 단계;컨텍스트 분류를 결정하기 위해 트레이닝 데이터에 대해 상기 오디오 데이터의 컨텐츠를 분류하는 단계; 및상기 오디오 데이터가 상기 컨텍스트 분류에 기초하여 상기 오디오 컨퍼런스의 컨텍스트를 나타내는지를 결정하는 단계를 포함하는, 오디오 컨퍼런스를 제어하기 위한 방법."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "제 27 항에 있어서, 상기 오디오 데이터의 분석물에 기초하여 상기 마이크로폰을 제어하거나 또는 상기 참가자의 상기 오디오 데이터를 조정하는 단계는:상기 참가자의 상기 오디오 데이터가 음소거된다고 결정하는 단계; 및상기 오디오 데이터가 상기 참가자의 음성을 나타낸다는 결정에 기초하여 그리고 상기 오디오 데이터가 상기 오디오 컨퍼런스의 상기 컨텍스트를 나타낸다는 결정에 기초하여 상기 참가자의 상기 오디오 데이터를 음소거해제하는 단계를 포함하는, 오디오 컨퍼런스를 제어하기 위한 방법.공개특허 10-2024-0063108-7-청구항 29 명령들을 저장한 비일시적 컴퓨터 판독가능 저장 매체로서,상기 명령들은 실행될 때 하나 이상의 프로세서들로 하여금:오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하게 하고;상기 오디오 데이터의 분석물을 생성하기 위해 상기 오디오 데이터의 스피커 또는 상기 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 상기 오디오 데이터를 분석하게 하고; 그리고상기 오디오 데이터의 상기 분석물에 기초하여 마이크로폰을 제어하거나 또는 상기 참가자의 상기 오디오 데이터를 조정하게 하는, 명령들을 저장한 비일시적 컴퓨터 판독가능 저장 매체."}
{"patent_id": "10-2024-7006947", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_30", "content": "오디오 컨퍼런스를 제어하도록 구성되는 장치로서,상기 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하기 위한 수단;상기 오디오 데이터의 분석물을 생성하기 위해 상기 오디오 데이터의 스피커 또는 상기 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 상기 오디오 데이터를 분석하기 위한 수단; 및상기 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 상기 참가자의 상기 오디오 데이터를조정하기 위한 수단을 포함하는, 오디오 컨퍼런스를 제어하도록 구성되는 장치."}
{"patent_id": "10-2024-7006947", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "오디오 컨퍼런스를 제어하기 위한 기법들은 오디오 컨퍼런스에서의 참가자로부터 오디오 데이터를 수신하는 것, 오디오 데이터의 분석물을 생성하기 위해 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상 을 결정하도록 오디오 데이터를 분석하는 것, 및 상기 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하 거나 또는 상기 참가자의 상기 오디오 데이터를 조정하는 것을 포함한다. 마이크로폰은 스피커가 참가자가 아니거나 오디오의 컨텐츠가 오디오 컨퍼런스의 컨텍스트 외부에 있다는 결정에 기초하여 음소거될 수 있다."}
{"patent_id": "10-2024-7006947", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 2021년 9월 7일자로 출원된 미국 특허출원 제17/468,177호의 이익을 주장하고, 그의 전체 내용은 본 명세서에 참조에 의해 통합된다."}
{"patent_id": "10-2024-7006947", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "기술분야 본 개시는 오디오 컨퍼런스 관리에 관한 것이다."}
{"patent_id": "10-2024-7006947", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "비디오 및 오디오 컨퍼런스는 개인용 및 업무용 양쪽으로 사용된다. 비디오 및 오디오 컨퍼런스 애플리케이 션은 2개 이상의 원격 위치에서 다수의 참가자들 간의 회의에 유용한 툴를 제공한다. 이러한 컨퍼런싱을 사 용하여 효율적인 회의를 수행하는 것은 일부 시나리오에서 어려울 수 있다. 예를 들어, 비디오 및 오디오 컨퍼런싱 애플리케이션들은 주어진 회의에 대해 많은 수의 사용자들을 지원할 수 있다. 특정 참가자가 말하 기를 중단할 때가 불분명할 수 있고, 많은 참가자가 한꺼번에 말을 할 수 있으며, 참가자의 마이크로폰이 음소 거해제된 상태를 유지하면 배경 잡음이 성가실 수 있기 때문에 많은 수의 참가자들을 갖는 컨퍼런스는 관리가 어려워질 수 있다."}
{"patent_id": "10-2024-7006947", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시는 오디오 컨퍼런스에서 참가자의 마이크로폰 및/또는 오디오 데이터의 자동 음소거 및/또는 음소거해제 를 위한 기법들을 설명한다. 오디오 컨퍼런스 및 가상 회의 동안, 원하지 않는 오디오가 오디오 컨퍼런스의 모든 참가자들에게 브로드캐스트되는 몇몇 시나리오들이 발생할 수 있다. 이러한 시나리오들은 참가자가 컨 텍스트를 벗어나 말하는 것, 배경 잡음, 및 컨퍼런스에 있지 않은 다른 지역 사람의 음성들이 참가자의 마이크 로폰에 의해 포착되는 것을 포함할 수 있다. 이러한 원하지 않는 오디오는 컨퍼런스에서 참가자에게 불편함을 야기할 수 있고 또한 스피커를 불편하게 만들 수 있다. 하나의 예에서, 본 개시는 오디오 컨퍼런스의 참가자의 마이크로폰 및/또는 오디오 데이터를 자동으로 음소거하 기 위한 기법들을 설명한다. 오디오 컨퍼런스가 실행중인 디바이스는 참가자의 마이크로폰에 의해 캡처된 오디오 데이터를 분석하고, 그리고 오디오 데이터가 참가자의 음성을 나타내는지를 결정하고/결정하거나 오디오 데이터의 컨텐츠가 회의의 컨텍스트 내에 있는지를 결정하도록 구성될 수도 있다. 디바이스는 하나 이상의 인공 지능 기술들, 이를 테면, 머신 러닝 기술 또는 뉴럴 네트워크를 사용하여 오디오 데이터를 분류할 수 있다. 오디오 데이터가 참가자의 음성에 매칭하지 않는 것으로서 분류되면, 디바이스는 참가자의 마이크로 폰 및/또는 오디오 데이터를 자동으로 음소거할 수도 있다. 이와 유사하게, 오디오 데이터의 컨텐츠가 오디 오 컨퍼런스의 컨텍스트와 매칭하지 않는 것으로서 분류되면 (예를 들어, 오디오 컨퍼런스의 토픽에 관련되지 않은 문장을 포함하면), 디바이스는 참가자의 마이크로폰 및/또는 오디오 데이터를 자동으로 음소거해제할 수 있다. 다른 예들에서, 본 개시는 또한 마이크로폰 및/또는 오디오 데이터의 자동 음소거해제를 위한 기법들을 설명한 다. 일부 사례들에서, 참가자는 음소거해제된 마이크로폰 또는 오디오 데이터를 가질 수 있지만, 마이크로 폰 또는 오디오 데이터를 먼저 음소거하지 않고 말하기를 시작할 수 있다. 오디오 컨퍼런스가 실행중인 디 바이스는 참가자의 마이크로폰에 의해 캡처된 오디오 데이터를 분석하고, 그리고 오디오 데이터가 참가자의 음 성을 나타내고 오디오 데이터의 컨텐츠가 회의의 컨텍스트 내에 있는지를 결정하도록 구성될 수도 있다. 그 렇다면, 디바이스는 참가자의 마이크로폰 또는 오디오 데이터를 자동으로 음소거해제할 수 있다. 본 개시의 기법들에 따른 마이크로폰들 및/또는 오디오 데이터의 자동 음소거 및 음소거해제는 오디오 컨퍼런스에서 원하 지 않는 오디오의 양을 제한할 수 있고 그러한 오디오 컨퍼런스들이 더 효율적으로 동작하게 할 수 있다. 또한, 본 개시의 자동 음소거해제 특징들은 참가자 발언들을 누락시키거나 발언들을 반복할 필요성을 회피할 수 있다. 다른 예에서, 본 개시는 오디오 컨퍼런스를 제어하도록 구성된 장치를 설명하며, 장치는 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하도록 구성된 메모리, 및 메모리와 통신하는 하나 이상의 프로세서를 포함 한다. 하나 이상의 프로세서들은, 오디오 데이터의 분석물을 생성하기 위해, 오디오 데이터의 스피커 또는 오디오 데 이터의 컨텍스트 중 하나 이상을 결정하도록 오디오 데이터를 분석하고, 그리고 오디오 데이터의 분석물에 기초 하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하도록 구성된다. 다른 예에서, 본 개시는 오디오 컨퍼런스를 제어하기 위한 방법을 설명하며, 방법은 오디오 컨퍼런스에서 참가 자로부터 오디오 데이터를 수신하는 단계, 오디오 데이터의 분석물을 생성하기 위해 오디오 데이터의 스피커 또 는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 오디오 데이터를 분석하는 단계, 및 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하는 단계를 포함한다. 다른 예에서, 본 개시는 명령들을 저장하는 비-일시적 컴퓨터 판독가능 저장 매체로서, 명령들은, 실행될 때, 하나 이상의 프로세서들로 하여금, 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하게 하고, 오디오 데이터의 분석물을 생성하기 위해 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정 하도록 오디오 데이터를 분석하게 하고, 그리고 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하게 한다. 다른 예에서, 본 개시는 오디오 컨퍼런스를 제어하도록 구성된 장치를 설명하며, 장치는 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하기 위한 수단, 오디오 데이터의 분석물을 생성하기 위해 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 오디오 데이터를 분석하기 위한 수단, 및 오 디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하기 위한 수단 을 포함한다. 본 개시의 하나 이상의 예들의 상세들이 첨부 도면들 및 이하의 설명에 제시된다. 다른 피처들, 목적들, 및 이점들은 설명, 도면들, 및 청구항들로부터 명백할 것이다."}
{"patent_id": "10-2024-7006947", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시는 오디오 컨퍼런스에서 참가자의 마이크로폰 또는 오디오 데이터의 자동 음소거 및/또는 음소거해제를 위한 기법들을 설명한다. 하나의 예에서, 본 개시는 오디오 컨퍼런스의 참가자의 마이크로폰 또는 오디오 데이터를 자동으로 음소거하기 위한 기법들을 설명한다. 오디오 컨퍼런스가 실행중인 디바이스는 참가자의 마이크로폰에 의해 캡처된 오디오 데이터를 분석하고 오디오 데이터가 참가자의 음성을 나타내는지를 결정하고/ 하거나 오디오 데이터의 컨텐츠가 회의의 컨텍스트 내에 있는지를 결정하도록 구성될 수도 있다. 디바이스 는 하나 이상의 인공 지능 또는 머신 러닝 기법, 이를 테면, 뉴럴 네트워크를 사용하여 오디오 데이터를 분류할 수 있다. 오디오 데이터가 참가자의 음성에 매칭하지 않는 것으로서 분류되면, 디바이스는 참가자의 마이크 로폰 또는 오디오 데이터를 자동으로 음소거할 수도 있다. 이와 유사하게, 오디오 데이터의 컨텐츠가 오디 오 컨퍼런스의 컨텍스트와 매칭하지 않는 것으로서 분류되면 (예를 들어, 오디오 컨퍼런스의 토픽에 관련되지 않은 문장을 포함하면), 디바이스는 참가자의 마이크로폰 또는 오디오 데이터를 자동으로 음소거해제할 수 있다. 다른 예들에서, 본 개시는 또한 마이크로폰 또는 오디오 데이터의 자동 음소거해제를 위한 기법들을 설명한다. 일부 사례들에서, 참가자는 음소거해제된 마이크로폰 또는 오디오 데이터를 가질 수 있지만, 마이크로폰 또 는 오디오 데이터를 먼저 음소거하지 않고 말하기를 시작할 수 있다. 오디오 컨퍼런스가 실행중인 디바이스 는 참가자의 마이크로폰에 의해 캡처된 오디오 데이터를 분석하고, 그리고 오디오 데이터가 참가자의 음성을 나 타내고 오디오 데이터의 컨텐츠가 회의의 컨텍스트 내에 있는지를 결정하도록 구성될 수도 있다. 그렇다면, 디바이스는 참가자의 마이크로폰 또는 오디오 데이터를 자동으로 음소거해제할 수 있다. 본 개시의 기법들 에 따른 마이크로폰들 또는 오디오 데이터의 자동 음소거 및 음소거해제는 오디오 컨퍼런스에서 원하지 않는 오 디오의 양을 제한할 수 있고 그러한 오디오 컨퍼런스들이 더 효율적으로 동작하게 할 수 있다. 도 1 은 본 개시의 기법들에 일치하는, 제 1 디바이스와 제 2 디바이스 사이의 오디오 컨퍼런스를 예시하는 블 록 다이어그램이다. 일부 예들에서, 도 1 에 도시된 오디오 컨퍼런스는 비디오 컨퍼런스일 수 있다. 도 1 은 오디오 컨퍼런싱에 참여하는 2 개의 디바이스들을 도시한다. 그러나, 본 개시의 기법들은 임의의 수의 디바이스들에의 사용에 적용가능하다. 제 1 디바이스 (디바이스 A) 는 카메라 및 디스플레이 유닛 , 마이크로폰 및 스피커 유닛 , 및 오디오 및 비디오 (A/V) 프로세싱 유닛 을 포함한다. 제 2 디바이스 (디바이스 B) 는 카메라 및 디스플레이 유닛 , 마이크로폰 및 스피커 유닛 , 및 오디오 및 비디오 (A/V) 프로세싱 유닛 을 포함한다. 제 1 디바이스 는 네트워크 를 통하여 제 2 디바이스 와 통신한다. 도 1 의 예에서, 제 1 디바이스 는 스마트폰, 태블릿 컴퓨터, 랩톱 컴퓨터, 데스크톱 컴퓨터, Wi-Fi 실행 전화, 비디오 컨퍼런싱 디바이스, 또는 오디오 및/또는 비디오 데이터를 송신할 수 있는 임의의 다른 디바이스 로서 구성될 수 있다. 마찬가지로, 제 2 디바이스 는 스마트폰, 태블릿 컴퓨터, 랩톱 컴퓨터, 데스크 톱 컴퓨터, Wi-Fi 가능 텔레비전, 화상 회의 디바이스, 또는 오디오 및/또는 비디오 데이터를 수신하고 사용자 입력 데이터를 수신할 수 있는 임의의 다른 디바이스로서 구성될 수 있다.카메라 및 디스플레이 유닛 및 카메라 및 디스플레이 유닛 은 각각 정지 또는 비디오 이미지들을 캡 처하기 위한 카메라 및 비디오 데이터를 제 1 디바이스 또는 제 2 디바이스 의 사용자에게 제시하기 위한 디스플레이를 포함할 수도 있다. 디스플레이는 임의의 여러 비디오 출력 디바이스들, 이를 테면, 액정 디스플레이 (LCD), 플라즈마 디스플레이, 유기 발광 다이오드 (OLED) 디스플레이, 또는 다른 타입의 디스플레이 디바이스를 포함할 수 있다. 이들 또는 다른 예들에서, 디스플레이 디바이스는 발광형 디스플레이 또는 투 과형 디스플레이일 수 있다. 마이크로폰 및 스피커 유닛 및 마이크로폰 및 스피커 유닛 은 각각 사운드를 캡처하기 위한 마이크로 폰 및 사운드를 제 1 디바이스 또는 제 2 디바이스 의 사용자에게 제시하기 위한 스피커를 포함할 수 도 있다. 스피커는 임의의 여러 오디오 출력 디바이스들, 이를 테면, 헤드폰, 단일-스피커 시스템, 멀티-스 피커 시스템, 또는 서라운드 사운드 시스템을 포함할 수 있다. A/V 프로세싱 유닛 및 A/V 프로세싱 유닛 은 오디오 및/또는 비디오 데이터를 프로세싱하는 것을 담 당하는 다수의 유닛들을 포함할 수도 있다. A/V 프로세싱 유닛 및 A/V 프로세싱 유닛 각각은 하 나 이상의 마이크로프로세서, 디지털 신호 프로세서들 (DSPs), 주문형 집적 회로들 (ASICs), 필드 프로그램가능 게이트 어레이(FPGA), 이산 로직, 하드웨어, 또는 이들의 임의의 조합으로서 구현될 수 있고, 소프트웨어 및/또 는 펌웨어를 실행하도록 구성될 수 있다. A/V 프로세싱 유닛 및 A/V 프로세싱 유닛 각각은 하나 이상의 비디오 인코더들 또는 비디오 디코더들을 포함할 수 있고, 이들 중 어느 하나는 결합된 인코더/디코더 (CODEC) 의 부분으로서 통합될 수도 있다. 네트워크 는 일반적으로 제 1 디바이스 로부터 제 2 디바이스 로 오디오 및/또는 비디오 데이터 를 송신하기 위한 임의의 적절한 통신 매체, 또는 상이한 통신 매체의 집합을 나타낸다. 네트워크 는 임의의 무선 또는 유선 통신 매체, 이를 테면, 라디오 주파수 (RF) 스펙트럼, 또는 하나 이상의 물리 송신 라인 들, 또는 무선 및 유선 매체의 임의의 조합, WiFi, 위성, 동축 케이블, 전력 라인, 또는 이들의 임의의 조합을 포함할 수 있다. 일부 예들에서, 네트워크 는 패킷 기반 네트워크, 이를 테면, 로컬 영역 네트워크, 광역 네트워크, 또는 글로벌 네트워크, 이를 테면, 인터넷의 부분을 형성할 수 있다. 따라서, 제 1 디바이 스 및 제 2 디바이스 는 IEEE 802.11 표준 패밀리로부터의 표준과 같은 통신 프로토콜을 사용하여 통 신 채널을 통해 통신할 수 있다. 도 1 의 예는 일반적으로 네트워크 를 통한 양방향 오디오 컨퍼런스 세션을 예시한다. 예를 들어, 제 1 디바이스 는 카메라 및 디스플레이 유닛 및 마이크로폰 및 스피커 유닛 을 사용하여 오디오 및/또는 비디오를 캡처하는 것을 담당하는 로컬 디바이스일 수 있다. A/V 프로세싱 유닛 은 오디오 및 /또는 비디오 데이터를 인코딩하거나 달리 압축할 수도 있다. A/V 프로세싱 유닛 은 또한 PS 네트워크 를 통한 송신을 위하여 데이터를 패킷화한다. 제 2 디바이스 에서, A/V 프로세싱 유닛 은 수신된 패킷들을 복조, 디-지터, 디코딩, A/V 싱크, 및/또는 포스트-프로세싱할 수도 있다. 그 후, A/V 프 로세싱 유닛 은 프로세싱된 데이터를 제 2 디바이스 의 사용자로의 재생을 위해 카메라 및 디스플레 이 유닛 및/또는 마이크로폰 및 스피커 유닛 에 전송할 수도 있다. 본 개시의 양태들에 따르면, 아래 보다 자세하게 설명될 바와 같이, 제 1 디바이스 는 오디오 컨퍼런스에 서 참가자로부터 (예를 들어, 마이크로폰 으로부터) 오디오 데이터를 수신하도록 구성될 수도 있다. 제 1 디바이스 는 오디오 데이터의 분석물을 생성하기 위해 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 오디오 데이터를 분석하고, 오디오 데이터의 분석물에 기초하여 참가자의 마이크로폰 및/또는 오디오 데이터를 제어하도록 추가로 구성될 수도 있다. 예를 들어, 디바이스 는, 오디오 데이터의 분석이 오디오 데이터가 참가자의 음성과 매칭하지 않고/않거나 오디오 데이터의 컨텐츠가 회의와의 컨텍스트에서 벗어난다고 (예를 들어, 오디오 데이터의 컨텐츠가 오디오 컨퍼런스의 컨텍스트와 연관 되지 않은 문법을 사용한다고) 결정하면, 마이크로폰 을 자동으로 음소거할 수 있다. 이와 유사하게, 제 1 디바이스 는, 오디오의 분석이 오디오 데이터가 참가자의 음성과 매칭하고 그리고/또는 오디오 컨퍼 런스의 컨텍스트와 매칭한다는 것을 나타내는 경우 마이크로폰 을 자동으로 음소거해제하도록 구성될 수 있다. 본 개시의 기법들은 마이크로폰을 제어 (예를 들어, 음소거 및/또는 음소거해제) 하는 것을 지칭할 수도 있다. 일부 예들에서, 마이크로폰을 음소거하는 것은 마이크로폰 이 더 이상 오디오 데이터를 캡처 및 생성하 지 않도록 마이크로폰 을 디스에이블하는 것을 지칭할 수도 있다. 다른 예들에서, 마이크로폰을 음소 거하는 것은 오디오 데이터가 오디오 컨퍼런스의 다른 참가자들에게 더 이상 가청가능하지 않도록 특정 마이크로폰에 의해 생성된 오디오 데이터를 음소거, 사일런싱, 널링, 볼륨을 (예를 들어, 0으로) 낮추는 것, 및/또는 (예를 들어, 오디오 스트림으로부터) 제거하는 것을 지칭할 수 있다. 이 예에서, \"음소거된\" 마이크로폰은 여전히 동작가능하고, 마이크로폰 및/또는 오디오 데이터를 음소거해제할지 및 언제 음소거해제할지를 결정하기 위한 계속된 분석과 같은 다른 목적들을 위해 사용될 수 있는 오디오 데이터를 계속 캡처 및 생성할 수 있다. 이 맥락에서, 마이크로폰을 제어하는 것은 또한 오디오 데이터를 조정하는 것으로 지칭될 수 있으며, 여기서 오디오 데이터를 조정하는 것은 오디오 데이터를 음소거 또는 음소거해제하는 것을 포함할 수 있다. 본 개시의 기법들은 제 1 디바이스 및 A/V 프로세싱 유닛 을 참조하여 설명될 것이다. 그러나, 디바이스 또는 오디오 컨퍼런스에 참여하는 임의의 다른 디바이스가 본 개시의 기법들을 적용할 수 있다는 것이 이해되어야 한다. 또한, 일부 예들에서, 본 개시의 기법들은 하나 이상의 원격 디바이스들의 마이크로 폰들을 제어하도록 구성된 단일의 중앙 집중식 디바이스 상에서 발생할 수도 있다. 도 2 는 마이크로폰 및/또는 오디오 데이터를 자동으로 음소거하기 위한 기법들을 예시하는 개념 다이어그램이 다. 도 2 는 온 상태인 마이크로폰 (예를 들어, 도 1 의 마이크로폰 ) 을 갖는 참가자 를 가진 오디오 컨퍼런스 (또는 임의의 유형의 가상 회의) 를 예시한다. 이 예에서, 제 1 디바이스 는 참 가자 의 마이크로폰으로부터 수신된 오디오 데이터가 상이한 시나리오들에 대해 음소거되어야 하는지를 결정하도록 구성되는 스피치 카테고리화 알고리즘 을 실행하도록 구성될 수도 있다. 시나리오 1 에서, 스피치 카테고리화 알고리즘 은 오디오 데이터의 컨텐츠가 오디오 컨퍼런스 의 컨텍스트의 컨텍스트 밖에 있다고 결정하도록 구성될 수 있다. 예를 들어, 참가자 는 회의의 컨텍스 트와 상이한 컨텍스트에 있는 단어, 구 또는 다른 문법을 사용하고 있을 수 있다. 컨텍스트에서 오디오 데 이터는 또한 컨텍스트-내 단어들의 리스트에 대한 미리 결정된 히트 레이트를 초과하는 단어들을 포함하는 오디 오 데이터일 수 있다. 시나리오 2 에서, 오디오 데이터는 참가자 (예를 들어, 활성 참가자) 가 말하고 있지 않는 동안, 참가자 이외의 배경 참가자들을 포함한다. 시나리오 3 에서, 오디오 데이터는 참가 자 가 활성 마이크로폰을 갖는 동안 배경 잡음을 포함한다. 시나리오들 각각에서, 제 1 디바이스 는 참가자 의 마이크로폰을 자동으로 음소거하도록 구성될 수 있다. 도 3 은 마이크로폰을 자동으로 음소거해제하기 위한 기법들을 예시하는 개념 다이어그램이다. 도 2 와 유 사하게, 도 3 은 오프 상태인 마이크로폰 (예를 들어, 도 1 의 마이크로폰 ) 을 갖는 참가자 를 가진 오디오 컨퍼런스 (또는 임의의 유형의 가상 회의) 를 예시한다. 이 컨텍스트에서, 마이크로폰이 \"오프\"인 것은 마이크로폰에 의해 캡처된 오디오 데이터가 오디오 컨퍼런스의 다른 참가자들에게 브로드캐스트 되지 않거나 가청되지 않도록 하지만, 마이크로폰이 스피치 카테고리화 알고리즘 에 의한 분석을 위해 오 디오 데이터를 계속 캡처한다는 것을 나타낸다. 이 예에서, 제 1 디바이스 는 참가자 의 마이크 로폰으로부터 수신된 오디오 데이터가 상이한 시나리오들에 대해 음소거해제되어야 하는지를 결정하도록 구성되 는 스피치 카테고리화 알고리즘 을 실행하도록 구성될 수도 있다. 시나리오 1 에서, 스피치 카테고리화 알고리즘 은 오디오 데이터의 컨텐츠가 오디오 컨퍼런스 의 컨텍스트의 컨텍스트 내에 있다고 결정하도록 구성될 수 있다. 예를 들어, 참가자 는 회의의 컨텍스 트와 동일한 컨텍스트에 있는 단어, 구 또는 다른 문법을 사용하고 있을 수 있다. 시나리오 2 에서, 말하고 있었던 배경 참가자들을 이전에 포함하였던 오디오 데이터는 더 이상 배경 참가자들로부터의 오디오를 포함하지 않는다. 즉, 이전에 말하는 배경 참가자가 말하기를 중단하고 올바른 참가자가 지금 말한다. 시나리오 3 에서, 배경 잡음을 이전에 포함하였던 오디오 데이터는 이제 배경 잡음이 없다. 시나리오들 각각에서, 제 1 디바이스 는 참가자 의 마이크로폰을 자동으로 음소거해제하도록 구성될 수 있다. 도 2 및 도 3 에서의 시나리오들은 마이크로폰들 및/또는 오디오 데이터가 음소거되거나 음소거해제될 수 있는 시나리오들의 예들인 것으로 의도되며, 배타적인 것으로 의도되지 않는다. 일반적으로, 제 1 디바이스 는 오디오 데이터가 스피커의 음성을 나타내는지를 결정하기 위해 활성 참가자의 음성 (예를 들어, 스피커 식별) 에 대한 오디오 데이터를 분류하도록 구성될 수 있다. 본 개시의 스피커 식별 기법은 제 1 디바이스 로 하여금, 오디오 데이터가 활성 참가자 이외의 다른 스피커를 나타내고/내거나 오디오 데이터가 잡음 (예를 들어, 임계 데시벨 레벨 위의 잡음) 을 나타내면 마이크로폰을 음소거하게 할 수 있다. 제 1 디바이 스 는 오디오 데이터의 컨텐츠가 컨텍스트를 벗어나는지 여부를 결정하기 위해 오디오 컨퍼런스의 예상 컨 텍스트에 대해 오디오 데이터를 분류하도록 추가로 구성될 수 있다. 본 개시에서, 오디오 컨퍼런스의 컨텍 스트는 오디오 컨퍼런스의 토픽 및/또는 컨텍스트를 나타낼 수 있는 단어들, 구문들, 용어들, 문법, 언어, 또는 다른 데이터의 예상된 세트를 포함할 수 있다.본 개시의 하나의 예에서, 음성 카테고리화 알고리즘 (210 및 310) 은 하나 이상의 인공 지능 및/또는 머신 러 닝 알고리즘을 사용하여 구현될 수 있다. 예시적인 인공 지능 및/또는 머신 러닝 알고리즘들은 딥 러닝 시 스템들, 뉴럴 네트워크들, 및 자연 언어 프로세싱의 사용을 포함하는 다른 유형의 예측 분석 시스템들을 포함할 수 있다. 딥 뉴럴 네트워크들 (DNNs) 을 포함한 인공 뉴럴 네트워크들 (ANNs) 은 분류 툴로서 큰 가능성을 보여 왔다. DNN 은 입력 계층, 출력 계층, 및 입력 계층과 출력 계층 사이의 하나 이상의 은닉 계층들을 포함한다. ANN들 및 DNN들은 또한 풀링 계층들과 같은 하나 이상의 다른 유형들의 계층들을 포함할 수 있다. 각각의 계층은 간단하게 \"뉴런\"으로 종종 지칭되는 인공 뉴런들의 세트를 포함할 수 있다. 입력 계층에서의 각각의 뉴런은 입력 벡터로부터 입력 값을 수신한다. 입력 계층에서의 뉴런들의 출력들은 ANN 에서의 다음 계층에 대한 입력들로서 제공된다. 입력 계층 뒤의 계층의 각각의 뉴런은 뉴런에 대한 입력 값을 생성하기 위해 이전 계층의 하나 이상의 뉴런들의 출력에 전파 함수를 적용할 수 있다. 뉴런은 그 후 활성화 값을 연 산하기 위해 입력에 활성화 함수를 적용할 수 있다. 그 다음, 뉴런은 출력 함수를 활성화 값에 적용하여 뉴 런에 대한 출력 값을 생성할 수 있다. ANN 의 출력 벡터는 ANN 의 출력 계층의 출력 값들을 포함한다. 본 개시의 예들에서, ANN 의 출력 값들은 스피커 식별에 관련된 하나 이상의 분류들 (예를 들어 스피커 분류) 및 오디오 컨퍼런스의 컨텍스트에 관련된 하나 이상의 분류 (예를 들어, 컨텍스트 분류) 를 포함할 수 있다. 위에 설명된 바와 같이, 음성 카테고리화 알고리즘들 (210 및 310) 은 오디오 데이터를 활성 참가자에 속하거 나 활성 참가자에 속하지 않는 것으로 분류하도록 구성될 수 있다. 예를 들어, 오디오 데이터는 다른 사람 의 음성을 나타낼 수도 있거나 잡음을 나타낼 수도 있다. 또한, 스피치 카테고리화 알고리즘들 (210 및 310) 은 오디오 컨퍼런스와의 컨텍스트를 벗어나는 것 또는 컨텍스트 내에 있는 것으로서 오디오 데이터의 컨텐 츠를 분류하도록 구성될 수 있다. 제 1 디바이스 는 하나 이상의 트레이닝 데이터세트들을 사용하여 스피치 카테고리화 알고리즘들 (210 및 310) 을 실행하는 뉴럴 네트워크를 트레이닝하도록 구성될 수 있다. 하나의 예에서, 트레이닝 데이터세트는 오디오 컨퍼런스의 토픽 및/또는 컨텍스트를 나타낼 수 있는 단어들, 구문들, 용어들, 문법, 언어, 또는 다른 데이터의 예상된 세트를 포함할 수 있다. 다른 예들에서, 트레이닝 데이터세트는 참가자의 음성의 등록된 버전을 포함할 수 있다. 각각의 개별적인 트레이닝 데이터세트에 대해, 개별적인 트레이닝 데이터세트의 트레이닝 입력 벡터는 복수의 입력 엘리먼트들의 각각의 엘리먼트에 대한 값을 포함한다. 각각의 개별적인 트레이닝 데이터세트에 대해, 개별적인 트레이닝 데이터세트의 타겟 출력 벡터는 복수의 출력 엘리먼트들의 각각의 엘리먼트에 대한 값을 포 함한다. 이 예에서, 제 1 디바이스 는 스피커 분류 및 컨텍스트 분류 양쪽 모두를 수행하기 위해 뉴럴 네트워크를 트레이닝하도록 복수의 트레이닝 데이터세트들을 사용할 수 있다. 이 예에서, 컴퓨팅 시스템은 참가자의 마이크로폰 (예를 들어, 도 1 의 마이크로폰 ) 으로부터 수신된 오디 오 데이터에 대응하는 현재 입력 벡터를 획득할 수 있다. 제 1 디바이스 는 DNN 을 현재 입력 벡터에 적용하여 현재 출력 벡터를 생성할 수도 있다. 그 다음, 제 1 디바이스 는 현재의 출력 벡터에 기초하 여, 수신된 오디오 데이터의 스피커 분류 및/또는 컨텍스트 분류를 결정할 수도 있다. 그 다음, 제 1 디바 이스 는 출력 분류에 기초하여 마이크로폰을 제어할 수도 있다. 일반적으로, 제 1 디바이스 는, 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하고, 오디오 데이터의 분석물을 생성하기 위해 (예를 들어, 뉴럴 네트워크를 사용하여) 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정 하도록 오디오 데이터를 분석하고, 그리고 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하도록 구성될 수 있다. 도 4 는 도 1 의 디바이스를 보다 자세하게 예시하는 블록 다이어그램이다. 특히, 도 4 는 A/V 프로세싱 유 닛 의 하나의 예시적인 구현을 도시한다. A/V 프로세싱 유닛 은 (예를 들어, 도 1 의 마이크로폰 으로부터) 오디오 데이터 를 수신하도록 구성될 수 있다. A/V 프로세싱 유닛 은 메모리 에 오디오 데이터를 저장할 수도 있다. 스피치 분류 유닛 은 메모리 에 저장된 오디오 데이터를 분석하도록 구성될 수도 있다. 일부 예들에서, 스피치 분류 유닛 은 위에 설명된 인공 지능 및/또는 머신 러닝 기술들 중 하나 이상을 사용하도록 구성될 수 있다. 예를 들어, 스피치 분류 유닛 은 도 2 및 도 3 을 참조하여 위에 설명된 스피치 분류 알고리즘을 수행하기 위해 뉴럴 네트워크를 구현하도록 구성될 수 있다. 일부 예들에서, A/V 프로세싱 유닛 은 마이크로폰을 갖는 디바이스에 로컬이 아닐 수 있지만, 원격으로 위치된 클라우드 디바이스일 수 있다. 도 4 의 예에서, 스피치 분류 유닛 은 스피커 식별 유닛 및 컨텍스트 식별 유닛 양쪽 모두를 포함한다. 스피커 식별 유닛 은 위에 설명된 스피커 분류를 수행하도록 구성될 수 있다. 컨텍스 트 식별 유닛 은 설명된 컨텍스트 분류를 수행하도록 구성될 수 있다. 도 4 는 스피커 식별 유닛 및 컨텍스트 식별 유닛 을 별개의 뉴럴 네트워크를 실행하는 별개의 유닛들로 도시한다. 다른 예들에서, 스피커 식별 유닛 및 컨텍스트 식별 유닛 은 다수의 출력들을 갖는 단일 뉴럴 네트워크 에서 결합될 수 있다. 스피치 분류 유닛 은 음성 등록 데이터 를 사용하여 스피커 식별 유닛 에 의해 실행되는 뉴 럴 네트워크를 트레이닝하도록 구성될 수 있다. 음성 등록 데이터 는 오디오 컨퍼런스의 특정 참가자 및/또는 제 1 디바이스 의 사용자의 음성의 오디오 데이터의 샘플일 수도 있다. 스피커 식별 유닛 은 오디오 데이터가 등록된 참가자의 음성을 나타내는지 여부를 결정하기 위해 오디오 데이터 를 분석하도록 구성될 수 있다. 예를 들어, 스피커 식별 유닛 은 스피커 분류를 결정하기 위해 음성 등 록 데이터 에 대해 오디오 데이터 를 분류할 수도 있다. 하나의 예에서, 스피커 분류는 오디오 데이터가 참가자의 음성을 나타내는지 여부를 나타낼 수 있다. 스피치 분류 유닛 은 스피커 분류를 마이크로폰 제어 유닛 에 전송할 수도 있다. 마이크로폰 제어 유닛 은 스피커 분류에 기초하여 오디오 데이터 가 참가자의 음성을 나타내는지를 결정하도록 구성될 수 있다. 마이크로폰 제어 유닛 은 오디오 데이터가 참가자의 음성을 나타내지 않는다는 결정 에 기초하여 참가자의 마이크로폰을 음소거하도록 구성될 수 있다. 다른 예들에서, 마이크로폰 제어 유닛 은 오디오 데이터가 참가자의 음성을 나타내지 않는다는 결정에 기초하여 참가자의 오디오 데이터를 음소 거하도록 구성될 수 있다. 마이크로폰 제어 유닛 은 오디오 데이터가 참가자의 음성을 나타낸다는 결 정에 기초하여 참가자의 마이크로폰 및/또는 오디오 데이터를 음소거하지 않도록 구성될 수 있다. 마이크로 폰 제어 유닛 은 마이크로폰/오디오 데이터가 음소거되는지를 사용자 인터페이스 제어 유닛 에 통 지할 수도 있다. 그러면, 사용자 인터페이스 제어 유닛은 사용자에게 UI 통지를 전송할 수 있다. UI 통 지는 시각적, 오디오 및/또는 햅틱 통지일 수 있다. 일부 예에서, 참가자는 사용자 인터페이스와의 상호 작 용을 통해 자동 음소거/음소거해제 제어를 오버라이드할 수 있다. 스피치 분류 유닛 은 다른 트레이닝 데이터 를 사용하여 컨텍스트 식별 유닛 에 의해 실행되 는 뉴럴 네트워크를 트레이닝하도록 추가로 구성될 수 있다. 다른 트레이닝 데이터 는 오디오 컨퍼런 스의 토픽 및/또는 컨텍스트를 나타낼 수 있는 단어들, 구문들, 용어들, 문법, 언어, 또는 다른 데이터의 예상 된 세트일 수 있다. 컨텍스트 식별 유닛 은 오디오 데이터의 컨텐츠가 오디오 컨퍼런스의 컨텍스트를 나타내는지 여부를 결정하기 위해 오디오 데이터 를 분석하도록 구성될 수도 있다. 이와 관련하여, 오디오 데이터의 컨텐츠는 오디오 데이터 내에 포함된 실제 단어들, 문구들, 용어들, 언어 등일 수 있다. 하나의 예에서, 컨텍스트 식별 유닛 은 오디오 데이터 의 컨텐츠를 결정하기 위해 자연 언어 프 로세싱 기술들을 사용할 수 있다. 컨텍스트 식별 유닛 은 컨텍스트 분류를 결정하기 위해 다른 트레 이닝 데이터 에 대해 오디오 데이터 를 분류할 수도 있다. 하나의 예에서, 컨텍스트 분류는 오 디오 데이터가 오디오 컨퍼런스의 컨텍스트를 나타내는지 여부를 나타낼 수 있다. 스피치 분류 유닛 은 컨텍스트 분류를 마이크로폰 제어 유닛 에 전송할 수도 있다. 마이크로폰 제어 유닛 은 컨텍스트 분류에 기초하여 오디오 데이터 가 오디오 컨퍼런스의 컨텍스트를 나타내는 지를 결정하도록 구성될 수 있다. 마이크로폰 제어 유닛 은 오디오 데이터가 오디오 컨퍼런스의 컨텍 스트를 나타내지 않는다는 결정에 기초하여 참가자의 마이크로폰 및/또는 오디오 데이터를 음소거하도록 구성될 수 있다. 마이크로폰 제어 유닛 은 오디오 데이터가 오디오 컨퍼런스의 컨텍스트를 나타낸다는 결정 에 기초하여 참가자의 마이크로폰 및/또는 오디오 데이터를 음소거하지 않도록 구성될 수 있다. 마이크로폰 제어 유닛 은 마이크로폰이 음소거되는지를 사용자 인터페이스 제어 유닛 에 통지할 수도 있다. 그러면, 사용자 인터페이스 제어 유닛은 사용자에게 UI 알림을 전송할 수 있다. UI 통지는 시각적, 오디 오 및/또는 햅틱 통지일 수 있다. 일부 예들에서, 스피치 분류 유닛 은 다른 트레이닝 데이터 를 수집하기 위해 오디오 컨퍼런스의 하나 이상의 참가자들로부터 오디오 데이터를 수신하도록 구성될 수 있다. 그 후, 스피치 분류 유닛 은 업데이트된 데이터에 기초하여 컨텍스트 식별 유닛에 의해 실행되는 뉴럴 네트워크를 주기적으로 재트 레이닝시킬 수 있다. 이 방식으로, 컨텍스트 식별 유닛 에 의해 생성된 컨텍스트 분류들의 정밀도가 개선될 수 있다. 마이크로폰들 또는 오디오 데이터를 자동으로 음소거하는 것에 더하여, 본 개시는 또한 A/V 프로세싱 유닛 이 이전에 음소거된 마이크로폰으로부터 오디오 데이터를 자동으로 음소거해제하도록 구성될 수도 있는 특징들 을 설명한다. 이 예에서, 위에서 논의된 바와 같이, 이전에 \"음소거된\" 마이크로폰은 계속 동작하고, 음소 거된 마이크로폰에 의해 캡처된 오디오 데이터는 오디오 컨퍼런스의 다른 참가자들에 대해 사일런싱된다. 일 예에서, 마이크로폰 제어 유닛 은 참가자의 오디오 데이터가 음소거된다고 결정하고, 오디오 데이터 가 참가자의 음성을 나타낸다는 결정에 기초하여 그리고/또는 오디오 데이터 가 오디오 컨퍼런스의 컨텍스트를 나타낸다는 결정에 기초하여 참가자의 오디오 데이터를 음소거해제하도록 구성될 수 있다. 본 개시의 하나의 예에서, 스피치 분류 유닛 의 사용 및 본 개시의 자동 음소거 및 음소거해제 특징들은 턴 온 또는 턴 오프될 수 있는 사용자 선택가능 특징으로서 구성될 수 있다. 도 5 는 마이크로폰을 음소거 및 음소거해제하는 일 예의 기법을 예시하는 플로우차트이다. 위에서 논의된 바와 같이, 마이크로폰들을 음소거해제 및 음소거하는 것은 또한 일반적으로, 마이크로폰이 동작 상태로 유지되 는 (예를 들어, 음소거될 때 오디오 데이터를 여전히 캡처하고 생성하는) 동안 오디오 데이터가 음소거 또는 음 소거해제되도록 오디오 데이터를 조정하는 것을 지칭할 수도 있다. 도 5 의 기법들은 A/V 프로세싱 유닛 을 포함한 디바이스 의 하나 이상의 구조적 컴포넌트들에 의해 수행될 수도 있다. 도 5 의 예에 서, 스피커 분류는 컨텍스트 분류 전에 수행된다. 다른 예들에서, 이 순서가 역으로 될 수 있다. 초기에, A/V 프로세싱 유닛 은 선택적으로 오디오 컨퍼런스의 참가자의 음성을 등록할 수 있다 . 위에 논의된 바와 같이, A/V 프로세싱 유닛 은 등록된 음성을 뉴럴 네트워크에 대한 트레이닝 데이터세트 로서 사용할 수 있다. A/V 프로세싱 유닛 은 오디오 컨퍼런스를 시작하고 참가자의 마이크로폰 으로부터 오디오 데이터를 수집하도록 구성될 수 있다. A/V 프로세싱 유닛 은 그 후 마이크로폰이 음소거되는지를 결정할 수도 있다 . 506 에서 no 이면, A/V 프로세싱 유닛 은 오디오 데이터를 분석하도록 구성될 것이다 . A/V 프로세싱 유닛 은 위에 설명된 인공 지능 기술을 사용하여 오디오 데이터를 분석할 수 있다. A/V 프로세싱 유닛 은 이어서 오디오 데이터가 참가자의 등록된 음성을 나타내는지 여부를 결정할 수 있다 . 510 에서 no 이면, A/V 프로세싱 유닛 은 마이크로폰을 음소거하고 오디오 데이터를 수집하는 것으로 복 귀할 수 있다. 510 에서 yes 이면, A/V 프로세싱 유닛 은 오디오 데이터 컨텐츠가 컨텍스트 내에 있는지를 결정할 수 있 다 . 514 에서 no 이면, A/V 프로세싱 유닛 은 마이크로폰을 음소거하고 오디오 데이터를 수집하는 것으로 복귀할 수 있다. 514 에서 yes 이면, A/V 프로세싱 유닛 은 오디오 데이터를 수집하는 것으로 복귀할 수도 있다. 506 으로 복귀하여, 마이크로폰이 현재 음소거되면, A/V 프로세싱 유닛 은 여전히 오디오 데이터를 분석하 도록 구성될 수 있다 . 이 브랜치에서, A/V 프로세싱 유닛 은 오디오 데이터가 참가자의 등록된 음성을 나타내고 오디오 컨퍼런스와 컨텍스트 내에 있는지 여부를 결정할 수 있다 . 520 에서 yes 이 면, A/V 프로세싱 유닛 은 마이크로폰을 음소거해제하고 오디오 데이터를 수집하는 것으로 복귀할 수 있다. 520 에서 no 이면, A/V 프로세싱 유닛 은 오디오 데이터를 수집하는 것으로 복귀할 수도 있다. 도 6 은 마이크로폰을 음소거 및 음소거해제하는 다른 예의 기법을 예시하는 플로우차트이다. 도 6 의 기법 들은 A/V 프로세싱 유닛 을 포함한 디바이스 의 하나 이상의 구조적 컴포넌트들에 의해 수행될 수도 있다. 일부 예들에서, 도 6 의 기법들은 마이크로폰을 갖는 디바이스와 별개로 위치된 원격 디바이스 (예를 들어, 클라우드 서버) 에 의해 수행될 수 있다. 본 개시의 하나의 예에서, A/V 프로세싱 유닛 은 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신 하도록 구성될 수 있다 . A/V 프로세싱 유닛 은 오디오 데이터의 분석물을 생성하기 위해 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하기 위하여 오디오 데이터를 분석하고 , 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 참가자의 오디오 데이터를 조정하도록 추가로 구성될 수도 있다. 개시의 하나의 예에서, 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 오 디오 데이터를 분석하기 위해, A/V 프로세싱 유닛 은 하나 이상의 인공 지능 기술 또는 머신 러닝 기술을 사용하여 오디오 데이터를 분석하도록 구성된다. 하나의 예에서, 하나 이상의 인공 지능 또는 머신 러닝 기 술은 뉴럴 네트워크를 포함한다. 다른 예에서, 하나 이상의 인공 지능 또는 머신 러닝 기술은 자연 언어 프로세싱을 포함한다. 본 개시의 일부 예들은 마이크로폰들 또는 오디오 데이터의 자동 음소거에 관한 것이다. 하나의 예에서, 오 디오 데이터의 스피커를 결정하도록 오디오 데이터를 분석하기 위해, A/V 프로세싱 유닛 은 스피커 분류를 결정하기 위해 참가자의 음성의 등록된 버전에 대해 오디오 데이터를 분류하도록 구성된다. 일 예에서, 참 가자의 음성의 등록된 버전은 뉴럴 네트워크에 대한 트레이닝 데이터로서 사용된다. A/V 프로세싱 유닛 은 스피커 분류에 기초하여 오디오 데이터가 참가자의 음성을 나타내는지를 결정하도록 추가로 구성될 수 있다. 하나의 예에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 참가자의 오디오 데이터를 조정하 기 위해, A/V 프로세싱 유닛 은 오디오 데이터가 참가자의 음성을 나타내지 않는다는 결정에 기초하여 마 이크로폰을 음소거하거나 참가자의 오디오 데이터를 음소거하도록 구성된다. 다른 예에서, A/V 프로세싱 유 닛 은 오디오 데이터가 참가자의 음성을 나타낸다는 결정에 기초하여 마이크로폰을 음소거하지 않거나 참 가자의 오디오 데이터를 음소거하지 않도록 구성된다. 다른 예에서, 오디오 데이터의 컨텍스트를 결정하도록 오디오 데이터를 분석하기 위해, A/V 프로세싱 유닛 은 컨텍스트 분류를 결정하기 위해 트레이닝 데이터에 대해 오디오 데이터의 컨텐츠를 분류하고, 그리고 오디오 데이터가 컨텍스트 분류에 기초하여 오디오 컨퍼런스의 컨텍스트를 나타내는지를 결정하도록 구성된다. 하 나의 예에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 참가자의 오디오 데이터를 조정하기 위해, A/V 프로세싱 유닛 은 오디오 데이터가 오디오 컨퍼런스의 컨텍스트를 나타내지 않는다는 결정에 기 초하여 마이크로폰을 음소거하거나 참가자의 오디오 데이터를 음소거하도록 구성된다. 다른 예에서, A/V 프 로세싱 유닛 은 오디오 데이터가 오디오 컨퍼런스의 컨텍스트를 나타낸다는 결정에 기초하여 마이크로폰을 음소거하지 않거나 참가자의 오디오 데이터를 음소거하지 않도록 구성된다. A/V 프로세싱 유닛 은 참가자의 등록된 음성 및/또는 오디오 컨퍼런스의 컨텍스트를 나타내는 문법을 포함 하는 트레이닝 데이터를 사용하여 뉴럴 네트워크를 트레이닝하도록 구성될 수도 있다. 그 다음, A/V 프로세 싱 유닛 은 트레이닝된 뉴럴 네트워크를 사용하여 스피커 식별 및/또는 컨텍스트를 위해 오디오 데이터를 분류할 수도 있다. 본 개시의 일부 예들은 마이크로폰들 또는 오디오 데이터의 자동 음소거해제에 관한 것이다. 하나의 예에서, 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하기 위해 오디오 데이터 를 분석하기 위하여, A/V 프로세싱 유닛 은 스피커 분류를 결정하기 위해 참가자의 음성의 등록된 버전에 대해 오디오 데이터를 분류하고, 스피커 분류에 기초하여 오디오 데이터가 참가자의 음성을 나타내는지를 결정 하도록 구성된다. 다른 예에서, A/V 프로세싱 유닛 은 컨텍스트 분류를 결정하기 위해 트레이닝 데이 터에 대해 오디오 데이터의 컨텐츠를 분류하고, 그리고 오디오 데이터가 컨텍스트 분류에 기초하여 오디오 컨퍼 런스의 컨텍스트를 나타내는지를 결정하도록 구성될 수 있다. 다른 예들에서, A/V 프로세싱 유닛 은 스피커 분류 및 컨텐츠 분류 양쪽 모두를 결정하도록 구성될 수도 있다. A/V 프로세싱 유닛 은 참가자의 오디오 데이터가 음소거된다고 결정하고, 오디오 데이터가 참가자의 음성 을 나타낸다는 결정에 기초하여 그리고/또는 오디오 데이터가 오디오 컨퍼런스의 컨텍스트를 나타낸다는 결정에 기초하여 참가자의 오디오 데이터를 음소거해제하도록 추가로 구성될 수 있다. 본 개시의 디바이스들, 방법들, 및 기법들의 다른 양태들이 이하에서 설명된다. 양태 1 - 오디오 컨퍼런스를 제어하도록 구성되는 장치는, 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하도록 구성된 메모리; 및 메모리와 통신하는 하나 이상의 프로세서들을 포함하고, 하나 이상의 프로세서들 은, 오디오 데이터의 분석물을 생성하기 위해 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 오디오 데이터를 분석하고; 그리고 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하 거나 또는 참가자의 오디오 데이터를 조정하도록 구성된다. 양태 2 - 양태 1 의 장치에서, 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하 도록 오디오 데이터를 분석하기 위해, 하나 이상의 프로세서들은 오디오 데이터의 분석물을 생성하기 위해 하나 이상의 인공 지능 기술 또는 머신 러닝 기술을 사용하여 오디오 데이터를 분석하도록 구성된다. 양태 3 - 양태 2 의 장치에서, 하나 이상의 인공 지능 또는 머신 러닝 기술은 뉴럴 네트워크를 포함한다. 양태 4 - 양태 2 의 장치에서, 하나 이상의 인공 지능 또는 머신 러닝 기술은 자연 언어 프로세싱을 포함한다. 양태 5 - 양태들 1-4 의 어느 하나의 장치에서, 오디오 데이터의 스피커를 결정하도록 오디오 데이터를 분석하 기 위해, 하나 이상의 프로세서들은 또한 스피커 분류를 결정하기 위해 참가자의 음성의 등록된 버전에 대한 오 디오 데이터를 분류하고; 그리고 오디오 데이터가 스피커 분류에 기초하여 참가자의 음성을 나타내는지를 결정 하도록 구성된다. 양태 6 - 양태 5 의 장치에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 참가자의 오디오 데이터를 조정하기 위해, 하나 이상의 프로세서는 오디오 데이터가 참가자의 음성을 나타내지 않는다는 결정에 기초하여 마이크로폰을 음소거하거나 참가자의 오디오 데이터를 음소거하도록 구성된다. 양태 7 - 양태 5 의 장치에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 참가자의 오디오 데이터를 조정하기 위해, 하나 이상의 프로세서는 오디오 데이터가 참가자의 음성을 나타낸다는 결정에 기초하 여 마이크로폰을 음소거하지 않거나 참가자의 오디오 데이터를 음소거하지 않도록 구성된다. 양태 8 - 양태 5 의 장치에서, 하나 이상의 프로세서들은 참가자의 음성의 등록된 버전을 사용하여 뉴럴 네트워 크를 트레이닝하도록 구성되고, 그리고 오디오 데이터를 분류하기 위해, 하나 이상의 프로세서들은 뉴럴 네트워 크를 사용하여 오디오 데이터를 분류하도록 구성된다. 양태 9 - 양태들 1-8 의 어느 하나의 장치에서, 오디오 데이터의 컨텍스트를 결정하도록 오디오 데이터를 분석 하기 위해, 하나 이상의 프로세서들은 또한 컨텍스트 분류를 결정하기 위해 트레이닝 데이터에 대해 오디오 데 이터의 컨텐츠를 분류하고; 그리고 오디오 데이터가 컨텍스트 분류에 기초하여 오디오 컨퍼런스의 컨텍스트를 나타내는지를 결정하도록 구성된다. 양태 10 - 양태 9 의 장치에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 참가자의 오디오 데이터를 조정하기 위해, 하나 이상의 프로세서는 오디오 데이터가 오디오 컨퍼런스의 컨텍스트를 나타내지 않 는다는 결정에 기초하여 마이크로폰을 음소거하거나 참가자의 오디오 데이터를 음소거하도록 구성된다. 양태 11 - 양태 9 의 장치에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 참가자의 오디오 데이터를 조정하기 위해, 하나 이상의 프로세서는 오디오 데이터가 오디오 컨퍼런스의 컨텍스트를 나타낸다는 결정에 기초하여 마이크로폰을 음소거하지 않거나 참가자의 오디오 데이터를 음소거하지 않도록 구성된다. 양태 12 - 양태 9 의 장치에서, 하나 이상의 프로세서들은 트레이닝 데이터를 사용하여 뉴럴 네트워크를 트레이 닝하도록 구성되고, 트레이닝 데이터는 오디오 컨퍼런스의 컨텍스트를 나타내는 문법을 포함하고, 오디오 데이 터를 분류하기 위해, 하나 이상의 프로세서들은 뉴럴 네트워크를 사용하여 오디오 데이터를 분류하도록 구성된 다. 양태 13 - 양태들 1-12 의 어느 하나의 장치에서, 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 오디오 데이터를 분석하기 위해, 하나 이상의 프로세서들은 또한 스피커 분류를 결정하 기 위해 참가자의 음성의 등록된 버전에 대한 오디오 데이터를 분류하고; 오디오 데이터가 스피커 분류에 기초 하여 참가자의 음성을 나타내는지를 결정하고; 컨텍스트 분류를 결정하기 위해 트레이닝 데이터에 대해 오디오 데이터의 컨텐츠를 분류하고; 그리고 오디오 데이터가 컨텍스트 분류에 기초하여 오디오 컨퍼런스의 컨텍스트를 나타내는지를 결정하도록 구성된다. 양태 14 - 양태 13 의 장치에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 참가자의 오디오 데이터를 조정하기 위해, 하나 이상의 프로세서는 참가자의 오디오 데이터가 음소거된다고 결정하고; 그리고 오 디오 데이터가 참가자의 음성을 나타낸다는 결정에 기초하여 그리고 오디오 데이터가 오디오 컨퍼런스의 컨텍스 트를 나타낸다는 결정에 기초하여 참가자의 오디오 데이터를 음소거해제하도록 구성된다. 양태 15 - 오디오 컨퍼런스를 제어하기 위한 방법은, 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신 하는 단계; 오디오 데이터의 분석물을 생성하기 위해 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 오디오 데이터를 분석하는 단계; 및 오디오 데이터의 분석물에 기초하여 마이크로폰 을 제어하거나 또는 참가자의 오디오 데이터를 조정하는 단계를 포함한다. 양태 16 - 양태 15 의 방법에서, 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정 하도록 오디오 데이터를 분석하는 단계는 오디오 데이터의 분석물을 생성하기 위해 하나 이상의 인공 지능 기술 또는 머신 러닝 기술을 사용하여 오디오 데이터를 분석하는 단계를 포함한다. 양태 17 - 양태 16 의 방법에서, 하나 이상의 인공 지능 또는 머신 러닝 기술은 뉴럴 네트워크를 포함한다. 양태 18 - 양태 16 의 방법에서, 하나 이상의 인공 지능 또는 머신 러닝 기술은 자연 언어 프로세싱을 포함한다. 양태 19 - 양태들 15-18 의 어느 하나의 방법에서, 오디오 데이터의 스피커를 결정하도록 오디오 데이터를 분석 하는 단계는 스피커 분류를 결정하기 위해 참가자의 음성의 등록된 버전에 대한 오디오 데이터를 분류하는 단계; 및 오디오 데이터가 스피커 분류에 기초하여 참가자의 음성을 나타내는지를 결정하는 단계를 포함한다. 양태 20 - 양태 19 의 방법에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하는 단계는 오디오 데이터가 참가자의 음성을 나타내지 않는다는 결정에 기초하여 마이크 로폰을 음소거하거나 참가자의 오디오 데이터를 음소거하는 단계를 포함한다. 양태 21 - 양태 19 의 방법에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하는 단계는 오디오 데이터가 참가자의 음성을 나타낸다는 결정에 기초하여 마이크로폰을 음소거하지 않거나 참가자의 오디오 데이터를 음소거하지 않는 단계를 포함한다. 양태 22 - 양태 19 의 방법은, 참가자의 음성의 등록된 버전을 사용하여 뉴럴 네트워크를 트레이닝하는 단계를 더 포함하고, 오디오 데이터를 분류하는 단계는 뉴럴 네트워크를 사용하여 오디오 데이터를 분류하는 단계를 포 함한다. 양태 23 - 양태들 15-22 의 어느 하나의 방법은, 오디오 데이터의 컨텍스트를 결정하기 위해 오디오 데이터를 분석하는 단계는 컨텍스트 분류를 결정하기 위해 트레이닝 데이터에 대해 오디오 데이터의 컨텐츠를 분류하는 단계; 및 오디오 데이터가 컨텍스트 분류에 기초하여 오디오 컨퍼런스의 컨텍스트를 나타내는지를 결정하는 단 계를 포함한다. 양태 24 - 양태 23 의 방법에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하는 단계는 오디오 데이터가 오디오 컨퍼런스의 컨텍스트를 나타내지 않는다는 결정에 기 초하여 마이크로폰을 음소거하거나 참가자의 오디오 데이터를 음소거하는 단계를 포함한다. 양태 25 - 양태 23 의 방법에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하는 단계는 오디오 데이터가 오디오 컨퍼런스의 컨텍스트를 나타낸다는 결정에 기초하여 마이크로폰을 음소거하지 않거나 참가자의 오디오 데이터를 음소거하지 않는 단계를 포함한다. 양태 26 - 양태 23 의 방법은, 트레이닝 데이터를 사용하여 뉴럴 네트워크를 트레이닝하는 단계를 더 포함하고, 트레이닝 데이터는 오디오 컨퍼런스의 컨텍스트를 나타내는 문법을 포함하고, 오디오 데이터를 분류하는 단계는 뉴럴 네트워크를 사용하여 오디오 데이터를 분류하는 단계를 포함한다. 양태 27 - 양태들 15-26 의 어느 하나의 방법에서, 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 오디오 데이터를 분석하는 단계는 스피커 분류를 결정하기 위해 참가자의 음성의 등록 된 버전에 대한 오디오 데이터를 분류하는 단계; 오디오 데이터가 스피커 분류에 기초하여 참가자의 음성을 나 타내는지를 결정하는 단계; 컨텍스트 분류를 결정하기 위해 트레이닝 데이터에 대해 오디오 데이터의 컨텐츠를 분류하는 단계; 및 오디오 데이터가 컨텍스트 분류에 기초하여 오디오 컨퍼런스의 컨텍스트를 나타내는지를 결 정하는 단계를 포함한다. 양태 28 - 양태 27 의 방법에서, 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하는 단계는 참가자의 오디오 데이터가 음소거된다고 결정하는 단계; 및 오디오 데이터가 참가자의 음성을 나타낸다는 결정에 기초하여 그리고 오디오 데이터가 오디오 컨퍼런스의 컨텍스트를 나타낸다 는 결정에 기초하여 참가자의 오디오 데이터를 음소거해제하는 단계를 포함한다. 양태 29 - 명령들을 저장한 비일시적 컴퓨터 판독가능 저장 매체로서, 명령들은, 실행될 때, 하나 이상의 프로 세서들로 하여금 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하게 하고, 오디오 데이터의 분석물을 생성하기 위해 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 오디오 데이 터를 분석하게 하고; 그리고 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디 오 데이터를 조정하게 한다. 양태 30 - 오디오 컨퍼런스를 제어하도록 구성되는 장치는, 오디오 컨퍼런스에서 참가자로부터 오디오 데이터를 수신하기 위한 수단; 오디오 데이터의 분석물을 생성하기 위해 오디오 데이터의 스피커 또는 오디오 데이터의 컨텍스트 중 하나 이상을 결정하도록 오디오 데이터를 분석하기 위한 수단; 및 오디오 데이터의 분석물에 기초하여 마이크로폰을 제어하거나 또는 참가자의 오디오 데이터를 조정하기 위한 수단을 포함한다. 하나 이상의 예들에서, 본 명세서에서 설명된 기능들 및 기법들은 하드웨어, 소프트웨어, 펌웨어, 또는 이들의 임의의 조합에서 구현될 수도 있다. 소프트웨어로 구현되는 경우, 그 기능들 및 기법들은 하나 이상의 명령 들 또는 코드로서, 컴퓨터-판독가능 매체 상에 저장되거나 또는 컴퓨터-판독가능 매체를 통해서 송신될 수도 있 으며, 하드웨어-기반의 프로세싱 유닛에 의해 실행될 수도 있다. 컴퓨터 판독가능 매체는 데이터 저장 매체 와 같은 유형의 매체에 대응하는 컴퓨터 판독가능 판독 매체, 또는 예를 들어, 통신 프로토콜에 따라, 일 장소 로부터 다른 장소로의 컴퓨터 프로그램의 전송을 가능하게 하는 임의의 매체를 포함하는 통신 매체를 포함할 수 도 있다. 이러한 방식으로, 컴퓨터 판독가능 매체들은 일반적으로 비일시적인 유형의 컴퓨터 판독가능 저장 매체들 또는 신호 또는 캐리어파와 같은 통신 매체에 대응할 수도 있다. 데이터 저장 매체들은 본 개시에서 설명된 기법들의 구현을 위한 명령들, 코드 및/또는 데이터 구조들을 취출하기 위해 하나 이상의 컴퓨 터들 또는 하나 이상의 프로세서들에 의해 액세스될 수 있는 임의의 가용 매체들일 수도 있다. 컴퓨터 프로 그램 제품이 컴퓨터 판독가능 매체를 포함할 수도 있다. 한정이 아닌 예시로서, 이러한 컴퓨터 판독가능 저장 매체들은 RAM, ROM, EEPROM, CD-ROM 또는 다른 광학 디스 크 저장, 자기 디스크 저장, 또는 다른 자기 저장 디바이스들, 플래시 메모리, 또는 명령들 또는 데이터 구조들 의 형태로 희망하는 프로그램 코드를 저장하기 위해 이용될 수 있으며 컴퓨터에 의해 액세스될 수 있는 임의의 다른 매체를 포함할 수 있다. 또한, 임의의 접속이 컴퓨터 판독가능 매체로 적절히 불린다. 예를 들어, 명령들이 동축 케이블, 광섬유 케이블, 트위스티드 페어, 디지털 가입자 라인 (DSL), 또는 적외선, 무선, 및 마 이크로파와 같은 무선 기술들을 사용하여 웹사이트, 서버, 또는 다른 원격 소스로부터 송신되면, 동축 케이블, 광섬유 케이블, 트위스티드 페어, DSL, 또는 적외선, 무선, 및 마이크로파와 같은 무선 기술들은 매체의 정의에 포함된다. 하지만, 컴퓨터 판독가능 저장 매체들 및 데이터 저장 매체들은 커넥션들, 캐리어파들, 신호들, 또는 다른 일시적 매체들을 포함하지 않지만 대신 비일시적인 유형의 저장 매체들로 지향됨이 이해되어야 한다. 본원에서 이용된 디스크 (disk) 와 디스크 (disc) 는, 컴팩트 디스크(CD), 레이저 디스크, 광학 디스크, 디 지털 다기능 디스크 (DVD), 플로피 디스크, 및 블루레이 디스크를 포함하며, 여기서 디스크 (disk) 들은 통상 자기적으로 데이터를 재생하는 반면, 디스크(disc) 들은 레이저들을 이용하여 광학적으로 데이터를 재생한다. 또한, 상기의 조합들은 컴퓨터 판독가능 매체들의 범위 내에 포함되어야 한다. 명령들은 하나 이상의 프로세서, 예컨대 하나 이상의 디지털 신호 프로세서 (DSP), 범용 마이크로프로세서, 주 문형 집적 회로 (ASIC), 필드 프로그램가능 로직 어레이 (FPGA), 또는 다른 등가의 집적 또는 이산 로직 회로에 의해 실행될 수도 있다. 따라서, 본원에 사용된 용어 \"프로세서\" 는 전술한 구조 중 임의의 것 또는 본원에 설명된 기법들의 구현에 적합한 임의의 다른 구조를 지칭할 수도 있다. 추가로, 일부 양태들에서, 본 명세 서에서 설명된 기능성은 인코딩 및 디코딩을 위해 구성되는 또는 결합된 코덱에 통합되는 전용 하드웨어 및/또 는 소프트웨어 유닛들 또는 모듈들 내에서 제공될 수도 있다. 또한, 기법들은 하나 이상의 회로들 또는 로 직 엘리먼트들에서 완전히 구현될 수도 있을 것이다. 본 개시의 기법들은 무선 핸드셋, 집적 회로 (IC) 또는 IC들의 세트 (예를 들면, 칩 세트) 를 포함하는, 매우 다양한 디바이스들 또는 장치들에서 구현될 수도 있다. 다양한 컴포넌트들, 모듈들, 또는 유닛들은 개시된 기법들을 수행하도록 구성된 디바이스들의 기능적 양태들을 강조하기 위해 본 개시에 설명되지만, 상이한 하드 웨어 유닛들에 의한 실현을 반드시 요구하는 것은 아니다. 오히려, 상기 설명된 바와 같이, 다양한 유닛들 은 코덱 하드웨어 유닛에서 결합되거나 또는 적합한 소프트웨어 및/또는 펌웨어와 함께, 상기 설명된 바와 같은 하나 이상의 프로세서들을 포함하는, 상호운용가능한 하드웨어 유닛들의 콜렉션에 의해 제공될 수도 있다. 다양한 예들이 설명되었다. 이들 및 다른 예들은 다음 청구항들의 범위 이내에 있다. 도면 도면1 도면2 도면3 도면4 도면5 도면6"}
{"patent_id": "10-2024-7006947", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1 은 본 개시의 기법들에 일치하는, 제 1 디바이스와 제 2 디바이스 사이의 비디오 전화 (VT) 세션을 예시하 는 블록 다이어그램이다. 도 2 는 마이크로폰 및/또는 오디오 데이터를 자동으로 음소거하기 위한 기법들을 예시하는 개념 다이어그램이 다. 도 3 은 마이크로폰 및/또는 오디오 데이터를 자동으로 음소거해제하기 위한 기법들을 예시하는 개념 다이어그 램이다. 도 4 는 도 1 의 디바이스를 보다 자세하게 예시하는 블록 다이어그램이다. 도 5 는 마이크로폰 및/또는 오디오 데이터를 음소거 및 음소거해제하는 일 예의 기법을 예시하는 플로우차트이 다. 도 6 은 마이크로폰 및/또는 오디오 데이터를 음소거 및 음소거해제하는 다른 예의 기법을 예시하는 플로우차트 이다."}
