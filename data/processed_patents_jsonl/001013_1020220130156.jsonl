{"patent_id": "10-2022-0130156", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0050585", "출원번호": "10-2022-0130156", "발명의 명칭": "이동통신 시스템에서의 스케줄링 방법 및 장치", "출원인": "세종대학교산학협력단", "발명자": "이현석"}}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "제1타임 슬롯에서의 스케줄링 상태 정보와, 강화 학습 모델의 타겟 스케줄링 정책 모델을 이용하여, 무선 자원을 스케줄링하는 단계; 및상기 스케줄링을 통해 획득된 경험 정보를 이용하여, 상기 타겟 스케줄링 정책 모델에 대한 학습을 수행하는 단계를 포함하며,상기 경험 정보는상기 제1타임 슬롯에서의 스케줄링 상태 정보, 스케줄링 액션 정보, 보상값 및 제2타임 슬롯에서의 스케줄링 상태 정보를 포함하는 이동통신 시스템에서의 스케줄링 방법."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 무선 자원을 스케줄링하는 단계는CU(Centralized Unit)에 할당된 DU(Distributed Unit)에서 수행되며,스케줄링 정책 모델에 대한 학습을 수행하는 단계는상기 CU에서 수행되는이동통신 시스템에서의 스케줄링 방법."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서,상기 타겟 스케줄링 정책 모델은미리 설정된 스케줄링 조건에 따라 학습된 복수의 스케줄링 정책 모델 중 하나인이동통신 시스템에서의 스케줄링 방법."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3항에 있어서,상기 타겟 스케줄링 정책 모델은상기 DU의 스케줄링 조건에 따라서, 상기 DU로 제공되는이동통신 시스템에서의 스케줄링 방법."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 3항에 있어서,공개특허 10-2024-0050585-3-상기 보상값은상기 스케줄링 조건에 따라 결정되는 값인이동통신 시스템에서의 스케줄링 방법."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 3항에 있어서,상기 스케줄링 조건은URLLC, 공평성, 총합 전송률 최대화 또는 QoE 를 포함하는 이동통신 시스템에서의 스케줄링 방법."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1항에 있어서,상기 타겟 스케줄링 정책 모델은상기 제1타임 슬롯에서의 스케줄링 상태 정보를 입력받아, 상기 제1타임 슬롯에서의 스케줄링 액션 정보를 출력하는 신경망 모델인이동통신 시스템에서의 스케줄링 방법."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서,상기 타겟 스케줄링 정책 모델에 대한 학습을 수행하는 단계는상기 보상값의 누적값이 최대가 되도록, 상기 타겟 스케줄링 정책 모델에 대한 학습을 수행하는이동통신 시스템에서의 스케줄링 방법."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1타임 슬롯에서의 스케줄링 상태 정보와, 강화 학습 모델의 타겟 스케줄링 정책 모델을 이용하여, 무선 자원을 스케줄링하는 DU; 및스케줄링을 통해 획득된 경험 정보를 상기 DU로부터 제공받아, 상기 타겟 스케줄링 정책 모델에 대한 학습을 수행하고, 상기 타겟 스케줄링 정책 모델을 상기 DU로 제공하는 CU를 포함하는 이동통신 시스템에서의 스케줄링 장치."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "상기 경험 정보는상기 제1타임 슬롯에서의 스케줄링 상태 정보, 스케줄링 액션 정보, 보상값 및 제2타임 슬롯에서의 스케줄링 상태 정보를 포함하는 이동통신 시스템에서의 스케줄링 장치.공개특허 10-2024-0050585-4-청구항 11 제 10항에 있어서,상기 스케줄링 상태 정보는상기 무선 자원의 스케줄링을 위한 기지국의 상태 정보를 포함하며,상기 스케줄링 액션 정보는상기 무선 자원의 할당 정보를 포함하는이동통신 시스템에서의 스케줄링 장치."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 10항에 있어서,상기 타겟 스케줄링 정책 모델은미리 설정된 스케줄링 조건에 따라 상기 CU에서 학습된 복수의 스케줄링 정책 모델 중 적어도 하나인이동통신 시스템에서의 스케줄링 장치."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12항에 있어서,상기 CU는상기 DU의 스케줄링 조건에 따라서, 복수의 스케줄링 정책 모델 중에서, 상기 타겟 스케줄링 정책 모델을 선택하여 상기 DU로 제공하는 스케줄러 코디네이터를 포함하는 이동통신 시스템에서의 스케줄링 장치."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 12항에 있어서,상기 DU는상기 DU의 스케줄링 조건을 나타내는 식별 정보와 함께 상기 경험 정보를 상기 CU로 제공하는이동통신 시스템에서의 스케줄링 장치."}
{"patent_id": "10-2022-0130156", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 12항에 있어서,상기 보상값은상기 스케줄링 조건에 따라 결정되는 값인이동통신 시스템에서의 스케줄링 장치."}
{"patent_id": "10-2022-0130156", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "5G 이동통신 시스템의 기능 분리 환경에서의 강화 학습 기반 스케줄링 방법 및 장치가 개시된다. 개시된 이동통 신 시스템에서의 스케줄링 방법은 제1타임 슬롯에서의 스케줄링 상태 정보와, 강화 학습 모델의 타겟 스케줄링 정책 모델을 이용하여, 무선 자원을 스케줄링하는 단계; 및 상기 스케줄링을 통해 획득된 경험 정보를 이용하여, 상기 타겟 스케줄링 정책 모델에 대한 학습을 수행하는 단계를 포함하며, 상기 경험 정보는 상기 제1타임 슬롯에 서의 스케줄링 상태 정보, 스케줄링 액션 정보, 보상값 및 제2타임 슬롯에서의 스케줄링 상태 정보를 포함한다."}
{"patent_id": "10-2022-0130156", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 이동통신 시스템에서의 스케줄링 방법 및 장치에 관한 것으로서 더욱 상세하게는 5G 이동통신 시스템 의 가지국 기능 분리 환경에서의 강화 학습 기반 스케줄링 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2022-0130156", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "5G 이동통신 시스템에서는, 기지국을 CU(Central Unit or Centralized Unit), DU(Digital Unit or Distributed Unit) 및 RU(Radio Unit)와 같은 세부 유닛으로 구성하고, 기지국의 기능을 분할하여 CU, DU 및 RU에서 서로 다 른 기능들을 담당하게 하는 기능 분할 환경이 고려되고 있다. RU는, 실제 무선 신호를 안테나로 전송하는 무선 하드웨어 유닛을 의미하며, PHY 계층 일부 및 디지털 빔포밍 등의 기능을 담당한다. DU는 RLC, MAC, 및 PHY 계층의 일부 기능을 담당하며, RRC SDAP, PDCP 제어 계층과 같은 상위 계층 기능을 담당한다. CU는 복수의 DU를 관리하며, 클라우드 형태로 구현될 수 있다. 한편, 최근 인공지능 기술의 발달에 따라서, 인공지능 모델에 기반한 스케줄링 기법에 대한 관심이 높아지고 있 다. 이동통신 시스템에서의 스케줄링 문제는 대표적인 순차적 의사결정(sequential decision-making) 문제로써, 이 같은 문제를 해결할 수 있는 대표적인 기계학습 기법인 심층 강화 학습을 이용한 스케줄링 방법의 연구가 다 수 진행되고 있다. 심층 강화 학습을 활용한 스케줄링 방법은, 스케줄링 정책이 스케쥴링 목적을 달성하기 위한 최적 정책으로 수 렴하도록, 스케줄링 정책 모델에 대한 학습을 통해 이루어진다. 스케줄링 정책 모델을 통해 획득된 스케줄링 액 션 정보를 통해 스케줄링이 수행된다. 기존의 강화 학습 기반의 스케줄링 방법은, 각 기지국별로 스케줄링 정책 모델에 대한 학습이 수행되고, 각 기 지국별로 스케줄링 정책 모델을 이용해 스케줄링이 수행되었다. 하지만 이와 같은 방법은, 5G 이동통신 시스템 의 기능 분리 환경에 적절치 않다. CU에서 CU에 할당된 모든 DU에 대한 스케줄링 정책 모델을 학습하거나, DU에 서 각각 스케줄링 정책 모델을 학습하는 경우, CU와 DU에서의 계산 복잡도가 증가되고, CU와 DU 사이의 협력적 학습이 어려워지는 문제가 있다. 이에 5G 이동통신 시스템의 기능 분리 환경을 고려한 강화 학습 기반의 스케줄링 방법이 요구된다. 관련 선행문헌으로 대한민국 공개특허 제2022-0063468호, 제2022-0112890호, 제2021-0103599호, 제2021- 0056093호가 있다."}
{"patent_id": "10-2022-0130156", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 5G 이동통신 시스템의 기능 분리 환경에서의 강화 학습 기반 스케줄링 방법 및 장치를 제공하기 위한 것이다. 또한 본 발명은 CU와 DU의 협력적인 학습을 이용하는 스케줄링 방법 및 장치를 제공하기 위한 것이다. 또한 본 발명은 스케줄링을 위한 학습 수행이 CU 및 DU 중 하나에 집중되지 않고 CU 및 DU에 분산될 수 있는 스 케줄링 방법 및 장치를 제공하기 위한 것이다."}
{"patent_id": "10-2022-0130156", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명의 일 실시예에 따르면, 제1타임 슬롯에서의 스케줄링 상태 정보와, 강화 학습 모델의 타겟 스케줄링 정책 모델을 이용하여, 무선 자원을 스케줄링하는 단계; 및 상기 스케줄링을 통해 획득된 경험 정보를 이용하여, 상기 타겟 스케줄링 정책 모델에 대한 학습을 수행하는 단계를 포함하며, 상기 경험 정보는 상기 제1타임 슬롯에서의 스케줄링 상태 정보, 스케줄링 액션 정보, 보상값 및 제2타임 슬롯에서의 스케줄링 상태 정보를 포함하며, 상기 경험 정보는 상기 제1타임 슬롯에서의 스케줄링 상태 정보, 스케줄링 액션 정보, 보상값 및 제2타임 슬롯에서의 스케줄링 상태 정보를 포함하는 이동통신 시스템에서의 스케줄링 방법 이 제공된다. 또한 상기한 목적을 달성하기 위한 본 발명의 다른 실시예에 따르면, 제1타임 슬롯에서의 스케줄링 상태 정보와, 강화 학습 모델의 타겟 스케줄링 정책 모델을 이용하여, 무선 자원을 스케줄링하는 DU; 및 스케줄링을 통해 획득된 경험 정보를 상기 DU로부터 제공받아, 상기 타겟 스케줄링 정책 모델에 대한 학습을 수행하고, 상 기 타겟 스케줄링 정책 모델을 상기 DU로 제공하는 CU를 포함하는 이동통신 시스템에서의 스케줄링 장치가 제공 된다."}
{"patent_id": "10-2022-0130156", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일실시예에 따르면, DU에서 학습에 필요한 경험 정보가 획득되고, CU에서 경험 정보 기반의 학습이 수행됨으로써, 스케줄링을 위한 학습 수행이 CU 및 DU 중 하나에 집중되지 않고 CU 및 DU에 분산될 수 있다. 또한 본 발명의 일실시예에 따르면, DU의 스케줄링 수행을 위한 스케줄링 정책 모델이 CU에서 통합적으로 학습 및 관리되고 CU를 통해 DU로 배포됨으로써, DU 별로 스케줄링 정책 모델을 학습하기 위해 소요되는 연산이 줄어 들 수 있다."}
{"patent_id": "10-2022-0130156", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 각 도면을 설명하면서 유사한 참조부호를 유사한 구성요소에 대해 사용하였다. 본 발명은, 기지국이 CU와 DU로 기능 분리된 환경에서, 스케줄링에 필요한 연산이 CU 및 DU 중 하나에 집중되지 않고 분산될 수 있는 스케줄링 방법 및 장치를 제안한다. 본 발명의 일실시예는 강화 학습 모델의 스케줄링 정책 모델을 이용하여, 스케줄링을 수행하는데, 이 때 스케줄 링 정책 모델에 대한 학습은 CU에서 수행하고, 학습에 필요한 데이터는 DU에서 수집한다. 즉, 본 발명의 일실시 예는 CU와 DU의 협력적 학습을 통해 스케줄링을 수행하고, 따라서 본 발명의 일실시예에 따르면 학습을 위한 연 산이 CU 및 DU 중 하나에 집중되지 않고 CU 및 DU에 분산될 수 있다. 스케줄링 정책 모델은, 스케줄링 조건별로 생성될 수 있으며, CU는 스케줄링 정책 모델을 통합적으로 관리하고, DU의 스케줄링 조건에 따라서, DU에 적절한 스케줄링 정책 모델을 배포할 수 있다. CU는 스케줄링을 위한 별도 의 학습을 수행함이 없이, CU로부터 제공된 스케줄링 정책 모델을 이용하여 스케줄링을 수행할 수 있다. 따라서, 본 발명의 일실시예에 따르면, DU 별로 스케줄링 정책 모델을 학습할 필요가 없으므로, DU 별로 스케줄 링 정책 모델을 학습하기 위해 소요되는 연산이 줄어들 수 있다. 이하에서, 본 발명에 따른 실시예들을 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 본 발명의 일실시예에 따른 이동통신 시스템을 설명하기 위한 도면이며, 도 2는 본 발명의 일실시예에 따른 강화 학습 모델을 설명하기 위한 도면이다. 도 1에서는 기지국의 기능이 분리된 5G 이동통신 시스템이 일실시예로서 설명된다. 도 1을 참조하면, 본 발명의 일실시예에 따른 이동통신 시스템은 기지국 및 단말을 포함하며, 기지국 은 CU, DU 및 RU를 포함한다. 하나의 CU에 복수개의 DU가 할당될 수 있으며, DU 각각에 하나의 RU가 할당될 수 있다. 기지국은 단말에 무선 자원을 할당하는 스케줄링 을 수행하므로, 스케줄링 관점에서 스케줄링 장치로 불려질 수 있다. CU는 스케줄링 정책 모델에 대한 학습을 수행하고, 학습된 스케줄링 정책 모델을 DU로 제공한다. DU는 CU로부터 제공된 스케줄링 정책 모델을 이용하여 스케줄링을 수행한다. RU는 DU의 스 케줄링에 따라 단말에 무선 자원을 할당하고, 단말로 데이터를 전송한다. 스케줄링을 통해 획득된 경 험 정보는 DU에서 CU로 제공되고, CU는 경험 정보를 이용하여 스케줄링 정책 모델에 대한 학습 을 수행한다. 스케줄링 정책 모델은, 강화 학습 모델의 정책 함수에 대응되는 모델이며, 본 발명의 일실시예에 따른 강 화 학습 모델은 도 2에 도시된 바와 같이, 에이전트(Agent, 210) 및 환경(Environment, 220)으로 구성된다. 에 이전트는 스케줄링 정책 모델을 포함하며, 환경으로부터 스케줄링 상태 정보 및 보상값을 제공 받아, 스케줄링 액션 정보를 출력한다. 환경은 스케줄링 액션 정보에 따라 갱신되는 스케줄링 상태 정보 및 보상값을 에이전트로 제공한다. 스케줄링 정책 모델은 스케줄링 상태 정보를 입력받아, 스케줄링 액션 정보를 출력하며, 일실시예로서 신 경망 모델일 수 있다. 스케줄링 정책 모델은 스케줄링 과정을 통해 획득된 보상값의 누적값이 최대가 되도 록 학습될 수 있으며, 이러한 학습 과정을 통해 신경망의 가중치가 갱신될 수 있다. 스케줄링 상태 정보는 무선 자원의 스케줄링을 위한 기지국의 상태 정보를 포함한다. 일실시예로서, 스케줄링 상태 정보는 무선 자원의 스케줄링에 필요한 사용자별 채널 상태 정보, 사용자별 QoS 우선권 정보, 현재 가용 무선 자원 정보 등을 포함할 수 있다. 그리고 스케줄링 액션 정보는 무선 자원의 할당 정보를 포함한다. 즉, 제 1사용자에게 무선 자원을 제1자원량만큼 할당하고, 제2사용자에게 무선 자원을 제2자원량만큼 할당하는 정보를 포함할 수 있다. 이와 같이, DU는 학습에 필요한 경험 정보를 획득하고, CU는 DU에서 획득된 경험 정보를 이용해 학습을 수행함 으로써, 스케줄링 정책 모델에 대한 CU와 CU의 협력 학습이 수행될 수 있다. 다시 도 1로 돌아와, CU는 스케줄링을 통해 획득된 경험 정보를 DU로부터 제공받아, 타겟 스케줄링 정책 모델에 대한 학습을 수행한다. 그리고 학습된 타겟 스케줄링 정책 모델을 DU로 제공한다. DU는 제1타임 슬롯에서의 스케줄링 상태 정보와, 강화 학습 모델의 타겟 스케줄링 정책 모델을 이용하여, 무선 자원을 스케줄링한다. 그리고 스케줄링을 수행함에 따라 획득된 경험 정보를 CU로 제공한다. 여기서, 경험 정보는 제1타임 슬롯에서의 스케줄링 상태 정보, 제1타임 슬롯에서의 스케줄링 액션 정보, 제1타 임 슬롯에서의 보상값과, 제1타임 슬롯의 다음 타임 슬롯은 제2타임 슬롯에서의 스케줄링 상태 정보를 포함한다. 제1타임 슬롯에서의 스케줄링 상태 정보는, 제1타임 슬롯에서의 스케줄링 액션 정보에 따라 스케줄링 이 수행됨에 따라, 제2타임 슬롯에서의 스케줄링 상태 정보로 갱신된다. 타임 슬롯은 스케줄링을 수행하는 시점 에 대응될 수 있다. 즉, DU는 제1타임 슬롯에서의 스케줄링 상태 정보를 타겟 스케줄링 정책 모델에 입력하여, 제1타임 슬롯에 서의 스케줄링 액션 정보를 획득하고, 이를 통해 스케줄링을 수행한다. 그리고 스케줄링의 결과에 따른 보상값 및 제2타임 슬롯에서의 스케줄링 상태 정보를 생성하여 CU로 전송한다. 그리고 CU는 DU로부터 수신된 경험 정보를 이용해 타겟 스케줄링 정책 모델에 대한 학습을 수행한다. CU는 복수의 스케줄링 정책 모델에 대한 학습을 수행하며, 타겟 스케줄링 정책 모델은, 이러한 스케줄링 정책 모델들 중 하나이다. 그리고 복수의 스케줄링 정책 모델은, 미리 설정된 스케줄링 조건에 따라 학습되는 모델이다. 다시 말해, 복수의 스케줄링 정책 모델은, 미리 설정된 스케줄링 조건 즉, 스케줄링 목적을 만족하도 록 학습되는 모델이다. 스케줄링 정책 모델의 개수는 스케줄링 조건의 개수에 대응될 수 있다. 스케줄링 조건은 일실시예로서, URLLC(Ultra-Reliable and Low Latency Communications, 초고신뢰/저지연 통신), 공평성(fairness), 총합 전송률 최대화(sumrate maximization) 또는 QoE(Quality of Experience)를 포 함할 수 있다. 예컨대 스케줄링 조건이 URLLC라면 지연이 없고 높은 신뢰 수준으로 통신 서비스가 제공되는 조 건이 만족되도록 무선 자원이 할당되어야 하며, 이와 같이 무선 자원이 할당되는 스케줄링 액션 정보가 출력되도록 스케줄링 정책 모델이 학습될 수 있다. 또는 스케줄링 조건이 공평성이라면, 사용자별로 공평하게 무선 자 원이 할당되어야 하며, 이와 같이 무선 자원이 할당되는 스케줄링 액션 정보가 출력되도록 스케줄링 정책 모델 이 학습될 수 있다. 또는 총합 전송률 최대화가 스케줄링 조건이라면, 기지국과 사용자 사이의 데이터 전송률의 총합이 최대가 되도록 무선 자원이 할당되어야 하며, 이와 같이 무선 자원이 할당되는 스케줄링 액션 정보가 출 력되도록 스케줄링 정책 모델이 학습될 수 있다. 마지막으로 스케줄링 조건이 QoE라면, 사용자의 체감 품질이 높아지도록 무선 자원이 할당되어야 하며, 이와 같이 무선 자원이 할당되는 스케줄링 액션 정보가 출력되도록 스케줄링 정책 모델이 학습될 수 있다. 전술된 스케줄링 조건에 대한 구체적인 조건은 실시예별로 다르게 설정될 수 있다. 예컨대, 공평성이 스케줄링 조건이라고 할 때, 실시예에 따라서 동일한 무선 자원이 사용자에게 할당될 때 공평성 조건이 만족될 수도 있고, 사용자별로 할당되는 무선 자원의 차이가 임계값 이하일 때 공평성 조건이 만족될 수도 있을 것이다. DU는, 스케줄링 조건을 만족하면서 스케줄링 정책 모델에 대한 학습이 수행될 수 있도록, 스케줄링 조건에 따라 보상값을 결정하며, 보상값은 스케줄링 조건의 만족 정도에 따라서 달라질 수 있다. 일실시예로서 DU(13 0)는 스케줄링 조건별로 미리 설정된 보상값 계산 모델과 스케줄링 조건의 만족 정도를 판단하기 위한 정보를 수집하여, 보상값을 계산할 수 있다. 예컨대, 공평성이 스케줄링 조건이라고 할 때, 보상값 계산 모델에 따른 보상값은, 공평성이 만족되지 않은 경 우보다 공평성이 만족된 경우 클 수 있다. 그리고 사용자별로 할당되는 무선 자원의 차이가 임계값보다 작을수 록, 보상값은 작아질 수 있다. 그리고 공평성의 만족 정도를 판단하기 위해, 사용자에게 할당된 무선 자원 정보 가 수집될 수 있다. 따라서, 스케줄링 정책 모델은, 입력된 스케줄링 상태 정보에 따라서, 스케줄링 조건을 만족하여 보상값의 누적 값이 최대가 되는 방향으로 스케줄링 액션 정보를 출력하도록, 학습될 수 있다. 스케줄링 조건은 DU 별로 설정될 수 있으며, CU는 DU의 스케줄링 조건에 따라 스케줄링 정책 모 델을 DU로 제공할 수 있다. 예컨대, 제1 내지 제4스케줄링 조건을 만족하도록 학습된 제1 내지 제4스케줄 링 정책 모델이 존재하며, DU의 스케줄링 조건이, 제1스케줄링 조건이라면, CU는 제1스케줄링 정책 모델을 DU로 제공한다. 복수의 DU의 스케줄링 조건이 제2스케줄링 조건이라면, CU는 복수의 DU 각각 으로 제2스케줄링 정책 모델을 제공할 수 있다. 전술된 바와 같이, 스케줄링 정책 모델의 개수는 미리 설정된 스케줄링 조건의 개수에 대응되며, DU의 개수에 대응되지 않는다. CU는 스케줄러 코디네이터 및 버퍼를 포함할 수 있으며, 스케줄러 코디네이터는 DU의 스케 줄링 조건에 따라서, 복수의 스케줄링 정책 모델 중에서, 타겟 스케줄링 정책 모델을 선택하여 DU로 제공 할 수 있다. 그리고 스케줄러 코디네이터는 DU로부터 제공된 경험 정보를 버퍼에 저장할 수 있다. DU가 타겟 스케줄링 정책 모델에 따라서 스케줄링을 수행하여 획득된 경험 정보는, 타겟 스케줄링 정책 모 델의 학습에 이용된다. DU는 경험 정보가 타겟 스케줄링 정책 모델의 학습에 이용될 수 있도록, DU의 스케줄링 조건을 나타내는 식별 정보와 함께 경험 정보를 CU로 제공할 수 있다. 도 3은 본 발명의 일실시예에 따른 이동통신 시스템에서의 스케줄링 방법을 설명하기 위한 도면이다. 본 발명의 일실시예에 따른 스케줄링 방법은, 크게 스케줄링 단계와 협력적 학습 단계로 이루어질 수 있다. 도 3을 참조하면, 본 발명의 일실시예에 따른 기지국은 제1타임 슬롯에서의 스케줄링 상태 정보와, 강화 학습 모델의 타겟 스케줄링 정책 모델을 이용하여, 무선 자원을 스케줄링(S310)한다. 단계 S310은 스케줄링 단계에 대응되며, CU에 할당된 DU에서 수행될 수 있다. CU는 스케줄링을 수행하고 그에 따른 경험 정보를 수집한다. 경 험 정보는 전술된 바와 같이, 경험 정보는 제1타임 슬롯에서의 스케줄링 상태 정보, 스케줄링 액션 정보, 보상 값 및 제2타임 슬롯에서의 스케줄링 상태 정보를 포함한다. 그리고 기지국은 스케줄링을 통해 획득된 경험 정보를 이용하여, 타겟 스케줄링 정책 모델에 대한 학습을 수행 (S320)한다. 단계 S320은 협력적 학습 단계에 대응된다. 단계 S320은 CU에서 수행되는데, CU는 DU에서 수집된 경험 정보를 학습에 이용하므로, CU와 DU의 협력적 학습이 수행된다고 볼 수 있다. 타겟 스케줄링 정책 모델은 미리 설정된 스케줄링 조건에 따라 학습된 복수의 스케줄링 정책 모델 중 하나로서, DU의 스케줄링 조건에 따라서, DU로 제공된다. CU는 복수의 스케줄링 정책 모델 각각이 정책 모델 별로 설정된스케줄링 조건, 즉 스케줄링 목적을 만족하도록 학습을 수행하면서, 복수의 스케줄링 정책 모델을 관리하며, DU 의 스케줄링 조건에 대응되는 타겟 스케줄링 정책 모델을 DU로 제공한다. 스케줄링 정책 모델은 제1타임 슬롯에서의 스케줄링 상태 정보를 입력받아, 제1타임 슬롯에서의 스케줄링 액션 정보를 출력하는 신경망 모델로서, CU는 보상값의 누적값이 최대가 되도록, 타겟 스케줄링 정책 모델에 대한 학 습을 수행한다. CU는, 스케줄링 정책 모델이 서로 다른 환경의 DU에서도 활용될 수 있도록, 메타 강화 학습 방 법을 이용할 수 있다. 스케줄링 정책 모델 별로 서로 다른 스케줄링 조건을 만족하도록 학습이 수행되기 위해, 보상값은 스케줄링 조 건에 따라 결정될 수 있다. 스케줄링 조건은 전술된 바와 같이, 일실시예로서 URLLC, 공평성, 총합 전송률 최대 화 또는 QoE 등일 수 있다. 도 4는 본 발명의 일실시예에 따른 스케줄링 방법의 의사 코드를 나타낸다. 도 4에서 는 L개로 구성된 스케줄링 정책 모델을 나타내며, l은 스케줄링 조건을 나타낸다. 그리고 g는 G개로 구성된 기지국 인덱스, t는 타임 슬롯, s는 스케줄링 상태 정보, a는 스케줄링 액션 정보, r은 보상값, e는 경 험 정보를 나타낸다. 먼저 스케줄링 절차에서 DUg는, DUg의 스케줄링 상태 정보( )와 DUg의 스케줄링 조건에 대응되는 스케줄링 정 책 모델( )을 이용하여, 스케줄링 액션 정보( )를 결정하고, 결정된 스케줄링 액션 정보에 따라 스케 줄링을 수행한다. RUg는 DUg의 스케줄링에 따라 무선 자원을 할당하고, 단말로 데이터를 전송한다. DUg는 데이터 전송 결과를 이용하여, 경험 정보( )를 획득한다. DUg는 경험 정보를 DUg 의 스케줄링 조건( )을 나타내는 식별 정보와 함께, CU로 전달한다. 그리고 협력적 학습 절차에서 CU는 DU 각각으로부터 경험 정보를 식별 정보와 함께 수신하고, 스케줄링 조건 별 로 할당된 버퍼에 경험 정보를 저장한다. 그리고 CU는 버퍼에 저장된 경험 정보를 이용하여 스케줄링 조건 별 스케줄링 정책 모델에 대한 학습을 수행하고, DU의 스케줄링 조건에 부합되는 스케줄링 정책 모델을 DU로 전송 한다. 앞서 설명한 기술적 내용들은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴 퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예들을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴 퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행 하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포 함한다. 하드웨어 장치는 실시예들의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성 될 수 있으며, 그 역도 마찬가지이다. 이상과 같이 본 발명에서는 구체적인 구성 요소 등과 같은 특정 사항들과 한정된 실시예 및 도면에 의해 설명되 었으나 이는 본 발명의 보다 전반적인 이해를 돕기 위해서 제공된 것일 뿐, 본 발명은 상기의 실시예에 한정되 는 것은 아니며, 본 발명이 속하는 분야에서 통상적인 지식을 가진 자라면 이러한 기재로부터 다양한 수정 및변형이 가능하다. 따라서, 본 발명의 사상은 설명된 실시예에 국한되어 정해져서는 아니되며, 후술하는 특허청 구범위뿐 아니라 이 특허청구범위와 균등하거나 등가적 변형이 있는 모든 것들은 본 발명 사상의 범주에 속한다 고 할 것이다."}
{"patent_id": "10-2022-0130156", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른 이동통신 시스템을 설명하기 위한 도면이다. 도 2는 본 발명의 일실시예에 따른 강화 학습 모델을 설명하기 위한 도면이다. 도 3은 본 발명의 일실시예에 따른 이동통신 시스템에서의 스케줄링 방법을 설명하기 위한 도면이다. 도 4는 본 발명의 일실시예에 따른 스케줄링 방법의 의사 코드를 나타낸다."}
