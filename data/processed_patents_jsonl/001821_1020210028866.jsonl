{"patent_id": "10-2021-0028866", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0125005", "출원번호": "10-2021-0028866", "발명의 명칭": "화자 적합성이 향상된 음성합성 모델 생성방법", "출원인": "핑퐁 주식회사", "발명자": "박세찬"}}
{"patent_id": "10-2021-0028866", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "심층 합성곱 신경망 인공지능을 기반으로 한 미세조정을 통해 화자 적합성이 향상된 음성합성 모델을 생성하는방법으로서,(a) 텍스트를 텍스트 정보를 표현하는 숫자로 변환하는 텍스트 인코더 단계;(b) 타겟 음성 파일을 스피커 임베딩으로 변환하는 스피커 인코더 단계;(c) 상기 텍스트 임베딩과 스피커 임베딩을 언어적 지식과 자소, 음소 지식을 이용하여 컨텍스트 벡터로 변환하는 퍼스널 어텐션 이용 단계;(d) 상기 컨텍스트 벡터를 예측된 멜-스펙트로그램으로 변환하는 오디오 디코더 단계;(e) 상기 예측된 멜-스펙트로그램과 에스알넷(SR-net)을 이용하여 웨이브폼 형식의 음성파일을 생성하는 보코더로 음성합성 모델을 생성하는 단계;를 포함하는 화자 적합성이 향상된 음성합성 모델 생성방법."}
{"patent_id": "10-2021-0028866", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 음성합성 모델 생성 방법은,상기 화자의 타겟 음성 데이터로부터 목소리를 생성하여 재구현하는데 필요한 모델의 생성을 위해 관련 모듈들을 활용하는 단계를 더 포함하는,화자 적합성이 향상된 음성합성 모델 생성방법."}
{"patent_id": "10-2021-0028866", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 단계(a) 내지 단계(e) 중 어느 하나의 단계에서,각각의 노드와 레이어에 가중치를 부여하는 단계를 더 포함하는,화자 적합성이 향상된 음성합성 모델 생성방법."}
{"patent_id": "10-2021-0028866", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 심층 합성곱 신경망 인공지능을 기반으로 한 미세조정을 통해 화자 적합성이 향상된 음성합성 모델 생 성방법에 관한 것으로서, (a) 텍스트를 텍스트 정보를 표현하는 숫자로 변환하는 텍스트 인코더 단계; (b) 타겟 음성 파일을 스피커 임베딩으로 변환하는 스피커 인코더 단계; (c) 상기 텍스트 임베딩과 스피커 임베딩을 언어 적 지식과 자소, 음소 지식을 이용하여 컨텍스트 벡터로 변환하는 퍼스널 어텐션 이용 단계; (d) 상기 컨텍스트 벡터를 예측된 멜-스펙트로그램으로 변환하는 오디오 디코더 단계; (e) 상기 예측된 멜-스펙트로그램과 에스알넷 (SR-net)을 이용하여 웨이브폼 형식의 음성파일을 생성하는 보코더로 음성합성 모델을 생성하는 단계로 이루어진 다."}
{"patent_id": "10-2021-0028866", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 기재는 음성합성 모델을 생성하는 방법에 관한 것으로서, 구체적으로 심층 합성곱 신경망 인공지능을 기반으 로 하는 미세 조정을 통해 소량의 샘플 만으로도 객체 구현이 가능한 화자 적합성이 향상된 음성합성 모델을 생 성하는 방법에 관한 것이다."}
{"patent_id": "10-2021-0028866", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(Artificial Intelligence, AI)은 인간의 학습능력, 추론능력, 지각능력과 자연언어의 이해능력 등을 컴퓨터 프로그램으로 실현한 기술을 의미하며, 비즈니스, 조직운영, 생활방식 그리고 커뮤니케이션 방법에 혁신 을 일으키고 있다.현재 개발이 진행되고 있는 인공지능은 대화형 사용자 인터페이스(Conversational User Interface, CUI)를 구현 하기 위해 필요한 기술에 집중되어 있다. 이러한 기술에는 음성인식(Speech to Text, STT), 자연어 이해 (Natural Language Understanding, NLU), 자연어 생성(Natural Language Generation, NLG), 음성합성(Text to Speech, TTS) 등이 있다. 이러한 기술들 중에서 음성합성 기술은 인공지능을 통한 대화형 사용자 인터페이스 구현을 위한 핵심 기술로서, 인간이 발화하는 것과 같은 소리를 컴퓨터나 기계를 통하여 만들어내는 것이다. 기존의 음성합성 방법은, 고정 합성 단위인 단어, 음절, 음소를 조합하여 파형을 만들어내는 방식(1세대)으로부 터 말뭉치를 이용한 가변 합성 단위 연결방식(2세대)를 거쳐 3세대 모델로 발전되었다. 3세대 모델은 음성인식 을 위해 음향 모델링에 주로 사용하는 마코프 모델(HMM, Hidden Markov Model) 방식을 음성합성에 적용하여 적 절한 크기의 데이터베이스를 이용하여 고품질 음성합성을 구현하는 기술이다. 그러나, 기존의 음성합성 기술은, 모델링 대상의 음성, 음색, 억양, 말투 등을 학습하기 위해서는 모델링 데이 터가 적게는 5시간, 고품질의 음성 출력을 위해서 많게는 10시간 이상이 요구되며, 그만큼의 특정 객체의 모델 링 데이터를 확보하는 곳에는 많은 비용과 시간이 소요되는 문제점이 존재하였다."}
{"patent_id": "10-2021-0028866", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 측면은, 기존의 화자에 적합 음성합성 모델을 생성하고 활용하는데 소요되는 많은 비용과 시간을 줄일 수 있는 신경망(Neural Network, NN) 인공지능을 기반으로 한 미세조정을 통해 소량의 샘플만으로 화자 적 합 음성합성 방법에 관한 것이다."}
{"patent_id": "10-2021-0028866", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시 예에 따른 심층 합성곱 신경망 인공지능을 기반으로 한 미세조정을 통해 화자 적합성이 향상 된 음성합성 모델을 생성하는 방법은, 다음과 같은 (a) 텍스트를 텍스트 정보를 표현하는 숫자로 변환하는 텍스 트 인코더 단계; (b) 타겟 음성 파일을 스피커 임베딩으로 변환하는 스피커 인코더 단계; (c) 상기 텍스트 임베 딩과 스피커 임베딩을 언어적 지식과 자소, 음소 지식을 이용하여 컨텍스트 벡터로 변환하는 퍼스널 어텐션 이 용 단계; (d) 상기 컨텍스트 벡터를 예측된 멜-스펙트로그램으로 변환하는 오디오 디코더 단계; (e) 상기 예측 된 멜-스펙트로그램과 에스알넷(SR-net)을 이용하여 웨이브폼 형식의 음성파일을 생성하는 보코더로 음성합성 모델을 생성하는 단계를 포함한다. 본 발명의 일 실시 예에 따르면, 상기 음성합성 모델 생성 방법은, 상기 화자의 타겟 음성 데이터로부터 목소리 를 생성하여 재구현하는데 필요한 모델의 생성을 위해 관련 모듈들을 활용하는 단계를 더 포함할 수 있다. 본 발명의 일 실시 예에 따르면, 상기 음성합성 모델 생성 방법은 상기 단계(a) 내지 단계(e) 중 어느 하나의 단계에서 각각의 노드와 레이어에 가중치를 부여하는 단계를 더 포함하는 단계를 더 포함할 수 있다."}
{"patent_id": "10-2021-0028866", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시 예에 따르면, 특정 화자의 목소리를 생성하는 음성합성 모델을 생성하는데 필요한 데이터 확 보에 필요한 시간을 단축시켰으며, 이를 통해 화자 적합성이 향상된 TTS를 구현하는데 소요되는 비용과 시간을 상당 부분 줄일 수 있다."}
{"patent_id": "10-2021-0028866", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참조하면서 본 발명의 실시 예에 대한 구성 및 작용을 상세하게 설명하기로 한다. 그러나, 이는 본 발명을 특정한 개시형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술범위에 포함 되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 본 명세서에서 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 아래에서는 본 발명의 실시 예에 따른 화자 적합 음성합성 모델 생성 방법 및 장치를 설명한다. 도 1은 본 발명의 일 실시 예에 따른 음성합성 모델 생성 방법의 흐름을 나타낸 블록도이고, 도 2는 도 1의 단 계(S101)에 대한 구조도, 도 3은 도 1의 단계(S102)에서 스펙트로그램을 생성하는 설명도, 도 4는 도 1의 단계 (S103)에서 이용되는 퍼스널 어텐션의 설명도, 도 5는 도 1의 단계(S104)의 설명도이며, 도 6은 도 1의 단계 (S105)의 설명도이다.` 도 1을 참조하면, 상기 음성합성 모델은 단계 S101 내지 단계 S105를 포함한다. 단계(S101)에서는 텍스트 인코 더(Text-Encoder)를 이용하여 입력된 텍스트를 텍스트 정보를 표현하는 숫자로 변환할 수 있으며, 이는 딥러닝 모델이 계산을 하기 위해 글자를 숫자로 바꾸는 캐릭터 임베딩(Character Embedding)에 해당한다. 예를 들면, 한국어의 경우 한글은 음소문자이나 한국어는 한글을 모아쓰기를 통해서 음절문자로 사용한다. 이에 따라, 알파벳이나 로마자와 같은 음소문자 계열의 언어와 발음 방식은 유사하지만 다른 발음 규칙을 적용해야 한다. 즉, 유니코드의 자모로 초성, 중성, 종성으로 나누는 전처리를 통한 방식과 발음기호와 언어 규칙을 적용한 방 식을 추가하여서 변환해야 한다. 이를 위해서 사용하는 것이 Text-net과 HC(Highway-conv)이다. 상기 Text-net은 Conv1D와 Relu를 거치고, Dropout을 한 뒤에, 다시 Conv1D와 Dropout을 거치게 된다. HC- net은 Highway-conv를 의미하며, Text-net을 통해 변환된 데이터는 (HC)**2 - (HC)**2를 거치고 컨벌루셔널 층 을 통해서 숫자 데이터가 된다. 단계(S101)에서 입력된 텍스트는 텍스트-넷(Text-net)과 HC-net를 거쳐서 텍스트 정보를 표현하는 숫자들로 텍 스트 임베딩(Text Embedding)된다. 이 때의 결과물은 (K, V)이다. K는 Keys를, V는 Values를 의미한다. 여기서, K는 배치(Batch), 길이(Length(Character)), 차원(Dimension)에 관한 정보를 나타내며, V는 배치 (Batch), 길이(Length(Character)), 차원(Dimension)을 포함한다. 단계(S102)에서는 모델링하려는 화자의 타겟 음성 오디오 파일이 스피커 인코더(Speaker-Encoder)를 거쳐 스피 커 임베딩으로 변환된다. 모델링 대상 화자의 음성 데이터를 이용하기 위해, 시계열 데이터인 음성 데이터를 국 소 푸리에 변환(Short-Time Fourier Transform)을 거쳐서 스펙트로그램(Spectrogram)으로 변환한다. 종래에는 모델링 대상인 화자와 유사한 음성을 출력하기 위해서는 그 화자의 타겟 음성 오디오 파일이 적게는 5 시간, 많게는 10시간을 입력 값으로 넣어야 하지만, 본 발명에서는 타겟 음성 오디오 파일을 10분 이상을 입력 값으로 넣으면 기존 기술의 출력값과 유사한 음성을 출력할 수 있다. 도 3은 국소 푸리에 변환(STFT)으로 스펙트로그램을 생성하는 방법을 도시하고 있으며, X축에는 시간, Y축에는 주파수(Hz), Z축에는 진폭(Amplitude)을 나타내어 주파수와 진폭이 시간에 따라 어떻게 변화하는지를 나타낸다. 이렇게 만들어진 스펙트로그램을 귀의 달팽이관의 특성을 반영하여 멜-스펙트로그램(Mel-Spectrogram)으로 변환 한다. 이 멜-스펙트로그 램(Mel-Spectrogram)을 입력값으로 넣어서 스피커 인코더 네트워크(SE-net)를 통과시킨 다. 이때 SE-net은 (Conv1D - ReLU - Dropout) - (Conv1D - ReLU - Dropout) - (Conv1D - Dropout) - (HC - HC - HC - HC)**2 - (HC) ** 2로 구성되어 있다. 이를 통해 멜-스펙트로그램이 타겟 화자의 음성 정보를 표현한 숫자인 스피커 임베딩(Speaker embedding)으로 변환된다. 단계(S101)과 단계(S102)를 통해 변환된 음성, 텍스트를 표현하는 숫자들은 단계(S103)에서 퍼스널 어텐션 (Personal Attention)을 거쳐서 컨텍스트 벡터들(Context Vectors)로 변환된다. 이 과정에서 생성 모델(Generative Model)이 이용되며, 이 모델은 언어적 지식, 자소와 음소의 지식을 포함한다. 이 정보들이 퍼스널 어텐션을 거쳐서 컨텍스트 벡터들이 된다. 도 4는 를 참조하면, 어텐션(Attention)은 입력된 데이터의 중요도를 반영하여, 각각 어디에 더 집중해야 할지 를 학습하는 모듈이다. 퍼스널 어텐션은 특정 화자의 목소리의 특징 중 어디에 더 집중해야 할지를 학습한다. 이 과정을 거쳐서 화자의 목소리를 구현하는데 필요한 특징값을 추출해낸다. 퍼스널 어텐션의 입력값으로는 키(Keys), 값(Values), 그리고 쿼리(Queries)가 입력되며, 이러한 입력값은 퍼스 널 어텐션을 거쳐서 R로 변환되는데, R은 컨텍스트 벡터(Context Vector)이다. 여기에는 입력값인 쿼리가 가지 고 있던 배치, T/r, 그리고 차원에 대한 정보가 포함된다. 도 5는 단계(S104)들 도시하며, 여기서는 단계(S103)에서 생성된 컨텍스트 벡터가 오디오 디코더(Audio Decoder)를 거쳐서 예측된 멜-스펙트로그램(Mel-Spectrogram)으로 변환된다. 컨텍스트 벡터는 먼저 디코더-넷(Decoder-net)을 거치면서 Conv1D-드랍아웃(Dropout)의 과정을 지난다. 그 다음 HC-넷(HC-net)에서 HC-HC-HC-HC의 과정을 거친다. 다음으로 컨벌루셔널 층에서 (HC)를 2번 거치고, (Conv1D+ ReLU + Dropout)을 3번 거치고, Conv1D-Dropout을 거쳐서 예측된 멜-스펙트로그램으로 변환된다. 이렇게 변환된 멜-스펙트로그램 예측값은 입력값이었던 컨텍스트 벡터가 가지고 있던 배치, T/r을 여전히 속성 으로 가지며, 동시에 n_mels라는 새로운 속성을 가지게 된다. 도 6은 단계(S105)를 도시하며, 여기서는 예측된 멜-스펙트로그램과 슈퍼 레졸루션(super resolution)을 입력값 으로 하여 보코더(Vocoder)를 거쳐서 음성파일을 생성한다. 먼저 SR-넷(SR-net)에서는 (Conv1D + Dropout) - HC - HC - (HC-HC-HC)**2 - (Conv1D + Dropout) - (HC)**2의 과정을 거친다. 이를 통해 cnn 시스템의 해상도를 향상시킨다. 그 다음 보코더넷(Vocoder-net)을 거쳐서 최종 웨이브폼(Waveform) 음성이 생성된다. 즉, 예측된 멜-스펙트로그램을 입력값으로 넣어서, 최종 웨이브폼이 완성되며, 이러한 웨이브폼은 입력된 목소 리와 어조, 음색,억양 등이 유사한 음성으로 합성된 결과이다."}
{"patent_id": "10-2021-0028866", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시 예에 따른 음성합성 모델 생성 방법의 흐름을 나타낸 블록도이다. 도 2는 도 1의 단계(S101)에 대한 구조도이다. 도 3은 도 1의 단계(S102)에서 스펙트로그램을 생성하는 설명도이다. 도 4는 도 1의 단계(S103)에서 이용되는 퍼스널 어텐션의 설명도이다. 도 5는 도 1의 단계(S104)의 설명도이다. 도 6은 도 1의 단계(S105)의 설명도이다."}
