{"patent_id": "10-2024-0085070", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0051551", "출원번호": "10-2024-0085070", "발명의 명칭": "동영상 편집 기능을 제공하는 전자 장치 또는 그의 동작 방법.", "출원인": "코드비전 주식회사", "발명자": "송응열"}}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,통신 장치; 영상에 포함된 적어도 하나의 객체에 대한 포인트(points)를 검출하도록 학습된 객체 인식 모델을 저장하는 저장 장치; 및적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는: 상기 통신 장치를 통해, 상기 전자 장치와 연결된 사용자 장치로부터 복수의 이미지들을 포함하는 제1 영상을획득하고,상기 제1 영상에 포함된 상기 복수의 이미지들을 기-설정된 기준 단위로 그룹핑(grouping)하여 이미지 그룹들을생성하고,상기 객체 인식 모델을 이용하여 상기 이미지 그룹들 별 적어도 하나의 객체를 추적하여, 상기 적어도 하나의객체에 대한 제1 포인트 세트를 획득하고,상기 제1 포인트 세트에 기반하여 상기 적어도 하나의 객체의 제1 윤곽을 식별하고, 상기 제1 포인트 세트를 표시하도록 상기 사용자 장치에 전달하고,상기 사용자 장치를 통해 상기 복수의 이미지들 중 제1 이미지에 포함된 상기 적어도 하나의 객체에 대한 제1사용자 입력을 획득하고, 상기 제1 사용자 입력에 기반하여 상기 제1 이미지의 상기 적어도 하나의 객체의 제2 윤곽을 식별하고, 상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 상기 제1 영상에 대한 상기 적어도 하나의 객체를 분할하는 마스크(mask)를 획득하고,상기 마스크를 이용해 상기 적어도 하나의 객체와 관련하여 상기 제1 영상을 보정한 제2 영상을 획득하고, 및상기 통신 장치를 통해, 상기 사용자 장치가 상기 제2 영상을 출력하도록 상기 제2 영상을 전달하도록 설정된전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서, 상기 적어도 하나의 프로세서는, 상기 제1 영상 내에서 복수의 이미지들 중 상기 제1 이미지의 순서 정보를 획득하고, 및상기 순서 정보 및 상기 제1 사용자 입력에 기반하여 상기 적어도 하나의 객체의 상기 제2 윤곽을 식별하도록설정된 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서,상기 적어도 하나의 프로세서는, 공개특허 10-2025-0051551-3-상기 객체 인식 모델을 이용하여 상기 제1 영상의 장면 전환을 식별하고, 및상기 장면 전환을 기준으로 상기 복수의 이미지들을 그룹핑하여 상기 이미지 그룹들을 생성하도록 설정된 전자장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 1에 있어서,상기 적어도 하나의 프로세서는, 상기 이미지 그룹들 중 상기 제1 이미지가 포함된 제1 그룹에 상기 제1 사용자입력을 적용함에 기반하여, 상기 제1 그룹의 이미지들에 포함된 상기 적어도 하나의 객체의 상기 제2 윤곽을 식별하도록 설정된 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서,상기 적어도 하나의 프로세서는, 상기 제1 사용자 입력의 적용에 대한 확장을 나타내는 제2 사용자 입력이 획득됨에 기반하여, 상기 이미지 그룹들 중 상기 제1 그룹에 후속하는 제2 그룹에 상기 제1 사용자 입력을 적용하여상기 제2 그룹의 이미지들에 포함된 상기 적어도 하나의 객체의 상기 제2 윤곽을 식별하도록 설정된 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 1에 있어서, 상기 제1 사용자 입력은 상기 제1 영상의 복수의 객체들 중 상기 적어도 하나의 객체를 선택하는 입력, 상기 제2 윤곽을 선택하는 것과 관련된 텍스트 입력, 상기 적어도 하나의 객체에 대한 제2 포인트 세트 입력, 또는 상기 적어도 하나의 객체에 대한 마스킹 입력 중 적어도 하나를 포함하는 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서, 상기 적어도 하나의 프로세서는, 상기 제1 사용자 입력에 상기 적어도 하나의 객체에 대한 마스킹 입력이 포함된 경우, 상기 마스킹 입력을 우선적으로 적용하여 상기 마스크를 획득하도록 설정된 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 6에 있어서, 상기 적어도 하나의 프로세서는,상기 제2 포인트 세트 입력에 기반하여 상기 적어도 하나의 객체에 대한 보정 요청을 식별하고,상기 보정 요청에 기반하여, 상기 제1 이미지에 포함된 상기 적어도 하나의 객체를 보정하고, 및보정된 적어도 하나의 객체의 상기 제2 윤곽을 식별하도록 설정된 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 8에 있어서,공개특허 10-2025-0051551-4-상기 적어도 하나의 프로세서는, 상기 보정된 적어도 하나의 객체를 포함하는 상기 제2 영상을 획득하도록 설정된 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 6에 있어서, 상기 적어도 하나의 객체는 상기 제1 영상의 상기 복수의 객체들 중 선택된 객체이고, 및상기 제2 영상은 상기 제1 영상에서 상기 선택된 객체를 제외한 나머지 객체들 및 배경이 제거된 영상인 전자장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "청구항 1에 있어서, 상기 적어도 하나의 프로세서는, 상기 사용자 장치를 통해 상기 제2 영상의 평가에 대한 제3 사용자 입력을 획득하고, 및상기 제3 사용자 입력에 기반하여 상기 객체 인식 모델을 업데이트 하도록 설정된 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "청구항 1에 있어서, 상기 적어도 하나의 프로세서는, 상기 제1 사용자 입력을 상기 저장 장치에 저장하고, 및상기 사용자 장치를 통해 상기 사용자가 복수의 이미지들 중 제2 이미지에 대하여 상기 제1 사용자 입력을 자동으로 적용할 수 있도록 제공하도록 설정된 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "청구항 1에 있어서,상기 적어도 하나의 프로세서는, 상기 적어도 하나의 객체에 대한 제1 바운딩 박스 세트를 획득하고,상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트에 기반하여 상기 적어도 하나의 객체의 상기 제1 윤곽을식별하도록 설정된, 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "전자 장치의 동작 방법에 있어서,상기 전자 장치와 연결된 사용자 장치로부터 복수의 이미지들을 포함하는 제1 영상을 획득하는 동작;상기 제1 영상에 포함된 상기 복수의 이미지들을 기-설정된 기준 단위로 그룹핑(grouping)하여 이미지 그룹들을생성하는 동작;영상에 포함된 적어도 하나의 객체에 대한 포인트를 검출하도록 학습된 객체 인식 모델을 이용하여, 상기 이미지 그룹들 별 적어도 하나의 객체를 추적하여, 상기 적어도 하나의 객체에 대한 제1 포인트 세트를 획득하는 동작;상기 제1 포인트 세트에 기반하여 상기 적어도 하나의 객체의 제1 윤곽을 식별하는 동작; 공개특허 10-2025-0051551-5-상기 제1 포인트 세트를 표시하도록 상기 사용자 장치에 전달하는 동작;상기 사용자 장치를 통해 상기 복수의 이미지들 중 제1 이미지에 포함된 상기 적어도 하나의 객체에 대한 제1사용자 입력을 획득하는 동작;상기 제1 사용자 입력에 기반하여 상기 제1 이미지의 상기 적어도 하나의 객체의 제2 윤곽을 식별하는 동작;상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 상기 제1 영상에 대한 상기 적어도 하나의 객체를 분할하는 마스크(mask)를 획득하는 동작;상기 마스크를 이용해 상기 적어도 하나의 객체와 관련하여 상기 제1 영상을 보정한 제2 영상을 획득하는 동작;및상기 사용자 장치가 상기 제2 영상을 출력하도록 상기 제2 영상을 전달하는 동작을 포함하는 전자 장치의 동작방법."}
{"patent_id": "10-2024-0085070", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "전자 장치에 있어서,적어도 하나의 카메라;입력 장치;표시 장치;통신 장치; 영상에 포함된 적어도 하나의 객체에 대한 포인트(points)를 검출하도록 학습된 객체 인식 모델을 저장하는 저장 장치; 및적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는: 상기 적어도 하나의 카메라를 통해 복수의 이미지들을 포함하는 제1 영상을 획득하고,상기 제1 영상에 포함된 상기 복수의 이미지들을 기-설정된 기준 단위로 그룹핑(grouping)하여 이미지 그룹들을생성하고,상기 객체 인식 모델을 이용하여 상기 이미지 그룹들 별 적어도 하나의 객체를 추적하여, 상기 적어도 하나의객체에 대한 제1 포인트 세트를 획득하고,상기 제1 포인트 세트에 기반하여 상기 적어도 하나의 객체의 제1 윤곽을 식별하고, 상기 제1 포인트 세트를 표시하도록 상기 표시 장치를 제어하고, 상기 표시 장치를 통해 상기 복수의 이미지들 중 제1 이미지에 포함된 상기 적어도 하나의 객체에 대한 제1 사용자 입력을 획득하고, 상기 제1 사용자 입력에 기반하여 상기 제1 이미지의 상기 적어도 하나의 객체의 제2 윤곽을 식별하고, 상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 상기 제1 영상에 대한 상기 적어도 하나의 객체를 분할하는 마스크(mask)를 획득하고,상기 마스크를 이용해 상기 적어도 하나의 객체와 관련하여 상기 제1 영상을 보정한 제2 영상을 획득하고, 및상기 표시 장치가 상기 제2 영상을 출력하도록 제어하도록 설정된 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "다양한 실시 예들에 따르면, 전자 장치는, 영상에 포함된 적어도 하나의 객체에 대한 포인트(points) 및/또는 바 운딩 박스(bounding box)를 검출하도록 학습된 객체 인식 모델을 저장하는 저장 장치 및 적어도 하나의 프로세서 를 포함하고, 적어도 하나의 프로세서는, 사용자 장치로부터 복수의 이미지 프레임들을 포함하는 제1 영상을 획 (뒷면에 계속)"}
{"patent_id": "10-2024-0085070", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 문서에서 개시되는 다양한 실시 예들은, 동영상 편집 기능을 제공하는 전자 장치 또는 그의 동작 방법에 관 한 것이다."}
{"patent_id": "10-2024-0085070", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근에는, 인공지능 모델을 이용하여 영상 내에서 포함된 특정 객체를 식별하는 기능을 제공하는 비전 시스템이 개발되고, 다양한 분야에서 활용되고 있다. 예를 들어, 복수의 이미지 프레임들을 포함하는 영상 내에서 객체를 식별하고, 식별된 객체와 관련된 다양한 서비스를 제공할 수 있다. 한편 이미지 내에서 객체를 식별하는 인공지능 모델과 관련하여 딥러닝 기반의 semantic segmentation이 고려될 수 있다. 딥러닝 기반의 semantic segmentation에 대해서 합성곱 신경망(Convolutional NeuralNetwork, CNN)을 이용한 방법들이 많이 연구되고 있다. 합성곱 신경망은 시각적 영상을 분석하는 데 사용되는 다층의 피드-포워 드적인 인공신경망의 한 종류이다. 합성곱 신경망은 인공신경망에 적용하여 이미지를 효과적으로 처리할 수 있 는 심층 신경망 기법으로 행렬로 표현된 필터의 각 요소가 데이터 처리에 적합하도록 자동으로 학습되는 과정을 통해 이미지를 분류하는 기법이다."}
{"patent_id": "10-2024-0085070", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "이미지 내 객체 식별 및 식별된 객체와 관련한 이미지 처리에 있어서, 합성곱 신경망을 이용한 대표적인 방법에 는 Fully Convolutional Networks(FCN)이 있다. 이때, FCN은 세멘틱 세그멘테이션(semantic segmentation) 방 법 중 가장 처음 제안된 것으로 분류(classification)를 위한 네트워크를 segmentation에 사용하는 것이다. 그 러나, FCN은 세그멘테이션 맵(segmentation map)을 생성하는 과정에서 객체의 윤곽이나 세밀한 정보가 손실되어 객체의 외형이 부정확하게 분할되는 문제가 있었다. 또한, 종래에는 객체 식별 기반의 이미지 처리에 있어서 인코더에서 다수의 다운 샘플링(down-sampling) 연산을 수행하였고, 이에 따라, 처리 속도가 현저히 떨어지고 식별된 객체에 대한 정보의 손실이 늘어나는 문제가 있었 다. 본 개시의 다양한 실시 예들에 따르면, 영상에 포함된 적어도 하나의 객체를 검출하고 추적하여 상기 적어도 하 나의 객체의 윤곽을 보다 높은 정확도로 식별할 수 있다. 또한, 본 개시의 다양한 실시 예들에 따르면, 포함된 적어도 하나의 객체의 윤곽을 높은 정확도로 식별하여 상 기 적어도 하나의 객체에 대하여 보정한 높은 품질의 영상을 제공할 수 있다."}
{"patent_id": "10-2024-0085070", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "다양한 실시 예들에 따르면, 전자 장치는, 통신 장치, 영상에 포함된 적어도 하나의 객체에 대한 포인트 (points)를 검출하도록 학습된 객체 인식 모델을 저장하는 저장 장치 및 적어도 하나의 프로세서를 포함하고, 적어도 하나의 프로세서는, 통신 장치를 통해, 전자 장치와 연결된 사용자 장치로부터 복수의 이미지들을 포함 하는 제1 영상을 획득하고, 제1 영상에 포함된 복수의 이미지들을 기-설정된 기준 단위로 그룹핑(grouping)하여 이미지 그룹들을 생성하고, 객체 인식 모델을 이용하여 이미지 그룹들 별 적어도 하나의 객체를 추적하여, 적어 도 하나의 객체에 대한 제1 포인트 세트를 획득하고, 제1 포인트 세트에 기반하여 적어도 하나의 객체의 제1 윤 곽을 식별하고, 제1 포인트 세트를 표시하도록 사용자 장치에 전달하고, 사용자 장치를 통해 복수의 이미지들 중 제1 이미지에 포함된 적어도 하나의 객체에 대한 제1 사용자 입력을 획득하고, 제1 사용자 입력에 기반하여 제1 이미지의 적어도 하나의 객체의 제2 윤곽을 식별하고, 제1 윤곽 및 제2 윤곽에 기반하여 제1 영상에 대한 적어도 하나의 객체를 분할하는 마스크(mask)를 획득하고, 마스크를 이용해 적어도 하나의 객체와 관련하여 제1 영상을 보정한 제2 영상을 획득하고, 및 통신 장치를 통해, 사용자 장치가 제2 영상을 출력하도록 제2 영상을 전달하도록 설정된 전자 장치."}
{"patent_id": "10-2024-0085070", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 문서에서 개시되는 다양한 실시 예들은, 사용자 장치로부터 획득되는 영상에 포함된 적어도 하나의 객체를 검출하고 추적하여 상기 적어도 하나의 객체의 윤곽을 식별할 수 있다. 이때, 다양한 실시 예들에 따른 전자 장 치는 적어도 하나의 객체에 대한 바운딩 박스(bounding box) 생성 및 키-포인트(key-points) 생성을 모두 활용 하여 보다 정확도 높게 상기 적어도 하나의 객체에 대한 윤곽을 식별할 수 있다.또한, 본 문서에서 개시되는 다양한 실시 예들은, 사용자 장치로부터 획득된 영상에 포함된 적어도 하나의 객체 를 식별하여 상기 적어도 하나의 객체에 대하여 보정한 영상을 생성할 수 있다. 예를 들어, 영상에 포함된 적어 도 하나의 객체를 인식하여 상기 적어도 하나의 객체를 제외한 나머지를 제거한 영상 또는 상기 적어도 하나의 객체와 관련하여 보정된 영상을 생성할 수 있다. 본 문서에서 개시된 다양한 실시 예들에 따른 전자 장치를 통해 사용자는 영상에 포함된 적어도 하나의 객체에 대하여 보다 정확도 높게 보정한 영상을 획득할 수 있다. 이에 따라, 사용자는 의도대로 편집된 영상을 획득할 수 있다. 본 문서에 개시된 다양한 실시 예들에 따른 전자 장치는 영상 편집과 관련된 다양한 기능을 제공하여 영상 편집 에 대한 증대된 사용자 편의성을 제공할 수 있다. 본 문서에 개시된 다양한 실시 예들에 따른 전자 장치는 잔차 U블록 기술을 이용함으로써, 디코더에서 저차원의 이미지와 고차원의 이미지를 활용하는 것이 가능해짐에 따라 객체 인식 모델이 적은 수의 학습 데이터로 학습을 수행하더라도, 정확한 객체 식별 성능을 발휘할 수 있다. 이 외에, 본 문서를 통해 직접적 또는 간접적으로 파악되는 다양한 효과들이 제공될 수 있다."}
{"patent_id": "10-2024-0085070", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "다양한 실시 예들에 대해서 특정한 구조적 내지 기능적 설명들은 단지 다양한 실시 예들에 따른 설명을 위한 목 적으로 예시된 것으로, 다양한 실시 예들은 다양한 형태로 실시될 수 있으며 본 명세서 또는 출원에 설명된 실 시 예들에 한정되는 것으로 해석되어서는 아니 된다. 다양한 실시 예들은 다양한 변경을 가할 수 있고 여러 가지 형태를 가질 수 있으므로 다양한 실시 예들을 도면 에 예시하고 본 명세서 또는 출원에 상세하게 설명하고자 한다. 그러나, 도면으로부터 개시되는 사항은 다양한 실시 예들을 특정하거나 또는 한정하려는 것이 아니며, 다양한 실시 예들의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1 및/또는 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으로 만, 예컨대 본 개시의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관 계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃하는\"과 \"~에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 본 명세서에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 다양한 실시 예들을 한정하 려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세 서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 설시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미이다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미인 것으로해석되어야 하며, 본 명세서에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 이하, 첨부한 도면을 참조하여 본 개시의 바람직한 실시 예를 설명함으로써, 본 개시를 상세히 설명한다. 각 도 면에 제시된 동일한 참조부호는 동일한 부재를 나타낸다. 도 1은 다양한 실시 예들에 따라 동영상 편집 기능을 제공하는 동영상 편집 시스템을 나타내는 도면이다. 도 1을 참조하면, 동영상 편집 시스템은 사용자 장치, 네트워크 및 전자 장치를 포함할 수 있다. 한편, 본 문서의 동영상은 영상으로 표현될 수 있다. 또한, 동영상 내 적어도 하나의 객체에 대한 포인트는 키- 포인트 표현으로 혼용될 수 있다. 다양한 실시 예들에 따르면, 사용자 장치는 표지 장치를 포함하는 장치로 휴대폰, 스마트폰, PDA(personal digital assistant), 노트북 컴퓨터, TV(television), 웨어러블 장치, HMD (head mounted device)일 수 있다. 다양한 실시 예들에 따르면, 사용자 장치는 동영상 컨텐츠를 사용자에게 제공할 수 있는 다양한 출력 장치 를 포함할 수 있다. 예를 들어, 사용자 장치는 오디오 장치, 표시 장치 또는 동영상을 획득할 수 있는 적 어도 하나의 카메라 중 적어도 하나를 포함할 수 있다. 다양한 실시 예들에 따르면, 사용자 장치는 사용자로부터 입력을 획득할 수 있는 다양한 입력 장치를 포함 할 수 있다. 예를 들어, 키보드, 터치 패드, 키(예: 버튼), 마우스, 마이크, 디지털 펜(예: 스타일러스 펜) 중 적어도 하나를 포함할 수 있다. 다양한 실시 예들에 따르면, 네트워크는 사용자 장치와 통신할 수 있도록 커플링하기에 적합한 임의 의 다양한 무선 통신 네트워크를 포함할 수 있다. 예를 들어, WLAN, WAN, PAN, 셀룰러, WMN, WiMAX, GAN, 6LowPAN 등을 포함할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 독립형 호스트 컴퓨팅 시스템, 사용자 장치와 통합된 온-보 드 컴퓨터 시스템, 모바일 디바이스, 또는 사용자 장치에 동영상 편집 기능 및 동영상 컨텐츠를 제공할 수 있는 임의의 다른 하드웨어 플랫폼을 포함할 수 있다. 예를 들어, 전자 장치는 사용자 장치에서 실행 되는 동영상 편집을 서비스하는데 적합한 클라우드-기반 컴퓨팅 아키텍쳐를 포함할 수 있다. 이에 따라, 전자 장치는 하나 이상의 서버 및 데이터 저장소를 포함할 수 있다. 예를 들어, 전자 장치는 서 비스로서의 소프트웨어(SaaS, Software as a service) 아키텍처, 서비스로서의 플랫폼(PaaS, Platform as a service) 아키텍처, 서비스로서의 인프라스트럭처(IaaS, Infrastructure as a service) 또는 다른 유사한 클라 우드-기반 컴퓨팅 아키텍처를 포함할 수 있다. 다양한 실시 예들에 따르면, 전자 장치 및/또는 사용자 장치는 도시된 예에 제한되지 않고 각각의 기 능을 수행하는 하나의 장치로 구성될 수 있다. 예를 들어, 전자 장치는 사용자 장치에 포함된 구성을 포함하여, 사용자 장치의 기능을 수행할 수 있다. 전자 장치가 사용자 장치의 기능을 제공하는 경우, 전자 장치는 전자 장치에 저 장된 동영상에 대한 편집 기능을 제공할 수 있다. 예를 들어, 전자 장치는 카메라 및 마이크를 통해 복수 의 이미지 프레임들, 오디오 정보 및/또는 자막 정보를 포함하는 동영상을 획득하여 저장할 수 있다. 또한, 전 자 장치는 사용자 입력에 기반하여 상기 동영상을 편집하고, 편집된 동영상을 표시 장치를 통해 출력할 수 있다. 다양한 실시 예들에 따라 전자 장치가 동영상 편집 기능을 제공하는 것은 후술된다. 도 2는 일 실시 예에 따른 전자 장치의 블록도이다. 도 2를 참조하면, 전자 장치(예: 도 1의 전자 장치)는 프로세서, 저장 장치(예: 도 1의 데 이터 저장소) 및/또는 통신 장치를 포함할 수 있다. 상기 열거된 구성요소들은 서로 작동적으로 (operatively) 또는 전기적으로 연결될 수 있다. 도 2에 도시된 전자 장치의 구성 요소들은 일 예로서 일 부가 변형되거나 삭제 또는 추가될 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 프로세서를 포함할 수 있다. 프로세서는 컴퓨터 프로 그램을 구성하는 명령과 같은 명령을 실행하기 위한 하드웨어를 포함할 수 있다. 예를 들어, 프로세서는 명령을 실행하기 위해, 내부 레지스터, 내부 캐시, 저장 장치(메모리를 포함)로부터 명령을 검색(또는 패 치)하고, 이를 디코딩 및 실행하고, 그 후 결과를 내부 레지스터, 내부 캐시, 저장 장치에 저장할 수 있다. 다양한 실시 예들에서, 프로세서는, 소프트웨어(예: 컴퓨터 프로그램)를 실행하여 프로세서에 연결된 전자 장치의 적어도 하나의 다른 구성요소(예: 하드웨어 또는 소프트웨어 구성요소)를 제어할 수 있고, 다 양한 데이터 처리 또는 연산을 수행할 수 있다. 다양한 실시 예들에 따르면, 데이터 처리 또는 연산의 적어도 일부로서, 프로세서는 다른 구성요소(예: 통신 장치)로부터 수신된 명령 또는 데이터를 휘발성 메모 리에 저장하고, 휘발성 메모리에 저장된 명령 또는 데이터를 처리하고, 결과 데이터를 비휘발성 메모리에 저장 할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 중앙처리장치(CPU), 그래픽처리장치(GPU), MCU(micro controller unit), 센서 허브, 보조프로세서(supplementary processor), 통신프로세서(communication processor), 애플리 케이션 프로세서(application processor), ASIC(application specific integrated circuit), FPGA(field programmable gate arrays), 또는 NPU(Neural Processing Unit) 중 적어도 하나를 포함할 수 있으며, 복수의 코어를 가질 수 있다. 다양한 실시 예들에 따르면, 프로세서(예: 신경망 처리 장치)는 인공지능 모델의 처리에 특화된 하드웨어 구조를 포함할 수 있다. 인공지능 모델은 기계 학습을 통해 생성될 수 있다. 학습 알고리즘은, 예를 들어, 지도 형 학습(supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)을 포함할 수 있으나, 전술한 예에 한정되지 않는다. 인공 지능 모델은, 복수의 인공 신경망 레이어들을 포함할 수 있다. 인공 신경망은 심층 신경망(DNN: deep neural network), CNN(convolutional neural network), RNN(recurrent neural network), RBM(restricted Boltzmann machine), DBN(deep belief network), BRDNN(bidirectional recurrent deep neural network), 심층 Q-네트워 크(deep Q-networks) 또는 상기 중 둘 이상의 조합 중 하나일 수 있으나, 전술한 예에 한정되지 않는다. 인공지 능 모델은 하드웨어 구조 이외에, 추가적으로 또는 대체적으로, 소프트웨어 구조를 포함할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 통신 장치를 통해 사용자 장치(예: 도 1의 사용자 장치(10 2)로부터 복수의 이미지 프레임들을 포함하는 제1 영상을 획득할 수 있다. 예를 들어, 제1 영상은 일련의 시간 적 흐름을 가진 복수의 이미지 프레임들 및 상기 복수의 이미지 프레임들에 대응하는 오디오 정보 및/또는 자막 정보를 포함한 영상을 포함할 수 있다. 다양한 실시 예들에 따르면, 상기 제1 영상은 사용자 장치의 적어도 하나의 카메라를 통해 촬영된 영상을 포함할 수 있다. 예를 들어, 사용자는 사용자 장치를 이용하여 영상을 촬영하고, 상기 영상을 편집하기 위 해 전자 장치에 전달할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 영상에 포함된 적어도 하나의 객체에 대한 포인트 및 바운딩 박스 를 생성하도록 학습된 객체 인식 모델(예: 도 3을 참조하여 설명된 객체 인식 모듈의 객체 인식 모델)을 이용하여 상기 제1 영상에 포함된 적어도 하나의 객체에 대한 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득 할 수 있다. 예를 들어, 프로세서는 객체 인식 모델에 제1 영상을 입력할 수 있다. 또한, 객체 인식 모델 로부터 출력되는 상기 제1 영상의 복수의 이미지 프레임들 각각에 대한 적어도 하나의 객체를 인식함에 따라 생 성되는 적어도 하나의 객체들의 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트에 기반하여 상기 적어도 하나의 객체에 대한 제1 윤곽을 인식할 수 있다. 예를 들어, 프로세서는 상기 제1 영상에 포 함된 상기 적어도 하나의 객체에 대한 스켈레톤 데이터를 추출한 상기 제1 포인트 세트와 상기 적어도 하나의 객체의 적어도 하나의 바운딩 박스를 포함한 상기 제1 바운딩 박스 세트를 모두 적용하여 상기 적어도 하나의 객체의 제1 윤곽을 식별할 수 있다. 다양한 실시 예들에 따른 전자 장치는 적어도 하나의 객체에 대한 바운딩 박스(bounding box) 세트 생성 및 키-포인트(key-points) 세트 생성을 모두 활용하여 보다 높은 정확도로 상기 적어도 하나의 객체에 대한 상 기 제1 윤곽을 식별할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 상기 제1 영상으로부터 식별된 상기 적어도 하나의 객체의 제1 윤 곽에 기반하여 상기 적어도 하나의 객체를 분할하는 마스크(mask)를 획득할 수 있다. 예를 들어, 프로세서(21 0)는 상기 제1 영상의 복수의 이미지 프레임들 별로 상기 적어도 하나의 객체를 분할하는 마스크를 획득할 수 있다. 다양한 실시 예들에 따르면, 상기 마스크는 이미지 프레임 중 특정 부분을 가리거나 수정 또는 편집하기 위한 이미지를 포함할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 상기 마스크를 이용하여 상기 제1 영상의 적어도 하나의 객체를 제외한 영역이 제거된 제2 영상을 획득할 수 있다. 예를 들어, 프로세서는 상기 제1 영상의 복수의 이미지 프레임들 각각에 대한 마스크를 이용하여 상기 제1 영상의 복수의 이미지 프레임들 별로 상기 적어도 하나의 객 체를 제외한 영역이 제거된 제2 영상을 획득할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 상기 마스크를 이용하여 상기 제1 영상의 복수의 이미지 프레임들 별로 상기 적어도 하나의 객체를 제외한 영역이 제거된 이미지 프레임들 및 상기 제1 영상의 오디오 정보 및/또 는 자막 정보를 인코딩(encoding)하여 상기 제2 영상을 획득할 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 객체가 상기 제1 영상의 전경인 경우, 상기 제2 영상은 상기 제1 영상에서 상기 전경을 제외한 배경이 제거된 영상을 포함할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 사용자 장치가 상기 제2 영상을 출력하도록 상기 제2 영상을 전달할 수 있다. 예를 들어, 프로세서는 통신 장치를 통해 사용자 장치로 상기 제2 영상을 전달 할 수 있다. 다양한 실시 예들에 따르면, 상기 제2 영상을 획득한 사용자 장치는, 사용자 장치에 포함된 표시 장 치(예: 디스플레이)를 통해 상기 제2 영상을 출력할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 사용자 장치를 통해 획득되는 사용자 입력에 기반하여 마스 크를 생성할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트를 표시하도 록 상기 사용자 장치에 전달할 수 있다. 예를 들어, 프로세서는 통신 장치를 통해 사용자 장치 로 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트에 대한 정보를 전달할 수 있다. 다양한 실시 예들에 따르면, 상기 제2 영상을 획득한 사용자 장치는, 사용자 장치에 포함된 표시 장 치(예: 디스플레이)를 통해 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트에 대한 정보를 시각적 객체를 이용하여 출력할 수 있다. 예를 들어, 사용자 장치는 상기 제1 영상에 포함된 복수의 이미지 프레임들 별 로 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트 각각에 대한 시각적 객체를 오버랩(overlap)하여 표시 할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 사용자 장치를 통해 상기 제1 영상에 대한 복수의 이미지 프 레임들 중 제1 이미지 프레임에 포함된 상기 적어도 하나의 객체에 대한 제1 사용자 입력을 획득할 수 있다. 예 를 들어, 프로세서는 사용자 장치를 통해 표시된 상기 제1 이미지 프레임의 포인트 세트 및 바운딩 박스 세트에 대한 사용자 입력을 획득할 수 있다. 다양한 실시 예들에 따르면, 상기 사용자 입력은 상기 제1 이미지 프레임의 상기 적어도 하나의 객체에 대한 다 양한 입력을 포함할 수 있다. 예를 들어, 상기 적어도 하나의 객체를 선택하는 입력, 상기 적어도 하나의 객체 의 포인트 세트에 대한 입력, 상기 적어도 하나의 객체의 바운딩 박스에 대한 입력, 상기 적어도 하나의 객체의 포인트 세트에 대한 마스킹 입력, 상기 적어도 하나의 객체의 윤곽을 선택하는 것과 관련된 텍스트 입력 중 적 어도 하나를 포함할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 상기 사용자 입력에 기반하여 상기 제1 이미지 프레임의 적어도 하나의 객체의 제2 윤곽을 식별할 수 있다. 예를 들어, 프로세서는 상기 제1 영상의 복수의 이미지 프레임 들 중 상기 제1 이미지 프레임의 적어도 하나의 객체의 윤곽은 상기 사용자 입력에 기반하여 상기 제2 윤곽으로 식별할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 상기 제1 영상에 대 한 상기 적어도 하나의 객체를 분할하는 마스크를 획득할 수 있다. 예를 들어, 상기 제1 영상에 포함된 복수의 이미지 프레임들 중 상기 제1 이미지 프레임의 적어도 하나의 객체에 대한 제2 윤곽과 나머지 이미지 프레임들 의 적어도 하나의 객체에 대한 제1 윤곽에 기반하여 마스크를 획득할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 획득된 마스크를 이 용해, 상기 제1 영상의 복수의 이미지 프레임들 별로 상기 적어도 하나의 객체와 관련하여 상기 제1 영상을 보 정한 제2 영상을 획득할 수 있다. 다양한 실시 예들에 따르면, 상기 제2 영상은 상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 획득된 마스크를 이 용해 상기 적어도 하나의 객체에 대하여 보정된 이미지 프레임들, 상기 제1 영상의 오디오 정보, 및/또는 상기 제1 영상의 자막 정보를 인코딩(encoding)하여 상기 제2 영상을 획득할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 사용자 장치가 상기 제2 영상을 출력하도록 상기 제2 영상을 전달할 수 있다. 예를 들어, 프로세서는 통신 장치를 통해 사용자 장치로 상기 제2 영상을 전달 할 수 있다. 다양한 실시 예들에 따르면, 상기 제2 영상을 획득한 사용자 장치는, 사용자 장치에 포함된 표시 장 치(예: 디스플레이)를 통해 상기 제2 영상을 출력할 수 있다. *다양한 실시 예들에 따르면, 저장 장치는 데이터 또는 명령을 위한 대용량 저장소를 포함할 수 있다. 예 를 들어, 저장 장치는 하드 디스크 드라이브(HDD), 플로피 디스크 드라이브, 플래시 메모리, 광 디스크, 광-자기 디스크, 자기 테이프, 또는 범용 직렬 버스(USB, Universal serial bus) 드라이브 또는 이들의 둘 이상 의 조합을 포함할 수 있다. 다양한 실시 예에서, 저장 장치는 비휘발성, 솔리드-스테이트 메모리, 판독-전용 메모리(ROM)를 포함할 수 있다. 이러한 ROM은 마스크-프로그래밍된 ROM, 프로그래머블 ROM(PROM), 삭제 가능 PROM(EPROM), 전기적 삭제 가능 PROM(EEPROM), 전기적 변경 가능 ROM(EAROM) 또는 플래시 메모리 또는 이들 중 둘 이상의 조합일 수 있다. 본 개시가 특정 저장 장치를 설명하고 나타내지만, 본 개시는 임의의 적절한 저장 장치를 고려하며, 다양한 실 시 예들에 따르면, 저장 장치는 전자 장치의 내부 또는 외부에 있을 수 있다. 다양한 실시 예들에 따르면, 프로세서는 저장 장치에 도 3을 참조하여 설명된 동영상 편집과 관련된 모듈 저장할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 저장 장치에 저장된 인스트럭션들을 이용하여 전자 장치 의 적어도 하나의 다른 구성요소들의 제어 및/또는 통신에 관한 연산이나 데이터 처리를 실행할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 저장 장치를 포함할 수 있다. 다양한 실시 예들에 따르면, 저장 장치는, 전자 장치의 적어도 하나의 구성요소(예: 프로세서)에 의해 사용되는 다양한 데이 터를 저장할 수 있다. 데이터는, 예를 들어, 소프트웨어(예: 프로그램) 및, 이와 관련된 명령에 대한 입력 데이 터 또는 출력 데이터를 포함할 수 있다. 다양한 실시 예들에 따르면, 프로그램은 저장 장치에 소프트웨어로서 저장될 수 있으며, 예를 들면, 운영 체제, 미들 웨어 또는 어플리케이션을 포함할 수 있다. 다양한 실시 예들에 따르면, 저장 장치는 프로세서 가 실행 시에 전자 장치의 동작을 수행하기 위해 데이터를 처리하거나 전자 장치의 구성요소를 제어하도록 하는 인스트럭션들(instructions)을 저장할 수 있다. 상기 인스트럭션들은 컴파일러에 의해 생성된 코드 또는 인터프리터에 의해 실행될 수 있는 코드를 포함할 수 있다. 다양한 실시 예들에 따르면, 저장 장치는 프로세서를 통해 획득되는 다양한 정보를 저장할 수 있다. 예를 들어, 저장 장치는 프로세서로부터 획득되는 복수의 이미지 프레임들, 복수의 이미지 프레임들 을 포함하는 영상, 복수의 이미지 프레임들 각각의 순서 정보, 복수의 이미지 프레임들을 그룹핑(grouping)한 이미지 프레임 그룹들에 대한 정보, 복수의 이미지 프레임들 각각의 적어도 하나의 객체에 대한 정보, 객체 인 식 모델을 통해 출력되는 상기 적어도 하나의 객체에 대한 포인트 세트 그룹 및 바운딩 박스 세트에 대한 정보, 사용자 장치로부터 획득되는 사용자 입력 정보 중 적어도 하나를 저장할 수 있다. 또한, 저장 장치는 전자 장치와 연결되는 사용자 장치에 대한 식별 정보를 저장할 수 있다. 다양한 실시 예들에 따르면, 저장 장치는 영상의 적어도 하나의 객체에 대한 스켈레톤 데이터를 추출하여 포인트 세트를 획득하고, 상기 적어도 하나의 객체에 대한 바운딩 박스 세트를 획득하도록 학습된 객체 인식 모 델을 저장할 수 있다. 예를 들어, 객체 인식 모델은 영상 내에서 객체를 식별하고 추적하여 영상 내 적어도 하나의 객체에 대한 키-포인트 세트 및 바운딩 박스 세트를 추출하도록 트레이닝된(trained) 심층 신경망 모델 (deep neural network model)일 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 통신 장치를 포함할 수 있다. 다양한 실시 예들에서, 통신 장치는 전자 장치와 외부 전자 장치(예: 도 1의 사용자 장치) 간의 직접(예: 유선) 통신 채널 또는 무선 통신 채널의 수립, 및 수립된 통신 채널을 통한 통신 수행을 지원할 수 있다. 통신 장치는 프로 세서와 독립적으로 운영되고, 직접(예: 유선) 통신 또는 무선 통신을 지원하는 하나 이상의 커뮤니케이션 프로세서를 포함할 수 있다. 일 실시 예에 따르면, 통신 장치는 무선 통신 모듈(예: 셀룰러 통신 모듈, 근 거리 무선 통신 모듈, 또는 GNSS(global navigation satellite system) 통신 모듈) 또는 유선 통신 모듈(예: LAN(local area network) 통신 모듈, 또는 전력선 통신 모듈)을 포함할 수 있다. 이들 통신 모듈 중 해당하는 통신 모듈은 제1 네트워크(예: 도 1의 네트워크)(예: 블루투스, WiFi 다이렉트(wireless fidelity direct) 또는 IrDA(infrared data association)와 같은 근거리 통신 네트워크) 또는 제2 네트워크(예: 도 1의 네트워크)(예: 레거시 셀룰러 네트워크, 5G 네트워크, 차세대 통신 네트워크, 인터넷, 또는 컴퓨터 네트워 크(예: 도 1의 네트워크)(예: LAN 또는 WAN)와 같은 원거리 통신 네트워크)를 통하여 외부의 전자 장치와 통신할 수 있다. 상기 여러 종류의 통신 모듈들은 하나의 구성요소(예: 단일 칩)로 통합되거나, 또는 서로 별도 의 복수의 구성요소들(예: 복수 칩들)로 구현될 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 통신 장치를 통해 다양한 외부 장치와 다양한 데이터를 송 수신할 수 있다. 또한, 전자 장치는 상기 획득된 데이터를 저장 장치에 저장할 수 있다. 예를 들어, 전자 장치는 통신 장치를 통해 사용자 장치로부터 복수의 이미지 프레임들을 포함하는 영상을 획득할 수 있다. 예를 들어, 전자 장치는 통신 장치를 통해 상기 영상에 포함된 적어도 하나의 객체 에 대한 사용자 입력을 획득할 수 있다. 예를 들어, 전자 장치는 통신 장치 통해 상기 영상의 적어도 하나의 객체에 대한 포인트 세트 및 바운딩 박스 세트에 대한 정보를 사용자 장치로 전달할 수 있다. 예를 들어, 전자 장치는 통신 장치 통해 상기 적어도 하나의 객체와 관련하여 상기 영상을 보정한 영상을 사용자 장치로 전달할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 컴퓨터 시스템을 포함할 수 있다. 예를 들어, 컴퓨터 시스템은 매립형 컴퓨터 시스템, 시스템-온-칩(SOC), 단일-보드 컴퓨터 시스템(SBC), 컴퓨터-온-모듈(COM), 시스템-온-모 듈(SOM), 데스크탑 컴퓨터 시스템, 랩탑 또는 노트북 컴퓨터 시스템, 서버, 태블릿 컴퓨터 시스템, 모바일 단말 중 적어도 하나일 수 있다. 예를 들어, 전자 장치는 하나 이상의 클라우드 구성 요소를 포함할 수 있는 클 라우드에 상주하는 하나 이상의 컴퓨터 시스템을 포함할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 실질적인 공간적 또는 시간적 제한 없이 본 개시에 설명되거나 나타난 하나 이상의 방법의 하나 이상의 동작을 수행할 수 있다. 또한, 전자 장치는 본 개시에서 설명되거 나 나타난 하나 이상의 방법의 하나 이상의 동작을 실시간으로 또는 배치 모드로 수행할 수 있다. 예를 들어, 전자 장치는 본 개시에서 설명되거나 나타난 하나 이상의 방법의 하나 이상의 동작을 상이한 시간 또는 상 이한 위치에서 수행할 수 있다. 다양한 실시 예들에 따르면, 본 개시에서 설명되는 전자 장치의 동작을 수행하도록 하는 컴퓨터 명령을 저 장하는 비일시적 판독가능 기록 매체가 제공될 수 있다. 비일시적 판독가능 기록 매체 또는 저장 매체는 (예를 들어, 필드-프로그래머블 게이트 어레이(FPGA) 또는 어플리케이션-특정 IC(ASIC)와 같은) 하나 이상의 반도체- 기반 또는 다른 집적 회로(IC), 하드 디스크 드라이브(HDD), 하이브리드 하드 드라이브(HHD), 광 디스크, 광 디 스크 드라이브(ODD), 자기-광 디스크, 자기-광 드라이브, 플로피 디스켓, 플로피 디스크 드라이브(FDD), 자기 테이프, 솔리드-스테이트 드라이브(SSD, solid-state drive), RAM-드라이브, SECURE DIGITAL 카드 또는 드라이 브, 임의의 다른 적절한 컴퓨터-판독 가능 비일시적 저장 매체 또는 적절한 경우 이들 중 둘 이상의 임의의 적 절한 조합을 포함할 수 있다. 다양한 실시 예들에 따르면, 컴퓨터-판독 가능 비일시적 기록 매체는 적절한 경우 휘발성, 비휘발성 또는 휘발 성과 비휘발성의 조합일 수 있다. 기기로 읽을 수 있는 기록 매체는, 비일시적(non-transitory) 기록 매체의 형 태로 제공될 수 있다. 여기서, '비일시적 기록 매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기 파)를 포함하지 않는다는 것을 의미할 뿐이며, 이 용어는 데이터가 기록 매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구분하지 않는다. 예를 들어, '비일시적 기록 매체'는 데이터가 임시적으로 저장되 는 버퍼를 포함할 수 있다. 본 문서에 개시된 다양한 실시 예들에 따른 전자 장치의 동작 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 기록 매체(예: compact disc read only memory (CDROM))의 형태로 배포되거나, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 또는 두개의 사용자 장치들(예: 스 마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 다양한 실시 예들에 따르면, 본 개시에 설명된 전자 장치가 사용자 장치의 기능을 제공하는 경우, 전 자 장치는 적어도 하나의 카메라(미도시) 및/또는 표시 장치(미도시)를 포함할 수 있다. 이하, 전자 장치 가 적어도 하나의 카메라(미도시) 및/또는 표시 장치(미도시)를 포함하는 경우에 대하여 설명한다. 다양한 실시 예들에 따르면, 프로세서는 적어도 하나의 카메라를 통해 복수의 이미지 프레임들을 포함하는 상기 제1 영상을 획득할 수 있다. 예를 들어, 프로세서는 촬영 시작 명령에 기반하여 상기 적어도 하나의 카메라를 활성화하고, 상기 적어도 하나의 카메라를 통해 복수의 이미지 프레임들, 오디오 정보 및/또는 자막 정보를 포함하는 상기 제1 영상을 획득할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 객체 인식 모델을 이용하여 상기 제1 영상에 포함된 적어도 하나 의 객체의 윤곽을 식별하고, 상기 적어도 하나의 객체의 제1 윤곽에 기반하여 상기 적어도 하나의 객체를 분할 하는 마스크를 획득할 수 있다. 이는, 상술된 설명과 중복되므로 약술한다. 다양한 실시 예들에 따르면, 프로세서는 상기 마스크를 이용하여 상기 제1 영상의 상기 적어도 하나의 객 체를 제외한 영역이 제거된 제2 영상을 획득하고, 상기 표시 장치가 상기 제2 영상을 출력하도록 상기 표시 장 치를 제어할 수 있다. 예를 들어, 프로세서는 전자 장치에 포함된 상기 표시 장치를 통해 상기 제2 영상을 출력할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 사용자 입력에 기반하여 마스크를 생성하는 경우, 상기 적어도 하 나의 객체에 대한 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트를 표시하도록 상기 표시 장치를 제어할 수 있다. 예를 들어, 상기 표시 장치는 상기 프로세서의 제어를 통해 상기 제1 영상에 포함된 복수의 이미 지 프레임들 별로 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트 각각에 대한 시각적 객체를 오버랩하여 표시할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 상기 복수의 이미지 프레임들 중 상기 제1 이미지 프레임에 포함 된 상기 적어도 하나의 객체에 대한 사용자 입력을 획득할 수 있다. 예를 들어, 프로세서는 전자 장치 에 포함된 입력 장치(예: 키보드, 터치 패드, 키(예: 버튼), 마우스, 마이크, 디지털 펜(예: 스타일러스 펜))를 통해 사용자 입력을 획득할 수 있다. 다양한 실시 예들에 따르면, 프로세서는 상기 사용자 입력에 기반하여 상기 제1 이미지 프레임의 상기 적 어도 하나의 객체의 제2 윤곽을 식별할 수 있다. 또한, 프로세서는 상기 적어도 하나의 객체의 제1 윤곽 및 제2 윤곽에 기반하여 상기 적어도 하나의 객체를 분할하는 마스크를 획득할 수 있다. 이는, 상술된 설명과 중복되므로 약술한다. 다양한 실시 예들에 따르면, 프로세서는 상기 마스크를 이용해 상기 적어도 하나의 객체와 관련하여 상기 제1 영상을 보정한 제2 영상을 획득하고, 상기 표시 장치가 상기 제2 영상을 출력하도록 제어할 수 있다. 도 3은 다양한 실시 예들에 따른 전자 장치에서 동영상 편집과 관련된 기능을 제어하는 개념을 나타낸다. 도 3을 참조하면, 전자 장치는 다양한 동영상 편집과 관련된 기능들을 지원하기 위해 하드웨어 및/또는 소 프트웨어 모듈을 이용할 수 있다. 예를 들어, 프로세서는 저장 장치에 저장된 명령어들을 실행 함으로써 영상 획득 모듈, 이미지 그룹핑 모듈, 객체 인식 모듈, 마스크 생성 모듈, 객체 보정 모듈, 객체 선택 모듈, 영상 생성 모듈, 평가 모듈 및/또는 개인화 모듈을 구동 할 수 있다. 다양한 실시 예에서, 도 3에 도시된 것과 다른 소프트웨어 모듈이 구현될 수 있다. 예를 들어, 적 어도 2개의 모듈이 하나의 모듈로 통합되거나, 하나의 모듈이 2개 이상의 모듈로 분할될 수 있다. 또한 하드웨 어와 소프트웨어 모듈이 하나의 기능을 분담함으로써 작업 성능을 개선시킬 수 있다. 예를 들어, 전자 장치는 하드웨어로 구현되는 인코더와 소프트웨어 모듈로 구현되는 인코더를 모두 포함할 수 있고, 적어도 하 나의 카메라 모듈을 통해 획득되는 데이터의 일부는 하드웨어 인코더에서, 나머지 일부는 소프트웨어 인코더에 서 처리할 수 있다. 다양한 실시 예들에 따르면, 영상 획득 모듈은 동영상 업로드와 관련된 UI(User interface)/GUI(graphical UI)를 사용자 장치를 통해 사용자에게 제공하고, 사용자 장치를 통해 영상(예: 도 2를 참조하여 설명된 제1 영상)을 획득할 수 있다. 예를 들어, 사용자 장치의 표시 장치를 통해 출력된 UI/GUI를 통해 제공되는 사용자 입력에 응답하여 동영상 업로드와 관련된 기능을 제어함에 따라 영상을 획득할 수 있다. 또한, 영상 획득 모듈은 사용자 장치를 통해 획득된 영상의 복수의 이미지 프레임들, 오디 오 정보, 및/또는 자막 정보를 추출할 수 있다. 다양한 실시 예들에 따르면, 이미지 그룹핑 모듈은 영상 획득 모듈을 통해 획득된 영상에 포함된 복 수의 이미지 프레임들을 기-설정된 기준 단위로 그룹핑하여 이미지 프레임 그룹들을 생성할 수 있다. 예를 들어, 이미지 그룹핑 모듈은 객체 인식 모듈에 포함된 객체 인식 모델을 이용하여 상기 영상의 장면 전환을 인식하고, 상기 장면 전환을 기준으로 영상의 복수의 이미지 프레임들을 그룹핑하여 이미지 프레임 그룹 들을 생성할 수 있다. 이미지 그룹핑 모듈이 장면 전환을 기준으로 이미지 프레임 그룹들을 생성하는 동작 은 도 5를 참조하여 후술된다. 다양한 실시 예들에 따르면, 객체 인식 모듈(object detection model)은 영상에 포함된 적어도 하나의 객 체의 포인트 세트 및 바운딩 박스를 검출하도록 학습된 객체 인식 모델을 포함할 수 있다. 예를 들어, 객체 인 식 모델은 다양한 학습 데이터를 통해 적어도 하나의 객체의 포인트 세트 및 바운딩 박스 세트를 검출하도록 학 습된 모델일 수 있다. 다양한 실시 예들에 따르면, 상기 객체 인식 모델을 학습하기 위한 학습 데이터는 영상에 포함된 복수의 이미지 프레임 내의 적어도 하나의 객체와 배경을 구분하고, 배경에 해당하는 레이블(lable) 및 적어도 하나의 객체 각 각에 해당하는 레이블을 각각 할당한 학습 데이터를 포함할 수 있다. 다양한 실시 예들에 따르면, 객체 인식 모델은 인공 신경망 모델로 구성될 수 있다. 예를 들어, 객체 인식 모델 은 영상 내에서 객체를 식별하고 추적하여 영상 내 적어도 하나의 객체에 대한 키-포인트 세트 및 바운딩 박스 세트를 추출하도록 트레이닝된(trained) 심층 신경망 모델(deep neural network model)일 수 있다. 예를 들어, 객체 인식 모델은 영역 기반 컨볼루션 신경망 모델(Region-based Convolution Neural Network, R-CNN), 고속 영역 기반 컨볼루션 신경망 모델(Faster Region-based Convolution Neural Network, Faster R-CNN), 싱글 샷 멀티박스 디텍터 모델(Single Shot multibox Detector, SSD), YOLO v4, 센터 넷(CenterNet), 또는 모바일 넷 (MobileNet)으로 구현될 수 있다. 그러나, 본 개시의 객체 인식 모델은 전술한 심층 신경망 모델에 한정되지 않 고 그 밖에 적절한 다른 신경망 모델로 구현될 수 있다. 다양한 실시 예들에 따르면, 객체 인식 모델은 영상에 포함된 적어도 하나의 객체의 스켈레톤 데이터(skeleto n)를 추출하여 포인트 세트를 획득할 수 있다. 예를 들어, 객체 인식 모델은 영상에 포함된 적어도 하나의 객체 를 추출하고, 추출된 객체의 스켈레톤 데이터를 추출하여 포인트 세트를 획득할 수 있다. 예를 들어, 객체가 사 람 객체나 동물인 경우, 객체의 관절 부위나 특정 부위를 검출할 수 있다. 또한 객체가 사람 객체인 경우 머리, 눈, 코, 입, 귀, 목, 어깨, 팔꿈치, 손목, 손끝, 몸통, 고관절, 손목, 무릎, 발목, 발끝 등의 신체 부분을 추출 할 수 있다. 상기 스켈레톤 데이터는 상기 영상에서의 좌표로서 XY좌표값으로 나타나 포인트 세트를 구성할 수 있다. 다양한 실시 예들에 따르면, 객체 인식 모델은 포인트 세트를 획득하기 위해 신체의 관절이나 특정 부위의 스켈 레톤 데이터를 추출하기 위해 Kinetics 데이터 세트나 NTU-RGB-D (Nanyang Technological University's Red Blue Green and Depth information) 데이터 세트와 같은 관절 검출 알고리즘을 활용할 수 있다. 이때 적어도 하나의 객체 당 스켈레톤 관절 개수는 임의로 정의할 수 있다. 다양한 실시 예들에 따르면, 포인트 세트는 신체의 형상을 구조화한 모델을 기반으로 생성된 XY좌표값으로 구성 된 스켈레톤 키 포인트(Skeleton Key Point)와 얼굴을 대상으로 눈, 코, 입 등의 개별 위치를 점에 대한 모델을 기반으로 하여 생성된 XY좌표값으로 페이스 키 포인트(Face Key Point)을 포함할 수 있다. 다양한 실시 예들에 따르면, 객체 인식 모델은 영상의 적어도 하나의 객체를 추적할 수 있다. 예를 들어, 영상 내 포함된 복수의 이미지 프레임들로부터 인식된 적어도 하나의 객체를 트래킹(tracking)하여 복수의 이미지 프 레임들 내의 적어도 하나의 객체 변화를 트래킹할 수 있다. 예를 들어, 객체 인식 모델은 이미지 프레임 내에복수의 객체들이 포함된 경우, 복수의 객체들 각각에 대한 스켈레톤 데이터를 추출하여 포인트 세트를 획득하고, 상기 복수의 객체들 각각에 대한 레이어를 생성하고 트래킹할 수 있다. 다양한 실시 예들에 따르면, 영상에 포함된 복수의 객체들이 특정 시간 구간별로 인식되는 경우 각 객체들이 인식되는 시간 구간별로 레이어 가 생성될 수 있다. 다양한 실시 예들에 따르면, 전자 장치의 객체 인식 모델은 영상내 복수의 이미지 프레임들로부터 바운딩 박스 세트를 추출하기 위한 인코더(encoder) 및 디코더(decoder)를 포함할 수 있다. 다양한 실시 예들에 따르면, 상기 인코더 및 상기 디코더는, 복수의 스테이지를 포함하고, 상기 복수의 스테이 지는, 일부 인코더 스테이지, 일부 디코더 스테이지, 상기 인코더 스테이지와 상기 디코더 스테이지 사이를 연 결하는 브릿지 스테이지(bridge stage)를 포함할 수 있다. 일 실시예에서, 상기 복수의 스테이지들 각각은 잔차 U블록(residual U block)을 포함할 수 있다. 다양한 실시 예들에 따르면, 상기 인코더 및 디코더는 중첩된 U자형 구조의 네트워크로 연결될 수 있다. 이러한 중첩된 U자형 구조는, 내부 스테이지(intra-stage)의 다중 스케일 특징들(multi-scale feature)을 잘 추출하고, 더 효과적으로 결합이 가능하다. 인코더는 영상의 이미지 프레임에서 특징을 추출하고 압축하여 콘텍 스트(context) 정보를 생성할 수 있다. 상기 디코더는 상기 콘텍스트 정보가 포함된 특징 맵을 확장하여 세그 멘테이션에 기반한 바운딩 박스 세트를 출력하도록 구성될 수 있다. 다양한 실시 예들에 따르면 상기 U자형 구조의 네트워크는 인코더 스테이지(브릿지 스테이지 포함), 디코더 스 테이지, 및 브릿지 스테이지를 포함할 수 있다. 이러한 U자형 구조의 네트워크는 컨캐테이션(concatenation) 연산으로 인코더의 중간 특징 정보를 디코더에서 활용하여 정보 손실을 최소화하는 효과가 있다. 그 결과 종래 의 FCN과 비교하여 segmentation 정확도(바운딩 박스 세트 추출의 정확도)가 개선될 수 있다. 다양한 실시 예들에 따르면, 객체 인식 모델은 잔차 U블록(residual U block)을 포함할 수 있다. 여기서, 잔차 U블록은, 기존 잔차 블록을 개선한 개념일 수 있다. 예를 들어, 잔차 블록은, ResNet 알고리즘에 사용되는 블록 으로, 잔차 U블록은, 다중 스케일의 특징(multi-scale feature)와 국소 특징점(local feature) 함수가 더해진 개념을 포함할 수 있다. 다양한 실시 예들에 따르면, 잔차 블록은, 각 층의 스케일 특징과 국소 특징점 함수의 입력값을 출력값에 더해 주는 과정을 포함할 수 있다. 전자 장치는 잔차 U블록의 사용으로 인해, 디코더에서 저차원의 이미지와 고 차원의 이미지를 활용하는 것이 가능할 수 있다. 이에 따라, 객체 인식 모델이 적은 수의 학습 데이터(R)로 학 습을 수행하더라도, 정확한 객체 식별(포인트 세트 및 바운딩 박스 세트 획득) 성능을 발휘할 수 있다. 따라서, 본 개시의 일 실시예에 따른 전자 장치의 객체 인식 모델은 잔차 U블록을 사용함으로써, 인코더및 디코더의 네트워크의 계층 수를 획기적으로 줄일 수 있어, 연산량을 줄여 처리 속도를 높이면서도 각 블록에서 계층화된 다운샘플링(downsampling) 벡터 계산을 증가시켜 다양한 해상도의 객체를 정확하게 식별할 수 있다. 다양한 실시 예들에 따르면, 객체 인식 모듈은 상기 객체 인식 모델을 이용하여 상기 영상의 복수이 이미 지 프레임들 각각의 적어도 하나의 객체에 대한 포인트 세트와 바운딩 박스를 모두 활용하여 보다 높은 정확도 로 상기 적어도 하나의 객체를 식별할 수 있다. 또한, 객체 인식 모듈은 입력된 영상에 복수의 객체들이 존재하는 경우 복수의 객체들 각각의 포인트 세트 및 바운딩 박스 세트를 생성할 수 있다. 다양한 실시 예들에 따르면, 마스크 생성 모듈은 상기 영상의 적어도 하나의 객체에 대한 포인트 세트와 바운딩 박스에 기반하여 상기 적어도 하나의 객체의 제1 윤곽을 식별하고 상기 식별된 적어도 하나의 객체에 대 한 윤곽(경계선을 포함함)에 기반하여 상기 적어도 하나의 객체를 분할하는 마스크를 생성할 수 있다. 다양한 실시 예들에 따르면, 마스크 생성 모듈은 영상에 포함된 복수의 이미지 프레임들 각각에 대하여 상기 적어 도 하나의 객체를 분할하는 마스크를 생성할 수 있다. 상기 마스크는 이미지 프레임 중 특정 부분을 가리거나 수정 또는 편집하기 위한 이미지를 포함할 수 있다. 또 는 상기 마스크는'0'과 '1'로 이뤄진 맵-데이터(map data)로, 이미지 프레임의 복수의 픽셀들 각각에 대하여 '0' 또는 '1'데이터를 포함할 수 있다. 다양한 실시 예들에 따르면, 마스크 생성 모듈은 상기 영상의 상기 복수의 이미지 프레임들 각각에 대한 순서 정보, 상기 포인트 세트 및 상기 바운딩 박스 세트, 및/또는 사용자 정보에 기반하여 마스크를 생성할 수 있다. 예를 들어, 마스크 생성 모듈은 사용자 장치로부터 상기 복수의 이미지 프레임들 중 특정 이미지 프레임의 상기 적어도 하나의 객체에 대한 사용자 입력을 획득하고, 상기 사용자 입력들을 추가로 이용하여 마스 크를 생성할 수 있다. 예를 들어 마스크 생성 모듈은 상기 특정 이미지 프레임의 순서 정보 및 사용자 입 력에 기반하여 상기 적어도 하나의 객체의 윤곽을 식별하고, 상기 식별된 윤곽에 기반하여 마스크를 생성할 수 있다. 예를 들어, 상기 객체 인식 모듈을 통해 결정된 포인트 세트 및 바운딩 박스 세트에 기반하여 제1 윤곽을 획득하고, 상기 적어도 하나의 객체에 대한 사용자 입력에 기반하여 제2 윤곽을 획득함으로써 상기 적어도 하나 의 객체의 형태를 식별하고 식별된 적어도 하나의 객체에 대한 마스크(들)을 획득할 수 있다. 다양한 실시 예들에 따르면, 마스크 생성 모듈은 사용자 장치를 통해 획득되는 사용자 입력에 따라 특정 이미지 프레임의 적어도 하나의 객체에 대한 사용자 입력을 상기 특정 이미지 프레임 외의 다른 이미지 프레임 들에도 적용하여 상기 다른 이미지 프레임들 각각에 대한 마스크를 생성할 수 있다. 다양한 실시 예들에 따르면 상기 적어도 하나의 객체에 대한 사용자 입력은 객체 보정 모듈 및/또는 객체 선택 모듈을 통해 획득될 수 있다. 다양한 실시 예들에 따르면, 객체 보정 모듈 사용자 장치를 통해 상기 영상의 복수의 이미지 프레임 들 중 특정 이미지 프레임(예: 도 2를 참조하여 설명된 제1 이미지 프레임)에 포함된 상기 적어도 하나의 객체 에 대한 사용자 입력에 대한 UI/GUI를 사용자에게 제공하고, 사용자 장치를 통해 상기 적어도 하나의 객체에 대 한 포인트 세트 및/또는 바운딩 박스 세트에 대한 사용자 입력을 획득할 수 있다. 일 예에서, 상기 사용자 입력 은 상기 적어도 하나의 객체에 대한 포인트 세트 입력 또는 바운딩 박스 세트 입력 중 적어도 하나를 포함할 수 있다. 이에 따라, 객체 보정 모듈은 포인트 세트 입력 또는 바운딩 박스 세트 입력에 기반하여 상기 적어 도 하나의 객체에 대한 보정 요청을 식별하고, 상기 보정 요청에 기반하여 상기 특정 이미지 프레임에 포함된 상기 적어도 하나의 객체를 보정할 수 있다. 객체 보정 모듈은 보정 요청과 관련된 사용자 입력에 대한 정 보를 마스크 생성 모듈로 제공할 수 있다. 상기 사용자 입력에 기반하여 보정된 적어도 하나의 객체의 윤 곽이 식별될 수 있다. 또한, 다양한 실시 예들에 따르면, 객체 보정 모듈은 상기 특정 이미지 프레임의 상기 적어도 하나의 객체 를 보정하여, 보정된 특정 이미지 프레임을 생성할 수 있다. 예를 들어, 상기 적어도 하나의 객체에 대한 보정 요청에 기반하여 상기 특정 이미지 프레임의 적어도 하나의 객체에 대한 픽셀들을 보정하여 보정된 특정 이미지 프레임을 생성할 수 있다. 다양한 실시 예들에 따르면 객체 보정 모듈은 보정된 특정 이미지 프레임을 생 성함에 있어서 다양한 생성형 신경망 모델(예: GAN(generative adversarial networks)을 이용할 수 있다. 다양한 실시 예들에 따르면, 객체 선택 모듈은 상기 영상의 복수의 이미지 프레임들 중 특정 이미지 프레 임(예: 도 2를 참조하여 설명된 제1 이미지 프레임)에 포함된 상기 적어도 하나의 객체에 대한 사용자 입력에 대한 UI/GUI를 사용자에게 제공하고, 사용자 장치를 상기 적어도 하나의 객체에 대한 사용자 입력을 획득할 수 있다. 예를 들어, 객체 선택 모듈은 영상에 포함된 복수의 객체들 중 식별하고자 하는 적어도 하나의 객체 를 선택하는 사용자 입력을 획득할 수 있다. 또한, 다양한 실시 예들에 따르면, 객체 선택 모듈은 특정 프레임에 포함된 상기 적어도 하나의 객체에 대 한 포인트 세트에 대한 입력, 상기 적어도 하나의 객체의 바운딩 박스에 대한 입력, 상기 적어도 하나의 객체의 포인트 세트에 대한 마스킹 입력, 상기 적어도 하나의 객체의 윤곽을 선택하는 것과 관련된 텍스트 입력 중 어 느 하나를 포함하는 사용자 입력을 획득하여 사용자 입력에 대한 정보를 마스크 생성 모듈로 제공할 수 있 다. 다양한 실시 예들에 따르면, 영상 생성 모듈은 상기 마스크 생성 모듈을 통해 획득된 복수의 이미지 프레임들 각각에 대응하는 마스크를 이용하여 상기 복수의 이미지 프레임들을 렌더링 함으로써 보정된 영상을 획득할 수 있다. 예를 들어, 복수의 이미지 프레임들 각각에 대응하는 마스크와 영상의 복수의 이미지 프레임들 을 컨볼루션(convolution) 함으로써 보정된 영상을 획득할 수 있다. 예를 들어, 마스크가 맵 데이터로 구성된 경우 이미지 프레임에 대한 마스크 값이'1'인 경우 지정된 색상으로 또는 제거하여 이미지 프레임을 출력할 수 있다. 또한, 마스크 맵 데이터가 '0'인 경우 객체로 판단하여 해당 픽셀의 이미지를 그대로 출력할 수 있다. 다양한 실시 예들에 따르면, 영상 생성 모듈은 객체 보정 모듈, 객체 선택 모듈 및/또는 마스크 생성 모듈로부터 제공되는 데이터에 기반하여, 원본 영상의 복수의 이미지 프레임들 별로 적어도 하나의 객체와 관련하여 보정한 영상 및 상기 원본 영상의 오디오 정보 및/또는 자막 정보를 인코딩(encoding)하여 보 정된 영상을 획득할 수 있다.다양한 실시 예들에 따르면, 평가 모듈은 마스크 생성 모듈로부터 생성된 마스크의 피드백과 관련된 UI/GUI를 사용자 장치를 통해 사용자에게 제공하고, 사용자 장치를 통해 사용자 피드백을 획득할 수 있다. 따라서, 다양한 실시 예들에 따르면, 평가 모듈은 복수의 이미지 프레임들 각각에 대하여 생성된 마스크에 대한 사용자 만족도를 나타내는 피드백 정보를 획득하고 상기 피드백 정보를 이용하여 상기 객체 인식 모듈 에 포함된 객체 인식 모델을 재학습 시킬 수 있다. 다양한 실시 예들에 따르면, 개인화 모듈은 마스크 생성 모듈로부터 생성된 마스크에 대한 정보, 다 양한 모듈을 통해 획득되는 사용자 입력 정보 및/또는 평가 모듈을 통해 획득된 피드백 정보를 이용하여 상기 마스크 생성 모듈이 사용자에게 특화되도록 제어할 수 있다. 예를 들어, 복수의 이미지 프레임들 각 각에 대하여 생성된 마스크에 대한 사용자 만족도를 나타내는 피드백 정보를 평가 모듈로부터 획득하고 상 기 피드백 정보를 이용하여 상기 객체 인식 모듈에 포함된 객체 인식 모델을 재학습 시켜 사용자 맞춤형 객체 인식 모델을 이용할 수 있다. 또는, 마스크 생성과 관련된 사용자 입력 정보를 저장해두고, 사용자가 재차 사용자 장치를 통해 반복적으로 입력하지 않더라도 상기 저장된 입력 정보에 기반하여 마스크를 생성(예: 매크 로 방식, 조작 히스토리 정보 이용)하도록 마스크 생성 모듈을 제어할 수 있다. 도 3의 실시 예에서, 영상 획득 모듈, 이미지 그룹핑 모듈, 객체 인식 모듈, 마스크 생성 모듈 , 객체 보정 모듈, 객체 선택 모듈, 영상 생성 모듈, 평가 모듈 및/또는 개인화 모듈 에 의해 수행되는 기능은, 프로세서가 저장 장치에 저장된 명령어들을 실행함으로써 수행되는 것으로 이해될 수 있다. 또한 다양한 실시 예에서, 전자 장치는 본 문서에서 개시되는 다양한 기능과 동작 을 수행하기 위해 하나 이상의 하드웨어 처리 회로를 이용할 수 있다 또한 도 3에 도시된 하드웨어/소프트웨어 사이의 연결 관계는 설명의 편의를 위한 것이며, 데이터나 명령의 흐 름/방향을 제한하지 않는다. 전자 장치에 포함되는 구성요소들은 다양한 전기적/작동적 연결 관계를 가질 수 있다. 도 4는 다양한 실시 예들에 따라 전자 장치가 적어도 하나의 객체를 제외한 영역이 제거된 영상을 생성하는 동 작을 나타내는 흐름도이다. 도 5는 다양한 실시 예들에 따라 전자 장치가 사용자 장치로부터 획득된 영상 내 복수의 이미지 프레임들을 그룹핑하여 적어도 하나의 객체에 대한 윤곽을 획득하는 동작을 나타내는 흐름도 이다. 이하에서 기술되는 동작들 각각은 서로 조합되어 수행될 수 있다. 또한, 이하에서 기술되는 동작들 중 전자 장 치(예: 도 1의 전자 장치)에 의한 동작은 전자 장치의 프로세서에 의한 동작을 의미할 수 있다. 아울러, 이하에서 기술되는 \"정보\"는 \"데이터\" 또는 \"신호\"의 의미로 해석될 수 있으며, \"데이터\"는 아날로그 데이터 및 디지털 데이터 모두를 포함하는 개념으로 이해될 수 있다. 다양한 실시예들에 따르면, 도 4 및 도 5에 도시되는 동작들은 도시되는 순서에 국한되지 않고 다양한 순서로 수행될 수 있다. 또한, 다양한 실시 예들에 따르면 도 4 및 도 5에 도시되는 동작들 보다 더 많은 동작들이 수 행되거나, 더 적은 적어도 하나의 동작이 수행될 수도 있다. 다양한 실시 예들에 따른 전자 장치의 동작들 중 상술한 설명과 중복되거나 유사한 설명은 생략될 수 있다. 도 4를 참조하면, 전자 장치는 동작 401에서 사용자 장치(예: 도 1의 사용자 장치)로부터 복수의 이 미지 프레임들을 포함하는 제1 영상을 획득할 수 있다. 예를 들어, 제1 영상은 일련의 시간적 흐름을 가진 복수 의 이미지 프레임들 및 상기 복수의 이미지 프레임들에 대응하는 오디오 정보, 자막 정보를 포함한 영상을 포함 할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작403에서 객체 인식 모델(예: 도 3의 객체 인식 모듈에 포함된 객체 인식 모델)을 이용하여 제1 영상에 포함된 적어도 하나의 객체에 대한 포인트 세트 및 바운딩 박스 세트를 획득할 수 있다. 예를 들어, 전자 장치는 저장 장치(예: 도 2의 저장 장치)에 저장된 객체 인 식 모델에 상기 제1 영상을 입력하고 객체 인식 모델로부터 출력되는 상기 제1 영상의 복수의 이미지 프레임들 각각에 대한 적어도 하나의 객체를 인식함에 따라 생성되는 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세 트를 획득할 수 있다. 다양한 실시 예들에 따르면, 상기 객체 인식 모델은 영상에 포함된 적어도 하나의 객체를검출 및 추적하여 적어도 하나의 객체에 대한 스켈레톤 데이터를 추출한 포인트 세트 및 바운딩 박스 세트를 출 력하도록 학습된 모델일 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 405에서 포인트 세트 및 바운딩 박스 세트에 기반하여 적어 도 하나의 객체의 윤곽을 식별할 수 있다. 예를 들어, 전자 장치는 상기 제1 영상에 포함된 상기 적어도 하나의 객체에 대한 스켈레톤 데이터를 추출한 상기 포인트 세트와 상기 적어도 하나의 객체의 적어도 하나의 바운딩 박스를 포함한 상기 바운딩 박스 세트를 모두 적용하여 상기 적어도 하나의 객체의 윤곽을 식별할 수 있 다. 다양한 실시 예들에 따르면, 전자 장치는 동작 407에서 제1 영상으로부터 식별된 적어도 하나의 객체의 윤 곽에 기반하여 적어도 하나의 객체를 분할하는 마스크를 획득할 수 있다. 예를 들어, 전자 장치는 상기 제 1 영상의 복수의 이미지 프레임들 별로 상기 적어도 하나의 객체를 분할하는 마스크를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 적어도 하나의 객체의 윤곽에 기반하여 적어도 하나의 객체 를 분할하는 복수의 마스크들을 획득하고, 복수의 마스크들의 신뢰도에 기반하여 상기 마스크를 결정함으로써 획득할 수 있다. 예를 들어, 전자 장치는 복수의 마스크들 및 복수의 마스크들 각각에 대한 신뢰도 값을 획득하고, 상기 신뢰도 값이 가장 높은 마스크를 이미지 프레임에 대응하는 상기 마스크로 결정할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 409에서, 마스크를 이용하여 제1 영상의 적어도 하나의 객 체를 제외한 영역이 제거된 제2 영상을 획득할 수 있다. 예를 들어, 전자 장치는 상기 제1 영상의 복수의 이미지 프레임들 각각에 대한 마스크를 이용하여 상기 제1 영상의 복수의 이미지 프레임들 별로 상기 적어도 하 나의 객체를 제외한 영역이 제거된 제2 영상을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 411에서, 사용자 장치가 제2 영상을 출력하도록 제2 영상을 전달할 수 있다. 예를 들어, 전자 장치는 통신 장치를 통해 사용자 장치로 상기 제2 영상을 전 달할 수 있다. *도 5를 참조하면, 전자 장치는 동작 401과 유사하게 동작 501에서 사용자 장치로부터 복수의 이미지 프레 임들을 포함하는 제1 영상을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 503에서 복수의 이미지 프레임들을 기-설정된 기준 단위로 그룹핑하여 이미지 프레임 그룹들을 생성할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 객체 인식 모델을 이용하여 상기 제1 영상의 장면 전환을 인식할 수 있다. 예를 들어, 전자 장치는 객체 인식 모델을 이용하여 상기 제1 영상의 복수의 이미지 프레 임들 각각에 포함된 적어도 하나의 객체를 식별하고, 상기 적어도 하나의 객체의 변경, 적어도 하나의 객체의 종류 변경, 적어도 하나의 객체의 개수 변경, 상기 복수의 이미지 프레임들 각각의 주된 색상 값 변경, 상기 제 1 영상의 오디오 정보, 상기 제1 영상의 자막 정보, 복수의 이미지 프레임들에 대한 순서 정보, 상기 복수의 이 미지 프레임들 각각의 촬영 시간 정보, 이미지 프레임 그룹핑에 대한 사용자 입력 중 적어도 하나에 기반하여 장면 전환을 인식할 수 있다. 또한, 상기 장면 전환을 기준으로 상기 제1 영상의 복수의 이미지 프레임들을 그 룹핑하여 상기 이미지 프레임 그룹들을 생성할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 505에서 객체 인식 모델을 이용하여 이미지 프레임 그룹들 별로 적어도 하나의 객체를 추적하여 포인트 세트 및 바운딩 박스 세트를 획득할 수 있다. 예를 들어, 전자 장 치 이미지 프레임 그룹들 별로 적어도 하나의 객체를 추적하여 보다 빠르게 높은 정확도로 포인트 세트 및 바운딩 박스 세트를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 복수의 이미지 프레임들 각 각에 대한 포인트 세트 및 바운딩 박스 세트를 획득함에 있어서, 포인트 세트 및 바운딩 박스 세트에 대응하는 이미지 프레임 그룹에 대한 식별 정보 및 이미지 프레임의 순서 정보를 매핑(Mapping)하여 저장할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 505에서 획득된 포인트 세트 및 바운딩 박스 세트를 이용하 여 동작 403 내지 동작 411을 이어서 수행할 수 있다. 도 6은 다양한 실시 예들에 따라 전자 장치가 생성한 마스크를 설명하기 위한 도면이다. 도 7은 다양한 실시 예 들에 따라 전자 장치가 생성한 마스크에 기반하여 배경이 제거된 영상을 설명하기 위한 도면이다. 도 6을 참조하면, 전자 장치가 제1 영상의 복수의 이미지 프레임들 중 하나의 제1 원본 이미지 프레임에 대해 적어도 하나의 객체를 제외한 나머지 영역을 제거한 제1 보정 이미지 프레임을 획득하는 것과 관련된 도면 이 도시된다. 도 7을 참조하면, 전자 장치가 복수의 이미지 프레임들을 포함하는 제1 영상에 대한 마스크를 획득하고, 상기 마스크에 기반하여 적어도 하나의 객체를 제외한 나머지 영역을 제거한 제2 영상을 획득하는 것과 관련된 도면이 도시된다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치로부터 획득되는 제1 영상에 포함된 복수의 이미지 프레임들에 포함된 적어도 하나의 객체에 대한 포인트 세트 및 바운딩 박스 세트를 획득할 수 있다. 예를 들어, 전자 장치는 객체 인식 모델을 이용하여 제1 영상에 포함된 복수의 이미지 프레임들 중 제1 원 본 이미지 프레임에 대한 바운딩 박스 세트를 획득할 수 있다. 예를 들어, 객체 인식 모델은 제1 원본 이 미지 프레임에 포함된 객체인 넥타이 객체와 사람 객체를 식별할 수 있다. 따라서, 바운딩 박스 세트는 사 람 객체에 대한 바운딩 박스 및 넥타이 객체에 대한 바운딩 박스를 포함할 수 있다. 예를 들어, 전자 장치는 객체 인식 모델을 이용하여 제1 영상에 포함된 복수의 이미지 프레임들 중 제1 원 본 이미지 프레임에 대한 포인트 세트를 획득할 수 있다. 예를 들어, 객체 인식 모델은 제1 원본 이미지 프레임에 포함된 사람에 대한 스켈레톤 데이터를 추출 및 추적하여 각 스켈레톤 데이터 별 키-포인트들을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 객체 인식 모델을 이용하여 획득되는 바운딩 박스 세트 및 포인트 세트에 기반하여 제1 원본 이미지 프레임에 포함된 적어도 하나의 객체(예: 사람)에 대한 윤곽 을 식별할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 적어도 하나의 객체(예: 사람)에 대한 바운딩 박스 세트 및 포인트 세트를 모두 활용하여 보다 정확도 높게 상기 적어도 하나의 객체에 대 한 윤곽을 식별할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제1 원본 이미지 프레임의 윤곽에 기반하여 상기 적어도 하 나의 객체(예: 사람)을 분할하는 마스크를 획득할 수 있다. 상기 마스크를 이용하여 상기 제1 원본 이미지 프레임의 상기 적어도 하나의 객체(예: 사람)를 제외한 영역이 제거된 제1 보정 이미지 프레임을 획득할 수 있다. 다양한 실시 예들에 따른, 전자 장치는 상기 제1 보정 이미지 프레임을 생성한 것과 유사한 방법을 통해 상기 제1 영상에 포함된 복수의 이미지 프레임들 각각에서 적어도 하나의 객체를 제외한 영역이 제거된 제 2 영상을 획득할 수 있다. 예를 들어, 전자 장치는 제1 영상에 포함된 복수의 이미지 프레임들을 포함하는 제1 영상 프레임들 각각에 대하여 마스크들을 획득할 수 있다. 또한, 전자 장치는 상기 마스크들에 기반하여 상기 제1 영상 프레임들 각각에서 적어도 하나의 객체를 제외한 영역이 제거된 복수의 이미지 프레임들을 포함 하는 제2 영상 프레임들을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제2 영상 프레임들에 상기 제1 영상에 대한 오디오 정보 및 /또는 상기 제1 영상에 대한 자막 정보를 포함하여 인코딩 동작을 수행함에 따라 제2 영상을 획득할 수 있다. 도 8은 다양한 실시 예들에 따라 전자 장치가 객체 인식 모델을 이용하여 생성된 적어도 하나의 객체에 대한 윤 곽에 대하여 사용자 입력을 획득하는 동작을 나타내는 흐름도이다. 도 9는 다양한 실시 예들에 따라 전자 장치가 사용자 입력에 기반하여 적어도 하나의 객체와 관련해 보정된 영상을 생성하는 동작을 나타내는 흐름도 이다. 이하에서 기술되는 동작들 각각은 서로 조합되어 수행될 수 있다. 또한, 이하에서 기술되는 동작들 중 전자 장 치(예: 도 1의 전자 장치)에 의한 동작은 전자 장치의 프로세서에 의한 동작을 의미할 수 있다. 아울러, 이하에서 기술되는 \"정보\"는 \"데이터\" 또는 \"신호\"의 의미로 해석될 수 있으며, \"데이터\"는 아날로그 데이터 및 디지털 데이터 모두를 포함하는 개념으로 이해될 수 있다.다양한 실시예들에 따르면, 도 8 및 도 9에 도시되는 동작들은 도시되는 순서에 국한되지 않고 다양한 순서로 수행될 수 있다. 또한, 다양한 실시 예들에 따르면 도 8 및 도 9에 도시되는 동작들 보다 더 많은 동작들이 수 행되거나, 더 적은 적어도 하나의 동작이 수행될 수도 있다. 다양한 실시 예들에 따른 전자 장치의 동작들 중 상술한 설명과 중복되거나 유사한 설명은 생략될 수 있다. 도 8를 참조하면, 전자 장치는 동작 801에서 사용자 장치(예: 도 1의 사용자 장치)로부터 복수의 이 미지 프레임들을 포함하는 제1 영상을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 803에서, 영상에 포함된 객체의 포인트 세트 및 바운딩 박 스 세트를 추출하도록 학습된 객체 인식 모델을 이용하여 상기 제1 영상에 포함된 적어도 하나의 객체에 대한 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 805에서, 상기 제1 포인트 세트 및 상기 제1 바운딩 세트에 기반하여 적어도 하나의 객체의 제1 윤곽을 식별할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 807에서, 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트를 표시하도록 사용자 장치에 전달할 수 있다. 예를 들어, 사용자 장치는 표시 장치를 통해 상기 제1 영상 에 포함된 복수의 이미지 프레임들 별로 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트 각각에 대한 시각 적 객체를 오버랩(overlap)하여 표시할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 809에서, 사용자 장치로부터 복수의 이미지 프레임들 중 제 1 이미지 프레임에 포함된 상기 적어도 하나의 객체에 대한 제1 사용자 입력을 획득할 수 있다. 예를 들어, 전 자 장치는 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트가 표시된 화면의 적어도 하나의 객체에 대 한 사용자 입력을 획득할 수 있다. 다양한 실시 예들에 따르면, 상기 사용자 입력은 상기 제1 이미지 프레임의 상기 적어도 하나의 객체에 대한 다 양한 입력을 포함할 수 있다. 예를 들어, 상기 적어도 하나의 객체를 선택하는 입력, 상기 적어도 하나의 객체 의 포인트 세트에 대한 입력, 상기 적어도 하나의 객체의 바운딩 박스에 대한 입력, 상기 적어도 하나의 객체의 포인트 세트에 대한 마스킹 입력, 상기 적어도 하나의 객체의 윤곽을 선택하는 것과 관련된 텍스트 입력 중 적 어도 하나를 포함할 수 있다. 도 9를 참조하면, 전자 장치는 동작 901에서, 제1 사용자 입력에 기반하여 제1 이미지 프레임의 적어도 하 나의 객체의 제2 윤곽을 식별할 수 있다. 예를 들어, 프로세서는 상기 제1 영상의 복수의 이미지 프레임들 중 상기 제1 이미지 프레임의 적어도 하나의 객체의 윤곽은 상기 사용자 입력에 기반하여 상기 제2 윤곽으로 식 별할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 903에서, 상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 상기 제1 영상에 대한 적어도 하나의 객체를 분할하는 마스크를 획득할 수 있다. 예를 들어, 상기 제1 영상에 포함된 복수의 이미지 프레임들 중 상기 제1 이미지 프레임의 적어도 하나의 객체에 대한 제2 윤곽과 나머지 이미지 프 레임들의 적어도 하나의 객체에 대한 제1 윤곽에 기반하여 마스크를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 905에서, 마스크를 이용해 적어도 하나의 객체와 관련하여 제1 영상을 보정한 제2 영상을 획득할 수 있다. 예를 들어, 전자 장치는 상기 제1 윤곽 및 상기 제2 윤곽 에 기반하여 획득된 마스크를 이용해, 상기 제1 영상의 복수의 이미지 프레임들 별로 상기 적어도 하나의 객체 와 관련하여 상기 제1 영상을 보정한 제2 영상을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 동작 907에서, 사용자 장치가 제2 영상을 출력하도록 제2 영상을 전달할 수 있다. 예를 들어, 전자 장치는 통신 장치를 통해 사용자 장치로 상기 제2 영상을 전 달할 수 있다. 도 10은 다양한 실시 예들에 따라 전자 장치가 사용자 입력에 기반하여 생성한 마스크를 통해 보정한 영상을 설 명하기 위한 도면이다. 도 10을 참조하면, 전자 장치(예: 도 1의 전자 장치)는 도 1의 사용자 장치로부터 복수의 이미 지 프레임들을 포함하는 원본 영상(예: 도 8의 제1 영상)을 획득할 수 있다. 다양한 실시 예들에 따르면,원본 영상은 사람(예: 도 2의 적어도 하나의 객체)을 포함한 복수의 이미지 프레임들로 구성될 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 원본 영상에 포함된 객체의 포인트 세트 및 바운딩 박스 세트를 추출하도록 학습된 객체 인식 모델(예: 도 3의 객체 인식 모델)을 이용하여 적어도 하나의 객체에 대한 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제1 포인트 세트 및 제1 바운딩 박스 세트에 기반하여 원본 영상 내 복수의 이미지 프레임들 각각에 포함된 사람의 윤곽을 식별할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 원본 영상으로부터 식별된 사람의 윤곽에 기반하여, 원본 영상 내 복수의 이미지 프레임들 각각에서 사람을 분할하는 마스크(들)을 획득할 수 있다. 예를 들어, 전 자 장치는 원본 영상의 복수의 이미지 프레임들 별로 사람을 분할하는 제1 마스크 세트를 획 득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 객체 인식 모델을 이용하여 생성된 상기 제1 포인트 세트 및/또는 상기 제1 바운딩 박스 세트를 표시하도록 사용자 장치에 전달할 수 있다. 예를 들어, 사용자 장치 는 표시 장치를 통해 상기 원본 영상에 포함된 복수의 이미지 프레임들 별로 상기 제1 포인트 세트 및/또는 상기 제1 바운딩 박스 세트 각각에 대한 시각적 객체를 오버랩(overlap)하여 표시할 수 있다. 예를 들 어, 원본 영상의 복수의 이미지 프레임들 각각별로 상기 시각적 객체를 오버랩한 영상을 표시할 수 있다. 한편, 다양한 실시 예들에 따르면, 객체 인식 모델을 이용하여 식별된 사람의 윤곽에서 일부 수정이 필요한 경 우가 있다. 예를 들어, 원본 영상의 복수이 이미지 프레임들 각각에 대한 사람의 윤곽 중에서 제1 이미지 프레임 내 사람의 실제 윤곽과 객체 인식 모델을 통해 획득된 윤곽이 차이가 있는 경우, 제1 이미지 프레임의 사람 윤곽에 대해서는 수정이 필요할 수 있다. 또한, 수정이 필요한 해당 윤곽에 기반하여 생성된 제1 마스크 에 대한 수정이 필요할 수 있다. 다양한 실시 예들에 따르면, 사용자는 복수의 이미지 프레임들 각각별로 상기 시각적 객체를 오버랩한 영상 을 통해 제1 이미지 프레임의 사람에 대한 윤곽의 수정이 필요하다고 판단할 수 있다. 이 경우 사용자는 시각적 객체를 오버랩한 영상의 복수의 이미지 프레임들 중 제1 이미지 프레임에 포함된 상기 사람 에 대한 사용자 입력 동작을 수행할 수 있다. 예를 들어, 사용자 장치는 제1 이미지 프레임의 사람 에 대한 바운딩 박스 설정 입력을 획득할 수 있다. 이에 따라, 사용자 장치는 제1 이미지 프레임에 사용자가 설정한 바운딩 박스가 오버랩된 이미지를 표시 장치를 통해 표시할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 입력에 기반하여 제1 이미지 프레임의 적어도 하나의 객 체의 수정 윤곽을 식별할 수 있다. 예를 들어, 전자 장치는 상기 바운딩 박스 설정 입력에 기반하여 제1 이미지 프레임 내 사람의 수정 윤곽을 식별할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 수정 윤곽에 기반하여 제1 이미지 프레임에서 사람을 분할하는 마스크를 획득할 수 있다. 또한, 전자 장치는 원본 영상에 포함된 복수의 이미지 프레임들 중 상기 제1 이미지 프레임의 사람에 대한 수정 윤곽과 나머지 이미지 프레임들의 적어도 하나의 객체에 대한 윤곽에 기 반하여 수정 마스크 세트를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 수정 마스크 세트를 이용해 사람과 관련하여 원본 영상을 보정한 보정 영상을 획득할 수 있다. 예를 들어, 전자 장치는 수정 마스크를 이용해, 상기 원본 영 상의 복수의 이미지 프레임들 별로 사람과 관련하여 상기 원본 영상을 보정한 보정 영상을 획득할 수 있다. 예를 들어, 전자 장치는 원본 영상에서 사람을 제외한 배경을 제거한 보정 영상 을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치가 보정 영상을 출력하도록 사용자 장치 로 보정 영상을 전달할 수 있다. 따라서, 사용자 장치는 보정 영상을 표시 장치를 통해 표시할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 객체 인식 모델을 이용해 획득되는 적어도 하나의 객체에 대한 윤곽에 사용자 입력을 활용함으로써, 보다 높은 정확도로 적어도 하나의 윤곽을 식별할 수 있다. 따라서, 원본 영상에 대하여 보다 사용자 의도를 정확히 반영한 영상을 획득할 수 있다.도 11은 다양한 실시 예들에 따라 전자 장치가 사용자 입력에 기반하여 생성한 마스크를 통해 보정한 영상을 설 명하기 위한 도면이다. 다양한 실시 예들에 따르면, 이미지 프레임에서 적어도 하나의 객체에 대한 사용자 입력은 다양한 입력을 포함 할 수 있다. 예를 들어, 상기 적어도 하나의 객체를 선택하는 입력, 상기 적어도 하나의 객체의 포인트 세트에 대한 입력, 상기 적어도 하나의 객체의 바운딩 박스에 대한 입력, 상기 적어도 하나의 객체의 포인트 세트에 대 한 마스킹 입력, 상기 적어도 하나의 객체의 윤곽을 선택하는 것과 관련된 텍스트 입력 중 적어도 하나를 포함 할 수 있다. 도 11을 참조하면, 상기 사용자 입력이 이미지 프레임 내에 포함된 복수의 객체들 중에 적어도 하나의 객체를 선택하는 사용자 입력이 도시된다. 다양한 실시 예들에 따르면, 전자 장치(예: 도 1의 전자 장치)는 도 1의 사용자 장치로부터 복 수의 이미지 프레임들을 포함하는 제1 영상을 획득하고, 제1 영상에 포함된 적어도 하나의 객체의 포인트 세트 및 바운딩 박스 세트를 추출하도록 학습된 객체 인식 모델(예: 도 3의 객체 인식 모델)을 이용하여 적어도 하나 의 객체에 대한 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득할 수 있다. 예를 들어, 전자 장치는 복 수의 사람들을 포함하는 제1 영상을 획득하고, 제1 영상에 포함된 복수의 사람들에 대한 제1 포인트 세트 및 제 1 바운딩 박스 세트를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제1 포인트 세트 및 제1 바운딩 박스 세트에 기반하여 복수의 이 미지 프레임들 각각에 포함된 복수의 사람들의 윤곽을 식별할 수 있다. 예를 들어, 전자 장치는 제1 이미 지 프레임에 포함된 복수의 사람들 각각의 윤곽을 식별할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제1 영상으로부터 식별된 복수의 사람들 각각의 윤곽에 기반하여, 복수의 이미지 프레임들 각각에서 복수의 사람들을 분할하는 마스크(들)을 획득할 수 있다. 예를 들 어, 전자 장치는 제1 이미지 프레임에 포함된 복수의 사람들을 분할하는 제1 마스크를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 객체 인식 모델을 이용하여 생성된 상기 제1 포인트 세트 및/또는 상기 제1 바운딩 박스 세트를 표시하도록 사용자 장치에 전달할 수 있다. 예를 들어, 사용자 장치 는 표시 장치를 통해 상기 제1 영상에 포함된 복수의 이미지 프레임들 별로 상기 제1 포인트 세트 및/또는 상기 제1 바운딩 박스 세트 각각에 대한 시각적 객체를 오버랩(overlap)하여 표시할 수 있다. 예를 들어, 사용 자 장치는 표시 장치를 통해 제1 이미지 프레임에 포함된 복수의 사람들 각각에 제1 포인트 세트를 나타내는 시각적 객체를 오버랩한 이미지 프레임을 표시할 수 있다. 한편, 다양한 실시 예들에 따르면, 객체 인식 모델을 이용하여 식별된 사람의 윤곽에서 일부 수정이 필요한 경 우가 있다. 예를 들어 사용자가 특정 사람만을 선택한 영상을 획득하려는 경우(예: 원본 영상 내에서 특정 사람 을 제외한 나머지 영역을 제거하려는 경우), 제1 영상의 복수이 이미지 프레임들 각각에 대한 복수의 사람들 각 각의 윤곽 중에서 상기 특정 사람을 제외한 다른 사람의 윤곽에 대해서는 수정이 필요할 수 있다. 이에 따라, 제1 마스크에서 제2 사람에 대한 마스크 영역에 대해서도 수정이 필요할 수 있다. 다양한 실시 예들에 따르면, 제1 영상에서 사용자가 특정 사람(예: 제1 사람)만을 선택한 영상을 획득하려는 경 우, 사용자는 제1 영상 내 복수의 이미지 프레임들 중 적어도 하나의 이미지 프레임에서 선택 대상이 아닌 사람 에 대한 사용자 입력 동작을 수행할 수 있다. 예를 들어, 사용자 장치는 제1 이미지 프레임의 제1 사람 및 제2 사람을 포함하는 복수의 사람들 중 제2 사람의 영역에 대한 사용자 입력을 획득할 수 있다. 예를 들어, 사용자 장치는 제2 사람에 대한 포인트 세트를 해제하고자 하는 사용자 입력을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 입력에 기반하여 제1 이미지 프레임의 적어도 하나 의 객체의 수정 윤곽을 식별할 수 있다. 예를 들어, 전자 장치는 제2 사람에 대한 포인트 세트를 해제하고 자 하는 사용자 입력에 기반하여 제1 이미지 프레임 내 복수의 사람들 중 제1 사람에 대한 윤곽만을 유지 한 수정 윤곽을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 수정 윤곽에 기반하여 제1 이미지 프레임에서 제1 사람을 분할하는 마스크를 획득할 수 있다. 예를 들어, 제1 이미지 프레임에 대한 제1 마스크 및 제2 사람에 대한 선택을 해제하고자 하는 사용자 입력에 기반하여 수정 마스크를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제1 영상의 복수의 이미지 프레임들 각각에 대한 마스크 세트에 제1 이미지 프레임 과 유사한 동작을 수행하여 제1 사람만을 분할하도록 하는 마스크 세트를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 수정 마스크를 이용해 제1 이미지 프레임의 복수의 사람들 중에서 제1 사람을 제외한 나머지 영역을 제거한 보정 이미지 프레임을 획득할 수 있다. 전자 장치 는 유사한 방법을 통해 제1 영상에서 제1 사람을 제외한 나머지 영역을 제거한 제2 영상(예: 도 9의 제2 영상) 을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치가 제2 영상을 출력하도록 사용자 장치로 제2 영상을 전달할 수 있다. 따라서, 사용자 장치는 제2 영상을 표시 장치를 통해 표시할 수 있다. 도 12는 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 실행 화면을 나타내는 도면이다. 도 12를 참조하면, 사용자 장치는 전자 장치를 통해 획득되는 제어 신호에 기반하여 동영상 편집 서 비스와 관련된 실행 화면을 표시할 수 있다. 사용자 장치를 통해 표시되는 동영상 편집 서비스의 실행 화면 은 제1 영역, 제2 영역, 제3 영역, 제4 영역, 제5 영역 및/또는 제6 영 역을 포함할 수 있다. 본 문서에서 아이콘은 버튼, 메뉴, 객체 등의 표현으로 대체될 수 있다. 또한 도 12에서 제1 영역, 제2 영역, 제3 영역, 제4 영역, 제5 영역 및/또는 제6 영역에 도시된 아이콘들은 예시적인 것이며, 다른 아이콘이 배치되거나, 임의의 아이콘이 다른 아이콘으로 대체되거나 생략될 수 있다. 다양한 실시 예들에 따르면, 사용자 장치는 전자 장치의 제어 신호에 기반하여 실행 화면을 표 시할 수 있다. 다양한 실시 예들에 따르면, 상기 실행 화면은 동영상 편집과 관련된 기능들 중에서 동영 상 재생과 관련된 기능을 실행하는 아이콘(예: 되돌리기, 앞으로 돌리기 등), 동영상 편집 서비스에서 제공되는 다양한 기능을 선택하기 위한 메뉴 아이콘, 전자 장치를 통해 실행된 동영상 편집 내용을 저장하기위한 저 장 아이콘 등을 포함하는 제1 영역을 포함할 수 있다. 전자 장치는 사용자 장치를 통해 획득되 는 제1 영역에 대한 사용자 입력에 기반하여, 동영상 편집 기능을 실행할 수 있다. 다양한 실시 예들에 따르면, 상기 실행 화면은 동영상 편집과 관련된 기능들 중에서 동영상 내 적어도 하 나의 객체에 대한 보정을 요청하는 사용자 입력과 관련된 제2 영역을 포함할 수 있다. 예를 들어, 사용자 장치는 제2 영역에 표시된 아이콘들에 대한 사용자 입력을 통해 도 8 내지 도 9를 참조하여 설명된 사용자 입력을 획득할 수 있다. 예를 들어, 실행 화면은 동영상 내 이미지 프레임에 대한 이미지 이동 및 /또는 이미지 회전 기능 실행을 위한 아이콘, 이미지 프레임 내 적어도 하나의 객체에 대한 바운딩 박스 입력과 관련된 아이콘, 적어도 하나의 객체에 대한 포인트 입력과 관련된 아이콘 등 다양한 아이콘 등이 배치되 는 제2 영역을 포함할 수 있다. 다양한 실시 예들에 따르면, 바운딩 박스 입력과 관련된 아이콘에 대한 사용자 입력이 획득되는 경우, 바 운딩 박스를 그리는 것과 관련된 기능 실행을 위한 제3 영역이 추가로 표시될 수 있다. 예를 들어, 제3 영역에 이미지 프레임 내 적어도 하나의 객체의 종류를 선택하기 위한(즉, 객체 인식도를 높이기 위한 레 이블(label)을 선택하기 위한) 메뉴 아이콘, 사용자 입력 방식을 선택하기 위한 아이콘으로 단일 이미지 프레임에 바운딩 박스를 그리기 위한 아이콘, 멀티 이미지 프레임에 바운딩 박스를 그리기 위한 아이콘 , 바운딩 박스를 프롬프트 입력 및 세그멘테이션 입력으로 그리기 위한 아이콘이 배치될 수 있다. 다양한 실시 예들에 따르면, 상기 아이콘이 선택되는 경우 도 2 내지 도 9를 참조하여 설명된, 객체 인식 모델을 이용한 바운딩 박스 세트 출력과 사용자 입력을 통한 바운딩 박스 입력을 모두 이용할 수 있다. 다양한 실시 예들에 따르면, 실행 화면은 업로드된 영상 내 복수의 이미지 프레임들에 대한 시각적 객체 와 대한 타임 라인을 표시하는 제4 영역, 이미지 편집과 관련된 편집 영역으로 제5 영역, 이 미지 내 객체의 라벨을 표시하는 영역으로 제6 영역 및/또는 제7 영역을 포함할 수 있다. 도 13a는 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 바운딩 박스에 대한 사용자 입력의 실행 화면 을 나타낸다. 도 13b는 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 바운딩 박스에 대한 사용자 입력 의 실행 화면을 나타낸다. 도 13c는 다양한 실시 예들에 따라 바운딩 박스에 대한 사용자 입력이 획득됨에 따라사용자 장치를 통해 출력되는 실행 화면을 나타낸다. 이하, 도시되는 실행화면들은 전자 장치가 사용자 장치를 제어함으로써 사용자 장치를 통해 표 시되는 화면으로 이해될 수 있다. 도 13a를 참조하여, 사용자 장치를 통해 제1 영상이 획득됨에 따라, 전자 장치는 도 12를 참조하여 설명된 제4 영역에 제1 영상의 복수의 이미지 프레임들에 대한 시각적 객체를 표시하고, 사용자 입력에 기반하여 선택된 이미지 프레임에 대한 시각적 객체를 제5 영역에 표시하도록 사용자 장치를 제어할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 업로드된 제1 영상 내 복수의 이미지 프레임들 중 제1 이미지 프레임이 선택되고, 바운딩 박스 입력과 관련된 아이콘에 대한 사용자 입 력이 획득되는 경우, 제5 영역에 제1 이미지 프레임 및 제1 이미지 프레임에 대한 픽셀 정보 를 표시할 수 있다. 이때, 사용자 장치는 제5 영역에 바운딩 박스 입력을 보조할 수 있는 가 로축 및/또는 세로축 직선을 추가로 표시할 수 있다. 도 13b를 참조하면, 전자 장치는 사용자 장치를 통해 제1 이미지 프레임에 대한 바운딩 박스 입력과 관련된 사용자 입력이 획득되는 경우, 사용자 장치를 통해 바운딩 박스 설정과 관련된 입력을 획득할 수 있다. 예를 들어, 사용자는 제5 영역에 표시된 제1 이미지 프레임 내에서 적어도 하나의 객체의 바 운딩 박스를 선택하려는 경우 시작 클릭, 드래그-앤-드롭 형태로 바운딩 박스를 입력할 수 있다. 이에 따라, 전 자 장치는 시작 클릭이 입력된 부분의 (x,y) 좌표 및 드롭 이벤트가 발생한 부분의 (x,y) 좌표를 통해 사 용자가 선택한 바운딩 박스의 영역을 식별할 수 있다. 사용자 입력을 통해 식별된 상기 바운딩 박스 영역은 제5 영역에 표시된 제1 이미지 프레임에 오버랩된 형태로 바운딩 박스에 대한 시각적 객체로 표 시될 수 있다. 다양한 실시 예들에 따르면, 사용자 입력이 획득됨에 따라, 사용자 장치는 바운딩 박스 입력이 완료되었음 을 나타내는 아이콘 및 바운딩 박스 입력을 추가로 진행하거나 동일한 입력을 수행하기 위한 아이콘 을 제5 영역에 표시할 수 있다. 다양한 실시 예들에 따르면, 바운딩 박스에 대한 사용자 입력이 획득됨에 따라, 사용자 장치는 제4 영역 에 표시된 제1 영상의 내 복수의 이미지 프레임들을 나타내는 시각적 객체와 연관하여 사용자 입력 에 대응되는 이미지 프레임들을 나타내는 시각적 객체를 표시할 수 있다. 예를 들어, 전자 장치는 복수의 이미지 프레임들을 그룹핑하고, 사용자 입력에 대응되는 이미지 프레임이 포함된 제1 이미지 프레 임 그룹의 이미지 프레임들을 나타내는 시각적 객체를 표시하도록 제어할 수 있다. 또한, 전자 장치(20 0)는, 획득된 사용자 입력을 제1 이미지 프레임 그룹에 후속하는 이미지 프레임 그룹에도 적용하는 기능 실행과 관련된 아이콘을 제4 영역에 표시하도록 제어할 수 있다. 도 13c를 참조하면, 상기 아이콘에 대한 사용자 입력이 획득됨에 따라 사용자 장치를 통해 표시되는 실행 화면이 도시된다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 이미지 프레임 내 적어도 하나의 객체 에 대하여 바운딩 박스를 선택하는 사용자 입력을 획득하고, 획득된 사용자 입력을 상기 제1 이미지 프레임 그 룹에 후속하는 이미지 프레임 그룹에도 적용하는 기능 실행과 관련된 아이콘에 대한 사용자 입력을 획득 할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 아이콘에 대한 사용자 입력이 획득되는 경우, 제1 이 미지 프레임 그룹에 후속하는 이미지 프레임 그룹에도 적어도 하나의 객체에 대한 바운딩 박스 입력을 적용하고, 바운딩 박스 입력이 적용된 이미지 프레임 그룹을 나타내는 시각적 객체를 표시하도록 제어할 수 있다. 또한, 전자 장치는 상기 아이콘과 유사한 기능을 수행하는 아이콘을 제4 영역(124 0)에 표시하도록 제어할 수 있다. 다양한 실시 예들에 따르면, 사용자는 특정 이미지 프레임 내 적어도 하나의 객체에 대한 바운딩 박스를 입력하 고, 상기 입력을 복수의 이미지 프레임(예: 후속하는 이미지 프레임들)에도 그대로 적용할 수 있다. 도 14는 다양한 실시 예들에 따라 전자 장치가 사용자 입력을 적용하는 것을 설명하기 위한 도면이다. 도 14를 참조하면, 도 12를 참조하여 설명된 제4 영역에 표시되는 복수의 이미지 프레임들을 나타내는 시 각적 객체가 도시된다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 획득되는 제1 영상 내 복수의 이미지 프레임들을 나타내는 시각적 객체를 표시할 수 있다. 또한, 전자 장치는 상기 복수의 이미지 프레임 들에 대하여 도 3을 참조하여 설명된 객체 인식 모델을 이용하여 적어도 하나의 객체에 대한 제1 윤곽을 식별할 수 있다. 상기 객체 인식 모델을 이용하여 복수의 이미지 프레임들 각각에 대하여 획득된 제1 윤곽은 베이스 세 그먼트(Base segment)로 이해될 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 복수의 이미지 프레임들 중 특정 이미지 프레임(들)에 대하 여, 적어도 하나의 객체의 바운딩 박스를 선택하는 사용자 입력을 획득할 수 있다. 예를 들어 전자 장치는 복수의 이미지 프레임들을 그룹핑한 이미지 그룹들 중 제1 이미지 그룹 및 제2 이미지 그룹 내 이미지 프레임들 에 바운딩 박스를 선택하는 사용자 입력을 획득할 수 있다. 전자 장치는 상기 사용자 입력에 기반하여 제1 이미지 그룹 및 제2 이미지 그룹 내 이미지 프레임들의 적어도 하나의 객체에 대한 제2 윤곽을 식별할 수 있다. 제1 이미지 그룹 및 제2 이미지 그룹의 이미지 프레임들 각각에 대하여 획득된 제2 윤곽은 제1 사용자 선택 세 그먼트(Bbox Human1)로 이해될 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제1 사용자 선택 세그먼트와 유사한 방법으로 제2 사용자 선택 세그먼트(Bbox Human2)을 획득할 수 있다. 예를 들어, 전자 장치는 복수의 이미지 프레임들 중 제3 이미지 그룹의 적어도 하나의 객체에 대한 바운딩 박스를 선택하는 사용자 입력을 획득하고, 사용자 입력에 기반하여 적어도 하나의 객체에 대한 제3 윤곽을 식별할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 제1 윤곽, 상기 제2 윤곽 및/또는 상기 제3 윤곽에 기반하 여 제1 영상의 적어도 하나의 객체를 구분하는 최종 세그먼트를 획득할 수 있다. 다양한 실시 예들에 따 르면, 전자 장치는 상기 최종 세그먼트에 기반하여 제1 영상에서 복수의 이미지 프레임별로 적어도 하나의 객체를 구분한 마스크를 생성할 수 있다. 도 15는 다양한 실시 예들에 따라 바운딩 박스에 대한 사용자 입력이 객체 추가 입력인 경우 전자 장치가 마스 크를 생성하는 것을 설명하기 위한 도면이다. 도 15를 참조하면, 도 13a 내지 도 13b를 참조하여 설명된 바운딩 박스 선택에 대한 사용자 입력이 이미지 프레 임 내에 포함된 복수의 객체들 중에 적어도 하나의 객체를 선택하는 사용자 입력이 도시된다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치로부터 복수의 이미지 프레임들을 포함하는 제1 영상을 획득하고, 제1 영상에 포함된 적어도 하나의 객체의 포인트 세트 및 바운딩 박스 세트를 추출하도록 학 습된 객체 인식 모델(예: 도 3의 객체 인식 모델)을 이용하여 적어도 하나의 객체에 대한 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득할 수 있다. 예를 들어, 전자 장치는 복수의 사람들을 포함하는 제1 영상을 획득하고, 제1 영상에 포함된 복수의 사람들 중 제1 사람에 대한 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제1 포인트 세트 및 제1 바운딩 박스 세트에 기반하여 복수의 이 미지 프레임들 각각에 포함된 적어도 하나의 객체의 윤곽을 식별할 수 있다. 예를 들어, 전자 장치는 제1 이미지 프레임에 포함된 복수의 사람들 중 제1 사람에 대한 윤곽을 식별할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제1 영상으로부터 식별된 제1 사람의 윤곽에 기반하여, 복수의 이미지 프레임들 각각에서 제1 사람을 분할하는 마스크(들)을 획득할 수 있다. 예를 들어, 전자 장치는 제 1 이미지 프레임에 포함된 복수의 사람들 중 제1 사람을 분할하는 제1 마스크를 획득할 수 있다. 제1 이미지 프레임 내의 복수의 사람들 중 제2 사람에 대한 윤곽은 식별되지 않았으므로, 제1 마스크 중 제2 사람에 대응되는 영역은 구분되지 않을 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 객체 인식 모델을 이용하여 생성된 상기 제1 포인트 세트 및/또는 상기 제1 바운딩 박스 세트를 표시하도록 사용자 장치에 전달할 수 있다. 예를 들어, 사용자 장치 는 표시 장치를 통해 상기 제1 영상에 포함된 복수의 이미지 프레임들 별로 상기 제1 포인트 세트 및/또는 상기 제1 바운딩 박스 세트 각각에 대한 시각적 객체를 오버랩(overlap)하여 표시할 수 있다. 예를 들어, 사용 자 장치는 표시 장치를 통해 제1 이미지 프레임에 포함된 복수의 사람들 중 제1 사람에 대한 바운딩박스 세트를 나타내는 시각적 객체를 오버랩한 이미지 프레임을 표시할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 제1 이미지 프레임에 포함된 복 수의 사람들 중 제1 사람뿐만 아니라 제2 사람을 선택하려는 사용자 입력을 획득할 수 있다. 예를 들어, 전자 장치는 제2 사람에 대한 바운딩 박스를 추가하는 사용자 입력을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 입력에 기반하여 제1 이미지 프레임의 적어도 하나 의 객체의 수정 윤곽을 식별할 수 있다. 예를 들어, 전자 장치는 제2 사람을 추가하고자 하는 사용자 입력 에 기반하여 제1 사람의 윤곽에 더하여 제2 사람의 윤곽을 포함한 수정 윤곽을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 수정 윤곽에 기반하여 제1 이미지 프레임에서 제1 사람 및 제2 사람을 분할하는 수정 마스크를 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 수정 마스크를 이용해 제1 이미지 프레임의 복수의 사람들 중에서 제1 사람 및 제2 사람을 제외한 나머지 영역을 제거한 보정 이미지 프레임을 획득할 수 있다. 도 16a는 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 키-포인트에 대한 사용자 입력의 실행 화면을 나타낸다. 도 16b는 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 키-포인트에 대한 사용자 입력의 실 행 화면을 나타낸다. 도 16c는 다양한 실시 예들에 따라 키-포인트에 대한 사용자 입력이 획득됨에 따라 사용자 장치를 통해 출력되는 실행 화면을 나타낸다. 도 16a를 참조하여, 사용자 장치를 통해 제1 영상이 획득됨에 따라, 전자 장치는 도 12를 참조하여 설명된 제4 영역에 제1 영상의 복수의 이미지 프레임들에 대한 시각적 객체를 표시하고, 사용자 입력에 기반하여 선택된 이미지 프레임에 대한 시각적 객체를 제5 영역에 표시하도록 사용자 장치를 제어할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 업로드된 제1 영상과 관련하여 키-포인 트 선택 입력과 관련된 아이콘에 대한 사용자 입력이 획득되는 경우, 키-포인트를 그리는 것과 관련된 기 능 실행을 위한 아이콘들을 추가로 표시하도록 제어할 수 있다. 예를 들어, 이미지 프레임 내 적어도 하나의 객 체의 종류를 선택하기 위한(즉, 객체 인식도를 높이기 위한 레이블(label)을 선택하기 위한) 메뉴 아이콘 , 사용자 입력 방식을 선택하기 위한 아이콘으로 단일 이미지 프레임에 키-포인트를 입력하기 위한 아이 콘, 멀티 이미지 프레임에 키-포인트를 입력하기 위한 아이콘, 키-포인트를 프롬프트 입력 및 세그 멘테이션 입력으로 그리기 위한 아이콘이 배치될 수 있다. 다양한 실시 예들에 따르면, 상기 아이콘 이 선택되는 경우 도 2 내지 도 9를 참조하여 설명된, 객체 인식 모델을 이용한 포인트 세트와 사용자 입 력을 통한 포인트 세트를 모두 이용할 수 있다. 도 16b를 참조하면, 전자 장치는 사용자 장치를 통해 제1 이미지 프레임에 대한 키-포인트 입 력과 관련된 사용자 입력이 획득되는 경우, 사용자 장치를 통해 키-포인트 설정과 관련된 입력을 획득할 수 있 다. 예를 들어, 사용자는 제5 영역에 표시된 제1 이미지 프레임 내에서 적어도 하나의 객체의 키- 포인트를 입력하는 경우 포인트 클릭, 포인트 클릭 및 드래그-앤-드롭 형태로 키-포인트를 입력할 수 있다. 이 에 따라, 전자 장치는 새로운 포인트를 입력하여는 클릭이 입력된 부분의 (x,y)좌표 또는 기존 포인트를 이동하려는 클릭 입력 및 드래그 드롭 이벤트가 발생한 부분의 (x,y) 좌표를 통해 사용자가 선택한 키-포인트를 식별할 수 있다. 사용자 입력을 통해 식별된 키-포인트는 제5 영역에 표시된 제1 이미지 프레임에 오버랩된 형태로 키-포인트에 대한 시각적 객체로 표시될 수 있다. 다양한 실시 예들에 따르면, 키-포인트에 대한 사용자 입력이 획득됨에 따라, 사용자 장치는 제4 영역 에 객체 인식 모델을 이용하여 복수의 이미지 프레임들에 대한 적어도 하나의 객체 식별을 나타내는 시각 적 객체, 제1 이미지 프레임에 대한 키-포인트 사용자 입력에 따른 제1 객체 인식에 대한 시각적 객체, 제2 객체 인식에 대한 시각적 객체를 표시할 수 있다. 예를 들어, 전자 장치는 복수의 이미지 프레임들을 그룹핑하고, 사용자 입력에 대응되는 이미지 프레임이 포함된 제1 이미지 프레임 그룹의 제1 객체 인식에 대한 시각적 객체, 제2 객체 인식에 대한 시각적 객체를 표시하도록 제어할 수 있다. 한편 사용자 장치는 도 13b 및 도 13c를 참조하여 설명한 획득된 사용자 입력을 제1 이미지 프레임 그룹에 후속하는 이미지 프레임 그룹에도 적용하는 기능 실행과 관련된 아이콘을 표시할 수 있다. 도 16c를 참조하면, 상기 아이콘에 대한 사용자 입력이 획득됨에 따라 사용자 장치를 통해 표시되는 실행 화면이 도시된다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 이미지 프레임 내 적어도 하나의 객체 (예: 제2 객체)에 대하여 키-포인트를 선택하는 사용자 입력을 획득하고, 획득된 사용자 입력을 상기 제1 이미 지 프레임 그룹에 후속하는 이미지 프레임 그룹에도 적용하는 기능 실행과 관련된 아이콘에 대한 사용자 입력을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 아이콘에 대한 사용자 입력이 획득되는 경우, 제1 이 미지 프레임 그룹에 후속하는 이미지 프레임 그룹에도 적어도 하나의 객체에 대한 키-포인트 입력을 적용하고, 키-포인트 입력이 적용된 제2 객체 인식에 대한 시각적 객체를 표시할 수 있다. 한편, 제2 객체에 대한 키-포인트 입력만이 적용 확대되었으므로, 제1 이미지 프레임 그룹 내 제1 객체에 대한 키-포인트 입력에 따른 제1 객체 인식에 대한 시각적 객체는 그대로 유지될 수 있다. 도 17은 다양한 실시 예들에 따라 전자 장치가 사용자 입력을 적용하는 것을 설명하기 위한 도면이다. 도 17을 참조하면, 도 12를 참조하여 설명된 제4 영역에 표시되는 복수의 이미지 프레임들 및 복수의 이 미지 프레임들에 대한 객체 인식을 나타내는 시각적 객체가 도시된다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 획득되는 제1 영상 내 복수의 이미지 프레임들을 나타내는 시각적 객체를 표시할 수 있다. 또한, 전자 장치는 상기 복수의 이미지 프레임들에 대하여 도 3을 참조하여 설명된 객체 인식 모델을 이용하여 적어도 하나의 객체에 대한 제1 윤곽을 식별할 수 있다. 상기 객체 인식 모델을 이용하여 복수의 이미지 프레임들 각각에 대하여 획득된 제1 윤곽은 베이스 세그 먼트(Base segment)로 이해될 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 복수의 이미지 프레임들 중 특정 이미지 프레임(들)에 대하 여, 적어도 하나의 객체의 포인트(예: 관절, 스켈레톤 데이터 등)를 선택하는 사용자 입력을 획득할 수 있다. 예를 들어 전자 장치는 복수의 이미지 프레임들을 그룹핑한 이미지 그룹들 중 제1 이미지 그룹 내 이미지 프레임들에 제1 객체 및 제2 객체에 대하여 포인트를 선택하는 사용자 입력을 획득할 수 있다. 전자 장치 는 상기 사용자 입력에 기반하여 제1 이미지 그룹 (특정 객체에 대하여 포인트 입력이 추가된 경우 추가된 그룹) 내 이미지 프레임들의 적어도 하나의 객체 각각에 대한 윤곽을 식별할 수 있다. 예를 들어, 전자 장치 는 제1 이미지 그룹 내 이미지 프레임들의 제1 객체에 대한 제2 윤곽과 제1 이미지 그룹 및 제2 이미지 그 룹 내 이미지 프레임들의 제2 객체에 대한 제3 윤곽을 식별할 수 있다. 제1 이미지 그룹의 이미지 프레임들 각 각에 대하여 획득된 제2 윤곽은 제1 사용자 선택 세그먼트(Key Human1)로 이해될 수 있다. 제1 이미지 그 룹 및 제2 이미지 그룹의 이미지 프레임들 각각에 대하여 획득된 제3 윤곽은 제2 사용자 선택 세그먼트(key Human2)로 이해될 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 제1 윤곽, 상기 제2 윤곽 및/또는 상기 제3 윤곽에 기반하 여 제1 영상의 적어도 하나의 객체를 구분하는 최종 세그먼트를 획득할 수 있다. 다양한 실시 예들에 따 르면, 전자 장치는 상기 최종 세그먼트에 기반하여 제1 영상에서 복수의 이미지 프레임별로 적어도 하나의 객체를 구분한 마스크를 생성할 수 있다. 도 18은 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 마스크에 대한 사용자 입력 입력의 실행 화면을 나타낸다. 도 18을 참조하면, 사용자 장치를 통해 제1 영상이 획득됨에 따라, 전자 장치는 도 12를 참조하여 설 명된 제4 영역에 제1 영상의 복수의 이미지 프레임들에 대한 시각적 객체를 표시하고, 사용자 입력에 기 반하여 선택된 이미지 프레임에 대한 시각적 객체를 제5 영역에 표시하도록 사용자 장치를 제어할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 획득되는 제1 영상 내 복수의 이미지 프레임들을 나타내는 시각적 객체를 표시할 수 있다. 또한, 전자 장치는 상기 복수의 이미지 프레임들에 대하여 도 3을 참조하여 설명된 객체 인식 모델을 이용하여 적어도 하나의 객체에 대한 제1 윤곽을 식별할 수 있다. 상기 객체 인식 모델을 이용하여 복수의 이미지 프레임들 각각에 대하여 획득된 제1 윤곽은 베이스 세그먼트(Base segment)로 이해될 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 복수의 이미지 프레임들 중 특정 이미지 프레임(들)에 대하 여, 적어도 하나의 객체에 대한 마스크를 직접 입력하는 사용자 입력(이하, 마스크 입력으로 표현될 수 있음)을 획득할 수 있다. 예를 들어 전자 장치는 복수의 이미지 프레임들을 중 제1 이미지 프레임의 적어도 하나의 객체에 대한 마스크 입력을 획득할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 제1 이미지 프레임에 대한 마스 크 입력을 획득할 수 있다. 예를 들어, 마스크 입력은 제1 이미지 프레임 내의 적어도 하나의 객체의 영 역을 직접 선택하는 입력으로 획득될 수 있다. 전자 장치는 마스크 입력을 위한 사용자의 영역 선택 입력 을 통해 사용자가 입력한 적어도 하나의 객체의 영역을 식별할 수 있다. 다양한 실시 예들에 따르면, 마스크 입력이 획득됨에 따라, 사용자 장치는, 제4 영역에, 객체 인식 모델을 이용하여 복수의 이미지 프레임들에 대한 적어도 하나의 객체 식별을 나타내는 시각적 객체, 제1 이미지 프레임에 대한 마스크 입력에 따른 적어도 하나의 객체 인식에 대한 시각적 객체를 표시할 수 있다. 도 19a는 다양한 실시 예들에 따라 이미지 프레임 내 복수의 객체들 중 적어도 하나의 객체를 선택하는 사용자 입력의 실행 화면을 나타낸다. 도 19b는 다양한 실시 예들에 따라 이미지 프레임 내 복수의 객체들 중 적어도 하나의 객체를 선택하는 텍스트 기반 사용자 입력의 실행 화면을 나타낸다. 도 19c는 다양한 실시 예들에 따라 이미지 프레임 내 복수의 객체들 중 적어도 하나의 객체를 선택하는 텍스트 기반 사용자 입력의 실행 화면을 나 타낸다. 도 19a를 참조하면, 사용자 장치를 통해 제1 영상이 획득됨에 따라, 전자 장치는 도 12를 참조하여 설명된 제4 영역에 제1 영상의 복수의 이미지 프레임들에 대한 시각적 객체를 표시하고, 사용자 입력에 기반하여 선택된 이미지 프레임에 대한 시각적 객체를 제5 영역에 표시하도록 사용자 장치를 제어할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 업로드된 제1 영상과 관련하여 키-포인 트 선택 입력과 관련된 아이콘에 대한 사용자 입력이 획득되는 경우, 키-포인트를 그리는 것과 관련된 기 능 실행을 위한 아이콘들을 추가로 표시하도록 제어할 수 있다. 또한, 다양한 실시 예들에 따르면, 전자 장치 는 키-포인트 선택 입력과 관련된 아이콘에 대한 사용자 입력이 획득되는 경우, 이미지 프레임 내 복수의 객체들 중 특정 객체를 선택하기 위한 입력을 추가로 획득할 수 있다. 예를 들어, 전자 장치는 사용자 장치를 통해 제1 영상 내 복수의 객체들 중에 제1 객체를 선택 하는 입력을 획득할 수 있다. 다양한 실시 예들에 따르면 제1 객체를 선택하는 입력은 다양한 방법을 통 해 획득될 수 있다. 예를 들어, 상기 다양한 방법은 마우스 클릭, 터치 입력, 사운드 입력, 문자 입력, 키보드 입력 등을 포함할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제1 객체를 선택하는 사용자 입력이 획득됨에 따라, 상기 사용자 입력에 기반하여 제1 객체의 윤곽을 식별할 수 있다. 이에 따라, 사용자 장치는 제4 영역 에 객체 인식 모델을 이용한 복수의 이미지 프레임들에 대한 적어도 하나의 객체 식별을 나타내는 시각적 객체(Base Segment), 제1 객체에 대한 사용자 입력에 따른 제1 객체 인식에 대한 시각적 객체 (Point Human1)를 표시할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 키-포인트 선택 입력과 관련된 아이콘에 대한 사용자 입력 이 획득되는 경우, 이미지 프레임 내 복수의 객체들 중 특정 객체를 선택하기 위한 입력을 추가로 획득할 수 있 다. 예를 들어, 전자 장치는 제1 영상 내 객체를 선택하기 위한 입력으로 텍스트(Text) 입력이 선택된 경우, 사용자 장치를 통해 텍스트 입력을 획득하기 위한 텍스트 입력 아이콘을 표시할 수 있다. 이때, 사 용자는 텍스트 입력 아이콘이 표시됨에 따라 객체를 선택하기 위한 텍스트를 입력할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 사용자 장치를 통해 객체를 선택하기 위한 텍스트 입 력이 획득된 경우, 텍스트에 따른 사용자 의도를 식별하여 제1 영상 내 적어도 하나의 객체를 선택할 수 있다. 예를 들어, 전자 장치는 사용자 장치를 통해 획득된 텍스트 입력이 “모든 사람을 추가해줘”인 경우, 텍스트를 이해하고 제1 영상에 포함된 모든 사람 객체를 식별할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 상기 사용자 입력(예: “모든 사람 추가해줘”)에 기반하여 제1 영상에 포함된 모든 사람 객체(제1 사람 객체, 제2 사람 객체, 제3 사람 객체)의 윤곽을 식별할 수 있다. 이에 따라, 사용자 장치는 제4 영역에 객체 인식 모델을 이용한 복수의 이미지 프레임들에 대한 적 어도 하나의 객체 식별을 나타내는 시각적 객체(Base Segment), 사용자 입력에 따른 제1 사람 객체 인식 에 대한 시각적 객체(Point Human1)에 더하여 제2 사람 객체 인식에 대한 시각적 객체(Point Human2), 제3 사람 객체 인식에 대한 시각적 객체(Point Human3)를 표시할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는 제1 영상 내 적어도 하나의 객체에 대한 선택, 적어도 하나의 객 체의 윤곽 식별, 적어도 하나의 객체에 대한 보정을 바운딩 박스 입력, 포인트 입력뿐만 아니라 텍스트 입력, 사운드 입력 등 다양한 방식을 통해 입력할 수 있다. 전자 장치는 사용자 장치를 통해 획득된 상기 입력에 대한 사용자 의도를 이해하고, 사용자 의도에 적합한 기능을 제공할 수 있다. 상술한 바와 같이, 전자 장치는, 통신 장치, 영상에 포함된 적어도 하나의 객체에 대한 포인트(points) 및 바운 딩 박스(bounding box)를 생성하도록 학습된 객체 인식 모델을 저장하는 저장 장치 및 적어도 하나의 프로세서 를 포함하고, 상기 적어도 하나의 프로세서는,상기 통신 장치를 통해, 상기 전자 장치와 연결된 사용자 장치로 부터 복수의 이미지 프레임들을 포함하는 제1 영상을 획득하고, 상기 객체 인식 모델을 이용하여 상기 제1 영상 에 포함된 적어도 하나의 객체에 대한 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득하고, 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트에 기반하여 상기 적어도 하나의 객체의 제1 윤곽을 식별하고, 상기 제1 포 인트 세트 또는 상기 제1 바운딩 박스 세트 중 적어도 하나를 표시하도록 상기 사용자 장치에 전달하고, 상기 사용자 장치를 통해 상기 복수의 이미지 프레임들 중 제1 이미지 프레임에 포함된 상기 적어도 하나의 객체에 대한 제1 사용자 입력을 획득하고, 상기 제1 사용자 입력에 기반하여 상기 제1 이미지 프레임의 상기 적어도 하 나의 객체의 제2 윤곽을 식별하고, 상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 상기 제1 영상에 대한 상기 적 어도 하나의 객체를 분할하는 마스크(mask)를 획득하고, 상기 마스크를 이용해 상기 적어도 하나의 객체와 관련 하여 상기 제1 영상을 보정한 제2 영상을 획득하고, 및 상기 통신 장치를 통해, 상기 사용자 장치가 상기 제2 영상을 출력하도록 상기 제2 영상을 전달하도록 설정될 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 프로세서는, 상기 제1 영상 내에서 복수의 이미지 프레임들 중 상기 제1 이미지 프레임의 순서 정보를 획득하고, 및 상기 순서 정보 및 상기 제1 사용자 입력에 기반하여 상기 적어도 하나의 객체의 상기 제2 윤곽을 식별하도록 설정될 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 프로세서는, 상기 제1 영상에 포함된 상기 복수의 이미지 프레 임들을 기-설정된 기준 단위로 그룹핑(grouping)하여 이미지 프레임 그룹들을 생성하고, 및 상기 객체 인식 모 델을 이용하여 상기 이미지 프레임 그룹들 별로 상기 적어도 하나의 객체를 추적하여 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트를 획득하도록 설정될 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 프로세서는, 상기 객체 인식 모델을 이용하여 상기 제1 영상의 장면 전환을 식별하고, 및 상기 장면 전환을 기준으로 상기 복수의 이미지 프레임들을 그룹핑하여 상기 이미지 프레임 그룹들을 생성하도록 설정될 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 프로세서는, 상기 이미지 프레임 그룹들 중 상기 제1 이미지 프레임이 포함된 제1 그룹에 상기 제1 사용자 입력을 적용함에 기반하여, 상기 제1 그룹의 이미지 프레임들에 포함된 상기 적어도 하나의 객체의 상기 제2 윤곽을 식별하도록 설정될 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 프로세서는, 상기 제1 사용자 입력의 적용에 대한 확장을 나타 내는 제2 사용자 입력이 획득됨에 기반하여, 상기 이미지 프레임 그룹들 중 상기 제1 그룹에 후속하는 제2 그룹 에 상기 제1 사용자 입력을 적용하여 상기 제2 그룹의 이미지 프레임들에 포함된 상기 적어도 하나의 객체의 상 기 제2 윤곽을 식별하도록 설정될 수 있다. 다양한 실시 예들에 따르면, 상기 제1 사용자 입력은 상기 제1 영상의 복수의 객체들 중 상기 적어도 하나의 객 체를 선택하는 입력, 상기 제2 윤곽을 선택하는 것과 관련된 텍스트 입력, 상기 적어도 하나의 객체에 대한 제2 포인트 세트 입력, 상기 적어도 하나의 객체에 대한 제2 바운딩 박스 세트 입력 또는 상기 적어도 하나의 객체 에 대한 마스킹 입력 중 적어도 하나를 포함할 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 프로세서는, 상기 제1 사용자 입력에 상기 적어도 하나의 객체 에 대한 마스킹 입력이 포함된 경우, 상기 마스킹 입력을 우선적으로 적용하여 상기 마스크를 획득하도록 설정 될 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 프로세서는, 상기 제2 포인트 세트 입력 또는 상기 제2 바운딩 박스 세트 입력에 기반하여 상기 적어도 하나의 객체에 대한 보정 요청을 식별하고, 상기 보정 요청에 기반하여, 상기 제1 이미지 프레임에 포함된 상기 적어도 하나의 객체를 보정하고, 및 보정된 적어도 하나의 객 체의 상기 제2 윤곽을 식별하도록 설정될 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 프로세서는, 상기 보정된 적어도 하나의 객체를 포함하는 상기 제2 영상을 획득하도록 설정될 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 객체는 상기 제1 영상의 상기 복수의 객체들 중 선택된 객체이 고, 및 상기 제2 영상은 상기 제1 영상에서 상기 선택된 객체를 제외한 나머지 객체들 및 배경이 제거된 영상일 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 프로세서는, 상기 사용자 장치를 통해 상기 제2 영상의 평가에 대한 제3 사용자 입력을 획득하고, 및 상기 제3 사용자 입력에 기반하여 상기 객체 인식 모델을 업데이트 하도 록 설정될 수 있다. 다양한 실시 예들에 따르면, 상기 적어도 하나의 프로세서는, 상기 제1 사용자 입력을 상기 저장 장치에 저장하 고, 및 상기 사용자 장치를 통해 상기 사용자가 복수의 이미지 프레임들 중 제2 이미지 프레임에 대하여 상기 제1 사용자 입력을 자동으로 적용할 수 있도록 제공하도록 설정될 수 있다. 상술한 바와 같이, 전자 장치의 동작 방법은, 상기 전자 장치와 연결된 사용자 장치로부터 복수의 이미지 프레 임들을 포함하는 제1 영상을 획득하는 동작, 영상에 포함된 적어도 하나의 객체에 대한 포인트(points) 및 바운 딩 박스(bounding box)를 생성하도록 학습된 객체 인식 모델을 이용하여 상기 제1 영상에 포함된 적어도 하나의 객체에 대한 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득하는 동작, 상기 제1 포인트 세트 및 상기 제1 바 운딩 박스 세트에 기반하여 상기 적어도 하나의 객체의 제1 윤곽을 인식하는 동작, 상기 제1 포인트 세트 또는 상기 제1 바운딩 박스 세트 중 적어도 하나를 표시하도록 상기 사용자 장치에 전달하는 동작, 상기 사용자 장치 를 통해 상기 복수의 이미지 프레임들 중 제1 이미지 프레임에 포함된 상기 적어도 하나의 객체에 대한 제1 사 용자 입력을 획득하는 동작, 상기 제1 사용자 입력에 기반하여 상기 제1 이미지 프레임의 상기 적어도 하나의 객체의 제2 윤곽을 식별하는 동작, 상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 상기 제1 영상에 대한 상기 적 어도 하나의 객체를 분할하는 마스크(mask)를 획득하는 동작, 상기 마스크를 이용해 상기 적어도 하나의 객체와 관련하여 상기 제1 영상을 보정한 제2 영상을 획득하는 동작 및 상기 사용자 장치가 상기 제2 영상을 출력하도 록 상기 제2 영상을 전달하는 동작을 포함할 수 있다. 다양한 실시 예들에 따르면, 전자 장치의 프로세서에 의해 실행되어 상기 전자 장치가 동작을 수행하도록 하는 컴퓨터 명령을 저장하는 비일시적 컴퓨터 판독가능 기록 매체에 있어서, 상기 전자 장치와 연결된 사용자 장치 로부터 복수의 이미지 프레임들을 포함하는 제1 영상을 획득하는 동작, 영상에 포함된 적어도 하나의 객체에 대 한 포인트(points) 및 바운딩 박스(bounding box)를 생성하도록 학습된 객체 인식 모델을 이용하여 상기 제1 영 상에 포함된 적어도 하나의 객체에 대한 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득하는 동작, 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트에 기반하여 상기 적어도 하나의 객체의 제1 윤곽을 식별하는 동작, 상기 제1 포인트 세트 또는 상기 제1 바운딩 박스 세트 중 적어도 하나를 표시하도록 상기 사용자 장치에 전달 하는 동작, 상기 사용자 장치를 통해 상기 복수의 이미지 프레임들 중 제1 이미지 프레임에 포함된 상기 적어도 하나의 객체에 대한 제1 사용자 입력을 획득하는 동작, 상기 제1 사용자 입력에 기반하여 상기 제1 이미지 프레 임의 상기 적어도 하나의 객체의 제2 윤곽을 식별하는 동작, 상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 상기 제1 영상에 대한 상기 적어도 하나의 객체를 분할하는 마스크(mask)를 획득하는 동작, 상기 마스크를 이용해 상 기 적어도 하나의 객체와 관련하여 상기 제1 영상을 보정한 제2 영상을 획득하는 동작 및 상기 사용자 장치가 상기 제2 영상을 출력하도록 상기 제2 영상을 전달하는 동작을 포함할 수 있다. 다양한 실시 예들에 따르면, 전자 장치는, 적어도 하나의 카메라, 입력 장치,표시 장치, 통신 장치,영상에 포함 된 적어도 하나의 객체에 대한 포인트(points) 및 바운딩 박스(bounding box)를 생성하도록 학습된 객체 인식 모델을 저장하는 저장 장치 및 적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는, 상기 적어 도 하나의 카메라를 통해 복수의 이미지 프레임들을 포함하는 제1 영상을 획득하고, 상기 객체 인식 모델을 이용하여 상기 제1 영상에 포함된 적어도 하나의 객체에 대한 제1 포인트 세트 및 제1 바운딩 박스 세트를 획득하 고, 상기 제1 포인트 세트 및 상기 제1 바운딩 박스 세트에 기반하여 상기 적어도 하나의 객체의 제1 윤곽을 식 별하고, 상기 제1 포인트 세트 또는 상기 제1 바운딩 박스 세트 중 적어도 하나를 표시하도록 상기 표시 장치를 제어하고, 상기 입력 장치를 통해, 상기 복수의 이미지 프레임들 중 제1 이미지 프레임에 포함된 상기 적어도 하나의 객체에 대한 제1 사용자 입력을 획득하고, 상기 제1 사용자 입력에 기반하여 상기 제1 이미지 프레임의 상기 적어도 하나의 객체의 제2 윤곽을 식별하고, 상기 제1 윤곽 및 상기 제2 윤곽에 기반하여 상기 제1 영상에 대한 상기 적어도 하나의 객체를 분할하는 마스크(mask)를 획득하고, 상기 마스크를 이용해 상기 적어도 하나의 객체와 관련하여 상기 제1 영상을 보정한 제2 영상을 획득하고, 및 상기 표시 장치가 상기 제2 영상을 출력하도 록 제어하도록 설정될 수 있다. 본 개시에서, \"A 또는 B\", \"A 및 B 중 적어도 하나\", \"A 또는 B 중 적어도 하나\", \"A, B 또는 C\", \"A, B 및 C 중 적어도 하나\", 및 \"A, B, 또는 C 중 적어도 하나\"와 같은 문구들 각각은 그 문구들 중 해당하는 문구에 함께 나열된 항목들 중 어느 하나, 또는 그들의 모든 가능한 조합을 포함할 수 있다. \"제1\", \"제2\", 또는 \"첫째\" 또는 \"둘째\"와 같은 용어들은 단순히 해당 구성요소를 다른 해당 구성요소와 구분하 기 위해 사용될 수 있으며, 해당 구성요소들을 다른 측면(예: 중요성 또는 순서)에서 한정하지 않는다. 본 개시의 다양한 실시 예들에서 사용된 용어 \"모듈\"은 하드웨어, 소프트웨어 또는 펌웨어로 구현된 유닛을 포 함할 수 있다. 예를 들면, 로직, 논리 블록, 부품, 또는 회로와 같은 용어와 상호 호환적으로 사용될 수 있다. 모듈은, 일체로 구성된 부품 또는 하나 또는 그 이상의 기능을 수행하는, 상기 부품의 최소 단위 또는 그 일부 가 될 수 있다. 본 개시의 다양한 실시 예들은 장치(device)(예: 전자 장치)에 의해 읽을 수 있는 저장 장치(예: 내 장 메모리 또는 외장 메모리)에 저장된 하나 이상의 명령어들을 포함하는 소프트웨어(예: 프로그램)로서 구현될 수 있다. 저장 장치는 저장 매체로 표현될 수 있다. 일 실시 예에 따르면, 본 문서에 개시된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래 될 수 있다. 컴퓨터 프로그램 제품은 장치로 읽을 수 있는 저장 매체(예: CD-ROM(compact disc read only memory))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두 개의 사용자 장치들 간에 직접, 온라 인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 다양한 실시 예들에 따르면, 상기 기술한 구성요소들의 각각의 구성요소(예: 모듈 또는 프로그램)는 단수 또는 복수의 개체를 포함할 수 있으며, 복수의 개체 중 일부는 다른 구성요소에 분리 배치될 수도 있다. 다양한 실시 예들에 따르면, 전술한 해당 구성요소들 중 하나 이상의 구성요소들 또는 동작들이 생략되거나, 또는 하나 이상 의 다른 구성요소들 또는 동작들이 추가될 수 있다. 추가적으로 또는 대체적으로, 복수의 구성요소들(예: 모듈 또는 프로그램)은 하나의 구성요소로 통합될 수 있다. 이런 경우, 통합된 구성요소는 상기 복수의 구성요소들 각각의 구성요소의 하나 이상의 기능들을 상기 통합 이전에 상기 복수의 구성요소들 중 해당 구성요소에 의해 수행되는 것과 동일 또는 유사하게 수행할 수 있다. 다양한 실시 예들에 따르면, 모듈, 프로그램 또는 다른 구성요소에 의해 수행되는 동작들은 순차적으로, 병렬적 으로, 반복적으로, 또는 휴리스틱하게 실행되거나, 상기 동작들 중 하나 이상이 다른 순서로 실행되거나, 생략 되거나, 또는 하나 이상의 다른 동작들이 추가될 수 있다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13a 도면13b 도면13c 도면14 도면15 도면16a 도면16b 도면16c 도면17 도면18 도면19a 도면19b 도면19c"}
{"patent_id": "10-2024-0085070", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 다양한 실시 예들에 따라 동영상 편집 기능을 제공하는 시스템을 나타내는 도면이다. 도 2는 다양한 실시 예들에 따른 전자 장치의 블록도이다. 도 3은 다양한 실시 예들에 따른 전자 장치에서 동영상 편집과 관련된 기능을 제어하는 개념을 나타낸다. 도 4는 다양한 실시 예들에 따라 전자 장치가 적어도 하나의 객체를 제외한 영역이 제거된 영상을 생성하는 동 작을 나타내는 흐름도이다. 도 5는 다양한 실시 예들에 따라 전자 장치가 사용자 장치로부터 획득된 영상 내 복수의 이미지 프레임들을 그 룹핑하여 적어도 하나의 객체에 대한 윤곽을 획득하는 동작을 나타내는 흐름도이다. 도 6은 다양한 실시 예들에 따라 전자 장치가 생성한 마스크를 설명하기 위한 도면이다. 도 7은 다양한 실시 예들에 따라 전자 장치가 생성한 마스크에 기반하여 배경이 제거된 영상을 설명하기 위한 도면이다. 도 8은 다양한 실시 예들에 따라 전자 장치가 객체 인식 모델을 이용하여 생성된 적어도 하나의 객체에 대한 윤 곽에 대하여 사용자 입력을 획득하는 동작을 나타내는 흐름도이다. 도 9는 다양한 실시 예들에 따라 전자 장치가 사용자 입력에 기반하여 적어도 하나의 객체와 관련해 보정된 영 상을 생성하는 동작을 나타내는 흐름도이다. 도 10은 다양한 실시 예들에 따라 전자 장치가 사용자 입력에 기반하여 생성한 마스크를 통해 보정한 영상을 설 명하기 위한 도면이다. 도 11은 다양한 실시 예들에 따라 전자 장치가 사용자 입력에 기반하여 생성한 마스크를 통해 보정한 영상을 설 명하기 위한 도면이다. 도 12는 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 실행 화면을 나타낸다. 도 13a는 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 바운딩 박스에 대한 사용자 입력의 실행 화면 을 나타낸다. 도 13b는 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 바운딩 박스에 대한 사용자 입력의 실행 화면 을 나타낸다. 도 13c는 다양한 실시 예들에 따라 바운딩 박스에 대한 사용자 입력이 획득됨에 따라 사용자 장치를 통해 출력 되는 실행 화면을 나타낸다. 도 14는 다양한 실시 예들에 따라 전자 장치가 사용자 입력을 적용하는 것을 설명하기 위한 도면이다. 도 15는 다양한 실시 예들에 따라 바운딩 박스에 대한 사용자 입력이 객체 추가 입력인 경우 전자 장치가 마스크를 생성하는 것을 설명하기 위한 도면이다. 도 16a는 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 키-포인트에 대한 사용자 입력의 실행 화면을 나타낸다. 도 16b는 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 키-포인트에 대한 사용자 입력의 실행 화면을 나타낸다. 도 16c는 다양한 실시 예들에 따라 키-포인트에 대한 사용자 입력이 획득됨에 따라 사용자 장치를 통해 출력되 는 실행 화면을 나타낸다. 도 17은 다양한 실시 예들에 따라 전자 장치가 사용자 입력을 적용하는 것을 설명하기 위한 도면이다. 도 18은 다양한 실시 예들에 따라 사용자 장치를 통해 출력되는 마스크에 대한 사용자 입력 입력의 실행 화면을 나타낸다. 도 19a는 다양한 실시 예들에 따라 이미지 프레임 내 복수의 객체들 중 적어도 하나의 객체를 선택하는 사용자 입력의 실행 화면을 나타낸다. 도 19b는 다양한 실시 예들에 따라 이미지 프레임 내 복수의 객체들 중 적어도 하나의 객체를 선택하는 텍스트 기반 사용자 입력의 실행 화면을 나타낸다. 도 19c는 다양한 실시 예들에 따라 이미지 프레임 내 복수의 객체들 중 적어도 하나의 객체를 선택하는 텍스트 기반 사용자 입력의 실행 화면을 나타낸다. 도면의 설명과 관련하여, 동일 또는 유사한 구성요소에 대해서는 동일 또는 유사한 참조 부호가 사용될 수 있다."}
