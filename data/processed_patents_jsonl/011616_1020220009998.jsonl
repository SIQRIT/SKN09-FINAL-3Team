{"patent_id": "10-2022-0009998", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0103782", "출원번호": "10-2022-0009998", "발명의 명칭": "사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법 및 장치", "출원인": "국민대학교산학협력단", "발명자": "김남규"}}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "텍스트의 문맥 정보를 사전학습하여 구축된 언어모델로부터 임베딩 행렬(embedding matrix)를 추출하는 임베딩행렬 추출단계;상기 임베딩 행렬이 적용된 추상 요약 모델을 정의하고 학습 데이터셋을 기초로 텍스트 요약문 생성을 위한 요약 학습을 수행하여 상기 추상 요약 모델을 구축하는 추상 요약 모델 구축단계; 및상기 추상 요약 모델을 이용하여 입력 텍스트에 대한 요약문을 생성하는 텍스트 요약문 생성단계;를 포함하는사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 임베딩 행렬 추출단계는사전학습된 코버트(KoBERT) 모델의 입력 계층(layer)에서 토큰 임베딩 행렬을 상기 임베딩 행렬로서 추출하는단계를 포함하는 것을 특징으로 하는 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 임베딩 행렬 추출단계는상기 입력 계층의 임베딩 계층들에서 적어도 토큰 임베딩 행렬을 포함하는 임베딩 조합을 통해 상기 임베딩 행렬을 생성하는 단계를 포함하는 것을 특징으로 하는 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약방법."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 추상 요약 모델 구축단계는트랜스포머(transformer) 모델을 상기 추상 요약 모델로서 정의하고 상기 트랜스포머 모델의 임베딩 계층에 상기 임베딩 행렬을 적용하는 단계를 포함하는 것을 특징으로 하는 사전학습 언어모델을 활용한 트랜스포머 기반텍스트 요약 방법."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 추상 요약 모델 구축단계는상기 트랜스포머 모델의 인코더 임베딩 행렬과 디코더(decoder) 임베딩 행렬을 각각 상기 임베딩 행렬로 대체하는 단계를 포함하는 것을 특징으로 하는 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서, 상기 추상 요약 모델 구축단계는상기 학습 데이터셋의 원문 텍스트를 상기 트랜스포머 모델의 인코더(encoder)에 입력하는 단계;상기 인코더의 임베딩 계층을 통해 상기 원문 텍스트에 대한 단어 벡터를 생성하는 단계;공개특허 10-2023-0103782-3-상기 단어 벡터를 상기 인코더의 인코더 계층에 통과시켜 상기 디코더에 전달하는 단계;상기 디코더를 통해 상기 원문 텍스트에 대응되는 요약 텍스트를 생성하는 단계; 및상기 원문 텍스트의 정답 텍스트와 상기 요약 텍스트 간의 차이를 비교하여 상기 인코더 및 상기 디코더를 갱신하는 단계를 포함하는 것을 특징으로 하는 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 텍스트 요약문 생성단계는상기 입력 텍스트를 복수의 부분 텍스트들로 분할하는 단계;상기 복수의 부분 텍스트들 각각을 상기 추상 요약 모델에 입력하여 복수의 부분 요약문들을 생성하는 단계; 및상기 복수의 부분 요약문들을 통합하여 상기 요약문을 생성하는 단계를 포함하는 것을 특징으로 하는 사전학습언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 요약문의 품질을 평가하는 요약문 품질 평가단계;를 더 포함하는 것을 특징으로 하는 사전학습 언어모델을활용한 트랜스포머 기반 텍스트 요약 방법."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 요약문 품질 평가단계는상기 요약문과 사용자에 의해 입력된 정답문 사이의 단어 일치 정도에 따라 적어도 하나의 평가 지표를 산출하는 단계를 포함하는 것을 특징으로 하는 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "텍스트의 문맥 정보를 사전학습하여 구축된 언어모델로부터 임베딩 행렬(embedding matrix)를 추출하는 임베딩행렬 추출부;상기 임베딩 행렬이 적용된 추상 요약 모델을 정의하고 학습 데이터셋을 기초로 텍스트 요약문 생성을 위한 요약 학습을 수행하여 상기 추상 요약 모델을 구축하는 추상 요약 모델 구축부; 및상기 추상 요약 모델을 이용하여 입력 텍스트에 대한 요약문을 생성하는 텍스트 요약문 생성부;를 포함하는 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 장치."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 임베딩 행렬 추출부는사전학습된 코버트(KoBERT) 모델의 입력 계층(layer)에서 토큰 임베딩 행렬을 상기 임베딩 행렬로서 추출하는것을 특징으로 하는 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 장치."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서, 상기 추상 요약 모델 구축부는공개특허 10-2023-0103782-4-트랜스포머(transformer) 모델을 상기 추상 요약 모델로서 정의하고 상기 트랜스포머 모델의 임베딩 계층에 상기 임베딩 행렬을 적용하는 것을 특징으로 하는 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약장치."}
{"patent_id": "10-2022-0009998", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 추상 요약 모델 구축부는상기 트랜스포머 모델의 인코더 임베딩 행렬과 디코더(decoder) 임베딩 행렬을 각각 상기 임베딩 행렬로 대체하는 것을 특징으로 하는 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 장치."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "방법 및 장치 요 약"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "요약", "paragraph": 2, "content": "본 발명은 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법 및 장치에 관한 것으로, 상기 방법은 텍스트의 문맥 정보를 사전학습하여 구축된 언어모델로부터 임베딩 행렬(embedding matrix)를 추출하는 임베딩"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "요약", "paragraph": 3, "content": "행렬 추출단계; 상기 임베딩 행렬이 적용된 추상 요약 모델을 정의하고 학습 데이터셋을 기초로 텍스트 요약문"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "요약", "paragraph": 4, "content": "생성을 위한 요약 학습을 수행하여 상기 추상 요약 모델을 구축하는 추상 요약 모델 구축단계; 및 상기 추상 요"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "요약", "paragraph": 5, "content": "약 모델을 이용하여 입력 텍스트에 대한 요약문을 생성하는 텍스트 요약문 생성단계;를 포함한다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 텍스트 요약 기술에 관한 것으로, 보다 상세하게는 소량의 학습 데이터가 주어진 상황에서도 양질의"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "요약문을 생성하기 위해 사전학습 언어 모델의 일부 요소를 추출하여 트랜스포머 기반의 추상 요약 모델에 적용"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 3, "content": "함으로써 요약 품질을 높일 수 있는 기술에 관한 것이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "텍스트는 다양한 정보를 쉽게 표현하고 전달할 수 있는 대표적인 수단으로 사용되어 왔으며, 최근 정보 전달 기 기가 대중화됨에 따라 더욱 많은 양의 텍스트 데이터가 생산 및 유통되고 있다. 특히, 소셜 미디어(Social Media)의 등장으로 정보 생산자는 정보 이용자에게 손쉽게 정보를 전달할 수 있게 되었고, 이는 오늘날 방대한 양의 텍스트 데이터가 온라인상에서 활발하게 유통되는 계기가 되었다. 하지만, 이로 인해 정보 이용자가 방대 한 정보로부터 필요한 정보만을 추려내는 작업은 더욱 어렵게 되었으며, 이는 정보의 손실을 최소화하면서 방대"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "한 텍스트 정보를 자동으로 적은 양으로 축약하려는 시도, 즉 자동 텍스트 요약(Automatic Text Summarization) 연구가 등장하는 배경이 되었다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "자동 텍스트 요약 연구는 요약을 생성하는 방식에 따라 크게 추출 요약(Extractive Summarization) 접근과 추상"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 4, "content": "요약(Abstractive Summarization) 접근으로 구분될 수 있다. 추출 요약은 원문(Source Text)을 이루고 있는 문"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 5, "content": "장 중 어떤 문장이 요약문으로서 의미가 있는지를 점수화(Scoring)하고, 이를 기반으로 주요 문장을 발췌하는"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 6, "content": "요약 방법이다. 이렇게 생성된 요약문은 원문으로부터 문장 단위로 발췌되었기 때문에, 문법적 오류가 적고 문 장 내 표현이 자연스럽다는 장점이 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 7, "content": "따라서, 빠르게 서비스화에 성공했으며, 대표적인 예로 네이버 뉴스의 요약봇 서비스, 다음 뉴스의 자동 요약"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 8, "content": "서비스, 연합뉴스의 인공지능 기사요약 서비스 등이 있다. 하지만, 해당 방식은 원문 내에 존재하는 문장만 요 약문에 포함될 수 있기 때문에 원본 문서에 없는 표현은 사용이 불가능하고, 문장간 연결이 어색할 수 있다는 단점을 가질 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 9, "content": "이와 달리, 추상 요약은 요약문을 생성하기 위해 원문에 존재하는 텍스트를 사용하되, 요약문 작성 과정에서 어 휘를 재구성 및 변환하여 원문에 존재하지 않는 새로운 텍스트 시퀀스(Sequence)를 생성할 수 있다. 따라서, 추"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 10, "content": "상 요약의 요약문은 새로운 표현을 풍부하게 사용할 수 있으며, 이로 인해 마치 사람이 요약한 것과 같은 요약"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 11, "content": "문을 생성할 수 있다. 이러한 장점에도 불구하고 전통적인 추상 요약 연구는 문법적으로 오류가 많거나 문맥이 자연스럽지 않은 텍스트 시퀀스를 생성하는 경우가 많다는 한계로 인해 현업에서 쉽게 적용되지 못했다. 선행기술문헌 특허문헌(특허문헌 0001) 한국등록특허 제10-0669534호 (2007.01.16)"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 실시예는 소량의 학습 데이터가 주어진 상황에서도 양질의 요약문을 생성하기 위해 사전학습 언어"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "모델의 일부 요소를 추출하여 트랜스포머 기반의 추상 요약 모델에 적용함으로써 요약 품질을 높일 수 있는 사"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 3, "content": "전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법 및 장치를 제공하고자 한다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "실시예들 중에서, 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법은 텍스트의 문맥 정보를 사전 학습하여 구축된 언어모델로부터 임베딩 행렬(embedding matrix)를 추출하는 임베딩 행렬 추출단계; 상기 임베"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "딩 행렬이 적용된 추상 요약 모델을 정의하고 학습 데이터셋을 기초로 텍스트 요약문 생성을 위한 요약 학습을"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "수행하여 상기 추상 요약 모델을 구축하는 추상 요약 모델 구축단계; 및 상기 추상 요약 모델을 이용하여 입력"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 4, "content": "텍스트에 대한 요약문을 생성하는 텍스트 요약문 생성단계;를 포함한다. 상기 임베딩 행렬 추출단계는 사전학습된 코버트(KoBERT) 모델의 입력 계층(layer)에서 토큰 임베딩 행렬을 상 기 임베딩 행렬로서 추출하는 단계를 포함할 수 있다. 상기 임베딩 행렬 추출단계는 상기 입력 계층의 임베딩 계층들에서 적어도 토큰 임베딩 행렬을 포함하는 임베딩 조합을 통해 상기 임베딩 행렬을 생성하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 5, "content": "상기 추상 요약 모델 구축단계는 트랜스포머(transformer) 모델을 상기 추상 요약 모델로서 정의하고 상기 트랜 스포머 모델의 임베딩 계층에 상기 임베딩 행렬을 적용하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 6, "content": "상기 추상 요약 모델 구축단계는 상기 트랜스포머 모델의 인코더 임베딩 행렬과 디코더(decoder) 임베딩 행렬을 각각 상기 임베딩 행렬로 대체하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 7, "content": "상기 추상 요약 모델 구축단계는 상기 학습 데이터셋의 원문 텍스트를 상기 트랜스포머 모델의 인코더(encode r)에 입력하는 단계; 상기 인코더의 임베딩 계층을 통해 상기 원문 텍스트에 대한 단어 벡터를 생성하는 단계; 상기 단어 벡터를 상기 인코더의 인코더 계층에 통과시켜 상기 디코더에 전달하는 단계; 상기 디코더를 통해 상"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 8, "content": "기 원문 텍스트에 대응되는 요약 텍스트를 생성하는 단계; 및 상기 원문 텍스트의 정답 텍스트와 상기 요약 텍 스트 간의 차이를 비교하여 상기 인코더 및 상기 디코더를 갱신하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 9, "content": "상기 텍스트 요약문 생성단계는 상기 입력 텍스트를 복수의 부분 텍스트들로 분할하는 단계; 상기 복수의 부분"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 10, "content": "텍스트들 각각을 상기 추상 요약 모델에 입력하여 복수의 부분 요약문들을 생성하는 단계; 및 상기 복수의 부분"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 11, "content": "요약문들을 통합하여 상기 요약문을 생성하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 12, "content": "상기 방법은 상기 요약문의 품질을 평가하는 요약문 품질 평가단계;를 더 포함할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 13, "content": "상기 요약문 품질 평가단계는 상기 요약문과 사용자에 의해 입력된 정답문 사이의 단어 일치 정도에 따라 적어 도 하나의 평가 지표를 산출하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 14, "content": "실시예들 중에서, 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 장치는 텍스트의 문맥 정보를 사전 학습하여 구축된 언어모델로부터 임베딩 행렬(embedding matrix)를 추출하는 임베딩 행렬 추출부; 상기 임베딩"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 15, "content": "행렬이 적용된 추상 요약 모델을 정의하고 학습 데이터셋을 기초로 텍스트 요약문 생성을 위한 요약 학습을 수"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 16, "content": "행하여 상기 추상 요약 모델을 구축하는 추상 요약 모델 구축부; 및 상기 추상 요약 모델을 이용하여 입력 텍스"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 17, "content": "트에 대한 요약문을 생성하는 텍스트 요약문 생성부;를 포함한다. 상기 임베딩 행렬 추출부는 사전학습된 코버트(KoBERT) 모델의 입력 계층(layer)에서 토큰 임베딩 행렬을 상기 임베딩 행렬로서 추출할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 18, "content": "상기 추상 요약 모델 구축부는 트랜스포머(transformer) 모델을 상기 추상 요약 모델로서 정의하고 상기 트랜스"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 19, "content": "포머 모델의 임베딩 계층에 상기 임베딩 행렬을 적용할 수 있다.상기 추상 요약 모델 구축부는 상기 트랜스포머 모델의 인코더 임베딩 행렬과 디코더(decoder) 임베딩 행렬을 각각 상기 임베딩 행렬로 대체할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "개시된 기술은 다음의 효과를 가질 수 있다. 다만, 특정 실시예가 다음의 효과를 전부 포함하여야 한다거나 다 음의 효과만을 포함하여야 한다는 의미는 아니므로, 개시된 기술의 권리범위는 이에 의하여 제한되는 것으로 이 해되어서는 아니 될 것이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 일 실시예에 따른 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법 및 장치는 소량의"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "학습 데이터가 주어진 상황에서도 양질의 요약문을 생성하기 위해 사전학습 언어 모델의 일부 요소를 추출하여"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 4, "content": "트랜스포머 기반의 추상 요약 모델에 적용함으로써 요약 품질을 높일 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명에 관한 설명은 구조적 내지 기능적 설명을 위한 실시예에 불과하므로, 본 발명의 권리범위는 본문에 설 명된 실시예에 의하여 제한되는 것으로 해석되어서는 아니 된다. 즉, 실시예는 다양한 변경이 가능하고 여러 가 지 형태를 가질 수 있으므로 본 발명의 권리범위는 기술적 사상을 실현할 수 있는 균등물들을 포함하는 것으로 이해되어야 한다. 또한, 본 발명에서 제시된 목적 또는 효과는 특정 실시예가 이를 전부 포함하여야 한다거나 그러한 효과만을 포함하여야 한다는 의미는 아니므로, 본 발명의 권리범위는 이에 의하여 제한되는 것으로 이해 되어서는 아니 될 것이다. 한편, 본 출원에서 서술되는 용어의 의미는 다음과 같이 이해되어야 할 것이다. \"제1\", \"제2\" 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하기 위한 것으로, 이들 용어들에 의해 권리범위가 한정되어서는 아니 된다. 예를 들어, 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\"있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결될 수 도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\"있다고 언급된 때에는 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 한편, 구성요소들 간의 관계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃 하는\"과 \"~에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한 복수의 표현을 포함하는 것으로 이해되어야 하고, \"포함 하다\"또는 \"가지다\" 등의 용어는 실시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이존재함을 지정하려는 것이며, 하나 또는 그 이상의 다른 특징이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이 들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 각 단계들에 있어 식별부호(예를 들어, a, b, c 등)는 설명의 편의를 위하여 사용되는 것으로 식별부호는 각 단 계들의 순서를 설명하는 것이 아니며, 각 단계들은 문맥상 명백하게 특정 순서를 기재하지 않는 이상 명기된 순 서와 다르게 일어날 수 있다. 즉, 각 단계들은 명기된 순서와 동일하게 일어날 수도 있고 실질적으로 동시에 수 행될 수도 있으며 반대의 순서대로 수행될 수도 있다. 본 발명은 컴퓨터가 읽을 수 있는 기록매체에 컴퓨터가 읽을 수 있는 코드로서 구현될 수 있고, 컴퓨터가 읽을 수 있는 기록 매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록 장치를 포함 한다. 컴퓨터가 읽을 수 있는 기록 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있다. 또한, 컴퓨터가 읽을 수 있는 기록 매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산 방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 여기서 사용되는 모든 용어들은 다르게 정의되지 않는 한, 본 발명이 속하는 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적으로 사용되는 사전에 정의되어 있는 용어들은 관 련 기술의 문맥상 가지는 의미와 일치하는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한 이 상적이거나 과도하게 형식적인 의미를 지니는 것으로 해석될 수 없다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "자동 텍스트 요약은 내용의 주요 정보와 전반적인 의미를 보존하면서 간결하고 자연스러운 요약문을 생성하는"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "과제이며, 요약을 생성하는 방식에 따라 크게 추출 요약 접근과 추상 요약 접근으로 구분될 수 있다. 추출 요약"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "(Extractive Summarization)은 원문과 가장 관련 있는 문장들을 선택 또는 발췌하고 이를 조합하여 요약문을 생"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "성하는 방법에 해당할 수 있다. 추출 요약은 원문을 이루고 있는 문장들 중 요약문에 포함될 주요 문장을 발췌 하는 것이 핵심일 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "한편, 추상 요약(Abstractive Summarization)은 언어학적 정보를 이용하여 원문을 해석하고 이를 토대로 정보를"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "간결하게 압축하여 요약문을 생성하는 방법으로, 전통적인 추상 요약은 트리 기반(Tree-based) 방법론, 규칙 기"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "반(Rule-based) 방법론 등과 같은 구조화된 언어학적 정보를 이용하여 요약문을 생성할 수 있다. 이러한 특징으"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "로 인해 추상 요약으로 생성된 요약문은 새로운 어휘를 포함하거나 새로운 시퀀스를 생성할 수 있어, 풍부한 표"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "현의 요약문을 생성할 수 있다는 장점이 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "하지만, 전통적인 추상 요약은 모든 언어학적 관계를 명확히 구조화하기 어려워, 전통적인 방법론으로 생성한"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "요약문은 문법적 오류를 포함하거나 요약문의 표현이 부자연스럽다는 한계를 가질 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "이러한 추상 요약의 한계는 신경망 기반의 방법론을 적용하면서 크게 개선되었는데, 특히 기계 번역(Machine"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "Translation) 과제에서 큰 성과를 이룬 Seq2Seq 모델과 어텐션 메커니즘 모델의 구조를 추상 요약에 적용하여"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "양질의 요약문을 생성할 수 있다. 순환 신경망은 주어진 텍스트 사이의 관계를 시퀀스 정보를 활용하여 효과적으로 파악할 수 있다는 점에서 다양 한 자연어 처리(Natural Language Processing; NLP) 과제에 활용되고 있다. 하지만, 순환 신경망은 텍스트 시 퀀스가 길어짐에 따라 텍스트 사이의 문맥 정보를 제대로 학습하지 못하는 장기 의존성 문제를 가질 수 있다. 또한, 순환 신경망 기반의 인코더-디코더 구조는 인코더에서 입력 시퀀스를 하나의 벡터 표현으로 압축하는 과 정에서 정보 손실이 발생한다는 구조적인 한계를 가지므로 장기 의존성 문제는 여전히 근본적으로 해결되지 못 했다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "트랜스포머 모델이 기계 번역 및 추상 요약과 같은 자연어 생성(Natural Language Generation) 과제에서 우수한 성능을 보임에 따라, 관련 분야의 연구자들은 트랜스포머의 자연어 이해(Understanding) 능력에 집중하게 되었 다. 트랜스포머 구조를 활용하여 많은 양의 텍스트 데이터로부터 언어학적 문맥 정보를 학습한 BERT, GPT, GPT- 2는 자연어 처리 분야에 존재하는 여러 벤치마크 데이터셋에 대하여 높은 수준의 성능을 달성하면서, 트랜스포 머를 기반으로 한 사전학습 언어 모델의 우수성을 보였다. 특히, BERT는 트랜스포머의 인코더를 활용하여 텍스트 데이터로부터 효과적으로 문맥 정보를 학습하도록 고안된 MLM(Masked Language Model)과 NSP(Next Sentence Prediction)를 학습함으로써, 입력된 텍스트 시퀀스 사이의 복잡한 문맥 정보를 잘 이해된 표현(Representation)으로 생성할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "추상 요약 과제에 대한 전통적인 트랜스포머와 트랜스포머 기반의 사전학습 언어 모델의 적용에도 불구하고, 트 랜스포머 기반의 요약 모델은 매우 많은 수의 매개변수 갱신을 위해 상당히 많은 양의 데이터와 학습 시간 및"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "자원을 필요로 한다는 한계를 가질 수 있다. 이러한 한계는 요약문 학습에 활용할 수 있는 데이터가 상대적으로"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "적은 양의 한국어 문서에 대해, 추상 요약 과제에 트랜스포머 모델을 적용할 때 더욱 큰 부작용으로 나타날 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 20, "content": "도 1은 본 발명에 따른 텍스트 요약 시스템을 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 21, "content": "도 1을 참조하면, 텍스트 요약 시스템은 사용자 단말, 텍스트 요약 장치 및 데이터베이스 를 포함할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 22, "content": "사용자 단말은 텍스트 요약 장치와 연결되어 요약문 생성에 필요한 텍스트를 직접 입력하거나 요약"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 23, "content": "모델에 의해 생성된 요약문을 수신하여 확인할 수 있는 컴퓨팅 장치에 해당할 수 있다. 사용자 단말은 스 마트폰, 노트북 또는 컴퓨터로 구현될 수 있으며, 반드시 이에 한정되지 않고, 태블릿 PC 등 다양한 디바이스로"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 24, "content": "도 구현될 수 있다. 사용자 단말은 텍스트 요약 장치와 네트워크를 통해 연결될 수 있고, 복수의 사"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 25, "content": "용자 단말들이 텍스트 요약 장치와 동시에 연결될 수도 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 26, "content": "텍스트 요약 장치는 본 발명에 따른 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법을 수"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 27, "content": "행하는 컴퓨터 또는 프로그램에 해당하는 서버로 구현될 수 있다. 텍스트 요약 장치는 사용자 단말과 유선 또는 무선 네트워크를 통해 연결될 수 있고 상호 간에 데이터를 주고받을 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 28, "content": "일 실시예에서, 텍스트 요약 장치는 본 발명에 따른 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 29, "content": "요약 방법을 수행하는 과정에서 다양한 외부 시스템(또는 서버)과 연동하여 동작할 수 있다. 이를 통해, 텍스트"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 30, "content": "요약 장치는 SNS 서비스, 포털 사이트, 블로그 등을 통해 텍스트로 이루어진 다양한 문서들에 접근할 수 있으며, 사전 학습을 위한 학습 데이터의 수집과 학습 모델 구축 등에 필요한 데이터를 수집할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 31, "content": "여기에서, 텍스트 요약 장치는 사용자의 요청에 대한 응답으로서 특정 텍스트에 대한 요약문을 생성하여"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 32, "content": "제공하는 동작을 수행할 수 있으며, 요약의 대상이 되는 텍스트는 복수의 단어, 구문 또는 문장을 포함하여 구 성될 수 있다. 예를 들어, 텍스트에는 인터넷 검색을 통해 접근 가능한 웹페이지, 해시 태그를 통해 공유되는 SNS 메시지와 블로그, 키워드 검색의 대상이 되는 게시글, 문서 파일 등을 포함할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 33, "content": "데이터베이스는 텍스트 요약 장치의 동작 과정에서 필요한 다양한 정보들을 저장하는 저장장치에 해 당할 수 있다. 예를 들어, 데이터베이스는 학습 및 평가를 위한 학습 데이터셋에 관한 정보를 저장할 수"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 34, "content": "있고, 텍스트 요약 모델 구축을 위한 학습 알고리즘 및 모델 정보를 저장할 수 있으며, 반드시 이에 한정되지"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 35, "content": "않고, 텍스트 요약 장치가 본 발명에 따른 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법 을 수행하는 과정에서 다양한 형태로 수집 또는 가공된 정보들을 저장할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 36, "content": "도 2는 도 1의 텍스트 요약 장치의 기능적 구성을 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 37, "content": "도 2를 참조하면, 텍스트 요약 장치는 임베딩 행렬 추출부, 추상 요약 모델 구축부, 텍스트 요"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 38, "content": "약문 생성부, 요약문 품질 평가부 및 제어부를 포함할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 39, "content": "다만, 본 발명의 실시예에 따른 텍스트 요약 장치가 상기의 기능적 구성들을 동시에 모두 포함해야 하는 것은 아니며, 각각의 실시예에 따라 상기의 구성들 중 일부를 생략하거나, 상기의 구성들 중 일부 또는 전부를"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 40, "content": "선택적으로 포함하여 구현될 수도 있다. 또한, 텍스트 요약 장치는 필요에 따라 상기의 구성들 중 일부를 선택적으로 포함하는 독립된 장치들로 구현될 수 있으며, 각 장치들 간의 연동을 통해 본 발명에 따른 텍스트"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 41, "content": "요약 방법을 수행할 수 있다. 이하, 각 구성들의 동작을 구체적으로 설명한다. 임베딩 행렬 추출부는 텍스트의 문맥 정보를 사전학습하여 구축된 언어모델로부터 임베딩 행렬(embedding matrix)을 추출할 수 있다. 즉, 임베딩 행렬 추출부는 학습 데이터가 충분하지 않은 조건에서 이미 구축된 사전학습 정보를 활용하기 위하여 사전학습된 언어모델의 임베딩 행렬을 선택적으로 수집할 수 있다. 여기에서, 임베딩 행렬은 임베딩 벡터(embedding vector)들의 집합에 해당할 수 있으며, 언어모델의 임베딩 계층에서 구축 되고 적용될 수 있다. 일 실시예에서, 임베딩 행렬 추출부는 사전학습된 코버트(KoBERT) 모델의 입력 계층(layer)에서 토큰 임베 딩 행렬(token embedding matrix)을 임베딩 행렬로서 추출할 수 있다. 코버트 모델은 입력 계층과 인코더 계층을 포함할 수 있으며, 임베딩 행렬 추출부는 입력 계층의 임베딩 계층들 각각에서 독립적으로 생성되는 임 베딩 행렬들 중에서 각 단어의 벡터를 출력하는 토큰 임베딩 행렬을 선택적으로 추출할 수 있다. 한편, 사전학 습된 코버트 모델에 관한 정보와 데이터는 쉽게 접근 및 이용 가능한 점에서 코버트 모델에 대한 사전학습의 구 체적인 과정은 생략한다. 일 실시예에서, 임베딩 행렬 추출부는 입력 계층의 임베딩 계층들에서 적어도 토큰 임베딩 행렬을 포함하 는 임베딩 조합을 통해 임베딩 행렬을 생성할 수 있다. 임베딩 행렬 추출부는 기본적으로 코버트 모델에서 토큰 임베딩 행렬을 추출할 수 있으며, 필요에 따라 토큰 임베딩 행렬과 함께 나머지 임베딩 행렬을 추출할 수 있다. 예를 들어, 임베딩 행렬 추출부는 토큰 임베딩(token embedding) 행렬과 세그먼트 임베딩(segment embedding) 행렬을 추출하거나 또는 토큰 임베딩 행렬과 포지션 임베딩(position embedding) 행렬을 추출할 수 도 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 42, "content": "추상 요약 모델 구축부는 임베딩 행렬이 적용된 추상 요약 모델을 정의하고 학습 데이터셋을 기초로 텍스"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 43, "content": "트 요약문 생성을 위한 요약 학습을 수행하여 추상 요약 모델을 구축할 수 있다. 여기에서, 추상 요약 모델은"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 44, "content": "텍스트의 요약문을 생성하는 학습 모델로서 추상 요약을 통해 원문에 존재하지 않는 새로운 텍스트 시퀀스를 요"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 45, "content": "약문으로 생성하도록 학습된 모델에 해당할 수 있다. 또한, 요약 학습은 본 발명에 따른 추상 요약 모델을 구축 하기 위한 학습 과정에 해당할 수 있으며, 사전에 수집되어 생성된 학습 데이터셋의 학습 데이터들을 이용하여"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 46, "content": "수행될 수 있으며, 텍스트 요약 장치는 학습 데이터의 수집과 학습 데이터셋의 구축을 수행하는 독립적인"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 47, "content": "모듈을 포함하여 구현될 수 있다. 추상 요약 모델 구축부는 사전학습된 언어모델에서 추출된 임베딩 행렬"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 48, "content": "을 추상 요약 모델에 적용한 후 요약 학습을 통해 추상 요약 모델을 구축할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 49, "content": "일 실시예에서, 추상 요약 모델 구축부는 트랜스포머(transformer) 모델을 추상 요약 모델로서 정의하고"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 50, "content": "트랜스포머 모델의 임베딩 계층에 임베딩 행렬을 적용할 수 있다. 즉, 추상 요약 모델 구축부는 많은 양의 학습 데이터를 필요로 하는 트랜스포머 모델에 대해 사전학습을 통해 도출된 임베딩 행렬을 적용하여 매개변수 를 효과적으로 갱신할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 51, "content": "일 실시예에서, 추상 요약 모델 구축부는 트랜스포머 모델의 인코더 임베딩 행렬과 디코더(decoder) 임베"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 52, "content": "딩 행렬을 각각 임베딩 행렬로 대체할 수 있다. 추상 요약 모델 구축부는 사전학습된 정보가 트랜스포머의"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 53, "content": "문서를 이해하는 인코더와 요약문을 생성하는 디코더 양 쪽에 모두 전달될 수 있도록 인코더 임베딩 행렬과 디 코더(decoder) 임베딩 행렬을 모두 임베딩 행렬로 대체하여 적용할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 54, "content": "일 실시예에서, 추상 요약 모델 구축부는 복수의 단계들로 구성된 학습 과정을 반복적으로 수행하여 추상"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 55, "content": "요약 모델을 생성할 수 있다. 보다 구체적으로, 추상 요약 모델 구축부는 학습 데이터셋의 원문 텍스트를 트랜스포머 모델의 인코더(encoder)에 입력하고, 인코더의 임베딩 계층을 통해 원문 텍스트에 대한 단어 벡터를 생성하며, 단어 벡터를 인코더의 인코더 계층에 통과시켜 디코더에 전달하고, 디코더를 통해 원문 텍스트에 대"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 56, "content": "응되는 요약 텍스트를 생성하며, 원문 텍스트의 정답 텍스트와 요약 텍스트 간의 차이를 비교하여 인코더 및 디"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 57, "content": "코더를 갱신할 수 있다. 임베딩 행렬이 적용된 트랜스포머의 요약 학습 과정에 대해서는 도 8을 통해 보다 자세 히 설명한다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 58, "content": "텍스트 요약문 생성부는 추상 요약 모델을 이용하여 입력 텍스트에 대한 요약문을 생성할 수 있다. 즉, 텍"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 59, "content": "스트 요약문 생성부는 사용자로부터 입력되거나 또는 선택된 특정 입력 텍스트를 추상 요약 모델에 입력하"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 60, "content": "여 자동으로 요약문을 생성할 수 있다. 텍스트 요약문 생성부는 사용자 단말로부터 입력 텍스트를 수 신하거나 또는 데이터베이스에 저장된 텍스트들 중 어느 하나를 입력 텍스트로 결정할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 61, "content": "또한, 텍스트 요약문 생성부는 입력 텍스트를 그대로 사용할 수도 있으나, 필요에 따라 소정의 전처리 동"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 62, "content": "작을 수행한 다음 추상 요약 모델에 입력할 수도 있다. 예를 들어, 텍스트 요약문 생성부는 입력 텍스트에 서 특정 단어, 구문 또는 문장을 삭제할 수 있으며, 특정 길이 또는 단락을 기준으로 분할할 수 있다. 다른 예"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 63, "content": "로서, 텍스트 요약문 생성부는 복수의 입력 텍스트들을 하나로 통합하거나 또는 복수의 입력 텍스트들에서 추출된 문장들을 통합하여 새로운 입력 텍스트를 생성할 수도 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 64, "content": "요약문 품질 평가부는 텍스트 요약문 생성부에 의해 생성된 요약문의 품질을 평가하는 동작을 수행할"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 65, "content": "수 있다. 즉, 요약문 품질 평가부는 추상 요약 모델이 생성한 요약문이 실제 입력 텍스트를 잘 요약하고"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 66, "content": "있는지에 대한 정량적인 평가 결과를 생성할 수 있다. 일 실시예에서, 요약문 품질 평가부는 요약문과 사"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 67, "content": "용자에 의해 입력된 정답문 사이의 단어 일치 정도에 따라 적어도 하나의 평가 지표를 산출할 수 있다. 요약문"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 68, "content": "품질 평가부는 요약문에 관한 품질 평가의 결과로서 기 정의된 평가 지표들을 산출할 수 있다. 예를 들어,요약문 품질 평가부는 요약문의 요약 품질을 다양하게 평가하기 위해 ROUGE-1, ROUGE-2 및 ROUGE-L 등의 평가 지표들을 생성할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 69, "content": "제어부는 텍스트 요약 장치의 전체적인 동작을 제어하고, 임베딩 행렬 추출부, 추상 요약 모델"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 70, "content": "구축부, 텍스트 요약문 생성부 및 요약문 품질 평가부 간의 제어 흐름 또는 데이터 흐름을 관리 할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 71, "content": "도 3은 본 발명에 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법을 설명하는 순서도이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 72, "content": "도 3을 참조하면, 텍스트 요약 장치는 임베딩 행렬 추출부를 통해 텍스트의 문맥 정보를 사전학습하 여 구축된 언어모델로부터 임베딩 행렬(embedding matrix)를 추출할 수 있다(단계 S410). 여기에서, 사전학습된 언어모델로 KoBERT가 사용될 수 있으나, 반드시 이에 한정되지 않음은 물론이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 73, "content": "텍스트 요약 장치는 추상 요약 모델 구축부를 통해 임베딩 행렬이 적용된 추상 요약 모델을 정의하고"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 74, "content": "학습 데이터셋을 기초로 텍스트 요약문 생성을 위한 요약 학습을 수행하여 추상 요약 모델을 구축할 수 있다(단"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 75, "content": "계 S430). 여기에서, 추상 요약 모델은 트랜스포머를 기반으로 정의될 수 있으나, 반드시 이에 한정되지 않음은 물론이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 76, "content": "텍스트 요약 장치는 텍스트 요약문 생성부를 통해 추상 요약 모델을 이용하여 입력 텍스트에 대한 요"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 77, "content": "약문을 생성할 수 있다(단계 S450). 일 실시예에서, 텍스트 요약 장치는 텍스트 요약문 생성부를 통"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 78, "content": "해 입력 텍스트를 복수의 부분 텍스트들로 분할하고, 복수의 부분 텍스트들 각각을 추상 요약 모델에 입력하여"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 79, "content": "복수의 부분 요약문들을 생성하며, 복수의 부분 요약문들을 통합하여 요약문을 생성할 수 있다. 즉, 텍스트 요"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 80, "content": "약 장치는 필요에 따라 복수의 추상 요약 모델들을 포함하고, 복수의 부분 텍스트들에 대해 복수의 부분"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 81, "content": "요약문들을 병렬적으로 생성할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 82, "content": "또한, 텍스트 요약 장치는 요약문 품질 평가부를 통해 요약문의 품질을 평가할 수 있다. 이때, 텍스"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 83, "content": "트 요약 장치는 요약문에 대한 정량적인 평가 결과를 생성하기 위해 다양한 평가 지표들을 활용할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 84, "content": "도 4는 본 발명에 따른 텍스트 요약 과정을 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 85, "content": "도 4를 참조하면, 텍스트 요약 장치는 본 발명의 일 실시예에 따른 텍스트 요약 방법을 실행할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 86, "content": "즉, 텍스트 요약 방법은 트랜스포머 기반 요약 모델이 소량의 데이터가 주어진 환경에서도 양질의 요약문을 생 성할 수 있도록 한국어 사전학습 언어 모델인 KoBERT의 일부 요소를 트랜스포머에 적용할 수 있다. 도 4의 좌측 흐름은 대규모의 한국어 텍스트 문서로부터 MLM과 NSP를 수행하여 한국어의 복잡한 문맥 정보를 학"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 87, "content": "습한 KoBERT의 임베딩 행렬(Embedding Matrix)를 선택적으로 추출하는 과정에 해당할 수 있다. 요약 학습을 수 행할 트랜스포머의 임베딩 행렬은 추출된 임베딩 행렬로 대체될 수 있으며, Dacon 데이터의 훈련 데이터와 검증"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 88, "content": "데이터를 기초로 요약 학습이 수행될 수 있다. 이를 통해 구축된 트랜스포머 모델은 KoBERT의 언어학적 지식에"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 89, "content": "힘입어 적은 양의 데이터에 대해서도 효과적으로 요약문을 생성할 수 있게 된다. 학습이 완료된 요약 모델은 우"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 90, "content": "측 흐름과 같이 시험 데이터에 대한 추론을 통해 요약문을 생성할 수 있다. 도 5는 KoBERT의 내부 구조를 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 91, "content": "도 5를 참조하면, 텍스트 요약 장치는 사전학습된 KoBERT로부터 선택적으로 임베딩 행렬을 추출할 수 있다. KoBERT는 트랜스포머의 인코더 구조를 활용하여 한국어 위키피디아와 뉴스 등으로부터 수집한 약 500만 개의 문장에 대해 한국어 문맥 정보를 학습한 결과로서 구축될 수 있다. KoBERT는 학습한 문맥 정보를 효과적으 로 보존하기 위해 전통적인 트랜스포머 모델과 비교하여 더 큰 은닉층(hidden layer) 크기와 더 많은 어텐션 헤 드(attention head) 및 인코더 계층(encoder layer)을 가질 수 있다. 또한, KoBERT는 도 5와 같이 기존의 트랜 스포머와는 상이한 형태의 입력 계층을 가질 수 있다. 도 5에서, KoBERT는 총 13개의 계층, 즉 12개의 인코더 계층과 1개의 입력 계층으로 구성될 수 있다. 특히, 입 력 계층은 3개의 임베딩 계층으로 이루어질 수 있으며, 각각 단어의 벡터를 출력하는 토큰 임베딩(TokenEmbedding), 문서 내에서 문장의 위치 벡터를 출력하는 세그먼트 임베딩(Segment Embedding) 그리고 문서 내에 서 단어의 위치 벡터를 출력하는 포지션 임베딩(Position Embedding)으로 구성될 수 있다. 이때, 3개의 임베딩 계층은 각 목적에 따라 입력된 문장 또는 문서에 대하여 같이 동일한 차원을 갖는 벡터를 생성할 수 있다. 도 6은 KoBERT 입력 계층의 벡터 표현을 설명하는 도면이고, 도 7은 사전학습에 의한 토큰 임베딩 값의 변화를 설명하는 도면이다. 도 6 및 7을 참조하면, KoBERT는 입력 계층을 통해 입력된 문장에 대한 벡터를 생성할 수 있다. 각 단어는 고유 의 단어 표현뿐 아니라, 문서 내의 위치와 해당 단어가 속한 문장의 위치를 함께 고려하여 표현될 수 있다. 즉, KoBERT의 입력 계층은 동일한 단어 '_비'에 대해서도 해당 단어가 출현한 위치에 따라 상이한 벡터값을 출력할 수 있다. KoBERT는 입력 계층에서 표현된 언어학적 정보를 12개의 인코더 계층에 전달하며, 인코더 계층은 주어진 언어학 적 정보를 토대로 복잡한 문맥 관계를 학습할 수 있다. 해당 과정에서 KoBERT는 인코더 계층으로부터 학습한 문 맥 관계를 다시 입력 계층에 역전파(Backpropagation)하여, 입력 계층이 언어학적 정보를 이전보다 더욱 잘 나 타낼 수 있도록 할 수 있다. 즉, 해당 학습 과정이 반복적으로 수행됨에 따라 입력 계층에서 단어의 고유 표현을 나타내는 토큰 벡터의 값도 도 7과 같이 갱신될 수 있다. 이는 사전학습 과정에서 인코더 계층으로부터 전달받은 문맥 정보를 활용하여, 토"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 92, "content": "큰 벡터의 값을 생성하기 위한 임베딩 행렬 자체를 갱신함을 의미할 수 있다. 본 발명에 따른 텍스트 요약 방법"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 93, "content": "은 이렇게 갱신된 KoBERT의 토큰 임베딩 행렬을 추출하여, 요약 학습을 수행하는 트랜스포머의 임베딩 행렬을 대체함으로써 KoBERT의 지식을 습득하는 방안을 제공할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 94, "content": "도 8은 본 발명에 따른 트랜스포머 기반 요약 모델의 학습 과정을 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 95, "content": "도 8을 참조하면, 전통적인 트랜스포머는 자연어에 대한 사전 지식 없이 원문에 의존하여 요약 학습을 수행하므 로, 방대한 양의 매개변수를 적절히 갱신하기 위해 많은 양의 데이터를 필요로 할 수 있다. 본 발명에 따른 텍"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 96, "content": "스트 요약 방법은 트랜스포머의 토큰 임베딩 행렬을 KoBERT의 토큰 임베딩 행렬로 대체하여, 트랜스포머가 적은 양의 데이터만으로도 매개변수를 효과적으로 갱신할 수 있는 방안을 제공할 수 있다. 구체적으로, KoBERT를 통"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 97, "content": "해 학습한 단어 수준의 지식이 트랜스포머의 문서를 이해하는 인코더와 요약문을 생성하는 디코더 양 쪽에 모두 전달될 수 있도록 하기 위해, KoBERT의 토큰 임베딩 행렬을 트랜스포머의 인코더 임베딩과 디코더 임베딩 행렬 에 모두 적용할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 98, "content": "도 8의 경우, 임베딩 행렬이 적용된 트랜스포머가 요약 학습을 수행하는 과정을 나타낼 수 있다. 먼저, KoBERT의 임베딩 행렬이 입력된 원문에 대하여 단어 벡터를 생성하고, 생성된 단어 벡터에 시퀀스 정보를 추가하여 트랜스포머의 인코더 계층에 전달할 수 있다. 인코더 계층은 입력받은 벡터 표현으로부터 문맥 정보를 파악하고 이를 다시 구조화하여 트랜스포머의 디코더 계층에 전달할 수 있다. 디코더 계층"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 99, "content": "은 인코더로부터 전달받은 정보를 통해 요약문을 생성하고, 생성한 요약문과 요약 정답의 차이를 비"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 100, "content": "교하는 학습을 수행하여 요약문 생성 모델을 생성할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 101, "content": "본 발명에 따른 텍스트 요약 장치는 위의 과정을 통해 생성한 트랜스포머 기반 요약 모델을 사용하여 자동"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 102, "content": "으로 요약문을 생성할 수 있고, 생성된 요약문에 대한 품질 평가 과정을 수행할 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 103, "content": "도 9는 본 발명에 따른 요약문 생성 및 품질 평가 과정을 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 104, "content": "도 9를 참조하면, 학습이 완료된 트랜스포머는 요약 정답 없이 원문만을 입력으로 받아 요약문"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 105, "content": "을 자동으로 생성할 수 있다. 즉, 텍스트 요약 장치는 입력된 문서를 벡터로 구조화하고, 이미 학습한 문"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 106, "content": "맥 정보를 이용하여 구조화된 문서 벡터에 대응하는 요약을 생성할 수 있다. 또한, 생성된 요약의 품질은 사람"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 107, "content": "이 작성한 요약 정답과의 정량적 비교를 통해 측정 가능하며, 다양한 평가 지표 중 ROUGE가 사용될 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 108, "content": "일 실시예에서, 텍스트 요약 장치는 요약 품질을 다양하게 평가하기 위해 ROUGE-1, ROUGE-2, 그리고"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 109, "content": "ROUGE-L의 세 가지 지표를 사용할 수 있으며, 각 지표의 정의는 이하 실험 내용에서 설명한다.도 10 내지 13는 본 발명에 따른 텍스트 요약 방법에 관한 실험 내용을 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 110, "content": "여기에서는 본 발명에 따른 텍스트 요약 방법에 관한 실험 내용을 설명한다. 실험은 Dacon에서 제공한 한국어"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 111, "content": "문서 생성요약 데이터셋을 활용하여 수행될 수 있으며, 기존의 요약 데이터셋인 CNN-Daily Mail, BBC XSum의 데 이터 구분 비율을 참조하여 다음의 표 1와 같이 데이터의 역할을 구분할 수 있다. 표 1 Dacon Dataset Train Validation Test 100% 50% 20% 38,522 19,261 7,704 2,140 2,141 구체적으로, 총 42,803건의 데이터는 우선 학습용 데이터 38,522건, 검증용 데이터 2,140건, 그리고 테스트 데 이터 2,141건으로 각각 90:5:5의 비율로 구분될 수 있다. 또한, 소량의 학습 데이터가 주어진 상황에서 나타나"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 112, "content": "는 요약 품질의 하락 수준을 평가하기 위해, 학습용 데이터 38,522건(100%)을 모두 학습에 사용한 실험, 학습용 데이터 중 19,261건(50%)만 학습에 사용한 실험, 그리고 7,704건(20%)만 학습에 사용한 실험이 수행될 수 있고, 그 결과가 비교될 수 있다. 실험 환경은 다음의 표 2와 같이 Python 3.7과 딥러닝 프레임워크인 Tensorflow, Pytorch를 통해 구축될 수 있 으며, CPU 8 Core와 Tesla P40 GPU 등의 하드웨어 자원이 사용될 수 있다. 표 2 실험 환경 하드웨어 CPU 8 core GPU Tesla P30 * 2ea Memory 60GB 언어 Python 3.7 딥러닝 프레임워크Tensorflow 2.3.0 Pytorch 1.9.0 실험 모형은 도 10과 같이 구성될 수 있으며, 본 발명에 따른 방법의 우수성을 평가하기 위하여 두 가지 기준"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 113, "content": "모델과의 요약 품질 비교 실험이 수행될 수 있다. 첫 번째 비교 모델(Emb. Random)은 전통적인 트랜스포머 기반"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 114, "content": "요약 모델로, 임베딩 행렬의 초기값은 임의의 값으로 설정될 수 있다. 즉, Emb. Random 모델은 자연어에 대한"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 115, "content": "어떠한 사전 지식도 없이 주어진 원문에만 의존하여 요약 학습을 수행함을 의미할 수 있다. 두 번째 비교 모델 (Emb. Word2Vec)은 주변 단어의 문맥 정보를 학습하여 어휘의 벡터값을 산출하는 알고리즘인 Word2Vec을 사용하 여 트랜스포머 임베딩 행렬의 초기값을 설정한 모델이다. 즉, Emb. Word2Vec 모델은 Word2Vec을 통해 자연어에"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 116, "content": "대한 지식을 습득한 상황에서 요약 학습을 수행할 수 있다. 마지막 Emb. KoBERT는 본 발명에 따른 요약 모델로, KoBERT의 토큰 임베딩을 통해 자연어에 대한 지식이 전달될 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 117, "content": "해당 실험에서는 도 10에 따라 각 모델을 통해 요약문을 생성한 뒤, 각 모델이 생성한 요약문의 품질을 대표적"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 118, "content": "인 요약 평가 지표인 ROUGE를 사용하여 평가할 수 있다. ROUGE는 비교 대상에 따라 더욱 세분화될 수 있으며, 대표적으로 ROUGE-N과 ROUGE-L이 사용될 수 있다. 해당 실험에서는 ROUGE-N 기반의 ROUGE-1, ROUGE-2 그리고"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 119, "content": "ROUGE-L를 활용하여 요약문의 품질이 평가될 수 있으며, 평가에 사용된 ROUGE 지표의 산출 방식은 도 11과 같다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 120, "content": "도 11에서, ROUGE 지표는 사람이 작성한 요약 정답과 요약 모델이 생성한 요약문 사이에 단어가 일치하는 정도"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 121, "content": "에 따라 요약 품질이 결정될 수 있다. 먼저, ROUGE-N은 일치하는 단어의 단위(N-Gram)에 따라 다양한 수준으로 구현될 수 있으며, 대표적으로 단어의 개별 단위(Uni-Gram)로 일치 정도를 평가하는 ROUGE-1과 연속된 두 개의 단어 단위(Bi-Gram)로 일치 정도를 평가하는 ROUGE-2가 사용될 수 있다. ROUGE-N의 산출에는 정밀도"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 122, "content": "(Precision)와 재현율(Recall)이 사용될 수 있으며, 정밀도는 생성된 요약문 관점에서의 일치 정도를, 재현율은"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 123, "content": "요약 정답 관점에서의 일치 정도를 측정할 수 있다.한편, ROUGE-L은 두 문장이 공통으로 갖는 서브시퀀스(Subsequence)의 길이로 요약문의 품질이 평가될 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 124, "content": "예를 들어, 도 11에서 요약 정답의 길이는 17, 모델이 생성한 요약문의 길이는 8이며, 두 문장이 공통으로 갖는 서브시퀀스 “_전국 / 적으로 / _비가 / _내 / 릴”의 길이는 5이다. 따라서, ROUGE-L(Recall)은 5/17이며, ROUGE-L(Precision)은 5/8가 될 수 있다. 해당 실험에서는 ROUGE-1, ROUGE-2 그리고 ROUGE-L의 지표 각각에 대해 정밀도, 재현율, 그리고 F1-Score가 모"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 125, "content": "두 측정되어 각 모델이 생성한 요약문의 품질이 평가될 수 있다. 구체적으로, 학습용 데이터로 모델이 학습되고 검증용 데이터로부터 손실(Loss) 값이 가장 낮은 에폭(Epoch)이 선정될 수 있으며, 이때의 모델이 테스트 데이"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 126, "content": "터에 적용되어 생성된 요약문에 대해 ROUGE 지표가 산출될 수 있다. 도 12는 학습 데이터의 크기에 따른 각 모델들의 ROUGE-1(F1-Score) 값의 변화를 도식화한 그래프이다. 도 12는"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 127, "content": "트랜스포머만 사용한 전통적인 요약 모델(Emb.Random), Word2Vec을 사용하여 트랜스포머 임베딩 행렬의 초기값 을 설정한 모델(Emb.Word2Vec), 그리고 본 발명에 따른 모델인 KoBERT를 사용하여 트랜스포머 임베딩 행렬의 초"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 128, "content": "기값을 설정한 모델(Emb.KoBERT)의 ROUGE-1(F1-Score) 측면에서의 요약 품질을 나타낼 수 있다. 세 가지 모델"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 129, "content": "모두에서 학습 데이터의 크기가 작을수록 요약 성능이 저하되는 모습이 나타날 수 있다. 이러한 양상은 ROUGE- 2, 그리고 ROUGE-L의 지표를 활용한 분석에서도 공통적으로 나타날 수 있으며, 해당 결과는 도 13을 통해 도시 될 수 있다. 도 12 및 13에서, Emb.Word2Vec 모델은 학습 데이터 사용 비율과 무관하게 가장 낮은 성능을 나타낼 수 있으며, Emb.Random과 Emb.KoBERT는 학습 데이터 사용 비율에 따라 성능 순위가 역전되는 모습을 나타낼 수 있다. 구체"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 130, "content": "적으로, 학습 데이터의 양이 충분한 경우에는 자연어에 대한 사전 지식 없이 원문에만 의존하여 요약 학습을 수 행한 Emb. Random 모델의 성능이 가장 우수하게 나타날 수 있으나, 학습 데이터의 양이 줄어들수록 Emb. Random 모델의 성능이 급격하게 하락할 수 있다. 이와 달리, 본 발명에 따른 모델인 Emb. KoBERT 모델은 학습 데이터 양의 감소에 따른 성능 저하 현상이 매우 완만하게 나타날 수 있으며, 이로 인해 적은 양의 학습 데이터가 사용"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 131, "content": "된 50%, 20% 실험에서는 Emb. KoBERT의 요약 성능이 가장 우수하게 나타날 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 132, "content": "결과적으로, 실험을 통해 본 발명에 따른 모델이 소량의 학습 데이터가 주어진 상황에서 기존의 요약 모델에 비"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 133, "content": "해 양질의 요약문을 생성할 수 있음이 확인될 수 있다. 해당 결과는 방대한 양의 텍스트 데이터로부터 문맥 정"}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 134, "content": "보를 학습한 KoBERT의 임베딩 행렬이 요약 학습을 수행할 트랜스포머에게 자연어 정보를 효과적으로 전달한 것 에 기인한 것으로 해석될 수 있다."}
{"patent_id": "10-2022-0009998", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 135, "content": "본 발명에 따른 텍스트 요약 방법 및 장치는 충분한 양의 학습 데이터가 확보되지 않으면 양질의 요약문을 생성 하기 어렵다는 한계를 극복하기 위하여 한국어 사전학습 언어 모델인 KoBERT의 일부 요소를 추출하여 전통적인 트랜스포머 모델에 적용할 수 있다. 상기에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특 허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다."}
{"patent_id": "10-2022-0009998", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 텍스트 요약 시스템을 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "도면", "subsection": "도면설명", "item": 2, "content": "도 2는 도 1의 텍스트 요약 장치의 기능적 구성을 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "도면", "subsection": "도면설명", "item": 3, "content": "도 3은 본 발명에 따른 사전학습 언어모델을 활용한 트랜스포머 기반 텍스트 요약 방법을 설명하는 순서도이다."}
{"patent_id": "10-2022-0009998", "section": "도면", "subsection": "도면설명", "item": 4, "content": "도 4는 본 발명에 따른 텍스트 요약 과정을 설명하는 도면이다. 도 5는 KoBERT의 내부 구조를 설명하는 도면이다. 도 6은 KoBERT 입력 계층의 벡터 표현을 설명하는 도면이다. 도 7은 사전학습에 의한 토큰 임베딩 값의 변화를 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "도면", "subsection": "도면설명", "item": 5, "content": "도 8은 본 발명에 따른 트랜스포머 기반 요약 모델의 학습 과정을 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "도면", "subsection": "도면설명", "item": 6, "content": "도 9는 본 발명에 따른 요약문 생성 및 품질 평가 과정을 설명하는 도면이다."}
{"patent_id": "10-2022-0009998", "section": "도면", "subsection": "도면설명", "item": 7, "content": "도 10 내지 13는 본 발명에 따른 텍스트 요약 방법에 관한 실험 내용을 설명하는 도면이다."}
