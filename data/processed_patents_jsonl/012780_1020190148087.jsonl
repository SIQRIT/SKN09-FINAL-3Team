{"patent_id": "10-2019-0148087", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0060252", "출원번호": "10-2019-0148087", "발명의 명칭": "실종자 탐색 방법 및 시스템", "출원인": "한국항공우주연구원", "발명자": "최연주"}}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "서버에 의해 수행되는 방법에 있어서,실종자의 상의 및 하의 색상을 포함하는 실종자 정보를 수신하는 단계;입력 영상을 수신하는 단계;사전에 트레이닝된 학습 모델을 이용하여 상기 입력 영상 내에 상기 상의 및 하의 색상에 부합하는 객체를 검색하는 단계; 및상기 입력 영상에서 상기 검색된 객체를 포함하는 부분을 출력하는 단계를 포함하는 실종자 탐색 방법."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 학습 모델은 착용하고 있는 상의 및 하의 색상이 라벨링된 사람의 이미지들을 기초로 트레이닝되는 것을특징으로 하는 실종자 탐색 방법."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 학습 모델은 영상으로부터 사람을 구분하기 위한 제1 학습 모델, 및 사람의 영상으로부터 상기 사람이 입고 있는 상의와 하의의 색상을 구분하기 위한 제2 학습 모델을 포함하는 실종자 탐색 방법."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 입력 영상은 무인기로부터 수신되고,상기 무인기의 프로세서는 일반 카메라로 촬영 영역을 촬영한 컬러 영상과 열화상 카메라로 상기 촬영 영역을촬영한 열화상 영상을 기초로 상기 입력 영상을 생성하도록 구성되는 것을 특징으로 하는 실종자 탐색 방법."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서,상기 무인기의 프로세서는 상기 컬러 영상에서 상기 열화상 영상의 미리 설정된 기준 온도 이하의 영역을 마스킹함으로써 상기 입력 영상을 생성하도록 구성되는 것을 특징으로 하는 실종자 탐색 방법."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제7 항에 있어서,상기 객체는 상기 컬러 영상에서 마스킹되지 않은 컬러 부분을 상기 학습 모델에 입력함으로써 검색되는 것을특징으로 하는 실종자 탐색 방법."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 실종자 정보는 단말기로부터 수신되고, 상기 검색된 객체를 포함하는 부분은 상기 단말기로 송신되는 것을특징으로 하는 실종자 탐색 방법."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "공개특허 10-2021-0060252-3-서버를 이용하여 제1 항 내지 제7항 중 어느 한 항의 방법을 실행시키기 위하여 매체에 저장된 컴퓨터프로그램."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "서로 통신하는 서버와 무인기를 포함하고,상기 서버는 실종자의 상의 및 하의 색상을 포함하는 실종자 정보를 수신하고, 상기 무인기로부터 입력 영상을수신하고, 사전에 트레이닝된 학습 모델을 이용하여 상기 입력 영상 내에 상기 상의 및 하의 색상에 부합하는객체를 검색하고, 상기 입력 영상에서 상기 검색된 객체를 포함하는 부분을 출력하도록 구성되는 것을 특징으로하는 실종자 탐색 시스템."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9 항에 있어서,상기 학습 모델은 입고 있는 상의 및 하의 색상이 라벨링된 사람의 이미지들을 기초로 트레이닝되는 것을 특징으로 하는 실종자 탐색 시스템."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9 항에 있어서,상기 학습 모델은 영상으로부터 사람을 구분하기 위한 제1 학습 모델, 및 사람의 영상으로부터 상기 사람이 입고 있는 상의와 하의의 색상을 구분하기 위한 제2 학습 모델을 포함하는 실종자 탐색 시스템."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9 항에 있어서,상기 무인기는,컬러 영상을 생성하는 일반 카메라;열화상 영상을 생성하는 열화상 카메라; 및 상기 컬러 영상과 상기 열화상 영상을 기초로 상기 입력 영상을 생성하도록 구성되는 프로세서를 포함하는 것을특징으로 하는 실종자 탐색 시스템."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12 항에 있어서,상기 일반 카메라와 상기 열화상 카메라는 동일 영역을 촬영하도록 구성되는 것을 특징으로 하는 실종자 탐색시스템."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12 항에 있어서,상기 프로세서는 상기 컬러 영상에서 상기 열화상 영상의 미리 설정된 기준 온도 이하의 영역을 마스킹함으로써상기 입력 영상을 생성하도록 구성되는 것을 특징으로 하는 실종자 탐색 시스템."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14 항에 있어서,상기 객체는 상기 컬러 영상에서 마스킹되지 않은 컬러 부분을 상기 학습 모델에 입력함으로써 검색되는 것을특징으로 하는 실종자 탐색 시스템."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제9 항에 있어서,공개특허 10-2021-0060252-4-상기 무인기는 인공위성 신호로부터 위치 정보를 생성하는 위치측정 모듈을 더 포함하고,상기 프로세서는 상기 입력 영상과 함께 상기 위치 정보를 상기 서버에 송신하는 것을 특징으로 하는 실종자 탐색 시스템."}
{"patent_id": "10-2019-0148087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제9 항에 있어서,상기 실종자 정보를 상기 서버에 전송하고, 상기 검색된 객체를 포함하는 부분을 수신하는 적어도 하나의 단말기를 더 포함하는 실종자 탐색 시스템."}
{"patent_id": "10-2019-0148087", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "다양한 실시예들에 따라서 실종자를 탐색하는 방법 및 시스템이 개시된다. 실종자 탐색 방법은 서버에 의해 수 행되며, 실종자의 상의 및 하의 색상을 포함하는 실종자 정보를 수신하는 단계, 입력 영상을 수신하는 단계, 사 전에 트레이닝된 학습 모델을 이용하여 상기 입력 영상 내에 상기 상의 및 하의 색상에 부합하는 객체를 검색하 는 단계, 및 상기 입력 영상에서 상기 검색된 객체를 포함하는 부분을 출력하는 단계를 포함한다."}
{"patent_id": "10-2019-0148087", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 실종자 탐색 방법 및 시스템에 관한 것으로서, 더욱 자세하게는 무인기를 활용하여 실종자의 의상을 중심으로 신속하게 실종자를 탐색하는 시스템, 방법 및 컴퓨터 프로그램에 관한 것이다."}
{"patent_id": "10-2019-0148087", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "치매 노인이나 유아의 실종 사건은 끊이지 않고 있다. 현재 실종자를 빨리 찾기 위해 다양한 방법이 활용되고 있다. 예를 들면, 전국에 수백만 개 이상 설치된 CCTV의 영상에서 실종자의 얼굴을 매칭하는 기술을 활용하고 있다. 그러나, 다량의 데이터 전체를 검색할 경우 너무 오랜 시간이 소요될 수 밖에 없고, 영상의 화질이 좋지 않은 경우 그 효과가 현저히 감소하게 된다."}
{"patent_id": "10-2019-0148087", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는 고정된 CCTV의 영상에서 실종자의 얼굴을 매칭하는 방식으로는 신속한 탐색 이 불가능하다는 점을 극복하고자, 무인기를 활용하고 실종자의 얼굴이 아닌 실종자의 의상을 중심으로 신속하 게 실종자를 탐색하는 시스템, 방법 및 컴퓨터 프로그램을 제공하는 것이다."}
{"patent_id": "10-2019-0148087", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제들을 달성하기 위한 기술적 수단으로서, 본 발명의 일 측면에 따른 서버에 의해 수행되는 실 종자 탐색 방법은 실종자의 상의 및 하의 색상을 포함하는 실종자 정보를 수신하는 단계, 입력 영상을 수신하는 단계, 사전에 트레이닝된 학습 모델을 이용하여 상기 입력 영상 내에 상기 상의 및 하의 색상에 부합하는 객체 를 검색하는 단계, 및 상기 입력 영상에서 상기 검색된 객체를 포함하는 부분을 출력하는 단계를 포함한다. 일 예에 따르면, 상기 학습 모델은 입고 있는 상의 및 하의 색상이 라벨링된 사람의 이미지들을 기초로 트레이 닝될 수 있다. 다른 예에 따르면, 상기 학습 모델은 영상으로부터 사람을 구분하기 위한 제1 학습 모델, 및 사람의 영상으로부 터 상기 사람이 입고 있는 상의와 하의의 색상을 구분하기 위한 제2 학습 모델을 포함할 수 있다. 또 다른 예에 따르면, 상기 입력 영상은 무인기로부터 수신될 수 있다. 상기 무인기의 프로세서는 일반 카메라 로 촬영 영역을 촬영한 컬러 영상과 열화상 카메라로 상기 촬영 영역을 촬영한 열화상 영상을 기초로 상기 입력 영상을 생성하도록 구성될 수 있다. 또 다른 예에 따르면, 상기 무인기의 프로세서는 상기 컬러 영상에서 상기 열화상 영상의 미리 설정된 기준 온 도 이하의 영역을 마스킹함으로써 상기 입력 영상을 생성하도록 구성될 수 있다. 또 다른 예에 따르면, 상기 객체는 상기 컬러 영상에서 마스킹되지 않은 컬러 부분을 상기 학습 모델에 입력함 으로써 검색될 수 있다. 또 다른 예에 따르면, 상기 실종자 정보는 단말기로부터 수신되고, 상기 검색된 객체를 포함하는 부분은 상기 단말기로 송신될 수 있다. 본 발명의 다른 측면에 따르면, 서버를 이용하여 전술한 실종자 탐색 방법을 실행시키기 위하여 매체에 저장된 컴퓨터 프로그램이 제공된다. 본 발명의 또 다른 측면에 따른 실종자 탐색 시스템은 서로 통신하는 서버와 무인기를 포함한다. 상기 서버는 실종자의 상의 및 하의 색상을 포함하는 실종자 정보를 수신하고, 상기 무인기로부터 입력 영상을 수신하고, 사 전에 트레이닝된 학습 모델을 이용하여 상기 입력 영상 내에 상기 상의 및 하의 색상에 부합하는 객체를 검색하 고, 상기 입력 영상에서 상기 검색된 객체를 포함하는 부분을 출력하도록 구성된다. 일 예에 따르면, 상기 학습 모델은 입고 있는 상의 및 하의 색상이 라벨링된 사람의 이미지들을 기초로 트레이 닝될 수 있다. 다른 예에 따르면, 상기 학습 모델은 영상으로부터 사람을 구분하기 위한 제1 학습 모델, 및 사람의 영상으로부 터 상기 사람이 입고 있는 상의와 하의의 색상을 구분하기 위한 제2 학습 모델을 포함할 수 있다. 또 다른 예에 따르면, 상기 무인기는 컬러 영상을 생성하는 일반 카메라, 열화상 영상을 생성하는 열화상 카메 라, 및 상기 컬러 영상과 상기 열화상 영상을 기초로 상기 입력 영상을 생성하도록 구성되는 프로세서를 포함할 수 있다. 또 다른 예에 따르면, 상기 일반 카메라와 상기 열화상 카메라는 동일 영역을 촬영하도록 구성될 수 있다. 또 다른 예에 따르면, 상기 프로세서는 상기 컬러 영상에서 상기 열화상 영상의 미리 설정된 기준 온도 이하의 영역을 마스킹함으로써 상기 입력 영상을 생성하도록 구성될 수 있다. 또 다른 예에 따르면, 상기 객체는 상기 컬러 영상에서 마스킹되지 않은 컬러 부분을 상기 학습 모델에 입력함 으로써 검색될 수 있다. 또 다른 예에 따르면, 상기 무인기는 인공위성 신호로부터 위치 정보를 생성하는 위치측정 모듈을 더 포함할 수 있다. 상기 프로세서는 상기 입력 영상과 함께 상기 위치 정보를 상기 서버에 송신할 수 있다. 또 다른 예에 따르면, 상기 실종자 탐색 시스템은 상기 실종자 정보를 상기 서버에 전송하고, 상기 검색된 객체 를 포함하는 부분을 수신하는 적어도 하나의 단말기를 더 포함할 수 있다."}
{"patent_id": "10-2019-0148087", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 무인기를 활용함으로써 더욱 빠르고 신속하게 실종자를 탐색할 수 있을 뿐만 아니라, 실종자 의 얼굴이 아닌 실종자의 의상 색상을 중심으로 실종자를 탐색하기 때문에 높은 연산량을 필요로 하지 않으며, 무인기는 일반 카메라뿐만 아니라 열화상 카메라를 이용하여 컬러 영상에서 인체의 체온에 해당하는 영역만을 추출할 수 있기 때문에, 연산량을 더욱 줄일 수 있다. 따라서, 본 발명에 따르면 실종자의 탐색 시간이 획기적 으로 감소될 수 있다."}
{"patent_id": "10-2019-0148087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 다양한 실시예들을 상세히 설명한다. 그러나 본 개시의 기술적 사상은 다양한 형태로 변형되어 구현 될 수 있으므로 본 명세서에서 설명하는 실시예들로 제한되지 않는다. 본 명세서에 개시된 실시예들을 설명함에 있어서 관련된 공지 기술을 구체적으로 설명하는 것이 본 개시의 기술적 사상의 요지를 흐릴 수 있다고 판단 되는 경우 그 공지 기술에 대한 구체적인 설명을 생략한다. 동일하거나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 본 명세서에서 어떤 요소가 다른 요소와 \"연결\"되어 있다고 기술될 때, 이는 \"직접적으로 연결\"되어 있는 경우 뿐 아니라 그 중간에 다른 요소를 사이에 두고 \"간접적으로 연결\"되어 있는 경우도 포함한다. 어떤 요소가 다 른 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 요소 외에 또 다른 요소를 배제하는 것이 아니라 또 다른 요소를 더 포함할 수 있는 것을 의미한다. 일부 실시예들은 기능적인 블록 구성들 및 다양한 처리 단계들로 설명될 수 있다. 이러한 기능 블록들의 일부 또는 전부는 특정 기능을 실행하는 다양한 개수의 하드웨어 및/또는 소프트웨어 구성들로 구현될 수 있다. 예 를 들어, 본 개시의 기능 블록들은 하나 이상의 마이크로프로세서들에 의해 구현되거나, 소정의 기능을 위한 회 로 구성들에 의해 구현될 수 있다. 본 개시의 기능 블록들은 다양한 프로그래밍 또는 스크립팅 언어로 구현될 수 있다. 본 개시의 기능 블록들은 하나 이상의 프로세서들에서 실행되는 알고리즘으로 구현될 수 있다. 본 개시의 기능 블록이 수행하는 기능은 복수의 기능 블록에 의해 수행되거나, 본 개시에서 복수의 기능 블록이 수 행하는 기능들은 하나의 기능 블록에 의해 수행될 수도 있다. 또한, 본 개시는 전자적인 환경 설정, 신호 처리, 및/또는 데이터 처리 등을 위하여 종래 기술을 채용할 수 있다. 도 1은 일 실시예에 따른 실종자 탐색 시스템의 구성을 도시한다. 도 1을 참조하면, 실종자 탐색 시스템은 무인기, 서버, 단말기, 및 이들을 연결하는 네트 워크를 포함한다. 무인기는 소위 드론으로 지칭되는 무인 비행기로서, 조종사가 탑승하지 않고 무선전파 유도에 의해 비행과 조종이 가능한 비행기나 헬리콥터 모양의 무인 비행체와 무인 비행체를 무선으로 조종하는 비행체 컨트롤러를 포함한다. 무인기는 고공 영상 또는 사진 촬영, 물품 배달, 기상정보 수집, 농약 살포 등 다양한 기능을 수행할 수 있다. 무인기와 통신한다는 것은 무인 비행체 또는 비행체 컨트롤러와 통신하는 것을 포함하고, 무인 비행체와 통신한다는 것은 비행체 컨트롤러를 통해 통신하는 것을 포함한다. 예를 들면, 무인기가 영상을 네트워크 에 송신하는 것은 무인 비행체가 네트워크에 직접 접속하여 영상을 송신할 수도 있고, 무인 비행체가 비행체 컨트롤러를 거쳐 네트워크에 영상을 송신할 수도 있다. 서버는 실종자 정보를 관리하고 영상으로부터 실종자를 탐색하는 기능을 수행한다. 서버는 네트워크 를 통해 무인기 및/또는 단말기와 통신하며, 명령, 코드, 파일, 콘텐트, 서비스 등을 제공하는 하나의 컴퓨터 장치 또는 네트워크를 통해 서로 접속하는 복수의 컴퓨터 장치들로 구현될 수 있다. 서버는 단말기로부터 실종자 정보를 수신하고, 무인기로부터 입력 영상을 수신하고, 학습 모델 을 이용하여 입력 영상에서 실종자 정보에 부합하는 객체를 검색하여, 입력 영상에서 검색된 객체가 포함된 부 분인 객체 영상을 출력할 수 있다. 단말기는 컴퓨팅 기능을 수행하는 사용자 단말기로서, 유선 통신 및/또는 무선 통신을 통해 네트워크 에 접속하여 서버와 통신할 수 있다. 단말기는 예컨대 개인용 컴퓨터, 노트북 컴퓨터, 스마트폰, 태 블릿 PC 등을 포함할 수 있다. 단말기는 서버에 실종자 정보를 송신하고, 서버로부터 객체 영 상을 수신할 수 있다. 단말기는 실종자의 가족, 친구 등이 사용하는 개인용 컴퓨터 장치일 수 있다. 단 말기는 경찰서나 소방서, 또는 실종자 신고를 받는 기관의 컴퓨터 장치일 수 있다. 네트워크는 무인기, 서버 및 단말기를 서로 통신 가능하게 연결할 수 있다. 예를 들어, 네 트워크는 단말기가 서버에 접속하여 실종자 정보를 송신하고 실종자 정보에 부합하는 객체의 영 상을 수신할 수 있는 접속 경로를 제공할 수 있다. 네트워크는 무인기가 서버에 입력 영상을 송 신하고, 서버로부터의 명령을 수신할 수 있는 접속 경로를 제공할 수 있다. 네트워크는 유선 네트워크 및/또는 무선 네트워크를 포함할 수 있다. 무인기와 서버 간에는 무 선 네트워크를 통해 통신할 수 있다. 예를 들면, 네트워크는 근거리 네트워크(LAN: Local Area Network), 도시권 네트워크(MAN: Metropolitan Area Network), 광역 네트워크(WAN: Wide Area Network) 등과 같은 다양한 네트워크를 포함할 수 있다. 네트워크는 월드 와이드 웹(WWW: World Wide Web)을 포함할 수도 있다. 그러나, 본 실시예에 따른 네트워크는 앞에서 열거된 네트워크에 국한되지 않고, 공지의 이동 통신네트워크, 공지의 무선 데이터 네트워크, 공지의 전화 네트워크, 공지의 유/무선 텔레비전 네트워크 중 적어도 하나를 포함할 수 있다. 네트워크는 버스 네트워크, 스타 네트워크, 링 네트워크, 메쉬 네트워크, 스타-버 스 네트워크, 트리 또는 계층적(hierarchical) 네트워크 등을 포함하는 네트워크 토폴로지 중 임의의 하나 이상 을 포함할 수 있다. 도 2는 일 실시예에 따른 무인기의 일부 내부 구성을 도시한다. 도 2를 참조하면, 무인기는 프로세서, 메모리, 통신모듈, 제1 카메라, 및 제2 카메라 을 포함할 수 있다. 무인기는 위치측정모듈을 더 포함할 수 있다. 프로세서, 메모리 , 통신 모듈, 제1 카메라, 제2 카메라, 및 위치측정모듈는 버스를 통해 서로 데이터를 교환할 수 있다. 도 2에 도시되지 않지만, 무인기는 비행 본체, 모터와 프로펠러 등으로 구성되 는 추력기, 관성 센서, 적외선 센서 등과 같은 센서부 등을 더 포함할 수 있다. 프로세서는 기본적인 산술, 로직 및 입출력 연산을 수행하고, 예컨대 메모리에 저장된 프로그램 코드 를 실행할 수 있다. 프로세서는 기본적으로 무인기의 비행을 위해 추력기를 제어할 수 있다. 프로 세서는 자율 비행을 위한 프로그램 코드 및/또는 비행체 컨트롤러로부터 수신되는 제어 명령에 따른 비행 을 위한 프로그램 코드를 실행할 수 있다. 프로세서는 무인기의 무인 비행체에 탑재된 제1 프로세서, 및 무인기의 비행체 컨트롤러에 탑재 된 제2 프로세서를 포함할 수 있다. 일 실시예에 따르면, 프로세서는 제1 카메라로부터의 컬러 영상 및 제2 카메라로부터의 열화상 영상을 영상 처리하여, 입력 영상을 생성할 수 있다. 입력 영상은 서버에 입력될 영상으로서, 컬러 영상 과 열화상 영상에 기초하여 프로세서에 의해 생성되는 영상을 지칭한다. 메모리는 프로세서가 판독할 수 있는 기록 매체로서, RAM(random access memory) 및/또는 ROM(read only memory)를 포함할 수 있다. 메모리에는 운영체제와 적어도 하나의 프로그램 코드가 저장될 수 있다. 메모리에는 다양한 실시예들에 따라서 컬러 영상과 열화상 영상을 영상 처리하기 위한 프로그램 코드가 저 장될 수 있다. 통신 모듈은 무선 네트워크를 통해 서버와 데이터를 교환할 수 있다. 예컨대, 통신 모듈은 프로세서에서 생성된 입력 영상을 서버에 송신하고 서버로부터 제어 명령을 수신할 수 있다. 통신 모듈은 무인기의 무인 비행체에 탑재되는 제1 통신 모듈과 무인기의 비행체 컨트롤러에 탑 재되는 제2 통신 모듈을 포함할 수 있다. 무인 비행체와 비행체 컨트롤러는 제1 통신 모듈 및 제2 통신 모듈을 통해 서로 통신할 수 있다. 제1 카메라는 일반적인 가시광선 카메라로서, 컬러 영상을 생성할 수 있다. 컬러 영상은 프로세서에 제공될 수 있다. 제1 카메라는 줌 기능을 가질 수 있으며, 프로세서 또는 비행체 컨트롤러의 제어에 따라 확대된 영상을 생성할 수 있다. 제2 카메라는 열화상 카메라로서, 열화상 영상을 생성할 수 있다. 열화상 영상은 방사되는 적외선의 측정 에 의해 얻어진 표면의 온도 분포를 흑백의 농담 또는 색상으로 표시되는 영상이다. 열화상 영상을 이용하면 산에 발생한 산불이나 일정한 체온을 유지할 수 있는 사람과 같은 정온 동물을 쉽게 탐지할 수 있다. 제2 카메라는 일화상 카메라로 지칭되고, 제1 카메라는 일반 카메라로 지칭될 수 있다. 제1 카메라와 제2 카메라는 동일한 영역을 촬영하도록 구성될 수 있다. 제1 카메라가 기본 배 율로 촬영한 영상과 제2 카메라가 기본 배율로 촬영한 영상은 실질적으로 동일한 대상 또는 영역을 촬영한 것일 수 있다. 이를 통해 제1 카메라가 생성한 컬러 영상과 제2 카메라가 촬영한 열화상 영상을 서 로 정합(Matching)하는 과정이 생략될 수 있다. 일 예에 따르면, 제1 카메라와 제2 카메라는 서로 평행한 촬영 방향을 갖도록 서로 인접하게 배치될 수 있다. 다른 예에 따르면, 제1 카메라와 제2 카메라는 프로세서에 의해 제어되는 구동부에 장착될 수 있다. 구동부는 제1 카메라와 제2 카메라를 회전시켜서 촬영 방향을 변경할 수 있다. 제 1 카메라와 제2 카메라는 하나의 구동부에 장착되어, 서로 동일하게 회전할 수 있다. 프로세서는 제1 카메라가 촬영한 컬러 영상을 수신하고, 제2 카메라가 촬영한 열화상 영상을 수 신하고, 컬러 영상과 열화상 영상에 기초하여 입력 영상을 생성할 수 있다.일 예에 따르면, 프로세서는 컬러 영상에서 열화상 영상의 미리 설정된 기준 온도 이하의 영역을 마스킹함 으로써 입력 영상을 생성하도록 구성될 수 있다. 예를 들면, 프로세서는 열화상 영상에서 미리 설정된 기 준 온도를 기초로 블랙 영역과 화이트 영역을 구분하여 필터 영상을 생성할 수 있다. 블랙 영역은 해당 픽셀에 해당하는 온도가 기준 온도 이하인 영역이고, 화이트 영역은 해당 픽셀에 해당하는 온도가 기준 온도보다 높은 영역일 수 있다. 기준 온도는 대기 온도를 기초로 사전에 설정될 수 있다. 예를 들면, 대기 온도가 20도 정도인 경우, 기준 온 도는 예컨대 22도로 설정될 수 있다. 사람의 체온으로 인하여, 열화상 영상에서 사람에 해당하는 영역은 화이 트 영역에 포함될 수 있다. 프로세서는 필터 영상과 컬러 영상을 결합하여 컬러 영상의 일부만을 추출할 수 있다. 필터 영상에서 블 랙 영역에 해당하는 컬러 영상의 일부분은 블랙 영역으로 마스킹되고, 필터 영상에서 화이트 영역에 해당하는 컬러 영상의 나머지 부분만 남을 수 있다. 프로세서는 필터 영상과 컬러 영상에 대하여 AND 연산을 수행 할 수 있다. 프로세서는 필터 영상의 화이트 영역을 미리 설정된 마진만큼 넓힌 후 컬러 영상과 결합함으로써, 컬러 영 상에서 사람에 해당하는 부분이 너무 타이트하게 설정되는 것을 방지할 수 있다. 프로세서는 위와 같은 과정을 통해 추출된 컬러 영상의 일부만을 서버에 전송할 수 있다. 프로세서 는 컬러 영상에서 사람이 존재할 가능성이 높은 영역만을 추출하여 서버에 전송하기 때문에, 서버 에서 실종자를 검색할 범위가 감소하므로 신속한 탐색이 가능하다. 또한, 사람이 존재할 가능성이 높은 영역만을 추출하여 서버에 전송하기 때문에, 무선 네트워크를 통해 전송할 데이터 량 역시 감소할 수 있다. 그에 따라, 무인기는 제1 카메라로 촬영하는 영상의 해상도를 더 높일 수 있다. 제1 카메라 의 해상도가 너무 높을 경우, 생성되는 컬러 영상의 데이터 크기가 커지기 때문에, 무선 네트워크를 통해 실시간으로 전송하기 어려울 수 있다. 그러나, 본 발명에 따르면 데이터의 일부만을 추출하여 전송하므로, 컬러 영상의 해상도가 높더라도 무선 네트워크의 대역폭 내에서 실시간으로 서버에 전송 할 수 있다. 위치측정모듈은 GNSS(Global Navigation Satellite System) 위성에서 발신하는 전파를 이용하여 지구 상 의 위치를 측정하는 모듈이다. 위치측정모듈은 무인기의 위치를 측정하여 위치 정보를 생성하고, 위 치 정보를 프로세서에 제공할 수 있다. 프로세서는 위치 정보를 기초로 자율 비행 등에 활용할 수 있다. 프로세서는 컬러 영상과 열화상 영상을 기초로 생성한 입력 영상과 함께 위치 정보를 서버에 전송할 수 있다. 도 3은 일 실시예에 따른 서버와 단말기의 내부 구성을 도시한다. 도 3을 참조하면, 실종자 탐색 시스템은 예컨대 네트워크에 접속하는 서버와 단말기를 포함할 수 있다. 서버는 프로세서, 메모리, 통신 모듈, 및 입출력 인터페이스를 포함한다. 프로세서 , 메모리, 통신 모듈, 및 입출력 인터페이스는 버스를 통해 서로 데이터를 교환할 수 있다. 프로세서는 기본적인 산술, 로직 및 입출력 연산을 수행하고, 예컨대 메모리에 저장된 프로그램 코드, 예컨대, 학습 모델을 실행할 수 있다. 메모리는 프로세서가 판독할 수 있는 기록 매체로서, RAM, ROM 및 디스크 드라이브와 같은 비소멸성 대용량 기록장치(permanent mass storage device)를 포함할 수 있다. 메모리에는 운영체제와 적어도 하 나의 프로그램 또는 어플리케이션 코드가 저장될 수 있다. 메모리에는 학습 모델을 이용하여 입력 영상으 로부터 실종자 정보에 부합하는 객체를 검출하는 방법을 수행하기 위한 프로그램 코드가 저장될 수 있다. 또한, 메모리에는 착용하고 있는 상의 및 하의 색상이 라벨링된 사람의 이미지들이 저장되고, 이렇게 라벨 링된 사람들의 이미지들을 기초로 학습 모델을 트레이닝하기 위한 프로그램 코드가 저장될 수 있다. 통신 모듈은 네트워크에 무선으로 접속하여 단말기로부터 데이터를 수신하고 단말기에 데이 터를 송신할 수 있다. 입출력 인터페이스는 입출력 장치와의 인터페이스 수단을 제공할 수 있다. 단말기는 프로세서, 메모리, 통신 모듈, 및 입출력 장치를 포함한다. 프로세서 , 메모리, 통신 모듈, 및 입출력 장치는 버스를 통해 서로 데이터를 교환할 수 있다. 프로세서는 기본적인 산술, 로직 및 입출력 연산을 수행하고, 예컨대 메모리에 저장된 프로그램 코드 를 실행할 수 있다. 메모리는 프로세서가 판독할 수 있는 기록 매체로서, RAM, ROM 및 디스크 드라이브와 같은 비소멸성 대용량 기록장치를 포함할 수 있다. 메모리에는 운영체제와 인터넷 브라우저와 같은 프로그램 또는 어플 리케이션 코드가 저장될 수 있다. 통신 모듈은 네트워크에 접속하여 서버와 데이터를 주고 받을 수 있다. 입출력 장치는 사 용자로부터의 입력을 수신하여 프로세서에 전달하고 프로세서로부터 수신된 정보를 사용자에게 출력 할 수 있다. 입출력 장치는 입력 장치로서, 키보드, 마우스, 터치 스크린, 마이크 등을 포함할 수 있다. 입출력 장치는 출력 장치로서, 디스플레이와 같은 영상 표시 장치를 포함할 수 있다. 사용자는 입력 장치를 통해 실종자 정보를 입력할 수 있다. 실종자 정보는 실종자가 착용하고 있던 상의 및 하의 색상을 포함할 수 있다. 실종자 정보는 신발 색상을 더 포함할 수 있다. 실종자 정보에는 그 외에도 실종자 이름, 실종 위치, 실종 시각, 실종자 사진 등이 포함될 수 있다. 사용자가 입력한 실종자 정보는 예컨대 인터넷 브라우저를 통해 서버에 전송될 수 있다. 서버는 실 종자 정보를 메모리에 저장할 수 있다. 실종자 정보는 전용 프로그램을 통해 서버에 전송될 수도 있 다. 도 4는 일 실시예에 따른 무인기의 프로세서의 내부 구성을 도시한다. 도 4를 참조하면, 무인기의 프로세서는 컬러 영상 수신부, 열화상 영상 수신부, 및 입력 영상 생성부를 포함한다. 컬러 영상 수신부는 제1 카메라가 촬영 영역을 촬영하여 생성한 컬러 영상을 수신한다. 열화상 영상 수신부는 제2 카메라가 촬영 영역을 촬영하여 생성한 열화상 영상을 수신한다. 제1 카메라와 제2 카메라는 동일 영역을 동시에 촬영하며, 컬러 영상과 열화상 영상은 실질적으로 서로 매칭될 수 있다. 입력 영상 생성부는 컬러 영상과 열화상 영역을 기초로 입력 영상을 생성한다. 일 예에 따르면, 입력 영 상 생성부는 컬러 영상에서 열화상 영상의 미리 설정된 기준 온도 이하의 영역을 마스킹함으로써 입력 영 상을 생성할 수 있다. 일 예에 따르면, 입력 영상 생성부는 열화상 영상에서 미리 설정된 기준 온도를 기초로 해당 픽셀에 해당 하는 온도가 기준 온도 이하인 블랙 영역과 해당 픽셀에 해당하는 온도가 기준 온도보다 높은 영역인 화이트 영 역을 구분한 필터 영상을 생성할 수 있다. 입력 영상 생성부는 필터 영상에서 블랙 영역에 해당하는 컬러 영상의 일부분은 블랙 영역으로 마스킹되고, 필터 영상에서 화이트 영역에 해당하는 컬러 영상의 나머지 부분만 남김으로써 입력 영상을 생성할 수 있다. 일 예에 따르면, 입력 영상 생성부는 필터 영상과 컬러 영상에 대하여 AND 연산을 수행하여 입력 영상을 생성할 수 있다. 필터 영상과 컬러 영상에 대하여 AND 연산이 수행되면, 컬러 영상에서 필터 영상의 블랙 영역 에 해당하는 영역의 픽셀 값은 블랙에 해당하는 값으로 변경될 수 있다. 입력 영상 생성부는 컬러 영상 중에서 사람이 존재할 가능성이 낮은 또는 없는 영역을 블랙 영역으로 마스 킹함으로써, 입력 영상의 크기를 줄일 수 있다. 다른 예에 따르면, 입력 영상 생성부는 필터 영상의 화이트 영역을 미리 설정된 마진만큼 넓혀서 수정 필 터 영상을 생성한 후, 수정 필터 영상을 컬러 영상과 결합함으로써 입력 영상을 생성할 수도 있다. 프로세서는 위치 정보 생성부를 더 포함할 수 있다. 위치 정보 생성부는 위치측정모듈을 이용하여 무인기의 현재 위치를 나타내는 위치 정보를 생성할 수 있다. 프로세서는 입력 영상과 함 께 이 입력 영상을 촬영한 무인기의 위치를 나타내는 위치 정보를 서버에 실시간으로 전송할 수 있다. 도 5는 일 실시예에 따른 서버의 프로세서의 내부 구성을 도시한다. 도 5를 참조하면, 서버의 프로세서는 실종자 정보 수신부, 입력 영상 수신부, 객체 검색부 , 및 객체 영상 출력부를 포함한다.실종자 정보 수신부는 단말기로부터 실종자 정보를 수신한다. 실종자 정보는 실종자가 착용했던 의 상의 색상 정보를 포함할 수 있다. 예를 들면, 실종자 정보는 실종자가 착용했던 상의의 색상 및 하의의 색상 을 포함할 수 있다. 그 외에도 실종자 정보는 실종자의 이름, 나이, 신장, 성별, 사진, 실종 장소와 실종 날짜 와 시간 등의 정보를 포함할 수 있다. 입력 영상 수신부는 무인기로부터 입력 영상을 수신한다. 입력 영상은 열화상 영상을 이용하여 사람 이 존재할 수 없는 부분이 블랙으로 마스킹된 컬러 영상일 수 있다. 객체 검색부는 입력 영상 내에 상의 색상 및 하의 색상에 부합하는 객체를 검색할 수 있다. 객체 검색부 는 사전에 트레이닝된 학습 모델(213m)을 이용하여 객체를 검색할 수 있다. 학습 모델(213m)은 인간의 뇌 구조를 컴퓨터 상에서 모의하도록 설계될 수 있다. 예를 들면, 학습 모델(213m) 은 인간의 신경망의 뉴런(neuron)을 모의하는, 가중치를 가지는 네트워크 노드들을 포함할 수 있다. 네트워크 노드들은 뉴런이 시냅스(synapse)를 통하여 신호를 주고 받는 시냅틱(synaptic) 활동을 모의하도록 각각 연결 관계를 형성할 수 있다. 학습 모델(213m)은 예컨대 인공 지능 신경망 모델, 또는 신경망 모델에서 발전한 딥 러닝 네트워크 모델을 포함 할 수 있다. 학습 모델(213m)은 지도 학습(Supervised Learning)으로 실종자 정보의 상의 및 하의 색상에 부합하는 객체를 인식하도록 학습된 모델이다. 지도 학습은 알고리즘을 통해 정해진 답을 찾는 것이 목적이다. 따라서, 학습 모델(213m)은 훈련용 데이터(training data)로부터 함수를 추론해내는 형태의 모델일 수 있다. 지도 학습에서 는 트레이닝에 라벨링된 샘플(labeled sample; 목표 출력 값이 있는 데이터)을 사용한다. 지도 학습 알고리즘은 일련의 학습 데이터와 그에 상응하는 목표 출력 값을 수신하고, 입력되는 데이터에 대한 실제 출력 값과 목표 출력 값을 비교하는 학습을 통해 오류를 찾아내고, 해당 결과를 근거로 모델을 수정하게 된다. 지도 학습은 결과물의 형태에 따라 다시 회귀(Regression)와 분류(Classification)으로 나뉜다. 지도 학습 알고리즘을 통해 도출된 함수는 다시 새로운 결과값을 예측하는데 사용된다. 이처럼, 학습 모델(213m)은 수많은 학습 데이터의 학습을 통해, 학습 모델(213m)의 파라미터를 최적화하게 된다. 본 발명에 따른 학습 모델(213m)은 복수의 히든 레이어를 포함하는 다층 퍼셉트론(multilayer perceptrons) 구 조를 가질 수 있다. 퍼셉트론이란 각 뉴런의 수학적 모델(y=Wx+b)을 일컫는 용어로서, 다층 퍼셉트론은 오류- 역전파 알고리즘(Error-backpropagation algorithm), 경사하강(Gradient Decent) 기법 등을 통한 학습을 통해 예측의 정확도를 높일 수 있다. 학습 모델(213m)이 오류-역전파 알고리즘을 통해 학습을 하는 방법은 입력 레 이어에서 시작하여 출력 레이어를 통해 y 값을 얻었을 때 기준 라벨 값과 비교하여 오답일 경우 다시 출력 레이 어에서 입력 레이어 방향으로 값을 전달하며 계산된 코스트에 따라 각 W와 b 값을 업데이트 하는 방식이다. 학습 모델(213m)에 제공되는 트레이닝용 데이터 셋은 상의와 하의를 착용하고 있는 사람의 이미지일 수 있다. 사람의 이미지에는 상의와 하의 색상에 대한 라벨이 포함될 수 있다. 여기서, 라벨은 해당 객체(object)의 클 래스(class)를 나타낼 수 있다. 일 실시예에 따른 클래스는 빨강, 파랑, 노랑, 흰색, 검정색 등과 같은 상의 및 하의의 색상을 나타낼 수 있다. 한편, 학습 모델(213m)이 트레이닝 데이터를 이용한 학습 과정을 수행하고 나면 최적화된 파라미터를 가지는 모 델이 생성되며, 생성된 모델에 라벨링되지 않은 데이터를 입력하였을 때 입력된 데이터에 상응하는 결과 값(라 벨)을 예측할 수 있게 된다. 학습 모델(213m)은 특징 추출 네트워크(Feature Extraction Network) 및 복수 개의 태스크(Task)를 수행하는 멀티-태스크 네트워크(Task Network)를 포함할 수 있다. 각각의 입력(x)에 대해 라벨(y)이 달린 데이터가 학습 모델(213m)에 제공되면, 특징 추출 네트워크가 입력 데이 터의 특징을 추출해내고, 멀티-태스크 네트워크가 상의와 하의 색상에 부합하는 객체를 인식하도록 학습된다. 특징 추출 네트워크는 컨볼루션 뉴럴 네트워크(Convolutional Neural Network, \"CNN\")일 수 있다. 특징 추출 네트워크는 복잡한 입력 데이터로부터 테두리, 선 색 등과 같은 \"특징들(features)\"을 추출하기 위해 이용될 수 있다. 특징 추출 네트워크는 복수의 레이어들(layers)을 포함할 수 있다. 각각의 레이어는 데이터 를 수신할 수 있고, 해당 레이어에 입력되는 데이터를 처리하여 해당 레이어에서 출력되는 데이터를 생성할 수 있다. 레이어에서 출력되는 데이터는 특징 추출 네트워크가 입력된 영상 또는 입력된 특징맵(Feature Map)을하나 이상의 필터 또는 하나 이상의 커널(Kernel)과 컨볼루션하여 생성한 특징맵일 수 있다. 특징 추출 네트워 크의 초기 레이어들은 입력으로부터 에지들 또는 그레디언트들과 같은 낮은 레벨의 특징들을 추출하도록 동작될 수 있다. 특징 추출 네트워크의 다음 레이어들은 점진적으로 더 복잡한 특징들을 추출할 수 있다. 특징 추출 네트워크 내에서 특징맵을 입력받고 출력하는 하나 이상의 또는 모든 레이어들은 히든(hidden) 레이 어들(예를 들어, 히든 컨볼루션 레이어들)일 수 있다. 한편, 특징 추출 네트워크는 특징맵에 컨볼루션 커널을 적용하는 연산 이외에 다른 프로세싱 연산들을 수행할 수 있다. 이와 같은 다른 프로세싱 연산들의 예들은, 활 성화 함수(activation function), 풀링(pooling), 리샘플링 등의 연산들을 포함할 수 있으나, 이에 제한되지 않 는다. 특징 추출 네트워크의 입력은 무인기로부터 수신되는 입력 영상을 전처리한 전처리 영상일 수 있다. 전처 리 영상은 입력 영상에서 블랙으로 마스킹되지 않은 컬러 영역들을 각각 분리하고, 미리 설정된 해상도로 축소 또는 확대될 수 있다. 전처리 영상은 미리 설정된 가로 방향과 세로 방향의 길이를 갖는 컬러 영상일 수 있다. 특징 추출 네트워크는 전처리 영상의 특징들을 추출할 수 있다. 특징 추출 네트워크는 복수 개의 컨볼루션 레 이어들을 포함할 수 있고, 각각의 컨볼루션 레이어들은 교정 선형 유닛(Rectified Linear Unit; \"ReLU\") 레이어 와 함께 하고, 최대 풀링(Max pooling) 레이어를 포함할 수 있다. 특징 추출 네트워크에는 드롭아웃(Drop out) 레이어가 삽입될 수 있다. 특징 추출 네트워크는 전처리 영상을 입력으로 제공받아, 출력 데이터를 출력할 수 있다. 이 때, 특징 추출 네 트워크에서 출력되는 출력 데이터는 멀티-태스크 네트워크의 입력으로 제공될 수 있다. 멀티-태스크 네트워크는 복수의 태스크 모듈을 포함하며, 각각의 모듈은 상호 보완적인 태스크를 수행한다. 멀 티-태스크 네트워크는 단일 포워드 경로(single forward pass) 상에서 상의 및 하의 색상에 부합하는 객체를 검 출/분류하며, 이와 동시에 해당 객체가 사람인지를 판단하는 태스크를 수행한다. 멀티-태스크 네트워크에 의하여 학습된 모델은 복수의 태스크를 통해 복수의 요소들(예를 들어, 사람인지의 여 부, 상의 색상, 하의 색상 등)을 함께 인식함으로써, 인식의 정확도를 향상시킬 수 있다. 멀티-태스크 네트워크의 각 태스크들은 전단에 특징 추출 네트워크에 의해 출력된 중간 레벨 속성들(mid-level attributes)을 공유할 수 있다. 중간 레벨 속성들은 날씨나 조명(illumination) 등의 변화에 강인(robust)하 므로, 멀티-태스크 네트워크는 중간 레벨 속성들을 조인트 학습(jointly learning)시킴으로써 인식의 정확도를 향상시킬 수 있다. 멀티-태스크 네트워크는, 각각의 네트워크가 수행하는 태스크의 종류에 따라 멀티-라벨 분류 모듈(Multi-label classification module), 객체 마스크 모듈(Object mask module), 및 그리드 회귀 모듈(Grid regression module)로 구성될 수 있다. 그리드 회귀 모듈은 그리드 검출 태스크를 수행한다. 회귀(Regression)는 연속형 변수들에 대해 두 변수 사이 의 모형을 구한 뒤 적합도를 측정해 내는 분석 기법으로서, 모형과 주어진 데이터를 분석하는 것이다. 그리드 검출 태스크는, 검출 대상의 위치를 그리드의 조합으로 검출하는 것일 수 있다. 객체 마스크 모듈은 객체들에 대한 바이너리 분류(binary classification) 태스크를 수행한다. 객체 마스크 모 듈은 주어진 입력 값을 일정한 기준에 따라 두 가지 범주(예를 들어, '0' 또는 '1')로 분류한다. 예를 들어, 객체 마스크 모듈은 검출된 객체가 사람인지의 여부와 같은 분류 태스크를 수행할 수 있다. 멀티-라벨 분류 모듈은 클래스 분류 태스크를 수행한다. 멀티-라벨 분류 모듈은 입력을 여러 개의 카테고리로 분류할 수 있다. 멀티-라벨 분류 모듈은 검출된 객체의 상의 색상을 분류하는 분류 태스크를 수행할 수 있다. 또한, 멀티-라벨 분류 모듈은 검출된 객체의 하의 색상을 분류하는 분류 태스크를 수행할 수 있다. 멀티-라벨 분류 모듈, 객체 마스크 모듈, 및 그리드 회귀 모듈은 각각 컨볼루션 레이어들, 완전 연결 레이어들 (Fully Connected Layers), 및 손실 레이어(Loss Layer)를 포함할 수 있다. 컨볼루션 레이어들은 입력 데이터를 필터링하는 필터링 요소일 수 있다. 컨볼루션 레이어들은 컨볼루션 필터링 레이어, 풀링 레이어, 또는 이들의 조합으로 구성될 수 있다. 완전 연결 레이어들은 복수의 레이어들을 포함하고, 각 레이어들은 다수 개의 노드들로 구성될 수 있다. 또한, 완전 연결 레이어들에는 모델 정규화 알고리즘(Model regularization Algorithm)인 드랍아웃(Dropout)이 적용 될 수 있다. 드랍아웃은 미리 정해진 비율의 노드(예를 들어, 50%의 노드)가 현재 학습 에폭(epoch)에서 랜덤하게 학습에 참여하지 않는 알고리즘이다. 손실 레이어는 완전 연결 레이어들의 출력들로부터 복수의 요소들(예를 들어, 사람, 상의/하의 색상 등)을 예측 할 수 있고, 예측된 요소들과 실제 요소들을 비교함으로써 손실들을 계산할 수 있다. 손실들은 역 전파 기법을 통하여 완전 연결 레이어들 및 컨볼루션 레이어들로 역 전파될 수 있다. 역 전파된 손실들에 기초하여, 컨볼루 션 레이어들 및 완전 연결 레이어들 내 연결 가중치들이 업데이트될 수 있다. 한편, 손실을 계산하는 방법은 특정 방식에 국한되지 않는다. 예를 들어, 힌지 손실(Hinge Loss), 스퀘어 손실(Square Loss), 소프트맥스 손 실(Softmax Loss), 크로스-엔트로피 손실(Cross-entropy Loss), 절대 손실(Absolute Loss), 인센시티브 손실 (Insensitive Loss) 등이 목적에 따라 사용될 수 있다. 다른 예에 따르면, 학습 모델(213m)은 영상에서 사람을 분류하기 위한 제1 학습 모델(213m1)과 사람의 영상에서 해당 인물이 착용하고 있는 상의의 색상과 하의의 색상을 분류하기 위한 제2 학습 모델(213m2)을 포함할 수 있 다. 객체 검색부는 제1 학습 모델(213m1)을 이용하여 무인기로부터 수신되는 입력 영상에 사람이 포 함되어 있는지를 먼저 판단하고, 사람이 포함된 영상이라고 판단되면 제2 학습 모델(213m2)을 이용하여 해당 인 물이 착용하고 있는 상의 색상과 하의 색상을 판단할 수 있다. 객체 영상 출력부는 객체 검색부가 입력 영상에서 상의 및 하의 색상에 부합하는 객체를 검색하면, 해당 객체를 포함하는 부분, 즉, 객체 영상을 출력할 수 있다. 객체 영상은 단말기로 전송될 수 있다. 도 6은 일 실시예에 따라서 무인기, 서버 및 단말기를 포함하는 실종자 탐색 시스템에서 실종자 를 탐색하는 방법을 설명하기 위한 도면이다. 도 6을 참조하면, 단말기는 서버에 실종자 정보를 송신할 수 있다(S10). 그후, 무인기는 실종 자를 수색하기 위해 기동하게 되고, 제1 카메라로 촬영한 컬러 영상과 제2 카메라로 촬영한 열화상 영상을 이용하여 입력 영상을 생성하여, 서버로 송신할 수 있다(S20). 서버는 객체 검색부의 학습 모델(213m)을 이용하여 입력 영상에서 실종자 정보, 특히, 상의 및 하의 색상 정보에 부합하는 사람 객체 가 존재하는지를 판단할 수 있다(S30). 상의 및 하의 색상 정보에 부합하는 객체가 존재하면, 서버는 해 당 객체가 포함된 객체 영상을 생성하여 단말기에 전송할 수 있다(S40). 또한, 서버는 상의 및 하의 색상 정보에 부합하는 객체가 존재하면, 무인기에 확대 영상 요청과 함께 해당 위치를 전송할 수 있다(S50). 무인기는 제1 카메라의 줌 기능을 활용하여 해당 위치의 영상을 확대하여 촬영함으로써 확대 영상을 생성할 수 있으며, 이 확대 영상을 서버로 전송할 수 있다(S60). 서 버는 수신된 확대 영상을 단말기로 전달할 수 있다. 단말기의 사용자는 서버로부터 수신된 검출 영상 및/또는 확대 영상을 기초로 실종자가 맞는지 직접 확인할 수 있다. 본 발명에 따르면, 무인기가 생성한 입력 영상은 사람이 존재할 가능성이 없는 영상이 마스킹되므로 이미 지 데이터의 크기를 줄일 수 있다. 네트워크의 대역폭이 일정하다고 할 때, 이미지 데이터의 해상도가 더 높아질 수도 있다. 서버는 실종자를 검출하기 위해 탐색해야 하는 데이터 량이 감소하였으므로, 신속하게 실종자를 검출할 수 있다. 또한, 기존에는 연산량이 많은 실종자 얼굴을 매칭하였으나, 본 발명에서는 실종자가 착용하고 있는 의 상은 잘 변하지 않는다는 점을 고려하여 기존보다 연산량이 적고 정확도가 높은 상의 및 하의 색상을 기초로 실 종자를 검색하기 때문에 더욱 신속하게 실종자를 탐색할 수 있다. 이상 설명된 다양한 실시예들은 예시적이며, 서로 구별되어 독립적으로 실시되어야 하는 것은 아니다. 본 명세 서에서 설명된 실시예들은 서로 조합된 형태로 실시될 수 있다. 이상 설명된 다양한 실시예들은 컴퓨터 상에서 다양한 구성요소를 통하여 실행될 수 있는 컴퓨터 프로그램의 형 태로 구현될 수 있으며, 이와 같은 컴퓨터 프로그램은 컴퓨터로 판독 가능한 매체에 기록될 수 있다. 이때, 매 체는 컴퓨터로 실행 가능한 프로그램을 계속 저장하거나, 실행 또는 다운로드를 위해 임시 저장하는 것일 수도 있다. 또한, 매체는 단일 또는 수개 하드웨어가 결합된 형태의 다양한 기록수단 또는 저장수단일 수 있는데, 어떤 컴퓨터 시스템에 직접 접속되는 매체에 한정되지 않고, 네트워크 상에 분산 존재하는 것일 수도 있다. 매 체의 예시로는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등을 포함하여 프로그램 명령어가 저장되도록 구성된 것이 있을 수 있다. 또한, 다른 매체의 예시로,애플리케이션을 유통하는 앱 스토어나 기타 다양한 소프트웨어를 공급 내지 유통하는 사이트, 서버 등에서 관리 하는 기록매체 내지 저장매체도 들 수 있다. 본 명세서에서, \"부\", \"모듈\" 등은 프로세서 또는 회로와 같은 하드웨어 구성(hardware component), 및/또는 프 로세서와 같은 하드웨어 구성에 의해 실행되는 소프트웨어 구성(software component)일 수 있다. 예를 들면, \"부\", \"모듈\" 등은 소프트웨어 구성 요소들, 객체 지향 소프트웨어 구성 요소들, 클래스 구성 요소들 및 태스 크 구성 요소들과 같은 구성 요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로 코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레 이들 및 변수들에 의해 구현될 수 있다."}
{"patent_id": "10-2019-0148087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본 발명의 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자는 본 발명 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 발명의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으 로 해석되어야 한다."}
{"patent_id": "10-2019-0148087", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 실종자 탐색 시스템의 구성을 도시한다. 도 2는 일 실시예에 따른 무인기의 일부 내부 구성을 도시한다. 도 3은 일 실시예에 따른 서버와 단말기의 내부 구성을 도시한다. 도 4는 일 실시예에 따른 무인기의 프로세서의 내부 구성을 도시한다. 도 5는 일 실시예에 따른 서버의 프로세서의 내부 구성을 도시한다. 도 6은 일 실시예에 따라서 무인기, 서버 및 단말기를 포함하는 실종자 탐색 시스템에서 실종자를 탐색하는 방 법을 설명하기 위한 도면이다."}
