{"patent_id": "10-2024-0003712", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0006705", "출원번호": "10-2024-0003712", "발명의 명칭": "인공 신경망 모델 성능 평가 방법 및 이를 이용하는 시스템", "출원인": "주식회사 딥엑스", "발명자": "김녹원"}}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "제1 구성의 제1 신경 프로세서 및 상기 제1 구성과 다른 제2 구성의 제2 신경 프로세서를 포함하는 복수의 신경프로세서;적어도 하나의 운영 프로세서; 및상기 적어도 하나의 운영 프로세서에 사용되는 명령어를 저장하는 메모리를 포함하고,상기 적어도 하나의 운영 프로세서는,인공 신경망 모델, 제1 신경 프로세서 및 제2 신경 프로세서 중 적어도 하나를 선택하는 제1 프로세서 선택 정보, 및 컴파일 옵션 정보를 수신하고,상기 컴파일 옵션에 따라, 상기 인공 신경망 모델 컴파일함으로써, 상기 제1 프로세서 선택 정보에 의해 선택된신경 프로세서에서 사용되는 상기 인공 신경망 모델을 처리하고, 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해 적어도 하나의 평가 데이터 세트를 상기 인공 신경망 모델에 적용하고,상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서의 성능과 관련된 적어도 하나의 제1 성능 파라미터를 생성하는, 인공 신경망 모델 성능 평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 네트워크를 통해 사용자 디바이스로부터 상기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도하나의 평가 데이터 세트를 수신하고,상기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도 하나의 평가 데이터 세트를 상기 적어도하나의 운영 프로세서에 보내고,상기 적어도 하나의 운영 프로세서부터 적어도 하나의 제1 성능 파라미터를 수신하고,수신된 적어도 하나의 제1 성능 파라미터를 네트워크를 통해 상기 사용자 디바이스에 전송하는 서버를 더 포함하는, 인공 신경망 모델 성능 평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 적어도 하나의 평가 데이터 세트에는 데이터 암호화 (Data Encryption), 차등 개인 정보 보호(Differential Privacy) 및 데이터 마스킹 (Data Masking) 중 적어도 하나가 적용되는, 인공 신경망 모델 성능평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 컴파일 옵션 정보는,공개특허 10-2025-0006705-3-양자화(quantization) 알고리즘, 프루닝 (pruning) 알고리즘, 재학습(retraining) 알고리즘, 모델 압축(modelcompression) 알고리즘, 인공지능 기반 최적화(AI based model optimization) 알고리즘 및 지식 증류(knowledge distillation) 알고리즘 중 적어도 하나를 포함하는, 인공 신경망 모델 성능 평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 제1 신경 프로세서는 적어도 하나의 내부 메모리와 적어도 하나의 곱셉 및 누적 연산기를 포함하고,상기 적어도 하나의 운영 프로세서는 상기 제1 구성에 기반하여, 상기 컴파일 옵션을 설정하는, 인공 신경망 모델 성능 평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,적어도 하나의 운영 프로세서는 상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리 가능한 레이어를 결정하는, 인공 신경망 모델 성능 평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,적어도 하나의 운영 프로세서는 상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 보고하는, 인공 신경망 모델 성능 평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 처리하는 그래픽 프로세서를 더 포함하는, 인공 신경망 모델 성능 평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제9항에 있어서,상기 그래픽 프로세서는,상기 인공 신경망 모델의 재학습을 수행시키는, 인공 신경망 모델 성능 평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,적어도 하나의 제1 성능 파라미터는 온도 프로파일, 소비 전력, Trillion Operations Per Second perWatt(TOPS/W), Frame Per Second (FPS), Inference Per Second (IPS) 및 추론 및 예측 정확도(Accuracy) 중 적어도 하나를 포함하는, 인공 신경망 모델 성능 평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2025-0006705-4-제1 항에 있어서,상기 적어도 하나의 운영 프로세서는,제1 신경 프로세서 및 제2 신경 프로세서 중 적어도 다른 하나를 선택하는 제2 프로세서 선택 정보를 더 수신하고,상기 인공 신경망 모델 컴파일함으로써, 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경망 모델을 처리하고, 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해 상기 적어도 하나의 평가 데이터 세트를 상기 인공 신경망 모델에 적용하고,상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서의 성능과 관련된 적어도 하나의 제2 성능 파라미터를 생성하는, 인공 신경망 모델 성능 평가 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 적어도 하나의 운영 프로세서는,상기 적어도 하나의 제1 성능 파라미터와 상기 적어도 하나의 제2 성능 파라미터를 비교하여 제1 프로세서 선택정보 및 제2 프로세서 선택 정보 중 어느 하나를 추천하는, 인공 신경망 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항에 있어서,상기 컴파일 옵션은 Post Training Quantization(PTQ) 알고리즘, 인공 신경망 모델의 레이어별 재학습 (layerwise retraining) 알고리즘 및 양자화 인식 재학습(quantization aware training; QAT) 알고리즘 중 적어도 하나를 포함하는, 인공 신경망 모델 시스템."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "적어도 하나의 운영 프로세서가 인공 신경망 모델, 제1 신경 프로세서 및 제2 신경 프로세서 중 적어도 하나를선택하는 제1 프로세서 선택 정보, 및 컴파일 옵션 정보를 수신하는 단계,상기 적어도 하나의 운영 프로세서가 상기 컴파일 옵션에 따라, 상기 인공 신경망 모델 컴파일함으로써, 상기제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경망 모델을 처리하는 단계,상기 적어도 하나의 운영 프로세서가 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해 적어도하나의 평가 데이터 세트를 상기 인공 신경망 모델에 적용하는 단계; 및상기 적어도 하나의 운영 프로세서가 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서의 성능과 관련된 적어도 하나의 제1 성능 파라미터를 생성하여 네트워크를 통해 전송하는 단계를 포함하는, 인공 신경망 모델성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14 항에 있어서,서버가 네트워크를 통해 사용자 디바이스로부터 상기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기적어도 하나의 평가 데이터 세트를 수신하고, 상기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도 하나의 평가 데이터 세트를 상기 적어도 하나의 운영 프로세서에 보내는 단계; 및공개특허 10-2025-0006705-5-상기 서버가 상기 적어도 하나의 운영 프로세서부터 적어도 하나의 제1 성능 파라미터를 수신하고, 수신된 적어도 하나의 제1 성능 파라미터를 네트워크를 통해 상기 사용자 디바이스에 전송하는 단계를 더 포함하는, 인공신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서,상기 적어도 하나의 평가 데이터 세트에는 데이터 암호화 (Data Encryption), 차등 개인 정보 보호(Differential Privacy) 및 데이터 마스킹 (Data Masking) 중 적어도 하나가 적용되는, 인공 신경망 모델 성능평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제14항에 있어서,상기 컴파일 옵션 정보는,양자화(quantization) 알고리즘, 프루닝 (pruning) 알고리즘, 재학습(retraining) 알고리즘, 모델 압축(modelcompression) 알고리즘, 인공지능 기반 최적화(AI based model optimization) 알고리즘 및 지식 증류(knowledge distillation) 알고리즘 중 적어도 하나를 포함하는, 인공 신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제14항에 있어서,제1 신경 프로세서 및 제2 신경 프로세서 중 적어도 하나의 하드웨어 구성에 기반하여, 상기 컴파일 옵션을 설정하는 단계를 더 포함하는, 인공 신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제14항에 있어서,상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 보고하는 단계를 더 포함하는, 인공 신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서,그래픽 프로세서가 상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 처리하는 단계를 더 포함하는, 인공 신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제14항에 있어서,그래픽 프로세서가 상기 인공 신경망 모델의 재학습을 수행시키는 단계를 더 포함하는, 인공 신경망 모델 성능평가 방법.공개특허 10-2025-0006705-6-청구항 22 제14항에 있어서,적어도 하나의 제1 성능 파라미터는 온도 프로파일, 소비 전력, Trillion Operations Per Second perWatt(TOPS/W), Frame Per Second (FPS), Inference Per Second (IPS) 및 추론 및 예측 정확도(Accuracy) 중 적어도 하나를 포함하는, 인공 신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제14 항에 있어서,적어도 하나의 운영 프로세서가 제1 신경 프로세서 및 제2 신경 프로세서 중 적어도 다른 하나를 선택하는 제2프로세서 선택 정보를 수신하는 단계,상기 적어도 하나의 운영 프로세서가 상기 컴파일 옵션에 따라, 상기 인공 신경망 모델 컴파일함으로써, 상기제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경망 모델을 처리하는 단계,상기 적어도 하나의 운영 프로세서가 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해 적어도하나의 평가 데이터 세트를 상기 인공 신경망 모델에 적용하는 단계; 및상기 적어도 하나의 운영 프로세서가 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서의 성능과 관련된 적어도 하나의 제2 성능 파라미터를 생성하여 네트워크를 통해 전송하는 단계를 더 포함하는, 인공 신경망모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제23항에 있어서,상기 적어도 하나의 제1 성능 파라미터와 상기 적어도 하나의 제2 성능 파라미터를 비교하여 제1 프로세서 선택정보 및 제2 프로세서 선택 정보 중 어느 하나를 추천하는 단계를 더 포함하는, 인공 신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제14항에 있어서,상기 컴파일 옵션은 Post Training Quantization(PTQ) 알고리즘, 인공 신경망 모델의 레이어별 재학습 (layerwise retraining) 알고리즘 및 양자화 인식 재학습(quantization aware training; QAT) 알고리즘 중 적어도 하나를 포함하는, 인공 신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제1 구성의 제1 신경 프로세서 및 상기 제1 구성과 다른 제2 구성의 제2 신경 프로세서를 포함하는 복수의 신경프로세서를 선택하기 위한 옵션을 표시하는 단계;상기 복수의 신경 프로세서 중 적어도 하나를 선택하는 제1 프로세서 선택 정보를 사용자로부터 수신하는 단계;인공 신경망 모델의 컴파일과 연관된 복수의 컴파일 옵션을 표시하는 단계;사용자로부터 복수의 컴파일 옵션 중 제1 컴파일 옵션 선택 정보를 수신하는 단계;상기 제1 프로세서 선택 정보, 제1 컴파일 옵션 선택 정보, 및 적어도 하나의 평가 데이터 세트를 상기 복수의신경 프로세서에 연결된 컴퓨팅 장치로 보내는 단계;상기 제1 컴파일 옵션 선택 정보에 의해 선택된 컴파일 옵션에 따른 상기 제1 프로세서 선택 정보에 의해 선택공개특허 10-2025-0006705-7-된 신경 프로세서의 처리 성능과 관련된 적어도 하나의 제1 성능 파라미터를 수신하는 단계; 및적어도 하나의 제1 성능 파라미터를 표시하는 단계를 포함하는, 인공 신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제26항에 있어서,사용자로부터 상기 복수의 신경 프로세서 중 적어도 다른 하나를 선택하는 제2 프로세서 선택 정보를 수신하는단계;사용자로부터 복수의 컴파일 옵션 중 제2 컴파일 옵션 선택 정보를 수신하는 단계;상기 제2 프로세서 선택 정보 및 제2 컴파일 옵션 선택 정보를 상기 복수의 신경 프로세서에 연결된 컴퓨팅 장치로 보내는 단계; 및상기 제2 컴파일 옵션 선택 정보에 의해 선택된 컴파일 옵션에 따른 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서의 처리 성능과 관련된 적어도 하나의 제2 성능 파라미터를 수신하는 단계를 더 포함하는, 인공 신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "제27항에 있어서,제1 프로세서 선택 정보 및 제2 프로세서 선택 정보 중 어느 하나를 추천하고, 이를 표시하는 단계를 더 포함하는, 인공 신경망 모델 성능 평가 방법."}
{"patent_id": "10-2024-0003712", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 인공 신경망 모델 성능 평가 방법 및 평가 시스템에 관한 것으로서, 사용자가 성능 평가를 진행할 적 어도 하나의 신경 처리 장치(Neural Processing Unit; NPU)의 종류 및 개수를 선택하는 단계, 선택된 적어도 하 나의 NPU가 처리할 인공 신경망 모델(Artificial Neural Network; ANN)의 복수의 컴파일 옵션 중 적어도 하나를 선택하는 단계, 선택된 적어도 하나의 NPU가 처리할 ANN 및 적어도 하나의 특정 평가데이터세트를 업로드하는 단 계, 선택된 복수의 컴파일 옵션 중 적어도 하나에 따라 상기 ANN의 컴파일하여 처리하는 단계 및 선택된 NPU가 ANN 처리하여 성능 평가를 보고하는 단계를 포함하도록 구성된다."}
{"patent_id": "10-2024-0003712", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공 신경망 모델 성능 평가 방법 및 이를 이용하는 시스템 및 이를 이용하는 시스템에 관한 것이다."}
{"patent_id": "10-2024-0003712", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간은 인식(recognition), 분류(classification), 추론(inference), 예측(predict), 조작/의사결정 (control/decision making) 등을 할 수 있는 지능을 갖추고 있다. 인공지능(Artificial Intelligence, AI)은 인간의 지능을 인공적으로 모방하는 것을 의미한다. 인간의 뇌는 뉴런(Neuron)이라는 수많은 신경세포로 이루어져 있다. 각각의 뉴런은 시냅스(synapse)라고 불리 는 연결부위를 통해 수백 개에서 수천 개의 다른 뉴런들과 연결되어 있다. 인간의 지능을 모방하기 위하여, 생 물학적 뉴런의 동작원리와 뉴런 간의 연결 관계를 모델링한 것을, 인공신경망(Artificial Neural Network, ANN) 모델이라고 한다. 즉, 인공 신경망 모델은 뉴런들을 모방한 노드들을 레이어(layer: 계층) 구조로 연결시킨 시 스템이다."}
{"patent_id": "10-2024-0003712", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 발명자는 인공 신경망 모델을 구동하는 신경 처리 장치 (NPU: Neural Processing Unit)의 상용화를 위해서 해결해야 할 다양한 문제가 있다는 사실을 인식하였다. 첫째, 사용자가 개발한 인공 신경망 모델을 구동하기 위한 프로세서를 선택하는데 정보가 부족하다. 둘째, NPU는 이제 상용화가 막 시작한 기술로, GPU 기반으로 개발된 인공 신경망 모델이 특정 NPU에서 동작 될 지 여부를 알기 위해서는 다양한 질문지와 데이터 시트 등을 검토하고, 엔지니어의 기술지원 등이 필요한 수준이다. 특히 인공 신경망 모델은 사용자의 필요에 따라 레이어의 개수, 파라미터의 크기가 바뀔 수 있고, 특수 함수 들이 추가될 수 있기에, 인공 신경망 모델을 일반화 하기 힘들 변화폭이 다양하다. 셋째, 따라서 사용자가 개발한 인공 신경망 모델이 특정 NPU에서 구동이 될지 여부를 사전에 알기 어렵다. 즉, NPU 구매 후 평가 결과 특정 연산 또는 오퍼레이션을 지원하지 않아 구동이 불가한 문제가 발생할 수도 있다. 넷째, 사용자가 개발한 인공 신경망 모델이 특정 NPU에서 구동시 어떤 성능을 구현할지 사전에 알기 어렵다. 즉, 원하는 소비 전력과 원하는 FPS를 만족하는지 여부를 사전에 알기 어렵다. 특히 인공 신경망 모델의 가중치의 크기, 특징맵의 크기, 레이어의 개수, 활성화 함수의 특성 등이 각각의 인공 신경망 모델마다 상이하기 때문에 원하는 성능을 사전에 알기 어렵다. 이에, 본 개시의 발명자들은 AI code (e.g., TensorFlowTM, PyTorchTM, ONNXTM 모델 파일 등)를 특정 온라인 시 뮬레이션 서비스에 드롭(업로드)하면 온라인으로 사용자에게 필요한 모든 일련의 작업을 일괄적으로 수행하여, 사용자에게 최상의 편의와 가치를 제공한다는 Solution 또는 서비스를 제공함으로써, 최적의 NPU 제품 선택과 상기 선택된 NPU에서 모델 최적화 조건을 보다 빠르게 결정할 수 있도록 방법 및 장치를 구성하였다. 본 개시의 과제들은 이상에서 언급한 과제들로 제한되지 않으며, 언급되지 않은 또 다른 과제들은 아래의 기재 로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2024-0003712", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시는 인공 신경망 모델 성능 평가 시스템에 관한 것으로서, 서로 다른 연산 처리 성능을 가지는, 제1 구성 의 제1 신경 프로세서 및 상기 제1 구성과 다른 제2 구성의 제2 신경 프로세서를 포함하는 복수의 신경 프로세 서, 적어도 하나의 운영 프로세서 및 적어도 하나의 운영 프로세서에 사용되는 명령어를 저장하는 메모리를 포 함한다. 그리고, 적어도 하나의 운영 프로세서는, 인공 신경망 모델, 제1 신경 프로세서 및 제2 신경 프로세서 중 적어도 하나를 선택하는 제1 프로세서 선택 정보, 및 컴파일 옵션 정보를 수신하고, 컴파일 옵션에 따라, 상 기 인공 신경망 모델 컴파일함으로써, 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경망 모델을 처리하고, 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해 적어도 하나 의 평가 데이터 세트를 상기 인공 신경망 모델에 적용하고, 제1 프로세서 선택 정보에 의해 선택된 신경 프로세 서의 성능과 관련된 적어도 하나의 제1 성능 파라미터를 생성할 수 있다. 본 개시의 다른 특징에 따르면, 인공 신경망 모델 성능 평가 시스템은 네트워크를 통해 사용자 디바이스로부터 상기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도 하나의 평가 데이터 세트를 수신하고, 상 기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도 하나의 평가 데이터 세트를 상기 적어도 하 나의 운영 프로세서에 보내고, 상기 적어도 하나의 운영 프로세서부터 적어도 하나의 제1 성능 파라미터를 수신 하고, 수신된 적어도 하나의 제1 성능 파라미터를 네트워크를 통해 상기 사용자 디바이스에 전송하는 서버를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 적어도 하나의 평가 데이터 세트에는 데이터 암호화 (Data Encryption), 차등 개인 정보 보호 (Differential Privacy) 및 데이터 마스킹 (Data Masking) 중 적어도 하나 가 적용될 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 컴파일 옵션 정보는, 양자화(quantization) 알고리즘, 프루닝 (pruning) 알고리즘, 재학습(retraining) 알고리즘, 모델 압축(model compression) 알고리즘, 인공지능 기반 최적화(AI based model optimization) 알고리즘 및 지식 증류 (knowledge distillation) 알고리즘 중 적어도 하나를 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 제1 신경 프로세서는 적어도 하나의 내부 메모리와 적어도 하나의 곱셉 및 누적 연산기를 포함하고, 적어도 하나의 운영 프로세서는 상기 제1 구성에 기반하여, 상기 컴파일 옵션을 설 정할 수 있다. 본 개시의 또 다른 특징에 따르면, 적어도 하나의 운영 프로세서는 상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리 가능한 레이어를 결정할 수 있다. 본 개시의 또 다른 특징에 따르면, 적어도 하나의 운영 프로세서는 상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 보고할 수있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 시스템은 상기 인공 신경망 모델의 복수의 레이 어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 처리하는 그래픽 프로세서를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 그래픽 프로세서는 인공 신경망 모델의 재학습을 수행시킬 수 있다. 본 개시의 또 다른 특징에 따르면, 적어도 하나의 제1 성능 파라미터는 온도 프로파일, 소비 전력, Trillion Operations Per Second per Watt(TOPS/W), Frame Per Second (FPS), Inference Per Second (IPS) 및 추론 및 예측 정확도(Accuracy) 중 적어도 하나를 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 적어도 하나의 운영 프로세서는, 제1 신경 프로세서 및 제2 신경 프로 세서 중 적어도 다른 하나를 선택하는 제2 프로세서 선택 정보를 더 수신하고, 상기 인공 신경망 모델 컴파일함 으로써, 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경망 모델을 처 리하고, 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해 상기 적어도 하나의 평가 데이터 세 트를 상기 인공 신경망 모델에 적용하고, 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서의 성능과 관련된 적어도 하나의 제2 성능 파라미터를 생성할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 적어도 하나의 운영 프로세서는, 적어도 하나의 제1 성능 파라미터와 상기 적어도 하나의 제2 성능 파라미터를 비교하여 제1 프로세서 선택 정보 및 제2 프로세서 선택 정보 중 어느 하나를 추천할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 컴파일 옵션은 Post Training Quantization(PTQ) 알고리즘, 인공 신경 망 모델의 레이어별 재학습 (layer wise retraining) 알고리즘 및 양자화 인식 재학습(quantization aware training; QAT) 알고리즘 중 적어도 하나를 포함할 수 있다. 본 개시는 인공 신경망 모델 성능 평가 방법에 관한 것으로서, 적어도 하나의 운영 프로세서가 인공 신경망 모 델, 제1 신경 프로세서 및 제2 신경 프로세서 중 적어도 하나를 선택하는 제1 프로세서 선택 정보, 및 컴파일 옵션 정보를 수신하는 단계, 상기 적어도 하나의 운영 프로세서가 상기 컴파일 옵션에 따라, 상기 인공 신경망 모델 컴파일함으로써, 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경 망 모델을 처리하는 단계, 상기 적어도 하나의 운영 프로세서가 상기 제1 프로세서 선택 정보에 의해 선택된 신 경 프로세서에 의해 적어도 하나의 평가 데이터 세트를 상기 인공 신경망 모델에 적용하는 단계; 및 상기 적어 도 하나의 운영 프로세서가 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서의 성능과 관련된 적어도 하나의 제1 성능 파라미터를 생성하여 네트워크를 통해 전송하는 단계를 포함할 수 있다. 본 개시의 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 서버가 네트워크를 통해 사용자 디바이스로 부터 상기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도 하나의 평가 데이터 세트를 수신하 고, 상기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도 하나의 평가 데이터 세트를 상기 적 어도 하나의 운영 프로세서에 보내는 단계; 및 상기 서버가 상기 적어도 하나의 운영 프로세서부터 적어도 하나 의 제1 성능 파라미터를 수신하고, 수신된 적어도 하나의 제1 성능 파라미터를 네트워크를 통해 상기 사용자 디 바이스에 전송하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 제1 신경 프로세서 및 제2 신경 프로세 서 중 적어도 하나의 하드웨어 구성에 기반하여, 상기 컴파일 옵션을 설정하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 보고하는 단 계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 그래픽 프로세서가 상기 인공 신경망 모 델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 처리하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 그래픽 프로세서가 상기 인공 신경망 모 델의 재학습을 수행시키는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 적어도 하나의 운영 프로세서가 제1 신 경 프로세서 및 제2 신경 프로세서 중 적어도 다른 하나를 선택하는 제2 프로세서 선택 정보를 수신하는 단계, 상기 적어도 하나의 운영 프로세서가 상기 컴파일 옵션에 따라, 상기 인공 신경망 모델 컴파일함으로써, 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경망 모델을 처리하는 단계, 상기 적어도 하나의 운영 프로세서가 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해 적어도 하나의 평가 데이터 세트를 상기 인공 신경망 모델에 적용하는 단계; 및 상기 적어도 하나의 운영 프로세서가 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서의 성능과 관련된 적어도 하나의 제2 성능 파라미터 를 생성하여 네트워크를 통해 전송하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 상기 적어도 하나의 제1 성능 파라미터 와 상기 적어도 하나의 제2 성능 파라미터를 비교하여 제1 프로세서 선택 정보 및 제2 프로세서 선택 정보 중 어느 하나를 추천하는 단계를 더 포함할 수 있다. 본 개시는 인공 신경망 모델 성능 평가 방법에 관한 것으로서, 제1 구성의 제1 신경 프로세서 및 상기 제1 구성 과 다른 제2 구성의 제2 신경 프로세서를 포함하는 복수의 신경 프로세서를 선택하기 위한 옵션을 표시하는 단 계, 상기 복수의 신경 프로세서 중 적어도 하나를 선택하는 제1 프로세서 선택 정보를 사용자로부터 수신하는 단계, 인공 신경망 모델의 컴파일과 연관된 복수의 컴파일 옵션을 표시하는 단계, 사용자로부터 복수의 컴파일 옵션 중 제1 컴파일 옵션 선택 정보를 수신하는 단계, 상기 제1 프로세서 선택 정보, 제1 컴파일 옵션 선택 정 보, 및 적어도 하나의 평가 데이터 세트를 상기 복수의 신경 프로세서에 연결된 컴퓨팅 장치로 보내는 단계, 상 기 제1 컴파일 옵션 선택 정보에 의해 선택된 컴파일 옵션에 따른 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서의 처리 성능과 관련된 적어도 하나의 제1 성능 파라미터를 수신하는 단계; 및 적어도 하나의 제1 성능 파라미터를 표시하는 단계를 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 사용자로부터 상기 복수의 신경 프로세 서 중 적어도 다른 하나를 선택하는 제2 프로세서 선택 정보를 수신하는 단계; 사용자로부터 복수의 컴파일 옵 션 중 제2 컴파일 옵션 선택 정보를 수신하는 단계; 상기 제2 프로세서 선택 정보 및 제2 컴파일 옵션 선택 정 보를 상기 복수의 신경 프로세서에 연결된 컴퓨팅 장치로 보내는 단계; 및 상기 제2 컴파일 옵션 선택 정보에 의해 선택된 컴파일 옵션에 따른 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서의 처리 성능과 관 련된 적어도 하나의 제2 성능 파라미터를 수신하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 제1 프로세서 선택 정보 및 제2 프로세 서 선택 정보 중 어느 하나를 추천하고, 이를 표시하는 단계를 더 포함할 수 있다. 기타 실시예의 구체적인 사항들은 상세한 설명 및 도면들에 포함되어 있다."}
{"patent_id": "10-2024-0003712", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 일 예시에 따르면, 사용자는 특정 NPU를 구매하기 전에 사전에 사용자의 인공 신경망 모델이 어떤 NPU에서 동작 가능한지 여부를 빠르게 알 수 있다. 본 개시의 일 예시에 따르면, 사용자는 특정 NPU를 구매하기 전에 사전에 사용자의 인공 신경망 모델이 특정 NPU에서 구동시 어떠한 성능으로 동작할지 여부를 빠르게 알 수 있다. 본 개시의 일 예시에 따르면, 각각의 NPU가 종류별로 서버를 통해서 연결될 경우, 사용자는 온라인으로 사용자 의 인공 신경망 모델을 각각 평가하여, 각 NPU마다 구동 여부를 온라인으로 평가하고 결과를 받을 수 있다. 본 개시의 다른 예시에 따르면, 사용자의 인공 신경망 데이터는 다양한 방식으로 보호될 수 있으므로, NPU의 성 능 평가시에 발생될 수 있는 보안 이슈를 해결할 수 있다. 본 개시에 따른 효과는 이상에서 예시된 내용에 의해 제한되지 않으며, 더욱 다양한 효과들이 본 개시 내에 포 함되어 있다."}
{"patent_id": "10-2024-0003712", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 예시 들을 참조하면 명확해질 것이다. 그러나, 본 개시는 이하에서 개시되는 예시들에 한정되는 것이 아니라 서로 다"}
{"patent_id": "10-2024-0003712", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "른 다양한 형태로 구현될 것이며, 단지 본 예시들은 본 개시가 완전하도록 하며, 본 개시가 속하는 기술분야에 서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 개시는 청구항의 범 주에 의해 정의될 뿐이다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조부호가 사용될 수 있다. 본 개시에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. 본 개시에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\" 등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 개시에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\" 등의 표현들은 다양한 구성요소들을, 순서 및/또는 중 요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들을 한정하지 않는다. 예를 들면, 제1 사용자 기기와 제2 사용자 기기는, 순서 또는 중요도와 무관하게, 서로 다른 사용자 기기를 나타낼 수 있다. 예를 들면, 본 개시에 기재된 권리범위를 벗어나지 않으면서 제1 구성요소는 제 2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 바꾸어 명명될 수 있다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요 소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제1 구성요소)가 다른 구성 요소(예: 제2 구성요소)에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 상기 어떤 구성요 소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 개시에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for),\" \"~하는 능력을 가지는(having the capacity to),\" \"~하도록 설계된(designed to),\" \"~하도 록 변경된(adapted to),\" \"~하도록 만들어진(made to),\" 또는 \"~ 를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것 만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장치 가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행하 도록 구성된(또는 설정된)프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메모리 장치에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다.본 개시에서 사용된 용어들은 단지 특정한 예시를 설명하기 위해 사용된 것으로, 다른 예시의 범위를 한정하려 는 의도가 아닐 수 있다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있"}
{"patent_id": "10-2024-0003712", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 개시에 기재된 기술분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 개시사용된 용어들 중 일반 적인 사전에 정의된 용어들은, 관련 기술의 문맥상 가지는 의미와 동일 또는 유사한 의미로 해석될 수 있으며, 본 개시에서 명백하게 정의되지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 경우에 따 라서, 본 개시에서 정의된 용어일지라도 본 개시의 예시들을 배제하도록 해석될 수 없다. 본 개시의 여러 예시들의 각각 특징들이 부분적으로 또는 전체적으로 서로 결합 또는 조합 가능하며, 당업자가 충분히 이해할 수 있듯이 기술적으로 다양한 연동 및 구동이 가능하며, 각 예시들이 서로에 대하여 독립적으로 실시 가능할 수도 있고 연관 관계로 함께 실시 가능할 수도 있다. 이하, 첨부한 도면을 참조하여 본 개시의 바람직한 예시들을 설명함으로써, 본 개시를 상세히 설명한다. 도 1은 본 개시의 일 예시에 따른 인공 신경망 모델 성능 평가 시스템의 구성을 나타낸 블록도이다. 도 1을 참조하면, 본 개시의 일 예시에 따른 인공 신경망 모델 성능 평가 시스템은 사용자 디바이스 , 인공 신경망 모델 처리 장치 및 서버를 포함할 수 있다. 도 1의 본 개시의 일 예시에 따른 인공 신경망 모델 성능 평가 시스템은 인공 신경망 모델 처리 장치 에서 특정 인공 신경망 모델을 처리하여, 인공 신경망 모델 처리 장치의 성능 평가 결과를 사용자에 게 온라인으로 제공하도록 구성될 수 있다. 사용자 디바이스는 인공 신경망 모델을 처리할 인공 신경망 모델 처리 장치의 성능 평가 결과정보를 획득하고자 하는 사용자가 소지한 디바이스일 수 있다. 사용자 디바이스는 서버에 접속하여, 인공 신 경망 모델과 관련된 정보를 확인하기 위한 사용자 인터페이스를 제공할 수 있는 스마트폰, 태블릿 PC, PC, 랩탑 등을 포함할 수 있다. 사용자 디바이스가 서버에 접속하는 방법으로는 웹 서비스를 통한 접속 방법, FTP 서버를 통한 접속 방법, 클라우드 서버를 통한 접속 방법, 혹은 사용자 디바이스 내부의 어플리케이션을 통한 접속 방법이 있을 수 있다. 다만, 사용자 디바이스가 서버에 접속하는 방법은 이에 한정되지 않고 다양한 공지된 통신 기술을 활용할 수 있다. 사용자는 다양한 통신 기술을 이용하여 인공 신경망 모델에 대한 정보를 서버로 송신할 수 있다. 구체적으 로, 사용자는 구매 의향이 있는 신경 처리 장치의 성능 평가를 위한 적어도 하나의 특정 인공 신경망 모델 및 상기 인공 신경망 모델의 적어도 하나의 특정 평가데이터세트를 사용자 디바이스를 통해 서버에 업로 드 한다. 상술한 특정 평가데이터세트는 인공 신경망 모델 처리 장치의 성능 평가를 위해 인공 신경망 모델 처리 장 치에 입력되는 데이터세트를 의미한다. 사용자 디바이스는 인공 신경망 모델 처리 장치로부터 인공 신경망 모델에 대한 인공 신경망 모델 처 리 장치의 성능 평가 결과를 수신하고, 이를 출력할 수 있다. 예를 들면, 사용자 디바이스는 인공 신경망 모델 성능 평가 시스템에 평가하고자 하는 인공 신경망 모델에 대한 정보를 서버에 업로드 할 수 있는 어떠한 종류의 단말기일 수 있다. 예를 들면, 사용자 디바이스는 인공 신경망 모델 성능 평가 시스템에 상기 인공 신경망 모델을 평가 하기 위한 평가데이터세트를 업로드 할 수 있는 어떠한 종류의 단말기일 수 있다. 예를 들면, 사용자 디바이스는 인공 신경망 모델 성능 평가 시스템에 상기 인공 신경망 모델을 재학 습하기 위한 학습 데이터세트를 업로드 할 수 있는 어떠한 종류의 단말기일 수 있다. 즉, 사용자 디바이스는 인공 신경망 모델의 성능 평가용 데이터 전송 유닛 또는 인공 신경망 모델의 성능 평가 결과 수신 유닛으로 지칭될 수 있다. 이를 위해, 사용자 디바이스는 프로세서, 디스플레이 장치, 사용자 인터페이스, 네트워크 인터페이스 및 메모리를 포함할 수 있다. 디스플레이 장치는 하나 이상의 NPU를 선택하기 위한 옵션을 표시할 수 있다. 또한, 디스플레이 장치는 인공 신경망 모델을 컴파일하기 위한 옵션도 표시한다.메모리는 프로세서가 서버에 접근할 수 있도록 실행 가능한 소프트웨어 모듈을 저장할 수 있고, 또한 서버를 통해 인공 신경망 모델 처리 장치로 전송하기 위한 인공 신경망 모델 및 성능 평가 데이 터 세트를 저장할 수 있다. 사용자 인터페이스는 키보드와 마우스를 포함하며, 사용자가 인공 신경망 모델 을 처리하기 위해 하나 이상의 신경 처리 장치를 선택하는 것과 인공 신경망 모델 컴파일과 관련된 컴파일 옵션 을 선택하는 것과 관련된 사용자 입력을 제공할 수 있다. 네트워크 인터페이스는 사용자 디바이스가 네트워크를 통해 서버와 통신할 수 있게 하는 하드웨어 구성요소(예를 들어, 네트워크 인터페이스 카드)이 다. 인공 신경망 모델 처리 장치는 서버를 통해 사용자 디바이스에서 수신된 인공 신경망 모델을 처 리하기 위한 신경 처리 장치를 포함한다. 인공 신경망 모델 처리 장치는 또한 인공 신경망 모델을 컴파일 하여 평가할 수 있다. 처리된 인공 신경망 모델의 성능을 파악하고, 그 성능 결과를 서버를 통해 사용자 디바이스에 보고한다. 인공 신경망 모델 처리 장치는 신경 처리 장치에 대한 정보를 결정하기 위한 다양한 프로그램을 수행하는 범용 컴퓨터, 랩탑, 클라우드 컴퓨터, 클라우드 서버 등으로 구성된 시스템을 포함할 수 있다. 인공 신경망 모 델 처리 장치는 서버로부터 신경 처리 장치의 성능 평가를 위한 적어도 하나의 특정 인공 신경망 모 델 및 인공 신경망 모델에 입력되는 적어도 하나의 특정 평가데이터세트를 획득하고, 인공 신경망 모델을 컴파 일하여 처리한 뒤 성능 평가 결과를 제공할 수 있다. 서버는 인공 신경망 모델 처리 장치에 대한 액세스를 관리하기 위해 사용자 디바이스와 통신하 는 컴퓨팅 장치이다. 서버는 프로세서, 네트워크 인터페이스 및 메모리를 포함할 수 있다. 네트워크 인터페이스는 서버가 네트워크를 통해 사용자 디바이스 및 인공 신경망 모델 처리 장 치와 통신할 수 있게 해준다. 메모리는 다음 작업 중 하나 이상을 수행하기 위해 프로세서에 의 해 실행 가능한 명령을 저장한다: (i) 사용자에 대한 계정을 관리하고, (ii) 사용자가 하나 이상의 신경 처리 장치를 평가하기 위해 인공 신경망 모델 처리 디바이스에 액세스하도록 인증하고 허용하고. (iii) 인공 신 경망 모델, 평가 데이터 세트, 평가할 신경 처리 장치에 대한 사용자 선택 및 컴파일 선택에 대한 사용자 선택 을 수신하고, (iv) 사용자로부터 수신된 데이터를 암호화 및 저장하고, (v) 인공 신경망 모델 및 사용자의 프로 세서 선택 정보를 네트워크를 통해 인공 신경망 모델 처리 디바이스에 전달하고, (vi) 선택된 신경 처리 장치에 대한 성능 보고서 및 신경 처리 장치에 대한 추천을 네트워크를 통해 사용자 디바이스에 전달한다. 서버는 그 밖에도 다양한 서비스를 수행할 수 있다. 부연 설명하면, 사용자가 개발한 인공 신경망 모델, 학습 데이터 세트, 평가 데이터 세트 등은 사용자의 지적 재산에 해당하므로 철저한 보안이 요구된다. 이하 사용자가 개발한 인공 신경망 모델, 학습 데이터 세트, 평가 데이터 세트 등은 사용자 데이터로 지칭될 수 있다. 따라서, 성능 평가 시스템 에 업로드 되는 사용자 데 이터의 보안을 위하여, 성능 평가 시스템 은 사용자 계정 로그인, 데이터 자체를 보호하는 데이터 암호화 (Data Encryption), 차등 개인 정보 보호 (Differential Privacy) 및 데이터 마스킹 (Data Masking)을 수행할 뿐만 아니라, 모델의 접근 제어 및 감사를 위하여, 접근 제어 (Access Control) 및 감사 로깅 (Audit Loggin g)을 수행하도록 구성될 수 있다. 데이터 암호화 (Data Encryption)는 사용자 데이터를 암호화하여 데이터의 기밀성을 보호한다. 차등 개인 정보 보호 (Differential Privacy) 사용자 데이터가 개인 정보를 포함한 데이터에 민감하게 반응하지 않도록 하는 통 계적 기법을 사용한다. 데이터 마스킹 (Data Masking)은 민감한 정보를 숨기기 위해 사용자 데이터의 일부를 가 려서 보호한다. 또한, 접근 제어 (Access Control)를 통해 사용자 데이터에 액세스할 수 있는 계정들을 제한하고 감사 로그를 통해 어느 계정을 통해 사용자 데이터에 접근했는지를 기록하고, 감사 로깅 (Audit Logging)을 통해 시스템 및 사용자 데이터 접근에 대한 로그를 유지하여 누가 언제 모델에 액세스했는지 추적하고 비정상적인 활동을 탐지 할 수 있다. 추가로, 학습 데이터 세트 및/또는 평가 데이터 세트 업로드 시 별도의 사용자 데이터 보호에 관한 약관에 서명 하는 단계도 더 수행될 수 있다. 따라서 사용자의 인공 신경망 모델, 학습 데이터 세트 및/또는 평가 데이터 세 트가 보호될 수 있다. 이하에서는 도 2를 참조하여, 인공 신경망 모델 처리 장치에 대하여 설명하도록 한다. 도 2는 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 구성을 나타낸 블록도이다. 도 2를 참조하면, 인공 신경망 모델 처리 장치는 중앙 처리 장치(a central processing unit ; CPU, 214), 복수의 신경 처리 장치(Neural Processing Unit Farm; NPU, 220)을 포함하는 NPU 농장(NPU FARM, 218), 그래픽 처리 장치(Graphic Processing Unit; GPU, 230), 메모리(MEMORY, 250)를 포함할 수 있으며, 각 구성은 하나 이상의 통신 버스 또는 신호 라인을 통해 서로 통신할 수 있다. 인공 신경망 모델 처리 장치는 특정 운영 시스템(operating system; OS)에 의해서 동작할 수 있다. 예를 들면, OS는 Microsoft Windows, MacOS, Linux (예를 들면, Ubuntu, Fedora, Debian, CentOS, Arch Linux), Unix, iOS, Android, 등이 사용될 수 있다. CPU는 메모리에 저장된 명령어를 실행하기 위한 하나 이상의 운영 프로세서를 포함할 수 있다. 메모 리는 컴파일러, 저장 모듈 및 리포팅 프로그램을 포함하지만 이에 제한되지 않는 다양한 소프트웨어 모듈을 저장할 수 있다. 한편, 메모리는 각종 데이터, 명령 및 정보를 저장할 수 있는 휘발성 또는 비휘발성 기록 매체를 포함할 수 있다. 예를 들어, 메모리는 플래시 메모리 타입, 하드디스크 타입, 멀티미디어 카드 마이크로 타입, 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램, SRAM, 롬, EEPROM, PROM, 네트워크 저장 스토리지, 클라우드, 블 록체인 데이터베이스 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 컴파일러는 특정 인공 신경망 모델을 복수의 신경 처리 장치가 실행할 수 있는 머신코드로 변환할 수 있다. 즉, 컴파일러는 서로 다른 특성을 가지는 복수의 신경 처리 장치에서 실행될 수 있는 머신코드 를 각각 생성할 수 있다. 이에 따라, 컴파일러는 복수의 신경 처리 장치 중 선택된 신경 처리 장치 에서 실행될 머신코드를 생성할 수 있다. 머신코드는 이진화코드로 지칭되는 것도 가능하다. 컴파일러는 성능 평가를 위해 선택된 적어도 하나의 신경 처리 장치의 성능을 평가할 인공 신경망 모 델의 머신코드를 생성할 수 있다. 컴파일러는 다양한 컴파일 옵션을 제공하도록 구성될 수 있다. 다양한 컴파일 옵션의 선택을 위해 사용자 디바이스의 화면에 UI로 컴파일 옵션이 제공될 수 있다. 컴파일러는 성능 평가를 위해 선택된 신경 처리 장치 별로 복수의 컴파일 옵션을 달리 설정하여, 최적화된 인공 신경망 모델의 머신코드를 생성할 수 있다. 복수의 신경 처리 장치의 종류에 따라 복수의 컴파일 옵션은 달라질 수 있으므로, 동일한 인공 신경망 모 델이라도 컴파일된 머신 코드는 복수의 신경 처리 장치의 종류에 따라 달리질 수 있다. 즉, 각각의 선택된 컴파일 옵션 별로 각각의 머신코드가 생성될 수 있다. 저장 모듈은 인공 신경망 모델 처리 장치에서 사용되는 다양한 데이터를 저장할 수 있다. 즉, 저장 모듈는 머신 코드의 형태로 컴파일된 인공 신경망 모델, 하나 이상의 트레이닝 데이터 세트, 하나 이상의 평가 데이터 세트, 성능 평가 결과 및 복수의 신경 처리 장치로부터의 출력 데이터를 저장할 수 있다. 리포팅 프로그램은 컴파일 된 인공 신경망 모델을 처리하여 상기 성능 평가 결과를 보고할 수 있다. 즉, 리포팅 프로그램은 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가능한지 여부를 먼저 판단한다. 만약, 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가 불가능한 경우, 리포팅 프로 그램은 인공 신경망 모델의 복수의 레이어 중 복수의 신경 처리 장치에 의해서 처리가 불가능한 특정 레이어 또는 처리가 불가능한 특정 오퍼레이션을 보고할 수 있다. 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치 중 특정 신경 처리 장치에 의해서 실행이 가능한 경 우, 리포팅 프로그램은 복수의 신경 처리 장치의 처리 성능을 보고할 수 있다. 처리 성능의 지표는 신경 처리 장치의 온도 프로파일, 소비 전력(Watt), Trillion Operations Per Second per Watt(TOPS/W), Frame Per Second (FPS), Inference Per Second (IPS) 및 정확도(Accuracy) 등 일 수 있다. 온도 프로파일은 신경 처리 장치가 동작할 때 시간에 따라 측정되는 신경 처리 장치의 온도 변화 데이터를 의미 한다. 소비 전력은 신경 처리 장치가 동작할 때 측정된 파워 데이터를 의미한다. 소비 전력은 사용자가 개발한 인공 신경망 모델의 연산량 특성에 따라 달라지기 때문에, 사용자의 인공 신경망 모델이 제공되어야 정확한 파워가측정될 수 있다. Trillion Operations Per Second per Watt (TOPS/W)는 AI 하드웨어의 효율성을 측정하는 지표로서, 1와트당 1 초 동안 수행할 수 있는 연산의 수를 의미한다. TOPS/W는 하드웨어가 소비하는 전력 당 얼마나 많은 연산을 수행할 수 있는지를 나타내므로, 복수의 신경 처리 장치의 에너지 효율성을 의미하는 지표이다. Inference Per Second (IPS)는 복수의 신경 처리 장치가 1초 동안 수행할 수 있는 추론 연산의 수를 나타 내는 지표로서, 복수의 신경 처리 장치의 연산 처리 속도를 의미한다. IPS는 Frame Per Second (FPS)로 지 칭되는 것도 가능하다. 정확도(Accuracy)는 전체 중에서 올바르게 예측한 샘플의 비율을 나타내는 지표로서, 복수의 신경 처리 장치 의 추론정확도를 의미한다. 부연 설명하면, 복수의 신경 처리 장치의 정확도와 그래픽 처리 장치 의 추론정확도는 서로 다를 수 있다. 이러한 이유는 그래픽 처리 장치에서 추론되는 인공 신경망 모 델의 파라미터는 부동소수점(floating point)일 수 있고 복수의 신경 처리 장치에서 추론되는 인공 신경망 모델의 파라미터는 정수(integer)일수 있다. 또한 다양한 최적화 알고리즘이 선택적으로 적용될 수 있다. 따라 서, 복수의 신경 처리 장치에서 추론되는 인공 신경망 모델의 파라미터는 다양한 연산에서 계산된 값의 차 이를 가질 수 있으며, 이에 그래픽 처리 장치에서 추론되는 인공 신경망 모델과 서로 다른 추론 정확도를 가질 수 있다. 추론 정확도 차이는 인공 신경망 모델의 구조 및 파라미터 크기 특성에 따라 달라질 수 있으며, 특히 양자화된 파라미터의 비트폭(bitwidth)의 길이가 짧아질수록 과도한 양자화에 따른 추론정확도 열화가 커 지는 경향이 있다. 예를 들면, 양자화되는 비트폭은 2bit 내지 16bit일 수 있다. 또한 과도한 프루닝에 따른 추 론정확도 열화가 커지는 경향 또한 있다. 한편, 리포팅 프로그램은 복수의 컴파일 옵션 각각에 따라 컴파일된 인공 신경망 모델의 처리 성능을 분석 하여, 상기 복수의 컴파일 옵션 중 어느 하나를 역으로 제안할 수 있다. 또한, 리포팅 프로그램은 또한 서로 다른 신경 처리 장치의 성능 매개변수에 기초하여 특정 유형의 신경 처리 장치를 추천할 수도 있습니다. 복수의 신경 처리 장치는 특정 회사가 판매 중인 성능과 가격이 각각 다른 다양한 NPU 제품군으로 구성된 NPU 농장(NPU farm, 218); 형태 일 수 있다. NPU 농장은 온라인으로 사용자가 개발한 인공 신경망 모델의 성능 평가를 수행하기 위해 제공될 수 있다. NPU 농장은 클라우드 NPU 형태로 제공될 수 있다. 복수의 신경 처리 장치는 평가데이터세트를 입력 받아, 이를 컴파일된 인공 신경망 모델에 입력하여 성능 평가를 수행할 수 있다. 복수의 신경 처리 장치 다양한 종류의 신경 처리 장치를 포함할 수 있다. 보다 구체적으로, 복수의 신경 처리 장치는 연산량을 기준으로 분류될 수 있다. 예를 들면, 제1 신경 처리 장치는 스마트 CCTV용 신경 처리 장치일 수 있다. 제1 신경 처리 장치는 초저전력, 낮은 수준의 추론 처리 성능 (예를 들면, 5 TOPS의 처리 성능), 매우 작은 반도체 패키지 크기, 매우 낮은 가격 특성을 가질 수 있다. 제1 신경 처리 장치는 성능제한 때문에, 특정 연산을 포함하고, 높은 메모리 대역폭을 요 구하는 특정 인공 신경망 모델을 지원하지 않을 수 있다. 예를 들면, 제1 신경 처리 장치는 모델명이 “”일 수 있고, “”신경 처리 장치는 ResNet, Mobilenet v1/v2, SSD, YOLOv5, YOLOv7 등의 인공 신경망 모델을 연산할 수 있다. 예를 들면, 제2 신경 처리 장치는 로봇의 영상 인식, 객체 감지, 객체 추적용 신경 처리 장치일 수 있다. 제2 신경 처리 장치는 저전력, 중간 수준의 추론 처리 성능 (예를 들면, 16 TOPS의 처리 성능), 작은 반도체 패키지 크기, 낮은 가격 특성을 가질 수 있다. 제2 신경 처리 장치는 높은 메모리 대역폭을 요구하는 특정 인공 신경망 모델을 지원하지 않을 수 있다. 예를 들면, 제2 신경 처리 장치는 모델명이 “”일 수 있고, “”신경 처리 장 치는 ResNet, Mobilenet v1/v2, SSD, YOLOv5, YOLOv7 등의 인공 신경망 모델을 연산할 수 있다. 예를 들면, 제3 신경 처리 장치는 자율주행 차량의 영상 인식, 객체 감지, 객체 추적, 생성형 AI 서비스용 신경 처리 장치일 수 있다. 제3 신경 처리 장치는 저전력, 높은 수준의 추론 처리 성능 (예를 들면, 25 TOPS의 처리 성능), 중간 반도체 패키지 크기, 중간 가격 특성을 가질 수 있다. 예를 들면, 제3 신경 처리 장치는 모델명이 “” 일 수 있고, “”신경 처리 장치는 ResNet, MobileNet v1/v2/v3, SSD, EfficientNet, EfficientDet,YOLOv5, YOLOv7, YOLOv8, DeepLabv3, PIDNet, ViT, Generative adversarial network, Stable diffusion등의 인공 신경망 모델을 연산할 수 있다. 예를 들면, 제4 신경 처리 장치는 CCTV 통제실, 관제 센터, 대형 언어 모델, 생성형 AI 서비스용 신경 처리 장 치일 수 있다. 제4 신경 처리 장치는 저전력, 높은 수준의 추론 처리 성능 (예를 들면, 400 TOPS의 처리 성능), 대형 반도체 패키지 크기, 높은 가격 특성을 가질 수 있다. 예를 들면, 제4 신경 처리 장치는 모델명이 “”일 수 있고, “”신경 처리 장치는 ResNet, Mobilenet v1/v2, SSD, YOLOv5, YOLOv7, YOLOv8, DeepLabv3, PIDNet, ViT, Generative adversarial network, Stable diffusion, 대형 LLM 등의 인공 신경망 모델을 연산할 수 있다. 즉, 각각의 신경 처리 장치는 서로 다른 연산처리능력, 서로 다른 반도체 칩 사이즈, 서로 다른 소비 전력 특성 등을 가질 수 있다. 다만, 복수의 신경 처리 장치의 종류는 이에 한정되지 않고, 다양한 분류 기준으로 구분될 수 있다. 그래픽 처리 장치(GPU; 230)은 그래픽과 이미지 처리뿐만 아니라, 복잡한 연산 작업을 수행하는 하드웨어이다. 그래픽 처리 장치는 하나로 도면에 도시되었지만, 이에 제한되지 않으며, 클라우드 GPU, NVLink, NVSwitch 등으로 연결된 복수의 그래픽 처리 장치일 수 있다. 그래픽 처리 장치는 복수의 코어를 포함하고, 복수의 코어를 통해 여러 작업을 병렬적으로 처리할 수 있다. 따라서, 그래픽 처리 장치는 과학 계산, 딥 러닝과 같은 대규모 데이터 처리 작업을 수행할 수 있다. 구체적으로, 그래픽 처리 장치는 대규모 데이터세트에서 딥 러닝 및 머신 러닝 모델을 훈련하는데 사용될 수 있다. 딥 러닝 모델은 많은 수의 파라미터를 가지고 있어 훈련이 시간이 많이 소요되는데, 그래픽 처리 장치 는 이러한 연산을 병렬로 처리하여 훈련 속도를 향상시킬 수 있다. 사용자가 복수의 신경 처리 장치 에서 특정 신경 처리 장치를 선택하고, 다양한 컴파일 옵션을 통해 인공 신경망 모델의 재학습을 수행할 경우, 그래픽 처리 장치에서 유용 가능한 그래픽 처리 장치를 선택하고, 선택된 그래픽 처리 장치는 각각의 컴파 일 옵션에 따른 인공 신경망 모델의 재학습을 수행할 수 있다. 복수의 신경 처리 장치와 그래픽 처리 장치는 다양한 연산 장치가 통합된 SoC(System on Chip)와 같 은 통합 칩(Integrated Chip (IC)), 통합 칩이 실장된 인쇄 회로 기판의 형태로 구현될 수 있다. 도 3은 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 컴파일러의 구성을 나타낸 블록도이다. 도 3을 참조하면, 인공 신경망 모델 처리 장치의 컴파일러은 복수의 컴파일 옵션에 기반하여 인공 신경망 모델을 머신코드로 컴파일 할 수 있다. 인공 신경망 모델 처리 장치의 컴파일러 는 최적화 모듈(Optimizer, 211), 검증 모듈(Verifier, 212), 코드 생성 모듈(Code Generator, 213)를 포함할 수 있다. 컴파일러는 복수의 신경 처리 장치 중 선택된 신경 처리 장치의 구조 데이터를 제공받을 수 있다. 신 경 처리 장치의 구조 데이터는 NPU 내부 메모리의 메모리 크기, NPU 내부 메모리의 계층 구조, 프로세싱 엘리먼 트들의 개수 정보, 특수 연산 회로 유닛의 정보 등을 포함할 수 있다. 컴파일러는 신경 처리 장치의 구조 데이터와 컴파일할 인공 신경망 모델의 그래프 정보를 기초로 각 레이어 별 프로세싱 순서를 결정할 수 있다. 최적화 모듈은 방향성 비순환 그래프(DAG)로 표현된 인공 신경망 모델을 복수의 신경 처리 장치 중 선택된 신경 처리 장치에 맞게 최적화하는 업무를 수행할 수 있다. 사용자는 사용자 디바이스를 통해서 최 적화 모듈에서 제공하는 다양한 최적화 옵션 중 적어도 하나를 온라인에서 선택할 수 있다. 예를 들어, 최적화 모듈은 특정 비트폭의 정수 파라미터로 변환하는 옵션을 제공할 수 있다. 특정 비트폭 은 2bit 내지 16bit 사이 중 하나의 비트폭일 수 있다. 이에, 최적화 모듈은 부동 소수점 파라미터 기반 의 인공 신경망 모델을 정수 파라미터로 기반의 인공 신경망 모델로 변환할 수 있다. 이때 복수의 신경 처리 장 치는 정수 파라미터를 처리하도록 설계된다. 예를 들어, 최적화 모듈은 비선형 삼각함수 연산 기반의 인공 신경망 모델을 조각별 선형 함수 근사화 (piecewise linear function approximation) 기반의 인공 신경망 모델로 변환할 수 있다. 이때 복수의 신경 처리 장치는 조각별 선형 함수 근사화 연산을 처리하도록 설계 될 수 있다. 예를 들어, 최적화 모듈은 다양한 최적화 알고리즘들을 적용하여 인공 신경망 모델의 가중치, 특징맵 등의 파라미터의 크기를 저감할 수 있다. 예를 들어, 최적화 모듈은 다양한 재학습 알고리즘을 통해 최적화한 인공 신경망 모델의 정확도 열화 문제를 개선시킬 수 있다. 검증 모듈은 복수의 신경 처리 장치에서 고객의 인공 신경망 모델이 실행 가능한지 여부를 판단하는 검증 기능을 수행할 수 있다. 검증 모듈은 최적화된 인공 신경망 모델의 구조를 분석하고, 각 레이어 별 연산자가 복수의 신경 처리 장치에서 하드웨어적으로 지원되는지를 검토하여 인공 신경망 모델의 실행 가 능 여부를 결정한다. 만약 실행이 불가능한 경우, 별도의 에러 리포트 파일을 생성하여 사용자에게 보고할 수 있다. 코드 생성 모듈은 검증 모듈에서 동작 가능하다고 판단된 인공 신경망 모델을 최적화 모듈로 통 해서 최적화하고, 최적화된 인공 신경망 모델을 복수의 신경 처리 장치 중 선택된 신경 처리 장치에서 실 행되는 머신코드를 각각 생성할 수 있다. 생성된 머신코드는 대응되는 신경 처리 장치에 제공되어 복수의 신경 처리 장치의 성능 평가를 수행할 수 있게 된다. 예를 들면, 복수의 신경 처리 장치중 제1 신경 처리 장치를 위해서 제1 인공 신경망 모델에 대응되는 제1 머신코드가 생성될 수 있다. 복수의 신경 처리 장치중 제2 신경 처리 장치를 위해서 제1 인공 신경망 모델 에 대응되는 제2 머신코드가 생성될 수 있다. 복수의 신경 처리 장치중 제3 신경 처리 장치를 위해서 제1 인공 신경망 모델에 대응되는 제3 머신코드가 생성될 수 있다. 복수의 신경 처리 장치중 제4 신경 처리 장 치를 위해서 제1 인공 신경망 모델에 대응되는 제4 머신코드가 생성될 수 있다. 도 4는 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 최적화 모듈의 구성을 나타낸 블록도이다. 최적화 모듈은 복수의 컴파일 옵션에 기반하여 인공 신경망 모델을 최적화시킬 수 있다. 보다 구체적으로, 최적화 모듈은 신경 처리 장치의 하드웨어 정보에 기반하여, 컴파일 옵션을 설정할 수 있다. 그리고, 최적화 모듈은 인공 신경망 모델의 파라미터의 특성(예를 들면, 가중치의 크기, 특징맵의 크기 등) 및 추론 정확도 열화 특성을 고려하여, 상기 복수의 컴파일 옵션을 설정할 수 있다. 최적화 모듈이 설정하는 복수의 컴파일 옵션은 양자화(quantization) 옵션, 프루닝 (pruning) 옵션, 재학 습(retraining) 옵션, 모델 압축(model compression) 옵션, 인공지능 기반 최적화(AI based model optimization) 옵션 및 지식 증류 (knowledge distillation) 옵션 중 적어도 하나일 수 있다. 프루닝 (pruning) 옵션은 인공 신경망 모델의 연산량을 저감할 수 있는 기술을 제공할 수 있다. 프루닝 알고리 즘은 인공 신경망 모델의 모든 레이어들의 가중치 중에서 0에 근접한 작은 값들을 0으로 치환하도록 구성될 수 있다. 복수의 신경 처리 장치는 0인 가중치와 관련된 곱셈 연산을 스킵(skip)하여 합성곱의 연산 속도 향 상 및 소비 전력을 저감할 수 있으며, 프루닝 옵션이 적용된 인공 신경망 모델의 머신코드의 파라미터 크기를 줄일 수 있다. 부연 설명하면, 프루닝 옵션에 의해서 특정 가중치 파라미터가 0으로 치환된 경우, 프루닝 알고 리즘은 해당 가중치 데이터를 가지는 인공 신경망 모델의 연결망을 단선 시키는 것과 실질적으로 동일한 효과를 제공할 수 있다. 예를 들면, 프루닝 옵션은 크기가 작은 가중치를 제거하는 크기 기반의 제1 프루닝 옵션과 가 장 작은 가중치의 특정 비율을 제거하는 백분율 기반의 제2 프루닝 옵션이 제공될 수 있다. 양자화(quantization) 옵션은 인공 신경망 모델의 파라미터의 크기를 저감할 수 있는 기술을 제공할 수 있다. 양자화 알고리즘은 인공 신경망 모델 각각의 레이어의 가중치 및 특징맵의 비트 수를 선택적으로 저감하도록 구 성될 수 있다. 양자화 옵션에 의해서 특정 특징맵 및 특정 가중치의 비트 수가 저감될 경우, 양자화 옵션이 적 용된 인공 신경망 모델의 머신코드의 파라미터 크기를 줄일 수 있다. 예를 들어, 양자화 옵션이 적용되면, 소수 점 (Floating point)을 표현하는 32bit 파라미터를 정수 (Integer)를 표현하는 2bit 내지 16bit의 비트폭의 파 라미터로 변환할 수 있다. 모델 압축 (model compression) 옵션은 인공 신경망 모델의 가중치 파라미터나, 특징 맵 파라미터 등을 압축하 는 기술을 제공할 수 있다. 모델 압축 기술은 종래의 알려진 압축 기술들을 활용하여 실시될 수 있다. 따라서 모델 압축 옵션이 적용된 인공 신경망 모델의 머신코드의 파라미터 크기를 줄일 수 있다. 모델 압축 옵션은 복 수의 신경 처리 장치 중 압축 해제 디코더(decoder)를 포함하는 신경 처리 장치에 선택적으로 제공될 수 있다. 지식 증류 (knowledge distillation) 옵션은 복잡한 모델(선생님 모델)에서 얻은 지식을 더 작고 단순한 모델 (학생 모델)로 전송하는 기술을 제공할 수 있다. 지식 증류 (knowledge distillation) 알고리즘에서 통상적으로 선생님 모델은 학생 모델에 비해서 파라미터 사이즈가 더 크고 더 높은 정확도를 가진다. 예를 들어, 후술될 재학습 옵션에서 소수점 (Floating point) 32bit 파라미터로 학습된 인공 신경망 모델이 선생님 모델로 설정하고, 다양한 최적화 옵션을 적용한 인공 신경망 모델을 학생 훈련 모델로 설정하는 지식 증류 옵션으로 학생 모델의 정확도를 향상시킬 수 있다. 여기서 학생 모델은 프루닝 옵션, 양자화 옵션, 모델 압축 옵션, 및 재학습 옵션 중 적어도 하나의 옵션이 선택된 모델일 수 있다. 재학습 (retraining) 옵션은 다양한 최적화 옵션 적용시 저하되는 추론 정확도를 보상할 수 있는 기술이다. 예 를 들면 양자화 옵션, 프루닝 옵션, 모델 압축 옵션 적용 시, 복수의 신경 처리 장치에서 추론되는 인공 신경망 모델의 정확도가 저하될 수 있다. 이러한 경우 프루닝, 양자화, 및/또는 모델 압축된 인공 신경망 모델 을 온라인상에서 재학습 시킬 수 있는 옵션이 제공될 수 있다. 재학습이 완료된 인공 신경망 모델의 추론 정확 도는 다시 상승할 수 있다. 구체적으로, 재학습(retraining) 옵션은 양자화 인식 재학습(quantization aware retraining) 옵션, 프루닝 인 식 재학습(pruning aware retraining) 옵션 및 전이 학습 (transfer learning) 옵션 등을 포함할 수 있다. 양자화 인식 재학습(quantization-aware retraining; QAT) 옵션은 양자화를 인공 신경망 모델의 학습 단계에 통합하여 모델이 양자화 에러를 반영하여 가중치를 미세 조정하는 옵션이다. 양자화 인식 재학습 알고리즘은 손 실 함수, 기울기 계산 및 최적화 알고리즘 수정이 포함될 수 있다. 양자화 인식 재학습 옵션은 인공신경망 학습 된 인공 신경망 모델을 양자화한 뒤, 양자화에 따른 손실을 최소화하는 방향으로 재학습을 하는 미세 튜닝 (fine-tunning)을 수행하여 양자화 에러를 보상할 수 있다. 프루닝 인식 재학습(pruning-aware retraining; PAT) 옵션은 학습된 인공 신경망 모델에서 덜 중요한 가중치를 식별하고 제거한다음 남아있는 가중치를 미세 조정하는 옵션이다. 프루닝 기준은 가중치 크기, 활성화 값, 민감 도 분석 등이 활용될 수 있다. 프루닝 인식 재학습 옵션은 인공 신경망 모델의 크기 축소, 추론 속도 향상, 재 학습시 과맞춤(overfitting) 문제점 개선 등의 문제를 보상할 수 있다. 전이 학습 (transfer learning) 옵션은 인공 신경망 모델이 한 작업에서 학습한 지식을 다른 관련 작업으로 전 송하여 학습하는 옵션을 의미한다. 전이 학습 (transfer learning) 알고리즘은 초기에 충분한 데이터가 없거나 인공 신경망 모델을 처음부터 학습하는 데 많은 계산 리소스가 필요한 경우에 효과적이다. 이에 한정되지 않고, 최적화 모듈은 인공지능 기반 최적화 옵션을 제공할 수 있다. 인공지능 기반 최적화 알고리즘은 컴파일 옵션의 다양한 알고리즘들을 인공지능 강화 학습 방식으로 인공 신경망 모델의 구조를 탐색 하는 과정을 거쳐 최적으로 경량화 된 인공 신경망 모델을 생성하는 방법 또는 양자화 알고리즘, 프루닝 알고리 즘, 재학습 알고리즘 모델 압축 알고리즘, 및 모델 압축 알고리즘과 같은 경량화 방법에 기초하지 않고, 최적화 모듈 내부의 인공지능이 자체적으로 경량화 프로세스를 진행하여 최적의 경량화 결과를 얻는 방법이다. 도 5a는 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 복수의 신경 처리 장치 및 컴파일 옵션을 선 택하기 위한 인터페이스를 나타내는 블록도이다. 사용자 인터페이스는 사용자가 사용자 디바이스를 사용하여 서버에 액세스한 후에 사용자 디바이스 의 디스플레이 장치에 표시될 수 있다. 디스플레이 장치는 NPU 선택 섹션과 컴파일 옵션 섹션의 두 섹션을 표시한다. 사용자는 하나 이 상의 평가 데이터 세트를 사용하여 인공신경망 모델에 대한 시뮬레이션을 실행하기 위해 NPU 선택 섹션에 서 하나 이상의 NPU를 선택할 수 있다. NPU 선택 섹션에는 DX-M1, DX-H1, DX-V1 및 DX-V2의 네 가지 유형 의 NPU가 표시되고, 사용자는 성능 평가를 위해 온라인 시뮬레이션에 사용할 NPU를 선택할 수 있다. 도 5A에서 는 테스트 및 평가를 위해 하나의 DX-M1이 선택되는 경우를 예로 표시하였다. 컴파일 옵션 섹션은 사용자의 컴파일 선택 선택을 용이하게 하기 위해 미리 설정된 옵션을 표시한다. 도 5a를 참조하면, 컴파일 옵션부는 제1 프리셋 옵션, 제2 프리셋 옵션, 및 제3 프리셋 옵션을 제공된다. 단 본 개시는 상기 프리셋 옵션에 제한되지 않으며, 더 다양한 프리셋 옵션들이 제공될 수 있다. 이하 설명의 편의를 위해서 양자화 알고리즘의 프리셋을 예시로 설명한다. 각각의 프리셋 옵션은 특정 관점에서 가장 효과적인 양자화 프리셋 옵션일 수 있으나, 본 개시는 이에 제한되지 않는다. 사용자는 각각의 프리셋 옵 션의 특징을 고려하여, 적어도 하나의 프리셋 옵션을 선택할 수 있다. 예를 들면, 제1 프리셋 옵션은 학습된 인공 신경망모델의 소수점 (Floating point)을 표현하는 32bit 데이터를 정수 (Integer)를 표현하는 8bit 데이터로 변환하는 양자화 알고리즘(quantization)만을 수행하는 옵션이다. 단 본 개시는 8bit에 제한되지 않으며, 2bit 내지 16bit 사이의 비트폭을 가지도록 설정될 수 있으며, 비트폭은 선택된 신경 처리 장치의 하드웨어 구성에 따라 특정 비트폭으로 제한될 수 있다. 즉, 제1 프리셋 옵션은 인공 신 경망 모델의 학습 이후에 실행되는 양자화 알고리즘(quantization)이므로, 이를 Post Training Quantization(PTQ)로 지칭할 수 있다. 제1 프리셋 옵션은 통상적으로 몇 분내로 양자화가 완료되기 때문에, 빠 르게 양자화를 수행할 수 있는 장점이 있다. 따라서, 복수의 신경 처리 장치 중 사용자가 선택한 신경 처 리 장치에서 사용자가 제공하는 인공 신경망 모델의 소비전력, 연산 처리 속도 등에 대한 결과를 빠르게 확인할 수 있는 장점이 있다. 예를 들어, 제1 양자화 옵션으로 구성된 제1 프리셋 옵션은 DXNN Lite라는 옵션으로 사용자에게 제공될 수 있다. 여기서 제1 양자화 옵션의 경우 재학습이 불필요하기 때문에 인공 신경망 모델의 재학습 단계를 수행하지 않을 수 있다. 예를 들면, 제2 프리셋 옵션은 인공 신경망모델의 소수점 (Floating point)을 표현하는 32bit 데이터를 정수 (Integer)를 표현하는 8bit 데이터로 변환하는 양자화 알고리즘(quantization)을 수행한 이후에 인공 신경망 모 델의 레이어별 재학습 (layer wise retraining)을 알고리즘을 수행하는 옵션이다. 단 본 개시는 8bit에 제한되 지 않으며, 2bit 내지 16bit 사이의 비트폭을 가지도록 설정될 수 있으며, 비트폭은 선택된 신경 처리 장치의 하드웨어 구성에 따라 특정 비트폭으로 제한될 수 있다. 즉, 제2 프리셋 옵션은 제1 프리셋 옵션을 수행한 인공 신경망 모델을 입력모델로 하여 레이어별 재학습 알고리 즘을 더 수행하도록 구성된 옵션일 수 있다. 따라서, 제2 프리셋 옵션은 양자화 알고리즘과 최적화 모듈에 서 제공되는 다양한 재학습 옵션 중 하나의 알고리즘이 조합된 옵션일 수 있다. 즉, 제2 프리셋 옵션에서 인공 신경망 모델의 복수의 레이어 중 일부의 레이어에 해당하는 데이터를 양자화 한 뒤, 이의 양자화 손실 함수를 계산한다. 이후, 인공 신경망 모델의 복수의 레이어 중 다른 일부의 레이어에 해 당하는 데이터를 양자화 한 뒤, 이의 양자화 손실 함수를 계산한다. 상술한 연산을 반복하여, 양자화 손실 함수 가 최소화되는 인공 신경망 모델의 복수의 레이어 중 일부의 레이어만 양자화를 수행하여 양자화 알고리즘 (quantization)을 최적화시킨다. 제2 프리셋 옵션의 경우, 레이어별 특징맵의 소수점 데이터(예를 들면, floating point 32)와 정수 데이터(예를 들면, integer 8)의 차이 값을 최소화 하는 방향으로 재학습을 수행할 수 있기 때문에 학습 데이터세트가 없더라도 재학습이 가능한 장점이 있다. 제2 프리셋 옵션은 통상적으로 몇시 간 내에 양자화가 완료되기 때문에, 적절한 시간내로 양자화를 수행할 수 있는 장점이 있다. 따라서, 복수의 신 경 처리 장치 중 사용자가 선택한 신경 처리 장치에서 사용자가 제공하는 인공 신경망 모델의 정확도를 제 1 프리셋 옵션 대비 상대적으로 더 향상시킬 수 있는 장점이 있다. 예를 들어, 제2 양자화 옵션으로 구성된 제2 프리셋 옵션은 DXNN pro라는 서비스 명으로 사용자에게 제공될 수 있다. 여기서 제2 양자화 옵션의 경우 인공 신경망 모델의 레이어별 재학습 알고리즘을 수행하기 때문에 인공 신경망 모델의 재학습 단계가 필요할 수 있다. 예를 들면, 제3 프리셋 옵션은 인공 신경망모델의 소수점 (Floating point)을 표현하는 32bit 데이터를 정수 (Integer)를 표현하는 8bit 데이터로 변환하는 양자화 알고리즘(quantization)을 수행한 이후에 양자화 인식 재 학습(quantization aware training; QAT) 알고리즘을 수행하는 옵션이다. 단 본 개시는 8bit에 제한되지 않으 며, 2bit 내지 16bit 사이의 비트폭을 가지도록 설정될 수 있으며, 비트폭은 선택된 신경 처리 장치의 하드웨어 구성에 따라 특정 비트폭으로 제한될 수 있다. 즉, 제3 프리셋 옵션은 제1 프리셋 옵션을 수행한 인공 신경망 모델을 입력모델로 하여 양자화 인식 재학습 알 고리즘을 더 수행하도록 구성된 옵션일 수 있다. 따라서, 제3 프리셋 옵션은 양자화 알고리즘과 최적화 모듈 에서 제공되는 다양한 재학습 옵션 중 하나의 알고리즘이 조합된 옵션일 수 있다. 즉, 제3 프리셋 옵션에서 양자화 인식 재학습 알고리즘은 학습된 인공 신경망 모델을 양자화한 뒤, 양자화에 따 른 추론 정확도 열화를 최소화하는 방향으로 재학습하여 미세 튜닝(fine-tunning)을 수행한다. 다만, 양자화에 따른 추론 정확도 열화를 최소화하는 방향으로 재학습을 위해서, 사용자가 인공 신경망 모델의 학습 데이터 세 트를 제공해야 한다. 또한 재학습시 과맞춤(overfitting)을 예방하기 위해서 평가 데이터 세트가 더 요구될 수 있다. 구체적으로, 양자화 인식 재학습 알고리즘은 양자화된 인공 신경망 모델의 머신코드 및 학습 데이터 세트 를 복수의 신경 처리 장치 중 대응되는 신경 처리 장치에 입력하여 양자화 에러에 따른 추론 정확도 열화 를 보상하도록 재학습한다. 제3 프리셋 옵션은 제1 내지 제2 프리셋 옵션보다 상대적으로 더 높은 추론 정확도 를 보장하는 장점이 있으나, 몇일 내에 양자화가 완료될 수 있기 때문에, 정확도가 중요한 경우일 때 수행될 수 있다. 예를 들어, 제3 양자화 옵션으로 구성된 제3 프리셋 옵션은 DXNN master라는 서비스 명으로 사용자에게 제공될 수 있다. 여기서 제3 양자화 옵션의 경우 인공 신경망 모델의 추론 정확도를 기준으로 재학습 알고리즘을 수행 하기 때문에 인공 신경망 모델의 재학습 단계가 필요할 수 있다. 제3 양자화 옵션의 양자화 인식 재학습 알고리 즘의 경우 양자화에 따른 손실을 최소화하는 방향으로 재학습하는 과정에서, 사용자로부터 인공 신경망 모델의 학습 데이터세트 및/또는 평가 데이터세트가 필요하다. 여기서 학습 데이터세트는 양자화 인식 재학습을 위해서 필요한 필수 데이터이다. 평가 데이터세트는 재학습시 과맞춤(overfitting)을 문제를 개선하기 위해서 선택적으 로 사용될 수 있는 데이터이다. 도 5b는 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 복수의 신경 처리 장치에 대한 성능 평가 및 제안을 위한 인터페이스를 나타내는 블록도이다. 도 5b에서 서로 다른 두 종류의 NPU를 이용하여 시뮬레이션 및 평가를 수행한 결과가 표시되어 있다. 왼쪽 위 상자는 DX-M1 NPU를 사용한 결과를 나타내고, 위 상자는 DX-H1 NPU를 사용한 결과를 표기한다. 하단 상자에는 서로 다른 두 NPU의 성능 매개변수를 기반으로 권장되는 NPU 선택이 표시될 수 있다. 도 6a 내지 도 6d는 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 하나의 신경 처리 장치의 구성을 나타낸 블록도이다. 구체적으로, 도 6a에서는 복수의 신경 처리 장치 중 제1 신경 처리 장치의 내부 구성에 대해서 도시하였다. 그 리고, 도 6b에서는 복수의 신경 처리 장치 중 제2 신경 처리 장치의 내부 구성에 대해서 도시하였다. 그리고, 도 6c에서는 복수의 신경 처리 장치 중 제3 신경 처리 장치의 내부 구성에 대해서 도시하였다. 그리고, 도 6d에 서는 복수의 신경 처리 장치 중 제3 신경 처리 장치의 내부 구성에 대해서 도시하였다. 도 6a에 도시된 제1 신경 처리 장치는 프로세싱 엘리먼트 어레이, NPU 내부 메모리 및 NPU 컨트 롤러를 포함하도록 구성된다. 예를 들면, 제1 신경 처리 장치는 프로세싱 엘리먼트 어레이, 프로세싱 엘리먼트 어레이에서 추 론될 수 있는 인공 신경망 모델을 저장 또는 인공 신경망 모델의 적어도 일부 데이터를 저장하도록 구성된 NPU 내부 메모리, 및 프로세싱 엘리먼트 어레이 및 NPU 내부 메모리을 제어하도록 구성된 NPU 컨트 롤러를 포함하도록 구성될 수 있다. 여기서, 인공 신경망 모델은 다양한 최적화 옵션이 적용되어 컴파일된 머신코드일 수 있다. NPU 컨트롤러는 제1 신경 처리 장치의 추론 연산을 위한 프로세싱 엘리먼트 어레이의 연산 및 NPU 내부 메모리의 읽기 및 쓰기 순서를 제어하도록 구성된다. NPU 컨트롤러는 머신코드에 의해서 프로세싱 엘리먼트 어레이 및 NPU 내부 메모리을 제어하도록 구성될 수 있다. NPU 컨트롤러는 머신 코드에 정의된 연산 스케줄링에 따라 프로세싱 엘리먼트 어레이 및 NPU 내부 메모리를 각 연산 단계별로 제어하도록 구성된다. 이에 신경 처리 장치는 인공 신경망 모델의 구조에 따라서 각 레이어 별 연산을 순차적으로 처리할 수 있다. 여기서 NPU 컨트롤러는 인공 신 경망 모델의 특징맵 및 가중치가 저장된 메모리 어드레스를 획득하거나 저장될 메모리 어드레스를 결정할 수 있 다. 프로세싱 엘리먼트 어레이는 복수의 프로세싱 엘리먼트들(PE1 to PE12)이 어레이 형태로 배치된 구성을 의 미한다. 각각의 프로세싱 엘리먼트는 MAC (multiply and accumulate) 연산기 및/또는 ALU (Arithmetic Logic Unit) 연산기를 포함하도록 구성될 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 도 6a에서는 예시적으로 복수의 프로세싱 엘리먼트들이 도시되었지만, 하나의 프로세싱 엘리먼트 내부에 MAC을 대체하여, 복수의 곱셈기(multiplier) 및 가산기 트리(adder tree)로 구현된 연산기들이 병렬로 배치되어 구성 되는 것도 가능하다. 이러한 경우, 프로세싱 엘리먼트 어레이는 복수의 연산기를 포함하는 적어도 하나의 프로세싱 엘리먼트로 지칭되는 것도 가능하다. 프로세싱 엘리먼트 어레이는 복수의 프로세싱 엘리먼트들(PE1 to PE12)을 포함하도록 구성된다. 도 6a에 도시된 복수의 프로세싱 엘리먼트들(PE1 to PE12)은 단지 설명의 편의를 위한 예시이며, 복수의 프로세싱 엘리 먼트들(PE1 to PE12)의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12)의 개수에 의해서 프 로세싱 엘리먼트 어레이의 크기 또는 개수가 결정될 수 있다. 프로세싱 엘리먼트 어레이의 크기는 N x M 매트릭스 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 프로세싱 엘리먼트 어레이는 N x M 개의 프로세싱 엘리먼트를 포함할 수 있다. 즉, 프로세싱 엘리먼트는 1개 이상일 수 있다.프로세싱 엘리먼트 어레이의 크기는 인공 신경망 모델의 특성을 고려하여 설계할 수 있다. 부연 설명하면, 프로세싱 엘리먼트의 개수는 작동할 인공 신경망 모델의 데이터 크기, 요구되는 동작 속도, 요구되는 소비 전력 등을 고려하여 결정될 수 있다. 인공 신경망 모델의 데이터 크기는 인공 신경망 모델의 레이어 수와 각각의 레 이어의 가중치 데이터 크기에 대응되어 크기가 결정될 수 있다. 따라서, 본 개시의 일 예시에 따른 제1 신경 처리 장치의 프로세싱 엘리먼트 어레이의 크기는 제한되 지 않는다. 프로세싱 엘리먼트 어레이의 프로세싱 엘리먼트들의 개수가 증가할수록 작동하는 인공 신경망 모델의 병렬 연산 능력이 증가되나, 제조 비용 및 물리적인 크기가 증가될 수 있다. 예를 들면, 도 6b에 도시된 바와 같이, 제2 신경 처리 장치(220-1)는 2개의 프로세싱 엘리먼트 어레이(221-1, 221-2)를 포함할 수 있다. 2개의 프로세싱 엘리먼트 어레이(221-1, 221-2) 각각은 복수의 프로세싱 엘리먼트들 (PE1 to PE12)을 포함하여 그룹화될 수 있다. 다른 예로는, 도 6c에 도시된 바와 같이, 제3 신경 처리 장치(220-2)는 4개의 프로세싱 엘리먼트 어레이(221-1, 221-2, 221-3, 221-4)를 포함할 수 있다. 4개의 프로세싱 엘리먼트 어레이(221-1, 221-2, 221-3, 221-4) 각각 은 복수의 프로세싱 엘리먼트들(PE1 to PE12)을 포함하여 그룹화될 수 있다. 또 다른 예로는, 도 6d에 도시된 바와 같이, 제4 신경 처리 장치(220-3)는 8개의 제1 신경 처리 장치를 포 함할 수 있다. 8개의 제1 신경 처리 장치 각각은 인공 신경망 모델의 연산을 분할하여 처리한다. 이에 따라, 제4 신경 처 리 장치(220-3)의 처리 속도가 더욱 향상될 수 있다. 따라서, 제4 신경 처리 장치(220-3)는 8개의 신경 처리 장치 각각의 동작을 할당하는 제어부를 더 포함할 수 있 다. 제1 내지 제4 신경 처리 장치의 특성 및 처리 모델에 대한 설명은 전술한 바와 같다. 도 7은 본 개시의 일 예시에 따른 복수의 신경 처리 장치의 구성을 나타낸 블록도이다. 복수의 신경 처리 장치는 복수의 종류의 신경 처리 장치를 포함할 수 있다. 그리고, 같은 종류의 신경 처 리 장치는 적어도 한 개 이상 배치될 수 있다. 예를 들어, “”신경 처리 장치는 복수 개가 배치되어, 제1 그룹(G1)을 형성할 수 있고, “”신경 처리 장치는 복수 개가 배치되어, 제2 그룹(G2)을 형성할 수 있고, “”신경 처리 장치는 복수 개가 배치되어, 제3 그룹(G 3)을 형성할 수 있고, “”신경 처리 장치는 복수 개가 배치되어, 제4 그룹(G4)을 형성할 수 있다. 복수의 종류 의 신경 처리 장치들의 그룹들을 NPU FARM으로 지칭할 수 있다. NPU FARM은 온라인으로 접속한 다수의 사용자 들의 성능 평가 요청에 실시간 대응 가능하도록 구성된 클라우드 형태의 NPU 시스템일 수 있다. 그리고, 제1 내지 제4 그룹(G1 내지 G4)에 포함된 복수의 신경 처리 장치는 사용자의 선택에 따라 모두 성 능 평가에 사용될 수 있고, 일부만 성능 평가에 사용될 수 있다. 보안이 필요한 사용자 데이터는 서버에 저장되거나 또는 인공 신경망 모델 처리 장치의 저장 모듈 에 저장될 수 있다. 연산에 사용되는 적어도 하나의 신경 처리 장치는 서버와 통신하여 신경 처리 장치의 성능 평가를 위 한 적어도 하나의 특정 인공 신경망 모델 및 인공 신경망 모델에 입력되는 적어도 하나의 특정 평가데이터세트 를 입력 받을 수 있다. 즉, 신경 처리 장치는 성능 평가에 필요한 사용자 데이터를 입력 받을 수 있다. 이하에서는, 상술한 인공 신경망 모델 성능 평가 시스템을 이용하여, 인공 신경망 모델 성능을 평가하는 방법들 에 대해서 설명한다. 도 8은 본 개시의 일 예시에 따른 인공 신경망 모델 성능 평가 방법을 설명하기 위한 흐름도이다. 도 8을 참조하면, 인공 신경망 모델 성능 평가 방법(S100)은 신경 처리 장치 종류 선택 단계(S110); 컴파일 옵 션 선택 단계(S120); 인공 신경망 모델 업로드 단계(S130); 인공 신경망 모델 컴파일 및 처리 단계(S140); 및 인공 신경망 모델 처리 결과 보고 단계(S150)를 포함한다. 신경 처리 장치 종류 선택 단계(S110)에서는 사용자가 성능 평가를 진행할 신경 처리 장치의 종류를 선택한다. 신경 처리 장치의 종류는 특정 회사에서 판매하는 신경 처리 장치의 제품군에 따라 달라질 수 있다. 도 7에 도시된 바와 같이, “”신경 처리 장치는 복수 개가 배치되어, 제1 그룹(G1)을 형성할 수 있고, “”신 경 처리 장치는 복수 개가 배치되어, 제2 그룹(G2)을 형성할 수 있고, “”신경 처리 장치는 복수 개가 배치되 어, 제3 그룹(G3)을 형성할 수 있고, “”신경 처리 장치는 복수 개가 배치되어, 제4 그룹(G4)을 형성할 수 있 다. 사용자는 “”신경 처리 장치, “”신경 처리 장치, “”신경 처리 장치 및 “”신경 처리 장치 중 성능 평가를 진행할 신경 처리 장치를 선택한다. 그리고, 서버는 선택된 특정 종류의 신경 처리 장치들 중 NPU FARM내에서 가용 가능한 신경 처리 장치를 적어도 하나 이상 선택할 수 있다. 이때 선택되는 특정 종류의 신경 처리 장치는 하나의 종류에 한정되지 않고 복수개의 종류일 수 있다. 이후, 컴파일 옵션 선택 단계(S120)에서는 선택된 적어도 하나의 신경 처리 장치가 처리할 인공 신경망 모델의 복수의 컴파일 옵션 중 적어도 하나를 선택한다. 보다 구체적으로, 컴파일 옵션 선택 단계(S120)에서는 신경 처리 장치의 하드웨어 정보에 기반하여, 컴파 일 옵션을 설정할 수 있다. 그리고, 컴파일 옵션 선택 단계에서는 사용자의 선택에 의해서 복수의 컴파일 옵션을 설정할 수 있다. 여기서 각각의 컴파일 옵션에 따른 장점과 단점에 관한 설명을 사용자 디바이스에 디스플레이 할 수 있다. 따라서 사용자는 다양한 컴파일 옵션을 사용자에게 필요한 형태로 취사 선택할 수 있게 된다. 즉, 성능 평가 시스템 은 사용자의 세부적인 요구사항에 맞게 프리셋 옵션이 아닌 사용자 맞춤(user customization) 방식으로 컴파일 옵션을 제공하는 것도 가능하다. 상술한, 컴파일 옵션은 프루닝 (pruning) 알고리즘, 양자화(quantization) 알고리즘, 모델 압축(model compression) 알고리즘, 지식 증류 (knowledge distillation) 알고리즘, 재학습(retraining) 알고리즘, 및 인 공지능 기반 최적화(AI based model optimization) 알고리즘 및 중 적어도 하나일 수 있다. 또는 컴파일 옵션은 준비된 프리셋 옵션들 중 하나를 선택하도록 구성될 수 있다. 이후 인공 신경망 모델 업로드 단계(S130)에서는 선택된 신경 처리 장치의 성능 평가를 위한 적어도 하나의 특 정 인공 신경망 모델을 서버에 업로드 한다. 여기서, 인공 신경망 모델 업로드 단계(S130)는 사용자 데이 터 업로드 단계로 지칭될 수 있다. 이후 인공 신경망 모델 컴파일 및 처리 단계(S140)에서는 선택된 컴파일 옵션에 따라, 입력된 인공 신경망 모델 을 컴파일하고, 컴파일된 머신코드를 NPU FARM 내에서 선택된 신경 처리 장치에 입력하여 처리한다. 인공 신경망 모델 처리 결과 보고 단계(S150)에서는 컴파일 된 인공 신경망 모델을 처리한 신경 처리 장치의 성 능 평가 결과를 보고할 수 있다. 성능 평가 결과 보고는 사용자의 계정에 저장되거나 또는 사용자의 이메일 주 소로 발송될 수 있다. 단, 이에 제한되지 않으며, 다양한 방식으로 성능 평가 결과를 사용자에게 제공할 수 있 다. 성능 평가 결과 또한 사용자 데이터로 취급되며 사용자 데이터에 적용된 보안 정책이 적용될 수 있다. 부연 설명하면, 인공 신경망 모델 처리 결과 보고 단계(S150)에서는 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가능한지 여부를 먼저 판단한다. 만약, 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가 불가능한 경우, 인공 신경망 모델 처리 결과 보고 단계(S150)에서는 인공 신경망 모델의 복수의 레이어 중 복수의 신경 처리 장치에 의 해서 처리가 불가능한 레이어를 보고할 수 있다. 그리고, 인공 신경망 모델의 복수의 레이어 중 복수의 신경 처리 장치에 의해서 처리가 불가능한 레이어는 그래픽 처리 장치에 의해서 처리될 수 있다. 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가 가능한 경우, 인공 신경망 모델 처 리 결과 보고 단계(S150)에서는 복수의 신경 처리 장치의 처리 성능을 보고할 수 있다. 처리 성능의 지표는 신경 처리 장치의 온도 프로파일, 소비 전력(Watt), Trillion Operations Per Second per Watt(TOPS/W), Frame Per Second (FPS), Inference Per Second (IPS) 및 정확도(Accuracy) 등 일 수 있다. 여기서 사용자가 평가 데이터 세트를 제공하지 않을 경우, 인공 신경망 모델 성능 평가 시스템은 인공 신 경망 모델의 입력 데이터 크기를 분석하여 대응되는 더미 데이터를 생성하고, 생성된 더미 데이터를 활용하여성능 평가를 수행할 수 있다. 예를 들어 더미 데이터의 크기는 (224 x 224 x 3), (288 x 288 x 3), (380 x 380 x 3), (515 x 512 x 3), (640 x 640 x 3) 등일 수 있으나, 본 개시는 더미 데이터의 크기에 제한되지 않는다. 즉, 추론 성능을 평가할 수 있는 데이터 세트가 제공되지 않더라도, 신경 처리 장치의 소비 전력, TOPS/W, FPS, IPS 등의 성능 평가 결과 생성은 가능할 수 있다. 다만 이러한 경우 추론 정확도 평가 결과는 제공되지 않을 수 있다. 본 개시의 일 예시에 따르면, 사용자는 특정 NPU를 구매하기 전에 사전에 사용자의 인공 신경망 모델이 어떤 NPU에서 동작 가능한지 여부를 빠르게 알 수 있다. 본 개시의 일 예시에 따르면, 사용자는 특정 NPU를 구매하기 전에 사전에 사용자의 인공 신경망 모델이 특정 NPU에서 구동시 어떠한 성능으로 동작할지 여부를 빠르게 알 수 있다. 본 개시의 일 예시에 따르면, 각각의 NPU가 종류별로 서버를 통해서 연결될 경우, 사용자는 온라인으로 사용자 의 인공 신경망 모델을 각각 평가하여, 구매가능한 각각의 NPU마다 구동 여부를 온라인으로 평가하고 결과를 받 을 수 있다. 따라서, 성능 평가 시스템은 사용자가 개발한 AI 서비스의 구현에 필요한 신경 처리 장치의 성능 및 가격 의 정보를 사용자에게 제공하여, 사용자의 구매를 빠르게 결정하는데 도움을 줄 수 있다. 도 9는 본 개시의 다른 예시에 따른 인공 신경망 모델 성능 평가 방법을 설명하기 위한 흐름도이다. 도 9를 참조하면, 인공 신경망 모델 성능 평가 방법(S200)은 신경 처리 장치 종류 선택 단계(S110); 컴파일 옵 션 선택 단계(S120); 인공 신경망 모델 및 데이터세트 업로드 단계(S230); 인공 신경망 모델 컴파일 및 처리 단 계(S140); 및 인공 신경망 모델 처리 결과 보고 단계(S150)를 포함한다. 신경 처리 장치 종류 선택 단계(S110)에서는 사용자가 성능 평가를 진행할 신경 처리 장치의 종류를 선택한다. 신경 처리 장치의 종류는 특정 회사에서 판매하는 신경 처리 장치의 제품군에 따라 달라질 수 있다. 도 7에 도시된 바와 같이, “”신경 처리 장치는 복수 개가 배치되어, 제1 그룹(G1)을 형성할 수 있고, “”신 경 처리 장치는 복수 개가 배치되어, 제2 그룹(G2)을 형성할 수 있고, “”신경 처리 장치는 복수 개가 배치되 어, 제3 그룹(G3)을 형성할 수 있고, “”신경 처리 장치는 복수 개가 배치되어, 제4 그룹(G4)을 형성할 수 있 다. 사용자는 “”신경 처리 장치, “”신경 처리 장치, “”신경 처리 장치 및 “”신경 처리 장치 중 성능 평가를 진행할 신경 처리 장치를 선택한다. 그리고, 서버는 선택된 특정 종류의 신경 처리 장치들 중 NPU FARM내에서 가용 가능한 신경 처리 장치를 적어도 하나 이상 선택할 수 있다. 이때 선택되는 특정 종류의 신경 처리 장치는 하나의 종류에 한정되지 않고 복수개의 종류일 수 있다. 이후, 컴파일 옵션 선택 단계(S120)에서는 선택된 적어도 하나의 신경 처리 장치가 처리할 인공 신경망 모델의 복수의 컴파일 옵션 중 적어도 하나를 선택한다. 보다 구체적으로, 컴파일 옵션 선택 단계(S120)에서는 신경 처리 장치의 하드웨어 정보에 기반하여, 컴파 일 옵션을 설정할 수 있다. 그리고, 컴파일 옵션 선택 단계에서는 사용자의 선택에 의해서 복수의 컴파일 옵션을 설정할 수 있다. 여기서 각각의 컴파일 옵션에 따른 장점과 단점에 관한 설명을 사용자 디바이스에 디스플레이 할 수 있다. 따라서 사용자는 다양한 컴파일 옵션을 사용자에게 필요한 형태로 취사 선택할 수 있게 된다. 즉, 성능 평가 시스템 은 사용자의 세부적인 요구사항에 맞게 프리셋 옵션이 아닌 사용자 맞춤(user customization) 방식으로 컴파일 옵션을 제공하는 것도 가능하다. 상술한, 컴파일 옵션은 프루닝 (pruning) 알고리즘, 양자화(quantization) 알고리즘, 모델 압축(model compression) 알고리즘, 지식 증류 (knowledge distillation) 알고리즘, 재학습(retraining) 알고리즘, 및 인 공지능 기반 최적화(AI based model optimization) 알고리즘 및 중 적어도 하나일 수 있다. 또는 컴파일 옵션은 준비된 프리셋 옵션들 중 하나를 선택하도록 구성될 수 있다. 이후 인공 신경망 모델 및 데이터세트 업로드 단계(S230)에서는 선택된 신경 처리 장치의 성능 평가를 위한 적 어도 하나의 특정 인공 신경망 모델 및 인공 신경망 모델에 입력되는 적어도 하나의 특정 평가데이터세트를 서버에 업로드 한다. 상술한 특정 평가데이터세트는 인공 신경망 모델 처리 장치의 성능 평가를 위해 인공 신경망 모델 처리 장치에 입력되는 평가 데이터세트를 의미한다. 여기서, 인공 신경망 모델 및 데이 터세트 업로드 단계(S230)는 사용자 데이터 업로드 단계로 지칭될 수 있다. 이후 인공 신경망 모델 컴파일 및 처리 단계(S140)에서는 선택된 컴파일 옵션에 따라, 입력된 인공 신경망 모델 을 컴파일하고, 컴파일된 머신코드 및 평가데이터세트를 NPU FARM 내에서 선택된 신경 처리 장치에 입력하여 처 리한다. 인공 신경망 모델 처리 결과 보고 단계(S150)에서는 컴파일 된 인공 신경망 모델을 처리한 신경 처리 장치의 성 능 평가 결과를 보고할 수 있다. 성능 평가 결과 보고는 사용자의 계정에 저장되거나 또는 사용자의 이메일 주 소로 발송될 수 있다. 단, 이에 제한되지 않으며, 다양한 방식으로 성능 평가 결과를 사용자에게 제공할 수 있 다. 성능 평가 결과 또한 사용자 데이터로 취급되며 사용자 데이터에 적용된 보안 정책이 적용될 수 있다. 부연 설명하면, 인공 신경망 모델 처리 결과 보고 단계(S150)에서는 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가능한지 여부를 먼저 판단한다. 만약, 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가 불가능한 경우, 인공 신경망 모델 처리 결과 보고 단계(S150)에서는 인공 신경망 모델의 복수의 레이어 중 복수의 신경 처리 장치에 의 해서 처리가 불가능한 레이어를 보고할 수 있다. 그리고, 인공 신경망 모델의 복수의 레이어 중 복수의 신경 처리 장치에 의해서 처리가 불가능한 레이어는 그래픽 처리 장치에 의해서 처리될 수 있다. 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가 가능한 경우, 인공 신경망 모델 처 리 결과 보고 단계(S150)에서는 복수의 신경 처리 장치의 처리 성능을 보고할 수 있다. 처리 성능의 지표는 신경 처리 장치의 온도 프로파일, 소비 전력(Watt), Trillion Operations Per Second per Watt(TOPS/W), Frame Per Second (FPS), Inference Per Second (IPS) 및 정확도(Accuracy) 등 일 수 있다. 본 개시의 다른 예시에 따르면, 사용자는 특정 NPU를 구매하기 전에 사전에 사용자의 인공 신경망 모델이 어떤 NPU에서 동작 가능한지 여부를 빠르게 알 수 있다. 본 개시의 다른 예시에 따르면, 사용자는 특정 NPU를 구매하기 전에 사전에 사용자의 인공 신경망 모델이 특정 NPU에서 구동시 어떠한 성능으로 동작할지 여부를 빠르게 알 수 있다. 본 개시의 다른 예시에 따르면, 각각의 NPU가 종류별로 서버를 통해서 연결될 경우, 사용자는 온라인으로 사용 자의 인공 신경망 모델을 각각 평가하여, 각 NPU마다 구동 여부를 온라인으로 평가하고 결과를 받을 수 있다. 따라서, 성능 평가 시스템은 사용자가 개발한 AI 서비스의 구현에 필요한 신경 처리 장치의 성능 및 가격 의 정보를 사용자에게 제공하여, 사용자의 구매를 빠르게 결정하는데 도움을 줄 수 있다. 이하에서는, 도10을 참조하여, 재학습 단계가 추가된 본 개시의 다른 예시에 따른 인공 신경망 모델 성능 평가 방법에 대해서 설명한다. 도 10은 본 개시의 다른 예시에 따른 인공 신경망 모델 성능 평가 방법을 설명하기 위한 흐름도이다. 도 10을 참조하면, 인공 신경망 모델 성능 평가 방법(S300)은 신경 처리 장치 종류 선택 단계(S110); 컴파일 옵 션 선택 단계(S120); 인공 신경망 모델 및 데이터세트 업로드 단계(S230); 인공 신경망 모델 컴파일 및 처리 단 계(S140); 인공 신경망 모델의 재학습 단계(S345); 및 인공 신경망 모델 처리 결과 보고 단계(S150)를 포함한다. 신경 처리 장치 종류 선택 단계(S110)에서는 사용자가 성능 평가를 진행할 신경 처리 장치의 종류를 선택한다. 신경 처리 장치의 종류는 특정 회사에서 판매하는 신경 처리 장치의 제품군에 따라 달라질 수 있다. 도 7에 도시된 바와 같이, “”신경 처리 장치는 복수 개가 배치되어, 제1 그룹(G1)을 형성할 수 있고, “”신 경 처리 장치는 복수 개가 배치되어, 제2 그룹(G2)을 형성할 수 있고, “”신경 처리 장치는 복수 개가 배치되 어, 제3 그룹(G3)을 형성할 수 있고, “”신경 처리 장치는 복수 개가 배치되어, 제4 그룹(G4)을 형성할 수 있 다. 사용자는 “”신경 처리 장치, “”신경 처리 장치, “”신경 처리 장치 및 “”신경 처리 장치 중 성능 평가를 진행할 신경 처리 장치를 선택한다. 그리고, 서버는 선택된 특정 종류의 신경 처리 장치들 중 NPU FARM내에서 가용 가능한 신경 처리 장치를 적어도 하나 이상 선택할 수 있다. 이때 선택되는 특정 종류의 신경 처리 장치는 하나의 종류에 한정되지 않고 복수개의 종류일 수 있다. 이후, 컴파일 옵션 선택 단계(S120)에서는 선택된 적어도 하나의 신경 처리 장치가 처리할 인공 신경망 모델의 복수의 컴파일 옵션 중 적어도 하나를 선택한다. 보다 구체적으로, 컴파일 옵션 선택 단계(S120)에서는 신경 처리 장치의 하드웨어 정보에 기반하여, 컴파 일 옵션을 설정할 수 있다. 그리고, 컴파일 옵션 선택 단계에서는 사용자의 선택에 의해서 복수의 컴파일 옵션을 설정할 수 있다. 여기서 각각의 컴파일 옵션에 따른 장점과 단점에 관한 설명을 사용자 디바이스에 디스플레이 할 수 있다. 따라서 사용자는 다양한 컴파일 옵션을 사용자에게 필요한 형태로 취사 선택할 수 있게 된다. 즉, 성능 평가 시스템 은 사용자의 세부적인 요구사항에 맞게 프리셋 옵션이 아닌 사용자 맞춤(user customization) 방식으로 컴파일 옵션을 제공하는 것도 가능하다. 상술한, 컴파일 옵션은 프루닝 (pruning) 알고리즘, 양자화(quantization) 알고리즘, 모델 압축(model compression) 알고리즘, 지식 증류 (knowledge distillation) 알고리즘, 재학습(retraining) 알고리즘, 및 인 공지능 기반 최적화(AI based model optimization) 알고리즘 및 중 적어도 하나일 수 있다. 또는 컴파일 옵션은 준비된 프리셋 옵션들 중 하나를 선택하도록 구성될 수 있다. 이후 인공 신경망 모델 및 데이터세트 업로드 단계(S230)에서는 선택된 신경 처리 장치의 성능 평가를 위한 적 어도 하나의 특정 인공 신경망 모델 및 인공 신경망 모델에 입력되는 적어도 하나의 특정 평가데이터세트를 서 버에 업로드 한다. 상술한 특정 평가데이터세트는 인공 신경망 모델 처리 장치의 성능 평가를 위해 인공 신경망 모델 처리 장치에 입력되는 평가 데이터세트를 의미한다. 여기서, 인공 신경망 모델 및 데이 터세트 업로드 단계(S230)는 사용자 데이터 업로드 단계로 지칭될 수 있다. 이후 인공 신경망 모델 컴파일 및 처리 단계(S140)에서는 선택된 컴파일 옵션에 따라, 입력된 인공 신경망 모델 을 컴파일하고, 컴파일된 머신코드 및 평가데이터세트를 NPU FARM 내에서 선택된 신경 처리 장치에 입력하여 처 리한다. 예를 들어, 사용자는 선택한 컴파일 옵션은 재학습을 수행하는 재학습 (retraining) 옵션이 포함될 수 있다. 만 약 컴파일 옵션에서 적어도 하나의 재학습 (retraining) 옵션이 선택된 경우, 인공 신경망 모델 컴파일 및 처리 단계(S140) 이후에, 인공 신경망 모델의 재학습 단계(S345)가 수행될 수 있다. 인공 신경망 모델의 재학습 단계 (S345)에서, 성능 평가 시스템은 인공 신경망 모델 처리 장치에서 재학습을 수행할 그래픽 처리 장 치을 할당한다. 예를 들어, 인공 신경망 모델의 재학습 단계(S345)에서, 그래픽 처리 장치는 프루닝 알고리즘 및/또는 양자화 알고리즘이 적용된 인공 신경망 모델과 학습 데이터세트를 입력받아 재학습을 수행할 수 있다. 재학습은 에포크(epoch) 단위로 수행될 수 있으며, 수회 내지 수백회의 에포크가 그래픽 처리 장치 에서 수행될 수 있다. 재학습 (retraining) 옵션은 다양한 최적화 옵션 적용시 저하되는 추론 정확도를 보상할 수 있는 기술이다. 예 를 들면 양자화 옵션, 프루닝 옵션, 모델 압축 옵션 적용 시, 복수의 신경 처리 장치에서 추론되는 인공 신경망 모델의 정확도가 저하될 수 있다. 이러한 경우 프루닝, 양자화, 및/또는 모델 압축된 인공 신경망 모델 을 온라인상에서 재학습 시킬 수 있는 옵션이 제공될 수 있다. 재학습이 완료된 인공 신경망 모델의 추론 정확 도는 다시 상승할 수 있다. 구체적으로, 재학습(retraining) 옵션은 양자화 인식 재학습(quantization aware retraining) 옵션, 프루닝 인 식 재학습(pruning aware retraining) 옵션 및 전이 학습 (transfer learning) 옵션 등을 포함할 수 있다. 인공 신경망 모델 처리 결과 보고 단계(S150)에서는 컴파일 된 인공 신경망 모델을 처리한 신경 처리 장치의 성 능 평가 결과를 보고할 수 있다. 성능 평가 결과 보고는 사용자의 계정에 저장되거나 또는 사용자의 이메일 주 소로 발송될 수 있다. 단, 이에 제한되지 않으며, 다양한 방식으로 성능 평가 결과를 사용자에게 제공할 수 있 다. 성능 평가 결과 또한 사용자 데이터로 취급되며 사용자 데이터에 적용된 보안 정책이 적용될 수 있다. 부연 설명하면, 인공 신경망 모델 처리 결과 보고 단계(S150)에서는 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가능한지 여부를 먼저 판단한다. 만약, 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가 불가능한 경우, 인공 신경망 모델 처리 결과 보고 단계(S150)에서는 인공 신경망 모델의 복수의 레이어 중 복수의 신경 처리 장치에 의 해서 처리가 불가능한 레이어를 보고할 수 있다. 그리고, 인공 신경망 모델의 복수의 레이어 중 복수의 신경 처리 장치에 의해서 처리가 불가능한 레이어는 그래픽 처리 장치에 의해서 처리될 수 있다. 컴파일된 인공 신경망 모델이 복수의 신경 처리 장치에 의해서 처리가 가능한 경우, 인공 신경망 모델 처 리 결과 보고 단계(S150)에서는 복수의 신경 처리 장치의 처리 성능을 보고할 수 있다. 처리 성능의 지표는 신경 처리 장치의 온도 프로파일, 소비 전력(Watt), Trillion Operations Per Second per Watt(TOPS/W), Frame Per Second (FPS), Inference Per Second (IPS) 및 정확도(Accuracy) 등 일 수 있다. 본 개시의 다른 예시에 따르면, 사용자는 특정 NPU를 구매하기 전에 사전에 사용자의 인공 신경망 모델이 어떤 NPU에서 동작 가능한지 여부를 빠르게 알 수 있다. 본 개시의 다른 예시에 따르면, 사용자는 특정 NPU를 구매하기 전에 사전에 사용자의 인공 신경망 모델이 특정 NPU에서 구동시 어떠한 성능으로 동작할지 여부를 빠르게 알 수 있다. 본 개시의 다른 예시에 따르면, 각각의 NPU가 종류별로 서버를 통해서 연결될 경우, 사용자는 온라인으로 사용 자의 인공 신경망 모델을 각각 평가하여, 각 NPU마다 구동 여부를 온라인으로 평가하고 결과를 받을 수 있다. 본 개시의 다른 예시에 따르면, 온라인상에서 특정 신경처리유닛에 최적화된 인공 신경망 모델 재학습 알고리즘 을 성능 평가 시스템을 통해 실시할 수 있다. 이때 상술한 보안 정책에 의해서 사용자 데이터를 성능 평 가 시스템의운영자로부터 분리 및 보호할 수 있다. 따라서, 성능 평가 시스템은 사용자가 개발한 AI 서비스의 구현에 필요한 신경 처리 장치의 성능 및 가격 의 정보를 사용자에게 제공하여, 사용자의 구매를 빠르게 결정하는데 도움을 줄 수 있다. 본 개시는 인공 신경망 모델 성능 평가 시스템에 관한 것으로서, 서로 다른 연산 처리 성능을 가지는, 복수의 신경 처리 장치 (Neural Processing Unit; NPU), 복수의 NPU 중 적어도 하나의 성능 평가를 위한 적어도 하나 의 인공 신경망 모델(Artificial Neural Network; ANN) 및 적어도 하나의 특정 평가데이터세트 가 업로드 되는, 서버, 상기 복수의 NPU 중 적어도 하나를 선택하고, 복수의 컴파일 옵션 중 어느 하나에 따라 상기 ANN을 컴파 일하는, 컴파일러 및 선택된 NPU에서 상기 컴파일 된 ANN을 처리하여 상기 성능 평가 결과를 보고하는, 리포팅 프로그램을 포함하도록 구성될 수 있다. 본 개시의 다른 특징에 따르면, 컴파일러는 양자화(quantization) 알고리즘, 프루닝 (pruning) 알고리즘, 재학 습(retraining) 알고리즘, 모델 압축(model compression) 알고리즘, 인공지능 기반 최적화(AI based model optimization) 알고리즘 및 지식 증류 (knowledge distillation) 알고리즘 중 적어도 하나를 기초로, 상기 복 수의 컴파일 옵션을 설정할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 NPU는 적어도 하나의 내부 메모리와 적어도 하나의 곱셉 및 누적 연산 기를 포함하고, 컴파일러는 상기 NPU의 하드웨어 정보에 기반하여, 상기 복수의 컴파일 옵션을 설정할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 복수의 컴파일 옵션 중 상기 ANN의 재학습 알고리즘 처리하는 그래픽 처리 장치를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 리포팅 프로그램은 복수의 컴파일 옵션 중 어느 하나에 따라 컴파일된 인공 신경망 모델이 상기 NPU에서 동작이 가능한지 여부를 보고할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 복수의 컴파일 옵션 중 어느 하나에 따라 컴파일된 인공 신경망 모델이 상기 NPU에서 동작이 불가능한 경우, 리포팅 프로그램은 인공 신경망 모델의 복수의 레이어 중 동작이 불가능한 레이어를 보고할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 리포팅 프로그램은 복수의 컴파일 옵션 각각에 따라 컴파일된 인공 신 경망 모델의 처리 성능을 보고하고, 처리 성능의 지표는 온도 프로파일, 소비 전력, Trillion Operations Per Second per Watt(TOPS/W), Frame Per Second (FPS), Inference Per Second (IPS) 및 정확도(Accuracy)를 포함 할 수 있다.본 개시의 또 다른 특징에 따르면, 상기 리포팅 프로그램은 복수의 컴파일 옵션 각각에 따라 컴파일된 인공 신 경망 모델의 처리 성능을 분석하여, 상기 복수의 컴파일 옵션 중 어느 하나를 제안할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 복수의 컴파일 옵션은 제1 내지 제3 프리셋 옵션을 포함하고, 제1 프리 셋 옵션은 Post Training Quantization(PTQ) 알고리즘을 포함하고, 제2 프리셋 옵션은 Post Training Quantization(PTQ) 알고리즘 및 인공 신경망 모델의 레이어별 재학습 (layer wise retraining) 알고리즘을 포 함하고, 제3 프리셋 옵션은 Post Training Quantization(PTQ) 알고리즘 및 양자화 인식 재학습(quantization aware training; QAT) 알고리즘을 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 서버에 업로드 되는 적어도 하나의 특정 평가데이터세트에는 보호하는 데이터 암호화 (Data Encryption), 차등 개인 정보 보호 (Differential Privacy) 및 데이터 마스킹 (Data Masking) 중 적어도 하나가 적용될 수 있다. 본 개시는 인공 신경망 모델 성능 평가 방법에 관한 것으로서, 사용자가 성능 평가를 진행할 적어도 하나의 신 경 처리 장치(Neural Processing Unit; NPU)의 종류 및 개수를 선택하는 단계; 선택된 적어도 하나의 NPU가 처 리할 인공 신경망 모델(Artificial Neural Network; ANN)의 복수의 컴파일 옵션 중 적어도 하나를 선택하는 단 계; 선택된 적어도 하나의 NPU가 처리할 ANN 및 적어도 하나의 특정 평가데이터세트를 업로드하는 단계; 선택된 복수의 컴파일 옵션 중 적어도 하나에 따라 상기 ANN의 컴파일하여 처리하는 단계; 및 선택된 NPU가 ANN 처리하 여 성능 평가를 보고하는 단계를 포함하도록 구성된다. 본 개시의 다른 특징에 따르면, 상기 복수의 컴파일 옵션 중 적어도 하나를 선택하는 단계에서, 양자화 (quantization) 알고리즘, 프루닝 (pruning) 알고리즘, 재학습(retraining) 알고리즘, 모델 압축(model compression) 알고리즘, 인공지능 기반 최적화(AI based model optimization) 알고리즘 및 지식 증류 (knowledge distillation) 알고리즘 중 적어도 하나를 기초로, 상기 복수의 컴파일 옵션을 설정할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 NPU는 적어도 하나의 내부 메모리와 적어도 하나의 곱셉 및 누적 연산 기를 포함하고, 복수의 컴파일 옵션 중 적어도 하나를 선택하는 단계에서, NPU의 하드웨어 정보에 기반하여, 상 기 복수의 컴파일 옵션을 설정할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 상기 ANN의 컴파일하여 처리하는 단계 이후, ANN의 재학습을 수행하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 성능 평가를 보고하는 단계에서, 복수의 컴파일 옵션 중 어느 하나에 따라 컴파일된 인공 신경망 모델이 상기 NPU에서 동작이 가능한지 여부를 보고할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 복수의 컴파일 옵션 중 어느 하나에 따라 컴파일된 인공 신경망 모델이 상기 NPU에서 동작이 불가능한 경우, 성능 평가를 보고하는 단계에서, 인공 신경망 모델의 복수의 레이어 중 동 작이 불가능한 레이어를 보고할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 성능 평가를 보고하는 단계에서, 복수의 컴파일 옵션 각각에 따라 컴파 일된 인공 신경망 모델의 처리 성능을 보고하고, 처리 성능의 지표는 Trillion Operations Per Second per Watt(TOPS/W), Frame Per Second (FPS), Inference Per Second (IPS) 및 정확도(Accuracy)를 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 성능 평가를 보고하는 단계에서, 복수의 컴파일 옵션 각각에 따라 컴파 일된 인공 신경망 모델의 처리 성능을 분석하여, 상기 복수의 컴파일 옵션 중 어느 하나를 제안할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 복수의 컴파일 옵션은 제1 내지 제3 프리셋 옵션을 포함하고, 제1 프리 셋 옵션은 Post Training Quantization(PTQ) 알고리즘을 포함하고, 제2 프리셋 옵션은 Post Training Quantization(PTQ) 알고리즘 및 인공 신경망 모델의 레이어별 재학습 (layer wise retraining) 알고리즘을 포 함하고, 제3 프리셋 옵션은 Post Training Quantization(PTQ) 알고리즘 및 양자화 인식 재학습(quantization aware training; QAT) 알고리즘을 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 적어도 하나의 특정 평가데이터세트에는 보호하는 데이터 암호화 (Data Encryption), 차등 개인 정보 보호 (Differential Privacy) 및 데이터 마스킹 (Data Masking) 중 적어도 하나 가 적용될 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 적어도 하나의 특정 평가데이터세트를 업로드하는 단계에서, 평가데이 터세트에 관한 사용자 데이터 보호에 관한 약관에 서명할 수 있다.본 개시는 인공 신경망 모델 성능 평가 시스템에 관한 것으로서, 서로 다른 연산 처리 성능을 가지는, 제1 구성 의 제1 신경 프로세서 및 상기 제1 구성과 다른 제2 구성의 제2 신경 프로세서를 포함하는 복수의 신경 프로세 서, 적어도 하나의 운영 프로세서 및 적어도 하나의 운영 프로세서에 사용되는 명령어를 저장하는 메모리를 포 함한다. 그리고, 적어도 하나의 운영 프로세서는, 인공 신경망 모델, 제1 신경 프로세서 및 제2 신경 프로세서 중 적어도 하나를 선택하는 제1 프로세서 선택 정보, 및 컴파일 옵션 정보를 수신하고, 컴파일 옵션에 따라, 상 기 인공 신경망 모델 컴파일함으로써, 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경망 모델을 처리하고, 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해 적어도 하나 의 평가 데이터 세트를 상기 인공 신경망 모델에 적용하고, 제1 프로세서 선택 정보에 의해 선택된 신경 프로세 서의 성능과 관련된 적어도 하나의 제1 성능 파라미터를 생성할 수 있다. 본 개시의 다른 특징에 따르면, 인공 신경망 모델 성능 평가 시스템은 네트워크를 통해 사용자 디바이스로부터 상기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도 하나의 평가 데이터 세트를 수신하고, 상 기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도 하나의 평가 데이터 세트를 상기 적어도 하 나의 운영 프로세서에 보내고, 상기 적어도 하나의 운영 프로세서부터 적어도 하나의 제1 성능 파라미터를 수신 하고, 수신된 적어도 하나의 제1 성능 파라미터를 네트워크를 통해 상기 사용자 디바이스에 전송하는 서버를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 적어도 하나의 평가 데이터 세트에는 데이터 암호화 (Data Encryption), 차등 개인 정보 보호 (Differential Privacy) 및 데이터 마스킹 (Data Masking) 중 적어도 하나 가 적용될 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 컴파일 옵션 정보는, 양자화(quantization) 알고리즘, 프루닝 (pruning) 알고리즘, 재학습(retraining) 알고리즘, 모델 압축(model compression) 알고리즘, 인공지능 기반 최적화(AI based model optimization) 알고리즘 및 지식 증류 (knowledge distillation) 알고리즘 중 적어도 하나를 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 제1 신경 프로세서는 적어도 하나의 내부 메모리와 적어도 하나의 곱셉 및 누적 연산기를 포함하고, 적어도 하나의 운영 프로세서는 상기 제1 구성에 기반하여, 상기 컴파일 옵션을 설 정할 수 있다. 본 개시의 또 다른 특징에 따르면, 적어도 하나의 운영 프로세서는 상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리 가능한 레이어를 결정할 수 있다. 본 개시의 또 다른 특징에 따르면, 적어도 하나의 운영 프로세서는 상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 보고할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 시스템은 상기 인공 신경망 모델의 복수의 레이 어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 처리하는 그래픽 프로세서를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 그래픽 프로세서는 인공 신경망 모델의 재학습을 수행시킬 수 있다. 본 개시의 또 다른 특징에 따르면, 적어도 하나의 제1 성능 파라미터는 온도 프로파일, 소비 전력, Trillion Operations Per Second per Watt(TOPS/W), Frame Per Second (FPS), Inference Per Second (IPS) 및 추론 및 예측 정확도(Accuracy) 중 적어도 하나를 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 적어도 하나의 운영 프로세서는, 제1 신경 프로세서 및 제2 신경 프로 세서 중 적어도 다른 하나를 선택하는 제2 프로세서 선택 정보를 더 수신하고, 상기 인공 신경망 모델 컴파일함 으로써, 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경망 모델을 처 리하고, 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해 상기 적어도 하나의 평가 데이터 세 트를 상기 인공 신경망 모델에 적용하고, 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서의 성능과 관련된 적어도 하나의 제2 성능 파라미터를 생성할 수 있다. 본 개시의 또 다른 특징에 따르면, 상기 적어도 하나의 운영 프로세서는, 적어도 하나의 제1 성능 파라미터와 상기 적어도 하나의 제2 성능 파라미터를 비교하여 제1 프로세서 선택 정보 및 제2 프로세서 선택 정보 중 어느 하나를 추천할 수 있다.본 개시의 또 다른 특징에 따르면, 상기 컴파일 옵션은 Post Training Quantization(PTQ) 알고리즘, 인공 신경 망 모델의 레이어별 재학습 (layer wise retraining) 알고리즘 및 양자화 인식 재학습(quantization aware training; QAT) 알고리즘 중 적어도 하나를 포함할 수 있다. 본 개시는 인공 신경망 모델 성능 평가 방법에 관한 것으로서, 적어도 하나의 운영 프로세서가 인공 신경망 모 델, 제1 신경 프로세서 및 제2 신경 프로세서 중 적어도 하나를 선택하는 제1 프로세서 선택 정보, 및 컴파일 옵션 정보를 수신하는 단계, 상기 적어도 하나의 운영 프로세서가 상기 컴파일 옵션에 따라, 상기 인공 신경망 모델 컴파일함으로써, 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경 망 모델을 처리하는 단계, 상기 적어도 하나의 운영 프로세서가 상기 제1 프로세서 선택 정보에 의해 선택된 신 경 프로세서에 의해 적어도 하나의 평가 데이터 세트를 상기 인공 신경망 모델에 적용하는 단계; 및 상기 적어 도 하나의 운영 프로세서가 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서의 성능과 관련된 적어도 하나의 제1 성능 파라미터를 생성하여 네트워크를 통해 전송하는 단계를 포함할 수 있다. 본 개시의 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 서버가 네트워크를 통해 사용자 디바이스로 부터 상기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도 하나의 평가 데이터 세트를 수신하 고, 상기 제1 프로세서 선택 정보, 상기 컴파일 옵션 정보 및 상기 적어도 하나의 평가 데이터 세트를 상기 적 어도 하나의 운영 프로세서에 보내는 단계; 및 상기 서버가 상기 적어도 하나의 운영 프로세서부터 적어도 하나 의 제1 성능 파라미터를 수신하고, 수신된 적어도 하나의 제1 성능 파라미터를 네트워크를 통해 상기 사용자 디 바이스에 전송하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 제1 신경 프로세서 및 제2 신경 프로세 서 중 적어도 하나의 하드웨어 구성에 기반하여, 상기 컴파일 옵션을 설정하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 상기 인공 신경망 모델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 보고하는 단 계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 그래픽 프로세서가 상기 인공 신경망 모 델의 복수의 레이어 중 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해서 처리가 불가능한 레이어를 처리하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 그래픽 프로세서가 상기 인공 신경망 모 델의 재학습을 수행시키는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 적어도 하나의 운영 프로세서가 제1 신 경 프로세서 및 제2 신경 프로세서 중 적어도 다른 하나를 선택하는 제2 프로세서 선택 정보를 수신하는 단계, 상기 적어도 하나의 운영 프로세서가 상기 컴파일 옵션에 따라, 상기 인공 신경망 모델 컴파일함으로써, 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에서 사용되는 상기 인공 신경망 모델을 처리하는 단계, 상기 적어도 하나의 운영 프로세서가 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서에 의해 적어도 하나의 평가 데이터 세트를 상기 인공 신경망 모델에 적용하는 단계; 및 상기 적어도 하나의 운영 프로세서가 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서의 성능과 관련된 적어도 하나의 제2 성능 파라미터 를 생성하여 네트워크를 통해 전송하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 상기 적어도 하나의 제1 성능 파라미터 와 상기 적어도 하나의 제2 성능 파라미터를 비교하여 제1 프로세서 선택 정보 및 제2 프로세서 선택 정보 중 어느 하나를 추천하는 단계를 더 포함할 수 있다. 본 개시는 인공 신경망 모델 성능 평가 방법에 관한 것으로서, 제1 구성의 제1 신경 프로세서 및 상기 제1 구성 과 다른 제2 구성의 제2 신경 프로세서를 포함하는 복수의 신경 프로세서를 선택하기 위한 옵션을 표시하는 단 계, 상기 복수의 신경 프로세서 중 적어도 하나를 선택하는 제1 프로세서 선택 정보를 사용자로부터 수신하는 단계, 인공 신경망 모델의 컴파일과 연관된 복수의 컴파일 옵션을 표시하는 단계, 사용자로부터 복수의 컴파일 옵션 중 제1 컴파일 옵션 선택 정보를 수신하는 단계, 상기 제1 프로세서 선택 정보, 제1 컴파일 옵션 선택 정 보, 및 적어도 하나의 평가 데이터 세트를 상기 복수의 신경 프로세서에 연결된 컴퓨팅 장치로 보내는 단계, 상 기 제1 컴파일 옵션 선택 정보에 의해 선택된 컴파일 옵션에 따른 상기 제1 프로세서 선택 정보에 의해 선택된 신경 프로세서의 처리 성능과 관련된 적어도 하나의 제1 성능 파라미터를 수신하는 단계; 및 적어도 하나의 제1 성능 파라미터를 표시하는 단계를 포함할 수 있다.본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 사용자로부터 상기 복수의 신경 프로세 서 중 적어도 다른 하나를 선택하는 제2 프로세서 선택 정보를 수신하는 단계; 사용자로부터 복수의 컴파일 옵 션 중 제2 컴파일 옵션 선택 정보를 수신하는 단계; 상기 제2 프로세서 선택 정보 및 제2 컴파일 옵션 선택 정 보를 상기 복수의 신경 프로세서에 연결된 컴퓨팅 장치로 보내는 단계; 및 상기 제2 컴파일 옵션 선택 정보에 의해 선택된 컴파일 옵션에 따른 상기 제2 프로세서 선택 정보에 의해 선택된 신경 프로세서의 처리 성능과 관 련된 적어도 하나의 제2 성능 파라미터를 수신하는 단계를 더 포함할 수 있다. 본 개시의 또 다른 특징에 따르면, 인공 신경망 모델 성능 평가 방법은 제1 프로세서 선택 정보 및 제2 프로세 서 선택 정보 중 어느 하나를 추천하고, 이를 표시하는 단계를 더 포함할 수 있다. 이상 첨부된 도면을 참조하여 본 개시의 일 예시들을 더욱 상세하게 설명하였으나, 본 개시는 반드시 이러한 예 시로 국한되는 것은 아니고, 본 개시의 기술사상을 벗어나지 않는 범위 내에서 다양하게 변형 실시될 수 있다. 따라서, 본 개시에 개시된 예시들은 본 개시의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위한 것이고, 이러한 예시에 의하여 본 개시의 기술 사상의 범위가 한정되는 것은 아니다. 그러므로, 이상에서 기술한 예시 들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 본 발명의 보호 범위는 아래의 청구 범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 발명의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2024-0003712", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 예시에 따른 인공 신경망 모델 성능 평가 시스템의 구성을 나타낸 블록도이다. 도 2는 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 구성을 나타낸 블록도이다. 도 3은 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 컴파일러의 구성을 나타낸 블록도이다.도 4는 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 최적화 모듈의 구성을 나타낸 블록도이다. 도 5a는 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 복수의 신경 처리 장치 및 컴파일 옵션을 선 택하기 위한 인터페이스를 나타내는 블록도이다. 도 5b는 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 복수의 신경 처리 장치에 대한 성능 평가 및 제안을 위한 인터페이스를 나타내는 블록도이다. 도 6a 내지 도 6d는 본 개시의 일 예시에 따른 인공 신경망 모델 처리 장치의 하나의 신경 처리 장치의 구성을 나타낸 블록도이다. 도 7은 본 개시의 일 예시에 따른 복수의 신경 처리 장치의 구성을 나타낸 블록도이다. 도 8은 본 개시의 일 예시에 따른 인공 신경망 모델 성능 평가 방법을 설명하기 위한 흐름도이다. 도 9는 본 개시의 다른 예시에 따른 인공 신경망 모델 성능 평가 방법을 설명하기 위한 흐름도이다. 도 10은 본 개시의 다른 예시에 따른 인공 신경망 모델 성능 평가 방법을 설명하기 위한 흐름도이다."}
