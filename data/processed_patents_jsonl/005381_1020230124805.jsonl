{"patent_id": "10-2023-0124805", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0041871", "출원번호": "10-2023-0124805", "발명의 명칭": "가변 주파수를 이용하여 신경 프로세싱 유닛의 파워를 낮추는 기술", "출원인": "주식회사 딥엑스", "발명자": "김녹원"}}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "시스템으로서,ANN(artificial neural network) 모델들 중 적어도 하나의 ANN을 위한 연산을 수행할 수 있는 복수의PE(processing element)를 포함하는 NPU(neural processing unit)와; 그리고스위칭 회로를 포함하고,상기 스위칭 회로는: 서로 다른 주파수를 갖는 복수의 클럭 신호들 중 하나의 클럭 신호를 선택하고, 그리고 상기 선택된 하나의 클럭 신호를 상기 NPU에 공급하고,상기 하나의 클럭 신호는 상기 적어도 하나의 ANN 모델 내의 복수의 레이어들 중 주어진 레이어에 대한 상기 PE들의 이용률에 기초하여 선택되는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 하나의 클럭 신호는, 상기 PE들의 이용률에 대한 임계값을 추가로 고려하여 선택되는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 주어진 레이어에서 연산을 수행할 때의 파워를 줄이기 위하여, 상기 PE들의 이용률에 대한 임계값이 결정되는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 적어도 하나의 ANN 모델의 상기 주어진 레이어 보다 이전 레이어에 대한 연산을 상기 NPU가 완료했을 때, 상기 하나의 클럭 신호가 선택되는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 적어도 하나의 ANN 모델의 상기 주어진 레이어 보다 이전 레이어에 대한 연산을 완료함으로써 상기 NPU에의해서 출력되는 이전 레이어에 대한 정보에 기초하여 상기 하나의 클럭 신호가 상기 주어진 레이어에 대해서선택되는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 스위칭 회로는다수의 클럭 신호를 출력하는 생성기와; 그리고상기 하나의 클럭 신호를 선택하는 선택기를 포함하는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 주어진 레이어에 대한 이용률이 이전 레이어 보다 높은 경우,상기 주어진 레이어에 대해서 선택된 하나의 클럭 신호는 상기 이전 레이어의 제1 주파수 보다 낮은 제2 주파수를 갖는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 주어진 레이어에 대한 이용률이 이전 레이어 보다 낮은 경우,공개특허 10-2025-0041871-3-상기 주어진 레이어에 대해서 선택된 하나의 클럭 신호는 상기 이전 레이어의 제1 주파수 보다 높은 제2 주파수를 갖는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서, 상기 PE들의 이용률은 상기 적어도 하나의 ANN 모델의 복수의 레이어들 각각에 대해서 결정되는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서, 상기 PE들의 이용률은상기 적어도 하나의 ANN 모델의 각 레이어에 대한 연산을 상기 복수의 PE들에게 할당하는데 사용되는 스케줄링정보에 기초하여 결정되는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "SoC(system-on-chip)로서, ANN(artificial neural network) 모델들 중 적어도 하나의 ANN 모델을 위한 연산을 수행할 수 있는 복수의PE(processing element)를 위해 배치된 제1 회로와; 그리고 스위칭 회로를 포함하고,상기 스위칭 회로는: 서로 다른 주파수를 갖는 복수의 클럭 신호들 중 하나의 클럭 신호를 선택하고, 그리고 상기 선택된 하나의 클럭 신호를 상기 NPU에 공급하고,상기 하나의 클럭 신호는 상기 적어도 하나의 ANN 모델 내의 복수의 레이어들 중 주어진 레이어에 대한 상기 PE들의 이용률에 기초하여 선택되는, 시스템."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 하나의 클럭 신호는, 상기 PE들의 이용률에 대한 임계값을 추가로 고려하여 선택되는,SoC."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서, 상기 주어진 레이어에서 연산을 수행할 때의 파워를 줄이기 위하여, 상기 PE들의 이용률에 대한 임계값이 결정되는, SoC."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1항에 있어서, 상기 적어도 하나의 ANN 모델의 상기 주어진 레이어 보다 이전 레이어에 대한 연산을 상기 NPU가 완료했을 때, 상기 하나의 클럭 신호가 선택되는, SoC."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서, 상기 적어도 하나의 ANN 모델의 상기 주어진 레이어 보다 이전 레이어에 대한 연산을 완료함으로써 상기 NPU에의해서 출력되는 이전 레이어에 대한 정보에 기초하여 상기 하나의 클럭 신호가 상기 주어진 레이어에 대해서선택되는, SoC."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항에 있어서, 상기 주어진 레이어에 대한 이용률이 이전 레이어 보다 높은 경우,상기 주어진 레이어에 대해서 선택된 하나의 클럭 신호는 상기 이전 레이어의 제1 주파수 보다 낮은 제2 주파수를 갖는, SoC.공개특허 10-2025-0041871-4-청구항 17 제11항에 있어서, 상기 주어진 레이어에 대한 이용률이 이전 레이어 보다 낮은 경우,상기 주어진 레이어에 대해서 선택된 하나의 클럭 신호는 상기 이전 레이어의 제1 주파수 보다 높은 제2 주파수를 갖는, SOC."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "NPU(neural processing unit)를 구동하는 방법으로서,서로 다른 주파수를 갖는 복수의 클럭 신호들 중 하나의 클럭 신호를 선택하는 단계와;상기 선택된 하나의 클럭 신호를 상기 NPU에 공급하는 단계를 포함하고,상기 NPU는 ANN(artificial neural network) 모델들 중 적어도 하나의 ANN을 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 포함하고,상기 하나의 클럭 신호는 상기 적어도 하나의 ANN 모델 내의 복수의 레이어들 중 주어진 레이어에 대한 상기 PE들의 이용률에 기초하여 선택되는, 방법."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서, 상기 하나의 클럭 신호는, 상기 PE들의 이용률에 대한 임계값을 추가로 고려하여 선택되는,방법."}
{"patent_id": "10-2023-0124805", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제18항에 있어서, 상기 주어진 레이어에서 연산을 수행할 때의 파워를 줄이기 위하여, 상기 PE들의 이용률에 대한 임계값이 결정되는, 방법."}
{"patent_id": "10-2023-0124805", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 개시는 시스템을 제시한다. 상기 시스템은 ANN(artificial neural network) 모델들 중 적어도 하 나의 ANN을 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 포함하는 NPU(neural processing unit)와; 그리고 스위칭 회로를 포함할 수 있다. 상기 스위칭 회로는: 서로 다른 주파수를 갖는 복수의 클럭 신 호들 중 하나의 클럭 신호를 선택하고, 그리고 상기 선택된 하나의 클럭 신호를 상기 NPU에 공급할 수 있다. 상 기 하나의 클럭 신호는 상기 적어도 하나의 ANN 모델 내의 복수의 레이어들 중 주어진 레이어에 대한 상기 PE들 의 이용률에 기초하여 선택될 수 있다."}
{"patent_id": "10-2023-0124805", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 가변 주파수를 이용하여 신경 프로세싱 유닛의 파워를 낮추는 기술에 관한 한 것이다."}
{"patent_id": "10-2023-0124805", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(artificial intelligence: AI)도 점차 발전하고 있다. AI는 인간의 지능, 즉 인식(Recognition), 분 류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정(Control/Decision making) 등을 할 수 있는 지능을 인공적으로 모방하는 것을 의미한다. 또한, 최근에는 인공지능(AI)을 위한 연산 속도를 가속하기 위하여, 신경 프로세싱 유닛 (Neural processing unit; NPU)가 개발되고 있다. 인공지능 서비스의 목적에 따라 여러 타입의 인공신경망(ANN) 모델이 존재할 수 있다. 예를 들어, 입력되는 데 이터가 이미지 또는 영상인 경우, 인공지능 서비스를 위하여 이미지/영상 내의 객체 분류, 객체 검출, 객체 추 적 등을 위한 CNN 타입의 인공신경망모델이 사용될 수 있다. 일반적으로, 인공신경망모델은 레이어 마다 연산량이 다를 수 있다. 특히, 특정 레이어에서 연산량이 크게 증가하면, 전력 소모량이 순간적으로 증가할 수 있다."}
{"patent_id": "10-2023-0124805", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "인공신경망 연산은 데이터 인텐시브(intensive) 한 특성을 가진다. 특히, 인공신경망 연산은 병렬 처리 연산이 필요하다. 즉, 인공신경망 연산은 동시에 방대한 데이터를 빠른 속도로 병렬로 처리하지 못하면 처리 속도가 저하되는 특성이 있다. 이에, 본 개시의 발명자들은 인공신경망 연산에 특화된 신경 프로세싱 유닛을 개발하였다. 본 개시의 발명자들 은 신경 프로세싱 유닛의 복수의 프로세싱 엘리먼트의 개수를 증가시켜서 신경 프로세싱 유닛의 병렬 처리 성능 을 향상시키고자 하였다. 또한 본 개시의 발명자들은 저전력 동작이 가능한 신경 프로세싱 유닛을 개발하고자 하였다. 한편, 엣지 디바이스에 설치되는 저전력 동작에 특화된 신경 프로세싱 유닛의 전원 공급부는 서버나 개인용 컴 퓨터(PC)에서 동작하는 그래픽 처리 장치(GPU)의 전원 공급부에 비해서 상대적으로 전원 공급 능력이 낮을 수 있다. 또한, 엣지 디바이스에 설치되는 저전력 동작에 특화된 신경 프로세싱 유닛의 전원 공급 부의 커패시턴스 의 용량이 순간적인 전원 공급을 감당하기에 부족할 수 있다. 하지만, 저전력 동작에 특화된 신경 프로세싱 유닛의 프로세싱 엘리먼트의 개수가 증가할수록, 순간적으로 공급 전압이 흔들리는 정도가 증가할 수 있다는 사실을 본 개시의 발명자들은 인식하였다. 부연 설명하면, 요구되는 신경 프로세싱 유닛의 순간 전원 공급량은 동작하는 프로세싱 엘리먼트의 개수에 비례할 수 있다. 또한, 인공신경망모델의 연산량은 각 레이어별로 편차가 상당하다. 따라서 인공신경망모델의 레이어의 연산량에 따라서 병렬로 동작하는 프로세싱 엘리먼트의 개수는 상이할 수 있다. 즉, 동시에 많은 프로세싱 엘리먼트들이 동작할 경우, 순간적으로 신경 프로세싱 유닛의 전원 공급부의 전압이 흔들리거나 또는 강하될 수 있다. 또한, 인공신경망모델의 특정 레이어의 연산량은 매우 작을 수 있다. 이러한 경우 신경 프로세싱 유닛의 구동 주파수를 증가시키더라도, 신경 프로세싱 유닛의 공급 전압 안정성이 보장될 수 있다는 것을 본 개시의 발명자 들이 인식하게 되었다. 또한, 공급 전압이 순간적으로 흔들리거나 또는 강하될 경우, 시스템 안전성을 위해서 공급 전압(VDD)을 올려야 할 경우가 발생될 수도 있다. 따라서 시스템 안전성을 확보하지 못하면 불필요하게 공급 전압이 상승되는 문제 가 발생될 수 있다. 공급 전압이 상승될 경우, 신경 프로세싱 유닛의 소비 전력이 급격하게 상승될 수 있는 문 제가 발생될 수 있다. 이러한 경우, 신경 프로세싱 유닛의 구동 주파수를 저감 시켜 신경 프로세시 유닛의 공급 전원부의 전압을 안정시킬 수 있다는 사실을 본 개시의 발명자들이 인식하였다. 이와 반대로, 공급 전압이 안정될수록, 공급 전압(VDD)을 저감 시킬 수 있다. 따라서 신경 프로세싱 유닛의 공 급 전압의 안전성을 확보하면 공급 전압을 저감 시킬 수 있으며, 결과적으로 신경 프로세싱 유닛의 소비 전력을 저감할 수 있다. 이에, 본 개시의 발명자들은 연산량이 급증하더라도 구동 주파수를 조정하여 인공신경망 연산에 특화된 신경 프 로세싱 유닛의 공급 전압의 안정성을 개선시키는 것이 필요하다고 인식하였다. 따라서, 본 개시의 개시들은 특정 연산 단계에서 과도한 파워에 기인한 공급 전원 부의 공급 전압의 떨림을 안 정화시키는 기술적 방안들을 제시하는 것을 목적으로 한다."}
{"patent_id": "10-2023-0124805", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "전술한 목적을 달성하기 위하여, 본 개시의 일 개시는일 예시는 시스템을 제시한다. 상기 시스템은 ANN(artificial neural network) 모델들 중 적어도 하나의 ANN을 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 포함하는 NPU(neural processing unit)와; 그리고 스위칭 회로를 포함할 수 있다. 상기 스위칭 회로는: 서로 다른 주파수를 갖는 복수의 클럭 신호들 중 하나의 클럭 신호를 선택하고, 그리고 상 기 선택된 하나의 클럭 신호를 상기 NPU에 공급할 수 있다. 상기 하나의 클럭 신호는 상기 적어도 하나의 ANN 모델 내의 복수의 레이어들 중 주어진 레이어에 대한 상기 PE들의 이용률에 기초하여 선택될 수 있다. 상기 하나의 클럭 신호는, 상기 PE들의 이용률에 대한 임계값을 추가로 고려하여 선택될 수 있다. 상기 주어진 레이어에서 연산을 수행할 때의 특정 연산 단계의 파워를 줄이기 위하여, 상기 PE들의 이용률에 대 한 임계값이 결정될 수 있다. 상기 적어도 하나의 ANN 모델의 상기 주어진 레이어 보다 이전 레이어에 대한 연산을 상기 NPU가 완료했을 때, 상기 하나의 클럭 신호가 선택될 수 있다. 상기 적어도 하나의 ANN 모델의 상기 주어진 레이어 보다 이전 레이어에 대한 연산을 완료함으로써 상기 NPU에 의해서 출력되는 이전 레이어에 대한 정보에 기초하여 상기 하나의 클럭 신호가 상기 주어진 레이어에 대해서선택될 수 있다. 상기 스위칭 회로는: 다수의 클럭 신호를 출력하는 생성기와; 그리고 상기 하나의 클럭 신호를 선택하는 선택기 를 포함할 수 있다. 상기 주어진 레이어에 대한 이용률이 이전 레이어 보다 높은 경우, 상기 주어진 레이어에 대해서 선택된 하나의 클럭 신호는 상기 이전 레이어의 제1 주파수 보다 낮은 제2 주파수를 가질 수 있다. 상기 주어진 레이어에 대한 이용률이 이전 레이어 보다 낮은 경우, 상기 주어진 레이어에 대해서 선택된 하나의 클럭 신호는 상기 이전 레이어의 제1 주파수 보다 높은 제2 주파수를 가질 수 있다. 상기 PE들의 이용률은 상기 적어도 하나의 ANN 모델의 복수의 레이어들 각각에 대해서 결정될 수 있다. 상기 PE들의 이용률은 상기 적어도 하나의 ANN 모델의 각 레이어에 대한 연산을 상기 복수의 PE들에게 할당하는 데 사용되는 스케줄링 정보에 기초하여 결정될 수 있다. 전술한 목적을 달성하기 위하여, 본 개시의 일 개시는 SoC(system-on-chip)를 제공한다. 상기 SoC는 ANN(artificial neural network) 모델들 중 적어도 하나의 ANN 모델을 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 위해 배치된 제1 회로와; 그리고 스위칭 회로를 포함할 수 있다. 상기 스위칭 회로는: 서로 다른 주파수를 갖는 복수의 클럭 신호들 중 하나의 클럭 신호를 선택하고, 그리고 상기 선택된 하 나의 클럭 신호를 상기 NPU에 공급할 수 있다. 상기 하나의 클럭 신호는 상기 적어도 하나의 ANN 모델 내의 복 수의 레이어들 중 주어진 레이어에 대한 상기 PE들의 이용률에 기초하여 선택될 수 있다. 전술한 목적을 달성하기 위하여, 본 개시의 일 개시는 NPU(neural processing unit)를 구동하는 방법을 제공할 수 있다. 상기 NPU 구동 방법은 서로 다른 주파수를 갖는 복수의 클럭 신호들 중 하나의 클럭 신호를 선택하는 단계와; 상기 선택된 하나의 클럭 신호를 상기 NPU에 공급하는 단계를 포함할 수 있다. 상기 NPU는 ANN(artificial neural network) 모델들 중 적어도 하나의 ANN을 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 포함할 수 있다. 상기 하나의 클럭 신호는 상기 적어도 하나의 ANN 모델 내의 복수의 레이어들 중 주어진 레이어에 대한 상기 PE들의 이용률에 기초하여 선택될 수 있다."}
{"patent_id": "10-2023-0124805", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 예시들에 따르면, 인공신경망 연산이 서로 다른 주파수에 따라 분산 동작되게 되어, 특정 연산 단계 의 파워를 낮출 수 있는 효과가 있다. 본 개시의 예시들에 따르면, 인공신경망 연산이 서로 다른 주파수에 따라 분산 동작되게 되어, 신경 프로세싱 유닛에 공급되는 공급 전압의 안전성을 개선할 수 있는 효과가 있다. 본 개시의 예시들에 따르면, 인공신경망 연산이 복수의 클럭 신호에 따라 분산 동작되게 되어, 신경 프로세싱 유닛에 공급되는 공급 전압을 저감하여 신경 프로세싱 유닛의 전력 소비량을 상당히 저감할 수 있는 효과가 있 다."}
{"patent_id": "10-2023-0124805", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시의 예시들의 특정한 구조적 내지 단계적 설명들은 단지 본 개시의 개념에 따른 예시를 설명하기 위한 것 이다. 따라서 본 개시의 개념에 따른 예시들은 다양한 형태로 실시될 수 있다. 본 개시의 개념에 따른 예시들은 다양한 형태로 실시될 수 있다. 본 개시는 본 개시의 예시들에 한정되는 것으로 해석되어서는 아니 된다. 본 개시의 개념에 따른 예시에 다양한 변경을 가할 수 있고 여러 가지 형태를 가질 수 있다. 이에, 특정 예시들 을 도면에 예시하고 본 개시 또는 출원에 대해서 상세하게 설명하고자 한다. 그러나, 이는 본 개시의 개념에 따 른 예시를 특정한 개시 형태에 대해 한정하려는 것이 아니다. 본 개시의 개념에 따른 여시는 본 개시의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1 및/또는 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만 사용될 수 있다. 상기 용어들은 본 개시의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제1 구성요소 는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있다. 하지만 복수의 구성요소들 중간에 다른 구성요소가 존재할 수도 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거 나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃하는\"과 \"~ 에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 본 개시에서 사용한 용어는 단지 특정한 예시를 설명하기 위해 사용된 것으로, 본 개시를 한정하려는 의도가 아 니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"포함하 다\" 또는 \"가지다\" 등의 용어는 서술된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존 재함을 지정하려는 것이다. 따라서 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미 를 가지는 것으로 해석되어야 한다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 본 개시에 서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 예시를 설명함에 있어서 본 개시가 속하는 기술 분야에 익히 알려져 있고 본 개시와 직접적으로 관련이 없는 기 술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 개시의 요지를 흐리지 않고 더욱 명확히 전달하기 위함이다.<용어의 정의> 이하, 본 개시에서 제시되는 개시들의 이해를 돕고자, 본 개시에서 사용되는 용어들에 대하여 간략하게 정리하 기로 한다. NPU: 신경 프로세싱 유닛(Neural Processing Unit)의 약어로서, CPU(Central processing unit)과 별개로 인공 신경망모델의 연산을 위해 특화된 프로세서를 의미할 수 있다. ANN: 인공신경망(artificial neural network)의 약어로서, 인간의 지능을 모방하기 위하여, 인간 뇌 속의 뉴런 들(Neurons)이 시냅스(Synapse)를 통하여 연결되는 것을 모방하여, 노드들을 레이어(Layer: 계층) 구조로 연결 시킨, 네트워크를 의미할 수 있다. DNN: 심층 신경망(Deep Neural Network)의 약어로서, 보다 높은 인공 지능을 구현하기 위하여, 인공신경망의 은 닉 레이어의 개수를 늘린 것을 의미할 수 있다. CNN: 컨볼루션 신경망(Convolutional Neural Network)의 약어로서, 인간 뇌의 시각 피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망이다. 컨볼루션 신경망은 영상처리에 적합한 것으로 알려져 있으며, 입력 데이 터의 특징들을 추출하고, 특징들의 패턴을 파악하기에 용이한 것으로 알려져 있다. 이하, 첨부한 도면을 참조하여 본 개시의 바람직한 예시를 설명함으로써, 본 개시를 상세히 설명한다. 이하, 본 개시의 예시를 첨부된 도면을 참조하여 상세하게 설명한다. <인공지능> 인간은 인식(Recognition), 분류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정 (Control/Decision making) 등을 할 수 있는 지능을 갖추고 있다. 인공지능(artificial intelligence: AI)은 인간의 지능을 인공적으로 모방하는 것을 의미한다. 인간의 뇌는 뉴런(Neuron)이라는 수많은 신경세포로 이루어져 있다. 각각의 뉴런은 시냅스(Synapse)라고 불리는 연결부위를 통해 수백에서 수천 개의 다른 뉴런들과 연결되어 있다. 인간의 지능을 모방하기 위하여, 생물학적 뉴런의 동작원리와 뉴런 간의 연결 관계를 모델링한 것을, 인공신경망모델이라고 한다. 즉, 인공신경망은 뉴런 들을 모방한 노드들을 레이어(Layer: 계층) 구조로 연결시킨, 시스템이다. 이러한 인공신경망모델은 레이어 수에 따라 '단층 신경망'과 '다층 신경망'으로 구분한다. 일반적인 다층신경망 은 입력 레이어와 은닉 레이어, 출력 레이어로 구성된다. 입력 레이어(input layer)은 외부의 자료들을 받 아들이는 레이어로서, 입력 레이어의 뉴런 수는 입력되는 변수의 수와 동일하다. 은닉 레이어(hidden layer)은 입력 레이어와 출력 레이어 사이에 위치하며 입력 레이어로부터 신호를 받아 특성을 추출하여 출력층 으로 전달한다. 출력 레이어(output layer)은 은닉 레이어로부터 신호를 받아 외부로 출력한다. 뉴런 간의 입력신호는 0에서 1 사이의 값을 갖는 각각의 연결강도와 곱해진 후 합산된다. 이 합이 뉴런의 임계치보다 크면 뉴런이 활성화되어 활성화 함수를 통하여 출력 값으로 구현된다. 한편, 보다 높은 인공 지능을 구현하기 위하여, 인공신경망의 은닉 레이어의 개수를 늘린 것을 심층 신경망 (Deep Neural Network, DNN)이라고 한다. DNN은 다양한 구조로 개발되고 있다. 예를 들면, DNN의 일 예시인 합성곱 신경망(convolutional neural network, CNN)은 입력 값 (영상 또는 이미지)의 특징들을 추출하고, 추출된 출력 값의 패턴을 파악하기에 용이 한 것으로 알려져 있다. CNN은 합성곱 연산, 활성화 함수 연산, 풀링(pooling) 연산 등이 특정 순서로 처리되는 형태로 구성될 수 있다. 예를 들면, DNN의 레이어 각각에서, 파라미터(i.e., 입력 값, 출력 값, 가중치 또는 커널 등)는 복수의 채널로 구성된 행렬일 수 있다. 파라미터는 합성곱 또는 행렬 곱셈으로 NPU에서 처리될 수 있다. 각 레이어에서 연산이 처리된 출력 값이 생성된다. 예를 들면, 트랜스포머(transformer)는 어텐션(attention) 기술에 기반한 DNN이다. 트랜스포머는 행렬 곱셈 (matrix multiplication) 연산을 다수 활용한다. 트랜스포머는 입력 값과 쿼리(query; Q), 키(key; K), 및 값 (value; V) 등의 파라미터를 사용하여 출력 값인 어텐션(Q,K,V)를 획득할 수 있다. 트랜스포머는 출력 값 (즉, 어텐션(Q,K,V))에 기초하여 다양한 추론 연산을 처리할 수 있다. 트랜스포머는 CNN 보다 더 우수한 추론 성능을 보여주는 경향이 있다.도 1은 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 이하 신경 프로세싱 유닛에서 작동될 수 있는 예시적인 인공신경망모델(110a)의 연산에 대하여 설명한다. 도 1의 예시적인 인공신경망모델(110a)은 객체 인식, 음성 인식 등 다양한 추론 기능을 수행하도록 학습된 인공 신경망일 수 있다. 인공신경망모델(110a)은 심층 신경망(DNN, Deep Neural Network)일 수 있다. 단, 본 개시의 예시들에 따른 인공신경망모델(110a)은 심층 신경망에 제한되지 않는다. 예를 들어, 인공신경망모델(110a)은 DaViT, MobileViT, Swin-Transformer, Transformer, YOLO, CNN, PIDNet, BiseNet, RCNN, VGG, VGG16, DenseNet, SegNet, DeconvNet, DeepLAB V3+, U-net, SqueezeNet, Alexnet, ResNet18, MobileNet-v2, GoogLeNet, Resnet-v2, Resnet50, Resnet101, Inception-v3 등의 모델로 구현될 수 있다. 단, 본 개시는 상술한 모델들에 제한되지 않는다. 또한 인공신경망모델(110a)은 적어도 두 개의 서로 다 른 모델들에 기초한 앙상블 모델일 수도 있다. 이하 예시적인 인공신경망모델(110a)에 의해서 수행되는 추론 과정에 대해서 설명하기로 한다. 인공신경망모델(110a)은 입력 레이어(110a-1), 제1 연결망(110a-2), 제1 은닉 레이어(110a-3), 제2 연결망 (110a-4), 제2 은닉 레이어(110a-5), 제3 연결망(110a-6), 및 출력 레이어(110a-7)을 포함하는 예시적인 심층 신경망 모델이다. 단, 본 개시는 도 1에 도시된 인공신경망모델에만 제한되는 것은 아니다. 제1 은닉 레이어 (110a-3) 및 제2 은닉 레이어(110a-5)는 복수의 은닉 레이어로 지칭되는 것도 가능하다. 입력 레이어(110a-1)는 예시적으로, x1 및 x2 입력 노드를 포함할 수 있다. 즉, 입력 레이어(110a-1)는 2개의 입력 값에 대한 정보를 포함할 수 있다. 제1 연결망(110a-2)은 예시적으로, 입력 레이어(110a-1)의 각각의 노드를 제1 은닉 레이어(110a-3)의 각각의 노 드로 연결시키기 위한 6개의 가중치 값에 대한 정보를 포함할 수 있다. 각각의 가중치 값은 입력 노드 값과 곱 해지고, 곱해진 값들의 누산된 값이 제1 은닉 레이어(110a-3)에 저장된다. 가중치 값과 입력 노드 값은 인공신 경망모델의 파라미터로 지칭될 수 있다. 제1 은닉 레이어(110a-3)는 예시적으로 a1, a2, 및 a3 노드를 포함할 수 있다. 즉, 제1 은닉 레이어(110a-3)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 도 1의 제1 프로세싱 엘리먼트(PE1)는 a1 노드의 연산을 처리할 수 있다. 도 1의 제2 프로세싱 엘리먼트(PE2)는 a2 노드의 연산을 처리할 수 있다. 도 1의 제3 프로세싱 엘리먼트(PE3)는 a3 노드의 연산을 처리할 수 있다. 제2 연결망(110a-4)은 예시적으로, 제 1 은닉 레이어(110a-3)의 각각의 노드를 제2 은닉 레이어(110a-5)의 각각의 노드로 연결시키기 위한 9개의 가중 치 값에 대한 정보를 포함할 수 있다. 제2 연결망(110a-4)의 가중치 값은 제1 은닉 레이어(110a-3)로부터 입력 되는 노드 값과 각각 곱해지고, 곱해진 값들의 누산된 값이 제2 은닉 레이어(110a-5)에 저장된다. 제2 은닉 레이어(110a-5)는 예시적으로 b1, b2, 및 b3 노드를 포함할 수 있다. 즉, 제2 은닉 레이어(110a-5)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 도 1의 제4 프로세싱 엘리먼트(PE4)는 b1 노드의 연산을 처리할 수 있다. 도 1의 제5 프로세싱 엘리먼트(PE5)는 b2 노드의 연산을 처리할 수 있다. 도 1의 제6 프로세싱 엘리먼트(PE6)는 b3 노드의 연산을 처리할 수 있다. 제3 연결망(110a-6)은 예시적으로, 제2 은닉 레이어(110a-5)의 각각의 노드와 출력 레이어(110a-7)의 각각의 노 드를 연결하는 6개의 가중치 값에 대한 정보를 포함할 수 있다. 제3 연결망(110a-6)의 가중치 값은 제2 은닉 레 이어(110a-5)로부터 입력되는 노드 값과 각각 곱해지고, 곱해진 값들의 누산된 값이 출력 레이어(110a-7)에 저 장된다. 출력 레이어(110a-7)는 예시적으로 y1, 및 y2 노드를 포함할 수 있다. 즉, 출력 레이어(110a-7)는 2개의 노드 값에 대한 정보를 포함할 수 있다. 도 1의 제7 프로세싱 엘리먼트(PE7)는 y1 노드의 연산을 처리할 수 있다. 도 1의 제8 프로세싱 엘리먼트(PE8)는 y2 노드의 연산을 처리할 수 있다. 각각의 노드는 특징 값에 대응될 수 있으며, 특징 값은 특징맵에 대응될 수 있다. 도 2a은 컨볼루션 신경망(CNN)의 기본 구조를 설명하기 위한 도면이다. 도 2a을 참조하면, 입력 이미지는 특정 사이즈(size)의 행과 특정 사이즈의 열로 구성된 2차원적 행렬로 표시될 수 있다. 입력 이미지는 복수의 채널을 가질 수 있는데, 여기서 채널은 입력 데이터 이미지의 컬러 성분의 수를 나타낼 수 있다. 컨볼루션 과정은 입력 이미지를 지정된 간격으로 순회하면서 커널과 합성곱 연산을 수행하는 것을 의미한다. 컨볼루션 신경망은 현재 레이어의 출력 값(합성곱 또는 행렬 곱셈)을 다음 레이어의 입력 값으로 전달하는 구조 를 가질 수 있다. 예를 들면, 합성곱(컨볼루션)은, 두 개의 주요 파라미터(입력 특징맵 및 커널)에 의해 정의된다. 파라미터는 입 력 특징맵, 출력 특징맵, 활성화 맵, 가중치, 커널, 및 어텐션(Q,K,V) 등을 포함할 수 있다, 합성곱(컨볼루션)은 입력 특징맵 위로 커널 윈도우를 슬라이딩 한다. 커널이 입력 특징맵을 슬라이딩 하는 단차 사이즈를 보폭(stride)이라고 한다. 합성곱 이후에는 풀링(pooling)이 적용될 수 있다. 또한, 컨볼루션 신경망의 끝단에는 FC (fully-connected)레 이어가 배치될 수 있다. 도 2b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 2b을 참조하면, 예시적으로 입력 이미지가 6 x 6 크기를 갖는 2차원적 행렬인 것으로 나타나 있다. 또한, 도 2b에는 예시적으로 3개의 노드, 즉 채널 1, 채널 2, 채널 3이 사용되는 것으로 나타내었다. 먼저, 합성곱 동작에 대해서 설명하기로 한다. 입력 이미지(도 2b에서는 예시적으로 6 x 6 크기인 것으로 나타내어짐)는 첫 번째 노드에서 채널 1을 위한 커널 1(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)과 합성곱되고, 그 결과로서 특징맵1(도 2b에서는 예시적으로 4 x 4 크기인 것으로 나타내어짐)이 출력된다. 또한, 상기 입력 이미지(도 2b에서는 예시적으로 6 x 6 크기인 것으로 나타내어짐)는 두 번째 노드에서 채널 2를 위한 커널 2(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)와 합성곱되고 그 결과로서 특징맵 2(도 2b에서는 예시적으로 4 x 4 크기인 것으로 나타내어 짐)가 출력된다. 또한, 상기 입력 이미지는 세 번째 노드에서 채널 3을 위한 커널 3(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)과 합성곱되고, 그 결과로서 특징맵3(도 2b에서는 예시적으로 4 x 4 크기인 것으 로 나타내어짐)이 출력된다. 각각의 합성곱을 처리하기 위해서 신경 프로세싱 유닛의 프로세싱 엘리먼트들(PE1 to PE12)은 MAC 연산을 수행하도록 구성된다. 다음으로, 활성화 함수의 동작에 대해서 설명하기로 한다. 합성곱 동작으로부터 출력되는 특징맵1, 특징맵2 그리고 특징맵3(도 2b에서는 각각의 크기는 예시적으로 4 x 4 인 것으로 나타내어짐)에 대해서 활성화 함수가 적용될 수 있다. 활성화 함수가 적용되고 난 이후의 출력은 예 시적으로 4 x 4의 크기일 수 있다. 다음으로, 폴링(pooling) 동작에 대해서 설명하기로 한다. 상기 활성화 함수로부터 출력되는 특징맵1, 특징맵2, 특징맵3(도 2b에서는 각각의 크기는 예시적으로 4 x 4인 것으로 나타내어짐)은 3개의 노드로 입력된다. 활성화 함수로부터 출력되는 특징맵들을 입력으로 받아서 폴링 (pooling)을 수행할 수 있다. 상기 폴링이라 함은 크기를 줄이거나 행렬 내의 특정 값을 강조할 수 있다. 폴링 방식으로는 최대값 폴링과 평균 폴링, 최소값 폴링이 있다. 최대값 폴링은 행렬의 특정 영역 안에 값의 최댓값 을 모으기 위해서 사용되고, 평균 폴링은 특정 영역내의 평균을 구하기 위해서 사용될 수 있다. 도 2b의 예시에서는 4 x 4 크기의 특징맵이 폴링에 의하여 2 x 2 크기로 줄어지는 것으로 나타내었다. 구체적으로, 첫 번째 노드는 채널 1을 위한 특징맵1을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출력한다. 두 번째 노드는 채널 2을 위한 특징맵2을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출 력한다. 세 번째 노드는 채널 3을 위한 특징맵3을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출력한다. 전술한 합성곱, 활성화 함수과 폴링이 반복되고 최종적으로는, 도 8a과 같이 fully connected로 출력될 수 있다. 해당 출력은 다시 이미지 인식을 위한 인공신경망으로 입력될 수 있다. 단, 본 개시는 특징맵, 커널의 크 기에 제한되지 않는다. 지금까지 설명한 CNN은 다양한 심층신경망(DNN) 방법 중에서도 컴퓨터 비전(Vision) 분야에서 가장 많이 쓰이는 방법이다. 특히, CNN은 이미지 분류(image classification) 및 객체 검출(objection detection)과 같은 다양한 작업을 수행하는 다양한 연구 영역에서 놀라운 성능을 보였다. <ANN의 연산을 위해 필요한 하드웨어 자원> 도 3은 본 개시의 일 예시에 따른 신경 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 3에 도시된 신경 프로세싱 유닛(neural processing unit, NPU)은 인공신경망을 위한 동작을 수행하도록 특화된 프로세서이다. 인공신경망은 여러 입력 또는 자극이 들어오면 각각 가중치를 곱해 더해주고, 추가적으로 편차를 더한 값을 활 성화 함수를 통해 변형하여 전달하는 인공 뉴런들이 모인 네트워크를 의미한다. 이렇게 학습된 인공신경망은 입 력 데이터로부터 추론(inference) 결과를 출력하는데 사용될 수 있다. 신경 프로세싱 유닛는 전기/전자 회로로 구현된 반도체일 수 있다. 전기/전자 회로라 함은 수많은 전자 소자, (예컨대 트렌지스터, 커패시터)를 포함하는 것을 의미할 수 있다. Transformer 및/또는 CNN 기반의 인공신경망모델인 경우, 신경 프로세싱 유닛는 행렬 곱셈 연산, 합성곱 연산, 등을 인공신경망의 구조(architecture)에 따라 선별하여, 처리할 수 있다. 예를 들어, 합성곱 신경망(CNN)의 레이어 각각에서, 입력 데이터에 해당하는 입력 특징맵(Input feature map)과 가중치(Weight)에 해당하는 커널(kernel)은 복수의 채널로 구성된 텐서(Tensor) 또는 행렬일 수 있다. 입력 특 징맵과 커널의 합성곱 연산이 수행되며, 각 채널에서 합성곱 연산과 풀링 출력 특징맵(output feature map)이 생성된다. 출력 특징맵에 활성화 함수를 적용하여 해당 채널의 활성화맵(activation map)이 생성된다. 이후, 활 성화맵에 대한 풀링이 적용될 수 있다. 여기서 포괄적으로 활성화맵은 출력 특징맵으로 지칭될 수 있다. 이하 설명의 편의를 위해 활성화맵은 출력 특징맵으로 지칭하여 설명한다. 단, 본 개시의 예시들은 이에 제한되지 않으며, 출력 특징맵은 행렬 곱셈 연산 또는 합성곱 연산등이 적용된 것 을 의미한다. 부연 설명하면, 본 개시의 예시들에 따른 출력 특징맵은 포괄적인 의미로 해석되어야 한다. 예를 들면, 출력 특 징맵은 행렬 곱셈 연산 또는 합성곱 연산 결과값일 수 있다. 이에, 복수의 프로세싱 엘리먼트는 추가 알고 리즘을 위한 처리 회로부를 더 포함하도록 변형 실시되는 것도 가능하다. 즉, 후술할 SFU의 일부 회로 유 닛들이 복수의 프로세싱 엘리먼트에 포함되도록 구성되는 것도 가능하다. 신경 프로세싱 유닛는 상술한 인공신경망 연산에 필요한 합성곱 및 행렬 곱셈을 처리하기 위한 복수의 프 로세싱 엘리먼트를 포함하도록 구성될 수 있다. 신경 프로세싱 유닛는 상술한 인공신경망 연산에 필요한 행렬 곱셈 연산, 합성곱 연산, 활성화 함수 연산, 풀링 연산, 스트라이드 연산, 배치 정규화 연산, 스킵 커넥션 연산, 접합 연산, 양자화 연산, 클리핑 연산, 패 딩 연산에 최적화된 각각의 처리 회로를 포함하도록 구성될 수 있다. 예를 들면, 신경 프로세싱 유닛는 상술한 알고리즘들 중 활성화 함수 연산, 풀링 연산, 스트라이드 연산, 배치 정규화 연산, 스킵 커넥션 연산, 접합 연산, 양자화 연산, 클리핑 연산, 패딩 연산 중 적어도 하나를 처리 하기 위한 SFU를 포함하도록 구성될 수 있다. 구체적으로, 신경 프로세싱 유닛는 복수의 프로세싱 엘리먼트(processing element: PE) , SFU, NPU 내부 메모리, NPU 컨트롤러, 및 NPU 인터페이스를 포함할 수 있다. 복수의 프로세싱 엘리먼 트, SFU, NPU 내부 메모리, NPU 컨트롤러, 및 NPU 인터페이스 각각은 수많은 트렌지 스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있 고, 동작에 의해서만 식별될 수 있다. 예컨대, 임의 회로는 복수의 프로세싱 엘리먼트으로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. NPU 컨트롤러는 신경 프로세싱 유닛의 인공신경망 추론 동작을 제어하도록 구성된 제어부 의 기능을 수행하도록 구성될 수 있다. 신경 프로세싱 유닛는 복수의 프로세싱 엘리먼트 및 SFU에서 추론될 수 있는 인공신경망모델의 파라미터를 저장하도록 구성된 NPU 내부 메모리, 및 복수의 프로세싱 엘리먼트, SFU, 및 NPU 내 부 메모리의 연산 스케줄을 제어하도록 구성된 스케줄러를 포함하는 NPU 컨트롤러를 포함할 수 있다. 신경 프로세싱 유닛는 SVC(scalable video coding) 또는 SFC(scalable feature-map coding)를 이용한 인 코딩 및 디코딩 방식에 대응되어 특징맵을 처리하도록 구성될 수 있다. 상기 방식들은 통신 채널 또는 통신 버 스의 실효 대역폭 및 신호대잡음비(signal to noise ratio; SNR)에 따라 가변적으로 데이터 전송량을 가변 하는 기술이다. 즉, 신경 프로세싱 유닛은 인코더 및 디코더를 더 포함하도록 구성되는 것도 가능하다. 복수의 프로세싱 엘리먼트는 인공신경망을 위한 동작의 일부를 수행할 수 있다. SFU는 인공신경망을 위한 동작의 다른 일부를 수행할 수 있다. 신경 프로세싱 유닛는 복수의 프로세싱 엘리먼트와 SFU를 사용하여 인공신경망모델의 연산을 하 드웨어적으로 가속하도록 구성될 수 있다. NPU 인터페이스는 시스템 버스를 통해서 신경 프로세싱 유닛와 연결된 다양한 구성요소들, 예컨대 메 모리와 통신할 수 있다. NPU 컨트롤러는 신경 프로세싱 유닛의 추론 연산을 위한 복수의 프로세싱 엘리먼트의 연산, SFU의 연산 및 NPU 내부 메모리의 읽기 및 쓰기 순서를 제어하도록 구성된 스케줄러를 포함할 수 있 다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 복수의 프로세싱 엘리먼트, SFU, 및 NPU 내부 메모리를 제어하도록 구성될 수 있다. NPU 컨트롤러 내의 스케줄러는 복수의 프로세싱 엘리먼트 및 SFU에서 작동할 인공신경망모델의 구조를 분석하거나 또는 이미 분석된 정보를 제공받을 수 있다. 분석된 정보는 컴파일러에 의해서 생성된 정보 일 수 있다. 예를 들면, 인공신경망모델이 포함할 수 있는 인공신경망의 데이터는 각각의 레이어의 노드 데이터 (즉, 특징맵), 레이어들의 배치 데이터, 지역성 정보 또는 구조에 대한 정보, 각각의 레이어의 노드를 연결하는 연결망 각각의 가중치 데이터 (즉, 가중치 커널) 중 적어도 일부를 포함할 수 있다. 인공신경망의 데이터는 NPU 컨트롤러 내부에 제공되는 메모리 또는 NPU 내부 메모리에 저장될 수 있다. 단, 이에 제한되지 않으 며 인공신경망의 데이터는 NPU 또는 NPU를 포함하는 SoC에 구비된 별도의 캐시 메모리 또는 레지스터 파일에 저 장될 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 신경 프로세싱 유닛이 수행할 인공신경망모델의 연산 순서를 스케줄링 할 수 있다. NPU 컨트롤러 내의 스케줄러는 컴파일 된 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 신경 프로세싱 유닛이 수행할 인공신경망모델의 연산 순서의 스케줄링 정보를 제공받을 수 있다. 예를 들면, 상기 스케줄링 정보는 컴파일러에서 생성된 정보일 수 있다. 컴파일러에서 생성된 스케줄링 정보는 머신 코드(machine code) 또는 이진화 코드(binary code) 등으로 지칭될 수 있다. 즉 NPU 컨트롤러에서 활용되는 스케줄링 정보는 인공신경망모델의 데이터 지역성 정보 또는 구조에 기초하 여 컴파일러에 의해서 생성된 정보일 수 있다. 부연 설명하면, 컴파일러는 인공신경망모델이 가지는 고유한 특성인 인공신경망 데이터 지역성을 얼마나 잘 이 해하고 재구성하는지에 따라 신경 프로세싱 유닛의 스케줄링을 효율적으로 할 수 있다. 부연 설명하면, 컴파일러는 신경 프로세싱 유닛의 하드웨어 구조와 성능을 얼마나 잘 이해하는지에 따라 신경 프로세싱 유닛의 스케줄링을 효율적으로 할 수 있다. 부연 설명하면, 컴파일러에 의해서 인공신경망모델이 신경 프로세싱 유닛에서 실행되도록 컴파일 될 때, 인공신경망 데이터 지역성이 재구성될 수 있다. 인공신경망 데이터 지역성은 인공신경망모델에 적용된 알고리즘 들, 및 프로세서의 동작 특성에 따라서 재구성될 수 있다. 부연 설명하면, 인공신경망 데이터 지역성은 신경 프로세싱 유닛이 해당 인공신경망모델을 처리하는 방식, 예를 들면, 특징맵 타일링, 프로세싱 엘리먼트의 스테이셔너리(Stationary) 방식에 따라 재구성될 수 있다. 부연 설명하면, 인공신경망 데이터 지역성은 신경 프로세싱 유닛의 프로세싱 엘리먼트의 개수, 내부 메모 리의 용량에 따라서 재구성될 수 있다. 부연 설명하면, 인공신경망 데이터 지역성은 신경 프로세싱 유닛과 통신하는 메모리의 대역폭에 따라서 재 구성될 수 있다. 왜냐하면, 상술한 각 요인들에 의해서 동일한 인공신경망모델을 연산 처리하더라도 신경 프로세싱 유닛이 클럭 단위로 매 순간 필요한 데이터의 순서를 상이하게 결정할 수 있기 때문이다. 컴파일러는 인공신경망모델의 연산에 필요한 데이터의 순서는 인공신경망의 레이어, 단위 합성곱 및/또는 행렬 곱의 연산 순서에 기초하여 데이터 지역성을 결정하고, 컴파일 된 머신 코드를 생성할 수 있다. 스케줄러는 머신 코드에 포함된 스케줄링 정보를 활용하도록 구성될 수 있다. NPU 컨트롤러 내의 스케줄러는 스케줄링 정보에 기초하여 인공신경망모델의 레이어의 특징맵 및 가중치 데 이터가 저장된 메모리 어드레스 값을 획득할 수 있다. 예를 들면, NPU 컨트롤러 내의 스케줄러는 메모리에 저장된 인공신경망모델의 레이어의 특징맵 및 가중치 데이터가 저장된 메모리 어드레스 값을 획득할 수 있다. 따라서 NPU 컨트롤러 내의 스케줄러는 구동할 인 공신경망모델의 레이어의 특징맵 및 가중치 데이터를 메인 메모리에서 가져와서 NPU 내부 메모리에 저장할 수 있다. 각각의 레이어의 특징맵은 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. 각각의 가중치 데이터는 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보, 예를 들면, 인공신경망모델의 인공신경망의 레이어들의 배치 데이터, 지역성 정보 또는 구조에 대한 정보에 기초해서 복수 의 프로세싱 엘리먼트의 연산 순서를 스케줄링 정보를 제공받을 수 있다. 스케줄링 정보는 컴파일 단계에 서 생성될 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 스케줄링 된 정보를 기초로 동작 하기 때문에, 일반적인 CPU의 스케줄링 개념과 다르게 동작할 수 있다. 일반적 인 CPU의 스케줄링은 공평성, 효율성, 안정성, 반응 시간 등을 고려하여, 최상의 효율을 낼 수 있도록 동작한다. 즉, 우선 순위, 연산 시간 등을 고려해서 동일 시간내에 가장 많은 프로세싱을 수행하도록 스케줄링 한다. 종래의 CPU는 각 프로세싱의 우선 순서, 연산 처리 시간 등의 데이터를 고려하여 작업을 스케줄링 하는 알고리 즘을 사용하였다. 이와 다르게 NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 에 기초하여 결정된 신경 프로세싱 유닛의 프로세싱 순서대로 신경 프로세싱 유닛를 제어할 수 있다. 더 나아가면, NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및/또는 사용하려는 신경 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 결정된 프로세싱 순서대로 NPU를 구동할 수 있다. 단, 본 개시는 신경 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보에 제한되지 않는다. NPU 컨트롤러 내의 스케줄러는 인공신경망의 데이터 지역성 정보 또는 구조에 대한 정보를 저장하도록 구 성될 수 있다. 즉, NPU 컨트롤러 내의 스케줄러는 적어도 인공신경망모델의 인공신경망의 데이터 지역성 정보 또는 구조 에 대한 정보만 활용하더라도 프로세싱 순서를 결정할 수 있다. 더 나아가서, NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및 신경 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보를 고려하여 신경 프로세싱 유닛(10 0)의 프로세싱 순서를 결정할 수 있다. 또한, 결정된 프로세싱 순서대로 신경 프로세싱 유닛의 프로세싱 최적화도 가능하다. 즉, NPU 컨트롤러는 컴파일러로부터 컴파일 된 머신 코드에 기초하여 동작하도록 구성될 수 있으나, 다른 예에서는 NPU 컨트롤러는 임베디드 컴파일러를 내장하도록 구성되는 것도 가능하다. 상술한 구성에 따르면, 신경 프로세싱 유닛은 다양한 AI 소프트웨어의 프레임워크의 형식의 파일을 입력받아 머신 코드를 생성하도록 구성될 수 있다. 예를 들면, AI 소프트웨어의 프레임워크는 TensorFlow, PyTorch, Keras, XGBoost, mxnet, DARKNET, ONNX 등이 있다. 복수의 프로세싱 엘리먼트는 인공신경망의 특징맵과 가중치 데이터를 연산하도록 구성된 복수의 프로세싱 엘리먼트들(PE1 to PE12)이 배치된 구성을 의미한다. 각각의 프로세싱 엘리먼트는 MAC (multiply and accumulate) 연산기 및/또는 ALU (Arithmetic Logic Unit) 연산기를 포함할 수 있다. 단, 본 개시에 따른 예시 들은 이에 제한되지 않는다. 각각의 프로세싱 엘리먼트는 추가적인 특수 기능을 처리하기 위해 추가적인 특수 기능 유닛을 선택적으로 더 포 함하도록 구성될 수 있다. 예를 들면, 프로세싱 엘리먼트(PE)는 배치-정규화 유닛, 활성화 함수 유닛, 인터폴레이션 유닛 등을 더 포함하 도록 변형 실시되는 것도 가능하다. SFU는 활성화 함수 연산, 풀링(pooling) 연산, 스트라이드(stride) 연산, 배치 정규화(batch- normalization) 연산, 스킵 커넥션(skip-connection) 연산, 접합(concatenation) 연산, 양자화(quantization) 연산, 클리핑(clipping) 연산, 패딩(padding) 연산 등을 인공신경망의 구조(architecture)에 따라 선별하여, 처리하도록 구성된 회로부를 포함할 수 있다. 즉, SFU는 복수의 특수 기능 연산 처리 회로 유닛들을 포함 할 수 있다. 도 3에서는 예시적으로 복수의 프로세싱 엘리먼트들이 도시되었지만, 하나의 프로세싱 엘리먼트 내부에 MAC을 대체하여, 복수의 곱셈기(multiplier) 및 가산기 트리(adder tree)로 구현된 연산기들이 병렬로 배치되어 구성 되는 것도 가능하다. 이러한 경우, 복수의 프로세싱 엘리먼트는 복수의 연산기를 포함하는 적어도 하나의 프로세싱 엘리먼트로 지칭되는 것도 가능하다. 복수의 프로세싱 엘리먼트는 복수의 프로세싱 엘리먼트들(PE1 to PE12)을 포함하도록 구성된다. 도 3에 도 시된 복수의 프로세싱 엘리먼트들(PE1 to PE12)은 단지 설명의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼 트들(PE1 to PE12)의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12)의 개수에 의해서 복수 의 프로세싱 엘리먼트의 사이즈 또는 개수가 결정될 수 있다. 복수의 프로세싱 엘리먼트의 사이즈는 N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 복수의 프로세싱 엘리먼트는 N x M 개의 프로세싱 엘리먼트를 포함할 수 있다. 즉, 프로세싱 엘리먼트는 1개 이상일 수 있다. 복수의 프로세싱 엘리먼트의 사이즈는 신경 프로세싱 유닛이 작동하는 인공신경망모델의 특성을 고려 하여 설계할 수 있다. 복수의 프로세싱 엘리먼트는 인공신경망 연산에 필요한 덧셈, 곱셈, 누산 등의 기능을 수행하도록 구성된 다. 다르게 설명하면, 복수의 프로세싱 엘리먼트는 MAC(multiplication and accumulation) 연산을 수행하 도록 구성될 수 있다. 이하 복수의 프로세싱 엘리먼트 중 제1 프로세싱 엘리먼트(PE1)를 예를 들어 설명한다. 도 4a는 본 개시의 일 예시에 적용될 수 있는 복수의 프로세싱 엘리먼트 중 하나의 프로세싱 엘리먼트를 설명하 는 개략적인 개념도이다. 본 개시의 일 예시에 따른 신경 프로세싱 유닛은 복수의 프로세싱 엘리먼트, 복수의 프로세싱 엘리먼 트에서 추론될 수 있는 인공신경망모델을 저장하도록 구성된 NPU 내부 메모리 및 복수의 프로세싱 엘 리먼트 및 NPU 내부 메모리를 제어하도록 구성된 NPU 컨트롤러를 포함하고, 복수의 프로세싱 엘 리먼트는 MAC 연산을 수행하도록 구성되고, 복수의 프로세싱 엘리먼트는 MAC 연산 결과를 양자화해서 출력하도록 구성될 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. NPU 내부 메모리은 메모리 사이즈와 인공신경망모델의 데이터 사이즈에 따라 인공신경망모델의 전부 또는 일부를 저장할 수 있다. 제1 프로세싱 엘리먼트(PE1)는 곱셈기, 가산기, 누산기, 및 비트 양자화 유닛을 포함할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않으며, 복수의 프로세싱 엘리먼트는 인공신경망의 연산 특성을 고려하여 변형 실시될 수도 있다. 곱셈기는 입력 받은 (N)bit 데이터와 (M)bit 데이터를 곱한다. 곱셈기의 연산 값은 (N+M)bit 데이터 로 출력된다. 곱셈기는 하나의 변수와 하나의 상수를 입력 받도록 구성될 수 있다. 누산기는 (L)loops 횟수만큼 가산기를 사용하여 곱셈기의 연산 값과 누산기의 연산 값을 누산 한다. 따라서 누산기의 출력부와 입력부의 데이터의 비트 폭은 (N+M+log2(L))bit로 출력될 수 있다. 여기서 L은 0보다 큰 정수이다. 누산기는 누산이 종료되면, 초기화 신호(initialization reset)를 입력 받아서 누산기 내부에 저장된 데이터를 0으로 초기화 할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 비트 양자화 유닛은 누산기에서 출력되는 데이터의 비트 폭을 저감할 수 있다. 비트 양자화 유닛 은 NPU 컨트롤러에 의해서 제어될 수 있다. 양자화된 데이터의 비트 폭은 (X)bit로 출력될 수 있다. 여기서 X는 0보다 큰 정수이다. 상술한 구성에 따르면, 복수의 프로세싱 엘리먼트는 MAC 연산을 수행하도 록 구성되고, 복수의 프로세싱 엘리먼트는 MAC 연산 결과를 양자화해서 출력할 수 있는 효과가 있다. 특히 이러한 양자화는 (L)loops가 증가할수록 소비 전력을 더 절감할 수 있는 효과가 있다. 또한 소비 전력이 저감되 면 발열도 저감할 수 있는 효과가 있다. 특히 발열을 저감하면 신경 프로세싱 유닛의 고온에 의한 오동작 발생 가능성을 저감할 수 있는 효과가 있다. 비트 양자화 유닛의 출력 데이터(X)bit은 다음 레이어의 노드 데이터 또는 합성곱의 입력 데이터가 될 수 있다. 만약 인공신경망모델이 양자화되었다면, 비트 양자화 유닛은 양자화된 정보를 인공신경망모델에서 제공받도록 구성될 수 있다. 단, 이에 제한되지 않으며, NPU 컨트롤러는 인공신경망모델을 분석하여 양자 화된 정보를 추출하도록 구성되는 것도 가능하다. 따라서 양자화된 데이터 사이즈에 대응되도록, 출력 데이터 (X)bit를 양자화 된 비트 폭으로 변환하여 출력될 수 있다. 비트 양자화 유닛의 출력 데이터(X)bit는 양자 화된 비트 폭으로 NPU 내부 메모리에 저장될 수 있다. 본 개시의 일 예시에 따른 신경 프로세싱 유닛의 복수의 프로세싱 엘리먼트는 곱셈기, 가산기 , 및 누산기를 포함한다. 비트 양자화 유닛은 양자화 적용 여부에 따라 취사 선택될 수 있다. 다른 예시에서는, 비트 양자화 유닛은 SFU에 포함되도록 구성되는 것도 가능하다. 도 4b는 본 개시의 일 예시에 적용될 수 있는 SFU를 설명하는 개략적인 개념도이다. 도 4b를 참고하면 SFU는 여러 기능 유닛을 포함한다. 각각의 기능 유닛은 선택적으로 동작될 수 있다. 각 각의 기능 유닛은 선택적으로 턴-온되거나 턴-오프될 수 있다. 즉, 각각의 기능 유닛은 설정이 가능하다. 다시 말해서, SFU는 인공신경망 추론 연산에 필요한 다양한 회로 유닛들을 포함할 수 있다. 예를 들면, SFU의 회로 유닛들은 건너뛰고 연결하기(skip-connection) 동작을 위한 기능 유닛, 활성화 함 수(activation function) 동작을 위한 기능 유닛, 풀링(pooling) 동작을 위한 기능 유닛, 양자화 (quantization) 동작을 위한 기능 유닛, NMS(non-maximum suppression) 동작을 위한 기능 유닛, 정수 및 부동 소수점 변환(INT to FP32) 동작을 위한 기능 유닛, 배치 정규화(batch-normalization) 동작을 위한 기능 유닛, 보간법(interpolation) 동작을 위한 기능 유닛, 연접(concatenation) 동작을 위한 기능 유닛, 및 바이아스 (bias) 동작을 위한 기능 유닛 등을 포함할 수 있다. SFU의 기능 유닛들은 인공신경망모델의 데이터 지역성 정보에 의해서 선택적으로 턴-온되거나 혹은 턴-오 프될 수 있다. 인공신경망모델의 데이터 지역성 정보는 특정 레이어를 위한 연산이 수행될 때, 해당 기능 유닛 의 턴-오프 혹은 턴-오프와 관련된 제어 정보를 포함할 수 있다. SFU의 기능 유닛들 중 활성화된 유닛은 턴-온 될 수 있다. 이와 같이 SFU의 일부 기능 유닛을 선택적 으로 턴-오프하는 경우, 신경 프로세싱 유닛의 소비 전력을 절감할 수 있다. 한편, 일부 기능 유닛을 턴- 오프하기 위하여, 파워 게이팅(power gating)을 이용할 수 있다. 또는, 일부 기능 유닛을 턴-오프하기 위하여, 클럭 게이팅(clock gating)을 수행할 수도 있다. 도 5는 도 3에 도시된 신경 프로세싱 유닛의 변형예를 나타낸 예시도이다. 도 5에 도시된 신경 프로세싱 유닛은 도 3에 예시적으로 도시된 프로세싱 유닛과 비교하면, 복수의 프로세싱 엘리먼트를 제외하곤 실질적으로 동일하기 때문에, 이하 단지 설명의 편의를 위해서 중복 설명은 생략할 수 있다. 도 5에 예시적으로 도시된 복수의 프로세싱 엘리먼트는 복수의 프로세싱 엘리먼트들(PE1 to PE12) 외에, 각각의 프로세싱 엘리먼트들(PE1 to PE12)에 대응되는 각각의 레지스터 파일들(RF1 to RF12)을 더 포함할 수 있 다. 도 5에 도시된 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12)은 단지 설명 의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12) 의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12)의 개수에 의해서 복수의 프 로세싱 엘리먼트의 사이즈 또는 개수가 결정될 수 있다. 복수의 프로세싱 엘리먼트 및 복수의 레지스 터 파일들(RF1 to RF12)의 사이즈는 N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 복수의 프로세싱 엘리먼트의 어레이 사이즈는 신경 프로세싱 유닛이 작동하는 인공신경망모델의 특성 을 고려하여 설계할 수 있다. 부연 설명하면, 레지스터 파일의 메모리 사이즈는 작동할 인공신경망모델의 데이 터 사이즈, 요구되는 동작 속도, 요구되는 소비 전력 등을 고려하여 결정될 수 있다. 신경 프로세싱 유닛의 레지스터 파일들(RF1 to RF12)은 프로세싱 엘리먼트들(PE1 to PE12)과 직접 연결된 정적 메모리 유닛이다. 레지스터 파일들(RF1 to RF12)은 예를 들면, 플립플롭, 및/또는 래치 등으로 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 대응되는 프로세싱 엘리먼트들(PE1 to PE12)의 MAC 연산 값을 저장하도 록 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 NPU 내부 메모리와 가중치 데이터 및/또는 노드 데이 터를 제공하거나 제공받도록 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 MAC 연산 시 누산기의 임시 메모리의 기능을 수행하도록 구성되는 것도 가능하 다. <본 개시의 발명자에 의해 찾아진 기술적 난관> 인공지능 서비스의 목적에 따라 여러 타입의 인공신경망(ANN) 모델이 존재할 수 있다. 예를 들어, 입력되는 데 이터가 이미지 또는 영상인 경우, 인공지능 서비스를 위하여 이미지/영상 내의 객체 분류, 객체 검출, 객체 추 적 등을 위한 CNN 타입의 인공신경망모델이 사용될 수 있다. 일반적으로, 인공신경망모델은 레이어 마다 연산량이 다를 수 있다. 이를 도 6을 참조하여 설명하기로 한다. 도 6a은 예시적인 인공신경망모델 내의 각 레이어 별 데이터의 크기를 나타낸 예시도이다. 도 6b는 도 6a에 도 시된 예시적 인공신경망모델에서 각 레이어 별 데이터 사이즈 등을 나타낸 예시적인 테이블이다. 도 6a에 도시된 예시적인 인공신경망모델은 Mobilenet V1인 것으로 나타나 있다. 도 6a에 도시된 가로축은 예시 적인 인공신경망모델에서 레이어들을 순차적으로 나타내고, 세로축은 데이터의 크기를 나타낸다. 도 6a에 도시된 레이어 1을 참고하면, 입력 특징맵의 크기(IFMAP_SIZE) 보다 출력 특징맵의 크기(OFMAP_SIZE)가 더 큰 것을 알 수 있다. 레이어 1의 출력 특징맵은 레이어 2로 전달되어, 상기 레이어 2의 입력 특징맵이 된다. 상기 레이어 2의 연산이 마쳐지면, 출력 특징맵이 출력된다. 상기 레이어 2의 출력 특징맵은 다시 레이어 3로 전달되어, 상기 레이어 3 의 입력 특징맵이 된다. 이처럼 각 레이어에 입력되는 입력 데이터의 크기와 각 레이어에서 출력되는 출력 특징맵의 크기는 상이할 수 있다. 이에 따라 임의 레이어에서는 연산량이 작을 수도 있지만 다른 레이어에서는 연산량이 매우 클 수 있다. 이처럼 레이어 별 연산량이 큰 폭으로 변화됨에 따라, 전력 제어가 쉽지 않게 되는 문제가 발생한다. 각 레이어는 신경 프로세싱 유닛의 프로세싱 엘리먼트의 개수 및 NPU 내부 메모리의 용량 한계에 의해서 복수의 연산 단계로 나누어져서 처리될 수 있다. 이에, 신경 프로세싱 유닛은 각 레이어를 복수의 타일로 나누어 복수의 연산 단계를 처리하도록 스케줄링 할 수 있다. 상기 스케줄링은 컴파일러에서 수행될 수 있다. 예를 들면, 하나의 레이어는 4개의 타일로 분할될 수 있다. 각각의 타일은 신경 프로세싱 유닛에서 순차적으로 처리될 수 있다. 컴파일 된 인공신경망모델은 컴파일 될 때 결정된 연산 순서 정보를 스케줄러에 저장할 수 있 다. 이때 각 연산 단계 마다 PE 이용률이 정보가 제공될 수 있다 각 레이어의 연산량은 MAC으로 알 수 있다. 각 레이어별 로 연산량은 최대 227배 차이가 나는 것을 알 수 있다. 신경 프로세싱 유닛은 각 레이어의 연산 단계마다 MAC 연산량에 비례하여 복수의 프로세싱 엘리먼트 중 가동되는 프로세싱 엘리먼트의 개수를 결정할 수 있다. 그리고 가동되는 프로세싱 엘리먼트의 개수에 비례하 여 전력 소비량이 증가할 수 있다. 따라서, 신경 프로세싱 유닛에서 처리되는 인공신경망모델의 각 레이어의 각 연산 단계마다 PE 이용률이 계산될 수 있다. 또한 신경 프로세싱 유닛이 처리하는 인공신경망모델의 구조가 변경되지 않는 한, 각 연 산 단계 별 PE 이용률은 반복적으로 정확히 예측될 수 있다. 즉, 신경 프로세싱 유닛은 특정 인공신경망모 델을 반복적으로 추론할 수 있다. 이러한 경우, 신경 프로세싱 유닛은 동일한 가중치 파라미터와 동일한 네트워크 레이어 구조를 반복 사용할 수 있다는 것을 본 개시의 발명자들이 인식하였다. 또한, 인접한 레이어들의 MAC 연산량 차이가 클수록, 인접한 레이어들 사이의 피크 파워의 편차가 증가할 수 있 다. 그리고 인접한 레이어들의 피크 파워의 편차가 클수록 공급 전압(VDD)이 더 흔들릴 수 있다. 이때, 공급 전 압(VDD)의 안정성을 고려하여 구동 주파수를 가변 할 수 있다는 사실을 본 개시의 발명자들이 인식하였다. 특히, 특정 레이어의 연산 단계에서 연산량이 크게 증가하면, 순간 전력 소모량이 증가함으로써, 시스템 안정성 이 저하되는 문제가 발생할 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. 예를 들어, 특정 레이어의 연산 단계의 연산을 위해서 동시에 많은 프로세싱 엘리먼트들이 가동될 수 있다. 각 각의 프로세싱 엘리먼트 구동을 위해서 소정의 전원이 필요하며, 상당한 개수의 프로세싱 엘리먼트들이 동시에 구동을 하면 순간적으로 필요한 파워가 급증할 수 있다. 만약 신경 프로세싱 유닛이 저전력 동작에 특화되어 설 계될 경우, 서버용 신경 프로세싱 유닛보다 파워 공급 능력이 상대적으로 부족할 수 있다. 따라서, 이러한 엣지 디바이스용 신경 프로세싱 유닛은 순간 파워 공급 이슈에 상대적으로 더 취약할 수 있으며, 파워 공급량이 폭증 할 경우, 공급 전압(VDD)이 흔들릴 수 있다. 특히 공급 전압(VDD)이 트랜지스터의 임계 전압 이하로 떨어질 경 우, 트랜지스터에 저장된 데이터가 손실될 수 있다. 다르게 설명하면, 공급 전압(VDD)이 낮아지는 경우 트랜지 스터의 동작 속도가 저하되어 setup/hold violation 문제가 발생해 오동작이 일어날 수 있다. 특히 이러한 현상 은 반도체 파운더리의 공정이 3nm, 4nm, 5nm, 7nm 와 같이 낮아질수록 더 심화될 수 있다. 다른 예를 들어, 인공신경망을 위한 연산, 즉 예컨대 가산(add), 곱셈(multiply), 누산(accumulate)을 수행하는 PE들이 전력 소모를 순간적으로 많이 사용함으로써, 신경 프로세싱 유닛 내의 다른 컴포넌트, 예컨대 내부 메모리에는 충분한 전력이 공급되지 못할 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. 구체적 으로 내부 메모리에 충분한 전력이 공급되지 못하면, 저장되어 있는 데이터 비트가 손상(compromise)될 가 능성도 배제할 수 없는 문제점이 있을 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. 더 나아가서, 특정 연산 단계에서 PE들의 이용률이 낮을 경우, 신경 프로세싱 유닛의 구동 주파수를 증가시키더 라도, 신경 프로세싱 유닛의 전압 안정성이 보장될 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. <본 특허의 개시들> 전술한 바와 같이, 신경 프로세싱 유닛이 처리하는 인공신경망모델은 레이어 별로 연산량이 급격하게 변화하기 때문에, 파워(W)도 크게 변화되면서 시스템 안정성이 저하되는 문제가 발생할 수 있다는 것을 본 개시의 발명자 들이 인식하게 되었다. 종래에는 신경 프로세싱 유닛을 설계함에 있어서 평균 소모 전력을 낮추는 것에만 중점을 두었지만, 본 개시의 발명자들은 특정 연산 단계의 파워를 줄이는 것이 더 중요하다는 것을 인식하게 되었다. 즉, 특정 연산 단계의 파워를 조절하여 공급 전압(VDD)의 안정화가 중요하다는 것을 인식하게 되었다. 따라서, 본 개시의 발명자들은, ANN 모델의 레이어의 연산 단계 별로 연산량이 급격하게 변화하더라도, 각 연산 단계별 파워의 편차가 저감되도록 하는 기법을 발명하게 되었다. 본 개시의 발명자들이 발명한 기법이 구현되는 여러 예시들을 이하 도면을 참고하여 설명하기로 한다. 도 7a는 본 개시의 일 예시에서 제시되는 기법을 간략하게 나타낸 예시도이다. 도 7b는 본 개시의 일 예시에서 제시되는 기법에서 사용되는 클럭 신호의 예를 나타낸 예시도이다. 도 7a을 참고하면, 본 개시의 일 예시에 따르면, 서로 다른 주파수를 갖는 복수의 클럭 신호들이 생성될 수 있 다(S710).예를 들면, 서로 다른 주파수를 갖는 복수의 클럭 신호들은 동시에 생성될 수 있다. 예를 들면, 서로 다른 주 파수를 갖는 복수의 클럭 신호들은 순차적으로 생성될 수 있다. 예를 들면, 서로 다른 주파수를 갖는 복수의 클럭 신호들은 동시 및/또는 순차적으로 생성될 수 있다. 예를 들면, 서로 다른 주파수를 갖는 복수의 클럭 신호들 중 하나의 클럭 신호가 선택되어 제공될 수 있다. 서 로 다른 주파수를 갖는 복수의 클럭 신호들 중 복수의 클럭 신호들이 동시에 선택되어 제공될 수 있다. 예를 들면, 상기 복수의 클럭 신호들은 도 7b에 도시된 바와 같이 제1 클럭 신호, 제2 클럭 신호, 제3 클럭 신 호일 수 있다. 도 7b에 도시된 바와 같이 제1 클럭 신호는 제2 클럭 신호 보다 주파수가 높을 수 있다. 도 7b에 도시된 바와 같이 제2 클럭 신호는 제3 클럭 신호 보다 주파수가 높을 수 있다. 주파수가 높다는 것은 동일 시간 구간 내에서 더 고속으로 신호가 변화된다는 의미이다. 반대로 말하면, 제2 클 럭 신호는 제1 클럭 신호 보다 주파수가 낮을 수 있다. 제3 클럭 신호는 제2 클럭 신호 보다 주파수가 낮을 수 있다. 예를 들면, 제1 클럭 신호는 1.2GHz일 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. 예를 들면, 제2 클럭 신호는 1.0GHz일 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. 예를 들면, 제3 클럭 신호는 800MHz일 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. 예를 들면, 제1 클럭 신호는 1.2GHz일 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. 예를 들면, 제2 클럭 신호는 1.1GHz일 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. 예를 들면, 제3 클럭 신호는 1.0MHz일 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. 예를 들면, 제1 클럭 신호의 주파수는 제2 클럭 신호의 주파수의 2배의 주파수를 가질 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. 예를 들면, 제2 클럭 신호의 주파수는 제3 클럭 신호의 주파수의 2배의 주파수 를 가질 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. 또한, 신경 프로세싱 유닛(NPU)에 의해서 처리되는 인공신경망 (ANN) 모델 내의 복수의 레이어들 중 주어진 레 이어의 연산 단계에 대한 연산량 또는 PE들의 이용률이 획득될 수 있다(S720). PE들의 이용률은 복수의 PE들의 개수 대비 작동하는 PE들의 비율을 의미한다. 예를 들면, 복수의 PE들은 1000개 이고, 해단 연산 단계에 동원되는 PE들이 700개일 경우, 이용률은 70%가 될 수 있다. PE들의 이용률은 연산 단 계별로 달라질 수 있다. 즉, 적어도 하나의 인공신경망모델에 관하여 각 연산 단계별 PE들의 이용률이 프로파일 링 될 수 있다. 예를 들면, 일정 기간 동안의 인접한 복수의 연산 단계들의 PE들의 이용률의 평균이 계산될 수 있다. 상기 복수의 PE들은 NPU 코어(core), NPU 엔진(engine), NPU 쓰레드(thread) 등으로 지칭되는 것도 가능하다. 상기 NPU 코어, NPU 엔진, NPU 쓰레드 등은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이 들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 신경 프로세싱 유닛은 복수의 NPU 코어, NPU 엔진, NPU 쓰레드를 포함하도록 구성되는 것도 가능하다. 상기 복수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함할 수 있다. 그러면, 상기 연산량 또는 PE들의 이용률에 기초하여, 상기 복수의 클럭 신호들 중 하나가 선택된 후(S730), 공 급될 수 있다(S740). 상기 하나의 클럭 신호는, 상기 연산량 또는 PE들의 이용률에 대한 임계값을 추가로 고려하여 선택될 수 있다. 즉, 상기 하나의 클럭 신호의 주파수는 상기 연산량 또는 PE들의 이용률에 대한 임계값을 기초로 선택될 수 있 다. 상기 주어진 레이어의 연산 단계에서 연산을 수행할 때의 공급 전압(VDD)의 흔들림을 줄이기 위하여, 상기 주어진 레이어의 연산 단계에 대한 연산량 또는 상기 PE들의 이용률에 대한 임계값이 결정될 수 있다. 각 연산 단계별 PE들의 이용률은 신경 프로세싱 유닛이 처리하는 인공신경망모델의 각 레이어의 연산 단계에 대 응되는 파라미터의 크기 및 MAC 연산량을 기초로 계산될 수 있다. 예를 들며, 특정 레이어의 연산 단계의 입력 특징맵의 파라미터 크기가 크고, 커널의 크기 및 커널의 채널의 개수가 클 경우, 해당 레이어의 연산 단계에 대 응되는 PE들의 이용률은 높을 수 있다. 인공신경망모델의 각 레이어의 특성은 입력 특징맵(Input Feature map, IF) 및/또는 가중치(weight)의 모양 (shape)을 포함할 수 있다. 입력 특징맵(Input Feature map, IF) 및/또는 가중치(weight)의 모양(shape)이란높이(height), 넓이(width), 깊이(depth), 채널(channel)으로 결정될 수 있다. 인공신경망의 입력 특징맵은 복수 개의 작은 타일(tile)로 분할될 수 있고, 각 타일은 커널과의 연산을 통해 특 징을 추출할 수 있다. 타일 별로 MAC 연산 시간을 최소화할 수 있는 Systolic array 구조가 상이할 수 있다. 컨볼루션 신경망에서, 대부분의 연산 시간은 컨볼루션 동작이 차지한다. 컨볼루션 동작을 위한 MAC 연산의 수는 아래의 수학식 1과 같은 알고리즘으로 계산될 수 있다. [수학식 1] for (m=0; m<M; m++) // Num filters for (e=0; e<E; e++) // num_conv_window(height) for (f=0; f<F; f++) // num_conv_window(width) for (c=0; c<C; c++) // Elements per filter(channel) for (r=0; r<R; r++) // Elements per filter(height) for (s=0; s<S; s++) // Elements per filter(width) { output[m][e][f] = input[c][e+r][f+s] * weight[m][c][r][s] time = time + 1; } 수학식 1을 참조하면, m은 필터의 개수, e는 출력 특징맵의 높이, f는 출력 특징맵의 넓이, c는 채널의 수, r은 필터의 높이, s는 필터의 넓이를 각각 나타내는 변수이다. M은 필터의 개수, E는 출력 특징맵의 높이, F는 출력 특징맵의 넓이, C는 채널의 수, R은 필터의 높이, S는 필터의 넓이를 각각 나타내는 상수이다. 수학식 1에 따른 알고리즘은 각 stationary 기법에 따라 각기 다른 for-loop를 병렬로 연산할 수 있다. Output stationary 모드에 따른 systolic array 구조인 경우, 수학식 1에 따른 알고리즘에서 첫 번째 내지 세 번째 for-loop를 루프 언롤링(loop-unrolling)을 수행할 수 있다. 구체적으로, Output stationary 모드에 따른 Systolic array 구조인 경우, 첫 번째의 for-loop(Num filters)를 프로세싱 엘리먼트 어레이의 행(row)의 수만 큼 병렬로 연산하고, 두 번째 for-loop(num_conv_window(height))와 세 번째 for- loop(num_conv_window(width))를 프로세싱 엘리먼트 어레이의 열(column)의 수만큼 병렬로 연산할 수 있다. 본 개시의 일 예시에 따른 신경 프로세싱 유닛은, 입력 특징맵과 1개의 커널을 입력으로 하는 MAC 연산을 1 cycle이라고 했을 때, 프로세싱 엘리먼트의 개수, 입력 특징맵 및/또는 가중치를 이용하여 총 MAC 연산 시간을 계산할 수 있다. 따라서 PE의 이용률이 계산될 수 있다. 상기 PE 이용률 프로파일링은 인공신경망모델의 컴파일 단계 또는 런타임 중에 수행될 수 있다. 예를 들어 아래의 표 1과 같이 특정 레이어의 특정 연산 단계의 PE 이용률이 임계값 90%를 넘는 경우, 제2 클럭 신호가 제공될 수 있다. 표 1 PE 이용률에 대한 임계값 클럭 신호 50% 제1 클럭 신호 70% 제2 클럭 신호 90% 제3 클럭 신호 상기 적어도 하나의 ANN 모델의 상기 주어진 레이어의 연산 단계 보다 이전 레이어의 연산 단계에 대한 연산을 상기 신경 프로세싱 유닛이 완료했을 때, 상기 하나의 클럭 신호가 선택될 수 있다. 즉, 다음 연산 단계의 PE 이용률을 고려한 최적의 주파수의 클럭 신호가 선택될 수 있다. 상기 클럭 신호의 주파수는 특정 연산 단계의 파워를 고려하여 선택된 주파수 일 수 있다. 또한 상기 클럭 신호의 주파수는 공급 전압(VDD)의 안정성을 고려 하여 선택된 주파수 일 수 있다.신경 프로세싱 유닛은 NPU 컨트롤러의 스케줄러에 저장된 인공신경망모델의 연산 단계 정보에 기초하 여 다음 연산 단계의 연산량 또는 상기 복수의 PE 이용률을 사전에 획득할 수 있다. 상기 주어진 레이어의 연산 단계에 대한 연산량 또는 상기 PE들의 이용률이 이전 레이어의 연산 단계 보다 높은 경우, 상기 주어진 레이어의 연산 단계에 대해서 선택된 하나의 클럭 신호는 상기 이전 레이어의 연산 단계의 제1 주파수 보다 낮은 제2 주파수를 가질 수 있다. 예를 들어, 상기 이전 레이어의 연산 단계를 위한 클럭 신호가 도 7b에 도시된 제1 클럭 신호였다면, 상기 주어 진 레이어의 연산 단계에 대해서 선택되는 클럭 신호는 도 7b에 도시된 제2 클럭 신호일 수 있다. 상기 주어진 레이어의 연산 단계에 대한 연산량 또는 상기 PE들의 이용률이 이전 레이어의 연산 단계 보다 낮은 경우, 상기 주어진 레이어의 연산 단계에 대해서 선택된 하나의 클럭 신호는 상기 이전 레이어의 연산 단계의 제1 주파수 보다 높은 제2 주파수를 가질 수 있다. 예를 들어, 상기 이전 레이어의 연산 단계를 위한 클럭 신호가 도 7b에 도시된 제2 클럭 신호였다면, 상기 주어 진 레이어의 연산 단계에 대해서 선택되는 클럭 신호는 도 7b에 도시된 제1 클럭 신호일 수 있다. 상기 주어진 레이어의 연산 단계에 대한 연산량 또는 상기 PE들의 이용률은, 상기 적어도 하나의 ANN 모델의 복 수의 레이어의 연산 단계들 각각에 대해서 결정될 수 있다. 상기 주어진 레이어의 연산 단계에 대한 연산량 또는 상기 PE들의 이용률은, 상기 적어도 하나의 ANN 모델의 각 레이어의 연산 단계에 대한 연산을 상기 복수의 PE들에게 할당하는데 사용되는 스케줄링 정보에 기초하여 결정 될 수 있다. 예를 들면, 적어도 하나의 인공신경망모델에 관하여 각 연산 단계별 PE들의 이용률의 프로파일을 활용할 수 있 다. 상기 프로파일을 기초로 각 연산 단계별 임계값을 기초로 해당 연산 단계에 복수의 PE들에 적용할 클럭 신 호의 주파수가 결정될 수 있다. 인공신경망모델의 각 연산 단계의 정보는 스케줄링 정보에 포함될 수 있다. 따 라서 각 연산 단계별 PE들의 이용률의 프로파일 정보는 스케줄링 정보를 기초로 획득될 수 있다. 도 7c는 본 개시의 일 예시에서 제시되는 기법을 구현하기 위한 하드웨어 구성을 간략하게 나타낸 개략도이다. 도 7c를 참조하면, 스위칭 회로와 하나 이상의 신경 프로세싱 유닛이 나타나 있다. 상기 하나 이상의 신경 프로세싱 유닛은 도 7b에서는 예시적으로 2개(100-1, 100-2)인 것으로 나타나 있다. 스위칭 회로는 특정 주파수 영역의 클럭 신호를 출력하도록 설계된 회로를 의미한다. 예를 들면, 스위칭 회로는 선택 신호에 기초하여 600MHz 내지 2.8GHz의 주파수 중 하나의 주파수를 선택하여 출력하도록 설계 된 회로를 의미한다. 단 본 개시의 일 예시는 스위칭 회로의 출력 주파수에 제한되지 않는다. 예를 들면, 스위칭 회로는 적어도 하나의 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더(divider) 회로, 클럭 게이팅(gating), 오실레이터(oscillator), 위상 루프 (PLL) 회로, 선택기 중 적어도 하나를 포함한 회로일 수 있다. 예를 들면, 스위칭 회로는 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더 (divider) 회로, 클럭 게이팅(gating) 회로, 오실레이터(oscillator) 회로, 위상 루프(PLL) 회로, 선택기 중 적어도 두개가 병합된 회로일 수 있다. 각 회로의 다양한 조합으로 다양한 주파수를 생성하는 기술은 공지된 기 술이기 때문에 자세한 설명은 이하 생략한다. 예를 들면, 스위칭 회로는 적어도 하나의 주파수의 클럭 신호를 생성하도록 설계된 회로일 수 있다. 예를 들면, 스위칭 회로는 적어도 하나의 주파수의 클럭 신호를 입력 받도록 설계된 회로일 수 있다. 이에, 스위칭 회로는 선택 신호에 따라서 다양한 주파수의 클럭 신호를 출력할 수 있다. 예를 들면, 스위 칭 회로는 선택 신호에 따라 1.2GHz, 1.1GHz, 1.0GHz, 900MHz, 800MHz 주파수들 중 적어도 하나를 주파수 의 클럭 신호를 출력할 수 있다. 스위칭 회로는 선택 신호에 따라 미리 설정된 특정 주파수들 중 특정 주 파수를 출력할 수 있다. 단 본 개시의 예시들은 스위칭 회로의 주파수에 제한되지 않으며, 스위칭 회로 는 다양한 주파수를 출력하도록 설계될 수 있다. 스위칭 회로는 선택 신호에 기초하여 특정 주파수의 클럭 신호를 출력하도록 설계될 수 있다. 특정 주파수 는 프리셋(preset)된 주파수들일 수 있다. 또는 특정 주파수는 특정 대역대의 가변 주파수일 수 있다. 상기 선택 신호는 ANN 모델 내의 복수의 레이어의 연산 단계들 중에서 임의의 주어진 레이어의 연산 단계에 대 해서 PE들의 이용률 또는 MAC 연산량에 대한 정보를 기초로 결정될 수 있다.상기 복수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함할 수 있다. 상기 주어진 레이어의 주어진 연산 단계에 대한 MAC 연산량 또는 상기 PE들의 이용률에 대한 정보는 스케줄링 정보에 포함될 수 있다. 스케줄링 정보에 기초하여 상기 적어도 하나의 ANN 모델의 각 레이어의 연산 단계에 대 한 MAC 연산을 상기 복수의 PE들에게 할당할 수 있다. 상기 스위칭 회로는 선택 신호에 기초하여 상기 다수의 클럭 신호들 중 하나의 클럭 신호를 선택하여 출력 할 수 있다. 즉, 상기 스위칭 회로는 선택 신호에 기초하여 특정 주파수를 가지는 클럭 신호를 출력할 수 있다. 예를 들어, 상기 주어진 레이어의 연산 단계에 대한 연산량 또는 상기 PE들의 이용률이 이전 레이어의 연산 단 계 보다 높은 경우, 상기 스위칭 회로는 상기 이전 레이어의 연산 단계의 제1 동작 주파수 보다 낮은 제2 동작 주파수를 갖는 클럭 신호를 선택할 수 있다. 반대로, 상기 주어진 레이어의 연산 단계에 대한 연산량 또는 상기 PE들의 이용률이 이전 레이어의 연산 단계 보다 낮은 경우, 상기 스위칭 회로는 상기 이전 레이어의 연산 단계의 제1 동작 주파수 보다 높은 제2 동 작 주파수를 갖는 클럭 신호를 선택할 수 있다. 신경 프로세싱 유닛은 특정 인공신경망모델의 모든 연산 단계의 연산량 또는 복수의 PE의 이용률 정보를 획득할 수 있다. 이러한 경우, 신경 프로세싱 유닛은 각 연단 단계마다 최적의 주파수의 클럭 신호를 사전 설정(preset)할 수 있다. 본 개시에서 제시되는 기법은 신경 프로세싱 유닛이 2개일 경우에도 적용될 수 있고, 또한 신경 프로세싱 유닛 이 3개, 4개 혹은 6개 혹은 8개일 경우에도 적용될 수 있다. 즉, 본 개시의 일 예시에 따른 신경 프로세싱 유닛 의 개수는 제한되지 않는다. 도 8a는 본 개시의 제1 개시의 일 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 8a을 참조하면, 예시적인 SoC은 복수의 신경 프로세싱 유닛들과 복수의 CPU(central processing unit)들, 복수의 메모리들을 포함할 수 있다. 상기 복수의 신경 프로세싱 유닛들은 예컨대 제1 NPU(100-1)과 제 2 NPU(100-2)를 포함할 수 있다. 그리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200-1)과 제2 CPU(200-2)를 포 함할 수 있다. 상기 복수의 메모리들은 제1 메모리(300-1)와 제2 메모리(300-2)를 포함할 수 있다. 도 8a에서는 상기 복수의 신경 프로세싱 유닛들, 복수의 CPU들 그리고 복수의 메모리들의 개수가 각기 2개인 것으로 도시되었지만, 이에 한정되는 것은 아니고 개수는 4개, 6개, 8개 등으로 다양하게 변형될 수 있다. 상기 예시적인 SoC는 메모리 컨트롤러와, 스위칭 회로와, 시스템 버스와 그리고 I/O(input output) 인터페이스를 포함할 수 있다. 상기 시스템 버스는 반도체 다이(die) 위에 형성된 전기 전도성 패턴(electrically conductive pattern) 에 의해서 구현될 수 있다. 상기 시스템 버스는 고속 통신을 가능하게 한다. 예를 들어, 상기 복수의 신경 프로 세싱 유닛들, 상기 복수의 CPU들, 상기 복수의 메모리들 그리고 상기 메모리 컨트롤러은 상기 시스템 버스 를 통하여 서로 통신할 수 있다. 상기 복수의 신경 프로세싱 유닛들과 그리고 상기 복수의 CPU들은 상기 시스템 버스를 통하여 상기 메모리 컨트롤러에 요청한다. 이에 메모리 컨트롤러는 상기 복수의 메모리들 중 적어도 하나 이상으로부터 데이터를 읽어내거나 혹은 기록할 수 있다. 도 8a에 도시된 상기 SoC 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에는 구동 전압(VDD)이 입력될 수 있 다. 도 8a에서는 공통된 구동 전압(VDD)이 입력되는 것으로 도시 되었으나, 본 개시의 예시들은 이에 제한되지 않는 다. 몇몇 예시들에서는, 상기 SoC 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 구동 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 다른 예시들에서는, 상기 SoC 내의 일부 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 구 동 전압(VDD)이 입력되도록 구성되는 것도 가능하다.또 다른 예시들에서는, 상기 SoC 내의 특정 엘리먼트에 제1 구동 전압과 상이한 제2 구동 전압이 입력되 도록 구성되는 것도 가능하다. 도 8a에 도시된 상기 스위칭 회로는 선택 신호를 입력 받도록 구성된다. 여기서 선택 신호는 복수개일 수 있다. 각각의 대응되는 선택 신호는 복수의 신경 프로세싱 유닛들(예컨대, 제1 NPU(100-1)과 제2 NPU(100-2))에 입력될 수 있다. 선택 신호는 상기 복수의 특정 신경 프로세싱 유닛에 의해서 처리되는 특정 ANN 모델의 특정 연산 단계에 대한 연산량 또는 적어도 하나의 신경 프로세싱 유닛 내의 PE들의 이용률에 대한 정보를 기초로 결 정될 수 있다. 상기 정보는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나로부터 제공될 수 있다. 또는, 상기 정보는 상기 복수의 CPU들 중 적어도 하나로부터 제공될 수 있다. 이 경우, 상기 정보에 대응되는 선택 신호가 생성될 수 있 다. 상기 선택 신호는 상기 복수의 CPU 또는 상기 복수의 신경 프로세싱 유닛에서 생성될 수 있다. 상기 생성된 선택 신호는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나 또는 상기 복수의 CPU들 중 적어도 하나에서 상 기 스위칭 회로로 전달될 수 있다. 여기서 상기 선택 신호는 시스템 버스를 통해서 스위칭 회로(40 0)로 전달될 수 있다. 상기 스위칭 회로는 다수의 클럭 신호들을 생성한 후, 상기 획득한 정보에 기초하여 상기 다수의 클럭 신 호들 중에서 하나의 클럭 신호를 선택할 수 있다. 선택된 주파수의 클럭 신호는 해당 신경 프로세싱 유닛으로 전달할 수 있다. 이를 위하여, 스위칭 회로는 적어도 하나의 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더 (divider) 회로, 클럭 게이팅(gating), 오실레이터(oscillator), 위상 루프 (PLL) 회로, 선택기 중 적어도 하 나를 포함한 회로일 수 있다. 스위칭 회로는 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더 (divider) 회로, 클럭 게이팅(gating) 회로, 오실레이터(oscillator) 회로, 위상 루프(PLL) 회로, 선택기 중 적어도 두개가 병합된 회로일 수 있다. 단, 본 개시의 예시는 이에 제한되지 않는다. 즉, 상기 스위칭 회로는 상기 복수의 신경 프로세싱 유닛들(예컨대, 제1 NPU(100-1)과 제2 NPU(100-2)) 각 각을 위한 클럭 신호를 생성한 후, 해당 신경 프로세싱 유닛으로 전달할 수 있다. 예를 들어, 상기 스위칭 회로는 상기 제1 클럭 신호를 상기 제1 NPU(100-1)로 제공하고, 상기 제2 클럭 신 호를 상기 제2 NPU(100-2)로 제공한다. 이에, 제1 NPU(100-1)는 제1 클럭 신호에 따라 동작되도록 하고, 제2 NPU(100-2)는 제2 클럭 신호에 따라 동작되도록 할 수 있다. 구체적인 예를 들어, 상기 제1 NPU(100-1)에 의해서 처리되는 제1 ANN 내의 복수의 레이어의 연산 단계들 중에 서 주어진 임의 레이어의 연산 단계에 대한 연산량 혹은 PE 이용률이 이전 레이어의 연산 단계 보다 높게 변화 되는 경우, 선택 신호가 주파수를 낮추도록 명령할 수 있다. 이에, 상기 스위칭 회로는 상기 이전 레이어 의 연산 단계의 주파수 보다 낮은 주파수를 갖는 제1 클럭 신호를 선택하여, 상기 제1 NPU(100-1)로 제공할 수 있다. 또는, 상기 스위칭 회로는 상기 제2 주파수의 제2 클럭 신호의 주파수를 변조하여 제1 주파수의 제1 클럭 신호를 생성하여, 상기 제1 NPU(100-1)로 제공할 수 있다. 반대로, 상기 제2 NPU(100-2)에 의해서 처리되는 제2 ANN 내의 복수의 레이어의 연산 단계들 중에서 주어진 임 의 레이어의 연산 단계에 대한 연산량 혹은 PE 이용률이 이전 레이어의 연산 단계 보다 낮게 변화되는 경우, 선 택 신호가 주파수를 높이도록 명령할 수 있다. 이에 상기 스위칭 회로는 상기 이전 레이어의 연산 단계의 주파수 보다 높은 주파수를 갖는 제2 클럭 신호를 선택하여, 상기 제2 NPU(100-2)로 제공할 수 있다. 또는, 상 기 스위칭 회로는 상기 제1 주파수의 제1 클럭 신호의 주파수를 변조하여 제2 주파수의 제2 클럭 신호를 생성하여, 상기 제2 NPU(100-2)로 제공할 수 있다. 이처럼 제1 NPU(100-1)와 상기 제2 NPU(100-2)가 서로 다른 주파수를 갖는 클럭 신호에 따라서 분산 동작되게 하면, 특정 연산 단계들 사이의 파워 편차를 저감하고 공급 전압의 안정성을 개선할 수 있는 장점이 있다. 도 8b는 본 개시의 제1 개시의 다른 일 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 8b을 참조하면, 예시적인 SoC은 복수의 신경 프로세싱 유닛들과 복수의 CPU(central processing unit)들, 복수의 메모리들을 포함할 수 있다. 상기 복수의 신경 프로세싱 유닛들은 예컨대 제1 NPU(100-1)과 제 2 NPU(100-2)를 포함할 수 있다. 그리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200-1)과 제2 CPU(200-2)를 포 함할 수 있다. 상기 복수의 메모리들은 제1 메모리(300-1)와 제2 메모리(300-2)를 포함할 수 있다. 도 8b에서는 상기 복수의 신경 프로세싱 유닛들, 복수의 CPU들 그리고 복수의 메모리들의 개수가 각기 2개인 것으로 도시되었지만, 이에 한정되는 것은 아니고 개수는 4개, 6개, 8개 등으로 다양하게 변형될 수 있다. 상기 예시적인 SoC는 메모리 컨트롤러와, 스위칭 회로와, 복수의 버스와 그리고 I/O(input output) 인터페이스를 포함할 수 있다. 상기 복수의 버스는 CPU를 위한 버스, 즉 CPU 버스(500-1), 신경 프로세싱 유닛을 위한 버스, 즉 NPU 버스(500- 2)와 주변 컴포넌트를 위한 버스, 즉 주변 버스(Peripheral Bus)(500-3)을 포함할 수 있다. 상기 CPU 버스(500-1)에는 상기 제1 CPU(200-1)과 상기 제2 CPU(200-2) 그리고 상기 제1 메모리(300-1)이 연결 될 수 있다. 상기 NPU 버스(500-2)에는 제1 NPU(100-1)과 제2 NPU(100-2)와 그리고 제2 메모리(300-2)가 연결 될 수 있다. 상기 주변 버스(Peripheral Bus)(500-3)에는 메모리 컨트롤러와, 스위칭 회로가 연결될 수 있다. 도 8b에 도시된 상기 SoC 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에는 구동 전압(VDD)이 입력될 수 있 다. 도 8b에서는 공통된 구동 전압(VDD)이 입력되는 것으로 도시 되었으나, 본 개시의 예시들은 이에 제한되지 않는 다. 몇몇 예시들에서는, 상기 SoC 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 구동 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 다른 예시들에서는, 상기 SoC 내의 일부 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 구 동 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 또 다른 예시들에서는, 상기 SoC 내의 특정 엘리먼트에 제1 구동 전압과 상이한 제2 구동 전압이 입력되 도록 구성되는 것도 가능하다. 도 8b에 도시된 상기 스위칭 회로는 선택 신호를 입력 받도록 구성된다. 여기서 선택 신호는 복수개일 수 있다. 각각의 대응되는 선택 신호는 복수의 신경 프로세싱 유닛들(예컨대, 제1 NPU(100-1)과 제2 NPU(100-2))에 입력될 수 있다. 선택 신호는 상기 복수의 특정 신경 프로세싱 유닛에 의해서 처리되는 특정 ANN 모델의 특정 연산 단계에 대한 연산량 또는 적어도 하나의 신경 프로세싱 유닛 내의 PE들의 이용률에 대한 정보를 기초로 결 정될 수 있다. 상기 정보는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나로부터 제공될 수 있다. 또는, 상기 정보는 상기 복수의 CPU들 중 적어도 하나로부터 제공될 수 있다. 이 경우, 상기 정보에 대응되는 선택 신호가 생성될 수 있 다. 상기 선택 신호는 상기 복수의 CPU 또는 상기 복수의 신경 프로세싱 유닛에서 생성될 수 있다. 상기 생성된 선택 신호는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나 또는 상기 복수의 CPU들 중 적어도 하나에서 상 기 스위칭 회로로 전달될 수 있다. 여기서 상기 선택 신호는 상기 NPU 버스(500-2), 상기 CPU 버스(500-1) 그리고 상기 주변 버스 (500-3) 중 적어도 하나를 통해서 스위칭 회로로 전달될 수 있다. 또는, 상기 정보는 상기 복수의 CPU들 중 적어도 하나로부터 제공될 수 있다. 이 경우, 상기 정보는 상기 복수 의 신경 프로세싱 유닛들 중 적어도 하나로부터 전송된 후, 상기 NPU 버스(500-2) 그리고 상기 CPU 버스(500- 1)를 거쳐서 상기 복수의 CPU들 중 적어도 하나로 전달될 수 있다. 상기 복수의 CPU들 중 적어도 하나에서 생성 된 선택신호는 상기 CPU 버스(500-1) 그리고 상기 주변 버스 (500-3)를 거쳐서 상기 스위칭 회로로 전달될 수 있다. 상기 스위칭 회로는 다수의 클럭 신호들을 생성한 후, 상기 획득한 정보에 기초하여 상기 다수의 클럭 신 호들 중에서 하나의 클럭 신호를 선택할 수 있다. 선택된 주파수의 클럭 신호는 해당 신경 프로세싱 유닛으로 전달할 수 있다. 이를 위하여, 스위칭 회로는 적어도 하나의 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더 (divider) 회로, 클럭 게이팅(gating), 오실레이터(oscillator), 위상 루프 (PLL) 회로, 선택기 중 적어도 하 나를 포함한 회로일 수 있다. 스위칭 회로는 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더 (divider) 회로, 클럭 게이팅(gating) 회로, 오실레이터(oscillator) 회로, 위상 루프(PLL) 회로, 선택기 중 적어도 두개가 병합된 회로일 수 있다. 단, 본 개시의 예시는 이에 제한되지 않는다. 즉, 상기 스위칭 회로는 상기 복수의 신경 프로세싱 유닛들(예컨대, 제1 NPU(100-1)과 제2 NPU(100-2)) 각 각을 위한 클럭 신호를 생성한 후, 해당 신경 프로세싱 유닛으로 전달할 수 있다.예를 들어, 상기 스위칭 회로는 상기 제1 클럭 신호를 상기 제1 NPU(100-1)로 제공하고, 상기 제2 클럭 신 호를 상기 제2 NPU(100-2)로 제공한다. 이에, 제1 NPU(100-1)는 제1 클럭 신호에 따라 동작되도록 하고, 제2 NPU(100-2)는 제2 클럭 신호에 따라 동작되도록 할 수 있다. 구체적인 예를 들어, 상기 제1 NPU(100-1)에 의해서 처리되는 제1 ANN 내의 복수의 레이어의 연산 단계들 중에 서 주어진 임의 레이어의 연산 단계에 대한 연산량 혹은 PE 이용률이 이전 레이어의 연산 단계 보다 높게 변화 되는 경우, 선택 신호가 주파수를 낮추도록 명령할 수 있다. 이에, 상기 스위칭 회로는 상기 이전 레이어 의 연산 단계의 주파수 보다 낮은 주파수를 갖는 제1 클럭 신호를 선택하여, 상기 제1 NPU(100-1)로 제공할 수 있다. 또는, 상기 스위칭 회로는 상기 제2 주파수의 제2 클럭 신호의 주파수를 변조하여 제1 주파수의 제1 클럭 신호를 생성하여, 상기 제1 NPU(100-1)로 제공할 수 있다. 반대로, 상기 제2 NPU(100-2)에 의해서 처리되는 제2 ANN 내의 복수의 레이어의 연산 단계들 중에서 주어진 임 의 레이어의 연산 단계에 대한 연산량 혹은 PE 이용률이 이전 레이어의 연산 단계 보다 낮게 변화되는 경우, 선 택 신호가 주파수를 높이도록 명령할 수 있다. 이에 상기 스위칭 회로는 상기 이전 레이어의 연산 단계의 주파수 보다 높은 주파수를 갖는 제2 클럭 신호를 선택하여, 상기 제2 NPU(100-2)로 제공할 수 있다. 또는, 상 기 스위칭 회로는 상기 제1 주파수의 제1 클럭 신호의 주파수를 변조하여 제2 주파수의 제2 클럭 신호를 생성하여, 상기 제2 NPU(100-2)로 제공할 수 있다. 이처럼 제1 NPU(100-1)와 상기 제2 NPU(100-2)가 서로 다른 클럭 신호에 따라서 분산 동작되게 하면, 인접한 연 산 단계들의 파워 편차를 저감하고, 공급 전압의 안정성을 개선할 수 있는 장점이 있다. 이상에서는 SoC를 위주로 설명되지만, 본 개시의 개시는 SoC에만 한정되는 것은 아니며, 본 개시의 내용은 SiP(System in Package) 혹은 PCB(Printed circuit board) 기반 보드 레벨 시스템에도 적용될 수 있다. 예를 들어, 각 기능 컴포넌트는 독립 반도체 칩으로 구현되고, PCB 상에 형성된 전기 전도성 패턴(electrically conductive pattern)에 의해서 구현되는 시스템 버스를 통해 서로 연결되는 형태로 구현될 수 있다. 구체적으로 는 도 9a 및 도 9b를 참조하여 설명하기로 한다. 도 9a는 본 개시의 제2 개시의 일 예시에 따른 시스템의 구조를 나타낸 예시도이다. 도 9a를 참조하면, 예시적인 시스템은 기판에 장착되는 복수의 신경 프로세싱 유닛들과 복수의 CPU(central processing unit)들, 복수의 메모리들을 포함할 수 있다. 상기 복수의 신경 프로세싱 유닛들은 예 컨대 제1 NPU(100-1)과 제2 NPU(100-2)를 포함할 수 있다. 그리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200- 1)과 제2 CPU(200-2)를 포함할 수 있다. 상기 복수의 메모리들은 제1 메모리(300-1)와 제2 메모리(300-2)를 포 함할 수 있다. 도 9a에서는 상기 복수의 신경 프로세싱 유닛들, 복수의 CPU들 그리고 복수의 메모리들의 개수가 각기 2개인 것으로 도시되었지만, 이에 한정되는 것은 아니고 개수는 4개, 6개, 8개 등으로 다양하게 변형될 수 있다. 상기 예시적인 시스템는 메모리 컨트롤러와, 스위칭 회로와, 시스템 버스와 그리고 I/O(input output) 인터페이스를 포함할 수 있다. 상기 시스템 버스는 상기 기판 위에 형성된 전기 전도성 패턴(electrically conductive pattern)에 의해서 구현될 수 있다. 상기 시스템 버스는 고속 통신을 가능하게 한다. 예를 들어, 상기 복수의 신경 프로세 싱 유닛들, 상기 복수의 CPU들, 상기 복수의 메모리들 그리고 상기 메모리 컨트롤러은 상기 시스템 버스 를 통하여 서로 통신할 수 있다. 도 9a에 도시된 상기 시스템 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에는 구동 전압(VDD)이 입력될 수 있다. 도 9a에서는 공통된 구동 전압(VDD)이 입력되는 것으로 도시 되었으나, 본 개시의 예시들은 이에 제한되지 않는 다. 몇몇 예시들에서는, 상기 시스템 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 구동 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 다른 예시들에서는, 상기 시스템 내의 일부 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 구동 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 또 다른 예시들에서는, 상기 시스템 내의 특정 엘리먼트에 제1 구동 전압과 상이한 제2 구동 전압이 입력 되도록 구성되는 것도 가능하다. 도 9a에 도시된 상기 스위칭 회로는 선택 신호를 입력 받도록 구성된다. 여기서 선택 신호는 복수개일 수 있다. 각각의 대응되는 선택 신호는 복수의 신경 프로세싱 유닛들(예컨대, 제1 NPU(100-1)과 제2 NPU(100-2))에 입력될 수 있다. 선택 신호는 상기 복수의 특정 신경 프로세싱 유닛에 의해서 처리되는 특정 ANN 모델의 특정 연산 단계에 대한 연산량 또는 적어도 하나의 신경 프로세싱 유닛 내의 PE들의 이용률에 대한 정보를 기초로 결 정될 수 있다. 상기 정보는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나로부터 제공될 수 있다. 또는, 상기 정보는 상기 복수의 CPU들 중 적어도 하나로부터 제공될 수 있다. 이 경우, 상기 정보에 대응되는 선택 신호가 생성될 수 있 다. 상기 선택 신호는 상기 복수의 CPU 또는 상기 복수의 신경 프로세싱 유닛에서 생성될 수 있다. 상기 생성된 선택 신호는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나 또는 상기 복수의 CPU들 중 적어도 하나에서 상 기 스위칭 회로로 전달될 수 있다. 여기서 상기 선택 신호는 시스템 버스를 통해서 스위칭 회로(40 0)로 전달될 수 있다. 상기 스위칭 회로는 다수의 클럭 신호들을 생성한 후, 상기 획득한 정보에 기초하여 상기 다수의 클럭 신 호들 중에서 하나의 클럭 신호를 선택할 수 있다. 선택된 주파수의 클럭 신호는 해당 신경 프로세싱 유닛으로 전달할 수 있다. 이를 위하여, 스위칭 회로는 적어도 하나의 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더 (divider) 회로, 클럭 게이팅(gating), 오실레이터(oscillator), 위상 루프 (PLL) 회로, 선택기 중 적어도 하 나를 포함한 회로일 수 있다. 스위칭 회로는 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더 (divider) 회로, 클럭 게이팅(gating) 회로, 오실레이터(oscillator) 회로, 위상 루프(PLL) 회로, 선택기 중 적어도 두개가 병합된 회로일 수 있다. 단, 본 개시의 예시는 이에 제한되지 않는다. 다른 예시로, 스위칭 회로는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나의 신경 프로세싱 유닛에서 처리되는 특정 ANN 모델의 특정 연산 단계에 대한 연산량 또는 PE들의 이용률에 대한 정보를 획득할 수 있다. 여기서, 스위칭 회로는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나의 신경 프로세싱 유닛에서 처리 되는 특정 ANN 모델의 특정 연산 단계에 대한 연산량 또는 PE들의 이용률에 대한 정보를 기초로 선택 신호를 생 성하도록 구성될 수 있다. 즉, 상기 스위칭 회로는 상기 복수의 신경 프로세싱 유닛들(예컨대, 제1 NPU(100-1)과 제2 NPU(100-2)) 각 각을 위한 클럭 신호를 생성한 후, 해당 신경 프로세싱 유닛으로 전달할 수 있다. 예를 들어, 상기 스위칭 회로는 상기 제1 클럭 신호를 상기 제1 NPU(100-1)로 제공하고, 상기 제2 클럭 신 호를 상기 제2 NPU(100-2)로 제공한다. 이에, 제1 NPU(100-1)는 제1 클럭 신호에 따라 동작되도록 하고, 제2 NPU(100-2)는 제2 클럭 신호에 따라 동작되도록 할 수 있다. 구체적인 예를 들어, 상기 제1 NPU(100-1)에 의해서 처리되는 제1 ANN 내의 복수의 레이어의 연산 단계들 중에 서 주어진 임의 레이어의 연산 단계에 대한 연산량 혹은 PE 이용률이 이전 레이어의 연산 단계 보다 높게 변화 되는 경우, 선택 신호가 주파수를 낮추도록 명령할 수 있다. 이에, 상기 스위칭 회로는 상기 이전 레이어 의 연산 단계의 주파수 보다 낮은 주파수를 갖는 제1 클럭 신호를 선택하여, 상기 제1 NPU(100-1)로 제공할 수 있다. 또는, 상기 스위칭 회로는 상기 제2 주파수의 제2 클럭 신호의 주파수를 변조하여 제1 주파수의 제1 클럭 신호를 생성하여, 상기 제1 NPU(100-1)로 제공할 수 있다. 반대로, 상기 제2 NPU(100-2)에 의해서 처리되는 제2 ANN 내의 복수의 레이어의 연산 단계들 중에서 주어진 임 의 레이어의 연산 단계에 대한 연산량 혹은 PE 이용률이 이전 레이어의 연산 단계 보다 낮게 변화되는 경우, 선 택 신호가 주파수를 높이도록 명령할 수 있다. 이에 상기 스위칭 회로는 상기 이전 레이어의 연산 단계의 주파수 보다 높은 주파수를 갖는 제2 클럭 신호를 선택하여, 상기 제2 NPU(100-2)로 제공할 수 있다. 또는, 상 기 스위칭 회로는 상기 제1 주파수의 제1 클럭 신호의 주파수를 변조하여 제2 주파수의 제2 클럭 신호를 생성하여, 상기 제2 NPU(100-2)로 제공할 수 있다. 이처럼 제1 NPU(100-1)와 상기 제2 NPU(100-2)가 서로 다른 주파수를 갖는 클럭 신호에 따라서 분산 동작되게 하면, 인접한 연산 단계들의 파워 편차를 저감하고, 공급 전압의 안정성을 개선할 수 있는 장점이 있다. 도 9b는 본 개시의 제2 개시의 일 예시에 따른 시스템의 구조를 나타낸 예시도이다. 도 9b를 참조하면, 예시적인 시스템은 기판에 장착되는 복수의 신경 프로세싱 유닛들과 복수의 CPU(central processing unit)들, 복수의 메모리들을 포함할 수 있다. 상기 복수의 신경 프로세싱 유닛들은 예 컨대 제1 NPU(100-1)과 제2 NPU(100-2)를 포함할 수 있다. 그리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200- 1)과 제2 CPU(200-2)를 포함할 수 있다. 상기 복수의 메모리들은 제1 메모리(300-1)와 제2 메모리(300-2)를 포 함할 수 있다. 도 9b에서는 상기 복수의 신경 프로세싱 유닛들, 복수의 CPU들 그리고 복수의 메모리들의 개수가 각기 2개인 것으로 도시되었지만, 이에 한정되는 것은 아니고 개수는 4개, 6개, 8개 등으로 다양하게 변형될 수 있다. 상기 예시적인 시스템는 메모리 컨트롤러와, 스위칭 회로와, 복수의 버스와 그리고 I/O(input output) 인터페이스를 포함할 수 있다. 상기 복수의 버스는 상기 기판 위에 형성된 전기 전도성 패턴(electrically conductive pattern)에 의해 서 구현될 수 있다. 상기 복수의 버스는 CPU를 위한 버스, 즉 CPU 버스(500-1), 신경 프로세싱 유닛을 위한 버스, 즉 NPU 버스(500- 2)와 주변 컴포넌트를 위한 버스, 즉 주변 버스(Peripheral Bus)(500-3)을 포함할 수 있다. 상기 CPU 버스(500-1)에는 상기 제1 CPU(200-1)과 상기 제2 CPU(200-2) 그리고 상기 제1 메모리(300-1)이 연결 될 수 있다. 상기 NPU 버스(500-2)에는 제1 NPU(100-1)과 제2 NPU(100-2)와 그리고 제2 메모리(300-2)가 연결 될 수 있다. 상기 주변 버스(Peripheral Bus)(500-3)에는 메모리 컨트롤러와, 스위칭 회로가 연결될 수 있다. 도 9b에 도시된 상기 시스템 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에는 구동 전압(VDD)이 입력될 수 있다. 도 9b에서는 공통된 구동 전압(VDD)이 입력되는 것으로 도시 되었으나, 본 개시의 예시들은 이에 제한되지 않는 다. 몇몇 예시들에서는, 상기 시스템 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 구동 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 다른 예시들에서는, 상기 시스템 내의 일부 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 구동 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 또 다른 예시들에서는, 상기 시스템 내의 특정 엘리먼트에 제1 구동 전압과 상이한 제2 구동 전압이 입력 되도록 구성되는 것도 가능하다. 도 9a에 도시된 상기 스위칭 회로는 선택 신호를 입력 받도록 구성된다. 여기서 선택 신호는 복수개일 수 있다. 각각의 대응되는 선택 신호는 복수의 신경 프로세싱 유닛들(예컨대, 제1 NPU(100-1)과 제2 NPU(100-2))에 입력될 수 있다. 선택 신호는 상기 복수의 특정 신경 프로세싱 유닛에 의해서 처리되는 특정 ANN 모델의 특정 연산 단계에 대한 연산량 또는 적어도 하나의 신경 프로세싱 유닛 내의 PE들의 이용률에 대한 정보를 기초로 결 정될 수 있다. 상기 정보는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나로부터 제공될 수 있다. 또는, 상기 정보는 상기 복수의 CPU들 중 적어도 하나로부터 제공될 수 있다. 이 경우, 상기 정보에 대응되는 선택 신호가 생성될 수 있 다. 상기 선택 신호는 상기 복수의 CPU 또는 상기 복수의 신경 프로세싱 유닛에서 생성될 수 있다. 상기 생성된 선택 신호는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나 또는 상기 복수의 CPU들 중 적어도 하나에서 상 기 스위칭 회로로 전달될 수 있다. 여기서 상기 선택 신호는 상기 NPU 버스(500-2), 상기 CPU 버스(500-1) 그리고 상기 주변 버스 (500-3) 중 적어도 하나를 통해서 스위칭 회로로 전달될 수 있다. 또는, 상기 정보는 상기 복수의 CPU들 중 적어도 하나로부터 제공될 수 있다. 이 경우, 상기 정보는 상기 복수 의 신경 프로세싱 유닛들 중 적어도 하나로부터 전송된 후, 상기 NPU 버스(500-2) 그리고 상기 CPU 버스(500- 1)를 거쳐서 상기 복수의 CPU들 중 적어도 하나로 전달될 수 있다. 상기 복수의 CPU들 중 적어도 하나에서 생성 된 선택신호는 상기 CPU 버스(500-1) 그리고 상기 주변 버스 (500-3)를 거쳐서 상기 스위칭 회로로 전달될 수 있다. 상기 스위칭 회로는 다수의 클럭 신호들을 생성한 후, 상기 획득한 정보에 기초하여 상기 다수의 클럭 신 호들 중에서 하나의 클럭 신호를 선택할 수 있다. 선택된 주파수의 클럭 신호는 해당 신경 프로세싱 유닛으로 전달할 수 있다.이를 위하여, 스위칭 회로는 적어도 하나의 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더 (divider) 회로, 클럭 게이팅(gating), 오실레이터(oscillator), 위상 루프 (PLL) 회로, 선택기 중 적어도 하 나를 포함한 회로일 수 있다. 스위칭 회로는 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더 (divider) 회로, 클럭 게이팅(gating) 회로, 오실레이터(oscillator) 회로, 위상 루프(PLL) 회로, 선택기 중 적어도 두개가 병합된 회로일 수 있다. 단, 본 개시의 예시는 이에 제한되지 않는다. 다른 예시로, 스위칭 회로는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나의 신경 프로세싱 유닛에서 처리되는 특정 ANN 모델의 특정 연산 단계에 대한 연산량 또는 PE들의 이용률에 대한 정보를 획득할 수 있다. 여기서, 스위칭 회로는 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나의 신경 프로세싱 유닛에서 처리 되는 특정 ANN 모델의 특정 연산 단계에 대한 연산량 또는 PE들의 이용률에 대한 정보를 기초로 선택 신호를 생 성하도록 구성될 수 있다. 즉, 상기 스위칭 회로는 상기 복수의 신경 프로세싱 유닛들(예컨대, 제1 NPU(100-1)과 제2 NPU(100-2)) 각 각을 위한 클럭 신호를 생성한 후, 해당 신경 프로세싱 유닛으로 전달할 수 있다. 예를 들어, 상기 스위칭 회로는 상기 제1 클럭 신호를 상기 제1 NPU(100-1)로 제공하고, 상기 제2 클럭 신 호를 상기 제2 NPU(100-2)로 제공한다. 이에, 제1 NPU(100-1)는 제1 클럭 신호에 따라 동작되도록 하고, 제2 NPU(100-2)는 제2 클럭 신호에 따라 동작되도록 할 수 있다. 구체적인 예를 들어, 상기 제1 NPU(100-1)에 의해서 처리되는 제1 ANN 내의 복수의 레이어의 연산 단계들 중에 서 주어진 임의 레이어의 연산 단계에 대한 연산량 혹은 PE 이용률이 이전 레이어의 연산 단계 보다 높게 변화 되는 경우, 선택 신호가 주파수를 낮추도록 명령할 수 있다. 이에, 상기 스위칭 회로는 상기 이전 레이어 의 연산 단계의 주파수 보다 낮은 주파수를 갖는 제1 클럭 신호를 선택하여, 상기 제1 NPU(100-1)로 제공할 수 있다. 또는, 상기 스위칭 회로는 상기 제2 주파수의 제2 클럭 신호의 주파수를 변조하여 제1 주파수의 제1 클럭 신호를 생성하여, 상기 제1 NPU(100-1)로 제공할 수 있다. 반대로, 상기 제2 NPU(100-2)에 의해서 처리되는 제2 ANN 내의 복수의 레이어의 연산 단계들 중에서 주어진 임 의 레이어의 연산 단계에 대한 연산량 혹은 PE 이용률이 이전 레이어의 연산 단계 보다 낮게 변화되는 경우, 선 택 신호가 주파수를 높이도록 명령할 수 있다. 이에 상기 스위칭 회로는 상기 이전 레이어의 연산 단계의 주파수 보다 높은 주파수를 갖는 제2 클럭 신호를 선택하여, 상기 제2 NPU(100-2)로 제공할 수 있다. 또는, 상 기 스위칭 회로는 상기 제1 주파수의 제1 클럭 신호의 주파수를 변조하여 제2 주파수의 제2 클럭 신호를 생성하여, 상기 제2 NPU(100-2)로 제공할 수 있다. 이처럼 제1 NPU(100-1)와 상기 제2 NPU(100-2)가 서로 다른 클럭 신호에 따라서 분산 동작되게 하면, 인접한 연 산 단계들의 파워 편차를 저감하고, 공급 전압의 안정성을 개선할 수 있는 장점이 있다. 도 10a는 본 개시의 제3 개시의 일 예시에 따른 신경 프로세싱 유닛의 구조를 나타낸 예시도이다. 도 10a에서는 신경 프로세싱 유닛이 복수의 PE들과 내부 메모리와 SFU 그리고 클럭 생성기 를 포함하는 것으로 도시되어 있다. 다만, 상기 신경 프로세싱 유닛은 도 3 또는 도 5에 도시된 바와 같이 NPU 컨트롤러 그리고 NPU 인터페이스를 더 포함할 수 있다. 상기 복수의 PE들과, 상기 내 부 메모리, NPU 컨트롤러, 상기 NPU 인터페이스, SFU 그리고 클럭 생성기 각각은 수 많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어 려울 수 있고, 동작에 의해서만 식별될 수 있다. 스위칭 회로는 클럭 신호 공급 회로로 지칭될 수 있다. 예컨대, 임의 회로는 복수의 PE들으로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. 상기 복수의 PE들은 가산기(adder), 곱셈기(multiplier) 그리고 누산기(accumulator)를 포함할 수 있다. 도 10a에 도시된 상기 스위칭 회로는 적어도 하나의 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이 더(divider) 회로, 클럭 게이팅(gating), 오실레이터(oscillator), 위상 루프 (PLL) 회로, 선택기 중 적어도 하나를 포함한 회로일 수 있다. 스위칭 회로는 클럭 소스, 클럭 스킵퍼(skipper) 회로, 클럭 디바이더 (divider) 회로, 클럭 게이팅(gating) 회로, 오실레이터(oscillator) 회로, 위상 루프(PLL) 회로, 선택기 중 적어도 두개가 병합된 회로일 수 있다. 단, 본 개시의 예시는 이에 제한되지 않는다. 신경 프로세싱 유닛은 상기 복수의 PE들에서 처리되는 특정 ANN 모델의 특정 연산 단계에 대한 연산 량 또는 PE들의 이용률에 대한 정보를 획득할 수 있다. 여기서, 신경 프로세싱 유닛은 상기 복수의 신경 프로세싱 유닛들 중 적어도 하나의 신경 프로세싱 유닛에서 처리되는 특정 ANN 모델의 특정 연산 단계에 대한연산량 또는 PE들의 이용률에 대한 정보를 기초로 선택 신호를 생성하도록 구성될 수 있다. 다른 예시로, 스위칭 회로는 상기 신경 프로세싱 유닛에서 처리되는 특정 ANN 모델의 특정 연산 단계 에 대한 연산량 또는 PE들의 이용률에 대한 정보를 획득할 수 있다. 여기서, 스위칭 회로190)는 상기 신경 프로 세싱 유닛에서 처리되는 특정 ANN 모델의 특정 연산 단계에 대한 연산량 또는 PE들의 이용률에 대한 정보 를 기초로 선택 신호를 생성하도록 구성될 수 있다. 상기 스위칭 회로는 선택 신호에 대응하여 대응되는 주파수의 클럭 신호를 출력할 수 있다. 상기 정보는 상기 NPU 컨트롤러 내의 스케줄러에 의해서 제공되는 것일 수 있다. 상기 신경 프로세싱 유닛에 의해서 처리되는 제1 ANN 내의 복수의 레이어의 연산 단계들 중에서 주어진 임 의 레이어의 연산 단계에 대한 연산량 혹은 PE 이용률이 이전 레이어의 연산 단계 보다 높게 변화되는 경우, 선 택 신호가 주파수를 낮추도록 명령할 수 있다. 이에, 상기 스위칭 회로는 상기 이전 레이어의 연산 단계의 주파수 보다 낮은 주파수를 갖는 제1 클럭 신호를 선택하여, 상기 복수의 PE들로 제공할 수 있다. 또는, 상기 스위칭 회로는 상기 제2 주파수의 제2 클럭 신호의 주파수를 변조하여 제1 주파수의 제1 클럭 신호를 생성하여, 상기 복수의 PE들로 제공할 수 있다. 반대로, 신경 프로세싱 유닛에 의해서 처리되는 제1 ANN 내의 복수의 레이어의 연산 단계들 중에서 주어진 임의 레이어의 연산 단계에 대한 연산량 혹은 PE 이용률이 이전 레이어의 연산 단계 보다 낮게 변화되는 경우, 선택 신호가 주파수를 높이도록 명령할 수 있다. 이에 상기 스위칭 회로는 상기 이전 레이어의 연산 단계 의 주파수 보다 높은 주파수를 갖는 제2 클럭 신호를 선택하여, 상기 복수의 PE들로 제공할 수 있다. 또는, 상기 스위칭 회로는 상기 제1 주파수의 제1 클럭 신호의 주파수를 변조하여 제2 주파수의 제2 클럭 신호를 생성하여, 상기 복수의 PE들로 제공할 수 있다. 이처럼 제1 NPU(100-1)와 상기 제2 NPU(100-2)가 서로 다른 클럭 신호에 따라서 분산 동작되게 하면, 인접한 연 산 단계들의 파워 편차를 저감하고, 공급 전압의 안정성을 개선할 수 있는 장점이 있다. 도 10b는 본 개시의 제3 개시의 다른 예시에 따른 신경 프로세싱 유닛의 구조를 나타낸 예시도이다. 도 10b에서는 신경 프로세싱 유닛이 복수의 PE들과 내부 메모리와 SFU를 포함하는 것으로 도시되어 있다. 다만, 상기 신경 프로세싱 유닛은 그 외에도 도 3 또는 도 5에 도시된 바와 같이 NPU 컨트 롤러 그리고 NPU 인터페이스를 더 포함할 수 있다. 상기 복수의 PE들과, 상기 내부 메모리 , NPU 컨트롤러, 상기 NPU 인터페이스, 그리고 SFU 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서 만 식별될 수 있다. 도 10b를 참조하면, 상기 스위칭 회로는 신경 프로세싱 유닛 내부가 아니라 외부에 위치하는 것으로 도시되어 있다. 그 외에는 도 10a에 도시된 것과 동일하므로, 도 10b에 대해서는 별도로 설명하지 않기로 하고, 도 10a를 참조하여 설명한 내용을 그대로 원용하기로 한다. 한편, 도 10a 및 도 10b에서 도시되지는 않았으나, 상기 SFU는 상기 NPU 내부 메모리에 직접 연결될 수 있다. 다른 한편, 도 10a 및 도 10b에서는 신경 프로세싱 유닛을 설명하였으나, 상기 신경 프로세싱 유닛은 SoC 형태 로 구현될 수도 있다. 도 11은 PE들의 이용률이 증가함에 따라 피크 파워가 커지는 예시를 도시한다. 도 11를 참조하면, 클럭 신호(CLK0)는 복수의 신경 프로세싱 유닛(100-1, 100-2)에 입력될 수 있다. 여기서 복 수의 신경 프로세싱 유닛(100-1, 100-2)은 동일한 위상의 클럭 신호를 입력 받도록 구성된다. 상기 복수의 신경 프로세싱 유닛은 상기 제1 NPU(100-1)과 상기 제2 NPU(100-2)을 포함할 수 있다. 도 11의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 11의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 11의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 11을 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워가 증가하고, 구동 전압(VDD)이 저하되는 경향이 도시되어 있다. 부연 설명하면, 각 클럭 사이클 마다 동작하는 PE들의 개수는 인공신경망모델을 컴파일하는 컴파일러의 성능에 따라 달라질 수 있다. 즉, 컴파일러의 성능이 우수할수록 단위 클럭마다 더 많은 PE들을 동작 시킬 수 있다. 여 기서 전체 PE들 중 동작하는 PE의 비율을 PE의 이용률(utilization rate %)로 지칭할 수 있다. 반대로, 컴파일 러의 성능이 우수할수록 신경 프로세싱 유닛의 피크 파워가 더 증가할 수 있다. 즉, 피크 파워는 PE의 이용률과 정비례할 수 있다. 따라서 컴파일러의 알고리즘이 정교해짐에 따라, 신경 프로세싱 유닛의 피크 파워는 더 증가 될 수 있는 문제가 있다. 다시 도 11를 참조하면, 클럭 신호(CLK0)를 기준으로 동작할 때의 피크 파워는 각 클럭당 동작하는 PE의 개수에 따라서 달라질 수 있다. 즉, 하나의 클럭에 동작하는 PE의 개수에 비례하여 피크 파워가 증가될 수 있다. 부연 설명하면, 저전력으로 개발된 신경 프로세싱 유닛은 엣지 디바이스에 적용될 때 M.2 인터페이스 또는 PCI Express 인터페이스에 대응되도록 개발될 수 있다. 특히 M.2 인터페이스의 경우 PCI Express 인터페이스에 비해 서 최대 파워가 상대적으로 더 낮을 수 있다. 예를 들면 M.2 인터페이스의 경우 3A의 전류와 10W의 파워 제한을 가질 수 있다. 만약 피크 파워가 특정 클럭에서 10W를 초과할 경우, 공급 전압(VDD)이 흔들릴 수 있다. 이때, PE의 이용률이 올라갈수록 피크 파워는 더 증가되고, 피크 파워가 파워 제한을 초과할수록 공급 전압(VDD)의 저 하 정도는 초과한 정보에 비례하여 더 커질 수 있다. 만약 공급 전압(VDD)이 IR-drop margin 이하로 떨어질 경우, 신경 프로세싱 유닛이 연산 중인 데이터에 오류가 발생할 수 있다. 부연 설명하면, 금속 상호 연결의 저항(R)과 이를 통해 흐르는 전류(I)로 인해 발생하며 옴의 법칙에 따라 전압 강하(V=IR)가 발생될 때 IR-drop이 발생할 수 있다. 신경 프로세싱 유닛의 복수의 PE와 관련 된 많은 수의 회로가 동시에 스위칭하면 신경 프로세싱 유닛이 상당한 양의 전류를 소모할 수 있다. 이 높은 전 류는 신경 프로세싱 유닛에서 더 큰 전압 강하를 발생시켜 SoC의 다른 부분에서 공급 전압을 감소시킬 수 있다. 도 11의 예시를 정리하면, 신경 프로세싱 유닛의 안정적인 작동을 보장하고 잠재적인 오류를 방지하려면 충분한 IR-drop margin을 확보해야 한다. IR-drop margin은 공급 전압(VDD)과 피크 파워 조건 동안 신경 프로세싱 유 닛의 모든 지점에서 허용 가능한 최저 전압 간의 차이를 의미할 수 있다. 본 개시의 예시들에 따르면, 특정 인공신경망모델의 특정 연산 단계의 PE들의 이용률을 기초로 IR-drop margin 이 예측될 수 있다. 따라서, IR-drop margin이 임계값 이하로 떨어질 것이 예상될 경우, PE들의 이용률과 IR- drop margin을 기초로 선택 신호를 결정하여 IR-drop margin을 확보할 수 있다. 도 12는 PE들의 이용률에 따라 클럭 신호를 다르게 사용할 때 파워의 예시를 도시한다. 도 12에는 시간 축을 따라서 특정 인공신경망모델의 3개의 연산 단계가 예시적으로 도시되어 있다. 해당 예시에 서는 구동 수파수에 따른 하나의 프로세싱 엘리먼트(PE)의 파워 특성을 나타낸다. 도 12를 참조하면, 주파수에 비례하여 파워가 점진적으로 증가한다. 예를 들어 아래의 표 2와 같이 선택 회로는 기 설정된 주파수의 클럭 신호들을 선택적으로 출력하도록 설 계될 수 있다. 따라서 특정 선택 신호가 스위칭 회로에 입력되면, 상기 선택 신호에 대응되는 클럭 신호가 신경 프로세싱 유닛에 공급될 수 있다. 표 2 주파수 선택 신호 600MHz 00 800MHz 01 1.0GHz 10 1.2GHz 11 다시 도 11을 고려하여 도 12를 참조하면, 프로세싱 엘리먼트의 이용률이 증가할수록 파워는 비례하여 증가할 수 있다는 점을 주목하여야 한다. 이에, 각 연산 단계별 PE들의 이용률 정보와 구동 주파수 정보를 기초로 각 연산 단계별 파워를 계산할 수 있다. 상기 파워 계산은 특정 인공신경망모델의 스케줄링 정보를 기초로 계산될수 있다. 여기서 파워(Watt)는 해당 연산 단계에 소비되는 에너지를 단위 시간으로 나눈 값(i.e., 1 watt (W) = 1 joule per second (J/s))으로 계산될 수 있다. 따라서 각 연산 단계의 파워를 스케줄링 정보에 기초하여 계산 할 수 있다. 예를 들면, 600MHz 구간의 파워는 800MHz 구간의 파워보다 낮다. 또한, 800MHz 구간의 파워는 1.0GHz 구간의 파 워보다 낮다. 스위칭 회로는 입력된 제1 선택 신호에 따라 600MHz의 제1 클럭 신호는 신경 프로세싱 유닛에 공급한 다. 이에, 제1 선택 신호는 스케줄링 정보에 따라 제1 연산 단계 동안 공급된다. 제1 선택 신호는 대 응되는 연산 단계의 PE 이용률을 기초로 선택될 수 있다. 제1 연산 단계의 PE 이용률은 제2 연산 단계의 PE 이 용률보다 높을 수 있다. 스위칭 회로는 입력된 제2 선택 신호에 따라 800MHz의 제2 클럭 신호는 신경 프로세싱 유닛에 공급한 다. 제2 선택 신호는 스케줄링 정보에 따라 제2 연산 단계 동안 공급된다. 제2 선택 신호는 대응되는 연산 단계의 PE 이용률을 기초로 선택될 수 있다. 제2 연산 단계의 PE 이용률은 제3 연산 단계의 PE 이용률보다 높을 수 있다. 스위칭 회로는 입력된 제3 선택 신호에 따라 1.0GHz의 제3 클럭 신호는 신경 프로세싱 유닛에 공급한 다. 제3 선택 신호는 스케줄링 정보에 따라 제3 연산 단계 동안 공급된다. 제3 선택 신호는 대응되는 연산 단계의 PE 이용률을 기초로 선택될 수 있다. 제3 연산 단계의 PE 이용률은 제2 연산 단계의 PE 이용률보다 낮을 수 있다. 몇몇 예시들에서는, 신경 프로세싱 유닛, 신경 프로세싱 유닛을 포함하는 SoC, 신경 프로세싱 유닛을 포함하는 SoC를 포함하는 시스템은 온도 센서를 더 포함할 수 있다. 상기 예시들에서는, 특정 임계 온도 이상 조건에서 선택적으로 구동 주파수를 더 낮게 설정하는 것도 가능하다. 이와 같이 특정 연산 단계에서 PE들의 이용률이 증가함에 따라, 주파수가 낮은 클럭 신호가 사용됨으로써, 특정 연산 단계의 파워가 순간적으로 상승되지 않을 수 있다. 순간적으로 PE들의 이용률이 증가하면, 피크 파워가 상 승되고, 결과적으로는 공급 전원(VDD)이 IR-drop margin 이하로 떨어질 수 있다. 즉, PE들의 이용률이 증가할 경우, 선택적으로 주파수가 낮은 클럭 신호가 사용될 수 있고 그로 인하여 공급 전원(VDD)이 IR-drop margin 이 하로 떨어지지 않을 수 있게 된다. <본 특허의 개시들의 간략 정리> 본 개시에서 제시되는 내용을 정리하면 다음과 같다. 먼저, 본 개시의 일 개시에 의하면 시스템이 제시된다. 상기 시스템은 ANN(artificial neural network) 모델들 중 적어도 하나의 ANN을 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 포함하는 NPU(neural processing unit)와; 그리고 스위칭 회로를 포함할 수 있다. 상기 스위칭 회로는: 서로 다른 주파수를 갖는 복 수의 클럭 신호들 중 하나의 클럭 신호를 선택하고, 그리고 상기 선택된 하나의 클럭 신호를 상기 NPU에 공급할 수 있다. 상기 하나의 클럭 신호는 상기 적어도 하나의 ANN 모델 내의 복수의 레이어들 중 주어진 레이어에 대 한 상기 PE들의 이용률에 기초하여 선택될 수 있다. 또한, 본 개시의 일 개시에 의하면, SoC(system-on-chip)가 제공된다. 상기 SoC는 ANN(artificial neural network) 모델들 중 적어도 하나의 ANN 모델을 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 위해 배치된 제1 회로와; 그리고 스위칭 회로를 포함할 수 있다. 상기 스위칭 회로는: 서로 다른 주파수를 갖는 복수의 클럭 신호들 중 하나의 클럭 신호를 선택하고, 그리고 상기 선택된 하나의 클럭 신호를 상기 NPU에 공급 할 수 있다. 상기 하나의 클럭 신호는 상기 적어도 하나의 ANN 모델 내의 복수의 레이어들 중 주어진 레이어에 대한 상기 PE들의 이용률에 기초하여 선택될 수 있다. 또한, 본 개시의 일 개시에 의하면, NPU(neural processing unit)를 구동하는 방법이 제공된다. 상기 NPU 구동 방법은 서로 다른 주파수를 갖는 복수의 클럭 신호들 중 하나의 클럭 신호를 선택하는 단계와; 상기 선택된 하 나의 클럭 신호를 상기 NPU에 공급하는 단계를 포함할 수 있다. 상기 NPU는 ANN(artificial neural network) 모델들 중 적어도 하나의 ANN을 위한 연산을 수행할 수 있는 복수의 PE(processing element)를 포함할 수 있다. 상기 하나의 클럭 신호는 상기 적어도 하나의 ANN 모델 내의 복수의 레이어들 중 주어진 레이어에 대한 상기 PE 들의 이용률에 기초하여 선택될 수 있다.전술한 개시들에 의한 시스템, SoC, 구동 방법에 적용될 수 있는 내용을 설명하면 다음과 같다. 상기 하나의 클럭 신호는, 상기 PE들의 이용률에 대한 임계값을 추가로 고려하여 선택될 수 있다. 상기 주어진 레이어에서 연산을 수행할 때의 파워를 줄이기 위하여, 상기 PE들의 이용률에 대한 임계값이 결정 될 수 있다. 상기 적어도 하나의 ANN 모델의 상기 주어진 레이어 보다 이전 레이어에 대한 연산을 상기 NPU가 완료했을 때, 상기 하나의 클럭 신호가 선택될 수 있다. 상기 적어도 하나의 ANN 모델의 상기 주어진 레이어 보다 이전 레이어에 대한 연산을 완료함으로써 상기 NPU에 의해서 출력되는 이전 레이어에 대한 정보에 기초하여 상기 하나의 클럭 신호가 상기 주어진 레이어에 대해서 선택될 수 있다. 상기 스위칭 회로는: 다수의 클럭 신호를 출력하는 생성기와; 그리고 상기 하나의 클럭 신호를 선택하는 선택기 를 포함할 수 있다. 상기 주어진 레이어에 대한 이용률이 이전 레이어 보다 높은 경우, 상기 주어진 레이어에 대해서 선택된 하나의 클럭 신호는 상기 이전 레이어의 제1 주파수 보다 낮은 제2 주파수를 가질 수 있다. 상기 주어진 레이어에 대한 이용률이 이전 레이어 보다 낮은 경우, 상기 주어진 레이어에 대해서 선택된 하나의 클럭 신호는 상기 이전 레이어의 제1 주파수 보다 높은 제2 주파수를 가질 수 있다. 상기 PE들의 이용률은 상기 적어도 하나의 ANN 모델의 복수의 레이어들 각각에 대해서 결정될 수 있다. 상기 PE들의 이용률은 상기 적어도 하나의 ANN 모델의 각 레이어에 대한 연산을 상기 복수의 PE들에게 할당하는 데 사용되는 스케줄링 정보에 기초하여 결정될 수 있다. 본 명세서와 도면에 나타난 본 개시의 예시들은 본 개시의 기술 내용을 쉽게 설명하고 본 개시의 이해를 돕기 위해 특정 예를 제시한 것뿐이며, 본 명의 범위를 한정하고자 하는 것은 아니다. 지금까지 설명한 예시들 이외 에도 다른 변형 예들이 실시 가능하다는 것은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 자명 한 것이다."}
{"patent_id": "10-2023-0124805", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 도 2a은 컨볼루션 신경망(CNN)의 기본 구조를 설명하기 위한 도면이다. 도 2b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 3은 본 개시의 일 예시에 따른 신경 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 4a는 본 개시의 일 예시에 적용될 수 있는 복수의 프로세싱 엘리먼트 중 하나의 프로세싱 엘리먼트를 설명하 는 개략적인 개념도이다. 도 4b는 본 개시의 일 예시에 적용될 수 있는 SFU를 설명하는 개략적인 개념도이다. 도 5는 도 3에 도시된 신경 프로세싱 유닛의 변형예를 나타낸 예시도이다. 도 6a은 예시적인 인공신경망모델 내의 각 레이어 별 데이터의 크기를 나타낸 예시도이고, 도 6b는 도 6a에 도 시된 예시적 인공신경망모델에서 각 레이어 별 데이터 사이즈 등을 나타낸 예시적인 테이블이다.도 7a는 본 개시의 일 예시에서 제시되는 기법을 간략하게 나타낸 예시도이다. 도 7b는 본 개시의 일 예시에서 제시되는 기법에서 사용되는 클럭 신호의 예를 나타낸 예시도이다. 도 7c는 본 개시의 일 예시에서 제시되는 기법을 구현하기 위한 하드웨어 구성을 간략하게 나타낸 개략도이다. 도 8a는 본 개시의 제1 개시의 일 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 8b는 본 개시의 제1 개시의 다른 일 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 9a는 본 개시의 제2 개시의 일 예시에 따른 시스템의 구조를 나타낸 예시도이다. 도 9b는 본 개시의 제2 개시의 일 예시에 따른 시스템의 구조를 나타낸 예시도이다. 도 10a는 본 개시의 제3 개시의 일 예시에 따른 신경 프로세싱 유닛의 구조를 나타낸 예시도이다. 도 10b는 본 개시의 제3 개시의 다른 예시에 따른 신경 프로세싱 유닛의 구조를 나타낸 예시도이다. 도 11은 PE들의 이용률이 증가함에 따라 피크 파워가 커지는 예시를 도시한다. 도 12는 PE들의 이용률에 따라 클럭 신호를 다르게 사용할 때 파워의 예시를 도시한다."}
