{"patent_id": "10-2020-0120742", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0037819", "출원번호": "10-2020-0120742", "발명의 명칭": "복수의 기동어를 인식하는 인공 지능 장치 및 그 방법", "출원인": "엘지전자 주식회사", "발명자": "최우진"}}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "복수의 기동어를 인식하는 인공 지능 장치에 있어서,마이크로폰;제1 기동어 인식 엔진을 저장하는 메모리; 제2 기동어 인식 엔진을 저장하는 인공 지능 서버와 통신하는 통신부; 및상기 마이크로폰을 통해 입력 오디오 신호를 수신하고, 상기 입력 오디오 신호로부터 전처리된 오디오 신호를생성하고, 상기 전처리된 오디오 신호에서 음성 구간을 추출하고, 상기 전처리된 오디오 신호 중에서 상기 음성구간 및 상기 음성 구간에 대응하는 버퍼 구간을 포함하는 기동어 인식 구간을 설정하고, 상기 전처리된 오디오신호 중에서 상기 기동어 인식 구간을 상기 제1 기동어 인식 엔진 및 상기 제2 기동어 인식 엔진에 전달하는 프로세서를 포함하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 프로세서는음성 활동 감지(VAD) 기능을 통해 상기 전처리된 오디오 신호에서 상기 음성 구간을 추출하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서,상기 프로세서는상기 음성 구간으로부터 제1 길이만큼의 이전 구간을 제1 버퍼 구간으로 설정하고, 상기 음성 구간으로부터 제2길이만큼의 이후 구간을 제2 버퍼 구간으로 설정하고, 상기 음성 구간, 상기 제1 버퍼 구간 및 상기 제2 버퍼구간을 포함하는 상기 기동어 인식 구간을 설정하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 2에 있어서,상기 프로세서는상기 제1 기동어 인식 엔진을 통해 제1 기동어에 대한 기동어 인식 결과를 획득하고, 상기 제2 기동어 인식 엔진을 통해 제2 기동어에 대한 기동어 인식 결과를 획득하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서,상기 프로세서는상기 제1 기동어 또는 상기 제2 기동어가 인식되면 상기 음성 활동 감지 기능을 비활성화하고, 상기 전처리된오디오 신호 중에서 인식된 기동어에 대한 기동어 구간 이후의 명령어 인식 구간에 대한 음성 인식 결과를 획득하고, 상기 음성 인식 결과에 기초한 동작을 수행하고, 상기 음성 활동 감지 기능을 활성화하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2022-0037819-3-청구항 5에 있어서,상기 프로세서는상기 인식된 기동어에 대응하는 음성 인식 플랫폼의 음성 엔진들을 이용하여 상기 명령어 인식 구간에 대한 상기 음성 인식 결과를 획득하고,상기 음성 엔진들은STT(Speech-To-Text) 엔진, 자연어 처리(NLP: Natural Language Processing) 엔진 및 음성 합성 엔진을 포함하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 4에 있어서,상기 프로세서는상기 제2 기동어 인식 엔진에 대한 API(Application Programming Interface)를 통해 상기 기동어 인식 구간을상기 인공 지능 서버로 전달하고, 상기 제2 기동어에 대한 기동어 인식 결과를 획득하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 2에 있어서,상기 프로세서는상기 전처리된 오디오 신호에서 상기 음성 활동 감지 기능을 이용하여 음성 존재 확률을 획득하고, 상기 음성존재 확률을 이용하여 상기 음성 구간을 추출하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 8에 있어서,상기 프로세서는상기 음성 존재 확률이 제1 기준값보다 큰 구간을 상기 음성 구간으로 추출하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 8에 있어서,상기 프로세서는상기 전처리된 오디오 신호의 진폭과 상기 음성 존재 확률을 곱한 값이 제2 기준값보다 큰 구간을 상기 음성 구간으로 추출하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "청구항 2에 있어서,상기 프로세서는동작 모드가 음성 등록 모드인 경우에는 상기 음성 활동 감지 기능을 비활성화하고, 음성 등록 기능의 종료 이후에 상기 음성 활동 감지 기능을 활성화하는, 인공 지능 장치."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "복수의 기동어를 인식하는 방법에 있어서,마이크로폰을 통해 입력 오디오 신호를 수신하는 단계;상기 입력 오디오 신호로부터 전처리된 오디오 신호를 생성하는 단계;상기 전처리된 오디오 신호에서 음성 구간을 추출하는 단계;공개특허 10-2022-0037819-4-상기 전처리된 오디오 신호 중에서 상기 음성 구간 및 상기 음성 구간에 대응하는 버퍼 구간을 포함하는 기동어인식 구간을 설정하는 단계; 및상기 전처리된 오디오 신호 중에서 상기 기동어 인식 구간을 메모리에 저장된 제1 기동어 인식 엔진 및 인공 지능 서버에 저장된 제2 기동어 인식 엔진에 전달하는 단계를 포함하는, 방법."}
{"patent_id": "10-2020-0120742", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "복수의 기동어를 인식하는 방법을 기록한 기록 매체에 있어서, 상기 방법은마이크로폰을 통해 입력 오디오 신호를 수신하는 단계;상기 입력 오디오 신호로부터 전처리된 오디오 신호를 생성하는 단계;상기 전처리된 오디오 신호에서 음성 구간을 추출하는 단계;상기 전처리된 오디오 신호 중에서 상기 음성 구간 및 상기 음성 구간에 대응하는 버퍼 구간을 포함하는 기동어인식 구간을 설정하는 단계; 및상기 전처리된 오디오 신호 중에서 상기 기동어 인식 구간을 메모리에 저장된 제1 기동어 인식 엔진 및 인공 지능 서버에 저장된 제2 기동어 인식 엔진에 전달하는 단계를 포함하는, 기록 매체."}
{"patent_id": "10-2020-0120742", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 실시 예는 복수의 기동어를 인식하는 인공 지능 장치에 있어서, 마이크로폰; 제1 기동어 인식 엔진 을 저장하는 메모리; 제2 기동어 인식 엔진을 저장하는 인공 지능 서버와 통신하는 통신부; 및 상기 마이크로폰 을 통해 입력 오디오 신호를 수신하고, 상기 입력 오디오 신호로부터 전처리된 오디오 신호를 생성하고, 상기 전 처리된 오디오 신호에서 음성 구간을 추출하고, 상기 전처리된 오디오 신호 중에서 상기 음성 구간 및 상기 음성 구간에 대응하는 버퍼 구간을 포함하는 기동어 인식 구간을 설정하고, 상기 전처리된 오디오 신호 중에서 상기 기동어 인식 구간을 상기 제1 기동어 인식 엔진 및 상기 제2 기동어 인식 엔진에 전달하는 프로세서를 포함하는, 인공 지능 장치를 제공한다."}
{"patent_id": "10-2020-0120742", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시(disclosure)는 복수의 기동어를 인식하는 인공 지능 장치 및 그 방법 에 관한 것이다."}
{"patent_id": "10-2020-0120742", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 음성 인식 기능이 탑재되어 사용자의 발화 음성을 인식하는 인공 지능 장치가 늘어나고 있다. 이러한 음 성 인식 기능은 통상적으로 미리 정해진 버튼 입력, 터치 입력 또는 음성 입력에 의하여 활성화되도록 설정되며, 음성 입력은 미리 정해진 기동어 (또는 음성 인식 기동어)가 인식되는 것을 의미할 수 있다. 음성 인식 기능의 활성화 여부를 판단함에 있어서 음성 인식 기동어를 인식하여야 한다는 점에서, 음성 인식 기동어 를 인식하기 위한 기동어 인식 모델은 거의 상시로 활성화되게 되며, 이에 따라 기동어 인식을 위한 적지 않은 리소스(resource)가 요구된다. 서로 다른 음성 인식 플랫폼은 서로 다른 기동어 인식 엔진을 이용하여 서로 다른 기동어를 인식하기 때문에, 하나의 인공 지능 장치에서 복수의 음성 인식 플랫폼을 지원하기 위하여는 하나의 인공 지능 장치에 복수의 기 동어 인식 엔진을 탑재하여야 한다. 그리고, 복수의 기동어 인식 엔진이 각자의 기동어를 인식하기 위하여 모 두 상시로 동작하며, 이 경우에 큰 리소스가 필요하여 프로세서 또는 중앙처리장치(CPU)의 점유율이 크게 상승 하는 문제점이 있다. 이와 같이 기동어 인식 엔진에 의한 CPU 점유율이 큰 경우, 인공 지능 장치가 다른 고부 하 작업의 수행이 느려지거나 반대로 다른 고부하 작업의 수행시 기동어의 인식이 정상적으로 동작하지 못할 수 있다."}
{"patent_id": "10-2020-0120742", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 복수의 음성 인식 기동어를 인식하는 인공 지능 장치 및 그 방법을 제공하고자 한다. 또한, 본 개시는 입력 오디오 신호 중에서 일부 구간에서만 복수의 음성 인식 기동어를 인식하는 인공 지능 장 치 및 그 방법을 제공하고자 한다."}
{"patent_id": "10-2020-0120742", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예는, 마이크로폰; 제1 기동어 인식 엔진을 저장하는 메모리; 제2 기동어 인식 엔진을 저장 하는 인공 지능 서버와 통신하는 통신부; 및 상기 마이크로폰을 통해 입력 오디오 신호를 수신하고, 상기 입력 오디오 신호로부터 전처리된 오디오 신호를 생성하고, 상기 전처리된 오디오 신호에서 음성 구간을 추출하고, 상기 전처리된 오디오 신호 중에서 상기 음성 구간 및 상기 음성 구간에 대응하는 버퍼 구간을 포함하는 기동어 인식 구간을 설정하고, 상기 전처리된 오디오 신호 중에서 상기 기동어 인식 구간을 상기 제1 기동어 인식 엔진 및 상기 제2 기동어 인식 엔진에 전달하는 프로세서를 포함하는, 복수의 기동어를 인식하는 인공 지능 장치를 제공한다. 상기 프로세서는 음성 활동 감지(VAD) 기능을 통해 상기 전처리된 오디오 신호에서 상기 음성 구간을 추출할 수 있다. 상기 프로세서는 상기 음성 구간으로부터 제1 길이만큼의 이전 구간을 제1 버퍼 구간으로 설정하고, 상기 음성 구간으로부터 제2 길이만큼의 이후 구간을 제2 버퍼 구간으로 설정하고, 상기 음성 구간, 상기 제1 버퍼 구간 및 상기 제2 버퍼 구간을 포함하는 상기 기동어 인식 구간을 설정할 수 있다. 상기 프로세서는 상기 제1 기동어 인식 엔진을 통해 제1 기동어에 대한 기동어 인식 결과를 획득하고, 상기 제2 기동어 인식 엔진을 통해 제2 기동어에 대한 기동어 인식 결과를 획득할 수 있다. 상기 상기 프로세서는 상기 제1 기동어 또는 상기 제2 기동어가 인식되면 상기 음성 활동 감지 기능을 비활성화 하고, 상기 전처리된 오디오 신호 중에서 인식된 기동어에 대한 기동어 구간 이후의 명령어 인식 구간에 대한 음성 인식 결과를 획득하고, 상기 음성 인식 결과에 기초한 동작을 수행하고, 상기 음성 활동 감지 기능을 활성 화할 수 있다. 상기 프로세서는 상기 인식된 기동어에 대응하는 음성 인식 플랫폼의 음성 엔진들을 이용하여 상기 명령어 인식 구간에 대한 상기 음성 인식 결과를 획득하고, 상기 음성 엔진들은 STT(Speech-To-Text) 엔진, 자연어 처리 (NLP: Natural Language Processing) 엔진 및 음성 합성 엔진을 포함할 수 있다. 상기 프로세서는 상기 제2 기동어 인식 엔진에 대한 API(Application Programming Interface)를 통해 상기 기 동어 인식 구간을 상기 인공 지능 서버로 전달하고, 상기 제2 기동어에 대한 기동어 인식 결과를 획득할 수 있 다. 상기 프로세서는 상기 전처리된 오디오 신호에서 상기 음성 활동 감지 기능을 이용하여 음성 존재 확률을 획득 하고, 상기 음성 존재 확률을 이용하여 상기 음성 구간을 추출할 수 있다. 상기 프로세서는 상기 음성 존재 확률이 제1 기준값보다 큰 구간을 상기 음성 구간으로 추출할 수 있다. 상기 프로세서는 상기 전처리된 오디오 신호의 진폭과 상기 음성 존재 확률을 곱한 값이 제2 기준값보다 큰 구 간을 상기 음성 구간으로 추출할 수 있다. 상기 프로세서는 동작 모드가 음성 등록 모드인 경우에는 상기 음성 활동 감지 기능을 비활성화하고, 음성 등록 기능의 종료 이후에 상기 음성 활동 감지 기능을 활성화할 수 있다. 또한, 본 개시의 일 실시 예는, 마이크로폰을 통해 입력 오디오 신호를 수신하는 단계; 상기 입력 오디오 신호 로부터 전처리된 오디오 신호를 생성하는 단계; 상기 전처리된 오디오 신호에서 음성 구간을 추출하는 단계; 상 기 전처리된 오디오 신호 중에서 상기 음성 구간 및 상기 음성 구간에 대응하는 버퍼 구간을 포함하는 기동어 인식 구간을 설정하는 단계; 및 상기 전처리된 오디오 신호 중에서 상기 기동어 인식 구간을 메모리에 저장된 제1 기동어 인식 엔진 및 인공 지능 서버에 저장된 제2 기동어 인식 엔진에 전달하는 단계를 포함하는, 복수의 기동어를 인식하는 방법 또는 그를 기록한 기록 매체를 제공한다."}
{"patent_id": "10-2020-0120742", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 다양한 실시 예에 따르면, 복수의 기동어 인식 모델을 탑재하여 하나의 인공 지능 장치에서 여러 음 성 인식 플랫폼을 지원할 수 있다. 또한, 본 개시의 다양한 실시 예에 따르면, 복수의 기동어 인식 모델을 탑재하더라도 유휴 상태에서의 복수의 기동어 인식 모델들이 소비하는 리소스를 효과적으로 줄일 수 있다."}
{"patent_id": "10-2020-0120742", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명 에서 사용되는 구성요소에 대한 접미사 '모듈' 및 '부'는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용 되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이 해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 개시의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 '연결되어' 있다거나 '접속되어' 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 '직접 연결되어' 있다거나 '직접 접속되어' 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 도 1은 본 개시의 일 실시 예에 따른 인공 지능 장치를 나타낸 블록도이다. 도 1을 참조하면, 인공 지능 장치는 원격 제어 장치, 사용자 단말기, 인공 지능 서버 또는 컨텐츠 제공자 중에서 적어도 하나 이상과 연결되어 데이터 또는 신호를 송수신할 수 있다. 인공 지능 장치는 디스플레이부(180, 또는 디스플레이 패널)를 포함하여 화상 출력이 가능한 디스플레이 장치일 수 있다. 예컨대, 인공 지능 장치는 TV, 프로젝터, 휴대폰, 스마트폰, 데스크탑 컴퓨터, 랩탑, 디 지털방송용 단말기, PDA(personal digital assistants), PMP(portable multimedia player), 네비게이션, 태블 릿 PC, 웨어러블 장치, 셋톱박스(STB), DMB 수신기, 라디오, 스피커, 세탁기, 냉장고, 디지털 사이니지, 로봇,차량 등과 같은, 고정형 기기 또는 이동 가능한 기기 등으로 구현될 수 있다. 사용자 단말기는 휴대 전화, 스마트폰, 태블릿 PC, 랩탑, 웨어러블 디바이스(wearable device), PDA 등으 로 구현될 수 있다. 사용자 단말기는 단순히 단말기라 칭할 수도 있다. 컨텐츠 제공자는 인공 지능 장치에서 출력할 컨텐츠에 상응하는 컨텐츠 데이터를 제공하는 장치를 의 미하며, 인공 지능 장치는 컨텐츠 제공자로부터 컨텐츠 데이터를 수신하여 컨텐츠를 출력할 수 있다. 인공 지능 장치는 통신부, 방송 수신부, 외부 장치 인터페이스부, 메모리, 입력부 , 프로세서, 디스플레이부, 오디오 출력부, 전원 공급부를 포함할 수 있다. 통신부는 유선 또는 무선 통신을 통해 외부 기기와 통신을 수행할 수 있다. 예컨대, 통신부는 다른 인공 지능 장치 등의 외부 장치들과 센서 정보, 사용자 입력, 학습 모델, 제어 신호 등을 송수신할 수 있다. 여기서, 다른 인공 지능 장치는 본 발명에 따른 인공 지능 장치와 데이터를 상호 교환하는 것이 가능 한(또는 연동 가능한) 웨어러블 디바이스(wearable device, 예를 들어, 스마트워치(smartwatch), 스마트 글래스 (smart glass), HMD(head mounted display)), 스마트 폰과 같은 이동 단말기가 될 수 있다. 통신부는 인공 지능 장치 주변의 통신 가능한 웨어러블 디바이스를 감지(또는 인식)할 수 있다. 나 아가, 프로세서는 감지된 웨어러블 디바이스가 인공 지능 장치와 통신하도록 인증된 디바이스인 경우, 인공 지능 장치에서 처리되는 데이터의 적어도 일부를 통신부를 통해 웨어러블 디바이스로 송 신할 수 있다. 따라서, 웨어러블 디바이스의 사용자는 인공 지능 장치에서 처리되는 데이터를 웨어러블 디바이스를 통해 이용할 수 있다. 통신부가 이용하는 통신 기술에는 GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), LTE(Long Term Evolution), 5G, WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), 블루투스 (Bluetooth쪠), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), ZigBee, NFC(Near Field Communication) 등이 있다. 통신부는 통신 모뎀(communication modem) 또는 통신 인터페이스(communication interface)라 칭할 수도 있다. 방송 수신부는 튜너, 복조부 및 네트워크 인터페이스부를 포함할 수 있다. 튜너는 채널 선국 명령에 따라 특정 방송 채널을 선국할 수 있다. 튜너는 선국된 특정 방송 채널에 대한 방송 신호를 수신할 수 있다. 복조부는 수신한 방송 신호를 비디오 신호, 오디오 신호, 방송 프로그램과 관련된 데이터 신호로 분리할 수 있고, 분리된 비디오 신호, 오디오 신호 및 데이터 신호를 출력이 가능한 형태로 복원할 수 있다. 외부 장치 인터페이스부는 인접하는 외부 장치 내의 애플리케이션 또는 애플리케이션 목록을 수신하여, 프 로세서 또는 메모리로 전달할 수 있다. 외부 장치 인터페이스부는 인공 지능 장치와 외부 장치 간의 연결 경로를 제공할 수 있다. 외부 장 치 인터페이스부는 인공 지능 장치에 무선 또는 유선으로 연결된 외부장치로부터 출력된 영상, 오디 오 중 하나 이상을 수신하여 프로세서로 전달할 수 있다. 외부 장치 인터페이스부는 복수의 외부 입 력 단자들을 포함할 수 있다. 복수의 외부 입력 단자들은 RGB 단자, 하나 이상의 HDMI(High Definition Multimedia Interface) 단자, 컴포넌트(Component) 단자를 포함할 수 있다. 외부 장치 인터페이스부를 통해 입력된 외부 장치의 영상 신호는 디스플레이부를 통해 출력될 수 있 다. 외부 장치 인터페이스부를 통해 입력된 외부장치의 음성 신호는 오디오 출력부를 통해 출력될 수 있다. 외부 장치 인터페이스부에 연결 가능한 외부 장치는 셋톱박스, 블루레이 플레이어, DVD 플레이어, 게임기, 사운드 바, 스마트폰, PC, USB 메모리, 홈 씨어터 중 어느 하나일 수 있으나, 이는 예시에 불과하다. 네트워크 인터페이스부는 인공 지능 장치를 인터넷을 포함하는 유/무선 네트워크와 연결하기 위한 인 터페이스를 제공할 수 있다. 네트워크 인터페이스부는 접속된 네트워크 또는 접속된 네트워크에 링크된 다른 네트워크를 통해, 다른 사용자 또는 다른 전자 기기와 데이터를 송신 또는 수신할 수 있다. 인공 지능 장치에 미리 등록된 다른 사용자 또는 다른 전자 기기 중 선택된 사용자 또는 선택된 전자기기 에 인공 지능 장치에 저장된 일부의 컨텐츠 데이터를 송신할 수 있다. 네트워크 인터페이스부는 접속된 네트워크 또는 접속된 네트워크에 링크된 다른 네트워크를 통해, 소정 웹 페이지에 접속할 수 있다. 즉, 네트워크를 통해 소정 웹 페이지에 접속하여, 해당 서버와 데이터를 송신 또는 수신할 수 있다. 네트워크 인터페이스부는 컨텐츠 제공자 또는 네트워크 운영자가 제공하는 컨텐츠 또는 데이터들을 수신할 수 있다. 즉, 네트워크 인터페이스부는 네트워크를 통하여 컨텐츠 제공자 또는 네트워크 제공자로부터 제 공되는 영화, 광고, 게임, VOD, 방송 신호 등의 컨텐츠 및 그와 관련된 정보를 수신할 수 있다. 네트워크 인터페이스부는 네트워크 운영자가 제공하는 펌웨어의 업데이트 정보 및 업데이트 파일을 수신할 수 있으며, 인터넷 또는 컨텐츠 제공자 또는 네트워크 운영자에게 데이터들을 송신할 수 있다. 네트워크 인터페이스부는 네트워크를 통해, 공중에 공개(open)된 애플리케이션들 중 원하는 애플리케이션 을 선택하여 수신할 수 있다. 메모리는 프로세서 내의 각 신호 처리 및 제어를 위한 프로그램이 저장하고, 신호 처리된 영상, 음성 또는 데이터 신호를 저장할 수 있다. 예컨대, 메모리는 입력부에서 획득한 입력 데이터, 학습 데이 터, 학습 모델, 학습 히스토리 등을 저장할 수 있다. 메모리는 외부 장치 인터페이스부 또는 네트워크 인터페이스부로부터 입력되는 영상, 음성, 또 는 데이터 신호의 임시 저장을 위한 기능을 수행할 수도 있으며, 채널 기억 기능을 통하여 소정 이미지에 관한 정보를 저장할 수도 있다. 메모리는 외부 장치 인터페이스부 또는 네트워크 인터페이스부로부터 입력되는 애플리케이션 또 는 애플리케이션 목록을 저장할 수 있다. 인공 지능 장치는 메모리 내에 저장되어 있는 컨텐츠 파일(동영상 파일, 정지영상 파일, 음악 파일, 문서 파일, 애플리케이션 파일 등)을 재생하여 사용자에게 제공할 수 있다. 입력부는 다양한 종류의 데이터를 획득할 수 있다. 입력부는 영상 신호 입력을 위한 카메라, 오디오 신호를 수신하기 위한 마이크로폰, 사용자로부터 정보를 입력 받기 위한 사용자 입력부 등을 포함할 수 있다. 사용자 입력부는 사용자가 입력한 신호를 프로세서로 전달하거나, 프로세서로부터의 신호를 사용자에 게 전달할 수 있다. 예를 들어, 사용자 입력부는 블루투스(Bluetooth), WB(Ultra Wideband), 지그비(ZigBee) 방식, RF(Radio Frequency) 통신 방식 또는 적외선(IR) 통신 방식 등 다양한 통신 방식에 따라, 원격 제어 장치 로부터 전원 온/오프, 채널 선택, 화면 설정 등의 제어 신호를 수신하여 처리하거나, 프로세서로부터 의 제어 신호를 원격 제어 장치로 송신하도록 처리할 수 있다. 사용자 입력부는 전원키, 채널키, 볼륨키, 설정치 등의 로컬키(미도시)에서 입력되는 제어 신호를 프로세서 에 전달할 수 있다. 러닝 프로세서는 학습 데이터를 이용하여 인공 신경망으로 구성된 모델을 학습시킬 수 있다. 여기서, 학 습된 인공 신경망을 학습 모델이라 칭할 수 있다. 학습 모델은 학습 데이터가 아닌 새로운 입력 데이터에 대하 여 결과 값을 추론해 내는데 사용될 수 있고, 추론된 값은 어떠한 동작을 수행하기 위한 판단의 기초로 이용될 수 있다. 러닝 프로세서는 인공 지능 서버의 러닝 프로세서과 함께 인공 지능 프로세싱을 수행할 수 있다. 러닝 프로세서는 인공 지능 장치에 통합되거나 구현된 메모리를 포함할 수 있다. 또는, 러닝 프로세 서는 메모리, 인공 지능 장치에 직접 결합된 외부 메모리 또는 외부 장치에서 유지되는 메모리 를 사용하여 구현될 수도 있다. 프로세서에서 영상 처리된 영상 신호는 디스플레이부로 입력되어 해당 영상 신호에 대응하는 영상으 로 표시될 수 있다. 또한, 프로세서에서 영상 처리된 영상 신호는 외부 장치 인터페이스부를 통하여 외부 출력장치로 입력될 수 있다. 프로세서에서 처리된 음성 신호는 오디오 출력부로 오디오 출력될 수 있다. 또한, 프로세서에 서 처리된 음성 신호는 외부 장치 인터페이스부를 통하여 외부 출력장치로 입력될 수 있다. 프로세서는 인공 지능 장치의 전반적인 동작을 제어할 수 있다. 프로세서는 사용자 입력부를 통하여 입력된 사용자 명령 또는 내부 프로그램에 의하여 인공 지능 장치 를 제어할 수 있으며, 네트워크에 접속하여 사용자가 원하는 애플리케이션 또는 애플리케이션 목록을 인공 지능 장치 내로 다운받을 수 있도록 할 수 있다. 프로세서는 사용자가 선택한 채널 정보 등이 처리한 영상 또는 음성신호와 함께 디스플레이부 또는 오디오 출력부를 통하여 출력될 수 있도록 한다. 프로세서는 사용자 입력부를 통하여 수신한 외부장치 영상 재생 명령에 따라, 외부 장치 인터페이스부 를 통하여 입력되는 외부 장치, 예를 들어, 카메라 또는 캠코더로부터의, 영상 신호 또는 음성 신호가 디 스플레이부 또는 오디오 출력부를 통해 출력될 수 있도록 한다. 프로세서는 영상을 표시하도록 디스플레이부를 제어할 수 있으며, 예를 들어 튜너를 통해 입력 되는 방송 영상, 또는 외부 장치 인터페이스부를 통해 입력되는 외부 입력 영상, 또는 네트워크 인터페이 스부를 통해 입력되는 영상, 또는 메모리에 저장된 영상이 디스플레이부에서 표시되도록 제어할 수 있다. 이 경우, 디스플레이부에 표시되는 영상은 정지 영상 또는 동영상일 수 있으며, 2D 영상 또는 3D 영상일 수 있다. 프로세서는 인공 지능 장치 내에 저장된 컨텐츠, 또는 수신된 방송 컨텐츠, 외부로 부터 입력되는 외 부 입력 컨텐츠가 재생되도록 제어할 수 있으며, 상기 컨텐츠는 방송 영상, 외부 입력 영상, 오디오 파일, 정지 영상, 접속된 웹 화면, 및 문서 파일 등 다양한 형태일 수 있다. 프로세서는 데이터 분석 알고리즘 또는 머신 러닝 알고리즘을 사용하여 결정되거나 생성된 정보에 기초하 여, 인공 지능 장치의 적어도 하나의 실행 가능한 동작을 결정할 수 있다. 그리고, 프로세서는 인공 지능 장치의 구성요소들을 제어하여 결정된 동작을 수행할 수 있다. 이를 위해, 프로세서는 러닝 프로세서 또는 메모리의 데이터를 요청, 검색, 수신 또는 활용할 수 있고, 상기 적어도 하나의 실행 가능한 동작 중 예측되는 동작이나, 바람직한 것으로 판단되는 동작을 실행 하도록 인공 지능 장치의 구성요소들을 제어할 수 있다. 프로세서는 사용자 입력에 대하여 의도 정보를 획득하고, 획득한 의도 정보에 기초하여 사용자의 요구 사 항을 결정할 수 있다. 프로세서는 음성 입력을 문자열로 변환하기 위한 STT(Speech To Text) 엔진 또는 자연어의 의도 정보를 획 득하기 위한 자연어 처리(NLP: Natural Language Processing) 엔진 중에서 적어도 하나 이상을 이용하여, 사용 자 입력에 상응하는 의도 정보를 획득할 수 있다. STT 엔진 또는 NLP 엔진 중에서 적어도 하나 이상은 적어도 일부가 머신 러닝 알고리즘에 따라 학습된 인공 신 경망으로 구성될 수 있다. 그리고, STT 엔진 또는 NLP 엔진 중에서 적어도 하나 이상은 러닝 프로세서에 의해 학습된 것이나, 인공 지능 서버의 러닝 프로세서에 의해 학습된 것이거나, 또는 이들의 분산 처 리에 의해 학습된 것일 수 있다. 프로세서는 인공 지능 장치의 동작 내용이나 동작에 대한 사용자의 피드백 등을 포함하는 이력 정보 를 수집하여 메모리 또는 러닝 프로세서에 저장하거나, 인공 지능 서버 등의 외부 장치에 전송 할 수 있다. 수집된 이력 정보는 학습 모델을 갱신하는데 이용될 수 있다. 디스플레이부는 프로세서에서 처리된 영상 신호, 데이터 신호, OSD 신호 또는 외부 장치 인터페이스 부에서 수신되는 영상 신호, 데이터 신호 등을 각각 R, G, B 신호로 변환하여 이미지를 출력할 수 있다. 한편, 도 1에 도시된 인공 지능 장치는 본 개시의 일 실시 예에 불과하며, 도시된 구성요소들 중 일부는 실제 구현되는 인공 지능 장치의 사양에 따라 통합, 추가, 또는 생략될 수 있다. 일 실시 예에서, 인공 지능 장치의 2 이상의 구성요소가 하나의 구성요소로 합쳐지거나, 혹은 하나의 구성 요소가 2 이상의 구성요소로 세분되어 구성될 수 있다. 또한, 각 블록에서 수행하는 기능은 본 개시의 실시 예 를 설명하기 위한 것이며, 그 구체적인 동작이나 장치는 본 개시의 권리 범위를 제한하지 아니한다. 본 개시의 일 실시 예에 따르면, 인공 지능 장치는, 도 1에 도시된 바와 달리, 튜너와 복조부를 구비하지 않고 네트워크 인터페이스부 또는 외부 장치 인터페이스부를 통해서 영상을 수신하여 재생할 수도 있다. 예를 들어, 인공 지능 장치는 방송 신호 또는 다양한 네트워크 서비스에 따른 컨텐츠들을 수신하기 위한 등과 같은 셋탑 박스 등과 같은 영상 처리 장치와 상기 영상 처리 장치로부터 입력되는 컨텐츠를 재생하는 컨텐츠 재생 장치로 분리되어 구현될 수 있다. 이 경우, 이하에서 설명할 본 개시의 일 실시 예에 따 른 인공 지능 장치의 동작 방법은 도 1을 참조하여 설명한 바와 같은 인공 지능 장치뿐 아니라, 상기 분리 된 셋탑 박스 등과 같은 영상 처리 장치 또는 디스플레이부 및 오디오 출력부를 구비하는 컨텐츠 재 생 장치 중 어느 하나에 의해 수행될 수도 있다. 도 2는 본 개시의 일 실시 예에 따른 원격 제어 장치를 나타낸 블록도이다. 도 2를 참조하면, 원격 제어 장치는 지문 인식부, 통신부, 사용자 입력부, 센서부, 출력부, 전원 공급부, 메모리, 프로세서, 음성 획득부를 포함할 수 있다. 통신부는 전술하여 설명한 본 개시의 실시 예들에 따른 인공 지능 장치 중 임의의 어느 하나와 신호 를 송수신할 수 있다. 원격 제어 장치는 RF 통신 규격에 따라 인공 지능 장치와 신호를 송수신할 수 있는 RF 모듈을 구비하며, IR 통신 규격에 따라 인공 지능 장치와 신호를 송수신할 수 있는 IR 모듈을 구비할 수 있 다. 또한, 원격 제어 장치는 블루투스 통신 규격에 따라 인공 지능 장치와 신호를 송수신할 수 있는 블루투스 모듈를 구비할 수 있다. 또한, 원격 제어 장치는 NFC(Near Field Communication) 통신 규 격에 따라 인공 지능 장치와 신호를 송수할 수 있는 NFC 모듈을 구비하며, WLAN(Wireless LAN) 통신 규격에 따라 인공 지능 장치와 신호를 송수신할 수 있는 WLAN 모듈을 구비할 수 있다. 통신부를 통해 인공 지능 장치로 원격 제어 장치의 움직임 등에 관한 정보가 담긴 신호를 통신 부를 통해 전송할 수 있다. 원격 제어 장치는 인공 지능 장치가 전송한 신호를 RF 모듈을 통하여 수신할 수 있으며, 필요에 따라 IR 모듈을 통하여 인공 지능 장치로 전원 온/오프, 채널 변경, 볼륨 변경 등에 관한 명령을 전 송할 수 있다. 사용자 입력부는 키패드, 버튼, 터치 패드, 또는 터치 스크린 등으로 구성될 수 있다. 사용자는 사용자 입력부를 조작하여 원격 제어 장치으로 인공 지능 장치와 관련된 명령을 입력할 수 있다. 사용 자 입력부가 하드 키 버튼을 구비할 경우, 사용자는 하드 키 버튼의 푸쉬 동작을 통하여 원격 제어 장치 으로 인공 지능 장치와 관련된 명령을 입력할 수 있다. 사용자 입력부에 터치 스크린이 포함될 경우, 사용자는 터치 스크린의 소프트 키를 터치하여 원격 제어 장 치로 인공 지능 장치와 관련된 명령을 입력할 수 있다. 또한, 사용자 입력부는 스크롤 키나, 조그 키 등 사용자가 조작할 수 있는 다양한 종류의 입력 수단을 구비할 수 있다. 센서부는 자이로 센서 또는 가속도 센서를 구비할 수 있으며, 자이로 센서는 원격 제어 장 치의 움직임에 관한 정보를 센싱할 수 있다. 예를 들어, 자이로 센서는 원격 제어 장치의 동작 에 관한 정보를 x,y,z 축을 기준으로 센싱할 수 있으며, 가속도 센서는 원격 제어 장치의 이동 속도 등에 관한 정보를 센싱할 수 있다. 한편, 원격 제어 장치는 거리 측정 센서를 더 구비할 수 있어, 인공 지능 장치의 디스플레이부와의 거리를 센싱할 수 있다. 출력부는 사용자 입력부의 조작에 대응하거나 인공 지능 장치에서 전송한 신호에 대응하는 영상 또는 음성 신호를 출력할 수 있다. 사용자는 출력부를 통하여 사용자 입력부의 조작 여부 또는 인공 지능 장치의 제어 여부를 인지할 수 있다. 예를 들어, 출력부는 사용자 입력부가 조작되거나 통신부를 통하여 인공 지능 장치와 신호가 송수신되면 점등되는 LED 모듈, 진동을 발생하는 진 동 모듈, 음향을 출력하는 음향 출력 모듈, 또는 영상을 출력하는 디스플레이 모듈을 구비할 수 있다. 전원 공급부는 원격 제어 장치으로 전원을 공급할 수 있다. 전원 공급부는 원격 제어 장치 이 소정 시간 동안 움직이지 않은 경우 전원 공급을 중단함으로서 전원 낭비를 줄일 수 있다. 전원 공급 부는 원격 제어 장치에 구비된 소정 키가 조작된 경우에 전원 공급을 재개할 수 있다. 메모리는 원격 제어 장치의 제어 또는 동작에 필요한 여러 종류의 프로그램, 애플리케이션 데이터 등 을 저장할 수 있다. 원격 제어 장치가 RF 모듈을 통하여 인공 지능 장치와 무선으로 신호를 송수신할 경우, 원격 제 어 장치과 인공 지능 장치는 소정 주파수 대역을 통하여 신호를 송수신할 수 있다. 이를 위하여, 원 격 제어 장치의 프로세서는 원격 제어 장치과 페어링된 인공 지능 장치와 신호를 무선으로 송수신할 수 있는 주파수 대역 등에 관한 정보를 메모리에 저장하고 참조할 수 있다. 프로세서는 원격 제어 장치의 제어에 관련된 제반사항을 제어할 수 있다. 프로세서는 사용자 입력부의 소정 키 조작에 대응하는 신호 또는 센서부에서 센싱한 원격 제어 장치의 움직임에 대 응하는 신호를 통신부를 통하여 인공 지능 장치로 전송할 수 있다. 음성 획득부는 음성을 획득할 수 있다. 음성 획득부는 적어도 하나 이상의 마이크을 포함할 수 있고, 마이크를 통해 음성을 획득할 수 있다. 도 3은 본 개시의 일 실시 예에 따른 원격 제어 장치를 나타낸 도면이다. 도 3을 참조하면, 원격 제어 장치는 복수의 버튼을 포함할 수 있다. 원격 제어 장치에 포함되는 복 수의 버튼에는 지문 인식 버튼, 전원 버튼, 홈 버튼, 라이브 버튼, 외부 입력 버튼, 음량 조절 버튼, 음성 인식 버튼, 채널 변경 버튼, 확인 버튼 및 뒤로 가기 버튼 등 이 포함될 수 있다. 지문 인식 버튼은 사용자의 지문을 인식하기 위한 버튼일 수 있다. 일 실시 예에서로, 지문 인식 버튼 은 푸쉬 동작이 가능하여, 푸쉬 동작 및 지문 인식 동작을 수신할 수도 있다. 전원 버튼은 인공 지 능 장치의 전원을 온/오프 하기 위한 버튼일 수 있다. 홈 버튼은 인공 지능 장치의 홈 화면으 로 이동하기 위한 버튼일 수 있다. 라이브 버튼은 실시간 방송 프로그램을 디스플레이 하기 위한 버튼일 수 있다. 외부 입력 버튼은 인공 지능 장치에 연결된 외부 입력을 수신하기 위한 버튼일 수 있다. 음량 조절 버튼은 인공 지능 장치가 출력하는 음량의 크기를 조절하기 위한 버튼일 수 있다. 음성 인식 버튼은 사용자의 음성을 수신하고, 수신된 음성을 인식하기 위한 버튼일 수 있다. 채널 변경 버튼 은 특정 방송 채널의 방송 신호를 수신하기 위한 버튼일 수 있다. 확인 버튼은 특정 기능을 선택하 기 위한 버튼일 수 있고, 뒤로 가기 버튼은 이전 화면으로 되돌아가기 위한 버튼일 수 있다. 도 4는 본 개시의 일 실시 예에서 원격 제어 장치를 통하여 인공 지능 장치와 상호 작용하는 예시를 나타낸다. 도 4를 참조하면, 원격 제어 장치에 대응하는 포인터가 디스플레이부에 표시될 수 있다. 도 4의 (a)를 참조하면, 사용자는 원격 제어 장치를 상/하, 좌/우로 움직이거나 회전할 수 있다. 인공 지능 장치의 디스플레이부에 표시된 포인터는 원격 제어 장치의 움직임에 대응하여 움직일 수 있다. 원격 제어 장치는 3D 공간 상의 움직임에 따라 해당 포인터가 이동되어 표시되므로, 공간 리모콘이라 명명할 수 있다. 도 4의 (b)를 참조하면, 사용자가 원격 제어 장치를 왼쪽으로 이동하면, 인공 지능 장치의 디스플레 이부에 표시된 포인터도 이에 대응하여 왼쪽으로 이동할 수 있다. 원격 제어 장치의 센서를 통하여 감지된 원격 제어 장치의 움직임에 관한 정보는 인공 지능 장치 로 전송될 수 있다. 인공 지능 장치는 원격 제어 장치의 움직임에 관한 정보로부터 포인터 의 좌표를 산출하고, 산출한 좌표에 대응하도록 포인터를 표시할 수 있다. 도 4의 (c)를 참조하면, 사용자가 원격 제어 장치 내의 특정 버튼을 누른 상태에서 원격 제어 장치를 디스플레이부에서 멀어지도록 이동시키는 경우, 포인터에 대응하는 디스플레이부 내의 선택 영 역이 줌 인되어 확대 표시될 수 있다. 이와 반대로, 사용자가 원격 제어 장치 내의 특정 버튼을 누른 상 태에서 원격 제어 장치를 디스플레이부에서 가까워지도록 이동시키는 경우, 포인터에 대응하는 디스플레이부 내의 선택 영역이 줌 아웃되어 축소 표시될 수 있다. 한편, 원격 제어 장치가 디스플레이부에서 멀어지는 경우에 선택 영역이 줌 아웃되고, 원격 제어 장 치가 디스플레이부에 가까워지는 경우에 선택 영역이 줌 인될 수도 있다. 또한, 원격 제어 장치 내의 특정 버튼을 누른 상태에서는 상/하, 좌/우 이동의 인식이 배제될 수 있다. 즉, 원격 제어 장치가 디스플레이부에서 멀어지거나 접근하도록 이동하는 경우, 원격 제어 장치(20 0)의 상/하/좌/우 이동은 인식되지 않고, 앞/뒤 이동만 인식될 수도 있다. 이 경우, 원격 제어 장치 내의 특정 버튼을 누르지 않은 상태에서는 원격 제어 장치의 상/하/좌/우 이동에 따라 포인터만 이동할 수 있다. 포인터의 이동 속도나 이동 방향은 원격 제어 장치의 이동 속도나 이동 방향에 대응할 수 있다. 본 개시에서 포인터는 원격 제어 장치의 동작에 대응하여 디스플레이부에 표시되는 오브젝트를 의미할 수 있다. 따라서, 포인터는 도 4에 도시된 화살표 형상 외에 다양한 형상의 오브젝트도 가능하다. 예를 들어, 포인터는 점, 커서, 프롬프트, 두꺼운 외곽선 등을 포함할 수 있다. 그리고, 포인터가 디스플레이부 상의 가로 축과 세로 축 중 어느 한 지점(point)에 대응하여 표시되는 것은 물론, 선(line), 면(surface) 등 복수 지점에 대응하여 표시될 수도 있다. 도 5는 본 개시의 일 실시 예에 따른 인공 지능 서버를 나타낸 블록도이다. 도 5를 참조하면, 인공 지능 서버는 머신 러닝 알고리즘을 이용하여 인공 신경망을 학습시키거나 학습된 인공 신경망을 이용하는 장치를 의미할 수 있다. 여기서, 인공 지능 서버는 복수의 서버들로 구성되어 분 산 처리를 수행할 수도 있고, 5G 네트워크로 정의될 수 있다. 인공 지능 서버는 인공 지능 장치의 인공 지능 프로세싱 중 적어도 일부를 함께 수행할 수도 있다. 인공 지능 프로세싱은 인공 지능 모델의 학습에 필요한 연산을 의미할 수 있다. 인공 지능 서버는 통신부, 메모리, 러닝 프로세서 및 프로세서 등을 포함할 수 있다. 통신부는 인공 지능 장치 등의 외부 장치와 데이터를 송수신할 수 있다. 메모리는 모델 저장부를 포함할 수 있다. 모델 저장부는 러닝 프로세서을 통하여 학습 중 인 또는 학습된 모델(431a, 또는 인공 신경망)을 저장할 수 있다. 러닝 프로세서는 학습 데이터를 이용하여 인공 신경망(431a)을 학습시킬 수 있다. 학습 모델은 인공 신경 망의 인공 지능 서버에 탑재된 상태에서 이용되거나, 인공 지능 장치 등의 외부 장치에 탑재되어 이 용될 수도 있다. 학습 모델은 하드웨어, 소프트웨어 또는 하드웨어와 소프트웨어의 조합으로 구현될 수 있다. 학습 모델의 일부 또는 전부가 소프트웨어로 구현되는 경우 학습 모델을 구성하는 하나 이상의 명령어(instruction)는 메모리 에 저장될 수 있다. 프로세서는 학습 모델을 이용하여 새로운 입력 데이터에 대하여 결과 값을 추론하고, 추론한 결과 값에 기 초한 응답이나 제어 명령을 생성할 수 있다. 음성 인식이 이루어지는 과정은 크게 음성 인식 기능을 활성화하기 위한 음성 인식 기동어를 인식하는 단계와 음성 인식 기능이 활성화된 상태에서 발화된 음성을 인식하는 단계로 이루어진다. 음성 인식 기동어는 미리 설 정된 (특히 제조자나 개발자에 의하여 설정된) 단어일 수 있다. 통상적으로, 음성 인식 기능이 활성화된 상태에서 발화된 음성은 미리 설정된 단어를 인식하는 것이 아닌 여러 단어들로 구성된 다양한 문장을 인식하여야 한다는 점에서, 일반적인 발화 음성을 인식하는 음성 엔진(예컨대, STT 엔진, NLP 엔진, NLU 엔진 등)은 음성 인식 기동어를 인식하는 기동어 인식 엔진보다 훨씬 복잡하고 많은 연산을 필요로한다. 이에 따라, 인공 지능 장치는 프로세서가 충분한 연산력을 가진 경우에 직접 음 성 엔진을 이용하여 일반적인 발화 음성을 인식할 수 있으며, 연산력이 충분하지 않을 경우에는 외부의 인공 지 능 서버를 통해 일반적인 발화 음성을 인식할 수 있다. 이에 반하여, 음성 인식 기동어를 인식하기 위한 기동어 인식 엔진은 미리 설정된 기동어만 인식하면 충분하기 때문에 일반적인 발화 음성을 인식하는 음성 엔진에 비하여 덜 복잡하고 적은 연산을 필요로 한다. 이에 따라, 인공 지능 장치는 외부의 인공 지능 서버의 도움 없이도 내부에 탑재된 기동어 인식 엔진을 이용하여음성 인식 기동어를 인식할 수 있다. 도 6은 본 개시의 일 실시 예에 따른 복수의 음성 인식 기동어를 인식하는 방법을 나타낸 흐름도이다. 도 6을 참조하면, 인공 지능 장치의 프로세서는 마이크로폰을 통해 입력 오디오 신호(input audio signal)를 수신한다(S601). 인공 지능 장치는 음성 인식 기능을 제공하기 위하여 마이크로폰(미도시)를 상시로 동작시킬 수 있고, 프 로세서는 음성 인식 기능을 제공하기 위하여 마이크로폰(미도시)를 통해 상시로 입력 오디오 신호를 수신 할 수 있다. 입력 오디오 신호가 상시로 수신된다는 점에서, 입력 오디오 신호는 입력 오디오 스트림(input audio stream)이라 칭할 수도 있다. 입력 오디오 신호에는 사용자의 음성이 포함될 수도 있고, 음성이 포함되지 않을 수도 있다. 또한, 입력 오디 오 신호에 사용자의 음성이 포함되더라도 음성 인식 기동어가 포함될 수도 있고, 음성 인식 기동어가 포함되지 않을 수도 있다. 그리고, 인공 지능 장치의 프로세서는 입력 오디오 신호로부터 전처리된(pre-processed) 오디오 신호 를 생성한다(S603). 입력 오디오 신호에 대한 전처리에는 노이즈 제거, 음성 강조 등이 포함될 수 있다. 사용자의 목소리를 제외한 모든 소리를 노이즈로 간주할 수 있고, 노이즈에는 주변의 소음뿐만 아니라 인공 지능 장치의 오디오 출력 부에서 출력되는 소리도 포함될 수 있다. 프로세서는 오디오 출력부를 통해 출력하는 출력 오디오 신호를 고려하여 입력 오디오 신호에서 출력 오디오 신호에 대응하는 소리 (또는 오디오 신호)를 제거할 수 있다. 또한, 프로세서는 대역 통과 필터 등으로 구성된 노이즈 제거 엔진을 이용하여 입력 오디오 신호에 포함된 노이즈를 제거할 수 있다. 이하에서, 오디오 신호는 전처리된 오디오 신호를 의미할 수 있다. 그리고, 인공 지능 장치의 프로세서는 전처리된 오디오 신호에서 음성 구간을 추출한다(S605). 프로세서는 전처리된 오디오 신호에서 음성이 포함된 구간인 음성 구간을 추출할 수 있다. 일 실시 예에서, 프로세서는 음성 활동 감지(VAD: Voice Activation Detection)를 통해 전처리된 오디오 신호에서 음성 구간을 추출할 수 있다. 음성 활동 감지는 전처리된 오디오 신호에서 음성이 포함된 음성 구간 과 음성이 포함되지 않은 비음성 구간을 구분하는 능력 또는 기능을 의미할 수 있다. 비음성 구간은 전처리된 오디오 신호에서 사용자의 음성이 전혀 포함되지 않은 구간만을 의미할 수도 있고, 사용자의 음성의 크기가 기 준값(reference value)보다 작은 구간을 포함하는 구간을 의미할 수도 있다 그리고, 인공 지능 장치의 프로세서는 전처리된 오디오 신호 중에서 음성 구간 및 음성 구간에 대응 하는 버퍼 구간을 포함하는 기동어 인식 구간을 설정한다(S607). 기동어 인식 구간은 음성 구간과 음성 구간에 대응하는 버퍼 구간을 포함할 수 있고, 전처리된 오디오 신호 중 에서 기동어를 인식하기 위하여 기동어 인식 엔진들에 전달되는 구간을 의미할 수 있다. 즉, 기동어 인식 구간 은 전처리된 오디오 신호에서 기동어가 포함될 가능성이 존재하는 구간을 의미할 수 있다. 버퍼 구간에는 추출된 음성 구간으로부터 제1 길이만큼의 이전 구간으로 구성된 제1 버퍼 구간 및 추출된 음성 구간으로부터 제2 길이만큼의 이후 구간으로 구성된 제2 버퍼 구간이 포함될 수 있다. 제1 버퍼 구간은 이전 버퍼 구간이라 칭할 수 있고, 제2 버퍼 구간은 이후 버퍼 구간이라 칭할 수 있다. 예컨대, 제1 버퍼 구간은 4 초 등으로 설정될 수 있고, 제2 버퍼 구간은 2~3초 등으로 설정될 수 있다. 기동어 인식 엔진들은 주변의 소리 대비 기동어의 발화 여부를 판단하기 때문에 음성 구간만을 이용하여 기동어 를 인식할 경우에는 음성 구간과 그 주변 구간을 이용하여 기동어를 인식하는 경우보다 정확도가 낮아지는 문제 점이 있다. 이러한 문제를 방지하기 위하여, 프로세서는 음성 구간뿐만 아니라 음성 구간에 대응하는 제1 버퍼 구간 및 제2 버퍼 구간도 포함하여 기동어 인식 구간을 설정할 수 있다. 프로세서는 모든 음성 구간을 포함하여 기동어 인식 구간을 설정하며, 나아가 각 음성 음성 구간에 대응하 는 제1 버퍼 구간과 제2 버퍼 구간을 추가로 포함하여 기동어 인식 구간을 설정할 수 있다. 즉, 특정 음성 구 간에 대응하는 제2 버퍼 구간에 다른 음성 구간 또는 다른 음성 구간의 제1 버퍼 구간이 중첩된다면, 두 음성구간과 그 사이의 구간이 모두 기동어 인식 구간에 포함될 수 있다. 그리고, 인공 지능 장치의 프로세서는 전처리된 오디오 신호 중에서 기동어 인식 구간을 복수의 기동 어 인식 엔진들 각각에 전달한다(S609). 기동어 인식 엔진들은 인공 지능 장치의 메모리에 저장될 수도 있고, 외부 서버에 저장될 수도 있다. 실시 예에 따라, 복수의 기동어 인식 엔진들이 모두 메모리에 저장될 수도 있고, 일부가 메모리에 저 장될 수도 있고, 모두 외부 서버에 저장될 수도 있다. 기동어 인식 엔진이 외부 서버에 저장되는 경우, 각 기 동어 인식 엔진은 서로 다른 외부 서버에 저장될 수 있다. 복수의 기동어 인식 엔진들은 각 음성 인식 플랫폼은 개별적으로 설정된 서로 다른 기동어에 의하여 호출되며, 인공 지능 장치는 메모리 또는 외부 서버에 저장된 기동어 인식 엔진을 이용하여 기동어를 인식할 수 있다. 이를 위하여, 프로세서는 복수의 기동어 인식 엔진들 각각에 전처리된 오디오 신호 중에서 기동어 인식 구간을 전송할 수 있다. 프로세서는 전처리된 오디오 신호의 기동어 인식 구간을 복수의 기동어 인식 엔진들 각각에 전달하고, 각 기동어 인식 엔진을 통해 기동어 인식 구간에서 각 기동어 인식 엔진에 대응하는 기동어를 인식할 수 있다. 예 컨대, 제1 기동어 인식 엔진은 기동어 인식 구간에서 제1 기동어 인식 엔진에 대응하는 하나 이상의 제1 기동어 를 인식할 수 있고, 제2 기동어 인식 엔진은 기동어 인식 구간에서 제2 기동어 인식 엔진에 대응하는 하나 이상 의 제2 기동어를 인식할 수 있다. 프로세서는 메모리에 저장된 기동어 인식 엔진을 이용하는 경우에는 직접 기동어 인식 엔진을 이용하 여 기동어 인식 구간에서 기동어를 인식하고, 외부 서버에 저장된 기동어 인식 엔진을 이용하는 경우에는 통신 부를 통해 외부 서버에 기동어 인식 구간만을 전달하고 외부 서버로부터 기동어 인식 결과를 수신할 수 있 다. 프로세서는 전처리된 오디오 신호 중에서 기동어 인식 구간만을 기동어 인식 엔진들로 전달하기 때문에, 기동어 인식 엔진들이 상시로 동작하지 않고 기동어 인식 구간이 전달되는 시점에만 동작하면 충분하다. 이에 따라, 기동어 인식 엔진들에 의하여 요구되는 리소스가 감소되어 기동어 인식 엔진들의 프로세서의 점유율 을 낮출 수 있다. 그리고, 프로세서는 기동어가 인식되었는지 판단한다(S611). 프로세서는 복수의 기동어 인식 엔진들 중에서 하나의 기동어 인식 엔진이라도 기동어를 인식하였다고 판 단한 경우라면, 기동어가 인식되었다고 판단할 수 있다. 복수의 음성 인식 플랫폼이 동시에 동작하는 상황은 바람직하지 않으므로, 하나의 기동어 인식 엔진만이 기동어 를 인식하는 상황이 바람직하겠으나 그렇지 않을 수도 있다. 만약, 둘 이상의 기동어 인식 엔진들이 각자의 기 동어를 인식했다고 판단하였다면, 프로세서는 기동어를 인식한 둘 이상의 기동어 인식 엔진들 중에서 하나 의 기동어 인식 엔진을 선택하고, 선택된 기동어 인식 엔진에서만 기동어가 인식된 것으로 판단할 수 있다. 만약 둘 이상의 기동어 인식 엔진들이 각자의 기동어를 인식했다고 판단한 경우, 프로세서는 음성 인식 플 랫폼들(또는 기동어 인식 엔진들) 사이에 미리 정해진 우선순위(priority) 또는 기동어 인식 엔진에서의 기동어 인식 점수 (또는 기동어 인식 정확도)에 기초하여 하나의 인식된 기동어만을 선택하고, 선택된 기동어에 대응하 는 음성 인식 플랫폼만을 활성화시킬 수 있다. 일 예로, 제1 기동어 인식 엔진이 제2 기동어 인식 엔진보다 우 선순위가 높고, 제1 기동어 인식 엔진과 제2 기동어 인식 엔진이 모두 각각의 기동어를 인식한 경우, 프로세서 는 우선순위가 더 높은 제1 기동어 인식 엔진만이 기동어를 인식하였다고 판단하며, 제1 기동어 인식 엔진 에 대응하는 제1 음성 인식 플랫폼만을 활성화할 수 있다. 다른 예로, 제1 기동어 인식 엔진과 제2 기동어 인 식 엔진이 모두 각각의 기동어를 인식하였고, 제1 기동어 인식 엔진의 기동어 인식 점수가 0.8이고 제2 기동어 인식 엔진의 기동어 인식 점수가 0.9인 경우, 프로세서는 더 높은 기동어 인식 점수를 갖는 제2 기동어 인 식 엔진만이 기동어를 인식하였다고 판단하며, 제2 기동어 인식 엔진에 대응하는 제2 음성 인식 플랫폼만을 활 성화할 수 있다. 단계(S611)의 판단 결과, 기동어가 인식되지 않은 경우, 단계(S601)로 돌아가 프로세서는 마이크로폰을 통 해 입력 오디오 신호를 수신한다. 단계(S611)의 판단 결과, 기동어가 인식된 경우, 프로세서는 기동어가 인식된 기동어 인식 엔진에 대응하 는 음성 인식 플랫폼의 음성 인식 서비스를 제공한다(S613). 특정 음성 인식 플랫폼의 음성 인식 서비스를 제공한다는 것은 해당 음성 인식 플랫폼에 기초하여 사용자의 음 성을 인식하고, 인식한 음성에 적합한 제어를 수행하거나 응답을 제공하는 것을 의미할 수 있다. 특정 음성 인 식 플랫폼의 음성 인식 서비스를 제공하는 것은 특정 음성 인식 플랫폼 또는 특정 음성 인식 플랫폼의 음성 인 식 서비스를 활성화하는 것을 의미할 수 있다. 이를 위하여, 프로세서는 음성 인식 플랫폼의 음성 엔진들 을 이용하여 전처리된 오디오 신호에 포함된 사용자의 음성을 인식할 수 있고, 그에 기초하여 적절한 제어를 수 행하거나 응답을 제공할 수 있다. 음성 인식 플랫폼에서 사용자의 음성을 인식하는 음성 엔진들에는 전처리된 오디오 신호에 포함된 발화 음성을 텍스트로 변환하는 STT(speech-to-text) 엔진, 텍스트로 변환된 발화문의 의도를 결정하는 자연어 처리(NLP: Natural Language Processing) 엔진, 결정된 의도에 기초하여 생성된 응답을 음성으로 합성하는 음성 합성 엔진 또는 TTS(text-to-speech) 엔진 등이 포함될 수 있다. 음성 합성 엔진들은 메모리에 저장될 수도 있고, 외부 서버(예컨대, 인공 지능 서버)에 저장될 수도 있다. 프로세서는 메모리에 저장된 음성 엔진들 또는 외부 서버에 저장된 음성 엔진들을 이용하여 특정 음 성 인식 플랫폼에 대응하는 음성 인식 서비스를 제공할 수 있다. 도 6에 도시된 단계들(steps)의 순서는 하나의 예시에 불과하며, 본 개시가 이에 한정되지는 않는다. 즉, 일 실시 예에서, 도 6에 도시된 단계들 중 일부 단계의 순서가 서로 바뀌어 수행될 수도 있다. 또한, 일 실시 예 에서, 도 6에 도시된 단계들 중 일부 단계는 병렬적으로 수행될 수도 있다. 또한, 도 6에 도시된 단계들 중 일 부만 수행될 수도 있다. 도 6은 복수의 음성 인식 기동어를 인식하는 방법을 한 사이클(cycle)만을 도시한 것으로, 도 6에 도시된 복수 의 음성 인식 기동어를 인식하는 방법은 반복적으로 수행될 수 있다. 즉, 음성 인식 서비스를 제공하는 단계 (S613)를 수행한 이후에 입력 오디오 신호를 수신하는 단계(S601)가 다시 수행될 수 있다. 도 7은 본 개시의 일 실시 예에 따른 음성 서버들을 나타낸 도면이다. 도 7을 참조하면, 인공 지능 장치는 음성 인식 서비스를 제공하기 위한 하나 이상의 음성 서버와 통신할 수 있다. 음성 서버에는 기동어 인식 엔진을 이용하여 오디오 신호에 포함된 기동어를 인식하는 기동어 인식 서버, STT 엔진을 이용하여 오디오 신호에 포함된 발화 음성을 텍스트로 변환하는 STT 서버, NLP 엔 진을 이용하여 텍스트로 변환된 발화문의 의도를 결정하는 NLP 서버 및 TTS 엔진을 이용하여 결정된 의도 에 기초하여 생성된 응답을 음성으로 합성하는 음성 합성 서버 등이 포함될 수 있다. 발화문의 의도에 대 응하는 제어나 응답을 결정하는 것은 NLP 서버에서 수행될 수도 있고, 음성 합성 서버에서 수행될 수 도 있다. 이러한 음성 서버들은 각 음성 인식 플랫폼마다 구분되어 존재할 수 있고, 인공 지능 장치는 활성화된 음 성 인식 플랫폼에 대응하는 음성 서버들과 통신하여 음성 인식 서비스를 제공할 수 있다. 예컨대, 제1 음성 인 식 플랫폼이 활성화된 경우, 인공 지능 장치는 제1 음성 인식 플랫폼에 대응하는 제1 STT 서버, 제1 NLP 서버 및 제1 음성 합성 서버와 통신하여 음성 인식 서비스를 제공할 수 있다. 기동어 인식 서버, STT 서버, NLP 서버 및 음성 합성 서버는 서로 구분되는 별도의 서버로 구성될 수도 있지만, 둘 이상이 하나의 서버로서 구성될 수도 있다. 예컨대, 기동어 인식 서버, STT 서버 , NLP 서버 및 음성 합성 서버가 하나의 인공 지능 서버로 구성될 수도 있고, 이 경우, 기 동어 인식 서버, STT 서버, NLP 서버 및 음성 합성 서버는 인공 지능 서버의 개별적 인 기능을 의미할 수 있다. 도 8은 전처리된 오디오 신호와 그에 대응하는 기동어 인식 구간의 예시를 나타낸 도면이다. 도 8을 참조하면, 프로세서는 전처리된 오디오 신호를 획득하면 음성 활동 감지(VAD)를 통해 음성 구 간을 추출할 수 있다. 도 8에 도시된 예시에서, 프로세서는, 음성 활동 감지 결과, 전처리된 오디오 신호 중에서 t2~t3 구간, t4~t5 구간, t6~t7 구간 및 t8~t9 구간을 음성 구간으로 추출할 수 있다. 그리고, 프로세서는 추출된 음성 구간 및 음성 구간에 대응하는 버퍼 구간을 포함하는 기동어 인식 구간을 설정할 수 있다. 구체적으로, 프로세서는 각 추출된 음성 구간으로부터 제1 길이 (T1)만큼의 이전 구간을 제1 버퍼 구간으로 설정하고, 각 추출된 음성 구간으로부터 제2 길이(T2)만큼의 이 후 구간을 제2 버퍼 구간으로 설정하고, 추출한 음성 구간과 그에 대응하는 버퍼 구간들을 포함하는 기동 어 인식 구간을 설정할 수 있다. 도 8에서는 제1 버퍼 구간의 크기(T1)가 제2 버퍼 구간의 크기(T2)보다 작게 도시되어 있으나, 본 개시가 이에 한정되지는 않는다. 다양한 실시 예에 따라, 제1 버퍼 구간의 크기와 제2 버퍼 구간의 크기가 서로 동일할 수 있고, 제1 버퍼 구간의 크기가 제2 버퍼 구간의 크기보다 클 수도 있다. 예컨대, 제1 버퍼 구간의 크기는 4초 로 설정되고, 제2 버퍼 구간의 크기는 3초로 설정될 수 있다. 도 8에 도시된 예시에서, 프로세서는 전처리된 오디오 신호 중에서 t1~t2 구간, t3~t4 구간, t5~t6 구 간, t7~t8 구간 및 t9~t10 구간을 버퍼 구간으로 설정하고, 음성 구간과 버퍼 구간을 포함하는 t1~t10 구간을 기동어 인식 구간으로 설정할 수 있다. t3~t4 버퍼 구간은 t2~t3 음성 구간에 대한 제2 버퍼 구간이면서 동 시에 t4~t5 음성 구간에 대한 제1 버퍼 구간이기도 하다. 즉, 인접한 두 음성 구간 사이의 버퍼 구간은 제1 버 퍼 구간이면서 동시에 제2 버퍼 구간일 수 있다. 전처리된 오디오 신호 중에서 기동어 인식 구간이 아닌 구간은 유휴 구간이라 칭할 수 있고, 도 8에 도시된 예시에서는 t0~t1 구간 및 t10~t11 구간이 유휴 구간이다. 프로세서는 원형 큐(circular queue)를 이용하여 음성 구간에 대응하는 제1 버퍼 구간을 설정할 수 있다. 예컨대, 프로세서는 5초 길이의 원형 큐에 순차적으로 전처리된 오디오 신호를 채우고, 원형 큐 내에서 음 성 활동이 감지되어 음성 구간이 추출되면 추출된 음성 구간으로부터 미리 정해진 길이(예컨대, 4초)만큼 이전 의 구간을 제1 버퍼 구간으로 설정할 수 있다. 원형 큐의 크기(또는 길이)는 제1 버퍼 구간의 크기(또는 길 이)보다 크다. 프로세서는 타이머(timer)를 이용하여 음성 구간에 대응하는 제2 버퍼 구간을 설정할 수 있다. 예컨대, 프로세서는 추출된 음성 구간이 종료되면 미리 정해진 길이(예컨대, 3초)만큼의 타이머를 가동하여 타이머 시간 이내에 새로운 음성 구간이 추출되는지 판단하고, 만약 타이머 시간 이내에 새로운 음성 구간이 추출되면 새로운 음성 구간의 종료 시점으로부터 미리 정해진 길이만큼의 타이머를 다시 가동하여 타이머 시간 이내에 또 다른 새로운 음성 구간이 추출되는지 판단할 수 있다. 음성 구간의 종료 시점으로부터 미리 정해진 길이 만큼 의 타이머 시간의 구간이 제2 버퍼 구간으로 설정될 수 있다. 그리고, 프로세서는 전처리된 오디오 신호 중에서 기동어 인식 구간만을 각 기동어 인식 엔진들 로 전달할 수 있다. 도 8에 도시된 예시에서, 프로세서는 전처리된 오디오 신호 중에서 기동어 인식 구간인 t1~t10 구간만을 각 기동어 인식 엔진들에 전달할 수 있다. 종래에는 프로세서가 전처리된 오디오 신호의 전체 구간인 t0~t11 구간이 모두 각 기동어 인식 엔진들로 전달되어야 했으나, 본 개시에서는 프로세서가 일부 기동어 인식 구간인 t1~t10 구간만을 각 기동어 인식 엔진만으로 전달하기에 CPU 연산량을 효과적으로 줄일 수 있다. 즉, 본 개시는 종래 기술과 비교하였을 때, 유휴 구간에서 불필요한 리소스의 낭비를 방지할 수 있다. 도 9는 도 6에 도시된 음성 인식 서비스를 제공하는 단계(S613)의 일 예를 나타낸 흐름도이다. 도 9를 참조하면, 프로세서는 기동어 구간 이후로 음성 활동 감지(VAD) 기능을 비활성화한다(S901). 프로세서는 기동어의 인식을 위하여 전처리된 오디오 신호에서 음성 활동 감지를 통해 음성 구간을 추출하 고, 추출한 음성 구간에 기초하여 기동어 인식 구간을 설정하였다. 그러나, 기동어가 인식된 이후에는 인식된 기동어에 대응하는 음성 인식 플랫폼의 음성 엔진들을 이용하여 전처리된 오디오 신호에 포함된 발화 음성의 의 도를 파악하게 되므로, 기동어 인식 구간의 설정이 불필요하다. 이에, 프로세서는 기동어가 인식된 기동어 구간 이후로 음성 활동 감지 기능을 비활성화할 수 있다. 프로세서가 음성 활동 감지 기능을 비활성화하는 이유에는 기동어가 인식되었기 때문에 기동어 인식 구간 을 설정할 필요가 없어지는 것도 있지만, 음성 엔진들의 학습에는 음성 활동 감지에 따라 추출된 음성 구간들만 이 아닌 주변 소리가 포함되는 원본 오디오 신호의 전체 구간이 이용되기 때문에 음성 엔진들에서의 성능을 보 장하기 위함이다. 그리고, 프로세서는 기동어 구간 이후의 전처리된 오디오 신호를 인식된 기동어에 대응하는 음성 인식 플 랫폼의 음성 엔진들에 전달한다(S903). 상술하였듯, 음성 엔진들은 메모리에 저장될 수도 있고, 외부 서버(음성 서버)에 저장될 수도 있다. 프로 세서는 여러 음성 인식 플랫폼의 음성 엔진들 중에서 인식된 기동어에 대응하는 음성 인식 플랫폼의 음성 엔진들에 기동어 구간 이후의 전처리된 오디오 신호를 전송함으로써, 기동어가 인식된 특정 음성 인식 플랫폼에 기초한 음성 인식 서비스를 제공할 수 있다. 기동어 구간은 인식된 기동어의 구간을 의미할 수 있다. 전처리된 오디오 신호에서 기동어 구간 이후 구간에는 음성 인식의 대상이 되는 명령어가 포함되며, 이를 명령어 인식 구간이라 칭할 수 있다. 그리고, 프로세서는 음성 인식 결과를 획득한다(S905). 프로세서 메모리에 저장된 음성 엔진들(예컨대, STT 엔진, NLP 엔진, 음성 합성 엔진 등)에 기동어 구간 이후의 전처리된 오디오 신호를 전송하여 전처리된 오디오 신호에 포함된 발화 음성의 의도를 파악하고, 그에 대응하는 음성 인식 결과(예컨대, 제어 또는 응답)을 결정할 수 있다. 또는, 프로세서는 통신부 를 통해 외부 서버(또는 음성 서버)에 기동어 구간 이후의 전처리된 오디오 신호를 전송하고, 외부 서버 (또는 음성 서버)로부터 전송한 전처리된 오디오 신호에 포함된 발화 음성 의도에 대응하는 음성 인식 결과(예 컨대, 제어 또는 응답)을 수신할 수 있다. 그리고, 프로세서는 음성 인식 결과에 기초한 동작을 수행한다(S907). 프로세서는 음성 인식 결과에 기초하여 입력된 발화 음성에 대응하는 제어를 수행하거나, 입력된 발화 음 성에 대응하는 응답을 출력하거나, 또는 둘 다 수행할 수 있다. 그리고, 프로세서는 음성 활동 감지(VAD) 기능을 활성화한다(S909). 기동어 발화 이후로 음성 인식 기능이 수행되었으므로, 프로세서는 기동어를 인식할 수 있도록 음성 활동 감지 기능을 활성화할 수 있다. 도 9에 도시된 단계들의 순서는 하나의 예시에 불과하며, 본 개시가 이에 한정되지는 않는다. 즉, 일 실시 예 에서, 도 9에 도시된 단계들 중 일부 단계의 순서가 서로 바뀌어 수행될 수도 있다. 또한, 일 실시 예에서, 도 9에 도시된 단계들 중 일부 단계는 병렬적으로 수행될 수도 있다. 또한, 도 9에 도시된 단계들 중 일부만 수행 될 수도 있다. 도 10은 기동어 인식에 따른 음성 활동 감지 기능을 제어하는 예시를 나타낸 도면이다. 도 10을 참조하면, 프로세서는 전처리된 오디오 신호를 획득하면 기동어를 인식하기 위하여 음성 활동 감지 기능을 활성화할 수 있다. 도 10에 도시된 예시에서, 전처리된 오디오 신호에서 t2~t3 구간에 기동어(wake-up word, 1011)가 포함되 어 있다. 프로세서는 t2~t3 구간에서 기동어를 인식하고, 그에 따라 t3 시점 또는 t3 시점으로부터 일정한 간격 이후의 t4 시점에 발화 음성에 포함된 명령어(command, 1012)를 인식하기 위하여 음성 활동 감지 기능을 비활성화할 수 있다. 예컨대, t4 시점은 기동어가 종료된 t3 시점으로부터 1초 이후의 시점일 수 있다. 그리고, 프로세서는 전처리된 오디오 신호 중에서 음성 활동 감지 기능이 비활성화된 구간을 명령어 인식 구간으로 설정할 수 있다. 도 10에 도시된 예시에서, 프로세서는 음성 활동 감지 기능이 비활성화된 t4~t5 구간을 명령어 인식 구간으로 설정하고, 인식된 기동어에 대응하는 음성 인식 플랫폼의 음성 엔진들에 명령어 인식 구 간을 전달함으로써 명령어에 대한 음성 인식 결과를 획득할 수 있다. 그리고, 프로세서는 명령어의 인식이 종료되면 음성 활동 감지 기능을 활성화할 수 있다. 도 10에 도시된 예시에서, 프로세서는 명령어의 인식이 종료된 t5 시점 이후는 음성 활동 감지 기능 를 활성화할 수 있다. 이와 같이, 명령어를 인식할 때에는 음성 엔진들에 음성 활동 감지 기능을 비활성화한 상태로 전처리된 오디오 신호만을 전달함에 따라 전처리된 오디오 신호에 포함된 명령어를 보다 정확하게 인식할 수 있다. 도 11은 본 개시의 일 실시 예에 따른 복수의 음성 인식 기동어를 인식하는 방법을 나타낸 흐름도이다. 도 11을 참조하면, 프로세서는 현재 동작 모드가 음성 등록 모드인지 판단한다(S1101). 음성 등록 모드는 음성 인식 서비스를 제공함에 있어서 특정 사용자의 목소리를 등록하는 모드를 의미하며, 개 별 사용자에 대한 음성 인식 정확도를 높이거나 개별 사용자별로 상이한 음성 인식 설정 값들을 설정하기 위하 여 제공될 수 있다. 단계(S1101)의 판단 결과, 동작 모드가 음성 등록 모드인 경우, 프로세서는 음성 활동 감지 기능을 비활성 화하고(S1103), 음성 등록 기능을 제공한하고(S1105), 음성 등록 기능이 종료되면 음성 활동 감지 기능을 활성 화한다(S1107). 음성 등록 모드가 특정 사용자의 목소리를 등록하는 모드라는 점에서, 노이즈나 에코(echo)만 제거된 오디오 신 호 (또는 전처리된 오디오 신호)를 그대로 이용해서 사용자의 음성을 등록하는 것이 바람직하다. 이에, 프로세 서는 음성 활동 감지 기능을 비활성화하고 사용자의 음성을 등록하는 기능을 제공할 수 있다. 예컨대, 프 로세서는 목소리 등록 인터페이스를 제공함으로써 사용자의 음성을 등록하는 기능을 제공할 수 있다. 단계(S1101)의 판단 결과, 동작 모드가 음성 등록 모드가 아닌 경우, 프로세서는 입력 오디오 신호를 획득 하는 단계(S601)를 수행한다. 프로세서는 인공 지능 장치가 음성 등록 모드로 동작하지 않는 경우에는 기동어를 인식하기 위한 도 6에 도시된 단계들(S601 내지 S613)을 수행할 수 있다. 도 11에 도시된 단계들의 순서는 하나의 예시에 불과하며, 본 개시가 이에 한정되지는 않는다. 즉, 일 실시 예 에서, 도 11에 도시된 단계들 중 일부 단계의 순서가 서로 바뀌어 수행될 수도 있다. 또한, 일 실시 예에서, 도 11에 도시된 단계들 중 일부 단계는 병렬적으로 수행될 수도 있다. 또한, 도 11에 도시된 단계들 중 일부만 수행될 수도 있다. 도 11은 복수의 음성 인식 기동어를 인식하는 방법을 한 사이클만을 도시한 것으로, 도 11에 도시된 복수의 음 성 인식 기동어를 인식하는 방법은 반복적으로 수행될 수 있다. 즉, 음성 활동 감지 기능을 활성화하는 단계 (S1107)를 수행한 이후에 동작 모드가 음성 등록 모드인지 판단하는 단계(S1101)가 다시 수행될 수 있다. 도 12는 본 개시의 일 실시 예에 따른 목소리 등록 인터페이스를 나타낸 도면이다. 도 12를 참조하면, 목소리 등록 인터페이스는 미리 정해진 횟수만큼 기동어가 인식될 때까지 기동어를 발 화할 것을 요청하는 안내문과 인식에 성공한 기동어의 수를 나타내는 정보를 포함할 수 있다. 나 아가, 목소리 등록 인터페이스는 입력되는 소리에 따라 색상이나 모양이 변화하는 소리 시각화 이미지 를 더 포함할 수 있다. 사용자는 소리 시각화 이미지를 통해 현재 디스플레이 장치에 소리가 잘 입력되는지를 확인할 수 있다. 프로세서는 새로운 목소리를 등록하는 경우에는 도 12의 (a)와 같은 목소리 등록 인터페이스를 제공 하고, 기존에 등록된 목소리를 재등록하는 경우에는 도 12의 (b)와 같은 목소리 등록 인터페이스를 제공 할 수 있다. 비록 도 12에 도시되지는 않았으나, 도 12에 도시된 목소리 등록 인터페이스를 통해 사용자의 목소리를 성공적으로 획득한 경우, 프로세서는 목소리의 명칭 또는 호칭을 설정하는 인터페이스를 제공할 수도있다. 도 13은 음성 등록 모드에서의 음성 활동 감지 기능을 제어하는 예시를 나타낸 도면이다. 도 13을 참조하면, 프로세서는 인공 지능 장치의 동작 모드가 음성 등록 모드인 경우, 음성 등록 모 드가 활성화된 동안에는 음성 활동 감지 기능을 비활성화하고 음성 등록 기능을 제공할 수 있다. 도 13에 도시된 예시에서, t1 시점에 음성 등록 모드가 활성화되고, t2 시점에 음성 등록 모드가 비활성화되며, 프로세서는 t1~t2 구간에서는 음성 활동 감지 기능를 비활성화할 수 있다. 그리고, 프로세서는 전처리된 오디오 신호 중에서 t1~t2 구간을 이용하여 사용자의 음성(또는 목소리)을 등록할 수 있다. 도 14는 본 개시의 일 실시 예에 따른 복수의 기동어를 인식하는 방법을 나타낸 래더 다이어그램이다. 도 14를 참조하면, 인공 지능 장치는 제1 음성 인식 플랫폼 및 제2 음성 인식 플랫폼을 지원하며, 제1 음 성 인식 플랫폼에 대응하는 제1 기동어를 인식하는 제1 기동어 인식 엔진을 내장할 수 있고, 제2 음성 인식 플 랫폼에 대응하는 제2 기동어를 인식하는 제2 기동어 인식 엔진은 탑재하지 않을 수 있다. 예컨대, 제2 음성 인 식 플랫폼은 인공 지능 장치를 기준으로 외부 음성 인식 플랫폼을 의미할 수 있고, 인공 지능 장치는 API(Application Programming Interface)를 이용하여 제2 음성 인식 플랫폼의 음성 인식 서비스를 제공할 수 있다. 제1 기동어는 복수 개의 기동어로 구성될 수 있고, 마찬가지로 제2 기동어도 복수 개의 기동어로 구성될 수 있다. 제1 인공 지능 서버(400_1)는 제1 음성 인식 플랫폼의 음성 인식 서비스를 제공하는 인공 지능 서버를 의 미하며, 제1 음성 인식 플랫폼에 대한 제1 기동어 인식 엔진 또는 제1 음성 인식 엔진들 중에서 적어도 하나 이 상을 저장할 수 있다. 제2 인공 지능 서버(400_2)는 제2 음성 인식 플랫폼의 음성 인식 서비스를 제공하는 인 공 지능 서버를 의미하며, 제2 음성 인식 플랫폼에 대한 제2 기동어 인식 엔진 또는 제2 음성 인식 엔진들 중에서 적어도 하나 이상을 저장할 수 있다. 인공 지능 장치의 프로세서는 입력 오디오 신호를 수신하고(S1401), 입력 오디오 신호로부터 전처리 된 오디오 신호를 생성하고(S1403), 전처리된 오디오 신호에서 음성 활동 감지(VAD)를 통해 음성 구간을 추출하 고(S1405), 음성 구간을 기초로 기동어 인식 구간을 설정한다(S1407). 그리고, 인공 지능 장치의 프로세서는 메모리에 저장된 제1 기동어 인식 엔진에 기동어 인식 구 간을 전달하고(S1409), 제1 기동어 인식 엔진을 통해 제1 기동어를 인식한다(S1413). 그리고, 인공 지능 장치의 프로세서는 통신부를 통해 제2 기동어 인식 엔진을 저장하는 제2 인 공 지능 서버(400_2)에 기동어 인식 구간을 전달하고(S1411), 제2 인공 지능 서버(400_2)의 프로세서는 제 2 기동어 인식 엔진을 통해 제2 기동어를 인식하고(S1415), 통신부를 통해 인공 지능 장치에 기동어 인식 결과를 전달한다(S1417). 프로세서는 제2 인공 지능 서버(400_2)에서 제공하는 제2 기동어 인식 엔진에 대한 API를 이용하여 제2 인 공 지능 서버(400_2)에 기동어 인식 구간을 전달하고, 제2 기동어에 대한 기동어 인식 결과를 획득할 수 있다. 인공 지능 장치는 제1 음성 인식 플랫폼에 대응하는 제1 기동어와 제2 음성 인식 플랫폼에 대응하는 제2 기동어를 모두 인식할 수 있으므로, 제1 기동어를 인식하는 단계들(S1409 및 S1413)과 제2 기동어를 인식하는 단계들(S1411, S1415 및 S1417)은 서로 병렬적으로 수행될 수 있다. 그리고, 인공 지능 장치의 프로세서는 어떤 기동어가 인식되었는지 판단한다(S1421). 단계(S1421)의 판단 결과, 기동어가 인식되지 않은 경우, 입력 오디오 신호를 수신하는 단계(S1401)로 진행한다. 단계(S1421)의 판단 결과, 인식된 기동어가 제1 기동어인 경우, 인공 지능 장치의 프로세서는 통신부 를 통해 제1 음성 엔진들을 저장하는 제1 인공 지능 서버(400_1)에 명령어 인식 구간을 전달하고(S1423), 제1 인공 지능 서버(400_1)의 프로세서는 제1 음성 엔진들을 통해 명령어를 인식하고(S1425), 통신부(41 0)를 통해 인공 지능 장치에 음성 인식 결과를 전달한다(S1427).프로세서는 제1 인공 지능 서버(400_1)에서 제공하는 제1 음성 엔진들에 대한 API를 이용하여 제1 인공 지 능 서버(400_1)에 명령어 인식 구간을 전달하고, 음성 인식 결과를 획득할 수 있다. 단계(S1421)의 판단 결과, 인식된 기동어가 제2 기동어인 경우, 인공 지능 장치의 프로세서는 통신부 를 통해 제2 음성 엔진들을 저장하는 제2 인공 지능 서버(400_1)에 명령어 인식 구간을 전달하고(S1429), 제2 인공 지능 서버(400_2)의 프로세서는 제2 음성 엔진들을 통해 명령어를 인식하고(S1431), 통신부(41 0)를 통해 인공 지능 장치에 음성 인식 결과를 전달한다(S1433). 프로세서는 제2 인공 지능 서버(400_2)에서 제공하는 제2 음성 엔진들에 대한 API를 이용하여 제2 인공 지 능 서버(400_2)에 명령어 인식 구간을 전달하고, 음성 인식 결과를 획득할 수 있다. 그리고, 인공 지능 장치의 프로세서는 획득한 음성 인식 결과에 기초한 동작을 수행한다(S1435). 도 14에 도시된 단계들의 순서는 하나의 예시에 불과하며, 본 개시가 이에 한정되지는 않는다. 즉, 일 실시 예 에서, 도 14에 도시된 단계들 중 일부 단계의 순서가 서로 바뀌어 수행될 수도 있다. 또한, 일 실시 예에서, 도 14에 도시된 단계들 중 일부 단계는 병렬적으로 수행될 수도 있다. 또한, 도 14에 도시된 단계들 중 일부만 수행될 수도 있다. 도 14는 복수의 음성 인식 기동어를 인식하는 방법을 한 사이클만을 도시한 것으로, 도 14에 도시된 복수의 음 성 인식 기동어를 인식하는 방법은 반복적으로 수행될 수 있다. 즉, 음성 인식 결과에 기초한 동작을 수행하는 단계(S1435)를 수행한 이후에 입력 오디오 신호를 수신하는 단계(S1401)가 다시 수행될 수 있다. 도 14는 제1 음성 엔진들이 제1 인공 지능 서버(400_1)에 저장된 실시 예를 나타내었으나, 본 개시가 이에 한정 되지 않는다. 즉, 일 실시 예에서, 인공 지능 장치는 제1 기동어 인식 엔진뿐만 아니라 제1 음성 엔진들 을 저장하고, 인공 지능 장치는 제1 인공 지능 서버(400_1)를 통하지 않고 직접 제1 음성 인식 플랫폼의 음성 인식 서비스를 제공할 수 있다. 도 15는 도 6에 도시된 음성 구간을 추출하는 단계(S605)의 일 예를 나타낸 흐름도이다. 도 15를 참조하면, 인공 지능 장치의 프로세서는 전처리된 오디오 신호에서 음성 활동 감지를 통해 음성 존재 확률을 획득한다(S1501). 프로세서는 음성 활동 감지를 통해 전처리된 오디오 신호의 각 시점에서 음성이 존재하는 확률 또는 가능 성을 획득할 수 있다. 그리고, 인공 지능 장치의 프로세서는 음성 존재 확률이 제1 기준값보다 큰 구간을 음성 구간으로 결 정한다(S1503). 즉, 프로세서는 오직 음성 존재 확률만에 기초하여 전처리된 오디오 신호에서 음성 구간을 추출할 수 있다. 도 16은 도 6에 도시된 음성 구간을 추출하는 단계(S605)의 일 예를 나타낸 흐름도이다. 도 16을 참조하면, 인공 지능 장치의 프로세서는 전처리된 오디오 신호에서 음성 활동 감지를 통해 음성 존재 확률을 획득한다(S1601). 그리고, 인공 지능 장치의 프로세서는 전처리된 오디오 신호의 진폭과 그에 대응하는 음성 존재 확률 을 곱한다(S1603). 전처리된 오디오 신호는 시간별 진폭으로 표현될 수 있고, 프로세서는 각 시간별로 전처리된 오디오 신호 의 진폭과 그에 대응하는 음성 존재 확률을 곱할 수 있다. 그리고, 인공 지능 장치의 프로세서는 전처리된 오디오 신호의 진폭과 음성 존재 확률의 곱이 제2 기 준값보다 큰 구간을 음성 구간으로 결정한다(S1605). 즉, 프로세서는 음성 존재 확률뿐만 아니라 전처리된 오디오 신호를 함께 고려하여 전처리된 오디오 신호 에서 음성 구간을 추출할 수 있다.실제 실험 결과, 단순히 음성 존재 확률만에 기초하여 음성 구간을 결정한 경우(도 15의 예)보다 전처리된 오디 오 신호의 진폭과 음성 존재 확률의 곱에 기초하여 음성 구간을 결정한 경우(도 16의 예)가 기동어 인식 성능이 더 좋게 나타났다. 도 17은 전처리된 오디오 신호에서 음성 구간을 추출하는 방법을 나타낸 도면이다. 도 17을 참조하면, 프로세서는 전처리된 오디오 신호을 획득하면, 음성 활동 감지(VAD)를 통해 전처 리된 오디오 신호에 대응하는 음성 존재 확률을 획득할 수 있다. 그리고, 프로세서는 전처리된 오디오 신호의 진폭과 음성 존재 확률을 곱한 값이 미리 정해진 기준값보다 큰 구간을 음성 구간으로 결정 및 추출할 수 있다. 도 17에 도시된 예시에서, 프로세서는 전처리된 오디오 신호의 진폭과 음성 존재 확률을 곱한 값이 미리 정해진 기준값보다 큰 구간인, t1~t2 구간, t3~t4 구간, t5~t6 구간, t7~t8 구간 및 t9~t10 구간을 음성 구간으로 추출할 수 있다. 본 개시의 일 실시 예에 따르면, 전술한 방법은 프로그램이 기록된 매체에 컴퓨터가 읽을 수 있는 코드로서 구 현하는 것이 가능하다. 컴퓨터가 읽을 수 있는 매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장 되는 모든 종류의 기록장치를 포함한다. 컴퓨터가 읽을 수 있는 매체의 예로는, HDD(Hard Disk Drive), SSD(Solid State Disk), SDD(Silicon Disk Drive), ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있다."}
{"patent_id": "10-2020-0120742", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시 예에 따른 인공 지능 장치를 나타낸 블록도이다. 도 2는 본 개시의 일 실시 예에 따른 원격 제어 장치를 나타낸 블록도이다. 도 3은 본 개시의 일 실시 예에 따른 원격 제어 장치를 나타낸 도면이다. 도 4는 본 개시의 일 실시 예에서 원격 제어 장치를 통하여 인공 지능 장치와 상호 작용하는 예시를 나타낸다. 도 5는 본 개시의 일 실시 예에 따른 인공 지능 서버를 나타낸 블록도이다. 도 6은 본 개시의 일 실시 예에 따른 복수의 음성 인식 기동어를 인식하는 방법을 나타낸 흐름도이다. 도 7은 본 개시의 일 실시 예에 따른 음성 서버들을 나타낸 도면이다. 도 8은 전처리된 오디오 신호와 그에 대응하는 기동어 인식 구간의 예시를 나타낸 도면이다. 도 9는 도 6에 도시된 음성 인식 서비스를 제공하는 단계(S613)의 일 예를 나타낸 흐름도이다. 도 10은 기동어 인식에 따른 음성 활동 감지 기능을 제어하는 예시를 나타낸 도면이다. 도 11은 본 개시의 일 실시 예에 따른 복수의 음성 인식 기동어를 인식하는 방법을 나타낸 흐름도이다. 도 12는 본 개시의 일 실시 예에 따른 목소리 등록 인터페이스를 나타낸 도면이다. 도 13은 음성 등록 모드에서의 음성 활동 감지 기능을 제어하는 예시를 나타낸 도면이다. 도 14는 본 개시의 일 실시 예에 따른 복수의 기동어를 인식하는 방법을 나타낸 래더 다이어그램이다. 도 15는 도 6에 도시된 음성 구간을 추출하는 단계(S605)의 일 예를 나타낸 흐름도이다. 도 16은 도 6에 도시된 음성 구간을 추출하는 단계(S605)의 일 예를 나타낸 흐름도이다. 도 17은 전처리된 오디오 신호에서 음성 구간을 추출하는 방법을 나타낸 도면이다."}
