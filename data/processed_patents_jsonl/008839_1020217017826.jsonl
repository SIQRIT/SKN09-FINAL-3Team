{"patent_id": "10-2021-7017826", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0072837", "출원번호": "10-2021-7017826", "발명의 명칭": "카메라를 이용한 차량 환경 모델링", "출원인": "모빌아이 비젼 테크놀로지스 엘티디.", "발명자": "블루멘탈 이테이"}}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "노면을 모델링하기 위한 디바이스로서, 노면을 나타내는 시간 순서의 이미지 시퀀스를 획득하기 위한 하드웨어 센서 인터페이스 - 상기 이미지 시퀀스중 하나의 이미지는 현재 이미지임 - 와,ANN(인공 신경 네트워크)에 데이터 세트를 제공하여 장면의 3 차원 구조를 생성하고, 상기 장면의 상기 3 차원구조를 사용하여 상기 노면을 모델링하는 처리 회로부를 포함하되, 상기 데이터 세트는, 상기 현재 이미지를 포함하는 상기 이미지 시퀀스의 일부와, 센서의 모션과, 상기 현재 이미지의 대응하는 픽셀로부터 전진방향 모션(forward motion)의 방향을 나타내는 벡터까지의 거리를나타내는 값을 나타내는 엔트리를 가진 매트릭스를 포함하는,디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 처리 회로부는 특징부가 상기 노면의 환경 내에서 움직이는 물체를 나타내는지 혹은 움직이지 않는 물체를나타내는지를 결정하기 위해 상기 3 차원 구조를 사용하여 제 2 ANN을 호출하도록 구성되는 디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 3 차원 구조를 사용하여 상기 제 2 ANN을 호출하기 위해서, 상기 처리 회로부는, 상기 제 2 ANN에, 상기현재 이미지, 상기 3 차원 구조를 사용하여 와핑된(warped) 이전 이미지, 및 타겟 식별자를 제공하는디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 타겟 식별자는, 상기 이미지의 픽셀이 타겟의 중심으로부터의 거리를 나타내는 이미지인디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2021-0072837-3-제3항에 있어서, 상기 타겟 식별자는 타겟의 사이즈를 포함하는디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3항에 있어서, 상기 타겟 식별자는 타겟에 대응하는 픽셀의 마스크인디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 장면의 3 차원 구조는 감마 이미지이고, 상기 감마 이미지는 평면 위에서의 포인트의 높이를 상기 현재 이미지를 캡처하는 센서로부터의 거리로 나눈 비율인 감마 값을 갖는 픽셀을 포함하며, 상기 평면은 상기 노면을 나타내는 디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 노면을 모델링하기 위해, 상기 처리 회로부는 상기 장면의 3 차원 구조를 제 2 ANN으로부터의 출력과 비교함으로써 반사성 구역을 식별하되, 상기 제 2 ANN은 상기 이미지 시퀀스의 일부를 수용하고 제 2의 3 차원 구조를 생성하도록 트레이닝되며, 상기 제 2 ANN의 트레이닝에서는 상기 이미지 시퀀스의 일부에서 상기 제 1 ANN을 트레이닝하는 것보다 더 많은사진 측량 손실이 사용되는디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 ANN 및 상기 제 2 ANN은 2-채널 출력을 생성하도록 트레이닝된 단일 ANN으로서 구현되되, 제 1 채널은 상기 장면의 3 차원 구조이고, 제 2 채널은 트레이닝에서 더 많은 사진 측량 손실을 사용한, 상기제 2 ANN에 의해 생성된 상기 3 차원 구조인디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서, 상기 노면을 모델링하기 위해, 상기 처리 회로부는 노면 특징부의 평면으로부터의 수직 편차를 계산하는공개특허 10-2021-0072837-4-디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서, 상기 ANN은 컨볼루션 신경 네트워크(CNN)인 디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 센서의 모션 및 상기 매트릭스는 병목 계층에서 상기 CNN에 제공되는디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항에 있어서, 상기 ANN은, 미래 이미지의 모델과 미래 이미지 사이의 차이를 측정함으로써 오차를 결정하는 비지도(unsupervised) 트레이닝 기술을 이용하여 트레이닝되며, 상기 미래 이미지의 모델은 상기 미래 이미지 이전의 이미지의 감마 와핑을 통해 생성되는디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1항에 있어서, 상기 ANN은, 한 위치에 대해 예측된 감마와 그 위치에서의 센서 움직임 사이의 차이를 측정함으로써 오차를 결정하는 비지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 센서 움직임은 피치(pitch), 요(yaw), 롤(roll), 또는 상기 평면에 수직인 병진이동(translation)을 포함하는디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1항에 있어서, 상기 ANN은, 2 개의 상이한 시간의 2 개의 이미지 사이에서 중첩하는 세그먼트의 감마 차이에 의해 오차를 결정하는 비지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 제 1 이미지에 대해 추론이 수행되고, 상기 중첩하는 세그먼트는 제 2 이미지에서 상기 센서에 더 근접하는디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "공개특허 10-2021-0072837-5-제1항에 있어서, 상기 이미지 시퀀스의 일부는 상기 현재 이미지에 바로 선행하는 이미지를 포함하는디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서, 상기 이미지 시퀀스의 일부는 총 3 개의 이미지인디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제1항에 있어서, 상기 이미지 시퀀스의 일부는, 상기 현재 이미지에 선행하며 상기 이미지 시퀀스에서 하나 이상의 이미지에 의해 분리되는 이미지를 포함하는디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제1항에 있어서, 상기 매트릭스는 상기 현재 이미지와 같은 차원을 갖는 디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제1항에 있어서, 베이스라인이 적어도 0.5m인 디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제1항에 있어서, 상기 매트릭스는 상기 현재 이미지보다 낮은 해상도를 갖는 디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제1항에 있어서, 상기 매트릭스는 상기 전진방향 모션의 방향을 나타내는 상기 벡터로부터의 수평 거리만을 나타내고, 상기 전진방향 모션의 방향을 나타내는 상기 벡터로부터의 수직 거리를 나타내기 위해 제 2 매트릭스가 상기 ANN에 제공되는공개특허 10-2021-0072837-6-디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제1항에 있어서, 상기 센서의 모션은 상기 현재 이미지와 동일한 차원을 갖는 상수 값 이미지로서 제공되는디바이스."}
{"patent_id": "10-2021-7017826", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제23항에 있어서, 상기 상수 값은 상기 센서의 전진방향 모션을 상기 평면으로부터 상기 센서의 높이로 나눈 비율인디바이스."}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "카메라를 이용한 차량 환경 모델링을 위한 시스템 및 기술이 설명된다. 노면을 나타내는 시간 순서의 이미지 시 퀀스가 획득될 수 있다. 이 시퀀스의 이미지는 현재 이미지이다. 이 후, 장면의 3 차원 구조를 생성하기 위해 인공 신경 네트워크(ANN)에 데이터 세트가 제공될 수 있다. 여기서, 데이터 세트는 현재 이미지, 이미지가 획득 된 센서의 모션 및 에피폴을 포함하는 이미지 시퀀스의 일부를 포함한다. 노면은 그 후 장면의 3 차원 구조를 사용하여 모델링된다."}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 특허 출원은 35 USC §119에 따라, 2018 년 4 월 18 일에 출원된 “ParallaxNet-Learning of Geometry from Monocular Video”라는 명칭의 미국 가출원 제62/659,470호; 2018 년 4 월 26 일에 출원된 “Moving/Not Moving DNN”이라는 명칭의 미국 가출원 제62/662,965호; 2018 년 4 월 27 일에 출원된 “Road Plane with DNN ”이라는 명칭의 미국 가출원 제62/663,529호; 2018 년 11 월 19 일에 출원된 “Puddle Detection for Autonomous Vehicle Control”이라는 명칭의 미국 가출원 제62/769,236호; 및 2018 년 11 월 19 일에 출원된 “Road Contour Measurement for Autonomous Vehicles”이라는 명칭의 미국 가출원 제62/769,241호에 대한 우 선권의 이익을 주장하며, 이들의 전체 내용은 본원에 참고로 포함된다. 본원에 기술되는 실시예는 일반적으로 컴퓨터 비전 기술에 관한 것으로, 보다 구체적으로는 카메라를 이용한 차 량 환경 모델링에 관한 것이다."}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "자동차에서 종종 \"자율 주행\" 또는 \"보조 주행\" 동작으로 지칭되는 자율 주행 또는 반 자율 주행 자동차 기술은 상업용 및 소비자용 차량에서 빠른 개발 및 전개가 진행되고 있다. 이들 시스템은 센서들의 어레이를 사용하여 차량의 모션과 주변 환경을 지속적으로 관찰한다. 노면 및 경계, 다른 차량, 보행자, 물체 및 위험물, 사이니 지(signage) 및 도로 표시(road markings), 및 기타 관련 항목과 같은 차량의 주변 환경을 관찰하는 데 다양한 센서 기술이 사용될 수 있다. 하나 이상의 카메라로 구현되는 이미지 캡처 센서는 물체의 탐지 및 인식과, 표지판 및 도로 표시의 판독에 특 히 유용하다. 도로의 수직 윤곽(vertical contour), 차선 마커(lane markers) 및 경계석(curbs)과 같은 3 차 원 구조를 측정하고 물체 또는 위험물을 탐지하는 데 카메라 기반 시스템이 적용되었다. 실제 센서 시스템은 다양한 날씨 및 도로 상황에서 안정적으로 동작할 것으로 예상된다. 이러한 예상은 입력의 처리에 있어 수많은 도전을 야기하는 경향이 있다. 밤에 그림자나 조명으로 인한 입력 노이즈는 노면 탐지를 방해할 수 있다. 젖 은 도로 또는 다른 반사성 노면은 종종 노면 모델에 상반되는 겉보기 모션을 유발하기도 한다. 또한, 자율 주 행 또는 보조 주행을 가능하게 하기 위해 노면을 모델링하는 동안 위험물의 빠른(예를 들어, 실시간의) 탐지의 필요성은 이러한 노면 탐지의 어려움을 감안할 때 하드웨어에 부담을 주게 된다."}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "다양한 차량 환경 모델링 기술은 다양한 센서 구성과 함께 사용될 수 있다. 카메라(예를 들어, 가시 광선 스펙 트럼, 적외선(IR) 등)를 사용할 때 센서는 픽셀로 구성된 이미지를 생성한다. 픽셀의 다양한 측면, 예컨대, 컬 러 또는 휘도는 모델링에 사용될 수 있다. 일반적으로, 동적 환경을 모델링하기 위해 이미지들의 시퀀스가 사 용된다. 이러한 타입의 모델링은 차량이 어떻게 움직이는지, 다른 차량이 어떻게 움직이는지, 물체(예를 들어, 사람, 동물, 공 등)가 어떻게 움직이는지, 도로 내의 장애물 등과 같은 환경의 측면을 추론하기 위해 순차적 이 미지들 간의 픽셀의 움직임(movement)을 추적한다. (예를 들어, 카메라 렌즈 왜곡을 보정하기 위해) 이미지들을 정규화된 상태로 변환하고, 이미지들 간의 픽셀들 을 순서대로 정렬하고(예를 들어, 호모그래피를 통해 선행 이미지(earlier image)를 후행 이미지(later image) 와 대략적으로 매칭시키기 위해 와핑(warping)하고, 그리고 남아 있는 픽셀의 모션(예를 들어, 잔차 모션 (residual motio))을 측정하는 반복 프로세스는 환경을 모델링하는 데 사용될 수 있다. 잔차 모션( )은 다음 과 같이 계산될 수 있다: , 여기서, 항은 감마이고, 즉 평면(예를 들어, 노면) 위의 픽셀의 높이( ) 및 센서까지의 픽셀의 거리( )의 비 율이며, 는 순방향으로의 센서의 병진 이동(translation)(예를 들어, 차량이 이미지들 사이에서 얼마나 멀리 이동했는지)을 나타내며, 는 평면으로부터의 센서의 높이를 나타내며, 는 에피폴 정보(epipole information)(예를 들어, 차량이 어디로 이동하는지)를 나타내고, 는 호모그래피 기반 와핑을 적용한 후의 픽셀의 해당 이미지 좌표를 나타낸다. 잔차 모션을 계산하는 몇 가지 추가적인 세부 사항이 아래에 기술된다. 그러나, 직접 픽셀 매칭(direct pixel matching)을 사용하는 데는 몇 가지 어려움이 있다. 예를 들어, 노면에 투영될 수 있는 많은 사물들은 그림자 또는 반사성 패치(예를 들어, 퍼들)와 같은 노면을 나타내지 않는다. 이러한 노이즈를 줄이는 데 필터링 기술 이 사용될 수 있지만, 보다 나은 솔루션에는 이미지들의 시퀀스로부터 감마를 직접 계산하도록 트레이닝되는 인 공 지능(예를 들어, 머신 러닝 시스템, 인공 신경 네트워크(artificial neural network)(ANN), 심층 ANN (DNN), 컨볼루션 ANN (CNN) 등)이 포함된다. 이는 노면 이미징에서 일반적인 노이즈 문제에 대한 강력한 솔루 션을 필요로 한다. 또한, 그러한 시스템은 또한 센서 모션 또는 에피폴 정보를 수용하여 자신의 감마 결과를 더욱 향상시킬 수 있다. 감마로부터, 노면 위의 픽셀의 높이 및 해당 픽셀까지의 거리가 결정될 수 있다. 그 러한 노면 모델링은, 예를 들어, 포트홀(potholes)을 회피하거나 스피드 범프를 위한 서스펜션을 조정하는 데 유용할 수 있다. (예를 들어, ANN에 의해) 센서 데이터로부터 직접 감마(gamma)를 결정하는 것은 픽셀 기술 중 에서 2 차원(2D) 광학 흐름을 사용하여 잔차 흐름을 확인하거나 ANN을 사용하여 평면 위의 높이와 센서까지의 거리를 결정하는 것과 같은 다른 기술보다 우수할 수 있는데, 이는 에피폴 제약 조건(epipolar constraints)을 강제하기 때문이다. 또한, 하나의 감마는 해당 포인트의 모든 이미지를 정렬(예를 들어, 와핑)하는 데 사용될 수 있다. ANN은 해당 포인트의 심도 또는 높이를 직접 결정하도록 트레이닝될 수 있지만, 감마는 몇 가지 이점 을 제공한다. 예를 들어, 감마 계산은 심도보다 더욱 안정적인데, 그 이유는 평면으로부터의 높이가 크게 변경 되더라도 카메라로부터의 심도의 변경은 상대적으로 적을 수 있기 때문이다. 또한, 및 참조 평면을 감안할 때, 심도( ) 및 잔차 흐름을 계산할 수 있지만, 이는 ANN이 동일한 결과를 위해 보다 많은 데이터를 처리하기 때문에 복잡성을 증가시킨다. 이것은 또한 평면 모델로 이미지를 미리 와핑하고 에고 모션(ego-motion)(EM)(예 를 들어, 에피폴( 및 )과 같은 센서 또는 차량의 모션)을 입력으로서 제공하는 이유이다. 일 예에서, 네트워크는 감마 대신에 유사한 기술을 사용하여 또는 를 계산하도록 트레이닝될 수 있다. 이 예에서, 호모그래피 평면 입력 파라미터가 ANN에 제공될 수 있다. 예를 들어, 평면은 수평 라인(예를 들어, 평 면의 소실 라인) 및 평면까지의 거리로 정의될 수 있다. 라인은 한 쌍의 거리 이미지로 제공될 수 있고, 평면 까지의 거리는 일정한 이미지로 제공될 수 있다. 이것은 epipole 및 가 위의 입력으로 제공되는 방식과 유사하다. 일 예에서, 입력 이미지들은 (예를 들어, 무한대의 평면을 사용하는 호모그래피를 사용하여) 회전만을 고려하고 를 계산하도록 정렬된다. (예를 들어, 서스펜션 제어를 위해) 예를 들어, 전체 이미지에 대한 감마를 계산하고 이후 특정 경로에 따른 감 마만을 사용하는 대신, ANN은 특정 경로를 따라서만 감마를 생성하도록 트레이닝될 수 있다. 이는, 예를 들어, 출력이 서스펜션 제어와 같이, 차량 타이어에 적용 가능한 것에 대해서만 사용되는 경우, 계산상 보다 효율적일 수 있는데, 그 이유는 디컨볼루션 동작(deconvolutional operations)은 계산상 비용이 고가일 수 있기 때문이다. (예를 들어, 해당 경로에 대해서만 감마를 생성하는) 경로 구별은 여러 방식으로 구현될 수 있다. 예를 들어, 경로는 ANN의 추론 단계에서 입력으로서 제공될 수 있으며, ANN은 그 경로에 따른 값만을 출력하도 록 트레이닝될 수 있다. 일 예에서, 전체 ANN은 전술한 바와 같이 감마를 생성하도록 트레이닝될 수 있다. 추 론 동안, 경로가 제공될 때, 해당 경로에 대한 확장 단계에서 어떠한 컨볼루션(디컨볼루션)이 필요한지에 대한 결정이 이루어지고 그것만이 적용된다. 예를 들어, 전체 출력 행에 대한 감마 값을 결정하려면 전체 행을 따라 컨볼루션이 필요하다. 그러나, 그 출력 행의 세그먼트만의 경우, 디컨볼루션은 그 세그먼트에 대응하는 특정 범위에서만 수행될 필요가 있다. 추가로, 상이하게 트레이닝된 유사한 구조의 ANN도 또한 물체를 움직이는 것 또는 움직이지 않는 것으로 분류할 수 있다. 움직임/비 움직임 분류는, 예를 들어, 사고 회피 액션을 보다 잘 선택하도록 호스트 차량의 능력을 향상시키는 데 사용될 수 있다. 또한, 입력 이미지들은 피처들의 잔차 모션을 식별하고 그 결과를 결정하는 데 직접적으로 사용된다. 추가적인 세부 사항 및 예는 아래에 기술된다. 도 1은 다양한 물체가 존재하는 차량 탑재 카메라의 예시적인 시야를 도시한 도면이다. 도시된 바와 같이, 시야는 노면을 포함하며, 이 노면은 함몰부(104A)(예를 들어, 포트홀(potholes), 그레이 트(grates), 함몰부(depressions) 등) 또는 돌출부(104B)(예를 들어, 스피드 범프(speed bumps), 경계석 (curbs), 잔해물(debris) 등)와 같은 하나 이상의 표면 특징부를 가질 수 있다. 시야는 또한 그림자 , 반사성 표면(예를 들어, 퍼들(puddle), 빙판(ice) 등), 보행자 또는 다른 차량을 포함 할 수 있다. 표면 특징부의 모델링은 차량이 이들 특징부를 회피하게 하거나, 운전자에게 경고하거나, 또 는 차량 자체를 조정하여 이를 잘 처리하도록(예를 들어, 차량 서스펜션을 조정하여 포트홀(104A)을 가로 지르 게) 할 수 있다. 움직이거나 또는 움직일 가능성이 있는 보행자 또는 차량을 이해하고 모델링하는 것은 유사하게 차량 제어 변경 또는 운전자 경고를 통해 그들과의 충돌을 회피하게 하거나, 또는 심지어는 그들 과의 바람직하지 않은 상호 작용(예를 들어, 퍼들을 관통하는 것에 의해 보행자에게 스플래시하는 것)을, 예를 들어, 주행 속도를 감속시키거나, 주행 경로를 조정하거나, 정지하는 등에 의해, 회피하거나 감소 시키게 할 수 있다. 이들 도로 모델링의 요소는 모두 일부의 도전을 유발할 수 있으며, 이는 본원에 기술되는 디바이스 및 기술에 의해 해결된다. 예를 들어, 그림자는 노면 포인트 추적에 대한 노이즈가 된다. 퍼들로부터의 반사는 하 부 노면을 모호하게 하여 포인트 추적을 손상시킬 뿐만 아니라, 실제로는 다른 곳의 픽셀 모션과는 종종 상반되 는 이미지들 간의 픽셀 모션을 나타내기도 한다. 도 2는 일 실시예에 따른 카메라를 이용한 차량 환경 모델링 시스템의 일 예의 블록도이다. 시스템 은 차량에 부착된다. 일 예에서, 시스템은 카메라 또는 다른 센서에 통합된다. 일 예에 서, 시스템은 카메라 또는 다른 센서(예를 들어, 차량의 인포테인먼트 시스템의 일부)와는 분리 된다. 여기서, 카메라는 일 예로 앞 유리(windshield)에 탑재된 전방 주시 카메라로서 도시되어 있다. 그러나, 본원에 기술된 기술은 차량의 내부 또는 외부에 탑재된 후방 또는 측방 카메라에도 동일하게 적용된다. 하나의 그러한 예는 시야가 전방 및 약간의 측방으로 향하게 지붕의 코너 상에 외부 탑재된 카메라이다. 시스템은 카메라로부터 획득된 이미지를 통해 차량 환경 모델링을 수행하기 위한 처리 회로부를 포함 한다. 차량 환경 모델링은 노면, 장애물(obstacles), 방해물(obstructions) 및 이동체(예를 들어, 다른 차량, 보행자, 동물 등)를 모델링하는 것을 포함할 수 있다. 이들 모델은 차량의 동작 파라미터를 조정하 기 위해 시스템에 의해 직접 사용될 수 있거나 또는 다른 관리 시스템을 통해 사용될 수 있다. 모델링을 수행하기 위해, 시스템은 노면을 나타내는 시간 순서의 이미지 시퀀스를 획득하도록 구성된다. 이미 지 시퀀스 중 하나는 현재 이미지(예를 들어, 카메라에 의해 마지막으로 촬영된 이미지)이다. 시스템은 인공 신경 네트워크(ANN)에 데이터 세트를 제공하여 감마 이미지(gamma image)를 생성하도록 구 성된다. 여기서, 감마 이미지의 픽셀들은 포인트들에 대한 감마 값들이다. 다른 곳에서 언급되는 바와 같이,감마 값은 현재 이미지를 캡처하는 센서로부터의 거리에 의한 평면 위의 포인트의 높이의 비율이다. 또한, 여 기서, 평면은 노면을 나타낸다. \"감마 이미지\"가 아래에서 사용되지만, 장면에서 감마를 표현하기 위해 다른 데이터 포맷이 사용될 수 있다. 따라서, 감마는 래스터 포맷이 아닐 수 있지만, 감마 값이 센서 데이터를 통해 표면과 상관될 수 있게 하는 임 의의 형태(예를 들어, 포인트에 대한 값의 감마 맵)일 수 있다. 집합적으로, 이러한 다양한 데이터 구조는 감 마 모델이라고 지칭될 수 있다. 일 예에서, 데이터 세트는 이미지 시퀀스의 일부를 포함한다. 여기서, 이미지 시퀀스의 일부는 현재 이미지를 포함한다. 데이터 세트는 또한 센서의 모션(예를 들어, 센서 움직임 정보) 및 에피폴(예를 들어, 에피폴 정보)을 포함한다. 일 예에서, 이미지 시퀀스의 일부는 현재 이미지에 바로 선행하는 이미지를 포함한다. 일 예에서, 이미지 시퀀스의 일부는 총 3 개의 이미지이다. 일 예에서, 이 시퀀스는 임의의 개의 이미지, 여기 서 은 1보다 큰 정수(즉, )이다. 일 예에서, 이 시퀀스의 이미지는 연속적으로 캡처된 이미지 일 수 있다. 일 예에서, 오리지널 프레임 시퀀스로부터의 일부 프레임은 데이터 세트에서 사용되는 이미지 시 퀀스를 생성하는 과정에서 생략될 수 있다. 일 예에서, 에피폴은 현재 이미지와 동일한 차원(비록 아마도 더 크거나 더 낮은 해상도일 수 있음)을 갖는 그 래디언트 이미지로서 제공된다. 여기서, 그래디언트 이미지의 픽셀의 값은 현재 이미지의 픽셀의 에피폴로부터 의 거리를 나타낸다. 일 예에서, 상기 그래디언트 이미지는 에피폴로부터의 수평(예를 들어, x 축) 거리만을 나타내고, 제 2 그래디언트 이미지는 에피폴로부터의 수직(예를 들어, y 축) 거리를 나타내기 위해 ANN에 제공 된다. 일 예에서, 센서의 모션은 현재 이미지와 동일한 차원을 갖는 상수 값의 이미지로서 제공된다. 일 예에서, 상수 값은 평면으로부터 센서의 높이에 의한 센서의 전진방향 모션(forward motion)(예 를 들어, z 축)의 비율이다. 일 예에서, ANN은 컨볼루션 신경 네트워크(convolutional neural network)(CNN)이다. 일 예에서, 센서의 모션 및 에피폴은 병목 계층에서의 CNN에 제공된다(예를 들어, 도 5와 관련한 아래의 설명 참조). 일 예에서, ANN은 미래 이미지의 모델과 실제 미래 이미지 간의 차이를 측정함으로써 오차를 결정하는 비 지도 트레이닝 기술(unsupervised training technique)을 이용하여 트레이닝된다. 여기서, 미래 이미지의 모델은 미 래 이미지 이전의 이미지의 감마 와핑(gamma warping)을 통해 생성된다. 따라서, 이 예에서, 추론된 감마 값은 미래 이미지가 어떻게 보일지를 예측하는 데 사용된다. 미래 이미지와 비교할 때, 모델로부터의 편차는 ANN을 수정하는 데 사용된다. 일 예에서, ANN은 위치에 대한 예측된 감마와 그 위치에서의 센서 움직임 간의 차이를 측정함으로써 오차 를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝된다. 따라서, 감마가 예측되고 센서 또는 차량 의 에고 모션이 사용되어 감마 추론이 정확한지(또는 감마 추론이 얼마나 잘못되었는지)가 결정된다. 이 예에서, ANN이 노면에서의 딥(dip)을 예측하고, 그러한 딥이 나중에 차량에 의해 탐지되지 않으면, 트레이 닝은 그 딥을 예측한 추론을 수정한다. 일 예에서, 센서 움직임은 평면에 수직인 피치, 요, 롤 또는 병진 중 하나 이상을 포함할 수 있다. 일 예에서, ANN은 2 개의 상이한 시간에서의 2 개의 이미지 간의 중첩 세그먼트의 감마 차이에 의해 오차를 결 정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 여기서 제 1 이미지에 대해 추론이 수행되고, 제 2 이 미지에서 중첩 세그먼트가 센서에 더 근접하게 된다. 따라서, 트레이닝시, 나중에 차량에 의해 횡단 되는 노면의 뷰(view)를 갖는 이미지는 이전 이미지가 된다. 중첩 세그먼트의 감마 값은 ANN에 의해 추론 되고, 미래 이미지에서 동일한 세그먼트의 감마 값을 계산함으로써 체킹된다. 센서가 특징부(예를 들어, 미래의 중첩 세그먼트)에 더 근접하게 될 때, 시스템의 감마 추정치는 아마도 보다 더 나아지게 되고, 손실 함 수에서 ANN을 트레이닝하는 데 사용될 수 있다. 따라서, 현재의 삼중 이미지(current triple of images)로부터 추론된 감마 맵은 현재의 감마 맵을 향해 와핑된 미래의 삼중 이미지로부터 추론된 감마 맵과 비교된다. 최근 접 노면 포인트에 대한 차이 또는 최근접 노면 포인트까지의 거리와 같은 두 감마 맵 간의 비교 값은 ANN을 트 레이닝할 때 손실의 일부로서 사용된다. 시스템은 감마 이미지를 사용하여 노면을 모델링하도록 구성된다. 일 예에서, 노면을 모델링하는 것 은 노면 특징부의 평면으로부터의 수직 편차를 계산하는 것을 포함한다. 일 예에서, 노면을 모델링하는 것은이미지 시퀀스에서 특징부의 잔차 모션(residual motion)을 계산하는 것을 포함한다. 여기서, 특징부의 잔차 모션은 감마 값, 센서의 모션 및 에피폴을 곱한 것이 된다. 일 예에서, 노면을 모델링하는 것은 감마 값을 사용하여 이전 이미지를 현재 이미지로 와핑하는 것을 포함한다. 감마 기반 와핑은, 감마가 센서의 거리로부터의 거리에 기반한 이미지와 노면 위의 높이에 기반한 이 미지 내의 특징부들의 픽셀들의 순간적이거나 복잡한 컬러 변동의 매칭을 때때로 시도하는 대신에 그러한 이미 지들 간의 특징부가 매칭될 수 있게 하기 때문에, 특히 정확하다. 일 예에서, 노면을 모델링하는 것은 감마 와핑된 이미지로부터 반사성 구역을 식별하는 것을 포함한다. 여기서, 감마 와프(gamma warp)의 정확도는 퍼들과 같은 반사성 구역을 식별할 수 있게 하는데, 그 이유는 반사 물이 이미지들 내의 다른 물체와는 다르게 동작하는 시각적 정보를 생성하기 때문이다. 예를 들어, 차량이 접 근함에 따라 폴(pole)의 상단은 폴의 하단보다 센서를 향해 더 빠르게 이동하는 것처럼 보일 수 있지만, 그 폴의 많은 반사물은 반대로 나타날 것이다. 따라서, 트레이닝된 ANN은 픽셀을 비 반사성 움직임과 매칭시키 고, 반사성 구역은 무시할 것인데, 그 이유는 반사성 구역 내부의 픽셀 모션이 다른 픽셀의 모션에 맞지 않기 때문이다. 이러한 거동은 반사성 표면에 걸쳐 평탄한 감마(flat gamma)를 발생시킨다. 감마 와핑된 이미지에 서 평탄한 감마의 인접한 구역은 반사성 구역으로 식별될 수 있다. 일 예에서, 노면을 모델링하는 것은 감마 값을 사용하여 와핑 후의 잔차 모션의 인접한 구역에 의해 반사성 구 역을 식별하는 것을 포함한다. 감마 와프를 사용한 후, 나머지 잔차 모션은 이러한 독특한 거동을 보이는 구역 으로 제한될 것이다. 일 예에서, 그러한 잔차 모션 구역은 또한 다른 차량, 보행자, 움직이는 잔해물 등에 의 한 움직임을 결정하는 데 사용될 수 있다. 예를 들어, 추가적인 ANN은 트레이닝시 제 1 ANN의 손실 함수에서 최초로 사용된 기하학적 제약 조건에 대한 그 추가적인 ANN의 손실 함수에서의 사진 측량 제약 조건을 사용하여 트레이닝될 수 있다. 전술한 바와 같이, 제 1 ANN은 일반적으로 최초의 기하학적 제약 조건을 사용한 트레이닝 후 반사성 표면에서의 모션은 무시할 것이다. 그러나, 추가적인 ANN은, 사진 측량 제약 조건이 반사성 표면에서의 모션에 대한 해명을 시도하도록 트레이닝 동안 추가적인 ANN을 조정할 것이므로, 반사성 표면에서의 모션을 무시하지는 않을 것이다. 따라서, 일 예에서, 제 1 ANN 및 제 2 ANN으로부터 생성된 감마를 비교하게 되면, 생성된 2 개의 감마 맵이 (예를 들어, 임계치를 초과하여) 일치하지 않는 반사성 표면들이 노출될 수 있다. 일 예에서, 시스템은 특징부들의 잔차 모션에 대해 제 2 ANN을 호출하여 특징부들이 노면의 환경 내 에서 움직이는 또는 움직이지 않는 물체를 나타내는지를 결정하도록 추가로 구성된다. 일 예에서, 제 2 ANN에 는 현재 이미지, 적어도 하나의 이전 이미지 및 타겟 식별자가 제공된다. 타겟 식별자는 차량 식별 시스템과 같은 다른 시스템에 의해 제공될 수 있다. 일 예에서, 타겟 식별자는, 이미지의 픽셀들이 에피폴 정보에 대해 전술한 그래디언트 이미지들과 유사하게, 이미지의 픽셀들이 타겟의 중심으로부터의 소정의 거리를 나타내는 하 나 이상의 이미지이다. 일 예에서, 타겟 식별자는 타겟의 사이즈를 포함한다. 일 예에서, 타겟의 사이즈는 상 수 값 이미지(예를 들어, 전술한 센서 모션 정보 이미지와 유사)이다. 일 예에서, 타겟 식별자는 타겟에 대응 하는 픽셀의 마스크이다. 그러한 제 2 ANN의 일 예가 도 11 및 도 12와 관련하여 아래에서 기술된다. 도 3은 일 실시예에 따른 현재 이미지 및 이전 이미지를 도시한다. 2 개의 라인(306 및 308)은 현재 이미지에서 타이어(tires)의 하단 및 스피드 범프(speed bump)의 상단에 배치된다. 라인이 이전 이 미지에서 타이어와 어떻게 정렬되는지에 주목해야 한다. 이 라인에서의 양방향 화살표는 경계석의 정지단 (stationary end)에 대한 라인의 움직임을 나타낸다. 유사하게, 라인은 스피드 범프의 상단이 이전 이미 지와 현재 이미지 사이에서 움직였음을 나타낸다. 이미지가 이미지로 와핑될 경우, 이미 지들의 정지 특징부들은 매칭되지만 차량의 하단은 움직일 것이다. 도 4는 일 실시예에 따른 노면의 감마 모델을 생성하는 신경 네트워크의 일 예를 도시한다. 도 5 내 지 도 10은 412와 같은 신경 네트워크의 일부 추가적인 세부 사항 및 예를 도시한다. 그러나, 개괄적으로, 각 각의 픽셀에 대한 잔차 모션은 감마, 센서(예를 들어, 차량) 모션, 및 에피폴 정보의 세 부분으로 다음과 같이 구성된다:"}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "에피폴 정보는 호모그래피( ) 및 에피폴( ) 이후의 이미지 좌표에 따라 달라진다. 이것은 센서의 에고 모 션(ego-motion)(EM)을 감안하여 각각의 픽셀마다 계산될 수 있다. 센서 움직임 정보는 평면( )으로부터의 전진방향 모션( ) 및 센서 높이에 따라 달라진다. 이것은 전체 이미지에 대해 고정되어 있다. 감마는 평면 위의 포인트의 높이( ) 및 센서로부터의 포인트까지의 거리( )를 통해 각각의 픽셀에서의 장면의 구조를 기술한다. 따라서, 센서 움직임 정보 및 에피폴 정보를 감안할 때, 신경 네트워크은 감마 모델 을 결정하고, 각각의 포인트에 대한 잔차 모션은 하나의 이미지가 다른 이미지로 와핑 가능하게 계산될 수 있다. 정확한 감마 모델을 감안할 때, 이미지 와핑은 매우 정확하며, 종종 각각의 픽셀의 거리 및 높이 때문에 이미지가 정적 장면인 것처럼 행동한다. 고전적인 기술은 먼저 잔차 흐름을 계산했고, 그후 에피폴 정보와 센 서 움직임 정보를 제거하여 감마를 계산했다. 감마로부터 포인트의 높이와 거리는 하나 이상의 트랙(예를 들어, 타이어 경로)을 따라 계산되었다. 그러나, 전술한 바와 같이, 노면 이미지에서의 노이즈 가변도는 때때 로 직접적인 잔차 모션 탐지에 문제를 유발하기도 했다. 이미지로부터 직접 감마를 계산하도록 신경 네트워크를 트레이닝하는 것은 이미지에서 발견된 노이즈에 대 한 강력한 카운터(counter)를 제공한다. 따라서, 현재 이미지, 호모그래피 및 에고 모션을 사용하여 와핑된 하나 이상의 이전 이미지 및 에피폴(예를 들어, 평면) 파라미터(이미지(406 및 408))가 입력으로 제공되면, 신경 네트워크는 감마 값의 이미지를 출력으로 생성한다. 도시된 바와 같이, 감마 모델에서 음 영이 밝을수록 감마 값은 낮아진다. 또한, 차량은 신경 네트워크를 트레이닝하기 위해 손실 계산에서 생 략된다. 이것은 트레이닝시 차량의 모션이 근처의 감마 값에 영향을 미치지 않도록 하기 위해 수행되지만, 차 량은 일반적으로 추론 동안 마스킹되지 않을 것이다. 일 예에서, 차량 또는 다른 움직이는 물체는 트레이닝시 신경 네트워크 손실 함수로부터 마스킹되지 않는다. 예시된 바와 같이, 에피폴 정보 및 센서 움직임 정보는 이미지(예를 들어, 값들의 래스터(raster of values))로 서 제공된다. 센서 움직임 정보 이미지는 상수 값 이미지이다(예를 들어, 모든 픽셀은 동일한 값을 갖는 다). 에피폴 정보는 수평(예를 들어, x) 및 수직(예를 들어, y) 방향으로 에피폴까지의 거리의 픽셀 값을 각각 갖는 2 개의 이미지로 표현된다. 컨볼루션 신경 네트워크(convolutional neural network)(CNN)를 사용할 때 에피폴 정보를 두 값이 아닌 그래디언트 이미지로 제공하면 도움이 된다. CNN에서, 동일한 필터 뱅 크가 전체 이미지에 걸쳐 실행되고, 각각의 이미지 영역은 에피폴과 관련하여 어디에 있는지를 전달받아야 한다. 그래디언트 이미지(406 및 406)를 사용함으로써, 필터는 각각의 컨볼루션에 대한 에피폴 정보를 갖는다. 도 5는 ML 기반 윤곽 엔진의 예시적인 DNN을 도시한 도면이다. 예를 들어, 도시된 바와 같이, DNN은 컨볼루션(convolution), 활성화(activation), 정규화(normalization) 및 풀링(pooling) 계층들을 포함할 수 있 는 다양한 동작 계층을 갖는 컨볼루션 네트워크 부분을 포함한다. 내부 제품 계층과 같은 다른 동작 계층 이 추가로 포함될 수 있다. 일 예에서, DNN은 디컨볼루션(예를 들어, 전치 컨볼루션), 활성화, 정규화 및 언풀링(un-pooling) 계층들을 포함하는 디컨볼루션 부분을 추가로 포함한다. 일 예에서, 전처리된 이미지 세트는 컨볼루션 네트워크 부분에 입력으로서 제공된다. 각각의 계층은 특징부 맵을 생성하고, 이는 다시 전진방향 전파 경로에 따른 추가 처리를 위해 후속 계층으로 전 달된다. 도시된 바와 같이, 컨볼루션 네트워크 부분의 동작부는 컨볼루션 전진방향 전파 경로(508A)를 따 라 특징부 맵의 채널 수(차원)를 증가시키면서, 특징부 맵의 해상도를 점진적으로 감소시키도록 동작한다. 디 컨볼루션 네트워크 부분의 동작부는 디컨볼루션 전진방향 전파 경로(508B)를 따라 특징부 맵의 차원을 감 소시키면서, 특징부 맵의 해상도를 점진적으로 증가시키도록 동작한다. 일 예에서, 전진방향 전파 경로에 더하여, 하나 이상의 바이패스 경로가 제공되어, 선행 계층과 후행 계층 사이에 위치된 하나 이상의 중개 계층을 건너 뛰면서 선행 계층으로부터 후행 계층으로 특징부 맵을 전달 하는 것을 가능하게 할 수 있다. 일 예로서, 바이패스 경로는 컨볼루션 네트워크 부분의 계층과 유 사한 차원의 디컨볼루션 네트워크 부분의 계층 사이에서 특징부 맵을 전달할 수 있다. \"병목\" 네트워크 부분은 컨볼루션 네트워크 부분과 디컨볼루션 네트워크 부분 사이에 위치된다. 일 예에서, 병목 네트워크 부분은 다른 계층들에 비해 상대적으로 낮은 해상도 및 높은 차원을 갖는 하나 이상의 계층을 갖는다. 일 예에서, 병목 부분은 이미지 포맷된 모션 표시 및 이미지 포맷된 에피폴 위치 데이터를 수용하도록 구성된 입력부를 포함한다. 일 예에서, DNN은 전처리된 이미지의 현재 (가장 최근) 이미지에 대응하는 감마 값의 픽셀 단위 매핑 으로서 도로 구조를 생성하도록 트레이닝된다. DNN의 출력으로서의 도로 구조는 전처리된 이미 지와 동일하거나 상이한 해상도일 수 있다. 예를 들어, 도로 구조의 해상도는 계수 또는 0.25, 0.5, 1, 1.5, 2 또는 다른 스케일링 계수에 의해 스케일링될 수 있으며, 이는 정수 또는 정수가 아닌 값일 수 있다. 다른 예에서, 도로 구조는 전처리된 이미지의 현재 이미지의 일부에 대응할 수 있다. 예를 들어, 도 로 구조는 노면을 나타내지 않는 그의 일부를 생략한 잘려진 시야(field of view)의 이미지(도 1)에 대응할 수 있다. 특히, 도로 구조의 픽셀에서의 감마 값은 무차원(dimensionless) 값이다. 일 예에서, DNN은 자신의 출력으로서 수평 위의 포인트들에 대해 와 같은 다른 무차원 값들의 매핑을 생성한다. 감마 값을 알면, 거리 ( ) 및 노면의 높이( )는 관계( )를 사용하여 복구될 수 있으며, 여기서, 는 전치된 이 고, ( , )는 이미지 좌표이며, 는 초점 거리이다. DNN 트레이닝 엔진은 트레이닝 데이터 세트에 기반하 여 DNN을 트레이닝하여 도로 구조의 정확한 결정을 생성하도록 구성된다. 도 9는 DNN 트레이닝 시스 템을 보다 상세히 도시한 도면이다. 도시된 바와 같이, DNN 트레이닝 시스템은 DNN과 동일하거 나 유사한 아키텍처를 갖는 DNN 및 멀티-모드 손실 함수 적용 엔진을 포함한다. 도 6은 일 실시예에 따른 DNN의 예시적인 아키텍처를 상세히 나타내는 표이다. 도시된 바와 같이, 각각의 계층 은 그의 동작 타입, 연결(Input0, Input1 및 Output0으로 표시됨), 출력 채널의 수, 및 컨볼루션/디컨볼루션 아 키텍처(커널 폭 및 스텝을 포함함) 뿐만 아니라, 활성화 함수 및 정규화 타입의 관점에서 기술되고 있다. 특히, Input/1 열에 표시된 제 2 입력 및 식별된 제 2 입력 소스를 갖는 계층은 바이패스 연결을 갖는다. 도 6의 DNN의 계층 1로의 입력은 Input/0 열에서 \"images\"로 표시된 전처리된 이미지 세트를 포함한다. 이미지 포맷된 에피폴 표시 및 이미지 포맷된 모션 표시는 Input/1 열에서 \"epipole/motion\"으로 표시된 바와 같은 계 층 8에 입력된다. 도 7 및 도 8은 일 실시예에 따른 DNN의 보다 복잡한 예시적인 아키텍처를 상세히 나타내는 표이다. Input/1 열에 \"image\"로 표시된 바와 같은 계층 1의 DNN에는 이미지가 입력된다. 이미지 포맷된 에피폴 표시 및 이미지 포맷된 모션 표시는 Input/1 열에서 \"epipole/motion\"으로 표시된 바와 같은 계층 9에 입력된다. 일부 계층(계 층 44 및 49)은 Input/2 열로 표시되는, 바이패스 연결을 위한 제 3 입력을 갖는다. 또한, 도 7 및 도 8의 예 시적인 DNN의 특정 계층은 계층 22, 28, 34, 42, 47 및 52와 같은 사이즈 조정 동작을 수행한다. 특히, 계층 52는 특징부 맵의 사이즈를 전처리된 이미지와 동일한 사이즈로 조정한다. 도 9는 일 실시예에 따른 DNN 트레이닝 시스템의 일 예를 도시한다. 여기서, 멀티-모드 손실 함수 적용 엔진 은 DNN 에 대한 입력으로서 트레이닝 데이터를 공급하도록 구성된다. 트레이닝 데이터는 하나 이상의 차량 탑재 카메라에 의해 캡처된 다양한 시퀀스들의 이미지 프레임들을 포함할 수 있다. 이미지 프레임은, 예를 들어, 다양한 조명 및 날씨 상황 하에서, 다양한 지리적 현장의 다양한 도로에 캡처된 비디오 영상(video footage)을 포함할 수 있다. 트레이닝 데이터는 트레이닝 데이터의 제각기의 부분에 대응하는 이미지 포맷된 모션 표시 및 이미지 포맷된 에피폴 표시를 동반할 수 있다. 이미지 포맷된 모션 표시 및 이미지 포맷된 에피폴 표시는 DNN 의 구조 및 동작 배열과 매칭하도록 트레이닝 데이터의 이미지 프레임을 위한 입력 계층과는 다른 입력 계층에 공급될 수 있다. 입력은 전진방향 전파 경로를 따라 DNN을 통해 진행되 어 DNN의 출력으로서 도로 구조를 생성한다. DNN은 초기에 계산 파라미터(예를 들어, 가중치, 바이어스 등)의 랜덤화된 값으로 구성될 수 있다. 트레 이닝 과정은 DNN의 출력인 도로 구조를 최적화하기 위해 계산 파라미터의 값을 조정하는 작용을 한다. 멀티-모드 손실 함수 적용 엔진은 파라미터 최적화를 수행하도록 구성된다. 일 예에서, DNN 의 출력의 정확성을 결정하는 데 다수의 상이한 손실 함수가 사용된다. 멀티-모드 손실 함수 적용 엔진은 DNN의 다양한 계층들에 대한 계산 파라미터 조정치를 생성하며, 이 조정치는 역 전파 경로를 따라 역 전파를 사용하여 도입된다. 일 예에서, DNN의 다양한 계층들에 대한 계산 파라미터 조정치는 DNN의 트레이닝 결과를 정의하 는 계산 파라미터 데이터 구조에 수집되고 저장된다. 일 예에서, 계산 파라미터 데이터 구조는 (예 를 들어, DNN 트레이닝 시스템의 출력의 일부로서) 수직 윤곽 검출 엔진으로 전달되고, 여기서 ML 기반 윤곽 엔 진을 구성하기 위한 계산 파라미터로서 저장된다. 일 예에서, 추론 엔진의 트레이닝은 현재의 삼중 항과 미래 의 삼중 항 모두에서 실행되어 각각 output_curr 및 output_future를 생성한다. 기하학적 손실(geometric loss)은 output_curr로부터의 다른 손실과 결합될 수 있고, 네트워크의 가중치를 조정하기 위해 역 전파될 수 있고, 또한 기하학적 손실이 없는 output_future로부터의 손실은 가중치를 조정하기 위해 전파된다. 예를 들어, output_future의 기하학적 손실은 무시되고, 트레이닝을 위해 output_curr만이 사용된다. 도 10은 일 실시예에 따른 멀티-모드 손실 함수 적용 엔진의 일 예를 도시한다. 도시된 예에서, 멀티-모 드 손실 함수 적용 엔진은 4 개의 개별 손실 함수 트레이닝 엔진: 즉, 사진 측량 손실 함수 트레이닝 엔 진, 예측 이미지 사진 측량 손실 함수 트레이닝 엔진, EM 손실 함수 트레이닝 엔진 및 기하 학적 손실 함수 트레이닝 엔진을 포함한다. 또한, 멀티-모드 손실 함수 적용 엔진은 역 전파 엔진 및 트레이닝 데이터 저장소를 포함한다. 손실 함수 트레이닝 엔진(1004 내지 1010)은 도로 구조 를 도로 구조의 정확도에서 오차 또는 손실을 확인하기 위해 전통적인 \"실측 자료(ground truth)\" 값 대신에 사용되는 대응 참조 기준과 비교하도록 구성된다. 일 예에서, (전통적인 지도 머신 러닝 시스템에서와 같은) 실제의 실측 자료 데이터(ground-truth data)는 사용 되지 않는다. 대신, 트레이닝 데이터의 이미지는 이 이미지에 대응하는 에고 모션, 카메라 높이, 에피폴 등과 같은 추가적인 이용 가능한 데이터와 함께 처리되어, 손실 함수의 평가를 위한 참조 기준을 생성한다. 어떤 의 미에서, 참조 기준은 트레이닝 데이터를 기반으로 하기 때문에, 이는 비 지도 러닝의 타입인 것으로 간주될 수 있다. 일 예에서, 실측 자료 데이터는 트레이닝 데이터용으로 이용 가능하다. 일 예로서, 실측 자료 데이터는 3 차원 이미징 또는 스캐닝 측정(예를 들어, 입체 이미징, LiDAR 스캔 등)과 같은 추가적인 측정 양식(measurement modality)에 의해 제공될 수 있다. 따라서, 하나 이상의 손실 함수는 실제의 실측 자료에 기반하여 지도 러닝 의 타입을 제공할 수 있다. 손실 함수 트레이닝 엔진(1004 내지 1010)은 각각 DNN을 트레이닝하는 데 사용되는 전체 손실 함수의 컴포넌트 에 기여할 수 있다. 역 전파 엔진은 그래디언트 하강 기술(gradient-descent technique)을 사용하여 각 각의 개별 동작 파라미터에 대한 조정 방향을 결정하기 위해 가변 계산 파라미터(예를 들어, 가중치, 바이어 스)에 대한 전체 손실 함수의 부분 도함수(partial derivatives)를 계산하도록 구성될 수 있다. 역 전파 엔진 은 역 전파 경로를 따라 각각의 연속적인 계층에서 업데이트된 계산 파라미터 값을 적용할 수 있다. 트 레이닝 데이터 저장소는 DNN의 적절한 입력 계층(들)에 적용될 트레이닝 데이터, 이미지 포맷된 모션 표 시, 및 이미지 포맷된 에피폴 표시를 포함할 수 있다. 일 예에서, 손실 함수는 그러한 프리미티브의 복잡한 조 합을 포함하는 텐서 흐름 프리미티브 함수(Tensor Flow primitive functions)의 관점에서 정의된다. 이러한 방 식으로 손실이 정의되면, 텐서 흐름은 부분 도함수를 계산하는 데 사용될 수 있다. 사진 측량 손실 함수 트레이닝 엔진은 전진방향 전파 경로에서 DNN에 제공된 트레이닝 데이터로부터 이미 지 프레임 세트에 기반하여 참조 기준을 생성하도록 구성된다. 일 예에서, 3 개의 이미지(현재, 이전 및 이전- 이전 이미지)가 DNN에 대한 입력으로 사용되는 경우, 도로 구조로서 생성된 감마 맵은 이전 및 이전-이전 이미지를 현재 이미지로 와핑하는 데 사용된다. 각각의 와핑된 이미지는 잔차 흐름을 보상하기 위해 보정되며 실제의 현재 이미지와 비교된다. 잔차 흐름 보상은 다음에 따라 결정될 수 있으며,"}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, 는 잔차 흐름을 나타내며, (감마)는 도로 구조이며, 항은 카메라 높이로 나눈 전진 방향 에고 모션을 나타내며, 항은 노면의 평면을 기술한다. 이미지 비교는 다음에 따라, 각 픽셀을 둘러싸는 이미지의 패치에 적용될 수 있는, 정규화된 상호 상관, 합산된 절대치 차이(sumed absolute difference)(SAD), 이진 디스크립터 거리 등과 같은 적절한 기술을 사용하여 계산 될 수 있다."}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, 는 언와핑된(un-warped) 현재 이미지이며, 는 감마 와핑된 및 잔차 흐름 보상된 이전(또는 이 전-이전) 이미지이고, 는 와핑 전의 이전(또는 이전-이전) 이미지이다. 일 예에서, 물체 탐지(예를 들어, 차량 탐지, 자전거/보행자 탐지)는 손실 함수로부터 움직이는 물체를 마스킹하여 비교된 이미지들 사이에 서 탐지된 모션을 감소시키는 데 사용된다. 이미지 비교는 이미지들 간의 그레이 레벨 비교를 포함할 수 있다. 일 예에서, 사진 측량 손실 함수 트레이닝 엔진은 도로 및 비도로 특징부에 대응하는 이미지 비교의 부분 들에 가변 가중치를 적용한다. 따라서, 비도로 부분들에서 발견된 비교 이미지들 간의 차이의 정도는 무시 (discounted)될 수 있다. 예측 이미지 사진 측량 손실 함수 트레이닝 엔진은, DNN이 도로 구조를 생성하기 위해 사용했던 이 미지를 사용하는 것 외에 이미지 비교 처리에 하나 이상의 \"미래\" 또는 \"과거\" 이미지(들)가 포함되는 것을 제 외하고는, 사진 측량 손실 함수 트레이닝 엔진과 유사한 이미지 와핑, 보상 및 비교 기술을 수행하도록 구성된다. \"미래\" 이미지는 DNN을 트레이닝하는 데 사용되는 현재의 이미지 세트보다 나중에 캡처된 이미지이 며, \"과거\" 이미지는 조기에 캡처되었던 이미지이다. 따라서, 미래의 이미지를 위해, 예측 이미지 사진 측량 손실 함수 트레이닝 엔진에 의해 제공되는 손실 함수 컴포넌트는 실시간으로 이용될 수 없는 트레이닝 데 이터를 사용한다. 특히, 계산된 추론은 추론이 입력으로 간주하지 않는 이미지에 작용하는 감마를 생성한다. EM 손실 함수 트레이닝 엔진은 도로 구조에 대응하는 도로 부분을 통한 차량의 통과를 나타내는 \" 미래\" 에고 모션에 대해 도로 구조을 비교하는 것에 기반하여 손실 함수 컴포넌트를 생성하도록 구성된다. 일 예로서, 도로 구조에서 임의의 범프 또는 홀의 표시가 없는 경우, 도로의 범프 또는 홀을 나타내는 에고 모션은 손실이 된다. 일 예에서, 상향 또는 하향 곡률이 사용될 수 있다. 일 예에서, EM은 20m 를 초과하게 (예를 들어, 최대 50m까지) 연장될 수 있다. 이는 도로의 일부가 너무 멀어 잔차 흐름을 계산할 수 없는 경우에도 DNN이 도로 구조로부터 표면의 장거리 형상을 올바르게 모델링하도록 도움을 줄 수 있다. 유 사하게, 도로 구조가 해당 위치에서 (특히, 차량 휠의 경로에서) 범프 또는 홀을 예측하는 동안 범프 또 는 홀에 대응하는 임의의 에고 모션이 존재하지 않으면, 손실을 형성하게 된다. 일 예에서, 차량이 도로의 지형을 지나갈 때 차량 서스펜션의 감쇠 효과를 모델링하기 위해 0.5Hz 주파수를 갖 는 저역 통과 필터 또는 감쇠 스프링 모델(damped-spring model)이 도로 구조에 적용된다. 다른 예에서, 차량의 서스펜션 상태가 이용 가능한 경우, 차량 휠의 수직 모션을 보다 정확하게 측정하기 위해 서스펜션 정보 가 에고 모션과 함께 고려된다. 기하학적 손실 함수 트레이닝 엔진은 \"미래\" 이미지 프레임 및 대응하는 \"미래\" 에고 모션을 포함하는 하 나 이상의 \"미래\" 트레이닝 데이터 세트를 사용하여 손실 함수 컴포넌트를 생성하도록 구성된다. \"미래\" 이미 지 프레임은 입력으로서 사용된 현재 이미지 프레임보다 앞서 (현재 이미지로부터 보다 먼 거리에서 또는 보다 나중에 캡처된) 정의된 거리 또는 시간 스텝에서 캡처된 이미지를 나타낸다. 예를 들어, \"미래\" 이미지 프레임 및 에고 모션은 트레이닝 데이터의 다음 후속하는 삼중 캡처 이미지에 대응할 수 있다. 다른 예에서, \"미래\" 이미지 프레임 및 에고 모션은 차량의 위치로부터 5 미터, 20 미터 또는 몇몇 다른 정의된 거리에 대응한다. 참조 기준은 \"미래\" 도로 구조(예를 들어, 감마 맵)를 기반으로 하며, 이는 DNN을 사용하여 계산된다. 기하학 적 손실 함수 트레이닝 엔진은 \"미래\" 에고 모션을 사용하여 \"미래\" 도로 구조를 현재 도로 구조로 와핑하거나, 현재 도로 구조를 \"미래\" 에고 모션을 사용하여 \"미래\" 도로 구조로 와핑한다. 일 예에서, \"미래\" 도로 구조는 현재 도로 구조로 와핑되고, 이들 사이에서 제 1 비교가 행해지고, 현재 도로 구조는 \"미래\" 도로 구조로 와핑되고, 이들 사이에서 제 2 비교가 행해진다. 제 1 및 제 2 비교의 결과는 집계된 비교를 생성하도록 조합(예를 들어, 평균화)될 수 있으며, 이 집계된 비교는 이어서 기하학적 손 실 함수 트레이닝 엔진에 대한 손실 함수를 결정하는 데 사용된다. 일 예에서, 다수의 카메라 및 중첩 시야가 사용되는 경우, 다수의 뷰로부터의 관련 이미지는 기하학적 손실 함 수 트레이닝을 달성하는 데 사용될 수 있다. 예를 들어, (시간 t3에서) \"미래\"의 좌측 및 중앙 이미지는 시간 t3으로부터의 감마 와핑된 이미지가 시간 t2에서의 중앙 이미지와 광도계로 유사해야 한다는 요건으로 처리될 수 있다. 미래의 두 쌍의 이미지는, 그러한 이미지로부터 추론되는 감마가 카메라 모션의 보정 후 시간 t1 및 t2로부터의 이미지를 사용하여 도출된 감마와 유사하다는 조건을 설정하는 데 사용될 수 있다. 일 예에서, 중 앙 메인 카메라는 차량 지붕의 좌측 또는 우측 코너에 탑재되어 전방 및 측방을 주시하는 하나 이상의 카메라와 함께 사용될 수 있다. 이러한 측방 카메라는 시야가 90 도보다 넓을 수 있다. 우측 카메라 시야는 메인 카메 라의 우측 시야와 상당히 중첩될 수 있고 후방으로 연장되는 시야를 가질 수 있다. 좌측 카메라는 메인 카메라 의 좌측 시야와 상당히 중첩될 수 있고 후방으로 연장되는 시야를 가질 수 있다. 이러한 카메라의 배열은 도 22에 도시되며, 여기서 카메라(2212B)는 메인 카메라이고, 카메라(2212A 및 2212C)는 각각 좌측 및 우측 카메라 이다. 일 예에서, 코너 카메라로부터의 이미지는 추론 단계에서는 사용되지 않고 손실 함수를 계산하기 위해 트레이닝 단계에서 사용될 수 있다. 2 개 이상의 손실 함수 트레이닝 엔진(1004 내지 1010)에 의해 기여된 손실 함수 컴포넌트는 역 전파 엔진 에 의해, 집계된 멀티-모드 손실 함수 내로 결합되며, 이 멀티-모드 손실 함수는, 예를 들어, 그래디언트 하강 기술을 사용하여 DNN을 트레이닝하여 계산 파라미터 조정치를 생성하는 데 사용된다. 도 11은 일 실시예에 따른 물체의 움직임 여부에 관한 판정을 생성하는 신경 네트워크(예를 들어, DNN)의 일 예를 도시한다. 신경 네트워크는 신경 네트워크와 같은 상술된 신경 네트워크와 유사하게 동작 한다. 신경 네트워크로의 입력은 현재 이미지, 하나 이상의 이전 이미지, 타겟 위치, 및 타 겟 사이즈를 포함한다. 도 11에 도시된 예는 신경 네트워크를 사용하여 타겟이 움직이는지를 결정하지만, 일 예에서, 움직이는 타겟은 전술한 네트워크로부터의 감마를 사용하는 감마 정렬을 사용하고 휴리스틱 기술로 잔차 모션을 측정함으로써 결정될 수 있다. 예를 들어, 차량 또는 차량의 휠과 같은 알려진 타입의 타겟의 베이스에서 잔차 모션이 검출되면, 차량이 움직이고 있다고 결론 지을 수 있다. 도시된 바와 같이, 타겟 위치 및 사이즈는 이미지로서 입력된다. 타겟 위치는 픽셀 값이 타겟의 중심으로부터 의 거리를 나타내는 두 개의 그래디언트 이미지를 포함한다. 여기서, 수평 그래디언트 이미지(예를 들어, 위치 x 또는 ) 및 수직 그래디언트 이미지(예를 들어, 위치 y 또는 )는 신경 네트워크에 입력되는 타겟 위치를 구성한다. 이 이미지는 타겟에 대한 그래디언트 관계를 설명하기 위한 타겟의 윤곽을 포 함한다. 여기에서 타겟 사이즈는 모든 픽셀이 타겟 사이즈를 나타내는 동일한 값(예를 들어, 상수 값 이미지) 을 갖는 이미지로 표현된다. 일 예에서, 마스크(예를 들어, 템플릿)는 타겟을 식별하는 데 사용될 수 있 다. 일 예에서, 마스크는 하나 이상의 그래디언트 이미지(1106 및 1108) 또는 사이즈 이미지를 대체한다. 마스크를 사용하면, 예를 들어, 단일 타겟의 경우에 신경 네트워크에 의해 처리되는 이미지 데이터의 양이 감소될 수 있다. 다수의 타겟을 사용하면, 마스크는 입력 이미지의 동일한 부분이 여러 번 처리 되도록 할 수 있지만, 이는, 예를 들어, 마스크가 나중에 컨볼루션 추론 체인에서 사용되면 경감될 수 있다. 신경 네트워크의 출력은 목표가 움직이고 있는지, 움직이지 않는지, 또는, 예를 들어, 물체가 신뢰 성 임계치로 움직이는지를 결정할 수 있는 능력이 없음을 나타내는 \"어쩌면\" 움직일 수도 있는지의 카테고리 라 벨이다. 일 예에서, 카테고리 라벨은 실수 값의 스코어로서, 여기서, 큰 값(예를 들어, 제 1 임계치 초과)은 움직임을 나타내고, 낮은 값(예를 들어, 제 2 임계치 미만)은 움직이지 않음을 나타내며, 제 1 임계치와 제 2 임계치 사이에 있는 값은 어쩌면 움직일 수도 있는 것을 의미한다. 출력은 단일 타겟 추론에 대해 단일 값이 될 수 있거나, 다수의 타겟에 대한 (도시된 바와 같은) 벡터일 수 있다. 한 번에 다수의 타겟을 처리하기 위해 출력 벡터가 생성될 수 있다. 예를 들어, 이미지에서 최대 8 대의 차량에 대한 8 개의 출력은 대부분의 장면을 커버할 것이다. 일 예에서, 각각의 차량은 1 내지 8의 숫자와 같 은 상이한 라벨을 사용하여 마스킹된다. 위치 에서 출력(예를 들어, )은 이후 라벨 에 의해 마 스킹된 구역에 해당할 것이다. 만약 차량이 탐지되면, 라벨( 내지 )이 사용되며 벡터 값( 내지 *)은 트레이닝 및 추론 단계에서 무시된다. 일 예에서, 출력은 단일이고, 신경 네트워크는 이진 마스크를 통해 각각의 타겟 상에서 독립적으로 (예를 들어, 직렬로) 동작한다. 일 예에서, 입력 이미지들(1102 및 1104)은 정렬된다. 일 예에서, 정렬은 호모그래피에 기반한다. 일 예에서, 정렬은 노면 정렬(예를 들어, 감마 정렬)에 기반한다. 이미지 정렬은 정지 차량의 휠과 도로의 접촉 포인트와 같은 노면 상의 포인트를 안정화시킴으로써 신경 네트워크 트레이닝 또는 추론을 단순화시킨다. 따라서,신경 네트워크는 타겟이 움직이고 있다는 것을 결정하기 위해 타겟의 잔차 모션만을 식별할 필요가 있다. 도 12는 일 실시예에 따른 물체의 움직임 여부에 관한 판정을 생성하는 컨볼루션 신경 네트워크의 일 예 를 도시한다. 여기서, 보다 정확한 결과를 제공하기 위해 2 개 대신 3 개 이상의 입력 이미지가 사용될 수 있다. 많은 효과적인 신경 네트워크 구조가 사용될 수 있지만, 여기에 도시된 것은 컨볼루션 스테이지 이고 이에 후속하여 전체 이미지로부터의 정보를 단일 값(예를 들어, 움직임/움직이지 않음/미지) 으로 수집하는 아핀 스테이지(affine stage)가 이어진다. 일 예에서, 컨볼루션 스테이지는 이미지를 3 개의 채널로 수신한다. 컨볼루션 스테이지는 이미지의 해상도를 감소시키지만 채널의 수를 증가시켜, 복잡한 특징부들을 생성한다. 일 예에서, 타겟 위치는 제 1 계층에서 제 4 채널로서 도입될 수 있다. 예를 들어, 타겟 위치는 나중에, 예를 들어, 작은 해상 도의 채널이 많은 컨볼루션 스테이지 내의 병목 부분 또는 좁은 부분에 도입될 수 있거나, 아핀 스테이지 에 도입될 수 있다. 나중의 스테이지에서 다수의 타겟을 위한 타겟 마스크를 도입하는 것이 유리 할 수 있다. 예를 들어, 타겟(예를 들어, 마스크)의 도입까지의 계산의 제 1 부분은 한 번 수행될 수 있 고 그 결과는 모든 타겟에 대해 사용될 수 있다. 일 예에서, 아핀 스테이지에 대한 입력으로서 오리지널 이미지를 갖는 것이 유용하다. 이것은 \"건 너 뛰기\" 경로를 통해 달성될 수 있다. 선택적 사항이지만, 이 구조는 성능 분류 성능을 향상시킬 수 있다. 도 13은 일 실시예에 따른 수직 윤곽 탐지 엔진을 동작시키는 방법의 일 예의 흐름도이다. 방법의 동작은 전술한 것 또는 후술되는 것과 같은 계산 하드웨어(예를 들어, 처리 회로부)에 의해 수행된다. 동작에서, 카메라 높이 정보뿐만 아니라 2 개 이상의 이미지 프레임들의 시퀀스, 접지면, 및 에고 모션 데이터를 포함하는 원시 데이터가 획득된다(예를 들어, 판독 또는 수신된다). 이미지 프레임은 현재(예를 들어, 가장 최근에 캡처된) 이미지, 및 하나 이상의 이전에 캡처된 이미지를 포함할 수 있다. 동작에서, 원시 데이터는 노면에 대한 이미지 프레임들의 시퀀스 중에서 호모그래피를 결정하도록 처리된다. 이미지 프레 임들 중 일부는 이후 노면을 시퀀스의 다른 이미지 프레임과 정렬시키도록 와핑될 수 있다. 이 와핑은 일 예에 따라, 측정된 에고 모션 및 접지면의 특성에 기반할 수 있다. 에고 모션은 측정된 모션일 수 있거나, 이미지 프레임들의 내용으로부터 계산적으로 결정될 수 있다. 와핑된 이미지 프레임은 현재 이미지 프레임, 및 현재 이미지 프레임에 대응하도록 와핑된 하나 이상의 이전 이미지 프레임을 포함할 수 있다. 다른 예에서, 현재 이 미지 프레임 및 하나 이상의 다른 프레임은 와핑되지 않은 이전 이미지 프레임에 대응하도록 와핑된다. 일 예에서, 이미지들은 DNN에 의해 사용되기 전에 방사상 렌즈 왜곡과 같은 렌즈 왜곡에 대해 보정된다. 이러 한 보정은 특정 렌즈 상에서의 DNN의 트레이닝을 회피한다. 또한, 특히, 초점 거리는 감마를 위한 방정식의 컴 포넌트가 아니므로 다수의 상이한 카메라 타입으로부터의 이미지 상에서의 트레이닝을 가능하게 한다. 동작에서, 하나 이상의 이미지로서 포맷된 모션 정보(예를 들어, 에피폴)를 생성하기 위해 에고 모션 데 이터, 접지면 데이터, 및 카메라 높이 데이터를 포함하는 추가의 원시 데이터가 처리된다. 동작에서, DNN은 추론을 생성하는 데 사용된다. DNN은 컨볼루션, 비선형 활성화, 및 풀링 동작을 수행할 수 있다. 일 예에서, 디컨볼루션 및 언풀링 동작이 수행된다. 다양한 계층들에서, 가중치 또는 바이어스와 같 은 트레이닝된 계산 파라미터는 DNN의 사전 수립된 트레이닝에 따라 DNN의 동작에 의해 적용된다. 추론 모드에 서 DNN의 동작은 전술한 바와 같이 감마 맵과 같은 도로 구조 맵을 생성한다. 예를 들어 DNN을 사용하면, 차량 에서 시간당 최대 50km(50 km/h 또는 시간당 약 31 마일)까지로 주행하면서 1 센티미터(1cm) 이내, 또는 심지어 는 밀리미터의 절반(0.5mm)에서 10 미터(10m) 내의 정확도의 지형 측정치를 생성할 수 있다. 동작에서, 도로 윤곽 정보가 도로 구조 맵으로부터 추출된다. 잔차 흐름 정보와 같은 추가 정보가 또한 도로 구조 맵으로부터 추출될 수 있고, 이는 관련된 적용을 위해 추가로 처리될 수 있다. 도로 윤곽 정보는 차량 동작의 일부 양태를 자동으로 조정하는 자율 주행 또는 반 자율 주행 차량 제어 시스템 으로 전달될 수 있다. 예를 들어, 서스펜션 제어 시스템은 차량의 예상 주행 경로를 나타내는 수직 윤곽 데이 터에 기반하여 차량의 서스펜션을 동적으로 조정할 수 있다. 서스펜션 조정은 서스펜션의 강성을 동적으로 가 변시키거나 도로의 수직 윤곽에 맞게 개별 휠의 높이를 가변시키는 것을 포함할 수 있다. 일 예에서, 도로 윤곽 정보는 주행 정책 시스템으로 전달될 수 있다. 주행 정책 시스템은 환경 모델을 사용하 여 미래의 내비게이션 액션을 결정할 수 있다. 주행 정책 시스템은 도로 윤곽 정보를 사용하여 내비게이션 액션을 선택하거나 결정할 수 있다. 주행 정책 시스템의 예는, 예를 들어, 국제 출원 공개 번호 WO2018/001684에 기술된 RSS이며, 이는 그 전체가 본원에 포함된다. 도 14는 일 실시예에 따른 ML 기반 윤곽 엔진에 사용하기 위해 DNN을 구성하는 방법의 일 예의 흐름도이 다. 방법의 동작은 전술한 것 또는 후술되는 것과 같은 계산 하드웨어(예를 들어, 처리 회로부)에 의해 수행된다. 동작에서, 트레이닝 데이터는 트레이닝 DNN에 공급된다. 트레이닝 데이터는 전진 방향으로 전파되고 동 시에 트레이닝 DNN은 자신의 추론 모드로 동작하여 테스트 결과를 그 출력으로서 생성한다. 테스트 결과는 다 수의 컴포넌트를 갖는 손실 함수와 비교된다. 동작에서, 사진 측량 손실 함수 컴포넌트가 적용된다. 사 진 측량 손실 함수 컴포넌트는 테스트 결과를 사용하여 트레이닝 데이터의 하나 이상의 이전 이미지를 트레이닝 데이터의 현재 이미지로 와핑하고, 현재 이미지와 이전 이미지 간의 차이에 기반하여 손실을 생성한다. 정규화 된 상호 상관 함수는 비교된 이미지 프레임들 간의 차이를 확인하기 위해 각 픽셀을 둘러싸는 패치 상에 사용될 수 있다. 동작에서, 예측 이미지 사진 측량 손실 함수 컴포넌트가 적용된다. 예측 이미지 사진 측량 손실 함수 컴 포넌트는, 비교를 가능하게 하는 테스트 결과 기반 이미지의 이미지 와핑에 후속해서 (예를 들어, 테스트 결과 를 생성하는 데 사용된 트레이닝 데이터 이외의) 추가적인 트레이닝 데이터를 현재 및 이전 이미지와 비교하는 것을 제외하고는, 동작에서와 유사한 기술을 적용한다. 비교로부터 발생하는 임의의 차이는 추가적인 손 실 컴포넌트로서 다루어진다. 선택적으로, 동작(1404 및 1406)에서, 도로 특징부 및 비도로 특징부는 비교 및 손실 계산을 위해 상이한 가중 치를 부여받을 수 있으며, 도로 특징부는 더 강하게 가중화된다. 또한, 차량 및 보행자와 같이 움직이는 알려 진 물체는 마스킹되어 비교된 이미지들 사이의 잔차 흐름의 탐지를 감소시킬 수 있다. 동작에서, EM 손실 함수 컴포넌트가 적용된다. EM 손실 함수 컴포넌트는 테스트 결과를 생성하기 위해 처리된 트레이닝 데이터 이미지에 대한 차량의 통과에 해당하는 EM 데이터를 사용하고, 테스트 결과에 기반하여 차량의 예상된 모션과 EM 데이터를 비교하여 손실 컴포넌트를 제공한다. 동작에서, 기하학적 손실 컴포넌트가 적용된다. 기하학적 손실 컴포넌트는 테스트 결과를 생성하는 데 사용되지 않은 트레이닝 데이터의 일부를 사용한다. 특히, \"미래\" 이미지는 전술한 바와 같이 \"미래\" 테스트 결과를 생성하기 위해 트레이닝 DNN에 의해 처리된다. \"미래\" 테스트 결과는 \"미래\" EM에 기반하여 와핑되어, 테스트 결과와 정렬되거나, 또는 대안적으로 또는 추가적으로, 테스트 결과는 \"미래\" EM에 기반하여 와핑되어 \"미래\" 테스트 결과와 정렬되며, \"미래\"와 현재 도로 구조 테스트 결과들 사이의 비교치가 계산되어 추가적인 손실 컴포넌트를 제공하게 된다. 동작에서, 손실 함수 컴포넌트는 그래디언트 하강 계산을 위해 멀티-모드 손실 함수 내로 집계된다. 일 예에서, 손실 함수 컴포넌트들 중 임의의 둘 이상이 이용될 수 있다. 예를 들어, 다음 표에 나타난 바와 같이 손실 함수 컴포넌트들 중의 임의의 것이 집계될 수 있다: 동작에서, 집계된 손실 함수는 트레이닝 DNN을 통해 역 전파되며, DNN의 각각의 트레이닝 가능한 계층에 서 계산 파라미터에 대한 부분 도함수가 계산된다. 동작에서, 각각의 트레이닝 가능한 계층에 대한 계산 파라미터는 손실을 최소화하기 위해 손실 함수의 계산된 그래디언트에 기반하여 조정된다. 동작에서, 트 레이닝 프로세스는 파라미터 값을 더 최적화하기 위해 추가적인 트레이닝 데이터를 사용하여 반복될 수 있다. 트레이닝 반복 기준은 추가적인 트레이닝 사이클이 요구되는지를 결정하기 위해 (예를 들어, 파라미터 수렴에 기반하여) 각각의 역 전파 반복 이후에 적용될 수 있다. 동작에서, DNN의 각각의 계층에 대해 최적화된 계산 파라미터 값을 포함하도록 계산 파라미터 데이터 구 조가 구축된다. 이 데이터 구조는 테이블, 링크된 리스트, 트리, 태그 포맷(예를 들어, 확장 가능한 마크업 언 어) 파일 등과 같은 임의의 적합한 형태를 취할 수 있다. 동작에서, 계산 파라미터 데이터 구조는 차량 연결 DNN을 구성하는 데 사용된다. 도 15는 일 실시예에 따른 자율 주행 차량이 도로를 따라 이동하는 동안 도로의 수직 윤곽의 실시간 측정 방법 의 일 예의 흐름도이다. 방법의 동작은 전술한 것 또는 후술되는 것과 같은 계산 하드웨어(예를 들어, 처리 회로부)에 의해 수행된다. 동작에서, 카메라의 시야에서 도로의 동일한 부분의 이미지 프레임들(예를 들어, 제 1 이미지 프레임(A), 제 2 이미지 프레임(B) 및 제 3 이미지 프레임(C))의 시퀀스가 캡처된다. 단계에서 제 1 이미지 프레임 (A)에서의 도로의 이미지 포인트가 제 2 이미지 프레임(B)에서의 도로의 대응하는 이미지 포인트에 매칭된다. 마찬가지로, 제 2 이미지 프레임(B)에서의 도로의 이미지 포인트는 동작에서 제 3 이미지 프레임(C)에서 의 도로의 대응 이미지 포인트에 매칭된다. 동작에서 근접한 이미지 쌍의 호모그래피가 계산된다. 동작에서, 제 1 이미지 프레임(A)을 제 2 이미지 프레임(B)으로 변환하는 제 1 호모그래피(HAB)가 계산된다. 제 1 호모그래피(HAB)는 제 1 이미지 프레임 (A)의 도로의 매칭하는 이미지 포인트와 제 2 이미지 프레임(B)의 도로의 대응하는 이미지 포인트의 세트로부터 계산될 수 있다. 도로의 제 2 이미지 프레임(B)을 제 3 이미지 프레임(C)으로 변환하는 제 2 호모그래피(HBC)는 또한 제 2 이미지 프레임(B)의 도로의 매칭하는 이미지 포인트와 제 3 이미지 프레임(C)의 도로의 대응하는 이 미지 포인트로부터 계산될 수 있다. 동작에서, 제 1 및 제 2 호모그래피(HAB 및 HBC)는, 예를 들어, 행렬 곱셈(matrix multiplication)에 의해 체인화(chained)될 수 있다. 동작에서, 체인화된 호모그래피를 초기 추정치(예를 들어, 추측치)로 사용 함으로써, 도로의 제 1 이미지를 도로의 제 3 이미지로 변환하는 제 3 호모그래피(HAC)가 계산될 수 있다. 동작 에서, 제 1 이미지 프레임(A)으로부터 제 2 및 제 3 이미지 프레임들(B 및 C)로의 제각기의 잔차 흐름은 제 3 호모그래피(HAC)를 사용하여 도로의 수직 윤곽을 계산하도록 처리될 수 있다. 도 16은 일 실시예에 따른 도로의 수직 윤곽을 측정하기 위해 이미지들의 시퀀스를 통한 잔차 흐름을 처리하기 위한 예시적인 접근 방법의 흐름도이다. 방법의 동작은 전술한 것 또는 후술되는 것과 같은 계산 하드웨어(예를 들어, 처리 회로부)에 의해 수행된다. 동작에서, 이미지 프레임은 와핑된 이미지를 생성하기 위해 초기에 제 2 이미지 프레임으로 와핑된다. 본 문맥에서 용어 \"와핑\"은 이미지 공간에서 이미지 공간으로의 변환을 지칭한다. 아래의 논의는 도로가 평탄 한 표면으로 모델링될 수 있다고 가정한다. 따라서, 도로의 이미지화된 포인트는 호모그래피에 따라 이미지 공 간에서 움직일 것이다. 와핑은 차량의 측정된 모션에 기반할 수 있다(예를 들어, 속도계 표시, 관성 센서 등에 기반할 수 있다). 예를 들어, 특정 초점 거리(예를 들어, 픽셀로 정의됨) 및 프레임들의 각각의 캡처 사이에서 발생하는 알려진 차량 모션을 갖는 알려진 높이에서의 주어진 카메라에 대해, 두 개의 이미지 프레임들 간의 노면의 이미지들 상 의 포인트들의 모션의 예측이 계산될 수 있다. 도로 포인트들의 모션에 대해 준평면 표면(almost-planar surface)의 모델을 사용하면, 제 2 이미지는 제 1 이미지를 향해 계산적으로 와핑된다. 다음의 Matlab™ 코드 는 동작에서 초기 와프를 수행하는 구현예이다: 이 예에서, dZ는 차량의 전진방향 모션, H는 카메라의 고도, f는 카메라의 초점 거리이다. 용어 p0 = (x0; y 0)는 도로 구조의 소실 포인트이다. 일 예에서, 차량에 시스템을 설치하는 동안 획득된 초기 캘리브레이션 값(여기서, x0은 차량의 전진 방향이고 y0은 차량이 수평 표면 상에 있을 때의 수평 라인임)이 사용될 수 있다. 변수 S는 카메라로부터 상이한 차량 거리 Z에서 캡처된 두 개의 이미지 프레임들 사이의 이미지 좌표와 관련된 전체 스케일 계수이다. 본 문맥에서 용어 \"상대적인 스케일 변화\"는 카메라까지의 거리 Z에 따른 이미지 좌표 에서의 전체적인 스케일 변화를 지칭한다. 일 예에서, 초기 와핑 동작은 제 2 이미지를 회전에 기반하여 차량 모션 보상 계수만큼 제 1 이미지로 변 환한다. 차량 모션 보상은 회전 추정치 또는 요, 피치 및 롤의 측정치에 기반하여 달성될 수 있다. 이러한 회 전 추정치 또는 측정치는 차량의 요, 피치 및 롤을 감지하도록 구성된 3 축 가속도계와 같은 관성 센서에 의해 제공될 수 있다. 관성 센서는 카메라에 통합되거나 차량 상의 또는 내부의 다른 곳에 탑재될 수 있다. 회전 추정치는 대신에 또는 추가적으로 하나 이상의 이전 이미지 프레임으로부터 계산적으로 획득될 수 있다. 동작에서 초기 와핑은 제 1 이미지와 제 2 이미지 사이의 상대적인 스케일 변화에 대한 조정을 더 포함할 수 있다. 상대적인 스케일 변화 조정은 회전 변환과 함께, 하나의 이중 선형 보간만이 수행되는 단일 와프 동 작 내로 결합될 수 있다. 일 예에서, 피치 및 요 회전만이 관련되는 경우, 이들은 이미지 시프트에 의해 근사화될 수 있다. 예를 들어, 요(yaw)는 다음 식으로부터 δθ 픽셀의 수평 이미지 시프트로 근사화될 수 있다:"}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "1601에서의 초기 와핑 동작 후에, 본 명세서에서 잔차 흐름으로 지칭되는 도로 상의 특징부의 겉보기 모션 (apparent motion)은 오리지널 이미지로부터 와핑된 이미지로의 이미지 패치의 균일한 변환으로서 로컬로 근사 화된다. 잔차 흐름은 오리지널 이미지와 와핑되지 않은 이미지 간의 실제 차량 모션 기반 차이와는 구별되며, 여기서 패치의 모션은 또한 균일하지 않은 스케일 변화를 포함한다. 일 예에서, 차선 마크 및 그림자와 같은 강한 특징부를 향해 바이어스를 변함없이 제공하게 될 특징부 포인트를 선택하는 대신에, 동작에서 포인트들의 고정된 그리드가 추적을 위해 사용될 수 있다. 따라서, 1603에서, 포인트들의 그리드는 이미지에서 전방으로 정의된 거리(예를 들어, 15 미터)까지 대략적으로 매핑되고 대략 1 차선(예를 들어, 2-3 미터)의 폭을 갖는 사다리꼴 영역으로부터 선택될 수 있다. 포인트들은 정의된 간격으로(예를 들어, 수평 방향으로 20 개의 픽셀마다 및 수직 방향으로 10 개의 픽셀마다) 이격될 수 있다. 유사한 효과를 갖는 다른 선택 방식이 사용될 수 있다. 예를 들어, 포인트들은 특정 분포에 따라 랜덤하게 선 택될 수 있다. 예를 들어, 표면(예를 들어, 도로)에 위치한 11 개의 포인트로 이루어진 3 개의 라인이 사용된 다. 이 라인들은 차량의 중심과 중심 라인으로부터 2 미터의 거리의 양 측면에 위치한다. 동작에서 오리지널 이미지의 각 포인트 주위에는 패치가 위치한다. 패치는, 예를 들어, 패치 중심 포인 트 주위에서 각 방향으로 정의된 형상 및 사이즈를 가질 수 있다. 예를 들어, 패치는 다수의 픽셀의 제곱일 수 있다. 일 예에서, 타원, 직사각형, 사다리꼴 등과 같은 다른 형상은 패치로 간주될 수 있다. 와핑된 이미지에 대해 정규화된 상관 관계가 (예를 들어, Matlab™ 함수(normxcorr2)를 사용하여) 계산되며, 패치 중심은 검색 영역에서 시프트된다. 실제로는 요 센서(yaw sensor)가 사용될 수 있지만 피치 센서(pitch sensors)는 사용되 지 않을 수 있으며; 따라서, y 방향이 아닌 x 방향에서는 보다 촘촘한 검색 영역(tighter search region)이 사 용될 수 있다. 일 예로서, x 방향으로의 (2x4 + 1) 개의 픽셀 및 y 방향으로의 (2x10 + 1) 개의 픽셀의 검색 영역이 사용될 수 있다. 일 예에서, 최대 상관 스코어를 생성하는 시프트가 결정되고, 이에 후속하여 서브-픽셀 해상도(예를 들어, 0.1 개의 픽셀)를 갖는 최고 스코어 위치 주위의 정제된 검색이 이어진다. 이 정제 단계는, 최대 스코어 주위의 정 수 스코어를 포물선 표면 또는 스플라인에 맞추고 최대 스코어 주위의 이들 정수 스코어를 사용하여 서브-픽셀 매칭을 계산하는 것에 비해 우수한 결과를 제공할 수 있다. 1607에서의 추적 동작의 결과로서 추적된 포인트를 남겨 두면서, 정의된 임계치(예를 들어, T=0.7)보다 높은 스코어를 갖는 포인트들을 선택함으로써, 유효하지 않은 트랙이 검색 스테이지에서 필터링 아웃될 수 있다. 와핑된 이미지로부터 오리지널 이미지로의 역 추적은 반대 방향으로 유사한 값을 제공한다. 추적 동작의 결과로서 추적된 포인트는 랜덤 샘플 합의(RANdom SAmple Consensus)(RANSAC)와 같은 적절한 기술을 사용하여 동작에서 호모그래피에 적합화(fit)된다. 포인트들의 세트는 랜덤하게 (예를 들 어, 4 개의 포인트로) 선택되고 호모그래피를 계산하는 데 사용된다. 이어서, 포인트는 호모그래피를 사 용하여 변환되고, 정의된 임계치보다 더 가까운 포인트들의 세트가 카운트된다. 포인트들의 세트를 랜덤하게 선택하고 임계치보다 가까운 포인트들의 수를 카운트하는 것은 다수 회 반복되며, 최고의 카운트를 제공한 네 개의 포인트가 보유된다. 1613에서, 네 개의 최상의 포인트는, 다시, 예를 들어, 최소 제곱 기술을 사용하여 이 포인트들 및 (아마도 상 이한) 정의된 임계치보다 더 가까운 모든 포인트들(인라이어(inliers))을 변환하여 호모그래피를 계산하는 데 사용된다. 임계치보다 가깝지 않은 나머지 포인트들은 아웃라이어(outliers)로 간주된다. 이러한 프로세스의 스테이지에서, 와핑된 이미지에서의 인라이어의 개수 및 그의 스프레드(spread)는 노면 모델 을 구하는 것의 성공과 관련한 유익한 정보가 된다. 인라이어가 50 %를 초과하고 적합도(fit)가 우수한 것이 일반적이다. 호모그래피는 그 후 동작에서 초기 정렬 와프를 보정하는 데 사용될 수 있다. 초기 정렬 와프의 보정은 이 보정을 동작에서의 초기 와프에 통합하거나 2 개의 와프를 연속적으로 수행함으로써 달 성될 수 있다. 전자는 단지 하나의 보간 단계만을 필요로 하기 때문에 유리하며, 2 개의 호모그래피 행렬의 행 렬 곱셈에 의해 선택적으로 수행될 수 있다. 일 예에서, 사전 와핑 후의 강력한 추적 기능은 호모그래피를 계산하는 데 적용된다. 이것은 이전 평면 추정치 를 조정하기 위해 에고 모션 및 평면 추정치를 도출하는 데 사용될 수 있다. 에고 모션과 조정된 평면 모델이 결합되어 호모그래피를 획득하게 된다. 일 예에서, 에고 모션은 에고 모션 센서 및 처리 엔진에 의해 제공될 수 있다. 이러한 타입의 엔진은 필수 행렬을 사용하여 도로 상의 포인트 및 도로 위의 포인트의 강력한 추적 기능을 사용한다. 이 엔진은 또한 사용 가능한 임의의 관성 센서 및 속도계 정보를 결합한다. 접지면 정보는 접지면 엔진에 의해 제공된다. 이 엔진은 (사전 와핑 후) 도로 상의 포인트를 추적하고 그 후 계산된 에고 모 션을 사용하여 각 포인트에 대한 심도를 도출하며, 이 심도는 포인트들의 3D 좌표를 생성한다. 그 후, 평면 표 면은 3D 포인트들에 알고리즘적으로 적합화된다. 일 예에서, 차량의 카메라 위치의 초기 캘리브레이션으로부터 도출된 것과 같은 디폴트 평면이 사용될 수 있다. 에고 모션과 평면 표면이 결합되어 호모그래피를 제공하게 된다. 일 예에서, 노면의 파라메트릭 모델(parametric models)이 사용될 수 있다. 예를 들어, 3D 스플라인 모델은 통 신가능하게 연결된 주행 경로의 기하학적 구조 또는 물체 감지 서브 시스템에 의해 제공될 수 있다. 하나의 그러한 서브 시스템의 일 예로서, Intel Corporation의 자회사인 Mobileye가 제조한 Road Experience Management ™ (REM™) 엔진은 3D 스플라인 형태의 도로 특징부의 표현을 제공할 수 있다. REM의 추가적인 세부 사항은 미 국 특허 번호 제9,665,100호 및 국제 특허 공개 번호 WO 2018/200522 A1에서 찾을 수 있으며, 이 둘 모두는 본 원에 참고로 포함된다. 예를 들어, 차선 마크는 3D 스플라인으로 표시될 수 있다. 서브 시스템은 3D 스플라인 특징부 표현을 이미지에 정렬한다. 3D 스플라인 특징부 표현은 추가 처리를 위해 도로의 캡처된 이미지 상으로 다시 투영될 수 있다. 3D 스플라인의 각각의 포인는 3D 좌표를 갖는다. 일 예에 따르면, 이 데이터가 사용될 수 있는 한 가지 방식은 캡처된 이미지의 행을 따라 3D 스플라인과 연관된 3D 위치를 보간 및 외삽하는 것이다. 측면들에 대한 외삽은 0 차 홀드(즉, 상수), 1 차 홀드(예를 들어, 2 개의 스플라인에 기반한 선형 확장) 등일 수 있다. 일 예에서, 보간은 이미지 자체의 외부에 있는 가상 이미지 좌표로의 특정 스플라인의 알려진 확장으로 발생할 수 있다. 보간 및 외삽은 이미지의 하부에 있는 모든 포인트에 대해 3D 좌표를 생성할 수 있다. 이들 포인트들은 매끄러 운 3D 표면 상에 있을 수 있다. 결과적으로, 표면은 이전 이미지를 현재 이미지로 (또는 그 반대로) 와핑하는 데 사용될 수 있다. 이러한 와핑은 트레이닝 및 추론 스테이지 모두에서 수행될 수 있다. 도 17은 일 실시예에 따른 차량 제어를 위한 퍼들 탐지 및 응답성 의사 결정 방법의 일 예의 흐름도이다. 방법의 동작은 전술한 것 또는 후술되는 것과 같은 계산 하드웨어(예를 들어, 처리 회로부)에 의해 수행 된다. 동작에서, 이미지들의 시퀀스는 하나 이상의 카메라에 의해 캡처된다. 동작에서, 에고 모션 및 참 조 평면 정보가 수신되거나 측정된다. 위에서 논의된 바와 같이, 에고 모션 정보는 가능하게는 차량의 속도계 로부터의 데이터와 조합하여 관성 센서로부터 제공될 수 있다. 접지면(예를 들어, 참조 평면) 정보는 카메라의 고정된 높이에 기반할 수 있고, 구성 또는 캘리브레이션 파라미터 중 하나로서 정의된 상수 값으로 저장될 수 있다. 동작에서, 이미지들은 실제 모션 측정치와 함께 처리되어 이미지들 간의 잔차 흐름을 결정한다. 잔차 흐 름은 (a) 차량의 실제 모션으로 인한 이미지들의 시퀀스 중 노면의 외관의 예측된 변화와 (b) 캡처된 바와 같은 시퀀스의 이미지들 간의 실제 차이 간의 비교로부터 검출된 차이를 나타낸다. 이러한 차이는 돌출부(범프) 및 함몰부(침몰한 부분 또는 홀)와 같은 것을 포함하여 수직 윤곽으로 더 해석될 수 있다. 동작에서, 도로 의 수직 윤곽은 전술된 기술 중 임의의 기술 또는 다른 적합한 이미지 처리 기반 기술을 사용하여 처리된 이미 지에 기반하여 계산적으로 결정될 수 있다. 이들 기술은 감마 또는 다른 형태의 구조를 사용하여 수직 윤곽을 결정한다. 일 예에서, 구조는 잔차 흐름으로 부터 결정될 수 있다. 일 예에서, 도 4와 관련하여 위에서 설명된 것과 같은 신경 네트워크는 구조를 직접 결 정할 수 있다. 감마가 결정된 후, 이미지는 감마에 따라 와핑되어 잔차 흐름을 결정한다. 다양한 광학 흐름 기술이 사용될 수 있지만, 둘 이상의 프레임들 간의 사진 측량 손실만을 사용하여 트레이닝된 제 2 신경 네트워 크를 사용하는 대안이 또한 존재한다. 퍼들과 같은 반사성 표면에 직면할 때, 제 1 신경 네트워크는 퍼들에서 움직이는 반사물을 무시하기 때문에 거의 평탄한 감마를 생성할 것이다. 사진 측량 손실을 매핑하도록 트레이 닝된 제 2 신경 네트워크는 퍼들에 대해 도로에서 큰 홀을 생성한다. 두 개의 출력을 비교하면 퍼들에 대한 기 준이 제공된다. 효율성은, 감마를 계산하고 그 후 잔차 흐름을 계산하는 것에 대해, 입력 이미지에 직접 동작하여 각각 감마 및 사진 측량 손실을 생성하는 두 개의 신경 네트워크의 출력을 비교할 때 획득되는 이점이다. 감마 와프를 수행 하는 것은 계산 비용이 많이 들 수 있으며 또한 프로세스를 병렬화하는 능력을 손상시킬 수 있는 일부의 순차적 제약을 도입할 수 있다. 제 1 및 제 2 신경 네트워크는 수 개의 동일한 층을 포함할 가능성이 있기 때문에, 매 우 효율적인 구현예는 전술한 2 개의 네트워크를 결합하여 단일 신경 네트워크를 2 개의 출력 채널로 트레이닝 시킬 수 있으며: 하나 출력 채널은 광도계 전용 감마 맵을 가지며; 두 번째 출력 채널은 결합된 광도 및 기하학 적 신경 네트워크를 갖는다. 퍼들과 같은 반사성 표면은 2 개의 채널에서 감마를 비교함으로써 탐지될 수 있다. 일 예에서, 2 개의 신경 네트워크는 네트워크의 부분을 공유하고, 그렇지 않으면 분리된다. 예를 들어, 제 1 및 제 2 신경 네트워크는 병목 현상이 발생할 때까지 동일한 구조를 공유할 수 있고, 이 후 분기될 수 있 다. 일 예에서, 제 1 및 제 2 신경 네트워크는 모두 출력 구조(예를 들어, 감마 또는 ) 맵으로 트레이닝되지만, 각 각은 상이한 손실 함수로 트레이닝된다. 일 예에서, 손실 함수에서의 차이는 두 개의 상이한 타입의 손실의 가중치에서의 차이이다. 일 예에서, 손실은 광도, 기하학적 구조, EM, 또는 향후 이미지 손실중의 하나 이상이다. 두 개의 출력의 결합이 사용되어, 움직이는 물체, 반사물, 또는 투명도를 탐지할 수 있다. 이것은 실제로 신경 네트워크를 사용하여 반사성 또는 거울 표면을 탐지하는 일반적인 방법이다. 이것은, 또 는 을 결정하고 그 후 무엇보다도 차량 또는 건물의 측면들(예를 들어, 창문) 상의 반사성 표면을 탐지하도록 트레이닝된 신경 네트워크에 적용될 수 있다. 이 기술의 흥미로운 사용은 사각 교차로에 설치된 거울을 탐지하 는 것이다. 거울 표면이 탐지되면, 그것은 반사된 이미지에서 차량 및 보행자의 탐지를 위해 분석될 수 있고, 차량 비전 시스템이 거울을 사용하여 사고를 피하게 할 수 있다. 예를 들어, 보행자 또는 차량이 교차로 거울 에서 탐지되면, 호스트 차량은 차량 또는 보행자가 카메라의 뷰에 나타날 때까지 기다리거나 다른 방법으로 자 신의 내비게이션 액션을 수정할 수 있다. 움직이는 물체에 대한 반사를 탐지하기 위해, 메인 및 코너 카메라를 사용하여 움직이는 스테레오 배열이 구현될 수 있다. 여기서, 신경 네트워크는 사진 측량 제약 조건을 사용하 고 기하학적 제약 조건을 추가하기 위해 시간 경과에 따른 스테레오 쌍을 사용하여 스테레오 이미지로부터 심도 를 추론하도록 트레이닝될 수 있다. 실제 모션으로 인한 이미지에서의 예측된 변화와 실제 변화 사이의 차이는 움직이는 물체(예를 들어, 다른 차량, 보행자, 자전거), 움직이는 그림자, 및 반사물을 더 나타낼 수 있다. 따라서, 일 예에서, 하나 이상의 퍼들의 존재는 동작에서 수직 윤곽 정보 및 추가적인 퍼들 탐지 기준에 기반하여 계산적으로 결정된다. 방법은 동작의 예를 제공한다. 동작에서, 현재 상황 시나리오가 결정된다. 현재 상황 시나리오는 현재 도로 상황, 차량 모션, 및 주변 환경을 나타낸다. 일 예에서, 상황 시나리오는, 호스트 차량의 온보드 센서로부터 획득된 데이터, 맵으로부터 획득되는 도로 및 도로의 주변 상황의 모델, 원격 서버로부터 또는 인접한 타겟 차량으로부터 호스트 차량으로 전달되는 데이터, 또는 스마트 기반 구조 물체 중 임의의 하나 이상으로부터 획득될 수 있다. 방법은 동 작의 예를 제공한다. 동작에서, 이용 가능한 주행 응답 솔루션이 현재 상황 시나리오에 기반하여 평가된다. 이 예에서 주행 응답 솔루션은 탐지된 퍼들의 존재에 응답하는 것에 특정적이다. 방법은 동작의 예를 제공한다. 동작에서, 다양한 주행 응답 옵션의 평가에 기반하여 주행 응답 솔루션이 선택된다. 특히, 주행 응답 솔 루션은 임의의 회피 또는 다른 액션을 취하지 않을 수 있다. 도 18은 일 실시예에 따른 수직 윤곽 정보 및 추가적인 퍼들 탐지 기준에 기반하여 하나 이상의 퍼들의 존재를 계산적으로 결정하기 위한 방법의 일 예의 흐름도이다. 방법의 동작은 전술한 것 또는 후술되는 것과 같은 계산 하드웨어(예를 들어, 처리 회로부)에 의해 수행된다. 동작에서, 이미지의 포맷(예를 들어, 노면 높이를 나타내는 값을 갖는 픽셀)으로 표현될 수 있는 포인트 단위의 수직 윤곽 데이터가 심도 기준에 대해 평가된다. 예를 들어, 도로 상의 깊은 홀로 나타나는(예를 들어, 정의된 최소 심도 임계치를 초과하거나, 이웃하는 포인트들로부터 정의된 최대 높이 변동을 초과하는 등) 포인 트(예를 들어, 픽셀)는 퍼들에 속하는 포인트들의 세트 내의 가능한 멤버의 표시로 태그되거나 또는 다른 방식 으로 연관될 수 있다. 일 예로서, 심도 기준 임계치는 50 cm 내지 1.5 m 정도일 수 있거나 또는 심지어는 훨씬 더 클 수 있다. 동작에서, 주어진 부근에서 심도 임계치를 충족하거나 초과하는 포인트들의 영역이 기하학적 기준에 대해 평가된다. 일 예에서, 기하학적 기준은 잠재적 퍼들 영역의 공간적 특징부를 정의한다. 예를 들어, 영역 사이 즈, 영역 형상, 및 그 영역의 심도 기준을 충족하거나 초과하는 포인트들의 인접성과 같은 특징부가 고려될 수 있다. 예를 들어, 사이즈 또는 형상 기준은 잠재적 퍼들로 간주될 영역에 대한 영역의 최소 표면적, 또는 최소 길이 또는 폭 치수를 지정할 수 있다. 인접성 기준은 최소한의 인접성 범위(예를 들어, 영역 내의 포인트들의 인접성, 또는 심도 기준을 충족하는 이웃 포인트들 간의 최대 거리)를 지정할 수 있다. 동작에서, 잠재적 퍼들 영역의 경계 첨예도(boundary sharpness)는 첨예도 기준에 대해 평가된다. 첨예 도 평가는 부드러운 경계(soft boundaries)를 갖는 경향이 있는 움직이는 그림자(이는 또한 이미지 분석 동작에 서의 잔차 흐름의 원인임)로부터, 첨예한 경계(sharp boundaries)를 갖는 경향이 있는 퍼들을 구별하도록 동작 할 수 있다. 첨예도 기준의 예로서, 다음의 두 개의 요인이 적용될 수 있다: 경계 부근의 의심되는 퍼들 영역 내부의 포인트들에 대해 적용될 네거티브 잔차 흐름에 대한 최소 임계치(또는 노면 함몰부의 심도) 및 의심되는 퍼들 영역 경계 외부의 포인트들에 대해 적용될 잔차 흐름의 최대 한계치(또는 참조 노면과 정렬될 노면의 평탄도). 동작에서, 동작들(1802, 1804 및 1806)에서의 다양한 기준을 충족시키거나 충족시키지 못하는 것에 기반 하여, 퍼들 결정의 표시가 생성된다. 도 19는 일 실시예에 따른 자율 주행 차량을 위한 현재 상황 시나리오를 계산적으로 결정하기 위한 방법 의 일 예의 흐름도이다. 방법의 동작은 전술한 것 또는 후술되는 것과 같은 계산 하드웨어(예를 들어, 처리 회로부)에 의해 수행된다. 동작에서, 차량 속도가 평가된다. 이 평가는, 예를 들어, 속도계 또는 차량 제어 시스템으로부터 획득될 수 있다. 동작에서, 노면 및 도로 상황이 평가된다. 이러한 평가는 노면의 마찰력(traction) 또는 미끄 러짐(slipperiness) 정도의 품질을 결정하는 것을 포함할 수 있다. 예를 들어, 도로 타입은 포장 도로, 돌, 흙, 자갈 중에서 선택될 수 있다. 노면은 지도 제작 데이터, 측정된 차량 성능(예를 들어, 조향각 및 속도와 관련한 휠 미끄러짐 및 횡 모션(wheel slippage and lateral motion)), 또는 이러한 요인들의 일부 조합에 기 반하여 결정될 수 있다. 도로 상황은 건조 상태, 젖은 상태, 눈 상태, 결빙 상태, 먼지 상태, 낙엽이 덮인 상 태 또는 기타 잔해물 상태 등과 같은 다양한 범주로부터 결정될 수 있다. 도로 상황은 날씨 보고 데이터, 측정 된 차량 성능, 또는 이러한 요인들의 조합에 기반하여 추정될 수 있다. 1904에서의 평가는 자율 주행 차량 제 어 시스템의 다른 엔진으로부터 이용 가능할 수 있다. 1902 및 1904에서의 차량 속도 및 도로 품질 및 상황 평 가는 적절한 주행 응답 솔루션을 결정하는 부분으로서 기동 한계치(maneuvering limits)를 계산적으로 결정하는 데 사용될 수 있다. 일 예에서, 기동 한계치는 둘 이상의 서브 범주의 한계치를 포함할 수 있다. 이러한 서브 범주의 한계치는, 예 를 들어, (다른 서브 범주의 기동과 비교할 때) 첨예하거나 강력한 기동과 연관될 수 있는 안전 기동 한계치, 및 일반적으로 차량의 승객, 및 도로의 다른 사용자 및/또는 차량 환경의 다른 사람(또는 동물)에게 보다 높은 수준의 안락함을 허용하는 부드럽거나 보다 점진적인 기동과 연관될 수 있는 안락한 기동 한계치를 포함할 수 있다. 동작에서, 퍼들 부근의 임의의 보행자의 존재 및 위치가 평가된다. 본 문맥에서의 보행자는, 걷는 사람, 자전거 라이더, 휠체어 또는 유모차를 타는 사람, 또는 길가 근처에 앉아 있는 사람과 같은 차량 외부의 사람을 포함한다. 이 보행자 평가는 자율 주행 차량 제어 시스템의 일부이고 사람을 탐지하도록 구성된 물체 인식 엔 진에 의해 공급되는 정보에 기반할 수 있다. 동작은 임의의 탐지된 보행자가 탐지된 퍼들 부근에(예를 들어, \"스플래시 범위(splash range)\" 내에) 있는지를 추가로 평가할 수 있다. 보행자 평가는 보행자에게 스플 래시하는 것을 피하기 위해 자율 주행 차량의 제어에서 회피 액션을 취하기 위한 선호도를 계산적으로 결정하는 데 사용될 수 있다. 일 예에서, 스플래시 범위는 퍼들의 중심 또는 가장자리로부터의 특정 거리와 같이 미리 정의될 수 있다. 예를 들어, 스플래시 범위는 퍼들의 사이즈, 차량의 속도 등에 기반하여 추정될 수 있다. 일 예에서, 다른 차량(이는, 예를 들어, 자전거 및 오토바이를 포함함)의 존재 및 위치가 평가될 수 있다. 스 플래시 범위 평가는 (어떤 방향에서든) 다가오는 차량이 다가오는 차량에 대한 가시성을 방해할 수 있는 방식으 로 호스트 차량의 스플래시 범위에 진입할 것인지를 결정하는 데 사용될 수 있다. 유사하게, 스플래시 범위 평 가는 호스트 차량이 호스트 차량에 대한 가시성을 방해할 수 있는 방식으로 다가오는 차량의 스플래시 범위에 진입할 것인지를 결정하는 데 사용될 수 있다. 스플래시 범위 평가는 호스트 차량 또는 다가오는 차량의 다양 한 특징을 이용하여 스플래시 범위 및 관련된 차량에 대한 영향을 평가할 수 있다. 차량 탐지 및 평가는 다가 오는 차량에게 스플래시하는 것을 피하기 위해 또는 다가오는 차량에 의해 스플래시되는 것을 회피하기 위해 자 율 주행 차량의 제어에서 회피 액션을 취하기 위한 선호도를 계산적으로 결정하는 데 사용될 수 있다. 동작에서, 탐지된 퍼들의 위치는 도로 경계에 대해 평가된다. 유사하게, 동작에서, 탐지된 퍼들의 위치는 도로 상의 임의의 차선 마커에 대해 평가된다. 동작(1908 및 1910)에서의 평가는 회피 액션 결정에 이 용되어, 퍼들을 회피하거나 임의의 스플래시를 감소시키거나 최소화하기 위해 자율 주행 차량이 조향될 수 있는 지 여부 및 자율 주행 차량이 어디로 조향될 수 있는지를 결정할 수 있다. 동작에서, 자율 주행 차량과 동일한 방향으로 주행하는 것과 같은 인근 차량의 위치 및 움직임 특징의 평 가가 행해진다. 동작에서, 임의의 다가오는 트래픽의 존재 및 움직임 특징의 평가가 행해진다. 인근의 또는 다가오는 트 래픽의 움직임 특징은 종 방향 거리, 횡 방향 거리(예를 들어, 현재 차선 위치 결정), 최종 속도, 예측된 움직 임 변화(예를 들어, 차선 변경, 속도 변화, 회전수) 등과 같은 요인을 포함할 수 있다. 동작(1912 및 1914)에서의 평가는 자율 주행 차량의 트래픽 감지/탐지 또는 제어 시스템으로부터 이용 가능할 수 있다. 부근 및 다 가오는 차량의 트래픽에 대한 이러한 평가는 회피 액션 결정에 사용되어, 안전을 유지하기 위해 자율 주행 차량 의 퍼들 회피를 아마도 제한할 수 있다. 도 20은 일 실시예에 따른 퍼들(또는 다른 반사성 표면)의 탐지에 응답하기 위해 선택될 수 있거나 선택되지 않 을 수 있는 이용 가능한 주행 응답 솔루션의 계산 평가 방법의 일 예의 흐름도이다. 방법의 동작은 전술 한 것 또는 후술되는 것과 같은 계산 하드웨어(예를 들어, 처리 회로부)를 사용하여 수행된다. 동작에서, 퍼들의 탐지에 응답하는 액션의 필요성이 평가된다. 액션의 필요성은 퍼들이 자율 주행 차량 의 경로 내에 있는지, 특히 차량의 휠들 중 적어도 하나의 예측된 경로 내에 있는지에 의존한다. 액션의 필요 성은 또한 퍼들 부근에 임의의 보행자가 존재하는지에 의존할 수 있다. 일부 구현예에서, 액션의 필요성은 또 한 차량의 운전자의 선호도에 의존할 수 있다. 예를 들어, 예시적인 실시예에서, 퍼들 탐지에 응답하는 주행 응답에 대한 다양한 정도의 필요성이 있을 수 있 다. 다음 표는 퍼들 응답에 대한 상이한 정도의 요구 또는 선호도를 요구하는 상황의 몇 가지 예를 나타낸다. 다양한 상황에는 해당 위험 스코어 증가 값이 표시된다. 위험 스코어 증가는 계산된 위험 평가에 대한 각 상황 의 해당 기여도를 나타낸다."}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "이들 예에서, 주로 도로의 가장자리에 있는 퍼들(예를 들어, 도로 경계에 부합하는 긴 형상을 갖는 퍼들)은 주 로 포트홀에 기인하는 것이 아니라 도로 경사면으로부터의 유실(run-off)에 의해 야기되는 것으로 추정될 수 있 다. 이러한 타입의 퍼들은 차량 자체 또는 부근 보행자에게 스플래시할 수 있다. 도로의 가장자리에서 떨어져 있는 퍼들은 차량이나 보행자에게 스플래시할 수 있지만, 이는 포트홀로 인한 것일 수도 있으며, 이 포트홀은 타이어, 휠, 서스펜션, 차량 하부 또는 차량의 다른 부분에 대한 손상과 같은 추가적인 해를 입힐 수 있다. 큰 포트홀은 또한 차량이 고속으로 주행하는 경우 차량을 코스에서 이탈시킬 가능성이 있다. 도로의 주요 부분을 가로 지르는 매우 큰 퍼들은 노면 내에서 물이 고인 딥(dip)으로 인한 것일 수 있다. 이러한 타입의 퍼들은 폐 처리와 같은 심각한 도로 위험을 가릴 수 있다. 노면의 대부분을 덮고 있는 매우 큰 퍼들은 또한 수막현상 (hyperplaning)으로 인한 것과 같이 차량 제어의 심각한 손실을 유발할 수 있는 심도를 가질 수 있거나 또는 차 량 엔진의 공기 흡입구 높이를 초과하는 극단적인 심도를 가질 수도 있다. 보행자의 존재는 퍼들을 빠른 속도로 치는 것에 대한 악화 요인으로 간주될 수 있다. 일부의 경우, 보행자의 존재는 안전 위험을 제시하는 것이 아니며, 오히려, 자율 주행 차량을 사회적 규범에 따라 공손하게 동작시키기 위한 선호도를 제시한다. 따라서, 보행자와 연관된 위험 증가는 차량의 오염물 침착과 연관된 위험 증가보다 크지만, 잠재적인 포트홀을 치는 것과 연관된 위험보다는 적다. 위의 표의 마지막 행은 경감 요인, 예를 들어, 정의된 임계치 미만인 퍼들의 사이즈 결정을 제시한다. 예를 들 어, 퍼들의 치수가 10 cm를 초과하지 않으면, 퍼들은 너무 작아 현저한 스플래시 또는 포트홀 관련 해를 야기하 지 않는 것으로 간주될 수 있다. 방법의 동작(2004 내지 2012)은 다양한 타입의 주행 응답의 평과와 관련된다. 각각의 평가는 다음과 같 은 요인을 고려한 다양한 기준을 기반으로 할 수 있다: 1. 안전 위험을 감소시키는 액션의 효과; 2. 자율 주행 차량에 의한 주행 공손성을 증진시키는 액션의 효과; 3. 안전 위험을 유발하거나 증가시키는 액션의 가능성; 4. 차량의 탑승자에게 불편을 유발하거나 증가시키는 액션의 가능성; 또는 5. 무례한 주행 거동을 유발하는 액션의 가능성. 각각의 주행 응답의 평가는 현재 상황 평가를 고려하여 각 기준에 수치 스코어를 할당함으로써 계산적으로 처리 될 수 있다. 따라서, 2004에서, 다양한 기준에 따라 속도 감소가 평가된다. 예를 들어, 속도 감소는 스플래시 를 감소시키거나 방지할 수 있으며 잠재적인 포트홀을 치는 것으로부터의 임의의 피해를 경감시킬 수 있지만; 속도 감소는 차량의 탑승자를 성가시게 할 수 있으며, 자율 주행 차량을 밀접하게 따르는 부근의 차량이 있는 경우, 속도 감소는 부근의 차량이 자신의 속도를 또한 감소시킬 필요성을 유발할 수 있고, 충돌의 위험을 증가 시킬 수 있고, 부근의 차량의 운전자 또는 탑승자를 잠재적으로 성가시게 할 수 있다. 동작에서, 차선 내 시프트(intra-lane shift)는 잠재적인 주행 응답으로 평가된다. 동작에서, 차 선 변경은 잠재적인 주행 응답으로 평가된다. 차선 내 시프트는 차선 변경보다 차량 탑승자 및 부근 차량에게 는 덜 혼란스러운 옵션(less-disruptive option)이지만, 이는 또한 퍼들의 위치 및 사이즈와 차량의 속도에 기 반하여 덜 효과적일 수도 있다. 동작에서, 도로의 갓길로의 순간적인 방향변경(swerve)이 평가된다. 동작에서, 다가오는 트래픽 차선으로의 순간적인 방향변경이 평가된다. 갓길 및 다가오는 차선의 방향변경은 위험도가 높은 주행 응답을 나타내며, 이는 현재 상황 시나리오에 따라 다른 주행 응답이 효과가 없거나 더 위험하다고 결정될 때 요구될 수 있다. 이들 기준의 각각은, 예를 들어, 상황 평가를 고려하여 계산적으로 평가될 수 있다. 동작(2004 내지 2012)에서 기준을 적용하는 일환으로서 상황 평가를 고려한 결과, 특정 주행 응답은 지배적인 상황에 따라 더 바람직하거 나 덜 바람직할 수 있다. 따라서, 예를 들어, 임의의 부근의 차량에 대한 존재 및 상대적인 거리(들)는, 퍼들 의 탐지에 응답하기 위해 속도 감소, 차선 변경, 차선 시프팅, 또는 방향변경, 또는 이들 주행 응답의 조합이 선택될 수 있는 범위에 영향을 줄 수 있다. 일 예에서, 방향변경은 차선 내 방향변경일 수 있으며, 이에 의해 차량은 차선을 변경하지 않고 차선 내에서 퍼들을 피하기 위해 자체적으로 방향을 재조정한다. 일 예에서, 차 선 내 방향변경 또는 속도 감소는 보행자가 있을 때만 수행된다. 따라서, 보행자가 없으면 기동이 수행되지 않 을 것이다. 예를 들어, 호스트 차량이 퍼들을 탐지하고 보행자를 탐지하면, 호스트 차량은 보행자가 탐지되지 않는 경우보다 퍼들을 보다 천천히 통과하도록 감속될 수 있다. 또한, 호스트 차량은 퍼들을 탐지하면 자신의 경로를 보행자로부터 멀어지도록 조정할 수 있고, 퍼들이 탐지되지 않으면 직선 경로를 유지할 수 있다. 또한, 보행자가 보도에 있고 퍼들이 존재하면 호스트 차량은 감속될 수 있지만, 보행자가 보도에 있고 퍼들이 존재하 지 않으면 감속되지 않을 것이다(예를 들어, 또는 덜 감속될 것이다). 도 21은 일 실시예에 따른 자율 주행 차량 제어 시스템과 함께 사용하기 위해 도로를 프로파일링하기 위한 카메 라 기반 차량 탑재 시스템을 도시한다. 도시된 바와 같이, 이 시스템은 다수의 서브 시스템, 검포넌트, 회로, 모듈, 또는 엔진(이들은 간결함 및 일관성을 위해 엔진이라 칭함)으로 구성되지만, 이들 용어는 상호 교환적으 로 사용될 수 있음을 이해할 것이다. 엔진은 하드웨어로 구현되거나 또는 소프트웨어 또는 펌웨어로 제어되는 하드웨어로 구현된다. 이와 같이, 엔진은 특정 동작을 수행하기 위해 특수하게 제안되는 유형의 엔티티이며 특 정 방식으로 구성된다. 일 예에서, 회로부는 하나의 엔진으로서 특정 방식으로 (예를 들어, 내부적으로 또는 다른 회로와 같은 외부 엔 티티와 관련하여) 배치될 수 있다. 일 예에서, 하나 이상의 하드웨어 프로세서의 전체 또는 일부는 특정 동작 을 수행하도록 동작하는 엔진으로서 펌웨어 또는 소프트웨어(예를 들어, 인스트럭션, 애플리케이션 부분, 또는 애플리케이션)에 의해 구성될 수 있다. 일 예에서, 소프트웨어는 유형의 머신 판독가능 저장 매체 상에 상주할 수 있다. 일 예에서, 소프트웨어는 엔진의 기본 하드웨어에 의해 실행될 때 하드웨어로 하여금 특정된 동작을 수행하게 한다. 따라서, 엔진은 본 명세서에 설명된 임의의 동작의 일부 또는 전부를 특정 방식으로 동작시키 거나 수행하도록, 물리적으로 구성되거나, 구체적으로 구성되거나(예를 들어, 하드와이어되거나), 또는 일시적으로 구성(예를 들어, 프로그래밍)된다. 엔진이 일시적으로 구성되는 예를 고려할 경우, 각각의 엔진은 어느 한 시점에 인스턴스화될 필요는 없다. 예 를 들어, 엔진이 소프트웨어를 사용하여 구성된 범용 하드웨어 프로세서 코어를 포함하는 경우, 범용 하드웨어 프로세서 코어는 상이한 시간에 제각기의 상이한 엔진으로서 구성될 수 있다. 따라서, 소프트웨어는 하드웨어 프로세서 코어를 구성하여, 예를 들어, 하나의 시간 인스턴스에서 특정 엔진을 구성하고 다른 시간 인스턴스에 서 상이한 엔진을 구성할 수 있다. 도시된 바와 같이, 이 시스템은 차량의 내부에 또는 차량 상에 탑재된 카메라 또는 이미지 센서를 포함한 다. 각각의 이미지 센서는 이미지 프로세서에 의해 판독되는 이미지 프레임을 제공하기 위 해 시야를 이미지화한다. 일 예에서, 하나 초과의 카메라가 차량 내에 탑재될 수 있다. 예를 들어, 이 시스템은 상이한 방향을 가리키는 다수의 카메라를 가질 수 있다. 시스템은 또한 차량에 대하여 동일하 거나 유사한 방향을 가리키는 다수의 카메라를 가질 수 있지만, 다른 위치에 탑재될 수 있다. 일 예에서, 시스템은 (예를 들어, 도 22 및 도 23과 관련하여 아래에서 설명되는 바와 같이) 부분적으로 또는 완전 히 중첩되는 시야를 갖는 다수의 카메라를 가질 수 있다. 일 예에서, 2 개의 병렬 카메라가 스테레오로 동작할 수 있다. 단일-카메라 시스템이 본 명세서에서 논의되지만, 관련 이미지 및 프레임의 일부 또는 전부가 상이한 카메라에 의해 캡처될 수 있거나, 다수의 카메라로부터 캡처된 이미지의 합성물로부터 생성될 수 있는 멀티-카 메라 시스템이 또한 사용될 수 있다. 본 맥락에서, 실시간 동작은, 시야 전체의 물체가 시야가 스캔되거나 캡 처되는 속도와 일치하는 속도로 탐지되도록, 인식할 수 없거나 공칭의 처리 지연으로 동작한다. 이미지 프로세서는 다수의 운전자 보조 시스템 또는 애플리케이션을 제공하기 위해 이미지 프레임 을 동시에 또는 병렬로 처리하는 데 사용될 수 있다. 이미지 프로세서는 카메라의 전방 시야에서 이미지 또는 이미지의 일부를 탐지하고 인식하기 위해 이미지 프레임을 처리하는 데 사용될 수 있다. 운 전자 보조 시스템은 스토리지의 온보드 소프트웨어 및/또는 소프트웨어 제어 알고리즘과 함께 특정 하드 웨어 회로부(미도시)를 사용하여 구현될 수 있다. 이미지 센서는 단색, 그레이스케일일 수 있거나, 또는 이미지 센서는 색상에 민감할 수 있다. 예로서, 이미지 프레임은 특징부 탐지 엔진, 트래픽 표지판 인식(TSR) 엔진, 전방 충돌 경고(FCW) 엔진, 및 도로의 수직 윤곽 또는 노면으로부터의 편 차의 수직 윤곽 탐지 엔진에 서빙하는 데 사용된다. 일 예에서, 이미지 프레임은 상이한 운전자 보조 애플리케이션들 사이에서 분할되고, 다른 경우에 이미지 프레임은 상이한 운전자 보조 애플리케이션 들 간에 공유될 수 있다. 일 예에서, 시스템은 노면의 평면(또는 바이 쿼드래틱(bi-quadratic)) 모델을 정확하게 추정하고, 평면(또는 바 이 쿼드래틱) 노면 모델로부터 작은 편차를 계산하여 다양한 표면 특징부를 탐지하거나 정량화하는 데 사 용된다. 본 맥락에서 용어 \"노면 모델\"은 노면의 평면 또는 바이 쿼드래틱 모델을 지칭한다. \"수직 윤곽\" 또 는 \"수직 편차\"라는 용어는 노면에 수직인 축을 따른 노면 모델로부터의 편차를 지칭한다. 일 예에서, 시스템은 호스트 차량에 탑재된 카메라를 사용하여 수직 윤곽과 같은 노면 형상의 모델(예를 들어, 형상)을 정확하게 검출하는 데 사용된다. 본 명세서에 제공된 시스템 및 방법을 사용하여, 범프 또는 홀, 스피드 범프, 경계석 또는 맨홀 커버와 같은 표면 특징부는 서브 픽셀 정확도(예를 들어, 1-2 센티미터의 정도)로 노면(예를 들어, 평면)으로부터의 수직 편차로서 측정되거나 모델링될 수 있다. 이들 기술은 전방, 측 방 또는 후방 카메라에 유사하게 적용될 수 있다. 감마 맵은 차량의 전방 또는 측방 및 후방의 주행 가 능한 구역을 결정하는 데 유용할 수 있다. 감마 맵은 표면 경사면이 너무 가팔라서 주행할 수 없는 장소를 결 정하기 위해 자체적으로 사용될 수 있거나, 미국 특허 공개 번호 제2018/0101177호에 기술된 바와 같이 그레이 스케일 또는 컬러 이미지 기반 시멘틱 자유 공간과 결합될 수 있으며, 그 전체가 본원에 참고로 포함된다. 도 로 평면의 감마 맵 또는 높이 맵은 첨예한 수직 가장자리 경계석, 부드럽게 경사진 경계석, 또는 갓길(예를 들 어, 도로가 낙하되는 지점)을 구별하는 데 사용될 수 있다. 그 후, 호스트 차량은 부드럽게 경사진 경계석으로 부터보다는 첨예한 경계석 또는 가장자리로부터의 거리가 더 크게 유지되도록 제어될 수 있다. 일 예에서, 시스템은 자율 주행 또는 반 자율 주행 운전에 영향을 주기 위해 차량의 전자 기계 액추에이터 시스 템에 대한 스로틀, 제동, 조향 또는 변속기 선택 커맨드를 생성하기 위한 하나 이상의 차량 제어 엔진을 구현하 는 차량 제어 프로세서를 더 포함할 수 있다. 차량 제어 프로세서는 엔진(2120 내지 2123)에 의해 생성된 다양한 머신 비전 평가에 관해 이미지 프로세서에 의해 공급된 다양한 출력을 수신할 수 있다. 도 22는 일 실시예에 따른 차량 상의 멀티-카메라 어레이를 도시한다. 도시된 바와 같이, 카메라(2212A 내지 2212F)는 (예를 들어, 후술되는 것과 같은) 시야를 제공하기 위해 차량 주위에 위치된다. 도 23은 일 실시예에 따른 멀티-카메라 어레이에 의해 캡처될 수 있는 시야의 예를 도시한다. 다수의 중첩된 시야(100A 내지 100F)가 도시되어 있다. 여기서, 노면은 모든 뷰에게 공통된다. 도 24는 일 실시예에 따른 수직 윤곽 탐지 엔진의 일 예의 블록도이다. 수직 윤곽 탐지 엔진은 전 처리기 엔진, 머신 러닝(machine-learning)(ML) 기반 윤곽 엔진 및 출력 추출 엔진을 포함 한다. 전 처리기 엔진은 원시 입력 데이터 세트를 수신하고, 원시 입력 데이터 세트를 처리하여 추가 처 리를 위해 ML 기반 윤곽 엔진으로 전달될 전처리된 입력 데이터 세트를 생성하도록 구성, 프로그래밍, 또 는 다른 방식으로 구성되며, 이들의 결과는 노면의 수직 윤곽의 측정치를 나타내는 출력이 된다. 원시 입력 데이터 세트는 차량 제어 시스템의 컴포넌트들로부터 제공될 수 있다. 예를 들어, 에고 모션 엔진 , 접지면 엔진, 또는 데이터 저장소는 각각 다양한 원시 데이터 항목을 제공할 수 있다. 특 히, 본 맥락에서, \"원시 데이터\"라는 용어는 전 처리기 엔진의 입력부 측 상의 정보를 지칭한다. 원시 데이터는, 센서(예를 들어, 하나 이상의 카메라)로부터 직접 제공되거나 다른 방식으로 처리되지 않은 것으로 제한되는 것이 아니다. 오히려, 원시 데이터는 그 소스 또는 중간 처리 엔티티에 의해 어느 정도 계산적으로 처리될 수 있다. 에고 모션 엔진은 센서의 실제 모션을 기술하는 에고 모션(EM) 정보를 제공하도록 구성된다. EM 정보는 차량의 속도계 또는 관성 센서(예를 들어, 차량의 요, 피치 및 롤을 감지하도록 구성된 3 축 가속도계)에 의해 획득되는 측정치에 기반할 수 있다. 관성 센서는 카메라에 통합되거나 호스트 차량 상의 또 는 내부의 다른 곳에 탑재될 수 있다. 접지면 엔진은 접지면(ground plane)(GP) 정보를 제공하도록 구성된다. 일 예에서, 접지면 엔진 은 에고 모션 엔진에 의해 제공된 피치 및 롤 데이터, 차량 서스펜션 정보, 또는 이들 입력의 일부 조합에 기반하여 GP 정보를 결정할 수 있다. GP 정보는 도로의 평면 법선 벡터 및 평면까지의 거 리를 나타낼 수 있다. 일 예에서, 접지면 엔진은 3D 스캐닝 측정치(예를 들어, LiDAR), 입체 이미징, 또 는 다른 적절한 기술(들)에 기반하여 평면 법선 벡터 및 평면까지의 거리를 계산할 수 있다. 일 예에서, 접지 면 엔진은 캡처된 이미지에 기반하여 GP 정보를 계산할 수 있거나, 고정된 디폴트 GP일 수 있다. 데이터 저장소는 카메라 높이 정보와 같은 저장된 데이터 항목, 및 캡처된 이미지을 제공하 며, 이 캡처된 이미지는 연속적으로 캡처된 이미지 프레임의 시퀀스를 포함할 수 있다. 캡처된 이미지 는, 예를 들어, 현재(가장 최근에 캡처된) 이미지, 이전 이미지, 및 이전-이전 이미지를 포함할 수 있다. 전 처리기 엔진은, 캡처된 이미지, 및 EM, GP 정보, 또는 카메라 높이 정보와 같은 다른 원시 데이터 입력 항목에 기반한 전처리된 이미지들의 세트를 출력한다. 또한, 전 처리기 엔 진은 ML 기반 윤곽 엔진에 의한 추가 처리를 위해, 이미지 포맷된 에피폴 표시 및 이미지 포 맷된 모션 표시를 출력한다. 일 예에서, 도로 구조는 현재 캡처된 이미지의 각 픽셀에 대해 (z 축을 따른) 전진방향 모션의 방향을 따 라 카메라로부터의 거리에 대한 노면 높이의 비율을 나타내는 맵이며, 이 맵은 본 명세서에서 \"감마\"로 지칭된 다. 일 예에서, 감마는 전처리된 이미지들의 세트 중 임의의 이미지와 동일하거나 유사한 해상도를 가질 수 있거나, 상이한 (예를 들어, 감소된) 해상도를 가질 수 있다. 일 예에서, ML 기반 윤곽 엔진은 전처리된 이미지들의 세트, 이미지 포맷된 에피폴 표시, 및 이미지 포맷된 모션 표시를 판독하고, 전 처리기 엔진의 이러한 이미지 포맷된 출력을 처리하여, 노면의 3 차원 구조를 나타내는 이미지 포맷된 맵인 도로 구조를 생성한다. 도로 구조는 도로의 수직 윤곽을 나타낸다. 그것은 노면 상의 포인트들에 대한 높이 정보를 포함할 수 있거나, 또는 추가 처리로부 터 높이 정보가 계산적으로 결정될 수 있는 다른 값들을 포함할 수 있다. 일 예에서, ML 기반 윤곽 엔진은 도로의 수직 윤곽을 계산하기 위해 트레이닝된 심층 신경 네트워크(deep neural network)(DNN)를 포함한다. DNN은, 가중치, 바이어스, 또는 각 계층에 특정된 다른 변수 설정치를 포함 하는 계산 파라미터에 기반하여 복수의 계층에서 다양한 단계적 동작을 수행한다. 계산 파라미터 는 DNN 트레이닝 시스템에 의해서와 같이, 오프라인으로 또는 원격으로 수행될 수 있는 DNN 트레이닝 동 작에 기반하여 수립되고 때때로는 업데이트될 수 있다. 출력 추출 엔진은 도로 구조의 감마( ) 값을 처리하여, 예를 들어, 카메라 높이 정보 및 GP를 사용하여 도로의 평면으로부터 수직 높이를 나타내는 높이 맵을 생성하도록 구성된다. 일 예 에서, 출력 추출 엔진은 다음에 기반하여 도로 구조로부터 잔차 모션(μ)을 계산하도록 구성 된다:"}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "여기서, 용어는 감마이며, 는 전진 방향으로의 병진 이동을 나타내고, 는 카메라 높이를 나타내고, 는 에피폴 정보를 나타내고, 는 호모그래피 기반 와핑의 적용 후 대응하는 이미지 좌표를 나타내는 용어이 다. 일 예에서, DNN은 감마 값의 픽셀 단위 출력을 생성하며, 여기서, 감마는 (z 축을 따른) 전진방향 모션의 방향 을 따라 카메라로부터의 거리에 대한 수직 윤곽 높이의 계산된 비율이다. 수직 윤곽 높이는 이미지 내의 대응 하는 위치에 대한 제각기의 감마 값으로부터 결정될 수 있다. 일 예에서, 보도와 같은 도로 경계 특징부는 높이( )로부터 직접 탐지될 수 있다. 일 예에서, 도로 경계 특징 부는 긴 라인에 의해 낮은 잔차 흐름의 구역으로부터 분리되는 비교적 균질한 포지티브 잔차 흐름 영역을 탐지 함으로써 탐지될 수 있다. 이들 라인은 확장 초점(focus of expansion)(FOE) 방향 또는 도로 방향(예를 들어, 차선 마크의 소실 포인트들의 방향)으로 대략 향하고 있다. 일 예에서, 범프 또는 장애물은 높이( )로부터 직접 유사하게 탐지될 수 있거나 또는 낮은 잔차 흐름의 구역에 의해 적어도 부분적으로 둘러싸인 포지티브 잔차 흐름의 영역으로서 탐지될 수 있다. 일 예에서, 홀 또는 함몰 부는, 한편으로는, 낮은 잔차 흐름의 구역에 의해 적어도 부분적으로 둘러싸인 네거티브 잔차 흐름의 영역으로 서 탐지될 수 있다. 일 예에서, 퍼들은 (예를 들어, 감마 이미지 또는 결정된 도로 경계 내부 영역의 위치로부터 결정될 수 있는) 이미지에서의 수평 표면의 분석에 기반하여 탐지된다. 퍼들을 결정하는 데 사용될 수 있는 일부 기준은 깊은 함몰부 또는 홀을 나타내는 상당한 잔차 흐름을 갖는 표면 상에 첨예한 가장자리의 존재에 대한 테스트를 포함 한다. 일 예로서, DNN 기반 구현예에서, 의심되는 퍼들 영역의 가장자리는 감마 와핑된 이미지에서 정지된 가 장자리 특징부에 대해 분석될 수 있다. 의심되는 퍼들 영역은 사진 측량 제약 조건만을 사용할 때 (예를 들어, 네거티브 방향으로 정의된 임계치를 초과하는) 큰 네거티브 감마 값을 갖는 것으로 식별될 수 있다. 일 예에서, 전처리된 (예를 들어, 와핑된, 정렬된, 및 안정화된) 이미지가 획득된다. 이러한 전처리된 이미지 는 도로 프로파일링 동작의 일부로서 생성되었을 수 있으며, 특히 퍼들 탐지용으로 생성된 것은 아닐 수 있다. 여기서, 전처리된 이미지의 이용 가능성은 이미지를 새로 전처리해야 하는 시스템에 비해 특징부 탐지를 위한 계산 효율 및 탐지 속도의 개선을 가능하게 한다. 도 25는 일 실시예에 따른 전 처리기(preprocessor) 엔진의 일 예를 도시한다. 도시된 바와 같이, 전 처리기 엔진은 참조 프레임 엔진, 호모그래피 엔진, 이미지 와핑 엔진, 에피폴 이미지 생성기 엔진, 및 모션 정보 이미지 생성기 엔진을 포함한다. EM 정보는 참조 프레임 엔진 및 모션 정보 이미지 생성기 엔진에 입력으로서 공급된다. 이 예에서, EM 정보는 차량의 모션 감지 디바이스로부터 제공되거나 도출되는 회전(R) 정보 및 병진 이동(T) 정보를 포함한다. GP 정보는 참조 프레임 엔진에 제공된다. 도시된 바와 같이, GP 정보는 도로 평면의 디폴트 평면 법선 벡터 표시기(N2516)를 포함한다. 캡처된 이미지는 참조 프레임 엔진, 호모그래피 엔진, 및 이미지 와핑 엔진에 제공된 다. 캡처된 이미지는 시야에서 동일한 도로 부분으로부터 캡처된 이미지 프레임들의 시퀀스(예를 들어, 제 1 이미지 프레임(A), 제 2 이미지 프레임(B) 및 제 3 이미지 프레임(C))를 포함한다. 일 예에서, 이 미지 프레임들의 시퀀스는 2 개 만큼의 이미지 프레임들, 3 개 이미지 프레임들, 또는 3 개보다 많은 이 미지 프레임들을 가질 수 있다. 카메라 높이 정보는 참조 프레임 엔진 및 모션 정보 이미지 생성기 엔진에 제공된다. 일 예 에서, 카메라 높이 정보는 공칭 카메라 높이를 나타내는 고정된 값을 포함한다. 일 예에서, 카메라 높이 정보는 차량의 서스펜션 상태와 같은 변수를 고려하는 현재 카메라 높이를 보고하는 동적 표시기 를 포함한다. 일 예에서, 참조 프레임 엔진은 EM 정보에 기반하여 GP 정보로부터의 접지면 표시를 접지면 의 업데이트된 표현으로 조정하도록 구성된다. 이는 안정화된 참조면에 대한 도로 프로파일 출력이 필요한 경 우에 중요할 수 있다. 일 예로서, 접지면의 평면 법선 벡터는 다음에 따라 조정된다:"}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, N(t)는 방향 및 크기 정보와 함께 업데이트된 평면 법선 벡터를 나타낸다. 예를 들어, N(t)는 평면 법 선 벡터(N)를 카메라 높이(즉, 본 명세서에서 camH로도 지칭되는 접지면(D)까지의 거리)로 나눈 값을 나타낼 수 있다. 일 예에서, N과 D는 동일한 , , 및 계수를 사용하여 개별적으로 업데이트될 수 있다:"}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "호모그래피 엔진은 한 쌍의 이미지 프레임들 사이의 호모그래피를 계산하도록 구성된다. 본 문맥 에서 용어 \"호모그래피\"는 직선 라인을 직선 라인으로 매핑하는 투영 공간에서 그 자체로의 반전 가능한 변환을 지칭한다. 컴퓨터 비전 분야에서, 공간에서 동일한 평면 표면의 두 이미지는 (핀홀 카메라 모델을 가정할 때) 호모그래피에 의해 관련된다. 주어진 호모그래피는 (어레이 또는 다른 적절한 데이터 구조로 실현된) 매트릭스 로서 계산적으로 표현되고 처리될 수 있다. 일 예로서, 제 1 호모그래피(HAB)는 도로의 제 1 이미지 프레임( )을 도로의 제 2 이미지 프레임( )으로 변환하 는 것을 나타내며, 호모그래피 엔진에 의해 계산된다. 제 1 호모그래피(HAB)는 제 1 이미지 프레임( )의 도로의 매칭하는 이미지 포인트와 제 2 이미지 프레임( )의 도로의 대응하는 이미지 포인트의 세트로부터 계산 될 수 있다. 일 예에서, 호모그래피( )는 에고 모션 및 접지면 정보에 기반하여 다음과 같이 표현될 수 있다:"}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "여기서, 과 는 제각기 (예를 들어, 프레임( )으로부터 프레임( )으로의) EM 2554로부터의 회전 및 병진 이 동을 나타내며, camH는 카메라 높이 정보로부터의 카메라 높이를 나타내고, 는 GP 2558로부터의 평면 법선 벡터를 나타내며, 및 는 캘리브레이션 요소를 나타낸다. 제 2 호모그래피(HBC)는 도로의 제 2 이미지 프레임( )을 도로의 제 3 이미지 프레임( )으로 변환하는 것을 나 타내며, 호모그래피 엔진에 의해, 제 2 이미지 프레임( )의 도로 상의 매칭하는 이미지 포인트와 제 3 이미지 프레임( )의 도로의 대응하는 이미지 포인트로부터 계산될 수 있다. 호모그래피 엔진은 제 1 및 제 2 호모그래피(HAB 및 HBC)를, 예를 들어, 행렬 곱셈에 의해 추가로 체인화 할 수 있다. 체인화된 호모그래피를 초기 추측치로서 사용함으로써, 도로의 제 1 이미지를 도로의 제 3 이미지 로 변환하는 것을 나타내는 제 3 호모그래피(HAC)가 계산될 수 있다. 일 예에서, 호모그래피 엔진은 이미지 프레임들 사이의 에고 모션을 사용하여 호모그래피를 계산한다. 예를 들어, 제 1 이미지 프레임(A)과 제 2 이미지 프레임(B) 사이에서 측정된 회전(RAB로 표시됨)은 제 1 이미 지 프레임(A)과 제 2 이미지 프레임(B) 사이의 변환(TAB)과 함께, 제 2 이미지 프레임(B)과 제 3 이미지 프레임 (C) 사이에서 제각기 회전(RBC로 표시됨) 및 병진 이동(TBC로 표시됨)과 함께 체인화된다. 이러한 에고 모션 체인화는 제 1 이미지 프레임(A)과 제 3 이미지 프레임(C) 사이의 회전(RAC로 표시됨) 및 병진 이동(TAC로 표시 됨)의 결정을 생성한다. 호모그래피 엔진은 에고 모션(RAB 및 TAB)을 사용하여 평면 법선 벡터(NBC)를 업 데이트하며, 이는 프레임(C)의 좌표 프레임에서 프레임(B 및 C)을 사용하여 계산된 평면 법선이다. 호모그래피 엔진은 제 2 및 제 3 이미지 프레임(B 및 C)에 공통된 안정화된 참조면을 나타낸다. 호모그래피 엔진 은 3 개의 이미지(A, B 및 C)의 시퀀스와 관련된 모든 호모그래피에 이용 가능한 안정화된 참조 프레임을 나타내는 업데이트된 평면 법선 벡터(예를 들어, NBC)를 결정한다. 다음으로, 호모그래피 엔진은 회전 (RAC), 병진 이동(TAC), 및 평면 법선(NAC)에 기반하여, 제 1 이미지 프레임(A)와 제 3 이미지 프레임(C) 사이의 이미지 프레임의 천이를 위한 호모그래피(HAC)를 구성한다. 일 예에서, 사전 와핑 후의 강력한 추적 기능은 호모그래피를 계산하는 데 적용된다. 이것은 이전 평면 추정치 를 조정하기 위해 에고 모션 및 평면 추정치를 도출하는 데 사용된다. 에고 모션과 조정된 평면 모델이 결합되 어 호모그래피를 획득하게 된다. 일 예로서, 에고 모션은 에고 모션 센서 및 처리 엔진에 의해 제공될 수 있다. 이러한 타입의 엔진은 필수 행렬을 사용하여 도로 상의 포인트 및 도로 위의 포인트의 강력한 추적 기능 을 사용한다. 이 엔진은 또한 사용 가능한 임의의 관성 센서 및 속도계 정보를 결합한다. 접지면 정보는 접지 면 엔진에 의해 제공된다. 이 엔진은 (사전 와핑 후) 도로 상의 포인트를 추적하고 그 후 계산된 에고 모션을 사용하여 각 포인트에 대한 심도를 도출하며, 이 심도는 포인트들의 3D 좌표를 생성한다. 그 후, 평면 표면은 3D 포인트들에 알고리즘적으로 적합화된다. 에고 모션과 평면 표면이 결합되어 호모그래피를 제공하게 된다. 일 예에서, 노면의 제각기의 파라메트릭 모델이 구현될 수 있다. 예를 들어, 3D 스플라인 모델은 통신가능하게 연결된 주행 경로의 기하학적 구조 또는 물체 감지 서브 시스템에 의해 제공될 수 있다. 하나의 그러한 서브 시스템의 일 예로서, Intel Corporation의 자회사인 Mobileye가 제조한 Road Experience Management™ (REM™) 엔진은 3D 스플라인 형태의 도로 특징부의 표현을 제공할 수 있다. 예를 들어, 차선 마크는 3D 스플라인으로 표시될 수 있다. 서브 시스템은 3D 스플라인 특징부 표현을 이미지에 정렬한다. 3D 스플라인 특징부 표현은 추가 처리를 위해 도로의 캡처된 이미지 상으로 다시 투영될 수 있다. 3D 스플라인의 각각의 포인는 3D 좌표를 갖는다. 일 예에서, 이 데이터는 캡처된 이미지의 행을 따라 3D 스플 라인과 연관된 3D 위치를 보간 및 외삽하기 위해 사용될 수 있다. 측면들에 대한 외삽은 0 차 홀드(즉, 상수), 1 차 홀드(예를 들어, 2 개의 스플라인에 기반한 선형 확장) 등일 수 있다. 일 예에서, 보간은 이미지 자체의 외부에 있는 가상 이미지 좌표로의 특정 스플라인의 알려진 확장으로 발생할 수 있다. 보간 및 외삽은 이미지의 하부에 있는 모든 포인트에 대해 3D 좌표를 생성할 수 있다. 이들 포인트들은 매끄러 운 3D 표면 상에 있을 수 있다. 결과적으로, 표면은 이전 이미지를 현재 이미지로 (또는 그 반대로) 와핑하는 데 사용될 수 있다. (예를 들어, 도 22 및 도 23을 참조하여 설명된 바와 같이) 다수의 중첩 뷰에 액세스하는 예에서, 호모그래피는 시야의 조합을 사용하여 호모그래피 엔진에 의해 계산될 수 있다. 예를 들어, 전방 좌측 시야(2300A)에 서 평면 노면의 중첩 영역들을 정렬시키는 (시간(t2)에서의) 전방 중심 시야(2300B)로의 호모그래피가 획득될 수 있으며, 이 호모그래피는 전방 좌측 시야(2300C)의 이미지를 전방 중심 시야(2300B)의 이미지로 와핑하는 데 사용될 수 있다. 또한, (단안의 경우에서와 같이) 시간(t1)에서의 전방 중심 시야(2300B)의 이미지와 시간(t 2)에서의 전방 중심 시야(2300B)의 이미지 간의 호모그래피가 계산될 수 있다. 또한, (시간(t1)에서의) 전방 좌측 시야(2300A)의 이미지에서 (또한 시간(t1)에서의) 전방 중심 시야(2300B)의 이미지로의 호모그래피가 계산 될 수 있다. 이 호모그래피를 사용하여, (시간(t1)에서의) 전방 좌측 시야(2300A)의 이미지는 (시간(t2)에서의) 전방 중심 시야(2300B)의 이미지와 정렬되도록 체인 와핑(chain-warped)될 수 있다. 일 예에서, (시간(t1)에서의) 전방 좌측 시야(2300A)의 이미지와 (또한 시간(t1)에서의) 전방 중심 시야(2300 B)의 이미지 간의 호모그래피는, 초점 거리 및 렌즈 왜곡과 같은 각 카메라의 내부 캘리브레이션 파라미터와 함 께, (시간(t1)에서의) 전방 중심 시야(2300A)의 이미지와 (시간(t2)에서의) 전방 중심 시야의 이미지 간의 호모 그래피용으로 사용되는 평면 법선과 전방 좌측 카메라(2212A) 및 전방 중심 카메라(2212B)의 알려진 위치(외부 캘리브레이션)로부터 도출된다. 이 기술은 글로벌 셔터 카메라(global-shutter cameras)와 함께 사용될 수 있지만 롤링 셔터 카메라 또는 비동 기화된 카메라의 경우에 오차를 도입시킬 수도 있다. 후자의 경우에, 2 개의 카메라의 상대적인 위치는, 중첩 하는 노면들 간의 3D 포인트 및 추적 포인트를 사용하고 호모그래피를 계산하여, 이미지로부터 도출될 수 있다.정렬용으로 사용되는 호모그래피는 일반적으로 일관된 접지면을 사용하기 때문에, 추적으로부터의 호모그래피가 상대적인 모션을 제공하도록 분해될 수 있고, 이러한 모션 및 일관된 접지면 법선을 사용하여 새로운 호모그래 피가 구성될 수 있다. 이미지 와핑 엔진은 3 개의 이미지 프레임 중 2 개에 대해 와핑 동작을 수행하도록 구성된다. 예 를 들어, 제 1 이미지 프레임(A)은 제 3 이미지 프레임(C)으로 와핑되고, 제 2 이미지 프레임(B)은 제 3 이미지 프레임(C)으로 와핑된다. 이 예에서, 이미지 프레임(C)은 현재 이미지를 나타낼 수 있고, 이미지 프레임(B)은 이전 이미지를 나타내고, 이미지 프레임(A)은 이전-이전 이미지를 나타낸다. 에피폴 이미지 생성기 엔진은 하나 이상의 이미지의 포맷으로 에피폴 위치 데이터를 생성하도록 구 성된다. 에피폴은 전진방향의 모션의 방향을 나타내는 벡터이다. 일 예에서, 이미지 포맷된 에피폴 위치 데이 터는 이미지 프레임(A, B 및 C)과 동일하거나 유사한 해상도를 갖는 한 쌍의 이미지를 포함한다. 에피폴 위치 데이터를 나타내는 이미지 쌍 중 제 1 이미지는 x 축을 따라 에피폴로부터 제각기의 거리를 나타내는 \"픽 셀\"을 포함한다. 에피폴 위치 데이터를 나타내는 이미지 쌍의 제 2 이미지는 y 축을 따라 에피폴로부터 제각기 의 거리를 나타내는 \"픽셀\"을 포함한다. 모션 정보 이미지 생성기 엔진은 차량의 측정된 모션을 나타내는 이미지 포맷된 모션 표시를 생성 하도록 구성된다. 이미지 포맷된 모션 표시는, 예를 들어, 에피폴 위치 데이터와 동일한 치수를 가질 수 있다. 이미지 포맷된 모션 표시의 내용은 차량 모션을 나타내는 고정된 값을 갖는 \"픽셀\"을 포 함할 수 있다. 일 예에서, 이미지 포맷된 모션 표시에서의 차량 모션은 EM에 기반할 수 있다. 일 예에서, 이미지 포맷된 모션 표시에서의 차량 모션은 카메라 높이 정보에 더 기반한다. 일 예에서, Tz/D로 표현된, 카메라 높이에 대한 전진방향(z 축)에 따른 현재의 병진이동 측정치의 비율은 이미지 포맷된 데이터 구조에서 이미지의 각 \"픽셀\"에 대한 상수 값으로서 제공된다. 도 26은 일 실시예에 따른 카메라를 이용한 차량 환경 모델링 방법의 일 예의 흐름도이다. 방법의 동작은 전술한 것 또는 후술되는 것과 같은 계산 하드웨어(예를 들어, 처리 회로부)에서 구현된다. 동작에서, 노면을 나타내는 시간 순서의 이미지 시퀀스가 획득된다. 여기서, 이미지 시퀀스 중 하나의 이미지는 현재 이미지를 포함한다. 동작에서, 데이터 세트는 장면의 3 차원 구조를 생성하기 위해 ANN에 제공된다. 일 예에서, 데이터 세트 는 이미지 시퀀스의 일부를 포함하며, 이미지 시퀀스의 일부는 현재 이미지, 이미지를 캡처한 센서의 모션, 및 에피폴을 포함한다. 일 예에서, 이미지 시퀀스의 일부는 현재 이미지에 바로 선행하는 이미지를 포함한다. 일 예에서, 이미지 시퀀스의 일부는 총 3 개의 이미지이다. 일 예에서, 이미지 시퀀스의 일부는 이미지 시퀀스에 서 하나 이상의 이미지에 의해 분리된 현재 이미지에 선행하는 이미지를 포함한다. 일 예에서, 에피폴은 현재 이미지와 동일한 차원을 갖는 그래디언트 이미지로서 제공된다. 여기서, 그래디언트 이미지의 픽셀의 값은 현재 이미지의 픽셀의 에피폴로부터의 거리를 나타낸다. 일 예에서, 베이스라인은 적어 도 0.5m 이다. 일 예에서, 그래디언트 이미지는 현재 이미지보다 낮은 해상도이다. 일 예에서, 그래디언트 이 미지는 에피폴로부터의 수평 거리만을 나타낸다. 일 예에서, 에피폴로부터의 수직 거리를 나타내기 위해 제 2 그래디언트 이미지가 ANN에 제공된다. 일 예에서, 센서의 모션은 현재 이미지와 동일한 차원을 갖는 상수 값 이미지로서 제공된다. 일 예에서, 상수 값은 평면으로부터 센서의 높이에 의한 센서의 전진방향 모션의 비율이다. 일 예에서, 장면의 3 차원 구조는 감마 이미지이다. 여기서, 감마 이미지는 현재 이미지를 캡처하는 센서로부 터의 거리에 의한 평면 위의 포인트의 높이의 비율인 감마 값을 갖는 픽셀을 포함하며, 상기 평면은 노면을 나 타낸다. 일 예에서, ANN은 컨볼루션 신경 네트워크(CNN)이다. 일 예에서, 센서의 모션 및 에피폴은 병목 계층에서 CNN 에 제공된다. 일 예에서, ANN은 미래 이미지의 모델과 미래 이미지 간의 차이를 측정함으로써 오차를 결정하는 비 지도 트레 이닝 기술을 이용하여 트레이닝된다. 여기서, 미래 이미지의 모델은 미래 이미지 이전의 이미지의 감마 와핑 (gamma warping)을 통해 생성된다. 일 예에서, ANN은 위치에 대한 예측된 감마와 그 위치에서의 센서 움직임 간의 차이를 측정함으로써 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 센서 움직임 은 피치, 요, 롤, 또는 상기 평면에 수직인 병진이동을 포함한다. 일 예에서, ANN은 두 개의 상이한 시간에 두개의 이미지 간의 중첩하는 세그먼트의 감마의 차이에 의해 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝된다. 여기서, 제 1 이미지에 대해 추론이 수행되고, 중첩 세그먼트는 제 2 이미지에서 센서에 더 가 깝다. 동작에서, 노면은 장면의 3 차원 구조를 사용하여 모델링된다. 일 예에서, 노면을 모델링하는 것은 장면 의 3 차원 구조를 제 2 ANN으로부터의 출력과 비교함으로써 반사성 구역을 식별하는 것을 포함한다. 여기서, 제 2 ANN은 이미지 시퀀스의 일부를 수용하고 제 2의 3 차원 구조를 생성하도록 트레이닝된다. 제 2 ANN의 트 레이닝은 제 1 ANN을 트레이닝하는 것보다 이미지 시퀀스의 일부에서 더 많은 사진 측량 손실을 수반했다. 일 예에서, ANN 및 제 2 ANN은 2 채널 출력을 생성하도록 트레이닝된 단일 ANN으로서 구현된다. 여기서, 제 1 채 널은 장면의 3 차원 구조이고, 제 2 채널은 제 2 ANN에 의해 생성된 3 차원 구조이며, 제 2 ANN은 그 트레이닝 에서 더 많은 사진 측량 손실을 사용했다. 일 예에서, 노면을 모델링하는 것은 노면 특징부의 평면으로부터의 수직 편차를 계산하는 것을 포함한다. 일 예에서, 방법은 특징부들이 노면의 환경 내에서 움직이거나 움직이지 않는 물체를 나타내는지를 결정 하기 위해 3 차원 구조를 사용하여 제 2 ANN을 호출하는 것을 포함하도록 확장된다. 일 예에서, 3 차원 구조를 사용하여 제 2 ANN을 호출하는 것은 제 2 ANN에, 현재 이미지, 3 차원 구조를 사용하여 와핑된 이전 이미지, 및 타겟 식별자를 제공하는 것을 포함한다. 일 예에서, 타겟 식별자는 이미지의 픽셀이 타겟의 중심으로부터의 거 리를 나타내는 이미지이다. 일 예에서, 타겟 식별자는 타겟의 사이즈를 포함한다. 일 예에서, 타겟 식별자는 타겟에 대응하는 픽셀의 마스크이다. 도 27은 본 명세서에서 논의된 하나 이상의 기술(예를 들어, 방법)이 수행될 수 있는 예시적인 머신의 블 록도를 도시한다. 본 명세서에 설명된 예는 머신의 로직 또는 다수의 컴포넌트 또는 메커니즘을 포함할 수 있거나 또는 이에 의해 동작할 수 있다. 회로부(예를 들어, 처리 회로부)는 하드웨어(예를 들어, 단순 회로, 게이트, 로직 등)를 포함하는 머신의 유형의 엔티티에서 구현되는 회로의 집합체이다. 회로부 멤 버십은 시간 경과에 따라 유연할 수 있다. 회로부는 작동시 특정 동작을 단독으로 또는 조합하여 수행할 수 있 는 멤버를 포함한다. 일 예에서, 회로부의 하드웨어는 특정 동작을 수행하도록 불변적으로 설계될 수 있다(예 를 들어, 하드 와이어드될 수 있다). 일 예에서, 회로부의 하드웨어는 특정 동작의 인스트럭션을 인코딩하도록 물리적으로 수정된 머신 판독 가능 매체(예를 들어, 불변의 질량 입자의 자기적으로, 전기적으로, 이동 가능한 배치, 등)를 포함한 가변적으로 연결되는 물리적 컴포넌트들(예를 들어, 실행 유닛, 트랜지스터, 간단한 회로, 등)을 포함할 수 있다. 물리적 컴포넌트들을 연결할 때, 하드웨어 구성 요소의 기본적인 전기적 특성이, 예를 들어, 절연체에서 도체로 또는 그 반대로 변경된다. 인스트럭션은 동작시 내장된 하드웨어(예를 들어, 실행 유 닛 또는 로딩 메커니즘)로 하여금 가변적인 연결을 통해 하드웨어에서 회로부의 멤버를 생성하게 하여 특정 동 작의 일부를 수행하게 한다. 따라서, 일 예에서, 머신 판독 가능 매체 요소는 회로부의 일부이거나 디바이스가 동작할 때 회로부의 다른 컴포넌트에 통신 가능하게 결합된다. 일 예에서, 임의의 물리적 컴포넌트는 하나 초 과의 회로부의 하나 초과의 멤버에 사용될 수 있다. 예를 들어, 동작 하에서, 실행 유닛은 한 시점에서 제 1 회로부의 제 1 회로에서 사용될 수 있고, 제 1 회로부의 제 2 회로에 의해, 또는 다른 시간에 제 2 회로부의 제 3 회로에 의해 재사용될 수 있다. 머신과 관련하여 이러한 컴포넌트의 추가적인 예가 이어진다. 일 예에서, 머신은 독립형 디바이스로서 동작할 수 있거나 또는 다른 머신에 연결(예를 들어, 네트워크화)될 수 있다. 네트워크 배치에서, 머신은 서버-클라이언트 네트워크 환경에서 서버 시스템, 클라이언트 시스템 또는 둘 다의 자격으로 동작할 수 있다. 예를 들어, 머신은 피어-투-피어(peer-to- peer)(P2P)(또는 다른 분산) 네트워크 환경에서 피어 시스템으로서 동작할 수 있다. 머신은 퍼스널 컴퓨 터(PC), 태블릿 PC, 셋탑 박스(STB), PDA(personal digital assistant), 모바일 텔레폰, 웹 기기, 네트워크 라 우터, 스위치 또는 브릿지일 수 있거나, 또는 머신에 의해 취해질 액션을 특정하는 인스트럭션을 (순차적으로 또는 다른 방식으로) 실행할 수 있는 임의의 머신일 수 있다. 또한, 단일 머신만이 도시되어 있지만, \"머신\"이 라는 용어는 본 명세서에서 설명한 임의의 하나 이상의 방법을 수행하는 인스트럭션의 세트(또는 다수의 세트) 를 개별적으로 또는 공동으로 실행하는 머신의 임의의 집합체, 예를 들어, 클라우드 컴퓨팅, SaaS (software as a service), 다른 컴퓨터 클러스터 구성을 포함하는 것으로 또한 간주될 것이다. 머신(예를 들어, 컴퓨터 시스템)은 하드웨어 프로세서(예를 들어, 중앙 처리 유닛(CPU), 그래픽 처 리 유닛(GPU), 하드웨어 프로세서 코어 또는 이들의 임의의 조합), 메인 메모리, 정적 메모리(예를 들어, 펌웨어, 마이크로 코드, 기본 입출력(BIOS), 통합 확장 가능 펌웨어 인터페이스(UEFI) 등을 위한 메모리 또는 스토리지), 또는 대용량 스토리지(예를 들어, 하드 드라이브, 테이프 드라이브, 플래시 스토리지, 또는 다른 블록 디바이스)를 포함할 수 있고, 이들의 일부 또는 모두는 인터링크(예를 들어, 버스)를 통해 서로 통신할 수 있다. 머신은 디스플레이 유닛, 영숫자 입력 디바이스(예를 들어, 키보 드), 및 사용자 인터페이스(UI) 내비게이션 디바이스(예를 들어, 마우스)를 더 포함할 수 있다. 일 예에 서, 디스플레이 유닛, 입력 디바이스 및 UI 내비게이션 디바이스는 터치 스크린 디스플레이 일 수 있다. 머신은 스토리지 디바이스(예를 들어, 드라이브 유닛), 신호 생성 디바이스(예 를 들어, 스피커), 네트워크 인터페이스 디바이스, 및 하나 이상의 센서(예를 들어, GPS 센서, 나 침반, 가속도계, 또는 다른 센서)를 추가로 포함할 수 있다. 머신은 출력 컨트롤러, 예를 들어, 하나 이상의 주변 디바이스(예를 들어, 프린터, 카드 리더 등)를 통신 또는 제어하기 위한 직렬(예를 들어, 범 용 직렬 버스(USB), 병렬, 또는 다른 유선 또는 무선(예를 들어, 적외선(IR), 근거리 통신(NFC) 등)) 연결을 포 함할 수 있다. 프로세서의 레지스터, 메인 메모리, 정적 메모리, 또는 대량 스토리지는 본 명세서에 서 기술된 임의의 하나 이상의 기술 또는 기능을 구현하거나 이에 의해 구현되는 하나 이상의 데이터 구조 또는 인스트럭션의 세트)(예를 들어, 소프트웨어)가 저장되어 있는 머신 판독 가능한 매체일 수 있거나 이를 포함할 수 있다. 인스트럭션은 또한 머신에 의한 실행 동안 프로세서의 임의의 레지스 터, 메인 메모리, 정적 메모리, 또는 대용량 스토리지 내에 완전히 또는 적어도 부분적으로 상주할 수 있다. 일 예에서, 하드웨어 프로세서, 메인 메모리, 정적 메모리, 또는 대용량 스토리지의 하나 또는 임의의 조합은 머신 판독 가능 매체를 구성할 수 있다. 머신 판독 가능 매 체가 단일 매체로서 도시되고 있지만, \"머신 판독 가능 매체\"라는 용어는 하나 이상의 인스트럭션 을 저장하도록 구성된 단일 매체 또는 다중 매체(예를 들어, 중앙 집중식 또는 분산형 데이터베이스, 및/또는 연관된 캐시 및 서버)를 포함할 수 있다. \"머신 판독 가능 매체\"라는 용어는 머신에 의한 실행을 위한 인스트럭션을 저장, 인코딩 또는 전달할 수 있고, 머신으로 하여금 본 개시 내용의 임의의 하나 이상의 기술을 수행하게 하거나 또는 그 인스트럭션 에 의해 이용되거나 그 인스트럭션과 연관되는 데이터 구조를 저장, 인코딩 또는 전달할 수 있는 임의의 매체를 포함할 수 있다. 비 제한적인 머신 판독 가능 매체 예는 고체 상태 메모리, 광학 매체, 자기 매체, 및 신호(예 를 들어, 무선 주파수 신호, 다른 광자 기반 신호, 사운드 신호 등)를 포함할 수 있다. 일 예에서, 비 일시적 머신 판독 가능 매체는 불변의(예를 들어, 정지) 질량을 갖는 복수의 입자를 구비한 머신 판독 가능 매체를 포 함하므로, 물질의 조성물이 된다. 따라서, 비 일시적 머신 판독 가능 매체는 일시적 전파 신호를 포함하지 않 는 머신 판독 가능 매체이다. 비 일시적 머신 판독 가능 매체의 특정 예는 비 휘발성 메모리, 예를 들어, 반도 체 메모리 디바이스(가령, 전기 프로그래밍 가능 판독 전용 메모리(EPROM), 전기 소거 가능 프로그래밍 가능 판 독 전용 메모리(EEPROM), 및 플래시 메모리 디바이스; 자기 디스크, 예를 들어, 내부 하드 디스크 및 이동식 디 스크; 자기-광학 디스크; 및 CD-ROM 및 DVD-ROM 디스크를 포함할 수 있다. 일 예에서, 머신 판독 가능 매체 상에 저장되거나 다른 방식으로 제공되는 정보는 인스트럭션, 가 령, 인스트럭션 자체 또는 인스트럭션이 도출될 수 있는 포맷을 나타낼 수 있다. 인스트럭션 이 도출될 수 있는 이러한 포맷은 소스 코드, 인코딩된 인스트럭션(예를 들어, 압축되거나 암호화된 형태 의 인스트럭션), 패키지된 인스트럭션(예를 들어, 다중 패키지로 분할된 인스트럭션) 등을 포함할 수 있다. 머 신 판독 가능 매체 내의 인스트럭션을 나타내는 정보는 본 명세서에서 논의된 동작 중 임의의 것을 구현하기 위해 처리 회로부에 의해 인스트럭션 내로 처리될 수 있다. 예를 들어, 정보로부터 인스트럭션(272 4)을 도출하는 것(예를 들어, 처리 회로부에 의해 처리하는 것)은 (예를 들어, 소스 코드, 객체 코드 등으로부 터) 컴파일하는 것, 해석하는 것, 로딩하는 것, 조직화하는 것(예를 들어, 동적으로 또는 정적으로 링크하는 것), 인코딩하는 것, 디코딩하는 것, 암호화하는 것, 암호화 해제하는 것, 패키징하는 것, 패키징 해제하는 것, 또는 다른 방식으로 정보를 인스트럭션 내로 조작하는 것을 포함할 수 있다. 일 예에서, 인스트럭션의 도출은 머신 판독 가능 매체에 의해 제공되는 일부 중간 또는 전처리된 포맷으로부터 인스트럭션을 생성하기 위해 (예를 들어, 처리 회로부에 의한) 정보의 조립, 컴파일 또는 해석을 포함할 수 있다. 정보는, 여러 부분으로 제공되는 경우, 인스트럭션을 생성하기 위해 결합, 언팩 킹, 및 수정될 수 있다. 예를 들어, 정보는 하나 또는 수 개의 원격 서버 상의 다중 압축 소스 코드 패키지(또 는 객체 코드, 또는 이진 실행 코드 등)에 있을 수 있다. 소스 코드 패키지는 네트워크를 통해 전송될 때 암호 화될 수 있고, 필요한 경우 해독, 압축 해제, 조립(예를 들어, 링크)될 수 있고, 로컬 머신에서 (예를 들어, 라 이브러리 내로, 독립 실행 등으로) 컴파일 또는 해석될 수 있고, 로컬 머신에 의해 실행될 수 있다. 인스트럭션은 다수의 전송 프로토콜(예를 들어, 프레임 릴레이, 인터넷 프로토콜(IP), 전송 제어 프로토 콜(TCP), 사용자 데이터그램 프로토콜(UDP), 하이퍼 텍스트 전송 프로토콜(HTTP) 등) 중 어느 하나를 사용하는네트워크 인터페이스 디바이스를 경유하는 전송 매체를 사용하여 통신 네트워크를 통해 추가로 송 신 또는 수신될 수 있다. 예시적인 통신 네트워크는 근거리 통신 네트워크(LAN), 광역 통신 네트워크(WAN), 패킷 데이터 네트워크(예를 들어, 인터넷), 모바일 텔레폰 네트워크(예를 들어, 셀룰러 네트워크), POTS (Plain Old Telephone) 네트워크, 및 무선 데이터 네트워크(예를 들어, 무엇보다도 Wi-Fi®로 알려진 IEEE (Institute of Electrical and Electronics Engineers) 802.11 표준 계열, WiMax®로 알려진 IEEE 802.16 표준 계열, IEEE 802.15.4 표준 계열, P2P 네트워크)를 포함할 수 있다. 일 예에서, 네트워크 인터페이스 디바이스는 통 신 네트워크에 연결하기 위해 하나 이상의 물리적 잭(예를 들어, 이더넷, 동축 또는 전화 잭) 또는 하나 이상의 안테나를 포함할 수 있다. 일 예에서, 네트워크 인터페이스 디바이스는 단일 입력 다중 출력 (SIMO), 다중 입력 다중 출력(MIMO), 또는 다중 입력 단일 출력(MISO) 기술 중 적어도 하나를 사용하여 무선으 로 통신하기 위한 복수의 안테나를 포함할 수 있다. \"전송 매체\"라는 용어는 머신에 의한 실행을 위한 인스트럭션을 저장, 인코딩 또는 전달할 수 있는 임의의 무형적인 매체를 포함하는 것으로 간주될 것이며, 그러 한 소프트웨어의 통신을 가능하게 하는 디지털 또는 아날로그 통신 신호 또는 다른 무형적인 매체를 포함한다. 전송 매체는 머신 판독 가능 매체이다. 도 28은 하드웨어 컴포넌트와 소프트웨어 컴포넌트 사이의 다양한 인터페이스가 도시된 컴퓨팅 디바이스의 예시 적인 하드웨어 및 소프트웨어 아키텍처를 도시한 도면이다. HW로 표시된 바와 같이, 하드웨어 컴포넌트는 분할 라인 아래에 표시되는 반면, SW로 표시된 소프트웨어 컴포넌트는 분할 라인 위에 위치한다. 하드웨어 측면에서, (하나 이상의 프로세서 코어를 각각 갖는 하나 이상의 마이크로 프로세서, 디지털 신호 프로세서 등 을 포함할 수 있는) 처리 디바이스는 메모리 관리 디바이스 및 시스템 인터커넥트와 인터페 이스된다. 메모리 관리 디바이스는 실행되는 프로세스에 의해 사용되는 가상 메모리와 물리적 메모리 간 의 매핑을 제공한다. 메모리 관리 디바이스는 처리 디바이스를 또한 포함하는 중앙 처리 유닛의 필수 부분일 수 있다. 인터커넥트는 메모리, 데이터 및 제어 라인과 같은 백플레인뿐만 아니라 PCI, USB 등과 같은 입력/출력 디바이스와의 인터페이스를 포함한다. 메모리(예를 들어, 동적 랜덤 액세스 메모리(DRAM)) 및 플래시 메 모리(예를 들어, 전기적으로 소거 가능한 판독 전용 메모리(EEPROM), NAND 플래시, NOR 플래시 등)와 같은 비 휘발성 메모리는 메모리 관리 디바이스 및 인터커넥트와 메모리 컨트롤러를 통해 인터 페이스된다. 일 예에서, 이 아키텍처는 주변 디바이스에 의해 직접 메모리 액세스(direct memory access)(DM A)를 지원할 수 있다. 비디오 및 오디오 어댑터, 비 휘발성 스토리지, 외부 주변 디바이스 링크(가령, USB, 블 루투스(Bluetooth) 등) 뿐만 아니라 Wi-Fi 또는 LTE 계열 인터페이스를 통해 통신하는 것과 같은 네트워크 인터 페이스 디바이스를 포함한 I/O 디바이스는 집합적으로, 대응하는 I/O 컨트롤러를 통해 인터커넥트 와 인터페이스하는 I/O 디바이스 및 네트워킹으로 표현된다. 소프트웨어 측면에서, 사전 운영 체제(pre-OS) 환경은 초기 시스템 시동시에 실행되고 운영 체제의 부팅 의 개시를 담당하고 있다. 사전 운영 체제(pre-OS) 환경의 하나의 전통적인 예는 시스템 기본 입/출력 시스템(BIOS)이다. 오늘날의 시스템에서는 UEFI (Unified Extensible Firmware Interface)가 구현된다. Pre- OS 환경은 운영 체제의 론칭을 개시하는 것을 담당하지만, 본 발명의 특정 양태에 따른 내장형 애플리케 이션을 위한 실행 환경을 제공한다. 운영 체제(OS)는, 하드웨어 디바이스를 제어하고, 메모리의 프로그램에 대한 메모리 액세스를 관리하며, 작업을 조정하고 멀티태스킹을 가능하게 하며, 저장될 데이터를 조직화하며, 메모리 공간 및 다른 리소스를 할 당하며, 프로그램 이진 코드를 메모리에 로링하고, 애플리케이션 프로그램의 실행을 개시하여 사용자 및 하드웨 어 디바이스와 상호 작용하고, 정의된 다양한 인터럽트를 감지하고 이에 응답하는 커널을 제공한다. 또한, 운 영 체제는 디바이스 드라이버, 및 주변 디바이스 및 네트워킹과의 인터페이싱을 가능하게 하는 것과 같이, 애플리케이션 프로그램에 대한 추상화를 제공하는 다양한 공통 서비스를 제공하여, 애플리케이션은 그러 한 공통 동작의 세부 사항을 처리할 책임을 질 필요는 없다. 운영 체제는 또한 모니터, 키보드, 마우스, 마이크로폰, 비디오 카메라, 터치 스크린 등과 같은 주변 디바이스를 통해 사용자와의 상호 작용을 가능하게 하 는 그래픽 사용자 인터페이스(GUI)를 제공한다. 런타임 시스템은, 함수 호출 전에 스택에 파라미터를 배치하는 것과 같은 동작, 디스크 입/출력(I/O)의 거동, 및 병렬 실행 관련 거동을 포함하는 실행 모델의 일부를 구현한다. 런타임 시스템은 또한 타입 체 킹, 디버깅, 또는 코드 생성 및 최적화와 같은 지원 서비스를 수행할 수 있다. 라이브러리는 애플리케이션 프로그램에 대한 추가 추상화를 제공하는 프로그램 기능의 집합체를 포함한다. 이 라이브러리는, 예를 들어, 공유 라이브러리 및 동적 연결 라이브러리(dynamic linked libraries)(DLL)를 포함한다. 라이브러리는 운영 체제 및 런타임 시스템에 통합될 수 있거 나, 부가 기능일 수 있거나, 또는 심지어는 원격 호스팅될 수 있다. 라이브러리는 운영 체제에 의 해 제공되는 서비스를 호출하기 위해 애플리케이션 프로그램에 의해 다양한 기능 호출이 행해질 수 있는 애플리케이션 프로그램 인터페이스(API)를 정의한다. 애플리케이션 프로그램은 컴퓨팅 디바이스 자체의 기본적인 조작성을 조정하는 하위 레벨 시스템 프로그램에 의해 수행되는 작업을 넘어서, 사용자를 위한 유용한 작업을 수행하는 프로그램이다. 도 29는 일 실시예에 따른 처리 디바이스의 블록도이다. 일 예에서, 도시된 둘 이상의 처리 디바이스 가 공통 반도체 기판 상에 형성된다. CPU는 하나 이상의 처리 코어를 포함할 수 있고, 이들 의 각각은 하나 이상의 산술 논리 유닛(ALU), 인스트럭션 페치 유닛, 인스트럭션 디코드 유닛, 제어 유닛, 레지 스터, 데이터 스택 포인터, 프로그램 카운터, 및 프로세서의 특정 아키텍처에 따른 다른 필수 컴포넌트를 갖는 다. 예시적인 예로서, CPU는 x86 타입의 프로세서일 수 있다. 처리 디바이스는 또한 그래픽 처리 유닛(GPU)을 포함할 수 있다. 일 예에서, GPU는 특정 계산 집약적인 동작, 특히 그래픽 렌더링과 연관된 동작을 CPU로부터 오프로드하는 특수 코프로세서일 수 있다. 특히, CPU 및 GPU는 일 반적으로 공동 작업을 수행하여 메모리 리소스 및 I/O 채널 등에 대한 액세스를 공유한다. 일 예에서, 처리 디바이스는 또한 관리자(caretaker) 프로세서를 포함할 수 있다. 관리자 프로세 서는 일반적으로 CPU 및 GPU가 수행하는 것과 같은 소프트웨어 코드를 수행하는 처리 작업에 참여하지 않는다. 일 예에서, 관리자 프로세서는 CPU 및 GPU와 메모리 공간을 공유하지 않 으며, 따라서, 운영 체제 또는 애플리케이션 프로그램을 실행하도록 구성되지 않는다. 대신에, 관리자 프로세 서는 CPU, GPU, 및 컴퓨터 시스템의 다른 컴포넌트의 기술적 동작을 지원하는 전용 펌웨어를 실행할 수 있다. 일 예에서, 관리자 프로세서는 마이크로 컨트롤러 디바이스로서 구현되며, 이는 CPU와 동일한 집적 회로 다이 상에 물리적으로 존재할 수 있거나 별개의 집적 회로 다이 상에 존재할 수 있다. 관리 자 프로세서는 또한 외부 엔티티와 통신 가능하게 하는 전용 I/O 기능 세트를 포함할 수 있다. 일 타입 의 실시예에서, 관리자 프로세서는 관리성(manageability) 엔진(ME) 또는 플랫폼 보안 프로세서(PSP)를 사용하여 구현된다. 입/출력(I/O) 컨트롤러는 다양한 처리 디바이스들(2940, 2944, 2946) 사이뿐만 아니 라 시스템 인터커넥트와 같은 외부 회로와의 정보 흐름을 조정한다. 도 30은 일 실시예에 따른 CPU의 예시적인 컴포넌트의 블록도이다. 도시된 바와 같이, CPU는 하나 이상의 코어(들), 캐시, 및 CPU 컨트롤러를 포함하며, CPU 컨트롤러는 코어 (들)의 상호 운용 및 작업을 조정할 뿐만 아니라, CPU의 다양한 내부 컴포넌트들 간의 데이터 흐름 및 메모리 버스 또는 시스템 인터커넥트와 같은 외부 컴포넌트와의 데이터 흐름을 가능하게 하는 인터페이스를 제공한다. 일 실시예에서, CPU의 모든 예시적인 컴포넌트는 공통 반도체 기판 상에 형성된다. CPU는 초기화 엔진과 같은 기초 코드, 및 마이크로 코드의 특정 부분을 저장하기 위한 비 휘발성 메모리 (예를 들어, 플래시, EEPROM 등)를 포함한다. 또한, CPU는 시스템 BIOS 또는 UEFI 코드와 같이 초기화 엔진에 의해 론칭되는 기본 코드를 저장하는 외부의 (예를 들어, 별개의 IC 상에 형성된) 비 휘발성 메 모리 디바이스와 인터페이싱될 수 있다. 추가 참고사항 및 예 예 1은 노면을 모델링하기 위한 디바이스로서, 이 디바이스는: 노면을 나타내는 시간 순서의 이미지 시퀀스를 획득하기 위한 하드웨어 센서 인터페이스 - 상기 시퀀스 중 하나의 이미지는 현재 이미지임 - ; 및 인공 신경 네트워크(ANN)에 데이터 세트를 제공하여 장면의 3 차원 구조를 생성하고, 상기 장면의 3 차원 구조를 사용하여 상기 노면을 모델링하는 처리 회로부를 포함하고, 상기 데이터 세트는 상기 이미지 시퀀스의 일부를 포함하고, 상기 이미지 시퀀스의 일부는 상기 현재 이미지, 상기 센서의 모션, 및 에피폴을 포함한다. 예 2에서, 예 1의 요지는 상기 이미지 시퀀스의 일부가 상기 현재 이미지에 바로 선행하는 이미지를 포함하는 것을 포함한다. 예 3에서, 예 2의 요지는 상기 이미지 시퀀스의 일부가 총 3 개의 이미지인 것을 포함한다. 예 4에서, 예 1-3의 요지는, 상기 이미지 시퀀스의 일부가, 상기 이미지 시퀀스에서 하나 이상의 이미지에 의해 분리되며 상기 현재 이미지에 선행하는 이미지를 포함하는 것을 포함한다. 예 5에서, 예 1-4의 요지는 상기 에피폴이 상기 현재 이미지와 동일한 차원을 갖는 그래디언트 이미지로서 제공 되며, 상기 그래디언트 이미지의 픽셀 값은 상기 현재 이미지 내의 픽셀의 에피폴로부터의 거리를 나타내는 것 을 포함한다. 예 6에서, 예 5의 요지는 베이스라인이 적어도 0.5m인 것을 포함한다. 예 7에서, 예 5-6의 요지는 상기 그래디언트 이미지가 상기 현재 이미지보다 낮은 해상도를 갖는 것을 포함한다. 예 8에서, 예 5-7의 요지는, 상기 그래디언트 이미지가 상기 에피폴로부터의 수평 거리만을 나타내고, 제 2 그 래디언트 이미지는 상기 에피폴로부터의 수직 거리를 나타내기 위해 상기 ANN에 제공되는 것을 포함한다. 예 9에서, 예 1-8의 요지는, 상기 센서의 모션은 상기 현재 이미지와 동일한 차원을 갖는 상수 값 이미지로서 제공되는 것을 포함한다. 예 10에서, 예 9의 요지는, 상기 상수 값은 평면으로부터 상기 센서의 높이에 의한 상기 센서의 전진방향 모션 의 비율인 것을 포함한다. 예 11에서, 예 1-10의 요지는, 상기 장면의 3 차원 구조가 감마 이미지이고, 상기 감마 이미지는 상기 현재 이 미지를 캡처하는 센서로부터의 거리에 의한 평면 위의 포인트의 높이의 비율인 감마 값을 갖는 픽셀을 포함하며, 상기 평면은 상기 노면을 나타내는 것을 포함한다. 예 12에서, 예 1-11의 요지는, 상기 노면을 모델링하기 위해, 상기 처리 회로부는 상기 장면의 3 차원 구조를 제 2 ANN으로부터의 출력과 비교함으로써 반사성 구역을 식별하고, 상기 제 2 ANN은 상기 이미지 시퀀스의 일부 를 수용하고 제 2의 3 차원 구조를 생성하도록 트레이닝되고, 상기 제 2 ANN의 트레이닝은 상기 제 1 ANN을 트 레이닝하는 것보다 상기 이미지 시퀀스의 일부에서 더 많은 사진 측량 손실을 사용한 것을 포함한다. 예 13에서, 예 12의 요지는, ANN 및 제 2 ANN이 2-채널 출력을 생성하도록 트레이닝된 단일 ANN으로서 구현되며, 여기서 제 1 채널은 장면의 3 차원 구조이고 제 2 채널은 트레이닝시 더 많은 사진 측량 손실을 사용 한 제 2 ANN에 의해 생성된 3 차원 구조인 것을 포함한다. 예 14에서, 예 1-13의 요지는, 상기 처리 회로부는 특징부가 상기 노면의 환경 내에서 움직이거나 움직이지 않 는 물체를 나타내는지를 결정하기 위해 3 차원 구조를 사용하여 제 2 ANN을 호출하도록 구성되는 것을 포함한다. 예 15에서, 예 14의 요지는, 3 차원 구조를 사용하여 제 2 ANN을 호출하기 위해, 상기 처리 회로부는 상기 제 2 ANN에, 상기 현재 이미지, 상기 3 차원 구조를 사용하여 와핑된 이전 이미지, 및 타겟 식별자를 제공하는 것을 포함한다. 예 16에서, 예 15의 요지는, 상기 타겟 식별자가 이미지의 픽셀이 타겟의 중심으로부터의 거리를 나타내는 이미 지인 것을 포함한다. 예 17에서, 예 15-16의 요지는, 상기 타겟 식별자가 타겟의 사이즈를 포함하는 것을 포함한다. 예 18에서, 예 15-17의 요지는, 상기 타겟 식별자가 타겟에 대응하는 픽셀들의 마스크인 것을 포함한다. 예 19에서, 예 1-18의 요지는, 노면을 모델링하기 위해, 상기 처리 회로부는 노면 특징부의 평면으로부터의 수 직 편차를 계산하는 것을 포함한다. 예 20에서, 예 1-19의 요지는, 상기 ANN은 컨볼루션 신경 네트워크(CNN)인 것을 포함한다. 예 21에서, 예 20의 요지는, 상기 센서의 모션 및 에피폴이 병목 계층에서의 CNN에 제공되는 것을 포함한다. 예 22에서, 예 1-21의 요지는, 상기 ANN이 미래 이미지의 모델과 미래 이미지 간의 차이를 측정함으로써 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 미래 이미지의 모델은 상기 미래 이미지 이전 의 이미지의 감마 와핑을 통해 생성되는 것을 포함한다. 예 23에서, 예 1-22의 요지는, 상기 ANN이 위치에 대한 예측된 감마와 해당 위치에서의 센서 움직임 간의 차이 를 측정함으로써 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 센서 움직임은 피치, 요, 롤, 또는 상기 평면에 수직인 병진이동을 포함하는 것을 포함한다. 예 24에서, 예 1-23의 요지는, 상기 ANN이 2 개의 상이한 시간에서의 2 개의 이미지 간의 중첩 세그먼트의 감마 차이에 의해 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 여기서 제 1 이미지에 대해 추론이 수행되고, 제 2 이미지에서 중첩 세그먼트가 센서에 더 근접한 것을 포함한다. 예 25은 노면을 모델링하기 위한 방법이며, 이 방법은: 노면을 나타내는 시간 순서의 이미지 시퀀스를 획득하는 단계 - 상기 시퀀스 중 하나의 이미지는 현재 이미지임 - ; 인공 신경 네트워크(ANN)에 데이터 세트를 제공하여 장면의 3 차원 구조를 생성하는 단계; 및 상기 장면의 3 차원 구조를 사용하여 상기 노면을 모델링하는 단계를 포함하고, 상기 데이터 세트는 상기 이미지 시퀀스의 일부를 포함하고, 상기 이미지 시퀀스의 일부는 상기 현재 이미지, 센서의 모션, 및 에피폴을 포함한다. 예 26에서, 예 25의 요지는 상기 이미지 시퀀스의 일부가 상기 현재 이미지에 바로 선행하는 이미지를 포함하는 것을 포함한다. 예 27에서, 예 26의 요지는 상기 이미지 시퀀스의 일부가 총 3 개의 이미지인 것을 포함한다. 예 28에서, 예 25-27의 요지는, 상기 이미지 시퀀스의 일부가, 상기 이미지 시퀀스에서 하나 이상의 이미지에 의해 분리되며 상기 현재 이미지에 선행하는 이미지를 포함하는 것을 포함한다. 예 29에서, 예 25-28의 요지는, 상기 에피폴이 상기 현재 이미지와 동일한 차원을 갖는 그래디언트 이미지로서 제공되며, 상기 그래디언트 이미지의 픽셀 값은 상기 현재 이미지 내의 픽셀의 에피폴로부터의 거리를 나타내는 것을 포함한다. 예 30에서, 예 29의 요지는 베이스라인이 적어도 0.5m인 것을 포함한다. 예 31에서, 예 29-30의 요지는 상기 그래디언트 이미지가 상기 현재 이미지보다 낮은 해상도를 갖는 것을 포함 한다. 예 32에서, 예 29-31의 요지는, 상기 그래디언트 이미지가 상기 에피폴로부터의 수평 거리만을 나타내고, 제 2 그래디언트 이미지는 상기 에피폴로부터의 수직 거리를 나타내기 위해 상기 ANN에 제공되는 것을 포함한다. 예 33에서, 예 25-32의 요지는, 상기 센서의 모션은 상기 현재 이미지와 동일한 차원을 갖는 상수 값 이미지로 서 제공되는 것을 포함한다. 예 34에서, 예 33의 요지는, 상기 상수 값은 평면으로부터 상기 센서의 높이에 의한 상기 센서의 전진방향 모션 의 비율인 것을 포함한다. 예 35에서, 예 25-34의 요지는, 상기 장면의 3 차원 구조가 감마 이미지이고, 상기 감마 이미지는 상기 현재 이 미지를 캡처하는 센서로부터의 거리에 의한 평면 위의 포인트의 높이의 비율인 감마 값을 갖는 픽셀을 포함하며, 상기 평면은 상기 노면을 나타내는 것을 포함한다. 예 36에서, 예 25-35의 요지는, 상기 노면을 모델링하는 것은 상기 장면의 3 차원 구조를 제 2 ANN으로부터의 출력과 비교함으로써 반사성 구역을 식별하는 것을 포함하고, 상기 제 2 ANN은 상기 이미지 시퀀스의 일부를 수 용하고 제 2의 3 차원 구조를 생성하도록 트레이닝되고, 상기 제 2 ANN의 트레이닝은 상기 제 1 ANN을 트레이닝 하는 것보다 상기 이미지 시퀀스의 일부에서 더 많은 사진 측량 손실을 사용한 것을 포함한다. 예 37에서, 예 36의 요지는, 상기 ANN 및 제 2 ANN이 2-채널 출력을 생성하도록 트레이닝된 단일 ANN으로서 구 현되며, 여기서 제 1 채널은 장면의 3 차원 구조이고 제 2 채널은 트레이닝시 더 많은 사진 측량 손실을 사용한 제 2 ANN에 의해 생성된 3 차원 구조인 것을 포함한다. 예 38에서, 예 25-37의 요지는, 특징부가 상기 노면의 환경 내에서 움직이거나 움직이지 않는 물체를 나타내는 지를 결정하기 위해 3 차원 구조를 사용하여 제 2 ANN을 호출하는 것을 포함한다. 예 39에서, 예 38의 요지는, 3 차원 구조를 사용하여 제 2 ANN을 호출하는 것은 상기 제 2 ANN에, 상기 현재 이 미지, 상기 3 차원 구조를 사용하여 와핑된 이전 이미지, 및 타겟 식별자를 제공하는 것을 포함한다. 예 40에서, 예 39의 요지는, 상기 타겟 식별자가 이미지의 픽셀이 타겟의 중심으로부터의 거리를 나타내는 이미 지인 것을 포함한다. 예 41에서, 예 39-40의 요지는, 상기 타겟 식별자가 타겟의 사이즈를 포함하는 것을 포함한다. 예 42에서, 예 39-41의 요지는, 상기 타겟 식별자가 타겟에 대응하는 픽셀들의 마스크인 것을 포함한다. 예 43에서, 예 25-42의 요지는, 노면을 모델링하는 것은 노면 특징부의 평면으로부터의 수직 편차를 계산하는 것을 포함한 것을 포함한다. 예 44에서, 예 25-43의 요지는, 상기 ANN은 컨볼루션 신경 네트워크(CNN)인 것을 포함한다. 예 45에서, 예 44의 요지는, 상기 센서의 모션 및 에피폴이 병목 계층에서의 CNN에 제공되는 것을 포함한다. 예 46에서, 예 25-45의 요지는, 상기 ANN이 미래 이미지의 모델과 미래 이미지 간의 차이를 측정함으로써 오차 를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 미래 이미지의 모델은 상기 미래 이미지 이 전의 이미지의 감마 와핑을 통해 생성되는 것을 포함한다. 예 47에서, 예 25-46의 요지는, 상기 ANN이 위치에 대한 예측된 감마와 해당 위치에서의 센서 움직임 간의 차이 를 측정함으로써 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 센서 움직임은 피치, 요, 롤, 또는 상기 평면에 수직인 병진이동을 포함하는 것을 포함한다. 예 48에서, 예 25-47의 요지는, 상기 ANN이 2 개의 상이한 시간에서의 2 개의 이미지 간의 중첩 세그먼트의 감 마 차이에 의해 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 여기서 제 1 이미지에 대해 추론이 수행되고, 제 2 이미지에서 중첩 세그먼트가 센서에 더 근접한 것을 포함한다. 예 49은 노면을 모델링하기 위한 인스트럭션을 포함한 적어도 하나의 머신 판독 가능 매체이며, 이 인스트럭션 은 처리 회로부에 의해 실행될 때, 상기 처리 회로부로 하여금 동작을 수행하게 하며, 상기 동작은: 노면을 나 타내는 시간 순서의 이미지 시퀀스를 획득하는 단계 - 상기 시퀀스 중 하나의 이미지는 현재 이미지임 - ; 인공 신경 네트워크(ANN)에 데이터 세트를 제공하여 장면의 3 차원 구조를 생성하는 단계; 및 상기 장면의 3 차원 구 조를 사용하여 상기 노면을 모델링하는 단계를 포함하고, 상기 데이터 세트는 상기 이미지 시퀀스의 일부를 포 함하고, 상기 이미지 시퀀스의 일부는 상기 현재 이미지, 센서의 모션, 및 에피폴을 포함한다. 예 50에서, 예 49의 요지는 상기 이미지 시퀀스의 일부가 상기 현재 이미지에 바로 선행하는 이미지를 포함하는 것을 포함한다. 예 51에서, 예 50의 요지는 상기 이미지 시퀀스의 일부가 총 3 개의 이미지인 것을 포함한다. 예 52에서, 예 49-51의 요지는, 상기 이미지 시퀀스의 일부가, 상기 이미지 시퀀스에서 하나 이상의 이미지에 의해 분리되며 상기 현재 이미지에 선행하는 이미지를 포함하는 것을 포함한다. 예 53에서, 예 49-52의 요지는, 상기 에피폴이 상기 현재 이미지와 동일한 차원을 갖는 그래디언트 이미지로서 제공되며, 상기 그래디언트 이미지의 픽셀 값은 상기 현재 이미지 내의 픽셀의 에피폴로부터의 거리를 나타내는 것을 포함한다. 예 54에서, 예 53의 요지는 베이스라인이 적어도 0.5m인 것을 포함한다. 예 55에서, 예 53-54의 요지는 상기 그래디언트 이미지가 상기 현재 이미지보다 낮은 해상도를 갖는 것을 포함 한다. 예 56에서, 예 53-55의 요지는, 상기 그래디언트 이미지가 상기 에피폴로부터의 수평 거리만을 나타내고, 제 2 그래디언트 이미지는 상기 에피폴로부터의 수직 거리를 나타내기 위해 상기 ANN에 제공되는 것을 포함한다. 예 57에서, 예 49-56의 요지는, 상기 센서의 모션은 상기 현재 이미지와 동일한 차원을 갖는 상수 값 이미지로 서 제공되는 것을 포함한다. 예 58에서, 예 57의 요지는, 상기 상수 값은 평면으로부터 상기 센서의 높이에 의한 상기 센서의 전진방향 모션 의 비율인 것을 포함한다. 예 59에서, 예 49-58의 요지는, 상기 장면의 3 차원 구조가 감마 이미지이고, 상기 감마 이미지는 상기 현재 이 미지를 캡처하는 센서로부터의 거리에 의한 평면 위의 포인트의 높이의 비율인 감마 값을 갖는 픽셀을 포함하며, 상기 평면은 상기 노면을 나타내는 것을 포함한다. 예 60에서, 예 49-59의 요지는, 상기 노면을 모델링하는 것은 상기 장면의 3 차원 구조를 제 2 ANN으로부터의 출력과 비교함으로써 반사성 구역을 식별하는 것을 포함하고, 상기 제 2 ANN은 상기 이미지 시퀀스의 일부를 수 용하고 제 2의 3 차원 구조를 생성하도록 트레이닝되고, 상기 제 2 ANN의 트레이닝은 상기 제 1 ANN을 트레이닝 하는 것보다 상기 이미지 시퀀스의 일부에서 더 많은 사진 측량 손실을 사용한 것을 포함한다. 예 61에서, 예 60의 요지는, 상기 ANN 및 제 2 ANN이 2-채널 출력을 생성하도록 트레이닝된 단일 ANN으로서 구 현되며, 여기서 제 1 채널은 장면의 3 차원 구조이고 제 2 채널은 트레이닝시 더 많은 사진 측량 손실을 사용한제 2 ANN에 의해 생성된 3 차원 구조인 것을 포함한다. 예 62에서, 예 49-61의 요지는, 상기 동작은 특징부가 상기 노면의 환경 내에서 움직이거나 움직이지 않는 물체 를 나타내는지를 결정하기 위해 3 차원 구조를 사용하여 제 2 ANN을 호출하는 단계를 포함하는 것을 포함한다. 예 63에서, 예 62의 요지는, 3 차원 구조를 사용하여 제 2 ANN을 호출하는 것은 상기 제 2 ANN에, 상기 현재 이 미지, 상기 3 차원 구조를 사용하여 와핑된 이전 이미지, 및 타겟 식별자를 제공하는 것을 포함한다. 예 64에서, 예 63의 요지는, 상기 타겟 식별자가 이미지의 픽셀이 타겟의 중심으로부터의 거리를 나타내는 이미 지인 것을 포함한다. 예 65에서, 예 63-64의 요지는, 상기 타겟 식별자가 타겟의 사이즈를 포함하는 것을 포함한다. 예 66에서, 예 63-65의 요지는, 상기 타겟 식별자가 타겟에 대응하는 픽셀들의 마스크인 것을 포함한다. 예 67에서, 예 49-66의 요지는, 노면을 모델링하는 것은 노면 특징부의 평면으로부터의 수직 편차를 계산하는 것을 포함한 것을 포함한다. 예 68에서, 예 49-67의 요지는, 상기 ANN은 컨볼루션 신경 네트워크(CNN)인 것을 포함한다. 예 69에서, 예 68의 요지는, 상기 센서의 모션 및 에피폴이 병목 계층에서의 CNN에 제공되는 것을 포함한다. 예 70에서, 예 49-69의 요지는, 상기 ANN이 미래 이미지의 모델과 미래 이미지 간의 차이를 측정함으로써 오차 를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 미래 이미지의 모델은 상기 미래 이미지 이 전의 이미지의 감마 와핑을 통해 생성되는 것을 포함한다. 예 71에서, 예 49-70의 요지는, 상기 ANN이 위치에 대한 예측된 감마와 해당 위치에서의 센서 움직임 간의 차이 를 측정함으로써 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 센서 움직임은 피치, 요, 롤, 또는 상기 평면에 수직인 병진이동을 포함하는 것을 포함한다. 예 72에서, 예 49-71의 요지는, 상기 ANN이 2 개의 상이한 시간에서의 2 개의 이미지 간의 중첩 세그먼트의 감 마 차이에 의해 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 여기서 제 1 이미지에 대해 추론이 수행되고, 제 2 이미지에서 중첩 세그먼트가 센서에 더 근접한 것을 포함한다. 예 73은 노면을 모델링하기 위한 시스템이며, 이 시스템은: 노면을 나타내는 시간 순서의 이미지 시퀀스를 획득 하는 수단 - 상기 시퀀스 중 하나의 이미지는 현재 이미지임 - ; 인공 신경 네트워크(ANN)에 데이터 세트를 제 공하여 장면의 3 차원 구조를 생성하는 수단; 및 상기 장면의 3 차원 구조를 사용하여 상기 노면을 모델링하는 수단을 포함하고, 상기 데이터 세트는 상기 이미지 시퀀스의 일부를 포함하고, 상기 이미지 시퀀스의 일부는 상 기 현재 이미지, 센서의 모션, 및 에피폴을 포함한다. 예 74에서, 예 73의 요지는 상기 이미지 시퀀스의 일부가 상기 현재 이미지에 바로 선행하는 이미지를 포함하는 것을 포함한다. 예 75에서, 예 74의 요지는 상기 이미지 시퀀스의 일부가 총 3 개의 이미지인 것을 포함한다. 예 76에서, 예 73-75의 요지는, 상기 이미지 시퀀스의 일부가, 상기 이미지 시퀀스에서 하나 이상의 이미지에 의해 분리되며 상기 현재 이미지에 선행하는 이미지를 포함하는 것을 포함한다. 예 77에서, 예 73-76의 요지는, 상기 에피폴이 상기 현재 이미지와 동일한 차원을 갖는 그래디언트 이미지로서 제공되며, 상기 그래디언트 이미지의 픽셀 값은 상기 현재 이미지 내의 픽셀의 에피폴로부터의 거리를 나타내는 것을 포함한다. 예 78에서, 예 77의 요지는 베이스라인이 적어도 0.5m인 것을 포함한다. 예 79에서, 예 77-78의 요지는 상기 그래디언트 이미지가 상기 현재 이미지보다 낮은 해상도를 갖는 것을 포함 한다. 예 80에서, 예 77-79의 요지는, 상기 그래디언트 이미지가 상기 에피폴로부터의 수평 거리만을 나타내고, 제 2 그래디언트 이미지는 상기 에피폴로부터의 수직 거리를 나타내기 위해 상기 ANN에 제공되는 것을 포함한다. 예 81에서, 예 73-80의 요지는, 상기 센서의 모션은 상기 현재 이미지와 동일한 차원을 갖는 상수 값 이미지로 서 제공되는 것을 포함한다. 예 82에서, 예 81의 요지는, 상기 상수 값은 평면으로부터 상기 센서의 높이에 의한 상기 센서의 전진방향 모션 의 비율인 것을 포함한다. 예 83에서, 예 73-82의 요지는, 상기 장면의 3 차원 구조가 감마 이미지이고, 상기 감마 이미지는 상기 현재 이 미지를 캡처하는 센서로부터의 거리에 의한 평면 위의 포인트의 높이의 비율인 감마 값을 갖는 픽셀을 포함하며, 상기 평면은 상기 노면을 나타내는 것을 포함한다. 예 84에서, 예 73-83의 요지는, 상기 노면을 모델링하는 수단은 상기 장면의 3 차원 구조를 제 2 ANN으로부터의 출력과 비교함으로써 반사성 구역을 식별하는 수단을 포함하고, 상기 제 2 ANN은 상기 이미지 시퀀스의 일부를 수용하고 제 2의 3 차원 구조를 생성하도록 트레이닝되고, 상기 제 2 ANN의 트레이닝은 상기 제 1 ANN을 트레이 닝하는 것보다 상기 이미지 시퀀스의 일부에서 더 많은 사진 측량 손실을 사용하는 것을 포함한다. 예 85에서, 예 84의 요지는, 상기 ANN 및 제 2 ANN이 2-채널 출력을 생성하도록 트레이닝된 단일 ANN으로서 구 현되며, 여기서 제 1 채널은 장면의 3 차원 구조이고 제 2 채널은 트레이닝시 더 많은 사진 측량 손실을 사용한 제 2 ANN에 의해 생성된 3 차원 구조인 것을 포함한다. 예 86에서, 예 73-85의 요지는, 특징부가 상기 노면의 환경 내에서 움직이거나 움직이지 않는 물체를 나타내는 지를 결정하기 위해 3 차원 구조를 사용하여 제 2 ANN을 호출하는 수단을 포함한다. 예 87에서, 예 86의 요지는, 3 차원 구조를 사용하여 제 2 ANN을 호출하는 수단은 상기 제 2 ANN에, 상기 현재 이미지, 상기 3 차원 구조를 사용하여 와핑된 이전 이미지, 및 타겟 식별자를 제공하는 수단을 포함한다. 예 88에서, 예 87의 요지는, 상기 타겟 식별자가 이미지의 픽셀이 타겟의 중심으로부터의 거리를 나타내는 이미 지인 것을 포함한다. 예 89에서, 예 87-88의 요지는, 상기 타겟 식별자가 타겟의 사이즈를 포함하는 것을 포함한다. 예 90에서, 예 87-89의 요지는, 상기 타겟 식별자가 타겟에 대응하는 픽셀들의 마스크인 것을 포함한다. 예 91에서, 예 73-90의 요지는, 노면을 모델링하는 수단이 노면 특징부의 평면으로부터의 수직 편차를 계산하는 수단을 포함하는 것을 포함한다. 예 92에서, 예 73-91의 요지는, 상기 ANN은 컨볼루션 신경 네트워크(CNN)인 것을 포함한다. 예 93에서, 예 92의 요지는, 상기 센서의 모션 및 에피폴이 병목 계층에서의 CNN에 제공되는 것을 포함한다. 예 94에서, 예 73-93의 요지는, 상기 ANN이 미래 이미지의 모델과 미래 이미지 간의 차이를 측정함으로써 오차 를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 미래 이미지의 모델은 상기 미래 이미지 이 전의 이미지의 감마 와핑을 통해 생성되는 것을 포함한다. 예 95에서, 예 73-94의 요지는, 상기 ANN이 위치에 대한 예측된 감마와 해당 위치에서의 센서 움직임 간의 차이 를 측정함으로써 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 상기 센서 움직임은 피치, 요, 롤, 또는 상기 평면에 수직인 병진이동을 포함하는 것을 포함한다. 예 96에서, 예 73-95의 요지는, 상기 ANN이 2 개의 상이한 시간에서의 2 개의 이미지 간의 중첩 세그먼트의 감 마 차이에 의해 오차를 결정하는 비 지도 트레이닝 기술을 이용하여 트레이닝되며, 여기서 제 1 이미지에 대해 추론이 수행되고, 제 2 이미지에서 중첩 세그먼트가 센서에 더 근접한 것을 포함한다. 예 97은 노면을 모델링하기 위한 방법이며, 이 방법은: 표면을 나타내는 시간 순서의 이미지 시퀀스를 획득하는 단계 - 상기 시퀀스 중 하나의 이미지는 현재 이미지임 - ; 제 1 인공 신경 네트워크(ANN)에 데이터 세트를 제 공하여 장면의 3 차원 구조의 제 1 출력을 생성하는 단계; 제 2 ANN에 상기 데이터 세트를 제공하여 상기 장면 의 사진 측량 손실의 제 2 출력을 생성하는 단계; 및 상기 제 1 출력과 상기 제 2 출력을 비교하여 상기 표면의 특징부를 결정하는 단계를 포함하고, 상기 데이터 세트는 상기 이미지 시퀀스의 일부를 포함하고, 상기 이미지 시퀀스의 일부는 상기 현재 이미지, 센서의 모션, 및 에피폴을 포함한다. 예 98에서, 예 97의 요지는, 상기 제 1 출력이 장면의 3 차원 구조이고, 상기 제 2 출력이 상기 장면의 사진 측 량 손실에 기반한 3 차원 구조인 것을 포함한다. 예 99에서, 예 98의 요지는 상기 제 1 출력이 감마 맵인 것을 포함한다. 예 100에서, 예 99의 요지는, 상기 제 1 출력과 상기 제 2 출력을 비교하는 것이 상기 제 2 출력 내의 홀을 상 기 제 1 출력 내의 일정한 감마의 인접한 구역에 정렬시키는 것을 포함하는 것을 포함한다. 예 101에서, 예 97-100의 요지는, 상기 제 1 ANN 및 상기 제 2 ANN이 손실 타입들 간의 가중치가 상이하고 동일 한 손실 타입을 사용하는 손실 함수를 사용하여 트레이닝되는 것을 포함한다. 예 102에서, 예 101의 요지는, 손실 타입이 광도 손실, 기하학적 손실, 센서 모션 손실, 또는 미래 이미지 손실 중 적어도 하나를 포함하는 것을 포함한다. 예 103에서, 예 97-102의 요지는, 상기 제 1 ANN 및 제 2 ANN이 2-채널 출력을 생성하도록 트레이닝된 단일 ANN 으로서 구현되며, 여기서 제 1 채널은 상기 제 1 출력이고, 상기 제 2 채널은 상기 제 2 출력인 것을 포함한다. 예 104에서, 예 97-103의 요지는, 상기 제 1 ANN 및 상기 제 2 ANN은 단일 ANN을 병목까지 공유하고, 그 후 분 기하도록 구현되는 것을 포함한다. 예 105은 노면을 모델링하기 위한 인스트럭션을 포함한 적어도 하나의 머신 판독 가능 매체이며, 이 인스트럭션 은 처리 회로부에 의해 실행될 때, 상기 처리 회로부로 하여금 동작을 수행하게 하며, 상기 동작은: 표면을 나 타내는 시간 순서의 이미지 시퀀스를 획득하는 단계 - 상기 시퀀스 중 하나의 이미지는 현재 이미지임 - ; 제 1 인공 신경 네트워크(ANN)에 데이터 세트를 제공하여 장면의 3 차원 구조의 제 1 출력을 생성하는 단계; 제 2 ANN에 상기 데이터 세트를 제공하여 상기 장면의 사진 측량 손실의 제 2 출력을 생성하는 단계; 및 상기 제 1 출력과 상기 제 2 출력을 비교하여 상기 표면의 특징부를 결정하는 단계를 포함하고, 상기 데이터 세트는 상기 이미지 시퀀스의 일부를 포함하고, 상기 이미지 시퀀스의 일부는 상기 현재 이미지, 센서의 모션, 및 에피폴을 포함한다. 예 106에서, 예 105의 요지는, 상기 제 1 출력이 장면의 3 차원 구조이고, 상기 제 2 출력이 상기 장면의 사진 측량 손실에 기반한 3 차원 구조인 것을 포함한다. 예 107에서, 예 106의 요지는 상기 제 1 출력이 감마 맵인 것을 포함한다. 예 108에서, 예 107의 요지는, 상기 제 1 출력과 상기 제 2 출력을 비교하는 것이 상기 제 2 출력 내의 홀을 상 기 제 1 출력 내의 일정한 감마의 인접한 구역에 정렬시키는 것을 포함하는 것을 포함한다. 예 109에서, 예 105-108의 요지는, 상기 제 1 ANN 및 상기 제 2 ANN이 손실 타입들 간의 가중치가 상이하고 동 일한 손실 타입을 사용하는 손실 함수를 사용하여 트레이닝되는 것을 포함한다. 예 110에서, 예 109의 요지는, 손실 타입이 광도 손실, 기하학적 손실, 센서 모션 손실, 또는 미래 이미지 손실 중 적어도 하나를 포함하는 것을 포함한다. 예 111에서, 예 105-110의 요지는, 상기 제 1 ANN 및 제 2 ANN이 2-채널 출력을 생성하도록 트레이닝된 단일 ANN으로서 구현되며, 여기서 제 1 채널은 상기 제 1 출력이고, 상기 제 2 채널은 상기 제 2 출력인 것을 포함한 다. 예 112에서, 예 105-111의 요지는, 상기 제 1 ANN 및 상기 제 2 ANN은 단일 ANN을 병목까지 공유하고, 그 후 분 기하도록 구현되는 것을 포함한다. 예 113은 노면을 모델링하기 위한 디바이스이며, 이 디바이스는: 표면을 나타내는 시간 순서의 이미지 시퀀스를 획득하는 이미지 캡처 디바이스 - 상기 시퀀스 중 하나의 이미지는 현재 이미지임 - ; 제 1 인공 신경 네트워크 (ANN)에 데이터 세트를 제공하여 장면의 3 차원 구조의 제 1 출력을 생성하고, 제 2 ANN에 상기 데이터 세트를 제공하여 상기 장면의 사진 측량 손실의 제 2 출력을 생성하고, 그리고 상기 제 1 출력과 상기 제 2 출력을 비 교하여 상기 표면의 특징부를 결정하는 처리 회로부를 포함하고, 상기 데이터 세트는 상기 이미지 시퀀스의 일 부를 포함하고, 상기 이미지 시퀀스의 일부는 상기 현재 이미지, 센서의 모션, 및 에피폴을 포함한다. 예 114에서, 예 113의 요지는, 상기 제 1 출력이 장면의 3 차원 구조이고, 상기 제 2 출력이 상기 장면의 사진 측량 손실에 기반한 3 차원 구조인 것을 포함한다. 예 115에서, 예 114의 요지는 상기 제 1 출력이 감마 맵인 것을 포함한다. 예 116에서, 예 115의 요지는, 상기 제 1 출력과 상기 제 2 출력을 비교하는 것이 상기 제 2 출력 내의 홀을 상 기 제 1 출력 내의 일정한 감마의 인접한 구역에 정렬시키는 것을 포함하는 것을 포함한다. 예 117에서, 예 113-116의 요지는, 상기 제 1 ANN 및 상기 제 2 ANN이 손실 타입들 간의 가중치가 상이하고 동 일한 손실 타입을 사용하는 손실 함수를 사용하여 트레이닝되는 것을 포함한다. 예 118에서, 예 117의 요지는, 손실 타입이 광도 손실, 기하학적 손실, 센서 모션 손실, 또는 미래 이미지 손실 중 적어도 하나를 포함하는 것을 포함한다. 예 119에서, 예 113-118의 요지는, 상기 제 1 ANN 및 제 2 ANN이 2-채널 출력을 생성하도록 트레이닝된 단일 ANN으로서 구현되며, 여기서 제 1 채널은 상기 제 1 출력이고, 상기 제 2 채널은 상기 제 2 출력인 것을 포함한 다. 예 120에서, 예 113-119의 요지는, 상기 제 1 ANN 및 상기 제 2 ANN은 단일 ANN을 병목까지 공유하고, 그 후 분 기하도록 구현되는 것을 포함한다. 예 121은 차량의 자율 주행 내비게이션 시스템이며, 이 시스템은: 사용자 차량 부근의 구역의 복수의 이미지를 획득하도록 구성된 적어도 하나의 이미지 캡처 디바이스; 데이터 인터페이스; 및 상기 데이터 인터페이스를 통 해 상기 복수의 이미지를 수신하고, 상기 차량의 경로 상의 퍼들의 존재를 상기 복수의 이미지로부터 결정하도 록 구성되는 적어도 하나의 처리 디바이스를 포함한다. 예 122에서, 예 121의 요지는, 상기 적어도 하나의 처리 디바이스가: 상기 복수의 이미지로부터, 타겟이 상기 퍼들을 통과하는 차량의 스플래시 존 내에 위치되어 있는지를 결정하고; 그리고 상기 차량의 새로운 스플래시 존이 상기 타겟을 포함하지 않게 상기 차량의 스플래시 존을 수정하기 위해 내비게이션 기동을 수행하는 차량을 제어하도록 구성되는 것을 포함한다. 예 123에서, 예 122의 요지는, 내비게이션 기동이 차량을 감속시키는 것, 차량의 새로운 경로가 상기 타겟으로 부터 충분히 멀어져 상기 타겟이 스플래시와 부딪치는 것을 방지하도록 차선 내 방향변경을 수행하는 것, 또는 상기 차량의 새로운 경로가 더 이상 상기 퍼들을 통과하지 않도록 차선 내 방향변경을 수행하는 것 중의 적어도 하나인 것을 포함한다. 예 124에서, 예 121-123의 요지는, 상기 타겟이 보행자인 것을 포함한다. 예 125에서, 예 121-124의 요지는, 상기 타겟이 차량인 것을 포함한다. 예 126은 도로를 따라 주행하는 자율 주행 차량을 제어하기 위한 방법이며, 상기 방법은 컴퓨팅 플랫폼에 의해 수행되고, 상기 방법은: 상기 도로의 일부를 포함하는 차량의 관점으로부터 적어도 하나의 시야를 나타내는 이 미지 시퀀스를 저장하는 단계; 상기 도로 상의 퍼들을 탐지하기 위해 상기 이미지 시퀀스를 처리하는 단계; 상 기 퍼들 부근의 보행자의 임의의 존재를 결정하는 단계; 및 상기 퍼들의 탐지에 응답하여 주행 응답 솔루션을 결정하는 단계를 포함하고, 상기 주행 응답 솔루션은 상기 보행자의 존재가 탐지되었는지에 기반한다. 예 127에서, 예 126의 요지는, 상기 차량에 탑재된 카메라 시스템에 의해 상기 이미지 시퀀스를 캡처하는 단계 를 포함하고; 상기 처리는 상기 이미지 시퀀스의 단안 이미지 처리를 포함한다. 예 128에서, 예 127의 요지는 상기 이미지 시퀀스가 상기 카메라 시스템의 복수의 이미지 센서에 의해 캡처되는 것을 포함한다. 예 129에서, 예 126-128의 요지는, 상기 도로 상의 퍼들을 탐지하기 위해 상기 이미지 시퀀스를 처리하는 단계 가: 상기 이미지 시퀀스에 기반하여 상기 도로의 수직 윤곽을 계산적으로 결정하는 단계; 상기 수직 윤곽에서 심도 임계치를 초과하는 함몰부의 존재를 결정하는 단계; 및 상기 함몰부의 특징에 대해 퍼들 탐지 기준을 적용 하는 단계를 포함하는 것을 포함한다. 예 130에서, 예 129의 요지는, 퍼들 탐지 기준이 경계 첨예도 기준을 포함하고, 퍼들 탐지 기준을 적용하는 단 계가 상기 퍼들의 경계의 계산적 평가치를 생성하는 단계 및 상기 계산적 평가치를 상기 경계 첨예도 기준에 대 해 비교하는 단계를 포함하는 것을 포함한다. 예 131에서, 예 130의 요지는, 상기 퍼들의 경계의 계산적 평가치가 상기 경계를 따라 상기 퍼들 내부의 포인트 들의 수직 윤곽 스코어링, 및 상기 경계를 따라 상기 퍼들 외부의 포인트들의 수직 윤곽 스코어링을 포함하고, 상기 경계 첨예도 기준이 상기 퍼들 내부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 제 1 임계치, 및 상 기 퍼들 외부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 제 2 임계치를 포함하는 것을 포함한다. 예 132에서, 예 129-131의 요지는, 상기 퍼들 탐지 기준은 상기 퍼들 내부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 인접성 기준을 포함하는 것을 포함한다. 예 133에서, 예 126-132의 요지는, 상기 퍼들 부근의 보행자의 임의의 존재를 결정하는 단계가 인간을 탐지하도 록 구성된 물체 인식 동작을 수행하기 위해 상기 이미지 시퀀스를 처리하는 단계를 포함하는 것을 포함한다. 예 134에서, 예 126-133의 요지는, 상기 퍼들의 탐지에 응답하여 주행 응답 솔루션을 결정하는 단계가 상기 퍼 들 부근의 보행자의 긍정적인 탐지에 응답하여 상기 퍼들로 보행자를 스플래시할 가능성을 감소시키거나 스플래 시를 회피하기 위해 상기 차량에 대한 코스 또는 속도 응답을 생성하는 단계를 포함하는 것을 포함한다. 예 135에서, 예 134의 요지는, 상기 주행 응답 솔루션을 결정하는 단계가 위험 회피 결정 방식에 기반하여 복수 의 잠재적 주행 응답 옵션 중에서 상기 주행 응답 솔루션을 선택하는 단계를 포함하는 것을 포함한다. 예 136에서, 예 135의 요지는, 상기 퍼들 부근의 보행자의 긍정적인 탐지가 위험 회피 결정 방식에 대한 증가 위험 요인으로서 표현되며, 상기 증가 위험 요인은 상기 퍼들과 연관된 총 위험 스코어에 기여하며, 상기 총 위 험 스코어는 상기 차량이 상기 주행 응답 솔루션을 수행할 필요의 정도를 나타내는 것을 포함한다. 예 137에서, 예 135-136의 요지는, 상기 퍼들의 탐지에 응답하여 주행 응답 솔루션을 결정하는 단계가 잠재적 주행 응답 옵션의 각각과 연관된 위험을 해당 잠재적 주행 응답 옵션이 위험 요인에 기여하는 현재 상황 시나리 오의 위험 평가치에 대해 비교하는 단계를 포함하는 것을 포함한다. 예 138에서, 예 137의 요지는 상기 현재 상황 시나리오의 평가를 수행하는 단계를 포함한다. 예 139는 도로를 따라 주행하는 차량에 사용하기 위한 머신 비전 시스템을 위한 장치이며, 이 장치는 적어도 하 나의 프로세서 및 저장 회로부를 구비하는 컴퓨팅 플랫폼을 포함하고, 상기 컴퓨팅 플랫폼은: 상기 도로의 일부 를 포함하는 상기 차량의 관점에서 적어도 하나의 시야를 나타내는 이미지 시퀀스를 포함하는 데이터 저장소; 상기 이미지 시퀀스에 기반하여 상기 도로 상의 퍼들의 임의의 존재를 결정하는 퍼들 탐지 엔진; 상기 퍼들 부 근의 보행자의 임의의 존재를 결정하는 보행자 탐지 엔진; 및 상기 퍼들의 탐지에 응답하여 주행 응답 솔루션을 결정하는 주행 응답 엔진을 구현하는 것이고, 상기 주행 응답 솔루션은 상기 보행자의 존재가 상기 보행자 탐지 엔진에 의해 검출되었는지에 기반한다. 예 140에서, 예 139의 요지는 상기 차량에 탑재되고 상기 이미지 시퀀스를 캡처하도록 동작 가능한 카메라 시스 템을 포함하고; 상기 퍼들 탐지 엔진은 상기 이미지 시퀀스의 단안 이미지 처리를 수행하는 것이다. 예 141에서, 예 140의 요지는 상기 이미지 시퀀스가 상기 카메라 시스템의 복수의 이미지 센서에 의해 캡처되는 것을 포함한다. 예 142에서, 예 139-141의 요지는, 상기 퍼들 탐지 엔진이: 상기 이미지 시퀀스에 기반하여 상기 도로의 수직 윤곽을 계산적으로 결정하고; 상기 수직 윤곽에서 심도 임계치를 초과하는 함몰부의 존재를 결정하고; 그리고 상기 함몰부의 특징에 대해 퍼들 탐지 기준을 적용하는 것을 포함한다. 예 143에서, 예 142의 요지는, 퍼들 탐지 기준이 경계 첨예도 기준을 포함하고, 퍼들 탐지 기준을 적용하는 것 은 상기 퍼들의 경계의 계산적 평가치를 생성하는 것 및 상기 계산적 평가치를 상기 경계 첨예도 기준에 대해 비교하는 것을 포함하는 것을 포함한다. 예 144에서, 예 143의 요지는, 상기 퍼들의 경계의 계산적 평가치가 상기 경계를 따라 상기 퍼들 내부의 포인트 들의 수직 윤곽 스코어링, 및 상기 경계를 따라 상기 퍼들 외부의 포인트들의 수직 윤곽 스코어링을 포함하고, 상기 경계 첨예도 기준이 상기 퍼들 내부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 제 1 임계치, 및 상 기 퍼들 외부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 제 2 임계치를 포함하는 것을 포함한다. 예 145에서, 예 142-144의 요지는, 상기 퍼들 탐지 기준은 상기 퍼들 내부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 인접성 기준을 포함하는 것을 포함한다. 예 146에서, 예 139-145의 요지는, 상기 보행자 탐지 엔진이 인간을 탐지하도록 구성된 물체 인식 동작을 수행 하기 위해 상기 이미지 시퀀스를 적어도 처리하는 것에 의해 상기 퍼들 부근의 보행자의 임의의 존재를 결정하 는 것을 포함한다. 예 147에서, 예 139-146의 요지는, 상기 주행 응답 엔진이 상기 퍼들 부근의 보행자의 긍정적인 탐지에 응답하 여 상기 퍼들로 보행자를 스플래시할 가능성을 감소시키거나 스플래시를 회피하기 위해 상기 차량에 대한 코스 또는 속도 응답을 생성하는 것을 포함한다. 예 148에서, 예 147의 요지는, 상기 주행 응답 솔루션이 위험 회피 결정 방식에 기반하여 복수의 잠재적 주행 응답 옵션 중에서 선택되는 것을 포함한다. 예 149에서, 예 148의 요지는, 상기 퍼들 부근의 보행자의 긍정적인 탐지가 위험 회피 결정 방식에 대한 증가 위험 요인으로서 표현되며, 상기 증가 위험 요인은 상기 퍼들과 연관된 총 위험 스코어에 기여하며, 상기 총 위 험 스코어는 상기 차량이 상기 주행 응답 솔루션을 수행할 필요의 정도를 나타내는 것을 포함한다. 예 150에서, 예 148-149의 요지는, 상기 주행 응답 솔루션 엔진이 잠재적 주행 응답 옵션의 각각과 연관된 위험 을 해당 잠재적 주행 응답 옵션이 위험 요인에 기여하는 현재 상황 시나리오의 위험 평가치에 대해 비교하는 것 을 포함한다. 예 151에서, 예 150의 요지는 상기 주행 응답 솔루션 엔진이 상기 현재 상황 시나리오의 평가를 수행하는 것을 포함한다. 예 152는 인스트럭션을 포함하는 적어도 하나의 머신 판독 가능 매체이며, 상기 인스트럭션은 도로를 따라 주행 하는 자율 주행 차량의 컴퓨팅 플랫폼에 의해 실행될 때 상기 컴퓨팅 플랫폼으로 하여금 동작을 수행하게 하며, 상기 동작은: 상기 도로의 일부를 포함하는 차량의 관점으로부터 적어도 하나의 시야를 나타내는 이미지 시퀀스 를 저장하는 단계; 상기 도로 상의 퍼들을 탐지하기 위해 상기 이미지 시퀀스를 처리하는 단계; 상기 퍼들 부근 의 보행자의 임의의 존재를 결정하는 단계; 및 상기 퍼들의 탐지에 응답하여 주행 응답 솔루션을 결정하는 단계 를 포함하고, 상기 주행 응답 솔루션은 상기 보행자의 존재가 탐지되었는지에 기반한다. 예 153에서, 예 152의 요지는, 상기 인스트럭션이 상기 컴퓨팅 플랫폼으로 하여금, 상기 차량에 탑재된 카메라 시스템에 의해 상기 이미지 시퀀스를 캡처하게 하며, 상기 처리가 상기 이미지 시퀀스의 단안 이미지 처리를 포 함하는 것을 포함한다. 예 154에서, 예 153의 요지는 상기 이미지 시퀀스가 상기 카메라 시스템의 복수의 이미지 센서에 의해 캡처되는 것을 포함한다. 예 155에서, 예 152-154의 요지는, 상기 도로 상의 퍼들을 탐지하기 위해 상기 이미지 시퀀스를 처리하는 인스 트럭션이: 상기 이미지 시퀀스에 기반하여 상기 도로의 수직 윤곽을 계산적으로 결정하고; 상기 수직 윤곽에서 심도 임계치를 초과하는 함몰부의 존재를 결정하고; 그리고 상기 함몰부의 특징에 대해 퍼들 탐지 기준을 적용 하기 위한 인스트럭션을 포함하는 것을 포함한다. 예 156에서, 예 155의 요지는, 퍼들 탐지 기준이 경계 첨예도 기준을 포함하고, 퍼들 탐지 기준을 적용하는 것 은 상기 퍼들의 경계의 계산적 평가치를 생성하는 것 및 상기 계산적 평가치를 상기 경계 첨예도 기준에 대해 비교하는 것을 포함하는 것을 포함한다. 예 157에서, 예 156의 요지는, 상기 퍼들의 경계의 계산적 평가치가 상기 경계를 따라 상기 퍼들 내부의 포인트 들의 수직 윤곽 스코어링, 및 상기 경계를 따라 상기 퍼들 외부의 포인트들의 수직 윤곽 스코어링을 포함하고, 상기 경계 첨예도 기준이 상기 퍼들 내부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 제 1 임계치, 및 상 기 퍼들 외부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 제 2 임계치를 포함하는 것을 포함한다. 예 158에서, 예 155-157의 요지는, 상기 퍼들 탐지 기준은 상기 퍼들 내부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 인접성 기준을 포함하는 것을 포함한다. 예 159에서, 예 152-158의 요지는, 상기 퍼들 부근의 보행자의 임의의 존재를 결정하는 인스트럭션이 인간을 탐 지하도록 구성된 물체 인식 동작을 수행하기 위해 상기 이미지 시퀀스를 처리하기 위한 인스트럭션을 포함하는 것을 포함한다. 예 160에서, 예 152-159의 요지는, 상기 퍼들의 탐지에 응답하여 주행 응답 솔루션을 결정하는 인스트럭션이 상 기 퍼들 부근의 보행자의 긍정적인 탐지에 응답하여 상기 퍼들로 보행자를 스플래시할 가능성을 감소시키거나 스플래시를 회피하기 위해 상기 차량에 대한 코스 또는 속도 응답을 생성하기 위한 인스트럭션을 포함하는 것을 포함한다. 예 161에서, 예 160의 요지는, 상기 주행 응답 솔루션을 결정하는 인스트럭션이 위험 회피 결정 방식에 기반하 여 복수의 잠재적 주행 응답 옵션 중에서 상기 주행 응답 솔루션을 선택하기 위한 인스트럭션을 포함하는 것을 포함한다. 예 162에서, 예 161의 요지는, 상기 퍼들 부근의 보행자의 긍정적인 탐지가 위험 회피 결정 방식에 대한 증가 위험 요인으로서 표현되며, 상기 증가 위험 요인은 상기 퍼들과 연관된 총 위험 스코어에 기여하며, 상기 총 위 험 스코어는 상기 차량이 상기 주행 응답 솔루션을 수행할 필요의 정도를 나타내는 것을 포함한다. 예 163에서, 예 161-162의 요지는, 상기 퍼들의 탐지에 응답하여 주행 응답 솔루션을 결정하는 인스트럭션이 잠 재적 주행 응답 옵션의 각각과 연관된 위험을 해당 잠재적 주행 응답 옵션이 위험 요인에 기여하는 현재 상황 시나리오의 위험 평가치에 대해 위한 명령어를 비교하기 위한 인스트럭션을 포함하는 것을 포함한다. 예 164에서, 예 163의 요지는 상기 현재 상황 시나리오의 평가를 수행하기 위한 인스트럭션을 포함한다. 예 165는 차량에 사용하기 위한 머신 비전 시스템을 동작시키기 위한 방법이며, 이 방법은: 상기 머신 비전 시 스템에 의해, 적어도 하나의 차량 탑재 카메라에 의해 캡처된 도로의 일부를 포함하는 적어도 하나의 시야를 나 타내는 시간적인 이미지 시퀀스를 판독하는 단계; 상기 머신 비전 시스템에 의해, 상기 시간적인 이미지 시퀀스 에 기반하여 상기 도로의 수직 윤곽을 계산적으로 결정하는 단계; 및 상기 머신 비전 시스템에 의해, 상기 도로 의 수직 윤곽에 기반하여 상기 도로 상의 퍼들을 탐지하는 단계 - 이 단계는 상기 수직 윤곽에서 심도 임계치를 초과하는 함몰부의 존재를 결정하는 단계; 및 상기 함몰부의 특징에 대해 퍼들 탐지 기준을 적용하는 단계를 포 함함 -를 포함한다. 예 166에서, 예 165의 요지는, 수직 윤곽을 결정하는 단계가 (a) 상기 이미지 시퀀스 중에서 상기 차량의 실제 모션으로 인한 도로의 외관에서의 예측된 변화와 (b) 캡처된 이미지 시퀀스의 이미지들 간의 실제 차이를 비교 하는 단계를 포함하고, 상기 비교 단계의 결과로서 탐지된 임의의 차이는 상기 이미지 시퀀스의 이미지들 간의 잔차 흐름을 나타내고, 상기 잔차 흐름은 상기 도로의 수직 윤곽을 나타내는 것을 포함한다. 예 167에서, 예 165-166의 요지는, 상기 퍼들 탐지 기준이 경계 첨예도 기준을 포함하고, 상기 퍼들 탐지 기준 을 적용하는 단계는 상기 퍼들의 경계의 계산적 평가치를 생성하는 단계 및 상기 계산적 평가치를 상기 경계 첨 예도 기준에 대해 비교하는 단계를 포함하는 것을 포함한다. 예 168에서, 예 167의 요지는, 상기 퍼들의 경계의 계산적 평가치가 상기 경계를 따라 상기 퍼들 내부의 포인트 들의 수직 윤곽 스코어링, 및 상기 경계를 따라 상기 퍼들 외부의 포인트들의 수직 윤곽 스코어링을 포함하고, 상기 경계 첨예도 기준이 상기 퍼들 내부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 제 1 임계치, 및 상 기 퍼들 외부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 제 2 임계치를 포함하는 것을 포함한다. 예 169에서, 예 167-168의 요지는, 상기 퍼들 탐지 기준은 상기 퍼들 내부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 인접성 기준을 포함하는 것을 포함한다. 예 170에서, 예 165-169의 요지는, 상기 시간적인 이미지 시퀀스가 단일 카메라로부터 획득되는 것을 포함한다. 예 171에서, 예 165-170의 요지는, 상기 시간적인 이미지 시퀀스가 복수의 카메라로부터 획득되는 것을 포함한 다. 예 172에서, 예 165-171의 요지는 상기 머신 비전 시스템에 의해, 상기 퍼들에 관한 현재 상황 시나리오를 결정 하는 단계를 포함한다. 예 173에서, 예 172의 요지는 상기 현재 상황 시나리오가 상기 퍼들 부근의 보행자의 임의의 존재를 포함하는 것을 포함한다. 예 174에서, 예 172-173의 요지는, 상기 현재 상황 시나리오가 도로 경계에 대한 상기 퍼들의 위치를 포함하는 것을 포함한다. 예 175에서, 예 172 내지 174의 요지는 상기 현재 상황 시나리오가 상기 도로 상의 차선 마커에 대한 상기 퍼들 의 위치를 포함하는 것을 포함한다. 예 176에서, 예 172-175의 요지는 상기 머신 비전 시스템에 의해, 상기 퍼들의 탐지에 응답하여 주행 응답 솔루 션을 결정하는 단계를 포함하며, 상기 주행 응답 솔루션은 상기 현재 상황 시나리오에 기반한다. 예 177에서, 예 176의 요지는, 상기 주행 응답 솔루션이 상기 차량이 퍼들과 부딪히는 임의의 충격을 회피하거 나 감소시키기 위한 차량에 대한 코스 또는 속도 응답을 포함하는 것을 포함한다. 예 178에서, 예 176-177의 요지는, 상기 주행 응답 솔루션은 위험 회피 결정 방식에 기반하여 복수의 잠재적 주 행 응답 옵션 중에서 상기 주행 응답 솔루션을 선택하는 것을 포함하는 것을 포함한다. 예 179에서, 예 178의 요지는, 상기 위험 회피 결정 방식이 상기 퍼들과 연관된 위험 스코어를 계산하며, 상기 위험 스코어는 상기 차량이 주행 응답 솔루션을 수행하기 위한 필요의 정도를 나타내는 것을 포함한다. 예 180에서, 예 178-179의 요지는, 상기 주행 응답 솔루션이 잠재적 주행 응답 옵션의 각각과 연관된 위험을 해 당 잠재적 주행 응답 옵션이 위험 요인에 기여하는 현재 상황 시나리오의 위험 평가치에 대해 비교한 것에 기반 하는 것을 포함한다. 예 181은 인스트럭션을 포함한 적어도 하나의 머신 판독 가능 매체이며, 이 인스트럭션은 도로를 따라 주행하는 자율 주행 차량의 머신 비전 시스템에 의해 실행될 때, 상기 머신 비전 시스템으로 하여금 동작을 수행하게 하 며, 상기 동작은: 적어도 하나의 차량 탑재 카메라에 의해 캡처된 도로의 일부를 포함하는 적어도 하나의 시야 를 나타내는 시간적인 이미지 시퀀스를 판독하는 단계; 상기 시간적인 이미지 시퀀스에 기반하여 상기 도로의 수직 윤곽을 계산적으로 결정하는 단계; 및 상기 도로의 수직 윤곽에 기반하여 상기 도로 상의 퍼들을 탐지하는 단계 - 이 단계는 상기 수직 윤곽에서 심도 임계치를 초과하는 함몰부의 존재를 결정하는 단계; 및 상기 함몰부 의 특징에 대해 퍼들 탐지 기준을 적용하는 단계를 포함함 -를 포함한다. 예 182에서, 예 181의 요지는, 수직 윤곽을 결정하는 인스트럭션이 (a) 상기 이미지 시퀀스 중에서 상기 차량의 실제 모션으로 인한 도로의 외관에서의 예측된 변화와 (b) 캡처된 이미지 시퀀스의 이미지들 간의 실제 차이를 비교하기 위한 인스트럭션을 포함하고, 상기 비교 단계의 결과로서 탐지된 임의의 차이는 상기 이미지 시퀀스의 이미지들 간의 잔차 흐름을 나타내고, 상기 잔차 흐름은 상기 도로의 수직 윤곽을 나타내는 것을 포함한다. 예 183에서, 예 181-182의 요지는, 상기 퍼들 탐지 기준이 경계 첨예도 기준을 포함하고, 상기 퍼들 탐지 기준 을 적용하는 단계는 상기 퍼들의 경계의 계산적 평가치를 생성하는 단계 및 상기 계산적 평가치를 상기 경계 첨 예도 기준에 대해 비교하는 단계를 포함하는 것을 포함한다. 예 184에서, 예 183의 요지는, 상기 퍼들의 경계의 계산적 평가치가 상기 경계를 따라 상기 퍼들 내부의 포인트 들의 수직 윤곽 스코어링, 및 상기 경계를 따라 상기 퍼들 외부의 포인트들의 수직 윤곽 스코어링을 포함하고, 상기 경계 첨예도 기준이 상기 퍼들 내부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 제 1 임계치, 및 상 기 퍼들 외부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 제 2 임계치를 포함하는 것을 포함한다. 예 185에서, 예 183-184의 요지는, 상기 퍼들 탐지 기준은 상기 퍼들 내부의 포인트들의 수직 윤곽 스코어링에 적용 가능한 인접성 기준을 포함하는 것을 포함한다. 예 186에서, 예 181-185의 요지는, 상기 시간적인 이미지 시퀀스가 단일 카메라로부터 획득되는 것을 포함한다. 예 187에서, 예 181-186의 요지는, 상기 시간적인 이미지 시퀀스가 복수의 카메라로부터 획득되는 것을 포함한 다. 예 188에서, 예 181-187의 요지는 상기 머신 비전 시스템으로 하여금 상기 퍼들에 관한 현재 상황 시나리오를 결정하게 하기 위한 인스트럭션을 포함한다. 예 189에서, 예 188의 요지는 상기 현재 상황 시나리오가 상기 퍼들 부근의 보행자의 임의의 존재를 포함하는 것을 포함한다. 예 190에서, 예 188-189의 요지는, 상기 현재 상황 시나리오가 도로 경계에 대한 상기 퍼들의 위치를 포함하는 것을 포함한다. 예 191에서, 예 188-190의 요지는 상기 현재 상황 시나리오가 상기 도로 상의 차선 마커에 대한 상기 퍼들의 위 치를 포함하는 것을 포함한다. 예 192에서, 예 188-191의 요지는 상기 머신 비전 시스템으로 하여금 상기 퍼들의 탐지에 응답하여 주행 응답 솔루션을 결정하게 하기 위한 인스트럭션을 포함하며, 상기 주행 응답 솔루션은 상기 현재 상황 시나리오에 기 반한다. 예 193에서, 예 192의 요지는, 상기 주행 응답 솔루션이 상기 차량이 퍼들과 부딪히는 임의의 충격을 회피하거 나 감소시키기 위한 차량에 대한 코스 또는 속도 응답을 포함하는 것을 포함한다. 예 194에서, 예 192-193의 요지는, 상기 주행 응답 솔루션은 위험 회피 결정 방식에 기반하여 복수의 잠재적 주 행 응답 옵션 중에서 상기 주행 응답 솔루션을 선택하는 것을 포함하는 것을 포함한다. 예 195에서, 예 194의 요지는, 상기 위험 회피 결정 방식이 상기 퍼들과 연관된 위험 스코어를 계산하며, 상기 위험 스코어는 상기 차량이 주행 응답 솔루션을 수행하기 위한 필요의 정도를 나타내는 것을 포함한다. 예 196에서, 예 194-195의 요지는, 상기 주행 응답 솔루션이 잠재적 주행 응답 옵션의 각각과 연관된 위험을 해 당 잠재적 주행 응답 옵션이 위험 요인에 기여하는 현재 상황 시나리오의 위험 평가치에 대해 비교한 것에 기반 하는 것을 포함한다. 예 197은 도로를 따라 주행하는 차량으로부터 도로의 구조를 측정하기 위한 머신으로 구현되는 방법이며, 이 방 법은: (a) 적어도 하나의 차량 탑재 카메라에 의해 캡처된 상기 도로의 일부를 포함하는 적어도 하나의 시야를 나타내는 시간적인 캡처된 이미지 시퀀스, (b) 상기 적어도 하나의 차량 탑재 카메라의 실제 모션을 나타내는 에고 모션 정보, 및 (c) 상기 도로의 표면의 파라메트릭 모델을 포함하는 입력 데이터 세트를 판독하는 단계; 상기 입력 데이터 세트를 전처리하여, 도로 평면 정보에 대한 상기 시간적인 캡처된 이미지 시퀀스 중에서 적어 도 하나의 호모그래피를 결정하고, 상기 시간적인 캡처된 이미지 시퀀스 중 적어도 하나의 이미지를 상기 적어 도 하나의 호모그래피에 기반하여 상기 시간적인 캡처된 이미지 시퀀스의 다른 이미지와 정렬시키도록 와핑하여, 상기 시간적인 캡처된 이미지 시퀀스에 기반한 전처리된 이미지 세트를 생성하는 단계; 상기 입력 데 이터 세트를 전처리하여 상기 적어도 하나의 차량 탑재 카메라의 이미지 포맷된 모션 표현을 생성하는 단계; 및 상기 전처리된 이미지 세트 및 상기 이미지 포맷된 모션 표현을 입력으로서 머신 러닝(ML) 시스템에 제공하는 제공하는 단계 - 상기 ML 시스템은 상기 입력에 기반하여 상기 도로의 구조를 나타내는 맵을 생성하도록 구성됨 -를 포함한다. 예 198에서, 예 197의 요지는, 적어도 하나의 시야가 단일 카메라에 의해 캡처된 단일의 전방을 향한 시야를 포 함하는 것을 포함한다. 예 199에서, 예 197-198의 요지는, 상기 적어도 하나의 시야가 복수의 카메라에 의해 캡처된 전방을 향한 시야 를 포함하는 것을 포함한다. 예 200에서, 예 197-199의 요지는, 상기 적어도 하나의 시야는 상이한 방향을 향하는 복수의 카메라에 의해 캡 처된 복수의 중첩된 시야를 포함하는 것을 포함한다. 예 201에서, 예 197-200의 요지는, 상기 도로의 표면의 파라메트릭 모델이 상기 도로의 표면에 수직인 평면 법 선 벡터를 나타내는 도로 평면 정보를 포함하는 것을 포함한다. 예 202에서, 예 197-201의 요지는 상기 도로의 표면의 파라메트릭 모델이 상기 도로의 3 차원 스플라인 모델을 포함하는 것을 포함한다. 예 203에서, 예 197-202의 요지는, 상기 도로의 표면의 파라메트릭 모델이 상기 캡처된 이미지의 적어도 일부로 부터 도출된 도로 평면 정보를 포함하는 것을 포함한다. 예 204에서, 예 197-203의 요지는 상기 시간적인 캡처된 이미지 시퀀스가 가장 최근에 캡처된 이미지, 이전 이 미지, 및 이전-이전 이미지를 포함하는 3 개의 연속 이미지의 시퀀스를 포함하는 것을 포함한다. 예 205에서, 예 197-204의 요지는, 상기 입력 데이터 세트를 전처리하여 상기 적어도 하나의 차량 탑재 카메라 의 이미지 포맷된 모션 표현을 생성하는 것이 상기 시간적인 캡처된 이미지 시퀀스의 적어도 하나의 이미지에 대응하는 이미지 포맷된 에피폴 표현을 생성하는 것을 포함하는 것을 포함한다. 예 206에서, 예 197-205의 요지는, 상기 입력 데이터 세트를 전처리하여 상기 적어도 하나의 차량 탑재 카메라 의 이미지 포맷된 모션 표현을 생성하는 것이 상기 카메라에 대한 전진 방향을 따른 병진이동의 현재 측정치의 비율의 이미지 포맷된 표현을 생성하는 것을 포함하는 것을 포함한다. 예 207에서, 예 197-206의 요지는, 상기 에고 모션 정보가 상기 차량의 모션 센서에 의해 획득된 측정치에 기반 한 것을 포함한다. 예 208에서, 예 197-207의 요지는, 상기 에고 모션 정보가 상기 차량의 회전 및 병진이동 움직임을 나타내는 것 을 포함한다. 예 209에서, 예 197-208의 요지는, 상기 ML 시스템에 의해 생성될 도로의 구조를 나타내는 맵이 노면 높이에 기 반한 값의 맵핑을 포함하는 도로 모델을 포함하는 것을 포함한다. 예 210에서, 예 209의 요지는, 상기 노면 높이에 기반한 값의 매핑이 관측 포인트로부터의 거리에 대한 노면 높 이의 비율을 포함하는 것을 포함한다. 예 211은 도로를 따라 주행하는 차량으로부터 상기 도로의 구조를 측정하기 위한 인스트럭션을 포함한 적어도 하나의 머신 판독 가능 매체이며, 상기 인스트럭션은 처리 회로부에 의해 실행될 때, 상기 처리 회로부로 하여 금 동작을 수행하게 하며, 상기 동작은: (a) 적어도 하나의 차량 탑재 카메라에 의해 캡처된 상기 도로의 일부 를 포함하는 적어도 하나의 시야를 나타내는 시간적인 캡처된 이미지 시퀀스, (b) 상기 적어도 하나의 차량 탑 재 카메라의 실제 모션을 나타내는 에고 모션 정보, 및 (c) 상기 도로의 표면의 파라메트릭 모델을 포함하는 입 력 데이터 세트를 판독하는 단계; 상기 입력 데이터 세트를 전처리하여, 도로 평면 정보에 대한 상기 시간적인 캡처된 이미지 시퀀스 중에서 적어도 하나의 호모그래피를 결정하고, 상기 시간적인 캡처된 이미지 시퀀스 중 적어도 하나의 이미지를 상기 적어도 하나의 호모그래피에 기반하여 상기 시간적인 캡처된 이미지 시퀀스의 다 른 이미지와 정렬시키도록 와핑하여, 상기 시간적인 캡처된 이미지 시퀀스에 기반한 전처리된 이미지 세트를 생 성하는 단계; 상기 입력 데이터 세트를 전처리하여 상기 적어도 하나의 차량 탑재 카메라의 이미지 포맷된 모션 표현을 생성하는 단계; 및 상기 전처리된 이미지 세트 및 상기 이미지 포맷된 모션 표현을 입력으로서 머신 러 닝(ML) 시스템에 제공하는 제공하는 단계 - 상기 ML 시스템은 상기 입력에 기반하여 상기 도로의 구조를 나타내 는 맵을 생성하도록 구성됨 -를 포함한다. 예 212에서, 예 211의 요지는, 상기 적어도 하나의 시야가 단일 카메라에 의해 캡처된 단일의 전방을 향한 시야 를 포함하는 것을 포함한다. 예 213에서, 예 211-212의 요지는, 상기 적어도 하나의 시야가 복수의 카메라에 의해 캡처된 전방을 향한 시야 를 포함하는 것을 포함한다. 예 214에서, 예 211-213의 요지는, 상기 적어도 하나의 시야는 상이한 방향을 향하는 복수의 카메라에 의해 캡 처된 복수의 중첩된 시야를 포함하는 것을 포함한다. 예 215에서, 예 211-214의 요지는, 상기 도로의 표면의 파라메트릭 모델이 상기 도로의 표면에 수직인 평면 법 선 벡터를 나타내는 도로 평면 정보를 포함하는 것을 포함한다. 예 216에서, 예 211-215의 요지는 상기 도로의 표면의 파라메트릭 모델이 상기 도로의 3 차원 스플라인 모델을 포함하는 것을 포함한다. 예 217에서, 예 211-216의 요지는, 상기 도로의 표면의 파라메트릭 모델이 상기 캡처된 이미지의 적어도 일부로 부터 도출된 도로 평면 정보를 포함하는 것을 포함한다. 예 218에서, 예 211-217의 요지는 상기 시간적인 캡처된 이미지 시퀀스가 가장 최근에 캡처된 이미지, 이전 이 미지, 및 이전-이전 이미지를 포함하는 3 개의 연속 이미지의 시퀀스를 포함하는 것을 포함한다. 예 219에서, 예 211-218의 요지는, 상기 입력 데이터 세트를 전처리하여 상기 적어도 하나의 차량 탑재 카메라 의 이미지 포맷된 모션 표현을 생성하는 것이 상기 시간적인 캡처된 이미지 시퀀스의 적어도 하나의 이미지에 대응하는 이미지 포맷된 에피폴 표현을 생성하는 것을 포함하는 것을 포함한다. 예 220에서, 예 211-219의 요지는, 상기 입력 데이터 세트를 전처리하여 상기 적어도 하나의 차량 탑재 카메라 의 이미지 포맷된 모션 표현을 생성하는 것이 상기 카메라에 대한 전진 방향을 따른 병진이동의 현재 측정치의 비율의 이미지 포맷된 표현을 생성하는 것을 포함하는 것을 포함한다. 예 221에서, 예 211-220의 요지는, 상기 에고 모션 정보가 상기 차량의 모션 센서에 의해 획득된 측정치에 기반 한 것을 포함한다. 예 222에서, 예 211-221의 요지는, 상기 에고 모션 정보가 상기 차량의 회전 및 병진이동 움직임을 나타내는 것 을 포함한다. 예 223에서, 예 211-222의 요지는, 상기 ML 시스템에 의해 생성될 도로의 구조를 나타내는 맵이 노면 높이에 기 반한 값의 맵핑을 포함하는 도로 모델을 포함하는 것을 포함한다. 예 224에서, 예 223의 요지는, 상기 노면 높이에 기반한 값의 매핑이 관측 포인트로부터의 거리에 대한 노면 높 이의 비율을 포함하는 것을 포함한다. 예 225은 도로를 따라 주행하는 차량으로부터 상기 도로의 구조를 측정하기 위한 시스템이며, 이 시스템은: (a) 적어도 하나의 차량 탑재 카메라에 의해 캡처된 상기 도로의 일부를 포함하는 적어도 하나의 시야를 나타내는시간적인 캡처된 이미지 시퀀스, (b) 상기 적어도 하나의 차량 탑재 카메라의 실제 모션을 나타내는 에고 모션 정보, 및 (c) 상기 도로의 표면의 파라메트릭 모델을 포함하는 입력 데이터 세트를 판독하는 수단; 상기 입력 데이터 세트를 전처리하여, 도로 평면 정보에 대한 상기 시간적인 캡처된 이미지 시퀀스 중에서 적어도 하나의 호모그래피를 결정하고, 상기 시간적인 캡처된 이미지 시퀀스 중 적어도 하나의 이미지를 상기 적어도 하나의 호모그래피에 기반하여 상기 시간적인 캡처된 이미지 시퀀스의 다른 이미지와 정렬시키도록 와핑하여, 상기 시 간적인 캡처된 이미지 시퀀스에 기반한 전처리된 이미지 세트를 생성하는 수단; 상기 입력 데이터 세트를 전처 리하여 상기 적어도 하나의 차량 탑재 카메라의 이미지 포맷된 모션 표현을 생성하는 수단; 및 상기 전처리된 이미지 세트 및 상기 이미지 포맷된 모션 표현을 입력으로서 머신 러닝(ML) 시스템에 제공하는 제공하는 수단 - 상기 ML 시스템은 상기 입력에 기반하여 상기 도로의 구조를 나타내는 맵을 생성하도록 구성됨 -을 포함한다. 예 226에서, 예 225의 요지는, 상기 적어도 하나의 시야가 단일 카메라에 의해 캡처된 단일의 전방을 향한 시야 를 포함하는 것을 포함한다. 예 227에서, 예 225-226의 요지는, 상기 적어도 하나의 시야가 복수의 카메라에 의해 캡처된 전방을 향한 시야 를 포함하는 것을 포함한다. 예 228에서, 예 225-227의 요지는, 상기 적어도 하나의 시야는 상이한 방향을 향하는 복수의 카메라에 의해 캡 처된 복수의 중첩된 시야를 포함하는 것을 포함한다. 예 229에서, 예 225-228의 요지는, 상기 도로의 표면의 파라메트릭 모델이 상기 도로의 표면에 수직인 평면 법 선 벡터를 나타내는 도로 평면 정보를 포함하는 것을 포함한다. 예 230에서, 예 225-229의 요지는 상기 도로의 표면의 파라메트릭 모델이 상기 도로의 3 차원 스플라인 모델을 포함하는 것을 포함한다. 예 231에서, 예 225-230의 요지는, 상기 도로의 표면의 파라메트릭 모델이 상기 캡처된 이미지의 적어도 일부로 부터 도출된 도로 평면 정보를 포함하는 것을 포함한다. 예 232에서, 예 225-231의 요지는 상기 시간적인 캡처된 이미지 시퀀스가 가장 최근에 캡처된 이미지, 이전 이 미지, 및 이전-이전 이미지를 포함하는 3 개의 연속 이미지의 시퀀스를 포함하는 것을 포함한다. 예 233에서, 예 225-232의 요지는, 상기 입력 데이터 세트를 전처리하여 상기 적어도 하나의 차량 탑재 카메라 의 이미지 포맷된 모션 표현을 생성하는 수단이 상기 시간적인 캡처된 이미지 시퀀스의 적어도 하나의 이미지에 대응하는 이미지 포맷된 에피폴 표현을 생성하는 수단을 포함하는 것을 포함한다. 예 234에서, 예 225-233의 요지는, 상기 입력 데이터 세트를 전처리하여 상기 적어도 하나의 차량 탑재 카메라 의 이미지 포맷된 모션 표현을 생성하는 수단이 상기 카메라에 대한 전진 방향을 따른 병진이동의 현재 측정치 의 비율의 이미지 포맷된 표현을 생성하는 수단을 포함하는 것을 포함한다. 예 235에서, 예 225-234의 요지는, 상기 에고 모션 정보가 상기 차량의 모션 센서에 의해 획득된 측정치에 기반 한 것을 포함한다. 예 236에서, 예 225-235의 요지는, 상기 에고 모션 정보가 상기 차량의 회전 및 병진이동 움직임을 나타내는 것 을 포함한다. 예 237에서, 예 225-236의 요지는, 상기 ML 시스템에 의해 생성될 도로의 구조를 나타내는 맵이 노면 높이에 기 반한 값의 맵핑을 포함하는 도로 모델을 포함하는 것을 포함한다. 예 238에서, 예 237의 요지는, 상기 노면 높이에 기반한 값의 매핑이 관측 포인트로부터의 거리에 대한 노면 높 이의 비율을 포함하는 것을 포함한다. 예 239는 도로를 따라 주행하는 차량에 사용하기 위한 머신 비전 시스템을 위한 장치이며, 이 장치는: 적어도 하나의 차량 탑재 카메라에 의해 캡처된 상기 도로의 일부를 포함하는 적어도 하나의 시야를 나타내는 시간적 이미지 시퀀스를 판독하기 위한 입력부; 및 상기 입력부에 결합된 도로 구조 측정 시스템을 포함하고, 상기 도 로 구조 측정 시스템은 상기 시간적 이미지 시퀀스에 기반하여 이미지 포맷된 입력을 수신하도록 구성된 제 1 심층 신경 네트워크(DNN)를 포함하며, 상기 제 1 DNN은 트레이닝 프로세스로부터 도출된 계산 파라미터로 구성 되며, 캡처된 시간적인 트레이닝 이미지 시퀀스 및 대응하는 에고 모션 측정치를 갖는 제 1 부분을 포함하는 트 레이닝 데이터는 트레이닝 DNN에 입력되고, 상기 트레이닝 DNN은 상기 제 1 부분의 전진방향 전파 처리에 기반 한 테스트 결과를 생성하며, 상기 전진방향 전파 처리는 상기 트레이닝 DNN에 의한 트레이닝 가능한 계산 파라미터의 적용을 포함하며; 상기 트레이닝 데이터의 상기 제 1 부분에 기반한 제 1 손실 컴포넌트 및 상기 제 1 부분에서는 존재하지 않는 적어도 하나의 캡처된 이미지 또는 에고 모션 측정치를 갖는 트레이닝 데이터의 제 2 부분에 기반한 제 2 손실 컴포넌트를 포함하는 복수의 개별 손실 컴포넌트의 집계에 기반하여 멀티-모드 손실 함수가 생성되고; 상기 멀티-모드 손실 함수는 상기 테스트 결과에 기반하여 평가되어 손실 값을 생성하고, 상 기 트레이닝 가능한 계산 파라미터는 트레이닝 프로세스에 따라 손실 값을 감소시키도록 정제되고; 정제된 트레 이닝 가능한 파라미터는 상기 제 1 DNN을 구성하도록 제공된다. 예 240에서, 예 239의 요지는, 상기 트레이닝 프로세스가 그래디언트 하강 트레이닝을 이용하는 트레이닝 DNN을 통해 손실 함수를 역 전파시키는 것을 포함하는 것을 포함한다. 예 241에서, 예 239-240의 요지는, 상기 제 1 DNN에 의해 수신될 이미지 포맷된 입력이: 시간적 이미지 시퀀스; 이미지로서 포맷된 에피폴 정보 - 상기 에피폴 정보는 에피폴로부터의 제각기의 거리를 나타내는 포인트들을 포 함함 -; 및 이미지로서 포맷된 모션 정보를 포함하고, 상기 모션 정보는 카메라 높이에 대한 전진 방향을 따른 병진이동의 현재 측정치의 비율을 나타내는 포인트들을 포함한다. 예 242에서, 예 239-241의 요지는, 상기 제 1 DNN이 컨볼루션, 활성화 및 풀링 계층을 구비한 복수의 계층을 포 함하는 컨볼루션 DNN이며; 상기 제 1 DNN은 제 1 입력 계층 및 상기 제 1 입력 계층과는 상이한 제 2 입력 계층 을 구비한 상이한 계층을 공급하는 복수의 입력 포트를 포함하고; 제 1 이미지 포맷된 입력은 상기 제 1 입력 계층에 제공되고, 제 2 이미지 포맷된 입력은 상기 제 2 입력 계층에 제공되는 것을 포함한다. 예 243에서, 예 242의 요지는, 상기 제 1 입력이 시간적 이미지 시퀀스를 포함하고, 상기 제 2 입력이 상기 차 량의 모션을 나타내는 이미지 포맷된 모션 정보를 포함하는 것을 포함한다. 예 244에서, 예 239-243의 요지는, 상기 제 1 DNN이 컨볼루션 부분 및 디컨볼루션 부분을 포함하는 컨볼루션 DNN인 것을 포함한다. 예 245에서, 예 244의 요지는, 제 1 DNN이 복수의 계층들을 포함하고, 각 계층은 특징부 맵을 생성하고, 상기 특징부 맵을 전진방향 전파 경로를 따른 처리를 위해 후속 계층으로 전진방향으로 전달하고; 상기 컨볼루션 부 분의 연속적인 계층들은 전진방향 전파 경로를 따라 그들의 대응하는 특징부 맵의 차원을 증가시키면서 그 대응 하는 특징부 맵의 해상도를 점진적으로 감소시키도록 동작하고; 디컨볼루션 부분의 연속적인 계층들은 상기 전 진방향 전파 경로를 따라 그들의 대응하는 특징부 맵의 차원을 감소시키면서 그 대응하는 특징부 맵의 해상도를 점진적으로 증가시키도록 동작하는 것을 포함한다. 예 246에서, 예 245의 요지는, 상기 제 1 DNN이 선행 계층과 후행 계층 사이에 위치한 하나 이상의 중개 계층을 건너 뛰면서 전진방향 전파 경로를 따라 선행 계층으로부터 후행 계층으로 특징부 맵의 전달을 가능하게 하도록 배열된 적어도 하나의 바이패스 경로를 포함하는 것을 포함한다. 예 247에서, 예 245-246의 요지는, 상기 제 1 DNN이 컨볼루션 부분과 디컨볼루션 부분 사이에 위치된 병목 네트 워크 부분을 포함하고, 상기 병목 네트워크 부분은 제 1 DNN의 다른 계층들과 비교하여 상대적으로 낮은 해상도 및 높은 차원을 갖는 적어도 하나의 계층을 포함하는 것을 포함한다. 예 248에서, 예 247의 요지는, 상기 병목 네트워크 부분이 이미지 포맷된 모션 표시 및 이미지 포맷된 에피폴 위치 데이터를 수용하도록 구성된 입력부를 포함하는 것을 포함한다. 예 249에서, 예 239-248의 요지는, 상기 제 1 DNN이, 노면 높이에 기반한 값의 매핑을 포함한 도로 모델을 포함 하여 도로의 구조를 나타내는 맵을 출력으로서 생성하는 것을 포함한다. 예 250에서, 예 248-249의 요지는, 상기 노면 높이에 기반한 값의 매핑이 관측 포인트로부터의 거리에 대한 노 면 높이의 비율을 포함하는 것을 포함한다. 예 251에서, 예 239-250의 요지는, 상기 제 1 DNN이 상기 트레이닝 DNN과 동일한 아키텍처를 갖는 것을 포함한 다. 예 252에서, 예 239-251의 요지는, 상기 제 1 손실 컴포넌트가 시간적인 트레이닝 이미지 시퀀스 및 상기 테스 트 결과가 생성되는 대응 에고 모션 측정치에 기반한 사진 측량 손실 컴포넌트를 포함하는 것을 포함한다. 예 253에서, 예 252의 요지는, 상기 제 2 손실 컴포넌트가 상기 시간적인 트레이닝 이미지 시퀀스의 이미지들 중 임의의 이미지가 캡처되었던 시간과는 상이한 시간에 캡처되는 적어도 하나의 과거 또는 미래의 트레이닝 이 미지에 기반하는 것을 포함한다. 예 254에서, 예 252-253의 요지는, 상기 제 2 손실 컴포넌트가 상기 테스트 결과가 생성된 에고 모션 측정치 중 임의의 측정치가 획득되었던 시간과는 상이한 시간에 취해진 적어도 하나의 과거 또는 미래의 에고 모션 측정치 에 기반하는 것을 포함한다. 예 255에서, 예 252-254의 요지는, 상기 제 2 손실 컴포넌트가 캡처된 이미지 및 상기 시간적인 트레이닝 이미 지 시퀀스의 임의의 이미지가 캡처되었던 시간보다는 나중의 시간에 캡처된 트레이닝 데이터의 부분으로부터 획 득된 대응하는 측정된 에고 모션에 기반한 적어도 하나의 미래 도로 구조 평가치에 기반하는 것을 포함한다. 예 256은 젖은 도로를 따라 주행하는 차량에 사용하기 위한 머신 비전 시스템을 위한 장치이며, 이 장치는: 적 어도 하나의 차량 탑재 카메라에 의해 캡처된 상기 도로의 일부를 포함하는 적어도 하나의 시야를 나타내는 시 간적인 이미지 시퀀스를 판독하기 위한 입력부; 및 상기 입력부에 결합된 도로 구조 측정 시스템을 포함하고, 상기 도로 구조 측정 시스템은 상기 시간적인 이미지 시퀀스에 기반한 이미지 포맷된 입력을 수신하도록 구성된 제 1 심층 신경 네트워크(DNN)를 포함하고, 여기서, 상기 시간적인 이미지 시퀀스 중 적어도 하나의 제 1 이미 지가 호모그래피에 따라 와핑되고 상기 시간적인 이미지 시퀀스 중 제 2 이미지와 정렬되어 상기 이미지 시퀀스 의 제 1 이미지와 제 2 이미지 간의 잔차 모션을 노출할 때, 잔차 모션은 적어도 (a) 노면 지형의 변화, 및 (b) 상기 노면으로부터의 정반사를 나타내며; 상기 도로 구조 측정 시스템은 상기 차량이 시간당 50 km의 속도로 주 행할 때, 상기 노면 지형의 변화를 상기 노면으로부터의 정반사와 구별하여, 적어도 10 미터의 거리에서 3 센티 미터의 노면 높이 변화 내로 정확한 지형 측정치를 생성하도록 구성된다. 예 257에서, 예 256의 요지는, 상기 제 1 DNN이 컨볼루션 부분 및 디컨볼루션 부분을 포함하는 컨볼루션 DNN인 것을 포함한다. 예 258에서, 예 257의 요지는, 제 1 DNN이 복수의 계층들을 포함하고, 각 계층은 특징부 맵을 생성하고, 상기 특징부 맵을 전진방향 전파 경로를 따른 처리를 위해 후속 계층으로 전진방향으로 전달하고; 상기 컨볼루션 부 분의 연속적인 계층들은 전진방향 전파 경로를 따라 그들의 대응하는 특징부 맵의 차원을 증가시키면서 그 대응 하는 특징부 맵의 해상도를 점진적으로 감소시키도록 동작하고; 디컨볼루션 부분의 연속적인 계층들은 상기 전 진방향 전파 경로를 따라 그들의 대응하는 특징부 맵의 차원을 감소시키면서 그 대응하는 특징부 맵의 해상도를 점진적으로 증가시키도록 동작하는 것을 포함한다. 예 259에서, 예 258의 요지는, 상기 제 1 DNN이 선행 계층과 후행 계층 사이에 위치한 하나 이상의 중개 계층을 건너 뛰면서 전진방향 전파 경로를 따라 선행 계층으로부터 후행 계층으로 특징부 맵의 전달을 가능하게 하도록 배열된 적어도 하나의 바이패스 경로를 포함하는 것을 포함한다. 예 260에서, 예 258-259의 요지는, 상기 제 1 DNN이 컨볼루션 부분과 디컨볼루션 부분 사이에 위치된 병목 네트 워크 부분을 포함하고, 상기 병목 네트워크 부분은 제 1 DNN의 다른 계층들과 비교하여 상대적으로 낮은 해상도 및 높은 차원을 갖는 적어도 하나의 계층을 포함하는 것을 포함한다. 예 261에서, 예 260의 요지는, 상기 병목 네트워크 부분이 이미지 포맷된 모션 표시 및 이미지 포맷된 에피폴 위치 데이터를 수용하도록 구성된 입력부를 포함하는 것을 포함한다. 예 262에서, 예 256-261의 요지는, 상기 제 1 DNN이, 노면 높이에 기반한 값의 매핑을 포함한 도로 모델을 포함 하여 도로의 구조를 나타내는 맵을 출력으로서 생성하는 것을 포함한다. 예 263에서, 예 261-262의 요지는, 상기 노면 높이에 기반한 값의 매핑이 관측 포인트로부터의 거리에 대한 노 면 높이의 비율을 포함하는 것을 포함한다. 예 264에서, 예 256-263의 요지는, 상기 제 1 DNN은 트레이닝 프로세스로부터 도출된 계산 파라미터로 구성되며, 캡처된 시간적인 트레이닝 이미지 시퀀스 및 대응하는 에고 모션 측정치를 갖는 제 1 부분을 포함하 는 트레이닝 데이터는 트레이닝 DNN에 입력되고, 상기 트레이닝 DNN은 상기 제 1 부분의 전진방향 전파 처리에 기반한 테스트 결과를 생성하며, 상기 전진방향 전파 처리는 상기 트레이닝 DNN에 의한 트레이닝 가능한 계산 파라미터의 적용을 포함하며; 상기 트레이닝 데이터의 상기 제 1 부분에 기반한 제 1 손실 컴포넌트 및 상기 제 1 부분에서는 존재하지 않는 적어도 하나의 캡처된 이미지 또는 에고 모션 측정치를 갖는 트레이닝 데이터의 제 2 부분에 기반한 제 2 손실 컴포넌트를 포함하는 복수의 개별 손실 컴포넌트의 집계에 기반하여 멀티-모드 손실 함수가 생성되고; 상기 멀티-모드 손실 함수는 상기 테스트 결과에 기반하여 평가되어 손실 값을 생성하고, 상 기 트레이닝 가능한 계산 파라미터는 트레이닝 프로세스에 따라 손실 값을 감소시키도록 정제되고; 정제된 트레 이닝 가능한 파라미터는 상기 제 1 DNN을 구성하도록 제공된다. 예 265에서, 예 264의 요지는, 상기 트레이닝 프로세스가 그래디언트 하강 트레이닝을 이용하는 트레이닝 DNN을 통해 손실 함수를 역 전파시키는 것을 포함하는 것을 포함한다. 예 266에서, 예 256-265의 요지는, 상기 제 1 DNN에 의해 수신될 이미지 포맷된 입력이: 시간적 이미지 시퀀스; 이미지로서 포맷된 에피폴 정보 - 상기 에피폴 정보는 에피폴로부터의 제각기의 거리를 나타내는 포인트들을 포 함함 -; 및 이미지로서 포맷된 모션 정보를 포함하고, 상기 모션 정보는 카메라 높이에 대한 전진 방향을 따른 병진이동의 현재 측정치의 비율을 나타내는 포인트들을 포함한다. 예 267에서, 예 256-266의 요지는, 상기 제 1 DNN이 컨볼루션, 활성화 및 풀링 계층을 구비한 복수의 계층을 포 함하는 컨볼루션 DNN이며; 상기 제 1 DNN은 제 1 입력 계층 및 상기 제 1 입력 계층과는 상이한 제 2 입력 계층 을 구비한 상이한 계층을 공급하는 복수의 입력 포트를 포함하고; 제 1 이미지 포맷된 입력은 상기 제 1 입력 계층에 제공되고, 제 2 이미지 포맷된 입력은 상기 제 2 입력 계층에 제공되는 것을 포함한다. 예 268에서, 예 267의 요지는, 상기 제 1 입력이 시간적 이미지 시퀀스를 포함하고, 상기 제 2 입력이 상기 차 량의 모션을 나타내는 이미지 포맷된 모션 정보를 포함하는 것을 포함한다. 예 269에서, 예 264-268의 요지는, 상기 제 1 DNN이 상기 트레이닝 DNN과 동일한 아키텍처를 갖는 것을 포함한 다. 예 270에서, 예 264-269의 요지는, 상기 제 1 손실 컴포넌트가 시간적인 트레이닝 이미지 시퀀스 및 상기 테스 트 결과가 생성되는 대응 에고 모션 측정치에 기반한 사진 측량 손실 컴포넌트를 포함하는 것을 포함한다. 예 271에서, 예 270의 요지는, 상기 제 2 손실 컴포넌트가 상기 시간적인 트레이닝 이미지 시퀀스의 이미지들 중 임의의 이미지가 캡처되었던 시간과는 상이한 시간에 캡처되는 적어도 하나의 과거 또는 미래의 트레이닝 이 미지에 기반하는 것을 포함한다. 예 272에서, 예 270-271의 요지는, 상기 제 2 손실 컴포넌트가 상기 테스트 결과가 생성된 에고 모션 측정치 중 임의의 측정치가 획득되었던 시간과는 상이한 시간에 취해진 적어도 하나의 과거 또는 미래의 에고 모션 측정치 에 기반하는 것을 포함한다. 예 273에서, 예 270-272의 요지는, 상기 제 2 손실 컴포넌트가 캡처된 이미지 및 상기 시간적인 트레이닝 이미 지 시퀀스의 임의의 이미지가 캡처되었던 시간보다는 나중의 시간에 캡처된 트레이닝 데이터의 부분으로부터 획 득된 대응하는 측정된 에고 모션에 기반한 적어도 하나의 미래 도로 구조 평가치에 기반하는 것을 포함한다. 예 274는 처리 회로부에 의해 실행될 때, 상기 처리 회로부로 하여금 예 1-273 중 임의의 하나를 구현하는 동작 을 수행하게 하는 인스트럭션을 포함한 적어도 하나의 머신 판독 가능 매체이다. 예 275는 예 1-273 중 임의의 하나를 구현하는 수단을 포함하는 장치이다. 예 276은 예 1-273 중 임의의 하나를 구현하는 시스템이다. 예 277은 예 1-273 중 임의의 하나를 구현하는 방법이다. 전술한 상세한 설명은 상세한 설명의 일부를 형성하는 첨부 도면에 대한 참조를 포함한다. 도면은 실시될 수 있는 특정 실시예를 예시로서 도시한다. 이들 실시예는 또한 본원에서 \"예\"라고 지칭된다. 이러한 예는 도시 되거나 기술된 것에 추가되는 요소를 포함할 수 있다. 그러나, 본 발명자는 또한 도시되거나 기술된 요소들만 이 제공되는 예들을 고려한다. 게다가, 본 발명자는 또한 본 명세서에 도시되거나 기술된 특정 예(또는 그의 하나 이상의 양태)에 관해 또는 다른 예(또는 그의 하나 이상의 양태)에 관해 도시되거나 기술된 요소(또는 그 의 하나 이상의 양태)의 임의의 조합 또는 치환을 사용하는 예를 고려한다. 본 명세서에서 언급된 모든 간행물, 특허 및 특허 문서는 참고 문헌에 의해 개별적으로 통합되는 것처럼 본 명 세서에 그 전체가 참고로 포함된다. 본 명세서와 참고 문헌에 의해 통합된 문서들 사이에서 일관성이 없는 사 용이 있는 경우, 통합된 참고 문헌(들)의 사용은 본 명세서의 사용을 보완하는 것으로 간주되어야 하며, 양립 불가능한 불일치의 경우 본 명세서의 사용이 지배하게 된다. 본 명세서에서, 단수의 용어는 특허 문헌에서 일반적으로 사용되는 것으로, \"적어도 하나\" 또는 \"하나 이상\"의 임의의 다른 사례 또는 사용과는 독립적으로, 하나 또는 하나 초과를 포함하는 것으로 사용된다. 본 명세서에 서, \"또는\"이라는 용어는, 다른 방식으로 명시되지 않는 한, \"A 또는 B\"는 \"A이지만 B가 아님\", \"B이지만 A가 아님\" 및 \"A 및 B\"를 포함하도록, \"비 배타적인 또는\"을 지칭하는 데 사용된다. 첨부된 청구항에서, “포함하는”이라는 용어는 “구비하는” 용어와 동등한 것으로 사용된다. 또한, 아래의 청구항에서, “포함하는” 및 “구비하는”이라는 용어는 개방형이며, 즉 청구항에서 그러한 용어에 속하는 요소에 대해 추가되는 요소를 포 함하는 시스템, 디바이스, 제조물 또는 프로세스는 여전히 그 청구항의 범위 내에 속하는 것으로 간주된다. 또한, 아래의 청구항에서, 제 1\", \"제 2\" 및 \"제 3\" 등의 용어는 단지 라벨로서만 사용되며, 그들의 물체에 대 한 수치적인 요건을 부여하기 위한 것으로 의도한 것은 아니다. 전술한 설명은 예시적인 것이며, 제한적인 것은 아니다. 예를 들어, 전술한 예들(또는 그의 하나 이상의 양태 들)은 서로 조합하여 사용될 수 있다. 전술한 설명을 검토하게 되면, 예컨대, 당업자에 의해 다른 실시예가 사"}
{"patent_id": "10-2021-7017826", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "용될 수 있다. 요약서는 독자가 기술 개시의 본질을 신속하게 확인할 수있게 하고, 청구항의 범위 또는 의미를 해석하거나 제한하는 데 사용되지 않을 것이라는 이해와 함께 제출된다. 또한, 전술한 상세한 설명에서, 다양 한 특징들은 본 개시 내용을 간소화하도록 함께 그룹화될 수도 있다. 이것은 청구되지 않은 공개된 특징이 임 의의 청구항에 필수적이라는 것을 의도하는 것으로 해석되어서는 안된다. 오히려, 본 발명의 대상은 특정의 개 시된 실시예의 모든 특징보다 적게 존재할 수도 있다. 따라서, 아래의 청구항은 전술한 상세한 설명에 포함되 며, 각각의 청구항은 그 자체가 개별 실시예가 된다. 본 실시예들의 범위는 첨부된 청구항들을 참조하여 그러 한 청구항들이 부여하는 균등물의 전체 범위와 함께 결정되어야 한다."}
{"patent_id": "10-2021-7017826", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도면은 반드시 축척대로 도시되지는 않으며, 동일한 참조 부호는 상이한 도면에서 동일한 컴포넌트를 나타낼 수 있다. 상이한 문자 접미사를 갖는 동일한 참조 부호는 동일한 컴포넌트의 상이한 사례를 나타낼 수 있다. 도 면은 본 문서에서 논의된 다양한 실시예를 일반적으로 예시하지만 제한적으로 도시되는 것은 아니다.도 1은 일 예의 차량 환경의 블록도이다. 도 2는 일 실시예에 따른 카메라를 이용한 차량 환경 모델링 시스템의 일 예의 블록도이다. 도 3은 일 실시예에 따른 현재 이미지 및 이전 이미지를 도시한다. 도 4는 일 실시예에 따른 노면의 감마 모델을 생성하는 신경 네트워크의 일 예를 도시한다. 도 5는 일 실시예에 따른 머신 러닝(ML) 기반 수직 윤곽 엔진의 예시적인 심층 신경 네트워크(deep neural network)(DNN)를 도시한다. 도 6은 일 실시예에 따른 DNN의 예시적인 아키텍처를 상세히 나타내는 표이다. 도 7 및 도 8은 일 실시예에 따른 DNN의 보다 복잡한 예시적인 아키텍처를 상세히 나타내는 표이다. 도 9는 일 실시예에 따른 DNN 트레이닝 시스템의 일 예를 도시한다. 도 10은 일 실시예에 따른 멀티-모드 손실 함수 적용 엔진의 일 예를 도시한다. 도 11은 일 실시예에 따른 물체의 움직임 여부에 관한 판정을 생성하는 신경 네트워크의 일 예를 도시한다. 도 12는 일 실시예에 따른 물체의 움직임 여부에 관한 판정을 생성하는 컨볼루션 신경 네트워크의 일 예를 도시 한다. 도 13은 일 실시예에 따른 수직 윤곽 탐지 엔진을 동작시키는 방법의 일 예의 흐름도이다. 도 14는 일 실시예에 따른 ML 기반 윤곽 엔진에 사용하기 위해 DNN을 구성하는 방법의 일 예의 흐름도이다. 도 15는 일 실시예에 따른 자율 주행 차량이 도로를 따라 이동하는 동안 도로의 수직 윤곽의 실시간 측정 방법 의 일 예의 흐름도이다. 도 16은 일 실시예에 따른 도로의 수직 윤곽을 측정하기 위해 이미지들의 시퀀스를 통한 잔차 흐름을 처리하기 위한 예시적인 접근법의 흐름도이다. 도 17은 일 실시예에 따른 차량 제어를 위한 퍼들 탐지 및 응답성 의사 결정 방법의 일 예의 흐름도이다. 도 18은 일 실시예에 따른 수직 윤곽 정보 및 추가적인 퍼들 탐지 기준에 기반하여 하나 이상의 퍼들의 존재를 계산적으로 결정하기 위한 방법의 일 예의 흐름도이다. 도 19는 일 실시예에 따른 자율 주행 차량을 위한 현재 상황 시나리오를 계산적으로 결정하기 위한 방법의 일 예의 흐름도이다. 도 20은 일 실시예에 따른 퍼들의 탐지에 응답하기 위해 선택될 수 있거나 선택되지 않을 수 있는 이용 가능한 주행 응답 솔루션의 계산 평가 방법의 일 예의 흐름도이다. 도 21은 일 실시예에 따른 자율 주행 차량 제어 시스템과 함께 사용하기 위해 도로를 프로파일링하기 위한 카메 라 기반 차량 탑재 시스템을 도시한다. 도 22는 일 실시예에 따른 차량 상의 멀티-카메라 어레이를 도시한다. 도 23은 일 실시예에 따른 멀티-카메라 어레이에 의해 캡처될 수 있는 시야의 예를 도시한다. 도 24는 일 실시예에 따른 수직 윤곽 탐지 엔진의 일 예의 블록도이다. 도 25는 일 실시예에 따른 전 처리기(preprocessor) 엔진의 일 예를 도시한다. 도 26은 일 실시예에 따른 카메라를 이용한 차량 환경 모델링 방법의 일 예의 흐름도이다. 도 27은 하나 이상의 실시예가 구현될 수 있는 일 예의 머신의 블록도이다. 도 28은 일 실시예에 따른 컴퓨팅 디바이스의 예시적인 하드웨어 및 소프트웨어 아키텍처를 도시하는 도면이다. 도 29는 일 실시예에 따라 사용될 수 있는 처리 디바이스의 블록도이다. 도 30은 일 실시예에 따른 중앙 처리 유닛의 예시적인 컴포넌트의 블록도이다."}
