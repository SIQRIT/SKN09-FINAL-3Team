{"patent_id": "10-2019-7037882", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0142784", "출원번호": "10-2019-7037882", "발명의 명칭": "데이터 처리방법 및 관련제품", "출원인": "캠브리콘 테크놀로지스 코퍼레이션 리미티드", "발명자": "장 야오"}}
{"patent_id": "10-2019-7037882", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "클라우드 측의 인공지능 프로세서에 적용되는 데이터 처리방법에 있어서, 엔드 측의 인공지능 프로세서의 설비정보에 의해 생성되는 바이너리 명령에 따라 확정되는 인공지능 학습 태스크를 수신하는 것, 및 상기 인공지능 학습 태스크를 수행하여 실행결과를 생성하는 것을 포함하는 것을 특징으로 하는 데이터 처리방법."}
{"patent_id": "10-2019-7037882", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 실행결과는 상기 인공지능 학습 태스크의 클라우드 측의 인공지능 프로세서에서의 실행시간, 상기 인공지능 학습 태스크의 수행시 클라우드 측의 인공지능 처리 시스템을 점용하는 로드 정보, 인공지능 학습 태스크를수행한 결과가 예상되는 요구 사항에 해당하는지 여부 중 적어도 하나를 포함하는 것을 특징으로 하는 데이터처리방법."}
{"patent_id": "10-2019-7037882", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서, 상기 클라우드 측의 인공지능 프로세서는 인공지능 프로세서 하드웨어 실체, 필드 프로그래머블 게이트 어레이,시뮬레이터 중 적어도 하나를 포함하는 것을 특징으로 하는 데이터 처리방법."}
{"patent_id": "10-2019-7037882", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서, 상기 클라우드 측의 인공지능 프로세서 하드웨어 실체는 아키텍처의 재구성이 가능한 인공지능 프로세서인 것을특징으로 하는 데이터 처리방법."}
{"patent_id": "10-2019-7037882", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "엔드 측의 인공지능 프로세서의 설비정보에 의해 생성되는 바이너리 명령에 따라 확정되는 인공지능 학습 태스크를 수신하기 위한 수신모듈과, 상기 인공지능 학습 태스크를 수행하여 실행결과를 생성하기 위한 수행모듈을 포함하는 것을 특징으로 하는 인공지능 프로세서."}
{"patent_id": "10-2019-7037882", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서, 상기 수행모듈이 생성하는 실행결과는 상기 인공지능 학습 태스크의 상기 클라우드 측의 인공지능 프로세서에서의 실행시간, 상기 인공지능 학습 태스크의 수행시 클라우드 측의 인공지능 처리 시스템을 점용하는 로드 정보,인공지능 학습 태스크를 수행한 결과가 예상되는 요구 사항에 해당하는지 여부 중 적어도 하나를 포함하는 것을특징으로 하는 인공지능 프로세서."}
{"patent_id": "10-2019-7037882", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5 항에 있어서, 상기 클라우드 측의 인공지능 프로세서는 인공지능 프로세서 하드웨어 실체, 필드 프로그래머블 게이트 어레이,시뮬레이터 중 적어도 하나를 포함하는 것을 특징으로 하는 인공지능 프로세서. 공개특허 10-2021-0142784-3-청구항 8 제7 항에 있어서, 상기 클라우드 측의 인공지능 프로세서 하드웨어 실체는 아키텍처의 재구성이 가능한 인공지능 프로세서인 것을특징으로 하는 인공지능 프로세서."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 출원의 실시예는 데이터 처리방법 및 관련제품을 개시한다. 당해 데이터 처리방법은 상기 범용 프로세서가 엔 드 측의 인공지능 프로세서의 설비정보에 따라 바이너리 명령을 생성하고, 상기 바이너리 명령에 따라 인공지능 학습 태스크를 생성하는 것; 상기 범용 프로세서가 상기 인공지능 학습 태스크를 클라우드 측의 인공지능 프로세 서에 송신하여 실행시키는 것; 상기 범용 프로세서가 상기 인공지능 학습 태스크에 대응하는 실행결과를 수신하 는 것; 상기 범용 프로세서가 상기 실행결과에 따라 오프라인 실행 파일을 확정하는 것을 포함하며, 상기 오프라 인 실행 파일은 실행결과가 미리 설정된 요구 사항을 충족하면 대응하는 상기 엔드 측의 인공지능 프로세서의 설 비정보와 바이너리 명령에 의해 생성된다. 본 출원에 따르면 사전에 인공지능 알고리즘 모델과 인공지능 프로세 서 사이의 디버깅 작업을 실현할 수 있다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "[관련출원의 상호참조] 본 출원은 파리 조약에 따라 2019년 4월 18일 중국 국가지식산권국에 제출한 중국 특허출원번호가 201910315962.9이고 발명의 명칭이 \"일종 데이터 처리방법 및 관련제품\"인 우선권 및 2019년 5월 23일에 중국 국가지식산권국에 제출한 중국 특허출원번호가 201910436801.5이고 발명의 명칭이 \"일종 데이터 처리방법 및 관 련제품\"인 우선권을 주장한다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "본 출원은 인공지능 프로세서 기술분야에 관련되며, 특히 데이터 처리방법 및 관련제품에 관한 것이다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래기술에서 인공지능 프로세서가 성공적으로 테이프 아웃 될 때 알고리즘 애플리케이션 개발자는 대응하는 버 전의 인공지능 프로세서(하드웨어 실체)에서 개발하고 테스트할 수 있다. 상술한 설명으로부터 이해할 수 있는 바와 같이, 종래기술에서 인공지능 프로세서가 테이프 아웃항 후에 라야 개발한 인공지능 알고리즘이 대응하는 버전의 인공지능 프로세서에서 실행한 기능결과 및 성능결과를 알 수 있 다. 그리고 엔드 측의 인공지능 프로세서가 테이프 아웃되지 않아도 알고리즘 애플리케이션 개발자가 인공지능 알고리즘 모델과 인공지능 프로세서 사이의 디버깅 작업을 실현할 수 있는 방법은 해결해야 할 시급한 문제로 되고 있다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 출원의 실시예는 인공지능 프로세서의 테이프 아웃 여부에 관계없이 인공지능 알고리즘 모델과 인공지능 프 로세서 사이의 디버깅 작업을 사전에 실현할 수 있는 데이터 처리방법 및 관련제품을 제공하는 것을 목적으로 하고 있다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 목적을 달성하기 위해 본 출원은 클라우드 측의 인공지능 프로세서에 적용되는 데이터 처리방법을 제안 하는 바, 엔드 측의 인공지능 프로세서의 설비정보에 의해 생성되는 바이너리 명령에 따라 확정되는 인공지능 학습 태스 크를 수신하는 것, 및 상기 인공지능 학습 태스크를 수행하여 실행결과를 생성하는 것을 포함한다. 상술한 목적을 달성하기 위해 본 출원은 엔드 측의 인공지능 프로세서의 설비정보에 의해 생성되는 바이너리 명 령에 따라 확정되는 인공지능 학습 태스크를 수신하기 위한 수신모듈과, 상기 인공지능 학습 태스크를 수행하여 실행결과를 생성하기 위한 수행모듈을 포함하는 인공지능 프로세서를 제안한다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 기술적 효과는 다음과 같다. 즉 본 발명이 소프트웨어 개발 플랫폼을 제공하며, 사용자는 당해 소프 트웨어 개발 플랫폼에서 알고리즘과 인공지능 프로세서 사이의 기능, 성능, 정확도의 디버깅을 완성할 수 있고, 디버깅 완료 후에 생성된 오프라인 실행 파일은 호환성이 있는 아키텍처 위의 다양한 SoC 칩에 배포할 수 있으 며, 사용자가 하드웨어 실체를 입수하지 않아도 사전에 알고리즘과 인공지능 프로세서 사이의 기능, 성능, 정확도를 디버깅할 수 있고, 제품의 개발주기가 크게 단축된다는 장점이 있다. 또한, SoC 칩마다 개별적으로 개발하 거나 또는 일련의 개발환경에 적응시킬 필요가 없다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 실시예의 기술방안에 대해 도면을 참조하면서 명확하고 완전하게 설명한다. 도면에 도시되고 이하의 설명에서 상세히 서술한 비 제한적으로 예시한 실시예를 참조하여 더 전면적으로 본 발명이 개시한 예시 적인 실시예 및 그들의 다양한 특징과 유리한 세부사항을 설명한다. 그리고 도면에 도시된 특징들은 반드시 일 정한 비율로 그려진 것이 아니라는 점에 유의해야 한다. 본 발명은 본 개시의 예시적인 실시예를 모호하게 하지 않기 위해 공개된 재료, 구성요소 및 프로세스 기술의 설명을 생략하고 있다. 주어진 예들은 본 발명의 실시예 들의 구현의 이해를 용이하게 하고 당해 발명이 속하는 분야에서 통상의 지식을 가진 자가 실시예를 실시할 수 있게 하기 위한 것일 뿐이다. 이와 같이 이들 예는 본 발명의 실시예의 범위를 한정하는 것으로 이해되지 않아 야 한다. 다르게 정의되지 않는 한, 본 발명에서 사용된 기술용어 또는 과학적 용어는 본 발명이 속하는 분야에서 통상의 지식을 가진 자에 의해 이해되는 일반적인 의미로 이해 될 것이다. 본 발명에서 사용된 \"제1\", \"제2\" 및 유사한 용어들은 임의의 순서, 수량 또는 중요도를 나타내는 것이 아니라 상이한 구성요소를 구별하기 위해서만 사용된 다. 또한, 본 발명의 각 실시예에서 동일하거나 유사한 참조부호는 동일하거나 유사한 부재를 나타낸다. 기술적 해결책의 보다 나은 이해를 돕기 위해 이하에서 본 출원의 실시예에 관련된 기술적 용어들을 먼저 설명 한다. 테이프 아웃(Tape-Out): 집적회로 설계분야에서 테이프 아웃은 시험 생산, 즉 미리 설정된 기능을 충족하는 집 적회로를 설계한 후 테스트용으로 먼서 몇개 또는 수십개를 생산하고, 테스트 수요가 충족되면 현재 테스트 수 요를 충족하는 집적회로의 구조에 따라 대량 생산을 진행한다. 인공지능 프로세서의 소프트웨어 스택: 도2를 참조하면, 당해 소프트웨어 스택 구조는 인공지능 애플리케이 션, 인공지능 프레임 워크, 인공지능 학습 라이브러리, 인공지능 런타임 라이브러리 및 드 라이버를 포함한다. 따라서 이에 대해 자세히 설명한다. 인공지능 애플리케이션은 상이한 애플리케이션 시나리오에 대응하여 상응한 인공지능 알고리즘 모델을 제 공한다. 당해 알고리즘 모델은 인공지능 프레임 워크의 프로그래밍 인터페이스에 의해 직적 분석될 수 있으며, 가능한 구현 중 하나에서 인공지능 학습 라이브러리를 통해 인공지능 알고리즘 모델을 바이너리 명 령으로 변환하고, 인공지능 런타임 라이브러리를 호출하여 바이너리 명령을 인공지능 학습 태스크로 변환 하며, 당해 인공지능 학습 태스크를 태스크 큐에 배치하고 드라이버에 의해 태스크 큐 중 인공지능 학습 태스크를 스케줄링하여 밑바닥의 인공지능 프로세서가 수행하게 한다. 다른 하나의 가능한 구현에서 인공지능 런타임 라이브러리를 직접 호출하여 이전에 이미 경화하여 생성된 오프라인 실행 파일을 실행하여 소프트 웨어 아키텍처의 중간 오버 헤드를 감소시키고 실행효율을 향상한다. 바이너리 명령: 밑바닥의 인공지능 프로세서가 인식할 수 있는 정보이다. 인공지능 프로세서: 특정 응용 프로세서라고도 하며, 특정 애플리케이션 또는 도메인을 위한 프로세서이다. 예 를 들어: 디스플레이 코어, 비주얼 프로세서, 디스플레이 칩으로도 알려진 그래픽 프로세서(Graphics Processing Unit, GPU)는 일종의 개인용 컴퓨터, 워크 스테이션, 게임 콘솔 및 일부 모바일 장치(태블릿, 스마 트 폰 등)에서 전문적으로 이미지 연산작업을 수행하는 특정 응용 프로세서이다. 또 예를 들면, 신경망 프로세 서(Neural Processing Unit, NPU)는 인공지능 분야의 애플리케이션에서 행렬 곱셈 연산을 위한 특정 응용 프로 세서로서, \"데이터 드라이버 병렬 컴퓨팅\"의 아키텍처를 채용하며, 특히 비디오, 이미지 유형의 대규모 멀티미 디어 데이터를 처리하는 데 탁월하다. 재구성이 가능한 아키텍처: 어느 인공지능 프로세서가 재사용 가능한 하드웨어 리소스를 활용할 수 있는 경우, 각기 다른 애플리케이션 수요에 맞는 아키텍처를 제공하기 위해 각 특정 애플리케이션 수요에 따라 자체의 아키 텍처를 유연하게 변경할 수 있다면 이 인공지능 프로세서를 재구성 가능한 컴퓨팅 시스템이라고 하며 아키텍처 를 재구성 가능한 아키텍처라고 한다. 전용 프로그래밍 언어: 특정 하드웨어를 기반으로 개발한 고급 프로그래밍 언어이다. 예를 들어 Cuda C. 이하 첨부된 도면을 참조하여 본 발명의 실시예에 의해 제공되는 데이터 처리방법 및 관련제품의 구체적인 실시 태양을 상세히 설명하기로 한다. 종래기술에서 알고리즘 애플리케이션의 개발자는 해당 버전 하드웨어 실체에서만 인공지능 알고리즘 모델과 인 공지능 프로세서 사이의 적응 및 디버깅 작업을 완료할 수 있다. 또한, 종래기술의 구현은 테이프 아웃되지 않 은 칩과 알고리즘 사이의 디버깅 프로세스를 실현할 수 없으며, 알고리즘 모델과 칩 사이의 적응작업을 완료하 기 위해 특별한 시간이 필요하다는 것을 이해할 수 있다. 따라서 제품 출시 기한이 지연될 것이며 시간 문제로 인해 시장을 점유할 시회를 놓치기 쉽다. 이를 기반으로 본 출원은 테이프 아웃 여부에 관계없이 엔드 측의 인공지능 프로세서의 설비정보에 따라 클라우 드 측에서 적합한 인공지능 프로세서를 매칭시켜 엔드 측의 인공지능 프로세서를 시뮬레이션하는 기술방안을 제 안한다. 본 기술방안의 소프트웨어 개발 플랫폼이 사용자에게 제공하는 알고리즘 모델은 일련의 처리를 통해 대 응하는 인공지능 학습 태스크를 얻으며, 당해 인공지능 학습 태스크는 클라우드 측의 인공지능 프로세서에 의해 실행되어 실행결과를 얻는다. 실행결과에 따라 소프트웨어 개발 플랫폼에서 인공지능 학습 태스크를 조정한다. 본 기술방안인 경우 인공지능 알고리즘 모델의 조정 여부에 관계없이 인공지능 학습 라이브러리를 최적화 및/또 는 엔드 측의 인공지능 프로세서의 설비정보를 조정하는 것을 통해 인공지능 학습 태스크를 조정하는 목적을 달 성할 수 있고, 엔드 측의 인공지능 프로세서와 인공지능 알고리즘 모델 사이의 적응을 실현할 수 있다. 도1을 참조하면, 본 기술방안의 시스템 아키텍처를 도시하고 있다. 도1에 도시된 바와 같이, 당해 시스템 아키 텍처는 클라우드 측 설비, 소프트웨어 개발 플랫폼, 및 엔드 측 설비를 포함한다. 구체적 실현에서 소프트웨어 개발 플랫폼은 일련의 도구를 제공한다. 그 도구는 애플리케이션 개발, 성능 튜닝(tuning), 기능 디버깅 등을 포함한다. 그 중, 애플리케이션 개발도구는 인공지능 학습 라이브러리, 인공지 능 런타임 라이브러리, 컴파일러(compiler) 및 특정 영역(비디오 분석 등)의 소프트웨어 개발도구를 포함한다. 기능 디버깅 도구는 프로그래밍 프레임 워크, 인공지능 학습 라이브러리 등 다양한 수준의 디버깅 수요를 충족 할 수 있다. 성능 튜닝 도구는 성능분석도구 및 시스템 모니터링 도구 등을 포함한다. 컴파일러는 C＋＋ 언어의 전통적인 컴파일러를 포함할 수 있고, C 언어와 유사한 언어를 기반으로 하는 기계학습 컴파일러, 또는 다른 고 급 언어 또는 특별히 설계된 도메인 전용 프로그래밍 언어(Domain Specific Language)의 기계학습 컴파일러를 포함할 수도 있다. 바람직하게는, 당해 소프트웨어 개발 플랫폼은 클라우드 측 설비의 프로세서에서 실행 할 수 있으며, 호스트(host)의 컴퓨터 장치의 프로세서에서 실행할 수 있다. 당해 호스트의 컴퓨터 장치는 범용 프로세서(CPU 등) 및 디스플레이 등을 포함할 수 있으며, 여기에서 구체적으로 한정하지 않는다. 또한 바람직하 게는, 전술한 소프트웨어 개발 플랫폼은 클라이언트 형태로 호스트의 컴퓨터 장치 또는 클라우드 장치에서 실행할 수 있으며, 본 출원의 실시예에서는 구체적으로 제한하지 않는다. 도3에 도시된 바와 같이, 본 기술방안의 애플리케이션 시나리오 중 하나를 모식적으로 나타내고 있다. 사용자가 데스크톱 컴퓨터의 소프트웨어 개발 플 랫폼에 로그인하고 소프트웨어 개발 플랫폼에서 알고리즘 모델에 대응하는 인공지능 학습 태스크를 생성하며, 인공지능 학습 태스크가 클라우드 측의 인공지능 프로세서에서 실행된 결과에 근거하여 인공지능 학습 태스크를 조정한다. 도4에 도시된 바와 같이, 본 기술방안의 애플리케이션 시나리오 중 다른 하나를 모식적으로 나타내고 있다. 클라우드 장치에는 인공지능 소프트웨어 개발 클라이언트가 설치되어 있다. 구체적으로, 클라우드 측 설비는 완전한 컴퓨터 시스템으로서 범용 프로세서와 적어도 하나의 인공지능 프로세서를 포함할 수 있다. 예를 들어, 인공지능 프로세서는 8개의 클러스터(cluster)를 포함하며, 각 클러스터에는 4개의 인공지능 프로세 서 코어가 포함될 수 있다. 실제로, 소프트웨어 개발 플랫폼은 사용자 레코드를 유지 및 보호하며, 당해 사용자 레코드는 데이터 블록 등 도구를 통해 저장되고, 레코드 내용은 사용자 개인정보(계정정보 등), 사용자 에 의해 요구되는 서비스 정보를 포함한다. 그 중, 서비스 정보는 디버깅 수요, 엔드 측의 인공지능 프로세서의 설비정보를 포함하지만 이에 한정되지 않는다. 당해 디버깅 수요는 기능 디버깅 및 성능 디버깅을 포함하지만 이에 한정되지 않는다. 설비정보는 하드웨어 아키텍처 정보 및 실행환경 파라미터를 포함하지만 이에 한정되지 않는다. 실행환경 파라미터는 엔드 측의 인공지능 프로세서의 실행 메인 주파수, 오프 칩 메모리부터 엔드 측의 인공지능 프로세서 사이의 액세스 대역폭, 온칩 스토리지 크기, 엔드 측의 인공지능 프로세서의 코어 수, 및 엔 드 측의 인공지능 프로세서의 연산장치 유형을 포함하지만 이에 한정되지 않는다. 구체적 실현에서 클라우드 측 설비에는 인공지능 프로세서가 설치되어 있으며, 당해 인공지능 프로세서는 클라우드 측의 인공지능 프로세서라고 칭한다. 클라우드 측의 인공지능 프로세서는 인공지능 프로세서 칩, 필드 프로그래머블 게이트 어레이, 시뮬레이터를 포함하지만 이들에 한정되지 않는다. 인공지능 프로세서 칩은 재구 성형 칩일 수도 있고, 비 재구성형 칩일 수도 있다. 클라우드 측 설비는 서버 보드 또는 서버 보드 클러스 터일 수도 있다. 구체적 실현에서 엔드 측 설비에는 인공지능 프로세서가 설치되어 있으며, 당해 인공지능 프로세서는 엔드 측의 인공지능 프로세서라고 칭한다. 엔드 측 설비는 예를 들어 태불릿 컴퓨터, 휴대폰과 같은 단말 장치일 수 있다. 엔드 측 설비는 예를 들어 카메라와 같은 에지 장치일 수도 있다. 본 출원의 실시예에서 엔드 측 설비 가 테이프 아웃 않된 상태의 설비일 수도 있고, 이미 테이프 아웃된 설비일 수도 있다. 본 기술방안의 작동원리는 소프트웨어 개발 플랫폼에서 드라이버가 엔드 측의 인공지능 프로세서의 설비정 보에 근거하여 클라우드 측 설비에서 엔드 측 설비와 적응되는 인공지능 프로세서를 필터링한다. 필 터링된 클라우드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보는 대응하는 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 호환되고, 클라우드 측의 인공지능 프로세서의 명령어 세트는 대응하는 엔드 측의 인 공지능 프로세서의 명령어 세트호와 호환된다. 여기에서 클라우드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보가 대응하는 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 호환된다는 것은 클라우드 측의 인 공지능 프로세서의 컴퓨팅 능력이 엔드 측의 인공지능 프로세서의 컴퓨팅 능력보다 크거나 같은 것을 포함한다. 소프트웨어 개발 플랫폼에서 엔드 측의 인공지능 프로세서의 설비정보에 근거하여 인공지능 학습 라이브러 리의 컴파일 된 인터페이스에 대응되는 소프트웨어 파라미터를 설정하고, 프로그래밍 프레임 워크에 의해 알고 리즘 모델을 얻으며, 이미 설정된 인공지능 학습 라이브러리의 컴파일 된 인터페이스를 호출하여 컴파일하며 대 응하는 엔드 측의 인공지능 프로세서의 바이너리 명령을 획득한다. 당해 바이너리 명령은 런타임 라이브러리의 처리에 의해 인공지능 학습 태스크를 생성한다. 인공지능 학습 태스크를 태스크 큐에 배치하여 최종적으로 드라 이버에 의해 태스크 큐 중 인공지능 학습 태스크를 스케줄링하여 클라우드 측의 인공지능 프로세서가 수행하게 한다. 클라우드 측의 인공지능 프로세서가 수행하는 인공지능 학습 태스크에 근거하여 실행결과를 소프트웨어 개발 플 랫폼에 피드백한다. 바람직하게는, 소프트웨어 개발 플랫폼은 실행결과를 표시할 수 있다. 실행결과 에 따라 소프트웨어 개발 플랫폼은 사용자의 조작명령을 수신하고, 소프트웨어 개발 플랫폼은 조작명 령에 따라 3가지 방법 중 적어도 한가지 방법을 수행하여 바이너리 명령을 조정한다. 이 3가지 방법은 상기 엔 드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보를 조정하는 것, 상기 엔드 측의 인공지능 프로세서의 실 행환경 파라미터를 조정하는 것, 및 인공지능 학습 태스크를 최적화하는 것으로 나뉜다. 조정 후의 바이너리 명 령을 대응하는 인공지능 학습 태스크로 변환하고 태스크 큐에 배치하며, 드라이버에 의해 태스크 큐 중 인공지 능 학습 태스크를 스케줄링하여 클라우드 측의 인공지능 프로세서가 피드백한 실행결과가 기대치와 일치할 때까 지 클라우드 측의 인공지능 프로세서가 수행하게 한다. 본 기술방안에서 엔드 측의 인공지능 프로세서의 설비정보와 인공지능 학습 라이브러리의 컴파일 된 인터페이스 의 소프트웨어 파라미터는 서로 대응되며, 당해 소프트웨어 파라미터는 보다 많은 정보, 예를 들어 램 크기(Ram size), 캐시(Cache) 크기, Cache를 통해 캐싱할지 여부 등을 포함한다. 이러한 정보는 바이너리 명령을 생성할 때 할당된 동작 도메인과 관련이 있으므로 인공지능 알고리즘 모델이 변경되지 않은 상황에서 엔드 측의 인공지 능 프로세서의 설비정보를 변경하면 바이너리 명령을 조정하여 인공지능 학습 태스크를 조정할 수 있다. 엔드 측의 인공지능 프로세서의 테이프 아웃에 관계없이 엔드 측의 인공지능 프로세서의 설비정보에 근거하여 클라우 드 측 설비에서 적응해 낸 클라우드 측의 인공지능 프로세서는 엔드 측의 인공지능 프로세서를 시뮬레이션 하여 클라우드 측의 인공지능 프로세서에서 대응하는 인공지능 학습 태스크를 수행할 수 있다. 실행결과에 따라 당해 소프트웨어 개발 플랫폼에서 알고리즘 모델과 인공지능 프로세서 사이의 기능, 성능, 정확도의 디버깅을 완성하며, 디버깅 완료 후에 생성된 오프라인 실행 파일은 호환 가능한 아키텍처 위의 다양한 엔드 측의 SoC칩 에 배포할 수 있으며, 사용자가 하드웨어 실체를 입수하지 않아도 사전에 알고리즘 모델과 인공지능 프로세서 사이의 기능, 성능, 정확도를 디버깅할 수 있고, 제품의 개발주기를 크게 단축시킬 수 있다는 장점이 있다. 또 한, 각 엔드 측의 SoC 칩마다 개별적으로 개발하거나 또는 일련의 개발환경에 적응시킬 필요가 없다. 또한, 본 기술방안에서 클라우드 측의 인공지능 프로세서의 설비정보에 대응하는 현재 실행환경 파라미터는 그 의 실제 실행환경 파라미터와 같을 수 있으며, 또한 그희 실제 실행 파라미터와 다를 수도 있다. 특정 인공지능 학습 태스크에 대한 클라우드 측의 인공지능 프로세서의 수행결과에 따라 엔드 측의 인공지능 프로세서의 설비 정보가 예상 조건에 해당하는지 여부를 확정한다. 엔드 측의 인공지능 프로세서의 설비정보가 예상 조건에 해당 하지 않으면 진일보로 엔드 측의 인공지능 프로세서의 설비정보를 당해 엔드 측의 인공지능 프로세서의 설비정 보가 예상 조건에 해당할 때까지 조정한다. 따라서, 본 기술방안인 경우, 엔드 측의 인공지능 프로세서의 아키 텍처는 설계 단계에서 애플리케이션에 근거하여 엔드 측의 SoC 칩 설계 사양을 평가할 수도 있다. 전술한 설명에 근거하여 도5에 도시된 바와 같이, 본 출원이 제안하는 한 가지 데이터 처리방법의 흐름도를 나 타내고 있다. 상기 방법은 범용 프로세서에 적용되며, 도1의 소프트웨어 개발 플랫폼에 대응된다. 상술한 바와 같이, 당해 범용 프로세서는 클라우드 측 설비의 범용 프로세서일 수도 있고, 호스트(host)의 컴퓨터 장치 의 범용 프로세서일 수도 있다. 당해 데이터 처리방법은 다음과 같은 단계를 포함한다. 단계: 상기 범용 프로세서가 엔드 측의 인공지능 프로세서의 설비정보에 따라 바이너리 명령을 생성하고, 상기 바이너리 명령에 따라 인공지능 학습 태스크를 생성한다. 본 기술방안에서 다양한 프로그래밍 프레임 워크, 예를 들어 구글 텐서 플로우 그래프 인공지능 학습시스템 TensorFlow, 딥 러닝 프레임 워크Caffe, Caffe2, MXNet등이 소프트웨어 개발 플랫폼에 통합된다. Caffe를 예로 들어 Caffe의 핵심모듈에는 Blobs, Layers 및 Nets 이 세 가지가 있다. 그 중 Blobs는 데이터 저장, 데이 터 인터랙티비티 및 처리를 하기 위한 것이며, Blobs를 통해 통일적으로 데이터 메모리의 인터페이스를 제조한 다. Layers는 신경망의 핵심으로서 많은 계층구조를 정의하였으며 Blobs를 입출력으로 간주하고 있다. Nets는 일련의 Layers의 집합이고 이러한 계층구조는 연결되어 네트워크 맵을 형성한다. 본 단계에서 엔드 측의 인공지능 프로세서의 설비정보에 근거하여 인공지능 학습 라이브러리의 컴파일 된 인터 페이스에 대응되는 소프트웨어 파라미터를 설정하고, 프로그래밍 프레임 워크에 의해 획득한 알고리즘 모델과 결합하여 이미 설정된 인공지능 학습 라이브러리의 컴파일 된 인터페이스를 호출하여 컴파일하며, 대응하는 엔 드 측의 인공지능 프로세서의 바이너리 명령을 획득한다. 당해 바이너리 명령은 런타임 라이브러리의 처리에 의 해 인공지능 학습 태스크를 생성한다. 인공지능 학습 태스크를 태스크 큐에 배치하여 최종적으로 드라이버에 의 해 태스크 큐 중 인공지능 학습 태스크를 스케줄링하여 클라우드 측의 인공지능 프로세서가 수행하게 한다. 실제 응용에서 인공지능 학습 라이브러리는 인공지능 프로세서에서 다양한 인공지능 학습 알고리즘을 가속화하 는 데 사용된다. 여기에서 인공지능 학습 알고리즘은 예를 들어 컨볼 루션 신경망 알고리즘, 순환 신경망 알고 리즘 등 딥 러닝 알고리즘을 포함하지만 이들에 한정되지 않는다. 구체적으로 인공지능 학습 라이브러리는 주로 다음과 같은 특서을 포함한다. 여러 유형의 기본 연산자를 지원 구체적 실현에서 기본 연산자의 조합을 통해 다양한 기계학습 알고리즘을 실현하여 범용성, 유연성, 확장성의 수요를 충족할 수 있다. 구체적으로 여기에 관련된 여러 유형의 기본 연산자는 일반적인 신경망 연산자, 행렬, 벡터, 및 스칼라 연산 자, 순환 신경망 연산자를 포함할 수 있다. 도6을 참조하면, 본 출원 실시예가 제공하는 인공지능 학습라이브러리가 지원하는 여러 유형의 기본 연산자의 모식도를 나타내고 있으며, 도6에 도시된 바와 같이, 인공지 능 학습 라이브러리가 지원하는 여러 유형의 기본 연산자는 일반적인 신경망 연산자를 포함하고, 당해 신경 망 연산자는 컨볼 루션/디컨볼 루션 연산자, 풀링 연산자, 활성화 연산자, 로컬 응답 정규화 LRN(LRN, Local Response Normalization)/배치 정규화 연산자, 분류기(Softmax) 연산자, 완전 연결 연산자를 포함한다. 그 중 활성화 연산자는 ReLU, Sigmoid, Tanh 및 보간에 의해 구현될 수 있는 다른 연산자를 포함할 수 있지만 이들에 한정되지 않는다. 행렬, 벡터, 및 스칼라 연산자는 행렬 곱셈 연산자 , 텐서 가감 연산자, 텐서 논리 연산 연산자, 텐서(Tensor) 변환 연산자, ROIPooling 연산자 , Proposal 연산자를 포함한다. 그 중 Tensor 변환 연산자는 크롭(Crop), 텐서 리모델링 Reshape, 텐서 분할 슬라이스(Slice), 텐서 스플라이스 Concat 등을 포함할 수 있지만 이들에 한정되지 않는다. 순환 신 경망 연산자는 장기 단기 메모리 네트워크 LSTM(LSTM, Long Short－Term Memory) 연산자, 기본 순환 신경망 RNN(Recurrent Neural Network, RNN), 순환 신경망 RNN 연산자, SVDF 연산자를 포함한다. 실 제 응용에서 사용자는 자신의 수요에 따라 자유롭게 인공지능 학습 라이브러리에 새로운 연산자를 추가하거나 서로 다른 버전의 인공지능 학습 라이브러리를 변경할 수도 있으며, 여기에서는 자세히 설명하지 않고 인공지능 학습 태스크를 디버깅할 때 소프트웨어 개발 플랫폼에서 어떻게 인공지능 학습 라이브러리를 기반으로 인공지능 학습 태스크를 최적화하는지 자세히 설명한다. 기본 연산자의 융합 지원 구체적 실현에서 융합된 연산자는 컴파일 기간에 메모리 다중화, 액세스 최적화, 명령 파이프 라인, 데이터 유 형 최적화(예를 들어, 적용 가능한 서로 다른 데이터 유형에 대한 선택) 등 컴파일 최적화 수단을 채택함으로써 융합 연산자의 전반적 성능을 크게 향상한다. 오프라인 실행 파일 생성 지원 여기에서 생성된 오프라인 실행 파일은 인공지능 알고리즘 모델에서 각 컴퓨팅 노드의 네트워크 가중치 및 명령 과 같은 필요한 네트워크 구조정보를 포함할 수 있으며, 명령은 당해 컴퓨팅 노드가 어떤 커퓨팅 기능을 수행하 는 데 사용하는지 나타낼 수 있다. 구체적으로 명령은 인공지능 학습모델에서 각 컴퓨팅 노드의 컴퓨팅 속성 및 각 컴퓨팅 노드 사이의 연결관계와 같은 정보를 포함할 수 있다. 구체적 실현에서 오프라인 실행 파일은 인공지능 학습 라이브러리에서 이탈하고 인공지능 런타임 라이브러리를 기반으로 개별적으로 실행할 수 있다. 실제 응용에서 오프라인 실행 파일이 상위 소프트웨어 스택과 분리되어 오프라인 실행 파일의 수행이 더 나은 성능과 범용성을 갖도록 한다. 단계: 상기 범용 프로세서가 상기 인공지능 학습 태스크를 송신한다. 본 기술방안인 경우, 엔드 측의 인공지능 프로세서의 설비정보에 근거하여 클라우드 측 설비에서 적응해 낸 클라우드 측의 인공지능 프로세서는 엔드 측의 인공지능 프로세서를 시뮬레이션할 수 있다. 따라서, 소프트 웨어 개발 플랫폼에서 생성된 인공지능 학습 태스크는 클라우드 측의 인공지능 프로세서에 송신되어 실행 하게 된다. 단계: 상기 범용 프로세서가 상기 인공지능 학습 태스크에 대응하는 실행결과를 수신한다. 클라우드 측의 인공지능 프로세서가 인공지능 학습 태스크를 수행할 때 실행결과를 생성하며, 그 실행결과는 소 프트웨어 개발 플랫폼에 피드백되어 표시된다. 본 기술방안인 경우, 실행결과는 상기 인공지능 학습 태스 크의 클라우드 측의 인공지능 프로세서에서의 실행시간이 예상되는 요구 사항에 해당하는지 여부, 상기 인공지 능 학습 태스크의 수행시 클라우드 측의 인공지능 처리 시스템을 점용하는 로드 정보가 예상되는 요구 사항에 해당하는지 여부, 인공지능 학습 태스크를 수행한 결과가 예상되는 요구 사항에 해당하는지 여부 중 하나 이상 의 정보를 포함할 수 있지만 이에 한정되지 않는다. 본 기술방안에서 클라우드 측의 인공지능 처리 시스템은 범 용 프로세서 및 클라우드 측의 인공지능 프로세서를 포함한다. 상기 인공지능 학습 태스크를 수행할 때, 상기 인공지능 학습 태스크를 수행할 때 클라우드 측의 인공지능 프로세서를 점용하는 로드 정보뿐만 아니라 수행과 정에서 메모리를 점용한 정보 및 범용 프로세서의 점용율 등을 알아야 한다. 실행결과에 로드 정보를 포함시키 는 이유는 범용 프로세서에서 하나의 인공지능 학습 태스크가 필요로 하는 리소스가 너무 많으면 엔드 측 설비 에서 실행될 때 효과가 나쁘거나 실행되지 않을 수 있기 때문이다. 단계: 상기 범용 프로세서가 상기 실행결과에 따라 오프라인 실행 파일을 확정한다. 상기 오프라인 실행 파일은 실행결과가 미리 설정된 요구 사항을 충족하면 대응하는 상기 엔드 측의 인공지능 프로세서의 설비정보와 바이너리 명령에 의해 생성된다. 오프라인 실행 파일에 오프라인 실행 파일 버전정보, 인공지능 프로세서 버전정보, 바이너리 명령, 상수 테이블, 입/출력 데이터 크기, 데이터 레이아웃 설명정보 및 파라미터 정보가 포함된다는 것이 이해도어야 한다. 구체적으로, 오프라인 실행 파일의 버전정보는 오프라인 실행 파일의 서로 다른 버전을 표징한다. 인공지 능 프로세서 버전정보는 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보를 지칭한다. 예를 들어, 칩 아 키텍처 버전번호를 통해 하드웨어 아키텍처 정보를 나타낼 수 있고, 또한 기능 설명을 통해 아키텍처 정보를 나 타낼 수도 있다. 데이터 레이아웃 설명정보는 하드웨어 특성에 기초하여 입/출력 데이터 레이아웃 및 유형 등을 전처리하는 것을 의미한다. 상수 테이블, 입/출력 데이터 크기 및 파라미터 정보는 개발한 인공지능 알고리즘 모델에 의해 확정된다. 그 중 파라미터 정보는 인공지능 알고리즘 모델에서의 가중치 데이터일 수 있다. 상수 테이블에는 바이너리 명령 연산을 수행하는 데 사용해야 하는 데이터가 저장된다. 엔드 측의 인공지능 프로세서의 설비정보는 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보 및 실 행환경 파라미터를 포함한다. 상기 실행환경 파라미터는 상기 엔드 측의 인공지능 프로세서의 실행 메인 주파수, 오프 칩 메모리부터 엔드 측의 인공지능 프로세서 사이의 액세스 대역폭, 온칩 스토리지 크기, 엔드 측 의 인공지능 프로세서의 코어 수 및 엔드 측의 인공지능 프로세서의 연산장치의 유형 중 적어도 하나를 포함한 다. 본 기술방안에서 상기 실행결과가 미리 설정된 요구 사항을 충족하면 상기 미리 설정된 요구 사항을 충족하는 바이너리 명령에 따라 대응하는 오프라인 실행 파일을 생성한다. 상기 실행결과가 미리 설정된 요구 사항을 충 족하지 않으면 기능 디버깅 도구 및/또는 성능 튜닝 도구를 통해 실행결과가 상기 미리 설정된 요구 사항을 충 족할 때까지 다음과 같은 프로세스 중 적어도 하나의 최적화 방법을 수행하고, 상기 미리 설정된 요구 사항을 충족하는 바이너리 명령에 따라 대응하는 오프라인 실행 파일을 생성한다. 상기 최적화 방법은 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보를 조정하는 것, 상기 엔드 측의 인공지능 프로세서의 실행환경 파 라미터를 조정하는 것, 및 인공지능 학습 태스크를 최적화하는 것을 포함한다. 구체적으로, 클라우드 측의 인공지능 프로세서가 피드백한 실행결과가 미리 설정된 요구 사항에 해당하면, 현재 수행되는 인공지능 학습 태스크에 대응하는 바이너리 명령은 오프라인 방식을 통해 오프라인 실행 파일로 경화 된다. 클라우드 측의 인공지능 프로세서가 피드백한 실행결과가 미리 설정된 요구 사항에 해당하지 않으면, 인공지능 학습 태스크의 디버깅은 두 가지 애플리케이션 시나리오로 나뉜다. 첫 버째 애플리케이션 시나리오로서 칩 설계 단계에서 애플리케이션을 기반으로 본 기술방안을 이용하여 칩 설계 사양을 평가한다. 이 경우 칩의 하드웨어 아키텍처 정보와 실행환경 파라미터는 모두 변경할 수 있다. 그러면 소프트웨어 개발 플랫폼에서의 수행은 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보를 조정하는 것, 상기 엔드 측의 인공지능 프로세서의 실행환경 파라미터를 조정하는 것, 및 인공지능 학습 태스크를 최적화하는 이 세 가지 최적화 방법 중 적어도 한 가지 최적화 방법을 포함하며, 모두 해당 인공지능 학습 태스크에 대응하는 바이너리 명령을 조정할 수 있다. 매회 조정한 후, 드라이버에 의해 태스크 큐 중 조정 후의 인공지능 학습 태스크를 스케줄링하고, 대응하 는 클라우드 측의 인공지능 프로세서를 수행시켜 새로운 실행결과를 획득한다. 새로운 실행결과가 여전히 기대 치에 해당하지 않으면 사용자는 실행결과가 기대치에 해당할 때까지 전술한 단계를 반복할 수 있다. 최종적으로 디버깅하여 획득한 바이너리 명령은 오프라인 방식을 통해 오프라인 실행 파일로 경화된다. 두 번째 애플리케이션 시나리오로서 엔드 측의 인공지능 프로세서의 테이프 아웃에 관계없이 사용자는 소프트웨 어 개발 플랫폼을 기반으로 설계 및 개발을 시작할 수 있으며, 엔드 측의 인공지능 프로세서와 인공지능 알고리 즘 모델 사이의 적응을 실현할 수 있다. 이런 경우, 실제로는 다른 아키텍처 버전의 칩 사용 권한을 다시 구매 하지 않으면 칩의 하드웨어 아키텍처 정보가 쉽게 변경되지 않는다. 칩의 하드웨어 아키텍처 정보가 변경되지 않는다고 가정하면, 소프트웨어 개발 플랫폼에서의 수행은 현재 하드웨어 아키텍처 정보에 대응하는 칩이 지원 하는 실행환경 파라미터 범위 내에 실행환경 파라미터를 조정하는 것, 및 인공지능 학습 태스크를 최적화하는 이 두 가지 최적화 방법 중 적어도 한 가지 최적화 방법을 포함하며, 모두 모두 해당 인공지능 학습 태스크에 대응하는 바이너리 명령을 조정할 수 있다. 매회 조정한 후, 드라이버에 의해 태스크 큐 중 조정 후의 인공지능 학습 태스크를 스케줄링하고, 대응하는 클라우드 측의 인공지능 프로세서를 수행시켜 새로운 실행결과를 획득한 다. 새로운 실행결과가 여전히 기대치에 해당하지 않으면 사용자는 실행결과가 기대치에 해당할 때까지 전술한 단계를 반복할 수 있다. 최종적으로 디버깅하여 획득한 바이너리 명령은 오프라인 방식을 통해 오프라인 실행 파일로 경화된다. 관건적인 것은, 오프라인 실행 파일을 클라우드 측의 인공지능 프로세서에서 실행할 수 있을 뿐만 아니라, 엔드 측의 인공지능 프로세서에서도 실행할 수 있게 하고, 인공지능 학습 태스크가 클라우드 측의 인공지능 프로세서 에서 수행될 때 생성되는 실행결과와 엔드 측의 인공지능 프로세서에서 수행할 때 생성되는 실행결과가 완전히 일치하거나 특정 허용 오차 범위 내에 있게 하기 위해, 본 기술방안에서 엔드 측의 인공지능 프로세서의 설비정 보에 근거하여 클라우드 측의 인공지능 프로세서의 집합에서 대응하는 엔드 측의 인공지능 프로세서를 시뮬레이 션할 수 있는 클라우드 측의 인공지능 프로세서를 필터링해 내며, 필터링해 낸 클라우드 측의 인공지능 프로세 서의 하드웨어 아키텍처 정보는 대응하는 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 호환되고, 클라우드 측의 인공지능 프로세서의 명령어 세트는 대응하는 엔드 측의 인공지능 프로세서의 명령어 세트와 호 환되므로 오프라인 실행 파일의 원활한 마이그레이션을 실현할 수 있다. 본 기술방안에서 소프트웨어 개발 플랫폼에 상이한 유형의 엔드 측의 인공지능 프로세서의 설비정보를 미 리 저장할 수 있다. 실제 수요에 따라 미리 저장한 설비정보에서 목표정보를 선택하고, 목표정보에 따라 클라우 드 측 설비에서 엔드 측의 인공지능 프로세서를 시뮬레이션할 클라우드 측의 인공지능 프로세서를 확정한 다. 본 기술방안인 경우 다른 하나의 가능한 방안은 실제 수요에 따라 설비정보가 조정될 때마다 사용자가 소프 트웨어 개발 플랫폼에 상이한 설비정보를 설정하고, 소프트웨어 개발 플랫폼은 엔드 측의 인공지능 프로세서의 설비정보를 수신하며, 현재 수신한 엔드 측의 인공지능 프로세서의 설비정보에 따라 클라우드 측 설 비에서 클라우드 측의 인공지능 프로세서를 선택하여 엔드 측의 인공지능 프로세서의 인공지능 프로세서를 시뮬레이션한다. 전술한 엔드 측의 인공지능 프로세서의 설비정보를 획득하는 방식은 철저한 것이 아니라 단지 예시적인 부분일 뿐이라 점에 유의해야 한다. 본 분야의 기술자라면 본 출원 기술방안의 본질을 이해하였으면 본 출원 기술방안에 기초하여 다른 변형 또는 변환을 이룰 수 있다. 예를 들어, 엔드 측 설비에서 요구정 보를 소프트웨어 개발 플랫폼에 송신하고, 소프트웨어 개발 플랫폼이 요구정보를 분석하여 엔드 측의 인공지능 프로세서의 설비정보를 획득한다. 그러나 실현된 기능 및 달성한 기술효과가 본 출원과 유사하면 모두 본 출원의 보호범위에 속해야 한다. 실제 응용에서 클라우드 측 설비에서 클라우드 측의 인공지능 프로세서를 선택하여 엔드 측의 인공지능 프 로세서를 시뮬레이션할 때, 현재 활성화된 엔드 측 설비정보를 드라이버에 써놓고, 드라이버에서의 설비정보에 따라 클라우드 측의 인공지능 프로세서를 적응시킨다. 클라우드 측의 인공지능 프로세서의 적응과정은 다음과 같다. 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보에 따라 클라우드 측의 인공지능 프로세서를 필터링하며, 필터링된 클라우드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보는 대응하는 엔드 측의 인공 지능 프로세서의 하드웨어 아키텍처 정보와 호환되며, 클라우드 측의 인공지능 프로세서의 명령어 세트는 대응 하는 엔드 측의 인공지능 프로세서의 명령어 세트와 호환된다. 또한, 엔드 측의 인공지능 프로세서의 실행환경 파라미터에 따라 필터링해 낸 클라우드 측의 인공지능 프로세서 에 대해 주파수 변조 및 대역 조정을 수행한다. 또한, 인공지능 학습 태스크를 최적화하는 것은 다음과 같은 네 가지 방법을 포함할 수 있다. 첫 번째 방법으로 서 사용자는 소프트웨어 개발 플랫폼에서 프로그래밍 언어를 기반으로 파일의 동적 링크 라이브러리에의 컴파일 을 실현하여 프레임 워크에서 당해 동적 링크 라이브러리를 호출할 수 있다. 두 번째 방법으로서 사용자는 소프트웨어 개발 플랫폼에서 프로그래밍 언어를 기반으로 새로운 연산자를 개발하 고, 호스트가 소유하고 있는 인공지능 학습 라이브러리와 결합하여 새로운 오프라인 실행 파일을 얻는다. 예를 들어 proposal 연산자를 예로 한다. PluginOp 연산자로 Faster－R－Cnn 중의 Proposal 연산자를 대체하고 전용 프로그래밍 언어로 작성한 proposal＿kernel. mlu 연산자를 호출하면 Cambricon－Caffe 프레임 워크의 Proposal 연산자는 PluginOp를 통해 전용 프로그래밍 언어로 실현되는 ProposalKernel로 대체된다. 따라서 전용 프로그래밍 언어와 종래의 인공지능 학습 라이브러리를 관련되도록 하며, 인공지능 학습 라이브러리의 각종 특 성 및 온라인, 오프라인, 계층 별, 융합 등 실행모드를 지원한다. 첫 번째 방법과 두 번째 방법으로부터 프레임 워크에서 이미 많은 계층과 연산자가 지원되어 있어 일반 모델은 모두 클라우드의 서버 보드에 넣고 실행될 수 있음을 알 수 있다. 그러나 알고리즘은 업데이트가 빠르게 바뀌며 개인 또는 조직도 일부 사용자 지정 연산자와 알고리즘을 누적할 수 있다. 한편으로는 사용자 지정 알고리즘을 노출하지 않으려 하고, 다른 편으로는 기본 라이브러리를 통해 실제 애플리케이션을 직접 지원하는 효율성은 수 요를 충족할 수 없기 때문에 전용 프로그래밍 언어를 제공하여 이전 개발모드의 유연성 문제를 해결할 수 있도록 개발자의 독립적 알고리즘 개발을 도운다. 세 번째 방법으로서 사용자는 소프트웨어 개발 플랫폼에서 현재 호스트가 소유하고 있는 인공지능 학습 라이브 러리의 버전에서 하나를 선택하고, 대응하는 인공지능 런타임 라이브러리에 매칭시키며, 현재 호스트가 소유하 고 있는 인공지능 학습 라이브러리가 수요를 충족할 수 없는 경우 소프트웨어 개발 플랫폼을 통해 요구를 송신 하여 호스트 인공지능 학습 라이브러리의 버전을 업그레이드하는 목적을 달성할 수 있다. 소프트웨어 개발 플랫 폼의 운영자는 요구에 따라 소프트웨어 개발 플랫폼에 해당한 새 버전의 인공지능 학습 라이브러리 및 대응하는 인공지능 런타임 라이브러리를 제공하며, 사용자는 소프트웨어 개발 플랫폼에서 최신 버전의 인공지능 학습 라 이브러리 및 대응하는 인공지능 런타임 라이브러리를 채택하고, 최신 버전의 인공지능 학습 라이브러리를 기반 으로 디버깅된 바이너리 명령을 획득한다. 네 번째 방법으로서 사용자는 인공지능 알고리즘 모델을 조정하는 것을 통해 인공지능 학습 태스크를 최적화하 는 목적을 달성할 수 있다. 실제 응용에서 상술한 네 가지 인공지능 학습 태스크를 최적화하는 방법에서 적어도 한 가지 방법을 채용하면 인공지능 학습 태스크를 최적화하는 목적을 달성할 수 있다. 인공지능 알고리즘 모델의 조정 여부에 관계없이 인공지능 학습 라이브러리를 최적화 및/또는 엔드 측의 인공지능 프로세서의 설비정보를 조정하여 인공지능 학 습 태스크를 조정하는 목적을 달성할 수 있고, 엔드 측의 인공지능 프로세서와 인공지능 알고리즘 모델 사이의 적응을 실현할 수 있다. 도5에 도시된 방안은 소프트웨어 개발 플랫폼을 제공하고 있으며, 사용자는 당해 소프트웨어 개발 플랫폼에서 알고리즘과 인공지능 프로세서 사이의 기능, 성능, 정확도의 디버깅을 완성할 수 있고, 디버깅 완료 후에 생성 된 오프라인 실행 파일은 호환성이 있는 아키텍처 위의 다양한 엔드 측의 SoC 칩에 배포할 수 있으며, 사용자가 하드웨어 실체를 입수하지 않아도 사전에 알고리즘과 인공지능 프로세서 사이의 기능, 성능, 정확도를 디버깅할 수 있고, 제품의 개발주기가 크게 단축된다는 장점이 있다. 또한, 각 엔드 측의 SoC 칩마다 개별적으로 개발하 거나 또는 일련의 개발환경에 적응시킬 필요가 없다. 도7에 도시된 바와 같이, 본 출원이 제공하는 데이터 처리방법의 다른 한 가지 흐름도이다. 상기 방법은 클라우 드 측의 인공지능 프로세서에 적용되며, 다음과 같은 단계를 포함한다. 단계: 엔드 측의 인공지능 프로세서의 설비정보에 의해 생성되는 바이너리 명령에 따라 확정되는 인공지능 학습 태스크를 수신한다. 단계: 상기 인공지능 학습 태스크를 수행하여 실행결과를 생성한다. 본 단계에서 상기 실행결과는 상기 인공지능 학습 태스크의 클라우드 측의 인공지능 프로세서에서의 실행시간이 예상되는 요구 사항에 해당하는지 여부, 상기 인공지능 학습 태스크의 수행시 클라우드 측의 인공지능 처리 시 스템을 점용하는 로드 정보가 예상되는 요구 사항에 해당하는지 여부, 및 인공지능 학습 태스크를 수행한 결과 가 예상되는 요구 사항에 해당하는지 여부 중 하나 이상의 정보를 포함할 수 있지만 이에 한정되지 않는다. 또한, 도5에 도시된 모든 관련 기술방안의 내용은 모두 도7에 도시된 기술방안에 적용되며, 여기에서 다시 상세 히 설명하지 않는다. 또한, 클라우드 측 설비인 경우 한 가지 애플리케이션 시나리오로서 클라우드 측의 인공지능 프로세서의 집합에서 가능한 한 엔드 측의 인공지능 프로세서의 모든 버전의 하드웨어 아키텍처 정보를 포함해야 하는 것을 이해해야 한다. 예를 들어 사이즈는 하드웨어 아키텍처 정보의 버전과 대응되며, 엔드 측의 인공지능 프로세서 의 사이즈로는 A, B, C 등이 있다. 클라우드 측의 인공지능 프로세서의 집합에는 모두 사이즈가 A인 인공지능 프로세서, 사이즈가 B인 인공지능 프로세서, 사이즈가 C인 인공지능 프로세서를 포함되어 있다. 다른 한 가지 애플리케이션 시나리오로서 클라우드 측의 인공지능 프로세서의 집합에서 인공지능 프로세서의 하드웨어 아키텍 처는 고사양 기능, 중사양 기능 또는 저사양 기능을 실현할 수 있다. 예를 들어 서로 다른 애플리케이션 시나리 오 및 실제수요에 따라 인공지능 프로세서의 일부 기능을 차단하여 저사양 기능의 인공지능 프로세서 또는 중사 양 기능의 인공지능 프로세서로 변환하여 서로 다른 사용자의 수요를 충족시킨다. 이 경우, 본 기술방안에서의 소프트웨어 개발 플랫폼은 바이너리 명령을 생성할 때 엔드 측의 인공지능 프로세서의 설비정보에 기초하 고, 또한 드라이버를 통해 필터링해 낸 클라우드 측의 인공지능 프로세서가 지원하는 서로 다른 실행환경 파라 미터를 변경함으로써 고사양 버전의 인공지능 프로세서 중의 일부 기능이 차폐되게 하며, 구현된 기능만이 대응 하는 엔드 측의 인공지능 프로세서의 기능과 서로 적응된다. 따라서, 고사양 버전의 인공지능 프로세서의 실행 환경 파라미터의 값 범위는 엔드 측의 인공지능 프로세서가 지원하는 모든 실행환경 파라미터를 포함한다. 예를들어 클라우드 측의 인공지능 프로세서의 온칩 메모리의 크기는 100M이고, 엔드 측의 인공지능 프로세서의 온칩 메모리의 크기는 100M보다 작은 값일 수 있다. 또한, 시분할 다중화의 방법을 이용하고, 가상 머신 기술을 통해 사용자가 클라우드 측 설비의 인공지능 프로세서의 리소스를 사용하는 시간대에 근거하여 클라우드 측 설비의 인공지능 프로세서를 합리적으로 할 당하면 리소스를 상이한 시간대의 인공지능 학습 태스크에 할당하여 배포해야 할 클라우드 측의 인공지능 프로 세서의 개발환경 수를 줄일 수 있다. 또한, 클라우드 측 설비의 인공지능 프로세서의 집합에는 반드시 모두가 칩 실체일 필요가 없으며 FPGA일 수 있다. 현대 IC 설계 검증의 주류 기술을 참조하면 하드웨어 기술 언어(Verilog 또는 VHDL)로 완성된 회로설 계는 간단한 종합 및 레이아웃을 통해 신속하게 FPGA에 프로그래밍할 수 있다. 본 기술방안인 경우 클라우드 측의 인공지능 프로세서에 적응하는 칩 실체가 존재하지 않으면 FPGA를 사용하여 사용자에게 서비스를 제공할 수 있다. 엔드 측의 인공지능 프로세서의 설비정보에 따라 수요에 해당한 FPGA를 필터링해 내며, 당해 FPGA는 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 대응하는 미러 파일을 갖는다. 수요에 해당한 FPGA가 존재하지 않으면 소프트웨어 개발 플랫폼은 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보에 대 응하는 미러 파일을 유휴 FPGA에 구울 수 있고, 당해 FPGA는 소프트웨어 개발 플랫폼에 의해 송신된 인공지능 학습 태스크를 수행한다. 클라우드 측의 인공지능 프로세서인 경우 보다 세립도(fine-grained)인 리소스 할당을 제공할 수 있다. 예를 들 어 사용자A가 소프트웨어 개발 플랫폼에서 M개의 코어(core)를 기초로 엔드 측의 인공지능 프로세서를 구 성하여 인공지능 학습 태스크를 생성하고, 적응된 클라우드 측의 인공지능 프로세서가 갖는 코어(core) 총수가 N이며, N개의 코어(core) 중 P개의 코어(core)가 이미 사용자B가 시작한 인공지능 학습 태스크에서 사용된다. 이 만족되고, 사용자A에 대응하는 엔드 측의 인공지능 프로세서의 설비정보가 반드시 사용자B에 대응하는 엔드 측의 인공지능 프로세서의 설비정보와 같으면, 소프트웨어 개발 플랫폼 중 인공지능 런타임 라이브러리는 서로 다른 사용자가 시작한 인공지능 학습 태스크를 서로 다른 코어에 할당하며, 각 코어에서 서 로 다른 인공지능 학습 태스크를 수행하여 클라우드 측의 인공지능 프로세서의 리소스에 대한 보다 세립도의 할 당을 달성할 수 있다. 또한, 클라우드 측의 인공지능 프로세서인 경우, 통상의 아키텍처의 재구성이 불 가능한 인공지능 프로세서일 수도 있고, 아키텍처의 재구성이 가능한 인공지능 프로세서일 수도 있다. 아키텍처의 재구성이 가능한 인공지능 프로세서인 경우, 드라이버 중의 설비정보를 이용하여 재구성형 칩 내부의 환경 실행 파라미터를 조정하고, 소 프트웨어 개발 플랫폼에 의해 송신된 인공지능 학습 태스크에 따라 재구성형 칩 내부의 대응하는 기능모듈 을 호출한다. 즉, 실제 응용에 따라 재구성형 칩 내부의 기능모듈을 조정하여 엔드 측의 인공지능 프로세서를 재구성 후의 칩으로 대체할 수 있다. 상술한 클라우드 측의 인공지능 프로세서에 관한 설명에 기초하여, 본 기술방안인 경우 소프트웨어 개발 플랫폼 이 일정한 시간 내의 각 시간대에서 서로 다른 하드웨어 아키텍처 정보를 사용하는 클라우드 측의 인공지 능 프로세서의 사용자 수를 통계하여, 사용자 수요를 충족시킬 수 있는 최소값V를 추정하여 얻으며, 당해 숫자V 는 클라우드 측의 인공지능 프로세서의 배치 수의 최소값이다. 이를 기초로 결함허용이나 사용자 수의 급격한 증가를 방지하기 위해 적은 수 W개 여분의 인공지능 프로세서를 추가하면(V＋W)가 클라우드 측 설비에 배 치해야 할 인공지능 프로세서의 수이다. 동시에, 소프트웨어 개발 플랫폼은 정기적으로 사용자 수의 변화 를 통계하며, 클라우드 측 설비에 배포하는 인공지능 프로세서의 수를 변화시켜 사용자의 수요를 충족시키 고 클라우드 측의 오버 헤드를 감소시킨다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "요약하면, 상술한 설명으로부터 실시간 배포 방법을 채택하고 소프트웨어 개발 플랫폼에 의해 송신된 인공지능 학습 태스크에 따라 클라우드 측 설비에 배포하는 인공지능 프로세서의 리소스를 동적으로 조정하여 사용 자가 감지하지 못하는 상황에서 시분할 다중화 방법을 이용하며, 사용자가 클라우드 측 설비의 인공지능 프로세서의 리소스를 사용하는 시간대에 따라 상이한 개발환경을 배치함으로써 동일 클라우드 측의 인공지능 프 로세서 리소스를 서로 다른 시간대의 인공지능 학습 태스크에 할당하여 배포해야 할 클라우드 측의 인공지능 프 로세서의 개발환경 수를 줄일 수 있다는 것을 알 수 있다. 도8에 도시된 바와 같이, 본 출원이 제안하는 또 다른 한 가지 데이터 처리방법의 흐름도이다. 상기 방법은 엔 드 측의 인공지능 프로세서에 적용되며, 다음과 같은 단계를 포함한다. 단계: 오프라인 실행 파일을 판독한다. 상기 오프라인 실행 파일은 실행결과가 미리 설정된 요구 사항을 충족할 때 대응하는 상기 엔드 측의 인공지능 프로세서의 설비정보와 바이너리 명령에 의해 확정된다. 또한, 도5, 도7에 도시된 모든 관련 기술방안의 내용은 모두 도8에 도시된 기술방안에 적용되며, 여기에서 다시 상세히 설명하지 않는다. 엔드 측의 인공지능 프로세서가 생성한 인공지능 학습 태스크에 대하여, 소프트웨어 개발 플랫폼에서 클라 우드 측의 인공지능 프로세서가 피드백한 실행결과에 따라 인공지능 학습 태스크를 최적화 및 디버깅하며, 실행 결과가 예상되는 요구 사항에 달하면 디버깅 후의 인공지능 학습 태스크의 바이너리 명령은 경화처리에 의해 오 프라인 실행 파일로 변환하며, 이전의 디버깅 및 성능 결과의 경화를 달성한다. 나중에, 오프라인의 애플리케이 션 프로그램을 작성하여 프로그래밍 프레임 워크를 벗어나 실제 애플리케이션 시나리오에서의 정확도 정상을 확 보하면 엔드 측 설비에 크로스 컴파일하여 현장 배포할 수 있다. 도9에 도시된 바와 같이, 본 출원이 제안하는 또 다른 한 가지 데이터 처리의 흐름도이다. 상기 시스템은 범용 프로세서와 클라우드 측의 인공지능 프로세서를 포함하며, 다음과 같은 단계를 포함한다. 단계(a): 상기 범용 프로세서가 엔드 측의 인공지능 프로세서의 설비정보에 따라 바이너리 명령을 생성하고, 상 기 바이너리 명령에 따라 인공지능 학습 태스크를 생성하며, 상기 인공지능 학습 태스크를 클라우드 측의 인공 지능 프로세서에 송신하여 실행한다. 단계(b): 상기 클라우드 측의 인공지능 프로세서가 인공지능 학습 태스크를 수신하고, 상기 인공지능 학습 태스 크를 수행하여 실행결과를 생성한다. 단계(c): 상기 범용 프로세서가 상기 인공지능 학습 태스크에 대응하는 실행결과를 수신하고, 상기 실행결과에 따라 오프라인 실행 파일을 확정한다. 그 중 상기 오프라인 실행 파일은 실행결과가 미리 설정된 요구 사항을 충족하면 대응하는 상기 엔드 측의 인공지능 프로세서의 설비정보와 바이너리 명령에 의해 생성된다. 또한, 도5, 도7에 도시된 모든 관련 기술방안의 내용은 모두 도9에 도시된 기술방안에 적용되며, 여기에서 다시 상세히 설명하지 않는다. 도10에 도시된 바와 같이, 데이터 처리장치의 한 기능 블록도이다. 상기 장치는 메모리 및 범용 프로세서를 포 함하며, 상기 메모리에는 상기 범용 프로세서에 의해 실행 가능한 컴퓨터 프로그램이 저장되어 있고, 상기 범용 프로세서가 상기 컴퓨터 프로그램을 수행할 때 실현하는 데이터 처리 흐름은, 엔드 측의 인공지능 프로세서의 설비정보에 따라 바이너리 명령을 생성하고, 상기 바이너리 명령에 따라 인공지 능 학습 태스크를 생성하는 것; 상기 인공지능 학습 태스크를 클라우드 측의 인공지능 프로세서에 송신하여 실행시키는 것; 상기 인공지능 학습 태스크에 대응하는 실행결과를 수신하는 것; 및 상기 실행결과에 따라 오프라인 실행 파일을 확정하는 것을 포함한다. 그 중, 상기 오프라인 실행 파일은 실행 결과가 미리 설정된 요구 사항을 충족하면 대응하는 상기 엔드 측의 인공지능 프로세서의 설비정보와 바이너리 명령에 의해 생성된다. 바람직하게는, 상기 설비정보는 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 실행환경 파라 미터를 포함한다. 바람직하게는, 상기 실행환경 파라미터는 상기 엔드 측의 인공지능 프로세서의 실행 메인 주파수, 오프 칩 메모 리부터 엔드 측의 인공지능 프로세서 사이의 액세스 대역폭, 온칩 스토리지 크기, 엔드 측의 인공지능 프로세서 의 코어 수, 엔드 측의 인공지능 프로세서 중 연산장치의 유형 중 적어도 하나를 포함한다. 바람직하게는, 상기 범용 프로세서가 상기 컴퓨터 프로그램을 수행할 때 실현하는 데이터 처리 흐름은 상기 엔 드 측의 인공지능 프로세서의 설비정보를 수신하는 것을 더 포함한다. 바람직하게는, 상기 범용 프로세서가 상기 컴퓨터 프로그램을 수행할 때 실현하는 데이터 처리 흐름은 설비정보 를 드라이버에 써놓고 상기 드라이버 중 설비정보에 따라 클라우드 측의 인공지능 프로세서에 적응시키기는 것 을 더 포함한다. 바람직하게는, 상기 범용 프로세서가 상기 실행결과에 따라 상기 오프라인 실행 파일을 확정하는 겨우, 상기 컴 퓨터 프로그램을 수행하여 실현하는 데이터 처리 흐름은, 상기 실행결과가 미리 설정된 요구 사항을 충족하면 상기 미리 설정된 요구 사항을 충족하는 바이너리 명령에 따라 대응하는 오프라인 실행 파일을 생성하는 것을포함한다. 바람직하게는, 상기 범용 프로세서가 상기 실행결과에 따라 상기 오프라인 실행 파일을 확정하는 경우, 상기 컴 퓨터 프로그램을 수행할 때 실현하는 데이터 처리 흐름은, 상기 실행결과가 미리 설정된 요구 사항을 충족하지 않으면 실행결과가 상기 미리 설정된 요구 사항을 충족할 때까지 다음과 같은 프로세스 중 적어도 하나의 최적화 방법을 수행하고, 상기 미리 설정된 요구 사항을 충족하 는 바이너리 명령에 따라 대응하는 오프라인 실행 파일을 생성하는 것을 더 포함하며, 상기 최적화 방법은, 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보를 조정하는 것, 상기 엔드 측의 인공지능 프로세서의 실행환경 파라미터를 조정하는 것, 또는 인공지능 학습 태스크를 최적화하는 것을 포함한다. 바람직하게는, 상기 범용 프로세서가 인공지능 학습 태스크를 최적화할 때 실현하는 데이터 처리 흐름은 전용 프로그래밍 언어에 따라 인공지능 학습 태스크를 최적화하는 것을 포함한다. 바람직하게는, 상기 범용 프로세서가 인공지능 학습 태스크를 최적화할 때 실현하는 데이터 처리 흐름은 인공지 능 학습 라이브러리의 버전을 업데이트하는 것을 통해 인공지능 학습 태스크를 최적화하는 것을 더 포함한다. 바람직하게는, 상기 범용 프로세서가 인공지능 학습 태스크를 최적화할 때 실현하는 데이터 처리 흐름은 인공지 능 알고리즘 모델을 조정하는 것을 더 포함한다. 바람직하게는, 상기 실행결과는 상기 인공지능 학습 태스크의 클라우드 측의 인공지능 프로세서에서의 실행시간 이 예상되는 요구 사항에 해당하는지 여부, 상기 인공지능 학습 태스크의 수행시 클라우드 측의 인공지능 처리 시스템을 점용하는 로드 정보가 예상되는 요구 사항에 해당하는지 여부, 인공지능 학습 태스크를 수행한 결과가 예상되는 요구 사항에 해당하는지 여부 중 적어도 하나의 정보를 포함할 수 있지만 이에 한정되지 않는다. 상기 메모리와 범용 프로세서가 실현하는 구체적 기능은 본 명세서에서의 전술한 실시태양과 비교하여 설명할 수 있으며, 또한 전술한 실시태양의 기술효과를 달성할 수 있어, 여기에서는 다시 설명하지 않는다. 본 실시태양에서 상기 메모리는 정보를 저장하기 위한 물리적 장치를 포함할 수 있다. 이런 장치는 일반적으로 정보를 디지털화한 후 다시 전기, 자기 또는 광학과 같은 방법을 이용한 매체에 저장할 수 있다. 본 실시태양에 기재된 메모리는 전기 에너지를 이용하여 정보를 저장하는 RAM, ROM 등과 같은 장치; 자기 에너지를 이용하여 정보를 저장하는 하드 디스크, 플로피 디스크, 자기 테이프, 자기 코어 메모리, 자기 버블 메모리, U 디스크와 같은 장치; 및 광학적으로 정보를 저장하는 CD 또는 DVD와 같은 장치를 포함할 수도 있다. 물론 양자 메모리, 그래 핀 메모리 등과 같은 다른 방식의 메모리도 있다. 도11에 도시된 바와 같이, 인공지능 프로세서의 한 기능 블록도이다. 당해 인공지능 프로세서는 엔드 측의 인공 지능 프로세서의 설비정보에 의해 생성되는 바이너리 명령에 따라 확정되는 인공지능 학습 태스크를 수신하기 위한 수신모듈, 및 상기 인공지능 학습 태스크를 수행하여 실행결과를 생성하기 위한 수행모듈을 포함한다. 바람직하게는, 상기 수행모듈이 생성하는 실행결과는 상기 인공지능 학습 태스크의 상기 클라우드 측의 인공지 능 프로세서에서의 실행시간이 예상되는 요구 사항에 해당하는지 여부, 상기 인공지능 학습 태스크의 수행시 클 라우드 측의 인공지능 처리 시스템을 점용하는 로드 정보가 예상되는 요구 사항에 해당하는지 여부, 인공지능 학습 태스크를 수행한 결과가 예상되는 요구 사항에 해당하는지 여부 중 하나 이상의 정보를 포함할 수 있지만, 이에 한정되지 않는다. 바람직하게는, 상기 클라우드 측의 인공지능 프로세서는 인공지능 프로세서 하드웨어 실체, 필드 프로그래머블 게이트 어레이, 시뮬레이터 중 적어도 하나를 포함한다. 바람직하게는, 상기 인공지능 프로세서는 아키텍처의 재구성이 가능한 인공지능 프로세서이다. 도12에 도시된 바와 같이, 인공지능 프로세서의 다른 하나의 기능 블록도이다. 당해 인공지능 프로세서는 오프라인 실행 파일을 획득하기 위한 획득모듈을 포함한다. 상기 오프라인 실 행 파일은 실행결과가 미리 설정된 요구 사항을 충족할 때 대응하는 상기 엔드 측의 인공지능 프로세서의 설비 정보와 바이너리 명령에 의해 확정된다. 바람직하게는, 요구정보를 송신하기 위한 송신모듈을 더 포함하며, 상기 요구정보는 상기 엔드 측의 인공지능 프로세서의 설비정보를 포함한다. 바람직하게는, 상기 설비정보는 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 실행환경 파라 미터를 포함한다. 바람직하게는, 상기 실행환경 파라미터는 상기 엔드 측의 인공지능 프로세서의 실행 메인 주파수, 오프 칩 메모 리부터 엔드 측의 인공지능 프로세서 사이의 액세스 대역폭, 온칩 스토리지 크기, 엔드 측의 인공지능 프로세서 의 코어 수, 엔드 측의 인공지능 프로세서의 연산장치의 유형 중 적어도 하나를 포함한다. 도13에 도시된 바와 같이, 데이터 처리 시스템을 나타내고 있다. 당해 시스템은 범용 프로세서와 클라우 드 측의 인공지능 프로세서를 포함한다. 또한, 본 실시예에서 범용 프로세서 및 클라우드 측의 인 공지능 프로세서의 구체적 구현에 대해서는 전술한 설명을 참조하고, 더 이상의 세부 사항은 여기에 제공 되지 않음을 이해해야 한다. 본 실시예에서 본 출원의 실시예는 판독 가능한 저장 매체도 제공하는 바, 그 매체에는 컴퓨터 프로그램이 저장 되고, 컴퓨터 프로그램을 수행하는 흐름이 전술한 도5, 도7, 도8, 및 도9에 도시된 데이터 처리방법을 구현한다. 이상에서 알 수 있는 바와 같이, 본 출원의 실시예는 데이터 처리방법 및 관련제품을 제공하며, 인공지능 프로 세서의 테이프 아웃 여부에 관계없이, 본 기술방안은 사전에 인공지능 알고리즘 모델과 인공지능 프로세서 사이 의 디버깅 작업을 실현할 수 있다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "본 발명이 속하는 기술분야에서 통상의 지식을 가진자라면 순수한 컴퓨터 판독 가능한 프로그램 코드 방식으로 클라이언트 및 서버에서 본 기술방안을 구현하는 것 외에도, 클라이언트 및 서버에서 로직 게이트, 스위치, 특 정 용도용 집적회로, 프로그래머블 로직 컨트롤러와 임베디드 마이크로 컨트롤러 등의 형태로 기술방안의 단계 를 완전히 실현할 수 있음을 이해할 수 있다. 따라서, 이러한 클라이언트 및 서버는 하드웨어 구성요소로 간주 될 수 있고, 그에 포함된 각종 기능을 실현하기 위한 장치도 하드웨어 구성요소 내의 구조로 간주될 수도 있다. 또는, 각종 기능을 실현하기 위한 장치를 방법을 실현하는 소프트웨어 모듈로 간주할 수도 있고 하드웨어 구성 요소 내의 구조로 간주할 수도 있다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "상기 실시태양에 대한 설명으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진자는 본 출원이 소프트 웨어 및 필요한 범용 하드웨어 플렛폼에 의해 실현될 수 있음을 명백히 이해할 수 있을 것이다. 이러한 이해에 기초하여, 본 출원의 기술방안은 실질적으로 종래기술에 기여하는 부분은 소프트웨어 제품의 형태로 구현할 수 있으며, 당해 컴퓨터 소프트웨어 제품은 ROM/RAM, 자기 디스크, 광 디스크 등과 같은 저장 매체에 저장할 수 있 고, 저장 매체에는 컴퓨터 장치(개인용 컴퓨터, 서버, 또는 네트워크 장치 등일 수 있음)로 하여금 본 출원 각 실시예 또는 실시예의 일부 부분에 기재된 방법을 수행하게 하기 위한 복수의 명령을 포함한다. 본 명세서에서의 각 실시예는 점진적으로 설명되며, 각 실시예 사이에서 동일하거나 유사한 부분은 서로 참조될 수 있다. 각 실시예는 기타 실시예와의 차이점에 초점을 두고 있다. 특히 클라이언트 및 서버의 실시예인 경우, 모두 전술한 방법의 실시예의 설명을 참조하여 비교하면서 설명할 수 있다. 본 출원은 일반적으로 컴퓨터에 의해 수행되는 컴퓨터 수행 가능한 명령을 프로그램 모듈과 같이 설명한다. 일 반적으로, 프로그램 모듈은 특정 태스크를 수행하거나 특정 추상 데이터 유형을 실현하는 루틴, 프로그램, 객체, 구성요소, 데이터 구조 등을 포함한다. 또한 분산 컴퓨팅 환경에서 본 출원을 실시할 수 있으며, 이러한 분산 컴퓨팅 환경에서 통신 네트워크를 통해 연결되는 원격 처리장치에 의해 태스크를 수행할 수 있다. 분산 컴 퓨팅 환경에서, 프로그램 모듈은 저장장치를 포함하여 호스트 및 원격 컴퓨터 저장 매체에 위치할 수 있다."}
{"patent_id": "10-2019-7037882", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "본 출원은 실시예를 통해 설명되었지만, 본 발명이 속하는 기술분야에서 통상의 지식을 가진자라면 본 출원의 사상을 벗어나지 않고 본 출원에 많은 변형 및 변경을 가져 올 수 있고, 본 출원의 사상을 벗어나지 않는 한 이 러한 변형 및 변경이 첨부한 특허 청구범위에 포함됨을 이해할 수 있다. 또한, 이하의 조목에 의해 상술한 내용이 더 잘 이해 될 것이다. A1. 범용 프로세서 및 클라우드 측의 인공지능 프로세서를 포함하는 데이터 시스템에 적용되는 데이터 처리방법 에 있어서, 상기 범용 프로세서가 엔드 측의 인공지능 프로세서의 설비정보에 따라 바이너리 명령을 생성하고, 상기 바이너 리 명령에 따라 인공지능 학습 태스크를 생성하며, 상기 인공지능 학습 태스크를 클라우드 측의 인공지능 프로 세서에 송신하여 실행시키고; 상기 클라우드 측의 인공지능 프로세서가 인공지능 학습 태스크를 수신하고, 상기 인공지능 학습 태스크를 수행 하여 실행결과를 생성하며; 상기 범용 프로세서가 상기 인공지능 학습 태스크에 대응하는 실행결과를 수신하고, 상기 실행결과에 따라 오프 라인 실행 파일을 확정하며, 상기 오프라인 실행 파일은 실행결과가 미리 설정된 요구 사항을 충족하면 대응하 는 상기 엔드 측의 인공지능 프로세서의 설비정보와 바이너리 명령에 의해 생성된다. A2. 상기 A1에 따른 상기 방법에서, 상기 설비정보는 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 실행환경 파라미터를 포함한다. A3. 상기A2에 따른 상기 방법에서, 상기 실행환경 파라미터는 상기 엔드 측의 인공지능 프로세서의 실행 메인 주파수, 오프 칩 메모리부터 엔드 측의 인공지능 프로세서 사이의 액세스 대역폭, 온칩 스토리지 크기, 엔드 측 의 인공지능 프로세서의 코어 수, 엔드 측의 인공지능 프로세서의 연산장치의 유형 중 적어도 하나를 포함한다. A4. 상기 A1에 따른 상기 방법에서, 상기 범용 프로세서가 상기 엔드 측의 인공지능 프로세서의 설비정보를 수 신는 것을 더 포함한다. A5. 상기 A4에 따른 상기의 방법에서, 상기 범용 프로세서가 설비정보를 드라이버에 써놓고, 상기 드라이버 중 설비정보에 따라 클라우드 측의 인공지능 프로세서에 적응시키기는 것을 더 포함한다. A6. 상기 A1에 따른 상기 방법에서, 상기 실행결과에 따라 상기 오프라인 실행 파일을 확정하는 것은, 상기 실 행결과가 미리 설정된 요구 사항을 충족하면 상기 미리 설정된 요구 사항을 충족하는 바이너리 명령에 따라 대 응하는 오프라인 실행 파일을 생성하는 것을 포함한다. A7. 상기 A1에 따른 상기 방법에서, 상기 실행결과에 따라 상기 오프라인 실행 파일을 확정하는 것은, 상기 실행결과가 미리 설정된 요구 사항을 충족하지 않으면 실행결과가 상기 미리 설정된 요구 사항을 충족할 때까지 다음과 같은 프로세스 중 적어도 하나의 최적화 방법을 수행하고, 상기 미리 설정된 요구 사항을 충족하 는 바이너리 명령에 따라 대응하는 오프라인 실행 파일을 생성하는 것을 더 포함하며, 상기 최적화 방법은, 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보를 조정하는 것, 상기 엔드 측의 인공지능 프로세서의 실행환경 파라미터를 조정하는 것, 또는 인공지능 학습 태스크를 최적화하는 것을 포함한다. A8. 상기 A7에 따른 상기의 방법에서, 상기 인공지능 학습 태스크를 최적화하는 단계는 전용 프로그래밍 언어에 따라 인공지능 학습 태스크를 최적화하는 것을 포함한다. A9. 상기 A7 또는 A8에 따른 상기 방법에서, 상기 인공지능 학습 태스크를 최적화하는 단계는 인공지능 학습 라 이브러리의 버전을 업데이트하는 것을 통해 인공지능 학습 태스크를 최적화하는 것을 더 포함한다. A10. 상기 A7－A9 중 어느 한 항에 따른 상기 방법에서, 상기 인공지능 학습 태스크를 최적화하는 단계는 인공 지능 알고리즘 모델을 조정하는 것을 더 포함한다. A11. 상기 A1－A10중 어느 한 항에 따른 상기 방법에서, 상기 실행결과는 상기 인공지능 학습 태스크의 클라우 드 측의 인공지능 프로세서에서의 실행시간이 예상되는 요구 사항에 해당하는지 여부, 상기 인공지능 학습 태스 크의 수행시 클라우드 측의 인공지능 처리 시스템을 점용하는 로드 정보가 예상되는 요구 사항에 해당하는지 여 부, 인공지능 학습 태스크를 수행한 결과가 예상되는 요구 사항에 해당하는지 여부 중 적어도 하나를 포함한다. A12. 상기 A1에 따른 상기 방법에서, 상기 클라우드 측의 인공지능 프로세서는 인공지능 프로세서 하드웨어 실 체, 필드 프로그래머블 게이트 어레이, 시뮬레이터 중 적어도 하나를 포함한다. A13. 상기 A12에 따른 상기 방법에서, 상기 인공지능 프로세서 하드웨어 실체는 아키텍처의 재구성이 가능한 인 공지능 프로세서이다. B14. 메모리, 범용 프로세서, 및 클라우드 측의 인공지능 프로세서를 포함하고, 상기 메모리에는 상기 범용 프 로세서 및/또는 상기 클라우드 측의 인공지능 프로세서에 의해 실행 가능한 컴퓨터 프로그램이 저장되어 있는 데이터 처리장치에 있어서, 상기 범용 프로세서는 엔드 측의 인공지능 프로세서의 설비정보를 기반으로 바이너리 명령을 생성하고, 상기 바 이너리 명령에 따라 인공지능 학습 태스크를 생성하며, 상기 인공지능 학습 태스크를 클라우드 측의 인공지능 프로세서에 송신하여 실행시키기 위한 것이고, 상기 클라우드 측의 인공지능 프로세서는 인공지능 학습 태스크를 수신하고 상기 인공지능 학습 태스크를 수행 하여 실행결과를 생성하기 위한 것이며, 상기 범용 프로세서는 상기 인공지능 학습 태스크에 대응하는 실행결과를 수신하고, 상기 실행결과에 따라 오프 라인 실행 파일을 확정하기 위한 것이기도 하며, 상기 오프라인 실행 파일은 실행결과가 미리 설정된 요구 사항 을 충족하면 대응하는 상기 엔드 측의 인공지능 프로세서의 설비정보와 바이너리 명령에 의해 생성된다. B15. 상기 B14에 따른 상기 장치에서, 상기 설비정보는 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 실행환경 파라미터를 포함한다. B16. 상기 B15에 따른 상기 장치에서, 상기 실행환경 파라미터는 상기 엔드 측의 인공지능 프로세서의 실행 메 인 주파수, 오프 칩 메모리부터 엔드 측의 인공지능 프로세서 사이의 액세스 대역폭, 온칩 스토리지 크기, 엔드 측의 인공지능 프로세서의 코어 수, 엔드 측의 인공지능 프로세서의 연산장치의 유형 중 적어도 하나를 포함한 다. B17. 상기 B14에 따른 상기 장치에서, 상기 범용 프로세서 상기 엔드 측의 인공지능 프로세서의 설비정보를 수 신하는 데에도 사용된다. B18. 상기 B17에 따른 상기 장치에서, 상기 범용 프로세서 설비정보를 드라이버에 써놓고, 상기 드라이버 중 설 비정보에 따라 클라우드 측의 인공지능 프로세서에 적응시키기는 데에도 사용된다. B19. 상기 B14에 따른 상기 장치에서, 상기 범용 프로세서는 상기 실행결과에 따라 상기 오프라인 실행 파일을 확정하는 데에도 사용되며, 상기 실행결과가 미리 설정된 요구 사항을 충족하면, 상기 미리 설정된 요구 사항을 충족하는 바이너리 명령에 따라 대응하는 오프라인 실행 파일을 생성한다. B20. 상기 B14에 따른 상기 장치에서, 상기 범용 프로세서 상기 실행결과에 따라 상기 오프라인 실행 파일을 확 정하는 데에도 사용되고, 상기 실행결과가 미리 설정된 요구 사항을 충족하지 않으면 실행결과가 상기 미리 설정된 요구 사항을 충족할 때까지 다음과 같은 프로세스 중 적어도 하나의 최적화 방법을 수행하고, 상기 미리 설정된 요구 사항을 충족하 는 바이너리 명령에 따라 대응하는 오프라인 실행 파일을 생성하며, 상기 최적화 방법은, 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보를 조정하는 것, 상기 엔드 측의 인공지능 프로세서의 실행환경 파라미터를 조정하는 것, 또는 인공지능 학습 태스크를 최적화하는 것을 포함한다. B21. 상기 B20에 따른 상기 장치에서, 상기 인공지능 학습 태스크를 최적화하는 단계는 전용 프로그래밍 언어에 따라 인공지능 학습 태스크를 최적화하는 것을 포함한다. B22. 상기 B20 또는 B21에 따른 상기 장치에서, 상기 인공지능 학습 태스크를 최적화하는 단계는 인공지능 학습 라이브러리의 버전을 업데이트하는 것을 통해 인공지능 학습 태스크를 최적화하는 것을 더 포함한다. B23. 상기 B20－B22 중 어느 한 항에 따른 상기 장치에서, 상기 인공지능 학습 태스크를 최적화하는 단계는 인 공지능 알고리즘 모델을 조정하는 것을 더 포함한다. B24. 상기 B14－B23 중 어느 한 항에 따른 상기 장치에서, 상기 실행결과는 상기 인공지능 학습 태스크의 클라 우드 측의 인공지능 프로세서에서의 실행시간, 상기 인공지능 학습 태스크의 수행시 클라우드 측의 인공지능 처 리 시스템을 점용하는 로드 정보, 인공지능 학습 태스크를 수행한 결과가 예상되는 요구 사항에 해당하는지 여 부 중 적어도 하나를 포함한다. B25. 상기 B14에 따른 상기 장치에서, 상기 클라우드 측의 인공지능 프로세서는 인공지능 프로세서 하드웨어 실 체, 필드 프로그래머블 게이트 어레이, 시뮬레이터 중 적어도 하나를 포함한다. B26. 상기 B25에 따른 상기 장치에서, 상기 인공지능 프로세서 하드웨어 실체는 아키텍처의 재구성이 가능한 인 공지능 프로세서이다. C1. 범용 프로세서에 적용되는 데이터 처리방법에 있어서, 상기 범용 프로세서가 엔드 측의 인공지능 프로세서의 설비정보에 따라 바이너리 명령을 생성하고, 상기 바이너 리 명령에 따라 인공지능 학습 태스크를 생성하는 것; 상기 범용 프로세서가 상기 인공지능 학습 태스크를 클라우드 측의 인공지능 프로세서에 송신하여 실행시키는 것; 상기 범용 프로세서가 상기 인공지능 학습 태스크에 대응하는 실행결과를 수신하는 것; 및 상기 범용 프로세서가 상기 실행결과에 따라 오프라인 실행 파일을 확정하는 것을 포함하고, 상기 오프라인 실 행 파일은 실행결과가 미리 설정된 요구 사항을 충족하면 대응하는 상기 엔드 측의 인공지능 프로세서의 설비정 보와 바이너리 명령에 의해 생성된다. C2. 상기 C1에 따른 상기 방법에서, 상기 설비정보는 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 실행환경 파라미터를 포함한다. C3. 상기 C2에 따른 상기 방법에서, 상기 실행환경 파라미터는 상기 엔드 측의 인공지능 프로세서의 실행 메인 주파수, 오프 칩 메모리부터 엔드 측의 인공지능 프로세서 사이의 액세스 대역폭, 온칩 스토리지 크기, 엔드 측 의 인공지능 프로세서의 코어 수, 엔드 측의 인공지능 프로세서의 연산장치의 유형 중 적어도 하나를 포함한다. C4. 상기 C1에 따른 상기 방법에서, 상기 범용 프로세서가 상기 엔드 측의 인공지능 프로세서의 설비정보를 수 신하는 것을 더 포함한다. C5. 상기 C4에 따른 상기 방법에서, 상기 범용 프로세서가 설비정보를 드라이버에 써놓고, 상기 드라이버 중 설 비정보에 따라 클라우드 측의 인공지능 프로세서에 적응시키기는 것을 더 포함한다. C6. 상기 C1에 따른 상기 방법에서, 상기 실행결과에 따라 상기 오프라인 실행 파일을 확정하는 것은, 상기 실 행결과가 미리 설정된 요구 사항을 충족하면 상기 미리 설정된 요구 사항을 충족하는 바이너리 명령에 따라 대 응하는 오프라인 실행 파일을 생성하는 것을 포함한다. C7. 상기 C1에 따른 상기 방법에서, 상기 실행결과에 따라 상기 오프라인 실행 파일을 확정하는 것은, 상기 실행결과가 미리 설정된 요구 사항을 충족하지 않으면 실행결과가 상기 미리 설정된 요구 사항을 충족할 때까지 다음과 같은 프로세스 중 적어도 하나의 최적화 방법을 수행하고, 상기 미리 설정된 요구 사항을 충족하 는 바이너리 명령에 따라 대응하는 오프라인 실행 파일을 생성하는 것을 더 포함하며, 상기 최적화 방법은, 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보를 조정하는 것, 상기 엔드 측의 인공지능 프로세서의 실행환경 파라미터를 조정하는 것, 또는 인공지능 학습 태스크를 최적화하는 것을 포함한다. C8. 상기 C7에 따른 상기 방법에서, 상기 인공지능 학습 태스크를 최적화하는 단계는 전용 프로그래밍 언어에 따라 인공지능 학습 태스크를 최적화하는 것을 포함한다. C9. 상기 C7 또는 C8에 따른 상기 방법에서, 상기 인공지능 학습 태스크를 최적화하는 단계는 인공지능 학습 라 이브러리의 버전을 업데이트하는 것을 통해 인공지능 학습 태스크를 최적화하는 것을 더 포함한다. C10. 상기 C7－C9 중 어느 한 항에 따른 상기 방법에서, 상기 인공지능 학습 태스크를 최적화하는 단계는 인공 지능 알고리즘 모델을 조정하는 것을 더 포함한다. C11. 상기 C1－C10 중 어느 한 항에 따른 상기 방법에서, 상기 실행결과는 상기 인공지능 학습 태스크의 클라우 드 측의 인공지능 프로세서에서의 실행시간, 상기 인공지능 학습 태스크의 수행시 클라우드 측의 인공지능 처리 시스템을 점용하는 로드 정보, 인공지능 학습 태스크를 수행한 결과가 예상되는 요구 사항에 해당하는지 여부 중 적어도 하나를 포함한다. D12. 메모리 및 범용 프로세서를 포함한 데이터 처리장치에 있어서, 상기 메모리에는 상기 범용 프로세서에 의 해 실행 가능한 컴퓨터 프로그램이 저장되어 있고, 상기 범용 프로세서가 상기 컴퓨터 프로그램을 수행할 때 실 현하는 데이터 처리 흐름은, 엔드 측의 인공지능 프로세서의 설비정보에 따라 바이너리 명령을 생성하고, 상기 바이너리 명령에 따라 인공지 능 학습 태스크를 생성하는 것; 상기 인공지능 학습 태스크를 클라우드 측의 인공지능 프로세서에 송신하여 실행시키는 것; 상기 인공지능 학습 태스크에 대응하는 실행결과를 수신하는 것; 및 상기 실행결과에 따라 오프라인 실행 파일을 확정하는 것을 포함하며, 상기 오프라인 실행 파일은 실행결과가 미리 설정된 요구 사항을 충족하면 대응하는 상기 엔드 측의 인공지능 프로세서의 설비정보와 바이너리 명령에 의해 생성된다. D13. 상기 D12에 따른 상기 장치에서, 상기 설비정보는 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 실행환경 파라미터를 포함한다. D14. 상기 D13에 따른 상기 장치에서, 상기 실행환경 파라미터는 상기 엔드 측의 인공지능 프로세서의 실행 메 인 주파수, 오프 칩 메모리부터 엔드 측의 인공지능 프로세서 사이의 액세스 대역폭, 온칩 스토리지 크기, 엔드 측의 인공지능 프로세서의 코어 수, 엔드 측의 인공지능 프로세서 중 연산장치의 유형 중 적어도 하나를 포함한 다. D15. 상기 D12에 따른 상기 장치에서, 상기 범용 프로세서가 상기 컴퓨터 프로그램을 수행할 때 실현하는 데이 터 처리 흐름은 상기 엔드 측의 인공지능 프로세서의 설비정보를 수신하는 것을 더 포함한다. D16. 상기 D12 또는 D15에 따른 상기 장치에서, 상기 범용 프로세서가 상기 컴퓨터 프로그램을 수행할 때 실현 하는 데이터 처리 흐름은 설비정보를 드라이버에 써놓고, 상기 드라이버 중 설비정보에 따라 클라우드 측의 인 공지능 프로세서에 적응시키기는 것을 더 포함한다. D17. 상기 D12에 따른 상기 장치에서, 상기 범용 프로세서가 상기 실행결과에 따라 상기 오프라인 실행 파일을 확정하는 경우, 상기 컴퓨터 프로그램을 수행할 때 실현하는 데이터 처리 흐름은 상기 실행결과가 미리 설정된 요구 사항을 충족하면 상기 미리 설정된 요구 사항을 충족하는 바이너리 명령에 따라 대응하는 오프라인 실행 파일을 생성하는 것을 포함한다. D18. 상기 D12에 따른 상기 장치에서, 상기 범용 프로세서가 상기 실행결과에 따라 상기 오프라인 실행 파일을 확정하는 겨우, 상기 컴퓨터 프로그램을 수행하여 실현하는 데이터 처리 흐름은, 상기 실행결과가 미리 설정된 요구 사항을 충족하지 않으면 실행결과가 상기 미리 설정된 요구 사항을 충족할 때까지 다음과 같은 프로세스 중 적어도 하나의 최적화 방법을 수행하고, 상기 미리 설정된 요구 사항을 충족하 는 바이너리 명령에 따라 대응하는 오프라인 실행 파일을 생성하는 것을 더 포함하며, 상기 최적화 방법은, 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보를 조정하는 것, 상기 엔드 측의 인공지능 프로세서의 실행환경 파라미터를 조정하는 것, 또는 인공지능 학습 태스크를 최적화하는 것을 포함한다. D19. 상기 D18에 따른 상기 장치에서, 상기 범용 프로세서가 인공지능 학습 태스크를 최적화할 때 실현하는 데 이터 처리 흐름은 전용 프로그래밍 언어에 따라 인공지능 학습 태스크를 최적화하는 것을 더 포함한다. D20. 상기 D18 또는 D19에 따른 상기 장치에서, 상기 범용 프로세서가 인공지능 학습 태스크를 최적화할 때 실 현하는 데이터 처리 흐름은 인공지능 학습 라이브러리의 버전을 업데이트하는 것을 통해 인공지능 학습 태스크 를 최적화하는 것을 더 포함한다. D21. 상기 D18－D20 중 어느 한 항에 따른 상기 장치에서, 상기 범용 프로세서가 인공지능 학습 태스크를 최적 화할 때 실현하는 데이터 처리 흐름은 인공지능 알고리즘 모델을 조정하는 것을 더 포함한다. D22. 상기 D12－D21 중 어느 한 항에 따른 상기 장치에서, 상기 실행결과는 상기 인공지능 학습 태스크의 클라 우드 측의 인공지능 프로세서에서의 실행시간이 예상되는 요구 사항에 해당하는지 여부, 상기 인공지능 학습 태 스크의 수행시 클라우드 측의 인공지능 처리 시스템을 점용하는 로드 정보가 예상되는 요구 사항에 해당하는지 여부, 인공지능 학습 태스크를 수행한 결과가 예상되는 요구 사항에 해당하는지 여부 중 적어도 하나를 포함한 다. E23. 클라우드 측의 인공지능 프로세서에 적용되는 데이터 처리방법에 있어서, 엔드 측의 인공지능 프로세서의 설비정보에 의해 생성되는 바이너리 명령에 따라 확정되는 인공지능 학습 태스 크를 수신하는 것; 및 상기 인공지능 학습 태스크를 수행하여 실행결과를 생성하는 것을 포함한다. E24. 상기 E23에 따른 상기 방법에서, 상기 실행결과는 상기 인공지능 학습 태스크의 클라우드 측의 인공지능 프로세서에서의 실행시간이 예상되는 요구 사항에 해당하는지 여부, 상기 인공지능 학습 태스크의 수행시 클라 우드 측의 인공지능 처리 시스템을 점용하는 로드 정보가 예상되는 요구 사항에 해당하는지 여부, 인공지능 학 습 태스크를 수행한 결과가 예상되는 요구 사항에 해당하는지 여부 중 적어도 하나를 포함한다. E25. 상기 E23에 따른 상기 방법에서, 상기 클라우드 측의 인공지능 프로세서는 인공지능 프로세서 하드웨어 실 체, 필드 프로그래머블 게이트 어레이, 시뮬레이터 중 적어도 하나를 포함한다. E26. 상기 E25에 따른 상기 방법에서, 상기 클라우드 측의 인공지능 프로세서 하드웨어 실체는 아키텍처의 재구 성이 가능한 인공지능 프로세서이다. F27. 인공지능 프로세서에 있어서, 엔드 측의 인공지능 프로세서의 설비정보에 의해 생성되는 바이너리 명령에 따라 확정되는 인공지능 학습 태스 크를 수신하기 위한 수신모듈; 및 상기 인공지능 학습 태스크를 수행하여 실행결과를 생성하기 위한 수행모듈을 포함한다. F28. 상기 F27에 따른 상기 인공지능 프로세서에서, 상기 수행모듈생성의 실행결과는 상기 인공지능 학습 태스 크의 상기 클라우드 측의 인공지능 프로세서에서의 실행시간, 상기 인공지능 학습 태스크의 수행시 클라우드 측 의 인공지능 처리 시스템을 점용하는 로드 정보, 인공지능 학습 태스크를 수행한 결과가 예상되는 요구 사항에 해당하는지 여부 중 적어도 하나를 포함한다. F29. 상기 F27에 따른 상기 인공지능 프로세서에서, 상기 클라우드 측의 인공지능 프로세서는 인공지능 프로세 서 하드웨어 실체, 필드 프로그래머블 게이트 어레이, 시뮬레이터 중 적어도 하나를 포함한다. F30. 상기 F29에 따른 상기 인공지능 프로세서에서, 상기 클라우드 측의 인공지능 프로세서 하드웨어 실체는 아 키텍처의 재구성이 가능한 인공지능 프로세서이다. G31. 엔드 측의 인공지능 프로세서에 적용되는 데이터 처리방법에 있어서, 오프라인 실행 파일을 획득하는 것을 포함하며, 상기 오프라인 실행 파일은 실행결과가 미리 설정된 요구 사항 을 충족하면 대응하는 상기 엔드 측의 인공지능 프로세서의 설비정보와 바이너리 명령에 의해 생성된다. G32. 상기 G31에 따른 상기 방법에서, 요구정보를 송신하는 것을 더 포함하며, 상기 요구정보는 상기 엔드 측의 인공지능 프로세서의 설비정보를 포함한다. G33. 상기 G31에 따른 상기 방법에서, 상기 설비정보는 상기 엔드 측의 인공지능 프로세서의 하드웨어 아키텍처 정보와 실행환경 파라미터를 포함한다. G34. 상기 G33에 따른 상기 방법에서, 상기 실행환경 파라미터는 상기 엔드 측의 인공지능 프로세서의 실행 메 인 주파수, 오프 칩 메모리부터 엔드 측의 인공지능 프로세서 사이의 액세스 대역폭, 온칩 스토리지 크기, 엔드 측의 인공지능 프로세서의 코어 수, 엔드 측의 인공지능 프로세서 중 연산장치의 유형 중 적어도 하나를 포함한 다. H35. 인공지능 프로세서에 있어서, 오프라인 실행 파일을 획득하기 위한 획득모듈을 포함하며, 상기 오프라인 실행 파일은 실행결과가 미리 설정된 요구 사항을 충족하면 대응하는 상기 엔드 측의 인공지능 프로세서의 설비정보와 바이너리 명령에 의해 생성된 다. H36. 상기 H35에 따른 상기 인공지능 프로세서에서, 요구정보를 송신하기 위한 송신모듈을 더 포함하며, 상기 요구정보는 상기 엔드 측의 인공지능 프로세서의 설비정보를 포함한다. H37. 상기 H36에 따른 상기 인공지능 프로세서에서, 상기 설비정보는 상기 엔드 측의 인공지능 프로세서의 하드 웨어 아키텍처 정보와 실행환경 파라미터를 포함한다. H38. 상기 H37에 따른 상기 인공지능 프로세서에서, 상기 실행환경 파라미터는 상기 엔드 측의 인공지능 프로세 서의 실행 메인 주파수, 오프 칩 메모리부터 엔드 측의 인공지능 프로세서 사이의 액세스 대역폭, 온칩 스토리 지 크기, 엔드 측의 인공지능 프로세서의 코어 수, 엔드 측의 인공지능 프로세서 중 연산장치의 유형 중 적어도 하나를 포함한다."}
{"patent_id": "10-2019-7037882", "section": "도면", "subsection": "도면설명", "item": 1, "content": "이하, 본 발명의 실시예의 기술방안을 보다 명확하게 설명하기 위해 실시예의 도면에 대해 간단히 설명하지만, 다음에 설명되는 도면이 본 발명의 일부 실시예에 불과하며 본 발명에 대한 제한이 아닌 것은 분명할 것이다. 도1은 본 기술방안에 따른 아키텍처의 모식도이고, 도2는 인공지능 프로세서의 소프트웨어 스택의 구조모식도이고, 도3은 본 기술방안에 따른 애플리케이션 시나리오의 제1모식도이고, 도4는 본 기술방안에 따른 애플리케이션 시나리오의 제2모식도이고, 도5는 본 출원이 제안하는 데이터 처리방법의 제1흐름도이고, 도6은 인공지능 학습 라이브러리에 의해 지원되는 다양한 유형의 기본 연산자의 모식도이고, 도7은 본 출원이 제안하는 데이터 처리방법의 제2흐름도이고, 도8은 본 출원이 제안하는 데이터 처리방법의 제3흐름도이고, 도9는 본 출원이 제안하는 데이터 처리방법의 제4흐름도이고, 도10은 본 출원이 제안하는 데이터 처리장치의 기능 블록도이고, 도11은 본 출원이 제안하는 인공지능 프로세서의 제1기능 블록도이고, 도12는 본 출원이 제안하는 인공지능 프로세서의 제2기능 블록도이고, 도13은 본 출원이 제안하는 데이터 처리 시스템의 구조모식도이다."}
