{"patent_id": "10-2021-0185933", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0096450", "출원번호": "10-2021-0185933", "발명의 명칭": "캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 방법 및", "출원인": "한양대학교 산학협력단", "발명자": "장준혁"}}
{"patent_id": "10-2021-0185933", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 제1텍스트 정보를 출력 정보로 하는 제1인공신경망 모듈을 포함하는 음향 모델(Acoustic Model);상기 제1텍스트 정보를 입력 정보로 하고, 언어 모델의 특징에 기초하여 상기 제1텍스트 정보에 대응 되는 제2텍스트 정보를 출력 정보로 출력하는 제2인공신경망 모듈을 포함하는 언어 모델(Language Model); 및상기 음향 모델이 출력하는 상기 음향 모델의 제1확률 분포 정보 및 상기 언어 모델이 출력하는 상기 언어 모델의 제2확률 분포 정보를 기초로 결 확률 분포를 생성하고, 상기 결합 확률 분포를 기초로 E2E(End-to-End) 음성모델을 생성하는 E2E 음성 모델 생성부;를 포함하고,상기 E2E 음성 모델 생성부는,상기 음향 모델 및 상기 언어 모델에 대해 각각 캘리브레이션을 진행한 후, 보정된 상기 음향 모델 및 상기 언어 모델에 기초하여 상기 E2E 음성 모델을 생성하는 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 장치."}
{"patent_id": "10-2021-0185933", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 E2E 음성 모델 생성부는,상기 제1확률 분포 정보에 대해 캘리브레이션을 수행하여 제1보정 파라미터를 생성한 후, 상기 제1보정 파라미터를 기초로 상기 결합 확률 분포를 생성하는,캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 장치."}
{"patent_id": "10-2021-0185933", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 E2E 음성 모델 생성부는,상기 제2확률 분포 정보에 대해 캘리브레이션을 수행하여 제2보정 파라미터)를 생성한 후, 상기 제2보정 파라미터를 기초로 상기 결합 확률 분포를 생성하는,캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 장치."}
{"patent_id": "10-2021-0185933", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 2항에 있어서,상기 E2E 음성 모델 생성부는,상기 제1확률 분포 정보에 대응되는 레퍼런스 데이터인 제1밸리데이션 세트(validation set)를 기초로 상기 제1확률 분포 정보에 대해 캘리브레이션을 진행하여 상기 제1보정 파라미터를 생성하는,캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 장치."}
{"patent_id": "10-2021-0185933", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 3항에 있어서,상기 E2E 음성 모델 생성부는,상기 제2확률 분포 정보에 대응되는 레퍼런스 데이터인 제2밸리데이션 세트(validation set)를 기초로 상기 제2공개특허 10-2023-0096450-3-확률 분포 정보에 대해 캘리브레이션을 진행하여 상기 제2보정 파라미터를 생성하는,캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 장치."}
{"patent_id": "10-2021-0185933", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 E2E 음성 모델 생성부는,상기 캘리브레이션을 진행하기 전에, 상기 음향 모델에 대해서는 상기 제1텍스트 정보에 대응되는 제1학습 세트(train set)를 기초로 상기 음향 모델의 손실 함수를 최소화하는 방향으로 학습을 수행하는, 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 장치."}
{"patent_id": "10-2021-0185933", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 E2E 음성 모델 생성부는,상기 캘리브레이션을 진행하기 전에, 상기 음향 모델에 대해서는 상기 제2텍스트 정보에 대응되는 제2학습 세트(train set)를 기초로 상기 음향 모델의 손실 함수를 최소화하는 방향으로 학습을 수행하는, 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 장치."}
{"patent_id": "10-2021-0185933", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 제1텍스트 정보를 출력 정보로 하는 제1인공신경망 모듈을 포함하는 음향 모델(Acoustic Model)을 이용하여 상기 음향 모델의 제1확률 분포 정보를 출력하는단계;상기 제1텍스트 정보를 입력 정보로 하고, 언어 모델의 특징에 기초하여 상기 제1텍스트 정보에 대응 되는 제2텍스트 정보를 출력 정보로 출력하는 제2인공신경망 모듈을 포함하는 언어 모델(Language Model)를 이용하여 상기 언어 모델의 제2확률 분포 정보를 출력하는 단계;상기 제1확률 분포 정보 및 상기 제2확률 분포 정보를 기초로 결합 확률 분포를 생성하고, 상기 결합 확률 분포를 기초로 E2E(End-to-End) 모델을 생성하는 E2E 모델 음성 생성 단계를 포함하고,상기 E2E 음성 모델 생성 단계는, 상기 음향 모델 및 상기 언어 모델에 대해 각각 캘리브레이션을 진행한 후, 보정된 상기 음향 모델 및 상기 언어 모델에 기초하여 상기 E2E 음성 모델을 생성하는 단계를 포함하는, 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 방법."}
{"patent_id": "10-2021-0185933", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8항에 있어서,상기 E2E 모델 생성 단계는, 상기 제1확률 분포 정보에 대해 캘리브레이션을 수행하여 제1보정 파라미터(scaling parameter)를 생성한 후,상기 제1보정 파라미터를 기초로 상기 결합 확률 분포를 생성하는 단계; 및상기 제2확률 분포 정보에 대해 캘리브레이션을 수행하여 제2보정 파라미터(scaling parameter)를 생성한 후,상기 제2보정 파라미터를 기초로 상기 결합 확률 분포를 생성하는 단계;를 포함하는, 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 방법."}
{"patent_id": "10-2021-0185933", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9항에 있어서,상기 E2E 모델 생성 단계는, 공개특허 10-2023-0096450-4-상기 제1확률 분포 정보에 대응되는 레퍼런스 데이터인 제1밸리데이션 세트(validation set)를 기초로 상기 제1확률 분포 정보에 대해 캘리브레이션을 진행하여 상기 제1보정 파라미터를 생성하는 단계; 및상기 제2확률 분포 정보에 대응되는 레퍼런스 데이터인 제2밸리데이션 세트(validation set)를 기초로 상기 제2확률 분포 정보에 대해 캘리브레이션을 진행하여 상기 제2보정 파라미터를 생성하는 단계;를 포함하는,캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 방법."}
{"patent_id": "10-2021-0185933", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "일 실시예에 따른 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 장치는, 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 제1텍스트 정보를 출력 정보로 하는 제1인공신 경망 모듈을 포함하는 음향 모델(Acoustic Model), 상기 제1텍스트 정보를 입력 정보로 하고, 언어 모델의 특징 (뒷면에 계속)"}
{"patent_id": "10-2021-0185933", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 장치에 관한 발 명으로서, 보다 구체적으로는 음향 모델과 언어 모델을 포함하는 E2E 음성 모델을 합성하여 생성함에 있어서, 음향 모델과 언어 모델의 특징을 반영하여 보다 신뢰도와 정확도가 높은 E2E 음성 모델을 생성하는 기술에 관한 발명이다."}
{"patent_id": "10-2021-0185933", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "음성은 인간의 가장 자연스러운 의사 소통 수단이면서 정보 전달 수단이자, 언어를 구현하는 수단으로서 인간이 내는 의미 있는 소리이다. 기술이 발전함에 따라 인간과 기계 사이의 음성을 통한 통신 구현에 대한 연구가 계속 이루어지고 있다. 더욱이 최근 음성 정보를 효과적으로 처리하기 위한 음성 정보 처리 기술(speech information technology;SIT) 분야가 괄목할 만한 발전을 이룩함에 따라 실생활에서도 적용이 되고 있다. 이러한 음성 정보 처리 기술을 크게 분류하면, 음성 인식(speech recognition), 음성 합성(speech synthesis), 화자 인증(speaker identification and verification), 음성 코딩(speech coding) 등의 카테고리로 분류될 수 있다. 음성 인식은 발화된 음성을 인식하여 문자열로 변환하는 기술이고, 음성 합성은 문자열을 음성 분석에서 얻어진 데이 터나 파라미터를 이용하여 원래의 음성으로 변환하는 기술이며, 화자 인증은 발화된 음성을 통하여 발화자 를 추정하 거나 인증하는 기술이며 음성 코딩은 음성 신호를 효과적으로 압축하여 부호화하는 기술이며, 음성합 성 기술은 실제 응용방식에 따라 크게 두 가지로 구분될 수 있다. 제한된 어휘 개수와 구문구조의 문장만을 합 성하는 제한 어휘합성 또는 자동음성응답시스템(ARS; Automatic Response System)과 임의의 문장을 입력받아 음 성 합성하는 무제한 어휘합성 또는 텍스트-음성 변환(TTS; Text-to-Speech) 시스템이 있다. 그 중, 텍스트-음성 변환(TTS) 시스템은 작은 합성 단위음성과 언어 처리를 이용하여 임의의 문장에 대한 음성 을 생성한다. 언어 처리를 이용하여 입력된 문장을 적당한 합성 단위의 조합으로 대응시키고, 문장으로부터 적 당한 억양과 지속시간을 추출하여 합성음의 운율을 결정한다. 언어의 기본 단위인 음소, 음절 등의 조합에 의해 음성을 합성해 내므로 합성 대상어휘에 제한이 없으며 주로 TTS(Text-to-Speech) 장치 및 CTS(Context-to- Speech) 장치 등에 적용된다. 특히, 인공 지능 기술이 발달함에 따라 인공 신경망 기반 알고리즘은 기존 모델 대비 큰 성능 향상을 보여주고 있다. 일반적으로 인공 신경망을 이용한 음성 합성 모델은 음향 모델 부분을 인공 신경망으로 대신하여, 인공 신경망이 분석된 문장 데이터를 기반으로 음성 파라미터를 합성할 수 있다. 여기서, 하나의 모듈로 음향 모델, 언어 모델, 발음 사전 등 음성 인식 전체 과정을 사람이 개입하지 않고 모든 것을 딥러닝을 통해 학습이 이루어지는 것을 End-to-End(E2E) 음성 인식 기술이라 불린다. 최근까지 대부분의 음성인식 시스템은 음향 모델, 언어 모델, 발음 사전 등으로 이루어진 독립된 모듈 구조를 사용하고 있다. 모듈 구조에서는 각각의 역할이 설명 가능하고 직관적이고 오류를 수정하기 용이하다는 장점이 있기 때문에, 이 장점을 바탕으로 많은 연구가 진행되었고, 딥러닝 기술이 발달한 이후에는 각 모듈에 딥러닝을 적용하는 연구가 진행 중이다. 오랜 연구를 통해 이러한 구조에 따른 성능은 많이 향상되었지만, 이러한 독립된 구조는 각 모듈이 분리되어 각 자의 손실 값으로 학습되므로 상호 보완적인 기능을 기대할 수 없는 단점이 존재하였다. 또한 각 모듈의 오류가 다음 모듈로 전파되고, 발음 사전과 같이 인간의 지식이 개입되는 모듈에서 명확히 입력하기 어려운 정보들이인식 성능 향상을 가로막는 등의 문제가 존재한다. 따라서, End-to-End(E2E) 음성인식은 하나의 모듈로 음성인식 전체 과정을 처리하는 기술로, 앞서 설명한 문제 를 해결하기 위한 방법으로서 연구가 진행되고 있으나, 아직까지는 각 모듈과의 호환성을 기초로 하여 E2E 전체 모델을 학습시키는 방법 및 E2E 모델의 출력 결과의 신뢰도를 향상시켜주는 기술은 존재하지 않은 실정이다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제10-1871604호 (2018.06.25. 공개) - '심화 신경망을 이용한 다채널 마이크 기 반의 잔향시간 추정 방법 및 장치' (특허문헌 0002) 한국등록특허 제10-1988504호 (2019.06.05.) - '딥러닝에 의해 생성된 가상환경을 이용한 강화 학습 방법'"}
{"patent_id": "10-2021-0185933", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "따라서, 일 실시예에 따른 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 방법 및 장치는 앞서 설명한 문제점을 해결하기 위해 고안된 발명으로서, 음향 모델과 언어 모델의 특징을 각각 정확히 반영한 E2E 음성 모델을 생성하여, 보다 신뢰도와 정확도가 높은 E2E 음성 모델을 생성하는 기술에 관한 발명이다. 보다 구체적으로는 음향 모델과 언어 모델의 보정 파라미터를 이용하여 E2E 음성 모델을 생성함에 있어서, 음향 모델과 언어 모델에 대해 캘리브레이션 보정을 진행한 후에 생성된 보정 파라미터를 이용하여 E2E 음성 모델을 생성함으로써, E2E 음성 모델이 출력하는 데이터의 신뢰도와 정확도를 높이는데 그 목적이 존재한다."}
{"patent_id": "10-2021-0185933", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 장치는, 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 제1텍스트 정보를 출력 정보로 하는 제1인공신 경망 모듈을 포함하는 음향 모델(Acoustic Model), 상기 제1텍스트 정보를 입력 정보로 하고, 언어 모델의 특징 에 기초하여 상기 제1텍스트 정보에 대응 되는 제2텍스트 정보를 출력 정보로 출력하는 제2인공신경망 모듈을 포함하는 언어 모델(Language Model) 및 상기 음향 모델이 출력하는 상기 음향 모델의 제1확률 분포 정보 및 상 기 언어 모델이 출력하는 상기 언어 모델의 제2확률 분포를 기초로 결합 확률 분포를 생성하고, 상기 결합 확률 분포를 기초로 E2E(End-to-End) 음성 모델을 생성하는 E2E 음성 모델 생성부를 포함하고, 상기 E2E 음성 모델 생성부는, 상기 음향 모델 및 상기 언어 모델에 대해 각각 캘리브레이션을 진행한 후, 보정된 상기 음향 모델 및 상기 언어 모델에 기초하여 상기 E2E 음성 모델을 생성할 수 있다. 상기 E2E 음성 모델 생성부는, 상기 제1확률 분포 정보에 대해 캘리브레이션을 수행하여 제1보정 파라미터를 생 성한 후, 상기 제1보정 파라미터를 기초로 상기 결합 확률 분포를 생성할 수 있다. 상기 E2E 음성 모델 생성부는, 상기 제2확률 분포 정보에 대해 캘리브레이션을 수행하여 제2보정 파라미터를 생 성한 후, 상기 제2보정 파라미터를 기초로 상기 결합 확률 분포를 생성할 수 있다. 상기 E2E 음성 모델 생성부는, 상기 제1확률 분포 정보에 대응되는 레퍼런스 데이터인 제1밸리데이션 세트 (validation set)를 기초로 상기 제1확률 분포 정보에 대해 캘리브레이션을 진행하여 상기 제1보정 파라미터를 생성할 수 있다. 상기 E2E 음성 모델 생성부는, 상기 제2확률 분포 정보에 대응되는 레퍼런스 데이터인 제2밸리데이션 세트 (validation set)를 기초로 상기 제2확률 분포 정보에 대해 캘리브레이션을 진행하여 상기 제2보정 파라미터를 생성할 수 있다. 상기 E2E 음성 모델 생성부는, 상기 캘리브레이션을 진행하기 전에, 상기 음향 모델에 대해서는 상기 제1텍스트 정보에 대응되는 제1학습 세트(train set)를 기초로 상기 음향 모델의 손실 함수를 최소화하는 방향으로 학습을수행할 수 있다. 상기 E2E 음성 모델 생성부는, 상기 캘리브레이션을 진행하기 전에, 상기 음향 모델에 대해서는 상기 제2텍스트 정보에 대응되는 제2학습 세트(train set)를 기초로 상기 음향 모델의 손실 함수를 최소화하는 방향으로 학습을 수행할 수 있다. 일 실시예에 따른 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 방법은, 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 제1텍스트 정보를 출력 정보로 하는 제1인공신 경망 모듈을 포함하는 음향 모델(Acoustic Model)을 이용하여 상기 음향 모델의 제1확률 분포 정보를 출력하는 단계, 상기 제1텍스트 정보를 입력 정보로 하고, 언어 모델의 특징에 기초하여 상기 제1텍스트 정보에 대응 되 는 제2텍스트 정보를 출력 정보로 출력하는 제2인공신경망 모듈을 포함하는 언어 모델(Language Model)를 이용 하여 상기 언어 모델의 제2확률 분포 정보를 출력하는 단계, 및 상기 제1확률 분포 정보 및 상기 제2확률 분포 정보를 기초로 결합 확률 분포를 생성하고, 상기 결합 확률 분포를 기초로 E2E(End-to-End) 모델을 생성하는 E2E 모델 생성 단계를 포함하고, 상기 E2E 모델 생성 단계는, 상기 음향 모델 및 상기 언어 모델에 대해 각각 캘리브레이션을 진행한 후, 보정된 상기 음향 모델 및 상기 언어 모델에 기초하여 상기 E2E 음성 모델을 생성하 는 단계를 포함할 수 있다. 상기 E2E 모델 생성 단계는, 상기 제1확률 분포 정보에 대해 캘리브레이션을 수행하여 제1보정 파라미터 (scaling parameter)를 생성한 후, 상기 제1보정 파라미터를 기초로 상기 결합 확률 분포를 생성하는 단계 및 상기 제2확률 분포 정보에 대해 캘리브레이션을 수행하여 제2보정 파라미터(scaling parameter)를 생성한 후, 상기 제2보정 파라미터를 기초로 상기 결합 확률 분포를 생성하는 단계를 포함할 수 있다. 상기 E2E 모델 생성 단계는, 상기 제1확률 분포 정보에 대응되는 레퍼런스 데이터인 제1밸리데이션 세트 (validation set)를 기초로 상기 제1확률 분포 정보에 대해 캘리브레이션을 진행하여 상기 제1보정 파라미터를 생성하는 단계 및 상기 제2확률 분포 정보에 대응되는 레퍼런스 데이터인 제2밸리데이션 세트(validation set) 를 기초로 상기 제2확률 분포 정보에 대해 캘리브레이션을 진행하여 상기 제2보정 파라미터를 생성하는 단계를 포함할 수 있다."}
{"patent_id": "10-2021-0185933", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "일 실시예에 따른 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 방법 및 장치는 음향 모델과 언어 모델을 결합하여 E2E 음성 인식 모델을 생성함에 있어서, 보정 파라미터를 경험적으로 찾아내는 종래 기술과 달리 ECE(Expected calibration error)를 최소화하는 방향으로 파라미터를 학습하고 도출 해내므로, 종래 기술과 달리 모델 결합을 보정 파라미터를 탐색을 위한 시간 및 비용을 효과적으로 감소시킬 수 있는 효과가 존재한다. 또한, 종래 기술의 경우 경험적으로 보정 파라미터를 생성하였기 때문에 보정 파라미터가 최적의 파라미터인지 보장을 하기가 어려웠으나, 본 발명의 경우 ECE를 최소화 시키는 방향으로 학습을 하고 이를 기초로 보정 파라 미터를 생성하므로 최적의 음성 인식 성능을 보장할 수 있는 보정 파라미터를 생성할 수 있는 장점이 존재한다. 또한, 켈리브레이션 보정을 수행한 후 이를 기초로 E2E 음성 인식 모델을 생성하므로, 생성된 E2E 음성 인식 모 델에서 출력되는 신뢰도 정보는 실제 확률에 가까워지는 효과가 존재한다."}
{"patent_id": "10-2021-0185933", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명에 따른 실시 예들은 첨부된 도면들을 참조하여 설명한다. 각 도면의 구성요소들에 참조 부호를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 부호를 가 지도록 하고 있음에 유의해야 한다. 또한, 본 발명의 실시 예를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 발명의 실시예에 대한 이해를 방해한다고 판단되는 경우에는 그 상세한 설명은 생략한 다. 또한, 이하에서 본 발명의 실시 예들을 설명할 것이나, 본 발명의 기술적 사상은 이에 한정되거나 제한되지 않고 당업자에 의해 변형되어 다양하게 실시될 수 있다. 또한, 본 명세서에서 사용한 용어는 실시 예를 설명하기 위해 사용된 것으로, 개시된 발명을 제한 및/또는 한정 하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\", \"구비하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들 이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않 는다. 또한, 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"간접적으로 연결\"되어 있는 경우도 포함하며, 본 명세서에 서 사용한 \"제 1\", \"제 2\" 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지 만, 상기 구성 요소들은 상기 용어들에 의해 한정되지는 않는다. 아래에서는 첨부한 도면을 참고하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략한다. 한편 본 명세서에 따른 발명의 명칭은 '캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인 식 모델 생성 방법 및 장치'로 기재하였으나, 이하 설명의 편의를 위해 '캘리브레이션 보정을 이용하여 E2E 음 성 인식 모델을 생성하는 음성 인식 모델 생성 방법 및 장치'는 '음성 인식 모델 생성 방법 또는 음성 인식 모 델 생성 장치'로 축약하여 설명하도록 한다. 도 1은 일 실시예에 따른 음성 인식 모델 생성 시스템의 일부 구성 요소를 도시한 블럭도이다. 도 1에 도시된 바와 같이, 인공 신경망을 이용한 음성 인식 모델 생성 시스템은, 음성 인식 모델 생성 장치 , 사용자 단말기 및 서버를 포함할 수 있으며, 각각의 구성요소는 네트워크에 의해 서로 통신 연결되어 있을 수 있다. 일 예로, 음성 인식 모델 생성 장치, 사용자 단말기 및 서버는 5G 통신 환경에서 서로 연결될 수 있으며, 도 1에 도시된 기기들 이외에 가정 또는 사무실에서 사용되는 다양한 전자 기기들이 사물 인터넷 환 경 하에서 서로 연결되어 동작할 수 있다. 음성 인식 모델 생성 장치는 입력되는 음성 데이터에 대해 대응되는 텍스트 정보 또는 음성 정보를 출력하 는 장치로서, 텍스트 정보를 출력하는 장치일 뿐 만 아니라, 각종 인공지능 알고리즘을 수행하는데 필요한 장치 들이 마련되어 있으며, 인공 신경망을 동작시키는데 필요한 데이터가 저장되어 있을 수 있다. 음성 인식 모델 생성 장치는 인공 신경망 모듈에 의한 학습과 추론 및 입력된 음성 데이터에 대응되는 텍 스트 정보 또는 음성 정보를 출력할 수 있는 장치로, 서버(Server), PC, 태블릿 PC, 스마트 폰(smart phobne), 스마트와치(smart watch), 스마트 글라스(smart glass), 웨어러블 기기(wearable device) 등과 같은 장치로 구 현될 수 있으며, 특정 어플리케이션이나 프로그램으로 구현될 수 도 있다. 사용자 단말기는 음성 모델 생성 어플리케이션 또는 음성 모델 생성 사이트에 접속한 후 인증 과정을 통하 여 음성 인식 모델 생성 장치의 상태 정보를 모니터링 할 수 있으며, 더 나아가 음성 인식 모델 생성 장치 를 구동하거나 또는 제어할 수 있는 서비스를 제공받을 수 있다. 본 실시 예에서 사용자 단말기는 사용자가 조작하는 데스크 탑 컴퓨터, 스마트폰, 노트북, 태블릿 PC, 스 마트 TV, 휴대폰, PDA(personal digital assistant), 랩톱, 미디어 플레이어, 마이크로 서버, GPS(globalpositioning system) 장치, 전자책 단말기, 디지털방송용 단말기, 네비게이션, 키오스크, MP3 플레이어, 디지털 카메라, 가전기기 및 기타 모바일 또는 비모바일 컴퓨팅 장치일 수 있으나, 이에 제한되지 않는다. 또한, 사용자 단말기는 통신 기능 및 데이터 프로세싱 기능을 구비한 시계, 안경, 헤어 밴드 및 반지 등의 웨어러블 단말기 일 수 있다. 사용자 단말기는 상술한 내용에 제한되지 아니하며, 웹 브라우징이 가능한 단말기는 제한 없이 차용될 수 있다. 서버는 각종 인공지능 알고리즘을 적용하는데 필요한 빅데이터 및 음성 인식 모델 생성 장치를 동작 시키는 데이터를 제공하는 데이터베이스 서버일 수 있다. 그 밖에 서버는 사용자 단말기에 설치된 음 성 합성 어플리케이션 또는 음성 합성 웹 브라우저를 이용하여 음성 인식 모델 생성 장치의 동작을 원격에 서 제어할 수 있도록 하는 웹 서버 또는 애플리케이션 서버를 포함할 수 있다. 여기서 인공 지능(artificial intelligence, AI)은, 인간의 지능으로 할 수 있는 사고, 학습, 자기계발 등을 컴 퓨터가 할 수 있도록 하는 방법을 연구하는 컴퓨터 공학 및 정보기술의 한 분야로, 컴퓨터가 인간의 지능적인 행동을 모방할 수 있도록 하는 것을 의미할 수 있다. 또한, 인공지능은 그 자체로 존재하는 것이 아니라, 컴퓨터 과학의 다른 분야와 직간접으로 많은 관련을 맺고 있다. 특히 현대에는 정보기술의 여러 분야에서 인공지능적 요소를 도입하여, 그 분야의 문제 풀이에 활용하려 는 시도가 매우 활발하게 이루어지고 있다. 머신 러닝(machine learning)은 인공지능의 한 분야로, 컴퓨터에 명시적인 프로그램 없이 배울 수 있는 능력을 부여하는 연구 분야를 포함할 수 있다. 구체적으로 머신 러닝은, 경험적 데이터를 기반으로 학습을 하고 예측을 수행하고 스스로의 성능을 향상시키는 시스템과 이를 위한 알고리즘을 연구하고 구축하는 기술이라 할 수 있다. 머신 러닝의 알고리즘들은 엄격하게 정해진 정적인 프로그램 명령들을 수행하는 것이라기보다, 입력 데이터를 기반으로 예측이나 결정을 이끌어내기 위해 특정한 모델을 구축하는 방식을 취할 수도 있다. 서버는 음성 인식 모델 생성 장치 및/또는 사용자 단말기와 신호를 송수신할 수 있다. 서버는 사용자 단말기로부터 수신한 음성 데이터를 수신한 후, 수신한 정보를 음성 인식 모델 생성 장치로 전송할 수 있다. 네트워크는 인공 신경망을 이용한 다화자 음성 인식 장치와, 사용자 단말기와, 서버를 연 결하는 역할을 수행할 수 있다. 이러한 네트워크는 예컨대 LANs(local area networks), WANs(wide area networks), MANs(metropolitan area networks), ISDNs(integrated service digital networks) 등의 유선 네트 워크나, 무선 LANs, CDMA, 블루투스, 위성 통신 등의 무선 네트워크를 망라할 수 있으나, 본 발명의 범위가 이 에 한정되는 것은 아니다. 또한 네트워크는 근거리 통신 및/또는 원거리 통신을 이용하여 정보를 송수신할 수 있다. 여기서 근거리 통신은 블루투스(Bluetooth), RFID(radio frequency identification), 적외선 통신 (IrDA, infrared data association), UWB(ultra-wideband), ZigBee, Wi-Fi (wireless fidelity) 기술을 포함 할 수 있고, 원거리 통신은 CDMA(code division multiple access), FDMA(frequency division multiple access), TDMA(time division multiple access), OFDMA(orthogonal frequency division multiple access), SC-FDMA(single carrier frequency division multiple access) 기술을 포함할 수 있다. 네트워크는 허브, 브리지, 라우터, 스위치 및 게이트웨이와 같은 네트워크 요소들의 연결을 포함할 수 있 다. 네트워크는 인터넷과 같은 공용 네트워크 및 안전한 기업 사설 네트워크와 같은 사설 네트워크를 비롯 한 하나 이상의 연결된 네트워크들, 예컨대 다중 네트워크 환경을 포함할 수 있다. 네트워크에의 액세스는 하나 이상의 유선 또는 무선 액세스 네트워크들을 통해 제공될 수 있다. 더 나아가 네트워크는 사물 등 분 산된 구성 요소들 간에 정보를 주고 받아 처리하는 IoT(Internet of Things, 사물인터넷) 망 및/또는 5G 통신을 지원할 수 있다. 지금까지 본 발명에 따른 음성 인식 모델 생성 시스템에 대해 알아보았다. 이하 종래 기술에 따른 음성 인 식 모델 생성 원리 및 이에 따른 종래 기술의 문제점에 대해 알아본 후, 본 발명에 따른 음성 인식 모델 생성 장치의 구성 및 효과에 대해 자세히 알아보도록 한다. 본 명세서에서 음성 인식이란 주어진speech (X)에 대응되는 가장 적합한 단어열(W)을 찾는 것을 목적으로 하며, 이를 확률적으로 모델링 하는 경우 아래와 같은 수학식로 표현을 할 수 가 있다.(수학식 1) P(W|X) ~ P(X|W)P(W) 확률적 음성인식 모델은 주어진 speech (X)에서 단어열(W)이 출현할 확률을 추정하며, 디코딩 과정을 통해 이중 가장 높은 확률값을 갖는 단어열을 탐색할 수 있으며, 베이즈룰을 통해 사후확률(P(W|X))을 사전확률(P(W))과 조건부 확률(P(X|W))의 곱으로 표현할 수 있다. 이렇게 음성인식 시스템을 사전확률부와 조건부 확률부로 구분하는 경우 얻는 장점은 unpaired 텍스트 데이터를 이용해 독립적으로 학습된 언어 모델의 음성 인식 성능을 증진시킬 수 있다는 점이다. 따라서, 많은 음성인식 시스템이 P(X|W)와 P(W)를 독립적으로 학습하며, 학습된 두 모델의 결합 확률 분포를 탐색하여 최종적으로 가장 높은 확률을 갖는 단어열을 출력한다. 딥러닝에 기반한 인공신경망은 높은 분류 정확도(P(X|W))를 보이지만, 신뢰도에 너무 중점을 준 나머지, 근래에 는 정교한 딥러닝 네트워크가 학습 데이터셋에 오버피팅(overfitting) 되는 경우가 많이 발생하고 있으며, 이러 한 문제는 음성 인식 분야에서도 마찬가지고 발생하고 있다. 즉, 인공신경망 모듈에 오버피티 문제가 발생하는 경우 독립적으로 학습된 확률 모델들의 결합 확률 분포를 이용하는 음성인식 시스템의 빔서치 디코딩 과정에 악 영향을 미칠 수 있으며, 이는 도 2의 도면에 따른 실험 결과를 통해서도 알 수 있다. 도 2는 종래 기술에 따라 E2E모델을 생성하는 경우 발생하는 문제점을 설명하기 위한 도면으로, 도 2의 (a)는 종래 기술에 따른 음향 모델에서 출력되는 출력 정보에 대한 신뢰도(confidenc)와 정확도(accuracy)에 대한 그 래프를 도시한 도면이고, 도 2의 (b)는 종래 기술에 따른 CTC(Connectionist Temporal Classification) 음향 모델에서 출력되는 출력 정보에 대한 신뢰도(confidenc)와 정확도(accuracy)에 대한 그래프를 도시한 도면이고, 도 2의 (c)는 종래 기술에 따른 언어 모델에서 출력되는 출력 정보에 대한 신뢰도(confidence)와 정확도 (accuracy)에 대한 그래프를 도시한 도면이며, 도 2에서 신뢰도는 인공신경망에서 출력되는 값들 중 가장 신뢰 도가 높은 값에 대한 출력 정보이며, 도 2에서의 정확도는 각각의 모델이 출력하는 출력 정보가 얼마나 실제 레 퍼런스 데이터가 되는 정보와 비교하였을 때, 얼마만큼의 정확도를 가지고 있는지에 대한 확률값을 의미한다. 도 2의 3가지 모델들을 모두 참고해보면, 신뢰도 그래프(파란색) 선과 정확도 그래프 선이 일치를 하는 경우가 가장 이상적인 그래프로서, 이러한 경우에는 각각의 모델에 대한 출력 정보가 실제 모델과 가장 유사한 정보를 출력하고 있다고 볼 수 있다. 그러나 도 2에 도시된 바와 같이 종래 기술에 따를 경우 각각의 모델에서 출력하 는 신뢰도 정보에 대한 정확도가 실제 정확도와 일치하지 않음을 알 수 있으며, 특히 음향 모델에서 그러한 편 차가 더 높음을 알 수 있다. 이러한 문제가 발생하는 이유는 E2E 모델은 음향 모델과 음성 모델이 연계되어 최종 정보를 출력하므로, 양 모 델 사이의 호환성이 매우 중요한데, 앞서 설명한 바와 같이 종래 기술과 같이 각각의 모델에 대해 학습을 독립 적으로 진행을 하다 보면 각각의 모델 자체에만 최적화된 결과를 출력하기 때문에, 이들을 합성해서 생성한 E2E 모델은 출력 결과가 그 정확성을 장담할 수 없는 문제가 존재한다. 만약, 도 2와 같은 출력 결과를 보여주는 모 델들을 기초로 음향 모델과 언어 모델을 합성하여 E2E 음성 모델을 합성하는 경우 신뢰도에 대한 정확도가 떨어 지는 음향 모델을 이용하여 합성을 하기 때문에, 합성된 E2E 음성 모델은 실제 E2E 음성 모델과 많은 차이를 보 이는 결과를 출력할 수 있다. 이를 자세히 살펴보면, 독립적으로 학습된 음향 모델과 언어 모델을 이용하는 음성 인식 시스템은 음향모델과 언어모델의 결합 확률 분포를 이용해 단어열을 추정하는데, 이 때 두 모델이 출력하는 엔트로피(entropy)가 극 단적으로 차이가 나는 경우 최종적인 확률 분포는 두 모델의 실제 성능과 무관하게 낮은 엔트로피(entropy) 출 력을 가지는 모델 분포를 따르게 된다. 결과적으로 이는 단순히 개별 모델의 정확도(accuracy)를 증진시켰다고 하여 합성된 모델이 최종적으로 높은 인식 성능을 보장하지 못하는 문제를 발생시킬 수 있다. 따라서, 일 실시예에 따른 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 방법 및 장치는 앞서 설명한 문제점을 해결하기 위해 고안된 발명으로서, 음향 모델과 언어 모델의 특징을 각각 반영하여 보다 신뢰도와 정확도가 높은 E2E 음성 모델을 생성하는데 그 목적이 존재한다. 구체적으로, 본 발명에 따른 E2E 음성 인식 모델은 독립적으로 학습되는 음향 모델과 언어 모델의 ECE(Expected calibration error)에 근거하여 각각의 모델이 출력하는 엔트로피를 조절하는 보정 파라미터를 생성한 후, 생성 된 보정 파라미터를 기초로 결합 확률 분포를 생성하는 방식으로 음향 모델과 언어 모델의 합성에서 생기는 괴 리를 최소화 하는데 그 목적이 있다. 이러한 보정 파라미터(tuning parameter)는 수치적 방법을 통해 학습될 수있으며, 제안하는 방법을 통해 학습된 음성인식 시스템의 구성요소는 보정된 확률 값을 생성하여 확률값의 결합 확률 분포를 보정하기 때문에, 음성인식 시스템의 인식 성능을 효과적으로 향상시킬 수 있다. 이하 본 발명의 구성 요소에 대해 알아본다. 도 3은 본 발명의 일 실시예에 따른 음성 인식 모델의 일부 구성 요소를 도시한 블록도이고, 도 4는 본 발명의 일 실시예에 따른 음성 인식 모델이 학습을 수행하는 방법을 설명하기 위한 도면이다. 도 3과 도 4를 참조하면, 본 발명의 일 실시예에 따른 음성 모델 생성 장치는 음향 모델, 언어 모델 및 결합 파라미터 생성부와 결합 확률 분포 생성부를 포함하는 E2E 음성 모델 생성부를 포함할 수 있다. 음향 모델(Acoustic Model,110)은 입력 되는 음성 정보를 텍스트 정보로 변환하는 모델로서, 구체적으로 본 발 명에 따른 음향 모델은 음성 정보를 입력 정보로 하고, 상기 음성 정보에 대해 대응되는 제1텍스트 정 보를 출력 정보로 하는 제1인공신경망 모듈을 포함할 수 있다. 따라서, 도면에는 도시하지 않았지만 음향 모델 이 포함하는 제1인공신경망 모듈은 입력되는 입력 정보에 대응하여 특정 정보를 출력하는 추론 세션과 입력 정 보 및 출력 정보와 레퍼런스 정보를 기초로 학습을 수행하는 학습 세션을 포함할 수 있다. 본 발명에 따라 인공 신경망이 학습되는 과정에 대한 설명은 후술하도록 한다. 음향 모델은 입력되는 음성 정보는 실시간으로 입력되는 사용자의 음성일 수도 있고, 기 저장되어 있 는 특정 화자의 음성일 수 도 있다. 음향 모델은 언어 모델이 아니기 때문에, 음향 모델이 출력하는 제1텍스트 정보 문맥의 의 미에 맞춰 재구성된 텍스트 정보는 아니고, 순순하게 음향 모델에 입력된 음성 정보를 텍스트 정보로 변환 한 정보를 의미한다. 언어 모델(Language Mode, 120)은 입력되는 텍스트에 대해 문맥 및 상황에 맞춰 언어를 예측하고 재배열하는 모 델로서, 구체적으로 방대한 텍스트로 분석한 언어 모델을 이용하여 현재 입력되고 있는 단어들 간의 관계를 확 률로 나타내어, 특정 단어 다음에 나올 확률이 높은 단어를 예측하고, 예측한 단어를 출력하는 모델을 의미한다. 따라서, 언어 모델은 사용 목적에 맞춰 학습 데이터를 준비하고 이를 기초로 학습하는데, 예를 들어 금융 용어 에 특화된 언어 모델, 법률 용어에 특화된 언어 모델, 과학 용어에 특화된 언어 모델 들 그 사용 목적에 맞춰 모델이 학습될 수 있다. 따라서, 언어 모델은 학습된 텍스트 데이터의 종류에 따라 입력되는 단어가 동일하더라 도 서로 다른 결과를 출력할 수 있다. 본 발명에 따른 언어 모델은 앞서 설명한 특징을 가지는 그대로 포함하는 언어 모델로서, 구체적으로 음향 모델이 출력하는 제1텍스트 정보를 입력 정보로 하고, 사용자의 목적에 맞춰져 학습된 언어 모델의 특 징에 기초하여 제1텍스트 정보에 대응 되는 제2텍스트 정보(1E)를 출력 정보로 출력하는 제2인공신경망 모 듈을 포함할 수 있다. 따라서, 도면에는 도시하지 않았지만 언어 모델이 포함하는 2인공신경망 모듈은 입력되는 입력 정보에 대 응하여 특정 정보를 출력하는 추론 세션과 입력 정보 및 출력 정보와 레퍼런스 정보를 기초로 학습을 수행하는 학습 세션을 포함할 수 있다. 본 발명에 따라 인공신경망이 학습되는 과정에 대한 설명은 후술하도록 한다. 본 발명에 따른 E2E 음성 모델 생성부는 음향 모델이 출력하는 출력 정보에 대한 확률 분포 정보를 포함하고 있는 제1확률 분포 정보와 언어 모델이 출력하는 출력 정보에 대한 확률 분포 정보를 포함하고 있는 제2확률 분포 정보를 기초로 결합 확률 분포를 생성하고, 결합 확률 분포를 기초로 E2E(End-to-End) 모델 을 생성할 수 있다. 캘리브레이션 보정이란, 인공신경망 모듈이 출력하는 출력 값이 실제 모델의 신뢰도(confidence)를 반영하도록 인공신경망 모듈의 각종 파라미터 값을 변경하는 보정을 의미한다. 예를 들어, 입력 정보 A의 출력 정보 B 에 대한 모형의 출력이 0.9가 나왔을 때, 90 % 확률로 B일 것 이라는 의미를 갖도록 인공신경망 모듈의 각종 파라 미터 값을 변경하고 보정하는 것을 캘리브레이션 또는 캘리브레이션 보정을 수행한다고 지칭할 수 있다. 따라서, E2E 음성 모델 생성부는 결합 확률 분포를 생성함에 있어서, 독립적으로 학습되는 음향 모델 및 언어 모델에 대해 각각 ECE(Expected calibration error)를 측정한 후, 측정된 ECE의 값을 최소화 하는 방향으로 출력의 엔트로피를 조절하는 보정 파라미터(tuning parameter)를 생성한다. 여기서 보정 파라미터 는 수치적 방법을 통해 학습될 수 있으며, 이러한 방법으로 학습된 음성인식 시스템의 구성요소는 보정된 확률 값을 생성하여 확률값의 결합 확률 분포 생성하고 이를 이용하기 때문에 음성인식 시스템의 인식 성능을 효과적 으로 향상시킬 수 있는 장점이 존재한다. E2E 음성 모델 생성부의 켈리브레이션에 따른 학습 방법에 대해 설명하면, 도 4에 도시된 바와 같이 E2E 음성 모델 생성부는 제1확률 분포 정보에 대응되는 레퍼런스 데이터인 제1밸리데이션 세트(validation set)를 기초로 제1확률 분포 정보에 대해 캘리브레이션을 진행하여 상기 제1보정 파라미터(scaling parameter) 를 생성하고, 생성된 제1보정 파라미터에 기초하여 상기 음향 모델을 학습시킬 수 있다. 제2확률 분포 정보에 대응되는 레퍼런스 데이터인 제2밸리데이션 세트(validation set)를 기초로 제2확률 분포 정보에 대해 캘리브레이션을 진행하여 제2보정 파라미터를 생성하고, 생성된 제2보정 파라미터를 기초로 언어 모델을 학습 시킬 수 있다. 즉, E2E 음성 모델 생성부는 아웃풋 레이어에 해당하는 소프트맥스 함수를 부드럽게(soften) 해주는 방법 으로 캘리브레이션을 수행할 수 있는데, 구체적으로 K개의 라벨(label)이 붙어 있는 다중 분류 문제 상황에서 로직 백터(Logit vector) Z를 단일 스칼라 파라미터(single scalar parameter)인 T를 이용하여 아래 수학식 2 과 변환함으로써, 캘리브레이션을 수행할 수 있다. (수학식 2)"}
{"patent_id": "10-2021-0185933", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "q^는 켈리버레이트 확률(calibrated probability)을 의미하며, 상기 수학식 3은 K개의 라벨(label)이 붙어 있 는 다중 분류 문제에서 로직 백터(Logit vector)를 단일 스칼라 파라미터(single scalar parameter)인 T로 나 눔으로써, 캘리브레이션을 수행하는 것을 의미한다. 수학식 2에서 단일 스칼라 파라미터인 T 가 1인 경우, 소프트맥스(Softmax)를 이용하여 신뢰도를 구하는 원래의 식이 되며, T 가 커질 수록 최종 q 는 1/K 로 수렴하게 되며, T가 0에 가까워질수록 q는 1에 가까워 진다. 또한, 단일 스칼라 파라미터 T는 학습에 의해 최적의 값이 결정되어 질 수 있는데, 구체적으로 E2E 음성 모델 생성부의 밸리데이션 세트(validation set)와 동일한 특징을 가지고 있는 또 다른 밸리데이션 세트인 캘리 브레이션 밸리데이션 세트를 이용하여 단일 스칼라 파라미터 T 대해 학습을 수행할 수 있다. 또한, E2E 음성 모델 생성부는 캘리브레이션 밸리데이션 세트를 이용하여, 캘리브레이션 밸리데이션 세트 에서의 NLL(Negative Log Likelihood) 값이 최소가 되도록 단일 스칼라 파라미터 T에 대해 학습을 수행할 수 있 다. 이와 같은 방법으로 캘리브레이션을 수행을 하는 경우, 단일 스칼라 파라미터인 T는 소프트 맥스의 아웃풋 의 최대값을 바꾸지 않으면서 캘리브레이션을 수행할 수 있다. 따라서, 이러한 방법은 종래 모델의 캘리브레이 션에만 영향을 주고 정확도(accuracy)에는 영향을 주지 않기 때문에 종래 학습되어 있는 인공신경망 모듈에도 적용할 수 있는 장점이 존재하며, 이렇게 학습이 되는 경우 E2E 음성 모델 생성부에서 생성하는 음성 모델 의 출력 정보에 대한 신뢰도 정보가 실제 확률과 가까워지는 효과가 발생한다. 또한, 본 발명에 따른 E2E 음성 모델 생성부는 앞서 설명한 캘리브레이션을 진행하기 전에, 도 4에 도시된 바와 같이 음향 모델에 대해서는 제1텍스트 정보에 대응되는 제1학습 세트(train set)를 기초로 상기 음향 모델의 손실 함수를 최소화하는 방향으로 학습을 수행할 수 있고, 상기 언어 모델에 대해서는 상기 제2텍스트 정보에 대응되는 제2학습 세트(train set)를 기초로 상기 음향 모델의 손실 함수를 최소화하는 방향으로 학습을 수행할 수 있다. 구체적으로, E2E 음성 모델 생성부는 음향 모델의 출력 정보인 제1텍스트 정보와 학습 데이터 (41, train set)에 있는 제1텍스트 정보에 대응되는 제1레퍼런스 데이터와의 차이를 제1손실함수로 하여, 제1손실함수를 최소화하는 방향으로 음향 모델의 파라미터를 보정하여, 음향 모델을 학습시킬 수 있 다. 또한, E2E 음성 모델 생성부는 언어 모델의 출력 정보인 제2텍스트 정보와 학습 데이터(41, train set)에 있는 제2텍스트 정보에 대응되는 제2레퍼런스 데이터와의 차이를 제2손실함수로 하여, 제2손 실함수를 최소화하는 방향으로 음향 모델의 파라미터를 보정하여, 언어 모델을 학습시킬 수 있다.캘리브레이션 보정을 진행하기 전에, 음향 모델과 언어 모델에 대해 출력 정보에 대한 학습을 수행한 후, 켈리브레이션 보정을 수행하면 음향 모델과 언어 모델의 출력도의 신뢰도와 정확도를 동시에 효 과적으로 높일 수 있는 효과가 발생한다. 도 5는 본 발명에 따른 효과를 설명하기 위한 도면으로서, 도 5의 (a)는 켈리브레이션과 에러율에 대한 관계를 도시한 그래프이고, 도 5의 (b)는 빔 와이드(beam width)에 따른 에러율을 도시한 그래프이며, 도 5에서 ECE는 Expected Calibration Error를 의미하고, WER은 단어 인식 에러율인 Word Error Rate를 의미한다. 도 5의 (a)를 참조하면, 본 발명에 따라 보정 파라미터를 통해 켈리브레이션을 진행한 경우 ECE가 도시된 바와 같이 감소함을 알 수 있으며, 이에 따라 결합 확률 분포가 실제 확률 분포를 근사하며 전체 음성 인식 시스템의 에러율 (WER, Word error rate)을 감소시키는 것을 알 수 있다. 또한, 도 5의 (b)를 참조하면, 빔서치를 이용한 음성 인식 디코딩 과정에서의 beam width에 따라 음성 인식 성 능이 차이가 있음을 알 수 있는데, beam width가 2 이상인 경우부터 본 발명에 따른 음성 인식 시스템의 에러 율이 감소되는 것을 알 수 있다. 도 6은 종래 기술에 따른 음성 모델과 본 발명에 따른 음성 모델을 비교하기 위한 도면으로서, 도 6의 (a)는 도 2에서 설명한 그래프와 동일하며, 도 6의 (b)는 본 발명에 따른 각각의 모델들에서 출력하는 신뢰도 정보와 정 확도 정보에 대한 그래프이다. 도 6의 경우 전체 실험 세트를 음성 인식 시스템의 출력 신뢰도(confidence)에 따라 총 15개의 빈(bin)으로 나 누어 실험을 진행하였으며, 도 6의 그래프는 각각의 빈(bin) 별로 신뢰도 정보와 실제 정확도(accuracy)를 비교 한 그래프이다. 도 6의 (a)의 경우 앞서 설명한 바와 같이 음성 인식 시스템 성능 저하의 원인이 되는 overconfidence 문제를 시각적으로 보여주고 있다. 이 때 overconfidence 문제는 실제 음성 인식 시스템의 출력 확률값(파란색)이 실제 정답을 맞출 확률(빨간색)보다 높기 때문에 발생한다. 그러나 도 6의 (b)와 같이 본 발명에 따른 음성 인식 시스템의 경우 에 빨간색 그래프와 파란색 그래프가 상대 적으로 도 6의 (a)에 비해 상대적으로 일치한 결과가 도시되었으며, 이를 통해 본 발명에 따른 음성 모델이 실 제 음성 모델이 출력하는 정보와 가장 유사한 정보를 출력하는 모델임을 알 수 있다. 도 7은 본 발명에 따른 E2E 음성 모델과 종래 기술에 따른 E2E 음성 모델의 성능을 비교하기 위한 도면이다. 도 7에 따른 실험의 경우 공인 데이터셋인 LibriSpeech 데이터셋을 이용하였으며, 도 7의 결과와 같이 본 발명 에 따라 제안된 방법(temperature scaling)이 테스트셋에 대해 음성 인식 시스템의 ECE와 Cross entropy를 효 과적으로 낮춘다는 것을 확인 할 수 있다. 지금까지 도면을 통해 본 발명에 따른 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 방법 및 장치에 대해 자세히 알아보았다. 일 실시예에 따른 캘리브레이션 보정을 이용하여 E2E 음성 인식 모델을 생성하는 음성 인식 모델 생성 방법 및 장치는 음향 모델과 언어 모델을 결합하여 E2E 음성 인식 모델을 생성함에 있어서, 보정 파라미터를 경험적으로 찾아내는 종래 기술과 달리 ECE(Expected calibration error)를 최소화하는 방향으로 파라미터를 학습하고 도출 해내므로, 종래 기술과 달리 모델 결합을 보정 파라미터를 탐색을 위한 시간 및 비용을 효과적으로 감소시킬 수 있는 효과가 존재한다. 또한, 종래 기술의 경우 경험적으로 보정 파라미터를 생성하였기 때문에 보정 파라미터가 최적의 파라미터인지 보장을 하기가 어려웠으나, 본 발명의 경우 ECE를 최소화 시키는 방향으로 학습을 하고 이를 기초로 보정 파라 미터를 생성하므로 최적의 음성 인식 성능을 보장할 수 있는 보정 파라미터를 생성할 수 있는 장점이 존재한다. 또한, 켈리브레이션 보정을 수행한 후 이를 기초로 E2E 음성 인식 모델을 생성하므로, 생성된 E2E 음성 인식 모 델에서 출력되는 신뢰도 정보는 실제 확률에 가까워지는 효과가 존재한다. 한편, 본 명세서에 기재된 \"~부\"로 기재된 구성요소들, 유닛들, 모듈들, 컴포넌트들 등은 함께 또는 개별적이지 만 상호 운용 가능한 로직 디바이스들로서 개별적으로 구현될 수 있다. 모듈들, 유닛들 등에 대한 서로 다른 특 징들의 묘사는 서로 다른 기능적 실시예들을 강조하기 위해 의도된 것이며, 이들이 개별 하드웨어 또는 소프트 웨어 컴포넌트들에 의해 실현되어야만 함을 필수적으로 의미하지 않는다. 오히려, 하나 이상의 모듈들 또는 유 닛들과 관련된 기능은 개별 하드웨어 또는 소프트웨어 컴포넌트들에 의해 수행되거나 또는 공통의 또는 개별의 하드웨어 또는 소프트웨어 컴포넌트들 내에 통합될 수 있다. 특정한 순서로 작동들이 도면에 도시되어 있지만, 이러한 작동들이 원하는 결과를 달성하기 위해 도시된 특정한 순서, 또는 순차적인 순서로 수행되거나, 또는 모든 도시된 작동이 수행되어야 할 필요가 있는 것으로 이해되지 말아야 한다. 임의의 환경에서는, 멀티태스킹 및 병렬 프로세싱이 유리할 수 있다. 더욱이, 상술한 실시예에서 다양한 구성요소들의 구분은 모든 실시예에서 이러한 구분을 필요로 하는 것으로 이해되어서는 안되며, 기술된 구성요소들이 일반적으로 단일 소프트웨어 제품으로 함께 통합되거나 다수의 소프트웨어 제품으로 패키징될 수 있다는 것이 이해되어야 한다. 컴퓨터 프로그램(프로그램, 소프트웨어, 소프트웨어 어플리케이션, 스크립트 또는 코드로도 알려져 있음)은 컴 파일되거나 해석된 언어나 선험적 또는 절차적 언어를 포함하는 프로그래밍 언어의 어떠한 형태로도 작성될 수 있으며, 독립형 프로그램이나 모듈, 컴포넌트, 서브루틴 또는 컴퓨터 환경에서 사용하기에 적합한 다른 유닛을 포함하여 어떠한 형태로도 전개될 수 있다. 부가적으로, 본 특허문헌에서 기술하는 논리 흐름과 구조적인 블럭도는 개시된 구조적인 수단의 지원을 받는 대 응하는 기능과 단계의 지원을 받는 대응하는 행위 및/또는 특정한 방법을 기술하는 것으로, 대응하는 소프트웨 어 구조와 알고리즘과 그 등가물을 구축하는 데에도 사용 가능하다. 본 명세서에서 기술하는 프로세스와 논리 흐름은 입력 데이터 상에서 작동하고 출력을 생성함으로써 기능을 수 행하기 위하여 하나 이상이 컴퓨터 프로그램을 실행하는 하나 이상이 프로그래머블 프로세서에 의하여 수행 가 능하다. 본 기술한 설명은 본 발명의 최상의 모드를 제시하고 있으며, 본 발명을 설명하기 위하여, 그리고 당업자가 본 발명을 제작 및 이용할 수 있도록 하기 위한 예를 제공하고 있다. 이렇게 작성된 명세서는 그 제시된 구체적인 용어에 본 발명을 제한하는 것이 아니다. 이상에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자 또는 해당 기술 분야에 통상의 지식을 갖는 자라면, 후술될 특허청구범위에 기재된 본 발명의 사상 및 기술 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다. 따라서, 본 발명의 기술적 범위는 명세서의 상세한 설명에 기재된 내용으로 한정되는 것이 아니라 특허청구범위 에 의해 정해져야 할 것이다."}
{"patent_id": "10-2021-0185933", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 음성 모델 생성 시스템의 일부 구성 요소를 도시한 블럭도이다. 도 2는 종래 기술에 따라 E2E모델을 생성하는 경우 발생하는 문제점을 설명하기 위한 도면이다. 도 3은 본 발명의 일 실시예에 따른 음성 인식 모델의 일부 구성 요소를 도시한 블록도이다. 도 4는 본 발명의 일 실시예에 따른 음성 인식 모델이 학습을 수행하는 방법을 설명하기 위한 도면이다. 도 5는 본 발명에 따른 효과를 설명하기 위한 도면으로서, 도 5의 (a)는 켈리브레이션과 에러율에 대한 관계를 도시한 그래프이다. 도 6은 종래 기술에 따른 음성 모델과 본 발명에 따른 음성 모델을 비교하기 위한 도면이다. 도 7은 본 발명에 따른 E2E 음성 모델과 종래 기술에 따른 E2E 음성 모델의 성능을 비교하기 위한 도면이다."}
