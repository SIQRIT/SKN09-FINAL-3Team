{"patent_id": "10-2023-0185630", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0040471", "출원번호": "10-2023-0185630", "발명의 명칭": "미래 이벤트와 관련된 캡션 정보를 제공하는 전자 장치 및 그 동작 방법", "출원인": "경북대학교 산학협력단", "발명자": "정호영"}}
{"patent_id": "10-2023-0185630", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "미래 이벤트와 관련된 캡션 정보를 제공하는 전자 장치의 동작 방법에 있어서,적어도 하나의 프로세서에 의하여, 제1 시점에 대응하는 비디오 정보 및 텍스트 정보를 획득하는 단계;상기 적어도 하나의 프로세서에 의하여, 상기 비디오 정보를 기반으로 상기 제1 시점 이후에 도래하는 제2 시점에 대응하는 시각 정보를 포함하는 시각 특징 정보를 도출하는 단계;상기 적어도 하나의 프로세서에 의하여, 상기 텍스트 정보를 기반으로 상기 제2 시점에 대응하는 문맥 정보를포함하는 문맥 특징 정보를 도출하는 단계;상기 적어도 하나의 프로세서에 의하여, 상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으로 미래에 발생할 것으로 예측되는 이벤트 정보를 도출하는 단계;상기 적어도 하나의 프로세서에 의하여, 상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으로 멀티 모달 특징 정보를 도출하는 단계; 및상기 적어도 하나의 프로세서에 의하여, 상기 이벤트 정보 및 상기 멀티 모달 특징 정보를 기반으로 도출된 상기 캡션 정보를 제공하는 단계를 포함하는 동작 방법."}
{"patent_id": "10-2023-0185630", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 시각 특징 정보를 도출하기 위한 제1 모델 및 상기 문맥 특징 정보를 도출하기 위한 제2 모델은 상기 제1시점의 정보 및 상기 제2 시점의 정보를 상호 연관시키기 위한 대조학습을 수행하는 동작 방법."}
{"patent_id": "10-2023-0185630", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 적어도 하나의 프로세서에 의하여, 상기 제1 시점에 대응하는 특징 정보 및 상기 제2 시점에 대응하는 특징 정보 간의 오류율을 산출하는 단계; 및상기 적어도 하나의 프로세서에 의하여, 상기 오류율을 기반으로 상기 시각 특징 정보를 도출하기 위한 제1 모델 및 상기 문맥 특징 정보를 도출하기 위한 제2 모델에 대한 업데이트를 수행하는 단계를 더 포함하는 동작 장법."}
{"patent_id": "10-2023-0185630", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 시각 특징 정보는 상기 비디오 정보를 샘플링(sampling)하여 도출된 복수의 프레임(frame)들 각각에 대하여 추출된 특징 벡터를 통합하여 도출되는 동작 방법."}
{"patent_id": "10-2023-0185630", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 이벤트 정보는 상기 시각 특징 정보 및 상기 문맥 특징 정보를 결합한 멀티 모달 특징에 대한 선형 투영으로부터 도출되는 동작 방법."}
{"patent_id": "10-2023-0185630", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서,공개특허 10-2025-0040471-3-상기 적어도 하나의 프로세서에 의하여, 상기 이벤트 정보 및 미리 저장된 이벤트 레이블 간 제1 교차 엔트로피오차를 측정하는 단계; 및상기 적어도 하나의 프로세서에 의하여, 상기 제1 교차 엔트로피 오차를 기반으로 상기 시각 특징 정보를 도출하기 위한 제1 모델 및 상기 문맥 특징 정보를 도출하기 위한 제2 모델에 대한 업데이트를 수행하는 단계를 더포함하는 동작 방법."}
{"patent_id": "10-2023-0185630", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 적어도 하나의 프로세서에 의하여, 상기 캡션 정보 및 미리 저장된 캡션 레이블 간 제2 교차 엔트로피 오차를 측정하는 단계;상기 적어도 하나의 프로세서에 의하여, 상기 제1 교차 엔트로피 오차 및 상기 제2 교차 엔트로피 오차를 합산하는 단계; 및상기 적어도 하나의 프로세서에 의하여, 상기 합산 결과를 기반으로 상기 제1 모델, 상기 제2 모델, 상기 멀티모달 특징을 도출하기 위한 제3 모델 및 상기 캡션 정보를 도출하기 위한 제4 모델 중 적어도 하나에 대한 업데이트를 수행하는 단계를 더 포함하는 동작 방법."}
{"patent_id": "10-2023-0185630", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항에 있어서,상기 멀티 모달 특징 정보는 상기 시각 특징 정보 및 상기 문맥 특징 정보에 대한 크로스 어텐션(CrossAttention)을 기반으로 도출되는 동작 방법."}
{"patent_id": "10-2023-0185630", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "미래 이벤트와 관련된 캡션 정보를 제공하는 전자 장치에 있어서,제1 시점에 대응하는 비디오 정보 및 텍스트 정보를 획득하는 입력부;상기 비디오 정보 및 상기 텍스트 정보를 기반으로 상기 캡션 정보를 도출하는 프로세서; 및상기 프로세서로부터 도출된 상기 캡션 정보를 출력하는 출력부를 포함하되,상기 프로세서는:상기 비디오 정보를 기반으로 상기 제1 시점 이후에 도래하는 제2 시점에 대응하는 시각 정보를 포함하는 시각특징 정보를 도출하고,상기 텍스트 정보를 기반으로 상기 제2 시점에 대응하는 문맥 정보를 포함하는 문맥 특징 정보를 도출하고,상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으로 미래에 발생할 것으로 예측되는 이벤트 정보를 도출하고,상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으로 멀티 모달 특징 정보를 도출하고,상기 이벤트 정보 및 상기 멀티 모달 특징 정보를 기반으로 도출된 상기 캡션 정보를 도출하는 전자 장치."}
{"patent_id": "10-2023-0185630", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램으로서, 상기 컴퓨터 프로그램은 하나 이상의 프로세서에서 실행되는 경우, 미래 이벤트와 관련된 캡션 정보를 제공하기 위하여 이하의 동작들을 수행하도록 하며, 상기동작들은:제1 시점에 대응하는 비디오 정보 및 텍스트 정보를 획득하는 동작;상기 비디오 정보를 기반으로 상기 제1 시점 이후에 도래하는 제2 시점에 대응하는 시각 정보를 포함하는 시각특징 정보를 도출하는 동작;상기 텍스트 정보를 기반으로 상기 제2 시점에 대응하는 문맥 정보를 포함하는 문맥 특징 정보를 도출하는공개특허 10-2025-0040471-4-동작;상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으로 미래에 발생할 것으로 예측되는 이벤트 정보를 도출하는 동작;상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으로 멀티 모달 특징 정보를 도출하는 동작; 및상기 이벤트 정보 및 상기 멀티 모달 특징 정보를 기반으로 도출된 상기 캡션 정보를 제공하는 동작을 포함하는컴퓨터 판독가능 저장매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0185630", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 미래 이벤트와 관련된 캡션 정보를 제공하는 전자 장치 및 그 동작 방법에 관한 것으로, 본 개시의 실 시 예에 따른 미래 이벤트와 관련된 캡션 정보를 제공하는 전자 장치의 동작 방법은 상기 적어도 하나의 프로세 서에 의하여, 제1 시점에 대응하는 비디오 정보 및 텍스트 정보를 획득하는 단계, 상기 적어도 하나의 프로세서 (뒷면에 계속)"}
{"patent_id": "10-2023-0185630", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공지능 기술에 관한 것으로, 보다 구체적으로, 미래에 발생할 이벤트를 예측하고, 예측된 이벤트와 관련된 캡션 정보를 제공하는 전자 장치 및 그 동작 방법에 관한 것이다."}
{"patent_id": "10-2023-0185630", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 기술의 개발 속도가 가속화되면서 생활, 의료, 금융 등 다양한 산업 분야에서 인공지능 기술이 활용되 고 있다. 초기 인공지능 기술에 대한 연구는 단일 속성에 대한 분류(Classification), 군집화(Clustering) 등에 초점이 맞추어져 있었으나, 기술의 발전에 따라 다중 속성에 대한 데이터 처리를 통하여 다양한 목적을 달성할 수 있게 되었다. 예를 들어, 이미지 캡셔닝(Image Captioning) 및 비디오 캡셔닝(Video Captioning) 기술은 시 각적 속성의 데이터를 처리하여 텍스트 속성의 데이터를 도출할 수 있다. 이미지 캡셔닝 기술은 이미지 정보를 기반으로 이미지 정보에 대응하는 캡션 정보를 생성하는 기술을 의미한다. 비디오 캡셔닝 기술은 비디오 시퀀스에 대한 자연어 설명을 생성하는 기술을 일컫는 것으로, 비디오 정보로부터 도출된 프레임들을 기반으로 비디오 정보에 대응하는 캡션 정보를 생성하는 기술을 의미한다. 그러나, 이미지 캡셔닝 기술 및 비디오 캡셔닝 기술은 입력된 시각 정보의 시점에 대한 정보를 제공하는 바, 다양한 산업 분야 로의 활용을 위하여 미래 시점의 예측 정보를 제공할 수 있는 기술의 필요성이 대두되고 있다. 관련하여, 일본 공개특허공보 JP2022-534781A 및 한국 등록특허공보 10-2458463B1을 참조할 수 있다."}
{"patent_id": "10-2023-0185630", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 미래에 발생할 이벤트를 예측하고, 예측된 이벤트와 관련된 캡션 정보를 제공하는 전자 장치 및 그 동작 방법을 제공하는 것을 목적으로 한다. 본 개시는 멀티 모달(multi modal) 기술을 기반으로 예측 정보를 제공하는 전자 장치 및 그 동작 방법을 제공하 는 것을 목적으로 한다. 본 개시가 해결하고자 하는 과제들은 상술한 과제에 제한되지 않으며, 언급되지 않은 또 다른 과제들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0185630", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 미래 이벤트와 관련된 캡션 정보를 제공하는 전자 장치의 동작 방법에 있어서, 상 기 동작 방법은 상기 적어도 하나의 프로세서에 의하여, 제1 시점에 대응하는 비디오 정보 및 텍스트 정보를 획 득하는 단계, 상기 적어도 하나의 프로세서에 의하여, 상기 비디오 정보를 기반으로 상기 제1 시점 이후에 도래 하는 제2 시점에 대응하는 시각 정보를 포함하는 시각 특징 정보를 도출하는 단계, 상기 적어도 하나의 프로세 서에 의하여, 상기 텍스트 정보를 기반으로 상기 제2 시점에 대응하는 문맥 정보를 포함하는 문맥 특징 정보를 도출하는 단계, 상기 적어도 하나의 프로세서에 의하여, 상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으 로 미래에 발생할 것으로 예측되는 이벤트 정보를 도출하는 단계, 상기 적어도 하나의 프로세서에 의하여, 상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으로 멀티 모달 특징 정보를 도출하는 단계 및 상기 적어도 하나 의 프로세서에 의하여, 상기 이벤트 정보 및 상기 멀티 모달 특징 정보를 기반으로 도출된 상기 캡션 정보를 제 공하는 단계를 포함할 수 있다.또한, 상기 시각 특징 정보를 도출하기 위한 제1 모델 및 상기 문맥 특징 정보를 도출하기 위한 제2 모델은 상 기 제1 시점의 정보 및 상기 제2 시점의 정보를 상호 연관시키기 위한 대조학습을 수행할 수 있다. 또한, 상기 동작 방법은 상기 적어도 하나의 프로세서에 의하여, 상기 제1 시점에 대응하는 특징 정보 및 상기 제2 시점에 대응하는 특징 정보 간의 오류율을 산출하는 단계 및 상기 적어도 하나의 프로세서에 의하여, 상기 오류율을 기반으로 상기 시각 특징 정보를 도출하기 위한 제1 모델 및 상기 문맥 특징 정보를 도출하기 위한 제 2 모델에 대한 업데이트를 수행하는 단계를 더 포함할 수 있다. 또한, 상기 시각 특징 정보는 상기 비디오 정보를 샘플링(sampling)하여 도출된 복수의 프레임(frame)들 각각에 대하여 추출된 특징 벡터를 통합하여 도출될 수 있다. 또한, 상기 이벤트 정보는 상기 시각 특징 정보 및 상기 문맥 특징 정보를 결합한 멀티 모달 특징에 대한 선형 투영으로부터 도출될 수 있다. 또한, 상기 동작 방법은 상기 적어도 하나의 프로세서에 의하여, 상기 이벤트 정보 및 미리 저장된 이벤트 레이 블 간 제1 교차 엔트로피 오차를 측정하는 단계 및 상기 적어도 하나의 프로세서에 의하여, 상기 제1 교차 엔트 로피 오차를 기반으로 상기 시각 특징 정보를 도출하기 위한 제1 모델 및 상기 문맥 특징 정보를 도출하기 위한 제2 모델에 대한 업데이트를 수행하는 단계를 더 포함할 수 있다. 또한, 상기 동작 방법은 상기 적어도 하나의 프로세서에 의하여, 상기 캡션 정보 및 미리 저장된 캡션 레이블 간 제2 교차 엔트로피 오차를 측정하는 단계, 상기 적어도 하나의 프로세서에 의하여, 상기 제1 교차 엔트로피 오차 및 상기 제2 교차 엔트로피 오차를 합산하는 단계 및 상기 적어도 하나의 프로세서에 의하여, 상기 합산 결과를 기반으로 상기 제1 모델, 상기 제2 모델, 상기 멀티 모달 특징을 도출하기 위한 제3 모델 및 상기 캡션 정보를 도출하기 위한 제4 모델 중 적어도 하나에 대한 업데이트를 수행하는 단계를 더 포함할 수 있다. 또한, 상기 멀티 모달 특징 정보는 상기 시각 특징 정보 및 상기 문맥 특징 정보에 대한 크로스 어텐션(Cross Attention)을 기반으로 도출될 수 있다. 본 개시의 실시 예에 따른 미래 이벤트와 관련된 캡션 정보를 제공하는 전자 장치에 있어서, 상기 전자 장치는 제1 시점에 대응하는 비디오 정보 및 텍스트 정보를 획득하는 입력부, 상기 비디오 정보 및 상기 텍스트 정보를 기반으로 상기 캡션 정보를 도출하는 프로세서 및 상기 프로세서로부터 도출된 상기 캡션 정보를 출력하는 출력 부를 포함하되, 상기 프로세서는 상기 비디오 정보를 기반으로 상기 제1 시점 이후에 도래하는 제2 시점에 대응 하는 시각 정보를 포함하는 시각 특징 정보를 도출하고, 상기 텍스트 정보를 기반으로 상기 제2 시점에 대응하 는 문맥 정보를 포함하는 문맥 특징 정보를 도출하고, 상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으로 미래에 발생할 것으로 예측되는 이벤트 정보를 도출하고, 상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반 으로 멀티 모달 특징 정보를 도출하고, 상기 이벤트 정보 및 상기 멀티 모달 특징 정보를 기반으로 도출된 상기 캡션 정보를 도출할 수 있다. 본 개시의 실시 예에 따른 컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램으로서, 상기 컴퓨터 프로그램 은 하나 이상의 프로세서에서 실행되는 경우, 미래 이벤트와 관련된 캡션 정보를 제공하기 위하여 이하의 동작 들을 수행하도록 하며, 상기 동작들은 제1 시점에 대응하는 비디오 정보 및 텍스트 정보를 획득하는 동작, 상기 비디오 정보를 기반으로 상기 제1 시점 이후에 도래하는 제2 시점에 대응하는 시각 정보를 포함하는 시각 특징 정보를 도출하는 동작, 상기 텍스트 정보를 기반으로 상기 제2 시점에 대응하는 문맥 정보를 포함하는 문맥 특 징 정보를 도출하는 동작, 상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으로 미래에 발생할 것으로 예측 되는 이벤트 정보를 도출하는 동작, 상기 시각 특징 정보 및 상기 문맥 특징 정보를 기반으로 멀티 모달 특징 정보를 도출하는 동작 및 상기 이벤트 정보 및 상기 멀티 모달 특징 정보를 기반으로 도출된 상기 캡션 정보를 제공하는 동작을 포함할 수 있다."}
{"patent_id": "10-2023-0185630", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면 미래에 발생할 이벤트를 예측하고, 예측된 이벤트와 관련된 캡션 정보를 제공함으로써, 미래 시점에 발생할 이벤트에 대한 사용자의 대응을 용이하게 할 수 있다. 본 개시에 따르면 멀티 모달(multi modal) 기술을 기반으로 예측 정보를 제공함으로써, 제공되는 정보에 대한 정확도를 향상시킬 수 있다. 본 개시에 따른 효과들은 상술한 효과에 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0185630", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면들에 기재된 내용들을 참조하여 본 발명에 따른 예시적 실시 예를 상세하게 설명한다. 다만, 본 발명이 예시적 실시 예들에 의해 제한되거나 한정되는 것은 아니다. 다른 정의가 없다면, 본 명세서에서 사 용되는 모든 용어(기술 및 과학적 용어를 포함)는 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 사용될 것이나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 일반적으로 사용되는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 특정한 경우, 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서, 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라, 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 본 명세서에서 사용된 단수형은 특별히 언급하지 않는 한 복수형도 포함한다. 또한, 본 명세서 전체에서 기재된 \"a, b 및/또는 c 중 적어도 하나\"의 표현은, 'a 단독', 'b 단독', 'c 단독', 'a 및 b', 'a 및 c', 'b 및 c', 또는 'a, b, c 모두'를 포괄할 수 있다. 한편, 본 명세서에서 사용되는 \"제1 및/또는 제2\" 등의 용어는 다양한 구성요소들을 설명하기 위하여 사용될 수 있으나, 이는 하나의 구성요소를 다른 구성요소로부터 구별하기 위한 목적으로만 사용될 뿐, 해당 용어로 지칭 되는 구성요소로 한정하기 위한 것은 아니다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않는 한, 제1 구성요 소는 제2 구성요소로 명명될 수 있으며, 제2 구성요소 또한 제1 구성요소로 명명될 수 있다. 또한, 본 명세서에 기재된 “…부”, “…모듈” 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또 한, 본 명세서에서 본 개시의 실시 예는 기능적인 블록 구성들 및 다양한 처리 단계들로 나타내어질 수 있다. 이러한 기능 블록들은 특정 기능들을 실행하는 다양한 개수의 하드웨어 또는/및 소프트웨어 구성들로 구현될 수 있다. 예를 들어, 본 개시의 실시 예는 하나 이상의 마이크로프로세서의 제어 또는 다른 제어 장치들에 의해서 다양한 기능들을 실행할 수 있는, 메모리, 프로세싱, 로직(logic), 룩 업 테이블(look-up table) 등과 같은 직 접 회로 구성들을 채용할 수 있다. 본 개시에 따른 실시 예에서, 인공지능과 관련된 기능은 프로세서 및 메모리를 통해 구현될 수 있다. 이 때, 프 로세서는 CPU(Center Processing Unit), AP(Application Processor), DSP(Digital Signal Processor) 등과 같 은 범용 프로세서, GPU(Graphic Processing Unit), VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 및 NPU(Neural network Processing Unit)와 같은 인공지능 전용 프로세서 중 어느 하나일 수 있다. 프로세서는 메모리에 저장된 기 정의된 동작 규칙 또는 인공지능 모델에 따라 입력 데이터를 처리할 수 있다. 또는, 프로세 서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 본 개시에 따른 일부 실시 예에서, 인공지능과 관련된 기능은 복수의 프로세서들을 통 해 구현될 수 있다. 본 개시에 따른 실시 예에서, 기 정의된 동작 규칙 또는 인공지능 모델은 기계학습을 수행하도록 구성될 수 있 다. 여기서, 기계학습을 수행하도록 구성된다는 것은, 기 정의된 동작 규칙 또는 인공지능 모델이 학습 알고리 즘을 기반으로 다수의 학습 데이터들을 이용하여 학습되어 원하는 특성(또는 목적)을 수행하도록 구성됨을 의미한다. 이러한 학습은 본 개시에 따른 인공지능이 구현되는 장치 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어질 수도 있다. 인공지능 모델은 뉴럴 네트워크(또는 인공 신경망)로 구현될 수 있으며, 기계학습과 인지과학에서 생물학의 신 경을 모방한 통계학적 학습 알고리즘에 기반하여 동작할 수 있다. 뉴럴 네트워크는 시냅스의 결합으로 네트워크 를 형성한 인공 뉴런(노드)이 학습을 통해 시냅스의 결합 세기를 변화시켜 문제 해결 능력을 가지는 모델 전반 을 의미할 수 있다. 뉴럴 네트워크는 복수의 신경망 레이어(layer)들로 구성될 수 있으며, 예시적으로 뉴럴 네 트워크는 입력 레이어(input layer), 은닉 레이어(hidden layer) 및 출력 레이어(output layer)를 포함할 수 있다. 복수의 신경망 레이어들 각각은 적어도 하나의 노드(node) 및 적어도 하나의 가중치(weight)를 포함할 수 있으며, 이전(precious) 레이어의 연산 결과와 가중치 간의 연산을 통해 신경망 연산을 수행할 수 있다. 복수의 신경망 레이어들이 가지고 있는 적어도 하나의 가중치는 인공지능 모델의 학습 결과에 의하여 최적화될 수 있다. 예를 들어, 학습 과정동안 인공지능 모델에서 획득한 손실(loss) 값 또는 비용(cost) 값이 감소 또는 최 소화되도록 적어도 하나의 가중치가 갱신될 수 있다. 뉴럴 네트워크는 임의의 입력으로부터 예측하고자 하는 결 과를 추론할 수 있다. 인공지능 모델의 학습 방법은 학습 방식에 따라 입력 데이터 및 출력 데이터가 훈련 데이터로써 제공되어 문제 (입력 데이터)에 대응하는 정답(출력 데이터)이 정해져 있는 지도학습(supervised learning), 출력 데이터 없이 입력 데이터만 제공되어 문제(입력 데이터)에 대응하는 정답(출력 데이터)이 정해지지 않은 비지도학습 (unsupervised learning) 및 현재 상태(state)에서 어떤 행동(action)을 취할 때마다 보상(reward)이 부여되고, 이러한 보상을 최대화하는 방향으로 학습을 진행하는 강화학습(reinforcement learning) 등으로 구분 될 수 있다. 또는, 학습 모델의 구조인 아키텍처에 따라 구분될 수도 있다. 본 개시의 실시 예에서, 인공지능 모델은 GoogleNet, AlexNet, VGG Network 등과 같은 CNN(Convolution Neural Network), R-CNN(Region with Convolution Neural Network), RPN(Region Proposal Network), RNN(Recurrent Neural Network), S-DNN(Stacking-based deep Neural Network), S-SDNN(State-Space Dynamic Neural Network), Deconvolution Network, DBN(Deep Belief Network), RBM(Restrcted Boltzman Machine), Fully Convolutional Network, LSTM(Long Short-Term Memory) Network, Classification Network, Generative Modeling, eXplainable AI, Continual AI, Representation Learning, AI for Material Design, 자연어 처리를 위한 BERT, SP-BERT, MRC/QA, Text Analysis, Dialog System, GPT-3, GPT-4, 비전 처리를 위한 Visual Analytics, Visual Understanding, Video Synthesis, ResNet 데이터 지능을 위한 Anomaly Detection, Prediction, Time-Series Forecasting, Optimization, Recommendation, Data Creation 등 다양한 인공지능 구 조 및 알고리즘 중 적어도 하나를 이용할 수 있으며, 상술한 예시들은 본 개시의 실시 예에 따라 이용되는 인공 지능 구조 및 알고리즘의 예를 나열한 것인 뿐, 본 개시의 실시 예에 따라 이용되는 인공지능 구조 및 알고리즘 을 제한하는 것은 아니다. 이하, 본 개시의 다양한 실시 예들은 첨부된 도면을 참조하여 상세하게 설명될 것이다. 실시 예를 설명함에 있 어서 본 발명이 속하는 기술 분야에 익히 알려져 있고 본 발명과 직접적으로 관련이 없는 기술 내용에 대해서는 설명을 생략할 것이다. 이는 불필요한 설명을 생략함으로써 본 발명의 요지를 흐리지 않고 더욱 명확히 전달하 기 위함이다. 마찬가지 이유로 첨부 도면에 있어서 일부 구성요소는 과장되거나 생략되거나 개략적으로 도시되 었다. 또한, 각 구성요소의 크기는 실제 크기를 전적으로 반영하는 것이 아니다. 본 명세서에서, 전문에 걸쳐 동일한 참조 부호는 동일한 또는 대응하는 구성 요소를 지칭할 수 있다. 한편, 이하 도 1 내지 도 5를 통하여 본 개시의 실시 예에 따른 장치의 구성, 구조, 동작 및 특징을 설명함에 있어서, 설명의 편의를 위하여 시점과 관련된 표현은 현재 시점(t)을 기준으로 표기될 것이다. 예를 들어, 현재 시점 이후 도래하는 미래 시점은 t+1로 표기될 것이며, 제1 미래 시점 이후 도래하는 제2 미래 시점은 t+2로 표 기될 것이다. 도 1은 본 개시의 실시 예에 따른 전자 장치를 설명하기 위한 블록도이다. 본 개시의 실시 예에 따른 전자 장치는 시각 정보 및 텍스트 정보를 기반으로 미래에 발생할 이벤트를 예측 하고, 예측된 이벤트에 대응하여 캡션 정보를 도출할 수 있다. 본 명세서에서, 캡션 정보란 예측된 미래 이벤트 를 설명하는 텍스트 정보를 의미한다. 도 1을 참조하면, 전자 장치는 입력부, 프로세서, 저장부 및 출력부를 포함할 수 있다. 본 개시에 따른 실시 예에서, 입력부는 임의의 시점(t)에 대한 시각(visual) 정보 및 텍스트(text) 정보 중 적어도 일부를 획득할 수 있다. 시각 정보는 이미지 데이터 및 비디오 데이터 중 적어도 일부를 포함할 수 있다. 일부 실시 예에서, 입력부는 촬영 장치, 키 패드(key pad), 터치 패드(touch pad) 등 다양한 입력 장치 중 적어도 하나를 포함할 수 있다. 또는, 일부 실시 예에서, 입력부는 외부에 구축된 서버로부터 시 각 정보 및 텍스트 정보 중 적어도 일부를 획득할 수 있으며, 이 경우, 입력부는 외부 서버와 통신을 수행 하기 위한 통신 모듈을 포함할 수 있다. 프로세서는 본 개시의 실시 예에 따른 전자 장치의 전반적인 동작을 제어할 수 있다. 본 개시의 실시 예에 따르면, 프로세서는 인공지능 모델을 통하여 입력부로부터 획득한 시각 정보 및 텍스트 정보 중 적어도 일부에 대응하여 미래에 발생할 이벤트를 예측하고, 예측된 이벤트에 대응하는 캡션 정보를 도출할 수 있다. 구체적으로, 본 개시의 실시 예에 따른 전자 장치는 t 시점에 대한 시각 정보 및 텍스트 정보 중 적 어도 일부가 입력부를 통하여 획득되는 경우, 인공지능 모델을 통하여 t+1 시점에 발생할 것으로 예측되는 이벤트 정보를 확인하고, 확인된 이벤트 정보 및 대조 학습된 특징 정보에 대응하는 캡션 정보를 도출할 수 있 다. 도 1에서, 전자 장치는 하나의 프로세서를 포함하는 것으로 도시되었으나, 이는 본 개시에 따른 일 실시 예일 뿐, 본 개시에 따른 전자 장치의 구성을 제한하는 것은 아니다. 일부 실시 예에서, 전자 장치 는 미래에 발생할 이벤트를 예측하고, 예측된 이벤트에 대응하여 캡션 정보를 도출하기 위하여 복수의 프로 세서들을 포함할 수 있다. 프로세서를 통해 캡션 정보를 도출하는 구체적인 방법은 후술할 도 2 내지 도 5 를 통하여 상세히 설명한다. 저장부는 본 개시의 실시 예에 따른 전자 장비의 동작을 제어하기 위하여 프로세서에 의해 실행 될 프로그램, 프로세서가 판독할 수 있는 명령어 및 프로그램 코드(program code) 중 적어도 일부를 저장 할 수 있다. 또한, 저장부는 프로세서에 의하여 처리되는 정보를 저장할 수 있다. 실시 예에서, 저장 부는 플래시 메모리(flash memory), 하드디스크(hard disk), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어, SD 또는 XD 메모리 등), 램(RAM, Random Access Memory), SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기디스크 및 광디스 크 중 적어도 하나의 타입의 저장 매체를 포함할 수 있다. 출력부는 프로세서로부터 도출된 캡션 정보를 외부로 출력할 수 있다. 일부 실시 예에서, 출력부 는 표시 장치, 음향 출력 장치 등 다양한 출력 장치 중 적어도 하나를 포함할 수 있다. 또는, 일부 실시 예에서, 출력부는 외부에 구축된 장치에 프로세서로부터 도출된 캡션 정보를 제공할 수 있으며, 이 경우, 출력부는 외부에 구축된 장치와 통신을 수행하기 위한 통신 모듈을 포함할 수 있다. 본 개시의 실시 예에 따른 전자 장치는 미래에 발생할 이벤트를 예측하고, 예측된 이벤트와 관련된 캡션 정 보를 제공함으로써, 미래 시점에 발생할 이벤트에 대한 사용자의 대응을 용이하게 할 수 있다. 또한, 본 개시의 실시 예에 따른 전자 장치는 멀티 모달(multi modal) 기술을 기반으로 예측 정보를 제공함으로써, 제공되는 정보에 대한 정확도를 향상시킬 수 있다. 본 개시의 실시 예에 따른 전자 장치는 미래에 발생할 위험을 탐지하기 위한 시스템에 적용될 수 있으며, 구체적인 예로서, 범죄 예방 시스템, 교통 사고 예방 시스템, 자동차 주행 보조 시스템, 도심 항공 모빌리티 (UAM, Urban Air Mobility) 사고 방지 시스템, 장애인을 위한 보호 시스템 등에 적용될 수 있다. 또는, 본 개시 의 실시 예에 따른 전자 장치는 사용자의 편의성을 향상시키기 위하여 스마트 의료, 스마트 팩토리, 스마트 자동차와 같은 스마트 장치와 융합될 수 있으며, 인공지능을 이용한 챗봇(chatbot) 시스템에 적용되어 사용자의 대화 방향을 미리 예측할 수도 있다. 도 2는 본 개시의 실시 예에 따른 전자 장치(10, 도 1 참조)의 아키텍처를 설명하기 위한 블록도이다. 본 개시의 실시 예에 따른 전자 장치는 인공지능 모델을 이용하여 미래 시점에 발생할 이벤트를 예측하고, 예측된 이벤트에 대응하는 캡션 정보를 생성할 수 있다. 도 2를 참조하면, 본 개시의 실시 예에 따른 전자 장치 의 아키텍처는 입력 데이터에 대한 특징 벡터를 추출하도록 학습된 제1 모델 및 제1 모델로부터 추출된 특징 벡터로부터 미래에 발생할 이벤트 정보를 확인하고, 이벤트 정보에 대응하여 캡션 정보를 도출하도 록 학습된 제2 모델로 구성될 수 있다. 도 2를 참조하면, 제1 모델은 입력된 비디오 정보에 대응하는 특징 벡터를 추출하기 위한 비디오 특징 추 출부 및 입력된 텍스트 정보에 대응하는 특징 벡터를 추출하기 위한 텍스트 특징 추출부를 포함할 수 있다. 비디오 특징 추출부는 입력된 현재 시점의 비디오 정보(Vt)와 상호 연관된 미래 시점의 시각 정보를 포함하는 시각 특징 벡터(또는, 비디오 특징 벡터)(VFcon)를 도출할 수 있다. 현재 시점이란, 특정 시점 또는 특정 시점을 포함하는 연속적인 구간을 의미할 수 있으며, 비디오 정보의 속성에 따라 특정 시점을 포함하는 단 기 또는 장기간 구간을 의미할 수 있다. 텍스트 특징 추출부는 입력된 현재 시점의 텍스트 정보(Tt)와 상 호 연관된 미래 시점의 문맥 정보를 포함하는 문맥 특징 벡터(또는, 텍스트 특징 벡터)(TFcon)를 도출할 수 있다. 본 개시에 따른 실시 예에서, 현재 시점의 정보와 미래 시점의 정보는 표현학습을 기반으로 상호 연관될 수 있으며, 표현학습은 대조학습(Contrastive Learning), 트리플렛학습(Triplet Learning), 앵귤러학습 (Angular Learning) 및 오토인코더(Auto-Encoder) 중 적어도 하나를 포함할 수 있다. 본 명세서에 개시된 일부 실시 예에서, 현재 시점의 정보와 미래 시점의 정보는 대조학습을 기반으로 상호 연관되는 것으로 기술될 것이 나, 이는 본 개시에 따른 하나의 실시 예일 뿐, 본 개시의 내용을 한정하는 것은 아니다. 본 개시의 실시 예에 서, 제1 모델로부터 도출된 미래 시점의 시각 정보를 포함하는 시각 특징 벡터(VFcon) 및 미래 시점의 문맥 정보를 포함하는 문맥 특징 벡터(TFcon)는 제2 모델에 대하여 입력될 수 있다. 제2 모델은 미래 시점의 시각 정보를 포함하는 시각 특징 벡터(VFcon) 및 미래 시점의 문맥 정보를 포함하 는 문맥 특징 벡터(TFcon)를 기반으로 미래에 발생할 이벤트 정보를 도출하는 미래 이벤트 예측부, 시각 정 보 및 텍스트 정보는 혼합하여 멀티 모달 특징 벡터를 도출하기 위한 멀티 모달 혼합부 및 미래 이벤트 정 보와 멀티 모달 특징 벡터를 기반으로 캡션 정보를 도출하기 위한 멀티 모달 디코더를 포함할 수 있다. 미 래 이벤트 예측부는 입력된 미래 시점의 정보를 포함하는 시각 특징 벡터(VFcon) 및 문맥 특징 벡터(TFcon) 를 기반으로 미래에 발생할 것으로 예측되는 이벤트 정보(Et+1)를 도출할 수 있으며, 멀티 모달 혼합부는 입력된 미래 시점의 정보를 포함하는 시각 특징 벡터(VFcon) 및 문맥 특징 벡터(TFcon)를 혼합하고, 혼합된 멀티 모달 정보로부터 미래 시점의 멀티 모달 정보를 포함하는 멀티 모달 특징 벡터(MFcon)를 도출할 수 있다. 멀티 모달 디코터는 입력된 미래에 발생할 것으로 예측되는 이벤트 정보(Et+1) 및 미래 시점의 멀티 모달 정보를 포함하는 멀티 모달 특징 벡터(MFcon)를 기반으로 미래 시점에 대한 캡션 정보(Ct+1)를 도출할 수 있다. 실시 예 에서, 미래 시점에 대한 캡션 정보(Ct+1)는 미래에 발생할 것으로 예측되는 이벤트 정보(Et+1)에 대응하는 텍스트 일 수 있다. 본 개시의 실시 예에 따르면, 제1 모델 및 제2 모델은 각 모델 내에서 수행되는 기능에 대응하는 서 브 모델을 포함할 수 있으며, 각 서브 모델은 기능을 기반으로 최적화된 인공지능 모델이 적용될 수 있다. 또한, 각 서브 모델은 각 서브 모델에 대한 훈련 데이터를 기반으로 기계학습이 수행될 수 있으며, 학습 결과에 기반하여 입력 데이터에 대응하는 결과 데이터를 도출할 수 있다. 본 명세서에서, 서브 모델이라는 용어는 모델 이라는 용어와의 구분을 위하여 사용되었을 뿐, 모델과 상이한 개념을 지칭하기 위하여 도입된 용어로 사용되지 않는다. 다시 말하면, 서브 모델은 모델이라는 용어와 마찬가지로 임의의 인공지능 모델을 의미하며, 상하위 개 념을 구분하기 위하여 사용되는 용어로 이해될 것이다. 도 3은 본 개시의 실시 예에 따른 제1 모델(210, 도 2 참조)의 동작을 설명하기 위한 블록도이다. 도 2에서 상술한 바와 같이 제1 모델은 비디오 특징 추출부(211, 도 2 참조) 및 텍스트 특징 추출부(212, 도 2 참조)를 포함할 수 있으며, 제1 모델은 비디오 특징 추출부에 대응하여 입력된 비디오 정보에 대응하는 특징 벡터를 추출하도록 학습을 수행하는 제1 서브 모델 및 텍스트 특징 추출부에 대응하여 입력된 텍스트 정보에 대응하는 특징 벡터를 추출하도록 학습을 수행하는 제2 서브 모델을 포함할 수 있다. 실시 예에서, C3D, ResNet 등의 CNN 기반 모델, ViT, ViViT, CLIP 등의 트랜스포머(Transformer) 기반의 모델 등이 제1 서브 모델로 활용될 수 있으며, 트랜스포머 기반 모델, BERT, ALBERT, XLNet, ELECTRA, RoBERTa 등이 제2 서브 모델로 활용될 수 있다. 도 3을 참조하면, 입력된 비디오 정보에 대응하는 특징 벡터를 추출하도록 학습을 수행하는 제1 서브 모델(31 0)은 공간적 비디오 특징 추출부 및 시간적 비디오 특징 추출부를 포함할 수 있다. 공간적 비디오 특 징 추출부는 연속적으로 이어진 현재 시점의 비디오 정보(Vt) 및 미래 시점의 비디오 정보(Vt+1)를 샘플링 (sampling)하여 프레임(frame)별로 특징을 추출할 수 있다. 일 실시 예에서, 샘플링은 1FPS(Frames Per Second) 단위로 수행될 수 있으나, 이는 본 개시의 샘플링 기법을 제한하는 것은 아니며, 본 개시의 실시 예에 따른 샘플링은 다양한 기법들이 적용될 수 있다. 시간적 비디오 특징 추출부는 공간적 비디오 특징 추출부 로부터 도출된 공간적 특징을 기반으로 연속적인 공간적 특징을 시간적으로 통합할 수 있다. 이를 기반으로 제1 서브 모델은 입력된 현재 시점의 비디오 정보(Vt) 및 미래 시점의 비디오 정보(Vt+1)에 대응하여 현 재 시점의 시각 특징 벡터(VFt) 및 미래 시점의 시각 특징 벡터(VFt+1)를 도출할 수 있다. 한편, 입력된 텍스트 정보에 대응하는 특징 벡터를 추출하도록 학습을 수행하는 제2 서브 모델은 텍스트 토크나이저 및 텍스트 특징 추출부를 포함할 수 있다. 텍스트 토크나이저는 현재 시점의 텍스트 정보(Tt) 및 미래 시점의 텍스트 정보(Tt+1)에 대하여 토큰화를 수행할 수 있다. 토큰화란, 텍스트를 단어, 서브 단어, 문장 부호 등의 지정된 단위로 분절하는 과정을 의미한다. 텍스트 특징 추출부는 토큰화 결과를 기 반으로 현재 시점의 문맥 특징 벡터(TFt) 및 미래 시점의 문맥 특징 벡터(TFt+1)를 도출할 수 있다. 일 실시 예 에서, 텍스트 특징 추출부는 자연어 처리 모델인 BERT가 이용될 수 있다. 이를 기반으로 제2 서브 모델 은 입력된 현재 시점의 텍스트 정보(Tt) 및 미래 시점의 텍스트 정보(Tt+1)에 대응하여 현재 시점의 문맥 특징 벡터(TFt) 및 미래 시점의 문맥 특징 벡터(TFt+1)를 도출할 수 있다. 한편, 제1 모델은 제1 서브 모델로부터 도출된 현재 시점의 시각 특징 벡터(VFt) 및 미래 시점의 시 각 특징 벡터(VFt+1)와 제2 서브 모델로부터 도출된 현재 시점의 문맥 특징 벡터(TFt) 및 미래 시점의 문맥 특징 벡터(TFt+1)를 기반으로 현재 시점의 특징 벡터 및 미래 시점의 특징 벡터 간의 오류율을 산출하는 미래 대 조 학습 오류율 산출부를 포함할 수 있다. 실시 예에서, 미래 대조 학습 오류율 산출부는 현재 시점 의 시각 특징 벡터(VFt) 및 미래 시점의 시각 특징 벡터(VFt+1)로 구성된 제1 포지티브 쌍(positive pair), 현재 시점의 시각 특징 벡터(VFt) 및 미래 시점의 문맥 특징 벡터(TFt+1)로 구성된 제2 포지티브 쌍, 현재 시점의 문 맥 특징 벡터(TFt) 및 미래 시점의 시각 특징 벡터(VFt+1)로 구성된 제3 포지티브 쌍, 현재 시점의 문맥 특징 벡 터(TFt) 및 미래 시점의 문맥 특징 벡터(TFt+1)로 구성된 제4 포지티브 쌍 각각에 대하여 현재 시점의 특징 벡터 및 미래 시점의 특징 벡터 간의 오류율을 산출할 수 있다. 구체적으로, 실시 예에서 오류율은 복수의 입력 정보 들과 관련된 네거티브 쌍(negative pair)에 대한 각 포지티브 쌍의 비중으로 산출될 수 있다. 여기서, 복수의 입력 정보들과 관련된 네거티브 쌍이란, 제1 입력 정보에 대응하는 특징 벡터 및 제2 입력 정보에 대응하는 특 징 벡터로 구성된 쌍을 의미한다. 제1 서브 모델 및 제2 서브 모델은 미래 대조 학습 오류율 산출부로부터 산출된 오류율을 기반 으로 오류 발생을 최소화하도록 학습을 수행할 수 있다. 다시 말하면, 제1 서브 모델 및 제2 서브 모델 은 미래 대조 학습 오류율 산출부로부터 산출된 오류율을 기반으로 현재 시점의 특징 벡터 및 미래 시점의 특징 벡터 간의 오차를 최소화하도록(포지티브 쌍을 구성하는 현재 시점의 특징 벡터 및 미래 시점의 특 징 벡터가 벡터 공간 상에서 보다 가깝게 위치하고, 네거티브 쌍을 구성하는 특징 벡터들이 벡터 공간 상에서 보다 멀리 위치할 수 있도록) 제1 서브 모델 및 제2 서브 모델 각각에 대응하는 가중치(weight)를 업 데이트할 수 있다. 본 개시의 실시 예에 따르면, 현재 시점의 특징 벡터 및 미래 시점의 특징 벡터 간의 오차를 최소화하도록 제1 서브 모델 및 제2 서브 모델이 업데이트됨으로써, 현재 시점에 대응하는 데이터만 이 입력되더라도 상호 연관된 미래 시점의 정보를 포함하는 특징 벡터를 도출할 수 있다. 도 4는 본 개시의 실시 예에 따른 제2 모델(220, 도 2 참조)의 동작을 설명하기 위한 블록도이다. 도 2에서 상술한 바와 같이 제2 모델은 미래 이벤트 예측부(221, 도 2 참조), 멀티 모달 혼합부(222, 도 2 참조) 및 멀티 모달 디코더(223, 도 2 참조)를 포함할 수 있으며, 제2 모델은 미래 이벤트 예측부에 대응하여 입력된 미래 시점 정보를 포함하는 시각 특징 벡터(VFt+1) 및 미래 문맥 정보를 포함하는 문맥 특징 벡 터(TFt+1)에 대응하여 미래에 발생할 것으로 예측되는 이벤트 정보(Et+1)를 도출하도록 학습을 수행하는 제3 서브 모델 및 멀티 모달 혼합부에 대응하여 입력된 미래 시점 정보를 포함하는 시각 특징 벡터(VFt+1) 및 미래 문맥 정보를 포함하는 문맥 특징 벡터(TFt+1)를 혼합하고, 혼합된 멀티 모달 정보로부터 미래 시점 정보와 유사한 멀티 모달 특징 벡터(MFt+1)를 도출하도록 학습을 수행하는 제4 서브 모델을 포함할 수 있다. 또한, 제2 모델은 입력된 미래에 발생할 것으로 예측되는 이벤트 정보(Et+1) 및 미래 시점 정보와 유사한 멀티 모 달 특징 벡터(MFt+1)를 기반으로 미래 시점에 대한 캡션 정보(Ct+1)를 도출하도록 학습을 수행하는 캡션 정보 생 성부에 대응하는 제5 서브 모델을 더 포함할 수 있으며, 실시 예에서, RNN 기반 구조의 모델, Transformer 기반 구조 모델인 GPT, T5, LLaMa 등이 제5 서브 모델로 활용될 수 있다. 도 2에 도시된 멀티 모달 디코터(223, 도 2 참조)(도 4의 멀티 모달 디코더과 대응)는 캡션 정보 생성부에 포함될 수 있다. 도 4를 참조하면, 특징 정보에 대응하여 이벤트 정보를 도출하도록 학습을 수행하는 제3 서브 모델은 멀티 모달 특징 결합부, 미래 이벤트 선형 투영부 및 미래 이벤트 오차 측정부를 포함할 수 있다. 멀 티 모달 특징 결합부는 미래 시점의 시각 정보를 포함하는 시각 특징 벡터(VFcon) 및 미래 시점의 문맥 정 보를 포함하는 문맥 특징 벡터(TFcon)를 멀티 모달 특징으로 결합할 수 있다. 미래 이벤트 선형 투영부는 멀티 모달 특징 결합부로부터 도출된 결과에 대하여 이벤트의 개수만큼 차원을 축소할 수 있으며, 선형 투 영을 통하여 도출된 값 중 최대(max) 값을 선택하여 미래에 발생할 것으로 예측되는 이벤트 정보(Et+1)로 확인할 수 있다. 이를 기반으로 제3 서브 모델은 입력된 미래 시점의 시각 정보를 포함하는 시각 특징 정보 (VFcon) 및 미래 시점의 문맥 정보를 포함하는 문맥 특징 정보(TFcon)에 대응하여 미래에 발생할 것으로 예측되는 이벤트 정보(Et+1)를 도출할 수 있다. 한편, 미래 이벤트 오차 측정부는 미래에 발생할 이벤트 레이블과 미래에 발생할 것으로 예측되는 이벤트 정보(Et+1) 간 제1 교차 엔트로피 오차(Eevent)를 측정할 수 있다. 실시 예에서, 미래 이벤트 선형 투영부는 미래 이벤트 오차 측정부로부터 측정된 제1 교차 엔트로피 오차(Eevent)를 기반으로 학습을 수행할 수 있다. 또한, 제1 모델(210, 도 2 참조)에 포함되는 비디오 특징 추출부(211, 도 2 참조) 및 텍스트 특징 추출부(212, 도 2 참조) 또한 미래 이벤트 오차 측정부로부터 측정된 제1 교차 엔트로피 오차(Eevent)를 기반으로 학습을 수행할 수 있다. 한편, 입력된 미래 시점의 특징 정보에 대응하여 멀티 모달 특징을 도출하도록 학습을 수행하는 제4 서브 모델 은 멀티 모달 특징 추출부를 포함할 수 있다. 멀티 모달 특징 추출부는 미래 시점의 시각 정보 를 포함하는 시각 특징 벡터(VFcon) 및 미래 시점의 문맥 정보를 포함하는 문맥 특징 벡터(TFcon)에 대하여 크로 스 어텐션(Cross Attention)을 통해 모달 간 관계를 확인 및 혼합하여 미래 시점의 멀티 모달 정보를 포함하는 멀티 모달 특징 벡터(MFcon)를 추출할 수 있다. 입력된 이벤트 정보 및 멀티 모달 특징 정보를 기반으로 캡션 정보를 도출하도록 학습을 수행하는 제5 서브 모 델은 멀티 모달 디코더, 미래 캡션 오차 측정부 및 오차 합산부를 포함할 수 있다. 멀티 모달 디코더는 미래에 발생할 것으로 예측되는 이벤트 정보(Et+1) 및 미래 시점의 멀티 모달 정보를 포함하 는 멀티 모달 특징 벡터(MFcon)를 기반으로 미래 시점에 대한 캡션 정보(Ct+1)를 도출할 수 있다. 미래 캡션 오차 측정부는 미래 캡션 레이블과 미래 시점에 대한 캡션 정보(Ct+1) 간 제2 교차 엔트로피 오차(Ecaption)를 측정 할 수 있다. 오차 합산부는 미래 이벤트 오차 측정부로부터 측정된 제1 교차 엔트로피 오차(Eevent) 및 미래 캡션 오차 측정부로부터 측정된 제2 교차 엔트로피 오차(Ecaption)를 합산할 수 있다. 실시 예에서, 멀티 모탈 특징 추출부 및 멀티 모달 디코더은 오차 합산부로부터 도출되는 합산 결과를 기반으로 학습을 수행할 수 있다. 또한, 제1 모델에 포함되는 비디오 특징 추출부 및 텍스트 특징 추출부 또한 오차 합산부로부터 도출되는 합산 결과를 기반으로 학습을 수행할 수 있다. 도 5는 본 개시의 실시 예에 따른 전자 장치(10, 도 1 참조)의 동작 방법을 설명하기 위한 순서도이다. S510 단계에서, 본 개시의 실시 예에 따른 전자 장치는 제1 시점에 대응하는 비디오 정보 및 텍스트 정보를 획득할 수 있다. 실시 예에서, 전자 장치로부터 획득되는 제1 시점에 대응하는 비디오 정보 및 텍스트 정보 는 미래 예측 대상 정보로 입력되는 데이터를 의미할 수 있다. S520 단계에서, 본 개시의 실시 예에 따른 전자 장치는 제1 시점에 대응하는 비디오 정보로부터 제2 시점에 대응하는 시각 특징 정보를 추출하고, 제1 시점에 대응하는 텍스트 정보로부터 제2 시점에 대응하는 문맥 특징 정보를 추출할 수 있다. 여기서, 제2 시점은 제1 시점 이후에 도래하는 임의의 시점을 의미할 수 있다. 실시 예 에서, 시각 특징 정보 및 문맥 특징 정보를 추출함에 있어서, 입력된 비디오 정보에 대응하여 특징 벡터를 추출 하도록 학습이 수행된 제1 서브 모델 및 입력된 텍스트 정보에 대응하여 특징 벡터를 추출하도록 학습이 수행된 제2 서브 모델이 이용될 수 있다. S530 단계에서, 본 개시의 실시 예에 따른 전자 장치는 S520 단계에서 도출된 시각 특징 정보 및 문맥 특징 정보를 기반으로 미래에 발생할 이벤트 정보를 도출할 수 있다. 실시 예에서, 미래에 발생할 이벤트 정보를 도출함에 있어서, 입력된 특징 정보에 대응하여 이벤트 정보를 도출하도록 학습이 수행된 제3 서브 모델이 이용될 수 있다. S540 단계에서, 본 개시의 실시 예에 따른 전자 장치는 S520 단계에서 도출된 시각 특징 정보 및 문맥 특징 정보를 기반으로 멀티 모달 특징 정보를 도출할 수 있다. 실시 예에서, 멀티 모달 특징 정보를 도출함에 있어서, 시각 특징 정보 및 문맥 특징 정보를 혼합하고, 혼합된 멀티 모달 정보로부터 멀티 모달 특징을 추출하 도록 학습이 수행된 제4 서브 모델이 이용될 수 있다. S550 단계에서, 본 개시의 실시 예에 따른 전자 장치는 S530 단계로부터 도출된 미래에 발생할 이벤트 정보 및 S540 단계에서 도출된 멀티 모달 특징 정보를 기반으로 캡션 정보를 생성하고, 생성된 캡션 정보를 외부로 제공할 수 있다. 실시 예에서, 캡션 정보를 도출함에 있어서, 이벤트 정보 및 멀티 모달 특징 정보를 기반으로 캡션 정보를 도출하도록 학습이 수행된 제5 서브 모델이 이용될 수 있다. 한편, 도 5에 도시되지 않았으나, 본 개시의 실시 예에 따른 전자 장치는 제1 서브 모델 및 제2 서브 모델 로부터 도출되는 제1 시점의 시각 특징 정보, 제1 시점의 문맥 특징 정보, 제2 시점의 시각 특징 정보 및 제2 시점의 문맥 특징 정보에 대한 대조 학습 오류율을 산출할 수 있다. 제1 서브 모델 및 제2 서브 모델은 대조 학 습 오류율을 기반으로 재학습(retraining)(또는, 업데이트)을 수행할 수 있다. 또한, 본 개시의 실시 예에 따른 전자 장치는 제4 서브 모델로부터 도출되는 미래 이벤트 오차 및 제5 서브 모델로부터 도출되는 미래 캡션 오차를 기반으로 재학습이 수행될 수도 있다. 한편, 본 명세서에 개시된 실시 예들은 컴퓨터에 의해 실행 가능한 명령어를 저장하는 기록매체의 형태로 구현 될 수 있다. 명령어는 프로그램 코드의 형태로 저장될 수 있으며, 프로세서에 의해 실행되었을 때, 프로그램 모 듈을 생성하여 개시된 실시 예들의 동작을 수행할 수 있다. 기록매체는 컴퓨터로 읽을 수 있는 기록매체로 구현 될 수 있다. 컴퓨터가 읽을 수 있는 기록매체는 컴퓨터에 의하여 해독될 수 있는 명령어가 저장된 모든 종류의 기록 매체를 포함할 수 있다. 예를 들어, ROM, RAM, 자기테이프, 자기디스크, 플래시 메모리, 광 데이터 저장 장치 등이 있을 수 있다. 상술된 내용은 본 개시를 실시하기 위한 구체적인 실시 예들이다. 본 개시는 상술된 실시 예들뿐만 아니라, 단 순하게 설계 변경되거나 용이하게 변경할 수 있는 실시 예들 또한 포함할 것이다. 또한, 본 개시는 상술된 실시 예들을 이용하여 용이하게 변형하여 실시할 수 있는 기술들도 포함할 것이다. 따라서, 본 개시의 범위는 상술된 실시 예들에 국한되어 정해져서는 안 되며 후술하는 특허청구범위뿐만 아니라 본 개시의 특허청구범위와 균등한 것들에 의해 정해져야 할 것이다."}
{"patent_id": "10-2023-0185630", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 실시 예에 따른 전자 장치를 설명하기 위한 블록도이다. 도 2는 본 개시의 실시 예에 따른 전자 장치의 아키텍처를 설명하기 위한 블록도이다. 도 3은 본 개시의 실시 예에 따른 제1 모델의 동작을 설명하기 위한 블록도이다. 도 4는 본 개시의 실시 예에 따른 제2 모델의 동작을 설명하기 위한 블록도이다. 도 5는 본 개시의 실시 예에 따른 전자 장치의 동작 방법을 설명하기 위한 순서도이다."}
