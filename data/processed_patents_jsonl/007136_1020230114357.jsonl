{"patent_id": "10-2023-0114357", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0032068", "출원번호": "10-2023-0114357", "발명의 명칭": "모터의 고장에도 빠르게 대응해 보행할 수 있는 보행 로봇을 위한 강화학습 방법", "출원인": "고려대학교 산학협력단", "발명자": "박성현"}}
{"patent_id": "10-2023-0114357", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 프로세서(processor)를 포함하는 컴퓨팅 장치에 의해 수행되는 n(n은 2 이상의 자연수)-족 보행 로봇의보행 학습 방법에 있어서,조건부 벡터(conditioned vector)에 대응하는 앵커 포인트(anchoring points)를 생성하는 단계;상기 앵커 포인트에 대응하는 패스(path)를 생성하는 단계;상기 패스에 대응하는 관절 궤적(joint trajectory)에 대한 시뮬레이션을 수행하는 단계;시뮬레이션 결과에 기초하여, 관절 궤적에 대한 보상을 획득하는 단계; 및획득된 보상에 기초하여 보행 정책(walking policy)을 업데이트하는 단계를 통해 보행 정책을 학습하는,n-족 보행 로봇의 보행 학습 방법."}
{"patent_id": "10-2023-0114357", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 n-족 보행 로봇의 보행 학습 방법은 상기 n-족 보행 로봇의 노멀 상태(normal conditions)에서 수행되는,n-족 보행 로봇의 보행 학습 방법."}
{"patent_id": "10-2023-0114357", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 보행 정책의 학습은 궤적 기반 강화학습 기법(joint-based reinforcement learning method)에 의해 수행되는,n-족 보행 로봇의 보행 학습 방법."}
{"patent_id": "10-2023-0114357", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 내지 제3항 중 어느 하나의 항에 의해 생성된 보행 정책을 포함하는 보행 모델이 탑재된 n-족 보행 로봇의 보행 방법에 있어서,상기 n-족 보행 로봇에 포함된 적어도 하나의 다리(leg)에 이상이 발생한 이상 상태(abnormal conditions)에서,관절 궤적 공간을 나타내는 액션 공간(action space)에 대한 지식을 추출하는 단계;상기 액션 공간에서의 탐색 공간을 축소하는 단계;임의의 조건 벡터에 대응하는 앵커 포인트를 생성하는 단계;생성된 앵커 포인트에 대응하는 패스를 생성하는 단계; 및생성된 패스에 대응하는 과전 궤적에 대응하는 제어 신호를 생성하는 단계를 포함하는 n-족 보행 로봇의 보행방법."}
{"patent_id": "10-2023-0114357", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 탐색 공간을 축소하는 단계는 상기 탐색 공간의 차원을 축소하는,n-족 보행 로봇의 보행 방법.공개특허 10-2025-0032068-3-"}
{"patent_id": "10-2023-0114357", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "n(n은 2 이상의 자연수)-족 보행 로봇의 보행 학습 방법 및 상기 보행 학습 방법을 통해 생성된 보행 정책을 포 함하는 보행 모델이 탑재된 n-족 보행 로봇의 보행 방법이 개시된다. 상기 보행 학습 방법은 적어도 프로세서 (processor)를 포함하는 컴퓨팅 장치에 의해 수행되고, 조건부 벡터(conditioned vector)에 대응하는 앵커 포인 트(anchoring points)를 생성하는 단계, 상기 앵커 포인트에 대응하는 패스(path)를 생성하는 단계, 상기 패스에 대응하는 관절 궤적(joint trajectory)에 대한 시뮬레이션을 수행하는 단계, 시뮬레이션 결과에 기초하여, 관절 궤적에 대한 보상을 획득하는 단계, 및 획득된 보상에 기초하여 보행 정책(walking policy)을 업데이트하는 단계 를 통해 보행 정책을 학습한다."}
{"patent_id": "10-2023-0114357", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 로봇의 보행에 적용할 수 있는 소프트웨어 기술에 관한 것으로, 보다 상세하게는 모터의 고장과 같은 보행 로봇의 장애에도 불구하고 빠르게 대응해 보행할 수 있는 보행 로봇을 위한 강화학습 기반 알고리즘에 관 한 것이다."}
{"patent_id": "10-2023-0114357", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "보행 제어기(walking controllers)와 같은 로봇 분야에서의 진보는 Spot-Mini나 ANYmal-C와 같은 보행 로봇 (legged robots)이 늪(swamps)이나 얼음으로 뒤덮인 경사면(icy slopes)과 같은 극한의 환경에서 효율적으로 활 용되도록 하였다. 예컨대, 보행 로봇은 다재다능하며(versatile) 수색 및 구조(search-and-rescue), 탐색 (exploration), 및 검역(inspection)을 포함하는 다양한 미션(missions)에 대한 충분한 가능성을 가지고 있다. 극한 환경에서 보행 로봇의 강건성(robustness)을 확보하기 위하여, 많은 연구들은 내구성 있는 물질(durable materials)을 이용하거나 보행 제어기(walking controllers)를 디자인하는 데에 집중하고 있다. 그러나, 예측하 지 못한 사고로 인해 장애가 발생하는 하드웨어에 적응하는 기법에 대한 연구는 이루어 지지 않고 있다. 예를 들어, 기존의 보행 제어기는 장애를 바로잡기 위한 복구 모드(rocovery mode)로 복귀하는 데에는 만족할만한 성 능을 보인다. 그러나, 하나 이상의 다리(one or more legs)에 대한 제어가 상실된 후 보행을 지속하는 능력은 부족하다. 탐색 및 구조 동작과 관련되는 상황에서, 로봇은 지속적으로 보호하고(preserve) 동작하여야만 한다. 반면에, 자연계의 생물체(nature's creatures)는, 팔다리의 절단(limb amputation)이나 감각의 마비(sensory paralysis)와 같은 예측하지 못한 변화가 발생했을 때, 행동하는 법을 알고 있다. 어떤 생물체들은 자절 (autotomy) 이후에 빠르게 탈출할 수 있고, 이는, 생물체가 위험한 상황에 직면했을 때, 팔다리나 꼬리의 자발 적인 방출(volutary release)을 의미한다. 예를 들어, 8 개의 다리를 가진 절지동물(arthropod)인 장님거미 (harvestman)는 보행에 대한 이전의 지식을 이용함으로써 다리를 잃은지 하루 내에 효율적으로 걷는 법을 배울 수 있다. 이러한 맥락에서, 본 발명은, 예측하지 못한 환경에 직면했을 때, 빠르게 보행에 적응하고 유지하기 위한 기법 을 제안한다. 즉, 본 발명에서는, 새로운 하드웨어 설정(configuration)이나 주변 환경의 변화에 적응하기 위해, 이전의 보행 경험(locomotion experiences)을 효율적으로 이용하게 하는 알고리즘(a Learning Rapid Adaptation (LRA) algorithm)을 제안한다. 이전의 학습 경험을 이용함으로써, 보행 로봇의 회복력(resilienc e)과 강건성(robustness)을 향상시킬 수 있다. 이에 따라, 보행 로봇은 예측하지 못한 환경에서 태스크를 보다 적절하게 수행할 수 있다. 로봇이 수정 모드(modification mode)로 동작하는 대신에, 보행 로봇의 보행 패턴에 빠르게 적응하도록 함으로써, 지속적으로 동작할 수 있다. 선행기술문헌 비특허문헌 (비특허문헌 0001) Miki, T., Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V., Hutter, M. . Learning robust perceptive locomotion for quadrupedal robots in the wild. Science Robotics, 7, eabk2822. (비특허문헌 0002) Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V., Hutter, M. . Learning quadrupedal locomotion over challenging terrain. Science robotics, 5, eabc5986. (비특허문헌 0003) M. Tranzatto, M. Dharmadhikari, L. Bernreiter, M. Camurri, S. Khattak, F. Mascarich, P. Pfreundschuh, D. Wisth, S. Zimmermann, M. Kulkarni, V. Reijgwart, B. Casseau, T. Homberger, P. De Petris, L. Ott, W. Tubby, G. Waibel, H. Nguyen, C. Cadena, R. Buchanan, L. Wellhausen, N. Khedekar, O. Andersson, L. Zhang, T. Miki, T. Dang, M. Mattamala, M. Montenegro, K. Meyer, X. Wu, A. Briod, M. Mueller, M. Fallon, R. Siegwart, M. Hutter, K. Alexis, Team CERBERUS Wins the DARPA Subterranean Challenge: Technical Overview and Lessons Learned , (available at http://arxiv.org/abs/2207.04914). (비특허문헌 0004) Montgomery, D. . D. Montgomery, “The Pentagon’s $82 Million Super Bowl of Robots”, The Washington Post Magazine , https://www.washingtonpost.com/magazine/2021/11/10/darpa-robot-competition/."}
{"patent_id": "10-2023-0114357", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "로봇이 구조 작업 등을 수행하기 위해서는, 고장이 발생하였음에도 불구하고 목적을 수행할 수 있어야 한다. 종 래 기술을 활용하면 다양한 지형에 대해 효과적으로 걸을 수 있지만, 고장이 난 이후의 상황은 고려하지 않는다. 실제로 장애가 발생한다면, 종래 기술의 경우 처음부터 로봇이 다시 출발해야 한다. 본 발명을 통해 고 장이 난 경우 처음부터 출발하는 것이 아닌, 효율적인 탐색을 통해 바뀐 상황에 대해 빠르게 적응해 이동성을 유지할 수 있다."}
{"patent_id": "10-2023-0114357", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 n(n은 2 이상의 자연수)-족 보행 로봇의 보행 학습 방법은 적어도 프로세서 (processor)를 포함하는 컴퓨팅 장치에 의해 수행되고, 조건부 벡터(conditioned vector)에 대응하는 앵커 포인 트(anchoring points)를 생성하는 단계, 상기 앵커 포인트에 대응하는 패스(path)를 생성하는 단계, 상기 패스 에 대응하는 관절 궤적(joint trajectory)에 대한 시뮬레이션을 수행하는 단계, 시뮬레이션 결과에 기초하여, 관절 궤적에 대한 보상을 획득하는 단계, 및 획득된 보상에 기초하여 보행 정책(walking policy)을 업데이트하 는 단계를 통해 보행 정책을 학습한다. 또한, 본 발명의 일 실시예에 따른 n-족 보행 로봇의 보행 방법은 상기 보행 학습 방법에 의해 생성된 보행 정 책을 포함하는 보행 모델이 탑재된 n-족 보행 로봇의 보행 방법으로써, 상기 n-족 보행 로봇에 포함된 적어도 하나의 다리(leg)에 이상이 발생한 이상 상태(abnormal conditions)에서, 관절 궤적 공간을 나타내는 액션 공간 (action space)에 대한 지식을 추출하는 단계, 상기 액션 공간에서의 탐색 공간을 축소하는 단계, 임의의 조건 벡터에 대응하는 앵커 포인트를 생성하는 단계, 생성된 앵커 포인트에 대응하는 패스를 생성하는 단계, 및 생성 된 패스에 대응하는 과전 궤적에 대응하는 제어 신호를 생성하는 단계를 포함한다."}
{"patent_id": "10-2023-0114357", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "비슷한 종래 기술은 주변 환경의 변화에만 적용 가능하지만, 본 발명은 로봇 자체의 결함이 생겼을 때도 빠르게 상황에 적응해 보행성을 유지할 수 있다. 본 발명의 기술을 활용하면 응급한 구조 작업에 사용되는 로봇 혹은 무슨 일이 있어도 물건을 목적지로 운반해야 하는 군용 로봇 등에서 효과적으로 사용 가능하다. 재난 상황에서 고장이 나거나 지뢰를 밟아 다리가 고장나는 상황 속에서도 효율적인 탐색을 통해 본래의 목적을 수행할 수 있 기 때문이다."}
{"patent_id": "10-2023-0114357", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에 개시되어 있는 본 발명의 개념에 따른 실시예들에 대해서 특정한 구조적 또는 기능적 설명들은 단 지 본 발명의 개념에 따른 실시예들을 설명하기 위한 목적으로 예시된 것으로서, 본 발명의 개념에 따른 실시예 들은 다양한 형태들로 실시될 수 있으며 본 명세서에 설명된 실시예들에 한정되지 않는다. 본 발명의 개념에 따른 실시예들은 다양한 변경들을 가할 수 있고 여러 가지 형태들을 가질 수 있으므로 실시예 들을 도면에 예시하고 본 명세서에서 상세하게 설명하고자 한다. 그러나, 이는 본 발명의 개념에 따른 실시예들 을 특정한 개시 형태들에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경,균등물, 또는 대체물을 포함한다. 제1 또는 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어 들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으로 만, 예컨대 본 발명의 개념에 따른 권리 범위로부터 벗어나지 않은 채, 제1 구성 요소는 제2 구성 요소로 명명 될 수 있고 유사하게 제2 구성 요소는 제1 구성 요소로도 명명될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성 요소가 존재할 수도 있다고 이해되어야 할 것이다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성 요소들 간의 관계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃하는\"과 \"~에 직접 이웃하는\" 등 도 마찬가지로 해석되어야 한다. 본 명세서에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로서, 본 발명을 한정하려는 의 도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 본 명세서에 기재된 특징, 숫자, 단계, 동작, 구성 요소, 부분품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구 성 요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적 으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미를 갖는 것으로 해석되어야 하며, 본 명세서에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의 미로 해석되지 않는다. 이하, 본 명세서에 첨부된 도면들을 참조하여 본 발명의 실시예들을 상세히 설명한다. 그러나, 특허출원의 범위 가 이러한 실시예들에 의해 제한되거나 한정되는 것은 아니다. 각 도면에 제시된 동일한 참조 부호는 동일한 부 재를 나타낸다. 본 발명은, 과거의 경험(past experiences)으로부터 도출된 이전 지식(prior knowledge)을 이용하여, 하드웨어 고장과 같은 예측하지 못한 변화에 빠르게 적응할 수 있는 보행 제어기(walking controller)를 위한 방법(또는 알고리즘)의 개발을 목표로 한다. 도 1은 본 발명의 일 실시예에 따른 보행 로봇의 학습 방법을 설명하기 위한 개념도이다. 도 1에 도시된 바와 같이, 제안하는 방법은 두 개의 구성을 포함한다. 첫째, 본 발명에서는 예측하지 못한 변화가 발생하지 않는 노멀 상태(normal conditions) 하에서의 에이전트 (agent, 로봇, 로봇의 보행 제어기, 또는 모델 등으로써, 강화학습을 주행하는 주체를 의미할 수 있음)을 학습 시키기 위해 궤적 기반 강화학습 기법(trajectory-based reinforcement learning method)을 채용할 수 있다. 보행 정책(walking policy)은 (고정-길이의) 관절 궤적(fixed-length joint trajectories)에 대한 분포 (distribution)를 정의한다. 관절 궤적 공간(joint trajectory space)을 행위 공간(action space)으로 이용하 여, 에이전트(agent)는 점진적으로, 롤아웃(rollouts)과 보상 획득(reward acquisition)의 주기(cycle)를 통해, 지속적으로 더 높은 보상을 나타내는 관절 궤적을 형성한다. 또한, 시뮬레이션 환경(simulation environment)과 실제 환경(real-world environment) 모두에서의 학습을 단일의 루프로 연결하는(links) 과정이 추가될 수도 있다. 이는 두 환경의 불일치를 최소화하기 위함이다. 둘째, 본 발명에서는, 제1 스테이지(노멀 상태에서의 학습 과정) 동안에 잘 학습된 에이전트에 의해 추출된 지 식을 증류한다(distill). 이와 같이 증류된 지식은 예측하지 못한 이벤트가 발생했을 때 효율적인 탐색 (exploration)을 위해 이용된다. 이를 용이하게 하기 위하여, 본 발명에서는, 효율적인 탐색에 적합한 정제된 관절 궤적 공간(refined joint trajectory space)을 생성하는 인코더-디코더 구조(encoder-decoder architecture)를 갖는 신경망(neural network)을 학습할 수 있다. 이러한 신경망의 도움으로, 에이전트는, 예측 하지 못한 이벤트가 발생했을 때, 제로-샷(zero-shot) 기법으로, 효율적인 탐색을 수행할 수 있다. 보행 행위의 학습(Learning locomotive behaviors) 효율성을 증대시키기 위하여, 보행(gaits)의 주기적인 특성(periodic nature)을 이용할 수 있다. 본 발명에서 제안하는 보행 행위(locomotive behaviors)는, 설정가능한 하이퍼파라미터(configurable hyperparameter)인, 시간 구간(time interval) 동안의 관절 궤적의 분포를 결정한다. 다시 말하면, 보행 정책(walking policy)은 각 각의 미리 결정된 주기(period) 내에서 관절 궤적을 따르는 이동(movements)으로부터 획득된 보상에 따라 업데 이트될 수 있다. 궤적-기반 강화학습(Trajectory-based reinforcement learning) 제안하는 보행 제어기(walking controller)는 두 가지의 핵심 요소, 즉 앵커 생성기(anchor generator, 제1 생 성기로 명명될 수 있음)와 가우시안 랜덤 패스 생성기(Gaussian random path generator, 제2 생성기로 명명될 수 있음)로 구성될 수 있다. CVAE(Conditional-Variational-Auto-Encoder)로 구현될 수 있는 앵커 생성기는 조건부 벡터(conditioned vector)에 기초하여 구별되는 출력들(distinct outputs)을 생성한다. 조건부 벡터는, 예컨대 전방(forward), 좌측(left), 또는 우측(right)으로의, 방향을 결정하는 다차원(예컨대, 3차원)의 원-핫 벡터(one-hot vector s)를 포함할 수 있다. 예컨대, 조건부 벡터가 [0,1,0]이면, 앵커 생성기는, 로봇을 전방으로 이동하게 하는 앵 커 포인트(anchoring points)를 생성한다. 이러한 구조를 통해, 동일한 목표가 주어지더라도, 다양한 궤적이 샘 플링될 수 있다. 가우시안 랜덤 패스 생성기는, 가우시안 랜덤 패스(S. Choi, K. Lee, S. Oh, \"Gaussian random paths for real-time motion planning\" in 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE, Daejeon, South Korea, 2016; http://ieeexplore.ieee.org/document/7759237/), pp. 1456- 1461.)로 알려진 스위프트 패스 샘플링 방법(swift path sampling method)을 채용할 수 있다. 이는, 가우시안 프로세스 회귀(Gaussian process regression)를 이용하여, 생성된 앵커 포인트를 통과하는 다양한 패스(path s)를 생성하도록 한다. 이용되는 커널 함수(kernel function)의 유형에 의존하여, 앵커 포인트를 지나가거나 또 는 앵커 포인트를 어느 정도 빗겨 이동하여야 하는지를 결정할 수 있다. 예시적으로, 보행 정책은 MuJoCo 시뮬레이션(E. Todorov, T. Erez, Y. Tassa, \"MuJoCo: A physics engine for model-based control\" in 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, Vilamoura-Algarve, Portugal, 2012; http://ieeexplore.ieee.org/document/6386109/), pp. 5026- 5033)을 이용하여 학습될 수 있다. 앵커(anchor)와 보상(reward)을 버퍼에 적층(stacking)한 후, 수집된 데이터 는, DLPG(Deep Latent Policy Gradient, Kingma, D. P., Welling, M. . Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114. Retrieved from http://arxiv.org/abs/1312.6114) 알고리즘을 이용 하여, 정책을 학습하기 위해 이용된다. 학습 과정에서 이용되는 보상 함수는 수학식 1과 같다. [수학식 1]"}
{"patent_id": "10-2023-0114357", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "상기 수학식 1의 보상 함수는 각 타임스템(timestep(dt))마다 계산되는 함수로, xafter는 일정 시간(dt) 이후의 위치, xbefore는 일정 시간 이동 이전의 위치를 나타낸다. 따라서, xafter-xbefore는 일정 시간 동안 앞으로 이동한 거리를 나타낸다. 여기서, 일정 시간을 의미하는 dt는 로봇이 다음 관절로의 이동하는데 걸린 시간이며 50Hz로 로봇을 제어할 시 0.02s를 의미할 수 있다. α는 토크(torque)에 대한 패널티를 얼마나 줄지와 관련된 스케일 (scale) 값을 나타낸다. 이는 실험 등에 따라 여러 값을 가질 수 있으며, 사전에 미리 정해진 값을 가질 수도 있다. 는 각 관절에 걸리는 토크(torque) 제곱의 총합으로, 보상 함수에 감쇄하여 반영해 준다면 로봇 제어 의 부드러움을 향상시킬 수 있다. rsurvive는 보행 로봇이 뒤집어 졌는지 여부를 나타내는 변수로, 보행 로봇이 뒤 집어 지지 않았다면 1의 값을, 보행 로봇이 뒤집어 졌다면 0의 값을 갖는다. 이는 보행 로봇이 걸을 때 뒤집어 지지 않고 똑바로 걸을 수 있도록 도와주는 항이다. 로봇의 롤(roll) 방향 각도가 일정 각도보다 커지면 뒤집어 진다고 판단할 수 있다. 상술한, 보상 함수에 사용되는 모든 부분은 시뮬레이션을 통해 얻을 수 있다. 예를 들 어, 시뮬레이션 내 로봇의 위치, 각 관절당 작용하는 토크 등이 행렬 값으로 얻어질 수 있다. 실제의 로봇을 사 용할 때는, 앞서 설명한 외부 카메라를 통해 얻어진 위치(position) 값을 사용할 수 있다. 위치 추적(Position tracking) 실제 환경에서 로봇의 위치를 추적하기 위하여, AprilTag(E. Olson, \"AprilTag: A robust and flexible visual fiducial system\" in 2011 IEEE International Conference on Robotics and Automation (IEEE, Shanghai, China, 2011;http://ieeexplore.ieee.org/document/5979561/), pp. 3400-3407.)와 RealSense D435와 같은 (외 부) 카메라를 이용할 수 있다. 본 발명에서는 TPS(Thin Plate Spline, F. L. Bookstein, Principal warps: thin-plate splines and the decomposition of deformations. IEEE Trans. Pattern Anal. Machine Intell. 11, 567-585 .) 알고리즘을 이용하는 파이프라인을 제안한다. 이는 카메라를 인스톨(installing)한 후 단 일의 셋업으로 추적을 가능케 하며, 카메라 위치를 주기적으로 조정할 필요가 없도록 한다. 이러한 파이프라인 은 또한 카메라의 앵글(angle)이나 위치(position)를 상세하게 명세(specify)할 필요가 없다. 프로세스는 카메 라 픽셀 공간(camera pixel space) 내의 5×5 전방 그리드(forward grid) 내의 픽셀을 선택하고 카메라 좌표 (camera coordinates) 내에서의 각 픽셀의 맵핑을 조사(investigating)함으로써 시작된다. 그런 다음, 각 포인 트에 의해 형성된 플레인(plane)의 근사(approximation)를 생성하고, 픽셀과 근사화된 플레인(approximated plane) 사이의 맵핑 함수(mapping function)를 추출하기 위하여 TPS 알고리즘을 이용한다. 마지막으로, 이전에 결정된 맵핑 함수를 이용하여 전역 좌표계 시스템(global coordinate system) 내에서의 로봇의 이동 궤적을 결 정하고, AprilTag를 사용하여 획득된 로봇의 인트라-픽셀 이동 궤적(intra-pixel movement trajectory)을 결정 한다. 시뮬레이션-투-리얼 모듈(Sim-to-real module) 실제 데이터(real-world data)는 sim2real module을 학습하기 위해 요구된다. 그러나, 시뮬레이션 내의 모든 궤 적을 물리적인 로봇에 적용하기는 힘들다. 따라서, DPP(Determinant Point Process, A. Kulesza, B. Taskar, Determinantal point processes for machine learning. FNT in Machine Learning. 5, 123-286 .) 알고 리즘을 이용하는 샘플러(sampler)를 이용할 수 있다. 샘플링된 궤적을 이용해 물리적인 로봇에 적용 후 시뮬레 이션과 실제 로봇의 움직임이 얼마나 차이나는지를 관측(또는 산출)한다. 샘플링된 궤적과 관측된 차이 값을 데 이터로 활용하여 인공지능 신경망인 sim-to-real module을 학습할 수 있다. 예시적인 인공지능 신경망은 다층 퍼셉트론(Multi-Layer Perceptron, MLP)일 수 있으며, 다층 퍼셉트론은 샘플링된 궤적 및/또는 관측된 차이 값 을 입력으로 받아 수정된 앵커 포인트를 출력(또는 추론)할 수 있다. 이에 따라 sim-to-real module을 활용한 보행기를 더욱 강건하게 만들 수 있다. 제로-샷 적응(Zero-shot adaptation) 지식 증류(knowledge distillation)의 프로세스는 잘 훈련된 에이전트의 궤적에 의해 주도된다. 갑작스런 변화 (alterations)의 이벤트의 경우에, 에이전트는, 임의의 관절 궤적 공간 대신에, 증류된 지식으로부터 도출된 공 간의 특정 부분을 이용하고, 이는 이전 경험(previous experiences)을 포함한다. 적절한 탐색 공간(search space) 내에서의 효율적인 탐색(exploration)은 에이전트 내에서의 제로-샷 적응을 가능케 한다. 보행 지식 추출(Locomotive-knowledge extraction) 지식을 증류하기 위하여, 첫째로 지식을 확보하는 것이 필요하다. 본 발명에서는 관절 궤적 공간을 나타내는 액 션 공간(action space)을 로봇의 지식으로써 이용한다. 제안하는 보행 제어기(walking controller)는 용이하게 궤적을 지식으로 이용할 수 있다. 명령(command)이 주어졌을 때, 하나의 궤적이 아닌 다양한 궤적을 도출하기 때문이다. 모든 관절 궤적은, 관절의 유형과 무관하게, 지식을 생성하기 위해 이용된다. 보행-지식 증류(Locomotive-knowledge distillation) 예상치 못한 이벤트에 대응하여 추출된 지식을 바로 적용하는 것은 부작용을 낳을 수 있다. 탐색 공간(search space)의 과제약 특성(over-constrained nature) 때문이다. 효과적인 탐색을 위해서, 본 발명에서는, 탐색 공간 축소(search space reduction)와 지식 최대화(knowledge maximization) 사이의 균형을 최적화하는 과정을 제안 한다. 이는 앵커의 차원을 축소함으로써 탐색 공간(search space)을 확장하는 것과 커널 함수를 이용하여 GRP(Gaussian Random Path)를 적용하는 것을 포함한다. 추가적으로, 상술한 지식을 VAE(Variational Autoencoder)의 입력으로 이용함으로써, 지식을 도출할 수 있다. 도 2는 도 1의 학습 방법을 설명하기 위한 흐름도이다. 도 2에 의한 학습 방법을 설명함에 있어, 앞선 기재와 중복되는 내용에 관하여는 그 구체적인 기재를 생략하기로 한다. 도 2를 참조하면, 보행 로봇의 학습 방법, 보행 로봇의 보행 모델 생성 방법, 보행 로봇의 보행 모델 학습 방법, 및 보행 로봇의 제어 방법 등으로 명명될 수도 있는 학습 방법은 보행 로봇 자체 및/또는 컴퓨팅 장치에 의해 수행될 수 있다. 보행 로봇은 n(n은 2 이상의 자연수)-족의 보행 로봇을 의미할 수 있으며, 컴퓨팅 장치는 적어로 프로세서(processor) 및/또는 메모리(memory)를 포함할 수 있다. 또한, 컴퓨팅 장치는 PC(PersonalComputer), 서버(server), 랩탑 컴퓨터, 태블릿 PC 등을 포함할 수 있다. 우선, 보행 로봇의 보행 정책(walking policy)이 학습된다(S110). 이때, 궤적 기반 강화학습 기법이 이용될 수 있다. 또한, S110 단계의 학습은 보행 로봇에 예상치 못한 장애나 고장이 발생하지 않은 상태인 노멀 상태 (normal conditions)에서의 학습을 의미할 수 있다. 보행 정책은 (고정-길이의) 관절 궤적에 대한 분포를 정의 한다. 특히, 관절 궤적 공간을 행위 공간(action space)으로 이용하여, 롤아웃과 보상 획득의 주기 통해, 지속 적으로 더 높은 보상을 나타내는 관절 궤적을 형성할 수 있다. 이를 위해, 보행 제어기의 앵커 생성기는 임의의 조건 벡터(또는 임의의 명령에 대응하는 조건 벡터)를 기초로 앵커 포인트를 생성하고, 가우시안 랜덤 패스 생성기는 생성된 앵커 포인트에 대응하는 적어도 하나의 패스(또 는 적어도 하나의 패스에 대응하는 관절 궤적) 및/또는 관절 궤적을 생성할 수 있다. 또한, 각 앵커(적어도 하 나의 패스, 또는 관절 궤적)에 대한 보상을 시뮬레이션을 통해 산출될 수 있다. 결국, 에이전트(보행 제어기 또 는 보행 제어기를 제어하는 제어 알고리즘을 의미할 수 있음)는 도출되는 보상에 기초하여 업데이트될 수 있다. 예컨대, 가장 높은 보상을 보이는 관절 궤적이 보행 정책으로 포함될 수 있다. 상술한 S110 단계는 복수회 반복 수행될 수 있다. 또한, S110 단계에서는 생성된 패스에 대응하는 시뮬레이션을 수행하고 보상을 산출하는 동작이 포함되기 때문에, 컴퓨팅 장치의 동작을 필요로 할 수 있다. 다시 말하면, S110 단계에서는 보행 로봇이 아닌 보행 로봇의 제어를 위한 제어 모델(보행 제어기의 제어 알고리즘이나 제어 모델을 의미할 수 있음) 자체를 학습시키기 위한 구성으로 이해될 수도 있다. 다음으로, 시뮬레이션-투-리얼 모듈이 학습될 수 있다(S120). 구체적으로, 앵커 포인트와 보상 쌍들에 대한 샘 플링이 수행된다. 즉, 앵커 포인트와 보상 쌍들 중 적어도 일부만이 시뮬레이션-투-리얼 모듈의 학습에 이용될 수 있다. 샘플링된 앵커 포인트와 이에 대응하는 관절 궤적이 보행 로봇에 적용된다. 보행 로봇은 앵커 포인트 와 이에 대응하는 관절 궤적에 따라 이동할 수 있고, 이에 따른 이동의 결과 및 궤적들은 (외부) 카메라로부터 획득될 수 있다. 카메라에 기초하여 획득된 데이터(즉, 실제 데이터)와 시뮬레이션 결과의 차이점이 스코어로 계산될 수 있다. 계산된 스코어에 따라 앵커 포인트 및/또는 보상은 업데이트될 수 있다. 상술한 S120 단계는, 실시예에 따라, 생략될 수도 있다. 마지막으로, 보행 로봇에 이상이 발생한 상태, 즉 이상 상태(abnormal conditions)에서의 학습이 수행된다 (S130). 이상 상태라 함은 보행 로봇에 포함되는 n 개의 다리 중 적어도 하나가 소실되거나 적어도 하나의 기능 에 이상이 발생한 경우를 의미할 수 있다. 구체적으로, 보행 지식이 추출된다. 예컨대, 관절 궤적 공간을 나타내는 액션 공간이 지식으로 추출될 수 있다. 예시적으로, 명령에 대한 관절 궤적들이 지식으로 추출될 수 있고, 이러한 관절 궤적들은 S110 단계의 학습을 마친 에이전트(즉, 보행 제어기)에 의해 생성되어 추출될 수 있다. 추출된 보행 지식에 대한 증류 동작이 수행된다. 이를 위해, 탐색 공간이 축소될 수 있다. 탐색 공간의 축소는 탐색 공간의 차원 축소를 의미할 수 있다. 그런 다음, 축소된 탐색 공간에서 조건부 벡터에 대한 앵커 포인트가 생성되고, 생성된 앵커 포인트에 대응하는 가우시안 랜덤 패스 및/또는 관절 궤적이 생성될 수 있다. 관절 궤적이 생성된 경우, 관절 궤적에 대응하는 제어 신호가 생성되고, 제어 신호는 각 관절의 움직임을 제어 하는 구동부로 전달되어 각 관절에 대한 움직임이 생성될 수 있다. 실시예에 따라, S130 단계는 S110 단계(S120 단계를 포함할 수 있음)을 통해 생성된 보행 보델(또는 제어 모델)이 탐재된 n-족 보행 로봇의 보행 방법이나 n-족 보행 로봇의 제어 방법으로 이해될 수도 있다. 이상에서 설명된 장치는 하드웨어 구성 요소, 소프트웨어 구성 요소, 및/또는 하드웨어 구성 요소 및 소프트웨 어 구성 요소의 집합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성 요소는, 예를 들어, 프로세서, 콘트롤러, ALU(Arithmetic Logic Unit), 디지털 신호 프로세서(Digital Signal Processor), 마이크 로컴퓨터, FPA(Field Programmable array), PLU(Programmable Logic Unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(Operation System, OS) 및 상기 운영 체제 상에서 수행되는 하나 이상의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응 답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사 용되는 것으로 설명된 경우도 있지만, 해당 기술 분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처 리 요소(Processing Element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서(Parallel Processor)와 같은, 다른 처리 구성(Processing Configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(Computer Program), 코드(Code), 명령(Instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (Collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처 리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성 요소(Component), 물리적 장치, 가상 장치(Virtual Equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(Signal Wave)에 영구적으로, 또는 일시적으로 구체화(Embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 하나 이상의 컴퓨터 판독 가능 기록 매 체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계 되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가 능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD- ROM, DVD와 같은 광기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기-광 매체 (Magneto-optical Media), 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계 어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상 기된 하드웨어 장치는 실시예의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 본 발명은 도면에 도시된 실시예를 참고로 설명되었으나 이는 예시적인 것에 불과하며, 본 기술 분야의 통상의 지식을 가진 자라면 이로부터 다양한 변형 및 균등한 타 실시예가 가능하다는 점을 이해할 것이다. 예를 들어, 설명된 기술들이 설명된 방법과 다른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성 요소들이 설명된 방법과 다른 형태로 결합 또는 조합되거나, 다른 구성 요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 따라서, 본 발명의 진정한 기술적 보호 범위는 첨부된 등록청구범 위의 기술적 사상에 의해 정해져야 할 것이다.도면 도면1 도면2"}
{"patent_id": "10-2023-0114357", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 발명의 상세한 설명에서 인용되는 도면을 보다 충분히 이해하기 위하여 각 도면의 상세한 설명이 제공된다. 도 1은 본 발명의 일 실시예에 따른 보행 로봇의 학습 방법을 설명하기 위한 개념도이다. 도 2는 도 1의 학습 방법을 설명하기 위한 흐름도이다."}
