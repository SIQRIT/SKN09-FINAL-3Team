{"patent_id": "10-2023-0101545", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0020096", "출원번호": "10-2023-0101545", "발명의 명칭": "위성의 제어를 위한 학습 방법 및 전자 장치", "출원인": "국방과학연구소", "발명자": "한명훈"}}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 의해 수행되는 위성의 제어를 위한 학습 방법에 있어서,제1 상태(state) 정보를 소정의 인공지능 기반 모델에 입력하여 상기 소정의 인공지능 기반 모델의 출력인 위성의 제1 행동(action) 정보를 확인하는 단계;상기 제1 행동 정보에 기초하여 행동이 제어된 상기 위성의 라우팅 경로에 대한 제1 보상(reward) 정보를 획득하는 단계; 및상기 제1 상태 정보, 상기 제1 행동 정보, 및 상기 제1 보상 정보를 기초로 상기 인공지능 기반 모델을 훈련시키는 단계를 포함하고,상기 제1 상태 정보는, 상기 위성의 행동 단위 시간 마다 임의로 변경되는 제1 정보를 포함하는,학습 방법."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 상태 정보는,상기 위성의 현재 위치, 상기 위성의 목표 위치, 상기 위성이 위치할 수 없는 경계면 위치, 및 상기 위성이 이동할 수 없는 링크 단절 위치 중 적어도 하나에 관한 정보를 포함하는, 학습 방법."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제1 정보는,상기 위성의 행동 단위 시간 마다 격자 단위로 임의로 변경되는 링크 단절 위치에 관한 정보를 포함하는학습 방법."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 위성의 제1 행동 정보를 확인하는 단계는,상기 위성의 현재 위치를 기준으로 미리 정의된 제1 방향으로 이동하는 행동,상기 제1 방향과 반대된 제2 방향으로 이동하는 행동,상기 제1 방향 및 상기 제2 방향과 모두 수직인 제3 방향으로 이동하는 행동, 상기 제3 방향과 반대된 제4 방향으로 이동하는 행동, 및 상기 위성의 현재 위치를 유지하는 행동 중 적어도 하나의 제1 행동 정보를 확인하는 단계를 포함하는,공개특허 10-2025-0020096-3-학습 방법."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 위성에 대한 제1 보상 정보를 획득하는 단계는,상기 위성이 상기 위성의 목표 위치에 도달하지 못한 경우, 미리 정해진 값 만큼 감소된 상기 위성에 대한 제1 보상 정보를 획득하는 단계를 포함하는,학습 방법."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 위성에 대한 제1 보상 정보를 획득하는 단계는,상기 위성이 링크 단절 위치에 위치한 경우,상기 위성이 링크 단절 위치에서 벗어날 때까지 상기 위성의 행동 단위 시간 마다 미리 정해진 값 만큼 감소된상기 위성에 대한 제1 보상 정보를 획득하는 단계를 포함하는,학습 방법."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 위성의 제1 행동 정보를 확인하는 단계는,상기 위성이 상기 위성이 위치할 수 없는 경계면 위치에 인접하고, 상기 위성의 행동이 상기 위성을 중심으로한 경계면으로 이동하는 것으로 결정된 경우,상기 위성의 현재 위치를 유지하는 행동으로 상기 위성의 제1 행동 정보를 확인하는 단계를 포함하는,학습 방법."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 위성에 대한 제1 보상 정보를 획득하는 단계는,상기 위성이 상기 위성의 목표 위치에 도달한 경우, 미리 정해진 값 만큼 증가된 상기 위성에 대한 제1 보상 정보를 획득하는 단계를 포함하는,학습 방법."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,제2 상태 정보를 훈련된 인공지능 기반 모델에 입력하여 상기 훈련된 인공지능 기반 모델의 출력인 위성의 제2공개특허 10-2025-0020096-4-행동 정보를 확인하는 단계;상기 제2 행동 정보에 기초하여 행동이 제어된 상기 위성의 라우팅 경로에 제2 보상 정보를 확인하는 단계; 및상기 제2 상태 정보, 상기 제2 행동 정보, 및 상기 제2 보상 정보를 기초로 상기 훈련된 인공지능 기반 모델을훈련시키는 단계를 더 포함하고,상기 제2 상태 정보는, 상기 위성의 행동 단위 시간 마다 임의로 변경되는 제2 정보를 포함하고,상기 제2 정보는,상기 위성의 행동 단위 시간 마다 격자 단위로 임의로 변경되는 링크 단절 위치에 관한 정보를 포함하는학습 방법."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 제2 상태 정보는,제1 상태 정보와는 독립적으로 상기 위성의 행동 단위 시간 마다 임의로 변경되는 것인, 학습 방법."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "위성의 제어를 위한 학습 방법을 수행하는 전자 장치로서,메모리; 및 프로세서(processor)를 포함하고, 상기 프로세서는,제1 상태 정보를 소정의 인공지능 기반 모델에 입력하여 상기 소정의 인공지능 기반 모델의 출력인 위성의 제1행동 정보를 확인하고,상기 제1 행동 정보에 기초하여 행동이 제어된 상기 위성의 라우팅 경로에 제1 보상(reward) 정보를 획득하고,및상기 제1 상태 정보, 상기 제1 행동 정보, 및 상기 제1 보상 정보를 기초로 상기 인공지능 기반 모델을 훈련시키고,상기 제1 상태 정보는, 상기 위성의 행동 단위 시간 마다 임의로 변경되는 제1 정보를 포함하는,전자 장치."}
{"patent_id": "10-2023-0101545", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "위성의 제어를 위한 학습 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 비일시적 기록매체로서,상기 위성의 제어를 위한 학습 방법은,상기 위성의 행동 단위 시간 마다 임의로 변경되는 제1 상태 정보를 소정의 인공지능 기반 모델에 입력하여 상기 소정의 인공지능 기반 모델의 출력인 위성의 제1 행동 정보를 확인하는 단계;상기 제1 행동 정보에 기초하여 행동이 제어된 상기 위성의 라우팅 경로에 제1 보상 정보를 획득하는 단계; 및상기 제1 상태 정보, 상기 제1 행동 정보, 및 상기 제1 보상 정보를 기초로 상기 인공지능 기반 모델을 훈련시키는 단계를 포함하고,공개특허 10-2025-0020096-5-상기 제1 상태 정보는, 상기 위성의 행동 단위 시간 마다 임의로 변경되는 제1 정보를 포함하는,비일시적 기록매체."}
{"patent_id": "10-2023-0101545", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치에 의해 수행되는 위성의 제어를 위한 학습 방법 및 이를 수행하는 전자 장치가 개시된다. 학습 방법 은 위성의 행동 단위 시간 마다 임의(random)로 변경되는 제1 상태(state) 정보를 소정의 인공지능 기반 모델에 입력하여 소정의 인공지능 기반 모델의 출력인 위성의 제1 행동(action) 정보를 확인하는 단계, 제1 행동 정보에 기초하여 행동이 제어된 위성의 라우팅 경로에 대한 제1 보상(reward) 정보를 획득하는 단계, 및 제1 상태 정보, 제1 행동 정보, 및 제1 보상 정보를 기초로 인공지능 기반 모델을 훈련시키는 단계를 포함한다."}
{"patent_id": "10-2023-0101545", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 위성의 제어를 위한 학습 방법에 관한 것이다."}
{"patent_id": "10-2023-0101545", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 지구 전역의 초고속 네트워크 구성을 위한 방법으로 저궤도 위성 네트워크(low earth orbit satellite network)의 활용이 증가하고 있다. 저궤도 위성 네트워크는 지구 저궤도에 수백 개 이상의 작은 위성을 배치하 여 통신, 인터넷, 센서 및 관측 서비스를 제공하는 데 사용된다. 저궤도 위성 네트워크의 원활한 운용을 통해 데이터 전송 지연이 적으면서 전역 커버리지를 달성할 수 있으며, 지상의 여러 지점 간 통신을 향상시키거나 거 리가 서로 먼 지역간이나 통신 기반이 없는 지역에서도 인터넷 접근성을 개선할 수 있다는 효과를 얻을 수 있다. 저궤도 위성의 빠른 실행 속도와 빈번하게 변화하는 위성 토폴로지로 실시간으로 변화하는 환경에 맞출 수 있는 새로운 저궤도 위성 네트워크 라우팅 알고리즘의 필요성이 증가하고 있다."}
{"patent_id": "10-2023-0101545", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 실시 예가 해결하고자 하는 과제는, 실시간으로 변화하는 LEO-SN(low earth orbit - satellite network) 환 경에 따라 적용할 수 있는 강화 학습 알고리즘을 위한 심층강화학습 기반 라우팅 기법을 통해 동적 환경에서의 링크 단절, 신뢰성 하락 등의 문제를 빠르게 해결하는 데 있다. 본 실시 예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 이하의 실시 예들로부터 또 다른 기술적 과제들이 유추될 수 있다."}
{"patent_id": "10-2023-0101545", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시 예에 따른 전자 장치에 의해 수행되는 위성의 제어를 위한 학습 방법은, 상기 위성의 행동 단위 시간 마다 임의(random)로 변경되는 제1 상태(state) 정보를 소정의 인공지능 기반 모델에 입력하여 상기 소정의 인 공지능 기반 모델의 출력인 위성의 제1 행동(action) 정보를 확인하는 단계; 상기 제1 행동 정보에 기초하여 행 동이 제어된 상기 위성의 라우팅 경로에 대한 제1 보상(reward) 정보를 획득하는 단계; 및 상기 제1 상태 정보, 상기 제1 행동 정보, 및 상기 제1 보상 정보를 기초로 상기 인공지능 기반 모델을 훈련시키는 단계를 포함하고, 상기 제1 상태 정보는, 상기 위성의 행동 단위 시간 마다 임의로 변경되는 제1 정보를 포함할 수 있다. 일 실시 예에 따른 위성의 제어를 위한 학습 방법을 수행하는 전자 장치는, 메모리; 및 프로세서(processor)를 포함하고, 상기 프로세서는, 상기 위성의 행동 단위 시간 마다 임의로 변경되는 제1 상태 정보를 소정의 인공지 능 기반 모델에 입력하여 상기 소정의 인공지능 기반 모델의 출력인 위성의 제1 행동 정보를 확인하고, 상기 제 1 행동 정보에 기초하여 행동이 제어된 상기 위성의 라우팅 경로에 제1 보상(reward) 정보를 획득하고, 및 상기 제1 상태 정보, 상기 제1 행동 정보, 및 상기 제1 보상 정보를 기초로 상기 인공지능 기반 모델을 훈련시킬 수 있고, 상기 제1 상태 정보는, 상기 위성의 행동 단위 시간 마다 임의로 변경되는 제1 정보를 포함할 수 있다. 일 실시 예에 따른 위성의 제어를 위한 학습 방법을 수행하는 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록 매체는 상술한 동작 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 비 일시적 기록 매체를 포함할 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 도면들에 포함되어 있다."}
{"patent_id": "10-2023-0101545", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면 저궤도 위성의 특성으로 인한 잦은 네트워크 토폴로지 변화와 위성 링크 단절 등에 강한 임의 보상 모델을 사용하여 보상 함수가 불명확한 경우에도 위성의 라우팅 경로와 관련하여 보다 정확도가 높은 학습 을 인공지능 기반 모델에 제공할 수 있고, 기존 알고리즘보다 빠른 학습 수렴 효과를 기대할 수 있다."}
{"patent_id": "10-2023-0101545", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "발명의 효과는 이상에서 언급한 효과로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 청구범위의 기재로부 터 당해 기술 분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0101545", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "실시 예들에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들 을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의 미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 명세서 전체에서 기재된 \"a, b, 및 c 중 적어도 하나\"의 표현은, 'a 단독', 'b 단독', 'c 단독', 'a 및 b', 'a 및 c', 'b 및 c', 또는 'a, b, 및 c 모두'를 포괄할 수 있다. 이하에서 언급되는 \"단말\"은 네트워크를 통해 서버나 타 단말에 접속할 수 있는 컴퓨터나 휴대용 단말로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱(desktop), 랩톱 (laptop) 등을 포함하고, 휴대용 단말은 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, IMT(International Mobile Telecommunication), CDMA(Code Division Multiple Access), W-CDMA(W-Code Division Multiple Access), LTE(Long Term Evolution) 등의 통신 기반 단말, 스마트폰, 태블릿 PC 등과 같은 모든 종류의 핸드헬드(Handheld) 기반의 무선 통신 장치를 포함할 수 있다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시 예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식 을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 이하에서는 도면을 참조하여 본 개시의 실시 예들을 상세히 설명한다. 도 1은 일 실시 예에 따른 위성의 제어 시스템을 나타내기 위한 도면이다. 도 1을 참조하면, 저궤도 위성 네트워크 시스템을 확인할 수 있다. 위성 라우팅은 위성을 활용하여 네트워크의 데이터 전송을 지원하는 기술로 일반적으로 지구에서 멀지 않은 저궤도에 위치한 위성 을 사용하여 데이터를 전송하고 수신하는 방식으로 작동하며 주로 지리적으로 떨어진 지역이나 통신 인프 라가 부족한 지역에서 전자 장치를 통해 네트워크 연결을 제공하는 데 사용된다. 위성 라우팅은 주로 군 사 통신, 원격 지역에서의 인터넷 접속, 재난 구호 및 비상 상황에서의 통신 등 다양한 분야에서 사용될 수 있 으며, 최근에는 저관측 궤도 위성 및 대량 위성 네트워크를 활용한 상용 위성 인터넷 서비스도 등장하고 있는 추세이다. 저궤도 위성 네트워크 시스템은 저궤도(LEO) 위성 네트워크에서 라우팅 기술을 통해 위성과 지구 에 위치한 각 네트워크간의 효율적이고 안정적인 통신을 제공하는 데 중요한 역할을 수행할 수 있다. 저궤도 위성 네트워크는 일반적으로 수백에서 수천 킬로미터 범위의 고도에서 지구 표면에 비교적 가까운 궤도를 도는 수많은 위성을 포함할 수 있으며, 이러한 저궤도 위성 네트워크 시스템은 광대역 인터넷 연결, 글로벌 커버리지 및 저지연 통신과 같은 다양한 서비스를 제공할 수 있다. LEO 위성의 네트워크의 라우팅에는 데이터 패킷이 소스 위성에서 대상 위성 또는 지상국으로 통과하는 최 적의 경로를 결정하는 프로세스가 포함될 수 있다. 예를 들어 LEO 위성의 네트워크의 라우팅에는 알려진 위성 궤도 및 이동 패턴을 활용하여 미래 위치를 추정함을 통해 위성의 미래 위치를 예측함으로써 라우팅 결정 을 최적화하여 대기 시간을 최소화하고 처리량을 최대화하는 프로세스가 포함될 수 있다. 또한 LEO 위성의 네 트워크의 라우팅에는 링크 품질, 혼잡 수준, 사용 가능한 대역폭 및 대기 시간과 같은 요소를 고려하여 실 시간 조건 및 네트워크 성능에 따라 라우팅 경로를 동적으로 조정하는 프로세스가 포함될 수 있다. LEO 위성의 네트워크에 사용되는 라우팅 기술은 네트워크 아키텍처, 시스템 요구 사항 및 배포 전략에 따라 달라질 수 있다. 지속적으로 이동하고 위치를 변경하는 LEO 위성 배치의 동적 특성으로 인해, 네트워크에 사용되는 기존 라우팅 기술을 이러한 특정 특성에 맞게 조정하고 최적화할 필요가 있다. 특히 저궤도 위성의 특성상 잦 은 네트워크 토폴로지 변화와 위성 링크 단절등의 문제점을 해결하지 않는다면 라우팅 알고리즘의 효과도 저하 될 수 있으므로, 데이터의 정보 유지와 전체 위성군의 신뢰성 확보, 외부 요인에 대한 단절을 고려하기 위해서 는 실시간으로 변화하는 환경에 대한 강한 내성을 가진 새로운 강화 학습 알고리즘의 필요성이 요구된다. 일 실시 예에 따른 전자 장치에 의해 수행되는 위성의 제어를 위한 학습 방법은, 저궤도 위성 네트워크에 서의 위성의 링크 단절 문제를 해결하기 위해 강화 학습을 통해 라우팅 경로를 결정하고 단절된 경로를 최대한 피해서 이동하도록 위성의 행동을 결정하는 방법을 제공할 수 있다. 일 실시 예에 따른 전자 장치에 의해 수행되는 위성의 제어를 위한 학습 방법은, 위성의 동적 환경 문제를 해결하기 위해 동적 환경에 강인한 강화 학습 기법을 학습의 대상이 되는 인공지능 기반 모델에 제공할 수 있다. 일 실시 예에 따른 전자 장치에 의해 수행되는 위성의 제어를 위한 학습 방법은 임의 보상 모델(random reward model)을 통한 임의의 보상 함수 를 인공지능 기반 모델에 제공하여 인공지능 기반 모델의 인공 지능 플레이어인 에이전트(agent)(예: 위성 )에게 예측할 수 없는 보상을 제공하고, 이렇게 주어진 보상을 통해 에이전트에게 다양한 행동을 시도하게 끔 하고, 예측하지 못한 보상을 얻게 하여 인공지능 기반 모델을 학습 시킬 수 있다. 일 실시 예에 따른 전자 장치에 의해 수행되는 위성의 제어를 위한 학습 방법은 Grid MDP 기반 동적 네트워크를 통해 마르코프 결 정 과정 및 그리드와 같은 이산형 공간을 사용하여 동적 네트워크 시스템을 모델링할 수 있으며, Dueling DQN 기반 학습을 통해 이득 값(advantage value)과 상태 값(state value)을 각각 계산하여 타 알고리즘 대비 안정적 이고 높은 성능의 학습을 위성에게 제공할 수 있다. 전자 장치를 통한 학습을 통해 임의 보상 모델 을 사용하여 보상 함수가 정확히 정의 되어있지 않거나 행동의 결과가 불확실한 경우에도 보다 높은 정확도의 학습을 인공지능 기반 모델에 제공할 수 있으며, 기존 알고리즘보다 빠른 학습 수렴 시간을 통해 위성의 동적 환경에서의 링크 단절, 신뢰성 하락 등의 문제를 해결할 수 있다. 도 2는 일 실시 예에 따른 위성의 제어를 위한 학습 방법에 사용되는 임의 보상 모델의 개념을 설명하기 위한 도면이다. 도 2를 참조하면, 일반적인 강화 학습 알고리즘과 임의 보상 모델을 이용한 강화 학습 알고리즘을 확인할 수 있 다. 강화 학습(reinforcement learning)은 기계 학습의 일종으로서, 어떤 환경에서 위성이 현재의 상태를 인식 하여 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 학습 방법일 수 있다. 강화 학습에서 다루는 환경은 예를 들어, 마르코프 결정 과정(markov decision process, MDP)으로 주어질 수 있 다. 어떤 시점 t에서, 마르코프 결정 과정은 어떤 상태에 존재할 수 있다. 학습의 대상이 되는 모델은 제1 상 태 정보에 기초하여 어떠한 제1 행동 정보를 결정할 수 있다. 예를 들어, 전자 장치는 제1 상태 정보를 수신하고, 입력된 제1 상태 정보를 소정의 인공지능 기반 모델에 입력하여 출력된 위성의 제어를 위한 제1 행동 정보를 확인할 수 있다. 제1 행동 정보가 결정되면, 결정된 제1 행동 정보에 의해 환경에 대한 상태 정보가 제 2 상태 정보로 전환될 수 있으며, 모델은 제2 상태 정보에 대응하여 새로운 제2 행동 정보를 결정할 수 있다. 예를 들어, 전자 장치는 제1 행동 정보에 대응하여 변화된 제2 상태 정보를 확인하고, 제2 상태 정보를 인 공지능 기반 모델에 입력하여 출력된 새로운 제2 행동 정보를 확인할 수 있다. 인공지능 기반 모델은 환경으로 부터 제1 행동 정보에 대응하는 제1 보상 정보를 제공받을 수 있으며, 어떠한 상태에서 결정한 행동에 따른 보 상을 통해 학습될 수 있다. 예를 들어, 전자 장치는 제1 상태 정보, 제2 상태 정보, 제1 행동 정보, 제2 행동 정보, 제1 보상 정보, 및 제2 보상 정보를 기초로 인공지능 기반 모델을 훈련시킬 수 있으며 일 실시 예에 따른 위성의 제어를 위한 학습 방법에서의 적용은 이하 도 4에서 구체적으로 살핀다. 이와 같이 일반적인 강화 학습 알고리즘은 강화 학습의 대상이 될 인공지능 기반 모델의 인공 지능 플레이어인 에이전트(agent)가 솔루션을 찾기 위한 상태(state)에서 특정 행동(action)을 하는 경우, 행동에 대 한 점수 혹은 결과에 대한 보상(reward)을 제공하고 에이전트가 속한 환경(environment)에 에 이전트의 행동이 영향을 미치면, 변화한 환경이 다시 에이전트의 상태에 영향을 주는 것을 반복하여 에이전트가 보상을 최대화하는 행동을 취하도록 인공지능 기반 모델을 학습시키는 과정으로 이루어진 다. 그러나 에이전트가 행동에 대해 얻게 될 보상과 관련한 함수가 정확히 정의되어 있지 않거 나 행동의 결과가 불확실해 인공지능 기반 모델의 학습의 진행이 어려운 경우, 임의 보상 모델을 이용하여 에이전트가 행동을 통해 얻게 될 임의 보상(random reward)을 인공지능 기반 모델에 제공할 수 있다. 임의 보상 모델은 임의 보상을 인공지능 기반 모델의 학습 프로세스에 통합하여 일반적인 강화 학 습 알고리즘이 갖는 문제를 해결하고 에이전트가 환경을 보다 효과적으로 탐색할 수 있도록 할 수 있 다. 임의 보상 모델을 통해 다양한 환경 변화에서 보상을 최대화 하기 위해 더욱 많은 데이터를 수집하고, 불 확실성을 고려하여 더욱 안정적인 행동을 하도록 인공지능 기반 모델을 학습 시킴으로써 위성의 라우팅 방법을 향상시킬 수 있다. 또한 특정 상황에서 위성이 지나치게 적응하는 것을 방지함을 통해 학습을 편향되게 수행하 는 것을 방지할 수 있다. 임의 보상 모델은 state-action 쌍에 대한 기댓 값을 나타내는 Q-function을 통해 학습을 수행하는 DQN(deep Q-network)과 같은 값(value) 기반 강화 학습 알고리즘에서 주로 사용될 수 있다. 즉 임의 보상 모델을 적용한 DQN은 보상 함수의 랜덤 한 변화를 통해 임의 보상을 획득하게 되고, Q-function의 예측이 어려워지게 되 어 일반적인 강화 학습 알고리즘을 이용했을 때 보다 인공지능 기반 모델을 더 강건하게 학습시킬 수 있다. 예 를 들어 임의 보상은 인공지능 기반 모델의 학습 과정에서 에이전트가 속한 환경을 임의로 변경 하고, 임의로 변경된 환경에서의 에이전트에 행동에 따라 임의로 보상을 제공하는 방식을 통해 구현 될 수 있다. 도 3은 일 실시 예에 따른 위성의 제어를 위한 학습 방법에 사용되는 Dueling DQN 기법을 설명하기 위한 도면이 다. 도 3을 참조하면, 일 실시 예에 따른 위성의 제어를 위한 학습 방법은 저궤도 위성군 라우팅 알고리즘 설계를 위한 심층강화학습 기법 중 하나인 Dueling DQN를 이용할 수 있다. 여기서 Dueling DQN은 DQN(31 0)과 같이 Q-learning 알고리즘을 기반으로 하는 신경망 기반의 알고리즘을 의미할 수 있다. DQN과 Dueling DQN은 컴퓨터 비전(computer vision)에서 사용되던 CNN을 이용하여 Q-network를 구성 한다는 점에서 공통점을 가질 수 있다. 그러나 기존의 DQN의 Q-network에서 마지막 fully-connected layer를 통과하면 행동에 대한 Q-value를 바로 추정하는 것과는 다르게, Dueling DQN에서는 fully-connected layer를 현재 상태의 가치가 얼마나 되는지를 나타내는 state-value layer와 현재 상태에서 해 당 행동이 다른 행동에 비해 가지는 상대적 가치가 얼마나 되는지를 나타내는 advantage layer를 분리하여 추정하고, 마지막에 이들을 aggregation layer에서 합한 뒤 Q-valuer를 추정할 수 있다. Dueling DQN의 aggregation layer의 활용으로 인해, 어떤 에이전트에게 어떤 상태가 가치 있는지를 판단할 때 해당 상태의 모든 행동을 수행하지 않더라도 다양한 환경에 따른 행동들을 학습하는데 유용하며, 노이즈(nois e)에 강한 학습을 수행할 수 있다. 일 실시 예에 따른 위성의 제어를 위한 학습 방법에서의 DQN의 사용 단계는, 이하 도 6에서 자세히 살핀다. 도 4는 일 실시 예에 따른 위성의 제어를 위한 학습 방법을 설명하기 위한 개념도이다. 도 4를 참조하면, 인공지능 기반 모델은 학습의 대상이 되는 인공지능 기반 모델일 수 있다. 제1 상태 정 보는 어떤 시점 t에서의 위성의 환경과 관련하여 결정된 입력 데이터일 수 있다. 제1 행동 정보는 시점 t에서의 위성을 제어하기 위한 행동 정보일 수 있다. 제1 보상 정보는 시점 t에서의 보상 정보일 수 있다. 제2 상태 정보는 시점 t+1에서의 상태 정보일 수 있고, 제2 보상 정보는 시점 t+1에서의 보상 정보일 수 있다. 일 실시 예에 따른 위성의 제어를 위한 학습 방법을 수행하는 전자 장치는 강화 학습에 기반하여 인공지능 기반 모델을 훈련시킬 수 있다. 일 실시 예에서, 위성의 제어를 위한 학습 방법을 수 행하는 전자 장치는 위성의 행동 단위 시간 마다 임의(random)로 변경되는 제1 상태 정보를 소정의 인공지능 기반 모델에 입력하여 소정의 인공지능 기반 모델의 출력인 위성의 제1 행동 정보를 확인할 수 있다. 여기서 제1 상태 정보는, 위성의 현재 위치, 위성의 목표 위치, 위성이 위치할 수 없는 경계면 위치, 및 위성이 이동할 수 없는 링크 단절 위치 중 적어도 하나에 관한 정보를 포함할 수 있다. 언급한 제1 상태 정 보에 기초하여 학습을 수행하는 실시 예는 이하 도 5a 및 도 5b에서 상세히 설명될 것이다. 도 5a 및 도 5b는 일 실시 예에 따른 위성의 제어를 위한 학습 방법에 사용되는 Grid MDP 기반 동적 네트워크 모델을 설명하기 위한 도면들이다. 일 실시 예에 따른 위성의 제어를 위한 학습 방법은 저궤도 위성군 라우팅 알고리즘의 적용을 위한 Grid MDP 기 반 동적 네트워크 모델을 이용할 수 있다. 여기서 Grid MDP 기반 동적 네트워크 모델은 마르코프 결정 과정 및 그리드와 같은 이산형 공간을 사용하여 동적인 시스템에서의 상호작용과 변화를 모델링하며, 최적 제어와 같은 응용 분야에서 활용되는 동적 네트워크 시스템을 모델링하는 방법을 의미할 수 있다. 여기서 Grid MDP는 이산 적인 상태 공간과 행동 공간을 갖는 마르코프 결정 과정을 의미할 수 있으며, 에이전트의 현재 위치와 링크 단 절 위성 정보, 경계면과 목표 위치에 대한 정보를 포함하는 2D 그리드를 사용할 수 있다. 도 5a를 참조하면, 일 실시 예에 따른 위성의 제어를 위한 학습 방법을 수행하는 전자 장치는 위성의 행동 단위 시간 마다 임의(random)로 변경되는 제1 상태 정보를 소정의 인공지능 기반 모델에 입력할 수 있다. 예를 들어 제1 상태 정보는 2차원 격자 형태의 정보를 포함할 수 있으며, 제1 상태 정보는 위성의 행동 단위 시간 마 다 임의로 변경되는 제1 정보를 포함할 수 있다. 예를 들어 전자 장치는 2차원 5 X 5 격자 형태의 정보를 포함하고, 위성의 행동 단위 시간 마다 임의로 변경되는 링크 단절 위치에 관한 정보를 포함하는 제1 상태 정보 를 인공지능 기반 모델에 입력할 수 있다. 여기서 격자는 위성의 현재 위치를 의미할 수 있다. 예를 들 어 위성의 현재 위치는 위성의 제어를 위한 학습 방법이 시작될 때 임의로 결정된 위성의 위치에 대한 정보를 포함할 수 있다. 격자는 위성이 도달하고자 하는 위성의 목표 위치를 의미할 수 있다. 예를 들어 위성의 목표 위치는 위성 의 제어를 위한 학습 방법이 시작될 때 임의로 결정된 위성이 도달해야 할 위성의 목표 위치에 대한 정보를 포 함할 수 있다. 격자는 위성을 중심으로 하는 경계면을 의미할 수 있다. 예를 들어 위성을 중심으로 하는 경계면은, 위성의 행동 단위 시간 마다 위성이 결정된 행동에 따라 이동할 때, 정해진 격자 크기 내에서 이동하 기 위해 결정된 경계면에 대한 정보를 포함할 수 있다. 격자는 장애물 등으로 인한 위성의 링크 단절 위 치를 의미할 수 있다. 예를 들어 링크 단절 위치는, 위성의 위치와 링크 단절 위치가 동일할 때 위성이 위성의 행동 단위 시간 이후에도 이동할 수 없는 링크 단절 위치에 대한 정보를 포함할 수 있다. 도 5b를 참조하면, 일 실시 예에 따른 위성의 제어를 위한 학습 방법을 수행하는 전자 장치는 제1 상태 정 보를 인공지능 기반 모델에 입력하여 인공지능 기반 모델의 출력인 제1 행동 정보를 확인할 수 있다. 예를 들 어 제1 상태 정보는 도 4b에서와 같이 15 X 15 격자 형태의 정보를 포함할 수 있다. 이 때 위성의 현재 위치는격자(510-1)과 같을 수 있으며, 위성의 목표 위성 위치는 격자(520-1)과 같을 수 있으며, 위성을 중심으로 하는 경계면 위치는 격자(530-1)과 같을 수 있으며, 위성의 링크 단절 위치는 격자(540-1)과 같을 수 있으나, 본 개 시에 따른 실시 예가 언급한 특정 경우에 한정되는 것은 아니다. 예를 들어 위성의 제1 행동 정보를 확인하는 단계는, 위성의 현재 위치를 기준으로 미리 정의된 제1 방향으로 이동하는 행동, 제1 방향과 반대된 제2 방향으로 이동하는 행동, 제1 방향 및 제2 방향과 모두 수직인 제3 방향 으로 이동하는 행동, 제3 방향과 반대된 제4 방향으로 이동하는 행동, 및 위성의 현재 위치를 유지하는 행동 중 적어도 하나의 제1 행동 정보를 확인하는 단계를 포함할 수 있다. 예를 들어 위성의 제1 행동 정보는 적어도 5 가지로, 위성의 현재 위치를 기준으로 우측으로 이동하는 행동, 위성의 현재 위치를 기준으로 우측과 반대된 좌 측으로 이동하는 행동, 위성의 현재 위치를 기준으로 우측 및 좌측과 모두 수직인 상부로 이동하는 행동, 위성 의 현재 위치를 기준으로 상부와 반대된 하부로 이동하는 행동, 및 위성의 현재 위치를 기준으로 동일한 위치를 유지하는 대기 행동 중 적어도 하나를 포함할 수 있다. 예를 들어 전자 장치는 위성이 격자(510-1)에서 우측으로 이동하여 격자(510-2)로 이동하는 행동을 포함하여 위성의 제1 행동 정보를 확인할 수 있다. 일 실시 예에 따른 전자 장치는, 위성이 위치할 수 없는 경계면 위치에 인접하고, 위성의 행동이 위성을 중심으로 한 경계면으로 이동하는 것으로 결정된 경우, 위성의 현재 위치를 유지하는 행동으로 위성의 제1 행동 정보를 확인할 수 있다. 예를 들어 전자 장치는, 위성이 위성을 중심으로 한 경계면(예: 격자(530-1)에 인 접한 격자(510-2))에 인접(예: 격자(530-1)에 인접한 격자(510-2)에 위성이 위치하는 경우)하고 위성의 행동이 위성을 중심으로 한 경계면으로 이동하는 것으로 결정된 경우, 위성의 현재 위치를 유지하는 행동으로 위성의 제1 행동 정보를 확인할 수 있다. 이를 통해 전자 장치는 인공지능 기반 모델을 훈련시키는 동안 위성이 제1 상태 정보에서 정의된 범주 내에서 훈련을 지속하도록 할 수 있다. 일 실시 예에 따른 제1 상태 정보는 위성의 행동 단위 시간 마다 임의로 변경되는 제1 정보를 포함할 수 있으며, 제1 정보는 위성의 행동 단위 시간 마다 격자 단위로 임의로 변경되는 링크 단절 위치에 관한 정보를 포함할 수 있다. 예를 들어 링크 단절 위치는 위성의 행동 단위 시간 마다 우측에서 좌측으로 격자 단위로 변 경되는 것일 수 있다. 예를 들어 위성의 링크 단절 위치가 격자(540-1)과 같을 때, 위성의 행동 단위 시간 이 후 위성의 링크 단절 위치는 우측에서 좌측으로 한 격자 씩 옮겨서 격자(540-2)와 같이 변경될 수 있다. 일 실시 예에 따른 전자 장치는, 위성이 각 행동을 수행할 때 마다 위성의 행동에 따른 제1 보상 정보를 획득할 수 있다. 일 실시 예에 따른 위성에 대한 제1 보상 정보를 획득하는 단계는, 위성이 위성의 목표 위치 에 도달하지 못한 경우, 미리 정해진 값 만큼 감소된 위성에 대한 제1 보상 정보를 획득하는 단계를 포함할 수 있다. 예를 들어 위성에 대한 제1 보상 정보를 획득하는 단계는, 위성이 도달하고자 하는 위성의 목표 위치(예: 격자(520-1))에 도달하지 못한 경우, 위성의 행동과 관계 없이 모두 미리 정해진 -1의 값 만큼 감소된 위성에 대한 제1 보상 정보를 획득하는 단계를 포함할 수 있다. 예를 들어 위성의 행동에 따른 보상을 획득하 는 단계는, 위성이 격자(510-1)에서 위성의 행동 단위 시간 이후 격자(510-2)로 이동한 경우, 위성이 위성의 목 표 위치(예: 격자(520-1))에 도달하지 못했으므로, 미리 정해진 -1의 값 만큼 감소된 위성에 대한 제1 보상 정 보를 획득하는 단계를 포함할 수 있다. 예를 들어 위성의 행동에 따른 보상을 획득하는 단계는, 위성이 링크 단절 위치(예: 격자(540-1) 또는 격자 (540-2))에 위치한 경우, 위성이 위성의 목표 위치에 도달하지 못했으므로, 미리 정해진 -1의 값 만큼 감소된 위성에 대한 제1 보상 정보를 획득하는 단계를 포함할 수 있다. 이 경우 위성이 링크 단절 위치에 이동하기까 지 하나의 단위 시간이 지났으므로 -1 점의 보상을 획득할 수 있으며, 링크 단절 위치에 도달한 위성은 상, 하, 좌, 우로 이동하는 행동을 할 수 없으므로 다시 단위 시간이 지나면서 링크 단절 위치가 우측에서 좌측으로 한 칸씩 옮겨져서 위성이 링크 단절 위치를 벗어날 때 까지 미리 정해진 -1의 값 만큼 감소된 위성에 대한 제1 보 상 정보를 획득할 수 있다. 예를 들어 위성의 행동에 따른 보상을 획득하는 단계는, 위성이 위성을 중심으로 한 경계면(예: 격자(530-1))에 인접하고 위성의 행동이 위성을 중심으로 한 경계면으로 이동하는 것으로 결정된 경우, 위성의 현재 위치를 유 지하며 위성의 목표 위치에 도달하지 못했으므로, 미리 정해진 -1의 값 만큼 감소된 위성에 대한 제1 보상 정보 를 획득할 수 있다. 일 실시 예에 따른 위성에 대한 제1 보상 정보를 획득하는 단계는, 위성이 위성의 목표 위치에 도달한 경우, 미 리 정해진 값 만큼 증가된 위성에 대한 제1 보상 정보를 획득하는 단계를 포함할 수 있다. 이 경우 하나의 학 습 단계가 종료되며 새로운 학습 단계가 시작될 수 있다. 일 실시 예에 따른 학습 방법은 위성의 행동 단위 시 간 마다 임의로 변경되는 제2 상태 정보를 훈련된 인공지능 기반 모델에 입력하여 훈련된 인공지능 기반 모델의출력인 위성의 제2 행동 정보를 확인하고, 제2 행동 정보에 기초하여 행동이 제어된 위성의 라우팅 경로에 제2 보상 정보를 확인하고, 제2 상태 정보, 제2 행동 정보, 및 제2 보상 정보를 기초로 훈련된 인공지능 기반 모델 을 훈련시키는 단계를 더 포함하는 할 수 있다. 예를 들어 제2 상태 정보는, 위성의 행동 단위 시간 마다 임의 로 변경되는 제2 정보를 포함할 수 있고, 제2 정보는 위성의 행동 단위 시간 마다 격자 단위로 임의로 변경되는 링크 단절 위치에 관한 정보를 포함할 수 있다. 일 실시 예에 따른 제2 상태 정보는, 제1 상태 정보와는 독립 적으로 위성의 행동 단위 시간 마다 임의로 변경되는 것일 수 있다. 예를 들어 학습 방법은 위성의 환경과 관 련하여 위성의 현재 위치, 위성의 목표 위치, 위성을 중심으로 하는 경계면 위치, 위성의 링크 단절 위치 중 적 어도 하나가 제1 상태 정보와는 독립적으로 위성의 행동 단위 시간 마다 임의로 변경되는 제2 상태 정보에 기초 하여 이전 학습 단계에서 제1 상태 정보, 제1 행동 정보, 제2 보상 정보에 기초하여 기 훈련된 인공지능 기반 모델을 추가적으로 훈련하는 단계를 더 포함할 수 있다. 제2 상태 정보는, 제1 상태 정보에 포함된 제1 정보가 위성의 행동 단위 시간 마다 격자 단위로 임의로 변경되는 링크 단절 위치에 관한 정보를 포함하는 것과 유사하 게, 위성의 행동 단위 시간 마다 격자 단위로 임의로 변경되는 링크 단절 위치에 관한 정보를 포함하는 제2 정 보를 포함할 수 있다. 이후에도 상술한 바와 같은 학습 단계를 거치면서 위성의 제어를 위한 학습 방법이 지속 적으로 진행될 수 있다. 일 실시 예에 따른 위성의 제어를 위한 학습 방법은 네트워크 토폴로지가 변화하는 Grid MDP 기반 동적 네트워크 모델에서의 동적인 환경을 표현하기 위해 링크 단절 위치를 위성의 행동 단위 시 간 우측에서 좌측으로 한 칸씩 옮겨 동적인 환경을 표현함을 통해 네트워크 토폴로지의 변화가 발생할 때마다 새로운 상태 및 행동 공간을 계산하고 최적 의사결정 전략을 다시 계산하여 문제를 해결할 수 있다. 도 6은 일 실시 예에 따른 위성의 제어를 위한 학습 방법에서 Dueling DQN 을 이용하여 품질 값을 획득하는 과 정을 설명하기 위한 흐름도이다. 도 6을 참조하면, 일 실시 예에 따른 위성의 제어를 위한 학습 방법을 수행하는 전자 장치에서 Dueling DQN 학습 기법을 이용하는 구체적인 실시 예를 확인할 수 있다. 일 실시 예에 따른 전자 장치는 단계S610에서 예를 들어 15 X 15 X 4의 격자 형태의 정보를 포함하는 입력 데이터를 수신할 수 있다. 여기서 15 X 15는 격자 형태의 배열의 크기, 즉 환경의 크기를 의미할 수 있다. 또 한 각각의 환경 내에서 위성의 현재 위치, 위성의 목표 위치, 위성을 중심으로 하는 경계면 위치, 및 위성 링크 단절 위치들 각각의 4가지 정보가 입력 데이터에 포함될 수 있으므로 전체 입력 데이터의 크기는 15 X 15 X 4의 격자 형태의 정보를 포함할 수 있다. 일 실시 예에 따른 전자 장치는 단계 S620에서 이전 단계에서 가공된 데이터를 Dueling DQN 클래스 안에 서 이득 값(advantage value)과 상태 값(state value)으로 나눈 뒤 4 X 4의 크기를 가지는 커널에 대해 컨볼루 션을 수행하여 12 X 12 X 16의 격자 형태의 정보를 포함하는 데이터를 획득할 수 있다. 일 실시 예에 따른 전자 장치는 단계S630에서 이전 단계에서 획득한 데이터를 다시 4 X 4의 크기를 가지는 커널을 통과시켜 32 X 32 X 9의 데이터로 만들고 이후 단계S640 및 단계 S650에서 각각 2 X 2의 크기를 가지는 커널을 총 두 번 통과시켜 128 X 7 X 7의 사이즈를 가지는 데이터를 획득할 수 있다. 전자 장치는 단계S660 및 단계S670에서 아핀(affine) 변환을 각각 한번씩 진행 한 뒤 이득 값을 이득 값 레이어(advantage value layer)에 입력하고, 상태 값을 상태 값 레이어(state value layer) 에 입력하여 데이 터를 가공하는 동작을 수행할 수 있다. 전자 장치는 단계S680-1에서 이득 값 레이어에 대한 아핀 변환을 두 번 진행하고, 단계S680-2에서 상태 값 레이어에 대한 아핀 변환을 두 번 진행한 뒤 이 과정에서 나온 값을 조합하여 위성의 상태를 행동으로 치환하여 단계 S690에서 최적의 위성의 행동을 결정하는 품질 값(Q-value)를 도출할 수 있으며, 이를 위성의 제어를 위한 학습 방법에 사용할 수 있다. 도 7은 일 실시 예에 따른 전자 장치의 동작 방법을 도시한다. 도 7을 참조하면, 일 실시 예에 따른 위성의 제어를 위한 학습 방법을 수행하는 전자 장치는 단계S710에서 제1 상태 정보를 소정의 인공지능 기반 모델에 입력하여 소정의 인공지능 기반 모델의 출력인 위성의 제1 행동 정보를 확인할 수 있다. 예를 들어 제1 상태 정보는, 위성의 현재 위치, 위성의 목표 위치, 위성이 위치할 수 없는 경계면 위치, 및 위성이 이동할 수 없는 링크 단절 위치 중 적어도 하나에 관한 정보를 포함할 수 있다.예를 들어 제1 상태 정보는, 위성의 행동 단위 시간 마다 임의로 변경되는 제1 정보를 포함할 수 있으며, 제1 정보는 위성의 행동 단위 시간 마다 격자 단위로 임의로 변경되는 링크 단절 위치에 관한 정보를 포함할 수 있 다. 예를 들어 위성의 제1 행동 정보는, 위성의 현재 위치를 기준으로 미리 정의된 제1 방향으로 이동하는 행 동, 제1 방향과 반대된 제2 방향으로 이동하는 행동, 제1 방향 및 제2 방향과 모두 수직인 제3 방향으로 이동하 는 행동, 제3 방향과 반대된 제4 방향으로 이동하는 행동, 및 위성의 현재 위치를 유지하는 행동 중 적어도 하 나에 대한 정보를 포함할 수 있다. 예를 들어 위성의 제1 행동 정보를 확인하는 단계는, 위성이 위성이 위치할 수 없는 경계면 위치에 인접하고, 위성의 행동이 위성을 중심으로 한 경계면으로 이동하는 것으로 결정된 경우, 위성의 현재 위치를 유지하는 행동으로 위성의 제1 행동 정보를 확인하는 단계를 포함할 수 있다. 전자 장치는 단계S720에서 제1 행동 정보에 기초하여 행동이 제어된 위성의 라우팅 경로에 대한 제1 보상 정보를 획득할 수 있다. 예를 들어 제1 보상 정보를 획득하는 단계는, 위성이 위성의 목표 위치에 도달하지 못 한 경우, 미리 정해진 값 만큼 감소된 위성에 대한 제1 보상 정보를 획득하는 단계를 포함할 수 있다. 예를 들 어 위성에 대한 제1 보상 정보를 획득하는 단계는, 위성이 링크 단절 위치에 위치한 경우, 위성이 링크 단절 위 치에서 벗어날 때까지 위성의 행동 단위 시간 마다 미리 정해진 값 만큼 감소된 위성에 대한 제1 보상 정보를 획득하는 단계를 포함할 수 있다. 예를 들어 위성에 대한 제1 보상 정보를 획득하는 단계는, 위성이 위성의 목표 위치에 도달한 경우, 미리 정해 진 값 만큼 증가된 위성에 대한 제1 보상 정보를 획득하는 단계를 포함할 수 있다. 전자 장치는 단계S730에서 제1 상태 정보, 제1 행동 정보, 및 제1 보상 정보를 기초로 인공지능 기반 모델 을 훈련시킬 수 있다. 전자 장치는 단계S730이후 위성의 행동 단위 시간 마다 임의로 변경되는 제2 상태 정보를 훈련된 인공지능 기반 모델에 입력하여 훈련된 인공지능 기반 모델의 출력인 위성의 제2 행동 정보를 확 인하고, 제2 행동 정보에 기초하여 행동이 제어된 위성의 라우팅 경로에 제2 보상 정보를 확인하고, 제2 상태 정보, 제2 행동 정보, 및 제2 보상 정보를 기초로 훈련된 인공지능 기반 모델을 더 훈련시킬 수 있다. 예를 들 어 제2 상태 정보는, 위성의 행동 단위 시간 마다 임의로 변경되는 제2 정보를 포함할 수 있고, 제2 정보는 위 성의 행동 단위 시간 마다 격자 단위로 임의로 변경되는 링크 단절 위치에 관한 정보를 포함할 수 있다. 이 때 의 제2 상태 정보는, 제1 상태 정보와는 독립적으로 위성의 행동 단위 시간 마다 임의로 변경되는 것일 수 있다. 이와 같이 전자 장치는 제1 상태 정보, 제1 행동 정보, 및 제1 보상 정보를 기초로 인공지능 기반 모델을 훈련시킬 수 있다. 이후 전자 장치는 또한 제1 상태 정보와는 독립적으로 결정된 제2 상태 정보, 제2 행동 정보, 및 제2 보상 정보를 기초로 인공지능 기반 모델을 더 훈련시켜 더욱 강인한 학습 효과를 제공할 수 있다. 도 8은 일 실시 예에 따른 전자 장치의 구성을 나타낸 블록도이다. 도 8을 참조하면, 전자 장치는 일 실시 예에 따라, 메모리 및 프로세서(processor)를 포함할 수 있다. 도 8에 도시된 전자 장치는 본 실시 예와 관련된 구성요소들만이 도시되어 있다. 따라서, 도 8에 도"}
{"patent_id": "10-2023-0101545", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "시된 구성요소들 외에 다른 범용적인 구성요소들이 더 포함될 수 있음을 본 실시 예와 관련된 기술분야에서 통 상의 지식을 가진 자라면 이해할 수 있다. 예를 들어, 전자 장치는 하나 이상의 트랜시버(transceiver)를 포함하는 통신부(communication device) (미도시)를 포함할 수 있다. 통신부는 유/무선 통신을 수행하기 위한 장치로서, 외부의 전자 장치와 통신할 수 있다. 외부의 전자 장치는 단말 또는 서버가 될 수 있다. 또한, 통신부가 이용하는 통신 기술에는 GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), LTE(Long Term Evolution), 5G, WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), 블루투스(Bluetooth쪠), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), ZigBee, NFC(Near Field Communication) 등이 있을 수 있다. 프로세서는 전자 장치의 전반의 동작을 제어하고 데이터 및 신호를 처리할 수 있다. 프로세서는 적어도 하나의 하드웨어 유닛으로 구성될 수 있다. 또한, 프로세서는 메모리에 저장된 프로그램 코드 를 실행하여 생성되는 하나 이상의 소프트웨어 모듈에 의해 동작할 수 있다. 프로세서는 프로세서 및 메모 리를 포함할 수 있는 바, 프로세서는 메모리에 저장된 프로그램 코드를 실행하여 전자 장치의 전반의 동작 을 제어하고 데이터 및 신호를 처리할 수 있다. 또한 실시 예에서 프로세서는 제어부(controller)에 포함 될 수 있다. 예를 들어 프로세서는, 위성의 행동 단위 시간 마다 임의로 변경되는 제1 상태 정보를 소정의 인공지능 기 반 모델에 입력하여 소정의 인공지능 기반 모델의 출력인 위성의 제1 행동 정보를 확인하고, 제1 행동 정보에 기초하여 행동이 제어된 위성의 라우팅 경로에 제1 보상 정보를 획득하고, 및 제1 상태 정보, 제1 행동 정보, 및 제1 보상 정보를 기초로 인공지능 기반 모델을 훈련시키도록 전자 장치를 제어할 수 있으며, 예를 들어 제1 상태 정보는, 위성의 행동 단위 시간 마다 임의로 변경되는 제1 정보를 포함할 수 있다. 전술한 실시 예들에 따른 전자 장치는 프로세서, 프로그램 데이터를 저장하고 실행하는 메모리, 디스크 드라이 브와 같은 영구 저장부(permanent storage), 외부 장치와 통신하는 통신 포트, 터치 패널, 키(key), 버튼 등과 같은 사용자 인터페이스 장치 등을 포함할 수 있다. 소프트웨어 모듈 또는 알고리즘으로 구현되는 방법들은 상 기 프로세서상에서 실행 가능한 컴퓨터가 읽을 수 있는 코드들 또는 프로그램 명령들로서 컴퓨터가 읽을 수 있 는 기록 매체 상에 저장될 수 있다. 여기서 컴퓨터가 읽을 수 있는 기록 매체로 마그네틱 저장 매체(예컨대, ROM(read-only memory), RAM(random-Access memory), 플로피 디스크, 하드 디스크 등) 및 광학적 판독 매체(예 컨대, 시디롬(CD-ROM), 디브이디(DVD: Digital Versatile Disc)) 등이 있다. 컴퓨터가 읽을 수 있는 기록 매 체는 네트워크로 연결된 컴퓨터 시스템들에 분산되어, 분산 방식으로 컴퓨터가 판독 가능한 코드가 저장되고 실 행될 수 있다. 매체는 컴퓨터에 의해 판독가능하며, 메모리에 저장되고, 프로세서에서 실행될 수 있다. 본 실시 예는 기능적인 블록 구성들 및 다양한 처리 단계들로 나타내어질 수 있다. 이러한 기능 블록들은 특정 기능들을 실행하는 다양한 개수의 하드웨어 또는/및 소프트웨어 구성들로 구현될 수 있다. 예를 들어, 실시 예 는 하나 이상의 마이크로프로세서들의 제어 또는 다른 제어 장치들에 의해서 다양한 기능들을 실행할 수 있는, 메모리, 프로세싱, 로직(logic), 룩 업 테이블(look-up table) 등과 같은 직접 회로 구성들을 채용할 수 있다. 구성 요소들이 소프트웨어 프로그래밍 또는 소프트웨어 요소들로 실행될 수 있는 것과 유사하게, 본 실시 예는 데이터 구조, 프로세스들, 루틴들 또는 다른 프로그래밍 구성들의 조합으로 구현되는 다양한 알고리즘을 포함하 여, C, C++, 자바(Java), 어셈블러(assembler) 등과 같은 프로그래밍 또는 스크립팅 언어로 구현될 수 있다. 기능적인 측면들은 하나 이상의 프로세서들에서 실행되는 알고리즘으로 구현될 수 있다. 또한, 본 실시 예는 전자적인 환경 설정, 신호 처리, 및/또는 데이터 처리 등을 위하여 종래 기술을 채용할 수 있다. \"매커니즘\", \"요소\", \"수단\", \"구성\"과 같은 용어는 넓게 사용될 수 있으며, 기계적이고 물리적인 구성들로서 한정되는 것은 아니다. 상기 용어는 프로세서 등과 연계하여 소프트웨어의 일련의 처리들(routines)의 의미를 포함할 수 있다. 전술한 실시 예들은 일 예시일 뿐 후술하는 청구항들의 범위 내에서 다른 실시 예들이 구현될 수 있다. 도면 도면1 도면2 도면3 도면4 도면5a 도면5b 도면6 도면7 도면8"}
{"patent_id": "10-2023-0101545", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시 예에 따른 위성의 제어 시스템을 나타내기 위한 도면이다. 도 2는 일 실시 예에 따른 위성의 제어를 위한 학습 방법에 사용되는 임의 보상 모델의 개념을 설명하기 위한 도면이다. 도 3은 일 실시 예에 따른 위성의 제어를 위한 학습 방법에 사용되는 Dueling DQN 기법을 설명하기 위한 도면이 다. 도 4는 일 실시 예에 따른 위성의 제어를 위한 학습 방법을 설명하기 위한 개념도이다. 도 5a 및 도 5b는 일 실시 예에 따른 위성의 제어를 위한 학습 방법에 사용되는 Grid MDP 기반 동적 네트워크 모델을 설명하기 위한 도면들이다. 도 6은 일 실시 예에 따른 위성의 제어를 위한 학습 방법에서 Dueling DQN 을 이용하여 품질 값을 획득하는 과 정을 설명하기 위한 흐름도이다. 도 7은 일 실시 예에 따른 전자 장치의 동작 방법을 도시한다. 도 8은 일 실시 예에 따른 전자 장치의 구성을 나타낸 블록도이다."}
