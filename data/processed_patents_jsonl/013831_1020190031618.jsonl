{"patent_id": "10-2019-0031618", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0129698", "출원번호": "10-2019-0031618", "발명의 명칭": "순환신경망을 압축하는 전자장치 및 그 방법", "출원인": "삼성전자주식회사", "발명자": "칠코바 나데즈다 알렉산드로브나"}}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "순환신경망(Recurrent Neural Network)을 압축하는 방법에 있어서,순환신경망의 입력요소(input element)에 관한 제1 곱셈변수(multiplicative variable)를 획득하는 단계;상기 순환신경망의 입력 뉴런(input neuron) 및 은닉 뉴런(hidden neuron)에 관한 제2 곱셈변수를 획득하는 단계;상기 순환신경망의 가중치(weight), 상기 제1 곱셈변수 및 상기 제2 곱셈변수에 대한 평균값(mean) 및 분산값(variance)을 획득하는 단계;상기 평균값(mean) 및 상기 분산값(variance)을 바탕으로 상기 순환신경망에 대해 희박화(sparsification)를 수행하는 단계;를 포함하는 방법."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 희박화를 수행하는 단계는,상기 평균값(mean) 및 상기 분산값(variance)을 바탕으로, 상기 희박화를 수행하기 위한 관련값을 계산하는 단계;상기 관련값이 기 설정된 값 보다 작은 가중치, 제1 곱셈변수 또는 제2 곱셈변수를 0으로 설정하는 단계;를 더포함하는 방법."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 관련값은 상기 평균값의 제곱에 대한 상기 분산값의 비율값(ratio of square of mean to variance)인 방법."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 기 설정된 값은 0.05인 방법."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 순환신경망이 게이트 구조(gated structure)를 포함하는 경우,상기 순환신경망의 순환레이어(recurrent layer)의 게이트(gate) 및 정보 흐름(information flow)요소를 일정하게 만들기 위해 게이트의 사전활성(preactivation)에 관한 제3 곱셈변수를 획득하는 단계;를 더 포함하고,공개특허 10-2019-0129698-2-상기 평균값(mean) 및 분산값(variance)을 획득하는 단계는,상기 순환신경망의 가중치, 상기 제1 곱셈변수, 상기 제2 곱셈변수 및 상기 제3 곱셈변수에 대한 평균값 및 분산값을 획득 하는 단계;를 더 포함하는 방법."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 게이트 구조는 상기 순환신경망의 LSTM(Long-Short term Memory)계층으로 구현되는 방법."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 평균값(mean) 및 분산값(variance)을 획득하는 단계는,상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수에 대한 평균값(mean) 및 분산값(variance)을 초기화 하는 단계;상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수의 상기 평균값(mean) 및 상기 분산값(variance)과 관련된 객체(objective)를 최적화 하여, 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수에 대한 상기 평균값(mean) 및 상기 분산값(variance)을 획득하는 단계;를 더 포함하는 방법."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 획득하는 단계는,객체(objective)들의 미니배치(mini batch)를 선택하는 단계;근사 사후 분포(approximated posterior distribution)로부터 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수를 생성하는 단계;상기 생성된 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수를 바탕으로 상기 미니배치를 이용하여 상기순환신경망을 순방향 통과(forward pass)시키는 단계;상기 객체(objective)를 계산하고, 상기 객체(objective)에 대한 그래디언트(gradient)를 계산하는 단계;상기 계산된 그래디언트를 바탕으로 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수에 대한 상기 평균값(mean) 및 상기 분산값(variance)을 획득하는 단계;를 더 포함하는 방법."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 가중치는 미니배치에 의해 생성되고, 상기 제1 그룹변수 및 상기 제2 그룹변수는 상기 객체(objective)로부터 개별적으로 생성되는 방법."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,공개특허 10-2019-0129698-3-상기 입력요소(input element)는 어휘(vocabulary) 또는 단어(word)인 방법."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "순환신경망(Recurrent Neural Network)을 압축하는 전자장치에 있어서,적어도 하나의 인스트럭션(instruction)을 포함하는 메모리,상기 적어도 하나의 인스트럭션을 제어하는 프로세서, 를 포함하고,상기 프로세서는,상기 순환신경망의 입력요소(input element)에 관한 제1 곱셈변수(multiplicative variable)를 획득하고,상기 순환신경망의 입력 뉴런(input neuron) 및 은닉 뉴런(hidden neuron)에 관한 제2 곱셈변수를 획득하고,상기 순환신경망의 가중치, 상기 제1 곱셈변수 및 상기 제2 곱셈변수에 대한 평균값(mean) 및 분산값(variance)을 획득하고,상기 평균값(mean) 및 상기 분산값(variance)을 바탕으로 상기 순환신경망에 대해 희박화(sparsification)를 수행하는 전자장치."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 프로세서는,상기 평균값(mean) 및 상기 분산값(variance)을 바탕으로, 희박화를 수행하기 위한 관련값을 계산하고,상기 관련값이 기 설정된 값 보다 작은 가중치, 제1 곱셈변수 또는 제2 곱셈변수를 0으로 설정하여 상기 희박화를 수행하는 전자장치"}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 관련값은 상기 평균값의 제곱에 대한 상기 분산값의 비율값(ratio of square of mean to variance)인 전자장치."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서,상기 기 설정된 값은 0.05인 전자장치"}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 순환신경망이 게이트 구조(gated structure)를 포함하는 경우,상기 프로세서는,상기 순환신경망의 순환레이어(recurrent layer)의 게이트(gate)및 정보 흐름(information flow)요소를 일정하게 만들기 위해 게이트의 사전활성(preactivation)에 관한 제3 곱셈변수를 획득하고,공개특허 10-2019-0129698-4-상기 가중치, 상기 제1 곱셈변수, 상기 제2 곱셈변수 및 상기 제3 곱셈변수에 대한 평균값(mean) 및 분산값(variance)을 획득하고,상기 평균값(mean) 및 상기 분산값(variance)을 바탕으로 상기 순환신경망에 대해 상기 희박화를 수행하는 전자장치."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 게이트 구조는 상기 순환신경망의 LSTM(Long-Short term Memory)계층으로 구현되는 전자장치."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항에 있어서,상기 프로세서는,상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수에 대한 평균값(mean) 및 분산값(variance)을 초기화 하고,상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수의 상기 평균값(mean) 및 상기 분산값(variance)과 관련된 객채(objective)를 최적화 하여, 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수에 대한 상기 평균값(mean) 및 상기 분산값(variance)을 획득하는 전자장치."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 프로세서는,객체(objective)들의 미니배치(mini batch)를 선택하고,근사 사후 분포(approximated posterior distribution)로부터 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수를 생성하고,상기 생성된 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수를 바탕으로 상기 미니배치를 이용하여 상기순환신경망을 순방향 통과(forward pass)시키고,상기 객체(objective)를 계산하고, 상기 객체(objective)에 대한 그라디언트(gradient)를 계산하고, 상기 계산된 그래디언트(gradient)를 바탕으로 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수에 대한상기 평균값(mean) 및 상기 분산값(variance)을 획득하는 전자 장치."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 가중치는 미니배치(mini batch)에 의해 생성되고, 상기 제1 그룹변수 및 상기 제2 그룹변수는 상기 객체(objective)로부터 개별적으로 생성되는 전자장치."}
{"patent_id": "10-2019-0031618", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제11항에 있어서,공개특허 10-2019-0129698-5-상기 입력요소(input element)는 어휘(vocabulary) 또는 단어(word)인 전자장치."}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "순환신경망(Recurrent Neural Network)을 압축하는 전자 장치 및 방법이 개시된다. 본 개시에 따른 전자 장치 및 방법은 순환신경망에 대한 희박화 기법(sparsification technique)을 이용하며, 제1 곱셈변수 내지 제3 곱셈변수 를 획득하여 순환신경망을 학습하고, 순환신경망에 대한 희박화를 수행하여 순환신경망을 압축 한다."}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 순환신경망을 압축하는 전자장치 및 그 방법에 관한 것으로, 보다 상세하게는 사용자 단말과 같은 전 자 장치에서 순환신경망 인공지능 모델을 효율적으로 사용하기 위한 전자장치 및 그 방법에 관한 것이다."}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(Artificial Intelligence, AI) 시스템은 인간 수준의 지능을 구현하는 컴퓨터 시스템이며, 기존 규칙 기반 스마트 시스템과 달리 기계가 스스로 학습하고 판단하며 똑똑해지는 시스템이다. 인공 지능 시스템은 사용 할수록 인식률이 향상되고 사용자 취향을 보다 정확하게 이해할 수 있게 되어, 기존 규칙 기반 스마트 시스템은 점차 딥러닝 기반 인공 지능 시스템으로 대체되고 있다. 인공 지능 기술은 기계학습(딥러닝) 및 기계 학습을 활용한 요소 기술들로 구성된다. 기계 학습은 입력 데이터들의 특징을 스스로 분류/학습하는 알고리즘 기술이며, 요소 기술은 딥러닝 등의 기계 학습 알고리즘을 활용하는 기술로서, 언어적 이해, 시각적 이해, 추론/예측, 지식 표현, 동작 제어 등의 기술 분야로 구성된다. 인공 지능 기술이 응용되는 다양한 분야는 다음과 같다. 언어적 이해는 인간의 언어/문자를 인식하고 응용/처리 하는 기술로서, 자연어 처리, 기계 번역, 대화 시스템, 질의 응답, 음성 인식/합성 등을 포함한다. 시각적 이해 는 사물을 인간의 시각처럼 인식하여 처리하는 기술로서, 객체 인식, 객체 추적, 영상 검색, 사람 인식, 장면 이해, 공간 이해, 영상 개선 등을 포함한다. 근래에는 순환신경망(recurrent neural network)을 이용한 인공지능 모델을 사용하여 언어모델링 작업(자연어처 리, 음성인식, 질의응답 등을 수행하기 위한 모델링 작업) 등을 수행하고 있다. 종래의 순환신경망 모델은 많은 수의 매개변수(Parameters)를 사용하기 때문에 많은 학습 시간 및 큰 저장 공간 을 필요로 하였다. 따라서, 종래의 순환신경망 모델의 학습 등은 큰 저장 공간 및 높은 연산 수행이 가능한 외 부 서버에서 이루어지는 경우가 많았으며, 스마트폰과 같은 제한된 메모리의 휴대용 장치 등에서 효율적으로 순 환신경망 인공지능 모델을 이용하기 위한 방법에 대한 논의가 필요하게 되었다."}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 이와 같은 문제점을 해결하기 위해 안출된 것으로, 본 개시의 목적은 순환신경망에 있어 베이지안 희 박화(sparsification) 기법을 이용해 순환신경망을 압축하는 전자 장치 및 그 방법을 제공함에 있다."}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 순환신경망을 압축하는 방법은 순환신경망의 입력요소(input element)에 관한 제1 곱셈변수(multiplicative variable)를 획득하는 단계, 상기 순환신경망의 입력 뉴런(input neuron) 및 은닉 뉴 런(hidden neuron)에 관한 제2 곱셈변수를 획득하는 단계, 상기 가중치(weight), 상기 제1 곱셈변수 및 상기 제 2 곱셈변수에 대한 평균값(mean) 및 분산값(variance)을 획득하는 단계, 상기 평균값 및 상기 분산값을 바탕으 로 상기 순환신경망에 대해 희박화(sparsification)를 수행하는 단계,를 포함하며, 상기 희박화를 수행하는 단 계는, 상기 평균값(mean) 및 상기 분산값(variance)을 바탕으로, 상기 희박화를 수행하기 위한 관련값을 계산하 는 단계, 상기 관련값이 기 설정된 값 보다 작은 가중치, 제1 곱셈변수 및 제2 곱셈변수를 0으로 설정하는 단계,를 더 포함 할 수 있다. 이때, 상기 관련값은 상기 평균값의 제곱에 대한 상기 분산값의 비율값(ratio of square of mean to varianc e)일 수 있다.이때, 상기 기 설정된 값은 0.05일 수 있다. 상기 순환신경망이 게이트 구조(gated structure)를 포함하는 경우, 상기 순환신경망의 순환레이어(recurrent layer)의 게이트(gate)및 정보 흐름(information flow)요소를 일정하게 만들기 위해 게이트의 사전활성 (preactivation)에 관한 제3 곱셈변수를 획득하는 단계,를 더 포함하고, 상기 평균값(mean) 및 분산값 (variance)을 획득하는 단계는, 상기 가중치, 상기 제1 곱셈변수, 상기 제2 곱셈변수 및 상기 제3 곱셈변수에 대한 평균값 및 분산값을 획득하는 단계;를 더 포함할 수 있다. 이때, 상기 게이트 구조는 상기 순환신경망의 LSTM(Long-Short term Memory)계층으로 구현될 수 있다. 이때, 상기 평균값(mean) 및 분산값(variance)을 획득하는 단계는, 상기 가중치, 상기 제1 그룹변수 및 상기 제 2 그룹변수에 대한 평균값 및 분산값을 초기화 하는 단계, 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변 수의 상기 평균값 및 상기 분산값과 관련된 객체(objective)를 최적화 하여, 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수에 대한 상기 평균값 및 상기 분산값을 획득하는 단계,를 더 포함할 수 있다. 이때, 상기 획득하는 단계는, 객체(objective)들의 미니배치를 선택하는 단계, 근사 사후 분포(approximated posterior distribution)로부터 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수를 생성하는 단계, 상기 생성된 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수를 바탕으로 상기 미니배치를 이용하여 상기 순환 신경망을 순방향 통과(forward pass)시키는 단계, 상기 객체(objective)를 계산하고, 상기 객체(objective)에 대한 그래디언트(gradient)를 계산하는 단계, 상기 계산된 그래디언트를 바탕으로 상기 가중치, 상기 제1 그룹 변수 및 상기 제2 그룹변수에 대한 상기 평균값 및 상기 분산값을 획득하는 단계,를 더 포함할 수 있다. 이때, 상기 가중치는 미니배치에 의해 생성되고, 상기 제1 그룹변수 및 상기 제2 그룹변수는 상기 객체로부터 개별적으로 생성될 수 있다. 이때, 상기 입력 요소는 어휘(vocabulary) 또는 단어(word)일 수 있다. 한편, 상술한 목적을 달성하기 위한 본 개시의 실시 예에 따른 순환신경망을 압축하는 전자장치는, 적어도 하나 의 인스트럭션(instruction)을 포함하는 메모리, 상기 적어도 하나의 인스트럭션을 제어하는 프로세서, 를 포함 하고, 상기 프로세서는, 상기 순환신경망의 입력요소(input element)에 관한 제1 곱셈변수(multiplicative variable)를 획득하고, 상기 순환신경망의 입력 뉴런(input neuron) 및 은닉 뉴런(hidden neuron)에 관한 제2 곱셈변수를 획득하고, 상기 순환신경망의 가중치, 상기 제1 곱셈변수 및 상기 제2 곱셈변수에 대한 평균값 (mean) 및 분산값(variance)을 획득하고, 상기 평균값(mean) 및 상기 분산값(variance)을 바탕으로 상기 순환신 경망에 대해 희박화(sparsification)를 수행한다. 그리고, 상기 프로세서는, 상기 평균값(mean) 및 상기 분산값(variance)을 바탕으로, 희박화를 수행하기 위한 관련값을 계산하고, 상기 관련값이 기 설정된 값 보다 작은 가중치, 제1 곱셈변수 또는 제2 곱셈변수를 0으로 설정하여 희박화를 수행할 수 있다. 또한, 상기 관련값은 상기 평균값의 제곱에 대한 상기 분산값의 비율값(ratio of square of mean to varianc e)일 수 있으며, 상기 기설정된 값은 0.05일 수 있다. 그리고, 상기 순환신경망이 게이트 구조(gated structure)를 포함하는 경우, 상기 프로세서는, 상기 순환신경망 의 순환레이어(recurrent layer)의 게이트(gate)및 정보 흐름(information flow)요소를 일정하게 만들기 위해 게이트의 사전활성(preactivation)에 관한 제3 곱셈변수를 획득하고, 상기 순환신경망의 가중치, 상기 제1 곱셈 변수, 상기 제2 곱셈변수 및 상기 제3 곱셈변수에 대한 평균값(mean) 및 분산값(variance)을 획득하고, 상기 평 균값(mean) 및 상기 분산값(variance)을 바탕으로 상기 순환신경망에 대해 희박화를 수행할 수 있다. 또한, 상기 게이트 구조는 상기 순환신경망의 LSTM(Long-Short term Memory)계층으로 구현될 수 있다. 그리고, 상기 프로세서는, 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수에 대한 평균값(mean) 및 분산 값(variance)을 초기화 하고, 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수의 상기 평균값(mean) 및 상기 분산값(variance)과 관련된 객채(objective)를 최적화 하여, 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수에 대한 상기 평균값(mean) 및 상기 분산값(variance)을 획득할 수 있다. 또한, 상기 프로세서는, 객체(objective)들의 미니배치(mini batch)를 선택하고, 근사 사후 분포(approximated posterior distribution)로부터 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수를 생성하고, 상기 생성 된 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수를 바탕으로 상기 미니배치를 이용하여 상기 순환신경망을 순방향 통과(forward pass)시키고, 상기 객체(objective)를 계산하고, 상기 객체(objective)에 대한 그래 디언트(gradient)를 계산하고, 상기 계산된 그래디언트(gradient)를 바탕으로 상기 가중치, 상기 제1 그룹변수 및 상기 제2 그룹변수에 대한 상기 평균값(mean) 및 상기 분산값(variance)을 획득할 수 있다. 그리고, 상기 가중치는 미니배치(mini batch)에 의해 생성되고, 상기 제1 그룹변수 및 상기 제2 그룹변수는 개 별적인 상기 객체(objective)로부터 생성될 수 있다. 또한, 상기 입력요소(input element)는 어휘(vocabulary) 또는 단어(word)일 수 있다."}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상과 같은 본 개시의 다양한 실시 예 들에 따르면, 희박화 기법을 이용해 순환신경망 인공지능 모델을 압축함 으로 언어모델링 작업을 가속화 할 수 있으며, 제한된 메모리의 휴대용 장치 등에서도 순환신경망 인공지능 모 델을 이용한 언어모델링 작업을 수행할 수 있다."}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 문서의 다양한 실시 예가 첨부된 도면을 참조하여 기재된다. 그러나, 이는 본 문서에 기재된 기술을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 문서의 실시예의 다양한 변경(modifications), 균등물 (equivalents), 및/또는 대체물(alternatives)을 포함하는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조 부호가 사용될 수 있다. 또한, 본 문서에서 사용된 \"제 1,\" \"제 2,\" 등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없 이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들을 한정하지 않는 다. 예를 들면, 제 1 사용자 기기와 제 2 사용자 기기는, 순서 또는 중요도와 무관하게, 서로 다른 사용자 기기 를 나타낼 수 있다. 예를 들면, 본 문서에 기재된 권리 범위를 벗어나지 않으면서 제 1 구성요소는 제 2 구성요 소로 명명될 수 있고, 유사하게 제 2 구성요소도 제 1 구성요소로 바꾸어 명명될 수 있다. 어떤 구성요소(예: 제 1 구성요소)가 다른 구성요소(예: 제 2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결 되어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제 3 구성요소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제 1 구성요소)가 다 른 구성요소(예: 제 2 구성요소)에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 상기 어 떤 구성요소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제 3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 문서에서 사용된 용어들은 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 다른 실시예의 범위를 한정 하려는 의도가 아닐 수 있다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 문서에 기재된 기술 분야에서 통상 의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 문서에 사용된 용어들 중 일반적인 사전에 정의된 용어들은, 관련 기술의 문맥상 가지는 의미와 동일 또는 유사한 의미로 해석될 수 있으 며, 본 문서에서 명백하게 정의되지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 경우에따라서, 본 문서에서 정의된 용어일지라도 본 문서의 실시 예들을 배제하도록 해석될 수 없다. 도 1은 본 개시의 일 실시 예에 따른, 전자장치의 구성을 간략히 도시한 블록도 이다. 도 1의 전자 장치는 순환신경망 인공지능 모델을 희박화하여 압축하는 장치의 일 실시 예를 나타낸다. 도 1의 전자 장치의 구체적인 설명에 앞서, 순환 신경망 인공지능 모델과 관련된 각종 용어들에 대하여 먼저 설명한다. 본 개시에서 레이어(Layer)라 함은 다층 신경망 구조에서 하나의 층을 의미할 수 있다. 즉, 인공지능 모델에서 하나의 층을 레이어라 정의할 수 있다. 일반적으로 레이어는 입력 벡터 x와 출력 벡터 y에 대하여 y=f(Wx+b)의 형태로 정의 될 수 있다. 이때, W 및 b는 레이어 파라미터라고 할 수 있다. 본 개시에서 레이어 함수(Layer Function)라 함은, 상기 레이어의 정의(y=f(Wx+b))를 일반화 하여 임의의 함수 g로 표현한 것을 의미할 수 있다. 예를 들어, 레이어 함수는 y = g(x) 혹은 y_j= g(x1, x2, ... x_N)와 같이 표현될 수 있다. 이때, 레이어 함수 g는 입력 x1,..., x_N의 선형 함수로 표현되나, 다른 형태로 표현도 가능함 은 물론이다. [베이지안 신경망] 베이지안 신경망에서는, 신경망의 가중치(weight)를 정해진 값으로 취급하는 것이 아닌 임의의 변수로 취급하여 인공지능 모델을 학습할 수 있다. 또한, 베이지안 신경망에서는 신경망의 가중치의 확률분포를 이용하여 인공지 능 모델을 학습할 수 있다. 즉, 가중치를 확률 분포 함수로 보는 것이다. 베이지안 신경망에서는 사전분포(prior distribution) 및 근사사후분포(approximated posterior distributio n)를 이용하여 인공지능 모델을 학습할 수 있다. 구체적으로 사전 분포(prior distribution)는 인공지능 모델의 학습 이전 가중치들의 예상 분포를 나타내며, 근사 사후분포는 인공지능 모델의 학습 이후 가중치들의 예상 분 포를 나타낸다. 근사 사후 분포(approximated posterior distribution)가 실제 사후 분포(true posterior distribution)에 가까워질수록 인공지능 모델의 성능이 좋음을 의미한다. 란 A의 사전분포로 현재 우리가 알 수 있는 A에 대한 분포도를 의미한다. 즉, 란 가중치행렬 W 에 대한 사전분포를 뜻하며, 현재 알 수 있는 가중치 행렬 W에 대한 분포도를 의미한다. 란 사건 A라는 증거에 대한 사후분포를 의미하며, 구체적으로 사건 A가 발생 한 경우, 사건 A가 사건 B 로부터 발생한 것이라고 생각되는 조건부 확률분포를 의미한다. 즉, 는 가 주어졌을때 가중치행렬 W에 대한 사후분포를 뜻한다. 베이지안 신경망에서는 입력 데이터가 들어올 때 마다 인공지능 모델을 학습하여 사전분포 및 사후분포를 획득 함으로 근사 사후 분포의 정확성을 높일 수 있다. 입력요소 에 대한 출력(목표)요소 의 의존성을 나타내는 가중치W를 갖는 베이지안 신경망에서, 가중치 W는 임의의 변수로 취급될 수 있다. 베이지안 신경망 에서는 사전분포 로부터 사 후분포 를 추론할 수 있다. 순환신경망(recurrent neural network) 에서 실제사후분포(true posterior)를 측정하기 어렵지만, 사후분포를 어떠한 파라매틱 분포(some parametric distribution)를 이루는 근사 사후분포 로 근사화 될 수 있다. 이 근사 사후 분포의 품질은 KL-divergence 인 에 의해 측정될 수 있다. 최적의 매개변수(optimal parameter) 는 에 대한 변이하한값(variational lower bound)을 최대화 함으로 구 할 수 있다. 여기서 변이하한값(variational lower bound)이란 실제 사후분포와 근사 사후분포 사이의 거리인 KL divergence( )를 최소화 시키기 위해 도입되는 개념이다.[수학식 1]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식1의 은 변이하한값(variational lower bound)을 의미하며, log-likelihood 항( )은 일반적 으로 Monte-Carlo 기법에 따라 근사화 될 수 있다. Monte-Carlo 기법에서 편향(bias)을 없애기 위해 가중치는 와 같은 함수로 매개변수화 될 수 있다. 여기서 는 일부 비 매개변수 분포(non-parametric distribution) 로부터 얻어질 수 있다. 수학식1의 KL-divergence항( )은 regularizer 역할을 하며, 일반적으로 계산되거나 분석적으로 근 사화 될 수 있다. 베이지안 신경망에 대한 희박화(sparsification) 기법의 이점은 pruning-based 기법과 비교 할때 초매개변수 (hyperparameters)가 적다는 점이다. 또한 베이지안 신경망에 대한 희박화 기법은 pruning-based 기법에 비해 더 높은 희박화 레벨(sparsity level)을 제공할 수 있다. [Sparse Variational Dropout(Sparse VD)] 드롭아웃(Dropout)은 신경망의 정규화(regularization)를 위한 표준기술이다. 드롭아웃은 각 레이어에 무작위로 생성된 노이즈(noise) 벡터를 곱하는 것을 의미한다. 노이즈 벡터의 요소는 cross-validation를 이용한 조정된 매개변수(parameter)를 사용하여 Bernoulli분포 또는 normal분포로부터 생성될 수 있다. 본 개시의 일 실시 예에 따른 노이즈가 획득되는 경우, 가중치의 사전 분포 및 사후분포는 각 가중치의 레이어 함수에 노이즈를 곱하여 나타난 레이어 함수에 대한 사전 분포 및 사후분포가 될 수 있다. 입력 크기(size)가 n이고, 출력 크기(size)가 m이며, 가중치 행렬W를 가지는 피드 포워드(feed-forward)신경망 의 한 완전연결(fully-connected) 레이어에 대한 가중치의 사전분포는 fully factorized log-uniform 분포로 로 표현 되며, 사후분포는 fully factorized normal분포로 수학식 2 와 같이 표현 될 수 있다. [수학식 2]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "사후분포는 수학식 3 또는 수학식 4와 같이 가중치에 정상노이즈(normal noise)를 곱하거나 더함으로 얻어질 수 있다. [수학식 3]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "[수학식 4]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수학식 4는 부가적인 재매개변수화(additive reparameterization)라고 불린다. 정상노이즈(normal noise)는 에 관한 변이하한값(variational lower bound) 의 그래디언트(gradients)의 분산(variance)을 줄일 수 있 다. 여기서, 는 에 대한 평균값을 의미하고, 은 에 대한 분산값을 의미한다. 그래디언트(gradients)란 각 뉴런의 입력값에 대한 손실함수(loss function)의 편미분을 계산하여 나타나는 값 으로, 손실함수(loss function)는 출력값에 대한 예측값과 실제값의 차이를 계산하기 위해 이용되는 함수로 자 세한 내용은 생략하도록 한다.정규분포(normal distributions)의 합은 계산가능한 파라미터(computable parameters)가 있는 정규분포 이므로, 가중치 대신 사전활성(preactivation)(입력 벡터에 가중치행렬 W를 곱한값)에 노이즈(noise)가 적용될 수 있다. 위 기법을 로컬 재매개변수화 기법(local reparameterization trick)이라고 부른다. 로컬 재매개변수 화 기법은 그래디언트(gradients)의 분산(variance)을 효율적으로 줄이고, 신경망에 대한 학습을 효율적으로 수 행할 수 있게 한다. Sparse VD 기법에서 {Θ, log σ}와 관련된 변이하한값(the variational lower bound)에 대한 최적화가 수행될 수 있다. KL-divergence항은 각각의 가중치를 인수분해(factorize)할 수 있으며, 각각의 가중치는 수학식 5와 같이 에만 의존될 수 있다. [수학식 5]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "KL-divergence항은 수학식 6의 우항과 같이 근사될 수 있다. [수학식 6]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "KL-divergence항에서 ->∞ 이면, 가중치에 대한 사후분포는 고분산정규분포(high-variance normal distribution)의 형태로 나타날 수 있다.. 사후분포의 정확성을 위해 =0, 즉 으로 설정할 수 있다. 그 결과 가중치의 사후분포는 zero-centered δ-function로 접근하고, 사후분포가 zero-centered δ- function로 접근한 가중치는 신경망의 출력에 영향이 없으므로 무시할 수 있다. [그룹희박화를 위한 SparseVD기법] 수학식 4 에서 SparseVD기법은 신경망 인공지능 모델의 그룹 희박화(group sparsity)를 수행하기 위해서도 적용 될 수 있다. 그룹 희박화에서는 가중치가 일부 그룹으로 나뉘어 지며, 그룹 희박화 기법은 개별적인 가중치 대 신 나뉘어진 그룹 단위의 가중치를 제거함으로 희박화를 수행하는 기법을 의미한다. 예로, 완전연결(fully-connected) 레이어에서 하나의 입력 뉴런에 해당하는 가중치의 그룹을 고려하면, 그룹 희 박화 기법을 수행하기 위해 각 그룹에 대해 제2 곱셈변수(multiplicative variable) 를 획득하여 부가하고, 와 같은 형태의 가중치를 이용해 신경망을 학습할 수 있다. 위 기법은 완전연결(fully-connected) 레이어에서 입력(input)레이어에 제2 곱셈변수를 부가하는 것과 동일하다. SparseVD 기법을 이용해, 으로 설정하여 와 관련된 뉴런을 제거함으로 신경망의 인공지능 모델에 대한 희박화를 수행할 수 있다. 에 대한 사전분포( ) 및 사후분포 ( )쌍은 기존의 SparseVD 기법에서와 동일하게 표현될 수 있다. 각각의 가중치 에 서는 학습가능한(learnable) 평균값(mean) 및 분산값(variance)을 가지는 정규사전(standard normal prior)분 포 및 정규근사사후(normal approximate posterior)분포를 사용할 수 있다. SparseVD 기법에서 각각의 가중치에 대한 사전분포(prior)는 을 0으로 유도(encourage)하며, 이는 각 그룹 의 평균값 가 0으로 설정되도록 할 수 있다. [순환신경망의 베이지안 희박화 기법(Bayesian Sparsification of recurrent neural networks)] 순환 신경망에서는 와 같은 배열(sequence)을 입력받아, 입력받은 배열을 은닉 레이어 (hidden states)에 매핑(maps)할 수 있다.. [수학식 7]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "수학식 7 은 순환신경망의 은닉(hidden)레이어에 대한 레이어 함수 를 나타낸다. 일반적인 신경망은 입력레이어에서 출력레이어로 한 방향으로만 흐르는 피드포워드(feedforward)신경망일 수 있 다. 순환신경망은 피드포워드 신경망과 비슷하지만, 출력값을 다시 입력값으로 받는 부분이 있다. 즉, 순환신경 망은 입력값을 받아 출력값을 만들고, 다시 출력값을 입력값으로 받을 수 있다. 각 순환신경망의 순환뉴런 은 수학식 7와 같이 두 개의 가중치행렬 와 을 포함할 수 있다. 는 입력배열 에 대응되는 가중치 행렬이며, 는 이전 타임스텝 t-1의 순환뉴런에 대한 출력인 에 대응되는 가중치 행렬이다. 는 타임스텝 t에서의 입력 배열 요소를 뜻하며, 구체적으로 타임스텝 t에서 모든 샘플의 입력값을 담고 있는 행렬이다. 는 입력 배열 요소 와 이전 타임스텝 t-1의 순환뉴런에 대한 출력인 에 의해 결정된다. 은 각 뉴런의 편향(bias)의 크기를 의미하는 편향 벡터이다. 는 와 의 함수이므로, 타임스텝 t=0에서부터 모든 입력에 대한 함수가 될 수 있다. 첫 번째 타임스 텝인 t=0에서는 이전의 출력이 없기 때문에 일반적으로 0으로 초기화 될 수 있다. [수학식 8]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "본 개시의 일 실시 예에 따르면 순환신경망의 출력값 y는 수학식 8과 같이 마지막 은닉 레이어에( )만 의존 할 수 있다. 수학식 7의 및 수학식 8의 는 비선형함수(nonlinear function)이다. 가중치에 대한 희박화를 수행하기 위해 순환신경망에 상술한 SparseVD기법을 적용할 수 있다. fully factorized log-uniform 분포를 가지는 사전분포가 SparseVD 기법을 이용한 순환신경망의 압축방법에 사 용될 수 있으며, [수학식 9]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "사후분포는 수학식 9와 같이 인 가중치에 대한 fully factorized normal분포로 근사화 될 수 있 다. 수학식 9의 및 는 수학식 4의 부가적인 재매개변수화(additive reparameterization)와 의미가 같을 수 있 다. [수학식 10] SparseVD 기법을 이용한 순환신경망 인공지능 모델의 학습결과 근사 변이하한값(lower bound approximation )이 최대화(maximized) 될 수 있다. SparseVD 기법을 이용한 순환신경망 인공지능 모델의 학습과정은, 먼저, 와 관련된 미니배치(mini- batch) 기법을 이용해 확률적 최적화(stochastic methods of optimization)가 수행된다. 미니배치 기법이란 인공지능 모델의 학습방법에 있어, 여러 학습 입력요소(예제)들의 그래디언트(gradients)를 동시에 계산하는 학습방법이다. 즉, 각 과정에서 전체 학습 요소나 하나의 샘플을 기반으로 그래디언트 (gradients)를 계산하는 것이 아닌, 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그래디언트(gradients)를 계산하는 것이다. 수학식 10의 적분(integral)항은 미니배치 기법당 하나의 샘플 로 추정될 수 있다. 비편향 적분추정 (unbiased integral estimation)을 위한 재매개변수화된 기법(reparameterization trick)과 그래디언트의 분산 값 감소(gradients variance reduction)를 위한 부가적인 재매개변수화(additive reparameterization )가 입력 -은닉 가중치(input-to-hidden weight, )와 은닉-은닉 가중치(hidden-to-hidden weight, )를 샘플링(생 성)하는데 사용된다. 로컬 재매개변수화 기법(local reparameterization trick)은 입력-은닉 가중치(input-to-hidden weight, ) 또는 은닉-은닉 가중치(hidden-to-hidden weight, )에 적용될 수 없다. [수학식 11]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "[수학식 12]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "3차원 노이즈(noise)는 많은 메모리 용량을 필요로 하므로, 수학식 11 및 수학식 12와 같이 하나의 노이즈 메트 릭스가 미니배치 기법에 의한 모든 객체(object)들에 대해 생성될 수 있다. 미니배치 기법에 의한 순환신경망의 학습 방법은 먼저, 입력-은닉 가중치(input-to-hidden weight, )또는 은 닉-은닉 가중치(hidden-to-hidden weight, )가 샘플링(생성)된다. 그 후, 수학식 10의 변이하한값(lower bound approximation)에 대해 와 관련하여 최적화가 수행된다. 그 후 KL-divergence항을 통해 순환신 경에 대한 희박화를 수행할 수 있으며, 다수의 가중치에 대한 사후분포가 zero-centered δ-function 형태로 얻 어질 수 있다. LSTM(Long-Short term Memory)계층의 경우에도 입력-은닉 가중치(input-to-hidden weight, )및 은닉-은닉 가중치(hidden-to-hidden weight, )에 대한 사전분포 및 사후분포 쌍이 사용되며, 위와 같은 희박화 기법이 동일하게 수행될 수 있다. LSTM(Long-Short term Memory)계층의 경우 입력-은닉(input-to-hidden weight)및 은닉-은닉(hidden-to-hidden weight) 행렬에 대한 노이즈가 게이트(gate)i, o, f 및 입력 조정(input modulation) g에 생성될 수 있다. [LSTM(Long-Short term Memory)계층에 대한 베이지안 그룹 희박화 수행 방법] 수학식 4에는 그룹가중치에 대한 노이즈 및 개별가중치에 대한 노이즈가 포함되어 있다. 일반적으로 많이 사용 되는 순환신경망은 압축 및 가속 수준을 향상시키기 위해 복잡한 게이트 구조(gated structure)를 갖는 LSTM계 층을 포함할 수 있다. LSTM계층에는 내부 메모리(internal memory) 를 포함하고, LSTM계층의 3개의 게이트 (i,o,f)는 내부 메모리 로부터 정보(information)를 업데이트(update),삭제(erasing) 및 릴리즈(releasing)할 수 있다. [수학식 13]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "[수학식 14]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "[수학식 15]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "수학식15는 내부메모리 에 대한 함수이고, 수학식 13 및 수학식 14는 각각 i, f, g, o에 대한 함수를 나타낸 다. i는 input gate, f는 forget gate, o는 out gate를 의미하며 자세한 내용은 생략하도록 한다. 본 개시의 일 실시 예에 따르면, 위 LSTM계층에 입력뉴런에 대한 제2 곱셈변수 및 은닉뉴런에 대한 제2 곱셈 변수 를 획득(도입)할 수 있다. 이에 더하여, 제3 곱셈변수 , , , 를 획득(도입)하여 각 게이트 (gate) i,f,o 및 정보흐름(information flow)g 에 부가할 수 있다. 제2 곱셈변수와 제3곱셈변수를 부가한 LSTM 계층을 포함하는 순환신경망 인공지능 모델은 다음과 같이 표현될 수 있다. [수학식 16]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "[수학식 17]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "[수학식 18]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "[수학식 19]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "[수학식 20]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "수학식 16 내지 수학식 20은 가중치 행렬의 행(column)뿐만 아니라 가중치 행렬의 열(row)에도 제2 곱셈변수 및 제3 곱셉변수를 부가한 것을 나타낸다. 예를 들어, 행렬에 대해 제2 곱셈변수 및 제3 곱셈변수 를 부가하면 와 같이 나타 낼 수 있다. LSTM계층을 포함하는 순환신경망 인공지능 모델에 대한 나머지 7개의 가중치 행렬 또한 위와 같이 나타낼 수 있다. 수학식 4 에서와 같이 제2 곱셈변수 및 가 0에 가까워 지면, 제2 곱셈변수에 대응되는 뉴런이 순환신경망 모델에서 제거 될 수 있다. 제3 곱셈변수 , , , 가 0에 가까워 지는 경우에는, 제3 곱셈변수에 대응되는 게이트 (gate)또는 정보흐 름(information flow)요소가 일정(constant)해질 수 있다. 게이트(gate) 또는 정보흐름(information flow)요소가 일정해진다는 의미는 게이트(gate)를 계산할 필요가 없다 는 뜻이며, 이 경우 LSTM계층의 순방향 통과(forward pass)가 가속화 될 수 있다. [수학식 21]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "[수학식 22]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 22, "content": "[수학식 23]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 23, "content": "수학식 21은 가중치행렬 에 대한 사전분포 및 사후분포를 나타낸 함수이며, 수학식 22는 제2 곱셈변수 에 대한 사전분포 및 사후분포를 나타낸 함수이며, 수학 식23은 제3 곱셈변수 에 대한 사전분포 및 사 후분포를 나타낸 함수이다. 본 개시의 일 실시 예에 따르면 개별 가중치에 대한 표준 정규(standard normal)분포 대신 log-uniform 사전분 포를 함으로 그룹변수들에 대한 희박화 기법이 향상될 수 있다. [자연 언어 처리를 위한 베이지안 압축 기법(Bayesian Compression for Natural Language Processing)] 자연 언어(Natural Language)처리 작업에서 순환신경망의 대부분의 가중치는 어휘(Vocabulary)와 연결된 첫 번 째 레이어에 집중될 수 있다. 그러나 순환신경망을 이용한 자연 언어 처리 작업의 경우 모든 단어가 필요하지는 않는다. 따라서 본 개시의 일 실시 예에 따르면 어휘 희박화(vocabulary sparsification)를 수행하기 위해 제1 곱셈변수 가 도입될 수 있다. 제1 곱셈변수가 순환신경망의 학습과정에서 0으로 설정되는 경우, 자연 언어 처리 작업에서 불필요한 단어가 필터링(제거) 될 수 있다. 자연 언어 처리작업에서 는 입력배열(input sequence)이며, y는 실제 출력값(true output)이 며, 는 순환신경망에 의해 예측되는 출력값을 의미한다. y 및 는 벡터의 배열(sequences of vectors)로 표현 될 수 있다. X 및 Y는 훈련세트 를 의미한다. 편향(bias)를 제외한 모든 순환신경망의 가 중치는 w로 표현될 수 있다. 편향(bias)에 대하여는 희박화를 수행하지 않고, 편향은 B로 표시된다. 본 개시의 일 실시 예에 따른 자연 언어 처리 작업을 위한 순환신경망 모델은 다음과 같이 이루어질 수 있다. 일때, 입력(embedding)레이어:"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 24, "content": "순환(recurrent)레이어:"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 25, "content": "완전연결(fully-connected )레이어:"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 26, "content": "위 순환신경망 모델에서 , 이며, 위 모델은 어느 순환신경망 구조(recurrent architecture)에도 직접 적용될 수 있다. 수학식 4 및 수학식 18에 따라, 가중치에 대한 fully-factorized log-uniform 분포를 이루는 사전분포 및 fully factorized normal 분포를 이루는 근사사후분포를 순환신경망 모델에 입력(put)된다. [수학식 24]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 27, "content": "수학식 24 에서 첫 번째 항( )은 손실함수(loss function)를 나타내며, 로부터 하나의 샘플을 이용하여 근사화될 수 있다. 수학식 24 에서 두 번째 항( )은 regularizer를 나타내며, 사후분포를 사전분포에 가깝게 만들며, 순환 신경망 인공지능 모델의 희박화를 수행하기 위해 사용되며, 수학식 26과 같이 근사화 될 수 있다. [수학식 25] , [수학식 26]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 28, "content": "완전 비편향(integral unbiased)을 추정하기 위해, 수학식 27과 같이 재매개변수화 기법(reparametrization trick)을 이용하는 경우 사후분포가 생성될 수 있다. [수학식 27]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 29, "content": "순환신경망은 일반적인 feed-forward 신경망과 달리 서로 다른 시간간격(different timestep)에서 동일한 가중 치를 사용할 수 있다. 따라서, likelihood ( , 우도)를 계산하기 위해 각 시간 스텝 t 마다 동일 한 가중치 샘플이 사용되어야 한다. 기존 feed-forward 신경망에서는 개별 가중치를 샘플링(생성)하는 대신, 사전활성화(preactivation)를 샘플링하 는 LRT(local reparametrization trick)기법을 사용하였다.. 그러나, 순환신경망에서는 가중치 행렬이 하나이상의 시간간격에 대해 사용되므로, 와 같이 묶인 가중치 샘플링(Tied weight sampling)로는 LRT기법이 순환신경망의 가중치 행렬에 대해 적용될 수 없 다. 가 이전 시간간격으로부터의 에 의존하기 때문에, 은닉-은닉 행렬(hidden-to-hidden matrix) 에 대 한 선형결합(linear combination) 은 정규분포(normally distribute)가 아니다. 따라서, 일정 계수 (constant coefficients)를 가지는 정규 분포의 합에 대한 규칙(rule about a sum of independent normal distributions)이 적용 될 수 없다. 따라서, 순환신경망을 LRT기법으로 학습시키는 경우 학습효율이 떨어지게 된다. 입력-은닉 행렬(input-to-hidden matrix) 에 대한 선형 결합 은 정규분포(normally distribute)를 이 룰 수 있다. 그러나 모든 시간간격에 대해 같은 를 샘플링 하는 것은 모든 시간간격에 대해 사전활성 (preactivations)에 대한 노이즈 를 샘플링하는 것과 대응되지 않으며, 모든 시간간격에 대해 같은 를 샘플링 하는 것은 다른 시간 간격에 대해 서로 다른 에 의한 서로 다른 노이즈 를 샘플링하는 것에 대응된다. 따라서 LRT기법은 순환신경망의 학습에 사용될 수 없다. 위의 학습과정은 2D잡음 텐서(noise tensor)에만 효과적이므로, 본 개시의 일 실시 예에 따르면, 개별 객체 (individual object)당 노이즈가 샘플링되는 것이 아닌 미니배치 기법당 가중치에 대한 노이즈가 샘플링될 수 있다. 따라서, 순환신경망을 이용한 자연언어 처리 학습은 수학식 26과 관련된 가중치를 생성하고, 생성된 가중치를 이용해 순환신경망을 순방향으로 통과(forward pass)시켜 미니배치 기법에 대한 순방향 통과를 수행할 수 있다. 그 후, , , B 에 관해 수학식24의 그래디언트(gradient)가 계산될 수 있다. 위 순환신경망 인공지능 모델의 학습과정에서 가중치의 평균값' '가 사용되며, 수학식25의 regularizer는 많은 수를 를 0으로 설정하게 하여 가중치에 대한 희박화를 수행할 수 있다. 본 개시의 일 실시 예에 따르면 평균값의 제곱에 대한 분산값의 비율값(ratio of square of mean to varianc e)을 의미하는 가 기 설정된 값 보다 작은 가중치를 제거함으로 가중치에 대한 희박화를 수행할 수 있다. 베이지안 희박화 기법은 이점 중 하나는 가중치 그룹의 희박화를 쉽게 일반화(generalization) 할 수 있다는 것 이다. 이를 위해 본 개시의 일 실시 예에 따르면, 각 그룹에 곱셈변수를 획득(도입)하고, 순환신경망의 학습을 통해 각 곱셈변수를 제거하면 곱셈변수에 대응되는 그룹이 제거된다. 구체적으로, 본 개시의 일 실시 예에 따르면, 어휘(vocabulary)내의 단어(word)에 대해 제1곱셈변수(확률적 곱 셈 가중치) 가 획득(도입)된다. V는 어휘의 크기를 나타낸다. 획득된 z를 이용한 순환신경망 인공지능 모 델의 학습방법은 다음과 같다. 1. 미니 배치 기법으로부터 각 입력 배열 에 대한 현재 사후분포(current approximation of the posterior) 로부터의 벡터가 샘플링(획득)된다. 2. 각 입력 배열 에 제1 곱셈변수 가 곱해진다. 3. 일반적인 순환신경망의 학습과 같이 순방향 통과(forward pass)가 수행된다. 다른 가중치에 대해서도 위의 방법과 동일하게 z를 샘플링(획득)하여 순방향 통과가 수행될 수 있다. 위 학습방 법 에서는 log-uniform 형태의 사전분포가 이용되며, 사후분포는 학습 가능한 평균값 및 분산값을 갖는 fully- factorized normal 분포 형태로 근사화 될 수 있다. z는 1차원 벡터 이므로, 미니 배치 기법의 각 객채(objective)에 대해 개별적으로 생성하여 그래디언트 (gradient)의 분산값을 줄일 수 있다. 위 z를 이용한 순환신경망의 학습 이후, 낮은 signal-to-noise(노이즈) 비율을 갖는 z는 제거되고, 이후에 z에 대응되는 단어는 사용되지 않을 수 있다. 위와 같은 순환신경망 인공지능 모델의 희박화 기법에 대한 동작들은 본 개시의 일 실시 예에 따른 전자장치의 프로세서에 의해 수행될 수 있으나, 이는 일 실시 예에 불과할 뿐, 여러 장치에 의해 수행될 수도 있다. 본 개시의 일 실시 예에 따른 텍스트 분류(text classification) 및 언어모델링(language modeling)의 두 가지 유형에 대해 LSTM구조를 이용하여 실험이 수행되였다. 실험에는 정규화(regularization)가 없는 모델, SparseVD 모델 및 곱셈변수가 있는 SparseVD 모델(SparseVD-Voc)의 세가지 모델이 사용되었다. 각 모델의 희박화 수준을 측정하기 위해 개별 가중치의 압축률은 |w| / |w ≠ 0| 와 같이 계산된다. 가중치의 희박화는 순환신경망 인공지능 모델의 압축뿐만 아니라, 순환신경망 인공지능 모델의 가속화를 가능하게 한다. 위 실험에서 각 모델에 대해 입력(input) 레이어, embedding 레이어, 순환(recurrent)레이어의 모든 레이어에 남아 있는 뉴런의 수가 계산된다. SparseVD-Voc 모델의 입력 레이어에 남아 있는 뉴런의 수를 계산하기 위해 변수가 획득(도입)된다. SparseVD 모델 및 SparseVD-Voc 모델의 다른 모든 레이어 에서, 뉴런에 연결된 가 중치가 제거되는 경우 그 뉴런이 제거된다. 위 실험에서 signal-to-noise(노이즈) 비율( )이 0.05보다 낮은 가중치가 제거된다. 텍스트 분류를 위한 실험에는(Text Classification) 2진분류(binary classification)를 위한 IMDb 데이터 세트 와, 4클래스 분류(four-class classification)를 위한 AGNews 데이터 세트가 이용된다. 실험에는 각각 15% 및 5%의 학습 데이터를 따로 설정하였고 두 데이터 세트 모두에서 가장 자주 사용되는 20,000단어의 어휘가 사용되었다. 실험에서 300유닛(unit)의 하나의 입력(embedding)레이어와 128/512 은닉 유닛(hidden unit)의 하나의 LSTM 레 이어가 사용되었다. 그리고, 완전연결 레이어(fully connected layer)가 LSTM레이어의 마지막 출력(last out- put)레이어에 적용되었다. 입력(embedding)레이어는 word2vec 및 GLoVe로 초기설정 되며, SparseVD 모델 및 SparseVD-Voc 모델은 IMDb 및 AGNews데이터세트에서 800/150 epochs에 대해 학습된다. [표 1]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 30, "content": "표 1 에는 각 모델의 학습결과를 도시하였다. SparseVD 모델이 품질 저하 없이 매우 높은 압축률을 나타내며, SparseVD-Voc 모델은 정확도(accuracy)를 유지하면서, 압축률을 더욱 높여 주었다. 위와 같은 높은 압축률은 어 휘(vocabulary)의 희박화를 수행하여 달성된다. 즉, 텍스트를 분류하기 위해 중요텍스트 위주로만 학습되어야 한다."}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 31, "content": "언어모델링(Language Modeling) 작업은 문자(character)수준 및 단어(word)수준 언어모델링 작업으로 수행되었 다. 실험에는 50문자 또는 10000개 단어의 문자 데이터 세트가 사용되었다. 문자/단어 수준의 작업을 수행하기 위해, 실험에는 10000/256 은닉(hidden)유닛의 하나의 LSTM레이어와 softmax활성화를 갖는 완전연결된(fully- connected)레이어를 갖는 순환신경망을 이용하여, 문자 또는 단어가 예측되었다. SparseVD 모델 및 SparseVD- Voc 모델은 단어수준/문자수준 작업에서 250/150 epochs에 대해 학습된다. [표 2]"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 32, "content": "표 2 에는 각 모델의 학습결과를 도시하였다. 위 실험을 위해 마지막 완전연결(last fully-connected )레이어에 LRT가 사용되었다. 마지막 레이어의 LRT는 최종 결과에 부정적인 영향을 미치지 않으며, 학습이 가속화 된다.실험에서 사용되는 어휘가 50자에 불과하므로, 문자수준의 실험에서는 입력 어휘에 대한 희박화를 수행하지 않 았다. 단어 수준의 실험에서는 절반 이상의 단어가 삭제되었다 텍스트 분류 실험에서 은닉-은닉(hidden-to-hidden) 가중치 행렬 이 직교(orthogonally)로 초기설정 되며, 다른 모든 행렬이 균일(uniformly)하게 초기설정 된다. 그리고 순환신경망은 크기 128 및 학습률 0.0005의 미니 배치 기법을 이용해 학습된다. 언어모델링 실험에서 모든 가중치 행렬이 직교(orthogonally)로 초기설정 되며, 모든 편향(bias)는 0으로 초기 화 된다. 은닉 요소(hidden element)와 LSTM 요소의 초기값은 학습할 수 없으며, 0과 같다. 언어모델링 실험에서 문자수준의 학습의 경우 겹치지 않는 100개의 문자 배열이 이용되었으며, 순환신경망은 0.002의 학습률과 기준값 1에 대한 clip 그래디언트(gradient)를 갖는 크기64의 미니배치 기법을 이용해 학습되 었다. 언어모델링 실험에서 단어수준의 학습의 경우, 순환 미니 배치 기법의 마지막 은닉상태(final hidden state)는 후속 미니 배치 기법상의 초기 은닉 상태(initial hidden state) 에 사용되었다. 각 미니 배치 기법의 크기는 32이며, 학습률 0.002 및 기준값 10 에 대한 클립 그래디언트(gradient)를 사용해 학습되었다. 곱셈변수가 있는 SparseVD 모델에서 IMDB을 이용한 작업(실험)에서 다음과 같은 단어가 사용되었다. 표 3 start, oov, and, to, is, br, in, it, this, was, film, t, you, not, have, It, just, good, very, would, story, if, only, see, even, no, were, my, much, well, bad, will, great, first, most, make, also, could, too, any, then, seen, plot, acting, life, over, off, did, love, best, better, i, If, still, man, some- thing, m, re, thing, years, old, makes, director, nothing, seems, pretty, enough, own, original, world, series, young, us, right, always, isn, least, interesting, bit, both, script, minutes, making, 2, performance, might, far, anything, guy, She, am, away, woman, fun, played, worst, trying, looks, especially, book, DVD, reason, money, actor, shows, job, 1, someone, true, wife, beautiful, left, idea, half, excellent, 3, nice, fan, let, rest, poor, low, try, classic, production, boring, wrong, enjoy, mean, No, instead, awful, stupid, remember, wonderful, often, become, terrible, others, dialogue, perfect, liked, supposed, entertaining, waste, His, problem, Then, worse, definitely, 4, seemed, lives, example, care, loved, Why, tries, guess, genre, history, enjoyed, heart, amazing, starts, town, favorite, car, today, decent, brilliant, horrible, slow, kill, attempt, lack, interest, strong, chance, wouldn, sometimes, except, looked, crap, highly, wonder, annoying, Oh, simple, reality, gore, ridiculous, hilarious, talking, female, episodes, body, saying, running, save, disappointed, 7, 8, OK, word, thriller, Jack, silly, cheap, Oscar, predictable, enjoyable, moving, Un- fortunately, surprised, release, effort, 9, none, dull, bunch, comments, realistic, fantastic, weak, atmosphere, apparently, premise, greatest, believable, lame, poorly, NOT, superb, badly, mess, perfectly, unique, joke, fails, masterpiece, sorry, nudity, flat, Good, dumb, Great, D, wasted, unless, bored, Tony, language, incredible, pointless, avoid, trash, failed, fake, Very, Stewart, awesome, garbage, pathetic, genius, glad, neither, laughable, beautifully, excuse, disappointing, disappointment, outstanding, stunning, noir, lacks, gem, F, redeeming, thin, absurd, Jesus, blame, rubbish, unfunny, Avoid, irritating, dreadful, skip, racist, Highly, MST3K. 도 1은 본 개시의 일 실시 예에 따른, 전자장치의 구성을 간략히 도시한 블록도 이다. 도 1에 도시된 바와 같이, 전자장치는 메모리 및 프로세서를 포함 할 수 있다. 본 개시의 다양 한 실시 예들에 따른 전자장치는, 예를 들면, 스마트폰, 태블릿PC, 이동 전화기, 영상 전화기, 전자책 리 더기, 데스크탑 PC, 랩탑 PC, 넷북 컴퓨터, 의료기기, 카메라, 또는 웨어러블 장치 중 적어도 하나를 포함할 수 있다. 웨어러블 장치는 액세서리형(예: 시계, 반지, 팔찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치 (head-mounted-device(HMD)), 직물 또는 의류 일체형(예: 전자 의복), 신체 부착형(예: 스킨 패드 또는 문신), 또는 생체 이식형 회로 중 적어도 하나를 포함 할 수 있다. 메모리는, 예를 들면, 전자장치의 적어도 하나의 다른 구성요소에 관계된 명령 또는 데이터를 저장할 수 있다. 특히, 메모리는 비휘발성 메모리, 휘발성 메모리, 플래시메모리(flash-memory), 하드디스크 드라이브(HDD) 또는 솔리드 스테이트 드라이브(SSD) 등으로 구현될 수 있다. 메모리는 프로세서에 의해 액세스되며, 프로세서에 의한 데이터의 독취/기록/수정/삭제/갱신 등이 수행될 수 있다. 본 개시에서 메모 리라는 용어는 메모리, 프로세서 내 롬(미도시), 램(미도시) 또는 전자 장치에 장착되는 메모리 카드(미도시)(예를 들어, micro SD 카드, 메모리 스틱)를 포함할 수 있다. 또한, 메모리에는 디스플레이의 디스플레이 영역에 표시될 각종 화면을 구성하기 위한 프로그램 및 데이터 등이 저장될 수 있다. 특히, 메모리는 인공지능 에이전트를 수행하기 위한 프로그램을 저장할 수 있다. 이때, 인공지능 에이전트 는 전자 장치에 대한 다양한 서비스를 제공하기 위한 개인화된 프로그램이다. 프로세서는, 중앙처리장치, 어플리케이션 프로세서, 또는 커뮤니케이션 프로세서(communication processor(CP)) 중 하나 또는 그 이상을 포함할 수 있다. 또한, 프로세서는 주문형 집적 회로(application specific integrated circuit, ASIC), 임베디드 프로세 서, 마이크로프로세서, 하드웨어 컨트롤 로직, 하드웨어 유한 상태 기계(hardware finite state machine, FSM), 디지털 신호 프로세서(digital signal processor, DSP), 중 적어도 하나로 구현될 수 있다. 도시하진 않 았으나, 프로세서는 각 구성들과 통신을 위한 버스(bus)와 같은 인터페이스를 더 포함할 수 있다. 프로세서는, 예를 들면, 운영 체제 또는 응용 프로그램을 구동하여 프로세서에 연결된 다수의 하드웨 어 또는 소프트웨어 구성요소들을 제어 할 수 있고, 각종 데이터 처리 및 연산을 수행할 수 있다. 프로세서 는, 예를 들면 SoC(system on chip)로 구현될 수 있다. 일 실시 예에 따르면, 프로세서는 GPU(graphic processing unit) 및/또는 이미지 신호 프로세서를 더 포함할 수 있다. 프로세서는 다른 구 성요소들(예: 비휘발성 메모리) 중 적어도 하나로부터 수신된 명령 또는 데이터를 휘발성 메모리에 로드하여 처 리하고, 결과 데이터를 비휘발성 메모리에 저장할 수 있다. 한편, 프로세서는 인공지능(AI: artificial intelligence)을 위한 전용 프로세서를 포함하거나, 기존의 범 용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작될 수 있다. 이 때, 인공 지능을 위한 전용 프로세서는 확률 연산에 특화된 전용 프로세서로서, 기존의 범용 프로세서 보다 병렬처리 성능이 높아 기계 학습과 같은 인공지능 분야의 연산 작업을 빠르게 처리할 수 있다. 특히, 본 개시의 일 실시 예에 따른, 프로세서는 순환신경망의 입력요소에 관한 제1 곱셈변수를 획득할 수 있다. 입력요소는 앞서 상술 하였듯이, 어휘(vocabulary) 또는 단어(word)일 수 있다. 또한, 프로세서는 순환신경망의 입력 뉴런 및 은닉 뉴런에 대한 제2 곱셈 변수를 획득할 수 있다. 입력 뉴런에 대한 제2 곱셈변수 는 상술한 바와 같이 로 표현되고, 은닉 뉴런에 대한 제2 곱셈변수는 와 같이 표현될 수 있다. 제1 곱셈변수 및 제2 곱셈변수를 획득한 후, 프로세서는 순환신경망의 가중치, 획득한 제1 곱셈변수 및 제 2 곱셈변수를 이용하여, 순환신경망을 학습할 수 있다. 프로세서는 순환신경망의 가중치, 획득한 제1 및 제2 곱셈변수에 대한 평균값 및 분산값을 초기화 하고, 가중치, 획득한 제1 및 제2 곱셈변수의 평균값 및 분산값과 관련된 객채(objective)를 최적화함으로 순환신경망 의 학습을 수행할 수 있다. 객채(objective)는 수학식 1의 에 해당한다. 객채(objective)에 대한 최적화는 확률적(stochastic) 최적화를 사용해 수행될 수 있다. 프로세서는 객채(objective)들의 미니배치를 선택하고, 근사 사후분포로부터 가중치, 제1 및 제2 곱셈변수 를 생성해 순환신경망을 순방향으로 통과(forward pass)시킨다. 이때, 가중치는 미니배치에 의해 생성될 수 있 고, 제1 그룹변수 및 제2 그룹변수는 객체로부터 개별적으로 생성될 수 있다. 그 후, 프로세서는 객채 (objective)를 계산하고, 객채(objective)에 대한 그래디언트(gradient)를 계산한다. 그리고, 프로세서는 계산된 그래디언트에 기초해 가중치, 제1 및 제2 곱셈변수에 대한 평균값 및 분산값을 획득(업데이트)하여 객체 에 대한 최적화를 수행할 수 있다. 순환신경망의 학습이 완료되면, 프로세서는 획득된 평균값 및 분산값을 바탕으로 가중치, 제1 곱셈변수, 제2 곱셈변수에 대해 희박화(sparsification)를 수행할 수 있다. 희박화는 일정 가중치, 제1 곱셈변수 또는 제2 곱셈변수를 0으로 만들어 순환신경망을 압축하는 방법으로, 프로 세서는 획득된 평균값(mean) 및 분산값(variance)을 바탕으로, 희박화를 수행하기 위한 관련값을 계산을계산할 수 있다. 관련값은 획득된 평균값의 제곱에 대한 분산값의 비율값(ratio of square of mean to variance )이며, 상술한 바와 같이 로 표현된다. 프로세서는 관련값이 기 설정된 값보다 작은 가중치, 제1 곱셈변수 또는 제2 곱셈변수를 0으로 설정함으로 순환신경망 인공지능 모델에 대한 희박화를 수행할 수 있다. 기 설정된 값은 0.05일 수 있으나, 이에 한정되지 않는다. 본 개시의 일 실시 예에 따르면, 순환신경망이 게이트 구조(gated structure)를 포함하는 경우 프로세서는 순환신경망의 순환레이어의 게이트를 일정하게 만들기 위해 게이트의 사전활성(preactivation)에 관한 제3 곱셈 변수를 획득(도입)한다. 제3 곱셈변수는 상술한 바와 같이, , , , 로 표현될 수 있다. 순환신경망이 게이트 구조(gated structure)를 포함하는 경우 프로세서가 최적화 및 희박화를 수행함에 있어, 프로세서는 제3 곱셈변수를 더 포함하여 순환신경망을 학습하고, 순환신경망 인공지능 모델에 대한 희박화를 수행할 수 있다. 즉, 프로세서는 제1 곱셈변수 내지 제3 곱셈변수를 획득한 후 순환신경망의 가 중치, 제1 곱셈변수, 제2 곱셈변수, 및 제3 곱셈변수를 이용하여, 순환신경망을 학습할 수 있다. 프로세서가 가중치 및 제1 내지 제3 곱셈변수에 대한 평균값 및 분산값을 초기화 하고, 가중치 및 제1 내 지 제3 곱셈변수의 평균값 및 분산값과 관련된 객채(objective)에 대해 최적화를 수행함으로 순환신경망을 학습 할 수 있다.. 프로세서는 객채(objective)들의 미니배치를 선택하고, 근사 사후분포로부터 가중치 및 제1 내지 제3 곱셈 변수를 샘플링(생성)하고, 생성된 가중치, 제1 그룹변수 내지 제3 그룹변수를 바탕으로 순환신경망을 순방향으 로 통과(forward pass)시켜, 객채(objective)를 계산할 수 있다. 그 후, 프로세서는 객채(objective)에 대한 그래디언트(gradient)를 계산하고, 그래디언트에 기초해 가중치 및 제1 내지 제3 곱셈변수에 대한 평균값 및 분산값을 획득하는 과정을 통해 객체에 대한 최적화를 수행할 수 있다. 순환신경망의 학습이 완료되면, 프로세서는 획득된 평균값 및 분산값을 바탕으로, 가중치, 제1 곱셈변수 내지 제3 곱셈변수에 대해 희박화를 수행할 수 있다. 희박화는 일정 가중치, 제1 곱셈변수, 제2 곱셈변수 또는 제3 곱셈변수를 0으로 만들어 순환신경망을 압축하는 방법으로, 프로세서는 획득된 평균값 및 분산값을 바탕으로 희박화를 수행하기 위한 관련 값을 계산할 수 있다. 관련값은 가중치 및 제1 곱셈변수 내지 제3 곱셈변수에 대한 획득된 평균값의 제곱에 대한 분산값의 비율 값(ratio of square of mean to variance )이며, 상술한 바와 같이 로 표현된다. 프로세서는 관련값이 기 설정된 값보다 작은 가중치, 제1 곱셈변수, 제2 곱셈변수 또는 제3 곱셈변수를 0 으로 설정함으로 순환신경망 인공지능 모델에 대한 희박화를 수행할 수 있다. 순환신경망의 게이트 구조는 LSTM(Long-Short term Memory)계층으로 구현될 수 있으며 자세한 내용은 상술 하였 으므로 생략한다. 도 2는 본 개시의 일 실시 예에 따른, 순환신경망 인공지능 모델의 압축방법을 나타내는 흐름도이다. 먼저, 전자 장치는 순환신경망의 입력 요소에 관한 제1 곱셈변수를 획득한다(S210). 입력요소는 앞서 상술 하였듯이, 어휘(vocabulary) 또는 단어(word)일 수 있다. 그리고, 전자 장치는 순환신경망의 입력 뉴런 및 은닉 뉴런에 대한 제2 곱셈변수를 획득한다(S220). 입력 뉴런에 대한 제2 곱셈변수는 상술한 바와 같이 로 표 현되고, 은닉 뉴런에 대한 제2 곱셈변수는 와 같이 표현될 수 있다. 순환 신경망이 게이트 구조를 포함하는 경우(S230-Y), 전자 장치는 게이트의 사전 활성(preactivation)에 관한 제3 곱셈변수를 획득한다(S240). 제3 곱셈변수는 상술한 바와 같이, , , , 로 표현될 수 있다. 획득한 곱셈변수들과 순환신경망의 가중치를 바탕으로 전자 장치는 순환 신경망을 학습한다(S250). 그리고, 학습된 가중치 및 곱셈변수를 바탕으로 순환신경망에 대해 희박화를 수행하여(S260) 처리를 종료한다. 순환 신경망이 게이트 구조를 포함하지 않는 경우(S230-N), 전자 장치는 순환신경망의 가중치, 제1 곱셈변 수 및 제2 곱셈변수를 바탕으로 순환 신경망을 학습하고(S250), 순환 신경망에 대해 희박를 수행하여(S260)처리 를 종료한다. 도 3은 본 개시의 일 실시 예에 따른, 순환신경망 인공지능 모델의 학습방법을 나타내는 흐름도이다. 우선, 전자 장치는 가중치 및 그룹변수들에 대한 평균값 및 분산값을 초기화 한다(S310). 그룹변수들은 제 1 및 제2 그룹변수를 포함하며, 순환신경망이 게이트 구조를 포함하는 경우 제3 그룹변수를 더 포함할 수 있다. 그리고, 전자 장치는 객체들의 미니배치를 선택하고(S320), 근사 사후분포로부터 가중치 및 그룹변수들을 생성(샘플링)한다(S330). 전자 장치는 생성된 가중치 및 그룹변수들을 바탕으로 미니배치를 이용하여 순환신경망을 순방향 통과 시 킨다(S340). 그리고, 전자 장치는 객체를 계산하고, 객체에 대한 그래디언트를 계산한다(S350). 그리고, 전자 장치는 계산된 그래디언트를 바탕으로 가중치 및 그룹변수들에 대한 평균값 및 분산값을 획 득 하여(S360) 순환신경망 인공지능 모델의 학습을 종료할 수 있다. 도 4는 본 개시의 일 실시 예에 따른, 순환신경망 인공지능 모델에 대한 희박화 수행방법을 나타내는 흐름도 이 다. 전자 장치는 획득된 평균값 및 분산값을 바탕으로 관련값을 계산한다(S410). 관련값은 획득된 평균값의 제 곱에 대한 분산값의 비율값(ratio of square of mean to variance)을 의미하며, 로 표현 될 수 있다. 관련값이 기 설정된 값 보다 작은 경우(S420-Y), 전자 장치는 관련값이 기 설정된 값 보다 작은 가중치 또 는 곱셈변수를 0으로 설정하여 순환신경망 인공지능 모델의 희박화를 수행한다(S430). 전자 장치는 관련값 이 기 설정된 값 보다 큰 가중치 또는 곱셈변수에 대하여는(S420-N) 희박화를 수행하지 않고 처리를 종료한다. 기 설정된 값은 0.05일 수 있으나 이에 한정되지 않는다. 도 5는 본 개시의 다른 실시 예에 따른, 순환신경망 인공지능 모델에 대한 압축방법을 나타내는 흐름도이다. 전자 장치는 순환신경망 인공지능 모델의 가중치에 대한 희박화를 수행할 수 있다(S510). 구체적으로 전자 장치는 가중치를 바탕으로 순환신경망을 학습하여, 가중치에 대한 평균값 및 분산값을 획득하고, 획득된 평균값 및 분산값을 바탕으로 평균값의 제곱에 대한 분산값의 비율값을 계산하고, 계산된 비율값이 기 설정된 값 보다 작은 가중치를 0으로 설정한다. 그리고, 전자 장치는 순환 신경망 인공지능 모델의 입력요소에 대한 희박화를 수행할 수 있다(S520). 구체 적으로 전자 장치는 입력 요소에 관한 제1 곱셈변수를 획득하고, 제1 곱셈변수를 바탕으로 순환신경망을 학습하여, 제1 곱셈변수에 대한 평균값 및 분산값을 획득하고, 획득된 평균값 및 분산값을 바탕으로 평균값의 제곱에 대한 분산값의 비율값을 계산하고, 계산된 비율값이 기 설정된 값 보다 작은 제1 곱셈변수를 0으로 설정 한다. 그리고, 전자 장치는 순환 신경망 인공지능 모델의 뉴런에 대한 희박화를 수행할 수 있다(S530). 구체적으 로 구체적으로 전자 장치는 입력뉴런 및 은닉뉴런에 관한 제2 곱셈변수를 획득하고, 제2 곱셈변수를 바탕 으로 순환신경망을 학습하여, 제2 곱셈변수에 대한 평균값 및 분산값을 획득하고, 획득된 평균값 및 분산값을 바탕으로 평균값의 제곱에 대한 분산값의 비율값을 계산하고, 계산된 비율값이 기 설정된 값 보다 작은 제2 곱 셈변수를 0으로 설정한다. 순환 신경망 인공지능 모델이 게이트 구조를 더 포함하는 경우, 전자 장치는 순환 신경망 인공지능 모델의 게이트에 대한 희박화를 수행할 수 있다(S540). 구체적으로 구체적으로 전자 장치는 게이트의 사전활성에 관한 제3 곱셈변수를 획득하고, 제3 곱셈변수를 바탕으로 순환신경망을 학습하여, 제3 곱셈변수에 대한 평균값 및 분산값을 획득하고, 획득된 평균값 및 분산값을 바탕으로 평균값의 제곱에 대한 분산값의 비율값을 계산하고, 계산된 비율값이 기 설정된 값 보다 작은 제3 곱셈변수를 0으로 설정한다. 한편, 본 개시의 일시 예에 따르면, 이상에서 설명된 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기 기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실시 예들에 따른 전자 장치(예: 전자 장치(A))를 포함할 수 있다. 명령이 프로세서에 의해 실행될 경우, 프로세서 가 직접, 또는 프로세서의 제어 하에 다른 구성요소들을 이용하여 명령에 해당하는 기능을 수행할 수 있다. 명 령은 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장 매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장매체가 신호 (signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임시적 으로 저장됨을 구분하지 않는다. 또한, 본 개시의 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품 (computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있 다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 또한, 상술한 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구 성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요 소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로 그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시 예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동 작들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2019-0031618", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 33, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2019-0031618", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시 예에 따른, 전자장치의 구성을 간략히 도시한 블록도 이다. 도 2는 본 개시의 일 실시 예에 따른, 순환신경망 인공지능 모델의 압축방법을 나타내는 흐름도 이다. 도 3은 본 개시의 일 실시 예에 따른, 순환신경망 인공지능 모델의 학습방법을 나타내는 흐름도 이다. 도 4는 본 개시의 일 실시 예에 따른, 순환신경망 인공지능 모델에 대한 희박화 수행방법을 나타내는 흐름도 이 다. 도 5는 본 개시의 다른 실시 예에 따른, 순환 신경망 인공지능 모델의 압축방법을 나타내는 흐름도 이다."}
