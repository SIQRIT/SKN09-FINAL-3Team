{"patent_id": "10-2019-0124385", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0041757", "출원번호": "10-2019-0124385", "발명의 명칭": "전자 장치 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "김재호"}}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,사용자 인터페이스;카메라;입력 이미지에 기초하여 감정에 대한 정보를 획득하도록 학습된 제1 인공 지능 모델(Artificial IntelligenceModel)을 저장하는 메모리; 및상기 사용자 인터페이스, 상기 카메라 및 상기 메모리와 연결되어 상기 전자 장치를 제어하는 프로세서;를 포함하고,상기 프로세서는,상기 사용자 인터페이스를 통해 텍스트가 입력되면 상기 텍스트가 사용자의 감정에 대한 제1 정보를 포함하는지식별하며,상기 텍스트가 상기 제1 정보를 포함하면 상기 카메라를 통해 획득된 이미지를 상기 제1 인공 지능 모델에 입력하여 감정에 대한 제2 정보를 획득하고,상기 텍스트로부터 획득된 상기 제1 정보 및 상기 이미지로부터 획득된 상기 제2 정보에 기초하여 상기 사용자의 감정의 타입을 식별하는, 전자 장치."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 프로세서는,상기 텍스트가 상기 제1 정보를 포함하면 상기 카메라를 턴 온하여 상기 이미지를 획득하고,상기 획득된 이미지를 상기 제1 인공 지능 모델에 입력하는, 전자 장치."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 프로세서는,상기 카메라가 턴 온된 후 추가로 입력된 텍스트가 상기 사용자의 감정에 대한 제1 정보를 포함하지 않으면 상기 카메라를 턴 오프하는, 전자 장치."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 제1 정보는, 적어도 하나의 제1 감정 정보 및 상기 적어도 하나의 제1 감정 정보 각각의 제1 신뢰도 정보를 포함하고,상기 제2 정보는, 적어도 하나의 제2 감정 정보 및 상기 적어도 하나의 제2 감정 정보 각각의 제2 신뢰도 정보를 포함하며,상기 프로세서는,상기 제1 신뢰도 정보 및 상기 제2 신뢰도 정보를 감정의 타입 별로 가중합(weighted sum)하여 상기 사용자의감정의 타입을 식별하는, 전자 장치.공개특허 10-2021-0041757-3-청구항 5 제1항에 있어서,상기 제1 정보는, 적어도 하나의 제1 감정 정보 및 상기 적어도 하나의 제1 감정 정보 각각의 제1 신뢰도 정보를 포함하고,상기 프로세서는,상기 적어도 하나의 제1 감정 정보의 제1 신뢰도 정보가 임계값 이상인 경우, 상기 텍스트가 상기 사용자의 감정에 대한 정보를 포함하는 것으로 식별하는, 전자 장치."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 메모리는,텍스트에 기초하여 감정에 대한 정보를 획득하도록 학습된 제2 인공 지능 모델을 저장하며,상기 프로세서는,상기 텍스트를 상기 제2 인공 지능 모델에 입력하여 획득된 정보로부터 상기 텍스트가 상기 제1 정보를 포함하는지 여부를 식별하는, 전자 장치."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,마이크;를 더 포함하며,상기 메모리는,오디오에 기초하여 감정에 대한 정보를 획득하도록 학습된 제3 인공 지능 모델을 저장하며,상기 프로세서는,상기 텍스트가 상기 제1 정보를 포함하면 상기 마이크를 턴 온하고,상기 마이크를 통해 수신된 오디오를 상기 제3 인공 지능 모델에 입력하여 감정에 대한 제3 정보를 획득하며,상기 제1 정보, 상기 제2 정보 및 상기 제3 정보에 기초하여 상기 사용자의 감정의 타입을 식별하는, 전자장치."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 프로세서는,상기 카메라를 통해 실시간으로 복수의 이미지를 획득하고,상기 텍스트가 상기 제1 정보를 포함하면, 상기 복수의 이미지 중 상기 텍스트가 입력된 시점부터 획득된 이미지를 상기 제1 인공 지능 모델에 입력하는, 전자 장치."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,디스플레이;를 더 포함하며,상기 프로세서는,상기 식별된 사용자의 감정의 타입에 대응되는 적어도 하나의 추천 이모티콘을 디스플레이하도록 상기 디스플레이를 제어하는, 전자 장치.공개특허 10-2021-0041757-4-청구항 10 제1항에 있어서,상기 전자 장치는,모바일 디바이스이고,디스플레이;를 더 포함하며,상기 카메라는, 상기 디스플레이의 전방을 촬영하는, 전자 장치."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "전자 장치의 제어 방법에 있어서,텍스트가 입력되면 상기 텍스트가 사용자의 감정에 대한 제1 정보를 포함하는지 식별하는 단계;상기 텍스트가 상기 제1 정보를 포함하면 상기 전자 장치의 카메라를 통해 획득된 이미지를 제1 인공 지능 모델(Artificial Intelligence Model)에 입력하여 감정에 대한 제2 정보를 획득하는 단계; 및상기 텍스트로부터 획득된 상기 제1 정보 및 상기 이미지로부터 획득된 상기 제2 정보에 기초하여 상기 사용자의 감정의 타입을 식별하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 제2 정보를 획득하는 단계는,상기 텍스트가 상기 제1 정보를 포함하면 상기 카메라를 턴 온하여 상기 이미지를 획득하고,상기 획득된 이미지를 상기 제1 인공 지능 모델에 입력하는, 제어 방법."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 카메라가 턴 온된 후 추가로 입력된 텍스트가 상기 사용자의 감정에 대한 제1 정보를 포함하지 않으면 상기 카메라를 턴 오프하는 단계;를 더 포함하는, 제어 방법."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 제1 정보는, 적어도 하나의 제1 감정 정보 및 상기 적어도 하나의 제1 감정 정보 각각의 제1 신뢰도 정보를 포함하고,상기 제2 정보는, 적어도 하나의 제2 감정 정보 및 상기 적어도 하나의 제2 감정 정보 각각의 제2 신뢰도 정보를 포함하며,상기 사용자의 감정의 타입을 식별하는 단계는,상기 제1 신뢰도 정보 및 상기 제2 신뢰도 정보를 감정의 타입 별로 가중합(weighted sum)하여 상기 사용자의감정의 타입을 식별하는, 제어 방법."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 제1 정보는, 적어도 하나의 제1 감정 정보 및 상기 적어도 하나의 제1 감정 정보 각각의 제1 신뢰도 정보를 포함하고,상기 제1 정보를 포함하는지 식별하는 단계는,상기 적어도 하나의 제1 감정 정보의 제1 신뢰도 정보가 임계값 이상인 경우, 상기 텍스트가 상기 사용자의 감공개특허 10-2021-0041757-5-정에 대한 정보를 포함하는 것으로 식별하는, 제어 방법."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항에 있어서,상기 제1 정보를 포함하는지 식별하는 단계는,상기 텍스트를 제2 인공 지능 모델에 입력하여 획득된 정보로부터 상기 텍스트가 상기 제1 정보를 포함하는지여부를 식별하는, 제어 방법."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항에 있어서,상기 텍스트가 상기 제1 정보를 포함하면 상기 전자 장치의 마이크를 턴 온하는 단계; 및상기 마이크를 통해 수신된 오디오를 제3 인공 지능 모델에 입력하여 감정에 대한 제3 정보를 획득하는 단계;를더 포함하며,상기 사용자의 감정의 타입을 식별하는 단계는,상기 제1 정보, 상기 제2 정보 및 상기 제3 정보에 기초하여 상기 사용자의 감정의 타입을 식별하는, 제어방법."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항에 있어서,상기 제2 정보를 획득하는 단계는,상기 카메라를 통해 실시간으로 복수의 이미지를 획득하고,상기 텍스트가 상기 제1 정보를 포함하면, 상기 복수의 이미지 중 상기 텍스트가 입력된 시점부터 획득된 이미지를 상기 제1 인공 지능 모델에 입력하는, 제어 방법."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항에 있어서,상기 식별된 사용자의 감정의 타입에 대응되는 적어도 하나의 추천 이모티콘을 상기 전자 장치의 디스플레이를통해 디스플레이하는 단계;를 더 포함하는, 제어 방법."}
{"patent_id": "10-2019-0124385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제11항에 있어서,상기 전자 장치는,디스플레이가 구비된 모바일 디바이스이고,상기 제2 정보를 획득하는 단계는,상기 카메라를 통해 상기 디스플레이의 전방을 촬영하는, 제어 방법."}
{"patent_id": "10-2019-0124385", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 본 전자 장치는 사용자 인터페이스, 카메라, 입력 이미지에 기초하여 감정에 대한 정보를 획득하도록 학습된 제1 인공 지능 모델(Artificial Intelligence Model)을 저장하는 메모리 및 사용자 인터페이 스, 카메라 및 메모리와 연결되어 전자 장치를 제어하는 프로세서를 포함하고, 프로세서는 사용자 인터페이스를 통해 텍스트가 입력되면 텍스트가 사용자의 감정에 대한 제1 정보를 포함하는지 식별하며, 텍스트가 제1 정보를 포함하면 카메라를 통해 획득된 이미지를 제1 인공 지능 모델에 입력하여 감정에 대한 제2 정보를 획득하고, 텍 스트로부터 획득된 제1 정보 및 이미지로부터 획득된 제2 정보에 기초하여 사용자의 감정의 타입을 식별할 수 있 다."}
{"patent_id": "10-2019-0124385", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 그 제어 방법에 대한 것으로, 더욱 상세하게는 사용자의 감정의 타입을 식별하는 전자 장치 및 그 제어 방법에 대한 것이다. 또한, 본 개시는 기계 학습 알고리즘을 활용하여 인간 두뇌의 인지, 판단 등의 기능을 모사하는 인공 지능 (Artificial Intelligence, AI) 시스템 및 그 응용에 관한 것이다."}
{"patent_id": "10-2019-0124385", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근에는 사용자와의 인터랙션이 가능한 다양한 전자 장치가 개발되고 있다. 특히, 사용자와의 인터랙션을 위해 사용자의 감정의 타입을 식별하고, 식별된 감정의 타입에 따라 동작하는 전자 장치들이 개발되고 있다. 이때, 인공 지능 기술을 이용하여 사용자의 감정의 타입을 식별할 수도 있다. 다만, 종래에는 사용자의 이미지나 사용자의 오디오에 기초하여 사용자의 감정의 타입을 식별하는 경우, 일정 수준의 신뢰도를 확보할 수 있으나 전자 장치의 카메라 또는 마이크가 항상 턴 온될 필요가 있기 때문에 전력 소모가 증가하는 문제가 있었다. 특히, 사용자의 이미지나 사용자의 오디오를 인공 지능 기술에 적용할 경우, 실시간 처리가 어렵거나, 실시간 처리에 따른 로드(load)가 증가하는 문제가 있었다. 전력 소모 문제나 로드가 증가하는 문제를 회피하기 위해 사용자로부터 입력된 텍스트를 이용할 수도 있으나, 이 경우 사용자의 이미지나 사용자의 오디오를 이용하는 경우보다 상대적으로 신뢰도가 낮아지는 문제가 있다. 그에 따라, 전력 소모 문제나 로드가 증가하는 문제를 회피하면서도 높은 신뢰도로 사용자의 감정의 타입을 식 별하는 방법이 개발될 필요가 있다."}
{"patent_id": "10-2019-0124385", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 필요성에 따른 것으로, 본 개시의 목적은 텍스트로부터 사용자의 감정의 타입을 식별하는 전 자 장치 및 그 제어 방법을 제공함에 있다."}
{"patent_id": "10-2019-0124385", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "이상과 같은 목적을 달성하기 위한 본 개시의 일 실시 예에 따르면, 전자 장치는 사용자 인터페이스, 카메라, 입력 이미지에 기초하여 감정에 대한 정보를 획득하도록 학습된 제1 인공 지능 모델(Artificial Intelligence Model)을 저장하는 메모리 및 상기 사용자 인터페이스, 상기 카메라 및 상기 메모리와 연결되어 상기 전자 장치 를 제어하는 프로세서를 포함하고, 상기 프로세서는 상기 사용자 인터페이스를 통해 텍스트가 입력되면 상기 텍 스트가 사용자의 감정에 대한 제1 정보를 포함하는지 식별하며, 상기 텍스트가 상기 제1 정보를 포함하면 상기 카메라를 통해 획득된 이미지를 상기 제1 인공 지능 모델에 입력하여 감정에 대한 제2 정보를 획득하고, 상기 텍스트로부터 획득된 상기 제1 정보 및 상기 이미지로부터 획득된 상기 제2 정보에 기초하여 상기 사용자의 감 정의 타입을 식별한다. 또한, 상기 프로세서는 상기 텍스트가 상기 제1 정보를 포함하면 상기 카메라를 턴 온하여 상기 이미지를 획득 하고, 상기 획득된 이미지를 상기 제1 인공 지능 모델에 입력할 수 있다. 그리고, 상기 프로세서는 상기 카메라가 턴 온된 후 추가로 입력된 텍스트가 상기 사용자의 감정에 대한 제1 정 보를 포함하지 않으면 상기 카메라를 턴 오프할 수 있다. 또한, 상기 제1 정보는 적어도 하나의 제1 감정 정보 및 상기 적어도 하나의 제1 감정 정보 각각의 제1 신뢰도 정보를 포함하고, 상기 제2 정보는 적어도 하나의 제2 감정 정보 및 상기 적어도 하나의 제2 감정 정보 각각의 제2 신뢰도 정보를 포함하며, 상기 프로세서는 상기 제1 신뢰도 정보 및 상기 제2 신뢰도 정보를 감정의 타입 별로 가중합(weighted sum)하여 상기 사용자의 감정의 타입을 식별할 수 있다. 그리고, 상기 제1 정보는 적어도 하나의 제1 감정 정보 및 상기 적어도 하나의 제1 감정 정보 각각의 제1 신뢰 도 정보를 포함하고, 상기 프로세서는 상기 적어도 하나의 제1 감정 정보의 제1 신뢰도 정보가 임계값 이상인 경우, 상기 텍스트가 상기 사용자의 감정에 대한 정보를 포함하는 것으로 식별할 수 있다. 또한, 상기 메모리는 텍스트에 기초하여 감정에 대한 정보를 획득하도록 학습된 제2 인공 지능 모델을 저장하며, 상기 프로세서는 상기 텍스트를 상기 제2 인공 지능 모델에 입력하여 획득된 정보로부터 상기 텍스트 가 상기 제1 정보를 포함하는지 여부를 식별할 수 있다. 그리고, 마이크를 더 포함하며, 상기 메모리는 오디오에 기초하여 감정에 대한 정보를 획득하도록 학습된 제3 인공 지능 모델을 저장하며, 상기 프로세서는 상기 텍스트가 상기 제1 정보를 포함하면 상기 마이크를 턴 온하고, 상기 마이크를 통해 수신된 오디오를 상기 제3 인공 지능 모델에 입력하여 감정에 대한 제3 정보를 획득하 며, 상기 제1 정보, 상기 제2 정보 및 상기 제3 정보에 기초하여 상기 사용자의 감정의 타입을 식별할 수 있다. 또한, 상기 프로세서는 상기 카메라를 통해 실시간으로 복수의 이미지를 획득하고, 상기 텍스트가 상기 제1 정 보를 포함하면, 상기 복수의 이미지 중 상기 텍스트가 입력된 시점부터 획득된 이미지를 상기 제1 인공 지능 모 델에 입력할 수 있다. 그리고, 디스플레이를 더 포함하며, 상기 프로세서는 상기 식별된 사용자의 감정의 타입에 대응되는 적어도 하 나의 추천 이모티콘을 디스플레이하도록 상기 디스플레이를 제어할 수 있다. 또한, 상기 전자 장치는 모바일 디바이스이고, 디스플레이를 더 포함하며, 상기 카메라는 상기 디스플레이의 전 방을 촬영할 수 있다. 한편, 본 개시의 일 실시 예에 따르면, 전자 장치의 제어 방법은 텍스트가 입력되면 상기 텍스트가 사용자의 감 정에 대한 제1 정보를 포함하는지 식별하는 단계, 상기 텍스트가 상기 제1 정보를 포함하면 상기 전자 장치의 카메라를 통해 획득된 이미지를 제1 인공 지능 모델(Artificial Intelligence Model)에 입력하여 감정에 대한 제2 정보를 획득하는 단계 및 상기 텍스트로부터 획득된 상기 제1 정보 및 상기 이미지로부터 획득된 상기 제2 정보에 기초하여 상기 사용자의 감정의 타입을 식별하는 단계를 포함한다. 또한, 상기 제2 정보를 획득하는 단계는 상기 텍스트가 상기 제1 정보를 포함하면 상기 카메라를 턴 온하여 상 기 이미지를 획득하고, 상기 획득된 이미지를 상기 제1 인공 지능 모델에 입력할 수 있다. 그리고, 상기 카메라가 턴 온된 후 추가로 입력된 텍스트가 상기 사용자의 감정에 대한 제1 정보를 포함하지 않 으면 상기 카메라를 턴 오프하는 단계를 더 포함할 수 있다. 또한, 상기 제1 정보는 적어도 하나의 제1 감정 정보 및 상기 적어도 하나의 제1 감정 정보 각각의 제1 신뢰도 정보를 포함하고, 상기 제2 정보는 적어도 하나의 제2 감정 정보 및 상기 적어도 하나의 제2 감정 정보 각각의 제2 신뢰도 정보를 포함하며, 상기 사용자의 감정의 타입을 식별하는 단계는 상기 제1 신뢰도 정보 및 상기 제2 신뢰도 정보를 감정의 타입 별로 가중합(weighted sum)하여 상기 사용자의 감정의 타입을 식별할 수 있다. 그리고, 상기 제1 정보는 적어도 하나의 제1 감정 정보 및 상기 적어도 하나의 제1 감정 정보 각각의 제1 신뢰 도 정보를 포함하고, 상기 제1 정보를 포함하는지 식별하는 단계는 상기 적어도 하나의 제1 감정 정보의 제1 신 뢰도 정보가 임계값 이상인 경우, 상기 텍스트가 상기 사용자의 감정에 대한 정보를 포함하는 것으로 식별할 수 있다. 또한, 상기 제1 정보를 포함하는지 식별하는 단계는 상기 텍스트를 제2 인공 지능 모델에 입력하여 획득된 정보 로부터 상기 텍스트가 상기 제1 정보를 포함하는지 여부를 식별할 수 있다. 그리고, 상기 텍스트가 상기 제1 정보를 포함하면 상기 전자 장치의 마이크를 턴 온하는 단계 및 상기 마이크를 통해 수신된 오디오를 제3 인공 지능 모델에 입력하여 감정에 대한 제3 정보를 획득하는 단계를 더 포함하며, 상기 사용자의 감정의 타입을 식별하는 단계는 상기 제1 정보, 상기 제2 정보 및 상기 제3 정보에 기초하여 상 기 사용자의 감정의 타입을 식별할 수 있다. 또한, 상기 제2 정보를 획득하는 단계는 상기 카메라를 통해 실시간으로 복수의 이미지를 획득하고, 상기 텍스 트가 상기 제1 정보를 포함하면, 상기 복수의 이미지 중 상기 텍스트가 입력된 시점부터 획득된 이미지를 상기 제1 인공 지능 모델에 입력할 수 있다. 그리고, 상기 식별된 사용자의 감정의 타입에 대응되는 적어도 하나의 추천 이모티콘을 상기 전자 장치의 디스 플레이를 통해 디스플레이하는 단계를 더 포함할 수 있다. 또한, 상기 전자 장치는 디스플레이가 구비된 모바일 디바이스이고, 상기 제2 정보를 획득하는 단계는 상기 카 메라를 통해 상기 디스플레이의 전방을 촬영할 수 있다."}
{"patent_id": "10-2019-0124385", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상과 같은 본 개시의 다양한 실시 예에 따르면, 전자 장치는 텍스트로부터 사용자의 감정의 타입이 식별되면 카메라 또는 마이크를 턴 온함에 따라 전력 소모를 줄이고, 카메라 또는 마이크로부터 획득된 이미지 또는 오디 오를 더 고려하여 사용자의 감정의 타입에 대한 신뢰도를 향상시킬 수 있다."}
{"patent_id": "10-2019-0124385", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서, 첨부된 도면을 이용하여 본 개시의 다양한 실시 예들에 대하여 구체적으로 설명한다. 도 1은 전자 장치의 구성의 일 예를 나타내는 블럭도이다. 전자 장치는 사용자와 인터랙션이 가능한 장치로서, 특히 사용자의 감정의 타입을 식별할 수 있는 장치일 수 있다. 예를 들어 전자 장치는 스피커, 스마트폰, 태블릿 PC, 이동 전화기, 영상 전화기, 전자책 리더기, 데스크탑 PC, 랩탑 PC, 넷북 컴퓨터, PDA, PMP(portable multimedia player), MP3 플레이어, 카메라 등과 같은 장치일 수 있다. 다만, 이에 한정되는 것은 아니며, 전자 장치는 사용자와 인터랙션이 가능한 장치라면 어떠한 장치라도 무방하다. 여기서, 사용자의 감정은 어떤 현상이나 일에 대하여 일어나는 마음이나 느끼는 기분을 의미하며, 예를 들어 기 쁨, 즐거움, 슬픔, 화남, 놀람 등일 수 있다. 다만, 이는 일 실시 예에 불과하고, 사용자의 감정은 얼마든지 다 양할 수 있다. 또한, 사용자의 감정은 단순히 정신적인 작용으로 발생하는 것뿐만 아니라, 신체적인 작용에 의 해 발생하는 것도 포함할 수 있다. 가령, 사용자가 감기가 걸림에 따른 아픔도 사용자의 감정에 포함될 수 있다. 전자 장치는 사용자로부터 입력된 텍스트, 사용자의 이미지 또는 사용자의 오디오 중 적어도 하나를 이용 하여 사용자의 감정의 타입을 식별하는 장치일 수 있다. 즉, 전자 장치는 텍스트를 입력받거나, 사용자의 이미지를 촬영하거나 사용자의 오디오를 획득할 수 있다. 예를 들어, 전자 장치는 사용자로부터 텍스트를 입력받고, 텍스트로부터 사용자의 감정의 타입을 식별할 수 있다. 전자 장치는 식별된 사용자의 감정의 타입에 기초하여 동작하는 장치일 수 있다. 예를 들어, 전자 장치 는 디스플레이를 포함하며, 식별된 사용자의 감정의 타입에 대응되는 적어도 하나의 추천 이모티콘을 디스 플레이를 통해 디스플레이하는 장치일 수 있다. 다만, 이에 한정되는 것은 아니며, 전자 장치는 음성 대화 또는 텍스트 대화가 가능한 장치로서, 식별된 사용자의 감정의 타입에 기초하여 사용자와 대화를 수행할 수도 있다. 도 1에 따르면, 전자 장치는 사용자 인터페이스, 카메라, 메모리 및 프로세서를 포함 한다. 사용자 인터페이스는 다양한 사용자 인터랙션(interaction)을 수신한다. 예를 들어, 사용자 인터페이스 는 사용자로부터 텍스트를 입력받는 구성으로, 키보드, 터치 패드, 마우스 등과 같은 장치일 수 있다. 여기서, 전자 장치가 터치 기반의 단말 장치로 구현되는 경우 사용자 인터페이스는 터치 패드와 상호 레이어 구조를 이루는 터치 스크린 형태로 구현될 수도 있다. 이 경우, 사용자 인터페이스는 디스플레이로 사용될 수도 있다. 다만, 이에 한정되는 것은 아니며, 사용자 인터페이스는 사용자로부터 텍스트 입력이 가능한 모든 구성을 포함할 수 있다. 예를 들어, 사용자 인터페이스는 마이크로부터 입력된 오디오를 텍스트로 변환하여 텍스 트를 획득하는 구성을 포함할 수도 있다. 카메라는 사용자 또는 후술할 프로세서의 제어에 따라 정지 영상 또는 동영상을 촬상하기 위한 구성 이다. 카메라는 특정 시점에서의 정지 영상을 촬영할 수 있으나, 연속적으로 정지 영상을 촬영할 수도 있다. 전자 장치가 디스플레이를 포함하는 모바일 디바이스인 경우, 카메라는 디스플레이의 전방을 촬 영하도록 구현될 수 있다. 카메라는 렌즈, 셔터, 조리개, 고체 촬상 소자, AFE(Analog Front End), TG(Timing Generator)를 포함한 다. 셔터는 피사체에 반사된 빛이 카메라로 들어오는 시간을 조절하고, 조리개는 빛이 들어오는 개구부의 크기를 기계적으로 증가 또는 감소시켜 렌즈에 입사되는 광량을 조절한다. 고체 촬상 소자는 피사체에 반사된 빛이 광전하로 축적되면, 광전하에 의한 상을 전기 신호로 출력한다. TG는 고체 촬상 소자의 픽셀 데이터를 리 드아웃 하기 위한 타이밍 신호를 출력하며, AFE는 고체 촬상 소자로부터 출력되는 전기 신호를 샘플링하여 디지 털화한다. 메모리는 다양한 인공 지능 모델(Artificial Intelligence Model)을 저장할 수 있다. 예를 들어, 메모리 는 입력 이미지에 기초하여 감정에 대한 정보를 획득하도록 학습된 제1 인공 지능 모델, 텍스트에 기초하 여 감정에 대한 정보를 획득하도록 학습된 제2 인공 지능 모델 또는 오디오에 기초하여 감정에 대한 정보를 획 득하도록 학습된 제3 인공 지능 모델 중 적어도 하나를 저장할 수 있다. 이하에서는 설명의 편의를 위하여, 입 력 이미지, 텍스트, 오디오 각각에 대응되는 인공 지능 모델을 제1 인공 지능 모델, 제2 인공 지능 모델, 제3 인공 지능 모델로서 설명한다. 또한, 메모리는 사용자로부터 입력된 텍스트, 카메라에 의해 촬영된 이미지, 마이크에 의해 수신된 오디오 등을 저장할 수도 있다. 메모리는 비휘발성 메모리 및 휘발성 메모리 등으로 구현될 수 있으나, 이에 한정되는 것은 아니다. 예를 들어, 메모리 대신 하드디스크가 이용될 수도 있으며, 데이터를 저장할 수 있는 구성이라면 어떠한 구성이 라도 가능하다. 프로세서는 전자 장치의 동작을 전반적으로 제어한다. 구체적으로, 프로세서는 전자 장치 의 각 구성과 연결되어 전자 장치의 동작을 전반적으로 제어할 수 있다. 예를 들어, 프로세서는 사용 자 인터페이스, 카메라 및 메모리와 연결되어 전자 장치의 동작을 제어할 수 있다. 일 실시 예에 따라 프로세서는 디지털 시그널 프로세서(digital signal processor(DSP), 마이크로 프로세 서(microprocessor), TCON(Time controller)으로 구현될 수 있다. 다만, 이에 한정되는 것은 아니며, 중앙처리 장치(central processing unit(CPU)), MCU(Micro Controller Unit), MPU(micro processing unit), 컨트롤러 (controller), 어플리케이션 프로세서(application processor(AP)), 또는 커뮤니케이션 프로세서 (communication processor(CP)), ARM 프로세서 중 하나 또는 그 이상을 포함하거나, 해당 용어로 정의될 수 있 다. 또한, 프로세서는 프로세싱 알고리즘이 내장된 SoC(System on Chip), LSI(large scale integration) 로 구현될 수도 있고, FPGA(Field Programmable gate array) 형태로 구현될 수도 있다. 프로세서는 사용자 인터페이스를 통해 텍스트가 입력되면 텍스트가 사용자의 감정에 대한 제1 정보를 포함하는지 식별할 수 있다. 예를 들어, 프로세서는 사용자 인터페이스를 통해 \"나 오늘 기분 좋 아!\"라는 텍스트가 입력되면 텍스트가 사용자의 감정에 대한 제1 정보를 포함하는 것으로 식별할 수 있다. 여기 서, 제1 정보는 기쁨일 수 있다. 또는, 프로세서는 사용자 인터페이스를 통해 \"10시까지 가야해!\"라 는 텍스트가 입력되면 텍스트가 사용자의 감정에 대한 제1 정보를 포함하지 않는 것으로 식별할 수 있다. 프로세서는 텍스트가 제1 정보를 포함하면 카메라를 통해 획득된 이미지를 제1 인공 지능 모델에 입 력하여 감정에 대한 제2 정보를 획득할 수 있다. 프로세서는 텍스트로부터 획득된 제1 정보 및 이미지로부터 획득된 제2 정보에 기초하여 사용자의 감정의 타입을 식별할 수 있다. 예를 들어, 프로세서는 텍스트로부터 획득된 제1 정보가 기쁨이고, 이미지로부터 획득된 제2 정보가 기쁨이면, 사용자의 감정의 타입을 기쁨으로 식별할 수 있다. 제1 정보 및 제2 정보가 다른 경우에 대하여는 후술한다. 한편, 프로세서는 텍스트가 제1 정보를 포함하면 카메라를 턴 온하여 이미지를 획득하고, 획득된 이 미지를 제1 인공 지능 모델에 입력할 수 있다. 즉, 프로세서는 사용자로부터 텍스트가 입력되고, 입력된 텍스트가 감정에 대한 정보를 포함하는 경우에만 카메라를 턴 온할 수도 있다. 이러한 동작에 따라 카메라 가 소비하는 전력 소모를 줄일 수 있다. 그리고, 프로세서는 카메라가 턴 온된 후 추가로 입력된 텍스트가 사용자의 감정에 대한 제1 정보를 포함하지 않으면 카메라를 턴 오프할 수 있다. 다만, 이에 한정되는 것은 아니며, 프로세서는 카메라가 턴 온된 후, 한 장의 이미지만을 촬영한 후 카메라를 턴 오프할 수도 있다. 또는, 프로세서 는 카메라가 턴 온된 후, 임계 시간 후에 카메라를 턴 오프할 수도 있다. 또는, 프로세서는 카 메라가 턴 온된 후, 촬영된 이미지에 사용자가 인식된 경우에 카메라를 턴 오프할 수도 있다. 한편, 이상에서는 설명의 편의를 위해, 제1 정보가 기쁨인 경우를 설명하였으나, 이에 한정되는 것은 아니다. 예를 들어, 프로세서는 텍스트로부터 복수의 감정에 대한 정보를 식별할 수도 있다. 즉, 제1 정보는 적어 도 하나의 제1 감정 정보 및 적어도 하나의 제1 감정 정보 각각의 제1 신뢰도 정보를 포함하고, 제2 정보는 적 어도 하나의 제2 감정 정보 및 적어도 하나의 제2 감정 정보 각각의 제2 신뢰도 정보를 포함할 수도 있다. 예를 들어, 제1 정보는 기쁨, 즐거움, 평정을 포함하고, 기쁨, 즐거움, 평정 각각이 30%, 70%, 10%의 신뢰도를 갖는 다는 신뢰도 정보를 포함하고, 제2 정보는 즐거움, 평정, 슬픔을 포함하고, 즐거움, 평정, 슬픔 각각이 50%, 30%, 60%의 신뢰도를 갖는다는 신뢰도 정보를 포함할 수 있다. 프로세서는 제1 신뢰도 정보 및 제2 신뢰도 정보를 감정의 타입 별로 가중합(weighted sum)하여 사용자의 감정의 타입을 식별할 수 있다. 상술한 예에 따르면, 프로세서는 기쁨, 즐거움, 평정, 슬픔 각각에 대한 가중합 결과로서 30%, 120%, 40%, 60%의 신뢰도 정보를 획득하고, 사용자의 감정의 타입을 즐거움으로 결정할 수 있다. 여기서, 설명의 편의를 위하여 각각의 가중치가 동일한 것으로 가정하였다. 한편, 제1 정보는 적어도 하나의 제1 감정 정보 및 적어도 하나의 제1 감정 정보 각각의 제1 신뢰도 정보를 포 함하고, 프로세서는 적어도 하나의 제1 감정 정보의 제1 신뢰도 정보가 임계값 이상인 경우, 텍스트가 사 용자의 감정에 대한 정보를 포함하는 것으로 식별할 수 있다. 예를 들어, 제1 정보는 기쁨, 즐거움, 평정을 포 함하고, 기쁨, 즐거움, 평정 각각이 5%, 7%, 1%의 신뢰도를 갖는다는 신뢰도 정보를 포함하며, 임계값이 10%인 경우, 프로세서는 텍스트가 사용자의 감정에 대한 정보를 포함하지 않는 것으로 식별할 수 있다. 이러한 동작은 제2 정보 및 후술할 제3 정보에 대하여도 동일하게 적용될 수 있다. 한편, 메모리는 텍스트에 기초하여 감정에 대한 정보를 획득하도록 학습된 제2 인공 지능 모델을 저장하며, 프로세서는 텍스트를 제2 인공 지능 모델에 입력하여 획득된 정보로부터 텍스트가 제1 정보를 포함하는지 여부를 식별할 수 있다. 다만, 이에 한정되는 것은 아니며, 프로세서는 인공 지능 모델을 이용 하지 않고, 텍스트에 포함된 단어에 기초하여 텍스트가 제1 정보를 포함하는지 식별할 수도 있다. 한편, 전자 장치는 마이크를 더 포함하며, 메모리는 오디오에 기초하여 감정에 대한 정보를 획득하도 록 학습된 제3 인공 지능 모델을 저장하며, 프로세서는 텍스트가 제1 정보를 포함하면 마이크를 턴 온하고, 마이크를 통해 수신된 오디오를 제3 인공 지능 모델에 입력하여 감정에 대한 제3 정보를 획득하며, 제1 정보, 제2 정보 및 제3 정보에 기초하여 사용자의 감정의 타입을 식별할 수도 있다. 즉, 프로세서는 사용 자로부터 텍스트가 입력되고, 입력된 텍스트가 감정에 대한 정보를 포함하는 경우에만 마이크를 턴 온할 수도 있다. 이러한 동작에 따라 마이크가 소비하는 전력 소모를 줄일 수 있다. 다만, 이에 한정되는 것은 아니며, 프로세서는 사용자로부터 텍스트가 입력되고, 입력된 텍스트가 감정에 대한 정보를 포함하는 경우, 마이크를 카메라보다 먼저 턴 온할 수도 있다. 이 경우, 프로세서는 텍 스트로부터 획득된 제1 정보 및 오디오로부터 획득된 제3 정보에 기초하여 사용자의 감정의 타입을 식별할 수도 있다. 또는, 프로세서는 사용자로부터 텍스트가 입력되고, 입력된 텍스트가 감정에 대한 정보를 포함하는 경우, 센서를 통해 사용자가 감지되면 카메라를 턴 온하고, 센서를 통해 사용자가 감지되지 않으면 마이크 를 턴 온할 수도 있다. 또는, 프로세서는 사용자로부터 텍스트가 입력되고, 입력된 텍스트가 감정에 대한 정보를 포함하는 경우, 카메라를 턴 온하여 촬영된 이미지가 사용자를 포함하면 이미지로부터 제2 정보를 획득하고, 촬영된 이미지가 사용자를 포함하지 않으면 마이크를 턴 온할 수도 있다. 한편, 이상에서는 프로세서가 입력된 텍스트에 감정이 포함된 경우, 카메라 또는 마이크를 턴 온하는 것으로 설명하였으나, 이에 한정되는 것은 아니다. 예를 들어, 카메라 또는 마이크는 항상 턴 온되어 있고, 프로세서는 입력된 텍스트에 감정이 포함된 경우, 카메라 또는 마이크로부터 획득된 데이터로 부터 사용자의 감정의 타입을 식별할 수도 있다. 즉, 카메라 또는 마이크로부터 연속적으로 데이터가 획득 되나, 프로세서는 입력된 텍스트에 감정이 포함되었다고 식별하기 전까지는 분석 동작을 수행하지 않을 수 도 있다. 가령, 프로세서는 카메라를 통해 실시간으로 복수의 이미지를 획득하고, 텍스트가 제1 정보를 포함하 면, 복수의 이미지 중 텍스트가 입력된 시점부터 획득된 이미지를 제1 인공 지능 모델에 입력할 수 있다. 또는, 프로세서는 마이크를 통해 실시간으로 오디오를 획득하고, 텍스트가 제1 정보를 포함하면, 오디오 중 텍스트가 입력된 시점부터 획득된 오디오 구간을 제3 인공 지능 모델에 입력할 수도 있다. 이 경우, 카메라 또는 마이크가 꺼져있는 경우보다 전력 소모가 증가할 수는 있으나, 모든 데이터로부터 감정의 타입을 식별하는 경우보다는 전력 소모가 감소할 수 있다. 또한, 카메라 또는 마이크가 꺼져있는 경우보다 카메라 또는 마이크가 켜져있는 경우가 감정의 타입을 식별하는 동작이 좀더 정확할 수도 있다. 예를 들어, 카메라 또는 마이크가 꺼져있는 경우, 이를 다시 턴 온하기까지 일정 시간이 소요될 수 있다. 가령, 텍스트를 분석하여 턴 온을 결정하기까지 필요한 시간, 턴 온 자체에 필요한 시간 및 턴 온 후 카메라 또는 마이크로부터 데이터를 획득하기까지 걸리는 시간이 소요될 수 있으며, 이러한 시간이 경과한 후에 사용자의 감정 상태가 순간적으로 변경될 수도 있기 때문에 정확도가 좀더 낮아질 수도 있다. 한편, 전자 장치는 디스플레이를 더 포함하며, 프로세서는 식별된 사용자의 감정의 타입에 대응되는 적어도 하나의 추천 이모티콘을 디스플레이하도록 디스플레이를 제어할 수 있다. 또는, 전자 장치는 모바일 디바이스이고, 디스플레이를 더 포함하며, 카메라는 디스플레이의 전방을 촬영하도록 구현될 수도 있다. 한편, 본 개시에 따른 인공 지능과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공 지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인공 지능 모델에 따라, 입력 데이터 를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공 지능 전용 프로세서인 경우, 인공 지능 전용 프로세서는, 특정 인공 지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공 지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만들어진다는 것은, 기본 인공 지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들 을 이용하여 학습됨으로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공 지능 모델이 만들어짐을 의미한다. 이러한 학습은 본 개시에 따른 인공 지능이 수행되는 기기 자체에서 이루어질 수 도 있고, 별도의 서버 및/또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습 (supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또 는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 인공 지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경 망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공 지능 모델의 학습 결과에 의 해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공 지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네 트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 한편, 전자 장치는 디스플레이, 마이크, 통신 인터페이스를 더 포함할 수도 있다. 디스플레이는 LCD(Liquid Crystal Display), OLED(Organic Light Emitting Diodes) 디스플레이, PDP(Plasma Display Panel) 등과 같은 다양한 형태의 디스플레이로 구현될 수 있다. 디스플레이 내에는 a- si TFT, LTPS(low temperature poly silicon) TFT, OTFT(organic TFT) 등과 같은 형태로 구현될 수 있는 구동 회로, 백라이트 유닛 등도 함께 포함될 수 있다. 한편, 디스플레이는 터치 센서와 결합된 터치 스크린, 플 렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display) 등으로 구현될 수 있다. 마이크는 사용자 음성이나 기타 소리를 입력받아 오디오 데이터로 변환하기 위한 구성이다. 마이크는 활성화 상태에서 사용자의 음성을 수신할 수 있다. 예를 들어, 마이크는 전자 장치의 상측이나 전면 방향, 측면 방향 등에 일체화된 일체형으로 형성될 수 있다. 마이크는 아날로그 형태의 사 용자 음성을 수집하는 마이크, 수집된 사용자 음성을 증폭하는 앰프 회로, 증폭된 사용자 음성을 샘플링하여 디지털 신호로 변환하는 A/D 변환회로, 변환된 디지털 신호로부터 노이즈 성분을 제거하는 필터 회로 등과 같은 다양한 구성을 포함할 수 있다. 한편, 전자 장치는 마이크를 포함하는 센서 장치로부터 사용자 음성을 포함하는 오디오 신호를 수신할 수 있다. 이 경우, 수신된 오디오 신호는 디지털 음성 신호일 수 있으나, 구현 예에 따라 아날로그 음성 신호일 수 있다. 일 예로, 전자 장치는 Bluetooth 또는 Wi-Fi 등의 무선 통신 방법을 통해 오디오 신호를 수신할 수 있다. 통신 인터페이스는 다양한 유형의 통신방식에 따라 다양한 유형의 외부 장치와 통신을 수행하는 구성이다. 통신 인터페이스는 와이파이 모듈, 블루투스 모듈, 적외선 통신 모듈 및 무선 통신 모듈 등을 포함한다. 여기서, 각 통신 모듈은 적어도 하나의 하드웨어 칩 형태로 구현될 수 있다. 프로세서는 통신 인터페이스를 이용하여 각종 외부 장치와 통신을 수행할 수 있다. 여기서, 외부 장 치는 TV와 같은 디스플레이 장치, 외부 서버, 블루투스 이어폰 등을 포함할 수 있다. 와이파이 모듈, 블루투스 모듈은 각각 WiFi 방식, 블루투스 방식으로 통신을 수행한다. 와이파이 모 듈이나 블루투스 모듈을 이용하는 경우에는 SSID 및 세션 키 등과 같은 각종 연결 정보를 먼저 송수 신하여, 이를 이용하여 통신 연결한 후 각종 정보들을 송수신할 수 있다. 적외선 통신 모듈은 시 광선과 밀리미터파 사이에 있는 적외선을 이용하여 근거리에 무선으로 데이터를 전 송하는 적외선 통신(IrDA, infrared Data Association)기술에 따라 통신을 수행한다. 무선 통신 모듈은 상술한 통신 방식 이외에 지그비(zigbee), 3G(3rd Generation), 3GPP(3rd Generation Partnership Project), LTE(Long Term Evolution), LTE-A(LTE Advanced), 4G(4th Generation), 5G(5th Generation)등과 같은 다양한 무선 통신 규격에 따라 통신을 수행하는 적어도 하나의 통신 칩을 포함할 수 있다. 그 밖에 통신 인터페이스는 LAN(Local Area Network) 모듈, 이더넷 모듈, 또는 페어 케이블, 동축 케이블 또는 광섬유 케이블 등을 이용하여 통신을 수행하는 유선 통신 모듈 중 적어도 하나를 포함할 수 있다. 통신 인터페이스는 입출력 인터페이스를 더 포함할 수 있다. 입출력 인터페이스는 HDMI(High Definition Multimedia Interface), MHL (Mobile High-Definition Link), USB (Universal Serial Bus), DP(Display Port), 썬더볼트(Thunderbolt), VGA(Video Graphics Array)포트, RGB 포트, D-SUB(D-subminiature), DVI(Digital Visual Interface) 중 어느 하나의 인터페이스일 수 있다. 입출력 인터페이스는 오디오 및 비디오 신호 중 적어도 하나를 입출력 할 수 있다. 구현 예에 따라, 입출력 인터페이스는 오디오 신호만을 입출력하는 포트와 비디오 신호만을 입출력하는 포트를 별개의 포트로 포함하거나, 오디오 신호 및 비디오 신호를 모두 입출력하는 하나의 포트로 구현될 수 있다. 그 밖에 전자 장치는 스피커(미도시)를 더 포함할 수도 있으며, 스피커는 입출력 인터페이스에서 처리된 각종 오디오 데이터뿐만 아니라 각종 알림 음이나 음성 메시지 등을 출력하는 구성 요소일 수 있다. 이상과 같이 전자 장치는 텍스트가 감정에 대한 정보를 포함하면, 카메라 또는 마이크 중 적어도 하 나로부터 획득된 데이터를 더 이용하여 사용자의 감정의 타입을 식별할 수 있다. 이하에서는 도 2 내지 도 6c를 통해 전자 장치의 동작을 좀더 구체적으로 설명한다. 도 2 내지 도 6c에서 는 설명의 편의를 위해 개별적인 실시 예에 대하여 설명한다. 다만, 도 2 내지 도 6c의 개별적인 실시 예는 얼 마든지 조합된 상태로 실시될 수도 있다. 도 2는 본 개시의 일 실시 예에 따른 사용자의 감정의 타입을 식별하는 방법을 설명하기 위한 도면이다. 도 2에 도시된 바와 같이, 프로세서는 제1 인공 지능 모델(Neural Network 1, NN1, 210), 제2 인공 지능 모델(Neural Network 2, NN2, 220) 및 제3 인공 지능 모델(Neural Network 3, NN3, 230)을 이용하여 사용자의 감정의 타입을 식별할 수 있다. 여기서, 제1 인공 지능 모델, 제2 인공 지능 모델 및 제3 인공 지능 모델은 메모리에 저장된 정보일 수 있다. 프로세서는 입력된 텍스트가 사용자의 감정에 대한 제1 정보를 포함하는지 식별하기 위해, 텍스트를 제2 인공 지능 모델에 입력할 수 있다. 여기서, 제2 인공 지능 모델은 텍스트에 기초하여 감정에 대한 정 보를 획득하도록 학습된 모델로서, 복수의 샘플 텍스트 및 복수의 샘플 텍스트에 각각 대응되는 감정에 대한 정보를 학습하여 획득될 수 있다. 특히, 제2 인공 지능 모델은 단어 뿐만 아니라 문장에 포함된 단어 간의 관계 및 대화 내에서 문장 간의 관계를 학습한 모델일 수 있다. 제2 인공 지능 모델의 출력(b)은 감정의 타입 별 신뢰도 정보를 포함할 수 있다. 예를 들어, 텍스트가 제2 인공 지능 모델에 입력되면, 기쁨(30%), 즐거움(5%)이 출력될 수 있다. 여기서, 괄호 안은 감정의 타입 별 신뢰도 정보이며, 높을수록 정확하다. 프로세서는 임계값 이상의 신뢰도 정보를 갖는 감정만을 이용할 수 있다. 이상의 예에서 임계값이 10%이면, 프로세서는 텍스트로부터 기쁨(30%)만을 제1 정보로서 이용할 수 있다. 이 경우, 텍스트가 제1 정보를 포함하므로, 프로세서는 카메라 또는 마이크를 이용하는 추가 동작을 수행하게 된다. 만약, 제2 인공 지능 모델의 출력(b)이 즐거움(5%)이면, 프로세서는 텍스트가 제1 정보를 포함하지 않는 것으로 식별하고, 카메라 또는 마이크를 이용하는 추가 동작을 수행하지 않게 된다. 다만, 이에 한정되는 것은 아니며, 제2 인공 지능 모델 자체가 임계값 이하의 신뢰도를 갖는 감정의 타입 을 출력하지 않도록 학습될 수도 있다. 이상의 예에서 임계값이 10%이며, 제2 인공 지능 모델은 기쁨 (30%), 즐거움(5%) 중 기쁨(30%)만을 출력할 수도 있다. 이 경우, 프로세서는 제2 인공 지능 모델의 출력이 있는 경우, 카메라 또는 마이크를 이용하는 추가 동작을 수행하고, 제2 인공 지능 모델 의 출력이 없는 경우, 카메라 또는 마이크를 이용하는 추가 동작을 수행하지 않을 수 있다. 프로세서는 텍스트가 제1 정보를 포함하면 카메라를 통해 획득된 이미지를 제1 인공 지능 모델 에 입력하여 제1 인공 지능 모델의 출력(a)으로서 감정에 대한 제2 정보를 획득할 수 있다. 여기서, 제1 인공 지능 모델은 이미지에 기초하여 감정에 대한 정보를 획득하도록 학습된 모델로서, 복수의 샘플 이미 지 및 복수의 샘플 이미지에 각각 대응되는 감정에 대한 정보를 학습하여 획득될 수 있다. 또는, 프로세서는 텍스트가 제1 정보를 포함하면 마이크를 통해 획득된 오디오를 제3 인공 지능 모델 에 입력하여 제3 인공 지능 모델의 출력(c)으로서 감정에 대한 제3 정보를 획득할 수 있다. 여기서, 제3 인공 지능 모델은 오디오에 기초하여 감정에 대한 정보를 획득하도록 학습된 모델로서, 복수의 샘플 오디오 및 복수의 샘플 오디오에 각각 대응되는 감정에 대한 정보를 학습하여 획득될 수 있다. 프로세서는 제1 정보, 제2 정보 및 제3 정보에 기초하여 사용자의 감정의 타입을 식별할 수 있다. 예를 들 어, 프로세서는 하기와 같은 수식으로 감정의 타입 별 가중합을 수행할 수 있다. 감정의 타입(i) 별 신뢰도 = Wa × ai + Wb × bi + Wc × ci 여기서, i는 감정의 타입을 나타내며, ai는 이미지로부터 획득된 적어도 하나의 감정의 타입 중 i번째 감정의 타입의 신뢰도, bi는 텍스트로부터 획득된 적어도 하나의 감정의 타입 중 i번째 감정의 타입의 신뢰도, ci는 오 디오로부터 획득된 적어도 하나의 감정의 타입 중 i번째 감정의 타입의 신뢰도, Wa는 이미지로부터 획득된 신뢰 도의 가중치, Wb는 텍스트로부터 획득된 신뢰도의 가중치, Wc는 오디오로부터 획득된 신뢰도의 가중치를 나타낸 다. 한편, 이상에서는 텍스트가 제1 정보를 포함하면, 제2 정보 또는 제3 정보 중 적어도 하나를 더 고려하여 사용 자의 감정의 타입을 식별하는 것으로 설명하였으나, 이에 한정되는 것은 아니다. 예를 들어, 프로세서는 텍스트가 제1 정보를 포함하면, 제2 정보 또는 제3 정보 중 적어도 하나에 기초하 여 제1 정보를 업데이트할 수도 있다. 그리고, 프로세서는 업데이트된 제1 정보, 제2 정보 및 제3 정보에 기초하여 사용자의 감정의 타입을 식별할 수도 있다. 즉, 프로세서는 이상의 수식에 제1 정보가 아닌 업데 이트된 제1 정보를 입력하여 사용자의 감정의 타입을 식별할 수도 있다. 이상과 같은 방법을 통해 텍스트로부터 획득된 사용자의 감정의 정확도를 좀더 향상시킬 수 있다. 또한, 텍스트 로부터 감정이 식별된 경우에만 카메라 또는 마이크를 이용하므로, 종래보다 전력 소모를 줄일 수 있 다. 도 3은 본 개시의 일 실시 예에 따른 감정의 타입을 식별하는 동작을 설명하기 위한 도면이다. 도 3에 포함된 복수의 유닛은 모듈과 같이 소프트웨어적으로 구현될 수도 있고, 프로세서의 일부 구성으로서 하드웨어로 구현될 수도 있다. 도 3에 도시된 바와 같이, 입력 유닛은 사용자 입력 또는 다른 입력을 수신할 수 있다. 예를 들어, 입력 유닛은 사용자로부터의 텍스트를 입력받을 수 있다. 또는, 입력 유닛은 사용자의 이미지 또는 사용자 의 오디오를 획득할 수 있으며, 이는 딥 러닝 유닛의 제어 신호에 의할 수 있다. 예를 들어, 딥 러닝 유닛 의 제2 인공 지능 모델의 출력이 감정을 포함하면, 카메라 또는 마이크를 턴 온시키기 위한 제 어 신호가 생성될 수 있으며, 입력 유닛은 제어 신호에 기초하여 사용자의 이미지 또는 사용자의 오디오를 획득할 수 있다. 딥 러닝 유닛은 복수의 인공 지능 모델을 포함하며, 입력된 데이터의 타입에 대응되는 인공 지능 모델로 입력된 데이터를 처리할 수 있다. 또한, 딥 러닝 유닛은 복수의 인공 지능 모델 각각의 출력으로부터 사용자의 감정의 타입을 식별할 수 있 다. 딥 러닝 유닛은 다른 입력을 이용하기 위한 제어 신호를 생성하여 입력 유닛으로 제공할 수 있고, 식 별된 감정의 타입을 상태 관리 유닛으로 제공할 수 있다. 상태 관리 유닛은 사용자의 감정의 타입을 저장할 수 있다. 그리고, 상태 관리 유닛은 서비스 유닛 의 요청에 따라 사용자의 감정의 타입을 서비스 유닛으로 제공할 수 있다. 서비스 유닛은 상태 관리 유닛으로 사용자의 감정의 타입을 요청하여 수신하고, 수신된 정보에 기초 하여 서비스를 제공할 수 있다. 예를 들어, 서비스 유닛 중 하나가 메시지 어플리케이션이면, 메시지 어플 리케이션은 상태 관리 유닛으로 사용자의 감정의 타입을 요청하여 화남이라는 감정의 타입을 수신하면, 화 난 표정의 이모티콘들을 추천 이모티콘으로 사용자에게 제공할 수 있다. 또는, 서비스 유닛 중 하나가 음악 어플리케이션이면, 음악 어플리케이션은 상태 관리 유닛으로 사용 자의 감정의 타입을 요청하여 평온이라는 감정의 타입을 수신하면, 클래식 음악을 추천하거나 재생할 수도 있다. 도 4는 본 개시의 일 실시 예에 따른 감정의 타입을 식별하는 동작을 좀더 구체적으로 설명하기 위한 도면이다. 도 4에 도시된 구성요소들 중 도 3에 도시된 구성요소와 중복되는 부분에 대해서는 자세한 설명을 생략하도록 한다. 이상에서는 텍스트가 감정을 포함하는 경우, 카메라 또는 마이크가 턴 온되는 것으로 설명하였으나, 이에 한정되는 것은 아니다. 예를 들어, 사용자가 텍스트를 입력하는 경우, 전자 장치의 움직임이 자이로 (Gyro) 센서에 의해 감지될 수 있다. 프로세서는 움직임이 감지되면, 카메라 또는 마이크를 턴 온시켜 이미지 또는 오디오를 획득할 수 있다. 이러한 동작을 통해, 프로세서는 텍스트를 입력하는 시점의 사용자의 이미지 또는 오디오를 획득할 수 있다. 특히, 프로세서는 이미지의 경우, 시간 제어(Time control)를 통해 일정 시간 동안 주기적으로 복수의 이미지를 획득할 수도 있다. 이후, 프로세서는 텍스트에 감정이 포함된 것으로 식별되면, 획득된 이미지 또는 오디오를 대응되는 인공 지능 모델로 입력할 수 있다. 여기서, 자이로 센서에 의해 이미지 또는 오디오를 획득한 경우라면, 프로세서 는 텍스트에 감정이 포함된 것으로 식별되더라도, 다시 카메라 또는 마이크를 턴 온시키지 않을 수 있다. 도 5a 내지 도 5c는 본 개시의 일 실시 예를 구체적으로 설명하기 위한 도면들이다. 도 5a에 도시된 바와 같이, 프로세서는 전자 장치의 사용자 A와 그 상대방인 사용자 B의 대화 어플 리케이션을 구동할 수 있다. 도 5b에 도시된 바와 같이, 프로세서는 사용자 A의 텍스트 중 일부 단어에 기초하여 사용자의 복수의 감정 의 타입을 식별하고, 각각의 신뢰도 정보를 산출할 수 있다. 그리고, 프로세서는 사용자 A의 추가 발화로부터 사용자의 감정의 타입을 슬픔으로 식별할 수 있다. 프로세서는 도 5c에 도시된 바와 같이, 텍스트가 감정을 포함함에 따라, 카메라를 턴 온시켜 사용자 의 이미지를 촬영하고, 촬영된 이미지를 제1 인공 지능 모델에 입력할 수 있다. 프로세서는 제1 인공 지능모델의 출력을 더 고려하여 사용자의 감정의 타입을 슬픔으로 확정할 수 있다. 여기서, 텍스트만을 이용하면 슬픔의 신뢰도가 70%이나, 이미지를 추가로 이용함에 따라 슬픔의 신뢰도를 100% 로 향상시킬 수 있다. 즉, 프로세서는 카메라에 의해 촬영된 이미지를 추가 이용하여, 텍스트로부터 획득된 감정의 타입의 신뢰도를 향상시킬 수 있다. 도 6a 내지 도 6c는 본 개시의 다양한 실시 예에 따른 전력 소모를 설명하기 위한 도면들이다. 도 6a 내지 도 6c에서는 설명의 편의를 위하여 카메라의 이용 및 전력 소모를 도시하였으며, 카메라에 대한 제어는 얇은 선으로, 전력 소모는 두꺼운 선으로 도시하였다. 도 6a는 종래 기술에 따른 전력 소모를 나타내며, 카메라에 대한 제어가 1회 있더라도, 이 경우는 카메라를 라 이브뷰로 유지하기 위한 제어로서 전력 소모는 지속적으로 유지될 수 있다. 도 6b는 자이로 센서를 이용한 경우로서, 자이로 센서에 의해 전자 장치의 움직임이 감지되면, 해당 순간 에 이미지를 촬영하고, 다시 카메라가 턴 오프될 수 있다. 그에 따라, 이미지의 촬영 순간에만 전력이 소 모될 수 있다. 도 6c는 텍스트가 감정을 포함하는 경우, 해당 순간에 이미지를 촬영하고, 다시 카메라가 턴 오프될 수 있 으며, 도 6b와 유사하게 전력 소모가 감소될 수 있다. 즉, 도 6b와 도 6c와 같이 특정 시점에만 카메라가 구동되어 전력 소모를 최소화할 수 있으며, 카메라 의 구동을 최소화하면서도 이미지에 의한 사용자의 감정의 타입을 식별하게 되어 정확도가 유지될 수 있다. 도 7은 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도이다. 먼저, 텍스트가 입력되면 텍스트가 사용자의 감정에 대한 제1 정보를 포함하는지 식별한다(S710). 그리고, 텍스 트가 제1 정보를 포함하면 전자 장치의 카메라를 통해 획득된 이미지를 제1 인공 지능 모델(Artificial Intelligence Model)에 입력하여 감정에 대한 제2 정보를 획득(S720)한다. 그리고, 텍스트로부터 획득된 제1 정 보 및 이미지로부터 획득된 제2 정보에 기초하여 사용자의 감정의 타입을 식별한다(S730). 그리고, 제2 정보를 획득하는 단계(S720)는 텍스트가 제1 정보를 포함하면 카메라를 턴 온하여 이미지를 획득하 고, 획득된 이미지를 제1 인공 지능 모델에 입력할 수 있다. 여기서, 카메라가 턴 온된 후 추가로 입력된 텍스트가 사용자의 감정에 대한 제1 정보를 포함하지 않으면 카메 라를 턴 오프하는 단계를 더 포함할 수 있다. 한편, 제1 정보는 적어도 하나의 제1 감정 정보 및 적어도 하나의 제1 감정 정보 각각의 제1 신뢰도 정보를 포 함하고, 제2 정보는 적어도 하나의 제2 감정 정보 및 적어도 하나의 제2 감정 정보 각각의 제2 신뢰도 정보를 포함하며, 사용자의 감정의 타입을 식별하는 단계(S730)는 제1 신뢰도 정보 및 제2 신뢰도 정보를 감정의 타입 별로 가중합(weighted sum)하여 사용자의 감정의 타입을 식별할 수 있다. 또는, 제1 정보는 적어도 하나의 제1 감정 정보 및 적어도 하나의 제1 감정 정보 각각의 제1 신뢰도 정보를 포 함하고, 제1 정보를 포함하는지 식별하는 단계(S710)는 적어도 하나의 제1 감정 정보의 제1 신뢰도 정보가 임계 값 이상인 경우, 텍스트가 사용자의 감정에 대한 정보를 포함하는 것으로 식별할 수 있다. 한편, 제1 정보를 포함하는지 식별하는 단계(S710)는 텍스트를 제2 인공 지능 모델에 입력하여 획득된 정보로부 터 텍스트가 제1 정보를 포함하는지 여부를 식별할 수 있다. 그리고, 텍스트가 제1 정보를 포함하면 전자 장치의 마이크를 턴 온하는 단계 및 마이크를 통해 수신된 오디오 를 제3 인공 지능 모델에 입력하여 감정에 대한 제3 정보를 획득하는 단계를 더 포함하며, 사용자의 감정의 타 입을 식별하는 단계(S730)는 제1 정보, 제2 정보 및 제3 정보에 기초하여 사용자의 감정의 타입을 식별할 수 있 다. 한편, 제2 정보를 획득하는 단계(S720)는 카메라를 통해 실시간으로 복수의 이미지를 획득하고, 텍스트가 제1 정보를 포함하면, 복수의 이미지 중 텍스트가 입력된 시점부터 획득된 이미지를 제1 인공 지능 모델에 입력할 수 있다. 그리고, 식별된 사용자의 감정의 타입에 대응되는 적어도 하나의 추천 이모티콘을 전자 장치의 디스플레이를 통 해 디스플레이하는 단계를 더 포함할 수 있다.한편, 전자 장치는 디스플레이가 구비된 모바일 디바이스이고, 제2 정보를 획득하는 단계(S720)는 카메라를 통 해 디스플레이의 전방을 촬영할 수 있다. 이상과 같은 본 개시의 다양한 실시 예에 따르면, 전자 장치는 텍스트로부터 사용자의 감정의 타입이 식별되면 카메라 또는 마이크를 턴 온함에 따라 전력 소모를 줄이고, 카메라 또는 마이크로부터 획득된 이미지 또는 오디 오를 더 고려하여 사용자의 감정의 타입에 대한 신뢰도를 향상시킬 수 있다. 한편, 본 개시의 일시 예에 따르면, 이상에서 설명된 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media)에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실 시 예들에 따른 전자 장치(예: 전자 장치(A))를 포함할 수 있다. 명령이 프로세서에 의해 실행될 경우, 프로세 서가 직접, 또는 프로세서의 제어 하에 다른 구성요소들을 이용하여 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저 장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장매체가 신 호(signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임시 적으로 저장됨을 구분하지 않는다. 또한, 본 개시의 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품 (computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있 다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 또한, 본 개시의 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들은 소프트웨어(software), 하드웨어 (hardware) 또는 이들의 조합을 이용하여 컴퓨터(computer) 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 일부 경우에 있어 본 명세서에서 설명되는 실시 예들이 프로세서 자체로 구현될 수 있 다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들은 별도의 소프트웨 어 모듈들로 구현될 수 있다. 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 동작을 수행할 수 있다. 한편, 상술한 다양한 실시 예들에 따른 기기의 프로세싱 동작을 수행하기 위한 컴퓨터 명령어(computer instructions)는 비일시적 컴퓨터 판독 가능 매체(non-transitory computer-readable medium)에 저장될 수 있 다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령어는 특정 기기의 프로세서에 의해 실행되었 을 때 상술한 다양한 실시 예에 따른 기기에서의 처리 동작을 특정 기기가 수행하도록 한다. 비일시적 컴퓨터 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체가 아니라 반영구 적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 비일시적 컴퓨터 판독 가능 매체의 구체적인 예로는, CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등이 있을 수 있다. 또한, 상술한 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구 성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요 소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로 그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작 들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생 략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2019-0124385", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2019-0124385", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 전자 장치의 구성의 일 예를 나타내는 블럭도이다. 도 2는 본 개시의 일 실시 예에 따른 사용자의 감정의 타입을 식별하는 방법을 설명하기 위한 도면이다. 도 3은 본 개시의 일 실시 예에 따른 감정의 타입을 식별하는 동작을 설명하기 위한 도면이다. 도 4는 본 개시의 일 실시 예에 따른 감정의 타입을 식별하는 동작을 좀더 구체적으로 설명하기 위한 도면이다. 도 5a 내지 도 5c는 본 개시의 일 실시 예를 구체적으로 설명하기 위한 도면들이다. 도 6a 내지 도 6c는 본 개시의 다양한 실시 예에 따른 전력 소모를 설명하기 위한 도면들이다. 도 7은 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도이다."}
