{"patent_id": "10-2019-0119791", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0037307", "출원번호": "10-2019-0119791", "발명의 명칭": "전자 장치 및 전자 장치의 제어 방법", "출원인": "삼성전자주식회사", "발명자": "윤윤진"}}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,적어도 하나의 인스트럭션(instruction)을 저장하는 메모리; 및상기 적어도 하나의 인스트럭션을 실행하는 프로세서; 를 포함하고,상기 프로세서는, 상기 적어도 하나의 인스트럭션을 실행함으로써,제1 언어의 제1 문장이 획득되면, 학습 코퍼스를 바탕으로 학습된 적어도 하나의 언어 모델을 이용하여 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정할 지 여부를 결정하고,상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하는 것으로 결정되면, 입력 문장과 임계 값 이상의 유사도를 갖는 다른 문장을 획득하도록 학습된 변환 모델에 상기 제1 문장을 입력하여 상기 제1 문장이 수정된 상기제1 언어의 제2 문장을 획득하며,상기 제2 문장이 획득되면, 상기 학습 코퍼스를 바탕으로 학습된 번역 모델에 상기 제2 문장을 입력하여 제2 언어의 제3 문장을 획득하는 전자 장치."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 적어도 하나의 언어 모델은 상기 학습 코퍼스 중 병렬 코퍼스(parallel corpus)를 바탕으로 학습된 제1 언어 모델 및 상기 학습 코퍼스 중 단일어 코퍼스(mono corpus)를 바탕으로 학습된 제2 언어 모델을 포함하고, 상기 프로세서는,상기 제1 언어 모델 및 상기 제2 언어 모델 각각에 상기 제1 문장을 입력하여 상기 제1 언어 모델에 대한 제1퍼플렉서티(perplexity) 값 및 제2 언어 모델에 대한 제2 퍼플렉서티 값을 획득하고,상기 제1 퍼플렉서티 값 및 상기 제2 퍼플렉서티 값을 바탕으로 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정할지 여부를 결정하는 전자 장치."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 프로세서는,상기 제1 퍼플렉서티 값이 기 설정된 제1 임계 값보다 크고 상기 제2 퍼플렉서티 값이 기 설정된 제2 임계 값보다 작으면, 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하는 것으로 결정하는 전자 장치."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 프로세서는,상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하는 것으로 결정되면, 상기 제1 문장 및 상기 제1 문장과의 유사도에 관련된 임계 값을 상기 변환 모델에 입력하여 상기 제1 문장과 상기 임계 값 이상의 유사도를 갖는공개특허 10-2021-0037307-3-상기 제2 문장을 획득하는 전자 장치."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2 항에 있어서,상기 변환 모델은,상기 단일어 코퍼스에 포함된 복수의 문장 및 상기 병렬 코퍼스에 포함된 복수의 문장 사이의 유사도를 바탕으로 학습되는 전자 장치."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제2 항에 있어서,상기 변환 모델은,상기 단일어 코퍼스에 포함된 상기 제1 언어의 복수의 문장 및 상기 제2 언어를 상기 제1 언어로 번역하기 위한병렬 코퍼스에 포함된 상기 제1 언어의 복수의 문장 사이의 유사도를 바탕으로 학습되는 전자 장치."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제2 항에 있어서,상기 변환 모델은,상기 제1 언어를 상기 제2 언어로 번역하기 위한 병렬 코퍼스에 포함된 제2 언어의 복수의 문장 사이의 유사도를 바탕으로 학습되는 전자 장치."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항에 있어서,상기 적어도 하나의 언어 모델, 상기 변환 모델 및 상기 번역 모델 중 적어도 두 개 이상은 통합된 하나의 인공지능 모델에 포함되는 전자 장치."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제2 항에 있어서,상기 프로세서는,상기 제1 퍼플렉서티 값이 상기 기 설정된 제1 임계 값보다 작거나 상기 제2 퍼플렉서티 값이 상기 기 설정된제2 임계 값보다 크면, 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하지 않는 것으로 결정하고,상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하지 않는 것으로 결정되면, 학습된 번역 모델에 상기 제1문장을 입력하여 상기 제2 언어의 제3 문장을 획득하는 전자 장치."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항에 있어서,회로를 포함하는 통신부; 를 포함하고,공개특허 10-2021-0037307-4-상기 프로세서는,상기 통신부를 통해 상기 전자 장치와 연결된 외부 장치로부터 상기 제1 문장을 수신하여 획득하고,상기 제3 문장이 획득되면, 상기 외부 장치에 상기 제3 문장을 전송하도록 상기 통신부를 제어하는 전자 장치."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "전자 장치의 제어 방법에 있어서,제1 언어의 제1 문장이 획득되면, 학습 코퍼스를 바탕으로 학습된 적어도 하나의 언어 모델을 이용하여 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정할 지 여부를 결정하는 단계; 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하는 것으로 결정되면, 입력 문장과 임계 값 이상의 유사도를 갖는 다른 문장을 획득하도록 학습된 변환 모델에 상기 제1 문장을 입력하여 상기 제1 문장이 수정된 상기제1 언어의 제2 문장을 획득하는 단계; 및 상기 제2 문장이 획득되면, 상기 학습 코퍼스를 바탕으로 학습된 번역 모델에 상기 제2 문장을 입력하여 상기제2 언어의 제3 문장을 획득하는 단계; 를 포함하는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서, 상기 적어도 하나의 언어 모델은 상기 학습 코퍼스 중 병렬 코퍼스(parallel corpus)를 바탕으로 학습된 제1 언어 모델 및 상기 학습 코퍼스 중 단일어 코퍼스(mono corpus)를 바탕으로 학습된 제2 언어 모델을 포함하고, 상기 제1 문장을 다른 문장으로 수정할 지 여부를 결정하는 단계는,상기 제1 언어 모델 및 상기 제2 언어 모델 각각에 상기 제1 문장을 입력하여 상기 제1 언어 모델에 대한 제1퍼플렉서티(perplexity) 값 및 제2 언어 모델에 대한 제2 퍼플렉서티 값을 획득하는 단계; 및 상기 제1 퍼플렉서티 값 및 상기 제2 퍼플렉서티 값을 바탕으로 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정할지 여부를 결정하는 단계; 를 포함하는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12 항에 있어서,상기 제1 문장을 다른 문장으로 수정할 지 여부를 결정하는 단계는,상기 제1 퍼플렉서티 값이 기 설정된 제1 임계 값보다 크고 상기 제2 퍼플렉서티 값이 기 설정된 제2 임계 값보다 작으면, 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하는 것으로 결정하는 단계; 를 포함하는 전자장치의 제어 방법."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11 항에 있어서,상기 제2 문장을 획득하는 단계는,상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하는 것으로 결정되면, 상기 제1 문장 및 상기 제1 문장과의 유사도에 관련된 임계 값을 상기 변환 모델에 입력하여 상기 제1 문장과 상기 임계 값 이상의 유사도를 갖는상기 제2 문장을 획득하는 단계; 를 포함하는 전자 장치의 제어 방법.공개특허 10-2021-0037307-5-청구항 15 제11 항에 있어서,상기 변환 모델은,상기 단일어 코퍼스에 포함된 복수의 문장 및 상기 병렬 코퍼스에 포함된 복수의 문장 사이의 유사도를 바탕으로 학습되는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11 항에 있어서,상기 변환 모델은, 상기 단일어 코퍼스에 포함된 상기 제1 언어의 복수의 문장 및 상기 제2 언어를 상기 제1 언어로 번역하기 위한병렬 코퍼스에 포함된 상기 제1 언어의 복수의 문장 사이의 유사도를 바탕으로 학습되는 전자 장치의 제어방법."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11 항에 있어서,상기 변환 모델은,상기 제1 언어를 상기 제2 언어로 번역하기 위한 병렬 코퍼스에 포함된 제2 언어의 복수의 문장 사이의 유사도를 바탕으로 학습되는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11 항에 있어서,상기 적어도 하나의 언어 모델, 상기 변환 모델 및 상기 번역 모델 중 적어도 두 개 이상은 통합된 하나의 인공지능 모델에 포함되는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제12 항에 있어서, 상기 제1 문장을 다른 문장으로 수정할 지 여부를 결정하는 단계는,상기 제1 퍼플렉서티 값이 상기 기 설정된 제1 임계 값보다 작거나 상기 제2 퍼플렉서티 값이 상기 기 설정된제2 임계 값보다 크면, 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하지 않는 것으로 결정하는 단계;를 포함하고,상기 제3 문장을 획득하는 단계는,상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하지 않는 것으로 결정되면, 학습된 번역 모델에 상기 제1문장을 입력하여 상기 제2 언어의 제3 문장을 획득하는 단계; 를 포함하는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0119791", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "전자 장치의 제어 방법을 실행하는 프로그램을 포함하는 컴퓨터 판독 가능 기록매체에 있어서,상기 전자 장치의 제어 방법은,공개특허 10-2021-0037307-6-제1 언어의 제1 문장이 획득되면, 학습 코퍼스를 바탕으로 학습된 적어도 하나의 언어 모델을 이용하여 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정할 지 여부를 결정하는 단계; 상기 제1 문장을 상기 제1 언어의 다른 문장으로 수정하는 것으로 결정되면, 입력 문장과 임계 값 이상의 유사도를 갖는 다른 문장을 획득하도록 학습된 변환 모델에 상기 제1 문장을 입력하여 상기 제1 문장이 수정된 상기제1 언어의 제2 문장을 획득하는 단계; 및 상기 제2 문장이 획득되면, 상기 학습 코퍼스를 바탕으로 학습된 번역 모델에 상기 제2 문장을 입력하여 상기제2 언어의 제3 문장을 획득하는 단계; 를 포함하는 컴퓨터 판독 가능 기록매체."}
{"patent_id": "10-2019-0119791", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "획득된 문장에 대한 기계 번역을 수행할 수 있는 전자 장치 및 전자 장치의 제어 방법이 개시된다. 구체적으로, 본 개시에 따른 전자 장치는 적어도 하나의 인스트럭션(instruction)을 저장하는 메모리 및 적어도 하나의 인스 트럭션을 실행하는 프로세서를 포함한다. 그리고, 프로세서는 제1 언어의 제1 문장이 획득되면, 학습 코퍼스를 바탕으로 학습된 제2 언어 모델을 이용하여 제1 문장을 제1 언어의 다른 문장으로 수정할 지 여부를 결정하고, 제1 문장을 제1 언어의 다른 문장으로 수정하는 것으로 결정되면, 입력 문장과 임계 값 이상의 유사도를 갖는 다 른 문장을 획득하도록 학습된 변환 모델에 제1 문장을 입력하여 제1 문장이 수정된 제1 언어의 제2 문장을 획득 하며, 제2 문장이 획득되면, 상기 학습 코퍼스를 바탕으로 학습된 번역 모델에 제2 문장을 입력하여 제2 언어의 제3 문장을 획득한다."}
{"patent_id": "10-2019-0119791", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 전자 장치의 제어 방법에 관한 것으로서, 구체적으로는 획득된 문장에 대한 기계 번역 을 수행할 수 있는 전자 장치 및 전자 장치의 제어 방법에 관한 것이다."}
{"patent_id": "10-2019-0119791", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "근래에는 기존의 통계 기반의 번역(Statistical Machine Translation, SMT)의 한계를 뛰어 넘는 기술로서, 인 공 신경망을 기반으로 한 기계 번역(Neural Machine Translation, NMT)에 대한 기술이 주목 받고 있다. 그런데, 기계 번역을 위한 번역 모델의 경우, 그 성능은 번역 모델의 학습에 이용된 학습 코퍼스에 의존한다. 구체적으로, 번역 모델에 따른 출력 문장의 번역 품질은 번역 모델에 입력된 입력 문장이 그 번역 모델의 학습 에 이용된 학습 코퍼스에 익숙한 지에 따라 결정될 수 있다. 여기서, 입력 문장이 학습 코퍼스에 익숙하다는 것은 번역 모델에 입력된 입력 문장이 그 번역 모델의 학습에 사용된 학습 코퍼스에 포함된 문장들의 표현과 동일한 유형의 표현을 얼마나 포함하는지에 따라 결정될 수 있다. 예를 들어, 입력 문장이 학습 코퍼스에 포함된 문장들에서 사용된 문법적 사항, 띄어쓰기 및 경어체 등과 동일한 유형을 다수 포함할수록 입력 문장이 학습 코퍼스에 익숙하다고 할 수 있다. 입력 문장이 학습 코퍼스에 익숙한지 여부를 나타내는 척도에 대한 구체적인 설명은 후술한다. 예를 들어, 사용자는 \"700번 버스를 타면 그 곳에 도착할 수 있다\"는 의미의 영어 문장을 획득하기 위해, 한국 어를 영어로 번역하기 위한 동일한 번역 모델에 각각 \"700번 타면 거기 가요\"와 같이 경어체가 사용된 한국어 문장과 \"700번 타면 거기 가\"와 같이 경어체가 사용되지 않은 한국어 문장을 입력할 수 있다. 이 때, \"700번 타면 거기 가요\"라는 입력 문장이 위 번역 모델의 학습에 이용된 학습 코퍼스에 익숙하면, 그 번 역 모델은 \"If you take number 700, you can go there\"와 같이 한국어의 의미가 제대로 반영된 높은 품질의 영어 문장을 출력할 수 있다. 반면, \"700번 타면 거기 가\"라는 입력 문장이 위 번역 모델의 학습에 이용된 학습 코퍼스에 익숙하지 않다면, 그 번역 모델은 \"Take number 700 and go there\"와 같이 한국어의 의미가 제대로 반 영되지 않은 낮은 품질의 영어 문장을 출력할 수 있다. 상술한 바와 같은 예를 비롯하여, 입력 문장에 포함된 경어체의 사용 여부, 문법적 사항의 차이 및 띄어쓰기의 차이 등에 따라 번역 모델이 출력하는 출력 문장의 번역 품질이 결정될 수 있다는 문제가 있다. 한편, 인간의 언어는 고정된 규칙을 따르지 않는다는 점에서, 위와 같은 문제점을 해결하기 위해 인간의 언어가 갖는 모든 표 현에 대해 학습 코퍼스를 구축하는 것도 현실적으로 불가능하다고 할 수 있다."}
{"patent_id": "10-2019-0119791", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 바와 같은 문제점을 극복하기 위해 안출된 것으로서, 본 개시의 목적은 기계 번역을 위한 번 역 모델에 입력 문장을 입력하기 전에 입력 문장을 수정한 후, 수정된 입력 문장을 번역 모델에 입력함으로써출력 문장의 번역 품질을 향상시키는 것에 있다."}
{"patent_id": "10-2019-0119791", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 바와 같은 목적을 달성하기 위한 본 개시의 일 실시 예에 따르면, 전자 장치는 적어도 하나의 인스트럭 션(instruction)을 저장하는 메모리 및 적어도 하나의 인스트럭션을 실행하는 프로세서를 포함한다. 그리고, 프 로세서는, 적어도 하나의 인스트럭션을 실행함으로써, 제1 언어의 제1 문장이 획득되면, 학습 코퍼스를 바탕으 로 학습된 적어도 하나의 언어 모델을 이용하여 제1 문장을 제1 언어의 다른 문장으로 수정할 지 여부를 결정하 고, 제1 문장을 제1 언어의 다른 문장으로 수정하는 것으로 결정되면, 입력 문장과 임계 값 이상의 유사도를 갖 는 다른 문장을 획득하도록 학습된 변환 모델에 제1 문장을 입력하여 제1 문장이 수정된 제1 언어의 제2 문장을 획득하며, 제2 문장이 획득되면, 상기 학습 코퍼스를 바탕으로 학습된 번역 모델에 제2 문장을 입력하여 제2 언 어의 제3 문장을 획득한다. 한편, 상술한 바와 같은 목적을 달성하기 위한 본 개시의 일 실시 예에 따르면, 전자 장치의 제어 방법은 제1 언어의 제1 문장이 획득되면, 병렬 코퍼스(parallel corpus)를 바탕으로 학습된 제1 언어 모델 및 단일어 코퍼 스(mono corpus)를 바탕으로 학습된 제2 언어 모델을 이용하여 제1 문장을 제1 언어의 다른 문장으로 수정할 지 여부를 결정하는 단계, 제1 문장을 제1 언어의 다른 문장으로 수정하는 것으로 결정되면, 입력 문장과 임계 값 이상의 유사도를 갖는 다른 문장을 획득하도록 학습된 변환 모델에 제1 문장을 입력하여 제1 문장이 수정된 제1 언어의 제2 문장을 획득하는 단계 및 제2 문장이 획득되면, 상기 학습 코퍼스를 바탕으로 학습된 번역 모델에 제2 문장을 입력하여 제2 언어의 제3 문장을 획득하는 단계를 포함한다. 한편, 상술한 바와 같은 목적을 달성하기 위한 본 개시의 일 실시 예에 따르면, 프로그램을 포함하는 컴퓨터 판 독 가능 기록매체는 전자 장치의 제어 방법을 실행하며, 전자 장치의 제어 방법은 제1 언어의 제1 문장이 획득 되면, 병렬 코퍼스(parallel corpus)를 바탕으로 학습된 제1 언어 모델 및 단일어 코퍼스(mono corpus)를 바탕 으로 학습된 제2 언어 모델을 이용하여 제1 문장을 제1 언어의 다른 문장으로 수정할 지 여부를 결정하는 단계, 제1 문장을 제1 언어의 다른 문장으로 수정하는 것으로 결정되면, 입력 문장과 임계 값 이상의 유사도를 갖는 다른 문장을 획득하도록 학습된 제1 인공 지능 모델에 제1 문장을 입력하여 제1 문장이 수정된 제1 언어의 제2 문장을 획득하는 단계 및 제2 문장이 획득되면, 상기 학습 코퍼스를 바탕으로 학습된 번역 모델에 제2 문장을 입력하여 제2 언어의 제3 문장을 획득하는 단계를 포함한다."}
{"patent_id": "10-2019-0119791", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참고하여 본 개시에 따른 실시 예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 도 1은 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 간략하게 설명하기 위한 흐름도이다. 본 개시의 일 실시 예에 따른 전자 장치는 제1 언어의 입력 문장을 획득하고, 입력 문장이 제2 언어로 번역된 출력 문장을 획득할 수 있다. 구체적으로, 전자 장치는 제1 언어를 제2 언어로 번역하기 위한 인공 지능 모델에 제1 언어의 입력 문장을 입력하여, 제2 언어의 출력 문장을 획득할 수 있다. 그런데, 제1 언어를 제2 언어로 번역하기 위한 번역 모델에 따른 출력 문장의 번역 품질은 번역 모델에 입력된 입력 문장이 그 번역 모델의 학습에 이용된 학습 코퍼스에 익숙한 지에 따라 결정될 수 있다. 여기서, 입력 문장이 학습 코퍼스에 익숙하다는 것은 번역 모델에 입력된 입력 문장이 그 번역 모델의 학습에 사용된 학습 코퍼스에 포함된 문장들의 표현과 동일한 유형의 표현을 얼마나 포함하는지에 따라 결정될 수 있다. 예를 들어, 입력 문장이 학습 코퍼스에 포함된 문장들에서 사용된 문법적 사항, 띄어쓰기 및 경어체 등과 동일한 유형을 다수 포함할수록 입력 문장이 학습 코퍼스에 익숙하다고 할 수 있다. 입력 문장이 학습 코퍼스에 익숙한지 여부를 나타내는 척도에 대한 구체적인 설명은 후술한다. 구체적으로, 입력 문장이 변역 모델의 학습에 이용된 학습 코퍼스에 익숙한 문장인 경우에 비해, 입력 문장이 변역 모델의 학습에 이용된 학습 코퍼스에 익숙하지 않은 문장인 경우에는 상대적으로 출력 문장의 번역 품질이 낮을 수 있다. 예를 들어, 사용자는 한국어를 영어로 번역하기 위한 동일한 번역 엔진에 각각 \"봄을 맞이하여 대청소 합시다\" 와 같은 한국어 문장과, \"봄을 맞이하여\" 대신에 조사와 띄어쓰기를 생략하고 \"봄맞이\"라는 복합 명사 형태의 단어를 포함하는 문장인 \"봄맞이 대청소 합시다\"라는 한국어 문장을 입력할 수 있다. 이 때, \"봄을 맞이하여 대청소 합시다\"라는 입력 문장이 위 번역 엔진의 학습에 이용된 학습 코퍼스에 익숙하면, 번역 엔진은 \"Let's clean up in the spring\"과 같이 한국어의 의미가 제대로 반영된 높은 품질의 영 어 문장을 출력할 수 있다. 반면, \"봄맞이 대청소 합시다\"라는 입력 문장이 위 번역 엔진의 학습에 이용된 학습 코퍼스에 익숙하지 않다면, 번역 엔진은 \"Let's clean the spring\"와 같이 한국어의 의미가 제대로 반영되지 않 은 낮은 품질의 영어 문장을 출력할 수 있다. 따라서, 본 개시에 따른 전자 장치는 입력 문장이 번역된 출력 문장을 획득하기에 앞서, 입력 문장을 번역 모델 의 학습에 이용된 학습 코퍼스에 익숙한 문장으로 수정하고 수정된 입력 문장을 번역 모델에 입력함으로써, 입 력 문장을 수정하지 않고 번역 모델에 입력하는 경우에 비하여 상대적으로 번역 품질이 높은 출력 문장을 획득 할 수 있다. 한편, 입력 문장을 수정하지 않고 번역 모델에 입력하더라도 높은 번역 품질의 출력 문장을 획득할 수 있는 경 우가 있을 수 있으며, 반대로 입력 문장을 수정하여 번역 모델이 입력하더라도 높은 품질의 출력 문장을 획득하 기 어려운 경우도 있을 수 있다. 따라서, 본 개시에 따른 전자 장치는 입력 문장을 수정하기에 앞서, 적어도 하나의 언어 모델을 이용하여 입력 문장의 수정 여부를 결정하고, 입력 문장을 수정하는 것으로 결정된 경우에 한하여 입력 문장을 수정할 수 있다. 이하에서는 도 1을 참조하여, 입력 문장의 수정 여부를 결정하는 과정, 입력 문장을 수정하는 것으로 결정된 경 우 입력 문장을 번역 모델의 학습에 이용된 학습 코퍼스에 익숙한 문장으로 수정하는 과정, 그리고 수정된 입력 문장을 번역함으로써 출력 문장을 획득하는 과정에 대해 설명한다. 한편, 설명의 편의를 위해, 이하에서는 제1 언어의 입력 문장을 제1 문장, 제1 문장이 수정된 제1 언어의 문장을 제2 문장, 그리고 제2 언어의 출력 문장을 제3 문장으로 지칭한다. 도 1에 도시된 바와 같이, 전자 장치는 제1 문장을 획득할 수 있으며(S110), 제1 문장이 획득되면, 전자 장치는 적어도 하나의 언어 모델을 이용하여 제1 문장을 다른 문장으로 수정할 지 여부를 결정할 수 있다(S120). 구체 적으로, 전자 장치는 병렬 코퍼스를 바탕으로 학습된 제1 언어 모델 및 단일어 코퍼스를 바탕으로 학습된 제2 언어 모델 중 적어도 하나를 이용하여, 획득된 제1 문장을 제1 문장과 동일한 언어의 다른 문장으로 수정할지 여부를 결정할 수 있다. 여기서, 제1 언어 모델은 병렬 코퍼스(parallel corpus)를 바탕으로 학습된 언어 모델이고, 제2 언어 모델은 단 일어 코퍼스(mono corpus)를 바탕으로 학습된 언어 모델을 말한다. 그리고, 학습 코퍼스는 자연어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합으로서, 그 중 단일어 코퍼스는 한 가지 언어의 문장들로 구성 된 코퍼스를 말하며, 병렬 코퍼스는 서로 다른 언어의 문장들이 서로 대응되도록 병렬적으로 구성된 코퍼스를 말한다. 특히, 병렬 코퍼스 또는 단일어 코퍼스는 본 개시에 따른 제1 언어 모델, 제2 언어 모델, 변환 모델 및 번역 모 델 중 적어도 하나의 학습에 이용될 수 있다. 구체적으로, 병렬 코퍼스는 제1 언어 모델, 변환 모델 및 번역 모 델의 학습에 이용될 수 있으며, 단일어 코퍼스는 제2 언어 모델, 변환 모델 및 번역 모델의 학습에 이용될 수 있다. 한편, 전자 장치가 제1 언어 모델 및 제2 언어 모델을 이용한다는 것은, 전자 장치가 제1 언어 모델 및 제2 언 어 모델 각각에 획득된 제1 문장을 입력하여, 각각 제1 문장이 제1 언어 모델의 학습에 이용된 병렬 코퍼스에 익숙한 지를 나타내는 척도에 관련된 값과 제1 문장이 제2 언어 모델의 학습에 이용된 단일어 코퍼스에 익숙한 지를 나타내는 척도에 관련된 값을 획득한다는 것을 의미한다. 여기서, 제1 문장이 병렬 코퍼스 및 단일어 코퍼 스에 익숙한 지를 나타내는 척도에 관련된 값의 예로는 퍼플렉서티(perplexity) 값을 들 수 있다. 즉, 전자 장 치는 제1 언어 모델 및 제2 언어 모델 각각에 획득된 제1 문장을 입력하여 제1 언어 모델에 대한 제1 퍼플렉서 티(perplexity) 값 및 제2 언어 모델 에 대한 제2 퍼플렉서티 값을 획득할 수 있다. 구체적으로, 퍼플렉서티 값은 언어 모델이 입력 문장에 대해 다음 단어를 예측하는 모든 시점(time-step)마다 평균적으로 몇 개의 단어 중에서 출력 단어를 선택하는지를 나타내는 척도를 말한다. 예를 들어, 퍼플렉서티가 10 이면, 언어 모델이 입력 문장에 대해 다음 단어를 예측하는 모든 시점마다 평균적으로 10 개의 단어 중에서 출력 단어를 선택하는 것이라고 할 수 있다. 반면, 퍼플렉서티가 3 이면, 언어 모델이 입력 문장에 대해 다음 단어를 예측하는 모든 시점마다 평균적으로 3 개의 단어 중에서 출력 단어를 선택하는 것이라고 할 수 있다. 즉, 퍼플렉서티 값이 높다는 것은 입력 문장이 언어 모델을 구성하는 코퍼스에 익숙하지 않다는 의미로 해석될 수 있다. 반면, 퍼플렉서티 값이 낮다는 것은 입력 문장에 대한 언어 모델의 성능이 높으며, 입력 문장이 언어 모델을 구성하는 코퍼스에 익숙하다는 의미로 해석될 수 있다. 제1 언어 모델에 대한 제1 퍼플렉서티 값 및 제2 언어 모델에 대한 제2 퍼플렉서티 값이 획득되면, 전자 장치는 획득된 제1 퍼플렉서티 값 및 제2 퍼플렉서티 값 중 적어도 하나를 바탕으로 제1 문장을 제1 언어의 다른 문장 으로 수정할 지 여부를 결정할 수 있다. 구체적으로, 전자 장치는 획득된 제1 퍼플렉서티 값을 기 설정된 제1 임계 값과 비교하거나, 제2 퍼플렉서티 값을 기 설정된 제2 임계 값과 비교하거나, 또는 제1 퍼플렉서티 값 및 제2 퍼플렉서티 값을 각각 기 설정된 제1 임계 값 및 제2 임계 값과 비교하여 제1 문장을 제1 언어의 다른 문장 으로 수정할지 여부를 결정할 수 있다. 특히, 제1 퍼플렉서티 값이 기 설정된 제1 임계 값보다 작으면, 전자 장치는 획득된 제1 문장을 다른 문장 으로 수정하지 않는 것으로 결정할 수 있다. 즉, 제1 퍼플렉서티 값이 기 설정된 제1 임계 값보다 작으면, 제1 문장이 병렬 코퍼스에 익숙하다는 의미로 해석될 수 있다. 따라서, 이 경우는 제1 문장을 다른 문장으로 수정하 지 않더라도 높은 번역 품질의 출력 문장을 획득할 수 있는 경우라고 할 수 있으므로, 전자 장치는 획득된 제1 문장을 다른 문장으로 수정하지 않는 것으로 결정할 수 있다. 한편, 제2 퍼플렉서티 값이 기 설정된 제2 임계 값보다 크면, 이 경우에도 전자 장치는 획득된 제1 문장을 다른 문장으로 수정하지 않는 것으로 결정할 수 있다. 즉, 제2 퍼플렉서티 값이 기 설정된 제2 임계 값보다 크면, 제 1 문장이 단일어 코퍼스에 익숙하지 않다는 의미로 해석될 수 있다. 그런데, 단일어 코퍼스는 병렬 코퍼스에 비 하여 많은 수의 문장을 포함할 수 있으며, 이 경우 입력 문장이 단일어 코퍼스에 익숙하지 않으면 병렬 코퍼스 에는 더욱 익숙하지 않을 수 있다. 따라서, 제2 퍼플렉서티 값이 기 설정된 제2 임계 값보다 크다는 것은 곧 제1 문장이 단일어 코퍼스에 익숙하지 않을 뿐만 아니라 병렬 코퍼스에는 더욱 익숙하지 않다는 의미로 해석될 수 있다. 따라서, 이 경우는 제1 문장 을 다른 문장으로 수정하더라도 높은 번역 품질의 출력 문장을 획득하기 어려운 경우라고 할 수 있으므로, 전자 장치는 획득된 제1 문장을 다른 문장으로 수정하지 않는 것으로 결정할 수 있다. 한편, 제1 퍼플렉서티 값이 기 설정된 제1 임계 값보다 크고 제2 퍼플렉서티 값이 기 설정된 제2 임계 값보다 작으면, 전자 장치는 제1 문장을 상기 다른 문장으로 수정하는 것으로 결정할 수 있다. 즉, 제1 퍼플렉서티 값 이 기 설정된 제1 임계 값보다 크고 제2 퍼플렉서티 값이 기 설정된 제2 임계 값보다 작으면, 제1 문장이 병렬 코퍼스에는 익숙하지 않지만 단일어 코퍼스에는 익숙하다는 의미로 해석될 수 있다. 다시 말해, 제1 퍼플렉서티값이 기 설정된 제1 임계 값보다 크고 제2 퍼플렉서티 값이 기 설정된 제2 임계 값보다 작으면, 제1 문장을 병 렬 코퍼스에 익숙한 문장으로 수정하여 번역함으로써 출력 문장의 번역 품질을 향상시킬 수 있는 경우라고 할 수 있으므로, 전자 장치는 제1 문장을 다른 문장으로 수정하는 것으로 결정할 수 있다. 이상에서 상술한 바와 같이, 제1 언어 모델을 통해 획득된 제1 퍼플렉서티 값은 제1 문장이 병렬 코퍼스에 익숙 한 지를 나타내는 척도에 관련된 값이 될 수 있으며, 제2 언어 모델을 통해 획득된 제2 퍼플렉서티 값은 제1 문 장이 단일어 코퍼스에 익숙한 지를 나타내는 척도에 관련된 값이 될 수 있다. 그리고, 제1 퍼플렉서티 값에 대 한 제1 임계 값은 제1 문장이 병렬 코퍼스에 얼만큼 익숙한 지를 구분하는 경계 값의 의미로 해석될 수 있으며, 제2 퍼플렉서티 값에 대한 제2 임계 값은 제1 문장이 단일어 코퍼스에 얼만큼 익숙한 지를 구분하는 경계 값의 의미로 해석될 수 있다. 한편, 제1 퍼플렉서티 값에 대한 제1 임계 값과 제2 퍼플렉서티 값에 대한 제2 임계 값은 각각 제1 언어 모델의 학습에 이용된 병렬 코퍼스 및 제2 언어 모델의 학습에 이용된 단일어 코퍼스에 따라 변경될 수 있으며, 사용자 의 설정에 의해 변경될 수도 있음은 물론이다. 그리고, 제1 퍼플렉서티 값에 대한 제1 임계 값 및 제2 퍼플렉서티 값에 대한 제2 임계 값을 변경함에 따라 제1 문장을 다른 문장으로 수정하는 범위가 결정될 수 있다. 예를 들어, 제1 퍼플렉서티에 대한 제1 임계 값을 작게 변경할수록 제1 문장이 병렬 코퍼스에 더 익숙한 경우에 한하여 제1 문장을 다른 문장으로 수정하지 않는 것으 로 결정한다는 의미가 될 수 있으며, 제1 퍼플렉서티에 대한 제1 임계 값을 크게 변경할수록 제1 문장이 병렬 코퍼스에 덜 익숙한 경우에도 제1 문장을 다른 문장으로 수정하는 것으로 결정한다는 의미가 될 수 있다. 제1 언어 모델 및 제2 언어 모델 중 적어도 하나를 이용하여 입력 문장의 수정 여부를 결정하는 과정에 대해서 는 도 3 및 도 4를 참조하여 보다 구체적으로 설명한다. 제1 문장을 다른 문장으로 수정하는 것으로 결정되면, 전자 장치는 변환 모델에 제1 문장을 입력하여 제1 문장 이 수정된 제2 문장을 획득할 수 있다(S130). 구체적으로, 전자 장치는 제1 문장 및 제1 문장과의 유사도에 관 련된 임계 값을 변환 모델에 입력하여, 제1 문장과 임계 값 이상의 유사도를 갖는 제2 문장을 획득할 수 있다. 예를 들어, 입력 문장이 \"봄맞이 대청소 합시다\"인 경우, 전자 장치는 입력 문장을 변환 모델에 입력하여, 제1 문장과 임계 값 이상의 유사도를 갖는 제2 문장으로서, \"봄을 맞이하여 대청소 합시다\"와 같은 문장을 획득할 수 있다. 여기서, 변환 모델은 입력 문장과 임계 값 이상의 유사도를 갖는 다른 문장을 획득하도록 학습된 인공 지능 모 델을 말한다. 변환 모델은 시퀀스-투-시퀀스(sequence-to-sequence) 모델일 수 있으며, 입력 데이터의 처리를 위한 인코더(encoder) 및 출력 데이터의 처리를 위한 디코더(decoder)를 포함할 수 있다. 변환 모델을 통해 제2 문장을 획득하는 구체적인 과정에 대해서는 도 5를 참조하여 후술한다. 그리고, 유사도에 관련된 임계 값은 0 에서 1 사이의 범위를 가질 수 있으며, 0 에 가까울수록 제1 문장과 제2 문장이 덜 유사하는 의미로 해석될 수 있으며, 1 에 가까울수록 제1 문장과 제2 문장이 유사하다는 의미로 해석 될 수 있다. 한편, 유사도에 관련된 임계 값은 학습 코퍼스에 따라 변경될 수 있으며, 번역 모델을 통해 획득된 제3 문장의 번역 품질을 바탕으로 재 설정될 수도 있다. 예를 들어, 유사도에 관련된 임계 값이 0.97와 같이 설정된 경우, 제1 문장과 제2 문장이 동일하거나 매우 유사 하여 제2 문장을 번역 모델에 입력하더라도 제1 문장을 동일한 번역 모델에 입력한 경우와 동일하거나 매우 유 사한 제3 문장을 획득할 수 있다. 즉, 제1 문장을 수정하지 않고 번역 모델에 입력한 경우의 출력 문장과 제1 문장을 제2 문장으로 수정하여 동일한 번역 모델에 입력한 경우의 출력 문장 사이의 번역 품질이 크게 달라지지 않을 수 있다. 반면, 유사도에 관련된 임계 값이 0.70과 같이 설정된 경우, 제1 문장을 수정함에 따라 제1 문장과는 상이한 의 미를 갖는 제2 문장이 획득되면, 제1 문장을 수정하지 않고 번역 모델에 입력한 경우의 출력 문장에 비하여 제1 문장을 제2 문장으로 수정하여 동일한 번역 모델에 입력한 경우의 출력 문장의 번역 품질이 오히려 더 떨어질 수도 있다. 한편, 상술한 바와 같은 변환 모델은 단일어 코퍼스에 포함된 복수의 문장 및 병렬 코퍼스에 포함된 복수의 문 장 사이의 유사도(sentence similarity)를 바탕으로 학습될 수 있다. 구체적으로, 변환 모델은 단일어 코퍼스에 포함된 제1 언어의 복수의 문장 및 제1 언어를 제2 언어로 번역하기 위한 병렬 코퍼스에 포함된 제1 언어의 복수의 문장 사이의 유사도를 바탕으로 학습될 수 있다. 예를 들어, 변환 모델은 단일어 코퍼스에 포함된 제1 언어의 복수의 문장 중 병렬 코퍼스에는 포함되지 않는 문장을 제1 언어 를 제2 언어로 번역하기 위한 병렬 코퍼스에 포함된 제1 언어의 복수의 문장 중 기 설정된 임계 값 이상의 유사 도를 갖는 문장으로 매핑시키는 과정을 통해 학습될 수 있다. 한편, 단일어 코퍼스가 제1 언어의 복수의 문장을 포함하는 경우, 변환 모델의 학습은 제1 언어를 제2 언어로 번역하기 위한 병렬 코퍼스뿐만 아니라, 제2 언어를 제1 언어로 번역하기 위한 병렬 코퍼스를 바탕으로 이루어 질 수도 있다. 뿐만 아니라, 변환 모델은 제1 언어를 제2 언어로 번역하기 위한 병렬 코퍼스에 포함된 제2 언어의 복수의 문장 사이의 유사도를 바탕으로 학습될 수도 있다. 변환 모델의 구체적인 학습 방법에 대해서는 도 6을 참조하여 후 술한다. 상술한 바와 같이 학습된 변환 모델을 통해 제1 언어의 제2 문장이 획득되면, 전자 장치는 번역 모델에 제2 문 장을 입력하여 제2 언어의 제3 문장을 획득할 수 있다(S140). 여기서, 번역 모델은 제1 언어를 제2 언어로 번역 하도록 학습된 인공 지능 모델로서, 소위 인공 신경망 기반 기계 번역(Neural Machine Translation, NMT)을 위 한 인공 지능 모델을 말한다. 구체적으로, 전자 장치는 제1 언어를 제2 언어로 번역하도록 학습된 번역 모델에 제2 문장을 입력하여, 제1 언어의 제2 문장이 제2 언어로 번역된 제3 문장을 획득할 수 있다. 한편, 상술한 바와 같은 과정을 통해 제1 퍼플렉서티 값 및 제2 퍼플렉서티 값을 바탕으로 제1 문장을 다른 문 장으로 수정하지 않는 것으로 결정되면, 전자 장치는 제1 문장을 수정하지 않고 번역 모델에 입력하여, 제1 언 어의 제1 문장이 제2 언어로 번역된 제3 문장을 획득할 수 있다. 이상에서 상술한 바와 같은 본 개시의 일 실시 예에 따르면, 기계 번역을 위한 인공 지능 모델에 입력 문장을 입력하기 전에 인공 지능 모델의 학습에 이용된 학습 코퍼스에 익숙한 문장으로 입력 문장을 수정한 후, 수정된 입력 문장을 인공 지능 모델에 입력함으로써 번역 품질이 높은 출력 문장을 향상시킬 수 있다. 도 2는 본 개시에 따른 전자 장치의 하드웨어 구조와 소프트웨어 구조 사이의 결합 관계를 전제로 본 개시 에 따른 일 실시 예를 설명하기 위한 도면이다. 도 2에 도시된 바와 같이, 본 개시의 일 실시 예에 따른 전자 장치는 메모리 및 프로세서를 포 함한다. 메모리에는 전자 장치에 관한 적어도 하나의 인스트럭션(instruction)이 저장될 수 있다. 구체적으로, 메모리에는 본 개시의 다양한 실시 예에 따라 전자 장치가 동작하기 위한 각종 소프트웨 어 모듈이 저장될 수 있다. 특히, 본 개시의 다양한 실시 예에 따른 인공 지능 모델 중 적어도 하나의 인공 지능 모델은 데이터의 형태로 메모리에 저장될 수 있다. 구체적으로, 도 2에 도시된 바와 같이, 메모리에는 언어 모델, 변환 모델 및 번역 모델이 저장될 수 있다. 도 2에서는 단순히 언어 모델로 기재하였으나, 전술한 바와 같이 본 개시에 따른 언어 모델은 제1 언어 모델 및 제2 언어 모델을 포함할 수 있다. 즉, 메모리에는 병렬 코퍼스를 바탕으로 학습된 제1 언어 모델 및 단일어 코퍼스를 바탕으로 학습된 제2 언어 모델이 저장될 수 있다. 여기서, 언어 모델, 변환 모델 및 번역 모델이 메모리에 저장된다는 것은, 언어 모델, 변 환 모델 및 번역 모델을 실행하고 그 기능을 수행하기 위해 필요한 모든 데이터가 메모리에 저장 된다는 것을 의미한다. 한편, 메모리에는 언어 모델, 변환 모델 및 번역 모델의 학습에 필요 한 데이터를 비롯하여, 언어 모델, 변환 모델 및 번역 모델에 관련된 다양한 데이터가 저장될 수도 있다. 프로세서는 전자 장치의 전반적인 동작을 제어한다. 구체적으로, 프로세서는 메모리에 저 장된 적어도 하나의 인스트럭션을 실행함으로써 전자 장치의 동작을 전반적으로 제어할 수 있다. 도 2를 참조하면, 프로세서는 본 개시에 따른 각각의 동작을 수행하도록 구현된 복수의 모듈(11, 12, 13) 을 포함할 수 있다. 구체적으로, 본 개시에 따른 프로세서는 입력 문장의 수정 여부를 결정하는 판단 모듈 , 입력 문장을 수정하는 변환 모듈, 입력 문장을 번역하는 번역 모듈을 포함할 수 있다. 특히, 복수의 모듈(11, 12, 13) 각각은 그에 대응되는 복수의 인공 지능 모델(21, 22, 23)을 이용하여 복수의 모듈(11, 12, 13) 각각의 동작을 수행할 수 있다. 구체적으로, 도 2에 도시된 바와 같이, 프로세서는 메모 리에 저장된 복수의 인공 지능 모델(21, 22, 23)을 로딩할 수 있으며, 프로세서에 포함된 복수의 모 듈(11, 12, 13)은 복수의 인공 지능 모델(21, 22, 23)을 이용하여 본 개시에 따른 다양한 동작을 수행할 수 있 다. 여기서, 로딩이란 프로세서가 엑세스할 수 있도록 인공 지능 모델을 불러들이는 동작을 의미하며, 구체적 인 의미는 메모리와 프로세서의 구현 예에 따라 결정될 수 있다. 예를 들어, 프로세서는 복수의 인공 지능 모델(21, 22, 23)을 프로세서 내의 SRAM(Static Random Access Memory)에 로딩하여 엑세스할 수 있으며, 복수의 인공 지능 모델(21, 22, 23)을 메모리에 포함된 비휘발성 메모리인 플래시 메모리 (Flash Memory)에서 메모리에 포함된 휘발성 메모리인 DRAM(Dynamic Random Access Memory)에 로딩하여 엑세스할 수도 있다. 특히, 제1 언어의 제1 문장이 획득되면, 프로세서에 포함된 판단 모듈은 제1 문장을 제1 언어의 다른 문장으로 수정할 지 여부를 결정할 수 있다. 구체적으로, 판단 모듈에 제1 언어의 제1 문장이 입력되면, 판 단 모듈은 적어도 하나의 언어 모델을 이용하여 제1 문장을 제1 언어의 다른 문장으로 수정할 지 여부 를 결정할 수 있다. 제1 문장을 제1 언어의 다른 문장으로 수정하는 것으로 결정되면, 프로세서에 포함된 변환 모듈은 제1 문장이 수정된 제1 언어의 제2 문장을 획득할 수 있다. 구체적으로, 변환 모듈에 제1 언어의 제1 문장 및 유사도에 관련된 임계 값이 입력되면, 변환 모듈은 입력 문장과 임계 값 이상의 유사도를 갖는 다른 문장을 획득하도록 학습된 변환 모델을 이용하여 제1 언어의 제2 문장을 획득할 수 있다. 제1 언어의 제2 문장이 획득되면, 프로세서에 포함된 번역 모듈은 제2 언어의 제3 문장을 획득할 수 있다. 구체적으로, 번역 모듈에 제1 언어의 제2 문장이 입력되면, 번역 모듈은 제1 언어를 제2 언어로 번역하도록 학습된 번역 모델을 이용하여 제2 언어의 제3 문장을 획득할 수 있다. 이상에서는 프로세서가 메모리에 포함된 복수의 인공 지능 모델(21, 22, 23)을 로딩하고, 프로세서 에 포함된 복수의 모듈(11, 12, 13)이 복수의 인공 지능 모델(21, 22, 23)을 이용하여 본 개시에 따른 동 작을 수행하는 과정에 대해 설명하였으나, 이는 본 개시에 따른 동작을 구현하기 위한 하드웨어와 소프트웨어의 유기적인 결합에 대해 상세하게 설명하기 위한 것일 뿐, 본 개시가 도 2에 도시된 바와 같은 소프트웨어 구조 (software architecture)을 포함하는 경우에 국한되는 것은 아니다. 즉, 본 개시의 목적을 달성하기 위한 범위 내에서, 복수의 모델 및 복수의 인공 지능 모델의 종류와 그 명칭은 도 2에 도시된 바와 상이하게 결정될 수도 있다. 한편, 본 개시의 다양한 실시 예에 따른 복수의 인공 지능 모델(21, 22, 23) 중 적어도 하나의 인공 지능 모델 은 해당 인공 지능 모델의 알고리즘을 수행할 수 있는 전용 하드웨어 칩 형태로 구현되어 프로세서에 포함 되도록 구현될 수도 있다. 이하에서는 도 3 내지 도 7을 참조하여, 본 개시의 다양한 실시 예에 따른 프로세서의 제어 과정 및 프로 세서의 제어에 따른 전자 장치의 동작에 대해 상술한다. 도 3은 본 개시의 일 실시 예에 따른 제어 과정을 구체적으로 설명하기 위한 흐름도, 도 4는 입력 문장의 수정 여부를 결정하는 과정을 상세하게 설명하기 위한 모식도, 그리고 도 5는 변환 모델을 통해 입력 문장을 병렬 코 퍼스에 익숙한 문장으로 수정하는 과정을 상세하게 설명하기 위한 도면이다. 도 1에 대한 설명과 마찬가지로, 이하에서는 제1 언어의 입력 문장을 제1 문장, 제1 문장이 수정된 제1 언어의 문장을 제2 문장, 그리고 제2 언어의 출력 문장을 제3 문장으로 지칭한다. 한편, 본 개시를 설명함에 있어서 제 1 문장을 제2 문장으로 수정한다는 것은 제1 문장을 제2 문장으로 변경한다는 것 내지는 제1 문장을 변경하여 제2 문장을 생성한다는 것을 포함하는 의미로 사용될 수 있다. 전술한 바와 같이, 제1 문장이 획득되면, 프로세서는 제1 문장을 다른 문장으로 수정할 지 여부를 결정할 수 있다. 제1 문장의 수정 여부는 제1 문장의 수정 여부는 제1 언어 모델 및 제2 언어 모델 중 적 어도 하나를 이용하여 획득된 제1 퍼플렉서티(perplexity) 값 및 제2 퍼플렉서티 값 중 적어도 하나를 바탕으로 결정될 수 있다. 특히, 도 3에 도시된 바와 같이, 제1 문장의 수정 여부는 제1 언어 모델 및 제2 언어 모델을 이용 하여 획득된 제1 퍼플렉서티 값 및 제2 퍼플렉서티 값을 바탕으로 결정될 수 있다. 도 3에서는 제1 문장을 제1 언어 모델에 입력하여 획득한 제1 퍼플렉서티 값을 P_P로 나타내었으며, 제1 문장을 제2 언어 모델에 입력하여 획득한 제2 퍼플렉서티 값을 P_M으로 나타내었다. 또한, 도 3에서는 제 1 퍼플렉서티 값에 대한 제1 임계 값을 τ_P로 나타내었으며, 제2 퍼플렉서티 값에 대한 제2 임계 값을 τ_M로 나타내었다. 도 3에 도시된 바와 같이, 제1 문장이 획득되면, 프로세서는 제1 문장을 제1 언어 모델에 입력하여 제1 퍼플렉서티 값을 획득할 수 있으며, 제1 문장을 제2 언어 모델에 입력하여 제2 퍼플렉서티 값을 획득 할 수 있다. 제1 퍼플렉서티 값 및 제2 퍼플렉서티 값이 획득되면, 프로세서는 제1 퍼플렉서티 값을 기 설 정된 제1 임계 값과 비교하고, 제2 퍼플렉서티 값을 기 설정된 제2 임계 값과 비교함으로써 제1 문장의 수정 여 부를 결정할 수 있다(S310). 여기서, 제1 언어 모델 제2 언어 모델 및 퍼플렉서티 값의 의미에 대해서는 도 1을 참조하여 전술 한 바 있다. 제1 문장의 수정 여부를 결정하는 과정(S310)에 대해서는 도 3과 함께 도 4를 참조하여 상세하게 설명한다. 구 체적으로, 도 4는 제1 퍼플렉서티 값과 제1 임계 값의 관계, 그리고 제2 퍼플렉서티 값과 제2 임계 값의 관계를 바탕으로, 제1 문장의 수정 여부를 결정하기 위한 기준을 관념적으로 나타낸 도면이다. 도 4의 제1 영역은 제1 퍼플렉서티 값이 기 설정된 제1 임계 값보다 작은 경우를 나타낸다(P_P<τ_P). 즉, 제1 영역은 제1 문장이 병렬 코퍼스에 익숙한 경우를 나타내기 위한 것이다. 따라서, 제1 문장이 제1 영역 내에 있는 경우라면, 제1 문장을 다른 문장으로 수정하지 않더라도 높은 번역 품질의 출력 문장을 획득할 수 있는 경우라고 할 수 있으므로, 프로세서는 획득된 제1 문장을 다른 문장으로 수정하지 않는 것으로 결 정할 수 있다. 도 4의 제2 영역은 제1 퍼플렉서티 값이 기 설정된 제1 임계 값보다 크고 제2 퍼플렉서티 값이 기 설정된 제2 임계 값보다 작은 경우를 나타낸다(P_P>τ_P & P_M <τ_M). 즉, 제2 영역은 제1 문장이 병렬 코퍼스에 익숙한 것은 아니지만 단일어 코퍼스에는 익숙한 경우를 나타낸다. 따라서, 제1 문장이 제2 영역 내에 있 는 경우라면, 이는 제1 문장을 병렬 코퍼스에 익숙한 문장으로 수정하여 번역함으로써 출력 문장의 번역 품질을 향상시킬 수 있는 경우라고 할 수 있으므로, 프로세서는 제1 문장을 다른 문장으로 수정하는 것으로 결정 할 수 있다 도 4의 제3 영역은 제2 퍼플렉서티 값이 기 설정된 제2 임계 값보다 큰 경우를 나타낸다(P_M>τ_M). 즉, 제 3 영역은 제1 문장이 단일어 코퍼스에 익숙하지 않은 경우를 나타낸다. 그런데, 단일어 코퍼스는 병렬 코 퍼스에 비하여 많은 수의 문장을 포함할 수 있으며, 이 경우 제1 문장이 단일어 코퍼스에 익숙하지 않으면 병렬 코퍼스에는 더욱 익숙하지 않을 수 있다. 즉, 제3 영역은 제1 문장이 단일어 코퍼스에 익숙하지 않을 뿐만 아니라, 병렬 코퍼스에는 더욱 익숙하지 않은 경우를 나타낸다고 할 수 있다. 따라서, 제1 문장이 제3 영역 내에 있는 경우라면, 제1 문장을 다른 문장으로 수정하더라도 높은 번역 품질의 출력 문장을 획득하기 어려운 경우라고 할 수 있으므로, 프로세서 는 획득된 제1 문장을 다른 문장으로 수정하지 않는 것으로 결정할 수 있다 한편, 전술한 바와 같이, 제1 퍼플렉서티 값에 대한 제1 임계 값은 입력 문장이 병렬 코퍼스에 얼만큼 익숙한 지를 구분하는 경계 값의 의미로 해석될 수 있으며, 제2 퍼플렉서티 값에 대한 제2 임계 값은 입력 문장이 단일 어 코퍼스에 얼만큼 익숙한 지를 구분하는 경계 값의 의미로 해석될 수 있는바, 도 4의 제1 영역과 제2 영 역을 구분하는 경계는 제1 임계 값에 대응되며, 도 4의 제2 영역과 제3 영역을 구분하는 경계는 제2 임계 값에 대응된다. 다만, 이상에서 설명한 바와 같은 도 4의 제1 영역, 제2 영역 및 제3 영역은 본 개시에 따라 제 1 문장의 수정 여부를 결정하는 기준을 관념적으로 나타내기 위한 것일 뿐이며, 본 개시에 있어서, 제1 언어를 제2 언어로 번역하기 위한 병렬 코퍼스에 포함되는 제1 언어의 복수의 문장이 모두 본 개시에 따른 단일어 코퍼 스에 포함되는 제1 언어의 복수의 문장에 포함되어야 한다는 등의 제한이 있는 것은 아니다. 그리고, 본 개시에 따른 병렬 코퍼스와 단일어 코퍼스 사이에 포함 관계가 있는 것이 아닌 이상, 병렬 코퍼스에 관련된 제1 임계 값과 단일어 코퍼스에 관련된 제2 임계 값 사이에도 특정한 관계가 있는 것은 아니다. 즉, 도4의 제1 영역과 제2 영역을 구분하는 경계보다 도 4의 제2 영역과 제3 영역을 구분하 는 경계가 외곽에 위치하도록 도시하였다는 점을 근거로, 제1 임계 값보다 제2 임계 값이 큰 것으로 해석 해서는 안 된다. 이상에서 상술한 바와 같은 과정을 통해 제1 문장을 다른 문장으로 수정하지 않는 것으로 결정되면(S310-N), 프 로세서는 제1 문장을 수정하지 않고 제1 언어를 제2 언어로 번역하도록 학습된 번역 모델에 입력하 여, 제1 언어의 제1 문장이 제2 언어로 번역된 제3 문장을 획득할 수 있다. 한편, 제1 문장을 다른 문장으로 수정하는 것으로 결정되면(S310-Y), 프로세서는 변환 모델에 제1 문장을 입력하여 제1 문장이 수정된 제2 문장을 획득할 수 있다. 변환 모델을 통해 제1 문장을 제2 문장으로 수정하는 과정(S320)에 대해서는 도 3과 함께 도 5를 참조하 여 보다 상세하게 설명한다. 도 5에 도시된 바와 같이, 본 개시의 일 실시 예에 따른 변환 모델은 시퀀스-투-시퀀스(sequence-to- sequence) 모델일 수 있다. 여기서, 시퀀스(sequence)란 서로 연관된 연속의 데이터를 의미하며, 본 개시에 있 어서의 제1 문장, 제2 문장 및 제3 문장과 같은 문장(sentence)이 바로 시퀀스의 예라고 할 수 있다. 변환 모델은 입력 데이터의 처리를 위한 인코더(encoder) 및 출력 데이터의 처리를 위한 디코더 (decoder)를 포함할 수 있다. 그리고, 인코더 및 디코더는 복수의 순환 신경망(Recurrent Neural Network, RNN)셀(530-1 내지 530-8)을 포함할 수 있다. 보다 구체적으로, 복수의 RNN 셀(530-1 내지 530-8)은 LSTM(Long Short-Term Memory) 또는 GRU(Gated Recurrent Unit)로 구성될 수 있다. 다만, 본 개시에 따른 변환 모델이 상술한 바와 같은 예에 국한되는 것은 아니다. 인코더는 입력 문장을 수신한다. 구체적으로, 수신된 입력 문장은 토큰화(tokenization)를 통해 단어 단위 로 나누어질 수 있다. 나누어진 각각의 단어는 워드 임베딩(word embedding)을 통해 벡터로 변환될 수 있다. 그 리고, 벡터로 변환된 각각의 단어는 인코더에 포함되는 각각의 RNN 셀에 대한 각 시점(time-step)의 입력 이 된다. 예를 들어, 도 5에 도시된 바와 같이, 입력 문장이 \"봄맞이 대청소 합시다\"인 경우, 입력 문장은 단어 토큰화를 통해 \"봄맞이\", \"대청소\" 및 \"합시다\"와 같은 단어로 나누어질 수 있다. 그리고, 나누어진 각각의 단 어는 워드 임베딩을 통해 벡터로 변환되어 각각의 RNN 셀에 입력된다. 그리고, 인코더는 컨텍스트 벡터(context vector) 또는 인코더 상태(encoder state)라고 지칭되는 인코더 RNN 셀의 마지막 시점의 은닉 상태(hidden state)를 디코더에 전달한다. 현재 시점에서 의 은닉 상태는 과거 시점의 RNN 셀에서의 모든 은닉 상태의 값들의 영향을 누적적으로 반영한 결과라고 볼 수 있기 때문에, 컨텍스트 벡터는 입력 문장의 모든 단어들의 정보를 담고 있다고 볼 수 있다. 한편, 본 개시에 따르면, 프로세서는 입력 문장인 제1 문장뿐만 아니라, 제1문장과 함께 제1 문장과의 유 사도에 관련된 임계 값을 변환 모델에 입력한다. 즉, 인코더는 제1 문장뿐만 아니라, 제1 문장 과의 유사도에 관련된 임계 값을 수신하고, 수신된 임계 값을 각각의 디코더 RNN 셀에 전달할 수 있다. 디코더는 입력 문장에 대응되는 출력 문장을 출력한다. 구체적으로, 컨텍스트 벡터가 디코더에 전달 되면, 디코더는 컨텍스트 벡터를 첫번째 시점의 디코더 RNN 셀의 은닉 상태로 사용하여, 첫번째 시점 의 단어를 확률적으로 예측한다. 그리고, 디코더는 첫번째 시점의 단어를 두번째 시점의 디코더 RNN 셀에 입력하여, 두번째 시점의 단어를 확률적으로 예측한다. 나아가, 디코더는 n-1 번째 시점의 단어를 n 번째 시점의 디코더 RNN 셀에 입력하여, n 번째 시점의 단어를 확률적으로 예측하는 방식으로 각각의 출력 단어를 예측할 수 있다. 보다 구체적으로, 디코더는 소 프트맥스(softrmax) 함수를 통해 출력 문장의 각 단어별 확률 값을 획득하고, 획득된 확률 값을 바탕으로 각각 의 출력 단어를 예측할 수 있다. 한편, 본 개시에 따르면, 각각의 디코더 RNN 셀은 제1 문장과의 유사도에 관련된 임계 값을 인코더로 부터 전달 받는바, 출력 단어를 예측함에 있어 수신된 임계 값을 고려하여 출력 단어들을 예측할 수 있다. 상술한 바와 같이, 출력 단어들이 예측되면, 디코더는 출력 단어들로 구성된 출력 문장을 출력할 수 있다. 예를 들어, 입력 문장이 \"봄맞이 대청소 합시다\"이고, 제1 문장과의 유사도에 관련된 임계 값이 0.95와 같 이 높은 유사도를 가지는 임계 값인 경우, 디코더는 \"봄을\", \"맞이하여\", \"대청소\" 및 \"합시다\"와 같은 단 어로 구성된 출력 문장을 출력할 수 있다.한편, 도 5에는 도시하지 않았으나, 변환 모델은 어텐션(attention) 모듈을 더 포함할 수도 있다. 어텐션 모듈은 출력 단어를 예측하는 각각의 시점마다, 입력 문장 전체를 참고하되 해당 시점에서 예측해야 할 단어와 관련이 있는 단어를 집중(attention)해서 참고하는 모듈이라고 할 수 있다. 변환 모델이 어텐션 모듈을 포함하면, 입력 문장이 길어지는 경우 초래되는 출력 문장의 품질 저하를 방지할 수 있다. 한편, 변환 모델은 입력 문장의 번역을 위한 것이 아니고, 단지 입력 문장을 병렬 코퍼스에 익숙한 문장 으로 수정하기 위한 것에 불과하다. 따라서, 후술하는 바와 같은 번역 모델에 비해 많은 레이어를 필요로 하지 않을 수 있다. 또한, 변환 모델을 통해 획득된 출력 문장의 길이가 입력 문장에 비해 크게 변하지 않을 수 있으므로, 다양한 길이의 입력 문장 및 출력 문장을 처리하기 위한 bucketing 및 padding과 같은 과정 이 간소화될 수 있으며, 디코더의 입력에서 이전 토큰에 대한 종속성을 제거하는 non-autoregressive decoder를 사용함으로써 디코딩 시간을 줄일 수도 있다. 즉, 전자 장치가 통상적인 인공 신경망 기반 기계 번역(Neural Machine Translation, NMT) 인공 지능 모델에 더하여 본 개시에 따른 변환 모델을 더 포함 하더라도 메모리 내지는 설계 상의 부담을 초래하지 않을 수 있다. 이상에서 상술한 바와 같이, 프로세서는 제1 문장 및 제1 문장과의 유사도에 관련된 임계 값을 변환 모델 에 입력하여, 제1 문장과 임계 값 이상의 유사도를 갖는 제2 문장을 획득할 수 있다. 그리고, 제1 문장과 임계 값 이상의 유사도를 갖는 제2 문장이 획득되면(S320-Y), 프로세서는 제1 언어를 제2 언어로 번역하도 록 학습된 번역 모델에 획득된 수정된 제2 문장을 입력하여, 제1 언어의 제2 문장이 제2 언어로 번역된 제3 문장을 획득할 수 있다. 반대로, 제1 문장과 임계 값 이상의 유사도를 갖는 제2 문장이 획득되지 않으면(S320-N), 프로세서는 제1 언어를 제2 언어로 번역하도록 학습된 번역 모델에 수정되지 않은 제1 문장을 입력하여, 제1 언어의 제1 문장이 제2 언어로 번역된 제3 문장을 획득할 수 있다. 여기서, 제1 언어의 제2 문장이 제2 언어로 번역된 문장과 제1 언어의 제1 문장이 제2 언어로 번역된 문장을 모 두 제3 문장으로 지칭하였으나, 이는 번역 모델을 통해 출력된 문장을 제3 문장으로 지칭함에 따른 것일 뿐이다. 즉, 번역 모델에 입력되는 문장이 각각 제1 문장과 제2 문장으로 상이하면, 번역 모델을 통해 출력되는 문장 또한 서로 상이할 수 있음은 물론이다. 한편, 입력 문장이 동일한 한국어의 제1 문장인 경우에도, 번역 모델이 한국어를 영어로 번역하도록 학습된 인 공 지능 모델인지, 아니면 번역 모델이 한국어를 중국어로 번역하도록 학습된 인공 지능 모델인지에 따라 제1 문장이 수정된 제2 문장과 제2 문장이 번역된 제3 문장이 달라질 수 있음은 물론이다. 즉, 기계 번역을 위한 인 공 지능 모델의 경우 번역 언어 별로 학습에 이용되는 병렬 코퍼스가 상이하기 때문에, 그 병렬 코퍼스에 익숙 하도록 수정된 제2 문장이 달라질 수 있으며, 제2 문장이 달라지면 제2 문장이 번역된 제3 문장도 달라질 수 있 다. 도 6은 본 개시의 일 실시 예에 따른 변환 모델의 학습 과정을 설명하기 위한 도면이다. 도 4를 참조하여 전술한 바와 같이, 일반적으로, 단일어 코퍼스는 병렬 코퍼스에 비하여 많은 수의 문장을 포함 한다. 즉, 입력 문장이 단일어 코퍼스에 익숙하지 않으면 병렬 코퍼스에도 익숙하지 않은 것이 일반적이다. 도 6은 이와 같은 병렬 코퍼스와 단일어 코퍼스의 관계를 고려하여, 병렬 코퍼스의 영역과 병렬 코퍼스의 영 역에 포함되지 않는 단일어 코퍼스의 영역, 그리고 병렬 코퍼스의 영역에 포함되는 문장(s2) 및 병렬 코퍼스의 영역에 포함되지 않는 단일어 코퍼스의 영역에 포함되는 문장(s1)을 관념적으로 나타 낸 것이다. 한편, 전술한 바와 같이, 전자 장치는 제1 문장 및 제1 문장과의 유사도에 관련된 임계 값을 변환 모델에 입력하여, 제1 문장과 임계 값 이상의 유사도를 갖는 제2 문장을 획득할 수 있다. 그런데, 변환 모델을 통해 획 득되는 제2 문장은 변환 모델의 학습에 따라 달라질 수 있다. 따라서, 병렬 코퍼스의 영역에 포함되지 않 는 단일어 코퍼스의 영역에 있는 문장(s1)을 병렬 코퍼스의 영역에 있는 문장(s2)으로 매핑시키는 과 정을 통해 변환 모델을 학습시킬 필요가 있다. 본 개시에 따른 변환 모델은 단일어 코퍼스에 포함된 복수의 문장 및 병렬 코퍼스에 포함된 복수의 문장 사이의 유사도(sentence similarity)를 바탕으로 학습될 수 있다. 첫째로, 변환 모델은 단일어 코퍼스에 포함된 제1 언어의 복수의 문장 및 제1 언어를 제2 언어로 번역하기 위한 병렬 코퍼스에 포함된 제1 언어의 복수의 문장 사이의 유사도를 바탕으로 학습될 수 있다. 구체적으로, 변환 모 델은 단일어 코퍼스에 포함된 제1 언어의 복수의 문장 중 병렬 코퍼스에는 포함되지 않는 문장(s1)을 제1 언어 를 제2 언어로 번역하기 위한 병렬 코퍼스에 포함된 제1 언어의 복수의 문장 중 기 설정된 임계 값 이상의 유사 도를 갖는 문장(s2)으로 매핑시키는 과정을 통해 학습될 수 있다. 보다 구체적으로, 변환 모델은 단일어 코퍼스에 포함된 제1 언어의 복수의 문장 중 병렬 코퍼스에는 포함되지 않는 제1 학습 문장(s1)을 식별하고, 제1 언어를 제2 언어로 번역하기 위한 병렬 코퍼스에 포함되는 제1 언어의 복수의 문장 중 제1 학습 문장(s1)과 기 설정된 임계 값 이상의 유사도를 갖는 제2 학습 문장(s2)을 식별한다. 여기서, 제2 학습 문장(s2)을 식별하는 과정은 레벤슈타인 거리(Lvenshtein distance) 또는 N-gram을 이용한 분 석 등과 같은 공지의 다양한 방법을 통해 수행될 수 있다. 제2 학습 문장(s2)이 식별되면, 변환 모델은 제1 학 습 문장(s1)을 제2 학습 문장(s2)으로 매핑시키는 과정을 통해 학습될 수 있다. 둘째로, 변환 모델은 단일어 코퍼스에 포함된 제1 언어의 복수의 문장 및 제2 언어를 제1 언어로 번역하기 위한 병렬 코퍼스에 포함된 제1 언어의 복수의 문장 사이의 유사도를 바탕으로 학습될 수 있다. 다시 말해, 단일어 코퍼스가 제1 언어의 복수의 문장을 포함하는 경우, 변환 모델의 학습은 제1 언어를 제2 언어로 번역하기 위한 병렬 코퍼스뿐만 아니라, 제2 언어를 제1 언어로 번역하기 위한 병렬 코퍼스를 바탕으로 이루어질 수도 있다. 셋째로, 변환 모델은 제1 언어를 제2 언어로 번역하기 위한 병렬 코퍼스에 포함된 제2 언어의 복수의 문장 사이 의 유사도를 바탕으로 학습될 수도 있다. 보다 구체적으로, 병렬 코퍼스가 제1 언어를 제2 언어로 번역하기 위 한 것인 경우, 병렬 코퍼스에 포함된 복수의 제2 언어의 문장 사이의 유사도 값을 복수의 제2 언어의 문장 각각 에 대응되는 복수의 제1 언어의 문장 사이의 유사도 값으로 설정하고, 설정된 유사도 값을 바탕으로 복수의 제1 언어의 문장이 서로 매핑되도록 변환 모델을 학습시킬 수도 있다. 이상에서는 변환 모델의 학습 방법에 대해서 상술하였으나, 변환 모델의 학습 방법이 상술한 바와 같은 실시 예 에 국한되는 것은 아니다. 한편, 변환 모델뿐만 아니라, 제1 언어 모델, 제2 언어 모델 및 번역 모델 또한 공지 된 다양한 방법을 통해 학습될 수 있음은 물론이며, 제1 언어 모델, 제2 언어 모델, 변환 모델 및 번역 모델의 전체 경로(pipeline)중 적어도 일부가 결합되어 학습될 수도 있다. 도 7a는 본 개시의 일 실시 예에 따른 전자 장치의 구성을 간략하게 나타내는 블록도이고, 도 7b는 본 개 시의 일 실시 예에 따른 전자 장치의 구성을 상세하게 나타내는 블록도이다. 도 7a에 도시된 바와 같이, 본 개시의 일 실시 예에 따른 전자 장치는 메모리 및 프로세서를 포 함한다. 프로세서가 메모리에 포함된 복수의 인공 지능 모델(21, 22, 23)을 로딩하고, 프로세서(12 0)에 포함된 복수의 모듈(11, 12, 13)이 복수의 인공 지능 모델(21, 22, 23)을 이용하여 본 개시에 따른 동작을 수행하는 과정에 대해서는 도 2를 참조하여 상술하였으므로, 도 7a에서는 메모리 및 프로세서의 구체 적인 구성을 중심으로 설명한다. 메모리에는 전자 장치에 관한 적어도 하나의 인스트럭션(instruction)이 저장될 수 있다. 그리고, 메 모리에는 전자 장치를 구동시키기 위한 O/S(Operating System)가 저장될 수 있다. 또한, 메모리 에는 본 개시의 다양한 실시 예들에 따라 전자 장치가 동작하기 위한 각종 소프트웨어 프로그램이나 애플리케이션이 저장될 수도 있다. 그리고, 메모리는 플래시 메모리(Flash Memory) 등과 같은 반도체 메모 리나 하드디스크(Hard Disk) 등과 같은 자기 저장 매체 등을 포함할 수 있다. 구체적으로, 메모리에는 본 개시의 다양한 실시 예에 따라 전자 장치가 동작하기 위한 각종 소프트웨 어 모듈이 저장될 수 있으며, 프로세서는 메모리에 저장된 각종 소프트웨어 모듈을 실행하여 전자 장 치의 동작을 제어할 수 있다. 즉, 메모리는 프로세서에 의해 액세스되며, 프로세서에 의한 데이터의 독취/기록/수정/삭제/갱신 등이 수행될 수 있다. 한편, 본 개시에서 메모리라는 용어는 메모리, 프로세서 내 롬(미도시), 램(미도시) 또는 전자 장치에 장착되는 메모리 카드(미도시)(예를 들어, micro SD 카드, 메모리 스틱)를 포함하는 의미로 사용될 수 있으며, 플래시 메모리 (Flash Memory) 및 PROM(Programmable Read-Only Memory) 등과 같은 비휘발성 메모리와 DRAM(Dynamic Random-Access Memory) 및 SRAM(Static RAM) 등과 같은 휘발성 메모리를 포함하는 의미 로 사용될 수 있다. 특히, 본 개시에 따른 메모리에는 적어도 하나의 언어 모델, 변환 모델 및 번역 모델이 저장될 수 있다. 메모리에 저장된 복수의 인공 지능 모델에 대해서는 도 2를 참조하여 상술하였으므로, 중복 설명은 생략한 다. 프로세서는 전자 장치의 전반적인 동작을 제어한다. 구체적으로, 프로세서는 상술한 바와 같은 메모리를 포함하는 전자 장치의 구성과 연결되며, 상술한 바와 같은 메모리에 저장된 적어도 하 나의 인스트럭션을 실행함으로써 전자 장치의 동작을 전반적으로 제어할 수 있다. 프로세서는 다양한 방식으로 구현될 수 있다. 예를 들어, 프로세서는 주문형 집적 회로(Application Specific Integrated Circuit, ASIC), 임베디드 프로세서, 마이크로 프로세서, 하드웨어 컨트롤 로직, 하드웨 어 유한 상태 기계(hardware Finite State Machine, FSM), 디지털 신호 프로세서(Digital Signal Processor, DSP) 중 적어도 하나로 구현될 수 있다. 한편, 본 개시에서 프로세서라는 용어는 CPU(Central Processing Unit), GPU(Graphic Processing Unit) 및 MPU(Main Processing Unit)등을 포함하는 의미로 사용될 수 있다. 특히, 본 개시에 따른 프로세서는 제1 언어의 제1 문장이 획득되면 학습 코퍼스를 바탕으로 학습된 제2 언 어 모델을 이용하여 제1 문장을 제1 언어의 다른 문장으로 수정할 지 여부를 결정하고, 제1 문장을 제1 언어의 다른 문장으로 수정하는 것으로 결정되면 입력 문장과 임계 값 이상의 유사도를 갖는 다른 문장을 획득하도록 학습된 변환 모델에 제1 문장을 입력하여 제1 문장이 수정된 제1 언어의 제2 문장을 획득하며, 제2 문장이 획득 되면 상기 학습 코퍼스를 바탕으로 학습된 번역 모델에 제2 문장을 입력하여 제2 언어의 제3 문장을 획득한다. 본 개시에 따른 다양한 실시 예에 대해서는 도 1 내지 도 6을 참조하여 상술하였으므로, 중복 설명은 생략한다. 본 개시에 따른 인공 지능과 관련된 기능은 메모리 및 프로세서를 통해 수행될 수 있다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP 등과 같은 범용 프로세서, GPU. VPU 등과 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공 지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인공 지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 기 정의된 동작 규칙 또는 인공 지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만들어진다는 것은, 다수의 학습 데이터들에 학습 알고리즘을 적용함으로써, 원하는 특성 의 기 정의된 동작 규칙 또는 인공 지능 모델이 만들어짐을 의미한다. 이러한 학습은 본 개시에 따른 인공 지능 이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버/시스템을 통해 이루어 질 수도 있다. 인공 지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 각 레이어는 복수의 가중치(weight values)을 갖 고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치의 연산을 통해 레이어의 연산을 수행한다. 신 경망의 예로는, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network), GAN(Generative Adversarial Networks) 및 심층 Q-네트워크 (Deep Q-Networks)이 있으 며, 본 개시에서의 신경망은 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. 학습 알고리즘은, 다수의 학습 데이터들을 이용하여 소정의 대상 기기(예컨대, 로봇)을 훈련시켜 소정의 대상 기기 스스로 결정을 내리거나 예측을 할 수 있도록 하는 방법이다. 학습 알고리즘의 예로는, 지도형 학습 (supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또 는 강화 학습(reinforcement learning)이 있으며, 본 개시에서의 학습 알고리즘은 명시한 경우를 제외하고 전술 한 예에 한정되지 않는다. 한편, 도 7b에 도시된 바와 같이, 본 개시의 일 실시 예에 따른 전자 장치는 메모리 및 프로세서 뿐만 아니라, 통신부, 입력부 및 출력부를 더 포함할 수 있다. 그러나, 이와 같은 구성은 예시적인 것으로서, 본 개시를 실시함에 있어 이와 같은 구성에 더하여 새로운 구성이 추가되거나 일부 구성이 생략될 수 있음을 물론이다. 통신부는 회로를 포함하며, 서버 또는 외부 장치와의 통신을 수행할 수 있다. 구체적으로, 통신부는 WiFi 모듈, Bluetooth 모듈, 무선 통신 모듈, 및 NFC 모듈 중 적어도 하나를 포함할 수 있다. 구체적으로, WiFi 모듈과 Bluetooth 모듈 각각은 WiFi 방식, Bluetooth 방식으로 통신을 수행할 수 있다. WiFi 모듈이나 Bluetooth 모듈을 이용하는 경우에는 SSID 등과 같은 각종 연결 정보를 먼저 송수신하여, 이를 이용하여 통신 연결한 후 각종 정보들을 송수신할 수 있다. 또한, 무선 통신 모듈은 IEEE, Zigbee, 3G(3rd Generation), 3GPP(3rd Generation Partnership Project), LTE(Long Term Evolution), 5G(5th Generation) 등과 같은 다양한 통신 규격에 따라 통신을 수행할 수 있다. 그리고, NFC 모듈은 135kHz, 13.56MHz, 433MHz, 860~960MHz, 2.45GHz 등과 같은 다양한 RF-ID 주파수 대역들 중에서 13.56MHz 대역을 사용하는 NFC(Near Field Communication) 방식으로 통신을 수행할 수 있다. 프로세서는 통신부를 통해 외부 장치와의 통신 연결을 수립하고, 외부 장치로부터 각종 데이터 또는 정보를 수신할 수 있으며, 외부 장치로 각종 데이터 또는 정보를 전송하도록 통신부를 제어할 수도 있다. 특히, 본 개시의 일 실시 예에 따르면, 프로세서는 통신부를 통해 전자 장치와 연결된 외부 장 치로부터 입력 문장을 수신하여 획득할 수 있다. 그리고, 프로세서는 상술한 바와 같은 과정을 통해, 획득 된 입력 문장을 바탕으로 입력 문장이 번역된 출력 문장을 획득할 수 있다. 출력 문장이 획득되면, 프로세서 는 외부 장치로 입력 문장이 번역된 출력 문장을 전송하도록 통신부를 제어할 수도 있다. 입력부는 회로를 포함하며, 프로세서는 입력부를 통해 전자 장치의 동작을 제어하기 위한 사용자 명령을 수신할 수 있다. 또한, 프로세서는 입력부를 통한 다양한 사용자 입력을 바탕으로 제1 언어의 입력 문장, 즉 제1 문장을 획득할 수 있다. 구체적으로, 입력부는 터치 스크린으로서 디스플레이에 포함된 형태로 구현될 수도 있다. 그리고, 프로세 서는 디스플레이 상에 표시된 사용자 인터페이스(User Interface, UI)를 터치하는 사용자 입력을 수신하고, 수신된 사용자 입력을 바탕으로 제1 언어의 입력 문장인 제1 문장을 획득할 수 있다. 한편, 입력부는 마이크를 포함할 수 있으며, 프로세서는 마이크를 통해 사용자 음성을 수신할 수 있 다. 그리고, 사용자 음성이 수신되면, 프로세서는 음성 인식을 위한 인공 지능 모델에 사용자 음성을 입력 하여 수신된 사용자 음성에 대응되는 텍스트, 즉 제1 언어의 입력 문장인 제1 문장을 획득할 수 있다. 출력부는 회로를 포함하며, 프로세서는 출력부를 통해 전자 장치가 수행할 수 있는 다양한 기능을 출력할 수 있다. 그리고, 출력부는 디스플레이 및 스피커 등을 포함할 수 있다. 구체적으로, 프로세서는 획득된 제1 언어의 제1 문장을 표시하도록 디스플레이를 제어할 수 있다. 그리고, 제1 언어를 제2 언어로 번역하도록 학습된 번역 모델을 통해 제2 언어의 제3 문장이 획득되면, 프로세서는 획득된 제2 언어의 제3 문장을 표시하도록 디스플레이를 제어할 수 있다. 또한, 입력 문장과 임계 값 이상의 유 사도를 갖는 다른 문장을 획득하도록 학습된 변환 모델을 통해 제1 언어의 제2 문장이 획득되면, 프로세서(12 0)는 획득된 제2 문장을 표시하도록 디스플레이를 제어할 수도 있다. 한편, 제1 언어를 제2 언어로 번역하도록 학습된 번역 모델을 통해 제2 언어의 제3 문장이 획득되면, 프로세서 는 음성 합성을 위한 인공 지능 모델에 획득된 제3 문장을 입력하여 제3 문장에 대응되는 음성을 획득하고, 스피커를 통해 제3 문장에 대응되는 음성을 출력할 수도 있다. 이상에서 상술한 바와 같은 본 개시의 일 실시 예에 따르면, 기계 번역을 위한 인공 지능 모델에 입력 문장을 입력하기 전에 인공 지능 모델의 학습에 이용된 학습 코퍼스에 익숙한 문장으로 입력 문장을 수정한 후, 수정된 입력 문장을 인공 지능 모델에 입력함으로써 번역 품질이 높은 출력 문장을 향상시킬 수 있다. 도 8은 본 개시에 따른 제어 과정의 적어도 일부가 전자 장치와 연결된 외부 장치에 의해 수행되는 실시 예를 설명하기 위한 시퀀스도이다. 즉, 본 개시에 따른 제어 과정이 모두 전자 장치에 의해 수행되는 것을 전제로 설명하였으나, 입력 문장의 수정 여부를 결정하는 과정, 입력 문장을 수정하는 과정 및 입력 문장이 번역된 출력 문장을 획득하는 과정 중 적어도 일부는 전자 장치와 연결된 외부 장치에 의해 수행될 수도 있다. 이하에서 도 8을 참조하여 본 개시의 일 실시 예를 설명함에 있어서는 도 1 내지 도 7에 대한 설명에서 상술한 바와 같은 내용과 중복되는 설명은 생략한다. 도 8에 도시된 바와 같이, 전자 장치는 제1 문장을 획득할 수 있다(S810). 제1 문장이 획득되면, 전자 장 치는 제1 문장을 제1 언어 모델 및 제2 언어 모델에 입력하여 제1 문장의 수정 여부를 결정할 수 있다 (S820). 제1 문장을 수정하는 것으로 결정되면, 전자 장치는 제1 문장을 변환 모델에 입력하여 제1 문장이수정된 제2 문장을 획득할 수 있다(S830). 한편, 전자 장치는 획득된 제2 문장에 대한 번역 과정을 전자 장치에서 수행할 것인지 여부를 결정할 수 있다(S840). 구체적으로, 전자 장치는 전자 장치에 포함된 제1 번역 모델 및 외부 장치에 포 함된 제2 번역 모델 중 제2 문장에 대한 번역 과정을 수행할 번역 모델을 결정할 수 있다. 여기서, 제2 문장에 대한 번역 과정을 수행할 번역 모델은, 제1 번역 모델 및 제2 번역 모델의 성능, 제1 번역 모델 및 제2 번역 모 델을 통해 번역 과정을 수행하는 경우의 처리 속도 등을 비교하여 결정될 수 있다. 제2 문장에 대한 번역 과정이 전자 장치에서 수행되는 것으로 결정되면(S840-N), 전자 장치는 획득된 제2 문장을 제2 번역 모델에 입력하여 제2 문장이 번역된 제3 문장을 획득할 수 있다(S890). 한편, 제2 문장에 대한 번역 과정이 외부 장치에서 수행되는 것으로 결정되면(S840-Y), 전자 장치는 획득된 제2 문장을 외부 장치에 전송할 수 있다(S860). 그리고, 외부 장치는 전자 장치로부터 수신된 제2 문장을 제2 번역 모델에 입력하여 제2 문장이 번역된 제3 문장을 획득하고(S870), 획득된 제3 문장 을 전자 장치로 전송할 수 있다(S880). 그리고, 전자 장치는 외부 장치로부터 제3 문장을 수신 함으로써 제3 문장을 획득할 수 있다(S890). 예를 들어, 전자 장치가 스마트 폰으로 구현되고 외부 장치가 서버로 구현되는 경우로서, 전자 장치 에 온-디바이스(on-device)로 포함된 제1 번역 모델에 비하여 외부 장치에 포함된 제2 번역 모델의 성능이 더 높다면, 전자 장치는 획득된 제2 문장에 대한 번역 과정을 외부 장치에서 수행하는 것으로 결정하고, 외부 장치를 통해 제2 문장이 번역된 제3 문장을 획득할 수 있다. 한편, 이상에서는 도 8을 참조하여, 입력 문장이 번역된 출력 문장을 획득하는 과정이 외부 장치에 의해 수행될 수 있음을 전제로 한 실시 예에 대해 설명하였으나, 그 밖에도 입력 문장의 수정 여부를 결정하는 과정, 입력 문장을 수정하는 과정과 같은 본 개시의 다양한 실시 예에 따른 과정 중 적어도 일부 역시 외부 장치(20 0)를 통해 수행될 수 있음은 물론이다. 한편, 이상에서는 제1 언어 모델, 제2 언어 모델, 변환 모델 및 번역 모델이 각각 별개의 독립된 인공 지능 모 델로 구현되는 경우를 전제로 설명하였으나, 이는 각각의 인공 지능 모델의 동작을 명확하게 설명하기 위한 것 일 뿐, 본 개시가 이에 국한되는 것은 아니다. 즉, 본 개시에 따른 제1 언어 모델, 제2 언어 모델, 변환 모델 및 번역 모델 중 적어도 두 개 이상은 통합된 하나의 인공 지능 모델로 구현될 수 있다. 구체적으로, 본 개시에 따른 제1 언어 모델, 제2 언어 모델 및 변환 모델이 하나의 통합된 인공 지능 모델로 구 현되고, 번역 모델만이 별개의 독립된 인공 지능 모델로 구현될 수도 있다. 즉, 제1 문장이 획득되면, 전자 장 치는 제1 언어 모델, 제2 언어 모델 및 변환 모델이 하나의 통합된 인공 지능 모델에 제1 문장을 입력하여, 제1 문장이 수정된 제2 문장을 획득할 수 있다. 그리고, 제2 문장이 획득되면, 전자 장치는 번 역 모델에 제2 문장을 입력하여, 제2 문장이 번역된 제3 문장을 획득할 수 있다. 한편, 본 개시에 따른 제1 언어 모델, 제2 언어 모델, 변환 모델 및 번역 모델 전체가 통합된 인공 지능 모델로 구현될 수도 있다. 즉, 상술한 바와 같은 입력 문장의 수정 여부를 결정하는 과정, 입력 문장을 수정하는 것으 로 결정된 경우 입력 문장을 병렬 코퍼스에 익숙한 문장으로 수정하는 과정, 그리고 수정된 입력 문장을 번역함 으로써 출력 문장을 획득하는 과정은 제1 언어 모델, 제2 언어 모델, 변환 모델 및 번역 모델 전체가 통합된 인 공 지능 모델을 바탕으로 통합적으로 수행될 수 있다. 특히, 번역 모델의 인코더 출력이 병렬 코퍼스의 특징 벡터 공간에 가깝게 출력되도록 인코더 앞 단의 구조를 변화시킴으로써, 변환 모델과 번역 모델이 통합된 인공 지능 모델을 구현할 수 있다. 이에 따라, 병렬 코퍼스에 포함되지 않는 입력 문장이 입력되는 경우에도, 병렬 코퍼스에 포함되는 입력 문장이 입력된 경우와 마찬가지의 인코더 출력을 획득할 수 있다. 한편, 도시하지는 않았으나, 제1 언어 모델 및 제2 언어 모델이 하나의 통합된 인공 지능 모델로 구현되고, 변 환 모델 및 번역 모델이 하나의 통합된 인공 지능 모델로 구현되는 경우에도 본 개시가 적용될 수 있음은 물론 이다. 한편, 상술한 바와 같은 다양한 유형의 통합 인공 지능 모델의 전체 경로(pipeline) 또는 부분 경로는 공지된 바와 같은 다양한 학습 방법을 통해 학습될 수 있다. 한편, 전술한 바와 같은 전자 장치의 제어 방법은 프로그램으로 구현되어 전자 장치에 제공될 수 있 다. 특히, 전자 장치의 제어 방법을 포함하는 프로그램은 비일시적 판독 가능 매체(non-transitorycomputer readable medium)에 저장되어 제공될 수 있다. 구체적으로, 전자 장치의 제어 방법을 실행하는 프로그램을 포함하는 컴퓨터 판독 가능 기록매체에 있어서, 전자 장치의 제어 방법은 제1 언어의 제1 문장이 획득되면, 학습 코퍼스를 바탕으로 학습된 언어 모델을 이용하여 제1 문장을 제1 언어의 다른 문장으로 수정할 지 여부를 결정하는 단계, 제1 문장을 제1 언어 의 다른 문장으로 수정하는 것으로 결정되면, 입력 문장과 임계 값 이상의 유사도를 갖는 다른 문장을 획득하도 록 학습된 변환 모델에 제1 문장을 입력하여 제1 문장이 수정된 제1 언어의 제2 문장을 획득하는 단계, 그리고, 제2 문장이 획득되면, 상기 학습 코퍼스를 바탕으로 학습된 번역 모델에 제2 문장을 입력하여 제2 언어의 제3 문장을 획득하는 단계를 포함한다. 여기서, 비일시적 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체 가 아니라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 구체적으로 는, 상술한 다양한 어플리케이션 또는 프로그램들은 CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등과 같은 비일시적 판독 가능 매체에 저장되어 제공될 수 있다. 이상에서 전자 장치의 제어 방법을 실행하는 프로그램을 포함하는 컴퓨터 판독 가능 기록매체에 대해 간략 하게 설명하였으나, 이는 중복 설명을 생략하기 위한 것일 뿐이며, 전자 장치의 제어 방법에 대한 다양한 실시 예는 전자 장치의 제어 방법을 실행하는 프로그램을 포함하는 컴퓨터 판독 가능 기록매체에 대해서도 적용될 수 있음은 물론이다. 이상에서 상술한 바와 같은 본 개시의 일 실시 예에 따르면, 기계 번역을 위한 인공 지능 모델에 입력 문장을 입력하기 전에 인공 지능 모델의 학습에 이용된 학습 코퍼스에 익숙한 문장으로 입력 문장을 수정한 후, 수정된 입력 문장을 인공 지능 모델에 입력함으로써 번역 품질이 높은 출력 문장을 향상시킬 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2019-0119791", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시가 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안 될 것이다."}
{"patent_id": "10-2019-0119791", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 간략하게 설명하기 위한 도면, 도 2는 본 개시에 따른 전자 장치의 하드웨어 구조와 소프트웨어 구조 사이의 결합 관계를 전제로 본 개시 에 따른 일 실시 예를 설명하기 위한 도면, 도 3은 본 개시의 일 실시 예에 따른 제어 과정을 구체적으로 설명하기 위한 흐름도, 도 4는 입력 문장의 수정 여부를 결정하는 과정을 상세하게 설명하기 위한 모식도, 도 5는 변환 모델을 통해 입력 문장을 병렬 코퍼스에 익숙한 문장으로 수정하는 과정을 상세하게 설명하기 위한 도면, 도 6은 본 개시의 일 실시 예에 따른 변환 모델의 학습 과정을 설명하기 위한 도면, 도 7a는 본 개시의 일 실시 예에 따른 전자 장치의 구성을 간략하게 나타내는 블록도, 도 7b는 본 개시의 일 실시 예에 따른 전자 장치의 구성을 상세하게 나타내는 블록도, 그리고, 도 8은 본 개시에 따른 제어 과정의 적어도 일부가 전자 장치와 연결된 외부 장치에 의해 수행되는 실시 예를 설명하기 위한 시퀀스도이다."}
