{"patent_id": "10-2020-0163822", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0075579", "출원번호": "10-2020-0163822", "발명의 명칭": "분산형 강화 학습 시스템 및 이에 있어서의 병렬 강화 학습 방법", "출원인": "한국전자통신연구원", "발명자": "장수영"}}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "각각이 복수의 시뮬레이터들과 복수의 학습모듈들을 적재하고 실행할 수 있으며, 상기 복수의 시뮬레이터들 중적어도 하나와 상기 복수의 학습모듈들 중 적어도 하나를 실행하여, 상기 적어도 하나의 시뮬레이터가 조성하는환경에서 상기 적어도 하나의 학습모듈이 경험을 쌓으면서 지능을 향상시키도록 하는 복수의 컴퓨팅 노드들; 및상기 복수의 컴퓨팅 노드들 각각의 상태 정보를 수집하고 수집된 상태 정보를 토대로 상기 복수의 컴퓨팅 노드들 각각에서 실행될 상기 적어도 하나의 시뮬레이터와 상기 적어도 하나의 학습모듈을 배치하는 학습 관리서버;를 구비하고,상기 관리 서버가프로세서; 및상기 프로세서에 의해 실행되는 프로그램 명령어들을 저장하는 메모리;를 포함하고,상기 프로그램 명령어들은 상기 프로세서에 의해 실행되었을 때: 상기 복수의 컴퓨팅 노드들 각각에 대하여 시뮬레이션 실행 부하를 토대로 상기 복수의 시뮬레이터들 중 실행되는 시뮬레이터의 숫자를 조절하는 동작;을 수행하는 명령을 포함하는 강화 학습 시스템."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서, 상기 실행되는 시뮬레이터의 숫자를 조절하는 동작을 수행하는 명령은 상기 복수의 컴퓨팅 노드 각각에서 에피소드가 종료될 때마다 소정의 속도 지표가 일정한 기준치보다 낮은지판단하는 동작; 및 상기 속도 지표가 상기 기준치보다 낮은 경우 종료된 에피소드에 연관된 시뮬레이터를 종료하고 새로운 시뮬레이터의 동작을 시작하지 않으며, 상기 속도 지표가 상기 기준치보다 낮지 않은 경우에만 새로운 시뮬레이터의동작이 시작되게 하는 동작;을 수행하는 명령을 포함하는 강화 학습 시스템."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서, 상기 속도 지표가 상기 기준치보다 낮은지 판단하는 동작을 수행하는 명령은 상기 프로세서에 의해 실행되었을 때 상기 기준치로서 상기 시뮬레이션 부하에 대한 임계치를 결정하는 동작; 일정한 시간간격마다 상기 복수의 컴퓨팅 노드 각각에서 실행중인 시뮬레이터들의 실시간계수 합산치를 기록하는 동작; 상기 복수의 컴퓨팅 노드 각각에서 에피소드 종료시마다 실행중이거나 실행이 종료되는 시뮬레이터들의 상기실시간계수 합산치의 평균치를 계산하는 동작; 및 상기 평균치가 상기 임계치보다 큰 지 판단하는 동작;을 수행하는 명령을 포함하는 강화 학습 시스템."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "공개특허 10-2022-0075579-3-청구항 2에 있어서, 상기 속도 지표가 상기 기준치보다 낮은지 판단하는 동작을 수행하는 명령은 상기 프로세서에 의해 실행되었을 때 일정한 시간간격마다 상기 복수의 컴퓨팅 노드 각각에서 실행중인 시뮬레이터들의 실시간계수 합산치를 기록하고, 상기 합산치의 이동평균을 계산하는 동작; 상기 복수의 컴퓨팅 노드 각각에서 에피소드 종료시마다 실행중이거나 실행이 종료되는 시뮬레이터들의 상기실시간계수 합산치의 평균치를 계산하는 동작; 및 상기 평균치가 상기 이동평균보다 큰 지 판단하는 동작;을 수행하는 명령을 포함하는 강화 학습 시스템."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서, 상기 평균치가 상기 이동평균보다 큰 지 판단하는 동작을 수행하는 명령은 상기 프로세서에의해 실행되었을 때 해당 컴퓨팅 노드의 프로세서 부하가 일정 기준보다 큰 지를 추가적으로 판단하는 동작;을 수행하는 명령을 포함하는 강화 학습 시스템."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 1에 있어서, 상기 프로그램 명령어들은 상기 프로세서에 의해 실행되었을 때:상기 복수의 컴퓨팅 노드들 각각에서의 각 시뮬레이터의 상태와 상기 시뮬레이터가 형성하는 시뮬레이션 환경에서 학습을 행한 학습 모듈의 경험을 받아들이고, 상기 시뮬레이터 상태와 상기 학습 모듈 경험을 맵핑시킨 형태의 학습 상태를 저장 장치에 저장하는 동작;다수의 학습 상태에 대하여 우선순위를 결정하는 동작; 및상기 복수의 컴퓨팅 노드 각각에서 에피소드가 종료되어 새로운 시뮬레이터의 동작이 시작되어야 할 때 상기 우선순위가 높은 학습 상태가 실행되게 하는 동작;을 수행하는 명령을 포함하는 강화 학습 시스템."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서, 상기 우선순위를 결정하는 동작을 수행하는 명령은 상기 프로세서에 의해 실행되었을 때 상기 복수의 학습모듈들이 잘 못하거나 낯선 학습 상태에 대하여 높은 우선순위를 부여하는 동작;을 수행하는 명령을 포함하는 강화 학습 시스템."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "각각이 복수의 시뮬레이터들과 복수의 학습모듈들을 적재하고 실행할 수 있는 복수의 컴퓨팅 노드들을 구비하는강화 학습 시스템에서의, 병렬 강화 학습 방법으로서,상기 복수의 컴퓨팅 노드들 각각에서 상기 복수의 시뮬레이터들 중 적어도 하나와 상기 복수의 학습모듈들 중적어도 하나가 실행되게 하여, 상기 적어도 하나의 시뮬레이터가 조성하는 환경에서 상기 적어도 하나의 학습모듈이 경험을 쌓으면서 지능을 향상시키도록 하는 단계; 및상기 복수의 컴퓨팅 노드 각각에서 에피소드가 종료될 때마다 상기 복수의 컴퓨팅 노드들 각각에 대하여 시뮬레이션 실행 부하를 토대로 상기 복수의 시뮬레이터들 중 실행되는 시뮬레이터의 숫자를 조절하는 단계;를 포함하는 병렬 강화 학습 방법."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 8에 있어서, 상기 시뮬레이터 숫자를 조절하는 단계가상기 복수의 컴퓨팅 노드 각각에서 에피소드가 종료될 때마다 소정의 속도 지표가 일정한 기준치보다 낮은지 판공개특허 10-2022-0075579-4-단하는 단계; 및상기 속도 지표가 상기 기준치보다 낮은 경우 종료된 에피소드에 연관된 시뮬레이터를 종료하고 새로운 시뮬레이터의 동작을 시작하지 않으며, 상기 속도 지표가 상기 기준치보다 낮지 않은 경우에만 새로운 시뮬레이터의동작이 시작되게 하는 단계;를 포함하는 병렬 강화 학습 방법."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 9에 있어서, 상기 속도 지표가 상기 기준치보다 낮은지 판단하는 단계가상기 기준치로서 상기 시뮬레이션 부하에 대한 임계치를 결정하는 단계;일정한 시간간격마다 상기 복수의 컴퓨팅 노드 각각에서 실행중인 시뮬레이터들의 실시간계수 합산치를 기록하는 단계;상기 복수의 컴퓨팅 노드 각각에서 에피소드 종료시마다 실행중이거나 실행이 종료되는 시뮬레이터들의 상기 실시간계수 합산치의 평균치를 계산하는 단계; 및상기 평균치가 상기 임계치보다 큰 지 판단하는 단계;를 포함하는 병렬 강화 학습 방법."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "청구항 9에 있어서, 상기 속도 지표가 상기 기준치보다 낮은지 판단하는 단계가일정한 시간간격마다 상기 복수의 컴퓨팅 노드 각각에서 실행중인 시뮬레이터들의 실시간계수 합산치를 기록하고, 상기 합산치의 이동평균을 계산하는 단계;상기 복수의 컴퓨팅 노드 각각에서 에피소드 종료시마다 실행중이거나 실행이 종료되는 시뮬레이터들의 상기 실시간계수 합산치의 평균치를 계산하는 단계; 및상기 평균치가 상기 이동평균보다 큰 지 판단하는 단계;를 포함하는 병렬 강화 학습 방법."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "청구항 11에 있어서, 상기 평균치가 상기 이동평균보다 큰 지 판단하는 단계가해당 컴퓨팅 노드에서 프로세서의 부하가 일정 기준보다 큰 지를 판단하는 단계;를 더 포함하는 병렬 강화 학습 방법."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "청구항 8에 있어서,상기 복수의 컴퓨팅 노드들 각각에서의 각 시뮬레이터의 상태와 상기 시뮬레이터가 형성하는 시뮬레이션 환경에서 학습을 행한 학습 모듈의 경험을 받아들이고, 상기 시뮬레이터 상태와 상기 학습 모듈 경험을 맵핑시킨 형태의 학습 상태를 저장 장치에 저장하는 단계;다수의 학습 상태에 대하여 우선순위를 결정하는 단계; 및상기 복수의 컴퓨팅 노드 각각에서 에피소드가 종료되어 새로운 시뮬레이터의 동작이 시작되어야 할 때 상기 우선순위가 높은 학습 상태가 실행되게 하는 단계;를 더 포함하는 병렬 강화 학습 방법."}
{"patent_id": "10-2020-0163822", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "청구항 13에 있어서, 상기 우선순위를 결정함에 있어서, 상기 복수의 학습모듈들이 잘 못하거나 낯선 학습 상태에 대하여 높은 우선순위를 부여하는 병렬 강화 학습 방법.공개특허 10-2022-0075579-5-"}
{"patent_id": "10-2020-0163822", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "분산 컴퓨팅 환경에서 효율적으로 시뮬레이터 기반 강화 학습을 실행할 수 있는 분산형 강화 학습 시스템과, 다 수의 컴퓨팅 노드에서 병렬로 강화 학습이 이루어지도록 하는 강화 학습 방법을 제공한다. 강화 학습 시스템은 복수의 컴퓨팅 노드들과 학습 관리 서버를 구비한다. 복수의 컴퓨팅 노드들은 각각이 복수의 시뮬레이터들과 복 (뒷면에 계속)"}
{"patent_id": "10-2020-0163822", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 강화 학습 시스템 및 방법에 관한 것으로서, 보다 상세하게는, 분산 컴퓨팅 환경에서 구현되고 시뮬 레이터를 기반으로 하는 강화 학습 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2020-0163822", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "하드웨어 소형화, 성능 향상, 및 가격 하락은 로봇 하드웨어 측면을, 인공 지능 기술의 발전은 로봇 소프트웨어 측면에서의 발전을 주도하며, 최근 들어 로봇 기술은 급속히 발전하고 있다. 그 결과 자율 주행 자동차, 자율 제조 로봇 팔, 자율 드론 배송, 그리고 로봇 바리스타에 이르기까지, 다양한 종류의 지능형 로봇들이 인간의 삶 에 곳곳에 녹아들고 있다. 지능형 로봇 기술의 발전에 따라 재난 현장 탐사 등 사람이 접근하기 어려운 환경에 로봇을 투입하여 요구조자 를 탐색하고 사고 원인을 분석하는 등의 임무를 수행하려는 시도가 많이 이루어지고 있다. 재난 현장에서는 로 봇이 화재, 연기, 열, 유리, 장애물, 낙하물 등으로 인해 예측 불가능한 다양한 상황에 마주칠 수 있으며, 통신 환경이 불안정하고 로봇들을 통제할 중앙집중적 요소가 없는 경우가 많다. 그렇기에 로봇은 각각이 지능을 갖고 자율적으로 동작 가능해야 한다. 즉 지능형 로봇이 센서, 통신 모듈 등을 통해 주변 정보를 수집하고, 수집된 정보에 따라 최적 행동을 선택할 수 있어야 한다. 최근 들어 이러한 임무 수행을 위한 지능을 강화 학습을 통해 학습시키려는 시도가 많이 이루어지고 있다. 강화 학습의 경우 에이전트 의 지능을 학습시키기 위해서 많은 경험을 필요로 하기 때문에, 분산 컴퓨팅 환경에서 시뮬레이터를 통해서 에 이전트를 학습시키는 것이 필요할 수 있다."}
{"patent_id": "10-2020-0163822", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 분산 컴퓨팅 환경에서 효율적으로 시뮬레이터 기반 강화 학습을 실행할 수 있는 분산형 강화 학습 시 스템을 제공한다, 또한 본 발명은 다수의 컴퓨팅 노드를 구비하고 시뮬레이터를 기반으로 하는 분산형 강화 학습 시스템에서 상기 다수의 컴퓨팅 노드에서 병렬로 강화 학습이 이루어지도록 하는 강화 학습 방법을 제공한다."}
{"patent_id": "10-2020-0163822", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 강화 학습 시스템은 복수의 컴퓨팅 노드들과 학습 관리 서버를 구비한다. 복수의 컴퓨팅 노드들은 각각이 복수의 시뮬레이터들과 복수의 학습모듈들을 적재하고 실행할 수 있으며, 상기 복수의 시뮬레이터들 중 적어도 하나와 상기 복수의 학습모듈들 중 적어도 하나를 실행하여, 상기 적어도 하나의 시뮬 레이터가 조성하는 환경에서 상기 적어도 하나의 학습모듈이 경험을 쌓으면서 지능을 향상시키도록 한다. 상기 학습 관리 서버는 상기 복수의 컴퓨팅 노드들 각각의 상태 정보를 수집하고 수집된 상태 정보를 토대로 상기 복 수의 컴퓨팅 노드들 각각에서 실행될 상기 적어도 하나의 시뮬레이터와 상기 적어도 하나의 학습모듈을 배치한 다. 상기 관리 서버는 프로세서와; 상기 프로세서에 의해 실행되는 프로그램 명령어들을 저장하는 메모리;를 포함한다. 상기 프로그램 명령어들은 상기 프로세서에 의해 실행되었을 때: 상기 복수의 컴퓨팅 노드들 각각에 대하여 시뮬레이션 실행 부하를 토대로 상기 복수의 시뮬레이터들 중 실행되는 시뮬레이터의 숫자를 조절하는 동작;을 수행하는 명령을 포함한다. 상기 실행되는 시뮬레이터의 숫자를 조절하는 동작을 수행하는 명령은 상기 복수의 컴퓨팅 노드 각각에서 에피 소드가 종료될 때마다 소정의 속도 지표가 일정한 기준치보다 낮은지 판단하는 동작; 및 상기 속도 지표가 상기 기준치보다 낮은 경우 종료된 에피소드에 연관된 시뮬레이터를 종료하고 새로운 시뮬레이터의 동작을 시작하지 않으며, 상기 속도 지표가 상기 기준치보다 낮지 않은 경우에만 새로운 시뮬레이터의 동작이 시작되게 하는 동 작;을 수행하는 명령을 포함할 수 있다. 상기 속도 지표가 상기 기준치보다 낮은지 판단하는 동작을 수행하는 명령은 상기 기준치로서 상기 시뮬레이션 부하에 대한 임계치를 결정하는 동작; 일정한 시간간격마다 상기 복수의 컴퓨팅 노드 각각에서 실행중인 시뮬레 이터들의 실시간계수 합산치를 기록하는 동작; 상기 복수의 컴퓨팅 노드 각각에서 에피소드 종료시마다 실행중이거나 실행이 종료되는 시뮬레이터들의 상기 실시간계수 합산치의 평균치를 계산하는 동작; 및 상기 평균치가 상기 임계치보다 큰 지 판단하는 동작;을 수행하는 명령을 포함할 수 있다. 상기 속도 지표가 상기 기준치보다 낮은지 판단하는 동작을 수행하는 명령은 일정한 시간간격마다 상기 복수의 컴퓨팅 노드 각각에서 실행중인 시뮬레이터들의 실시간계수 합산치를 기록하고, 상기 합산치의 이동평균을 계산 하는 동작; 상기 복수의 컴퓨팅 노드 각각에서 에피소드 종료시마다 실행중이거나 실행이 종료되는 시뮬레이터 들의 상기 실시간계수 합산치의 평균치를 계산하는 동작; 및 상기 평균치가 상기 이동평균보다 큰 지 판단하는 동작;을 수행하는 명령을 포함할 수 있다. 상기 평균치가 상기 이동평균보다 큰 지 판단하는 동작을 수행하는 명령은 해당 컴퓨팅 노드의 프로세서 부하가 일정 기준보다 큰 지를 추가적으로 판단하는 동작;을 수행하는 명령을 포함할 수 있다. 상기 프로그램 명령어들은 상기 프로세서에 의해 실행되었을 때: 상기 복수의 컴퓨팅 노드들 각각에서의 각 시 뮬레이터의 상태와 상기 시뮬레이터가 형성하는 시뮬레이션 환경에서 학습을 행한 학습 모듈의 경험을 받아들이 고, 상기 시뮬레이터 상태와 상기 학습 모듈 경험을 맵핑시킨 형태의 학습 상태를 저장 장치에 저장하는 동작; 다수의 학습 상태에 대하여 우선순위를 결정하는 동작; 및 상기 복수의 컴퓨팅 노드 각각에서 에피소드가 종료 되어 새로운 시뮬레이터의 동작이 시작되어야 할 때 상기 우선순위가 높은 학습 상태가 실행되게 하는 동작;을 수행하는 명령을 포함할 수 있다. 상기 우선순위를 결정하는 동작을 수행하는 명령은 상기 프로세서에 의해 실행되었을 때 상기 복수의 학습모듈 들이 잘 못하거나 낯선 학습 상태에 대하여 높은 우선순위를 부여하는 동작;을 수행하는 명령을 포함할 수 있다. 본 발명의 일 실시예에 따른 병렬 강화 학습 방법은 각각이 복수의 시뮬레이터들과 복수의 학습모듈들을 적재하 고 실행할 수 있는 복수의 컴퓨팅 노드들을 구비하는 강화 학습 시스템에서의 실행될 수 있다. 상기 병렬 강화 학습 방법은 상기 복수의 컴퓨팅 노드들 각각에서 상기 복수의 시뮬레이터들 중 적어도 하나와 상기 복수의 학 습모듈들 중 적어도 하나가 실행되게 하여, 상기 적어도 하나의 시뮬레이터가 조성하는 환경에서 상기 적어도 하나의 학습모듈이 경험을 쌓으면서 지능을 향상시키도록 하는 단계; 및 상기 복수의 컴퓨팅 노드 각각에서 에 피소드가 종료될 때마다 상기 복수의 컴퓨팅 노드들 각각에 대하여 시뮬레이션 실행 부하를 토대로 상기 복수의 시뮬레이터들 중 실행되는 시뮬레이터의 숫자를 조절하는 단계;를 포함한다. 시뮬레이터 숫자를 조절하는 단계는 상기 복수의 컴퓨팅 노드 각각에서 에피소드가 종료될 때마다 소정의 속도 지표가 일정한 기준치보다 낮은지 판단하는 단계; 및 상기 속도 지표가 상기 기준치보다 낮은 경우 종료된 에피 소드에 연관된 시뮬레이터를 종료하고 새로운 시뮬레이터의 동작을 시작하지 않으며, 상기 속도 지표가 상기 기 준치보다 낮지 않은 경우에만 새로운 시뮬레이터의 동작이 시작되게 하는 단계;를 포함할 수 있다. 상기 속도 지표가 상기 기준치보다 낮은지 판단하는 단계는 상기 기준치로서 상기 시뮬레이션 부하에 대한 임계 치를 결정하는 단계; 일정한 시간간격마다 상기 복수의 컴퓨팅 노드 각각에서 실행중인 시뮬레이터들의 실시간 계수 합산치를 기록하는 단계; 상기 복수의 컴퓨팅 노드 각각에서 에피소드 종료시마다 실행중이거나 실행이 종 료되는 시뮬레이터들의 상기 실시간계수 합산치의 평균치를 계산하는 단계; 및 상기 평균치가 상기 임계치보다 큰 지 판단하는 단계;를 포함할 수 있다. 상기 속도 지표가 상기 기준치보다 낮은지 판단하는 단계는 일정한 시간간격마다 상기 복수의 컴퓨팅 노드 각각 에서 실행중인 시뮬레이터들의 실시간계수 합산치를 기록하고, 상기 합산치의 이동평균을 계산하는 단계; 상기 복수의 컴퓨팅 노드 각각에서 에피소드 종료시마다 실행중이거나 실행이 종료되는 시뮬레이터들의 상기 실시간 계수 합산치의 평균치를 계산하는 단계; 및 상기 평균치가 상기 이동평균보다 큰 지 판단하는 단계;를 포함할 수 있다. 상기 평균치가 상기 이동평균보다 큰 지 판단하는 단계는 해당 컴퓨팅 노드에서 프로세서의 부하가 일정 기준보 다 큰 지를 판단하는 단계;를 더 포함할 수 있다. 한편, 상기 병렬 강화 학습 방법은 상기 복수의 컴퓨팅 노드들 각각에서의 각 시뮬레이터의 상태와 상기 시뮬레 이터가 형성하는 시뮬레이션 환경에서 학습을 행한 학습 모듈의 경험을 받아들이고, 상기 시뮬레이터 상태와 상 기 학습 모듈 경험을 맵핑시킨 형태의 학습 상태를 저장 장치에 저장하는 단계; 다수의 학습 상태에 대하여 우 선순위를 결정하는 단계; 및 상기 복수의 컴퓨팅 노드 각각에서 에피소드가 종료되어 새로운 시뮬레이터의 동작 이 시작되어야 할 때 상기 우선순위가 높은 학습 상태가 실행되게 하는 단계;를 더 포함할 수 있다.상기 우선순위를 결정함에 있어서는, 상기 복수의 학습모듈들이 잘 못하거나 낯선 학습 상태에 대하여 높은 우 선순위를 부여할 수 있다."}
{"patent_id": "10-2020-0163822", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 분산 컴퓨팅 환경에서 각 컴퓨팅 노드의 시뮬레이션 실행 부하에 따라 시뮬레이터 및 학습모 듈의 배치를 실시간으로 최적화함으로써, 시뮬레이터의 시뮬레이션 실행 속도와 및 학습모듈의 학습 속도를 향 상시킬 수 있게 된다. 특히, 학습에 효과적인 학습 상태를 판단하고 이를 우선순위 결정하여, 각 에피소드가 종료될 때마다 우선순위 가 높은 시뮬레이터 및 학습모듈이 실행되도록 함으로써, 학습 에이전트가 효율적으로 효과적인 경험을 수집할 수 있게 하며, 이를 통해 강화 학습 속도 및 성능이 더욱 향상시키게 된다. 본 발명에 따른 강화 학습 시스템은 시뮬레이터 및 학습모듈을 크게 수정하지 않고서 강화 학습 관리 서버를 보 강하여 구축할 수 있기 때문에, 범용성이 높다는 이점이 있다."}
{"patent_id": "10-2020-0163822", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 각 도면을 설명하면서 유사한 참조부호를 유사한 구성요소에 대해 사용하였다. 제1, 제2, 등의 용어는 다양한 구성요소들을 설명하는 데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용 된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. “및/또는”이라는 용어는 복수의 관련된 기재된 항 목들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 본 출원에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조 합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의 미를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다. 본 발명을 설명함에 있어 전체적인 이해를 용이하게 하기 위하여 도면상의 동일한 구성요소에 대해서는 동일한 참조부호를 사용하고 동일한 구성요소에 대해서 중복된 설명은 생략한다. 도 1은 본 발명의 일 실시예에 따른 분산형 강화 학습 시스템의 구성을 보여준다. 도 1을 참고하면, 본 발명의 일 실시예에 따른 분산형 강화 학습 시스템은 복수의 컴퓨팅 노드들(10a~10p)와 강 화 학습 관리 서버를 구비한다. 상기 복수의 컴퓨팅 노드들(10a~10p)과 강화 학습 관리 서버은 네트 웍을 통하여 접속되어 분산 컴퓨팅 환경을 구성하게 된다. 상기 네트웍은 인터넷일 수 있지만, 본 발명이 이에 한정되는 것은 아니며 상기 네트웍은 근거리통신망일 수 있고, 특히 한 건물 내부와 같이 지리적으로 극히 한정 된 영역에 설치되는 네트웍일 수도 있다. 복수의 컴퓨팅 노드들(10a~10p) 각각은 하나 이상의 프로세서에 의해 구현되며, 현실의 환경을 시뮬레이션하는 하나 이상의 시뮬레이터와 하나 이상의 에이전트(이하, \"학습모듈\"이라고 칭함)를 실행하면서, 상기 하나 이상 의 학습모듈이 다양한 시뮬레이션 환경에서 많은 경험을 쌓으면서 에이전트 지능을 학습시키게 된다. 각 시뮬 레이터는 에피소드 단위로 시뮬레이션을 수행한다. 에피소드 종료 조건이 충족되면 에피소드가 종료되고, 새로 운 에피소드가 시작될 수 있다. 에피소드 종료 조건에는 에피소드의 시간적 길이가 일정 기준에 도달하였거나 에이전트가 벽에 부딪혀서 추락했거나 에이전트가 고도를 일정 이상 올렸거나 에이전트가 함정에 빠지는 경우 등 시나리오에 따라 다양할 수 있다. 강화 학습 관리 서버는 복수의 컴퓨팅 노드들(10a~10p)에서 효율적으로 학습모듈의 학습이 진행될 수 있도 록 각 컴퓨팅 노드(10a~10p)에서 실행될 시뮬레이터와 에이전트를 결정하고, 각 컴퓨팅 노드(10a~10p)의 학습 결과 내지 상태를 취합한다. 청구범위를 포함하여 이하의 설명에서, 강화 학습 관리 서버가 각 컴퓨팅 노 드들(10a~10p)에서 실행될 시뮬레이터와 에이전트를 결정하는 작용을 \"시뮬레이터와 에이전트를 배치\"한다는 표 현을 사용하기로 한다. 한편, 강화 학습 관리 서버는 각 컴퓨팅 노드(10a~10p)의 상태 정보를 수집하고, 수집된 상태 정보를 토대로 각 컴퓨팅 노드(10a~10p)에 시뮬레이터와 에이전트를 배치하게 된다. 도 1에 도시된 분산형 강화 학습 시스템에서, 복수의 컴퓨팅 노드들(10a~10p)은 적어도 부분적으로 서로 다른 시뮬레이터를 실행할 수 있으며, 이에 따라 분산형 강화 학습 시스템은 다양하고 많은 경험을 병렬로 수집하고 수집된 경험을 바탕으로 에이전트 지능을 학습시킬 수 있다. 분산 컴퓨팅 환경에서 컴퓨팅 노드들(10a~10p)의 성능은 동일하지 않을 수 있으며, 스케쥴링된 작업 혹은 발열 로 인한 스로틀링 등으로 인해 가용 성능이 실시간으로 변할 수 있다. 또한 에피소드 내에서 에이전트가 처한 상태에 따라서 시뮬레이터 렌더링 작업 차이로 인한 시뮬레이터 실행 부하가 변하기 때문에, 컴퓨팅 노드의 성 능 및 부하뿐만 아니라 시뮬레이터의 시뮬레이션 속도 역시 각 컴퓨팅 노드들(10a~10p)의 가용 성능에 영향을 미치게 된다. 따라서 에피소드를 랜덤하게 혹은 고정된 주기로 초기화하면, 에이전트는 학습에 비효율적인 경험을 불필요하게 수행하게 되며 이를 통해 얻은 경험으로 학습이 이루어지게 되어서 학습 속도 및 성능이 저하될 수 있다. 본 발명에 따르면, 강화 학습 관리 서버는 각 컴퓨팅 노드(10a~10p)의 시뮬레이션 실행 부하에 따라 시뮬레이 터 및 학습모듈의 배치를 실시간으로 최적화함으로써, 시뮬레이터의 시뮬레이션 실행 속도와 및 학습모듈의 학 습 속도가 향상되도록 한다. 또한, 강화 학습 관리 서버는 학습에 효과적인 학습 상태를 판단하고 이를 우선순위 결정하여, 각 에피소드가 종료될 때마다 우선순위가 높은 시뮬레이터 및 학습모듈이 실행되도록 함으 로써, 학습 에이전트가 효율적으로 효과적인 경험을 수집할 수 있게 한다. 도 2는 본 발명의 일 실시예에 따른 컴퓨팅 노드(10a~10p)의 블록도이다. 복수의 컴퓨팅 노드들(10a~10p) 각각은 하나 이상의 프로세서에 의해 구현되며, 하나 이상의 시뮬레이터 (30a~30q)와 하나 이상의 에이전트(40a~40r: 이하 \"학습모듈\"이라고 칭함)를 적재하고 실행할 수 있다. 상기 하나 이상의 시뮬레이터(30a~30q)는 현실의 환경을 시뮬레이션하고, 상기 하나 이상의 학습모듈(40a~40r)은 다 양한 시뮬레이션 환경에서 많은 경험을 쌓으면서 학습을 하게 된다. 즉, 각 시뮬레이터(30a~30q)가 시뮬레이션하는 가상환경 내에서, 해당 시뮬레이터에 배치된 학습모듈(40a~40r) 은 내부 연산에 의하여 취할 행동(Action)을 선택하고 시뮬레이터(30a~30q)에 입력하며, 상기 행동에 따라 생긴 상태(State) 변화에 따라 보상(Reward)을 받게 된다. 그리고, 학습모듈(40a~40r)은 보상을 극대화하는 행동을 지속적으로 찾는 경험을 하면서 학습을 이루어나가게 된다. 일 실시예에 있어서, 학습모듈(40a~40r)이 축적하 는 경험들은 추후에 집산되어 행동 결정 모형을 이루게 되고, 로봇에 탑재될 수 있다. 그리고, 시뮬레이터들 (30a~30q)은 이러한 로봇이 실제 환경에서 겪을 수 있는 다양한 가상상황을 시뮬레이션하여 조성하게 된다. 학 습모듈들(40a~40r)이 학습 과정에서 취하는 행동은 공간 내에서의 동작과 이동을 의미할 수 있다. 한편, 도 2에 도시된 바와 같이, 각 컴퓨팅 노드(10a~10p)는 학습모듈(40a~40r)이 학습 과정에서 선택하는 행동 과, 이에 따른 상태 변화와, 학습모듈(40a~40r)에게 부여되는 보상 등 시뮬레이터(30a~30q)와 학습모듈 (40a~40r)의 동작 관련 정보를 저장하는 데이터베이스를 추가적으로 구비할 수 있다. 데이터베이스는 시뮬레이터/학습모듈 배치 테이블, 학습 진행 테이블, 및 학습 결과 테이블을 포함할 수 있다. 시뮬레이터/학습모듈 배치 테이블은 강화 학습 관리 서버의 명령에 따라 학습에 투입된 시뮬레이터 (30a~30q)와 학습모듈(40a~40r)에 대한 정보를 저장한다. 도 3에 도시된 바와 같이, 시뮬레이터/학습모듈 배치 테이블은 각 시뮬레이터(30a~30q)와 각 학습모듈(40a~40r)이 학습에 배치 또는 해제된 일시에 대한 데이터 를 저장한다. 도 4에 도시된 바와 같이, 학습 진행 테이블은 각 시뮬레이터(30a~30q)가 조성하는 시뮬레이션 상황 별로 해당 시뮬레이터에 배치된 학습모듈(40a~40r)이 취하는 행동과, 이에 따른 상태 변화, 상기 행동에 대하여 부여 된 보상, 시뮬레이터(30a~30q)의 속도를 나타내는 속도 지표 등의 정보를 저장한다. 상기 시뮬레이터 속도 지 표는 초당 프레임(FPS: frame per second)이나 실시간계수(real time factor)가 될 수 있는데, 이에 한정되는 것은 아니다. 학습 결과 테이블은 시뮬레이터(30a~30q)가 조성하는 시뮬레이션 상황에서 학습모듈들(40a~40r)이 얻게 되 는 경험 데이터를 축적한다. 그밖에, 데이터베이스는 시뮬레이터(30a~30q) 각각의 상태와 각 학습모듈(40a~40r)이 축적하는 경험을 별도 의 테이블로 저장할 수 있다. 도 5는 본 발명의 일 실시예에 따른 강화 학습 관리 서버의 기능적인 블록도이다. 강화 학습 관리 서버는 학습 시스템 관리 서브시스템과, 학습 상태 설정 서브시스템와, 학습 상 태 DB을 구비할 수 있다. 그리고, 강화 학습 관리 서버는 컴퓨팅 노드들(10a~10p)에 인터페이스하기 위한 시뮬레이터 연동 인터페이스와 학습모듈 연동 인터페이스를 더 구비할 수 있다. 상기 시뮬레이 터 연동 인터페이스와 학습모듈 연동 인터페이스는 하나의 인터페이스로 통합될 수도 있다. 학습 시스템 관리 서브시스템은 실시간으로 각 컴퓨팅 노드(10a~10p)에서의 시뮬레이터들(30a~30q) 및 학 습모듈들(40a~40r)의 배치를 최적화하기 위해서 각 컴퓨팅 노드(10a~10p)를 모니터링하고 관리한다. 즉, 학습 시스템 관리 서브시스템은 각 컴퓨팅 노드(10a~10p)에서 과도하게 많은 시뮬레이터들(30a~30q)이 실행되지 않도록 함으로써, 해당 컴퓨팅 노드에서의 시뮬레이션 실행 부하가 과다해지지 않도록 하게 된다. 학습 시스템 관리 서브시스템은 학습 시스템 모니터와 시뮬레이터/학습모듈 관리 유닛을 포함한 다. 학습 시스템 모니터는 실시간으로 각 시뮬레이터(30a~30q)의 속도와, 시뮬레이터가 수행되고 있는 각 컴퓨팅 노드(10a~10p)의 상태 및 부하를 모니터링하고 이에 대한 이력을 관리한다. 각 컴퓨팅 노드(10a~10p)의 상태는 해당 컴퓨팅 노드(10a~10p)의 하드웨어 상태를 측정할 수 있는 하나 이상의 지표, 예컨대 중앙처리장치 (CPU), 그래픽처리장치(GPU), 메모리, 저장 장치 등의 사양 및 부하 등으로 측정될 수 있다. 시뮬레이터/학습 모듈 관리 유닛은 학습 시스템에 대한 모니터링 정보에 기반하여 각 컴퓨팅 노드(10a~10p)에서의 시뮬레이 터들(30a~30q) 및 학습모듈들(40a~40r)의 배치를 관리한다. 학습 상태 설정 서브시스템은 각 컴퓨팅 노드(10a~10p)에서 각 시뮬레이터(30a~30q)의 에피소드가 종료될 때마다 해당 시뮬레이터(30a~30q)에서의 학습 상태를 초기화한다. 본 명세서에서, \"학습 상태“란 시뮬레이터의 상태와, 해당 시뮬레이터가 조성하는 가상환경에서 학습한 학습모듈의 경험을 조합한 시뮬레이터-학습모델 상태 조합을 의미한다. \"학습 상태 초기화“란 새로이 실행시킬 시뮬레이터와, 해당 시뮬레이터가 조성하는 가 상환경에서 학습시킬 학습모듈을 결정하여, 결정된 시뮬레이터에 대하여 결정된 학습모듈이 학습을 개시하게 하 는 것을 말한다. 학습 상태 설정 서브시스템은 각 학습 상태 즉, 각 시뮬레이터-학습모델 조합에 대한 우 선순위를 계산하고, 각 시뮬레이터(30a~30q)에서 학습이 초기화될 때마다 학습 효율 및 효과를 최대화할 수 있 는 학습 상태를 선택하여 선택된 학습 상태로 초기화가 되도록 한다. 학습 상태 설정 서브시스템은 학습 상태 모니터, 학습 상태 저장 유닛, 학습 상태 우선순위 결 정 유닛, 학습 상태 선택 유닛, 및 학습 상태 복원 유닛을 포함할 수 있다. 학습 상태 모니터는 시뮬레이터 연동 인터페이스를 통해서 시뮬레이터들(30a~30q)의 시뮬레이션 상태 를 획득하고 모니터링한다. 또한, 학습 상태 모니터는 학습모듈 연동 인터페이스를 통해서 학습모듈 들(40a~40r)의 에이전트 경험을 획득하고 모니터링한다. 학습 상태 모니터는 모니터링을 통해 획득한 시 뮬레이터들(30a~30q)의 시뮬레이션 상태에 관한 데이터, 그리고 학습모듈들(40a~40r)의 에이전트 경험에 관한 데이터를 학습 상태 저장 유닛에 전달하여 학습 상태 DB에 저장되도록 한다. 학습 상태 모니터(12 2)가 모니터링하면서 획득하는 시뮬레이터 상태 데이터와 에이전트 경험 데이터는 학습 상태 복원 내지 초기화 에 필요한 정보를 포함한다. 시뮬레이터 상태의 예로는 world 이름, model 이름, model 상태 등이 있으며, 에 이전트 경험의 예로는 에이전트 관측, 행동, 보상 등이 있다, 또한, 학습 상태 모니터는 각 시뮬레이터(30a~30q)에서의 에피소드 종료 시에, 학습 상태 모니터는 에피소드 종료를 시뮬레이터/학습모듈 관리 유닛을 통해 학습 시스템 모니터에 알리고, 학습 상태 선 택 유닛에도 알린다. 한편, 학습 상태 모니터는 학습 상태 복원 유닛으로부터 학습 상태 복원에 필요한 정보를 받아서 시 뮬레이터 연동 인터페이스와 학습모듈 연동 인터페이스를 통해서 해당 컴퓨팅 노드(10a~10p)의 시뮬 레이터(30a~30q)와 학습모듈(40a~40r)로 보낸다. 학습 상태 저장 유닛은 학습 상태 모니터로부터 시뮬레이터 상태 데이터와 에이전트 경험 데이터를 받아서 학습 상태 DB에 저장한다. 이때, 학습 상태 저장 유닛은 학습 상태 모니터로부터 수신 되는 다수의 시뮬레이터 및 학습모듈 데이터들을 분류하고, 시뮬레이터 상태 데이터와 에이전트 경험 데이터를 맵핑시켜 조합한 형태로 학습 상태 DB에 저장하게 된다. 학습 상태 우선순위 결정 유닛은 학습 상태 DB에 저장되어 있는 학습 상태들에 대해서 주기적 또는 비주기적으로 우선순위를 계산하고, 계산된 우선순위 정보를 다시 학습 상태 DB에 기록한다. 여기서, 학 습 상태 우선순위 결정 유닛이 학습 상태의 우선순위를 계산하는 목적은 에이전트 즉, 학습모듈들 (40a~40r)이 잘 못하는 상태 그리고 학습모듈들(40a~40r)에게 낯선 상태에 한 우선순위를 높임으로써, 학습모듈 들(40a~40r)이 우선순위가 높은 상태에 대하여 집중적으로 많은 경험을 쌓도록 하여 학습 효율 및 효과를 높이 기 위한 것이다. 학습 상태 선택 유닛은 학습 상태 모니터로부터 에피소드 종료 통지와 함께 학습 상태 선택 요청을 받아들일 수 있다. 학습 상태 선택 유닛은 학습 상태 선택 요청에 응답하여 학습 상태 DB에서 학습 상태들의 우선순위를 확인하고, 상기 우선순위에 기반하여, 에피스드가 종료된 컴퓨팅 노드에서 재개할 학습 상 태를 선택할 수 있다. 학습 상태 선택 유닛은 선택된 학습 상태에 대한 정보를 학습 상태 복원 유닛(13 0)에 전달한다. 학습 상태 복원 유닛은 학습 상태 선택 유닛으로부터 수신한 학습 상태 정보를 바탕으로 시뮬레이터 연동 인터페이스와 학습모듈 연동 인터페이스를 통해서 해당 컴퓨팅 노드에 전달하여, 해당 컴퓨팅 노드에서 컴퓨팅 파워 중 일부가 선택된 학습 상태로 초기화되어 학습이 진행하도록 하고, 초기화 사실을 학습 상태 모니터에 알린다. 도 6은 본 발명의 일 실시예에 따른 강화 학습 관리 서버의 물리적인 블록도이다. 도 6을 참조하면, 본 발명의 일 실시예에 따른 강화 학습 관리 서버는 적어도 하나의 프로세서, 메 모리, 및 저장 장치를 포함할 수 있다. 프로세서는 메모리 및/또는 저장 장치에 저장된 프로그램 명령을 실행할 수 있다. 프로세서 는 적어도 하나의 중앙 처리 장치(central processing unit, CPU)나 그래픽 처리 장치(graphicsprocessing unit, GPU)에 의해 구현될 수 있으며, 그밖에 본 발명에 따른 방법을 수행할 수 있는 여타의 프로세 서일 수 있다. 메모리는 예컨대 ROM(Read Only Memory)와 같은 휘발성 메모리와, RAM(Random Access Memory)과 같은 비 휘발성 메모리를 포함할 수 있다. 메모리는 저장 장치에 저장된 프로그램 명령을 로드하여, 프로 세서에 제공할 수 있다. 저장 장치는 프로그램 명령과 데이터를 저장하기에 적합한 기록매체로서, 예컨대 하드 디스크, 플로피 디 스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 플래시 메모리나 EPROM(Erasable Programmable ROM) 또는 이들을 기반으로 제작되는 SSE와 같은 반도체 메모리를 포함한다. 저장 장치는 상기 프로그램 명령을 저장한다. 특히, 상기 프로그램 명령은 도 5에 도시된 학습 상태 설 정 서브 시스템, 학습 상태 설정 서브 시스템, 학습 상태 DB, 시뮬레이터 연동 인터페이스 , 및 학습모듈 연동 인터페이스를 구현하는데 필요한 프로그램 명령과 데이터베이스를 구현하기 위한 명령어를 포함한다. 이와 같은 프로그램 명령은 프로세서의 제어에 의해 메모리에 로드된 상태에 서, 프로세서에 의해 실행되어 본 발명에 의한 방법을 구현할 수 있다. 상기 강화 학습 시스템은 다음과 같이 동작한다. 도 7은 본 발명의 일 실시예에 따른 분산형 강화 학습 시스템의 전체적인 동작을 보여주는 흐름도이다. 강화 학습 관리 서버의 관리 하에 각 컴퓨팅 노드(10a~10p)에서는 복수의 시뮬레이터들(30a~30q)이 실행되 고, 시뮬레이터들(30a~30q)이 조성하는 가상환경에서 학습모듈(40a~40r)의 학습이 진행될 수 있다(제200단계). 학습 진행 과정에서, 학습 시스템 관리 서브시스템의 학습 시스템 모니터는 실시간으로 각 시뮬레이 터(30a~30q)의 속도와 시뮬레이터(30a~30q)가 수행되고 있는 각 컴퓨팅 노드(10a~10p)의 상태를 모니터링하고 그 이력을 관리한다. 학습 시스템 모니터는 일정한 시간간격마다 컴퓨팅 노드들(10a~10p) 각각에서 실행 중인 시뮬레이터들 (30a~30q)의 실행 속도를 측정하여 저장한다. 여기서, 측정되어 저장되는 시뮬레이션 속도는 실시간계수(real time factor)가 될 수 있는데, 본 발명이 이에 한정되는 것은 아니며, 초당 프레임(FPS: frame per second) 등 다른 지표가 사용될 수도 있다. 일 실시예에 있어서, 학습 시스템 모니터는 실시간계수를 저장함에 있어 서, 모든 시뮬레이터들(30a~30q)에 대한 실시간계수의 합산치를 저장할 수 있다(제210단계). 한편, 학습 상태 설정 서브시스템의 학습 상태 모니터는 학습 상태 데이터를 학습 상태 저장 유닛 을 통해서 학습 상태 DB에 저장하고, 학습 상태 우선순위 결정 유닛은 학습 상태 DB에 저 장되어 있는 학습 상태들에 대해서 주기적 또는 비주기적으로 우선순위를 계산한다(제220단계). 이와 같이 실시간계수 합산치 저장과 우선순위 갱신을 수반하여 학습이 진행되는 과정에서 어느 한 컴퓨팅 노드 (10a~10p)에서 에피소드가 종료되면(제230단계), 학습 시스템 관리 서브시스템의 학습 시스템 모니터(11 2)는 시뮬레이터 실행 부하를 점검하여 에피소드가 종료된 시뮬레이터의 시뮬레이션 종료 여부를 결정한다(제 240단계). 제250단계에서 에피소드가 종료된 시뮬레이터의 시뮬레이션을 종료하기로 결정되면, 시뮬레이터/학 습모듈 관리 유닛에 의해 해당 시뮬레이터가 종료된다(제260단계). 이때, 여타 시뮬레이터들은 계속 동작 을 하게 된다. 한편, 제250단계에서 에피소드가 종료된 시뮬레이터의 동작을 재개하기로 결정되면, 학습 상태 설정 서브시스템 의 학습 상태 선택 유닛이 학습 상태 DB에 저장되어 있는 학습 상태들 중에서 우선순위가 높은 학습 상태를 선택하게 되고, 학습 상태 복원 유닛에 의해 선택된 학습상태로 에피소드가 종료된 시뮬레이 터가 점유하던 컴퓨팅 파워를 초기화하여 학습을 복원하게 된다(270단계). 도 8은 학습 시스템 관리 서브시스템이 수행하는 컴퓨팅 노드 관리 프로세스의 일 예를 구체적으로 보여준 다. 학습 시스템 관리 서브시스템의 학습 시스템 모니터는 시뮬레이터 실행 부하의 적정성을 판정하기 위 한 임계치를 결정한다(제300단계). 상기 임계치는 시뮬레이터들(30a~30q)과 학습모듈들(40a~40r)의 응용분야, 학습모듈들(40a~40r)의 학습 완성도, 시뮬레이터들(30a~30q)이 조성하는 가상환경의 복잡성, 학습 목표 일정 등에 따라 달라질 수 있다. 임계치는 관리자에 의해 일률적으로 정해질 수도 있고, 학습 시스템 관리 서브시스템 내에서 실행되는 프로그램에 의해 일정한 규칙에 따라 정해질 수도 있으며, 주기적 또는 비주기적으로 조 정될 수도 있다. 특히, 분산형 강화 학습 시스템 내에 있는 컴퓨팅 노드들(10a~10p)의 전체적인 시뮬레이션 실 행 부하에 따라 가변될 수도 있다. 학습 시스템 모니터는 일정한 시간간격(Tm)마다 컴퓨팅 노드들(10a~10p) 각각에서 실행 중인 시뮬레이터들 (30a~30q)의 실시간계수의 합산치를 기록한다(제310단계). 도 8에는 실시간계수 합산치을 기록하는 제310단계 가 제300단계의 임계치 결정 후에 실행되는 것으로 도시되어 있지만, 두 단계의 순서는 뒤바뀔 수도 있고 두 단 계가 동시에 이루어질 수도 있다. 학습 시스템 모니터는 각 컴퓨팅 노드(10a~10p)에서 각 시뮬레이터(30a~30q)의 에피소드 종료시마다, 시뮬 레이터들(30a~30q)의 실시간계수 합산치의 평균치를 계산한다(제320단계). 이어서, 학습 시스템 모니터 또는 시뮬레이터/학습모듈 관리 유닛은 시뮬레이터들(30a~30q)의 실시간 계수 합산치의 평균치를 상기 임계치와 비교한다(제330단계). 만약 시뮬레이터들(30a~30q)의 실시간계수 합산치의 평균치가 상기 임계치보다 크다면, 시뮬레이터/학습모듈 관 리 유닛은 상기 에피소드가 종료된 시뮬레이터를 종료한다(제380단계). 이는 컴퓨팅 노드의 자원에 비해 시뮬레이터가 과하게 실행되고 있어서 비효율적이라고 볼 수 있기 때문이다. 한편, 해당 컴퓨팅 노드에서 전체 시뮬레이터들의 실시간계수 합산치의 평균치가 상기 임계치보다 크지 않다면, 시뮬레이터/학습모듈 관리 유닛는 해당 컴퓨팅 노드에서의 컴퓨팅 파워가 시뮬레이션 실행 부하를 감당할 수 있는 범위에 있는 것으로 판단하고, 에피소드가 새로 시작되도록 한다(제390단계). 이때 새로 시작되는 에 피소드는 종료된 에피소드일 수도 있고, 다른 에피소드일 수도 있다. 예컨대, i-번째 컴퓨팅 노드에서 실행 중인 k-번째 시뮬레이터의 에피소드가 종료되어 동일한 에피소드 또는 다 른 에피소드를 새로 시작해야 하는 시점에, 일정한 시간간격(Te) 동안 i-번째 컴퓨팅 노드에서 실행 중인 전체 시뮬레이터들의 실시간계수 합산치의 평균치를 계산된다. 이 평균치가 i-번째 컴퓨팅 노드에서의 실시간계수 합산치의 이동평균보다 크다면 해당 시뮬레이터를 종료한다. 한편, 실시간계수 합산치의 평균치가 i-번째 컴퓨 팅 노드에서의 실시간계수 합산치의 이동평균보다 크지 않다면, 에피소드가 새로 시작된다. 도 9는 도 8에 도시된 컴퓨팅 노드 관리 프로세스의 변형 예를 보여준다. 본 실시예에 있어서는, 컴퓨팅 노드에서의 시뮬레이션 실행 부하를 판정하기 위한 임계치로써 시뮬레이터들의 실시간계수 합산치의 이동평균이 사용된다. 학습 시스템 모니터는 일정한 시간간격(Tm)마다 컴퓨팅 노드들(10a~10p) 각각에서 실행 중인 시뮬레이터들 (30a~30q)의 실시간계수 합산치를 기록하고(제310단계), 실시간계수 합산치의 이동평균을 계산한다(제340단계). 실시간계수 합산치의 이동평균을 구하는 시뮬레이터들에는 모든 컴퓨팅 노드들(10a~10p)에 있는 전체 시뮬레이 터가 포함될 수 있다. 그렇지만, 변형된 실시예에 있어서는 일부 컴퓨팅 노드들에 있는 전체 시뮬레이터만이 포함될 수도 있다. 학습 시스템 모니터는 각 컴퓨팅 노드(10a~10p)에서 각 시뮬레이터(30a~30q)의 에피소드 종료시마다, 시뮬 레이터들(30a~30q)의 실시간계수 합산치의 평균치를 계산한다(제350단계). 이어서, 학습 시스템 모니터 또는 시뮬레이터/학습모듈 관리 유닛은 시뮬레이터들(30a~30q)의 실시간 계수 합산치의 평균치가 상기 이동평균과 비교한다(제360단계). 만약 시뮬레이터들(30a~30q)의 실시간계수 합산치의 평균치가 상기 이동평균보다 크다면, 시뮬레이터/학습모듈 관리 유닛은 상기 에피소드가 종료된 시뮬레이터를 종료한다(제380단계). 이는 컴퓨팅 노드의 자원에 비 해 시뮬레이터가 과하게 실행되고 있어서 비효율적이라고 볼 수 있기 때문이다. 한편, 해당 컴퓨팅 노드에서 전체 시뮬레이터들의 실시간계수 합산치의 평균치가 상기 이동평균보다 크지 않다 면, 시뮬레이터/학습모듈 관리 유닛는 해당 컴퓨팅 노드에서의 컴퓨팅 파워가 시뮬레이션 실행 부하를 감 당할 수 있는 범위에 있는 것으로 판단하고, 에피소드가 새로 시작되도록 한다(제390단계). 이때 새로 시작되 는 에피소드는 종료된 에피소드일 수도 있고, 다른 에피소드일 수도 있다. 도 9에 도시된 실시예에 따르면 시뮬레이터 실행 부하의 적정성을 판정하기 위한 임계치로써 시뮬레이터들 (30a~30q)의 실시간계수 합산치의 이동평균이 사용되기 때문에, 에피소드가 종료된 시뮬레이터의 종료 여부를판단함에 있어 전체 시뮬레이터들의 평균 부하에 대비한 상대적인 부하를 즉각적으로 반영할 수 있어서 시의적 절한 판단을 할 수 있게 된다. 도 10은 학습 시스템 관리 서브시스템이 수행하는 컴퓨팅 노드 관리 프로세스의 또 다른 예를 보여준다. 본 실시예에 따르면, 시뮬레이터들의 실시간계수 합산치의 평균치 이외에, CPU 부하를 명시적으로 추가 고려하 여 에피소드의 시작 여부를 결정하게 된다. 도 10에 도시된 프로세스에서 제340단계 내지 제360단계는 도 9의 프로세스에서 대응하는 단계들과 각각 유사하며, 이에 대한 자세한 설명은 생략한다. 제330단계에서 시뮬레이터들의 실시간계수의 합산치의 평균치가 상기 이동평균보다 크다고 판단되면, 시뮬레이 터/학습모듈 관리 유닛은 상기 에피소드가 종료된 시뮬레이터가 종료되도록 한다(제380단계). 한편, 해당 컴퓨팅 노드에서 전체 시뮬레이터들의 실시간계수 합산치의 평균치가 이동평균보다 크지 않다면, CPU 부하가 일정 기준보다 높은지 추가적으로 판단된다(제370단계). 만약 CPU 부하가 일정 기준보다 높다면, 시뮬레이터가 과하게 실행되고 있다는 견지에서, 상기 에피소드가 종료된 시뮬레이터가 종료된다(제380단계). 그렇지만, CPU 부하가 일정 기준보다 높지 않다면, 상기 에피소드가 종료된 시뮬레이터에서 에피소드가 새로 시 작된다(제390단계). 즉, 본 실시예에 따르면, 컴퓨팅 노드에서 전체 시뮬레이터들의 실시간계수 합산치의 평균치가 이동평균보다 크 지 않고 CPU 부하가 일정 기준보다 높지 않은 경우에만, 해당 컴퓨팅 노드에서의 컴퓨팅 파워가 시뮬레이션 실 행 부하를 감당할 수 있는 범위에 있는 것으로 판단하고, 에피소드가 새로 시작된다. 도 11 및 도 12을 참조하여, 학습 상태 설정 서브시스템 동작을 설명한다. 도 11은 본 발명의 일 실시예에 따른 학습 상태 우선순위 결정 프로세스를 구체적으로 보여주는 흐름도이다. 학습 상태 모니터는 각 컴퓨팅 노드(10a~10p)에 대하여 시뮬레이터들(30a~30q)의 시뮬레이션 상태와, 학습 모듈들(40a~40r)의 에이전트 경험을 획득하고 모니터링한다(제400단계). 학습 상태 모니터는 획득한 시뮬 레이션 상태와 학습모듈들(40a~40r)의 에이전트 경험을 학습 상태 저장 유닛에 전달하고, 학습 상태 저장 유닛은 시뮬레이션 상태와 에이전트 경험을 맵핑시켜 조합한 형태로 학습 상태 데이터를 형성하고, 학습 상태 데이터를 학습 상태 DB에 저장한다(제410단계). 학습 상태 데이터가 학습 상태 DB에 축적되는 상태에서, 학습 상태 우선순위 결정 유닛은 학습 상태 DB에 저장되어 있는 학습 상태들에 대해서 주기적 또는 비주기적으로 우선순위를 계산하고, 계산된 우선순 위 정보를 다시 학습 상태 DB에 기록한다(제420단계). 우선순위를 계산함에 있어, 학습 상태 우선순위 결 정 유닛는 에이전트 즉, 학습모듈들(40a~40r)이 잘 못하는 상태 그리고 학습모듈들(40a~40r)에게 낯선 상 태에 한 우선순위를 높임으로써, 학습모듈들(40a~40r)이 우선순위가 높은 상태에 대하여 집중적으로 많은 경험 을 쌓도록 하여 학습 효율 및 효과를 높일 수 있게 해준다. 일 실시예에 있어서, 학습 상태 우선순위 결정 유닛은 수학식 1에 따라서 에이전트 경험 정보를 사용하여 학습 상태에 대한 우선순위를 결정한다. 수학식 1"}
{"patent_id": "10-2020-0163822", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서, 'P(training state)'는 특정 학습 상태(training state)에 대한 우선순위를 나타내고, 'Re'는 에이전트 즉, 학습모듈이 해당 에피소드에서 받은 총 보상 값 즉, 해당 에피소드 동안 학습모듈이 받은 보상을 모두 합한 값을 나타낸다. 's'와 's''은 현재 학습모듈 관측값과 다음 학습모듈 관측값을 각각 나타낸다. 'α'와 'β'는 가중치를 나타내는 양수 값이다. 함수 f는 현재 학습모듈 관측값인 s에서 다음 학습모듈 관측값인 s'을 예측하는 함수이다. 이 함수 f는 강화 학습이 진행됨에 따라서 같이 학습되는 신경망으로서, 학습모듈이 많이 경험해본 상태일수록 많이 학습되기 때 문에 s로부터 s'을 잘 예측하게 되고, 많이 경험해보지 않은 상태인 경우에는 예측 오차가 커지게 된다. 즉, 다음 학습모듈 관측값에 대한 실제값 s'과 예측값 f(s)가 유사하면 관측값 s가 에학습모듈에게 익숙하다는 의미 이며, 차이가 크다면 관측값 s가 학습모듈에게 낯설다는 의미이다. 따라서, 어려운 에피소드일수록 그리고 학습모듈에게 낯선 상태일수록 우선순위가 높아지게 된다. 수학식 1에 따르면, 각 학습 상태(training state)에 대한 우선순위 P(training state)는 학습모듈이 해당 에 피소드에서 받은 총 보상(Re) 값과 현재 학습모듈 관측값인 s가 학습모듈에게 얼마나 익숙한지를 지표화한 값의 가중 합이다. 학습 상태 우선순위 결정 유닛은 수학식 1을 사용하여 학습모듈이 잘 못하는 상태 그리고 학습모듈에게 낯선 상태에 대해 우선순위를 높임으로써 해당 학습모듈이 해당 시뮬레이션 상태를 더 많이 경험 하도록 하게 된다. 이에 따라, 각 노드에서의 학습 효율 및 효과를 높일 수 있게 된다. 도 12는 본 발명의 일 실시예에 따른 학습 상태 초기화 프로세스를 구체적으로 보여주는 흐름도이다. 각 컴퓨팅 노드(10a~10p)에서 학습을 진행하면서 학습 상태 정보를 획득하던 중에 에피소드가 종료되면(제450단 계, 제460단계), 학습 상태 설정 서브시스템의 학습 상태 선택 유닛은 학습 상태 모니터로부터 학습 상태 선택 요청을 받아들일 수 있다. 학습 상태 선택 유닛은 학습 상태 선택 요청에 응답하여 학습 상태 DB에서 학습 상태들의 우선순위를 확인하고, 상기 우선순위에 기반하여 학습 상태를 선택할 수 있다 (제470단계). 학습 상태 선택 유닛은 선택된 학습 상태 정보를 학습 상태 복원 유닛에 전달하고, 학 습 상태 복원 유닛은 학습 상태 선택 유닛으로부터 수신한 정보를 바탕으로 시뮬레이터 연동 인터페 이스와 학습모듈 연동 인터페이스를 통해서 해당 컴퓨팅 노드에 전달하여, 해당 컴퓨팅 노드에서 컴 퓨팅 파워 중 일부가 선택된 학습 상태로 초기화되어 학습 진행이 이루어지도록 한다(제480단계). 위에서 언급한 바와 같이 본 발명의 실시예에 따른 동작은 컴퓨터로 읽을 수 있는 기록매체에 컴퓨터가 읽을 수 있는 프로그램 또는 코드로서 구현하는 것이 가능하다. 컴퓨터가 읽을 수 있는 기록매체는 컴퓨터 시스템에 의 해 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 또한 컴퓨터가 읽을 수 있는 기록매체 는 네트워크로 연결된 컴퓨터 시스템에 분산되어 분산 방식으로 컴퓨터로 읽을 수 있는 프로그램 또는 코드가 저장되고 실행될 수 있다. 상기 컴퓨터가 읽을 수 있는 기록매체는 롬(rom), 램(ram), 플래시 메모리(flash memory) 등과 같이 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치를 포함할 수 있다. 프로그램 명령은 컴파일러 (compiler)에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터(interpreter) 등을 사용해서 컴퓨 터에 의해 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 본 발명의 일부 측면들은 장치의 문맥에서 설명되었으나, 그것은 상응하는 방법에 따른 설명 또한 나타낼 수 있 고, 여기서 블록 또는 장치는 방법 단계 또는 방법 단계의 특징에 상응한다. 유사하게, 방법의 문맥에서 설명된 측면들은 또한 상응하는 블록 또는 아이템 또는 상응하는 장치의 특징으로 나타낼 수 있다. 방법 단계들의 몇몇 또는 전부는 예를 들어, 마이크로프로세서, 프로그램 가능한 컴퓨터 또는 전자 회로와 같은 하드웨어 장치에 의 해(또는 이용하여) 수행될 수 있다. 몇몇의 실시예에서, 가장 중요한 방법 단계들의 하나 이상은 이와 같은 장 치에 의해 수행될 수 있다. 실시예들에서, 프로그램 가능한 로직 장치(예를 들어, 필드 프로그래머블 게이트 어레이)가 여기서 설명된 방법 들의 기능의 일부 또는 전부를 수행하기 위해 사용될 수 있다. 실시예들에서, 필드 프로그래머블 게이트 어레이 는 여기서 설명된 방법들 중 하나를 수행하기 위한 마이크로프로세서와 함께 작동할 수 있다. 일반적으로, 방법 들은 어떤 하드웨어 장치에 의해 수행되는 것이 바람직하다. 상기에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특 허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12"}
{"patent_id": "10-2020-0163822", "section": "도면", "subsection": "도면설명", "item": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명의 바람직한 실시예를 설명한다. 도면 중, 도 1은 본 발명의 일 실시예에 따른 분산형 강화 학습 시스템의 개략도; 도 2는 본 발명의 일 실시예에 따른 컴퓨팅 노드의 블록도; 도 3은 컴퓨팅 노드 내에서 유지, 운용되는 시뮬레이터/학습모듈 배치 테이블의 필드 구성을 보여주는 도면; 도 4는 컴퓨팅 노드 내에서 유지, 운용되는 학습 진행 테이블의 필드 구성을 보여주는 도면; 도 5는 본 발명의 일 실시예에 따른 강화 학습 관리 서버의 기능적인 블록도; 도 6은 본 발명의 일 실시예에 따른 강화 학습 관리 서버의 물리적인 블록도; 도 7은 본 발명의 일 실시예에 따른 분산형 강화 학습 시스템의 전체적인 동작을 보여주는 흐름도이다. 도 8은 본 발명의 일 실시예에 따른 컴퓨팅 노드 관리 프로세스의 일 예를 구체적으로 보여주는 흐름도; 도 9는 본 발명의 일 실시예에 따른 컴퓨팅 노드 관리 프로세스의 다른 예를 보여주는 흐름도; 도 10은 본 발명의 일 실시예에 따른 컴퓨팅 노드 관리 프로세스의 또 다른 예를 보여주는 흐름도; 도 11은 본 발명의 일 실시예에 따른 학습 상태 우선순위 결정 프로세스를 구체적으로 보여주는 흐름도; 그리고 도 12는 본 발명의 일 실시예에 따른 학습 상태 초기화 프로세스를 구체적으로 보여주는 흐름도이다."}
