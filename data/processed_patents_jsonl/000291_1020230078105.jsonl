{"patent_id": "10-2023-0078105", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0177087", "출원번호": "10-2023-0078105", "발명의 명칭": "인공지능 모델을 활용하여 전자파 관련 문서의 분류를 수행하는 전자 장치", "출원인": "한림대학교 산학협력단", "발명자": "김의직"}}
{"patent_id": "10-2023-0078105", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,BERT(Bidirectional Encoder Representations from Transformers) 모델 및 CNN(Convolutional NeuralNetwork) 모델을 포함하는 인공지능 모델이 저장된 메모리; 및일 텍스트를 상기 인공지능 모델에 입력하여, 상기 텍스트의 유형을 동물노출실험, 세포노출실험,인체노출실험, 및 역학연구 중 하나로 식별하는 프로세서;를 포함하는, 전자 장치."}
{"patent_id": "10-2023-0078105", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 프로세서는,상기 텍스트를 상기 BERT 모델로 입력하여 hidden state vector를 획득하고,상기 hidden state vector를 상기 CNN 모델에 입력하는, 전자 장치."}
{"patent_id": "10-2023-0078105", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 프로세서는,상기 텍스트를 구성하는 각 토큰에 대하여 토큰 임베딩(Token Embeddings), 세그먼트 임베딩(SegmentEmbeddings), 및 위치 임베딩(Position Embeddings)을 획득하고,각 토큰에 대하여 상기 토큰 임베딩, 상기 세그먼트 임베딩, 및 상기 위치 임베딩을 합산하여 입력 임베딩을 획득하고,상기 입력 임베딩을 상기 BERT 모델로 입력하는, 전자 장치."}
{"patent_id": "10-2023-0078105", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 프로세서는,상기 텍스트로부터 획득된 토큰의 수가 상기 BERT 모델에 대해 기설정된 토큰 임베딩의 수보다 작은 경우, 부족한 토큰 임베딩에 대해서 제로 패딩을 수행하여 상기 BERT 모델에 입력하고,상기 텍스트로부터 획득된 토큰의 수가 상기 BERT 모델에 대해 기설정된 토큰 임베딩의 수보다 큰 경우, 상기기설정된 토큰 임베딩의 수를 초과하는 토큰을 제외하여 상기 BERT 모델에 입력하는, 전자 장치."}
{"patent_id": "10-2023-0078105", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서,상기 CNN 모델은,상기 BERT 모델이 출력하는 hidden state vector를 입력 받는 적어도 하나의 convolutional layer;상기 convolutional layer가 출력하는 feature map를 입력 받는 적어도 하나의 pooling layer; 및상기 pooling layer에 의해 축소된 feature map을 입력 받는 적어도 하나의 fully-connected layer;를 포함하고,공개특허 10-2024-0177087-3-상기 프로세서는,상기 fully-connected layer의 출력을 바탕으로 상기 텍스트의 유형을 식별하는, 전자 장치."}
{"patent_id": "10-2023-0078105", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 프로세서는,상기 BERT 모델이 출력하는 hidden state vector 중 문장의 시작(CLS)에 매칭되는 hidden state vector TCLS를상기 축소된 feature map과 결합하여 결합된 features를 획득하고,상기 결합된 features를 상기 fully-connected layer로 입력하는, 전자 장치."}
{"patent_id": "10-2023-0078105", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서,상기 CNN 모델은,window size가 3인 128개의 컨볼루션 필터로 구성된 제1 세트; 및window size가 4인 128개의 컨볼루션 필터로 구성된 제2 세트;를 포함하는, 전자 장치."}
{"patent_id": "10-2023-0078105", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 프로세서는,상기 제1 세트 및 상기 제2 세트를 바탕으로 256개의 feature map을 획득하고,상기 feqture map에 배치 정규화를 수행하여 상기 pooling layer에 입력하는, 전자 장치."}
{"patent_id": "10-2023-0078105", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "BERT(Bidirectional Encoder Representations from Transformers) 모델 및 CNN(Convolutional NeuralNetwork) 모델을 포함하는 인공지능 모델이 저장된 전자 장치의 제어 방법에 있어서,일 논문을 구성하는 텍스트를 상기 인공지능 모델에 입력하는 단계; 및상기 인공지능 모델의 출력을 바탕으로 상기 텍스트의 유형을 동물노출실험, 세포노출실험, 인체노출실험, 및역학연구 중 하나로 식별하는 단계;를 포함하는, 전자 장치."}
{"patent_id": "10-2023-0078105", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "전자 장치의 프로세서에 의해 실행되어 상기 전자 장치로 하여금 제9항의 제어 방법을 수행하도록 하는, 컴퓨터프로그램이 저장된 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 본 전자 장치는, BERT(Bidirectional Encoder Representations from Transformers) 모델 및 CNN(Convolutional Neural Network) 모델을 포함하는 인공지능 모델이 저장된 메모리, 일 텍스트를 인공지능 모델에 입력하여 텍스트의 유형을 동물노출실험, 세포노출실험, 인체노출실험, 및 역학연구 중 하나로 식별하는 프로세서를 포함한다."}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공지능 모델을 바탕으로 문서의 카테고리를 분류하는 전자 장치에 관한 것으로, 보다 상세하게는, 전자파의 건강 위험 평가와 관련된 연구를 위해 관련 문서를 유형 별로 분류하는 전자 장치에 관한 것이다."}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "전자파 노출이 건강에 미치는 영향에 대한 논쟁은 수십 년간 이어져 오고 있지만 너무나도 다양한 실험 결과와 연구 결과들이 산재되어 있어 주류의 이론을 특정하기 어려운 상황이다. 전자파 노출의 위험성에 논문/기사/리서치 등의 결과는 종종 서로 모순되므로 EMF의 인체 건강 위험을 평가하기 위해서는 숙련된 전문가가 방대한 수의 논문/기사/리서치를 검토해야 한다. 이러한 검토 프로세스는 시간이 많이 걸리고 비용이 많이 든다. 게다가 전문가의 판단이 어떻게 사용되는지 이해관계자들에게 불분명한 경우가 많 다. 전문가에 의한 논문 검토 프로세스의 효율성과 신뢰성을 높이기 위해 많은 연구에서 합성곱 신경망(CNN) 또는 순환 신경망(RNN) 모델을 사용하여 과학 논문 분류를 수행했다. Teodoroet al.은 단백질 관련 기사를 단백질 기 능에 따라 11가지 범주로 분류하는 CNN 모델을 제안했다. 연구에서 저자는 k-최근접 이웃(KNN) 알고리즘을 사용 하여 기사에서 단백질 관련 문장을 추출하고 이 추출된 문장을 모델의 입력으로 사용했다. Hsuet al.은 게놈 관련 기사를 T1 범주(새로운 발견 또는 실험)와 T2-4(기존 연구의 적용 또는 검증)로 분류하 는 CNN 모델을 제안했다. 본 모델은 임베딩 레이어, 컨볼루션 레이어, 풀링 레이어, 완전 연결 레이어로 구성되 며 제목과 초록 텍스트를 입력으로 받는다. 또한 다른 연구에서는 사전 훈련된 BERT를 사용하여 과학 기사 분류를 수행했다. Chenet al. 은 사전 훈련된 BERT와 1차원 CNN 레이어를 결합하여 의료 관계 추출을 수행하는 DL 모델을 제안했다. 연구에서 저자는 세 가지 데이터 세트를 사용하여 제안된 모델을 평가했다. 구체적으로, BioCreative V CDR 작업 말뭉치 데이터 세트를 사용할 때 모델은 생물 의학 논문의 제목과 초록을 수신하고 이를 관련 의료 관계 범주로 분류했다. 유사하게, Lin et al. 은 COVID-19 문헌 주석에서 다중 레이블 주제 분류를 위한 BERT 기반 앙상블 DL 모델을 제안했다. 이 앙상블 모델은 사전 훈련된 4가지 BERT 모델을 사용하여 임베딩을 생성한 다음 결합되어 완전 연 결 계층의 입력으로 사용된다. 이 모델 구조를 이용하여 앙상블 모델은 논문의 제목과 초록을 받아 7개의 주제 로 분류하였다. 그러나 이러한 연구 중 어느 것도 EMF의 인체 건강 위험과 관련된 과학 기사의 분류를 다루지 않았다. Won et al.은 EMF의 인체 건강 위험과 관련된 과학 기사의 분류를 수행했다. 연구에서 저자는 EMF 관련 실험 기 사를 생체 내 및 체외의 두 가지 범주로 분류하는 BERT 기반 DL 모델을 제안했다. 이 모델은 사전 훈련된 BERT 와 완전히 연결된 하나의 레이어로 구성되며 기사의 제목과 초록을 입력으로 사용한다. 그러나 저자는 EMF의 인 체 건강 위험과 관련된 과학 기사 분류에 가장 적합한 모델 구조를 탐색하지 않았다. 또한 모델은 상대적으로 적은 수의 기사(즉, 455개 기사)를 사용하여 미세 조정(Fine-tuned)되었다."}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 과학 기사 분류를 위한 BERT(Bidirectional Encoder Representations from Transformers) 기반 딥러 닝 모델의 양방향 인코더 표현을 제시한다. 전자기장(EMF)과 관련된 인체 건강 위험 평가의 효율성과 신뢰성을 높이는 것을 목표하여 제안하는 본 개시의 전자 장치 및 인공지능 모델은, 전자파 관련 논문의 제목과 초록을 취하여 동물노출실험, 세포노출실험, 인체노출실험, 역학연구의 네 가지 범주로 분류하였다. 본 개시는 논문, 리서치, 기사 등의 카테고리를 분류하기 위한 BERT 및 CNN 기반의 인공지능 모델을 활용하는 전자 장치를 제공한다. 본 개시의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 개시의 다른 목적 및 장점들 은 하기의 설명에 의해서 이해될 수 있고, 본 개시의 실시 예에 의해 보다 분명하게 이해될 것이다. 또한, 본 개시의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것이다."}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 전자 장치는, BERT(Bidirectional Encoder Representations from Transformers) 모델 및 CNN(Convolutional Neural Network) 모델을 포함하는 인공지능 모델이 저장된 메모리, 일 텍스트를 상 기 인공지능 모델에 입력하여, 상기 텍스트의 유형을 동물노출실험, 세포노출실험, 인체노출실험, 및 역학연구 중 하나로 식별하는 프로세서를 포함한다. 상기 프로세서는, 상기 텍스트를 상기 BERT 모델로 입력하여 hidden state vector를 획득하고, 상기 hidden state vector를 상기 CNN 모델에 입력할 수 있다. 이 경우, 상기 프로세서는, 상기 텍스트를 구성하는 각 토큰에 대하여 토큰 임베딩(Token Embeddings), 세그먼 트 임베딩(Segment Embeddings), 및 위치 임베딩(Position Embeddings)을 획득하고, 각 토큰에 대하여 상기 토큰 임베딩, 상기 세그먼트 임베딩, 및 상기 위치 임베딩을 합산하여 입력 임베딩을 획득하고, 상기 입력 임베딩 을 상기 BERT 모델로 입력할 수 있다. 여기서, 상기 프로세서는, 상기 텍스트로부터 획득된 토큰의 수가 상기 BERT 모델에 대해 기설정된 토큰 임베딩 의 수보다 작은 경우, 부족한 토큰 임베딩에 대해서 제로 패딩을 수행하여 상기 BERT 모델에 입력하고, 상기 텍 스트로부터 획득된 토큰의 수가 상기 BERT 모델에 대해 기설정된 토큰 임베딩의 수보다 큰 경우, 상기 기설정된 토큰 임베딩의 수를 초과하는 토큰을 제외하여 상기 BERT 모델에 입력할 수도 있다. 한편, 상기 CNN 모델은, 상기 BERT 모델이 출력하는 hidden state vector를 입력 받는 적어도 하나의 convolutional layer, 상기 convolutional layer가 출력하는 feature map를 입력 받는 적어도 하나의 pooling layer, 상기 pooling layer에 의해 축소된 feature map을 입력 받는 적어도 하나의 fully-connected layer를 포함할 수 있다. 여기서, 상기 프로세서는, 상기 fully-connected layer의 출력을 바탕으로 상기 텍스트의 유형 을 식별할 수 있다. 이때, 상기 프로세서는, 상기 BERT 모델이 출력하는 hidden state vector 중 문장의 시작(CLS)에 매칭되는 hidden state vector TCLS를 상기 축소된 feature map과 결합하여 결합된 features를 획득하고, 상기 결합된 features를 상기 fully-connected layer로 입력할 수 있다. 또한, 상기 CNN 모델은, window size가 3인 128개의 컨볼루션 필터로 구성된 제1 세트, window size가 4인 128 개의 컨볼루션 필터로 구성된 제2 세트를 포함할 수 있다. 이 경우, 상기 프로세서는, 상기 제1 세트 및 상기 제2 세트를 바탕으로 256개의 feature map을 획득하고, 상기 feqture map에 배치 정규화를 수행하여 상기 pooling layer에 입력할 수 있다. 본 개시의 일 실시 예에 따라 BERT(Bidirectional Encoder Representations from Transformers) 모델 및 CNN(Convolutional Neural Network) 모델을 포함하는 인공지능 모델이 저장된 전자 장치의 제어 방법은, 일 논 문을 구성하는 텍스트를 상기 인공지능 모델에 입력하는 단계, 상기 인공지능 모델의 출력을 바탕으로 상기 텍 스트의 유형을 동물노출실험, 세포노출실험, 인체노출실험, 및 역학연구 중 하나로 식별하는 단계를 포함한다."}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 전자 장치의 인공지능 모델에 있어서, BERT 모델이 출력하는 임베딩의 일부가 pooling layer의 출력 과 함께 fully-connected layer의 입력으로 다시 사용되는 방식은, 다양한 표현 기능을 고려하고 임베딩 내 정 보 사용을 극대화할 수 있다. 성능 평가에서 본 개시의 전자 장치가 활용하는 인공지능 모델은 평균 98.33%의 정확도를 달성하여 CNN, BiLSTM, BERT 및 사전 훈련된 임베딩을 사용하는 단순 NN과 같은 다른 딥러닝 모델을 능가했다."}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에 대하여 구체적으로 설명하기에 앞서, 본 명세서 및 도면의 기재 방법에 대하여 설명한다. 먼저, 본 명세서 및 청구범위에서 사용되는 용어는 본 개시의 다양한 실시 예들에서의 기능을 고려하여 일반적 인 용어들을 선택하였다. 하지만, 이러한 용어들은 당해 기술 분야에 종사하는 기술자의 의도나 법률적 또는 기 술적 해석 및 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 일부 용어는 출원인이 임의로 선정한 용어 도 있다. 이러한 용어에 대해서는 본 명세서에서 정의된 의미로 해석될 수 있으며, 구체적인 용어 정의가 없으 면 본 명세서의 전반적인 내용 및 당해 기술 분야의 통상적인 기술 상식을 토대로 해석될 수도 있다.또한, 본 명세서에 첨부된 각 도면에 기재된 동일한 참조번호 또는 부호는 실질적으로 동일한 기능을 수행하는 부품 또는 구성요소를 나타낸다. 설명 및 이해의 편의를 위해서 서로 다른 실시 예들에서도 동일한 참조번호 또 는 부호를 사용하여 설명한다. 즉, 복수의 도면에서 동일한 참조 번호를 가지는 구성요소가 모두 도시되어 있다 고 하더라도, 복수의 도면들이 하나의 실시 예를 의미하는 것은 아니다. 또한, 본 명세서 및 청구범위에서는 구성요소들 간의 구별을 위하여 \"제1\", \"제2\" 등과 같이 서수를 포함하는 용어가 사용될 수 있다. 이러한 서수는 동일 또는 유사한 구성요소들을 서로 구별하기 위하여 사용하는 것이며 이러한 서수 사용으로 인하여 용어의 의미가 한정 해석되어서는 안 된다. 일 예로, 이러한 서수와 결합된 구성 요소는 그 숫자에 의해 사용 순서나 배치 순서 등이 제한되어서는 안 된다. 필요에 따라서는, 각 서수들은 서로 교체되어 사용될 수도 있다. 본 명세서에서 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이 들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성 요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시의 실시 예에서 \"모듈\", \"유닛\", \"부(Part)\" 등과 같은 용어는 적어도 하나의 기능이나 동작을 수행하는 구성요소를 지칭하기 위한 용어이며, 이러한 구성요소는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\", \"유닛\", \"부(Part)\" 등은 각각이 개별적인 특정 한 하드웨어로 구현될 필요가 있는 경우를 제외하고는, 적어도 하나의 모듈이나 칩으로 일체화되어 적어도 하나 의 프로세서로 구현될 수 있다. 또한, 본 개시의 실시 예에서, 어떤 부분이 다른 부분과 연결되어 있다고 할 때, 이는 직접적인 연결뿐 아니라, 다른 매체를 통한 간접적인 연결의 경우도 포함한다. 또한, 어떤 부분이 어떤 구성요소를 포함한다는 의미는, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것 을 의미한다. 도 1은 본 개시의 일 실시 예에 따른 전자 장치의 구성을 설명하기 위한 블록도이다. 도 1을 참조하면, 전자 장치는 메모리 및 프로세서를 포함한다. 전자 장치는 적어도 하나 의 컴퓨터를 포함하는 서버 내지 시스템에 해당할 수 있다. 또한, 전자 장치는 데스크탑 PC, 노트북 PC, 태블릿 PC, 스마트폰 등 다양한 단말 기기에 해당할 수도 있다. 메모리는 전자 장치의 구성요소들의 전반적인 동작을 제어하기 위한 운영체제(OS: Operating System) 및 전자 장치의 구성요소와 관련된 적어도 하나의 인스트럭션 또는 데이터를 저장하기 위한 구성이다. 메모리는 ROM, 플래시 메모리 등의 비휘발성 메모리를 포함할 수 있으며, DRAM 등으로 구성된 휘발성 메모 리를 포함할 수 있다. 또한, 메모리는 하드 디스크, SSD(Solid State Drive) 등을 포함할 수도 있다. 도 1을 참조하면, 메모리는 텍스트의 유형을 분류하기 위한 적어도 하나의 인공지능 모델을 포함할 수 있다. 여기서, 텍스트는 적어도 하나의 과학과 관련된 문서(ex. 논문, 기사, 리서치 등)를 구성하는 텍스트"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "일 수 있다. 구체적으로, 텍스트는 과학 관련 문서의 초록(요약)에 해당할 수도 있다. 본 개시의 일 실시 예에 따른 인공지능 모델은 BERT 모델 및 CNN 모델이 결합된 구조일 수 있다. BERT 모델은, 논문, 기사, 리서치 등 다양한 문서를 바탕으로 출력을 획득하도록 사전 훈련된 모델일 수 있다."}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "구체적으로, BERT 모델은 각 문서의 초록(요약)을 바탕으로 훈련될 수 있으며, 이때 각 문서의 초록을 구성하는 텍스트에 포함된 복수의 토큰(ex. 단어 등) 각각에 대하여 후술할 hidden state vector를 출력하도록 훈련될 수 있다. 훈련 과정은, 전자 장치에 의해 수행된 것일 수도 있고 외부 전자 장치 상에서 수행된 것일 수도 있 다. CNN 모델은 적어도 하나의 convolutional layer, 적어도 하나의 pooling layer, 적어도 하나의 fully- connected layer를 포함할 수 있다. BERT 모델 및 CNN 모델의 구체적인 내용은 후술할 도면들을 통해 설명한다. 한편, 인공지능 모델에 의한 분류의 대상이 되는 텍스트는 논문, 리서치, 기사 등에 해당할 수 있다. 구체 적인 예로, 분류의 대상이 되는 텍스트는 각 논문/기사의 제목 및 초록 중 적어도 하나에 해당할 수 있다.분류의 결과에 해당되는 유형은, 예를 들어, 과학 논문, 과학 리서치, 인문학 논문, 인문학 리서치, 실험, 역학 연구, 특정 대상과 관련된 실험 등에 해당할 수 있으나 이에 한정되지 않는다. 구체적인 예로, 본 개시의 일 실시 예에 따른 인공지능 모델은 전자기장(EMF)의 위험성과 관련된 과학 논 문/기사를 동물노출실험, 세포노출실험, 인체노출실험, 및 역학연구 중 하나의 유형으로 분류하도록 구성될 수 있다(본 개시에서 제공되는 실험의 내용 및 일부 목적에 부합한다). 프로세서는 전자 장치를 전반적으로 제어하기 위한 구성이다. 구체적으로, 프로세서는 메모리 와 연결되는 한편 메모리에 저장된 적어도 하나의 인스트럭션을 실행함으로써 본 개시의 다양한 실시 예들에 따른 동작을 수행할 수 있다. 프로세서는 하나 이상의 프로세서로 구성될 수 있다. 이때, 하나 이상의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit) 등과 같은 그래픽 전용 프로 세서 또는 NPU와 같은 인공지능 전용 프로세서 등을 포함할 수 있다. 일 실시 예로, 프로세서는 논문, 리서치, 또는 기사에 해당하는 텍스트를 인공지능 모델로 입력할 수 있으며, 인공지능 모델의 출력에 따라 텍스트의 유형을 분류할 수 있다. 본 개시에 따른 인공지능 모델은 사전 훈련된 BERT와 CNN을 통합하여 분류를 수행할 수 있다. 특히 사전 훈련된 BERT는 임베딩을 생성한 다음 CNN의 입력으로 사용할 수 있다. 또한 임베딩의 일부는 CNN의 출력과 함께 fully-connected layer의 입력으로 사용된다. 본 모델 구조는 표현된 다양한 특징을 고려한 논문 등의 분류를 가능하게 한다. 도 2는 본 개시의 일 실시 예에 따른 모델의 구조를 보여주고 있으며, pre-trained BERT, convolutional layer, pooling layer, fully-connected layer로 구성되어 있다. 사전 훈련된 BERT는 텍스트 데이터를 수신하 고 hidden state vector를 생성한 다음 convolutional layer의 입력으로 사용한다. convolutional layer는 convolutional filter들을 사용하여 feature map을 생성한 다음 max-over-time pooling을 통해 크기를 줄이기 위해 pooling layer로 전달한다. CLS token의 hidden state vector는 축소된 feature map에 추가되어 컨텍스트 정보를 더 중요하게 고려한다. 결합된 기능은 분류 결과를 생성하는 fully-connected layer의 입력으로 사용된 다. 입력 텍스트를 hidden state vector로 변환하기 위해 본 개시의 일 실시 예에 따른 인공지능 모델은 다양 한 논문이나 기사, 논문 등 다양한 문서(ex. 출처: MEDLINE/PubMed)의 초록을 바탕으로 사전 훈련된 BERT 모델 을 포함할 수 있다. 이러한 사전 훈련 과정은, 전자 장치에 의해 수행될 수도 있고, 적어도 하나의 외부 전자 장치에 의해 수행될 수도 있다. 도 3은 제안된 인공지능 모델에 사용된 사전 훈련된 BERT의 구조를 보여준다. E는 토큰 임베딩, 세그먼트 임베딩 및 위치 임베딩의 합인 BERT의 입력 임베딩을 나타낸다. 토큰 임베딩은 WordPiece에 의해 생성된 토큰화 된 단어이다. 토큰 임베딩에서 CLS 토큰과 SEP 토큰은 각각 문장의 시작과 끝을 나타내며 입력 텍스트에서 파생 된 i번째 단어 토큰(Word Token)을 나타낸다. 예를 들어, 토큰 임베딩의 크기가 150으로 고정될 수 있다. 데이터 세트(ex. 다양한 논문/기사의 초록)의 평균 토큰 길이를 기준으로 해당 크기가 선택되었다. 생성된 토큰의 수가 150 미만인 경우 제로 패딩이 사용되어 토 큰 임베딩의 나머지 부분을 채울 수 있다. 그러나 생성된 토큰의 수가 150개를 초과하면 토큰 임베딩의 뒷부분 이 잘릴 수 있다. BERT의 입력은 문장 또는 문장 쌍일 수 있으며 세그먼트 임베딩은 각 토큰이 첫 번째 문장에 속하는지 두 번째 문장에 속하는지를 나타낸다. 그러나, 본 개시에서 제안하는 인공지능 모델에서는 입력 텍스트를 한 문장 으로 간주한다. 두 문장을 구분하는 것은 입력 텍스트가 쿼리-답변 쌍으로 구성된 경우에만 사용되기 때문이다. 따라서 모든 토큰은 동일한 세그먼트 임베딩을 갖는다. 위치 임베딩은 토큰의 순서를 나타낸다. 입력 임베딩을 취한 후 사전 훈련된 BERT는 입력 임베딩의 크기와 동일한 hidden state vector를 생성한다. 특 히, TCLS 및 TSEP는 각각 CLS 및 SEP 토큰에 대한 hidden state vector에 해당하며 i번째 단어 토큰에 대한 hidden state vector에 해당한다. 각 hidden state vector에는 768개의 차원이 있다. 따라서 사전 훈련된 BERT 의 출력은 768 x 150 행렬이며 다음과 같이 표현될 수 있다.수학식 1"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "사전 훈련된 BERT의 출력은 convolutional layer의 입력 역할을 하고, TCLS는 다시 fully-connected layer의 입 력에 추가된다. TCLS는 모든 입력 토큰을 고려한 집계 정보를 나타낸다. 이는 BERT의 양방향 구조가 모든 입력 토큰의 정보를 반영하기 때문이다. 따라서 fully-connected layer는 convolutional layer에서 파생된 기능과 함께 집계 표현을 사용한다. 이 구조를 통해 제안된 모델은 TCLS 내 집계 정보를 보다 중요하게 고려할 수 있다. Convolutional layer는 hidden state vector를 수신하고 256개의 컨볼루션 필터 를 사용하여 컨볼루션 작업을 수행한다. 일 예로, 각각 window size가 3과 4인 두 세트의 128개 필터가 활용되었다. 컨볼루션 필터의 크기는 window size에 해당하는 d와 hidden state vector k의 차원인 768에 의해 결정되었다. Feature map의 일 특징은 다음과 같이 파생된다. 수학식 2"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서 b는 바이어스이고 F는 텍스트 데이터에서 비선형 관계를 표현하기 위해 사용되는 비선형 함수이다. 본 개시의 일 실시 예에 따른 인공지능 모델에서는 수학식 3과 같이 LeakyReLU를 F로 사용한다. 수학식 3"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "Feature map을 생성하기 위해 임베딩 매트릭스 에서 가능한 각 단어 창에 수학식 2가 적용된다. 따라서 각 feature map은 다음과 같이 표현될 수 있다. 수학식 4"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "결과적으로 256개의 컨벌루션 필터를 통해 256개의 feature map이 생성된다. 생성된 feature map은 covariate shift를 피하기 위해 배치 정규화(batch normalization)를 통해 정규화된 후 pooling layer의 입력으로 사용될 수 있다. Pooling layer에서는 수학식 5와 같이 feature map에서 가장 큰 값을 선택하는 max-over-time pooling을 사용 하여 feature map을 줄일 수 있다. 수학식 5"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "따라서 256개의 feature map은 수학식 6과 같이 256차원 벡터로 축소된다. 수학식 6"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기서 j는 feature map의 수(즉, 256)일 수 있다. 이 256차원 벡터 는 fully-connected layer의 입력으로 사 용될 수 있다. fully connected layer의 입력값은 1024차원 벡터이며 pooling layer의 출력값과 TCLS가 결합된 것이다. Fully- connected layer는 512개의 뉴런을 포함하므로 fully-connected layer에 대한 가중치 계산은 수학식 7과 같이 수행될 수 있다. 수학식 7"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "여기서 w는 가중치 매개변수(Weight Parameters)의 행렬이고 b는 편향 행렬(Bias Matrix)이다. 그런 다음 ReLU 및 softmax 함수는 이하 수학식 8 및 9와 같이 최종 분류 결과를 생성한다. 수학식 8"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "수학식 9"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "여기서 는 ReLU 함수의 결과이고 L은 분류 레이블의 수이다. 이때, 분류 레이블마다 확률에 해당하는 실 수가 출력될 수 있다. 예를 들어, 논문/기사를 동물 노출 실험, 세포 노출 실험, 인간 노출 실험, 및 역학 연구 중 하나로 분류하도록 인공지능 모델이 구성 및 훈련된 경우를 가정한다. 이 경우, 4개의 분류 레이블이 있으므로 softmax 함수 의 결과에는 확률 분포를 형성하는 4개의 실수가 포함된다. 따라서 가장 큰 숫자에 해당하는 레이블이 분류 결 과로 선택될 수 있다. 한편, 학습 중에는 과적합(Overfitting)을 방지하기 위해 fully-connected layer에 30% 드롭아웃이 적용될 수 있다. 이하 내용에서는, 본 개시에 따른 인공지능 모델의 성능을 검증하기 위해 예시적으로 수행된 실험의 내용 및 결과에 대해 설명한다. 본 실험에서, 인공지능 모델은 본 개시의 일부 목적에 따라 EMF의 인체 건강 위 험 평가의 효율성과 신뢰성을 높이는 것을 목표로 하여 관련 논문/기사의 텍스트 데이터(즉, 논문 제목 및 초록)를 수신하고 동물 노출 실험, 세포 노출 실험, 인간 노출 실험, 및 역학 연구의 네 가지 범주로 분류하여 분류를 수행하도록 설계되었다."}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "과학 논문/기사에 대한 분류를 수행하기 위해 EMF-portal에서 기사 정보가 수집되었다. 이 포털은 6,943개 기사 의 선별된 요약을 포함하여 35,044개의 EMF 관련 기사 목록을 제공한다. EMF-포털의 기사는 EMF의 인체 건강 위 험과 관련된 \"실험 연구\" 및 \"역학 연구\"와 함께 10개의 주제로 분류된다. \"실험 연구\"에는 동물, 세포 및 인간 에 대한 EMF 노출 실험에 대한 기사가 포함되며 \"역학 연구\"에는 기지국 및 TV 타워와 같은 EMF 소스에 대한 연 구에 대한 기사가 포함된다. 나머지 8개 주제는 전자기 간섭, 기술/선량 측정 연구, 의료 응용, 전기 부상, 법 률/지침, 검토/설문 조사 및 위험 커뮤니케이션을 다룬다. 데이터 세트를 만들기 위해 \"실험 연구\" 및 \"역학 연 구\"에서 논문의 제목과 초록을 수집했다. 우리의 목표는 EMF의 인체 건강 위험 평가를 위한 과학적 문서 분류를 수행하는 것이었다."}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "먼저 EMF-portal이 선별된 요약을 제공하는 \"실험 연구\"에서 기사 제목이 수집되었다. 다음으로, \"실험적 연"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "구\"의 기사는 선별된 요약의 \"노출된 시스템\" 섹션에 제공된 정보를 기반으로 3개의 레이블(즉, 동물 노출 실험, 세포 노출 실험 및 인체 노출 실험)로 분류되었다. \"노출된 시스템\" 섹션의 예는 도 1에 나와 있으며 실"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "험에서 노출되는 대상을 설명하고 레이블 정보로 사용할 수 있다. \"역학 연구\"의 기사는 선별된 요약에 의존하 지 않고 수집되었으며 역학 연구로 분류되었다. 제목과 레이블 정보를 수집한 후 개방형 생물의학 학술 데이터 베이스인 PubMed에서 논문의 초록을 수집했다. 그 결과 동물노출실험 1,175개, 세포노출실험 812개, 인체노출실 험 434개, 역학연구 943개를 보유하고 있다. 본 개시에 따른 인공지능 모델의 우수성을 검증하기 위해 사전 훈련된 임베딩을 사용하여 다른 DL 모델(즉, CNN, BiLSTM 및 단순 NN)의 성능을 비교했다. 또한 제안 모델의 성능을 구체적으로 평가하기 위해 K- fold cross-validation이 수행되었다. 도 4는 비교를 위한 DL 모델의 구조를 도시한 것이다. 도 4에서 각 모델은 사전 훈련된 임베딩과 다양한 레이어 조합으로 구성된다. 미리 훈련된 임베딩은 입력 텍스트를 임베딩 매트릭스로 변환하는 데 사용된다. PubMed- BERT, BioWordVec 및 PubMed-word2vec의 세 가지 사전 훈련된 임베딩을 고려했다. PubMed-BERT는 제안된 모델 에 사용된 사전 훈련된 BERT이다. 따라서 PubMed-BERT를 사용하는 경우 임베딩 행렬의 크기는 768 X 150이다. BioWordVec 및 PubMed-word2vec은 생의학 단어에 대한 사전 훈련된 200차원 단어 임베딩에 해당한다. 두 가지 모두 PubMed의 기사 텍스트(즉, 제목 및 초록)에 대해 교육을 받았다. 그러나 BioWordVec은 FastText 모델을 기 반으로 학습한 반면 PubMed-word2vec은 word2vec 모델을 기반으로 학습했다. 두 모델 모두 일반적으로 skip- gram 모델을 기반으로 단어 임베딩을 학습하지만 두 모델은 텍스트를 다르게 토큰화한다. FastText의 경우 텍스 트는 단어보다 작은 하위 단어로 토큰화된다. 반면에 word2vec의 경우 텍스트가 단어로 토큰화된다. BioWordVec 또는 PubMed-word2vec를 사용하면 입력 텍스트의 각 단어가 BioWordVec 또는 PubMed-word2vec의 해 당 임베딩으로 대체되어 임베딩 매트릭스를 생성한다. 따라서 임베딩 행렬의 크기는 200 X n이며, 여기서 n은 입력 텍스트의 단어 수이다. 모델에서 임베딩 매트릭스를 같은 크기로 만들기 위해 n은 600으로 고정되었다. 텍 스트 데이터의 단어 수가 600개 미만이면 임베딩 매트릭스의 나머지 부분을 채우기 위해 제로 패딩이 사용될 수 있다. 텍스트 데이터에서 가장 많은 단어 수가 597개였기 때문에 n을 600으로 설정했다. CNN 및 BiLSTM 모델에서 임베딩 매트릭스는 CNN 레이어 또는 BiLSTM 레이어의 입력으로 사용된다. CNN layer, pooling layer, 및 fully-connected layer는 앞서와 동일하게 정의된다. BiLSTM 레이어는 숨겨진 상태 크기가 64인 BiLSTM 셀을 포함하도록 정의된다. 한편 단순 NN 모델에서는 임베딩 행렬이 fully-connected layer의 입력으로 직접 사용되지 않는다. 이는 fully- connected layer가 2차원 행렬이 아닌 벡터를 입력으로 받기 때문이다. 따라서 PubMed-BERT를 사용하면 fully- connected layer의 입력으로만 사용된다. BioWordVec 또는 PubMed-word2vec를 사용하는 경우 임베딩 매트릭스 는 120,000차원 벡터로 평면화된다. 평가에서 데이터 세트의 80%는 훈련에 사용되었고 데이터 세트의 20%는 검증에 사용되었다. 훈련 과정을 위해 교차 엔트로피와 Adam이 각각 손실 함수와 optimizer로 사용되었다. 학습률은 0.0001로 설정했고 batch size와 epoch는 각각 20과 10으로 설정되었다. 표 1은 제안 모델과 다른 DL 모델의 성능 비교를 보여준다. 표 1에서 본 개시의 제안 모델은 다른 DL 모델에 비 해 가장 높은 성능을 보인다. 제안하는 모델은 생물의학에 특화된 사전 훈련된 BERT와 CNN 모델을 결합하고 다 시 fully-connected layer의 입력에 집계 표현을 추가한다. 따라서 제안된 모델은 pre-trained BERT를 미세 조 정하고 convolutional layer와 fully-connected layer 모두의 입력으로 집계 정보를 두 배로 사용하여 다른 DL 모델보다 성능이 뛰어나다.표 1"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "한편, 표 1에서 사전 훈련된 BERT를 사용하는 모델은 BioWordVec 및 PubMed-word2vec를 사용하는 동일한 유형의 모델에 비해 더 나은 성능을 나타낸다. 이러한 결과는 EMF의 인체 건강 위험 평가를 위한 과학 기사 분류에서 사전 훈련된 BERT를 사용하는 DL 모델이 다른 사전 훈련된 임베딩을 사용하는 DL 모델보다 더 나은 성능을 얻을 수 있음을 의미한다. 본 개시에서 제안한 인공지능 모델의 성능을 구체적으로 평가하기 위해 K-fold 교차 검증을 진행했다. 데 이터 세트를 5개의 fold로 나누고 5번의 실험을 수행했다. 표 2는 confusion matrix를 사용한 K-fold 교차 검증 결과를 보여준다. 5번의 실험에서 제안 모델은 98.06%에서 98.66%의 정확도를 나타내어 제안 모델이 과적합 없 이 과학 논문을 정확하게 분류하고 있음을 확인할 수 있다.표 2"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "그러나 confusion matrix는 제안된 모델이 종종 동물 노출 실험을 세포 노출 실험으로 잘못 분류하거나 그 반대 의 경우도 있음을 보여준다. 두 레이블의 기사에 유사한 단어가 포함되는 경우가 많기 때문이다. 예를 들어, 동 물 노출 실험의 일부 기사에는 동물 세포 이름이 포함되는 반면 세포 노출 실험의 일부 기사에는 표적 세포가 수집된 동물의 이름이 포함된다. 또한 제안된 모델은 일반적으로 인체 및 EMF 노출 환경과 관련된 단어를 포함하기 때문에 인체 노출 실험을 역 학 연구로 잘못 분류하거나 그 반대로 잘못 분류하는 경우가 많다. 또한 데이터 세트의 인체 노출 실험 항목 수 는 다른 라벨의 항목보다 상대적으로 적다. 따라서 앞으로 더 많은 인체 노출 논문을 수집하면 인체 노출 실험 을 역학 연구로 오분류하거나 그 반대로 오분류하는 오류를 줄일 수 있을 것으로 기대된다. 한편, 이상에서 설명된 다양한 실시 예들은 서로 저촉되지 않는 한 복수의 실시 예가 결합되어 구현될 수 있다. 한편, 이상에서 설명된 다양한 실시 예들은 소프트웨어(Software), 하드웨어(Hardware) 또는 이들의 조합된 것 을 이용하여 컴퓨터(Computer) 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 하드웨어적인 구현에 의하면, 본 개시에서 설명되는 실시 예들은 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices),PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서(Processors), 제어기 (Controllers), 마이크로 컨트롤러(Micro-controllers), 마이크로 프로세서(Microprocessors), 기타 기능 수행 을 위한 전기적인 유닛(Unit) 중 적어도 하나를 이용하여 구현될 수 있다. 일부의 경우에 본 명세서에서 설명되는 실시 예들이 프로세서 자체로 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들은 별도의 소프트웨어 모듈들로 구현될 수 있다. 상술한 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 작동을 수행할 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 각 장치에서의 처리동작을 수행하기 위한 컴퓨터 명령어 (Computer Instructions) 또는 컴퓨터 프로그램은 비일시적 컴퓨터 판독 가능 매체(Non-transitory Computer- readable Medium)에 저장될 수 있다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령어 또는 컴 퓨터 프로그램은 HAP 등 특정 기기의 프로세서에 의해 실행되었을 때 상술한 다양한 실시 예에 따른 전자 장치 의 동작을 상술한 특정 기기가 수행하도록 한다. 비일시적 컴퓨터 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체 가 아니라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(Reading)이 가능한 매체를 의미한다. 비일시적 컴퓨터 판독 가능 매체의 구체적인 예로는, CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등 이 있을 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2023-0078105", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안 될 것이다."}
{"patent_id": "10-2023-0078105", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시 예에 따른 전자 장치의 구성을 설명하기 위한 블록도, 도 2는 본 개시의 일 실시 예에 따라 BERT 모델과 CNN 모델을 포함하는 인공지능 모델의 구조를 설명하기 위한 도면, 도 3은 본 개시의 일 실시 예에 따른 BERT 모델의 입출력을 설명하기 위한 도면, 그리고 도 4는 본 개시의 일 실시 예에 따른 인공지능 모델의 성능을 검증하는 과정에서 비교 대상에 해당하는 종래의 모델들의 구조를 설명하기 위한 도면이다."}
