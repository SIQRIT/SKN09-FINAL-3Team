{"patent_id": "10-2021-0067321", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0159571", "출원번호": "10-2021-0067321", "발명의 명칭": "딥러닝 기반의 장기 게임 서비스 방법 및 그 장치", "출원인": "엔에이치엔클라우드 주식회사", "발명자": "박정훈"}}
{"patent_id": "10-2021-0067321", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "게임 플레이 서버가 유저 단말기에 딥러닝 기반의 장기 게임 서비스를 제공하는 방법으로서, 복수의 액션(action) 별 장기판 상태를 포함하는 트레이닝 데이터 셋(Training data set)을 획득하는 단계; 상기 획득된 트레이닝 데이터 셋을 기초로 입력 특징(Input features)을 추출하는 단계; 상기 추출된 입력 특징을 기초로 딥러닝 모델(Deep-learning model)을 학습시키는 단계; 제1 장기판 상태를 수신하는 단계; 및 상기 딥러닝 모델을 기초로 상기 수신된 제1 장기판 상태에 대하여 수행할 액션인 수행액션 및 승패 예측정보중 적어도 하나를 획득하는 단계를 포함하는 딥러닝 기반의 장기 게임 서비스 방법."}
{"patent_id": "10-2021-0067321", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 입력 특징(Input features)은, 장기 기물(棋物)의 종류, 제1 플레이어의 최근 8개 액션에 대한 히스토리(history) 정보, 상기 제1 플레이어의최근 8개 액션에 대한 기물 위치정보, 제2 플레이어의 최근 8개 액션에 대한 히스토리(history) 정보, 상기 제2플레이어의 최근 8개 액션에 대한 기물 위치정보 및 현재 플레이어가 상기 제1 플레이어 또는 상기 제2 플레이어 중 어느 플레이어인지에 대한 차례 정보 중 적어도 일부를 포함하는 딥러닝 기반의 장기 게임 서비스 방법."}
{"patent_id": "10-2021-0067321", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서, 상기 딥러닝 모델(Deep-learning model)을 학습시키는 단계는, 소정의 장기판 상태를 입력 데이터로 하고, 상기 입력된 소정의 장기판 상태에서 수행 가능한 액션인 후보액션 별 승률추정값, 상기 소정의 장기판 상태에대하여 수행할 액션인 수행액션 및 상기 소정의 장기판 상태에 대한 승패 예측정보 중 적어도 하나를 출력 데이터로 하도록 상기 딥러닝 모델을 학습시키는 단계를 포함하는 딥러닝 기반의 장기 게임 서비스 방법."}
{"patent_id": "10-2021-0067321", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서, 상기 액션(action)은, 장기 기물(棋物) 별 행마법(行馬法)을 기초로 장기판 상에서 특정 기물을 움직여 착수시키는 행동인 딥러닝 기반의 장기 게임 서비스 방법."}
{"patent_id": "10-2021-0067321", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서, 상기 액션(action)은, 공개특허 10-2022-0159571-3-궁성 외 이동액션, 마(馬) 이동액션, 상(象) 이동액션, 궁성 내 이동액션 및 패스(pass) 액션을 포함하는 딥러닝 기반의 장기 게임 서비스 방법."}
{"patent_id": "10-2021-0067321", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서, 상기 제1 장기판 상태에 대한 수행액션 및 승패 예측정보 중 적어도 하나를 획득하는 단계는, 상기 제1 장기판 상태를 상기 딥러닝 모델에 입력하고, 상기 제1 장기판 상태에서 수행 가능한 액션인 후보액션별 승률추정값을 획득하는 단계를 포함하는 딥러닝 기반의 장기 게임 서비스 방법."}
{"patent_id": "10-2021-0067321", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서, 상기 제1 장기판 상태에 대한 수행액션 및 승패 예측정보 중 적어도 하나를 획득하는 단계는, 상기 승률추정값이 가장 높은 후보액션을 상기 제1 장기판 상태에 대한 수행액션으로 결정하는 단계를 더 포함하는 딥러닝 기반의 장기 게임 서비스 방법."}
{"patent_id": "10-2021-0067321", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서, 상기 제1 장기판 상태에 대한 수행액션 및 승패 예측정보 중 적어도 하나를 획득하는 단계는, 상기 결정된 수행액션이 수행됨에 따른 승패 여부를 기초로 상기 제1 장기판 상태에 대한 상기 승패 예측정보를획득하는 단계를 더 포함하는 딥러닝 기반의 장기 게임 서비스 방법."}
{"patent_id": "10-2021-0067321", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 항에 있어서, 상기 제1 장기판 상태에 대한 상기 수행액션을 수행하는 단계를 더 포함하는 딥러닝 기반의 장기 게임 서비스 방법."}
{"patent_id": "10-2021-0067321", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "장기판 상태를 수신하는 통신부; 딥러닝 모델을 저장하는 메모리; 및 상기 수신된 장기판 상태를 기초로 상기 딥러닝 모델을 학습시키고, 상기 딥러닝 모델을 기초로 상기 장기판 상태에서 가능한 복수의 후보액션 중 가장 높은 승률추정값을 가지는액션인 수행액션과, 상기 수행액션이 수행됨에 따른 상기 장기판 상태에 대한 승패 예측정보 중 적어도 하나를획득하는 프로세서;를 포함하는 것을 특징으로 하는 딥러닝 기반의 장기 게임 서비스 장치."}
{"patent_id": "10-2021-0067321", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법은, 게임 플레이 서버가 유저 단말기에 딥러닝 기 반의 장기 게임 서비스를 제공하는 방법으로서, 복수의 액션(action) 별 장기판 상태를 포함하는 트레이닝 데이 터 셋(Training data set)을 획득하는 단계; 상기 획득된 트레이닝 데이터 셋을 기초로 입력 특징(Input features)을 추출하는 단계; 상기 추출된 입력 특징을 기초로 딥러닝 모델(Deep-learning model)을 학습시키는 단계; 제1 장기판 상태를 수신하는 단계; 및 상기 딥러닝 모델을 기초로 상기 수신된 제1 장기판 상태에 대하여 수행할 액션인 수행액션 및 승패 예측정보 중 적어도 하나를 획득하는 단계를 포함한다."}
{"patent_id": "10-2021-0067321", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치에 관한 것이다. 보다 상세하게는, 장기 게임 서비 스의 딥러닝 모델을 학습시키고, 학습된 딥러닝 모델을 이용하여 장기 게임 서비스에서의 대국 상황에 따라서수행할 액션 (action)과 대국 결과를 예측하는 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치에 관한 것이다."}
{"patent_id": "10-2021-0067321", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 들어, 통신 및 네트워크 기술의 발달에 따라서 유/무선 인터넷의 보급이 급격하게 증가됨에 따라 인터넷이 라는 동질의 매체를 통하여 여러 종류의 서비스가 이루어지고 있다. 특히, 게임 서비스는 인터넷을 통해 제공되는 서비스 중에서도 많은 사용자들이 이용하는 서비스로 다양한 게임 들이 서비스되고 있다. 그 중에서도 승리의 대가로 게임 포인트를 주고받는 대전(對戰) 형식의 게임들이 많이 서비스되고 있으며, '장 기', '바둑', '고스톱' 또는 '체스' 등의 게임들은 많은 사용자들을 확보하고 있는 대중적인 게임들이다. 한편, 이러한 추세와 더불어 최근에는 사람이 아닌 프로그램된 인공지능 컴퓨터와 위와 같은 게임을 통해 대전 을 수행할 수 있게 되었다. 더하여, 인공지능 컴퓨터가 사람 수준으로 대전을 수행할 수 있도록 인공지능 컴퓨터의 기력을 높이기 위한 연 구가 활발하게 진행되고 있는 추세이다. 그러나 종래의 인공지능 컴퓨터는, 바둑이나 체스와 같이 좀 더 대중적인 게임 서비스에 특화되어 있는 실정이 어서, 장기 게임 서비스에서 높은 성능을 가지는 인공지능 컴퓨터를 구현하기 위한 기술이 미비한 상황이다. 즉, 장기 게임 서비스는 장기의 기물(棋物)마다 제각기 독특하게 정해진 행마법(行馬法)에 의하여 바둑과 같은 다른 게임 서비스에 비해 가능한 액션의 수가 보다 방대하다는 특성 등과 같이, 장기 게임 서비스에서 특징적으 로 구현되는 게임의 속성을 보다 소상하게 고려하며 그에 최적화된 딥러닝을 수행할 수 있는 인공지능 컴퓨터의 개발이 미흡한 실정이다. 선행기술문헌 특허문헌 (특허문헌 0001) KR 2007371 B1"}
{"patent_id": "10-2021-0067321", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은, 장기 게임 서비스의 딥러닝 모델을 학습시키고, 학습된 딥러닝 모델을 이용하여 장기 게임 서비스에 서의 대국 상황에 따라서 수행할 액션(action)과 대국 결과를 예측하는 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치를 제공하는데 그 목적이 있다. 자세히, 본 발명은, 장기 게임 서비스에 특화되도록 딥러닝 모델을 학습시키는 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치를 제공하고자 한다. 또한, 본 발명은, 학습된 딥러닝 모델을 이용하여 장기 게임 서비스에서의 대국 상황에 기반한 액션 별 승률, 해당 대국 상황에 대하여 수행할 액션과 그에 따른 대국 결과를 예측하는 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치를 제공하고자 한다. 또한, 본 발명은, 예측된 수행 액션을 기반으로 장기 대국을 수행하는 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치를 제공하고자 한다. 다만, 본 발명 및 본 발명의 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되 지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2021-0067321", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법은, 게임 플레이 서버가 유저 단말기에 딥러닝 기반의 장기 게임 서비스를 제공하는 방법으로서, 복수의 액션(action) 별 장기판 상태를 포함하는 트레이닝 데이터 셋(Training data set)을 획득하는 단계; 상기 획득된 트레이닝 데이터 셋을 기초로 입력 특징(Input features)을 추출하는 단계; 상기 추출된 입력 특징을 기초로 딥러닝 모델(Deep-learning model)을 학습시키는 단계; 제1 장기판 상태를 수신하는 단계; 및 상기 딥러닝 모델을 기초로 상기 수신된 제1 장기판 상태에 대하여 수행할 액션인 수행액션 및 승패 예측정보 중 적어도 하나를 획득하는 단계를 포함한다. 이때, 상기 입력 특징(Input features)은, 장기 기물(棋物)의 종류, 제1 플레이어의 최근 8개 액션에 대한 히스 토리(history) 정보, 상기 제1 플레이어의 최근 8개 액션에 대한 기물 위치정보, 제2 플레이어의 최근 8개 액션 에 대한 히스토리(history) 정보, 상기 제2 플레이어의 최근 8개 액션에 대한 기물 위치정보 및 현재 플레이어 가 상기 제1 플레이어 또는 상기 제2 플레이어 중 어느 플레이어인지에 대한 차례 정보 중 적어도 일부를 포함 한다. 또한, 상기 딥러닝 모델(Deep-learning model)을 학습시키는 단계는, 소정의 장기판 상태를 입력 데이터로 하고, 상기 입력된 소정의 장기판 상태에서 수행 가능한 액션인 후보액션 별 승률추정값, 상기 소정의 장기판 상태에 대하여 수행할 액션인 수행액션 및 상기 소정의 장기판 상태에 대한 승패 예측정보 중 적어도 하나를 출 력 데이터로 하도록 상기 딥러닝 모델을 학습시키는 단계를 포함한다. 또한, 상기 액션(action)은, 장기 기물(棋物) 별 행마법(行馬法)을 기초로 장기판 상에서 특정 기물을 움직여 착수시키는 행동이다. 또한, 상기 액션(action)은, 궁성 외 이동액션, 마(馬) 이동액션, 상(象) 이동액션, 궁성 내 이동액션 및 패스 (pass) 액션을 포함한다. 또한, 상기 제1 장기판 상태에 대한 수행액션 및 승패 예측정보 중 적어도 하나를 획득하는 단계는, 상기 제1 장기판 상태를 상기 딥러닝 모델에 입력하고, 상기 제1 장기판 상태에서 수행 가능한 액션인 후보액션 별 승률 추정값을 획득하는 단계를 포함한다. 또한, 상기 제1 장기판 상태에 대한 수행액션 및 승패 예측정보 중 적어도 하나를 획득하는 단계는, 상기 승률 추정값이 가장 높은 후보액션을 상기 제1 장기판 상태에 대한 수행액션으로 결정하는 단계를 더 포함한다. 또한, 상기 제1 장기판 상태에 대한 수행액션 및 승패 예측정보 중 적어도 하나를 획득하는 단계는, 상기 결정 된 수행액션이 수행됨에 따른 승패 여부를 기초로 상기 제1 장기판 상태에 대한 상기 승패 예측정보를 획득하는 단계를 더 포함한다. 또한, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법은, 상기 제1 장기판 상태에 대한 상기 수 행액션을 수행하는 단계를 더 포함한다. 한편, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 장치는, 장기판 상태를 수신하는 통신부; 딥러 닝 모델을 저장하는 메모리; 및 상기 수신된 장기판 상태를 기초로 상기 딥러닝 모델을 학습시키고, 상기 딥러 닝 모델을 기초로 상기 장기판 상태에서 가능한 복수의 후보액션 중 가장 높은 승률추정값을 가지는 액션인 수 행액션과, 상기 수행액션이 수행됨에 따른 상기 장기판 상태에 대한 승패 예측정보 중 적어도 하나를 획득하는 프로세서;를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2021-0067321", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치는, 장기 게임 서비스에 특화되도록 딥러닝 모델을 학습시킴으로써 장기 게임 서비스에서 특징적으로 구현되는 게임의 속성을 보다 소상하게 고려하 며 그에 최적화된 딥러닝을 수행할 수 있는 인공지능 컴퓨터를 구현할 수 있다. 또한, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치는, 학습된 딥러닝 모델을 이 용하여 장기 게임 서비스에서의 대국 상황에 기반한 액션 별 승률을 예측하고, 이를 기초로 해당 대국 상황에 대하여 수행할 액션과 그에 따른 대국 결과를 예측함으로써 장기 게임 서비스에 대한 인공지능 컴퓨터의 성능을 보다 향상시킬 수 있다. 또한, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치는, 예측된 수행 액션을 기반 으로 장기 대국을 수행함으로써 인공지능 컴퓨터의 장기 대국 수준을 증진시킬 수 있다. 다만, 본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효 과들은 아래의 기재로부터 명확하게 이해될 수 있다."}
{"patent_id": "10-2021-0067321", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변환을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고"}
{"patent_id": "10-2021-0067321", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "상세한 설명에 상세하게 설명하고자 한다. 본 발명의 효과 및 특징, 그리고 그것들을 달성하는 방법은 도면과 함께 상세하게 후술되어 있는 실시예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시 예들에 한정되는 것이 아니라 다양한 형태로 구현될 수 있다. 이하의 실시예에서, 제1, 제2 등의 용어는 한정적 인 의미가 아니라 하나의 구성 요소를 다른 구성 요소와 구별하는 목적으로 사용되었다. 또한, 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 또한, 포함하다 또는 가지다 등의 용어는 명 세서상에 기재된 특징, 또는 구성요소가 존재함을 의미하는 것이고, 하나 이상의 다른 특징들 또는 구성요소가 부가될 가능성을 미리 배제하는 것은 아니다. 또한, 도면에서는 설명의 편의를 위하여 구성 요소들이 그 크기가 과장 또는 축소될 수 있다. 예컨대, 도면에서 나타난 각 구성의 크기 및 두께는 설명의 편의를 위해 임의로 나 타내었으므로, 본 발명이 반드시 도시된 바에 한정되지 않는다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예들을 상세히 설명하기로 하며, 도면을 참조하여 설명할 때 동일 하거나 대응하는 구성 요소는 동일한 도면부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 도 1은 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 시스템에 대한 개념도이다. 도 1을 참조하면, 본 발명의 실시예에 따른 딥러닝 기반의 장기(將棋) 게임 서비스 시스템은, 단말기, 장기서버, 게임 플레이 서버 및 네트워크를 포함할 수 있다. 도 1의 각 구성요소는, 네트워크를 통해 연결될 수 있다. 단말기, 장기서버 및/또는 게임 플레 이 서버 등과 같은 각각의 노드 상호 간에 정보 교환이 가능한 연결 구조를 의미하는 것으로, 이러한 네트 워크의 일 예에는 3GPP(3rd Generation Partnership Project) 네트워크, LTE(Long Term Evolution) 네트워크, WIMAX(World Interoperability for Microwave Access) 네트워크, 인터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), 블루투스(Bluetooth) 네트워크, 위성 방송 네트워크, 아날로그 방송 네트워크, DMB(Digital Multimedia Broadcasting) 네트워크 등이 포함되나 이에 한정되지는 않는다. - 단말기(100: Terminal) 먼저, 단말기는, 장기 게임 서비스를 제공받고자 하는 유저의 단말기이다. 또한, 단말기는 다양한 작 업을 수행하는 애플리케이션들을 실행하기 위한 유저가 사용하는 하나 이상의 컴퓨터 또는 다른 전자 장치이다. 예컨대, 컴퓨터, 랩탑 컴퓨터, 스마트 폰, 모바일 전화기, PDA, 태블릿 PC, 혹은 장기서버 및/또는 게임 플레이 서버와 통신하도록 동작 가능한 임의의 다른 디바이스를 포함한다. 다만 이에 한정되는 것은 아니고 단말기는 다양한 머신들 상에서 실행되고, 다수의 메모리 내에 저장된 명 령어들을 해석하여 실행하는 프로세싱 로직을 포함하고, 외부 입력/출력 디바이스 상에 그래픽 사용자 인터페이 스(GUI)를 위한 그래픽 정보를 디스플레이하는 프로세스들과 같이 다양한 기타 요소들을 포함할 수 있다. 아울러 단말기는 입력 장치(예를 들면 마우스, 키보드, 터치 감지 표면 등) 및 출력 장치(예를 들면 디스 플레이장치, 모니터, 스크린 등)에 접속될 수 있다. 단말기에 의해 실행되는 애플리케이션들은 게임 애플리케이션, 웹 브라우저, 웹 브라우저에서 동작하는 웹 애플리케이션, 워드 프로세서들, 미디어 플레이어들, 스프레드시트들, 이미지 프로세서들, 보안 소프트웨어 또 는 그 밖의 것을 포함할 수 있다. 또한, 단말기는 명령들을 저장하는 적어도 하나의 메모리, 적어도 하나의 프로세서 및 통신부 를 포함할 수 있다. 단말기의 메모리는 단말기에서 구동되는 다수의 응용 프로그램(application program) 또는 애플 리케이션(application), 단말기의 동작을 위한 데이터들, 명령어들을 저장할 수 있다. 명령들은 프로세서로 하여금 동작들을 수행하게 하기 위해 프로세서에 의해 실행 가능하고, 동작들은 장기 게임 실행 요청 신호를 전송, 게임 데이터 송수신, 액션 정보 송수신, 승률 정보 송수신, 승패 예측정보 송수신, 게임 시간 정보 요청 및 게임 시간 정보 수신 등의 각종 정보를 송수신하는 동작들을 포함할 수 있다. 또한, 메모리는 하드웨어적으로, ROM, RAM, EPROM, 플래시 드라이브, 하드 드라이브 등과 같은 다양한 저 장기기 일 수 있고, 메모리는 인터넷(internet)상에서 상기 메모리의 저장 기능을 수행하는 웹 스토 리지(web storage)일 수도 있다. 단말기의 프로세서는 전반적인 동작을 제어하여 장기 게임 서비스를 제공받기 위한 데이터 처리를 수 행할 수 있다. 단말기에서 장기 게임 애플리케이션이 실행되면, 단말기에서 장기 게임 환경이 구성된다. 그리고 장 기 게임 애플리케이션은 네트워크를 통해 장기서버와 장기 게임 데이터를 교환하여 단말기 상에 서 장기 게임 서비스가 실행되도록 한다. 이러한 프로세서는 ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서 (microprocessors), 기타 기능 수행을 위한 임의의 형태의 프로세서일 수 있다. 단말기의 통신부는, 하기 통신방식(예를 들어, GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), LTE(Long Term Evolution), LTE-A(Long Term Evolution-Advanced) 등), WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), Wi-Fi(Wireless Fidelity) Direct, DLNA(Digital Living Network Alliance), WiBro(Wireless Broadband), WiMAX(World Interoperability for Microwave Access)에 따라 구축된 네트워크망 상에서 기지국, 외부의 단말, 서버 중 적어도 하나와 무선 신호를 송수신할 수 있다. 이러한 단말기는, 후술되는 장기서버 및 게임 플레이 서버 중 적어도 하나에서 수행되는 기능 동작의 적어도 일부를 수행할 수도 있다. - 장기서버(200: JANG-GI server) 장기서버가 제공하는 장기 게임 서비스는 장기서버가 제공하는 가상의 컴퓨터 유저와 실제 유저가 함 께 게임에 참여하는 형태로 구성될 수 있다. 이는 유저 측 단말기 상에서 구현되는 장기 게임 환경에서 하 나의 실제 유저와 하나의 컴퓨터 유저가 함께 게임을 플레이 한다. 다른 측면에서, 장기서버가 제공하는 장기 게임 서비스는 복수의 유저 측 디바이스가 참여하여 장기 게임 이 플레이되는 형태로 구성될 수도 있다. 장기서버는 명령들을 저장하는 적어도 하나의 메모리, 적어도 하나의 프로세서 및 통신부 를 포함할 수 있다. 장기서버의 메모리는 장기서버에서 구동되는 다수의 응용 프로그램(application program) 또는 애플리케이션(application), 장기서버의 동작을 위한 데이터들, 명령어들을 저장할 수 있다. 명령들은 프로세서로 하여금 동작들을 수행하게 하기 위해 프로세서에 의해 실행 가능하고, 동작들은 게임 실행 요청 신호 수신, 게임 데이터 송수신, 액션 정보 송수신, 승률 정보 송수신, 승패 예측정보 송수신, 게임 시간 정보 요청 및 게임 시간 정보 수신 등의 각종 전송 동작을 포함할 수 있다. 또한, 메모리는 장기서버에서 대국을 하였던 복수의 장기 기보(이하, 기보) 또는 기존에 공개된 복수 의 기보를 저장할 수 있다. 복수의 기보 각각은, 대국 시작의 첫 액션 정보인 제1 액션부터 대국이 종료되는 최 종 액션까지의 정보를 모두 포함할 수 있다. 즉, 복수의 기보는 액션에 관한 히스토리 정보를 포함할 수 있다. 또한, 복수의 기보의 각 기보는 장기 대국에서의 액션 순서에 따른 각각의 장기판 상태(S)를 포함할 수 있다. 여기서, 상기 장기판 상태는, 장기판 상에 장기 기물(棋物)이 놓여져 있는 상태일 수 있다. 또한, 장기서버는 게임 플레이 서버의 트레이닝을 위하여 저장된 복수의 기보를 게임 플레이 서버 에 제공할 수 있게 한다. 또한, 메모리는 하드웨어적으로, ROM, RAM, EPROM, 플래시 드라이브, 하드 드라이브 등과 같은 다양한 저 장기기 일 수 있고, 메모리는 인터넷(internet)상에서 상기 메모리의 저장 기능을 수행하는 웹 스토 리지(web storage)일 수도 있다. 장기서버의 프로세서는 전반적인 동작을 제어하여 장기 게임 서비스를 제공하기 위한 데이터 처리를 수행할 수 있다. 이러한 프로세서는 ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro-controllers), 마이 크로 프로세서(microprocessors), 기타 기능 수행을 위한 임의의 형태의 프로세서일 수 있다. 장기서버는 통신부를 통해 네트워크를 경유하여 단말기 및 게임 플레이 서버와 통신 을 수행할 수 있다. - 게임 플레이 서버(300: Game playing server) 게임 플레이 서버는, 별도의 클라우드 서버나 컴퓨팅 장치를 포함할 수 있다. 또한, 게임 플레이 서버 는 단말기의 프로세서 또는 장기서버의 데이터 처리부에 설치된 신경망 시스템일 수 있지만, 이 하에서 게임 플레이 서버는, 단말기 또는 장기서버와 별도의 장치로 설명한다. 게임 플레이 서버는 명령들을 저장하는 적어도 하나의 메모리, 적어도 하나의 프로세서 및 통신 부를 포함할 수 있다. 게임 플레이 서버는 장기 규칙에 따라 스스로 학습하는 딥러닝 모델인 액션 모델(action model)을 구축하 고 단말기의 유저와 대국을 할 수 있는 인공지능 컴퓨터로써 자신의 턴에서 대국에서 이길 수 있도록 장기 게임에서의 액션을 수행할 수 있다. 게임 플레이 서버가 액션 모델로 트레이닝하는 자세한 설명은 도 2 내 지 도 6의 액션 모델에 관한 설명을 따른다. 여기서, 실시예에 따른 액션(action)은, 장기 기물(이하, 기물) 별 행마법(行馬法)을 기초로 장기판 상에서 특 정 기물을 움직여 착수시키는 행동을 의미할 수 있다. 이때, 상기 행마법은 기물들이 제각기 독특하게 정해진 규칙에 의해서 움직이는 것을 말한다. 게임 플레이 서버의 메모리는 게임 플레이 서버에서 구동되는 다수의 응용 프로그램 (application program) 또는 애플리케이션(application), 게임 플레이 서버의 동작을 위한 데이터들, 명 령어들을 저장할 수 있다. 명령들은 프로세서로 하여금 동작들을 수행하게 하기 위해 프로세서에 의 해 실행 가능하고, 동작들은 액션 모델 학습(트레이닝) 동작, 액션 정보 송수신, 액션 준비 시간 수신, 게임 시 간 정보 수신 및 각종 전송 동작을 포함할 수 있다. 또한, 메모리는 딥러닝 모델인 액션 모델을 저장할 수 있다. 또한, 메모리는 하드웨어적으로, ROM, RAM, EPROM, 플래시 드라이브, 하드 드라이브 등과 같은 다양한 저장기기 일 수 있고, 메모리는 인터넷 (internet)상에서 상기 메모리의 저장 기능을 수행하는 웹 스토리지(web storage)일 수도 있다.게임 플레이 서버의 프로세서는 메모리에 저장된 액션 모델을 독출하여, 구축된 신경망 시스템 에 따라서 하기 기술하는 액션 모델 학습 및 장기 게임에서의 액션을 수행하게 된다. 실시예에 따라서 프로세서는, 전체 유닛들을 제어하는 메인 프로세서와, 액션 모델에 따라서 신경망 구동 시 필요한 대용량의 연산을 처리하는 복수의 그래픽 프로세서(Graphics Processing Unit, GPU)를 포함하도록 구 성될 수 있다. 게임 플레이 서버는 통신부를 통해 네트워크를 경유하여 장기서버와 통신을 수행할 수 있 다. <액션 모델> 본 발명의 실시예에 따른 액션 모델은, 소정의 장기판 상태를 입력 데이터로 하고, 상기 입력된 소정의 장기판 상태에서 수행 가능한 액션(action)인 후보액션 별 승률추정값(V), 적어도 하나 이상의 후보액션 중 대국에서 이기기 위한 액션으로 판단되는 수행액션(A) 및/또는 해당 대국의 결과 즉, 승패 여부를 예측한 승패 예측정보 (O)를 출력 데이터로 할 수 있다. 도 2는 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스에서 인공지능 컴퓨터의 액션(action)을 위한 게임 플레이 서버의 액션 모델 구조를 설명하기 위한 도면이다. 도 2를 참조하면, 본 발명의 실시예에 따른 액션 모델은 게임 플레이 서버의 딥러닝 모델로서, 탐색부 , 셀프 플레이부 및 딥러닝 신경망을 포함할 수 있다. 자세히, 액션 모델은 탐색부, 셀프 플레이부 및 딥러닝 신경망을 이용하여 대국에서 이기기 위 한 액션(action)을 결정하는 모델로 학습할 수 있다. 구체적으로, 탐색부는 딥러닝 신경망의 가이드에 따라서 몬테 카를로 트리 서치(Monte Carlo Tree Search; MCTS) 동작을 수행할 수 있다. MCTS는 모종의 의사 결정을 위한 체험적 탐색 알고리즘이다. 즉, 탐색 부는 딥러닝 신경망이 제공하는 이동 확률값(p) 및/또는 승률추정값(V)에 기초하여 MCTS를 수행할 수 있다. 일 예로, 딥러닝 신경망에 의해 가이드된 탐색부는 MCTS를 수행하여 액션들에 대한 확률분포값 인 탐색 확률값( )을 출력할 수 있다. 셀프 플레이부는 탐색 확률값( )에 따라 스스로 장기 대국을 할 수 있다. 셀프 플레이부는 게임의 승패가 결정되는 시점까지 스스로 장기 대국을 진행하고, 자가 대국(즉, self-play)이 종료되면 장기판 상태 (S), 탐색 확률값( ), 자가 플레이 가치값(z)을 딥러닝 신경망에 제공할 수 있다. 여기서 장기판 상태 (S)는 장기판 상에 기물이 놓여진 상태일 수 있다. 자가 플레이 가치값(z)은 장기판 상태(S)에서 자가 대국을 하였을 때 승률 값이다. 딥러닝 신경망은 이동 확률값(p)과 승률추정값(V)을 출력할 수 있다. 이동 확률값(p)은 장기판 상태(S)에 따라서 어떠한 액션을 수행하는 것이 게임을 이길 수 있는 좋은 액션인지를 수치로 나타낸 확률분포값일 수 있 다. 승률추정값(V)은 해당 액션 수행 시의 승률을 나타낸다. 예를 들어, 이동 확률값(p)이 높은 액션이 좋은 액 션일 수 있다. 딥러닝 신경망은 이동 확률값(p)이 탐색 확률값( )과 동일해지도록 트레이닝되고, 승률추 정값(V)이 자가 플레이 가치값(z)과 동일해지도록 트레이닝될 수 있다. 이후 트레이닝된 딥러닝 신경망은 탐색부를 가이드하고, 탐색부는 이전 탐색 확률값( )보다 더 좋은 액션을 찾도록 액션 준비 시간 동안 MCTS를 진행하여 새로운 탐색 확률값( )을 출력하게 한다. 예를 들어, 액션 준비 시간은 MCTS 진행 시간 에 따라 평균 액션 준비 시간, 제1 액션 준비 시간 및 제2 액션 준비 시간 중 어느 하나의 액션 준비 시간을 따 를 수 있다. 액션 준비 시간은 기본적으로 평균 액션 준비 시간으로 설정되어 있을 수 있다. 셀프 플레이부는 새로운 탐색 확률값( )에 기초하여 장기판 상태(S)에 따른 새로운 자가 플레이 가치값 (z)을 출력하고 장기판 상태(S), 새로운 탐색 확률값( ), 새로운 자가 플레이 가치값(z)을 딥러닝 신경망 에 제공할 수 있다. 딥러닝 신경망은 이동 확률값(p)과 승률추정값(V)이 새로운 탐색 확률값( )과 새로운 자가 플레이 가치값(z)으로 출력되도록 다시 트레이닝될 수 있다. 즉, 액션 모델은 이러한 과정을 반복하여 딥러닝 신경망이 대국에서 이기기 위한 더 좋은 액션을 찾도록 트레이닝 될 수 있다. 일 예로, 액션 모델은 액션 손실(l)을 이용할 수 있다. 액션 손실(l)은 수학식 1과 같다. [수학식 1]"}
{"patent_id": "10-2021-0067321", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "는 신경망의 파라미터이고, c는 매우 작은 상수이다. 수학식 1의 액션 손실(l)에서 z와 v가 같아 지도록 하는 것은 평균 제곱 손실(mean square loss) 텀에 해당되고, 와 p가 같아 지도록 하는 것은 크로스 엔트로피 손실(cross entropy loss) 텀에 해당되고, 에 c를 곱하는 것은 정규화 텀으로 오버피팅(overfitting)을 방지하기 위한 것이다. 한편, 딥러닝 신경망은 신경망 구조로 구성될 수 있다. 일 예로, 딥러닝 신경망은 한 개의 컨볼루션 (convolution) 블록과 19개의 레지듀얼(residual) 블록으로 구성될 수 있다. 컨볼루션 블록은 3X3 컨볼루션 레 이어가 여러개 중첩된 형태일 있다. 하나의 레지듀얼 블록은 3X3 컨볼루션 레이어가 여러개 중첩되고 스킵 커넥 션을 포함한 형태일 수 있다. 스킵 커넥션은 소정의 레이어의 입력이 해당 레이어의 출력값과 합하여서 출력되 어 다른 레이어에 입력되는 구조이다. 또한, 딥러닝 신경망의 입력은 기물의 종류, 제1 플레이어(예컨대, 청 플레이어)의 최근 8개의 액션에 대 한 히스토리 및 제1 플레이어의 최근 8개의 액션에 대한 기물의 위치 정보와, 제2 플레이어(예컨대, 홍 플레이 어)의 최근 8개의 액션에 대한 히스토리 및 제1 플레이어의 최근 8개의 액션에 대한 기물의 위치 정보와, 현재 플레이어가 제1 플레이어인지 제2 플레이어인지에 대한 차례 정보 등을 포함하는 114*10*9의 RGB 이미지일 수 있다. 이에 대한 자세한 설명은 도 2 내지 도 8의 액션 모델에 관한 설명을 따른다. 도 3은 본 발명의 실시예에 따른 액션 모델이 탐색부의 파이프 라인에 따라서 액션을 수행하는 과정을 설명하기 위한 도면이다. 도 3을 참조하면, 본 발명의 실시예에 따른 액션 모델은, 자신의 차례에서 딥러닝 신경망과 탐색부를 이용하여 소정의 액션을 수행할 수 있다. 자세히, 액션 모델은 탐색부, 셀프 플레이부 및 딥러닝 신 경망을 이용하여 몬테 카를로 트리 서치(MCTS)를 수행할 수 있다. 구체적으로, 액션 모델은 선택 과정(a)을 통하여 현재 제1 장기판 상태(S1)에서 MCTS를 통해 탐색하지 않은 가 지 중 활동 함수(Q)와 신뢰값(U)이 높은 액션을 가지는 제2 장기판 상태(S1-2)를 선택한다. 활동 함수(Q)는 해 당 가지를 지날 때마다 산출된 승률추정값(V)들의 평균값이다. 신뢰값(U)은 해당 가지를 지나는 방문 횟수(N)에 반비례하고 이동 확률값(p)에 비례한다. 액션 모델은 확장과 평가 과정(b)을 통하여 선택된 액션에서의 제3 장기판 상태(S1-2-1)로 확장하고 이동 확률 값(p)을 산출할 수 있다. 액션 모델은 상기 확장된 제3 장기판 상태(S1-2-1)의 승률추정값(V)을 산출하고 백업 과정(c)을 통하여 지나온 가지들의 활동 함수(Q), 방문 횟수(N), 이동 확률값(p)을 저장할 수 있다. 액션 모델은 액션 준비 시간 동안 선택(a), 확장 및 평가(b), 백업(c) 과정을 반복하고 각 액션에 대한 방문 횟 수(N)를 이용하여 확률 분포를 만들어서 탐색 확률값( )을 출력할 수 있다. 액션 모델은 액션들 중 가장 높은 탐색 확률값( )을 가지는 액션을 검출할 수 있다. 또한, 액션 모델은 상기 검출된 액션을 기초로 수행액션 (A)으로 결정할 수 있다. 또한, 액션 모델은 결정된 수행액션(A)을 수행할 수 있다. 또한, 액션 모델은 결정된 일련의 수행액션(A)이 수행됨에 따른 대국의 결과 즉, 승패 여부를 판단한 승패 예측정보(O)를 제공할 수 있다. - 딥러닝 기반의 장기 게임 서비스 방법 도 4는 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스를 제공하는 방법을 설명하기 위한 흐름도이다. 도 4를 참조하면, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스는, 게임 플레이 서버가 장기 게임 서비스의 딥러닝 모델인 액션 모델을 학습시키고, 학습된 액션 모델을 이용하여 장기 게임 서비스에서의 대국 상황에 따라서 수행할 액션(action)과 대국 결과를 예측하게 할 수 있다. 구체적으로, 게임 플레이 서버는 장기 게임 서비스에 특화되도록 액션 모델을 학습시킬 수 있다. 또한, 게 임 플레이 서버는 학습된 액션 모델을 이용하여 장기 게임 서비스에서의 대국 상황에 기반한 액션 별 승률, 해당 대국 상황에 대한 수행 액션과 그에 따른 대국 결과를 예측할 수 있다. 또한, 게임 플레이 서버 는 예측된 수행 액션을 기반으로 장기 대국을 수행할 수 있다. 여기서, 실시예에 따른 상기 액션(action)은, 기물 별 행마법(行馬法)을 기초로 장기판 상에서 특정 기물을 움 직여 착수시키는 행동을 의미할 수 있다. 이때, 상기 행마법은 기물들이 제각기 독특하게 정해진 규칙에 의해서 움직이는 것을 말한다. 자세히, 도 4를 더 참조하면, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법은 게임 플레이 서 버가 트레이닝 데이터 셋(Training data set)을 수신하는 단계(S101)를 포함할 수 있다. 보다 상세히, 게임 플레이 서버는, 장기서버와 연동하여 액션 모델의 학습을 위한 트레이닝 데이터 셋을 수신할 수 있다. 여기서, 트레이닝 데이터 셋은 장기서버에서 대국을 하였던 복수의 기보 또는 기존 에 공개된 복수의 기보를 포함할 수 있다. 이때, 상기 복수의 기보 각각은, 대국 시작의 첫 액션 정보인 제1 액 션부터 대국이 종료되는 최종 액션까지의 정보를 모두 포함할 수 있다. 즉, 복수의 기보는 액션에 관한 히스토 리 정보를 포함할 수 있다. 또한, 복수의 기보의 각 기보는 장기 대국에서의 액션 순서에 따른 각각의 장기판 상태(S)를 포함할 수 있다. 여기서, 상기 장기판 상태(S)는, 장기판 상에 기물이 놓여져 있는 상태일 수 있다. 또한, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법은 게임 플레이 서버가 수신된 트레 이닝 데이터 셋을 기초로 입력 특징(Input features)을 추출하는 단계(S103)를 포함할 수 있다. 여기서, 상기 입력 특징은, 기물의 종류, 제1 플레이어(예컨대, 청 플레이어)의 최근 8개의 액션에 대한 히스토 리 및 제1 플레이어의 최근 8개의 액션에 대한 기물의 위치 정보와, 제2 플레이어(예컨대, 홍 플레이어)의 최근 8개의 액션에 대한 히스토리 및 제1 플레이어의 최근 8개의 액션에 대한 기물의 위치 정보와, 현재 플레이어가 제1 플레이어인지 제2 플레이어인지에 대한 차례 정보 등을 포함하는 114*10*9의 RGB 이미지일 수 있다. 도 5는 본 발명의 실시예에 따른 입력 특징(Input features)을 설명하기 위한 도면의 일례이다. 자세히, 도 5를 참조하면, 게임 플레이 서버는 복수의 기보의 장기판 상태(S)에 기초하여 상술된 입력 특 징을 추출하는 입력 특징 추출 프로세스를 수행할 수 있다. 구체적으로, 게임 플레이 서버는 상기 입력 특징 추출 프로세스를 수행하는 입력 특징 추출부를 더 포함할 수 있다. 일 예로, 입력 특징 추출부는 신경망 구조로 되어 있을 수 있으며 일종의 인코더를 포함할 수 있다. 또한, 게임 플레이 서버는 상기 입력 특징 추출부를 이용하여 상기 장기판 상태(S)에 대한 입력 특징을 추 출할 수 있다. 이때, 게임 플레이 서버는, 위와 같이 추출된 입력 특징을 이미지화하여 사용할 수 있다. 자세히, 게임 플 레이 서버는 추출된 적어도 하나 이상의 입력 특징을 딥러닝 모델(실시예에서, 액션 모델)의 입력 형태에 맞는 이미지 형태로 변환할 수 있다. 실시예로, 게임 플레이 서버는, 현재 플레이어가 제1 플레이어인지 제2 플레이어인지를 나타내기 위한 차 례 정보를 이미지로 변환하기 위하여, 제1 플레이어(예컨대, 청 플레이어)의 순서인 경우 상기 제1 플레이어의 색상(예컨대, 청색)을 가지는 복수의 기물들로 장기판이 모두 채워진 장기판 상태(S)의 이미지를 생성하여 상기 차례 정보를 나타낼 수 있다. 또는, 게임 플레이 서버는, 제2 플레이어(예컨대, 홍 플레이어)의 순서인 경 우 상기 제2 플레이어의 색상(예컨대, 홍색)을 가지는 복수의 기물들로 장기판이 모두 채워진 장기판 상태(S)의 이미지를 생성하여 상기 차례 정보를 나타낼 수 있다. 즉, 게임 플레이 서버는 액션 모델에 대한 입력 데이터의 형태에 적합한 이미지 형태로 입력 특징들을 추 출할 수 있다. 또한, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법은 게임 플레이 서버가 추출된 입력 특징을 기초로 딥러닝 모델을 학습시키는 단계(S105)를 포함할 수 있다. 자세히, 게임 플레이 서버는 위와 같이 추출된 입력 특징을 기초로 딥러닝 기반의 액션 모델을 학습시킬 수 있다. 구체적으로, 게임 플레이 서버는 이미지 형태를 가지는 복수의 입력 특징을 트레이닝 데이터 셋 으로 하여 상기 액션 모델을 학습시킬 수 있다. 이때, 게임 플레이 서버는 소정의 장기판 상태(S)를 입력 데이터로 하고, 상기 입력된 소정의 장기판 상태 (S)에 기반한 적어도 하나 이상의 후보액션 별 승률추정값(V), 적어도 하나 이상의 후보액션 중 대국에서 이기 기 위한 액션으로 판단되는 수행액션(A) 및/또는 해당 대국의 결과 즉, 대국의 승패 여부를 예측한 승패 예측정 보(O)를 출력 데이터(Output features)로 제공하도록 액션 모델을 학습시킬 수 있다. 실시예로, 게임 플레이 서버는 액션 모델이 상기 추출된 입력 특징에 기초하여 복수의 기보에 따라서 수행 된 액션 및 그에 따른 대국 결과를 획득하게 할 수 있다. 또한, 게임 플레이 서버는 액션 모델이 상기 복 수의 기보에 따라서 수행된 액션과 그에 따른 대국 결과에 기초하여 상술된 출력 데이터를 생성하게 할 수 있다. 일 례로, 게임 플레이 서버는 상기 복수의 기보에 기반한 입력 특징을 바탕으로, 소정의 제1 액션을 수행 한 경우의 승패 여부를 판단하여 상기 제1 액션에 대한 승률추정값(V)을 추정하도록 상기 액션 모델을 학습시킬 수 있다. 또한, 게임 플레이 서버는 상기 복수의 기보에 기반한 입력 특징을 바탕으로 복수의 후보액션 각 각에 대한 승률추정값(V)을 추정하고, 가장 높은 승률추정값(V)을 가지는 후보액션을 수행액션(A)으로 결정하도 록 상기 액션 모델을 학습시킬 수 있다. 또한, 게임 플레이 서버는 상기 복수의 기보에 기반한 입력 특징 을 바탕으로, 장기판 상태(S) 별 가장 높은 승률추정값(V)을 가지는 일련의 액션(실시예에서, 수행액션(A))을 수행함에 따른 대국의 승패 여부를 추정하여 승패 판단정보를 생성하도록 상기 액션 모델을 학습시킬 수 있다. 이때, 다시 말하자면 본 발명의 실시예에 따른 액션은, 기물 별 행마법(行馬法)을 기초로 장기판 상에서 특정 기물을 움직여 착수시키는 행동을 의미할 수 있다. 여기서, 상기 행마법은 기물들이 제각기 독특하게 정해진 규 칙에 의해서 움직이는 것을 말한다. 도 6은 본 발명의 실시예에 따른 장기 기물 기반의 가능한 액션 수(number of possible actions)를 설명하기 위 한 도면이다. 보다 구체적으로, 도 6을 참조하면, 위와 같은 액션은, 각 기물 별 행마법에 따라서 궁성 외 이동액션, 마(馬) 이동액션, 상(象) 이동액션, 궁성 내 이동액션 및 패스(pass) 액션을 포함할 수 있다. 자세히, 궁성 외 이동액션은, 장기판의 궁성 영역을 제외한 그라운드 영역에서, 차(車), 포(包) 또는 졸(卒)(또 는 병(兵)) 기물이 행마법에 따라서 이동하는 액션을 포함할 수 있다. 또한, 마 이동액션은, 마(馬) 기물이 행 마법에 따라서 이동하는 액션을 포함할 수 있다. 또한, 상 이동액션은, 상(象) 기물이 행마법에 따라서 이동하 는 액션을 포함할 수 있다. 또한, 궁성 내 이동액션은, 궁성 영역에서 소정의 기물이 행마법을 따라서 이동하는 액션을 포함할 수 있다. 또한, 패스 액션은, 액션을 수행하지 않는 액션을 의미할 수 있다. 이때, 상기 각 기물 별 행마법에 따른 상기 액션의 수는, 총 2451개로 구현될 수 있다. 자세히, 상기 궁성 외 이동액션의 수는 궁성 영역을 제외한 그라운드 영역에서, 차(車), 포(包) 또는 졸(卒)(또는 병(兵)) 기물의 행 마법에 따라서 가능한 이동 수의 총합으로 1530개일 수 있다. 또한, 상기 마 이동액션의 수는, 마(馬) 기물의 행마법에 따라서 가능한 이동 수의 총합으로 508개일 수 있다. 또한, 상기 상 이동액션의 수는, 상(象) 기물의 행마법에 따라서 가능한 이동 수의 총합으로 388개일 수 있다. 또한, 상기 궁성 내 이동액션의 수는, 궁성 영역 에서의 가능한 이동 수의 총합으로 24개일 수 있다. 또한, 패스 액션의 수는, 액션을 수행하지 않는 액션 1개일 수 있다. 그리하여 상기 액션의 수는 총 2451개로 구현될 수 있다. 즉, 게임 플레이 서버는, 상술된 바와 같이 장기 대국에 특화된 액션들 중에서 기보에 따라서 수행된 액션 과 그에 따른 대국 결과를 기초로, 소정의 장기판 상태(S)를 입력 데이터로 하고 상기 입력된 소정의 장기판 상 태(S)에 기반한 적어도 하나 이상의 후보액션 별 승률추정값(V), 수행액션(A) 및/또는 승패 예측정보(O)를 출력 데이터로 하는 액션 모델을 학습시킬 수 있다. 이와 같이, 실시예에 따른 딥러닝 기반의 장기 게임 서비스 장치는 소정의 장기판 상태(S)로부터 장기 대국에 특화된 입력 특징을 추출하여 딥러닝 모델 학습에 사용할 수 있다. 한편, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법은 게임 플레이 서버가 제1 장기판 상태를 수신하는 단계(S107)를 포함할 수 있다. 자세히, 게임 플레이 서버는 장기서버와 연동하여 소정의 제1 장기판 상태를 수신할 수 있다. 실시예로, 게임 플레이 서버는 장기서버와 연동하여 현재 진행 중인 대국에서의 현재 장기판 상태를 상기 제1 장기판 상태로 수신할 수 있다. 또한, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법은 게임 플레이 서버가 수신된 제1 장기판 상태에 기반한 딥러닝을 수행하는 단계(S109)를 포함할 수 있다. 도 7은 본 발명의 실시예에 따른 액션 모델이 제1 장기판 상태에 기반한 딥러닝을 수행하여 출력 데이터를 제공 하는 모습을 나타내는 도면의 일례이다. 자세히, 도 7을 참조하면, 게임 플레이 서버는 액션 모델을 이용하여 상기 수신된 제1 장기판 상태에 기반한 딥러닝을 수행할 수 있다. 또한, 게임 플레이 서버는 수행된 딥러닝에 기초하여 상기 제1 장기판 상태에서 수행 가능한 액션인 후보액션 별 승률추정값(V), 적어도 하나 이상의 후보액션 중 대국에서 이기기 위한 액션으로 판단되는 수행액션(A) 및/또는 해당 대국의 결과 즉, 승패 여부를 예측한 승패 예측정보(O)를 획 득할 수 있다. 보다 상세히, 게임 플레이 서버는 상기 수신된 제1 장기판 상태를 액션 모델에 입력 데이터로 입력할 수 있다. 이때, 상기 제1 장기판 상태를 입력받는 액션 모델은 복수의 기보를 포함하는 트레이닝 데이터 셋에 기반하 여 학습된 딥러닝 모델일 수 있다. 그리하여 상기 액션 모델은 입력된 제1 장기판 상태에 기초한 딥러닝을 수행해 상기 제1 장기판 상태에 기반한 적어도 하나 이상의 후보액션 별 승률추정값(V), 수행액션(A) 및/또 는 승패 예측정보(O)를 출력 데이터로 제공할 수 있다. 다시 돌아와서, 또한 게임 플레이 서버는 상기 제1 장기판 상태를 입력받은 액션 모델에서 수행된 딥러 닝을 통해 출력된 상기 제1 장기판 상태에 기반한 적어도 하나 이상의 후보액션 별 승률추정값(V), 수행액션 (A) 및/또는 승패 예측정보(O)를 획득할 수 있다. 또한, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법은 게임 플레이 서버가 위와 같이 수 행된 딥러닝의 결과를 제공하는 단계(S111)를 포함할 수 있다. 자세히, 게임 플레이 서버는 상술된 딥러닝을 통하여 획득된 제1 장기판 상태에 대한 후보액션 별 승률 추정값(V), 수행액션(A) 및/또는 승패 예측정보(O)를 유저의 단말기로 송신할 수 있다. 이때, 상기 유저의 단말기는 수신된 정보를 소정의 방식(예컨대, 장기판 상태(S)를 나타내는 장기판 이미지 상에 표시 등)에 따라서 출력하여 제공할 수 있다. 이와 같이, 실시예에 따른 딥러닝 기반의 장기 게임 서비스 장치는 현재 장기판 상태와 같은 특정 장기판 상태 (S)에서 가능한 액션 별 승률 정보와, 해당 시점에서의 대국 승패를 예측한 정보 등을 유저에게 제공할 수 있다. 또한, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법은 게임 플레이 서버가 위와 같이 수 행된 딥러닝을 기초로 제1 장기판 상태에 대한 액션(action)을 수행하는 단계(S113)를 포함할 수 있다. 도 8은 본 발명의 실시예에 따른 액션 모델이 제1 장기판 상태에 대하여 결정된 수행 액션을 수행하는 모습 을 나타내는 도면의 일례이다. 자세히, 도 8을 참조하면, 게임 플레이 서버는 수신된 제1 장기판 상태에 대하여 수행된 딥러닝을 기초 로, 상기 제1 장기판 상태에서 대국에서 이기기 위한 최선의 액션 즉, 상기 제1 장기판 상태에서 가능한 복수의 후보액션 중 가장 높은 승률을 가지는 액션이라고 판단되는 수행액션(A)을 상기 제1 장기판 상태에 대한 액션으로 결정할 수 있다. 또한, 게임 플레이 서버는 결정된 액션을 수행할 수 있다. 따라서, 이와 같이 실시예에 따른 딥러닝 기반의 장기 게임 서비스 장치는 해당하는 대국에서 이기기 위한 최선의 액션으로 판단되는 수행액션(A)을 기초로 장기 대국을 수행할 수 있다. 이상, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치는, 장기 게임 서비스에 특화 되도록 딥러닝 모델을 학습시킴으로써 장기 게임 서비스에서 특징적으로 구현되는 게임의 속성을 보다 소상하게 고려하며 그에 최적화된 딥러닝을 수행할 수 있는 인공지능 컴퓨터를 구현할 수 있다. 또한, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치는, 학습된 딥러닝 모델을 이 용하여 장기 게임 서비스에서의 대국 상황에 기반한 액션 별 승률을 예측하고, 이를 기초로 해당 대국 상황에 대한 수행 액션과 그에 따른 대국 결과를 예측함으로써 장기 게임 서비스에 대한 인공지능 컴퓨터의 성능을 보다 향상시킬 수 있다. 또한, 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 방법 및 그 장치는, 예측된 수행 액션을 기반 으로 장기 대국을 수행함으로써 인공지능 컴퓨터의 장기 대국 수준을 증진시킬 수 있다. 또한, 이상에서 설명된 본 발명에 따른 실시예는 다양한 컴퓨터 구성요소를 통하여 실행될 수 있는 프로그램 명 령어의 형태로 구현되어 컴퓨터 판독 가능한 기록 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능한 기록 매체 는 프로그램 명령어, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터 판 독 가능한 기록 매체에 기록되는 프로그램 명령어는 본 발명을 위하여 특별히 설계되고 구성된 것이거나 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수 있다. 컴퓨터 판독 가능한 기록 매체의 예에는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등과 같 은, 프로그램 명령어를 저장하고 실행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령어의 예에 는, 컴파일러에 의하여 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용하여 컴퓨터에 의해 서 실행될 수 있는 고급 언어 코드도 포함된다. 하드웨어 장치는 본 발명에 따른 처리를 수행하기 위하여 하나 이상의 소프트웨어 모듈로 변경될 수 있으며, 그 역도 마찬가지이다. 본 발명에서 설명하는 특정 실행들은 일 실시 예들로서, 어떠한 방법으로도 본 발명의 범위를 한정하는 것은 아 니다. 명세서의 간결함을 위하여, 종래 전자적인 구성들, 제어 시스템들, 소프트웨어, 상기 시스템들의 다른 기 능적인 측면들의 기재는 생략될 수 있다. 또한, 도면에 도시된 구성 요소들 간의 선들의 연결 또는 연결 부재들 은 기능적인 연결 및/또는 물리적 또는 회로적 연결들을 예시적으로 나타낸 것으로서, 실제 장치에서는 대체 가 능하거나 추가의 다양한 기능적인 연결, 물리적인 연결, 또는 회로 연결들로서 나타내어질 수 있다. 또한, “필 수적인”, “중요하게” 등과 같이 구체적인 언급이 없다면 본 발명의 적용을 위하여 반드시 필요한 구성 요소 가 아닐 수 있다. 또한 설명한 본 발명의 상세한 설명에서는 본 발명의 바람직한 실시 예를 참조하여 설명하였지만, 해당 기술 분"}
{"patent_id": "10-2021-0067321", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "야의 숙련된 당업자 또는 해당 기술분야에 통상의 지식을 갖는 자라면 후술할 특허청구범위에 기재된 본 발명의 사상 및 기술 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다. 따라서, 본 발명의 기술적 범위는 명세서의 상세한 설명에 기재된 내용으로 한정되는 것이 아 니라 특허청구범위에 의해 정하여져야만 할 것이다."}
{"patent_id": "10-2021-0067321", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스 시스템에 대한 개념도이다. 도 2는 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스에서 인공지능 컴퓨터의 액션(action)을 위한 게임 플레이 서버의 액션 모델 구조를 설명하기 위한 도면이다. 도 3은 본 발명의 실시예에 따른 액션 모델이 탐색부의 파이프 라인에 따라서 액션을 수행하는 과정을 설명하기 위한 도면이다. 도 4는 본 발명의 실시예에 따른 딥러닝 기반의 장기 게임 서비스를 제공하는 방법을 설명하기 위한 흐름도이다. 도 5는 본 발명의 실시예에 따른 입력 특징(Input features)을 설명하기 위한 도면의 일례이다. 도 6은 본 발명의 실시예에 따른 장기 기물 기반의 가능한 액션 수(number of possible actions)를 설명하기 위 한 도면이다. 도 7은 본 발명의 실시예에 따른 액션 모델이 제1 장기판 상태에 기반한 딥러닝을 수행하여 출력 데이터를 제공 하는 모습을 나타내는 도면의 일례이다. 도 8은 본 발명의 실시예에 따른 액션 모델이 제1 장기판 상태에 대하여 결정된 수행 액션을 수행하는 모습을 나타내는 도면의 일례이다."}
