{"patent_id": "10-2020-0126999", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0043509", "출원번호": "10-2020-0126999", "발명의 명칭": "일반화 가능한 인간 모사형 강화학습 알고리즘 설계를 위한 정책 신뢰도, 정보처리 효율 및", "출원인": "한국과학기술원", "발명자": "이상완"}}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터를 통해 수행되는 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 방법에 있어서, 인간의 강화학습 과정이 가진 일반화 능력을 강화학습 모델로 이식하기 위해, 역 강화학습을 통해 도출된 강화학습 모델이 작업의 문맥 변화를 정책에 얼마나 반영하고 있는지에 대한 정량화를 수행하는 정책 신뢰도 정량화단계를 포함하고, 상기 정책 신뢰도 정량화 단계는, 상기 작업의 작업 매개변수와 인간의 행동 프로파일간의 매핑 함수를 근사화하는 단계; 상기 작업 매개변수와 강화학습 알고리즘의 행동 프로파일간의 매핑 함수를 근사화하는 단계; 및 근사화된 두 개의 상기 매핑 함수를 비교하는 단계를 포함하는, 정량화 방법."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 일반화 능력의 정밀한 검증을 위해, 상기 작업의 실제 문제의 복잡도와 문맥 변화를 매개변수화 한 작업공간에서 샘플링된 작업에 대한 일반화 가능성을 검증하는 일반화 능력 검증 단계를 더 포함하는, 정량화 방법."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 환경으로부터 행동으로 연결되는 정보의 전환 또는 이동 과정이 핵심적인 인간 지능의 행동 원리를 제대로 반영하고 있는지를 정량화하는 문제해결 정보처리 효율 정량화 단계를 더 포함하는, 정량화 방법."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 문제해결 정보처리 효율 정량화 단계는, 상기 문맥 변화에 따라 문제해결 정책을 변화시키는 인간 모사형 강화학습 모델의 정책 신뢰도 정량화를 통한적응 능력과 문제해결을 위해 검증된 일반화 능력과의 연결성 확인을 위해 마르코프 체인(Markov chain)을 이용하여 정량화 하는 것을 특징으로 하는, 정량화 방법."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서, 상기 문제해결 정보처리 효율 정량화 단계는, 문제해결 과정에서 발생되는 과거 에피소드가 강화학습 정책에 반영되는 정보 압축 효율과 강화학습 정책으로부터 도출되는 행동의 최적성 비율을 에피소드-정책-행동으로 이어지는 마르코프 체인상의 상호 정보량(mutualinformation)을 이용하여 계산하는 것공개특허 10-2022-0043509-3-을 특징으로 하는, 정량화 방법."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 행동의 최적성 비율은 최적의 문제해결을 위해 에피소드 정보를 강화학습 정책결정에 반영하는 정보전달체계를 나타내는 지표가 되는 것을 특징으로 하는, 정량화 방법."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 강화학습 모델은, 인간이 학습한 정책 정보를 신뢰성 있게 인코딩하는 모델 기반 제어와 모델 없는(model-free) 제어를 결합한 계산 모델인 것을 특징으로 하는, 정량화 방법."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 강화학습 모델은, 목표 매칭(goal matching, GM), 행동 복제(behavior cloning, BC) 및 정책 매칭(policy matching, PM)의 학습방법을 통해 구축되는 것을 특징으로 하는, 정량화 방법."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 장치에 있어서, 인간의 강화학습 과정이 가진 일반화 능력을 강화학습 모델로 이식하기 위해, 역 강화학습을 통해 도출된 강화학습 모델이 작업의 문맥 변화를 정책에 얼마나 반영하고 있는지에 대한 정량화를 수행하는 정책 신뢰도 정량화부를 포함하고, 상기 정책 신뢰도 정량화부는, 상기 작업의 작업 매개변수와 인간의 행동 프로파일간의 매핑 함수를 근사화하고, 상기 작업 매개변수와 강화학습 알고리즘의 행동 프로파일간의 매핑 함수를 근사화한 후, 근사화된 두 개의 상기 매핑 함수를 비교하는 것을 특징으로 하는, 정량화 장치."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 일반화 능력의 정밀한 검증을 위해, 상기 작업의 실제 문제의 복잡도와 문맥 변화를 매개변수화 한 작업공간에서 샘플링된 작업에 대한 일반화 가능성을 검증하는 일반화 능력 검증부를 더 포함하는, 정량화 장치."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 환경으로부터 행동으로 연결되는 정보의 전환 또는 이동 과정이 핵심적인 인간 지능의 행동 원리를 제대로 반영하고 있는지를 정량화하는 문제해결 정보처리 효율 정량화부공개특허 10-2022-0043509-4-를 더 포함하는, 정량화 장치."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 문제해결 정보처리 효율 정량화부는, 상기 문맥 변화에 따라 문제해결 정책을 변화시키는 인간 모사형 강화학습 모델의 상기 정책 신뢰도 정량화부를통한 적응 능력과 상기 일반화 능력 검증부를 통한 문제해결을 위해 검증된 일반화 능력과의 연결성 확인을 위해 마르코프 체인(Markov chain)을 이용하여 정량화 하는 것을 특징으로 하는, 정량화 장치."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서, 상기 문제해결 정보처리 효율 정량화부는, 문제해결 과정에서 발생되는 과거 에피소드가 강화학습 정책에 반영되는 정보 압축 효율과 강화학습 정책으로부터 도출되는 행동의 최적성 비율을 에피소드-정책-행동으로 이어지는 마르코프 체인상의 상호 정보량(mutualinformation)을 이용하여 계산하는 것을 특징으로 하는, 정량화 장치."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 행동의 최적성 비율은 최적의 문제해결을 위해 에피소드 정보를 강화학습 정책결정에 반영하는 정보전달체계를 나타내는 지표가 되는 것을 특징으로 하는, 정량화 장치."}
{"patent_id": "10-2020-0126999", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서, 상기 강화학습 모델은, 인간이 학습한 정책 정보를 신뢰성 있게 인코딩하는 모델 기반 제어와 모델 없는(model-free) 제어를 결합한 계산 모델인 것을 특징으로 하는, 정량화 장치."}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "일반화 가능한 인간 모사형 강화학습 알고리즘 설계를 위한 정책 신뢰도, 정보처리 효율 및 일반화 능력의 정량 화 방법 및 장치가 제시된다. 일 실시예에 따른 컴퓨터를 통해 수행되는 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 방법은, 인간의 강화학습 과정이 가진 일반화 능력을 강화학습 모델로 이식하기 위해, 역 강화학습을 통해 도출된 강화학습 모델이 작업의 문맥 변화를 정책에 얼마나 반영하고 있는지에 대한 정량화 를 수행하는 정책 신뢰도 정량화 단계를 포함하고, 상기 정책 신뢰도 정량화 단계는, 상기 작업의 작업 매개변수 와 인간의 행동 프로파일간의 매핑 함수를 근사화하는 단계; 상기 작업 매개변수와 강화학습 알고리즘의 행동 프 로파일간의 매핑 함수를 근사화하는 단계; 및 근사화된 두 개의 상기 매핑 함수를 비교하는 단계를 포함하여 이 루어질 수 있다."}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "아래의 실시예들은 인공지능 강화학습 알고리즘에 결여된 인간 지능의 장점을 함양하도록 강화학습 알고리즘을 개발하고 검증하는 기술에 관한 것으로, 더욱 상세하게는 일반화 가능한 인간 모사형 강화학습 알고리즘 설계를 위한 정책 신뢰도, 정보처리 효율 및 일반화 능력의 정량화 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "강화학습(Reinforcement Learning, RL)의 급속한 발전은 다양한 유형의 복잡한 문제를 해결하기 위한 알고리즘 개발에 큰 잠재력을 제공했다. 예를 들어, 계층 구조는 희소 보상으로 효과적인 탐구를 촉진하는 것으로 입증 되었다. 모델 기반 RL(model-based RL)은 많은 상황에서 샘플 효율을 개선할 수 있는 능력을 입증했다. RL 알 고리즘도 생물학적 관련성을 확립해 인간다운 지능을 가진 모델 구축에 대한 낙관론을 키웠다. 다양한 과제를 해결할 수 있는 역량에도 불구하고 샘플 효율성 향상, 적응성, 일반화 등 몇 가지 핵심 과제가 남아 있다. 예 를 들어, RL 알고리즘은 환경의 구조를 빠르게 학습할 수 있는 능력이 부족하다. 게다가, 그 행동 정책은 종종매우 편향적이어서 변화하는 환경에 적응하거나 그것의 작업 지식을 일반 상황에 전달하기 어렵다. 이전의 연구에서는 가치 기반 의사결정이 보상 예측 오류(Reward Prediction Error, RPE)에 의해 유도되며, 중 간 뇌 도파민 뉴런은 이러한 정보를 암호화한다는 것을 보여주었다. 후자의 연구는 인간의 뇌가 actor-critic 방식을 실행하는 것처럼 보인다는 것을 발견했다. 이러한 연구는 뇌가 경험에서 배우는 방식이 모델 없는 RL(model-free RL)과 유사하다는 생각을 뒷받침한다. 말하자면, 단일 모델 없는 RL은 행동과 신경 데이터의 비 교적 작은 변동성을 설명할 수 있다. 이 관습적인 견해는 뇌가 하나 이상의 RL을 구현한다는 생각에 의해 도전 을 받았다. 실제로 인간의 뇌는 모델 없는 RL과 모델 기반 RL을 결합할 수 있을 뿐만 아니라, 문맥 변화에 따 라 다른 전략보다 한 전략을 적응적으로 선택할 수 있다. 이러한 적응 과정은 측면 전두엽 피질의 일부에 의해 유도되는 것으로 확인되었으며, 이는 모델 없는 RL 및 모델 기반 RL 전략에 의해 각 예측의 신뢰성을 종합한다. 또한 뇌는 모델 없는 RL과 같이 계산적으로 덜 비싼 전략을 추구하는 경향이 있는데, 특히 매우 안정적이거나 휘발성이 높은 환경에서는 더욱 그러하다. 반면, 전두엽 피질은 성능 신뢰성을 떨어뜨려 모델 기반 학습의 샘 플 효율을 획기적으로 향상시키는 데 관여한다. 이는 뇌가 성능, 샘플 효율성 및 계산 비용 사이의 절충을 처 리할 수 있는 선천적인 능력을 가지고 있음을 의미한다. 비판적으로, 그것은 두뇌가 환경의 새로운 도전에 가 장 잘 대응하는 방법으로 학습 전략을 탐구한다는 이론적 암시로 이어진다. 적응 RL에 대한 두뇌와 알고리즘 솔루션 사이에는 몇 가지 공통점이 있지만, 실질적인 차이는 여전히 그들이 문 제에 접근하는 방식에 있다. 더욱이 RL의 난제를 효과적으로 다룰 수 있는 뇌의 능력은 RL 알고리즘에 의해 완 전히 개발되지 않았다. 이로 인해 다음과 같은 흥미로운 질문이 제기된다. RL 모델이 인간의 행동 데이터에서 인간 RL에 대한 정보를 직접 수집할 수 있는가? 그렇다면 이 모사 모델들은 인간과 유사한 정책을 가지고 있을 까? 많은 작품들이 모사를 통해 정책 학습의 효과를 성공적으로 입증했지만, 그들의 정책이 인간의 잠재 정책과 유사한지, 혹은 정책이 다른 과제에 일반화될 수 있는지에 대해서는 거의 알려져 있지 않다. 또 다른 잠재적인 이슈는 과적합이다. 특히, 인간 행동의 회복성을 조사하는 최근의 연구에서는 모델이 원래 적합했던 인간 행동 데이터를 바탕으로 연구 결과를 복제하지 못하는 경우가 종종 있다는 것을 보여주었다. 이는 컴퓨터 모델의 학 습된 행동 정책이 인간 RL의 선천적 에너지를 완전히 반영하지 못한다는 것을 시사한다. 선행기술문헌 비특허문헌 (비특허문헌 0001) Sang Wan Lee, Shinsuke Shimojo, and John P O'Doherty. Neural computations underlying arbitration between model-based and model-free learning. Neuron, 81:687-699, 2014. (비특허문헌 0002) Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In AAAI, pages 2094-2100, 2016. (비특허문헌 0003) Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016. (비특허문헌 0004) Jane X Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Demis Hassabis, and Matthew Botvinick. Prefrontal cortex as a meta-reinforcement learning system. Nature neuroscience, 21:860, 2018. (비특허문헌 0005) Dongjae Kim, Geon Yeong Park, PO John, Sang Wan Lee, et al. Task complexity interacts with state-space uncertainty in the arbitration between model-based and model-free learning. Nature communications, 10:1-14, 2019. (비특허문헌 0006) Dongjae Kim and Sang Wan Lee. Behavioral and neural evidence for intrinsic motivation effect on reinforcement learning. In 4th Multidisciplinary Conference on Reinforcement Learning and Decision Making. RLDM 2019, 2019. (비특허문헌 0007) Alexandre LS Filipowicz, Jonathan Levine, Eugenio Piasini, Gaia Tavoni, Joseph W Kable, and Joshua Ian Gold. The complexity of model-free and model-based learning strategies. bioRxiv,"}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "pages 2019-12, 2020. 발명의 내용"}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "실시예들은 일반화 가능한 인간 모사형 강화학습 알고리즘 설계를 위한 정책 신뢰도, 정보처리 효율 및 일반화 능력의 정량화 방법 및 장치에 관하여 기술하며, 보다 구체적으로 자율적, 고효율, 일반화 능력을 갖는 인간 모 사형 강화학습 알고리즘의 설계에 필수적인 다각적 정량화 프로세스를 제공한다. 실시예들은 인간의 강화학습 과정이 가진 일반화 능력을 강화학습 알고리즘으로 과적합 없이 알고리즘화 할 수 있는, 일반화 가능한 인간 모사형 강화학습 알고리즘 설계를 위한 정책 신뢰도, 정보처리 효율 및 일반화 능력 의 정량화 방법 및 장치를 제공하는데 있다."}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 컴퓨터를 통해 수행되는 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 방법 은, 인간의 강화학습 과정이 가진 일반화 능력을 강화학습 모델로 이식하기 위해, 역 강화학습을 통해 도출된 강화학습 모델이 작업의 문맥 변화를 정책에 얼마나 반영하고 있는지에 대한 정량화를 수행하는 정책 신뢰도 정 량화 단계를 포함하고, 상기 정책 신뢰도 정량화 단계는, 상기 작업의 작업 매개변수와 인간의 행동 프로파일간 의 매핑 함수를 근사화하는 단계; 상기 작업 매개변수와 강화학습 알고리즘의 행동 프로파일간의 매핑 함수를 근사화하는 단계; 및 근사화된 두 개의 상기 매핑 함수를 비교하는 단계를 포함하여 이루어질 수 있다. 상기 일반화 능력의 정밀한 검증을 위해, 상기 작업의 실제 문제의 복잡도와 문맥 변화를 매개변수화 한 작업공 간에서 샘플링된 작업에 대한 일반화 가능성을 검증하는 일반화 능력 검증 단계를 더 포함할 수 있다. 환경으로부터 행동으로 연결되는 정보의 전환 또는 이동 과정이 핵심적인 인간 지능의 행동 원리를 제대로 반영 하고 있는지를 정량화하는 문제해결 정보처리 효율 정량화 단계를 더 포함할 수 있다. 상기 문제해결 정보처리 효율 정량화 단계는, 상기 문맥 변화에 따라 문제해결 정책을 변화시키는 인간 모사형 강화학습 모델의 정책 신뢰도 정량화를 통한 적응 능력과 문제해결을 위해 검증된 일반화 능력과의 연결성 확인 을 위해 마르코프 체인(Markov chain)을 이용하여 정량화 할 수 있다. 상기 문제해결 정보처리 효율 정량화 단계는, 문제해결 과정에서 발생되는 과거 에피소드가 강화학습 정책에 반 영되는 정보 압축 효율과 강화학습 정책으로부터 도출되는 행동의 최적성 비율을 에피소드-정책-행동으로 이어 지는 마르코프 체인상의 상호 정보량(mutual information)을 이용하여 계산할 수 있다. 상기 행동의 최적성 비율은 최적의 문제해결을 위해 에피소드 정보를 강화학습 정책결정에 반영하는 정보전달 체계를 나타내는 지표가 될 수 있다. 상기 강화학습 모델은, 인간이 학습한 정책 정보를 신뢰성 있게 인코딩하는 모델 기반 제어와 모델 없는(model- free) 제어를 결합한 계산 모델일 수 있다. 상기 강화학습 모델은, 목표 매칭(goal matching, GM), 행동 복제(behavior cloning, BC) 및 정책 매칭(policy matching, PM)의 학습 방법을 통해 구축될 수 있다. 다른 실시예에 따른 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 장치는, 인간의 강화학습 과 정이 가진 일반화 능력을 강화학습 모델로 이식하기 위해, 역 강화학습을 통해 도출된 강화학습 모델이 작업의 문맥 변화를 정책에 얼마나 반영하고 있는지에 대한 정량화를 수행하는 정책 신뢰도 정량화부를 포함하고, 상기 정책 신뢰도 정량화부는, 상기 작업의 작업 매개변수와 인간의 행동 프로파일간의 매핑 함수를 근사화하고, 상 기 작업 매개변수와 강화학습 알고리즘의 행동 프로파일간의 매핑 함수를 근사화한 후, 근사화된 두 개의 상기 매핑 함수를 비교할 수 있다. 상기 일반화 능력의 정밀한 검증을 위해, 상기 작업의 실제 문제의 복잡도와 문맥 변화를 매개변수화 한 작업공 간에서 샘플링된 작업에 대한 일반화 가능성을 검증하는 일반화 능력 검증부를 더 포함할 수 있다. 환경으로부터 행동으로 연결되는 정보의 전환 또는 이동 과정이 핵심적인 인간 지능의 행동 원리를 제대로 반영 하고 있는지를 정량화하는 문제해결 정보처리 효율 정량화부를 더 포함할 수 있다. 상기 문제해결 정보처리 효율 정량화부는, 상기 문맥 변화에 따라 문제해결 정책을 변화시키는 인간 모사형 강 화학습 모델의 상기 정책 신뢰도 정량화부를 통한 적응 능력과 상기 일반화 능력 검증부를 통한 문제해결을 위해 검증된 일반화 능력과의 연결성 확인을 위해 마르코프 체인(Markov chain)을 이용하여 정량화 할 수 있다. 상기 문제해결 정보처리 효율 정량화부는, 문제해결 과정에서 발생되는 과거 에피소드가 강화학습 정책에 반영 되는 정보 압축 효율과 강화학습 정책으로부터 도출되는 행동의 최적성 비율을 에피소드-정책-행동으로 이어지 는 마르코프 체인상의 상호 정보량(mutual information)을 이용하여 계산할 수 있다. 상기 행동의 최적성 비율은 최적의 문제해결을 위해 에피소드 정보를 강화학습 정책결정에 반영하는 정보전달 체계를 나타내는 지표가 될 수 있다. 상기 강화학습 모델은, 인간이 학습한 정책 정보를 신뢰성 있게 인코딩하는 모델 기반 제어와 모델 없는(model- free) 제어를 결합한 계산 모델일 수 있다."}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "실시예들에 따르면 인간의 강화학습 과정이 가진 일반화 능력을 강화학습 알고리즘으로 과적합 없이 알고리즘화 할 수 있는, 일반화 가능한 인간 모사형 강화학습 알고리즘 설계를 위한 정책 신뢰도, 정보처리 효율 및 일반화 능력의 정량화 방법 및 장치를 제공할 수 있다."}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 실시예들을 설명한다. 그러나, 기술되는 실시예들은 여러 가지 다른 형태로 변 형될 수 있으며, 본 발명의 범위가 이하 설명되는 실시예들에 의하여 한정되는 것은 아니다. 또한, 여러 실시"}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "예들은 당해 기술분야에서 평균적인 지식을 가진 자에게 본 발명을 더욱 완전하게 설명하기 위해서 제공되는 것 이다. 도면에서 요소들의 형상 및 크기 등은 보다 명확한 설명을 위해 과장될 수 있다. 현재 강화학습(RL) 알고리즘은 일부 문제에 대해서 인간 지능을 뛰어넘는 해결 능력을 보이지만, 아래와 같은 측면에서는 인간의 강화학습이 우수하다. 인간의 강화학습은 데이터 수가 부족하여도 비교적 잘 학습되는 최소 지도(minimal supervision) 학습이 가능하 며, 생물의 인지능력(cognitive resource) 한계에 대응하여 낮은 에너지 소모와 높은 성능을 보이는 고효율 학 습이 일어난다. 이러한 학습 능력으로 인해 인간의 강화학습은 궁극적으로 다양한 작업(multi-task)으로의 일 반화(generalization)를 가능하게 한다. 아래의 본 발명의 실시예들은 자율적, 고효율, 일반화 능력을 갖는 인간 모사형 강화학습 알고리즘의 설계에 필 수적인 다각적 정량화 프로세스를 제안한다. - 프로세스 1. 정책 신뢰도 정량화 프로세스: 문맥 의존적인 인간의 강화학습 행동 데이터는 매우 복잡한 시간- 공간적 상관관계를 가지고 있어 역 강화학습 과정에서 과적합이 일어나기 쉽다. 이를 방지하기 위해 다음과 같이 강화학습 알고리즘의 정책 신뢰도를 정량화한다. 작업 매개변수와 인간의 행동 프로파일간의 매핑 함수를 근사화하고, 작업 매개변수와 강화학습 알고리즘의 행동 프로파일간의 매핑 함수를 근사화한 후, 두 매핑 함수 를 비교하는 정량화 프로세스(도 3(b))를 실행한다. - 프로세스 2. 일반화 능력 검증 프로세스: 인간의 강화학습 과정 모사형 알고리즘의 궁극적인 목적인 일반화 능력의 정밀한 검증을 위해, 실제 문제의 복잡도와 문맥 변화를 매개변수화 시킨 연속적 작업공간에서 샘플링된 일련의 작업에 대한 성능(작업 일반화 가능성)을 검증하는 프로세스(도 3(c))를 제공한다. - 프로세스 3. 문제해결 정보처리 효율 정량화 프로세스: 문맥 변화에 따라 문제해결 정책을 변화시키는 인간 강화학습 모사형 알고리즘의 적응 능력(앞에서 설명한 정책 신뢰도 정량화 프로세스(프로세스 1)로 정량화)과 다양한 종류의 문제해결을 위한 일반화 능력(앞에서 설명한 일반화 능력 검증 프로세스(프로세스 2))와의 “유 기적 연결성”을 확인하기 위하여 마르코프 체인(Markov chain) 관점에서의 정량화(에피소드 인코딩 효율) 프로 세스를 제공한다. 문제해결 과정에서 생기는 과거 에피소드가 강화학습 정책에 반영되는 정보 압축 효율과 강 화학습 정책으로부터 도출되는 행동의 최적성 비율을 에피소드-정책-행동으로 이어지는 마르코프 체인상의 상호 정보량(mutual information)을 이용해 계산한다. 이 비율은 최적의 문제해결/작업수행을 위해 에피소드 정보를 강화학습 정책결정에 반영하는 정보전달 체계를 나타내는 지표가 된다. 상기 3가지 프로세스는 모두 기존에 없는 새로운 기술이다. 본 발명은 “일반화 가능한 인간의 강화학습 능력 ”을 과적합 없이 알고리즘화 할 수 있음을 실제로 보인 최초의 사례이다. 이러한 일련의 프로세스를 통해 과적합 없는 고신뢰도의 일반화 가능한 인간의 강화학습 모사형 알고리즘 설계 가 가능함을 보였으며, 또한 이는 기존의 단순한 역 강화학습 과정만으로는 구현할 수 없음을 보였다. 프로세스 1의 지표인 정책 신뢰도(reliability) 측면에서는, 최신 강화학습 알고리즘 대비 5배 이상 향상시킬 수 있다. 프로세스 2의 지표인 일반화 능력(generalizability)을 12.8% 향상시킬 수 있다. 프로세스 3의 지표 인 에피소드 인코딩 효율 대비 최적행동 효과를 약 100% 향상시킬 수 있다. 이는 아래에서 제안 기술을 이용한 실증연구 결과를 통해 보다 상세히 설명한다. 강화학습 알고리즘은 생물의 도파민 시스템과 유사하게 가치 기반 (value-based)의 학습을 통해 문제를 해결한 다. 최근의 연구에서는 딥러닝 기반의 강화학습 알고리즘(예컨대 알파고, 알파제로 등)이 등장하여 바둑과 같 이 복잡한 문제에 대해서도 인간의 지능을 뛰어넘는 성능을 보여준다. 그러나 이러한 고성능 강화학습 알고리 즘은 인간 지능의 특성을 전부 놓치고 있기에 그 성능에 한계가 명확히 존재한다. 일반적인 인공지능 강화학습 알고리즘은 학습에 있어서 많은 데이터를 필요로 하고, 효율보다는 성능을 높이는 것을 목표로 하며, 특정 문제 상황을 해결하는 것에 특화되어 있어 다양한 문제로의 일반화가 불가능하다. 반 면, 인간의 강화학습 과정은 이와 반대로 적은 데이터 수에 비해 학습 가능한 뛰어난 최소 지도 학습(minimal supervision learning)의 특성이 있으며, 생물학적 인지능력의 한계에 따라 에너지 소비를 줄이며 학습하는 고 효율의 특성이 있고, 특히 특정 문제 상황에만 국한되지 않고 다양한 상황에 대한 일반적 지능을 갖는 특성이 있다. 이와 같은 인간의 강화학습 과정의 장점만을 인공지능 강화학습 알고리즘으로 이식하기 위해서는 다음과 같은 접근 방법이 필요하다. 인간 강화학습 모사형 강화학습 알고리즘을 최적화한다. 강화학습 알고리즘의 인간 지능적 특징을 확인(행동 수준)한다: 해당 강화학습 알고리즘을 통해 시뮬레이션된 행동은 인간 지능의 행 동과 유사한 형태를 보이는지 다양한 행동 프로파일을 통해 직접적으로 비교할 수 있다. 강화학습 알고리 즘의 인간 지능 특징을 확인(매개변수 수준)한다: 각 강화학습 알고리즘을 통해 추출된 시뮬레이션 행동은 다시 각 강화학습 알고리즘으로 재학습되어 매개변수 수준에서의 변화 유무를 통해 인간 지능의 특징을 유지하는지 검증할 수 있다. 정보 이론 수준에서 인간 지능의 특성을 검증한다: 행동과 환경 간의 상호 정보량(mutual information)의 비교를 통해 자연 지능의 특성을 분석할 수 있다. 특히 상호 정보량은 그 분포를 통해 특정 강 화학습 알고리즘이 각 자연 지능의 특성에 대해 얼마나 높은 신뢰도를 갖는 알고리즘인지 분석할 수 있다. 이와 같이 제안된 본 발명은 인공지능 강화학습 알고리즘에 결여된 인간 지능의 장점을 함양하도록 강화학습 알 고리즘을 개발하고 검증하는 기술을 다룬다. 이러한 개발 및 다른 강화학습 알고리즘과의 비교를 통한 검증 방 법은 기존에 유사한 연구 사례가 없는 독자적 기술이다.본 발명은 인간의 강화학습 과정이 가진 일반화 능력을 강화학습 알고리즘으로 이식하는데 필수적인 정량화 프 로세스를 포함한다. 역 강화학습을 통해 도출된 모델이 작업의 문맥 변화를 정책에 얼마나 반영하고 있는 지에 대한 정량화, 매개변수화된 작업공간으로부터 샘플링된 작업들에 대한 일반화 능력 정량화, 마지막으 로, 정보 이론 관점에서 환경으로부터 행동으로 연결되는 정보의 전환 및 이동 과정이 핵심적인 인간 지능 의 행동 원리를 제대로 반영하고 있는지를 정량화 함으로써, 고 신뢰도의 일반화 가능한 강화학습 알고리즘을 설계할 수 있다. 도 1은 일 실시예에 따른 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 방법을 나타내는 흐름 도이다. 도 1을 참조하면, 일 실시예에 따른 컴퓨터를 통해 수행되는 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 방법은, 인간의 강화학습 과정이 가진 일반화 능력을 강화학습 모델로 이식하기 위해, 역 강화학습 을 통해 도출된 강화학습 모델이 작업의 문맥 변화를 정책에 얼마나 반영하고 있는지에 대한 정량화를 수행하는 정책 신뢰도 정량화 단계(S110)를 포함할 수 있다. 또한, 일반화 능력의 정밀한 검증을 위해, 작업의 실제 문제의 복잡도와 문맥 변화를 매개변수화 한 작업공간에 서 샘플링된 작업에 대한 일반화 가능성을 검증하는 일반화 능력 검증 단계(S120)를 더 포함할 수 있다. 또한, 환경으로부터 행동으로 연결되는 정보의 전환 또는 이동 과정이 핵심적인 인간 지능의 행동 원리를 제대 로 반영하고 있는지를 정량화하는 문제해결 정보처리 효율 정량화 단계(S130)를 더 포함할 수 있다. 아래에서 일 실시예에 따른 컴퓨터를 통해 수행되는 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정 량화 방법의 각 단계를 보다 상세히 설명한다. 일 실시예에 따른 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 방법은 일반화 가능한 인간 모 사형 강화학습 모델 설계를 위한 정량화 장치를 예를 들어 설명할 수 있다. 도 2는 일 실시예에 따른 일 실시예에 따른 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 장치 를 개략적으로 나타내는 블록도이다. 도 2를 참조하면, 일 실시예에 따른 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 장치는 정책 신뢰도 정량화부를 포함하여 이루어질 수 있고, 실시예에 따라 일반화 능력 검증부 및 문제해결 정보처리 효율 정량화부를 더 포함할 수 있다. 정책 신뢰도 정량화 단계(S110)에서, 정책 신뢰도 정량화부는 인간의 강화학습 과정이 가진 일반화 능력을 강화학습 모델로 이식하기 위해, 역 강화학습을 통해 도출된 강화학습 모델이 작업의 문맥 변화를 정책에 얼마 나 반영하고 있는지에 대한 정량화를 수행할 수 있다. 작업, 즉 인간이 학습을 경험하는 모든 상황에서 인간의 강화학습은 다양한 문맥(예컨대, 환경의 불확실성, 복 잡도, 보상 조건 등) 변화에 따라서 특정한 행동 양식을 보이는 식의 정책의 변화를 통해 대응한다. 예를 들어, 환경의 불확실성이 높아지는 문맥 변화가 생기는 경우 인간이 목표 지향적 행동을 보이는 것의 효용성이 없기에 이를 지양하는 정책을 선택한다. 역 강화학습을 통해 인간을 모사한 강화학습 모델 역시 동일한 정책을 보이는지 그것을 검증하는 것이 필요하다. 문맥 변화에 따른 행동 양식 변화(즉, 정책의 변화)를 정량화하기 위한 방법으로는 다양한 방법이 제시될 수 있으나, 대표적으로 회귀 분석을 통해 특정 문맥 변화가 정책 변화에 기여하는 영향을 회귀 계수를 통해 정량화할 수 있다. 보다 구체적으로, 정책 신뢰도 정량화 단계(S110)는 작업의 작업 매개변수와 인간의 행동 프로파일간의 매핑 함 수를 근사화하는 단계, 작업 매개변수와 강화학습 알고리즘의 행동 프로파일간의 매핑 함수를 근사화하는 단계, 및 근사화된 두 개의 매핑 함수를 비교하는 단계를 포함하여 이루어질 수 있다. 여기서, 강화학습 모델은 인간이 학습한 정책 정보를 신뢰성 있게 인코딩하는 모델 기반 제어와 모델 없는 (model-free) 제어를 결합한 계산 모델일 수 있다. 또한, 강화학습 모델은 목표 매칭(goal matching, GM), 행 동 복제(behavior cloning, BC) 및 정책 매칭(policy matching, PM)의 학습 방법을 통해 구축될 수 있다. 이 는 아래에서 보다 상세히 설명한다. 일반화 능력 검증 단계(S120)에서, 일반화 능력 검증부는 일반화 능력의 정밀한 검증을 위해, 작업의 실제 문제의 복잡도와 문맥 변화를 매개변수화 한 작업공간에서 샘플링된 작업에 대한 일반화 가능성을 검증할 수 있다. 일반화 능력은 인간이 갖는 학습 특성으로, 한 작업에서 보이는 문맥 변화에 따른 정책 변화 특성을 다른 작업 에 있어서도 동일하게 보이는 것이다. 특정한 작업을 학습하고 보상을 최대화하기 위해 보인 인간의 강화학습 특성, 즉 문맥 변화에 따른 정책 변화를 성공적으로 반영한 모델(즉, 단계(S110)을 통해 검증된)은 문제의 복잡 도 등 다른 문맥이 변화하는 작업에서도 인간이 보였던 특성을 통해 일반화 가능한 성능을 보이는 것을 확인할 수 있다. 이를 폭 넓게 검증하기 위해 문제의 복잡도 및 문맥 변화를 매개변수화 및 이를 조절하여 다양한 작 업을 만들고 이에 노출시켜 그 성능을 통해 일반화 능력을 검증할 수 있다. 문제해결 정보처리 효율 정량화 단계(S130)에서, 문제해결 정보처리 효율 정량화부는 환경으로부터 행동으 로 연결되는 정보의 전환 또는 이동 과정이 핵심적인 인간 지능의 행동 원리를 제대로 반영하고 있는지를 정량 화할 수 있다. 인간 지능의 행동 원리는 자원의 효율적 분배에 있다. 문맥의 변화에 따라, 인지적 노력이 많이 필요하지만 확 실한 고성능을 보일 수 있는 목표 지향적 행동을 보일 때도 있고 효율성을 강조한 습관적 행동을 보일 때도 있 다. 일반적으로, 인간은 두 정책의 적절한 분배를 통해 고성능이며 고효율인 행동 양식을 갖는다. 이 적절한 정책의 변화가 일어나는 지 정량화하기 위해, 두 종류의 상호정보량(mutual information)을 활용할 수 있다. 첫째는 이전의 경험과 현재의 선택 사이의 상호정보량으로, 이 값이 낮다면 정보의 압축을 통한 효율적 선택으 로 이해할 수 있다(효율성 지표). 둘째는 현재의 선택과 현재의 선택지 중 최고의 보상 값을 갖는 선택(최적 선 택) 사이의 상호정보량으로, 이 값이 높다면 고성능으로 볼 수 있다(성능 지표). 두 상호정보량의 비율(성능 지 표/ 효율성 지표)을 통해 인간 지능의 행동 원리를 복원하는 지 그 정보처리의 효율을 정량화할 수 있다. 문제해결 정보처리 효율 정량화부는 문맥 변화에 따라 문제해결 정책을 변화시키는 인간 모사형 강화학습 모델의 정책 신뢰도 정량화부를 통한 적응 능력과 일반화 능력 검증부를 통한 문제해결을 위해 검증 된 일반화 능력과의 연결성 확인을 위해 마르코프 체인(Markov chain)을 이용하여 정량화 할 수 있다. 또한, 문제해결 정보처리 효율 정량화부는 문제해결 과정에서 발생되는 과거 에피소드가 강화학습 정책에 반영되는 정보 압축 효율과 강화학습 정책으로부터 도출되는 행동의 최적성 비율을 에피소드-정책-행동으로 이 어지는 마르코프 체인상의 상호 정보량(mutual information)을 이용하여 계산할 수 있다. 여기서, 행동의 최적성 비율은 최적의 문제해결을 위해 에피소드 정보를 강화학습 정책결정에 반영하는 정보전 달 체계를 나타내는 지표가 될 수 있다. 아래에서 일 실시예에 따른 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 방법 및 장치에 대해 보다 상세히 설명한다. 심층 강화학습(deep RL) 모델은 최소한의 지도(supervision)로 다양한 유형의 작업을 해결할 수 있는 큰 잠재력 을 보여주었지만, 제한된 경험에서 빠르게 학습하고, 환경 변화에 적응하며, 단일 작업에서 학습을 일반화한다 는 측면에서 몇 가지 핵심 과제가 남아 있다. 의사결정 신경과학의 최근 증거는 인간의 뇌가 이러한 문제들을 해결할 수 있는 선천적인 능력을 가지고 있다는 것을 보여주었고, 이는 샘플 효율적이고 적응적이며 일반화될 수 있는 RL 알고리즘에 대한 신경과학에서 영감을 받은 해결책 개발에 대한 낙관론으로 이어졌다. 여기에서는 전두엽(prefrontal) RL이라고 부르는 모델 기반 제어와 모델 없는(model-free) 제어를 적응적으로 결합한 계산 모델이 인간이 학습한 높은 수준의 정책 정보를 신뢰성 있게 인코딩하는 것을 보여주며, 이 모델은 학습된 정책을 광범위한 작업에 일반화할 수 있다. 먼저, 피험자들이 2단계 마르코프 의사결정 과제를 수행하는 동안 수집된 82명의 피실험자의 데이터에 대해 전 두엽 RL, 심층 RL, 메타 RL 알고리즘을 훈련시켰는데, 이 과정에서 목표, 상태-변환 불확실성, 상태-공간 복잡 성을 실험적으로 조작했다. 잠재적 행동 프로파일과 매개변수 회복성 시험을 조합한 신뢰도 시험에서, 전두엽 RL이 인간 피험자의 잠재된 정책을 신뢰성 있게 학습한 반면, 다른 모든 모델은 이 시험을 통과하지 못했다는 것을 보여주었다. 둘째, 이러한 모델들이 본래의 작업에서 배운 것을 일반화하는 능력을 실증적으로 시험하기 위해, 그것들을 환경 변동성 문맥에 배치했다. 구체적으로, 10가지 다른 마르코프 의사결정 작업으로 대규모 시뮬레이션을 실행했는데, 이 작업에서 잠재적 문맥 변수는 시간이 지남에 따라 변화한다. 실시예들에 따른 정 보이론적 분석은 전두엽 RL이 가장 높은 수준의 적응성과 성공적 인코딩 효과를 보인다는 것을 알 수 있다. 이 것은 두뇌가 일반적인 문제를 해결하는 방법을 모방한 컴퓨터 모델이 기계학습의 주요 난제에 대한 실질적인 해결책으로 이어질 수 있는 가능성을 공식적으로 시험하기 위한 첫 번째 시도이다. 본 발명은 다음과 같은 근본적인 질문을 검토한다. 알고리즘이 인간으로부터 일반화할 수 있는 정책을 배우는 것이 가능한가? 이를 위해 이 문제를 신뢰도 시험과 경험적 일반화 시험의 전제조건으로 두 가지 공식 시험으로"}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "삼는다. 본 발명의 작업은 다음과 같이 요약된다. 인간의 잠재 정책 학습. 여기에서는 82명의 피험자의 데이터를 다양한 RL 모델에 장착했는데, 각 모델은 심층 RL, 메타 RL, 전두엽 RL 등 다양한 방식으로 모델 없는 제어 및 모델 기반 제어를 구현한다. 여기서 목표, 상 태-변환 불확실성, 상태-공간 복잡성이 실험적으로 조작되는 2단계 마르코프 의사결정 과제를 수행하는 인간 참 가자들로부터 수집된 데이터를 사용했다. 신뢰도 시험. 엄격한 잠재적 행동 프로파일 회복성 시험을 사용하여 모델 기반 제어와 모델 없는 제어(전두엽 RL이라 함)를 적응적으로 결합한 계산 모델의 잠재 정책이 인간 피험자와 질적으로 유사하지만, 다른 모든 모델 은 효과를 재현하지 못한다. 경험적 일반화 능력 시험. 원래 작업에서 학습한 내용을 일반화하는 모델의 능력을 시험하기 위해, 시간에 따라 잠재적 상황 변수가 변화하는 10가지 마르코프 의사결정 작업으로 대규모 시뮬레이션을 실행했다. 여기서 전두 엽 RL이 가장 높은 수준의 적응성과 성공적 인코딩 효능을 보인다는 것을 발견했다. 이 작업은 컴퓨팅 모델이 인간의 잠재된 정책을 신뢰성 있게 학습할 수 있는 가능성을 공식적으로 시험하기 위 한 첫 번째 시도이다. 더욱이 이 접근방식은 기계학습의 주요 난제에 대한 실질적인 해결책을 제공하여 보다 인간과 같은 지능을 설계할 수 있게 해준다. 인간의 잠재 정책 학습 도 3은 일 실시예에 따른 인간의 잠재 정책 학습, 신뢰도 시험 및 경험적 일반화 시험을 설명하기 위한 도면이 다. 도 3의 (a)를 참조하면, 인간과 유사한 방식으로 작업을 배우고 수행하는 RL 모델을 구축하기 위해 목표 매칭 (goal matching, GM), 행동 복제(behavior cloning, BC), 정책 매칭(policy matching, PM) 등 3가지 훈련 방법 을 고려한다. 여기서 인간의 잠재 정책 학습이라고 부르는 이 과정은 인간의 행동 데이터에서 직접 행동 정책 을 학습하기 위한 것이다. RL 모델은 작업 환경과 상호 작용하여 향후 예상되는 보상의 양을 최대화하므로 훈련에는 인간의 행동 데이터를 사용하지 않는다. 그러나 모델을 훈련시키는 데 사용되는 작업(목표)은 인간 피험자가 수행하는 과제와 정확히 같다. 따라서 이 방법을 목표 매칭(GM)이라고 부른다. 정책 매칭(PM)은 목표 매칭(GM)과 행동 복제(BC)가 결합되어 목표 매칭과 행동 복제를 모두 달성할 수 있다. 구체적으로, RL 모델은 인간이 보상 극대화를 수행하는 방식을 모방하는 방식으로 훈련된다. 각 훈련 에폭 (epoch)에는 RL 모델이 보상(목표 매칭)을 극대화하기 위한 작업의 에피소드를 완성하고, 이후 모델의 행동과 인간 대상 행동의 차이를 손실함수(행동 복제)로 환산한다. 이 방법은 이전에 신경 데이터를 설명하기 위한 계 산 모델을 훈련하기 위해 사용되었다(비특허문헌 1). 표준 역 RL 방법은 빠른 문맥 변화를 가진 작업에 직접 적 용할 수 없기 때문에 여기에서는 표준 역 RL 방법을 고려하지 않음에 주목해야 한다. 실제로 시간이 지남에 따 라 보상가치와 환경통계가 모두 변하며, 샘플 크기가 너무 작은 보상함수를 역 RL 방법으로 추정하는 것은 거의 불가능하다(과제당 약 400회의 실험). 도 4는 일 실시예에 따른 실험에 사용된 RL 모델의 구조를 설명하기 위한 도면이다. 도 4를 참조하면, 실험을 위해 심층 RL(비특허문헌 2), 메타 RL(비특허문헌 3, 4), 전두엽 RL(비특허문헌 1, 5)의 세 가지 RL 모델을 사용했다. 첫 번째 유형은 DDQN이라고도 알려진 Double DQN(심층 RL)으로 구현되었다. 그것은 모델 없는 RL에 근접한 대표적인 심층 RL 모델 중 하나이다. 이 모델(각각 GM-DDQN, PM-DDQN)을 훈련하 기 위해 목표 매칭과 정책 매칭 방법을 모두 사용했다. 두 번째 유형은 메타 RL(meta RL)로 구현되었다. 이 모델은 모델 없는 RL 및 모델 기반 RL을 모두 수용한다. 특히, 메타 RL은 환경 문맥 변화에 적응적으로 반응하는 것으로 알려져 있다. 이 모델(각각 GM-metaRL, PM-metaRL)을 교육하기 위해 목표 매칭과 정책 매칭 방법을 모두 사용했다. 세 번째 유형의 RL 모델은 측면 전두엽 피질 및 복측 선조체(전두엽 RL)의 신경 활동을 설명하기 위해 연산 모 델로 구현되었다. 이 모델에는 기준 모델(비특허문헌 1)과 적응형 모델(비특허문헌 6)의 두 가지 버전이 있다. 이 모델들은 모델 없는 RL과 모델 기반 RL 사이에서 동적으로 중재함으로써 작업을 학습한다. 구체적으로는 모 델 없는 RL 및 모델 기반 RL 전략에 할당된 제어의 정도를 시험별로 조정하며, 이 하향 조정 신호는 각 RL 전략 의 예측 신뢰도에 근거하여 계산한다. 정책 매칭 방법을 사용하여 이 두 모델(PM-pfcRL1과 PM-pfcRL2)을 학습 하였다. 이전 연구에서는 이러한 모델을 데이터에 적합시키는 데 이 방법이 효과적이지 않다는 것을 밝혀냈기 때문에 이 경우에는 목표 매칭을 사용하지 않았다. 뇌에서 영감을 받은 RL 모델의 신뢰도 도 3의 (b)에 도시된 바와 같이, RL 모델이 인간 행동과 잠재 정책을 얼마나 신뢰성 있게 모방하는지를 평가하 기 위해 신뢰도 시험을 실시했다. 이 시험은 인간이 과제를 수행하면서 학습한 고도의 정책 정보를 인코딩할 수 있는 능력을 검증한다. 이 과정은 잠재 행동 프로파일링과 회복성 시험으로 구성된다. 인간이 작업로부터 배우는 잠재 정책을 평가하는 한 가지 일반적인 방법은 잠재적 작업 매개변수(예: 목표 및 상태-변환 불확실성)가 행동에 미치는 영향을 정량화하는 것이다. 이 척도는 학습 에이전트가 환경구조의 변화 에 대응해 어떻게 행동을 변화시키는지 반영한다. 각각 주어진 작업 매개변수 θ와 행동 데이터 x에 대해 잠재 행동 프로파일 h는 다음 식과 같이 정의된다. [수학식 1]"}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, h는 다항식 기능이나 신경망과 같은 매개변수화된 기능일 수 있다. 에이전트의 작업 수행이 문맥 변경 과 무관하거나 에이전트가 임의로 선택하는 경우, 효과 크기(즉, h의 매개변수 값)는 0이 된다. 여기에서는 일 반 선형 모델을 h로 간단하게 사용한다. 잠재 동작 프로파일 회복성 시험의 목적은 인간의 잠재 정책과 RL 모델의 잠재 정책 사이의 일관성을 평가하는 것이다. 모델의 매개변수를 인간 피실험자의 데이터 xHuman에 맞춘 후, 원래 작업에서 원래 피팅 모델로 시뮬레 이션을 실행하여 시뮬레이션한 데이터 xModel을 생성한다. 그런 다음 xHuman과 xModel에 대해 각각 잠재 행동 프로 파일링을 실시한다. 이들 두 잠재적 프로파일 간의 유의미한 양의 상관관계는 RL 모델이 학습한 잠재 정책이 인간의 잠재 정책과 유사하다는 것을 나타낸다. 신뢰도 시험을 위해, 잠재 행동 프로파일의 회복성을 조사하기 위해 6가지 RL 모델(도 4)과 임의 에이전트를 제 어조건으로 하여 일련의 실험을 실시했다. 첫 번째 단계에서는 82명의 피험자 데이터(도 3(b)의 xHuman)에 대한 전두엽 RL, 메타 RL, 심층 RL을 교육했다. 피험자들이 2단계 마르코프 의사결정 작업을 수행하는 동안 데이터 집합이 수집되었다. 두 번째 단계에서는 모든 RL 모델이 동일한 2단계 마르코프 의사결정 작업을 수행하는 다 른 시뮬레이션 집합을 실행하여 또 다른 행동 데이터 집합(도 3(b)의 xModel)을 수집했다. 그런 다음 잠재적 행 동 프로파일 hHuman, hModel을 다음 식과 같이 계산했다. [수학식 2]"}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, θTask는 작업 매개변수를 나타낸다. 이는 1000개 이상의 모델 피팅 공정을 포함한 대규모 실험이다: 7 (모델) × 82 (대상) × 2 (훈련 및 재교육) 도 5은 일 실시예에 따른 신뢰도 시험 결과를 나타내는 도면이다. 도 5의 (a)를 참조하면, 시뮬레이션 결과를 보여준다. RL 모델과 피험자 대상 간 동작 매칭을 정량화하는 모델 피팅 면에서는 PM-meta RL이 가장 높은 성능을 보였고, 전두엽 RL과 심층 RL이 그 뒤를 이었다. 예상대로 목표 매칭으로 훈련된 RL 모델은 상대적으로 피팅 성능이 떨어지는 것으로 나타났다. 그러나 잠재 행동 프로파일의 체계적인 회복 분석에서 전두엽 RL 모델(PM-pfcRL2)의 잠재 행동 프로파일이 인간 피험자와 질적으로 유사한 반면, 도 5의 (b)를 참조하면 다른 모든 RL 모델은 효과를 복제하지 못했다. PM 방 법으로 훈련된 메타 RL의 경우 의한 상관관계를 보였지만, 이 모델이 작업을 수행하는 방식이 인간의 그것과 근 본적으로 다를 수 있음을 나타내는 음의 상관관계가 있다. 도 5의 (c)에 도시된 바와 같이, 상관관계의 가파름 과 유의성을 모두 고려한 적합도 통계량을 계산할 때 이 효과는 더 극적으로 나타난다. 전두엽 RL 모델(PM- pfcRL2)의 효과 크기는 다른 모든 RL 모델의 효과 크기보다 3배 이상 크다. 이러한 결과는 단순히 인간의 행동 을 모사하는 것(도 5(a))은 에이전트가 실제로 인간의 잠재 정책(도 5(b), 도 5(c))을 학습하는 것을 의미하지 는 않는다는 것을 시사한다. 뇌에서 영감을 받은 RL 모델의 경험적 일반화 능력 도 6는 일 실시예에 따른 각 RL 모델의 일반화 시험을 위한 시뮬레이션 환경을 설명하기 위한 도면이다. 도 6를 참조하면, 모델들이 원래 작업에서 다른 작업으로 배운 것을 일반화할 수 있는 능력을 경험적으로 시험 하기 위해(도 3(c)) 환경 변동성의 문맥에서 모델을 배치했다. 앞에서 설명한 바와 동일한 RL 모델 집합을 사 용하여 각각 다른 방식으로 잠재적 상황 변수를 조작하는 10개의 서로 다른 마르코프 의사결정 과제로 대규모 시뮬레이션을 실행했다. 작업은 작업 구조(사다리(Ladder) 및 트리(Tree))와 작업 불확실성(고정(Fixed), 드리 프트(Drift), 스위치(Switch), 드리프트 + 스위치(Drift+Switch))의 두 가지 작업 매개변수를 체계적으로 조작 해 만들어졌다. 도 6의 (b)에 도시된 바와 같이, 작업 구조는 사다리와 트리 타입을 사용하였다. 도 6의 (c) 에 도시된 바와 같이, 작업 불확실성 변동에 대해, 4가지 다른 유형의 상태 전환 함수를 검토했는데, 각각의 상 태 전환 확률 값은 시험마다 다른 방식으로 변경되었다. 첫 번째 유형(\"고정\")은 고정 상태 변환 확률을 사용한다. 두 번째 유형(\"드리프트\")은 무작위 보행에 따른 상 태 변환 확률을 사용하며, 상태 변환 확률 값은 상대적으로 느리게 변경된다. 세 번째 유형(\"스위치\")은 각각 낮은 불확실성과 높은 불확실성을 가진 조건이라는 두 가지 다른 상태 변환 조건 사이에서 번갈아 나타난다. 이 작업에서 학습 에이전트는 작업 구조의 급격한 변화를 경험하며, 신속하게 적응할 필요가 있다. 네 번째 유 형(\"드리프트 + 스위치\")은 두 번째 유형과 세 번째 유형의 혼합물이다. 도 6의 (d)에 도시된 바와 같이, 각 작업의 전체 구성을 나타낼 수 있다. Task 1과 Task 10은 뇌의 RL 과정을 조사하는 이전 연구에서 사용된 작업 에 해당한다(비특허문헌 1). 도 7는 일 실시예에 따른 RL 모델의 적응 능력에 대한 시뮬레이션 결과를 나타내는 도면이다. 경험적 일반화 능력을 시험하기 위해, 원래 데이터 집합에 대해 훈련된 6개의 RL 모델(앞에서 언급한 RL 모델) 이 10개의 마르코프 의사결정 과제를 수행하는 시뮬레이션을 실행했다. 여기에는 총 4,920개의 시뮬레이션(= 82개 과제(subject) × 6 RL 모델 × 10개 작업(task))이 포함되었다. 모든 작업에 걸친 평균 성능은 경험적 일반화 능력을 나타내며, 각 작업에 대한 성능은 서로 다른 상황에서 해당 모델의 적응 능력을 나타낸다. 도 7 을 참조하면, 전두엽 RL 모델이 가장 높은 수준의 일반화 능력을 보인다는 것을 발견할 수 있다. [표 1]"}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "특히, 표 1을 참조하면, PM-pfcRL2는 10개 작업 중 9개 작업을 성공적으로 해결하고, 9개 작업 중 8개 작업에서 정규화 보상으로 가장 높은 점수를 받았다. GM-metaRL과 PM-pfcRL1은 두 번째로 좋은 성능을 보였다. PM- pfcRL1의 성능은 GM-metaRL과 동일했지만 PM-pfcRL1은 6개 작업 중 5개 작업에서 월등히 우수한 성적을 거뒀다. 이러한 결과를 종합하면, 전두엽 RL 모델(PM-pfcRL1 및 PM-pfcRL2)이 원래 작업에서 배운 것을 일반화할 수 있 는 최고의 능력을 가지고 있음을 시사한다.RL 모델의 일반화 능력을 정량화하기 위해 잠재적 정보-이론적 척도를 제공할 수 있다. 일반화 능력의 성격을 보다 잘 이해하기 위해 정보이론적 분석을 실시했다. 이 분석은 사건의 과거 에피소드의 관측에서 RL 모델 의 작용으로 전달되는 정보의 양과, 그 작용의 최적성의 정도를 정량화하기 위해 설계되었다. 일반성이 높 을수록 RL 모델이 에피소드 정보를 보다 효과적으로 인코딩하여 최적의 작용을 발생시킨다는 가설을 세웠다. 이와 같이, 모델의 일반화 능력을 에피소드 사건 및 에이전트의 행동(비특허문헌 7)(\"에피소드 인코딩 효과\")에서 얻은 상호 정보뿐만 아니라, 에이전트의 행동과 최적의 행동(\"선택적 최적성\")으로 정량화할 수 있을 것으로 기대한다. 최적의 행동은 작업의 매개변수 변경에 대해 충분히 알고 있다고 가정하여 이상적인 에 이전트가 취한 행동으로 정의되었다. 에피소드 인코딩 효과는 로 정의되며, 여기서 Ft-1과 at는 시 도 t-1에서의 에피소드 변수 및 시도 t에서의 행동이다. 선택 최적성은 로 정의되며, 여기서 at와 at*는 각각 RL 에이전트와 이상적인 에이전트의 선택(행동)이다. 여기서 일반화할 수 있는 RL 에이전트의 한 가지 기본적인 요구사항은 과거 에피소드에서 그것의 행동과 작업 수행으로 정보를 전송하는 능력이라고 가정했 다. 따라서 \"episodic encoding efficacy\"라고 불리는 에피소드 인코딩 효과와 선택적 최적성의 상관관계는 RL 모델의 일반화 능력을 나타내는 하나의 잠재적 정보-이론적 지표가 될 수 있다. 도 8은 일 실시예에 따른 에피소드 인코딩 효과 검증을 나타내는 도면이다. 도 8의 (a)를 참조하면, 각 RL 모델이 10개의 마르코프 의사결정 과제를 수행하는 동안 이 두 가지 척도를 계산 했다. 그런 다음, 도 8의 (b)에 도시된 바와 같이, 이 척도를 사용하여 비율 및 에피소드 인코딩 효과의 대용으로서 적합도 통계량을 계산했다. 여기서 전두엽 RL(PM-pfcRL1과 PM-pfcRL2 둘 다)이 가장 높은 수준의 에피소드 인코딩 효과를 보인다는 것을 발견했다. 특히, 도 8의 (c)에 도시된 바와 같이, 가장 일반화할 수 있는 모델인 PM-pfcRL2는 10개 작업 중 8개 작업에서 에피소드 인코딩 효과와 선택적 최적성 사이에 유의미한 상관관계를 보였다. 또한, 경험적 일반화 능력(도 7)은 대부분 성공적 인코딩 효과(도 8(b))의 R2와 일치한다는 점에 주목한다. 이러한 결과는 3가지 중요한 의미를 갖는다. 첫째, 에피소드 인코딩 효과는 일반화 능력의 성격을 더 잘 이해하는데 도움을 준다. 둘째, 에피소드 인코딩 효과는 에이전트의 일반 화 능력을 계량화할 수 있는 좋은 후보가 될 수 있다. 이 척도는 매우 일반화할 수 있는 RL 알고리즘 설계에 직접 사용될 수 있다. 실시예들은 인간 지능의 모든 행동은 고차원적인 인지 기능에 근거하여 일어나므로, 이 행동을 예측하여 활용할 가치가 있는 모든 분야에서 응용될 수 있다. 일례로, 인간의 문맥 의존적인 강화학습 과정을 모사하는 모델을 이용하여 인간 행동의 보조에 있어서 효율적으로 대응하는 시스템을 구축하여 인간이 훌륭한 성과를 거둘 수 있 도록 보조할 수 있다. 사물인터넷(Internet-of-Things, IoT) 분야에서는 다양한 기기를 컨트롤 해야 하므로 각 기기의 컨트롤에 활용 되는 인지 기능이 다양할 수 있다. 이 때, 실시예들에 따른 시스템의 범용성은 각 기기를 제어함에 있어서 요 구되는 인지 상태의 종류 차이에 관계없이 인간을 보조할 수 있을 뿐만 아니라, 이미 구축된 IoT 생태계에 새로 운 기기가 포함이 되었을 때도 과적합 없이 행동을 예측할 수 있는 AI를 개발할 수 있다. 또한, 다양한 문제로의 일반화 능력은 인간의 작업 수행 지능과도 직결되므로, 실시예들에 따른 기술을 통해 복 잡한 의사결정이 중요한 판사, 의사, 금융 전문가, 군사 작전 지휘관 등에 대한 작업 수행능력 프로파일링이 가 능하다. 또한 스마트 교육을 위한 맞춤형 시스템의 기반 기술로도 활용이 가능하다. 실시예들에 따른 기술을 이용하여 도출되는 인간의 강화학습 모사형 알고리즘은 인간의 의사결정의 핵심과정을 이해하는 도구로도 활용될 수 있다. 기존의 AI는 이러한 인간의 의사결정 과정에 대한 이해가 존재하지 않으나, 인간의 행동 특성을 그대로 예측하는 AI의 개발을 통해 로보틱스 분야에서는 인간의 행동을 더 잘 예측 하고 보조하는 AI를 개발할 수 있으며, 게임 분야에서는 인간과 자연스러운 상호작용이 가능한 더욱 지능적인 AI 엔진을 개발할 수 있다. 한편, 현행 광고 제안 기술은 인간의 과거 검색 기록을 바탕으로 새로운 광고를 추천하고 있다. 그러나 이러한 광고 제안 기술은 개별 인간의 행동 특성에 대한 이해가 결여되어 있어 사용자의 관심범위와 완전히 동떨어진 광고를 제안하는 경우가 많다. 실시예들에 따른 기술을 활용하면 인간-AI 간의 공진화를 통해 사용자의 행동범위 내에 존재하는 광고를 추천할 수 있다. 이상과 같이, 인간 지능의 특성을 함양하도록 하는 인간 모방형 인공지능의 설계는 단순히 인간의 행동을 더욱 유사하게 예측할 수 있을 뿐만 아니라, 그 특성이 학습과 성능의 효율에 있으므로 더욱 적은 노력으로 더 나은 결과를 얻을 수 있다는 점에서 인공지능 산업 전반에 적용 가능한 유익한 기술이다. 특히, 강화학습은 문제 해 결 및 의사결정에 큰 도움이 되므로 인간을 포함한 지능적 판단이 필요한 모든 인공지능 개발에 중요하다. 인공지능의 개발은 특정 문제 상황에 대한 해결을 위해 상당한 계산과 시간 자원이 투자됨에도 불구하고 그 인 공지능이 다양한 문제 해결이 아닌 특정 문제 해결에만 적용 가능하다는 큰 단점이 있다. 이와 반대로 본 시스 템은 일반화 가능한 알고리즘의 개발이 가능해 다양한 문제 해결에 적용될 수 있다. 개발 중 및 개발된 모든 인공지능의 자연 지능적 특성 검증에 적용 가능하다. 인간 지능을 모사하여 인간의 인 지 과정을 예측하고자 하는 모델은 쉽게 과적합의 오류에 빠지기 때문에 반드시 이러한 과적합의 오류를 제거해 야 한다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 예를 들어, 프로 세서, 컨트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴 퓨터, FPA(field programmable array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상 의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설"}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치 는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 컨트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치에 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터 는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2020-0126999", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2020-0126999", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 방법을 나타내는 흐름 도이다. 도 2는 일 실시예에 따른 일 실시예에 따른 일반화 가능한 인간 모사형 강화학습 모델 설계를 위한 정량화 장치 를 개략적으로 나타내는 블록도이다. 도 3은 일 실시예에 따른 인간의 잠재 정책 학습, 신뢰도 시험 및 경험적 일반화 시험을 설명하기 위한 도면이 다. 도 4는 일 실시예에 따른 실험에 사용된 RL 모델의 구조를 설명하기 위한 도면이다. 도 5은 일 실시예에 따른 신뢰도 시험 결과를 나타내는 도면이다. 도 6는 일 실시예에 따른 각 RL 모델의 일반화 시험을 위한 시뮬레이션 환경을 설명하기 위한 도면이다. 도 7는 일 실시예에 따른 RL 모델의 적응 능력에 대한 시뮬레이션 결과를 나타내는 도면이다. 도 8은 일 실시예에 따른 에피소드 인코딩 효과 검증을 나타내는 도면이다."}
