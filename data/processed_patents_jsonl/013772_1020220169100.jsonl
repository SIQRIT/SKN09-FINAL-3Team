{"patent_id": "10-2022-0169100", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0057297", "출원번호": "10-2022-0169100", "발명의 명칭": "신경망 모델을 학습시키는 방법 및 전자 장치", "출원인": "삼성전자주식회사", "발명자": "주재용"}}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측(prediction)된 제1 객체 인식 결과를 획득하는 단계(S1310);상기 제1 카메라에 대응되는 제1 카메라 좌표계와 상기 공간을 제2 시점으로 촬영하는 제2 카메라에 대응되는제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하는 단계(S1320);상기 제1 영상에 대응되는, 상기 제2 카메라에서 촬영된 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링(labeling)을 수행함으로써 학습 데이터를 생성하는 단계(S1330); 및상기 생성된 학습 데이터를 이용하여, 제2 신경망 모델을 학습시키는 단계(S1340);를 포함하는, 신경망 모델을 학습시키는 방법."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 예측된 제1 객체 인식 결과를 변환하는 단계(S1320)는,상기 변환 관계에 기초하여, 상기 제1 영상 내에서 검출 및 식별된 객체의 위치와 방향을 표현하는 특징점들의공간 좌표들로 이루어진 3차원 포즈 정보를 상기 제2 영상 내에서 검출 및 식별되는 상기 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이루어진 2차원 포즈 정보로 변환하고,상기 학습 데이터를 생성하는 단계(S1330)는상기 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 상기 2차원 포즈 정보가 상기제2 영상 내의 상기 객체의 2차원 포즈 정보로 이용되도록 라벨링을 수행함으로써 상기 학습 데이터를생성하는, 방법."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항 또는 제2 항에 있어서,상기 예측된 제1 객체 인식 결과를 변환하는 단계(S1320)는,상기 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계에 기초하여, 상기 제1 카메라 좌표계에서의 상기 3차원포즈 정보를 상기 월드 좌표계에서의 3차원 포즈 정보로 변환하는 단계;상기 월드 좌표계와 상기 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 월드 좌표계에서의 3차원 포즈정보를 상기 제2 카메라 좌표계에서의 3차원 포즈 정보로 변환하는 단계; 및상기 제2 카메라 좌표계에서의 공간 좌표를 제2 카메라의 제2 영상 좌표계로 투영함으로써, 상기 제2 카메라 좌표계에서의 3차원 포즈 정보를 상기 제2 영상 좌표계에서의 상기 2차원 포즈 정보로 변환하는 단계를 포함하는,방법."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항 내지 제3 항 중 어느 한 항에 있어서,상기 제1 카메라 좌표계와 상기 월드 좌표계 간의 변환 관계는,제1 카메라 캘리브레이션을 수행하여, 상기 제1 카메라의 내부 파라미터와 소정의 개수의 특징점들에 대한 상기월드 좌표계의 공간 좌표와 상기 제1 카메라의 영상 좌표계의 영상 좌표의 매칭 쌍들을 이용하여 상기 제1 카메라의 외부 파라미터를 구함으로써 획득되고,공개특허 10-2024-0057297-3-상기 특징점들은 상기 제1 카메라의 관측 시야 내에서 전자 장치(100)를 이동시켜, 소정의 검출 영역의 중심점이나, 상기 전자 장치(100)에 부착한 마커 또는 상기 전자 장치(100)의 외관 특징을 검출한 것인, 방법."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항 내지 제4 항 중 어느 한 항에 있어서,상기 학습 데이터를 생성하는 단계(S1330)는,상기 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 상기 2차원 포즈 정보 중 상기 제2 영상 내에 해당하는 제1 부분과 상기 제2 영상 내에 해당하지 않는 제2 부분을 구분하여 라벨링을 수행함으로써, 상기 학습 데이터를 생성하는, 방법."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항 내지 제5 항 중 어느 한 항에 있어서,상기 제2 카메라에 의해 상기 제2 시점으로 촬영된 복수 개의 제2 영상들이 있는 경우,상기 학습 데이터를 생성하는 단계(S1330)는,상기 복수 개의 제2 영상들 각각에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을수행함으로써 학습 데이터를 생성하는, 방법."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항 내지 제6 항 중 어느 한 항에 있어서,상기 제2 시점으로 촬영된 복수 개의 제2 영상들은,상기 제2 카메라가 상기 제2 시점을 유지한 상태에서 동적 객체를 소정의 시간 간격으로 촬영된 영상인, 방법."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항 내지 제7 항 중 어느 한 항에 있어서,상기 제2 카메라에 의해 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들이 있는 경우,상기 변환하는 단계(S1320)는,상기 제1 카메라 좌표계와 상기 복수 개의 제2 시점들 각각에서의 상기 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하고,상기 학습 데이터를 생성하는 단계(S1330)는,상기 복수 개의 제2 영상들 각각에 대하여, 상기 복수 개의 제2 시점들 각각으로 변환된 제1 객체 인식 결과에기초한 라벨링을 수행함으로써 학습 데이터를 생성하는, 방법."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 항 내지 제8 항 중 어느 한 항에 있어서,상기 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들은,상기 제2 카메라가 전자 장치(100)에 탑재된 상태로 정적 객체 주위를 돌거나 상기 정적 객체와의 거리를 변경하면서 상기 서로 다른 복수 개의 제2 시점들로 촬영된 것인, 방법."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항 내지 제9 항 중 어느 한 항에 있어서,상기 학습 데이터를 생성하는 단계(S1330)는,상기 제2 카메라를 탑재한 전자 장치(100)가 상기 객체를 소정의 기준보다 낮은 수준으로 인식하거나, 상기 전자 장치(100)가 이용하는 공간 맵에서 특정된 위치로 상기 전자 장치(100)가 이동한 경우, 상기 학습 데이터를공개특허 10-2024-0057297-4-생성하는, 방법."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "하나 이상의 인스트럭션을 저장하는 메모리(110);상기 메모리(110)에 저장된 상기 하나 이상의 인스트럭션을 실행하는 프로세서(120); 카메라(131); 및통신부(140)를 포함하고,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 통신부(140)를 통해, 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 외부 장치(300)의 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측(prediction)된 제1 객체 인식 결과를상기 외부 장치(300)로부터 획득하고,상기 외부 장치(300)의 카메라에 대응되는 제1 카메라 좌표계와 상기 공간을 제2 시점으로 촬영하는 상기 카메라(131)에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하고,상기 제1 영상에 대응되는, 상기 카메라(131)에서 촬영된 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링(labeling)을 수행함으로써 학습 데이터를 생성하고, 상기 생성된 학습 데이터 이용하여, 제2 신경망 모델을 학습시키는, 전자 장치(100)."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 변환 관계에 기초하여, 상기 제1 영상 내에서 검출 및 식별된 객체의 위치와 방향을 표현하는 특징점들의공간 좌표들로 이루어진 3차원 포즈 정보를 상기 제2 영상 내에서 검출 및 식별되는 상기 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이루어진 2차원 포즈 정보로 변환하고,상기 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 상기 2차원 포즈 정보가 상기제2 영상 내의 상기 객체의 2차원 포즈 정보로 이용되도록 라벨링을 수행함으로써 상기 학습 데이터를생성하는, 전자 장치(100)."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11 항 또는 제12 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계에 기초하여, 상기 제1 카메라 좌표계에서의 상기 3차원포즈 정보를 상기 월드 좌표계에서의 3차원 포즈 정보로 변환하고,상기 월드 좌표계와 상기 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 월드 좌표계에서의 3차원 포즈정보를 상기 제2 카메라 좌표계에서의 3차원 포즈 정보로 변환하고,상기 제2 카메라 좌표계에서의 공간 좌표를 카메라의 제2 영상 좌표계로 투영함으로써, 상기 제2 카메라 좌표계에서의 3차원 포즈 정보를 상기 제2 영상 좌표계에서의 상기 2차원 포즈 정보로 변환하는, 전자 장치(100)."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11 항 내지 제13 항 중 어느 한 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,제1 카메라 캘리브레이션을 수행하여, 상기 외부 장치(300)의 카메라의 내부 파라미터와 소정의 개수의 특징점들에 대한 상기 월드 좌표계의 공간 좌표와 상기 외부 장치(300)의 카메라의 영상 좌표계의 영상 좌표의 매칭공개특허 10-2024-0057297-5-쌍들을 이용하여 상기 외부 장치(300)의 외부 파라미터를 구함으로써 상기 제1 카메라 좌표계와 상기 월드 좌표계 간의 변환 관계를 획득하고,상기 특징점들은 상기 외부 장치(300)의 카메라의 관측 시야 내에서 상기 전자 장치(100)를 이동시켜, 소정의검출 영역의 중심점이나, 상기 전자 장치(100)에 부착한 마커 또는 상기 전자 장치(100)의 외관 특징을 검출한것인, 전자 장치(100)."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11 항 내지 제14 항 중 어느 한 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 상기 2차원 포즈 정보 중 상기 제2 영상 내에 해당하는 제1 부분과 상기 제2 영상 내에 해당하지 않는 제2 부분을 구분하여 라벨링을 수행함으로써, 상기 학습 데이터를 생성하는, 전자 장치(100)."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11 항 내지 제15 항 중 어느 한 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 카메라(131)에 의해 상기 제2 시점으로 촬영된 복수 개의 제2 영상들이 있는 경우, 상기 복수 개의 제2 영상들 각각에 대하여, 상기 제2 시점으로변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성하는, 전자 장치(100)."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11 항 내지 제16 항 중 어느 한 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 카메라(131)에 의해 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들이 있는 경우,상기 제1 카메라 좌표계와 상기 복수 개의 제2 시점들 각각에서의 상기 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하고, 상기 복수 개의 제2 영상들 각각에 대하여, 상기 복수 개의 제2 시점들 각각으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성하는,전자 장치(100)."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11 항 내지 제17 항 중 어느 한 항에 있어서,상기 프로세서(120)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 전자 장치(100)가 상기 객체를 소정의 기준보다 낮은 수준으로 인식하거나, 상기 전자 장치(100)가 이용하는 공간 맵에서 특정된 위치로 상기 전자 장치(100)가 이동한 경우, 상기 학습 데이터를 생성하는, 전자 장치(100)."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "하나 이상의 인스트럭션을 저장하는 메모리(210);상기 메모리(210)에 저장된 상기 하나 이상의 인스트럭션을 실행하는 프로세서(220); 및통신부(230)를 포함하고,상기 프로세서(220)는 상기 하나 이상의 인스트럭션을 실행함으로써,적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 획득하고,상기 통신부(230)를 통해, 상기 공간을 제2 시점으로 촬영하는 제2 카메라에서 촬영된 제2 영상을 획득하고,상기 제1 카메라에 대응되는 제1 카메라 좌표계와 상기 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관공개특허 10-2024-0057297-6-계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하고,상기 제1 영상에 대응되는 상기 제2 영상에 대하여, 상기 제2 시점 기준으로 변환된 제1객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성하고, 상기 생성된 학습 데이터 이용하여, 제2 신경망 모델을 학습시키는, 클라우드 서버(200)."}
{"patent_id": "10-2022-0169100", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19 항에 있어서,상기 프로세서(220)는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 통신부(230)를 통해, 상기 학습된 제2 신경망 모델의 네트워크 파라미터 값들을 상기 제2 신경망 모델을탑재한 전자 장치(100)에 전송하는, 클라우드 서버(200)."}
{"patent_id": "10-2022-0169100", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 획득하고, 제1 카메라에 대응되는 제1 카메라 좌표계와 공 간을 제2 시점으로 촬영하는 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환하고, 제1 영상에 대응되는, 제2 카메라에서 촬영된 제2 영상에 대하여, 제2 시점으로 변 환된 제1 객체 인식 결과에 기초한 라벨링(labeling)을 수행함으로써 학습 데이터를 생성하고, 생성된 학습 데이 터를 이용하여, 제2 신경망 모델을 학습시키는, 신경망 모델을 학습시키는 방법 및 전자 장치가 개시된다."}
{"patent_id": "10-2022-0169100", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "신경망 모델을 학습시키는 방법 및 전자 장치에 관한 것이다."}
{"patent_id": "10-2022-0169100", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인터넷은 인간이 정보를 생성하고 소비하는 인간 중심의 연결 망에서, 사물 등 분산된 구성 요소들 간에 정보를 주고 받아 처리하는 IoT(Internet of Things, 사물인터넷) 망으로 진화하고 있다. 클라우드 서버 등과의 연결을 통한 빅데이터(Big data) 처리 기술 등이 IoT 기술에 결합된 IoE (Internet of Everything) 기술도 대두되고 있다. IoT는 기존의 IT(information technology)기술과 다양한 산업 간의 융합 및 복합을 통하여 스마트 가전, 스마트홈, 스마트 빌딩, 스마트 시티 등의 분야에 응용될 수 있다. IoT 환경에서 서로 연결된 전자 장치들 각각은 데이터를 수집, 생성, 분석, 또는 가공하고, 상호 간에 데이터를 서로 공유하여, 각 장치의 태스크에 활용할 수 있다. 최근에는 컴퓨터 비전 분야의 비약적인 발전에 따라, 비전 태스크를 수행하는 신경망 모델을 활용하는 다양한 종류의 전자 장치가 개발되고 있다. 이에 따라, IoT 환경에 서 다양한 종류의 전자 장치 간의 연결에 대한 관심이 고조되고 있다."}
{"patent_id": "10-2022-0169100", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따르면, 신경망 모델을 학습시키는 방법이 제공된다. 신경망 모델을 학습시키는 방법은 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입력으로 하 는 제1 신경망 모델로부터 예측(prediction)된 제1 객체 인식 결과를 획득하는 단계를 포함한다. 또한, 신경망 모델을 학습시키는 방법은 상기 제1 카메라에 대응되는 제1 카메라 좌표계와 상기 공간을 제2 시점으로 촬영하 는 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환하는 단계를 포함한다. 또한, 신경망 모델을 학습시키는 방법은 상기 제1 영상에 대응되는, 상기 제2 카메 라에서 촬영된 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링(labeling)을 수행함으로써 학습 데이터를 생성하는 단계를 포함한다. 또한, 신경망 모델을 학습시키는 방법은 상기 생성된 학습 데이터를 이용하여, 제2 신경망 모델을 학습시키는 단계를 포함한다. 본 개시의 일 실시예에 따르면, 신경망 모델을 학습시키는 전자 장치가 제공된다. 신경망 모델을 학습시키는 전 자 장치는 하나 이상의 인스트럭션을 저장하는 메모리, 상기 메모리에 저장된 상기 하나 이상의 인스트럭션을 실행하는 프로세서, 카메라, 및 통신부를 포함한다. 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 상기 통신부를 통해, 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 외부 장치의 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 상기 외부 장치로부터 획득한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 상기 외부 장치의 카메라에 대응되는 제1카메라 좌표계와 상기 공간을 제2 시점으로 촬영하는 상기 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관 계에 기초하여, 상기 예측된 제1 객체 인식 결과를 변환한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행 함으로써, 상기 제1 영상에 대응되는, 상기 카메라에서 촬영된 제2 영상에 대하여, 상기 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 상기 생성된 학습 데이터 이용하여, 제2 신경망 모델을 학습시킨다. 본 개시의 일 실시예에 따르면, 신경망 모델을 학습시키는 클라우드 서버가 제공된다. 신경망 모델을 학습시키 는 클라우드 서버는 하나 이상의 인스트럭션을 저장하는 메모리, 상기 메모리에 저장된 상기 하나 이상의 인스 트럭션을 실행하는 프로세서, 및 통신부를 포함한다. 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 적어 도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입력으로 하는 제 1 신경망 모델로부터 예측된 제1 객체 인식 결과를 획득한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행 함으로써, 상기 통신부를 통해, 상기 공간을 제2 시점으로 촬영하는 제2 카메라에서 촬영된 제2 영상을 획득한 다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 상기 제1 카메라에 대응되는 제1 카메라 좌표계 와 상기 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 상기 예측된 제1 객체 인식 결 과를 변환한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 상기 제1 영상에 대응되는 상기 제2 영상에 대하여, 상기 제2 시점 기준으로 변환된 제1객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이 터를 생성한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 상기 생성된 학습 데이터 이용하여, 제2 신경망 모델을 학습시킨다."}
{"patent_id": "10-2022-0169100", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 개시에 대해 구체적으로 설명하기로 한다. 본 개시에 서, \"a, b 또는 c 중 적어도 하나\" 표현은 \"a\", \"b\", \"c\", \"a 및 b\", \"a 및 c\", \"b 및 c\", \"a, b 및 c 모두\", 혹은 그 변형들을 지칭할 수 있다. 본 개시에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의 미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 또한, 본 명세서에서 사용되는 '제1' 또는 '제2' 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용할 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만 사용된다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소 프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 본 개시에 따른 인공지능과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등 과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인 공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인 공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미 한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도 형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경 망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의 해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 이하 첨부된 도면을 참고하여 본 개시를 상세히 설명하기로 한다. 도 1은 전자 장치와 외부 장치들이 서로 연결되는 댁내 IoT 환경을 설명하기 위한 도면이다. 본 개시에서 전자 장치는 로봇 청소기인 것을 전제로 설명하나, 사용자의 편의를 위해 구동되는 다양한 종 류의 어시스턴트 로봇이나 모바일 장치, AR(Augmented Reality) 기기, VR(Virtual Reality) 기기, 주변 환경을 감지하고 특정 위치 또는 공간에서 소정의 서비스를 제공하는 기기일 수 있다. 전자 장치는 공간을 스캔하 고 공간 내의 객체를 검출하기 위한 다양한 종류의 센서와 신경망(Neural Network) 모델을 탑재할 수 있다. 예 를 들어, 전자 장치는 카메라와 같은 이미지 센서, LDS(Laser Distance Sensor)와 같은 라이다(LiDAR, Light Detection And Ranging) 센서, ToF(Time of Flight) 센서 중 적어도 하나를 구비할 수 있다. 전자 장치 는 DNN(Deep Neural Network), CNN(Convolution Neural Network), RNN(Recurrent Neural Network), BRDNN(Bidirectional Recurrent Deep Neural Network)과 같은 모델을 적어도 하나 탑재할 수 있고, 이들을 조 합적으로 이용할 수도 있다. 전자 장치와 서로 연결되는 외부 장치들은 클라우드 서버와 다양한 종류의 IoT 기기들(300-1, 300-2, 300-3)일 수 있다. IoT 기기들은 도 1에 도시된 바와 같이, 버틀러 로봇(300-1), 애완로봇(300-2), 스마트 홈카 메라(300-3) 등이 될 수 있으나, 이에 한정되는 것은 아니며, 전자 장치와 동종의 기기일 수 있다. 버틀러 로봇(300-1), 애완로봇(300-2), 스마트 홈카메라(300-3) 각각은 구비된 다양한 종류의 센서를 이용하여 공간을 스캔하고 공간 내의 객체를 검출할 수 있다. 본 개시의 일 실시예에 따르면, 전자 장치, 버틀러 로봇(300-1), 애완로봇(300-2), 및 스마트 홈카메라 (300-3)는 각각에서 수집된 공간 스캔 정보 또는 객체 정보를 이용하여 공간 맵을 적어도 하나의 객체를 포함하 는 공간에 대한 공간 정보로써 생성하고 저장할 수 있다. 전자 장치, 버틀러 로봇(300-1), 애완로봇(300- 2), 및 스마트 홈카메라(300-3)는 공간 스캔 정보나 객체 정보, 또는 공간 맵을 상호 간에 송신 또는 수신하여 저장함으로써 서로 공유할 수 있다. 동일한 공간 내에 있는 장치들이라하더라도, 각 장치의 위치나 각 장치의 성능 또는 센싱 레인지, 각 장치가 고 정형인지 이동형인지, 각 장치가 동작하는 행태 등에 따라, 서로 다른 시점(viewpoint)에서 서로 다른 화각으로 공간을 스캔하고 객체를 검출하기 때문에, 어느 하나의 장치에서 획득된 영상 또는 음성을 비롯한 센싱 정보가 다른 장치에 탑재된 인공지능 모델을 학습시키는데 유용하게 활용될 수 있다. 본 개시의 일 실시예에 따르면, 전자 장치, 버틀러 로봇(300-1), 애완로봇(300-2), 및 스마트 홈카메라 (300-3) 중 어느 하나의 기기가 마스터 디바이스 또는 서버 디바이스가 될 수 있고, 나머지 기기들은 슬레이브 디바이스 또는 클라이언트 디바이스가 될 수 있다. 마스터 디바이스 또는 서버 디바이스에 해당하는 기기는 다 른 IoT 기기들로부터 공간 스캔 정보나 객체 정보, 또는 공간 맵을 수신하여, 저장하고 관리할 수 있다. 마스터 디바이스 또는 서버 디바이스에 해당하는 기기는 수신된 정보들을 위치 별로 분류하여 저장하고 관리할 수 있다. 예를 들어, 마스터 디바이스 또는 서버 디바이스에 해당하는 기기는 동일한 공간인지, 동일한 구역인지, 또는 동일한 지역인지에 따라, 공간 스캔 정보나 객체 정보, 또는 공간 맵을 분류하여 취합하고 관리할 수 있다. 마스터 디바이스 또는 서버 디바이스에 해당하는 기기는 저장하고 있던 제1 정보를 동일한 위치에 대응되 는 제2 정보로 업데이트함으로써, 해당 위치와 관련된 정보의 최신성 및 적확성을 유지할 수 있다. 본 개시의 일 실시예에 따르면, 전자 장치, 버틀러 로봇(300-1), 애완로봇(300-2), 및 스마트 홈카메라 (300-3)는 공간 스캔 정보나 객체 정보, 또는 공간 맵을 클라우드 서버로 전송하여, 클라우드 서버를 통해 공간 스캔 정보나 객체 정보, 또는 공간 맵을 저장하고 관리할 수 있다. 예를 들어, IoT 기기들의 전원이 꺼져 있거나 IoT 기기에서 특정 기능을 실행 중이어서 전자 장치에 공간 스캔 정보나 객체 정보, 또는 공 간 맵의 송신이 불가능한 경우, 전자 장치는 클라우드 서버에 공간 스캔 정보나 객체 정보, 또는 공 간 맵을 요청하여 수신할 수 있다. 도 1에 도시된 바와 같이, 클라우드 서버는 전자 장치, 버틀러 로봇(300-1), 애완로봇(300-2), 스마 트 홈카메라(300-3) 각각으로부터 수신된 공간 스캔 정보나 객체 정보, 또는 공간 맵을 관리하고, 댁내 공간을 모니터링할 수 있다. 클라우드 서버는 복수의 IoT 기기들로부터 수집된 공간 스캔 정보나 객체 정보, 또는 공간 맵을 등록된 사용자의 계정이나 등록된 위치 별로 저장 및 관리할 수 있다. 예를 들어, 클라우드 서버 는 동일한 공간인지 또는 동일한 구역인지에 따라, 공간 스캔 정보나 객체 정보, 또는 공간 맵을 분류하여 취합하고 관리할 수 있다. 클라우드 서버는 댁내에 위치한 전자 장치, 버틀러 로봇(300-1), 애완로봇 (300-2), 스마트 홈카메라(300-3)의 요청에 응답하여, 공간 맵과 같은 댁내 공간에 대한 정보를 전송할 수 있다. 본 개시의 일 실시예에 따르면, 클라우드 서버 대신, 댁내 위치한 AI 허브(예를 들어, AI 스피커)가 댁내 IoT 기기로부터 공간 스캔 정보나 객체 정보, 또는 공간 맵을 수신하여, 저장하고 관리할 수 있다. AI 허브는 복수의 IoT 기기들로부터 수집된 공간 스캔 정보나 객체 정보, 또는 공간 맵을 댁내의 공간 또는 구역 별로 저 장 및 관리할 수 있다. 본 개시의 일 실시예에 따르면, 댁내 위치한 AI 허브는 클라우드 서버와 함께 공간 스캔 정보나 객체 정보, 또는 공간 맵을 저장하고 관리할 수 있다. 예를 들어, AI 허브는 공간 맵을 생성 또는 관리하기 위해, 공 간 스캔 정보 또는 객체 정보를 가공 내지 처리하거나, 개인 정보가 보호되도록 데이터를 변환하여 클라우드 서 버로 전송할 수 있다. 클라우드 서버는 AI 허브로부터 수신된 정보를 가공 내지 처리하여, 공간 스캔 정보나 객체 정보, 또는 공간 맵을 저장하고 관리할 수 있으며, AI 허브로 전송할 수 있다. 본 개시의 일 실시예에 따르면, 로봇 청소기와 같은 전자 장치는 청소와 같은 태스크를 수행하기 위해 공 간 맵을 이용할 수 있다. 이를 위해, 전자 장치는 다양한 종류의 센서를 이용하여 공간을 스캔하여 최신의 공간 스캔 정보로 공간 맵을 업데이트할 수 있다. 전자 장치는 직접 센싱한 정보뿐만 아니라, 댁내 IoT 환 경에서 서로 연결되는 클라우드 서버, 버틀러 로봇(300-1), 애완로봇(300-2), 스마트 홈카메라(300-3)로부 터 수신된 공간 맵의 일부 또는 전체를 이용하여 전자 장치에 저장된 공간 맵을 업데이트할 수 있다. 예를 들어, 로봇 청소기는 댁내의 공간을 청소하기 위해, 충전 스테이션에서 충전이 완료되면, 로봇 청소기에 저장된 공간 맵을 이용하여 청소를 수행할 수 있다. 로봇 청소기는 동일한 공간에 대한 청소를 수행하기 위해, 최근에 이용한 공간 맵을 그대로 이용할 수도 있다. 하지만, 이전 청소 시의 공간의 상태와 현재의 공간의 상태 가 서로 다르므로, 효율적인 청소를 수행하기 위해, 공간 내 위치한 객체들의 최신 정보를 공간 맵에 반영하는 것이 바람직하다. 이를 위해, 로봇 청소기는 충전 스테이션에서 출발하여, 주요 루트를 사전 주행하여, 공간 내 의 객체 정보를 직접 수집할 수 있다. 그러나 사전 주행을 하게 되면, 사전 주행을 위한 시간이 더 소요되고, 사전 주행에 따라 배터리가 더 소모될 수 있다. 이때, 로봇 청소기는 다른 로봇 청소기나 동일한 공간에 위치한 적어도 하나의 외부 장치로부터 최신의 공간 맵을 수신하여, 로봇 청소기에 저장된 공간 맵을 업데이트할 수 있 다. 로봇 청소기는 외부 장치로부터 수신된 공간 맵의 일부 또는 전부를 활용할 수 있다. 로봇 청소기는 동종의 로 봇 청소기로부터 수신된 공간 맵을 그대로 활용하거나 잦은 위치 변화가 예상되는 객체들에 관한 정보를 공간 맵의 업데이트에 활용할 수 있다. 로봇 청소기는 이종의 기기로부터 수신된 공간 맵이라 하더라도, 동일한 공간 에 대한 공간 맵의 일부 또는 전부를 공간 맵의 업데이트에 활용할 수 있다. 도 2a 및 도 2b는 공간 맵을 설명하기 위한 흐름도이다. 도 2a를 참조하면, 로봇 청소기인 전자 장치에 저장된 공간 맵과 공간 맵을 구성하는 복수의 레이어들 간 의 계층적 구조를 나타내고 있다. 도 2a에 도시된 바와 같이, 공간 맵은 베이스 레이어, 시맨틱 맵 레이어, 실 시간 레이어로 구성될 수 있으나, 이에 한정되는 것은 아니며, 태스크의 특성에 따라 레이어가 가감될 수 있다. 베이스 레이어는 벽, 기둥, 통로 등 공간 전체의 기본 구조에 관한 정보를 제공한다. 3차원 포인트 클라우드 데 이터를 처리하여, 좌표계를 정합하고, 위치를 저장함으로써, 베이스 레이어는 공간의 3차원 정보, 객체의 위치 정보, 이동 궤적 정보 등을 제공할 수 있다. 베이스 레이어는 베이스 맵과 지오메트릭 맵의 역할을 수행한다. 시멘틱 맵 레이어는 베이스 레이어 위에 시멘틱 정보를 제공하는 레이어이다. 전자 장치의 사용자는 베이 스 레이어의 공간 전체의 기본 구조에 'Room 1', 'Room 2', '접근 제한 구역' 등과 같은 시멘틱 정보를 부여하 여, 전자 장치의 태스크 수행에 활용할 수 있다. 예를 들어, 전자 장치가 로봇 청소기인 경우, 사용 자는 'Room 2'만 청소하게 하거나, '접근 제한 구역'은 로봇 청소기가 청소하지 않도록, 시멘틱 맵 레이어에 시 멘틱 정보를 설정할 수 있다. 실시간 레이어는 공간 내의 적어도 하나의 객체 정보를 제공하는 레이어이다. 객체는 정적 객체와 동적 객체 모 두 포함될 수 있다. 본 개시에서 실시간 레이어는 객체의 속성 정보에 기초한 복수의 레이어들을 포함할 수 있 으며, 레이어들 간의 계층적 구조를 가질 수 있다. 도 2a에 도시된 바와 같이, 실시간 레이어는 제1 레이어, 제 2 레이어, 제3 레이어를 포함할 수 있으나, 이에 한정되는 것은 아니며, 객체의 속성 정보의 분류 기준에 따라 레이어의 개수가 가감될 수 있다. 도 2a를 보면, 제1 레이어에는 시스템 옷장, 붙박이 장이 포함되고, 제2 레이 어에는 테이블과 소파가 포함되고, 제3 레이어에는 의자가 포함됨을 알 수 있다. 도 2b를 참조하면, 객체의 속성 정보에 기초한 복수의 레이어들을 포함하는 실시간 레이어의 다양한 예를 나타 내고 있다. 객체의 속성 정보는 객체의 종류, 형상, 사이즈, 높이 등과 같은 객관적인 기준이나 복수의 기준을 조합하여 분 류될 수 있는 정보일 수 있다. 또한, 객체의 속성 정보는 사용자 및 환경에 따라 달라질 수 있으므로, 객체 별 로 레이블링하여 속성 정보를 입력해 둘 수있다. 본 개시의 일 실시예에 따르면, 객체의 속성 정보가 객체의 이동성 레벨(ML, Mobability Level)인 경우, 제1 레 이어에는 ML 1에 해당하는 객체가 포함되고, 제2 레이어에는 ML 2와 ML 3에 해당하는 객체가 포함되며, 제3 레 이어에는 ML 4에 해당하는 객체가 포함될 수 있다. 객체의 이동성 레벨은 객체의 객관적인 특징을 이동성을 평 가하는 소정의 분류 기준에 적용함으로써 정해질 수 있다. 예를 들어, ML 1은 이동이 불가능한 객체, ML 2는 이 동이 가능하지만, 주로 고정된 상태로 있는 객체, ML 3는 이동이 가능하지만, 가끔 이동하는 객체, ML 4는 이동 가능하며, 자주 이동하는 객체에 각각 대응된다. 본 개시의 일 실시예에 따르면, 객체의 속성 정보가 객체의 위치 이동 주기(Position Movement Cycle)인 경우, 제1 레이어에는 1개월 내로 위치 이동이 없었던 객체가 포함되고, 제2 레이어에는 1개월 내로 위치 이동이 있었 던 객체가 포함되며, 제3 레이어에는 1주일 내로 위치 이동이 있었던 객체가 포함될 수 있다. 객체의 객관적 특 징에 기초하여 분류되는 이동성 레벨과 달리, 위치 이동 주기는 객체를 사용하는 사용자나 객체가 위치하는 환 경에 따라 동일한 객체라 하더라도, 위치 이동 주기는 다를 수 있다. 예를 들어, 'A'라는 객체는 제1 사용자가 자주 사용되는 객체인 반면, 제2 사용자는 거의 사용되지 않는 객체일 수 있다. 'B'라는 객체는 제1 장소에서는 자주 사용되는 객체인 반면, 제2 장소에서는 거의 사용되지 않는 객체일 수 있다. 본 개시의 일 실시예에 따르면, 객체의 속성 정보가 객체가 위치하는 높이(Height)인 경우, 제1 레이어에는 1m 이하에 해당하는 객체가 포함되고, 제2 레이어에는 1m 이상 2m 이하에 해당하는 객체가 포함되며, 제3 레이어에 는 2m를 초과하는 객체가 포함될 수 있다. 본 개시의 일 실시예에 따르면, 실시간 레이어에 포함되는 복수의 레이어들의 분류 기준은 사용자에 의해 정의 될 수 있다. 예를 들어, 사용자는 분류 기준에 대해 복수의 종류의 객체의 속성 정보를 조합하여 설정해둠으로 써, 태스크의 특성을 반영한 공간 맵을 생성할 수 있다. 예를 들어, 로봇 청소기의 경우, 일반적으로 50cm 높이 보다 아래에서 이동하기 때문에 1m 보다 높은 곳에 위치하는 객체들, 예를 들어, 전등이나 벽에 걸린 액자 등은 고려할 필요가 없다. 따라서, 사용자는 각 레이어를 구분하는 분류 기준을 직접 설정하여, 제1 레이어는 ML 1이 고 1m 이하에 위치하는 객체가 포함되고, 제2 레이어는 ML 2 또는 ML 3이고 1m 이하에 위치하는 객체가 포함되 며, 제3 레이어는 ML 4이고 1m 이하에 위치하는 객체가 포함되도록 할 수 있다. 도 3a, 도 3b, 도 3c, 도 3d는 공간 맵을 구성하는 레이어를 활용하는 방식을 설명하기 위한 도면이다. 전자 장치와 IoT 기기들의 종류나 태스크의 특성에 따라, 각 장치에서 이용되는 공간 맵이 서로 다를 수 있다. 전자 장치는 전자 장치에 저장된 기존의 공간 맵을 그대로 활용할 수도 있지만, 태스크를 수행 할 공간에 변화가 생긴 경우, 해당 변화를 반영하기 위해 공간 맵을 업데이트할 수 있다. 전자 장치는 공 간에 생긴 변화를 이미 반영하고 있는 공간 맵을 적어도 하나의 외부 장치로부터 수신하여, 기존의 공간 맵을 업데이트 할 수 있다. 전자 장치는 기존의 공간 맵을 기초로, 새로운 공간 맵을 생성할 수 있다. 도 3a를 보면, 전자 장치는 저장되어 있던 기존의 공간 맵(이하, 제1 공간 맵)을 불러올 수 있다. 제1 공 간 맵은 베이스 레이어, 제1 레이어, 제2 레이어, 및 제3 레이어로 구성되어 있다. 이하, 설명의 편의상, 제1 레이어 내지 제3 레이어가 도 2b의 임의의 분류 기준에 따라 객체를 포함하는 것을 전제로 설명한다. 제1 공간 맵이 불과 몇분 전에 생성되었거나 제1 공간 맵이 이용된 후로 공간 내에 변화가 없는 경우, 전자 장치는 제1 공간 맵을 그대로 활용하여, 새로운 공간 맵(이하, 제2 공간 맵)을 획득하고, 새로운 태스크를 수행하는데 제2 공간 맵을 이용할 수 있다. 도 3b를 보면, 전자 장치는 저장되어 있던 제1 공간 맵을 불러올 수 있다. 전자 장치가 태스크를 수 행함에 있어서, 자주 이동하는 ML 4의 객체 정보는 필요하지 않거나, 1주일 이상 이동이 없었던 객체 정보만 이 용하는 경우, 제1 공간 맵을 구성하는 레이어들 중에 베이스 레이어, 제1 레이어, 및 제2 레이어를 선별하거나 제1 공간 맵에서 제3 레이어를 제거하여, 제2 공간 맵을 획득할 수 있다. 도 3c를 보면, 전자 장치는 저장되어 있던 제1 공간 맵을 불러올 수 있다. 전자 장치가 새로운 태스 크를 수행함에 있어서, ML 1의 객체 정보만 필요하거나, 1개월 이상 이동이 없었던 객체 정보만 이용하는 경우, 제1 공간 맵을 구성하는 레이어들 중에 베이스 레이어 및 제1 레이어를 선별하거나 제1 공간 맵에서 제2 레이어와 제3 레이어를 제거하여, 제2 공간 맵을 획득할 수 있다. 도 3d를 보면, 전자 장치는 저장되어 있던 제1 공간 맵을 불러올 수 있다. 전자 장치가 새로운 태스 크를 수행함에 있어서, 이동이 가능한 ML 2, ML 3, 및 ML 4에 해당하는 객체들의 최신 정보를 반영할 필요가 경 우, 제1 공간 맵을 구성하는 레이어들 중에 베이스 레이어 및 제1 레이어를 선별하거나 제1 공간 맵에서 제2 레 이어와 제3 레이어를 제거하여, 제2 공간 맵을 획득할 수 있다. 이후, 전자 장치는 외부 장치로부터 수신 된 공간 맵에서 제2 레이어와 제3 레이어를 추출하여, 제2 공간 맵에 반영하여, 제3 공간 맵을 획득할 수 있다. 또는, 전자 장치에 구비된 적어도 하나의 센서를 이용하여, ML 2, ML 3, 및 ML 4에 해당하는 객체들을 검 출하여, 제2 공간 맵에 반영하여, 제3 공간 맵을 획득할 수 있다. 도 4는 본 개시의 일 실시예에 따른 공간 맵을 획득하는 방법을 설명하기 위한 흐름도이다. 전자 장치는 제1 공간 맵을 획득할 수 있다.(S410) 제1 공간 맵은 객체의 속성 정보에 기초한 복수의 레이 어들로 구성될 수 있다. 제1 공간 맵은 전자 장치에서 생성되었거나, 전자 장치의 외부 장치로부터 수신된 것일 수 있다. 전자 장치는 제1 공간 맵에 대한 업데이트가 필요한지 판단할 수 있다.(S420) 예를 들어, 전자 장치 는 태스크의 특성에 따라 제1 공간 맵의 업데이트가 필요한 지 판단할 수 있다. 태스크는 전자 장치 고유 의 용도 또는 전자 장치에서 실행할 수 있는 기능을 통해 전자 장치가 수행하도록 설정된 작업을 의 미한다. 태스크의 수행과 관련된 설정 정보는 사용자에 의해 전자 장치에 직접 입력되거나 모바일 장치나 전용 리모컨 등과 같은 단말을 통해 전자 장치로 전송될 수 있다. 예를 들어, 전자 장치가 로봇 청소 기인 경우, 로봇 청소기의 태스크는 댁내 청소 또는 사용자가 설정한 영역에 대한 청소이거나 예약 기능에 따른 예약 청소, 저소음 모드 청소 등일 수 있다. 전자 장치는 태스크의 수행에 이용되는 정보가 부족한 경우, 제1 공간 맵에 대한 업데이트가 필요한 것으로 판단할 수 있다. 전자 장치는 태스크가 수행될 공간 내의 객체 정보에 대한 최신 정보가 필요한 경우, 업데이트가 필요한 것으로 판단할 수 있다. 또는, 전자 장치 는 제1 공간 맵을 획득한 시점으로부터의 경과 시간이나 설정된 업데이트 주기에 따라 제1 공간 맵에 대한 업데 이트가 필요한지 판단할 수 있다. 제1 공간 맵에 대한 업데이트가 필요 없는 경우, 전자 장치는 제1 공간 맵을 태스크를 수행하는데 이용되는 제2 공간 맵으로써 활용할 수 있다. 제1 공간 맵에 대한 업데이트가 필요한 경우, 전자 장치는 객체 정보를 획득할 수 있다.(S430) 전자 장치 는 적어도 하나의 센서를 이용하여, 공간 스캔 정보 또는 객체 정보를 직접 수집할 수 있다. 전자 장치 는 외부 장치로부터 공간 맵의 일부 또는 전부를 수신하거나, 공간 스캔 정보 또는 객체 정보를 수신할 수 있다. 전자 장치는 획득된 공간 스캔 정보 또는 객체 정보를 이용하여, 제1 공간 맵에 공간 스캔 정보 또는 객체 정보를 업데이트할 수 있다.(S440) 예를 들어, 전자 장치는 이동이 잦은 객체에 대해서는 최신 위치 정보 가 반영되도록 해당 객체 정보 또는 해당 객체가 위치하던 곳의 공간 스캔 정보를 새로 획득하여, 제1 공간 맵 에 업데이트할 수 있다. 전자 장치는 제2 공간 맵을 획득할 수 있다.(S450) 전자 장치는 제1 공간 맵을 그대로 활용하거나, 일부 객체 정보나 일부 레이어가 수정된 형태의 제1 공간 맵을 활용하거나, 제1 공간 맵을 업데이트 함으로써, 제2 공간 맵을 획득할 수 있다. 한편, 태스크를 수행하기 위해 이용되는 제2 공간 맵은 전자 장치의 기능이나 태스크의 특성에 따라, 적절 한 형태의 맵으로 변형되거나 생성되어 이용될 수 있다. 예를 들어, 전자 장치가 로봇 청소기인 경우, 공 간 맵에 기초한 네비게이션 맵을 생성하여, 네비게이션 맵이 제공하는 이동 경로를 따라 청소를 수행할할 수 있 다. 본 개시의 일 실시예에서, 공간 맵은 서로 다른 장치들로부터 수집된 공간 스캔 정보 또는 객체 정보를 취합함 으로써 생성될 수 있다. 공간 맵은 수집된 각각의 정보가 어느 장치로부터 획득된 것인지 또는 어느 위치 및/또 는 시점으로부터 획득된 것인지 나타내는 메타데이터 형태의 정보를 더 포함할 수 있다. 예를 들어, 서로 다른 장치들이 서로 다른 위치 및/또는 시점에서 소정의 공간을 스캔하고 객체를 검출한 경우, 공간 맵은 각 장치들 로부터 획득된 영상 또는 영상으로부터 획득된 객체 인식 결과를 이용하여 생성될 수 있다. 공간 맵은 공간 내 의 각 객체에 대하여, 객체 인식 결과와 객체 인식 결과가 어느 위치 및/또는 시점으로부터 획득된 것인지를 나 타내는 정보가 태그 또는 라벨링(labeling)되어 있을 수 있다.도 5는 서로 다른 시점으로 촬영한 영상들 각각에서의 객체 인식 결과를 설명하기 위한 도면이다. 적어도 하나의 객체를 포함하는 공간에 위치한 장치들은 각 장치의 위치나 동작 방식, 각 장치에 구비된 카메라 에 따라, 서로 다른 시점으로 객체를 촬영할 수 있다. 도 5에 도시한 바와 같이, 외부 장치에 구비된 제1 카메라는 공간 내의 객체를 촬영하여, 제1 시점으로 촬영된 제1 영상을 획득할 수 있다. 전자 장치에 구비 된 제2 카메라는 공간 내의 객체를 촬영하여, 제2 시점으로 촬영된 제2 영상을 획득할 수 있다. 스마트 홈카메라와 같은 외부 장치에 구비된 제1 카메라는 버즈 아이(bird's eye) 뷰, 아이 레벨(Eye level) 뷰, 또는 하이 앵글(High Angle) 뷰 방식으로 객체를 촬영할 수 있으나, 고정형이라 모든 공간을 촬영할 수 없다. 비전 태스크를 수행하는 신경망 모델을 학습시키기 위한 라지-스케일(large scale) 데이터 셋은 일반 적으로 버즈 아이 뷰, 아이 레벨 뷰, 또는 하이 앵글 뷰 방식으로 촬영된 영상이 대다수이며, 로우 앵글(Low Angle) 뷰 방식으로 촬영된 영상에 비해 영상 내에서 객체를 잘 파악할 수 있다. 반면, 로봇 청소기와 같은 전자 장치는 이동 가능하므로 전자 장치에 구비된 제2 카메라는 공간의 이 곳 저곳을 촬영하는데 유리하나, 낮은 높이에 카메라가 장착되어 있기 때문에, 로우 앵글 뷰 방식으로 객체를 촬영하게 된다. 로우 앵글 뷰 방식으로 촬영된 영상은 시야가 제한적이어서, 객체의 일부만 포함되어 있거나 비 전 인식이 어려운 형태로 객체가 포함되어 있을 수 있다. 로우 앵글 뷰 방식으로 촬영된 영상의 학습 데이터 셋 의 확보가 어려움에 따라, 로우 앵글 뷰 방식으로 촬영된 영상을 입력으로 하는, 비전 태스크를 수행하는 신경 망 모델은 학습이 어려워, 객체 인식의 정확도 및 안정성이 떨어질 수 밖에 없다. 이를 보완하기 위한 학습 데 이터 셋의 확보에는 많은 비용이 소요되어 현실적인 어려움이 있다. 로봇 청소기와 같은 전자 장치는 실시 간 연산을 위해 고해상도의 영상을 입력받기 어려울 뿐더러, 고성능 카메라를 장착하는 경우 비용 이슈가 발생 하게 된다. 스마트 홈카메라와 같은 외부 장치에 탑재된 신경망 모델은 로봇 청소기와 같은 전자 장치에 탑재된 신경망 모델에 비해 상대적으로 비전 태스크 성능이 안정적이고 정확하다. 이하, 전자 장치가 외부 장치 에 탑재된 신경망 모델로부터 예측된 객체 인식 결과를 이용하여, 학습 데이터 셋에 제약이 있는 시점 예 를 들어, 로우 앵글 뷰 방식으로 촬영된 영상을 입력으로 하는 신경망 모델을 학습시키는 방법에 대해 설명한다. 도 6은 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 이용하여, 학습 데이터 를 생성함으로써, 제2 신경망 모델을 학습시키는 과정을 설명하기 위한 도면이다. 도 6에서는 설명의 편의상 전 자 장치가 제2 신경망 모델을 학습시키는 것을 나타내고 있으나, 이에 제한되는 것은 아니며, 제2 신경망 모델을 학습시키는 주체는 제2 신경망 모델을 관리하는 클라우드 서버일 수 있다. 이하, 도 6 내지 도 12 의 설명에서도 설명의 편의상 전자 장치를 제2 신경망 모델을 학습시키는 주체로 설명한다. 도 6을 참조하면, 외부 장치는 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라 를 포함하고 있다. 외부 장치에 탑재된 제1 신경망 모델은 제1 카메라에서 촬영된 제1 영상을 입력으로 하 여, 제1 객체 인식 결과를 출력할 수 있다. 제1 시점으로 촬영된 제1 영상은 버즈 아이 뷰, 아이 레벨 뷰, 또는 하이 앵글 뷰 방식으로 촬영된 영상일 수 있다. 전자 장치는 적어도 하나의 객체를 포함하는 공간을 제2 시점으로 촬영하는 제2 카메라를 포함하고 있다. 전자 장치에 탑재된 제2 신경망 모델은 제2 카메라에서 촬영된 제2 영상을 입력으로 하여, 제2 객체 인식 결과를 출력할 수 있다. 제2 시점으로 촬영된 제2 영상은 로 우 앵글 뷰 방식으로 촬영된 영상일 수 있다. 도 7은 제1 신경망 모델 또는 제2 신경망 모델의 일 예를 설명하기 위한 도면이다. 본 개시의 일 실시예에 따라 제1 신경망 모델 또는 제2 신경망 모델은 도 7에 도시된 바와 같은 비전 태스크를 수행하는 멀티 태스크 모델일 수 있다. 멀티-태스크 모델은 복수의 레이어들로 이루어지는 뉴럴 네트워크의 형 태일 수 있다. 멀티-태스크 모델은 쉐어드 백본(shared backbone) 레이어들과 쉐어드 백본의 출력을 각각 입력 으로 하는, 각 태스크 별 추가 레이어들로 이루어진 형태일 수 있다. 멀티-태스크 모델은 예측에 필요한 특징을 추출하는 특징 추출기(feature extractor)와 이로부터 추출된 특징을 이용해서 예측을 수행하는 복수의 예측기 (predictor)를 포함할 수 있다. 예를 들어, 비전 분야의 태스크를 수행하는 비전 모델의 경우, 특징 추출기에 각 태스크 별 예측 헤드(prediction head)를 연결한 형태의 멀티-태스크 모델을 사용할 수 있다. 다만, 본 개시 의 일 실시예에 따른 멀티-태스크 모델의 종류가 도 7에 개시된 모델에 한정되는 것은 아니다. 외부 장치에 탑재된 제1 신경망 모델과 전자 장치에 탑재된 제2 신경망 모델은 도 7에 도시된 바와 같은 멀티-태스크 모델일 수 있으나, 이에 제한되는 것은 아니다.도 7에 도시된 멀티 태스크 모델을 살펴보면, 쉐어드 백본에서 추출된 특징을 각 태스크 별 추가 레이어들에 입 력하여, 각 태스크 별 출력 값을 확보할 수 있다. 각 태스크 별 예측 헤드를 보면, 검출 및 2차원 포즈 회귀 분 기(detection & 2D pose regression branch), 뎁스 회귀 분기(depth regression branch), 재식별 특징 분기 (Re-ID branch)를 포함한다. 이에 따라, 각 태스크 별 예측 헤드의 출력 값은 객체 검출 정보, 2차원 포즈 정보, 3차원 포즈 정보, 및 재식별 특징 정보 등을 포함할 수 있다. 이때, 연속하는 영상에 대해 객체 검출 정 보 및 재식별 특징 정보를 통해 객체 추적을 수행하고, 객체의 동일성이 유지되는지 확인하여 객체의 식별 정보 를 획득할 수도 있다. 결과적으로, 도 7에 도시된 멀티 태스크 모델은 영상을 입력으로 하여, 영상 내 객체 인 식 결과를 출력할 수 있다. 도 7에 도시된 멀티 태스크 모델로부터 예측된 객체 인식 결과는 객체의 검출 정보, 영상 내의 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이루어진 2차원 포즈 정보, 객체의 위치와 방향을 표현하는 특징점들의 공 간 좌표들로 이루어진 3차원 포즈 정보, 객체의 식별 정보를 포함할 수 있다. 다시 도 6을 참조하면, 제1 시점으로 촬영된 제1 영상은 공간 내의 객체를 온전히 포함하고 있다. 반면, 제2 시 점으로 촬영된 제2 영상은 동일한 공간 내의 동일한 객체에 대해, 객체의 뒷모습 일부만 포함하고 있다. 비전 태스크를 수행하는 신경망 모델의 경우, 제2 영상과 같이 객체의 일부만을 포함하고 있는 영상에 대해 비전 태 스크를 수행하면, 객체를 놓치거나, 객체를 잘못 인식하거나, 객체의 위치를 잘못 파악하는 등의 오류를 범할 수 있다. 만일, 비전 태스크를 수행하는 신경망 모델이 자기 지도 학습 방식에 따를 경우, 객체 인식 오류에 따 라 학습 데이터 셋을 확보하는 것도 어려워져, 신경망 모델의 정확도를 높이지 못하는 악순환 구조가 된다. 본 개시의 일 실시예에 따라, 외부 장치와 전자 장치가 동일한 공간 내에서 동일한 객체를 각각 제1 시점의 제1 영상과 제2 시점의 제2 영상으로 촬영한 경우, 두 시점 간의 관계를 이용하면, 외부 장치에 탑 재된 제1 신경망 모델의 제1 영상에 대한 제1 객체 인식 결과를 변환하여, 제2 시점으로 변환된 제1 객체 인식 결과를 전자 장치에서 촬영된 제2 영상에 대한 라벨(label)로서 이용할 수 있다. 라벨은 원본 데이터와 관 련된 정보로서, 원본 데이터에 대한 추가적인 정보를 제공하는 메타데이터를 의미한다. 라벨링은 원본 데이터에 라벨을 입력 또는 추가하는 것으로, 그 방식에 따라, 바운딩 박스, 폴리곤 세그멘테이션, 포인트, 키포인트 등 의 어노테이션을 수행할 수 있다. 예를 들어, 전자 장치는 외부 장치의 제1 카메라 좌표계와 전자 장 치의 제2 카메라 좌표계 간의 변환 관계에 기초하여, 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예 측된 제1 객체 인식 결과를 변환하여, 제2 영상에 대한 라벨을 획득할 수 있다. 도 8은 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 변환하여 제2 영상에 대 응되는 제2 시점으로 변환된 제1 객체 인식 결과를 획득하는 과정을 설명하기 위한 도면이다. 도 8의 변환 과정 은 전자 장치 또는 클라우드 서버에서 수행될 수 있다. 제1 객체 인식 결과는 객체의 검출 정보, 제1 영상 내의 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이 루어진 제1 영상 좌표계에서의 2차원 포즈 정보, 객체의 위치와 방향을 표현하는 특징점들의 공간 좌표들로 이 루어진 제1 카메라 좌표계에서의 3차원 포즈 정보, 및 객체의 식별 정보를 포함할 수 있다. 도 8을 참조하면, 제1 객체 인식결과 중 제1 카메라 좌표계에서의 3차원 공간 포즈 정보를 이용하여, 제2 영상 좌표계에서의 2차원 포즈 정보로 변환하고, 2차원 포즈 정보를 바운딩 박스(bounding box, bbox)로 변환하는 과 정을 나타내고 있다. 먼저, 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계에 기초하여, 제1 카메라 좌표계에서의 3차원 포즈 정보 가 월드 좌표계에서의 3차원 포즈 정보로 변환된다. 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계는 제1 카 메라 캘리브레이션을 수행하여, 제1 카메라의 내부 파라미터와 외부 파라미터를 구함으로써 알 수 있다. 내부 파라미터는 카메라의 초점 거리, 주점(principal point), 비대칭계수(skew coefficient) 등 카메라의 스펙 명 세서를 통해 파악 가능하다. 외부 파라미터는 카메라의 설치 높이, 방향(팬, 틸트) 등 카메라와 외부 공간 간의 기하학적 관계에 관한 것으로, 카메라를 어떤 위치에 어떤 방향으로 설치했는지, 월드 좌표계를 어떻게 정의했 는지에 따라 달라질 수 있다. 제1 카메라의 외부 파라미터는 제1 카메라의 내부 파라미터를 구한 후, 미리 알고 있는 월드 좌표계의 공간 좌표와 이에 대응되는 제1 카메라의 영상 좌표계의 영상 좌표의 매칭 쌍들을 이용하여 구할 수 있다. 예를 들어, 스마트홈 카메라와 같은 외부 장치에 장착된 제1 카메라의 외부 파라미터는 제1 카메라의 내부 파라미터와 최소 4개 이상의 특징점에 대한 월드 좌표계의 공간 좌표와 이에 대응되는 제1 카메라의 영상 좌표 계의 영상 좌표의 매칭 쌍들을 이용하여 구할 수 있다. 이때, 매칭 쌍들은 이동이 가능한 전자 장치를 이용하여, 외부 장치의 제1 카메라의 관측 시야 내에서 다수의 특징점이 검출되도록 함으로써 획득할 수 있 다. 예를 들어, 전자 장치가 로봇 청소기인 경우, 소정의 검출 영역의 중심점을 특징점으로 이용하거나, 로봇 청소기에 부착한 마커 또는 로봇 청소기의 외관 특징을 특징점으로 이용할 수 있다. 특징점에 대한 월드 좌표계의 공간 좌표는 로봇 청소기의 높이 및 로봇 내 특징점의 위치와 SLAM(Simultaneous Localization And Mapping)을 통해 획득할 수 있고, 특징점에 대한 제1 카메라의 영상 좌표계의 영상 좌표는 스마트 홈카메라 영 상에서 검출된 특징점의 2차원 좌표를 통해 획득할 수 있다. 월드 좌표계와 제2 카메라 좌표계 간의 변환 관계에 기초하여, 월드 좌표계에서의 3차원 포즈 정보가 제2 카메 라 좌표계에서의 3차원 포즈 정보로 변환된다. 월드 좌표계와 제2 카메라 좌표계 간의 변환 관계는 제2 카메라 캘리브레이션을 수행하여, 제2 카메라의 외부 파라미터를 구함으로써 알 수 있으며, 월드 좌표계를 제2 카메라 좌표계로 변환시키기 위한 회전 행렬과 병진 행렬로 나타낼 수 있다. 제2 카메라 좌표계에서의 공간 좌표를 제2 카메라의 제2 영상 좌표계로 투영함으로써, 제2 카메라 좌표계에서의 3차원 포즈 정보는 제2 영상 좌표계에서의 2차원 포즈 정보로 변환된다. 투영 행렬은 제2 카메라 캘리브레이션 을 수행하여, 제2 카메라의 내부 파라미터를 구함으로써 알 수 있다. 제2 영상 좌표계에서의 2차원 포즈 정보의 x와 y 각각의 최소 및 최대 값에 소정의 마진 값을 허용함으로써, 제 2 영상 좌표계에서의 2차원 포즈 정보로부터 객체 영역을 추정하는 바운딩 박스(bounding box, bbox) 정보를 생 성할 수 있다. 이에 따라, 객체의 검출 정보 및 식별 정보, 제2 영상 좌표계에서의 2차원 포즈 정보, 제2 카메라 좌표계에서의 3차원 포즈 정보, 제2 영상 내에 객체의 존재를 확인할 수 있는 바운딩 박스 정보를 포함하는 제2 시점으로 변 환된 제1 객체 인식 결과를 획득할 수 있다.다시 도 6을 참조하면, 전자 장치는 제1 영상에 대응되는 제2 영상에 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링(labeling)을 수행함으로써 학습 데이터를 생 성할 수 있다. 제1 영상과 제2 영상이 대응된다는 것은 제1 영상을 입력으로 하여 신경망 모델로부터 예측된 객 체 인식 결과를 제2 영상의 학습 데이터를 생성하는데 이용할 수 있는 가장 적합한 관계임을 의미한다. 전자 장 치는 제1 객체 인식 결과에 대응되는 타임 스탬프와 제2 영상의 타임 스탬프를 비교하여 서로 대응되는 쌍 을 확인할 수 있다. 제1 객체 인식 결과에 대응되는 타임 스탬프는 제1 영상의 타임 스탬프일 수 있다. 제1 영 상에 대응되는 제2 영상 내에 객체가 존재한다고 판단되면, 제1 객체 인식 결과를 변환하여 획득한, 제2 시점으 로 변환된 제1 객체 인식 결과에 기초하여 제2 영상에 라벨링을 수행할 수 있다. 예를 들어, 객체의 포즈 정보를 인식하여 태스크를 수행하는 전자 장치가 로봇 청소기인 경우, 객체의 포 즈 정보를 잘 인식할 수 있도록 로봇 청소기에 탑재된 신경망 모델을 학습시켜야 하는데, 로봇 청소기에 탑재된 신경망 모델을 학습시키기 위한 학습 데이터 셋에 제약이 있다. 이를 위해, 전자 장치는, 외부 장치 에 탑재된 제1 신경망 모델로부터 예측된 객체 인식 결과를 이용하여, 로우 앵글 뷰 방식으로 촬영된 영상을 학 습 데이터로 생성할 수 있다. 전자 장치는 앞서 도 8에서 설명한 바와 같이, 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 변환하여 제2 영상에 대응되는 제2 시점으로 변환된 제1 객체 인식 결과를 획득할 수 있 다. 예를 들어, 전자 장치는 제1 시점으로 촬영하는 제1 카메라에 대응되는 제1 카메라 좌표계와 제2 시점 으로 촬영하는 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 제1 영상 내에서 검출 및 식별된 객체의 위치와 방향을 표현하는 특징점들의 공간 좌표들로 이루어진 3차원 포즈 정보를 제2 영상 내에서 검출 및 식별되는 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이루어진 2차원 포즈 정보로 변환할 수 있 다. 전자 장치는 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2차원 포즈 정보 가 제2 영상 내의 객체의 2차원 포즈 정보로 이용되도록 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다. 전자 장치는 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2차원 포즈 정보 중 제2 영상 내에 해당하는 제1 부분과 제2 영상 내에 해당하지 않는 제2 부분을 구분하여 라벨링을 수행함으로써, 학습 데이터를 생성할 수 있다. 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2차원 포즈 정보 중 일부가 제2 영상 내에 보이지 않는 경우, 제2 영상에서 보이지 않는 부분까지 제2 영상에 대하여 라벨링을 수행할 수 있다. 예를 들어, 2차원 포즈 정보가 객체의 전신에 대한 스켈레톤 정보인 경우, 제2 영상 내에 객체의 상반신 이 보이지 않더라도, 전자 장치는 제2 영상에 대하여, 객체의 전신에 대한 스켈레톤 정보인 2차원 포즈 정 보에 기초하여 라벨링을 수행할 수 있다. 이와 같은 경우, 스켈레톤 정보를 구성하는 각 관절 포인트의 위치 및 가시성 값인 (x, y, visibility)에 대하여, 전자 장치는 제2 영상 내에서 보이는 하반신에 해당하는 각 관 절 포인트에 대한 가시성(visibility) 값을 '1'로 설정하고, 제2 영상 내에서 보이지 않는 상반신에 해당하는각 관절 포인트에 대한 가시성 값을 '0'으로 설정하여, 라벨링을 수행함으로써, 학습 데이터를 생성할 수 있다. 본 개시의 일 실시예에 의하면, 전자 장치는 복수 개의 제2 영상들을 학습 데이터로 생성할 수 있다. 이하, 도 9 내지 도 11을 참조하여 설명한다. 도 9는 제2 시점으로 촬영된 복수 개의 제2 영상들 각각에 대하여 학습 데이터를 생성하는 예를 설명하기 위한 도면이다. 도 9를 참조하면, 전자 장치의 제2 카메라가 제2 시점을 유지한 상태에서 동적 객체를 소정의 시간 간격으 로 촬영하여, 제2 시점으로 촬영된 복수 개의 제2 영상들을 획득한 것을 나타내고 있다. 이와 같이, 제2 카메라 에 의해 제2 시점으로 촬영된 복수 개의 제2 영상들이 있는 경우, 전자 장치는 복수 개의 제2 영상들 각각 에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다. 도 9에 도시된 바와 같이, 제2 카메라가 제2 시점을 유지하여 소정의 시간 동안 촬영한 결과, t=1인 경우, 객체 가 전자 장치의 관측 시야에서 우측 끝에 등장하는 모습이 촬영되고, t=2인 경우, 객체가 전자 장치 의 관측 시야의 중앙에 위치하는 모습이 촬영되고, t=3인 경우, 객체가 전자 장치의 관측 시야에서 좌측 끝으로 사라지는 모습이 촬영될 수 있다. 전자 장치의 제2 카메라는 로우 앵글 뷰 방식으로 촬영하여 객체 의 전신이 아닌 하반신 위주의 제2 영상들을 촬영하게 되는데, 객체의 검출 및 포즈 정보를 인식하기 어려울 수 있다. 이와 같은 경우, 외부 장치의 제1 카메라에서 객체를 촬영한 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 이용하여, 복수 개의 제2 영상들 각각에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행할 수 있다. 도 9를 참조하면, t=1인 경우와 t=3인 경우, 제2 시점 으로 변환된 제1 객체 인식 결과에 포함된 2차원 포즈 정보 중 일부만 제2 시점으로 촬영된 제2 영상 내의 객체 의 포즈 정보에 대한 라벨로 이용될 수 있다. t=2인 경우, 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2 차원 포즈 정보 중 하반신과 상반신의 일부에 대응되는 관절 포인트 정보가 제2 시점으로 촬영된 제2 영상 내의 객체의 포즈 정보에 대한 라벨로 이용될 수 있다. 도 10 및 도 11은 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들 각각에 대하여 학습 데 이터를 생성하는 예를 설명하기 위한 도면이다. 도 10 및 도 11을 참조하면, 전자 장치의 제2 카메라가 객체를 서로 다른 복수 개의 제2 시점들로 촬영한 복수 개의 제2 영상들을 획득한 것을 나타내고 있다. 도 10은 전자 장치의 제2 카메라가 전자 장치에 탑재된 상태로 정적 객체 주위를 돌면서, 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들 을 나타내고 있다. 도 10에 도시된 바와 같이, 전자 장치의 제2 카메라는 객체의 후방과 좌,우 측면에 대 해, 로우 앵글 뷰 방식으로 촬영하여 객체의 전신이 아닌 하반신 위주의 제2 영상들을 촬영하게 되는데, 객체의 검출 및 포즈 정보를 인식하기 어려울 수 있다. 도 11은 전자 장치의 제2 카메라가 정적 객체와의 거리를 변경하면서, 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들을 나타내고 있다. 도 11에 도시된 바와 같이, 전자 장치의 제2 카메라는 객체와의 거리가 가까워지거나 멀어지면서 객체를 확대 또는 축소된 형태로, 로우 앵글 뷰 방식으로 촬영하여 객체의 전신이 아닌 하반신 위주의 제2 영상들을 촬영하게 되 는데, 객체의 검출 및 포즈 정보를 인식하기 어려울 수 있다. 이와 같이, 제2 카메라에 의해 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들이 있는 경 우, 전자 장치는 제1 카메라 좌표계와 복수 개의 제2 시점들 각각에서의 제2 카메라 좌표계 간의 변환 관 계에 기초하여, 외부 장치의 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 변환할 수 있다. 서로 다른 복수 개의 제2 시점들 각각을 기준으로 변환된 제1 객체 인식 결과가 라벨링에 필요하기 때문이다. 전자 장치는 복수 개의 제2 영상들 각각에 대하여, 복수 개의 제2 시점들 각각으로 변환된 제1 객체 인식 결과 에 기초한 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다. 한편, 전자 장치는 상시로 또는 사용자의 설정에 따라 학습 데이터를 생성할 수 있다. 또한, 제2 카메라를 탑재한 전자 장치가 객체를 소정의 기준보다 낮은 수준으로 인식하거나, 전자 장치가 이용하는 공간 맵에서 특정된 위치로 전자 장치가 이동한 경우, 전자 장치는 학습 데이터를 생성할 수 있다. 예를 들어, 공간 맵에서 특정된 위치는 전자 장치가 위치하게 되면, 외부 장치의 제1 카메라의 관측 시야 와 전자 장치의 제2 카메라의 관측 시야가 중첩될 수 있는 위치일 수 있다. 또한, 공간 맵에서 특정된 위 치는 객체의 이동이 잦은 영역이나 사용자가 임의로 마킹한 위치일 수 있다. 다시 도 6을 참조하면, 전자 장치는 생성된 학습 데이터를 이용하여 제2 신경망 모델을 학습시킬 수 있다. 도 12는 제1 신경망 모델의 출력을 이용하여 생성된 학습 데이터를 이용하여 제2 신경망 모델을 학습시키는 과 정을 설명하기 위한 도면이다. 앞서 설명한 바와 같이, 외부 장치의 제1 신경망 모델에서 예측된 제1 객체 인식 결과는 제1 시점의 제1 영상으로부터 획득된 것이므로, 제1 객체 인식 결과를 제2 시점으로 변환하고, 제2 시점으로 변환된 제1 객체 인식 결과를 라벨링을 수행하는데 이용하여 전자 장치의 제2 신경망 모델의 학습 데이터를 생성할 수 있다. 전자 장치는 생성된 학습 데이터를 이용하여, 제2 신경망 모델을 학습시킬 수 있다. 이때, 전자 장치(10 0)는 제2 신경망 모델의 각 태스크 별 예측 헤드(prediction head)를 파인 튜닝(fine tuning)할 수 있다. 파인 튜닝은 기존의 학습된 신경망 모델을 목적에 맞게 아키텍쳐를 변형하여 학습시킴으로서 기존의 학습된 신경망 모델을 업데이트하는 것을 의미한다. 이에 따라 학습된 제2 신경망 모델은 객체 인식의 정확도 및 안정성을 높 여 제2 객체 인식 결과를 예측할 수 있다. 도 13은 신경망 모델을 학습시키는 방법을 설명하기 위한 흐름도이다. 전자 장치 또는 클라우드 서버는 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측(prediction)된 제1 객체 인식 결과를 획득할 수 있다.(S1310) 일 실시예에 따르면, 외부 장치에 구비된 제1 카메라는 적어도 하나의 객체를 포함하는 공간을 제1 시점으 로 촬영할 수 있다. 외부 장치에 탑재된 제1 신경망 모델은 제1 시점으로 촬영된 제1 영상을 입력으로 하 여, 제1 객체 인식 결과를 예측하여 출력할 수 있다. 전자 장치 또는 클라우드 서버는 외부 장치 로부터 제1 객체 인식 결과를 수신할 수 있다. 일 실시예에 따르면, 외부 장치에 구비된 제1 카메라는 적어도 하나의 객체를 포함하는 공간을 제1 시점으 로 촬영할 수 있다. 외부 장치는 제1 카메라에서 제1 시점으로 촬영된 제1 영상을 제1 신경망 모델을 관리 하는 클라우드 서버로 전송할 수 있다. 클라우드 서버는 외부 장치로부터 제1 시점으로 촬영된 제1 영상을 수신할 수 있다. 클라우드 서버에서 관리하는 제1 신경망 모델은 제1 시점으로 촬영된 제1 영 상을 입력으로 하여, 제1 객체 인식 결과를 예측하여 출력할 수 있다. 클라우드 서버는 제1 신경망 모델로 부터 예측된 제1 객체 인식 결과를 획득할 수 있다. 전자 장치 또는 클라우드 서버는 제1 카메라에 대응되는 제1 카메라 좌표계와 동일한 공간을 제2 시 점으로 촬영하는 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 변환할 수 있다.(S1320) 이에 따라, 제1 시점 기준의 제1 인식 결과는 제2 시점 기준으로 변환될 수 있다. 전자 장치 또는 클라우드 서버는 제2 시점으로 변환된 제1 객체 인식 결과 를 제2 영상에 대한 라벨로서 이용할 수 있다. 제1 객체 인식 결과는 객체의 검출 정보, 제1 영상 내의 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이 루어진 제1 영상 좌표계에서의 2차원 포즈 정보, 객체의 위치와 방향을 표현하는 특징점들의 공간 좌표들로 이 루어진 제1 카메라 좌표계에서의 3차원 포즈 정보, 및 객체의 식별 정보를 포함할 수 있다. 전자 장치 또는 클라우드 서버는 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계에 기초하여, 제1 카메라 좌표계에서의 3차원 포즈 정보를 월드 좌표계에서의 3차원 포즈 정보로 변환한다. 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계는, 제1 카메라 캘리브레이션을 수행하여, 제1 카메라의 내부 파라미터와 소정의 개 수의 특징점들에 대한 월드 좌표계의 공간 좌표와 제1 카메라의 영상 좌표계의 영상 좌표의 매칭 쌍들을 이용하 여 제1 카메라의 외부 파라미터를 구함으로써 획득될 수 있다. 이때, 특징점들은 제1 카메라의 관측 시야 내에 서 전자 장치를 이동시켜, 소정의 검출 영역의 중심점이나, 전자 장치에 부착한 마커 또는 전자 장치 의 외관 특징을 검출한 것일 수 있다. 전자 장치 또는 클라우드 서버는 월드 좌표계와 제2 카메라 좌표계 간의 변환 관계에 기초하여, 월드 좌표계에서의 3차원 포즈 정보를 제2 카메라 좌표계에서의 3차원 포즈 정보로 변환한다. 전자 장치 또는 클라우드 서버는 제2 카메라 좌표계에서의 공간 좌표를 제2 카메라의 제2 영상 좌표계로 투영함으로써, 제 2 카메라 좌표계에서의 3차원 포즈 정보를 제2 영상 좌표계에서의 2차원 포즈 정보로 변환한다. 전자 장치 또는 클라우드 서버는 제2 영상 좌표계에서의 2차원 포즈 정보로부터 바운딩 박스(bounding box, bbox) 정 보를 생성한다. 이에 따라, 객체의 검출 정보 및 식별 정보, 제2 영상 좌표계에서의 2차원 포즈 정보, 제2 카메라 좌표계에서의 3차원 포즈 정보, 제2 영상 내에 객체의 존재를 확인할 수 있는 바운딩 박스 정보를 포함하는 제2 시점으로 변 환된 제1 객체 인식 결과를 획득할 수 있다. 제2 시점으로 변환된 제1 객체 인식 결과 중에서 바운딩 박스 정보 는 제2 영상 내에서 객체 영역을 추정하는 라벨로서 이용될 수 있다. 포즈 정보는 제2 영상 내에서 객체의 포즈 를 인식하는 라벨로서 이용될 수 있다. 객체의 식별 정보는 연속하는 제2 영상 내에서 동일한 객체인지 확인하 는 라벨로서 이용될 수 있다. 전자 장치 또는 클라우드 서버는 제1 영상에 대응되는 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다.(S1330) 전자 장치 또는 클라우드 서버는 제1 객체 인식 결과에 대응되는 타임 스탬프와 제2 영상의 타임 스탬프를 비교하여 서로 대응되는 쌍을 확인할 수 있다. 제1 객체 인식 결과에 대응되는 타임 스탬프는 제1 영상의 타임 스탬프일 수 있 다. 전자 장치 또는 클라우드 서버는 제1 영상에 대응되는 제2 영상 내에 객체가 존재한다고 판단되 면, 제1 객체 인식 결과를 변환하여 획득한, 제2 시점으로 변환된 제1 객체 인식 결과에 기초하여, 제2 영상에 라벨링을 수행할 수 있다. 전자 장치는 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 포 함된 2차원 포즈 정보 중 제2 영상 내에 해당하는 제1 부분과 제2 영상 내에 해당하지 않는 제2 부분을 구분하 여 라벨링을 수행함으로써, 학습 데이터를 생성할 수 있다. 예를 들어, 전자 장치가 사용자의 포즈 정보를 인식하여 청소를 수행하는 로봇 청소기의 경우, 객체의 포 즈 정보를 잘 인식할 수 있도록 전자 장치에 탑재된 신경망 모델을 학습시키기 위해, 외부 장치에 탑 재된 신경망 모델로부터 예측된 객체 인식 결과를 이용하여, 로우 앵글 뷰 방식으로 촬영된 영상을 학습 데이터 로 생성할 수 있다. 전자 장치는 제1 시점으로 촬영하는 제1 카메라에 대응되는 제1 카메라 좌표계와 제2 시점으로 촬영하는 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 제1 영상 내에서 검 출 및 식별된 객체의 위치와 방향을 표현하는 특징점들의 공간 좌표들로 이루어진 3차원 포즈 정보를 제2 영상 내에서 검출 및 식별되는 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이루어진 2차원 포즈 정보로 변환 할 수 있다. 전자 장치는 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2차원 포 즈 정보가 제2 영상 내의 객체의 2차원 포즈 정보로 이용되도록 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다. 본 개시의 일 실시예에 따라, 전자 장치는 제2 카메라에 의해 제2 시점으로 촬영된 복수 개의 제2 영상들 이 있는 경우, 복수 개의 제2 영상들 각각에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링 을 수행함으로써 학습 데이터를 생성할 수 있다. 본 개시의 일 실시예에 따라, 전자 장치는 제2 카메라에 의해 서로 다른 복수 개의 제2 시점들로 각각 촬 영된 복수 개의 제2 영상들이 있는 경우, 제1 카메라 좌표계와 복수 개의 제2 시점들 각각에서의 제2 카메라 좌 표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환할 수 있다. 전자 장치는 복수 개의 제2 영상들 각각에 대하여, 복수 개의 제2 시점들 각각으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행 함으로써 학습 데이터를 생성할 수 있다. 본 개시의 일 실시예에 따라, 전자 장치는 제2 카메라를 탑재한 전자 장치가 객체를 소정의 기준보다 낮은 수준으로 인식하거나, 전자 장치가 이용하는 공간 맵에서 특정된 위치로 전자 장치가 이동한 경 우, 학습 데이터를 생성할 수 있다. 예를 들어, 공간 맵에서 특정된 위치는 전자 장치가 위치하게 되면, 외부 장치의 제1 카메라의 관측 시야와 전자 장치의 제2 카메라의 관측 시야가 중첩될 수 있는 위치 일 수 있다. 또한, 공간 맵에서 특정된 위치는 객체의 이동이 잦은 영역이나 사용자가 임의로 마킹한 위치일 수 있다. 전자 장치 또는 클라우드 서버는 생성된 학습 데이터를 이용하여, 제2 신경망 모델을 학습할 수 있다.(S1340) 전자 장치 또는 클라우드 서버는 생성된 학습 데이터를 이용하여, 제2 신경망 모델의 각 태스크 별 예측 헤드를 파인 튜닝할 수 있다. 도 14는 전자 장치에서 신경망 모델을 학습시키는 일 예를 나타낸 도면이다. 도 14를 참조하면, 외부 장치는 제1 신경망 모델을 탑재하고, 전자 장치는 제2 신경망 모델을 탑재한 다. 외부 장치에 구비된 제1 카메라는 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영할 수 있 다. 외부 장치에 탑재된 제1 신경망 모델은 제1 시점으로 촬영된 제1 영상을 입력으로 하여, 제1 객체 인 식 결과를 예측하여 출력할 수 있다. 외부 장치는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 전자 장치에 전송할 수 있다. 전자 장치는 외부 장치로부터 제1 객체 인식 결과를 수신할 수 있다. 전자 장치는 동일한 공간을 제2 시점으로 촬영하는 제2 카메라에 대응되는 제2 카메라 좌표계와 외부 장치 의 제1 카메라에 대응되는 제1 카메라 좌표계 간의 변환 관계에 기초하여, 외부 장치로부터 수신한 제1 객체 인식 결과를 변환할 수 있다. 이에 따라, 외부 장치로부터 수신한 제1 인식 결과는 제2 시점 기 준으로 변환될 수 있다. 전자 장치는 제2 시점으로 변환된 제1 객체 인식 결과를 제2 영상에 대한 라벨로 서 이용할 수 있다. 전자 장치는 외부 장치의 제1 카메라에 의해 제1 시점으로 촬영된 제1 영상에 대응되는 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성할 수 있 다. 전자 장치는 생성된 학습 데이터를 이용하여, 제2 신경망 모델을 학습시킬 수 있다. 도 15는 클라우드 서버에서 신경망 모델을 학습시키는 일 예를 나타낸 도면이다. 도 15를 참조하면, 외부 장치는 제1 신경망 모델을 탑재하고, 클라우드 서버와 전자 장치는 제2 신경망 모델을 탑재한다. 외부 장치에 구비된 제1 카메라는 적어도 하나의 객체를 포함하는 공간을 제1 시 점으로 촬영할 수 있다. 외부 장치에 탑재된 제1 신경망 모델은 제1 시점으로 촬영된 제1 영상을 입력으로 하여, 제1 객체 인식 결과를 예측하여 출력할 수 있다. 외부 장치는 제1 신경망 모델로부터 예측된 제1 객 체 인식 결과를 클라우드 서버에 전송할 수 있다. 클라우드 서버는 외부 장치로부터 제1 객체 인식 결과를 수신할 수 있다. 전자 장치에 구비된 제2 카메라는 외부 장치에 구비된 제1 카메라가 촬영한 동일한 공간을 제2 시점 으로 촬영하여, 제2 영상을 획득할 수 있다. 전자 장치는 제2 영상을 클라우드 서버로 전송할 수 있 다. 클라우드 서버는 전자 장치로부터 제2 영상을 수신할 수 있다. 클라우드 서버는 외부 장치의 제1 카메라에 대응되는 제1 카메라 좌표계와 전자 장치의 제2 카 메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 외부 장치로부터 수신한 제1 객체 인식 결과를 변환할 수 있다. 이를 위해, 클라우드 서버는 제1 카메라 좌표계와 관련된 제1 카메라의 내부 파라 미터와 외부 파라미터 정보 및 제2 카메라 좌표계와 관련된 제2 카메라의 내부 파라미터와 외부 파라미터 정보 를 저장하고 있을 수 있다. 클라우드 서버는 제2 시점으로 변환된 제1 객체 인식 결과를 제2 영상에 대한 라벨로서 이용할 수 있다. 클라우드 서버는 외부 장치의 제1 카메라에 의해 제1 시점으로 촬영된 제1 영상에 대응되는 제2 영상 에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다. 클라우드 서버는 제2 신경망 모델을 탑재하고 있는 전자 장치에 생성된 학습 데이터를 전송할 수 있다. 이 경우, 전자 장치는 수신된 학습 데이터를 이용하여, 전자 장치에 탑재된 제2 신경망 모 델을 학습시킬 수 있다. 클라우드 서버는 생성된 학습 데이터를 이용하여, 제2 신경망 모델을 학습시킬 수 있다. 클라우드 서버 에서 제2 신경망 모델이 학습된 경우, 클라우드 서버는 학습된 제2 신경망 모델의 업데이트된 네트워 크 파라미터 값들을 제2 신경망 모델을 탑재한 전자 장치에 전송할 수 있다. 전자 장치는 수신한 업 데이트된 네트워크 파라미터 값들에 기초하여, 제2 신경망 모델을 업데이트할 수 있다. 도 16은 클라우드 서버에서 신경망 모델을 학습시키는 일 예를 나타낸 도면이다. 도 16을 참조하면, 클라우드 서버는 제1 신경망 모델과 제2 신경망 모델을 탑재하고, 전자 장치는 제 2 신경망 모델을 탑재한다. 외부 장치에 구비된 제1 카메라는 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영할 수 있다. 외부 장치는 제1 시점으로 촬영된 제1 영상을 클라우드 서버로 전송할 수 있다. 클라우드 서버는 제1 영상을 외부 장치로부터 수신할 수 있다. 클라우드 서버는 제1 영상을 제1 신경망 모델에 입력으로 하여, 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 획득할 수 있다. 전자 장치에 구비된 제2 카메라는 외부 장치에 구비된 제1 카메라가 촬영한 동일한 공간을 제2 시점 으로 촬영하여, 제2 영상을 획득할 수 있다. 전자 장치는 제2 영상을 클라우드 서버로 전송할 수 있 다. 클라우드 서버는 전자 장치로부터 제2 영상을 수신할 수 있다.클라우드 서버는 외부 장치의 제1 카메라에 대응되는 제1 카메라 좌표계와 전자 장치의 제2 카 메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 변환할 수 있다. 클라우드 서버는 제2 시점으로 변환된 제1 객체 인식 결과를 제2 영상에 대한 라 벨로서 이용할 수 있다. 클라우드 서버는 외부 장치의 제1 카메라에 의해 제1 시점으로 촬영된 제1 영상에 대응되는 제2 영상 에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다. 클라우드 서버는 제2 신경망 모델을 탑재하고 있는 전자 장치에 생성된 학습 데이터를 전송할 수 있다. 이 경우, 전자 장치는 수신된 학습 데이터를 이용하여, 전자 장치에 탑재된 제2 신경망 모 델을 학습시킬 수 있다. 클라우드 서버는 생성된 학습 데이터를 이용하여, 제2 신경망 모델을 학습시킬 수 있다. 클라우드 서버 에서 제2 신경망 모델이 학습된 경우, 클라우드 서버는 학습된 제2 신경망 모델의 업데이트된 네트워 크 파라미터 값들을 제2 신경망 모델을 탑재한 전자 장치에 전송할 수 있다. 전자 장치는 수신한 업 데이트된 네트워크 파라미터 값들에 기초하여, 제2 신경망 모델을 업데이트할 수 있다. 도 17은 전자 장치가 사용자의 포즈를 인식하여 태스크를 수행하는 일 예를 설명하기 위한 도면이다. 도 17을 참조하면, 전자 장치는 사용자의 포즈 정보를 인식하여 청소를 수행하는 로봇 청소기이다. 로봇 청소기에 탑재된 신경망 모델이 객체의 포즈 정보를 잘 인식할 수 있도록 앞서 설명한 방식에 따라 학습이 완료 된 경우, 로우 앵글 뷰 방식으로 촬영된 영상을 통해서도 사용자의 포즈 정보나 제스쳐 정보를 잘 인식할 수 있 게 된다. 도 17에 도시된 바와 같이, 사용자가 \"하이! 클린봇! 저쪽 방 좀 청소해줘\"라고 음성 명령을 내렸을 때, 로봇 청소기는 자연어 처리를 수행하는 신경망 모델을 통해 사용자로부터 청소 명령이 개시된 것을 인식할 수 있는데, '저쪽 방'이 어디인지 구체적인 장소를 특정짓지 못할 수 있다. 이와 같은 경우, 로봇 청소기는 객체 검출 및 포즈 인식이 가능한 신경망 모델로부터 추정된 사용자의 포즈 정보를 확인하여, 사용자가 가르키는 방 향의 방을 특정할 수 있게 된다. 그 결과, 로봇 청소기는 사용자의 추정된 포즈 정보에 따라 특정된 방에 대한 청소 명령을 확인하여, 청소를 수행할 수 있다. 도 18 및 도 19는 본 개시의 일 실시예에 따른 전자 장치의 구성을 나타낸 블록도이다. 도 18을 참조하면, 일 실시예에 따른 전자 장치는, 메모리, 프로세서, 센싱부를 포함할 수 있으나, 이에 한정되는 것은 아니며, 범용적인 구성이 더 추가될 수 있다. 예를 들어, 도 19에 도시된 바와 같 이, 전자 장치는, 메모리, 프로세서, 카메라, 통신부 외에 카메라를 포함하는 센싱부, 입출력부, 구동부를 더 포함할 수 있다. 이하, 도 18 및 도 19를 참조하여, 각 구성에 대해 상세히 설명한다. 일 실시예에 따른 메모리는, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수 있고, 전자 장치 로 입력되거나 전자 장치로부터 생성되는 데이터를 저장할 수 있다. 메모리는 프로세서가 판독할 수 있는 명령어들, 데이터 구조, 및 프로그램 코드(program code)가 저장될 수 있다. 개시된 실시예들에 서, 프로세서가 수행하는 동작들은 메모리에 저장된 프로그램의 명령어들 또는 코드들을 실행함으로 써 구현될 수 있다. 일 실시예에 따른 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모 리 등)를 포함할 수 있으며, 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read- Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나를 포함하는 비 휘발성 메모리 및 램(RAM, Random Access Memory) 또는 SRAM(Static Random Access Memory)과 같 은 휘발성 메모리를 포함할 수 있다. 일 실시예에 따른 메모리는 전자 장치가 신경망 모델을 학습시킬 수 있도록 제어하는 하나 이상의 인 스트럭션 및/또는 프로그램을 저장할 수 있다. 예를 들어, 메모리에는 객체 인식 결과 변환 모듈, 학습 데 이터 생성 모듈, 학습 모듈 등이 저장될 수 있다. 일 실시예에 따른 프로세서는 메모리에 저장된 명령어들이나 프로그램화된 소프트웨어 모듈을 실행함 으로써, 전자 장치가 태스크를 수행할 수 있도록 동작이나 기능을 제어할 수 있다. 프로세서는 산술,로직 및 입출력 연산과 시그널 프로세싱을 수행하는 하드웨어 구성 요소로 구성될 수 있다. 프로세서는 메 모리에 저장된 하나 이상의 인스트럭션(instructions)을 실행함으로써, 전자 장치가 신경망 모델을 학습시키고, 학습된 신경망 모델을 이용하여 태스크를 수행하는 전반적인 동작들을 제어할 수 있다. 프로세서 는 메모리에 저장된 프로그램들을 실행함으로써, 카메라 또는 카메라를 포함하는 센싱부 , 통신부, 입출력부, 통신부, 구동부를 제어할 수 있다. 일 실시예에 따른 프로세서는 예를 들어, 중앙 처리 장치(Central Processing Unit), 마이크로 프로세서 (microprocessor), 그래픽 처리 장치(Graphic Processing Unit), ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 애플리케이션 프로세서 (Application Processor), 신경망 처리 장치(Neural Processing Unit) 또는 인공지능 모델의 처리에 특화된 하 드웨어 구조로 설계된 인공지능 전용 프로세서 중 적어도 하나로 구성될 수 있으나, 이에 제한되는 것은 아니다. 프로세서를 구성하는 각 프로세서는 소정의 기능을 수행하기 위한 전용 프로세서일 수 있다. 일 실시예에 따른 인공 지능(AI; artificial intelligence) 프로세서는, 인공지능(AI) 모델을 이용하여, 전자 장치가 수행하도록 설정된 태스크의 처리를 위해, 연산 및 제어를 수행할 수 있다. AI 프로세서는, 인공 지능(AI)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또는 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 전자 장치에 탑재될 수도 있다. 일 실시예에 따른 센싱부는, 전자 장치 주변 환경에 관한 정보를 감지하도록 구성되는 다수의 센서들 을 포함할 수 있다. 예를 들어, 센싱부는, 카메라, 라이다(LiDAR, Light Detection And Ranging) 센 서, 적외선 센서, 초음파 센서, ToF(Time of Flight) 센서, 자이로 센서 등을 포함 할 수 있으나, 이에 한정되는 것은 아니다. 일 실시예에 따른 카메라는 스테레오 카메라, 모노 카메라, 와이드 앵글 카메라, 어라운드 뷰 카메라 또는 3D 비전 센서 등을 포함할 수 있다. 라이다 센서는 레이저를 목표물에 비춰 사물과의 거리 및 다양한 물성을 감지할 수 있다. 라이다 센서 는 주변 사물, 지형지물 등을 감지하고 이를 3D 영상으로 모델링하는데 이용될 수 있다. 적외선 센서는 적외선을 복사해 빛이 차단됨으로써 변화를 검지하는 능동식 적외선 센서와 발광기를 가지 지 않고 외계로부터 받는 적외선의 변화만을 감지하는 수동식 적외선 센서 중 어느 하나가 될 수 있다. 예를 들 어, 적외선 근접 센서는 전자 장치의 바퀴 주변에 설치되어, 바닥으로 적외선을 조사한 후 수신함으로써, 추락 방지 센서로 사용될 수 있다. 초음파 센서는 초음파를 이용하여 물체까지의 거리를 측정할 수 있으며, 물체의 근접성에 대한 정보를 전 달하는 초음파 펄스를 방출 및 검출할 수 있다. 초음파 센서는 근접한 물체의 감지 및 투명한 물체의 감지 에 이용될 수 있다. ToF 센서는 물체로 발사한 빛이 튕겨져 돌아오는 거리를 시간으로 계산하여, 사물의 입체감과 움직임, 공 간 정보를 획득할 수 있다. ToF 센서는 복잡한 공간이나 어두운 곳, 그리고 눈앞의 장애물까지 수준 높은 사물 인지가 가능하도록 하여, 전자 장치가 장애물을 피해가도록 할 수 있다. 자이로 센서는 각속도를 검출할 수 있다. 자이로 센서는 전자 장치의 위치 측정과 방향 설정에 이용될 수 있다. 본 개시의 일 실시예에 의하면, 센싱부는 카메라를 이용하여, 적어도 하나의 객체를 포함하는 공간을 촬영하고, 공간 정보를 생성하는데 이용될 수 있다. 전자 장치는 카메라, 라이다 센서, 적외선 센서, 초음파 센서, ToF 센서, 및 자이로 센서 중 동종 또는 이종의 복수의 센서를 이용하 여, 공간 스캔 정보 또는 객체 정보를 획득함으로써, 적어도 하나의 객체를 포함하는 공간에 대한 공간 정보를 획득할 수 있다. 다만, 전자 장치가 라이다 센서 또는 ToF 센서와 같이 뎁스(depth) 정보를 알 수 있는 센서를 포함하는 경우라 하더라도, 객체가 바라보는 방향이나 객체와 전자 장치 간의 거리 또는 상대적 위치에 따라, 전자 장치가 객체를 감지하기 어려운 경우가 있을 수 있다. 통신부는, 전자 장치가 외부 장치 예컨대, 클라우드 서버, IoT 기기들(300-1, 300-2 , 300-3), 사용자 단말(미도시)과 통신을 가능하게 하는 하나 이상의 구성요소를 포함할 수 있다. 예를 들어, 통신부(14 0)는, 근거리 통신부(short-range wireless communication unit), 이동 통신부 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 근거리 통신부(short-range wireless communication unit)는, 블루투스 통신부, BLE(Bluetooth Low Energy) 통신부, 근거리 무선 통신부(Near Field Communication unit), WLAN(와이파이) 통신부, 지그비 (Zigbee) 통신부, Ant+ 통신부, WFD(Wi-Fi Direct) 통신부, UWB(ultra wideband) 통신부, 적외선(IrDA, infrared Data Association) 통신부, 마이크로 웨이브(uWave) 통신부 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 이동 통신부는, 이동 통신망 상에서 기지국, 외부의 단말, 서버 중 적어도 하나와 무선 신호를 송수신한다. 여기에서, 무선 신호는, 음성 호 신호, 화상 통화 호 신호 또는 문자/멀티미디어 메시지 송수신에 따른 다양한 형태의 데이터를 포함할 수 있다. 도 19에 도시된 바와 같이, 전자 장치는 입출력부, 구동부를 더 포함할 수 있으며, 도 19에 도 시되지 않았지만 전원부와 같은 구성을 더 포함할 수 있다. 입출력부는 입력부와 출력부을 포함할 수 있다. 입출력부는 입력부와 출력부가 분리된 형태이거나, 터치스크린과 같이 통합된 하나의 형태일 수 있다. 입출력부는 사용자로부터 입력 정 보를 수신할 수 있고, 사용자에게 출력 정보를 제공할 수 있다. 입력부는, 사용자가 전자 장치를 제어하기 위한 데이터를 입력하는 수단을 의미할 수 있다. 예를 들 어, 입력부는 키 패드(key pad), 터치 패널(접촉식 정전 용량 방식, 압력식 저항막 방식, 적외선 감지 방 식, 표면 초음파 전도 방식, 적분식 장력 측정 방식, 피에조 효과 방식 등) 등이 될 수 있다. 뿐만 아니라, 입 력부는 조그 휠, 조그 스위치 등이 있을 수 있으나 이에 한정되는 것은 아니다. 출력부는, 오디오 신호 또는 비디오 신호 또는 진동 신호를 출력할 수 있으며, 출력부는 디스플레이 부, 음향 출력부, 및 진동 모터를 포함할 수 있다. 디스플레이부는 전자 장치에서 처리되는 정보를 표시할 수 있다. 예를 들어, 디스플레이부는 사용자의 조 작을 입력받기 위한 사용자 인터페이스를 디스플레이할 수 있다. 디스플레이부와 터치패드가 레이어 구조를 이 루어 터치 스크린으로 구성되는 경우, 디스플레이부는 출력 장치 이외에 입력 장치로도 사용될 수 있다. 디스플 레이부는 액정 디스플레이(liquid crystal display), 박막 트랜지스터 액정 디스플레이(thin film transistor- liquid crystal display), 유기 발광 다이오드(organic light-emitting diode), 플렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display) 중에서 적어도 하나를 포함할 수 있다. 전자 장치의 구현 형태에 따라 전자 장치는 디스플레이부를 2개 이상 포함할 수 있다. 음향 출력부는 메모리에 저장된 오디오 데이터를 출력할 수 있다. 음향 출력부는 전자 장치에서 수행 되는 기능과 관련된 음향 신호를 출력할 수 있다. 음향 출력부에는 스피커(speaker), 버저(Buzzer) 등이 포함될 수 있다. 진동 모터는 진동 신호를 출력할 수 있다. 예를 들어, 진동 모터는 오디오 데이터 또는 비디오 데이터의 출력에 대응하는 진동 신호를 출력할 수 있다. 진동 모터는 터치스크린에 터치가 입력되는 경우 진동 신호를 출력할 수 있다. 구동부는 전자 장치의 구동(주행) 및 전자 장치 내부의 장치들의 동작에 이용되는 구성들을 포 함할 수 있다. 전자 장치가 로봇 청소기인 경우, 구동부는 흡입부, 주행부 등을 포함할 수 있으나, 이에 한정되는 것은 아니며, 구동부는 전자 장치의 종류에 따라 다를 수 있다. 흡입부는, 공기를 흡입하면서 바닥의 먼지를 집진하는 기능을 하는데, 회전브러쉬 또는 빗자루, 회전브러쉬 모 터, 공기흡입구, 필터, 집진실, 공기배출구 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 흡입부는, 부 가적으로 구석 먼지를 쓸어낼 수 있는 솔이 회전되는 구조로 장착될 수도 있다. 주행부는 전자 장치에 설치된 바퀴를 각각 회전 구동시키는 모터 및 바퀴에서 발생되는 동력을 전달할 수 있도록 설치된 타이밍 벨트 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예에 따르면, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 통신부를 통해, 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 외부 장치 의 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 외 부 장치로부터 획득할 수 있다. 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 외부 장치의 카메라에 대응되는 제1 카메라 좌표계와 공간을 제2 시점으로 촬영하는 카메라 에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환할 수 있다. 프 로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 제1 영상에 대응되는, 카메라 에서 촬영된 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로 써 학습 데이터를 생성할 수 있다. 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으 로써, 생성된 학습 데이터 이용하여, 제2 신경망 모델을 학습시킬 수 있다. 본 개시의 일 실시예에 따르면, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 외부 장치의 카메라에 대응되는 제1 카메라 좌표계와 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 제1 영상 내에서 검출 및 식별된 객체의 위치와 방향을 표현하는 특징점들 의 공간 좌표들로 이루어진 3차원 포즈 정보를 제2 영상 내에서 검출 및 식별되는 객체의 위치를 표현하는 특징 점들의 영상 좌표들로 이루어진 2차원 포즈 정보로 변환할 수 있다. 프로세서는 메모리에 저장된 하 나 이상의 인스트럭션을 실행함으로써, 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2차원 포즈 정보가 제2 영상 내의 객체의 2차원 포즈 정보로 이용되도록 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다. 본 개시의 일 실시예에 따르면, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계에 기초하여, 제1 카메라 좌표계에서의 3차원 포즈 정보를 월드 좌표계에서의 3차원 포즈 정보로 변환할 수 있다. 프로세서는 메모리에 저장된 하 나 이상의 인스트럭션을 실행함으로써, 월드 좌표계와 제2 카메라 좌표계 간의 변환 관계에 기초하여, 월드 좌 표계에서의 3차원 포즈 정보를 제2 카메라 좌표계에서의 3차원 포즈 정보로 변환할 수 있다. 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 제2 카메라 좌표계에서의 공간 좌표를 제2 카메 라의 제2 영상 좌표계로 투영함으로써, 상기 제2 카메라 좌표계에서의 3차원 포즈 정보를 제2 영상 좌표계에서 의 2차원 포즈 정보로 변환할 수 있다. 본 개시의 일 실시예에 따르면, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 제1 카메라 캘리브레이션을 수행하여, 외부 장치의 카메라의 내부 파라미터와 소정의 개수 의 특징점들에 대한 월드 좌표계의 공간 좌표와 외부 장치의 카메라의 영상 좌표계의 영상 좌표의 매칭 쌍 들을 이용하여 외부 장치의 카메라의 외부 파라미터를 구함으로써 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계를 획득할 수 있다. 이때, 특징점들은 외부 장치의 카메라의 관측 시야 내에서 전자 장치를 이동시켜, 소정의 검출 영역의 중심점이나, 전자 장치에 부착한 마커 또는 전자 장치의 외관 특징을 검출한 것일 수 있다. 본 개시의 일 실시예에 따르면, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2차원 포즈 정보 중 제2 영상 내에 해당하는 제1 부분과 제2 영상 내에 해당하지 않는 제2 부분을 구분하여 라벨링을 수행함으로써, 학 습 데이터를 생성할 수 있다. 본 개시의 일 실시예에 따르면, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 카메라에 의해 제2 시점으로 촬영된 복수 개의 제2 영상들이 있는 경우, 복수 개의 제2 영 상들 각각에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다. 본 개시의 일 실시예에 따르면, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 카메라에 의해 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들이 있 는 경우, 제1 카메라 좌표계와 복수 개의 제2 시점들 각각에서의 제2 카메라 좌표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환할 수 있다. 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 복수 개의 제2 영상들 각각에 대하여, 복수 개의 제2 시점들 각각으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다. 본 개시의 일 실시예에 따르면, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 전자 장치가 객체를 소정의 기준보다 낮은 수준으로 인식하거나, 전자 장치가 이용하 는 공간 맵에서 특정된 위치로 전자 장치가 이동한 경우, 학습 데이터를 생성할 수 있다. 도 20은 본 개시의 일 실시예에 따른 서버의 구성을 도시한 블록도이다. 이상에서 전술한 전자 장치의 동작들은 서버에서도 비슷한 방식으로 수행될 수 있다. 일 실시예에 따 른 서버는 메모리, 프로세서, 통신부, 스토리지를 포함할 수 있다. 서버의 구성 들은 도 18 및 도 19의 전자 장치의 메모리, 프로세서, 통신부에 각각 대응될 수 있으므로, 앞서 설명한 내용과 동일한 설명은 이하 생략한다. 메모리는 서버를 구동하고 제어하기 위한 다양한 데이터, 프로그램 또는 어플리케이션을 저장할 수 있다. 메모리에 저장된 하나 이상의 명령어들 또는 어플리케이션은 프로세서에 의해 실행될 수 있다. 메모리에는 전자 장치에 저장된 모듈과 동일한 기능을 수행하는 모듈이 저장되어 있을 수 있다. 예를 들어, 메모리에는 객체 인식 결과 변환 모듈, 학습 데이터 생성 모듈, 학습 모듈, 예측 모듈과 이에 대응 되는 데이터 및 프로그램 명령어 코드들이 저장될 수 있다. 프로세서는 서버를 전반적으로 제어할 수 있다. 일 실시예에 따른 프로세서는 메모리에 저장되는 하나 이상의 명령어들을 실행할 수 있다. 통신부는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN), 부가가치 통 신망(Value Added Network; VAN), 이동 통신망(mobile radio communication network), 위성 통신망 및 이들의 상호 조합을 통하여 통신을 하게 하는 하나 이상의 구성요소를 포함할 수 있다. 스토리지는 제1 신경망 모델 또는 제2 신경망 모델이 저장될 수 있다. 스토리지는 각 종 인공지능 모 델들을 학습시키는데 이용되는 학습용 데이터셋을 저장할 수 있다. 본 개시의 일 실시예에 따른 서버는, 전자 장치보다 많은 연산을 빠르게 수행 가능하도록, 컴퓨팅 성 능이 전자 장치보다 높은 장치일 수 있다. 서버는 상대적으로 많은 연산량이 요구되는, 인공지능 모 델의 학습을 수행할 수 있다. 본 개시의 일 실시예에 따르면, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상 을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 획득할 수 있다. 프로세서는 메모 리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 통신부를 통해, 공간을 제2 시점으로 촬영하는 제2 카메라에서 촬영된 제2 영상을 획득할 수 있다. 프로세서는 메모리에 저장된 하나 이상의 인스트 럭션을 실행함으로써, 제1 카메라에 대응되는 제1 카메라 좌표계와 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환할 수 있다. 프로세서는 메모리에 저 장된 하나 이상의 인스트럭션을 실행함으로써, 제1 영상에 대응되는 제2 영상에 대하여, 제2 시점 기준으로 변 환된 제1객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성할 수 있다. 프로세서는 메모 리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 생성된 학습 데이터 이용하여, 제2 신경망 모델을 학습시킬 수 있다. 본 개시의 일 실시예에 따르면, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 통신부를 통해, 학습된 제2 신경망 모델의 네트워크 파라미터 값들을 제2 신경망 모델을 탑 재한 전자 장치에 전송할 수 있다. 한편, 본 개시의 실시예들은 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행 가능한 명령어 를 포함하는 기록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스 될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨 터 판독 가능 매체는 컴퓨터 저장 매체 및 통신 매체를 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독 가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구 현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독 가 능 명령어, 데이터 구조, 또는 프로그램 모듈과 같은 변조된 데이터 신호의 기타 데이터를 포함할 수 있다. 또한, 컴퓨터에 의해 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, ‘비일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다 는 것을 의미할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경 우를 구분하지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두개의 사용자 장치들(예: 스마트폰들) 간에 직접,온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운 로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 본 개시의 일 실시예에 따르면, 신경망 모델을 학습시키는 방법이 제공된다. 신경망 모델을 학습시키는 방법은 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입력으로 하 는 제1 신경망 모델로부터 예측(prediction)된 제1 객체 인식 결과를 획득하는 단계(S1310)를 포함한다. 또한, 신경망 모델을 학습시키는 방법은 제1 카메라에 대응되는 제1 카메라 좌표계와 공간을 제2 시점으로 촬영하는 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환하는 단계(S1320)를 포함한다. 또한, 신경망 모델을 학습시키는 방법은 제1 영상에 대응되는, 제2 카메라에서 촬영된 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링(labeling)을 수행함으로써 학습 데이터를 생성하는 단계(S1330)를 포함한다. 또한, 신경망 모델을 학습시키는 방법은 생성된 학습 데이터를 이 용하여, 제2 신경망 모델을 학습시키는 단계(S1340)를 포함한다. 또한, 본 개시의 일 실시예에 따르면, 예측된 제1 객체 인식 결과를 변환하는 단계(S1320)는 제1 카메라 좌표계 와 제2 카메라 좌표계 간의 변환 관계에 기초하여, 제1 영상 내에서 검출 및 식별된 객체의 위치와 방향을 표현 하는 특징점들의 공간 좌표들로 이루어진 3차원 포즈 정보를 제2 영상 내에서 검출 및 식별되는 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이루어진 2차원 포즈 정보로 변환한다. 또한, 학습 데이터를 생성하는 단계 (S1330)는 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2차원 포즈 정보가 제2 영상 내의 객체의 2차원 포즈 정보로 이용되도록 라벨링을 수행함으로써 학습 데이터를 생성한다. 또한, 예측된 제1 객체 인식 결과를 변환하는 단계(S1320)는 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계 에 기초하여, 제1 카메라 좌표계에서의 3차원 포즈 정보를 월드 좌표계에서의 3차원 포즈 정보로 변환하는 단계 를 포함한다. 또한, 예측된 제1 객체 인식 결과를 변환하는 단계(S1320)는 월드 좌표계와 제2 카메라 좌표계 간 의 변환 관계에 기초하여, 월드 좌표계에서의 3차원 포즈 정보를 제2 카메라 좌표계에서의 3차원 포즈 정보로 변환하는 단계를 포함한다. 또한, 예측된 제1 객체 인식 결과를 변환하는 단계(S1320)는 제2 카메라 좌표계에서 의 공간 좌표를 제2 카메라의 제2 영상 좌표계로 투영함으로써, 제2 카메라 좌표계에서의 3차원 포즈 정보를 제 2 영상 좌표계에서의 2차원 포즈 정보로 변환하는 단계를 포함한다. 또한, 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계는 제1 카메라 캘리브레이션을 수행하여, 제1 카메라의 내부 파라미터와 소정의 개수의 특징점들에 대한 월드 좌표계의 공간 좌표와 제1 카메라의 영상 좌표계의 영상 좌표의 매칭 쌍들을 이용하여 제1 카메라의 외부 파라미터를 구함으로써 획득될 수 있다. 또한, 특징점들은 제1 카메라의 관측 시야 내에서 전자 장치를 이동시켜, 소정의 검출 영역의 중심점이나, 전자 장치에 부 착한 마커 또는 전자 장치의 외관 특징을 검출한 것일 수 있다. 또한, 학습 데이터를 생성하는 단계(S1330)는, 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2차원 포즈 정보 중 제2 영상 내에 해당하는 제1 부분과 제2 영상 내에 해당하지 않는 제2 부분을 구분 하여 라벨링을 수행함으로써, 학습 데이터를 생성한다. 또한, 본 개시의 일 실시예에 따르면, 제2 카메라에 의해 제2 시점으로 촬영된 복수 개의 제2 영상들이 있는 경 우, 학습 데이터를 생성하는 단계(S1330)는, 복수 개의 제2 영상들 각각에 대하여, 제2 시점으로 변환된 제1 객 체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성한다. 또한, 제2 시점으로 촬영된 복수 개의 제2 영상들은, 제2 카메라가 제2 시점을 유지한 상태에서 동적 객체를 소 정의 시간 간격으로 촬영된 영상이다. 또한, 본 개시의 일 실시예에 따르면, 2 카메라에 의해 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개 의 제2 영상들이 있는 경우, 변환하는 단계(S1320)는, 제1 카메라 좌표계와 복수 개의 제2 시점들 각각에서의 제2 카메라 좌표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환한다. 또한, 학습 데이터를 생성하는 단계(S1330)는, 복수 개의 제2 영상들 각각에 대하여, 복수 개의 제2 시점들 각각으로 변환된 제1 객 체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성한다. 또한, 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들은, 제2 카메라가 전자 장치에 탑재된 상태로 정적 객체 주위를 돌거나 정적 객체와의 거리를 변경하면서 서로 다른 복수 개의 제2 시점들로 촬영된 것이다. 또한, 본 개시의 일 실시예에 따르면, 학습 데이터를 생성하는 단계(S1330)는, 제2 카메라를 탑재한 전자 장치 가 객체를 소정의 기준보다 낮은 수준으로 인식하거나, 전자 장치가 이용하는 공간 맵에서 특정된 위 치로 전자 장치가 이동한 경우, 학습 데이터를 생성한다. 본 개시의 일 실시예에 따르면, 전자 장치는 메모리, 메모리에 저장된 하나 이상의 인스트럭션 을 실행하는 프로세서, 카메라, 및 통신부를 포함한다. 또한, 프로세서는 하나 이상의 인 스트럭션을 실행함으로써, 통신부를 통해, 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 외부 장치의 카메라에서 촬영된 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 외부 장치로부터 획득한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 외부 장치의 카메라에 대응되는 제1 카메라 좌표계와 공간을 제2 시점으로 촬영하는 카메라에 대응되는 제 2 카메라 좌표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 제1 영상에 대응되는, 카메라에서 촬영된 제2 영상에 대하여, 제 2 시점으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성한다. 또한, 프로세 서는 하나 이상의 인스트럭션을 실행함으로써, 생성된 학습 데이터 이용하여, 제2 신경망 모델을 학습시킨 다. 또한, 본 개시의 일 실시예에 따르면, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 변환 관계에 기초하여, 제1 영상 내에서 검출 및 식별된 객체의 위치와 방향을 표현하는 특징점들의 공간 좌표들로 이루어진 3차원 포즈 정보를 제2 영상 내에서 검출 및 식별되는 객체의 위치를 표현하는 특징점들의 영상 좌표들로 이루 어진 2차원 포즈 정보로 변환한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2차원 포즈 정보가 제2 영상 내의 객체의 2차원 포 즈 정보로 이용되도록 라벨링을 수행함으로써 학습 데이터를 생성한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계에 기초하여, 제1 카메라 좌표계에서의 3차원 포즈 정보를 월드 좌표계에서의 3차원 포즈 정보로 변환한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 월드 좌표계와 제2 카메라 좌표계 간의 변환 관계에 기초하여, 월드 좌표계에서의 3차원 포즈 정보를 제2 카메라 좌표계에서의 3차원 포즈 정보로 변환한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 제2 카메라 좌표계에서의 공간 좌표를 카메라 의 제2 영상 좌표계로 투영함으로써, 제2 카메라 좌표계에서의 3차원 포즈 정보를 제2 영상 좌표계에서의 2차원 포즈 정보로 변환한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 제1 카메라 캘리브레이션을 수행하여, 외부 장 치의 카메라의 내부 파라미터와 소정의 개수의 특징점들에 대한 월드 좌표계의 공간 좌표와 외부 장치 의 카메라의 영상 좌표계의 영상 좌표의 매칭 쌍들을 이용하여 외부 장치의 외부 파라미터를 구함으 로써 제1 카메라 좌표계와 월드 좌표계 간의 변환 관계를 획득한다. 또한, 특징점들은 외부 장치의 카메라 의 관측 시야 내에서 전자 장치를 이동시켜, 소정의 검출 영역의 중심점이나, 전자 장치에 부착한 마 커 또는 전자 장치의 외관 특징을 검출한 것이다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 제2 영상에 대하여, 제2 시점으로 변환된 제1 객체 인식 결과에 포함된 2차원 포즈 정보 중 제2 영상 내에 해당하는 제1 부분과 제2 영상 내에 해당하지 않는 제2 부분을 구분하여 라벨링을 수행함으로써, 학습 데이터를 생성한다. 또한, 본 개시의 일 실시예에 따르면, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 카메라에 의해 제2 시점으로 촬영된 복수 개의 제2 영상들이 있는 경우, 복수 개의 제2 영상들 각각에 대하여, 제2 시점 으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성한다. 또한, 본 개시의 일 실시예에 따르면, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 카메라에 의해 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들이 있는 경우, 제1 카메라 좌표계와 복수 개의 제2 시점들 각각에서의 제2 카메라 좌표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환하고, 복수 개의 제2 영상들 각각에 대하여, 복수 개의 제2 시점들 각각으로 변환된 제1 객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생성한다. 또한, 본 개시의 일 실시예에 따르면, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 전자 장치 가 객체를 소정의 기준보다 낮은 수준으로 인식하거나, 전자 장치가 이용하는 공간 맵에서 특정된 위 치로 전자 장치가 이동한 경우, 학습 데이터를 생성한다. 본 개시의 일 실시예에 따르면, 클라우드 서버는 메모리, 메모리에 저장된 하나 이상의 인스트 럭션을 실행하는 프로세서, 및 통신부를 포함한다. 프로세서는 하나 이상의 인스트럭션을 실행 함으로써, 적어도 하나의 객체를 포함하는 공간을 제1 시점으로 촬영하는 제1 카메라에서 촬영된 제1 영상을 입 력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 획득한다. 또한, 프로세서는 하나 이상 의 인스트럭션을 실행함으로써, 통신부를 통해, 공간을 제2 시점으로 촬영하는 제2 카메라에서 촬영된 제2 영상을 획득한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 제1 카메라에 대응되는 제1 카메라 좌표계와 제2 카메라에 대응되는 제2 카메라 좌표계 간의 변환 관계에 기초하여, 예측된 제1 객체 인식 결과를 변환한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 제1 영상에 대응되는 제2 영 상에 대하여, 제2 시점 기준으로 변환된 제1객체 인식 결과에 기초한 라벨링을 수행함으로써 학습 데이터를 생 성한다. 또한, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 생성된 학습 데이터 이용하여, 제2 신 경망 모델을 학습시킨다. 본 개시의 일 실시예에 따르면, 프로세서는 하나 이상의 인스트럭션을 실행함으로써, 통신부를 통해, 학습된 제2 신경망 모델의 네트워크 파라미터 값들을 제2 신경망 모델을 탑재한 전자 장치에 전송한다."}
{"patent_id": "10-2022-0169100", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본 개시의 설명은 예시를 위한 것이며, 본 개시가 속하는 기술분야의 통상의 지식을 가진 자는 본 개시 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 개시의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 개시의 범위에 포함되는 것으 로 해석되어야 한다.도면 도면1 도면2a 도면2b 도면3a 도면3b 도면3c 도면3d 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17 도면18 도면19 도면20"}
{"patent_id": "10-2022-0169100", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 전자 장치와 외부 장치들이 서로 연결되는 댁내 IoT 환경을 설명하기 위한 도면이다. 도 2a 및 도 2b는 공간 맵을 설명하기 위한 흐름도이다. 도 3a, 도 3b, 도 3c, 도 3d는 공간 맵을 구성하는 레이어를 활용하는 방식을 설명하기 위한 도면이다. 도 4는 본 개시의 일 실시예에 따른 공간 맵을 획득하는 방법을 설명하기 위한 흐름도이다. 도 5는 서로 다른 시점으로 촬영한 영상들 각각에서의 객체 인식 결과를 설명하기 위한 도면이다. 도 6은 제1 영상을 입력으로 하는 제1 신경망 모델로부터 예측된 제1 객체 인식 결과를 이용하여, 학습 데이터 를 생성함으로써, 제2 신경망 모델을 학습시키는 과정을 설명하기 위한 도면이다. 도 7은 제1 신경망 모델 또는 제2 신경망 모델의 일 예를 설명하기 위한 도면이다. 도 8은 제1 영상으로부터 예측된 제1 객체 인식 결과를 변환하여 제2 영상에 대응되는 제2 시점으로 변환된 제1 객체 인식 결과를 획득하는 과정을 설명하기 위한 도면이다. 도 9는 제2 시점으로 촬영된 복수 개의 제2 영상들 각각에 대하여 학습 데이터를 생성하는 예를 설명하기 위한 도면이다. 도 10 및 도 11은 서로 다른 복수 개의 제2 시점들로 각각 촬영된 복수 개의 제2 영상들 각각에 대하여 학습 데 이터를 생성하는 예를 설명하기 위한 도면이다. 도 12는 제1 신경망 모델의 출력을 이용하여 생성된 학습 데이터를 이용하여 제2 신경망 모델을 학습시키는 과 정을 설명하기 위한 도면이다 도 13은 신경망 모델을 학습시키는 방법을 설명하기 위한 흐름도이다. 도 14는 전자 장치에서 신경망 모델을 학습시키는 일 예를 나타낸 도면이다. 도 15는 클라우드 서버에서 신경망 모델을 학습시키는 일 예를 나타낸 도면이다. 도 16은 클라우드 서버에서 신경망 모델을 학습시키는 일 예를 나타낸 도면이다. 도 17은 전자 장치가 사용자의 포즈를 인식하여 태스크를 수행하는 일 예를 설명하기 위한 도면이다. 도 18 및 도 19는 본 개시의 일 실시예에 따른 전자 장치의 구성을 나타낸 블록도이다. 도 20은 본 개시의 일 실시예에 따른 서버의 구성을 도시한 블록도이다."}
