{"patent_id": "10-2023-0010071", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0117801", "출원번호": "10-2023-0010071", "발명의 명칭": "보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법", "출원인": "한국전자통신연구원", "발명자": "유병현"}}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전체 에이전트의 상태, 상기 전체 에이전트의 행동 및 전역 보상 참값을 포함한 학습 데이터를 기초로 신경망학습을 통해 양수 보상 추정 모델을 생성하는 단계;에이전트 각각에 대하여, 상기 전역 보상 참값을 기초로 제1 개별 유틸리티 함수를 생성하고, 상기 양수 보상추정 모델을 이용하여 제2 개별 유틸리티 함수를 생성하는 단계; 및상기 에이전트 각각의 상태를 기초로, 상기 제1 개별 유틸리티 함수와 상기 제2 개별 유틸리티 함수를 이용하여상기 에이전트 각각의 행동을 결정하는 단계;를 포함하는 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법."}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 양수 보상 추정 모델을 생성하는 단계는,상기 학습 데이터에 포함된 에이전트의 상태를 인코딩 신경망에 입력하여 상태 인코딩 벡터를 생성하고, 상기학습 데이터에 포함된 에이전트의 행동을 인코딩 신경망에 입력하여 행동 인코딩 벡터를 생성하며,상기 상태 인코딩 벡터와 상기 행동 인코딩 벡터를, 전역 보상 신경망에 입력하여 전역 보상 추정값을생성하며, 상기 전역 보상 추정값과 상기 전역 보상 참값을 기초로 손실 함수를 이용하여 상기 전역 보상 신경망에 포함된양수의 지역 보상 신경망을 훈련하여 상기 양수 보상 추정 모델을 생성하는 것인 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법."}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 양수 보상 추정 모델을 생성하는 단계는,상기 전역 보상 추정값과 전역 보상 참값을 상기 손실 함수에 입력하고, 상기 손실 함수의 함수값이 최소가 되도록 상기 양수의 지역 보상 신경망을 훈련하여 상기 양수 보상 추정 모델을 생성하는 것인 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법."}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 행동을 결정하는 단계는,소정의 기준에 따라 상기 제1 개별 유틸리티 함수 및 상기 제2 개별 유틸리티 함수 중 어느 하나를 선택하는 단계 및상기 에이전트 각각의 상태를 기초로, 소정의 기준에 따라, 임의의 행동 및 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동 중 어느 하나를 선택하는 단계를 포함하는 것인 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법."}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 제1 개별 유틸리티 함수 및 상기 제2 개별 유틸리티 함수 중 어느 하나를 선택하는 단계는,기 설정된 확률에 따라 상기 제1 개별 유틸리티 함수 및 상기 제2 개별 유틸리티 함수 중 어느 하나를 선택하는것이고,상기 제1 개별 유틸리티 함수를 선택할 확률은 1-ζ, 상기 제2 개별 유틸리티 함수를 선택할 확률은 ζ로 설정공개특허 10-2024-0117801-3-되고, 상기 제2 개별 유틸리티 함수를 선택할 확률은 최초에 1로 설정되고, 탐색 과정의 진행에 따라 0에 수렴하는 것인 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법."}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서, 상기 행동을 결정하는 단계는,기 설정된 행동 선택 확률에 따라 상기 임의의 행동 및 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동중 어느 하나의 행동을 선택하는 것이며,상기 임의의 행동을 선택할 확률은 ε, 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동을 선택할 확률은1-ε로 설정되고, 상기 임의의 행동을 선택할 확률은 최초 1로 설정되고, 탐색 과정의 진행에 따라 0에 수렴하는 것인 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법."}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "컴퓨터에서 판독 가능한 명령들을 저장하는 메모리; 및상기 명령들을 실행하도록 구현되는 적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는, 상기 명령들을 실행함으로써,전체 에이전트의 상태, 상기 전체 에이전트의 행동 및 전역 보상 참값을 포함한 학습 데이터를 기초로 신경망학습을 통해 양수 보상 추정 모델을 생성하고,에이전트 각각에 대하여, 상기 전역 보상 참값을 기초로 제1 개별 유틸리티 함수를 생성하고, 상기 양수 보상추정 모델을 이용하여 제2 개별 유틸리티 함수를 생성하며,상기 에이전트 각각의 상태를 기초로, 상기 제1 개별 유틸리티 함수와 상기 제2 개별 유틸리티 함수를이용하여, 상기 에이전트 각각의 행동을 결정하도록 구성되는 것인 컴퓨터 시스템."}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 적어도 하나의 프로세서는,상기 학습 데이터에 포함된 에이전트의 상태를 인코딩 신경망에 입력하여 상태 인코딩 벡터를 생성하고, 상기학습 데이터에 포함된 에이전트의 행동을 인코딩 신경망에 입력하여 행동 인코딩 벡터를 생성하며,상기 상태 인코딩 벡터와 상기 행동 인코딩 벡터를, 전역 보상 신경망에 입력하여 전역 보상 추정값을생성하며, 상기 전역 보상 추정값과 상기 전역 보상 참값을 기초로 손실 함수를 이용하여 상기 전역 보상 신경망에 포함된양수의 지역 보상 신경망을 훈련하여 상기 양수 보상 추정 모델을 생성하도록 구성되는 것인 컴퓨터 시스템."}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 적어도 하나의 프로세서는,상기 전역 보상 추정값과 전역 보상 참값을 상기 손실 함수에 입력하고, 상기 손실 함수의 함수값이 최소가 되도록 상기 양수의 지역 보상 신경망을 훈련하여 상기 양수 보상 추정 모델을 생성하도록 구성되는 것인 컴퓨터 시스템.공개특허 10-2024-0117801-4-청구항 10 제7항에 있어서, 상기 적어도 하나의 프로세서는,소정의 기준에 따라 상기 제1 개별 유틸리티 함수 및 상기 제2 개별 유틸리티 함수 중 어느 하나를 선택하고,상기 에이전트 각각의 상태를 기초로, 소정의 기준에 따라, 임의의 행동 및 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동 중 어느 하나를 선택하도록 구성되는 것인 컴퓨터 시스템."}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 적어도 하나의 프로세서는,기 설정된 확률에 따라 상기 제1 개별 유틸리티 함수 및 상기 제2 개별 유틸리티 함수 중 어느 하나를 선택하도록 구성되는 것이며,상기 제1 개별 유틸리티 함수를 선택할 확률은 1-ζ, 상기 제2 개별 유틸리티 함수를 선택할 확률은 ζ로 설정되고, 상기 제2 개별 유틸리티 함수를 선택할 확률은 최초에 1로 설정되고, 탐색 과정의 진행에 따라 0에 수렴하는 것인 컴퓨터 시스템."}
{"patent_id": "10-2023-0010071", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서, 상기 적어도 하나의 프로세서는,기 설정된 행동 선택 확률에 따라 상기 임의의 행동 및 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동중 어느 하나의 행동을 선택하도록 구성되는 것이며,상기 임의의 행동을 선택할 확률은 ε, 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동을 선택할 확률은1-ε로 설정되고, 상기 임의의 행동을 선택할 확률은 최초 1로 설정되고, 탐색 과정의 진행에 따라 0에 수렴하는 것인 컴퓨터 시스템."}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법에 관한 것이다. 본 발명에 따른 탐색 방 법은, 전체 에이전트의 상태, 전체 에이전트의 행동 및 전역 보상 참값을 포함한 학습 데이터를 기초로 신경망 학습을 통해 양수 보상 추정 모델을 생성하는 단계와, 에이전트 각각에 대하여, 전역 보상 참값을 기초로 제1 개 별 유틸리티 함수를 생성하고, 양수 보상 추정 모델을 이용하여 제2 개별 유틸리티 함수를 생성하는 단계와, 에 이전트 각각의 상태를 기초로, 제1 개별 유틸리티 함수와 제2 개별 유틸리티 함수를 이용하여 에이전트 각각의 행동을 결정하는 단계를 포함한다."}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 멀티 에이전트 강화학습에서의 탐색 방법에 관한 것이다. 보다 상세하게는 복잡한 보상을 제공하는 멀티 에이전트 환경에서 효율적인 학습 데이터를 수집하기 위한 멀티 에이전트 강화학습에서의 탐색 방법에 관 한 것이다."}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "멀티 에이전트 강화학습(Multi-agent reinforcement learning)은 둘 이상의 에이전트(Agent)가 존재하는 환경 에서 에이전트 간의 협업 또는 경쟁에 대한 최적의 전략을 학습하는 기법을 의미한다. 특히, 다수의 에이전트 간의 협업 전략을 찾는 협력적인 멀티 에이전트 강화학습(Cooperative multi-agent reinforcement learning)은 자율주행차, 무인항공기 등 다양한 분야에서 활용이 가능하여 많은 연구기관에서 주목하고 있는 기술이다. 일반 적으로 기존의 멀티 에이전트 강화학습 방법들은 협업 전략을 찾기 위해, 목표에 맞는 보상을 설정하고 모든 에 이전트가 이 보상을 공유한다. 이 공유된 보상을 모든 에이전트가 최대화함으로써 목표에 맞는 협업 전략을 찾 게 된다. 모든 에이전트에 공유되는 이러한 보상을 전역 보상(Global reward)이라고 한다. 실제로는 이 전역 보 상은 다양한 종류의 지역 보상(Local reward)의 합으로 구성되지만, 대부분의 환경에서 지역 보상은 관측되지 않고 전역 보상만이 관측되기 때문에 일반적인 강화학습 또는 멀티 에이전트 강화학습에서는 전역 보상만을 이 용해 학습을 수행한다.멀티 에이전트 강화학습에서는 에이전트들이 환경과의 상호 작용을 통해 현재 상태에서 에이전트들이 행동을 선 택하고, 그 때 환경으로부터 에이전트들이 받게 되는 전역 보상의 값이 얼마인지를 지속적으로 수집하고, 이 데 이터를 기반으로 학습을 수행한다. 이 때 에이전트의 상태에 따라 어떤 행동을 선택할지 정하는 것을 정책 (Policy)이라고 한다. 결과적으로 전역 보상의 누적 합이 최대가 되는 정책을 찾는 것이 학습의 목표가 된다. 최적 정책을 빠르고 정확하게 찾기 위해서는 에이전트가 데이터를 수집할 때 학습 효율에 도움이 되는 행동을 선택하는 것이 필요하다. 에이전트들이 데이터를 수집하기 위해 효율적으로 행동을 선택하는 것을 탐색 (Exploration) 기법이라고 한다. 싱글 에이전트 강화학습에서는 일반적으로 전역 보상의 합에 대한 기대값인 가 치 함수를 학습하고 이 가치 함수 값이 최대가 되는 행동을 선택하거나 임의의 행동을 선택하는 ε- greedy(Epsilon-Greedy) 기법을 사용한다. 멀티 에이전트 강화학습에서는 모든 에이전트에 대한 전역 보상의 합 의 기대값인 공동 가치 함수를 학습하고 이를 에이전트 별로 할당한다. 에이전트 별로 할당된 함수를 개별 유틸 리티 함수(Utility function)라고 하며, 이 함수는 각 에이전트의 가치 함수와 유사한 역할을 한다. 개별 유틸 리티 함수가 정해지면 각 에이전트는 자신의 개별 유틸리티 함수를 기반으로 싱글 에이전트 강화학습과 마찬가 지로 ε-greedy 방식으로 탐색을 수행한다. 그런데, 이러한 방식은 다양한 멀티 에이전트 환경에서 동작하지만 복잡한 형태의 협업을 학습하기 위해 복잡한 보상 구조를 사용하는 경우 최적 정책을 찾지 못할 수 있다는 문제 가 있다."}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 양수와 음수의 보상이 동시다발적으로 발생하는 복잡한 보상 구조를 갖는 멀티 에이전트 환경에서 최 적 정책을 찾기 위하여, 전역 보상을 양수의 지역 보상과 음수의 지역 보상으로 분해하고, 전역 보상과 양수의 지역 보상을 이용하여 가치 함수를 학습하고, 학습한 가치 함수를 이용하여 행동을 선택하도록 하는, 보상 분해 에 기반한 멀티 에이전트 강화학습에서의 탐색 방법을 제공하는 것을 그 목적으로 한다. 본 발명의 목적은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 또 다른 목적들은 아래의 기재로 부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른, 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법은, 전체 에이전트의 상태, 상기 전체 에이전트의 행동 및 전역 보상 참값을 포함한 학습 데이터를 기초로 신경망 학습을 통해 양수 보상 추정 모델을 생성하는 단계; 에이전트 각각에 대하여, 상기 전역 보상 참값을 기초로 제1 개별 유틸리티 함수를 생성하고, 상기 양수 보상 추정 모델을 이용하여 제2 개별 유틸리티 함수를 생성하는 단계; 및 상기 에 이전트 각각의 상태를 기초로, 상기 제1 개별 유틸리티 함수와 상기 제2 개별 유틸리티 함수를 이용하여 상기 에이전트 각각의 행동을 결정하는 단계를 포함한다. 본 발명의 일 실시예에서, 상기 양수 보상 추정 모델을 생성하는 단계는, 상기 학습 데이터에 포함된 에이전트 의 상태를 인코딩 신경망에 입력하여 상태 인코딩 벡터를 생성하고, 상기 학습 데이터에 포함된 에이전트의 행 동을 인코딩 신경망에 입력하여 행동 인코딩 벡터를 생성하며, 상기 상태 인코딩 벡터와 상기 행동 인코딩 벡터 를, 전역 보상 신경망에 입력하여 전역 보상 추정값을 생성하며, 상기 전역 보상 추정값과 상기 전역 보상 참값 을 기초로 손실 함수를 이용하여 상기 전역 보상 신경망에 포함된 양수의 지역 보상 신경망을 훈련하여 상기 양 수 보상 추정 모델을 생성하는 것일 수 있다. 본 발명의 일 실시예에서, 상기 양수 보상 추정 모델을 생성하는 단계는, 상기 전역 보상 추정값과 전역 보상 참값을 상기 손실 함수에 입력하고, 상기 손실 함수의 함수값이 최소가 되도록 상기 양수의 지역 보상 신경망을 훈련하여 상기 양수 보상 추정 모델을 생성하는 것일 수 있다. 본 발명의 일 실시예에서, 상기 행동을 결정하는 단계는, 소정의 기준에 따라 상기 제1 개별 유틸리티 함수 및 상기 제2 개별 유틸리티 함수 중 어느 하나를 선택하는 단계 및 상기 에이전트 각각의 상태를 기초로, 소정의 기준에 따라, 임의의 행동 및 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동 중 어느 하나를 선택하는 단계를 포함할 수 있다. 본 발명의 일 실시예에서, 상기 제1 개별 유틸리티 함수 및 상기 제2 개별 유틸리티 함수 중 어느 하나를 선택 하는 단계는, 기 설정된 확률에 따라 상기 제1 개별 유틸리티 함수 및 상기 제2 개별 유틸리티 함수 중 어느 하나를 선택하는 것일 수 있다. 이 경우, 상기 제1 개별 유틸리티 함수를 선택할 확률은 1-ζ, 상기 제2 개별 유틸리티 함수를 선택할 확률은 ζ로 설정된다. 그리고, 상기 제2 개별 유틸리티 함수를 선택할 확률은 최초에 1로 설정되고, 탐색 과정의 진행 에 따라 0에 수렴하도록 설정될 수 있다. 본 발명의 일 실시예에서, 상기 행동을 결정하는 단계는, 기 설정된 행동 선택 확률에 따라 상기 임의의 행동 및 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동 중 어느 하나의 행동을 선택하는 것일 수 있다. 이 경우, 상기 임의의 행동을 선택할 확률은 ε, 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동을 선택 할 확률은 1-ε로 설정된다. 그리고, 상기 임의의 행동을 선택할 확률은 최초 1로 설정되고, 탐색 과정의 진행 에 따라 0에 수렴하도록 설정될 수 있다. 본 발명의 일 실시예에 따른, 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법을 수행하는 컴퓨터 시스템은, 컴퓨터에서 판독 가능한 명령들을 저장하는 메모리 및 상기 명령들을 실행하도록 구현되는 적어도 하 나의 프로세서를 포함한다. 본 발명의 일 실시예에서, 상기 적어도 하나의 프로세서는, 상기 명령들을 실행함으로써, 전체 에이전트의 상태, 상기 전체 에이전트의 행동 및 전역 보상 참값을 포함한 학습 데이터를 기초로 신경망 학습을 통해 양수 보상 추정 모델을 생성하고, 에이전트 각각에 대하여, 상기 전역 보상 참값을 기초로 제1 개별 유틸리티 함수를 생성하고, 상기 양수 보상 추정 모델을 이용하여 제2 개별 유틸리티 함수를 생성하며, 상기 에이전트 각각의 상 태를 기초로, 상기 제1 개별 유틸리티 함수와 상기 제2 개별 유틸리티 함수를 이용하여, 상기 에이전트 각각의 행동을 결정하도록 구성된다. 본 발명의 일 실시예에서, 상기 적어도 하나의 프로세서는, 상기 학습 데이터에 포함된 에이전트의 상태를 인코 딩 신경망에 입력하여 상태 인코딩 벡터를 생성하고, 상기 학습 데이터에 포함된 에이전트의 행동을 인코딩 신 경망에 입력하여 행동 인코딩 벡터를 생성하며, 상기 상태 인코딩 벡터와 상기 행동 인코딩 벡터를, 전역 보상 신경망에 입력하여 전역 보상 추정값을 생성하며, 상기 전역 보상 추정값과 상기 전역 보상 참값을 기초로 손실 함수를 이용하여 상기 전역 보상 신경망에 포함된 양수의 지역 보상 신경망을 훈련하여 상기 양수 보상 추정 모 델을 생성하도록 구성될 수 있다. 본 발명의 일 실시예에서, 상기 적어도 하나의 프로세서는, 상기 전역 보상 추정값과 전역 보상 참값을 상기 손 실 함수에 입력하고, 상기 손실 함수의 함수값이 최소가 되도록 상기 양수의 지역 보상 신경망을 훈련하여 상기 양수 보상 추정 모델을 생성하도록 구성될 수 있다. 본 발명의 일 실시예에서, 상기 적어도 하나의 프로세서는, 소정의 기준에 따라 상기 제1 개별 유틸리티 함수 및 상기 제2 개별 유틸리티 함수 중 어느 하나를 선택하고, 상기 에이전트 각각의 상태를 기초로, 소정의 기준 에 따라, 임의의 행동 및 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동 중 어느 하나를 선택하도록 구 성될 수 있다. 본 발명의 일 실시예에서, 상기 적어도 하나의 프로세서는, 기 설정된 확률에 따라 상기 제1 개별 유틸리티 함 수 및 상기 제2 개별 유틸리티 함수 중 어느 하나를 선택하도록 구성될 수 있다. 이 경우, 상기 제1 개별 유틸리티 함수를 선택할 확률은 1-ζ, 상기 제2 개별 유틸리티 함수를 선택할 확률은 ζ로 설정된다. 그리고, 상기 제2 개별 유틸리티 함수를 선택할 확률은 최초에 1로 설정되고, 탐색 과정의 진행 에 따라 0에 수렴하도록 설정될 수 있다. 본 발명의 일 실시예에서, 상기 적어도 하나의 프로세서는, 기 설정된 행동 선택 확률에 따라 상기 임의의 행동 및 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동 중 어느 하나의 행동을 선택하도록 구성될 수 있다. 이 경우, 상기 임의의 행동을 선택할 확률은 ε, 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동을 선택 할 확률은 1-ε로 설정된다. 그리고, 상기 임의의 행동을 선택할 확률은 최초 1로 설정되고, 탐색 과정의 진행 에 따라 0에 수렴하도록 설정될 수 있다."}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "멀티 에이전트 환경에서는 싱글 에이전트와 달리, 다수의 에이전트가 서로 상호 작용하기 때문에 그에 따른 보 상 구조 또한 복잡한데, 단순히 전역 보상만을 이용해서는 에이전트 들의 행동을 정확히 평가하기 어렵다. 본 발명은 전역 보상을 양수의 지역 보상과 음수의 지역 보상으로 나누어 행동을 평가하고, 이로 인해 현재 에이전 트가 선택한 행동 중에서 목표에 부합하는 부분과 그렇지 않은 부분을 정량적으로 평가할 수 있기 때문에 에이 전트들의 행동에 대한 평가의 정확도가 향상될 수 있다. 이러한 행동에 대한 정확한 평가는 탐색 단계에서 최적 정책을 찾기 위해 수집해야 할 데이터를 효율적으로 수집하는 데 도움이 되고, 결과적으로 더 빠르게 최적 정책 을 찾을 수 있게 한다. 본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은"}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 멀티 에이전트 강화학습에서의 탐색 방법에 관한 것으로, 보다 상세하게는 복잡한 보상을 제공하는 멀티 에이전트 환경에서 효율적인 학습 데이터를 수집하기 위한 멀티 에이전트 강화학습에서의 탐색 방법에 관 한 것이다. 최근 멀티 에이전트 강화학습에서는 보다 복잡한 형태의 협업 전략을 찾기 위해 성공적인 협업을 수행하면 양수 의 보상을 제공하고, 원하지 않는 형태의 협업을 수행하면 음수의 보상을 제공하는 복잡한 보상 구조를 사용한 다. 복잡한 형태의 협업 전략을 찾기 위해서 이러한 보상 구조의 사용이 필요하지만, 기존의 탐색 기법은 전역 보상만으로 학습한 가치 함수를 이용해 탐색을 수행하기 때문에 에이전트들의 행동을 정확히 평가하기 어렵다."}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "본 발명이 해결하려는 과제는 양수와 음수의 보상이 동시다발적으로 발생하는 복잡한 보상 구조를 갖는 멀티 에 이전트 환경에서 최적 정책을 찾기 위한 효율적인 탐색 기법을 제공하는 것이다. 본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 한편, 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포 함한다. 명세서에서 사용되는 \"포함한다(comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성소자, 단계, 동작 및/또는 소자는 하나 이상의 다른 구성소자, 단계, 동작 및/또는 소자의 존재 또는 추가를 배제하지 않는다. 본 발명을 설명함에 있어서, 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 본 발명에 있어서 생략된 부분이 있더라도, 인공지능 기술 분야 또는 멀티 에이전트 강화학습 기술 분야의 통상의 기술자는 공지 기술을 바탕으로 본 발명의 기술적 내용 을 이해할 수 있을 것이다. 이하, 본 발명의 실시예를 첨부한 도면들을 참조하여 상세히 설명한다. 본 발명을 설명함에 있어 전체적인 이해 를 용이하게 하기 위하여 도면 번호에 상관없이 동일한 수단에 대해서는 동일한 참조 번호를 사용하기로 한다. 본 발명은 양수와 음수의 보상이 동시다발적으로 발생하는 멀티 에이전트 환경에서, 전역 보상을 양수 보상과 음수 보상으로 분해하여 양수 보상의 값을 추정하고, 이를 통해 높은 양수의 보상을 받는 행동에 대한 선택 확 률을 높이는 탐색 방법을 제안한다. 이 탐색 방법은 전역 보상으로부터 양수와 음수의 보상을 분해하는 단계, 전역 보상과 양수 보상으로 각각의 가치 함수를 학습하는 단계, 학습된 가치 함수들을 기반으로 행동을 선택하 는 단계를 포함한다. 도 1은 본 발명의 일 실시예에 따른, 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법을 설명하기 위한 흐름도이다. 본 발명의 일 실시예에 따른, 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법은, S110 단계 내지 S140 단계를 포함한다. 설명의 편의를 위해 상기 탐색 방법은 멀티 에이전트 강화학습을 위한 보상 분해 기반 탐색 장치(1000, 이하 ' 탐색 장치'로 약칭)에 의해 수행되는 것으로 전제한다. S110 단계는 전역 보상 분해 단계이다. 본 단계는 전역 보상으로부터 양수와 음수의 보상을 분해하는 단계이다. 본 단계에서 탐색 장치는 보상 분해를 위한 학습 데이터를 기초로 보상 분해 신경망 모델의 학습을 통해 전역 보상 추정 모델과 양수의 지역 보상 추정 모델(이하 '양수 보상 추정 모델'로 약칭)을 생성한다. 탐색 장치는 통신 장치를 통해 학습 데이터를 외부에서 입력받을 수 있다. 학습 데이터는 전체 에 이전트의 현재 상태 및 현재 행동과 전역 보상 참값으로 구성된다. 도 2는 전역 보상을 분해하기 위한 보상 분해(Reward decomposition) 신경망 모델의 구성도이다. 탐색 장치 는 학습 데이터에 포함된 전체 에이전트에 대한 현재 상태와 행동을 인코딩(Encoding) 신경망을 통해 상 태 인코딩 벡터와 행동 인코딩 벡터로 변환한다. 즉, 탐색 장치는 전체 에이전트의 현재 상태를 인코딩 신경망에 입력하여 상태 인코딩 벡터를 생성하고, 전체 에이전트의 현재 행동을 인코딩 신경망에 입력하여 행동 인코딩 벡터를 생성한다. 탐색 장치는 상태 인코딩 벡터와 행동 인코딩 벡터를 전역 보상 신경망 모델에 입력한다. 전역 보상 신경 망 모델(이하 '보상 분해 모델'로도 호칭될 수 있음)은 양수의 지역 보상 신경망과 음수의 지역 보상 신경망을 포함하여 구성된다. 즉, 탐색 장치는 상태 인코딩 벡터와 행동 인코딩 벡터를 양수와 음수 보상을 만드는 지역 보상 신경망 모델에 입력한다. 탐색 장치는 각 지역 보상 모델에서 생성된 양수와 음수의 보상을 더 하여 최종적으로 전역 보상에 대한 추정값(이하, '전역 보상 추정값'으로 지칭함)을 생성한다. 구체적으로 설명하면, 탐색 장치는 상태 인코딩 벡터와 행동 인코딩 벡터를 양수의 지역 보상 신경망에 입력하여 양수 보상 추정값을 생성하고, 상태 인코딩 벡터와 행동 인코딩 벡터를 음수의 지역 보상 신경망에 입 력하여 음수 보상 추정값을 생성하며, 양수 보상 추정값과 음수 보상 추정값을 합하여 전역 보상 추정값을 생성 한다. 탐색 장치는 전체 에이전트의 모든 상태 및 행동에 대하여 전역 보상 값을 기초로 양수 보상 추정 모델을 학습한다. 탐색 장치는 전역 보상 추정값과 학습 데이터에 포함된 레이블인 전역 보상 참값을 기초로 소 정의 손실 함수를 이용하여 보상 분해 모델에 포함된 전역 보상 신경망과 상기 양수의 지역 보상 신경망을 훈련 하여 전역 보상 추정 모델과 양수의 지역 보상 추정 모델(이하 '양수 보상 추정 모델')을 생성한다. 구체적으로, 탐색 장치는 전역 보상 추정값과 전역 보상 참값을 손실 함수에 입력하고, 손실 함수의 함수 값이 최소가 되도록 전역 보상 신경망과 상기 양수의 지역 보상 신경망을 훈련하여 전역 보상 추정 모델과 양수 보상 추정 모델을 생성한다.보상 분해 모델의 훈련을 위한 손실 함수는 수학식 1과 같다. 수학식 1"}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수학식 1에서 rk는 전역 보상의 참값이다. 그리고, 는 양수 지역 보상의 추정값, 는 음수 지역 보상의 추정 값, b는 학습에 사용하는 데이터의 수이 다. 수학식 1을 통해 보상 분해 모델을 학습하는 경우, 수학식 1에 대한 다양한 해(Solution)가 존재할 수 있다. 그 래서 추가적인 조건을 적용해서 하나의 정확한 보상 분해 모델을 학습할 수 있도록 하는 것이 일반적이다. 대표 적인 방법으로는 RD2(Reward Decomposition with Representation Disentanglement)가 있다. RD2에서는 보상 분 해 모델의 엔트로피를 최적화하는 조건을 추가하여, 보상 분해 모델을 구성할 때 각 지역 보상 모델의 정보량은 최소화하여 모델 내의 불필요한 정보를 줄이고, 지역 보상 모델 간의 정보량은 최대화하여 지역 보상 모델 간에 는 중복되는 정보량이 적도록 한다. 본 발명에서는 특정 방식의 보상 분해 모델에 제한되어 있지 않으며, 어떠 한 보상 분해 모델을 사용해도 무방하다. 다시 도 1로 돌아와, 아래에 S120 단계 이후의 단계에 대하여 설명한다. S120 단계는 공동 가치 함수 및 개별 유틸리티 함수 학습 단계이다. 본 단계는 전역 보상과 양수 보상으로 공동 가치 함수를 학습하는 단계이다. 탐색 장치는 전체 에이전트의 모든 상태 및 행동에 대한 전역 보상 참값 을 기초로 학습을 통해 전역 공동 가치 함수와 제1 개별 유틸리티 함수를 생성한다. 또한, 탐색 장치는 양수 보상 추정값을 기초로 학습을 통해 양수 보상 공동 가치 함수와 제2 개별 유틸리티 함수를 생성한다. 탐색 장치는 보상 분해 모델을 통해 양수의 지역 보상을 추정할 수 있는데, 탐색 장치는, 기존의 멀티 에이전트 강화학습에서 전역 보상으로 학습하는 공동 가치 함수 외에도, 양수의 지역 보상을 이용해 추가 적인 공동 가치 함수를 학습할 수 있다. 탐색 장치가 전역 보상과 양수의 지역 보상으로 학습한 두 가지 종류의 공동 가치 함수는 수학식 2와 같이 각 에이전트에 대한 개별 유틸리티 값의 함수로 구성된다. 수학식 2"}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수학식 2에서 Qjt는 전역 보상 참값으로 학습한 공동 가치 함수('전역 공동 가치 함수'로 칭함), Qi는 Qjt값을 에 이전트 별로 할당하기 위한 개별 유틸리티 함수('제1 개별 유틸리티 함수'로 칭함), Qjt+는 양수의 지역 보상(양 수 보상 추정값)으로 학습한 공동 가치 함수('양수 보상 공동 가치 함수'로 칭함), Qi+는 Qjt+값을 에이전트 별로 할당하기 위한 개별 유틸리티 함수('제2 개별 유틸리티 함수'로 칭함)이다.수학식 2를 통해 계산되는 개별 유틸리티 함수(제1 개별 유틸리티 함수와 제2 개별 유틸리티 함수)를 이용해 학 습에 사용할 데이터(학습 데이터)를 생성하는 탐색 과정(S130 단계 및 S140 단계)이 수행된다. 참고로, 상기 학 습 데이터는 각 에이전트의 상태, 행동, 전역 보상으로 구성될 수 있다. 양수와 음수의 지역 보상이 동시다발적으로 발생하는 경우, 양수와 음수의 지역 보상의 크기가 하나만 큰 경우 에는 전역 보상 값만으로 충분히 평가가 가능하다. 그러나 양수와 음수의 보상이 모두 크거나 모두 작은 행동은 전역 보상만으로는 행동을 정확히 평가하기 어렵다. 표 1은 전역 보상만으로 행동에 대한 평가를 정확히 수행하 기 어려운 대표적인 예시이다. 표 1 Global reward Positive local rewardNegative local reward Case 1 0 +3 -3 Case 2 0 0 0 표 1에서 Case 1과 Case 2는 모두 전역 보상 값이 0으로 같지만, Case 1에서는 양수의 지역 보상을 받기 때문에 에이전트 중 일부는 목표로 하는 협업에 가까운 행동을 선택한다고 볼 수 있다. 반면, Case 2는 어떠한 보상도 받지 못하는 행동을 수행한다. 탐색 기법의 관점에서는 Case 2와 같은 상태보다는 Case 1과 같은 상태를 우선적 으로 탐색해서 에이전트 중 일부라도 목표로 하는 협업에 대한 행동을 수행해서, 목표로 하는 협업에 대한 행동 과 연관된 데이터를 수집하는 것이 필요하다. 이러한 이유로 전역 보상을 이용해 학습한 가치 함수를 사용하는 방식은 기존의 방법대로 유지하되, 양수의 지역 보상으로 학습한 가치 함수가 높은 행동을 학습 초기에 높은 확 률로 선택하는 탐색 기법을 구성하게 된다. S130 단계는 개별 유틸리티 함수 선택 단계이다. 본 단계는 탐색 장치가 소정의 기준에 따라 제1 개별 유 틸리티 함수 및 제2 개별 유틸리티 함수 중 어느 하나를 선택하는 단계이다. 그리고, S140 단계는 행동 선택 단계(행동 결정 단계)이다. 본 단계는 상태를 기초로 선택된 개별 유틸리티 함 수를 이용하여 에이전트의 행동을 결정하는 단계이다. 탐색 장치는 에이전트의 상태를 기초로 소정의 기 준에 따라 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동 및 임의의 행동 중 어느 하나를 선택한다. 수학식 3은 S130 단계에서 개별 유틸리티 함수를 선택하기 위하여 제1, 제2 개별 유틸리티 함수에 확률을 설정 한 것이다. 수학식 3"}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "그리고 수학식 4는 S140 단계에서 개별 유틸리티 함수를 이용하여 행동을 선택함에 있어서, S130 단계에서 선택 된 개별 유틸리티 함수값이 최대가 되는 행동과 임의의 행동 중 어느 하나를 선택하기 위하여 각 행동에 확률을 설정한 것이다.수학식 4"}
{"patent_id": "10-2023-0010071", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "수학식 3과 수학식 4에서, i는 각 에이전트의 식별자이다. 그리고 ui는 에이전트 i의 행동을 의미한다. Ψi는 Qi 와 Qi+ 중 확률적으로 선택된, 에이전트 i에 대한 개별 유틸리티 함수를 의미한다. 수학식 3에서 ζ는 두 종류의 가치 함수를 선택하는 확률을 결정하는 파라미터로, 학습 초기에 양수의 지역 보 상을 높게 받는 행동을 선택하기 위해, 학습 초기에는 ζ를 1로 설정되었다가 학습이 진행되면서 점점 0에 가까 운 값으로 작아지도록 설정된다. 개별 유틸리티 함수가 선택되면, 이후에는 ε-greedy 방식과 마찬가지로 개별 유틸리티 함수가 최대가 되는 행 동이나 임의의 행동을 선택하게 된다. 수학식 4에서 ε은 ε-greedy(Epsilon-Greedy) 방식에서 행동 선택을 조절하는 파라미터로, 학습 초기에는 임의 의 행동을 주로 선택하도록 1로 설정되었다가 학습이 진행되면서 0에 가까운 값으로 작아지도록 설정된다. 도 3은 전술한 내용을 정리한 것으로, 전역 보상과 양수의 지역 보상으로 학습한 가치 함수들을 이용해 행동을 선택하는 탐색 과정을 나타낸 도면이다. 전역 공동 가치 함수는 복수의 제1 개별 유틸리티 함수로 구성되며, 양수 보상 공동 가치 함수는 복수의 제2 개 별 유틸리티 함수로 구성된다. 탐색 장치가 전역 보상 참값을 기초로 학습을 통해 전역 공동 가치 함수를 생성하면, 제1 개별 유틸리티 함수도 전역 공동 가치 함수와 함께 생성된다. 즉, 탐색 장치는 전역 보상 참값을 기초로 전역 공동 가치 함수와 제1 개별 유틸리티 함수를 학습시킨다. 또한, 탐색 장치는 전체 에이전트에 대하여 상태와 행동을 인코딩 신경망에 입력하여 상태 인코딩 벡터와 행동 인코딩 벡터를 생성하고, 상태 인코딩 벡터와 행동 인코딩 벡터를 양수 보상 추정 모델에 입력하여 양수 보상 추정값을 생성한다. 탐색 장치가 양수 보상 추정값을 기초로 학습을 통해 양수 보상 공동 가치 함수 를 생성하면, 제2 개별 유틸리티 함수도 양수 보상 공동 가치 함수와 함께 생성된다. 즉, 탐색 장치는 양 수 보상 추정값을 기초로 양수 보상 공동 가치 함수와 제2 개별 유틸리티 함수를 학습시킨다. 제1 개별 유틸리티 함수와 제2 개별 유틸리티 함수가 생성되면, 탐색 장치는 제1 개별 유틸리티 함수와 제2 개별 유틸리티 함수를 이용하여 학습에 사용할 데이터(학습 데이터)를 생성하는 탐색 과정을 수행할 수 있 다. 여기에서 학습 데이터는 전체 에이전트의 상태와 행동에 대한 데이터를 포함한다. 수학식 3과 수학식 4를 통해 제시한 바와 같이, 탐색 장치는 기 설정된 유틸리티 함수 선택 확률(ζ)에 따라 제1 개별 유틸리티 함수(확률: 1- ζ)와 제2 개별 유틸리티 함수(확률: ζ) 중 어느 하나를 선택하며, 주어진 에이전트의 상태에 대하여, 기 설정된 행동 선택 확률(ε)에 따라 임의의 행동(확률: ε)과 앞서 선택된 유틸리티 함수의 함수값을 최대화하는 행동(확률: 1-ε) 중 어느 하나를 선택하는 방식으로 학습 데이터를 구축한다. 본 발명의 일 실시예에 따른 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법은 도면에 제시된 흐 름도를 참조로 하여 설명되었다. 간단히 설명하기 위하여 상기 방법은 일련의 블록들로 도시되고 설명되었으나, 본 발명은 상기 블록들의 순서에 한정되지 않고, 몇몇 블록들은 다른 블록들과 본 명세서에서 도시되고 기술된 것과 상이한 순서로 또는 동시에 일어날 수도 있으며, 동일한 또는 유사한 결과를 달성하는 다양한 다른 분기, 흐름 경로, 및 블록의 순서들이 구현될 수 있다. 또한, 본 명세서에서 기술되는 방법의 구현을 위하여 도시된모든 블록들이 요구되지 않을 수도 있다. 한편 도 1 내지 도 3를 참조한 설명에서, 각 단계는 본 발명의 구현예에 따라서, 추가적인 단계들로 더 분할되 거나, 더 적은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있다. 아울러, 기타 생략된 내용이라 하더라도 도 4의 탐색 장치에 관한 내용은 도 1 내지 도 3의 내용에 적용될 수 있다. 또한, 도 1 내지 도 3의 내용은 도 4의 탐색 장치에 관한 내용에 적용될 수 있다. 도 4는 본 발명의 일 실시예에 따른, 멀티 에이전트 강화학습을 위한 보상 분해 기반 탐색 장치를 설명하기 위 한 블록도이다. 도 4를 참조하면, 멀티 에이전트 강화학습을 위한 보상 분해 기반 탐색 장치 (1000, '탐색 장치'로 약칭)은, 본 발명에 따른 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법을 수행하기 위한 장치이다. 상기 탐 색 장치는 컴퓨터 시스템일 수 있다. 탐색 장치는, 버스를 통해 통신하는 1개 이상의 프로세서, 메모리, 입력 인터페이스 장치, 출력 인터페이스 장치 및 저장 장치 중 적어도 하나를 포함할 수 있다. 탐색 장치 는 또한 무선 또는 유선 네트워크에 결합된 통신 장치를 더 포함할 수 있다. 프로세서는 본 발명에 따른 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법을 실행한다. 즉 프로세서는 전역 보상으로부터 양수와 음수의 보상을 분해하고, 전역 보상과 양수 보상으로 공동 가치 함 수 및 개별 유틸리티 함수를 학습하며, 전술한 기준에 따라 개별 유틸리티 함수를 선택하고, 각 에이전트의 상 태를 기초로 선택된 개별 유틸리티 함수를 이용하여 각 에이전트의 행동을 선택한다. 프로세서의 기능에 관한 상세한 내용은 도 1 내지 도 3을 참조한 설명을 참조하여 이해할 수 있다. 프로세서는 중앙 처리 장치(central processing unit, CPU)이거나, 또는 메모리 또는 저장 장치 에 저장된 명령을 실행하는 반도체 장치일 수 있다. 메모리 및 저장 장치는 컴퓨터에서 판독 가능한 명령들을 저장한다. 또한 메모리 및 저장 장 치는, 통신 장치로 입력된 학습 데이터나 보상 분해 모델, 손실함수, 프로세서가 본 발명에 따른 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법을 실행하는 과정에서 생성된 다양한 데이터 (전역 보상 추정값, 개별 유틸리티 함수 선택확률, 에이전트의 행동 선택 확률) 등을 저장한다. 프로세서는, 메모리에 저장된, 컴퓨터에서 판독 가능한 명령들을 실행함으로써, 전체 에이전트의 상태, 전체 에이전트의 행동 및 전역 보상 참값을 포함한 학습 데이터를 기초로 신경망 학습을 통해 양수 보상 추정 모델을 생성하고, 에이전트 각각에 대하여, 전역 보상 참값을 기초로 제1 개별 유틸리티 함수를 생성하고, 양수 보상 추정 모델을 이용하여 제2 개별 유틸리티 함수를 생성하며, 에이전트 각각의 상태를 기초로, 제1 개 별 유틸리티 함수와 제2 개별 유틸리티 함수를 이용하여, 에이전트 각각의 행동을 결정하도록 구성된다. 프로세서는 학습 데이터에 포함된 에이전트의 상태를 인코딩 신경망에 입력하여 상태 인코딩 벡터를 생성 하고, 학습 데이터에 포함된 에이전트의 행동을 인코딩 신경망에 입력하여 행동 인코딩 벡터를 생성한다. 프로세서는 상태 인코딩 벡터와 행동 인코딩 벡터를, 전역 보상 신경망에 입력하여 전역 보상 추정값을 생성한다. 프로세서는 전역 보상 추정값과 전역 보상 참값을 기초로 손실 함수를 이용하여 전역 보상 신경망에 포함 된 양수의 지역 보상 신경망을 훈련하여 양수 보상 추정 모델을 생성하도록 구성된다. 프로세서는 전역 보상 추정값과 전역 보상 참값을 손실 함수에 입력하고, 손실 함수의 함수값이 최소가 되도록 양수의 지역 보상 신경망을 훈련하여 양수 보상 추정 모델을 생성하도록 구성된다. 프로세서는, 소정의 기준에 따라 제1 개별 유틸리티 함수 및 제2 개별 유틸리티 함수 중 어느 하나를 선 택한다. 그리고 프로세서는 에이전트 각각의 상태를 기초로, 소정의 기준에 따라, 임의의 행동 및 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동 중 어느 하나를 선택하도록 구성된다. 프로세서는, 기 설정된 확률에 따라 상기 제1 개별 유틸리티 함수 및 상기 제2 개별 유틸리티 함수 중 어 느 하나를 선택하도록 구성될 수 있다. 이 경우, 제1 개별 유틸리티 함수를 선택할 확률은 1-ζ, 제2 개별 유틸 리티 함수를 선택할 확률은 ζ로 설정될 수 있다. 제2 개별 유틸리티 함수를 선택할 확률(ζ)은 최초에 1로 설 정되고, 탐색 과정의 진행에 따라 0에 수렴하도록 설정될 수 있다. 프로세서는, 기 설정된 행동 선택 확률에 따라 상기 임의의 행동 및 상기 선택된 개별 유틸리티 함수값이 최대가 되는 행동 중 어느 하나의 행동을 선택하도록 구성될 수 있다. 이 경우, 임의의 행동을 선택할 확률은 ε, 선택된 개별 유틸리티 함수값이 최대가 되는 행동을 선택할 확률은 1-ε로 설정될 수 있다. 그리고, 임의의 행동을 선택할 확률은 최초 1로 설정되고, 탐색 과정의 진행에 따라 0에 수렴하도록 설정될 수 있다. 메모리 및 저장 장치는 다양한 형태의 휘발성 또는 비휘발성 저장 매체를 포함할 수 있다. 예를 들 어, 메모리는 ROM(read only memory) 및 RAM(random access memory)를 포함할 수 있다. 본 기재의 실시예에서 메모리는 프로세서의 내부 또는 외부에 위치할 수 있고, 메모리는 이미 알려진 다양한 수단을 통해 프로세서와 연결될 수 있다. 메모리는 다양한 형태의 휘발성 또는 비휘발성 저장 매체이며, 예를 들어, 메모리는 읽기 전용 메모리(read-only memory, ROM) 또는 랜덤 액세스 메모리(random access memory, RAM)를 포함할 수 있다. 따라서, 본 발명의 실시예는 컴퓨터에 구현된 방법으로서 구현되거나, 컴퓨터 실행 가능 명령이 저장된 비일시 적 컴퓨터 판독 가능 매체로서 구현될 수 있다. 한 실시예에서, 프로세서에 의해 실행될 때, 컴퓨터 판독 가능 명령은 본 기재의 적어도 하나의 양상에 따른 방법을 수행할 수 있다. 통신 장치는 유선 신호 또는 무선 신호를 송신 또는 수신할 수 있다. 통신 장치는 전역 보상 추정 모델과 양수 보상 추정 모델의 생성을 위한 학습 데이터를 입력받을 수 있다. 또한, 본 발명의 실시예에 따른 방법은 다양한 컴퓨터 수단을 통해 수행될 수 있는 프로그램 명령 형태로 구현 되어, 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터 판독 가능 매체에 기록되는 프로그램 명령은, 본 발명의 실시예를 위해 특별히 설계되어 구성된 것이거나, 컴퓨터 소프트웨어 분야의 통상의 기술자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체는 프로그램 명령을 저장하고 수행하도록 구성된 하드웨어 장치를 포함할 수 있다. 예를 들 어, 컴퓨터 판독 가능 기록 매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광 기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 롬(ROM), 램(RAM), 플래시 메모리 등일 수 있다. 프로그램 명령은 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라, 인터프리터 등을 통해 컴퓨터에 의해 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 상기에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특 허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다."}
{"patent_id": "10-2023-0010071", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른, 보상 분해에 기반한 멀티 에이전트 강화학습에서의 탐색 방법을 설명하기 위한 흐름도. 도 2는 본 발명의 일 실시예에서 전역 보상을 양수 보상과 음수 보상으로 분해하기 위한 보상 분해 신경망 모델 의 구성을 나타낸 도면. 도 3은 본 발명의 일 실시예에서 전역 보상과 양수의 지역 보상으로 학습한 가치 함수들을 이용해 행동을 선택 하는 탐색 과정을 나타낸 도면. 도 4는 본 발명의 일 실시예에 따른, 멀티 에이전트 강화학습을 위한 보상 분해 기반 탐색 장치를 설명하기 위 한 블록도."}
