{"patent_id": "10-2023-0059024", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0162237", "출원번호": "10-2023-0059024", "발명의 명칭": "하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템 및 방법", "출원인": "씨제이올리브네트웍스 주식회사", "발명자": "계세원"}}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "학습 데이터를 이용하여 인공지능 모델을 학습하는 학습부; 상기 학습된 인공지능 모델을 저장하는 저장부; 상기 학습된 인공지능 모델을 중간 표현 형식(Intermediate Representation, IR)으로 변환하고 미리 정해진 배포 조건을 만족하도록 상기 변환된 인공지능 모델을 최적화하는 변환부; 및 상기 최적화된 인공지능 모델이 미리 정해진 배포 조건을 만족하면 상기 최적화된 인공지능 모델을 배포하는 추론부를 포함하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 학습부는 온프레미스(On-premise) 시스템에서 구축되고, 상기 저장부, 상기 변환부 및 상기 추론부는 퍼블릭 클라우드(Public Cloud) 시스템에서 구축되는, 하이브리드아키텍처 기반의 인공지능 모델 배포 자동화 시스템."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 미리 정해진 배포 조건은, 모델 용량, 응답 지연 시간, CPU 사용 여부, GPU 사용 여부, CPU 사용량 및 메모리 사용량 중에서 적어도 하나를 포함하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 변환부는, 상기 변환된 인공지능 모델이 상기 미리 정해진 배포 조건에 부합하는 경우, 상기 변환된 인공지능 모델의 모델최적화 동작을 완료하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 변환부는, 상기 변환된 인공지능 모델이 상기 미리 정해진 배포 조건에 부합하지 않는 경우, 상기 변환된 인공지능 모델을하드웨어에 최적화 가능한 런타임(Runtime)에 대응되는 모델 형식으로 추가 변환한 후 배포 조건 부합 여부를추가 확인하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 변환부는, 상기 미리 정해진 배포 조건의 부합 여부에 따라 상기 변환된 인공지능 모델을 다른 모델 형식으로 추가 변환한후 배포 조건 부합 여부를 추가 확인하는 동작을 반복적으로 수행하는, 하이브리드 아키텍처 기반의 인공지능모델 배포 자동화 시스템. 공개특허 10-2024-0162237-3-청구항 7 제1항에 있어서, 상기 변환부는, 상기 변환된 모델 형식이 런타임 최적화 이후 상기 미리 정해진 배포 조건에 부합하지 않는 경우, 기설정된 모델 압축 기법을 이용하여 소프트웨어적으로 추가 변환된 인공지능 모델을 최적화하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 추론부는, 상기 미리 정해진 배포 조건을 만족하는 인공지능 모델 중에서 사용자가 배포될 시스템의 특성에 맞는 인공지능모델을 선택하면 상기 선택된 인공지능 모델을 배포하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "인공지능 모델 배포 자동화 시스템에 의해 수행되는 인공지능 모델 배포 자동화 방법에 있어서, 학습 데이터를 이용하여 인공지능 모델을 학습하는 단계; 상기 학습된 인공지능 모델을 중간 표현 형식(Intermediate Representation, IR)으로 변환하고 미리 정해진 배포 조건을 만족하도록 상기 변환된 인공지능 모델을 최적화하는 단계; 및 상기 최적화된 인공지능 모델이 미리 정해진 배포 조건을 만족하면 상기 최적화된 인공지능 모델을 배포하는 단계를 포함하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 학습하는 단계는 온프레미스(On-premise) 시스템에서 수행되고, 상기 최적화하는 단계 및 상기 배포하는 단계는 퍼블릭 클라우드(Public Cloud) 시스템에서 수행되는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 상기 미리 정해진 배포 조건은, 모델 용량, 응답 지연 시간, CPU 사용 여부, GPU 사용 여부, CPU 사용량 및 메모리 사용량 중에서 적어도 하나를 포함하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서, 상기 최적화하는 단계는, 상기 변환된 인공지능 모델이 상기 미리 정해진 배포 조건에 부합하는 경우, 상기 변환된 인공지능 모델의 모델최적화 동작을 완료하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서, 상기 최적화하는 단계는, 공개특허 10-2024-0162237-4-상기 변환된 인공지능 모델이 상기 미리 정해진 배포 조건에 부합하지 않는 경우, 상기 변환된 인공지능 모델을하드웨어에 최적화 가능한 런타임(Runtime)에 대응되는 모델 형식으로 추가 변환한 후 배포 조건 부합 여부를추가 확인하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제9항에 있어서, 상기 최적화하는 단계는, 상기 미리 정해진 배포 조건의 부합 여부에 따라 상기 변환된 인공지능 모델을 다른 모델 형식으로 추가 변환한후 배포 조건 부합 여부를 추가 확인하는 동작을 반복적으로 수행하는, 하이브리드 아키텍처 기반의 인공지능모델 배포 자동화 방법."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서, 상기 최적화하는 단계는, 상기 변환된 모델 형식이 런타임 최적화 이후 상기 미리 정해진 배포 조건에 부합하지 않는 경우, 기설정된 모델 압축 기법을 이용하여 소프트웨어적으로 추가 변환된 인공지능 모델을 최적화하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법."}
{"patent_id": "10-2023-0059024", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제9항에 있어서, 상기 배포하는 단계는, 상기 미리 정해진 배포 조건을 만족하는 인공지능 모델 중에서 사용자가 배포될 시스템의 특성에 맞는 인공지능모델을 선택하면 상기 선택된 인공지능 모델을 배포하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법."}
{"patent_id": "10-2023-0059024", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템 및 방법에 관한 것으로, 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템은 학습 데이터를 이용하여 인공지 능 모델을 학습하는 학습부, 상기 학습된 인공지능 모델을 저장하는 저장부, 상기 학습된 인공지능 모델을 중간 표현 형식(Intermediate Representation, IR)으로 변환하고 미리 정해진 배포 조건을 만족하도록 상기 변환된 인공지능 모델을 최적화하는 변환부, 및 상기 최적화된 인공지능 모델이 미리 정해진 배포 조건을 만족하면 상기 최적화된 인공지능 모델을 배포하는 추론부를 포함한다."}
{"patent_id": "10-2023-0059024", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 모델 자동화 기술에 관한 것으로, 하이브리드 아키텍처 기반으로 인공지능 모델에 대한 학 습, 최적화 및 배포를 자동화시키기 위한, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0059024", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일반적으로, 인공지능(AI, Artificial Intelligence) 시스템에서의 동작에는 데이터 전처리 동작, 모델 학습 동 작, 모델 분석 동작, 모델 배포 동작 등 다양한 동작들이 포함될 수 있다. 그러나 일부 동작에 대해 작업자가 수동으로 수행해야 하며, 이런 단계를 수동으로 수행하기란 번거롭고 오류가 발생하기 쉽다. 또한, 인공지능 시스템에서 새로운 학습 데이터를 사용하려면 데이터 검증, 전처리, 모델 훈련, 분석 및 배포를 포함하는 워크플로우를 재설정해야 한다. 작업자는 이런 단계를 수작업으로 수행할 수 있다. 하지만, 비용이 많 이 들고 오류를 발생시킬 수 있다. 특히, 종래의 인공지능 시스템은 인공지능 모델 학습과 인공지능 모델 배포 과정이 분리되어 있는 경우가 대부 분이다. 종래의 인공지능 시스템은 이러한 모델 학습이나 모델 배포 동작이 자동화되지 않아 작업의 대부분을 작업자가 직접 진행해야 한다. 여기서, 인공지능 모델 성능에 관한 요구 조건은 있으나 배포 관점에서의 요구 조건은 생략되는 경우가 대다수 이다. 인공지능 모델의 배포 관점에서의 요구 조건이 존재하더라도 인공지능 모델 최적화와 관련된 추가 인력이"}
{"patent_id": "10-2023-0059024", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "필요하게 된다. 발명의 내용"}
{"patent_id": "10-2023-0059024", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 실시예들은 온프레미스 시스템 및 퍼블릭 클라우드 시스템과 자동화된 인공지능 모델 학습 및 모델 최적화를 통해 사전에 정해진 배포 조건을 만족하는 인공지능 모델들이 자동으로 배포될 수 있도록 보장하기 위 한, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템 및 방법을 제공하고자 한다. 다만, 본 발명의 해결하고자 하는 과제는 이에 한정되는 것이 아니며, 본 발명의 사상 및 영역으로부터 벗어나 지 않는 범위의 환경에서도 다양하게 확장될 수 있을 것이다."}
{"patent_id": "10-2023-0059024", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 학습 데이터를 이용하여 인공지능 모델을 학습하는 학습부; 상기 학습된 인공지 능 모델을 저장하는 저장부; 상기 학습된 인공지능 모델을 중간 표현 형식(Intermediate Representation, IR)으 로 변환하고 미리 정해진 배포 조건을 만족하도록 상기 변환된 인공지능 모델을 최적화하는 변환부; 및 상기 최 적화된 인공지능 모델이 미리 정해진 배포 조건을 만족하면 상기 최적화된 인공지능 모델을 배포하는 추론부를 포함하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템이 제공될 수 있다. 상기 학습부는 온프레미스(On-premise) 시스템에서 구축되고, 상기 저장부, 상기 변환부 및 상기 추론부는 퍼블 릭 클라우드(Public Cloud) 시스템에서 구축될 수 있다. 상기 미리 정해진 배포 조건은, 모델 용량, 응답 지연 시간, CPU 사용 여부, GPU 사용 여부, CPU 사용량 및 메 모리 사용량 중에서 적어도 하나를 포함할 수 있다. 상기 변환부는, 상기 변환된 인공지능 모델이 상기 미리 정해진 배포 조건에 부합하는 경우, 상기 변환된 인공 지능 모델의 모델 최적화 동작을 완료할 수 있다. 상기 변환부는, 상기 변환된 인공지능 모델이 상기 미리 정해진 배포 조건에 부합하지 않는 경우, 상기 변환된 인공지능 모델을 하드웨어에 최적화 가능한 런타임(Runtime)에 대응되는 모델 형식으로 추가 변환한 후 배포 조 건 부합 여부를 추가 확인할 수 있다. 상기 변환부는, 상기 미리 정해진 배포 조건의 부합 여부에 따라 상기 변환된 인공지능 모델을 다른 모델 형식 으로 추가 변환한 후 배포 조건 부합 여부를 추가 확인하는 동작을 반복적으로 수행할 수 있다. 상기 변환부는, 상기 변환된 모델 형식이 런타임 최적화 이후 상기 미리 정해진 배포 조건에 부합하지 않는 경 우, 기설정된 모델 압축 기법을 이용하여 소프트웨어적으로 추가 변환된 인공지능 모델을 최적화할 수 있다. 상기 추론부는, 상기 미리 정해진 배포 조건을 만족하는 인공지능 모델 중에서 사용자가 배포될 시스템의 특성 에 맞는 인공지능 모델을 선택하면 상기 선택된 인공지능 모델을 배포할 수 있다. 한편, 본 발명의 다른 실시예에 따르면, 인공지능 모델 배포 자동화 시스템에 의해 수행되는 인공지능 모델 배 포 자동화 방법에 있어서, 학습 데이터를 이용하여 인공지능 모델을 학습하는 단계; 상기 학습된 인공지능 모델 을 중간 표현 형식(Intermediate Representation, IR)으로 변환하고 미리 정해진 배포 조건을 만족하도록 상기 변환된 인공지능 모델을 최적화하는 단계; 및 상기 최적화된 인공지능 모델이 미리 정해진 배포 조건을 만족하 면 상기 최적화된 인공지능 모델을 배포하는 단계를 포함하는, 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법이 제공될 수 있다. 상기 학습하는 단계는 온프레미스(On-premise) 시스템에서 수행되고, 상기 최적화하는 단계 및 상기 배포하는 단계는 퍼블릭 클라우드(Public Cloud) 시스템에서 수행될 수 있다. 상기 미리 정해진 배포 조건은, 모델 용량, 응답 지연 시간, CPU 사용 여부, GPU 사용 여부, CPU 사용량 및 메 모리 사용량 중에서 적어도 하나를 포함할 수 있다. 상기 최적화하는 단계는, 상기 변환된 인공지능 모델이 상기 미리 정해진 배포 조건에 부합하는 경우, 상기 변 환된 인공지능 모델의 모델 최적화 동작을 완료할 수 있다. 상기 최적화하는 단계는, 상기 변환된 인공지능 모델이 상기 미리 정해진 배포 조건에 부합하지 않는 경우, 상 기 변환된 인공지능 모델을 하드웨어에 최적화 가능한 런타임(Runtime)에 대응되는 모델 형식으로 추가 변환한후 배포 조건 부합 여부를 추가 확인할 수 있다. 상기 최적화하는 단계는, 상기 미리 정해진 배포 조건의 부합 여부에 따라 상기 변환된 인공지능 모델을 다른 모델 형식으로 추가 변환한 후 배포 조건 부합 여부를 추가 확인하는 동작을 반복적으로 수행할 수 있다. 상기 최적화하는 단계는, 상기 변환된 모델 형식이 런타임 최적화 이후 상기 미리 정해진 배포 조건에 부합하지 않는 경우, 기설정된 모델 압축 기법을 이용하여 소프트웨어적으로 추가 변환된 인공지능 모델을 최적화할 수 있다. 상기 배포하는 단계는, 상기 미리 정해진 배포 조건을 만족하는 인공지능 모델 중에서 사용자가 배포될 시스템 의 특성에 맞는 인공지능 모델을 선택하면 상기 선택된 인공지능 모델을 배포할 수 있다."}
{"patent_id": "10-2023-0059024", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "개시된 기술은 다음의 효과를 가질 수 있다. 다만, 특정 실시예가 다음의 효과를 전부 포함하여야 한다거나 다 음의 효과만을 포함하여야 한다는 의미는 아니므로, 개시된 기술의 권리범위는 이에 의하여 제한되는 것으로 이 해되어서는 아니 될 것이다. 본 발명의 실시예들은 온프레미스 시스템 및 퍼블릭 클라우드 시스템과 자동화된 인공지능 모델 학습 및 모델 최적화를 통해 사전에 정해진 배포 조건을 만족하는 인공지능 모델들이 자동으로 배포될 수 있도록 보장할 수 있다. 본 발명의 실시예들은 인공지능 모델을 중간 표현 형식(Intermediate Representation, IR)으로 변환한 후, 런타 임 엔진 및 모델 압축 기법을 적용하여 모델 관련 코드가 사용된 머신러닝 라이브러리와 무관하게 인공지능 시 스템을 이용 가능하게 한다. 본 발명의 실시예들은 학습이 완료된 인공지능 모델을 런타임 엔진 최적화 및 압축 기법을 통해 모델의 추론 시 간을 최소화시킬 수 있다. 본 발명의 실시예들은 온프레미스와 퍼블릭 클라우드를 이용하여 비용효율적으로 인공지능 모델을 학습 및 추론 하는 동시에, 학습과 배포를 하나의 파이프라인으로 구성하여 필요한 인력과 시간을 절감하고, 모델 배포와 관 련된 제약 조건을 만족하기 위한 모델 최적화를 자동으로 진행할 수 있다. 본 발명의 실시예들은 모델 학습 담당자가 기업에 이미 구축되어 있는 모든 인프라를 모델 연구 및 성능 향상을 위해 사용할 수 있다. 또한, 본 발명의 실시예들은 모델 배포 책임자가 인공지능 모델 학습 및 변환부에서 일어나는 일에 대한 신경을 안쓰고도 배포된 모델의 최소한의 성능을 보장할수 있으며, 최적화 과정을 전문 인력 없이 진행할 수 있다."}
{"patent_id": "10-2023-0059024", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변환을 가할 수 있고 여러가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 구체적으로 설명하고자 한다. 그러나 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 기술적 사상 및 기술 범위에 포함되는 모든 변환, 균등물 내지 대체물을 포함하는 것으로 이해될 수 있다. 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 구성요소들이 용어들에 의해 한정되 는 것은 아니다. 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 본 발명에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 본 발명에서 사용한 용어는 본 발명에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용 어들을 선택하였으나 이는 당 분야에 종사하는 기술자의 의도, 판례, 또는 새로운 기술의 출현 등에 따라 달라 질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분 에서 상세히 그 의미를 기재할 것이다. 따라서 본 발명에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용 어가 가지는 의미와 본 발명의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 발명에서, \"포함하다\" 또 는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이 들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 이하, 본 발명의 실시예들을 첨부 도면을 참조하여 상세히 설명하기로 하며, 첨부 도면을 참조하여 설명함에 있 어, 동일하거나 대응하는 구성요소는 동일한 도면번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 도 1은 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템의 구성도이 다. 도 1에 도시된 바와 같이, 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템은 학습부, 저장부, 변환부 및 추론부를 포함한다. 그러나 도시된 구성요소 모 두가 필수 구성요소인 것은 아니다. 도시된 구성요소보다 많은 구성요소에 의해 하이브리드 아키텍처 기반의 인 공지능 모델 배포 자동화 시스템이 구현될 수도 있고, 그보다 적은 구성요소에 의해서도 하이브리드 아키 텍처 기반의 인공지능 모델 배포 자동화 시스템이 구현될 수 있다. 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템은 온프레미스 와 퍼블릭 클라우드를 이용하여 비용효율적으로 인공지능 모델을 학습 및 추론하는 동시에, 학습과 배포를 하나 의 파이프라인으로 구성하여 필요한 인력과 시간을 절감하고, 모델 배포와 관련된 제약 조건을 만족하기 위한 모델 최적화를 자동으로 진행할 수 있다. 여기서, 온프레미스 시스템은 기업/조직에서 자체적으로 보유하고 있 는 서버들에서 시스템을 구동하는 방식으로 동작한다. 퍼블릭 클라우드 시스템은 구글(Google), AWS와 같은 업 체에서 컴퓨팅 서비스 및 인프라를 관리/제공하고 기업들은 필요만큼 인프라를 구입하여 시스템을 구동하는 방 식으로 동작한다. 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템은 인공지능 모델을 중간 표현 형식(Intermediate Representation, IR)으로 변환한 후 런타임 엔진 및 모델 압축 기법을 적 용하여 모델 관련 코드가 사용된 인공지능 라이브러리와 무관하게 시스템을 이용 가능하다. 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템은 학습이 완 료된 인공지능 모델을 런타임 엔진 최적화 및 압축 기법을 통해 모델의 추론 시간을 최소화 가능하다. 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템은 온프레미스 시스템과 퍼블릭 클라우드 시스템을 사전에 정해진 배포 조건을 만족하는 인공지능 모델들이 자동으로 배포될 수 있도록 보장하는 자동화된 AI 모델 학습 및 모델 최적화 기술을 이용할 수 있다. 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템을 통해서 모 델 학습 담당자는 기업에 이미 구축되어 있는 모든 인프라를 모델 연구 및 성능 향상을 위해 사용할 수 있다. 또한, 모델 배포 책임자는 인공지능 모델 학습용 인프라를 안 쓰면서도 배포된 모델의 최소한의 성능을 보장할 수 있으며, 최적화 과정을 전문 인력 없이 진행할 수 있다. 이하, 도 1의 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템의 각 구성요소들의 구체적인 구성 및 동작을 설명한다. 학습부는 학습 데이터를 이용하여 인공지능 모델을 학습한다. 일례로, 학습부는 온프레미스 시스템의 일부로 구축될 수 있으며, 인공지능 모델의 학습을 위해 사용될 수 있다. 저장부는 학습부에서 학습된 인공지능 모델을 저장한다. 일례로, 저장부는 퍼블릭 클라우드 시 스템의 일부로 구축될 수 있으며, 인공지능 모델과 인공지능 모델 관련 파일 또는 관련 로그 등을 저장하기 위 해 사용될 수 있다. 변환부는 학습부에서 학습된 인공지능 모델을 중간 표현 형식(Intermediate Representation, IR)으 로 변환하고 미리 정해진 배포 조건을 만족하도록 그 변환된 인공지능 모델을 최적화한다. 일례로, 변환부(13 0)는 퍼블릭 클라우드 시스템의 일부로 구축될 수 있으며, 학습된 인공지능 모델을 중간 표현 형식(IR)으 로 변환하고, 추론 엔진(런타임 엔진) 및 인공지능 모델의 경량화를 적용하기 위해 사용될 수 있다. 추론부는 변환부에서 최적화된 인공지능 모델이 미리 정해진 배포 조건을 만족하면, 최적화된 인공지 능 모델을 배포한다. 일례로, 추론부는 퍼블릭 클라우드 시스템의 일부로 구축될 수 있으며, 최종 선 택된 인공지능 모델의 배포를 위해 사용될 수 있다. 추론 엔진은 런타임 엔진으로 지칭될 수 있으며, 인공지능 모델을 다양한 플랫폼(예컨대, CPU, GPU, 에지 등)과 다양한 프레임워크에서 사용할 수 있도록 만들어진 모델 실행 장치를 나타낸다. 추론부는 추론 엔진을 이용하여 추론부의 동작을 수행할 수 있다. 모델 배포와 관련하여, 추론부는 최종 선택된 인공지능 모델의 배포를 위해, 인공지능 모델을 사용(추론) 할 수 있는 주소(엔드포인트)를 생성한다. 따라서 추론부는 최종 선택된 인공지능 모델을 VPC(Virtual Private Cloud) 상에 로드하고, 최종 선택된 인공지능 모델을 사용할 수 있는 주소를 외부로 노출하여 해당 주 소로 요청이 들어올 경우 인공지능 모델을 실행시켜 나온 결과를 반환하는 동작을 수행한다. 실시예들에 따르면, 학습부는 온프레미스(On-premise) 시스템에서 구축되고, 저장부, 변환부 및 추론부는 퍼블릭 클라우드(Public Cloud) 시스템에서 구축될 수 있다. 실시예들에 따르면, 미리 정해진 배포 조건은, 모델 용량, 응답 지연 시간, CPU 사용 여부, GPU 사용 여부, CPU 사용량 및 메모리 사용량 중에서 적어도 하나를 포함할 수 있다. 실시예들에 따르면, 변환부는 변환된 인공지능 모델이 미리 정해진 배포 조건에 부합하는 경우, 변환된 인 공지능 모델의 모델 최적화 동작을 완료할 수 있다. 실시예들에 따르면, 변환부는 변환된 인공지능 모델이 미리 정해진 배포 조건에 부합하지 않는 경우, 변환 된 인공지능 모델을 하드웨어에 최적화 가능한 런타임(Runtime)에 대응되는 모델 형식으로 추가 변환한 후 배포 조건 부합 여부를 추가 확인할 수 있다. 실시예들에 따르면, 변환부는 미리 정해진 배포 조건의 부합 여부에 따라 변환된 인공지능 모델을 다른 모 델 형식으로 추가 변환한 후 배포 조건 부합 여부를 추가 확인하는 동작을 반복적으로 수행할 수 있다. 실시예들에 따르면, 변환부는 변환된 모델 형식이 런타임 최적화 이후 미리 정해진 배포 조건에 부합하지 않는 경우, 기설정된 모델 압축 기법을 이용하여 소프트웨어적으로 추가 변환된 인공지능 모델을 최적화할 수 있다. 실시예들에 따르면, 추론부는 미리 정해진 배포 조건을 만족하는 인공지능 모델 중에서 사용자가 배포될 시스템의 특성에 맞는 인공지능 모델을 선택하면 선택된 인공지능 모델을 배포할 수 있다. 도 2는 본 발명의 일 실시예에 따른 인공지능 모델 배포 자동화 시스템이 하이브리드 아키텍처 기반으로 구축된 예시를 나타난 도면이다. 도 2에 도시된 바와 같이, 인공지능 모델 배포 자동화 시스템은 온프레미스 시스템 및 퍼블릭 클라우 드 시스템이 포함된 하이브리드 아키텍처 기반으로 구축될 수 있다. 여기서, 인공지능 모델 배포 자동화 시스템의 학습부, 저장부, 변환부 및 추론부 모두 온프레미스 시스템 또는 퍼블 릭 클라우드 시스템에서 구축될 수 있지만, 온프레미스 시스템 및 퍼블릭 클라우드 시스템 각각 이 가진 장점을 극대화하기 위하여 함께 이용될 수 있다. 인공지능 모델 배포 자동화 시스템의 각 구성요소가 구축되는 곳의 장단점에 따라 서로 다르게 구축될 수 있다. 일례로, 인공지능 모델의 학습부는 다수의 GPU를 사용하게 된다. 학습부가 퍼블릭 클라우드 시스템 에서 구축하게 되면 단기적으로는 온프레미스 시스템에 구축하는 것 보다 비용이 적을 수 있지만 장기적으로는 온프레미스 시스템에 구축되는 것 대비 큰 비용이 들어갈 수 있다. 따라서 학습부는 온프 레미스 시스템에 구축하는 것이 바람직할 수 있다. 반대로, 추론부는 GPU를 항상 사용하지 않아도 되며, 퍼블릭 클라우드 시스템에서는 요청의 수에 따 라 스케일 아웃/인, 스케일 업/다운을 자유롭게 할 수 있다. 또한, 온프레미스 시스템은 서버 관리 주체가 서버 관리자에게 있는 반면, 퍼블릭 클라우드 시스템에서는 서버에 문제가 생길 경우 자동으로 서버를 재 부팅하는 등의 기능이 존재하여 상대적으로 안정적인 서비스가 가능하다. 따라서 추론부는 온프레미스 시 스템에 구축하는 것이 바람직할 수 있다. 저장부와 변환부 역시 온프레미스 시스템에도 위치할 수 있다. 다만, 저장부는 퍼블릭 클 라우드 시스템에서 자동 백업 및 일정 수준 이상의 가용성을 보장한다는 측면과 퍼블릭 클라우드 시스템 에 위치한 추론부에서 바로 데이터를 이용할 수 있다는 장점 때문에 퍼블릭 클라우드 시스템에 위치하는 것이 효율적일 수 있다. 변환부는 필요한 자원이 그리 크지 않고, 저장부에 바로 접근 가능 하도록 퍼블릭 클라우드 시스템의 일부로 구성될 수 있다. 이와 같이, 인공지능 모델 배포 자동화 시스템의 각 구성요소는 온프레미스 시스템 또는 퍼블릭 클라 우드 시스템에 개별적으로 구축될 수 있으며, 특정 시스템에만 구축되는 것으로 한정되지 않는다. 도 3은 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법을 나타낸 흐 름도이다. 도 3에 도시된 바와 같이, 단계 S101에서, 인공지능 모델 배포 자동화 시스템은 하이브리드 아키텍처 기반 으로 인공지능 모델의 학습을 시작한다. 단계 S102에서, 인공지능 모델 배포 자동화 시스템의 학습부는 학습 데이터를 이용하여 인공지능 모 델을 학습시킨다. 단계 S103에서, 인공지능 모델 배포 자동화 시스템의 변환부는 학습된 인공지능 모델이 저장부 에 업로드되면 업로드된 인공지능 모델을 새로운 인공지능 모델로 인식한다. 단계 S104에서, 인공지능 모델 배포 자동화 시스템의 변환부는 인공지능 모델을 중간 표현 형식으로 변환한다. 단계 S105에서, 인공지능 모델 배포 자동화 시스템의 변환부는 변환된 인공지능 모델에 대해 미리 정 해진 배포 조건이 만족할 때까지 런타임 최적화 및 인공지능 모델의 압축을 실험한다. 단계 S106에서, 인공지능 모델 배포 자동화 시스템의 추론부는 미리 정해진 배포 조건이 만족되는 인 공지능 모델을 배포한다. 도 4는 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법에서 모델 학 습 동작을 나타낸 순서도이다. 도 4에 도시된 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법에서 모델 학습 동작은 온프레미스 시스템에 구축된 학습부에 의해 수행될 수 있다. 다만, 학습부는 온프레미스 시스템에 구 축되는 것으로 한정되지 않는다. 단계 S201 내지 S203에서 인공지능 모델 배포 자동화 시스템의 학습부는 모델 학습에 대한 프로세스 진행 전 사전 정의가 필요한 부분을 준비한다. 단계 S201에서, 학습부는 인공지능 모델의 배포 자동화를 위한 배포 조건을 미리 정의한다. 학습부는 인공지능 모델이 배포되기 위한 배포 조건을 작성한다. 배포 조건에는 모델 용량, 응답 지연 시간, CPU 사용 여 부, GPU 사용 여부, CPU 사용량 및 메모리 사용량 중에서 적어도 하나를 포함될 수 있다. 이와 같이, 배포 조건 에는 미리 정해진 모델 용량이나 응답 지연 시간 등이 포함될 수 있다. 또한, 배포 조건에는 미리 정해진 특정 CPU 제조 업체의 사용 여부나 GPU 사용 여부 등의 하드웨어적인 배포 조건을 포함될 수 있다. 배포 조건에는 인 공지능 분석에 필요한 CPU 및 Memory 사용량이 포함될 수 있다. 단계 S202에서, 학습부는 학습 데이터를 저장부에 업로드한다. 이는 인공지능 모델의 모델 학습에 필 요한 학습 데이터를 저장부에 업로드하는 것이다. 단계 S203에서, 학습부는 인공지능 모델의 코드를 작성한다. 학습부는 우선 인공지능 모델을 정의하 고 그 정의된 모델에 맞게 학습을 진행할 코드를 작성할 수 있다. 단계 S204에서, 학습부는 저장부에 업로드된 학습 데이터를 이용하여 인공지능 모델을 학습시킨다. 단계 S205에서, 학습부는 인공지능 모델의 학습을 진행한 후 학습된 인공지능 모델을 저장부에 업로 드한다. 또한, 학습부는 인공지능 모델의 학습 결과물 파일과 인공지능 모델을 가지고 입력 데이터에 대한 예측을 위한 코드가 포함된 데이터들을 업로드할 수 있다. 또한, 학습부는 인공지능 모델 변환 시 결과물 의 변경이 없는지 확인하기 위한 예시 입력 및 출력 데이터도 함께 저장부에 업로드할 수 있다. 도 5는 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법에서 모델 최 적화 및 모델 배포 동작을 나타낸 순서도이다. 도 4에 도시된 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법에서 모델 최적화 동작은 퍼블릭 클 라우드 시스템에 구축된 변환부에 의해 수행될 수 있다. 다만, 변환부는 퍼블릭 클라우드 시스 템에 구축되는 것으로 한정되지 않는다. 단계 S301에서, 인공지능 모델 배포 자동화 시스템의 변환부는 새로운 인공지능 모델이 저장부 에 업로드되면, 변환부가 업로드된 인공지능 모델을 인식하여 인공지능 모델의 변환 과정을 시작한다. 이 때, 변환부는 이벤트 리스너(Event Listener) 기능을 통해 업로드된 인공지능 모델을 인식할 수 있다. 단계 S302 및 S303에서, 변환부는 저장부에 저장된 인공지능 모델을 변환부로 이동시킨 후, 중 간 표현 형식으로 인공지능 모델을 변환한다. 여기서, 중간 표현 형식(Intermediate Representation, IR)이란 특정 프레임워크로 작성된 인공지능 모델을 다양한 프레임워크, 런타임 엔진, 또는 컴파일러 등에서도 사용될 수 있도록 변환한 공통 규격 파일(Common file format)을 의미한다. 예를 들면, 대표적인 중간 표현 형식(IR)은 ONNX(Open Neural Network Exchange), 또는 MLIR(Multi-Level Intermediate Representation) 등이 될 수 있다. 단계 S304에서, 변환부는 중간 표현 형식으로 변환된 인공지능 모델을 이용해 다양한 런타임 엔진, 컴파일 러와 양자화(Quantization), 가지치기(Pruning) 등의 모델 압축 기법을 이용하여 주어진 배포 조건을 만족할 수 있도록 인공지능 모델을 변환한다. 단계 S305에서, 변환부는 변환된 인공지능 모델이 배포 조건을 만족하는지를 확인한다. 여기서, 인공지능 모델이 배포 조건을 만족하는지를 확인하는 동작은 배포 조건에 따라 배포 조건의 만족 여부를 판단하는 방법이 달라질 수 있다. 예를 들어, 실행 시간과 관련된 배포 조건이 있다면 변환부에서 여러 번의 실행을 통해 해당 배포 조건이 만족하는지를 확인할 수 있다. 또 다른 예시로, 모델 파일의 크기와 관련된 배포 조건이 있다 면 변환부에서 모델 크기 확인을 통해 해당 조건이 만족되었는지 확인할 수 있다. 단계 S306에서, 변환부는 변환된 인공지능 모델이 배포 조건을 만족하는 경우, 즉, 변환된 인공지능 모델 의 결과물이 앞서 정의된 배포 조건에 만족을 할 경우 인공지능 모델을 저장부로 이동한다. 만약, 변환부 는 변환된 인공지능 모델이 배포 조건을 만족하지 않은 경우, 런타임 엔진 및 모델 압축 기법을 실험하는 단계 S304를 다시 수행한다. 한편, 도 5에 도시된 모델 최적화 동작은 저장부에 인공지능 모델이 들어오게 되면 자동으로 시작될 수 있 다. 모델 최적화를 위한 각 과정은 소프트웨어 파이프라인으로 관리되어 한 단계가 끝나면 자동으로 실행될 수 있다. 세부 과정을 예시로 들어 살펴보면 다음과 같다. 변환부는 인공지능 모델 파일과 모델 코드가 저장부에 업로드 되면, 트리거(Event Listener)가 작동 하여 우선적으로 중간 표현 형식 중 ONNX 형식으로 모델을 변환한다. 그리고 변환부는 ONNX 모델을 구동시키기 위한 런타임(Runtime)에서 변환된 모델 형식이 배포 조건(예컨대, 응답 지연시간 또는 모델 크기)에 부합하는지 체크한다. ONNX 형식으로 변환된 인공지능 모델이 배포 조건에 부합하는 경우 모델 최적화 동작을 완료할 수 있다. 만약, 변환부는 ONNX 형식으로 변환된 인공지능 모델이 배포 조건에 부합하지 않는 경우 해당 인공지능 모 델을 구동하는 런타임 최적화를 진행한다. 변환부는 중간 표현 형식인 ONNX 형식의 인공지능 모델을 하드 웨어에 최적화할 수 있는 런타임(예컨대, OpenVINO 인텔 CPU, TensorRT Nvidia 등)과 같은 모델 형식으로 추가 변환한 후 배포 조건의 부합 여부를 확인한다. 변환부는 추가된 변환된 인공지능 모델이 배포 조건에 부합 하는 경우 모델 최적화를 종료할 수 있다. 예를 들어, 변환부는 파이토치(PyTorch)로 작성된 인공지능 모델을 ONNX로 변환한 후 ONNX 런타임 및 ONNX 에서 제공하는 다양한 모델 압축 기법을 활용해 볼 수 있다. 변환부는 ONNX 내에서 제공하는 기능으로 배 포 조건(예컨대, 응답 지연시간 또는 메모리 사용량)을 만족할 수 없는 경우 ONNX로 변환된 인공지능 모델을 TFLite(Tensorflow Lite), TensorRT, OpenVINO 등으로 추가 변환하여 동일한 과정을 다시 수행할 수 있다. 여기서, 배포 조건에 중간 표현 형식 혹은 모델 압축 기법의 적용 순서가 바뀔 수 있다. 예를 들어, 변환부 는 엔비디아(Nvidia) GPU를 사용한다는 조건이 존재할 경우 ONNX 형식의 인공지능 모델로 변환한 후, 곧바 로 TensorRT 형식의 인공지능 모델로 추가 변환할 수 있다. 또는 변환부는 인텔(Intel) CPU를 사용할 경우 ONNX 형식의 인공지능 모델로 변환한 후 OpenVINO 형식의 인공지능 모델로 추가 변환할 수 있다. 한편, 변환부는 런타임 최적화 후에도 변환된 인공지능 모델이 배포 조건이 부합하지 않는 경우 추가로 양 자화(Quantization) 또는 가지치기(Pruning) 기법을 인공지능 모델에 적용하여 소프트웨어적으로 모델을 최적화 를 진행할 수 있다. 여기서, 변환부는 하드웨어 최적화 동작을 먼저 수행할 수 있다. 하드웨어 최적화를 먼저 수행하는 것은 인공지능 모델의 구조를 최대한 변경하지 않는 상태로 최적화를 진행할 수 있기 때문이다. 따라서, 변환부는 변환된 인공지능 모델에 대한 런타임 최적화를 진행한 후 소프트웨어적 기법을 적용할 수 있다. 변환부는 각종 기법을 적용한 후에도 배포 조건을 부합하지 않는 경우, 배포 조건을 다시 리뷰 하거나 문 제 정의를 다시 시작할 수 있다. 한편, 인공지능 모델의 변환 전과 후의 결과물은 어느 정도의 오차가 있는 것이 일반적이다. 따라서, 변환부 는 오차의 정도를 파악하기 위하여 모델 변환 전/후의 결과물로 나온 상수 혹은 벡터의 모든 요소들이 사 전에 지정된 상대적 오차범위(Relative Tolerance) 혹은 절대적 오차범위(Absolute Tolerance) 내에 있는지 확 인하는 방식으로 해당 모델 변환 동작의 안정성을 확인할 수 있다. 만약, 지정된 오차범위를 벗어날 경우 후속 보완 절차는 어느 단계에서 그러한 일이 일어났는지에 따라 다를 수 있다. 예를 들어, 인공지능 모델을 중간 표현 형식으로 변경하는 단계에서 해당 문제가 발생한 경우, 변환부 는 중간 표현 형식으로 변경하지 않아도 모든 조건이 만족한다면 원본의 인공지능 모델을 그대로 사용할 수 있다. 또는, 변환부는 배포 조건이 만족하지 않은 경우, 중간 표현 형식으로 바꾸되 사용자가 이를 충 분히 인지할 수 있도록 별도의 메시지를 제공할 수 있다. 이때, 변환부는 양자화 등과 같은 모델 압축 기 법 사용 시 이러한 문제가 발생한 경우 해당 기법을 사용하지 않을 수 있다. 이후, 단계 S307에서, 추론부는 배포 조건을 만족하는 인공지능 모델이 저장부로 이동된 후, 이동된 인공지능 모델을 배포한다. 도 5에 도시된 모델 배포 동작은 퍼블릭 클라우드 시스템에 구축된 추론부 에 의해 수행될 수 있다. 다만, 추론부는 퍼블릭 클라우드 시스템에 구축되는 것으로 한정되지 않는다. 여기서, 추론부는 사전에 정의된 배포 조건을 통과한 인공지능 모델들 중에서 배포될 시스템의 특성에 맞 는 인공지능 모델들을 모델 배포 관리자가 선택하면 선택된 인공지능 모델을 배포할 수 있다. 예시로, 추론부는 고성능의 퍼블릭 클라우드가 구축되어 있어 응답 지연 시간을 최소화해야 하는 상황에서 는 인공지능 모델의 응답 지연 시간이 최소화된 런타임 엔진과 상응하는 인공지능 모델로 배포할 수 있다. 다른 예시로, 추론부는 응답 지연 속도 보단 최대한 메모리를 적게 사용해야 되는 상황에는 인공지능 모델 의 용량이 최소화된 런타임 엔진과 상응하는 인공지능 모델로 배포할 수 있다. 이때, 추론부는 컨테이너 기반의 가상화 기술로 각종 런타임 엔진 관리를 통해 새로운 인공지능 모델들이 배포되어도 균일된 인터페이스 규칙 제공을 통해 다른 시스템과의 연동을 용이하게 할 수 있다. 인공지능 모델 을 만들기 위해선 다양한 라이브러리(Library)들이 존재한다. 모델 배포시 각 라이브러리에 특성에 맞도록 배포 방법을 개발하고 인터페이스 규칙을 정립해야 한다. 배포 방법 및 인터페이스 규칙을 통일하여 불편함을 최소화 하기 위해서, 추론부는 모델 최적화 과정에서 다양한 라이브러리로 만들어진 인공지능 모델들을 ONNX를 활 용해서 중간 표현 형식으로 변환하는 동작을 수행한다. 즉, 어떤 라이브러리를 활용해서 인공지능 모델이 학습 되었든, 최종적으로는 ONNX 기반의 모델 형식으로 변환되기 때문에 균일한 인터페이스 규칙을 제공할 수 있다. 또한, 추론부는 ONNX 형식 기반의 인공지능 모델을 제공하는 서버 환경을 컨테이너 기반의 가상화 기술로 보편적인 환경을 구축해두어 새로운 인공지능 모델을 배포해야 할 시 인공지능 모델 파일들만 변경하면 되도록 시스템을 구성할 수 있다.이하, 사전 학습된 BERT(Bidirectional Encoder Representations from Transformers) 인공지능 모델을 신문 요 약 데이터를 통해 파인 튜닝하여 모델 배포 자동화 동작을 모델 학습, 최적화 및 배포 동작에 대한 예시로 설명 하기로 한다. 학습부는 우선 모델 학습 동작을 수행할 수 있다. 학습부는 추론 시 GPU 사용하지 않음, 모델 크기 상한 500MB, 및 추론 시간 2초 제한과 같은 배포 조건을 사전에 정의한다."}
{"patent_id": "10-2023-0059024", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "그리고 학습부는 학습 데이터를 저장부에 업로드하기 위해, 신문 요약 데이터를 퍼블릭 클라우드 상 에 구축된 저장부에 업로드한다. 학습부는 모델 코드 작성과 관련하여, 사전 학습된 BERT 인공지능 모델을 다운로드하여 파인 튜닝(Fine- tuning)하고 파인 튜닝된 BERT 인공지능 모델을 퍼블릭 클라우드 시스템에 구축된 저장부에 업로드하 는 코드를 작성한다. 다음으로, 변환부는 모델 최적화 동작을 수행할 수 있다. 변환부는 BERT 인공지능 모델이 업로드되는 저장부의 위치를 바라보는 이벤트 리스너(Event Listener)가 새로운 모델 업로드를 인식하면, 변환부(13 0)는 자동으로 퍼블릭 클라우드 상에서 버추얼 프라이빗 클라우드(Virtual Private Cloud, VPC)에 사전에 모델 변환 과정이 들어가 있는 가상화된 컨테이너를 새로 띄우고 해당 BERT 인공지능 모델을 컨테이너 내부에 다운로 드한다. 이어서, 변환부는 BERT 인공지능 모델을 ONNX를 활용하여 중간 표현 형식으로 변환한다. 변환부는 ONNX 런타임(Runtime)을 이용하여 미리 정의된 배포 조건이 만족하는지를 확인한다. 변환부는 미리 정의된 배포 조건이 만족하는 경우 인공지능 모델을 저장부로 이동시킨다. 반면, 변환부는 미리 정의된 배포 조건이 만족하지 않은 경우 인공지능 모델을 OpenVINO 형식으로 추가 변환할 수 있다. 변환부는 미리 정의 된 배포 조건이 만족할 때까지 모델 변환 동작, 런타임 최적화 동작 또는 모델 압축 동작 등을 반복적으로 수행 할 수 있다. 이후, 추론부는 모델 배포 동작을 수행할 수 있다. 추론부는 모델 최적화 동작이 종료됐다는 이벤트 (Event)와 함께 변환된 인공지능 모델이 저장된 경로를 데이터베이스(DB)에 업데이트하면 모델 배포와 관련된 이벤트 리스너(Event Listener)가 이를 인식한다. 그리고 추론부는 사전에 작성된 쿠버네티스(Kubernetes) 명세를 이용해 새로운 인공지능 모델을 배포할 수 있다. 한편, 프로세서에 의해 실행될 때, 상기 프로세서로 하여금 방법을 실행하게 하는 명령어들을 저장하기 위한 비 일시적 컴퓨터 판독가능 저장 매체로서, 상기 방법은: 학습 데이터를 이용하여 인공지능 모델을 학습하는 단계; 상기 학습된 인공지능 모델을 중간 표현 형식(Intermediate Representation, IR)으로 변환하고 미리 정해진 배 포 조건을 만족하도록 상기 변환된 인공지능 모델을 최적화하는 단계; 및 상기 최적화된 인공지능 모델이 미리 정해진 배포 조건을 만족하면 상기 최적화된 인공지능 모델을 배포하는 단계를 포함하는, 비일시적 컴퓨터 판독 가능한 저장 매체가 제공될 수 있다. 한편, 본 발명의 일 실시예에 따르면, 이상에서 설명된 다양한 실시예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media)에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개 시된 실시예들에 따른 전자 장치(예: 전자 장치(A))를 포함할 수 있다. 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 프로세서의 제어 하에 다른 구성요소들을 이용하여 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있 는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장매체 가 신호(signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임시적으로 저장됨을 구분하지 않는다. 또한, 본 발명의 일 실시예에 따르면, 이상에서 설명된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품 (computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있 다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 또한, 본 발명의 일 실시예에 따르면, 이상에서 설명된 다양한 실시예들은 소프트웨어(software), 하드웨어 (hardware) 또는 이들의 조합을 이용하여 컴퓨터(computer) 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 일부 경우에 있어 본 명세서에서 설명되는 실시예들이 프로세서 자체로 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시예들은 별도의 소프트 웨어 모듈들로 구현될 수 있다. 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 동작을 수행할 수 있다. 한편, 상술한 다양한 실시예들에 따른 기기의 프로세싱 동작을 수행하기 위한 컴퓨터 명령어(computer instructions)는 비일시적 컴퓨터 판독 가능 매체(non-transitory computer-readable medium)에 저장될 수 있 다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령어는 특정 기기의 프로세서에 의해 실행되었 을 때 상술한 다양한 실시예에 따른 기기에서의 처리 동작을 특정 기기가 수행하도록 한다. 비일시적 컴퓨터 판 독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체가 아니라 반영구적 으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 비일시적 컴퓨터 판독 가능 매 체의 구체적인 예로는, CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등이 있을 수 있다. 또한, 상술한 다양한 실시예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구 성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요 소가 다양한 실시예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로 그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작 들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생 략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 발명의 바람직한 실시예에 대하여 도시하고 설명하였지만, 본 발명은 상술한 특정의 실시예에 한"}
{"patent_id": "10-2023-0059024", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통 상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2023-0059024", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 시스템의 구성도이 다. 도 2는 본 발명의 일 실시예에 따른 인공지능 모델 배포 자동화 시스템이 하이브리드 아키텍처 기반으로 구축된 예시를 나타난 도면이다. 도 3은 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법을 나타낸 흐 름도이다. 도 4는 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법에서 모델 학 습 동작을 나타낸 순서도이다. 도 5는 본 발명의 일 실시예에 따른 하이브리드 아키텍처 기반의 인공지능 모델 배포 자동화 방법에서 모델 최 적화 및 모델 배포 동작을 나타낸 순서도이다."}
