{"patent_id": "10-2022-0114224", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0040278", "출원번호": "10-2022-0114224", "발명의 명칭": "분산형 기계 학습 모델의 트레이닝 방법, 장치, 설비 및 매체", "출원인": "베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니", "발명자": "징, 보"}}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "분산형 기계 학습 모델의 트레이닝 방법에 있어서, 상기 방법은 모델의 트레이닝에 참여하는 여러 참여자 중 어느 하나에 의해 실행되고, 제1 참여자로 표시하며,기타 참여자는 제2 참여자로 표시하되, 상기 방법은:상기 제1 참여자의 제1 원본 샘플 데이터를 표준화 처리하여, 샘플 특징 차원의 제1 표준화 값 및 제1 샘플 데이터를 획득하는 단계-표준화 처리된 제1 샘플 데이터는 모델을 트레이닝하는데 사용됨-;제3 암호키로 상기 제1 표준화 값을 암호화하여, 제1 암호화된 표준화 값을 형성하고, 상기 제1 암호화된 표준화 값을 신뢰하는 제3자에게 제공하는 단계;제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하는 단계-여기서, 상기 제1 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 사용하여 제1 트레이닝 중간 파라미터를 암호화한 데이터이고, 상기 제1 트레이닝 중간 파라미터는 제1 참여자가 제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이고, 상기 제2 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를사용하여 제2 트레이닝 중간 파라미터를 암호화한 데이터이며, 상기 제2 트레이닝 중간 파라미터는 제2 참여자가 제2 샘플 데이터를 기반으로 제2 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이고, 상기 기계 학습모델은 적어도 2개의 참여자 각각의 로컬 서브 모델로 구성됨-;제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여 제1 암호화 간섭 파라미터를 형성하는 단계;상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터, 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성하는 단계;상기 제2 참여자가 제2 동형 개인키를 사용하여 상기 제1 암호화 키 파라미터를 복호화할 수 있도록 상기 제1암호화 키 파라미터를 제2 참여자에게 전송하는 단계;상기 제2 참여자의 복호화된 제1 키 파라미터를 획득하는 단계; 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상기 제1 서브 모델의 트레이닝이 완료될 때까지 상기 제1 서브 모델을 반복적으로 업데이트하는 단계; 상기 제1 서브 모델을 제3 암호키로 암호화하여, 제1 암호화 서브 모델을 형성하고, 상기 제1 암호화 서브 모델을 상기 신뢰하는 제3자에게 제공하여, 상기 신뢰하는 제3자가 각 참여자가 제공한 암호화 서브 모델을 기반으로 결합하고, 각 참여자가 제공한 암호화된 표준화 값을 기반으로 비표준화 처리를 한 다음 분할하는 단계;상기 신뢰하는 제3자가 리턴한 분할된 제1 비표준화 서브 모델을 수신하는 단계; 를 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터, 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성하는 단계는:상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하고, 상기 제1 암호화 간섭 파라미터를 선형 계산에 기반하여 계산 결과에 추가하여(superposition) 제1 암호화 키 파라미터를 생성하는 단계; 를 포함하고,상응하게, 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상기 제1 서브 모델의 트레이닝이 완료될때까지 상기 제1 서브 모델을 반복적으로 업데이트하는 단계는:공개특허 10-2023-0040278-3-상기 제1 간섭 파라미터를 기반으로 상기 제1 키 파라미터에 대해 역으로 선형 계산을 하고, 상기 제1 서브 모델의 트레이닝이 완료될 때까지, 역으로 선형 계산한 후의 키 파라미터에 따라 상기 제1 서브 모델을 반복적으로 업데이트하는 단계; 를 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항 내지 제 2 항 중 어느 한 항에 있어서, 상기 간섭 파라미터는 난수인 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항 내지 제 2 항 중 어느 한 항에 있어서, 상기 제1 간섭 파라미터는 제1 구배 간섭 파라미터와 제1 손실 간섭 파라미터를 포함하고;상응하게, 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터,및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성하는 단계는:상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 구배 간섭 파라미터 및 제1서브 모델의 동형 구배 계산 함수를 기반으로 계산하여 제1 암호화 구배 파라미터를 생성하는 단계;상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 손실 간섭 파라미터 및 제1서브 모델의 동형 손실 계산 함수를 기반으로 계산하여 제1 암호화 손실 파라미터를 생성하는 단계; 를 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서, 제1 동형 키 쌍을 생성하는 단계-상기 동형 키 쌍은 제1 동형 공개키 및 제1 동형 개인키를 포함함-; 상기 제1 동형 공개키를 제2 참여자에게 송신하는 단계;상기 제2 참여자가 생성하고 상기 제2 참여자가 송신한 제2 동형 키 쌍의 제2 동형 공개키를 수신하는 단계; 를더 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1 항에 있어서, 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하기 전에, 상기 제1 참여자와 상기 제2 참여자의 샘플 데이터의 교집합 식별자를 확정하는 단계를 더 포함하고, 적어도 하나의 참여자의 샘플 데이터는 대응하게 태그가 표기되는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝방법."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항에 있어서, 제1 동형 암호화 중간 파라미터를 획득하는 단계는,제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후, 제1 트레이닝 중간 파라미터를 생성하는 단계;설정된 정밀도에 따라 상기 제1 트레이닝 중간 파라미터의 부동 소수점 수(floating-point number)를 제1 큰 정수로 변환하는 단계;상기 제1 큰 정수의 정수 값을 인코딩하여 제1 양의 정수로 변환하는 단계;제2 참여자의 제2 동형 공개키를 사용하여 제1 양의 정수를 암호화하여 상기 제1 동형 암호화 중간 파라미터를얻는 단계; 를 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "공개특허 10-2023-0040278-4-제 7 항에 있어서, 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상기 제1 서브 모델을 반복적으로 업데이트하는 단계는:상기 제1 간섭 파라미터를 기반으로 상기 제1 키 파라미터에 대해 간섭 제거 동작을 수행하는 단계;간섭 제거된 후의 제1 키 파라미터를 디코딩하고, 상기 설정된 정밀도에 따라 스케일링하여 부동 소수점 수로변환하는 단계;변환된 부동 소수점 수 형태의 제1 키 파라미터를 사용하여 상기 제1 서브 모델을 반복적으로 업데이트하는 단계; 를 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "분산형 기계 학습 모델의 트레이닝 장치에 있어서,제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하는데 사용되는 중간 파라미터 획득모듈-여기서, 상기 제1 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 이용하여 제1 트레이닝중간 파라미터를 암호화한 데이터이고, 상기 제1 트레이닝 중간 파라미터는 제1 참여자가 제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이고, 상기 제2 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 사용하여 제2 트레이닝 중간 파라미터를 암호화한 데이터이며, 상기 제2 트레이닝 중간 파라미터는 제2 참여자가 제2 샘플 데이터를 기반으로 제2 서브 모델을 트레이닝한 후 생성되는 중간파라미터이고, 상기 기계 학습 모델은 적어도 2개의 참여자 각각의 로컬 서브 모델로 구성됨-;제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여, 제1 암호화 간섭 파라미터를 형성하도록 구성된 간섭 파라미터 형성 모듈;상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 간섭 파라미터, 및 제1 서브모델의 동형 계산 함수를 기반으로 계산하여, 제1 암호화 키 파라미터를 생성하도록 구성된 파라미터 생성모듈;상기 제2 참여자가 제2 동형 개인키를 사용하여 상기 제1 암호화 키 파라미터를 복호화할 수 있도록, 상기 제1암호화 키 파라미터를 제2 참여자에게 전송하도록 구성된 파라미터 복호화 모듈;상기 제2 참여자의 복호화된 제1 키 파라미터를 획득하도록 구성된 제1 키 파라미터 획득 모듈;상기 제1 서브 모델의 트레이닝이 완료될 때까지, 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상기 제1 서브 모델을 반복적으로 업데이트하도록 구성된 서브 모델 트레이닝 모듈; 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하기 전에, 상기 제1 참여자의 제1 원본 샘플 데이터를 표준화 처리하여, 샘플 특징 차원의 제1 표준화 값 및 제1 샘플 데이터를 획득하는데 사용되는 샘플 데이터 처리 모듈-표준화 처리된 제1 샘플 데이터는 모델을 트레이닝하는데 사용됨-;제3 암호키로 상기 제1 표준화 값을 암호화하여, 제1 암호화된 표준화 값을 형성하고, 상기 제1 암호화된 표준화 값을 신뢰하는 제3자에게 제공하는데 사용되는 제1 표준화 값 암호화 모듈;상기 제1 서브 모델의 트레이닝이 완료된 후, 상기 제1 서브 모델을 제3 암호키로 암호화하여, 제1 암호화 서브모델을 형성하고, 상기 제1 암호화 서브 모델을 상기 신뢰하는 제3자에게 제공하여, 상기 신뢰하는 제3자가 각참여자가 제공한 암호화 서브 모델을 기반으로 결합하고, 각 참여자가 제공한 암호화된 표준화 값을 기반으로비표준화 처리를 한 다음 분할하는데 사용되는 모델 분할 모듈;상기 신뢰하는 제3자가 리턴한 분할된 제1 비표준화 서브 모델을 수신하는데 사용되는 서브 모델 수신 모듈; 을포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서, 상기 파라미터 생성 모듈은,상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 및 제1 서브 모델의 동형 계산 함수를공개특허 10-2023-0040278-5-기반으로 계산하고, 상기 제1 암호화 간섭 파라미터를 선형 계산에 기반하여 계산 결과에 추가하여, 제1 암호화키 파라미터를 생성하는데 사용되는 파라미터 생성 유닛; 을 포함하고,상응하게, 상기 서브 모델 트레이닝 모듈은,상기 제1 간섭 파라미터를 기반으로 상기 제1 키 파라미터에 대해 역으로 선형 계산을 하고, 상기 제1 서브 모델의 트레이닝이 완료될 때까지, 역으로 선형 계산 후의 키 파라미터에 따라 상기 제1 서브 모델을 반복적으로업데이트하는 서브 모델 트레이닝 유닛; 을 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 9 항 내지 제 10 항 중 어느 한 항에 있어서, 상기 간섭 파라미터는 난수인 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 9 항 내지 제 10 항 중 어느 한 항에 있어서, 상기 제1 간섭 파라미터는 제1 구배 간섭 파라미터와 제1 손실 간섭 파라미터를 포함하고;상응하게, 상기 파라미터 생성 모듈은:상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 구배 간섭 파라미터 및 제1서브 모델의 동형 구배 계산 함수를 기반으로 계산하여 제1 암호화 구배 파라미터를 생성하는데 사용되는 구배파라미터 생성 모듈;상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 손실 간섭 파라미터 및 제1서브 모델의 동형 손실 계산 함수를 기반으로 계산하여 제1 암호화 손실 파라미터를 생성하는데 사용되는 손실파라미터 생성 모듈; 을 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 9 항에 있어서, 제1 동형 키 쌍을 생성하도록 구성된 제1 동형 키 쌍 생성 모듈-상기 제1 동형 키 쌍은 제1 동형 공개키 및 제1동형 개인키를 포함함-;상기 제1 동형 공개키를 제2 참여자에게 송신하도록 구성되는 제1 동형 공개키 송신 모듈;상기 제2 참여자가 생성하고 상기 제2 참여자가 송신한 제2 동형 키 쌍의 제2 동형 공개키를 수신하도록 구성되는 제2 동형 공개키 수신 모듈; 을 더 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 9 항에 있어서, 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하기 전에, 상기 제1 참여자와 상기제2 참여자의 샘플 데이터의 교집합 식별자를 확정하는데 사용되는 교집합 식별자 확정 모듈; 을 더 포함하고,적어도 하나의 참여자의 샘플 데이터는 대응하게 태그가 표기되는 것을 특징으로 하는 분산형 기계 학습 모델의트레이닝 장치."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 9 항에 있어서, 상기 중간 파라미터 획득 모듈은,제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후, 제1 트레이닝 중간 파라미터를 생성하는데 사용되는 제1 트레이닝 중간 파라미터 생성 유닛;설정된 정밀도에 따라 상기 제1 트레이닝 중간 파라미터의 부동 소수점 수를 제1 큰 정수로 변환하는데 사용되공개특허 10-2023-0040278-6-는 제1 큰 정수 확정 유닛;상기 제1 큰 정수의 정수 값을 인코딩하여 제1 양의 정수로 변환하는데 사용되는 제1 양의 정수 확정 유닛;제2 참여자의 제2 동형 공개키를 사용하여 제1 양의 정수를 암호화하여 상기 제1 동형 암호화 중간 파라미터를얻는데 사용되는 제1 동형 암호화 중간 파라미터 확정 유닛; 을 포함하는 것을 특징으로 하는 분산형 기계 학습모델의 트레이닝 장치."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15 항에 있어서, 상기 서브 모델 트레이닝 모듈은:상기 제1 간섭 파라미터를 기반으로 상기 제1 키 파라미터에 대해 간섭 제거 동작을 수행하는데 사용되는 간섭제거 동작 유닛;간섭 제거된 후의 제1 키 파라미터를 디코딩하고, 상기 설정된 정밀도에 따라 스케일링하여, 부동 소수점 수로변환하는데 사용되는 부동 소수점 수 변환 유닛;변환된 부동 소수점 수 형태의 제1 키 파라미터를 사용하여 상기 제1 서브 모델을 반복적으로 업데이트하는데사용되는 반복 업데이트 유닛; 을 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신적으로 연결되는 메모리; 를 포함하되, 여기서,상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되어 있고, 상기 명령이 상기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서가 제 1 항 내지 제8 항 중 어느 한 항에따른 분산형 기계 학습 모델의 트레이닝 방법을 실행하는 것을 특징으로 하는 전자 설비."}
{"patent_id": "10-2022-0114224", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "컴퓨터 명령이 저장된 비일시적 컴퓨터로 판독 가능한 저장 매체에 있어서,상기 컴퓨터 명령은 상기 컴퓨터가 제 1 항 내지 제 8 항 중 어느 한 항에 따른 분산형 기계 학습 모델의 트레이닝 방법을 실행하는데 사용되는 것을 특징으로 하는 저장 매체."}
{"patent_id": "10-2022-0114224", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 분산형 기계 학습 모델의 트레이닝 방법, 장치, 설비 및 매체를 제공하고, 인공지능"}
{"patent_id": "10-2022-0114224", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한"}
{"patent_id": "10-2022-0114224", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "것으로서, 특히 딥러닝 기술분야에 관한 것이다. 구체적인 구현 방법은, 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하는 단계; 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 (뒷면에 계속) 대 표 도 - 도1"}
{"patent_id": "10-2022-0114224", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 3, "content": "공개특허10-2023-0040278 암호화하여 제1 암호화 간섭 파라미터를 형성하는 단계; 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터, 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성하는 단계; 제2 참여자가 복호화할 수 있도록 제1 암호화 키 파라미터를 제2 참여자에게 전송하 는 단계; 제2 참여자의 복호화된 제1 키 파라미터를 획득하는 단계; 제1 키 파라미터 및 제1 간섭 파라미터에 따 라, 제1 서브 모델의 트레이닝이 완료될 때까지 제1 서브 모델을 반복적으로 업데이트하는 단계; 를 포함한다. 본 개시는 트레이닝 프로세스의 효율성을 향상시킬 수 있다. CPC특허분류 H04L 9/083 (2013.01)명 세 서 청구범위 청구항 1 분산형 기계 학습 모델의 트레이닝 방법에 있어서, 상기 방법은 모델의 트레이닝에 참여하는 여러 참여자 중 어느 하나에 의해 실행되고, 제1 참여자로 표시하며, 기타 참여자는 제2 참여자로 표시하되, 상기 방법은: 상기 제1 참여자의 제1 원본 샘플 데이터를 표준화 처리하여, 샘플 특징 차원의 제1 표준화 값 및 제1 샘플 데 이터를 획득하는 단계-표준화 처리된 제1 샘플 데이터는 모델을 트레이닝하는데 사용됨-; 제3 암호키로 상기 제1 표준화 값을 암호화하여, 제1 암호화된 표준화 값을 형성하고, 상기 제1 암호화된 표준 화 값을 신뢰하는 제3자에게 제공하는 단계; 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하는 단계-여기서, 상기 제1 동형 암 호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 사용하여 제1 트레이닝 중간 파라미터를 암호화한 데이 터이고, 상기 제1 트레이닝 중간 파라미터는 제1 참여자가 제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이 닝한 후 생성되는 중간 파라미터이고, 상기 제2 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 사용하여 제2 트레이닝 중간 파라미터를 암호화한 데이터이며, 상기 제2 트레이닝 중간 파라미터는 제2 참여자 가 제2 샘플 데이터를 기반으로 제2 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이고, 상기 기계 학습 모델은 적어도 2개의 참여자 각각의 로컬 서브 모델로 구성됨-; 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여 제1 암호화 간섭 파라미터를 형성 하는 단계; 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터, 및 제1 서 브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성하는 단계; 상기 제2 참여자가 제2 동형 개인키를 사용하여 상기 제1 암호화 키 파라미터를 복호화할 수 있도록 상기 제1 암호화 키 파라미터를 제2 참여자에게 전송하는 단계; 상기 제2 참여자의 복호화된 제1 키 파라미터를 획득하는 단계; 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상기 제1 서브 모델의 트레이닝이 완료될 때까지 상 기 제1 서브 모델을 반복적으로 업데이트하는 단계; 상기 제1 서브 모델을 제3 암호키로 암호화하여, 제1 암호화 서브 모델을 형성하고, 상기 제1 암호화 서브 모델 을 상기 신뢰하는 제3자에게 제공하여, 상기 신뢰하는 제3자가 각 참여자가 제공한 암호화 서브 모델을 기반으 로 결합하고, 각 참여자가 제공한 암호화된 표준화 값을 기반으로 비표준화 처리를 한 다음 분할하는 단계; 상기 신뢰하는 제3자가 리턴한 분할된 제1 비표준화 서브 모델을 수신하는 단계; 를 포함하는 것을 특징으로 하 는 분산형 기계 학습 모델의 트레이닝 방법. 청구항 2 제 1 항에 있어서, 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터, 및 제1 서 브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성하는 단계는: 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터 및 제1 서브 모델의 동형 계산 함수를 기 반으로 계산하고, 상기 제1 암호화 간섭 파라미터를 선형 계산에 기반하여 계산 결과에 추가하여 (superposition) 제1 암호화 키 파라미터를 생성하는 단계; 를 포함하고, 상응하게, 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상기 제1 서브 모델의 트레이닝이 완료될 때까지 상기 제1 서브 모델을 반복적으로 업데이트하는 단계는:상기 제1 간섭 파라미터를 기반으로 상기 제1 키 파라미터에 대해 역으로 선형 계산을 하고, 상기 제1 서브 모 델의 트레이닝이 완료될 때까지, 역으로 선형 계산한 후의 키 파라미터에 따라 상기 제1 서브 모델을 반복적으 로 업데이트하는 단계; 를 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법. 청구항 3 제 1 항 내지 제 2 항 중 어느 한 항에 있어서, 상기 간섭 파라미터는 난수인 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법. 청구항 4 제 1 항 내지 제 2 항 중 어느 한 항에 있어서, 상기 제1 간섭 파라미터는 제1 구배 간섭 파라미터와 제1 손실 간섭 파라미터를 포함하고; 상응하게, 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터, 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성하는 단계는: 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 구배 간섭 파라미터 및 제1 서브 모델의 동형 구배 계산 함수를 기반으로 계산하여 제1 암호화 구배 파라미터를 생성하는 단계; 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 손실 간섭 파라미터 및 제1 서브 모델의 동형 손실 계산 함수를 기반으로 계산하여 제1 암호화 손실 파라미터를 생성하는 단계; 를 포함하 는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법. 청구항 5 제 1 항에 있어서, 제1 동형 키 쌍을 생성하는 단계-상기 동형 키 쌍은 제1 동형 공개키 및 제1 동형 개인키를 포함함-; 상기 제1 동형 공개키를 제2 참여자에게 송신하는 단계; 상기 제2 참여자가 생성하고 상기 제2 참여자가 송신한 제2 동형 키 쌍의 제2 동형 공개키를 수신하는 단계; 를 더 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법. 청구항 6 제 1 항에 있어서, 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하기 전에, 상기 제1 참여자와 상기 제2 참여자의 샘플 데이터의 교집합 식별자를 확정하는 단계를 더 포함하고, 적어도 하 나의 참여자의 샘플 데이터는 대응하게 태그가 표기되는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법. 청구항 7 제 1 항에 있어서, 제1 동형 암호화 중간 파라미터를 획득하는 단계는, 제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후, 제1 트레이닝 중간 파라미터를 생성하는 단계; 설정된 정밀도에 따라 상기 제1 트레이닝 중간 파라미터의 부동 소수점 수(floating-point number)를 제1 큰 정 수로 변환하는 단계; 상기 제1 큰 정수의 정수 값을 인코딩하여 제1 양의 정수로 변환하는 단계; 제2 참여자의 제2 동형 공개키를 사용하여 제1 양의 정수를 암호화하여 상기 제1 동형 암호화 중간 파라미터를 얻는 단계; 를 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법. 청구항 8 제 7 항에 있어서, 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상기 제1 서브 모델을 반복적으로 업데이트하는 단계 는: 상기 제1 간섭 파라미터를 기반으로 상기 제1 키 파라미터에 대해 간섭 제거 동작을 수행하는 단계; 간섭 제거된 후의 제1 키 파라미터를 디코딩하고, 상기 설정된 정밀도에 따라 스케일링하여 부동 소수점 수로 변환하는 단계; 변환된 부동 소수점 수 형태의 제1 키 파라미터를 사용하여 상기 제1 서브 모델을 반복적으로 업데이트하는 단 계; 를 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 방법. 청구항 9 분산형 기계 학습 모델의 트레이닝 장치에 있어서, 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하는데 사용되는 중간 파라미터 획득 모듈-여기서, 상기 제1 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 이용하여 제1 트레이닝 중간 파라미터를 암호화한 데이터이고, 상기 제1 트레이닝 중간 파라미터는 제1 참여자가 제1 샘플 데이터를 기 반으로 제1 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이고, 상기 제2 동형 암호화 중간 파라미터는 제 2 참여자의 제2 동형 공개키를 사용하여 제2 트레이닝 중간 파라미터를 암호화한 데이터이며, 상기 제2 트레이 닝 중간 파라미터는 제2 참여자가 제2 샘플 데이터를 기반으로 제2 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이고, 상기 기계 학습 모델은 적어도 2개의 참여자 각각의 로컬 서브 모델로 구성됨-; 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여, 제1 암호화 간섭 파라미터를 형성 하도록 구성된 간섭 파라미터 형성 모듈; 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 간섭 파라미터, 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여, 제1 암호화 키 파라미터를 생성하도록 구성된 파라미터 생성 모듈; 상기 제2 참여자가 제2 동형 개인키를 사용하여 상기 제1 암호화 키 파라미터를 복호화할 수 있도록, 상기 제1 암호화 키 파라미터를 제2 참여자에게 전송하도록 구성된 파라미터 복호화 모듈; 상기 제2 참여자의 복호화된 제1 키 파라미터를 획득하도록 구성된 제1 키 파라미터 획득 모듈; 상기 제1 서브 모델의 트레이닝이 완료될 때까지, 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상 기 제1 서브 모델을 반복적으로 업데이트하도록 구성된 서브 모델 트레이닝 모듈; 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하기 전에, 상기 제1 참여자의 제1 원 본 샘플 데이터를 표준화 처리하여, 샘플 특징 차원의 제1 표준화 값 및 제1 샘플 데이터를 획득하는데 사용되 는 샘플 데이터 처리 모듈-표준화 처리된 제1 샘플 데이터는 모델을 트레이닝하는데 사용됨-; 제3 암호키로 상기 제1 표준화 값을 암호화하여, 제1 암호화된 표준화 값을 형성하고, 상기 제1 암호화된 표준 화 값을 신뢰하는 제3자에게 제공하는데 사용되는 제1 표준화 값 암호화 모듈; 상기 제1 서브 모델의 트레이닝이 완료된 후, 상기 제1 서브 모델을 제3 암호키로 암호화하여, 제1 암호화 서브 모델을 형성하고, 상기 제1 암호화 서브 모델을 상기 신뢰하는 제3자에게 제공하여, 상기 신뢰하는 제3자가 각 참여자가 제공한 암호화 서브 모델을 기반으로 결합하고, 각 참여자가 제공한 암호화된 표준화 값을 기반으로 비표준화 처리를 한 다음 분할하는데 사용되는 모델 분할 모듈; 상기 신뢰하는 제3자가 리턴한 분할된 제1 비표준화 서브 모델을 수신하는데 사용되는 서브 모델 수신 모듈; 을 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치. 청구항 10 제 9 항에 있어서, 상기 파라미터 생성 모듈은, 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 및 제1 서브 모델의 동형 계산 함수를기반으로 계산하고, 상기 제1 암호화 간섭 파라미터를 선형 계산에 기반하여 계산 결과에 추가하여, 제1 암호화 키 파라미터를 생성하는데 사용되는 파라미터 생성 유닛; 을 포함하고, 상응하게, 상기 서브 모델 트레이닝 모듈은, 상기 제1 간섭 파라미터를 기반으로 상기 제1 키 파라미터에 대해 역으로 선형 계산을 하고, 상기 제1 서브 모 델의 트레이닝이 완료될 때까지, 역으로 선형 계산 후의 키 파라미터에 따라 상기 제1 서브 모델을 반복적으로 업데이트하는 서브 모델 트레이닝 유닛; 을 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장 치. 청구항 11 제 9 항 내지 제 10 항 중 어느 한 항에 있어서, 상기 간섭 파라미터는 난수인 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치. 청구항 12 제 9 항 내지 제 10 항 중 어느 한 항에 있어서, 상기 제1 간섭 파라미터는 제1 구배 간섭 파라미터와 제1 손실 간섭 파라미터를 포함하고; 상응하게, 상기 파라미터 생성 모듈은: 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 구배 간섭 파라미터 및 제1 서브 모델의 동형 구배 계산 함수를 기반으로 계산하여 제1 암호화 구배 파라미터를 생성하는데 사용되는 구배 파라미터 생성 모듈; 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 손실 간섭 파라미터 및 제1 서브 모델의 동형 손실 계산 함수를 기반으로 계산하여 제1 암호화 손실 파라미터를 생성하는데 사용되는 손실 파라미터 생성 모듈; 을 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치. 청구항 13 제 9 항에 있어서, 제1 동형 키 쌍을 생성하도록 구성된 제1 동형 키 쌍 생성 모듈-상기 제1 동형 키 쌍은 제1 동형 공개키 및 제1 동형 개인키를 포함함-; 상기 제1 동형 공개키를 제2 참여자에게 송신하도록 구성되는 제1 동형 공개키 송신 모듈; 상기 제2 참여자가 생성하고 상기 제2 참여자가 송신한 제2 동형 키 쌍의 제2 동형 공개키를 수신하도록 구성되 는 제2 동형 공개키 수신 모듈; 을 더 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치. 청구항 14 제 9 항에 있어서, 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하기 전에, 상기 제1 참여자와 상기 제2 참여자의 샘플 데이터의 교집합 식별자를 확정하는데 사용되는 교집합 식별자 확정 모듈; 을 더 포함하고, 적어도 하나의 참여자의 샘플 데이터는 대응하게 태그가 표기되는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치. 청구항 15 제 9 항에 있어서, 상기 중간 파라미터 획득 모듈은, 제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후, 제1 트레이닝 중간 파라미터를 생성하는데 사용되 는 제1 트레이닝 중간 파라미터 생성 유닛; 설정된 정밀도에 따라 상기 제1 트레이닝 중간 파라미터의 부동 소수점 수를 제1 큰 정수로 변환하는데 사용되는 제1 큰 정수 확정 유닛; 상기 제1 큰 정수의 정수 값을 인코딩하여 제1 양의 정수로 변환하는데 사용되는 제1 양의 정수 확정 유닛; 제2 참여자의 제2 동형 공개키를 사용하여 제1 양의 정수를 암호화하여 상기 제1 동형 암호화 중간 파라미터를 얻는데 사용되는 제1 동형 암호화 중간 파라미터 확정 유닛; 을 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치. 청구항 16 제 15 항에 있어서, 상기 서브 모델 트레이닝 모듈은: 상기 제1 간섭 파라미터를 기반으로 상기 제1 키 파라미터에 대해 간섭 제거 동작을 수행하는데 사용되는 간섭 제거 동작 유닛; 간섭 제거된 후의 제1 키 파라미터를 디코딩하고, 상기 설정된 정밀도에 따라 스케일링하여, 부동 소수점 수로 변환하는데 사용되는 부동 소수점 수 변환 유닛; 변환된 부동 소수점 수 형태의 제1 키 파라미터를 사용하여 상기 제1 서브 모델을 반복적으로 업데이트하는데 사용되는 반복 업데이트 유닛; 을 포함하는 것을 특징으로 하는 분산형 기계 학습 모델의 트레이닝 장치. 청구항 17 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신적으로 연결되는 메모리; 를 포함하되, 여기서, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되어 있고, 상기 명령이 상기 적 어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서가 제 1 항 내지 제8 항 중 어느 한 항에 따른 분산형 기계 학습 모델의 트레이닝 방법을 실행하는 것을 특징으로 하는 전자 설비. 청구항 18 컴퓨터 명령이 저장된 비일시적 컴퓨터로 판독 가능한 저장 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터가 제 1 항 내지 제 8 항 중 어느 한 항에 따른 분산형 기계 학습 모델의 트레 이닝 방법을 실행하는데 사용되는 것을 특징으로 하는 저장 매체. 발명의 설명 기 술 분 야"}
{"patent_id": "10-2022-0114224", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 4, "content": "본 개시는 인공지능 기술분야에 관한 것으로서, 구체적으로 딥러닝 기술분야에 관한 것이고, 특히 분산형 기계 학습 모델의 트레이닝 방법, 장치, 설비 및 매체에 관한 것이다."}
{"patent_id": "10-2022-0114224", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 지능 기술의 발전과 함께 기계 학습은 다양한 시나리오에서 점점 더 널리 사용되고 있다. 분산형 기계 학습이 보급됨에 따라, 프라이버시 보호가 문제로 되고 있다. 기계 학습 모델의 트레이닝의 경우, 보통 여러 참여자가 각각 제공한 데이터 샘플로 합동 트레이닝을 한다. 그러나 여러 참여자는 자신이 보유하고 있는 데이터 샘플에 대해 프라이버시 보호 수요가 있으며, 자신이 보유하고 있는 데이터 샘플이 다른 참여자에 게 공개되거나 획득되는 것을 원하지 않는다. 심지어 여러 참여자는 제각기 기계 학습 모델 중의 일부 모델을 트레이닝해야 할 수도 있으며, 모델의 해당 부분에 대한 트레이닝 프로세스 데이터도 다른 참여자에게 알려지기 를 원하지 않을 수 있다. 상기 수요에 대해, 기존의 기술은 일반적으로 신뢰할 수 있는 제3자를 사용하여 조정하고, 다른 참여자에게 노 출되는 것을 희망치 않는 프라이버시 데이터를 신뢰할 수 있는 제3자에 배치하여 처리한다. 기존의 기술에서는여전히 신뢰할 수 있는 제3자가 악의적으로 유출하는 가능성이 있으며, 또한 여러 참여자와 신뢰할 수 있는 제3 자 사이에 대량의 데이터가 인터랙션하므로 모델의 트레이닝 프로세스의 데이터 전송량이 크고, 효율이 낮다."}
{"patent_id": "10-2022-0114224", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 다자간 트레이닝 모델의 프라이버시 데이터 보호 및 모델의 트레이닝 효율성을 고려하여 분산형 기계 학습 모델의 트레이닝 방법, 장치, 설비 및 매체를 제공한다."}
{"patent_id": "10-2022-0114224", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면에 따르면, 분산형 기계 학습 모델의 트레이닝 방법을 제공하며, 해당 방법은: 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하는 단계; 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여 제1 암호화 간섭 파라미터를 형성 하는 단계; 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터, 및 제1 서 브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성하는 단계; 상기 제2 참여자가 제2 동형 개인키를 사용하여 상기 제1 암호화 키 파라미터를 복호화할 수 있도록 상기 제1 암호화 키 파라미터를 제2 참여자에게 전송하는 단계; 상기 제2 참여자의 복호화된 제1 키 파라미터를 획득하는 단계; 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상기 제1 서브 모델의 트레이닝이 완료될 때까지 상 기 제1 서브 모델을 반복적으로 업데이트하는 단계; 를 포함한다. 본 개시의 다른 측면에 따르면, 분산형 기계 학습 모델의 트레이닝 장치를 더 제공하며, 해당 장치는: 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하는데 사용되는 중간 파라미터 획득 모듈; 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여, 제1 암호화 간섭 파라미터를 형성 하도록 구성된 간섭 파라미터 형성 모듈; 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 간섭 파라미터, 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여, 제1 암호화 키 파라미터를 생성하도록 구성된 파라미터 생성 모듈; 상기 제2 참여자가 제2 동형 개인키를 사용하여 상기 제1 암호화 키 파라미터를 복호화할 수 있도록, 상기 제1 암호화 키 파라미터를 제2 참여자에게 전송하도록 구성된 파라미터 복호화 모듈; 상기 제2 참여자의 복호화된 제1 키 파라미터를 획득하도록 구성된 제1 키 파라미터 획득 모듈; 상기 제1 서브 모델의 트레이닝이 완료될 때까지, 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라 상 기 제1 서브 모델을 반복적으로 업데이트하도록 구성된 서브 모델의 트레이닝 모듈; 을 포함한다. 본 개시의 다른 측면에 따르면, 전자 설비를 더 제공하며, 해당 설비는: 적어도 하나의 프로세서; 및 적어도 하나의 프로세서와 통신적으로 연결되는 메모리; 를 포함하되, 여기서, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되어 있고, 상기 명령이 상기 적 어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서가 본 개시의 실시예에서 제공하는 임의의 분산형 기계 학습 모델의 트레이닝 방법을 실행한다. 본 개시의 다른 측면에 따르면, 컴퓨터 명령이 저장된 비일시적 컴퓨터로 판독 가능한 저장 매체를 더 제공하며, 상기 컴퓨터 명령은 컴퓨터가 본 개시의 실시예에서 제공하는 임의의 분산형 기계 학습 모델의 트레 이닝 방법을 실행하는데 사용된다.본 개시의 다른 측면에 따르면, 컴퓨터 프로그램 제품을 더 제공하고, 해당 제품은 컴퓨터 프로그램을 포함하며, 상기 컴퓨터 프로그램이 프로세서에 의해 실행되면 본 개시의 실시예에서 제공하는 임의의 분산형 기 계 학습 모델의 트레이닝 방법을 구현한다."}
{"patent_id": "10-2022-0114224", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 기술에 따르면, 다자간 트레이닝 모델의 개인 데이터 보호 및 모델의 트레이닝 효율성을 모두 고려한 다. 본 문에서 설명된 내용은 본 개시의 실시예의 핵심적인 특징 또는 중요한 특징을 식별하기 위한 것이 아니고, 본 개시의 범위를 한정하려는 의도가 아님을 이해하여야 한다. 본 개시의 기타 특징은 하기 명세서를 통해 쉽게 이해될 수 있다."}
{"patent_id": "10-2022-0114224", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 결합하여 본 개시의 예시적인 실시예에 대해 설명하고, 여기서 본 개시의 실시예를 포함하는 각종 상세한 설명은 이해를 돕기 위한 것이며, 이들을 예시적인 것으로 간주하여야 한다. 따라서, 본 분야의 당업자 는 본 개시의 범위 및 사상을 벗어나지 않고 여기서 설명된 실시예에 대하여 다양한 변경 및 수정이 가능함을 이해할 것이다. 마찬가지로, 명확하고 간결하게 하기 위해, 이하의 설명에서는 공지된 기능 및 구조에 대한 설 명을 생략하도록 한다. 이하, 도면을 결합하여 본 개시의 실시예에서 제공하는 해당 방안에 대해 설명하도록 한다. 도 1은 본 개시의 실시예에서 제공하는 분산형 기계 학습 모델의 트레이닝 방법의 개략도이다. 본 개시의 실시 예는 프라이버시 보호가 요구되는 다자간의 데이터 인터랙션에서 기계 학습 모델에 대한 트레이닝을 수행하는 경우에 적용될 수 있다. 해당 방법은 분산형 기계 학습 모델의 트레이닝 장치에 의해 실행될 수 있고, 해당 장 치는 하드웨어 및/또는 소프트웨어를 통해 구현될 수 있으며, 전자 설비에 구성될 수 있다. 해당 전자 설비는 임의의 참여자에 속하는 설비일 수 있으며, 다자간의 합동 트레이닝 시나리오에서는, 일반적으로 최소 2개의 참 여자가 모델의 트레이닝에 참여하고, 각 참여자는 독립적인 전자 설비를 구비할 수 있으며, 여러 참여자가 인터 랙션하여, 모델의 트레이닝을 완성한다. 본 실시예는 임의의 참여자의 관점에서 설명하며, 각 참여자의 모델의 트레이닝 프로세스는 기본적으로 동일하다. 도 1을 참조하면, 해당 방법은 구체적으로 다음 단계를 포함한다. 단계(S101), 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하고; 여기서, 제1 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 사용하여 제1 트레이닝 중간 파라미 터를 암호화한 데이터이고, 제1 트레이닝 중간 파라미터는 제1 참여자가 제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이며, 제2 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 사용하여 제2 트레이닝 중간 파라미터를 암호화한 데이터이며, 제2 트레이닝 중간 파라미터는 제2 참 여자가 제2 샘플 데이터를 기반으로 제2 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이고, 기계 학습 모 델은 최소 2개의 참여자 각각의 로컬 서브 모델로 구성된다. 기계 학습 모델의 트레이닝은 적어도 2개의 참여자가 참여하고, 기계 학습 모델은 적어도 2개의 참여자 각각의 로컬 서브 모델로 구성되며, 각 서브 모델은 모두 트레이닝할 모델이며, 서브 모델 각자의 트레이닝 프로세스는 기밀성이 있으므로 비밀로 유지해야 한다. 여기서, 제1 참여자는 모델의 트레이닝에 참여하는 여러 개의 참여자중 어느 하나일 수 있고, 제2 참여자는 모델의 트레이닝에 참가하는 여러 개의 참여자 중 제1 참여자 이외의 기 타 참여자일 수 있으며, 제2 참여자는 하나이거나 여러 개일 수 있다. 제1 서브 모델은 제1 참여자의 로컬 서브 모델일 수 있고, 제2 서브 모델은 제2 참여자의 로컬 서브 모델일 수 있다. 여기서, 제1 샘플 데이터는 제1 참여자가 제1 서브 모델을 트레이닝하는데 사용되는 샘플 데이터일 수 있고, 제 2 샘플 데이터는 제2 참여자가 제2 서브 모델을 트레이닝하는데 사용되는 샘플 데이터일 수 있다. 각 참여자가 보유하고 있는 샘플 데이터에 대응하는 객체는 일치하지만, 객체의 속성 데이터는 상이하다. 예를 들어, 온라인 쇼핑 플랫폼이나 음식 배달 플랫폼에서, 각 플랫폼측은 모델 트레이닝의 참여자이고, 트레이닝 객체는 일치하며, 모두 동일한 사용자를 기반으로 하고, 각 참여자는 사용자의 상이한 속성 데이터를 보유한다. 예를 들어, 온라인 쇼핑 플랫폼은 사용자의 온라인 쇼핑과 관련된 데이터를 보유하고, 음식 배달 플랫폼은 사용자의 음식 배달 플랫폼에 기록된 관련 데이터를 보유한다. 온라인 쇼핑 플랫폼이든 음식 배달 플랫폼이든, 사용자의 프라이버시 데이터에 대해 보이지 않게 처리하거나 사용자의 프라이버시 데이터를 숨긴 후, 각 플랫폼에서의 온 라인 쇼핑 정보 또는 음식 배달 정보를 사용해야 한다. 각 참여자가 트레이닝하는 서브 모델은 자신의 플랫폼이 보유한 속성 데이터와 관련된 데이터 처리 서브 모델이다. 각 서브 모델을 트레이닝된 후 결합하여 기계 트레이 닝 모델을 얻고, 사용자의 다양한 속성 데이터를 결합하여, 플랫폼이 요구하는 데이터를 얻을 수 있다. 적어도 하나의 참여자의 샘플 데이터에는 태그가 있으며, 예를 들어 사용자가 계약위반 위험이 있는지 여부를 태그로 하는 것을 온라인 쇼핑 플랫폼에서 알고 있으면, 온라인 쇼핑 플랫폼측의 샘플 데이터에 태그가 있는 것이다. 기타 참여자가 제공한 샘플 데이터에는 태그가 없을 수 있다. 구체적으로, 제1 참여자는 제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝하여 제1 트레이닝 중간 파라 미터를 획득하고; 제2 참여자는 제2 샘플 데이터를 기반으로 제2 서브 모델을 트레이닝하여 제2 트레이닝 중간 파라미터를 획득한다. 상기 트레이닝 프로세스는, 참여자 자신이 보유하고 있는 샘플 데이터를 서브 모델에 입 력하여 계산하고, 트레이닝 중간 파라미터에는 서브 모델에서 출력된, 타겟 손실 함수 및 구배계산에 사용되는 파라미터를 포함한다. 서브 모델의 트레이닝 중간 파라미터는 기타 참여자의 트레이닝 중간 파라미터와 함께 타 겟 손실 함수 및 구배 함수와 같은 계산을 거쳐, 서브 모델의 트레이닝이 수렴되었는지, 어떤 구배로 반복 업데 이트를 해야 할 것인지 등을 확인한다. 제2 참여자는 제2 동형 공개키를 통해, 제2 트레이닝 중간 파라미터에 대해 암호화 동작을 수행하고, 제2 동형 암호화 중간 파라미터를 획득하여, 제1 참여자에게 송신한다. 제1 참여자는 제2 참여자가 송신한 제2 동형 공개 키를 통해, 제1 트레이닝 중간 파라미터를 암호화하여 제1 동형 암호화 중간 파라미터를 획득한다. 여기서, 각 참여자는 자신의 동형 키 쌍을 가지며, 동형 키 쌍은 동형 개인키 및 동형 공개키를 포함하고, 동형 공개키는 기타 참여자에게 제공할 수 있으며, 동형 개인키는 자신이 보류한다. 구체적으로, 제2 참여자는 제2 동형 키 쌍을 생성하고, 제2 동형 키 쌍은 제2 동형 공개키 및 제2 동형 개인키를 포함한다. 제2 동형 개인키는 제2 동형 공개키로 암호화된 데이터를 복호화하는데 사용된다. 제2 동형 공개키는 제2 참여자가 제2 동형 공개 키에 따라 제2 트레이닝 중간 파라미터를 암호화하는데 사용된다. 제2 참여자는 제2 동형 공개키를 제1 참여자 에게 송신하고, 제1 참여자는 제2 동형 공개키에 따라 제1 트레이닝 중간 파라미터를 암호화하여, 제2 참여자는 제2 동형 개인키를 사용하여 제1 참여자가 제2 동형 공개키에 따라 암호화한 데이터를 복호화할 수 있다. 단계(S102), 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여 제1 암호화 간섭 파라 미터를 형성한다. 여기서, 제1 간섭 파라미터는 제1 참여자가 생성한 간섭 파라미터일 수 있으며, 제1 참여자의 모델의 트레이닝 프로세스를 스크램블링하여 프라이버시를 보장하는데 사용된다. 선택적으로, 제1 간섭 파라미터는 난수이고, 예를 들어 임의의 정수 또는 임의의 부동 소수점 수일 수 있다. 여 기서, 임의의 부동 소수점 수의 값 범위는 0 내지 1 사이의 임의의 부동 소수점 수일 수 있다. 제1 참여자는 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키를 사용하여 제1 간섭 파라미터를 암호화하여, 제1 암호화 간섭 파라미터를 형성한다. 단계(S103), 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터, 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성한다. 여기서, 동형 계산 함수는 동형 암호화 데이터를 계산할 수 있으며, 또한 계산의 과정에서 암호화된 데이터가 누출되지 않으며, 계산된 결과는 여전히 암호화된 결과이다. 동형 계산 함수를 통해 동형 암호화 데이터를 계산 하여 얻은 암호화 결과는, 동형 암호화 데이터에 대응하는 평문 데이터를 계산하여 얻은 결과를 복호화한 후와동일하다. 따라서 동형 암호화 기술은 계산 과정에 영향을 미치지 않지만, 계산 과정을 비밀로 유지할 수 있다. 상기 동작에서의 동형 계산 함수는 제1 암호화 키 파라미터를 계산하는데 사용되며, 제1 암호화 키 파라미터는 제1 서브 모델의 트레이닝 종료 조건 판단 파라미터 및/또는 반복 업데이트 파라미터를 포함한다. 예를 들어, 타겟 손실 함수에 의해 계산한 손실값, 또는 반복 업데이트하는 구배값 등이 있다. 구체적으로, 공개키를 이용하여 데이터를 암호화하여 암호화된 데이터를 생성하고, 동형 계산 함수를 이용하여 암호화된 데이터를 계산하여 암호화된 결과를 얻는다면, 개인키를 가진 참여자는 개인키를 통해 암호화된 결과 를 복호화할 수 있다. 제1 참여자는 제1 서브 모델의 동형 계산 함수를 통해, 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터 및 제1 암호화 간섭 파라미터를 계산하여, 제1 암호화 키 파라미터를 생성할 수 있다. 설명해야 할 것 은, 제1 참여자는 제1 서브 모델의 동형 계산 함수를 통해, 제1 동형 암호화 중간 파라미터 및 제2 암호화 중간 파라미터를 계산할 수 있고, 선형 계산 또는 비선형 계산을 통해 제1 암호화 간섭 파라미터를 계산하여 계산 결 과에 추가하여, 제1 암호화 키 파라미터를 생성한다. 일 선택적인 실시예에서, 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터를 기반으로, 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성하 는 단계는: 제1 동형 암호화 중간 파라미터와 제2 동형 암호화 중간 파라미터를 기반으로, 제1 서브 모델의 동 형 계산 함수를 기반으로 계산하고, 제1 암호화 간섭 파라미터를 선형 계산을 통해 계산 결과에 추가하여 제1 암호화 키 파라미터를 생성한다. 예시적으로, 제1 참여자는 제1 서브 모델의 동형 계산 함수를 통해, 제1 동형 암호 중간 파라미터 및 제2 동형 암호 중간 파라미터를 계산하고, 제1 암호화 간섭 파라미터를 선형 계산을 통해 계산 결과에 추가하여, 제1 암 호화 키 파라미터를 생성한다. 예시적으로, 계산 결과의 상이한 형태에 따라, 상이한 선형 계산의 방식으로 계산할 수 있으며, 예를 들어, 계 산 결과로서 중간 파라미터의 형태는 집합 형태일 수 있다. 예를 들어 계산 결과 형태가 집합 형태이면, 집합을 순회하는 방식으로 제1 암호화 간섭 파라미터를 집합 중의 각 요소에 선형으로 추가하여, 제1 암호화 키 파라미 터 집합을 얻을 수 있다. 본 선택적인 실시예는 제1 서브 모델의 동형 계산 함수를 통해, 제1 동형 암호화 중간 파라미터 및 제2 동형 암 호화 중간 파라미터를 계산하고, 제1 암호화 간섭 파라미터를 선형 계산에 기반하여 계산 결과에 추가하는 방식 은 제1 암호화 키 파라미터의 획득을 구현하며, 제1 참여자의 데이터 안전성을 향상시키며, 데이터 누출의 발생 을 방지한다. 단계(S104), 제2 참여자가 제2 동형 개인키를 사용하여 제1 암호화 키 파라미터를 복호화할 수 있도록, 제1 암 호화 키 파라미터를 제2 참여자에게 전송한다. 제1 참여자는 제1 암호화 키 파라미터를 제2 참여자에게 전송하고, 제1 참여자는 제2 동형 개인키를 이용하여 수신한 제1 암호화 키 파라미터를 복호화하여, 제1 키 파라미터를 생성한다. 설명해야 할 것은, 제2 참여자가 생성한 제1 키 파라미터에 제1 간섭 파라미터를 포함하므로, 제2 참여자는 제1 간섭 파라미터를 포함하지 않는 제1 키 파라미터를 획득할 수 없으며, 즉 제1 참여자의 제1 서브 모델이 타겟 함수에 의한 손실값 및 구배 파라미터 등 데이터를 알 수 없다. 따라서 제1 참여자가 사용한 타겟 함수와 구배 함수를 유추할 수 없으며, 제1 참여자의 서브 모델의 트레이닝이 종료하는 조건, 구배 정밀도 등 정보를 알 수 없으므로, 제1 참여자의 모델의 트레이닝 프로세스의 프라이버시가 보장된다. 단계(S105), 제2 참여자의 복호화된 제1 키 파라미터를 획득한다. 단계(S106), 제1 키 파라미터 및 제1 간섭 파라미터에 따라, 제1 서브 모델의 트레이닝이 완료될 때까지 제1 서 브 모델을 반복적으로 업데이트한다. 제1 참여자는 제1 키 파라미터에서 제1 간섭 파라미터를 추출하여 최종의 키 파라미터를 확정하고, 최종적으로 확정된 키 파라미터를 사용하여, 제1 서브 모델의 트레이닝이 완료될 때까지 제1 서브 모델을 반복적으로 업데 이트한다. 설명해야 할 것은, 제1 참여자는 제1 서브 모델의 동형 계산 함수에 따라, 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 기반으로 계산하고, 제1 참여자가 제1 암호화 간섭 파라미터를 선형 계산의방식으로 계산 결과에 추가하면, 제1 참여자는 제1 키 파라미터를 수신한 후, 제1 간섭 파라미터를 기반으로 제 1 키 파라미터에 대해 역으로 선형 계산을 더 해야 한다. 예를 들어, 선형 계산이 서로 가하는 것이면, 역으로 선형 계산은 서로 감하는 것일 수 있다. 일 선택적인 실시예에서, 제1 키 파라미터 및 제1 간섭 파라미터에 따라, 제1 서브 모델의 트레이닝이 완료될 때까지 제1 서브 모델을 반복적으로 업데이트하는 단계는: 제1 간섭 파라미터를 기반으로 제1 키 파라미터에 대 해 역으로 선형 계산을 하고, 제1 서브 모델의 트레이닝이 완료될 때까지, 역으로 선형 계산 후의 키 파라미터 에 따라, 제1 서브 모델을 반복적으로 업데이트하는 단계; 를 포함한다. 제1 참여자가 선형 계산의 방법을 사용하여, 제1 암호화 간섭 파라미터를 계산 결과에 추가하는 경우, 제1 참여 자는 제1 키 파라미터를 수신한 후, 제1 간섭 파라미터에 따라 제1 키 파라미터를 역으로 선형 계산하여 제1 키 파라미터에서 제1 간섭 파라미터를 추출하고, 역으로 선형 계산 후의 키 파라미터를 확정한다. 구체적으로, 선 형 계산에 대응하는 규칙을 사용하여, 제1 간섭 파라미터를 기반으로 제1 키 파라미터에 대한 역으로 선형 계산 을 수행한다. 제1 참여자는 제1 서브 모델의 트레이닝이 완료될 때까지, 역으로 선형 계산 후의 키 파라미터에 따라, 제1 서브 모델을 반복적으로 업데이트한다. 각 참여자는 트레이닝이 완료된 서브 모델을 보유할 수 있으 며, 기계 학습 모델을 사용하여 처리해야 하는 경우, 입력된 처리해야 할 데이터를 각 참여자에게 각각 제공하 고, 서브 모델의 처리를 거친 후 다시 결과를 집계한다. 제1 키 파라미터가 집합 형태인 것을 예로 들어, 선형 계산의 규칙이 제1 암호화 간섭 집합의 각 요소를 순회하 고, 각 요소에 대해 동일한 제1 암호화 간섭 파라미터를 선형으로 추가하는 것이라면, 역으로 선형 계산은 제1 키 파라미터 집합 중의 각 요소를 순회하고, 각 요소에서 제1 간섭 파라미터를 빼내는 것일 수 있다. 본 선택적인 실시예에서, 역으로 선형 계산하는 방식으로 키 파라미터를 확정하고, 상기 제1 참여자가 선형 계 산의 방식으로, 제1 암호화 간섭 파라미터를 계산 결과에 추가하는 것과 대응하며, 상기 선형 계산의 방식을 사 용하여 결과를 확정하는 것을 위해 해결하는 방법을 제공하므로, 제1 간섭 파라미터에 따라 키 파라미터를 정확 하게 확정할 수 있다. 본 개시의 실시예에서, 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하고; 제1 간 섭 파라미터를 생성하며, 제2 참여자의 제2 동형 공개키로 암호화하여, 제1 암호화 간섭 파라미터를 형성하며; 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 간섭 파라미터, 및 제1 서브 모델 의 동형 계산 함수를 기반으로 계산하여, 제1 암호화 키 파라미터를 생성하며; 제2 참여자가 제2 동형 개인키를 사용하여 제1 암호화 키 파라미터를 복호화할 수 있도록, 제1 암호화 키 파라미터를 제2 참여자에게 전송하고; 제2 참여자의 복호화된 제1 키 파라미터를 획득하고; 제1 서브 모델의 트레이닝이 완료될 때까지, 제1 키 파라 미터 및 상기 제1 간섭 파라미터에 따라, 제1 서브 모델을 반복적으로 업데이트한다. 상기 방안은 다자간 트레 이닝 모델의 프라이버시 데이터 및 트레이닝 프로세스 데이터의 보호를 구현함과 동시에 신뢰할 수 있는 제3자 를 사용하여 조정할 필요가 없으므로 신뢰할 수 있는 제3자가 데이터에 대한 악의적으로 유출하는 가능성을 방 지할 수 있으며, 대량의 데이터가 여러 참여자에서 인터랙션하는 것을 방지하여 모델의 트레이닝 프로세스의 데 이터 전송량을 절감하여, 모델의 트레이닝의 효율성을 향상시킨다. 설명해야 할 것은, 제1 참여자와 제2 참여자가 트레이닝한 서브 모델의 정밀도를 보장하기 위해, 제1 참여자와 제2 참여자에 대응하는 샘플 데이터가 동일한 객체에 속하여야 하고, 따라서, 제1 참여자 및 제2 참여자의 샘플 데이터에서 동일한 사용자에 속하는 샘플 데이터를 획득하여 제1 참여자에 대응하는 제1 샘플 데이터 및 제2 참 여자에 대응하는 제2 샘플 데이터로 해야 한다. 일 선택적인 실시예에서, 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하기 전에, 제1 참여자와 제2 참여자의 샘플 데이터의 교집합 식별자를 확정하는 단계를 더 포함하고, 적어도 하나의 참여 자의 샘플 데이터는 대응하게 태그가 표기된다. 적어도 하나의 참여자의 샘플 데이터는 대응하게 태그가 표기된다. 여기서, 태그는 모델의 트레이닝의 결과일 수 있으며, 예를 들어, 샘플 데이터가 사용자 계약위반 위험계수인 경우, 대응하는 태그는 \"위험 있음\"이거나 \"위험 없음\"일 수 있다. 예시적으로, 제1 참여자의 샘플 데이터와 제2 참여자의 샘플 데이터에 대해 프라이버시 집합의 교집합 연산 (Private Set Intersection, PSI)을 통해 각각 교집합 연산을 수행할 수 있다. 여기서, PSI는 각 참여자가 임 의의 추가 정보를 누출하지 않고 각 참여자가 보유한 샘플 데이터의 교집합을 얻을 수 있다. 여기서, 추가 정보 는 각 참여자의 샘플 데이터의 교집합 이외의 임의의 정보일 수 있다.구체적으로, 제1 참여자의 샘플 데이터와 제2 참여자의 샘플 데이터에 대해 PSI 계산을 수행하고, 제1 참여자와 제2 참여자가 공동으로 보유한 샘플 데이터의 교집합을 확정하며, 샘플 데이터 교집합 식별자를 확정한다. 여기 서 샘플 데이터 교집합 식별자는 사용자의 주민등록번호, 사용자의 이름 등일 수 있다. 제1 참여자와 제2 참여 자는 샘플 데이터 교집합 식별자만 알 수 있고, 샘플 데이터 교집합 식별자 외의 기타 추가 정보를 알 수 없으 므로, 각 참여자의 각 샘플 데이터의 안전성을 보장한다. 제1 참여자는 PSI 계산에 의해 확정된 샘플 데이터 교 집합 식별자에 따라 제1 샘플 데이터를 획득하고, 제2 참여자는 PSI 계산에 의해 확정된 샘플 데이터 교집합 식 별자에 따라 제2 샘플 데이터를 획득한다. 예를 들어, 제1 참여자의 샘플 데이터는 사용자의 주민등록번호 데이터 및 사용자의 신용도 데이터이고, 제2 참 여자의 샘플 데이터는 사용자의 주민등록번호 데이터 및 사용자의 나이 데이터이다. PSI 계산을 통해 제1 참여 자와 제2 참여자의 샘플 데이터 교집합 식별자가 사용자의 주민등록번호이라는 것을 확정할 수 있으며, 사용자 의 주민등록번호를 통해 각 참여자의 샘플 데이터의 교집합을 확정할 수 있다. 제1 참여자는 제2 참여자의 샘플 데이터에 사용자의 주민등록번호에 대응하는 샘플 데이터가 있다는 것만 알 수 있고, 제2 참여자의 샘플 데이터 에 있는 기타 샘플 데이터 식별자, 예를 들어 사용자의 나이, 더 나아가 사용자의 나이 데이터를 알 수 없다. 마찬가지로, 제2 참여자는 제1 참여자의 샘플 데이터에서의 기타 샘플 데이터 식별자를 알 수 없으며, 예를 들 어 사용자의 신용도, 더 나아가 사용자의 신용도 데이터를 알 수 없다. 본 선택적인 실시예에서, PSI 계산 방법을 사용함으로써, 제1 참여자 및 제2 참여자의 샘플 데이터 교집합 식별 자가 확정되어, 제1 참여자가 샘플 데이터 교집합 식별자에 따라 제1 샘플 데이터를 확정하고, 제2 참여자가 샘 플 데이터 교집합 식별자에 따라 제2 샘플 데이터를 확정하는 것을 구현하여, 샘플 데이터의 일관성을 보장한다. 또한, PSI를 통해 샘플 데이터의 교집합을 계산하는 방식은 각 참여자가 샘플 데이터 교집합 식별자 이외의 기타 추가 정보를 얻을 수 없도록 하여, 각 참여자의 각 샘플 데이터의 안전성을 보장한다. 적어도 하나 의 참여자의 샘플 데이터에 태그를 표기하는 방식으로 모델의 트레이닝 결과 타입의 확정을 구현한다. 도 2는 본 개시의 실시예에서 제공하는 다른 분산형 기계 학습 모델의 트레이닝 방법의 개략도이다. 본 실시예 는 상기 실시예를 기반으로 제안된 선택적인 방안이다. 도 2를 참조하면, 본 실시예에서 제공하는 분산형 기계 학습 모델의 트레이닝 방법은 다음 단계를 포함한다. 단계(S201), 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득한다. 여기서, 제1 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 이용하여 제1 트레이닝 중간 파라미 터를 암호화한 데이터이고, 제1 트레이닝 중간 파라미터는 제1 참여자가 제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이고, 제2 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 사용하여 제2 트레이닝 중간 파라미터를 암호화한 데이터이며, 제2 트레이닝 중간 파라미터는 제2 참 여자가 제2 샘플 데이터를 기반으로 제2 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이고, 기계 학습 모 델은 최소 2개의 참여자 각각의 로컬 서브 모델로 구성된다. 여기서, 제1 트레이닝 중간 파라미터는 제1 중간 손실 파라미터와 제1 중간 구배 파라미터를 포함할 수 있고, 제2 트레이닝 중간 파라미터는 제2 중간 손실 파라미터와 제2 중간 구배 파라미터를 포함할 수 있다. 상응하게, 제1 동형 암호화 중간 파라미터는 제1 동형 암호화 손실 파라미터와 제1 동형 암호화 손실 파라미터를 포함할 수 있고, 제2 동형 암호화 중간 파라미터는 제2 동형 암호화 손실 파라미터와 제2 동형 암호화 손실 파라미터를 포함할 수 있다. 여기서, 손실 파라미터는 모델의 수렴 정도를 나타낼 수 있으며, 구배 파라미터는 모델의 파라 미터를 업데이트할 수 있다. 선택적으로, 각 참여자는 각 로컬 서브 모델을 통해 파라미터를 트레이닝하기 전에, 서브 모델을 초기화할 수 있으며, 초기화 과정에서 현재 손실 파라미터를 랜덤으로 설정할 수 있으며, 예를 들어 0으로 설정할 수 있다. 설명해야 할 것은, 기계 학습 모델의 트레이닝 프로세스에서, 샘플 데이터의 크기(Magnitude)가 크기 때문에, 생성된 파라미터는 일반적으로 집합의 형태로 제시되며, 집합에 포함된 요소는 모델이 트레이닝 프로세스에서 생성된 중간 파라미터, 예를 들어, 손실 파라미터 집합은 손실 중간 파라미터를 포함하고, 구배 파라미터 집합 은 구배 중간 파라미터를 포함한다. 따라서, 본 실시예에 관련된 모델의 트레이닝을 통해 획득한 손실 파라미터 및 구배 파라미터의 존재 형태는 집합 형태이며, 동형 암호화를 통해 획득한 암호화 손실 파라미터 및 암호화 구배 파라미터의 존재 형태는 여전히 집합의 형태이다. 단계(S202), 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여 제1 암호화 간섭 파라 미터를 형성한다.여기서, 제1 간섭 파라미터는 제1 구배 간섭 파라미터 및 제1 손실 간섭 파라미터를 포함한다. 제2 참여자의 제 2 동형 공개키를 사용하여 제1 구배 간섭 파라미터와 제1 손실 간섭 파라미터를 암호화하여, 제1 암호화 구배 간섭 파라미터 및 제1 암호화 손실 간섭 파라미터를 형성한다. 제1 구배 간섭 파라미터 및 제1 손실 간섭 파라미터는 난수이고, 예를 들어 임의의 정수 또는 임의의 부동 소수 점 수일 수 있으며, 제1 구배 간섭 파라미터 및 제1 손실 간섭 파라미터의 난수는 같거나 다를 수 있으며, 구체 적으로 담당자가 실제 수요에 따라 인위적으로 설정할 수 있다. 단계(S203), 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 구배 간섭 파라미터 및 제1 서브 모델의 동형 구배 계산 함수를 기반으로 계산하여, 제1 암호화 구배 파라미터를 생성한다. 제1 동형 암호화 중간 파라미터 중의 제1 동형 암호화 구배 파라미터, 제2 동형 암호화 중간 파라미터 중의 제2 동형 암호화 구배 파라미터와 제1 암호화 구배 간섭 파라미터를 기반으로, 제1 서브 모델의 동형 구배 계산 함 수를 기반으로 계산하여, 제1 암호화 구배 파라미터를 생성한다. 여기서, 동형 구배 계산 함수는 제1 동형 암호화 구배 파라미터 및 제2 동형 암호화 구배 파라미터에 대한 구배 계산을 수행하는데 사용될 수 있다. 예시적으로, 제1 참여자는 제1 서브 모델의 동형 구배 계산 함수를 통해, 제1 동형 암호화 구배 파라미터 및 제2 동형 암호화 구배 파라미터에 대해 구배 계산을 수행하고, 계산하여 얻 은 구배 계산 결과에서, 제1 암호화 구배 간섭 파라미터를 선형으로 추가한다. 예를 들어, 계산 결과로 얻은 구 배 집합을 순회할 수 있으며, 제1 암호화 간섭을 집합 중의 각 요소에 선형으로 추가하고, 선형으로 추가된 결 과를 제1 암호화 구배 파라미터로 한다. 단계(S204), 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 손실 간섭 파라미터 및 제1 서브 모델의 동형 손실 계산 함수를 기반으로 계산하여, 제1 암호화 손실 파라미터를 생성한다. 제1 동형 암호화 중간 파라미터 중의 제1 동형 암호화 손실 파라미터, 제2 동형 암호화 중간 파라미터 중의 제2 동형 암호화 손실 파라미터와 제1 암호화 손실 간섭 파라미터를 기반으로, 제1 서브 모델의 동형 손실 계산 함 수를 기반으로 계산하여, 제1 암호화 손실 파라미터를 생성한다. 여기서, 동형 손실 계산 함수는 제1 동형 암호화 손실 파라미터와 제2 동형 암호화 손실 파라미터에 대해 손실 을 계산하는데 사용될 수 있으며, 선택적으로, 동형 손실 계산 함수의 설정은 상기 동형 구배 계산 함수 설정과 동일하거나 다를 수 있다. 예시적으로, 제1 참여자는 제1 서브 모델의 동형 손실 계산 함수를 통해, 제1 동형 암호화 손실 파라미터와 제2 동형 암호화 손실 파라미터의 손실을 계산하고, 계산 결과로 얻은 손실 집합을 순회할 수 있으며, 제1 암호화 손실 간섭을 집합 중의 각 요소에 선형으로 추가하고, 선형으로 추가된 결과를 제1 암호화 손실 파라미터로 한 다. 단계(S205), 제2 참여자가 제2 동형 개인키를 사용하여 제1 암호화 키 파라미터를 복호화할 수 있도록, 제1 암 호화 키 파라미터를 제2 참여자에게 전송한다. 여기서, 제1 암호화 키 파라미터는 제1 암호화 손실 파라미터와 제1 암호화 구배 파라미터를 포함한다. 구체적 으로, 제1 참여자는 제1 암호화 손실 파라미터와 제1 암호화 구배 파라미터를 제2 참여자에게 전송하여, 제2 파 라미터 참여자가 제2 동형 개인키를 사용하여 제1 암호화 손실 파라미터와 제2 암호화 구배 파라미터를 복호화 할 수 있도록 한다. 단계(S206), 제2 참여자의 복호화된 제1 키 파라미터를 획득한다. 여기서, 제1 키 파라미터는 제1 구배 파라미터 및 제1 손실 파라미터를 포함한다. 구체적으로, 제1 참여자는 제 2 참여자에 의해 복호화된 제1 구배 파라미터 및 제1 손실 파라미터를 획득한다. 단계(S207), 제1 키 파라미터 및 제1 간섭 파라미터에 따라, 제1 서브 모델의 트레이닝이 완료될 때까지 제1 서 브 모델을 반복적으로 업데이트한다. 제1 키 파라미터 중의 제1 구배 파라미터 및 제1 간섭 파라미터 중의 제1 구배 간섭 파라미터에 따라, 최종 구 배 키 파라미터를 결정하고; 제1 키 파라미터 중의 제1 손실 파라미터 및 제1 간섭 파라미터 중의 제1 손실 간 섭 파라미터에 따라 최종 손실 키 파라미터를 결정한다. 최종 손실 키 파라미터를 사용하여 제1 서브 모델이 수 렴되었는지 여부를 판단하며, 수렴되면 트레이닝이 완료되고, 수렴되지 않으면 최종 구배 키 파라미터를 사용하 여 제1 서브 모델을 반복적으로 업데이트하고, 제1 참여자의 제1 서브 모델의 트레이닝이 완료될 때까지 다시다음 라운드의 트레이닝 중간 파라미터 계산과 키 파라미터 계산을 실행한다. 본 개시의 실시예는 제1 서브 모델의 동형 손실 계산 함수를 사용하여, 제1 동형 암호화 손실 파라미터 및 제2 동형 암호화 손실 파라미터에 대한 계산을 구현하고, 제1 암호화 손실 간섭 파라미터를 선형으로 계산 결과에 추가하여, 제1 암호화 손실 파라미터를 생성하여, 제1 암호화 손실 파라미터를 확정하는 정밀도를 향상시킨다. 제1 서브 모델의 동형 구배 계산 함수를 사용하여, 제1 동형 암호화 구배 파라미터 및 제2 동형 암호화 구배 파 라미터에 대한 계산을 구현하고, 제1 암호화 구배 간섭 파라미터를 선형으로 계산 결과에 추가하여, 제1 암호화 구배 파라미터를 생성하여, 제1 암호화 구배 파라미터를 확정하는 정밀도를 향상시킨다. 설명해야 할 것은, 제2 참여자가 최종적으로 확정된 키 파라미터에 따라 제2 서브 모델을 트레이닝할 수 있도록 확보하기 위해, 제1 참여자의 제1 동형 공개키도 수신하여, 제2 참여자가 제1 동형 공개키에 따라 제1 트레이닝 중간 파라미터와 제2 트레이닝 중간 파라미터를 암호화한다. 일 선택적인 실시예에서, 제1 동형 키 쌍을 생성하고, 제1 동형 키 쌍은 제1 동형 공개키 및 제1 동형 개인키를 포함하고; 제1 동형 공개키를 제2 참여자에게 송신하며; 제2 참여자가 생성하고 상기 제2 참여자가 송신한 제2 동형 키 쌍의 제2 동형 공개키를 수신한다. 제1 참여자는 제1 동형 키 쌍을 생성하고, 제1 동형 키 쌍은 제1 동형 공개키 및 제1 동형 개인키를 포함하며, 제2 참여자는 제2 동형 키 쌍을 생성하고, 제2 동형 키 쌍은 제2 동형 공개키 및 제2 동형 개인키를 포함한다. 제1 참여자는 제1 동형 공개키를 제2 참여자에게 송신하고, 제2 참여자는 제2 동형 공개키를 제1 참여자에게 송 신한다. 본 선택적인 실시예는 제1 동형 키 쌍을 생성하고, 제1 동형 키 쌍의 제1 동형 공개키를 제2 참여자에게 송신함 으로써, 제2 참여자가 제1 동형 공개키에 따라 제1 트레이닝 중간 파라미터 및 제2 트레이닝 중간 파라미터에 대해 암호화하여, 따라서 후속하는 제2 참여자가 키 파라미터를 확정하고, 키 파라미터에 따라 제2 서브 모델을 트레이닝할 수 있도록 보장한다. 도 3은 본 개시의 실시예에서 제공하는 또 다른 분산형 기계 학습 모델의 트레이닝 방법의 개략도이다. 본 실시 예는 상기 실시예를 기반으로 제안된 선택적인 방안이고, 샘플 데이터에 대한 표준화 처리, 및 기계 트레이닝 모델에 대한 비표준화 처리를 추가한다. 도 3을 참조하면, 본 실시예에서 제공하는 분산형 기계 학습 모델의 트 레이닝 방법은 다음 단계를 포함한다. 단계(S301), 제1 참여자의 제1 원본 샘플 데이터를 표준화 처리하여, 샘플 특징 차원의 제1 표준화 값 및 제1 샘플 데이터를 획득하고; 표준화된 제1 샘플 데이터는 모델을 트레이닝하는데 사용된다. 여기서, 제1 원본 샘플 데이터는 상이한 특징 차원에서, 상이한 차원 및 수량급을 갖는다. 제1 원본 샘플 데이 터의 상이한 특징 차원의 수치 레벨 차이가 비교적 큰 경우, 제1 참여자는 제1 원본 샘플 데이터를 직접 사용하 여 모델을 트레이닝하므로, 이는 모델의 트레이닝에서 수치 레벨이 더 높은 특징 차원의 역할을 강조하므로써 모델의 트레이닝에서의 수치 레벨이 낮은 특징 차원의 역할을 약화시킨다. 예를 들어, 샘플 특징 차원은 사용자 의 월 소득, 사용자의 연체 날짜 수 또는 사용자의 신용도일 수 있다. 여기서, 사용자의 월 소득의 수량급이 상 대적으로 크기에, 대응하는 수치 레벨이 상대적으로 높으며, 사용자의 월 소득에 비해 사용자의 연체 날짜 수의 수량급이 상대적으로 작으며, 대응하는 수치 레벨이 상대적으로 낮다. 데이터 표준화 알고리즘을 통해, 제1 참여자의 제1 원본 샘플 데이터를 표준화한다. 여기서, 데이터 표준화 알 고리즘은 min-max 표준화(Min-maxnormalization) 알고리즘, z-score 표준화(zero-meannormalization) 알고리즘, log 함수 변환 알고리즘 또는 표준화 알고리즘 등일 수 있다. 데이터 표준화 알고리즘은 실제 수요에 따라 당업자가 선택할 수 있으며, 이는 본 실시예에서 한정하지 않는다. 데이터 표준화 알고리즘을 통해, 제1 참여자의 제1 원본 샘플 데이터를 표준화하여, 샘플 특징 차원의 제1 표준 화 값과 제1 샘플 데이터를 얻는다. 여기서, 제1 표준화 값은 샘플 특징 차원의 제1 표준차 및 제1 평균치를 포 함할 수 있다. 표준화된 제1 샘플 데이터를 사용하여 모델에 대해 트레이닝한다. 단계(S302), 제3 암호키로 제1 표준화 값을 암호화하여, 제1 암호화된 표준화 값을 형성하고, 제1 암호화된 표 준화 값을 신뢰할 수 있는 제3자에게 제공한다. 여기서 신뢰할 수 있는 제3자는 하드웨어 기반의 신뢰된 실행 환경(Trusted Execution Environment, TEE) 또는 안전한 다자간 계산(MPC: Secure Muti-Party Computation) 프로토콜일 수 있다.신뢰할 수 있는 제3자는 제3 암호키를 생성할 수 있으며, 제3자 암호키는 대칭 암호키 또는 비대칭 암호키일 수 있다. 신뢰할 수 있는 제3자는 제3 암호키를 각 참여자에게 각각 송신하여, 각 참여자가 제3 암호키에 따라 데 이터를 암호화할 수 있도록 한다. 일 선택적인 실시예에서, 제3 암호키가 대칭 암호키인 경우, 신뢰할 수 있는 제3자는 제3 암호키를 제1 참여자 및 제2 참여자에게 송신한다. 제1 참여자는 제3 암호키를 사용하여 제1 표준화 값을 암호화하여 제1 암호화된 표준화 값을 형성하고, 형성된 제1 암호화된 표준화 값을 신뢰할 수 있는 제3자에게 제공한다. 마찬가지로, 제2 참여자는 제3 암호키로 제2 표준화 값을 암호화하여, 제2 암호화된 표준화 값을 형성하고, 형성된 제2 암호화된 표준화 값을 신뢰할 수 있는 제3자에게 제공한다. 또 다른 선택적인 실시예에서, 제3 암호키가 비대칭키인 경우, 제3 암호키는 제3 암호화 공개키 및 제3 암호화 개인키를 포함하고, 신뢰할 수 있는 제3자가 제3 암호키를 제1 참여자와 제2 참여자에게 송신한다. 제1 참여자 는 제3 암호화 공개키를 사용하여 제1 표준화 값을 암호화하여, 제1 암호화된 표준화 값을 형성하고, 형성된 제 1 암호화된 표준화 값을 신뢰할 수 있는 제3자에게 제공한다. 마찬가지로, 제2 참여자는 제3 암호화 공개키를 사용하여 제2 표준화 값을 암호화하여, 제2 암호화된 표준화 값을 형성하고, 형성된 제2 암호화된 표준화 값을 신뢰할 수 있는 제3자에게 제공한다. 단계(S303), 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득한다. 설명해야 할 것은, 제1 서브 모델이 트레이닝된 후 생성된 제1 트레이닝 중간 파라미터의 정밀도가 균일하도록 보장하기 위해, 제1 트레이닝 중간 파라미터의 정밀도를 조정할 수도 있으며, 따라서 후속 연산에 의해 얻은 데 이터의 정밀도도 균일하도록 보장한다. 일 선택적인 실시예에서, 제1 동형 암호화 중간 파라미터를 획득하는 단계는: 제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후, 제1 트레이닝 중간 파라미터를 생성하는 단계; 설정된 정밀도에 따라 제1 트레이닝 중간 파라미터의 부동 소수점 수(floating-point number)를 제1 큰 정수로 변환하는 단계; 제1 큰 정수의 정수 값을 인코딩하여 제1 양의 정수로 변환하는 단계; 제2 참여자의 제2 동형 공개키를 사용하여 제1 양의 정수를 암호화하여 제1 동형 암호화 중간 파라미터를 얻는 단계; 를 포함한다. 여기서, 정밀도는 당업자가 실제 수요에 따라 설정할 수 있으며, 구체적으로 소수점 이하 자릿수일 수 있으며, 예를 들어, 정밀도를 소수점 이하 2자리까지 설정할 수 있다. 설정된 정밀도에 따라, 제1 트레이닝 중간 파라미터의 부동 소수점 수는 큰 정수 변환 함수를 통해 제1 큰 정수 로 변환된다. 예를 들어, 큰 정수 변환 함수는 가우스 함수 또는 ceil 함수 등일 수 있다. 제1 큰 정수의 정수 값을 인코딩하여, 제1 양의 정수로 변환하고, 구체적으로 원본코드, 보수코드 및/또는 역코드의 인코딩 방법을 사용하여 제1 큰 정수의 정수 값을 인코딩하여, 제1 양의 정수로 변환한다. 제2 참여자의 제2 동형 공개키를 사 용하여 제1 양의 정수를 암호화하여, 제1 동형 암호화 중간 파라미터를 얻는다. 본 선택적인 실시예는 정밀도를 설정함으로써 제1 트레이닝 중간 파라미터의 부동 소수점 수를 제1 큰 정수로 변환하고; 제1 큰 정수의 정수 값을 인코딩하여 제1 양의 정수로 변환하여, 제1 트레이닝 중간 파라미터의 정밀 도를 균일하도록 하여, 따라서 후속 연산에 의해 얻은 데이터의 정밀도도 균일하도록 한다. 단계(S304), 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여 제1 암호화 간섭 파라 미터를 형성한다. 단계(S305), 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터, 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여, 제1 암호화 키 파라미터를 생성한다. 단계(S306), 제2 참여자가 제2 동형 개인키를 사용하여 제1 암호화 키 파라미터를 복호화할 수 있도록 제1 암호 화 키 파라미터를 제2 참여자에게 전송한다. 단계(S307), 제2 참여자의 복호화된 제1 키 파라미터를 획득한다. 단계(S308), 제1 키 파라미터 및 제1 간섭 파라미터에 따라, 제1 서브 모델의 트레이닝이 완료될 때까지 제1 서 브 모델을 반복적으로 업데이트한다. 제1 트레이닝 중간 파라미터에 대해 정밀도 처리를 수행하였기 때문에, 제1 트레이닝 중간 파라미터의 부동 소 수점 수는 제1 양의 정수로 최종적으로 변환되고, 따라서 후속 생성된 제1 키 파라미터도 정밀도 처리 후의 데 이터이다. 따라서 제1 키 파라미터를 역으로 처리해야 하며, 제1 키 파라미터에 대해 정밀도 스케일링(scaling)하여 원래 정밀도를 복원할 수 있다. 일 선택적인 실시예에서, 제1 키 파라미터 및 제1 간섭 파라미터에 따라, 제1 서브 모델을 반복적으로 업데이트 하는 단계는: 제1 간섭 파라미터를 기반으로 제1 키 파라미터에 대해 간섭 제거 동작을 수행하는 단계; 간섭 제 거된 후의 제1 키 파라미터를 디코딩하고, 설정된 정밀도에 따라 스케일링(scaling)하여 부동 소수점 수로 변환 하는 단계; 변환된 부동 소수점 수 형태의 제1 키 파라미터를 사용하여 제1 서브 모델을 반복적으로 업데이트하 는 단계; 를 포함한다. 제1 키 파라미터를 디코딩하는 동작의 경우, 먼저, 제1 키 파라미터의 수치 타입을 원래의 양의 정수에서 정수 로 변환하고, 여기서 정수는 양의 정수 또는 음의 정수를 포함하고, 다음으로 변환된 정수를 정밀도에 따라 스 케일링하며, 여기서 정밀도는 인코딩 시 설정된 정밀도와 일치하도록 설정된다. 마지막으로 정밀도가 변환된 정 수를 부동 소수점 수로 변환한다. 변환된 부동 소수점 수 형태의 제1 키 파라미터를 사용하여 상기 제1 서브 모 델을 반복적으로 업데이트한다. 본 선택적인 실시예는 역처리의 방식을 사용하여, 간섭 제거된 후의 제1 키 파라미터를 디코딩하고, 설정된 정 밀도에 따라 스케일링하여 부동 소수점 수로 변환하여, 제1 키 파라미터의 정밀도의 스케일링을 구현하고, 원래 정밀도로 복원하여, 데이터의 수치 타입의 일관성을 보장한다. 단계(S309), 제1 서브 모델을 제3 암호키로 암호화하여, 제1 암호화 서브 모델을 형성하고, 제1 암호화 서브 모 델을 신뢰할 수 있는 제3자에게 제공하여, 신뢰할 수 있는 제3자가 각 참여자가 제공한 암호화 서브 모델을 기 반으로 결합하고, 각 참여자가 제공한 암호화된 표준화 값을 기반으로 비표준화 처리를 한 다음 분할한다. 일 선택적인 실시예에서, 신뢰할 수 있는 제3자에 의해 송신된 제3 암호키가 대칭 암호키인 경우, 제1 참여자는 제1 서브 모델을 제3 암호키로 암호화하여, 제1 암호화 서브 모델을 형성하고, 제1 암호화 서브 모델을 신뢰할 수 있는 제3자에게 제공한다. 마찬가지로, 제2 참여자는 제2 서브 모델을 제3 암호키로 암호화하여, 제2 암호화 서브 모델을 형성하고, 제2 암호화 서브 모델을 신뢰할 수 있는 제3자에게 제공한다. 신뢰할 수 있는 제3자는 제3 암호키를 사용하여, 수신한 제1 참여자가 제공한 제1 암호화 서브 모델 및 제1 암 호화된 표준화 값을 복호화하고; 수신한 제2 참여자가 제공한 제2 암호화 서브 모델 및 제2 암호화된 표준화 값 을 복호화한다. 신뢰할 수 있는 제3자는 복호화된 제1 참여자와 제2 참여자의 서브 모델을 결합하고, 복호화된 제1 참여자와 제2 참여자의 표준화 값을 기반으로 비표준화 처리를 하고; 비표준화 처리된 서브 모델을 분할하 여, 비표준화된 제1 서브 모델과 비표준화된 제2 서브 모델로 분할한다. 신뢰할 수 있는 제3자는 비표준화된 서 브 모델을 제3 암호키로 암호화하여 각 참여자에게 각각 송신한다. 또 다른 선택적인 실시예에서, 신뢰할 수 있는 제3자가 송신한 제3 암호키가 비대칭 암호키인 경우, 제1 참여자 는 신뢰할 수 있는 제3자가 송신한 제3 암호화 공개키를 사용하여 제1 서브 모델을 암호화하여, 제1 암호화 서 브 모델을 형성하고, 제1 암호화 서브 모델을 신뢰할 수 있는 제3자에게 제공한다. 마찬가지로, 제2 참여자는 신뢰할 수 있는 제3자가 송신한 제3 암호화 공개키를 사용하여 제2 서브 모델을 암호화하여, 제2 암호화 서브 모델을 형성하고, 제2 암호화 서브 모델을 신뢰할 수 있는 제3자에게 제공한다. 신뢰할 수 있는 제3자는 제3 암호화 개인키를 사용하여, 수신된 제1 참여자가 제공한 제1 암호화 서브 모델 및 제1 암호화된 표준화 값을 복호화하고, 수신된 제2 참여자가 제공한 제2 암호화 서브 모델 및 제2 암호화된 표 준화 값을 복호화한다. 신뢰할 수 있는 제3자는 복호화된 제1 참여자와 제2 참여자의 서브 모델을 결합하고, 복 호화된 제1 참여자와 제2 참여자의 표준화 값을 기반으로 비표준화 처리를 하고; 비표준화 처리된 서브 모델을 분할하여, 비표준화된 제1 서브 모델과 비표준화된 제2 서브 모델로 분할한다. 신뢰할 수 있는 제3자는 비표준 화된 서브 모델을 제3 암호키로 암호화하여 각 참여자에게 각각 송신한다. 단계(S310), 신뢰할 수 있는 제3자가 리턴한 분할된 제1 비표준화 서브 모델을 수신한다. 일 선택적인 실시예에서, 신뢰할 수 있는 제3자가 송신한 제3 암호키가 대칭 암호키인 경우, 각 참여자는 제3 암호키로 신뢰할 수 있는 제3자에 의해 리턴한 분할된 각 비표준화된 서브 모델을 복호화하여, 복호화된 비표준 화된 서브 모델을 얻는다. 다른 선택적인 실시예에서, 신뢰할 수 있는 제3자가 송신한 제3 암호키가 비대칭 암호키인 경우, 각 참여자는 제3 암호화 공개키로 신뢰할 수 있는 제3자에 의해 리턴한 분할된 각 비표준화된 서브 모델을 복호화하여, 복호 화된 비표준화된 서브 모델을 얻는다. 본 선택적인 실시예는 각 참여자의 원본 샘플 데이터를 표준화 처리함으로써 샘플 데이터의 상이한 특징 차원의 차원 및 수학의 통합을 구현함으로써, 상이한 특징 차원의 수치 레벨 차이가 너무 큼으로 인해 트레이닝 결과가 정확하지 않는 것을 방지한다. 각 참여자의 표준화된 값과 표준화된 값으로 트레이닝된 서브 모델을 신뢰할 수 있는 제3자에게 송신함으로써, 신뢰할 수 있는 제3자는 각 참여자의 서브 모델을 비표준화함으로써, 데이터의 안전성을 보장하여 데이터가 노출되는 경우가 발생하는 것을 방지한다. 본 개시의 실시예에서 제공하는 분산형 기계 학습 모델의 트레이닝 방법을 제공하며, 본 실시예에서는 2개의 참 여자를 예로 들어 설명한다. 해당 모델 트레이닝의 전체 프로세스는 모델의 트레이닝 단계와 모델의 복구 단계로 나눌 수 있다. 참여자 A(태 그가 없는 샘플 데이터)와 참여자 B(태그가 있는 샘플 데이터)의 두 참여자가 있다고 가정한다. 신뢰할 수 있는 제3자 C도 있으며, TEE 환경에서 실행될 수 있다. 모델의 트레이닝 프로세스는 다음을 포함한다: 단계(S1), 참여자 A는 A의 동형 키 쌍을 생성한다. 단계(S2), 참여자 B는 B의 동형 키 쌍을 생성한다. 단계(S3), 신뢰할 수 있는 제3자 C는 A의 비동형 암호키 쌍을 생성한다. 단계(S4), 신뢰할 수 있는 제3자 C는 B의 비동형 암호키 쌍을 생성한다. 단계(S5), 참여자 A는 A의 동형 공개키를 B에게 송신한다. 단계(S6), 참여자 B는 B의 동형 공개키를 A에게 송신한다. 단계(S7), 신뢰할 수 있는 제3자 C는 A의 비동형 암호화 공개키를 A에게 송신한다. 단계(S8), 신뢰할 수 있는 제3자 C는 B의 비동형 암호화 공개키를 B에게 송신한다. 단계(S9), 참여자 A는 로컬 서브 모델 W-A(상수항 없음)를 초기화하고, 현재 손실 Loss-Last는 0으로 설정된다. 단계(S10), 참여자 B는 로컬 서브 모델 W-B(상수항 있음)를 초기화하고, 현재 손실 Loss-Last는 0으로 설정된다. 단계(S11), PSI 프로세스에 의해 샘플 집합의 교집합을 확정하여, 각 데이터베이스에서 이번 트레이닝에서 사용 되는 샘플 데이터를 결정한다. 단계(S12), 참여자 A는 A의 샘플 데이터 집합을 표준화하여, 각 특징 차원의 표준차 및 평균치를 획득한다. 단계(S13), 참여자 B는 B의 샘플 데이터 집합을 표준화하여, 각 특징 차원 및 태그의 표준차 및 평균치를 획득 한다. 단계(S14), 참여자 A는 신뢰할 수 있는 제3자 C가 송신한 비동형 암호화 공개키를 사용하여, 자신의 데이터 집 합의 각 특징 차원의 표준차 및 평균치를 암호화한 다음, 이를 신뢰할 수 있는 제3자 C에게 전송한다. 단계(S15), 참여자 B는 신뢰할 수 있는 제3자 C가 송신한 비동형 암호화 공개키를 사용하여, 자신의 데이터 집 합의 각 특징 차원 및 태그의 표준차 및 평균치를 암호화한 다음, 이를 신뢰할 수 있는 제3자 C에게 전송한다. 단계(S16), 신뢰할 수 있는 제3자 C는 수신한 암호화된 표준화 데이터를 TEE로 전송하고, 신뢰할 수 있는 실행 환경의 격리 영역에서 데이터를 복호화하고, 모든 특징 차원 및 태그의 표준차 및 평균치를 획득한다. 모델의 후속 비표준화를 위해 준비한다. 단계(S17), 참여자 A는 A의 원본 샘플 데이터를 기반으로, 서브 모델 W-a를 기반으로 계산을 수행하여, 파라미 터 집합 P-A1, 즉 구배와 손실을 계산하는 키 파라미터가 포함된 제1 트레이닝 중간 파라미터를 얻는다. 파라미터 집합 P-A1에 포함된 각 파라미터는 각 샘플 데이터가 모델 W-a를 기반으로 계산한 구배 중간 파라미터 및 손실 중간 파라미터이다. 단계(S18), 참여자 B는 B의 원본 샘플 데이터를 기반으로, 서브 모델 W-b를 기반으로 계산을 수행하여, 파라미 터 집합 P-B1, 즉 구배와 손실을 계산하는 키 파라미터가 포함된 제2 트레이닝 중간 파라미터를 얻는다. 파라미터 집합 P-B1에 포함된 각 파라미터는 각 샘플 데이터가 모델 W-b를 기반으로 계산한 구배 중간 파라미터 및 손실 중간 파라미터이다.단계(S19), 참여자 A는 파라미터 집합 P-A1의 부동 소수점 수를 정밀도(소수점 아래 몇 자릿수 유지)에 따라 큰 정수로 변환하여, 파라미터 집합 P-A2를 얻는다. 단계(S20), 참여자 B는 파라미터 집합 P-B1의 부동 소수점 수를 정밀도(소수점 아래 몇 자릿수 유지)에 따라 큰 정수로 변환하여, 파라미터 집합 P-B2를 얻는다. 단계(S21), 참여자 A는 파라미터 집합 P-A2의 정수 값을 인코딩하여, 음수를 양의 정수로 변환하고, 파라미터 집합 P-A3을 얻는다. 단계(S22), 참여자 B는 파라미터 집합 P-B2의 정수 값을 인코딩하여, 음수를 양의 정수로 변환하고 파라미터 집 합 P-B3을 얻는다. 단계(S23), 참여자 A는 파라미터 집합 P-A3의 모든 양의 정수를 A의 동형 공개키를 사용하여 암호화하여, 파라 미터 집합 P-A4-A를 얻는다. 단계(S24), 참여자 B는 파라미터 집합 P-B3의 모든 양의 정수를 B의 동형 공개키를 사용하여 암호화하여, 파라 미터 집합 P-B4-B, 즉 제2 동형 암호화 중간 파라미터를 얻는다. 단계(S25), 참여자 A는 파라미터 집합 P-A4-A를 참여자 B에게 송신한다. 단계(S26), 참여자 B는 파라미터 집합 P-B4-B를 참여자 A에게 송신한다. 단계(S27), 참여자 A는 참여자 B가 송신한 암호화 파라미터 집합 P-B4-B를 수신한다. 단계(S28), 참여자 B는 참여자 A가 송신한 암호화 파라미터 집합 P-A4-A를 수신한다. 단계(S29), 참여자 A는 파라미터 집합 P-A3의 모든 양의 정수를 참여자 B의 동형 공개키로 암호화하여, 파라미 터 집합 P-A4-B, 즉 제1 동형 암호화 중간 파라미터를 얻는다. 단계(S30), 참여자 B는 파라미터 집합 P-B3의 모든 양의 정수를 참여자 A의 동형 공개키로 암호화하여, 파라미 터 집합 P-B4-A를 얻는다. 단계(S31), 참여자 A는 난수 R-A1(즉, 제1 간섭 파라미터)을 생성하고, 참여자 B의 동형 공개키로 암호화한 후, R-A1-B 즉 제1 암호화 간섭 파라미터를 얻는다. 단계(S32), 참여자 B는 난수 R-B1을 생성하고, 참여자 A의 동형 공개키로 암호화한 후, R-B1-A를 얻는다. 단계(S33), 참여자 A는 동형 연산 함수 F1을 실행하고, P-A4-B, P-B4-B, R-A1-B를 기반으로, 파라미터 집합 G- A1-B, 즉 제1 암호화 구배 파라미터를 얻고; 동형 연산 함수 F1은 참여자 A의 구배 계산 함수에 해당한다. 단계(S34), 참여자 B는 동형 연산 함수 F2를 실행하고, P-B4-A, P-A4-A, R-B1-A를 기반으로, 파라미터 집합 G- B1-A를 얻고; 동형 연산 함수 F2는 참여자 B의 구배 계산 함수에 해당한다. 단계(S35), 참여자 A는 난수 R-A2를 생성하고, 참여자 B의 동형 공개키로 암호화한 후 R-A2-B를 얻는다. 단계(S36), 참여자 B는 난수 R-B2를 생성하고, 참여자 A의 동형 공개키로 암호화한 후 R-B2-A를 얻는다. 단계(S37), 참여자 A는 동형 연산 함수 F3을 실행하고, P-A4-B, P-B4-B 및 R-A2-B를 기반으로, 파라미터 집합 L-A1-B, 즉 제1 암호화 손실 파라미터를 얻고; 동형 연산 함수 F3은 참여자 A의 손실 함수에 해당한다. 단계(S38), 참여자 B는 동형 연산 함수 F4를 실행하고 P-B4-A, P-A4-A, R-B2-A 를 기반으로 파라미터 집합 L- B1-A를 얻고; 동형 연산 함수 F4는 참여자 B의 손실 함수에 해당한다. 단계(S39), 참여자 A는 암호화 파라미터 집합 G-A1-B 및 L-A1-B를 참여자 B에게 송신한다. 단계(S40), 참여자 B는 암호화 파라미터 집합 G-B1-A 및 L-B1-A를 참여자 A에게 송신한다. 단계(S41), 참여자 A는 참여자 B가 송신한 암호화 파라미터 집합 G-B1-A 및 L-B1-A를 수신한다. 단계(S42), 참여자 B는 참여자 A가 송신한 암호화 파라미터 집합 G-A1-B 및 L-A1-B를 수신한다. 단계(S43), 참여자 A는 A의 동형 개인키를 사용하여 G-B1-A 및 L-B1-A를 복호화하여, 파라미터 집합 G-B2 및 L- B2를 얻어 참여자 B에게 송신한다. 단계(S44), 참여자 B는 B의 동형 개인키를 사용하여 G-A1-B 및 L-A1-B를 복호화하여, 파라미터 집합 G-A2 및 L- A2, 즉 제1 키 파라미터를 얻어 참여자 A에게 송신한다. 단계(S45), 참여자 A는 참여자 B가 송신한 파라미터 집합 G-A2 및 L-A2를 수신한다. 단계(S46), 참여자 B는 참여자 A가 송신한 파라미터 집합 G-B2 및 L-B2를 수신한다. 단계(S47), 참여자 A는 파라미터 집합 L-A2를 순회하여, 난수 R-A2를 감하고 파라미터 집합 L-A3을 얻는다. 단계(S48), 참여자 B는 파라미터 집합 L-B2를 순회하여, 난수 R-B2를 감하고 파라미터 집합 L-B3을 얻는다. 단계(S49), 참여자 A는 파라미터 집합 L-A3의 수치를 디코딩하여 L-A4를 획득함으로써 실제 수치를 획득한다(양 수는 양수 또는 음수로 변환됨). 단계(S50), 참여자 B는 파라미터 집합 L-B3의 수치를 디코딩하여 L-B4를 획득함으로써 실제 수치를 획득한다(양 수는 양수 또는 음수로 변환됨). 단계(S51), 참여자 A는 정밀도에 따라 파라미터 집합 L-A4의 파라미터를 스케일링하여, 원래 정밀도로 복원하고, 정수를 부동 소수점 수로 변환하고, 파라미터 집합 L-A5를 얻는다. 단계(S52), 참여자 B는 정밀도에 따라 파라미터 집합 L-B4의 파라미터를 스케일링하여, 원래 정밀도로 복원하고, 정수를 부동 소수점 수로 변환하고, 파라미터 집합 L-B5를 얻는다. 단계(S53), 참여자 A는 파라미터 집합 L-A5를 통해 현재 서브 모델 W-a의 손실 Loss-Current를 계산해낸다. 단계(S54), 참여자 B는 파라미터 집합 L-B5를 통해 현재 서브 모델 W-b의 손실 Loss-Current를 계산해낸다. 단계(S55), 참여자 A는 현재 손실과 이전 손실의 차이, 즉 Loss-Current에서 Loss-Last를 감한 값을 계산한다. 손실 차이가 수렴 조건을 충족할 만큼 충분히 작은지 판단하여, 만족하는 경우, 모델의 트레이닝을 종료하고, 만족하지 않은 경우, 다음 단계를 계속한다. 단계(S56), 참여자 B는 현재 손실과 이전 손실의 차이를 계산하고. 손실 차이가 수렴 조건을 만족할 만큼 작은 지 판단하여, 만족하는 경우, 모델의 트레이닝을 종료하고, 만족하지 않은 경우, 다음 단계를 계속한다. 단계(S57), 만족하는 경우, 모델의 트레이닝을 종료하고 다음과 같은 단계를 진행한다. 단계(a), 모델의 트레이닝이 종료되고, 참여자 A는 서브 모델 W-A를 얻고, 참여자 B는 서브 모델 W-B를 얻는다. 단계(b), 참여자 A는 신뢰할 수 있는 제3자 C가 보낸 공개키를 사용하여 서브 모델 W-A를 암호화한 다음 신뢰할 수 있는 제3자 C에게 송신한다. 단계(c), 참여자 B는 신뢰할 수 있는 제3자 C가 보낸 공개키를 사용하여 서브 모델 W-B를 암호화한 다음 신뢰할 수 있는 제3자 C에게 송신한다. 단계(d), 신뢰할 수 있는 제3자 C는 암호화된 서브 모델 W-A와 W-B를 TEE로 전송하고, 신뢰할 수 있는 실행 환 경의 격리 영역에서 개인키를 사용하여 복호화한 다음 모델을 집합하여 완전한 모델 W를 얻는다. 단계(e), 신뢰할 수 있는 제3자 C는 TEE에서, 이전 단계의 각 특징 및 태그의 평균치 및 표준차를 사용하고, 완 전한 모델 W를 결합하여, 비표준화 작업을 수행하여, 실제 모델 W-REAL을 얻는다. 단계(f), 신뢰할 수 있는 제3자 C는 TEE에서, 모델 W-REAL을 W-A-REAL와 W-B-REAL로 분할한다. 단계(g), 신뢰할 수 있는 제3자 C는 TEE에서, 이전 단계에서 참여자 A가 전송한 비동형 공개키를 사용하여 모델 W-A-REAL을 암호화하고, 이를 참여자 A에게 전송한다. 단계(h), 신뢰할 수 있는 제3자 C는 TEE에서, 이전 단계에서 참여자 B가 전송한 비동형 공개키를 사용하여 모델 W-B-REAL을 암호화하고, 이를 참여자 B에게 전송한다. 단계(i), 참여자 A는 로컬 개인키를 사용하여 모델을 복호화하여 W-A-REAL을 얻는다. 단계(j) 참여자 B는 로컬 개인키를 사용하여 모델을 복호화하여 W-B-REAL을 얻는다. 단계(S58), 만족하지 않으면 모델의 트레이닝을 계속하고, 후속 단계를 실행한다. 단계(S59), 참여자 A는 파라미터 집합 G-A2를 순회하고 난수 R-A1을 감하여 파라미터 집합 G-A3을 얻는다. 단계(S60), 참여자 B는 파라미터 집합 G-B2를 순회하고 난수 R-B1을 감하여 파라미터 집합 G-B3을 얻는다. 단계(S61), 참여자 A는 파라미터 집합 G-A3의 수치를 복호화하여 G-A4를 얻음으로써 실제 수치를 얻는다(양수는 양수 또는 음수로 변환됨). 단계(S62), 참여자 B는 파라미터 집합 G-B3의 수치를 복호화하여 G-B4를 얻음으로써 실제 수치를 얻는다(양수는 양수 또는 음수로 변환됨). 단계(S63), 참여자 A는 정밀도에 따라 G-A4의 파라미터를 스케일링하여, 원래 정밀도로 복원하고, 정수를 부동 소수점 수로 변환하고, 파라미터 집합 G-A5를 얻는다. 단계(S64), 참여자 B는 정밀도에 따라 G-B4의 파라미터를스케일링하여, 원래 정밀도로 복원하고, 정수를 부동 소수점 수로 변환하고, 파라미터 집합 G-B5를 얻는다. 단계(S65), 참여자 A는 파라미터 집합 G-A5를 통해 로컬 모델를 업데이트하는데 사용되는 구배 Grad-A를 계산하 여 얻는다. 단계(S66), 참여자 B는 파라미터 집합 G-B5를 통해 로컬 모델를 업데이트하는데 사용되는 구배 Grad-B를 계산하 여 얻는다. 단계(S67), 참여자 A는 구배 Grad-A를 사용하여 서브 모델 W-A를 업데이트한다. 단계(S68), 참여자 B는 구배 Grad-B를 사용하여 서브 모델 W-B를 업데이트한다. 단계(S69), 참여자 A는 현재 서브 모델의 손실 Loss-Current를 Loss-Last에 할당한다. 단계(S70), 참여자 B는 현재 서브 모델의 손실 Loss-Current를 Loss-Last에 할당한다. 단계(S71), 참여자 A는 단계를 반복한다. 단계(S72), 참여자 B는 단계를 반복한다. 본 실시예에서, 각 참여자는 서브 모델의 트레이닝 프로세스의 프라이버시 데이터를 효과적으로 보호할 수 있어 서, 모델의 트레이닝 프로세스의 프라이버시 및 트레이닝 효율을 향상시킨다. 참여자의 수가 더 많을 경우, 예를 들어, 참여자 D를 더 포함하는 경우, 본 실시예의 방법은: 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하는 단계, 여기서, 상기 제1 동형 암 호화 중간 파라미터는 각 제2 참여자 B 및 D의 자신의 제2 동형 공개키를 사용하여 제1 트레이닝 중간 파라미터 를 암호화하는데 사용되는 데이터이고, 상기 제1 트레이닝 중간 파라미터는 제1 참여자가 제1 샘플 데이터를 기 반으로 제1 서브 모델을 트레이닝한 후 생성된 중간 파라미터이고, 상기 제2 동형 암호화 중간 파라미터는 여러 개 있으며, 각 제2 참여자가 자신의 제2 동형 공개키 쌍으로 각 제2 트레이닝 중간 파라미터를 암호화하는 데이 터이고, 각 상기 제2 트레이닝 중간 파라미터는 각 제2 참여자가 자신의 제2 샘플 데이터를 기반으로 제2 서브 모델을 트레이닝한 후 생성된 중간 파라미터이며; 상기 기계 학습 모델은 적어도 두 개 이상의 참여자 자신의 로컬 서브 모델로 구성되고; 즉 참여자 B는 트레이닝하여 하나의 제2 트레이닝 중간 파라미터를 얻고, 참여자 D 는 트레이닝하여 다른 하나의 제2 트레이닝 중간 파라미터를 얻으며; 각 제2 트레이닝 중간 파라미터는 참여자 B와 D의 동형 암호화 공개키를 동시에 사용하여 암호화하여, 2개의 제2 동형 암호화 중간 파라미터를 형성하는 단계; 제1 간섭 파라미터를 생성하고, 각 제2 참여자의 자신의 제2 동형 공개키로 동시에 암호화하여, 제1 암호화 간 섭 파라미터를 형성하는 단계; 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터와 제1 암호화 간섭 파라미터, 및 제1 서 브 모델의 동형 계산 함수를 기반으로 계산하여 제1 암호화 키 파라미터를 생성하는 단계; 상기 제2 참여자가 제2 동형 개인키를 사용하여 상기 제1 암호화 키 파라미터를 복호화할 수 있도록 상기 제1 암호화 키 파라미터를 제2 참여자에게 전송하는 단계; 상기 제2 참여자의 복호화된 제1 키 파라미터를 획득하는 단계; 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상기 제1 서브 모델의 트레이닝이 완료될 때까지 상 기 제1 서브 모델을 반복적으로 업데이트하는 단계; 를 포함한다. 도 4는 본 개시의 실시예에서 제공하는 분산형 기계 학습 모델의 트레이닝 장치의 개략도이다. 본 실시예는 프 라이버시 보호가 요구되는 다자간의 데이터 인터랙션에서 기계 학습 모델에 대한 트레이닝을 수행하는 경우에 적용될 수 있다. 해당 장치는 전자 장치에 구성되고, 본 개시의 임의의 실시예에서 설명된 분산형 기계 학습 모 델의 트레이닝 방법을 구현할 수 있다. 도 4에 도시된 바와 같이, 해당 분산형 기계 학습 모델의 트레이닝 장치 는 구체적으로 다음을 포함한다: 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하는 데 사용되는 중간 파라미터 획득 모듈; 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여, 제1 암호화 간섭 파라미터를 형성 하도록 구성된 간섭 파라미터 형성 모듈; 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 간섭 파라미터, 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하여, 제1 암호화 키 파라미터를 생성하도록 구성된 파라미터 생성 모듈 ; 상기 제2 참여자가 제2 동형 개인키를 사용하여 상기 제1 암호화 키 파라미터를 복호화할 수 있도록, 상기 제1 암호화 키 파라미터를 제2 참여자에게 전송하도록 구성된 파라미터 복호화 모듈; 상기 제2 참여자의 복호화된 제1 키 파라미터를 획득하도록 구성된 제1 키 파라미터 획득 모듈; 상기 제1 서브 모델의 트레이닝이 완료될 때까지, 상기 제1 키 파라미터 및 상기 제1 간섭 파라미터에 따라, 상 기 제1 서브 모델을 반복적으로 업데이트하도록 구성된 서브 모델 트레이닝 모듈; 을 포함한다. 본 개시의 실시예에서, 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터가 획득함으로써 제1 간섭 파라미터를 생성하고, 제2 참여자의 제2 동형 공개키로 암호화하여, 제1 암호화 간섭 파라미터를 형성하고; 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터 및 제1 암호화 간섭 파라미터에 기 반으로, 제1 서브 모델의 동형 계산 함수에 기반으로 계산하여, 제1 암호화 키 파라미터를 생성하고; 제1 암호 화 키 파라미터를 제2 참여자에게 전송하여, 제2 참여자가 제2 동형 개인키를 사용하여 제1 암호화 키 파라미터 를 복호화하여; 제2 참여자의 복호화된 제1 키 파라미터를 획득하고; 제1 서브 모델의 트레이닝이 완료될 때까 지, 제1 키 파라미터 및 제1 간섭 파라미터에 따라, 제1 서브 모델을 반복적으로 업데이트한다. 상기 방안은 다 자간 트레이닝 모델의 프라이버시 데이터 보호를 구현함과 동시에 신뢰할 수 있는 제3자를 사용하여 조정할 필 요가 없으므로, 신뢰할 수 있는 제3자가 데이터에 대한 악의적으로 유출하는 가능성을 방지할 수 있으며, 대량 의 데이터가 여러 참여자에서 인터랙션하는 것을 방지하여 모델의 트레이닝 프로세스의 데이터 전송량을 절감하 여, 모델의 트레이닝의 효율성을 향상시킨다. 일 선택적인 실시방식에서, 상기 제1 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 이용하여 제1 트레이닝 중간 파라미터를 암호화한 데이터이고, 상기 제1 트레이닝 중간 파라미터는 제1 참여자가 제1 샘 플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후 생성되는 중간 파라미터이며, 상기 제2 동형 암호화 중간 파라미터는 제2 참여자의 제2 동형 공개키를 사용하여 제2 트레이닝 중간 파라미터를 암호화한 데이터이며, 상 기 제2 트레이닝 중간 파라미터는 제2 참여자가 제2 샘플 데이터를 기반으로 제2 서브 모델을 트레이닝한 후 생 성되는 중간 파라미터이고, 상기 기계 학습 모델은 최소 2개의 참여자 각각의 로컬 서브 모델로 구성된다. 일 선택적인 실시방식에서, 상기 파라미터 생성 모듈은: 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 및 제1 서브 모델의 동형 계산 함수를 기반으로 계산하고, 상기 제1 암호화 간섭 파라미터를 선형 계산에 기반하여 계산 결과에 추가하여, 제1 암호화 키 파라미터를 생성하는데 사용되는 파라미터 생성 유닛; 을 포함하고, 상응하게, 상기 서브 모델 트레이닝 모듈은, 상기 제1 간섭 파라미터를 기반으로 상기 제1 키 파라미터에 대해 역으로 선형 계산을 하고, 상기 제1 서브 모 델의 트레이닝이 완료될 때까지, 역으로 선형 계산 후의 키 파라미터에 따라, 상기 제1 서브 모델을 반복적으로 업데이트하는 서브 모델 트레이닝 유닛; 을 포함한다. 일 선택적인 실시방식에서, 상기 간섭 파라미터는 난수이다. 일 선택적인 실시방식에서, 상기 제1 간섭 파라미터는 제1 구배 간섭 파라미터와 제1 손실 간섭 파라미터를 포 함하고;상응하게, 상기 파라미터 생성 모듈은: 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 구배 간섭 파라미터 및 제1 서브 모델의 동형 구배 계산 함수를 기반으로 계산하여 제1 암호화 구배 파라미터를 생성하는데 사용되는 구배 파라미터 생성 모듈; 상기 제1 동형 암호화 중간 파라미터, 제2 동형 암호화 중간 파라미터, 제1 암호화 손실 간섭 파라미터 및 제1 서브 모델의 동형 손실 계산 함수를 기반으로 계산하여 제1 암호화 손실 파라미터를 생성하는데 사용되는 손실 파라미터 생성 모듈; 를 포함한다. 일 선택적인 실시방식에서, 상기 장치는: 제1 동형 키 쌍을 생성하도록 구성된 제1 동형 키 쌍 생성 모듈-상기 제1 동형 키 쌍은 제1 동형 공개키 및 제1 동형 개인키를 포함함-; 상기 제1 동형 공개키를 제2 참여자에게 송신하도록 구성되는 제1 동형 공개키 송신 모듈; 상기 제2 참여자가 생성하고 상기 제2 참여자가 송신한 제2 동형 키 쌍의 제2 동형 공개키를 수신하도록 구성되 는 제2 동형 공개키 수신 모듈; 을 더 포함한다. 일 선택적인 실시예에서, 상기 장치는: 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하기 전에, 상기 제1 참여자와 상기 제2 참여자의 샘플 데이터의 교집합 식별자를 확정하는데 사용되는 교집합 식별자 확정 모듈; 을 더 포함하고, 적어도 하나의 참여자의 샘플 데이터는 대응하게 태그가 표기된다. 일 선택적인 실시예에서, 상기 장치는: 제1 동형 암호화 중간 파라미터 및 제2 동형 암호화 중간 파라미터를 획득하기 전에, 상기 제1 참여자의 제1 원 본 샘플 데이터를 표준화 처리하여, 샘플 특징 차원의 제1 표준화 값 및 제1 샘플 데이터를 획득하는데 사용되 는 샘플 데이터 처리 모듈-표준화된 제1 샘플 데이터는 모델을 트레이닝하는데 사용됨-; 제3 암호키로 상기 제1 표준화 값을 암호화하여, 제1 암호화된 표준화 값을 형성하고, 상기 제1 암호화된 표준 화 값을 신뢰할 수 있는 제3자에게 제공하는데 사용되는 제1 표준화 값 암호화 모듈; 을 더 포함하고, 상응하게, 해당 장치는: 상기 제1 서브 모델의 트레이닝이 완료된 후, 상기 제1 서브 모델을 제3 암호키로 암호화하여, 제1 암호화 서브 모델을 형성하고, 상기 제1 암호화 서브 모델을 상기 신뢰할 수 있는 제3자에게 제공하여, 상기 신뢰할 수 있는 제3자가 각 참여자가 제공한 암호화 서브 모델을 기반으로 결합하고, 각 참여자가 제공한 암호화된 표준화 값을 기반으로 비표준화 처리를 한 다음 분할하는데 사용되는 모델 분할 모듈; 상기 신뢰할 수 있는 제3자가 리턴한 분할된 제1 비표준화 서브 모델을 수신하는데 사용되는 서브 모델 수신 모 듈; 를 더 포함한다. 일 선택적인 실시방식에서, 상기 중간 파라미터 획득 모듈은: 제1 샘플 데이터를 기반으로 제1 서브 모델을 트레이닝한 후, 제1 트레이닝 중간 파라미터를 생성하는데 사용되 는 제1 트레이닝 중간 파라미터 생성 유닛; 설정된 정밀도에 따라 상기 제1 트레이닝 중간 파라미터의 부동 소수점 수를 제1 큰 정수로 변환하는데 사용되 는 제1 큰 정수 확정 유닛; 상기 제1 큰 정수의 정수 값을 인코딩하여 제1 양의 정수로 변환하는데 사용되는 제1 양의 정수 확정 유닛; 제2 참여자의 제2 동형 공개키를 사용하여 제1 양의 정수를 암호화하여 상기 제1 동형 암호화 중간 파라미터를 얻는데 사용되는 제1 동형 암호화 중간 파라미터 확정 유닛; 을 포함한다. 일 선택적인 실시방식에서, 상기 서브 모델 트레이닝 모듈은: 상기 제1 간섭 파라미터를 기반으로 상기 제1 키 파라미터에 대해 간섭 제거 동작을 수행하는데 사용되는 간섭 제거 동작 유닛;간섭 제거된 후의 제1 키 파라미터를 디코딩하고, 상기 설정된 정밀도에 따라 스케일링하여, 부동 소수점 수로 변환하는데 사용되는 부동 소수점 수 변환 유닛; 변환된 부동 소수점 수 형태의 제1 키 파라미터를 사용하여 상기 제1 서브 모델을 반복적으로 업데이트하는데 사용되는 반복 업데이트 유닛; 을 포함한다. 본 개시의 실시예의 기술방안에 따라 제공하는 분산형 기계 학습 모델의 트레이닝 장치는 본 개시의 임의의 실 시예에서 제공하는 분산형 기계 학습 모델의 트레이닝 방법을 구현할 수 있고, 분산형 기계 학습 모델의 트레이 닝 방법을 실행하기 위한 상응한 기능 모듈 및 유리한 효과를 구비한다. 본 개시의 기술방안에서, 이와 관련된 사용자의 개인정보의 취득, 저장 및 응용 등은 모두 관련 법령의 규정을 준수하며, 공서양속에 위반하지 않는다. 본 개시의 실시예에 따르면, 본 개시는 전자 설비, 판독 가능한 저장 매체, 및 컴퓨터 프로그램 제품을 더 제공 한다. 도 5는 본 개시의 실시예를 구현하는데 사용되는 전자 설비의 예시적인 블록도이다. 전자 설비는 다양한 형태의 디지털 컴퓨터, 예를 들어, 랩톱 컴퓨터, 데스크톱 컴퓨터, 워크테이블, 개인용 정보 단말기, 서버, 블 레이드 서버, 대형 컴퓨터 및 기타 적합한 컴퓨터를 의미한다. 전자 설비는 다양한 형태의 모바일 장치, 예를 들어, 개인 디지털 처리, 휴대 전화기, 스마트폰, 웨어러블 설비 및 기타 유사한 컴퓨팅 장치를 나타낼 수도 있 다. 본 문에서 설명된 컴포넌트, 이들의 연결과 관계 및 이들의 기능은 단지 예시적인 것일 뿐, 본 문에서 기술 및/또는 요구한 본 개시의 실현을 제한하려는 의도가 아니다. 도 5에 도시된 바와 같이, 설비는 컴퓨팅 유닛을 포함하되, 이는 판독 전용 메모리(ROM)에 저장 된 컴퓨터 프로그램 또는 저장 유닛으로부터 랜덤 액세스 메모리(RAM)에 로딩되는 컴퓨터 프로그램에 따라, 각종 적합한 동작 및 처리를 수행할 수 있다. RAM에는 설비의 조작에 필요한 다양한 프로그램 및 데이터가 저장될 수도 있다. 컴퓨팅 유닛, ROM 및 RAM은 버스를 통해 서로 연결된다. 입력/출력(I/O) 인터페이스도 버스에 연결된다. 설비 중의 여러 개의 컴포넌트는 I/O 인터페이스에 연결되고, 해당 컴포넌트는 예를 들어 키패드, 마 우스 등과 같은 입력 유닛; 예를 들어 다양한 유형의 표시장치, 스피커 등과 같은 출력 유닛; 예를 들어 자기디스크, 광디스크 등과 같은 저장 유닛; 및 예를 들어 네트워크 카드, 모뎀, 무선통신 트랜시버 등과 같은 통신 유닛 등을 포함한다. 통신 유닛은 설비가 인터넷과 같은 컴퓨터 네트워크 및/또 는 다양한 전기 통신망을 통해 기타 설비와 정보/데이터를 교환하는 것을 허용한다. 컴퓨팅 유닛은 처리 및 컴퓨팅 능력을 갖는 다양한 범용 및/또는 전용 처리 컴포넌트일 수 있다. 컴퓨팅 유닛의 일부 예시는 중앙 처리 유닛(CPU), 그래픽 처리 유닛(GPU), 다양한 전용 인공 지능(AI) 컴퓨팅 칩, 기계 러닝 모델 알고리즘을 수행하는 다양한 컴퓨팅 유닛, 디지털 신호 프로세서(DSP) 및 임의의 적합한 프로세 서, 컨트롤러, 마이크로 컨트롤러 등을 포함하지만, 이에 한정되지 않는다. 컴퓨팅 유닛은 위에서 설명한 각각의 방법 및 처리를 수행하고, 예를 들어, 분산형 기계 학습 모델의 트레이닝 방법을 수행한다. 예를 들어, 일부 실시예에서, 분산형 기계 학습 모델의 트레이닝 방법은 컴퓨터 소프트웨어 프로그램으로 구현될 수 있고, 이는 저장 유닛과 같은 기계 판독 가능 매체에 유형적으로 포함된다. 일부 실시예에서, 컴퓨터 프로그램의 일부 또는 전체는 ROM 및/또는 통신 유닛에 의해 설비에 로딩 및/또는 장착될 수 있다. 컴퓨터 프로그램이 RAM에 로딩되어 컴퓨팅 유닛에 의해 실행되는 경우, 위에서 설명한 분산형 기계 학습 모 델의 트레이닝 방법의 하나 이상의 단계를 수행할 수 있다. 대안적으로, 기타 실시예에서, 컴퓨팅 유닛은 기타 임의의 적합한 방식(예를 들어, 펌웨어를 통해)을 통해 분산형 기계 학습 모델의 트레이닝 방법을 수행하 도록 구성될 수 있다. 본 문에서 상술한 시스템 및 기술의 다양한 실시형태는 디지털 전자 회로 시스템, 집적 회로 시스템, 필드 프로 그래머블 게이트 어레이(FPGA), 주문형 집적 회로(ASIC), 특정 용도 표준 제품(ASSP), 시스템온칩(SOC), 복합 프로그래밍 로직 설비(CPLD), 컴퓨터 하드웨어, 펌웨어, 소프트웨어 및/또는 이들의 조합으로 구현될 수 있다. 이러한 각종 실시형태는 하나 이상의 컴퓨터 프로그램에서 실시되는 것을 포함할 수 있고, 해당 하나 이상의 컴 퓨터 프로그램은 적어도 하나의 프로그램 가능한 프로세서를 포함하는 프로그램 가능한 시스템에서 실행 및/또 는 해석(interpretating)될 수 있으며, 해당 프로그램 가능한 프로세서는 전용 또는 범용 프로그램 가능한 프로 세서일 수 있고, 저장 시스템, 적어도 하나의 입력 장치 및 적어도 하나의 출력 장치로부터 데이터 및 명령을 수신할 수 있으며, 데이터 및 명령을 해당 저장 시스템, 해당 적어도 하나의 입력 장치 및 해당 적어도 하나의출력 장치로 전송한다. 본 개시의 방법을 실시하기 위한 프로그램 코드는 하나 이상의 프로그래밍 언어의 임의의 조합을 사용하여 작성 될 수 있다. 이러한 프로그램 코드는 범용 컴퓨터, 전용 컴퓨터 또는 기타 프로그램 가능한 데이터 처리 장치의 프로세서 또는 컨트롤러에 제공되어, 프로그램 코드가 프로세서 또는 컨트롤러에 의해 실행되면, 흐름도 및/또 는 블록도에 규정된 기능/조작이 실시될 수 있도록 한다. 프로그램 코드는 전체가 기계에서 실행되거나, 일부가 기계에서 실행되고, 독립적인 소프트웨어 패키지로서 일부가 기계에서 실행되고 일부가 원격 기계에서 실행되거 나, 전부가 원격 기계 또는 서버에서 실행될 수 있다. 본 개시의 전문에서, 기계 판독 가능 매체는 유형 매체(tangible medium)일 수 있고, 이는 명령 실행 시스템, 장치 또는 설비에 의해 사용되거나, 명령 실행 시스템, 장치 또는 설비와 결합하여 사용되는 프로그램을 포함하 거나 저장할 수 있다. 기계 판독 가능 매체는 기계 판독 가능 신호 매체 또는 기계 판독 가능 저장 매체일 수 있다. 기계 판독 가능 매체는 전자, 자기, 광학, 전자기, 적외선 또는 반도체 시스템, 장치 또는 설비, 또는 상 기 내용의 임의의 적합한 조합을 포함할 수 있지만 이에 한정되지 않는다. 기계 판독 가능 저장 매체의 보다 구 체적인 예시는 하나 이상의 와이어에 기반한 전기적 연결, 휴대용 컴퓨터 디스크, 하드디스크, 랜덤 액세스 메 모리(RAM), 판독 전용 메모리(ROM), 소거 및 프로그램 가능한 판독 전용 메모리(EPROM 또는 플래시 메모리), 광 섬유, 휴대용 콤팩트 디스크 판독 전용 메모리(CD-ROM), 광학 저장 설비, 자기 저장 설비 또는 상기 내용의 임 의의 적합한 조합을 포함한다. 사용자와의 인터랙션을 제공하기 위해, 여기서 설명된 시스템 및 기술을 컴퓨터에서 실시할 수 있고, 해당 컴퓨 터는 사용자에게 정보를 표시하기 위한 표시장치(예를 들어, CRT(음극선관) 또는 LCD(액정 표시장치)모니터), 키보드 및 방향지시 장치(예를 들어, 마우스 또는 트랙볼)를 구비하며, 사용자는 해당 키보드 및 해당 방향지시 장치를 통해 컴퓨터에 입력을 제공할 수 있다. 기타 유형의 장치는 사용자와의 인터랙션을 제공할 수도 있고, 예를 들어, 사용자에게 제공되는 피드백은 임의의 형태의 센싱 피드백(예를 들어, 시각 피드백, 청각 피드백 또 는 촉각 피드백)일 수 있으며, 임의의 형태(사운드 입력, 음성 입력 또는 촉각 입력)로 사용자로부터의 입력을 수신할 수 있다. 여기서 설명된 시스템 및 기술은 백엔드 컴포넌트를 포함하는 컴퓨팅 시스템(예를 들어, 데이터 서버), 또는 미 들웨어 컴포넌트를 포함하는 컴퓨팅 시스템(예를 들어, 애플리케이션 서버), 또는 프런트엔드 컴포넌트를 포함 하는 컴퓨팅 시스템(예를 들어, 그래픽 사용자 인터페이스 또는 웹브라우저를 구비하는 사용자 컴퓨터, 사용자 는 해당 그래픽 사용자 인터페이스 또는 해당 웹브라우저를 통해 여기서 설명된 시스템 및 기술의 실시형태와 인터랙션할 수 있음), 또는 이러한 백엔드 컴포넌트, 미들웨어 컴포넌트, 또는 프런트엔드 컴포넌트를 포함하는 임의의 조합의 컴퓨팅 시스템에서 실시될 수 있다. 임의의 형태 또는 매체의 디지털 데이터 통신(예를 들어, 통 신 네트워크)으로 시스템의 컴포넌트를 서로 연결할 수 있다. 통신 네트워크의 예시는 근거리 통신망(LAN), 광 역 네트워크(WAN), 블록체인 네트워크 및 인터넷을 포함한다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트 및 서버는 일반적으로 서로 멀리 떨어져 있 으며, 통상적으로 통신 네트워크를 통해 인터랙션을 수행한다. 클라이언트와 서버의 관계는 상응하는 컴퓨터에 서 작동되고 서로 클라이언트-서버 관계를 갖는 컴퓨터 프로그램에 의해 발생된다. 상술한 각종 형태의 프로세스를 사용하여, 단계의 순서재배정, 추가 또는 삭제를 수행할 수 있음을 이해해야 한 다. 예를 들어, 본 개시에 기재된 각 단계는 병렬로 수행될 수 있거나 순차적으로 수행될 수도 있거나 서로 다 른 순서로 수행될 수도 있으며, 본 개시에서 개시한 기술방안이 희망하는 결과를 달성하기만 하면 되기 때문에, 본 문에서는 이에 대해 한정하지 않는다. 상기 구체적인 실시형태는 본 개시의 보호 범위를 한정하지 않는다. 본 분야의 당업자는 설계 요구 및 기타 요 소에 따라 다양한 수정, 조합, 부분 조합 및 대체가 가능함을 이해할 수 있을 것이다. 본 개시의 사상 및 원칙 내에서 이루어진 수정, 등가적 대체 및 개선 등은 모두 본 개시의 보호 범위 내에 포함되어야 한다.도면 도면1 도면2 도면3 도면4 도면5"}
{"patent_id": "10-2022-0114224", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도면은 본 방안을 보다 잘 이해하기 위한 것이고, 본 개시를 한정하지 않는다. 여기서: 도 1은 본 개시의 실시예에서 제공하는 분산형 기계 학습 모델의 트레이닝 방법의 개략도이다. 도 2는 본 개시의 실시예에서 제공하는 다른 분산형 기계 학습 모델의 트레이닝 방법의 개략도이다. 도 3은 본 개시의 실시예에서 제공하는 또 다른 분산형 기계 학습 모델의 트레이닝 방법의 개략도이다. 도 4는 본 개시의 실시예에서 제공하는 분산형 기계 학습 모델의 트레이닝 장치의 개략도이다. 도 5는 본 개시의 실시예의 분산형 기계 학습 모델의 트레이닝 방법의 전자 설비의 블록도이다."}
