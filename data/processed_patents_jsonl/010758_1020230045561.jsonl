{"patent_id": "10-2023-0045561", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0149684", "출원번호": "10-2023-0045561", "발명의 명칭": "인공 신경망 기반의 질의 처리 장치 및 방법", "출원인": "테크온미디어", "발명자": "김현태"}}
{"patent_id": "10-2023-0045561", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공 지능 기반의 질의 처리 장치로서, 각 분야 별로 배경 지식을 획득하고, 상기 획득한 배경 지식을 전처리 하여 기 설정된 기준 길이의 문장들을 생성하는 전처리 모듈; 및제1 언어 처리 모델을 포함하고, 상기 기준 길이의 문장을 복수 개의 청크 단위로 분할하여 상기 제1 언어 처리모델로 입력하며, 상기 제1 언어 처리 모델을 통해 각 청크 단위에 대해 문장 임베딩 벡터를 생성하고, 생성한상기 문장 임베딩 벡터를 색인하여 신경망 데이터베이스에 저장하는 제1 언어 처리 모듈을 포함하며, 상기 제1 언어 처리 모듈은,질의가 입력되는 경우, 상기 질의를 상기 제1 언어 처리 모델에 입력하여 질의에 대한 문장 임베딩 벡터를 생성하고, 상기 질의에 대한 문장 임베딩 벡터를 포함하는 조회 요청을 상기 신경망 데이터베이스로 전달하여 상기질의에 대한 하나 이상의 검색 결과를 획득하는, 질의 처리 장치."}
{"patent_id": "10-2023-0045561", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서, 상기 제1 언어 처리 모듈은, 상기 문장 임베딩 벡터를 키-값(key-value) 형태로 색인화 하여 상기 신경망 데이터베이스에 저장하고, 상기 키(key)는 해당 문장 임베딩 벡터이고, 상기 값(value)은 상기 문장 임베딩 벡터와 관련된 텍스트로서 제1값 및 제2 값을 포함하며, 상기 제1 값은 상기 문장 임베딩 벡터에 대응하는 청크 단위의 기준 길이 문장이고, 상기 제2 값은 상기 기준길이 문장에서 상기 제1 값의 청크 단위에 연속되는 다음 청크 단위의 문장인, 질의 처리 장치."}
{"patent_id": "10-2023-0045561", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서, 상기 신경망 데이터베이스는, 기 저장된 키들 중 상기 질의에 대한 문장 임베딩 벡터 간의 유사도에 기초하여 하나 이상의 키를 확인하고, 확인된 키와 매칭되는 제1 값 및 제2 값을 검색 결과로서 추출하는, 질의 처리 장치."}
{"patent_id": "10-2023-0045561", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "에 있어서, 상기 제2 언어 처리 모델은, 인코더 및 디코더를 포함하고, 상기 제2 언어 처리 모듈은, 상기 질의를 토큰 단위로 임베딩하여 질의 임베딩 시퀀스를 생성하고, 상기 검색 결과를 토큰 단위로 임베딩하여 검색 결과 임베딩 시퀀스를 생성하며, 상기 검색 결과 임베딩 시퀀스를 상기 인코더로 입력하고 상기 질의임베딩 시퀀스를 상기 디코더로 입력하는, 질의 처리 장치."}
{"patent_id": "10-2023-0045561", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 5에 있어서, 상기 인코더는, 복수 개의 인코더 블록이 적층된 형태로 마련되고, 상기 인코더 블록은, 상기 검색 결과 임베딩 시퀀스에 대해 셀프 어텐션을 수행하여 검색 결과 셀프 어텐션 시퀀스를 생성하며, 상기 인코더는, 상기 검색 결과 셀프 어텐션 시퀀스를 상기 디코더로 전달하는, 질의 처리 장치."}
{"patent_id": "10-2023-0045561", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서, 상기 디코더는, 복수 개의 단위 디코더 블록이 적층된 형태로 마련되고, 상기 단위 디코더 블록은, 제1 타입의 디코더 블록 및 제2 타입의 디코더 블록을 포함하며, 상기 제1 타입의 디코더 블록은, 상기 질의 임베딩 시퀀스에 대해 셀프 어텐션을 수행하도록 마련되고, 상기 제2 타입의 디코더 블록은, 상기 질의 임베딩 시퀀스에 대해 셀프 어텐션을 수행하고, 상기 질의 임베딩시퀀스와 상기 검색 결과 셀프 어텐션 시퀀스 간의 크로스 어텐션을 수행하도록 마련되는, 질의 처리 장치."}
{"patent_id": "10-2023-0045561", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 5에 있어서, 상기 인코더는, 상기 검색 결과 임베딩 시퀀스로부터 검색 결과 셀프 어텐션 시퀀스를 생성하여 상기 디코더로전달하고, 상기 디코더는, 상기 질의 임베딩 시퀀스 및 상기 검색 결과 셀프 어텐션 시퀀스에 기초하여 상기 질의에 대한 답변을추론하는, 질의 처리 장치."}
{"patent_id": "10-2023-0045561", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "에 있어서, 상기 제2 언어 처리 모듈은, 상기 신경망 데이터베이스로부터 상기 추가 조회 요청에 대응하여 상기 추가 조회 요청의 문장 임베딩 벡터와가장 유사한 문장 임베딩 벡터를 갖는 키와 매칭되는 제1 값과 제2 값을 추가 검색 결과로 수신하는, 질의 처리장치."}
{"patent_id": "10-2023-0045561", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "청구항 10에 있어서, 상기 제2 언어 처리 모듈은, 상기 추가 검색 결과를 토큰 단위로 임베딩하여 추가 검색 결과 임베딩 시퀀스를 생성하고, 상기 질의에 대한 추론된 답변을 토큰 단위로 임베딩하여 추론된 답변 임베딩 시퀀스를 생성하며, 상기 추가 검색 결과 임베딩 시퀀스를 상기 인코더로 입력하고, 상기 추론된 답변 임베딩 시퀀스를 상기 디코더로 입력하여 상기 질의에 대한 최종 답변을 추론하도록 하는, 질의 처리 장치."}
{"patent_id": "10-2023-0045561", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "전처리 모듈에서, 각 분야 별로 배경 지식을 획득하고, 상기 획득한 배경 지식을 전처리 하여 기 설정된 기준길이의 문장들을 생성하는 단계; 제1 언어 처리 모듈에서, 상기 기준 길이의 문장을 복수 개의 청크 단위로 분할하여 제1 언어 처리 모델로 입력하며, 상기 제1 언어 처리 모델을 통해 각 청크 단위에 대해 문장 임베딩 벡터를 생성하고, 생성한 상기 문장임베딩 벡터를 색인하여 신경망 데이터베이스에 저장하는 단계; 및상기 제1 언어 처리 모듈에서, 질의가 입력되는 경우, 상기 질의를 상기 제1 언어 처리 모델에 입력하여 질의에대한 문장 임베딩 벡터를 생성하고, 상기 질의에 대한 문장 임베딩 벡터를 포함하는 조회 요청을 상기 신경망데이터베이스로 전달하여 상기 질의에 대한 하나 이상의 검색 결과를 획득하는 단계를 포함하는, 질의 처리 방법."}
{"patent_id": "10-2023-0045561", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공 신경망 기반의 질의 처리 장치 및 방법이 개시된다. 개시되는 일 실시예에 따른 질의 처리 장치는, 각 분야 별로 배경 지식을 획득하고, 획득한 배경 지식을 전처리 하여 기 설정된 기준 길이의 문장들을 생성하는 전처리 모듈 및 제1 언어 처리 모델을 포함하고, 기준 길이의 문장을 복수 개의 청크 단위로 분할하여 제1 언어 처리 모 델로 입력하며, 제1 언어 처리 모델을 통해 각 청크 단위에 대해 문장 임베딩 벡터를 생성하고, 생성한 문장 임 베딩 벡터를 색인하여 신경망 데이터베이스에 저장하는 제1 언어 처리 모듈을 포함하며, 제1 언어 처리 모듈은, 질의가 입력되는 경우, 질의를 제1 언어 처리 모델에 입력하여 질의에 대한 문장 임베딩 벡터를 생성하고, 질의 에 대한 문장 임베딩 벡터를 포함하는 조회 요청을 신경망 데이터베이스로 전달하여 질의에 대한 하나 이상의 검 색 결과를 획득한다."}
{"patent_id": "10-2023-0045561", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명의 실시예는 인공 신경망 기반의 질의 처리 기술과 관련된다."}
{"patent_id": "10-2023-0045561", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근, 인공 지능 기술의 발달로 언어를 처리하고 생성하는 대규모 언어 모델(Large Language Model)이 등장하였 다. 일 예로, 트랜스포머(Transformer) 기반의 모델은 기계 번역 분야에서 기존 모델의 성능을 훨씬 능가하였고, 구글의 BERT(Bidirectional Encoder Representations from Transformers)는 사전 훈련(Pre- training) 후 미세 조정(finetunning) 프로세스를 이용하여 트랜스포머 기반의 상황 별 단어 임베딩 기법을 제 안하였다. 그러나, 구글의 BERT 이후 자연어 처리 모델은 태스크와 무관한 표현(representation)을 학습 하는 방향으로 발 전하고 있으며 자연어 처리 모델의 크기를 키우는데 목표를 두고 있다. 그로 인해, 각 모델 별 파라미터 수가 1억개(GPT-1)에서 1,750억개(GPT-3)까지 증가하였으며, 결과적으로 대규모 언어 모델은 대규모 인력, 장비, 및 비용 투자가 가능한 글로벌 기업만이 진행할 수 있게 되었다. 또한, 기존의 트랜스포머 기반의 언어 모델들은 학습 데이터를 통해 지식 정보를 언어 모델 내 파라미터에 저장 하는 구조인데, 실제 추론 시(즉, 태스크 적용 시) 언어 모델을 GPU 기반 메모리에 저장하고 있어야 하며, 언어 모델의 크기가 증가할수록 응답 속도가 느려진다는 문제점이 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국공개특허공보 제10-2022-0154935호(2022.11.22)"}
{"patent_id": "10-2023-0045561", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 실시예는 질의에 대해 처리하는 언어 모델을 경량화 할 수 있는 인공 신경망 기반의 질의 처리 장치 및 방법을 제공하기 위한 것이다."}
{"patent_id": "10-2023-0045561", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "개시되는 일 실시예에 따른 인공 지능 기반의 질의 처리 장치는, 각 분야 별로 배경 지식을 획득하고, 상기 획 득한 배경 지식을 전처리 하여 기 설정된 기준 길이의 문장들을 생성하는 전처리 모듈; 및 제1 언어 처리 모델 을 포함하고, 상기 기준 길이의 문장을 복수 개의 청크 단위로 분할하여 상기 제1 언어 처리 모델로 입력하며, 상기 제1 언어 처리 모델을 통해 각 청크 단위에 대해 문장 임베딩 벡터를 생성하고, 생성한 상기 문장 임베딩 벡터를 색인하여 신경망 데이터베이스에 저장하는 제1 언어 처리 모듈을 포함하며, 상기 제1 언어 처리 모듈은, 질의가 입력되는 경우, 상기 질의를 상기 제1 언어 처리 모델에 입력하여 질의에 대한 문장 임베딩 벡터를 생성 하고, 상기 질의에 대한 문장 임베딩 벡터를 포함하는 조회 요청을 상기 신경망 데이터베이스로 전달하여 상기 질의에 대한 하나 이상의 검색 결과를 획득한다. 상기 제1 언어 처리 모듈은, 상기 문장 임베딩 벡터를 키-값(key-value) 형태로 색인화 하여 상기 신경망 데이 터베이스에 저장하고, 상기 키(key)는 해당 문장 임베딩 벡터이고, 상기 값(value)은 상기 문장 임베딩 벡터와 관련된 텍스트로서 제1 값 및 제2 값을 포함하며, 상기 제1 값은 상기 문장 임베딩 벡터에 대응하는 청크 단위 의 기준 길이 문장이고, 상기 제2 값은 상기 기준 길이 문장에서 상기 제1 값의 청크 단위에 연속되는 다음 청 크 단위의 문장일 수 있다. 상기 신경망 데이터베이스는, 기 저장된 키들 중 상기 질의에 대한 문장 임베딩 벡터 간의 유사도에 기초하여 하나 이상의 키를 확인하고, 확인된 키와 매칭되는 제1 값 및 제2 값을 검색 결과로서 추출할 수 있다. 상기 질의 처리 장치는, 제2 언어 처리 모델을 포함하고, 상기 질의 및 상기 질의에 대한 검색 결과를 입력 받 아 상기 제2 언어 처리 모델로 입력하고, 상기 제2 언어 처리 모델로부터 상기 질의에 대한 답변을 추론하는 제 2 언어 처리 모듈을 더 포함할 수 있다. 상기 제2 언어 처리 모델은, 인코더 및 디코더를 포함하고, 상기 제2 언어 처리 모듈은, 상기 질의를 토큰 단위 로 임베딩하여 질의 임베딩 시퀀스를 생성하고, 상기 검색 결과를 토큰 단위로 임베딩하여 검색 결과 임베딩 시 퀀스를 생성하며, 상기 검색 결과 임베딩 시퀀스를 상기 인코더로 입력하고 상기 질의 임베딩 시퀀스를 상기 디 코더로 입력할 수 있다. 상기 인코더는, 복수 개의 인코더 블록이 적층된 형태로 마련되고, 상기 인코더 블록은, 상기 검색 결과 임베딩 시퀀스에 대해 셀프 어텐션을 수행하여 검색 결과 셀프 어텐션 시퀀스를 생성하며, 상기 인코더는, 상기 검색 결과 셀프 어텐션 시퀀스를 상기 디코더로 전달할 수 있다.상기 디코더는, 복수 개의 단위 디코더 블록이 적층된 형태로 마련되고, 상기 단위 디코더 블록은, 제1 타입의 디코더 블록 및 제2 타입의 디코더 블록을 포함하며, 상기 제1 타입의 디코더 블록은, 상기 질의 임베딩 시퀀스 에 대해 셀프 어텐션을 수행하도록 마련되고, 상기 제2 타입의 디코더 블록은, 상기 질의 임베딩 시퀀스에 대해 셀프 어텐션을 수행하고, 상기 질의 임베딩 시퀀스와 상기 검색 결과 셀프 어텐션 시퀀스 간의 크로스 어텐션을 수행하도록 마련될 수 있다. 상기 인코더는, 상기 검색 결과 임베딩 시퀀스로부터 검색 결과 셀프 어텐션 시퀀스를 생성하여 상기 디코더로 전달하고, 상기 디코더는, 상기 질의 임베딩 시퀀스 및 상기 검색 결과 셀프 어텐션 시퀀스에 기초하여 상기 질 의에 대한 답변을 추론할 수 있다. 상기 제2 언어 처리 모듈은, 상기 질의에 대한 추론된 답변의 문장 길이가 기 설정된 임계 문장 길이보다 짧은 경우, 상기 질의에 대한 검색 결과의 문장 임베딩 벡터를 생성하고, 상기 생성한 문장 임베딩 벡터를 포함하는 추가 조회 요청을 상기 신경망 데이터베이스로 전달할 수 있다. 상기 제2 언어 처리 모듈은, 상기 신경망 데이터베이스로부터 상기 추가 조회 요청에 대응하여 상기 추가 조회 요청의 문장 임베딩 벡터와 가장 유사한 문장 임베딩 벡터를 갖는 키와 매칭되는 제1 값과 제2 값을 추가 검색 결과로 수신할 수 있다. 상기 제2 언어 처리 모듈은, 상기 추가 검색 결과를 토큰 단위로 임베딩하여 추가 검색 결과 임베딩 시퀀스를 생성하고, 상기 질의에 대한 추론된 답변을 토큰 단위로 임베딩하여 추론된 답변 임베딩 시퀀스를 생성하며, 상 기 추가 검색 결과 임베딩 시퀀스를 상기 인코더로 입력하고, 상기 추론된 답변 임베딩 시퀀스를 상기 디코더로 입력하여 상기 질의에 대한 최종 답변을 추론하도록 할 수 있다. 개시되는 일 실시예에 따른 질의 처리 방법은, 전처리 모듈에서, 각 분야 별로 배경 지식을 획득하고, 상기 획 득한 배경 지식을 전처리 하여 기 설정된 기준 길이의 문장들을 생성하는 단계; 제1 언어 처리 모듈에서, 상기 기준 길이의 문장을 복수 개의 청크 단위로 분할하여 제1 언어 처리 모델로 입력하며, 상기 제1 언어 처리 모델 을 통해 각 청크 단위에 대해 문장 임베딩 벡터를 생성하고, 생성한 상기 문장 임베딩 벡터를 색인하여 신경망 데이터베이스에 저장하는 단계; 및 상기 제1 언어 처리 모듈에서, 질의가 입력되는 경우, 상기 질의를 상기 제1 언어 처리 모델에 입력하여 질의에 대한 문장 임베딩 벡터를 생성하고, 상기 질의에 대한 문장 임베딩 벡터를 포함하는 조회 요청을 상기 신경망 데이터베이스로 전달하여 상기 질의에 대한 하나 이상의 검색 결과를 획득하 는 단계를 포함한다."}
{"patent_id": "10-2023-0045561", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "개시되는 실시예에 의하면, 질의에 대한 검색 결과를 얻는 과정을 제1 언어 처리 모델과 신경망 데이터베이스) 를 통해 수행하고, 제2 언어 처리 모델에서는 질의 및 질의에 대한 배경 지식에 기초하여 질의에 대한 답변만 추론하면 되므로, 제2 언어 처리 모델의 신경망 크기를 경량화 할 수 있게 된다."}
{"patent_id": "10-2023-0045561", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 발명의 구체적인 실시형태를 설명하기로 한다. 이하의 상세한 설명은 본 명세서에서 기술된 방법, 장치 및/또는 시스템에 대한 포괄적인 이해를 돕기 위해 제공된다. 그러나 이는 예시에 불과하며본 발명은 이에 제한되지 않는다. 본 발명의 실시예들을 설명함에 있어서, 본 발명과 관련된 공지기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명을 생략하기로 한다. 그리고, 후술되는 용어들은 본 발명에서의 기능을 고려하여 정의된 용어들로서 이는 사용자, 운용자의 의도 또는 관례 등에 따라 달라질 수 있다. 그러므로 그 정의는 본 명세서 전반에 걸친 내용을 토대로 내려져야 할 것이다. 상세한 설명에서 사용되 는 용어는 단지 본 발명의 실시예들을 기술하기 위한 것이며, 결코 제한적이어서는 안 된다. 명확하게 달리 사 용되지 않는 한, 단수 형태의 표현은 복수 형태의 의미를 포함한다. 본 설명에서, \"포함\" 또는 \"구비\"와 같은 표현은 어떤 특성들, 숫자들, 단계들, 동작들, 요소들, 이들의 일부 또는 조합을 가리키기 위한 것이며, 기술된 것 이외에 하나 또는 그 이상의 다른 특성, 숫자, 단계, 동작, 요소, 이들의 일부 또는 조합의 존재 또는 가능 성을 배제하도록 해석되어서는 안 된다. 또한, 제1, 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로 사용될 수 있다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성 요소는 제2 구성 요소로 명 명될 수 있고, 유사하게 제2 구성 요소도 제1 구성 요소로 명명될 수 있다. 도 1은 본 발명의 일 실시예에 따른 인공 지능 기반의 질의 처리 장치를 나타낸 도면이다. 도 1을 참조하면, 질의 처리 장치는 전처리 모듈, 제1 언어 처리 모듈, 신경망 데이터베이스 , 및 제2 언어 처리 모듈을 포함할 수 있다. 전처리 모듈은 분야 별로 해당 분야의 배경 지식을 획득할 수 있다. 일 실시예에서, 전처리 모듈은 웹 크롤링 기술을 통해 각 분야(정치, 경제, 인문, 사회, 지리, 스포츠, 예술, 엔터테인먼트, 과학 등)의 배경 지식을 수집할 수 있다. 그러나, 이에 한정되는 것은 아니며 각 분야의 배경 지식을 저장하고 있는 오픈 데이터 베이스를 활용하여 수집할 수도 있다. 배경 지식은 문서 형태일 수도 있고 말뭉치 형태일 수도 있다. 전처리 모듈은 각 분야의 배경 지식을 전처리 하여 정제된 말뭉치(corpus)를 생성할 수 있다. 전처리 모듈 은 배경 지식을 기 설정된 단위로 토큰화(tokenizing) 할 수 있다. 기 설정된 단위는 형태소 단위 또는 단 어 단위일 수 있으나, 이에 한정되는 것은 아니다. 전처리 모듈은 각 분야의 배경 지식을 특정 주제로 분류하고 기 설정된 기준 길이(예를 들어, 토큰 수를 기준으로 64개 또는 128개 등)의 문장으로 분할 할 수 있다. 이때, 전처리 모듈은 수집한 배경 지식이 기 설정된 기준 길이보다 긴 경우 기준 길이까지만 잘라 문장으로 사용하고, 수집한 배경 지식이 기 설정된 기준 길이보다 짧은 경우 [PAD] 토큰으로 기준 길이까지 패딩 처리하여 문장으로 사용할 수 있다. 전처리 모듈은 각 분야의 배경 지식에 대해 띄어쓰기, 맞춤법, 오타 수정, 및 불용어 제거 등의 전처리를 수행할 수 있다. 전처리 모듈은 수집한 배경 지식에 대해 중복 검사를 수행하여 중복된 배경 지식을 제거 할 수 있다. 일 실시예에서, 전처리 모듈은 배경 지식들의 기 설정된 기준 길이의 문장 간 유사도에 기초하여 중복 문 장인지 여부를 확인하고 중복 문장을 제거할 수 있다. 이때, 기준 길이의 문장 간 유사도는 토크나이징 (tokenizing) 된 문장 간 유사도를 산출할 수 있다. 예를 들어, 전처리 모듈은 기준 길이의 문장 간 유사 도가 기 설정된 임계 값(예를 들어, 0.95) 이상인 경우 중복 문장인 것으로 결정할 수 있다. 전처리 모듈 은 전처리를 통해 정제된 말뭉치(기준 길이의 문장)들을 제1 언어 처리 모듈로 전달할 수 있다. 제1 언어 처리 모듈은 배경 지식들의 기준 길이의 문장에 대해 임베딩을 수행할 수 있다. 이를 위해, 제1 언어 처리 모듈은 제1 언어 처리 모델(104a)을 포함할 수 있다. 제1 언어 처리 모델(104a)은 입력되는 문 장에 대해 임베딩을 수행하여 임베딩 벡터를 생성하기 위한 모델일 수 있다. 제1 언어 처리 모듈은 배경 지식의 기준 길이의 문장을 복수 개의 청크(chunk) 단위로 분할하여 제1 언어 처리 모델(104a)로 입력할 수 있다. 예를 들어, 청크 단위는 토큰 수를 기준으로 16개 또는 32개 등일 수 있다. 제1 언어 처리 모델(104a)은 입력되는 복수 개의 청크 단위를 각각 임베딩하여 문장 임베딩 벡터를 생성할 수 있다. 이때, 제1 언어 처리 모델(104a)은 복수 개의 청크 단위를 토큰 단위로 각각 임베딩한 후 이를 평균하여 해당 청크 단위에 대한 문장 임베딩 벡터를 생성 할 수 있다. 일 실시예에서, 제1 언어 처리 모델(104a)은 BERT(Bidirectional Encoder Representations from Transformer) 기반의 모델일 수 있으나, 이에 한정되는 것은 아니며 임베딩을 수행하는 다양한 형태의 언어 처리 모델이 사용될 수 있다. 제1 언어 처리 모듈은 배경 지식의 각 기준 길이 문장에서 청크 단위의 문장 임베딩 벡터를 신경망 데이터 베이스에 저장할 수 있다. 이때, 제1 언어 처리 모듈은 문장 임베딩 벡터를 색인화 하여 신경망 데이 터베이스에 저장할 수 있다. 제1 언어 처리 모듈은 문장 임베딩 벡터를 키-값(key-value) 형태로 색 인화 하여 신경망 데이터베이스에 저장할 수 있다. 여기서, 키(key)는 해당 문장 임베딩 벡터가 될 수 있다. 그리고, 값(value)은 키와 관련된 텍스트로서 제1 값 과 제2 값을 포함할 수 있다. 제1 값은 해당 문장 임베딩 벡터에 대응하는 청크 단위의 기준 길이 문장이 될 수 있다. 제2 값은 배경 지식의 기준 길이 문장에서 상기 청크 단위에 연속되는 다음 청크 단위의 문장이 될 수 있 다. 표 1은 본 발명의 일 실시예에서 문장 임베딩 벡터가 키-값(key-value) 형태로 색인화되는 상태를 나타낸 표이다. 표 1 키(key) : 문장 임베딩 벡터 값(value) [A1, B2, C3, ~ , B792] 제1 값: 2022년 한국 시리즈 우승팀은 SSG 제2 값: 랜더스 팀이다 [A1, B2, C3, ~ , B792] 제1 값: 한국시리즈 우승을 위하여 제2 값: 랜더스는 선수 보강을 진행하였다 즉, 배경 지식의 각 기준 길이 문장(예를 들어, 토큰 수 64개 또는 128개 길이의 문장)은 그보다 작은 청크 단 위(예를 들어, 토큰 수 16개 또는 24개 길이의 문장)로 분할되어 신경망 데이터베이스에 저장될 수 있다. 다시 말하면, 질의 처리 장치는 각 분야의 배경 지식의 기준 길이 문장의 각 청크 단위에 대해 문장 임베 딩 벡터를 색인화 하여 신경망 데이터베이스에 저장할 수 있다. 여기서, 질의 처리 장치는 기존의 키 워드 기반의 색인이 아닌 임베딩 기반의 색인을 통해 배경 지식을 신경망 데이터베이스에 저장하게 된다. 여기서, 기준 길이 문장을 청크 단위로 분할하여 저장하는 이유는, 후술하는 질의(쿼리)에 대한 배경 지식을 조 회 결과로 출력할 때, 질의와 관련성 없는 내용이 포함되는 것을 최소화 하려고 하는 것이다. 즉, 신경망 데이 터베이스에 저장되는 문장의 길이가 길면 질의와 관련 없는 내용이 포함될 가능성이 크므로 이를 방지하기 위해 기준 길이 문장을 청크 단위로 분할하여 저장할 수 있다. 한편, 질의 처리 장치는 질의(쿼리)가 입력되는 경우, 제1 언어 처리 모듈을 통해 입력되는 질의에 대한 문장 임베딩 벡터를 생성할 수 있다. 제1 언어 처리 모듈은 입력되는 질의를 복수 개의 청크 단위로 분할하여 제1 언어 처리 모델(104a)로 입력할 수 있다. 그러면, 제1 언어 처리 모델(104a)은 각 청크 단위를 각 각 임베딩하여 문장 임베딩 벡터를 생성할 수 있다. 한편, 질의는 제2 언어 처리 모듈로도 입력될 수 있다. 제1 언어 처리 모듈은 질의에 대한 문장 임베딩 벡터를 포함하는 조회 요청을 신경망 데이터베이스로 전달할 수 있다. 신경망 데이터베이스는 질의에 대한 문장 임베딩 벡터와 기 저장된 문장 임베딩 벡터 간 유사도에 기초하여 질의와 관련된 배경 지식을 검색 결과로 추출할 수 있다. 일 실시예에서, 신경망 데이터베이스는 ANN(Approximate Nearest Neighbors) 기반의 벡터 유사도 계산을 통해 질의와 관련된 배경 지식을 추출할 수 있다. 이때, 신경망 데이터베이스는 질의에 대한 문장 임베딩 벡터와 기 저장된 키(key)(즉, 문장 임베딩 벡터)들 간에 벡터 유사도를 산출할 수 있다. 신경망 데이터베이스 는 ANN(Approximate Nearest Neighbors) 기반의 벡터 유사도 계산을 통해 기 저장된 키들 중 질의에 대한 문장 임베딩 벡터와 가장 유사한 기 설정된 개수의 키 또는 유사도가 기 설정된 임계 값 이상인 키를 확인할 수 있다. 신경망 데이터베이스는 확인된 키들과 각각 매칭되는 제1 값 및 제2 값을 질의와 관련된 배경 지식 으로 추출할 수 있다. 도 2는 본 발명의 일 실시예에서 질의와 관련된 배경 지식을 추출하는 과정을 나타낸 도면이다. 여기서는, 질의 에 대한 문장 임베딩 벡터와 가장 유사한 2개의 키 및 그와 매칭되는 제1 값과 제2 값을 추출하는 상태를 도시 하였으나 이에 한정되는 것은 아니다. 도 2를 참조하면, 질의가 \"2022년 한국 시리즈 우승팀은?\"인 경우, 제1 언어 처리 모듈은 질의에 대한 문장 임베딩 벡터를 생성하여 조회 요청을 신경망 데이터베이스로 전 달할 수 있다. 그러면, 신경망 데이터베이스는 질의에 대한 문장 임베딩 벡터와 가장 유사한 문장 임베딩 벡터를 갖는 제 1 키 및 제2 키를 확인할 수 있다. 신경망 데이터베이스는 제1 키와 매칭되는 제1 값(2022년 한국 시리즈 는 SSG 랜더스에서)과 제2 값(우승의 영광을 차지했다)을 제1 배경 지식으로 추출하며, 제2 키와 매칭되는 제1 값(한국 시리즈 우승을 위하여)과 제2 값(랜더스는 선수 보강을 진행하였다)을 제2 배경 지식으로 추출할 수 있 다. 신경망 데이터베이스는 추출한 질의에 대한 배경 지식(즉, 검색 결과)을 제2 언어 처리 모듈로 전달할 수 있다. 제2 언어 처리 모듈은 질의 및 질의에 대한 배경 지식을 각각 입력 받고, 질의 및 질의에 대한 배경 지식 에 기초하여 질의에 대한 답변을 추론할 수 있다. 이를 위해 제2 언어 처리 모듈은 제2 언어 처리 모델 (108a)을 포함할 수 있다. 제2 언어 처리 모델(108a)은 질의 및 질의에 대한 배경 지식을 각각 입력 받고 질의 에 대한 답변을 추론하도록 학습된 언어 모델일 수 있다. 이때, 제2 언어 처리 모듈은 질의 및 질의에 대 한 배경 지식을 각각 토큰 단위로 임베딩하여 제2 언어 처리 모델(108a)로 입력할 수 있다. 제2 언어 처리 모델(108a)은 경량화된 언어 모델일 수 있다. 즉, 개시되는 실시예에서는 질의에 대한 배경 지식 을 얻는 과정을 제1 언어 처리 모델(104a)과 신경망 데이터베이스를 통해 수행하고, 제2 언어 처리 모델 (108a)에서는 질의 및 질의에 대한 배경 지식에 기초하여 질의에 대한 답변만 추론하면 되므로, 제2 언어 처리 모델(108a)의 신경망 크기를 경량화 할 수 있게 된다. 즉, 제2 언어 처리 모델(108a)은 질의 및 질의에 대한 검색 결과(배경 지식)를 입력 받고, 이에 기초하여 질의 에 대한 답변에 대응하는 문장을 생성하여 출력하기만 하면 되므로, 제2 언어 처리 모델(108a)의 신경망 크기를 경량화 할 수 있게 된다. 일 실시예에서, 제2 언어 처리 모델(108a)은 트랜스포머(transformer) 기반의 언어 처 리 모델일 수 있다. 도 3은 본 발명의 일 실시예에 따른 제2 언어 처리 모델(108a)에서 질의에 대한 답변을 추론하는 상태를 나타낸 도면이고, 도 4는 본 발명의 일 실시예에 따른 제2 언어 처리 모델(108a)의 구성을 개략적으로 나타낸 도면이다. 도 3 및 도 4를 참조하면, 제2 언어 처리 모델(108a)은 인코더(encoder) 및 디코더 (decoder)를 포함할 수 있다. 예를 들어, \"2022년 한국 시리즈 우승팀은?\"이라는 질의에 대해 \"2022년 한국 시리즈는 SSG 랜더스에서 우승의 영광을 차지했다\"를 제1 검색 결과로 얻고, \"한국 시리즈 우승을 위하여 랜더스는 선수 보강을 진행하였다\"를 제2 검색 결과로 얻은 경우, 제1 검색 결과와 제2 검색 결과는 각각 인코더로 입력되고, 질의는 디코더 로 입력될 수 있다. 인코더는 복수 개의 인코더 블록이 적층된 형태로 마련될 수 있다. 여기서, 인코더 블록은 셀프 어텐션(self-attention)층과 피드 포워드 신경망(feed-forward neural network)층으로 구성될 수 있다. 앞의 예에서, 제1 검색 결과와 제2 검색 결과의 문장들은 각각 토큰 단위로 임베딩되어 인코더로 입력될 수 있다. 또한, 질의는 토큰 단위로 임베딩되어 디코더로 입력될 수 있다. 즉, 제2 언어 처리 모델(108a)은 제1 검색 결과를 토큰 단위로 임베딩하여 제1 검색 결과 임베딩 시퀀스를 생성 하고, 제2 검색 결과를 토큰 단위로 임베딩하여 제2 검색 결과 임베딩 시퀀스를 생성하며, 제1 검색 결과 임베 딩 시퀀스 및 제2 검색 결과 임베딩 시퀀스를 각각 인코더로 입력할 수 있다. 또한, 제2 언어 처리 모델 (108a)은 질의를 토큰 단위로 임베딩하여 질의 임베딩 시퀀스를 생성하며, 질의 임베딩 시퀀스를 디코더로 입력할 수 있다. 이때, 각 인코더 블록은 제1 검색 결과 임베딩 시퀀스를 입력 받고, 이에 대해 셀프 어텐션을 수행하여 제 1 검색 결과 셀프 어텐션 시퀀스를 생성할 수 있다. 제1 검색 결과 셀프 어텐션 시퀀스는 제1 검색 결과 임베딩 시퀀스에서 각 토큰이 다른 토큰과 얼마나 관련이 있는지에 대한 가중치 값을 포함할 수 있다. 또한, 각 인코더 블록은 제2 검색 결과 임베딩 시퀀스를 입력 받고, 이에 대해 셀프 어텐션을 수행하여 제2 검색 결과 셀프 어텐션 시퀀스를 생성할 수 있다. 제2 검색 결과 셀프 어텐션 시퀀스는 제2 검색 결과 임베딩 시퀀스에서 각 토 큰이 다른 토큰과 얼마나 관련이 있는지에 대한 가중치 값을 포함할 수 있다. 제1 검색 결과 셀프 어텐션 시퀀 스 및 제2 검색 결과 셀프 어텐션 시퀀스는 디코더로 전달될 수 있다. 디코더는 복수 개의 단위 디코더 블록이 적층된 형태로 마련될 수 있다. 여기서, 각 단위 디코더 블 록은 제1 타입의 디코더 블록(121a) 및 제2 타입의 디코더 블록(121b)을 포함할 수 있다. 제1 타입의 디코더 블록(121a)은 질의 임베딩 시퀀스에 대해 셀프 어텐션을 수행하도록 마련될 수 있다. 일 실 시예에서, 제1 타입의 디코더 블록(121a)은 셀프 어텐션(self-attention)층과 피드 포워드 신경망(feed- forward neural network)층으로 구성될 수 있다. 제1 타입의 디코더 블록(121a)은 질의에 대해 문맥을 분석하는 역할을 할 수 있다. 제2 타입의 디코더 블록(121b)은 질의 임베딩 시퀀스에 대해 셀프 어텐션을 수행하고, 질의 임베딩 시퀀스와 검 색 결과 셀프 어텐션 시퀀스 간의 크로스 어텐션을 수행하도록 마련될 수 있다. 여기서, 셀프 어텐션(self- attention) 및 크로스 어텐션(cross-attention)은 이미 공지된 기술이므로 이에 대한 자세한 설명은 생략하기로 한다. 일 실시예에서, 제2 타입의 디코더 블록(121b)은 셀프 어텐션(self-attention)층, 크로스 어텐션(cross- attention)층, 및 피드 포워드 신경망(feed-forward neural network)층으로 구성될 수 있다. 제2 타입의 디코 더 블록(121b)은 질의와 검색 결과 간 문맥을 분석하여 질의에 해당하는 문장 다음에 검색 결과의 토큰 중 어떤 토큰들이 어떤 위치에 위치하는 것이 알맞은지에 대한 확률을 산출할 수 있다. 예를 들어, \"2022년 한국 시리즈 우승팀은?\"이라는 질의에 대해 제2 타입의 디코더 블록(121b)은 \"2022년 한국 시리즈 우승팀은\" 다음에 생성될 단어(즉, 토큰)이 \"SSD 랜더스\"임을 질의 임베딩 시퀀스와 검색 결과 셀프 어텐션 시퀀스 간의 크로스 어텐션을 통해 추론할 수 있다. 디코더는 질의 임베딩 시퀀스를 입력 받고, 인코더로부터 검색 결과 셀프 어텐션 시퀀스들을 입력 받 으며, 질의 임베딩 시퀀스와 검색 결과 셀프 어텐션 시퀀스에 기초하여 질의에 대한 답변을 추론할 수 있다. 이때, 추론된 답변의 문장 길이가 기 설정된 임계 문장 길이보다 짧은 경우, 제2 언어 처리 모듈은 질의에 대한 검색 결과에 기초하여 추가 조회 요청을 신경망 데이터베이스로 전달할 수 있다. 제2 언어 처리 모듈 은 질의에 대한 검색 결과 예를 들어, 제1 검색 결과에 대해 문장 임베딩 벡터를 생성하고, 상기 문장 임 베딩 벡터를 포함하는 추가 조회 요청을 신경망 데이터베이스로 전달할 수 있다. 그러면, 신경망 데이터베이스는 추가 조회 요청의 문장 임베딩 벡터와 가장 유사한 문장 임베딩 벡터를 갖 는 키를 확인하고, 확인된 키와 매칭되는 제1 값과 제2 값을 추출하여 추가 조회 요청에 대한 추가 검색 결과로 서 제2 언어 처리 모듈로 전달할 수 있다. 제2 언어 처리 모듈은 추가 검색 결과 및 질의에 대해 추론된 답변을 각각 제2 언어 처리 모델(108a)로 입 력하여 질의에 대한 최종 답변을 추론할 수 있다. 이때, 추가 검색 결과는 인코더로 입력되고, 질의에 대 해 추론된 답변은 디코더로 입력될 수 있다. 제2 언어 처리 모듈은 추가 검색 결과 및 질의에 대해 추론된 답변을 각각 토큰 단위로 임베딩 하여 추가 검색 결과 임베딩 시퀀스 및 추론된 답변 임베딩 시퀀스를 생성하고 이를 각각 인코더 및 디코더로 입력할 수 있다. 인코더는 인코더 블록을 통해 추가 검색 결과 임베딩 시퀀스에 대해 셀프 어텐션을 수행하여 추가 검 색 결과 셀프 어텐션 시퀀스를 생성할 수 있다. 디코더는 제1 타입의 디코더 블록(121a)을 통해 추론된 답 변 임베딩 시퀀스에 대해 셀프 어텐션을 수행하고, 제2 타입의 디코더 블록(121b)을 통해 추론된 답변 임베딩 시퀀스와 추가 검색 결과 셀프 어텐션 시퀀스 간의 크로스 어텐션을 수행할 수 있다. 이를 통해 디코더는 추론된 답변에 연속되는 단어들을 추가 검색 결과로부터 추론하여 질의에 대한 최종 답변을 생성할 수 있다. 질 의에 대한 최종 답변의 문장 길이는 기 설정된 임계 문장 길이에 대응할 수 있다. 개시되는 실시예에 의하면, 질의에 대한 배경 지식(검색 결과)을 얻는 과정을 제1 언어 처리 모델(104a)과 신경 망 데이터베이스를 통해 수행하고, 제2 언어 처리 모델(108a)에서는 질의 및 질의에 대한 배경 지식에 기 초하여 질의에 대한 답변만 추론하면 되므로, 제2 언어 처리 모델(108a)의 신경망 크기를 경량화 할 수 있게 된 다. 한편, 여기서는 질의 처리 장치가 신경망 데이터베이스를 포함하는 것으로 설명하였으나, 이에 한정 되는 것은 아니며 신경망 데이터베이스는 질의 처리 장치가 통신 가능하게 연결된 외부 장치에 구비 되어 있을 수도 있다. 본 명세서에서 모듈이라 함은, 본 발명의 기술적 사상을 수행하기 위한 하드웨어 및 상기 하드웨어를 구동하기 위한 소프트웨어의 기능적, 구조적 결합을 의미할 수 있다. 예컨대, 상기 \"모듈\"은 소정의 코드와 상기 소정의 코드가 수행되기 위한 하드웨어 리소스의 논리적인 단위를 의미할 수 있으며, 반드시 물리적으로 연결된 코드를 의미하거나, 한 종류의 하드웨어를 의미하는 것은 아니다.도 5는 예시적인 실시예들에서 사용되기에 적합한 컴퓨팅 장치를 포함하는 컴퓨팅 환경을 예시하여 설명하 기 위한 블록도이다. 도시된 실시예에서, 각 컴포넌트들은 이하에 기술된 것 이외에 상이한 기능 및 능력을 가 질 수 있고, 이하에 기술된 것 이외에도 추가적인 컴포넌트를 포함할 수 있다. 도시된 컴퓨팅 환경은 컴퓨팅 장치를 포함한다. 일 실시예에서, 컴퓨팅 장치는 질의 처리 장치 일 수 있다. 컴퓨팅 장치는 적어도 하나의 프로세서, 컴퓨터 판독 가능 저장 매체 및 통신 버스를 포함한다. 프로세서는 컴퓨팅 장치로 하여금 앞서 언급된 예시적인 실시예에 따라 동작하도록 할 수 있 다. 예컨대, 프로세서는 컴퓨터 판독 가능 저장 매체에 저장된 하나 이상의 프로그램들을 실행할 수 있 다. 상기 하나 이상의 프로그램들은 하나 이상의 컴퓨터 실행 가능 명령어를 포함할 수 있으며, 상기 컴퓨터 실 행 가능 명령어는 프로세서에 의해 실행되는 경우 컴퓨팅 장치로 하여금 예시적인 실시예에 따른 동작 들을 수행하도록 구성될 수 있다. 컴퓨터 판독 가능 저장 매체는 컴퓨터 실행 가능 명령어 내지 프로그램 코드, 프로그램 데이터 및/또는 다 른 적합한 형태의 정보를 저장하도록 구성된다. 컴퓨터 판독 가능 저장 매체에 저장된 프로그램은 프로 세서에 의해 실행 가능한 명령어의 집합을 포함한다. 일 실시예에서, 컴퓨터 판독 가능 저장 매체는 메 모리(랜덤 액세스 메모리와 같은 휘발성 메모리, 비휘발성 메모리, 또는 이들의 적절한 조합), 하나 이상의 자 기 디스크 저장 디바이스들, 광학 디스크 저장 디바이스들, 플래시 메모리 디바이스들, 그 밖에 컴퓨팅 장치 에 의해 액세스되고 원하는 정보를 저장할 수 있는 다른 형태의 저장 매체, 또는 이들의 적합한 조합일 수 있다. 통신 버스는 프로세서, 컴퓨터 판독 가능 저장 매체를 포함하여 컴퓨팅 장치의 다른 다양한 컴 포넌트들을 상호 연결한다. 컴퓨팅 장치는 또한 하나 이상의 입출력 장치를 위한 인터페이스를 제공하는 하나 이상의 입출력 인터 페이스 및 하나 이상의 네트워크 통신 인터페이스를 포함할 수 있다. 입출력 인터페이스 및 네트워 크 통신 인터페이스는 통신 버스에 연결된다. 입출력 장치는 입출력 인터페이스를 통해 컴퓨팅 장치의 다른 컴포넌트들에 연결될 수 있다. 예시적인 입출력 장치는 포인팅 장치(마우스 또는 트랙패드 등), 키보드, 터치 입력 장치(터치패드 또는 터치스크린 등), 음성 또는 소리 입력 장치, 다양한 종류의 센서 장치 및/또는 촬영 장치와 같은 입력 장치, 및/또는 디스플레이 장치, 프린터, 스피커 및/또는 네트워크 카드와 같은 출력 장치를 포함할 수 있다. 예시적인 입출력 장치는 컴퓨팅 장치를 구성하는 일 컴포넌트로서 컴퓨팅 장치의 내부에 포함될 수도 있고, 컴퓨팅 장치와는 구별되는 별개의 장치로 컴퓨팅 장치와 연결될 수도 있다."}
{"patent_id": "10-2023-0045561", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이상에서 본 발명의 대표적인 실시예들을 상세하게 설명하였으나, 본 발명이 속하는 기술분야에서 통상의 지식 을 가진 자는 상술한 실시예에 대하여 본 발명의 범주에서 벗어나지 않는 한도 내에서 다양한 변형이 가능함을 이해할 것이다. 그러므로 본 발명의 권리범위는 설명된 실시예에 국한되어 정해져서는 안 되며, 후술하는 특허 청구범위뿐만 아니라 이 특허청구범위와 균등한 것들에 의해 정해져야 한다."}
{"patent_id": "10-2023-0045561", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 인공 지능 기반의 질의 처리 장치를 나타낸 도면 도 2는 본 발명의 일 실시예에서 질의와 관련된 배경 지식을 추출하는 과정을 나타낸 도면 도 3은 본 발명의 일 실시예에 따른 제2 언어 처리 모델에서 질의에 대한 답변을 추론하는 상태를 나타낸 도면 도 4는 본 발명의 일 실시예에 따른 제2 언어 처리 모델의 구성을 개략적으로 나타낸 도면 도 5는 예시적인 실시예들에서 사용되기에 적합한 컴퓨팅 장치를 포함하는 컴퓨팅 환경을 예시하여 설명하기 위 한 블록도"}
