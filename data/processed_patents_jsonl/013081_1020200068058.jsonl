{"patent_id": "10-2020-0068058", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0075825", "출원번호": "10-2020-0068058", "발명의 명칭": "시맨틱 표현 모델의 처리 방법, 장치, 전자 기기 및 저장 매체", "출원인": "베이징 바이두 넷컴 사이언스 앤 테크놀로지 코.,", "발명자": "순, 위"}}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "시맨틱 표현 모델의 처리 방법에 있어서,복수의 훈련 언어 자료를 포함하는 훈련 코퍼스를 수집하는 것; 및상기 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해훈련을 수행하는 것을 포함하는것을 특징으로 하는 시맨틱 표현 모델의 처리 방법."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 훈련 코퍼스를 사용하여 형태론에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것은,상기 훈련 코퍼스를 사용하여 단어, 구절 및/또는 실체의 마스킹 책략에 기반하여 상기 시맨틱 표현 모델에 대해 훈련을 수행하는 것;상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 서로 다른 언어 환경 하의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하는 능력을 학습하도록 하는 것; 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자료 중의 어휘가 원본 문서의 기타 세그먼트에서 출현할지 여부를 예측하는 능력을 학습하도록 하는 것; 중 적어도 하나를 포함하는것을 특징으로 하는 시맨틱 표현 모델의 처리 방법."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 훈련 코퍼스를 사용하여 문법에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것은,상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자료 중 서로 다른 세그먼트의 순서 관계를 인식하는 능력을 학습하도록 하는 것; 및상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 임의의 두 개의 어구 쌍의 위치 관계를 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 포함하는것을 특징으로 하는 시맨틱 표현 모델의 처리 방법."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 훈련 코퍼스를 사용하여 시맨틱에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것은,상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 연속된 두 개의 어구 사이의 논리적 관계를 인식하는 능력을 학습하도록 하는 것; 및상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자료 중의 검색 어구와 웹 페이지 주제 사이의 상관성을 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를포함하는것을 특징으로 하는 시맨틱 표현 모델의 처리 방법.공개특허 10-2021-0075825-3-청구항 5 제1항 내지 제4항 중 어느 한 항에 있어서,상기 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해훈련을 수행하는 것 후에,상기 방법은,사전에 수집한 자연 언어 처리의 태스크 코퍼스에 기반하여 상기 시맨틱 표현 모델에 대해 훈련을 수행하여, 대응하는 자연 언어 처리의 태스크 모델을 얻는 것; 상기 자연 언어 처리의 태스크 모델에 기반하여 상기 자연 언어 처리의 태스크를 실행하는 것을 더 포함하는것을 특징으로 하는 시맨틱 표현 모델의 처리 방법."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "시맨틱 표현 모델의 처리 장치에 있어서,복수의 훈련 언어 자료를 포함하는 훈련 코퍼스를 수집하기 위한 수집 모듈; 및상기 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해훈련을 수행하기 위한 시맨틱 표현 모델 훈련 모듈을 구비하는것을 특징으로 하는 시맨틱 표현 모델의 처리 장치."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 시맨틱 표현 모델 훈련 모듈은, 형태론 훈련 유닛, 문법 훈련 유닛 및 시맨틱 훈련 유닛을 구비하는것을 특징으로 하는 시맨틱 표현 모델의 처리 장치."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 형태론 훈련 유닛은,상기 훈련 코퍼스를 사용하여 단어, 구절 및/또는 실체의 마스킹 책략에 기반하여 상기 시맨틱 표현 모델에 대해 훈련을 수행하는 것;상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 서로 다른 언어 환경 하의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하는 능력을 학습하도록 하는 것; 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자료 중의 어휘가 원본 문서의 기타 세그먼트에서 출현할지 여부를 예측하는 능력을 학습하도록 하는 것 중 적어도 하나를 실행하는것을 특징으로 하는 시맨틱 표현 모델의 처리 장치."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 문법 훈련 유닛은,상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자료 중 서로 다른 세그먼트의 순서 관계를 인식하는 능력을 학습하도록 하는 것; 및상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 임의의 두 개의 어구 쌍의 위치 관계를 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 실행하는공개특허 10-2021-0075825-4-것을 특징으로 하는 시맨틱 표현 모델의 처리 장치."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서,상기 시맨틱 훈련 유닛은,상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 연속된 두 개의 어구 사이의 논리적 관계를 인식하는 능력을 학습하도록 하는 것; 및상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자료 중의 검색 어구와 웹 페이지 주제 사이의 상관성을 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를실행하는것을 특징으로 하는 시맨틱 표현 모델의 처리 장치."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제6항 내지 제10항 중 어느 한 항에 있어서,사전에 수집한 자연 언어 처리의 태스크 코퍼스에 기반하여 상기 시맨틱 표현 모델에 대해 훈련을 수행하여, 대응하는 자연 언어 처리의 태스크 모델을 얻기 위한 태스크 모델 훈련 모듈; 및상기 자연 언어 처리의 태스크 모델에 기반하여 상기 자연 언어 처리의 태스크를 실행하기 위한 실행 모듈을 더구비하는것을 특징으로 하는 시맨틱 표현 모델의 처리 장치."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "전자 기기에 있어서,적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며; 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되어 있으며, 상기 명령이 상기적어도 하나의 프로세서에 의해 실행됨으로써, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제5항 중 어느 한 항의 방법을 실행할 수 있도록 하는것을 특징으로 하는 전자 기기."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "컴퓨터 명령이 저장되어 있는 비 일시적 컴퓨터 판독 가능 저장 매체에 있어서,상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제5항 중 어느 한 항의 방법을 실행하도록 하는것을 특징으로 하는 비 일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2020-0068058", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "컴퓨터로 하여금 제1항 내지 제5항 중 어느 한 항의 방법을 실행하도록 하는것을 특징으로 하는 컴퓨터 프로그램."}
{"patent_id": "10-2020-0068058", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본원은 시맨틱 표현 모델의 처리 방법, 장치, 전자 기기 및 저장 매체를 개시하는 바, 인공 지능"}
{"patent_id": "10-2020-0068058", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관 한 것이다. 구체적인 구현 방안 은, 복수의 훈련 언어 자료를 포함하는 훈련 코퍼스를 수집하는 것; 및 상기 훈 련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수 행하는 것을 포함한다. 본원에 있어서, 형태론, 문법, 시맨틱 이러한 세 개의 서로 다른 레벨의 무 감독 또는 약 감독의 사전 훈련 태스크를 구축함으로써, 시맨틱 표현 모델이 대규모 데이터 중에서 어휘, 문법, 시맨틱과 같은 서로 다른 등급의 지식을 학습하도록 하여, 범용 시맨틱 표현의 능력을 강화하였고, NLP 태스크의 처리 효과를 향상시켰다. 대 표 도 - 도1"}
{"patent_id": "10-2020-0068058", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2021-0075825 CPC특허분류 G06F 40/268 (2020.01) G06F 40/279 (2020.01) G06N 20/00 (2019.01) 발명자 왕, 슈오환 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 바이두 캠퍼스, 넘버 10, 2층 리, 위쿤 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 바이두 캠퍼스, 넘버 10, 2층 펑, 스쿤 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 바이두 캠퍼스, 넘버 10, 2층티엔, 하오 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 바이두 캠퍼스, 넘버 10, 2층 우, 화 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 바이두 캠퍼스, 넘버 10, 2층명 세 서 청구범위 청구항 1 시맨틱 표현 모델의 처리 방법에 있어서, 복수의 훈련 언어 자료를 포함하는 훈련 코퍼스를 수집하는 것; 및 상기 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것을 포함하는 것을 특징으로 하는 시맨틱 표현 모델의 처리 방법. 청구항 2 제1항에 있어서, 상기 훈련 코퍼스를 사용하여 형태론에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것은, 상기 훈련 코퍼스를 사용하여 단어, 구절 및/또는 실체의 마스킹 책략에 기반하여 상기 시맨틱 표현 모델에 대 해 훈련을 수행하는 것; 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 서로 다른 언어 환 경 하의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하는 능력을 학습하도록 하는 것; 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자 료 중의 어휘가 원본 문서의 기타 세그먼트에서 출현할지 여부를 예측하는 능력을 학습하도록 하는 것; 중 적어 도 하나를 포함하는 것을 특징으로 하는 시맨틱 표현 모델의 처리 방법. 청구항 3 제1항에 있어서, 상기 훈련 코퍼스를 사용하여 문법에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것은, 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자 료 중 서로 다른 세그먼트의 순서 관계를 인식하는 능력을 학습하도록 하는 것; 및 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 임의의 두 개의 어 구 쌍의 위치 관계를 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 포함하는 것을 특징으로 하는 시맨틱 표현 모델의 처리 방법. 청구항 4 제1항에 있어서, 상기 훈련 코퍼스를 사용하여 시맨틱에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것은, 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 연속된 두 개의 어 구 사이의 논리적 관계를 인식하는 능력을 학습하도록 하는 것; 및 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자 료 중의 검색 어구와 웹 페이지 주제 사이의 상관성을 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 포함하는 것을 특징으로 하는 시맨틱 표현 모델의 처리 방법.청구항 5 제1항 내지 제4항 중 어느 한 항에 있어서, 상기 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것 후에, 상기 방법은, 사전에 수집한 자연 언어 처리의 태스크 코퍼스에 기반하여 상기 시맨틱 표현 모델에 대해 훈련을 수행하여, 대 응하는 자연 언어 처리의 태스크 모델을 얻는 것; 상기 자연 언어 처리의 태스크 모델에 기반하여 상기 자연 언어 처리의 태스크를 실행하는 것을 더 포함하는 것을 특징으로 하는 시맨틱 표현 모델의 처리 방법. 청구항 6 시맨틱 표현 모델의 처리 장치에 있어서, 복수의 훈련 언어 자료를 포함하는 훈련 코퍼스를 수집하기 위한 수집 모듈; 및 상기 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하기 위한 시맨틱 표현 모델 훈련 모듈을 구비하는 것을 특징으로 하는 시맨틱 표현 모델의 처리 장치. 청구항 7 제6항에 있어서, 상기 시맨틱 표현 모델 훈련 모듈은, 형태론 훈련 유닛, 문법 훈련 유닛 및 시맨틱 훈련 유닛을 구비하는 것을 특징으로 하는 시맨틱 표현 모델의 처리 장치. 청구항 8 제7항에 있어서, 상기 형태론 훈련 유닛은, 상기 훈련 코퍼스를 사용하여 단어, 구절 및/또는 실체의 마스킹 책략에 기반하여 상기 시맨틱 표현 모델에 대 해 훈련을 수행하는 것; 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 서로 다른 언어 환 경 하의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하는 능력을 학습하도록 하는 것; 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자 료 중의 어휘가 원본 문서의 기타 세그먼트에서 출현할지 여부를 예측하는 능력을 학습하도록 하는 것 중 적어 도 하나를 실행하는 것을 특징으로 하는 시맨틱 표현 모델의 처리 장치. 청구항 9 제7항에 있어서, 상기 문법 훈련 유닛은, 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자 료 중 서로 다른 세그먼트의 순서 관계를 인식하는 능력을 학습하도록 하는 것; 및 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 임의의 두 개의 어 구 쌍의 위치 관계를 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 실행하는것을 특징으로 하는 시맨틱 표현 모델의 처리 장치. 청구항 10 제7항에 있어서, 상기 시맨틱 훈련 유닛은, 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 연속된 두 개의 어 구 사이의 논리적 관계를 인식하는 능력을 학습하도록 하는 것; 및 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자 료 중의 검색 어구와 웹 페이지 주제 사이의 상관성을 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 실행하는 것을 특징으로 하는 시맨틱 표현 모델의 처리 장치. 청구항 11 제6항 내지 제10항 중 어느 한 항에 있어서, 사전에 수집한 자연 언어 처리의 태스크 코퍼스에 기반하여 상기 시맨틱 표현 모델에 대해 훈련을 수행하여, 대 응하는 자연 언어 처리의 태스크 모델을 얻기 위한 태스크 모델 훈련 모듈; 및 상기 자연 언어 처리의 태스크 모델에 기반하여 상기 자연 언어 처리의 태스크를 실행하기 위한 실행 모듈을 더 구비하는 것을 특징으로 하는 시맨틱 표현 모델의 처리 장치. 청구항 12 전자 기기에 있어서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며; 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 실행됨으로써, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제5항 중 어 느 한 항의 방법을 실행할 수 있도록 하는 것을 특징으로 하는 전자 기기. 청구항 13 컴퓨터 명령이 저장되어 있는 비 일시적 컴퓨터 판독 가능 저장 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제5항 중 어느 한 항의 방법을 실행하도록 하는 것을 특징으로 하는 비 일시적 컴퓨터 판독 가능 저장 매체. 청구항 14 컴퓨터로 하여금 제1항 내지 제5항 중 어느 한 항의 방법을 실행하도록 하는 것을 특징으로 하는 컴퓨터 프로그램. 발명의 설명 기 술 분 야 본원은 컴퓨터 기술 분야에 관한 것이며, 특히 인공 지능 기술에 관한 것인 바, 구체적으로 시맨틱 표현 모델의 처리 방법, 장치, 전자 기기 및 저장 매체에 관한 것이다."}
{"patent_id": "10-2020-0068058", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 지능(Artificial Intelligence; AI）은 인간 지능을 시뮬레이션, 연장 및 확장하는 이론, 방법, 기술 및 적용 시스템을 연구 및 개발하는 새로운 기술 과학이다. 인공 지능은 컴퓨터 과학의 하나의 분파로서, 지능의 실질을 이해하려고 시도하고 있으며, 또한 인류 지능과 유사한 방식으로 반응을 보일 수 있는 새로운 스마트 기 계를 생산한다. 당해 분야의 연구는 로봇, 언어 인식, 화상 인식, 자연 언어 처리（Natural Language Processing; NLP） 및 전문가 시스템 등을 포함한다. 특히 NLP 분야는 최근 몇 AI 연구의 비교적 인기 있는 방 향이다. NLP 분야에 있어서, 신경망 모델을 사용하여 어구에 대해 시맨틱 표현을 수행하고, 또한 취득한 시맨틱 표현에 기반하여 NLP의 태스크 처리를 수행한다. 종래 기술에 있어서, 전형적인 시맨틱 표현 기술은 Word2Vec, Glove 모델 등을 사용하여 구현하는 앞뒤 문맥이 상관성 없는 시맨틱 표현, 및 Elmo, 변환기의 양방향 인코딩 표시 (Bidirectional Encoder Representations from Transformers; BERT） 모델, XLNET 등을 사용하여 구현하는 앞 뒤 문맥이 상관성 있는 시맨틱 표현을 포함한다. 여기서, 앞뒤 문맥이 상관성 있는 시맨틱 표현은, 앞뒤 문맥이 상관성 없는 시맨틱 표현에 비해 효과 상 선명한 진보를 갖는다. 그러나, BERT, XLNet등 현재 기술은 주요하게 단어 또는 문장의 공기(묾폅) 정보를 통해 학습하는 바, 태스크 목표가 단일하다. 예를 들면, BERT는 마스킹 언어 모델과 다음 구절 예측 태스크를 통해 훈련을 수행하며; XLNet는 완전하게 배열된 언어 모델을 구축하여, 자기 회귀의 방식을 통해 사전 훈련을 수행한다. 따라서, 종래 기술을 통해 훈련한 시맨틱 표현 모델은 훈련 언어 자료 중의 각 등급의 정보를 충분히 학습하지 못하여, 기존 의 시맨틱 표현 모델의 능력이 제한적이며, 시맨틱 표현의 정확성이 비교적 낮다."}
{"patent_id": "10-2020-0068058", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본원은 시맨틱 표현 모델의 시맨틱 표현 능력을 풍부히 하고, 시맨틱 표현의 정확성을 향상시키기 위한, 시맨틱 표현 모델의 처리 방법, 장치, 전자 기기 및 저장 매체를 제공한다."}
{"patent_id": "10-2020-0068058", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본원은 시맨틱 표현 모델의 처리 방법을 제공하는 바, 당해 방법은, 복수의 훈련 언어 자료를 포함하는 훈련 코퍼스를 수집하는 것; 및 상기 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것을 포함한다. 또한 옵션으로서, 상기와 같은 방법에 있어서, 상기 훈련 코퍼스를 사용하여 형태론에 기반하여 시맨틱 표현 모 델에 대해 훈련을 수행하는 것은, 상기 훈련 코퍼스를 사용하여 단어, 구절 및/또는 실체의 마스킹 책략에 기반하여 상기 시맨틱 표현 모델에 대 해 훈련을 수행하는 것; 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 서로 다른 언어 환 경 하의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하는 능력을 학습하도록 하는 것; 및 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자 료 중의 어휘가 원본 문서의 기타 세그먼트에서 출현할지 여부를 예측하는 능력을 학습하도록 하는 것 중 적어 도 하나를 포함한다. 또한 옵션으로서, 상기와 같은 방법에 있어서, 상기 훈련 코퍼스를 사용하여 문법에 기반하여 시맨틱 표현 모델 에 대해 훈련을 수행하는 것은, 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자 료 중 서로 다른 세그먼트의 순서 관계를 인식하는 능력을 학습하도록 하는 것; 및 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 임의의 두 개의 어 구 쌍의 위치 관계를 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 포함한다.또한 옵션으로서, 상기와 같은 방법에 있어서, 상기 훈련 코퍼스를 사용하여 시맨틱에 기반하여 시맨틱 표현 모 델에 대해 훈련을 수행하는 것은, 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 연속된 두 개의 어 구 사이의 논리적 관계를 인식하는 능력을 학습하도록 하는 것; 및 상기 훈련 코퍼스를 사용하여 상기 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 상기 훈련 언어 자 료 중의 검색 어구와 웹 페이지 주제 사이의 상관성을 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 포함한다. 또한 옵션으로서, 상기와 같은 방법에 있어서, 상기 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어 도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것 후에, 상기 방법은, 사전에 수집한 자연 언어 처리의 태스크 코퍼스에 기반하여 상기 시맨틱 표현 모델에 대해 훈련을 수행하여, 대 응하는 자연 언어 처리의 태스크 모델을 얻는 것; 및 상기 자연 언어 처리의 태스크 모델에 기반하여 상기 자연 언어 처리의 태스크를 실행하는 것을 더 포함한다. 본원은 시맨틱 표현 모델의 처리 장치를 더 제공하는 바, 당해 장치는, 복수의 훈련 언어 자료를 포함하는 훈련 코퍼스를 수집하기 위한 수집 모듈; 및 상기 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하기 위한 시맨틱 표현 모델 훈련 모듈을 구비한다. 본원은 전자 기기를 더 제공하는 바, 당해 전자 기기는, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며; 여기서, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 실행됨으로써, 상기 적어도 하나의 프로세서로 하여금 상기의 임의의 방법을 실 행할 수 있도록 한다. 본원은 컴퓨터 판독 가능 저장 매체가 더 제공되는 바, 컴퓨터 명령이 저장되어 있는 비 일시적 컴퓨터 판독 가 능 저장 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터로 하여금 상기의 임의의 방법을 실행하도록 한다."}
{"patent_id": "10-2020-0068058", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상술한 본원 중의 하나의 실시예는 이하의 이점 또는 유익한 효과를 가진다. 복수의 훈련어를 포함하는 훈련 코 퍼스를 수집하고; 또한 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행함으로써, 훈련 후의 시맨틱 표현 모델로 하여금 형태론, 문법 및 시맨틱 등 각 등급의 정보를 충분히 학습할 수 있도록 하여, 시맨틱 표현 모델의 시맨틱 표현 능력을 풍부히 하였고, 시맨틱 표현의 정확성을 향상시켰다. 또한, 본원에 있어서, 시맨틱 표현 모델에 대해 형태론에 기반한 태스크 훈련을 수행함으로써, 시맨틱 표현 모 델로 하여금 단어, 구절 및 실체의 공기 지식을 학습하고, 지식의 융합성을 이해할 수 있도록 하여, 시맨틱 표 현 모델의 시맨틱 표현 능력을 강화할 수 있고, 매 하나의 어구에 대해 정확하게 시맨틱 표현을 수행할 수 있다. 이와 동시에, 또한 서로 다른 언어 환경 하의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하 는 능력을 학습할 수 있기에, 서로 다른 언어 환경 하의 어휘의 정확한 표현 방식을 정확히 학습할 수 있고; 또 한, 어휘가 원본 문서의 기타 세그먼트의 공기할지를 예측하는 능력을 학습할 수 있기에, 문서 중에서 어떠한 어휘가 문서의 중심 사상을 나타낼 수 있는지를 예측할 수 있다. 상술한 형태론에 기반한 태스크 훈련을 통해, 시맨틱 표현 모델로 하여금 풍부한 형태론 지식을 학습할 수 있도록 하고, 형태론이 표시하는 의미를 충분히 이 해하도록 하여, 더욱 정확하게 시맨틱 표현을 수행할 수 있다. 또한, 본원에 있어서, 또한 시맨틱 표현 모델에 대해 문법에 기반한 태스크 훈련을 수행함으로써, 시맨틱 표현 모델로 하여금 문장의 배열을 학습할 수 있도록 하고, 또한 서로 다른 문장의 위치 관계를 인식할 수 있도록 하 여, 시맨틱 표현 과정에서 매 하나의 문장에 대해 정확히 위치 확정을 수행하여, 시맨틱 표현의 정확성을 향상 시켰다.또한, 본원에 있어서, 또한 시맨틱 표현 모델에 대해 시맨틱에 기반한 태스크 훈련을 수행함으로써, 시맨틱 표 현 모델로 하여금 문장의 논리적 관계 태스크 및 검색 상관성 태스크를 학습할 수 있도록 하여, 시맨틱 표현 시, 시맨틱을 정확히 이해하고, 시맨틱 표현의 정확성을 강화할 수 있다. 총괄적으로, 본원에 있어서, 형태론, 문법, 시맨틱 이러한 세 개의 서로 다른 레벨의 무 감독 또는 약 감독의 사전 훈련 태스크를 구축함으로써, 시맨틱 표현 모델이 대규모 데이터 중에서 어휘, 문법, 시맨틱과 같은 서로 다른 등급의 지식을 학습하도록 하여, 범용 시맨틱 표현의 능력을 강화하였고, NLP 태스크의 처리 효과를 향상 시켰다. 상술한 옵션의 방식이 갖는 기타 효과는 아래에서 구체적인 실시예를 참조하여 설명될 것이다."}
{"patent_id": "10-2020-0068058", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 이해를 용이하게 하기 위하여, 도면을 참조하여 본원 실시예의 다양한 세부 사항을 포함하는 본원의 예시 적인 실시예를 설명하는 바, 이들을 단지 예시적인 것으로 간주하여야 한다. 따라서, 당업자는 여기서 설명되는 실시예에 대해 여러가지 변형과 수정을 이룰 수 있으며, 이들은 본원의 법위와 정신을 벗어나지 않음을 인식해 야 한다. 마찬가지로, 명확성과 간결성을 위하여 이하의 설명에서는 공지된 기능과 구조에 대한 설명을 생략한 다. 도 1은 본원의 제1 실시예의 모식도이다. 도 1에 나타낸 바와 같이, 본원의 시맨틱 표현 모델의 처리 방법의 실 시예의 플로우 차트를 상세히 설명하였다. 도 1에 나타낸 바와 같이, 본 실시예의 시맨틱 표현 모델의 처리 방 법은 이하의 단계를 포함한다. S101에 있어서, 복수의 훈련 언어 자료를 포함하는 훈련 코퍼스를 수집하고; S102에 있어서, 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모 델에 대해 훈련을 수행한다. 본 실시예의 시맨틱 표현 모델의 처리 방법의 실행 본체는 시맨틱 표현 모델의 처리 장치이며, 당해 시맨틱 표 현 모델의 처리 장치는 독립된 전자 실체 또는 소프트웨어를 사용하여 통합한 응용 시스템일 수 있는 바, 시맨 틱 표현 모델의 각 처리를 구현할 수 있으며, 예를 들면, 시맨틱 표현 모델의 훈련 등을 구현할 수 있다. 훈련하는 시맨틱 표현 모델의 시맨틱 표현의 정확성을 향상시키기 위하여, 본 실시예에 있어서, 훈련 코퍼스를 사용하여 하나, 둘 또는 복수의 훈련 태스크에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행할 수 있으며, 예 를 들면 구체적으로 형태론, 문법 및 시맨틱 중의 적어도 하나의 훈련 태스크에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행할 수 있다. 형태론, 문법 및 시맨틱 중의 적어도 두 개의 훈련 태스크에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행할 경우, 형태론, 문법 및 시맨틱 중의 적어도 두 개의 훈련 태스크에 기반하여 동시에 시맨틱 표현 모델에 대해 훈련을 수행할 수도 있고, 각각 하나의 훈련 태스크에 기반하여 차례로 시맨틱 표현 모델에 대해 훈련을 수행할 수도 있는 바, 구체적으로 먼저 어느 훈련 태스크에 기반하여 훈련을 수행하고, 그 후 어느 훈련 태스크에 기반하여 훈련을 수행하는지에 대해 순서 한정이 없음을 설명할 필요가 있다. 시맨틱 표현 모델의 훈련 효과를 보증하기 위하여, 본 실시예에 있어서 훈련 코퍼스에는 백만급 이상이 포함될 수 있으며, 심지어 더 많은 훈련 언어 자료를 포함할 수 있다. 또한, 본 실시예에 있어서 훈련 코퍼스 중의 매 하나의 훈련 언어 자료의 데이터 내용은 매우 풍부할 수 있으며, 이렇게 하여 서로 다른 훈련 태스크의 요구를 만족시킬 수 있다. 또는 본 실시예의 훈련 코퍼스에 있어서, 매 하나의 훈련 언어 자료가 응용되는 훈련 태스크 의 마크를 대응되게 라벨링할 수 있으며, 예를 들면 각각 숫자 1, 2, 3으로 세가지 훈련 태스크에 각각 필요한 훈련 언어 자료를 표시할 수 있다. 구체적으로, 각 훈련 태스크에 대해, 작은 태스크로 더 한층 세분화할 필요 가 있을 경우, 필요한 훈련 언어 자료가 서로 다르며, 또한 훈련 코퍼스 중에서 각 훈련 언어 자료가 적용되는 작은 태스크의 마크를 라벨링할 수 있다. 옵션으로서, 본 실시예 중의 단계 S102에서 훈련 코퍼스를 사용하여 형태론에 기반하여 시맨틱 표현 모델에 대 해 훈련을 수행하는 것은, （a）훈련 코퍼스를 사용하여 단어, 구절 및/또는 실체의 마스킹 책략에 기반하여 시맨틱 표현 모델에 대해 훈 련을 수행하는 것; （b）훈련 코퍼스를 사용하여 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 서로 다른 언어 환경 하 의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하는 능력을 학습하도록 하는 것; 및 （c）훈련 코퍼스를 사용하여 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 훈련 언어 자료 중의 어 휘가 원본 문서의 기타 세그먼트에서 출현할지 여부를 예측하는 능력을 학습하도록 하는 것 중 적어도 하나를 포함한다. 상술한 (a）, (b） 및(c） 이러한 세 가지 방식은 형태론 레벨의 태스크 훈련에 속하며, 실제 응용에 있어서, 시맨틱 표현 모델을 훈련할 때, 동시에 상술한 세가지 방식을 사용하여 훈련을 수행하거나, 또는 한가지를 선택 하는 방식, 또는 임의의 두 가지를 선택하여 조합하는 방식을 통해 시맨틱 표현 모델을 훈련할 수 있다. 물론, 선택한 훈련 방식이 많을수록, 훈련시킨 시맨틱 표현 모델이 학습한 능력이 더 강하며, 시맨틱 표현을 더 정확 히 수행할 수 있다. 도 2는 본원의 제2 실시예의 모식도이다. 도 2에 나타낸 바와 같이, 상술한 실시예 중의 방식(a）이 훈련 코퍼 스를 사용하여 단어, 구절 및/또는 실체의 마스킹 책략에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 구체적인 구현 과정을 상세히 설명하였다. 도 2에 나타낸 바와 같이, 본 실시예의 훈련 코퍼스를 사용하여 단어, 구절 및/또는 실체의 마스킹 책략에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것은, 구체적으 로 이하의 단계를 포함할 수 있다. S201에 있어서, 훈련 코퍼스 중에서 취득한 각 훈련 어구 중의 단어, 구절 및 실체에 대해 라벨링(labeling)을 수행한다. 예를 들면, 본 실시예에 있어서, 사전에 훈련한 시퀀스 라벨링 모델을 사용하여 코퍼스 중의 각 훈련 어구 중의 단어, 구절 및 실체에 대해 라벨링을 수행할 수 있다. 여기서, 단어는 말의 최소 단위이고, 구절는 단어의 조합 일 수 있으며, 예를 들면 동위 구절인 “수도 북경”, 방위 구절인 “땅에 있음”, 수량 구절인 “꽃 한송이” 등을 포함할 수 있다. 본 실시예의 단어 입도와 실체 입도는 글자 입도보다 크며, 예를 들면, 단어는 구체적으로 “이쁨”, “아름다 움”, “진실함”, “기쁨”, “가지각색”, “성도（省都）”, “도시” 등과 같은 연속된 두 개 또는 복수의 글자로 구성된다. 실체는 구체적으로 사람 이름, 지명 또는 회사 명칭 등일 수 있다. 구절의 입도는 단어 입도 및 실체 입도보다 크다. 여기서, 훈련 코퍼스는 사전에 수집한 것으로, 복수의 훈련 언어 자료를 포함한다. 본 실시예에 있어서, 서로 다른 훈련 태스크에 필요하는 훈련 언어 자료가 서로 다른 바, 예를 들면 일부 훈련 태스크에 필요한 훈련 언어 자료는 일부 비교적 짧은 훈련 어구이고, 다른 일부 훈련 태스크에 필요한 훈련 언어 자료는 복수의 어구를 포 함하는 비교적 긴 단락일 수 있다. 본 실시예에 있어서, 훈련 코퍼스 중에서 매 하나의 훈련 언어 자료가 적용 되는 훈련 태스크의 마크를 라벨링할 수 있다. 예를 들면, 방식(a）을 통해, 훈련 코퍼스를 사용하여 단어, 구 절 및/또는 실체의 마스킹 책략에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행할 경우, 훈련 코퍼스 중에서 본 훈련 태스크에 적용되는 각 훈련 어구를 취득한 후, 취득한 각 훈련 어구에 기반하여 단어, 구절 및/또는 실 체의 마스킹 책략을 통해 시맨틱 표현 모델에 대해 훈련을 수행한다.구체적으로, 사용할 때, 매 하나의 훈련 어구를 당해 시퀀스 라벨링 모델에 입력할 수 있으며, 당해 시퀀스 라 벨링은 당해 훈련 데이터 중에 포함된 단어, 구절 및 실체를 출력할 수 있다. 이에 따라, 시퀀스 라벨링 모델을 훈련할 때, 복수의 세트의 훈련 어구 및 각 훈련 어구에 대해 라벨링한 단어, 구절 및 실체를 취득할 수 있다. 구체적으로, 매 하나의 훈련 데이터를 시퀀스 라벨링 모델에 입력하고, 당해 시퀀스 라벨링 모델은 여기에 포함 된 단어, 구절 및 실체를 예측한다. 그 후, 예측한 단어, 구절 및 실체와 라벨링한 단어 및 실체가 일치한지 여 부를 판단하며, 일치하지 않을 경우, 예측한 단어, 구절 및 실체와 라벨링한 단어, 구절 및 실체에 기반하여, 시퀀스 라벨링 모델의 매개 변수를 조정하며, 훈련 횟수가 미리 설정한 최대 횟수 임계 값에 도달하거나, 또는 연속된 미리 설정한 횟수의 훈련 중에서 모두 매개 변수를 조정할 필요가 없을 때까지, 복수의 훈련 어구와 대 응되는 라벨링한 단어, 구절 및 실체를 사용하여 시퀀스 라벨링 모델에 대해 훈련을 수행하며, 이 경우는 시퀀 스 라벨링 모델이 이미 훈련을 완성하였다는 것을 나타내며, 이때 시퀀스 라벨링 모델의 매개 변수를 확정하고, 또한 시퀀스 라벨링 모델을 확정한다. 또는 본 실시예에 있어서, 사전에 수집한 단어 라이브러리, 구절 라이브러리 및 실체 라이브러리를 사용하여 각 각 코퍼스 중의 각 훈련 어구 중의 단어, 구절 및 실체에 대해 라벨링을 수행할 수 있다. 여기서, 단어 라이브 러리 또는 구절 라이브러리는, 사전에 수집한 방대한 예측 라이브러리에 기반하여 모든 단어 또는 구절을 수집 할 수 있다. 마찬가지로, 실체 라이브러리도 일부 지명, 사람 이름, 회사 명칭의 실체를 수집할 수 있으며, 동 시에 하나의 회사 명칭의 템플릿을 사용자 지정하여, 실체 라이브러리 이외의 기타 회사 실체에 대해 라벨링을 수행할 수 있다. 실제 응용에 있어서, 상술한 두 방식 이외의 기타 방식을 통해 단어 및 실체의 라벨링을 구현할 수 있는 바, 여 기서 더는 하나씩 예를 들어 설명하지 않는다. S202에 있어서, 취득한 각 훈련 어구 및 라벨링한 단어를 사용하여 단어 입도에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하고; S203에 있어서, 취득한 각 훈련 어구 및 라벨링한 구절을 사용하여 구절 입도에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하며; S204에 있어서, 취득한 각 훈련 어구 및 라벨링한 실체를 사용하여 실체 입도에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행한다. 예를 들면, 글자 입도의 훈련의 경우, 시맨틱 표현 모델은 \"하*빈\" 중간의 글자가 \"얼\"자임을 학습할 수 있다. 이에 따라, 단어 입도에 기반한 훈련의 경우, \"하얼빈\"이 \"흑룡강\"의 성도인 시맨틱 관계를 학습할 수 있다. 구 절 입도에 기반한 훈련의 경우, 유명한 문화 도시와 같은 구절 관계를 학습할 수 있다. 실체 입도에 기반한 훈 련의 경우, 시맨틱 표현 모델은 또한 \"하얼빈\"과 \"흑룡강\"이 시맨틱 관계가 있음을 학습할 수 있다. 본 실시예 에 있어서, 일부 실체 자체가 하나의 단어이며; 일부 실체는 단어보다 긴 바, 구체적으로 두 개 또는 복수의 단 어를 포함할 수 있다. 구체적으로, 단계 S202~S204를 통해, 시맨틱 표현 모델에 대해 각각 단어 입도, 구절 입도 및 실체 입도에 기반 하여 훈련을 수행함으로써, 시맨틱 표현 모델로 하여금 문장 중의 단어, 구절 및 실체에 기반한 지식 융합성을 학습할 수 있도록 한다. 또한, 본 실시예의 단계 S202~S204의 순서 관계가 제한되지 않는 바, 먼저 시맨틱 표현 모델에 대해 단어 입도에 기반한 훈련을 수행한 후, 차례로 각각 구절 입도와 실체 입도에 기반한 훈련을 수행 하거나, 또는 차례로 각각 실체 입도와 구절 입도에 기반한 훈련을 수행할 수 있다. 또는 먼저 시맨틱 표현 모 델에 대해 실체 입도에 기반한 훈련을 수행한 수, 차례로 각각 구절 입도와 단어 입도에 기반한 훈련을 수행하 거나, 또는 차례로 각각 단어 입도와 구절 입도에 기반한 훈련을 수행할 수도 있다. 심지어 동시에 세가지 입도 에 기반한 훈련을 수행할 수 있는 바, 예를 들면, 훈련 과정에서, 먼저 구절 입도에 기반하여 시맨틱 표현 모델 에 대해 일회 또는 고정된 횟수의 훈련을 수행한 후, 단어 입도에 기반하여 시맨틱 표현 모델에 대해 일회 또는 고정된 횟수의 훈련을 수행하고, 그 후 이어서 실체 입도에 기반하여 시맨틱 표현 모델에 대해 일회 또는 고정 된 횟수의 훈련을 수행할 수 있다. 상술한 흐름에 따라 중복적으로 시맨틱 표현 모델에 대해 훈련을 수행할 수 있다. 총괄적으로, 단어 입도, 구절 입도 및 실체 입도에 기반한 훈련을 완성할 수 있는 한, 먼저 어느 훈련을 수행한 후 어느 훈련을 수행하거나, 심지어 동시에 훈련을 수행할 수 있다. 훈련 코퍼스 중에서 취득한 본 훈련 태스크에 적용되는 각 훈련 어구 및 라벨링한 단어를 사용하여, 단어 입도 에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 과정에서, 매 하나의 훈련 어구 및 당해 훈련 어구 중의 라벨링한 단어에 대해, 랜덤으로 일정 비율의 단어를 숨기어, 모델이 당해 훈련 어구 중의 기타 단어 등 앞뒤문맥 정보에 기반하여 당해 단어를 예측할 수 있으며, 이렇게 하여, 앞뒤 문맥의 단어의 지식 융합성을 학습할 수 있다. 도 3에 나타낸 훈련 어구와 같이, 단어 입도에 기반한 훈련의 경우, 랜덤으로 “하얼빈”, “흑룡강”, “성도”, “국제” 또는 “빙설” 등 단어를 숨기어, 시맨틱 표현 모델이 단어인 \"흑룡강\"과 \"하 얼빈\" 사이가 성도의 관계이고, \"하얼빈\"이 빙설로 유명한 문화 도시임 등을 학습하도록 한다. 구체적으로 훈련 할 때, 매 하나의 훈련 어구에 대래, 일정 비율의 단어를 랜덤으로 숨긴 후, 시맨틱 표현 모델이 숨긴 단어를 예측한다. 또한 예측이 정확한지 여부를 판단하며, 정확하지 않을 경우, 시맨틱 표현 모델의 매개 변수를 조정 하여, 예측한 단어와 숨긴 단어가 일치하도록 한다. 복수의 훈련 어구 및 라벨링한 단어를 사용하여, 연속된 미 리 설정한 횟수의 훈련에서 시맨틱 표현 모델의 정확도가 줄곧 미리 설정한 정확도 임계 값보다 클 때까지, 연 속적으로 시맨틱 표현 모델에 대해 훈련을 수행하는 바, 이 경우는 당해 시맨틱 표현 모델이 당해 능력의 학습 을 완성하여, 단어 입도에 기반한 훈련이 종료되었음을 나타낸다. 본 실시예의 미리 설정한 정확도는 실제 수요 에 따라 설정할 수 있는 바, 예를 들면 99%, 98% 또는 기타 백분비로 설정할 수 있다. 도 3 중의 Transform은 Transform 모델을 나타낸다. 본 실시예의 시맨틱 표현 모델은 Transform 모델을 통해 구현한다. 훈련 코퍼스 중에서 취득한 본 훈련 태스크에 적용되는 각 훈련 어구 및 라벨링한 실체를 사용하여, 실체 입도 에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행한다. 매 하나의 훈련 어구 및 당해 훈련 어구 중의 라벨링한 실체에 대해, 랜덤으로 일정 비율의 실체를 숨기어, 모델이 당해 훈련 어구 중의 기타 실체 등 앞뒤 문맥 정보 에 기반하여 당해 실체를 예측하도록 하며, 이렇게 하여, 앞뒤 문맥 중의 실체의 지식 융합성을 학습할 수 있다. 도 3에 나타낸 훈련 어구와 같이, 실체 입도에 기반한 훈련의 경우, “하얼빈”, “흑룡강” 중 임의의 하나의 실체를 랜덤으로 숨기어, 시맨틱 표현 모델이 실체 \"흑룡강\"과 실체 \"하얼빈\" 사이가 성도의 관계를 갖 음을 학습할 수 있도록 한다. 구체적으로 훈련할 때, 매 하나의 훈련 어구에 대해, 일정 비율의 실체를 랜덤으 로 숨긴 후, 시맨틱 표현 모델이 숨긴 실체를 예측한다. 또한 예측이 정확한지 여부를 판단하며, 정확하지 않을 경우, 시맨틱 표현 모델의 매개 변수를 조정하여, 예측한 실체와 숨긴 실체 일치하도록 한다. 복수의 훈련 어구 및 라벨링한 실체를 사용하여, 연속된 미리 설정한 횟수의 훈련에서 시맨틱 표현 모델의 정확도 줄곧 미리 설정 한 정확도 임계 값보다 클 때까지, 연속적으로 시맨틱 표현 모델에 대해 훈련을 수행하는 바, 이 경우는 당해 시맨틱 표현 모델이 당해 능력의 학습을 완성하여, 실체 입도에 기반한 훈련이 종료되었음을 나타낸다. 마찬가 지로, 본 실시예의 미리 설정한 정확도도 실제 수요에 따라 설정할 수 있는 바, 예를 들면 99%, 98% 또는 기타 백분비로 설정할 수 있다. 본 실시예의 구절에 기반한 훈련의 구현 원리는 단어 입도에 기반한 훈련 원리와 동일한 바, 상세한 내용은 상 술한 실시예의 관련 설명을 참고할 수 있으므로, 여기서 반복적으로 설명하지 않는다. 본 실시예의 시맨틱 표현 모델은, 단어 입도에 기반한 훈련, 구절 입도의 훈련 및 실체 입도의 훈련을 거쳤기에, 시맨틱 표현 모델의 시맨틱 표현 능력을 강화할 수 있으며, 따라서 본 실시예의 시맨틱 표현 모델은, 지식이 강화된 시맨틱 표현 모델（Enhanced Representation through kNowledge IntEgration; ERNIE）로 불리울 수도 있다. 본 실시예의 ERNIE 모델도 Transformer 모델에 기반하여 구현한 것이다. 본 실시예의 기술 방안을 채택하면, 시맨틱 표현 모델이 단어 입도에 기반한 훈련, 구절 입도의 훈련 및 실체 입도의 훈련을 거친 후, 앞뒤 문맥 중 글자 사이의 지식 융합성, 단어 사이의 지식 융합성 및 실체 사이의 지식 융합성을 학습할 수 있고, 시맨틱 표현 모델의 시맨틱 표현 능력을 강화할 수 있으며, 매 하나의 어구에 대해 정확하게 시맨틱 표현을 수행할 수 있다. 또한 본 실시예를 통해 훈련하여 얻은 시맨틱 표현 모델은 더욱 강한 범용성과 확장성을 가지며, 임의의 NLP의 태스크를 처리하는데 응용될 수 있다. 또한, 상술한 도 1에 나타낸 실시예 중의 방식(b）는 훈련 코퍼스를 사용하여 시맨틱 표현 모델을 훈련하여, 상 기 시맨틱 표현 모델이 서로 다른 언어 환경 하의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하는 능력을 학습하도록 한다. 예를 들면, 당해 능력은 영어 등 언어 환경 중에 적용될 수 있다. 서로 다른 언어 환경 시나리오 하에서, 일부 두문자는 대문자로 쓸 필요가 있고, 다른 일부 두문자는 대문자로 쓸 필요가 없다. 예를 들면, 영어에 있어서, 완전한 한마디 말의 경우, 문장이 시작되는 두문자는 대문자로 쓸 수 있으나, 앞이 쉼표일 경우는 한마디 말이 끝나지 않았음을 나타내므로, 그 후의 어휘의 두문자는 대문자로 쓸 필요가 없다. 다시 예를 들면, 일부 언어 환경에 있어서, 예를 들면 Apple, Harry Potter와 같은 일부 어휘가 브랜드 명칭 또는 사람 이름으로 출현할 때, 두문자를 대문자로 쓸 필요가 있다. 실제 응용에서 어휘의 두문자는 대문자로 쓸 필요가 있거나 또는 대문 자로 쓸 필요가 없는 등 많은 언어 환경이 존재한다. 본 실시예에 있어서, 시맨틱 표현 모델을 훈련하여, 어느 언어 환경 하에서 어휘의 두문자를 대문자로 쓸 필요가 있고, 어느 언어 환경 하에서 두문자를 대문자로 쓸 필요가 없는지를 자동적으로 인식할 수 있다. 마찬가지로, 우선 훈련 코퍼스 중에서 본 훈련 태스크에 적용되는 각 훈련 언어 자료를 취득할 필요가 있으며, 당해 태스크의 훈련 언어 자료 중에는 어휘의 두문자를 대문자로 쓸 필요가 있는 훈련 언어 자료와 어휘의 두문 자 대문자로 쓸 필요가 없는 훈련 언어 자료가 포함되지만, 당해 훈련 언어 자료 중에는 어휘뿐만이 아니라, 당 해 어휘의 앞의 한마디 말 등 어휘의 언어 환경도 포함될 필요가 있다. 훈련할 때, 훈련 언어 자료를 당해 시맨 틱 표현 모델에 입력하며, 시맨틱 표현 모델이 어느 어휘의 두문자를 대문자로 쓸 필요가 있거나 또는 대문자로 쓸 필요가 없는지를 예측한 후, 알려진 정확한 쓰는 법과 비교한다. 일치하지 않을 경우, 시맨틱 표현 모델의 매개 변수를 조정하여, 예측이 알려진 것과 일치하도록 한다. 취득한 각 훈련 언어 자료를 사용하여 상술한 방 식에 따라, 시맨틱 표현 모델의 예측 정확도가 99%, 98% 또는 기타 백분비와 같은 미리 설정한 정확도 임계 값 에 도달할 때까지, 시맨틱 표현 모델의 당해 능력을 연속적으로 훈련하는 바, 이 경우는 당해 시맨틱 표현 모델 이 당해 능력의 학습을 완성하여, 본 태스크의 훈련이 종료되었음을 나타낸다. 또한, 상술한 실시예 중의 방식(c）는 훈련 코퍼스를 사용하여 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 훈련 언어 자료 중의 어휘가 원본 문서의 기타 세그먼트에서 출현할지 여부를 예측하는 능력을 학습하도 록 한다. 당해 능력은 시맨틱 표현 모델로 하여금 어느 어휘가 문장의 중심 사상을 대표하는지를 예측할 수 있 도록 한다. 마찬가지로, 우선 훈련 코퍼스 중에서 본 훈련 태스크에 적용되는 각 훈련 언어 자료를 취득할 필요가 있으며, 당해 태스크의 훈련 언어 자료는 어구인 동시에, 당해 훈련 언어 자료 중 매 하나의 어휘가 원본 문서의 기타 세그먼트 중에서 출현한 적이 있는지를 나타내는 마크가 라벨링되어 있을 수 있다. 훈련할 때, 훈련 언어 자료 를 시맨틱 표현 모델에 입력하며, 당해 시맨틱 표현 모델이 당해 훈련 언어 자료 중의 각 어휘가 원본 문서의 기타 세그먼트에서 출현하는지 여부를 예측하여 출력한다. 그 후, 미리 라벨링한 당해 훈련 언어 자료 중의 각 어휘가 원본 문서의 기타 세그먼트에서 출현하였는지를 나타내는 마크에 기반하여, 시맨틱 표현 모델의 예측 정 확한지 여부를 판단하며, 정확하지 않을 경우, 시맨틱 표현 모델의 매개 변수를 조정하여, 예측한 것이 미리 라 벨링한 것과 일치해지도록 한다. 본 훈련 태스크의 각 훈련 언어 자료를 사용하여 상술한 방식에 따라, 시맨틱 표현 모델의 예측 정확도가 99%, 98% 또는 기타 백분비와 같은 미리 설정한 정확도 임계 값에 도달할 때까지, 연속적으로 시맨틱 표현 모델에 대해 훈련을 수행하는 바, 이 경우는 당해 시맨틱 표현 모델이 당해 능력의 학 습을 완성하여, 본 태스크의 훈련이 종료되었음을 나타낸다. 상술한 방식 (a）, (b） 및 (c）의 훈련은 무 감독의 태스크 훈련이다. 상술한 방식 (a）, (b） 및 (c）는 상술한 단계 S102 중의 훈련 코퍼스를 사용하여 형태론에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 여러 구현 방식이며, 실제 응용에 있어서, 기타 유사한 방식을 통해 형태론 에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행할 수 있는 바, 여기서 더는 하나씩 예를 들어 설명하지 않는 다. 또한 옵션으로서, 상술한 도 1에 나타낸 실시예 중의 단계 S102에서 훈련 코퍼스를 사용하여 문법에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것은, （A）훈련 코퍼스를 사용하여 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 훈련 언어 자료 중 서로 다른 세그먼트의 순서 관계를 인식하는 능력을 학습하도록 하는 것; 및 （B）훈련 코퍼스를 사용하여 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 임의의 두 개의 어구 쌍 의 위치 관계를 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 포함한다. 본 실시예의 방식 (A）과 방식 (B）는 문법 레벨의 태스크 훈련에 속한다. 본 실시예의 문법이 고려하는 것은 문장 사이의 관계이므로, 현재 훈련 태스크의 훈련 언어 자료 중에는 적어도 두 개의 문장이 포함되며, 훈련할 때 여기서의 임의의 두 개의 문장을 대상으로 훈련을 수행한다. 마찬가지로, 우선 훈련 코퍼스 중에서 현재 훈련 태스크에 필요한 모든 훈련 언어 자료를 취득할 필요가 있다. 현재 훈련 태스크의 훈련 언어 자료는 단락 또는 복수의 어구를 포함하는 세그먼트일 수 있으며, 우선 훈련 언 어 자료를 복수의 세그먼트로 나누고, 또한 랜덤으로 순서를 교란시킨 후, 그 중에서 임의의 두 개의 세그먼트 를 취득하며, 훈련 언어 자료에 기반하여 당해 두 개의 세그먼트의 순서 관계를 라벨링하여, 후속의 훈련에 사 용한다. 예를 들면, 시맨틱 표현 모델을 훈련하여 훈련 언어 자료 중 서로 다른 세그먼트의 순서 관계를 인식하 는 능력을 학습할 때, 훈련 언어 자료에 기반하여 취득한 두 개의 세그먼트를시맨틱 표현 모델에 입력하며, 당 해 시맨틱 표현 모델이 당해 두 개의 세그먼트의 순서 관계 즉 어느 세그먼트가앞에 있고, 어느 세그먼트가 뒤에 있는지를 예측하여 출력한다. 그 후, 알려진 두 개의 세그먼트의 순서 관계에 기반하여, 예측한 것과 알려진 것이 일치한지 여부를 판단하며, 일치하지 않을 경우, 시맨틱 표현 모델의 매개 변수를 조정하여, 예측한 것이 알려진 것과 일치해지도록 한다. 본 훈련 태스크의 각 훈련 언어 자료를 사용하여 상술한 방식에 따라, 시맨틱 표현 모델의 예측 정확도가 99%, 98% 또는 기타 백분비와 같은 미리 설정한 정확도 임계 값에 도달할 때까지, 연속적으로 시맨틱 표현 모델에 대해 훈련을 수행하는 바, 이 경우는 당해 시맨틱 표현 모델이 당해 능력의 학 습을 완성하여, 본 태스크의 훈련이 종료되었음을 나타낸다. 본 태스크의 훈련을 통해, 시맨틱 표현 모델로 하 여금 세그먼트 사이의 순서 및 인과 관계를 학습하도록 함으로써, 서로 다른 세그먼트의 순서 관계를 정확히 인 식하는 능력을 갖도록 한다. 당해 훈련 태스크는 문장의 순서 배열 태스크로 불리울 수도 있다. 본 실시예에 있어서, 시맨틱 표현 모델을 훈련하여 임의의 두 개의 어구 쌍의 위치 관계를 인식하는 능력을 학 습할 때, 3개의 분류 태스크를 설정하여, 어구 쌍 사이의 위치 관계가 근접한 문장, 문서 내에서 근접하지 않은 문장, 및 동일한 문서 내에 존재하지 않는 문장과 같은 세가지 유형인지를 판단한다. 물론 실제 응용에 있어서, 수요에 따라 더 많은 분류를 설정하여, 시맨틱 표현 모델로 하여금 문장 정보를 더 잘 이용하여 시맨틱 상관성 을 학습하도록 할 수 있다. 마찬가지로, 우선 훈련 코퍼스 중에서 현재 훈련 태스크에 필요한 모든 훈련 언어 자료를 취득할 필요가 있다. 현재 훈련 태스크의 훈련 언어 자료는 어구 쌍이며, 훈련 언어 자료를 풍부히 하기 위하여, 본 실시예의 훈련 언어 자료는 동일한 문서 내의 두 개의 근접한 문장을 취득하거나, 동일한 문서 내의 근접하지 않은 두 개의 문 장을 취득하거나, 동일하지 않은 문서 내의 두 개의 문장을 취득할 수 있으며, 각종의 서로 다른 훈련 언어 자 료는 미리 설정한 비율에 따라 사전에 수집하고, 두 개의 문장의 위치 관계가 동일 문서 내의 근접한 문장, 동 일 문서 내의 근접하지 않은 문장 또는 동일하지 않은 문서 내의 두 개의 문장인지를 라벨링한다. 훈련할 때, 당해 태스크의 훈련 언어 자료 중의 두 개의 문장을 시맨틱 표현 모델에 입력하며, 시맨틱 표현 모델이 이 두 개의 문장의 위치 관계를 예측하여 출력한 후, 예측한 위치 관계와 알려진 위치 관계가 일치한지 여부를 비료하 며, 일치하지 않을 경우, 시맨틱 표현 모델의 매개 변수를 조정하여, 예측한 것이 알려진 것과 일치해지도록 한 다. 본 훈련 태스크의 각 훈련 언어 자료를 사용하여 상술한 방식에 따라, 시맨틱 표현 모델의 예측 정확도가 99%, 98% 또는 기타 백분비와 같은 미리 설정한 정확도 임계 값에 도달할 때까지, 연속적으로 시맨틱 표현 모델 에 대해 훈련을 수행하는 바, 이 경우는 당해 시맨틱 표현 모델이 당해 능력의 학습을 완성하여, 본 태스크의 훈련이 종료되었음을 나타낸다. 상술한 방식 (A）와 (B）는 무 감독 훈련이다. 상술한 방식 (A）와 (B）는 단계 S102 중의 훈련 코퍼스를 사용하여 문법에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 여러 구현 방식이며, 실제 응용에 있어서, 기타 유사한 방식을 통해 문법에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행할 수 있는 바, 여기서 더는 하나씩 예를 들어 설명하지 않는다. 또한 옵션으로서, 상술한 도 1에 나타낸 실시예 중의 단계 S102에서 훈련 코퍼스를 사용하여 시맨틱에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것은, （1） 훈련 코퍼스를 사용하여 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 연속된 두 개의 어구 사 이의 논리적 관계를 인식하는 능력을 학습하도록 하는 것; 및 （2） 훈련 코퍼스를 사용하여 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 훈련 언어 자료 중의 검 색 어구(Query）와 웹 페이지 주제（Title） 사이의 상관성을 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 포함한다. 본 실시예의 방식 (1）과 방식 (2）은 시맨틱 레벨의 태스크 훈련에 속하며, 문장의 시맨틱 분석에 기반하여, 문장의 논리적 관계 태스크와 검색 상관성 태스크를 각각 학습한다. 예를 들면, 연속된 두 개의 어구 사이의 논리적 관계를 인식하는 능력을 학습하면, 두 개의 문장 사이가 “하지 만”, “만약”, “그러나”, “또한” 등 접속사를 통해 연결되었는지 여부를 학습하여, 두 개의 문장 사이의 논리적 관계를 명확히 한다. 마찬가지로, 우선 훈련 코퍼스 중에서 현재 훈련 태스크에 필요한 모든 훈련 언어 자료를 취득할 필요가 있다. 당해 훈련 태스크의 훈련 언어 자료는 각 원본 문서 중에서 \"하지만\", \"만약\", \"그러나\", \"또한\" 등 접속사를 통해 연결된 두 개의 어구를 수집할 수 있다. 물론, 접속사가 없는 일부 두 개의 어구를 수집하여, 훈련 샘플의 부분적 예로 설정할 수 있다. 훈련할 때, 훈련 언어 자료 중의 두 개의 어구를 시맨틱 표현 모델에 입력하며, 당해 시맨틱 표현 모델이 이 두 개의 어구의 논리적 관계를 예측하여 출력하는 바, 예를 들면 서로 다른 마크를사용하여 각각 \"하지만\", \"만약\", \"그러나\", \"또한\" 등 논리적 관계를 라벨링한다. 그 후, 예측한 논리적 관계 와 알려진 논리적 관계가 일치한지 여부를 판단하며, 일치하지 않을 경우, 시맨틱 표현 모델의 매개 변수를 조 정하여, 예측한 것이 알려진 것과 일치해지도록 한다. 본 훈련 태스크의 각 훈련 언어 자료를 사용하여 상술한 방식에 따라, 시맨틱 표현 모델의 예측 정확도가 99%, 98% 또는 기타 백분비와 같은 미리 설정한 정확도 임계 값에 도달할 때까지, 연속적으로 시맨틱 표현 모델에 대해 훈련을 수행하는 바, 이 경우는 당해 시맨틱 표현 모 델이 당해 능력의 학습을 완성하여, 본 태스크의 훈련이 종료되었음을 나타낸다. 예를 들면, 훈련 언어 자료 중의 검색 어구(Query）와 웹 페이지 주제（Title） 사이의 상관성을 인식하는 능력 을 학습하면, Query와 Title의 관계 유형을 학습할 수 있으며, 예를 들면 유형 0은 강 관련인 바, 이 경우는 유 저가 Query를 검색한 후 당해 Title를 클릭하였음을 나타낼 수 있다. 유형 1은 약 관련인 바, 이 경우는 유저가 Query를 검색한 후 펼쳐보였으나 Title를 클릭하지 않았음을 나타낼 수 있다. 유형 2는 비 관련인 바, 이 경우 는 유저가 Query를 검색한 후, 당해 Title를 펼쳐보이지 않았음을 나타낼 수 있다. 마찬가지로, 우선 훈련 코퍼스 중에서 현재 훈련 태스크에 필요한 모든 훈련 언어 자료를 취득할 필요가 있다. 당해 훈련 태스크의 훈련 언어 자료는 검색 일지 중에서 취득할 수 있으며, 구체적으로 매 일회 검색한 Query와 검색하여 얻은 매 하나의 Title을 취득할 수 있다. 그 후, 매 회 검색한 Query와 Title를 랜덤으로 조합하여, 본 훈련 태스크의 훈련 언어 자료를 구성할 수 있으며, 동시에 훈련 태스크 중에 당해 Query와 Title의 강 관련, 약 관련 또는 비 관련과 같은 관계를 라벨링한다. 훈련할 때, 훈련 언어 자료 중의 Query와 Title을 시맨 틱 표현 모델에 입력하며, 당해 시맨틱 표현 모델이 Query와 Title의 관계를 예측하여 출력하며, 그 후, 예측한 관계와 알려진 관계가 일치한지 여부를 판단하며, 일치하지 않을 경우, 시맨틱 표현 모델의 매개 변수를 조정하 여, 예측한 것이 알려진 것과 일치해지도록 한다. 본 훈련 태스크의 각 훈련 언어 자료를 사용하여 상술한 방식 에 따라, 시맨틱 표현 모델의 예측 정확도가 99%, 98% 또는 기타 백분비와 같은 미리 설정한 정확도 임계 값에 도달할 때까지, 연속적으로 시맨틱 표현 모델에 대해 훈련을 수행하는 바, 이 경우는 당해 시맨틱 표현 모델이 당해 능력의 학습을 완성하여, 본 태스크의 훈련이 종료되었음을 나타낸다. 상술한 방식 (1）은 무 감독 훈련이고, 상술한 방식 (2）는 약 감독 훈련이다. 상술한 방식 (1）과 (2）는 단계 S102 중의 훈련 코퍼스를 사용하여 시맨틱에 기반하여 시맨틱 표현 모델에 대 해 훈련을 수행하는 여러 구현 방식이며, 실제 응용에 있어서, 기타 유사한 방식을 통해 문법에 기반하여 시맨 틱 표현 모델에 대해 훈련을 수행할 수 있는 바, 여기서 더는 하나씩 예를 들어 설명하지 않는다. 본 실시예에 있어서, 시맨틱 표현 모델이 충분히 풍부한 능력을 학습할 수 있도록 보증하기 위하여, 매 하나의 능력의 학습에 대해, 훈련 코퍼스 중에 대응되는 태스크의 훈련 언어 자료의 수량도 반드시 충분히 강대하여야 하는 바, 예를 들면 백만급 이상에 달하여, 시맨틱 표현 모델의 학습 효과를 보증할 수 있다. 본 실시예의 시맨틱 표현 모델은 Transformer을 저층의 Encoder 구조로 사용함을 설명할 필요가 있다. 저층에서 단어 표시(Token Embedding), 위치 표시(Positional Embedding) 및 단락 표시(Sentence Embedding)를 입력할 수 있으며, 기타 시맨틱 표현 모델과 다른 것은, Task Embedding(태스크 임베딩）을 동시에 도입하여 서로 다른 태스크를 세밀하게 모델링하는데 사용하며, 서로 다른 훈련 태스크는 0부터 N까지의 ID를 사용하여 표시한다. 이렇게 하여, 각 훈련 언어 자료가 입력된 후, 이에 대응되는 태스크 마크에 기반하여, 어느 태스크의 훈련에 사용되는지를 확정하여, 훈련할 때 일 세트의 훈련 언어 자료를 랜덤으로 입력하여, 멀티 태스크의 동시 훈련을 구현함으로써, 시맨틱 표현 모델의 훈련 속도를 제고하여, 시맨틱 표현 모델의 훈련 효과를 향상시킬 수 있다. 도 4는 본원에 의해 제공되는 시맨틱 표현 모델의 멀티 태스크 학습의 아키텍처 다이어그램이다. 도 5는 본원에 의해 제공되는 시맨틱 표현 모델의 응용 아키텍처의 모식도이다. 도 4와 도 5에 나타낸 바와 같이, 본 실시예의 시맨틱 표현 모델은 훈련할 때, 특수한 [CLS]를 입력한 특수한 시작 마크로 사용하여, 전반 시맨틱을 모델링한다. 훈련할 때 복수의 훈련 언어 자료의 세그먼트의 입력이 있을 경우, [SEP]를 사용하여 분할하며, Sentence Embedding은 서로 다른 id를 사용하여 표시된다. Transformer의 멀티 층 Attention 메커니즘을 통해, 최상 층의 앞뒤 문맥 관련 표시를 계산하여 얻는다. Sequence 전반 레벨의 Loss에 대해, [CLS]의 최상 층 표시에 여거 분류기를 추가한다. Token 레벨의 태스크에 대해, 시퀀스 중의 매 하나의 Token 상에 여러 Token 레벨 분류 태스크를 추가한다. 도 4에 나타낸 실시예에서 두 가지 레벨의 손실 함수(Loss）가 모두 세 가지 서로 다른 방식을 통해 계산하는 예를 들었으며, 실제 응용에 있어서, 하나, 둘 또 는 복수의 손실 함수의 계산 방식을 사전에 정의할 수 있다. 그 후, 다시 각 계산 방식에 대응되는 손실 함수를 통해, 미리 설정한 수학식을 통해 각 레벨의 최종 손실 함수를 계산하여 얻을 수 있다. 이해의 편의를 위하여, Token을 하나의 문장의 하나의 세그먼트로 이해할 수 있으며, 구체적으로, 당해 Token는 하나의 글자, 단어, 구절 또는 실체 등일 수 있는 바, 훈련 태스크에 따라 서로 다를 수 있다. Token이 대응되는 훈련 태스크를 갖는 경우, 이때 대응하는 손실 함수가 존재하며, 태스크가 없을 경우, 대응하는 손실 함수가 없다. 도 4에서 4개의 Token를 포함하는 예를 들었으나, 실제 응용에 있어서 Token의 수량은 문장의 길이에 따라 변화하는 바, 여기서 한정하지 않는다. 도 5에 나타낸 바와 같이, 제1 층은 시맨틱 표현 모델의 응용 태스크의 시나리오이고, 아래는 시맨틱 표현 모델 의 멀티 태스크 훈련 아키텍처이며, 멀티 태스크를 동시에 사용하여 시맨틱 표현 모델을 훈련할 때, 복수의 훈 련 언어 자료를 동시에 입력할 수 있으며, 각 훈련 언어 자료는 하나의 어구일 수 있다. 도 5에 나타낸 바와 같 이, 훈련할 때, 훈련 언어 자료가 적용되는 태스크의 유형의 임베딩 정보（Task Embedding）, 각 token의 위치 임베딩 정보(Position Embedding） 및 대응하는 어구 임베딩 정보(Sentence embedding）를 더 입력할 필요가 있으며, 도 5 에서는 3개의 어구 A, B 및 C를 동시에 입력하여, 모두 Task 유형이 3인 태스크의 훈련의 예로 한 다. Token Embedding을 입력할 때, CLS를 시작 입력으로 하며, 서로 다른 훈련 언어 자료의 세그먼트 사이는 SEP 간격을 사용한다. 매 하나의 훈련 언어 자료는 복수의 token 세그먼트로 나뉘어 입력될 수 있으며, 도 5의 실시예에서 3개의 세그먼트의 예를 들었지만, 실제 응용에 있어서 훈련 언어 자료의 길이 및 훈련 태스크가 수 행할 필요가 있는 절분에 따라 서로 다른 바, 여기서 한정하지 않는다. 모든 정보를 입력한 후, 상술한 실시예 의 각 태스크의 훈련 방식에 따라 훈련을 수행하여, 훈련된 시맨틱 표현 모델을 최종적으로 얻는다. 본원의 상술한 실시예를 통해 훈련한 시맨틱 표현 모델은, 하나의 범용의 시맨틱 표현 모델이며, 당해 시맨틱 표현 모델이 상술한 각 능력의 학습을 거쳤기에, 당해 시맨틱 표현 모델로 하여금 앞뒤 문맥 지식 융합성을 학 습할 수 있을뿐만 아니라, 각 형태론, 문법 및 시맨틱 지식을 학습할 수 있도록 하여, 시맨틱을 더욱 정확하게 표시할 수 있다. 또한 옵션으로서, 상술한 실시예의 단계 S102에서 훈련 코퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행한 후, 시맨틱 표현 모델의 응용을 더 포함할 수 있는 바, 구체적으로, 사전에 수집한 자연 언어 처리（Natural Language Processing; NLP）의 태스크 코퍼스에 기반 하여, 시맨틱 표현 모델에 대해 훈련을 수행하여, 대응하는 NLP의 태스크 모델을 얻는 것; 및 NLP의 태스크 모 델에 기반하여 NLP의 태스크를 실행하는 것을 더 포함할 수 있다. 본 실시예의 시맨틱 표현 모델에 있어서, 상술한 각종 태스크의 훈련과 학습을 거친 후 얻은 시맨틱 표현 모델 은 하나의 범용의 시맨틱 표현 모델이며, 훈련할 때 필요한 데이터 양이 매우 큰 바, 예를 들면 각 태스크의 학 습에 필요한 데이터 양이 백만급 이상에 달할 수 있다. 상술한 훈련을 통해 얻은 시맨틱 표현 모델은 아직 NLP 태스크를 처리하는데 직접 사용될 수 없다. 사용하기 전에, 먼저 NLP의 태스크 코퍼스를 사용하여 당해 시맨틱 표현 모델에 대해 훈련을 수행하여야만, 대응하는 NLP의 태스크에 기반한 모델을 얻어, 대응하는 NLP 태스크의 처리를 수행할 수 있다. 즉 본 실시예의 NLP의 태스크 코퍼스는 범용의 시맨틱 표현 모델에 대해 훈련을 수행하 는데 사용되며, 하나의 대응하는 NLP의 태스크 모델로 훈련한다. 당해 태스크 코퍼스는, 범용의 시맨틱 표현 모 델을 훈련하는 훈련 코퍼스와 비교하면, 단지 하나의 소규모의 태스크 세트이다. 여기서, 훈련 코퍼스는 단지 시맨틱 표현 모델의 시맨틱 표현 능력을 훈련하는데 사용되며, 태스크와 어떠한 관계도 없다. 태스크 코퍼스는 태스크와 관련된 능력 훈련인 바, 시맨틱 표현 능력을 훈련할 필요가 없다. 본 실시예를 통해 얻은 범용의 시맨 틱 표현 모델은 이미 매우 정확하게 시맨틱 표현을 수행할 수 있으며, 사용할 때 소규모의 태스크 세트만을 사 용하여 당해 시맨틱 표현 모델을 대응하는 태스크 모델로 훈련할 수 있다. 도 5에 나타낸 바와 같이, 상술한 훈련을 거친 후의 시맨틱 표현 모델은, 대응하는 태스크 세트를 사용하여 훈 련을 수행하여, 텍스트 유사도, 스마트 문답, 감정 분석, 자연 언어 추론 등 태스크의 처리를 각각 구현할 수 있는 바, 여기서 더는 하나씩 예를 들어 설명하지 않는다. 본 실시예의 시맨틱 표현 모델의 처리 방법, 복수의 훈련어를 포함하는 훈련 코퍼스를 수집하고; 또한 훈련 코 퍼스를 사용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행함 으로써, 훈련 후의 시맨틱 표현 모델로 하여금 형태론, 문법 및 시맨틱 등 각 등급의 정보를 충분히 학습할 수 있도록 하여, 시맨틱 표현 모델의 시맨틱 표현 능력을 풍부히 하였고, 시맨틱 표현의 정확성을 향상시켰다. 또한, 본 실시예에 있어서, 시맨틱 표현 모델에 대해 형태론에 기반한 태스크 훈련을 수행함으로써, 시맨틱 표 현 모델로 하여금 단어, 구절 및 실체의 공기 지식을 학습하고, 지식의 융합성을 이해할 수 있도록 하여, 시맨 틱 표현 모델의 시맨틱 표현 능력을 강화할 수 있고, 매 하나의 어구에 대해 정확하게 시맨틱 표현을 수행할 수 있다. 이와 동시에, 또한 서로 다른 언어 환경 하의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하 는 능력을 학습할 수 있기에, 서로 다른 언어 환경 하의 어휘의 정확한 표현 방식을 정확히 학습할 수 있고; 또한, 어휘가 원본 문서의 기타 세그먼트의 공기할지를 예측하는 능력을 학습할 수 있기에, 문서 중에서 어떠한 어휘가 문서의 중심 사상을 나타낼 수 있는지를 예측할 수 있다. 상술한 형태론에 기반한 태스크 훈련을 통해, 시맨틱 표현 모델로 하여금 풍부한 형태론 지식을 학습할 수 있도록 하고, 형태론이 표시하는 의미를 충분히 이 해하도록 하여, 더욱 정확하게 시맨틱 표현을 수행할 수 있다. 또한, 본 실시예에 있어서, 또한 시맨틱 표현 모델에 대해 문법에 기반한 태스크 훈련을 수행함으로써, 시맨틱 표현 모델로 하여금 문장의 배열을 학습할 수 있도록 하고, 또한 서로 다른 문장의 위치 관계를 인식할 수 있도 록 하여, 시맨틱 표현 과정에서 매 하나의 문장에 대해 정확히 위치 확정을 수행하여, 시맨틱 표현의 정확성을 향상시켰다. 또한, 본 실시예에 있어서, 또한 시맨틱 표현 모델에 대해 시맨틱에 기반한 태스크 훈련을 수행함으로써, 시맨 틱 표현 모델로 하여금 문장의 논리적 관계 태스크 및 검색 상관성 태스크를 학습할 수 있도록 하여, 시맨틱 표 현 시, 시맨틱을 정확히 이해하고, 시맨틱 표현의 정확성을 강화할 수 있다. 총괄적으로, 본 실시예에 있어서, 형태론, 문법, 시맨틱 이러한 세 개의 서로 다른 레벨의 무 감독 또는 약 감 독의 사전 훈련 태스크를 구축함으로써, 시맨틱 표현 모델이 대규모 데이터 중에서 어휘, 문법, 시맨틱과 같은 서로 다른 등급의 지식을 학습하도록 하여, 범용 시맨틱 표현의 능력을 강화하였고, NLP 태스크의 처리 효과를 향상시켰다. 도 6은 본원에 따른 제3 실시예의 모식도이다. 도 6에 나타낸 바와 같이, 본 실시예의 시맨틱 표현 모델의 처리 장치의 구성을 설명하는바, 본 실시예의 시맨틱 표현 모델의 처리 장치는, 복수의 훈련 언어 자료를 포함하는 훈련 코퍼스를 수집하기 위한 수집 모듈; 및 수집 모듈이 수집한 훈련 코퍼스를 사용하여, 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨 틱 표현 모델에 대해 훈련을 수행하기 위한 시맨틱 표현 모델 훈련 모듈을 구비한다. 도 7은 본원에 따른 제4 실시예의 모식도이다. 도 7에 나타낸 바와 같은 시맨틱 표현 모델의 처리 장치는, 상술한 도 6에 나타낸 바와 같은 실시예의 기초 상에서, 시맨틱 표현 모델 훈련 모듈이 형태론 훈련 유닛 , 문법 훈련 유닛 및 시맨틱 훈련 유닛을 구비하는 예이다. 예를 들면, 형태론 훈련 유닛은, 수집 모듈이 수집한 훈련 코퍼스를 사용하여, 단어, 구절 및/또는 실체의 마스킹 책략에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행하는 것; 수집 모듈이 수집한 훈련 코퍼스를 사용하여, 시맨틱 표현 모델을 훈련하여, 상기 시맨틱 표현 모델이 서 로 다른 언어 환경 하의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하는 능력을 학습하도록 하는 것; 및 수집 모듈이 수집한 훈련 코퍼스를 사용하여, 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 훈 련 언어 자료 중의 어휘가 원본 문서의 기타 세그먼트에서 출현할지 여부를 예측하는 능력을 학습하도록 하는 것 중 적어도 하나를 실행한다. 예를 들면, 문법 훈련 유닛은, 수집 모듈이 수집한 훈련 코퍼스를 사용하여, 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 훈 련 언어 자료 중 서로 다른 세그먼트의 순서 관계를 인식하는 능력을 학습하도록 하는 것; 및 수집 모듈이 수집한 훈련 코퍼스를 사용하여, 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 임 의의 두 개의 어구 쌍의 위치 관계를 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 실행한다. 예를 들면, 시맨틱 훈련 유닛은, 수집 모듈이 수집한 훈련 코퍼스를 사용하여, 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 연 속된 두 개의 어구 사이의 논리적 관계를 인식하는 능력을 학습하도록 하는 것; 및 수집 모듈이 수집한 훈련 코퍼스를 사용하여, 시맨틱 표현 모델을 훈련하여, 당해 시맨틱 표현 모델이 상 기 훈련 언어 자료 중의 검색 어구와 웹 페이지 주제 사이의 상관성을 인식하는 능력을 학습하도록 하는 것 중 적어도 하나를 실행한다.또한 옵션으로서, 도 7에 나타낸 바와 같이, 본 실시예의 시맨틱 표현 모델의 처리 장치는, 사전에 수집한 자연 언어 처리의 태스크 코퍼스에 기반하여 시맨틱 표현 모델 훈련 모듈이 훈련하여 얻은 시맨틱 표현 모델에 대해 훈련을 수행하여, 대응하는 자연 언어 처리의 태스크 모델을 얻기 위한 태스크 모델 훈련 모듈; 및 태스크 모델 훈련 모듈이 훈련하여 얻은 자연 언어 처리의 태스크 모델에 기반하여 자연 언어 처리의 태스 크를 실행하기 위한 실행 모듈을 더 구비한다. 본 실시예의 상술한 시맨틱 표현 모델의 처리 장치는 상술한 모듈을 통해 시맨틱 표현 모델의 처리를 구현하며, 그 구현 원리 및 기술 효과 상술한 관련된 방법 실시예의 구현과 동일하기에, 상세한 내용은 상술한 관련된 방 법 실시예의 기재를 참고할 수 있는 바, 여기서 반복적으로 설명하지 않는다. 본원의 실시예에 따르면, 본원은 전자 기기 및 판독 가능 저장 매체를 더 제공한다. 도 8에 나타낸 바와 같이, 도 8은 본원 실시예에 의해 제공되는 시맨틱 표현 모델의 처리 방법을 구현하는 전자 기기의 모식도이다. 전자 기기는 랩톱 컴퓨터, 데스크톱 컴퓨터, 워크 벤치, 개인 디지털 비서, 서버, 블레이드 서버, 메인 프레임 컴퓨터 및 기타 적합한 컴퓨터와 같은 다양한 형태의 디지털 컴퓨터를 나타낼 수 있다. 전자 기기는 또한 개인 디지털 처리, 셀룰러 폰, 스마트 폰, 웨어러블 장치 및 다른 유사한 컴퓨팅 장치와 같은 다양 한 형태의 모바일 장치를 나타낼 수 있다. 본 명세서에 나타낸 구성 요소, 이들의 연결과 관계, 및 이들의 기능 은 단지 예일뿐이며, 본 명세서 중의 설명 및/또는 요구하는 본원의 구현을 한정하려는 것이 아니다. 도 8에 나타낸 바와 같이, 당해 전자 기기는 하나 또는 복수의 프로세서, 메모리, 및 고속 인터페이 스와 저속 인터페이스를 포함하는 각 구성 요소를 연결하기 위한 인터페이스를 구비한다. 각 구성 요소는 서로 다른 버스를 이용하여 서로 연결되며, 공통 마더 보드에 장착하거나 필요에 따라 기타 방식으로 장착될 수 있다. 프로세서는 전자 기기 내에서 수행되는 명령을 처리하며, 당해 명령은 메모리 내 또는 메모리 상에 저장 되어, 외부 입력/출력 장치(예를 들면 인터페이스에 연결된 디스플레이 기기） 상에 그래픽 사용자 화면을 디스 플레이하기 위한 그래픽 정보의 명령을 포함한다. 기타 실시 방식에 있어서, 필요할 경우, 복수의 프로세서 및/ 또는 복수의 버스와 복수의 메모리를 복수의 메모리와 함께 사용할 수 있다. 마찬가지로, 복수의 전자 기기(예 를 들면, 서버 어레이, 블레이드 서버 그룹, 또는 멀티 프로세서 시스템을 연결함）를 연결할 수 있으며, 각 기 기는 필요한 일부 동작을 제공한다. 도 8에서는 하나의 프로세서의 예를 든다. 메모리는 본원에 의해 제공되는 비 일시적 컴퓨터 판독 가능 저장 매체이다. 여기서, 상기 메모리에는 적 어도 하나의 프로세서에 의해 수행되는 명령이 저장되어 있으며, 상기 적어도 하나의 프로세서로 하여금 본원에 의해 제공되는 방법을 수행하도록 한다. 본원의 비 일시적 컴퓨터 판독 가능 저장 매체는 컴퓨터 명령을 저장하 며, 당해 컴퓨터 명령은 컴퓨터로 하여금 본원에 의해 제공되는 시맨틱 표현 모델의 처리 방법을 수행하도록 한 다. 메모리는 비 일시적 컴퓨터 판독 가능 저장 매체로서, 본원 실시예 중의 시맨틱 표현 모델의 처리 방법에 대응하는 프로그램 명령/모듈（예를 들면, 도 2에 나타낸 준비 유닛 및 번역 유닛）과 같은, 비 일시 적 소프트웨어 프로그램, 비 일시적 컴퓨터 수행 가능 프로그램 및 모듈을 저장하는데 사용될 수 있다. 프로세 서는 메모리 내에 저장된 비 일시적 소프트웨어 프로그램, 명령 및 모듈을 실행함으로써, 서버의 각 기능 애플리케이션 및 데이터 처리를 수행하는 바, 즉 상술한 방법 실시예 중의 방법을 실현한다. 메모리는 프로그램 저장 영역과 데이터 저장 영역을 포함할 수 있으며, 여기서 프로그램 저장 영역은 운영 체제, 적어도 하나의 기능에 필요한 애플리케이션 프로그램을 저장할 수 있으며; 데이터 저장 영역은 전자 기기 의 사용에 따라 생성된 데이터 등을 저장할 수 있다. 또한, 메모리는 고속 랜덤 액세스 메모리를 포함할 수 있고, 또한 비 일시적 메모리를 포함할 수 있는 바, 예를 들면 적어도 하나의 자기 디스크 메모리 장치, 플 래시 메모리 장치, 또는 기타 비 일시적 고체 메모리 장치를 포함할 수 있다. 일부 실시예에 있어서, 메모리 는 프로세서에 대해 원격 설치된 메모리를 선택적으로 포함할 수 있으며, 이러한 원격 메모리는 네트 워크를 통해 전자 기기에 연결된다. 상술한 네트워크의 예는 인터넷, 인트라넷, 근거리 네트워크, 모바일 통신 네트워크 및 이들의 조합을 포함하나 이에 한정되지 않는다. 시맨틱 표현 모델의 처리 방법의 전자 기기는 입력 장치와 출력 장치를 더 구비할 수 있다. 프로세서 , 메모리, 입력 장치 및 출력 장치는 버스 또는 기타 방식으로 연결될 수 있으며, 도 8에 서는 버스를 통해 연결한 예를 든다.입력 장치는 입력된 숫자 또는 문자 정보를 수신할 수 있고, 또한 전자 기기의 사용자 설정 및 기능 제어 와 관련된 키 신호 입력을 생성할 수 있으며, 당해 입력 장치는 예를 들면 터치 스크린, 키패드, 마우스, 트랙 패드, 터치 패드, 포인팅 스틱, 하나의 또는 복수의 마우스 버튼, 트랙볼, 조이스틱 등과 같은 입력 장치일 수 있다. 출력 장치는 디스플레이 기기, 보조 조명 장치(예를 들면 LED） 및 햅틱 피드백 장치（예를 들면 진 동 모터） 등을 포함할 수 있다. 당해 디스플레이 기기는 액정 디스플레이(LCD）, 발광 다이오드 디스플레이 (LED） 및 플라즈마 디스플레이를 포함하나 이에 한정되지 않는다. 일부 실시 방식에 있어서, 디스플레이 기기 는 터치 스크린일 수 있다. 여기서 설명되는 시스템과 기술의 각 실시 방식은, 디지털 전자 회로 시스템, 직접 회로 시스템, 전용 직접 회 로(ASIC）, 컴퓨터 하드웨어, 펌웨어, 소프트웨어, 및/또는 이들의 조합에서 실현될 수 있다. 이러한 각 실시 방식은 하나의 또는 복수의 컴퓨터 프로그램 중에서 실시되며, 당해 하나의 또는 복수의 컴퓨터 프로그램은 적 어도 하나의 프로그램 가능 프로세서를 포함하는 프로그램 가능 시스템 상에서 수행 및/또는 해석될 수 있으며, 당해 프로그램 가능 프로세서는 전용 또는 공통 프로그램 가능 프로세서일 수 있으며, 저장 시스템, 적어도 하 나의 입력 장치 및 적어도 하나의 출력 장치로부터 데이터와 명령을 수신할 수 있으며, 또한 데이터와 명령을 당해 저장 시스템, 당해 적어도 하나의 입력 장치 및 당해 적어도 하나의 출력 장치에 전송할 수 있다. 이러한 계산 프로그램（프로그램, 소프트웨어, 소프트웨어 애플리케이션 또는 코드라고도 함）는 프로그램 가능 프로세서의 기계 명령을 포함하며, 또한 고급 프로세스 및/또는 객체 지향 프로그래밍 언어, 및/또는 어셈블리/ 기계 언어를 이용하여 이러한 계산 프로그램을 실시할 수 있다. 예를 들면 본 명세서에서 사용되는 용어인 “기 계 판독 가능 매체” 및 “컴퓨터 판독 가능 매체”는, 기계 명령 및/또는 데이터를 프로그램 가능 프로세서에 제공하는 임의의 컴퓨터 프로그램 제품, 기기, 및/또는 장치（예를 들면, 자기 디스크, 광학 디스크, 메모리, 프로그램 가능 논리 장치(PLD））를 가르키며, 기계 판독 가능 신호로 수신하는 기계 명령의 기계 판독 가능 매 체를 포함한다. 용어인 “기계 판독 가능 신호”는 기계 명령 및/또는 데이터를 프로그램 가능 프로세서에 제공 하는 임의의 신호를 가르킨다. 사용자와의 대화를 제공하기 위하여, 컴퓨터 상에서 여기서 설명되는 시스템과 기술을 실시할 수 있으며, 당해 컴퓨터는 사용자에게 정보를 디스플레이 하기 위한 디스플레이 장치（예를 들면, 음극선 관(CRT） 또는 액정 디 스플레이(LCD） 모니터）; 및 키보드와 포인팅 장치(예를 들면, 마우스 또는 트랙볼）를 구비할 수 있으며, 사 용자는 당해 키보드와 포인팅 장치를 통해 입력을 컴퓨터에 제공할 수 있다. 기타 종류의 장치는 또한 사용자와 의 대화를 제공할 수 있는 바; 예를 들면, 사용자에게 제공하는 피드백은 임의의 형식의 감각 피드백(예를 들면, 시각 피드백, 청각 피드백, 또는 촉각 피드백）일 수 있고; 또한 임의의 형식(음향 입력, 음성 입력 또는 촉각 입력）을 통해 사용자의 입력을 수신할 수 있다. 여기서 설명하는 시스템과 기술을, 백엔드 구성 요소를 포함하는 계산 시스템（예를 들면, 데이터 서버）, 또는 미들웨어 구성 요소를 포함하는 계산 시스템（예를 들면, 애플리케이션 서버）, 또는 프런트 엔드 구성 요소를 포함하는 계산 시스템（예를 들면, 그래픽 사용자 화면 또는 네트워크 브라우저를 구비하는 사용자 컴퓨터임, 사용자는 당해 그래픽 사용자 화면 또는 당해 네트워크 브라우저를 통해 여기서 설명하는 시스템과 기술의 실시 방식과 대화할 수 있음）, 또는 이러한 백엔드 구성 요소, 미들웨어 구성 요소, 또는 프런트 엔드 구성 요소를 포함하는 임의의 조합의 계산 시스템에서 실시할 수 있다. 임의의 형식 또는 매체의 디지털 데이터 통신(예를 들면, 통신 네트워크）을 통해 시스템의 구성 요소를 서로 연결할 수 있다. 통신 네트워크의 예는 근거리 네트 워크(LAN）, 광역 통신망(WAN） 및 인터넷을 포함한다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트와 서버는 일반적으로 서로 멀리 떨어져 있 으며, 일반적으로 통신 네트워크를 통해 대화한다. 해당하는 컴퓨터 상에서 서로 클라이언트-서버 관계를 갖는 컴퓨터 프로그램을 수행하여 클라이언트와 서버의 관계를 구현한다. 본원 실시예의 기술 방안에 따르면, 복수의 훈련어를 포함하는 훈련 코퍼스를 수집하고; 또한 훈련 코퍼스를 사 용하여 형태론, 문법 및 시맨틱 중의 적어도 하나에 기반하여 시맨틱 표현 모델에 대해 훈련을 수행함으로써, 훈련 후의 시맨틱 표현 모델로 하여금 형태론, 문법 및 시맨틱 등 각 등급의 정보를 충분히 학습할 수 있도록 하여, 시맨틱 표현 모델의 시맨틱 표현 능력을 풍부히 하였고, 시맨틱 표현의 정확성을 향상시켰다. 또한, 본 실시예에 있어서, 시맨틱 표현 모델에 대해 형태론에 기반한 태스크 훈련을 수행함으로써, 시맨틱 표 현 모델로 하여금 단어, 구절 및 실체의 공기 지식을 학습하고, 지식의 융합성을 이해할 수 있도록 하여, 시맨 틱 표현 모델의 시맨틱 표현 능력을 강화할 수 있고, 매 하나의 어구에 대해 정확하게 시맨틱 표현을 수행할 수 있다. 이와 동시에, 또한 서로 다른 언어 환경 하의 어휘의 두문자를 대문자로 쓸 필요가 있는지 여부를 분석하는 능력을 학습할 수 있기에, 서로 다른 언어 환경 하의 어휘의 정확한 표현 방식을 정확히 학습할 수 있고; 또 한, 어휘가 원본 문서의 기타 세그먼트의 공기할지를 예측하는 능력을 학습할 수 있기에, 문서 중에서 어떠한 어휘가 문서의 중심 사상을 나타낼 수 있는지를 예측할 수 있다. 상술한 형태론에 기반한 태스크 훈련을 통해, 시맨틱 표현 모델로 하여금 풍부한 형태론 지식을 학습할 수 있도록 하고, 형태론이 표시하는 의미를 충분히 이 해하도록 하여, 더욱 정확하게 시맨틱 표현을 수행할 수 있다. 또한, 본 실시예에 있어서, 또한 시맨틱 표현 모델에 대해 문법에 기반한 태스크 훈련을 수행함으로써, 시맨틱 표현 모델로 하여금 문장의 배열을 학습할 수 있도록 하고, 또한 서로 다른 문장의 위치 관계를 인식할 수 있도 록 하여, 시맨틱 표현 과정에서 매 하나의 문장에 대해 정확히 위치 확정을 수행하여, 시맨틱 표현의 정확성을 향상시켰다. 또한, 본 실시예에 있어서, 또한 시맨틱 표현 모델에 대해 시맨틱에 기반한 태스크 훈련을 수행함으로써, 시맨 틱 표현 모델로 하여금 문장의 논리적 관계 태스크 및 검색 상관성 태스크를 학습할 수 있도록 하여, 시맨틱 표 현 시, 시맨틱을 정확히 이해하고, 시맨틱 표현의 정확성을 강화할 수 있다. 총괄적으로, 본 실시예에 있어서, 형태론, 문법, 시맨틱 이러한 세 개의 서로 다른 레벨의 무 감독 또는 약 감 독의 사전 훈련 태스크를 구축함으로써, 시맨틱 표현 모델이 대규모 데이터 중에서 어휘, 문법, 시맨틱과 같은 서로 다른 등급의 지식을 학습하도록 하여, 범용 시맨틱 표현의 능력을 강화하였고, NLP 태스크의 처리 효과를 향상시켰다. 상기에 나타낸 각 형식의 프로세스를 사용하여 단계를 재정렬, 추가 또는 삭제할 수 있음을 이해해야 한다. 예 를 들면, 본원에 기재된 각 단계는 병렬, 순차적 또는 서로 다른 순서로 수행될 수 있으며, 본원에 개시된 기술 방안이 원하는 결과를 실현할 수 있는 한, 본 명세서는 이에 대해 한정하지 않는다. 상술한 구체적인 실시 방식은 본원의 보호 범위에 대한 한정을 구성하지 않는다. 당업자는 설계 요건 및 기타 요인에 따라 다양한 변형, 조합, 하위 조합 및 치환이 이루어질 수 있음을 이해해야 한다. 본원의 정신 및 원칙 내에서 이루어진 어떠한 변경, 동등한 대체 및 개량 등은 모두 본원의 보호 범위 내에 포함된다."}
{"patent_id": "10-2020-0068058", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도면은 본 방안을 더욱 잘 이해하게 하기 위한 것인 바, 본원을 한정하지 않는다. 여기서： 도 1은 본원에 따른 제1 실시예의 모식도이고; 도 2는 본원에 따른 제2 실시예의 모식도이며; 도 3은 본원에 의해 제공되는 훈련 응용 예시도이다. 도 4는 본원에 의해 제공되는 시맨틱 표현 모델의 멀티 태스크 학습의 아키텍처 다이어그램이다. 도 5는 본원에 의해 제공되는 시맨틱 표현 모델의 응용 아키텍처의 모식도이다. 도 6은 본원에 따른 제3 실시예의 모식도이다. 도 7은 본원에 따른 제4 실시예의 모식도이다. 도 8은 본원 실시예의 시맨틱 표현 모델의 처리 방법을 구현하기 위한 전자 기기의 블럭도이다."}
