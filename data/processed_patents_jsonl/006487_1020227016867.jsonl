{"patent_id": "10-2022-7016867", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0080191", "출원번호": "10-2022-7016867", "발명의 명칭": "정보 처리 방법 및 장치, 컴퓨터 판독 가능형 저장 매체 및 전자 장치", "출원인": "텐센트 테크놀로지", "발명자": "창 톈위안"}}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 의해 수행되는 정보 처리 방법으로서,게임 시나리오에서 게임 액션 주체(game action subject)를 결정하고, 상기 게임 액션 주체가 게임 액션을 실행하도록 제어하는 데 사용되는 액션 모델을 획득하는 단계;상기 게임 시나리오에 대한 특징 추출을 수행하여 상기 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득하는 단계;상기 액션 모델을 사용함으로써 상기 모델 게임 상태 정보에 대해 매핑 처리를 수행하여 적어도 2개의 후보 게임 액션에 대응하는 모델 게임 액션 선택 정보를 획득하는 단계; 및상기 모델 게임 액션 선택 정보에 따라, 상기 적어도 2개의 후보 게임 액션으로부터 상기 게임 액션 주체에 의해 실행되는 모델 게임 액션을 선택하는 단계를 포함하는 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 게임 시나리오에 대한 특징 추출을 수행하여 상기 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득하는 단계는:상기 게임 시나리오 내의 시나리오 상태 정보 및 상기 게임 액션 주체의 주체 상태 정보를 획득하는 단계;상기 시나리오 상태 정보에 대한 특징 추출을 수행하여 시나리오 특징 벡터를 획득하고, 상기 주체 상태 정보에대한 특징 추출을 수행하여 주체 특징 벡터를 획득하는 단계; 및상기 시나리오 특징 벡터 및 상기 주체 특징 벡터에 대한 스플라이싱 처리(splicing processing)를 수행하여 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득하는 단계를 포함하는, 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 액션 모델을 사용함으로써 상기 모델 게임 상태 정보에 대해 매핑 처리를 수행하여 적어도 2개의 후보 게임 액션에 대응하는 모델 게임 액션 선택 정보를 획득하는 단계는:상기 게임 액션 주체와 관련된 적어도 두 개의 후보 게임 액션을 결정하는 단계;상기 액션 모델을 사용함으로써 상기 모델 게임 상태 정보에 대한 매핑 처리를 수행하여, 각 후보 게임 액션의선택 확률을 획득하는 단계;각 후보 게임 액션의 액션 가용 상태 정보를 획득하고, 상기 액션 가용 상태 정보에 따라 상기 후보 게임 액션에 대응하는 액션 스크리닝 정보(action screening information)를 결정하는 단계; 및상기 액션 스크리닝 정보에 따라 상기 후보 게임 액션의 선택 확률을 조정하는 단계, 및 조정된 선택 확률을 상기 모델 게임 액션 선택 정보로 결정하는 단계를 포함하는, 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,공개특허 10-2022-0080191-3-상기 액션 스크리닝 정보에 따라 상기 후보 게임 액션의 선택 확률을 조정하는 단계는:상기 액션 스크리닝 정보에 따라 상기 후보 게임 액션의 액션 유형을 결정하는 단계 - 상기 액션 유형은 가용액션 및 비가용 액션을 포함함 - ; 및상기 후보 게임 액션의 액션 유형이 비가용 액션인 경우, 상기 후보 게임 액션의 선택 확률을 미리 설정된 확률로 조정하는 단계를 포함하는, 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 모델 게임 액션 선택 정보에 따라, 상기 적어도 2개의 후보 게임 액션으로부터 상기 게임 액션 주체에 의해 실행되는 모델 게임 액션을 선택하는 단계는:랜덤 액션 선택 정책(random action selection policy의 제1 확률 및 고확률 액션 선택 정책(high probabilityaction selection policy)의 제2 확률을 결정하는 단계;상기 제1 확률 및 상기 제2 확률에 따라, 상기 모델 게임 액션을 선택하는 데 사용되는 모델 선택 정책을 결정하는 단계;상기 모델 선택 정책이 상기 랜덤 액션 선택 정책인 경우, 상기 적어도 2개의 후보 게임 액션 중 하나의 후보게임 액션을 상기 모델 게임 액션으로 무작위로 선택하는 단계; 및상기 모델 선택 정책이 고가치 액션 선택 정책(high value action selection policy)인 경우, 상기 적어도 2개의 후보 게임 액션 중 액션 값이 가장 높은 후보 게임 액션을 상기 모델 게임 액션으로 선택하는 단계를 포함하는, 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 모델 게임 액션의 모델 게임 상태 정보와 모델 게임 액션 정보를 모델 게임 샘플로 결합하는 단계;상기 게임 액션 주체와 관련된 사용자 게임 데이터를 획득하고, 상기 사용자 게임 데이터에 따라, 사용자 게임상태 정보 및 사용자 게임 액션 정보를 포함하는 사용자 게임 샘플을 결정하는 단계;상기 모델 게임 샘플 및 상기 사용자 게임 샘플을 트레이닝 샘플로 결정하고, 상기 트레이닝 샘플을 판별자 모델에 입력하는 단계;상기 판별자 모델을 사용함으로써 상기 트레이닝 샘플에 대한 매핑 처리를 수행하여 샘플 판별 정보를 획득하는단계 - 상기 샘플 판별 정보는 상기 트레이닝 샘플이 모델 게임 샘플인지 사용자 게임 샘플인지 판별하는 데 사용됨 - ; 및상기 샘플 판별 정보에 따라 상기 액션 모델의 모델 파라미터 및 상기 판별자 모델의 모델 파라미터를 업데이트하는 단계를 더 포함하는 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 모델 게임 액션의 모델 게임 상태 정보와 모델 게임 액션 정보를 모델 게임 샘플로 결합하는 단계는:게임 시나리오에서 적어도 하나의 게임 라운드를 결정하고, 상기 게임 라운드의 게임 순서 정보를 획득하는 단계;게임 라운드에 대응하는 모델 게임 상태 정보와 모델 게임 액션의 모델 게임 액션 정보를 모델 결정 정보 쌍으로 결합하는 단계; 및공개특허 10-2022-0080191-4-모든 게임 라운드의 모델 결정 정보 쌍을 게임 순서 정보에 따라 모델 결정 정보 쌍 순서로 결합하고, 상기 모델 결정 정보 쌍 순서를 상기 게임 시나리오에 대응하는 모델 게임 샘플로 결정하는 단계를 포함하는, 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 사용자 게임 데이터에 따라, 사용자 게임 상태 정보 및 사용자 게임 액션 정보를 포함하는 사용자 게임 샘플을 결정하는 단계는:상기 사용자 게임 데이터에 따라 상기 게임 시나리오에서 적어도 하나의 게임 라운드를 결정하고, 게임 라운드의 게임 순서 정보를 획득하는 단계;게임 라운드에 대응하는 사용자 게임 상태 정보와 사용자 게임 액션 정보를 사용자 결정 정보 쌍으로 결합하는단계; 및모든 게임 라운드의 사용자 결정 정보 쌍을 게임 순서 정보에 따라 사용자 결정 정보 쌍 순서로 결합하고, 상기사용자 결정 정보 쌍 순서를 상기 게임 시나리오에 대응하는 사용자 게임 샘플로 결정하는 단계를 포함하는, 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서,상기 트레이닝 샘플을 판별자 모델에 입력하는 단계는:상기 트레이닝 샘플로부터 게임 순서로 배열된 결정 정보 쌍을 획득하고, 각 결정 정보 쌍에서 게임 상태 정보및 게임 액션 정보를 획득하는 단계;상기 게임 상태 정보에 대응하는 제1 특징 벡터 및 상기 게임 액션 정보에 대응하는 제2 특징 벡터를 획득하는단계;상기 제1 특징 벡터 및 상기 제2 특징 벡터에 대해 스플라이싱 처리를 수행하여 상기 결정 정보 쌍의 샘플 특징벡터를 획득하는 단계; 및상기 트레이닝 샘플의 결정 정보 쌍의 샘플 특징 벡터를 상기 게임 순서에 따라 상기 판별자 모델에 순차적으로입력하는 단계를 포함하는, 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 판별자 모델을 사용함으로써 상기 트레이닝 샘플에 대한 매핑 처리를 수행하여 샘플 판별 정보를 획득하는단계는:상기 트레이닝 샘플에서 결정 정보 쌍의 정보 쌍 수량을 획득하는 단계;상기 판별자 모델을 사용함으로써 각 결정 정보 쌍의 샘플 특징 벡터에 대해 매핑 처리를 수행하여 각 결정 정보 쌍의 정보 쌍 분류 확률을 획득하는 단계; 및상기 정보 쌍 수량 및 상기 정보 쌍 분류 확률에 따라 상기 트레이닝 샘플의 샘플 분류 확률을 결정하고, 상기샘플 분류 확률을 상기 샘플 판별 정보로 결정하는 단계를 포함하는, 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제6항에 있어서,상기 샘플 판별 정보에 따라 상기 액션 모델의 모델 파라미터 및 상기 판별자 모델의 모델 파라미터를 업데이트공개특허 10-2022-0080191-5-하는 단계는:상기 사용자 게임 샘플에 대응하는 사용자 샘플 기대치 및 상기 모델 게임 샘플에 대응하는 모델 샘플 기대치를포함하는 목적 함수(objective function)를 결정하는 단계; 및상기 샘플 판별 정보 및 상기 목적 함수에 따라 상기 액션 모델의 모델 파라미터 및 상기 판별자 모델의 모델파라미터를 교대로 업데이트하는 단계를 포함하는, 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 샘플 판별 정보 및 상기 목적 함수에 따라 상기 액션 모델의 모델 파라미터 및 상기 판별자 모델의 모델파라미터를 교대로 업데이트하는 단계는:상기 액션 모델의 모델 파라미터를 고정하고, 상기 샘플 판별 정보 및 상기 목적 함수에 따라 상기 판별자 모델의 모델 파라미터를 업데이트하여 상기 사용자 게임 샘플의 샘플 분류 확률을 높이고 상기 모델 게임 샘플의 샘플 분류 확률을 낮추는 단계; 및상기 판별자 모델의 모델 파라미터를 고정하고, 상기 샘플 판별 정보 및 상기 목적 함수에 따라 상기 액션 모델의 모델 파라미터를 업데이트하여 상기 모델 게임 샘플의 샘플 분류 확률을 높이는 단계를 포함하는, 정보 처리 방법."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "정보 처리 장치로서,게임 시나리오에서 게임 액션 주체(game action subject)를 결정하고, 상기 게임 액션 주체가 게임 액션을 실행하도록 제어하는 데 사용되는 액션 모델을 획득하도록 구성되어 있는 모델 획득 모듈;상기 게임 시나리오에 대한 특징 추출을 수행하여 상기 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득하도록 구성되어 있는 특징 추출 모듈;상기 액션 모델을 사용함으로써 상기 모델 게임 상태 정보에 대해 매핑 처리를 수행하여 적어도 2개의 후보 게임 액션에 대응하는 모델 게임 액션 선택 정보를 획득하도록 구성되어 있는 매핑 처리 모듈; 및상기 모델 게임 액션 선택 정보에 따라, 상기 적어도 2개의 후보 게임 액션으로부터 상기 게임 액션 주체에 의해 실행되는 모델 게임 액션을 선택하도록 구성되어 있는 액션 선택 모듈을 포함하는 정보 처리 장치."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "컴퓨터 판독 가능형 저장 매체로서,컴퓨터 프로그램을 저장하며, 상기 컴퓨터 프로그램은 프로세서에 의해 실행될 때, 제1항 내지 제12항 중 어느한 항에 따른 정보 처리 방법을 구현하는, 컴퓨터 판독 가능형 저장 매체."}
{"patent_id": "10-2022-7016867", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "전자 장치로서,프로세서, 및상기 프로세서의 실행 가능한 명령을 저장하도록 구성된 메모리를 포함하며,상기 프로세서는 상기 실행 가능한 명령을 실행함으로써 제1항 내지 제12항 중 어느 한 항에 따른 정보 처리 방법을 수행하도록 구성되어 있는, 전자 장치.공개특허 10-2022-0080191-6-"}
{"patent_id": "10-2022-7016867", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "정보 처리 방법 및 장치, 컴퓨터 판독 가능형 저장 매체 및 전자 장치. 정보 처리 방법은: 게임 시나리오에서 게 임 액션 주체를 결정하고, 상기 게임 액션 주체가 게임 액션을 실행하도록 제어하기 위한 액션 모델을 획득하는 단계; 상기 게임 시나리오에 대한 특징 추출하여 상기 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득하는 단계; 상기 액션 모델에 의해 상기 모델 게임 상태 정보에 대해 매핑 처리를 수행하여 적어도 두 유형의 후보 게 임 액션에 대응하는 모델 게임 액션 선택 정보를 획득하는 단계; 및 상기 모델 게임 액션 선택 정보에 따라, 상 기 적어도 두 유형의 후보 게임 액션으로부터 상기 게임 액션 주체에 의해 실행될 모델 게임 액션을 선택하는 단 계를 포함한다."}
{"patent_id": "10-2022-7016867", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 2020년 2월 11일에 출원된 중국 특허 출원 번호 202010086291.6을 기초로 하며, 그 전문이 참고로 여 기에 포함된 전술한 중국 출원에 대한 우선권을 주장한다. 본 출원은 인공 지능 기술 분야에 관한 것으로, 기계 학습 기술에 관한 것으로, 특히 정보 처리 방법, 정보 처 리 장치, 컴퓨터 판독 가능형 저장 매체 및 전자 장치에 관한 것이다."}
{"patent_id": "10-2022-7016867", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "비디오 게임에는 일반적으로 인간의 게임 액션을 모방할 수 있는 게임 인공지능(AI)이 구성되어 있다. 게임 AI 는 게임 프로세스에서 실제 게임 사용자와 상호 작용하는 가상 플레이어로 사용될 수 있으며, 사용자가 게임 규 칙을 이해하도록 돕거나 게임 프로세스에서 사용자에게 게임 의사 결정 제안을 제공할 수 있으며, 비디오 게임 의 자동화 테스트에 추가로 사용된다. 비디오 게임 산업의 발전에 따라 비디오 게임에 포함되는 게임 콘텐츠 요소의 종류와 양이 증가하고 있으며, 게 임 환경 상태는 점점 더 복잡해지고 있다. 게임 AI가 다양화되고 복잡해지는 게임 콘텐츠와 게임 환경에 직면했 을 때, 실제 사용자의 행동 습관에 부합하거나 사용자의 기대에 부합하는 게임 액션 결정을 내리는 것은 어렵다. 따라서 지능이 낮고 의사 결정 능력이 떨어지는 등의 문제가 발생한다. 본 출원의 실시예는 게임 AI가 보다 높은 의인화 효과 및 더 높은 지능 수준을 갖도록 게임 AI의 의사 결정 능 력을 향상시킬 수 있는 정보 처리 방법, 정보 처리 장치, 컴퓨터 판독 가능 저장 매체 및 전자 장치를 제공한다. 본 출원의 실시예는 정보 처리 방법을 제공하며, 상기 방법은: 게임 시나리오에서 게임 액션 주체를 결정하고, 게임 액션 주체가 게임 액션을 실행하도록 제어하는 데 사용되 는 액션 모델을 획득하는 단계; 게임 시나리오에 대한 특징 추출을 수행하여 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득하는 단계; 액션 모델을 사용하여 모델 게임 상태 정보에 대해 매핑 처리를 수행하여 적어도 2개의 후보 게임 액션에 대응 하는 모델 게임 액션 선택 정보를 획득하는 단계; 및 모델 게임 액션 선택 정보에 따라, 적어도 2개의 후보 게임 액션으로부터 게임 액션 주체에 의해 실행되는 모델 게임 액션을 선택하는 단계 를 포함한다. 본 출원의 실시예는 정보 처리 장치를 더 제공하며, 상기 장치는: 게임 시나리오에서 게임 액션 주체를 결정하고, 게임 액션 주체가 게임 액션을 실행하도록 제어하는 데 사용되 는 액션 모델을 획득하도록 구성된 모델 획득 모듈; 게임 시나리오에 대한 특징 추출을 수행하여 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득하도록 구성 된 특징 추출 모듈; 액션 모델을 사용하여 모델 게임 상태 정보에 대해 매핑 처리를 수행하여 적어도 2개의 후보 게임 액션에 대응 하는 모델 게임 액션 선택 정보를 획득하도록 구성된 매핑 처리 모듈; 및 모델 게임 액션 선택 정보에 따라, 적어도 2개의 후보 게임 액션으로부터 게임 액션 주체에 의해 실행되는 모델 게임 액션을 선택하도록 구성된 액션 선택 모듈 을 포함한다. 본 출원의 실시예는 프로세서에 의해 실행될 때 전술한 기술 솔루션의 정보 처리 방법을 구현하는 컴퓨터 프로 그램, 컴퓨터 프로그램을 저장하는 컴퓨터 판독 가능한 저장 매체를 더 제공한다.본 출원의 실시예는 전자 장치를 더 제공하며, 상기 전자 장치는 프로세서; 및 프로세서의 실행 가능한 명령어 를 저장하도록 구성된 메모리를 포함하며, 프로세서는 전술한 기술 솔루션의 정보 처리 방법을 수행하기 위해 실행 가능한 명령을 실행하도록 구성된다. 본 출원의 실시예에서 제공되는 정보 처리 방법, 정보 처리 장치, 컴퓨터 판독 가능 저장 매체 및 전자 장치에 따르면, 게임 시나리오에 대한 특징 추출, 분석을 수행함으로써 시나리오 특징이 획득되고, 사전 트레이닝된 액 션 모델을 사용하여 시나리오 특징에 대한 분석 및 의사 결정이 수행되고, 게임 액션 주체에 의해 실행된 모델 게임 액션이 후보 게임 액션 중에서 선택되어 사용자 기대를 충족시키는 게임 액션 결정을 얻을 수 있다. 게임 에서 게임 AI를 구성하는 경우, 게임의 시나리오 특성에 따라 모델 게임 액션이 선택되어 그 선택된 모델 게임 액션이 게임 시나리오에 더 적응할 수 있고, 해당 모델 게임 액션이 다른 게임 시나리오에 선택되며, 이에 의해 게임 AI가 구현하는 게임 액션을 풍부하게 하고 게임 AI의 의사 결정 능력을 크게 향상시켜 게임 AI가 더 높은 의인화 효과와 더 높은 지능 수준을 가질 수 있게 하고 나아가 인간-컴퓨터 상호 작용 효율성을 향상시킨다."}
{"patent_id": "10-2022-7016867", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이제 예시적인 구현이 첨부 도면을 참조하여 보다 포괄적으로 설명된다. 그러나 본 발명의 실시 예는 다양한 형 태로 구현될 수 있으며 여기에서 설명하는 예에 한정되는 것은 아니다. 역으로, 이러한 구현은 본 출원의 실시 예를 보다 포괄적이고 완전하게 만들고 예시적인 구현의 개념을 당업자에게 완전히 전달하기 위해 제공된다. 또한, 설명된 특징, 구조 또는 특성은 임의의 적절한 방식으로 하나 이상의 실시예에서 조합될 수 있다. 다음 설명에서, 본 출원의 실시예에 대한 포괄적인 이해를 제공하기 위해 많은 세부사항이 제공된다. 그렇지만, 당업 자는 본 출원의 실시예의 기술적 솔루션이 하나 이상의 특정 세부사항 없이 실행될 수 있거나, 또는 다른 방법, 구성요소, 장치, 단계 등이 채택될 수 있음을 인식할 것이다. 다른 경우에, 공개 방법, 장치, 구현 또는 동작은 본 출원의 실시예의 측면을 흐리게 하는 것을 피하기 위해 상세하게 도시되거나 설명되지 않는다. 첨부된 도면에 도시된 블록도는 단지 기능적 개체일 뿐이며 물리적으로 독립된 개체에 반드시 대응하는 것은 아 니다. 즉, 기능 엔티티는 소프트웨어 형태로, 또는 하나 이상의 하드웨어 모듈 또는 집적 회로로, 또는 상이한 네트워크 및/또는 프로세서 장치 및/또는 마이크로컨트롤러 장치로 구현될 수 있다. 첨부된 도면에 도시된 흐름도는 단지 설명을 위한 예시일 뿐이며, 모든 내용 및 작업/단계를 포함할 필요도, 설 명된 순서대로 수행될 필요도 없다. 예를 들어, 일부 작업/단계는 더 분할될 수 있는 반면 일부 작업/단계는 결 합되거나 부분적으로 결합될 수 있다. 따라서 실제 실행 순서는 실제 사례에 따라 변경될 수 있다. 관련 기술 분야에서 인공 지능 기술을 기반으로 하는 비디오 게임에서 게임 AI를 구성하는 것은 비디오 게임 산 업에서 일반적인 관행이 되었다. 예를 들어, 게임 AI는 게임 튜토리얼 단계에서 사용자에게 튜토리얼 및 게임규칙 안내를 제공하고, 게임 프로세스에서 사용자에게 게임 의사 결정 제안을 제공하고, 인간-컴퓨터 전투를 구 현하기 위한 가상 플레이어로 사용될 수 있거나, 또는 게임 개발 프로세스에서 자동화된 테스트를 수행하는 데 사용된다. AI는 디지털 컴퓨터 또는 디지털 컴퓨터에 의해 제어되는 기계를 사용하여 인간 지능을 시뮬레이션, 확장 및 확 장하고, 환경을 인식하고, 지식을 획득하고, 지식을 사용하여 최적의 결과를 획득하는 이론, 방법, 기술 및 응 용 시스템이다. 다시 말해, 인공지능은 지능의 본질을 이해하려고 시도하고 인간의 지능과 유사한 방식으로 반 응할 수 있는 새로운 지능 기계를 생산하는 컴퓨터 과학의 종합 기술이다. AI는 다양한 지능형 기계의 설계 원 리와 구현 방법을 연구하여 기계가 인식, 추론 및 의사 결정 기능을 가질 수 있도록 하는 것이다. AI 기술은 포괄적인 학문으로서 하드웨어 수준의 기술과 소프트웨어 수준의 기술을 모두 포함하는 광범위한 분 야와 관련이 있다. 기본 AI 기술에는 일반적으로 센서, 전용 AI 칩, 클라우드 컴퓨팅, 분산 스토리지, 빅 데이 터 처리 기술, 운영/상호작용 시스템, 전자 기계 통합과 같은 기술이 포함된다. AI 소프트웨어 기술은 주로 컴 퓨터 비전(computer vision, CV) 기술, 음성 처리 기술, 자연어 처리 기술, 머신 러닝(ML)/딥 러닝을 포함한다. ML은 다분야 학제간(multi-field interdiscipline)이며, 확률 이론, 통계학, 근사 이론, 볼록 분석 및 알고리 즘 복잡도 이론과 같은 다수의 학문 분야에 관한 것이다. ML은 컴퓨터가 인간의 학습 행동을 시뮬레이션하거나 구현하여 새로운 지식이나 기술을 얻고 기존 지식 구조를 재구성하여 성능을 계속 향상시키는 방법을 연구하는 것을 전문으로 한다. ML은 AI의 핵심이자 컴퓨터를 지능화하는 기본 방법으로 인공지능의 다양한 분야에 적용되 고 있다. ML 및 딥 러닝에는 일반적으로 인공 신경망, 신념 네트워크, 강화 학습, 전이 학습, 귀납 학습 및 데 모 학습과 같은 기술이 포함된다. 머신 러닝 또는 딥 러닝을 통한 게임 AI 트레이닝은 실제 사용자의 게임 결정 동작을 모방하여 게임 AI의 의사 결정 능력을 향상시켜 게임 AI가 인간 행동 논리와 같은 효과적인 결정을 내릴 수 있도록 한다. 그러나 복잡하 고 다양한 규칙을 가진 일부 비디오 게임의 경우 많은 양의 트레이닝 시간과 컴퓨팅 자원이 소모되더라도 여전 히 좋은 트레이닝 효과를 얻기 어렵다. 예를 들어 턴 기반 롤 플레잉 게임(turn-based role playing game)을 취하면 일부 게임에서 대부분의 NPC(non- player character)는 결정 트리를 설정하여 자동 스킬 시전를 구현한다. 의사 결정 트리 알고리즘은 관련 기능 을 선택하여 트리 구조를 설정하며, 각 상위 노드의 하위 노드는 해당 노드의 모든 스킬 시전 정책을 나타낸다. 이 기술은 적은 양의 샘플 데이터에 국한되고, 생성되는 게임 액션 정책은 상대적으로 고정적이고 단일하며, 스 킬은 무작위로 선택되어 공개되며 지능 수준이 낮다. 따라서 트레이닝 세트에서 정책의 성능은 비교적 좋은 반 면 실제 게임 환경에서는 상대적으로 열악하며 심각한 과적합 현상이 존재한다. 게임 AI가 실제 게임 사용자와 멀티플레이어 전투를 하는 경우 게임 AI의 스킬 시전 정책, 즉 게임 AI가 실행하는 게임 액션은 대개 상대적으 로 단일하고 고정적이며, 현재 게임 시나리오에 부합하지 않는다. 이처럼 게임 AI는 지능이 낮아 인간과 컴퓨터 의 상호작용 효율이 낮다. 전술한 기술 솔루션의 문제를 해결하기 위해, 본 출원의 실시예는 게임 AI의 지능 수준을 상당히 향상시킬 수 있는 정보 처리 방법, 정보 처리 장치, 컴퓨터 판독 가능한 저장 매체 및 전자 장치를 제공한다. 도 1은 본 출원의 기술 솔루션이 적용된 예시적인 시스템 아키텍처의 개략도이다. 도 1에 도시된 바와 같이, 시스템 아키텍처는 클라이언트, 네트워크 및 서버를 포함할 수 있다. 클라이언트는 스마트폰, 태블릿 컴퓨터, 랩톱 컴퓨터, 데스크톱 컴퓨터와 같은 다양한 단말 장치를 포함할 수 있다. 서버는 네트워크 서버, 애플리케이션 서버, 데이터베이스 서버 등의 다양한 서버 장치를 포함할 수 있다. 네트워크는 클라이언트와 서버 사이의 다양한 연결 형태의 통신 링크, 예를 들 면, 유선 통신 링크 또는 무선 통신 링크를 제공할 수 있는 통신 매체일 수 있다. 구현 요구사항에 따르면, 본 출원의 이 실시예의 시스템 아키텍처는 클라이언트, 네트워크 및 서버의 임의의 수 량을 가질 수 있다. 일부 실시예에서, 서버는 다수의 서버 장치를 포함하는 서버 클러스터일 수 있다. 서 버 장치는 계산을 수행할 수 있는 임의의 하드웨어 장치를 포함할 수 있지만 이에 국한되지 않으며 독립된 물리 적 서버일 수 있거나 서버 클러스터 또는 복수의 물리적 서버를 포함하는 분산 시스템일 수 있거나 클라우드 서 비스, 클라우드 데이터베이스, 클라우드 컴퓨팅, 클라우드 기능, 클라우드 스토리지, 네트워크 서비스, 클라우 드 통신, 미들웨어 서비스, 도메인 네임 서비스, 보안 서비스, 콘텐츠 전송 네트워크(CDN), 빅데이터, 및 인공 지능 플랫폼과 같은 기본적인 클라우드 컴퓨팅 서비스를 제공하는 클라우드 서버일 수 있다. 또한, 본 출원의 본 실시예의 기술 솔루션은 클라이언트에 적용될 수 있거나, 서버에 적용될 수 있거나, 클라이언트와 서버에 의해 공동으로 구현될 수 있으며, 이는 본 출원의 이 실시예에서 특별히 제한되지 않는다. 예를 들어, 클라이언트에 설치된 게임 애플리케이션 프로그램은 실제 게임 사용자의 게임 데이터를 수집하 고, 네트워크를 이용하여 게임 데이터를 서버에 업로드할 수 있다. 서버는 수신된 게임 데이터 를 기반으로 머신 러닝 모델을 학습시켜 사용자 게임 액션을 모방하여 게임 결정을 내리는 게임 AI 모델을 획득 한다. 게임 AI 모델은 클라이언트에 의해 업로드된 게임 데이터에 따라 게임 결정을 내리고, (스킬 시전 액션과 같은) 게임 액션을 선택하고, 액션 선택 정보를 클라이언트에 보낼 수 있다. 클라이언트는 서 버에 의해 반환된 게임 액션 선택 정보에 따라, 게임 애플리케이션 프로그램에서 게임 역할을 제어하여 해 당 게임을 구현한다. 예를 들어, 턴 기반 롤 플레잉 게임에서, 본 출원에서 제공되는 기술 솔루션은 NPC 역할을 하는 게임 AI의 지능 형 스킬 시전 트레이닝에 사용될 수 있다. 턴 기반 롤 플레잉 게임에서, 게임 시나리오에서 게임 사용자와 NPC 역할의 게임 역할은 일반적으로 여러 스킬을 가지고 있으며, 각각의 스킬 시전 라운드는 현재 라운드에서 각각 의 게임 역할의 상태와 관련이 있다. 따라서 NPC 역할에 의한 게임 정책 학습의 복잡성과 어려움이 상대적으로 높고 네트워크 모델이 0에서 직접 학습을 시작하는 비용이 매우 높다. 본 출원의 이 실시예는 모방 학습의 개념 에 근거하며, 즉 실제 게임 사용자의 스킬 시전 동작을 관찰하고 모방하여 전투 정책 학습을 수행한다는 아이디 어에 기초한다. 본 출원의 이 실시예에서는 생성적 적대적 모방 학습 방식으로 게임 AI를 NPC 역할로 트레이닝 시키기 위해 모방 학습을 기반으로 생성적 적대적 학습의 개념을 도입한다. 도 2는 생성적 적대적 모방 학습 원 리의 개략도이다. 도 2에 도시된 바와 같이, 실제 게임 사용자의 게임 실행에 의해 생성된 사용자 게임 데이터 집합 은 분포를 따른다고 가정하고, 여기서 사용자 게임 데이터 이고, 는 게임 시나리오에서 실제 게임 사용자에 대응하는 게임 액션 주체(예를 들어, 게임 사용자가 제어하는 게임 역할)의 사용자 게임 상태를 나타내고, 는 해당 사용자 게임 상태에 대해 실제 게임 사용자가 수행한 사용자 게임 액션을 나타낸다. 본 출원의 이 실시예에서, 실제 게임 사용자의 게임 액션을 모방한 모델 게임 데이터 세 트 은 게임 시나리오와 액션 모델 배우 간의 지속적인 상호작용에 의해 생성될 수 있고, 여기서 이고, s는 게임 시나리오에서 액션 모델에 대응하는 게임 액션 주체(예를 들어, NPC 역 할)의 모델 게임 상태를 나타내며, α는 해당 모델 게임 상태에 대한 액션 모델에 의해 만들어진 모델 게임 액 션을 나타낸다. 생성적 적대 학습 방법을 사용하면 모델 게임 데이터 세트가 사용자 게임 데이터 세트의 확률 분포에 점차 가까워질 수 있다. 지속적으로 학습함으로써 액션 모델 배우는 최종적으로 사용자 게임 데이터의 확률 분포를 학습할 수 있다. 이러한 방식으로, 액션 모델이 출력하는 스킬 시전 및 킬 타겟 선택과 같은 게임 액션은 실제 게임 사용자의 액션에 더 가깝다. 게임 AI는 의인화 효과와 지능 수준이 높아 인간-컴퓨터 상호 작 용 효율성이 향상된다. 도 3은 본 출원의 실시예에 따른 생성적 적대적 모방 학습의 모델 아키텍처의 개략도이다. 도 3에 도시된 바와 같이, 생성적 적대적 모방 학습 프로세스에서 액션 모델 Actor의 학습 목표는 정책 π를 지속적으로 최적화하여 액션 모델 Actor에 의해 생성된 모델 게임 데이터 세트 의 확률 분포를 실제 게임 사용자의 게임 데이터 세트 의 확률 분포에 최대한 가깝게 되도록 판별자 모델 Discriminator가 모델에 입력된 데 이터를 실제 게임 사용자의 사용자 게임 데이터 또는 액션 모델 배우에 의해 생성된 모델 게임 데이터와 구별할 수 없도록 하는 것이다. 판별자 모델 Discriminator의 학습 목표는 모델에 입력된 데이터가 사용자 게임 데이터 인지 모델 게임 데이터인지 최대한 구별하는 것이다. 일부 실시예에서, 액션 모델 Actor의 학습 알고리즘은 심 층 강화 학습에서 정책 기울기 알고리즘(policy gradient algorithm)을 사용하여 최적화될 수 있고, 감독 학습 의 분류 알고리즘은 판별자 모델 Discriminator의 파라미터를 업데이트하는 데 사용될 수 있다. 본 출원의 기술 솔루션은 다음에서 상세히 설명된다. 다음 실시예는 예를 들어 턴 기반 롤 플레잉 게임을 취하 는 것으로 설명되지만, 본 출원의 실시예는 이에 제한되지 않는다는 점에 유의해야 한다. 도 4는 본 출원의 실시예에 따른 게임 액션 결정을 위한 정보 처리 방법의 개략적인 흐름도이다. 일부 실시예에 서, 상기 방법은 전자 장치에 의해 수행될 수 있고, 상기 전자 장치는 단말 또는 서버일 수 있다. 도 4에 도시 된 바와 같이, 방법은 주로 다음 단계를 포함할 수 있다. 단계 S410. 게임 시나리오에서 게임 액션 주체를 결정하고, 게임 액션 주체가 게임 액션을 실행하도록 제어하는 데 사용되는 액션 모델을 획득한다. 게임 액션 주체는 게임 시나리오에서 게임 액션을 수행하는 주체로서, 예를 들어 게임에서 NPC 역할을 수행하며, NPC 역할은 게임 사용자가 제어하는 역할로 전투를 수행할 수 있다. 액션 모델은 사전 트레이닝된 게 임 AI 모델이며, 액션 모델은 게임 시나리오에 따라 게임 액션을 실행할 게임 액션 대상을 제어할 수 있다. 예 를 들어, 하나의 NPC 역할에는 8개의 서로 다른 게임 스킬이 있고 8개의 게임 스킬 시전는 8개의 서로 다른 게 임 액션에 속한다. 다른 예에서, NPC 역할은 가상 무기 및 가상 기사와 같은 다양한 유형의 게임 소품을 사용할 수 있다. 다른 유형의 게임 소품을 사용하는 것도 다른 유형의 게임 액션으로 간주될 수 있다. 단계 S420. 게임 시나리오에서 특징 추출을 수행하여 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득한다. 게임 액션 주체에 의해 구현되는 게임 액션은 게임 시나리오의 시나리오 특징과 관련된다. 다수의 상이한 유형 의 시나리오 특징에 대한 특징 추출을 수행함으로써, 게임 액션 주체와 관련된 모델 게임 상태 정보가 획득될 수 있으며, 여기서 시나리오 특징은 예를 들어 게임 역할의 점유, 히트 포인트 및 사용 가능한 기술과 같은 정 보를 포함할 수 있다. 특징 추출을 수행하여 획득한 모델 게임 상태 정보는 특정 길이의 특징 벡터, 예를 들어 38차원 벡터일 수 있다. 예를 들어, 게임 시나리오의 시나리오 특징의 특징을 추출하는 방법은 먼저 시나리오 특징에 대해 인코딩 처리를 수행하여 인코딩 벡터를 획득하고, 그런 다음 인코딩 벡터에 임베디드 행렬을 곱하 여 지정된 길이의 특징 벡터를 획득하는 단계를 포함할 수 있다. 단계 S430. 액션 모델을 사용함으로써 모델 게임 상태 정보에 대한 매핑 처리를 수행하여 적어도 2개의 후보 게 임 액션에 대응하는 모델 게임 액션 선택 정보를 획득한다. 액션 모델은 다중 네트워크 계층을 포함하는 신경망 모델, 예를 들어 순차적으로 연결된 다중 완전 연결 계층을 포함하는 완전 연결 네트워크 모델일 수 있다. 액션 모델의 각 네트워크 계층은 매핑 기능으로 간주될 수 있다. 액션 모델에 입력된 모델 게임 상태 정보에 대해 특징 매핑 처리를 계층별로 수행하여 모델 게임 액션 선택 정 보를 출력한다. 모델 게임 액션 선택 정보는 적어도 2개의 후보 게임 액션에 대응하는 액션 의사 결정 정보이다. 예를 들어, 모델 게임 액션 선택 정보는 액션 모델 분석 및 의사 결정을 통해 획득된 각 후보 게임 액션의 선택 확률일 수 있다. 후보 게임 액션은 예를 들어 게임 시나리오에서 게임 액션 주체에 의해 시전될 수 있는 다양한 유형의 게임 기술일 수 있다. 단계 S440. 모델 게임 액션 선택 정보에 따라, 적어도 2개의 후보 게임 액션 중에서 게임 액션 주체에 의해 실 행되는 모델 게임 액션을 선택한다. 모델 게임 액션 선택 정보는 각 후보 게임 액션의 선택 확률일 수 있다. 이 단계에서, 게임 액션 주체가 실행하 는 모델 게임 액션으로 그리디 알고리즘(greedy algorithm)을 이용하여 선택 확률이 가장 높은 후보 게임 액션 을 결정할 수 있다. 대안으로, 게임 행위 주체에 의해 실행되는 모델 게임 행위는 선택 확률에 따라 적어도 2개 의 후보 게임 행위 중에서 무작위로 선택될 수 있다. 본 출원의 본 실시예에서 제공되는 정보 처리 방법에 따르면, 게임 시나리오에 대한 특징 추출을 수행함으로써 시나리오 특징이 획득되고, 사전 트레이닝된 액션 모델을 사용하여 시나리오 특징에 대한 분석 및 의사 결정이 수행되고, 게임 액션 주체에 의해 실행된 모델 게임 액션은 후보 게임 액션으로부터 선택될 수 있어, 사용자 기 대에 부합하는 게임 액션 결정을 얻을 수 있다. 게임에서 게임 AI를 구성하는 경우 게임의 시나리오 특성에 따 라 모델 게임 액션이 선택되어 선택된 모델 게임 액션이 게임 시나리오에 더 적응할 수 있고 해당 모델 게임 액 션이 다른 게임에 선택된다. 시나리오를 통해 게임 AI가 구현하는 게임 액션을 풍부하게 하고 게임 AI의 의사 결정 능력을 크게 향상시켜 게임 AI가 더 높은 의인화 효과와 더 높은 지능 수준을 가질 수 있게 하고 인간-컴 퓨터 상호 작용 효율성을 더욱 향상시킨다. 도 5는 본 출원의 실시예에 따른 게임 시나리오에 대한 특징 추출의 개략적인 흐름도이다. 도 5에 도시된 바와 같이, 전술한 실시예에 기초하여, 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득하기 위해 게임 시나리 오에 대한 특징 추출을 수행하는 단계(S420)는 다음 단계를 포함할 수 있다. 단계 S510. 게임 시나리오 내의 시나리오 상태 정보와 게임 액션 주체의 주체 상태 정보를 획득한다. 시나리오 상태 정보는 게임 시나리오 내의 시나리오 환경에 관한 정보이고, 주체 상태 정보는 게임 액션 주체의 게임 속성에 관한 정보이다. 예를 들어, 턴 기반 롤 플레잉 게임에서 시나리오 상태 정보는 현재 전투 라운드 및 전투 대상의 분포 위치와 같은 환경 정보를 포함할 수 있고, 주체 상태 정보는 게임 액션 주체의 점유, 히트 포인트, 그리고 매직 포인트와 같은 속성 정보를 포함할 수 있다. 단계 S520. 시나리오 상태 정보에 대한 특징 추출을 수행하여 시나리오 특징 벡터를 획득하고, 주체 상태 정보 에 대한 특징 추출을 수행하여 주체 특징 벡터를 획득한다. 시나리오 상태 정보는 서로 다른 여러 유형의 환경 정보를 포함할 수 있으며, 모든 유형의 환경 정보에 대해 개 별적으로 특징 추출을 수행하여 여러 시나리오 특징 벡터를 얻을 수 있다. 주체 상태 정보 역시 다양한 유형의 속성 정보를 포함할 수 있으며, 모든 유형의 속성 정보에 대해 개별적으로 특징 추출을 수행하여 다중 주체 특 징 벡터를 얻을 수 있다. 단계 S530. 시나리오 특징 벡터와 주체 특징 벡터에 대한 스플라이싱 처리를 수행하여 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득한다. 시나리오 특징 벡터와 대상 특징 벡터를 지정된 스플라이싱 시퀀스에 따라 스플라이싱 처리함으로써 지정된 길 이의 스플라이싱 벡터가 형성될 수 있고 스플라이싱 벡터는 게임 액션 주체와 관련된 모델 게임 상태 정보로 결 정된다. 시나리오 상태 정보와 주체 상태 정보에 대해 개별적으로 특징 추출을 수행함으로써, 다양한 유형의 환경 정보 및 속성 정보를 포함하는 모델 게임 상태 정보가 획득될 수 있고, 정보는 다중 특징 차원을 갖는다. 여러 기능 차원에 기반한 행동 의사 결정은 액션 모델의 분석 및 의사 결정 능력을 향상시키고 액션 모델의 지능 수준을 향상시킬 수 있다. 도 6은 본 출원의 일 실시예에 따른 액션 모델을 이용한 특징 매핑의 개략적인 흐름도이다. 도 6에 도시된 바와 같이, 전술한 실시예에 기초하여, 액션 모델을 사용함으로써 모델 게임 상태 정보에 대해 매핑 처리를 수행하여 적어도 두 개의 후보 게임 액션에 대응하는 모델 게임 액션 선택 정보를 획득하는 단계(S430)는 다음 단계를 포 함할 수 있다: 단계 S610. 게임 액션 주체와 관련된 적어도 2개의 후보 게임 액션을 결정한다. 후보 게임 액션은 게임 액션 주체가 선택하여 실행할 수 있는 게임 액션이다. 예를 들어, 게임 액션 주체가 게 임 역할인 경우, 후보 게임 액션은 게임 역할이 게임 스킬을 시전하거나 게임 소품을 사용하는 것일 수 있다. 단계 S620. 액션 모델을 사용함으로써 모델 게임 상태 정보에 대한 매핑 처리를 수행하여 각 후보 게임 액션의 선택 확률을 획득한다. 선택 확률은 각 후보 게임 액션이 모델 게임 액션으로 선택될 확률을 결정한다. 후보 게임 액션의 선택 확률이 높을수록 액션 모델이 후보 게임 액션을 실행할 때 더 나은 게임 이득 효과를 예측한다는 것을 의미한다. 단계 S630. 각 후보 게임 액션의 액션 가용 상태 정보를 획득하고, 액션 가용 상태 정보에 따라 후보 게임 액션 에 대응하는 액션 스크리닝 정보를 결정한다. 액션 가용 상태 정보는 각 후보 게임 액션이 현재 게임 시나리오에서 가용한지 여부를 나타내며, 해당 액션 스 크리닝 정보는 각 후보 게임 액션의 액션 가용 상태 정보에 기초하여 결정될 수 있다. 액션 스크리닝 정보는 지 정된 길이의 스크리닝 벡터일 수 있고, 스크리닝 벡터의 길이는 후보 게임 액션의 양이다. 예를 들어, 8개의 후 보 게임 액션은 8차원 스크리닝 벡터에 대응할 수 있다. 스크리닝 벡터의 각 요소는 0 또는 1의 값을 가질 수 있고, 여기서 0의 값은 해당 후보 게임 액션을 선택 및 실행할 수 없음을 나타내고 1의 값은 해당 후보 게임 액 션을 선택 및 실행할 수 있음을 나타낸다. 단계 S640. 액션 스크리닝 정보에 따라 후보 게임 액션의 선택 확률을 조정하고, 조정된 선택 확률을 모델 게임 액션 선택 정보로 결정한다. 액션 스크리닝 정보에 따라, 후보 게임 액션이 가용 액션인지 비가용 액션인지가 판단될 수 있다. 가용 액션은 선택하여 실행할 수 있는 게임 액션이고, 비가용 액션은 선택하여 실행할 수 없는 게임 액션이다. 후보 게임 액 션이 가용 액션인 경우, 후보 게임 액션의 선택 확률은 변하지 않는다. 후보 게임 액션이 비가용 액션인 경우, 후보 게임 액션의 선택 확률은 미리 설정된 확률로 조정된다. 예를 들어, 후보 게임 액션의 선택 확률은 0으로 조정되거나 0에 가까운 최솟값으로 조정될 수 있다. 액션 스크리닝 정보를 획득하고, 액션 스크리닝 정보를 기반으로 후보 게임 액션을 필터링하여 선택 확률을 조 정함으로써 액션 모델의 의사 결정 정확도를 향상시키고 무효 액션 판정 문제를 방지한다. 모델 게임 액션은 획득된 게임 액션 선택 정보에 따라 상이한 액션 선택 정책을 사용하여 선택될 수 있다. 도 7 은 본 출원의 실시예에 따른 ε-greedy 정책에 기초한 모델 게임 액션을 선택하는 개략적인 흐름도이다.도 7에 도시된 바와 같이, 전술한 실시예에 기초하여, 모델 게임 액션 선택 정보에 따라, 적어도 2개의 후보 게 임 액션 중에서 게임 액션 주체에 의해 실행되는 모델 게임 액션을 선택하는 단계(S440)는 다음 단계를 포함할 수 있다: 단계 S710. 랜덤 액션 선택 정책의 제1 확률과 고확률 액션 선택 정책의 제2 확률을 결정한다. 랜덤 액션 선택 정책과 고확률 액션 선택 정책은 서로 다른 게임 액션 선택 정책이다. 랜덤 액션 선택 정책은 동일한 확률의 무작위 선택 방식으로 여러 후보 게임 액션 중 하나를 모델 게임 액션으로 선택하는 것을 말한다. 고확률 액션 선택 정책은 다수의 후보 게임 액션 중에서 선택 확률이 가장 높은 후보 게임 액션을 모델 게임 액션으로 결정하는 것을 의미한다. 예를 들어, 랜덤 액션 선택 정책의 제1 확률이 ε인 경우, 고확률 액션 선택 정책의 제2 확률은 대응하여 1-ε로 결정될 수 있다. 단계 S720. 제1 확률과 제2 확률에 따라 모델 게임 액션을 선택하는 데 사용되는 모델 선택 정책을 결정한다. 모델 게임 액션이 선택되기 전에 매번 제1 확률 및 제2 확률에 따라 모델 선택 정책이 먼저 결정될 수 있다. 예 를 들어, ε의 값이 0.1일 때 랜덤 액션 선택 정책을 모델 선택 정책으로 사용할 확률은 10%이고, 모델 선택 정 책으로 고확률 액션 선택 정책을 사용할 확률이 90%이다. 또 다른 예로, ε의 값이 0.01일 때 랜덤 액션 선택 정책을 모델 선택 정책으로 사용할 확률은 1%이고, 모델 선택 정책으로 고확률 액션 선택 정책을 사용할 확률은 99%이다. 단계 S730. 모델 선택 정책이 랜덤 액션 선택 정책인 경우, 적어도 2개의 후보 게임 액션 중에서 하나의 후보 게임 액션을 모델 게임 액션으로 무작위로 선택한다. 모델 선택 정책이 랜덤 액션 선택 정책인 경우, 이 단계에서 하나의 후보 게임 액션은 모델 게임 액션으로서 등 가 확률 랜덤 선택 방식으로 다수의 후보 게임 액션 중에서 무작위로 선택될 수 있다. 단계 S740. 모델 선택 정책이 고가치 액션 선택 정책인 경우, 적어도 2개의 후보 게임 액션 중 액션 값이 가장 높은 후보 게임 액션을 모델 게임 액션으로 선택한다. 고가치의 액션 선택 정책이 여기에 설명되어 있다. 본 출원의 이 실시예에서, 고가치 액션 선택 정책은 모델 게 임 액션을 결정하기 위해 사용되는 정책이다. 각각의 후보 게임 액션의 액션 값을 획득함으로써, 모델 게임 액 션으로서 액션 값에 따라 가장 높은 액션 값을 갖는 후보 게임 액션이 적어도 2개의 후보 게임 액션으로부터 선 택된다. 실제 구현에서, 후보 게임 액션에 대응하는 액션 값은 후보 게임 액션에 대응하는 액션 값을 결정하기 위해 평 가될 수 있다. 일부 실시예에서, 후보 게임 액션에 대응하는 액션 값은 후보 게임 액션이 구현된 후 획득된 성 능에 따라 결정될 수 있고, 성능은 손상 지수 또는 획득된 보상으로 표현될 수 있다. 예를 들어, 후보 게임 액 션 A의 데미지 지수가 95이고, 후보 게임 액션 B의 데미지 지수가 80이고, 후보 게임 액션 C의 데미지 지수가 65인 경우, 후보 게임 액션 A는 액션 값이 가장 높은(데미지 지수가 가장 높다) 고가치 액션 선택 정책에 따라 모델 게임 액션으로 선택된다. 다른 예로, 후보 게임 액션 D가 구현된 후 얻은 보상이 가상 코인 100개이고, 후 보 게임 액션 E가 구현된 후 얻는 보상이 가상 코인 200개이고, 후보 게임 액션 F가 구현된 후 얻은 보상이 가 상 코인 150개인 경우, 최고 가치(보상이 가장 높다)를 가진 후보 게임 액션 E가 고가치 액션 선택 정책에 따라 모델 게임 액션으로 선택된다. 현재 선택 정책이 고확률 액션 선택 정책인 경우, 이 단계에서 선택 확률이 가장 높은 후보 게임 액션을 모델 게임 액션으로 결정할 수 있다. 예를 들어, 게임 스킬 A 시전, 게임 스킬 B 시전, 게임 스킬 C 시전는 3가지 후 보 게임 액션이며, 게임 스킬의 선택 확률은 각각 70%, 20%, 10%이다. 이 단계에서 선택 확률이 가장 높은 게임 스킬 A가 모델 게임 액션으로 결정될 수 있다. 본 출원의 이 실시예에서, 모델 게임 액션은 ε-greedy 정책을 사용하여 선택되고, 대응하는 모델 게임 액션 선 택은 상이한 액션 선택 정책에 대해 수행되며, 이는 액션 모델의 연속 최적화 능력을 향상시킬 수 있고 모델 게 임 액션 선택을 위한 액션 모델의 정확도를 더욱 개선한다. 실제 응용에서, 실제 게임 사용자의 사용자 게임 데이터를 지속적으로 수집하고 액션 모델의 모델 게임 데이터 를 획득함으로써, 액션 모델에 대한 파라미터 업데이트 및 최적화가 연속적으로 수행될 수 있다. 도 8은 본 출 원의 실시예에 따른 액션 모델에 대한 모델 최적화의 개략적인 흐름도이다. 도 8에 도시된 바와 같이, 전술한 실시예에 기초하여, 액션 모델에 대한 모델 최적화를 수행하는 방법은 다음 단계를 포함할 수 있다.단계 S810. 모델 게임 액션의 모델 게임 상태 정보와 모델 게임 액션 정보를 모델 게임 샘플로 결합한다. 이 단계에서, 게임 시나리오에서 적어도 하나의 게임 라운드(예를 들어, 턴 기반 게임에서 하나의 게임 라운 드)가 먼저 결정될 수 있고, 게임 라운드의 게임 순서 정보가 획득될 수 있다. 그런 다음, 게임 라운드에 대응하는 모델 게임 액션의 모델 게임 상태 정보 및 모델 게임 액션 정보는 모델 결 정 정보 쌍으로 결합된다. 마지막으로, 모든 게임 라운드의 모델 결정 정보 쌍은 게임 순서 정보에 따라 모델 결정 정보 쌍 순서로 결합되 고, 모델 결정 정보 쌍 순서는 게임 시나리오에 대응하는 모델 게임 샘플로 결정된다. 모델 결정 정보 쌍 순서를 모델 게임 샘플로 사용하면 샘플의 표현 능력이 향상될 수 있고, 트레이닝 프로세스 에서 더 나은 모델 트레이닝 효과를 얻기 위해 여러 연속 동작 간의 본질적인 연관 특성을 더 잘 학습할 수 있 다. 예를 들어, 3개의 게임 라운드가 게임 시나리오에 포함된 경우, 모델 결정 정보 쌍 (s1, a1), (s2, a2) 및 (s3, a3)은 게임 라운드에 대해 각각 결정될 수 있다. s는 모델 게임 상태 정보를 나타내고, a는 모델 게임 액션 정 보를 나타낸다. 3개의 모델 결정 정보 쌍은 게임 라운드의 순서에 따라 배열되며, 모델 결정 정보 쌍 순서 {s1, a1, s2, a2, s3, a3}를 형성할 수 있다. 모델 결정 정보 쌍 순서는 게임 시나리오에 대응하는 모델 게임 샘플로 결정된다. 단계 S820. 게임 액션 주체와 관련된 사용자 게임 데이터를 획득하고, 사용자 게임 데이터에 따라 사용자 게임 상태 정보 및 사용자 게임 액션 정보를 포함하는 사용자 게임 샘플을 결정한다. 액션 모델의 모방 학습 대상으로서, 이 단계에서, 게임 액션 주체와 관련된 사용자 게임 데이터를 획득할 수 있 고, 사용자 게임 데이터를 기반으로 사용자 게임 샘플을 획득할 수 있다. 모델 게임 샘플을 획득하는 방식과 유사하게, 이 단계에서, 게임 시나리오에서 적어도 하나의 게임 라운드가 사 용자 게임 데이터에 따라 먼저 결정될 수 있고, 게임 라운드의 게임 순서 정보가 획득될 수 있다. 그런 다음, 게임 라운드에 대응하는 사용자 게임 상태 정보 및 사용자 게임 액션 정보는 사용자 결정 정보 쌍으 로 결합된다. 마지막으로, 모든 게임 라운드의 사용자 결정 정보 쌍은 게임 순서 정보에 따라 사용자 결정 정보 쌍 순서로 결 합되고, 사용자 결정 정보 쌍 순서는 게임 시나리오에 대응하는 사용자 게임 샘플로 결정된다. 단계 S830. 모델 게임 샘플과 사용자 게임 샘플을 트레이닝 샘플로 결정하고 트레이닝 샘플을 판별자 모델에 입 력한다. 일부 실시예에서, 이 단계에서 판별자 모델에 입력하기 위해 트레이닝 샘플에 대해 벡터화 처리를 수행하는 방 법은 다음 단계를 포함할 수 있다: 게임 상태 정보(이것은 모델 게임 상태 정보 또는 사용자 게임 상태 정보일 수 있다)에 대응하는 제1 특징 벡터 및 게임 액션 정보(이것은 모델 게임 액션 정보 또는 사용자 게임 액션 정보일 수 있다)에 대응하는 제2 특징 벡터를 획득하는 단계. 예를 들어, 제1 특징 벡터는 38차원 벡터이고 제2 특징 벡터는 8차원 벡터이다. 결정 정보 쌍(이것은 모델 결정 정보 쌍 또는 사용자 결정 정보 쌍일 수 있다)의 샘플 특징 벡터를 획득하기 위 해 제1 특징 벡터 및 제2 특징 벡터에 대해 스플라이싱 처리가 수행된다. 샘플 특징 벡터는, 예를 들어, 제1 특 징 벡터와 제2 특징 벡터를 나열된 순서대로 스플라이싱하여 형성된 46차원 벡터일 수 있다. 트레이닝 샘플의 결정 정보 쌍의 샘플 특징 벡터는 게임 순서에 따라 판별자 모델에 순차적으로 입력된다. 트레이닝 샘플 내의 결정 정보에 대해 벡터화 처리를 수행함으로써, 통일된 형태의 샘플 특징 벡터를 획득한 후, 판별자 모델에 연속적으로 입력함으로써, 판별자 모델의 판별 효율을 향상시키고, 모델 트레이닝 효율을 개 선하고, 컴퓨팅 리소스 소비를 줄인다. 단계 S840. 판별자 모델을 사용하여 트레이닝 샘플에 대한 매핑 처리를 수행하여 샘플 판별 정보를 획득한다. 샘플 판별 정보는 트레이닝 샘플이 모델 게임 샘플인지 사용자 게임 샘플인지를 판별하는 데 사용된다. 일부 실시예에서, 이 단계에서 샘플 판별 정보를 획득하는 방법은 다음 단계: 트레이닝 샘플에서 결정 정보 쌍의 정보 쌍 수량을 획득하는 단계 - 여기서 예를 들어, 정보 쌍 수량은 T임 - ; 판별자 모델을 사용하여 각 결정 정보 쌍의 샘플 특징 벡터에 대해 매핑 처리를 수행하여 각 결정 정보 쌍의 정 보 쌍 분류 확률을 획득하는 단계 - 여기서 예를 들어, 각 결정 정보 쌍의 정보 쌍 분류 확률은 dt이고, dt의 값은 1에서 T까지임 - ; 및 정보 쌍 수량 및 정보 쌍 분류 확률에 따라 트레이닝 샘플의 샘플 분류 확률을 결정하고, 샘플 분류 확률을 샘 플 판별 정보로 결정하는 단계 - 여기서 예를 들어, 정보 쌍 분류 확률의 평균값 를 샘플 분류 확률로 직접 결정할 수 있다. 샘플 분류 확률이 0.5보다 큰 경우, 트레이닝 샘플은 사용자 게임 샘플로 판별될 수 있다. 샘플 분류 확률이 0.5보다 작거나 같은 경우, 트레이닝 샘플은 모델 게임 샘플로 판별될 수 있음 - ; 를 포함할 수 있다 샘플 분류 확률은 다수의 결정 정보 쌍의 정보 쌍 분류 확률에 따라 계산되며, 이는 판별자 모델의 판별 정확도 를 개선하고 비정상적인 판별 결과의 문제를 피할 수 있다. 단계 S850. 샘플 판별 정보에 따라 액션 모델의 모델 파라미터 및 판별자 모델의 모델 파라미터를 업데이트한다. 액션 모델 및 판별자 모델은 적대적 프로세스 동안 액션 모델 및 판별자 모델의 모델 파라미터를 지속적으로 업 데이트하기 위해 생성적 적대 네트워크를 형성할 수 있다. 판별자 모델은 모델 파라미터를 업데이트하고 최적화 하여 샘플 판별 정보의 정확도를 향상시키기 위해 자체 판별 기능을 최대한 개선해야 한다. 또한, 판별자 모델 이 트레이닝 샘플의 샘플 유형을 정확하게 구별하기 위해, 액션 모델은 자신의 모방 능력을 최대한 향상시키고, 모델 파라미터를 업데이트 및 최적화하여 사용자 게임 샘플과 확률 분포가 모델 게임 샘플의 확률 분포와 가까 운 모델 게임 샘플을 출력해야 한다. 적대적 학습을 통해 모델 파라미터를 반복적으로 업데이트함으로써 실제 게임 사용자의 결정 행동 특성에 가까운 액션 모델을 얻을 수 있다. 일부 실시예에서, 이 단계에서, 목적 함수가 먼저 결정될 수 있으며, 목적 함수는 사용자 게임 샘플에 대응하는 사용자 샘플 기대치 및 모델 게임 샘플에 대응하는 모델 샘플 기대치를 포함하고; 액션 모델의 모델 파라미터 및 판별자 모델의 모델 파라미터는 샘플 판별 정보 및 목적 함수에 따라 교대로 업데이트된다. 실제 구현에서, 액션 모델과 판별자 모델의 파라미터는 적대적 게임을 통해 업데이트되며, 액션 모델과 판별자 모델의 공통 목적 함수는 사용자 게임 샘플에 대응하는 사용자 샘플 기대치 및 모델 게임 샘플에 대응하는 모델 샘플 기대치를 포함한다. 예를 들어, 사용자 샘플 기대치는 로 표현될 수 있고, 모델 샘플 기대치는 로 표현될 수 있고. 는 사용자 게임 샘플의 확률 분포를 나타내고, 는 판별자 모델에서 사용자 게임 샘플의 샘플 분류 확률을 나타낸다. 는 모델 게임 샘플의 확률 분 포를 나타내고, 는 판별자 모델에서 모델 게임 샘플의 샘플 분류 확률을 나타낸다. 액션 모델 및 판별자 모델의 파라미터 업데이트 프로세스는 교대로 수행될 수 있다. 예를 들어, 액션 모델의 모 델 파라미터가 한 번 업데이트된 후, 판별자 모델의 모델 파라미터가 즉시 한 번 업데이트되어 모델 파라미터의 반복 업데이트가 교대로 반복적으로 수행될 수 있다. 다른 예에서, 모델 트레이닝 효율을 향상시키기 위해, 액 션 모델은 여러 번 계속해서 반복적으로 업데이트될 수 있고, 그 후 판별자 모델은 한 번 업데이트될 수 있다. 판별자 모델의 트레이닝 목적은 트레이닝 샘플에서 사용자 게임 샘플과 모델 게임 샘플을 가능한 한 정확하게 구별하는 것이다. 따라서, 판별자 모델의 트레이닝 라운드에서 액션 모델의 모델 파라미터는 고정될 수 있고, 판별자 모델의 모델 파라미터는 샘플 판별 정보 및 목적 함수에 따라 업데이트되어, 샘플 분류 확률을 높일 수 있다. 사용자 게임 샘플을 추출하고 모델 게임 샘플의 샘플 분류 확률을 줄인다. 액션 모델의 학습 목표는 판별자 모델을 최대한 속이기 때문에 판별자 모델이 학습 샘플에서 사용자 게임 샘플 과 모델 게임 샘플을 서로 정확하게 구별하기 어렵도록 하는 것이다. 따라서, 액션 모델의 트레이닝 라운드에서 판별자 모델의 모델 파라미터는 고정될 수 있고, 액션 모델의 모델 파라미터는 샘플 판별 정보 및 목적 함수에 따라 업데이트되어 모델 게임 샘플의 샘플 분류 확률을 높일 수 있다. 본 출원의 본 실시예에서 제공된 액션 모델에 대한 모델 최적화 방법에서, 사용자 게임 샘플의 확률 분포는 생 성적 적대적 모방 학습을 통해 실제 게임 사용자의 게임 데이터로부터 학습되고, 액션 모델은 실제 게임 사용자 의 행동 특성에 가깝거나 실제 게임 사용자의 행동 기대에 부합하는 게임 액션 정책을 만들도록 가이드될 수 있 다. 생성적 적대적 모방 학습에 기반한 트레이닝 방법은 모델 트레이닝 프로세스에서 컴퓨팅 자원 소비를 줄일 수 있을 뿐만 아니라 모델 트레이닝 효율성을 향상시켜 더 나은 트레이닝 효과를 얻을 수 있다. 턴 기반 게임에서의 응용 시나리오를 참조하여, 전술한 실시예에 관련된 액션 모델에 대한 트레이닝 방법이 이 하에서 설명된다. 도 9는 본 출원의 실시예에 따른 액션 모델의 네트워크 아키텍처의 개략도이다. 도 9에 도시된 바와 같이, 액션 모델은 주로 다층 지각 네트워크 구조이고, 모델에 대한 입력은 현재 게임 라운드의 게임 상태 특징 상태이다. 예를 들어, 게임 상태 특징 상태는 38차원 특징 벡터로 표현될 수 있으며 관련된 정보는 예를 들어 다음을 포함 할 수 있다: a) 체력, 물리 공격, 마법 공격, 마법, 치료, 물리 방어, 마법 방어, 속도, 봉인 적중률 및 봉인 저항과 같은 역할의 기본 속성; b) 역할의 점유; c) 전투의 전술적 특징; d) 현재 전투의 라운드 수; 및 e) 현재 이용 가능한 기술. 액션 모델의 전체 구조는 주로: 차원이 각각 1024, 512 및 256인 3개의 완전 연결 계층 FC910, FC920 및 FC930 과, 출력 벡터 차원이 8인 하나의 완전 연결 출력 계층을 포함하고, 완전 연결 출력 계층은 총 8개의 게임 스킬 스킬(skill_1, skill_2, …, skill_8)의 선택 확률을 출력할 수 있다. 완전히 연결된 출력 계층 에서 출력된 8차원 벡터를 차원이 8인 스킬 스크리닝 벡터와 곱하여 출력 벡터 차원이 8인 스킬 출력 계층을 최종적으로 획득하고, 스킬 출력 계층에 의해 출력된 결과는 게임에서 게임 역할의 각 스킬이 이번 라운드에서 출시될 확률의 분포이다. 완전 연결 출력 계층의 출력 벡터의 차원은 8이며, 이는 게임 역할이 최대 8개의 스킬을 갖는다는 것을 나 타낸다. 턴 기반 게임에서 게임 역할의 일부 스킬은 특정 라운드에서 시전된 후 추가 효과를 가질 수 있으며, 예를 들어 플레이어가 다음 라운드에서 스킬을 시전할 수 없도록 휴식 상태로 렌더링하거나, 일부 특정 기술을 사용할 수 없도록 게임 역할의 히트 포인트를 특정 상태보다 낮게 렌더링한다. 따라서 각 라운드에서 게임 역할 의 실제 가용 스킬을 얻기 위해서는 액션 모델이 예측하여 출력하는 각 스킬의 예측 확률에 스킬 스크리닝 벡터 를 곱해야 한다. 차원이 8인 스킬 스크리닝 벡터는 0 또는 1 값을 갖는 8개의 요소를 포함하며, 0 또는 1 값은 각 라운드에서 게임 클라이언트가 전송하는 사용 가능한 스킬 목록에 의해 결정된다. 예를 들어 턴 기반 롤 플 레잉 게임에서 힘을 점유하는 남성 역할을 맡는 경우 게임 역할의 스킬 목록은 [\"타오르는 태양 충격\", \"삼중의 강력한 베기\", \"생명 인식\", \"혈액 약탈\", \"섀도우 델버\", \"아이오 ?지구 크랙 스타\"]([\"Blazing sun shock\", \"Triple powerful slashes\", \"Life perception\", \"Blood plundering\", \"Shadow delver\", \"Aoyi ?Earth crack star\"])이고, 이것은 총 6개의 액티브 스킬이 포함한다. 게임 라운드에서 게임 역할의 스킬 심사 벡터가 [1, 1, 1, 0, 1, 1, 0, 0]인 경우, 게임 심사 벡터의 네 번째 위치의 값은 0이고, 이것은 이 라운드에서 해당 역할의 네 번째 스킬 \"Blood plundering\"을 사용할 수 없음을 나타내며, 일곱 번째 위치의 값과 여덟 번째 위치의 값은 0으로 채워지는데, 그 이유는 해당 역할은 시전할 수 있는 활성 스킬이 6개뿐이기 때문이며, 나머지 요소의 값 은 1이며, 이는 이 라운드에서 역할의 나머지 스킬을 사용할 수 있음을 나타낸다. 이러한 방식으로 스킬 스크리 닝 벡터가 액션 모델에 도입되어 서로 다른 전투 라운드에서 서로 다른 게임 역할에 대해 선택할 수 있는 게임 스킬을 선택한다. 이러한 방식으로 액션 모델 예측의 정확도를 향상시킬 수 있을 뿐만 아니라 잘못된 기술 선택 을 방지할 수 있다. 도 10은 본 출원의 실시예에 따른 판별자 모델의 네트워크 아키텍처의 개략도이다. 도 10에 도시된 바와 같이, 판별자 모델에 대한 입력은 게임 역할의 게임 트랙 데이터이며, 예를 들어 38차원 게임 상태 특징 st와 8차원 게임 액션 특징 at을 스플라이싱하여 형성된 특징 벡터이다. 판별자 모델에 대한 입력은 실제 게임 사용자의 상 태-액션 추적 데이터일 수 있거나, 또는 액션 모델에 의해 생성된 상태-액션 추적 데이터일 수 있다. 판별자 모 델의 전체 구조는 주로 차원이 각각 1024, 512 및 256인 3개의 완전 연결 계층 FC1010, FC1020 및 FC1030을 포 함한다. 판별자 모델의 출력 계층은 2차원 분류기이다. 모델에 의해 예측된 분류 확률이 0.5보다 크면,판별자 모델은 모델에 입력된 트랙 데이터가 실제 플레이어의 상태-액션 트랙 데이터 Real인 것으로 결정한다. 분류 확률이 0.5보다 작으면, 판별자 모델은 입력된 트랙 데이터가 액션 모델에 의해 생성된 상태-액션 트랙 데 이터 Fake인 것으로 결정한다. 모델 트레이닝 프로세스에서, 판별자 모델의 트레이닝 목적은 액션 모델에 의해 생성된 트랙 데이터 와 실제 게임 사용자의 트랙 데이터 를 최대한 구별하기 위한 것이며, 즉 여 트레이닝 샘플에 올바른 레이블이 할당될 확률이 최대가 되도록 를 최대화하는 것이며, 여기 서 이고, 는 입력된 결정 정보 쌍 에 대한 액션 모델의 출력 확률이다. 액 션 모델의 트레이닝 목표는 분포가 실제 게임 사용자의 실제 게임 트랙 데이터의 분포와 가까운 샘플을 생성하 여 판별자 모델을 최대한 속이는 것이며, 즉 를 최소화하는 것이다. 생성적 적대적 모방 학습의 본질은 최소-최대 적대 게임을 지속적으로 수행하는 것이다. 목적 함수는 다음과 같다: . 모델 트레이닝의 초기 단계에서는 트레이닝 초기의 액션 모델의 모방 능력이 떨어지기 때문에, 모델이 출력하는 결과는 실제 게임 사용자의 게임 데이터와 명백히 다르다. 따라서 판별자 모델은 매우 높은 신뢰도로 진위 여부 를 판단할 수 있으며, 출력되는 확률 값은 1 또는 0에 가깝다. 이러한 방법으로, 생성 네트워크의 기울기가 쉽 게 사라진다. 이 경우, 목적 함수에서, 의 최소화는 액션 모델의 트레이닝 라운드에서 의 최대화로 대체될 수 있고, 이는 트레이닝의 초기 단계에서 상대적으로 큰 기울기를 제공한다. 액션 모델과 판별 모델에 의해 형성된 생성적 적대 네트워크가 구축된 후, 모델 트레이닝이 시작될 수 있다. 먼저, 액션 모델 및 판별자 모델의 가중치 파라미터를 무작위로 초기화하고, 신경망 모델의 가중치를 무작위로 초기화함으로써 모델의 수렴 속도 및 성능을 향상시킬 수 있다. 그런 다음, 게임의 현재 게임 라운드에서 게임 상태 특징 상태는 가중치 파라미터가 인 액션 모델에 대한 입 력으로 사용되며, 액션 모델에서 출력되는 게임 액션 특징 액션은 게임 라운드에서 게임 역할에 의해 시전된 스 킬로 결정되고, 상태 액션 시퀀스 는 액션 모델과 게임 환경 간의 지속적인 상호작용에 의해 생성될 수 있다. 이러한 방식으로, 액션 모델에 의해 생성된 트랙 데이터 세트 는 게임에서 N 개 라운드의 전투 후에 획득될 수 있다. 교차 엔트로피 손실 함수를 이용하여 판별자 모델의 모델 파라미터를 업데이트하고, 실제 게임 사용자의 실제 게임 트랙에 대응하는 의 출력 확률을 높이고, 액션 모델에 의해 생성된 생성 게임 트랙에 대응하는 의 출력 확률을 감소시킨다. 액션 모델의 모델 파라미터를 업데이트하기 위해 심층 강화 학습의 정책 기울기 알고리즘을 사용하여 의 출 력 확률을 높인다. 강화 학습의 목적 함수는 다음과 같다: , 여기서 는 상태 및 동작 시퀀스의 그룹을 나타낸다. 는 시퀀스 τ의 누적 보상의 합을 나타낸다. 는 시퀀스 τ의 발생 확률을 나타낸다. 정책 기울기 방법의 목적은 누적 보상의 기대치가 최대가 되도록 정책 함수를 나타내는 최적 파라미터 집합 을 찾는 것이고, 즉:. 최적의 파라미터 를 검색하는 프로세스는 최적의 정책 또는 최적의 경로를 찾는 프로세스이고, 이것은 파라미 터 최적화를 수행하고 정책 기울기 알고리즘에서 기울기 하강 알고리즘을 이용하여 업데이트함으로써 구현되며, 즉, , 여기서 η는 학습률이다. 목적 함수의 기울기는 다음과 같이 계산된다:"}
{"patent_id": "10-2022-7016867", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": ". 기울기의 계산은 의 기대치의 풀이로 변환되며, Monte Carlo 방법을 사용하여 기울기를 근사할 수 있으며, 즉 현재 정책에 따라 N개의 트랙을 샘플링하여 목적 함수의 기울기를 근사화한다: . 생성적 적대적 모방 학습 동안, 액션 모델이 정책 기울기 알고리즘을 사용하여 파라미터를 업데이트할 때, R(τ)은 시스템에서 직접 제공되지 않는다. 대신, 판별자 모델의 출력 D(τ)는 시퀀스 τ의 보상의 합을 나타내 는 R(τ)로 사용된다. 따라서, 작업 모델의 파라미터는 다음과 같이 업데이트된다: . 본 출원의 본 실시예에서 제공되는 액션 모델 트레이닝을 위한 기술적 솔루션에 기초하여, 실제 게임 사용자의 액션 결정 습관에 가깝거나 실제 게임 사용자의 액션 결정 기대를 충족시키는 게임 AI가 구성될 수 있다. 비디 오 게임에서. 게임 AI는 게임 실행 프로세스에 참여하기 위한 NPC 역할로 사용될 수도 있고, 게임 개발 프로세 스에서 자동화된 테스트를 수행하기 위한 테스트 역할로 사용될 수도 있다. 본 출원의 방법의 실시예의 단계가 첨부 도면에서 특정 순서로 설명되지만, 이는 단계가 특정 순서로 수행되어 야 하는 것으로 요구하지 않고 암시하지 않거나, 도시된 모든 단계가 예상 결과를 달성하기 위해 수행된다. 추 가적으로 또는 대안적으로, 일부 단계가 생략될 수 있고, 복수의 단계가 실행을 위해 하나의 단계로 결합될 수 있고, 및/또는 하나의 단계가 실행을 위해 복수의 단계로 분해될 수 있다. 다음은 본 출원의 실시예의 장치 실시예를 설명하고, 장치 실시예는 전술한 실시예의 액션 모델에 대한 트레이 닝 방법 또는 AI에 기초한 게임 액션 의사 결정 방법을 수행하는 데 사용될 수 있다. 본 출원의 장치 실시예에 개시되지 않은 세부사항에 대해, 본 출원의 전술한 방법 실시예를 참조할 수 있다. 도 11은 본 출원의 실시예에 따른 정보 처리 장치의 개략적인 구조적 블록도이다. 도 11에 도시된 바와 같이, 정보 처리 장치는 주로: 게임 시나리오에서 게임 액션 주체(game action subject)를 결정하고, 상기 게임 액션 주체가 게임 액션을 실행 하도록 제어하는 데 사용되는 액션 모델을 획득하도록 구성되어 있는 모델 획득 모듈; 상기 게임 시나리오에 대한 특징 추출을 수행하여 상기 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득하 도록 구성되어 있는 특징 추출 모듈; 상기 액션 모델을 사용함으로써 상기 모델 게임 상태 정보에 대해 매핑 처리를 수행하여 적어도 2개의 후보 게 임 액션에 대응하는 모델 게임 액션 선택 정보를 획득하도록 구성되어 있는 매핑 처리 모듈; 및 상기 모델 게임 액션 선택 정보에 따라, 상기 적어도 2개의 후보 게임 액션으로부터 상기 게임 액션 주체에 의 해 실행되는 모델 게임 액션을 선택하도록 구성되어 있는 액션 선택 모듈 을 포함한다. 본 출원의 실시예에서, 특징 추출 모듈은: 게임 시나리오 내의 시나리오 상태 정보 및 게임 액션 주체의 주체 상태 정보를 획득하도록 구성된 정보 획득 유닛; 시나리오 특징 벡터를 획득하기 위해 시나리오 상태 정보에 특징 추출을 수행하고, 주체 특징 벡터를 획득하기 위해 주체 상태 정보에 특징 추출을 수행하도록 구성된 특징 추출 유닛; 및 게임 액션 주체와 관련된 모델 게임 상태 정보를 획득하기 위해 시나리오 특징 벡터 및 주체 특징 벡터에 대해 스플라이싱 처리를 수행하도록 구성된 벡터 스플라이싱 유닛 을 포함할 수 있다. 일부 실시예에서, 매핑 처리 모듈은: 게임 액션 주체와 관련된 적어도 2개의 후보 게임 액션을 결정하도록 구성된 액션 결정 유닛; 액션 모델을 사용함으로써 모델 게임 상태 정보에 대한 매핑 처리를 수행하여 각 후보 게임 액션의 선택 확률을 획득하도록 구성된 매핑 처리 유닛; 각각의 후보 게임 액션의 액션 가용 상태 정보를 획득하고, 액션 가용 상태 정보에 따라 후보 게임 액션에 대응 하는 액션 스크리닝 정보를 결정하도록 구성된 액션 스크리닝 유닛; 및 액션 스크리닝 정보에 따라 후보 게임 액션의 선택 확률을 조정하고, 모델 게임 액션 선택 정보로서 조정된 선 택 확률을 결정하도록 구성된 확률 조정 유닛 을 포함할 수 있다. 일부 실시예에서, 확률 조정 유닛은: 액션 스크리닝 정보에 따라 후보 게임 액션의 액션 유형을 결정하도록 구성된 액션 결정 서브유닛 - 상기 액션 유형은 사용 가능한 액션 및 사용할 수 없는 액션을 포함함 - ; 및 후보 게임 액션의 액션 유형이 비가용 액션인 경우에, 후보 게임 액션의 선택 확률을 미리 설정된 확률로 조정 하도록 구성된 확률 조정 서브유닛 을 포함할 수 있다. 일부 실시예에서, 액션 선택 모듈: 랜덤 액션 선택 정책의 제1 확률 및 높은 확률 액션 선택 정책의 제2 확률을 결정하도록 구성된 확률 결정 유닛; 제1 확률 및 제2 확률에 따라, 모델 게임 액션을 선택하는 데 사용되는 모델 선택 정책을 결정하도록 구성된 정 책 결정 유닛; 모델 선택 정책이 랜덤 액션 선택 정책인 경우, 모델 게임 액션으로서 적어도 2개의 후보 게임 액션 중에서 하 나의 후보 게임 액션을 무작위로 선택하도록 구성된 제1 선택 유닛; 및 모델 선택 정책이 고가치의 액션 선택 정책인 경우에 모델 게임 액션으로서 적어도 2개의 후보 게임 액션들 중 에서 가장 높은 행동 값을 갖는 후보 게임 액션을 선택하도록 구성된 제2 선택 유닛 을 포함할 수 있다. 일부 실시예에서, 정보 처리 장치는: 모델 게임 액션의 모델 게임 상태 정보 및 모델 게임 액션 정보를 모델 게임 샘플로 결합하도록 구성된 모델 샘 플 획득 모듈; 게임 액션 주체와 관련된 사용자 게임 데이터를 획득하고, 사용자 게임 데이터에 따라 사용자 게임 상태 정보 및 사용자 게임 액션 정보를 포함하는 사용자 게임 샘플을 결정하도록 구성된 사용자 샘플 획득 모듈; 모델 게임 샘플 및 사용자 게임 샘플을 트레이닝 샘플로 결정하고, 트레이닝 샘플을 판별자 모델에 입력하고도 록 구성된 샘플 입력 모듈;샘플 판별 정보를 획득하기 위해 판별자 모델을 사용하여 트레이닝 샘플에 대한 매핑 처리를 수행하도록 구성된 샘플 판별 모듈 - 상기 샘플 판별 정보는 트레이닝 샘플이 모델 게임 샘플인지 또는 사용자 게임 샘플인지를 판 별하는 데 사용됨 - ; 및 샘플 판별 정보에 따라 액션 모델의 모델 파라미터 및 판별자 모델의 모델 파라미터를 업데이트하도록 구성된 파라미터 업데이트 모듈 을 더 포함할 수 있다. 일부 실시예에서, 모델 샘플 획득 모듈은: 게임 시나리오에서 적어도 하나의 게임 라운드를 결정하고, 게임 라운드의 게임 순서 정보를 획득하도록 구성된 모델 라운드 결정 유닛; 게임 라운드에 대응하는 모델 게임 액션의 모델 게임 상태 정보 및 모델 게임 액션 정보를 모델 결정 정보 쌍으 로 결합하도록 구성된 모델 정보 획득 유닛; 및 모든 게임 라운드의 모델 결정 정보 쌍을 게임 순서 정보에 따라 모델 결정 정보 쌍 순서로 결합하고, 모델 결 정 정보 쌍 순서를 게임 시나리오에 대응하는 모델 게임 샘플로서 결정하도록 구성된 모델 샘플 획득 유닛 을 포함할 수 있다. 일부 실시예에서, 사용자 샘플 획득 모듈은: 사용자 게임 데이터에 따라 게임 시나리오에서 적어도 하나의 게임 라운드를 결정하고, 게임 라운드의 게임 순 서 정보를 획득하도록 구성된 사용자 라운드 결정 유닛; 게임 라운드에 대응하는 사용자 게임 상태 정보 및 사용자 게임 액션 정보를 사용자 결정 정보 쌍으로 결합하도 록 구성된 사용자 정보 획득 유닛; 및 모든 게임 라운드의 사용자 결정 정보 쌍을 게임 순서 정보에 따라 사용자 결정 정보 쌍 순서로 결합하고, 사용 자 결정 정보 쌍 순서를 게임 시나리오에 대응하는 사용자 게임 샘플로서 결정하도록 구성된 사용자 샘플 획득 유닛 을 포함할 수 있다. 일부 실시예에서, 샘플 입력 모듈은: 트레이닝 샘플로부터, 게임 순서로 배열된 결정 정보 쌍을 획득하고, 각 결정 정보 쌍에서 게임 상태 정보 및 게임 액션 정보를 획득하도록 구성된 샘플 정보 획득 유닛; 게임 상태 정보에 대응하는 제1 특징 벡터 및 게임 액션 정보에 대응하는 제2 특징 벡터를 획득하도록 구성된 샘플 벡터 획득 유닛; 결정 정보 쌍의 샘플 특징 벡터를 획득하기 위해 제1 특징 벡터 및 제2 특징 벡터에 대해 스플라이싱 처리를 수 행하도록 구성된 샘플 벡터 스플라이싱 유닛; 및 트레이닝 샘플의 결정 정보 쌍의 샘플 특징 벡터를 게임 순서에 따라 판별자 모델에 순차적으로 입력하도록 구 성된 샘플 벡터 입력 유닛 을 포함할 수 있다. 일부 실시예에서, 샘플 식별 모듈은: 트레이닝 샘플에서 결정 정보 쌍의 정보 쌍 수량을 획득하도록 구성된 정보 쌍 수량 획득 유닛; 판별자 모델을 사용하여 각 결정 정보 쌍의 샘플 특징 벡터에 대해 매핑 처리를 수행하여 각 결정 정보 쌍의 정 보 쌍 분류 확률을 획득하도록 구성된 정보 쌍 확률 결정 유닛; 및 정보 쌍 수량 및 정보 쌍 분류 확률에 따라 트레이닝 샘플의 샘플 분류 확률을 결정하고, 샘플 분류 확률을 샘 플 판별 정보로서 결정하도록 구성된 샘플 확률 결정 유닛 을 포함할 수 있다. 일부 실시예에서, 파라미터 업데이트 모듈은: 사용자 게임 샘플에 대응하는 사용자 샘플 기대치 및 모델 게임 샘플에 대응하는 모델 샘플 기대치를 포함하는 목적 함수를 결정하도록 구성된 함수 결정 유닛; 및 샘플 판별 정보 및 목적 함수에 따라 액션 모델의 모델 파라미터 및 판별자 모델의 모델 파라미터를 교대로 업 데이트하도록 구성된 파라미터 업데이트 유닛 을 포함할 수 있다. 일부 실시예에서, 파라미터 업데이트 유닛은: 액션 모델의 모델 파라미터를 고정하고 샘플 판별 정보 및 목적 함수에 따라 판별자 모델의 모델 파라미터를 업 데이트하여 사용자 게임 샘플의 샘플 분류 확률을 증가시키고 모델 게임 샘플의 샘플 분류 확률을 감소시키도록 구성된 액션 모델 업데이트 서브유닛; 및 판별자 모델의 모델 파라미터를 고정하고, 샘플 판별 정보 및 목적 함수에 따라 액션 모델의 모델 파라미터를 업데이트하여, 모델 게임 샘플의 샘플 분류 확률을 증가시키도록 구성된 판별자 모델 업데이트 서브유닛 을 포함할 수 있다. 본 출원의 실시예에서 제공되는 정보 처리 장치의 세부사항은 대응하는 방법 실시예에서 상세히 설명되었다. 따 라서 여기에서 세부 사항을 다시 설명하지 않는다. 도 12는 본 출원의 실시예에 따른 전자 장치의 컴퓨터 시스템의 개략적인 구조도이다. 도 12에 도시된 전자 장치의 컴퓨터 시스템은 예시일 뿐, 본 출원의 실시예의 기능 및 사용 범위를 제한 하지 않는다는 것에 유의해야 한다. 도 12에 도시된 바와 같이, 컴퓨터 시스템은 중앙 처리 장치(CPU)를 포함하는데, 이것은 읽기 전용 메모리(ROM)에 저장된 프로그램 또는 저장부로부터 RAM으로 로드된 프로그램에 기초하여 다 양한 적절한 동작 및 처리를 수행할 수 있다. RAM은 시스템 동작에 필요한 다양한 프로그램 및 데이터를 더 저장한다. CPU, ROM, 및 RAM은 버스를 통해 서로 연결된다. 입출력(I/O) 인터페이 스도 버스에 연결된다. I/O 인터페이스에는 다음: 키보드, 마우스 등을 포함하는 입력부, 음극선관(CRT), 액정 디스플레이 (LCD), 스피커 등을 포함하는 출력부, 하드 디스크 등을 포함하는 저장부, 및 근거리 통신망(LAN) 카드 또는 모뎀과 같은 네트워크 인터페이스 카드를 포함하는 통신부와 같은 구성요소가 연결된다. 통신 부는 인터넷 등의 네트워크를 이용하여 통신 처리를 행한다. 드라이버는 또한 필요에 따라 I/O 인 터페이스에 연결된다. 탈착식 매체로부터 읽어들인 컴퓨터 프로그램이 저장부에 필요에 따라 설치 되도록 드라이버에는 자기디스크, 광디스크, 광자기디스크, 반도체 메모리 등과 같은 탈착식 매체 가 필요에 따라 설치된다. 본 출원의 실시예에 따르면, 각 방법의 흐름도를 참조하여 전술한 프로세스는 컴퓨터 소프트웨어 프로그램으로 구현될 수 있다. 예를 들어, 본 출원의 이 실시예는 컴퓨터 프로그램 제품을 포함하고, 컴퓨터 프로그램 제품은 컴퓨터 판독 가능 매체에 저장된 컴퓨터 프로그램을 포함하고, 컴퓨터 프로그램은 흐름도에 도시된 방법을 수행 하는 데 사용되는 프로그램 코드를 포함한다. 그러한 실시예에서, 컴퓨터 프로그램은 통신부를 통해 네트 워크로부터 다운로드 및 설치될 수 있고 및/또는 탈착식 매체로부터 설치될 수 있다. 컴퓨터 프로그램이 CPU에 의해 실행될 때, 이 응용 프로그램의 시스템에 정의된 다양한 기능이 실행된다. 본 출원의 실시예에 도시된 컴퓨터 판독 가능형 저장 매체는 컴퓨터 판독 가능 신호 매체 또는 컴퓨터 판독 가 능 저장 매체 또는 이들의 임의의 조합일 수 있다는 점에 유의해야 한다. 컴퓨터 판독 가능형 저장 매체는 예를 들어 전기, 자기, 광학, 전자기, 적외선, 또는 반도체 시스템, 장치, 또는 구성요소, 또는 이들의 임의의 조합 일 수 있지만 이에 제한되지는 않는다. 컴퓨터 판독 가능형 저장 매체의 예는: 하나 이상의 와이어를 갖는 전기 적 연결, 휴대용 컴퓨터 자기 디스크, 하드 디스크, RAM, ROM, 소거 가능한 프로그래머블 읽기 전용 메모리 (erasable programmable read-only memory, EPROM), 플래시 메모리, 광섬유, CD-ROM(Compact Disc Read-Only Memory), 광학 저장 장치, 자기 저장 장치 또는 이들의 적절한 조합을 포함한다. 본 출원의 실시예에서, 컴퓨터 판독 가능형 저장 매체는 프로그램을 포함하거나 저장하는 임의의 유형의 매체일 수 있고, 프로그램은 명령 실 행 시스템, 장치 또는 구성요소에 의해 또는 조합되어 사용될 수 있다. 본 출원의 실시예들에서, 컴퓨터 판독가능형 신호 매체는 기저대역에서 또는 캐리어의 일부로 전파되는 데이터 신호를 포함할 수 있고, 컴퓨터 판독 가능형 프로그램 코드를 저장한다. 이러한 방식으로 전파되는 데이터 신호는 전자기 신호, 광 신호, 또는 이들 의 임의의 적절한 조합을 포함하지만 이에 제한되지 않는 복수의 형태를 취할 수 있다. 컴퓨터 판독 가능 신호 매체는 대안적으로 컴퓨터 판독 가능형 저장 매체 이외의 임의의 컴퓨터 판독 가능형 저장 매체일 수 있다. 컴 퓨터 판독 가능형 저장 매체는 명령 실행 시스템, 장치 또는 장치에 의해 또는 조합하여 사용하기 위한 프로그 램을 전송, 전파 또는 전송할 수 있다. 컴퓨터 판독 가능형 매체에 포함된 프로그램 코드는 무선 매체, 유선 매 체 등, 또는 이들의 임의의 적절한 조합을 포함하나 이에 제한되지 않는 임의의 적절한 매체를 사용하여 송신될 수 있다. 첨부 도면의 흐름도 및 블록도는 본 출원의 다양한 실시예에 따른 시스템, 방법, 및 컴퓨터 프로그램 제품에 의 해 구현될 수 있는 가능한 시스템 아키텍처, 기능 및 동작을 예시한다. 이와 관련하여 흐름도 또는 블록도의 각 상자는 모듈, 프로그램 세그먼트 또는 코드의 일부를 나타낼 수 있다. 모듈, 프로그램 세그먼트 또는 코드의 일 부는 지정된 논리 기능을 구현하는 데 사용되는 하나 이상의 실행 가능한 명령을 포함한다. 대안으로 사용되는 일부 구현에서, 상자에 주석이 달린 기능은 대안적으로 첨부 도면에 주석이 달린 것과 다른 순서로 발생할 수 있다. 예를 들어, 실제로 연속적으로 도시된 두 개의 박스는 기본적으로 병렬로 수행될 수 있고, 때로는 두 개 의 박스가 역순으로 수행될 수 있다. 이것은 관련 함수에 의해 결정된다. 블록도 및/또는 흐름도의 각 상자와 블록도 및/또는 흐름도의 상자 조합은 지정된 기능 또는 동작을 수행하도록 구성된 전용 하드웨어 기반 시스템 을 사용하여 구현될 수 있거나 전용 하드웨어와 컴퓨터 명령의 조합을 사용하여 구현될 수 있다. 전술한 상세한 설명에서 액션 실행을 위한 장치의 여러 모듈 또는 유닛이 언급되었지만 그 구분이 필수는 아니 다. 실제로, 본 출원의 구현에 따르면, 상술한 둘 이상의 모듈 또는 유닛의 특징 및 기능은 하나의 모듈 또는 유닛에 명시될 수 있다. 반대로, 상술한 하나의 모듈 또는 유닛의 특징 및 기능을 더 세분화하여 복수의 모듈 또는 유닛으로 지정할 수 있다."}
{"patent_id": "10-2022-7016867", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같은 실시예들에 대한 설명을 통하여, 본 기술분야의 통상의 지식을 가진 자는 여기서 설명하는 실시예 들이 소프트웨어를 통해 구현될 수도 있고, 필요한 하드웨어와 결합된 소프트웨어를 통해 구현될 수도 있음을 쉽게 이해할 수 있을 것이다. 따라서, 본 출원의 실시예의 기술 솔루션은 소프트웨어 제품의 형태로 구현될 수 있다. 소프트웨어 제품은 컴퓨팅 장치(이것은 개인용 컴퓨터, 서버, 터치 단말, 네트워크 장치 등일 수 있다)가 본 출원의 실시예에 따른 방법을 수행하도록 명령하는 수 개의 명령을 포함하는 비휘발성 저장 매체(이것은 CD- ROM, USB 플래시 드라이브, 탈착식 하드 디스크 등이 될 수 있다) 또는 네트워크 상에 저장될 수 있다. 명세서를 고려하고 본 개시를 실시한 후, 당업자는 본 출원의 다른 구현을 용이하게 생각할 수 있다. 본 출원은 본 출원의 일반 원칙에 따른 본 출원의 임의의 변형, 사용 또는 적응적 변경을 포함하도록 의도되었으며, 본 출 원에서 공개되지 않은 해당 기술 분야의 잘 알려진 지식 및 통상적인 기술 수단을 포함한다. 본 출원은 위에서 설명되고 첨부 도면에 도시된 정확한 구조에 제한되지 않으며, 본 출원의 범위를 벗어나지 않 고 다양한 수정 및 변경이 이루어질 수 있음을 이해해야 한다. 본 출원의 범위는 첨부된 청구범위에만 적용된다. 산업적 실용성 본 출원의 실시예에서, 전자 장치는 게임 시나리오에서 게임 액션 주체를 결정하고, 게임 액션 주체가 게임 액 션을 실행하도록 제어하는 데 사용되는 액션 모델을 획득하고; 게임 시나리오에 대한 특징 추출을 수행하여 게 임 액션 주체와 관련된 모델 게임 상태 정보를 획득하고; 액션 모델을 사용함으로써 모델 게임 상태 정보에 대 한 매핑 처리를 수행하여 적어도 두 개의 후보 게임 액션에 대응하는 모델 게임 액션 선택 정보를 획득하고; 그 리고 모델 게임 액션 선택 정보에 따라, 적어도 2개의 후보 게임 액션으로부터 게임 액션 주체에 의해 실행되는 모델 게임 액션을 선택한다. 이와 같이 게임에서 게임 AI를 구성하는 경우 게임 AI의 의사 결정 능력이 크게 향 상되어 게임 AI는 의인화 효과와 지능 수준이 더 높아지고 게임 사용자의 게임 경험을 높일 수 있다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12"}
{"patent_id": "10-2022-7016867", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 출원의 실시예에 따른 예시적인 시스템 아키텍처의 개략도이다. 도 2는 본 출원의 일 실시예에 따른 생성적 적대적 모방 학습 원리의 개략도이다. 도 3은 본 출원의 실시예에 따른 생성적 적대적 모방 학습의 모델 아키텍처의 개략도이다. 도 4는 본 출원의 실시예에 따른 게임 액션 결정을 위한 정보 처리 방법의 개략적인 흐름도이다. 도 5는 본 출원의 실시예에 따른 게임 시나리오에 대한 특징 추출의 개략적인 흐름도이다. 도 6은 본 출원의 일 실시예에 따른 액션 모델을 이용한 특징 매핑의 개략적인 흐름도이다. 도 7은 본 출원의 실시예에 따른 ε-greedy 정책에 기초한 모델 게임 액션을 선택하는 개략적인 흐름도이다. 도 8은 본 출원의 실시예에 따른 액션 모델에 대한 모델 최적화의 개략적인 흐름도이다. 도 9는 본 출원의 실시예에 따른 액션 모델의 네트워크 아키텍처의 개략도이다. 도 10은 본 출원의 실시예에 따른 판별자 모델의 네트워크 아키텍처의 개략도이다. 도 11은 본 출원의 일 실시예에 따른 정보 처리 장치의 구조적 블록도이다. 도 12는 본 출원의 실시예에 따른 전자 장치의 컴퓨터 시스템의 개략적인 구조도이다."}
