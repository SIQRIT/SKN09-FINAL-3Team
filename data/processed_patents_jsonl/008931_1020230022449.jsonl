{"patent_id": "10-2023-0022449", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0053501", "출원번호": "10-2023-0022449", "발명의 명칭": "동영상을 처리하기 위한 장치 및 이의 동작 방법", "출원인": "삼성전자주식회사", "발명자": "김경래"}}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "이미지 신호 및 믹스된 오디오 신호를 포함하는 동영상을 처리하기 위한 전자 장치(200)에 있어서,상기 동영상을 처리하기 위한 프로그램이 저장되는 메모리(250); 및상기 프로그램을 실행함으로써 상기 동영상을 처리하는 적어도 하나의 프로세서(240)를 포함하고,상기 적어도 하나의 프로세서(240)는,제1 인공지능 모델(310)을 이용하여, 상기 이미지 신호 및 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오신호에 포함된 복수의 음원의 겹침(overlap) 정도를 나타내는 오디오 관련 정보를 생성하고,상기 오디오 관련 정보를 제2 인공지능 모델(320)에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 오디오 관련 정보는 시간-주파수 도메인에서 복수의 음원이 겹치는 정도를 확률로 나타낸 맵(map)을 포함하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 또는 제2항에 있어서,상기 제1 인공지능 모델(310)은,상기 이미지 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원에 대응하는 복수의 화자의 시간적 발화 정보를 나타내는 복수의 입 움직임 정보를 생성하는 제1 서브모델(311), 및상기 복수의 입 움직임 정보에 기초하여, 상기 믹스된 오디오 신호로부터, 상기 오디오 관련 정보를 생성하는제2 서브모델(312)을 포함하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 내지 제3항 중 어느 한 항에 있어서,상기 제1 인공지능 모델(310)은 학습용 이미지 신호 및 학습용 오디오 신호로부터 추정된 오디오 관련 정보와그라운드 트루스(ground truth)를 비교함으로써 학습되고,상기 그라운드 트루스는 상기 학습용 오디오 신호에 포함된 복수의 학습용 개별 음원 각각에 기초하여 생성된복수의 스펙트로그램(spectrogram)으로부터 복수의 확률 맵을 생성하고, 상기 복수의 확률 맵 상호 간의 곱(product) 연산에 의해 생성되는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항 내지 제4항 중 어느 한 항에 있어서,상기 복수의 확률 맵 각각은 에 의해 생성되고,여기서, 는 상기 복수의 스펙트로그램 중 대응하는 스펙트로그램의 크기이고, 은 x가 1공개특허 10-2024-0053501-3-보다 작은 경우 x를 출력하고, x가 1이상인 경우 1을 출력하는 함수인, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제5항 중 어느 한 항에 있어서,상기 제2 인공지능 모델(320)은 입력 레이어(321), 복수의 특징 레이어를 포함하는 인코더(322), 및 병목(bottleneck) 레이어(323)를 포함하고,상기 오디오 관련 정보를 상기 제2 인공지능 모델(320)에 적용하는 것은,상기 오디오 관련 정보를 상기 입력 레이어(321)에 적용하는 것,상기 오디오 관련 정보를 상기 인코더(322)에 포함된 상기 복수의 특징 레이어 각각에 적용하는 것, 또는상기 오디오 관련 정보를 상기 병목 레이어(323)에 적용하는 것중 적어도 하나를 포함하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 내지 제6항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(240)는,제3 인공지능 모델(330)을 이용하여, 상기 믹스된 오디오 신호로부터, 또는 상기 믹스된 오디오 신호 및 시각적정보로부터, 상기 믹스된 오디오 신호에 포함된 화자 수 관련 정보를 생성하고,상기 화자 수 관련 정보에 기초하여, 상기 제1 인공지능 모델(310)을 이용하여, 상기 이미지 신호 및 상기 믹스된 오디오 신호로부터, 상기 오디오 관련 정보를 생성하고,상기 화자 수 관련 정보 및 상기 오디오 관련 정보를 상기 제2 인공지능 모델(320)에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하되,상기 시각적 정보는 상기 이미지 신호에 포함된 적어도 하나의 키 프레임(key frame)을 포함하고,상기 적어도 하나의 키 프레임은 상기 믹스된 오디오 신호에 포함된 적어도 하나의 음원에 대응되는 적어도 하나의 화자의 입술을 포함하는 안면 영역을 포함하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항 내지 제7항 중 어느 한 항에 있어서,상기 믹스된 오디오 신호에 포함된 화자 수 관련 정보는,상기 믹스된 오디오 신호와 관련된 제1 화자 수 관련 정보, 또는상기 시각적 정보와 관련된 제2 화자 수 관련 정보 중 적어도 하나를 포함하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제8항 중 어느 한 항에 있어서,상기 제1 화자 수 관련 정보는 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원에 대응되는 화자 수의 확률분포도를 포함하고,상기 제2 화자 수 관련 정보는 상기 시각적 정보에 포함된 화자 수의 확률 분포도를 포함하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제9항 중 어느 한 항에 있어서,상기 제2 인공지능 모델(320)은 입력 레이어(321), 복수의 특징 레이어를 포함하는 인코더(322), 및 병목(bottleneck) 레이어(323)를 포함하고,상기 화자 수 관련 정보를 상기 제2 인공지능 모델(320)에 적용하는 것은,공개특허 10-2024-0053501-4-상기 화자 수 관련 정보를 상기 입력 레이어(321)에 적용하는 것,상기 화자 수 관련 정보를 상기 인코더(322)에 포함된 상기 복수의 특징 레이어 각각에 적용하는 것, 또는상기 화자 수 관련 정보를 상기 병목 레이어(323)에 적용하는 것중 적어도 하나를 포함하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항 내지 제10항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(240)는,상기 이미지 신호로부터 상기 복수의 화자와 연관된 복수의 입 움직임 정보를 획득하고,상기 획득된 복수의 입 움직임 정보를 상기 제2 인공지능 모델에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항 내지 제11항 중 어느 한 항에 있어서,상기 동영상이 재생되는 화면을 표시하고, 사용자로부터 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원에대응하는 복수의 화자 중 적어도 하나의 화자를 선택하기 위한 입력을 수신하는 입출력 인터페이스(220); 및상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 선택된 상기 적어도 하나의 화자에 대응되는 적어도 하나의 음원을 출력하는 오디오 출력부(230)를 더 포함하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항 내지 제12항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(240)는,상기 선택된 적어도 하나의 화자에 대응하는 적어도 하나의 음원의 음량을 조절하기 위한 사용자 인터페이스를상기 화면에 표시하고,사용자로부터의 상기 음량의 조절에 응답하여, 상기 오디오 출력부(230)를 통해 출력되는 상기 적어도 하나의음원의 음량을 조절하는, 전자 장치."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "이미지 신호 및 믹스된 오디오 신호를 포함하는 동영상을 처리하기 위한 방법(900)에 있어서,제1 인공지능 모델(310)을 이용하여, 상기 이미지 신호 및 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오신호에 포함된 복수의 음원의 겹침(overlap) 정도를 나타내는 오디오 관련 정보를 생성하는 단계; 및상기 오디오 관련 정보를 제2 인공지능 모델(320)에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 오디오 관련 정보는 시간-주파수 도메인에서 복수의 음원이 겹치는 정도를 확률로 나타낸 맵(map)을 포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항 또는 제15항에 있어서,상기 오디오 관련 정보를 생성하는 단계는,상기 이미지 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원에 대응하는 복수의 화자의 시간공개특허 10-2024-0053501-5-적 발화 정보를 나타내는 복수의 입 움직임 정보를 생성하는 단계, 및상기 복수의 입 움직임 정보에 기초하여, 상기 믹스된 오디오 신호로부터, 상기 오디오 관련 정보를 생성하는단계를 포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제14항 내지 제16항 중 어느 한 항에 있어서,상기 제1 인공지능 모델(310)은 학습용 이미지 신호 및 학습용 오디오 신호로부터 추정된 오디오 관련 정보와그라운드 트루스(ground truth)를 비교함으로써 학습되고,상기 그라운드 트루스는 상기 학습용 오디오 신호에 포함된 복수의 학습용 개별 음원 각각에 기초하여 생성된복수의 스펙트로그램(spectrogram)으로부터 복수의 확률 맵을 생성하고, 상기 복수의 확률 맵 상호 간의 곱(product) 연산에 의해 생성되는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제14항 내지 제17항 중 어느 한 항에 있어서,상기 복수의 확률 맵 각각은 에 의해 생성되고,여기서, 는 상기 복수의 스펙트로그램 중 대응하는 스펙트로그램의 크기이고, 은 x가 1보다 작은 경우 x를 출력하고, x가 1이상인 경우 1을 출력하는 함수인, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제14항 내지 제18항 중 어느 한 항에 있어서,상기 제2 인공지능 모델(320)은 입력 레이어(321), 복수의 특징 레이어를 포함하는 인코더(322), 및 병목(bottleneck) 레이어(323)를 포함하고,상기 오디오 관련 정보를 상기 제2 인공지능 모델(320)에 적용하는 것은,상기 오디오 관련 정보를 상기 입력 레이어(321)에 적용하는 것,상기 오디오 관련 정보를 상기 인코더(322)에 포함된 상기 복수의 특징 레이어 각각에 적용하는 것, 또는상기 오디오 관련 정보를 상기 병목 레이어(323)에 적용하는 것중 적어도 하나를 포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제14항 내지 제19항 중 어느 한 항에 있어서,제3 인공지능 모델(330)을 이용하여, 상기 믹스된 오디오 신호로부터, 또는 상기 믹스된 오디오 신호 및 시각적정보로부터, 상기 믹스된 오디오 신호에 포함된 화자 수 관련 정보를 생성하는 단계,상기 화자 수 관련 정보에 기초하여, 상기 제1 인공지능 모델(310)을 이용하여, 상기 이미지 신호 및 상기 믹스된 오디오 신호로부터, 상기 오디오 관련 정보를 생성하는 단계, 및상기 화자 수 관련 정보 및 상기 오디오 관련 정보를 상기 제2 인공지능 모델(320)에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하는 단계를 더 포함하되,상기 시각적 정보는 상기 이미지 신호에 포함된 적어도 하나의 키 프레임(key frame)을 포함하고,상기 적어도 하나의 키 프레임은 상기 믹스된 오디오 신호에 포함된 적어도 하나의 음원에 대응되는 적어도 하공개특허 10-2024-0053501-6-나의 화자의 입술을 포함하는 안면 영역을 포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제14항 내지 제20항 중 어느 한 항에 있어서,상기 믹스된 오디오 신호에 포함된 화자 수 관련 정보는,상기 믹스된 오디오 신호와 관련된 제1 화자 수 관련 정보, 또는상기 시각적 정보와 관련된 제2 화자 수 관련 정보 중 적어도 하나를 포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제14항 내지 제21항 중 어느 한 항에 있어서,상기 제1 화자 수 관련 정보는 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원에 대응되는 화자 수의 확률분포도를 포함하고,상기 제2 화자 수 관련 정보는 상기 시각적 정보에 포함된 화자 수의 확률 분포도를 포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제14항 내지 제22항 중 어느 한 항에 있어서,상기 제2 인공지능 모델(320)은 입력 레이어(321), 복수의 특징 레이어를 포함하는 인코더(322), 및 병목(bottleneck) 레이어(323)를 포함하고,상기 화자 수 관련 정보를 상기 제2 인공지능 모델(320)에 적용하는 것은,상기 화자 수 관련 정보를 상기 입력 레이어(321)에 적용하는 것,상기 화자 수 관련 정보를 상기 인코더(322)에 포함된 상기 복수의 특징 레이어 각각에 적용하는 것, 또는상기 화자 수 관련 정보를 상기 병목 레이어(323)에 적용하는 것중 적어도 하나를 포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제14항 내지 제23항 중 어느 한 항에 있어서,상기 이미지 신호로부터 상기 복수의 화자와 연관된 복수의 입 움직임 정보를 획득하는 단계, 및상기 획득된 복수의 입 움직임 정보를 상기 제2 인공지능 모델에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제14항 내지 제24항 중 어느 한 항에 있어서,상기 동영상이 재생되는 화면을 표시하고, 사용자로부터 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원에대응하는 복수의 화자 중 적어도 하나의 화자를 선택하기 위한 입력을 수신하는 단계, 및상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 선택된 상기 적어도 하나의 화자에 대응되는 적어도 하나의 음원을 출력하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제14항 내지 제25항 중 어느 한 항에 있어서,상기 선택된 적어도 하나의 화자에 대응하는 적어도 하나의 음원의 음량을 조절하기 위한 사용자 인터페이스를상기 화면에 표시하는 단계, 및사용자로부터의 상기 음량의 조절에 응답하여, 상기 적어도 하나의 음원의 음량을 조절하는 단계를 더공개특허 10-2024-0053501-7-포함하는, 방법."}
{"patent_id": "10-2023-0022449", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제14항 내지 제26항 중 어느 한 항의 방법을 수행하기 위한 컴퓨터 프로그램이 저장된 컴퓨터-판독 가능한 기록매체."}
{"patent_id": "10-2023-0022449", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "일 실시예에 따른, 이미지 신호 및 믹스된 오디오 신호를 포함하는 동영상을 처리하기 위한 전자 장치는 상기 동 영상을 처리하기 위한 프로그램이 저장되는 메모리, 및 상기 프로그램을 실행함으로써 상기 동영상을 처리하는 적어도 하나의 프로세서를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 제1 인공지능 모델을 이용하여, 상 기 이미지 신호 및 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 복수의 음원의 겹침 (overlap) 정도를 나타내는 오디오 관련 정보를 생성할 수 있고, 상기 오디오 관련 정보를 제2 인공지능 모델에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나 의 음원을 분리할 수 있다."}
{"patent_id": "10-2023-0022449", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 동영상을 처리하기 위한 장치 및 이의 동작 방법에 관한 것이고, 더 상세하게는 이미지 신호 및 믹스 된 오디오 신호를 포함하는 동영상에 대하여 믹스된 오디오 신호에 포함된 복수의 음원 중 적어도 하나의 음원 을 분리하기 위한 장치 및 이의 동작 방법에 관한 것이다."}
{"patent_id": "10-2023-0022449", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 동영상을 시청하는 환경이 점점 다양해짐에 따라 사용자가 동영상에 대해 상호작용을 할 수 있는 방법이 늘어나고 있다. 예를 들어, 스마트폰이나 태블릿 등과 같이 터치 입력이 가능한 화면을 통해 동영상이 재생된다 면, 사용자는 터치 입력을 통해 동영상 중 일부 영역(예: 특정 등장인물이 나오는 영역)을 확대하는 등의 방식 으로 동영상의 포커스가 특정 등장인물에 고정되도록 할 수 있다. 이와 같이 사용자의 입력에 의해 동영상의 포커스가 특정 등장인물에 고정되는 경우, 오디오 출력에도 이를 반 영함으로써 사용자에게 직관적인 피드백을 줄 수 있을 것이며, 이를 위해서는 동영상에 포함된 음원(음성)들을 분리하는 것이 선행될 필요가 있다."}
{"patent_id": "10-2023-0022449", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른, 이미지 신호 및 믹스된 오디오 신호를 포함하는 동영상을 처리하기 위한 전자 장치는 상기 동영상을 처리하기 위한 프로그램이 저장되는 메모리, 및 상기 프로그램을 실행함으로써 상기 동영상을 처리하 는 적어도 하나의 프로세서를 포함할 수 있다. 일 실시예에서, 상기 적어도 하나의 프로세서는, 제1 인공지능 모델을 이용하여, 상기 이미지 신호 및 상기 믹 스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 복수의 음원의 겹침(overlap) 정도를 나타내는 오 디오 관련 정보를 생성할 수 있다. 일 실시예에서, 상기 적어도 하나의 프로세서는, 상기 오디오 관련 정보를 제2 인공지능 모델에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리 할 수 있다. 일 실시예에 따른, 이미지 신호 및 믹스된 오디오 신호를 포함하는 동영상을 처리하기 위한 방법은, 제1 인공지 능 모델을 이용하여, 상기 이미지 신호 및 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 복수의 음원의 겹침(overlap) 정도를 나타내는 오디오 관련 정보를 생성하는 단계를 포함할 수 있다. 일 실시예에서, 상기 방법은, 상기 오디오 관련 정보를 제2 인공지능 모델에 적용하여, 상기 믹스된 오디오 신 호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하는 단계를 포함 할 수 있다."}
{"patent_id": "10-2023-0022449", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시를 설명함에 있어서 본 개시가 속하는 기술 분야에 익히 알려져 있고 본 개시와 직접적으로 관련이 없는 기술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 개시의 요지를 흐리지 않고 더 욱 명확히 전달하기 위함이다. 그리고 후술되는 용어들은 본 개시에서의 기능을 고려하여 정의된 용어들로서 이 는 사용자, 운용자의 의도 또는 관례 등에 따라 달라질 수 있다. 그러므로 그 정의는 본 명세서 전반에 걸친 내 용을 토대로 내려져야 할 것이다. 마찬가지 이유로 첨부된 도면에 있어서 일부 구성요소는 과장되거나 생략되거나 개략적으로 도시되었다. 또한, 각 구성요소의 크기는 실제 크기를 전적으로 반영하는 것이 아니다. 각 도면에서 동일한 또는 대응하는 구성요 소에는 동일한 참조번호를 부여하였다. 본 개시의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예를 참조하면 명확해질 것이다. 그러나 본 개시는 이하에서 개시되는 실시예에 한정되는 것이 아니라 서로 다 른 다양한 형태로 구현될 수 있다. 개시된 실시예는 본 명세서의 개시가 완전하도록 하고, 본 개시가 속하는 기 술분야에서 통상의 지식을 가진 자에게 개시의 범주를 완전하게 알려주기 위해 제공되는 것이다. 본 개시의 일 실시예는 청구범위에 따라 정의될 수 있다. 명세서 전체에 걸쳐 동일한 참조부호는 동일한 구성요소를 나타낸다. 또한, 본 개시의 실시예를 설명함에 있어서 관련된 기능 또는 구성에 대한 구체적인 설명이 본 개시의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명은 생략한다. 그리고 후술되는 용어들은 본 개시에서의 기능을 고려하여 정의된 용어들로서 이는 사용자, 운용자의 의도 또는 관례 등에 따라 달라질 수 있 다. 그러므로 그 정의는 본 개시 전반에 걸친 내용을 토대로 내려져야 할 것이다. 일 실시예에서, 흐름도 도면들의 각 블록과 흐름도 도면들의 조합들은 컴퓨터 프로그램 인스트럭션들 (instructions)에 의해 수행될 수 있다. 컴퓨터 프로그램 인스트럭션들은 범용 컴퓨터, 특수용 컴퓨터 또는 기 타 프로그램 가능한 데이터 프로세싱 장비의 프로세서에 탑재될 수 있고, 컴퓨터 또는 기타 프로그램 가능한 데 이터 프로세싱 장비의 프로세서를 통해 수행되는 그 인스트럭션들이 흐름도 블록(들)에서 설명된 기능들을 수행 하는 수단을 생성할 수 있다. 컴퓨터 프로그램 인스트럭션들은 특정 방식으로 기능을 구현하기 위해 컴퓨터 또 는 기타 프로그램 가능한 데이터 프로세싱 장비를 지향할 수 있는 컴퓨터 이용 가능 또는 컴퓨터 판독 가능 메 모리에 저장되는 것도 가능하고, 컴퓨터 이용 가능 또는 컴퓨터 판독 가능 메모리에 저장된 인스트럭션들은 흐 름도 블록(들)에서 설명된 기능을 수행하는 인스트럭션 수단을 내포하는 제조 품목을 생산하는 것도 가능하다. 컴퓨터 프로그램 인스트럭션들은 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에 탑재되는 것도 가능하다. 또한, 흐름도 도면의 각 블록은 특정된 논리적 기능(들)을 실행하기 위한 하나 이상의 실행 가능한 인스트럭션 들을 포함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있다. 일 실시예에서, 블록들에 언급된 기능들이 순서를 벗어나서 발생하는 것도 가능하다. 예를 들면, 잇달아 도시되어 있는 두 개의 블록들은 실질적으로 동시 에 수행되는 것도 가능하고 또는 기능에 따라 역순으로 수행되는 것도 가능하다. 본 개시의 일 실시예에서 사용되는 '~부'라는 용어는 소프트웨어 구성요소, 또는 FPGA(Field Programmable Gate Array) 또는 ASIC(Application Specific Integrated Circuit)과 같은 하드웨어 구성요소를 나타낼 수 있고, '~ 부'는 특정한 역할을 수행할 수 있다. 한편, '~부'는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고, 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 실시예에서 '~부'는 소프트웨어 구성 요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로 세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로 코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들, 및 변수들을 포함할 수 있다. 특정한 구성요소나 특정한 '~부'를 통해 제공되는 기능은 그 개수를 줄이도록 결합되거나 추가적인 구성요소들로 분리 될 수 있다. 또한, 실시예에서 '~부'는 하나 이상의 프로세서를 포함할 수 있다. 이하, 본 개시의 실시예를 첨부된 도면을 참조하여 상세히 설명한다. 도 1은 본 개시의 일 실시예에 따른, 동영상에 포함된 화자별로 음원을 분리하고, 분리 결과에 따라서 동영상의 재생을 제어하는 상황을 설명하기 위한 도면이다. 도 1의 제1 화면(100a)은 재생 중인 동영상의 어느 한 장면에 해당되며, 제2 화면(100b)은 제1 화면(100a)의 일 부 영역이 확대된 영상이다. 제1 화면(100a)에는 두 명의 인물(1, 2)이 등장하는데, 이하에서는 동영상에 등장하는 인물들을 동영상에 포함된 '화자(speaker)'라고 표현한다. 동영상에서 제1 화자와 제2 화자가 모두 말을 하고 있다면, 두 화자(1, 2)의 음성(voice)이 섞여서 출력 될 것이다. 이하에서는, 음성(voice)과 음원(sound source)을 동일한 의미로 사용한다. 도 1에 도시된 바와 같이, 제1 화면(100a)에서 일부 영역이 확대되어 제2 화면(100b)이 표시될 때 제1 화자 의 음성이 강조되어 출력되거나, 제1 화자의 음성만 출력되도록 하려면, 제1 화자의 음성, 제2 화자 의 음성, 및 (경우에 따라서) 제1 화면(100a)에 등장하지 않는 제3 화자의 음성 등이 믹스된 오디오 신호로 부터 개별 음원을 분리할 수 있어야 한다. 따라서, 이하에서는 동영상에 포함된 믹스된 오디오 신호로부터 개별 음원을 분리하는 방법에 대해서 자세히 설명하고, 이어서 분리 결과에 따라서 동영상 재생을 제어하는 실시예에 대해서 설명한다. 도 2는 본 개시의 일 실시예에 따른, 동영상을 처리하기 위한 전자 장치의 구성을 개략적으로 도시한 블록도이 다. 도 2에 도시된 전자 장치는 동영상을 재생하는 디스플레이 장치(예: 스마트폰, 태블릿 등)일 수도 있고, 또는 디스플레이 장치와 유무선 통신을 통해 연결된 별도의 서버일 수도 있다. 본 개시에서 소개되는 실시예들 에 따른, 동영상을 처리하기 위한 방법은, 동영상을 표시하는 디스플레이를 포함하는 전자 장치에 의해 수행될 수도 있고, 디스플레이를 포함하는 전자 장치와 연결된 별도의 서버에 의해서 수행될 수도 있으며, 또는 디스플 레이를 포함하는 전자 장치 및 서버에 의해 공동으로 수행(방법에 포함되는 프로세스들을 두 장치가 나누어서수행)될 수도 있다. 이하에서는 설명의 편의를 위해 도 2의 전자 장치가 디스플레이를 포함하는 전자 장치이고, 실시예들에 따 른 동영상을 처리하는 방법을 수행하는 것으로 가정하고 설명한다. 하지만, 앞서 설명한 바와 같이 이에 한정되 지 않고, 디스플레이를 포함하는 전자 장치와 연결된 별도의 서버가 존재하고, 서버가 프로세스의 일부 또는 전 부를 대신 수행할 수도 있음은 자명하다. 따라서, 이하에서 설명되는 실시예들에서 전자 장치가 수행하는 동작들 중에서, 디스플레이에 동영상을 표시하는 동작을 제외한 나머지 동작들은, 별다른 설명이 없더라도 서버 등과 같은 별도의 장치에 의해 수행될 수도 있다고 해석되어야 한다. 도 2를 참조하면, 본 개시의 일 실시예에 따른 전자 장치는 통신 인터페이스, 입출력 인터페이스 , 오디오 출력부, 프로세서 및 메모리를 포함할 수 있다. 다만, 전자 장치의 구성요 소는 전술한 예에 한정되는 것은 아니고, 전자 장치는 전술한 구성요소들보다 더 많은 구성요소를 포함하 거나, 또는 더 적은 구성요소를 포함할 수도 있다. 일 실시예에서, 통신 인터페이스, 입출력 인터페이스 , 오디오 출력부, 프로세서 및 메모리 중 일부 또는 전부는 하나의 칩(chip) 형태로 구현 될 수도 있으며, 프로세서는 하나 이상의 프로세서를 포함할 수도 있다. 통신 인터페이스는 외부의 장치와 유선 또는 무선으로 신호(제어 명령 및 데이터 등)를 송수신하기 위한 구성으로서, 다양한 통신 프로토콜을 지원하는 통신 칩셋을 포함하도록 구성될 수 있다. 통신 인터페이스 는 외부로부터 신호를 수신하여 프로세서로 출력하거나, 프로세서로부터 출력된 신호를 외부로 전송 할 수 있다. 입출력 인터페이스는 사용자로부터 제어 명령이나 정보 등을 입력받기 위한 입력 인터페이스(예: 터치 스 크린, 하드 버튼, 마이크 등)와, 사용자의 제어에 따른 동작의 실행 결과나 전자 장치의 상태를 표시하기 위한 출력 인터페이스(예: 디스플레이 패널)를 포함할 수 있다. 일 실시예에 따르면, 입출력 인터페이스는 재생되는 동영상을 표시하고, 사용자로부터 동영상 중 일부 영역을 확대하거나, 동영상에 포함된 특정 화자 또 는 특정 음원을 선택하기 위한 입력을 수신할 수 있다. 오디오 출력부는 동영상에 포함된 오디오 신호를 출력하기 위한 구성으로서, 전자 장치에 내장되어 오디오 신호에 대응하는 소리를 직접 재생할 수 있는 출력 장치(예: 내장 스피커)일 수도 있고, 전자 장치(20 0)가 유선 오디오 재생 장치(예: 스피커, 사운드바, 이어폰, 헤드폰 등)와 오디오 신호를 송수신할 수 있도록 하는 인터페이스(예: 3.5mm 단자, 4.4mm 단자, RCA 단자, USB 등)일 수도 있고, 전자 장치가 무선 오디오 재생 장치(예: 무선 이어폰, 무선 헤드폰, 무선 스피커 등)와 오디오 신호를 송수신할 수 있도록 하는 인터페이 스(예: 블루투스 모듈, 무선 랜(WLAN) 모듈 등)일 수도 있다. 프로세서는 이하에서 설명되는 실시예들에 따라 전자 장치가 동작하도록 일련의 과정을 제어하는 구 성으로서, 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU(Central Processing Unit), AP(Application Processor), DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU(Graphic Processing Unit), VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU(Neural Processing Unit)와 같은 인공지능 전용 프로세서일 수 있다. 예를 들어, 하나 또는 복수의 프로세서가 인공지 능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설 계될 수 있다. 프로세서는 메모리에 데이터를 기록하거나, 메모리에 저장된 데이터를 읽을 수 있으며, 특히 메 모리에 저장된 프로그램을 실행함으로써 미리 정의된 동작 규칙 또는 인공지능 모델에 따라 데이터를 처리 할 수 있다. 따라서, 프로세서는 이하의 실시예에서 설명되는 동작들을 수행할 수 있으며, 이하의 실시예 에서 전자 장치가 수행한다고 설명되는 동작들은 특별한 설명이 없는 한 프로세서가 수행하는 것으로 볼 수 있다. 메모리는 다양한 프로그램이나 데이터를 저장하기 위한 구성으로서, 롬(ROM), 램(RAM), 하드디스크, CD- ROM 및 DVD 등과 같은 저장 매체 또는 저장 매체들의 조합으로 구성될 수 있다. 메모리는 별도로 존재하지 않고 프로세서에 포함되도록 구성될 수도 있다. 메모리는 휘발성 메모리, 비휘발성 메모리 또는 휘발 성 메모리와 비휘발성 메모리의 조합으로 구성될 수도 있다. 메모리에는 이하에서 설명되는 실시예에 따른 동작들을 수행하기 위한 프로그램이 저장될 수 있다. 메모리는 프로세서의 요청에 따라 저장된 데이 터를 프로세서에 제공할 수도 있다. 이하에서는 전자 장치가 동영상에 포함된 믹스된 오디오 신호로부터 개별 음원을 분리하는 방법에 대해서 자세히 설명하고, 이어서 분리 결과에 따라서 동영상 재생을 제어하는 실시예들에 대해서 설명한다. 도 3은 본 개시의 일 실시예에 따른, 전자 장치가 동영상을 처리하기 위한 방법을 수행하도록 하는 프로그램을 복수의 인공지능 모델들로 표현한 도면이다. 도 3에 도시된 모델들(310, 320, 330)은 프로세서가 메모리 에 저장된 프로그램을 실행함으로써 수행하는 동작들을 기능에 따라 분류한 것일 수 있다. 따라서, 이하에서 도 3에 도시된 모델들(310, 320, 330)이 수행한다고 설명되는 동작들은 실제로는 프로세서가 수 행하는 것으로 볼 수 있다. 본 개시에 따른 인공지능 모델과 관련된 기능은 프로세서와 메모리를 통해 동작될 수 있다. 프로세서 는 메모리에 저장된 기 정의된 동작 규칙 또는 인공지능 모델에 따라 입력 데이터를 처리하도록 제어 할 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미 한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도 형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경 망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의 해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공신경망은 심층신경망(DNN: Deep Neural Network)을 포함할 수 있으며, 예를 들어, CNN(Convolutional Neural Network), DNN(Deep Neural Network), RNN(Recurrent Neural Network), RBM(Restricted Boltzmann Machine), DBN(Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크(Deep Q-Network) 등이 있으나, 전 술한 예에 한정되지 않는다. 도 3을 참조하면, 일 실시예에 따른 동영상을 처리하기 위한 프로그램은, 음원 특성 분석 모델, 음원 분리 모델 및 화자 수 분석 모델을 포함할 수 있다. 이하에서, 음원 특성 분석 모델, 음원 분리 모델 및 화자 수 분석 모델은 각각 제1 인공지능 모델, 제2 인공지능 모델 및 제3 인공지능 모델로 지칭될 수 있다. 일 실시예에서, 음원 특성 분석 모델은, 동영상에 포함된 이미지 신호 및 믹스된 오디오 신호로부터, 믹스 된 오디오 신호에 포함된 복수의 음원의 겹침(overlap) 정도를 나타내는 오디오 관련 정보를 생성한다. 이때, 이미지 신호는 동영상에 등장하는 화자의 입술을 포함하는 안면 영역이 포함된 영상(즉, 하나 이상의 프레임 (frame))을 의미한다. 입술을 포함하는 안면 영역은 입술뿐만 아니라 입술로부터 일정 거리 이내의 안면부를 포 함할 수 있다. 예를 들어, 이미지 신호는 동영상에 등장하는 제1 화자의 입술을 포함하는 안면 영역이 포함된 영상으로서 88x88 크기의 프레임 64개로 구성될 수 있다. 후술하는 바와 같이, 음원 특성 분석 모델은 이 미지 신호를 이용하여 화자의 시간적 발화 정보(temporal pronouncing information)를 나타내는 화자의 입 움직 임 정보를 생성할 수 있다. 일 실시예에서, 이미지 신호는 동영상에 등장하는 복수의 화자의 입술을 포함하는 안면 영역이 포함된 복수의 영상일 수 있다. 예를 들어, 동영상에 2명의 화자가 등장하는 경우, 이미지 신호는 제1 화자의 입술을 포함하는 안면 영역이 포함된 제1 영상 및 제2 화자의 입술을 포함하는 안면 영역이 포함된 제2 영상으로 구성될 수 있다. 일 실시예에서, 이미지 신호는 화자 수 분석 모델이 생성한 화자 수 분포 정보에 기초하여 결정되는 수만 큼의 화자의 입술을 포함하는 안면 영역이 포함된 영상일 수 있다. 다시 말하면, 이미지 신호에 포함된 영상의 개수는 화자 수 분석 모델이 생성한 화자 수 분포 정보에 기초하여 결정될 수 있다. 예를 들어, 화자 수 분포 정보에 따라 화자가 3명일 확률이 가장 높은 경우 이미지 신호는 3명의 화자에 대응되는 3개의 영상으로 구성될 수 있다. 일 실시예에서, 오디오 관련 정보는 시간-주파수 도메인에서 복수의 화자에 대응되는 복수의 음원이 겹치는 정 도를 확률로 나타낸 맵(map)을 포함한다. 즉, 오디오 관련 정보의 각 빈(bin)은 해당하는 시간-주파수 영역에서 동시에 발화하는 화자들의 음성(음원)이 겹치는 정도에 대응하는 확률 값을 가진다. 여기서, 확률이라는 용어는 복수의 음원이 겹치는 정도를 0과 1사이의 값으로 표현한다는 것을 의미한다. 예를 들어, 특정 빈(bin)에서의 값은 해당 시간-주파수 영역에서의 각각의 음원의 음량에 따라 결정될 수 있다. 음원 특성 분석 모델의 세부 구조 및 동작에 관하여는 도 4a, 4b 및 4c를 참조하여 후술한다. 일 실시예에서, 음원 분리 모델은, 오디오 관련 정보를 이용하여, 믹스된 오디오 신호로부터, 믹스된 오디 오 신호에 포함된 복수의 음원 중 적어도 하나의 음원을 분리한다. 일 실시예에서, 음원 분리 모델은, 오 디오 관련 정보에 더해, 타겟 화자에 대응되는 이미지 신호를 더 이용하여, 믹스된 오디오 신호로부터, 믹스된 오디오 신호에 포함된 복수의 음원 중 타겟 화자에 대응되는 음원을 분리할 수 있다. 음원 분리 모델은 입력 레이어(도 7의 321), 복수의 특징 레이어를 포함하는 인코더(도 7의 322), 및 병목 (bottleneck) 레이어(도 7의 323)를 포함할 수 있다. 후술하는 바와 같이, 오디오 관련 정보는 입력 레이어(도 7의 321), 인코더(도 7의 322)에 포함된 복수의 특징 레이어 각각, 또는 병목 레이어(도 7의 323) 중 적어도 하 나에 적용될 수 있다. 다시 말하면, 프로세서는 오디오 관련 정보를 음원 분리 모델의 입력 레이어 (도 7의 321), 인코더(도 7의 322)에 포함된 복수의 특징 레이어 각각, 또는 병목 레이어(도 7의 323) 중 적어 도 하나에 적용할 수 있다. 오디오 관련 정보가 음원 분리 모델에 적용되는 방법에 대해서는 도 5를 참조 하여 후술한다. 복수의 음원이 믹스된 오디오 신호로부터 개별 음원을 분리할 수 있는 임의의 신경망 모델은 음원 분리 모델 로서 채용될 수 있다. 예를 들어, 화자의 입술 움직임 정보, 화자의 안면 정보, 및 믹스된 오디오 신호를 입력으로 받아 타겟 화자의 음성(음원)을 분리하는 방식으로 동작하는 \"VISUAL VOICE\"와 같은 모델이 채용될 수 있다. \"VISUAL VOICE\"의 구체적인 동작은 다음의 문헌에 개시되어 있다. GAO, Ruohan; GRAUMAN, Kristen. Visualvoice: Audio-visual speech separation with cross-modal consistency. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2021. p. 15490-15500. \"VISUAL VOICE\"와 같은, 종래의 음원 분리 모델은 믹스된 오디오 신호를 별도로 분석하여 생성한 추가 정보 없 이 모델 내의 인코더가 직접 복수의 음성이 겹치는 영역을 분석하여 음원을 분리하는 방식으로 동작한다. 따라 서 종래의 음원 분리 기술은 믹스된 오디오 신호에 포함되는 복수의 음성이 시간-주파수 영역에서 겹치는 경우 복수의 음성의 특징에 따라 분리 성능이 급격히 감소하는 문제점이 있다. 예를 들어, 2명의 남성 화자의 음성 (음원)이 믹스된 경우 두 음성은 시간-주파수 영역의 상당한 부분에서 겹치게 되므로 1명의 남성 화자의 음성 (음원)과 1명의 여성 화자의 음성(음원)이 믹스된 경우보다 정확하게 분리하기 어렵다. 또한 실사용 환경에서는 믹스된 오디오 신호에 포함된 음원들의 특징과 대응되는 화자 수를 특정할 수 없고, 시간에 따라 주변 환경이 변화하는 상황에서 발생하는 다양한 노이즈(noise)에 의해서도 분리 성능이 떨어진다. 본 개시의 일 실시예에 따른 전자 장치는 동영상에 포함된 이미지 신호 및 믹스된 오디오 신호로부터, 믹 스된 오디오 신호에 포함된 복수의 음원의 겹침 정도를 나타내는 오디오 관련 정보를 생성하고, 이를 음원 분리 모델에 적용함으로써 다양한 노이즈가 섞인 실사용 환경에서 음원을 분리하거나 특징이 유사한 음원(음 성)들이 믹스된 오디오 신호에 대해서도 우수한 분리 성능을 달성할 수 있다. 이는 오디오 관련 정보가 음원 분 리 모델에서 어텐션 맵(attention map)으로서의 역할을 수행하기 때문이다. 여기서, 어텐션 맵이란 목표로 하는 과제를 더 잘 달성하기 위한, 입력 데이터(즉, 벡터) 내 개별 데이터(즉, 벡터의 각 성분)의 상대적 중요 도를 나타낸 정보를 의미한다. 일 실시예에서, 화자 수 분석 모델은 믹스된 오디오 신호로부터, 또는 믹스된 오디오 신호 및 시각적 정보 로부터, 믹스된 오디오 신호에 포함된 화자 수 관련 정보를 생성할 수 있다. 여기서, 화자 수 관련 정보는 화자 수의 확률 분포도(Distribution on Number of Speakers)를 포함한다. 예를 들어, 화자 수 관련 정보는 믹스된 음원에 포함된 화자 수가 0명일 확률, 1명일 확률, 쪋, 및 N명일 확률을 각각 포함하는 N차원의 특징 벡터일 수 있다. 일 실시예에서, 화자 수 관련 정보는 믹스된 오디오 신호와 관련된 제1 화자 수 관련 정보, 또는 시각적 정보와 관련된 제2 화자 수 관련 정보 중 적어도 하나를 포함할 수 있다. 예를 들어, 제1 화자 수 관련 정보는 믹스된 오디오 신호에 포함된 복수의 음원에 대응되는 화자 수의 확률 분포도를 포함할 수 있고, 제2 화자 수 관련 정보는 시각적 정보에 포함된 화자 수의 확률 분포도를 포함할 수 있다. 일 실시예에서, 화자 수 분석 모 델은 선택적인 구성으로서 생략될 수도 있다.상술한 \"VISUAL VOICE\"와 같은, 종래의 음원 분리 모델은 믹스된 오디오 신호로부터 단일의 타겟 화자의 음성 (음원)을 분리하도록 학습되거나 특정 수의 타겟 화자의 음성(음원)을 분리하도록 학습된다. 따라서, 종래의 음 원 분리 모델을 이용하는 경우, 믹스된 오디오 신호에 다수의 화자에 대응하는 음성(음원)이 포함된 경우 개별 음성(음원)을 모두 분리하기 위해서는 각각의 화자마다 별도의 모델이 필요하다. 또한 믹스된 오디오 신호에 포 함된 복수의 음원에 대응하는 화자 수가 학습된 타겟 화자 수와 다른 경우 분리 성능이 급격히 감소하게 된다. 반면에, 본 개시의 일 실시예에 따른 전자 장치는 화자 수 관련 정보를 음원 분리 모델에 적용함으로 써 단일 모델만으로도 믹스된 오디오 신호에 포함된 복수의 개별 음원을 모두 분리할 수 있으며, 화자 수에 관 계없이 우수한 분리 성능을 갖는다. 화자 수 분석 모델의 세부적인 동작에 관하여는 도 6a, 6b 및 도 7을 참조하여 후술한다. 도 4a, 4b 및 4c는 일 실시예에 따른 음원 특성 분석 모델의 세부 구조 및 동작을 설명하기 위한 도면이다. 구 체적으로, 도 4a는 본 개시의 일 실시예에 따른 음원 특성 분석 모델의 구조 및 입출력 관계를 도시하고, 도 4b는 일 실시예에 따른 음원 특성 분석 모델에 포함된 제1 서브모델의 구조 및 입출력 관계를 도 시하고, 그리고 도 4c는 본 개시의 일 실시예에 따른 음원 특성 분석 모델에 포함된 제2 서브모델의 구조 및 입출력 관계를 도시한다. 일 실시예에서, 음원 특성 분석 모델은 이미지 신호 및 믹스된 오디오 신호를 입력으로 받아 오디오 관련 정보를 출력한다. 음원 특성 분석 모델의 동작은 제1 서브모델 및 제2 서브모델에 의해 수행될 수 있다. 일 실시예에서, 제1 서브모델은 이미지 신호로부터, 믹스된 오디오 신호에 포함된 복수의 음원에 대응하는 복수의 화자의 시간적 발화 정보를 생성한다. 예를 들어, 제1 서브 모델은 88x88 크기의 64 프레임으로 구 성된 이미지 신호로부터 이미지 신호에 포함된 화자의 입 움직임에 대한 특징을 추출할 수 있다. 이때 입 움직 임에 대한 특징은 해당 화자의 시간적 발화 정보를 나타낸다. 일 실시예에서, 제1 서브모델은 3차원 컨볼 루션 레이어 및 풀링 레이어로 구성된 제1 레이어, 2차원 컨볼루션 레이어로 구성된 제2 레이어(Shuffle), 특징 벡터의 결합 및 크기 변환을 위한 제3 레이어, 1차원 컨볼루션 레이어로 구성된 제4 레이어, 및 완전 연결 레이 어로 구성된 제5 레이어를 포함할 수 있다. 일 실시예에서, 제2 서브모델은 복수의 화자의 시간적 발화 정보에 기초하여, 믹스된 오디오 신호로부터, 오디오 관련 정보를 생성한다. 예를 들어, 믹스된 오디오 신호가 STFT(Short-Time Fourier Transform)에 의해 시간-주파수 영역의 벡터로 변환되어 제2 서브모델에 입력될 수 있다. 도 4a의 예시에서, 믹스된 오디오 신호로부터 변환된 시간-주파수 영역의 벡터는 실수부와 허수부를 갖는 2x256x256 차원의 데이터일 수 있다. 일 실시예에서, 제2 서브모델은 2차원 컨볼루션 레이어 및 렐루(Relu) 활성화 함수로 구성된 제1 레이어, 2차 원 컨볼루션 레이어로 구성되어 특징 벡터를 다운스케일링 하는 제2 레이어(인코더), 다운스케일링 결과와 제1 서브모델에서 생성된 시간적 발화 정보(즉, 화자의 입 움직임에 대한 특징 벡터)를 결합하는 제3 레이어, 2차원 컨볼루션 레이어로 구성되어 특징 벡터를 업스케일링 하는 제4 레이어(디코더), 및 2차원 컨볼루션 레이 어 및 렐루 활성화 함수로 구성된 제5 레이어를 포함할 수 있다. 도 4c에 도시된 바와 같이, 제1 서브모델(31 1)에서 생성된 시간적 발화 정보는 제2 서브모델의 인코더와 디코더의 사이에 적용된다. 도 4a, 4b 및 4c 의 예시에서, 제2 서브모델에서 생성된 오디오 관련 정보는 1x256x256 차원의 데이터일 수 있다. 도 5는 본 개시의 일 실시예에 따른 음원 분리 모델의 예시적인 구조 및 오디오 관련 정보가 음원 분리 모델에 적용되는 방법을 설명하기 위한 도면이다. 도 5를 참조하면, 일 실시예에서, 음원 분리 모델은 입력 레이어, 복수의 특징 레이어를 포함하는 인 코더, 및 병목 레이어을 포함하고, 추가적으로 업스케일링을 위한 디코더, 및 2차원 컨볼루션 레이어 및 렐루 활성화 함수로 구성된 출력 레이어를 더 포함할 수 있다. 일 실시예에서, 입력 레이어는 2차원 컨 볼루션 레이어 및 렐루 활성화 함수로 구성될 수 있고, 인코더는 다운스케일링을 위한 복수의 컨볼루션 레 이어로 구성될 수 있으며, 병목 레이어는 2차원 컨볼루션 레이어 및 렐루 활성화 함수로 구성될 수 있다. 일 실시예에서, 오디오 관련 정보는 입력 레이어, 인코더에 포함된 복수의 특징 레이어 각각, 또는 병목 레이어 중 적어도 하나에 적용될 수 있다. 즉, 오디오 관련 정보는 도 5에 도시된 화살표들(510, 520, 530)과 같이 음원 분리 모델에 적용될 수 있다. 화살표는 오디오 관련 정보가 믹스된 오디오 신 호와 결합(concatenation)되어 입력 레이어에 적용되는 것을 나타내고, 화살표는 오디오 관련 정보가 인코더에 포함된 복수의 특징 레이어 각각에 적용되는 것을 나타내고, 화살표는 오디오 관련 정보가인코더의 출력과 결합되어 병목 레이어에 적용된 후 디코더로 전달되는 것을 나타낸다. 오디오 관련 정보가 음원 분리 모델에 적용되기 위해서는 해당 레이어에 입력되는 데이터(즉, 이전 레이어 의 출력)와 결합되어야 한다. 화살표의 경우를 예로 들어 설명하면, 입력 레이어에는 믹스된 오디오 신호가 시간-주파수 도메인으로 변환되어 입력된다. 따라서 오디오 관련 정보를 입력 레이어에 적용하기 위해서는 믹스된 오디오 신호의 시간-주파수 도메인 변환 결과와 오디오 관련 정보가 결합되어야 한다. 상술한 예시에서, 믹스된 오디오 신호의 시간-주파수 도메인으로의 변환 결과인 2x256x256 차원의 벡터는 1x256x256 차 원을 갖는 오디오 관련 정보와 결합되고, 최종적으로 3x256x256 차원의 벡터가 입력 레이어에 입력된다. 도 6a 및 6b는 본 개시의 일 실시예에 따른, 화자 수 분석 모델의 입 출력 관계를 도시하고, 도 7은 본 개 시의 일 실시예에 따른, 화자 수 관련 정보가 음원 분리 모델에 적용되는 방법을 설명하기 위한 도면이다. 도 6a를 참조하면, 일 실시예에서, 화자 수 분석 모델은 믹스된 오디오 신호로부터, 믹스된 오디오 신호와 관련된 제1 화자 수 관련 정보를 생성할 수 있다. 상술한 바와 같이, 화자 수 관련 정보는 화자 수의 확률 분포 도를 포함한다. 예를 들어, 화자 수 관련 정보는 믹스된 음원에 포함된 화자 수가 0명일 확률, 1명일 확률, 쪋, 및 N명일 확률을 각각 포함하는 N차원의 특징 벡터일 수 있다. 믹스된 오디오 신호로부터 생성한 제1 화자 수 관련 정보를 음원 분리 모델에 적용하면, 예를 들어, 처음에는 제1 화자만 발언하고, 특정 시점부터 제1 화자와 제2 화자가 동시에 발언하고, 그 후 다시 제1 화자만 발언을 하는 경우와 같이, 믹스된 오디오 신호의 단위 구간 내에서 화자 수가 변하거나 화자 수를 특정할 수 없는 상황에서도 강인한 분리 성능을 달성할 수 있 다. 도 6b를 참조하면, 일 실시예에서, 화자 수 분석 모델은 믹스된 오디오 신호 및 시각적 정보로부터, 믹스 된 오디오 신호와 관련된 제1 화자 수 관련 정보 및 시각적 정보와 관련된 제2 화자 수 관련 정보를 생성할 수 있다. 여기서, 시각적 정보는 이미지 신호에 포함된 복수의 프레임 중 적어도 하나의 키 프레임(key frame)이며, 키 프레임은 믹스된 오디오 신호에 포함된 적어도 하나의 음원에 대응되는 적어도 하나의 화자의 입술을 포함하는 안면 영역을 포함할 수 있다. 제1 화자 수 관련 정보는 믹스된 오디오 신호에 기초하여 생성되 고, 제2 화자 수 관련 정보는 시각적 정보에 기초하여 생성되므로, 동영상에 등장하는 화자의 수의 분포와 동영 상에 등장하지 않는 화자의 수를 함께 고려한 화자 수 분포 정보를 얻을 수 있다. 따라서, 제1 화자 수 관련 정 보와 제2 화자 수 관련 정보를 음원 분리 모델에 적용하면, 동영상에 등장하지 않는, 배경 영역에서 발생 하여 노이즈로 작용하는 음원을 분리할 수 있다. 예를 들어, 스마트폰으로 촬영한 동영상에 촬영자의 음성이 포 함된 경우에도 동영상에 등장하는 화자의 음성뿐만 아니라 촬영자의 음성도 분리할 수 있다. 도 7을 참조하면, 화자 수 관련 정보는 음원 분리 모델의 입력 레이어, 인코더에 포함된 복수의 특징 레이어 각각, 또는 병목 레이어 중 적어도 하나에 적용될 수 있다. 즉, 화자 수 관련 정보는 도 7에 도시된 화살표들(710, 720, 730)과 같이 음원 분리 모델에 적용될 수 있다. 화살표는 화자 수 관련 정보가 믹스된 오디오 신호와 결합되어 입력 레이어에 적용되는 것을 나타내고, 화살표는 화자 수 관 련 정보가 인코더에 포함된 복수의 특징 레이어 각각에 적용되는 것을 나타내고, 화살표는 화자 수 관련 정보가 인코더의 출력과 결합되어 병목 레이어에 적용된 후 디코더로 전달되는 것을 나타낸다. 도 8은 본 개시의 일 실시예에 따른, 음원 특성 분석 모델의 학습 방법을 설명하기 위한 도면이다. 도 8을 참조하면, 일 실시예에서, 음원 특성 분석 모델은 학습용 이미지 신호 및 학습용 오디오 신호로부 터 추정된 오디오 관련 정보와 그라운드 트루스(ground truth)를 비교함으로써 학습될 수 있다. 이때 그라운드 트루스는 학습용 오디오 신호에 포함된 복수의 학습용 개별 음원 각각에 기초하여 생성된 복수의 스펙트로그램 (spectrogram)으로부터 복수의 확률 맵을 생성하고, 상기 복수의 확률 맵 상호 간의 곱(product) 연산에 의해 생성될 수 있다. 구체적으로, STFT(Short-Time Fourier Transform)에 의해 학습용 개별 음원을 시간-주파수 영역의 스펙트로그램 으로 변환하고, 각각의 스펙트로그램에 대하여 수학식 1의 연산 적용하여 대응하는 확률 맵을 생성할 수 있다. 생성된 확률 맵의 대응하는 각각의 성분 간의 곱연산에 의해 그라운드 트루스가 생성될 수 있다. 수학식 1 여기서, 는 스펙트로그램(F)에 대한 노름(norm) 연산으로서, 스펙트로그램의 크기를 나타내고, 은 x가 1보다 작은 경우 x를 출력하고, x가 1이상인 경우 1을 출력하는 함수이다. 도 9는 본 개시의 일 실시예에 따른, 동영상을 처리하기 위한 방법의 순서도이다. 동영상을 처리하기 위한 방법 은 전자 장치에 의해 수행될 수 있다. 도 9를 참조하면, 동작 910에서, 제3 인공지능 모델을 이용하여, 믹스된 오디오 신호로부터, 또는 믹스된 오디오 신호 및 시각적 정보로부터, 믹스된 오디오 신호에 포함된 화자 수 관련 정보를 생성한다. 여기서, 화자 수 관련 정보는 화자 수의 확률 분포도(Distribution on Number of Speakers)를 포함한다. 예를 들어, 화자 수 관련 정보는 믹스된 음원에 포함된 화자 수가 0명일 확률, 1명일 확률, 쪋, 및 N명일 확률을 각각 포함하는 N차 원의 특징 벡터일 수 있다. 일 실시예에서, 화자 수 관련 정보는 믹스된 오디오 신호와 관련된 제1 화자 수 관 련 정보, 또는 시각적 정보와 관련된 제2 화자 수 관련 정보 중 적어도 하나를 포함할 수 있다. 전술한 바와 같 이, 구현 방식에 따라 동작 910은 생략될 수 있다. 동작 920에서, 제1 인공지능 모델을 이용하여, 이미지 신호 및 믹스된 오디오 신호로부터, 믹스된 오디오 신호 에 포함된 복수의 음원의 겹침(overlap) 정도를 나타내는 오디오 관련 정보를 생성한다. 여기서, 오디오 관련 정보는 시간-주파수 도메인에서 복수의 화자에 대응되는 복수의 음원이 겹치는 정도를 확률로 나타낸 맵(map)을 포함한다. 즉, 오디오 관련 정보의 각 빈(bin)은 해당하는 시간-주파수 영역에서 동시에 발화하는 화자들의 음 성(음원)이 겹치는 정도에 대응하는 확률 값을 가진다. 여기서, 확률이라는 용어는 복수의 음원이 겹치는 정도 를 0과 1사이의 값으로 표현한다는 것을 의미한다. 동작 930에서, 오디오 관련 정보를 제2 인공지능 모델에 적용하여, 믹스된 오디오 신호로부터, 믹스된 오디오 신호에 포함된 복수의 음원 중 적어도 하나의 음원을 분리한다. 동작 910이 생략되지 않은 경우, 동작 930에서, 오디오 관련 정보 및 화자 수 관련 정보를 제2 인공지능 모델에 적용하여, 믹스된 오디오 신호로부터, 믹스된 오디오 신호에 포함된 복수의 음원 중 적어도 하나의 음원을 분리한다. 도 10a 및 10b는 본 개시의 일 실시예에 따른, 동영상을 처리하기 위한 전자 장치 및 방법의 성능에 관한 실험 결과를 도시한다. 도 10a는 믹스된 오디오 신호, 그라운드 트루스, 및 오디오 관련 정보의 정성적 비교 결과를 나타낸 것이고, 도 10b는 정량적 비교 결과를 나타낸 것이다. 도 10a에서 그라운드 트루스는 믹스된 오디오 신호에 포함된 개별 음 원을 이용하여 도 8에 도시된 과정에 의해 생성된 것이다. 도 10a 및 10b를 참조하면, 복수의 음원이 겹치는 영역이 많은 경우(Case 1), 겹치는 영역이 적은 경우(Case 3), 그리고 겹치는 영역이 Case 1과 Case 3의 중간 정도인 경우(Case 2) 모두에서, 오디오 관련 정보는 그라운 드 트루스와 거의 차이가 없으며, 믹스된 오디오 신호 내에서 시간-주파수 도메인의 어느 영역에서 복수의 음원 이 겹쳐 있는지를 상당히 정확하게 반영하는 것을 확인할 수 있다. 음원 분리 모델의 오디오 관련 정보 추 정의 정확도는 97.27%로 측정되었으며, 이때 F-스코어(F-measure)는 0.77이다. 도 11a 및 11b는 본 개시의 일 실시예에 따른, 동영상을 처리하기 위한 전자 장치 및 방법의 성능을 종래기술과 비교한 실험 결과를 도시한다. 도 11a는 믹스된 오디오 신호로부터 제1 화자의 음원을 분리한 결과의 정성적 비교 결과를 나타낸 것이고, 도 11b는 정량적 비교 결과를 나타낸 것이다. 도 11a 및 11b에서 비교 대상인 종래기술로서 \"VISUAL VOICE\"가 사용 되었다. 도 11a 및 11b를 참조하면, 본 개시에 따른 음원 분류에서는 좌측에 도시된 오디오 관련 정보를 음원 분리 모델 에 적용하였고, \"VISUAL VOICE\"에서는 오디오 관련 정보를 음원 분리 모델에 적용하지 않았다. 제1 시간-주파수 영역에서 믹스된 오디오 신호로부터 제1 화자의 음원을 분리할 때, \"VISUAL VOICE\"는 영역 및 영역 에서 제2 화자의 음원이 함께 섞여 있으나 본 개시에 따른 분리 결과에는 제2 화자의 음원이 거의 섞여 있지 않다. 또한 제2 시간-주파수 영역에서 믹스된 오디오 신호로부터 제2 화자의 음원을 분리할 때, \"VISUAL VOICE\"는 영역에서 제1 화자의 음원이 함께 섞여 있으나 본 개시에 따른 분리 결과에는 제1 화자 의 음원이 거의 섞여 있지 않다. 정량적 비교 결과, 본 개시에 따른 분리 결과의 분리 성능이 \"VISUAL VOICE\"의 분리 성능에 비하여 약 1.5dB 향상되었다. 도 12는 본 개시의 일 실시예에 따른, 동영상 처리 결과에 기초하여 동영상의 재생을 제어하는 예시적인 방법을 도시한다. 일 실시예에서, 전자 장치는 분리된 음원들이 복수의 스피커에 할당되도록 제어할 수 있다. 도 12를 참조하면, 화면에 포함되어 있는, 두 화자들(1, 2)은 각각 음성(음원)을 출력하고 있으며, 제1 화자에는 제1 음원(Voice#1)이 대응되고, 제2 화자에는 제2 음원(Voice#2)이 대응된다고 가정한다. 전자 장치는 도 9의 동작들(910, 920, 930)을 수행함으로써 믹스된 음원으로부터 제1 음원(Voice#1) 및 제 2 음원(Voice#2)을 분리한다. 영상 내에서 제1 화자는 우측에 위치하고, 제2 화자는 좌측에 위치하므로, 전자 장치는 제1 음원 (Voice#1) 및 제2 음원(Voice#2)을 각각 제1 화자 및 제2 화자에 매칭시킨 후, 좌측 스피커(1210L)에서 는 제2 음원(Voice#2)을 증폭시켜 출력하고, 우측 스피커(1210R)에서는 제1 음원(Voice#1)을 증폭시켜 출력하도 록 제어할 수 있다. 사용자는 화자의 위치에 따른 음원 출력으로 인해 현장감이나 음향의 입체감을 더 잘 느낄 수 있게 된다. 도 13은 본 개시의 일 실시예에 따른, 동영상 처리 결과에 기초하여 동영상의 재생을 제어하는 예시적인 방법을 도시한다. 일 실시예에서, 전자 장치는 동영상의 재생 중 복수의 화자들 중 어느 하나를 선택하는 사용자 입력을 수 신하면, 복수의 음원들 중 선택된 화자에 대응되는 음원의 출력을 강조하도록 제어할 수 있다. 도 13을 참조하면, 재생화면에 포함되어 있는, 두 화자들(1, 2)은 각각 음성을 출력하고 있으며, 제1 화 자에는 제1 음원(Voice#1)이 대응되고, 제2 화자에는 제2 음원(Voice#2)이 대응된다고 가정한다. 전자 장치는 도 9의 동작들(910, 920, 930)을 수행함으로써 믹스된 음원으로부터 제1 음원(Voice#1) 및 제 2 음원(Voice#2)을 분리한다. 사용자가 손가락이나 마우스 커서 등의 선택수단을 통해 제1 화자를 선택하면, 전자 장치는 제1 화자에 대응되는 제1 음원(Voice#1)을 강조하여 출력하도록 제어할 수 있다. 따라서, 좌측 스피커(1310L)와 우측 스피커(1310R) 모두에서 제1 음원(Voice#1)이 증폭되어 출력된다. 한편 도 13에는 두 스피커(1310L, 1310R)에서 제1 음원(Voice#1) 및 제2 음원(Voice#2)이 모두 출력되되, 제1 음성(Voice#1)만이 증폭되어 출력되는 것으로 도시되어 있으나, 이와 다르게 전자 장치는 제1 화자가 선택되었다면 제2 음원(Voice#2)은 제외하고 제1 음원(Voice#1)만이 출력되도록 제어할 수도 있다. 도 14는 본 개시의 일 실시예에 따른, 동영상 처리 결과에 기초하여 동영상의 재생을 제어하는 예시적인 방법을 도시한다. 일 실시예에서, 전자 장치는 동영상의 재생 중 복수의 화자들 중 어느 하나가 표시되는 영역이 줌인(zoom- in)되는 사용자 입력을 수신하면, 복수의 음원들 중 줌인된 영역에 포함된 객체에 대응되는 음원의 출력을 강조 하도록 제어할 수 있다. 도 14를 참조하면, 재생화면은 도 13의 재생화면에서 제1 화자가 포함된 영역이 확대된 화면이 다. 확대되기 전 전체화면에 포함되어 있는 두 화자들(1, 2)은 각각 음원을 출력하고 있으며, 제1 화자에는 제1 음원(Voice#1)이 대응되고, 제2 화자에는 제2 음성(Voice#2)이 대응된다고 가정한다. 전자 장치는 도 9의 동작들(910, 920, 930)을 수행함으로써 믹스된 음원으로부터 제1 음원(Voice#1) 및 제 2 음원(Voice#2)을 분리한다. 사용자가 터치 입력 등을 통해 제1 화자가 포함된 영역을 확대시켜 재생화면이 표시되면, 전자 장치 는 제1 화자에 대응되는 제1 음원(Voice#1)을 강조하여 출력하도록 제어할 수 있다. 따라서, 좌측 스피 커(1410L)와 우측 스피커(1410R) 모두에서 제1 음원(Voice#1)만이 출력된다. 한편, 도 14에는 두 스피커(1410L, 1410R)에서 제2 음원(Voice#2)은 제외하고 제1 음원(Voice#1)만이 출력되는 것으로 도시되었는데, 이와 다르게 전자 장치는 두 스피커(1410L, 1410R)에서 제1 음원(Voice#1) 및 제2 음원(Voice#2)이 모두 출력되되, 제1 음원(Voice#1)만이 증폭되어 출력되도록 제어할 수도 있다. 도 15는 본 개시의 일 실시예에 따른, 동영상 처리 결과에 기초하여 동영상의 재생을 제어하는 예시적인 방법을 도시한다.일 실시예에서, 전자 장치는 동영상이 재생되는 화면을 표시하고, 사용자로부터 믹스된 오디오 신호에 포 함된 복수의 음원에 대응하는 복수의 화자 중 적어도 하나의 화자를 선택하기 위한 입력을 수신하고, 선택된 적 어도 하나의 화자에 대응하는 적어도 하나의 음원의 음량을 조절하기 위한 사용자 인터페이스를 화면에 표시하 고, 사용자로부터의 음량의 조절에 응답하여, 오디오 출력부를 통해 출력되는 적어도 하나의 음원의 음량을 조 절할 수 있다. 도 15를 참조하면, 재생화면은 사용자가 직접 촬영한 동영상이 재생되는 화면이다. 동영상을 촬영할 때 \"촬영 중이야\"와 같은 사용자의 음성이 함께 녹음되었으며, 동영상에 포함된 제1 화자에는 제1 음원 (Voice#1)이 대응되고, 동영상에 포함되지 않은 제2 화자(즉, 촬영자)에는 제2 음원(Voice#2)이 대응된다고 가 정한다. 전자 장치는 도 9의 동작들(910, 920, 930)을 수행함으로써 믹스된 음원으로부터 제1 음원(Voice#1) 및 제 2 음원(Voice#2)을 분리한다. 사용자가 터치 입력 등을 통해 화자 선택 및 대응하는 음원의 음량 조절을 위한 인터페이스(1520, 1530, 1540) 가 화면에 표시되도록 한 후, 제1 화자에 대응되는 제1 음원(Voice#1)의 음량을 높이고, 동영상 밖의 제2 화자에 대응되는 제2 음원(Voice#2)의 음량을 낮추기 위한 사용자 입력이 입력되면, 전자 장치는 제1 화자에 대응되는 제1 음원(Voice#1)을 강조하여 출력하고 제2 화자에 대응되는 제2 음원(Voice#2)을 작게 출 력하도록 제어할 수 있다. 일 실시예에 따른, 이미지 신호 및 믹스된 오디오 신호를 포함하는 동영상을 처리하기 위한 전자 장치는 상기 동영상을 처리하기 위한 프로그램이 저장되는 메모리, 및 상기 프로그램을 실행함으로써 상기 동영상 을 처리하는 프로세서를 포함할 수 있다. 일 실시예에서, 상기 프로세서는, 제1 인공지능 모델을 이용하여, 상기 이미지 신호 및 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 복수의 음원의 겹침(overlap) 정도를 나타내는 오디오 관련 정보를 생성할 수 있다. 일 실시예에서, 상기 프로세서는, 상기 오디오 관련 정보를 제2 인공지능 모델에 적용하여, 상기 믹 스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리할 수 있다. 일 실시예에서, 상기 오디오 관련 정보는 시간-주파수 도메인에서 복수의 음원이 겹치는 정도를 확률로 나타낸 맵(map)을 포함할 수 있다. 일 실시예에서, 상기 제1 인공지능 모델은, 상기 이미지 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원에 대응하는 복수의 화자의 시간적 발화 정보를 나타내는 복수의 입 움직임 정보를 생성하는 제1 서브모델, 및 상기 복수의 입 움직임 정보에 기초하여, 상기 믹스된 오디오 신호로부터, 상기 오디오 관련 정보를 생성하는 제2 서브모델을 포함할 수 있다. 일 실시예에서, 상기 제1 인공지능 모델은 학습용 이미지 신호 및 학습용 오디오 신호로부터 추정된 오디 오 관련 정보와 그라운드 트루스(ground truth)를 비교함으로써 학습될 수 있고, 상기 그라운드 트루스는 상기 학습용 오디오 신호에 포함된 복수의 학습용 개별 음원 각각에 기초하여 생성된 복수의 스펙트로그램 (spectrogram)으로부터 복수의 확률 맵을 생성하고, 상기 복수의 확률 맵 상호 간의 곱(product) 연산에 의해 생성될 수 있다. 일 실시예에서, 상기 복수의 확률 맵 각각은 MaxClip(log(1 + ||F||2), 1) 에 의해 생성될 수 있다. 여기서, ||F||2는 상기 복수의 스펙트로그램 중 대응하는 스펙트로그램의 크기이고, MaxClip(x, 1)은 x가 1보다 작은 경 우 x를 출력하고, x가 1이상인 경우 1을 출력하는 함수이다. 일 실시예에서, 상기 제2 인공지능 모델은 입력 레이어, 복수의 특징 레이어를 포함하는 인코더 , 및 병목(bottleneck) 레이어를 포함할 수 있고, 상기 오디오 관련 정보를 상기 제2 인공지능 모델 에 적용하는 것은, 상기 오디오 관련 정보를 상기 입력 레이어에 적용하는 것, 상기 오디오 관련 정 보를 상기 인코더에 포함된 상기 복수의 특징 레이어 각각에 적용하는 것, 또는 상기 오디오 관련 정보를 상기 병목 레이어에 적용하는 것 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 상기 프로세서는, 제3 인공지능 모델을 이용하여, 상기 믹스된 오디오 신호로부터, 또는 상기 믹스된 오디오 신호 및 시각적 정보로부터, 상기 믹스된 오디오 신호에 포함된 화자 수 관련 정보를 생성하고, 상기 화자 수 관련 정보에 기초하여, 상기 제1 인공지능 모델을 이용하여, 상기 이미지 신호 및 상기 믹스된 오디오 신호로부터, 상기 오디오 관련 정보를 생성하고, 상기 화자 수 관련 정보 및 상기 오디오 관련 정보를 상기 제2 인공지능 모델에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신 호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하되, 상기 시각적 정보는 상기 이미지 신호에 포 함된 적어도 하나의 키 프레임(key frame)을 포함하고, 상기 적어도 하나의 키 프레임은 상기 믹스된 오디오 신 호에 포함된 적어도 하나의 음원에 대응되는 적어도 하나의 화자의 입술을 포함하는 안면 영역을 포함할 수 있 다. 일 실시예에서, 상기 믹스된 오디오 신호에 포함된 화자 수 관련 정보는, 상기 믹스된 오디오 신호와 관련된 제 1 화자 수 관련 정보, 또는 상기 시각적 정보와 관련된 제2 화자 수 관련 정보 중 적어도 하나를 포함할 수 있 다. 일 실시예에서, 상기 제1 화자 수 관련 정보는 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원에 대응되는 화자 수의 확률 분포도를 포함하고, 상기 제2 화자 수 관련 정보는 상기 시각적 정보에 포함된 화자 수의 확률 분포도를 포함할 수 있다. 일 실시예에서, 상기 제2 인공지능 모델은 입력 레이어, 복수의 특징 레이어를 포함하는 인코더 , 및 병목(bottleneck) 레이어를 포함하고, 상기 화자 수 관련 정보를 상기 제2 인공지능 모델 에 적용하는 것은, 상기 화자 수 관련 정보를 상기 입력 레이어에 적용하는 것, 상기 화자 수 관련 정보를 상기 인코더에 포함된 상기 복수의 특징 레이어 각각에 적용하는 것, 또는 상기 화자 수 관련 정보를 상기 병목 레이어에 적용하는 것 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 상기 프로세서는, 상기 이미지 신호로부터 상기 복수의 화자와 연관된 복수의 입 움직임 정보를 획득하고, 상기 획득된 복수의 입 움직임 정보를 상기 제2 인공지능 모델에 적용하여, 상기 믹스된 오디 오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리할 수 있다. 일 실시예에서, 상기 전자 장치는, 상기 동영상이 재생되는 화면을 표시하고, 사용자로부터 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원에 대응하는 복수의 화자 중 적어도 하나의 화자를 선택하기 위한 입력 을 수신하는 입출력 인터페이스, 및 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 선택된 상기 적어도 하나의 화자에 대응되는 적어도 하나의 음원을 출력하는 오디오 출력부를 더 포함할 수 있다. 일 실시예에서, 상기 프로세서는, 상기 선택된 적어도 하나의 화자에 대응하는 적어도 하나의 음원의 음량 을 조절하기 위한 사용자 인터페이스를 상기 화면에 표시하고, 사용자로부터의 상기 음량의 조절에 응답하여, 상기 오디오 출력부를 통해 출력되는 상기 적어도 하나의 음원의 음량을 조절할 수 있다. 일 실시예에 따른, 이미지 신호 및 믹스된 오디오 신호를 포함하는 동영상을 처리하기 위한 방법은, 제1 인공지 능 모델을 이용하여, 상기 이미지 신호 및 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포 함된 복수의 음원의 겹침(overlap) 정도를 나타내는 오디오 관련 정보를 생성하는 단계를 포함할 수 있다. 일 실시예에서, 상기 방법은, 상기 오디오 관련 정보를 제2 인공지능 모델에 적용하여, 상기 믹스된 오디 오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하는 단계를 포함할 수 있다. 일 실시예에서, 상기 오디오 관련 정보는 시간-주파수 도메인에서 복수의 음원이 겹치는 정도를 확률로 나타낸 맵(map)을 포함할 수 있다. 일 실시예에서, 상기 오디오 관련 정보를 생성하는 단계는, 상기 이미지 신호로부터, 상기 믹스된 오디오 신호 에 포함된 상기 복수의 음원에 대응하는 복수의 화자의 시간적 발화 정보를 나타내는 복수의 입 움직임 정보를 생성하는 단계, 및 상기 복수의 입 움직임 정보에 기초하여, 상기 믹스된 오디오 신호로부터, 상기 오디오 관련 정보를 생성하는 단계를 포함할 수 있다. 일 실시예에서, 상기 제1 인공지능 모델은 학습용 이미지 신호 및 학습용 오디오 신호로부터 추정된 오디 오 관련 정보와 그라운드 트루스(ground truth)를 비교함으로써 학습되고, 상기 그라운드 트루스는 상기 학습용 오디오 신호에 포함된 복수의 학습용 개별 음원 각각에 기초하여 생성된 복수의 스펙트로그램(spectrogram)으로 부터 복수의 확률 맵을 생성하고, 상기 복수의 확률 맵 상호 간의 곱(product) 연산에 의해 생성될 수 있다.일 실시예에서, 상기 복수의 확률 맵 각각은 MaxClip(log(1 + ||F||2), 1)에 의해 생성될 수 있다. 여기서, ||F||2는 상기 복수의 스펙트로그램 중 대응하는 스펙트로그램의 크기이고, MaxClip(x, 1)은 x가 1보다 작은 경 우 x를 출력하고, x가 1이상인 경우 1을 출력하는 함수이다. 일 실시예에서, 상기 제2 인공지능 모델은 입력 레이어, 복수의 특징 레이어를 포함하는 인코더 , 및 병목(bottleneck) 레이어를 포함하고, 상기 오디오 관련 정보를 상기 제2 인공지능 모델에 적용하는 것은, 상기 오디오 관련 정보를 상기 입력 레이어에 적용하는 것, 상기 오디오 관련 정보를 상기 인코더에 포함된 상기 복수의 특징 레이어 각각에 적용하는 것, 또는 상기 오디오 관련 정보를 상기 병목 레이어에 적용하는 것 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 상기 방법은, 제3 인공지능 모델을 이용하여, 상기 믹스된 오디오 신호로부터, 또는 상기 믹스된 오디오 신호 및 시각적 정보로부터, 상기 믹스된 오디오 신호에 포함된 화자 수 관련 정보를 생성하는 단계, 상기 화자 수 관련 정보에 기초하여, 상기 제1 인공지능 모델을 이용하여, 상기 이미지 신호 및 상 기 믹스된 오디오 신호로부터, 상기 오디오 관련 정보를 생성하는 단계, 및 상기 화자 수 관련 정보 및 상기 오 디오 관련 정보를 상기 제2 인공지능 모델에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디 오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하는 단계를 더 포함하되, 상기 시각적 정보 는 상기 이미지 신호에 포함된 적어도 하나의 키 프레임(key frame)을 포함하고, 상기 적어도 하나의 키 프레임 은 상기 믹스된 오디오 신호에 포함된 적어도 하나의 음원에 대응되는 적어도 하나의 화자의 입술을 포함하는 안면 영역을 포함할 수 있다. 일 실시예에서, 상기 믹스된 오디오 신호에 포함된 화자 수 관련 정보는, 상기 믹스된 오디오 신호와 관련된 제 1 화자 수 관련 정보, 또는 상기 시각적 정보와 관련된 제2 화자 수 관련 정보 중 적어도 하나를 포함할 수 있 다. 일 실시예에서, 상기 제1 화자 수 관련 정보는 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원에 대응되는 화자 수의 확률 분포도를 포함하고, 상기 제2 화자 수 관련 정보는 상기 시각적 정보에 포함된 화자 수의 확률 분포도를 포함할 수 있다. 일 실시예에서, 상기 제2 인공지능 모델은 입력 레이어, 복수의 특징 레이어를 포함하는 인코더 , 및 병목(bottleneck) 레이어를 포함하고, 상기 화자 수 관련 정보를 상기 제2 인공지능 모델 에 적용하는 것은, 상기 화자 수 관련 정보를 상기 입력 레이어에 적용하는 것, 상기 화자 수 관련 정보를 상기 인코더에 포함된 상기 복수의 특징 레이어 각각에 적용하는 것, 또는 상기 화자 수 관련 정보를 상기 병목 레이어에 적용하는 것 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 상기 방법은, 상기 이미지 신호로부터 상기 복수의 화자와 연관된 복수의 입 움직임 정보를 획 득하는 단계, 및 상기 획득된 복수의 입 움직임 정보를 상기 제2 인공지능 모델에 적용하여, 상기 믹스된 오디오 신호로부터, 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 적어도 하나의 음원을 분리하는 단 계를 더 포함할 수 있다. 일 실시예에서, 상기 방법은, 상기 동영상이 재생되는 화면을 표시하고, 사용자로부터 상기 믹스된 오디오 신호 에 포함된 상기 복수의 음원에 대응하는 복수의 화자 중 적어도 하나의 화자를 선택하기 위한 입력을 수신하는 단계, 및 상기 믹스된 오디오 신호에 포함된 상기 복수의 음원 중 선택된 상기 적어도 하나의 화자에 대응되는 적어도 하나의 음원을 출력하는 단계를 더 포함할 수 있다. 일 실시예에서, 상기 방법은, 상기 선택된 적어도 하나의 화자에 대응하는 적어도 하나의 음원의 음량을 조절하 기 위한 사용자 인터페이스를 상기 화면에 표시하는 단계, 및 사용자로부터의 상기 음량의 조절에 응답하여, 상 기 적어도 하나의 음원의 음량을 조절하는 단계를 더 포함할 수 있다. 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, ‘비 일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다는 것을 의미 할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구분하 지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두개의 사용자 장치들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운 로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다."}
{"patent_id": "10-2023-0022449", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른, 동영상에 포함된 화자별로 음원을 분리하고, 분리 결과에 따라서 동영상의 재생을 제어하는 상황을 설명하기 위한 도면이다. 도 2는 본 개시의 일 실시예에 따른, 동영상을 처리하기 위한 전자 장치의 구성을 개략적으로 도시한 블록도이다. 도 3은 본 개시의 일 실시예에 따른, 전자 장치가 동영상을 처리하기 위한 방법을 수행하도록 하는 프로그램을 복수의 인공지능 모델들로 표현한 도면이다. 도 4a는 본 개시의 일 실시예에 따른, 음원 특성 분석 모델의 구조 및 입출력 관계를 도시한다. 도 4b는 본 개시의 일 실시예에 따른, 음원 특성 분석 모델에 포함된 제1 서브모델의 구조 및 입출력 관계를 도 시한다. 도 4c는 본 개시의 일 실시예에 따른, 음원 특성 분석 모델에 포함된 제2 서브모델의 구조 및 입출력 관계를 도 시한다. 도 5는 본 개시의 일 실시예에 따른, 음원 분리 모델의 예시적인 구조 및 오디오 관련 정보가 음원 분리 모델에 적용되는 방법을 설명하기 위한 도면이다. 도 6a 및 6b는 본 개시의 일 실시예에 따른, 화자 수 분석 모델의 입 출력 관계를 도시한다. 도 7은 본 개시의 일 실시예에 따른, 화자 수 관련 정보가 음원 분리 모델에 적용되는 방법을 설명하기 위한 도 면이다. 도 8은 본 개시의 일 실시예에 따른, 음원 특성 분석 모델의 학습 방법을 설명하기 위한 도면이다. 도 9는 본 개시의 일 실시예에 따른, 동영상을 처리하기 위한 방법의 순서도이다. 도 10a 및 10b는 본 개시의 일 실시예에 따른, 동영상을 처리하기 위한 전자 장치 및 방법의 성능에 관한 실험 결과를 도시한다. 도 11a 및 11b는 본 개시의 일 실시예에 따른, 동영상을 처리하기 위한 전자 장치 및 방법의 성능을 종래기술과 비교한 실험 결과를 도시한다. 도 12는 본 개시의 일 실시예에 따른, 동영상 처리 결과에 기초하여 동영상의 재생을 제어하는 예시적인 방법을 도시한다. 도 13은 본 개시의 일 실시예에 따른, 동영상 처리 결과에 기초하여 동영상의 재생을 제어하는 예시적인 방법을 도시한다. 도 14는 본 개시의 일 실시예에 따른, 동영상 처리 결과에 기초하여 동영상의 재생을 제어하는 예시적인 방법을 도시한다. 도 15는 본 개시의 일 실시예에 따른, 동영상 처리 결과에 기초하여 동영상의 재생을 제어하는 예시적인 방법을 도시한다."}
