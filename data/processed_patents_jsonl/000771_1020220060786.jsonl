{"patent_id": "10-2022-0060786", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0161143", "출원번호": "10-2022-0060786", "발명의 명칭": "인공지능 추론 서비스 방법, 및 이에 적용되는 서비스장치", "출원인": "에스케이텔레콤 주식회사", "발명자": "정승록"}}
{"patent_id": "10-2022-0060786", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "서비스장치에서 수행되는 멀티 GPU 기반의 인공지능 추론 서비스 방법에 있어서,서비스 응답성에 따른 우선순위가 기 설정된 다수의 인공지능 모델 별 서비스 데이터인 태스크에 대해 우선순위를 확인하는 우선순위확인단계; 및큐(Queue)에 도착한 신규 태스크의 우선순위가 상기 큐에 누적된 현재 태스크의 우선순위보다 높은 경우, 상기신규 태스크의 선점(Preemption)을 허용하여 상기 현재 태스크에 앞서 상기 신규 태스크가 상기 멀티 GPU 각각에 할당되도록 하는 스케줄링단계를 포함하는 것을 특징으로 하는 인공지능 추론 서비스 방법."}
{"patent_id": "10-2022-0060786", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 우선순위확인단계는,상기 다수의 인공지능 모델에서 제공되는 기본 서비스 단위인 배치(Batch)로부터 2 이상으로 분할된 서비스 단위인 로컬 배치(Local Batch)마다 상기 서비스 응답성에 따른 우선순위를 확인하는 것을 특징으로 하는 인공지능 추론 서비스 방법."}
{"patent_id": "10-2022-0060786", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 방법은,상기 다수의 인공지능 모델 각각에 대해서, 상기 서비스 응답성에 따른 우선순위를 정의한 우선순위태그(Priority Tag)를 상기 배치 단위로 설정하는 우선순위설정단계를 더 포함하며,상기 우선순위확인단계는,상기 배치 단위로 확인되는 상기 우선순위태그를 참조로, 상기 현재 태스크와 상기 신규 태스크에 대해 상기 서비스 응답성에 따른 우선순위를 확인하는 것을 특징으로 하는 인공지능 추론 서비스 방법."}
{"patent_id": "10-2022-0060786", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "멀티 GPU 기반의 서비스장치에 있어서,서비스 응답성에 따른 우선순위가 기 설정된 다수의 인공지능 모델 별 서비스 데이터인 태스크에 대해 우선순위를 확인한 결과, 큐(Queue)에 도착한 신규 태스크의 우선순위가 상기 큐에 누적된 현재 태스크의 우선순위보다높은 경우, 상기 신규 태스크의 선점(Preemption)을 허용하여 상기 현재 태스크에 앞서 상기 신규 태스크가 상기 멀티 GPU 각각에 할당되도록 하는 스케줄링부를 포함하는 것을 특징으로 하는 서비스장치."}
{"patent_id": "10-2022-0060786", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항 내지 제 3 항 중 어느 한 항의 방법을 실행시키기 위한 프로그램을 기록한 컴퓨터 판독 가능 기록매체."}
{"patent_id": "10-2022-0060786", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "하드웨어와 결합되어, 제 1 항 내지 제 3 항 중 어느 한 항의 방법을 실행시키기 위해 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2022-0060786", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은, 멀티 GPU 기반의 인공지능 추론 서비스 방안에 관한 것으로서, 서비스 응답성(예: 신속 처리 VS 여유 있는 처리)에 따른 우선순위를 기준으로 인공지능 모델 별 서비스 데이터를 멀티 GPU 각각에서 나눠서 처리하기 위한 방안에 관한 것이다."}
{"patent_id": "10-2022-0060786", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은, 멀티 GPU 기반의 인공지능 추론 서비스에 관한 것으로서, 서비스 응답성(예: 신속 처리 VS 여유 있 는 처리)에 따른 우선순위를 기준으로 인공지능 모델 별 서비스 데이터를 멀티 GPU 각각에서 나눠서 처리하기 위한 방안에 관한 것이다."}
{"patent_id": "10-2022-0060786", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 모델을 이용한 추론 서비스에 있어서, 인공지능 모델의 크기는 지속적으로 커지고 있는데 반해서 GPU 의 메모리 크기 및 연산 증가 속도에는 한계가 있다. 특히 최근 자연어 처리에서 다양한 응용에 활용되는 attention 기반 transformer 모델의 경우 모델 사이즈와 모 델 정확도가 비례하는 경향이 있어서, 인공 지능 모델의 크기가 점점 커지는 추세이다. 이에 따라, 인공지능 모델의 서비스 데이터를 처리하는 GPU의 하드웨어(HW) 성능 개선을 고려해볼 수 있으나, 실제 하드웨어(HW)의 성능 개선이 모델 사이즈 증가 폭을 따라가지 못해 인공지능 모델 기반의 인공지능 추론 서비스를 실시간으로 대용량으로 제공하는데 제약이 따른다. 이러한 문제를 현실적으로 해결하기 위해서 인공 지능 모델을 여러 GPU에 나눠서 구동하는 방식인 멀티 GPU 기 반의 인공지능 추론 서비스가 다양한 방면에서 시도되고 있다. 그러나, 멀티 GPU 기반의 인공지능 추론 서비스에서는 하나의 GPU에서 처리가 완료되어야 다음 GPU에서의 처리 가 가능한 dependency가 존재할 수 있으며, 이 경우, 여러 GPU에 나눠서 추론 서비스를 제공함에도 불구하고, 각 GPU마다 활용률이 없는 유휴 시간이 발생하게 되어, 기대만큼의 효율을 달성할 수 없다. 이에, 본 발명에서는 멀티 GPU 기반의 인공지능 추론 서비스에서 있어서, 위에서와 같이 각 GPU마다의 활용률 낮아지는 문제점을 해소할 수 있는 새로운 방안을 제안하고자 한다."}
{"patent_id": "10-2022-0060786", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기한 사정을 감안하여 창출된 것으로서, 본 발명에서 도달하고자 하는 목적은, 본 발명은, 멀티 GPU 기반의 인공지능 추론 서비스 방안에 관한 것으로서, 서비스 응답성(예: 신속 처리 VS 여유 있는 처리)에 따른 우선순위를 기준으로 인공지능 모델 별 서비스 데이터를 멀티 GPU 각각에서 나눠서 처리하는데 있다."}
{"patent_id": "10-2022-0060786", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 멀티 GPU 기반의 인공지능 추론 서비스 방법은, 서비스 응답성에 따른 우선순위가 기 설정된 다수의 인공지능 모델 별 서비스 데이터인 태스크에 대해 우선순위를 확인 하는 우선순위확인단계; 및 큐(Queue)에 도착한 신규 태스크의 우선순위가 상기 큐에 누적된 현재 태스크의 우 선순위보다 높은 경우, 상기 신규 태스크의 선점(Preemption)을 허용하여 상기 현재 태스크에 앞서 상기 신규 태스크가 상기 멀티 GPU 각각에 할당되도록 하는 스케줄링단계를 포함하는 것을 특징으로 한다. 구체적으로, 상기 우선순위확인단계는, 상기 다수의 인공지능 모델에서 제공되는 기본 서비스 단위인 배치 (Batch)로부터 2 이상으로 분할된 서비스 단위인 로컬 배치(Local Batch)마다 상기 서비스 응답성에 따른 우선 순위를 확인할 수 있다. 구체적으로, 상기 방법은, 상기 다수의 인공지능 모델 각각에 대해서, 상기 서비스 응답성에 따른 우선순위를 정의한 우선순위태그(Priority Tag)를 상기 배치 단위로 설정하는 우선순위설정단계를 더 포함하며, 상기 우선 순위확인단계는, 상기 배치 단위로 확인되는 상기 우선순위태그를 참조로, 상기 현재 태스크와 상기 신규 태스 크에 대해 상기 서비스 응답성에 따른 우선순위를 확인할 수 있다. 상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 멀티 GPU 기반의 서비스장치는, 서비스 응답성에 따른 우선순위가 기 설정된 다수의 인공지능 모델 별 서비스 데이터인 태스크에 대해 우선순위를 확인한 결과, 큐 (Queue)에 도착한 신규 태스크의 우선순위가 상기 큐에 누적된 현재 태스크의 우선순위보다 높은 경우, 상기 신 규 태스크의 선점(Preemption)을 허용하여 상기 현재 태스크에 앞서 상기 신규 태스크가 상기 멀티 GPU 각각에 할당되도록 하는 스케줄링부를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2022-0060786", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이에, 본 발명의 실시 예에 따르면, 멀티 GPU 기반의 인공지능 추론 서비스를 제공함에 있어서, 서비스 응답성 (예: 신속 처리 VS 여유 있는 처리)에 따른 우선순위를 기준으로 인공지능 모델 별 서비스 데이터를 멀티 GPU 각각에서 나눠서 처리함으로써, 각 GPU의 활용성 개선뿐만 아니라 서비스 응답성을 제고시키는 효과를 성취할 수 있다."}
{"patent_id": "10-2022-0060786", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명의 다양한 실시 예에 대하여 설명한다. 본 발명의 일 실시예에서는 인공지능 모델을 이용한 추론 서비스에 관한 기술을 다룬다. 인공지능 모델을 이용한 추론 서비스에 있어서, 인공지능 모델의 크기는 지속적으로 커지고 있는데 반해서 GPU 의 메모리 크기 및 연산 증가 속도에는 한계가 있다. 특히 최근 자연어 처리에서 다양한 응용에 활용되는 attention 기반 transformer 모델의 경우 모델 사이즈와 모 델 정확도가 비례하는 경향이 있어서, 인공 지능 모델의 크기가 점점 커지는 추세이다. 이에 따라, 인공지능 모델의 서비스 데이터를 처리하는 GPU의 하드웨어(HW) 성능 개선을 고려해볼 수 있으나, 실제 하드웨어(HW)의 성능 개선이 모델 사이즈 증가 폭을 따라가지 못해 인공지능 모델 기반의 인공지능 추론 서비스를 실시간으로 대용량으로 제공하는데 제약이 따른다. 이러한 문제를 현실적으로 해결하기 위해서 인공 지능 모델을 여러 GPU에 나눠서 구동하는 방식인 멀티 GPU 기 반의 인공지능 추론 서비스가 다양한 방면에서 시도되고 있다. 이러한 멀티 GPU 기반의 인공지능 추론 서비스에서는, 예컨대, 도 1에서와 같이, 하나의 모델을 입력 Feature map과 인공지능 연산 중간 layer를 수평으로 나눠서 여러 GPU에서 처리하는 Tensor parallel방식과, 인공지능 layer별로 여러 GPU에서 나눠서 처리하는 Model parallel방식이 존재한다. 위 두 가지 방식은 인공지능 추론 서비스의 처리량 및 실시간성 목적 및 인공지능 모델의 특성에 따라 동시에 혹은 하나만 적용하는 경우가 있다. 이와 관련하여, Model parallel 방식으로 하나의 모델을 분산 GPU에서 추론을 하는 경우, 모델 전체를 GPU의 수 로 나누어서 각각의 GPU에서 처리를 할 수 있다. 예를 들어, 도 2에서와 같이 GPU#1은 처음 layer인 A1에 해당하는 부분을, 마지막 GPU#4는 최종 layer인 A4에 해당하는 부분을 처리하게 되며, 인공지능 모델 A에 대해서 j는 GPU index일 경우, Aj는 GPUj에 할당 된다. 다만, Model parallel 방식은 GPUj-1의 처리가 끝나야만 GPUj의 처리가 가능한 dependency가 존재하므로, GPU의 활용률이 없는 유휴 시간(Bubble)이 발생하게 되어 GPU의 활용률이 낮아지는 한계를 가진다. 이에, 본 발명의 일 실시예에서는, 위 Model parallel 방식의 한계점을 개선하기 위해, 각 모델을 추론 서비스 할 때 서비스의 단위를 전체 job volume인 배치(Batch) 단위가 아닌 배치를 2 이상의 로컬 배치(Local Batch)로 분할하고, 이러한 로컬 배치를 GPU의 timing slot으로 나눠서 pipelining하는 pipeline parallel 추론 방식을 적용한다. 즉, 인공지능 모델 A에 대해서 i는 ith 로컬 배치(local batch)이고, j는 GPU index일 경우, Ai,j는 아래 도 3에 서와 같이 처리가 되어, GPU에서의 유휴 시간인 bubble을 없앨 수 있다. 한편, 이러한 Pipeline parallel 방식을 이용하여 다수의 인공지능 모델 기반의 추론 서비스를 제공하는 경우, 각 인공지능 모델 간의 처리 순서는 배치(batch)가 생성된 순서인 A에서 B 순으로 따르게 된다. 그러나, 인공지능 모델의 경우 그 특성에 따라 요구되는 서비스 응답성(예: 신속 처리 VS 여유 있는 처리)이 달 라질 수 있는데, 만약 위에서와 같이 배치(batch)가 생성된 순서대로 각 인공지능 모델 간의 처리 순서가 정해 진다면, 신속 처리가 요구되는 인공지능 모델임에도 불구하고, 원하는 만큼의 서비스 응답성을 얻지 못하게 되 는 결과가 초래될 수 있다. 이에, 본 발명의 일 실시예에서는 멀티 GPU 기반의 인공지능 추론 서비스에 있어서, 인공지능 모델 별로 서비스 응답성에 따른 우선순위를 선점(Preemption)하는 것을 가능하게 하는 새로운 스케줄링 방안을 제안하고자 한다. 이와 관련하여, 도 4에는 본 발명의 일 실시예에 따른 멀티 GPU 기반의 인공지능 추론 서비스 환경을 예시적으 로 보여주고 있다. 도 4에 도시된 바와 같이, 본 발명의 일 실시예에 따른 멀티 GPU 기반의 인공지능 추론 서비스 환경에서는, 클 라이언트(client)에 대해서 인공지능 추론 서비스를 제공하는 서비스장치를 포함할 수 있다. 여기서, 서비스장치는 Pipeline parallel방식으로 인공지능 추론 서비스를 제공하되, 인공지능 모델 별로 서비스 응답성에 따른 우선순위를 선점(Preemption)할 수 있도록 스케줄링하여 각 노드(node) 별 혹은 노드에 할당된 GPU 별 구동을 수행하는 장치를 일컫는다. 이러한, 서비스장치에는 스마트폰 등의 이동단말기와 노트북, 데스크탑(Desktop), 랩탑(Laptop), 유선식 또는 무선식의 이동 또는 고정 컨트롤러, 또는 서버 등이 포함될 수 있으며, 물론 이에 제한되는 것이 아닌 연 산 수단 및 통신 수단이 구비된 모든 종류의 장치가 포함될 수 있다. 참고로, 서비스장치가 서버 형태로 구현되는 경우에는, 예컨대, 웹 서버, 데이터베이스 서버, 프록시 서버 등의 형태로 구현될 수 있으며, 네트워크 부하 분산 메커니즘, 내지 서비스 장치가 인터넷 또는 다른 네트워크 상에서 동작할 수 있도록 하는 다양한 소프트웨어 중 하나 이상이 설치될 수 있으며, 이를 통해 컴퓨터화된 시 스템으로도 구현될 수 있다. 이상 본 발명의 일 실시예에 따른 멀티 GPU 기반의 인공지능 추론 서비스 환경에서는, 전술한 구성을 통해서 각 GPU의 활용성 개선뿐만 아니라 서비스 응답성을 제고시킬 수 있는데, 이하에서는 이를 실현하기 위한 서비스장 치의 구성을 보다 구체적으로 설명하기로 한다. 도 5는 본 발명의 일 실시예에 따른 서비스장치의 개략적인 구성을 보여주고 있다. 도 5에 도시된 바와 같이, 본 발명의 일 실시예에 따른 서비스장치는 클라이언트의 요청에 따라 우선순위 를 설정하는 설정부, 서비스 데이터의 처리 순서를 결정하는 스케줄링부를 포함하는 구성을 가질 수 있다. 또한, 본 발명의 일 실시예에 따른 서비스장치는 전술한 구성 이외에, 각 노드(node) 별 혹은 노드에 할당 된 GPU 별 구동을 수행하는 구동부를 더 포함하는 구성을 가질 수 있다. 이상의 설정부, 스케줄링부, 및 구동부를 포함하게 되는 서비스장치의 구성 전체 내지는 적어도 일부는 하드웨어 모듈 형태 또는 소프트웨어 모듈 형태로 구현되거나, 하드웨어 모듈과 소프트웨어 모듈 이 조합된 형태로도 구현될 수 있다. 여기서, 소프트웨어 모듈이란, 예컨대, 서비스장치 내에서 연산을 제어하는 프로세서에 의해 실행되는 명 령어로 이해될 수 있으며, 이러한 명령어는 서비스장치 내 메모리에 탑재된 형태를 가질 수 있다. 결국, 본 발명의 실시 예에 따른 서비스장치는 전술한 구성을 통해 각 GPU의 활용성 개선뿐만 아니라 서비 스 응답성을 제고시킬 수 있는데, 이하에서는 이를 실현하기 위한 서비스장치 내 각 구성에 대해 보다 구 체적인 설명을 이어 가기로 한다. 한편, 본 발명의 일 실시예에 따른 서비스장치는 멀티(Multi)-모델 환경의 지원에 따라 다수의 인공지능 모델을 관리할 수 있는데, 이하에서는 설명의 편의를 위해 제1 인공지능 모델(Model A), 제2 인공지능 모델 (Model B), 및 제3 인공지능 모델(Model C)을 관리하는 것을 전제로 설명을 이어 가기로 한다. 설정부는 서비스 응답성에 따른 우선순위를 설정하는 기능을 담당한다. 보다 구체적으로, 설정부는 클라이언트(client)로부터의 요청에 따라 제1 인공지능 모델(Model A), 제2 인 공지능 모델(Model B), 및 제3 인공지능 모델(Model B) 각각에 대해서 서비스 응답성에 따른 우선순위를 설정하게 된다. 이때, 설정부는 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인공지능 모델(Model B) 간에 서비스 응답성에 따른 우선순위를 정의한 우선순위태그(Priority Tag)를, 기본 서비스 단위인 배치 (Batch) 단위로 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인공지능 모델(Model C) 각 각에 대해서 설정할 수 있다. 예를 들어, 제3 인공지능 모델(Model C)이 제2 인공지능 모델(Model B)에 비해 실시간성이 높은 서비스인 반면, 제1 인공지능 모델(Model A)은 지연이 일어나도 상관이 없는 서비스인 경우를 가정하면, 제3 인공지능 모델 (Model C: 1 순위), 제2 인공지능 모델(Model B: 2 순위), 제1 인공지능 모델(Model A: 3 순위)의 순서로 서비 스 응답성에 따른 우선순위가 설정될 수 있는 것이다. 스케줄링부는 서비스 데이터의 처리 순서를 결정하는 기능을 담당한다. 보다 구체적으로, 스케줄링부는 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인공지 능 모델(Model B) 각각에 대해서 서비스 응답성에 따른 우선순위가 설정되면, 설정된 우선순위를 참조하여 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인공지능 모델(Model B) 각각의 서비스 데이터에 대해서 처리 순서를 결정하게 된다. 이를 위해서, 스케줄링부는 기 정의된 추론 서비스 단위마다, 큐(Queue)에 누적되어 있는 인공지능 모델의 서비스 데이터인 현재 태스크와, 큐에 도착한 인공지능 모델의 서비스 데이터인 신규 태스크에 대해 서비스 응 답성에 따른 우선순위를 확인한다. 여기서, 기 정의된 추론 서비스 단위는, 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인 공지능 모델(Model B)에서 제공되는 기본 서비스 단위인 배치(Batch)로부터 2 이상으로 분할된 서비스 단위인 로컬 배치(Local Batch) 단위로 이해될 수 있다. 다시 말해, 스케줄링부는 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인공지능 모 델(Model B)의 추론 서비스 단위를 배치(Batch)가 아닌 로컬 배치(Local Batch) 단위로 나누고, 이러한 로컬 배 치(Local Batch)를 GPU의 timing slot으로 나눠서 처리하는 Pipeline parallel 추론 과정에서 각 로컬 배치마 다 큐(Queue)에 누적되어 있는 인공지능 모델의 서비스 데이터인 현재 태스크와, 큐에 도착한 인공지능 모델의 서비스 데이터인 신규 태스크에 대해서 서비스 응답성에 따른 우선순위를 확인하고 있는 것이다. 이때, 스케줄링부는 로컬 배치(Local Batch)마다 배치 단위로 확인되는 우선순위태그를 참조로, 큐(Queu e)에 누적되어 있는 인공지능 모델의 서비스 데이터인 현재 태스크와, 큐에 도착한 인공지능 모델의 서비스 데 이터인 신규 태스크에 대해 서비스 응답성에 따른 우선순위를 확인할 수 있다. 나아가, 스케줄링부는 서비스 응답성에 따른 우선순위 확인 결과에 따라 큐에 도착한 인공지능 모델의 서 비스 데이터인 신규 태스크에 대한 선점(Preemption) 허용 여부를 결정하게 된다. 이때, 스케줄링부는 신규 태스크의 우선순위가 현재 태스크의 우선순위보다 높은 것으로 확인되면, 신규 태스크의 선점(Preemption)을 허용하여 현재 태스크에 앞서 신규 태스크가 멀티 GPU 각각에 할당되도록 스케줄 링할 수 있다. 물론, 스케줄링부는 현재 태스크의 우선순위가 신규 태스크의 우선순위보다 높은 것으로 확인되면, 신규 태스크의 선점(Preemption)을 불허하여 큐에 누적된 기존 순서대로 신규 태스크에 앞서 현재 태스크가 멀티 GPU 각각에 할당되도록 스케줄링한다. 예를 들어, 도 6에서와 같이, t2시점에서 제2 인공지능 모델(model B)의 로컬 배치(Local Batch)인 B1,j가 이미 큐(Queue)에 도착을 하였고, 우선순위태그(Priority Tag) 상 제2 인공지능 모델(Model B)의 우선순위가 제1 인 공지능 모델(Model A)보다 높으면 다음 처리 순서(issue)는 A2,j가 아닌 B1,j가 되는 것이다. 마찬가지로 t5에서 제2 인공지능 모델(Model B)보다 높은 우선순위를 갖는 제3 인공지능 모델(Model C)의 로컬 배치(Local Batch)인 C1,j가 도착하면 역시 선점권을 C1,j에게 주는 방식으로 이해될 수 있다. 한편, t12, t13, t16에서는 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인공지능 모델 (Model C)의 기본 추론 서비스 단위인 배치(Batch)가 모두 끝이 나게 되며, 큐(Queue)에는 다음 job의 태스크가 누적되는 과정이 반복된다.정리하자면, 스케줄링부는 이렇듯, 큐(Queue)에 태스크를 누적하고, 각 인공지능 모델의 로컬 배치(Local Batch)마다 우선순위태그(Priority Tag)를 통해 우선순위를 확인하게 되며, 만약 이 과정에서 큐(Queue)에 누적 되어 있는 현재 태스크보다 큐에 도착한 신규 태스크의 우선순위가 높은 경우, 신규 태스크에 대해 선점 기회를 주고, 그렇지 않은 경우 큐(Queue)에 누적된 순서대로 태스크가 처리되도록 스케줄링을 수행하고 있는 것이다. 구동부는 멀티-GPU를 구동하는 기능을 담당한다. 보다 구체적으로, 구동부는 로컬 배치(Local Batch)마다 서비스 응답성에 따른 우선순위를 기준으로 수행 된 스케줄링 결과에 따라, 멀티 GPU 각각에 큐(Queue)에 누적되어 있는 현재 태스크, 또는 큐에 도착한 신규 태 스크를 할당 및 구동하게 된다. 이때, 구동부는 큐(Queue)에 누적되어 있는 현재 태스크의 우선순위가 큐에 도착한 신규 태스크보다 높은 경우, 신규 태스크에 앞서 현재 태스크를 멀티 GPU 각각에 할당하게 되며, 반대의 경우라면 현재 태스크에 앞서 신규 태스크를 멀티 GPU 각각에 할당하는 방식으로 각 노드(node) 별 혹은 노드에 할당된 GPU 별 구동을 수행할 수 있다. 한편, 구동부는 GPU 별 구동을 통해서 각 로컬 배치(Local Batch)에 대한 처리가 완료되면 종료 플래그 (Flag)를 스케줄링부로 전달(broadcasting)하게 되며, 이러한 동작은 계속 반복된다. 이상에서 살펴본 바와 같이 본 발명의 일 실시예에 따른 서비스장치의 구성에 따르면, 기 정의된 추론 서 비스 단위인 로컬 배치(Local Batch)마다 서비스 응답성(예: 신속 처리 VS 여유 있는 처리)에 따른 우선순위를 기준으로 인공지능 모델 별 서비스 데이터를 멀티 GPU 각각에서 나눠서 처리함으로써, 각 GPU의 활용성 개선뿐 만 아니라 서비스 응답성을 제고시키는 효과를 성취할 수 있다. 이하에서는, 도 7을 참조하여 본 발명의 일 실시예에 따른 인공지능 추론 서비스 방법을 설명하기로 한다. 본 발명의 일 실시예에 따른 인공지능 추론 서비스 방법의 동작 주체는 서비스장치가 되므로, 앞서 설명한 도 5의 참조번호를 언급하여 설명을 이어 가기로 한다. 먼저, 설정부는 클라이언트(client)로부터의 요청에 따라 제1 인공지능 모델(Model A), 제2 인공지능 모델 (Model B), 및 제3 인공지능 모델(Model B) 각각에 대해서 서비스 응답성에 따른 우선순위를 설정한다(S110). 이때, 설정부는 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인공지능 모델(Model B) 간에 서비스 응답성에 따른 우선순위를 정의한 우선순위태그(Priority Tag)를, 기본 서비스 단위인 배치 (Batch) 단위로 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인공지능 모델(Model C) 각 각에 대해서 설정할 수 있다. 예를 들어, 제3 인공지능 모델(Model C)이 제2 인공지능 모델(Model B)에 비해 실시간성이 높은 서비스인 반면, 제1 인공지능 모델(Model A)은 지연이 일어나도 상관이 없는 서비스인 경우를 가정하면, 제3 인공지능 모델 (Model C: 1 순위), 제2 인공지능 모델(Model B: 2 순위), 제1 인공지능 모델(Model A: 3 순위)의 순서로 서비 스 응답성에 따른 우선순위가 설정될 수 있는 것이다. 그리고 나서, 스케줄링부는 기 정의된 추론 서비스 단위마다, 큐(Queue)에 누적되어 있는 인공지능 모델의 서비스 데이터인 현재 태스크와, 큐에 도착한 인공지능 모델의 서비스 데이터인 신규 태스크에 대해 서비스 응 답성에 따른 우선순위를 확인한다(S120-S130). 여기서, 기 정의된 추론 서비스 단위는, 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인 공지능 모델(Model B)에서 제공되는 기본 서비스 단위인 배치(Batch)로부터 2 이상으로 분할된 서비스 단위인 로컬 배치(Local Batch) 단위로 이해될 수 있다. 다시 말해, 스케줄링부는 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인공지능 모 델(Model B)의 추론 서비스 단위를 배치(Batch)가 아닌 로컬 배치(Local Batch) 단위로 나누고, 이러한 로컬 배 치(Local Batch)를 GPU의 timing slot으로 나눠서 처리하는 Pipeline parallel 추론 과정에서 각 로컬 배치마 다 큐(Queue)에 누적되어 있는 인공지능 모델의 서비스 데이터인 현재 태스크와, 큐에 도착한 인공지능 모델의 서비스 데이터인 신규 태스크에 대해서 서비스 응답성에 따른 우선순위를 확인하고 있는 것이다. 이때, 스케줄링부는 로컬 배치(Local Batch)마다 배치 단위로 확인되는 우선순위태그를 참조로, 큐(Queu e)에 누적되어 있는 인공지능 모델의 서비스 데이터인 현재 태스크와, 큐에 도착한 인공지능 모델의 서비스 데이터인 신규 태스크에 대해 서비스 응답성에 따른 우선순위를 확인할 수 있다. 나아가, 스케줄링부는 서비스 응답성에 따른 우선순위 확인 결과에 따라 큐에 도착한 인공지능 모델의 서 비스 데이터인 신규 태스크에 대한 선점(Preemption) 허용 여부를 결정한다(S140-S160). 이때, 스케줄링부는 신규 태스크의 우선순위가 현재 태스크의 우선순위보다 높은 것으로 확인되면, 신규 태스크의 선점(Preemption)을 허용하여 현재 태스크에 앞서 신규 태스크가 멀티 GPU 각각에 할당되도록 스케줄 링할 수 있다. 물론, 스케줄링부는 현재 태스크의 우선순위가 신규 태스크의 우선순위보다 높은 것으로 확인되면, 신규 태스크의 선점(Preemption)을 불허하여 큐에 누적된 기존 순서대로 신규 태스크에 앞서 현재 태스크가 멀티 GPU 각각에 할당되도록 스케줄링한다. 앞서 예시한, 도 6에서와 같이, t2시점에서 제2 인공지능 모델(model B)의 로컬 배치(Local Batch)인 B1,j가 이 미 큐(Queue)에 도착을 하였고, 우선순위태그(Priority Tag) 상 제2 인공지능 모델(Model B)의 우선순위가 제1 인공지능 모델(Model A)보다 높으면 다음 처리 순서(issue)는 A2,j가 아닌 B1,j가 되는 것이다. 마찬가지로 t5에서 제2 인공지능 모델(Model B)보다 높은 우선순위를 갖는 제3 인공지능 모델(Model C)의 로컬 배치(Local Batch)인 C1,j가 도착하면 역시 선점권을 C1,j에게 주는 방식으로 이해될 수 있다. 한편, t12, t13, t16에서는 제1 인공지능 모델(Model A), 제2 인공지능 모델(Model B), 및 제3 인공지능 모델 (Model C)의 기본 추론 서비스 단위인 배치(Batch)가 모두 끝이 나게 되며, 큐(Queue)에는 다음 job의 태스크가 누적되는 과정이 반복된다. 정리하자면, 스케줄링부는 이렇듯, 큐(Queue)에 태스크를 누적하고, 각 인공지능 모델의 로컬 배치(Local Batch)마다 우선순위태그(Priority Tag)를 통해 우선순위를 확인하게 되며, 만약 이 과정에서 큐(Queue)에 누적 되어 있는 현재 태스크보다 큐에 도착한 신규 태스크의 우선순위가 높은 경우, 신규 태스크에 대해 선점 기회를 주고, 그렇지 않은 경우 큐(Queue)에 누적된 순서대로 태스크가 처리되도록 스케줄링을 수행하고 있는 것이다. 이후, 구동부는 로컬 배치(Local Batch)마다 서비스 응답성에 따른 우선순위를 기준으로 수행된 스케줄링 결과에 따라, 멀티 GPU 각각에 큐(Queue)에 누적되어 있는 현재 태스크, 또는 큐에 도착한 신규 태스크를 할당 및 구동한다(S170). 이때, 구동부는 큐(Queue)에 누적되어 있는 현재 태스크의 우선순위가 큐에 도착한 신규 태스크보다 높은 경우, 신규 태스크에 앞서 현재 태스크를 멀티 GPU 각각에 할당하게 되며, 반대의 경우라면 현재 태스크에 앞서 신규 태스크를 멀티 GPU 각각에 할당하는 방식으로 각 노드(node) 별 혹은 노드에 할당된 GPU 별 구동을 수행할 수 있다. 한편, 구동부는 GPU 별 구동을 통해서 각 로컬 배치(Local Batch)에 대한 처리가 완료되면 종료 플래그 (Flag)를 스케줄링부로 전달(broadcasting)하게 되며, 이러한 동작은 계속 반복된다. 이상에서 살펴본 바와 같이 본 발명의 일 실시예에 따른 인공지능 추론 서비스 방법에 의하면, 기 정의된 추론 서비스 단위인 로컬 배치(Local Batch)마다 서비스 응답성(예: 신속 처리 VS 여유 있는 처리)에 따른 우선순위 를 기준으로 인공지능 모델 별 서비스 데이터를 멀티 GPU 각각에서 나눠서 처리함으로써, 각 GPU의 활용성 개선 뿐만 아니라 서비스 응답성을 제고시키는 효과를 성취할 수 있다. 본 발명의 실시 예에 따른 동작 방법은, 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구 현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있 다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체 (magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장 하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작 동하도록 구성될 수 있으며, 그 역도 마찬가지이다.지금까지 본 발명을 바람직한 실시 예를 참조하여 상세히 설명하였지만, 본 발명이 상기한 실시 예에 한정되는 것은 아니며, 이하의 특허청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 본 발명이 속하는 기술 분야 에서 통상의 지식을 가진 자라면 누구든지 다양한 변형 또는 수정이 가능한 범위까지 본 발명의 기술적 사상이 미친다 할 것이다. 산업상 이용가능성 본 발명의 인공지능 추론 서비스 방법, 및 이에 적용되는 서비스장치에 따르면, 서비스 응답성(예: 신속 처리 VS 여유 있는 처리)에 따른 우선순위를 기준으로 인공지능 모델 별 서비스 데이터를 멀티 GPU 각각에서 나눠서 처리할 수 있다는 점에서, 기존 기술의 한계를 뛰어 넘음에 따라 관련 기술에 대한 이용만이 아닌 적용되는 장 치의 시판 또는 영업의 가능성이 충분할 뿐만 아니라 현실적으로 명백하게 실시할 수 있는 정도이므로 산업상 이용가능성이 있는 발명이다."}
{"patent_id": "10-2022-0060786", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1 내지 도 3은 종래 기술에 따른 멀티 GPU 기반의 인공지능 추론 서비스 방식을 설명하기 위한 예시도. 도 4는 본 발명의 일 실시예에 따른 멀티 GPU 기반의 인공지능 추론 서비스 환경을 설명하기 위한 예시도. 도 5는 본 발명의 일 실시예에 따른 서비스장치의 개략적인 구성도. 도 6은 본 발명의 일 실시예에 따른 스케줄링 방식을 설명하기 위한 예시도. 도 7은 본 발명의 일 실시예에 따른 인공지능 추론 서비스 방법을 설명하기 위한 예시도."}
