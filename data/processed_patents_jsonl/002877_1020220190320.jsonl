{"patent_id": "10-2022-0190320", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0107561", "출원번호": "10-2022-0190320", "발명의 명칭": "뉴럴 렌더링 기반의 3차원 가상공간 생성 방법 및 이를 위한 서버", "출원인": "씨제이올리브네트웍스 주식회사", "발명자": "서이안"}}
{"patent_id": "10-2022-0190320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "2차원 영상 기반으로 3차원 가상공간을 생성하는 방법에 있어서,(a) 상기 2차원 영상으로부터 동적 오브젝트를 제거하여 공간 이미지를 생성하는 단계;(b) 상기 2차원 영상, 또는 상기 2차원 영상을 촬영한 적어도 하나 이상의 촬영장치로부터 렌더링 데이터를 수집하는 단계; 및(c) 상기 렌더링 데이터를 참고하여 상기 공간 이미지를 3차원 가상공간으로 생성하는 단계;를 포함하는,3차원 가상공간 생성 방법."}
{"patent_id": "10-2022-0190320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 (a)단계는,상기 2차원 영상으로부터 동적 오브젝트 및 공간 이미지를 분리하는 단계;상기 2차원 영상으로부터 동적 오브젝트를 제거하는 단계; 및상기 2차원 영상으로부터 공간 이미지를 획득하는 단계;를 포함하는,3차원 가상공간 생성 방법."}
{"patent_id": "10-2022-0190320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 동적 오브젝트 및 공간 이미지를 분리하는 단계는, 상기 2차원 영상을 구성하는 연속된 프레임들 중 적어도 일부에서 형상 또는 색깔의 변화가 감지된 영역을 식별하는 단계를 포함하는 것을 특징으로 하는,3차원 가상공간 생성 방법."}
{"patent_id": "10-2022-0190320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 렌더링 데이터는, 상기 2차원 영상을 촬영한 촬영장치의 자세 또는 구동에 관한 정보를 포함하는 세팅정보; 및상기 2차원 영상이 촬영된 위치, 또는 상기 2차원 영상을 촬영한 적어도 하나 이상의 촬영장치가 존재하였던 위치에 관한 정보를 포함하는 위치정보;를 포함하는,공개특허 10-2024-0107561-3-3차원 가상공간 생성 방법."}
{"patent_id": "10-2022-0190320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 세팅정보는, 상기 촬영장치 내 렌즈의 초점거리정보, 상기 촬영장치의 자세정보, 또는 촬영시간정보 중 적어도 하나를 포함하는,3차원 가상공간 생성 방법."}
{"patent_id": "10-2022-0190320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 (b)단계는, 임의의 데이터베이스로부터 탐색 가능한 라이브러리 데이터로부터 렌더링 데이터를 더 수집하는 것을 특징으로 하는,3차원 가상공간 생성 방법."}
{"patent_id": "10-2022-0190320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 (c)단계는, 복수 개의 공간 이미지들 및 렌더링 데이터를 로드하는 단계;상기 공간 이미지들 및 렌더링 데이터를 참고하여 특정 공간정보를 구성하는 단계; 및구성된 상기 공간정보를 기초로 3차원 가상공간 구조를 생성하는 단계; 를 포함하는,3차원 가상공간 생성 방법."}
{"patent_id": "10-2022-0190320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 3차원 가상공간 구조를 생성하는 단계 이후, 상기 3차원 가상공간 구조 상에 질감을 렌더링하는 단계;를더 포함하는,3차원 가상공간 생성 방법."}
{"patent_id": "10-2022-0190320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 복수 개의 공간 이미지들은, 각각 상이한 시점에서 촬영된 2차원 영상들로부터 획득된 것을 특징으로하는,3차원 가상공간 생성 방법.공개특허 10-2024-0107561-4-청구항 10 3차원 가상공간을 생성하는 서버에 있어서, 상기 서버는 중앙처리유닛 및 메모리를 포함하며,상기 중앙처리유닛은, 상기 메모리에 저장되어 있는 3차원 가상공간 생성 방법을 실행시키기 위한 명령어들을실행시키는 것을 특징으로 하되,상기 3차원 가상공간 생성 방법은,(a) 상기 2차원 영상으로부터 동적 오브젝트를 제거하여 공간 이미지를 생성하는 단계;(b) 상기 2차원 영상, 또는 상기 2차원 영상을 촬영한 적어도 하나 이상의 촬영장치로부터 렌더링 데이터를 수집하는 단계; 및(c) 상기 렌더링 데이터를 참고하여 상기 공간 이미지를 3차원 가상공간으로 생성하는 단계;를 포함하는,서버."}
{"patent_id": "10-2022-0190320", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 뉴럴 렌더링 기반의 3차원 가상공간 생성 방법 및 이를 위한 서버에 관한 것으로, 구체적으로는 2차원 영상으로부터 동적 오브젝트를 제거한 뒤 공간 이미지를 획득하고, 위 2차원 영상, 그리고 위 2차원 영상을 촬영 하는 데에 활용된 장치들로부터 렌더링 데이터를 수집한 뒤 위 공간 이미지 및 렌더링 데이터를 활용하여 3차원 가상공간을 생성하는 방법 및 서버에 관한 것이다."}
{"patent_id": "10-2022-0190320", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 뉴럴 렌더링 기반의 3차원 가상공간 생성 방법 및 이를 위한 서버에 관한 것으로, 구체적으로는 2차 원 영상으로부터 동적 오브젝트를 제거한 뒤 공간 이미지를 획득하고, 위 2차원 영상, 그리고 위 2차원 영상을 촬영하는 데에 활용된 장치들로부터 렌더링 데이터를 수집한 뒤 위 공간 이미지 및 렌더링 데이터를 활용하여 3 차원 가상공간을 생성하는 방법 및 서버에 관한 것이다."}
{"patent_id": "10-2022-0190320", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 관련 소프트웨어 및 하드웨어의 발전으로 3차원 가상공간을 구현한 뒤 이를 활용하는 비즈니스 수요가 크 게 증가하였다. 엔터테인먼트 차원에서 메타버스 공간을 구축하거나, 공공 차원에서 교통체증과 같은 현실적인 문제 해결을 위해 3차원 가상공간을 구축해 시뮬레이션 하고, 위험한 건설 현장에서 3차원 데이터를 확보해 원 격으로 안전 점검을 수행하는 등의 활용이 적극적으로 논의되고 있거나 일부는 실제 구현되어 이루어지고 있다. 한편 관련 기술의 발달도 빠르게 뒷받침되고 있다. 메타버스, VR, AR, 디지털 트윈 등을 지원하는 다양한 3D 관 련 기술이 개발되고 공유되고 있다. 현재는 3차원 가상공간을 구현해 내기 위해 3차원 촬영장치들을 반드시 필요로 하였다. 그러나 이들 촬영장치들 은 고가의 것들이고 3차원 작업용으로만 그 용도가 제한되어 있는 등 3차원 가상공간을 구현해 내는 데에 기본 적으로 많은 비용과 노력이 소요되었다. 한편, 앞서 언급한 바와 같이 최근 늘어나는 3차원 가상공간 니즈를 맞추기 위해서는 현재보다 더 효율적인 방 식, 간이한 방식의 3차원 가상공간 생성 방법이 요구된다. 본 발명은 이러한 문제점에 착안하여 제안된 것으로, 보편적으로 촬영된 2차원 영상으로부터 3차원 가상공간을 구현해 냄으로써 효율성을 크게 높이는 것을 목적으로 한다. 한편, 본 발명은 위의 기술적 문제점을 해소시키는"}
{"patent_id": "10-2022-0190320", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "것 외에도 본 기술분야에서 통상의 지식을 가진 자가 용이하게 발명할 수 없는 추가적인 기술요소들을 제공하기 위해 발명되었다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 공개특허 제10-2007-0076285호(2007.07.24 공개)"}
{"patent_id": "10-2022-0190320", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "발명의 내용"}
{"patent_id": "10-2022-0190320", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 고가의 3차원 촬영장치 없이도 쉽게 3차원 가상공간을 구현해 낼 수 있는 환경을 제공하는 것을 목적 으로 한다. 특히 본 발명은 일반적인 2차원 영상들만으로도 3차원 가상공간 구현이 가능하게 함으로써 가상공간 생성 과정 을 효율화 하는 것을 목적으로 한다. 또한 본 발명은 2차원 영상을 촬영할 당시의 촬영장치들로부터도 관련 데이터를 확보함으로써 구현하고자 하는 3차원 가상공간이 정확성을 높이는 것을 목적으로 한다. 또한 본 발명은 구현된 3차원 가상공간을 다양한 용도로 활용이 가능하게 하는 것을 목적으로 한다. 한편, 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0190320", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명은 위와 같은 문제점을 해결하기 위한 것으로, 본 발명에 따라 2차원 영상을 기반으로 3차원 가상공간을 생성하는 방법은, (a) 상기 2차원 영상으로부터 동적 오브젝트를 제거하여 공간 이미지를 생성하는 단계; (b) 상기 2차원 영상, 또는 상기 2차원 영상을 촬영한 적어도 하나 이상의 촬영장치로부터 렌더링 데이터를 수집하 는 단계; 및 (c) 상기 렌더링 데이터를 참고하여 상기 공간 이미지를 3차원 가상공간으로 생성하는 단계;를 포 함한다. 또한, 상기 3차원 가상공간 생성 방법에 있어서 상기 (a)단계는, 상기 2차원 영상으로부터 동적 오브젝트 및 공 간 이미지를 분리하는 단계; 상기 2차원 영상으로부터 동적 오브젝트를 제거하는 단계; 및 상기 2차원 영상으로 부터 공간 이미지를 획득하는 단계; 를 포함할 수 있다. 또한, 상기 3차원 가상공간 생성 방법에 있어서 상기 동적 오브젝트 및 공간 이미지를 분리하는 단계는, 상기 2 차원 영상을 구성하는 연속된 프레임들 중 적어도 일부에서 형상 또는 색깔의 변화가 감지된 영역을 식별하는 단계를 포함할 수 있다. 또한, 상기 3차원 가상공간 생성 방법에 있어서 상기 렌더링 데이터는, 상기 2차원 영상을 촬영한 촬영장치의 자세 또는 구동에 관한 정보를 포함하는 세팅정보; 및 상기 2차원 영상이 촬영된 위치, 또는 상기 2차원 영상을 촬영한 적어도 하나 이상의 촬영장치가 존재하였던 위치에 관한 정보를 포함하는 위치정보;를 포함할 수 있다. 또한, 상기 3차원 가상공간 생성 방법에 있어서 상기 세팅정보는, 상기 촬영장치 내 렌즈의 초점거리정보, 상기 촬영장치의 자세정보, 또는 촬영시간정보 중 적어도 하나를 포함할 수 있다. 또한, 상기 3차원 가상공간 생성 방법에 있어서 상기 (b)단계는, 임의의 데이터베이스로부터 탐색 가능한 라이 브러리 데이터로부터 렌더링 데이터를 더 수집하는 것을 특징으로 할 수 있다. 또한, 상기 3차원 가상공간 생성 방법에 있어서 상기 (c)단계는, 복수 개의 공간 이미지들 및 렌더링 데이터를 로드하는 단계; 상기 공간 이미지들 및 렌더링 데이터를 참고하여 특정 공간정보를 구성하는 단계; 및 구성된 상기 공간정보를 기초로 3차원 가상공간 구조를 생성하는 단계;를 포함할 수 있다. 또한, 상기 3차원 가상공간 생성 방법에 있어서 상기 3차원 가상공간 구조를 생성하는 단계 이후, 상기 3차원 가상공간 구조 상에 질감을 렌더링하는 단계;를 더 포함할 수 있다. 또한, 상기 3차원 가상공간 생성 방법에 있어서 상기 복수 개의 공간 이미지들은, 각각 상이한 시점에서 촬영된 2차원 영상들로부터 획득된 것을 특징으로 할 수 있다. 한편, 본 발명의 또 다른 실시예에 따라 3차원 가상공간을 생성하는 서버에 있어서, 상기 서버는 중앙처리유닛 및 메모리를 포함하며, 상기 중앙처리유닛은, 상기 메모리에 저장되어 있는 3차원 가상공간 생성 방법을 실행시 키기 위한 명령어들을 실행시키는 것을 특징으로 하되, 상기 3차원 가상공간 생성 방법은, (a) 상기 2차원 영상 으로부터 동적 오브젝트를 제거하여 공간 이미지를 생성하는 단계; (b) 상기 2차원 영상, 또는 상기 2차원 영상 을 촬영한 적어도 하나 이상의 촬영장치로부터 렌더링 데이터를 수집하는 단계; 및 (c) 상기 렌더링 데이터를 참고하여 상기 공간 이미지를 3차원 가상공간으로 생성하는 단계;를 포함할 수 있다."}
{"patent_id": "10-2022-0190320", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 상대적으로 적은 비용 및 노력만으로 3차원 가상공간을 생성해 낼 수 있는 효과가 있으며, 특 히 2차원 영상만으로 3차원 가상공간을 생성할 수 있는 효과가 있다. 또한 본 발명에 따르면 2차원 영상으로부터 획득이 가능한 데이터들 뿐만 아니라, 2차원 영상을 촬영할 당시의 촬영장치들 혹은 주변장치들로부터 데이터를 수집하고 이를 3차원 가상공간 구현에 활용함으로써 최종적으로 생 성되는 3차원 가상공간의 정확도를 높일 수 있는 효과가 있다. 한편, 본 발명에 의한 효과는 이상에서 언급한 것들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 효과들 은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0190320", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 목적과 기술적 구성 및 그에 따른 작용 효과에 관한 자세한 사항은 본 발명의 명세서에 첨부된 도면 에 의거한 이하의 상세한 설명에 의해 보다 명확하게 이해될 것이다. 첨부된 도면을 참조하여 본 발명에 따른 실시예를 상세하게 설명한다. 본 명세서에서 개시되는 실시예들은 본 발명의 범위를 한정하는 것으로 해석되거나 이용되지 않아야 할 것이다. 이 분야 통상의 기술자에게 본 명세서의 실시예를 포함한 설명은 다양한 응용을 갖는다는 것이 당연하다. 따라 서, 본 발명의 상세한 설명에 기재된 임의의 실시예들은 본 발명을 보다 잘 설명하기 위한 예시적인 것이며 본 발명의 범위가 실시예들로 한정되는 것을 의도하지 않는다. 도면에 표시되고 아래에 설명되는 기능 블록들은 가능한 구현의 예들일 뿐이다. 다른 구현들에서는 상세한 설명 의 사상 및 범위를 벗어나지 않는 범위에서 다른 기능 블록들이 사용될 수 있다. 또한, 본 발명의 하나 이상의 기능 블록이 개별 블록들로 표시되지만, 본 발명의 기능 블록들 중 하나 이상은 동일 기능을 실행하는 다양한 하드웨어 및 소프트웨어 구성들의 조합일 수 있다. 또한, 어떤 구성요소들을 포함한다는 표현은 “개방형”의 표현으로서 해당 구성요소들이 존재하는 것을 단순히 지칭할 뿐이며, 추가적인 구성요소들을 배제하는 것으로 이해되어서는 안 된다. 나아가 어떤 구성요소가 다른 구성요소에 “연결되어” 있다거나 “접속되어” 있다고 언급될 때에는, 그 다른 구성요소에 직접적으로 연결 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 한다. 도 1은 본 발명에 따른 3차원 가상공간 생성 방법을 개념적으로 이해하기 위한 도면이다. 도면을 참고할 때, 본 발명은 기본적으로 2차원 영상으로부터 3차원 가상공간을 구현해 내는 방법에 관한 것이며, 이 때 2차 원 영상 내 포함되어 있는 동적 오브젝트, 예를 들어 도면과 같이 걸어다니는 인물과 같이 영상 내에서 움직이는 오브젝트를 제거함으로써 공간에 대한 이미지만 획득을 하고, 이렇게 획득한 공간 이미지를 토대로 각 종 렌더링 데이터를 참고하여 3차원 가상공간을 생성해 내는 것을 특징으로 한다. 참고로 본 상세한 설명에서 언급될 용어 2차원 영상은, 엄밀하게는 종래의 보편적인 촬영장치(카메라)에 의해 촬영된 영상데이터를 의미하는 것으로, 일반적으로 평면(또는 곡면)의 스크린 상에 상영 내지 출력시키기 위한 용도로 만들어진 영상을 의미할 수 있다. 2차원 영상은, 예를 들어 영화관의 스크린을 통해 출력되는 영화 영상, 가정 내 텔레비전 화면을 통해 출력되는 드라마 영상, PC 모니터 화면을 통해 재생되는 애니메이션 영상등을 의미할 수 있다. 반대로, 본 상세한 설명에서 언급될 용어 3차원 가상공간은, 예를 들어 VR 또는 AR 환경을 구현하기 위해 활용 되는 데이터를 의미하는 것으로, 시청자(사용자)의 움직임에 따라 시점 변경이 가능하여 시청자(사용자)로 하여 금 자신이 특정 공간 내에 존재하는 것과 같은 경험을 할 수 있게 하는 출력용 데이터를 의미할 수 있다. 3차원 가상공간은, 예를 들어 시청자(사용자)가 VR기기를 착용하였을 때 실제 공간에 있는 것과 같은 경험을 제공하기 위해 VR기기에서 출력되는 데이터일 수 있다. 또는 특정 공간 내(육면체의 닫힌 룸 등)에서 다수 개의 프로젝터 를 이용하여 임의의 가상환경을 구현해 낼 수 있는 시스템이 존재한다고 가정할 때, 위 임의의 가상환경을 구현 해 내는 데에 필요한 데이터일 수도 있다. 이처럼 본 상세한 설명에서 언급되는 3차원 가상공간이라는 용어는, 사용자가 3차원 공간으로 인지할 수 있을 정도의 환경을 구현하기 위해 요구되는 데이터, 내지 3차원 공간으로 출력시키기 위한 영상(이미지) 데이터일 수 있다. 다시 도 1을 참고할 때, 우측의 3차원 가상공간에는 바닥면, 좌우측 벽면, 천장면 등이 포함되 어 있음을 확인할 수 있으며, 이러한 가상공간은 사용자의 움직임, 특히 시선이동, 걸음이동에 따라 실시간으로 변하도록 렌더링 될 수 있다. 이상 도 1을 참고하여 본 발명에 따른 3차원 가상공간 생성 방법의 개요에 대해 알아보았다. 도 2는 본 발명에 따른 3차원 가상공간 생성 방법의 개략적인 순서를 도시한 것이다. 도면을 참고할 때, 3차원 가상공간 생성 방법은 2차원 영상으로부터 동적 오브젝트를 제거함으로써 공간 이미지를 생성하는 단계(S10), 상기 2차원 영상, 또는 상기 2차원 영상을 촬영한 촬영장치로부터 가상공간 구현을 위해 필요한 렌더링 데이터 를 수집하는 단계(S20), 그리고 상기 렌더링 데이터를 참고하여 상기 공간 이미지를 3차원 가상공간으로 생성하 는 단계(S30)를 포함할 수 있다. 각 단계들에 대해 자세히 살펴보기로 한다. S10단계는 2차원 영상에 등장하는 동적인 물체(동적 오브젝트)를 인식하고, 이러한 동적 오브젝트를 제거함으로 써 상기 2차원 영상으로부터 공간에 대한 이미지 정보만 남기는 단계로 이해될 수 있다. 2차원 영상에서 등장하 는 공간을 가상의 3차원 공간으로 변환하기 위해서는 상기 2차원 영상에서 공간에 대한 이미지 정보와 동적 오 브젝트(사람, 물건 등 카메라의 움직임과 독립적으로 움직이는 물체)의 이미지 정보를 분리하는 것이 필요하다. 이때 분리되어야 할 것으로 예상되는 오브젝트(사람, 동물 등)에 대해 대량의 이미지 데이터를 활용하여 미리 학습시켜 둔 인공지능 모델을 활용하여 빠르게 이미지 분할을 통한 마스크를 생성하여 공간 이미지 정보와 분리 할 수 있고, 사용자의 의도에 따라 제거되어야 할 영역을 지정 가능하게 함으로써 서버로 하여금 동적 오브젝트 와 공간 이미지 간 분리작업을 수행하게 할 수 있다. 도 3에는 S10단계를 보다 세분화 한 단계들이 도시되어 있다. 도면을 참고할 때, S10단계는 가장 먼저 2차원 영 상을 로드(S101)하는 단계로부터 시작될 수 있다. 본 단계는 이미지 프로세스의 대상이 되는 2차원 영상을 메모 리 상에 로드(load)하는 단계로 이해될 수 있으며, 이 단계에서는 작업자(편집자)의 컴퓨터 상에 편집용 인터페 이스를 갖춘 프로그램에 실행된 후 해당 프로그램에서 2차원 영상을 로드하는 작업자 행위들이 수반될 수 있다. 한편, 상기 2차원 영상은 비단 1개로 한정되는 것은 아니며, 필요에 따라 복수 개의 것들이 로드될 수 있음을 이해한다. S101단계 후에는 상기 2차원 영상으로부터 동적 오브젝트과 공간 이미지를 식별 및 분리하는 단계(S102)가 실행 될 수 있다. 동적 오브젝트를 식별하는 과정과 공간 이미지를 식별하는 과정은 동시에 진행될 수도 있겠으나, 그러하지 않더라도 2차원 영상으로부터 동적 오브젝트를 식별하는 과정이 종결되었을 때 자연스럽게 공간 이미 지가 나머지 부분으로서 식별되는 결과물이 얻어질 수도 있음을 이해하기로 한다. 동적 오브젝트의 식별, 또는 공간 이미지의 식별은 상기 2차원 영상을 구성하는 복수 개의 프레임들 내에서 변 화량을 감지해 냄으로써 이루어질 수 있다. 영상 내에서 움직이는 물체의 경우 연속된 프레임 별로 픽셀값의 변 화를 반드시 유발하는데, 이러한 픽셀값의 변화가 감지되는 영역을 동적 오브젝트로 식별해 낼 수 있다. 또는, 같은 맥락에서, 연속된 프레임들 중 적어도 일부에서 객체의 형상 또는 색깔의 변화가 감지된 영역이 존재한다 면 해당 영역에 대응되는 닫힌 면적을 동적 오브젝트로 추정 및 식별하도록 구현할 수도 있다. 동적 오브젝트와 반대로, 복수 개의 연속된 프레임들 내에서 변화가 감지되지 않는 영역은 공간 이미지로 식별하도록 구현할 수 있다. 한편, 동적 오브젝트를 식별하는 과정 또는 공간 이미지를 식별하는 과정에서는 학습된 인공지능 알고리즘이 활 용될 수 있다. 대량의 학습용 2차원 영상들을 대상으로 사람, 자동차, 동물 등의 동적 오브젝트를 식별하는 과정을 학습한 인공지능 알고리즘, 또는 이러한 동적 오브젝트와 임의의 공간 이미지를 식별 및 분리하는 과정을 학습한 인공지능 알고리즘을 활용함으로써 상기 S102단계가 신속하고 정확하게 수행되도록 할 수 있다. 또 다른 한편, 작업자(편집자)의 의도에 따라 동적 오브젝트가 존재하는 영역, 따라서 제거되어야 할 영역을 지정할 수 있도록 인터페이스가 제공될 수도 있다. 특정 프레임 상에서 작업자(편집자)가 제거의 대상이 되는 동적 오브젝 트, 또는 이러한 동적 오브젝트가 포함된 영역을 임의로 한정하여 지정해 줄 수 있다면, 서버는 S102단계를 실 행할 때에 리소스를 절감할 수 있으며 그만큼 작업속도 및 정확도가 향상될 수 있다. S102단계 이후에는 상기 식별된 동적 오브젝트를 제거하는 단계(S103), 그리고 공간 이미지를 획득하는 단계 (S104)가 실행될 수 있다. 상기 S103단계에서는 개별 프레임 별로 동적 오브젝트가 식별되었으므로 이에 해당되 는 영역을 이미지 프레임으로부터 제거하는 작업이 실행될 수 있으며, 이 단계에서는 마스킹 기술이 활용될 수 있다. 또한, 동적 오브젝트가 제거된 영역 또는 마스킹 영역에는 상기 동적 오브젝트가 존재하던 영역의 주변 영역이 참고되어 픽셀값들이 결정될 수 있다. 도 4에는 2차원 영상로부터 움직이는 사람, 즉 동적 오브젝트가 제거되는 과정이 도시되어 있다. 어느 2차원 영상 내에서 사람이 창문 앞 공간을 걸어가는 모습이 포함되어 있다고 가정할 때, 걸어가는 사람이 곧 동적 오브젝트로 정의될 수 있다. 이 때 2차원 영상을 구성하는 연속된 프레임들 중에는 사람이 걸 어가는 모습의 형상이 포함되어 있을 것이며, S102단계에서는 이렇게 걸어가는 사람, 그리고 배경이 되는 공간 이미지를 식별해 낼 수 있다. 또한, 걸어가는 사람을 공간 이미지만 남긴 채 제거하기 위해서는 같은 형상의 마 스킹 영역이 정의될 수 있으며, 위 마스킹 영역에 대해서는 주변 공간 이미지, 즉 창문, 바닥면 등의 형상이나 색깔이 참고되어 픽셀값들이 결정됨으로써 도 4의 (c)와 같은 공간 이미지가 생성될 수 있다. 또 한, 도 4의 (b)에는 점선으로 임의의 영역이 지정되어 있는 모습을 확인할 수 있는데, 이러한 임의의 영역 은 작업자(편집자)에 의해 지정이 가능한 것으로서 서버로 하여금 동적 오브젝트가 어디에 존재하는지, 따라서 어느 영역에 대해 마스킹 등의 이미지 프로세스를 수행하여야 하는지를 가이드 하는 영역일 수 있다. 위 임의의 영역을 지정하는 과정에서는 작업자(편집자)가 마우스 또는 터치입력 수단을 드래깅 하는 행위, 또는 하나 의 닫힌 영역을 정의하기 위한 복수 개(적어도 3개 이상)의 점들을 지정하는 행위 등이 수반될 수 있다. 이상 도 3 및 도 4를 참고하여 2차원 영상으로부터 동적 오브젝트를 제거하는 과정, 그리고 공간 이미지를 획득 하는 과정에 대해 살펴보았다. 다음으로 가상공간 구현을 위해 필요한 렌더링 데이터를 수집하는 단계(S20)에 대해 살펴보기로 한다. 렌더링 데이터는, 3차원 가상공간을 구현하는 데에 있어서 필요한 모든 종류의 데이터를 의미할 수 있으며, 데이터의 종류에 제한이 없다 할 것이다. 다만, 바람직하게는, 적어도 상기 2차원 영상을 촬영한 촬영장치의 자세 또는 구동에 관한 정보인 세팅정보, 그리고 상기 2차원 영상이 촬영된 위치, 또는 상기 2차원 영상을 촬영한 적어도 하나 이상의 촬영장치가 존재하였던 위치에 관한 정보인 위치정보가 포함될 수 있다. 2차원 영상이 촬영될 때에는 반드시 이 영상을 촬영한 촬영장치가 적어도 하나 이상 존재할 수 있는데, 본 단계 에서는 이러한 촬영장치가 촬영구동을 할 때에 기록하였거나 네트워크로 연계된 별도의 서버(데이터베이스)에 공유하였던 정보들, 예를 들어 상기 촬영장치 내 렌즈의 초점거리정보, 상기 촬영장치의 자세정보(촬영장치가 입체 공간을 정의하는 축들을 중심으로 회전함에 따라 측정되는 값들의 집합), 또는 촬영시간정보 중 적어도 하 나의 세팅정보를 수집할 수 있다. 또한, 2차원 영상이 촬영될 당시의 위치정보 역시 촬영장치 내에 기록되어 있 거나 혹은 촬영장치와 네트워크로 연계된 별도의 서버(데이터베이스)에 저장될 수 있는데, 이러한 위치정보도 수집할 수 있다. 한편, 비단 촬영장치만이 아니라 촬영을 원활하게 하기 위해 필요한 주변장치들, 혹은 촬영 당 시 스태프들이 소지하고 있던 개인장치들로부터도 렌더링 데이터가 수집될 수 있다. 도 5는 2차원 영상이 촬영되고 있는 현장을 도시한 것으로, 해당 현장에는 적어도 3대의 촬영장치들(41 내지 43), 스태프가 소지하고 있는 개인장치(예. 스마트폰; 45), 그리고 하나의 주변장치(마이크; 50)가 존재하고 있 음을 확인할 수 있다. 앞선 설명에서는 각 촬영장치들로부터 렌더링 데이터가 수집될 수 있다고 하였는데, 예를 들어 제1 촬영장치 내지 제3 촬영장치들에는 각각의 촬영장치 구동 시 기록될 수 있는 세팅정보들이 저 장될 수 있으며, 경우에 따라 위 세팅정보들은 네트워크를 통해, 또는 다른 복사수단을 통해 별개의 서버 또는 데이터베이스에 저장될 수 있다. 각각의 촬영장치들은 모두 기능이 상이할 수 있는데, 이에 따라 각 촬영장치들 로부터 공유 가능한 정보의 종류도 다를 수 있다. 예를 들어 제1 촬영장치나 제3 촬영장치는 렌즈 초점 거리정보, 자세정보 등의 세팅정보(제1 세팅정보, 제3 세팅정보)만을 기록 및 공유할 수 있고, 제2 촬영장치 는 제2 세팅정보(렌즈 초점거리정보, 자세정보 등)에 더하여 현재 제2 촬영장치가 위치한 곳의 위치정보까지도 기록 및 공유할 수 있다. 다른 한편, 상기 촬영 현장 내에는 다수의 스태프들이 존재하고 있으며, 각 각의 스태프들은 개인장치, 예를 들어 스마트폰 등을 소지할 수 있는데, 렌더링 데이터는 이러한 개인장치 들로부터도 수집 내지 공유된 것일 수 있다. 이 때 상기 개인장치에 기록되어 있는 위치정보 등은 상기 촬영장 치들의 세팅정보 또는 위치정보가 기록된 시간대와 매칭된 시간대의 것이 활용될 것이다. 또한, 상기 촬영 현장 내에는 촬영장치, 개인장치 외에 촬영을 위해 필요한 부수적인 장치들, 즉 주변장치가 더 존재할 수 있는데, 가 령 마이크와 같은 주변장치가 더 존재할 수 있다. 일 실시예에 있어 상기 마이크와 같은 주변장치는 현 재의 위치정보를 기록 및 공유할 수 있는 기능을 갖춘 것일 수 있으며, 주변장치가 작동 중일 때에 현재 위치정 보를 정해진 저장소, 또는 네트워크 상의 서버(데이터베이스)에 저장하도록 구현된 것일 수 있다. 한편, 언급된 서버(데이터베이스)는 위 장치들로부터 공유된 정보들(세팅정보, 위치정보 등)을 서로 매핑시켜 저장할 수 있다. 렌더링 데이터는 임의의 서버가 앞서 언급된 장치들(촬영장치, 개인장치, 주변장치 등)로부터 직접 수집한 것일 수도 있으나, 렌더링 데이터는 3차원 가상공간을 생성하고자 하는 시점에, 기 저장되어 있던 렌더링 데이터, 소 위 임의의 데이터베이스로부터 탐색 가능한 라이브러리 데이터로부터 수집될 수도 있다. 즉, 2차원 영상이 촬영 될 당시의 렌더링 데이터들이 임의의 서버 내에 상기 2차원 영상과 함께 라이브러리화 되어 기록 또는 공유되어 있음을 전제로, 작업자(편집자)는 특정 2차원 영상을 탐색하여 로드함과 동시에 관련된 렌더링 데이터들 역시 함께 획득할 수 있다. 참고로, 라이브러리 데이터는 상기 임의의 데이터베이스 내에 임의의 주체가 업로드 시킴으로써 저장되어 있는 것일 수도 있으며, 이 때 상기 임의의 데이터베이스에 2차원 영상 또는 이에 대응되는 렌더링 데이터를 업로드 할 시에는 기 정해진 보상이 업로드를 한 주체에게 제공될 수 있다. 예를 들어, 특정 장소에서 3분 가량의 스마 트폰 영상을 촬영한 일반 사용자가 해당 스마트폰 영상(2차원 영상)을 업로드 하였거나, 나아가 이에 대응되는 렌더링 데이터(상기 스마트폰 영상을 촬영할 당시 스마트폰 내에 기록되어 있는 스마트폰 자세정보 및 위치정보)를 업로드 하였다면 해당 사용자에게는 포인트 지급, 또는 쿠폰 지급 등의 보상을 제공할 수 있다. 2 차원 영상, 또는 렌더링 데이터를 업로드 할 수 있는 주체는 반드시 개인 사용자들로 한정되는 것은 아니며, 콘 텐츠를 제작하는 업체들, 특히 전문적인 영상 촬영용 장치들을 활용하여 콘텐츠를 촬영하는 업체들, 또는 콘텐 츠를 관리하는 업체들에 의해서도 업로드가 이루어질 수 있으며, 보상 역시 주체에 따라 상이할 수 있다. 이를 통해 다양한 주체들에 의해 생성된 2차원 영상, 렌더링 데이터가 라이브러리화 되어 저장될 수 있으며, 본 발명 에 따른 서버가 이를 3차원 가상공간을 생성하는 과정에서 참고하게 할 수 있다. 한편, 도 5를 참고로 한 설명에서는 렌더링 데이터를 생성할 수 있는 장치들의 예시로 촬영장치, 개인장치, 주 변장치만을 언급하였으나, 2차원 영상을 촬영하거나 생성하는 시점에 자동으로 관련기록을 생성 및 저장할 수 있는 기능을 갖춘 장치가 있다면 이러한 장치들 역시 렌더링 데이터를 생성할 수 있는 장치라 할 것이다. 서버 가 상기 라이브러리 데이터를 탐색할 때에는 기본적으로 위치정보, 특히 GPS정보, 주소정보를 기준으로 1차적인 탐색이 이루어질 수 있으며, 상기 위치정보를 기준으로 탐색된 2차원 영상 또는 렌더링 데이터들 중 적어도 일 부의 것들이 3차원 가상공간을 생성하는 데에 활용될 수 있다. 또 다른 한편, S20단계에서 수집될 수 있는 렌더링 데이터는 반드시 특정 장치에 의하여 자동적으로 기록된 것 은 아닐 수 있다. 특정 2차원 영상을 촬영할 때 촬영장치들의 배치가 규격이 정해져 있는 공간화면 내에서 작업 자(편집자)에 의해 입력될 수 있으며 이러한 입력 데이터 역시 렌더링 데이터의 한 종류로 활용 가능할 수 있다. 나아가, 각 촬영장치들의 하드웨어 스펙들 역시 작업자(편집자)에 의해 입력될 수 있는데, 이러한 하드웨 어 스펙들 역시 3차원 가상공간을 구현하는 데에 필요한 렌더링 데이터의 한 종류가 될 수 있다. 또한, S20단계에서 수집될 수 있는 렌더링 데이터는 2차원 영상 그 자체로부터도 획득될 수 있다. 예를 들어, 2 차원 영상이 포함하고 있는 메타데이터가 렌더링 데이터로 활용될 수 있으며, 또는 2차원 영상을 분석함으로써 촬영 당시 촬영장치의 위치, 혹은 촬영장치의 자세가 추정될 수 있다면 이렇게 영상 분석에 의해 추정된 정보들 이 렌더링 데이터로 활용될 수 있다. 이렇듯 S20단계는 2차원의 공간 이미지로부터 3차원 가상공간을 구현하는 데에 필요한 모든 종류의 데이터(렌더 링 데이터)를 수집하는 단계라 할 것이며, 이 때 수집의 대상이 되는 렌더링 데이터는 촬영장치 등에 의해 자동 적으로 기록, 생성, 및 공유되는 것, 또는 작업자(편집자)에 의해 임의로 입력되는 것 등 그 소스와 생성방식을 가리지 않는다 할 것이다. 마지막으로 렌더링 데이터를 참고하여 3차원 가상공간을 생성하는 S30단계에 대해 설명하기로 한다. 도 6에는 S30단계를 구체적으로 세분화 한 단계들이 도시되어 있는데, 이를 참고할 때 S30단계는 다수 시점의 공간 이미 지들 및 렌더링 데이터를 로드하는 단계(S301), 공간정보를 구성하는 단계(S302), 3차원 가상공간 구조를 생성 하는 단계(S303), 그리고 3차원 가상공간의 질감을 렌더링하는 단계(S304)를 포함할 수 있다. 2차원 공간 이미지와 이에 대응하는 렌더링 데이터, 특히 촬영장치들에 의해 생성된 세팅정보, 위치정보 등을 활용하여서는 3차원 가상공간을 생성할 수 있다. 하나의 공간에 대해서 다수의 시점에서 촬영한 영상 및 공간 이미지를 활용하여 공간의 입체적인 정보를 추정하고, 추정한 입체적인 정보는 공간의 구조적인 정보를 표현하 게 할 수 있다. 그리고 이에 대응하는 공간 이미지를 참고하여 공간의 질감을 표현함으로써 3차원 가상공간을 재현할 수 있다. S301단계는 3차원 가상공간을 구현하는 데에 필요한 공간 이미지들 및 렌더링 데이터를 로드(load)하는 단계로, 앞선 단계들에서 이미지 프로세스의 결과물로 얻어진 공간 이미지들, 그리고 수집된 렌더링 데이터들을 메모리 상에 로드하는 단계로 이해될 수 있다. 3차원의 공간을 구성하기 위해서는 가급적 여러 시점에서 촬영된 2차원 영상들, 더 정확하게는 이들 2차원 영상들로부터 획득된 공간 이미지들이 필요할 수 있는데, 이러한 공간 이미 지들은 상기 2차원 영상 내 복수 개의 프레임들을 이미지 프로세스하는 과정에서 얻어질 수 있다. 또 다른 한편, 다수 시점의 공간 이미지들은 비단 어느 한 2차원 영상을 기초로 한 것만은 아닐 수 있으며, 또 다른 2차 원 영상을 이미지 프로세스 한 결과물일 수 있다. 가령 서로 다른 3개의 2차원 영상들이 동일한 장소(동일한 세 트장, 또는 동일한 지역명소 등)에서 촬영된 것이라 가정할 때, 서로 다른 3개의 영상들로부터는 다수 개의 공 간 이미지들이 획득될 수 있는데, 이들 공간 이미지들, 특히 서로 다른 3개의 영상들로부터 얻어진 것들은 시점 들이 그만큼 더 다양할 것이므로 이들 공간 이미지들을 모두 활용하도록 할 수 있다. S302단계는 위 공간 이미지들, 그리고 렌더링 데이터를 참고하여 임의의 공간정보를 구성하는 단계이다. 공간정 보를 구성한다는 것의 의미는 가상의 입체공간을 하나 정의함에 있어 필요한 파라미터들을 정의하는 것을 뜻할 수 있다. 예를 들어, 하나의 입체공간을 결정하기 위해서는 적어도 공간을 구별하기 위한 벽면의 높이와 너비, 바닥면의 가로 및 세로 길이, 천장면의 가로 및 세로 길이, 나아가 벽면의 두께, 벽면에 설치되어 있는 창문틀 의 높이와 너비 등이 결정되어야 하는데, 이러한 공간 파라미터들에 대한 값들이 본 단계에서 결정될 수 있다. 또한, 입체공간 내에는 여러 종류의 부속품들이 배치되어 있을 수 있는데, 이러한 부속품들의 위치 및 형상은 현재 시점을 기준으로 가장 최근에 촬영된 2차원 영상의 공간 이미지를 기초로 결정될 수 있다. S303단계는 3차원 가상공간의 구조를 생성하는 단계로, 실질적으로는 작업자(편집자)가 인식할 수 있는 형태로 가상공간을 출력하는 단계로도 이해될 수 있다. 앞선 S302단계가 특정 입체공간을 정의하는 단계였다면 S303단 계는 이를 시각화 하여 표시하는 단계일 수 있다. 한편, 본 단계에서는 서버가 작업자(편집자)에게 편집용 인터 페이스를 제공함으로써 표시된 가상공간 중 수정이 필요한 부분을 직접 수정하게 할 수 있다. 예를 들어 바닥면 의 가로세로 길이, 벽면의 두께, 천장면의 기울기 등 파라미터 값들을 작업자가 직접 수정할 수 있게 함으로써 가상공간의 구조를 작업자 의도대로 편집하게 할 수 있다. S304단계는 앞서 생성된 가상공간의 질감을 렌더링하는 단계이다. 생성된 가상공간의 각 면들, 각 부속품들은 실제로는 저마다의 고유한 재료로 만들어진 것일 수 있는데, 가상공간에서도 이를 가능한 한 실제의 공간 또는 물건처럼 보이게 하기 위해 질감을 유사하게 렌더링할 수 있다. 질감 렌더링은 공간 이미지를 참고할 수 있다. 즉, 가상공간과 공간 이미지를 비교하여 서로 대응되는 공간 이미지 내에서의 표면을 가상공간 영역에 적용시킴 으로써 질감 렌더링을 수행할 수 있다. 이상 도면들을 참고하여 본 발명에 따른 3차원 가상공간 생성 방법에 대하여 살펴보았다. 한편, 위의 방법에 따라 생성된 3차원 가상공간은 다양하게 활용이 가능할 수 있다. 예를 들어, 영화, 드라마 등 영상 콘텐츠를 시청한 뒤에 소비자(시청자)가 추가적인 소비를 하고자 할 때 2차원 영상에 등장한 공간에 대 한 3차원 가상공간을 제공하고, 해당 가상공간 내에서 관련 상품 또는 서비스를 판매하는 등 가상공간을 활용한 메타버스 마케팅을 기획할 수 있고, 나아가 공간에 대한 3차원 데이터를 활용하여 2차 창작물(웹툰, 팬아트 등)을 제작하는 것 역시 가능할 수 있다. 또한, 공장 혹은 사람이 직접 모니터링하거나 작업하기 어려운 환경에 대해 기존의 촬영 방식으로 2차원 영상을 촬영하여 해당 장소에 대한 3차원 가상공간을 재현하여 효율적으로 모니티링할 수 있고, 3차원 가상공간 기반의 시뮬레이션 환경을 구성하여 다양한 조건과 시나리오에 대한 테스트를 수행하게 함으로써 사고에 대한 대처, 예 지 정비, 최적화 테스트 등에 활용할 수 있다.참고로 위 3차원 가상공간 생성 방법은 임의의 연산장치, 예를 들어 서버에 의하여 실행될 수 있다. 서버의 형 태는, 어느 특정 운영자가 관리하는 적어도 하나의 서버 컴퓨터일 수 있으며, 또는 타 업체에서 제공하는 클라 우드 서버의 형태, 즉 운영자가 회원가입하여 사용할 수 있는 클라우드 서버의 형태일 수도 있다. 서버가 서버 용 PC로 구현된 경우, 해당 서버는 중앙처리유닛 및 메모리를 포함할 수 있다. 이 때 중앙처리유닛은 컨트롤러 (controller), 마이크로 컨트롤러(microcontroller), 마이크로 프로세서(microprocessor), 마이크로 컴퓨터 (microcomputer) 등으로도 불릴 수 있다. 또한 중앙처리유닛은 하드웨어(hardware) 또는 펌웨어(firmware), 소 프트웨어, 또는 이들의 결합에 의해 구현될 수 있는데, 하드웨어를 이용하여 구현하는 경우에는 ASIC(application specific integrated circuit) 또는 DSP(digital signal processor), DSPD(digital signal processing device), PLD(programmable logic device), FPGA(field programmable gate array) 등으로, 펌웨어 나 소프트웨어를 이용하여 구현하는 경우에는 위와 같은 기능 또는 동작들을 수행하는 모듈, 절차 또는 함수 등 을 포함하도록 펌웨어나 소프트웨어가 구성될 수 있다. 또한, 메모리는 ROM(Read Only Memory), RAM(Random Access Memory), EPROM(Erasable Programmable Read Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), 플래쉬(flash) 메모리, SRAM(Static RAM), HDD(Hard Disk Drive), SSD(Solid State Drive) 등으로 구현될 수 있다. 이상 본 발명에 따른 3차원 가상공간 생성 방법 및 이를 위한 서버에 대해 살펴보았다. 한편, 본 발명은 상술한 특정의 실시예 및 응용예에 한정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해"}
{"patent_id": "10-2022-0190320", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "발명이 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사상이나 전망으로부터 구별되어 이해되어서는 안 될 것이다."}
{"patent_id": "10-2022-0190320", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 상세한 설명에서 제안하는 3차원 가상공간 생성 방법을 개념적으로 이해하기 위해 도시한 것이다. 도 2는 본 발명에 따른 3차원 가상공간 생성 방법을 순서에 따라 도시한 것이다. 도 3은 2차원 영상으로부터 공간 이미지를 획득하는 단계를 보다 구체적으로 나누어 도시한 것이다. 도 4는 2차원 영상으로부터 동적 오브젝트를 제거하는 과정을 설명하기 위해 도시한 것이다. 도 5는 렌더링 데이터를 수집하는 실시예를 설명하기 위한 도면이다. 도 6은 렌더링 데이터를 참고하여 3차원 가상공간을 생성하는 단계를 보다 구체적으로 나누어 도시한 것이다."}
