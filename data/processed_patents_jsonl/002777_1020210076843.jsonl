{"patent_id": "10-2021-0076843", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0157337", "출원번호": "10-2021-0076843", "발명의 명칭": "추천 시스템 최적화 방법, 장치, 기기 및 컴퓨터 기록 매체", "출원인": "베이징 바이두 넷컴 사이언스 앤 테크놀로지 코.,", "발명자": "류, 리항"}}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "추천 시스템(recommendation system)의 최적화 방법으로서,상기 추천 시스템을 에이전트(agent)로 하고 사용자(user)를 환경(environment)으로 하며, 상기 추천 시스템의매번 추천 내용을 상기 에이전트의 동작(action)으로 하며, 상기 사용자의 장기적인 행동 수익(long-termbehavior benefit)을 상기 환경의 보상(reward)으로 하는 단계； 및강화 학습(reinforcement learning)의 방식을 사용하여, 상기 추천 시스템 중 최적화될 파라미터를 최적화하여,상기 환경의 보상을 극대화하는 단계; 를 포함하는 것을 특징으로 하는,추천 시스템의 최적화 방법."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 최적화될 파라미터는 상기 추천 시스템 중 적어도 일부 모델 파라미터(model parameter) 및/또는 하이퍼파라미터(hyper parameter)를 포함하는 것을 특징으로 하는,추천 시스템의 최적화 방법."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 강화 학습의 방식은 진화 학습(evolutional learning)을 포함하며；상기 추천 시스템 중 최적화될 파라미터를 최적화하는 단계는， 일 라운드 이상의 반복 업데이트를 포함하며； 그 중에서 각 라운드의 반복 업데이트는：각각의 사용자에 대해 한 세트의 랜덤 방해를 각각 생성하는 단계；상기 한 세트의 랜덤 방해를 상기 최적화될 파라미터에 증가하는 단계- 상기 증가된 랜덤 방해와 상기 최적화될파라미터의 차원은 같음-；랜덤 방해 증가 후의 추천 시스템에 기반하여 사용자에게 내용을 추천한 후 각각의 사용자의 장기적인 행동 수익을 통계하는 단계；통계하여 얻은 각각의 사용자의 장기적인 행동 수익에 따라, 각각의 세트의 랜덤 방해에서 상기 최적화될 파라미터의 진화 방향을 결정하고, 결정된 진화 방향에 기반하여 상기 최적화될 파라미터를 업데이트하는 단계;를 포함하는 것을 특징으로 하는,추천 시스템의 최적화 방법."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 반복 업데이트는 순환하여 수행하거나； 또는，상기 추천 시스템이 수렴 조건을 만족할 때까지 상기 반복 업데이트를 수행하거나； 또는，상기 반복 라운드 수가 미리 설정된 라운드 수 임계 값에 도달할 때까지 반복 업데이트를 수행하는 것을 특징으공개특허 10-2021-0157337-3-로 하는,추천 시스템의 최적화 방법."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서， 상기 추천 시스템은 감독 학습(supervised learning) 방식을 사용하거나 또는 감독 학습과 인공 규칙(artificial rule)을 결합하는 방식을 사용하여 미리 얻은 추천 시스템인 것을 특징으로 하는,추천 시스템의 최적화 방법."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제5항 중 어느 한 항에 있어서,상기 사용자의 장기적인 행동 수익은：사용자가 상기 추천 시스템의 소속 서비스를 사용하는 총 시간, 사용자의 상기 추천 시스템에 의해 추천된 내용에 대한 총 클릭수 또는 총 클릭률, 사용자의 상기 추천 시스템에서의 프로모션 전환율, 또는 상기 추천 시스템의 소속 서비스의 사용자 유지율 을 포함하는 것을 특징으로 하는,추천 시스템의 최적화 방법."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "추천 시스템(recommendation system)의 최적화 장치로서,상기 추천 시스템을 에이전트(agent)로 하고 사용자(user)를 환경(environment)으로 하며, 상기 추천 시스템의매번 추천 내용을 상기 에이전트의 동작(action)으로 하며, 상기 사용자의 장기적인 행동 수익(long-termbehavior benefit)을 상기 환경의 보상(reward)으로 하는 모델링 유닛； 및강화 학습(reinforcement learning)의 방식을 사용하여, 상기 추천 시스템 중 최적화될 파라미터를 최적화하여상기 환경의 보상을 극대화하는 최적화 유닛;을 포함하는 것을 특징으로 하는,추천 시스템의 최적화 장치."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 최적화될 파라미터는 상기 추천 시스템 중 적어도 일부 모델 파라미터(model parameter) 및/또는 하이퍼파라미터(hyper parameter)를 포함하는 것을 특징으로 하는,추천 시스템의 최적화 장치."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서, 상기 강화 학습의 방식은 진화 학습(evolutional learning)을 포함하며；상기 최적화 유닛은：각각의 사용자에 대해 한 세트의 랜덤 방해를 각각 생성하는 방해 생성 서브 유닛；상기 한 세트의 랜덤 방해를 상기 최적화될 파라미터에 증가하는 방해 증가 서브 유닛- 상기 증가된 랜덤 방해와 상기 최적화될 파라미터의 차원은 같음-；공개특허 10-2021-0157337-4-랜덤 방해 증가 후의 추천 시스템에 기반하여 사용자에게 내용을 추천한 후 각각의 사용자의 장기적인 행동 수익을 통계하는 수익 통계 서브 유닛； 및상기 수익 통계 서브 유닛에 의해 통계하여 얻은 각각의 사용자의 장기적인 행동 수익에 따라, 각각의 세트의랜덤 방해에서 상기 최적화될 파라미터의 진화 방향을 결정하고, 결정된 진화 방향에 기반하여 상기 최적화될파라미터를 업데이트하는 파라미터 진화 서브 유닛;을 포함하며, 일 라운드 이상의 반복 업데이트를 수행하는 것을 특징으로 하는,추천 시스템의 최적화 장치."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서,상기 추천 시스템은 감독 학습(supervised learning) 방식 또는 감독 학습과 인공 규칙(artificial rule)을 결합하는 방식을 사용하여 미리 얻은 추천 시스템인 것을 특징으로 하는,추천 시스템의 최적화 장치."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제7항 내지 제10항 중 어느 한 항에 있어서,상기 사용자의 장기적인 행동 수익은：사용자가 상기 추천 시스템의 소속 서비스의 총 시간, 사용자의 상기 추천 시스템에 의해 추천된 내용에 대한 총 클릭수 또는 총 클릭률, 사용자의 상기 추천 시스템에서의 프로모션 전환율, 또는 상기 추천 시스템의 소속 서비스의 사용자 유지율을 포함하는 것을 특징으로 하는,추천 시스템의 최적화 장치."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며,상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 명령이 상기적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제6항 중 어느 한항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는,전자 기기."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "컴퓨터 명령이 기억되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서,상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제6항 중 어느 한 항에 기재된 방법을 수행하도록 하는것을 특징으로 하는,기록 매체."}
{"patent_id": "10-2021-0076843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "비 일시적 컴퓨터 판독 가능 기록 매체에 기억되어 있는 컴퓨터 프로그램에 있어서,상기 컴퓨터 프로그램은 상기 컴퓨터로 하여금 제1항 내지 제6항 중 어느 한 항에 기재된 방법을 수행하도록 하공개특허 10-2021-0157337-5-는 것을 특징으로 하는,컴퓨터 프로그램."}
{"patent_id": "10-2021-0076843", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 출원은 본 출원 추천 시스템의 최적화 방법, 장치, 기기 및 컴퓨터 기록 매체를 개시하였으며 특히, 인공 지 능에서의 딥 러닝 및 지능 검색"}
{"patent_id": "10-2021-0076843", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것이다. 구체적인 구현 방법은： 상기 추천 시스템을 에이전트 로 하고 사용자를 환경으로 하며, 상기 추천 시스템 매번 추천 내용을 상기 에이전트의 동작으로 하고 사용자의 장기적인 행동 수익을 상기 환경의 보상으로 하며； 강화 학습의 방식을 사용하여 상기 추천 시스템 중 최적화될 파라미터를 최적화하여 상기 환경의 보상을 극대화한다. 본 출원은 사용자 장기적인 수익 행동을 효과적으로 최 적화한다. 대 표 도 - 도2"}
{"patent_id": "10-2021-0076843", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2021-0157337 CPC특허분류 G06N 20/00 (2021.08) 발명자 왕, 판 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 넘버 10, 바이두 캠퍼스 2층허, 징저우 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 넘버 10, 바이두 캠퍼스 2층명 세 서 청구범위 청구항 1 추천 시스템(recommendation system)의 최적화 방법으로서, 상기 추천 시스템을 에이전트(agent)로 하고 사용자(user)를 환경(environment)으로 하며, 상기 추천 시스템의 매번 추천 내용을 상기 에이전트의 동작(action)으로 하며, 상기 사용자의 장기적인 행동 수익(long-term behavior benefit)을 상기 환경의 보상(reward)으로 하는 단계； 및 강화 학습(reinforcement learning)의 방식을 사용하여, 상기 추천 시스템 중 최적화될 파라미터를 최적화하여, 상기 환경의 보상을 극대화하는 단계; 를 포함하는 것을 특징으로 하는, 추천 시스템의 최적화 방법. 청구항 2 제1항에 있어서, 상기 최적화될 파라미터는 상기 추천 시스템 중 적어도 일부 모델 파라미터(model parameter) 및/또는 하이퍼 파라미터(hyper parameter)를 포함하는 것을 특징으로 하는, 추천 시스템의 최적화 방법. 청구항 3 제1항에 있어서, 상기 강화 학습의 방식은 진화 학습(evolutional learning)을 포함하며； 상기 추천 시스템 중 최적화될 파라미터를 최적화하는 단계는， 일 라운드 이상의 반복 업데이트를 포함하며； 그 중에서 각 라운드의 반복 업데이트는： 각각의 사용자에 대해 한 세트의 랜덤 방해를 각각 생성하는 단계； 상기 한 세트의 랜덤 방해를 상기 최적화될 파라미터에 증가하는 단계- 상기 증가된 랜덤 방해와 상기 최적화될 파라미터의 차원은 같음-； 랜덤 방해 증가 후의 추천 시스템에 기반하여 사용자에게 내용을 추천한 후 각각의 사용자의 장기적인 행동 수 익을 통계하는 단계； 통계하여 얻은 각각의 사용자의 장기적인 행동 수익에 따라, 각각의 세트의 랜덤 방해에서 상기 최적화될 파라 미터의 진화 방향을 결정하고, 결정된 진화 방향에 기반하여 상기 최적화될 파라미터를 업데이트하는 단계; 를 포함하는 것을 특징으로 하는, 추천 시스템의 최적화 방법. 청구항 4 제3항에 있어서, 상기 반복 업데이트는 순환하여 수행하거나； 또는， 상기 추천 시스템이 수렴 조건을 만족할 때까지 상기 반복 업데이트를 수행하거나； 또는， 상기 반복 라운드 수가 미리 설정된 라운드 수 임계 값에 도달할 때까지 반복 업데이트를 수행하는 것을 특징으로 하는, 추천 시스템의 최적화 방법. 청구항 5 제1항에 있어서， 상기 추천 시스템은 감독 학습(supervised learning) 방식을 사용하거나 또는 감독 학습과 인공 규칙 (artificial rule)을 결합하는 방식을 사용하여 미리 얻은 추천 시스템인 것을 특징으로 하는, 추천 시스템의 최적화 방법. 청구항 6 제1항 내지 제5항 중 어느 한 항에 있어서, 상기 사용자의 장기적인 행동 수익은： 사용자가 상기 추천 시스템의 소속 서비스를 사용하는 총 시간, 사용자의 상기 추천 시스템에 의해 추천된 내용에 대한 총 클릭수 또는 총 클릭률, 사용자의 상기 추천 시스템 에서의 프로모션 전환율, 또는 상기 추천 시스템의 소속 서비스의 사용자 유지율 을 포함하는 것을 특징으로 하는, 추천 시스템의 최적화 방법. 청구항 7 추천 시스템(recommendation system)의 최적화 장치로서, 상기 추천 시스템을 에이전트(agent)로 하고 사용자(user)를 환경(environment)으로 하며, 상기 추천 시스템의 매번 추천 내용을 상기 에이전트의 동작(action)으로 하며, 상기 사용자의 장기적인 행동 수익(long-term behavior benefit)을 상기 환경의 보상(reward)으로 하는 모델링 유닛； 및 강화 학습(reinforcement learning)의 방식을 사용하여, 상기 추천 시스템 중 최적화될 파라미터를 최적화하여 상기 환경의 보상을 극대화하는 최적화 유닛; 을 포함하는 것을 특징으로 하는, 추천 시스템의 최적화 장치. 청구항 8 제7항에 있어서, 상기 최적화될 파라미터는 상기 추천 시스템 중 적어도 일부 모델 파라미터(model parameter) 및/또는 하이퍼 파라미터(hyper parameter)를 포함하는 것을 특징으로 하는, 추천 시스템의 최적화 장치. 청구항 9 제7항에 있어서, 상기 강화 학습의 방식은 진화 학습(evolutional learning)을 포함하며； 상기 최적화 유닛은： 각각의 사용자에 대해 한 세트의 랜덤 방해를 각각 생성하는 방해 생성 서브 유닛； 상기 한 세트의 랜덤 방해를 상기 최적화될 파라미터에 증가하는 방해 증가 서브 유닛- 상기 증가된 랜덤 방해 와 상기 최적화될 파라미터의 차원은 같음-；랜덤 방해 증가 후의 추천 시스템에 기반하여 사용자에게 내용을 추천한 후 각각의 사용자의 장기적인 행동 수 익을 통계하는 수익 통계 서브 유닛； 및 상기 수익 통계 서브 유닛에 의해 통계하여 얻은 각각의 사용자의 장기적인 행동 수익에 따라, 각각의 세트의 랜덤 방해에서 상기 최적화될 파라미터의 진화 방향을 결정하고, 결정된 진화 방향에 기반하여 상기 최적화될 파라미터를 업데이트하는 파라미터 진화 서브 유닛; 을 포함하며, 일 라운드 이상의 반복 업데이트를 수행하는 것을 특징으로 하는, 추천 시스템의 최적화 장치. 청구항 10 제7항에 있어서, 상기 추천 시스템은 감독 학습(supervised learning) 방식 또는 감독 학습과 인공 규칙(artificial rule)을 결 합하는 방식을 사용하여 미리 얻은 추천 시스템인 것을 특징으로 하는, 추천 시스템의 최적화 장치. 청구항 11 제7항 내지 제10항 중 어느 한 항에 있어서, 상기 사용자의 장기적인 행동 수익은： 사용자가 상기 추천 시스템의 소속 서비스의 총 시간, 사용자의 상기 추천 시스템에 의해 추천된 내용에 대한 총 클릭수 또는 총 클릭률, 사용자의 상기 추천 시스템에서의 프로모션 전환율, 또는 상기 추천 시스템의 소속 서비스의 사용자 유지율 을 포함하는 것을 특징으로 하는, 추천 시스템의 최적화 장치. 청구항 12 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제6항 중 어느 한 항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는, 전자 기기. 청구항 13 컴퓨터 명령이 기억되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제6항 중 어느 한 항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는, 기록 매체. 청구항 14 비 일시적 컴퓨터 판독 가능 기록 매체에 기억되어 있는 컴퓨터 프로그램에 있어서, 상기 컴퓨터 프로그램은 상기 컴퓨터로 하여금 제1항 내지 제6항 중 어느 한 항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는, 컴퓨터 프로그램. 발명의 설명 기 술 분 야"}
{"patent_id": "10-2021-0076843", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 3, "content": "본 출원은 컴퓨터 응용 기술분야에 관한 것이고 특히 인공 지능의 딥 러닝(artificial intelligent deep learning) 및 지능 검색(intelligent searching) 기술 분야에 관한 것이다."}
{"patent_id": "10-2021-0076843", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "추천 시스템(recommendation system)은 검색 플랫폼, 정보 플랫폼, 쇼핑 플랫폼, 음악 플랫폼 등 다양한 분야에 서 널리 사용되고 있다. 플랫폼 서비스의 경우 선호도에 맞는 내용을 사용자에게 추천하는 방법은 사용자 체험 과 사용자 유지에 중요하다. 사용자 또는 서비스 제공 업체 (예: 플랫폼)에 관계없이 사용자가 서비스를 사용하는 총 시간, 플랫폼에서 사용 자의 총 클릭수, 사용자의 프로모션 전환율과 같은 사용자의 장기적인 행동 수익은 매우 중요하나 현재 추천 시 스템은 사용자의 장기적인 행동 수익을 효과적으로 최적화할 수 없다."}
{"patent_id": "10-2021-0076843", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "이점을 감안하여， 본 출원은 사용자 장기적인 행동 수익에 대한 추천 시스템의 최적화 방법, 장치, 기기 및 컴 퓨터 기록 매체를 제공한다."}
{"patent_id": "10-2021-0076843", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "제1 양태에 있어서， 본 출원은 추천 시스템의 최적화 방법을 제공하며， 상기 방법은： 상기 추천 시스템을 에이전트로, 사용자를 환경으로, 상기 추천 시스템의 매번 추천 내용을 에이전트의 동작으 로, 사용자의 장기 행동 수익을 상기 환경에 대한 보상으로 취한다； 강화 학습의 방식을 사용하여 상기 추천 시스템 중 최적화될 파라미터를 최적화하여 상기 환경의 보상을 극대화 하는 단계를 포함한다. 제2 양태에 있어서, 본 출원은 추천 시스템의 최적화 장치를 제공하며 상기 장치는： 상기 추천 시스템을 에이전트로 하고 사용자를 환경으로 하며 상기 추천 시스템 매번 추천 내용을 상기 에이전 트의 동작으로 하며 사용자의 장기적인 행동 수익을 상기 환경의 보상으로 하는 모델링 유닛； 강화 학습의 방식을 사용하여 상기 추천 시스템 중 최적화될 파라미터를 최적화하여 상기 환경의 보상을 극대화 하는 최적화 유닛을 포함한다. 제3 양태에 있어서， 본 출원은 전자 기기를 제공하며， 상기 전자 기기는： 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서로 하여금 상술한 방법을 수행하도록 한 다. 제4 양태에 있어서， 본 출원은 컴퓨터 명령이 기억되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체를 더 제공 하며 상기 컴퓨터 명령은 상기 컴퓨터로 하여금 상술한 방법을 수행하도록 한다."}
{"patent_id": "10-2021-0076843", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상 기술 방안으로부터 알 수 있는 바, 본 출원은 강화 학습의 방식을 통해 사용자의 장기적인 수익 행동에 대 해 효과적으로 최적화할 수 있다. 상술한 선택 가능 방식이 구비한 기타 효과는 아래에서 구체적인 실시예를 결합하여 설명한다."}
{"patent_id": "10-2021-0076843", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 출원의 시범적인 실시예를 설명하는 바, 본 출원에 대한 이해를 돕기 위해 여기에는 본 출원 실시예의 다양한 세부 사항이 포함되며, 이러한 세부 사항을 단지 시범적인 것으로 간주해야 할 것이다. 따라서, 당업자는 본 발명의 범위 및 정신을 벗어나지 않는 전제 하에서, 여기서 설명되는 실시예에 대 해 다양한 변경 및 수정을 수행할 수 있음을 인식해야 한다. 마찬가지로, 명확성 및 간결성을 위하여 이하의 기 술에서는 잘 알려진 기능 및 구조의 기술을 생략하였다. 전통적인 추천 시스템은 일반적으로 감독 알고리즘(supervised algorism)을 사용하거나 감독 알고리즘과 인공 규칙(artificial rule)의 조합을 사용한다. 감독 알고리즘으로 훈련 후 추천 시스템은 사용자의 현재 수익을 잘 예측하고 극대화할 수 있다. 예를 들어, 사 용자가 좋아하는 내용을 추천하여 현재 추천 내용에 대한 사용자의 좋아요률( ), 클릭률, 브라우징 시간 등을 개선할 수 있다. 그러나 감독 알고리즘은 사용자가 좋아하는 내용을 탐욕스럽게 추천하므로 추천 내용이 단일하여 사용자는 지루해지기 쉬우며 사용자의 장기적인 행동 수익을 향상시키는데 유리하지 않다. 인공 규칙은 감독 알고리즘이 직접 최적화할 수 없는 목표 또는 알고리즘의 결함을 보완하는 것을 커버하기 위 해 사용되며, 인공 규칙은 더 제어하기 쉽다. 예를 들면 인공 규칙에는 서로 다른 유형의 내용 비례가 추가되어 추천 내용의 다양성을 보장한다. 동시에 인공 규칙은 훈련이 필요하지 않으며, 더 빨리 시작할 수 있다. 또한 추천 장면에 일반적으로 백만 천만 급의 내용이 있으며 인공 규칙이 더 빠르게 실행된다. 그러나 인공 규칙은 일방적이고 비효율적이다. 한편으로 추천 시스템의 업그레이드에 따라 인공 규칙은 점점 더 책임감 있고 분석하 기 어려워질 것이다； 반면， 인공 규칙은 온라인의 A/B 테스트에 지속적으로 의존해 하이퍼 파라미터를 선택해 야 하므로 여기에 필요한 대가는 엄청나며 사용자 또는 시스템 업데이트에 따라 만료될 수 있으며, 또한 자동으 로 업데이트 할 수 없다. 이에 감안하여, 본 출원의 핵심 사상은 강화 학습의 방식을 이용하여 추천 시스템의 파라미터를 최적화하여 사 용자 장기적인 행동 수익을 극대화하는 것이다. 도 1은 일반적인 강화 학습 시스템의 개략도이다. 도 1에 나타낸 바와 같이， 일반적으로 강화 학습 시스템에는 에이전트（Agent, ) 및 환경（Environment）이 포함된다. 에이전트는 환경과의 상호 작용 및 피드백을 통해 그 정책을 지속적으로 학습 및 최적화한다. 구체적으로 에이전트는 환경의 상태（state）를 관찰하고 획득 하며, 특정 정책에 따라 현재 환경의 상태에 대해 취해야 할 동작（action）을 결정한다. 이러한 행동은 환경에 작용하여 환경의 상태를 개변하고 동시에 에이전트에게 보상( , reward）이라고도 하는 하나의 피드백을 생 성한다. 에이전트는 획득된 보상에 따라 이전 동작이 올바른지, 정책을 조정해야 하는지 여부를 판단한 다음 그 정책을 업데이트한다. 반복적으로 동작을 결정하고 보상을 받음으로써 에이전트는 업데이트 정책을 지속적으로 업데이트할 수 있으며, 궁극적인 목표는 획득된 보상이 축적 극대화되도록 하는 정책을 학습할 수 있도록 하는 것이다. 도 2는 본 출원 실시예에서 제공하는 추천 시스템의 최적화 방법 순서도이다. 상기 방법의 수행 주체는 추천 시 스템의 최적화 장치일 수 있으며， 상기 장치는 서버 측 유지 애플리케이션일 수 있으며 또는 서버 측에 위치한애플리케이션의 플러그인 또는 소프트웨어 개발 키트（Software Development Kit，SDK） 등 기능 유닛일 수도 있으며, 또는 강력한 컴퓨팅 기능을 갖춘 단말기에 위치할 수도 있으며, 본 발명 실시예에서는 이에 대해 특별 히 제한하지 않는다. 도 2에 나타낸 바와 같이, 상기 방법은 다음 단계를 포함할 수 있다： 단계 201에서 추천 시스템 장면에 대해 강화 학습 모델링(reinforcement learning modeling)을 수행한다. 구체적으로 추천 시스템을 에이전트（Agent）로 하고, 사용자를 환경（Environment）으로 하며, 추천 시스템 매 번 추천 내용을 에이전트의 동작（Action）으로 하고, 사용자의 장기적인 행동 수익을 환경의 보상（Reward）으 로 하는 것을 포함한다. 본 출원 실시예에서， 사용자의 장기적인 행동 수익이 구현하는 것은 사용자의 장기적인 선호이며 다음 몇 가지 를 포함할 수 있지만 이에 제한되지 않는다： 첫 번째 종류： 사용자가 추천 시스템이 속한 서비스를 사용한 총 시간이다. 예를 들면 동영상 서비스 플랫폼에 대하여 추천 시스템을 통해 사용자에게 동영상을 추천하면 사용자가 동영상 서비스를 사용한 총 시간은 사용자 의 장기적인 행동 수익을 구현한다. 두 번째 종류： 사용자가 추천 시스템에 의해 추천한 내용에 대한 총 클릭수 또는 총 클릭률이다. 예를 들면 정 보류 서비스 플랫폼의 경우， 추천 시스템을 통해 사용자에게 추천 상담을 추천하면 사용자의 추천된 상담에 대 한 클릭수 및 클릭률은 사용자의 장기적인 행동 수익을 구현할 수 있다. 세 번째 종류： 사용자의 추천 시스템에서의 프로모션 전환율이다. 예를 들면 동영상 서비스 플랫폼에 대하여 추천 시스템을 통해 사용자에게 동영상을 추천하고 동영상 추천 과정에서 일부 프로모션 내용（예를 들면 광고 등）이 삽입된다. 상기 프로모션 내용을 클릭하면 해당 프로모션으로 이동하여 프로모션 전환이 발생한다. 발생 된 이 프로모션 전환율에 기반하여 사용자의 장기적인 행동 수익을 상당 부분 반영한다. 네 번째 종류： 추천 시스템이 속한 서비스의 사용자 유지율이다. 예를 들면， 동영상 서비스 플랫폼의 경우， 지난 기간 동안 상기 동영상 서비스 플랫폼을 사용한 사용자가 1만 명이고 현재 기간 동안 상기 동영상 서비스 플랫폼을 사용했던 사용자 1만 명 중 5천 명만 남았기 때문에 유지율이 0.5이다. 그중 기간의 길이는 일 수준, 주 수준, 월 수준 등이 될 수 있다. 사용자 유지율은 사용자의 장기적인 행동 수익을 반영할 수도 있다. 본 출원은 최적화하려는 추천 시스템이 사용한 모델 유형에 국한되지 않으며， 대부분의 추천 시스템은 모두 감 독 학습 방식으로 훈련하여 얻은 랭킹 모델을 사용하며， 랭킹 모델의 각각의 후보 내용에 대한 랭킹 점수에 기 반하여 사용자에게 추천하는 내용을 결정한다. 또는 감독 학습 방식으로 훈련하고 인공 규칙을 결합한 랭킹 모 델일 수도 있다. 이러한 방식으로 훈련하여 얻은 추천 시스템에 기반하여， 더한층 강화 학습을 이용하여 최적 화하여， 추천 시스템으로 하여금 콜드 스타트(cold start) 비용을 부담할 필요가 없도록 하고 사용자 경험에 대한 피해도 감소한다. 단계 에서 강화 학습의 방식으로 추천 시스템 중 최적화될 파라미터를 최적화하여 환경의 보상을 극대화한 다. 본 출원 실시예에서 추천 시스템의 모든 모델 파라미터 및/또는 하이퍼 파라미터가 최적화될 수 있다. 추천 시 스템은 리콜 레이어, 랭킹 레이어 등과 같은 다층 레이어 구조를 포함하기 때문에 더 세분화된 레이어 등급일 수도 있다. 일부 레이어의 파라미터는 추천 시스템의 결과에 비교적 큰 영향을 미칠 수 있으며, 일부는 덜 영향 을 미친다. 또는 중요도에 따라 그중 일부 모델 파라미터 및/또는 하이퍼 파라미터를 최적화할 수도 있다. 예를 들면 추천 시스템의 결과에 비교적 큰 영향을 미치는 일부 모델 파라미터 및/또는 하이퍼 파라미터를 선택하여 최적화하여 최적화 효과를 보장하는 것을 전제로 최적화 과정의 계산량을 줄일 수 있다. 본 출원에서 강화 학습의 방식은 진화 학습, DQN（Deep Q-learning， 강화 학습）, 정책 경사（Policy Gradient, 策略梯度） 등을 사용할 수 있으나 이에 제한되지 않는다. 그중 진화 학습은 블랙박스 최적화（Black Box Optimizaion, )의 기술이고, 강화 학습의 하나의 분기 에 속한다. 여기서 진화 학습을 예로 본 단계의 구현 방식을 상세하게 설명한다. 진화 학습 과정에서 반복 업데 이트의 하나 이상의 라운드가 주기적으로 수행된다. 즉 복수 라운드의 반복 업데이트가 수행된다. 도 3에 나타 낸 바와 같이， 각 라운드 반복 업데이트에는 다음 단계가 포함될 수 있다： 단계 에서 각각의 사용자에 대해 한 세트의 랜덤 방해를 각각 생성한다. m개 사용자가 존재할 경우，m세트 랜덤 방해가 생성되고， 각 세트 랜덤 방해는 하나의 사용자에 대응된다. 제 개 사용자의 랜덤 방해를 로 표시하고, 한 세트의 벡터이다. 그중 랜덤 방해는 예를 들면 가우스 노이즈 등 일 수 있다. 단계 에서 생성된 한 세트의 랜덤 방해는 최적화될 파라미터에 추가된다. 추천 시스템에서 최적화될 파라미터 집합을 로 표시하면 파라미터 집합 중 각각의 파라미터에는 하나의 방해 가 증가되고, 증가된 랜덤 방해와 최적화될 파라미터의 차원은 같다. 예를 들면 최적화될 어느 파라미터가 n차 원이면 증가된 랜덤 방해도 n차원이다. 단계 에서 랜덤 방해 증가 후의 추천 시스템에 기반하여 사용자에게 내용을 추천한 후 각각의 사용자의 장 기적인 행동 수익을 통계한다. 상술한 반복 업데이트는 실제로 온라인 “시행 오류” 과정이다. 랜덤 방해 증가 후의 추천 시스템에 기반하여 온라인에서 사용자에게 내용을 추천하고 사용자 상태에서 생성된 보상을 기반으로 파라미터를 최적화하는 방법 을 결정한다. 따라서， 랜덤 방해 증가 후， 예를 들면 m개 사용자의 장기적인 행동 수익 및 상기 m개 사용자에 대해 생성된 랜덤 방해 등 각각의 사용자가 수집된다. 제 개 사용자의 장기적인 행동 수익을 로 표시한다. 단계 에서는 통계를 통해 획득한 각각의 사용자의 장기적인 행동 수익에 따라 각각의 세트의 랜덤 방해에 서 최적화될 파라미터의 진화 방향을 결정하고， 결정된 진화 방향에 따라 최적화될 파라미터를 업데이트한다. 다음 단계 를 수행하는 것으로 이동하여 순환을 반복한다. 이 단계는 실제로 사용자의 장기적인 혜택을 극대화하기 위해 다양한 방해 방향을 시도하여 최적의 진화 방향을 결정하는 것이다. 예를 들면 다음 공식에 따라 최적화될 파라미터를 업데이트한다："}
{"patent_id": "10-2021-0076843", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 는 업데이트 단계 크기를 나타낸다. 상기 단계 값은 예를 들면 실험값 또는 경험값과 같은 설정값을 사용할 수 있다. 상술한 반복 업데이트는 순환 수행될 수 있으며, 상기 순환 수행은 지속적인 과정일 수 있다. 즉, 추천 시스템 이 온라인에서 사용자에게 내용을 추천함에 따라 반복적인 최적화가 지속적으로 수행된다. 그러나 지속적인 반 복 최적화 후에는 파라미터가 실제로 점진적으로 최적화되고, 최적화될 수 있는 정도는 이미 낮다. 다만 일부 사용자 상태, 사회적 상태 등이 변경되기 때문에 파라미터를 지속적으로 최적화해야 하므로 진화 학습의 학습률 을 낮출 수 있다. 그중 학습률의 특정 값은 실험값 또는 경험값을 사용할 수 있다. 또한 반복 업데이트가 일정 정도에 도달하면 반복 업데이트를 중지하고 다음 번에 반복 업데이트를 시작할 때 일정 정도에 도달할 때까지 반복 업데이트를 다시 수행할 수 있다. 그 중에서 상기 일정 정도는 추천 시스템이 수렴 조건을 만족시키는 것일 수 있으며 예를 들면, 사용자의 장기적인 행동 수익이 점차 수렴될 수 있으며； 교체 라운드 수가 미리 설정된 라운드 수 임계 값에 도달할 수도 있다. 상술한 실시예에서 제공하는 방식에 따르면 다음과 같은 이점을 구비한다： 1） 강화 학습의 방식을 통해 사용자의 장기적인 행동 수익에 대해 추천 시스템의 최적화를 수행할 수 있다. 즉, 사용자의 장기적인 행동 수익을 최적화 목표로 직접 사용하여 추천 시스템이 사용자 장기적인 행동 수익의 증가 방향에 따라 지속적으로 발전할 수 있도록 보장한다. 2） 딥 러닝 프레임 워크를 필요로 하는 방식에 비해 강화 학습의 학습 비용은 낮고 간단한 데이터 처리 및 수 학적 계산만 필요하다. 3） 강화 학습은 추천 시스템 내부의 구체적인 모델 구조를 알 필요가 없고 추천 시스템을 “블랙박스”로 취급 하여 온라인 접속의 어려움을 크게 줄여준다. 4） 진화 학습은 매번 현재 파라미터 주변의 파라미터 공간을 탐색하고 이를 기반으로 진화 방향을 선택하므로 사용자 경험에 큰 영향을 주지 않는다. 5） 현재 추천 시스템은 대부분 감독 학습 모델과 인공 규칙을 결합하는 방식을 사용하고 강화 학습을 기반으로 원래의 감독 학습 모델 구조를 다른 모델 구조로 대체할 필요가 없고, 원래 모델 구조를 그대로 유지하고 그중 파라미터를 강화 학습의 최적화 파라미터에 추가하며, 단기 수익을 중시하는 추천 시스템에서 장기 수익을 중시 하는 추천 시스템으로 천천히 진화한다. 이상 본 출원에서 제공하는 방법을 상세히 설명하였다. 아래에 실시예를 결합하여 본 출원에서 제공하는 장치를 상세히 설명한다. 도 4는 본 출원 실시예에서 제공하는 추천 시스템의 최적화 장치 구조도이다. 도 4에 나타낸 바와 같이, 상기 장치는 모델링 유닛 및 최적화 유닛을 포함할 수 있다. 그중 각각의 조성 유닛의 주요 기능은 다음과 같다： 모델링 유닛은 추천 시스템을 에이전트로 하고 사용자를 환경으로 하며, 추천 시스템의 매번 추천 내용을 에이전트의 동작으로 하며, 사용자의 장기적인 행동 수익을 환경의 보상으로 한다. 여기서， 사용자의 장기적인 행동 수익은 사용자가 추천 시스템의 소속 서비스를 사용하는 총 시간, 사용자의 추천 시스템이 추천한 내용에 대한 총 클릭수 또는 총 클릭률, 사용자의 추천 시스템에서의 프로모션 전환율 또 는 추천 시스템 소속 서비스의 사용자 유지율을 포함할 수 있으나 이에 제한되지 않는다. 최적화 유닛은 강화 학습의 방식을 사용하여 추천 시스템 중 최적화될 파라미터를 최적화하여 환경의 보상 을 극대화한다. 본 출원 실시예에서 추천 시스템 중 전부 모델 파라미터 및/또는 하이퍼 파라미터가 최적화될 수 있다. 추천 시 스템은 예를 들면 리콜 레이어, 랭킹 레이어 등과 같은 다층 레이어 구조를 포함하기 때문에 더 세분화된 레이 어 등급일 수도 있다. 일부 레이어의 파라미터는 추천 시스템의 결과에 비교적 큰 영향을 미칠 수 있으며, 일부 는 덜 영향을 미친다. 또는 중요도에 따라 그중 일부 모델 파라미터 및/또는 하이퍼 파라미터를 최적화할 수도 있다. 예를 들면 추천 시스템의 결과에 비교적 큰 영향을 미치는 일부 모델 파라미터 및/또는 하이퍼 파라미터 를 선택하여 최적화하여 최적화 효과를 보장하는 것을 전제로 최적화 과정의 계산량을 줄일 수 있다. 본 출원에서 강화 학습의 방식은 진화 학습, DQN（Deep Q-learning， 강화 학습）, 정책 경사（Policy Gradient） 등을 사용할 수 있으나 이에 제한되지 않는다. 강화 학습의 방식으로 진화 학습을 채택하는 경우， 최적화 유닛은 구체적으로 방해 생성 서브 유닛, 방해 증가 서브 유닛, 수익 통계 서브 유닛 및 파라미터 진화 서브 유닛을 포함하여 라운드 이상의 반복 업데이트를 수행할 수 있다. 각 라운드 반복 업데이트에서， 방해 생성 서브 유닛은 각각의 사용자에 대해 한 세트의 랜덤 방해를 각각 생성하는데 사용된다. 방해 증가 서브 유닛은 한 세트의 랜덤 방해를 최적화될 파라미터에 증가하는데 사용되고 증가된 랜덤 방해 와 최적화될 파라미터의 차원은 같다. 수익 통계 서브 유닛은 랜덤 방해 증가 후의 추천 시스템에 기반하여 사용자에게 내용을 추천한 후 각각의 사용자의 장기적인 행동 수익을 통계하는데 사용된다. 파라미터 진화 서브 유닛은 수익 통계 서브 유닛에 의해 통계하여 얻은 각각의 사용자의 장기적인 행동 수 익에 따라 각각의 세트의 랜덤 방해에서 최적화될 파라미터의 진화 방향을 결정하고, 결정된 진화 방향에 기반 하여 최적화될 파라미터를 업데이트하는데 사용된다. 실제로 다양한 방향의 방해를 시도하여 최적의 진화 방향 을 결정하여 사용자의 장기적인 행동 수익을 극대화한다. 상술한 반복 업데이트는 순환 수행될 수 있고 상기 순환 수행은 지속적인 과정일 수 있다. 즉, 추천 시스템이 온라인에서 사용자에게 내용을 추천함에 따라 반복적인 최적화가 지속적으로 수행된다. 그러나 반복적인 최적화 후에는 파라미터가 실제로 점진적으로 최적화되어 최적화 정도가 이미 낮다. 다만 일부 사용자 상태, 사회적 상 태 등이 변경되기 때문에 파라미터를 지속적으로 최적화해야 하므로 진화 학습의 학습률을 낮출 수 있다. 그 중 에서 학습률의 특정 값은 실험값 또는 경험값을 사용할 수 있다. 또한 반복 업데이트가 일정 정도에 도달하면 반복 업데이트를 중지할 수 있으며, 다음번에 반복 업데이트를 시 작할 때 일정 정도에 도달할 때까지 반복 업데이트를 다시 순환 수행할 수 있다. 여기서 상기 일정 정도는 추천 시스템이 수렴 조건을 만족하는 것일 수 있으며 예를 들면, 사용자의 장기적인 행동 수익이 점진적으로 수렴되거나 교체 라운드 수가 미리 설정된 라운드 수 임계 값에 도달하는 것일 수 있다. 여기서 상술한 추천 시스템은 감독 학습 방식 또는 감독 학습과 인공 규칙을 결합한 방식을 사용하여 미리 얻은 추천 시스템일 수 있다. 본 출원의 실시예에 따르면 본 출원은 전자 기기 및 판독 가능 기록 매체를 더 제공한다. 도 5에 나타낸 바와 같이, 본 출원 실시예에 따른 추천 시스템의 최적화 방법의 전자 기기의 블럭도이다. 전자 기기는 예를 들면 랩톱 컴퓨터, 데스크톱 컴퓨터, 워크 스테이션, 개인 디지털 보조기, 서버, 블레이드 서버, 대형 컴퓨터 및 기타 적합한 컴퓨터와 같은 다양한 형태의 디지털 컴퓨터를 나타낸다. 전자 기기 또한 예를 들 면 개인 디지털 처리기, 셀폰, 스마트 전화, 웨어러블 기기 및 기타 유사한 계산 장치와 같은 다양한 형태의 모 바일 장치를 나타낼 수 있다. 본 명세서에 나타낸 구성 요소, 이들의 연결과 관계 및 이들의 기능은 단지 예일 뿐이며, 본 명세서에서 기술하거나 및/또는 요구하는 본 발명의 구현을 한정하려는 것이 아니다. 도 5에 나타낸 바와 같이 당해 전자 기기는 하나 또는 복수의 프로세서, 메모리 및 각각의 구성 요소 를 연결하기 위한 인터페이스를 구비하며, 당해 인터페이스는 고속 인터페이스 및 저속 인터페이스를 포함한다. 각각의 구성 요소는 서로 다른 버스를 통해 상호 연결되며, 공통 마더 보드에 설치되거나 또는 수요에 따라 기 타 방식으로 설치된다. 프로세서 전자 기기 내에서 수행되는 명령에 대해 처리를 수행할 수 있으며, 메모리 내 에 기억되어 외부 입력/출력 장치 （예를 들면 인터페이스에 연결된 디스플레이 기기） 상에 GUI(화면 유저 계 면)의 그래픽 정보를 표시하기 위한 명령을 포함한다. 기타 실시 방식에 있어서, 필요할 경우, 복수의 프로세서 및/또는 복수의 버스와 복수의 메모리를 함께 사용할 수 있다. 마찬가지로, 복수의 전자 기기를 연결할 수 있으 며, 각각의 기기는 부분적인 필요한 조작 （예를 들면, 서버 어레이, 일 그룹의 블레이드 서버, 또는 다중 프로 세서 시스템）을 제공한다. 도 5에서는 하나의 프로세서의 예를 들었다. 메모리는 본 출원에 의해 제공되는 비 일시적 컴퓨터 판독 가능 기록 매체이다. 여기서, 상기 메모리에는 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 적어도 하나의 프로세서로 하여금 본 출원에 의해 제공되는 추천 시스템의 최적화 방법을 수행하도록 한다. 본 출원의 비 일시적 컴퓨터 판독 가 능 기록 매체는 컴퓨터 명령을 기억하며, 상기 컴퓨터 명령은 컴퓨터로 하여금 본 출원에 의해 제공되는 추천 시스템의 최적화 방법을 수행하도록 한다. 메모리는 일종의 비 일시적 컴퓨터 판독 가능 기록 매체로서, 비 일시적 소프트웨어 프로그램, 비 일시적 컴퓨터 수행 가능 프로그램 및 모듈을 기억하는데 사용될 수 있는 바, 예를 들면 본 출원 실시예의 추천 시스템 의 최적화 방법에 대응하는 프로그램 명령/모듈을 기억하는데 사용될 수 있다. 프로세서는 메모리 내 에 기억된 비 일시적 소프트웨어 프로그램, 명령 및 모듈을 운행함으로써, 서버의 다양한 기능 응용 및 데이터 처리를 수행하는 바, 상술한 방법 실시예의 상술한 방법 실시예의 추천 시스템의 최적화 방법을 구현한다. 메모리는 프로그램 기억 영역 및 데이터 기억 영역을 포함할 수 있으며, 여기서, 프로그램 기억 영역은 운 영 체제 및 적어도 하나의 기능에 필요한 앱을 기억할 수 있고, 데이터 기억 영역은 상기 전자 기기의 사용을 통해 생성된 데이터 등을 기억할 수 있다. 또한, 메모리는 고속 랜덤 액세스 메모리를 포함할 수 있고, 비 일시적 메모리를 더 포함할 수 있는 바, 예를 들면 적어도 하나의 자기 디스크 저장 장치, 플래시 장치, 또는 기타 비 일시적 고체 저장 장치를 포함할 수 있다. 일부 실시예에 있어서, 메모리는 선택적으로 프로세서 에 대해 원격 설치한 메모리를 포함할 수 있으며, 이러한 원격 메모리는 네트워크를 통해 상기 전자 기기 에 연결될 수 있다. 상술한 네트워크의 실시예는 인터넷, 기업 인트라 넷, 근거리 통신망, 이동 통신 네트워크 및 이들의 조합을 포함하나 이에 한정되지 않는다. 상기 전자 기기는 입력 장치 및 출력 장치를 더 포함할 수 있다. 프로세서, 메모리, 입력 장치 및 출력 장치는 버스 또는 다른 방식을 통해 연결될 수 있으며 도 5에서는 버스를 통해 연결하 는 예를 들었다. 입력 장치는 입력된 디지털 또는 문자 정보를 수신하고 또한 상기 전자 기기의 유저 설정 및 기능 제어에 관한 키 신호 입력을 생성할 수 있다. 예를 들면 터치 스크린, 키패드, 마우스, 트랙 패드, 터치 패드, 포인팅 스틱, 하나 또는 복수의 마우스 버튼, 트랙볼, 조이스틱 등 입력 장치를 포함할 수 있다. 출력 장치 는 디 스플레이 기기, 보조 조명 장치（예를 들면 LED） 및 촉각 피드백 장치（예를 들면 진동 모터） 등을 포함할 수 있다. 당해 디스플레이 기기는 액정 디스플레이（LCD）, 발광 다이오드（LED） 디스플레이 및 등 플라즈마 디스 플레이를 포함할 수 있으나 이에 한정되지 않는다. 일부 실시 방식에 있어서, 디스플레이 기기는 터치 스크린일 수 있다.여기서 설명하는 시스템 및 기술의 다양한 실시 방식은 디지털 전자 회로 시스템, 집적 회로 시스템, 전용 ASIC （전용 집적 회로）, 컴퓨터 하드웨어, 펌웨어, 소프트웨어 및/또는 이들의 조합에서 구현될 수 있다. 이러한 다양한 실시형태는 하나 또는 복수의 컴퓨터 프로그램에서 실시되고, 당해 하나 또는 복수의 컴퓨터 프로그램은 적어도 하나의 프로그램 가능 프로세서를 포함하는 프로그램 가능 시스템 상에서 수행 및/또는 해석될 수 있으 며, 당해 프로그램 가능 프로세서는 전용 또는 일반 프로그램 가능 프로세서일 수 있고, 기억 시스템, 적어도 하나의 입력 장치 및 적어도 하나의 출력 장치로부터 데이터 및 명령을 수신할 수 있으며, 또한 데이터 및 명령 을 당해 기억 시스템, 당해 적어도 하나의 입력 장치 및 당해 적어도 하나의 출력 장치에 전송할 수 있다. 이러한 계산 프로그램 （프로그램, 소프트웨어, 소프트웨어 응용 또는 코드로도 불림）은 프로그램 가능 프로세 서의 기계 명령을 포함하며, 또한 고급 과정 및/또는 객체 지향 프로그래밍 언어 및/또는 어셈블리/기계 언어를 이용하여 이러한 계산 프로그램을 실시할 수 있다. 본 명세서에서 사용되는 “기계 판독 가능 매체” 및 “컴퓨 터 판독 가능 매체”와 같은 용어는, 기계 명령 및/또는 데이터를 프로그램 가능 프로세서의 임의의 컴퓨터 프 로그램 제품, 기기 및/또는 장치 （예를 들면, 자기 디스크, 광 디스크, 메모리, 프로그램 가능 논리 장치 （ PLD））에 제공하기 위한 것을 의미하며, 기계 판독 가능 신호로서의 기계 명령을 수신하는 기계 판독 가능 매 체를 포함한다. “기계 판독 가능 신호”와 같은 용어는 기계 명령 및/또는 데이터를 프로그램 가능 프로세서에 제공하기 위한 임의의 신호를 의미한다. 유저와의 대화를 제공하기 위하여, 컴퓨터 상에서 여기서 설명하는 시스템 및 기술을 실시할 수 있으며, 당해 컴퓨터는 유저에게 정보를 표시하기 위한 디스플레이 장치 （예를 들면 CRT（음극선관） 또는 LCD（액정 디스플 레이） 모니터） 및 키보드와 포인팅 장치（예를 들면, 마우스 또는 트랙볼）를 구비할 수 있으며, 유저는 당해 키보드 및 당해 포인팅 장치를 통해 입력을 컴퓨터에 제공할 수 있다. 기타 유형의 장치는 또한 유저와의 대화 를 제공하는데 사용될 수 있다. 예를 들면, 유저에 제공하는 피드백은 임의의 형태의 감각 피드백 （예를 들면, 시각적 피드백, 청각적 피드백, 또는 촉각 피드백）일 수 있으며, 또한 임의의 형태（음향 입력, 음성 입력 또 는 촉각 입력을 포함함）를 통해 유저로부터의 입력을 수신할 수 있다. 여기서 설명하는 시스템 및 기술을 백엔드 구성 요소를 포함하는 계산 시스템 （예를 들면 데이터 서버）, 또는 미들웨어 구성 요소를 포함하는 계산 시스템 （예를 들면 응용 서버）, 또는 프런트엔드 구성 요소를 포함하는 계산 시스템 （예를 들면 그래픽 유저 인터페이스 또는 웹 브라우저를 구비하는 유저 컴퓨터인 바, 유저는 당해 그래픽 유저 인터페이스 또는 당해 웹 브라우저를 통해 여기서 설명하는 시스템 및 기술의 실시 방식과 대화함 ）, 또는 이러한 백엔드 구성 요소, 미들웨어 구성 요소, 또는 프런트엔드 구성 요소의 임의의 조합을 포함하는 계산 시스템에서 실시할 수 있다. 임의의 형태 또는 매체의 디지털 데이터 통신 （예를 들면, 통신 네트워크） 을 통해 시스템의 구성 요소를 상호 연결할 수 있다. 통신 네트워크의 예는 근거리 통신망（LAN）, 광역 통신망 （WAN） 및 인터넷을 포함한다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트와 서버는 일반적으로 서로 멀리 떨어져 있 고, 또한 일반적으로 통신 네트워크를 통해 대화를 수행한다. 해당되는 컴퓨터 상에서 운행되고, 또한 클라이언 트 - 서버 관계를 갖는 컴퓨터 프로그램을 통해 클라이언트와 서버의 관계를 발생시킬 수 있다. 상술한 구체적인 실시 방식은 본 발명의 보호 범위를 한정하지 않는다. 당업자는 설계 요건 및 기타 요인에 따 라 다양한 수정, 조합, 서브 조합 및 대체를 수행할 수 있음을 이해해야 한다. 본 출원의 정신 및 원칙 내에서 이루어진 임의의 수정 동등한 대체 및 개선 등은 모두 본 발명의 보호 범위 내에 포함되어야 한다.도면 도면1 도면2 도면3 도면4 도면5"}
{"patent_id": "10-2021-0076843", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도면은 본 방안을 더 잘 이해하기 위해서만 사용되며 본 출원을 제한하지 않는다. 여기서： 도 1은 전형적인 강화 학습 시스템의 개략도이다； 도 2는 본 출원 실시예에서 제공하는 추천 시스템의 최적화 방법 순서도이다； 도 3은 본 출원 실시예에서 제공하는 진화 학습 과정에서 반복 업데이트 방법의 순서도이다； 도 4는 본 출원 실시예에서 제공하는 추천 시스템의 최적화 장치 구조도이다； 도 5는 본 출원 실시예를 구현하기 위한 전자 기기의 블럭도이다."}
