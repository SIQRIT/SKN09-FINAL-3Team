{"patent_id": "10-2020-0086395", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0008153", "출원번호": "10-2020-0086395", "발명의 명칭": "관제 영상에서의 이벤트 발생 예측 방법 및 장치", "출원인": "주식회사 케이티", "발명자": "이윤정"}}
{"patent_id": "10-2020-0086395", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 동작하는 방법으로서,관제 영상을 입력받는 단계,상기 관제 영상으로부터 위험 영역과 관제 대상 객체를 추출하는 단계, 그리고학습된 에이전트를 이용하여 상기 관제 대상 객체의 위치와 상기 위험 영역 간 거리에 기반하여 상기 관제 대상객체가 임의의 보안 이벤트를 발생시킬 확률을 계산하는 단계를 포함하고, 상기 학습된 에이전트는,상기 관제 대상 객체의 위치가 상기 위험 영역과 가까울수록 높은 위험 확률을 출력하도록 학습된 것인, 동작방법."}
{"patent_id": "10-2020-0086395", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서,상기 추출하는 단계는,상기 관제 영상으로부터 상기 위험 영역을 포함하는 환경 정보를 추출하고, 상기 환경 정보를 기반으로 상기 관제 영상을 2차원 좌표로 변환하는, 동작 방법."}
{"patent_id": "10-2020-0086395", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에서,상기 환경 정보는 상기 관제 대상 객체의 크기를 포함하고, 상기 관제 대상 객체의 크기는 상기 관제 영상 화면에 표시된 상기 관제 대상 객체의 위치에 따라 변하는 값인,동작 방법."}
{"patent_id": "10-2020-0086395", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에서,상기 계산하는 단계는, 상기 학습된 에이전트로부터 출력된 위험 확률, 상기 관제 대상 객체의 이전 위치, 상기 관제 대상 객체의 현재위치 그리고 상기 관제 대상 객체가 상기 위험 영역과 설정된 거리 이내에 머무는 시간 중 적어도 하나를 기반으로 상기 확률을 계산하는, 동작 방법."}
{"patent_id": "10-2020-0086395", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "컴퓨팅 장치로서,메모리, 그리고상기 메모리에 로드된 프로그램의 명령들(instructions)을 실행하는 적어도 하나의 프로세서를 포함하고,상기 프로그램은위험 영역과 보안 이벤트 발생 조건들을 포함하는 학습용 관제 영상을 입력받는 단계,상기 학습용 관제 영상에 포함된 객체의 위치가 상기 위험 영역과 가까울수록 또는 상기 보안 이벤트 발생 조건들을 만족할수록 높은 위험 확률을 출력하도록 에이전트를 학습시키는 단계, 공개특허 10-2022-0008153-3-새로운 관제 영상을 입력받는 단계, 그리고상기 새로운 관제 영상으로부터 관제 대상 객체를 추출하고, 학습된 에이전트로부터 출력된 상기 관제 대상 객체의 위험 확률을 이용하여, 상기 관제 대상 객체가 상기 보안 이벤트를 발생시킬 확률을 예측하는 단계를 실행하도록 기술된 명령들을 포함하는, 컴퓨팅 장치."}
{"patent_id": "10-2020-0086395", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에서,상기 학습시키는 단계는, 상기 학습용 관제 영상으로부터 2차원 가상 환경을 생성하는 단계, 그리고상기 가상 환경에서의 상기 객체의 행동과 상기 위험 영역, 그리고 상기 보안 이벤트 발생 조건들의 관계를 강화 학습시키는 단계를 포함하는, 컴퓨팅 장치."}
{"patent_id": "10-2020-0086395", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에서,상기 가상 환경은, 상기 객체가 이동할 수 있는 범위, 상기 객체의 이동에 따른 상기 객체의 크기 변화 그리고 위험성에 따른 복수의 위험 영역들 중 적어도 하나를 포함하는, 컴퓨팅 장치."}
{"patent_id": "10-2020-0086395", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제5항에서,상기 보안 이벤트는, 상기 관제 대상 객체가 상기 위험 영역을 통과하는 이벤트, 상기 위험 영역과 일정 거리 이내에서 일정 시간 이상 머무는 이벤트 또는 일정 시간 동안 움직이지 않는 이벤트 중 어느 하나를 포함하는, 컴퓨팅 장치."}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 동작하는 방법으로서, 관제 영상을 입력받는 단계, 상기 관제 영상으로부터 위험 영역과 관제 대상 객체를 추출하는 단계, 그리고 학습된 에이전트를 이용하여 상기 관제 대상 객체의 위치와 상기 위험 영역 간 거리에 기반하여 상기 관제 대상 객체가 임의의 보안 이벤트를 발생시킬 확률을 계산하는 단계를 포함하고, 상기 학습된 에이전트는, 상기 관제 대상 객체의 위치가 상기 위험 영역과 가 까울수록 높은 위험 확률을 출력하도록 학습된 것이다."}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 관제 영상에서 이벤트 발생을 예측하는 기술에 관한 것이다."}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "관제 영상에서 영상 분석을 이용하여 이벤트를 탐지하기 위해서, 관리자는 이벤트 별로 관심 영역을 설정하고, 관심 영역 내 특정 시나리오에 따른 일련의 조건들을 미리 설정해야 한다. 설정된 조건들이 모두 만족되어야 이 벤트가 발생한 것으로 탐지된다. 예를 들어 관리자가 특정 구역을 위험 영역으로 설정하고, 해당 영역에 사람이 진입한 것을 ‘진입’이벤트 발 생 조건으로 설정해 둔 경우, 위험 영역으로 설정된 관심 영역 밖에 사람이 서성거리고 있다면, 이벤트가 발생 한 것으로 볼 수 없다. 관제사는 위험 영역 주위에 사람이 움직이는 것에 대해 해당 영상을 주의 깊게 보거나 조치를 취해야할 수 있으 나, 이벤트 발생 조건에 해당하지 않아 알람 등이 발생하지 않으므로 해당 영상에 주의를 기울이기 힘들다. 이러한 상황을 포함하는 미탐지 문제를 해결하기 위해, 관심 영역을 넓게 잡거나, 민감도를 높이거나, 이벤트 발생 시나리오를 추가할 수 있다. 그러나 모든 카메라에서 관심 영역과 민감도에 따른 시나리오를 일일이 추가하는 것은 물리적, 현실적으로 어렵 다. 또한 단순히 관심 영역과 민감도를 높이는 것은 대량의 오탐을 발생시킬 수 있다. 한 예로서, 쓰레기 무단투기가 빈번하게 일어나는 곳에 설치한 CCTV 카메라의 높은 민감도로 인해, 행인들이 근처로 지나가기만 해도 경고음이 발생하여 불편을 초래할 수 있다."}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "해결하고자 하는 과제는 보안 이벤트가 발생한 관제 영상들로 에이전트를 강화 학습시키고, 학습된 에이전트를 이용하여 관제 영상에 포함된 관제 대상이 보안 이벤트를 발생시킬 확률을 예측하는 방법을 제공하는 것이다."}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "한 실시예에 따른 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 동작하는 방법으로서, 관제 영상을 입력받는 단계, 상기 관제 영상으로부터 위험 영역과 관제 대상 객체를 추출하는 단계, 그리고 학습된 에이전트 를 이용하여 상기 관제 대상 객체의 위치와 상기 위험 영역 간 거리에 기반하여 상기 관제 대상 객체가 임의의 보안 이벤트를 발생시킬 확률을 계산하는 단계를 포함하고, 상기 학습된 에이전트는, 상기 관제 대상 객체의 위 치가 상기 위험 영역과 가까울수록 높은 위험 확률을 출력하도록 학습된 것이다. 상기 추출하는 단계는, 상기 관제 영상으로부터 상기 위험 영역을 포함하는 환경 정보를 추출하고, 상기 환경 정보를 기반으로 상기 관제 영상을 2차원 좌표로 변환할 수 있다. 상기 환경 정보는 상기 관제 대상 객체의 크기를 포함하고, 상기 관제 대상 객체의 크기는 상기 관제 영상 화면 에 표시된 상기 관제 대상 객체의 위치에 따라 변하는 값일 수 있다. 상기 계산하는 단계는, 상기 학습된 에이전트로부터 출력된 위험 확률, 상기 관제 대상 객체의 이전 위치, 상기 관제 대상 객체의 현재 위치 그리고 상기 관제 대상 객체가 상기 위험 영역과 설정된 거리 이내에 머무는 시간 중 적어도 하나를 기반으로 상기 확률을 계산할 수 있다. 한 실시예에 따른 컴퓨팅 장치로서, 메모리, 그리고 상기 메모리에 로드된 프로그램의 명령들(instructions)을 실행하는 적어도 하나의 프로세서를 포함하고, 상기 프로그램은 위험 영역과 보안 이벤트 발생 조건들을 포함하 는 학습용 관제 영상을 입력받는 단계, 상기 학습용 관제 영상에 포함된 객체의 위치가 상기 위험 영역과 가까 울수록 또는 상기 보안 이벤트 발생 조건들을 만족할수록 높은 위험 확률을 출력하도록 에이전트를 학습시키는 단계, 새로운 관제 영상을 입력받는 단계, 그리고 상기 새로운 관제 영상으로부터 관제 대상 객체를 추출하고, 학습된 에이전트로부터 출력된 상기 관제 대상 객체의 위험 확률을 이용하여, 상기 관제 대상 객체가 상기 보안 이벤트를 발생시킬 확률을 예측하는 단계를 실행하도록 기술된 명령들을 포함한다. 상기 학습시키는 단계는, 상기 학습용 관제 영상으로부터 2차원 가상 환경을 생성하는 단계, 그리고 상기 가상 환경에서의 상기 객체의 행동과 상기 위험 영역, 그리고 상기 보안 이벤트 발생 조건들의 관계를 강화 학습시키 는 단계를 포함할 수 있다. 상기 가상 환경은, 상기 객체가 이동할 수 있는 범위, 상기 객체의 이동에 따른 상기 객체의 크기 변화 그리고 위험성에 따른 복수의 위험 영역들 중 적어도 하나를 포함할 수 있다. 상기 보안 이벤트는, 상기 관제 대상 객체가 상기 위험 영역을 통과하는 이벤트, 상기 위험 영역과 일정 거리 이내에서 일정 시간 이상 머무는 이벤트 또는 일정 시간 동안 움직이지 않는 이벤트 중 어느 하나를 포함할 수 있다."}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 정해진 룰이 아닌 강화 학습 기반의 알고리즘을 사용하므로, 학습에 사용되는 관제 영상이 늘 어날수록 보안 이벤트 예측의 정확도를 높일 수 있고, 오탐지를 방지할 수 있다."}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위 해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 명세서 전체에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 명세서에 기재 된 \"…부\", \"…기\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드 웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 본 발명은 강화 학습을 이용하여 관제 영상에서 등장하는 객체의 행동을 예측하고, 객체가 보안 이벤트를 발생 시키는지 예측하는 것이 목적이다. 이하에서는, 강화 학습의 일반적인 내용에 대해 설명한다. 강화 학습이란, 여러 단계를 거쳐 복잡한 목표를 달성하거나 특정 차원을 따라 최대화하는 방법을 학습하는 목 표 지향적 알고리즘을 의미한다. 강화 학습 알고리즘으로 선택된 행동이 잘못된 결정인 경우 페널티가 부여되고, 옳은 결정인 경우 보상이 주어지는데, 페널티와 보상을 이용하여 알고리즘을 학습시킬 수 있다. 강화 학습 알고리즘 중 대표적인 Q 러닝(Q-Learning)은, 주어진 상태에서 주어진 행동을 수행하는 것이 가져다 줄 효용의 기대값을 예측하는 Q 함수를 학습함으로써 최적의 정책을 학습한다. 강화 학습은 에이전트(Agent), 환경(Environment), 상태(State), 행동(Action), 보상(Reward)이라는 5가지의 개념을 포함한다. 에이전트는 행동을 하는 주체, 환경은 에이전트가 속한 세계로서 에이전트를 제외한 나머지 모든 것, 상태는 현 재의 상황, 행동은 에이전트가 취할 수 있는 동작, 보상은 에이전트의 행동에 대한 피드백을 의미한다. 한편 본 발명은 관제 영상에 포함된 실제 영상과 유사하도록 생성된 이미지를 에이전트의 학습 환경으로 이용한 다. 생성된 이미지 내에서 환경을 구성하기 위해 필요한 정보는 일반적인 영상 분석을 위한 변수들을 포함하고, 예를 들어 위험 영역, 객체의 중심, 객체의 크기 등을 포함할 수 있다. 보상은 즉시 주어지거나 먼 미래에 주어질 수 있고, 모든 행동에 대해 주어지지 않고, 확률적으로 주어질 수도 있다. 보안 이벤트는 관제 영상에서 객체가 보안에 위협을 가할 수 있는 여러 행동 유형들을 의미한다. 특정 행동이 어떤 유형의 보안 이벤트에 해당하는지는 관리자 또는 미리 저장된 기준에 따라 달라질 수 있다. 예를 들어 객체가 위험 영역의 경계를 통과하는 보안 이벤트를 ‘침입’, 객체가 위험 영역의 일정 거리 이내를 일정 시간 이상 머무는 보안 이벤트를 ‘배회’로 정의할 수 있다. 한편 실제 촬영된 영상에 포함된 객체와, 에이전트가 행동을 조작하는 객체를 구분하기 위해 전자를 자연 객체, 후자를 가상 객체라고 호칭한다. 또한, 별도의 언급이 없는 한 본 명세서에서 이벤트는 보안 이벤트를 의미한다. 도 1은 한 실시예에 따른 이벤트 예측 장치의 구성도이다. 도 1을 참고하면, 이벤트 예측 장치는 네트워크로 연결된 CCTV로부터 관제 영상을 입력받고, 관제 영상에 포함된 객체, 즉 관제 대상이 미리 설정된 보안 이벤트를 발생시킬 확률을 예측하여 제공한다. 이벤트 예측 장 치는 관제 영상으로부터 위험 영역과 객체를 추출하고, 가상 환경 정보를 구성하는 전처리부, 특정 상 태에서 특정 행동을 취하고, 취한 행동에 따른 보상을 얻는 에이전트, 에이전트의 행동에 따라 단기 적인 보상과 장기적인 보상을 계산하는 보상 계산부, 에이전트의 행동이 임의의 이벤트를 발생시킬 확률을 계산하여 출력하는 확률 출력부를 포함한다. 설명을 위해, 전처리부, 에이전트, 보상 계산부, 확률 출력부로 명명하여 부르나, 이들은 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치이다. 여기서, 전처리부, 에이전트, 보상 계산 부, 확률 출력부는 하나의 컴퓨팅 장치에 구현되거나, 별도의 컴퓨팅 장치에 분산 구현될 수 있다. 별도의 컴퓨팅 장치에 분산 구현된 경우, 전처리부, 에이전트, 보상 계산부, 확률 출력부 는 통신 인터페이스를 통해 서로 통신할 수 있다. 컴퓨팅 장치는 본 발명을 수행하도록 작성된 소프트웨어 프로 그램을 실행할 수 있는 장치이면 충분하고, 예를 들면, 서버, 랩탑 컴퓨터 등일 수 있다. 전처리부, 에이전트, 보상 계산부, 확률 출력부 각각은 하나의 인공지능 모델일 수 있고, 복수의 인공지능 모델로 구현될 수도 있다. 이벤트 예측 장치는 하나의 인공지능 모델일 수 있고, 복수의 인공지능 모델로 구현될 수도 있다. 이에 따라, 상술한 구성들에 대응하는 하나 또는 복수의 인공지능 모델은 하나 또는 복수의 컴퓨팅 장치에 의해 구현될 수 있다. 전처리부는 관제 영상을 입력받고, 관제 영상으로부터 가상 환경을 구성하기 위한 정보들을 추출한다. 에 이전트는 추출된 환경 정보로 생성된 가상 환경에서 학습을 진행할 수 있다. 환경 정보는 해당 영상 내의 위험 영역, 영상 내에서 자연 객체 또는 가상 객체가 이동할 수 있는 범위, 이동 경로, 이동에 따른 객체의 크기 변 화율 등을 포함할 수 있다. 위험 영역은 복수개로 설정될 수 있으며, 위험성의 정도에 따라 구분될 수 있다. 관제 영상으로부터 위험 영역을 설정하는 방법은 관리자에 의한 직접 설정이거나 영상 분석 기반의 딥러닝 등의 모델을 이용한 것일 수 있으며, 어느 하나로 제한되지 않는다. 위험 영역은 관심 지역(Region of Interest, ROI)이라고 호칭될 수 있다. 전처리부는 관제 영상에서 관측의 대상이 되는 자연 객체를 추출한다. 자연 객체를 추출하기 위해, 공지된 객체 탐지(Object Tracking) 방법을 사용할 수 있다. 이때 자연 객체의 이동 경로를 파악하기 위해 중심점을 추 출할 수 있다. 자연 객체의 이동 경로를 바탕으로 가상 객체의 이동 경로가 설정될 수 있다. 또한 전처리부는 영상 화면의 위치에 따라 자연 객체의 크기가 원근법에 의해 달라지는 효과를 고려할 수 있다. 이는 자연 객체의 크기가 기준값 이하인 경우는 보안 이벤트를 발생시키지 않으므로, 자연 객체의 크기를 고려하여 가상 객체를 생성하기 위함이다. 전처리부는 관제 영상으로부터 생성된 가상 환경에서 에이전트에 의해 행동되는 가상 객체를 생성한 다. 가상 객체는 인간의 형태를 모사하는 Deep AI walk로 구현되거나, 사각형 등의 단순한 형태로 구현될 수 있 다. 에이전트는 정책에 따라 시간 t의 주어진 환경에서, 시간 t+1에서의 가상 객체의 행동을 선택할 수 있다. 선택의 기준은 보상 계산부에서 결정된 보상 값 또는 정책 결과일 수 있다. 에이전트는 단기 또는 장 기 보상 값이 최대가 되도록 가상 객체의 행동을 결정할 수 있다. 가상 객체를 이동시킬 수 있는 선택 가능한 행동은, 입력된 영상에서 2차원 좌표값으로 표현될 수 있다. 예를 들어 에이전트는, 시간 t에서 (3, 3)에 위치한 가상 객체를 시간 t+1에서 9개의 좌표 (2, 4), (3, 4), (4, 4), (2, 3), (3, 3), (4, 3), (2, 2), (3, 2), (4, 2) 중 어느 하나로 이동시킬 수 있다. 이때 t+1에서의 가상 객체의 좌표가 반드시 t에서의 좌표와 인접해야 하는 것은 아니며, 단위 시간 당 가상 객체의 이동 범위는 관리자에 의해 변경될 수 있다. 보상 계산부는 에이전트가 가상 객체의 행동을 결정하기 위한 정책을 관리하고, 선택된 행동에 따른 보상 값을 결정한다. 또한 보상 값을 계산하기 위해 사용되는 요인들, 보상 값이 계산되는 함수 등을 관리할 수 있다. 보상값을 최대로 하기 위한 정책은 심층 신경망, 다항식 또는 룩업 테이블 중 어느 하나로 표현될 수 있 다. 한편, 본 발명에서 에이전트는 가상 객체의 행동을 통해 임의의 이벤트가 발생하는 패턴을 학습하게 된다. 즉 이벤트를 발생시키도록 객체가 움직이는 패턴을 학습하는 것이 목적이다. 따라서 본 발명에서 보상값은 가상 객체의 행동이 임의의 이벤트를 발생시키기 한 행동에 가까울수록 큰 값일 수 있다. 보상값이 계산되는 방법은 어느 하나로 제한되지 않으며, 가상 객체의 행동과 이벤트 발생 간 관계를 나타내는 어떠한 형태든 가능하다. 한 예로서, 보상 계산부는 에이전트가 선택한 행동에 대한 단기 보상값과 장기 보상값을 구분하여 계 산할 수 있다. 단기 보상값은 에이전트가 선택한 가상 객체의 위치가 설정된 위험 영역과 가깝거나 자연 객체의 위치와 가까울수록 큰 값이 되고, 장기 보상값은 에이전트가 선택한 가상 객체의 행동이 먼 미래에 미리 설정된 보안 이벤트를 발생시키는 경우 큰 값이 될 수 있다. 구체적으로, 단기 보상값은 수학식 1로 계산될 수 있고, 장기 보상값은 자연수 형태의 점수일 수 있다. [수학식 1]"}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서, A는 자연 객체와 위험 영역 간 거리를 의미하고, D는 자연 객체와 가상 객체 간 거리를 의미한다. 한편 A와 D는 각각 수학식 2, 수학식 4를 통해 구할 수 있다. [수학식 2]"}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "한편 수학식 2에서, A는 자연 객체와 위험 영역 간 거리를 정규화한 것으로, A를 구하기 위해 사용되는 는 수 학식 3을 통해 구할 수 있다. [수학식 3]"}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수학식 3에서, 는 자연 객체를 중심으로 위험 영역까지의 시야각을 의미하고, (x_n, y,n)과 (x_m, y_m)은 자 연 객체가 원점에 있다고 가정할 때 위험 영역에 속한 좌표를 의미한다. [수학식 4]"}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수학식 4에서, (xi, yi)는 자연 객체의 좌표를 의미하고, (xj, yj)는 가상 객체의 좌표를 의미하고, Os 는 자연 객체의 크기를 의미한다. 장기 보상값은, 객체의 행동이 누적되어 먼 미래에 이벤트 발생 여부에 따라 결정되는 것이므로, 시간 t에서 입 력된 환경 정보를 기반으로, 시간 t+1에서 에이전트가 선택한 행동에 대해 시간 t+2에서 곧바로 계산되는 것이 아니다. 한편 발생할 수 있는 이벤트는 복수 개일 수 있으며, 각 이벤트 별로 발생 조건이 다르므로 장기 보상 값을 계 산하는 방법이 달라질 수 있다. 각 이벤트를 정의하는 일련의 행동들은 관리자에 의해 결정되거나 딥러닝 모델에 의해 결정될 수 있다. 예를 들 어, 침입 또는 라인 크로스라는 이벤트는 객체의 중심이 설정된 기준선을 통과할 때 발생하는 것으로 정의될 수 있다. 예를 들어, 가상 객체가 설정된 위험 영역에 일정 거리 이내로 접근하는 경우 장기적으로는 임의의 이벤트 발생 확률이 높아지는 것이므로, 단기 보상값 역시 높아질 수 있다. 확률 출력부는 에이전트가 결정한 가상 객체의 행동이 어떤 이벤트를 발생시킬 것인지 확률을 계산하 여 출력한다. 즉, 가상 객체의 행동은 단기 미래 또는 장기 미래에 임의의 이벤트를 발생시킬 수 있는데, 이러 한 행동이 미리 등록된 복수의 이벤트들을 발생시킬 확률을 각각 계산할 수 있다. 구체적으로, 수학식 5에 의해계산될 수 있다. [수학식 5]"}
{"patent_id": "10-2020-0086395", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "수학식 5에서, P는 이벤트 발생 확률을 의미하고, b는 에이전트를 의미하고, s는 가상 객체의 상태를 의미 하고, a는 에이전트가 선택한 가상 객체의 행동을 의미하고, π는 정책을 의미하고, Ra는 선택된 행동에 대한 보상 값을 의미한다. 예를 들어, 침입과 배회 이벤트가 정의된 경우, 에이전트가 시간 t+1에 가상 객체가 (1, 1)에서 (2, 2)로 이동시키는 행동을 선택한 경우를 가정한다. 이 선택에 의해 확률 출력부는 침입 이벤트가 발생할 확률과 배회 이벤트가 발생할 확률을 각각 특정 수치로 출력할 수 있다. 확률 출력부에서 사용하는 이벤트 종류는 보상 계산부에서 사용한 이벤트 정보에 의한 것일 수 있다. 한편, 확률 출력부는 에이전트의 매 선택마다 각 이벤트의 확률 값을 계산하거나 미리 설정된 주기마 다 각 이벤트의 확률 값을 계산하여 제공할 수 있다. 도 2는 한 실시예에 따른 이벤트 예측 장치가 동작하는 방법의 흐름도이고, 도 3은 한 실시예에 따른 전처리부 가 동작하는 방법의 설명도이고, 도 4는 한 실시예에 따른 환경 정보의 예시도이고, 도 5는 한 실시예에 따른 에이전트가 동작하는 방법의 설명도이다. 도 2를 참고하면, 이벤트 예측 장치는 관제 영상을 입력받는다(S110). 예를 들어 도 3의 (a)와 같은 영상일 수 있다. 전처리부가 입력받는 관제 영상은 실시간 영상이거나, 별도의 데이터베이스(미도시)에 저장된 영상일 수 있다. 또한 설정된 단위 시간 동안의 동영상이거나, 영상의 각 프레임 이미지일 수 있다. 한편 입력되는 영상은 이벤트를 포함하거나 포함하지 않을 수 있다. 따라서 보상 값을 계산하기 위해 사용되는 이벤트 발생 여부는 상수가 아닌 확률로 표현될 수 있다. 이벤트 예측 장치는 관제 영상에 위험 영역을 설정하고, 관제 대상이 되는 자연 객체를 추출한다 (S120). 이벤트 예측 장치는 환경 정보를 더 추출할 수 있다. 환경 정보는, 객체의 크기 변화를 포함할 수 있고, 객체의 크기는 화면 내 객체의 위치에 따라 달라질 수 있다. 위험 영역을 설정하는 방법과, 자연 객체 를 추출하는 방법은 도 1에서 설명한 바와 같다. 이벤트 예측 장치는 에이전트의 학습 환경이 되는 가상 환경과 가상 객체를 설정한다(S130). 이벤트 예측 장치는 도 3의 (b)와 같이 2개의 위험 영역을 설정하고, 자연 객체를 추출할 수 있다. 노 란색 영역과 초록색 영역이 위험 영역이고, 빨간색 영역이 자연 객체로 추출된 영역이다. 이를 이용하여, 도 4의 (a)와 같은 가상 환경을 구성할 수 있다. 가상 환경은 또한, 위험 영역, 자연 객체 및 가상 객체의 위치를 좌표로 변환하여 나타낼 수 있다. 도 4의 (a)를 참고하면, 위험 영역1과 위험 영역2는 위험성에 따라 구분될 수 있다. 한편 S120 내지 S130 단계는 영상이 입력될 때마다 수행될 수 있고, 하나의 단위 영상이 입력되는 경우에는 맨 처음 부분이 입력될 때에만 수행될 수 있다. 이벤트 예측 장치의 에이전트는 정책을 바탕으로 가상 객체의 t+1에서의 행동을 선택한다(S140). 도 4의 (a)와 도 5를 참고하면, 에이전트는 현재 위치에서 가상 객체를 x축 방향으로 +1만큼 이동시 키는 행동을 선택할 수 있다. 에이전트의 행동을 결정하는 정책은, 보상 값을 최대로 하기 위해 보상 계산부에서 결정한 것일 수 있다. 초기 정책은 다른 인공 신경망 모델에 의해 결정될 수 있으며, 강화 학습의 특성상, 에이전트가 행 동 선택을 반복함에 따라 정책은 수정될 수 있다. 정책 함수를 생성하기 위한 보상 값은 도 1에서 설명한 내용 과 같다. 이벤트 예측 장치는 에이전트가 선택한 행동이 이벤트를 발생시킬 확률을 계산하여 출력한다(S150). 도 1에서 설명한 바와 같이, 이벤트 발생 확률은 수학식 5에 의해 계산될 수 있다. 이후, 이벤트 예측 장치는 시간 t+1에서의 관제 영상을 입력받고, 영상에서 자연 객체의 크기와 위치 를 추출한다(S160). S140 단계에서 선택한 행동에 대한 보상 값을 계산하기 위한 것이다. 도 5를 참고하면, 시 간 t+1에서의 자연 객체는 시간 t에서와 비교하면 y축 방향으로 -1만큼 이동하였다. 이때 시간 t에서의 환 경과 시간 t+1에서의 객체의 크기를 포함한 환경 요인이 서로 다를 수 있으므로, 전처리부는 시간 t+1에서 의 관제 영상에 대한 환경 분석을 다시 진행할 수 있다. 이벤트 예측 장치는 시간 t+1에서의 가상 객체와 위험 영역의 관계에 따라 단기 보상 값을 계산한다 (S170). 구체적으로, 가상 객체가 위험 영역에 접근하면 이벤트 발생 확률이 커지므로, 가상 객체와 위험 영역 간 거리가 가까울 수록 단기 보상 값은 커질 수 있다. 단기 보상 값은 수학식 1 내지 수학식 4에 의 해 계산될 수 있다. 이벤트 예측 장치는 가상 객체의 누적된 행동이 자연 객체가 발생시키는 이벤트에 해당하면 큰 보상을 얻도록, 장기 보상 값을 계산한다(S180). 이벤트 예측 장치는 보상 값을 최대로 하기 위한 에이전트의 행동 정책을 업데이트한다(S190). 정책 함수는 Q 함수의 정의에 따라 결정될 수 있다. 도 6은 한 실시예에 따른 컴퓨팅 장치의 하드웨어 구성도이다. 도 6을 참고하면, 전처리부, 에이전트, 보상 계산부, 확률 출력부는 적어도 하나의 프로세 서에 의해 동작하는 컴퓨팅 장치에서, 본 발명의 동작을 실행하도록 기술된 명령들(instructions)이 포함 된 프로그램을 실행한다. 컴퓨팅 장치의 하드웨어는 적어도 하나의 프로세서, 메모리, 스토리지, 통신 인터페이스 를 포함할 수 있고, 버스를 통해 연결될 수 있다. 이외에도 입력 장치 및 출력 장치 등의 하드웨어가 포함 될 수 있다. 컴퓨팅 장치는 프로그램을 구동할 수 있는 운영 체제를 비롯한 각종 소프트웨어가 탑재될 수 있다. 프로세서는 컴퓨팅 장치의 동작을 제어하는 장치로서, 프로그램에 포함된 명령들을 처리하는 다양한 형태의 프로세서일 수 있고, 예를 들면, CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit) 등 일 수 있다. 메모리는 본 발명의 동작을 실행하도록 기술된 명령들이 프로세서에 의해 처리되도록 해당 프로그램을 로드한다. 메모리는 예를 들면, ROM(read only memory), RAM(random access memory) 등 일 수 있다. 스토리지는 본 발명의 동작을 실행하는데 요구되는 각종 데이터, 프로그램 등을 저장한다. 통신 인터페이스는 유/무선 통신 모듈일 수 있다. 본 발명에 따르면 정해진 룰이 아닌 강화 학습 기반의 알고리즘을 사용하므로, 학습에 사용되는 관제 영상이 늘 어날수록 보안 이벤트 예측의 정확도를 높일 수 있고, 오탐지를 방지할 수 있다. 이상에서 설명한 본 발명의 실시예는 장치 및 방법을 통해서만 구현이 되는 것은 아니며, 본 발명의 실시예의 구성에 대응하는 기능을 실현하는 프로그램 또는 그 프로그램이 기록된 기록 매체를 통해 구현될 수도 있다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속하는 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6"}
{"patent_id": "10-2020-0086395", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 한 실시예에 따른 이벤트 예측 장치의 구성도이다. 도 2는 한 실시예에 따른 이벤트 예측 장치가 동작하는 방법의 흐름도이다. 도 3은 한 실시예에 따른 전처리부가 동작하는 방법의 설명도이다.도 4는 한 실시예에 따른 환경 정보의 예시도이다. 도 5는 한 실시예에 따른 에이전트가 동작하는 방법의 설명도이다. 도 6은 한 실시예에 따른 컴퓨팅 장치의 하드웨어 구성도이다."}
