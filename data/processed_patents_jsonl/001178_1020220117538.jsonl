{"patent_id": "10-2022-0117538", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0039245", "출원번호": "10-2022-0117538", "발명의 명칭": "인공지능을 이용한 다국어 자막송출 시스템", "출원인": "히즈콥 주식회사", "발명자": "김대갑"}}
{"patent_id": "10-2022-0117538", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 인식부, 음성 추출부 및 인공지능 음성 분석부를 포함하는 영상분석 서버; 및인공지능 번역부 및 자막 생성부를 포함하는 텍스트처리 서버를 포함하는 인공지능을 이용한 다국어 자막송출시스템."}
{"patent_id": "10-2022-0117538", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 음성 인식부는, 동영상의 음성을 분석하여 화자가 구사하는 언어의 종류에 대한 데이터, 화자의 수에 대한데이터를 포함하는 음성 인식 데이터를 생성하여 상기 음성 추출부 및 상기 인공지능 음성 분석부로 전달하는인공지능을 이용한 다국어 자막송출 시스템."}
{"patent_id": "10-2022-0117538", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 음성 추출부는, 상기 음성 인식 데이터를 전달받고, 각각의 화자 및 언어의 종류별로 음성을 추출한 음성추출 데이터를 생성하여 상기 인공지능 음성 분석부로 전달하는 인공지능을 이용한 다국어 자막송출 시스템."}
{"patent_id": "10-2022-0117538", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,상기 인공지능 음성 분석부는, 상기 음성 인식 데이터 및 상기 음성 추출 데이터를 전달받고, 타임라인별 오디오 레벨 변화량을 타임라인 인덱스에 매핑하여 타임라인 데이터를 생성하여 상기 인공지능 번역부 및 스트리밍서버에 전달하는 인공지능을 이용한 다국어 자막송출 시스템."}
{"patent_id": "10-2022-0117538", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서,상기 인공지능 음성 분석부는 상기 음성 추출 데이터를 전달받고, 상기 음성 추출 데이터에서 텍스트를 추출하여 제1 인공지능 텍스트 추출 데이터를 생성하고, 상기 제1 인공지능 텍스트 추출 데이터의 전후 발화의 문맥흐름을 분석하여 정확도가 높은 단어를 재선택하여 제2 인공지능 텍스트 추출 데이터를 생성하여, 텍스트 처리서버로 전달하는 인공지능을 이용한 다국어 자막송출 시스템."}
{"patent_id": "10-2022-0117538", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서,상기 인공지능 음성 분석부는 상기 음성 인식 데이터와, 상기 음성 추출 데이터 및 상기 동영상을 분석하여, 같은 국가의 같은 방송을 보는 사용자라고 판단되거나, 이미 자막 생성이 완료된 동영상과 동일한 동영상이라고판단되는 경우 인공지능 분석 데이터를 생성하여 상기 인공지능 번역부로 전달하는 인공지능을 이용한 다국어공개특허 10-2024-0039245-3-자막송출 시스템."}
{"patent_id": "10-2022-0117538", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 인공지능 번역부는 상기 동영상 및 상기 제2 인공지능 텍스트 추출 데이터를 기반으로 번역을 진행하여 번역 데이터를 생성하여 상기 자막 생성부 및 서버 저장부에 전달하는 인공지능을 이용한 다국어 자막송출시스템."}
{"patent_id": "10-2022-0117538", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서,상기 인공지능 번역부는 상기 인공지능 분석 데이터를 전달받는 경우 서버 저장부에 기 저장된 번역 데이터를선별하여 자막 생성부에 전달하는 인공지능을 이용한 다국어 자막송출 시스템."}
{"patent_id": "10-2022-0117538", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 자막 생성부는 상기 인공지능 번역부로부터 번역 데이터를 전달받고 자막 데이터를 생성하여, 스트리밍 서버로 전달하는 인공지능을 이용한 다국어 자막송출 시스템."}
{"patent_id": "10-2022-0117538", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9 항에 있어서,상기 스트리밍 서버는 상기 인공지능 음성 분석부로부터 전달받은 타임라인 데이터와 상기 자막 생성부로부터전달받은 자막 데이터를 바탕으로 동영상에 자막을 제공하는 인공지능을 이용한 다국어 자막송출 시스템."}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공지능을 이용한 다국어 자막송출 시스템이 제공된다. 인공지능을 이용한 다국어 자막송출 시스템은, 음성 인 식부, 음성 추출부 및 인공지능 음성 분석부를 포함하는 영상분석 서버 및 인공지능 번역부 및 자막 생성부를 포 함하는 텍스트처리 서버를 포함한다."}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능을 이용한 다국어 자막송출 시스템에 관한 것이다."}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인터넷의 발달로 다양한 컨텐츠 영상에 대한 제작 및 재생이 가능해졌으며, 다양한 나라의 사용자들이 쉽고 빠 르게 시청할 수 있게 되었다. 따라서, 컨텐츠 영상을 제작한 제작자는 각 나라의 사용자들이 보다 편하게 시청 할 수 있도록 해당 컨텐츠 영상에 포함된 소스 언어를 다양한 언어로 번역하고 있다. 그러나, 제작자 스스로 다양한 언어로 번역하는 것은 한계가 있으며, 번역을 번역가에게 의뢰할 경우, 해당 번 역가에 대한 정보가 부족하여 컨텐츠 영상과 상이한 의미로 해석될 수 있는 언어로 번역되거나, 의뢰인이 요청 한 언어의 번역이 가능한 번역가를 찾는데 많은 시간을 소비할 수 있는 문제가 있다. 따라서, 컨텐츠 영상에 대한 보다 쉽고 빠른 번역 작업이 진행될 수 있는 다양한 방안이 모색되어야 한다. 선행기술문헌특허문헌 (특허문헌 0001) 대한민국등록특허공보 10-2226765호"}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는 동영상과 자막이 실시간으로 일치할 수 있는 신뢰성 높은 인공지능을 이용한 다국어 자막송출 시스템을 제공하는 것을 목적으로 한다. 본 발명의 과제들은 이상에서 언급한 과제로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 과제를 해결하기 위한 일 실시예에 따른 인공지능을 이용한 다국어 자막송출 시스템은, 음성 인식부, 음성 추출부 및 인공지능 음성 분석부를 포함하는 영상분석 서버 및 인공지능 번역부 및 자막 생성부를 포함하는 텍 스트처리 서버를 포함한다. 상기 음성 인식부는, 동영상의 음성을 분석하여 화자가 구사하는 언어의 종류에 대한 데이터, 화자의 수에 대한 데이터를 포함하는 음성 인식 데이터를 생성하여 상기 음성 추출부 및 상기 인공지능 음성 분석부로 전달할 수 있다. 상기 음성 추출부는, 상기 음성 인식 데이터를 전달받고, 각각의 화자 및 언어의 종류별로 음성을 추출한 음성 추출 데이터를 생성하여 상기 인공지능 음성 분석부로 전달할 수 있다. 상기 인공지능 음성 분석부는, 상기 음성 인식 데이터 및 상기 음성 추출 데이터를 전달받고, 타임라인별 오디 오 레벨 변화량을 타임라인 인덱스에 매핑하여 타임라인 데이터를 생성하여 상기 인공지능 번역부 및 스트리밍 서버에 전달할 수 있다. 상기 인공지능 음성 분석부는 상기 음성 추출 데이터를 전달받고, 상기 음성 추출 데이터에서 텍스트를 추출하 여 제1 인공지능 텍스트 추출 데이터를 생성하고, 상기 제1 인공지능 텍스트 추출 데이터의 전후 발화의 문맥 흐름을 분석하여 정확도가 높은 단어를 재선택하여 제2 인공지능 텍스트 추출 데이터를 생성하여, 텍스트 처리 서버로 전달할 수 있다. 상기 인공지능 음성 분석부는 상기 음성 인식 데이터와, 상기 음성 추출 데이터 및 상기 동영상을 분석하여, 같 은 국가의 같은 방송을 보는 사용자라고 판단되거나, 이미 자막 생성이 완료된 동영상과 동일한 동영상이라고 판단되는 경우 인공지능 분석 데이터를 생성하여 상기 인공지능 번역부로 전달할 수 있다. 상기 인공지능 번역부는 상기 동영상 및 상기 제2 인공지능 텍스트 추출 데이터를 기반으로 번역을 진행하여 번 역 데이터를 생성하여 상기 자막 생성부 및 서버 저장부에 전달할 수 있다. 상기 인공지능 번역부는 상기 인공지능 분석 데이터를 전달받는 경우 서버 저장부에 기 저장된 번역 데이터를 선별하여 자막 생성부에 전달할 수 있다. 상기 자막 생성부는 상기 인공지능 번역부로부터 번역 데이터를 전달받고 자막 데이터를 생성하여, 스트리밍 서 버로 전달할 수 있다. 상기 스트리밍 서버는 상기 인공지능 음성 분석부로부터 전달받은 타임라인 데이터와 상기 자막 생성부로부터 전달받은 자막 데이터를 바탕으로 동영상에 자막을 제공할 수 있다."}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "일 실시예에 따른 인공지능을 이용한 다국어 자막송출 시스템에 의하면, 동영상에 맞는 다국어 자막이 제공될 수 있음과 동시에, 동영상 인물의 안면인식 또는 음성인식을 통한 자막 송출로 정밀한 자막송출이 가능하게 된 다. 실시예들에 따른 효과는 이상에서 예시된 내용에 의해 제한되지 않으며, 더욱 다양한 효과들이 본 명세서 내에 포함되어 있다."}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 모든 흐름도, 상태 변환도, 의사 코드 등은 컴퓨터가 판독 가능한 매체에 실질적으로 나타낼 수 있고 컴퓨터 또 는 프로세서가 명백히 도시되었는지 여부를 불문하고 컴퓨터 또는 프로세서에 의해 수행되는 다양한 프로세스를 나타내는 것으로 이해되어야 한다. 프로세서 또는 이와 유사한 개념으로 표시된 기능 블럭을 포함하는 도면에 도시된 다양한 소자의 기능은 전용 하드웨어뿐만 아니라 적절한 소프트웨어와 관련하여 소프트웨어를 실행할 능력을 가진 하드웨어의 사용으로 제 공될 수 있다. 프로세서에 의해 제공될 때, 상기 기능은 단일 전용 프로세서, 단일 공유 프로세서 또는 복수의 개별적 프로세서에 의해 제공될 수 있고, 이들 중 일부는 공유될 수 있다. 명세서 전체에 걸쳐 동일 참조 부호 는 동일 구성 요소를 지칭한다. 이하 첨부된 도면을 참조하여 구체적인 실시예들에 대해 설명한다. 도 1은 일 실시예에 따른 인공지능을 이용한 다국어 자막송출 시스템의 연동 구조를 개략적으로 나타낸 도면이 고, 도 2는 일 실시예에 따른 다국어 자막 송출 시스템의 구성을 개략적으로 나타낸 도면이며, 도 3은 일 실시 예에 따른 영상분석 서버, 텍스트처리 서버, 스트리밍 서버의 구성을 개략적으로 나타낸 도면이고, 도 4는 일 실시예에 따른 사용자 단말 연동부를 개략적으로 나타낸 도면이며, 도 5는 일 실시예에 따른 인공지능을 이용한 다국어 자막송출 시스템의 자막 송출 방법을 설명하기 위한 도면이다. 도 1을 참조하면, 인공지능을 이용한 다국어 자막송출 시스템은, 사용자 단말과, 동영상 및 다국 어 자막 송출 시스템을 포함할 수 있다. 인공지능을 이용한 다국어 자막송출 시스템은 인공지능 신경망 딥러닝 기반 프로그램으로 번역의 정확도 및 완성도를 높여 동영상에 다국어 자막을 제공할 수 있다. 몇몇 실시예에서 인공지능을 이용한 다국어 자막송 출 시스템은 인공지능 번역 엔진의 API를 이용하여 동영상에 다국어 자막을 제공할 수도 있다. 또한, 몇몇 실시예에서 인공지능을 이용한 다국어 자막송출 시스템은 인공지능 신경망 딥러닝 기반 프로그램과 인공지 능 번역 엔진의 API 각각을 이용하여 동영상에 다국어 자막을 제공할 수도 있다. 예를 들어, 접속국가 중 처음 청취되는 방송의 현지어 자막의 경우, API 서비스를 통하여 자막이 송출됨과 동시에 서버 저장부에 자막이 저장될 수 있고, 같은 국가의 같은 방송을 보는 사용자의 경우에는 서버 저장부에 기저장된 현지어 방송자막이 송출 됨으로써 API 번역 엔진 사용 비용을 줄이고, 자막 송출 서비스 속도를 개선시킬 수 있게 된다. 인공지능을 이용한 다국어 자막송출 시스템은 동영상 진행자의 안면을 인식하여 진행자의 음성에 맞게 자막 을 제공할 수 있다. 또한, 인공지능을 이용한 다국어 자막송출 시스템은 진행자가 다수인 경우 안면 인식 및 음성 인식을 통하여 각 진행자에 맞는 자막을 송출할 수 있다. 이를 통하여, 동영상에 생동감 있는 자막을 제공함과 동시에 시청자의 자막 시인성을 향상시키게 된다. 또한, 인공지능을 이용한 다국어 자막송출 시스템 은 동영상에 진행자의 얼굴이 보이지 않는 경우 음성 인식(목소리 톤, 발성 등 개인적인 음성 특징 인식)을 통하여 각 진행자에 맞는 자막을 제공할 수도 있다. 이하, 인공지능을 이용한 다국어 자막송출 시스템의 구체적인 구성을 설명한다. 사용자 단말은 복수의 사용자들(100-1,100-2,..100-M)을 포함할 수 있다. 복수의 사용자들(100-1,100- 2,..100-M)은 각각 다국어 자막송출 시스템을 통한 자막 제공을 원하는 자를 의미하며, 개별 사용자 또는 법인 등 다양한 형태로 이루어질 수 있다. 동영상은 복수의 동영상들(300-1,300-2,..300-N)을 포함할 수 있다. 복수의 동영상들(300-1,300-2,..300- N)은 다양한 컨텐츠가 수록된 동영상일 수 있으며, 동영상에는 특별한 제한이 없으며, 동영상의 음성은 다양한 국가의 언어로 출력될 수 있다. 도 1에 도시된 각 구성요소들은 일반적으로 네트워크(network)를 통해 연결될 수 있다. 사용자 단말은 네트워크를 통하여 다국어 자막 송출 시스템과 연결될 수 있다. 몇몇 실시예에서 사용자는 사용자 단말은 다국어 자막 송출 시스템에서 제공하는 동영상을 전달받을 수 있으며, 사용자는 사용자 단말을 통하여 원하는 자막 언어를 선택할 수 있다. 동영상은 네트워크를 통하여 다국어 자막 송출 시스템과 연결될 수 있다. 예를 들어, 복수의 동영상 들(300-1,300-2,..300-N)은 다국어 자막 송출 시스템을 통하여 다국어 자막이 입력되며 사용자 단말 로 전달될 수 있다. 이를 위하여, 다국어 자막 송출 시스템은 동영상을 송출할 수 있는 방송 센터를 포함 할 수 있다. 이러한 방송 센터는 일반적 방송국만을 의미하는 것은 아니며, 일반 사용자가 제작하여 인터넷 등 에 유포하는 동영상을 포함할 수 있다. 동영상은 MP4, AVI, MOV와 같은 동영상 파일의 포맷이 될 수 있으나, 이 에 한정되는 것은 아니다. 네트워크는, 복수의 단말 및 서버들과 같은 각각의 노드 상호 간에 정보 교환이 가능한 연결 구조를 의미하는 것으로, 이러한 네트워크의 일 예에는 RF, 3GPP(3rd Generation Partnership Project) 네트워크, LTE(Long Term Evolution) 네트워크, 5GPP(5rd Generation Partnership Project) 네트워크, WIMAX(World Interoperability for Microwave Access) 네트워크, 인터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), 블루투스 (Bluetooth) 네트워크, NFC 네트워크, 위성 방송 네트워크, 아날로그 방송 네트워크, DMB(Digital Multimedia Broadcasting) 네트워크 등이 포함되나 이에 한정되지는 않는다. 사용자 단말은, 네트워크를 통하여 원격지의 서버나 단말에 접속할 수 있는 컴퓨터로 구현될 수 있다. 여 기서, 컴퓨터는 예를 들어, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱(Desktop), 랩톱(Laptop) 등을 포함할 수 있다. 이때, 사용자 단말 및 동영상은, 네트워크를 통해 원격지의 서버나 단말에 접속할 수 있는 단말로 구현될 수 있다. 사용자 단말 및 동영상은, 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, 네비게이션, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)- 2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말, 스마트폰 (smartphone), 스마트 패드(smartpad), 타블렛 PC(Tablet PC) 등과 같은 모든 종류의 핸드헬드(Handheld) 기반 의 무선 통신 장치를 포함할 수 있다. 다국어 자막 송출 시스템은, 다국어 자막을 제공하는 동영상 등을 제공하는 웹 페이지, 앱 페이지, 프로그 램 또는 애플리케이션을 제공하는 서버일 수 있다. 그리고, 다국어 자막 송출 시스템은, 사용자 단말(10 0)로부터 원하는 동영상 및/또는 자막에 대한 정보를 수신하고, 사용자의 요청에 대한 동영상 및 자막을 사용자 단말로 전송하는 서버일 수 있다. 여기서, 자막에 대한 정보는 사용자가 원하는 자막 언어에 대한 정보를포함할 수 있다. 도 2를 참조하면, 다국어 자막 송출 시스템은, 표시부, 영상분석 서버, 텍스트처리 서버, 스트리밍 서버, 서버 저장부 및 사용자 단말 연동부를 포함할 수 있다. 전술한 바와 같이, 다국 어 자막 송출 시스템은 방송 센터를 더 포함할 수도 있다. 여기서, 방송 센터는 동영상을 수신 및 송신하 는 통신부를 더 포함할 수 있다. 통신부(는 유/무선 통신망을 통해 내부의 임의의 구성 요소 또는 외부의 임의의 적어도 하나의 단말기와 통신 연결한다. 이때, 상기 외부의 임의의 단말기는 사용자 단말, 동영상 등을 포함할 수 있다. 여기서, 무선 인터넷 기술로는 무선랜 (Wireless LAN: WLAN), DLNA(Digital Living Network Alliance), 와이브로 (Wireless Broadband: Wibro), 와이 맥스(World Interoperability for Microwave Access: Wimax), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), IEEE 802.16, 롱 텀 에볼루션 (Long Term Evolution: LTE), LTE- A(Long Term Evolution-Advanced), 광대역 무선 이동 통신 서비스(Wireless Mobile Broadband Service: WMBS) 등이 있으며, 상기 통신부는 상기에서 나열되지 않은 인터넷 기술까지 포함한 범위에서 적어도 하나의 무선 인터넷 기술에 따라 데이터를 송수신하게 된다. 또한, 근거리 통신 기술로는 블루 투스(Bluetooth), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association: IrDA), UWB(Ultra Wideband), 지그비(ZigBee), 인접 자장 통신(Near Field Communication: NFC), 초음파 통신(Ultra Sound Communication: USC), 가시광 통신(Visible Light Communication: VLC), 와이 파이(Wi-Fi), 와이 파이 다이렉 트(Wi-Fi Direct) 등이 포함될 수 있다. 또한, 유선 통신 기술로는 전력선 통신(Power Line Communication: PLC), USB 통신, 이더넷(Ethernet), 시리얼 통신(serial communication), 광/동축 케이블 등이 포함될 수 있다. 또한, 상기 통신부는 유니버설 시리얼 버스(Universal Serial Bus: USB)를 통해 임의의 단말과 정보를 상호 전 송할 수 있다. 또한, 상기 통신부는 이동통신을 위한 기술표준들 또는 통신방식(예를 들어, GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), CDMA2000(Code Division Multi Access 2000), EV- DO(Enhanced Voice-Data Optimized or Enhanced Voice-Data Only), WCDMA(Wideband CDMA), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), LTE(Long Term Evolution), LTE- A(Long Term Evolution-Advanced) 등)에 따라 구축된 이동 통신망 상에서 기지국, 상기 사용자 단말, 상기 동영상 등과 무선 신호를 송수신한다. 여기서, 동영상은 동영상 공유 서버 등에 저장되어 있을 수 있으며, 통신부는 동 영상 공유 서버 등에 등과 무선 신호를 송수신할 수도 있다. 몇몇 실시예에서 표시부는 사용자에게 제공될 동영상 정보를 표시할 수 있다. 예를 들어, 표시부는 사용자에게 동영상 정보를 항목별로 구별하여 사용자에게 제공할 수 있다. 예를 들어, 표시부는 사용자에게 동 영상의 컨텐츠에 대한 정보, 동영상이 제공할 수 있는 자막 언어에 대한 정보, 동영상의 출연자에 대한 정보, 동영상 음성 언어에 대한 정보, 동영상에 제작된 국가에 대한 정보, 동영상 재생 시간에 대한 정보, 동영상 평 가에 대한 정보, 동영상 자막 평가에 대한 정보 등을 제공할 수 있다. 이와 같은 정보 전달을 위하여, 표시부는 사용자가 구비한 장치에 연결될 수 있다. 예를 들어, 스마트 폰, 태블릿 컴퓨터, 노트북 컴퓨터, PDA(personal digital assistant), PMP(portable multimedia player), 웨어러 블 디바이스(wearable device) 등에 연결될 수 있다. 몇몇 실시예에서 표시부는 사용자 인터페이스 및/또는 그래픽 사용자 인터페이스를 이용하여 다양한 메뉴 화면 등과 같은 다양한 콘텐츠를 표시할 수 있다. 여기서, 표시부에 표시되는 콘텐츠는 다양한 텍스트 또 는 이미지 데이터(각종 정보 데이터 포함)와 아이콘, 리스트 메뉴, 콤보 박스 등의 데이터를 포함하는 메뉴 화 면 등을 포함할 수도 있다. 몇몇 실시예에서 영상분석 서버는 각각의 동영상의 음성을 추출하고 음성시간을 분석하는 서버일 수 있다. 이를 위하여, 영상분석 서버는 음성 인식부, 음성 추출부, 인공지능 음성 분석부를 포함할 수 있다. 몇몇 실시예에서는 음성 인식부는 통신부로부터 동영상을 전달받고, 동영상에 포함된 음성을 인식할 수 있 다. 음성 인식부는 동영상의 음성을 분석하여 화자가 구사하는 언어의 종류에 대한 데이터, 화자의 수에 대한 데이터 등을 포함하는 음성 인식 데이터를 생성하여 음성 추출부 및 인공지능 음성 분석부로 전달할 수 있다. 음성 추출부는 음성 인식부로부터 전달받은 음성 인식 데이터를 기반으로 각각 화자 및 언어의 종류별로 음성을 추출한 음성 추출 데이터를 생성할 수 있다. 예를 들어, 화자 1은 불어, 화자 2는 영어, 화자 3은 중국어를 사용하는 경우 각각 화자 1에 음성을 추출한 제1 음성 추출 데이터, 화자 2에 대한 음성을 추출한 제2 음성 추출 데이터, 화자 3에 대한 음성을 추출한 제3 음성 추출 데이터를 각각 생성하여 인공지능 음성 분석부 및 텍스트 처리 서버에 각각 전달할 수 있다. 인공지능 음성 분석부는 동영상 데이터 및 음성 인식 데이터 및 음성 추출 데이터를 딥 러닝(deep learning) 신 경망 회로를 통하여 해당 데이터를 학습함과 동시에 음성 인식부의 음성 인식 데이터 및 음성 추출부의 음성 추 출 데이터가 동영상 컨텐츠와 일치하는지 여부를 분석한 후 인공지능 음성 분석 데이터를 생성하여 텍스트처리 서버에 전달할 수 있다. 인공지능 음성 분석 데이터는 음성 인식부의 음성 인식 데이터 및 음성 추출부의 음성 추출 데이터가 동영상 컨텐츠와 일치하는 경우, 일치함이 기록된 제1 인공지능 음성 분석 데이터를 생성하 여 텍스트처리 서버에 전달할 수 있으며, 음성 인식부의 음성 인식 데이터 및 음성 추출부의 음성 추출 데 이터가 동영상 컨텐츠와 일치하지 않는 경우, 불일치함이 기록된 제2 인공지능 음성 분석 데이터를 생성하여 텍 스트처리 서버에 전달할 수 있다. 인공지능 음성 분석부는 제2 인공지능 음성 분석 데이터를 생성하여 텍스트처리 서버에 전달하는 경우, 자 체적으로 분석된 인공지능 음성 인식 데이터 및 인공지능 음성 추출 데이터 생성하여 텍스트처리 서버에 전달할 수 있다. 인공지능 음성 분석부는 음성 인식부의 음성 인식 데이터 및 음성 추출부의 음성 추출 데이터 및 동영상 컨텐츠 를 분석하여, 같은 국가의 같은 방송을 보는 사용자라고 판단되거나, 종래 자막 생성이 완료된 동영상과 동일한 동영상이라고 판단되는 경우 제3 인공지능 분석 데이터를 생성하여 텍스트처리 서버로 전달할 수 있다. 또한, 인공지능 음성 분석부는 음성 추출 데이터를 전달받고, 음성 추출 데이터에서 텍스트를 추출하여 제1 인 공지능 텍스트 추출 데이터를 생성할 수 있다. 또한, 인공지능 음성 분석부는 제1 인공지능 텍스트 추출 데이터 의 전후 발화의 문맥 흐름을 분석하여 정확도가 높은 단어를 재선택하여 제2 인공지능 텍스트 추출 데이터를 생 성하여, 텍스트 처리 서버에 전달할 수 있다. 제1 인공지능 텍스트 추출 데이터는 동영상의 화자의 발화 문장을 추출한 제1 언어에 대한 데이터이며, 제2 인공지능 텍스트 추출 데이터는 제1 인공지능 텍스트 추출 데 이터의 전후 발화의 문맥 흐름을 분석하여 정확도 높은 단어로 재선택한 제1 언어에 대한 데이터이다. 인공지능 음성 분석부의 텍스트 추출은 동영상 파일 또는 영상 스트림의 오디오 신호로부터 STT(Speech To Text) 프로세스를 이용하여 생성되는 STT 텍스트 정보로부터 획득될 수 있다. 몇몇 실시예에서 인공지능 음성 분석부는 음성 인식 데이터 및 음성 추출 데이터에 기초하여, 타임라인별 오디 오 레벨 변화량을 타임라인 인덱스에 매핑하여 타임라인 데이터를 생성할 수 있다. 이에 따라 동영상 정보에서 의 대화 시작점과 오디오 레벨 변화량에 기초한 대화 시작점의 비교 기준을 결정할 수 있다. 또한, 인공지능 음 성 분석부는 생성된 타임라인 데이터를 상기 인공지능 번역부 및 스트리밍 서버에 전달할 수 있다. 텍스트처리 서버는 영상분석 서버로부터 음성 인식 데이터, 음성 추출 데이터, 제2 인공지능 텍스트 추출 데이터, 동영상을 전달받고 다국어 번역 및 자막을 생성할 수 있다. 또한, 텍스트처리 서버는 인공지능 음성 분석부로부터 제1 인공지능 음성 분석 데이터를 전달받는 경우 음 성 추출부로부터 전달받은 음성 추출 데이터를 기반으로 다국어 번역 및 자막을 생성할 수 있다. 또한, 텍스트 처리 서버는 인공지능 음성 분석부로부터 제2 인공지능 음성 분석 데이터를 전달받는 경우 인공지능 분석 부로부터 전달받은 인공지능 음성 추출 데이터를 기반으로 다국어 번역 및 자막을 생성할 수 있다. 몇몇 실시예에서 텍스트처리 서버는 인공지능 번역부 및 자막 생성부를 포함할 수 있다. 인공지능 번역부에서는 음성 추출 데이터, 제2 인공지능 텍스트 추출 데이터와 함께, 동영상의 화자가 발화 문 장을 발화하는데 따른 입술의 움직임을 포함하는 입술 움직임 영상 정보를 딥 러닝(deep learning) 신경망 회로 로 입력하여 상기 발화 문장에 대한 번역 문장을 도출할 수 있다. 여기서, 상기 딥 러닝(deep learning) 신경망 회로는, 복수 종류의 문장 구성 단위(예를 들어, 형태소, 단어, 어절, 문장 등)에 대하여, 각 종류별로 상기 번 역 문장을 구성하는 문장 구성 단위 및 그에 대한 매칭 확률을 도출하고, 이어서 상기 문장 구성 단위의 각 종 류별로 상기 번역 문장 전체에 대한 매칭 확률을 산출하여 어느 종류의 문장 구성 단위에서 번역 문장을 도출할 것인지를 결정한 후, 상기 제1 언어의 발화 문장에 대한 제2 언어의 번역 문장을 도출하게 된다. 여기서, 제2 언어는 제1 언어와 다른 언어이며, 제2 언어는 복수개 일 수 있다. 나아가, 상기 딥 러닝(deep learning) 신경망 회로는 문장 구성 단위 및 확률 도출 수단, 문장 구성 단위 종류 결정 수단 및 번역 문장 도출 수단을 포함할 수 있다. 이때, 상기 문장 구성 단위 및 확률 도출 수단은 복수 종류의 문장 구성 단위에 대하여 각 종류별로 상기 번역 문장을 구성하는 문장 구성 단위 및 그에 대한 매칭 확률 을 도출하게 된다. 또한, 상기 문장 구성 단위 종류 결정 수단에서는 상기 도출된 각 문장 구성 단위 및 각 매 칭 확률을 고려하여, 각 종류별로 상기 번역 문장 전체에 대한 매칭 확률을 산출하여, 어느 종류의 문장 구성 단위에서 번역 문장을 도출할 것인지를 결정하게 된다. 나아가, 상기 문장 구성 단위 종류 결정 수단에서는 상기 문장 구성 단위 중 보다 큰 단위의 문장 구성 단위에 대하여 보다 작은 단위의 문장 구성 단위보다 높을 가중치를 부여할 수도 있다. 예를 들어, 문장 단위에서의 번 역 문장 전체에 대한 매칭 확률과 어절 단위에서의 번역 문장 전체에 대한 매칭 확률이 동일할 경우, 보다 큰 단위의 문장 구성 단위인 문장에 대하여 소정의 가중치를 부여함으로써, 문장 단위에서 번역 문장을 도출하도록 결정할 수도 있다. 마지막으로, 상기 번역 문장 도출 수단에서는 상기 결정된 종류의 문장 구성 단위를 사용하 여 상기 제2 언어의 번역 문장을 도출할 수 있다. 인공지능 번역부는 신경망 기술을 이용하여 정확하고 자연스 러운 번역을 함과 동시에 지속적인 추가 학습을 통하여 번역 성능을 지속적으로 향상시킬 수 있다. 인공지능 번역부는 동영상 및 제2 인공지능 텍스트 추출 데이터를 기반으로 번역을 진행하여 번역 데이터를 생 성하여 자막 생성부 및 서버 저장부에 전달할 수 있다. 예를 들어, 대한민국 동영상을 미국의 사용자에게 전달하는 경우, 제1 언어는 한국어가 될 수 있으며, 제2 언어는 영어가 될 수 있고, 번역 데이터는 영어 번역 데이터일 수 있다. 다만, 이에 한정되는 것은 아니며, 사용자의 요청에 따라 제2 언어는 다양하게 변화될 수도 있다. 인공지능 번역부는 제2 언어 설정부를 더 포함할 수 있고, 제2 언어 설정부로부터 번역되어야 하는 국가 의 언어를 입력받을 수 있다. 전술한 바와 같이 제2 언어는 복수개로 설정될 수 있다. 또한, 인공지능 번역부는 인공지능 음성 분석부로부터 제3 인공지능 분석 데이터를 전달받는 경우 서버 저장부 에 기저장된 번역 데이터를 선별하여 자막 생성부에 전달할 수 있다. 자막 생성부는 인공지능 번역부로부터 번역 데이터를 전달받고 자막 데이터를 생성할 수 있으며, 생성된 자막 데이터를 스트리밍 서버에 전달할 수 있다. 스트리밍 서버는 영상분석 서버로부터 전달받은 타임라인 데이터와 텍스트처리 서버로부터 전달 받은 자막 데이터를 바탕으로 동영상 원본에 자막을 제공할 수 있다. 스트리밍 서버는 타임라인 데이터를 통하여 음성인식 시간에 맞춰 이에 해당되는 자막 데이터를 동영상에 표시할 수 있다."}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "몇몇 실시예에서 스트리밍 서버는 자막 제공부, 요약 정보 제공부, 부가 서비스부를 포함할 수 있다. 자막 제공부는 전술한 바와 같이, 타임라인 데이터를 통하여 음성인식 시간에 맞춰 이에 해당되는 자막 데이터 를 동영상에 표시할 수 있다, 또한, 자막 제공부는 사용자의 IP를 감지하고, 해당 IP 국가에 해당되는 언어 자 막을 제공할 수도 있으며, 또한, 자막 제공부는 언어설정부로부터 입력된 언어 자막을 제공할 수도 있고, 자막 제공부는 복수의 언어 자막을 제공할 수도 있다. 몇몇 실시예에서 자막 제공부는 안면 인식부를 더 포함할 수 있으며, 안면 인식부는 동영상의 화자가 발화 문장 을 발화하는데 따른 입술의 움직임을 포함하는 입술 움직임 영상 정보를 딥 러닝(deep learning) 신경망 회로로 입력하여 상기 발화자 및 발화 타임에 맞춰 자막 데이터를 동영상에 표시할 수도 있다."}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "요약 정보 제공부는 현재 제공되는 자막의 언어에 대한 정보, 제공가능한 자막에 대한 정보, 동영상에 대한 정 보, 동영상 재생 시간에 대한 정보, 동영상 컨텐츠에 대한 정보 등을 표시할 수 있다. 부가 서비스부는 자막 평가에 대한 정보를 제공할 수 있다. 예를 들어, 부가서비스는, 동영상 재생이 완료되거 나, 동영상 재생이 중단된 경우, 자막에 대한 정확성, 자막에 대한 가독성 등에 대한 정보를 입력받을 수 있으 며, 입력된 정보를 바탕으로 자막 평가 데이터를 생성하여 동영상과 함께 사용자에게 제공할 수 있다. 도 4를 참조하면, 사용자 단말 연동부는 입력부(470A)와, 신뢰성 검증부(470B), 데이터 베이스부(470C), 정보제공부(470D) 및 전송부(470D)를 포함할 수 있다. 입력부(470A)는 사용자 단말에 사용자에 대한 정보를 입력할 수 있는 웹페이지를 제공할 수 있다. 입력부 (470A)는 사용자 단말을 통하여 입력된 사용자에 대한 정보를 신뢰성 검증부(470B), 데이터 베이스부 (470C) 및 전송부(470D)에 전달할 수 있다. 여기서, 사용자 단말을 통하여 입력된 정보는 사용자의 인적사 항에 대한 정보 등을 포함할 수 있다. 또한, 입력부(470A)는 사용자에 대한 정보의 입력이 완료된 경우 사용자 단말에 사용자에 대한 정보에 대 한 입력이 완료되었다는 메시지를 전송할 수 있다.몇몇 실시예에서 입력부(470A)는 회원 인증 서비스를 제공할 수도 있다. 회원 인증 서비스는 매수인의 아이디 및 패스워드를 입력 받아 이를 수집하고, 로그인 정보로 활용할 수 있다. 몇몇 실시예에서 입력부(470A)는 페이 스북(Face book)이나 트위터(Twitter) 등과 같은 SNS와 연계하여 SNS 정보를 로그인 정보로 활용할 수도 있다. 신뢰성 검증부(470B)는, 데이터 베이스부(470C)에 저장된 인적 사항에 대한 데이터를 기반으로 사용자가 입력한 인적사항의 신뢰성을 판단할 수 있다. 또한, 신뢰성 검증부(470B)는 인적사항의 진위여부를 확인하기 위하여 사 용자의 핸드폰에 확인 요청을 진행할 수도 있다. 데이터 베이스부(470C)는 입력부(470A)에 입력된 사용자에 대한 정보 및 신뢰성 검증에 대한 정보, 사용자의 아 이디 및 패스워드, 결제에 대한 정보 등을 저장, 제어 및 관리할 수 있다. 또한, 데이터 베이스부(470C)는 사용 자 단말을 통하여 수정된 정보가 입력되는 경우 저장된 정보 등을 수정하여 저장할 수도 있다. 또한, 사용 자는 사용자 단말을 통하여 데이터 베이스부(470C)에 접근이 가능하고, 본인의 인적사항에 대한 정보, 신 뢰성 검증에 대한 정보, 수수료 정보 및 결제 정보 등을 확인할 수 있다. 몇몇 실시예에서 데이터 베이스부(470C)에는 사용자에 대한 정보가 축적 및 저장될 수 있다. 몇몇 실시예에서 데이터 베이스부(470C)는 저장부를 포함할 수 있다. 저장부는 RAM(Random Access Memory), 플 레시 메모리, ROM(Read Only Memory), EPROM(Erasable Programmable ROM), EEPROM(Electronically Erasable and Programmable ROM), 레지스터, 하드디스크, 리무버블 디스크, 메모리 카드 등과 같은 내장된 형태의 저장소 자는 물론, USB 메모리 등과 같은 착탈가능한 형태의 저장 소자로 구현될 수도 있다. 정보제공부(470D)는 데이터 베이스부(470C)에 입력된 정보를 바탕으로 사용자에게 유용한 정보를 산출할 수 있 다. 예를 들어, 사용자의 접속 시간대, 사용자의 월평균 접속시간, 사용자와 유사한 연령의 평균 접속시간 등을 분석하여 사용자의 접속 및 사용 정보를 종합적으로 산출할 수 있다. 전송부(470D)는 데이터 베이스부(470C) 및 정보제공부(470D)에서 제공되는 정보들을 서버 저장부 및 사용 자 단말로 전송할 수 있다. 도 5를 참조하면, 인공지능을 이용한 다국어 자막송출 시스템의 자막 제공 단계는, 다국어 자막 송출 시스 템이 동영상을 수신하고(S10), 영상분석 서버가 영상을 분석하며(S20), 텍스트처리 서버가 인공 지능을 이용하여 번역 및 자막을 생성하며(S30), 스트리밍 서버가 동영상 영상시간에 맞춰 자막을 제공하 는(S40, S50) 단계를 포함한다. 특히, 다국어 자막 송출 시스템에 포함된 SST 미들웨어는 방송 번호를 체크하고, 진행중인 방송인 경우 국 가를 체크하여, 방송번호SEQ + 국가번호SEQ를 결합하여 데몬에서 검색을 진행할 수 있다. 검색한 결과 해당 데 이터가 있는 경우 같은 방송 같은 나라를 청취하고 있는 사람들에게 자막을 전송할 수 있다. 검색한 결과 해당 데이터가 없는 경우 방송번호SEQ + 국가번호SEQ를 결합하여 데몬서버에 쓰레드를 생성하여 실 시간으로 자막을 생성 및 저장하고, 같은 방송 같은 나라를 정취하고 있는 사람들에게 자막을 전송할 수 있다. 따라서, 본 발명에 의하면 실시간 방송에 신뢰성 높은 다국적 자막을 표시할 수 있게 된다. 또한, 본 발명에 의 하면 기존 동영상에 신뢰성 높은 다국적 자막을 표시할 수도 있다."}
{"patent_id": "10-2022-0117538", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이상 첨부된 도면을 참조하여 본 발명의 실시예들을 설명하였지만, 본 발명이 속하는 기술분야에서 통상의 지식 을 가진 자는 본 발명의 그 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 실시될 수 있다는 것을 이해할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정 적이 아닌 것으로 이해해야만 한다."}
{"patent_id": "10-2022-0117538", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 인공지능을 이용한 다국어 자막송출 시스템의 연동 구조를 개략적으로 나타낸 도면이 다. 도 2는 일 실시예에 따른 다국어 자막 송출 시스템의 구성을 개략적으로 나타낸 도면이다. 도 3은 일 실시예에 따른 영상분석 서버, 텍스트처리 서버, 스트리밍 서버의 구성을 개략적으로 나타낸 도면이 다. 도 4는 일 실시예에 따른 사용자 단말 연동부를 개략적으로 나타낸 도면이다. 도 5는 일 실시예에 따른 인공지능을 이용한 다국어 자막송출 시스템의 자막 송출 방법을 설명하기 위한 도면이 다."}
