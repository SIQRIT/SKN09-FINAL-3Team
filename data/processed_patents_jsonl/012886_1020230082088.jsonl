{"patent_id": "10-2023-0082088", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0000356", "출원번호": "10-2023-0082088", "발명의 명칭": "로봇 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "예동희"}}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇에 있어서,카메라;구동부; 및상기 카메라를 통해 획득된 이미지에 기초하여 상기 로봇 주변에 위치한 적어도 하나의 사용자를 인식하고, 상기 카메라의 화각 내의 복수의 영역 중 상기 적어도 하나의 사용자가 위치하는 적어도 하나의 타겟 영역의 위치 및 상기 적어도 하나의 타겟 영역에 대한 응시 순서에 기초하여 상기 로봇의 회전 각도 및 회전 방향을 식별하고,상기 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는 하나 이상의프로세서를 포함하고, 상기 하나 이상의 프로세서는,상기 로봇이 회전하는 동안 상기 카메라를 통해 획득된 이미지에 기초하여 새로운 사용자가 인식되면, 상기 회전된 로봇의 카메라의 화각 내의 복수의 영역 중 상기 새로운 사용자가 위치하는 새로운 타겟 영역이 상기 응시순서에 포함되도록 상기 응시 순서를 조정하고,상기 새로운 타겟 영역의 위치 및 상기 조정된 응시 순서에 기초하여 상기 로봇의 회전 각도 및 회전 방향을 재식별하고, 상기 재 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 하나 이상의 프로세서는,상기 카메라의 화각을 기 설정된 각도로 분할하여 상기 복수의 영역을 식별하고, 상기 적어도 하나의 타겟 영역이 복수이면, 상기 응시 순서에 기초하여 상기 복수의 타겟 영역 중 최초 타겟 영역을 식별하고, 상기 최초 타겟 영역의 위치에 기초하여 상기 최초 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하고, 상기 복수의 타겟 영역 중 나머지 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 상기 나머지 타겟 영역 이전 순서의 타겟 영역의 위치 및 상기 나머지 타겟 영역의 위치에 기초하여 식별하고,상기 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 상기 응시 순서에 따라 회전하도록 상기 구동부를제어하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 하나 이상의 프로세서는,상기 회전된 로봇의 카메라의 화각을 기 설정된 각도로 분할하여 복수의 영역을 식별하고, 상기 회전된 로봇의 카메라를 통해 획득된 이미지에 기초하여 상기 새로운 사용자가 인식되면, 상기 복수의 영역 중 상기 새로운 사용자가 위치하는 상기 새로운 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하고,공개특허 10-2025-0000356-3-상기 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 하나 이상의 프로세서는,상기 적어도 하나의 타겟 영역이 복수이면, 상기 조정된 응시 순서에 기초하여 미응시 타겟 영역 및 상기 새로운 타겟 영역 중 상기 회전된 로봇이 응시하고 있는 타겟 영역의 다음 순서의 다음 타겟 영역을 식별하고, 상기 다음 타겟 영역의 위치에 기초하여 상기 다음 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하고, 상기 미응시 타겟 영역 및 상기 새로운 타겟 영역 중 상기 다음 타겟 영역 이후의 나머지 타겟 영역에 대응되는상기 로봇의 회전 각도 및 회전 방향을 상기 나머지 타겟 영역 이전 순서의 타겟 영역의 위치 및 상기 나머지타겟 영역의 위치에 기초하여 식별하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 하나 이상의 프로세서는,상기 다음 타겟 영역이 상기 새로운 타겟 영역이면, 상기 새로운 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하고, 상기 새로운 타겟 영역의 위치에 기초하여 상기 미응시 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전방향을 재식별하고, 상기 로봇이 상기 새로운 타겟 영역을 응시한 후 상기 재식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,상기 하나 이상의 프로세서는,상기 다음 타겟 영역이 상기 미응시 타겟 영역이면, 상기 새로운 타겟 영역의 위치 및 상기 새로운 타겟 영역이전의 미응시 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전방향을 식별하고,상기 로봇이 상기 새로운 타겟 영역 이전의 미응시 타겟 영역을 응시한 후 상기 새로운 타겟 영역에 대응되는상기 로봇의 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 하나 이상의 프로세서는,상기 새로운 타겟 영역을 응시하기 위한 상기 로봇의 회전 각도가 기 설정된 임계 각도 이상이면, 상기 새로운타겟 영역을 상기 응시 순서에 포함시키지 않고 상기 응시 순서를 유지하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 하나 이상의 프로세서는,상기 카메라를 통해 획득된 이미지에 기초하여 상기 로봇 주변에 위치한 복수의 제1 사용자를 식별하고, 공개특허 10-2025-0000356-4-상기 복수의 제1 사용자 간의 거리, 상기 복수의 제1 사용자가 상기 카메라의 화각 내 위치한 시간 및 상기 복수의 제1 사용자 간의 대화 여부 중 적어도 하나에 기초하여, 상기 복수의 제1 사용자 중 동일한 사용자 그룹에포함되는 복수의 사용자를 식별하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,사용자 음성을 수신하는 마이크;를 더 포함하고, 상기 하나 이상의 프로세서는,상기 적어도 하나의 사용자와 상기 적어도 하나의 타겟 영역이 복수이면, 상기 카메라를 통해 획득된 이미지에기초하여, 상기 복수의 사용자의 상기 로봇에 대한 응시 시간을 식별하고,상기 마이크를 통해 획득된 사용자 음성에 기초하여 상기 복수의 사용자의 상기 로봇에 대한 음성 입력 횟수를식별하고, 상기 식별된 응시 시간 및 음성 입력 횟수를 기초로, 상기 복수의 사용자의 인터렉션 스코어를 산정하고, 각 타겟 영역에 위치한 적어도 하나의 사용자의 인터렉션 스코어에 기초하여, 상기 각 타겟 영역에 대응하는 인터렉션 스코어를 식별하고, 상기 식별된 각 타겟 영역에 대응하는 인터렉션 스코어에 기초하여 상기 복수의 타겟 영역에 대한 응시 순서를결정하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 하나 이상의 프로세서는,상기 식별된 각 타겟 영역에 대응하는 인터렉션 스코어에 기초하여, 상기 각 타겟 영역에 대한 응시 시간을 결정하고, 상기 로봇이 상기 결정된 응시 시간 동안 상기 각 타겟 영역을 응시하도록 상기 구동부를 제어하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "로봇을 제어하는 방법에 있어서, 카메라를 통해 획득된 이미지에 기초하여 상기 로봇 주변에 위치한 적어도 하나의 사용자를 인식하는 단계;상기 카메라의 화각 내의 복수의 영역 중 상기 적어도 하나의 사용자가 위치하는 적어도 하나의 타겟 영역의 위치 및 상기 적어도 하나의 타겟 영역에 대한 응시 순서에 기초하여 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계;상기 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는 단계;상기 로봇이 회전하는 동안 상기 카메라를 통해 획득된 이미지에 기초하여 새로운 사용자가 인식되면, 상기 회전된 로봇의 카메라의 화각 내의 복수의 영역 중 상기 새로운 사용자가 위치하는 새로운 타겟 영역이 상기 응시순서에 포함되도록 상기 응시 순서를 조정하는 단계;상기 새로운 타겟 영역의 위치 및 상기 조정된 응시 순서에 기초하여 상기 로봇의 회전 각도 및 회전 방향을 재식별하는 단계; 및 상기 재 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는 단계를 포함하는, 제어 방법."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계는, 공개특허 10-2025-0000356-5-상기 카메라의 화각을 기 설정된 각도로 분할하여 상기 복수의 영역을 식별하는 단계;상기 적어도 하나의 타겟 영역이 복수이면, 상기 응시 순서에 기초하여 상기 복수의 타겟 영역 중 최초 타겟 영역을 식별하고, 상기 최초 타겟 영역의 위치에 기초하여 상기 최초 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계; 및 상기 복수의 타겟 영역 중 나머지 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 상기 나머지 타겟 영역 이전 순서의 타겟 영역의 위치 및 상기 나머지 타겟 영역의 위치에 기초하여 식별하는 단계를포함하는, 제어 방법."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 로봇의 회전 각도 및 회전 방향을 재 식별하는 단계는, 상기 회전된 로봇의 카메라의 화각을 기 설정된 각도로 분할하여 복수의 영역을 식별하는 단계; 및상기 회전된 로봇의 카메라를 통해 획득된 이미지에 기초하여 상기 새로운 사용자가 인식되면, 상기 복수의 영역 중 상기 새로운 사용자가 위치하는 상기 새로운 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계를 더 포함하는, 제어 방법."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계는, 상기 적어도 하나의 타겟 영역이 복수이면, 상기 조정된 응시 순서에 기초하여 미응시 타겟 영역 및 상기 새로운 타겟 영역 중 상기 회전된 로봇이 응시하고 있는 타겟 영역의 다음 순서의 다음 타겟 영역을 식별하는 단계; 상기 다음 타겟 영역의 위치에 기초하여 상기 다음 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계; 및상기 미응시 타겟 영역 및 상기 새로운 타겟 영역 중 상기 다음 타겟 영역 이후의 나머지 타겟 영역에 대응되는상기 로봇의 회전 각도 및 회전 방향을 상기 나머지 타겟 영역 이전 순서의 타겟 영역의 위치 및 상기 나머지타겟 영역의 위치에 기초하여 식별하는 단계를 더 포함하는, 제어 방법."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 다음 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계는, 상기 다음 타겟 영역이 상기 새로운 타겟 영역이면, 상기 새로운 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하고, 상기 나머지 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계는, 상기 새로운 타겟 영역의 위치에 기초하여 상기 미응시 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전방향을 재식별하고, 상기 재 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는 단계는,상기 로봇이 상기 새로운 타겟 영역을 응시한 후 상기 재식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서,상기 다음 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계는, 공개특허 10-2025-0000356-6-상기 다음 타겟 영역이 상기 미응시 타겟 영역이면, 상기 새로운 타겟 영역의 위치 및 상기 새로운 타겟 영역이전의 미응시 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전방향을 식별하고,상기 재 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는 단계는, 상기 로봇이 상기 새로운 타겟 영역 이전의 미응시 타겟 영역을 응시한 후 상기 새로운 타겟 영역에 대응되는상기 로봇의 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는, 제어 방법."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항에 있어서,상기 응시 순서를 조정하는 단계는, 상기 새로운 타겟 영역을 응시하기 위한 상기 로봇의 회전 각도가 기 설정된 임계 각도 이상이면, 상기 새로운타겟 영역을 상기 응시 순서에 포함시키지 않고 상기 응시 순서를 유지하는, 로봇."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항에 있어서,상기 카메라를 통해 획득된 이미지에 기초하여 상기 로봇 주변에 위치한 복수의 제1 사용자를 식별하는 단계;및상기 복수의 제1 사용자 간의 거리, 상기 복수의 제1 사용자가 상기 카메라의 화각 내 위치한 시간 및 상기 복수의 제1 사용자 간의 대화 여부 중 적어도 하나에 기초하여, 상기 복수의 제1 사용자 중 동일한 사용자 그룹에포함되는 복수의 사용자를 식별하는 단계를 더 포함하는, 제어 방법."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항에 있어서,상기 적어도 하나의 사용자와 상기 적어도 하나의 타겟 영역이 복수이면, 상기 카메라를 통해 획득된 이미지에기초하여, 상기 복수의 사용자의 상기 로봇에 대한 응시 시간을 식별하는 단계; 상기 로봇의 마이크를 통해 획득된 사용자 음성에 기초하여 상기 복수의 사용자의 상기 로봇에 대한 음성 입력횟수를 식별하는 단계;상기 식별된 응시 시간 및 음성 입력 횟수를 기초로, 상기 복수의 사용자의 인터렉션 스코어를 산정하는 단계;각 타겟 영역에 위치한 적어도 하나의 사용자의 인터렉션 스코어에 기초하여, 상기 각 타겟 영역에 대응하는 인터렉션 스코어를 식별하는 단계; 및상기 식별된 각 타겟 영역에 대응하는 인터렉션 스코어에 기초하여 상기 복수의 타겟 영역에 대한 응시 순서를결정하는 단계를 포함하는, 제어 방법."}
{"patent_id": "10-2023-0082088", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서,상기 구동부를 제어하는 단계는, 상기 식별된 각 타겟 영역에 대응하는 인터렉션 스코어에 기초하여, 상기 각 타겟 영역에 대한 응시 시간을 결정하고, 상기 로봇이 상기 결정된 응시 시간 동안 상기 각 타겟 영역을 응시하도록 상기 구동부를 제어하는, 제어 방법."}
{"patent_id": "10-2023-0082088", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "로봇 및 그 제어 방법을 제공한다. 로봇은 카메라, 구동부 및 하나 이상의 프로세서를 포함한다. 하나 이상의 프로세서는 카메라를 통해 획득된 이미지에 기초하여 로봇 주변에 위치한 적어도 하나의 사용자를 인식하고, 카 메라의 화각 내의 복수의 영역 중 적어도 하나의 사용자가 위치하는 적어도 하나의 타겟 영역의 위치 및 적어도 하나의 타겟 영역에 대한 응시 순서에 기초하여 로봇의 회전 각도 및 회전 방향을 식별하고, 식별된 회전 각도 및 회전 방향에 기초하여 로봇이 회전하도록 구동부를 제어한다."}
{"patent_id": "10-2023-0082088", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 로봇 및 그 제어 방법에 관한 것이다. 보다 구체적으로, 복수의 사용자에 대한 응시 순서를 결정하고, 결정된 응시 순서에 기초하여 복수의 사용자를 순차적으로 응시하는 로봇 및 그 제어 방법에 관한 것이다."}
{"patent_id": "10-2023-0082088", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 전자 기술의 발달로 다양한 산업 분야에서 로봇이 이용되고 있다. 객체 인식 기술의 발달로 로봇은 다양한 사물을 정확히 구별할 수 있게 되었고, 자율 주행 기술의 발달로 로봇은 주행 공간에서 사람의 통행을 방해하지 않으면서 안정적인 주행이 가능해졌다. 음식점에서 사용자가 주문한 음식을 서빙 하는 서빙 로봇, 공항이나 대 형 마트 등에서 사용자에게 길을 안내하는 가이드 로봇 등을 보더라도, 로봇이 이용되는 분야와 방식이 다양해 진 것을 알 수 있다. 로봇이 다양한 분야에서 이용됨에 따라, 다수의 사용자가 동시에 로봇을 이용하는 경우가 발생하기도 한다. 예 를 들어, 전시회에서 다수의 관람객이 로봇의 가이드를 받는 경우 또는 공항에서 다수의 여행객들이 로봇의 안 내를 받는 경우가 이에 해당할 수 있다."}
{"patent_id": "10-2023-0082088", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 로봇은 카메라, 구동부 및 하나 이상의 프로세서를 포함한다. 상기 하나 이상의 프로세서는 카메라를 통해 획득된 이미지에 기초하여 상기 로봇 주변에 위치한 적어도 하나의 사용자를 인식한 다. 상기 하나 이상의 프로세서는 상기 카메라의 화각 내의 복수의 영역 중 상기 적어도 하나의 사용자가 위치 하는 적어도 하나의 타겟 영역의 위치 및 상기 적어도 하나의 타겟 영역에 대한 응시 순서에 기초하여 상기 로 봇의 회전 각도 및 회전 방향을 식별한다. 상기 하나 이상의 프로세서는 상기 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어한다. 상기 하나 이상의 프로세서는, 상기 로봇이 회전하 는 동안 상기 카메라를 통해 획득된 이미지에 기초하여 새로운 사용자가 인식되면, 상기 회전된 로봇의 카메라 의 화각 내의 복수의 영역 중 상기 새로운 사용자가 위치하는 새로운 타겟 영역이 상기 응시 순서에 포함되도록 상기 응시 순서를 조정한다. 상기 하나 이상의 프로세서는 상기 새로운 타겟 영역의 위치 및 상기 조정된 응시 순서에 기초하여 상기 로봇의 회전 각도 및 회전 방향을 재 식별한다. 상기 하나 이상의 프로세서는 상기 재 식 별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어한다. 여기서, 상기 하나 이상의 프로세서는 상기 카메라의 화각을 기 설정된 각도로 분할하여 상기 복수의 영역을 식 별하고, 상기 적어도 하나의 타겟 영역이 복수이면, 상기 응시 순서에 기초하여 상기 복수의 타겟 영역 중 최초 타겟 영역을 식별하고, 상기 최초 타겟 영역의 위치에 기초하여 상기 최초 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하고, 상기 복수의 타겟 영역 중 나머지 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 상기 나머지 타겟 영역 이전 순서의 타겟 영역의 위치 및 상기 나머지 타겟 영역의 위치에 기초하여 식별하고, 상기 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 상기 응시 순서에 따라 회전 하도록 상기 구동부를 제어한다. 또한, 상기 하나 이상의 프로세서는, 상기 회전된 로봇의 카메라의 화각을 기 설정된 각도로 분할하여 복수의 영역을 식별하고, 상기 회전된 로봇의 카메라를 통해 획득된 이미지에 기초하여 상기 새로운 사용자가 인식되면, 상기 복수의 영역 중 상기 새로운 사용자가 위치하는 상기 새로운 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하고, 상기 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어한다. 여기서, 상기 하나 이상의 프로세서는, 상기 적어도 하나의 타겟 영역이 복수이면, 상기 조정된 응시 순서에 기 초하여 미응시 타겟 영역 및 상기 새로운 타겟 영역 중 상기 회전된 로봇이 응시하고 있는 타겟 영역의 다음 순 서의 다음 타겟 영역을 식별하고, 상기 다음 타겟 영역의 위치에 기초하여 상기 다음 타겟 영역에 대응되는 상 기 로봇의 회전 각도 및 회전 방향을 식별하고, 상기 미응시 타겟 영역 및 상기 새로운 타겟 영역 중 상기 다음 타겟 영역 이후의 나머지 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 상기 나머지 타겟 영역 이전 순서의 타겟 영역의 위치 및 상기 나머지 타겟 영역의 위치에 기초하여 식별한다. 여기서, 상기 하나 이상의 프로세서는, 상기 다음 타겟 영역이 상기 새로운 타겟 영역이면, 상기 새로운 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하고, 상기 새로운 타겟 영역의 위치에 기초하여 상기 미응시 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 재 식별하고, 상기 로봇이 상기 새로운 타겟 영역을 응시한 후 상기 재 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어한다. 또한, 상기 하나 이상의 프로세서는 상기 다음 타겟 영역이 상기 미응시 타겟 영역이면, 상기 새로운 타겟 영역 의 위치 및 상기 새로운 타겟 영역 이전의 미응시 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응 되는 상기 로봇의 회전 각도 및 회전 방향을 식별하고, 상기 로봇이 상기 새로운 타겟 영역 이전의 미응시 타겟 영역을 응시한 후 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향에 기초하여 상기 로 봇이 회전하도록 상기 구동부를 제어한다. 상기 하나 이상의 프로세서는, 상기 새로운 타겟 영역을 응시하기 위한 상기 로봇의 회전 각도가 기 설정된 임 계 각도 이상이면, 상기 새로운 타겟 영역을 상기 응시 순서에 포함시키지 않고 상기 응시 순서를 유지한다. 또한, 상기 하나 이상의 프로세서는, 상기 카메라를 통해 획득된 이미지에 기초하여 상기 로봇 주변에 위치한 복수의 제1 사용자를 식별하고, 상기 복수의 제1 사용자 간의 거리, 상기 복수의 제1 사용자가 상기 카메라의 화각 내 위치한 시간 및 상기 복수의 제1 사용자 간의 대화 여부 중 적어도 하나에 기초하여, 상기 복수의 제1 사용자 중 동일한 사용자 그룹에 포함되는 복수의 사용자를 식별한다. 또한, 상기 로봇은 사용자 음성을 수신하는 마이크를 더 포함하고, 상기 하나 이상의 프로세서는, 상기 적어도 하나의 사용자와 상기 적어도 하나의 타겟 영역이 복수이면, 상기 카메라를 통해 획득된 이미지에 기초하여, 상 기 복수의 사용자의 상기 로봇에 대한 응시 시간을 식별하고, 상기 마이크를 통해 획득된 사용자 음성에 기초하 여 상기 복수의 사용자의 상기 로봇에 대한 음성 입력 횟수를 식별하고, 상기 식별된 응시 시간 및 음성 입력 횟수를 기초로, 상기 복수의 사용자의 인터렉션 스코어를 산정하고, 각 타겟 영역에 위치한 적어도 하나의 사용 자의 인터렉션 스코어에 기초하여, 상기 각 타겟 영역에 대응하는 인터렉션 스코어를 식별하고, 상기 식별된 각 타겟 영역에 대응하는 인터렉션 스코어에 기초하여 상기 복수의 타겟 영역에 대한 응시 순서를 결정한다. 여기서, 상기 하나 이상의 프로세서는 상기 식별된 각 타겟 영역에 대응하는 인터렉션 스코어에 기초하여, 상기 각 타겟 영역에 대한 응시 시간을 결정하고, 상기 로봇이 상기 결정된 응시 시간 동안 상기 각 타겟 영역을 응 시하도록 상기 구동부를 제어한다. 본 개시의 일 실시 예에 따른 로봇을 제어하는 방법은 카메라를 통해 획득된 이미지에 기초하여 상기 로봇 주변 에 위치한 복수의 사용자를 인식하는 단계를 포함한다. 상기 제어 방법은 상기 카메라의 화각 내의 복수의 영역 중 상기 복수의 사용자가 위치하는 복수의 타겟 영역의 위치 및 상기 복수의 타겟 영역에 대한 응시 순서에 기 초하여 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계를 포함한다. 상기 제어 방법은 상기 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는 단계를 포함한다. 상기 제어 방 법은 상기 로봇이 회전하는 동안 상기 카메라를 통해 획득된 이미지에 기초하여 새로운 사용자가 인식되면, 상 기 회전된 로봇의 카메라의 화각 내의 복수의 영역 중 상기 새로운 사용자가 위치하는 새로운 타겟 영역이 상기 응시 순서에 포함되도록 상기 응시 순서를 조정하는 단계를 포함한다. 상기 제어 방법은 상기 새로운 타겟 영역 의 위치 및 상기 조정된 응시 순서에 기초하여 상기 로봇의 회전 각도 및 회전 방향을 재 식별하는 단계를 포함 한다. 상기 제어 방법은 상기 재 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동 부를 제어하는 단계를 포함한다. 여기서, 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계는 상기 카메라의 화각을 기 설정된 각도로 분할 하여 상기 복수의 영역을 식별하는 단계, 상기 적어도 하나의 타겟 영역이 복수이면, 상기 응시 순서에 기초하 여 상기 복수의 타겟 영역 중 최초 타겟 영역을 식별하고, 상기 최초 타겟 영역의 위치에 기초하여 상기 최초 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계 및 상기 복수의 타겟 영역 중 나머 지 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 상기 나머지 타겟 영역 이전 순서의 타겟 영역 의 위치 및 상기 나머지 타겟 영역의 위치에 기초하여 식별하는 단계를 포함한다. 또한, 상기 로봇의 회전 각도 및 회전 방향을 재 식별하는 단계는 상기 회전된 로봇의 카메라의 화각을 기 설정 된 각도로 분할하여 복수의 영역을 식별하는 단계 및 상기 회전된 로봇의 카메라를 통해 획득된 이미지에 기초 하여 상기 새로운 사용자가 인식되면, 상기 복수의 영역 중 상기 새로운 사용자가 위치하는 상기 새로운 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단 계를 더 포함한다. 여기서, 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계는 상기 적어도 하나의 타겟 영역이 복수이면, 상 기 조정된 응시 순서에 기초하여 미응시 타겟 영역 및 상기 새로운 타겟 영역 중 상기 회전된 로봇이 응시하고 있는 타겟 영역의 다음 순서의 다음 타겟 영역을 식별하는 단계, 상기 다음 타겟 영역의 위치에 기초하여 상기다음 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계 및 상기 미응시 타겟 영역 및 상기 새로운 타겟 영역 중 상기 다음 타겟 영역 이후의 나머지 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 상기 나머지 타겟 영역 이전 순서의 타겟 영역의 위치 및 상기 나머지 타겟 영역의 위치에 기초하 여 식별하는 단계를 더 포함한다. 여기서, 상기 다음 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계는, 상기 다음 타겟 영역이 상기 새로운 타겟 영역이면, 상기 새로운 타겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별한다. 상기 나머지 타겟 영역에 대응되는 상기 로봇의 회 전 각도 및 회전 방향을 식별하는 단계는 상기 새로운 타겟 영역의 위치에 기초하여 상기 미응시 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 재 식별한다. 상기 재 식별된 회전 각도 및 회전 방향에 기초 하여 상기 로봇이 회전하도록 상기 구동부를 제어하는 단계는 상기 로봇이 상기 새로운 타겟 영역을 응시한 후 상기 재식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어한다. 또한, 상기 다음 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별하는 단계는 상기 다음 타겟 영역이 상기 미응시 타겟 영역이면, 상기 새로운 타겟 영역의 위치 및 상기 새로운 타겟 영역 이전의 미응시 타 겟 영역의 위치에 기초하여 상기 새로운 타겟 영역에 대응되는 상기 로봇의 회전 각도 및 회전 방향을 식별한다. 상기 재 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는 단계는 상기 로봇이 상기 새로운 타겟 영역 이전의 미응시 타겟 영역을 응시한 후 상기 새로운 타겟 영역에 대 응되는 상기 로봇의 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어한다. 상기 응시 순서를 조정하는 단계는 상기 새로운 타겟 영역을 응시하기 위한 상기 로봇의 회전 각도가 기 설정된 임계 각도 이상이면, 상기 새로운 타겟 영역을 상기 응시 순서에 포함시키지 않고 상기 응시 순서를 유지하는 단계를 더 포함할 수 있다. 또한, 상기 제어 방법은 상기 카메라를 통해 획득된 이미지에 기초하여 상기 로봇 주변에 위치한 복수의 제1 사 용자를 식별하는 단계 및 상기 복수의 제1 사용자 간의 거리, 상기 복수의 제1 사용자가 상기 카메라의 화각 내 위치한 시간 및 상기 복수의 제1 사용자 간의 대화 여부 중 적어도 하나에 기초하여, 상기 복수의 제1 사용자 중 동일한 사용자 그룹에 포함되는 복수의 사용자를 식별하는 단계를 더 포함한다. 또한, 상기 제어 방법은 상기 적어도 하나의 사용자와 상기 적어도 하나의 타겟 영역이 복수이면, 상기 카메라 를 통해 획득된 이미지에 기초하여, 상기 복수의 사용자의 상기 로봇에 대한 응시 시간을 식별하는 단계, 상기 로봇의 마이크를 통해 획득된 사용자 음성에 기초하여 상기 복수의 사용자의 상기 로봇에 대한 음성 입력 횟수 를 식별하는 단계, 상기 식별된 응시 시간 및 음성 입력 횟수를 기초로, 상기 복수의 사용자의 인터렉션 스코어 를 산정하는 단계, 각 타겟 영역에 위치한 적어도 하나의 사용자의 인터렉션 스코어에 기초하여, 상기 각 타겟 영역에 대응하는 인터렉션 스코어를 식별하는 단계 및 상기 식별된 각 타겟 영역에 대응하는 인터렉션 스코어에 기초하여 상기 복수의 타겟 영역에 대한 응시 순서를 결정하는 단계를 포함한다. 여기서 상기 구동부를 제어하는 단계는, 상기 식별된 각 타겟 영역에 대응하는 인터렉션 스코어에 기초하여, 상 기 각 타겟 영역에 대한 응시 시간을 결정하고, 상기 로봇이 상기 결정된 응시 시간 동안 상기 각 타겟 영역을 응시하도록 상기 구동부를 제어한다. 본 개시의 일 실시 예에 따른 로봇의 프로세서에 의해 실행되는 경우 상기 로봇의 동작을 수행하도록 하는 컴퓨 터 명령을 저장하는 비일시적 컴퓨터 판독 가능 기록 매체에 있어서, 상기 동작은, 카메라를 통해 획득된 이미 지에 기초하여 상기 로봇 주변에 위치한 적어도 하나의 사용자를 인식하는 단계를 포함한다. 상기 제어 방법은 상기 카메라의 화각 내의 복수의 영역 중 상기 적어도 하나의 사용자가 위치하는 적어도 하나의 타겟 영역의 위 치 및 상기 적어도 하나의 타겟 영역에 대한 응시 순서에 기초하여 상기 로봇의 회전 각도 및 회전 방향을 식별 하는 단계를 포함한다. 상기 제어 방법은 상기 식별된 회전 각도 및 회전 방향에 기초하여 상기 로봇이 회전하 도록 상기 구동부를 제어하는 단계를 포함한다. 상기 제어 방법은 상기 로봇이 회전하는 동안 상기 카메라를 통 해 획득된 이미지에 기초하여 새로운 사용자가 인식되면, 상기 회전된 로봇의 카메라의 화각 내의 복수의 영역 중 상기 새로운 사용자가 위치하는 새로운 타겟 영역이 상기 응시 순서에 포함되도록 상기 응시 순서를 조정하 는 단계를 포함한다. 상기 제어 방법은 상기 새로운 타겟 영역의 위치 및 상기 조정된 응시 순서에 기초하여 상 기 로봇의 회전 각도 및 회전 방향을 재 식별하는 단계를 포함한다. 상기 제어 방법은 상기 재 식별된 회전 각 도 및 회전 방향에 기초하여 상기 로봇이 회전하도록 상기 구동부를 제어하는 단계를 포함한다."}
{"patent_id": "10-2023-0082088", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 실시 예들은 다양한 변환을 가할 수 있고 여러 가지 실시 예를 가질 수 있는 바, 특정 실시 예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나 이는 특정한 실시 형태에 대해 범위를 한정하려는 것이 아니며, 본 개시의 실시 예의 다양한 변경(modifications), 균등물(equivalents), 및/또는 대체물 (alternatives)을 포함하는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유 사한 참조 부호가 사용될 수 있다. 본 개시를 설명함에 있어서, 관련된 공지 기능 혹은 구성에 대한 구체적인 설명이 본 개시의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그에 대한 상세한 설명은 생략한다. 덧붙여, 하기 실시 예는 여러 가지 다른 형태로 변형될 수 있으며, 본 개시의 기술적 사상의 범위가 하기 실시 예에 한정되는 것은 아니다. 오히려, 이들 실시 예는 본 개시를 더욱 충실하고 완전하게 하고, 당업자에게 본 개시의 기술적 사상을 완전하게 전달하기 위하여 제공되는 것이다. 본 개시에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 권리범위를 한정하려는 의도 가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. 본 개시에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\"등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 개시에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중 요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요 소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"직접 연결되어\" 있다거나 \"직 접 접속되어\" 있다고 언급된 때에는, 상기 어떤 구성요소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 개시에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for),\" \"~하는 능력을 가지는(having the capacity to),\" \"~하도록 설계된(designed to),\" \"~하도 록 변경된(adapted to),\" \"~하도록 만들어진(made to),\" 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장치가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행하도록 구성된(또는 설정된) 프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메모리 장치에 저장된 하나 이상의 소프트 웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 실시 예에 있어서 '모듈' 혹은 '부'는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 '모듈' 혹은 복수의 '부'는 특정 한 하드웨어로 구현될 필요가 있는 '모듈' 혹은 '부'를 제외하고는 적어도 하나의 모듈로 일체화되어 적어도 하 나의 프로세서로 구현될 수 있다. 한편, 도면에서의 다양한 요소와 영역은 개략적으로 그려진 것이다. 따라서, 본 발명의 기술적 사상은 첨부한 도면에 그려진 상대적인 크기나 간격에 의해 제한되지 않는다. 이하에서는 첨부한 도면을 참고하여 본 개시에 따른 실시 예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 도 1은 본 다수의 사용자를 응대하는 로봇에 대한 예시도이다. 도 1을 참조하면, 최근 다양한 분야에 로봇이 이용됨에 따라, 로봇이 다수의 사용자(200-1, 200-2 및 200-3, 이하 200)를 응대해야 하는 경우가 발생한다. 여기서, 로봇이 다수의 사용자를 응대한다는 것 은, 다수의 사용자와 동시에 인터렉션을 수행하는 것을 의미할 수 있다. 예를 들어, 전시회 또는 박람회에 서 다수의 관람객을 가이드하는 로봇 또는 공항이나 대형 마트 등에서 다수의 방문객에게 길을 안내하거나 또는 정보를 제공하는 로봇 등이 이에 해당할 수 있다. 이때, 기존의 로봇은 다수의 사용자 중 특정 사용자와 인터렉션을 수행하거나 또는 무작위적으로 다 수의 사용자와 인터렉션을 수행하였다. 이에 따라, 다수의 사용자가 로봇과 적절하게 인터렉션 을 하고 있음을 느끼지 못하는 경우가 대부분이었다. 특히, 복수의 사용자가 로봇에 특정 정보를 제 공할 것을 요청한 경우, 로봇이 복수의 사용자 중 오직 특정 사용자와 인터렉션을 수행한다면, 로봇 이 제공하는 정보는 특정 사용자를 제외한 나머지 사용자에게는 적절하게 전달되지 못하는 문제가 발생한 다. 또한, 기존의 로봇은 다수의 사용자 중 로봇이 인식하지 못한 특정 사용자를 뒤늦게 인식되더라 도 특정 사용자를 인터렉션 대상에 포함시키지 않았다. 이때, 뒤늦게 인식된 특정 사용자가 다수의 사용자(20 0)와 동일한 사용자 그룹(예를 들어, 동일한 관람객 그룹 또는 방문객 그룹 등)에 포함됨에도 불구하고, 단지 다른 사용자들보다 뒤늦게 인식되었다는 이유 만으로 로봇의 인터렉션 대상에서 제외되는 불이익을받는다. 즉, 뒤늦게 인식된 특정 사용자는 로봇과 인터렉션을 수행할 수 없는 문제가 발생한다. 본 개시는 이러한 과제를 해결하기 위한 것으로, 다수의 사용자가 로봇을 이용하는 경우, 로봇 은 다수의 사용자와 골고루 인터렉션을 수행할 수 있도록 적절한 인터렉션 순서를 결정하고, 응시 순서에 따라 다수의 사용자를 순차적으로 인터렉션을 수행한다. 또한 새로운 사용자가 인식되는 경우에도, 로봇 은 새로운 사용자를 누락시키지 않고, 기존의 다수의 사용자와 함께 새로운 사용자와도 인터렉션을 수행한다. 이하, 이와 관련된 본 개시의 실시 예에 대하여 자세히 설명하도록 한다. 도 2는 본 개시의 일 실시 예에 따른 로봇의 구성을 나타낸 블록도이다. 도 2를 참조하면, 로봇은 카메라, 구동부 및 하나 이상의 프로세서를 포함한다. 본 개시의 일 실시 예에 따른 로봇은 사용자와의 인터렉션을 수행하면서, 사용자에게 다양한 서비스를 제 공하는 다양한 전자 장치로 구현될 수 있다. 예를 들어, 로봇은 공항 또는 마트 등에 위치하여 사용자로부 터 입력(예를 들어, 음성 입력, 터치 입력 등)을 수신하면, 수신된 입력에 대한 응답으로 사용자에게 특정 정보 를 제공하거나 길을 안내하는 등 다양한 서비스를 제공하는 가이드 로봇일 수 있다. 한편, 본 개시의 실시 예에 따라, 로봇은 헤드(Head)와 바디(Body)로 구분될 수 있다. 일 예로, 로봇(10 0)의 헤드는 카메라 또는 센서(미도시)를 포함하여, 로봇 주변의 객체에 대한 이미지를 획득하거나 또는 객체를 감지하는 기능을 수행할 수 있다. 또한, 로봇의 헤드는 디스플레이(미도시)를 포함할 수 있으 며, 로봇은 디스플레이(미도시)를 통해 다양한 영상 정보를 표시할 수 있다. 특히, 로봇은 사용자와 의 인터렉션을 수행하는 동안, 헤드에 포함된 디스플레이(미도시)를 통해 눈, 코, 입을 나타내는 그래픽 객체를 표시함으로써, 로봇을 이용하는 사용자에게 생동감 있는 인터렉션 경험을 제공할 수 있다. 로봇의 바디는 구동부와 연결되어 로봇을 이동시키거나 또는 로봇을 지지하는 프레임의 기 능을 수행할 수 있다. 또한, 로봇의 바디는 디스플레이(미도시)를 포함할 수 있으며, 로봇은 디스플 레이(미도시)를 통해 다양한 영상 정보를 표시할 수 있다. 특히, 로봇은 디스플레이(미도시)를 통해 사용 자가 요청하거나 또는 사용자에게 필요한 정보(예를 들어, UI 등)를 표시할 수 있으며, 디스플레이(미도시)에 포함된 터치 패널을 통해 사용자로부터 사용자 명령을 입력 받을 수도 있다. 카메라는 로봇 주변의 객체를 촬영하여 객체에 관한 복수의 이미지를 획득한다. 구체적으로, 카메라 는 로봇 주변에 존재하는 객체(예를 들어, 사람, 동물, 그 밖에 사물 등)에 관한 이미지를 획득할 수 도 있다. 이를 위해, 카메라는 CMOS 구조를 가진 촬상 소자(CIS, CMOS Image Sensor), CCD 구조를 가진 촬상 소자 (Charge Coupled Device) 등의 촬상 소자로 구현될 수 있다. 다만, 이에 한정되는 것은 아니며, 카메라는 피사체를 촬영할 수 있는 다양한 해상도의 카메라 모듈로 구현될 수 있다. 또한, 카메라는 뎁스 카메라, 스테레오 카메라 또는 RGB 카메라 등으로 구현될 수 있다. 이를 통해, 카메라는 객체에 관한 이미지와 함께 객체의 뎁스 정보를 함께 획득할 수 있다. 구동부는 로봇의 적어도 일부를 구동시킨다. 일 예로, 구동부는 로봇의 헤드와 바디를 연 결하는 회전 가능한 모터로 구현될 수 있다. 이에 따라, 구동부는 구동부에 연결된 헤드를 360° 회 전시켜 로봇이 카메라를 통해 로봇 주변의 객체에 대한 정보를 획득할 수 있도록 한다. 다만, 이에 제한되는 것은 아니며, 로봇은 헤드와 바디가 구분되지 않는 일체형으로 구현될 수도 있으며, 이때 바디와 헤드는 각각 수행하는 기능에 따라 로봇 상에서 구분될 수 있다. 또한 일 예로, 구동부는 로봇의 바디의 하부에 구비되어 로봇을 이동시킬 수 있다. 즉, 구동부 는 로봇을 주행 시킬 수 있는 장치일 수 있다. 이때, 구동부는 프로세서의 제어에 따라 주행 방 향 및 주행 속도를 조절할 수 있다. 이를 위해, 구동부는 로봇이 주행하기 위한 동력을 발생시키는 동력 발생 장치(예: 사용 연료(또는 에너지원)에 따라 가솔린 엔진(engine), 디젤 엔진, LPG(liquefied petroleum gas) 엔진, 전기 모터 등), 주행 방향을 조절하기 위한 조향 장치(예: 기계식 스티어링(manual steering), 유압식 스티어링(hydraulics steering), 전자식 스티어링(electronic control power steering; EPS) 등)를 포함할 수 있다. 또한, 구동부는 복수의 휠을 포함할 수 있다. 복수의 휠은 프로세서의 제어에 기초하여 회전되며, 이에 따 라 로봇을 이동시킬 수 있다. 또는, 복수의 휠은 프로세서의 제어에 기초하여 서로 다른 회전 속도로 회전될 수 있으며(또는 복수의 휠 중 일부 휠 만이 회전될 수 있으며), 이에 따라 로봇을 회전시킬 수도 있다. 하나 이상의 프로세서는 카메라 및 구동부와 전기적으로 연결되어 로봇의 전반적인 동작 및 기 능을 제어한다. 하나 이상의 프로세서는 CPU (Central Processing Unit), GPU (Graphics Processing Unit), APU (Accelerated Processing Unit), MIC (Many Integrated Core), DSP (Digital Signal Processor), NPU (Neural Processing Unit), 하드웨어 가속기 또는 머신 러닝 가속기 중 하나 이상을 포함할 수 있다. 하나 이상의 프로 세서는 로봇의 다른 구성요소 중 하나 또는 임의의 조합을 제어할 수 있으며, 통신에 관한 동작 또는 데이터 처리를 수행할 수 있다. 하나 이상의 프로세서는 메모리(미도시)에 저장된 하나 이상의 프로그램 또는 명령어(instruction)을 실행할 수 있다. 예를 들어, 하나 이상의 프로세서는 메모리에 저장된 하나 이상의 명령어를 실행함으로써, 본 개시의 일 실시 예에 따른 방법을 수행할 수 있다. 본 개시의 일 실시 예에 따른 방법이 복수의 동작을 포함하는 경우, 복수의 동작은 하나의 프로세서에 의해 수 행될 수도 있고, 복수의 프로세서에 의해 수행될 수도 있다. 예를 들어, 일 실시 예에 따른 방법에 의해 제 1 동작, 제 2 동작, 제 3 동작이 수행될 때, 제 1 동작, 제 2 동작, 및 제 3 동작 모두 제 1 프로세서에 의해 수 행될 수도 있고, 제 1 동작 및 제 2 동작은 제 1 프로세서(예를 들어, 범용 프로세서에 의해 수행되고 제 3 동 작은 제 2 프로세서(예를 들어, 인공지능 전용 프로세서)에 의해 수행될 수도 있다. 하나 이상의 프로세서는 하나의 코어를 포함하는 단일 코어 프로세서(single core processor)로 구현될 수 도 있고, 복수의 코어(예를 들어, 동종 멀티 코어 또는 이종 멀티 코어)를 포함하는 하나 이상의 멀티 코어 프 로세서(multicore processor)로 구현될 수도 있다. 하나 이상의 프로세서가 멀티 코어 프로세서로 구현되 는 경우, 멀티 코어 프로세서에 포함된 복수의 코어 각각은 캐시 메모리, 온 칩(On-chip) 메모리와 같은 프로세 서 내부 메모리를 포함할 수 있으며, 복수의 코어에 의해 공유되는 공통 캐시가 멀티 코어 프로세서에 포 함될 수 있다. 또한, 멀티 코어 프로세서에 포함된 복수의 코어 각각(또는 복수의 코어 중 일부)은 독립적으로 본 개시의 일 실시 예에 따른 방법을 구현하기 위한 프로그램 명령을 판독하여 수행할 수도 있고, 복수의 코어 전체(또는 일부)가 연계되어 본 개시의 일 실시 예에 따른 방법을 구현하기 위한 프로그램 명령을 판독하여 수 행할 수도 있다. 본 개시의 일 실시 예에 따른 방법이 복수의 동작을 포함하는 경우, 복수의 동작은 멀티 코어 프로세서에 포함 된 복수의 코어 중 하나의 코어에 의해 수행될 수도 있고, 복수의 코어에 의해 수행될 수도 있다. 예를 들어, 일 실시 예에 따른 방법에 의해 제 1 동작, 제 2 동작, 및 제 3 동작이 수행될 때, 제 1 동작, 제2 동작, 및 제 3 동작 모두 멀티 코어 프로세서에 포함된 제 1 코어에 의해 수행될 수도 있고, 제 1 동작 및 제 2 동작은 멀티 코어 프로세서에 포함된 제 1 코어에 의해 수행되고 제 3 동작은 멀티 코어 프로세서에 포함된 제 2 코어에 의 해 수행될 수도 있다. 본 개시의 실시 예에서, 프로세서는 하나 이상의 프로세서 및 기타 전자 부품들이 집적된 시스템 온 칩 (SoC), 단일 코어 프로세서, 멀티 코어 프로세서, 또는 단일 코어 프로세서 또는 멀티 코어 프로세서에 포함된 코어를 의미할 수 있으며, 여기서 코어는 CPU, GPU, APU, MIC, DSP, NPU, 하드웨어 가속기 또는 기계 학습 가속 기 등으로 구현될 수 있으나, 본 개시의 실시 예들이 이에 한정되는 것은 아니다. 이하에서는, 설명의 편의를 위해 하나 이상의 프로세서를 프로세서로 지칭하도록 한다. 도 3은 본 개시의 일 실시 예에 따른 로봇을 제어하는 방법의 순서도이다. 도 3을 참조하면, 프로세서는 카메라를 통해 획득된 이미지에 기초하여 로봇 주변에 위치한 적어도 하나의 사용자를 인식한다(S310). 구체적으로, 프로세서는 카메라를 통해 로봇 주변의 사물, 사람, 동물 등에 관한 복수의 이미지를 획 득할 수 있다. 그리고, 프로세서는 카메라를 통해 획득된 이미지에 포함된 적어도 하나의 사용자에 대한 특징 정보를 추출하고, 추출된 특징 정보와 이미지 내 적어도 하나의 사용자를 매칭하여 적어도 하나 의 사용자를 인식할 수 있다. 그리고, 로봇은 적어도 하나의 사용자에 매칭된 특징 정보에 기초 하여 복수의 이미지 내에서 적어도 하나의 사용자를 트래킹하여 인식할 수 있다. 이때, 프로세서는 로봇 주변의 복수의 사람들 중 로봇과 인터렉션을 수행하거나 또는 로봇(10 0)과 인터렉션을 수행하고자 하는 사람 만을 사용자로 선별하여 인식할 수 있다. 다시 말해, 프로세서 는 복수의 사람들 중 로봇을 이용할 목적으로 로봇 주변에 위치한 사람을 사용자로 선별하여 인식할 수 있다. 예를 들어, 프로세서는 단순히 로봇 주변에 위치하거나 또는 로봇 주변을 통행하는 사람은 사용자로 인식하지 않을 수 있으며, 로봇을 이용하기 위하여 로봇 주변에 위치 한 사람은 사용자로 인식할 수 있다. 이를 위해, 프로세서는 획득된 이미지에 기초하여, 복수의 사람과 로봇 간의 거리를 각각 식별하고, 식별된 거리에 기초하여 복수의 사람 중 적어도 하나의 사용자를 선별할 수 있다. 예를 들어, 프로세서 는 획득된 이미지에 포함된 뎁스 정보에 기초하여 복수의 사람과 로봇 간의 거리를 각각 식별하고, 복수의 사람 중 식별된 거리가 기 설정된 거리 이내인 사람 만을 사용자로 구분하여 인식할 수 있다. 이때, 프로세서는 기 설정된 거리 이내에서 기 설정된 시간 동안 머무르는 사람 중 로봇을 응시하는 것으로 식별되는 사람을 사용자로 식별할 수 있다. 한편, 프로세서는 획득된 이미지 내에서 사용자 에 대한 특징 정보를 추출하고, 추출된 특징 정보에 기초하여 복수의 이미지에 포함된 사용자를 계속 적으로 트래킹하여 인식할 수 있다. 또한, 프로세서는 각각의 사람의 로봇에 대한 인터렉션 여부에 따라 복수의 사람들 중 적어도 하나의 사용자를 선별하여 식별할 수 있다. 여기서, 인터렉션은 로봇의 응시 여부(또는 응시 시간), 로봇 에 대한 음성 명령어의 입력 여부(또는 입력 횟수), 터치 입력 여부(및 입력 횟수), 로봇과 사람 간 의 거리 변화 등을 포함할 수 있다. 예를 들어, 프로세서는, 이미지에 포함된 복수의 사람들에 대한 특징 정보를 추출하고, 추출된 특징 정보에 기초하여 복수의 사람들의 눈을 인식하여, 각각의 사람이 로봇을 응 시하는지 식별할 수 있다. 그리고, 프로세서는 각각의 사람의 로봇에 대한 응시 여부 및 응시 시간에 기초하여 복수의 사람들 중 로봇을 이용하고자 하는 적어도 하나의 사용자를 구분하여 식별할 수 있 다. 한편, 본 개시의 일 실시 예에 따라, 프로세서는 복수의 사용자 중 동일한 사용자 그룹에 포함되는 복수의 사용자 만을 선별하여 인식할 수 있다. 여기서, 사용자 그룹은 로봇에 대한 동일한 이용 목적 을 가지고 로봇 주변에 위치한 복수의 사용자를 포함하는 그룹일 수 있다. 프로세서는 동일한 사용자 그룹에 포함되는 복수의 사용자 만을 로봇이 응시하도록 로봇을 제어할 수 있다. 이를 위해, 프로세서는 로봇 주변의 복수의 사용자가 동일한 사용자 그룹에 포함되는지 식별할 수 있다. 일 예로, 프로세서는 카메라를 통해 획득된 이미지에 기초하여 복수의 사용자 간의 인터렉 션 여부, 복수의 사용자 간의 거리, 복수의 사용자가 인식된 시간 차이 등에 기초하여 동일한 사용자 그룹에 포함되는 복수의 사용자를 식별할 수 있다. 도 4는 본 개시의 일 실시 예에 따른 동일한 사용자 그룹에 포함되는 복수의 사용자를 식별하는 방법을 나타낸 예시도이다. 본 개시의 일 실시 예에 따라 프로세서는, 카메라를 통해 획득된 이미지에 기초하여 로봇 주변 에 위치한 복수의 사용자를 식별할 수 있다. 그리고, 프로세서는 복수의 사용자 간의 거리, 복 수의 사용자가 카메라의 화각 내 위치한 시간 및 복수의 사용자 간의 대화 여부 중 적어도 하나에 기초하여, 복수의 사용자 중 동일한 사용자 그룹에 포함되는 복수의 사용자를 식별할 수 있다. 구체적으로 프로세서는 카메라를 통해 획득된 이미지에 기초하여 로봇 주변에 위치한 복수의 사 용자를 식별할 수 있다. 특히, 프로세서는 카메라를 통해 획득된 이미지에 기초하여 로봇 주변에 위치한 복수의 사람 중 복수의 사용자를 식별할 수 있다. 일 예로, 프로세서는 이미지에 포함 된 각각의 사람에 대한 뎁스 정보에 기초하여, 로봇과 각각의 사람 간의 거리를 식별할 수 있다. 그리고, 프로세서는 복수의 사람 중 기 설정된 거리 이내에 위치한 복수의 사람 만을 복수의 사용자로 식별할 수 있다. 도 4를 참조하면, 프로세서는 카메라의 화각(150°) 내에서 로봇으로부터 2m 이내에 위치한 복수의 사람(200-1 내지 200-8)을 복수의 사용자로 식별할 수 있다. 이에 따라, 프로세서는 로봇 주변의 8명의 사람(200-1 내지 200-8) 중 카메라의 화각(150°) 내에서 로봇으로부터 2m 이내에 위치한 6명의 사람(200-1 내지 200-6) 만을 복수의 사용자로 식별할 수 있다. 특히, 프로세서(13 0)는 로봇으로부터 2m 이상 떨어져 위치한 사람(200-7)은 사용자로 식별하지 않을 수 있다. 또한, 프로세 서는 카메라의 화각(150°)을 벗어나 위치한 사람(200-8) 또한 사용자로 식별하지 않을 수 있다. 이때, 카메라의 화각(150°)을 벗어나 위치한 사람(200-9)은 카메라를 통해 획득된 이미지에 포함되지 않으므로, 프로세서는 카메라의 화각(150°)을 벗어나 위치한 사람은 인지하지 못할 수 있다. 프로세서는 복수의 사용자를 식별한 후 복수의 사용자 중 동일한 사용자 그룹에 포함되는 복수 의 사용자를 선별할 수 있다. 특히, 프로세서는 복수의 사용자 간의 거리, 복수의 사용자 가 카메라의 화각 내 위치한 시간 및 복수의 사용자 간의 대화 여부 중 적어도 하나에 기초하여 복수의 사용자 중 동일한 사용자 그룹에 포함되는 복수의 사용자를 식별할 수 있다. 구체적으로, 프로세서는 식별된 복수의 사용자 간의 거리를 식별할 수 있다. 구체적으로, 프로세서 는 카메라를 통해 획득된 이미지 내에서 각각의 사용자 간의 간격을 식별할 수 있다. 이를 통해, 프 로세서는 복수의 사용자 간의 거리를 식별할 수 있다. 그리고, 프로세서는 복수의 사용자 간의 거리가 기 설정된 거리(또는 간격) 이내인 것으로 식별되는 복수의 사용자 만을 동일한 사용자 그룹 에 포함된 것으로 식별할 수 있다. 예를 들어, 도 4를 참조하면, 프로세서는 식별된 6명의 사용자(200-1 내지 200-6) 중 복수의 사용자(또는 인접한 사용자) 간의 간격이 30cm 이내인 것으로 식별되는 복수의 사 용자 만을 동일한 사용자 그룹으로 식별할 수 있다. 이때, 프로세서는 복수의 사용자 중 특정 사용자를 기준으로 나머지 사용자의 간격이 기 설정된 거리 (또는 간격) 이내인지 식별하고, 특정 사용자로부터 기 설정된 거리 이내인 적어도 하나의 사용자와 특정 사용 자를 사용자 그룹에 포함시킬 수 있다. 여기서, 특정 사용자는 복수의 사용자 중 최초로 카메라의 화 각 및 기 설정된 거리(로봇으로부터 기 설정된 거리) 이내에 위치한 사용자일 수 있다. 또한, 프로세서는 복수의 사용자가 카메라의 화각 내 위치한 시간을 식별할 수 있다. 구체 적으로, 프로세서는 카메라를 통해 획득된 복수의 이미지 내에서 각각의 사용자가 포함되기 시작한 시간을 식별할 수 있다. 이를 통해 프로세서는 각각의 사용자가 카메라의 화각 내 위치한 시간을 식별할 수 있다. 특히, 프로세서는 각각의 사용자가 카메라의 화각 내 뿐만 아니라, 기 설정된 거리 이내에 위치한 시간 까지도 식별할 수 있다. 이는, 앞서 설명한 것과 같이 이미지에 포함된 뎁스 정보를 바탕으로 식별될 수 있다. 이로써, 프로세서는 복수의 사용자 간의 카메라의 화각(및 기 설 정된 거리) 내 위치한 시간 차이를 식별할 수 있다. 그리고, 프로세서는 복수의 사용자 간의 시간 차 이가 기 설정된 시간 이내인 것으로 식별되는 복수의 사용자 만을 동일한 사용자 그룹에 포함된 것으로 식 별할 수 있다. 예를 들어, 도 4를 참조하면, 프로세서는 식별된 6명의 사용자(200-1 내지 200-6)가 카메라 의 화각(및 기 설정된 거리) 이내에 위치한 시간을 각각 식별할 수 있다. 그리고, 프로세서는 복 수의 사용자(또는 인접한 사용자) 간의 카메라의 화각(및 기 설정된 거리) 이내에 위치한 시간 차이가 1초 이하인 것으로 식별되는 복수의 사용자 만을 동일한 사용자 그룹으로 식별할 수 있다. 이때, 프로세서는 복수의 사용자 중 특정 사용자가 카메라의 화각(및 기 설정된 거리) 내 위치한 시간을 기준으로 나머지 사용자가 카메라의 화각(및 기 설정된 거리) 내 위치한 시간을 식별할 수 있다. 그리고, 프로세서는 특정 사용자가 카메라의 화각(및 기 설정된 거리) 내 위치한 시간 을 기준으로, 기 설정된 시간(예를 들어, 3초) 이내에 카메라의 화각(및 기 설정된 거리) 내 위치한 적어도 하나의 사용자를 식별할 수 있다. 그리고, 프로세서는 식별된 적어도 하나의 사용자와 특정 사용자 를 사용자 그룹에 포함시킬 수 있다. 여기서, 특정 사용자는 복수의 사용자 중 최초로 카메라의 화각 및 기 설정된 거리(로봇으로부터 기 설정된 거리) 이내에 위치한 사용자일 수 있다. 또한, 프로세서는 복수의 사용자 간의 대화 여부(또는 응시 여부)에 따라서 복수의 사용자 중 동일한 사용자 그룹에 포함되는 복수의 사용자를 선별할 수 있다. 구체적으로, 프로세서는 카메라 를 통해 획득된 복수의 이미지를 바탕으로, 각각의 사용자의 얼굴을 인식하거나 직접 사용자의 입과 눈을 검출할 수 있다. 예를 들어, AdaBoost 알고리즘, HOG(Histogram of Oriented Gradients) 알고리즘, Haar Cascade 알고리즘 등을 이용하여 프로세서는 이미지 내에서 사람의 눈과 입을 검출할 수 있다. 한편, 프로 세서는 검출된 사용자의 입과 눈을 트래킹하여 각각의 사용자가 응시하거나 또는 대화를 하는 대상을 식별 할 수 있다. 이를 통해, 프로세서는 복수의 사용자 간의 대화 여부(또는 응시 여부)를 식별할 수 있 다. 또한, 프로세서는 로봇의 마이크(미도시)를 통해 입력되는 각각의 사용자의 음성을 인식하고, 각 각의 사용자와 사용자 음성을 매칭한 후 복수의 사용자간의 대화 여부를 식별할 수도 있다. 이를 위해, 프 로세서는 HMM(Hidden Markov Models) 알고리즘, LSTM(Long Short-Term Memory) 모델, CNN 모델, Transformer 모델 등을 이용할 수 있다. 한편, 프로세서는 대화를 하는 복수의 사용자를 동일한 사용자 그룹으로 식별할 수 있다. 예를 들어, 도 4를 참조하면, 프로세서는 식별된 6명의 사용자(200-1 내지 200-6) 중 대화를 하는 복수의 사용자 만을 동일한 사용자 그룹으로 식별할 수 있다. 한편, 프로세서는 복수의 사용자 간의 거리, 복수의 사용자가 카메라의 화각 내 위치 한 시간 및 복수의 사용자 간의 대화 여부에 기초하여 복수의 사용자 중 동일한 사용자 그룹에 포함 되는 복수의 사용자를 선별할 수 있다. 구체적으로, 프로세서는 복수의 사용자 간의 거리, 복수의 사용자가 카메라의 화각 내 위치한 시간 및 복수의 사용자 간의 대화 여부에 기초하여 각각의 사용자의 동일한 사용자 그룹에 포함되 는 확률 값을 산출할 수 있다. 일 예로, 프로세서는 복수의 사용자 간의 거리에 기초하여, 동일한 그룹에 포함되는 제1 확률 값을 각각의 사용자에 대하여 산출할 수 있다. 이때, 프로세서는 복수의 사용자 간의 거리가 가까울수록 동일한 사용자 그룹에 포함되는 제1 확률 값을 높은 값으로 산출할 수 있다. 특히, 프로세서는 상술한 특 정 사용자와의 거리가 가까운 사용자일 수록 특정 사용자와 동일한 사용자 그룹에 포함되는 제1 확률 값을 높은 값으로 산출할 수 있다. 또한, 프로세서는 복수의 사용자가 카메라의 화각 내 위치한 시간에 기초하여 동일한 그룹 에 포함되는 제2 확률 값을 각각의 사용자에 대하여 산출할 수 있다. 이때, 프로세서는 복수의 사용자 가 카메라의 화각 내 위치한 시간 차이가 작을 수록, 복수의 사용자의 사용자 그룹에 포함 되는 재2 확률 값을 높은 값으로 산출할 수 있다. 특히, 프로세서는 상술한 특정 사용자가 카메라의 화각 내 위치한 시간과의 시간 차이가 짧은 사용자에 대해서는 제2 확률 값을 높은 값으로 산출할 수 있다. 또한, 프로세서는 복수의 사용자 간의 대화 여부에 기초하여 동일한 그룹에 포함되는 제3 확률 값을 각각의 사용자에 대하여 산출할 수 있다. 이때, 프로세서는 복수의 사용자 간의 대화 여부에 대한 제 3 확률 값을 산출할 수 있다. 즉, 프로세서는 획득된 이미지에 기초하여 복수의 사용자의 대화 여부 를 식별하고, 각각의 사용자가 대화를 수행할 제3 확률 값을 산출할 수 있다. 그리고, 프로세서는 산출된 제1, 제2 및 제3 확률 값에 기초하여 각각의 사용자의 동일한 사용자 그룹에 포함될 확률 값을 획득할 수 있다. 일 예로, 프로세서는 제1, 제2 및 제3 확률 값에 각각 가중치를 적용하 여 각각의 사용자의 동일한 사용자 그룹에 포함될 확률 값을 획득할 수 있다. 특히, 프로세서는 제3 확률 값에 대해서는 제1 및 제2 확률 값 보다 높은 가중치를 적용할 수 있다. 즉, 프로세서는 대화를 수행하는 복수의 사용자의 경우 동일한 사용자에 포함될 가능성이 높은 것으로 식별할 수 있다. 그리고, 프로세서 는 서로 다른 가중치가 적용된 제1, 제2 및 제3 확률 값을 합산하여 각각의 사용자의 동일한 사용자 그룹 에 포함될 확률 값을 획득할 수 있다. 그리고, 프로세서는 획득된 확률 값이 기 설정된 값 이상인 복수의 사용자 만을 동일한 사용자 그룹으로 식별할 수 있다. 예를 들어, 도 4를 참조하면, 프로세서는 식별된 6명의 사용자(200-1 내지 200-6)에 대하여, 6명의 사용자 (200-1 내지 200-6) 간의 거리, 6명의 사용자(200-1 내지 200-6)가 카메라의 화각 내 위치한 시간 및 6명의 사용자(200-1 내지 200-6) 간의 대화 여부에 기초하여 각각 동일한 사용자 그룹에 포함될 확률 값을 획득 할 수 있다. 이때, 기 설정된 값이 0.5인 경우 프로세서는 6명의 사용자(200-1 내지 200-6) 중 0.5 이상의 확률 값을 가진 4명의 사용자(200-1 내지 200-4) 만을 동일한 사용자 그룹에 포함되는 것으로 식별할 수 있다. 이하에서는, 로봇이 동일한 사용자 그룹에 포함된 것으로 식별된 4명의 사용자와 인터렉션을 수행하는 것으로 상정하여 설명하도록 한다. 다시 도 3을 참조하면, 프로세서는 카메라의 화각 내의 복수의 영역 중 적어도 하나의 사용자 가 위치하는 적어도 하나의 타겟 영역의 위치 및 적어도 하나의 타겟 영역에 대한 응시 순서에 기 초하여 로봇의 회전 각도 및 회전 방향을 식별할 수 있다(S320). 구체적으로, 프로세서는 카메라의 화각 내 복수의 영역을 식별할 수 있다. 예를 들어, 프로세서 는 카메라의 화각 내에서 기 설정된 범위를 설정하고, 기 설정된 범위를 기 설정된 크기의 복수의 영 역으로 분할하여 식별할 수 있다. 일 예로, 프로세서는 카메라의 화각을 기 설정된 각도로 분할하여 복수의 영역을 식별할 수 있다. 여기서, 기 설정된 각도는 카메라의 화각 성능에 따라 사전에 설정될 수 있다. 또는 기 설정된 각도는 카메 라의 화각 내에서 식별되는 사용자의 수에 따라 프로세서에 의해 설정될 수도 있다. 예를 들어,프로세서는 화각 내에서 식별되는 사용자의 수가 적을수록, 기 설정된 각도의 크기를 감소시킬 수 있다. 또한, 프로세서는 복수의 영역을 기 설정된 거리 이내에서 식별할 수 있다. 이에 따라, 프로세서(13 0)는 기 설정된 각도 및 크기를 갖는 복수의 영역을 식별할 수 있다. 즉, 복수의 영역은 동일한 크기를 갖으며, 각각 중복되지 않은 형태로 카메라의 화각 내에서 식별될 수 있다. 기 설정된 거리는 카메라를 통 해 획득된 이미지에 포함된 뎁스 정보 또는 로봇의 센서(미도시)를 통해 획득된 거리 정보에 기초하여 식 별될 수 있다. 그리고, 프로세서는 카메라의 화각 내 복수의 영역을 식별한 후 복수의 영역 중 적어도 하나 의 사용자가 위치한 적어도 하나의 타겟 영역을 식별할 수 있다. 여기서, 타겟 영역은 복수의 영 역 중 적어도 하나의 사용자가 위치한 영역으로, 이후 로봇을 회전시켜 로봇이 응시할 영역 이 될 수 있다. 한편, 로봇이 타겟 영역을 응시한다는 것은 로봇 또는 로봇의 일 부분이 타겟 영역을 향하는 것일 수 있다. 예를 들어, 프로세서가 로봇의 헤드와 바디 사이에 구비된 구동부(예를 들어, 모터 등)를 제어함에 따라, 로봇의 헤드가 회전하여 타겟 영역을 향하는 경우, 로봇이 타겟 영역 을 응시하는 것에 해당할 수 있다. 또는, 프로세서가 로봇의 바디의 하측에 구비된 구동부(예를 들어, 휠 등)을 제어함에 따라, 로봇의 바디가 회전하여 타겟 영역을 향하는 경우, 로봇이 타겟 영역을 응시하는 것에 해당할 수 있다. 이처럼, 프로세서의 제어에 따라 로봇 또는 로봇의 일 부분이 타겟 영역을 응시함으로써, 타겟 영역에 위치한 복수의 사용자는 로봇이 적극적으 로 사용자와 인터렉션을 수행하는 것으로 느낄 수 있다. 도 5는 본 개시의 일 실시 예에 따른 카메라의 화각 내 복수의 영역 중 타겟 영역을 식별하는 것을 나타낸 예시 도이다. 이하에서는, 본 개시의 설명의 편의를 위해 적어도 하나의 사용자와 적어도 하나의 타겟 영역이 각각 복수인 것으로 상정하여 설명하도록 한다. 도 5를 참조하면, 프로세서는 150°의 화각을 2m의 거리 내에서 30°의 각도로 분할하여, 5개의 영역 을 식별하였다. 그리고, 프로세서는 5개의 영역 중 1명의 사용자가 위치한 제1 영역과 2명의 사용자 가 위치한 제2 영역과 1명의 사용자가 위치한 제5 영역이 타겟 영역으로 식별할 수 있다. 이에 따라, 프로세서는 로봇을 회전시켜 로봇이 응시할 영역으로 제1, 제2 및 제5 영역을 식별할 수 있다. 타겟 영역을 식별한 후 프로세서는 복수의 사용자가 위치하는 복수의 타겟 영역의 위치 및 복수의 타겟 영역에 대한 응시 순서에 기초하여 로봇의 회전 각도 및 회전 방향을 식별할 수 있다. 구체적으로, 프로세서는 화각 내 타겟 영역의 위치에 기초하여 로봇의 회전 각도와 회전 방 향을 식별할 수 있다. 복수의 영역은 상술한 바와 같이 서로 중복되지 않은 형태로 화각 내에서 식별되 므로, 프로세서는 복수의 영역 중에서 식별된 타겟 영역의 위치를 카메라의 화각 내에서 식별 할 수 있다. 그리고, 프로세서는 식별된 타겟 영역의 위치에 기초하여 로봇의 회전 방향을 식별 할 수 있다. 복수의 영역은 동일한 기 설정된 각도로 분할되므로, 프로세서는 로봇이 타겟 영역 을 응시하기 위하여(또는 향하기 위하여) 로봇을 회전시켜야 하는 회전 각도를 식별할 수도 있다. 도 5를 다시 참조하면, 프로세서는 복수의 타겟 영역(즉, 제1, 제2 및 제 5 타겟 영역) 중 제1 및 제2 영역은 로봇의 좌측에 위치한 것으로 식별되므로, 제1 및 제2 영역에 대한 로봇의 회전 방향을 좌측 방향으로 식별할 수 있다. 그리고, 프로세서는 제1 영역에 대한 로봇의 회전 각도는 60°로, 제2 영 역에 대한 로봇의 회전 각도는 30°로 식별할 수 있다. 반면에, 프로세서는 복수의 타겟 영역(즉, 제1, 제2 및 제 5 타겟 영역) 중 제5 영역은 로봇의 우측에 위치한 것으로 식별되므로, 제5 영역에 대한 로봇의 회전 방향을 우측 방향으로 식별할 수 있다. 그리고, 프로세서는 제1 영역에 대한 로봇의 회전 각도는 60°로 식별할 수 있다. 다만, 복수의 타겟 영역이 식별되는 경우 프로세서는 복수의 타겟 영역의 위치와 응시 순서를 고 려하여 각각의 타겟 영역에 대한 로봇의 회전 방향 및 회전 각도를 식별할 수 있다. 여기서, 응시 순 서는 로봇이 복수의 타겟 영역을 응시하는 순서일 수 있다. \\일 예로, 응시 순서는 기 설정된 회전 방향에 기초하여 복수의 타겟 영역에 대하여 순차적으로 설정될 수 있다. 예를 들어, 도 5를 참조하면 기 설정된 회전 방향이 시계 방향인 경우, 복수의 타겟 영역(즉, 제1, 제2 및 제 5 영역)에 대한 응시 순서는 제1 영역, 제2 영역 및 제5 영역의 순서일 수 있다. 일 예로, 응시 순서는 타겟 영역에 위치한 사용자와 로봇 간의 거리에 기초하여 설정될 수 있다. 구체적으로, 프로세서는 사용자의 위치에 기초하여 각각의 타겟 영역에 위치한 적어도 하나의 사 용자와 로봇 간의 거리를 식별할 수 있다. 여기서, 사용자의 위치는 획득된 이미지에 포함된 뎁 스 정보 또는 센서(미도시)를 통해 획득된 거리 정보에 기초하여 식별될 수 있다. 그리고, 프로세서는 식별된 적어도 하나의 사용자와 로봇 간의 거리에 기초하여, 타겟 영역(3 0)에 대응하는 거리 정보를 식별할 수 있다. 그리고, 프로세서는 식별된 거리 정보에 기초하여, 사용자 와 로봇 간의 거리가 가까운 순서로 타겟 영역에 대한 응시 순서를 결정할 수 있다. 이때, 타겟 영역에 복수의 사용자가 위치한 경우, 프로세서는 복수의 사용자 중 로봇과의 거리가 더 가까운 사용자의 거리로, 타겟 영역의 거리 정보를 식별할 수 있다. 또한, 일 예로 응시 순서는 타겟 영역에 위치한 사용자의 인터렉션 스코어에 기초하여 설정될 수 있다. 여기서, 인터렉션 스코어는 사용자가 로봇에 대하여 수행하는 인터렉션의 빈도 수, 시간 등에 기초하여 산출된 스코어일 수 있다. 이하, 각각의 사용자의 인터렉션에 기초하여 응시 순서를 결정하는 본 개시의 실시 예에 대하여 설명하도록 한다. 도 6은 본 개시의 일 실시 예에 따른 인터렉션 스코어에 기초하여 응시 순서를 결정하는 방법을 나타낸 순서도 이다. 도 7a 및 7b는 본 개시의 일 실시 예에 따른 인터렉션 스코어에 기초하여 응시 순서를 결정하는 방법을 나타낸 예시도이다. 도 6에 도시된 S605 및 S635 내지 S655 단계는 도 3에 도시된 S310 내지 S360 단계에 각각 대응될 수 있다. 프로세서는 먼저 복수의 사용자의 인터렉션 정보를 획득할 수 있다. 이와 관련하여, 본 개시의 일 실 시 예에 따라 프로세서는 카메라를 통해 획득된 이미지에 기초하여, 복수의 사용자의 로봇(10 0)에 대한 응시 시간을 식별할 수 있다(S610). 구체적으로, 프로세서는 카메라를 통해 획득된 복수의 이미지를 바탕으로, 각각의 사용자의 얼굴을 인식하거나 직접 사용자의 눈을 검출할 수 있다. 예를 들어, AdaBoost 알고리즘, HOG(Histogram of Oriented Gradients) 알고리즘, Haar Cascade 알고리즘 등을 이용하여 프로세서는 이미지 내에서 사람의 눈을 검출할 수 있다. 그리고, 프로세서는 복수의 이미지 내에서 검출된 사용자의 눈을 트래킹하여 각각의 사용자의 로봇의 응시 여부 및 로봇에 대한 응시 시간을 식 별할 수 있다. 또한, 프로세서는 로봇의 마이크(미도시)를 통해 획득된 사용자 음성에 기초하여 복수의 사용자(20 0)의 로봇에 대한 음성 입력 횟수를 식별할 수 있다(S615). 구체적으로, 프로세서는 로봇의 마 이크(미도시)를 통해 입력되는 복수의 사용자의 음성을 각각 인식하고, 각각의 사용자와 인식된 사용 자 음성을 매칭한 후 복수의 사용자의 음성 입력 횟수를 식별할 수도 있다. 이를 위해, 프로세서 는 HMM(Hidden Markov Models) 알고리즘, LSTM(Long Short-Term Memory) 모델, CNN 모델, Transformer 모델 등을 이용할 수 있다. 이때, 프로세서는 식별된 응시 시간 및 음성 입력 횟수를 기초로, 복수의 사용자의 인터렉션 스코어 를 산정할 수 있다(S620). 구체적으로, 프로세서는 각각의 사용자의 응시 시간 및 음성 입력 횟수에 대응 하는 인터렉션 스코어를 산정할 수 있다. 일 예로, 프로세서는 각각의 사용자의 로봇의 응시 시간에 대응하는 제1 인터렉션 스코어를 산정할 수 있다. 특히, 프로세서는 사용자의 로봇에 대한 응시 시간 이 길수록 제1 인터렉션 스코어를 높은 값으로 산정할 수 있다. 그리고, 프로세서는 각각의 사용자의 로봇 에 대한 음성 입력 획수에 대응하는 제2 인터렉션 스코어를 산정할 수 있다. 특히, 프로세서는 음성 입력 횟수가 많을수록, 제2 인터렉션 스코어를 높은 값으로 산정할 수 있다. 이 밖에도, 프로세서는 로봇의 디스플레이(미도시)를 통해 입력되는 사용자의 터치 입력 횟수를 식별하고, 식별된 터치 입력 횟수에 대응되는 제3 인터렉션 스코어를 각각의 사용자에 대하여 산정할 수도 있다. 한편, 프로세서는 제1 및 제2 인터렉션 스코어를 합산하여, 각각의 사용자에 대한 인터렉션 스코어를 산정 할 수 있다. 도 7a 및 7b를 참조하면 프로세서는 4명의 사용자(200-1 내지 200-4) 의 로봇에 대한 응 시 시간 및 음성 입력 횟수에 기초하여 각각 0.66, 0.55, 0.59 및 0.78의 인터렉션 스코어를 산정하였다. 그리고 프로세서는 각 타겟 영역에 위치한 적어도 하나의 사용자의 인터렉션 스코어에 기초하여, 각 타겟 영역에 대응하는 인터렉션 스코어를 식별할 수 있다(S625). 일 예로, 프로세서는 각 타겟 영역에 위치한 복수의 사용자의 인터렉션 스코어를 합산한 값을 각 타 겟 영역에 대응하는 인터렉션 스코어로 식별할 수 있다. 구체적으로, 도 7a를 참조하면 프로세서는 4명의 사용자(200-1 내지 200-4) 의 인터렉션 스코어(0.66, 0.55, 0.59 및 0.78)에 기초하여 각 타겟 영역에 대응하 는 인터렉션 스코어를 식별할 수 있다. 이때, 프로세서는 2명의 사용자가 위치한 제2 타겟 영역에 대응하 는 인터렉션 스코어로 2명의 사용자의 인터렉션 스코어(0.55 및 0.59)를 합산한 1.14로 식별할 수 있다. 또 다른 예로, 프로세서는 각 타겟 영역에 위치한 복수의 사용자의 인터렉션 스코어 중 가장 높은 값 에 해당하는 인터렉션 스코어를 각 타겟 영역에 대응하는 인터렉션 스코어로 식별할 수 있다. 구체적으로, 도 7a를 참조하면 프로세서는 2명의 사용자가 위치한 제2 타겟 영역에 대응하는 인터렉션 스코어를 높은 값을 갖는 사용자의 인터렉션 스코어(0.59)로 식별할 수 있다. 그리고, 프로세서는 식별된 각 타겟 영역에 대응하는 인터렉션 스코어에 기초하여 복수의 타겟 영역에 대한 응시 순서를 결정할 수 있다(S630). 구체적으로, 프로세서는 인터렉션 스코어가 높은 순서대로 복수 의 타겟 영역에 대한 응시 순서를 결정할 수 있다. 예를 들어, 도 7a에 도시된 바와 같이, 타겟 영역에 위치한 복수의 사용자의 인터렉션 스코어를 합산한 값 에 기초하여 각 타겟 영역에 대응하는 인터렉션 스코어를 식별하는 경우, 프로세서는 복수의 타겟 영역 (즉, 제1, 제2 및 제5 영역)에 대한 응시 순서를 제2 영역, 제5 영역 및 제1 영역의 순서로 결정할 수 있다. 또 다른 예를 들어, 도 7b에 도시된 바와 같이, 각 타겟 영역에 위치한 복수의 사용자의 인터렉션 스코어 중 가장 높은 값에 해당하는 인터렉션 스코어를 각 타겟 영역에 대응하는 인터렉션 스코어를 식별하는 경우, 프 로세서는 복수의 타겟 영역(즉, 제1, 제2 및 제5 영역)에 대한 응시 순서를 제5 영역, 제1 영역 및 제 2 영역의 순서로 결정할 수 있다. 한편, 프로세서는 각 타겟 영역에 위치한 사용자와 로봇 간의 거리 및 각 사용자의 인터렉션 스코어 를 모두 고려하여 응시 순서를 결정할 수 있다. 구체적으로, 프로세서는 복수의 사용자의 인터렉션 스코어에 기초하여 복수의 사용자 중 인터렉션 스코어가 가장 높은 메인 사용자를 식별할 수 있다. 이때, 프로세서는 메인 사용자가 위치한 타겟 영역을 최초 응시 타겟 영역으로 응시 순서를 결정할 수 있다. 그 리고, 프로세서는 나머지 타겟 영역에 대한 응시 순서는 각각의 타겟 영역에 위치한 사용자와 로봇 간의 거리가 가까운 순서대로 결정할 수도 있다. 다만, 이에 제한되는 것은 아니며 프로세서는 각 타겟 영역에 위치한 사용자와 로봇 간의 거리 및 각 사용자의 인터렉션 스코어에 기초하여 다양한 방법으 로 응시 순서를 결정할 수 있다. 이하에서는, 본 개시의 설명의 편의를 위하여 프로세서가 복수의 타겟 영역에 대한 응시 순서를 제5 영역, 제1 영역 및 제2 영역의 순서로 결정한 것으로 상정하여 설명하도록 한다. 한편, 프로세서는 복수의 타겟에 대한 응시 순서에 기초하여 복수의 타겟 영역을 연속적으로 응시하도록 로봇을 회전 시킬 수 있다. 예를 들어, 도 5에서 복수의 타겟 영역에 대한 응시 순서가 제5 영역, 제1 영 역 및 제2 영역의 순서라면, 프로세서는 로봇이 제5 영역을 최초로 응시한 후 제1 영역을 응시하고, 마지막으로 제2 영역을 응시하도록 구동부를 제어할 수 있다. 이를 위해, 프로세서는 응시 순서에 고려하여 각각의 타겟 영역에 대한 회전 방향 및 회전 각도를 식 별할 수 있다. 특히, 로봇이 복수의 타겟 영역 중 어느 하나의 타겟 영역을 응시한 후 또 다른 타 겟 영역을 응시하도록 구동부를 제어하기 위해서, 프로세서는 응시 순서 내에서 이전 순서의 타겟 영 역의 위치를 고려하여 다음 순서의 타겟 영역에 대한 로봇의 회전 방향과 회전 각도를 식별해야 한다. 도 8은 본 개시의 일 실시 예에 따른 응시 순서에 따른 복수의 타겟 영역에 대한 회전 방향 및 회전 각도를 식 별하기 위한 제어 방법을 나타낸 순서도이다. 도 8에 도시된 S510, S550 내지 S580은 도 3에 도시된 S310, S330 내지 S360에 대응될 수 있다. 도 9 및 도 10은 본 개시의 일 실시 예에 따른 응시 순서에 따른 복수의 타겟 영 역에 대한 회전 방향 및 회전 각도를 식별하는 것을 나타낸 예시도이다. 도 8를 참조하면, 본 개시의 일 실시 예에 따라 프로세서는 응시 순서에 기초하여 복수의 타겟 영역 중 최초 타겟 영역을 식별하고, 최초 타겟 영역의 위치에 기초하여 최초 타겟 영역에 대응되는 로 봇의 회전 각도 및 회전 방향을 식별할 수 있다(S830). 여기서, 최초 타겟 영역은 응시 순서에서 첫 번째 순서에 해당하는 타겟 영역일 수 있다. 그리고, 최초 타겟 영역에 대응하는 회전 각도 및 회전 방향은 로봇이 최초 타겟 영역을 향하도록 하는데 필요 한 회전 각도와 회전 방향일 수 있다. 구체적으로, 프로세서는 복수의 타겟 영역을 식별하는 시점의 로봇의 자세를 기준으로 최초 타겟 영역에 대응하는 회전 각도 및 회전 방향을 식별할 수 있다. 이때, 프로세서는, 복수의 타겟 영역(3 0)을 식별하는 시점의 로봇의 자세에서 최초 타겟 영역의 위치를 식별하고, 식별된 최초 타겟 영역 의 위치에 기초하여 최초 타겟 영역에 대응하는 회전 각도와 회전 방향을 식별할 수 있다. 특히, 프로세서는 카메라의 화각을 분할하는데 이용된 기 설정된 각도를 바탕으로, 최초 타겟 영역 에 대응하는 회전 각도와 회전 방향을 식별할 수 있다. 도 9을 참조하면, 프로세서는 응시 순서에서 최초 타겟 영역을 제5 영역으로 식별하고, 제5 영역의 위 치를 식별할 수 있다. 프로세서는, 복수의 영역 중 5번째에 위치한 제5 영역의 위치를 식별한 후 로봇 의 현재 자세에서 제5 영역을 향하기 위해 필요한 회전 각도와 회전 방향을 식별할 수 있다. 그리고, 프로 세서는, 제5 영역에 대한 회전 각도를 60°로 회전 방향을 시계 방향으로 식별할 수 있다. 이때, 제5 영역 에 대한 회전 각도 (60°= 2Х30°)는 카메라의 화각을 분할하는데 이용된 기 설정된 각도(30°)를 바탕으로 식 별될 수 있다. 한편, 프로세서는 최초 타겟 영역에 대응하는 회전 방향과 회전 각도를 식별한 후에는 나머지 타겟 영 역에 대한 회전 방향과 회전 각도를 식별할 수 있다(S840). 이때, 나머지 타겟 영역은 응시 순서에서 최초 타겟 영역 이후의 순서에 대응하는 타겟 영역일 수 있다. 이와 관련하여 본 개시의 일 실시 예에 따라 프로세서는 복수의 타겟 영역 중 나머지 타겟 영역에 대응되는 로봇의 회전 각도 및 회전 방향을 나머지 타겟 영역 이전 순서의 타겟 영역의 위치 및 나머지 타겟 영역의 위치에 기초하여 식별할 수 있다(S840). 최초 타겟 영역 이후의 나머지 타겟 영역의 경우, 프로세서는 특정 타겟 영역을 로봇이 향하도록 제어한 후 연속적으로 또 다른 타겟 영역을 향하도록 로봇을 제어해야 하므로, 이전 순서의 타겟 영역의 위치를 고려하여 각각의 타겟 영역에 대한 회전 방향 및 회전 식별해야만 한다. 구체적으로, 프로세서는 나머지 타겟 영역에 대해서는, 응시 순서에서 각각의 타겟 영역에 대응하 는 순서의 이전 순서에 대응하는 타겟 영역의 위치를 각각 식별할 수 있다. 그리고, 프로세서는 이전 순서의 타겟 영역의 위치와 다음 순서의 타겟 영역의 위치를 바탕으로, 이전 순서의 타겟 영역을 로봇이 응시한 후 다음 순서의 타겟 영역을 응시하기 위하여 필요한 회전 각도와 회전 방향을 식별할 수 있다. 이때, 프로세서는 카메라의 화각을 분할하는데 이용된 기 설정된 각도를 바탕으로, 나머지 타겟 영역에 대응하는 회전 각도와 회전 방향을 식별할 수 있다. 도 9 및 도 10을 참조하면, 프로세서는 응시 순서에서 첫 번째 순서에 대응하는 최초 타겟 영역에 대 한 회전 방향과 회전 각도를 식별한 후 첫 번째 순서 이후의 나머지 타겟 영역(즉, 제1 및 제2 영역)에 대 한 회전 방향과 회전 각도를 식별할 수 있다. 이때, 프로세서는 응시 순서에 기초하여 두 번째 타겟 영역 으로 제1 영역을 세 번째 타겟 영역을 제2 영역을 식별할 수 있다. 도 9을 참조하면, 프로세서는 응시 순서에서 두 번째 타겟 영역인 제1 영역에 대한 로봇의 회전 방향과 회전 각도를 식별하기 위하여, 제1 영역의 위치와 제1 영역의 이전 순서(즉, 첫 번째)인 제5 영역(즉, 첫 번째 타겟 영역)의 위치를 고려할 수 있다. 구체적으로, 프로세서는 제5 영역의 위치와 제1 영역의 위치를 고려하여, 제5 영역에서 제1 영역으로 로봇 을 회전시키기 위한 회전 방향과 회전 각도를 식별할 수 있다. 이때, 프로세서는 제1 영역에 대한 회 전 방향을 반 시계 방향으로, 회전 각도를 120°로 식별할 수 있다. 이후, 도 10을 참조하면 프로세서는 세 번째 타겟 영역인 제2 영역에 대한 로봇의 회전 방향과 회전 각도를 식별하기 위하여, 제2 영역의 위치와 제2 영역의 이전 순서(즉, 두 번째)인 제1 영역(즉, 두 번째 타겟 영역)의 위치를 고려할 수 있다. 구체적으로, 프로세서는 제1 영역의 위치와 제2 영역의 위치를 고려하여, 제1 영역에서 제2 영역으로 로봇 을 회전시키기 위한 회전 방향과 회전 각도를 식별할 수 있다. 그리고, 프로세서는 제2 영역에 대한 회전 방향을 시계 방향으로, 회전 각도를 30°로 식별할 수 있다. 한편, 프로세서는 식별된 회전 각도 및 회전 방향에 기초하여 로봇이 회전하도록 구동부를 제어할 수 있다(S330, S550). 이때, 프로세서는 회전 각도 및 회전 방향에 기초하여 로봇이 각각의 타겟 영역을 향하도록 구동 부를 제어할 수 있다. 상술한 것과 같이 프로세서는 구동부를 제어하여 로봇의 헤드 또는 로봇 의 바디가 복수의 타겟 영역을 순차적으로 응시하도록 회전시킬 수 있다. 이로써, 각각의 타겟 영역에 위치한 복수의 사용자는 로봇이 적극적으로 사용자와 인터렉션 을 수행하는 것으로 인지할 수 있다. 특히, 기존의 로봇의 경우 로봇 주변에 복수의 사용자가 위치한 경우, 특정 사용자 만을 응시하거나 또는 무분별하게 사용자를 응시함으로써, 복수의 사용자 가 로봇과 적절한 인터렉션을 수행하고 있음을 느끼기 어려웠다. 그러나 본 개시는 복수의 사용자 중 특정 사용자 만을 응시하는 것이 아니며, 복수의 사용자를 순차적으로 그리고 반복하여 응 시함으로써 로봇을 이용하는 복수의 사용자가 보다 적극적이고 원활하게 로봇과 인터렉션을 수 행하고 있음을 느낄 수 있도록 한다. 한편, 프로세서는 로봇이 각각의 타겟 영역을 기 설정된 시간 동안 응시하도록 제어할 수 있다. 이하에서는, 타겟 영역을 응시하도록 설정된 시간을 응시 시간이라 지칭하도록 한다. 프로세서는 구동 부를 제어하여 로봇이 타겟 영역을 향해 회전하면, 기 설정된 응시 시간 동안 로봇이 타겟 영역 을 향하여 정지하도록 구동부를 제어할 수 있다. 이로써, 로봇은 복수의 타겟 영역에 대하여 기 설정된 응시 시간 동안 정지한 상태로 타겟 영역을 향해 서 있을 수 있다. 한편, 각각의 타겟 영역에 대한 응시 시간은 다르게 설정될 수 있다. 일 예로, 프로세서는 각각의 타 겟 영역에 위치한 적어도 하나의 사용자의 인터렉션 스코어에 기초하여, 각 타겟 영역에 대응하는 인 터렉션 스코어를 식별하고, 각 타겟 영역에 대응하는 인터렉션 스코어에 기초하여 각 타겟 영역에 대한 응시 시 간을 결정할 수 있다. 특히, 프로세서는 각 타겟 영역에 대응하는 인터렉션 스코어와 비례하여 각 타겟 영 역에 대한 응시 시간을 결정할 수 있다. 즉, 프로세서는 타겟 영역에 대응하는 인터렉션 스코어가 클수록 타겟 영역에 대한 응시 시간을 길게 설정할 수 있다. 또한, 일 예로 프로세서는 각 타겟 영역에 위치한 사용자의 수에 기초하여 각각의 타겟 영역에 대한 응시 시간을 설정할 수 있다. 특히, 프로세서는 각각의 타겟 영역에 위치한 사용자의 수와 비례하여 각 타겟 영역에 대한 응시 시간을 결정할 수 있다. 즉, 프로세서는 타겟 영역에 위치한 사용 자의 수가 많을수록, 타겟 영역에 대한 응시 시간을 길게 설정할 수 있다. 한편, 프로세서는 로봇이 결정된 응시 시간 동안 각 타겟 영역을 응시하도록 구동부를 제어할 수 있 다. 한편, 프로세서는 식별된 회전 각도 및 회전 방향에 기초하여 응시 순서에 따라 반복하여 로봇이 복 수의 타겟 영역을 응시하도록 구동부를 제어할 수 있다. 상술한 예를 들어 다시 설명하면, 응시 순서가 제5 영역, 제1 영역 및 제2 영역이면, 프로세서는 응시 순서에 따라 로봇이 마지막 순서인 제2 영역을 응 시하도록 구동부를 제어한 이후, 다시 제5 영역을 향하도록 구동부를 제어할 수 있다. 보다 구체적으로, 제5 영 역, 제1 영역 및 제2 영역의 응시 순서를 1 주기(또는 1 Cycle)라고 한다면, 프로세서는 주기적으로(또는 반복하여) 제5 영역, 제1 영역 및 제2 영역을 로봇이 응시하도록 구동부를 제어할 수 있다. 한편, 응시 순 서에는 복수의 주기 동안의 타겟 영역에 대한 응시 순서가 포함될 수 있다. 즉, 도 9 및 도 10에는 응시 순 서 내 1 주기의 응시 순서만이 포함된 것으로 도시되었으나, 실시 예에 따라서는 1 주기 이후의 2 주기, 3 주기 의 응시 순서까지도 포함될 수 있다. 한편, 프로세서는 로봇이 식별된 회전 각도 및 회전 방향에 기초하여 회전하는 동안, 새로운 사용자 를 식별할 수 있다. 여기서, 새로운 사용자는 프로세서가 기존에는 인식하지 못했으나 새롭게 인식된 사용자일 수 있다. 예를 들어, 로봇이 회전됨에 따라 카메라의 화각은 변경될 수 있으며, 이때 변경된 화각 내에서 새롭게 인지되는 사용자가 새로운 사용자에 해당할 수 있다. 특히, 새 로운 사용자 또한 복수의 사용자와 동일한 사용자 그룹에 포함된 것으로 식별되는 사용자일 수 있다. 또는 계속하여 로봇 주변에 위치한 사용자로 프로세서가 기존에 인식하였으나, 동일한 사용자 그룹에 포함되지 않은 것으로 식별되어 제외된 사용자일 수 있다. 이때, 프로세서는 제외된 사용자 가 이후 사용자 그룹에 포함된 복수의 사용자와의 인터렉션을 통해 뒤늦게 동일한 사용자 그룹에 합 류된 것으로 식별되면, 제외된 사용자를 새로운 사용자로 인식할 수도 있다. 본 개시의 일 실시 예에 따라 프로세서는, 로봇이 회전하는 동안 카메라를 통해 획득된 이미지에 기 초하여 새로운 사용자가 인식되면, 회전된 로봇의 카메라의 화각 내의 복수의 영역 중 새로 운 사용자가 위치하는 새로운 타겟 영역이 응시 순서에 포함되도록 응시 순서를 조정할 수 있다 (S360). 구체적으로, 프로세서는 로봇이 회전함에 따라 기존에 인식되었던 복수의 사용자가 아닌 새로운 사용자가 인식되면, 로봇이 새로운 사용자를 응시하도록 제어할 수 있다. 특히, 새로운 사용자 가 기존의 복수의 사용자와 동일한 사용자 그룹에 포함된 것으로 식별되면, 프로세서는 새로운 사용자를 사용자 그룹에 포함시키고, 복수의 사용자와 함께 새로운 사용자 또한 로봇이 응 시할 수 있도록 로봇을 제어할 수 있다. 이를 위해, 프로세서는 기존의 응시 순서를 조정할 수 있다. 특히 새로운 사용자가 기존의 식별된 복 수의 타겟 영역이 아닌, 새로운 타겟 영역에 위치하는 경우, 프로세서는 기존의 응시 순서를 조정 할 수 있다. 또한, 새로운 타겟 영역이 기존의 식별된 복수의 영역 중에서 식별되지 않고, 다른 위치 (또는 다른 복수의 영역)에서 식별되는 경우, 프로세서는 새로운 타겟 영역이 응시 순서에 포함되 도록 응시 순서를 조정할 수 있다. 이하, 이와 관련된 본 개시의 실시 예에 대하여 도 11 내지 도 16를 참조하여 자세히 설명하도록 한다. 도 11은 본 개시의 일 실시 예에 따른 새로운 사용자가 인식되면, 새로운 사용자가 위치한 새로운 영역을 인식 하는 것을 나타낸 예시도이다. 도 12는 새로운 사용자가 인식되면, 복수의 타겟 영에 대한 회전 각도 및 회전 방향을 재 식별하기 위한 제어 방법을 나타낸 순서도이다. 도 12에 도시된 S1210, S1220, S1230, S1270 및 S1280은 도 3에 도시된 S310, S320, S330, S350 및 S360에 각각 대응될 수 있다. 프로세서는 로봇이 회전하는 동안 카메라를 통해 획득된 이미지에 기초하여 로봇 주변에 위치한 새로운 사용자가 인식할 수 있다(S1240). 구체적으로, 프로세서는 복수의 사용자에 대한 특징 정보를 메모리(미도시)에 저장하고, 저장된 특징 정보에 기초하여 로봇이 카메라를 통해 획득된 이미지 내 에서 새로운 사용자를 인식할 수 있다. 프로세서는 이미지 내 사용자에 대한 특징 정보를 획득 한 후 메모리(미도시)에 저장된 복수의 사용자에 대한 특징 정보와 비교하여, 사용자가 기존에 인식 된 복수의 사용자 중 어느 하나인지 또는 새로운 사용자인지 식별할 수 있다. 특히, 프로세서는 획득된 특징 정보가 메모리(미도시)에 저장된 특징 정보와 상이한 것으로 식별되면, 획득된 특징 정보에 대응되 는 사용자가 새로운 사용자임을 인식할 수 있다. 도 12를 참조하면, 프로세서는 새로운 사용자가 인식되면(S1240), 로봇의 카메라의 화각을 기 설정된 각도로 분할하여 복수의 영역을 식별할 수 있다(S1250). 그리고, 로봇은 복수의 영역에 서 새로운 사용자가 위치한 새로운 타겟 영역을 식별할 수 있다. 이에 관하여는 상술한 본 개시의 설 명이 동일하게 적용될 수 있으므로, 상세한 설명은 생략하도록 한다. 구체적으로, 도 11을 참조하면 프로세서는 응시 순서에 따라 회전된 로봇이 제5 영역을 응시하는 동 안, 카메라를 통해 복수의 이미지를 획득할 수 있다. 그리고, 프로세서는 획득된 이미지에 기초하여, 로봇 의 주변의 새로운 사용자를 인식할 수 있다. 새로운 사용자는 로봇이 회전하기 전에 카메라 화 각 범위 밖에 위치함 사용자(200-8)일 수 있다. 이때, 프로세서는 새로운 사용자가 인식되면, 로봇이 제5 영역을 응시하는 상태에서, 카메라의 화각을 기 설정된 각도로 분할하여 복수의 영역을 식별할 수 있다. 그리고, 프로세서는 복수의 영 역 중 새로운 사용자가 위치한 타겟 영역으로 제6 영역을 식별할 수 있다. 한편, 도 11을 참조하면, 프로세서는 로봇이 회전된 상태에서 식별된 복수의 영역은, 로봇 이 타겟 영역을 응시하는 상태에서 식별되었다는 점에서 기존의 복수의 영역과는 상이할 수 있다. 다만, 본 개시의 설명의 편의를 위해 새로운 복수의 영역에 대해서도 기존의 복수의 영역에 대해 부가 된 참조 번호를 중복하여 이용하여 본 개시의 실시 예에 대하여 설명하도록 한다. 그리고, 프로세서는 새로운 타겟 영역이 응시 순서에 포함되도록 응시 순서를 조정할 수 있다(S1260). 이때, 프로세서는 새로운 사용자와 로봇 간의 거리 또는 새로운 사용자의 인터렉션 스코어 에 기초하여 응시 순서를 조정할 수 있다. 일 예로, 프로세서는 로봇이 응시 순서 내에서 아직 응시하지 않은 타겟 영역(또는 응시할 예정 이 영역)(이하, 미응시 타겟 영역)과 새로운 타겟 영역 간의 응시 순서를 조정할 수 있다. 일 예로, 프로세서는 미응시 타겟 영역에 위치한 사용자와 새로운 타겟 영역에 위치한 새로운 사 용자의 인터렉션 스코어에 기초하여, 미응시 타겟 영역)과 새로운 타겟 영역 간의 응시 순서를 조정할 수 있다. 프로세서는 새로운 타겟 영역에 위치한 새로운 사용자의 인터렉션 스코어를 산정할 수 있다. 구체적으로, 로봇이 회전된 상태에서 카메라를 통해 획득된 이미지에 기초하여, 프로세서(13 0)는 새로운 사용자의 로봇에 대한 응시 시간을 식별할 수 있다. 또한, 프로세서는 마이크(미도 시)를 통해 획득된 새로운 사용자의 음성에 기초하여 새로운 사용자의 로봇에 대한 음성 입력 횟수를 식별할 수 있다. 또한, 프로세서는 디스플레이(미도시)를 통해 입력된 새로운 사용자의 터치 입력 횟수 또한 식별할 수 있다. 그리고, 프로세서는 응시 시간 및 음성 입력 횟수(및 터치 입력 횟수)에 기초하여 새로운 사용자의 인터렉션 스코어를 산정할 수 있다. 이와 관련해서는 상술한 본 개시의 실시 예에 대 한 설명이 동일하게 적용될 수 있으므로, 자세한 설명은 생략하도록 한다. 한편, 프로세서는 미응시 타겟 영역과 새로운 타겟 영역에 대응하는 인터렉션 스코어를 각각 비교하고, 미응시 타겟 영역과 새로운 타겟 영역 간의 응시 순서를 조정할 수 있다. 인터렉션 스코어에 기 초하여 새로운 타겟 영역과 미응시 타겟 영역 간의 응시 순서를 조정하는 방법은 상술한 본 개시의 실시 예 의 설명이 동일하게 적용될 수 있으므로, 자세한 설명은 생략하도록 한다. 또한, 프로세서는 미응시 타겟 영역에 위치한 사용자와 로봇 간의 거리 및 새로운 타겟 영역 에 위치한 사용자와 로봇 간의 거리를 각각 비교하여, 미응시 타겟 영역과 새로운 타겟 영역 간의 응시 순서를 조정할 수도 있다. 이와 관련해서는 상술한 본 개시의 실시 예에 대한 설명이 동일하게 적용 될 수 있으므로, 자세한 설명은 생략하도록 한다. 도 13은 본 개시의 일 실시 예에 따른 새로운 타겟 영역을 응시하기 위한 로봇의 회전 각도가 기 설정된 임계 각도 이상인 경우, 새로운 타겟 영역을 응시 순서에 포함시키지 않는 것을 나타낸 예시도이다. 한편, 본 개시의 일 실시 예에 따라 프로세서는 새로운 타겟 영역을 응시하기 위한 로봇의 회전 각도가 기 설정된 임계 각도 미만인 경우에, 새로운 타겟 영역이 응시 순서에 포함되도록 응시 순서를 조정 할 수 있다. 이에 따라, 프로세서는 새로운 타겟 영역을 응시하기 위한 로봇의 회전 각도가 기 설정된 임계 각도 이상이면, 새로운 타겟 영역을 응시 순서에 포함시키지 않고 상기 응시 순서를 유지할 수 있다. 구체적으로, 프로세서가 로봇이 회전하는 동안 새로운 사용자 및 새로운 사용자가 위치한 새로운 타 겟 영역이 식별될 때마다, 새로운 타겟 영역을 포함하도록 응시 순서를 조정한다면, 오히려 기존의 복 수의 사용자들의 로봇과의 인터렉션 기회가 감소하는(또는 인터렉션을 위한 대기 시간이 증가하는) 문제가 발생할 수 있다. 그렇기 때문에, 프로세서는 최초로 응시 순서를 결정하는 시점에 로봇의 회전 각도에 대한 임계 각도 를 설정할 수 있다. 그리고, 프로세서는 새로운 사용자이 인식될 때마다, 새로운 사용자가 위치한 새로운 타겟 영역을 응시하기 위한 로봇의 회전 각도가 설정된 임계 각도 이내인지 식별할 수 있다. 예를 들 어, 프로세서는 최초로 응시 순서를 결정하는 시점에 임계 범위를 설정할 수 있다. 여기서, 임계 범위 는 최초로 응시 순서를 결정하는 시점에 로봇의 자세, 화각 범위, 복수의 영역의 각도, 로봇 의 회전 각도의 임계 각도 등에 기초하여 설정될 수 있다. 그리고, 프로세서는 새로운 타겟 영역의 위치가 기 설정된 임계 범위 이내인지 식별함으로써 새로 운 타겟 영역을 응시하기 위한 로봇의 회전 각도가 설정된 임계 각도 이내인지 식별할 수 있다. 프로 세서는 새로운 타겟 영역의 위치가 기 설정된 임계 범위 이내인 것으로 식별되면, 새로운 타겟 영 역이 응시 순서에 포함되도록 응시 순서를 조정할 수 있다. 반면에, 프로세서는 새로운 타겟 영역(3 1)의 위치가 기 설정된 임계 범위를 벗어난 것으로 식별되면, 새로운 타겟 영역을 응시 순서에 포함시 키지 않고, 응시 순서를 그대로 유지할 수 있다.도 13을 참조하면, 프로세서는 로봇이 최초 응시 순서를 결정한 시점에 로봇의 회전 각도와 관 련한 임계 범위를 210°로 설정할 수 있다. 이때, 프로세서는 로봇이 제5 영역의 사용자를 응시 하는 동안 새롭게 인식된 새로운 사용자가 위치한 타겟 영역을 새로운 복수의 영역 중에서 식별할 수 있다. 이때, 새로운 타겟 영역이 제7 영역인 경우, 프로세서는 제7 영역이 설정된 임계 범위(210 °)를 벗어난 것으로 식별할 수 있다. 따라서, 프로세서는 새로운 타겟 영역(제7 영역)에 대해서는 로 봇이 응시하지 않는 것으로 결정할 수 있다. 즉, 프로세서는 새로운 타겟 영역 (제7 영역)을 응 시 순서에 포함시키지 않으며, 기존의 타겟 영역(제1, 제2 및 제5 영역) 하는 응시 순서를 그대로 유지시킬 수 있다. 한편, 응시 순서를 조정한 후 프로세서는 새로운 타겟 영역의 위치 및 조정된 응시 순서에 기초하여 로봇의 회전 각도 및 회전 방향을 재 식별할 수 있다(S1270). 그리고, 프로세서는 재 식별된 회전 각 도 및 회전 방향에 기초하여 로봇이 회전하도록 구동부를 제어할 수 있다(S1280). 일 예로, 프로세서는 회전된 로봇의 카메라를 통해 획득된 이미지에 기초하여 새로운 사용자가 인식되면, 복수의 영역 중 새로운 사용자가 위치하는 새로운 타겟 영역의 위치에 기초하여 새로운 타겟 영역에 대응되는 로봇의 회전 각도 및 회전 방향을 식별할 수 있다. 그리고, 프로세서는 식 별된 타겟 영역에 대응되는 로봇의 회전 각도 및 회전 방향에 기초하여 로봇이 회전하도록 구동 부를 제어할 수 있다. 구체적으로, 도 11을 참조하면 로봇이 제5 영역을 응시하는 동안 식별된 새로운 사용자가 제6 영역에 위치한 것으로 식별되면, 프로세서는 새롭게 식별된 타겟 영역(즉, 제6 영역)에 대한 위치를 식별할 수 있다. 이때, 프로세서는 제6 영역이 새롭게 식별된 복수의 영역 중 4번째 위치한 것으로 식별하고, 제6 영역에 대한 회전 각도 및 회전 방향을 시계 방향 30°로 식별할 수 있다. 그리고, 프로세서는 식별된 제6 영역에 대한 회전 각도 및 회전 방향을 기초하여 로봇이 기존의 복수의 타겟 영역뿐만 아니라 새 롭게 식별된 새로운 타겟 영역 또한 응시하도록 구동부를 제어할 수 있다. 다만, 새로운 타겟 영역이 식별됨에 따라 응시 순서가 조정되는 경우 기존의 타겟 영역에 대한 회전 방 향과 회전 각도가 변경될 수 있다. 뿐만 아니라, 새로운 타겟 영역 또한 조정된 응시 순서 내에서의 새로운 타겟 영역의 순서에 따라서, 프로세서는 새로운 타겟 영역의 위치뿐만 아니라 다른 타겟 영역(3 0)의 위치 까지도 함께 고려하여 새로운 타겟 영역에 대한 로봇의 회전 방향과 회전 각도를 식별해야 만 한다. 이를 위해, 일 예로 프로세서는 프로세서는 조정된 응시 순서에 기초하여 미응시 타겟 영역 및 새로 운 타겟 영역 중 회전된 로봇이 응시하고 있는 타겟 영역의 다음 순서의 다음 타겟 영역을 식별할 수 있다. 여기서, 다음 타겟 영역은 프로세서가 로봇이 회전된 상태에서 새로운 사용자를 인식하고, 새로 운 사용자가 위치한 새로운 타겟을 영역을 식별한 시점에 로봇이 응시하는 영역의 다음 순서의 타겟 영역일 수 있다. 이때, 다음 타겟 영역은 조정된 응시 순서에 기초하여 식별될 수 있다. 구체적으로, 도 11을 참조하면 회전된 로봇이 복수의 타겟 영역 중 제5 영역을 응시하는 동안, 프로세 서는 로봇 주변의 새로운 사용자를 인식하였다. 이때, 프로세서는 새로운 타겟 영역인 제6 영역을 응시 순서에 포함되도록 응시 순서를 조정할 수 있다. 그리고, 프로세서는 조정된 응시 순서 내에서 로봇이 응시하고 있는 제5 영역 이후의 다음 순서의 타겟 영역을 식별할 수 있다. 도 14, 도 15 및 도 16은 본 개시의 일 실시 예에 따른 새로운 타겟 영역의 위치에 기초하여 로봇의 회전 방향 및 회전 각도를 재 식별하는 것을 나타낸 예시도이다. 예를 들어, 도 14을 참조하면, 로봇이 기존의 응시 순서(제5 영역→제1 영역→제2 영역)를 새로운 타겟 영 역인 제6 영역을 포함시켜 조정한 경우, 조정된 응시 순서(제5 영역→제1 영역→제2 영역→제6 영역) 내에 서의 다음 타겟 영역은 제5 영역 이후의 순서인 제1 영역일 수 있다. 즉, 기존의 응시 순서와 동일하게 조정된 응시 순서를 따르더라도, 프로세서는 로봇이 제5 영역을 응시한 후에는 제1 영역을 응시하도록 구동 부를 제어하여 로봇을 제1 영역을 향해 회전시킬 수 있다. 반면에 도 15을 참조하면, 로봇이 기존의 응시 순서(제5 영역*제1 영역*제2 영역)를 새로운 타겟 영역(3 1)인 제6 영역을 포함시켜 조정한 경우, 조정된 응시 순서(제5 영역*제6 영역*제1 영역*제2 영역) 내에서의 다음 타겟 영역은 제5 영역 이후의 순서인 제6 영역일 수 있다. 이 경우, 프로세서는 조정된 응시 순서에 따 라 로봇이 제5 영역을 응시한 후에는 제1 영역이 아닌 제6 영역을 응시하도록 구동부를 제어하여 로봇 을 제6 영역을 향해 회전시킬 수 있다. 한편, 프로세서는 다음 타겟 영역의 위치에 기초하여 다음 타겟 영역에 대응되는 로봇의 회전 각도 및 회전 방향을 재 식별할 수 있다. 그리고, 프로세서는 미응시 타겟 영역 및 새로운 타겟 영역 중 다 음 타겟 영역 이후의 나머지 타겟 영역에 대응되는 로봇의 회전 각도 및 회전 방향을 나머지 타겟 영 역 이전 순서의 타겟 영역의 위치 및 나머지 타겟 영역의 위치에 기초하여 재 식별할 수 있다. 구체적으로, 프로세서는 새로운 타겟 영역을 식별하는 시점의 로봇의 자세를 기준으로 다음 타겟 영역에 대응하는 회전 각도 및 회전 방향을 식별할 수 있다. 이때, 프로세서는, 새로운 타겟 영역을 식별하는 시점에 로봇이 응시하고 있는 타겟 영역의 위치를 기준으로 다음 타겟 영역의 위치를 식별할 수 있다. 그리고, 프로세서는 식별된 다음 타겟 영역의 위치에 기초하여 다음 타겟 영역에 대응하는 회전 각도와 회전 방향을 식별할 수 있다. 특히, 프로세서는 카메라의 화각을 분할하는데 이용된 기 설정된 각도를 바탕으로, 최초 타겟 영역에 대응하는 회전 각도와 회전 방향을 식별할 수 있다. 도 14을 참조하면, 프로세서는 조정된 응시 순서에서 다음 타겟 영역을 제1 영역으로 식별하고, 제1 영역 의 위치를 식별할 수 있다. 특히 프로세서는 응시 순서가 조정되기 전에 제1 영역에 대한 회전 방향 및 회 전 각도에 기초하여 화각 내에서 식별되지 않는 제1 영역의 위치를 식별할 수 있다. 그리고, 프로세서(13 0)는 제5 영역을 응시하는 로봇이 다음 타겟 영역인 제1 영역을 향하기 위해 필요한 회전 각도와 회전 방 향을 식별할 수 있다. 그리고, 프로세서는, 제1 영역에 대한 회전 각도를 120°로 회전 방향을 시계 방향 으로 식별할 수 있다. 이때, 제1 영역에 대한 회전 각도 (120°= 4Х30°)는 카메라의 화각을 분할하는데 이용된 기 설정된 각도(30°)를 바탕으로 식별될 수 있다. 즉, 조정된 응시 순서 내에서도 제5 영역 후의 다음 타겟 영역이 제1 영역으로 유지된다는 점에서, 제1 영역에 대한 로봇의 회전 방향 및 회전 각도는 변경되 지 않을 수 있다. 반면에 도 15을 참조하면, 프로세서는 조정된 응시 순서에서 다음 타겟 영역을 제6 영역으로 식별하고, 제 6 영역의 위치를 식별할 수 있다. 프로세서는 로봇이 제5 영역을 응시한 상태에서 카메라의 화각 내 복수의 영역을 식별하고, 식별된 복수의 영역 내에서 제6 타겟 영역의 위치를 식별할 수 있다. 그리고, 프로세서는 제5 영역을 응시하는 로봇이 다음 타겟 영역인 제6 영역을 향하기 위해 필요한 회전 각도와 회전 방향을 식별할 수 있다. 그리고, 프로세서는, 제6 영역에 대한 회전 각도를 30°로 회전 방향을 시계 방향으로 식별할 수 있다. 이때, 제6 영역에 대한 회전 각도 (30°= 1Х30°)는 카메라의 화각을 분할하는데 이용된 기 설정된 각도 (30°)를 바탕으로 식별될 수 있다. 즉, 조정된 응시 순서 내에서도 제5 영역 후의 다음 타겟 영역은 제1 영역 에서 제6 영역으로 변경되었다는 점에서, 제5 영역을 응시한 이후에 프로세서가 구동부를 제어하여 로봇 을 회전시켜야 하는 각도와 방향이 변경되었다. 한편, 프로세서는 다음 타겟 영역에 대응하는 회전 방향과 회전 각도를 식별한 후에는 나머지 타겟 영역 에 대한 회전 방향과 회전 각도를 식별할 수 있다. 이때, 나머지 타겟 영역은 응시 순서에서 다음 타겟 영역 이후의 순서에 대응하는 타겟 영역일 수 있다. 이는 상술한 최초 타겟 영역 이후의 나머지 타겟 영역에 대한 회전 방향과 회전 각도를 식별하는 본 개시에 대한 설명이 동일하게 적용될 수 있다. 특히, 프로세서는 다음 타겟 영역이 미응시 타겟 영역이면, 새로운 타겟 영역의 위치 및 새로운 타겟 영역 이전의 미응시 타겟 영역의 위치에 기초하여 새로운 타겟 영역에 대응되는 로봇의 회전 각도 및 회전 방향을 식별할 수 있다. 구체적으로, 다음 타겟 영역이 미응시 타겟 영역이며, 프로세서는 조정된 응시 순서에 추가된 새로운 타겟 영역의 이전 순서의 타겟 영역의 위치와 새로운 타겟 영역의 위치 를 고려하여 새로운 타겟 영역에 대한 회전 방향 및 회전 각도를 식별할 수 있다. 도 14을 다시 참조하면, 프로세서는 조정된 응시 순서 내에서 제6 영역의 응시 순서를 식별할 수 있다. 그 리고, 프로세서는 4번째의 응시 순서를 갖는 제6 영역 이전 순서인 3번째의 응시 순서를 갖는 타겟 영역 으로 제2 영역 식별할 수 있다. 프로세서는 제2 영역의 위치를 식별한 후 제2 영역에서 제6 영역으로 로봇을 회전시키기 위한 회전 방향과 회전 각도를 제2 영역의 위치와 제6 영역의 위치에 기초하여 식별할 수 있다. 구체적으로, 프로세서는 제2 영역의 위치를 응시 순서가 조정되기 전에 식별된 제2 영역에 대한 회전 방향 에 회전 각도에 기초하여 식별할 수 있다. 그리고, 프로세서는, 제2 영역에 위치를 기준으로 복수의 영역 에서의 제6 영역의 위치를 고려하여, 로봇이 제2 영역을 응시한 후 제6 영역을 응시하기 위하여 회전 해야 하는 회전 방향과 회전 각도를 식별할 수 있다. 이때, 프로세서는 제6 영역에 대한 로봇의 회전 방향과 회전 각도를 시계 방향으로 120°인 것으로 식별할 수 있다. 그리고, 프로세서는 조정된 응시 순서에 따라 로봇이 새로운 타겟 영역 이전의 미응시 타겟 영역 (즉, 제2 영역)을 응시한 후 새로운 타겟 영역(즉, 제6 영역)에 대응되는 로봇의 회전 각도 및 회전 방향(즉, 시계 방향으로 120°)에 기초하여 로봇이 새로운 타겟 영역을 응시하도록 구동부를 제어하여 로봇을 회전시킬 수 있다. 반면에 프로세서는, 다음 타겟 영역이 새로운 타겟 영역이면, 새로운 타겟 영역의 위치에 기초하 여 새로운 타겟 영역에 대응되는 로봇의 회전 각도 및 회전 방향을 식별하고, 새로운 타겟 영역의 위치에 기초하여 미응시 타겟 영역에 대응되는 로봇의 회전 각도 및 회전 방향을 재 식별할 수 있다. 구체적으로, 다음 타겟 영역이 새로운 타겟 영역이며, 프로세서는 조정된 응시 순서에 추가된 새로운 타겟 영역의 이후 순서의 타겟 영역에 대한 회전 방향 및 회전 각도를 새로운 타겟 영역의 위치를 고려하여 재 식별할 수 있다. 도 15 및 도 16를 참조하면, 프로세서는 조정된 응시 순서 내에서 제6 영역의 다음 순서 갖는 타겟 영역 을 식별할 수 있다. 그리고, 프로세서는 2번째의 응시 순서를 갖는 제6 영역 이후 순서인 3번째의 응 시 순서를 갖는 타겟 영역으로 제1 영역을 식별할 수 있다. 따라서, 프로세서는 제6 영역의 위치를 식 별한 후 제6 영역에서 제1 영역으로 로봇을 회전시키기 위한 회전 방향과 회전 각도를 제1 영역의 위치와 제6 영역의 위치에 기초하여 식별할 수 있다. 구체적으로, 프로세서는 제1 영역의 위치를 응시 순서가 조정되기 전에 식별된 제1 영역에 대한 회전 방향 에 회전 각도에 기초하여 식별할 수 있다. 그리고, 프로세서는, 복수의 영역에서의 제6 영역의 위치를 기준으로 제1 영역에 위치를 고려하여, 로봇이 제6 영역을 응시한 후 제1 영역을 응시하기 위하여 회전해 야 하는 회전 방향과 회전 각도를 식별할 수 있다. 이때, 프로세서는 제1 영역에 대한 로봇의 회전 방향과 회전 각도를 반 시계 방향으로 150°인 것으로 식별할 수 있다. 그리고, 프로세서는 조정된 응시 순서에 따라 로봇이 새로운 타겟 영역(즉, 제6 영역)을 응시한 후 다음 순서의 타겟 영역(즉, 제1 영역)에 대응되는 로봇의 회전 각도 및 회전 방향(즉, 반 시계 방 향으로 150°)에 기초하여 로봇이 제1 영역을 응시하도록 구동부를 제어하여 로봇을 회전시킬 수 있 다. 도 17은 본 개시의 일 실시 예에 따른, 로봇의 세부적인 구성도이다. 도 17을 참조하면, 로봇은 카메라, 구동부, 디스플레이, 메모리, 센서, 스피커 , 마이크 및 통신 인터페이스를 포함한다. 도 12에 도시된 구성 중 도 2에 도시된 구성과 중복 되는 구성에 대해서는 자세한 설명을 생략하도록 한다. 디스플레이는 다양한 시각 정보를 표시할 수 있다. 일 예로, 디스플레이는 복수의 사용자가 요청한 다양한 정보를 표시할 수 있다. 이를 위해, 디스플레이는 자발광 소자를 포함하는 디스플레이 또는, 비자 발광 소자 및 백라이트를 포함하는 디스플레이로 구현될 수 있다. 예를 들어, LCD(Liquid Crystal Display), OLED(Organic Light Emitting Diodes) 디스플레이, LED(Light Emitting Diodes), 마이크로 LED(micro LED), Mini LED, PDP(Plasma Display Panel), QD(Quantum dot) 디스플레이, QLED(Quantum dot light-emitting diodes) 등과 같은 다양한 형태의 디스플레이로 구현될 수 있다. 디스플레이 내에는 a-si TFT, LTPS(low temperature poly silicon) TFT, OTFT(organic TFT) 등과 같은 형 태로 구현될 수 있는 구동 회로, 백라이트 유닛 등도 함께 포함될 수 있다. 한편, 디스플레이는 터치 패널 및 터치 센서와 결합된 터치 스크린, 플렉시블 디스플레이(flexible display), 롤러블 디스플레이(rollable display), 3차원 디스플레이(3D display), 복수의 디스플레이 모듈이 물리적으로 연결된 디스플레이 등으로 구현될 수 있다. 디스플레이가 터치 패널과 함께 터치스크린으로 구 현하는 경우, 디스플레이는 로봇과 사용자 사이의 정보를 출력하는 출력부로써 기능함과 동시에, 로 봇과 사용자 사이의 입력 인터페이스를 제공하는 입력부로써 기능할 수 있다. 특히, 프로세서는 디스플레 이를 통해 입력된 터치 입력에 기초하여 복수의 사용자에 대한 인터렉션 스코어를 산정할 수 있다. 메모리는 본 개시의 다양한 실시 예를 위해 필요한 데이터를 저장할 수 있다. 일 예로, 메모리에는 로봇이 위치한 주행 공간에 관한 맵 데이터가 저장될 수 있다. 메모리는 데이터 저장 용도에 따라 로 봇에 임베디드 된 메모리 형태로 구현되거나, 로봇에 탈부착이 가능한 메모리 형태로 구현될 수도 있 다. 예를 들어, 로봇의 구동을 위한 데이터의 경우 로봇에 임베디드 된 메모리에 저장되고, 로봇 의 확장 기능을 위한 데이터의 경우 로봇에 탈부착이 가능한 메모리에 저장될 수 있다. 한편, 로봇에 임베디드 된 메모리의 경우 휘발성 메모리(예: DRAM(dynamic RAM), SRAM(static RAM), 또는 SDRAM(synchronous dynamic RAM) 등), 비휘발성 메모리(non-volatile Memory)(예: OTPROM(one time programmable ROM), PROM(programmable ROM), EPROM(erasable and programmable ROM), EEPROM(electrically erasable and programmable ROM), mask ROM, flash ROM, 플래시 메모리(예: NAND flash 또는 NOR flash 등), 하드 드라이브, 또는 솔리드 스테이트 드라이브(solid state drive(SSD)) 중 적어도 하나로 구현될 수 있다. 또한, 로봇에 탈부착이 가능한 메모리의 경우 메모리 카드(예를 들어, CF(compact flash), SD(secure digital), Micro-SD(micro secure digital), Mini-SD(mini secure digital), xD(extreme digital), MMC(multi-media card) 등), USB 포트에 연결 가능한 외부 메모리(예를 들어, USB 메모리) 등과 같은 형태로 구현될 수 있다. 일 예에 따라, 메모리는 복수의 신경망(또는, 인공 지능) 모델에 관한 정보를 저장할 수 있다. 여기서, 신 경망 모델에 관한 정보를 저장한다는 것은 신경망 모델의 동작과 관련된 다양한 정보, 예를 들어 신경망 모델에 포함된 적어도 하나의 레이어에 대한 정보, 적어도 하나의 레이어 각각에서 이용되는 파라미터, 바이어스 등에 대한 정보 등을 저장한다는 것을 의미할 수 있다. 다만, 프로세서의 구현 형태에 따라 신경망 모델에 관한 정보가 프로세서의 내부 메모리에 저장될 수 있음은 물론이다. 예를 들어, 프로세서가 전용 하드웨어 로 구현되는 경우, 신경망 모델에 관한 정보는 프로세서 내부 메모리에 저장될 수도 있다. 한편, 로봇은 적어도 하나의 센서를 포함한다. 적어도 하나의 센서는 상술하여 설명한 로봇 주변의 객체를 감지하는 센서(예를 들어, 라이다 센서, ToF 센서 등)과 로봇의 자세를 감지하는 센 서(예를 들어, IMU 센서 등) 이외에도 제스처 센서, 자이로 센서, 기압 센서, 마그네틱 센서, 가속도 센서, 그 립 센서, 근접 센서, 컬러(color) 센서(예: RGB(red, green, blue) 센서), 생체 센서, 온/습도 센서, 조도 센 서 또는 UV(ultra violet) 센서 중의 적어도 하나를 더 포함할 수도 있다. 프로세서는 적어도 하나의 센서(16 0)를 통해 획득된 센싱 정보에 기초하여 로봇 주변의 복수의 사용자를 감지할 수도 있으며, 로봇 과 사용자 간의 거리를 식별할 수도 있다. 스피커는, 음향 신호를 로봇의 외부로 출력할 수 있다. 스피커는 멀티미디어 재생, 녹음 재생, 각종 알림 음, 음성 메시지 등을 출력할 수 있다. 로봇는 스피커와 같은 오디오 출력 장치를 포함할 수 있으나, 오디오 출력 단자와 같은 출력 장치를 포함할 수 있다. 특히, 스피커는 획득한 정보, 획득한 정보에 기초하여 가공·생산한 정보, 사용자 음성에 대한 응답 결과 또는 동작 결과 등을 음성 형태로 제공할 수 있다. 마이크는 사용자 음성을 수신할 수 있다. 특히, 마이크는 로봇 주변의 위치한 사용자의 음성을 수신할 수 있다. 그리고, 마이크는 입력된 사용자 음성을 전기적 신호로 변환하여 프로세서로 전송할 수 있다. 이에 따라, 프로세서는 복수의 사용자의 음성 명령 입력 횟수를 식별할 수 있다. 또는 프로세 서는 복수의 사용자 간의 대화 여부를 식별할 수도 있다. 통신 인터페이스는 다양한 타입의 컨텐츠를 송신하거나 또는 수신할 수 있다. 일 예로, 통신 인터페이스 는 AP 기반의 Wi-Fi(와이파이, Wireless LAN 네트워크), 블루투스(Bluetooth), 지그비(Zigbee), 유/무선 LAN(Local Area Network), WAN(Wide Area Network), 이더넷(Ethernet), IEEE 1394, HDMI(High-Definition Multimedia Interface), USB(Universal Serial Bus), MHL(Mobile High-Definition Link), AES/EBU(Audio Engineering Society/ European Broadcasting Union), 옵티컬(Optical), 코액셜(Coaxial 등과 같은 통신 방식 을 통해 외부 장치(예를 들어, 사용자 단말기), 외부 저장 매체(예를 들어, USB 메모리), 외부 서버(예를 들어 웹 하드) 등으로부터 스트리밍 또는 다운로드 방식으로 신호를 수신하거나 송신할 수 있다. 이 밖에도 로봇은 사용자 인터페이스(미도시)를 더 포함할 수 있다. 사용자 인터페이스(미도시)는 로봇 이 사용자와 인터렉션(Interaction)을 수행하는 데 이용되는 구성으로, 프로세서는 사용자 인터페이 스를 통해 로봇의 제어 정보 등 다양한 정보를 입력 받을 수 있으며, 입력 받은 정보를 바탕으로, 각각의 사용자에 대한 인터렉션 스코어를 산정할 수 있다. 한편, 사용자 인터페이스는 터치 센서, 모션 센서, 버튼, 조그(Jog) 다이얼, 스위치 중 적어도 하나를 포함할 수 있으나 이에 한정되는 것은 아니다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 로봇 에 설치 가능한 어플리케이션 형태로 구 현될 수 있다. 또는 상술한 본 개시의 다양한 실시 예들에 따른 방법들은 딥 러닝 기반의 학습된 신경망(또는 심층 학습된 신경망) 즉, 학습 네트워크 모델을 이용하여 수행될 수 있다. 또한, 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 로봇에 대한 소프트웨어 업그레이드, 또는 하드웨어 업그레이드 만으로도 구현될 수 있다. 또한, 상술한 본 개시의 다양한 실시 예들은 로봇에 구비된 임베디드 서버, 또는 로봇의 외부 서버를 통해 수행되는 것도 가능하다. 한편, 본 개시의 일시 예에 따르면, 이상에서 설명된 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media)에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실 시 예들에 따른 디스플레이 장치(예: 디스플레이 장치(A))를 포함할 수 있다. 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 프로세서의 제어 하에 다른 구성요소들을 이용하여 명령에 해당하는 기능을 수행 할 수 있다. 명령은 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장 매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저 장 매체가 신호(signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구 적 또는 임시적으로 저장됨을 구분하지 않는다. 또한, 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래 될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD- ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 또한, 상술한 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구 성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요 소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로 그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시 예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동 작들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2023-0082088", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2023-0082088", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 다수의 사용자를 응대하는 로봇에 대한 예시도이다. 도 2는 본 개시의 일 실시 예에 따른 로봇의 구성을 나타낸 블록도이다. 도 3은 본 개시의 일 실시 예에 따른 로봇을 제어하는 방법의 순서도이다. 도 4는 본 개시의 일 실시 예에 따른 동일한 사용자 그룹에 포함되는 복수의 사용자를 식별하는 방법을 나타낸 예시도이다. 도 5는 본 개시의 일 실시 예에 따른 카메라의 화각 내 복수의 영역 중 타겟 영역을 식별하는 것을 나타낸 예시 도이다. 도 6은 본 개시의 일 실시 예에 따른 인터렉션 스코어에 기초하여 응시 순서를 결정하는 방법을 나타낸 순서도 이다. 도 7a 및 7b는 본 개시의 일 실시 예에 따른 인터렉션 스코어에 기초하여 응시 순서를 결정하는 방법을 나타낸 예시도이다. 도 8은 본 개시의 일 실시 예에 따른 응시 순서에 따른 복수의 타겟 영역에 대한 회전 방향 및 회전 각도를 식 별하기 위한 제어 방법을 나타낸 순서도이다. 도 9 및 도 10은 본 개시의 일 실시 예에 따른 응시 순서에 따른 복수의 타겟 영역에 대한 회전 방향 및 회전 각도를 식별하는 것을 나타낸 예시도이다. 도 11은 본 개시의 일 실시 예에 따른 새로운 사용자가 인식되면, 새로운 사용자가 위치한 새로운 영역을 인식 하는 것을 나타낸 예시도이다. 도 12는 새로운 사용자가 인식되면, 복수의 타겟 영역에 대한 회전 각도 및 회전 방향을 재 식별하기 위한 제어 방법을 나타낸 순서도이다. 도 13은 본 개시의 일 실시 예에 따른 새로운 타겟 영역을 응시하기 위한 로봇의 회전 각도가 기 설정된 임계 각도 이상인 경우, 새로운 타겟 영역을 응시 순서에 포함시키지 않는 것을 나타낸 예시도이다. 도 14, 도 15 및 도 16은 본 개시의 일 실시 예에 따른 새로운 타겟 영역의 위치에 기초하여 로봇의 회전 방향 및 회전 각도를 재 식별하는 것을 나타낸 예시도이다. 도 17은 본 개시의 일 실시 예에 따른, 로봇의 세부적인 구성도이다."}
