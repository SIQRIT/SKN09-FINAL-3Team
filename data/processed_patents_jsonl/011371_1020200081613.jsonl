{"patent_id": "10-2020-0081613", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0085696", "출원번호": "10-2020-0081613", "발명의 명칭": "사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법", "출원인": "주식회사 제네시스랩", "발명자": "유대훈"}}
{"patent_id": "10-2020-0081613", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법에 있어서,사람의 외형을 표현하는 영상과 음성을 제공하는, 상기 영상은 제1 영상부와, 상기 제1 영상부를 바로 뒤따른제2 영상부와, 상기 제2 영상부를 바로 뒤따르는 제3 영상부를 포함하는, 단계;상기 제1 영상부에서 상기 사람의 감성 상태를 결정하기 위하여 상기 제1 영상부를 처리하며, 상기 제1 영상부에서는 상기 사람의 얼굴 및 적어도 하나의 손이 보여지며 상기 적어도 하나의 손이 상기 사람의 얼굴의 어떠한일부도 중첩되지 않는 것을 특징으로 하는 단계; 및상기 제2 영상부에서 상기 사람의 감성 상태를 결정하기 위하여 상기 제2 영상부를 처리하며, 상기 제2 영상부에서는 상기 사람의 얼굴과 적어도 하나의 손이 보여지며 상기 적어도 하나의 손이 상기 사람의 얼굴과 중첩되는 것을 특징으로 하는 단계;를 포함하고,상기 제1 영상부를 처리하는 단계는,상기 적어도 하나의 손이 상기 사람의 얼굴을 가리는지 여부를 결정하기 위하여 상기 제1 영상부의 적어도 하나의 프레임을 처리하는 단계와,상기 제1 영상부의 상기 적어도 하나의 프레임에서 상기 사람의 제1 얼굴 요소를 찾는 단계와,상기 제1 얼굴 요소가 위치된 상태에서, 상기 제1 영상부의 상기 적어도 하나의 프레임에서 보여지는 상기 제1얼굴 요소의 모양에 기초하여 상기 제1 영상부 제1 얼굴 특징 데이터를 획득하는 단계와,상기 제1 영상부에서 상기 사람의 목소리의 특성(characteristics)에 기초한 voice feature를 획득하기 위하여제1 영상부의 오디오 데이터를 처리하는 단계;와,상기 제1 영상부의 제1 얼굴 특징 데이터 및 voice feature를 포함하는 복수의 데이터에 기초하여 상기 제1 영상부에 대한 상기 사람의 감성 상태를 결정하는 단계;를 포함하고,상기 제2 영상부를 처리하는 단계는,상기 사람의 얼굴이 적어도 하나의 손에 의하여 가려지는 지 여부를 결정하기 위하여 상기 제2 영상부의 적어도하나의 프레임을 처리하는, 특히 상기 제2 영상부에서 상기 사람의 얼굴을 상기 적어도 하나의 손이 가리는 지여부가 결정되는, 단계와,상기 제2 영상부의 적어도 하나의 프레임에서 상기 사람의 상기 제1 얼굴 요소를 찾는 단계와,상기 제1 얼굴 요소가 위치된 상태에서, 상기 제2 영상부의 상기 적어도 하나의 프레임에서 보여지는 상기 제1얼굴 요소의 모양에 기초하여 상기 제1 영상부 제1 얼굴 특징 데이터를 획득하는 단계와,상기 제2 영상부에서 상기 사람의 목소리 특성에 기초한 음성 특징 데이터를 획득하기 위하여 제2 영상부의 오디오 데이터를 처리하는 단계와,상기 제2 영상부의 상기 제1 얼굴 특징 데이터와, 상기 제2 영상부의 상기 음성 특징 데이터와, 상기 사람의 얼굴 일부를 적어도 하나의 손이 가린 위치를 지시하는 부가 데이터를 포함하는 복수의 데이터에 기초하여 상기제2 영상부에 대한 상기 사람의 감성 상태를 결정하는 단계를 포함하는 사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법."}
{"patent_id": "10-2020-0081613", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,공개특허 10-2020-0085696-3-상기 복수의 데이터에 기초하여 상기 제2 영상부에 대한 상기 사람의 감성 상태를 결정하는 단계는,상기 제2 영상부에서 상기 사람의 얼굴의 일부가 적어도 하나의 손에 의하여 가려지는 경우, 상기 제2 영상부의상기 제1 얼굴 특징 데이터보다 상기 제2 영상부의 상기 음성 특징 데이터에 더 가중치를 두는 사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법."}
{"patent_id": "10-2020-0081613", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 복수의 데이터에 기초하여 상기 제2 영상부에 대한 상기 사람의 감성 상태를 결정하는 단계는,상기 제1 영상부에서는 상기 사람의 얼굴의 어느 부분도 적어도 하나의 손에 의하여 가려지지 않았으나, 상기제2 영상부에서 상기 사람의 얼굴의 일부가 적어도 하나의 손에 의하여 가려지는 경우, 상기 제1 영상부의 상기음성 특징 데이터보다 상기 제2 영상부의 상기 음성 특징 데이터에 더 가중치를 두는 사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법."}
{"patent_id": "10-2020-0081613", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 제1 영상부를 처리하는 단계는,상기 제1 영상부의 상기 적어도 하나의 프레임에서 상기 사람의 제2 얼굴 요소를 찾는 단계와,상기 제2 얼굴 요소가 위치된 상태에서, 상기 제1 영상부의 상기 적어도 하나의 프레임에서 보여지는 상기 제2얼굴 요소의 형상에 기초하여 상기 제1 영상부의 제2 얼굴 특징 데이터를 획득하는 단계를 더 포함하고,특히, 상기 제1 영상부의 상기 제1 얼굴 요소 특징 데이터와, 제1 영상부의 상기 제2 얼굴 요소 특징 데이터와,상기 제1 영상부의 음성 특징 데이터를 포함하는 복수의 데이터에 기초하여 상기 제1 영상부에 대한 상기 사람의 감성 상태를 결정하며,상기 제2 영상부를 처리하는 단계는,상기 제2 영상부의 상기 적어도 하나의 프레임에서 상기 사람의 제2 얼굴 요소를 찾는, 적어도 하나의 손에 의하여 상기 제2 얼굴 요소가 가려지는 지 여부를 결정하는, 단계와, 상기 제1 영상부의 상기 제2 face feature와 제2 얼굴 요소의 가려짐에 대한 기설정된 가중치에 기초하여 제2영상부의 제2 얼굴 특징 데이터를 획득하는 단계를 더 포함하고,특히, 상기 제2 영상부의 상기 제1 얼굴 요소 특징 데이터와, 상기 제2 영상부의 제2 얼굴 요소 특징 데이터와,상기 제2 영상부의 상기 음성 특징 데이터와, 상기 사람의 얼굴의 일부를 가지는 상기 적어도 하나의 손의 위치를 지시하는 부가 데이터에 기초하여 상기 제2 영상부에 대한 상기 사람의 감성 상태를 결정하는 것을 특징으로하는 사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법."}
{"patent_id": "10-2020-0081613", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서, 제3 영상부에 대한 상기 사람의 감성 상태를 결정하기 위하여 상기 3 영상부를 처리하는 단계를 더 포함하고,상기 사람의 얼굴의 어느 부분도 적어도 나의 손에 의하여 가려지지 않은 상태에서, 상기 제3 영상부 상에 상기사람의 얼굴 및 상기 적도 하나의 손이 보여지며,상기 제3 영상부를 처리하는 단계는,상기 사람의 얼굴을 상기 적어도 하나의 손이 가리는 지 여부를 결정하기 위하여 상기 제3 영상부의 적어도 하나의 프레임을 처리하는 단계;공개특허 10-2020-0085696-4-상기 제3 영상부의 적어도 하나의 프레임에서 상기 사람의 제1 얼굴 요소를 찾는 단계;제1 얼굴 요소가 위치된 상태에서, 상기 제3 영상부의 상기 적어도 하나의 프레임에서 보여지는 상기 제1 얼굴요소의 형상에 기초하여 상기 제3 영상부의 제1 얼굴 특징 데이터를 획득하는 단계;상기 제3 영상부에서 상기 사람의 목소리 특성에 기초하여 상기 제3 영상부의 음성 특징 데이터를 획득하기 위하여 상기 제1 영상부의 오디오 데이터를 처리하는 단계; 및상기 제3 영상부의 상기 제1 얼굴 특징 데이터 및 상기 음성 특징 데이터를 포함하는 복수의 데이터에 기초하여상기 제1 영상부의 상기 사람의 감성 상태를 결정하는 단계;를 포함하는 사람의 감성 상태를 결정하기 위하여영상을 처리하는 감성인식 방법."}
{"patent_id": "10-2020-0081613", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "컴퓨터에 의하여 실행될 때, 제1 항의 방법을 수행하는 명령어를 저장하는 컴퓨터 판독가능한 저장 매체."}
{"patent_id": "10-2020-0081613", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 기재의 인공지능을 이용한 멀티모달 감성인식 방법은, 사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법에 있어서, 사람의 외형을 표현하는 영상과 음성을 제공하는, 상기 영상은 제1 영상부와, 상기 제1 영상부를 바로 뒤따른 제2 영상부와, 상기 제2 영상부를 바로 뒤따르는 제3 영상부를 포함하는, 단계; 상기 제1 (뒷면에 계속)"}
{"patent_id": "10-2020-0081613", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명의 실시예들은 사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법에 관한 것이다."}
{"patent_id": "10-2020-0081613", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래의 기술에서는 가림(Occlusion)을 인식하여 오류로 처리한다. 손으로 입을 가린다는 것은 중요한 정보로 감 정 상태의 세기 정도를 알아낼 수 있다. 단순히 정적 이미지로는 가림(Occlusion) 문제로 인식 정보가 부족할 수가 있다. 또한, 얼굴 표정으로 감정을 인식할 때 대상자가 말을 하면 잘못된 감정 인식 결과를 도출한다. 표정인식을 통 한 감정인식은 입모양이 매우 중요한 정보지만 말을 할 때는 입모양이 수시로 변하기 때문에 놀람, 화, 웃음 등 과 같은 입모양이 나올 수 있어 잘못된 인식 결과를 초래한다. 이와 같이, 종래의 기술 중에는 얼굴 표정만으로 감정을 인식하는 경우 이를 해결하기 위한 대안은 거의 없으며, 멀티 모달인 경우에는 이러한 노이즈를 최소화하기 위해 얼굴 표정과 음성 정보를 혼용하여 오류를 최 소화하는 방법으로 접근하고 있다. 본 특허에서는 얼굴 혹은 입모양을 추적하여 현재 말하는 상태인지 판별한 후, 말하는 상태인 경우에는 입모양 정보를 최소화하고 음성 특징정보의 비중을 확대하는 방법으로 정확한 감정 인식 결과를 도출 할 수 있도록 한다."}
{"patent_id": "10-2020-0081613", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 실시예들은 손의 움직임 및 식별 정보, 입모양에 대한 정보, 음성 정보, 부분 표정 정보와 더불어 시 간적 정보를 이용하여 보다 정확한 감정인식을 수행하는 멀티 모달 감성인식 장치, 방법 및 저장매체를 제공하 고자 한다."}
{"patent_id": "10-2020-0081613", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시예의 일 측면에 따른 사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법은, 사 람의 외형을 표현하는 영상과 음성을 제공하는, 상기 영상은 제1 영상부와, 상기 제1 영상부를 바로 뒤따른 제2 영상부와, 상기 제2 영상부를 바로 뒤따르는 제3 영상부를 포함하는, 단계; 상기 제1 영상부에서 상기 사람의 감성 상태를 결정하기 위하여 상기 제1 영상부를 처리하며, 상기 제1 영상부에서는 상기 사람의 얼굴 및 적어도 하나의 손이 보여지며 상기 적어도 하나의 손이 상기 사람의 얼굴의 어떠한 일부도 중첩되지 않는 것을 특징으 로 하는 단계; 및 상기 제2 영상부에서 상기 사람의 감성 상태를 결정하기 위하여 상기 제2 영상부를 처리하며,상기 제2 영상부에서는 상기 사람의 얼굴과 적어도 하나의 손이 보여지며 상기 적어도 하나의 손이 상기 사람의 얼굴과 중첩되는 것을 특징으로 하는 단계;를 포함하고, 상기 제1 영상부를 처리하는 단계는, 상기 적어도 하나 의 손이 상기 사람의 얼굴을 가리는지 여부를 결정하기 위하여 상기 제1 영상부의 적어도 하나의 프레임을 처리 하는 단계와, 상기 제1 영상부의 상기 적어도 하나의 프레임에서 상기 사람의 제1 얼굴 요소를 찾는 단계와, 상 기 제1 얼굴 요소가 위치된 상태에서, 상기 제1 영상부의 상기 적어도 하나의 프레임에서 보여지는 상기 제1 얼 굴 요소의 모양에 기초하여 상기 제1 영상부 제1 얼굴 특징 데이터를 획득하는 단계와, 상기 제1 영상부에서 상 기 사람의 목소리의 특성(characteristics)에 기초한 voice feature를 획득하기 위하여 제1 영상부의 오디오 데 이터를 처리하는 단계;와, 상기 제1 영상부의 제1 얼굴 특징 데이터 및 voice feature를 포함하는 복수의 데이 터에 기초하여 상기 제1 영상부에 대한 상기 사람의 감성 상태를 결정하는 단계;를 포함하고, 상기 제2 영상부 를 처리하는 단계는, 상기 사람의 얼굴이 적어도 하나의 손에 의하여 가려지는 지 여부를 결정하기 위하여 상기 제2 영상부의 적어도 하나의 프레임을 처리하는, 특히 상기 제2 영상부에서 상기 사람의 얼굴을 상기 적어도 하 나의 손이 가리는 지 여부가 결정되는, 단계와, 상기 제2 영상부의 적어도 하나의 프레임에서 상기 사람의 상기 제1 얼굴 요소를 찾는 단계와, 상기 제1 얼굴 요소가 위치된 상태에서, 상기 제2 영상부의 상기 적어도 하나의 프레임에서 보여지는 상기 제1 얼굴 요소의 모양에 기초하여 상기 제1 영상부 제1 얼굴 특징 데이터를 획득하는 단계와, 상기 제2 영상부에서 상기 사람의 목소리 특성에 기초한 음성 특징 데이터를 획득하기 위하여 제2 영상 부의 오디오 데이터를 처리하는 단계와, 상기 제2 영상부의 상기 제1 얼굴 특징 데이터와, 상기 제2 영상부의 상기 음성 특징 데이터와, 상기 사람의 얼굴 일부를 적어도 하나의 손이 가린 위치를 지시하는 부가 데이터를 포함하는 복수의 데이터에 기초하여 상기 제2 영상부에 대한 상기 사람의 감성 상태를 결정하는 단계를 포함한 다. 또한, 상기 복수의 데이터에 기초하여 상기 제2 영상부에 대한 상기 사람의 감성 상태를 결정하는 단계는, 상기 제2 영상부에서 상기 사람의 얼굴의 일부가 적어도 하나의 손에 의하여 가려지는 경우, 상기 제2 영상부의 상기 제1 얼굴 특징 데이터보다 상기 제2 영상부의 상기 음성 특징 데이터에 더 가중치를 둘 수 있다. 또한, 상기 복수의 데이터에 기초하여 상기 제2 영상부에 대한 상기 사람의 감성 상태를 결정하는 단계는, 상기 제1 영상부에서는 상기 사람의 얼굴의 어느 부분도 적어도 하나의 손에 의하여 가려지지 않았으나, 상기 제2 영 상부에서 상기 사람의 얼굴의 일부가 적어도 하나의 손에 의하여 가려지는 경우, 상기 제1 영상부의 상기 음성 특징 데이터보다 상기 제2 영상부의 상기 음성 특징 데이터에 더 가중치를 둘 수 있다. 또한, 상기 제1 영상부를 처리하는 단계는, 상기 제1 영상부의 상기 적어도 하나의 프레임에서 상기 사람의 제2 얼굴 요소를 찾는 단계와, 상기 제2 얼굴 요소가 위치된 상태에서, 상기 제1 영상부의 상기 적어도 하나의 프레 임에서 보여지는 상기 제2 얼굴 요소의 형상에 기초하여 상기 제1 영상부의 제2 얼굴 특징 데이터를 획득하는 단계를 더 포함하고, 특히, 상기 제1 영상부의 상기 제1 얼굴 요소 특징 데이터와, 제1 영상부의 상기 제2 얼굴 요소 특징 데이터와, 상기 제1 영상부의 음성 특징 데이터를 포함하는 복수의 데이터에 기초하여 상기 제1 영상 부에 대한 상기 사람의 감성 상태를 결정하며, 상기 제2 영상부를 처리하는 단계는, 상기 제2 영상부의 상기 적 어도 하나의 프레임에서 상기 사람의 제2 얼굴 요소를 찾는, 적어도 하나의 손에 의하여 상기 제2 얼굴 요소가 가려지는 지 여부를 결정하는, 단계와, 상기 제1 영상부의 상기 제2 face feature와 제2 얼굴 요소의 가려짐에 대한 기설정된 가중치에 기초하여 제2 영상부의 제2 얼굴 특징 데이터를 획득하는 단계를 더 포함하고, 특히, 상기 제2 영상부의 상기 제1 얼굴 요소 특징 데이터와, 상기 제2 영상부의 제2 얼굴 요소 특징 데이터와, 상기 제2 영상부의 상기 음성 특징 데이터와, 상기 사람의 얼굴의 일부를 가지는 상기 적어도 하나의 손의 위치를 지 시하는 부가 데이터에 기초하여 상기 제2 영상부에 대한 상기 사람의 감성 상태를 결정할 수 있다. 또한, 제3 영상부에 대한 상기 사람의 감성 상태를 결정하기 위하여 상기 3 영상부를 처리하는 단계를 더 포함 하고, 상기 사람의 얼굴의 어느 부분도 적어도 나의 손에 의하여 가려지지 않은 상태에서, 상기 제3 영상부 상 에 상기 사람의 얼굴 및 상기 적도 하나의 손이 보여지며, 상기 제3 영상부를 처리하는 단계는, 상기 사람의 얼 굴을 상기 적어도 하나의 손이 가리는 지 여부를 결정하기 위하여 상기 제3 영상부의 적어도 하나의 프레임을 처리하는 단계; 상기 제3 영상부의 적어도 하나의 프레임에서 상기 사람의 제1 얼굴 요소를 찾는 단계; 제1 얼 굴 요소가 위치된 상태에서, 상기 제3 영상부의 상기 적어도 하나의 프레임에서 보여지는 상기 제1 얼굴 요소의 형상에 기초하여 상기 제3 영상부의 제1 얼굴 특징 데이터를 획득하는 단계; 상기 제3 영상부에서 상기 사람의 목소리 특성에 기초하여 상기 제3 영상부의 음성 특징 데이터를 획득하기 위하여 상기 제1 영상부의 오디오 데 이터를 처리하는 단계; 및 상기 제3 영상부의 상기 제1 얼굴 특징 데이터 및 상기 음성 특징 데이터를 포함하는 복수의 데이터에 기초하여 상기 제1 영상부의 상기 사람의 감성 상태를 결정하는 단계;를 포함할 수 있다. 본 발명의 실시예의 다른 측면에 따른 컴퓨터에 의하여 실행될 때, 기설정된 명령어를 저장하는 컴퓨터 판독가 능한 저장 매체는, 사람의 외형을 표현하는 영상과 음성을 제공하는, 상기 영상은 제1 영상부와, 상기 제1 영상 부를 바로 뒤따른 제2 영상부와, 상기 제2 영상부를 바로 뒤따르는 제3 영상부를 포함하는, 단계; 상기 제1 영 상부에서 상기 사람의 감성 상태를 결정하기 위하여 상기 제1 영상부를 처리하며, 상기 제1 영상부에서는 상기 사람의 얼굴 및 적어도 하나의 손이 보여지며 상기 적어도 하나의 손이 상기 사람의 얼굴의 어떠한 일부도 중첩 되지 않는 것을 특징으로 하는 단계; 및 상기 제2 영상부에서 상기 사람의 감성 상태를 결정하기 위하여 상기 제2 영상부를 처리하며, 상기 제2 영상부에서는 상기 사람의 얼굴과 적어도 하나의 손이 보여지며 상기 적어도 하나의 손이 상기 사람의 얼굴과 중첩되는 것을 특징으로 하는 단계;를 포함하고, 상기 제1 영상부를 처리하는 단계는, 상기 적어도 하나의 손이 상기 사람의 얼굴을 가리는지 여부를 결정하기 위하여 상기 제1 영상부의 적 어도 하나의 프레임을 처리하는 단계와, 상기 제1 영상부의 상기 적어도 하나의 프레임에서 상기 사람의 제1 얼 굴 요소를 찾는 단계와, 상기 제1 얼굴 요소가 위치된 상태에서, 상기 제1 영상부의 상기 적어도 하나의 프레임 에서 보여지는 상기 제1 얼굴 요소의 모양에 기초하여 상기 제1 영상부 제1 얼굴 특징 데이터를 획득하는 단계 와, 상기 제1 영상부에서 상기 사람의 목소리의 특성(characteristics)에 기초한 voice feature를 획득하기 위 하여 제1 영상부의 오디오 데이터를 처리하는 단계;와, 상기 제1 영상부의 제1 얼굴 특징 데이터 및 voice feature를 포함하는 복수의 데이터에 기초하여 상기 제1 영상부에 대한 상기 사람의 감성 상태를 결정하는 단계;를 포함하고, 상기 제2 영상부를 처리하는 단계는, 상기 사람의 얼굴이 적어도 하나의 손에 의하여 가려지 는 지 여부를 결정하기 위하여 상기 제2 영상부의 적어도 하나의 프레임을 처리하는, 특히 상기 제2 영상부에서 상기 사람의 얼굴을 상기 적어도 하나의 손이 가리는 지 여부가 결정되는, 단계와, 상기 제2 영상부의 적어도 하나의 프레임에서 상기 사람의 상기 제1 얼굴 요소를 찾는 단계와, 상기 제1 얼굴 요소가 위치된 상태에서, 상 기 제2 영상부의 상기 적어도 하나의 프레임에서 보여지는 상기 제1 얼굴 요소의 모양에 기초하여 상기 제1 영 상부 제1 얼굴 특징 데이터를 획득하는 단계와, 상기 제2 영상부에서 상기 사람의 목소리 특성에 기초한 음성 특징 데이터를 획득하기 위하여 제2 영상부의 오디오 데이터를 처리하는 단계와, 상기 제2 영상부의 상기 제1 얼굴 특징 데이터와, 상기 제2 영상부의 상기 음성 특징 데이터와, 상기 사람의 얼굴 일부를 적어도 하나의 손 이 가린 위치를 지시하는 부가 데이터를 포함하는 복수의 데이터에 기초하여 상기 제2 영상부에 대한 상기 사람 의 감성 상태를 결정하는 단계를 포함하는 사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법 을 수행하는 명령어를 저장한다."}
{"patent_id": "10-2020-0081613", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기한 바와 같은 본 발명의 실시예에 따르면, 사람의 감성 상태를 결정하기 위하여 영상을 처리하는 감성인식 방법은 대화하는 경우 및 손과 같은 객체에 의한 표정 가림을 하는 경우의 감정 상태를 정확하게 파악할 수 있 다."}
{"patent_id": "10-2020-0081613", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부한 도면을 참고로 하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시 할 수 있도록 상세히 설명한다.본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 동일 또는 유사한 구성요소에 대해서는 동일한 참조부호를 붙였다. 또한, 도면에서 나타난 각 구성의 크기 및 두께는 설명 의 편의를 위해 임의로 나타내었으므로, 본 발명이 반드시 도시된 바에 한정되지 않는다. 본 발명에 있어서 \"~상에\"라 함은 대상부재의 위 또는 아래에 위치함을 의미하는 것이며, 반드시 중력방향을 기 준으로 상부에 위치하는 것을 의미하는 것은 아니다. 또한, 명세서 전체에서, 어떤 부분이 어떤 구성요소를 \"포 함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소 를 더 포함할 수 있는 것을 의미한다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예들을 상세히 설명하기로 하며, 도면을 참조하여 설명할 때 동일 하거나 대응하는 구성 요소는 동일한 도면부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 본 발명은 대상자의 동영상과 음성 데이터를 기반으로 얼굴 표정, 말 상태, 손, 음성을 고려한 인공지능을 이용 하여 보다 정확한 감성인식 결과를 도출한다. 도 1은 본 발명의 실시예에 따른 멀티모달 감성 인식 장치의 구성을 개략적으로 도시한 도면이다. 도 1을 참조하면, 멀티 모달 감성 인식 장치는, 데이터 입력부, 데이터 전처리부, 예비 추론부 , 메인 추론부 및 출력부를 포함할 수 있다. 데이터 입력부는 사용자의 영상 데이터(DV) 및 음성 데이터(DS)를 입력 받을 수 있다. 데이터 입력부는 사용자의 감성 인식을 하기 위한 영상 데이터(DV)를 수신 받는 영상 입력부 및 사용 자의 음성 데이터(DS)를 수신 받는 음성 입력부를 포함할 수 있다. 또한, 데이터 전처리부는 음성 데이터(DS)로부터 음성 특징 데이터(DF2)를 생성하는 음성 전처리부, 영상 데이터(DV)로부터 하나 이상의 얼굴 특징 데이터(DF1)를 생성하는 영상 전처리부를 포함할 수 있다. 이 때, 얼굴 특징 데이터(DF1)는 이미지, 위치 정보, 크기 정보, 얼굴 비율 정보, 뎁스 정보(Depth Information) 중 적어도 하나 이상을 포함할 수 있고, 음성 특징 데이터(DF2)는 억양, 음높이 정보, 발성 강도, 발화속도 등 음성의 특징을 나타낼 수 있는 정보를 포함할 수 있다. 영상 전처리부는 영상 데이터(DV)로부터 사용자의 얼굴 특징 데이터(DF1)를 추출하기 위한 영상 전처리를 수행한다. 상기 영상 전처리는, 얼굴 전체 또는 부분 인식, 노이즈 제거, 사용자 얼굴 특징 및 이미지 추출 등 학습 모델 을 사용하기 위한 영상 데이터(DV)를 적절한 양태로 변환할 수 있다. 음성 전처리부는 음성 데이터(DS)로부터 사용자의 음성 특징 데이터(DF2)를 추출하기 위한 음성 전처리를 수행한다. 상기 음성 전처리는, 외부 소음 제거, 노이즈 제거, 사용자 음성 특징 추출 등 학습 모델을 사용하기 위한 적절 한 양태로 음성 데이터(DS)를 변환할 수 있다. 예비 추론부는, 영상 데이터(DV)에 기반하여, 시간적 순서에 따른 사용자의 상황 변화 여부에 관한 상황 판단 데이터(P)를 생성할 수 있다. 이 때, 상황 판단 데이터(P)는, 사용자가 대화 상태인지 여부에 대한 대화 판단 데이터(P1) 또는 영상 데이터 (DV)의 전체 영상 영역 중 일부인 추적 대상 영역(B)과 다른 인식 대상 영역(A)과의 중첩 여부에 대한 중첩 판 단 데이터(P2)를 포함할 수 있다. 상세하게는, 예비 추론부는 영상 데이터(DV)에 기반하여 추적 대상 영역(B)의 위치를 추론하기 위한 위치 추론 데이터(DM1)를 생성하고, 얼굴 특징 데이터(DF1) 및 위치 추론 데이터(DM1)에 기반하여, 추적 대상 영역 (B)과 인식 대상 영역(A)의 중첩 여부에 대한 중첩 판단 데이터(P2)를 생성할 수 있다. 또한, 예비 추론부는, 얼굴 특징 데이터(DF1)에 기반하여 사용자가 대화 상태 인지 여부를 판단하는 대화 판단 데이터(P1)를 생성할 수 있다. 메인 추론부는, 음성 특징 데이터(DF2) 또는 얼굴 특징 데이터(DF1)에 기반하여 적어도 하나의 서브 특징맵 (FM)을 생성하고, 서브 특징맵(FM) 및 상황 판단 데이터(P)에 기반하여 사용자의 감성 상태를 추론할 수 있다. 상기 감성 상태는 행복, 화, 두려움, 혐오, 슬픔, 놀람 등의 사용자의 감정 상태 정보를 포함할 수 있다. 출력부는 메인 추론부에서 추론된 감성상태의 결과를 출력할 수 있다. 이 때, 출력부는 시그모이드 함수(Sigmoid Function), 단계 함수(Step Function), 소프트맥스 함수 (Softmax), ReLU(Rectified Linear Unit)등 활성화 함수를 이용하여 다양한 형태로 출력할 수 있다. 도 2는 도 1의 멀티모달 감성 인식 장치 중 데이터 전처리부의 구성을 개략적으로 도시한 도면이다. 도 2를 참조하면, 데이터 전처리부는 영상 전처리부 및 음성 전처리부를 포함할 수 있다. 영상 전처리부는 얼굴 검출기, 이미지 전처리 모듈, 랜드 마크 검출모듈, 위치 조정모듈 및 얼굴 요소 추출 모듈을 포함 할 수 있다. 얼굴 검출기는 영상 데이터(DV)의 전체 영역에서 사용자의 얼굴에 대응되는 영역인 인식 대상 영역(A)을 검출할 수 있다. 이미지 전처리 모듈은 인식 대상 영역(A)을 보정할 수 있다. 상세하게는, 이미지 전처리 모듈은 이미지의 밝기, 블러(Blur)의 보정, 및 영상 데이터(DV)의 노이즈 제거 를 수행할 수 있다. 랜드마크 검출모듈은 인식 대상 영역(A)의 얼굴 요소 위치 정보(AL)를 추출할 수 있다. 상세하게는, 인식 대상 영역(A) 중 얼굴, 눈, 입, 코, 이마 등 얼굴 중요 요소의 위치 정보를 파악하여 얼굴 인 식이 가능하게 수행할 수 있다. 위치 조정모듈은 인식 대상 영역(A)의 얼굴 요소 위치 정보(AL)에 기반하여 위치를 조정할 수 있다. 상세하게는, 위치 조정모듈은 랜드마크 검출모듈로부터 추출된 얼굴 요소 위치 정보(AL)를 기준으로 수평 또는 수직에 맞춰 이미지를 정렬할 수 있다. 얼굴 요소 추출 모듈은 인식 대상 영역(A) 내에 위치하며 인식 대상 영역(A)보다 작은 서브 인식 대상 영 역(AA)을 설정하고, 서브 인식 대상 영역(AA)의 얼굴 특징 데이터(DF1)를 생성할 수 있다. 서브 인식 대상 영역(AA)은 얼굴, 눈, 입, 코, 이마 등 적어도 하나 이상의 얼굴 요소가 판별된 복수의 영역 또 는 하나의 영역일 수 있다. 예를 들어, 인식 대상 영역(A) 중 얼굴 요소 위치 정보(AL)가 추출된 눈, 코, 입이 추출될 경우, 얼굴 요소 추 출 모듈은 서브 인식 대상 영역(AA)인 눈 인식 영역(A1), 코 인식 영역(A2), 입 인식 영역(A3)을 설정 및 상기 설정된 서브 인식 대상 영역(AA)에 대해 적어도 하나 이상의 얼굴 특징 데이터(DF1)를 생성할 수 있다. 또한, 얼굴 요소 추출 모듈은 서브 인식 대상 영역(AA)이 설정되지 않을 경우, 인식 대상 영역(A)을 기반 으로 얼굴 특징 데이터(DF1)를 생성할 수 있다. 음성 전처리부는 음성 보정 모듈, 음성 특징 데이터 추출 모듈을 포함할 수 있다. 음성 보정 모듈은 음성 데이터(DS)를 보정할 수 있다. 상세하게는, 음성 보정 모듈은 음성 데이터(DS)에 포함된 다양한 노이즈 및 외부 소음 제거, 음량 조절, 주파수 보정 등 다양한 보정 방법을 수행하여, 보정된 음성 데이터를 생성할 수 있다. 음성 특징 데이터 추출 모듈은 음성 보정 모듈을 거친 음성 데이터(DS)의 특징을 추출하여, 음성 특 징 데이터(DF2)를 생성할 수 있다. 상세하게는, 음성 특징 데이터 추출 모듈은 MFCC(Mel-frequency Cepstral Coefficients), eGeMAPS(Geneva Minimalistic Acoustic Parameter Set), Logbank 등과 같은 음성 데이터, 주파수 및 스펙트럼 분석 모듈 중 하나 이상의 모듈을 통하여 사용자의 음성 특징 데이터(DF2)를 생성 할 수 있다. 이 때, 음성 특징 데이터 추출 모듈은 상기 보정된 음성 데이터를 사용하거나, 음성 데이터(DS)를 사용할 수도 있다. 도 3은 도 1의 멀티모달 감성 인식 장치 중 예비 추론부의 구성을 개략적으로 도시한 도면이다. 도 3을 참조하면, 예비 추론부는 손 검출 추론모듈, 대화 상태 추론모듈 및 얼굴 겹침 검사모듈 을 포함할 수 있다. 대화 상태 추론모듈은, 제1 학습 모델(LM1)을 이용하고, 얼굴 특징 데이터(DF1)에 기반하여 대화 판단 데이 터(P1)를 생성할 수 있다. 상세하게는, 대화 상태 추론모듈은 사용자의 얼굴 특징 데이터(DF1)의 전체 또는 부분을 사용하여, 사용자 가 대화 상태인지를 판별할 수 있는 제1 학습 모델(LM1)을 이용하여, 대화 판단 여부인 대화 판단 데이터(P1)를 생성할 수 있다. 얼굴 특징 데이터(DF1)는, 인식 대상 영역(A) 중 사용자의 입에 대응되는 부분에 대한 영상 데이터(DV)인 입 영상 데이터(DV2)를 포함하고, 제1 학습 모델(LM1)을 이용하여, 입 영상 데이터(DV2)로부터 사용자의 대화 상태 여부에 대한 대화 판단 데이터(P1)를 생성할 수 있다. 제1 학습 모델(LM1)은 LSTM(Long Short-Term Memory), RNNs(Recurrent Neural Network), DNN(Deep Neural Networks), CNN(Convolutional Neural Network) 등 시간적 특징 또는 공간적 특징을 추론 할 수 있는 인공지능 모델, 머신 러닝, 딥 러닝 방법 중 적어도 하나 이상의 방법일 수 있다. 손 검출 추론모듈은, 영상 데이터(DV)에서 추적 대상 영역(B)에 대한 손 영상 데이터(DV1)를 검출하고, 제 2 학습 모델(LM2)을 이용하여 손 영상 데이터(DV1)에 기반한 위치 추론 데이터(DM1)를 생성할 수 있다. 이 때, 제2 학습 모델(LM2)은 LSTM(Long Short-Term Memory), RNNs(Recurrent Neural Network), DNN(Deep Neural Networks), CNN(Convolutional Neural Network) 등 시간적 특징 또는 공간적 특징을 추론 할 수 있는 인공지능 모델, 머신 러닝, 딥 러닝 방법 중 적어도 하나 이상의 방법이며, 이를 통해 손에 대한 위치 추론 데 이터(DM1)를 생성할 수 있다. 또한, 손 검출 추론모듈은, 위치 추론 데이터(DM1)에 대한 위치 추론 특징맵(FM1)을 생성하고, 서브 특징맵 (FM), 상황 판단 데이터(P), 및 위치 추론 특징맵(FM1)에 기반하여 사용자의 감성 상태를 추론할 수 있다. 이 때, 위치 추론 특징맵(FM1)은 손에 대한 특징 정보, 즉, 손에 대한 제스처 및 손에 대한 위치에 대한 정보 등 손의 움직임의 의미 있는 정보를 포함할 수 있다. 얼굴 겹침 검사모듈은, 얼굴 특징 데이터(DF1) 및 위치 추론 데이터(DM1)에 기반하여 인식 대상 영역(A)과 추적 대상 영역(B)의 중첩 여부를 판단하고, 중첩 여부 판단 결과에 따라 중첩 판단 데이터(P2)를 생성 할 수 있다. 상세하게는, 중첩 판단 데이터(P2)는 인식 대상 영역(A)과 추적 대상 영역(B)의 중첩 여부를 판단하여, 인식 대 상 영역(A)의 해당하는 얼굴 특징 데이터(DF1)와 음성 특징 데이터(DF2)의 중요도 및 사용 여부를 결정하는 하나 이상의 파라미터를 생성할 수 있다. 도 4는 도 1의 멀티모달 감성 인식 장치 중 메인 추론부의 구성을 개략적으로 도시한 도면이다. 도 4를 참조하면, 메인 추론부는, 복수의 서브 특징맵 생성부(410; 411, 412, 413, 414), 멀티 모달 특징 맵 생성부 및 감성 인식 추론부를 포함할 수 있다. 복수의 서브 특징맵 생성부(410; 411, 412, 413, 414)는 제3 학습 모델(LM3)을 이용하여 음성 특징 데이터 (DF2) 및 얼굴 특징 데이터(DF1)에 기반하여 음성 특징 데이터(DF2) 및 얼굴 특징 데이터(DF1)에 대한 복수의 서브 특징맵(FM)을 생성할 수 있다. 상세하게는, 제3 학습 모델(LM3)은 DNN(Deep Neural Networks), CNN(Convolutional Neural Network) 등을 적어 도 하나 이상의 공간적 특징을 추론할 수 있는 인공지능 모델, 머신 러닝, 딥 러닝 방법 중 적어도 하나 이상의 방법일 수 있고, 제3 학습 모델(LM3)을 이용하여, 음성 특징 데이터(DF2) 및 얼굴 특징 데이터(DF1)의 특징이 함 축된 복수의 서브 특징맵(FM)을 생성할 수 있다. 멀티 모달 특징맵 생성부는 상황 판단 데이터(P)를 참조하여, 복수의 서브 특징맵(FM)으로부터 멀티 모달 특징맵(M)을 생성할 수 있다. 상황 판단 데이터(P)는, 사용자의 상황에 따라 기설정된 상황 판단값(PV)을 가지며, 멀티 모달 특징맵 생성부 는, 복수의 서브 특징맵(FM) 중 적어도 하나의 상황 판단값(PV)을 적용하여 멀티 모달 특징맵(M)을 생성할 수 있다. 상세하게는, 상황 판단값(PV)은 각각의 서브 특징맵(FM)이 가지는 중요도 및 사용여부를 나타내는 파라미터일 수 있다. 상황 판단 데이터(P)와 서브 특징맵(FM)과의 연산을 통하여 상황 판단 데이터(P)의 상황 판단값(PV)이 적용된 서브 특징맵(FM)을 생성하고, 복수의 서브 특징맵(FM)을 통합하여, 멀티 모달 특징맵(M)을 생성할 수 있다. 예를 들면, 사용자의 눈이 가려졌을 경우, 눈에 대한 상황 판단값을 0으로 출력하여, 상기 눈에 대한 상황 판단 값과 눈에 대한 서브 특징맵(FM)의 곱연산을 통해 0을 출력하게 되어, 메인 추론부가 상기 눈에 대한 서브 특징맵을 제외한 다른 서브 특징맵을 기준으로 멀티 모달 특징맵(M)을 생성할 수 있다. 또한, 손 검출 추론모듈로부터 위치 추론 특징맵(FM1)을 생성하고, 서브 특징맵(FM), 상황 판단 데이터(P) 및 위치 추론 특징맵(FM1)에 기반하여 사용자의 감성 상태를 추론하는 멀티 모달 특징맵(M)을 생성할 수 있다. 멀티 모달 특징맵(M)은 Concat, Merge 및 딥 네트워크(Deep Network) 등을 사용하여 서브 특징맵(FM) 및 위치 추론 특징맵(FM1) 적어도 하나 이상을 병합하여 생성될 수 있다. 감성 인식 추론부는 제4 학습 모델(LM4)을 사용하여, 멀티 모달 특징맵(M)에 기반하여 감성상태를 추론할 수 있다. 이 때, 제4 학습 모델(LM4)은, LSTM(Long Short-Term Memory), RNNs(Recurrent Neural Network), GRU(Gated Recurrent Unit) 등 순환 신경망과 같은 시간적 학습 모델일 수 있고, 시간적 특징과 공간적 특징을 추론 또는 분석할 수 있는 인공지능 모델, 머신 러닝, 딥 러닝 방법 중 적어도 하나 이상의 방법일 수 있다. 도 5는 도 1의 멀티모달 감성 인식 장치에 의한 멀티모달 감성 인식 방법을 보여주는 순서도이다. 도 5를 참조하면, 사용자의 영상 데이터(DV) 및 음성 데이터(DS)를 입력 받는 데이터 입력 단계(S100)를 수행된 다. 그 다음, 음성 데이터(DS)로부터 음성 특징 데이터(DF2)를 생성하는 음성 전처리 단계, 영상 데이터(DV)로부터 하나 이상의 얼굴 특징 데이터(DF1)를 생성하는 영상 전처리단계를 포함하는 데이터 전처리 단계(S200)가 수행 될 수 있다. 이 때, 데이터 전처리 단계(S200)는 학습 모델을 사용하기 위한 얼굴 특징 데이터(DF1)와 음성 특징 데이터(DF 2)를 생성할 수 있다. 상기 학습 모델은 인공지능, 머신 러닝 및 딥 러닝 방법이 될 수 있다. 그 다음, 영상 데이터(DV)에 기반하여, 시간적 순서에 따른 사용자의 상황 변화 여부에 관한 상황 판단 데이터 (P)를 생성하는 예비 추론 단계(S300)가 수행될 수 있다. 이 때, 상기 시간적 순서는 대화상태의 여부가 될 수 있고, 신체부분의 움직임에 대한 특징을 파악하기 위한 데 이터일 수 있다. 또한, 상황 판단 데이터(P)는 영상 데이터(DV)로부터 겹칩 여부와 대화 상태의 여부를 판별하여, 하나 이상의 얼굴 특징 데이터(DF1) 또는 음성 특징 데이터(DF2)의 중요도 또는 사용 여부를 나타내는 파라미터를 포함할 수 있다. 또한, 데이터 전처리 단계(S200)에서 생성된 하나 이상의 얼굴 특징 데이터(DF1) 이외의 사용자의 신체 부분에 대한 특징 정보를 추출하여 생성할 수 있다. 그 다음, 음성 특징 데이터(DF2) 또는 얼굴 특징 데이터(DF1)에 기반하여 적어도 하나의 서브 특징맵(FM)을 생성 하고, 서브 특징맵(FM) 및 상황 판단 데이터(P)에 기반하여 사용자의 감성 상태를 추론하는 메인 추론 단계 (S400)가 수행될 수 있다. 이 때, 사용자로부터 추출된 특징 정보를 포함한 서브 특징맵(FM)과 특징 정보의 중요도 또는 사용여부에 대한 파라미터를 포함한 상황 판단 데이터(P)를 연산하여, 서브 특징맵(FM)에 중요도 또는 사용여부에 대한 정보를 포함하여, 사용자의 감성 상태를 추론할 수 있다. 그 다음, 메인 추론 단계(S400)에서의 감성 상태의 추론 결과를 출력하는 결과 도출 단계(S500)가 수행된다. 도 6은 도 5의 멀티모달 감성 인식 방법 중 데이터 전처리 단계를 상세하게 보여주는 순서도이다. 도 6을 참조하면, 데이터 전처리 단계(S200)는 영상 전처리 단계(S210)와 음성 전처리 단계(S220)를 포함한다. 영상 전처리 단계(S210)는, 영상 데이터(DV)의 전체 영역에서 인식 대상 영상 영역, 인식 대상 영역(A)은 사용 자의 얼굴에 대응되는 영역인,을 검출하는 얼굴 검출 단계가 수행된다. 그 다음, 인식 대상 영역(A)을 보정하는 이미지 전처리 단계가 수행된다. 상세하게는, 상기 이미지 전처리 단계에서 이미지의 밝기, 블러(Blur)의 보정, 및 영상 데이터(DV)의 노이즈 제 거가 수행될 수 있다 그 다음, 인식 대상 영역(A)의 얼굴 요소 위치 정보(AL)를 추출하는 랜드마크 검출 단계가 수행된다. 상세하게는, 인식 대상 영역(A) 중 얼굴, 눈, 코, 입, 이마 등 얼굴 중요 요소의 위치 정보를 파악하여 얼굴 인 식이 가능하게 수행될 수 있다. 그 다음, 인식 대상 영역(A)의 얼굴 요소 위치 정보(AL)에 기반하여 위치를 조정하는 위치 조정 단계가 수행될 수 있다. 상세하게는, 랜드마크 검출모듈로부터 추출된 얼굴 요소 위치 정보(AL)를 기준으로 수평 또는 수직에 맞춰 이미지가 정렬될 수 있다. 그 다음, 인식 대상 영역(A)에서 얼굴 요소 위치 정보(AL)에 기반하여 인식 대상 영역(A) 내에 위치하며 인식 대상 영역(A)보다 작은 서브 인식 대상 영역(AA)을 설정하고, 서브 인식 대상 영역(AA)의 얼굴 특징 데이터 (DF1)를 생성하는 얼굴 요소 추출 단계가 수행될 수 있다. 이 때, 서브 인식 대상 영역(AA)은 얼굴전체, 눈, 입, 코, 이마 등 적어도 하나 이상의 얼굴 요소가 판별된 복 수의 영역 또는 하나의 영역일 수 있다. 예를 들어, 인식 대상 영역(A) 중 얼굴 요소 위치 정보(AL)가 추출된 눈, 코, 입이 추출될 경우, 얼굴 요소 추 출 모듈은 서브 인식 대상 영역(AA)인 눈 인식 영역(A1), 코 인식 영역(A2), 입 인식 영역(A3)을 설정 및 상기 설정된 서브 인식 대상 영역(AA)에 대해 적어도 하나 이상의 얼굴 특징 데이터(DF1)를 생성할 수 있다. 또한, 상기 얼굴 요소 추출 단계는 서브 인식 대상 영역(AA)이 설정되지 않을 경우, 인식 대상 영역(A)을 기반 으로 얼굴 특징 데이터(DF1)를 생성할 수 있다. 음성 전처리 단계(S220)는 음성 보정 단계 및 음성 특징 데이터 추출 단계를 포함한다. 먼저, 음성 데이터(DS)를 보정하는 상기 음성 보정 단계가 수행된다. 상세하게는, 상기 음성 보정 단계에서 음성 데이터(DS)에 포함된 다양한 노이즈 및 외부 소음 제거, 음량 조절, 주파수 보정 등 다양한 보정 방법을 수행하여, 보정된 음성 데이터를 생성될 수 있다. 상기 음성 보정 단계를 거친 음성 데이터(DS)의 특징을 추출하여, 음성 특징 데이터(DF2)를 생성하는 상기 음성 특징 데이터 추출 단계가 수행된다. 상세하게는, MFCC(Mel-frequency cepstral coefficients), eGeMAPS(Geneva Minimalistic Acoustic Parameter Set), Logbank 등과 같은 음성 데이터, 주파수 및 스펙트럼 분석 모듈 중 하나 이상의 모듈을 통하여 사용자의 음성 특징 데이터(DF2)를 생성 될 수 있다. 이 때, 상기 음성 특징 데이터 추출 단계는 상기 보정된 음성 데이터를 사용하거나, 상기 음성 보정 단계가 수 행되지 않고 음성 데이터(DS)하여 음성 특징 데이터(DF2)를 생성할 수도 있다. 또한, 이는 예시적인 것으로서 적어도 일부의 단계들은 전후의 단계들과 동시에 수행되거나 또는 순서를 바꾸어 수행될 수도 있다. 도 7은 도 5의 멀티모달 감성 인식 방법 중 예비 추론 단계를 상세하게 보여주는 순서도이다. 제1 학습 모델(LM1)을 이용하고, 얼굴 특징 데이터(DF1)에 기반하여 대화 판단 데이터(P1)를 생성하는 대화 상태 추론 단계(S310)가 수행될 수 있다. 대화 상태 추론 단계(S310)에서, 제1 학습 모델(LM1)을 이용하여 이전 상황에서의 대화 여부와 얼굴 특징 데이 터(DF1)로부터 얼굴 요소의 특징 및 움직임을 감지하여, 대화 상태 여부를 감지될 수 있다. 상세하게는, 사용자의 얼굴 특징 데이터(DF1)의 전체 또는 부분을 사용하여, 사용자가 대화 중인지를 제1 학습 모델(LM1)을 이용하여, 대화 판단 여부인 대화 판단 데이터(P1)가 생성될 수 있다. 이 때, 얼굴 특징 데이터(DF1)는, 인식 대상 영역(A) 중 사용자의 입에 대응되는 부분에 대한 입 영상 데이터 (DV2)를 포함할 수 있다. 또한, 제1 학습 모델(LM1)을 이용하여, 입 영상 데이터(DV2)로부터 사용자의 대화 상태 여부에 대한 대화 판단 데이터(P1)를 생성할 수 있다. 그 다음, 영상 데이터(DV)에서 추적 대상 영역(B)에 대한 손 영상 데이터(DV1)를 검출하고, 제2 학습 모델(LM2) 을 이용하여 손 영상 데이터(DV1)에 기반한 위치 추론 데이터(DM1)를 생성하는 손 검출 추론 단계(S320)가 수행 된다. 이 때, 제2 학습 모델(LM2)을 사용하여 손에 대한 위치에 대한 이전 상황과의 시간적 추론이 가능할 수 있다. 예를 들어, 일시적으로 손이 얼굴에 겹쳤는지 여부를 판별할 수 있다. 또한, 손 검출 추론 단계(S320)는, 위치 추론 데이터(DM1)에 대한 위치 추론 특징맵(FM1)을 생성하고, 서브 특징 맵(FM), 상황 판단 데이터(P), 및 위치 추론 특징맵(FM1)에 기반하여 사용자의 감성 상태를 추론할 수 있다. 상세하게는, 위치 추론 특징맵(FM1)은 손에 대한 제스처를 파악할 수 있는 특징 및 손에 대한 위치에 대한 정보 등 손의 움직임의 의미 있는 정보를 포함할 수 있다. 그 다음, 얼굴 특징 데이터(DF1) 및 위치 추론 데이터(DM1)에 기반하여 인식 대상 영역(A)과 추적 대상 영역(B) 의 중첩 여부를 판단하고, 중첩 여부 판단 결과에 따라 중첩 판단 데이터(P2)를 생성하는 얼굴 겹침 검사 단계 (S330)가 수행된다. 상세하게는, 중첩 판단 데이터(P2)는 인식 대상 영역(A)과 추적 대상 영역(B)의 중첩 여부를 판단하여, 인식 대 상 영역(A)의 해당하는 얼굴 특징 데이터(DF1)와 음성 특징 데이터(DF2)의 중요도 및 사용 여부를 결정하는 하나 이상의 파라미터를 포함할 수 있다. 도 8은 도 5의 멀티모달 감성 인식 방법 중 메인 추론 단계를 상세하게 보여주는 순서도이다. 도 8을 참조하면, 메인 추론 단계(S400)는, 복수의 서브 특징맵 생성 단계(S410), 멀티 모달 특징맵 생성 단계 (S420) 및 감성 인식 추론 단계(S430)를 포함한다.먼저, 제3 학습 모델(LM3)을 이용하여 음성 특징 데이터(DF2) 및 얼굴 특징 데이터(DF1)에 기반하여 음성 특징 데이터(DF2) 및 얼굴 특징 데이터(DF1)에 대한 복수의 서브 특징맵(FM)을 생성하는 복수의 서브 특징맵 생성 단 계(S410)가 수행된다. 그 다음, 제3 학습 모델(LM3)은 상황 판단 데이터(P)를 참조하여, 복수의 서브 특징맵(FM)으로부터 멀티 모달 특징맵(M)을 생성하는 멀티 모달 특징맵 생성 단계(S420)가 수행된다. 이 때, 상황 판단 데이터(P)는, 사용자의 상황에 따라 기설정된 상황 판단값(PV)을 가지며, 멀티 모달 특징맵 생성 단계(S420)는, 복수의 서브 특징맵(FM) 중 적어도 하나에 상황 판단값(PV)을 적용하여 멀티 모달 특징맵 (M)을 포함할 수 있다. 또한, 멀티 모달 특징맵 생성 단계(S420)에서, 손 검출 추론모듈로부터 위치 추론 특징맵(FM1)을 생성하고, 서브 특징맵(FM), 상황 판단 데이터(P) 및 위치 추론 특징맵(FM1)에 기반하여 사용자의 감성 상태를 추론하는 멀티 모달 특징맵(M)이 생성될 수 있다. 그 다음, 제4 학습 모델(LM4)을 사용하여, 멀티 모달 특징맵(M)에 기반하여 감성상태를 추론하는 감성 인식 추 론 단계(S430)가 수행된다. 이 때, 제4 학습 모델(LM4)은, LSTM(Long Short-Term Memory), RNNs(Recurrent Neural Network), GRU(Gated Recurrent Unit) 등 순환 신경망과 같은 시간적 학습 모델일 수 있고, 시간적 특징과 공간적 특징을 추론 또는 분석할 수 있는 인공지능 모델, 머신 러닝, 딥 러닝 방법 중 적어도 하나 이상의 방법일 수 있다. 도 9는 도 1의 멀티모달 감성 인식 장치에서 상황 변화 여부에 따른 얼굴 인식 과정을 보여주는 예시적인 도면 이다. 도 9를 참조하면, ((A)단계) 사용자가 손을 얼굴에 대고 있으며, 손이 입과 코를 가리고 있지는 않는 상황을 나 타내고 있다. 영상 입력부를 통해 사용자의 영상 데이터(DV)가 입력되고, 음성 입력부를 통해 사용자의 음성 데이 터(DS)가 입력된다. 이 후, 영상 전처리부는 영상 전처리가 된 얼굴 특징 데이터(DF1)를 생성하고, 또한, 음성 전처리부 를 통해 음성 전처리가 된 음성 특징 데이터(DF2)를 생성하고, 영상 전처리부는 인식 가능한 사용자의 눈, 코, 입의 얼굴 요소 위치 정보(AL)를 기반으로 눈 인식 영역(A1), 코 인식 영역(A2), 입 인식 영역(A3)을 포함하 는 인식 대상 영역(A)이 설정되고, 인식 대상 영역(A)을 예비 추론부로 송신한다. 이 후, 예비 추론부는 영상 데이터(DV)로부터 검출된 추적 대상 영역(B1)에 대한 손 영상 데이터(DV1)를 생 성한다. 이 때, 예비 추론부는 손 영상 데이터(DV1)를 통해 손의 움직임을 파악하는 위치 추론 데이터(DM1)를 생성 되고, 위치 추론 데이터(DM1)에 기반한 추적 대상 영역(B1)과 인식 대상 영역(A)의 중첩됨 여부 판단을 기반으로 중첩 판단 데이터(P2)가 생성된다. 여기서, 중첩 판단 데이터(P2)는 눈 인식 영역(A1), 코 인식 영역(A2), 입 인식 영역(A3)을 사용을 나타내는 파 라미터를 포함할 수 있다. 또한, 대화 상태 추론모듈은 입 영상 데이터(DV2)에 기반한 입 인식 영역(A3)을 통하여 대화 상태 여부를 판단하여 대화 판단 데이터(P1)를 생성한다. 이 후, 서브 특징맵 생성부는 눈, 코, 입에 해당되는 얼굴 특징 데이터(DF1)를 제3 학습 모델(LM3)을 사용 하여 복수의 서브 특징맵(FM)을 생성한다. 이 후, 멀티 모달 특징맵 생성부는 복수의 서브 특징맵(FM)과 손에 해당되는 위치 추론 특징맵(FM1)을 통 합하여 멀티 모달 특징맵(M)을 생성한다.이 후, 제4 학습 모델(LM4)을 통해 이전의 사용자의 행동을 고려하여 감성인식을 추론하고, 이를 감성인식 결과 로 나타낼 수 있다. ((B)단계) B단계는, A단계의 연속적인 동작을 나타내고 있다. 예를 들어, B단계는 30FPS 속도로 A단계에 이어 연속적으로 촬영된 영상으로 가정 할 수 있다. A단계와 마찬가지로, 영상 입력부를 통해 사용자의 영상 데이터(DV)가 입력되고, 음성 입력부를 통해 사용자의 음성 데이터(DS)가 입력된다. 이 후, 음성 전처리부를 통해 음성 전처리가 된 음성 특징 데이터(DF2)를 생성하고, 영상 전처리부는 얼굴 특징 데이터(DF1) 및 얼굴 요소 위치 정보(AL)를 생성하고, 얼굴 요소 위치 정보(AL)를 기반으로 눈 인식 영역(A1), 코 인식 영역(A2), 입 인식 영역(A3)을 포함하는 인식 대상 영역(A)을 설정하고, 인식 대상 영역(A)을 예비 추론부로 송신한다. 이 때, 인식 대상 영역(A)이 사용자의 동작에 따라 크기가 변화할 수 있다. B단계는 A단계와 비교하여, 인식 대상 영역(A)이 동작에 따라 크기가 변화되는 것을 나타내고 있다. 이 후, 예비 추론부는 손 영상 데이터(DV1)에 기반한 위치 추론 데이터(DM1)를 생성하여, A단계에서 B단계 로의 손의 움직임을 추적할 수 있다. 예비 추론부는 위치 추론 데이터(DM1)에 기반한 추적 대상 영역(B2)과 인식 대상 영역(A)의 중첩됨 여부 판 단을 기반으로 중첩 판단 데이터(P2)가 생성된다. 또한, 예비 추론부는 대화 상태 여부를 판단하여 대화 판단 데이터(P1)를 생성한다. 이 때, 예비 추론부는 제1 학습 모델(LM1)을 이용하여, (A)단계를 포함한 이전 상황에서 감성인식 대상이 되는 사용자의 대화 여부가 지속되고 있는지를 고려하여 대화 상태 여부를 판단 할 수 있다. 예를 들어, A단계에서 사용자가 대화 상태가 아닌 것으로 추론된 경우, 상기 결과를 바탕으로, B단계에서 입 인 식 영역(A3)에 기초하여 일시적으로 사용자의 입 모양이 대화 상태에서의 입 모양과 유사하더라도, 예비 추론부 는 제1 학습 모델(LM1)을 이용하여, 사용자가 대화 상태가 아닌 것으로 판단할 수 있다. 즉, 예비 추론부 는 A단계에서의 대화 상태 판단 결과에 기초하여, 다음 장면인 B단계에서의 대화 상태 판단 여부에 대한 추론을 실시할 수 있다. 이 후, 메인 추론부는 수신된 얼굴 특징 데이터(DF1) 및 음성 특징 데이터(DF2)를 제3 학습 모델(LM3)을 사 용하여 복수의 서브 특징맵(FM)을 생성하고, 복수의 서브 특징맵(FM)과 손에 해당되는 위치 추론 특징맵(FM1)을 통합하여 멀티 모달 특징맵(M)을 생성한다. 이 후, 메인 추론부는 제4 학습 모델(LM4)을 통해 이전((A)단계)의 사용자의 행동을 고려하여 감성인식을 추론하고, 이를 감성인식 결과로 나타낼 수 있다. ((C)단계) B단계 이후, 사용자가 입을 손으로 가리는 행동을 나타내고 있다. 영상 전처리부는 인식 가능한 사용자의 눈의 얼굴 요소 위치 정보(AL)를 기반으로 눈 인식 영역(A1)을 포 함하는 인식 대상 영역(A)이 설정되고, 인식 대상 영역(A)을 예비 추론부로 송신한다. 이 후, 예비 추론부는 영상 데이터(DV)로부터 검출된 추적 대상 영역(B3)에 대한 손 영상 데이터(DV1)를 생 성한다. 이 때, 손 영상 데이터(DV1)를 통해 손의 움직임을 파악하는 위치 추론 데이터(DM1)를 생성하고, 위치 추론 데이터(DM1)에 기반한 추적 대상 영역(B3)과 인식 대상 영역(A)의 중첩 여부 판단을 기반으로 중첩 판단 데 이터(P2)가 생성된다. 여기서, 중첩 판단 데이터(P2)는 눈 인식 영역(A1)에 기초한 얼굴 특징 데이터(DF1)의 사용 여부 또는 얼굴 특징 데이터(DF1)에 적용되는 가중치를 나타내는 파라미터를 포함할 수 있다.또한, 예비 추론부는 (A)단계, (B)단계에서 인식 대상 영역(A)이었던 코 인식 영역(A2) 또는 입 인식 영역 (A3)과 사용자의 손 위치에 대한 영역인 추적 대상 영역(B3)과의 중첩을 인지하여, 감성인식 추론에서 제외됨 또 는 중요도가 떨어짐을 나타내는 파라미터가 중첩 판단 데이터(P2)에 포함될 수 있다. 또한, 예비 추론부는 입 인식 영역(A3)에 대응되는 입 영상 데이터(DV2)가 인식되지 않는 상황과 사용자가 이전 대화 상태 여부의 판단 결과를 고려하여, 음성 특징 데이터(DF2)의 사용 판단 여부의 나타내는 값을 대화 판단 데이터(P1)에 포함시킬 수 있다. 여기서, 상기 이전 대화 상태 여부의 판단 결과는 시간적 학습 모델을 통해 추론한다. 이 때, 시간적 학습 모델 은 LSTM(Long Short-Term Memory), RNNs(Recurrent Neural Network), GRU(Gated Recurrent Unit) 등 순환 신 경망과 같은 시간적 학습 모델일 수 있다. 이 후, 서브 특징맵 생성부는 눈에 해당되는 영역의 얼굴 특징 데이터(DF1)를 제3 학습 모델(LM3)을 사용하 여 복수의 서브 특징맵(FM)을 생성한다. 이 후, 멀티 모달 특징맵 생성부는 복수의 서브 특징맵(FM)과 손에 해당되는 위치 추론 특징맵(FM1)을 통 합하여 멀티 모달 특징맵(M)을 생성한다. 이 후, 감정인식 추론부는 제4 학습 모델(LM4)을 통해 이전의 사용자의 행동을 고려하여 감성인식을 추론 하고, 이를 감성인식 결과로 나타낼 수 있다."}
{"patent_id": "10-2020-0081613", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다. 이상에서 설명된 시스템 또는 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 시스템, 장치 및 구성요소는, 예를 들어, 프로세서, 콘트롤러, ALU(Arithmetic Logic Unit), 디지털 신호 프로세서(Digital signalprocessor), 마이크로컴퓨터, FPA(Field Programmable Array), PLU(Programmable Logic Unit), 마이크로 프로세서, 또는 명령(Instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴 퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응 답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사"}
{"patent_id": "10-2020-0081613", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "용되는 것으로 설명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처 리요소(Processing Element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (Parallel Processor)와 같은, 다른 처리 구성(Processing Configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(Computer Program), 코드(Code), 명령(Instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처 리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(Component), 물리적 장치, 가상 장치(Virtual Equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(Signal Wave)에 영구적으로, 또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 하나 이상의 컴퓨터 판독 가능 기록 매 체에 저장될 수 있다. 실시예들에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되 고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM, DVD와 같은 광기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기-광 매체(Magneto- optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드 뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하 드웨어 장치는 실시예의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다."}
{"patent_id": "10-2020-0081613", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 멀티모달 감성 인식 장치의 구성을 개략적으로 도시한 도면이다. 도 2는 도 1의 멀티모달 감성 인식 장치 중 데이터 전처리부의 구성을 개략적으로 도시한 도면이다. 도 3는 도 1의 멀티모달 감성 인식 장치 중 예비 추론부의 구성을 개략적으로 도시한 도면이다. 도 4는 도 1의 멀티모달 감성 인식 장치 중 메인 추론부의 구성을 개략적으로 도시한 도면이다. 도 5는 도 1의 멀티모달 감성 인식 장치에 의한 멀티모달 감성 인식 방법을 보여주는 순서도이다. 도 6은 도 5의 멀티모달 감성 인식 방법 중 데이터 전처리 단계를 상세하게 보여주는 순서도이다. 도 7은 도 5의 멀티모달 감성 인식 방법 중 예비 추론 단계를 상세하게 보여주는 순서도이다. 도 8은 도 5의 멀티모달 감성 인식 방법 중 메인 추론 단계를 상세하게 보여주는 순서도이다. 도 9는 도 1의 멀티모달 감성 인식 장치에서 상황 변화 여부에 따른 얼굴 인식 과정을 보여주는 예시적인 도면 이다."}
