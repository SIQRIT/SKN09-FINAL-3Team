{"patent_id": "10-2024-0078785", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0105330", "출원번호": "10-2024-0078785", "발명의 명칭": "텍스트 이미지 모델의 트레이닝 방법, 모델, 장치 및 전자 기기", "출원인": "베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니", "발명자": "스 이쉬안"}}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "텍스트 이미지 모델의 트레이닝 방법으로서, 제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하되, 상기 제1 텍스트 이미지 모델은 입력 텍스트를 기반으로 대응하는 생성 이미지를 생성하는 데 사용되고, 상기 사전 트레이닝된 보상 모델은 상기 입력 텍스트 및 상기 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점하는 데 사용되는 단계; 및상기 사전 트레이닝된 보상 모델 및 강화 학습 전략을 기반으로 상기 제1 텍스트 이미지 모델의 파라미터를 조정하여 제2 텍스트 이미지 모델을 획득하는 단계를 포함하고,상기 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상은 기설정된 조건을 충족시키며, 상기 누적 보상은 상기 생성 시퀀스의 각 단계의 보상을 기반으로 획득된 것인 텍스트 이미지모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 기설정된 조건은 상기 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상이 상기 제1 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상보다높은 것을 포함하는 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 또는 제2항에 있어서,상기 강화 학습 전략은 근접 정책 최적화 알고리즘을 포함하는 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 근접 정책 최적화 알고리즘은 행동 서브 모델 및 판정 서브 모델을 사용하고, 상기 행동 서브 모델은 상기제1 텍스트 이미지 모델을 기반으로 초기화되어 획득된 것이며, 상기 판정 서브 모델은 상기 사전 트레이닝된보상 모델을 기반으로 초기화되어 획득된 것인 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 생성 시퀀스는 적어도 하나의 단계를 포함하되, 상기 생성 시퀀스의 각 단계에 대해,상기 행동 서브 모델은 제공된 입력 텍스트를 기반으로 대응하는 출력 노이지 이미지를 생성하고;상기 판정 서브 모델은 현재 단계의 상기 입력 텍스트 및 상기 출력 노이지 이미지를 기반으로 상기 현재 단계의 보상을 출력하는 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 현재 단계의 보상은 상기 현재 단계 전 이전 단계에서 상기 행동 서브 모델의 출력과 상기 현재 단계에서상기 행동 서브 모델의 출력 사이의 상대 엔트로피를 포함하는 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "공개특허 10-2024-0105330-3-제5항에 있어서,상기 현재 단계의 보상은 상기 현재 단계 전 이전 단계의 평가값과 상기 현재 단계의 평가값 사이의 차이값을포함하되, 상기 평가값은 제공된 입력 텍스트 및 대응하는 출력 노이지 이미지를 기반으로 상기 사전 트레이닝된 보상 모델로 채점하여 획득된 것인 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항 또는 제7항에 있어서,상기 생성 시퀀스에서 획득된 상기 누적 보상은 총 평점 및 손실항을 포함하되, 상기 총 평점은 상기 생성 시퀀스의 초기 입력 및 최종 출력을 기반으로 상기 사전 트레이닝된 보상 모델에 의해 획득되고, 상기 손실항은 상기 생성 시퀀스의 마지막 단계의 보상과 손실 계수의 곱인 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제8항 중 어느 한 항에 있어서,상기 제2 텍스트 이미지 모델의 생성 시퀀스 중의 상기 누적 보상을 기반으로, 역전파 알고리즘을 통해 상기 제2 텍스트 이미지 모델의 파라미터를 획득하는 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제9항 중 어느 한 항에 있어서,상기 사전 트레이닝된 보상 모델은 피드백 데이터 세트를 기반으로 트레이닝하여 획득된 것이고, 상기 피드백데이터 세트는 복수의 피드백 데이터를 포함하며, 상기 복수의 피드백 데이터는 상기 입력 텍스트 및 상기 대응하는 생성 이미지로 구성된 데이터 쌍 및 상기 데이터 쌍에 대응하는 피드백 상태를 포함하고, 상기 피드백 상태는 동일한 입력 텍스트에 대해 생성된 상기 대응하는 생성 이미지가 긍정적 피드백 또는 부정적 피드백에 속하는 것을 특성화하는 데 사용되는 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 보상 모델을 트레이닝하는 단계는,상기 복수의 피드백 데이터를 기반으로, 비교 학습 형태로 상기 보상 모델을 트레이닝하여 상기 보상 모델이 상기 피드백 상태가 긍정적 피드백인 상기 데이터 쌍에 제1 보상 평점을 출력하고, 상기 피드백 상태가 부정적 피드백인 상기 데이터 쌍에 제2 보상 평점을 출력하도록 하되, 상기 제1 보상 평점과 상기 제2 보상 평점의 차이값은 상기 대응하는 생성 이미지의 품질 차이를 특성화하는 데 사용되는 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항 또는 제11항에 있어서,상기 피드백 데이터 세트는 상이한 소스로부터의 적어도 2가지 상기 복수의 피드백 데이터를 포함하는 텍스트이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 복수의 피드백 데이터는 사용자 피드백 데이터, 인위적 표기 데이터 및 수동 비교 데이터 중 적어도 2가지를 포함하되,상기 사용자 피드백 데이터는 사용자 행동을 기반으로 상기 피드백 상태를 획득하고;상기 인위적 표기 데이터는 인위적 표기 결과를 기반으로 상기 피드백 상태를 획득하며;상기 수동 비교 데이터는 상이한 버전의 생성 이미지를 기반으로 상기 피드백 상태를 획득하는 텍스트 이미지모델의 트레이닝 방법.공개특허 10-2024-0105330-4-청구항 14 제1항 내지 제14항 중 어느 한 항에 있어서, 제1 텍스트 이미지 모델을 획득하는 상기 단계는, 인위적 표기 이미지 텍스트 쌍을 트레이닝할 상기 제1 텍스트 이미지 모델의 트레이닝 샘플로서 획득하는 단계;및역전파 알고리즘을 기반으로 트레이닝할 상기 제1 텍스트 이미지 모델의 파라미터를 업데이트하여 지도 트레이닝을 거친 상기 제1 텍스트 이미지 모델을 획득하는 단계를 포함하는 텍스트 이미지 모델의 트레이닝 방법."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1항 내지 제14항 중 어느 한 항에 따른 방법으로 트레이닝하여 획득된 텍스트 이미지 모델."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "텍스트 이미지 모델의 트레이닝 장치로서,제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하도록 구성되되, 상기 제1 텍스트 이미지 모델은입력 텍스트를 기반으로 대응하는 생성 이미지를 생성하고, 상기 사전 트레이닝된 보상 모델은 상기 입력 텍스트 및 상기 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점하는 획득 모듈; 및상기 사전 트레이닝된 보상 모델 및 강화 학습 전략을 기반으로 상기 제1 텍스트 이미지 모델의 파라미터를 조정하여 제2 텍스트 이미지 모델을 획득하도록 구성되는 조정 모듈을 포함하고,상기 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상은 기설정된 조건을 충족시키며, 상기 누적 보상은 상기 생성 시퀀스의 각 항의 보상의 합을 기반으로 획득된 것인 텍스트 이미지 모델의 트레이닝 장치."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 기설정된 조건은 상기 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상이 상기 제1 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상보다높은 것을 포함하는 텍스트 이미지 모델의 트레이닝 장치."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항 또는 제17항에 있어서,상기 강화 학습 전략은 근접 정책 최적화 알고리즘을 포함하는 텍스트 이미지 모델의 트레이닝 장치."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 근접 정책 최적화 알고리즘은 행동 서브 모델 및 판정 서브 모델을 사용하고;상기 조정 모듈은,상기 제1 텍스트 이미지 모델을 기반으로 상기 행동 서브 모델을 초기화하도록 구성되는 행동 서브 모듈; 및상기 사전 트레이닝된 보상 모듈을 기반으로 상기 판정 서브 모델을 초기화하도록 구성되는 판정 서브 모듈을포함하는 텍스트 이미지 모델의 트레이닝 장치."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서,상기 생성 시퀀스는 적어도 하나의 단계를 포함하되, 상기 생성 시퀀스의 각 단계에 대해,공개특허 10-2024-0105330-5-상기 행동 서브 모듈은 제공된 입력 텍스트를 기반으로 대응하는 출력 노이지 이미지를 생성하도록 구성되고;상기 판정 서브 모듈은 또한 현재 단계의 입력 텍스트 및 출력 노이지 이미지를 기반으로 현재 단계의 보상을출력하도록 구성되는 텍스트 이미지 모델의 트레이닝 장치."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제20항에 있어서,상기 조정 모듈은,총 평점 및 손실항을 기반으로 상기 생성 시퀀스 중의 누적 보상을 생성하도록 구성되는 보상 모듈을 더 포함하되;상기 총 평점은 상기 생성 시퀀스의 초기 입력 및 최종 출력을 기반으로 상기 사전 트레이닝된 보상 모델에 의해 획득되고;상기 손실항은 상기 생성 시퀀스의 마지막 단계의 보상과 손실 계수의 곱인 텍스트 이미지 모델의 트레이닝 장치."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제16항 내지 제21항 중 어느 한 항에 있어서,피드백 데이터 세트를 기반으로 보상 모델을 트레이닝하여 상기 사전 트레이닝된 보상 모델을 획득하도록 구성되는 제1 사전 트레이닝 모듈을 더 포함하되,상기 피드백 데이터 세트는 복수의 피드백 데이터를 포함하며, 상기 복수의 피드백 데이터는 상기 입력 텍스트및 상기 대응하는 생성 이미지로 구성된 데이터 쌍 및 상기 데이터 쌍에 대응하는 피드백 상태를 포함하고, 상기 피드백 상태는 동일한 입력 텍스트에 대해 생성된 상기 대응하는 생성 이미지가 긍정적 피드백 또는 부정적피드백에 속하는 것을 특성화하는 데 사용되는 텍스트 이미지 모델의 트레이닝 장치."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제16항 내지 제22항 중 어느 한 항에 있어서,인위적 표기 이미지 텍스트 쌍을 기반으로 트레이닝할 상기 제1 텍스트 이미지 모델을 트레이닝하여 지도 트레이닝을 거친 상기 제1 텍스트 이미지 모델을 획득하도록 구성되는 제2 사전 트레이닝 모듈을 더 포함하는 텍스트 이미지 모델의 트레이닝 장치."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "전자 기기로서,적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하되;상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되고, 상기 명령은 상기 적어도하나의 프로세서에 의해 실행되어 상기 적어도 하나의 프로세서가 제1항 내지 제14항 중 어느 한 항에 따른 방법을 수행할 수 있도록 하는 전자 기기."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "컴퓨터 명령이 저장된 비일시적 컴퓨터 판독 가능 저장 매체로서, 상기 컴퓨터 명령은 상기 컴퓨터가 제1항 내지 제14항 중 어느 한 항에 따른 방법을 수행하도록 하기 위한 것인비일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2024-0078785", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "컴퓨터 판독 가능 저장 매체에 저장된 컴퓨터 프로그램으로서, 공개특허 10-2024-0105330-6-상기 컴퓨터 프로그램은 명령을 포함하고, 상기 명령이 적어도 하나의 프로세서에 의해 실행될 경우 제1항 내지제14항 중 어느 한 항에 따른 방법을 구현하는, 컴퓨터 판독 가능 저장 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 텍스트 이미지 모델의 트레이닝 방법, 모델, 장치 및 전자 기기를 제공한다. 강화 학습, 컴퓨터 비전"}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것이다. 구현 수단은, 제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하되, 여기 서 제1 텍스트 이미지 모델은 입력 텍스트를 기반으로 대응하는 생성 이미지를 생성하는 데 사용되고, 사전 트레 이닝된 보상 모델은 입력 텍스트 및 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점하는 데 사용되는 것이며; 상기 텍스트 이미지 모델의 트레이닝 방법은, 사전 트레이닝된 보상 모델 및 강화 학습 전략을 기반으로 제1 텍스트 이미지 모델의 파라미터를 조정하여 제2 텍스트 이미지 모델을 획득하는 단계를 더 포함한다. 대 표 도 - 도3"}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2024-0105330 CPC특허분류 G06N 3/084 (2023.01) G06N 3/09 (2023.01) G06N 3/092 (2023.01) G06T 2207/20081 (2013.01) 발명자 류 자천 중국 100085 베이징 하이뎬 디스트릭트 상디 10번 스트리트 넘버 10 바이두 캠퍼스 2층샤오 신옌 중국 100085 베이징 하이뎬 디스트릭트 상디 10번 스트리트 넘버 10 바이두 캠퍼스 2층명 세 서 청구범위 청구항 1 텍스트 이미지 모델의 트레이닝 방법으로서, 제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하되, 상기 제1 텍스트 이미지 모델은 입력 텍스 트를 기반으로 대응하는 생성 이미지를 생성하는 데 사용되고, 상기 사전 트레이닝된 보상 모델은 상기 입력 텍 스트 및 상기 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점하는 데 사용되는 단계; 및 상기 사전 트레이닝된 보상 모델 및 강화 학습 전략을 기반으로 상기 제1 텍스트 이미지 모델의 파라미터를 조 정하여 제2 텍스트 이미지 모델을 획득하는 단계를 포함하고, 상기 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상은 기설정된 조 건을 충족시키며, 상기 누적 보상은 상기 생성 시퀀스의 각 단계의 보상을 기반으로 획득된 것인 텍스트 이미지 모델의 트레이닝 방법. 청구항 2 제1항에 있어서, 상기 기설정된 조건은 상기 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누 적 보상이 상기 제1 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상보다 높은 것을 포함하는 텍스트 이미지 모델의 트레이닝 방법. 청구항 3 제1항 또는 제2항에 있어서, 상기 강화 학습 전략은 근접 정책 최적화 알고리즘을 포함하는 텍스트 이미지 모델의 트레이닝 방법. 청구항 4 제3항에 있어서, 상기 근접 정책 최적화 알고리즘은 행동 서브 모델 및 판정 서브 모델을 사용하고, 상기 행동 서브 모델은 상기 제1 텍스트 이미지 모델을 기반으로 초기화되어 획득된 것이며, 상기 판정 서브 모델은 상기 사전 트레이닝된 보상 모델을 기반으로 초기화되어 획득된 것인 텍스트 이미지 모델의 트레이닝 방법. 청구항 5 제4항에 있어서, 상기 생성 시퀀스는 적어도 하나의 단계를 포함하되, 상기 생성 시퀀스의 각 단계에 대해, 상기 행동 서브 모델은 제공된 입력 텍스트를 기반으로 대응하는 출력 노이지 이미지를 생성하고; 상기 판정 서브 모델은 현재 단계의 상기 입력 텍스트 및 상기 출력 노이지 이미지를 기반으로 상기 현재 단계 의 보상을 출력하는 텍스트 이미지 모델의 트레이닝 방법. 청구항 6 제5항에 있어서, 상기 현재 단계의 보상은 상기 현재 단계 전 이전 단계에서 상기 행동 서브 모델의 출력과 상기 현재 단계에서 상기 행동 서브 모델의 출력 사이의 상대 엔트로피를 포함하는 텍스트 이미지 모델의 트레이닝 방법. 청구항 7 제5항에 있어서, 상기 현재 단계의 보상은 상기 현재 단계 전 이전 단계의 평가값과 상기 현재 단계의 평가값 사이의 차이값을 포함하되, 상기 평가값은 제공된 입력 텍스트 및 대응하는 출력 노이지 이미지를 기반으로 상기 사전 트레이닝 된 보상 모델로 채점하여 획득된 것인 텍스트 이미지 모델의 트레이닝 방법. 청구항 8 제6항 또는 제7항에 있어서, 상기 생성 시퀀스에서 획득된 상기 누적 보상은 총 평점 및 손실항을 포함하되, 상기 총 평점은 상기 생성 시퀀 스의 초기 입력 및 최종 출력을 기반으로 상기 사전 트레이닝된 보상 모델에 의해 획득되고, 상기 손실항은 상 기 생성 시퀀스의 마지막 단계의 보상과 손실 계수의 곱인 텍스트 이미지 모델의 트레이닝 방법. 청구항 9 제1항 내지 제8항 중 어느 한 항에 있어서, 상기 제2 텍스트 이미지 모델의 생성 시퀀스 중의 상기 누적 보상을 기반으로, 역전파 알고리즘을 통해 상기 제 2 텍스트 이미지 모델의 파라미터를 획득하는 텍스트 이미지 모델의 트레이닝 방법. 청구항 10 제1항 내지 제9항 중 어느 한 항에 있어서, 상기 사전 트레이닝된 보상 모델은 피드백 데이터 세트를 기반으로 트레이닝하여 획득된 것이고, 상기 피드백 데이터 세트는 복수의 피드백 데이터를 포함하며, 상기 복수의 피드백 데이터는 상기 입력 텍스트 및 상기 대응 하는 생성 이미지로 구성된 데이터 쌍 및 상기 데이터 쌍에 대응하는 피드백 상태를 포함하고, 상기 피드백 상 태는 동일한 입력 텍스트에 대해 생성된 상기 대응하는 생성 이미지가 긍정적 피드백 또는 부정적 피드백에 속 하는 것을 특성화하는 데 사용되는 텍스트 이미지 모델의 트레이닝 방법. 청구항 11 제10항에 있어서, 상기 보상 모델을 트레이닝하는 단계는, 상기 복수의 피드백 데이터를 기반으로, 비교 학습 형태로 상기 보상 모델을 트레이닝하여 상기 보상 모델이 상 기 피드백 상태가 긍정적 피드백인 상기 데이터 쌍에 제1 보상 평점을 출력하고, 상기 피드백 상태가 부정적 피 드백인 상기 데이터 쌍에 제2 보상 평점을 출력하도록 하되, 상기 제1 보상 평점과 상기 제2 보상 평점의 차이 값은 상기 대응하는 생성 이미지의 품질 차이를 특성화하는 데 사용되는 텍스트 이미지 모델의 트레이닝 방법. 청구항 12 제10항 또는 제11항에 있어서, 상기 피드백 데이터 세트는 상이한 소스로부터의 적어도 2가지 상기 복수의 피드백 데이터를 포함하는 텍스트 이미지 모델의 트레이닝 방법. 청구항 13 제12항에 있어서, 상기 복수의 피드백 데이터는 사용자 피드백 데이터, 인위적 표기 데이터 및 수동 비교 데이터 중 적어도 2가지 를 포함하되, 상기 사용자 피드백 데이터는 사용자 행동을 기반으로 상기 피드백 상태를 획득하고; 상기 인위적 표기 데이터는 인위적 표기 결과를 기반으로 상기 피드백 상태를 획득하며; 상기 수동 비교 데이터는 상이한 버전의 생성 이미지를 기반으로 상기 피드백 상태를 획득하는 텍스트 이미지 모델의 트레이닝 방법.청구항 14 제1항 내지 제14항 중 어느 한 항에 있어서, 제1 텍스트 이미지 모델을 획득하는 상기 단계는, 인위적 표기 이미지 텍스트 쌍을 트레이닝할 상기 제1 텍스트 이미지 모델의 트레이닝 샘플로서 획득하는 단계; 및 역전파 알고리즘을 기반으로 트레이닝할 상기 제1 텍스트 이미지 모델의 파라미터를 업데이트하여 지도 트레이 닝을 거친 상기 제1 텍스트 이미지 모델을 획득하는 단계를 포함하는 텍스트 이미지 모델의 트레이닝 방법. 청구항 15 제1항 내지 제14항 중 어느 한 항에 따른 방법으로 트레이닝하여 획득된 텍스트 이미지 모델. 청구항 16 텍스트 이미지 모델의 트레이닝 장치로서, 제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하도록 구성되되, 상기 제1 텍스트 이미지 모델은 입력 텍스트를 기반으로 대응하는 생성 이미지를 생성하고, 상기 사전 트레이닝된 보상 모델은 상기 입력 텍스 트 및 상기 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점하는 획득 모듈; 및 상기 사전 트레이닝된 보상 모델 및 강화 학습 전략을 기반으로 상기 제1 텍스트 이미지 모델의 파라미터를 조 정하여 제2 텍스트 이미지 모델을 획득하도록 구성되는 조정 모듈을 포함하고, 상기 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상은 기설정된 조 건을 충족시키며, 상기 누적 보상은 상기 생성 시퀀스의 각 항의 보상의 합을 기반으로 획득된 것인 텍스트 이 미지 모델의 트레이닝 장치. 청구항 17 제16항에 있어서, 상기 기설정된 조건은 상기 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누 적 보상이 상기 제1 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상보다 높은 것을 포함하는 텍스트 이미지 모델의 트레이닝 장치. 청구항 18 제16항 또는 제17항에 있어서, 상기 강화 학습 전략은 근접 정책 최적화 알고리즘을 포함하는 텍스트 이미지 모델의 트레이닝 장치. 청구항 19 제18항에 있어서, 상기 근접 정책 최적화 알고리즘은 행동 서브 모델 및 판정 서브 모델을 사용하고; 상기 조정 모듈은, 상기 제1 텍스트 이미지 모델을 기반으로 상기 행동 서브 모델을 초기화하도록 구성되는 행동 서브 모듈; 및 상기 사전 트레이닝된 보상 모듈을 기반으로 상기 판정 서브 모델을 초기화하도록 구성되는 판정 서브 모듈을 포함하는 텍스트 이미지 모델의 트레이닝 장치. 청구항 20 제19항에 있어서, 상기 생성 시퀀스는 적어도 하나의 단계를 포함하되, 상기 생성 시퀀스의 각 단계에 대해,상기 행동 서브 모듈은 제공된 입력 텍스트를 기반으로 대응하는 출력 노이지 이미지를 생성하도록 구성되고; 상기 판정 서브 모듈은 또한 현재 단계의 입력 텍스트 및 출력 노이지 이미지를 기반으로 현재 단계의 보상을 출력하도록 구성되는 텍스트 이미지 모델의 트레이닝 장치. 청구항 21 제20항에 있어서, 상기 조정 모듈은, 총 평점 및 손실항을 기반으로 상기 생성 시퀀스 중의 누적 보상을 생성하도록 구성되는 보상 모듈을 더 포함하 되; 상기 총 평점은 상기 생성 시퀀스의 초기 입력 및 최종 출력을 기반으로 상기 사전 트레이닝된 보상 모델에 의 해 획득되고; 상기 손실항은 상기 생성 시퀀스의 마지막 단계의 보상과 손실 계수의 곱인 텍스트 이미지 모델의 트레이닝 장 치. 청구항 22 제16항 내지 제21항 중 어느 한 항에 있어서, 피드백 데이터 세트를 기반으로 보상 모델을 트레이닝하여 상기 사전 트레이닝된 보상 모델을 획득하도록 구성 되는 제1 사전 트레이닝 모듈을 더 포함하되, 상기 피드백 데이터 세트는 복수의 피드백 데이터를 포함하며, 상기 복수의 피드백 데이터는 상기 입력 텍스트 및 상기 대응하는 생성 이미지로 구성된 데이터 쌍 및 상기 데이터 쌍에 대응하는 피드백 상태를 포함하고, 상 기 피드백 상태는 동일한 입력 텍스트에 대해 생성된 상기 대응하는 생성 이미지가 긍정적 피드백 또는 부정적 피드백에 속하는 것을 특성화하는 데 사용되는 텍스트 이미지 모델의 트레이닝 장치. 청구항 23 제16항 내지 제22항 중 어느 한 항에 있어서, 인위적 표기 이미지 텍스트 쌍을 기반으로 트레이닝할 상기 제1 텍스트 이미지 모델을 트레이닝하여 지도 트레 이닝을 거친 상기 제1 텍스트 이미지 모델을 획득하도록 구성되는 제2 사전 트레이닝 모듈을 더 포함하는 텍스 트 이미지 모델의 트레이닝 장치. 청구항 24 전자 기기로서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하되; 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되고, 상기 명령은 상기 적어도 하나의 프로세서에 의해 실행되어 상기 적어도 하나의 프로세서가 제1항 내지 제14항 중 어느 한 항에 따른 방 법을 수행할 수 있도록 하는 전자 기기. 청구항 25 컴퓨터 명령이 저장된 비일시적 컴퓨터 판독 가능 저장 매체로서, 상기 컴퓨터 명령은 상기 컴퓨터가 제1항 내지 제14항 중 어느 한 항에 따른 방법을 수행하도록 하기 위한 것인 비일시적 컴퓨터 판독 가능 저장 매체. 청구항 26 컴퓨터 판독 가능 저장 매체에 저장된 컴퓨터 프로그램으로서, 상기 컴퓨터 프로그램은 명령을 포함하고, 상기 명령이 적어도 하나의 프로세서에 의해 실행될 경우 제1항 내지 제14항 중 어느 한 항에 따른 방법을 구현하는, 컴퓨터 판독 가능 저장 매체에 저장된 컴퓨터 프로그램. 발명의 설명 기 술 분 야"}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 3, "content": "본 발명은 강화 학습, 컴퓨터 비전 기술분야에 관한 것으로, 구체적으로 텍스트 이미지 모델의 트레이닝 방법, 텍스트 이미지 모델, 텍스트 이미지 모델의 트레이닝 장치, 전자 기기, 컴퓨터 판독 가능 저장 매체 및 컴퓨터 프로그램 제품에 관한 것이다."}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능은 컴퓨터가 인간의 특정 사고 과정과 지능적인 행동(예를 들어, 학습, 추론, 사고, 계획 등)을 시뮬레 이션하도록 하는 학과를 연구하는 것으로, 하드웨어 수준의 기술과 소프트웨어 수준의 기술을 모두 갖추고 있다. 인공지능 하드웨어 기술에는 일반적으로 센서, 전용 인공지능 칩, 클라우드 컴퓨팅, 분산 스토리지, 빅데 이터 처리 등의 기술이 포함되며; 인공지능 소프트웨어 기술에는 주로 컴퓨터 비전 기술, 음성 인식 기술, 자연 언어 처리 기술 및 기계 학습/딥러닝, 빅데이터 처리 기술, 지식 이미지 기술 등 여러 방향이 포함된다. 텍스트 이미지 모델(Text-Image Model, TIM)은 입력 텍스트를 기반으로 대응하는 이미지를 생성하는 모델로, 최 근 연구에서는 일반적으로 사용자의 모호한 자연 언어 설명을 기반으로 보다 예술적이고 심미적인 이미지를 생 성할 수 있는 확산 모델(Diffusion Model)을 주요로 한다. 텍스트 이미지 모델에서 모델 출력 이미지가 입력 텍 스트의 의미와 세부사항에 일치시켜 최대한 예술적이도록 하는 것은 많은 사람들이 주목하고 있는 연구 방향이 다. 이 부분에서 설명된 방법은 반드시 이전에 구상되었거나 사용된 방법이 아닐 수도 있다. 달리 명시되지 않는 한, 이 부분에 설명된 방법이 단지 이 부분에 포함되어 있다는 이유만으로 선행 기술이라고 가정해서는 안된다. 마찬가지로, 달리 명시되지 않는 한, 이 부분에 언급된 문제는 선행 기술에서 공인되는 것으로 간주되어서는 안 된다."}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 텍스트 이미지 모델의 트레이닝 방법, 텍스트 이미지 모델, 텍스트 이미지 모델의 트레이닝 장치, 전 자 기기, 컴퓨터 판독 가능 저장 매체 및 컴퓨터 프로그램 제품을 제공한다."}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 양태에 따르면, 제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하되, 제1 텍스트 이미지 모델은 입력 텍스트를 기반으로 대응하는 생성 이미지를 생성하는 데 사용되고, 사전 트레이닝된 보상 모델은 입력 텍스트 및 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점하는 데 사용되는 단계를 포 함하고; 사전 트레이닝된 보상 모델 및 강화 학습 전략을 기반으로 제1 텍스트 이미지 모델의 파라미터를 조정 하여 제2 텍스트 이미지 모델을 획득하는 단계를 더 포함하되, 여기서 제2 텍스트 이미지 모델이 텍스트 이미지 를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상은 기설정된 조건을 충족시키며, 누적 보상은 생성 시퀀스의 각 단계의 보상을 기반으로 획득된 것인 텍스트 이미지 모델의 트레이닝 방법을 제공한다. 본 발명의 다른 양태에 따르면, 위에서 제공된 텍스트 이미지 모델의 트레이닝 방법에 의해 트레이닝되어 획득 된 텍스트 이미지 모델을 제공한다. 본 발명의 다른 양태에 따르면, 획득 모듈 및 조정 모듈을 포함하는 텍스트 이미지 모델의 트레이닝 장치를 제 공한다. 획득 모듈은 제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하도록 구성되되, 여기서 제 1 텍스트 이미지 모델은 입력 텍스트를 기반으로 대응하는 생성 이미지를 생성하고, 사전 트레이닝된 보상 모델 은 입력 텍스트 및 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점한다. 조정 모듈은 사전 트레이닝 된 보상 모델 및 강화 학습 전략을 기반으로 제1 텍스트 이미지 모델의 파라미터를 조정하여 제2 텍스트 이미지 모델을 획득하도록 구성되되, 여기서 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에 서 얻은 누적 보상은 기설정된 조건을 충족시키며, 누적 보상은 생성 시퀀스의 각 단계의 보상을 기반으로 획득된 것이다. 본 발명의 다른 양태에 따르면, 적어도 하나의 프로세서; 및 적어도 하나의 프로세서와 통신 연결되는 메모리를 포함하되; 메모리에는 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되고, 명령은 적어도 하나의 프 로세서에 의해 실행되어 적어도 하나의 프로세서가 본 발명의 위에서 제공된 텍스트 이미지 모델의 트레이닝 방 법을 수행할 수 있도록 하는 전자 기기를 제공한다. 본 발명의 다른 양태에 따르면, 컴퓨터 명령이 저장된 비일시적 컴퓨터 판독 가능 저장 매체를 제공하며, 컴퓨 터 명령은 컴퓨터가 본 발명의 위에서 제공된 텍스트 이미지 모델의 트레이닝 방법을 수행하도록 하기 위한 것 이다. 본 발명의 다른 양태에 따르면, 컴퓨터 판독 가능 저장 매체에 저장된 컴퓨터 프로그램을 제공하며, 상기 컴퓨 터 프로그램은 명령을 포함하고, 상기 명령이 적어도 하나의 프로세서에 의해 실행될 경우 본 발명의 위에서 제 공된 텍스트 이미지 모델의 트레이닝 방법을 구현한다."}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 하나 이상의 실시예에 따르면, 강화 학습을 기반으로 텍스트 이미지 모델의 생성 과정을 최적화하여 보상 신호가 전체 생성 과정을 제어함으로써 모델이 누적 보상이 높은 방향으로 최적화할 수 있도록 한다. 이해해야 할 것은, 본 부분에서 설명된 내용은 본 발명의 실시예의 핵심 또는 중요한 특징을 식별하려는 의도가 아니며, 본 발명의 범위를 한정하려는 것도 아니다. 본 발명의 다른 특징은 아래 명세서로부터 쉽게 이해될 것 이다."}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 도면을 결부하여 본 발명의 예시적인 실시예를 설명하되, 여기에는 이해를 돕기 위한 본 발명의 실시예의 다양한 세부사항들이 포함되지만, 이들은 단지 예시적인 것으로 이해되어야 한다. 따라서, 당업자는 본 발명의 범위를 벗어나지 않으면서 여기서 설명된 실시예에 대해 다양한 변형 및 수정을 진행할 수 있음을 이해해야 한 다. 마찬가지로, 명확성 및 간략함을 위해, 아래의 설명에서 공지 기능 및 구조에 대한 설명을 생략한다. 본 발명에서, 달리 설명되지 않는 한, 용어 \"제1\", \"제2\" 등을 사용하여 다양한 요소를 설명하는 것은 이러한 요소의 위치 관계, 시간 순서 관계 또는 중요성 관계를 한정하려는 의도가 아니며, 이러한 용어는 단지 하나의 요소를 다른 요소와 구분하기 위한 것이다. 일부 예시에서, 제1 요소와 제2 요소는 상기 요소의 동일한 예를 지 칭할 수 있고, 일부 경우 문맥적인 설명에 기반하여 상이한 예를 지칭할 수도 있다. 본 발명에서 다양한 상기 예시에 대한 설명에 사용되는 용어는 단지 특정 예시를 설명하기 위한 목적일 뿐 제한 하려는 의도가 아니다. 문맥상 다른 명확한 설명이 없고, 요소의 개수를 특별히 한정하지 않는 한, 상기 요소는 하나 이상일 수 있다. 이 밖에, 본 발명에 사용되는 용어 \"및/또는\"은 나열된 항목 중 어느 하나 및 전부 가능 한 조합 방식을 포함한다. 아래에 도면을 결부하여 본 발명의 실시예를 상세하게 설명한다. 도 1은 본 발명의 실시예에 따라 본문에 설명된 다양한 방법 및 장치가 구현될 수 있는 예시적인 시스템의 모식도를 도시한다. 도 1을 참조하면, 상기 시스템은 하나 이상의 클라이언트 기기(101, 102, 103, 104, 105 및 106), 서버 및 하나 이상의 클라이언트 기기를 서버에 연결하는 하나 이상의 통신 네트워크 를 포함한다. 클라이언트 기기(101, 102, 103, 104, 105 및 106)는 하나 이상의 응용 프로그램을 실행하도 록 구성될 수 있다. 본 발명의 실시예에서, 서버는 본 발명의 실시예에서 제공하는 텍스트 이미지 모델의 트레이닝 방법을 수 행할 수 있도록 하는 하나 이상의 서비스 또는 소프트웨어 애플리케이션을 실행할 수 있다. 일부 실시예에서, 서버는 또한 비가상 환경 및 가상 환경을 포함할 수 있는 다른 서비스 또는 소프트웨어 애플리케이션을 제공할 수 있다. 일부 실시예에서, 이러한 서비스는 클라이언트 기기(101, 102, 103, 104, 105 및/또는 106)의 사용자에게 서비스로서의 소프트웨어(SaaS) 모델에서와 같이 web 기반 서비스 또는 클라우드 서 비스로서 제공될 수 있다. 도 1에 도시된 구성에서, 서버는 서버에 의해 실행되는 기능을 구현하는 하나 이상의 구성요소를 포 함할 수 있다. 이러한 구성요소에는 하나 이상의 프로세서에 의해 실행 가능한 소프트웨어 구성요소, 하드웨어 구성요소 또는 이들의 조합이 포함될 수 있다. 클라이언트 기기(101, 102, 103, 104, 105 및/또는 106)를 조작 하는 사용자는 하나 이상의 클라이언트 애플리케이션을 순차적으로 이용하여 서버와 상호작용함으로써 이 들 구성요소에 의해 제공되는 서비스를 활용할 수 있다. 시스템과 상이할 수 있는 다양한 시스템 구성이 가능하다는 것을 이해해야 한다. 따라서, 도 1은 본 명세서에 설명된 다양한 방법을 구현하기 위한 시스템의 일 예시이며 제한하려는 의도는 아니다. 클라이언트 기기는 클라이언트 기기의 사용자가 클라이언트 기기와 상호작용할 수 있도록 하는 인터페이스를 제 공할 수 있다. 클라이언트 기기는 상기 인터페이스를 통해 사용자에게 정보를 출력할 수도 있다. 도 1은 단지 6 종의 클라이언트 기기를 도시하고 있지만, 당업자는 본 발명이 임의의 개수의 클라이언트 기기를 지원할 수 있 다는 것을 이해할 수 있을 것이다. 클라이언트 기기(101, 102, 103, 104, 105 및/또는 106)는 휴대용 핸드헬드 기기, 범용 컴퓨터(예를 들어, 개인 용 컴퓨터 및 랩톱 컴퓨터), 워크스테이션 컴퓨터, 웨어러블 기기, 스마트 스크린 기기, 셀프 서비스 단말 기기, 서비스 로봇, 게임 시스템, 씬 클라이언트, 다양한 메시지 송수신 기기, 센서 또는 기타 감지 기기와 같 은 다양한 유형의 컴퓨터 기기를 포함할 수 있다. 이러한 컴퓨터 기기는 MICROSOFT Windows, APPLE iOS, UNIX 유사 운영 체제, Linux 또는 Linux 유사 운영 체제(예를 들어, GOOGLE Chrome OS)와 같은 다양한 유형 및 버전 의 소프트웨어 애플리케이션 및 운영 체제를 실행할 수 있거나 MICROSOFT Windows Mobile OS, iOS, Windows Phone, Android와 같은 다양한 모바일 운영 체제를 포함할 수 있다. 휴대용 핸드헬드 기기는 셀룰러 폰, 스마트 폰, 태블릿 PC, 개인 휴대 정보 단말기(PDA) 등을 포함할 수 있다. 웨어러블 기기는 헤드 마운트 디스플레이(예 를 들어, 스마트 안경) 및 다른 기기를 포함할 수 있다. 게임 시스템은 다양한 휴대용 게임 기기, 인터넷 기반 게임 기기 등을 포함할 수 있다. 클라이언트 기기는 다양한 인터넷 관련 애플리케이션 프로그램, 통신 애플리케 이션 프로그램(예를 들어, 이메일 애플리케이션 프로그램), 단문 메시지 서비스(SMS) 애플리케이션 프로그램과 같은 다양한 애플리케이션 프로그램을 실행할 수 있으며 다양한 통신 프로토콜을 사용할 수 있다. 네트워크는 다양한 이용 가능한 프로토콜 중 어느 하나(TCP/IP, SNA, IPX 등을 포함하지만 이에 제한되지 않음)를 사용하여 데이터 통신을 지원할 수 있는 당업자에게 잘 알려진 임의의 유형의 네트워크일 수 있다. 단 지 예시로서, 하나 이상의 네트워크는 근거리 통신망(LAN), 이더넷 기반 네트워크, 토큰 링, 광역 통신망 (WAN), 인터넷, 가상 네트워크, 가상 사설망(VPN), 인트라넷, 엑스트라넷, 블록체인 네트워크, 공중 교환 전화 망(PSTN), 적외선 네트워크, 무선 네트워크(예를 들어, 블루투스, WIFI) 및/또는 이러한 네트워크 및/또는 기타네트워크의 임의의 조합일 수 있다. 서버는 하나 이상의 범용 컴퓨터, 전용 서버 컴퓨터(예를 들어, PC(개인용 컴퓨터) 서버, UNIX 서버, 중급 서버), 블레이드 서버, 메인프레임 컴퓨터, 서버 클러스터, 또는 임의의 다른 적절한 배치 및/또는 조합을 포함 할 수 있다. 서버는 가상 운영 체제를 실행하는 하나 이상의 가상 머신, 또는 가상화에 관한 다른 컴퓨팅 아키텍처(예를 들어, 서버의 가상 저장 기기를 유지하기 위해 가상화될 수 있는 논리적 저장 기기의 하나 이상 의 플렉시블 풀)를 포함할 수 있다. 다양한 실시예에서, 서버는 아래에 설명된 기능을 제공하는 하나 이상 의 서비스 또는 소프트웨어 애플리케이션을 실행할 수 있다. 서버 중의 컴퓨팅 유닛은 위에서 설명한 임의의 운영 체제뿐만 아니라 임의의 상업적으로 이용 가능한 서 버 운영 체제를 포함하는 하나 이상의 운영 체제를 실행할 수 있다. 서버는 또한 HTTP서버, FTP서버, CGI 서버, JAVA서버, 데이터베이스 서버 등을 포함하는 다양한 부가적 서버 애플리케이션 프로그램 및/또는 중간 계 층 애플리케이션 프로그램 중 어느 하나를 실행할 수 있다. 일부 실시형태에서, 서버는 클라이언트 기기(101, 102, 103, 104, 105 및/또는 106)의 사용자로부터 수신 된 데이터 피드 및/또는 이벤트 업데이트를 분석하고 합병하기 위한 하나 이상의 애플리케이션 프로그램을 포함 할 수 있다. 서버는 또한 클라이언트 기기(101, 102, 103, 104, 105 및/또는 106)의 하나 이상의 디스플 레이 기기를 통해 데이터 피드 및/또는 실시간 이벤트를 표시하는 하나 이상의 애플리케이션 프로그램을 포함할 수 있다. 일부 실시형태에서, 서버는 분산 시스템의 서버일 수도 있고, 블록체인과 결합된 서버일 수도 있다. 서버 는 클라우드 서버일 수도 있고, 인공지능 기술이 적용된 지능형 클라우드 컴퓨팅 서버 또는 지능형 클라우 드 호스트일 수도 있다. 클라우드 서버는 기존의 물리적 호스트와 가상사설서버(VPS) 서비스의 관리가 어렵고 사업 확장성이 취약한 결함을 해결하기 위한 클라우드 컴퓨팅 서비스 시스템 중의 호스트 제품이다. 시스템은 하나 이상의 데이터베이스를 더 포함할 수 있다. 일부 실시예에서, 이러한 데이터베이스는 데이터 및 기타 정보를 저장하는 데 사용될 수 있다. 예를 들어, 하나 이상의 데이터베이스는 오디오 파일 및 비디오 파일과 같은 정보를 저장하는 데 사용될 수 있다. 데이터베이스는 다양한 위치에 상주할 수 있 다. 예를 들어, 서버에 의해 사용되는 데이터베이스는 서버에 로컬되거나 서버로부터 멀리 떨어 져 있을 수 있고 네트워크 기반 또는 전용 연결을 통해 서버와 통신할 수도 있다. 데이터베이스는 상 이한 유형일 수 있다. 일부 실시예에서, 서버에 의해 사용되는 데이터베이스는 예를 들어 관계형 데이터베 이스일 수 있다. 이러한 데이터베이스 중 하나 이상은 명령에 응답하여 데이터베이스와 데이터베이스로부터의 데이터를 저장, 업데이트 및 검색할 수 있다. 일부 실시예에서, 데이터베이스 중 하나 이상은 또한 애플리케이션 프로그램 데이터를 저장하기 위해 애플 리케이션 프로그램에 의해 사용될 수도 있다. 애플리케이션 프로그램에서 사용하는 데이터베이스는 키-값 저장 소, 객체 저장소 또는 파일 시스템이 지원하는 일반 저장소와 같은 다양한 유형의 데이터베이스일 수 있다. 도 1의 시스템은 다양한 방식으로 구성 및 작동되어 본 발명에 따라 설명된 다양한 방법 및 장치가 적용될 수 있도록 한다. 도 2는 본 발명의 복수의 실시예에 따라 설명된 각 텍스트 이미지 모델의 인터랙션 모식도를 도시한다. 도 2를 참조하면, 텍스트 이미지 모델(Text-Image Model, TIM)은 입력 텍스트를 기반으로 대응하는 이미지를 생성 하는 모델을 의미하고, 최근 연구에서는 일반적으로 사용자의 모호한 자연 언어 설명(Prompt), 즉 입력 텍스트 를 기반으로 보다 예술적이고 심미적인 이미지, 즉 상기 입력 텍스트를 기반으로 대응하는 생성 이미 지를 생성할 수 있는 확산 모델(Diffusion Model)을 주요로 한다. 텍스트 이미지 모델에서 모델의 생 성 이미지가 입력 텍스트의 의미와 세부사항에 일치시켜 최대한 예술적이도록 하는 것은 많은 사람들 이 주목하고 있는 연구 방향이다. 도 2에 도시된 텍스트 이미지 모델을 예로 들면, 입력 텍스트에는 \"채색 구름이 황금 궁전, 무리를 지은 새, 중국 선녀, 리본이 달린 옷을 둘러싸고 있습니다\"이 포함되고, 즉 입력 텍스트에는 구름, 궁전, 새, 선녀 4개의 엔티티가 적어도 포함된다. 여기서 구름과 궁전의 엔티티 속성은 색채 속성(채색 구름과 황금 궁전)이고, 새의 엔티티 속성은 수 속성(복수의 새의 엔티티가 한 무리의 새 엔티티를 구성함)이며, 선녀의 엔 티티 속성은 스타일 속성(중식 선녀, 옷에 리본이 달려있음) 등이고, 텍스트 이미지 모델의 생성 이미지 에는 \"채색 구름이 황금 궁전과 무리를 지은 새를 둘러싸고 있습니다\"만 포함되고, 즉 생성 이미지에 는 구름, 궁전 및 새 3개의 엔티티만 포함되며 선녀라는 엔티티가 포함되지 않아 생성 이미지와 입력 텍스트의 엔티티 수가 일치하지 않다. 따라서, 텍스트 이미지 모델을 사용하여 이미지를 생성하는 사용자의 경 우, 인간의 시각을 기반으로 텍스트 이미지 모델의 생성 이미지가 입력 텍스트와 일치한지 여부 를 평가하면, 대체적으로 다음과 같은 여러 방향의 세부사항이 더 개선될 필요가 있다. 1) 엔티티 수; 2) 엔티 티 속성; 3) 다중 엔티티 조합; 4) 회화 배경; 5) 회화 스타일이다. 여러 방향에서 세부사항에 오류가 없는 이 미지를 생성하면 텍스트 이미지 유형의 제품의 기술적 기능이 향상되고 사용자 만족도가 높아질 것이다. 상술한 기술적 문제에 대해, 본 발명은 텍스트 이미지 모델의 트레이닝 방법을 제공한다. 도 3은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 방법의 흐름도를 도시한다. 도 3에 도시된 바 와 같이, 텍스트 이미지 모델의 트레이닝 방법은 다음과 같은 단계를 포함한다. 단계(S301)에서, 제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하되, 여기서 제1 텍스트 이미지 모델은 입력 텍스트를 기반으로 대응하는 생성 이미지를 생성하는 데 사용되고, 사전 트레이닝된 보상 모델은 입력 텍스트 및 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점하는 데 사용된다. 동일한 입력 텍스트는 복수의 출력 결과(즉, 생성 이미지)를 생성하므로 하나의 보상 모델(Reward Model, RM)을 사용하여 생성된 이미지를 채점한 후 보상 신호를 생성해야 하며, 보상 신호는 생성된 이미지를 인간의 시각에 서 정렬하거나 평가한다. 실행 가능한 일 실시형태로서, \"+\" 또는 \"-\" 부호와 같은 간단한 2원 보상 신호를 사용하여 주어진 보상 또는 처벌을 나타낼 수 있으며, 즉 보상 모델은 0 또는 1로 채점한다. 2원 보상 신호가 일부 경우에 생성 이미지의 차이를 충분하게 반영할 수 없을 수 있으므로, 실행 가능한 일 실 시형태로서, 보상 신호를 0 내지 5 사이의 정수를 사용하여 나타내고, 즉 보상 모델은 0 내지 5 사이의 정수로 채점하되, 여기서 5는 최고 보상을 나타내고, 0은 최저 보상을 나타낸다. 이와 같은 보상 신호는 모델로 하여금 생성 이미지의 품질 상태를 더 잘 이해할 수 있도록 하고, 후속의 조정 단계에서 모델의 성능을 향상시키는데 도움이 된다. 동일한 생성 이미지에 대해 상이한 판정 각도에서 채점할 경우, 예를 들어, 상이한 평가자가 동일한 생성 이미 지를 채점하는 경우, 평가자 1은 5점으로 채점할 수 있고 평가자 2는 3점으로 채점할 수 있으며, 따라서 모델이 학습 중에 있으면 해당 이미지가 좋은지 아닌지 분별하기 어렵다. 절대적인 점수로 채점하면 판정 표준을 통일 하기가 어려우므로, 실행 가능한 일 실시형태로서, 상대적 배열 방식을 사용하여 결과의 좋고 나쁨을 배열할 수 있는 바, 예를 들어 생성 이미지 A 및 생성 이미지 B에 대해, 평가자 1은 A>B, 즉 평가자 1은 생성 이미지 A가 생성 이미지 B보다 예상에 더 부합된다고 간주하고, 평가자 2도 A>B로 간주하면, 모델은 상대적 배열 방식을 기 반으로 많은 생성 이미지에서 품질이 우수한 이미지와 품질이 낮은 이미지를 더 잘 구분할 수 있다. 보상 모델을 통해 인위적으로 피드백된 데이터를 수집하여 비교 학습 형태로 보상 모델을 트레이닝함으로써 보 상 모델 판별 순서가 사람들의 보편적인 인식에 부합되도록 한다. 단계(S302)에서, 사전 트레이닝된 보상 모델 및 강화 학습 전략을 기반으로 제1 텍스트 이미지 모델의 파라미터 를 조정하여 제2 텍스트 이미지 모델을 획득하되, 여기서 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상은 기설정된 조건을 충족시키며, 누적 보상은 생성 시퀀스의 각 단계의 보 상을 기반으로 획득된 것이다. 강화 학습 전략은 기계학습 행동주의의 생성물이며, 기본 아이디어는 에이전트가 환경과의 지속적인 상호작용을 통해 지능을 획득한다는 것이다. 강화 학습 전략은 환경(State), 주체(Actor), 행동(Action) 및 보상(Reward)을 기반으로 하며, 여기서 환경은 현재 상태이고, 주체는 환경과 상호작용하며 행동을 수행하는 객체이고, 행동은 주체가 수행하는 동작이며, 보상은 주체의 구체적인 행동에 대해 제공되는 피드백이다. 텍스트 이미지를 구현하기 위한 생성 시퀀스에 대응하여, 주체(Actor)는 현재 단계의 텍스트 이미지 모델이고, 환경(State)은 텍스트 이미지 모델에 대응하는 입력 텍스트와 생성된 생성 이미지일 수 있으며, 행동(Action)은 텍스트 이미지 모델에 대응하는 출력 노이즈이고, 보상(Reward)은 인위적 요구에 따라 설계될 수 있으며, 예를 들어 제품에 대한 사용자 피드백에 관심을 가질 경우, 사용자 피드백을 기반으로 보상을 설계할 수 있다. 이 과 정에서, 텍스트 이미지 모델에 의한 노이즈 제거 및 이미지 생성 단계를 강화 학습 궤적으로 사용하여 보상 신 호가 전체 생성 과정을 제어함으로써 모델이 누적 보상이 높은 방향으로 최적화되도록 한다. 하나의 제1 텍스트 이미지 모델을 트레이닝 기초로 사용하여 모델의 순차적 수행 시 발생하는 오류를 효과적으 로 줄일 수 있음으로써 생성 결과의 품질을 향상시킨다. 이러한 방식을 통해 초기 모델(제1 텍스트 이미지모델)로 하여금 각 입력과 이에 대응되는 출력을 더 잘 이해하여 이에 대해 상응한 조작을 수행할 수 있도록 한 다. 실행 가능한 일 실시형태로서, 품질이 비교적 우수한 데이터 쌍을 이용하여 제1 텍스트 이미지 모델을 미세 하게 조정하여 트레이닝함으로써 모델의 전체 성능을 향상시킬 수 있다. 여기서 품질이 비교적 우수한 데이터 쌍은 인위적 표기를 거친 이미지 텍스트 쌍과 같이 별도의 이미지 텍스트 쌍일 수 있다. 아래에 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 방법의 다양한 양태를 추가로 설명한다. 일부 실시예에 따르면, 기설정된 조건은 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스 에서 얻은 누적 보상이 제1 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상보다 높은 것을 포함한다. 텍스트 이미지 모델이 환경(State)-행동(Action)을 여러 번 경험한 후에야만 최종 결과를 가져오는 경우가 많기 때문에, 즉 복수의 입력 텍스트와 생성된 생성 이미지가 존재할 수 있으며, 각 행동마다 보상(Reward)이 있지만, 즉 각 출력 노이즈를 채점하지만 최종적으로 결과에 반영되는 것은 이러한 모든 보상의 총합, 즉 누적 보상이다. 강화 학습 전략 중의 환경(State)에 무수히 많은 상황이 있을 수 있고, 하나의 환경(State)에도 실행 가능한 솔루션이 많이 있을 수 있다. 따라서 환경(State)-행동(Action)-보상(Reward)의 각 주기 후에 모두 파라 미터가 1회 업데이트되면 해당 모델은 매우 \"근시안적\"이 되고 심지어 수렴하기 어려워질 가능성이 높으므로, 상기 모델은 \"현재 상황\"에만 대처할 수 있으며, 반면 무궁무진한 환경(State)에 대처할 수는 없다. 이 때문에 강화 학습 전략의 최종 목표는 어느 한 번의 행동(Action)의 최적화가 아닌 시퀀스(Trajectory)의 최적화이다. 일부 실시예에 따르면, 도 4는 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 방법 중의 강화 학습 전략 모식도를 도시한다. 도 4에 도시된 바와 같이, 강화 학습 전략은 근접 정책 최적화(Proximal Policy Optimization, PPO) 알고리즘을 포함한다. 근접 정책 최적화 알고리즘은 전략 기울기의 개선 알고리즘으로, 기존의 전략 기울기 알고리즘에서 타겟 함수 기울기 및 스텝 길이에 따라 전략 가중치를 업데이트하는데, 이러한 업데이트 과정에는 2개의 흔히 볼 수 있는 문제가 발생할 수 있으며, 즉 오버슈팅(Overshooting) 및 언더슈팅(Undershooting)인데, 여기서 오버슈팅은 업 데이트가 보상 피크를 놓치고 2차 최적화 전략 영역에 포함되는 것을 의미하고, 언더슈팅은 기울기 방향에서 너 무 작은 업데이트 스텝 길이를 취하여 수렴이 느려지는 것을 의미한다. 지도 학습 문제에서 데이터는 고정된 것이므로 오버슈팅은 큰 문제가 아니며 다음 단계(epoch)에서 다시 수정하 면 될 수 있지만, 강화 학습 문제에서 오버슈팅이 비교적 열악한 전략 영역에 포함되면 미래 샘플 배치에서 많 은 의미 있는 정보를 제공할 수 없어 비교적 열악한 데이터 샘플로 다시 전략을 업데이트하게 되는데 이로써 더 열악한 긍정적 피드백에 빠져 복원될 수 없게 된다. 근접 정책 최적화(PPO) 알고리즘은 업데이트때마다 모두 목표 발산 근처의 특정 구간에 위치하기를 기대하면서 목표 발산을 설정하는 방식을 통해 이러한 문제를 해결한다. 여기서 목표 발산은 전략을 크게 변경할 수 있을 만큼 커야 하지만 또한 안정적으로 업데이트할 만큼 작아야 한다. 또한 업데이트할 때마다 근접 정책 최적화 (PPO) 알고리즘은 모두 업데이트 크기를 검사한다. 최종 업데이트된 발산이 목표 발산의 1.5배를 초과하면 다음 반복에서는 손실 계수 β를 배로 늘려 처벌을 더 가중화한다. 반대로, 업데이트가 너무 작으면 손실 계수 β를 절반으로 하여 신뢰 영역을 효과적으로 확장한다. 일부 실시예에 따르면, 근접 정책 최적화(PPO) 알고리즘은 행동 서브 모델 및 판정 서브 모델을 사용 하고, 행동 서브 모델은 제1 텍스트 이미지 모델을 기반으로 초기화된 것이며, 판정 서브 모델 은 사전 트레이닝된 보상 모델을 기반으로 초기화된 것이다. 초기점의 선택은 일정한 정도에서 알고리즘의 수렴 여부를 결정할 수 있고, 수렴 시 초기점은 학습이 얼마나 빨 리 수렴되는지, 비용이 높거나 낮은 점으로 수렴할 수 있는지 여부를 결정할 수 있다. 너무 큰 초기화는 기울기 폭발을 초래하고 너무 작은 초기화는 기울기가 사라지게 된다. 따라서 오프라인 데이터(즉 인간 시범자, 스크립 트 전략 또는 기타 강화 학습 에이전트가 수집한 데이터)를 이용하여 전략을 트레이닝하고 이를 새로운 강화 학 습 전략을 초기화하는데 사용할 수 있다. 이 과정은 새로운 강화 학습 전략이 사전에 트레이닝된 것처럼 보이게 한다. 다음, 상기 전략으로 주체(즉 행동 서브 모델, Actor)-평가(즉 판정 서브 모델, Critic) 네트워크를 초기 화하여 미세 조정하고, 여기서 사전 트레이닝된 제1 텍스트 이미지 모델을 초기 주체(Actor)로 사용하며, 사전 트레이닝된 보상 모델을 초기 평가(Critic)로 사용한다. 선험적 정보를 사용하여 상태 공간의 확률론 적 탐색을 계획적으로 피한다. 이러한 선험적 정보는 에이전트가 환경의 어떤 상태가 좋고 추가로 탐색해야 하 는지 이해하는 데 도움이 된다. 동시에, 보상 모델과 제1 텍스트 이미지 모델을 함께 미세 조정함으로써, 미세 조정된 제2 텍스트 이미지 모델이 보상 모델 요소를 고려하여 세부적인 문제를 피할 수 있도록 한다. 일부 실시예에 따르면, 생성 시퀀스는 적어도 하나의 단계를 포함하되, 여기서 생성 시퀀스의 각 단계에 대해, 행동 서브 모델은 제공된 입력 텍스트를 기반으로 대응하는 출력 노이지 이미지를 생성하고; 판정 서브 모 델은 현재 단계의 상기 입력 텍스트 및 상기 출력 노이지 이미지를 기반으로 상기 현재 단계의 보상을 출 력한다. 예를 들어, 동일한 입력 텍스트 X를 기반으로, 2개의 생성 이미지 Y1 및 Y2를 생성하였으며, 그 중 하나는 제1 텍스트 이미지 모델로부터 획득되고, 다른 하나는 강화 학습 전략을 거친 현재 반복 텍스트 이미지 모델로부터 획득된 것이다. 상기 2개의 모델의 생성 이미지를 비교하여 차이에 대한 보상을 계산하고, 상기 보상은 양수 또 는 음수일 수 있으므로 처벌항으로 간주될 수도 있다. 상기 항은 각 훈련 배치에서 강화 학습 전략이 초기 모델 (즉 제1 텍스트 이미지 모델)을 벗어나는 정도를 보상하거나 처벌함으로써 모델이 합리적으로 생성 이미지를 출 력하도록 보장한다. 이 처벌항을 제거하면 모델이 최적화 중에 횡설수설로 긁어모은 이미지를 생성하여 보상 모 델을 속임으로써 높은 보상값을 제공할 수 있게 된다. 보상은 하나의 함수로 하나의 스칼라를 생성하며, 일부 특정 상태에 있고 특정 동작을 취하는 에이전트의 \"우수 정도\"를 나타낸다. 일부 실시예에 따르면, 현재 단계의 보상은 현재 단계 전 이전 단계에서 행동 서브 모델의 출력과 현재 단 계에서 행동 서브 모델의 출력 사이의 상대 엔트로피를 포함한다. 생성 시퀀스 내에서, 노이지 이미지의 보상은 다음 항의 손실이 있는 Kullback-Leible 발산(즉 KL 발산)일 뿐이 며, KL 발산은 2개의 분포 사이의 차이 정도를 평가할 수 있다. 둘 사이의 차이가 작을수록 KL 발산이 더 작다. 두 분포가 일치한 경우, 이의 KL 발산은 0이다. 따라서, KL 발산을 강화 학습 전략 중의 처벌항으로 사용하여 보상 모델과 사전 트레이닝 모델을 함께 미세 조 정함으로써 생성 모델이 보상 모델 요소를 고려하여 세부적인 문제를 피할 수 있도록 한다. 일부 실시예에 따르면, 현재 단계의 보상은 현재 단계 전 이전 단계의 평가값과 현재 단계의 평가값 사이의 차 이값을 포함할 수 있되, 여기서 평가값은 사전 트레이닝된 보상 모델이 제공된 입력 텍스트 및 대응하는 출력 노이지 이미지를 기반으로 채점하여 획득된 것이다. 생성된 노이지 이미지 자체가 평가될 수 있으므로 보상 모델의 채점을 통해 자체도 보상으로 사용될 수 있으며; 생성 시퀀스 내 각 단계의 보상을 보상 모델의 채점으로 설정할 수 있다. 실행 가능한 일 실시형태로서, 인위적으로 감정된 보상 점수로 보상 모델을 직접 대체할 수도 있다. 이해할 수 있는 것은, 보상 모델은 인위적 요구에 따라 설계될 수 있으며, 예를 들어 제품에 대한 사용자 피드백에 관심을 가질 경우, 사용자 피드백을 기반으로 보상을 설계할 수 있다. 일부 실시예에 따르면, 생성 시퀀스에서 획득된 누적 보상은 총 평점 및 손실항을 포함하되, 총 평점은 생성 시 퀀스의 초기 입력 및 최종 출력을 기반으로 사전 트레이닝된 보상 모델에 의해 획득되고, 손실항은 생성 시퀀스 의 마지막 단계의 보상과 손실 계수의 곱이다. 보상 함수는 다음과 같이 설계될 수 있다."}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, θ는 생성 모델 파라미터이고; score는 입력 텍스트(초기 입력) 및 생성 이미지(최종 출력)에 대한 보 상 모델의 채점이며; at, st는 각각 t 순간의 행동(Action), 환경(State)이고 즉 텍스트 이미지 모델에 대응하 는 출력 노이즈, 입력 텍스트 및 생성된 노이지 이미지이며; πSFT는 사전 트레이닝된 제1 텍스트 이미지 모델 파라미터이고, πθ’는 강화 학습 전략 중 현재 반복 텍스트 이미지 모델 파라미터이며; β는 손실 계수이다. 공식의 앞부분은 양수로, score의 목적은 총 평점을 더 크게 누적하여 예상에 더 부합되도록 하며; 제2항은 처 벌항으로, 트레이닝된 모델이 이전에 이미 조정된 모델을 벗어나지 않도록 하는 것이며, 그렇지 않으면 예상에 부합되지 않는 일부 결과가 발생할 수 있다. 일부 실시예에 따르면, 제2 텍스트 이미지 모델의 생성 시퀀스 중의 누적 보상을 기반으로, 역전파 알고리 즘을 통해 제2 텍스트 이미지 모델의 파라미터를 획득한다. 역전파(Back-propagation, BP) 알고리즘의 등장은 신경망 개발의 획기적인 발전이며 현재 많은 딥러닝 트레이닝 방법의 기초이기도 하다. 상기 방법은 신경망의 각 파라미터에 대한 손실 함수의 기울기를 계산하고 최적화 방 법을 결합하여 파라미터를 업데이트한 후 손실 함수를 감소시킨다. 보상 함수는 양의 손실 함수로 간주할 수 있 으며, 보상 신호가 전체 생성 과정을 제어함으로써 모델이 누적 보상이 높은 방향으로 최적화되도록 한다. 일부 실시예에 따르면, 도 5는 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 방법의 흐름도를 도시 한다. 도 5에 도시된 바와 같이, 텍스트 이미지 모델의 트레이닝 방법은 하기와 같은 단계를 포함한다. 단계(S501)에서, 제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하되, 여기서 제1 텍스트 이미지 모델은 입력 텍스트를 기반으로 대응하는 생성 이미지를 생성하는 데 사용되고, 사전 트레이닝된 보상 모델은 입력 텍스트 및 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점하는 데 사용된다. 단계(S502)에서, 사전 트레이닝된 보상 모델 및 강화 학습 전략을 기반으로 제1 텍스트 이미지 모델의 파라미터 를 조정하여 제2 텍스트 이미지 모델을 획득하되, 여기서 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상은 기설정된 조건을 충족시키며, 누적 보상은 생성 시퀀스의 각 단계의 보 상을 기반으로 획득된 것이다. 단계(S501) 이전에 피드백 데이터 세트를 기반으로 보상 모델을 트레이닝하는 단계(S503)를 더 포함한다. 실행 가능한 일 실시형태로서, 사전 트레이닝된 보상 모델은 피드백 데이터 세트를 기반으로 트레이닝하여 획득 된 것이고, 여기서 피드백 데이터 세트는 복수의 피드백 데이터를 포함하며, 복수의 피드백 데이터는 입력 텍스 트 및 대응하는 생성 이미지로 구성된 데이터 쌍 및 데이터 쌍에 대응하는 피드백 상태를 포함하고, 피드백 상 태는 동일한 입력 텍스트에 대해 생성된 대응하는 생성 이미지가 긍정적 피드백 또는 부정적 피드백에 속하는 것을 특성화하는 데 사용된다. 즉 피드백 데이터 세트 중의 피드백 데이터는 실제로 \"입력-출력-평가\"의 3원 그 룹이며, 여기서 피드백 상태는 일반적으로 인간의 피드백을 기반으로 제공된다. 입력 텍스트 x를 기반으로 4장의 배열된 생성 이미지 A, B, C 및 D가 있다고 가정하면, 인간 피드백을 기반으로 A > B > C >D로 정렬된다. 여기서, 입력 텍스트 x에 대해, 인간의 보편적인 인식에 따르면 이미지 A는 이미지 B 보다 품질이 높다. 주어진 배열로 보상 모델을 트레이닝할 경우, 더 앞에 배열된 데이터일수록 긍정적 피드백 (품질이 우수한 이미지)에 더 편향하고, 더 뒤로 배열된 데이터일수록 부정적 피드백(품질이 낮은 이미지)에 더 편향한다. 일부 실시예에 따르면, 상기 보상 모델을 트레이닝하는 단계는, 복수의 피드백 데이터를 기반으로, 비교 학습 형태로 보상 모델을 트레이닝하여 보상 모델이 피드백 상태가 긍정적 피드백인 데이터 쌍에 제1 보상 평점을 출 력하고, 피드백 상태가 부정적 피드백인 데이터 쌍에 제2 보상 평점을 출력하도록 하되, 제1 보상 평점과 제2 보상 평점의 차이값은 대응하는 생성 이미지의 품질 차이를 특성화하는 데 사용되는 단계를 포함한다. 입력 텍스트 x를 기반으로 4장의 배열된 생성 이미지 A, B, C 및 D에서, 인간 피드백에 의하면 A > B > C >D로 정렬된다. 4장의 생성 이미지에 대한 보상 모델의 채점은 r(A) > r(B) > r(C) > r(D)를 만족해야 하므로, 보상 모델의 손실 함수는 다음과 같다."}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, θ는 보상 모델 파라미터이고; x는 입력 텍스트이며; yw, yl은 각각 품질이 비교적 우수한 이미지, 품질 이 비교적 낮은 이미지이고; DRM은 보상 모델에서 사용하는 데이터 세트이며; r은 보상 모델이고 출력은 스칼라 이며, 이는 입력 텍스트, 출력 이미지에 대한 모델의 보상 점수를 의미한다. 차이값을 더 잘 정규화하기 위해, 각 두 항 차이값에 대해 모두 sigmoid 함수 σ를 사용하여 차이값을 0과 1 사이로 당길 수 있다. 피드백 데이터 세트의 데이터는 기본적으로 내림차순으로 정렬되어 있으므로 이전 항목과 이후 항목 간의 점수 차이를 순회하여 합산하면 된다. 실행 가능한 일 실시형태로서, 단계 503을 여러차례 수행하여 보상 모델의 더 나은 최적화 효과를 달성할 수 있 다. 일부 실시예에 따르면, 피드백 데이터 세트는 상이한 소스로부터의 적어도 2가지 복수의 피드백 데이터를 포함 한다. 피드백 데이터 세트는 상이한 원천의 다양한 피드백 데이터를 포함할 수 있다. 더 많은 데이터 소스를 도입하고 사용자 피드백, 인위적 표기 등 다각도에서 데이터를 수집한다. 다양한 원천의 피드백 데이터에 대해, 최적화된 텍스트 이미지 모델은 수, 속성 및 배경에 초점을 맞추는 것 외에도 다중 엔티티 조합 및 회화 스타일 과 같은 정렬 요소를 고려할 수 있다. 일부 실시예에 따르면, 복수의 피드백 데이터는 사용자 피드백 데이터, 인위적 표기 데이터 및 수동 비교 데이 터 중 적어도 2가지를 포함하되, 여기서 사용자 피드백 데이터는 사용자 행동을 기반으로 상기 피드백 상태를 획득하고; 인위적 표기 데이터는 인위적 표기 결과를 기반으로 상기 피드백 상태를 획득하며; 수동 비교 데이터 는 상이한 버전의 생성 이미지를 기반으로 상기 피드백 상태를 획득한다. DRM은 보상 모델에서 사용하는 데이터 세트로, 사용자 피드백, 인위적 표기 및 수동 비교 세 부분을 포함한다. 여기서 사용자 피드백은 일반적으로 제품 형태와 관련이 있는데, 예를 들어 사용자가 좋아요를 누를 수 있는 데 이터를 좋아하거나, 분할하거나, 이러한 행동을 확대하거나 논평할 있으며, 사용자의 이러한 행동 판정을 통해 회화 스타일을 고려할 수 있고; 인위적 표기는 일반적으로 모두 전문적인 표기자가 있을 수 있으며, 좋은 이미 지와 나쁜 이미지를 표기하여 장단점을 구분하고; 수동 비교는 상이한 버전의 텍스트 이미지 모델이 동일한 입 력 텍스트 및 생성 이미지의 데이터 쌍을 비교하는 것이므로 엔티티 조합을 개선할 수 있다. 일부 실시예에 따르면, 도 6은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 방법의 흐름도를 도시 한다. 도 6에 도시된 바와 같이, 텍스트 이미지 모델의 트레이닝 방법은 하기와 같은 단계를 포함한다. 단계(S601)에서, 제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하되, 여기서 제1 텍스트 이미지 모델은 입력 텍스트를 기반으로 대응하는 생성 이미지를 생성하는 데 사용되고, 사전 트레이닝된 보상 모델은 입력 텍스트 및 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점하는 데 사용된다. 단계(S602)에서, 사전 트레이닝된 보상 모델 및 강화 학습 전략을 기반으로 제1 텍스트 이미지 모델의 파라미터 를 조정하여 제2 텍스트 이미지 모델을 획득하되, 여기서 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상은 기설정된 조건을 충족시키며, 누적 보상은 생성 시퀀스의 각 단계의 보 상을 기반으로 획득된 것이다. 단계(S601) 이전에 인위적 표기 이미지 텍스트 쌍을 트레이닝할 제1 텍스트 이미지 모델의 트레이닝 샘플로서 획득하는 단계(S603); 및 역전파 알고리즘을 기반으로 트레이닝할 제1 텍스트 이미지 모델의 파라미터를 업데이 트하여 지도 트레이닝을 거친 제1 텍스트 이미지 모델을 획득하는 단계(S604)를 더 포함한다. 사전 트레이닝된 텍스트 이미지 모델 미세 조정(Supervised Fine-Tuning, SFT) 방식을 통해 제1 텍스트 이미지 모델을 획득할 수 있고, 사전 트레이닝된 텍스트 이미지 모델 미세 조정(STF)을 통해 미세 조정할 경우, 표준적 인 지도 학습 방법을 사용할 수 있으며, 즉 인위적으로 표기된(입력, 출력) 텍스트 쌍을 트레이닝 샘플로 사용 하고, 역전파 알고리즘을 사용하여 모델 파라미터를 업데이트한다. 이러한 방식을 통해, 모델로 하여금 각 입력 과 이에 대응되는 출력을 더 잘 이해하여 이에 대해 상응한 조작을 수행할 수 있도록 한다. 이 밖에, 사전 트레 이닝된 텍스트 이미지 모델 미세 조정(STF)은 또한 모델의 순차적 수행 시 발생하는 오류를 효과적으로 줄일 수 있음으로써 생성 결과의 품질을 향상시킨다. 일부 실시예에 따르면, 본 발명은 텍스트 이미지 모델을 더 제공하며, 상기 텍스트 이미지 모델은 전술한 실시 예에서 제공하는 텍스트 이미지 모델의 트레이닝 방법에 의해 트레이닝된 것이다. 도 7은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 장치의 구조 블록도를 도시한다. 도 7에 도시된 바와 같이, 텍스트 이미지 모델의 트레이닝 장치는 획득 모듈 및 조정 모듈을 포함한다. 획득 모듈은 제1 텍스트 이미지 모델 및 사전 트레이닝된 보상 모델을 획득하도록 구성되되, 상기 제1 텍 스트 이미지 모델은 입력 텍스트를 기반으로 대응하는 생성 이미지를 생성하고, 상기 사전 트레이닝된 보상 모 델은 상기 입력 텍스트 및 상기 대응하는 생성 이미지로 구성된 데이터 쌍을 기반으로 채점한다. 동일한 입력 텍스트는 복수의 출력 결과(생성 이미지)를 생성하므로 하나의 보상 모델(Reward Model, RM)을 사 용하여 생성된 이미지를 채점한 후 보상 신호를 생성해야 하며, 보상 신호는 생성된 이미지를 인간의 시각에서 정렬하거나 평가한다. 보상 모델을 통해 인위적으로 피드백된 데이터를 수집하여 비교 학습 형태로 보상 모델을 트레이닝함으로써 보 상 모델 판별 순서가 사람들의 보편적인 인식에 부합되도록 한다. 조정 모듈은 상기 사전 트레이닝된 보상 모델 및 강화 학습 전략을 기반으로 상기 제1 텍스트 이미지 모델 의 파라미터를 조정하여 제2 텍스트 이미지 모델을 획득하도록 구성되되, 여기서 상기 제2 텍스트 이미지 모델 이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상은 기설정된 조건을 충족시키며, 상기 누적 보상은 상기 생성 시퀀스의 각 항의 보상의 합을 기반으로 획득된 것이다. 텍스트 이미지 모델에 의한 노이즈 제거 및 이미지 생성 단계를 강화 학습 궤적으로 사용하여 보상 신호가 전체 생성 과정을 제어함으로써 모델이 누적 보상이 높은 방향으로 최적화되도록 한다. 하나의 제1 텍스트 이미지 모델을 트레이닝 기초로 사용하여 모델의 순차적 수행 시 발생하는 오류를 효과적으 로 줄일 수 있음으로써 생성 결과의 품질을 향상시킨다. 이러한 방식을 통해 초기 모델(제1 텍스트 이미지 모델)로 하여금 각 입력과 이에 대응되는 출력을 더 잘 이해하여 이에 대해 상응한 조작을 수행할 수 있도록 한 다. 실행 가능한 일 실시형태로서, 품질이 비교적 우수한 데이터 쌍을 이용하여 제1 텍스트 이미지 모델을 미세 하게 조정하여 트레이닝함으로써 모델의 전체 성능을 향상시킬 수 있다. 여기서 품질이 비교적 우수한 데이터 쌍은 인위적 표기를 거친 이미지 텍스트 쌍과 같이 별도의 이미지 텍스트 쌍일 수 있다. 일부 실시예에 따르면, 기설정된 조건은 제2 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스 에서 얻은 누적 보상이 제1 텍스트 이미지 모델이 텍스트 이미지를 구현하기 위한 생성 시퀀스에서 얻은 누적 보상보다 높은 것을 포함한다. 텍스트 이미지 모델이 환경(State)-행동(Action)을 여러 번 경험한 후에야만 최종 결과를 가져오는 경우가 많기 때문에, 즉 복수의 입력 텍스트와 생성된 생성 이미지가 존재할 수 있으며, 각 행동마다 보상(Reward)이 있지만, 즉 각 출력 노이즈를 채점하지만 최종적으로 결과에 반영되는 것은 이러한 모든 보상의 총합, 즉 누적 보상이다. 강화 학습 전략 중의 환경(State)에 무수히 많은 상황이 있을 수 있고, 하나의 환경(State)에도 실행 가능한 솔루션이 많이 있을 수 있다. 따라서 환경(State)-행동(Action)-보상(Reward)의 각 주기 후에 모두 파라 미터가 1회 업데이트되면 해당 모델은 매우 \"근시안적\"이 되고 심지어 수렴하기 어려워질 가능성이 높으므로, 상기 모델은 \"현재 상황\"에만 대처할 수 있으며, 반면 무궁무진한 환경(State)에 대처할 수는 없다. 이 때문에 강화 학습 전략의 최종 목표는 어느 한 번의 행동(Action)의 최적화가 아닌 시퀀스(Trajectory)의 최적화이다. 일부 실시예에 따르면, 강화 학습 전략은 근접 정책 최적화(Proximal Policy Optimization, PPO) 알고리즘을 포함한다. 근접 정책 최적화 알고리즘은 전략 기울기의 개선 알고리즘으로, 기존의 전략 기울기 알고리즘에서 타겟 함수 기울기 및 스텝 길이에 따라 전략 가중치를 업데이트하는데, 이러한 업데이트 과정에는 2개의 흔히 볼 수 있는 문제가 발생할 수 있으며, 즉 오버슈팅(Overshooting) 및 언더슈팅(Undershooting)인데, 여기서 오버슈팅은 업 데이트가 보상 피크를 놓치고 2차 최적화 전략 영역에 포함되는 것을 의미하고, 언더슈팅은 기울기 방향에서 너 무 작은 업데이트 스텝 길이를 취하여 수렴이 느려지는 것을 의미한다. 지도 학습 문제에서 데이터는 고정된 것이므로 오버슈팅은 큰 문제가 아니며 다음 단계(epoch)에서 다시 수정하 면 될 수 있지만, 강화 학습 문제에서 오버슈팅이 비교적 열악한 전략 영역에 포함되면 미래 샘플 배치에서 많 은 의미 있는 정보를 제공할 수 없어 비교적 열악한 데이터 샘플로 다시 전략을 업데이트하게 되는데 이로써 더 열악한 긍정적 피드백에 빠져 복원될 수 없게 된다. 근접 정책 최적화(PPO) 알고리즘은 업데이트때마다 모두 목표 발산 근처의 특정 구간에 위치하기를 기대하면서 목표 발산을 설정하는 방식을 통해 이러한 문제를 해결한다. 여기서 목표 발산은 전략을 크게 변경할 수 있을 만큼 커야 하지만 또한 안정적으로 업데이트할 만큼 작아야 한다. 또한 업데이트할 때마다 근접 정책 최적화 (PPO) 알고리즘은 모두 업데이트 크기를 검사한다. 최종 업데이트된 발산이 목표 발산의 1.5배를 초과하면 다음 반복에서는 손실 계수 β를 배로 늘려 처벌을 더 가중화한다. 반대로, 업데이트가 너무 작으면 손실 계수 β를 절반으로 하여 신뢰 영역을 효과적으로 확장한다. 일부 실시예에 따르면, 근접 정책 최적화 알고리즘은 행동 서브 모델 및 판정 서브 모델을 사용한다. 도 8은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 장치의 구조 블록도를 도시한다. 도 8에 도시된 바 와 같이, 상기 조정 모듈은, 상기 제1 텍스트 이미지 모델을 기반으로 상기 행동 서브 모델을 초기화하도록 구성되는 행동 서브 모듈; 및 상기 사전 트레이닝된 보상 모듈을 기반으로 상기 판정 서브 모델을 초기화하도록 구성되는 판정 서브 모듈 을 포함한다.초기점의 선택은 일정한 정도에서 알고리즘의 수렴 여부를 결정할 수 있고, 수렴 시 초기점은 학습이 얼마나 빨 리 수렴되는지, 비용이 높거나 낮은 점으로 수렴할 수 있는지 여부를 결정할 수 있다. 너무 큰 초기화는 기울기 폭발을 초래하고 너무 작은 초기화는 기울기가 사라지게 된다. 따라서 오프라인 데이터(즉 인간 시범자, 스크립 트 전략 또는 기타 강화 학습 에이전트가 수집한 데이터)를 이용하여 전략을 트레이닝하고 이를 새로운 강화 학 습 전략을 초기화하는데 사용할 수 있다. 이 과정은 새로운 강화 학습 전략이 사전에 트레이닝된 것처럼 보이게 한다. 다음, 상기 전략으로 주체(즉 행동 서브 모델, Actor)-평가(즉 판정 서브 모델, Critic) 네트워크를 초기 화하여 미세 조정하고, 여기서 사전 트레이닝된 제1 텍스트 이미지 모델을 초기 주체(Actor)로 사용하며, 사전 트레이닝된 보상 모델을 초기 평가(Critic)로 사용한다. 선험적 정보를 사용하여 상태 공간의 확률론적 탐색을 계획적으로 피한다. 이러한 선험적 정보는 에이전트가 환경의 어떤 상태가 좋고 추가로 탐색해야 하는지 이해하 는 데 도움이 된다. 동시에, 보상 모델과 제1 텍스트 이미지 모델을 함께 미세 조정함으로써, 미세 조정된 텍스 트 이미지 모델이 보상 모델 요소를 고려하여 세부적인 문제를 피할 수 있도록 한다. 조정 모듈은 전술한 실시예의 조정 모듈과 동일하므로 여기서 반복 서술하지 않는다. 일부 실시예에 따르면, 생성 시퀀스는 적어도 하나의 단계를 포함하되, 여기서 생성 시퀀스의 각 단계에 대해, 행동 서브 모듈은 또한 제공된 입력 텍스트를 기반으로 대응하는 출력 노이지 이미지를 생성하도록 구성 되고; 판정 서브 모듈은 또한 현재 단계의 입력 텍스트 및 출력 노이지 이미지를 기반으로 현재 단계의 보상을 출력하도록 구성된다. 예를 들어, 동일한 입력 텍스트 X를 기반으로, 2개의 생성 이미지 Y1 및 Y2를 생성하였으며, 그 중 하나는 제1 텍스트 이미지 모델로부터 획득되고, 다른 하나는 강화 학습 전략을 거친 현재 반복 텍스트 이미지 모델로부터 획득된 것이다. 상기 2개의 모델의 생성 이미지를 비교하여 차이에 대한 보상을 계산하고, 상기 보상은 양수 또 는 음수일 수 있으므로 처벌항으로 간주될 수도 있다. 상기 항은 각 훈련 배치에서 강화 학습 전략이 초기 모델 (즉 제1 텍스트 이미지 모델)을 벗어나는 정도를 보상하거나 처벌함으로써 모델이 합리적으로 생성 이미지를 출 력하도록 보장한다. 이 처벌항을 제거하면 모델이 최적화 중에 횡설수설로 긁어모은 이미지를 생성하여 보상 모 델을 속임으로써 높은 보상값을 제공할 수 있게 된다. 보상은 하나의 함수로 하나의 스칼라를 생성하며, 일부 특정 상태에 있고 특정 동작을 취하는 에이전트의 \"우수 정도\"를 나타낸다. 일부 실시예에 따르면, 도 9는 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 장치의 구조 블록 도를 도시한다. 도 9에 도시된 바와 같이, 조정 모듈은, 총 평점 및 손실항을 기반으로 생성 시퀀스 중의 누적 보상을 생성하도록 구성되되, 총 평점은 생성 시퀀스의 초기 입력 및 최종 출력을 기반으로 사전 트레이닝된 보상 모델에 의해 획득되고; 손실항은 생성 시퀀스의 마지 막 단계의 보상과 손실 계수의 곱인 보상 서브 모듈을 더 포함한다. 보상 함수는 다음과 같이 설계될 수 있다."}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, θ는 생성 모델 파라미터이고; score는 입력 텍스트(초기 입력) 및 생성 이미지(최종 출력)에 대한 보 상 모델의 채점이며; at, st는 각각 t 순간의 행동(Action), 환경(State)이고 즉 텍스트 이미지 모델에 대응하 는 출력 노이즈, 입력 텍스트 및 생성된 노이지 이미지이며; πSFT는 사전 트레이닝된 제1 텍스트 이미지 모델 파라미터이고, πθ’는 강화 학습 전략 중 현재 반복 텍스트 이미지 모델 파라미터이며; β는 손실 계수이다. 공식의 앞부분은 양수로, score의 목적은 총 평점을 더 크게 누적하여 예상에 더 부합되도록 하며; 제2항은 처 벌항으로, 트레이닝된 모델이 이전에 이미 조정된 모델을 벗어나지 않도록 하는 것이며, 그렇지 않으면 예상에 부합되지 않는 일부 결과가 발생할 수 있다. 획득 모듈은 전술한 실시예의 획득 모듈과 동일하므로 여기서 반복 서술하지 않는다. 일부 실시예에 따르면, 도 10은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 장치의 구조 블 록도를 도시한다. 도 10에 도시된 바와 같이, 텍스트 이미지 모델의 트레이닝 장치는,피드백 데이터 세트를 기반으로 보상 모델을 트레이닝하여 사전 트레이닝된 보상 모델을 획득하도록 구성되되, 피드백 데이터 세트는 복수의 피드백 데이터를 포함하며, 복수의 피드백 데이터는 입력 텍스트 및 대응하는 생 성 이미지로 구성된 데이터 쌍 및 데이터 쌍에 대응하는 피드백 상태를 포함하고, 피드백 상태는 동일한 입력 텍스트에 대해 생성된 대응하는 생성 이미지가 긍정적 피드백 또는 부정적 피드백에 속하는 것을 특성화하는 데 사용되는 제1 사전 트레이닝 모듈을 더 포함한다. 사전 트레이닝된 보상 모델은 피드백 데이터 세트를 기반으로 트레이닝하여 획득된 것이고, 피드백 데이터 세트 는 복수의 피드백 데이터를 포함하며, 복수의 피드백 데이터는 입력 텍스트 및 대응하는 생성 이미지로 구성된 데이터 쌍 및 데이터 쌍에 대응하는 피드백 상태를 포함하고, 피드백 상태는 동일한 입력 텍스트에 대해 생성된 대응하는 생성 이미지가 긍정적 피드백 또는 부정적 피드백에 속하는 것을 특성화하는 데 사용된다. 즉 피드백 데이터 세트 중의 피드백 데이터는 실제로 \"입력-출력-평가\"의 3원 그룹이며, 여기서 피드백 상태는 일반적으로 인간의 피드백을 기반으로 제공된다. 입력 텍스트 x를 기반으로 4장의 배열된 생성 이미지 A, B, C 및 D가 있다고 가정하면, 인간 피드백을 기반으로 A > B > C >D로 정렬된다. 여기서, 입력 텍스트 x에 대해, 인간의 보편적인 인식에 따르면 이미지 A는 이미지 B 보다 품질이 높다. 주어진 배열로 보상 모델을 트레이닝할 경우, 더 앞에 배열된 데이터일수록 긍정적 피드백 (품질이 우수한 이미지)에 더 편향하고, 더 뒤로 배열된 데이터일수록 부정적 피드백(품질이 낮은 이미지)에 더 편향한다. 입력 텍스트 x를 기반으로 4장의 배열된 생성 이미지 A, B, C 및 D에서, 인간 피드백에 의하면 A > B > C >D로 정렬된다. 4장의 생성 이미지에 대한 보상 모델의 채점은 r(A) > r(B) > r(C) > r(D)를 만족해야 하므로, 보상 모델의 손실 함수는 다음과 같다."}
{"patent_id": "10-2024-0078785", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, θ는 보상 모델 파라미터이고; x는 입력 텍스트이며; yw, yl은 각각 품질이 비교적 우수한 이미지, 품질 이 비교적 낮은 이미지이고; DRM은 보상 모델에서 사용하는 데이터 세트이며; r은 보상 모델이고 출력은 스칼라 이며, 이는 입력 텍스트, 출력 이미지에 대한 모델의 보상 점수를 의미한다. 차이값을 더 잘 정규화하기 위해, 각 두 항 차이값에 대해 모두 sigmoid 함수 σ를 사용하여 차이값을 0과 1 사이로 당길 수 있다. 피드백 데이터 세트의 데이터는 기본적으로 내림차순으로 정렬되어 있으므로 이전 항목과 이후 항목 간의 점수 차이를 순회하여 합산하면 된다. 획득 모듈 및 조정 모듈은 전술한 실시예의 획득 모듈 및 조정 모듈과 동일하므로 여기서 반복 서 술하지 않는다. 일부 실시예에 따르면, 도 11은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 장치의 구조 블 록도를 도시한다. 도 11에 도시된 바와 같이, 텍스트 이미지 모델의 트레이닝 장치는, 인위적 표기 이미지 텍스트 쌍을 기반으로 트레이닝할 상기 제1 텍스트 이미지 모델을 트레이닝하여 지도 트레 이닝을 거친 상기 제1 텍스트 이미지 모델을 획득하도록 구성되는 제2 사전 트레이닝 모듈을 더 포함한다. 사전 트레이닝된 텍스트 이미지 모델 미세 조정(Supervised Fine-Tuning, SFT) 방식을 통해 제1 텍스트 이미지 모델을 획득할 수 있고, 사전 트레이닝된 텍스트 이미지 모델 미세 조정(STF)을 통해 미세 조정할 경우, 표준적 인 지도 학습 방법을 사용할 수 있으며, 즉 인위적으로 표기된(입력, 출력) 텍스트 쌍을 트레이닝 샘플로 사용 하고, 역전파 알고리즘을 사용하여 모델 파라미터를 업데이트한다. 이러한 방식을 통해, 모델로 하여금 각 입력 과 이에 대응되는 출력을 더 잘 이해하여 이에 대해 상응한 조작을 수행할 수 있도록 한다. 이 밖에, 사전 트레 이닝된 텍스트 이미지 모델 미세 조정(STF)은 또한 모델의 순차적 수행 시 발생하는 오류를 효과적으로 줄일 수 있음으로써 생성 결과의 품질을 향상시킨다. 획득 모듈 및 조정 모듈은 전술한 실시예의 획득 모듈 및 조정 모듈과 동일하므로 여기서 반복 서 술하지 않는다. 본 발명의 실시예에 따르면, 전자 기기, 판독 가능 저장 매체 및 컴퓨터 프로그램 제품을 더 제공한다. 도 12를 참조하면, 본 발명의 각 양태의 하드웨어 기기에 적용될 수 있는 예시로서, 본 발명의 서버 또는 클라 이언트로 사용될 수 있는 전자 기기의 구조 블록도를 설명한다. 전자 기기는 랩톱 컴퓨터, 데스크톱 컴퓨 터, 운영 플랫폼, 개인 정보 단말기, 서버, 블레이드 서버, 대형 컴퓨터, 및 다른 적합한 컴퓨터와 같은 다양한 형태의 디지털 전자의 컴퓨터 기기를 의미한다. 전자 기기는 개인 디지털 처리, 셀룰러폰, 스마트폰, 웨어러블 기기 및 다른 유사한 컴퓨팅 장치와 같은 다양한 형태의 모바일 장치를 의미할 수도 있다. 본문에 표시된 부재, 이들의 연결과 관계, 및 이들의 기능은 단지 예시적인 것으로, 본문에 설명되거나 및/또는 요구된 본 발명의 구 현을 한정하지 않는다. 도 12에 도시된 바와 같이, 전자 기기는 판독 전용 메모리(ROM)에 저장된 컴퓨터 프로그램 또는 저 장 유닛으로부터 랜덤 액세스 메모리(RAM)에 로딩된 컴퓨터 프로그램에 따라 다양하고 적절한 동작 및 처리를 수행할 수 있는 컴퓨팅 유닛을 포함한다. RAM에는 또한 전자 기기의 조작에 필요 한 다양한 프로그램 및 데이터가 저장될 수 있다. 컴퓨팅 유닛, ROM 및 RAM은 버스를 통해 서로 연결된다. 입/출력(I/O) 인터페이스 역시 버스에 연결된다. 입력 유닛, 출력 유닛, 저장 유닛 및 통신 유닛을 포함하는 전자 기기 중의 복 수의 부재는 I/O 인터페이스 연결된다. 입력 유닛은 전자 기기에 정보를 입력할 수 있는 임 의의 유형의 기기일 수 있으며, 입력 유닛은 입력된 숫자 또는 문자 정보를 수신할 수 있고, 전자 기기의 사용자 설정 및 기능 제어와 관련된 키 신호 입력을 발생할 수 있으며, 마우스, 키보드, 터치스크린, 트랙패드, 트랙볼, 조이스틱, 마이크 및/또는 리모컨을 포함할 수 있지만 이에 한정되지 않는다. 출력 유닛은 정보 를 나타낼 수 있는 임의의 유형의 기기일 수 있으며, 디스플레이, 스피커, 비디오/오디오 출력 단말기, 진동기 및/또는 프린터를 포함할 수 있지만 이에 한정되지 않는다. 저장 유닛은 자기 디스크 및 광 디스크를 포 함할 수 있지만 이에 한정되지 않는다. 통신 유닛은 전자 기기가 인터넷 등의 컴퓨터 네트워크 및/ 또는 다양한 통신망을 통해 다른 기기와 정보/데이터를 교환하도록 허용하며, 모뎀, 네트워크 카드, 적외선 통 신 기기, 무선 통신 트랜시버 및/또는 블루투스 기기, 802.11 기기, WiFi 기기, WiMax 기기, 셀룰러 통신 기기 및/또는 유사물과 같은 칩셋을 포함할 수 있지만 이에 한정되지 않는다. 컴퓨팅 유닛은 처리 및 컴퓨팅 기능을 갖는 일반 및/또는 전용 처리 구성요소일 수 있다. 컴퓨팅 유닛 의 일 예는 중앙처리유닛(CPU), 그래픽 처리 유닛(GPU), 다양한 전용 인공 지능(AI) 컴퓨팅 칩, 기계 학 습 모델 알고리즘을 실행하는 다양한 컴퓨팅 유닛, 디지털 신호 처리 프로세서(DSP) 및 적절한 프로세서, 컨트 롤러, 마이크로컨트롤러 등을 포함하지만 이에 한정되지 않는다. 컴퓨팅 유닛은 전술한 실시예에서 제공 하는 텍스트 이미지 모델의 트레이닝 방법과 같이 상술한 다양한 방법 및 처리를 수행한다. 예를 들어, 일부 실 시예에서, 전술한 실시예에서 제공하는 텍스트 이미지 모델의 트레이닝 방법은 저장 유닛과 같은 기계 판 독 가능 매체에 유형적으로 포함되는 컴퓨터 소프트웨어 프로그램으로 구현될 수 있다. 일부 실시예에서, 컴퓨 터 프로그램의 일부 또는 전부는 ROM 및/또는 통신 유닛을 통해 전자 기기에 로드되거나 및/ 또는 설치될 수 있다. 컴퓨터 프로그램이 RAM에 로딩되어 컴퓨팅 유닛에 의해 실행될 경우, 전술한 실시예에서 제공하는 텍스트 이미지 모델의 트레이닝 방법 중 하나 이상의 단계를 수행할 수 있다. 대안적으로, 다른 실시예에서, 컴퓨팅 유닛은 임의의 다른 적절한 방식(예를 들어, 펌웨어에 의해)으로 전술한 실시예 에서 제공하는 텍스트 이미지 모델의 트레이닝 방법을 수행하도록 구성될 수 있다. 본문의 이상에서 설명된 시스템 및 기술의 다양한 실시형태는 디지털 전자 회로 시스템, 집적 회로 시스템, 현 장 프로그래머블 게이트 어레이(FPGA), 전용 집적 회로(ASIC), 특수표준제품(ASSP), 시스템 온 칩(SOC), 복합 프로그래머블 논리 기기(CPLD), 컴퓨터 하드웨어, 펌웨어, 소프트웨어, 및/또는 이들의 조합에서 구현될 수 있 다. 이러한 다양한 실시형태는 하나 이상의 컴퓨터 프로그램에서의 구현을 포함할 수 있고, 상기 하나 이상의 컴퓨터 프로그램은 적어도 하나의 프로그래머블 프로세서를 포함하는 프로그래머블 시스템에서 실행되거나 및/ 또는 해석될 수 있으며, 상기 프로그래머블 프로세서는 전용 또는 범용 프로그래머블 프로세서일 수 있고, 저장 시스템, 적어도 하나의 입력 장치, 및 적어도 하나의 출력 장치로부터 데이터 및 명령을 수신할 수 있으며, 데 이터 및 명령을 상기 저장 시스템, 상기 적어도 하나의 입력 장치, 및 상기 적어도 하나의 출력 장치에 전송할 수 있다. 본 발명의 방법을 구현하는 프로그램 코드는 하나 이상의 프로그래밍 언어의 임의의 조합으로 편집할 수 있다. 이러한 프로그램 코드는 범용 컴퓨터, 전용 컴퓨터 또는 다른 프로그래머블 데이터 처리 장치의 프로세서 또는 컨트롤러에 제공되어, 프로그램 코드가 프로세서 또는 컨트롤러에 의해 실행될 경우, 흐름도 및/또는 블록도에 지정된 기능/동작이 구현될 수 있도록 한다. 프로그램 코드는 완전히 기계에서 실행되거나, 부분적으로 기계에 서 실행되거나, 독립형 소프트웨어 패키지로서 일부는 기계에서 실행되며, 일부는 원격 기계에서 실행되거나 완전히 원격 기계 또는 서버에서 실행될 수 있다. 본 발명의 컨텍스트에서, 기계 판독 가능 매체는 명령 실행 시스템, 장치 또는 기기에 의해 또는 명령 실행 시 스템, 장치 또는 기기와 결합하여 사용하기 위한 프로그램을 포함하거나 저장할 수 있는 유형 매체일 수 있다. 기계 판독 가능 매체는 기계 판독 가능 신호 매체 또는 기계 판독 가능 저장 매체일 수 있다. 기계 판독 가능 매체는 전자, 자기, 광학, 전자기, 적외선 또는 반도체 시스템, 장치 또는 기기, 또는 상기 내용의 임의의 적절 한 조합을 포함할 수 있지만 이에 한정되지 않는다. 기계 판독 가능 저장 매체의 보다 구체적인 예는 하나 이상 의 와이어에 기반한 전기 연결, 휴대용 컴퓨터 디스크, 하드 디스크, 랜덤 액세스 메모리(RAM), 판독 전용 메모 리(ROM), 소거 가능 프로그래머블 판독 전용 메모리(EPROM 또는 플래시 메모리), 광섬유, 휴대용 컴팩트 디스크 판독 전용 메모리(CD-ROM), 광학 저장 기기, 자기 저장 기기 또는 상술한 내용의 임의의 적절한 조합을 포함한 다. 사용자와의 인터랙션을 제공하기 위하여, 컴퓨터에서 여기서 설명된 시스템 및 기술을 구현할 수 있고, 상기 컴 퓨터는 사용자에게 정보를 표시하기 위한 표시 장치(예를 들어, CRT(음극선관) 또는 LCD(액정 표시 장치) 모니 터); 및 키보드 및 지향 장치(예를 들어, 마우스 또는 트랙 볼)를 구비하며, 사용자는 상기 키보드 및 상기 지 향 장치를 통해 컴퓨터에 입력을 제공한다. 다른 유형의 장치는 또한 사용자와의 인터랙션을 제공할 수 있는데, 예를 들어, 사용자에게 제공된 피드백은 임의의 형태의 센싱 피드백(예를 들어, 시각 피드백, 청각 피드백, 또 는 촉각 피드백)일 수 있고; 임의의 형태(소리 입력, 음성 입력, 또는 촉각 입력)로 사용자로부터의 입력을 수 신할 수 있다. 여기서 설명된 시스템 및 기술은 백그라운드 부재를 포함하는 컴퓨팅 시스템(예를 들어, 데이터 서버로 사용 됨), 또는 미들웨어 부재를 포함하는 컴퓨팅 시스템(예를 들어, 응용 서버), 또는 프론트 엔드 부재를 포함하는 컴퓨팅 시스템(예를 들어, 그래픽 사용자 인터페이스 또는 웹 브라우저를 구비하는 사용자 컴퓨터이고, 사용자 는 상기 그래픽 사용자 인터페이스 또는 웹 브라우저를 통해 여기서 설명된 시스템 및 기술의 실시형태와 인터 랙션할 수 있음), 또는 이러한 백그라운드 부재, 미들웨어 부재, 또는 프론트 엔드 부재의 임의의 조합을 포함 하는 컴퓨팅 시스템에서 구현될 수 있다. 임의의 형태 또는 매체의 디지털 데이터 통신(예를 들어, 통신 네트워 크)을 통해 시스템의 부재를 서로 연결시킬 수 있다. 통신 네트워크의 예시로 근거리 통신망(LAN), 광역 통신망 (WAN), 인터넷 및 블록체인 네트워크가 포함된다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트 및 서버는 일반적으로 서로 멀리 떨어져 있 고 일반적으로 통신 네트워크를 통해 서로 인터랙션한다. 대응하는 컴퓨터에서 실행되고 또한 서로 클라이언트- 서버 관계를 가지는 컴퓨터 프로그램을 통해 클라이언트 및 서버의 관계를 생성한다. 서버는 클라우드 서버일 수 있고 분산형 시스템의 서버, 또는 블록체인을 결합한 서버일 수도 있다. 위에서 설명한 다양한 형태의 프로세스를 사용하여 단계를 재배열, 추가 또는 삭제할 수 있음을 이해해야 한다. 예를 들어, 본 발명에 기재된 각 단계는 병렬로 수행될 수 있거나 순차적으로 수행될 수 있거나 상이한 순서로 수행될 수 있고, 본 발명에서 공개된 기술적 해결수단이 이루고자 하는 결과를 구현할 수만 있으면, 본문은 여 기서 한정하지 않는다. 본 발명의 실시예 또는 예시는 도면을 참조하여 설명되었지만, 상기 방법, 시스템 및 기기는 단지 예시적인 실 시예 또는 예시이며, 본 발명의 범위는 이러한 실시예 또는 예시에 의해 한정되는 것이 아니라 부여된 청구범위 및 동등 범위에 의해서만 한정되는 것을 이해해야 할 것이다. 실시예 또는 예시 중의 다양한 요소는 생략되거나 다른 동등 요소에 의해 대체될 수 있다. 이 밖에, 본 발명에 설명된 것과 다른 순서로 각 단계를 수행할 수 있 다. 또한, 다양한 방식으로 실시예 또는 예시 중의 다양한 요소를 조합할 수 있다. 중요한 것은, 기술의 발전과 더불어, 여기서 설명된 많은 요소는 본 발명 이후에 나타나는 동등한 요소에 의해 교체될 수 있다는 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12"}
{"patent_id": "10-2024-0078785", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도면은 실시예를 예시적으로 도시하고 명세서의 일부를 구성하며, 명세서의 문자 설명과 함께 실시예의 예시적 인 실시형태를 설명하기 위한 것이다. 도시된 실시예는 단지 예시 목적일 뿐 청구 범위를 제한하지 않는다. 모 든 도면에서 동일한 도면 부호는 유사하지만 반드시 동일하지는 않은 요소를 지칭한다. 도 1은 본 발명의 실시예에 따라 본문에 설명된 다양한 방법 및 장치가 구현될 수 있는 예시적인 시스템의 모식도를 도시한다. 도 2는 본 발명의 복수의 실시예에 따라 설명된 각 텍스트 이미지 모델의 인터랙션 모식도를 도시한다. 도 3은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 방법의 흐름도를 도시한다. 도 4는 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 방법 중의 강화 학습 전략 모식도를 도시한다. 도 5는 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 방법의 흐름도를 도시한다. 도 6은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 방법의 흐름도를 도시한다. 도 7은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 장치의 구조 블록도를 도시한다. 도 8은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 장치의 구조 블록도를 도시한다. 도 9는 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 장치의 구조 블록도를 도시한다. 도 10은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 장치의 구조 블록도를 도시한다. 도 11은 본 발명의 실시예에 따른 텍스트 이미지 모델의 트레이닝 장치의 구조 블록도를 도시한다. 도 12는 본 발명의 실시예를 구현할 수 있는 예시적인 전자 기기의 구조 블록도를 도시한다."}
