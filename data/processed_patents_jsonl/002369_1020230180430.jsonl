{"patent_id": "10-2023-0180430", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0050666", "출원번호": "10-2023-0180430", "발명의 명칭": "이종의 센서 간의 교정을 위한 방법 및 장치", "출원인": "리벨리온 주식회사", "발명자": "문성주"}}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "이종의 센서 간의 교정을 위한 컴퓨터 구현 방법으로서,제1 센서의 데이터로부터 제1 깊이 이미지를 생성하는 과정;상기 제1 센서와 구별되는 모달리티를 갖는 제2 센서의 데이터로부터 제2 깊이 이미지를 생성하는 과정; 및트랜스포머 기반의 네트워크를 이용하여, 상기 제1 깊이 이미지 및 상기 제2 깊이 이미지로부터 상기 제1 센서와 상기 제2 센서 사이의 외부 파라미터(extrinsic parameter)를 추정하는 과정을 포함하는 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 네트워크는,상기 제1 깊이 이미지 및 상기 제2 깊이 이미지를 포함하는 입력 이미지로부터 특징을 추출하도록 구성된 트랜스포머 인코더;상기 트랜스포머 인코더에서 추출된 특징을 기초로, 상기 제1 깊이 이미지의 하나 이상의 쿼리 포인트들에 매칭되는 상기 제2 깊이 이미지의 하나 이상의 대응 포인트들을 추정하도록 구성된 트랜스포머 디코더;상기 쿼리 포인트들 및 상기 대응 포인트들을 기초로, 회전(rotation) 행렬을 추정하도록 구성된 제1 회귀기(regressor); 및상기 쿼리 포인트들 및 상기 대응 포인트들을 기초로, 이동(translation) 행렬을 추정하도록 구성된 제2 회귀기를 포함하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 입력 이미지는, 상기 제1 깊이 이미지 및 상기 제2 깊이 이미지가, 너비 또는 높이 방향으로 결합(concatenate)된 것인, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 네트워크는,훈련 샘플에 대해 추정된 대응 포인트들, 회전 행렬, 및 이동 행렬을 기초로 산출되는 손실을 이용하여 엔드-투-엔드(end-to-end)로 훈련된 것인, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 손실은, 상기 제1 회귀기에 의해 추정된 회전 행렬과 G/T(ground truth) 회전 행렬 간의 차를 기초로 산출되는 제1손실; 및상기 제2 회귀기에 의해 추정된 이동 행렬과 G/T 이동 행렬 간의 차를 기초로 산출되는 제2 손실을 포함하는, 방법.공개특허 10-2025-0050666-3-청구항 6 제4항에 있어서,상기 훈련 샘플은, 상기 제2 센서의 데이터로서, 포인트클라우드(point cloud)를 포함하고,상기 손실은, 상기 추정된 회전 행렬 및 추정된 이동 행렬을 기초로 재구성된 포인트클라우드와, 상기 포인트클라우드 간의 거리를 기초로 산출되는 제3 손실을 포함하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4항에 있어서,상기 손실은, 상기 쿼리 포인트들과 상기 트랜스포머 디코더에 의해 추정된 대응 포인트들 사이의 편차(deviation)를 나타내는 교정 플로우(calibration flow)와 상기 쿼리 포인트들과 G/T 대응 포인트들 사이의 편차를 나타내는 G/T 교정 플로우 사이의 차를 기초로 산출되는 제4 손실을 포함하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제4항에 있어서,상기 손실은, 상기 트랜스포머 디코더에 의해 추정된 대응 포인트들과 G/T 대응 포인트들 간의 거리를 기초로산출되는 제5 손실을 포함하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제2항에 있어서,상기 추정하는 과정은,상기 쿼리 포인트들과 상기 트랜스포머 디코더에 의해 추정된 대응 포인트들 사이의 편차를 기초로, 교정 플로우들을 산출하는 과정을 포함하고,상기 제1 회귀기 및 상기 제2 회귀기는, 상기 쿼리 포인트들 및 상기 대응 포인트들, 및 상기 교정 플로우들에 기초하여 상기 회전 행렬 및 상기 이동행렬을 각각 추정하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제2항에 있어서,상기 추정하는 과정은,상기 제1 깊이 이미지에서, 상기 제2 깊이 이미지의 적어도 하나의 포인트와 유사한 깊이 값을 갖는 포인트들을필터링하는 과정; 및상기 필터링된 포인트들 중에서 선별된 미리 정해진 개수의 포인트들을 상기 쿼리 포인트들로서 상기 트랜스포머 디코더에 입력하는 과정을 포함하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제2항에 있어서,상기 추정하는 과정은,상기 제1 깊이 이미지의 포인트들 중에서 랜덤하게 선별된 미리 정해진 개수의 포인트들을 상기 쿼리 포인트들로서 상기 트랜스포머 디코더에 입력하는 과정을 포함하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "공개특허 10-2025-0050666-4-제2항에 있어서,상기 추정된 회전 행렬 및 추정된 이동 행렬은, 상기 제1 깊이 이미지의 좌표계와 상기 제2 깊이 이미지의 좌표계 사이의 회전 및 이동을 각각 표현하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제2항에 있어서,상기 제1 회귀기 및 상기 제2 회귀기는 각각, MLP(Multi-Layer Perceptron)를 포함하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1항에 있어서,상기 제1 깊이 이미지를 생성하는 과정은, 단일 카메라를 이용하여 복수개의 시점들에서 촬영한 이미지들로부터 깊이를 추정하도록 사전-훈련된 깊이 추정모델을 이용하여, 상기 제1 깊이 이미지를 생성하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1항에 있어서,상기 제2 깊이 이미지를 생성하는 과정은,기설정된 초기 외부 파라미터를 이용하여, 상기 제2 센서로부터 획득된 3차원 데이터를 2차원 깊이 이미지로 투영하는 과정; 및상기 투영된 2차원 깊이 이미지에 보간을 적용하여, 상기 투영된 2차원 깊이 이미지 대비 상대적으로 밀도가 높은(dense) 상기 제2 깊이 이미지를 생성하는 과정을 포함하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 보간을 적용하는 것은, 상기 투영된 2차원 깊이 이미지에 양방향 필터(bi-lateral filter)를 적용하는 것을 포함하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제1항에 있어서,상기 추정된 외부 파라미터를 기초로, 교정된 제2 깊이 이미지를 생성하는 과정을 더 포함하는, 방법"}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제1항에 있어서,상기 제1 센서는 카메라를 포함하고, 상기 제2 센서는 라이다를 포함하는, 방법."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "명령어들을 저장하는 적어도 하나의 메모리; 및 적어도 하나의 프로세서를 포함하되,상기 적어도 하나의 프로세서는 상기 명령어들을 실행함으로써,제1 센서의 데이터로부터 제1 깊이 이미지를 생성하고,상기 제1 센서와 구별되는 모달리티를 갖는 제2 센서의 데이터로부터 제2 깊이 이미지를 생성하고,트랜스포머 기반의 네트워크를 이용하여, 상기 제1 깊이 이미지 및 상기 제2 깊이 이미지로부터 상기 제1 센서공개특허 10-2025-0050666-5-와 상기 제2 센서 사이의 외부 파라미터를 추정하는, 장치."}
{"patent_id": "10-2023-0180430", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서에 의해 실행될 때 상기 적어도 하나의 프로세서로 하여금 서로 다른 좌표계 사이의 외부 파라미터(extrinsic parameter)를 추정하는 네트워크를 구현하게 하는 명령어들을 저장하는 적어도 하나의 메모리를 포함하되,상기 네트워크는, 제1 센서의 데이터로부터 생성된 제1 깊이 이미지, 및 상기 제1 센서와 구별되는 모달리티를 갖는 제2 센서의데이터로부터 생성된 제2 깊이 이미지로부터 특징을 추출하도록 구성된 트랜스포머 인코더;상기 트랜스포머 인코더에서 추출된 특징을 기초로, 상기 제1 깊이 이미지의 하나 이상의 쿼리 포인트들에 매칭되는 상기 제2 깊이 이미지의 하나 이상의 대응 포인트들을 추정하도록 구성된 트랜스포머 디코더; 및상기 쿼리 포인트들 및 상기 대응 포인트들을 기초로, 상기 외부 파라미터를 추정하도록 구성된 하나 이상의 회귀기들을 포함하는, 장치."}
{"patent_id": "10-2023-0180430", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "이종의 센서 간의 교정을 위한 방법 및 장치를 개시한다. 본 개시의 일 측면에 의하면, 이종의 센서 간의 교정을 위한 컴퓨터 구현 방법으로서, 제1 센서의 데이터로부터 제1 깊이 이미지를 생성하는 과정; 상기 제1 센서와 구별되는 모달리티를 갖는 제2 센서의 데이터로부터 제2 깊 이 이미지를 생성하는 과정; 및 트랜스포머 기반의 네트워크를 이용하여, 상기 제1 깊이 이미지 및 상기 제2 깊 이 이미지로부터 상기 제1 센서와 상기 제2 센서 사이의 외부 파라미터를 추정하는 과정을 포함하는 방법을 제공 한다."}
{"patent_id": "10-2023-0180430", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 이종의 센서 간의 교정을 위한 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0180430", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이하에 기술되는 내용은 단순히 본 실시예와 관련되는 배경 정보만을 제공할 뿐 종래기술을 구성하는 것이 아니 다. 자율주행 모빌리티에서 라이다나 카메라와 같은 이종의 센서들 정보를 통합하여 사용하기 위해서는 센서 퓨징 (sensor fusing) 기술이 필요하다. 센서 퓨징을 위해서는, 센서들의 상대적 위치를 교정하는 작업이 선결적으로 이루어져야 한다. 센서 교정에 기술은 크게 타겟 기반(target-based) 방식과 무타겟(targetless) 방식으로 구분될 수 있다. 타겟 기반 방식은 모빌리티의 출고 및 운행 전에, 타겟보드의 피처정보들을 식별하여 센서들 사이의 위치정보를 교정 한다. 이러한 방식은, 모빌리티의 운행 중에 발생하는 뒤틀림(예컨대, 센서의 물리적 뒤틀림 또는 센서의 온도 변화로 인한 열화)는 교정할 수 없다는 단점이 있다."}
{"patent_id": "10-2023-0180430", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는, 센서 간의 상대적 위치를 실시간 또는 준-실시간으로 교정할 수 있는 방법 및 장치를 제공하는 데 일 목적이 있다. 본 개시는, 센서 간의 위치정보를 엔드-투-엔드 방식으로 교정할 수 있는 딥러닝 모델, 이의 학습 방법, 및 이 를 이용한 추론방법을 제공하는데 일 목적이 있다. 본 발명이 해결하고자 하는 과제들은 이상에서 언급한 과제들로 제한되지 않으며, 언급되지 않은 또 다른 과제 들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0180430", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면에 의하면, 이종의 센서 간의 교정을 위한 컴퓨터 구현 방법으로서, 제1 센서의 데이터로부터 제1 깊이 이미지를 생성하는 과정; 상기 제1 센서와 구별되는 모달리티를 갖는 제2 센서의 데이터로부터 제2 깊 이 이미지를 생성하는 과정; 및 트랜스포머 기반의 네트워크를 이용하여, 상기 제1 깊이 이미지 및 상기 제2 깊 이 이미지로부터 상기 제1 센서와 상기 제2 센서 사이의 외부 파라미터(extrinsic parameter)를 추정하는 과정 을 포함하는 방법을 제공한다.본 개시의 다른 측면에 의하면, 명령어들을 저장하는 적어도 하나의 메모리; 및 적어도 하나의 프로세서를 포함 하되, 상기 적어도 하나의 프로세서는 상기 명령어들을 실행함으로써, 제1 센서의 데이터로부터 제1 깊이 이미 지를 생성하고, 상기 제1 센서와 구별되는 모달리티를 갖는 제2 센서의 데이터로부터 제2 깊이 이미지를 생성하 고, 트랜스포머 기반의 네트워크를 이용하여, 상기 제1 깊이 이미지 및 상기 제2 깊이 이미지로부터 상기 제1 센서와 상기 제2 센서 사이의 외부 파라미터를 추정하는, 장치를 제공한다. 본 개시의 또 다른 측면에 의하면, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서에 의해 실행될 때 상기 적어도 하나의 프로세서로 하여금 서로 다른 좌표계 사이의 외부 파라미터를 추정하는 네트워크를 구현하 게 하는 명령어들을 저장하는 적어도 하나의 메모리를 포함하되, 상기 네트워크는, 제1 센서의 데이터로부터 생 성된 제1 깊이 이미지, 및 상기 제1 센서와 구별되는 모달리티를 갖는 제2 센서의 데이터로부터 생성된 제2 깊 이 이미지로부터 특징을 추출하도록 구성된 트랜스포머 인코더; 상기 트랜스포머 인코더에서 추출된 특징을 기 초로, 상기 제1 깊이 이미지의 하나 이상의 쿼리 포인트들에 매칭되는 상기 제2 깊이 이미지의 하나 이상의 대 응 포인트들을 추정하도록 구성된 트랜스포머 디코더; 및 상기 쿼리 포인트들 및 상기 대응 포인트들을 기초로, 상기 외부 파라미터를 추정하도록 구성된 하나 이상의 회귀기들을 포함하는, 장치를 제공한다."}
{"patent_id": "10-2023-0180430", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 실시예에 의하면, 센서 간의 상대적 위치를 실시간 또는 준-실시간으로 교정할 수 있다. 이를 통해 자율주행 모빌리티의 운행시 인지 태스크(perception task)의 정확도를 향상시킬 수 있다. 본 개시의 실시예에 의하면, 센서 교정을 위한 별도의 인프라(infra)가 요구되지 않으며, 온라인 자동 교정(on- line auto-calibration)이 가능하다. 본 개시의 실시예에 의하면, 이종의 센서 데이터들을 각각 깊이 이미지(depth image)로 변환함으로써, 시스템의 멀티-모달리티(multi-modality)(예컨대, 라이다 및 카메라)을 단일-모달리티(sing-modality)로 환원하여 시스템 의 복잡성을 낮추고 정확도를 높일 수 있다. 본 개시의 실시예에 의하면, 딥러닝 모델이 대응점 추출 및 외부 파라미터 산출의 과정을 엔드-투-엔드(end-to- end)로 학습함에 따라, 해당 태스크를 처리하는 모듈이 추상화 및/또는 단순화되어, 모듈화 개발이 용이해 진다 는 효과가 있다. 본 개시의 실시예에 의하면, 트랜스포머 기반의 네트워크를 사용하여 이종의 이미지(예컨대, 카메라 이미지로부 터 추정된 깊이 이미지 및 라이다 포인트클라우드로부터 투영된 깊이 이미지) 사이의 대응점을 추론함으로써, 모델의 일반성(robust)을 높일 수 있다. 본 개시의 효과들은 이상에서 언급한 효과들로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재 로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0180430", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 일부 실시예들을 예시적인 도면을 이용해 상세하게 설명한다. 각 도면의 구성 요소들에 참조 부호를 부가함에 있어서, 동일한 구성 요소들에 대해서는 비록 다른 도면 상에 표시되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 개시를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략한다. 본 개시에 따른 실시예의 구성요소를 설명하는 데 있어서, 제1, 제2, i), ii), a), b) 등의 부호를 사용할 수 있다. 이러한 부호는 그 구성요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 부호에 의해 해당 구성요소의 본질 또는 차례나 순서 등이 한정되지 않는다. 명세서에서 어떤 부분이 어떤 구성요소를 '포함' 또는 '구비'한 다고 할 때, 이는 명시적으로 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 첨부된 도면과 함께 이하에 개시될 상세한 설명은 본 개시의 예시적인 실시형태를 설명하고자 하는 것이며, 본 개시가 실시될 수 있는 유일한 실시형태를 나타내고자 하는 것이 아니다. 도 1은 본 개시의 일 실시예에 따른 교정 장치를 개략적으로 나타낸 블록구성도이다. 도 2는 본 개시의 일 실시 예에 따른 전처리 모듈 및 추정모듈의 동작을 설명하기 위해 참조되는 예시도이다. 도 1에 도시되듯이, 본 개시의 일 실시예에 따른 교정장치는 전처리 모듈, 추정모듈 및 교정모듈 을 전부 또는 일부 포함할 수 있다. 도 1에 도시된 모든 블록이 필수 구성요소인 것은 아니며, 다른 실시 예에서 에 포함된 일부 블록이 추가, 변경 또는 삭제될 수 있다. 한편, 도 1에 도시된 구성요소들은 기능적으로 구분되는 요소들을 나타낸 것으로서, 적어도 하나의 구성요소가 실제 물리적 환경에서는 서로 통합되는 형태로 구현될 수도 있다. 전처리 모듈은 카메라 및 라이다로부터 획득되는 데이터들을 각각 전처리하여, 깊이 이미지들(D1 및 D2,miss)을 생성한다. 카메라로부터 획득되는 데이터는 2차원 이미지(예컨대, 2차원 컬러 이미지 또는 2차 원 그레이스케일 이미지)이고, 라이다로부터 획득되는 데이터는 포인트클라우드(point cloud, PCL)일 수 있 다. 도 2를 참조하면, 전처리 모듈은 2차원 이미지에 깊이 추정(depth estimation) 알고리즘을 적용하여, 제1 깊이 이미지(D1)를 생성할 수 있다. 전처리 모듈은 제1 깊이 이미지(D1)의 생성을 위해 다양한 학습 기반 및/또는 규칙 기반 깊이 추정 알고리즘을 활용할 수 있다. 예컨대, 전처리 모듈은 단일 카메라에서 촬영된 이미지들을 기초로 깊이를 추정하도록 사전-훈련된(pre-trained) 단안 깊이 추정(mono depth estimation) 모델 을 이용하여, 제1 깊이 이미지(D1)를 생성할 수 있다. 전처리 모듈은 포인트클라우드를 2차원 평면으로 투영(projection)하여, 제2 깊이 이미지(D2,miss)를 생성할 수 있다. 전처리 모듈은 미리 설정된 초기 외부 파라미터(Tinit)를 이용하여 포인트클라우드를 투영할 수 있 다. 초기 외부 파라미터는, 이전 타임스텝(또는 이터레이션)에서, 결정된 외부 파라미터일 수 있다. 제2 깊이 이미지(D2,miss)는 초기 외부 파라미터에 의해 오교정된(miss-calibrated) 깊이 이미지일 수 있다. 일반적으로, 포인트클라우드로부터 투영된 깊이 이미지는, 2D 이미지로부터 추정된 깊이 이미지 대비 상대적으 로 희소(sparse)한 해상도를 갖는다. 이를 보완하기 위해, 전처리 모듈은 투영된 깊이 이미지에 대해 보간 (interpolation)을 수행하여, 투영된 깊이 이미지 대비 상대적으로 밀도가 높은(dense) 제2 깊이 이미지(D2,mis s)를 생성할 수 있다. 예컨대, 전처리 모듈은 양방향 필터(bi-lateral filter)를 이용하여 깊이 이미지를 보간할 수 있다. 추정모듈은 미리 훈련된 네트워크를 이용하여, 카메라 및 라이다의 자세(pose) 정보를 추정한 다. 추정모듈은 카메라의 자세와 라이다의 자세 사이의 상대적인 회전 및 이동을 나타내는 6-자유 도(6-degree of freedom, 6-DoF) 외부 파라미터(extrinsic parameter)를 추정할 수 있다. 네트워크는 트랜스포머 인코더(transformer encoder, 200), 트랜스포머 디코더(transformer decoder, 220), 회전 회귀기(rotation regressor, 240), 및 이동 회귀기(translation regressor, 260)를 전부 또는 일부 포함할 수 있다. 추정모듈은 제1 깊이 이미지(D1) 및 제2 깊이 이미지(D2,miss)를 결합(concatenate)하여, 트랜스포머 인코더 에 입력할 수 있다. 제1 깊이 이미지(D1) 및 제2 깊이 이미지(D2,miss)는 너비 방향 또는 높이 방향으로 결 합될 수 있다. 예컨대, 제1 깊이 이미지(D1) 및 제2 깊이 이미지(D2,miss)의 크기(즉, 높이×너비×채널)가 각각 256×256×3 인 경우, 트랜스포머 인코더에는 256×512×3의 크기를 갖는 입력 이미지가 입력될 수 있다. 추정모듈은 입력 이미지를 복수개의 토큰들로 분리하여 트랜스포머 인코더에 입력할 수 있다. 일 예 로, 추정모듈은 입력 이미지를 복수개의 패치(path)로 분할하고, 해당 패치들을 벡터화하여 토큰들의 시퀀스를 생성할 수 있다. 다른 예로, 추정모듈은 입력 이미지 또는 이로부터 생성된 특징맵의 공간 차원을 플 래튼(flatten) 하여 토큰들의 시퀀스를 생성할 수 있다. 트랜스포머 디코더는, 트랜스포머 인코더에 의해 추출된 특징들을 기초로 제1 깊이 이미지(D1) 및 제 2 깊이 이미지(D2,miss) 사이의 매칭(matching)을 수행한다. 추정모듈은 제1 깊이 이미지(D1)에서 하나 이상 의 포인트들(이하, 쿼리 포인트들)을 선별하여, 쿼리 정보로서 트랜스포머 디코더에 주입할 수 있다. 트랜 스포머 디코더는 해당 쿼리 포인트들에 각각 대응되는 제2 깊이 이미지(D2,miss)의 하나 이상의 포인트들(이 하, 대응 포인트들)을 추정할 수 있다. 예컨대, 트랜스포머 디코더는 100개의 쿼리 포인트들을 입력받아, 100개의 쿼리 포인트-대응 포인트의 쌍들을 생성할 수 있다. 쿼리 포인트들은 제1 깊이 이미지(D1)의 전체 포인 트들 중에서 랜덤하게 추출되거나, 필터링된 포인트들 중에서 선별될 수도 있다. 예컨대, 추정모듈은 제1 깊이 이미지(D1)의 포인트들 중에서, 제2 깊이 이미지(D2,miss)의 포인트들 중 어느 하나 이상과 유사한 깊이 값을 갖는 포인트들을 필터링하고, 필터링된 포인트들을 정제하여 미리 정해진 개수(예컨대, 100개)의 쿼리 포인트를 구할 수 있다. 포인트들의 필터링 및/또는 정제에는 KNN(K-Nearest Neighbor) 알고리즘이 이용될 수 있으나, 이 에 한정되는 것은 아니다. 추정모듈은 쿼리 포인트-대응 포인트의 쌍들을 회전 회귀기 및 이동 회귀기에 주입하여, 회전 행렬(Rpred) 및 이동 행렬(tpred)을 획득할 수 있다. 회전 회귀기 및 이동 회귀기는 회귀 태스크 (regression task)를 수행하도록 훈련된 네트워크로, 예컨대, MLP(Multi-Layer Perceptron)로 구현될 수 있다. 전역 공간 컨텍스트(global spatial context)를 증강하기 위해, 교정 플로우(calibration flow)가 회귀기들 (240 및 260)에 추가로 입력될 수 있다. 교정 플로우는, 쿼리 포인트와 대응 포인트 사이의 편차(deviation)로, 쿼리 포인트와 대응 포인트를 각각 (x,y,z) 및 (x',y',z')라 할 때, 교정 플로우는 (x-x', y-y', z-z')와 같이 계산될 수 있다. 추정모듈은 쿼리 포인트-대응 포인트의 쌍들의 정보 {(x,y,z), (x',y',z')}와 이로부터 산출된 교정 플로우 (x-x', y-y', z-z')를 결합(concatenate)하여 회귀기들(240 및 260)에 입력할 수 있다. 추정모듈은 회전 행렬(Rpred) 및 이동 행렬(tpred)을 기초로 외부 파라미터(Tpred)를 계산할 수 있다. 외부 파 라미터(Tpred)는 수학식 1과 같이 계산될 수 있다. 수학식 1"}
{"patent_id": "10-2023-0180430", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "네트워크의 출력으로부터 산출되는 외부 파라미터는, 제2 깊이 이미지(D2,miss)(즉, 초기 외부 파라미터에 의 해 오교정된 깊이 이미지)의 좌표계와 제1 깊이 이미지(D1)의 좌표계 사이의 변환관계를 나타낼 수 있다. 추정모 듈은 초기 외부 파라미터(Tinit)와 추정된 외부 파라미터(Tpred)를 이용하여, 카메라와 라이다 사이 의 외부 파라미터를 산출할 수 있다. 예컨대, 카메라와 라이다 사이의 최종 외부 파라미터(TL→C)는 수 학식 2와 같이 산출될 수 있다. 수학식 2"}
{"patent_id": "10-2023-0180430", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "교정모듈은 산출된 외부 파라미터를 이용하여, 라이다의 데이터를 교정할 수 있다. 일 예로, 교정모듈 은 산출된 최종 외부 파라미터(TL→C)를 기초로 포인트클라우드를 카메라의 이미지 평면에 투영함으로써, 교정된 제2 깊이 이미지(D2,cal)를 생성할 수 있다. 다른 예로, 교정모듈은 추정된 외부 파라 미터(Tpred)를 기초로, 제2 깊이 이미지(D2,miss)로부터 교정된 제2 깊이 이미지(D2,cal)를 생성할 수도 있다. 선택적으로, 교정모듈은 교정된 제2 깊이 이미지(D2,cal)를 보간(예컨대, 양방향 필터를 적용)하고/거나, 교정된 제2 깊이 이미지(D2,cal)와 2D 이미지를 결합(예컨대, 채널 방향으로 결합)할 수도 있다. 도 3은 본 개시의 일 실시예에 따른 네트워크의 학습과정을 개략적으로 나타낸 도면이다. 본 개시의 일 실시예에 따른 네트워크의 학습과정은, 학습장치에 의해 실행되고, 학습장치는 컴퓨 팅 디바이스 상에서 실행될 수 있다. 학습장치는, 컴퓨팅 디바이스가 가용할 수 있는 하나 이상의 프로세서에 의해 각 기능을 수행하고, 이러한 프로세서와 연결되어 내부에 저장된 명령어들을 가지는 컴퓨터 판독가능 스토 리지를 포함할 수 있으나, 이에 한정되는 것은 아니다. 네트워크의 학습을 위한 훈련 샘플은, 카메라의 데이터(예컨대, 2D 이미지, 라이다의 데이터(예컨 대, 포인트클라우드), 및/또는 카메라와 라이다 간 G/T(ground truth) 외부 파라미터를 포함할 수 있다. 학습장치는 카메라의 데이터에 전처리를 수행하여 제1 깊이 이미지를 생성하고, 라이다의 데이터에 대해 전처리를 수행하여 제2 깊이 이미지를 생성할 수 있다. 학습장치에 의해 수행되는 전처리 동작은, 도 1 및 도 2에서 전술한 전처리 모듈의 동작과 동일하거나 상응하므로 이에 대한 자세한 설명은 생략한다. 학습장치는, G/T 외부 파라미터에 임의의 값들 추가하여 초기 외부 파라미터로 이용할 수 있으나 이에 한정 되는 것은 아니다. 학습장치는 깊이 이미지들을 너비 방향 또는 높이 방향으로 결합하여, 네트워크의 입력 이미지를 생성 할 수 있다. 학습장치는 입력 이미지를 네트워크의 트랜스포머 인코더에 입력하고, 제1 깊이 이미 지에서 선별된 하나 이상의 쿼리 포인트들을 쿼리 정보로서 네트워크의 트랜스포머 디코더에 입력할 수 있다. 네트워크는 입력 이미지 및 쿼리 포인트들을 기초로, 쿼리 포인트들에 대응하는 제2 깊이 이미지 의 대응 포인트들, 회전 행렬(Rpred), 및 이동 행렬(tpred)을 추정할 수 있다. 학습장치는 네트워크에 의해 추정된 대응 포인트들, 회전 행렬(Rpred), 및/또는 이동 행렬(tpred)에 대한 손실(loss)을 기초로, 네트워크의 파라미터를 업데이트할 수 있다. 학습장치는, 네트워크의 학습을 위한 손실로, 회전 손실(rotation loss, LR), 이동 손실(translation loss, LT), 포인트클라우드 거리 손실(point cloud distance loss, LP), 대응 매칭 포인트 거리 손실 (correspondence matching point distance, LC) 및 교정 플로우 손실(calibration flow loss, LCF)의 전부 또는 일부를 이용할 수 있다. 회전 손실(LR) 및 이동 손실(LT)은 각각 회전 회귀기와 이동 회귀기의 출력에 대한 회귀 손실 (regression loss)을 나타낸다. 회전 손실(LR)은 회전 회귀기에 의해 추정된 회전 행렬(Rpred)과 G/T 회전 행렬 간의 차(예컨대, 각 거리(angular distance) 또는 평균 제곱 오차 등)를 기초로 산출될 수 있고, 이동 손 실(LT)은 이동 회귀기에 의해 추정된 이동 행렬(tpred)과 G/T 이동 행렬 간의 차(예컨대, L1 norm 또는 평 균 제곱 오차 등)를 기초로 산출될 수 있다. G/T 회전 행렬 및 G/T 이동 행렬은, 초기 외부 파라미터와 G/T 외 부 파라미터 사이의 편차를 기초로 산출될 수 있으나 이에 한정되는 것은 아니다. 포인트클라우드 거리 손실(LP)은 추정된 외부 파라미터(Tpred)를 기초로 재구성된 포인트클라우드와, 원본 포인트 클라우드 간의 포인트별 거리(예컨대, L2 norm)를 나타낸다. 학습장치는 제2 깊이 이미지의 생성에 사용된 초기 외부 파라미터, 추정된 외부 파라미터(Tpred), 및 G/T 외부 파라미터를 이용하여 포인트클라우드를 재구성할 수 있으나 이에 한정되는 것은 아니다. 대응 매칭 포인트 거리 손실(LC)은 트랜스포머 디코더에 의해 추정된 대응 포인트들과, G/T 대응 포인트들 간의 거리(예컨대, L2 norm)를 나타낸다. 학습장치는 초기 외부 파라미터 및/또는 G/T 외부 파라미터를 이 용하여, G/T 대응 포인트들을 산출할 수 있으나, 이에 한정되는 것은 아니다. 교정 플로우 손실(LCF)은 쿼리 포인트들과 트랜스포머 디코더에 의해 추정된 대응 포인트들을 이용하여 산 출된 교정 플로우와, G/T 교정 플로우 사이의 차(예컨대, L1 norm)를 나타낼 수 있다. 여기서, G/T 교정 플로우 는, 쿼리 포인트들과 G/T 대응 포인트들 사이의 편차일 수 있다. 구현예에 따라, 쿼리 포인트들 대신 초기 외부파라미터를 기초로 산출된 오교정된 대응 포인트들(예컨대, 쿼리 포인트와 동일한 픽셀 위치를 갖고, 포인트클 라우드로부터 추출 또는 보간된 깊이 값을 갖는 포인트들)이 이용될 수도 있다. 네트워크의 학습에 이용되는 총 손실(L)은 수학식 3과 같이 표현될 수 있다. 수학식 3"}
{"patent_id": "10-2023-0180430", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, λT, λR, λP, λC, 및 λCF는 각각의 손실에 적용되는 가중치를 나타낸다. 구현예에 따라, 대응 포인트 거리 손실(LC) 또는 교정 플로우 손실(LCF) 중 어느 하나가 생략(또는, 가중치가 0으로 설정)될 수도 있다. 도 4는 본 개시의 일 실시예에 따른 센서 교정방법을 나타내는 흐름도이다. 도 4에 도시된 방법은, 전술한 교정장치 및/또는 학습장치의 하나 이상의 기능이 적어도 하나의 컴퓨팅 장치에 의해 실행됨으로써 구현될 수 있다. 이하의 설명은 컴퓨팅 장치가 수행하는 동작 측면에서 서술한다. 과정 S400에서, 컴퓨팅 장치는 제1 센서의 데이터로부터 제1 깊이 이미지를 생성한다. 제1 센서는 카메라일 수 있다. 컴퓨팅 장치는 사전-훈련된 깊이 추정 모델을 이용하여, 제1 깊이 이미지를 생성할 수 있다. 깊이 추정 모델은 단일 카메라를 이용하여 복수개의 시점들에서 촬영한 이미지들로부터 깊이를 추정하도록 훈련된 모델일 수 있다. 과정 S420에서, 컴퓨팅 장치는 제1 센서와 구별되는 모달리티를 갖는 제2 센서의 데이터로부터 제2 깊이 이미지 를 생성한다. 제2 센서는 라이다일 수 있다. 컴퓨팅 장치는 제2 센서로부터 3차원 데이터(예컨대, 포인트클라우 드(point cloud))를 획득할 수 있다. 컴퓨팅 장치는 기설정된 초기 외부 파라미터를 이용하여, 획득된 3차원 데 이터를 2차원 깊이 이미지로 투영할 수 있다. 컴퓨팅 장치는 투영된 2차원 깊이 이미지에 보간을 적용하여, 해 당 투영된 2차원 깊이 이미지 대비 상대적으로 밀도가 높은(dense) 제2 깊이 이미지를 생성할 수 있다. 여기서, 보간을 적용하는 것은, 투영된 2차원 깊이 이미지에 양방향 필터(bi-lateral filter)를 적용하는 것을 포함할 수 있다. 과정 S440에서, 컴퓨팅 장치는 트랜스포머 기반의 네트워크를 이용하여, 제1 깊이 이미지 및 제2 깊이 이미지로 부터 제1 센서와 제2 센서 사이의 외부 파라미터(extrinsic parameter)를 추정한다. 네트워크는, 트랜스포머 인 코더, 트랜스포머 디코더, 및 하나 이상의 회귀기(regressor)들을 포함할 수 있다. 트랜스포머 인코더는, 제1 깊이 이미지 및 제2 깊이 이미지를 포함하는 입력 이미지로부터 특징을 추출하도록 구성될 수 있다. 컴퓨팅 장치는 제1 깊이 이미지 및 제2 깊이 이미지를, 너비 또는 높이 방향으로 결합 (concatenate)하여 입력 이미지를 생성할 수 있다. 트랜스포머 디코더는, 트랜스포머 인코더에서 추출된 특징을 기초로, 제1 깊이 이미지의 하나 이상의 쿼리 포인 트들에 각각 매칭되는, 제2 깊이 이미지의 하나 이상의 대응 포인트들을 추정하도록 구성될 수 있다. 쿼리 포인 트들은, 제1 깊이 이미지의 포인트들 중에서 선별된 미리 정해진 개수의 포인트들일 수 있다. 일 예로, 컴퓨팅 장치는, 제1 깊이 이미지의 포인트들 중에서 랜덤하게 선별된 하나 이상의 포인트들을 쿼리 포인트들로서 트랜 스포머 디코더에 입력할 수 있다. 다른 예로, 컴퓨팅 장치는, 제1 깊이 이미지에서, 제2 깊이 이미지의 적어도 하나의 포인트와 유사한 깊이 값을 갖는 포인트들을 필터링하고, 필터링된 포인트들 중에서 선별된 미리 정해진 개수의 포인트들을 쿼리 포인트들로서 트랜스포머 디코더에 입력할 수도 있다. 유사한 깊이 값을 갖는 포인트의 필터링 및/또는 쿼리 포인트의 선별에는 KNN(K-Nearest Neighbor) 알고리즘이 이용될 수 있으나, 이에 한정되는 것은 아니다. 하나 이상의 회귀기들은, 쿼리 포인트 및 대응 포인트의 쌍들을 기초로, 회전(rotation) 행렬을 추정하도록 구 성된 회전 회귀기를 포함할 수 있다. 하나 이상의 회귀기들에 의해 추정되는 회전 행렬 및 이동 행렬은, 제1 깊 이 이미지의 좌표계와 제2 깊이 이미지의 좌표계 사이의 회전 및 이동을 각각 표현할 수 있다. 하나 이상의 회 귀기들은, 쿼리 포인트 및 대응 포인트의 쌍들을 기초로, 이동(translation) 행렬을 추정하도록 구성된 이동 회 귀기를 포함할 수 있다. 하나 이상의 회귀기들은 각각, MLP(Multi-Layer Perceptron)를 포함할 수 있다. 구현예 에 따라, 하나 이상의 회귀기들은 하나 이상의 레이어를 공유할 수도 있다. 하나 이상의 회귀기들은, 쿼리 포인 트들 및 대응 포인트들, 및 교정 플로우들에 기초하여 회전 행렬 및 이동 행렬을 각각 추정할 수 있따. 이를위해, 컴퓨팅 장치는 쿼리 포인트들과 트랜스포머 디코더에 의해 추정된 대응 포인트들 사이의 편차를 기초로, 교정 플로우들을 산출할 수 있다. 네트워크는, 훈련 샘플에 대해 추정된 대응 포인트들, 추정된 회전 행렬, 및 추정된 이동 행렬을 기초로 산출되 는 손실을 이용하여 엔드-투-엔드(end-to-end)로 훈련될 수 있다. 손실은 제1 회귀기에 의해 추정된 회전 행렬 과 G/T(ground truth) 회전 행렬 간의 차를 기초로 산출되는 제1 손실을 포함할 수 있다. 부가적으로 또는 대안 적으로, 손실은 제2 회귀기에 의해 추정된 이동 행렬과 G/T 이동 행렬 간의 차를 기초로 산출되는 제2 손실을 포함할 수 있다. 부가적으로 또는 대안적으로, 손실은, 회귀기들에 의해 추정된 회전 행렬 및 이동 행렬을 기초 로 재구성된 포인트클라우드와, 제2 센서로부터 획득한 원본 포인트클라우드 간의 거리를 기초로 산출되는 제3 손실을 포함할 수 있다. 부가적으로 또는 대안적으로, 손실은, 훈련 샘플로부터 산출된 쿼리 포인트들과, 트랜 스포머 디코더에 의해 추정된 대응 포인트들 사이의 편차(deviation)를 나타내는 교정 플로우(calibration flow)에 대한 제4 손실을 포함할 수 있다. 제4 손실은, 예컨대, 교정 플로우와, 쿼리 포인트들과 G/T 대응 포인 트들 사이의 편차를 나타내는 G/T 교정 플로우 사이의 차를 기초로 산출될 수 있다. 부가적으로 또는 대안적으 로, 손실은, 트랜스포머 디코더에 의해 추정된 대응 포인트들과 G/T 대응 포인트들 간의 거리를 기초로 산출되 는 제5 손실을 포함할 수 있다. 과정 S460에서, 컴퓨팅 장치는 추정된 외부 파라미터를 기초로, 교정된 제2 깊이 이미지를 생성할 수 있다. 도 5는 본 개시에 따른 방법 또는 장치를 구현하기 위해 사용될 수 있는 예시적인 컴퓨팅 장치를 개략적으로 나 타낸 블록구성도이다. 컴퓨팅 장치는 메모리, 프로세서, 스토리지, 입출력 인터페이스 및 통신 인터페이스 중 일부 또는 전부를 포함할 수 있다. 컴퓨팅 장치는 교정장치 및/또는 학습장치의 적어도 일부를 구조적 및/또는 기능적으로 포함할 수 있다. 컴퓨팅 장치는 데스크탑 컴퓨터, 서버 등과 같은 고정 형(stationary) 컴퓨팅 장치뿐만 아니라, 랩탑 컴퓨터, 스마트 폰 등과 같은 휴대용(mobile) 컴퓨팅 장치일 수 도 있다. 컴퓨팅 장치는 자율주행 차량, AMR(Autonomous Mobile Robots)과 같은 다종의 센서들을 구비한 모빌리티에 탑재될 수도 있다. 컴퓨팅 장치는 인공지능 모델에 대한 연산들을 효율적인 방식으로 처리하는 것이 가능한 임의의 특수화된 하드웨어 가속기(accelerator)로 구현될 수도 있다. 예컨대, 컴퓨팅 장치는 그래픽 처리 장치(graphic processing unit, GPU), 텐서 처리 장치(Tensor Processing Unit, TPU) 또는 신경망 처리 장치(neural processing unit, NPU)를 포함할 수 있다. 메모리는 프로세서로 하여금 본 개시의 다양한 실시예에 따른 방법 또는 동작을 수행하도록 하는 프 로그램을 저장할 수 있다. 예를 들면, 프로그램은 프로세서에 의해서 실행 가능한(executable) 복수의 명 령어들을 포함할 수 있고, 복수의 명령어들이 프로세서에 의해서 실행됨으로써 도 4에 도시된 방법이 수행 될 수 있다. 메모리는 단일 메모리 또는 복수의 메모리들일 수 있다. 이 경우, 본 개시의 다양한 실시예에 따른 방법 또는 동작을 수행하기 위해 필요한 정보는 단일 메모리에 저장되거나 복수의 메모리들에 나뉘어 저장 될 수 있다. 메모리가 복수의 메모리들로 구성된 경우, 복수의 메모리들은 물리적으로 분리될 수 있다. 메 모리는 휘발성 메모리 및 비휘발성 메모리 중 적어도 하나를 포함할 수 있다. 휘발성 메모리는 SRAM(Static Random Access Memory) 또는 DRAM(Dynamic Random Access Memory) 등을 포함하고, 비휘발성 메모 리는 플래시 메모리(flash memory) 등을 포함한다. 프로세서는 적어도 하나의 명령어들을 실행할 수 있는 적어도 하나의 코어를 포함할 수 있다. 프로세서 는 메모리에 저장된 명령어들을 실행할 수 있다. 프로세서는 단일 프로세서 또는 복수의 프로세 서들일 수 있다. 스토리지는 컴퓨팅 장치에 공급되는 전력이 차단되더라도 저장된 데이터를 유지한다. 예를 들면, 스토 리지는 비휘발성 메모리를 포함할 수도 있고, 자기 테이프, 광학 디스크, 자기 디스크와 같은 저장 매체를 포함할 수도 있다. 스토리지에 저장된 프로그램은 프로세서에 의해서 실행되기 이전에 메모리로 로딩될 수 있다. 스토리지는 프로그램 언어로 작성된 파일을 저장할 수 있고, 파일로부터 컴파일러 등에 의해서 생성된 프로그램은 메모리로 로딩될 수 있다. 스토리지는 프로세서에 의해서 처리될 데 이터 및/또는 프로세서에 의해서 처리된 데이터를 저장할 수 있다. 입출력 인터페이스는 키보드, 마우스 등과 같은 입력 장치를 포함할 수 있고, 디스플레이 장치, 프린터 등 과 같은 출력 장치를 포함할 수 있다. 사용자는 입출력 인터페이스를 통해 프로세서에 의한 프로그램 의 실행을 트리거하고/거나 프로세서의 처리 결과를 확인할 수 있다.통신 인터페이스는 외부 네트워크에 대한 액세스를 제공할 수 있다. 컴퓨팅 장치는 통신 인터페이스 를 통해 다른 장치들과 통신할 수 있다. 본 발명에 따른 장치 또는 방법의 각 구성요소는 하드웨어 또는 소프트웨어로 구현되거나, 하드웨어 및 소프트 웨어의 결합으로 구현될 수 있다. 또한, 각 구성요소의 기능이 소프트웨어로 구현되고 마이크로프로세서가 각 구성요소에 대응하는 소프트웨어의 기능을 실행하도록 구현될 수도 있다. 본 명세서에 설명되는 시스템들 및 기법들의 다양한 구현예들은, 디지털 전자 회로, 집적회로, FPGA(field programmable gate array), ASIC(application specific integrated circuit), 컴퓨터 하드웨어, 펌웨어, 소프 트웨어, 및/또는 이들의 조합으로 실현될 수 있다. 이러한 다양한 구현예들은 프로그래밍가능 시스템 상에서 실 행 가능한 하나 이상의 컴퓨터 프로그램들로 구현되는 것을 포함할 수 있다. 프로그래밍가능 시스템은, 저장 시 스템, 적어도 하나의 입력 디바이스, 그리고 적어도 하나의 출력 디바이스로부터 데이터 및 명령들을 수신하고 이들에게 데이터 및 명령들을 전송하도록 결합되는 적어도 하나의 프로그래밍가능 프로세서(이것은 특수 목적 프로세서일 수 있거나 혹은 범용 프로세서일 수 있음)를 포함한다. 컴퓨터 프로그램들(이것은 또한 프로그램들, 소프트웨어, 소프트웨어 애플리케이션들 혹은 코드로서 알려져 있음)은 프로그래밍가능 프로세서에 대한 명령어 들을 포함하며 \"컴퓨터가 읽을 수 있는 기록매체\"에 저장된다. 컴퓨터가 읽을 수 있는 기록매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기 록장치를 포함한다. 이러한 컴퓨터가 읽을 수 있는 기록매체는 ROM, CD-ROM, 자기 테이프, 플로피디스크, 메모 리 카드, 하드 디스크, 광자기 디스크, 스토리지 디바이스 등의 비휘발성(non-volatile) 또는 비일시적인(non- transitory) 매체일 수 있으며, 또한 데이터 전송 매체(data transmission medium)와 같은 일시적인 (transitory) 매체를 더 포함할 수도 있다. 또한, 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수도 있다. 본 명세서의 흐름도/타이밍도에서는 각 과정들을 순차적으로 실행하는 것으로 기재하고 있으나, 이는 본 개시의 일 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것이다. 다시 말해, 본 개시의 일 실시예가 속하는 기 술 분야에서 통상의 지식을 가진 자라면 본 개시의 일 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 흐 름도/타이밍도에 기재된 순서를 변경하여 실행하거나 각 과정들 중 하나 이상의 과정을 병렬적으로 실행하는 것으로 다양하게 수정 및 변형하여 적용 가능할 것이므로, 흐름도/타이밍도는 시계열적인 순서로 한정되는 것은 아니다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5"}
{"patent_id": "10-2023-0180430", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 교정 장치를 개략적으로 나타낸 블록구성도이다. 도 2는 본 개시의 일 실시예에 따른 전처리 모듈 및 추정모듈의 동작을 설명하기 위해 참조되는 예시도이다. 도 3은 본 개시의 일 실시예에 따른 네트워크의 학습과정을 개략적으로 나타낸 도면이다. 도 4는 본 개시의 일 실시예에 따른 센서 교정방법을 나타내는 흐름도이다. 도 5는 본 개시에 따른 방법 또는 장치를 구현하기 위해 사용될 수 있는 예시적인 컴퓨팅 장치를 개략적으로 나 타낸 블록구성도이다."}
