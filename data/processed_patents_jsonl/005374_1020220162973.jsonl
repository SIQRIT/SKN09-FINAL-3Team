{"patent_id": "10-2022-0162973", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0079747", "출원번호": "10-2022-0162973", "발명의 명칭": "인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모델의 입 모양과 표정을 실시간으로 표", "출원인": "전준혁", "발명자": "전준혁"}}
{"patent_id": "10-2022-0162973", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모델의 입 모양과 표정을 실시간으로 표시하는 시스템으로서,사용자로부터 입력되는 음성 신호를 획득하고 저장하는 음성 신호 저장부;입력되는 음성 신호에 대한 특징 벡터를 생성하는 발화 처리부;상기 발화 처리부에서 생성된 특징 벡터에 기초하여 입력되는 음성 신호에 대한 단어와 감정을 분류하여 단어및 감정 정보를 출력하는 단어 및 감정 판별부;상기 단어 및 감정 판별부에서 출력된 단어 및 감정 정보에 기초하여 3D 얼굴 모델의 리그 포인트를 이동시키는얼굴 리그 포인트 이동부; 및3D 얼굴 모델을 저장하는 3D 얼굴 모델 저장부를 포함하고, 상기 발화 처리부는, 현재 입력되는 음성 신호인 현재 스텝의 음성 신호 이전의 스텝에서의 음성 신호에 대한 특징 벡터를 생성하는이전 스텝 발화 처리부; 및현재 스텝의 음성 신호에 대한 특징 벡터를 생성하는 현재 스텝 발화 처리부를 포함하는 것을 특징으로 하는 시스템."}
{"patent_id": "10-2022-0162973", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 이전 스텝 발화 처리부 및 현재 스텝 발화 처리부는, 각 스텝에서의 음성 신호를 입력으로 하여 특징 벡터를 생성하는 신경망 모델로 구현되고,상기 현재 스텝 발화 처리부는, 상기 이전 스텝 발화 처리부에서 생성된 특징 벡터와 현재 스텝의 음성 신호를입력으로 하여 특징 벡터를 생성하는 것을 특징으로 하는 시스템."}
{"patent_id": "10-2022-0162973", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서, 상기 현재 스텝 발화 처리부는, 마스크드 멀티 헤드 어텐션 레이어(masked multi-head attention layer)를 포함하는 것을 특징으로 하는 시스템."}
{"patent_id": "10-2022-0162973", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모델의 입 모양과 표정을 실시간으로 표시하는방법으로서,사용자로부터 입력되는 음성 신호를 획득하고 저장하는 단계;입력되는 음성 신호에 대한 특징 벡터를 생성하는 단계;공개특허 10-2024-0079747-3-상기 생성된 특징 벡터에 기초하여 입력되는 음성 신호에 대한 단어와 감정을 분류하여 단어 및 감정 정보를 출력하는 단계; 및상기 출력된 단어 및 감정 정보에 기초하여 3D 얼굴 모델의 리그 포인트를 이동시키는 단계를 포함하고, 상기 특징 벡터를 생성하는 단계는, 현재 입력되는 음성 신호인 현재 스텝의 음성 신호 이전의 스텝에서의 음성신호에 대한 특징 벡터를 생성하는 단계와, 현재 스텝의 음성 신호에 대한 특징 벡터를 생성하는 단계를 포함하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2022-0162973", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모델의 입 모양과 표정을 실시간으로 표시하는 시스템에 관한 것으로서, 사용자로부터 입력되는 음성 신호를 획득하고 저장하는 음성 신호 저장부; 입력되는 음 성 신호에 대한 특징 벡터를 생성하는 발화 처리부; 상기 발화 처리부에서 생성된 특징 벡터에 기초하여 입력되 (뒷면에 계속)"}
{"patent_id": "10-2022-0162973", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모델의 입 모양과 표정을 실시간으로 표시하는 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2022-0162973", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 스마트폰의 보급에 따라 카메라로 얼굴을 촬영하여 아바타(avartar)를 생성하여 다양한 부가 서비스를 제 공하는 기술이 제안되고 있다. 이러한 서비스 중에서, 자신의 얼굴이나 다른 사람의 얼굴에 음성을 합성하여 표시하는 이른바 \"페이셜 애니메 이션(facial animation))\"을 이용한 서비스도 점차 확대되고 있다. 그러나, 이러한 서비스를 구현하는 종래의 기술은, 아직까지 부자연스럽고 입 모양과 감정을 제대로 표현하지 못하고 입 모양과 음성 신호가 일치하지 않거나 감정이 제대로 표현되지 않아서 부자연스럽고 어색한 경우가 많 다."}
{"patent_id": "10-2022-0162973", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모델의 입 모양과 표정을 실시간으로 표시하는 시스템 및 방법을 제공하는 것을 목적으로 한다."}
{"patent_id": "10-2022-0162973", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 바와 같은 과제를 해결하기 위하여 본 발명은, 인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모 델의 입 모양과 표정을 실시간으로 표시하는 시스템으로서, 사용자로부터 입력되는 음성 신호를 획득하고 저장 하는 음성 신호 저장부; 입력되는 음성 신호에 대한 특징 벡터를 생성하는 발화 처리부; 상기 발화 처리부에서 생성된 특징 벡터에 기초하여 입력되는 음성 신호에 대한 단어와 감정을 분류하여 단어 및 감정 정보를 출력하 는 단어 및 감정 판별부; 상기 단어 및 감정 판별부에서 출력된 단어 및 감정 정보에 기초하여 3D 얼굴 모델의 리그 포인트를 이동시키는 얼굴 리그 포인트 이동부; 및 3D 얼굴 모델을 저장하는 3D 얼굴 모델 저장부를 포함 하고, 상기 발화 처리부는, 현재 입력되는 음성 신호인 현재 스텝의 음성 신호 이전의 스텝에서의 음성 신호에 대한 특징 벡터를 생성하는 이전 스텝 발화 처리부; 및 현재 스텝의 음성 신호에 대한 특징 벡터를 생성하는 현 재 스텝 발화 처리부를 포함하는 것을 특징으로 하는 시스템을 제공한다. 여기에서, 상기 이전 스텝 발화 처리부 및 현재 스텝 발화 처리부는, 각 스텝에서의 음성 신호를 입력으로 하여 특징 벡터를 생성하는 신경망 모델로 구현되고, 상기 현재 스텝 발화 처리부는, 상기 이전 스텝 발화 처리부에 서 생성된 특징 벡터와 현재 스텝의 음성 신호를 입력으로 하여 특징 벡터를 생성할 수 있다. 또한, 상기 현재 스텝 발화 처리부는, 마스크드 멀티 헤드 어텐션 레이어(masked multi-head attention laye r)를 포함할 수 있다. 본 발명의 다른 측면에 의하면, 인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모델의 입 모양과 표정을 실시간으로 표시하는 방법으로서, 사용자로부터 입력되는 음성 신호를 획득하고 저장하는 단계; 입력되는 음성 신호에 대한 특징 벡터를 생성하는 단계; 상기 생성된 특징 벡터에 기초하여 입력되는 음성 신호에 대한 단어와 감정을 분류하여 단어 및 감정 정보를 출력하는 단계; 및 상기 출력된 단어 및 감정 정보에 기초하여 3D 얼굴 모델의 리그 포인트를 이동시키는 단계를 포함하고, 상기 특징 벡터를 생성하는 단계는, 현재 입력되는 음성 신 호인 현재 스텝의 음성 신호 이전의 스텝에서의 음성 신호에 대한 특징 벡터를 생성하는 단계와, 현재 스텝의 음성 신호에 대한 특징 벡터를 생성하는 단계를 포함하는 것을 특징으로 하는 방법을 제공한다."}
{"patent_id": "10-2022-0162973", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의하면, 인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모델의 입 모양과 표정을 실시간으로 표시하는 시스템 및 방법을 제공할 수 있다. 본 발명에 의하면, 현재 입력되는 음성 신호 보다 이전 스텝에서 입력된 과거의 음성 신호까지 고려하여 단어 및 감정 정보를 추론하기 때문에, 정확한 맥락을 파악할 수 있어서 보다 정확한 입 모양 및 감정을 표현할 수 있는 장점이 있다."}
{"patent_id": "10-2022-0162973", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부 도면을 참조하여 본 발명에 의한 실시예를 상세하게 설명하기로 한다. 도 1은 본 발명에 의한 인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모델의 입 모양과 표정을 실시간으 로 표시하는 시스템(100, 이하 \"시스템\"이라 한다)의 일실시예의 전체적인 구성을 나타낸 도면이다. 도 1을 참조하면, 시스템은, 음성 신호 저장부, 발화 처리부, 단어 및 감정 판별부, 얼굴 리 그 포인트 이동부 및 3D 얼굴 모델 저장부를 포함한다. 음성 신호 저장부는 사용자로부터 입력되는 음성 신호를 획득하고 저장하는 수단이다. 또한, 음성 신호 저장부는, 음성 신호가 입력될 때마다 입력되는 스텝별로 구분하여 음성 신호를 저장한다. 사용자가 복수인 경우, 음성 신호 저장부는, 사용자별로 구분하여 음성 신호를 저장한다. 이 경우에도, 음 성 신호 저장부는, 음성 신호가 입력되는 스텝별로 음성 신호를 저장한다. 발화 처리부는, 입력되는 음성 신호에 대한 특징 벡터를 생성하는 수단이다. 본 발명에서 발화 처리부는, 현재 입력되는 음성 신호로부터 미리 설정된 이전 스텝까지의 음성 신호에 대 해 발화 처리를 수행한다. 이는 기존의 대화 내용을 반영하는 경우 문장이나 단어의 맥락을 보다 잘 파악할 수 있기 때문이다. 이하에서 발화 처리부는 현재 입력되는 음성 신호를 현재 스텝의 음성 신호라 할 때, 2 스텝 이전까지의 음 성 처리에 대한 발화 신호를 처리하는 것을 예로 들어서 설명한다.도 2는 발화 처리부의 구성을 나타낸 도면이다. 도 2를 참조하면, 발화 처리부는, 2 스텝 이전 발화 처리부, 1 스텝 이전 발화 처리부 및 현재 스 텝 발화 처리부를 포함할 수 있다. 2 스텝 이전 발화 처리부는, 현재 스텝의 음성 신호보다 2 스텝 이전의 음성 신호에 대한 발화 처리를 수행 하여 특징 벡터를 생성하는 수단이고, 1 스텝 이전 발화 처리부는 현재 스텝의 음성 신호보다 1 스텝 이전 의 음성 신호에 대한 발화 처리를 수행하여 특징 벡터를 생성하는 수단이다. 2 스텝 이전 발화 처리부 및 1 스텝 이전 발화 처리부는 각 스텝별 음성 신호를 입력으로 하는 딥 러닝 에 기반한 신경망 모델로 구현될 수 있다. 예컨대, 2 스텝 이전 발화 처리부 및 1 스텝 이전 발화 처리부는 음성 신호를 입력으로 하여 특징 벡터 를 생성하는 신경망 모델을 포함할 수 있다. 이러한 신경망 모델은 예컨대 2D CNN(Convolutional Neural Network)로 구현될 수 있다. 이 경우, 신경망 모델 은 시간 및 주파수 영역의 음성 신호를 2D CNN에 통과시켜 특징 벡터를 생성할 수 있다. 이 때 생성되는 특징 벡터는 음성 신호에 대한 임베딩 벡터(embedding vector)로 볼 수 있다. 또한, 2 스텝 이전 발화 처리부 및 1 스텝 이전 발화 처리부는 상기 생성된 특징 벡터를 예컨대, Gated Residual Network(GRN)과 같은 신경망 모델로 입력시켜서 멀티 헤드 어텐션 레이어(multi-head attention layer)를 적용하여 각 스텝별 음성 신호에 대한 특징 벡터를 생성할 수 있다. 도 3은 GRN의 구성의 일예를 나타낸 것이고, 도 4는 멀티 헤드 어텐션 레이어의 이례를 나타낸 도면이다. 이러한 GRN 및 멀티 헤드 어텐션 레이어를 거치면, 예컨대 256 크기의 특징 벡터가 생성되고 생성된 특징 벡터 는 후술하는 현재 스텝 발화 처리부에 전달된다. 이러한 2D-CNN, GRN 및 멀티 헤드 어텐션 레이어의 개념은 종래 기술에 의해 알려져 있는 것이고 본 발명의 직 접적인 목적은 아니므로 여기서는 상세 설명은 생략한다. 현재 스텝 발화 처리부는 현재 스텝의 음성 신호에 대한 발화 처리를 수행하는 수단이다. 현재 스텝 발화 처리부는 현재 스텝의 음성 신호와 상기 2 스텝 이전 발화 처리부 및 1 스텝 이전 발화 처리부에 의해 생성된 특징 벡터를 함께 고려하여 현재 스텝의 음성 신호에 대한 특징 벡터를 생성한다. 이를 위하여, 현재 스텝 발화 처리부 또한 현재 스텝의 음성 신호를 입력으로 하는 2D CNN으로 구현되어, 시간 및 주파수 영역의 음성 신호를 2D CNN에 통과시켜 벡터를 생성할 수 있다. 또한, 현재 스텝 발화 처리부는, 2D CNN을 거친 벡터와, 상기 2 스텝 이전 발화 처리부 및 1 스텝 이전 발화 처리부에 의해 생성된 특징 벡터를 함께 예컨대 버트 레이어(BERT Layer)에 통과시킬 수 있다. 이 때, 현재 스텝 발화 처리부는, 현재 입력되는 음성 신호의 각 단어별로 전술한 바와 같은 Gated Residual Network(GRN)과 같은 신경망 모델로 입력시키되 마스크드 멀티 헤드 어텐션 레이어(masked multi- head attention layer)를 적용하여 현재 스텝의 음성 신호에 대한 특징 벡터를 생성할 수 있다. 여기에서, 현재 스텝의 음성 신호에 대한 특징 벡터는, 전술한 바와 같이, 2 스텝 이전까지의 음성 신호에 대한 특징 벡터까지 고려된 특징 벡터이다. 또한, 여기에서, 마스크드(masked)란, 연속적으로 입력되는 단어들에 대해 신경망 모델에 적용할 때 일정 부분 까지의 단어만 입력될 수 있으므로 다른 부분을 제외하고 입력시키는 것을 의미한다. 예컨대, \"음성을 통해 단 어와 감정을 판별합니다.\"라는 문장이 음성 신호로 입력되는 경우, 발화 처리부에서 학습을 수행할 때 문장 전체를 한꺼번에 계산하지만, 실제 사용자가 발화할 때는, \"음성을 통해\" 까지만 말하고 뒷 부분의 말은 신경망 모델에 입력될 수 없다. 따라서, 뒷 부분은 제외하고 \"음성을 통해\"까지만 입력으로 사용하여 결과를 예측할 수 있도록 하는 것을 의미한다. 현재 스텝 발화 처리부는 상기와 같은 과정을 거친 후 생성되는 특징 벡터를 단어 및 감정 판별부로 전 달한다. 단어 및 감정 판별부는 발화 처리부에서 생성된 특징 벡터에 기초하여 입력되는 음성 신호에 대한 단어 와 감정을 분류하여 단어 및 감정 정보를 출력하는 기능을 수행한다.즉, 단어 및 감정 판별부는 상기 발화 처리부에서 생성된 특징 벡터에 의해 현재 스텝에서 입력되는 음 성 신호의 단어 및 감정 정보를 출력한다. 이는 신경망 모델의 분류를 위한 예컨대 검벨 소프트맥스 레이어(Gumbel softmax layer)로 구현될 수 있다. 단 어 및 감정 판별부는, 발화 처리부가 학습된 후 상기한 바와 같은 특징 벡터를 생성했을 때, 현재 스텝 의 음성 신호에 대한 단어와 감정에 해당하는 정보를 출력할 수 있다. 예컨대, 단어는 0 -> school, 1-> apple과 같은 식으로 출력하고, 감정은 0 -> happy, 1-> angry 와 같은 방식 으로 출력함으로써, 현재 스텝의 음성 신호에 대한 단어 및 감정 정보를 판별할 수 있다. 얼굴 리그 포인트 이동부는 상기 단어 및 감정 판별부에서 출력된 단어 및 감정 정보에 기초하여 3D 얼 굴 모델의 뼈대를 구성하여 움직임을 나타내는 리그 포인트(rig point)를 이동시키는 수단이다. 이에 의해, 입력되는 음성 신호에 상응하도록 3D 얼굴 모델의 입모양을 바꾸고 표정을 표현할 수 있다. 여기에서, 3D 얼굴 모델은 3D 얼굴 모델 저장부에 각 사용자별로 미리 저장되어 있다. 또한, 각각의 3D 얼굴 모델은 리그 포인트가 미리 설정되어 있으며, 특히 입 주변, 눈썹, 코, 귀 등의 발화시에 입 모양과 감정 표현에 영향을 미치는 위치에 설정된다. 예컨대, 얼굴 코딩 액션 시스템(Facial Coding Action System)의 44개 얼굴 세부 표정을 조합하여 감정과 매치 시켜 3D 얼굴 모델의 입모양과 표정을 표현할 수 있다. 이러한 리그 포인트를 설정하고 이동시키는 기술 자체는 종래 기술에 의해 알려져 있는 것이고 본 발명의 직접 적인 목적은 아니므로 여기서는 상세 설명은 생략한다. 3D 얼굴 모델 저장부는 3D 얼굴 모델을 저장하는 수단이다. 전술한 바와 같이, 각각의 3D 얼굴 모델은 사용자별로 리그 포인트가 미리 설정되어 있다. 한편, 도시하지는 않았으나, 상기와 같은 시스템은 3D 얼굴 모델을 표시하는 실시간 런타임 서비스에 적용 되는 경우, 실시간으로 입력되는 음성 신호에 상응하여 3D 얼굴 모델을 표현하기 위한 실시간 런타임 수행부를 더 포함할 수 있다. 예컨대, 게임 서비스인 경우, 최소 30프레임 이상의 속도를 유지하며 예측된 결과를 런타임 서비스 내에 전송할 수 있으며, 단어 및 감정 정보에 상응하도록 리그 포인트를 런타임 서비스 내에서 이동시키 면서 얼굴 모양과 감정을 표현할 수 있다. 도 5는 도 1 내지 도 4를 참조하여 설명한 본 발명의 시스템에서 수행되는 방법의 일실시예를 나타낸 흐름 도이다. 도 5를 참조하면, 우선 전술한 바와 같이 음성 신호 저장부가 음성 신호를 획득하고 저장한다(S100). 다음으로, 발화 처리부가 전술한 바와 같은 방식으로 입력되는 음성 신호에 대한 특징 벡터를 생성한다 (S110,S120). 전술한 바와 같이, 발화 처리부는 예컨대 현재 스텝의 음성 신호의 2 스텝 이전의 음성 신호와 1 스텝 이전 의 음성 신호까지 함께 고려하여 특징 벡터를 생성할 수 있다. 다음으로, 단어 및 감정 판별부는, 전술한 바와 같이 발화 처리부에서 생성된 특징 벡터에 기초하여 입 력되는 음성 신호에 대한 단어와 감정을 분류하여 단어 및 감정 정보를 출력한다(S130). 그리고, 얼굴 리그 포인트 이동부는, 단어 및 감정 판별부에서 출력된 단어 및 감정 정보에 기초하여 3D 얼굴 모델의 리그 포인트를 이동시킴으로써, 3D 얼굴 모델의 입 모양과 감정을 표현한다(S140). 이상에서, 본 발명에 의한 바람직한 실시예를 참조하여 본 발명을 설명하였으나, 본 발명은 상기 실시예에 한정 되는 것이 아니며 첨부한 청구범위 및 도면에 의해 파악되는 본 발명의 범위 내에서 다양한 수정 및 변형 실시 가 가능함은 물론이다. 예컨대, 상기 실시예에서 3D 얼굴 모델을 예로 들어 설명하였으나, 2D 얼굴 모델에도 본 발명을 그대로 적용할 수 있음은 물론이다.부호의 설명 100...캐릭터의 입 모양과 표정을 실시간으로 표시하는 시스템 10...음성 신호 저장부 20...발화 처리부 21...2 스텝 이전 발화 처리부 22...1 스텝 이전 발화 처리부 23...현재 스텝 발화 처리부 30...단어 및 감정 판별부 40...얼굴 리그 포인트 이동부 50...3D 얼굴 모델 저장부"}
{"patent_id": "10-2022-0162973", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 인공 지능에 기반하여 음성 신호에 상응하여 3D 얼굴 모델의 입 모양과 표정을 실시간으로 표시하는 시 스템의 전체적인 구성을 나타낸 도면이다. 도 2는 발화 처리부의 구성을 나타낸 도면이다. 도 3은 GRN의 구성의 일예를 나타낸 것이다. 도 4는 멀티 헤드 어텐션 레이어의 이례를 나타낸 도면이다. 도 5는 도 1 내지 도 4를 참조하여 설명한 본 발명의 시스템에서 수행되는 방법의 일실시예를 나타낸 흐름 도이다."}
