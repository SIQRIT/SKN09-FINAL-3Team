{"patent_id": "10-2022-0033925", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0136335", "출원번호": "10-2022-0033925", "발명의 명칭": "강화학습 기반 다중 드론 네트워크 협업 운용 계획 생성 방법 및 장치", "출원인": "한국전자통신연구원", "발명자": "최의환"}}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "(a) 강화학습 하이퍼파라미터를 정의하고, 상기 정의된 하이퍼파라미터에 따라 MADDPG 알고리즘을 기반으로 각드론 에이전트별 액터 신경망을 학습시키는 단계;(b) 다중 드론 네트워크 임무 정보를 기초로 마르코프 게임 정식화 정보를 생성하고, 상기 정식화 정보를 기초로 상기 학습된 액터 신경망을 이용하여 상태-행동 이력 정보를 생성하는 단계; 및(c) 상기 상태-행동 이력 정보를 기초로 다중 드론 네트워크 운용 계획을 생성하는 단계;를 포함하는 강화학습 기반 다중 드론 네트워크 운용 계획 생성 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 다중 드론 네트워크 임무 정보는,기지국에 관한 정보, 표적지점에 관한 정보, 드론 에이전트에 관한 정보, 통신에 관한 정보 및 임무 종료 조건을 포함하는 것인 강화학습 기반 다중 드론 네트워크 운용 계획 생성 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 (b) 단계는,(b1) 상기 임무 정보를 기초로 상기 정식화 정보를 생성하는 단계;(b2) 상기 정식화 정보에 따라 드론 에이전트별 상태를 초기화하는 단계;(b3) 드론 에이전트별 상태에 기초하여 드론 에이전트별 관측을 획득하는 단계;(b4) 상기 관측을 상기 액터 신경망에 입력하여 드론 에이전트별 행동을 추론하는 단계;(b5) 상기 상태 및 상기 행동을 기초로 드론 에이전트별 다음 상태를 획득하는 단계; 및(b6) 상기 다음 상태를 기초로, 상기 임무 정보에 포함되어 있는 임무 종료 조건에 도달했는지 여부를판단하고, 도달하지 않은 경우 (b3) 내지 (b5) 단계를 반복하고, 도달한 경우 상기 상태 및 상기 행동을 종합하여 상기 상태-행동 이력 정보를 생성하는 단계;를 포함하는 것인 강화학습 기반 다중 드론 네트워크 운용 계획 생성 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 상태-행동 이력 정보는,의사결정 시점별 드론의 위치 정보를 포함하고,상기 (c) 단계는,상기 위치 정보를 기초로 상기 운용 계획에 포함되는 드론의 비행경로 정보를 생성하는 것인 강화학습 기반 다중 드론 네트워크 운용 계획 생성 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 상태-행동 이력 정보는,의사결정 시점별 드론의 임무 시간과 드론의 위치 정보를 포함하고,상기 (c) 단계는,공개특허 10-2023-0136335-3-상기 임무 시간과 상기 위치 정보를 기초로 상기 운용 계획에 포함되는 드론의 속도 정보를 생성하는 것인 강화학습 기반 다중 드론 네트워크 운용 계획 생성 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 상태-행동 이력 정보는,의사결정 시점별 네트워크 토폴로지 이력 정보를 포함하고,상기 (c) 단계는,상기 토폴로지 이력 정보를 기초로 상기 운용 계획에 포함되는 토폴로지 정보를 생성하는 것인 강화학습 기반 다중 드론 네트워크 운용 계획 생성 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 상태-행동 이력 정보는,의사결정 시점별 드론의 임무수행의도 및 드론의 행동을 포함하고,상기 (c) 단계는,상기 임무수행의도 및 상기 드론의 행동을 기초로 상기 운용 계획에 포함되는 임무수행 정보를 생성하는 것인 강화학습 기반 다중 드론 네트워크 운용 계획 생성 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "(a) 강화학습 하이퍼파라미터를 정의하는 단계;(b) 마르코프 게임 상태를 초기화하고, 상기 상태를 기초로 드론 에이전트별 관측을 획득하는 단계;(c) 상기 정의된 하이퍼파라미터와 상기 상태를 기초로 MADDPG 알고리즘을 이용하여 드론 에이전트별로 관측,행동, 보상 및 다음 관측을 포함하는 튜플 데이터를 생성하고, 상기 튜플 데이터를 리플레이 버퍼에 저장하는단계;(d) 상기 리플레이 버퍼에서 랜덤 샘플링으로 튜플 데이터의 미니배치를 추출하는 단계; 및(e) 상기 미니배치를 기초로 드론 에이전트별 액터 신경망을 업데이트하는 단계;를 포함하는 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 (e) 단계 이후에,(f) 반복 회수를 1 증가시키고, 상기 반복 회수가 설정된 상한에 도달했는지 여부를 판단하여, 도달하지 않은경우 상기 (c) 단계 내지 (e) 단계를 반복하는 단계를 더 포함하는 MADDPG 알고리즘 기반의 다중 드론 에이전트강화학습 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 (f) 단계 이후에,(g) 소정의 학습 종료 조건에 도달했는지 여부를 판단하여, 도달한 경우 학습을 종료하고, 도달하지 않은 경우상기 (b) 단계 내지 (f) 단계를 반복하는 단계를 더 포함하는 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항에 있어서, 상기 (c) 단계는,상기 상태를 기초로 상기 관측을 획득하고,공개특허 10-2023-0136335-4-상기 관측을 기초로 상기 행동을 추론하며,상기 상태 및 상기 행동을 기초로 상기 보상 및 드론 에이전트별 다음 상태를 획득하며,상기 다음 상태를 기초로 상기 다음 관측을 획득하는 것인 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항에 있어서, 상기 하이퍼파라미터는,상기 액터 신경망에 대한 파라미터를 포함하고,상기 (c) 단계는,상기 액터 신경망을 이용하여 상기 행동을 추론하는 것인 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8항에 있어서, 상기 하이퍼파라미터는,다중 드론의 통신 네트워크에 관한 토폴로지 모델 및 통신 비용 모델을 포함하고,상기 (c) 단계는,상기 상태 및 상기 행동을 기초로 상기 토폴로지 모델 및 통신 비용 모델을 이용하여 상기 통신 네트워크의 통신 비용을 산출하고, 상기 상태, 상기 행동 및 상기 통신 비용을 기초로 상기 보상을 산출하는 것인 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8항에 있어서, 상기 상태는,임무 시간, 드론 에이전트별 위치 벡터, 다중 드론 통신 네트워크 토폴로지, 다중 드론 통신 네트워크의 연결성및 드론 에이전트별 임무 완료 여부를 포함하는 것인 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제8항에 있어서, 상기 관측은,현재 임무 시간, 드론 에이전트의 위치, 드론 에이전트의 현재 임무수행의도, 다중 드론의 통신 네트워크 연결성, 지상국의 상대적 위치 좌표, 표적지점의 상대적 위치 좌표, 드론 에이전트의 임무 완료 여부 및 다른 드론에이전트의 상대적 위치 좌표를 포함하며,상기 임무수행의도는,다른 드론 에이전트 간의 통신 중계, 상기 드론 에이전트의 임무 수행, 다른 드론 에이전트가 있는 방향으로 이동하기 및 지상국 방향으로 이동하기 중 어느 하나인 것인 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제8항에 있어서, 상기 보상은,다중 드론 통신 네트워크의 연결성, 상기 네트워크의 통신 비용 및 드론 에이전트별 임무 완료 여부를 기초로정의되는 것인 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법.공개특허 10-2023-0136335-5-청구항 17 제8항에 있어서, 상기 드론 에이전트는,매 의사결정 시점마다 하나의 임무수행의도를 가지며,상기 행동은,단순 이동방향 결정 행동 및 의도명시적 결정 행동 중 어느 하나의 행동에 해당하며,상기 단순 이동방향 결정 행동은 현재의 임무수행의도를 다음 의사결정시점에서 변경하지 않고 이동방향만을 결정하는 행동이고,상기 의도명시적 결정 행동은 다음 의사결정시점의 임무수행의도를 명시적으로 선택하는 행동이며,상기 임무수행의도는,다른 드론 에이전트 간의 통신 중계, 상기 드론 에이전트의 임무 수행, 다른 드론 에이전트가 있는 방향으로 이동하기 및 지상국 방향으로 이동하기 중 어느 하나인 것인 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "강화학습 하이퍼파라미터와 다중 드론 네트워크 임무 정보를 입력받는 입력부;상기 강화학습 하이퍼파라미터에 따라 MADDPG 알고리즘을 이용하여 각 드론 에이전트별 액터 신경망을 학습시키는 학습부; 및상기 다중 드론 네트워크 임무 정보를 기초로 상기 학습된 액터 신경망을 이용하여 상태-행동 이력 정보를 생성하고, 상기 상태-행동 이력 정보를 기초로 다중 드론 네트워크 운용 계획을 생성하는 계획 생성부;를 포함하는 강화학습 기반 다중 드론 네트워크 운용 계획 생성기."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서, 상기 학습부는,상기 강화학습 하이퍼파라미터에 따라 MADDPG 알고리즘을 이용하여 드론 에이전트별 관측, 행동, 보상 및 다음관측을 포함하는 튜플 데이터를 생성하고, 상기 튜플 데이터의 미니배치를 기초로 상기 액터 신경망을 학습시키는 것인 강화학습 기반 다중 드론 네트워크 운용 계획 생성기."}
{"patent_id": "10-2022-0033925", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제18항에 있어서, 상기 계획 생성부는,상기 임무 정보를 기초로 드론 에이전트별 상태를 초기화하고,상기 상태에 기초하여 드론 에이전트별 관측을 획득하며,상기 관측을 상기 학습된 액터 신경망에 입력하여 드론 에이전트별 행동을 추론하며,상기 상태 및 상기 행동을 기초로 상기 상태를 천이시키며,상기 상태를 기초로 상기 임무 정보에 포함되어 있는 임무 종료 조건에 도달하였는지 여부를 판단하고, 임무 종료 조건에 도달한 것으로 판단한 경우 상기 상태 및 상기 행동의 이력을 종합하여 상기 상태-행동 이력 정보를생성하는 것인 강화학습 기반 다중 드론 네트워크 운용 계획 생성기."}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 강화학습 기반 다중 드론 네트워크 운용 계획 생성 방법 및 장치에 관한 것이다. 본 발명에 따른 강화 학습 기반 다중 드론 네트워크 운용 계획 생성 방법은, 강화학습 하이퍼파라미터를 정의하고, 상기 정의된 하이 퍼파라미터에 따라 MADDPG 알고리즘을 기반으로 각 드론 에이전트별 액터 신경망을 학습시키는 단계와, 다중 드 론 네트워크 임무 정보를 기초로 마르코프 게임 정식화 정보를 생성하고, 상기 정식화 정보를 기초로 상기 학습 된 액터 신경망을 이용하여 상태-행동 이력 정보를 생성하는 단계와, 상기 상태-행동 이력 정보를 기초로 다중 드론 네트워크 운용 계획을 생성하는 단계를 포함한다."}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 네트워크로 연결된 복수 개의 드론(무인기, 비행로봇 등)이 다중 데이터 수집 임무를 수행하는 것과 관련, 강화 학습 기반의 드론 네트워크 운용 계획 생성 방법 및 장치에 관한 것이다. 본 발명에 따른 방법 및 장치는 멀티에이전트 강화학습을 위한 관측 및 행동 모델, 다중 드론 네트워크 통신 비용 및 보상 모델, 신경망 기반 운용 계획 생성 알고리즘(neural operational planner)을 활용하여 구현된다."}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "드론 제작 기술 및 비행 제어 기술의 발전에 따라, 드론에 고성능의 관측/센싱 임무 장비(mission equipment) 및 통신 장치를 탑재할 수 있게 되었다. 이러한 임무 장비를 탑재한 드론을 다수 운용할 경우, 다수의 임무 지 점에 대한 저비용-고효율의 데이터 수집(관측)이 가능해지며, 또한 드론 자체를 이동성 있는 통신 중계 장치로 활용해 다중 드론 시스템의 운용 범위를 극대화할 수 있다. 그러나 다양한 임무 장비를 장착한 다중 드론 간의 협업 시너지를 극대화할 수 있는 효과적인 드론 운용 계획을 수립하는 것은 어려움이 따른다. 특히 드론 통신의 경우, 기지국-드론 및 드론 간 통신 거리의 제약이 있기 때문에, 운용 계획 수립 시 원활한 통신링크 유지를 위 해 통신거리 제약을 엄격히 고려하여야 한다. 또한, 드론 운용 계획은 일반적으로 숙련된 드론 관제 인원이 많 은 시간을 들여 설계하여야 한다. 즉, 데이터 수집 임무와 통신 중계 임무를 수행하는 다중 드론 네트워크에 대한 운용 계획을 수립하는 것은 이 동성이 큰 드론 고유 특성 및 엄격한 통신 제약 등으로 인해 난도(難度)가 높다."}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기와 같은 어려움을 해결하고 드론 관제 인원의 운용 부담을 경감시키기 위하여, 데이터 수집과 통 신 중계에 관한 협업 임무를 수행하는 인공지능 기반 알고리즘을 도입하여 다중 드론 간의 네트워크 상 협업 운 용 계획을 준실시간으로 자동 생성하는 방법 및 장치를 제공하는 것을 그 목적으로 한다. 본 발명의 목적은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 또 다른 목적들은 아래의 기재로 부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 강화학습 기반 다중 드론 네트워크 운용 계획 생성 방 법은, (a) 강화학습 하이퍼파라미터를 정의하고, 상기 정의된 하이퍼파라미터에 따라 MADDPG 알고리즘을 기반으 로 각 드론 에이전트별 액터 신경망을 학습시키는 단계; (b) 다중 드론 네트워크 임무 정보를 기초로 마르코프 게임 정식화 정보를 생성하고, 상기 정식화 정보를 기초로 상기 학습된 액터 신경망을 이용하여 상태-행동 이력 정보를 생성하는 단계; 및 (c) 상기 상태-행동 이력 정보를 기초로 다중 드론 네트워크 운용 계획을 생성하는 단계;를 포함한다. 본 발명의 일 실시예에서, 상기 다중 드론 네트워크 임무 정보는, 기지국에 관한 정보, 표적지점에 관한 정보, 드론 에이전트에 관한 정보, 통신에 관한 정보 및 임무 종료 조건을 포함할 수 있다. 본 발명의 일 실시예에서, 상기 (b) 단계는, (b1) 상기 임무 정보를 기초로 상기 정식화 정보를 생성하는 단계; (b2) 상기 정식화 정보에 따라 드론 에이전트별 상태를 초기화하는 단계; (b3) 드론 에이전트별 상태에 기초하 여 드론 에이전트별 관측을 획득하는 단계; (b4) 상기 관측을 상기 액터 신경망에 입력하여 드론 에이전트별 행 동을 추론하는 단계; (b5) 상기 상태 및 상기 행동을 기초로 드론 에이전트별 다음 상태를 획득하는 단계; 및 (b6) 상기 다음 상태를 기초로, 상기 임무 정보에 포함되어 있는 임무 종료 조건에 도달했는지 여부를 판단하고, 도달하지 않은 경우 (b3) 내지 (b5) 단계를 반복하고, 도달한 경우 상기 상태 및 상기 행동을 종합하 여 상기 상태-행동 이력 정보를 생성하는 단계;를 포함할 수 있다. 본 발명의 일 실시예에서, 상기 상태-행동 이력 정보는, 의사결정 시점별 드론의 위치 정보를 포함할 수 있다. 이 경우, 상기 (c) 단계는, 상기 위치 정보를 기초로 상기 운용 계획에 포함되는 드론의 비행경로 정보를 생성 하는 것일 수 있다. 본 발명의 일 실시예에서, 상기 상태-행동 이력 정보는, 의사결정 시점별 드론의 임무 시간과 드론의 위치 정보 를 포함할 수 있다. 이 경우, 상기 (c) 단계는, 상기 임무 시간과 상기 위치 정보를 기초로 상기 운용 계획에 포함되는 드론의 속도 정보를 생성하는 것일 수 있다. 본 발명의 일 실시예에서, 상기 상태-행동 이력 정보는, 의사결정 시점별 네트워크 토폴로지 이력 정보를 포함 할 수 있다. 이 경우 상기 (c) 단계는, 상기 토폴로지 이력 정보를 기초로 상기 운용 계획에 포함되는 토폴로지 정보를 생성하는 것일 수 있다. 본 발명의 일 실시예에서, 상기 상태-행동 이력 정보는, 의사결정 시점별 드론의 임무수행의도 및 드론의 행동 을 포함할 수 있다. 이 경우 상기 (c) 단계는, 상기 임무수행의도 및 상기 드론의 행동을 기초로 상기 운용 계 획에 포함되는 임무수행 정보를 생성하는 것일 수 있다. 그리고, 본 발명의 일 실시예에 따른 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법은, (a) 강화학 습 하이퍼파라미터를 정의하는 단계; (b) 마르코프 게임 상태를 초기화하고, 상기 상태를 기초로 드론 에이전트 별 관측을 획득하는 단계; (c) 상기 정의된 하이퍼파라미터와 상기 상태를 기초로 MADDPG 알고리즘을 이용하여 드론 에이전트별로 관측, 행동, 보상 및 다음 관측을 포함하는 튜플 데이터를 생성하고, 상기 튜플 데이터를 리 플레이 버퍼에 저장하는 단계; (d) 상기 리플레이 버퍼에서 랜덤 샘플링으로 튜플 데이터의 미니배치를 추출하 는 단계; 및 (e) 상기 미니배치를 기초로 드론 에이전트별 액터 신경망을 업데이트하는 단계;를 포함한다. 본 발명의 일 실시예에서, 상기 (e) 단계 이후에, (f) 반복 회수를 1 증가시키고, 상기 반복 회수가 설정된 상 한에 도달했는지 여부를 판단하여, 도달하지 않은 경우 상기 (c) 단계 내지 (e) 단계를 반복하는 단계를 더 포 함할 수 있다. 본 발명의 일 실시예에서, 상기 (f) 단계 이후에, (g) 소정의 학습 종료 조건에 도달했는지 여부를 판단하여, 도달한 경우 학습을 종료하고, 도달하지 않은 경우 상기 (b) 단계 내지 (f) 단계를 반복하는 단계를 더 포함할 수 있다. 본 발명의 일 실시예에서, 상기 (c) 단계는, 상기 상태를 기초로 상기 관측을 획득하고, 상기 관측을 기초로 상 기 행동을 추론하며, 상기 상태 및 상기 행동을 기초로 상기 보상 및 드론 에이전트별 다음 상태를 획득하며, 상기 다음 상태를 기초로 상기 다음 관측을 획득하는 것일 수 있다. 본 발명의 일 실시예에서, 상기 하이퍼파라미터는, 상기 액터 신경망에 대한 파라미터를 포함할 수 있다. 이 경 우, 상기 (c) 단계는, 상기 액터 신경망을 이용하여 상기 행동을 추론하는 것일 수 있다. 본 발명의 일 실시예에서, 상기 하이퍼파라미터는, 다중 드론의 통신 네트워크에 관한 토폴로지 모델 및 통신 비용 모델을 포함할 수 있다. 이 경우, 상기 (c) 단계는, 상기 상태 및 상기 행동을 기초로 상기 토폴로지 모델 및 통신 비용 모델을 이용하여 상기 통신 네트워크의 통신 비용을 산출하고, 상기 상태, 상기 행동 및 상기 통 신 비용을 기초로 상기 보상을 산출하는 것일 수 있다. 본 발명의 일 실시예에서, 상기 상태는, 임무 시간, 드론 에이전트별 위치 벡터, 다중 드론 통신 네트워크 토폴 로지, 다중 드론 통신 네트워크의 연결성 및 드론 에이전트별 임무 완료 여부를 포함할 수 있다. 본 발명의 일 실시예에서, 상기 관측은, 현재 임무 시간, 드론 에이전트의 위치, 드론 에이전트의 현재 임무수 행의도, 다중 드론의 통신 네트워크 연결성, 지상국의 상대적 위치 좌표, 표적지점의 상대적 위치 좌표, 드론 에이전트의 임무 완료 여부 및 다른 드론 에이전트의 상대적 위치 좌표를 포함할 수 있다. 이 경우, 상기 임무 수행의도는, 다른 드론 에이전트 간의 통신 중계, 상기 드론 에이전트의 임무 수행, 다른 드론 에이전트가 있는 방향으로 이동하기 및 지상국 방향으로 이동하기 중 어느 하나이다. 본 발명의 일 실시예에서, 상기 보상은, 다중 드론 통신 네트워크의 연결성, 상기 네트워크의 통신 비용 및 드 론 에이전트별 임무 완료 여부를 기초로 정의될 수 있다. 본 발명의 일 실시예에서, 상기 드론 에이전트는, 매 의사결정 시점마다 하나의 임무수행의도를 가질 수 있다. 이 경우, 상기 행동은, 단순 이동방향 결정 행동 및 의도명시적 결정 행동 중 어느 하나의 행동에 해당하며, 상 기 단순 이동방향 결정 행동은 현재의 임무수행의도를 다음 의사결정시점에서 변경하지 않고 이동방향만을 결정 하는 행동이고, 상기 의도명시적 결정 행동은 다음 의사결정시점의 임무수행의도를 명시적으로 선택하는 행동이 며, 상기 임무수행의도는, 다른 드론 에이전트 간의 통신 중계, 상기 드론 에이전트의 임무 수행, 다른 드론 에 이전트가 있는 방향으로 이동하기 및 지상국 방향으로 이동하기 중 어느 하나이다. 그리고, 본 발명의 일 실시예에 따른 강화학습 기반 다중 드론 네트워크 운용 계획 생성기는, 강화학습 하이퍼 파라미터와 다중 드론 네트워크 임무 정보를 입력받는 입력부; 상기 강화학습 하이퍼파라미터에 따라 MADDPG 알 고리즘을 이용하여 각 드론 에이전트별 액터 신경망을 학습시키는 학습부; 및 상기 다중 드론 네트워크 임무 정 보를 기초로 상기 학습된 액터 신경망을 이용하여 상태-행동 이력 정보를 생성하고, 상기 상태-행동 이력 정보 를 기초로 다중 드론 네트워크 운용 계획을 생성하는 계획 생성부;를 포함한다. 본 발명의 일 실시예에서, 상기 학습부는, 상기 강화학습 하이퍼파라미터에 따라 MADDPG 알고리즘을 이용하여 드론 에이전트별 관측, 행동, 보상 및 다음 관측을 포함하는 튜플 데이터를 생성하고, 상기 튜플 데이터의 미니 배치를 기초로 상기 액터 신경망을 학습시킬 수 있다. 본 발명의 일 실시예에서, 상기 계획 생성부는, 상기 임무 정보를 기초로 드론 에이전트별 상태를 초기화하고, 상기 상태에 기초하여 드론 에이전트별 관측을 획득하며, 상기 관측을 상기 학습된 액터 신경망에 입력하여 드 론 에이전트별 행동을 추론하며, 상기 상태 및 상기 행동을 기초로 상기 상태를 천이시키며, 상기 상태를 기초 로 상기 임무 정보에 포함되어 있는 임무 종료 조건에 도달하였는지 여부를 판단하고, 임무 종료 조건에 도달한 것으로 판단한 경우 상기 상태 및 상기 행동의 이력을 종합하여 상기 상태-행동 이력 정보를 생성할 수 있다."}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "종래 기술에 따를 때, 드론 운용 계획 수립 시 숙련된 관제 인원이 개입되거나, 복잡한 시뮬레이션/최적화 툴이 사용되었으나, 본 발명에 따르면, 강화학습 기법을 통해 인공지능이 준최적(sub-optimal)의 운용 계획을 수립하 는 방법을 스스로 학습할 수 있다는 효과가 있다. 본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은"}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 한편, 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포 함한다. 명세서에서 사용되는 \"포함한다(comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성소자, 단계, 동작 및/또는 소자는 하나 이상의 다른 구성소자, 단계, 동작 및/또는 소자의 존재 또는 추가를 배제하지 않는다. 본 발명을 설명함에 있어서, 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 이하, 본 발명의 실시예를 첨부한 도면들을 참조하여 상세히 설명한다. 본 발명을 설명함에 있어 전체적인 이해 를 용이하게 하기 위하여 도면 번호에 상관없이 동일한 수단에 대해서는 동일한 참조 번호를 사용하기로 한다.도 1은 본 발명의 적용 대상인 다중 드론 시스템의 임무 개요에 관한 도면이다. 단일 드론이 담당하기 어려운 광범위한 임무 영역에 M개의 \"표적 지점\"들이 랜덤하게 분포한다. 각 표적 지점들은 EO/IR(Electro- Optical/Infrared)과 같은 드론 영상 촬영 포인트일 수도 있고, 대기 오염물질 측정 등을 위한 특수 센서가 필 요한 데이터 수집 대상 지점일 수도 있다. 제한된 드론 체공 시간 안에 M개의 표적 지점에 대한 데이터 수집을 효과적으로 마무리하기 위해 복수 개(N개)의 임무 드론을 투입할 수 있다. 각 드론은 표적 지점에 대한 데이터 를 수집하거나(sensing), UAV 네트워크를 이용하여 지상 관제국(GCS, Ground Control Station)과 통신하거나, 드론 간 통신 중계(relay) 작업을 수행한다. 도 2는 상술한 다중 드론 시스템 하에서 임무를 수행하는 임무드론의 구성을 나타낸 블록도이다. 임무드론은 드 론용 비행제어기(flight controller), 구동기(actuator)는 물론, 미션 컴퓨터(mission computer)를 추가적으로 탑재하고 있다. 미션 컴퓨터에는 '통신 모듈(communication module)'과 '데이터 수집 장비(data sensing equipment)' 등의 주요 임무 장비가 연결될 수 있다. 통신 모듈은 드론에 온보드로 장착가능한 통신 모뎀, 라우 터 및 액세스 포인트(AP, Access Point) 등을 의미하며, 드론 자신과 지상 관제국 사이의 통신 링크(air-to- ground link)와 드론-드론 사이의 통신 링크(air-to-air link)를 구축해 공중 통신 중계(aerial relay)를 가능 하게 하는 용도로 사용된다. 데이터 수집 장비는 표적지점들에 대한 데이터 수집을 가능하게 하는 온보드 임무 장비로서, 드론 탑재 카메라, 열화상 카메라, 미세먼지 측정 센서 등 다양한 드론 임무 형태에 사용되는 임무 장비들을 포함할 수 있다. N개의 다중 임무드론들은 서로 협력하여 M개의 표적지점들에 대한 데이터 수집을 수행하여야 한다. 최대한 빨 리, 더 많은 표적지점에 대한 데이터 수집을 하기 위해, 각 임무드론들은 서로 흩어져서 서로 다른 표적지점에 대한 데이터 수집을 수행하는 것을 일차적인 임무 목표로 한다. 또한, 모든 임무드론들은 지상관제국(또는 지상기지국)과의 통신을 유지하며, 자신들이 수집한 데이터를 실시 간으로 지상국에 보내는 것을 이차적인 임무 목표로 한다. 그러나 각 임무드론들은 탑재하고 있는 통신모듈의 성능 한계로 인해, 최대 통신 가능거리의 제한이 있다. 모든 드론은 지상국과의 통신 링크를 유지해야 하는데, 일반적으로 드론-지상국간 통신(air-to-ground link)의 통신 거리가 드론간 통신(air-to-air link)의 통신 거 리보다 짧다. 통신 중계 기능을 보유한 각 드론은 공중 애드혹 네트워크(aerial ad-hoc network)를 구축해 흩어 져 있는 모든 드론이 하나의 네트워크로 연결될 수 있도록 상호 협력한다. 각 드론은 통신 링크를 유지하면서 임무 수행이 가능한 영역을 확장해야 한다. 상술한 다중 드론 시스템에 속해 있는 다중 드론의 통신 중계 및 데이터 수집 협업 임무를 최적화하는 경우, 드 론의 통신 가능 거리 제약, 애드혹 네트워크 토폴로지의 가변성, 드론의 이동성 등으로 인하여 계산 복잡도가 높으므로 기존 기법으로 운용 계획 해를 도출할 경우 과도한 연산 시간이 소요될 수 있다. 본 발명은 심층신경 망을 통한 멀티에이전트 강화학습 기법을 적용해 이러한 복잡한 다중 드론 협업 임무에 대한 준최적 운용 계획 을 실시간으로 생성할 수 있는 방법과 장치를 제시한다. 강화학습 기법을 적용하여 상기 다중 드론 협업 임무에 대한 운용 계획을 생성하기 위해서는, 먼저 본 임무 상 황을 멀티에이전트 마르코프 게임(Markov game)으로 정식화하는 과정이 필요하다. 마르코프 게임은 순차적 의사 결정 문제인 마르코프 결정 프로세스(MDP, Markov Decision Process)를 멀티에이전트 의사결정 문제로 확장한 형태이다. 마르코프 게임에서, 각 N개의 에이전트들은 전체 시스템 상태를 지역 관측(local observation)으로 인식하고, 각자 보유하고 있는 분산 지역 정책(distributed local policy)에 따라 지역 행동(local action)을 수행한다. 임무수행의도 다중 드론 네트워크의 통신 중계 및 데이터 수집 협업 임무 상황에 대하여, 강화학습을 통해 에이전트를 효과적 으로 훈련시키기 위해서는 관측 모델과 행동 모델, 그리고 보상 모델(reward model)을 상기 상황에 적합하게 정 의하는 것이 매우 중요하다. 본 발명에서는 상기 행동 모델과 관련하여 각 드론 에이전트(본 발명에서, '드론 에이전트'는 '드론'으로 약칭될 수 있음)에 부여되는 '임무수행의도(task-intent, TI)'라는 개념을 정의하여 원 활한 학습을 유도하고자 하였다. 도 3은 드론 에이전트의 임무수행의도를 설명하기 위한 도면이다. 도 3에 도시한 바와 같이, 각각의 드론 에이 전트는 매 의사결정 시점(decision step)마다 다음 중 하나의 임무수행의도를 가진다. 드론 에이전트는 매 의사 결정 시점마다 직전 임무수행의도를 변경하지 않고 유지하거나, 동일하거나 다른 임무수행의도를 선택할 수 있 다.① TIR: 현 위치에서의 통신중계 임무(task)를 우선함 ② TIT(m): 특정 m번째 표적지점에 대한 데이터 수집 임무를 우선함 ③ TIA(j): 다른 드론 에이전트(UAV(j))가 있는 방향으로 이동하는 것을 우선함 ④ TIB: 지상국 방향으로 이동/복귀하는 것을 우선함 본 발명은 상술한 바와 같은 '임무수행의도'에 대한 정의를 토대로, 마르코프 게임(Markov game)의 구성요소인 관측모델, 행동모델 및 보상모델을 다중 드론 네트워크 상황에 맞게 정의하였다. 상태(state) 관측모델의 정의에 앞서, 마르코프 게임에 대한 상태(state) 정의가 필요하다. 다중 드론 네트워크의 통신중계- 데이터수집 협업 임무를 마르코프 게임 문제로 정식화하기 위해, 다중 드론의 위치, 드론 통신 상태 및 전체 임 무 진행 상황 등을 종합하여 '상태'로 정의한다. 상기 정식화에서 다루는 마르코프 게임의 상태(state, s)는 아 래와 같이 나타낼 수 있다. s=<t,{pi},c,η,{δm},{τi} > 여기서 t는 임무 시간, {pi}(i=1,…,N)는 각 드론의 수평 위치 좌표 벡터의 집합(pi=[px,i,py,i]), c는 다중 드론 의 통신 네트워크 토폴로지 정보(예: 트리구조, 일렬구조, 메쉬 네트워크 등), η는 드론 통신 네트워크의 연결 성 정보 (통신이 원활한 경우는 0, 통신 단절 위험이 있는 경우는 1), {δm}(m=1,...,M)은 각 표적지점(데이터 수집지점)에 대한 데이터 수집 완료 여부(0:미완료, 1:완료)를 나타내는 플래그의 집합, {τi}는 각 드론 에이 전트들의 임무수행의도의 집합이다. 상기 다중 드론의 통신 네트워크 토폴로지 정보(c)는 토폴로지 모델에 의하여 결정되지만, 토폴로지 모델에 기 초하여 매 시점마다 복수의 드론 및 기지국의 위치를 고려하여 동적으로 변경될 수 있다. 관측 모델 각 드론 에이전트들은 마르코프 게임의 틀에서 상술한 상태(s)를 자신의 입장에서 관측하며, 이러한 로컬 관측 (local observation)을 기준으로 각 드론 에이전트들이 분산적으로 독립적인 의사결정을 수행한다. 원활한 멀티에이전트 강화학습을 위해, i번째 드론 에이전트(이하 UAV(i)로 약칭)가 관측 가능한 상태 정보인 관측 모델(oi: S→O)을 다음과 같이 정의한다. ① 자기 자신에 대한 관측: 현재 임무 시간(t), 자신의 위치(pi), 자신의 현재 임무수행의도(τi) ② 지상국으로부터 획득한 관측: 전체 드론 네트워크의 통신 연결성(η), 자신의 위치 좌표를 기준으로 구한 지 상국(GCS)의 상대적 위치 좌표(pGCS - pi), 지상국과의 상대 거리, 표적지점(TG)들(m=1, ... ,M)의 상대적 위치 좌표(pTG,m - pi), 각 표적지점에 대한 데이터 수집 완료 여부({δm}) ③ 타 드론과의 교신을 통해 획득한 타 드론에 대한 관측: 자신의 위치 좌표를 기준으로 구한 다른 드론 에이전 트(j)의 상대적 위치 좌표(pj-pi) 및 상대 거리 참고로, 지상국과 타 드론의 상대적 위치 좌표를 기초로 상대 거리를 산출할 수 있으나, 본 발명에서는 각 드론 에이전트에 할당된 신경망 학습의 효율을 높이기 위하여 관측 모델(관측 정보)에 상대적 위치 좌표와 함께 상대 거리도 포함하였다. 행동 모델 드론 에이전트(UAV(i))의 행동 모델 Ai는 특정한 임무수행의도를 명시하지 않는 단순 이동방향 결정 행동(aGoTo) 과 다음 의사결정에서 채택할 임무수행의도를 명시적으로 선택하는 의도명시적 결정 행동(aτ)으로 구성된다. 각 드론 에이전트는 매 의사결정 시점마다 이러한 행동들 중 하나를 선택한다. 다음은 상술한 행동 모델(Ai)에 대한 수식 표현이다.Ai={{aGoTo}∪{aτ}} {aGoTo}={astay,a+x,a-x,a+y,a-y} {aτ}={arelay,atoTg1, …, atoTgM,atoUAV,…,atoUAV(N),atoBase} 단순 이동방향 결정 행동(aGoTo)의 경우, 자신의 현재 임무수행의도를 변경하지 않고, x축 또는 y축 방향의 사각 그리드 형태의 단순 이동을 수행한다. 의도명시적 결정 행동(aτ)의 경우, 자신의 다음 임무수행의도를 명시적으 로 선택하고, 이동 역시 해당 임무수행의도에 맞게 수행한다. 표 1은 의도명시적 결정 행동(aτ)에 따른 임무수 행의도(τi) 및 이동 방식에 관한 것이다. 표 1 의도명시적 결정 행동(aτ) 다음 임무수행의도(τi) 이동 방식 arelay TIR 현 위치 유지(호버링) atoTg(m) TIT(m) m번째 표적지점 방향으로 이동 atoUAV(j) TIA(j) UAV(j) 방향으로 이동 atoBase TIB 지상국 방향으로 이동 보상 모델 드론 에이전트(UAV(i))의 보상 모델(ri: S×Ai→R)은 수학식 1과 같이 정의될 수 있다. 기본적으로, 각 드론 에 이전트는 전체 임무 종료 시간이 짧을수록 높은 보상을 획득하고, 통신 네트워크의 상태에 따라 페널티를 받는 다. 수학식 1"}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 1에서, T는 최대 의사결정시점, k는 현재 의사결정시점, kf,i는 드론 에이전트 UAV(i)가 모든 임무를 수 행하고 기지국으로 복귀한 시점, nk는 현재 시점(k)에서 전체 다중 드론에 의해 데이터수집이 끝난 표적지점의 개수이다. 예를 들어, k=10 시점에서 드론 에이전트 UAV, UAV이 각각 표적지점 TG와 TG에 대한 데 이터수집을 동시에 종료했을 경우, n10은 2가 된다. η는 관측 모델 요소 중 하나인 현재 네트워크 통신 연결성 (η이 0이면 원활, η이 1이면 위험)을 나타낸다. 그리고, εrJcomm(<1)은 애드혹 통신네트워크의 전반적 통신성 능을 반영한 페널티 항이다. 여기에서 Jcomm은 다중 드론 네트워크의 통신 비용이며, εr은 통신 페널티 정규화 계수이다. εr은 강화학습 과정의 안정화를 위하여, 통신 비용에 의한 페널티 항(εrJcomm)의 누적량의 절대값이 1 미만이 되도록 고안된 것이다. 상술한 보상모델에 따라 각 드론 에이전트들은 전체 M개의 표적지점에 대한 데이터 수집 임무를 성공한 시점과, 기지 복귀를 포함한 전체 임무 종료 시점이 단축될수록 더 큰 보상을 얻는다. 그러나 현재 시점(k)에서 전체 다중 드론 및 지상국의 상대적 위치에 의해 비효율적인 애드혹 네트워크가 생성되거나, 또는 통신링크가 단절될 경우 페널티를 받는다. 수학식 1에서, 통신 두절 위험이 있는 경우(η=1) 상기 페널티는 1이 되고, 통신 이 원활한 경우(η=0) 상기 페널티는 통신 비용에 따라 정해지는데, 1 미만의 값이 된다. 다중 드론 네트워크의 통신 비용 Jcomm은 기존에 알려진 통신 네트워크 토폴로지 모델 및 통신 비용 모델을 조합 해 연산된다. 통신 페널티 정규화 계수 εr은 강화학습 과정에서의 안정화를 위해 통신 페널티의 누적값(최초 시점(k=0)부터 최대 의사결정시점(k=T)까지의 페널티 누적값)이 1이 넘지 않도록 설계된다. 페널티 누적의 종료 시점의 기준을 임무 종료 시간(k=kf,i) 대신 최대 의사결정시점(k=T)으로 삼은 이유는 학습의 안정성을 위한 것 이다. 임무 종료 시간까지의 페널티 누적값을 학습에 활용할 경우 임무 종료 시간이 학습 과정 중에 계속적으로 변하여 학습 과정이 불안정하게 될 수 있기 때문이다. 기술적/이론적으로 가능한 통신 비용 Jcomm의 최대치(Jcomm,max)를 알고 있을 경우, εr=1/(Jcomm,max * (T+1))을 적용 할 수 있다. 다중 드론 네트워크 통신 비용 Jcomm을 산출할 수 있는 모델로는 아래와 같은 기존 모델들을 고려할 수 있다. 본 발명은 다중 드론 네트워크 통신 비용을 산출하기 위한 통신 네트워크 토폴로지 모델 및 통신 비용 모델에 관하여 제한을 두지 않는다. - 다중 드론 통신 네트워크 비용 모델: GMC(Global Message Connectivity) - 통신 네트워크 토폴로지: 최소 신장 트리(minimum spanning tree) - 통신 환경 모델: 자유전파모델, 밀집도심모델 등 본 발명에 따른 다중 드론 네트워크 운용 계획 생성 방법 및 다중 드론 네트워크 운용 계획 생성기는, 강화학습 알고리즘(MADDPG)에 기반하면서도 상술한 임무수행의도, 마르코프 게임의 상태(state), 관측모델(oi), 행동모델 (Ai), 보상모델(ri)을 적용함으로써, 드론 네트워크 상에서 원활한 통신을 유지하면서도 최단 시간 내에 표적지 점에 대한 데이터 수집을 완료할 수 있는 계획을 자동으로 생성한다. 도 4는 본 발명의 일 실시예에 따른 다중 드론 네트워크 운용 계획 생성 방법을 설명하기 위한 흐름도이다. 본 발명의 일 실시예에 따른 다중 드론 네트워크 운용 계획 생성 방법은 S120 단계, S140 단계 및 S160 단계를 포함하며, 다중 드론 네트워크 운용 계획 생성기에 의해 수행될 수 있다. S120 단계는 다중 드론 네트워크 운용 계획 생성기가 각 드론 에이전트별로 액터 신경망을 학습시키는 단계이고, S140 단계는 다중 드론 네 트워크 운용 계획 생성기가 학습된 액터 신경망을 활용하여 상태-행동 이력 정보를 생성하는 단계이며, S160 단계는 다중 드론 네트워크 운용 계획 생성기가 상태-행동 이력 정보를 후처리하여 다중 드론 네트워 크 운용 계획을 생성하는 단계이다. 만약, 학습된 액터 신경망을 다시 사용하여 다중 드론 네트워크 운용 계획 을 생성하는 경우라면, 상술한 단계 중에서 S140 단계와 S160 단계만 수행한다. 하기에 각 단계의 수행 내용에 대해 구체적으로 설명한다. S120 단계는 다중 드론 네트워크 운용 계획 생성기가 강화학습 기법을 이용하여, 다중 드론 네트워크에 속 한 각 드론 에이전트의 행동을 추론하는 모델인 액터(Actor) 신경망을 학습시키는 단계이다. 본 단계에 대하여 는 도 5를 참조하여 상세히 설명한다. 도 5는 본 발명의 일 실시예에 따른 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법을 설명하기 위 한 흐름도이다. 상기 다중 드론 에이전트 강화학습 방법은 본 발명의 일 실시예에 따른 다중 드론 네트워크 운 용 계획 생성 방법의 S120 단계에 해당한다. 다중 드론 네트워크 상황을 구현하기 위해 다양한 강화학습 알고리 즘이 적용될 수 있는데, 본 발명에서는 MADDPG(Multi-Agent Deep Deterministic Policy Gradient) 알고리즘에 기초한 다중 드론 에이전트 학습 방법을 예시한다. MADDPG는 마르코프 게임으로 정식화된 멀티에이전트 의사결 정 문제에 적용할 수 있는 강화학습 알고리즘 중 하나로, 중앙집중형 훈련 및 분산 실행(centralized training and decentralized execution, CTDE) 프레임워크를 기반으로 설계되었다. 본 발명의 일 실시예에 따른 MADDPG 알고리즘 기반의 다중 드론 에이전트 학습 방법은 S121 단계 내지 S133 단 계를 포함한다. S121 단계는 강화학습 하이퍼파라미터를 정의하는 단계이다. 다중 드론 네트워크 운용 계획 생성기는 입력 부를 통해 하이퍼파라미터에 대한 설정값을 입력받는다. 입력부는 입력받은 하이퍼파라미터 설정값을 학습부에 전달한다. 입력부는 하이퍼파라미터 설정값을 메모리에 저장할 수 있다. 상기 하이퍼파라미터는 마르코프 게임 상태 초기화 이후 학습 반복 횟수(의사결정 시점의 상한), 학습 종료 조 건(알고리즘 최대 반복 회수 또는 최대 연산 시간 등)은 물론이고, 다중 드론 네트워크에 속한 드론 에이전트의 수, 표적지점의 개수, 드론 파라미터(드론 최대 속도, 드론 비행 고도, 드론 임무장비의 센싱 거리 등) 및 통신 파라미터(다중 드론 네트워크 통신 비용 모델, 두 드론 간의 최대 통신 가능 거리, 드론과 기지국 간의 최대 통 신 가능 거리, 토폴로지 모델 등)가 포함된다. 여기서 통신 파라미터에 포함되는 '토폴로지 모델'은 학습에 사용될 통신 토폴로지 모델을 학습 전에 설정하는 것으로서, 예를 들면 일렬 구조, 메쉬 구조, 트리 구조 등이 있 다. 또한 상기 하이퍼파라미터는 각 드론 에이전트의 신경망에 대한 정의를 포함한다. 각 드론 에이전트의 신경망은 액터(Actor) 신경망이 포함되며, MADDPG 알고리즘의 본질 상 크리틱(Critic) 신경망과 각 액터(Actor) 및 크리 틱(Critic) 신경망에 대한 타겟(Target) 신경망이 더 포함될 수 있다. 신경망에 대한 파라미터는 입력 노드의 수, 출력 노드의 수, 은닉 층(hidden lay)의 수, 각 은닉 층의 노드의 수, 노드 간의 연결구조가 포함될 수 있 다. S122 단계는 정의된 하이퍼파라미터를 기초로 마르코프 게임 상태를 초기화하는 단계이다. 다중 드론 네트워크 운용 계획 생성기의 학습부는 S121 단계에서 입력된 하이퍼파라미터를 기초로 마르코프 게임 상태를 초기화한다. '마르코프 게임 상태의 초기화'는 전술한 내용와 같이 구성된 마르코프 게임의 상태 정보 s=<t,{pi},c,η,{δm},{τi}>를 초기화하는 것을 의미한다. 예를 들어 학습부는 본 단계에서 모든 표적 지 점(데이터수집지점)에 대한 데이터수집 완료 여부(δm)를 0(미완료)으로 초기화한다. S123 단계는 의사결정 시점(decision step)을 초기화하는 단계이다. 도 5의 'STEP'은 의사결정 시점(decision step)을 의미한다. 학습부는 설정된 반복 회수(의사결정 시점의 상한)만큼 각 드론 에이전트의 신경망이 업데이트되도록 의사결정 시점(STEP, decision step)을 0으로 초기화한다. S124 단계는 마르코프 게임 상태를 기초로 드론 에이전트별 관측을 획득하는 단계이다. 각 드론 에이전트들은 마르코프 게임의 틀에서 상태(s)를 자신의 입장에서 관측한다. 즉, 학습부는 마르코프 게임 상태(s)를 기 초로 각 드론 에이전트의 관측 정보를 생성한다. S125 단계는 드론 에이전트별로 할당된 액터(Actor) 신경망을 이용하여 각 드론 에이전트의 행동을 추론하는 단 계이다. 학습부는 관측 정보를 액터 신경망에 입력하여 행동을 추론한다. 이 과정에서 검블 소프트맥스 (Gumbel-softmax)를 통한 랜덤 샘플링이 적용될 수 있다. 검블 소프트맥스는 강화학습 과정에서 탐험 (exploration)과 활용(exploitation)의 균형(balance)을 위하여 사용되는 기법으로서, 본 발명에서 검블 소프 트맥스는 행동을 랜덤하게 선택하기 위해 사용된다. 다만, 행동에 대한 랜덤 샘플링은 강화학습 중(S120 단계) 에만 사용되며, 학습이 종료된 후 액터 신경망을 이용하여 운용계획을 생성하는 과정(S140 단계 및 S160 단계) 중에는 행동에 대한 랜덤 샘플링이 수행되지 않는다. 행동 모델에 대하여 전술한 바와 같이, 드론 에이전트는 액터 신경망의 추론 결과에 따라 단순 이동방향 결정 행동(aGoTo) 및 의도명시적 결정 행동(aτ)에 속한 행동 중 어느 하나의 행동을 선택하게 된다. 즉, 학습부는 관측 정보를 기초로 액터 신경망을 이용하여 행동 정보 를 생성한다. S126 단계는 다중 드론 네트워크 통신 비용 모델을 이용하여 다중 드론 네트워크의 통신 비용을 산출하는 단계 이다. 학습부는 현재 상태(s) 및 S125 단계에서 생성한 행동 정보(ai)에 기초하여, 통신 네트워크 토폴로 지 모델 및 통신 비용 모델을 이용하여 다중 드론 네트워크의 통신 비용을 산출한다. S127 단계는 드론 에이전트별로 보상을 획득하는 단계이다. 학습부는 현재 상태(s), S125 단계에서 생성한 행동 정보(ai)를 기초로 각 드론 에이전트의 데이터 수집 완료 여부 및 드론 통신 네트워크의 연결성(통신 네트 워크 상태)을 판단할 수 있다. 학습부는 각 드론 에이전트의 데이터 수집 완료 여부, 드론 통신 네트워크 의 연결성 및 S126 단계에서 산출한 통신 비용을 기초로, 전술한 보상 모델을 적용하여 드론 에이전트별 보상을 산출할 수 있다. S128 단계는 상태 천이(state transition) 및 관측 획득 단계이다. 학습부는 S125 단계에서 생성한 각 드 론 에이전트별 행동 정보(ai)에 기초하여, 드론 에이전트별로 현재 상태(s)에서 다음 상태(s')로 천이한다. 즉, 학습부는 현재 상태(s)와 드론 에이전트별 행동(ai)을 기초로 다음 시점의 드론 에이전트별 상태(s') 정보 를 획득한다. 또한, 학습부는 드론 에이전트별로 업데이트된 상태(마르코프 게임 상태, s')를 기초로 각 드론 에이전트의 관측 정보(다음 관측, oi')를 생성(획득)한다. S129 단계는 드론 에이전트별 <관측,행동,보상,다음 관측> 데이터를 리플레이 버퍼에 저장하는 단계이다. 여기 서, 직전 단계(S128) 단계에서 획득한 관측 데이터가 '다음 관측' 데이터가 되고, 그 이전에 관측한 데이터가 '관측' 데이터가 된다. 상기 리플레이 버퍼는 학습부가 자체적으로 가진 내부 저장소에 위치할 수도 있고, 메모리에 위치할 수도 있다.S130 단계는 리플레이 버퍼에서 미니배치 데이터를 랜덤 샘플링을 통해 추출하는 단계이다. 상기 미니배치 데이 터는 학습부가 드론 에이전트별 신경망을 학습시키는데 사용된다. S131 단계는 드론 에이전트별로 액터(Actor) 신경망을 업데이트하는 단계이다. 학습부는 각 드론 에이전트 에 대하여 미니배치에 대한 정책 경사(policy gradient)를 계산하여 액터 신경망을 업데이트한다. 다른 예로, 학습부는 MADDPG의 기본적인 알고리즘에 따라 랜덤 샘플링 된 미니배치를 기초로 손실함수(loss functio n)을 최소화하는 방향으로 크리틱(Critic) 신경망을 먼저 업데이트한 후, 미니배치에 대한 정책경사(policy gradient)를 계산하여 액터 신경망을 업데이트할 수도 있다. S132 단계는 의사결정 시점을 차기 시점으로 변경하는 단계이다. 즉, 학습부는 의사결정 시점(decision step)을 나타내는 'STEP'값을 1만큼 증가한다. S133 단계는 의사결정 시점(decision step)이 기 설정된 상한에 도달했는지 판단하는 단계이다. 학습부는 의사결정 시점이 기 설정된 상한(반복 회수, 'STEP_MAX')에 도달했는지 여부를 판단하여, 도달한 경우 S134 단 계를 수행하고, 그렇지 않은 경우 S125(드론 에이전트의 행동 추론) 단계를 수행한다. S134 단계는 학습 종료 조건에 도달하였는지 판단하는 단계이다. 학습부는 기 설정된 학습 종료 조건(알고 리즘 최대 반복 회수 또는 최대 연산 시간 등)에 도달했는지 판단하고, 도달한 경우 학습을 종료한다. 즉, 학습 부는 최종적으로 업데이트된 각 드론 에이전트별 액터(Actor) 신경망을 S140 단계에서 상태-행동 이력 정 보 생성에 사용될 액터(Actor) 신경망('학습된 액터 신경망')으로 확정한다. 학습부는 기 설정된 학습 종 료 조건에 도달하지 못한 것으로 판단된 경우, S122 단계(마르코프 게임 상태 초기화)를 수행한다. 다시 도 4로 돌아와, S140 단계에 대하여 설명한다. S140 단계는 다중 드론 네트워크 운용 계획 생성기가 S120 단계에서 학습된 액터 신경망을 활용하여, 다중 드론 네트워크 운용 계획 생성에 필요한 상태-행동 이력 정보를 생성하는 단계이다. 본 단계에 대하여는 도 6을 참조하여 상세히 설명한다. 도 6은 본 발명의 일 실시예에 따른 상태-행동 이력 정보 생성 방법을 설명하기 위한 흐름도이다. 상기 상태-행 동 이력 정보 생성 방법은 본 발명의 일 실시예에 따른 다중 드론 네트워크 운용 계획 생성 방법의 S140 단계에 해당한다. 본 발명의 일 실시예에 따른 상태-행동 이력 정보 생성 방법은 S141 단계 내지 S147 단계를 포함한다. S141 단계는 다중 드론 네트워크 임무 정보를 입력받는 단계이다. 다중 드론 네트워크 운용 계획 생성기는 입력부를 통해 다중 드론 네트워크 임무 정보를 입력받는다. 입력부는 입력받은 상기 임무 정보를 계 획 생성부에 전달한다. 입력부는 상기 임무 정보를 메모리에 저장할 수 있다. 본 실시예의 초기 입력 및 설정인 '다중 드론 네트워크 임무 정보'는 다음과 같은 항목을 포함한다. ① 기지국과 표적지점에 관한 정보: 기지국 위치, 표적지점 개수 및 위치(분포) ② 드론에 관한 정보: 다중 드론 대수 및 각 드론의 위치, 드론 파라미터(드론 최대 속도, 드론 비행 고도, 드 론 임무장비 센싱 거리 등) ③ 통신에 관한 정보: 다중 드론 네트워크 통신 비용 모델(Jcomm 등), 통신 파라미터(드론과 드론 간의 최대 통신 가능 거리, 드론과 기지국 간의 최대 통신 가능거리 등) ④ 임무 종료 조건(예: 모든 표적 지점에 대한 데이터 수집 완료 또는 데이터 수집 완료 후 복귀) S142 단계는 마르코프 게임 문제 정식화 단계이다. 계획 생성부은 상기 임무 정보를 기초로 마르코프 게임 정식화 정보를 생성한다. 즉, 계획 생성부는 상기 임무 정보를 마르코프 게임 정식화 정보(마르코프 게임 의 상태, 관측 모델, 행동 모델 및 보상 모델)로 변환한다. 상기 변환을 통해 마르코프 게임의 상태, 관측 모델, 행동 모델 및 보상 모델이 정의된다. 계획 생성부는 마르코프 게임 정식화 정보를 계획 생성부(23 0)의 내부 저장소 또는 메모리에 저장할 수 있다. S143 단계는 마르코프 게임 상태 초기화 및 저장 단계이다. '마르코프 게임 상태의 초기화'는 전술한 내용와 같 이 정의된 마르코프 게임의 상태 정보 s=<t,{pi},c,η,{δm},{τi}>를 초기화하는 것을 의미한다. 예를 들어 계 획 생성부는 모든 표적 지점(데이터수집지점)에 대한 데이터수집 완료 여부(δm)를 0(미완료)으로 초기화 한다. 또한, 계획 생성부는 마르코프 게임의 초기 상태(s[0])를 계획 생성부의 내부 저장소 또는 메 모리에 저장한다.S144 단계는 드론 에이전트별 관측 획득 단계이다. 각 드론 에이전트들은 마르코프 게임의 틀에서 현재 시점의 상태(s[k])를 자신의 입장에서 관측한다. 즉, 계획 생성부는 마르코프 게임 상태(s[k])를 기초로 각 드론 에이전트의 관측 정보를 생성한다. 계획 생성부는 관측 정보를 내부 저장소나 메모리에 저장할 수 있 다. S145 단계는 학습된 액터 신경망을 이용하여 드론 에이전트별로 행동을 추론하고, 추론된 행동을 저장하는 단계 이다. 드론 에이전트는 관측 정보를 액터 신경망에 입력하여 행동을 추론한다. 즉, 계획 생성부은 관측 정 보를 기초로 각 드론 에이전트에 할당된 액터 신경망을 이용하여 각 드론 에이전트의 행동을 추론하고, 추론 결 과를 통합하여(a[k]) 현재 시점(k)의 상태(s[k])와 매칭하여 계획 생성부의 내부 저장소 또는 메모리(24 0)에 저장한다. 즉, 상기 내부 저장소 또는 메모리에는 의사결정 시점 별로 상태-행동의 쌍이 저장된다. S146 단계는 상태 천이 및 저장 단계이다. 계획 생성부은 추론된 각 드론 에이전트의 행동을 기초로 공지 의 다중 드론 네트워크 상태 천이 모델(예: 드론 이동 모델)을 이용하여 현재 시점의 상태(s[k])를 다음 상태 (s[k+1])로 천이시킨다. 즉, 계획 생성부는 현재 상태(s)와 드론 에이전트별 행동(ai)을 기초로 다음 시점 의 드론 에이전트별 상태(s') 정보를 획득한다. 예를 들어, 계획 생성부는 드론 이동 모델을 이용하여 현 재 상태 중 i번째 드론의 위치(pi[k])를 행동(ai[k])에 기초하여 다음 위치(pi[k+1])로 천이시킨다(pi[k],ai[k] -> pi[k+1]). 또한 계획 생성부는 천이된 상태(s[k+1])을 계획 생성부의 내부 저장소 또는 메모리 에 저장한다. S147 단계는 임무 종료 조건에 도달했는지 판단하는 단계이다. 계획 생성부는 천이된 상태(다음 상태, s[k+1])를 기초로 기 설정된 임무 종료 조건(예: 모든 목표 지점에 대한 데이터 수집 완료)을 달성했는지 여부 를 판단한다. 계획 생성부는 임무 종료 조건에 도달한 경우, 최대 의사 결정 시점(T) 변수에 현재 시점 (k)을 대입하고, 의사결정 시점 별 상태-행동의 쌍의 저장을 종료한다. 계획 생성부는 초기 상태 및 행동 (s[0],a[0])부터 최대 의사 결정 시점(T)의 상태 및 행동(s[T],a[T])까지의 의사결정 시점 별 상태-행동의 쌍을 종합하여 상태-행동 이력 정보를 생성한다. 여기서, a[k]는 {ai[k]}, 즉 k 시점의 에이전트별 행동의 집합이다. 임무 종료 조건에 아직 도달하지 않은 경우, 계획 생성부는 현재 시점을 1만큼 증가하고, S144 단계로 진 행한다. 이후, S144 단계에서는 k+1을 현재 시점으로 삼아, 상태 s[k+1]을 기준으로 드론 에이전트별로 관측을 획득하게 된다. 다시 도 4로 돌아와, S160 단계에 대해 하기에 상세히 설명한다. S160 단계는 다중 드론 네트워크 운용 계획 생 성기가 상태-행동 이력 정보를 후처리하여 다중 드론 네트워크 운용 계획을 생성하는 단계이다. 상태-행동 이력 정보는 전술한 바와 같이 의사결정 시점 별 상태-행동의 쌍의 집합으로 구성된다({s[0],a[0],s[1],a[1], … ,s[T],a[T]}). 한편, 본 실시예에서 다중 드론 네트워크 운용 계획은 다음 ① 내지 ④의 정보를 포함하여 구 성될 수 있다. ① 드론별 의사결정 시점에 따른 위치 정보(비행 경로 정보)/속도 정보 ② 드론별 의사결정 시점에 따른 임무수행 상태(통신중계, 데이터수집, 이동) ③ 의사결정 시점에 따른 네트워크 토폴로지(드론-드론 링크, 드론-기지국 링크) ④ 데이터 수집 완료 시점 및 기지 복귀 시점 하기에, 계획 생성부가 상태-행동 이력 정보를 기초로 상기 다중 드론 네트워크 운용 계획에 포함되는 정 보를 생성하는 과정을 설명한다. 아래는 k=0 시점부터 k=T 시점까지의 상태-행동 이력 정보를 나타낸 것이다. s[0]=<0,{pi[0]},c[0],η[0],{δm[0]},{τi[0]}>, a[0]={ai[0]} s[1]=<Δt,{pi[1]},c[1],η[1],{δm[1]},{τi[1]}>, a[1]={ai[1]} .... s[k]=<kΔt,{pi[k]},c[k],η[k],{δm[k]},{τi[k]}>, a[k]={ai[k]} .... s[T]=<TΔt,{pi[T]},c[T],η[T],{δm[T]},{τi[T]}>, a[T]={ai[T]} 계획 생성부는 상태-행동 이력 정보를 의사결정 시점에 따라 요소별로 재조합하여 다중 드론 네트워크 운 용 계획에 포함되는 정보를 생성한다. 예를 들어, 계획 생성부는 상태 이력 정보에 포함된 드론의 수평 위 치 좌표 벡터(pi)를 의사결정 시점에 따라 종합하여, 드론별 의사결정 시점에 따른 위치 정보(①)를 생성할 수 있고, 각 의사결정 시점 간의 시차(Δt)와 드론의 수평 위치 좌표 벡터(pi)를 기초로 속도 정보(①)를 생성할 수 있다. 또한, 계획 생성부는 드론의 상태 이력 정보에 포함된 드론별 임무수행의도와 행동 이력 정보에 포함된 드론별 행동을 종합하여 드론별 의사결정 시점에 따른 임무수행 상태(통신중계, 데이터수집, 이동) 정보 (②)를 생성할 수 있다. 또한, 계획 생성부는 드론의 상태 이력 정보에 있는 토폴로지 이력(c[0], … ,c[T])을 종합하여 의사결정 시점에 따른 네트워크 토폴로지(드론-드론 링크, 드론-기지국 링크) 정보(③)를 생 성할 수 있다. 드론 네트워크 토폴로지는 동적으로 변경되는데, 상기 네트워크 토폴로지 정보(③)는, 의사결정 시점별로 각 드론의 역할(수신자/송신자/중계자) 및 데이터 수신/송신 노드 정보 업로드를 위해 사용된다. 각 드론은 네트워크 토폴로지 정보(③)에 따라 현재 시점의 수신/송신 대상, 다음 시점의 수신/송신 대상, 그 다 음 시점의 수신/송신 대상에 대한 정보를 순차적으로 설정하고 미리 업로드함으로써 통신 지연을 최소화할 수 있다. 또한, 상기 네트워크 토폴로지 정보(③)는 무선통신 데이터 비트레이트(bitrate) 및 대역폭(bandwidth) 제한과도 밀접하게 연관된다. 또한, 계획 생성부는 상태 이력 정보의 데이터 수집 완료 여부 정보(δm)를 기초로 데이터 수집 완료 시점 및 기지 복귀 시점 정보(④)를 생성할 수 있다. 예를 들어, 계획 생성부는 각 의사결정 시점별 데이터 수 집 완료 여부 정보(δm[k])를 기초로 각 표적에 대한 데이터 수집 시점을 산출할 수 있다. m번째 표적에 대한 데이터 수집 시점은 수학식 2와 같이 나타낼 수 있다. 수학식 2"}
{"patent_id": "10-2022-0033925", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "m번째 표적에 대한 데이터 수집 시점 정보는 드론이 데이터 수집 장비의 가동 여부(on/off)와 영상 촬영 시 데 이터 수집 장비의 설정(예:화질, 촬영 모드(열화상/적외선))을 판단하는 기초 정보가 된다. 상기 정보는 임무 시작 전에 드론에 업로드될 수 있다. 또한, 상기 정보는 지상 관제국(GCS)이 운용/관제 모니터링 화면에 표적에 대한 데이터를 표시함에 있어서 참고할 수 있는 정보도 된다. 한편, 계획 생성부는 다중 드론 네트워크 운용 계획의 통신 품질 관점에서의 타당성을 검증하여 소정 기준 을 충족한 경우에 한하여 상기 운용 계획에서 도출된 임무 계획을 드론 에이전트별로 배포할 수 있다. 즉, 계획 생성부는 다중 드론 네트워크 운용 계획에서 도출되는 최종적인 임무 계획을 각 드론에 업로드하기에 앞서, 다중 드론 네트워크 운용 계획의 통신 품질상의 타당성을 검증한 후, 소정의 조건에 부합하지 않는 경우, 하이퍼파라미터를 일부 조정하여 다중 드론 에이전트 학습을 다시 수행하거나, 다중 드론 네트워크 임무 정보를 수정하여 상태-행동 이력 정보를 다시 생성한 후에 네트워크 운용 계획을 수정할 수 있다. 다중 드론 네트워크 운용 계획의 통신 품질상의 타당성을 검증하는 방법으로서 통신 연결성 이력을 확인하는 방법이 있을 수 있다. 예를 들어, 계획 생성부는 상태-행동 이력 정보를 재조합하여 통신 연결성 이력(η[0], …, η[T])을 도출 한 후, 모든 시점(k=0 내지 k=T)에 걸쳐 대부분의 시간동안(예: 99%) 통신 연결성이 유지되는지(η[k]=0) 검증 하여, 기준이 충족될 경우 상기 운용 계획에서 도출한 드론 에이전트별 최종적인 임무 계획을 드론 에이전트별 로 업로드한다. 상기 통신 연결성 이력은 상기 운용 계획이나 상기 임무 계획 자체에는 포함되지 않으나, 임무 계획의 통신 품질을 검증하기 위한 데이터로서 의미를 가진다. 계획 생성부는 상술한 바와 같이 상태-행동 이력 정보에서 추출한 정보로 다중 드론 네트워크 운용 계획을 생성할 수 있고, 다중 드론 네트워크 운용 계획을 계획 생성부의 내부 저장소나 메모리에 저장할 수 있다. 다중 드론 네트워크 운용 계획 생성기는, 계획 생성부가 다중 드론 네트워크 운용 계획을 기초로 생 성한 드론 에이전트별 임무 계획을 각 드론 에이전트에 내장된 미션 컴퓨터에 업로드하여 운용 계획이 드론에의해 실제로 활용되도록 할 수 있다. 한편 도 4 내지 도 6을 참조한 설명에서, 각 단계는 본 발명의 구현예에 따라서, 추가적인 단계들로 더 분할되 거나, 더 적은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있다. 아울러, 기타 생략된 내용이라 하더라도 도 1 내지 도 3과 도 7의 내용은 도 4 내지 도 6의 내용에 적용될 수 있다. 또한, 도 4 내지 도 6의 내용은 도 1 내지 도 3과 도 7의 내용에 적용될 수 있다. 전술한 다중 드론 네트워크 운용 계획 생성 방법, MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법, 상태-행동 이력 정보 생성 방법은 도면에 제시된 흐름도를 참조로 하여 설명되었다. 간단히 설명하기 위하여 상 기 방법은 일련의 블록들로 도시되고 설명되었으나, 본 발명은 상기 블록들의 순서에 한정되지 않고, 몇몇 블록 들은 다른 블록들과 본 명세서에서 도시되고 기술된 것과 상이한 순서로 또는 동시에 일어날 수도 있으며, 동일 한 또는 유사한 결과를 달성하는 다양한 다른 분기, 흐름 경로, 및 블록의 순서들이 구현될 수 있다. 또한, 본 명세서에서 기술되는 방법의 구현을 위하여 도시된 모든 블록들이 요구되지 않을 수도 있다. 도 7은 본 발명의 일 실시예에 따른 다중 드론 네트워크 운용 계획 생성기의 구성을 나타낸 블록도이다. 본 발명의 일 실시예에 따른 다중 드론 네트워크 운용 계획 생성기는 입력부, 학습부 및 계획 생성부를 포함하며, 메모리를 더 포함할 수 있다. 입력부는 강화학습 하이퍼파라미터를 입력받아 학습부에 전달하며, 다중 드론 네트워크 임무 정보를 입력받아 계획 생성부에 전달한다. 상기 하이퍼파라미터의 구체적인 예시 및 다중 드론 네트워크 임무 정 보의 구체적인 예시는 전술한 S121 단계 및 S141 단계에 관한 설명을 참조할 수 있다. 학습부는 강화학습 하이퍼파라미터를 기초로 MADDPG 알고리즘을 이용하여 각 다중 드론 에이전트에 할당된 액터(Actor) 신경망을 학습시킨다. 학습부는 정의된 하이퍼파라미터를 기초로 마르코프 게임 상태를 정의 및 초기화하고, 마르코프 게임 상태(s)를 기초로 각 드론 에이전트의 관측 정보를 생성한다. 그리고, 학습부 는 드론 에이전트별로 할당된 액터(Actor) 신경망을 이용하여 각 드론 에이전트의 행동을 추론하며, 각 드 론 에이전트의 행동에 따라 상태를 천이시키며, 그에 따른 다중 드론 네트워크의 통신 비용을 산출하고 보상 모 델을 통해 드론 에이전트별 보상을 산출한다. 학습부는 드론 에이전트별 <관측,행동,보상,다음 관측> 데이 터를 리플레이 버퍼에 저장하고, 리플레이 버퍼에서 랜덤 샘플링을 통해 미니배치 데이터를 추출하여 드론 에이 전트별 신경망을 학습시킨다. 상기 드론 에이전트별 신경망에 액터 신경망이 포함되는 것은 물론이다. 학습부 는 각 드론 에이전트에 대하여 미니배치에 대한 정책 경사(policy gradient)를 계산하여 액터 신경망을 업 데이트한다. 다른 예로, 학습부는 MADDPG의 기본적인 알고리즘에 따라 랜덤 샘플링 된 미니배치를 기초로 손실함수(loss function)을 최소화하는 방향으로 크리틱(Critic) 신경망을 먼저 업데이트한 후, 미니배치에 대 한 정책경사(policy gradient)를 계산하여 액터 신경망을 업데이트할 수도 있다. 학습부는 설정된 의사결 정 시점의 상한까지 상술한 학습 과정을 반복하며, 학습 종료 조건(알고리즘 최대 반복 회수 또는 최대 연산 시 간 등)에 도달한 경우, 학습을 종료한다. 학습부는 최종적으로 업데이트된 각 드론 에이전트별 액터 (Actor) 신경망을 계획 생성부에 전달한다. 학습된 액터 신경망은 계획 생성부가 상태-행동 이력 정 보를 생성하는 데 활용된다. 학습부에 대한 상세한 내용은 도 5을 참조한 설명에 전술하였다. 계획 생성부는 다중 드론 네트워크 임무 정보를 기초로 학습된 액터 신경망을 이용하여 상태-행동 이력 정 보를 생성하고, 상태-행동 이력 정보를 후처리(재조합)하여 다중 드론 네트워크 운용 계획을 생성한다. 계획 생성부는 입력부에서 전달받은 다중 드론 네트워크 임무 정보를 기초로 마르코프 게임 정식화 정보(마르코프 게임의 상태, 관측 모델, 행동 모델 및 보상 모델)를 생성한다. 그리고 계획 생성부는 마르 코프 게임의 상태 정보를 초기화한다. 계획 생성부는 상태 정보를 기초로 각 드론 에이전트의 관측 정보를 생성하고, 학습된 액터 신경망을 이용하여 드론 에이전트별로 행동을 추론한다. 계획 생성부는 상태 정보 와 추론된 행동(행동 정보)를 동일한 의사결정 시점을 기준으로 매칭한 상태-행동의 쌍을 내부 저장소나 메모리 에 저장한다. 계획 생성부는 추론된 각 드론 에이전트의 행동을 기초로 공지의 다중 드론 네트워크 상태 천이 모델을 이용하여 현재 시점의 상태(s[k])를 다음 상태(s[k+1])로 천이시킨다. 계획 생성부는 임 무 종료 조건에 도달할 때까지 상술한 과정을 반복 실시하여 상태-행동의 쌍을 축차적으로 저장하고, 임무 종료 조건에 도달한 경우, 그 시점의 현재 시점(k)의 값을 최대 의사 결정 시점(T)으로 삼아서, T 시점까지 저장된 상태-행동의 쌍을 종합하여 상태-행동 이력 정보를 생성한다. 이후, 계획 생성부는 상태-행동 이력 정보를 요소별로 재조합하여 다중 드론 네트워크 운용 계획을 생성한 다.계획 생성부에 대한 상세한 내용은 도 4 내지 도 6을 참조한 설명에 전술하였다. 메모리는 입력부에서 입력받은 정보나 학습부 및 계획 생성부에서 생성한 정보를 저장한다. 예를 들어, 메모리는 입력부에서 입력받은 강화학습 하이퍼파라미터 설정값 및 다중 드론 네트워크 임무정보를 저장할 수 있고, 학습부가 생성한 상태 정보, 관측 정보 및 행동 정보를 저장할 수 있다. 또한, 메모리는 강화학습에 필요한 리플레이 버퍼를 포함할 수 있다. 또한, 메모리는 계획 생 성부에서 생성한 마르코프 게임 정식화 정보, 마르코프 게임 상태 정보, 행동 정보, 상태-행동 이력 정보 및 다중 드론 네트워크 운용계획을 저장할 수 있다. 참고로, 본 발명의 실시예에 따른 구성 요소들은 소프트웨어 또는 FPGA(Field Programmable Gate Array) 또는 ASIC(Application Specific Integrated Circuit)와 같은 하드웨어 형태로 구현될 수 있으며, 소정의 역할들을 수행할 수 있다. 그렇지만 '구성 요소들'은 소프트웨어 또는 하드웨어에 한정되는 의미는 아니며, 각 구성 요소는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 구성 요소는 소프트웨어 구성 요소들, 객체지향 소프트웨어 구성 요소들, 클래스 구성 요소 들 및 태스크 구성 요소들과 같은 구성 요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로 그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로 코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테 이블들, 어레이들 및 변수들을 포함한다. 구성 요소들과 해당 구성 요소들 안에서 제공되는 기능은 더 작은 수의 구성 요소들로 결합되거나 추가적인 구 성 요소들로 더 분리될 수 있다. 이 때, 처리 흐름도 도면들의 각 블록과 흐름도 도면들의 조합들은 컴퓨터 프로그램 인스트럭션들에 의해 수행 될 수 있음을 이해할 수 있을 것이다. 이들 컴퓨터 프로그램 인스트럭션들은 범용 컴퓨터, 특수용 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서에 탑재될 수 있으므로, 컴퓨터 또는 기타 프로그램 가 능한 데이터 프로세싱 장비의 프로세서를 통해 수행되는 그 인스트럭션들이 흐름도 블록(들)에서 설명된 기능들 을 수행하는 수단을 생성하게 된다. 이들 컴퓨터 프로그램 인스트럭션들은 특정 방식으로 기능을 구현하기 위해 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 지향할 수 있는 컴퓨터를 이용하거나 또는 컴퓨터 판독 가능 메모리에 저장되는 것도 가능하므로, 그 컴퓨터를 이용하거나 컴퓨터 판독 가능 메모리에 저장된 인 스트럭션들은 흐름도 블록(들)에서 설명된 기능을 수행하는 인스트럭션 수단을 내포하는 제조 품목을 생산하는 것도 가능하다. 컴퓨터 프로그램 인스트럭션들은 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에 탑재되는 것도 가능하므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에서 일련의 동작 단계 들이 수행되어 컴퓨터로 실행되는 프로세스를 생성해서 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장 비를 수행하는 인스트럭션들은 흐름도 블록(들)에서 설명된 기능들을 실행하기 위한 단계들을 제공하는 것도 가 능하다. 또한, 각 블록은 특정된 논리적 기능(들)을 실행하기 위한 하나 이상의 실행 가능한 인스트럭션들을 포함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있다. 또, 몇 가지 대체 실행 예들에서는 블록들에서 언급된 기 능들이 순서를 벗어나서 발생하는 것도 가능함을 주목해야 한다. 예컨대, 잇달아 도시되어 있는 두 개의 블록들 은 사실 실질적으로 동시에 수행되는 것도 가능하고 또는 그 블록들이 때때로 해당하는 기능에 따라 역순으로 수행되는 것도 가능하다. 이 때, 본 실시예에서 사용되는 '~부'라는 용어는 소프트웨어 또는 FPGA또는 ASIC과 같은 하드웨어 구성요소를 의미하며, '~부'는 어떤 역할들을 수행한다. 그렇지만 '~부'는 소프트웨어 또는 하드웨어에 한정되는 의미는 아 니다. '~부'는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재 생시키도록 구성될 수도 있다. 따라서, 일 예로서 '~부'는 소프트웨어 구성요소들, 객체지향 소프트웨어 구성 요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저 들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들, 및 변수들을 포함한다. 구성요소들과 '~부'들 안에서 제공 되는 기능은 더 작은 수의 구성요소들 및 '~부'들로 결합되거나 추가적인 구성요소들과 '~부'들로 더 분리될 수 있다. 뿐만 아니라, 구성요소들 및 '~부'들은 디바이스 또는 보안 멀티미디어카드 내의 하나 또는 그 이상의 CPU들을 재생시키도록 구현될 수도 있다. 상기에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특 허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다."}
{"patent_id": "10-2022-0033925", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 다중 드론 시스템의 임무 개요에 관한 도면. 도 2는 임무드론의 구성을 나타낸 블록도. 도 3은 드론 에이전트의 임무수행의도를 설명하기 위한 도면. 도 4는 본 발명의 일 실시예에 따른 다중 드론 네트워크 운용 계획 생성 방법을 설명하기 위한 흐름도. 도 5는 본 발명의 일 실시예에 따른 MADDPG 알고리즘 기반의 다중 드론 에이전트 강화학습 방법을 설명하기 위 한 흐름도. 도 6은 본 발명의 일 실시예에 따른 상태-행동 이력 정보 생성 방법을 설명하기 위한 흐름도. 도 7은 본 발명의 일 실시예에 따른 다중 드론 네트워크 운용 계획 생성기의 구성을 나타낸 블록도."}
