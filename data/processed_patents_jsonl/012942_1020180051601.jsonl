{"patent_id": "10-2018-0051601", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0127233", "출원번호": "10-2018-0051601", "발명의 명칭": "심화 신경망을 이용한 다화자 음성 합성 방법 및 시스템", "출원인": "한양대학교 산학협력단", "발명자": "장준혁"}}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성하는 단계; 입력 받은 화자 데이터를 임베딩하여 화자 입력을 구성하는 단계; 및 임베딩된 상기 문장 데이터와 임베딩된 상기 화자 데이터를 연결(Concatenate)하여 하나의 입력 벡터열을 생성하는 단계를 포함하는, 음성 합성 방법."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성하는 단계는, 상기 입력 받은 문장 데이터를 한글 자모 단위로 분해하여 자모 단위 입력을 생성하는 단계; 상기 자모 단위 입력을 색인하여 숫자 데이터로 매핑하는 단계; 상기 숫자 데이터로 매핑된 상기 문장 데이터를 원-핫 인코딩(One-hot encoding)하여 원-핫 인코딩된 벡터열을생성하는 단계; 및 상기 원-핫 인코딩된 벡터열을 문장 임베딩 매트릭스와 곱하여 문장 데이터 특징벡터로 변환하는 단계를 포함하는, 음성 합성 방법."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 입력 받은 화자 데이터를 임베딩하여 화자 입력을 구성하는 단계는, 상기 입력 받은 화자 데이터를 색인하여 숫자 데이터로 매핑하는 단계; 상기 숫자 데이터로 매핑된 상기 화자 데이터를 원-핫 인코딩(One-hot encoding)하여 원-핫 인코딩된 벡터열을생성하는 단계; 및 상기 원-핫 인코딩된 벡터열을 화자 임베딩 매트릭스와 곱하여 화자 데이터 특징벡터로 변환하는 단계를 포함하는, 음성 합성 방법."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 임베딩된 상기 문장 데이터와 임베딩된 상기 화자 데이터를 연결(Concatenate)하여 하나의 입력 벡터열을생성하는 단계는, 특징벡터로 변환된 임베딩된 상기 문장 데이터와 임베딩된 상기 화자 데이터를 연결(Concatenate)하여, 문장의특정 부분에 특정 화자를 할당하는 것을 특징으로 하는, 음성 합성 방법."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 생성된 상기 입력 벡터열을 하나의 인공 신경망으로 정제하여 정제된 문장 및 화자 데이터를 생성하는 단계공개특허 10-2019-0127233-3-를 더 포함하는, 음성 합성 방법."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 정제된 문장 및 화자 데이터를 입력 받아 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하는 단계를 더 포함하고, 상기 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하는 단계는, 상기 정제된 문장 및 화자 데이터와 어텐션(Attention) 순환 신경망의 출력을 입력 받아 어텐션 메커니즘을 통해 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성하는 단계; 선택된 상기 고정된 길이의 벡터를 상기 어텐션 순환 신경망에서 합성된 멜 필터 뱅크 스펙트럼과 연결(Concatenate)하여 음성 특징벡터 합성 순환 신경망의 입력을 생성하는 단계; 및 상기 음성 특징벡터 합성 순환 신경망을 통해 새로운 멜 필터 뱅크 스펙트럼을 합성하는 단계를 포함하는, 음성 합성 방법."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하는 단계는, 음향 모델 역할을 하는 상기 음성 특징벡터 합성 순환 신경망을 학습시켜 복수의 화자의 목소리를 합성할 수 있는 순환 신경망을 생성하는 단계를 더 포함하는, 음성 합성 방법."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서, 상기 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하는 단계는, 상기 음성 특징벡터 합성 순환 신경망을 통해 합성된 상기 새로운 멜 필터 뱅크 스펙트럼을 포스트 프로세싱 인공 신경망을 통해 정제하여 로그 파워 스펙트럼을 합성하는 단계를 더 포함하는, 음성 합성 방법."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서, 합성된 상기 음성 특징벡터를 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 음성으로 변환하는 단계를 더 포함하는, 음성 합성 방법."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성하는 문장 임베딩부; 입력 받은 화자 데이터를 임베딩하여 화자 입력을 구성하는 화자 임베딩부; 임베딩된 상기 문장 데이터와 임베딩된 상기 화자 데이터를 연결(Concatenate)하여 하나의 입력 벡터열을 생성하는 연결부를 포함하는, 음성 합성 시스템."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2019-0127233-4-제10항에 있어서, 상기 문장 임베딩부는, 상기 입력 받은 문장 데이터를 한글 자모 단위로 분해하여 자모 단위 입력을 생성하는 문장 분해부; 상기 자모 단위 입력을 색인하여 숫자 데이터로 매핑하는 색인부; 상기 숫자 데이터로 매핑된 상기 문장 데이터를 원-핫 인코딩(One-hot encoding)하여 원-핫 인코딩된 벡터열을생성하는 원-핫 인코딩부; 및 상기 원-핫 인코딩된 벡터열을 문장 임베딩 매트릭스와 곱하여 문장 데이터 특징벡터로 변환하는 특징벡터 변환부를 포함하는, 음성 합성 시스템."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서, 상기 화자 임베딩부는, 상기 입력 받은 화자 데이터를 색인하여 숫자 데이터로 매핑하는 색인부; 상기 숫자 데이터로 매핑된 상기 화자 데이터를 원-핫 인코딩(One-hot encoding)하여 원-핫 인코딩된 벡터열을생성하는 원-핫 인코딩부; 및상기 원-핫 인코딩된 벡터열을 화자 임베딩 매트릭스와 곱하여 화자 데이터 특징벡터로 변환하는 특징벡터 변환부를 포함하는, 음성 합성 시스템."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10항에 있어서, 생성된 상기 입력 벡터열을 하나의 인공 신경망으로 정제하여 정제된 문장 및 화자 데이터를 생성하는 문장 및화자 데이터 정제 인공 신경망을 더 포함하는, 음성 합성 시스템."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 정제된 문장 및 화자 데이터를 입력 받아 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하는 음성 특징벡터 합성부를 더 포함하고, 상기 음성 특징벡터 합성부는, 상기 정제된 문장 및 화자 데이터와 어텐션(Attention) 순환 신경망의 출력을 입력 받아 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성하는 어텐션 메커니즘; 선택된 상기 고정된 길이의 벡터를 상기 어텐션 순환 신경망에서 합성된 멜 필터 뱅크 스펙트럼과 연결(Concatenate)하여 음성 특징벡터 합성 순환 신경망의 입력을 생성하는 연결부; 및 상기 음성 특징벡터 합성 순환 신경망을 통해 새로운 멜 필터 뱅크 스펙트럼을 합성하는 음성 특징벡터 합성 순환 신경망을 포함하는, 음성 합성 시스템."}
{"patent_id": "10-2018-0051601", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "공개특허 10-2019-0127233-5-제14항에 있어서, 합성된 상기 음성 특징벡터를 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 음성으로 변환하는 음성재구성부를 더 포함하는, 음성 합성 시스템."}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "심화 신경망을 이용한 다화자 음성 합성 방법 및 시스템이 제시된다. 일 실시예에 따른 음성 합성 방법은, 입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성하는 단계; 입력 받은 화자 데이터를 임베딩하여 화자 입력을 구성하는 단계; 및 임베딩된 상기 문장 데이터와 임베딩된 상기 화자 데이터를 연결(Concatenate)하여 하나의 입 력 벡터열을 생성하는 단계를 포함하여 이루어질 수 있다."}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 다화자 음성 합성 방법 및 시스템에 관한 것으로, 더욱 상세하게는 심화 신경망을 이용한 다화자 음 성 합성 방법 및 시스템에 관한 것이다. 본 연구는 2017년도 정부(과학기술정보통신부)의 재원으로 정보통신기 술진흥센터의 지원을 받아 수행되었다(No.2017-0-00474, AI스피커 음성비서를 위한 지능형 음성신호처리 기술 개발)."}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "음성 합성 기술은 문장 데이터로부터 음성 데이터를 생성해내는 기술로, 일반적으로 다음과 같은 세 단계를 따 른다. 첫 번째 단계인 문장 데이터 분석 단계에서는 규칙 기반 기술을 이용하여 문장 데이터에서 음소 데이터 와 각 음소별 지속시간 데이터를 분석한다. 두 번째 단계는 음소 데이터를 바탕으로 미리 학습된 음향 모델을 이용하여 입력된 음소 데이터가 어떤 음성 데이터에 가장 가까운지 판단하여, 이로부터 음성 특징벡터를 합성하 는 단계이다. 이 때, 사용되는 각 음소별 확률 모델을 음향 모델이라고 부른다. 세 번째 단계는 음성 특징벡 터를 바탕으로 실제 음성을 합성하는 단계이며, 이러한 작업을 하는 모델을 보코더(Vocoder)라고 한다. 이러한 음성 합성 기술에 따라 생성된 합성 음성의 품질은 자연스러움과 음질 두 가지 척도로 평가할 수 있다. 여기서, 두 가지 척도 중 자연스러움은 세 단계 중 첫 번째 단계의 영향을 크게 받는다. 음질은 음향 모델과 보코더의 성능에 영향을 크게 받는다. 음향 모델은 음질을 크게 좌우하기 때문에 다양한 알고리즘이 새롭게 많 이 제시되어 왔다. 특히, 인공 신경망 기반 알고리즘은 기존 모델 대비 큰 성능 향상을 보여준다. 일반적으로 인공 신경망을 이용 한 음성 합성 모델은 음향 모델 부분을 인공 신경망으로 대신하여, 인공 신경망이 분석된 문장 데이터를 기반으 로 음성 파라미터를 합성한다. 종래 기술인 Wang Y, Skerry-Ryan RJ, Stanton D, Wu Y, Weiss RJ, Jaitly N, Yang Z, Xiao Y, Chen Z, Bengio S, Le Q. Tacotron, \"A fully end-to-end text-to-speech synthesis model. arXiv preprint arXiv\", 1703.10135. 2017 Mar 29.은 음향 모델뿐만 아니라 문장 데이터 분석 단계 및 음성 특징벡터 합성 단계를 하나 의 인공 신경망을 이용하여 구현하는 기술을 기재하고 있다. 다화자 음성 합성 모델은 음향 모델의 변경을 통해 구현된다. 먼저, 각 화자에 대한 음성 데이터를 이용하여 화자별 음향 모델을 구성하고, 음향 모델의 변경을 통해 화자별로 음성을 합성한다. 종래 기술은 음향 모델이 음성 특징벡터를 합성하기 때문에 음향 모델의 교체를 통해 각 음성 특성을 반영하는 화자별 음성 특징벡터를 합성할 수 있다. 선행기술문헌 비특허문헌 (비특허문헌 0001) Wang Y, Skerry-Ryan RJ, Stanton D, Wu Y, Weiss RJ, Jaitly N, Yang Z, Xiao Y, Chen Z, Bengio S, Le Q. Tacotron, \"A fully end-to-end text-to-speech synthesis model. arXiv preprint arXiv\", 1703.10135. 2017 Mar 29."}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 심화 신경망을 이용한 다화자 음성 합성 방법 및 시스템에 관하여 기술하며, 보다 구체적으로 화자 임베딩을 이용한 시퀀스-투-시퀀스(Sequence-to-Sequence) 신경망 기반 다화자 음성 합성 모델 생성 기술을 제 공한다. 본 발명은 하나의 음향 모델이 여러 화자의 목소리를 합성할 수 있도록 설계하여 전체 시스템의 용량이 화자 수 에 비례해 커지지 않도록 하는 심화 신경망을 이용한 다화자 음성 합성 방법 및 시스템을 제공하는데 있다. 또한, 본 발명은 문장 데이터 분석 단계에서 화자별 특성이 반영되어 더 자연스러운 다화자 음성 합성이 이루어 질 수 있도록 하는 심화 신경망을 이용한 다화자 음성 합성 방법 및 시스템을 제공하는데 있다."}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 음성 합성 방법은, 입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성하는 단계; 입력 받 은 화자 데이터를 임베딩하여 화자 입력을 구성하는 단계; 및 임베딩된 상기 문장 데이터와 임베딩된 상기 화자 데이터를 연결(Concatenate)하여 하나의 입력 벡터열을 생성하는 단계를 포함하여 이루어질 수 있다. 상기 입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성하는 단계는, 상기 입력 받은 문장 데이터를 한글 자모 단위로 분해하여 자모 단위 입력을 생성하는 단계; 상기 자모 단위 입력을 색인하여 숫자 데이터로 매핑하 는 단계; 상기 숫자 데이터로 매핑된 상기 문장 데이터를 원-핫 인코딩(One-hot encoding)하여 원-핫 인코딩 (One-hot encoding)된 벡터열을 생성하는 단계; 및 상기 원-핫 인코딩된 벡터열을 문장 임베딩 매트릭스와 곱하 여 문장 데이터 특징벡터로 변환하는 단계를 포함할 수 있다. 상기 입력 받은 화자 데이터를 임베딩하여 화자 입력을 구성하는 단계는, 상기 입력 받은 화자 데이터를 색인하 여 숫자 데이터로 매핑하는 단계; 상기 숫자 데이터로 매핑된 상기 화자 데이터를 원-핫 인코딩(One-hot encoding)하여 원-핫 인코딩된 벡터열을 생성하는 단계; 및 상기 원-핫 인코딩된 벡터열을 화자 임베딩 매트릭 스와 곱하여 화자 데이터 특징벡터로 변환하는 단계를 포함하여 이루어질 수 있다. 상기 임베딩된 상기 문장 데이터와 임베딩된 상기 화자 데이터를 연결(Concatenate)하여 하나의 입력 벡터열을 생성하는 단계는, 특징벡터로 변환된 임베딩된 상기 문장 데이터와 임베딩된 상기 화자 데이터를 연결 (Concatenate)하여, 문장의 특정 부분에 특정 화자를 할당할 수 있다. 생성된 상기 입력 벡터열을 하나의 인공 신경망으로 정제하여 정제된 문장 및 화자 데이터를 생성하는 단계를 더 포함할 수 있다. 상기 정제된 문장 및 화자 데이터를 입력 받아 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하 는 단계를 더 포함하고, 상기 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하는 단계는, 상기 정제된 문장 및 화자 데이터와 어텐션(Attention) 순환 신경망의 출력을 입력 받아 어텐션 메커니즘을 통해 스 펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성하는 단계; 선택된 상기 고정된 길이의 벡터 를 상기 어텐션 순환 신경망에서 합성된 멜 필터 뱅크 스펙트럼과 연결(Concatenate)하여 음성 특징벡터 합성 순환 신경망의 입력을 생성하는 단계; 및 상기 음성 특징벡터 합성 순환 신경망을 통해 새로운 멜 필터 뱅크 스 펙트럼을 합성하는 단계를 포함할 수 있다. 상기 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하는 단계는, 음향 모델 역할을 하는 상기 음성 특징벡터 합성 순환 신경망을 학습시켜 복수의 화자의 목소리를 합성할 수 있는 순환 신경망을 생성하는 단계를 더 포함할 수 있다. 상기 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하는 단계는, 상기 음성 특징벡터 합성 순환 신경망을 통해 합성된 상기 새로운 멜 필터 뱅크 스펙트럼을 포스트 프로세싱 인공 신경망을 통해 정제하여 로 그 파워 스펙트럼을 합성하는 단계를 더 포함할 수 있다. 합성된 상기 음성 특징벡터를 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 음성으로 변환하는 단계 를 더 포함할 수 있다. 다른 실시예에 따른 음성 합성 시스템은, 입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성하는 문장 임베 딩부; 입력 받은 화자 데이터를 임베딩하여 화자 입력을 구성하는 화자 임베딩부; 임베딩된 상기 문장 데이터와 임베딩된 상기 화자 데이터를 연결(Concatenate)하여 하나의 입력 벡터열을 생성하는 연결부를 포함하여 이루어 질 수 있다. 상기 문장 임베딩부는, 상기 입력 받은 문장 데이터를 한글 자모 단위로 분해하여 자모 단위 입력을 생성하는 문장 분해부; 상기 자모 단위 입력을 색인하여 숫자 데이터로 매핑하는 색인부; 상기 숫자 데이터로 매핑된 상기 문장 데이터를 원-핫 인코딩(One-hot encoding)하여 원-핫 인코딩된 벡터열을 생성하는 원-핫 인코딩부; 및 상기 원-핫 인코딩된 벡터열을 문장 임베딩 매트릭스와 곱하여 문장 데이터 특징벡터로 변환하는 특징벡터 변환 부를 포함할 수 있다. 상기 화자 임베딩부는, 상기 입력 받은 화자 데이터를 색인하여 숫자 데이터로 매핑하는 색인부; 상기 숫자 데 이터로 매핑된 상기 화자 데이터를 원-핫 인코딩(One-hot encoding)하여 원-핫 인코딩된 벡터열을 생성하는 원- 핫 인코딩부; 및 상기 원-핫 인코딩된 벡터열을 화자 임베딩 매트릭스와 곱하여 화자 데이터 특징벡터로 변환하 는 특징벡터 변환부를 포함할 수 있다. 생성된 상기 입력 벡터열을 하나의 인공 신경망으로 정제하여 정제된 문장 및 화자 데이터를 생성하는 문장 및 화자 데이터 정제 인공 신경망을 더 포함할 수 있다. 상기 정제된 문장 및 화자 데이터를 입력 받아 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하 는 음성 특징벡터 합성부를 더 포함하고, 상기 음성 특징벡터 합성부는, 상기 정제된 문장 및 화자 데이터와 어 텐션(Attention) 순환 신경망의 출력을 입력 받아 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터 를 형성하는 어텐션 메커니즘; 선택된 상기 고정된 길이의 벡터를 상기 어텐션 순환 신경망에서 합성된 멜 필터 뱅크 스펙트럼과 연결(Concatenate)하여 음성 특징벡터 합성 순환 신경망의 입력을 생성하는 연결부; 및 상기 음성 특징벡터 합성 순환 신경망을 통해 새로운 멜 필터 뱅크 스펙트럼을 합성하는 음성 특징벡터 합성 순환 신 경망을 포함할 수 있다. 합성된 상기 음성 특징벡터를 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 음성으로 변환하는 음성 재구성부를 더 포함할 수 있다."}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들에 따르면 문장 데이터와 화자 데이터가 동일한 인공 신경망을 통해 학습되어, 규칙 기반 기 술의 문장 분석부와 달리, 화자 데이터를 반영하는 문장 분석 데이터를 생성함으로써, 화자의 억양, 발음 등을 반영하여 음성을 합성할 수 있는 심화 신경망을 이용한 다화자 음성 합성 방법 및 시스템을 제공할 수 있다. 또한, 본 발명의 실시예들에 따르면 하나의 음향 모델로 여러 화자의 음성 특징벡터를 합성함으로써, 화자의 수 의 증가가 시스템의 용량 및 학습 시간에 크게 영향을 미치지 않는 심화 신경망을 이용한 다화자 음성 합성 방 법 및 시스템을 제공할 수 있다."}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명의 실시예들을 설명한다. 그러나, 기술되는 실시예들은 여러 가지 다른 형태로 변형될 수 있으며, 본 발명의 범위가 이하 설명되는 실시예들에 의하여 한정되는 것은 아니다. 또한,"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여러 실시예들은 당해 기술분야에서 평균적인 지식을 가진 자에게 본 발명을 더욱 완전하게 설명하기 위해서 제 공되는 것이다. 도면에서 요소들의 형상 및 크기 등은 보다 명확한 설명을 위해 과장될 수 있다.기존의 음향 모델 기반 다화자 음성 합성 시스템은 각 화자에 대해서 음향 모델을 따로 생성하는 방식으로 구현 되어, 화자의 수가 늘어날수록 전체 시스템의 용량이 커진다. 이는 각각의 음향 모델을 따로 학습시켜야 하기 때문에 학습 시간 또한 화자 수에 비례하여 늘어난다. 또한, 화자 특성이 음향 모델에만 반영되는 문제가 있다. 규칙 기반 기술은 화자의 억양, 발음에 따른 특성을 반영하지 못한다. 따라서 화자 데이터별 특징을 담지 못하는 문장 데이터를 이용하여 음향 모델을 거쳐도 화자 의 억양, 발음을 제대로 반영하는 음성을 합성할 수 없다. 본 발명은 기존 다화자 음성 합성 시스템이 가지는 문제를 해결하기 위하여, 새로운 방법을 제안한다. 기존 다 화자 음성 합성 시스템이 각 화자에 대해서 각각의 모델을 따로 학습시키는 방법으로 구현되어 화자 수가 늘어 날수록 전체 시스템의 용량이 커지고 학습 속도 등이 증가하는 것과 달리, 본 발명은 하나의 음향 모델이 여러 화자의 목소리를 합성할 수 있도록 설계하여 전체 시스템의 용량이 화자 수에 비례해 커지지 않을 수 있는 방법 을 제공하는 것을 그 목적으로 한다. 또한, 문장 데이터 분석 단계에서도 화자별 특성이 반영되어 더 자연스러 운 다화자 음성 합성이 이루어질 수 있는 방법을 제공하는 것을 그 목적으로 한다. 목적을 달성하기 위한 본 발명의 특징에 따른 음성 합성 시스템은 문장 데이터와 화자 데이터를 함께 분석하는 문장 및 화자 데이터 분석부, 음성 특징벡터를 합성하는 음성 특징벡터 합성부 및 음성 특징벡터를 음성으로 변 환하는 음성 재구성부를 포함할 수 있다. 아래에서 첨부된 도면을 참조하여 본 발명에 따른 심화신경망을 이용 한 다화자 음성 합성 시스템 및 방법에 대하여 상세히 설명한다. 도 1은 본 발명의 일 실시예에 따른 음성 합성 시스템의 개략적인 구성을 나타내는 도면이다. 도 1을 참조하면, 본 발명의 일 실시예에 따른 심화 신경망을 이용한 다화자 음성 합성 시스템은 문장 및 화자 데이터 분석부, 음성 특징벡터 합성부 및 음성 재구성부를 포함하여 이루어질 수 있다. 일 실시예에 따른 심화 신경망을 이용한 다화자 음성 합성 시스템은 음성 합성 시스템의 구성을 포함하거 나 음성 합성 시스템을 나타낼 수 있는 것으로, 아래에서는 간단히 음성합성 시스템이라 하기로 한다. 문장 및 화자 데이터 분석부는 화자 데이터와 문장 데이터를 각각 임베딩하여 다양한 특성을 가지는 특징 벡터로 만들고, 임베딩된 데이터를 인공 신경망을 이용하여 정제한 후, 정제된 데이터를 음향 모델로 넘겨줄 수 있다. 음성 특징벡터 합성부는 정제된 문장 및 화자 데이터를 입력으로 받아 음성 특징벡터를 합성하는 음향 모 델을 포함하여 구성될 수 있다. 음향 모델은 순환 신경망으로 구성되며, 순환 신경망이 시간 순서에 따라 적절 한 문장 화자 데이터 입력을 받을 수 있도록 도와주는 어텐션 메커니즘(Attention mechanism)이 이용될 수 있다. 음성 재구성부는 음성 특징벡터를 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 음성으로 변환 시킬 수 있다. 아래에서 각 구성에 대해 보다 구체적으로 설명하기로 한다. 도 2는 본 발명의 일 실시예에 따른 음성 합성 시스템의 문장 및 화자 데이터 분석부의 구성을 나타내는 도면이 다. 도 2를 참조하면, 일 실시예에 따른 음성 합성 시스템의 문장 및 화자 데이터 분석부는 한글 자모 단위로 들어오는 문장 입력 데이터를 심화 신경망의 입력으로 변경하는 문장 데이터 임베딩(Character embedding) 부분, 화자 입력 데이터를 심화 신경망의 입력으로 변경하는 화자 데이터 임베딩(Speaker embedding) 부분, 임 베딩된 두 데이터를 연결(Concatenate)하여 하나의 벡터로 만드는 부분, 그리고 심화 신경망이 문장 및 화자 입 력 데이터에서 필요한 정보를 취합하여 벡터열로 정제하는 부분으로 이루어진다. 즉, 일 실시예에 따른 음성 합성 시스템의 문장 및 화자 데이터 분석부는 문장 임베딩부, 화자 임베 딩부 및 연결부를 포함하여 이루어질 수 있다. 그리고 음성 합성 시스템의 문장 및 화자 데이터 분 석부는 문장 및 화자 데이터 정제 인공 신경망을 더 포함할 수 있다. 문장 임베딩부는 입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성할 수 있다. 이러한 문장 임베딩 부는 문장 분해부, 색인부, 원-핫 인코딩부 및 특징벡터 변환부를 포함하여 이루어질 수 있다. 여기서, 임베딩은 문장, 화자 등의 불연속적인 기호 데이터를 연속적이고 다양한 특성을 가지는 특징 벡터로 변환하기 위한 과정이다. 문장 분해부는 한글 문장을 자모로 쪼갤 수 있다. 즉, 문장 분해부는 입력 받은 문장 데이터를 한글 자모 단위로 분해하여 자모 단위 입력을 생성할 수 있다. 색인부는 쪼개진 자모에 각각 번호를 매길 수 있으며, 각 자모별 번호가 일대일 대응되는 색인표를 구성하 고 색인표에 따라 번호를 매기게 된다. 즉, 색인부는 자모 단위 입력을 색인하여 숫자 데이터로 매핑할 수 있다. 이러한 두 과정(문장 분해 및 색인)을 거치면 문장 데이터가 숫자 데이터로 변하게 된다. 원-핫 인코딩부는 숫자 데이터로 매핑된 문장 데이터를 원-핫 인코딩(One-hot encoding)하여 원-핫 인코딩 된 벡터열을 생성할 수 있다. 생성된 문장 특징벡터열은 다음 식과 같이 나타낼 수 있다. [수학식 1]"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "그리고, 특징벡터 변환부는 원-핫 인코딩된 벡터열을 문장 임베딩 매트릭스와 곱하여 문장 데이터 특징벡 터로 변환할 수 있다. 즉, 원-핫 인코딩부에서 생성된 숫자 데이터를 원-핫 인코딩(One-hot encoding)한 후, 특징벡터 변환부 에서 생성된 원-핫(One-hot) 벡터(xi)를 수학식 2와 같이 임베딩 매트릭스(e)와 각각 곱하여 특징벡터로 변환할 수 있으며, 다음 식과 같이 표현될 수 있다. [수학식 2]"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "화자 임베딩부는 입력 받은 화자 데이터를 임베딩하여 화자 입력을 구성할 수 있다. 화자 임베딩부 는 색인부, 원-핫 인코딩부 및 특징벡터 변환부를 포함하여 이루어질 수 있다. 색인부는 입력 받은 화자 데이터를 색인하여 숫자 데이터로 매핑할 수 있다. 원-핫 인코딩부는 숫자 데이터로 매핑된 화자 데이터를 원-핫 인코딩(One-hot encoding)할 수 있다. 그리고 특징벡터 변환부는 원-핫 인코딩된 벡터열을 화자 임베딩 매트릭스와 곱하여 화자 데이터 특징벡터로 변환할 수 있다. 화자 데이터 임베딩 과정은 문장 데이터 임베딩 과정과 유사하지만 문장 분해 과정 없이 각 화자 색인표에 의해 화자 데이터가 숫자 데이터로 변경되는 색인 과정부터 시작된다. 화자 데이터 특징벡터 변환은 다음 식과 같이 표현될 수 있다. [수학식 3]"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "연결부는 임베딩된 문장 데이터와 임베딩된 화자 데이터를 연결(Concatenate)하여 하나의 입력 벡터열을 생성할 수 있다. 그리고, 문장 및 화자 데이터 정제 인공 신경망은 생성된 입력 벡터열을 하나의 인공 신경망으로 정제하여 정제된 문장 및 화자 데이터를 생성할 수 있다. 생성된 화자 데이터 특징벡터와 문장 데이터 특징벡터열은 연결(Concatenate)되며, 다음 식과 같이 나타낼 수 있다. 이와 같은 방식으로 화자 벡터를 연결하면 문장의 원하는 부분에 특정 화자를 할당할 수 있게 된다. [수학식 4]"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "수학식 5는 한 문장 내에서 화자 변화를 나타낸 입력 구성의 예시이다. [수학식 5]"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "여기에서 문장의 첫 부분 부터 까지는 화자가 할당되고, 부터 까지는 화자가 할당되 었다. 수학식5와 같은 입력을 이용하여 문장을 합성하면 문장 중간에 음성이 1번 화자에서 2번 화자로 변하는 것을 확인할 수 있다. 도 3은 본 발명의 일 실시예에 따른 음성 합성 시스템의 음성 특징벡터 합성부를 설명하기 위한 도면이다. 도 3을 참조하면, 일 실시예에 따른 음성 합성 시스템의 음성 특징벡터 합성부는 어텐션 메커니즘, 연결부 및 음성 특징벡터 합성 순환 신경망을 포함할 수 있다. 또한, 음성 합성 시스템의 음성 특징 벡터 합성부는 어텐션 순환 신경망을 더 포함할 수 있다. 이러한 음성 합성 시스템의 음성 특징벡터 합성부는 정제된 문장 및 화자 데이터를 입력 받아 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터 를 합성할 수 있다. 어텐션 메커니즘은 정제된 문장 및 화자 데이터와 어텐션(Attention) 순환 신경망의 출력을 입력 받 아 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성할 수 있다. 연결부는 선택된 고정된 길이의 벡터를 어텐션 순환 신경망에서 합성된 멜 필터 뱅크 스펙트럼과 연 결(Concatenate)하여 음성 특징벡터 합성 순환 신경망의 입력을 생성할 수 있다. 음성 특징벡터 합성 순환 신경망은 음성 특징벡터 합성 순환 신경망을 통해 새로운 멜 필터 뱅크 스펙트럼 을 합성할 수 있다. 아래에서 음성 합성 시스템의 음성 특징벡터 합성부의 각 구성에 대해 보다 구체적으로 설명한다. 음성 합성 시스템의 음성 특징벡터 합성부에서는 도 1 및 도 2에서 설명한 문장 및 화자 분석부에서 정제 된 벡터열을 조건으로 음성 특징벡터를 합성할 수 있으며, 다음 식과 같이 나타낼 수 있다. [수학식 6]"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, 는 정제된 문장 및 화자 데이터 벡터열이 생성 과정에서 조건이 되는 것을 의미한다. 순환 인공 신경망(순환 신경망)은 이전 단계에서 합성된 스펙트럼( )을 입력으로 받아 다음 단계 스펙트럼( )을 생 성할 수 있다. 도 3은 음성 특징벡터 합성부의 순환 신경망이 i 스텝 음성 특징벡터를 생성하는 과정의 예를 나타낸 것으로, 음성 벡터를 합성하는 음성 특징벡터 합성 순환 신경망은 정제된 문장 및 화자 데이터를 효율적으로 이용하기 위해 어텐션 메커니즘(Attention mechanism)을 이용할 수 있다. 여기서, 어텐션 메커니즘은 총 세 단계로 이루어질 수 있으며, 정제된 문장 및 화자 데이터 벡터열에서 필요한 부분의 정보를 취하여 고정된 길이의 벡터로 바꿔주는 역할을 할 수 있다. 수학식 7는 어텐션 메커니즘의 첫 단계를 나타낸다. [수학식 7]"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기서, 은 어텐션 순환 신경망이 생성한 결과물이다. 는 정제된 문장 및 화자 데이터 와 함께 어텐 션 순환 신경망의 입력이 된다. 어텐션 순환 신경망은 두 입력이 매칭되는 정도를 로 나타낸다. 이 과정 을 거친 결과물은 다음 식과 같이 생성될 수 있다. [수학식 8]"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "수학식 8의 데이터를 소프트맥스(soft-max) 함수를 이용해 확률의 의미를 가지는 로 변환할 수 있으며, 다음 식과 같이 나타낼 수 있다. [수학식 9]"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "그리고, 수학식 8 및 수학식 9에서 구한 확률 값을 문장 및 확률 데이터와 곱한 후 더하여, 입력 기댓값 벡터 를 생성할 수 있으며, 다음 식과 같이 나타낼 수 있다. [수학식 10]"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "이렇게 구해진 기대값 벡터 는 입력 문장 데이터의 길이 n에 관계 없이 고정된 길이의 벡터가 될 수 있다. 생성된 벡터 는 과 연결(concatenate)되어 음성 특징벡터 합성 순환 신경망의 입력이 될 수 있다. 위 과정을 거쳐 합성된 새로운 멜 필터 뱅크 음성 특징벡터열(y)은 포스트 프로세싱 인공 신경망을 거쳐 로그 파워(log-power) 스펙트로그램(spectrogram) 음성 특징벡터열로 매핑될 수 있다. 문장 및 화자 데이터 분석부 와 음성 특징벡터 합성부에서 사용되는 인공 신경망은 합성된 두 개의 특징벡터열과 실제 음성 특징벡터 데이터 사이의 오차를 통해 학습될 수 있다. 음성 합성 시스템은 도 1에서 설명한 바와 같이 음성 재구성부를 더 포함할 수 있다. 음성 재구성부는 합성된 음성 특징벡터를 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 음성으로 변환할 수 있다. 음성 재구성부에서는 이전 단계에서 최종 합성된 로그 파워 스펙트럼 spectrum) 음성 특징벡터를 이용하여 음성 데이터를 복구할 수 있다. 심화 신경망이 합성한 로그 파워 스펙트럼은 페이즈(Phase) 정보가 없이 매그니튜드 (Magnitude) 정보만을 가지고 있기 때문에 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 새로운 페이 즈 정보를 생성하여 재구성할 수 있다. 아래에서는 일 실시예에 따른 심화 신경망을 이용한 다화자 음성 합성 방법에 대해 설명한다. 일 실시예에 따 른 심화 신경망을 이용한 다화자 음성 합성 방법은 도 1 내지 도 3에서 설명한 일 실시예에 따른 심화 신경망을이용한 다화자 음성 합성 시스템을 이용하여 보다 구체적으로 설명할 수 있다. 일 실시예에 따른 심화 신경망 을 이용한 다화자 음성 합성 시스템은 문장 및 화자 데이터 분석부, 음성 특징벡터 합성부 및 음성 재구성부를 포함할 수 있다. 또한, 음성 합성 시스템의 문장 및 화자 데이터 분석부는 문장 임베딩부, 화자 임베딩부 및 연결부를 포함하여 이루어질 수 있다. 그리고 음성 합성 시스템의 문장 및 화자 데이터 분석부는 문장 및 화자 데이터 정제 인공 신경망을 더 포함할 수 있다. 또한, 음성 합성 시스템의 음성 특징벡터 합성부는 어텐션 메 커니즘, 연결부 및 음성 특징벡터 합성 순환 신경망을 포함할 수 있으며, 어텐션 순환 신경망을 더 포함할 수 있다. 도 4는 본 발명의 일 실시예에 따른 음성 합성 방법의 문장 및 화자 데이터 분석 방법을 나타내는 흐름도이다. 그리고, 도 5는 본 발명의 일 실시예에 따른 문장 임베딩 방법을 나타내는 흐름도이고, 도 6은 본 발명의 일 실 시예에 따른 화자 임베딩 방법을 나타내는 흐름도이다. 도 4를 참조하면, 일 실시예에 따른 심화 신경망을 이용한 다화자 음성 합성 방법의 문장 및 화자 데이터 분석 방법은, 입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성하는 단계, 입력 받은 화자 데이터를 임베 딩하여 화자 입력을 구성하는 단계 및 임베딩된 문장 데이터와 임베딩된 화자 데이터를 연결(Concatenat e)하여 하나의 입력 벡터열을 생성하는 단계를 포함하여 이루어질 수 있다. 또한, 심화 신경망을 이용한 다화자 음성 합성 방법의 문장 및 화자 데이터 분석 방법은 생성된 입력 벡터열을 하나의 인공 신경망으로 정제하여 정제된 문장 및 화자 데이터를 생성하는 단계를 더 포함할 수 있다. 단계에서, 문장 및 화자 데이터 분석부의 문장 임베딩부는 입력 받은 문장 데이터를 임베딩하여 문장 입력 을 구성할 수 있다. 이러한 문장 임베딩부는 입력 받은 문장 데이터에서 한글 자모 단위의 문장 입력 데이터를 심화 신경망의 입력으로 변경하는 문장 임베딩을 할 수 있다. 보다 구체적으로, 도 5를 참조하면, 입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성하는 단계에서, 문장 임베딩부는 입력 받은 문장 데이터를 한글 자모 단위로 분해하여 자모 단위 입력을 생성하는 단계, 자모 단위 입력을 색인하여 숫자 데이터로 매핑하는 단계, 숫자 데이터로 매핑된 문장 데이터를 원-핫 인 코딩(One-hot encoding)하여 원-핫 인코딩(One-hot encoding)된 벡터열을 생성하는 단계 및 원-핫 인코딩 된 벡터열을 문장 임베딩 매트릭스와 곱하여 문장 데이터 특징벡터로 변환하는 단계를 수행할 수 있다. 즉, 문장 임베딩부는 원-핫 인코딩된 벡터열을 문장 임베딩 매트릭스와 곱하여 다양하고 연속된 특성을 가지는 벡터열로 이루어진 문장 입력 벡터열을 생성할 수 있다. 단계에서, 문장 및 화자 데이터 분석부의 화자 임베딩부는 입력 받은 화자 데이터를 임베딩하여 화자 입력 을 구성할 수 있다. 화자 임베딩부는 입력 받은 화자 데이터를 심화 신경망의 입력으로 변경하는 화자 임베딩 을 할 수 있다. 보다 구체적으로, 도 6을 참조하면, 입력 받은 화자 데이터를 임베딩하여 화자 입력을 구성하는 단계에서, 화자 임베딩부는 입력 받은 화자 데이터를 색인하여 숫자 데이터로 매핑하는 단계, 숫자 데이터로 매핑된 화자 데이터를 원-핫 인코딩(One-hot encoding)하여 원-핫 인코딩된 벡터열을 생성하는 단계 및 원-핫 인 코딩된 벡터열을 화자 임베딩 매트릭스와 곱하여 화자 데이터 특징벡터로 변환하는 단계를 수행할 수 있다. 즉, 화자 임베딩부는 원-핫 인코딩된 벡터열을 화자 임베딩 매트릭스와 곱하여 다양하고 연속된 특성을 가지는 벡터열로 이루어진 화자 입력 벡터열을 생성할 수 있다. 단계에서, 문장 및 화자 데이터 분석부의 연결부는 임베딩된 문장 데이터와 임베딩된 화자 데이터를 연결 (Concatenate)하여 하나의 입력 벡터열을 생성할 수 있다. 즉, 연결부는 특징벡터로 변환된 임베딩된 문장 데 이터와 임베딩된 화자 데이터를 연결(Concatenate)하여, 문장의 특정 부분에 특정 화자를 할당할 수 있다. 단계에서, 문장 및 화자 데이터 분석부의 문장 및 화자 데이터 정제 인공 신경망은 생성된 입력 벡터열을 하나의 인공 신경망으로 정제하여 정제된 문장 및 화자 데이터를 생성할 수 있다. 이와 같은 방식으로 화자 벡 터를 연결하면 문장의 원하는 부분에 특정 화자를 할당할 수 있게 된다. 여기에서, 생성된 입력 벡터열을 하나 의 인공 신경망으로 정제하여 정제된 문장 및 화자 데이터를 생성하기 위해, 문장 및 화자 데이터를 이용하여 인공 신경망을 학습시킬 수 있다. 도 7은 본 발명의 일 실시예에 따른 음성 합성 방법을 나타내는 흐름도이다. 그리고, 도 8은 본 발명의 일 실 시예에 따른 음성 특징벡터 합성을 나타내는 흐름도이다. 도 7을 참조하면, 일 실시예에 따른 심화 신경망을 이용한 다화자 음성 합성 방법은, 입력 받은 문장 데이터를 임베딩하여 문장 입력을 구성하는 단계, 입력 받은 화자 데이터를 임베딩하여 화자 입력을 구성하는 단계 , 임베딩된 문장 데이터와 임베딩된 화자 데이터를 연결(Concatenate)하여 하나의 입력 벡터열을 생성하는 단계, 생성된 입력 벡터열을 하나의 인공 신경망으로 정제하여 정제된 문장 및 화자 데이터를 생성하는 단 계, 정제된 문장 및 화자 데이터를 입력 받아 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합 성하는 단계 및 합성된 음성 특징벡터를 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하여 음성으로 변환하는 단계를 포함할 수 있다. 일 실시예에 따른 심화 신경망을 이용한 다화자 음성 합성 방법은 도 4 내지 도 6에서 설명한 음성 합성 방법의 문장 및 화자 데이터 분석 방법과 중복되어, 중복되는 부분(단계 내지 단계)의 설명은 생략하고 그 이후 단계에 대해 설명하기로 한다. 단계에서, 음성 특징벡터 합성부는 정제된 문장 및 화자 데이터를 입력 받아 음성 특징벡터 합성 순환 신 경망을 통해 음성 특징벡터를 합성할 수 있다. 보다 구체적으로, 도 8을 참조하면, 음성 특징벡터 합성 순환 신경망을 통해 음성 특징벡터를 합성하는 단계 에서, 음성 특징벡터 합성부는 정제된 문장 및 화자 데이터와 어텐션(Attention) 순환 신경망의 출력을 입 력 받아 어텐션 메커니즘을 통해 스펙트럼 합성에 필요한 부분을 선택하여 고정된 길이의 벡터를 형성하는 단계 , 선택된 고정된 길이의 벡터를 어텐션 순환 신경망에서 합성된 멜 필터 뱅크 스펙트럼과 연결 (Concatenate)하여 음성 특징벡터 합성 순환 신경망의 입력을 생성하는 단계 및 음성 특징벡터 합성 순환 신경망을 통해 새로운 멜 필터 뱅크 스펙트럼을 합성하는 단계를 수행할 수 있다. 그리고, 음성 특징벡터 합성부는 음향 모델 역할을 하는 음성 특징벡터 합성 순환 신경망을 학습시켜 복수의 화 자의 목소리를 합성할 수 있는 순환 신경망을 생성하는 단계를 더 수행할 수도 있다. 또한, 음성 특징벡터 합 성부는 음성 특징벡터 합성 순환 신경망을 통해 합성된 새로운 멜 필터 뱅크 스펙트럼을 포스트 프로세싱 인공 신경망을 통해 정제하여 로그 파워 스펙트럼을 합성하는 단계를 더 수행할 수도 있다. 여기에서, 포스트 프로 세싱 인공 신경망을 학습시킬 수 있다. 단계에서, 음성 재구성부는 합성된 음성 특징벡터를 그리핀-림 알고리즘(Griffin-lim algorithm)을 이용하 여 음성으로 변환할 수 있다. 이상과 같이, 본 발명에 따르면 문장 데이터와 화자 데이터가 동일한 인공 신경망을 통해 학습되기 때문에 규칙 기반 기술의 문장 분석부와 달리 화자 데이터를 반영하는 문장 분석 데이터를 생성할 수 있다. 따라서 기존 기 술에 비하여 화자의 억양, 발음 등을 더 많이 반영하는 음성을 합성할 수 있다. 또한, 하나의 음향 모델로 여러 화자의 음성 특징벡터를 합성해 낼 수 있기 때문에 기존 기술이 화자의 수가 늘 어남에 따라 비례하여 시스템의 용량, 학습 시간이 늘어났던 것과 달리 크게 변하지 않는다는 이점이 있다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 예를 들어, 프로 세서, 컨트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴 퓨터, FPA(field programmable array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상 의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설"}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치 는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 컨트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치에 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터 는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2018-0051601", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2018-0051601", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 음성 합성 시스템의 개략적인 구성을 나타내는 도면이다. 도 2는 본 발명의 일 실시예에 따른 음성 합성 시스템의 문장 및 화자 데이터 분석부의 구성을 나타내는 도면이 다. 도 3은 본 발명의 일 실시예에 따른 음성 합성 시스템의 음성 특징벡터 합성부를 설명하기 위한 도면이다. 도 4는 본 발명의 일 실시예에 따른 음성 합성 방법의 문장 및 화자 데이터 분석 방법을 나타내는 흐름도이다. 도 5는 본 발명의 일 실시예에 따른 문장 임베딩 방법을 나타내는 흐름도이다. 도 6은 본 발명의 일 실시예에 따른 화자 임베딩 방법을 나타내는 흐름도이다. 도 7은 본 발명의 일 실시예에 따른 음성 합성 방법을 나타내는 흐름도이다. 도 8은 본 발명의 일 실시예에 따른 음성 특징벡터 합성을 나타내는 흐름도이다."}
