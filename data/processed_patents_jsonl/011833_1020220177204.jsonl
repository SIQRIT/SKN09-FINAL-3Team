{"patent_id": "10-2022-0177204", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0095661", "출원번호": "10-2022-0177204", "발명의 명칭": "강화학습 기반 히트펌프 제어방법, 장치 및 이를 이용한 냉난방 시스템", "출원인": "한국에너지기술연구원", "발명자": "한광우"}}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "신재생 에너지에 의해 생성된 열을 복수의 히트펌프 중 적어도 하나를 통해 축열조에 저장하고, 상기 축열조에저장된 열을 소정의 공간에 공급하는 냉난방 시스템에서 히트펌프 제어장치가 상기 히트펌프의 구동을 제어하는방법에 있어서,상기 공간의 부하수요, 상기 히트펌프의 성능계수(COP), 상기 신재생 에너지에 의해 생산된 열의 양, 상기 축열조의 축열상태(SOC), 및 기상 데이터를 포함하는 입력데이터를 수집하는 단계;상기 입력데이터를 기 훈련된 강화학습 네트워크에 입력하여, 일정 기간 동안 상기 축열조의 축열상태를 기 설정된 일정 범위로 유지하면서 전기 요금을 최소화시켜 상기 히트펌프 운영이 최적화되도록 액션을 도출하는 단계; 및상기 액션에 기초하여 상기 복수의 히트펌프 각각의 온-오프 구동을 제어하는 단계를 포함하고,상기 강화학습 네트워크는,상기 피크 전력 기반 기본요금에 관련된 피크 보상, 상기 히트펌프가 소모한 에너지량과 관련된 에너지 보상 및상기 축열조의 축열상태에 관련된 SOC 보상에 기초하는 누적 보상이 최대가 되도록 상기 액션을 도출하는,히트펌프 제어방법."}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 입력데이터를 수집하는 단계는,공간 운영 일정을 획득하는 단계; 및기 학습된 제1인공신경망에 상기 기상 데이터 및 상기 공간 운영 일정을 입력하여 상기 공간의 부하수요를 예측하는 단계를 포함하고,상기 공간 운영 일정은 상기 공간을 이용하는 목적 정보, 공간 설정 온도 및 공간 단열 일정 중 적어도 하나를포함하고,상기 기상 데이터는 조도(Irradiance, 일사량), 실외 온도(T), 실외 습도(R.H.), 공간 설정 온도(Tset schedule)및 공간 단열 일정 중 적어도 하나를 포함하는,히트펌프 제어방법."}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 입력데이터를 수집하는 단계는,상기 히트펌프의 부하측 입구온도를 획득하는 단계; 및기 학습된 제2 인공신경망에 상기 히트펌프의 부하측 입구온도 및 상기 실외 온도를 입력하여 상기 히트펌프의성능계수를 예측하는 단계를 포함하는,히트펌프 제어방법."}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,공개특허 10-2024-0095661-3-상기 액션을 도출하는 단계는,상기 복수의 히트펌프에 대한 작동 대수를 결정하는,히트펌프 제어방법."}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서,상기 강화학습 네트워크는,DQN, Double DQN, Prioritized Experience Replay, Dueling Network, Multi-step learning, DistributionalRL 및 Noisy Networks을 적용한 아키텍쳐인 레인보우 DQN을 활용하는, 히트펌프 제어방법."}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "신재생 에너지에 의해 생성된 열을 복수의 히트펌프 중 적어도 하나를 통해 축열조에 저장하고, 상기 축열조에저장된 열을 소정의 공간에 공급하는 난방 시스템에서 상기 복수의 히트펌프의 구동을 제어하는 장치에 있어서,상기 공간의 부하수요, 상기 히트펌프의 성능계수(COP), 상기 신재생 에너지에 의해 생산된 열의 양, 상기 축열조의 축열상태(SOC), 및 기상 데이터를 포함하는 입력데이터를 수집하는 입력부;상기 입력데이터를 미리 학습된 강화학습 네트워크에 입력하여, 기 설정된 일정 기간 동안 상기 축열조의 축열상태를 기 설정된 일정 범위로 유지하면서 냉난방 시스템의 전기 요금을 최소화시켜 상기 히트펌프 운영이 최적화되도록 액션을 도출하는 동작, 및 상기 액션을 기반으로 상기 복수의 히트펌프 각각의 온-오프 구동을 제어하는 동작을 실행하는 프로세서를 포함하고,상기 강화학습 네트워크는,상기 피크 전력 기반 기본요금에 관련된 피크 보상, 상기 히트펌프가 소모한 에너지량과 관련된 에너지 보상 및상기 축열조의 축열상태에 관련된 SOC 보상에 기초하는 누적 보상이 최대가 되도록 상기 액션을 도출하는,히트펌프 제어장치."}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 입력부는,공간 운영 일정을 획득하고, 기 학습된 제1인공신경망에 상기 기상 데이터 및 상기 공간 운영 일정을 입력하여상기 공간 부하수요에 대한 예측을 획득하며, 상기 공간 운영 일정은 상기 공간을 이용하는 목적 정보, 공간 설정 온도 및 공간 단열 일정 중 적어도 하나를포함하고,상기 기상 데이터는 조도(Irradiance, 일사량), 실외 온도(T), 실외 습도(R.H.), 공간 설정 온도(Tset schedule)및 공간 단열 일정 중 적어도 하나를 포함하는,히트펌프 제어장치."}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 6 항에 있어서,상기 입력부는,상기 히트펌프의 부하측 입구온도를 획득하고, 기 학습된 제2 인공신경망에 상기 히트펌프의 부하측 입구온도및 상기 실외 온도를 입력하여 상기 히트펌프의 성능계수에 대한 예측을 획득하는,히트펌프 제어장치.공개특허 10-2024-0095661-4-청구항 9 제 6 항에 있어서,상기 강화학습 네트워크는, DQN, Double DQN, Prioritized Experience Replay, Dueling Network, Multi-step learning, DistributionalRL 및 Noisy Networks을 적용한 아키텍쳐인 레인보우 DQN을 활용하는, 히트펌프 제어장치."}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "신재생 에너지를 통해 열을 생성하는 신재생 에너지 생산장치;상기 신재생 에너지 생산장치에 의해 생성된 열을 저장하는 축열조;상기 축열조에 저장된 열을 공급받는 적어도 하나의 공간;상기 축열조로부터 상기 공간장치로 열을 전달하는 복수의 히트펌프; 및미리 학습된 강화학습 네트워크에 기초하여 공간 냉난방 시스템의 전기 요금을 최소화시켜 상기 히트펌프의 운영이 최적화되도록 상기 복수의 히트펌프의 각 구동을 제어하는 히트펌프 제어장치를 포함하고,상기 강화학습 네트워크는,상기 피크 전력 기반 기본요금에 관련된 피크 보상, 상기 히트펌프가 소모한 에너지량과 관련된 에너지 보상 및상기 축열조의 축열상태에 관련된 SOC 보상에 기초하는 누적 보상이 최대가 되도록 상기 액션을 도출하는,냉난방 시스템."}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10 항에 있어서,상기 심층강화학습 네트워크는, DQN, Double DQN, Prioritized Experience Replay, Dueling Network, Multi-step learning, DistributionalRL 및 Noisy Networks을 적용한 아키텍쳐인 레인보우 DQN을 활용하는, 냉난방 시스템."}
{"patent_id": "10-2022-0177204", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "컴퓨터 프로그램을 저장하고 있는 컴퓨터 판독 가능 기록매체로서,상기 컴퓨터 프로그램은, 프로세서에 의해 실행되면,신재생 에너지에 의해 생성된 열을 복수의 히트펌프 중 적어도 하나를 통해 축열조에 저장하고, 상기 축열조에저장된 열을 소정의 공간에 공급하는 난방 시스템에서 히트펌프 제어장치가 상기 히트펌프의 구동을 제어하는방법에 있어서,상기 공간의 부하수요, 상기 히트펌프의 성능계수(COP), 상기 신재생 에너지에 의해 생산된 열의 양, 상기 축열조의 축열상태(SOC), 및 기상 데이터를 포함하는 입력데이터를 수집하는 단계;상기 입력데이터를 미리 학습된 강화학습 네트워크에 입력하여, 일정 기간 동안 상기 축열조의 축열상태를 기설정된 일정 범위로 유지하면서 전기 요금을 최소화시켜 상기 히트펌프의 운영이 최적화되도록 히트펌프 제어신호를 도출하는 단계; 및상기 히트펌프 제어신호를 기반으로 상기 복수의 히트펌프 각각에 대한 온-오프 구동을 제어하는 단계를 포함하는 방법을 상기 프로세서가 수행하도록 하기 위한 명령어를 포함하고,상기 강화학습 네트워크는,상기 피크 전력 기반 기본요금에 관련된 피크 보상, 상기 히트펌프가 소모한 에너지량과 관련된 에너지 보상 및공개특허 10-2024-0095661-5-상기 축열조의 축열상태에 관련된 SOC 보상에 기초하는 누적 보상이 최대가 되도록 상기 액션을 도출하는,컴퓨터 판독 가능한 기록매체."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 강화학습 기반의 히트펌프 제어방법, 장치 및 이를 이용한 냉난방 시스템에 관한 것으로, 더욱 상세하 게는 공간의 부하수요, 신재생 에너지에 의해 생산된 열의 양, 히트펌프의 성능계수 및 축열조의 축열상태를 종 합적으로 고려하며 강화학습을 활용하여 전기 요금을 최소화시켜 히트펌프의 운영을 최적화하는 정책을 도출하는 강화학습 기반의 히트펌프 제어방법, 장치 및 이를 이용한 냉난방 시스템에 관한 것이다."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 강화학습 기반 히트펌프 제어방법, 장치 및 이를 이용한 냉난방 시스템에 관한 것으로, 더욱 상세하 게는 공간의 부하수요, 신재생 에너지에 의해 생산된 열의 양, 히트펌프의 성능계수 및 축열조의 축열 상태를 종합적으로 고려하며 강화학습을 활용하여 전기 요금을 최소화시켜 히트펌프의 운영을 최적화하는 정책을 도출 하는 강화학습 기반 히트펌프 제어방법, 장치 및 이를 이용한 냉난방 시스템에 관한 것이다."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "국제에너지기구(IEA) 보고서에 따르면 2050년 탄소중립을 위한 다양한 감축 수단 중 가장 큰 부분을 차지하는 방안이 전기화이다. 이러한 중에 히트펌프는 산업, 건축, 농업, 운송의 모든 분야에서 열 에너지의 생산과 사용 에 중요한 역할을 하고 있으며, 탄소중립 실현을 위한 핵심 기술로 진화하고 있다. 히트펌프는 전기로 저온의 열을 고온으로 전달하는 장치로 에너지 절약형 냉난방기나 난방기의 역할을 해왔다. 최근 히트펌프는 건물 냉난방, 산업 공정 열 공급, 전기 자동차 주행거리 증가, 지열과 같은 재생 에너지 보급 확대를 위한 필수 수단이 되었다. 히트펌프의 도입을 가속화하기 위해서는 기존의 저렴한 화석연료 기반 열부하 수요를 전력소비로 대체해야 하기 때문에 히트펌프 운영에 대한 경제성 확보가 필수적이다. 일반적으로 전기 요금은 전기사용량에 따른 전력량 요금, 피크 전력 기반 기본 요금 및 세금 등으로 구성된다. 이는 히트펌프가 소비하는 총 누적 에너지뿐만 아니라 최대 전력도 최적으로 관리해야 함을 의미한다. 신재생 에너지 생산과 에너지 소비가 일치하지 않거나 저장된 열을 모두 소모하게 되면 히트펌프에 큰 부담이 된다. 히트펌프의 최대 피크 전력이 최대 난방 수요에 닿는 경우에는 운영 비용이 크게 증가하는 문제가 발생한 다. 종래에는 룰 또는 모델 기반 예측을 통하여 히트펌프를 제어하였다. 그러나, 이러한 제어 방식들은 히트펌프의 구동을 제어함에 있어서 히트펌프가 환경 변수에 의해 효율이 달라지는 현상을 반영하기 위해 여러 정보들을 종 합적으로 고려하는 데에 한계가 있다. 즉, 룰 이나 모델 기반 예측을 통해 부하 수요의 변동성, 재생 가능한 열 생산, 히트펌프의 성능 계수, 실외 온도, 축열조의 축열상태 등을 종합적으로 고려하여 히트펌프의 구동을 제어 하는 데에 한계가 있다. 따라서, 여러 변수를 종합적으로 고려하여 히트펌프의 구동을 전략적으로 제어하는 기술이 필요하다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 등록특허 제10-2227514호(2020.08.25)"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "따라서, 본 발명은 상술한 바와 같은 문제를 개선하기 위하여 제안된 것으로, 냉난방 시스템에서 공간의 부하수 요, 신재생 에너지에 의해 생산된 열의 양, 히트펌프의 성능계수 및 축열조의 축열 상태를 종합적으로 고려하며 심층강화학습을 활용하여 전기 요금이 최적화되는 정책을 도출하는 강화학습 기반 히트펌프 제어방법, 장치 및 시스템을 제공하는데 목적이 있다. 본 발명의 목적은 이상에서 언급한 것으로 제한되지 않으며, 언급되지 않은 또 다른 목적들은 아래의 기재로부 터 본 발명이 속하는 기술 분야의 통상의 지식을 가진 자에게 명확히 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기와 같은 목적을 달성하기 위한 본 발명의 실시예에 따른 강화학습 기반 히트펌프 제어방법은, 신재생 에너 지에 의해 생성된 열을 복수의 히트펌프 중 적어도 하나를 통해 축열조에 저장하고, 상기 축열조에 저장된 열을 소정의 공간에 공급하는 냉난방 시스템에서 히트펌프 제어장치가 상기 히트펌프의 구동을 제어하는 방법에 있어 서, 상기 공간의 부하수요, 상기 히트펌프의 성능계수(COP), 상기 신재생 에너지에 의해 생산된 열의 양, 상기 축열조의 축열상태(SOC), 및 기상 데이터를 포함하는 입력데이터를 수집하는 단계; 상기 입력데이터를 기 훈련 된 강화학습 네트워크에 입력하여, 일정 기간 동안 상기 축열조의 축열상태를 기 설정된 일정 범위로 유지하면 서 전기 요금을 최소화시켜 상기 히트펌프 운영이 최적화되도록 액션을 도출하는 단계; 및 상기 액션에 기초하 여 상기 복수의 히트펌프 각각의 온-오프 구동을 제어하는 단계를 포함하고, 상기 강화학습 네트워크는, 상기 피크 전력 기반 기본요금에 관련된 피크 보상, 상기 히트펌프가 소모한 에너지량과 관련된 에너지 보상 및 상기 축열조의 축열상태에 관련된 SOC 보상에 기초하는 누적 보상이 최대가 되도록 상기 액션을 도출할 수 있다. 본 발명의 일 실시예에 따른 강화학습 기반 히트펌프 제어장치는, 신재생 에너지에 의해 생성된 열을 복수의 히 트펌프 중 적어도 하나를 통해 축열조에 저장하고, 상기 축열조에 저장된 열을 소정의 공간에 공급하는 난방 시 스템에서 상기 복수의 히트펌프의 구동을 제어하는 장치에 있어서, 상기 공간의 부하수요, 상기 히트펌프의 성 능계수(COP), 상기 신재생 에너지에 의해 생산된 열의 양, 상기 축열조의 축열상태(SOC), 및 기상 데이터를 포 함하는 입력데이터를 수집하는 입력부; 상기 입력데이터를 미리 학습된 강화학습 네트워크에 입력하여, 기 설정 된 일정 기간 동안 상기 축열조의 축열상태를 기 설정된 일정 범위로 유지하면서 냉난방 시스템의 전기 요금을 최소화시켜 상기 히트펌프 운영이 최적화되도록 액션을 도출하는 동작, 및 상기 액션을 기반으로 상기 복수의 히트펌프 각각의 온-오프 구동을 제어하는 동작을 실행하는 프로세서를 포함하고, 상기 강화학습 네트워크는, 상기 피크 전력 기반 기본요금에 관련된 피크 보상, 상기 히트펌프가 소모한 에너지량과 관련된 에너지 보상 및 상기 축열조의 축열상태에 관련된 SOC 보상에 기초하는 누적 보상이 최대가 되도록 상기 액션을 도출할 수 있다. 본 발명의 다른 실시 예에 따른 냉난방 시스템은 신재생 에너지를 통해 열을 생성하는 신재생 에너지 생산장치; 상기 신재생 에너지 생산장치에 의해 생성된 열을 저장하는 축열조; 상기 축열조에 저장된 열을 공급받는 적어 도 하나의 공간; 상기 축열조로부터 상기 공간장치로 열을 전달하는 복수의 히트펌프; 및 미리 학습된 강화학습 네트워크에 기초하여 공간 냉난방 시스템의 전기 요금을 최소화시켜 상기 히트펌프의 운영이 최적화되도록 상기 복수의 히트펌프의 각 구동을 제어하는 히트펌프 제어장치를 포함하고, 상기 강화학습 네트워크는, 상기 피크 전력 기반 기본요금에 관련된 피크 보상, 상기 히트펌프가 소모한 에너지량과 관련된 에너지 보상 및 상기 축열 조의 축열상태에 관련된 SOC 보상에 기초하는 누적 보상이 최대가 되도록 상기 액션을 도출할 수 있다."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예에 따른 강화학습 기반 히트펌프 제어 방법에 따르면, 냉난방 시스템에서 공간의 부하수요, 신 재생 에너지에 의해 생산된 열의 양, 히트펌프의 성능계수 및 축열조의 축열 상태를 종합적으로 고려하며 강화 학습을 활용하여 전기 요금을 최소화시켜 히트펌프 운영을 최적화하는 정책을 도출할 수 있다. 또한, 본 발명에 따르면, 기존의 모델 베이스가 아니라, 데이터 기반으로 히트펌프 제어 모델을 한번 구축하면, 하나의 사이트만이 아니라 다른 사이트에서도 히트펌프 제어 모델을 전이 학습 및 데이터 기반 학습을 통해 활 용함으로써, 활용도를 높이고 관리비용을 절감할 수 있다."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 이상에서 언급한 것으로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재로부 터 본 발명이 속하는 기술 분야의 통상의 지식을 가진 자에게 명확히 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 목적 및 효과, 그리고 그것들을 달성하기 위한 기술적 구성들은 첨부되는 도면과 함께 상세하게 뒤에 설명이 되는 실시 예들을 참조하면 명확해질 것이다. 본 발명을 설명함에 있어서 공지 기능 또는 구성에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명을 생략할 것 이다. 그리고 뒤에 설명되는 용어들은 본 발명에서의 구조, 역할 및 기능 등을 고려하여 정의된 용어들로서 이 는 사용자, 운용자의 의도 또는 관례 등에 따라 달라질 수 있다. 그러나 본 발명은 이하에서 개시되는 실시 예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "다. 단지 본 실시 예들은 본 발명의 개시가 완전하도록 하고, 본 발명이 속하는 기술분야에서 통상의 지식을 가 진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 오로지 특허청구범위에 기재된 청구항의 범주에 의하여 정의될 뿐이다. 그러므로 그 정의는 본 명세서 전반에 걸친 내용을 토대로 내려져야 할 것이다. 명세서 전체에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 이하 첨부된 도면을 참고하여 본 발명의 일 실시예에 따른 심층강화학습 기반의 히트펌프 제어방법, 장치 및 시 스템을 상세히 설명하기로 한다. 도1은 본 발명의 일 실시에에 따른 히트펌프가 이용되는 환경의 일 예를 설명하기 위한 참고도이다. 도 1을 참고하면, 일반적으로 냉난방 시스템에서 평판형 태양열 집열기(FPC)와 진공관형 태양열 집열기(ETC) 모 두에서 수집된 열은 비가열 기간 동안 탱크형 계절 열에너지 저장 장치(STES)와 지하 열 에너지 저장장치(UTE S)에 저장된다. STES 및 GTES에 저장된 열은 난방 기간 동안 축열조로 전달되어 공간 난방 부하로 활용된다. 저장된 물은 이중 수원 HP DSHP의 수원으로 사용할 수 있을 정도로 온도가 충분히 낮을 때 공급한다. 마지막으로 저장된 열이 모두 소진되면 3개의 히트펌프(DSHP도 히트펌프에 의해 구동됨)이 부하 수요를 충당하 는데 사용된다. 한국에서 적용되는 실증규모의 농전기 전기 요금은 피크 전력 기반 기본요금과 전기사용량에 따른 전력량 요금, 기후환경요금, 연료비조정요금, 세금 등을 합산한 금액이다. 설명의 간결성을 위해 기후환경요금, 연료비조정요 금, 세금 등은 생략하기로 한다. 농업용역의 경우 수요 전력을 최대 전력으로 설정한 경우에는 무난방 기간에 전력수요가 없더라도 1년간 동일한 기본요금을 납부하여야 한다. 예를 들어, 3대의 ASHP 장치가 모두 동시에 작 동하는 경우, 최대 부하는 약 127.5kW이며, 기본요금은 1년동안 지불해야 한다. 따라서 동시에 가동되는 히트펌 프의 최대 개수는 2개로 제한되어야 하며, 운영방식은 수요와 에너지 요금을 최소화할 수 있도록 최적화되어야 하는 것이다.도 2는 본 발명의 일 실시예에 따른 심층강화학습 기반의 히트펌프 제어장치를 구비하는 난방 시스템의 개략적 인 구성을 도시하는 예시도이다. 도 2를 참조하면, 본 발명의 일 실시예에 따른 심층강화학습 기반의 히트펌프 제어장치를 채용하는 난방 시스템 은 신재생 에너지 생산장치, 축열조(Buffer tank, 20), 공간, 히트펌프 및 히트펌프 제어장치 를 포함할 수 있다. 본 개시에서는 공간을 온실인 경우를 예로 들었으나, 이에 한정하지 않으며, 난방 공급의 대상이 되는 영역(예를 들면, 건물 등)이라면 모두 채용될 수 있다. 본 발명의 일 실시예에서는 신재생 에너지 생산장치로서 태양열 집열기(ETC)를 채용하고, 신재생 에너지를 통해 열을 생산하는 장치라면 태양열 이외의 다른 신재생을 통한 에너지 생산장치로 대체 가능하고, 일 예로 태 양광일 수 있다. 한국에서 실제로 단기 TES 기간 동안 제어 시스템에서 ETC에서 생산된 열은 축열조에 저장되고, 축 열조에 저장된 열은 온실에서 재배하는 작물별 공간 부하 수요(Thermal load demand)에 따라 온실에 공 급된다. 이때, 축열조에 저장된 열은 히트펌프를 통해 온실로 전달된다. 히트펌프 제어장치는 복수의 히트펌프(40: 41~43) 각각의 온-오프 구동을 결정함으로써 히트펌프 작동 대수를 제어할 수 있다. 본 발명의 일 실시예에서는 공간에서 작물을 재배하는 경우를 일 예로 들었으나, 이에 한정하지 않는다. 즉, 공 간의 부하 수요는 공간의 활용 및 이용 목적 등에 따라 변경될 수 있다. 예를 들어, 공간이 건물인 경우, 건물 내부를 바깥 기온으로부터 보호하기 위한 냉난방 시스템에 관련된 부하 수요일 수 있고, 공간이 온실인 경우, 온실에서 작물을 재배하기 위한 냉난방 시스템에 관련된 부하 수요일 수 있다. 여기서, 축열조의 축열상태(SOC: Stats of charge)는 온실에 공급되는 열량과 열 생산량에 따라 달라진 다. 히트펌프 제어장치는 축열조의 축열 상태가 예를 들어, 기 설정된 일정(min~max) 범위로 유지되도록 축열조에 열을 공급하는 자체 운영 전략에 따라 일 예로, 총 3개의 전기식 공기열원 히트펌프(HP1: 41, HP2: 42, HP3: 43)를 제어할 수 있다. 히트펌프의 개수는 이에 한정하지 않을 수 있다. 히트펌프 제어장치는 히트펌프의 구동에 의한 에너지 소비 효율은 높이면서 전력 요금은 최소화하도록 히트펌프 구동에 대한 운전방식을 최적화하는 것을 목표로 한다. 즉, 히트펌프 제어장치는 공간의 부 하수요, 신재생 에너지에 의해 생산된 열의 양, 히트펌프의 성능계수 및 축열조의 축열 상태를 고려하며 히트펌 프의 작동 대수를 제어함으로써, 축열조의 축열상태를 기 설정된 소정의 min~max 범위로 유지하면서 전기 요금은 최소화하는 방향으로 절감하도록 한다. 특히, 히트펌프 제어장치는 이산동작에 대한 밸류 기반 방법에 특화된 최첨단 DRL 알고리즘인 Rainbow DQN 을 접목하여 히트펌프의 작동 수에 대한 제어 프레임 워크를 구현한다. 여기서, 태양열 집열기는 신재생 에너지를 생산하는 역할로서, 실제로 단기 TES 이전의 난방기간에는 계통 전력(grid electricity)을 거의 사용하지 않고, 이 단기 TES 기간 동안에만 계통전력으로 난방부하를 충당한다 고 할 수 있다. 도 3은 본 발명의 일 실시예에 따른 히트펌프 제어장치의 구성을 도시하는 블록도이다. 히트펌프 제어장치는 학습 데이터를 이용하여 기계 학습을 수행할 수 있는 장치로서, 인공 신경망으로 구 성된 모델을 이용하여 학습하는 장치를 포함할 수 있다. 즉, 히트펌프 제어장치는 데이터 마이닝, 데이터 분석, 지능형 의사 결정 및 기계 학습 알고리즘을 위해 이용될 정보를 수신, 분류, 저장 및 출력하도록 구성될 수 있다. 여기서, 기계 학습 알고리즘은 인공신경망 (ANN: Artificial Neural Network), 레인보우 심층강화학습 네트워크를 포함할 수 있다. 즉, 히트펌프 제어장치는 적어도 하나의 외부 장치(미도시) 또는 단말기(미도시)와 통신할 수 있고, 외부 장치를 대신하여 혹은 도와 데이터를 분석하거나 학습하여 결과를 도출할 수 있다. 여기서, 다른 장치를 도운다 는 의미는 분산 처리를 통한 연산력의 분배를 의미할 수 있다. 히트펌프 제어장치는 인공 신경망을 학습하기 위한 다양한 장치로서, 통상적으로 서버를 의미할 수 있고, 신경망 학습 장치 또는 신경망 학습 서버 등으로 지칭할 수 있다.특히, 히트펌프 제어장치는 단일한 서버뿐만 아니라 복수의 서버 세트, 클라우드 서버 또는 이들의 조합으 로 구현될 수 있다. 즉, 히트펌프 제어장치는 복수로 구성되어 신경망 학습 장치 세트(혹은 클라우드 서버)를 구성할 수 있고, 신경망 학습 장치 세트에 포함된 적어도 하나 이상의 히트펌프 제어장치는 분산 처리를 통하여 데이터 분 석 또는 학습하여 결과를 도출할 수 있다. 히트펌프 제어장치는 주기적으로 혹은 요청에 의하여 외부 장치(미도시)에 기계 학습, 딥 러닝, 심층강화 학습, 레인보우 심층강화학습 네트워크에 의하여 학습한 모델을 전송할 수 있다. 도 3을 참조하면, 히트펌프 제어장치는 통신부, 입력부, 메모리, 러닝 프로세서, 전 원 공급부 및 프로세서 등을 포함할 수 있다. 통신부는 무선 통신부(미도시) 및 인터페이스부(미도시)를 포함하는 구성을 의미할 수 있다. 즉, 통신부 는 유무선 통신이나 인터페이스를 통하여 다른 장치와 데이터를 송수신할 수 있다. 입력부는 모델 학습을 위한 훈련 데이터 또는 학습된 모델을 이용하여 출력을 획득하기 위한 입력 데이터 등을 획득할 수 있다. 입력부는 가공되지 않은 입력 데이터를 획득할 수도 있으며, 이 경우 러닝 프로세서 또는 프로세서 는 획득한 데이터를 전처리하여 모델 학습에 입력이 가능한 훈련 데이터 또는 전처리된 입력 데이터를 생 성할 수 있다. 이때, 입력부는 가공되지 않은 입력 데이터를 획득할 수도 있으며, 이 경우 러닝 프로세서 또는 프로 세서는 획득한 데이터를 전처리하여 모델 학습에 입력이 가능한 훈련 데이터 또는 전처리된 입력 데이터를 생성할 수 있다. 이때, 입력부에서 수행하는 입력 데이터에 대한 전처리는, 입력 데이터로부터 입력 특징점(input featur e)을 추출하는 것을 의미할 수 있다. 또한, 입력부는 통신부를 통하여 데이터를 수신함으로써 데이터를 획득할 수도 있다. 일 실시예로, 입력부는 작물별 온실의 부하수요, 히트펌프의 성능계수(COP), 신재생 에너지 생산 장치에 의해 생성된 열의 양, 및 축열조의 축열상태(SOC)를 포함하는 입력데이터를 수집할 수 있다. 아울러, 입력부는 기상 데이터 및 온실(공간) 운영 일정을 수집할 수 있다. 기상 데이터는 조도(Irradiance, 일사량), 실외 온도(T), 실외 습도(R.H.), 공간 설정 온도(Tset schedule) 및 공간 단열 커튼 일정(Curtain schedule) 중 적어도 하나를 포함할 수 있다. 공간 운영 일정은 공간의 이용 목적, 공간 설정 온도 및 공간 단열 일정 중 적어도 하나를 포함할 수 있다. 또한, 입력부는 히트펌프의 부하측 입구 온도와 히트펌프의 실외 온도 정보를 수집할 수 있다."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "메모리는 러닝 프로세서 또는 히트펌프 제어장치에서 학습된 모델을 저장할 수 있다. 이때, 메모리는 필요에 따라 학습된 모델을 학습 시점 또는 학습 진척도 등에 따라 복수의 버전으로 구분 하여 저장할 수 있다. 이때, 메모리는 입력부에서 획득한 입력 데이터, 모델 학습을 위하여 이용되는 학습 데이터(또는 훈 련 데이터), 모델의 학습 히스토리 등을 저장할 수 있다. 이때, 메모리에 저장된 입력 데이터는 모델 학습에 적합하게 가공된 데이터뿐만 아니라, 가공되지 않은 입 력 데이터 그 자체일 수 있다. 메모리는 모델 저장부 및 데이터베이스 등을 포함할 수 있다. 모델 저장부는 러닝 프로세스을 통하여 학습 중인 또는 학습된 신경망 모델(또는 인공 신경망) 및 학 습된 레인보우 심층강화학습 모델을 저장하며, 학습을 통하여 모델이 업데이트되면 업데이트된 모델을 저장한다. 이때, 모델 저장부는 필요에 따라 학습된 모델을 학습 시점 또는 학습 진척도 등에 따라 복수의버전으로 구분하여 저장할 수 있다. 도 3에 도시된 인공 신경망은 복수의 은닉층을 포함하는 인공 신경망의 하나의 예시일 뿐이며, 본 발명의 인공 신경망이 이에 한정되는 것은 아니다. 인공 신경망(또는 심층강화학습 모델)은 하드웨어, 소프트웨어 또는 하드웨어와 소프트웨어의 조합으로 구현될 수 있다. 인공신경망(또는 강화학습 모델)은 일부 또는 전부가 소프트웨어로 구현되는 경우, 인공신경망(또는 강화학습 모델)을 구성하는 하나 이상의 명령어는 메모리에 저장될 수 있다. 데이터베이스는 입력부에서 획득한 입력 데이터, 모델 학습을 위하여 이용되는 학습 데이터(또는 훈 련 데이터), 모델의 학습 히스토리 등을 저장할 수 있다. 데이터베이스에 저장된 입력 데이터는 모델 학습에 적합하게 가공된 데이터뿐만 아니라, 가공되지 않은 입 력 데이터 그 자체일 수 있다. 러닝 프로세서는 훈련 데이터 또는 트레이닝 셋을 이용하여 인공 신경망 및 강화학습모델 중 적어도 하나 를 훈련시킬 수 있다. 러닝 프로세서는 프로세서가 입력부를 통해 획득한 입력 데이터를 전처리한 데이터를 바로 획득 하여 인공 신경망 또는 심층강화학습모델을 학습하거나, 데이터베이스에 저장된 전처리된 입력 데이터를 획득하여 인공 신경망 또는 심층강화학습모델을 학습할 수 있다. 구체적으로, 러닝 프로세서는 앞서 설명한 다양한 학습 기법을 이용하여 인공신경망(ANN)을 반복적으로 학 습시킴으로써, 인공신경망의 최적화된 모델 파라미터들을 결정할 수 있다. 또한, 러닝 프로세서는 심층강화학습 모델을 반복적으로 실행시켜 시행착오를 거치면서 보상을 최대화하도 록 학습될 수 있다. 즉, 에이전트(agent)는 환경(environment)과 통신하면서, 현재 상태(state)가 주어지면, 인 공지능 에이전트가 특정 행위, 즉 히트펌프 구동 제어를 결정한다. 그리고 결정 사항이 환경(environment)에서 실행되어 상태를 다음 상태(next state)로 변화시킨다. 상태 변화에 따라 환경은 미리 정의한 보상(reward)을 에이전트에 제시한다. 그러면, 에이전트는 미래의 보상의 합이 최대화되도록 특정 상태에 대한 최선의 액션을 제시하는 네트워크를 학습시킨다. 즉, 심층강화학습(DRL: Deep Reinforcement Learning)은 확률적 환경에서 시간 단계에 따른 누적 보상을 극대화 하는 정책을 찾는 것을 목표로 한다. 본 발명에서는 먼저, DRL 접근 방식을 사용하여 히트펌프 제어의 최적화 문제를 해결하기 위해 Markov 결정 프 로세스(MDP)를 공식화한다. MDP는 순차적 결정 문제를 수학적으로 정의하며 상태, 액션 및 보상으로 구성된다. 그 후, 최적의 히트펌프 제어방법을 얻기 위해 DRL 접근 방식으로 레인보우 DQN 알고리즘을 활용한다. 이에 대 해서는 하기에서 자세히 상술하기로 한다. 본 명세서에서는 학습 데이터를 이용하여 학습됨으로써 파라미터가 결정된 인공 신경망을 학습 모델 또는 학습 된 모델이라 칭할 수 있다. 이때, 학습 모델은 인공 신경망의 학습 장치에 탑재된 상태에서 결과 값을 추론할 수도 있으며, 통신부 를 통해 단말기 또는 외부 장치와 같은 다른 장치에 전송되어 탑재될 수도 있다. 또한, 학습 모델이 업데이트되는 경우, 업데이트된 학습 모델은 통신부를 통해 단말기 또는 외부 장치와 같은 다른 장치에 전송되어 탑재될 수 있다. 또한, 학습 모델은 학습 데이터가 아닌 새로운 입력 데이터에 대하여 결과 값을 추론해 내는데 사용될 수 있다. 러닝 프로세서는 데이터 마이닝, 데이터 분석, 지능형 의사 결정, 및 기계 학습 알고리즘 및 기술을 위해 이용될 정보를 수신, 분류, 저장 및 출력하도록 구성될 수 있다. 러닝 프로세서는 히트펌프 제어장치에 통합되거나 구현된 메모리를 포함할 수 있다. 일부 실시 예에 서, 러닝 프로세서는 메모리를 사용하여 구현될 수 있다. 선택적으로 또는 부가적으로, 러닝 프로세서는 클라우드 컴퓨팅 환경에서 유지되는 메모리, 또는 네트워크 와 같은 통신 방식을 통해 단말기에 의해 액세스 가능한 다른 원격 메모리 위치를 이용하여 구현될 수 있다.러닝 프로세서는 일반적으로 감독 또는 감독되지 않은 학습, 데이터 마이닝, 예측 분석 또는 다른 머신에 서 사용하기 위해 데이터를 식별, 색인화, 카테고리화, 조작, 저장, 검색 및 출력하기 위해 데이터를 하나 이상 의 데이터베이스에 저장하도록 구성될 수 있다. 여기서, 데이터베이스는 메모리, 클라우드 컴퓨팅 환 경에서 유지되는 메모리, 또는 네트워크와 같은 통신 방식을 통해 단말기에 의해 액세스 가능한 다른 원격 메모 리 위치를 이용하여 구현될 수 있다. 러닝 프로세서에 저장된 정보는 다양한 상이한 유형의 데이터 분석 알고리즘 및 기계 학습 알고리즘 중 임 의의 것을 사용하여 프로세서에 의해 이용될 수 있다. 프로세서는 데이터 분석 및 기계 학습 알고리즘을 사용하여 결정되거나, 생성된 정보에 기초하여 히트펌프 제어장치의 적어도 하나의 실행 가능한 동작을 결정 또는 예측할 수 있다. 이를 위해, 프로세서는 러 닝 프로세서의 데이터를 요청, 검색, 수신 또는 활용할 수 있고, 상기 적어도 하나의 실행 가능한 동작 중 예측되는 동작이나, 바람직한 것으로 판단되는 동작을 실행하도록 히트펌프 제어장치를 제어할 수 있다. 프로세서는 지능적 에뮬레이션(즉, 지식 기반 시스템, 추론 시스템 및 지식 획득 시스템)을 구현하는 다양 한 기능을 수행할 수 있다. 이는 적응 시스템, 기계 학습 시스템, 인공신경망 등을 포함하는, 다양한 유형의 시 스템(예컨대, 퍼지 논리 시스템)에 적용될 수 있다. 프로세서는 또한 I/O 처리 모듈, 환경 조건 모듈, 음성-텍스트(STT)처리 모듈, 자연어 처리 모듈, 작업 흐 름 처리 모듈 및 서비스 처리 모듈과 같이, 음성 및 자연 언어 음성 처리를 수반하는 연산을 가능하게 하는 서 브 모듈을 포함할 수 있다. 이들 서브 모듈들 각각은, 단말기에서의 하나 이상의 시스템 또는 데이터 및 모델, 또는 이들의 서브셋 또는 수 퍼 셋에 대한 액세스를 가질 수 있다. 또한, 이들 서브 모듈들 각각은, 어휘 색인, 사용자 데이터, 작업 흐름 모델, 서비스 모델 및 자동 음성 인식(ASR) 시스템을 비롯한 다양한 기능을 제공할 수 있다. 다른 실시예에서, 프로세서 또는 히트펌프 제어장치의 다른 양태는 서브 모듈, 시스템, 또는 데이터 및 모델로 구현될 수 있다. 일부 예에서, 러닝 프로세서의 데이터에 기초하여, 프로세서는 사용자 입력 또는 자연 언어 입력으로 표현된 문맥 조건 또는 사용자의 의도에 기초하여 요구 사항을 검출하고 감지하도록 구성될 수 있다. 프로세서는 문맥 조건 또는 사용자의 의도에 기초하여 요구 사항을 완전히 결정하는데 필요한 정보를 능동 적으로 이끌어 내고, 획득할 수 있다. 예를 들어, 프로세서는 역사적 입력 및 출력, 패턴 매칭, 모호하지 않은 단어, 입력 의도 등을 포함하는 과거 데이터를 분석함으로써 요구 사항을 결정하는데 필요한 정보를 능동 적으로 이끌어 낼 수 있다. 프로세서는 문맥 조건 또는 사용자의 의도에 기초하여 요구 사항에 응답하는 기능을 실행하기 위한 태스크 흐름을 결정할 수 있다. 프로세서는 러닝 프로세서에서 프로세싱 및 저장을 위한 정보를 수집하기 위해, 단말기에서 하나 이 상의 감지 컴포넌트를 통해 분석 및 기계 학습 작업에 사용되는 신호 또는 데이터를 수집, 감지, 추출, 검출 및 /또는 수신하도록 구성될 수 있다. 정보 수집은 센서를 통해 정보를 감지하는 것, 메모리에 저장된 정보를 추출하는 것 또는 통신 수단을 통 해 외부 단말기, 엔티티 또는 외부 저장 장치로부터 정보를 수신하는 것을 포함할 수 있다. 프로세서는 히트펌프 제어장치에서 사용 히스토리 정보를 수집하여 메모리에 저장할 수 있다. 프로세서는 저장된 사용 히스토리 정보 및 예측 모델링을 사용하여 특정 기능을 실행하기 위한 최상의 매 치를 결정할 수 있다. 프로세서는 입력부로부터 이미지 정보(또는 해당 신호), 오디오 정보(또는 해당 신호), 데이터 또는 사용 자 입력 정보를 수신할 수 있다. 프로세서는 정보를 실시간으로 수집하고, 정보(예를 들어, 지식 그래프, 명령 정책, 개인화 데이터베이스, 대화 엔진 등)를 처리 또는 분류하고, 처리된 정보를 메모리 또는 러닝 프로세서에 저장할 수 있다. 히트펌프 제어장치의 동작이 데이터 분석 및 기계 학습 알고리즘 및 기술에 기초하여 결정될 때, 프로세서 는 결정된 동작을 실행하기 위해 히트펌프 제어장치의 구성 요소를 제어할 수 있다. 그리고, 프로세서는 제어 명령에 따라 히트펌프 제어장치를 제어하여 결정된 동작을 수행할 수 있다. 프로세서는 특정 동작이 수행되는 경우, 데이터 분석 및 기계 학습 알고리즘 및 기법을 통해 특정 동작의 실행을 나타내는 이력 정보를 분석하고, 분석된 정보에 기초하여 이전에 학습한 정보의 업데이트를 수행할 수 있다. 따라서, 프로세서는 러닝 프로세서와 함께, 업데이트 된 정보에 기초하여 데이터 분석 및 기계 학습 알고리즘의 정확성을 향상시킬 수 있다. 전원 공급부는 프로세서의 제어 하에서, 외부의 전원, 내부의 전원을 인가받아 히트펌프 제어장치 에 포함된 각 구성요소들에 전원을 공급하기 위한 장치를 포함한다. 또한, 이러한 전원 공급부는 배터리를 포함하며, 상기 배터리는 내장형 배터리 또는 교체 가능한 형태의 배터리가 될 수 있다. 다음, 본 발명의 이해를 돕기 위해 심층강화학습 접근법을 설명하기로 한다. DRL(Deep Reinforcement Learning)은 확률적 환경에서 시간 단계에 따른 누적 보상을 극대화하는 정책을 찾는 것을 목표로 한다. 먼저, DRL 접근 방식을 사용하여 히트펌프 제어의 최적화 문제를 해결하기 위해 Markov 결정 프로세스(MDP)를 공식화한다. MDP는 순차적 결정문제를 수학적으로 정의하며 상태, 액션, 보상으로 구성된다. 그 후, 최적의 히트펌프 제어 방법을 얻기 위해 DRL 접근 방식으로 레인보우 DQN 알고리즘을 활용한다. <마르코프 결정 과정> 도 5는 MDP 프레임워크를 도시한다. 에이전트는 ETC, HP, 공간(온실) 및 BT로 구성된 환경과 상호작용하는 히트 펌프를 제어한다. 1) 상태 공식화 에이전트가 환경에서 관찰해야 하는 상태는 태양열 생산, 공간 부하, 축열조의 SOC, 히트펌프의 COP 및 실외 온 도이고, 이들 상태를 종합적으로 고려하여 전기비용을 최소화하고 공간에 안정적으로 열을 공급하기 위해 축열 조를 관리해야 한다. 환경의 복합 상태는 수학식 1과 같이 각 시간 단계에 대한 값으로 설정된다. 와 는 계산된 열 생산 및 각 시간 단계에 대한 예상 부하 수요이다. 와 는 각 시간 단계에서 에이전트의 액션에 영향을 받 는 값이다. 수학식 1"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "2) 액션 공식화 에이전트가 환경에 적용하는 액션은 수학식 2와 같이 시간 단계 t에서 동시에 작동하는 히트펌프의 수로 정의된 다. 이 액션의 값은 수학식에서와 같이 와 사이의 이산 값으로 정의된다."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "의 값은 0이고, 최대 히트펌프의 단위 수가 3일 때 의 값은 3이다. 액션은 의 값이 같더라도 실제 히트펌프들의 드라이브를 제한하지 않기 위해 불연속적인 숫자로 설정된다. 예를 들어, 로컬 로직을 사용하여 동일한 수의 신호로 서로 다른 히트펌프를 번갈아 작동시키는 것이 가능하다. 이것은 히트펌프의 내구성 향상을 위해 필요하다.수학식 2"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "수학식 3"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "3) 보상 공식화 에이전트는 전기 요금을 최소화하는 전략을 추구해야 한다. 또한 축열조의 축열상태가 바운더리를 초과하지 않 아야 한다. 이러한 목적을 고려하여 에이전트가 액션 을 수행한 후, 다음 시간 단계(t+1)에서 얻는 보상은 수 학식 3과 같이 설정된다. 수학식 4"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "는 히트펌프가 소모한 에너지량과 관련된 에너지 보상이고, 는 히트펌프의 동작 대수에 따른 전력 에 기반한 부하 수요에 관련된 피크 보상(피크 전력 기반 기본 요금에 관한 피크 보상)이며, 는 축열조의 축열상태(SOC)에 관련된 SOC 보상이다. 는 각 항의 가중치를 나타낸다. 는 수학식 5에서 7까지로 각각 정의된다. 수학식 5"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "수학식 6"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "수학식 7 패널티 계수는 의 경우 동작되는 히트펌프의 동작대수에 따라 음의 값으로 다르게 부과된다. 히트펌프의 최대 수를 동작시키면 가장 큰 패널티 가 주어지고, 최대 히트펌프 동작 수에서 하나 적은 수를 동작시키면 중간 패널티 가 주어진다. 이러한 패널티 값은 히트펌프의 동시 동작 수를 제한하기 위해 -1과 0 사이에서 휴리스틱하게 설정된다. , 의 경우, 가 바운더리(경계)에 있으면, 보상 계수 가 주어지고, 경계를 벗어나면 패널 티 계수가 로 부과된다. 계수의 값은 1과 -1이다. 최대 경계는 축열조 온도가 55°C 보다 약간 높 더라도 공간 난방에 문제가 없도록 에 1.2를 곱한다. 의 값은 요소의 중요성과 훈 련 과정의 수렴성을 모두 고려하여 결정된다. 세 계수의 합은 하나로 설정된다. 은 DRL이 신경망을 활용하 기 때문에 안정적인 훈련을 위해 항상 -1과 1의 범위에서 도출되도록 공식화된다. 심층강화네트워크는 전술한 피크 전력 기반 기본요금에 관한 피크 보상, 히트펌프가 소모한 에너지량과 관련된 에너지 보상 및 상기 축열조의 축열상태에 관련된 SOC 보상에 기초하는 누적 보상이 최대가 되도록 액션을 도출 한다. < 레인보우 심층강화네트워크 > 히트펌프 작업의 최적 동작을 도출하기 위해서는 특정 유한 시간 단계 동안의 누적 보상을 고려해야 한다. 누적 보상은 수학식 8에 정의되어 있고, 수학식 9로도 도출될 수 있다. 수학식 8"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "수학식 9"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "여기서, 는 할인율로, 미래 보상을 현재 보상으로 전환하기 위해 미래에 받을 보상을 곱한 값이다. 에이전트는 각 상태에서 최대 수익을 위한 최상의 액션을 선택하기 위해 정책 이 필요하다. Q값은 에이전트 가 상태 에서 액션 을 수행할 때 예상되는 총 미래 보상(누적 보상)이며, 수학식 10으로 정의된다. 최적의 정책을 찾는다는 것은 상태 에서 최대 Q함수를 찾는 것으로 해석할 수 있다.수학식 10"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "수학식 11"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "여기서, 는 상태 state s에서 최선의 정책이고, 는 인덱스(action, a)를 최대값으로 출력하는 함 수이다. 레인보우 DQN은 최적의 정책을 찾기 위해 개별적으로 분할되거나 부분적으로 사용되는 7개의 알고리즘을 통합한 다. 레인보우 DQN이란 DQN(Deep Q Network)에서 파생된 7가지 기법들이 혼합된 네트워크이다. 혼합된 기법들은 DQN, Double DQN, Prioritized Experience Replay, Dueling Network, Multi-step learning, Distributional RL 및 Noisy Networks이다. 여기서, DQN은 데이터 간의 상관성을 줄이는 기본적인 DQN이고, Double DQN은 두 개의 Q값, 즉 main Q, target Q 두 개를 두고 서로 보정하면서 학습하는 기법이며, Prioritized Experience Replay는 중요하고 희소한 경험에 가중치를 두는 기법이고, Dueling Network는 Network Q값을 V와 A 두 개로 나누어 서로 결투시키는 방식으로 학 습하게 하는 기법이며, Multi-step learning은 한 번에 여러 개의 스텝을 진행하고 업데이트하게 하는 기법이고, Distributional RL은 가치를 하나의 스칼라 값이 아닌 이산확률 분포의 기댓값으로 예측하게 하는 기 법이며, Noisy Networks는 신경망 가중치에 가우시안 노이즈를 적용해 탐험하게 하는 기법이다. 본 발명에서는 Deep Q network에 Double DQN과 advantage layer와 value layer로 구성된 Dueling DQN을 적용한 아키텍쳐에서, 이 아키텍쳐의 리턴값을 계산하기 위해 기대값이 아닌 distribution형태의 return값을 Distributional RL을 적용하는 것을 특징으로 한다. 이때 1-step의 return distribution이 아닌 n-step의 return값을 활용하는 multi step learning을 적용한다. 또한, Noisy network를 아키텍쳐의 linear layer에 적용하여 기존의 상태 의존적인 입실론 그리디 exploration 보다 개선된 학습효과를 유도할 수 있다. 또한, 리플레이 버퍼의 경우 uniform sampling이 아닌 중요한 정보가 많은 샘플을 더 자주 학습에 활용하는 Prioritized experience replay를 적용한다. 보통 Discrete action에는 value-based 알고리즘이 적합하고 continuous action에는 policy-based 알고리즘이 적합한데, 본 발명에서는 히트펌프의 대수를 제어하기 때문에 레인보우 DQN을 채용하여 discrete action에 적합 한 value-based알고리즘을 적용한다. 여기서, Factorized Gaussian noise가 포함된 노이즈 선형 레이어는 Dueling Network의 어드밴티지 및 밸류 레 이어의 선형 레이어에 적용된다. 분산 RL은 Dueling Network의 아키텍처에 적용된다. 이때, 수학식 12와 같이 1단계 분포 , 대신 다단계 변형 분포 로 대체한다. 수학식 12"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "또한, 다단계 분포 손실과 이중 Q 학습이 결합된다. 온라인 네트워크 에서 선택된 무리한 액션을 부트스트랩 액션 으로 사용하고 이 액션을 타겟 네트워크 를 사용하여 평가할 수 있다. Kullbeck-Leibler (KL) 다이버전스는 수학식 13과 같이 오차로 사용될 수 있다. 는 서포트 z에 대한 L2 투영 을 의미한다. 수학식 13"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "출력을 갖는 어드밴티지 스트림과 출력을 갖는 밸류 스트림을 각 아톰에서 집계하고, 정규화된 파라메트릭 분포를 얻기 위해 소프트맥스를 적용할 수 있다. 그리고, PER(Prioritized Replay) 버퍼의 우선순위를 시간차 오차 대신 전술한 수학식 12의 KL 발산을 사용하도 록 변경할 수 있다. 이 히트펌프 제어를 위한 레인보우 DQN이 적용된 프레임워크의 자세한 내용은 후술하기로 한다. 도 6은 본 발명의 일 실시예에 따른 심층강화학습 기반의 히트펌프 제어 시스템이 히트펌프의 구동 제어를 위해 학습을 실행하는 플로우를 도시한다. 도 6을 참조하면, 본 발명의 일 실시예에 따른 히트펌프의 구동 제어를 위해 모델 학습을 실행하는 방법은 크게 (A) 데이터 준비 과정(Data preparation), (B) 공간의 부하수요와 히트펌프의 성능계수를 예측하는 모델을 학습 시키는 과정(ANN for forecasting), 및 (C) 최적의 히트펌프 구동 정책을 위해 레인보우 심층강화학습을 학습시 키는 과정(Rainbow DQN for control)을 포함할 수 있다. (A) 데이터 준비 과정 입력부는 히트펌프의 구동 제어에 필요한 상태(State)값 중 각 공간의 부하수요와 히트펌프의 성 능계수(COP)를 인공신경망(ANN)을 통해 예측할 수 있도록 데이터를 준비할 수 있다. 여기서, 공간의 부하수요는 공간의 활용 및 이용 목적 등에 따라 변경될 수 있다. 예를 들어, 공간이 온실인 경 우, 온실에서 작물을 재배하기 위한 냉난방 시스템에 관련된 부하 수요일 수 있다. 또는, 공간이 건물인 경우, 건물 내부를 바깥 기온으로부터 보호하기 위한 냉난방 시스템에 관련된 부하 수요일 수 있다. 그리고, 복수의 히트펌프(41~43)는 용량과 성능계수가 동일한 것으로 가정할 수도 있고, 각 히트펌프(41~43) 별로 용량과 성능계수가 다른 것으로 가정할 수도 있다. 구체적으로, 공간의 부하수요를 예측하기 위한 인공신경망 기반으로 5가지 유형의 입력특성에 대해 실제 데이터 를 사용하고, 레이블은 TRNSYS 물리적 모델을 통해 계산된 값을 사용할 수 있다. 여기서, 5가지 유형의 입력특 성은 조도(Irradiance, 일사량), 실외 온도(T), 실외 습도(R.H.), 공간 설정 온도(Tset schedule) 및 공간 단열 일정을 포함하는 입력 데이터를 의미한다.이중 일조량, 실외 온도, 실외 습도에 대해서는 기상청을 통해 과거 기상자료 정보를 획득할 수 있다. 설명을 위해, 각 공간(31, 32)에서 한라봉과 애플망고를 각각 재배하는 경우를 일 예로 들고, 이에 한정하지 않 는다. 각 작물의 생육시기에 따라 공간 설정 온도 및 공간 단열 커튼 일정이 달라질 수 있다. 도 3과 같이 실제 애플망고와 한라봉을 재배하는 각 공간의 가동일정과 설정온도, 및 공간 단열 커튼 일정을 입력 데이터로서 사 용할 수 있다. 과거의 공간 부하수요를 계산하기 위해 예를 들어, 2011년부터 2021년까지의 실제 11년 데이터와 일반 기상 데 이터(년도 단위)가 사용될 수 있다. 공간 모델링에 필요한 기타 파라미터는 도 4의 표와 같고, 실제 값은 TRNSYS type 56 라이브러리를 사용하여 모델링 할 수 있다. 그리고, 입력부는 히트펌프의 부하측 입구온도 및 외기(외부 공기)온도 정보를 획득하고, 그에 매칭하 는 히트펌프의 성능계수를 학습 데이터로서 수집할 수 있다. 히트펌프의 부하측 입구온도 및 실외 온도는 특징 (Feature)이고, 레이블은 성능계수이다. 일 예로, COP 예측을 위한 학습 데이터는 총 1080점의 데이터를 오픈 소스 디지타이저를 통해 제조사에서 제공 한 36점의 그래프로 보간하여 활용할 수 있다. (B) 공간 부하수요와 히트펌프의 성능계수를 예측하는 각 모델을 학습시키는 과정 러닝 프로세서는 기상 데이터 및 공간 운영 일정을 포함하는 입력 데이터를 기반으로 제1 인공신경망을 학 습시켜 학습된 제1 인공신경망이 공간 부하수요를 예측하도록 할 수 있다. 여기서, 기상 데이터는 조도(Irradiance, 일사량), 실외 온도(T), 실외 습도(R.H.), 공간 설정 온도(Tset schedule) 및 공간 단열 일정 중 적어도 하나를 포함할 수 있고, 전술했듯이 학습을 위해 과거 기상 데이터가 수집될 수 있다. 예를 들어, 한 종의 작물을 재배하는 공간 별로 부하수요를 예측하고, 각 공간 별로 예측된 부하수요의 값을 합 산할 수 있다. 본 발명에서는 제1 온실에서는 한라봉을 재배하고, 제2 온실에서는 애플망고를 재배하는 경우를 가정하였다. 이 경우, 각 작물별 온실의 부하수요를 예측하는 인공신경망이 각각 구비될 수 있다. 또한, 러닝 프로세서는 히트펌프의 부하측 입구온도 및 실외 온도를 포함하는 입력 데이터를 기반으로 제 2 인공신경망을 학습시켜 학습된 제2 인공신경망이 히트펌프의 성능계수를 예측하도록 할 수 있다. 여기서, 복 수의 히트펌프(HP1, HP2, HP3)의 각 용량 및 성능계수는 동일한 것으로 가정한다. 여기서, 피쳐와 레이블 쌍을 사용하여 물리적 모델없이 레이블을 회귀할 수 있는 ANN을 훈련하고 확인하는 이유 는 다음과 같다. 먼저, 전문가이 미세 조정이 필요한 모델 기반 방식이 아닌 전이 학습을 통해 실제 운영 데이터가 축적되면서 보다 정확하고 배포하기 쉬운 방식을 개발할 수 있기 때문이다. 그리고, PyTorch 프레임 워크와 병렬텐서 GPU 계산을 통해 GRL 에이전트와 더 연결 가능하고 더 빠른 계산 방법을 개발하기 위함이다. 본 발명의 일 실시예에서 사용된 세 가지 유형의 ANN(두 가지 유형의 공간 부하 수요 및 COP)의 아키텍처는 다 층 퍼셉트론을 기반으로 할 수 있다. 히트펌프의 부하측 입구온도를 측정하는 방법은 공지의 기술을 이용할 수 있으므로, 자세한 방법은 생략하기로 한다. (C) 최적의 히트펌프 구동 정책을 위해 심층강화학습을 학습시키는 과정(Rainbow DQN for control) 일 실시 예로, 환경(environment)은 두 가지 목적(예를 들어, 재배 작물) 유형에 대응하는 공간의 부하수요를 회귀할 수 있는 학습된 두 개의 제1 인공신경망(ANN for load demand)과, 히트펌프의 성능계수를 회귀할 수 있 는 제2 인공신경망(ANN for COP)을 포함하는 3개의 모델과, 신재생 에너지 생산 열과 축열조의 축열상태(SOC)를 계산할 수 있는 2개의 모델을 갖춘다. 그리고, 레인보우 에이전트는 도 6을 참조하여 설명한 바와 같이 조기 종료에 빠지지 않고 높은 누적 보상을 얻 을 수 있도록 훈련되었다. 이에 시행착오를 거쳐 환경과 상호작용하며 결국 레인보우 에이전트는 축열조의 축열상태(SOC)를 기 설정된 일정 범위로 유지하면서, 전력량 요금과 기본요금으로 구성된 전기 요금을 최소화하는 방법을 학습할 수 있다. 이에, 레인보우 에이전트는 피크 전력 기반 기본요금에 관련된 피크 보상, 히트펌프가 소모한 에너지량과 관련 된 에너지 보상 및 축열조의 축열상태에 관련된 SOC 보상에 기초하는 누적 보상이 최대가 되도록 액션을 도출함 으로써 히트펌프의 구동을 최적화할 수 있다. 도 7은 본 발명의 일 실시예에 따른 히트펌프 제어를 위한 DRL 훈련 과정을 나타내는 흐름도이다. 훈련 과정 설명에 앞서, 배경 지식을 간단히 설명하자면, DQN은 강화학습 요소 중 environment를 모델링 하는데, 현재의 action에 대한 다음의 state를 environment로부터 생성하고, 그 state를 다음 입력데이터로 사 용하기 때문에, 학습 중에 계속 변경되는 environment에 의해 다음 입력 데이터인 state도 계속 변경되어 학습 이 불안정해진다. 그래서 특정 action으로 environment의 변화를 줄이기 위해서 environment 모델을 두 개로 만 들어, 온라인 모델의 environment는 학습 즉시 변경시키고, 타겟 모델의 environment는 변경시키지 않는다. 타 겟 모델은 일정시점이 지나면 온라인 모델을 그대로 복사해서 변경시킨다. 그리고 다음 state는 타겟 모델에서 획득하기 때문에, 입력데이터의 안정성이 타겟 모델이 복사될 때 까지는 유지될 수 있는 것이다. 먼저, 셋팅 단계(S101)로서, 하이퍼파라미터를 설정하고, 기 훈련된 인공신경망(ANNs)과 모델들을 포함하는 환 경을 준비하고, 훈련 날씨와 스케줄 조건(공간 운영 일정)을 환경에 적용할 수 있다. 다음, 네트워크 초기화 단계(S102)로서, 랜덤 가중치로 온라인 네트워크를 초기화하고, 온라인 네트워크의 가중 치를 복사하여 타겟 네트워크를 초기화하며, 에피소드 i=0으로 설정할 수 있다. 다음으로, 에피소드 초기화 단계(S103)로서, 타임 스텝 t=0, 스코어(점수)=0, 전력량 요금 KPI1=0, 기본요금 KPI2=0으로 설정하고, 환경으로부터 초기 상태 s0 를 관찰할 수 있다. 이때, 축열조의 초기 SOC는 일반화를 위해 랜덤으로 설정될 수 있다. 다음으로, 온라인 네트워크로부터 액션을 선택할 수 있다(S104). 다음으로, 액션을 환경에 수행하고, 환경으로부터 보상, 다음 스텝, 완료 정보를 관찰할 수 있다(S105). 다음으로, 전환 정보를 저장하는 단계(S106)로서, 상태, 액션, 보상, 다음 상태, 완료 정보를 재생 메모리에 저 장할 수 있다. 다음으로, 메트릭 업데이트 단계(S107)로서, 보상으로 점수를 업데이트하고, 전력량 요금 KPI1, 기본요금 KPI2 을 업데이트할 수 있다. 다음으로, 온라인 네트워크 업데이트 단계(S108)로서, 재생 버퍼로부터 샘플 전환, 온라인 네트워크 및 타겟 네 트워크 간의 손실 계산, 손실을 사용하여 경사 하강법 수행, 재생 버퍼의 우선순위 업데이트, 온라인과 타겟 네 트워크의 노이즈 재선정의 실행을 수행할 수 있다. 다음으로, 타겟 네트워크 업데이트 단계(S109)로서, 타겟 네트워크 업데이트 주기마다 타겟 네트워크의 가중치 를 온라인 네트워크의 가중치로 대체할 수 있다. 다음으로, 조기 종료 여부를 판단하는 단계(S110)로서, SOC에 대해 기 설정된 바운더리를 벗어났는지 여부를 확 인할 수 있다. 여기서, SOC에 대한 바운더리를 초과하지 않은 경우, 시간 단계가 종료되었는지 판단하고(S111), 시간 단계가 종료된 경우, 최고의 온라인 네트워크를 저장하는 단계(S112)로서, 점수가 최고 점수를 초과하는 경우 해당 온라인 네트워크를 저장할 수 있다. 반면, 조기 종료 여부를 판단하는 단계(S110)에서 SOC에 대한 바운더리를 초과한 경우, 에피소드 i를 하나 늘려 서 S103 단계로 회귀할 수 있다.그리고, S111 단계에서 시간 단계가 종료되지 않은 경우, 타임 스텝을 하나 늘려 S104 단계로 회귀할 수 있다. 핵심 성과 지표 본 발명의 일 실시예에 따른 학습된 심층강화학습 기반의 히트펌프 제어장치의 핵심성과지표(KPI: Key performance indicator)는 수학식 14와 같이 전력량 요금 KPI1과 기본요금 KPI2로 구성된 전기요금이다. 수학식 14"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "KPI 계산에 사용된 시간 단계는 단기 TES 기간인 2월 1일부터 4월 30일까지로 연간 8760 시간 중 2136 시간이며, T는 2136에 해당한다. 에너지 축열량은 각 단계 에 대한 동작, 의 정격 소비 전력, kWh당 에너지 축열량 , 축열 효율 의 누적관계로 수학식 15와 같이 정의된다. 수학식 15"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "기본요금은 초기 타임스텝에서 현재 타임스텝까지의 최대 동작 , 히트펌프의 정격 소비 전력 , Kw 당 에너지 축열량 , 축열 효율 의 의 관계로 수학식 16과 같이 정의된다. 수학식 16"}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "본 발명의 일 실시예에 따른 심층강화학습 기반의 히트펌프 제어장치는 전력량 요금 KPI1과 기본요금 KPI2를 최 소화하기 위해 각 시간 단계에 대한 액션을 최적화하는 것을 목표로 한다. 도 8은 본 발명의 일 실시예에 따른 학습 과정시 심층강화학습 네트워크 업데이트 과정을 도시하고, 도 9는 본 발명의 일 실시예에 따른 학습된 심층강화학습을 활용하여 히트펌프의 구동을 제어하는 순서도를 도시한다. 도 8 및 도 9를 참조하여, 히트펌프 제어장치가 학습된 강화학습을 활용하여 히트펌프의 구동을 제어하는 방법 을 설명하기로 한다. 히트펌프 제어 장치를 구비하는 냉난방 시스템 환경은, 신재생 에너지에 의해 생성된 열을 복수의 히트펌프 중 적어도 하나를 통해 축열조에 저장하고, 상기 축열조에 저장된 열을 소정의 공간에 공급한다. 먼저, 히트펌프 제어장치는 최적의 하이퍼파라미터를 설정하고, 사전 학습된 ANNs 및 모델, 그리고 학습된 온라 인 네트워크를 공간 냉난방 시스템에 셋팅할 수 있다(S201). 여기서, 최적의 하이퍼파라미터는 도 8을 참조하여 설명한 훈련 과정에서 미리 획득될 수 있고, ANN의 학습 방 법에 대해서도 전술한 바 있다. 다음으로, 히트펌프 제어장치는 입력 데이터를 획득할 수 있다(S202). 입력 데이터는 공간의 부하수요, 히트펌프의 성능계수(COP), 신재생 에너지에 의해 생산된 열의 양, 축열조의 축열상태(SOC), 및 기상 데이터를 포함할 수 있다. 구체적으로, 기상 데이터 및 공간 운영 일정을 획득하고, 기 학습된 제1인공신경망에 기상 데이터 및 공간 운영 일정을 입력하여 공간의 부하수요를 예측할 수 있다. 일 실시 예로, 공간 운영 일정은 공간의 이용 목적, 공간 설정 온도 및 공간 단열 일정 중 적어도 하나를 포함 하고, 기상 데이터는 조도(Irradiance, 일사량), 실외 온도(T), 실외 습도(R.H.), 공간 설정 온도(Tset schedule) 및 공간 단열 일정 중 적어도 하나를 포함할 수 있다. 예를 들어, 공간 운영 일정은 온실에서 재배하는 작물 정보, 온실 설정 온도 및 온실 단열 커튼 일정을 포함하 고, 기상 데이터는 조도(Irradiance, 일사량), 실외 온도(T), 실외 습도(R.H.), 온실 설정 온도(Tset schedule) 및 온실 단열 커튼 일정(Curtain schedule) 중 적어도 하나를 포함할 수 있다. 그리고, 히트펌프의 부하측 입구온도 및 실외 온도를 획득하고, 기 학습된 제2 인공신경망에 히트펌프의 부하측 입구온도 및 실외 온도를 입력하여 히트펌프의 성능계수를 예측할 수 있다. 아울러, 신재생 에너지로서 태양열 집열기(ETC)가 생산한 열을 측정할 수 있다. 다음으로, 히트펌프 제어장치는 입력데이터를 미리 학습된 강화학습 네트워크(온라인 네트워크)에 입력하여 일 정기간 동안 축열조의 축열상태를 기 설정된 일정 범위로 유지하면서 전기 요금을 최소화시켜 히트펌프 운영이 최적화되도록 하는 액션을 도출할 수 있다(S203). 이를 위해, 피크 전력 기반 기본요금에 관련된 피크 보상, 히트펌프가 소모한 에너지량과 관련된 에너지 보상 및 축열조의 축열상태에 관련된 SOC 보상에 기초하는 누적 보상이 최대가 되도록 액션을 도출할 수 있다. 다음으로, 액션에 기초하여 복수의 히트펌프의 온-오프 구동을 제어할 수 있다(S204). 여기서, 복수의 히트펌프 의 온-오프 구동에 대한 제어는 복수의 히트펌프에 대한 작동 대수를 결정하는 것을 의미할 수 있다. 이러한 과정을 기 설정된 시간 단위로 각 시간 단계마다 S202 단계로 회귀하여 반복 실행할 수 있다. 도 10은 TMY(Typical meteorological year) 환경 조건에서 레인보우 에이전트를 훈련시킨 후 5년간의 환경조건 에 대한 2016-202년 히트펌프 제어 결과를 RBC(Rule-based control: 규칙 기반 제어)에 의한 히트펌프 제어 결 과와 비교하여 도시하는 그래프이다. 비교 개선을 확인하기 위해 동일한 환경에서 RBC를 대조군으로 사용했다. COP의 경우 RBC는 5년간 평균 3.02, 레인보우 에이전트는 3.73으로 레인보우 에이전트가 히트펌프를 평균 23.5% 더 효율적으로 운영했음을 알 수 있 다. 복수의 히트펌프의 용량과 성능계수가 동일한 경우, 히트펌프의 가동 대수는 RBC의 경우 2017년, 2018년, 2020 년에 비교적 부하수요가 많은 날이 있을 때 히트펌프를 3대까지 가동했다. 반면, 레인보우 에이전트의 경우 부하 수요 조건과 상관없이 모든 히트펌프를 2대로 제한하는 것이 가능했다. 또한, COP도 상대적으로 높게 유지하고, 축열상태(SOC)도 레인보우 에이전트에 의해 상대적으로 더 낮은 상태에 서 유지함을 알 수 있다. 이에 의해, 히트펌프의 최대 운영대수를 줄임으로써 기본요금 절감 효과를 볼 수 있고, COP를 상대적으로 높게 유지함으로써 전력량 요금의 절감 효과를 볼 수 있다. 아울러, 축열 상태를 상대적으로 더 낮은 상태로 유지함 으로써, 기 설정된 일정 범위를 벗어나지 않게 하여 경제성을 추구하면서도 운영 안정성 역시 확보할 수 있게 된다. 도 11은 레인보우 에이전트에 의한 5년 평균 에너지 요금을 RBC에 의한 에너지 요금과 비교하여 도시하는 그래 프이다. 도 12를 참조하면, RBC Ver1 및 Ver2는 RBC 베이스라인과 비교하여 5년 평균 에너지 요금을 각각 6.1% 및 12.2% 줄인 것을 알 수 있다. 그러나, 수요 요금은 두 경우 모두 7.7% 인상되었고, 총 전기요금 절감은 각각 1.1%와 5.0%에 그쳤다. 이는 규칙제어기반 설정값을 변경하여 전기요금을 줄이는 데에는 한계가 있음을 시사한다. 반면, RBC 베이스라인 대비 레인보우 에이전트에 의하면 5년 평균 에너지 요금은 21.7%, 수요요금은 23.1% 인하 되어 총 전기요금이 22.2% 절감되었음을 알 수 있다. 이와 같은 비교 결과를 기반으로, 레인보우 에이전트 기반 히트펌프 제어에 의하면, 5년 연속 시뮬레이션 결과, 기존 규칙기반제어의 베이스라인, 규칙기반 개선 모델 보다 더 효과적으로 기본요금과 전력량 요금 모두 크게 절감하는 개선 효과를 나타내는 것을 확인할 수 있다. 결론적으로, 본 발명의 실시예에 따른 히트펌프 제어 방법에 따르면, 공간 난방 시스템에서 부하수요, 신재생 에너지에 의해 생산된 열의 양, 히트펌프의 성능계수 및 축열조의 축열 상태를 종합적으로 고려하며 강화학습을 활용하여 전기 요금이 최적화되는 정책을 도출할 수 있다. 또한, 본 발명에 따르면, 기존의 모델 베이스가 아니라, 데이터 기반으로 히트펌프 제어 모델을 한번 구축하면, 하나의 사이트만이 아니라 다른 사이트에서도 히트펌프 제어 모델을 전이 학습 및 데이터 기반 학습을 통해 활 용함으로써, 활용도를 높이고 관리비용을 절감할 수 있다. 전술한 히트펌프 제어장치는, 프로세서, 메모리, 사용자 입력장치, 프레젠테이션 장치 중 적어도 일부를 포함하 는 컴퓨팅 장치에 의해 구현될 수 있다. 메모리는, 프로세서에 의해 실행되면 특정 태스크를 수행할 수 있도록 코딩되어 있는 컴퓨터-판독가능 소프트웨어, 애플리케이션, 프로그램 모듈, 루틴, 인스트럭션(instructions), 및/또는 데이터 등을 저장하는 매체이다. 프로세서는 메모리에 저장되어 있는 컴퓨터-판독가능 소프트웨어, 애 플리케이션, 프로그램 모듈, 루틴, 인스트럭션, 및/또는 데이터 등을 판독하여 실행할 수 있다. 사용자 입력장치는 사용자로 하여금 프로세서에게 특정 태스크를 실행하도록 하는 명령을 입력하거나 특정 태스 크의 실행에 필요한 데이터를 입력하도록 하는 수단일 수 있다. 사용자 입력장치는 물리적인 또는 가상적인 키 보드나 키패드, 키버튼, 마우스, 조이스틱, 트랙볼, 터치-민감형 입력수단, 또는 마이크로폰 등을 포함할 수 있 다. 프레젠테이션 장치는 디스플레이, 프린터, 스피커, 또는 진동장치 등을 포함할 수 있다. 컴퓨팅 장치는 스마트폰, 태블릿, 랩탑, 데스크탑, 서버, 클라이언트 등의 다양한 장치를 포함할 수 있다. 컴퓨 팅 장치는 하나의 단일한 스탠드-얼론 장치일 수도 있고, 통신망을 통해 서로 협력하는 다수의 컴퓨팅 장치들로 이루어진 분산형 환경에서 동작하는 다수의 컴퓨팅 장치를 포함할 수 있다. 또한 전술한 히트펌프 제어방법은, 프로세서를 구비하고, 또한 프로세서에 의해 실행되면 인공지능 모델을 활용 한 심층강화학습 기반의 히트펌프 제어방법을 수행할 수 있도록 코딩된 컴퓨터 판독가능 소프트웨어, 애플리케 이션, 프로그램 모듈, 루틴, 인스트럭션, 및/또는 데이터 구조 등을 저장한 메모리를 구비하는 컴퓨팅 장치에 의해 실행될 수 있다. 상술한 본 실시예들은 다양한 수단을 통해 구현될 수 있다. 예를 들어, 본 실시예들은 하드웨어, 펌웨어 (firmware), 소프트웨어 또는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 실시예들에 따른 인공지능 모델을 활용한 영상 진단 방법은 하나 또는 그 이 상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 컨트롤러, 마이크로 컨트롤러 또는 마이크로 프로세서 등에 의해 구현될 수 있다. 예를 들어, 실시예들에 따른 히트펌프 제어방법은 심층 신경망의 뉴런(neuron)과 시냅스(synapse)가 반도체 소 자들로 구현된 인공지능 반도체 장치를 이용하여 구현될 수 있다. 이때 반도체 소자는 현재 사용하는 반도체 소 자들, 예를 들어 SRAM이나 DRAM, NAND 등일 수도 있고, 차세대 반도체 소자들, RRAM이나 STT MRAM, PRAM 등일 수도 있고, 이들의 조합일 수도 있다. 실시예들에 따른 히트펌프 제어방법을 인공지능 반도체 장치를 이용하여 구현할 때, 인공지능 모델을 소프트웨 어로 학습한 결과(가중치)를 어레이로 배치된 시냅스 모방소자에 전사하거나 인공지능 반도체 장치에서 학습을 진행할 수도 있다. 펌웨어나 소프트웨어에 의한 구현의 경우, 본 실시예들에 따른 히트펌프 제어방법은 이상에서 설명된 기능 또는 동작들을 수행하는 장치, 절차 또는 함수 등의 형태로 구현될 수 있다. 소프트웨어 코드는 메모리 유닛에 저장 되어 프로세서에 의해 구동될 수 있다. 메모리 유닛은 상기 프로세서 내부 또는 외부에 위치하여, 이미 공지된 다양한 수단에 의해 프로세서와 데이터를 주고 받을 수 있다. 또한, 위에서 설명한 \"시스템\", \"프로세서\", \"컨트롤러\", \"컴포넌트\", \"모듈\", \"인터페이스\", \"모델\", 또는 \"유 닛\" 등의 용어는 일반적으로 컴퓨터 관련 엔티티 하드웨어, 하드웨어와 소프트웨어의 조합, 소프트웨어 또는 실 행 중인 소프트웨어를 의미할 수 있다. 예를 들어, 전술한 구성요소는 프로세서에 의해서 구동되는 프로세스, 프로세서, 컨트롤러, 제어 프로세서, 개체, 실행 스레드, 프로그램 및/또는 컴퓨터일 수 있지만 이에 국한되지 않는다. 예를 들어, 컨트롤러 또는 프로세서에서 실행 중인 애플리케이션과 컨트롤러 또는 프로세서가 모두 구 성 요소가 될 수 있다. 하나 이상의 구성 요소가 프로세스 및/또는 실행 스레드 내에 있을 수 있으며, 구성 요 소들은 하나의 장치(예: 시스템, 컴퓨팅 디바이스 등)에 위치하거나 둘 이상의 장치에 분산되어 위치할 수 있다. 한편, 또 다른 실시예는 전술한 히트펌프 제어방법을 수행하는, 컴퓨터 기록매체에 저장되는 컴퓨터 프로그램을 제공한다. 또한 또 다른 실시예는 전술한 방법을 실현시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기 록매체를 제공한다. 기록매체에 기록된 프로그램은 컴퓨터에서 읽히어 설치되고 실행됨으로써 전술한 단계들을 실행할 수 있다. 이 와 같이, 컴퓨터가 기록매체에 기록된 프로그램을 읽어 들여 프로그램으로 구현된 기능들을 실행시키기 위하여, 전술한 프로그램은 컴퓨터의 프로세서(CPU)가 컴퓨터의 장치 인터페이스(Interface)를 통해 읽힐 수 있는 C, C++, JAVA, 기계어 등의 컴퓨터 언어로 코드화된 코드(Code)를 포함할 수 있다. 이러한 코드는 전술한 기능들을 정의한 함수 등과 관련된 기능적인 코드를 포함할 수 있고, 전술한 기능들을 컴 퓨터의 프로세서가 소정의 절차대로 실행시키는데 필요한 실행 절차 관련 제어 코드를 포함할 수도 있다. 또한, 이러한 코드는 전술한 기능들을 컴퓨터의 프로세서가 실행시키는데 필요한 추가 정보나 미디어가 컴퓨터 의 내부 또는 외부 메모리의 어느 위치(주소 번지)에서 참조 되어야 하는지에 대한 메모리 참조 관련 코드를 더 포함할 수 있다. 또한, 컴퓨터의 프로세서가 전술한 기능들을 실행시키기 위하여 원격(Remote)에 있는 어떠한 다른 컴퓨터나 서 버 등과 통신이 필요한 경우, 코드는 컴퓨터의 프로세서가 컴퓨터의 통신 모듈을 이용하여 원격(Remote)에 있는 어떠한 다른 컴퓨터나 서버 등과 어떻게 통신해야만 하는지, 통신 시 어떠한 정보나 미디어를 송수신해야 하는 지 등에 대한 통신 관련 코드를 더 포함할 수도 있다. 이상에서 전술한 바와 같은 프로그램을 기록한 컴퓨터로 읽힐 수 있는 기록매체는, 일 예로, ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 미디어 저장장치 등이 있다. 또한 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수 있다."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "그리고, 본 발명을 구현하기 위한 기능적인(Functional) 프로그램과 이와 관련된 코드 및 코드 세그먼트 등은, 기록매체를 읽어서 프로그램을 실행시키는 컴퓨터의 시스템 환경 등을 고려하여, 본 발명이 속하는 기술분야의 프로그래머들에 의해 용이하게 추론되거나 변경될 수도 있다. 히트펌프 제어방법은, 컴퓨터에 의해 실행되는 애플리케이션이나 프로그램 모듈과 같은 컴퓨터에 의해 실행 가 능한 명령어를 포함하는 기록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스 될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독가능 매체는 컴퓨터 저장 매체를 모두 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구 현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 전술한 히트펌프 제어방법은, 단말기에 기본적으로 설치된 애플리케이션(이는 단말기에 기본적으로 탑재된 플랫 폼이나 운영체제 등에 포함된 프로그램을 포함할 수 있다)에 의해 실행될 수 있고, 사용자가 애플리케이션 스토 어 서버, 애플리케이션 또는 해당 서비스와 관련된 웹 서버 등의 애플리케이션 제공 서버를 통해 마스터 단말기 에 직접 설치한 애플리케이션(즉, 프로그램)에 의해 실행될 수도 있다. 이러한 의미에서, 전술한 심층강화학습 기반의 히트펌프 제어방법은 단말기에 기본적으로 설치되거나 사용자에 의해 직접 설치된 애플리케이션(즉, 프 로그램)으로 구현되고 단말기에 등의 컴퓨터로 읽을 수 있는 기록매체에 기록될 수 있다. 이상, 본 발명의 특정 실시예에 대하여 상술하였다. 그러나, 본 발명의 사상 및 범위는 이러한 특정 실시예에 한정되는 것이 아니라, 본 발명의 요지를 변경하지 않는 범위 내에서 다양하게 수정 및 변형이 가능하다는 것을 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자라면 이해할 것이다."}
{"patent_id": "10-2022-0177204", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "따라서, 이상에서 기술한 실시예들은 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이므로, 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 하 며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11"}
{"patent_id": "10-2022-0177204", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도1은 본 발명의 일 실시예에 따른 히트펌프가 이용되는 환경의 일 예를 설명하기 위한 참고도이다. 도 2는 본 발명의 일 실시예에 따른 심층강화학습 기반의 히트펌프 제어장치를 구비하는 난방 시스템의 개략적 인 구성을 도시하는 예시도이다. 도 3은 본 발명의 일 실시예에 따른 히트펌프 제어장치의 구성을 도시하는 블록도이다. 도 4는 공간 모델링에 필요한 기타 파라미터의 일 예를 나타내는 표이다.도 5는 MDP 프레임워크를 도시한다. 도 6은 본 발명의 일 실시예에 따른 심층강화학습 기반의 히트펌프 제어 시스템이 히트펌프의 구동 제어를 위해 학습(training)을 실행하는 플로우를 도시한다. 도 7은 본 발명의 일 실시예에 따른 히트펌프 제어를 위한 DRL 훈련 과정을 나타내는 흐름도이다. 도 8은 본 발명의 일 실시예에 따른 학습 과정시 심층 강화학습 네트워크 업데이트 과정을 도시한 개념도이다. 도 9는 본 발명의 일 실시예에 따른 학습된 심층강화학습을 활용하여 히트펌프의 구동을 제어하는 순서도를 도 시한다. 도 10은 레인보우 에이전트 및 RBC 각각에 의한 히트펌프 제어 결과의 비교를 도시하는 그래프이다. 도 11은 레인보우 에이전트 및 RBC 각각에 의한 5년 평균 에너지 요금을 비교하는 그래프이다."}
