{"patent_id": "10-2022-0015293", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0119313", "출원번호": "10-2022-0015293", "발명의 명칭": "심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템", "출원인": "상명대학교산학협력단", "발명자": "김소현"}}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사람들의 우울 및 불안을 완화하기 위해 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 긍정적인단어들로 구성된 명상용 자기긍정 내면소통 콘텐츠를 제공함으로써 심리 테라피 서비스를 제공하는, 웹서버가구비되는 챗봇 서버; 및 상기 챗봇 서버와 유무선 통신망을 통해 연결되어, AI 챗봇 서비스를 제공받는 사용자 단말을 포함하는 심리테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 챗봇 서버는 1) 사용자를 심층적으로 이해하고 사용자의 현재 감정 상태 및 상황을 파악하는 기술(sentiment analysis); 및 긍정적인 단어들로 구성된 명상 관련 문장들이 생성하는 자기긍정 스크립트 생성(affirmative script generation)을 제공하는, 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 영문 생성 모델은 openAI GPT-2의 경우, simple-GPT를 통해 비교적 적은 데이터로 기확보된 ‘자기긍정 내면소통 스크립트 콘텐츠’를 일차 학습 데이터로 활용하여 미리 학습된(pre-trained) 자연어 생성 모델을 자동으로생성하며, 사전 학습 모델(fine-tuning)을 통해 문법과 어순에 적합한 긍정적인 단어들로 구성된 문장의 결과물을 생성하는, 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "사람들의 우울 및 불안을 완화하기 위한 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대화로 수집된 데이터를 자연어처리와 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 불안하고 우울한 부정적인 단어들을대체하여 긍정적인 단어들로 구성된 명상용 자기긍정 내면소통 콘텐츠를 제공하여 심리 테라피 서비스를 제공하는, 웹서버가 구비되는 챗봇 서버; 및 상기 챗봇 서버와 유무선 통신망을 통해 연결되며, 챗봇 에이전트 U-Me가 설치된 AI 챗봇 서비스를 제공받는 사용자 단말을 포함하는 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항 또는 제2항에 있어서, 상기 사용자 단말은 챗봇 어플리케이션이 설치된 PC, 노트북, 스마트폰 및 태블릿 PC를 사용하는 심리 테라피용자기긍정 내면소통 챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서, 상기 챗봇 서버는 1) 사용자를 심층적으로 이해하고 사용자의 현재 감정 상태 및 상황을 파악하는 기술(sentiment analysis); 및 2) 감정 상태 및 상황에 알맞은 자기긍정 내면소통 콘텐츠 제공을 위한 부정적 감정을 내포하는 단어들을 대체하여 긍정적인 단어들로 구성된 명상 관련 문장들이 생성하는 자기긍정 스크립트 생성(affirmative script generation)을 제공하는, 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "공개특허 10-2023-0119313-3-제4항에 있어서, 영문 생성 모델은 openAI GPT-2의 경우, simple-GPT를 통해 비교적 적은 데이터로 기 확보된 ‘자기긍정 내면소통 스크립트 콘텐츠’를 일차 학습 데이터로 활용하여 미리 학습된(pre-trained) 자연어 생성 모델에 ‘자기긍정 내면소통 스크립트 콘텐츠’를 이용하여 fine-tuning하고, 컨텍스트(context)별 자기긍정 내면소통 스크립트를 자동으로 생성하며, 사전 학습 모델(fine-tuning)을 통해 문법과 어순에 적합한 긍정적인 단어들로 구성된문장의 결과물을 생성하는, 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항 또는 제4항에 있어서, 한글용 명상 스크립트 생성 모델의 베이스 모델은 한글 버전 openAI GPT2 (koGPT2) 모델을 사용하며, 명상용 책및 전문가 집필 명상용 문장을 데이터베이스화하여 Training set로 준비하며, koGPT2 fine-tuning 파이프라인을구축하고, 학습 데이터 기반 전처리 및 feeding 파이프라인 구축 후 학습을 진행하며,기 확보된 ‘자기긍정 내면소통 스크립트 콘텐츠’를 일차 학습 데이터로 활용하여 미리 학습된(pre-trained)자연어 생성 모델에 ‘자기긍정 내면소통 스크립트 콘텐츠’를 이용하여 fine-tuning하고, 컨텍스트(context)별자기긍정 내면소통 스크립트를 자동으로 생성하며, 명상 훈련 대본 종류는 문법과 어순에 맞게 학습 데이터 중 긍정적인 단어들로 구성된 선호도가 높은 문장 또는품질이 높은 문장에 가중치를 주고, 학습 시 더 많이 반영될 수 있도록 학습 데이터 형식을 구조화하는, 심리테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 또는 제4항에 있어서, 상기 챗봇 서버는 WWW 서버; 대화 데이터의 단어들을 기초로 사용자의 감정 상태를 분석하여 부정적인 단어들의 사용 대신에 긍정적인 단어들로 구성된 긍정적인 자기긍정 내면소통 콘텐츠를 제공하도록 AI 챗봇 서비스를 제어하는 제어부; 상기 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대화 데이터를 수집하는 대화 데이터 수집부; 영문 모델 및 한글 모델에 대하여 자연어 처리 알고리즘(GPT-2, fine tuning)을 사용하여 문장에 포함된 단어들로부터 감정을 분석하며, 감정 상태 및 상황에 알맞은 자기긍정 내면소통 콘텐츠를 제공하기 위한 부정적 감정을 내포하는 단어들에 대해서도 명상 관련 문장들이 생성하는 자기긍정 스크립트 생성하는(affirmative scriptgeneration) AI 감정 분석부; 새롭게 생성된 자기긍정 스크립트 생성 유사도와 기준 스크립트 사이의 유사도를 BLEU 알고리즘을 통해 계산하며; 자기긍정 스크립트 생성 완성도(새롭게 생성된 자기긍정 스크립트와 기준 스크립트 사이의 의미 측면에서유사도, 형태 측면에서 차이를 측정하는 자기긍정 스크립트의 품질 평가를 위해 MOS(Mean opinion scores) 단위로 학습데이터의 품질을 평가하는 학습 데이터 평가부; 사용자의 감정 상태 및 상황에 알맞은 특히 부정적 감정을 내포하는 단어들을 대체하여 긍정적인 단어들로 구성된 명상 관련 문장들이 생성하는 자기긍정 내면소통 콘텐츠를 제공하는 자기긍정 내면소통 콘텐츠 제공부; 및 학습데이터와 자기긍정 내면소통 콘텐츠를 제공하는 데이터베이스를 포함하는 심리 테라피용 자기긍정 내면소통챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 또는 제4항에 있어서, 상기 챗봇 서버는 1) 감정 상태 및 상황에 알맞은 부정적인 단어들의 사용 대신에 긍정적인 단어들로 구성된 자기긍정 내면소통콘텐츠를 제공하기 위한 자기긍정 스크립트 생성(affirmative script generation)하기 위해 자연어 생성공개특허 10-2023-0119313-4-(Natural language generation)을 위한 심층 생성모델 OpenAI GPT2 사전 학습 모델 fine-tunning을 활용한 명상용 자기긍정 스크립트를 생성하는, 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 챗봇 서버는 (1) 자기긍정 스크립트 생성 유사도, 즉 새롭게 생성된 자기긍정 스크립트와 기준 스크립트 사이의 유사도를BLEU 알고리즘을 통해 계산하며, BLEU(Bilingual Evaluation Understudy) score는 성과지표로, 생성된 문장 X가 일정한 순서의 단어로 이루어져있고, 기준 문장 Y 또한 일정한 단어들의 배열로 이루어진 경우, syntactic 레벨에서 둘 사이의 유사도를 계산하는 척도인 것을 특징으로 하는, 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템."}
{"patent_id": "10-2022-0015293", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템 및 방법이 개시된다. 심리 테라피용 자기긍정 내면 소통 챗봇 어플리케이션 시스템은 사람들의 우울 및 불안을 완화하기 위한 사용자 단말의 AI 챗봇 에이전트 U-Me 와의 대화로 수집된 데이터를 자연어처리와 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 불안하 고 우울한 부정적인 정서로부터 긍정적인 명상용 자기긍정 내면소통 콘텐츠를 제공하여 심리 테라피 서비스를 제 공하는, 웹서버가 구비되는 챗봇 서버; 및 상기 챗봇 서버와 유무선 통신망을 통해 연결되며, 챗봇 에이전트 U- Me가 설치된 AI 챗봇 서비스를 제공받는 사용자 단말을 포함한다."}
{"patent_id": "10-2022-0015293", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템에 관한 것으로, 보다 상세하게는 2030 청년들과 현대인들과 노인들의 우울증 및 불안감을 완화하기 위한 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대 화로 수집된 데이터를 챗봇 서버의 자연어처리(NLP)와 스크립트 생성기술과 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 긍정적인 단어들로 구성된 명상용 자기긍정 내면소통 콘텐츠를 제공하여 자기긍정 내 면소통 훈련을 하게함으로써 심리 테라피 서비스를 제공하는, 심리 테라피용 자기긍정 내면소통 챗봇 어플리케 이션 시스템에 관한 것이다."}
{"patent_id": "10-2022-0015293", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근, 우울/불안을 겪고 있는 2030 청년세대와 현대인들이 겪고 있는 ‘우울' 및 ‘불안’은 큰 사회적 문제로 대두되고 있다. WHO에 따르면, 우울과 불안에 의해 어려움을 겪는 사람의 수가 1990년 4억 1천만명에서 2013년에는 6억 1천600만 명으로 증가하였으며, 세계 인구의 10%에 달하는 사람들이 불안과 우울을 겪고 있는 것으로 추정하였다. 이러한 우울 및 불안은 지속적인 상실감이나 무력감을 유발해 일상생활을 영위하는데 어려움을 줄 뿐만 아니라 기능 장애 유발, 자살의 위험과 사망률 증가와 같은 심각한 결과를 초래할 수 있다(김경덕&김경혜, 2015). 특히, Twenge, et al에 따르면 다른 연령층과 비교해 우울과 불안을 호소하는 사람의 수가 20대 젊은 층 에서 크게 늘어나고 있는 것으로 나타났다. 우리나라 역시, 20대의 우울감 경험률이 큰 폭으로 증가하고 있다 (보건복지부 국민건강 영양조사보고서, 2016). 이처럼 우울과 불안으로 일상생활에 어려움을 겪는 2030 청년의 비율이 증가함에 따라 심리 센터 확충 등의 정 신질환 문제 해결을 위한 다양한 사회적 현안들이 마련되고 있다. 그러나, 우울과 불안을 겪고 있는 대부분의 20대들은 혼자 스스로 해결하고자 하는 의지가 크게 나타남 (Mojtabai, et al., 2011). 뿐만 아니라 대부분의 20대는 학생 또는 사회 초년생으로 정신질환 치료을 위한 경 제적 여건이 부족하며 정신 질환 진료기록에 따른 취업에 대한 불이익, 타인에 의한 부정적인 사회적 인식 등으 로 인해 정신 질환 치료에 대한 진입 장벽이 크다. 위와 같은 이유에 따라 불안장애와 우울증의 정신적 어려움을 겪고 있는 20대 젊은 층이 제대로된 정신 질환 치 료를 받지 못하고 있는 상황이다. 이러한 한계를 해결하기 위한 한 가지 방안으로 모바일 기반의 디지털 서비스가 한 가지 방안이 될 수 있다. 20 대는 디지털 기기에 익숙한 세대로 모바일 기기 접근성이 뛰어나며 (Bapuji, et al., 2014), 모바일 기반의 디 지털 서비스는 경제적 비용 감소 등의 이유로 정신질환 예방/관리를 위한 효과적 솔루션으로 각광받고 있다 (Marzano, et al., 2015). 그러나, 많은 정신 건강을 위한 모바일 기반 디지털 서비스가 노년층 등 사회 취약 계층에 더욱 집중하여 제공 되고 있으며 20대 젊은 층의 우울 및 불안을 해소하기 위한 디지털 서비스는 아직 미약한 실정이다(전진아&최지 희, 2017). 특히, 한국의 2030 청년들의 우울/불안 지수 및 스트레스 지수가 급격하게 높아지면서 이를 위한 완화 방법으로 심리치료 및 테라피에 대한 관심이 늘어나고 있는 상황이다. 이와 관련된 선행기술1로써, 특허공개번호 10-2018-0098840에서는, \"실시간 심리 치료 시스템 및 그 방법\"이 공 개되어 있다. 도 1은 종래의 챗봇 서버를 사용한 심리 치료 시스템의 구성도이다. 심리 치료 시스템의 중앙 서버는 사용자 정보 저장부, 채팅 질의 처리부, 채팅 감정 저장부, 스트레스 위험도 산출부 및 콘텐츠 추천부를 포함하여 구성된다. 사용자 정보 저장부는 사용자 기기로부터 생체 정보, 사용자 특징 정보 및 사용자 상황 정보 등을 포함 하는 사용자 정보를 전달받고 저장한다. 사용자 정보 저장부는 사용자 정보 중 생체 정보를 분석하여 생체 특징 정보를 추출하고, 생체 특징 정보를 기반으로 생체 감정 정보를 추출한다. 사용자 정보 저장부는 생체 정보를 그대로 저장할 수 있고, 생체 정보에서 추출한 생체 특징 정보를 저장할 수 있다. 또한 생체 정보와 추 출한 생체 특징 정보를 같이 저장할 수도 있다. 또한 사용자 정보 저장부는 사용자 정보를 블록체인에 저장 할 수 있다. 여기서 저장한 사용자 정보는 사용자별로 업데이트될 수도 있고, 사용자별로 사용자 정보를 누적하 여 저장될 수도 있다. 또한, 사용자 정보 중 일부 정보가 업데이트될 수도 있고, 일부 정보는 누적하여 저장될 수도 있다. 예를 들면, 나이 정보는 업데이트하여 저장되고, 생체 특징 정보는 누적하여 저장될 수 있다. 여기서 콘텐츠 전달부는 중앙 서버로부터 수신된 콘텐츠 주소 정보를 상기 사용자 기기에게 전달하 여 콘텐츠를 추천한다. 콘텐츠 주소 정보는 여기서 콘텐츠 주소 정보는 추출한 콘텐츠 정보를 볼 수 있는 URL 주소인 것이 바람직하다. 또한 콘텐츠 정보의 카테고리 정보를 중앙 서버로부터 수신받아 사용자 기기 에게 전달하여 콘텐츠를 추천할 수 있다. 그러나, 심리한 원리가 적용된 앱(App)을 통한 치료, 인지행동적 치료법, mood 모니터링 프로그램 사용 효과 차 이를 알 수 없기 때문에, 특정 심리 테라피 서비스의 장기적 사용 유도가 어렵다. 선행기술문헌 특허문헌 (특허문헌 0001) 특허공개번호 10-2018-0098840 (공개일자 2018년 09월 05일), \"실시간 심리 치료 시스템 및 그 방법\", (주)코아메소드, 김철연"}
{"patent_id": "10-2022-0015293", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상기 문제점을 해결하기 위한 본 발명의 목적은 2030 청년들과 현대인들과 노인들의 우울증 및 불안감을 완화하 기 위한 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대화로 수집된 데이터를 챗봇 서버의 자연어처리(NLP)와 스 크립트 생성기술과 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 부정적인 단어들을 대체하여 긍정적인 단어들로 구성된 명상용 자기긍정 내면소통 콘텐츠를 제공하여 자기긍정 내면소통 훈련을 하게함으로 써 심리 테라피 서비스를 제공하는, 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템을 제공한다. 2030 청년들, 현대들인들, 노인들의 우울증 및 불안을 완화하기 위한 기술을 개발하며, 구체적으로 AI 챗봇 에 이전트 U-Me와의 대화로 수집된 데이터를 챗봇 서버의 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악 하여 자기긍정 내면소통 콘텐츠를 제공하는 AI 챗봇 서비스 “U-Me”를 제공한다."}
{"patent_id": "10-2022-0015293", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 목적을 달성하기 위해, 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템은 사람들의 우울 및 불안을 완화하기 위한 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대화로 수집된 데이터를 자연어처리와 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 불안하고 우울한 부정적인 단어들을 대체하여 긍정적인 단어들로 구성된 명상용 자기긍정 내면소통 콘텐츠를 제공하여 심리 테라피 서비스를 제공하는, 웹서버가 구 비되는 챗봇 서버; 및 상기 챗봇 서버와 유무선 통신망을 통해 연결되며, 챗봇 에이전트 U-Me가 설치된 AI 챗봇 서비스를 제공받는 사용자 단말을 포함한다."}
{"patent_id": "10-2022-0015293", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템은 2030 청년들과 현대인들과 노인들의 우울증 및 불안감을 완화하기 위한 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대화로 수집된 데이터를 챗봇 서 버의 자연어처리(NLP)와 스크립트 생성기술과 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 부 정적인 단어들을 대체하여 긍정적인 단어들로 구성된 명상용 자기긍정 내면소통 콘텐츠를 제공하여 자기긍정 내 면소통 훈련을 하게함으로써 내측전두엽이 활성화되고 편도체가 안정화됨으로써 긍정적인 정서를 유도하고 불안 및 우울의 부정적인 정서를 완화시키는 심리 테라피 서비스를 제공하는 효과가 있다. 2030 청년들의 우울 및 불안을 완화하기 위한 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대화로 수집된 데이터 를 챗봇 서버의 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 부정적인 단어들을 대체하여 긍정 적인 단어들(예, 불행하지 않도록 -> 행복하도록)로 구성된 자기긍정 내면소통 콘텐츠를 제공하는 심리 테라피 AI 챗봇 서비스 “U-Me”를 제공한다. 본 제품의 챗봇 에이전트 U-Me와의 대화로 수집한 데이터를 AI 자연어처리 기술을 통해 분석된 챗봇 서비스 및 기술은 챗봇 서버가 개인의 정신 건강 케어 뿐만 아니라 자신의 내면소통 콘텐츠를 제공하며, 증가하고 있는 우 울/불안감 관련 정신건강 케어 시장에서 적은 비용으로 심리 테라피 치료 효과를 내는 프로그램이며, 우울감에 대한 부정적인 사회적 시선을 벗어나 다양한 우울/불안감의 증상을 보이는 사용자들이 스스로 정신케어를 할 수 있도록 도와준다. 챗봇 서비스는 뿐만 아니라 챗봇 서버가 명상용 자기 긍정 콘텐츠를 제공하여 부정적인 인지적 사고를 긍정적으 로 바꿔줌으로써 행복하고 활발한 생활을 할 수 있도록 독려한다. 따라서, 일반적으로 부정적 증상만을 완화시 키는 심리 테라피의 효과 뿐만 아니라 자주적으로 어려운 심리적 상황을 극복할 수 있도록 하는 셀프케어 서비 스가 접목되어 자기개발 시장 또한 관심을 가지며 성장할 것으로 예상된다. 본 챗봇 서비스는 2030의 젊은 현대인을 주요 대상으로 하고 있으며, 서비스 론칭 후 급격하게 늘어나고 있는 1 인가구, 독거노인 등 다양한 연령층에 적합한 서비스 형태로 확장할 예정이다. 챗봇 서비스는 스마트 기기로 챗봇 서버가 명상용 자기긍정 내면소통 콘텐츠를 제공하여 개인의 정신 건강 케어 뿐만 아니라 자신의 자기긍정 내면소통 콘텐츠를 서비스 사용자들에게 심리적 어려움을 겪는 사람들이 활발한 사회적 관계망을 형성할 수 있도록 확장 가능하다."}
{"patent_id": "10-2022-0015293", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 바람직한 실시예를 첨부된 도면을 참조하여 발명의 구성 및 동작을 상세하게 설명한다. 본 발 명의 설명에 있어서 관련된 공지의 기술 또는 공지의 구성에 대한 구체적인 설명이 본 발명의 요지를 불필요하 게 흐릴 수 있다고 판단되는 경우 그 자세한 설명을 생략한다. 또한, 첨부된 도면 번호는 동일한 구성을 표기할 때에 다른 도면에서 동일한 도면번호를 부여한다. 먼저 하기에서 언급하는 openAI GPT-2에 대해 설명한다. openAI GPT-2는 NLG(Natural Language Generation)에서 가장 탁월한 성능을 보인 소설쓰는 인공지능 모델이라 고 불린다. GPT-2는 WebText라 불리는 40GB 크기의 거대한 코퍼스에 인터넷에서 크롤링한 데이터를 합쳐서 훈련 시킨 언어 모델(Language Model)이다. 자연어 처리(NLP)에서, BERT는 트랜스포머(transformer)의 인코더 스택(encoder stack)만 사용한 모델이고, GPT-2는 트랜스포머의 디코더 스택(decoder stack)만 사용한 모델이다. Encoder는 단순한 self-attention 레이어를 사용하는 반면, Decoder는 Masked Self Attention을 사용한다. GPT-2는 언어 모델에서 자기 회귀 모델(auto-regressive model)이며, 자기 회귀 모델은 사람의 언어, 즉 자연어 의 단어들로 이루어진 시퀀셜 데이터를 이전의 출력이 다음의 입력이 되는 모델이다. GPT-2는 단순히 pre- trained된 모델을 돌려보는 것만에서 그치지 않고 fine-tuning하는 방법을 제공한다. GPT-2는 Byte Pair Encoding을 거친 토큰(token)을 입력 단위로 사용한다. BPE(Byte Pair Encoding)는 서브워드(subword)를 분리하는 알고리즘으로, 빈도수에 따라 문자를 병합하여 서브 워드를 구성한다. 단어를 문자(char) 단위로 쪼갠 뒤, 가장 빈도수가 높은 쌍을 하나로 통합하는 과정을 반복하 여 토큰 딕셔너리를 만든다. 문장의 시작을 알리는 start token과 문장의 끝을 알리는 end token이 각 문장마다 붙어있다고 가정한다. 다른 자연어처리 모델들과 마찬가지로, GPT-2에서도 Positional Encoding을 추가적으로 거쳐 입력으로 임베딩 행렬(embedding matrix)을 입력받는다. 여기서 positional encoding은 각 단어에 순서 정보를 추가하는 것을 의미한다. 즉, 문장 내 단어의 순서들을 넘겨준다고 생각할 수 있다. 입력 벡터는 각 디코더셀의 self-attention 과정을 거친 뒤, 신경망 레이어를 지난 다. 이 과정은 스택에 있는 디코더셀, transformer block의 개수만큼 진행된다. 언어는 단순한 단어의 조합으로는 이해할 수 없으며 맥락(context)을 필요로 한다. 맥락(context) 정보 없이 언어를 완전히 이해하는 것은 불가능하며, 단어들의 조합인 문장의 의미는 다음의 절 차를 겪으며 의미가 확정된다. Process of Composition 은 아래와 같다. 1) Lexical component + Grammatical meaning + Syntactic strcture = Compositonal meaning (one or more), 2) Fix referent (by context), 3) Disambiguation (by context), 4) Meaning shift to fit context 맥락(context)을 고려할 수 있게 하는 기법이 attention이며, 자신을 잘 드러내는 기법이 self-attention이다. 모든 디코더 블럭을 거친 최종 결과물은 입력값에 대한 최종 셀프 어텐션값을 가지고 있습니다. 이를 임베딩 벡 터(embedding vector)와 곱해주면, 각 단어가 다음 단어로 등장할 확률값이 계산된다. 이 중에서 가장 확률값이 높은 것이 출력값이 되며, 또 다음의 입력값이 된다. 본 발명의 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템은2030 청년들과 현대인들과 노인들의 우 울증 및 불안감을 완화하기 위한 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대화로 수집된 데이터를 챗봇 서버 의 자연어처리 알고리즘(GPT-2, fine tuning)와 스크립트 생성기술과 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 부정적인 단어들을 대체하여 긍정적인 단어들로 구성된 명상용 자기긍정 내면소통 콘텐츠를 제공하여 자기긍정 내면소통 훈련을 하게함으로써 내측전두엽이 활성화되고 편도체가 안정화됨으로써 긍정적인 정서를 유도하고 불안 및 우울의 부정적인 정서를 완화시키는 심리 테라피 서비스를 제공하며, 2030 청소년들과 현대인들과, 노인들에게 불안과 우울한 부정적인 정서로부터 긍정적인 정서를 제공하게 된다. 자연어처리(Natual Language Processing, NLP) 알고리즘은 GPT-1, GPT-2, GPT-3, 및 BERT(Bidirectional Encoder Representation of Transformer) 알고리즘이 주로 사용된다. 본 발명의 실시예에서는, 영문 모델은 GPT-2를 사용하였으며, GPT-2(Generative Pre-trained Transformer-2, 생성적 사전학습 변환기-2)는 OpenAI에 서 만든 인공지능이다. 한글 모델은 한글용 명상 스크립트 생성을 위해 한글 버전 openAI GPT2 (koGPT2) 모델을 사용하였다. 2030 청년들, 현대들인들, 노인들의 우울증 및 불안을 완화하기 위한 기술을 개발하며, 구체적으로 AI 챗봇 에 이전트 U-Me와의 대화로 수집된 데이터를 챗봇 서버의 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악 하여 부정적인 단어들을 대체하여 긍정적인 단어들(예, 불행하지 않도록 -> 행복하도록)로 구성된 자기긍정 내 면소통 콘텐츠를 제공하는 AI 챗봇 서비스 “U-Me”를 제공한다. * U-Me 챗봇 서비스 개요 U-Me는 사용자가 챗봇 서버와 함께 뇌 가소성을 높여 마음근력을 강화하면서 우울 및 불안을 경감시키는 자기긍 정 내면소통 훈련을 수행하는 서비스를 제공받는다. 자기긍정 내면소통 훈련은 자기용서, 자기수용, 자기존중등을 포함하는 자기긍정의 글을 자신의 목소리를 따라 읽고 말하는 훈련을 말한다. 이와 같은 내면소통 훈련은 자신에 대한 정보처리, 즉 자신 스스로의 생각과 감정, 행동을 실시간으로 알도록 하며, 자신의 가치를 확인할 수 있는 자기긍정을 유도한다. 자신의 가치를 받아들이고 확인할 수 있는 자기긍정은 기존의 많은 연구들을 통해 긍정적 정서를 유발하는 내측 전전두엽을 활성화한다고 입증되어 왔다. 뿐만 아니라 내측전전두엽의 활성화는 불안이나 분노와 같은 부정적 정서를 유발하는 편도체를 안정화하는 효과 를 주며(Steele, 1988; Cohen&Sherman, 2014). 이는 간단히 말해, 뇌 가소성을 높이는 효과가를 준다. 또한, 자기긍정 내면소통 훈련이 일상생활에서 지속적으로 이루어진다면 정신 건강을 튼튼하게 유지시켜주는 마 음근력이 강화되어(김주환, 2011) 불안 및 우울 등의 정신질환을 효과적으로 관리하고 예방할 수 있다. 따라서, 불안 및 우울을 겪는 사용자가 U-Me 챗봇 서비스를 통해 긍정적인 단어들로 구성된 자기긍정의 내면소 통 훈련을 한다면 내측전전두엽이 활성화되고, 편도체가 안정화됨으로써 긍정적 정서는 유발되고 불안 및 우울 의 부정적 정서는 완화될 것이라 기대되며, 본 과제는 궁극적으로 이러한 내면소통 훈련을 일상생활에서 습관화 할 수 있도록 도움을 주는 것에 목표를 둔다. * U-Me 챗봇의 딥러닝 기반 마음챙김방식 치료용 스크립트 생성 본 과제의 개발 대상은 자기긍정 내면소통 훈련을 통해 긍정적 정서를 유발하는 효과를 가짐. 자기긍정 콘텐츠 를 통해 긍정적 정서를 유발하는데 도움을 주는 기존의 서비스 제품으로는 명상 관련 어플리케이션이 대표적이 다. 그러나, 대부분의 명상 어플리케이션은 사용자에 대한 이해가 부족한 상태로 일방적인 자기긍정 명상 콘텐츠를 제공하고 있다. 사용자는 해당 콘텐츠에 대해 사용자가 공감 및 수긍하기 어렵다. 본 과제는 챗봇 에이전트 U-Me와의 대화로 수집된 데이터를 AI 분석을 통해 사용자를 심층적으로 이해하고 사용 자의 현재 감정 상태 및 상황을 파악하여 이에 적합한 자기긍정 내면소통 콘텐츠를 제공한다. 기술개발 최종목표는 자기긍정 내면소통 챗봇 어플리케이션을 위한 딥러닝 기반 마음챙김방식 치료용 스크립트 생성 기술을 제공하는 것이다. 본 제안 AI 챗봇 서비스의 구체적인 기술 개발 내용은 다음과 같다. 1) 사용자를 심층적으로 이해하고 사용자의 현재 감정 상태 및 상황을 파악하는 기술(문장에 포함된 단어들의 감정 분석, Sentiment analysis) 2) 감정 상태 및 상황에 알맞은 부정적인 단어들을 대체하여 긍정적인 단어들로 구성된 자기긍정 내면소통 콘텐 츠를 제공하기 위한 자기긍정 스크립트 생성(Affirmative script generation) - 최신 딥러닝 기술을 이용한 목표 달성 (목표 1) 감정 상태 및 상황에 알맞은 긍정적인 단어들로 구성된 자기긍정 내면소통 콘텐츠를 제공하기 위한 자 기긍정 스크립트 생성(Affirmative script generation) : 자연어 생성(Natural language generation)을 위한 심층 생성모델 OpenAI GPT2 사전 학습 모델 fine-tunning을 활용한 명상용 자기긍정 스크립트 생성 기술 -> fine-tuning용 학습 데이터 수집은 외부 데이터. 필요시, 데이터 재가공 자기긍정 스크립트 생성 유사도(새롭게 생성된 자기긍정 스크립트와 기준 스크립트 사이의 유사도를 BLEU 알고리즘을 통해 계산) - BLEU(Bilingual Evaluation Understudy) score는 성과지표로, 생성된 문장 X가 일정한 순서의 단어로 이루어 져 있고, 기준 문장 Y 또한 일정한 단어들의 배열로 이루어진 경우, syntactic 레벨에서 둘 사이의 유사도를 계 산하는 척도이다. - 새롭게 생성된 자기 긍정 스크립트와 사람이 제시한 기준 스크립트가 주어졌을 때, 둘 사이의(전체 corpus의 n-gram 맞은 갯수) / (전체 corpus의 n-gram 개수)로 계산된다(여기서, corpus는 문장, n-gram 개수는 연속 배 열된 필수 단어 세트를 지칭한다). 자기긍정 스크립트 생성 완성도(새롭게 생성된 자기긍정 스크립트와 기준 스크립트 사이의 의미 측면에서는 유사도, 형태 측면에서는 차이를 측정하는 자기긍정 스크립트의 품질 평가법)- MOS(Mean opinion scores) 단위 - 튜링 테스트와 유사한 방법을 통해 생성된 자기긍정 스크립트의 품질을 평가하는 방법. 의견 테스트/조사 테 스트 방법을 이용하여 5점 척도 내에서 생성된 스크립트 품질을 평가한다. - 기준 스크립트와 의미상 유사도를 5점 척도로 조사(1점 매우 다름, 6점 매우 유사) - 기준 스크립트와 형태상 차이를 5첨 척도로 조사(1점 매우 유사, 6점 매우 다름) : Natural Language Generation 분야 세계 최고 학회 EMNLP에서 발표한 NLG 평가 방식에 대한 논문의 제안을 따른다(Novikova et al., WhyWe Need New Evaluation Metrics for NLG, EMNLP 2017). 도 2는 본 발명에 따른 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템 구성도이다. 도 3은 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 U-Me 챗봇 서비스의 예상 시나리오를 나타낸 도면 이다. 본 발명의 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템은 사람들의 우울 및 불안을 완화하기 위한 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대화로 수집된 데이터를 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 불안하고 우울한 부정적인 단어들을 대체하여 긍정적 인 단어들로 구성된 명상용 자기긍정 내면소통 콘텐츠를 제공하는 심리 테라피 서비스를 제공하여 자기긍정의 내면소통 훈련을 하게 하는, 웹서버가 구비되는 챗봇 서버; 및 상기 챗봇 서버와 유무선 통신망을 통 해 연결되며, 챗봇 에이전트 U-Me가 설치된 AI 챗봇 서비스를 제공받는 사용자 단말을 포함하며, 2030 청 년들의 우울 및 불안을 완화하기 위한 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대화로 수집된 데이터를 챗봇 서버의 AI 분석을 통해 사용자의 현재 감정 상태 및 상황을 파악하여 2030 청소년들의 불안하고 우울한 부정적 인 단어를 대체하여 긍정적인 정서의 단어들로 구성된 명상용 자기긍정 내면소통 콘텐츠를 제공하여 자기긍정의 내면소통 훈련을 함으로써 심리 테라피 서비스를 제공한다. 사용자 단말은 챗봇 어플리케이션 U-Me가 설치된 PC, 노트북, 스마트폰 및 태블릿 PC를 사용한다. 챗봇 서버는 1) 챗봇 대화 데이터에 기초하여 사용자를 심층적으로 이해하고 사용자의 현재 감정 상태 및 상황을 파악하는 기술(Sentiment analysis); 및 2) 감정 상태 및 상황에 알맞은 자기긍정 내면소통 콘텐츠 제 공을 위한 부정적 감정을 내포하는 단어들에 대해서도 긍정적인 단어들로 구성된 명상 관련 문장들이 생성하는 자기긍정 스크립트 생성한다(affirmative script generation). 챗봇 서버는 사용자 단말을 유무선 통신망을 통해 연결하는 WWW 서버; 대화 데이터의 단어들을 기초 로 사용자의 감정 상태를 분석하여 부정적인 단어들의 사용 대신에 긍정적인 단어들로 구성된 자기긍정 내면소 통 콘텐츠를 제공하도록 AI 챗봇 서비스를 제어하는 제어부; 챗봇 대화의 질의 응답 시에, 사용자 단말의 AI 챗봇 에이전트 U-Me와의 대화 데이터를 수집하는 대화 데이터 수집부; 영문 모델 및 한글 모델에 대하 여 자연어 처리 알고리즘(GPT-2, fine tuning)을 사용하여 문장에 포함된 단어들로부터 감정을 분석하며 (sentiment analysis), 사용자의 감정 상태 및 상황에 알맞은 자기긍정 내면소통 콘텐츠 제공을 위한 부정적 감 정을 내포하는 단어들에 대해서도 명상 관련 문장들이 생성하는 자기긍정 스크립트 생성하는(Affirmative script generation) AI 감정 분석부; 새롭게 생성된 자기긍정 스크립트 생성 유사도와 기준 스크립트 사이 의 유사도를 BLEU 알고리즘을 통해 계산하며; 자기긍정 스크립트 생성 완성도(새롭게 생성된 자기긍정 스크립트 와 기준 스크립트 사이의 의미 측면에서는 유사도, 형태 측면에서는 차이를 측정하는 자기긍정 스크립트의 품질 평가를 위해 MOS(Mean opinion scores) 단위로 학습데이터의 품질을 평가하는 학습 데이터 평가부; 감정 상태 및 상황에 알맞은 특히 부정적 감정을 내포하는 단어들에 대해서도 명상 관련 문장들을 생성하는 자기긍정 내면소통 콘텐츠를 제공하는 자기긍정 내면소통 콘텐츠 제공부; 및 학습데이터와 자기긍정 내면소통 콘텐 츠를 제공하는 데이터베이스를 포함한다. 영문 생성 모델은 openAI GPT-2의 경우, simple-GPT를 통해 비교적 적은 데이터로 사전 학습 모델(fine- tuning)을 통해 기 확보된 ‘자기긍정 내면소통 스크립트 콘텐츠’를 일차 학습 데이터로 활용하여 사용 가능성 을 검증하고 미리 학습된(pre-trained) 자연어 생성 모델에 ‘자기긍정 내면소통 스크립트 콘텐츠’를 이용하여 fine-tuning하고, 컨텍스트(context)별 자기긍정 내면소통 스크립트 자동 생성을 수행하며, 사전 학습 모델 (fine-tuning)을 통해 문법과 어순에 적합한 긍정적인 단어들로 구성된 문장의 결과물을 생성한다. 한글용 명상 스크립트 생성 모델의 베이스 모델은 한글 버전 openAI GPT2 (koGPT2) 모델을 사용하며, 명상용 책 및 전문가 집필 명상용 문장을 데이터베이스화하여 Training set로 준비하며, koGPT2 fine-tuning 파이프라인을구축하고, 학습 데이터 기반 전처리 및 feeding 파이프라인 구축 후 학습을 진행하며, 사전 학습 모델(fine- tuning)을 통해 기 확보된 ‘자기긍정 내면소통 스크립트 콘텐츠’를 일차 학습 데이터로 활용하여 사용 가능성 을 검증하고 미리 학습된(pre-trained) 자연어 생성 모델에 ‘자기긍정 내면소통 스크립트 콘텐츠’를 이용하여 fine-tuning하고, 컨텍스트(context)별 자기긍정 내면소통 스크립트 자동 생성을 수행하며, 사전 학습 모델 (fine-tuning)을 통해 문법과 어순에 적합한 긍정적인 단어들로 구성된 문장의 결과물을 생성하고, 명상 훈련 대본 종류는 문법과 어순에 맞게 학습 데이터 중 선호도가 높은 문장 또는 품질이 높은 문장(긍정적인 단어들로 구성된 문장)에 가중치를 주고, 학습 시 더 많이 반영될 수 있도록 학습 데이터 형식을 구조화한다. [연구개발 목표] 1) 사용자의 감정 상태 및 상황에 알맞은 긍정적인 자기긍정 내면소통 콘텐츠 제공을 위한 자기긍정 스크립트 생성(Affirmative script generation) 기술 개별 사용자가 반복적으로 스마트폰의 명상 앱(App) 사용시, 사용자의 컨텐스트(user context)에 알맞는 스크립 트가 제공되는 기술 사용자의 대화 데이터에서 특정 항목에 대한 데이터가 쌓일수록, 명상 훈련 컨텐츠는 문맥, 리액션, 위로 등 다 양한 상황 포함한 사용자의 맥락 (user context)에 알맞은 텍스트 생성을 수행하고, 이를 자기긍정 내면소통 명 상에 반영 (도 4) 주어진 문자열 히스토리를 바탕으로 다음 단어를 예측하도록 기계학습 모델을 훈련시켜, 개별화된 다양한 훈련 스크립트를 생성. 앞의 단어들이 주어지면 그 다음에 나올 단어를 예측하는 방식을 사용한다. 도 4는 스마트폰에서 챗봇 어플리케이션에서 (A) 자기긍정 내면소통 스크립트 생성 개념도, (B) 자기긍정 내면 소통 스크립트 생성 예시를 나타낸다. ACL, EMNLP 등 세계 최고 자연어 처리 학회에서 인정한 pre-trained 자연어 처리 모델 이용. ACL, NeurIPS 등 세계 최고 AI학회에서 검증된 Transformer 기반 스크립트 자동 생성기 기술을 제안한다. 기 확보된 ‘자기긍정 내면소통 스크립트 콘텐츠’를 일차 학습 데이터로 활용하여 사용 가능성 선험적 검증하 며, 미리 학습된(pre-trained) 자연어 생성 모델에 ‘자기긍정 내면소통 스크립트 콘텐츠’를 이용하여 fine- tuning하고, 맥락별 자기긍정 내면소통 스크립트 자동 생성을 수행한다. 생성 스크립트 별 의미 보존 확인을 위해 BLEU 스코어를 통해 생성 품질 검정. 또한 생성 스크립트 별 문장 형 식 차이 확인을 위해 User study로 품질 검정을 한다. 도 5는 Pre-trained 모델, Transfer learning 기반 마음챙김 훈련용 스크립트 자동생성 기술을 보인 도면이다. 1) 감정 상태 및 상황에 알맞은 자기긍정 내면소통 콘텐츠 제공을 위한 자기긍정 스크립트 생성 사전연구 GPT-2(Generative Pre-trained Transformer-2, 생성적 사전학습 변환기-2)는 OpenAI에서 만든 인공지능이다. 비지도 학습과 생성적 사전학습(generative pre-training)기법, 변환기(transformer)를 적용하였다. 자연어처 리(Natual Language Processing, NLP) 시에 OpenAI(open-source artificial intelligence)에서 만든 GPT, GPT-2, GPT-3도 변환기(transformer)를 사용한다. 번역과 대화, 작문을 할 수 있다. openAI GPT2는 State-of-the-art 기준 text generation 성능이 가장 우수한 모델이며, Causal transformer 기 반으로 QA, 챗봇, 기사 작성 등 다양한 분야에서 높은 성능을 보였다. 도 6은 많은 데이터로 일반적인 모델을 생성하여 제공하고, 각각의 목적에 맞게 사전 학습(fine-tunning)하도록 고안된 GPT-2 모델(Generative Pre-Trained model)이다. 도 7은 자연어처리 GPT-2 알고리즘의 Causal transformer 구조 개요를 나타낸다. 그러나, GPT-2 모델은 우수한 성능만큼 매우 큰 아키텍처를 사용하고, 많은 수의 데이터를 활용하므로, from scratch로 customizing된 모델을 만드는 것에는 많은 무리가 따른다. 이에 따라 기존에 학습된 모델로부터 활용 용도에 맞게 딥러닝 모델을 재학습하는 fine-tuning 방법을 채택한다. openAI GPT-2의 경우, 영문 생성 모델은 simple-GPT를 통해 비교적 적은 데이터로 사전 학습 모델 (fine-tuning)을 통해 괜찮은 수준의 결과물을 생성할 수 있을 것으로 기대한다. 도 8은 영문 생성용 GPT-2 심플 버전 simple-GPT (본 과제 베이스 모델로 사용) 나타낸다. 도 9는 한글 명상 스크립트 생성을 위한 베이스 모델 KoGPT-2 이다. 한글의 경우, 영문의 문장 구조와 언어 모델이 달라 simple-GPT로 fine-tunning 했을 경우, 언어학적 차이에 의 한 한계를 보일 가능성이 농후하다. 타사, 타 기관(예: SK T-brain)사례를 보아도 한글 모델 구축을 위해 from- scratch를 통해 한글 독립적 모델 훈련 후 사용한다. 특히, fine-tuning을 통해 openAI GPT2의 성능을 보장하면 서 적은 데이터로 현재 목적에 맞는 모델 구성의 필요성이 있다. 이에 따라, 한글용 명상 스크립트 생성 모델의 베이스 모델은 SK T-brain의 협조와 허가를 바탕으로 KoGPT-2로 선정하여 사용하였다. 기술적 성숙도(TRL) 정의를 통해, 각 단계별 시험평가를 수행한다."}
{"patent_id": "10-2022-0015293", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "연구과제 수행결과 1) 감정 상태 및 상황에 알맞은 자기긍정 내면소통 콘텐츠 제공을 위한 자기긍정 스크립트 생성 (Affirmative script generation) 기술개발 결과 [영문 모델] - openAI GPT2를 사용하는 경우, simple-GPT를 통해 비교적 적은 데이터로 사전 학습 모델(fine-tuning)을 통 해 괜찮은 수준의 결과물을 생성한다. - 하기 모델을 바탕으로 소수 데이터로 fine-tuning 학습을 진행하고, 결과물을 확인한다. - 셰익스피어 희곡을 소수 데이터 입력으로 simple-GPT fine-tuning을 수행. 아래와 같이 generated text가 희 곡의 형태로 생성된 것을 볼 수 있다. 도 10a는 감정 상태 및 상황에 알맞은 자기긍정 내면소통 콘텐츠 제공을 위한 자기긍정 스크립트 생성 (Affirmative script generation) 기술개발 결과 : 영문 모델 - openAI GPT-2를 사용하는 경우 simple-GPT를통해 비교적 적은 데이터로 fine-tuning을 통해 괜찮은 수준의 결과물을 생성 -. 도 10b는 성경을 학습데이터(Bible data)로 사용하여 생성한 사례 - 성경의 장, 절 형식을 동일하게 생성(Bible data에서 확인 가능한 장:절의 포맷이 그대로 나타나고, 반복적으로 등장하는 God, LORD 등의 단어가 대/소문자 구분까지 고려해 나타나는 것을 확인); 상대적으로 소량의 데이터 (<4MB)로도 성경의 내용과 형식을 재구성한다. 성경을 학습 데이터로 사용하여 fine-tuning 성공 가능성을 위해 극단적인 과적합(overfitting)이 수행되도록 소량의 데이터로 200 epoch 학습 진행. 학습 데이터와 거의 동일한 문장 세트를 생성한다. 이후, gpt-2-simple (https://github.com/minimaxir/gpt-2-simple) 기반에 4MB 분량의 bible data를 바탕으로 fine-tuning 시도하며, 최소한의 데이터 학습으로 생성 효과를 극대화하기 위한 적정 학습 configuration 확인 한다. 성경 데이터(Bible data)에서 확인 가능한 장:절의 포맷이 그대로 나타나고, 반복적으로 등장하는 God, LORD 등 의 단어가 대/소문자 구분까지 고려해서 나타나는 것을 확인한다. 기존 4MB 분량 학습 데이터에서 문장 구조, 형식 및 대체적으로 양호한 문장 생성 결과를 확인(gpt-2-simple; https://github.com/minimaxir/gpt-2-simple 기반). 빠른 학습과 동시에 높은 생성 품질에 적합한 최소 학습 데이터 사이즈의 확인이 필요하다. 도 10c는 기존 4MB 분량 학습 데이터에서 문장 구조, 형식 및 대체적으로 양호한 문장 생성 결과를 확인(gpt-2- simple; https://github.com/minimaxir/gpt-2-simple 기반)(빠른 학습과 동시에 높은 생성 품질에 적합한 최소 학습 데이터 사이즈 확인 필요) 결과 1 (학습 데이터 400KB 이하, 600 epochs), 결과 2 (학습 데이터 600KB 이하, 1800 epochs) 상기 두 학습 데이터는 해리포터 소설 각각의 시리즈를 사용한다. 상대적으로 적은 data(평균 약 500KB)로 fine tuning 했을 때, 소설의 지문, 대사 및 형식이 적절히 반영되어 있음을 확인하였다. 학습 손실(training loss)이 0.1 이하에서 단어의 배열과 형식이 양호한 수준으로 나타났으나, 각 문장 세부로 보면 문법 오류가 간혹 나타난다. 단락별 문장 조합으로 내용이 전개되는 과정에 오류가 있으며, 매끄러운 스토리 전개에 한계가 보인다. 장문 중심, 복잡한 문장 구조 및 스토리 구조에서 양질의 문장 생성 검수가 까다로운 관계로, 단문 중심, 단순 한 문장 구조, 개별적 문장 전개 수준에서 fine-tuning 가능성의 확인이 필요하다. [한글 모델] 도 10d는 한글모델 - 한글 버전 openAI GPT2 (koGPT2) 모델을 사용하여 fine-tuning을 수행한 화면이다. 한글 버전 openAI GPT2 (koGPT2) 모델은 한글 텍스트를 우수한 성능으로 생성 가능한 모델 (SK-T brain에서 제 공). 그러나, 상기 모델의 경우 대용량 서버를 요구하고, 사전 학습(fine-tuning)에 대한 부분은 amazon AWS 사 용을 강제하는 바, 대안을 확인할 필요가 생겼다. 이에 작사용 koGPT2 대안 모델을 채택하였다. koGPT2 는 fine-tuning에 대한 가능성을 제시하고, 직접 tutorial을 통해 모델 생성 및 가사 script 생성에 대한 지원이 가능하다. 모델 replication 및 텍스트 생성 가능성을 확인하였으며, 도 10d의 아래와 같은 결과를 확인하였다. 향후 명상 용 데이터 적용 시, 텍스트 생성 가능성을 확인할 수 있었다. 도 10e는 다양한 명상 대본 스타일 맞춤형 생성을 위한 학습 데이터 형식 정의 - 명상 훈련 대본 종류는 명상 훈련 대본 묶음(SA, SCM, HST, ACT) 등을 고려; 학습 데이터 중 선호도가 높은 문장 또는 품질이 높은 문장에 가중치를 주고, 학습 시 더 많이 반영될 수 있도록 학습 데이터 형식을 구조화 <표: 학습 데이터 형식 및 예제> * 초기 학습 데이터 - 명상용 책 및 전문가(연세대학교 교수) 집필 명상용 문장을 데이터베이스화 하여 Training set로 준비한다. * KoGPT2 fine-tuning 파이프라인 구축 상기 학습 데이터 기반 전처리 및 feeding 파이프라인 구축 후 학습 진행 확인, ACT 및 SA가 모두 포함된 training set 750문장으로 KoGPT-2 fine-tuning 200 epochs 수행 중 생성 진행. ‘우리’ 단어 입력 후, 750문장 중 ‘우리’로 시작하는 유사 문장 생성 결과 확인하였다. ACT-SA 포함 명상용 스크립트 모음 - Training set로 사용:‘우리’ 단어를 시작으로 문장 생성 실험. 다양한 문장들이 생성될 뿐만 아니라 (검정색 문장) 기존 문장들을 학습하고 재생성하는 능력을 보였다(빨간색 문장). 실험 결과, training set에 포함된 대부분의 문장을 생성해냈다. 명상 스크립트의 문장 구조를 배우고 생성 가 능한 모델 생성 가능성을 확인하였다. 200 epochs 수준, 적은 fine-tuning 반복으로도 training set와 유사한 문장 생성, 동일한 입력 단어를 기준으 로 다양한 종류의 문장을 생성하는 능력 확인 Training set와 유사한 문장 위주로 생성되는 것은 over-fitting 발생을 의미. 다양한 문장 세트 및 긴 길이의 문장을 추가하여 생성 다양성을 확보하는 것이 중요할 것으로 예상된다. * Script generation: 869 명상 스크립트 기반 명상 대본 생성 실험 ACT 및 SA가 모두 포함된 training set 869문장으로 KoGPT-2 fine-tuning 도 10f는 Script generation: 869 명상 스크립트 기반 명상 대본 생성 실험- ACT 및 SA가 모두 포함된 training set 869문장으로 KoGPT-2 fine-tuning : 다양한 단어 입력에 대한 명상 대본 문장 생성을 위한 모델 훈련 - 모델 생성 후 다양한 단어 입력에 대한 명상 대본 문장 생성 실험 실험 결과, 일반적 문장 구조를 배운 모델에 명상 스크립트의 문장 스타일링이 추가되어, 명상 훈련용 대본 생 성 가능성을 확인하였다. 다양한 단어를 시작 단어로 설정했을 때, 자연스러운 문장들이 생성됨. 27개의 서로 다른 단어를 시작 단어로 입력했을 때, 27개의 서로 다른 문장 (형식, 내용 측면)이 생성된다. ‘앞으로의’, ‘앞으로의 나의’ 처럼 어 미/조사 변화를 주었을 경우에도 해당 변화를 반영하여 문맥에 맞는 문장들이 생성되었다. Fine-tuning 데이터 세트들은 주로 인칭 대명사로 시작하는 바, 인칭 대명사 (우리, 나)를 시작 단어로 입력하 면 대부분 명상 관련 문장이 생성되었다. 부정적 감정을 내포하는 단어들에 대해서도 명상 관련 문장들이 생성되었다 (어지러운, 나는 부정적인 사람이다. 하지만…, 실패를 한다해도…). 도 10g는 부정적 감정을 내포하는 단어들에 대해서도 명상 관련 문장들이 생성되는 명상 훈련용 대본 생성 결과: Script generation: 명상 대본 생성 모델 실험 완료 - KoGPT2 기반 명상 데이터 869문장 fine-tuning 모 델 생성 완료(training set에 generate된 문장과 비슷한 구절이 있는 경우 오른쪽 하단에 함께 표기했으며, 이 를 별도로 표기하지 않은 문장들은 training set에 없는 새롭게운 문장임: 2차 테스트 확장 - 문장 시작 단어를 다양한 감정과 상황을 고려하여 입력으로 전달했을 때, 문장 내용 및 형식을 확인) * Script generation: 명상 대본 생성 모델 실험 완료(KoGPT2 기반 명상 데이터 869문장 fine-tuning 모델 생 성 완료) 2차 테스트: 챗봇 형식을 차용, 명상 앱처럼 ‘오늘 하루는 어땠나요’로 문장 생성 품질 확인(문장 구조 및 내 용의 다양성 확인) ‘오늘 하루는 어땠나요? 오늘 나는…’을 입력으로 전달. 100 epoch fine-tuning, training set에 generate된 문장과 비슷한 구절이 있는 경우 오른쪽 하단에 함께 표기했으며, 이를 별도로 표기하지 않은 문장들은 training set에 없는 새롭게운 문장이다. 2차 테스트 확장: 문장 시작 단어를 다양한 감정과 상황을 고려하여 입력으로 전달했을 때, 문장 내용 및 형식 을 확인하였다. 도 11은 Script generation: 명상 대본 생성 모델 성능 평가: programmable 방식 BLEU 스코어를 이용한 생성된 문장 성능 평가 진행, State-of-the-art Natural Language Generation에서 공식적으로 사용되는 generative model 측정 방법(생성된 문장과 실제 문장 사이의 연관성을 비율로 나타내는 방법으로, 생성된 문장의 단어들이 실제 문장의 단어들과 얼마만큼의 비율로 유사한 지 나타냄) 문법에 어긋나지 않으면서, 입력 단어마다 다른 형식의 문장이 생성되는 것을 확인하였다. 자연스러운 대화에서 나올법한 단어들을 시작 단어로 입력했을 때, 형식과 내용이 다른 문장들이 생성되는 것을 확인하였다. 인칭 대명사를 제외한 단어들에서도 행복, 감사 등과 같은 명상 컨텐츠 관련 단어들의 빈도가 높게 나타나고 있 어, fine-tuning에 의한 효과로 보고 있다. 100 epochs로 적은 fine-tuning을 했음에도 불구하고 긍정적인 단어들로 구성된 문장 생성이 성공적으로 되는 바, 생성 모델 fine-tuning 성능이 보장되는 것으로 여기고, 모델 생성 task를 완료하였다. * Script generation: 명상 대본 생성 모델 성능 평가 프로그래머블(programmable) 방식 BLEU 스코어를 이용한 생성된 문장 성능 평가 진행 중State-of-the-art Natural Language Generation에서 공식적으로 사용되는 generative model 측정 방법이다. 생성된 문장과 실제 문장 사이의 연관성을 비율로 나타내는 방법으로, 생성된 문장의 단어들이 실제 문장의 단 어들과 얼마만큼의 비율로 유사한 지 나타내는 수치 단위 gram 기준에 따라, 겹치는 단어 비율조사 기반: 1 단어 조합부터 n-단어 조합에 이르기까지 겹치는 단어 비율을 구하고 이들 간 log likelihood of joint probability로 측정한다. 도 12는 사용자 설문 방식의 Mean Opinion Score(MOS) 산출식을 이용한 생성된 문장 성능 평가(튜링 테스트와 유사한 방법을 통해 생성된 자기긍정 스크립트의 품질을 평가하는 방법) * Bilingual Evaluation Understudy (BLEU) 산출식 및 측정 방법 고안 생성된 문장과 기준 문장과의 N-gram (n=1, 2, 3, 4)을 통한 순서쌍들이 얼마나 겹치는지 측정(precision) 같은 단어가 연속적으로 나올 때 과적합되는 것을 보정한다(Clipping). 문장 길이에 대한 과적합 보정(Brevity penalty). 상기 세 정보를 고려하여 BLEU 값을 측정한다."}
{"patent_id": "10-2022-0015293", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "- 의견 테스트/조사 테스트 방법을 이용하여 5점 척도 내에서 생성된 스크립트 품질을 평가. - 기준 스크립트와 의미상 유사도를 6점 척도로 조사(1점 매우 다름, 6점 매우 유사) - 기준 스크립트와 형태상 차이를 6첨 척도로 조사(1점 매우 유사, 6점 매우 다름) -> BLEU 점수와 BLEU 기반 유사도 분석, MOS 기반 유사도 분석 화면. - 사용자 설문 방식의 Mean Opinion Score(MOS)를 이용한 생성된 문장 성능 평가 진행 - 튜링 테스트와 유사한 방법을 통해 생성된 자기긍정 스크립트의 품질을 평가하는 방법 - 의견 테스트/조사 테스트 방법을 이용하여 5점 척도 내에서 생성된 스크립트 품질을 평가함. - 기준 스크립트와 의미상 유사도를 6점 척도로 조사(1점 매우 다름, 6점 매우 유사) - 기준 스크립트와 형태상 차이를 6첨 척도로 조사(1점 매우 유사, 6점 매우 다름) ** Natural Language Generation 분야 세계 최고 학회 EMNLP에서 발표한 NLG 평가 방식에 대한 논문의 제안을 따른다(Novikova et al., WhyWe Need New Evaluation Metrics for NLG, EMNLP 2017) * MOS 산출식 및 측정 방법 고안 MOS를 직역 하자면 '평균 의견 점수. 모집된 여러 명의 피실험자들이 부여한 점수를 평균 낸 것이 바로 MOS이다. 예를 들면, 10명의 피실험자들이 어떤 생성된 문장을 보고 각각 95, 89, 82, 93, 90, 81, 88, 95, 92, 91과 같 이 점수를 부여하였다면, 생성된 문장의 MOS는 이 점수들의 평균인 89.6이다. MOS의 정의에 해당하는 formulation"}
{"patent_id": "10-2022-0015293", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "- 생성된 문장 세트들 에 대한 MOS 측정 사례 연구 피험자를 대상으로 기준 문장, 생성 문장을 무작위 순서로 배치하고 설문 진행, 여기서 무작위는 상단에 기준 문장, 하단에 생성 문장 위치를 고정하는 것이 아니라, 매 trial마다 기준/생성 문장 위치가 무작위로 선정됨. 사용자로 하여금 선택 bias 최소화하는 방법을 사용한다. 피험자는 제시된 문장을 보고, 5점/6점 척도로 유사도 순위를 제공하며, 상기 수식을 바탕으로 각 질문에 대한 MOS 계산을 한다. 도 13은 2) 감정 상태 및 상황에 알맞은 자기긍정 내면소통 콘텐츠를 제공하기 위한 자기긍정 스크립트 생성 (affirmative script generation) 기술 평가 결과 * Script generation: 명상 대본 생성 예제 시작 단어를 2개(‘나는 어려운’)과 3개 (‘나는 어려운 상황일수록’)로 설정하고, 문장 생성 자유도는 높은 단계로 설정하여 생성한 결과 사례 - Script generation: 명상 대본 생성 예제 * Script generation: 명상 대본 생성 BLEU, BERT 스코어 - 명상 대본 생성을 위해 시작 단어 2개/3개 설정 및 문장 생성 자유도 Low/Mid/High 설정하고, 각 case를 조합 하여 총 6가지 설정에서 두 스코어를 확인 - 설정에서 두 스코어(BLEU, BERT score)를 확인; BLEU는 평균 81.7%, BERT는 평균 92.9% 이상으로 KPI를 만족) * 스크립트 생성(Script generation): 명상 대본 생성 MOS 결과 Readability, Grammatically, Naturalness, Overall Quality, Writing Style이 6점 척도 기준 평균 3.93 점으 로 KPI를 만족한다. 본 제품의 챗봇 에이전트 U-Me와의 대화로 수집한 데이터를 AI 자연어처리 기술을 통해 분석된 챗봇 서비스 및 기술은 챗봇 서버가 개인의 정신 건강 케어 뿐만 아니라 자신의 내면소통 콘텐츠를 제공하며, 증가하고 있는 우 울/불안감 관련 정신건강 케어 시장에서 적은 비용으로 심리 테라피 치료 효과를 내는 프로그램이며, 우울감에 대한 부정적인 사회적 시선을 벗어나 다양한 우울/불안감의 증상을 보이는 사용자들이 스스로 정신케어를 할 수 있도록 도와준다. AI 챗봇 서비스 뿐만 아니라 챗봇 서버가 명상용 자기 긍정 콘텐츠를 제공하여 부정적인 인지적 사고를 긍정적 으로 바꿔줌으로써 행복하고 활발한 생활을 할 수 있도록 독려한다. 따라서, 일반적으로 부정적 증상만을 완화 시키는 심리 테라피의 효과 뿐만 아니라 자주적으로 어려운 심리적 상황을 극복할 수 있도록 하는 셀프케어 서 비스가 접목되어 자기개발 시장 또한 관심을 가지며 성장할 것으로 예상된다. 본 챗봇 서비스는 2030의 젊은 현대인을 주요 대상으로 하고 있으며, 서비스 론칭 후 급격하게 늘어나고 있는 1 인가구, 독거노인 등 다양한 연령층에 적합한 서비스 형태로 확장 예정이다. 챗봇 서비스는 사용자 단말(스마트 기기)로 챗봇 서버가 명상용 자기긍정 내면소통 콘텐츠를 제공하여 개인의 정신 건강 케어 뿐만 아니라 자신의 자기긍정 내면소통 콘텐츠를 서비스 사용자들에게 심리적 어려움을 겪는 사 람들이 활발한 사회적 관계망을 형성할 수 있도록 확장 가능하다. 본 발명에 따른 실시예들은 다양한 컴퓨터 수단을 통해 수행될 수 있는 프로그램 명령 형태로 구현되고 컴퓨터 판독 가능 기록 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 기록 매체는 프로그램 명령, 데이터 파일, 데이 터 구조를 단독으로 또는 조합하여 포함할 수 있다. 컴퓨터 판독 가능 기록 매체는 스토리지, 하드 디스크, 플 로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리, 스토리지 등과 같은 저장 매체에 프로그램 명령을 저장하고 수행하도록 구성된 하드웨어 장치가 포함될 수 있다. 프로그램 명령의 예는 컴파일러에 의해 만들어지는 것과, 기계어 코드 뿐만아니라 인 터프리터를 사용하여 컴퓨터에 의해 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 상기 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로써 작동하도록 구성될 수 있다. 이상에서 설명한 바와 같이, 본 발명의 방법은 프로그램으로 구현되어 컴퓨터의 소프트웨어를 이용하여 읽을 수 있는 형태로 기록매체(CD-ROM, RAM, ROM, 메모리 카드, 하드 디스크, 광자기 디스크, 스토리지 디바이스 등)에 저장될 수 있다. 본 발명의 구체적인 실시예를 참조하여 설명하였지만, 본 발명은 상기와 같이 기술적 사상을 예시하기 위해 구 체적인 실시 예와 동일한 구성 및 작용에만 한정되지 않고, 본 발명의 기술적 사상과 범위를 벗어나지 않는 한 도 내에서 다양하게 변형하여 실시될 수 있으며, 본 발명의 범위는 후술하는 특허청구범위에 의해 결정되어야 한다."}
{"patent_id": "10-2022-0015293", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 종래의 챗봇 서버를 사용한 심리 치료 시스템의 구성도이다. 도 2는 본 발명에 따른 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 시스템 구성도이다. 도 3은 심리 테라피용 자기긍정 내면소통 챗봇 어플리케이션 U-Me 챗봇 서비스의 예상 시나리오를 나타낸 도면 이다. 도 4는 스마트폰에서 챗봇 어플리케이션에서 (A) 자기긍정 내면소통 스크립트 생성 개념도, (B) 스크립트 생성 예시를 나타낸다. 도 5는 Pre-trained 모델, Transfer learning 기반 마음챙김 훈련용 스크립트 자동생성 기술을 보인 도면이다. 도 6은 많은 데이터로 일반적인 모델을 생성하여 제공하고, 각각의 목적에 맞게 사전 학습(fine-tunning)하도록 고안된 GPT-2 모델(Generative Pre-Trained model)이다. 도 7은 자연어처리 GPT-2 알고리즘의 Causal transformer 구조 개요를 나타낸다. 도 8은 영문 생성용 GPT-2 심플 버전 simple-GPT (본 과제 베이스 모델로 사용) 나타낸다. 도 9는 한글 명상 스크립트 생성을 위한 베이스 모델 KoGPT-2 이다. 도 10a는 감정 상태 및 상황에 알맞은 자기긍정 내면소통 콘텐츠 제공을 위한 자기긍정 스크립트 생성(Affirmative script generation) 기술개발 결과 : 영문 모델 - openAI GPT2의 경우 simple-GPT를 통해 비교적 적은 데이터로 fine-tuning을 통해 괜찮은 수준의 결과물을 생성 -. 도 10b는 성경을 학습데이터(Bible data)로 사용하여 생성한 사례(성경의 장, 절 형식을 동일하게 생성(Bible data에서 확인 가능한 장:절의 포맷이 그대로 나타나고, 반복적으로 등장하는 God, LORD 등의 단어가 대/소문자 구분까지 고려해 나타나는 것을 확인); 상대적으로 소량의 데이터 (<4MB)로도 성경의 내용과 형식을 재구성) 도 10c는 기존 4MB 분량 학습 데이터에서 문장 구조, 형식 및 대체적으로 양호한 문장 생성 결과를 확인(gpt-2- simple; https://github.com/minimaxir/gpt-2-simple 기반)(빠른 학습과 동시에 높은 생성 품질에 적합한 최소 학습 데이터 사이즈 확인 필요) 결과 1 (학습 데이터 400KB 이하, 600 epochs), 그리고 결과 2 (학습 데이터 600KB 이하, 1800 epochs) 도 10d는 한글모델 - 한글 버전 openAI GPT2 (koGPT2) 모델을 사용하여 fine-tuning을 수행한 화면이다. 도 10e는 다양한 명상 대본 스타일 맞춤형 생성을 위한 학습 데이터 형식 정의 - 명상 훈련 대본 종류는 명상 훈련 대본 묶음(SA, SCM, HST, ACT) 등을 고려; 학습 데이터 중 선호도가 높은 문장 또는 품질이 높은 문장에 가중치를 주고, 학습 시 더 많이 반영될 수 있도록 학습 데이터 형식을 구조화, 도 10f는 Script generation: 869 명상 스크립트 기반 명상 대본 생성 실험- ACT 및 SA가 모두 포함된 training set 869문장으로 KoGPT-2 fine-tuning : 다양한 단어 입력에 대한 명상 대본 문장 생성을 위한 모델 훈련, 도 10g는 부정적 감정을 내포하는 단어들에 대해서도 명상 관련 문장들이 생성되는 명상 훈련용 대본 생성 결과: Script generation: 명상 대본 생성 모델 실험 완료 - KoGPT2 기반 명상 데이터 869문장 fine-tuning 모 델 생성 완료(training set에 generate된 문장과 비슷한 구절이 있는 경우 오른쪽 하단에 함께 표기했으며, 이 를 별도로 표기하지 않은 문장들은 training set에 없는 새롭게운 문장임: 2차 테스트 확장 - 문장 시작 단어를 다양한 감정과 상황을 고려하여 입력으로 전달했을 때, 문장 내용 및 형식을 확인) 도 11은 Script generation: 명상 대본 생성 모델 성능 평가: programmable 방식 BLEU 스코어를 이용한 생성된 문장 성능 평가 진행, State-of-the-art Natural Language Generation에서 공식적으로 사용되는 generative model 측정 방법(생성된 문장과 실제 문장 사이의 연관성을 비율로 나타내는 방법으로, 생성된 문장의 단어들이 실제 문장의 단어들과 얼마만큼의 비율로 유사한 지 나타냄) 도 12는 사용자 설문 방식의 Mean Opinion Score(MOS) 산출식을 이용한 생성된 문장 성능 평가(튜링 테스트와 유사한 방법을 통해 생성된 자기긍정 스크립트의 품질을 평가하는 방법) - 의견 테스트/조사 테스트 방법을 이 용하여 5점 척도 내에서 생성된 스크립트 품질을 평가. - 기준 스크립트와 의미상 유사도를 6점 척도로 조사(1 점 매우 다름, 6점 매우 유사) - 기준 스크립트와 형태상 차이를 6첨 척도로 조사(1점 매우 유사, 6점 매우 다 름) -> MOS 기반 유사도 분석, BLEU 기반 유사도 분석 화면. 도 13은 2) 감정 상태 및 상황에 알맞은 자기긍정 내면소통 콘텐츠를 제공하기 위한 자기긍정 스크립트 생성 (Affirmative script generation) 기술 평가 결과(Script generation: 명상 대본 생성 예제, Script generation: 명상 대본 생성 BLEU, BERT 스코어)을 나타낸 그림이다."}
