{"patent_id": "10-2022-0164411", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0094125", "출원번호": "10-2022-0164411", "발명의 명칭": "다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법 및 이를 위한 장치", "출원인": "한국전자통신연구원", "발명자": "안백송"}}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치가, 학습 모델을 계층(LAYER) 수준에서 파이프라인 단계별로 분할하여 CPU 소켓 별 NUMA(NON-UNIFORM MEMORYACCESS) 노드로 할당하는 컴파일 단계; 및학습에 필요한 파라미터를 초기화하고, 병렬화 알고리즘 별 정책을 고려하여 생성된 복수개의 쓰레드들을 상기NUMA 노드에 포함된 복수개의 코어들로 할당하여 수행시키는 런타임 단계를 포함하는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 CPU 소켓 별 NUMA 노드는 복수개의 코어들을 포함하는 CPU와 메모리로 구성되고, 상기 복수개의 코어들은코어 간 인터커넥트를 통해 상기 메모리를 공유하고, 상기 CPU 소켓 별 NUMA 노드는 소켓 간 인터커넥트를 통해NUMA 노드 별 메모리를 공유하는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화방법."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서,상기 파이프라인 단계 수의 기본 값은 상기 NUMA 노드의 개수에 상응하게 설정되고, 상기 NUMA 노드마다 동일한개수의 모델 연산이 분배되는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화방법."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 1에 있어서,상기 파라미터는상기 복수개의 쓰레드들 간의 데이터 공유를 위한 전역 파라미터 및 상기 복수개의 쓰레드들마다 개별적으로 사용되는 지역 파라미터를 포함하는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화방법."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서,상기 지역 파라미터는 상기 학습 모델의 역전파 과정에서 사용되는 오차 값에 대한 미분 값(GRADIENT) 및 상기미분 값의 적용 여부를 결정하는 옵티마이저(OPTIMIZER) 상태를 저장하는 것을 특징으로 하는 다중 소켓 구조의호스트 CPU를 이용한 기계학습 병렬화 방법."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 4에 있어서,상기 런타임 단계는상기 NUMA 노드마다 할당된 쓰레드의 실행을 동기화하는 단계; 및상기 전역 파라미터를 기반으로 NUMA 노드마다 파라미터를 업데이트하는 단계를 포함하는 것을 특징으로 하는공개특허 10-2024-0094125-3-다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서,상기 파라미터를 업데이트하는 단계는상기 복수개의 쓰레드들이 동기적(SYNCHRONOUS)으로 업데이트하는 방식 및 비동기적(ASYNCHRONOUS)으로 업데이트하는 방식 중 어느 하나의 방식으로 파라미터 업데이트를 수행하는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "학습 모델을 계층(LAYER) 수준에서 파이프라인 단계별로 분할하여 CPU 소켓 별 NUMA(NON-UNIFORM MEMORYACCESS) 노드로 할당하고, 학습에 필요한 파라미터를 초기화하고, 병렬화 알고리즘 별 정책을 고려하여 생성된복수개의 쓰레드들을 상기 NUMA 노드에 포함된 복수개의 코어들로 할당하여 수행시키는 프로세서; 및상기 병렬화 알고리즘을 저장하는 메모리를 포함하는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 8에 있어서,상기 CPU 소켓 별 NUMA 노드는 복수개의 코어들을 포함하는 CPU와 메모리로 구성되고, 상기 복수개의 코어들은코어 간 인터커넥트를 통해 상기 메모리를 공유하고, 상기 CPU 소켓 별 NUMA 노드는 소켓 간 인터커넥트를 통해NUMA 노드 별 메모리를 공유하는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화장치."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 8에 있어서,상기 파이프라인 단계 수의 기본 값은 상기 NUMA 노드의 개수에 상응하게 설정되고, 상기 NUMA 노드마다 동일한개수의 모델 연산이 분배되는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화장치."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "청구항 8에 있어서,상기 파라미터는상기 복수개의 쓰레드들 간의 데이터 공유를 위한 전역 파라미터 및 상기 복수개의 쓰레드들마다 개별적으로 사용되는 지역 파라미터를 포함하는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화장치."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "청구항 11에 있어서,상기 지역 파라미터는 상기 학습 모델의 역전파 과정에서 사용되는 오차 값에 대한 미분 값(GRADIENT) 및 상기미분 값의 적용 여부를 결정하는 옵티마이저(OPTIMIZER) 상태를 저장하는 것을 특징으로 하는 다중 소켓 구조의호스트 CPU를 이용한 기계학습 병렬화 장치."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "청구항 11에 있어서,상기 프로세서는공개특허 10-2024-0094125-4-상기 NUMA 노드마다 할당된 쓰레드의 실행을 동기화하고, 상기 전역 파라미터를 기반으로 NUMA 노드마다 파라미터를 업데이트하는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치."}
{"patent_id": "10-2022-0164411", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "청구항 13에 있어서,상기 프로세서는상기 복수개의 쓰레드들이 동기적(SYNCHRONOUS)으로 업데이트하는 방식 및 비동기적(ASYNCHRONOUS)으로 업데이트하는 방식 중 어느 하나의 방식으로 파라미터 업데이트를 수행하는 것을 특징으로 하는 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치."}
{"patent_id": "10-2022-0164411", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법 및 이를 위한 장치가 개시된다. 본 발명의 일실시 예에 따른 기계학습 병렬화 방법은 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치가, 학습 모델을 계층(LAYER) 수준에서 파이프라인 단계별로 분할하여 CPU 소켓 별 NUMA(NON-UNIFORM MEMORY ACCESS) 노드로 할 당하는 컴파일 단계; 및 학습에 필요한 파라미터를 초기화하고, 병렬화 알고리즘 별 정책을 고려하여 생성된 복 수개의 쓰레드를 상기 NUMA 노드에 포함된 코어마다 할당하여 수행시키는 런타임 단계를 포함한다."}
{"patent_id": "10-2022-0164411", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 기술에 관한 것으로, 특히 GPU와 같은 별도 의 연산장치 없이 일반적인 다중 소켓 기반의 서버에서 호스트 CPU 기반의 대규모 기계학습 수행 시 성능 부하 를 최소화하면서 병렬 학습 및 추론이 가능한 기술에 관한 것이다."}
{"patent_id": "10-2022-0164411", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 인공지능, 딥러닝 기술의 폭발적인 확산과 함께 인공지능의 정확도 및 성능에 대한 요구 수준 또한 급격히 높아지고 있다. 이에 따라, 인공지능 모델의 규모 또한 기하급수적으로 증가하고 있으며, 그 추세는 관련 하드 웨어의 발전 속도를 넘어서고 있다. 인공지능 학습 및 추론에 가장 폭넓게 사용되는 GPU 장치를 예로 들면, 현 재 널리 사용중인 대규모 모델이 필요로 하는 연산량 및 메모리 요구량 모두 단일 GPU에서 감당할 수 없는 수준 으로 엄청나게 증가하였다. 결과적으로, 대규모의 인공지능 모델을 활용한 학습 및 서비스를 위해서는 다수의 GPU가 장착된 시스템으로 모델을 분산시켜 병렬 학습 및 추론하는 방법을 사용할 수밖에 없는 상황이 되었다. 이 때, 하나의 모델을 다수의 GPU로 분할하여 실행할 경우, 공유되지 않는 여러 개의 GPU 메모리에 모델을 분산 하여 저장해야 한다. 이는 막대한 통신 부하를 유발하는 집합 통신(Collective Communication)을 필요로 하며, 병렬 학습의 성능 저하를 유발하는 가장 큰 원인 중 하나이다. GPU가 인공지능 학습 및 추론에 가장 많이 사용되는 연산 장치이나, 최근 발표되는 호스트 CPU도 딥러닝 가속에 필요한 부가 기능들을 다수 장착하여 출시되고 있다. 이를 통해, X86 아키텍쳐의 대표적인 SIMD(Single Instruction Multiple Data) 연산인 AVX-512 등을 활용하여 딥러닝에 필요한 각종 연산들을 가속함으로써 GPU 혹은 별도의 연산 장치 없이 호스트 CPU 만으로 기계학습을 수행 가능한 기법들이 지속적으로 발표되고 있다. 이러한 호스트 CPU 기반 기계학습은 향후 더욱 활용도가 높아질 것으로 전망된다. 호스트 CPU 기반 기계학습의 가장 큰 장점은 대규모로 공유되는 시스템 메모리를 직접 활용 가능하다는 점이다. GPU 메모리는 크기도 작고 서로 공유되지 않기 때문에 대규모 모델을 분산하여 처리하는 것이 어렵다. 반면, 큰 시스템 메모리를 이용하면 모델을 분할하지 않고 메모리에 올릴 수 있으며, 모든 CPU 코어가 공유 가능하기 때 문에 이 메모리를 통해 코어 간 통신이 가능하다. 뿐만 아니라, 노드 내 CPU 소켓을 연결하는 로컬 버스, 소켓 내 코어를 연결하는 NoC(Network-on-Chip)의 성능은 매우 뛰어나므로 통신 부하를 최소화할 수 있다. 하지만, 다수의 CPU를 장착하는 다중 소켓 기반 서버는 대부분 NUMA(Non-Uniform Memory Access) 아키텍처 기 반으로 설계되어 있다. 이는 각 CPU 소켓마다 연결된 메모리를 공유하는 구조로, 다른 CPU 소켓의 메모리를 참 조할 때의 성능은 현재 자신의 CPU 소켓의 메모리를 참조할 때보다 낮아진다. CPU 기반 분산 학습을 진행할 때, 이러한 NUMA 구조를 인지하여 이에 최적화된 병렬 학습이 가능한 기법이 필요하다. 즉, 인공지능 모델을 구성하는 연산과 변수 등을 다수의 NUMA 노드들에 분할하여 수행하기 위해서는 NUMA 구조 를 인지하여 분할을 수행할 필요가 있다. 뿐만 아니라, 최근 대규모 기계학습을 위해 제안된 각종 병렬화 기법 (파이프라인 병렬화, 텐서 병렬화, 데이터 병렬화 등)들은 대부분 다중 GPU 환경을 가정하여 제안되었기 때문에, 이를 효과적으로 호스트 CPU 기반 딥러닝 환경에 적용하기 위한 새로운 방법이 필요하다. 선행기술문헌특허문헌 (특허문헌 0001) 미국 공개 특허 US2021/0149729, 2021년 5월 20일 공개(명칭: TASK SCHEDULING FOR MACHINE- LEARNING WORKLOADS)"}
{"patent_id": "10-2022-0164411", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 다중 소켓 시스템의 계층별 물리적 특성을 활용하여 호스트 CPU 기반의 병렬 기계학습 수행 시 성능 부하를 최소화하면서 효율적인 대규모 기계학습이 가능하도록 하는 것이다. 또한, 본 발명의 목적은 GPU와 같은 별도의 연산 장치없이 NUMA 노드 시스템의 물리적 특성을 활용하여 호스트 CPU와 시스템 메모리를 이용한 분산 기계학습 모델을 효과적으로 병렬화하는 것이다. 또한, 본 발명의 목적은 시스템을 구성하는 다 계층 인터커넥트의 성능 격차를 이용하여 부하 수준을 최소화하 는 병렬화 기법 적용함으로써 대규모 모델의 병렬 학습 및 추론 시 성능을 향상시키는 것이다."}
{"patent_id": "10-2022-0164411", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법은 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치가, 학습 모델을 계층(LAYER) 수준에서 파이프라인 단계별로 분할하여 CPU 소켓 별 NUMA(NON-UNIFORM MEMORY ACCESS) 노드로 할당하는 컴파일 단계; 및 학습에 필 요한 파라미터를 초기화하고, 병렬화 알고리즘 별 정책을 고려하여 생성된 복수개의 쓰레드들을 상기 NUMA 노드 에 포함된 복수개의 코어들로 할당하여 수행시키는 런타임 단계를 포함한다. 이 때, CPU 소켓 별 NUMA 노드는 복수개의 코어들을 포함하는 CPU와 메모리로 구성되고, 상기 복수개의 코어들 은 코어 간 인터커넥트를 통해 상기 메모리를 공유하고, 상기 CPU 소켓 별 NUMA 노드는 소켓 간 인터커넥트를 통해 NUMA 노드 별 메모리를 공유할 수 있다. 이 때, 파이프라인 단계 수의 기본 값은 상기 NUMA 노드의 개수에 상응하게 설정되고, 상기 NUMA 노드마다 동일 한 개수의 모델 연산이 분배될 수 있다. 이 때, 파라미터는 상기 복수개의 쓰레드들 간의 데이터 공유를 위한 전역 파라미터 및 상기 복수개의 쓰레드들 마다 개별적으로 사용되는 지역 파라미터를 포함할 수 있다. 이 때, 지역 파라미터는 상기 학습 모델의 역전파 과정에서 사용되는 오차 값에 대한 미분 값(GRADIENT) 및 상 기 미분 값의 적용 여부를 결정하는 옵티마이저(OPTIMIZER) 상태를 저장할 수 있다. 이 때, 런타임 단계는 상기 NUMA 노드마다 할당된 쓰레드의 실행을 동기화하는 단계; 및 상기 전역 파라미터를 기반으로 NUMA 노드마다 파라미터를 업데이트하는 단계를 포함할 수 있다. 이 때, 파라미터를 업데이트하는 단계는 상기 복수개의 쓰레드들이 동기적(SYNCHRONOUS)으로 업데이트하는 방식 및 비동기적(ASYNCHRONOUS)으로 업데이트하는 방식 중 어느 하나의 방식으로 파라미터 업데이트를 수행할 수 있 다. 또한, 본 발명의 일실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치는 학습 모델을 계층(LAYER) 수준에서 파이프라인 단계별로 분할하여 CPU 소켓 별 NUMA(NON-UNIFORM MEMORY ACCESS) 노드로 할 당하고, 학습에 필요한 파라미터를 초기화하고, 병렬화 알고리즘 별 정책을 고려하여 생성된 복수개의 쓰레드들 을 상기 NUMA 노드에 포함된 복수개의 코어들로 할당하여 수행시키는 프로세서; 및 상기 병렬화 알고리즘을 저 장하는 메모리를 포함한다. 이 때, CPU 소켓 별 NUMA 노드는 복수개의 코어들을 포함하는 CPU와 메모리로 구성되고, 상기 복수개의 코어들 은 코어 간 인터커넥트를 통해 상기 메모리를 공유하고, 상기 CPU 소켓 별 NUMA 노드는 소켓 간 인터커넥트를 통해 NUMA 노드 별 메모리를 공유할 수 있다. 이 때, 파이프라인 단계 수의 기본 값은 상기 NUMA 노드의 개수에 상응하게 설정되고, 상기 NUMA 노드마다 동일 한 개수의 모델 연산이 분배될 수 있다. 이 때, 파라미터는 상기 복수개의 쓰레드들 간의 데이터 공유를 위한 전역 파라미터 및 상기 복수개의 쓰레드들 마다 개별적으로 사용되는 지역 파라미터를 포함할 수 있다. 이 때, 지역 파라미터는 상기 학습 모델의 역전파 과정에서 사용되는 오차 값에 대한 미분 값(GRADIENT) 및 상 기 미분 값의 적용 여부를 결정하는 옵티마이저(OPTIMIZER) 상태를 저장할 수 있다. 이 때, 프로세서는 상기 NUMA 노드마다 할당된 쓰레드의 실행을 동기화하고, 상기 전역 파라미터를 기반으로 NUMA 노드마다 파라미터를 업데이트할 수 있다. 이 때, 프로세서는 상기 복수개의 쓰레드들이 동기적(SYNCHRONOUS)으로 업데이트하는 방식 및 비동기적 (ASYNCHRONOUS)으로 업데이트하는 방식 중 어느 하나의 방식으로 파라미터 업데이트를 수행할 수 있다."}
{"patent_id": "10-2022-0164411", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 다중 소켓 시스템의 계층별 물리적 특성을 활용하여 호스트 CPU 기반의 병렬 기계학습 수행 시 성능 부하를 최소화하면서 효율적인 대규모 기계학습이 가능할 수 있다. 또한, 본 발명은 GPU와 같은 별도의 연산 장치없이 NUMA 노드 시스템의 물리적 특성을 활용하여 호스트 CPU와 시스템 메모리를 이용한 분산 기계학습 모델을 효과적으로 병렬화할 수 있다. 또한, 본 발명은 시스템을 구성하는 다 계층 인터커넥트의 성능 격차를 이용하여 부하 수준을 최소화하는 병렬 화 기법 적용함으로써 대규모 모델의 병렬 학습 및 추론 시 성능을 향상시킬 수 있다."}
{"patent_id": "10-2022-0164411", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명을 첨부된 도면을 참조하여 상세히 설명하면 다음과 같다. 여기서, 반복되는 설명, 본 발명의 요지를 불 필요하게 흐릴 수 있는 공지 기능, 및 구성에 대한 상세한 설명은 생략한다. 본 발명의 실시형태는 당 업계에서 평균적인 지식을 가진 자에게 본 발명을 보다 완전하게 설명하기 위해서 제공되는 것이다. 따라서, 도면에서의 요소들의 형상 및 크기 등은 보다 명확한 설명을 위해 과장될 수 있다. 본 문서에서, \"A 또는 B\", \"A 및 B 중 적어도 하나\", \"A 또는 B 중 적어도 하나\", \"A, B 또는 C\", \"A, B 및 C 중 적어도 하나\", 및 \"A, B, 또는 C 중 적어도 하나\"와 같은 문구들 각각은 그 문구들 중 해당하는 문구에 함께 나열된 항목들 중 어느 하나, 또는 그들의 모든 가능한 조합을 포함할 수 있다. 이하, 본 발명에 따른 바람직한 실시예를 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 본 발명의 일실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법을 나타낸 동작 흐름도이다. 도 1을 참조하면, 본 발명의 일실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법은, 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치가, 학습 모델을 계층(LAYER) 수준에서 파이프라인 단계별로 분할하여 CPU 소켓 별 NUMA(NON-UNIFORM MEMORY ACCESS) 노드로 할당하는 컴파일 단계를 수행한다 (S110). 이 때, CPU 소켓 별 NUMA 노드는 복수개의 코어들을 포함하는 CPU와 메모리로 구성되고, 복수개의 코어들은 코 어 간 인터커넥트를 통해 메모리를 공유하고, CPU 소켓 별 NUMA 노드는 소켓 간 인터커넥트를 통해 NUMA 노드 별 메모리를 공유할 수 있다. 예를 들어, 도 2는 본 발명의 일실시예에 따라 병렬 기계학습을 수행할 다중 소켓 구조의 호스트 CPU를 나타낸 것이다. 도 2의 예시에 따르면, 호스트 CPU에는 4개의 CPU 소켓이 존재하며, 각 소켓에 4개의 코어들을 가진 CPU(210, 220, 230, 240)가 장착될 수 있다. 각각의 4 코어 CPU(210, 220, 230, 240)는 메모리(211, 221, 231, 241)와 연결되어 각각의 NUMA 노드를 구성할 수 있다. 이 때, 동일한 CPU에 속한 코어들은 해당하는 NUMA 노드의 메모리를 공유할 수 있으며, 각각의 코어들이 동일한 성능으로 메모리에 접근할 수 있다. 또한, CPU(210, 220, 230, 240) 내부의 코어들은 코어 간 인터커넥트를 통해 상호 연결되고, 각각의 NUMA 노드들은 소켓 간 인터커넥트를 통해 서로 연결될 수 있는데, 소켓 간 인터커넥트는 코어 간 인터커 넥트에 비해 낮은 성능을 보인다. 따라서, 이 때, 타 NUMA 노드로의 메모리 접근은 동일 NUMA 노드로의 메모리 접근에 비해 낮은 성능을 보인다. 이 때, 파이프라인 단계 수의 기본 값은 상기 NUMA 노드의 개수에 상응하게 설정되고, NUMA 노드마다 동일한 개 수의 모델 연산이 분배될 수 있다. 예를 들어, 도 3은 컴파일 단계에서 파이프라인 병렬화를 수행하는 과정을 상세하게 나타낸 것으로, 먼저 파이 프라인 단계 수와 해당 모델의 연산 개수, 즉 모델 계층 수를 고려하여 각 단계별로 모델 연산을 배분할 수 있 다(S310). 이 때, 소켓 간 인터커넥트(NUMA 노드 간 인터커넥트) 성능이 소켓 내 코어 간 인터커넥트 성능보다 떨어지며, 파이프라인 병렬화 기법이 텐서 병렬화 기법이나 데이터 병렬화 기법에 비해 인터커넥트 성능에 덜 민감한 기법 이므로, 파이프라인 단계 수의 기본값은 시스템의 NUMA 노드 수로 설정될 수 있다. 만약, 모델의 계층 수를 l, 파이프라인 단계 수를 n 이라고 한다면, 파이프라인 i 번째 단계 별 배분되는 계층 수 ki 는 [수학식 1]과 같이 계산할 수 있다. [수학식 1]"}
{"patent_id": "10-2022-0164411", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이러한 방법은 모델을 구성하는 계층별 성능 부하가 동일한 수준이라는 가정 하에 파이프라인 단계 당 동일한 수의 계층을 배분하는 방식에 상응할 수 있다. 만약, 계층 간 성능 편차가 심한 모델의 경우에는 프로파일링을 통해 계층별 성능 부하를 측정한 후 이를 고려하여 배분하는 방법을 적용할 수도 있다. 또한, 본 발명의 일실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법은, 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치가, 학습에 필요한 파라미터를 초기화하고, 병렬화 알고리즘 별 정책을 고려하여 생성된 복수개의 쓰레드들을 NUMA 노드에 포함된 복수개의 코어들로 할당하여 수행시키는 런타임 단계를 수행한다(S120). 이 때, 파라미터는 복수개의 쓰레드들 간의 데이터 공유를 위한 전역 파라미터 및 복수개의 쓰레드들마다 개별 적으로 사용되는 지역 파라미터를 포함할 수 있다. 예를 들어, 컴파일 단계를 통해 파이프라인 단계별 모델 계층 배분이 끝나면, 인접한 파이프라인 단계 간 출력 값 및 입력 값의 공유를 위한 전역 파라미터를 생성할 수 있다. 이렇게 생성 또는 정의된 전역 파라미터는 서로다른 파이프라인 단계를 담당하는 쓰레드 간 데이터 공유 시 사용될 수 있다. 이 때, 지역 파라미터는 학습 모델의 역전파 과정에서 사용되는 오차 값에 대한 미분 값(GRADIENT) 및 미분 값 의 적용 여부를 결정하는 옵티마이저(OPTIMIZER) 상태를 저장할 수 있다. 예를 들어, [표 1]은 사용자가 작성하는 기계학습 모델을 구현한 코드의 일 예를 나타낸 것이다. 표 1 parameter_1[]...[]; parameter_2[]...[]; ... parameter_16[]...[]; model(...) { optimizer_state_1[]...[]; optimizer_state_2[]...[]; ... optimizer_state_16[]...[]; gradient_1[]...[]; gradient_2[]...[]; ... gradient_16[]...[]; operation_1(); operation_2(); ... operation_8();"}
{"patent_id": "10-2022-0164411", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "operation_8_back(); operation_7_back(); ... operation_1_back(); } 상기 [표 1]의 예시에서는 편의상 8개의 연산으로 정의된 기계학습 모델을 가정하여 설명하도록 한다. 이 때, 추론만을 위해 모델을 사용할 경우에는 순방향(Forward) 연산만을 정의하면 되고, 모델 학습을 수행할 경우에는 역전파(Backpropagation) 연산 또한 정의해야 한다. 예를 들어, [표 1]에서 'operation_X' 로 정의된 연산은 순방향 연산에 상응하고, 'operation_X_back'로 정의된 연산은 역방향 연산에 상응할 수 있다. 이렇게 정의한 기계학습 모델은 추후 복수개의 쓰레드들의 형태로 병렬 실행될 수 있다. 이 때, 파라미터는 병렬 실행 시 복수개의 쓰레드들에 의해 공유되므로, 쓰레드 간 데이터 또는 자료 공유를 위 해 전역 파라미터 형태로 정의될 수 있다. 예를 들어, [표 1]에서 'parameter_X[]쪋[]'로 정의된 파라미터는 전 역 파라미터에 상응할 수 있다. 또한, 모델 학습 시 수행되는 역전파 과정에서 추가적으로 필요한 것은 오차(Loss)값에 대한 미분값(Gradient) 과 미분값을 파라미터 업데이트 시 어떻게 적용할 것인지를 결정하는 옵티마이저(Optimizer) 그리고 파라미터의 업데이트 작업에 해당할 수 있다. 따라서, 모델 학습 과정에 필요한 미분값(gradient_X[]쪋[]), 옵티마이저 상태(optimizer_state_X[]쪋[]) 등을 저장할 파라미터가 선언되어야 하는데, 미분값과 옵티마이저 상태는 쓰레드 간 공유되지 않고 개별적으로 사용 되므로 지역 파라미터로 선언할 수 있다. 최근 많은 수의 딥러닝 라이브러리는 자동 미분 기능을 제공하므로, 이를 이용하면 모델의 순방향 과정만을 정 의하는 것만으로도 역전파 과정은 자동으로 생성되어 진행될 수 있다. 이와 같은 자동 미분 기능을 사용할경우, 모델의 순방향 연산만을 정의하여 명시하면 되고, 상술한 미분 값이나 옵티마이저 상태는 자동으로 생성 되므로 별도로 정의하지 않을 수도 있다. 이 때, NUMA 노드마다 할당된 쓰레드의 실행을 동기화할 수 있다. 이 때, 전역 파라미터를 기반으로 NUMA 노드마다 파라미터를 업데이트할 수 있다. 이 때, 복수개의 쓰레드들이 동기적(SYNCHRONOUS)으로 업데이트하는 방식 및 비동기적(ASYNCHRONOUS)으로 업데 이트하는 방식 중 어느 하나의 방식으로 파라미터 업데이트를 수행할 수 있다. 상술한 바와 같이, 본 발명은 크게 컴파일 단계와 런타임 단계로 구분되며, 컴파일 단계에서 목적 시스템의 특 성을 반영한 모델 분할이 이루어지고, 런타임 단계에서는 이를 기반으로 다중 쓰레드를 생성 및 관리하는 방식 으로 구동될 수 있다. 따라서, 파이프라인 병렬화 작업은 컴파일 기능이 담당하며, 그 이후 진행되는 작업들은 런타임 기능이 담당할 수 있다. 예를 들어, 파이프라인 병렬화 작업을 통해 모델을 계층 수준에서 파이프라인 단계별로 분할하는 작업을 수행하 여 컴파일 단계의 작업이 완료되면, 사용자가 작성한 모델 코드는 병렬화 설정에 맞게 다중 쓰레드 형태로 변환 되어 컴파일된 후 최종 실행 파일로 생성될 수 있다. 이렇게 생성된 실행 파일은 런타임 기능을 통해 실행될 수 있는데, 먼저 파라미터 값을 초기화한 후 병렬화 알고리즘 별 정책에 따라 다수의 쓰레드를 생성하여 CPU 코어 에 배치하고, 각 코어에 배치된 쓰레드를 실행하여 모델의 학습을 시작할 수 있다. 이 때, 일반적으로 많이 쓰이는 모델 병렬화 방식으로 파이프라인 병렬화, 데이터 병렬화, 텐서 병렬화의 세 가 지 기법이 있다. 본 발명에 따른 컴파일 단계에서는 파이프라인 병렬화만을 처리하였는데, 이유는 다음과 같다. 데이터 병렬화에서 가장 중요한 작업은 전역 파라미터를 통해 쓰레드의 파라미터를 업데이트하는 것인데, 이는 본 발명의 런타임 단계에서 처리 가능한 작업이므로 별도의 컴파일 과정을 필요로 하지 않는다. 텐서 병렬화를 수행하는 목적은 모델의 단일 계층이 한정된 GPU 메모리에 탑재가 불가능할 경우에 계층 내부를 분할하기 위함인데, 본 발명이 가정하는 호스트 CPU 기반 딥러닝의 경우에는 동일한 파이프라인 단계를 담당하 는 모든 쓰레드는 같은 NUMA 노드에서 수행되며, 이미 해당 NUMA 노드의 메모리를 공유하고 있다. 이러한 상황 에서 모델 계층 내부를 분할하는 텐서 병렬화는 성능 향상을 위한 의미가 없으므로 본 발명에서는 고려하지 않 는다. 이와 같은 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법을 통해 다중 소켓 시스템의 계층별 물 리적 특성을 활용하여 호스트 CPU 기반의 병렬 기계학습 수행 시 성능 부하를 최소화하면서 효율적인 대규모 기 계학습이 가능할 수 있다. 또한, GPU와 같은 별도의 연산 장치없이 NUMA 노드 시스템의 물리적 특성을 활용하여 호스트 CPU와 시스템 메모 리를 이용한 분산 기계학습 모델을 효과적으로 병렬화할 수 있다. 또한, 시스템을 구성하는 다 계층 인터커넥트의 성능 격차를 이용하여 부하 수준을 최소화하는 병렬화 기법 적 용함으로써 대규모 모델의 병렬 학습 및 추론 시 성능을 향상시킬 수 있다. 도 5 내지 도 7은 본 발명에 따른 다중 소켓 구조의 호스트 CPU를 활용하여 다중 쓰레드 기반 모델 병렬 실행의 일 예를 나타낸 도면이다. 먼저, 도 5는 4개의 코어를 가진 CPU가 4개 장착된 서버에서 호스트 CPU를 활용한 다중 쓰레드 기반 모델 병렬 실행을 예를 들어 나타낸 것이다. 이 때, 모델을 구성하는 계층(연산) 수가 총 8개라고 가정하고 파이프라인 병렬화를 수행하면, 도 5에 도시된 CPU 소켓 당 한 개씩 총 4개의 파이프라인 단계를 할당한 뒤 각 단계별로 2개의 계층 씩을 할당할 수 있다. 이 후, 도 6에 도시된 것처럼 동일한 CPU 소켓 내 각 코어에 배치된 쓰레드(610~640)마다 하나씩 별도의 입력 데이터(DATA_0 ~ DATA_3)를 처리하도록 적용함으로써 총 4개의 입력 데이터를 병렬로 처리하는 병렬화를 수행할 수 있다. 이 때, 동일한 입력 데이터를 처리하는 파이프라인 단계별 쓰레드들은, 도 7에 도시된 것처럼 현재 출력값을 다 음 소켓의 해당 쓰레드의 입력값으로 전달함으로써 파이프라인 병렬 처리를 진행할 수 있다. 이에 따라, 4개의 코어를 가진 CPU가 4개 장착된 서버에서는 총 16개의 쓰레드들이 생성되어 학습이 진행될 수 있으며, 이 쓰레드들 간의 동기화 작업은 도 10을 통해 후술하는 런타임 과정에서 수행될 수 있다. 도 8은 본 발명의 일실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치를 나타낸 블록 도이다. 도 8을 참조하면, 본 발명의 일실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치는, 통신부, 프로세서 및 메모리를 포함한다. 통신부는 통신망을 통해 기계학습 병렬화를 위한 정보를 송수신하는 역할을 할 수 있다. 프로세서는 학습 모델을 계층(LAYER) 수준에서 파이프라인 단계별로 분할하여 CPU 소켓 별 NUMA(NON- UNIFORM MEMORY ACCESS) 노드로 할당하는 컴파일 단계를 수행한다. 이 때, CPU 소켓 별 NUMA 노드는 복수개의 코어들을 포함하는 CPU와 메모리로 구성되고, 복수개의 코어들은 코 어 간 인터커넥트를 통해 메모리를 공유하고, CPU 소켓 별 NUMA 노드는 소켓 간 인터커넥트를 통해 NUMA 노드 별 메모리를 공유할 수 있다. 예를 들어, 도 2는 본 발명의 일실시예에 따라 병렬 기계학습을 수행할 다중 소켓 구조의 호스트 CPU를 나타낸 것이다. 도 2의 예시에 따르면, 호스트 CPU에는 4개의 CPU 소켓이 존재하며, 각 소켓에 4개의 코어들을 가진 CPU(210, 220, 230, 240)가 장착될 수 있다. 각각의 4 코어 CPU(210, 220, 230, 240)는 메모리(211, 221, 231, 241)와 연결되어 각각의 NUMA 노드를 구성할 수 있다. 이 때, 동일한 CPU에 속한 코어들은 해당하는 NUMA 노드의 메모리를 공유할 수 있으며, 각각의 코어들이 동일한 성능으로 메모리에 접근할 수 있다. 또한, CPU(210, 220, 230, 240) 내부의 코어들은 코어 간 인터커넥트를 통해 상호 연결되고, 각각의 NUMA 노드들은 소켓 간 인터커넥트를 통해 서로 연결될 수 있는데, 소켓 간 인터커넥트는 코어 간 인터커 넥트에 비해 낮은 성능을 보인다. 따라서, 이 때, 타 NUMA 노드로의 메모리 접근은 동일 NUMA 노드로의 메모리 접근에 비해 낮은 성능을 보인다. 이 때, 파이프라인 단계 수의 기본 값은 상기 NUMA 노드의 개수에 상응하게 설정되고, NUMA 노드마다 동일한 개 수의 모델 연산이 분배될 수 있다. 예를 들어, 도 3은 컴파일 단계에서 파이프라인 병렬화를 수행하는 과정을 상세하게 나타낸 것으로, 먼저 파이 프라인 단계 수와 해당 모델의 연산 개수, 즉 모델 계층 수를 고려하여 각 단계별로 모델 연산을 배분할 수 있 다(S310). 이 때, 소켓 간 인터커넥트(NUMA 노드 간 인터커넥트) 성능이 소켓 내 코어 간 인터커넥트 성능보다 떨어지며, 파이프라인 병렬화 기법이 텐서 병렬화 기법이나 데이터 병렬화 기법에 비해 인터커넥트 성능에 덜 민감한 기법 이므로, 파이프라인 단계 수의 기본값은 시스템의 NUMA 노드 수로 설정될 수 있다. 만약, 모델의 계층 수를 l, 파이프라인 단계 수를 n 이라고 한다면, 파이프라인 i 번째 단계 별 배분되는 계층 수 ki 는 [수학식 1]과 같이 계산할 수 있다. 이러한 방법은 모델을 구성하는 계층별 성능 부하가 동일한 수준이라는 가정 하에 파이프라인 단계 당 동일한 수의 계층을 배분하는 방식에 상응할 수 있다. 만약, 계층 간 성능 편차가 심한 모델의 경우에는 프로파일링을 통해 계층별 성능 부하를 측정한 후 이를 고려하여 배분하는 방법을 적용할 수도 있다. 또한, 프로세서는 학습에 필요한 파라미터를 초기화하고, 병렬화 알고리즘 별 정책을 고려하여 생성된 복 수개의 쓰레드들을 NUMA 노드에 포함된 복수개의 코어들로 할당하여 수행시키는 런타임 단계를 수행한다. 이 때, 파라미터는 복수개의 쓰레드들 간의 데이터 공유를 위한 전역 파라미터 및 복수개의 쓰레드들마다 개별 적으로 사용되는 지역 파라미터를 포함할 수 있다. 예를 들어, 컴파일 단계를 통해 파이프라인 단계별 모델 계층 배분이 끝나면, 인접한 파이프라인 단계 간 출력 값 및 입력 값의 공유를 위한 전역 파라미터를 생성할 수 있다. 이렇게 생성 또는 정의된 전역 파라미터는 서로 다른 파이프라인 단계를 담당하는 쓰레드 간 데이터 공유시 사용될 수 있다. 이 때, 지역 파라미터는 학습 모델의 역전파 과정에서 사용되는 오차 값에 대한 미분 값(GRADIENT) 및 미분 값 의 적용 여부를 결정하는 옵티마이저(OPTIMIZER) 상태를 저장할 수 있다. 예를 들어, [표 1]은 사용자가 작성하는 기계학습 모델을 구현한 코드의 일 예를 나타낸 것이다. 상기 [표 1]의 예시에서는 편의상 8개의 연산으로 정의된 기계학습 모델을 가정하여 설명하도록 한다. 이 때, 추론만을 위해 모델을 사용할 경우에는 순방향(Forward) 연산만을 정의하면 되고, 모델 학습을 수행할 경우에는 역전파(Backpropagation) 연산 또한 정의해야 한다. 예를 들어, [표 1]에서 'operation_X' 로 정의된 연산은 순방향 연산에 상응하고, 'operation_X_back'로 정의된 연산은 역방향 연산에 상응할 수 있다. 이렇게 정의한 기계학습 모델은 추후 복수개의 쓰레드들의 형태로 병렬 실행될 수 있다. 이 때, 파라미터는 병렬 실행 시 복수개의 쓰레드들에 의해 공유되므로, 쓰레드 간 데이터 또는 자료 공유를 위 해 전역 파라미터 형태로 정의될 수 있다. 예를 들어, [표 1]에서 'parameter_X[]쪋[]'로 정의된 파라미터는 전 역 파라미터에 상응할 수 있다. 또한, 모델 학습 시 수행되는 역전파 과정에서 추가적으로 필요한 것은 오차(Loss)값에 대한 미분값(Gradient) 과 미분값을 파라미터 업데이트 시 어떻게 적용할 것인지를 결정하는 옵티마이저(Optimizer) 그리고 파라미터의 업데이트 작업에 해당할 수 있다. 따라서, 모델 학습 과정에 필요한 미분값(gradient_X[]쪋[]), 옵티마이저 상태(optimizer_state_X[]쪋[]) 등을 저장할 파라미터가 선언되어야 하는데, 미분값과 옵티마이저 상태는 쓰레드 간 공유되지 않고 개별적으로 사용 되므로 지역 파라미터로 선언할 수 있다. 최근 많은 수의 딥러닝 라이브러리는 자동 미분 기능을 제공하므로, 이를 이용하면 모델의 순방향 과정만을 정 의하는것 만으로도 역전파 과정은 자동으로 생성되어 진행될 수 있다. 이와 같은 자동 미분 기능을 사용할 경우, 모델의 순방향 연산만을 정의하여 명시하면 되고, 상술한 미분 값이나 옵티마이저 상태는 자동으로 생성 되므로 별도로 정의하지 않을 수도 있다. 이 때, NUMA 노드마다 할당된 쓰레드의 실행을 동기화할 수 있다. 이 때, 전역 파라미터를 기반으로 NUMA 노드마다 파라미터를 업데이트할 수 있다. 이 때, 복수개의 쓰레드들이 동기적(SYNCHRONOUS)으로 업데이트하는 방식 및 비동기적(ASYNCHRONOUS)으로 업데 이트하는 방식 중 어느 하나의 방식으로 파라미터 업데이트를 수행할 수 있다. 메모리는 병렬화 알고리즘을 저장한다. 또한, 메모리는 상술한 바와 같이 본 발명의 일실시예에 따른 기계학습 병렬화 장치에서 발생하는 다양한 정보를 저장한다. 실시예에 따라, 저장부는 기계학습 병렬화 장치와 독립적으로 구성되어 기계학습 병렬화를 위한 기능을 지 원할 수 있다. 이 때, 저장부는 별도의 대용량 스토리지로 동작할 수 있고, 동작 수행을 위한 제어 기능을 포함할 수도 있다. 이와 같은 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치를 통해 다중 소켓 시스템의 계층별 물 리적 특성을 활용하여 호스트 CPU 기반의 병렬 기계학습 수행 시 성능 부하를 최소화하면서 효율적인 대규모 기 계학습이 가능할 수 있다. 또한, GPU와 같은 별도의 연산 장치없이 NUMA 노드 시스템의 물리적 특성을 활용하여 호스트 CPU와 시스템 메모 리를 이용한 분산 기계학습 모델을 효과적으로 병렬화할 수 있다. 또한, 시스템을 구성하는 다계층 인터커넥트의 성능 격차를 이용하여 부하 수준을 최소화하는 병렬화 기법 적용 함으로써 대규모 모델의 병렬 학습 및 추론 시 성능을 향상시킬 수 있다. 도 9는 본 발명의 다른실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치를 나타낸 블 록도이다. 도 9를 참조하면, 본 발명의 다른 실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치 는 컴파일 모듈 및 런타임 모듈로 구성될 수 있다. 이 때, 컴파일 모듈은 목적 시스템의 특성을 반영한 모델 분할을 수행할 수 있고, 런타임 모듈은 컴 파일 모듈에서 수행한 내용을 기반으로 다중 쓰레드를 생성 및 관리하는 방식으로 구동될 수 있다. 따라서, 파이프라인 병렬화 작업은 컴파일 모듈이 담당하며, 그 이후 진행되는 작업들은 런타임 모듈(92 0)이 담당할 수 있다. 도 10은 도 9에 도시된 런타임 모듈을 상세하게 나타낸 블록도이다. 도 10을 참조하면, 본 발명의 일실시예에 따른 런타임 모듈은 크게 쓰레드 초기화 및 생성/종료 관리부 , 파이프라인 병렬 실행 관리부 및 데이터 병렬 실행 관리부로 구성될 수 있다. 쓰레드 초기화 및 생성/종료 관리부는 딥러닝 수행을 위한 파라미터를 초기화하고, 쓰레드의 생성 및 종 료를 관리할 수 있다. 파이프라인 병렬 실행 관리부는 파이프라인 병렬화 적용 시 단계별 출력값 및 입력값의 전달과, 스케줄링 정책에 따른 쓰레드 실행을 동기화하는 기능을 담당할 수 있다. 데이터 병렬 실행 관리부 모델의 데이터 병렬 수행을 위한 파라미터 업데이트 관련 쓰레드 동기화 작업을 담당할 수 있다. 이 때, 파라미터 업데이트는 쓰레드 간 공유되는 전역 파라미터를 통해 수행될 수 있으며, 모든 쓰레드가 동기 적(Synchronous)으로 업데이트하는 방식과 성능 향상을 위해 비동기적(Asynchronous)으로 업데이트하는 방식을 선택적으로 활용할 수 있다. 도 11은 본 발명의 일실시예에 따른 컴퓨터 시스템을 나타낸 도면이다. 도 11을 참조하면, 본 발명의 실시예는 컴퓨터로 읽을 수 있는 기록매체와 같은 컴퓨터 시스템에서 구현될 수 있다. 도 11에 도시된 바와 같이, 컴퓨터 시스템은 버스를 통하여 서로 통신하는 하나 이상의 프 로세서, 메모리, 사용자 입력 장치, 사용자 출력 장치 및 스토리지를 포함할 수 있다. 또한, 컴퓨터 시스템은 네트워크에 연결되는 네트워크 인터페이스를 더 포함할 수 있 다. 프로세서는 중앙 처리 장치 또는 메모리나 스토리지에 저장된 프로세싱 인스트럭션들을 실행하는 반도체 장치일 수 있다. 메모리 및 스토리지는 다양한 형태의 휘발성 또는 비휘발성 저 장 매체일 수 있다. 예를 들어, 메모리는 ROM이나 RAM을 포함할 수 있다. 따라서, 본 발명의 실시예는 컴퓨터로 구현된 방법이나 컴퓨터에서 실행 가능한 명령어들이 기록된 비일시적인 컴퓨터에서 읽을 수 있는 매체로 구현될 수 있다. 컴퓨터에서 읽을 수 있는 명령어들이 프로세서에 의해서 수 행될 때, 컴퓨터에서 읽을 수 있는 명령어들은 본 발명의 적어도 한 가지 측면에 따른 방법을 수행할 수 있다. 이상에서와 같이 본 발명에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법 및 이를 위한 장 치는 상기한 바와 같이 설명된 실시예들의 구성과 방법이 한정되게 적용될 수 있는 것이 아니라, 상기 실시예들 은 다양한 변형이 이루어질 수 있도록 각 실시예들의 전부 또는 일부가 선택적으로 조합되어 구성될 수도 있다."}
{"patent_id": "10-2022-0164411", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 방법을 나타낸 동작 흐름도이다. 도 2는 본 발명에 따른 다중 소켓 구조의 호스트 CPU의 일 예를 나타낸 도면이다. 도 3은 본 발명의 일실시예에 따른 기계학습 병렬화 방법 중 컴파일 단계를 상세하게 나타낸 동작 흐름도이다. 도 4는 본 발명의 일실시예에 따른 기계학습 병렬화 방법 중 런타임 단계를 상세하게 나타낸 동작 흐름도이다. 도 5 내지 도 7은 본 발명에 따른 다중 소켓 구조의 호스트 CPU를 활용하여 다중 쓰레드 기반 모델 병렬 실행의 일 예를 나타낸 도면이다. 도 8은 본 발명의 일실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치를 나타낸 블록 도이다. 도 9는 본 발명의 다른 실시예에 따른 다중 소켓 구조의 호스트 CPU를 이용한 기계학습 병렬화 장치를 나타낸 블록도이다. 도 10은 도 9에 도시된 런타임 모듈을 상세하게 나타낸 블록도이다. 도 11은 본 발명의 일실시예에 따른 컴퓨터 시스템을 나타낸 도면이다."}
