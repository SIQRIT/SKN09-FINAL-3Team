{"patent_id": "10-2020-0053234", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0135379", "출원번호": "10-2020-0053234", "발명의 명칭": "서사 자동생성을 위한 콘텐츠 정보 추출 및 분류 시스템", "출원인": "인하대학교 산학협력단", "발명자": "장준도"}}
{"patent_id": "10-2020-0053234", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인물, 사물 또는 음악을 포함하는 영상, 음성 또는 텍스트 형태의 콘텐츠를 입력받는 수신부;수신된 상기 콘텐츠로부터 영상 정보, 음성 정보 또는 언어 정보를 추출하고 분류하여 저장하는 데이터베이스;및상기 영상 정보, 상기 음성 정보 또는 상기 언어 정보를 상기 콘텐츠의 종류별로 분류하는 분류부를 포함하며,상기 데이터베이스는,수신된 상기 콘텐츠의 상기 영상 정보를 추출하는 영상 정보 추출부;수신된 상기 콘텐츠의 상기 음성 정보를 추출하는 음성 정보 추출부; 및수신된 상기 콘텐츠의 텍스트로부터 상기 언어 정보를 추출하는 언어 정보 추출부를 포함하는 것을 특징으로 하는 콘텐츠 정보 추출 및 분류 시스템."}
{"patent_id": "10-2020-0053234", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 영상 정보 추출부는,상기 콘텐츠에서 추출하는 상기 영상 정보로서 상기 콘텐츠에 포함된 인물의 동작 정보 또는 사물 정보를 추출하여 인식하는 것을 특징으로 하는 콘텐츠 정보 추출 및 분류 시스템."}
{"patent_id": "10-2020-0053234", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 음성 정보 추출부는,상기 콘텐츠에서 추출하는 상기 음성 정보로서 상기 콘텐츠에 포함된 인물의 발화 정보 또는 음악 정보를 추출하여 인식하는 것을 특징으로 하는 콘텐츠 정보 추출 및 분류 시스템."}
{"patent_id": "10-2020-0053234", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 언어 정보 추출부는,상기 콘텐츠에서 추출하는 상기 언어 정보로서 상기 음성 정보 추출부에서 추출한 인물의 상기 발화 정보 또는상기 콘텐츠에 포함된 텍스트를 추출하여 인식하는 것을 특징으로 하는 콘텐츠 정보 추출 및 분류 시스템."}
{"patent_id": "10-2020-0053234", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서,상기 언어 정보 추출부는,Speech To Text 또는 Lip Read 방법을 이용하여 상기 콘텐츠에 포함된 인물의 상기 발화 정보로부터 상기 언어공개특허 10-2021-0135379-3-정보를 추출하여 인식하는 것을 특징으로 하는 콘텐츠 정보 추출 및 분류 시스템."}
{"patent_id": "10-2020-0053234", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1 항에 있어서,상기 분류부는,상기 콘텐츠의 종류로서 상기 콘텐츠에 포함된 인물의 감정의 유형, 인물의 감정 수준, 인물 간의 갈등 유형 또는 인물 간의 갈등의 심화 수준을 기준으로 분류하는 것을 특징으로 하는 콘텐츠 정보 추출 및 분류 시스템."}
{"patent_id": "10-2020-0053234", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인물, 사물 또는 음악을 포함하는 영상, 음성 또는 텍스트 형태의 콘텐츠를 입력받는 수신부; 수신된 상기 콘텐츠로부터 영상 정보, 음성 정보, 언어 정보를 추출하고 분류하여 저장하는 데이터베이스; 및 상기 영상 정보, 상기 음성 정보 또는 상기 언어 정보를 상기 콘텐츠의 종류별로 분류하는 분류부를 포함하며, 상기 데이터 베이스는, 수신된 상기 콘텐츠의 영상 정보를 추출하는 영상 정보 추출부; 수신된 상기 콘텐츠의 음성 정보를 추 출하는 음성 정보 추출부; 및 수신된 상기 콘텐츠의 텍스트로부터 언어 정보를 추출하는 언어 정보 추출부를 포 함하여, 상기 콘텐츠의 상황 또는 종류에 따른 데이터를 추출하고 분류하여 저장할 수 있는 것을 특징으로 한다."}
{"patent_id": "10-2020-0053234", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 입력 데이터인 콘텐츠에서 영상, 음성 또는 언어 데이터 등을 추출할 수 있는 시스템에 관한 것이다."}
{"patent_id": "10-2020-0053234", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "서사(내러티브; narrative)는 시간과 공간에서 일어나는, 원인과 결과로 연결된 일련의 사건으로서, 모든 매체 의 내러티브에 있어 인과관계, 시간 그리고 공간처럼 의미를 파악하게 하는 요소들은 중요한 정보가 된다. 서사 는 하나의 상황으로부터 시작되어 그 후 원인과 결과의 양식을 따라 일련의 변화들이 일어나게 된다. 즉, 사건 은 인과관계 또는 시간적인 질서에 따라 배치되어야 한다. 이를 최근에는 인공지능 등을 이용하여 언어 정보, 문장 또는 글을 생성하는 단계까지 발전하였다. 특히 신문기 사를 작성함에 있어서 기존의 데이터를 바탕으로 기존 문맥에 어울리는 신문기사를 생성해내는 로봇의 알고리즘 을 이용하는 저널리즘이 광범위하게 사용되고 있다. 그러한, 현재의 로봇 저널리즘은 스포츠, 금융 등 수치화된 데이터가 빠르게 업데이트되는 특징을 가지는 특정 분야에서만 독자의 편의를 제공해주는 수준으로 문맥이 추가 되는 역할로 사용되고 있다. 다시 말하면, 이러한 기술이 수준 높은 콘텐츠를 전달하고자 하는 목적으로 사용되 지 않는다. OpenAI의 GPT-2 인공지능 기계학습 모델은 정형화 된 데이터가 아닌 대규모 데이터를 기반으로 한 기계학습으로 범용 언어 생성을 목표로 하고 있다. 이 학습 모델은 문단 단위 이상의 작문에서도 인간이 읽기에 위화감이 없 는 수준의 성과물을 만들어낸 바는 있다. 그러나, 방대한 데이터와 수억개의 파라미터를 근거로 하여 연산을 수 행함에도 불구하고 수 페이지 이상의 서사적 콘텐츠를 완결된 형태로 만들어내는 데에는 아직 이르지 못하고 있 는 실정이다. 한편, 본 출원인은 서사 콘텐츠의 구성에서 영향력이 큰 요소들을 변수로 제시하여 창작물의 기계적 모방에서 연산량을 감소시키고 연산 절차를 감소시키는 방법에 대한 추가적인 연구 개발을 진행하였다. 이에, 본 발명을 이용하여 서사 콘텐츠의 기계적 모방이 서사 전체의 맥락 수준에서 타당성을 느낄 수 있는 수준으로 적용될 수 있음을 확인하게 되었다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제10-2073979호"}
{"patent_id": "10-2020-0053234", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2020-0053234", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 서사 자동생성을 위한 콘텐츠 정보 추출 및 분류 시스템으로서, 콘텐츠를 입력받아 해당 콘텐츠에 관 한 음성, 영상 또는 언어 정보를 추출하고 이를 콘텐츠 별로 분류하는 시스템을 제공하고자 한다. 본 발명은 서 사 자동 생성을 위해 필요한 데이터 수집의 기준을 설정하고자 하는 발명으로서, 문장이나 문단 수준을 넘어선 서사 단위의 의미 있는 데이터를 생성하기 위한 기준을 제시한다."}
{"patent_id": "10-2020-0053234", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "본 발명이 해결하려는 과제들은 앞에서 언급한 과제들로 제한되지 않는다. 본 발명의 다른 과제 및 장점들은 아 래 설명에 의해 더욱 분명하게 이해될 것이다."}
{"patent_id": "10-2020-0053234", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위하여 본 발명은, 인물, 사물 또는 음악을 포함하는 영상, 음성 또는 텍스트 형태의 콘 텐츠를 입력받는 수신부; 수신된 상기 콘텐츠로부터 영상 정보, 음성 정보 또는 언어 정보를 추출하고 분류하여 저장하는 데이터베이스; 및 상기 영상 정보, 상기 음성 정보 또는 상기 언어 정보를 상기 콘텐츠의 종류별로 분 류하는 분류부를 포함하며, 상기 데이터베이스는, 수신된 상기 콘텐츠의 상기 영상 정보를 추출하는 영상 정보 추출부; 수신된 상기 콘텐츠의 상기 음성 정보를 추출하는 음성 정보 추출부; 및 수신된 상기 콘텐츠의 텍스트 로부터 상기 언어 정보를 추출하는 언어 정보 추출부를 포함하는 것을 특징으로 한다. 바람직하게, 상기 영상 정보 추출부는, 상기 콘텐츠에서 추출하는 상기 영상 정보로서 상기 콘텐츠에 포함된 인 물의 동작 정보 또는 사물 정보를 추출하여 인식할 수 있다. 바람직하게, 상기 음성 정보 추출부는, 상기 콘텐츠에서 추출하는 상기 음성 정보로서 상기 콘텐츠에 포함된 인 물의 발화 정보 또는 음악 정보를 추출하여 인식할 수 있다. 바람직하게, 상기 언어 정보 추출부는, 상기 콘텐츠에서 추출하는 상기 언어 정보로서 상기 음성 정보 추출부에 서 추출한 인물의 상기 발화 정보 또는 상기 콘텐츠에 포함된 텍스트를 추출하여 인식할 수 있다. 바람직하게, 상기 언어 정보 추출부는, Speech To Text 또는 Lip Read 방법을 이용하여 상기 콘텐츠에 포함된 인물의 상기 발화 정보로부터 상기 언어 정보를 추출하여 인식할 수 있다. 바람직하게, 상기 분류부는, 상기 콘텐츠의 종류로서 상기 콘텐츠에 포함된 인물의 감정의 유형, 인물의 감정 수준, 인물 간의 갈등 유형 또는 인물 간의 갈등의 심화 수준을 기준으로 분류할 수 있다."}
{"patent_id": "10-2020-0053234", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 서사 단위의 의미 있는 데이터를 생성하기 위한 기준을 제시함으로써 향후 서사 자동생성 시 활용할 수 있다. 또한, 이러한 기준을 사전에 정의함으로써 언어 생성을 위한 기계학습이 보다 효율적으로 이루 어지도록 할 수 있다. 즉, 문단 단위 이상의 의미 있는 언어 생성이 가능한 장점을 갖는다."}
{"patent_id": "10-2020-0053234", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면들에 기재된 내용들을 참조하여 본 발명을 상세히 설명한다. 다만, 본 발명이 예시적 실시 예 들에 의해 제한되거나 한정되는 것은 아니다. 각 도면에 제시된 동일 참조부호는 실질적으로 동일한 기능을 수행하는 부재를 나타낸다. 본 발명의 목적 및 효과는 하기의 설명에 의해서 자연스럽게 이해되거나 보다 분명해 질 수 있으며, 하기의 기 재만으로 본 발명의 목적 및 효과가 제한되는 것은 아니다. 또한, 본 발명을 설명함에 있어서 본 발명과 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명을 생략하기로 한다. 도 1은 본 발명의 실시 예에 따른 콘텐츠 정보 추출 및 분류 시스템의 구성도를 나타낸다. 콘텐츠 정보 추출 및 분류 시스템은 수신부, 데이터베이스 및 분류부를 포함할 수 있다. 수신부는 인물, 사물 또는 음악을 포함하는 영상, 음성 또는 텍스트 형태의 콘텐츠를 입력받을 수 있다. 데이터베이스는 수신된 콘텐츠로부터 영상 정보, 음성 정보, 언어 정보를 추출하고 분류하여 저장할 수 있 다. 데이터베이스는 영상 정보 추출부, 음성 정보 추출부 및 언어 정보 추출부를 포함할 수 있다. 영상 정보 추출부는 수신된 콘텐츠의 영상 정보를 추출할 수 있다. 영상 정보 추출부는 콘텐츠에서 추출하는 영상 정보로서 콘텐츠에 포함된 인물의 동작 정보 또는 사물 정보를 추출하여 인식할 수 있다. 영상 정보 추출부에서 추출하는 영상 정보는 입력되는 콘텐츠에 등장하는 인물의 표정, 인물의 움직임에 관한 정보를 포함할 수 있다. 또한, 영상 정보 추출부는 콘텐츠에 등장하는 사물, 소품, 인물의 복장과 해당 영 상의 배경 등의 사물적 요소를 인식할 수 있다. 음성 정보 추출부는 수신된 콘텐츠의 음성 정보를 추출할 수 있다. 음성 정보 추출부는 콘텐츠에서 추출하는 음성 정보로서 콘텐츠에 포함된 인물의 발화 정보 또는 음악 정보를 추출하여 인식할 수 있다. 음성 정보 추출부는 입력되는 콘텐츠에 포함된 음악을 Audio Finger Printing을 통해 해당 음악의 아티스트, 곡 명, 출시된 연도 등의 정보를 추출 및 분석할 수 있다. 또한, 입력되는 콘텐츠에 포함된 해당 음악의 분위기를 분석할 수 있다. 음의 높낮이, 음의 간격, 리듬 등의 정보를 콘텐츠로부터 추출할 수 있다. 언어 정보 추출부는 수신된 콘텐츠의 텍스트로부터 언어 정보를 추출할 수 있다. 언어 정보 추출부는 콘텐츠에서 추출하는 언어 정보로서 음성 정보 추출부에서 추출한 인물의 발화 정보 또는 콘텐츠에 포함된 텍스 트를 추출하여 인식할 수 있다. 언어 정보 추출부는 Speech To Text 또는 Lip Read 방법을 이용하여 콘텐 츠에 포함된 인물의 발화 정보로부터 언어 정보를 추출하여 인식할 수 있다. 분류부는 데이터베이스에 저장된 영상 정보, 음성 정보 또는 언어 정보를 콘텐츠의 종류별로 분류할 수 있으며, 분류된 영상 정보, 음성 정보 또는 언어 정보를 그룹화하여 저장할 수 있다. 분류부는 콘텐츠의 종 류로서 콘텐츠에 포함된 인물의 유형, 인물의 감정의 유형, 인물의 심리상태, 인물의 욕구, 인물의 감정 수준, 인물의 행동 의도, 인물 간의 갈등 유형, 인물 간의 갈등 단계 또는 인물 간의 갈등의 심화 수준 등을 기준으로 분류할 수 있다. 분류부는 해당 기준에 따라 데이터베이스에 저장된 영상 정보, 음성 정보, 언어 정보 를 그룹화하여 구성할 수 있다. 또한, 분류부는 영상 정보 추출부에서 추출한 인물의 동작 정보를 인 물의 행동에 따라 키워드 화하여 키워드별로 분류할 수 있다. 분류부는 영상 정보 추출부에서 추출한 사물 또는 배경 정보를 인물의 행동, 인물의 감정 또는 사물이 가지는 의미에 따라 키워드 화하여 키워드별로 분류할 수 있다. 도 2는 본 발명의 실시 예에 따른 입력 데이터인 영상 서사 저작물로부터 추출되는 음성 정보 및 영상 정보의 유형의 구조도를 나타낸다. 입력 데이터로서 표현되는 영상 서사 저작물은 유형별로 분류 처리가 될 원본 콘텐 츠를 의미한다. 영상 서사 저작물은 영상과 음성을 모두 가진 비디오 콘텐츠일 수 있고, 영상이 없이 음성만으 로 이루어진 오디오 콘텐츠 또는 멀티미디어 정보가 없는 텍스트 콘텐츠인 경우를 포함한다. 콘텐츠 정보 추출 및 분류 시스템은 영상 서사저작물을 입력받은 뒤, 음성 정보와 영상 정보로 분류하여 추출한다. 음성 정보 중 언어 정보에 대해서는 Speech To Text 또는 Lip Read 방법을 이용하여 영상 서사저작물 내의 단위 시간 또는 단위 분량을 기준으로 해당 콘텐츠 내에서 이루어진 인물의 발화에 관한 언어 정보(어휘, 구(句), 문 장, 단락 등)를 추출할 수 있다. 음악 정보에 대해서는 Audio Finger Print, Foxonomy Data, Other Musical Components등을 이용하여 영상 서사저작물 내의 단위 시간 또는 단위 분량에 대한 정보를 추출할 수 있다. 단위 시간 또는 단위 분량에 대해 해당 콘텐츠 내에 삽입된 음악 콘텐츠를 대상으로 배경 음악 등의 음악의 감정적 정보를 추출할 수 있다. 이렇게 음악을 통해 추출한 감정적 정보는 영상 정보 추출부에서 추출한 인물의 표정 또는 행동 정보, 음성 정보 추출부에서 추출한 인물의 발화 내용, 목소리의 높낮이 등의 정보와 연관되어 함께 분류 및 분석될 수 있다. 영상 정보에 대해서는 영상 서사저작물의 영상에 등장하는 인물의 동작(변화)을 인식하거나 사물을 인식할 수 있다. 영상 정보 중 동작을 인식함에 있어서, Motion detection 또는 Facial Detection 방법을 이용할 수 있다. 인물의 행동과 표정에 관한 정보의 추출은 이후 인물의 감정분석에 사용될 수 있다. 사물은 인물 주변에 소품으로 놓여져 인물과 함께 또는 사물 단독으로 영상을 구성하거나, 영상을 구성하는 배경으로서 인물과 함께 또는 배경 단독으로 영상을 구성할 수 있다. 따라서, 영상 정보 중 사물을 인식함에 있어서, 소품 또는 배경인 사물 정보를 추출하여 인식할 수 있다. 영상 서사저작물에서 추출된 각 음성 정보와 영상 정보는 데이터베이스 에 저장될 수 있다. 이상에서 대표적인 실시예를 통하여 본 발명을 상세하게 설명하였으나, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자는 상술한 실시예에 대하여 본 발명의 범주에서 벗어나지 않는 한도 내에서 다양한 변형이 가능 함을 이해할 것이다. 그러므로 본 발명의 권리 범위는 설명한 실시예에 국한되어 정해져서는 안 되며, 후술하는 특허청구범위뿐만 아니라 특허청구범위와 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태에 의하여 정 해져야 한다."}
{"patent_id": "10-2020-0053234", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시 예에 따른 콘텐츠 정보 추출 및 분류 시스템의 구성도를 나타낸다. 도 2는 본 발명의 실시 예에 따른 입력 데이터인 영상 서사 저작물로부터 추출되는 음성 정보 및 영상 정보의 유형의 구조도를 나타낸다."}
