{"patent_id": "10-2019-0115674", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0033850", "출원번호": "10-2019-0115674", "발명의 명칭": "목소리 및 얼굴 안면 감정값의 산출 방법 및 이를 이용한 인공지능 스피커의 출력 방법", "출원인": "주식회사 세미콘네트웍스", "발명자": "백대원"}}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "a. 디바이스로부터 얼굴 안면 및 목소리를 수집하는 단계;b. 상기 수집된 얼굴 안면 및 목소리를 이미지화 또는 데이터화하는 단계;c. 상기 데이터화된 목소리의 감정을 인식하여 제1 분석값을 산출하는 단계;d. 상기 이미지화된 얼굴 안면의 감정을 인식하여 제2 분석값을 산출하는 단계;e. 상기 제1 분석값 및 제2 분석값에 가중치 지수테이블(matrix table)을 산정하여 최종 감정값을 산출하는 단계;를 포함하는 목소리 및 얼굴 안면 감정값의 산출 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 기재된 목소리 및 얼굴 안면으로부터 산출된 감정값을 디바이스를 통해 출력하는 방법에 있어서,f. 디바이스에서 산출된 최종 감정값을 서버에 제공하는 단계;g. 상기 서버에서 최종 감정값을 반영한 답변 데이터를 산출하는 단계;h. 상기 답변 데이터를 디바이스에 제공하는 단계; 및i. 상기 디바이스에 내장된 음성합성(text to speech) 알고리즘을 통해 음성데이터로 변환하며, 변환된 음성데이터를 출력하는 단계;를 포함하는 인공지능 스피커의 출력 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서,상기 디바이스는 얼굴 안면 및 목소리를 인식할 수 있는 인식수단을 포함하는 목소리 및 얼굴 안면 감정값의 산출 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 1 및 청구항 2에 있어서,상기 디바이스는 팔 또는 다리 형상 중 적어도 어느 한 부위가 구비된 디바이스를 포함하며, 상기 음성데이터를 출력할 시, 상기 팔 또는 다리 중 적어도 어느 한 부위가 상,하,좌,우로 움직이는 목소리 및얼굴 안면 감정값의 산출 방법 및 이를 이용한 인공지능 스피커의 출력 방법,"}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서,얼굴 위치 또는 목소리의 방향을 탐지하는 방향탐색부를 포함하며, 공개특허 10-2021-0033850-3-상기 디바이스가 상기 얼굴 안면 또는 목소리의 방향이 일치하지 않는 경우, 상기 방향탐색부를 통해 인식된 방향으로 구동되는 목소리 및 얼굴 안면 감정값의 산출 방법 및 이를 이용한 인공지능 스피커의 출력 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 1에 있어서,상기 a 단계는 목소리를 수집하면서 배경음악 또한 수집될 경우, 수집된 배경음악을 분석하고, 분석된 배경음악의 가수, 작곡가 또는 동일장르음악 중 어느 하나를 선택하여 이를 추천하는 목소리 및 얼굴 안면 감정값의 산출 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "또는 8 중 적어도 어느 한 항에 있어서,상기 제1 신경망 및 제2 신경망은 다중 나선형 신경망(Multi Convolutionary Neural Network)에 포함되고, 제1신경망 및 제2 신경망 중 어느 하나는 메인 신경망이고, 또 다른 하나는 서브 신경망인 것을 포함하는 목소리및 얼굴 안면 감정값의 산출 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 1에 있어서,상기 d 단계는 상기 얼굴 안면 이미지를 외곽선 추출 알고리즘을 통해 인간의 얼굴 안면만 추출하고, 얼굴 안면이 조정되는 단계;상기 조정된 얼굴 안면 이미지를 제1 신경망 및 제2 신경망에 각각 입력하는 단계;입력받은 얼굴 안면 이미지의 감정값을 산출하는 단계;상기 산출된 감정값의 확신율(confidence level)이 높은 순으로 배열하고, 1순위 및 2순위로 높은 감정값을 선택하는 단계;상기 1순위의 감정값 및 확신율과, 상기 2순위의 감정값 및 확신율을 저장매체에 저장하는 단계;를 포함하는 목소리 및 얼굴 안면 감정값의 산출 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 7 또는 8 중 적어도 어느 한 항에 있어서,상기 최종 감정값은 서버에 내장된 데이터베이스에 저장되고, 이를 빅데이터를 구축하며, 서버에 내장된 학습모듈에 의해 학습되는 목소리 및 얼굴 안면 감정값의 산출 방법.공개특허 10-2021-0033850-4-청구항 10"}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "청구항 7 또는 8 중 적어도 어느 한 항에 있어서,상기 감정값이 사전에 설정된 기준값보다 초과된 경우, 사전에 설정된 데이터를 먼저 노출한 이후, 음성데이터를 출력하는 목소리 및 얼굴 안면 감정값의 산출 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "청구항 7 또는 8 중 적어도 어느 한 항에 있어서,상기 감정값은 분노, 싫음, 공포, 멸시, 슬픔, 놀람, 행복, 중립 총 8개의 감정에 대한 값을 산출하는 목소리및 얼굴 안면 감정값의 산출 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "청구항 8에 있어서,상기 인간의 얼굴 안면만 추출된 이미지는 감정값에 도출할 것인지의 유무를 결정하는 단계;를 포함하며, 상기한 단계는 디바이스의 내장모듈에 의해 결정되는 목소리 및 얼굴 안면 감정값의 산출 방법.."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "청구항 1에 있어서,상기 가중치 지수테이블은 수집된 목소리의 언어의 가중치를 산정하는 언어가중치; 및 수집된 얼굴 안면의 태생 국가를 산정하는 국가가중치;를 포함하는 목소리 및 얼굴 안면 감정값의 산출 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "청구항 2에 있어서,상기 음성합성 알고리즘은 상기 최종 감정값을 참조하여 이에 상응하는 음성의 볼륨, 피치 및 스피드를 산출하고, 상기 산출한 값을 반영하여 음성데이터로 변환되는 인공지능 스피커의 출력 방법."}
{"patent_id": "10-2019-0115674", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "청구항 1 및 청구항 2에 있어서,상기 디바이스로부터 분석된 최종 감정값이 상기 서버에 저장된 이후, 디바이스로부터 기 수집된 목소리 및 얼굴 안면 데이터를 삭제함으로써 사용자의 사생활을 보호하는 목소리 및 얼굴 안면 감정값의 산출 방법 및 이를이용한 인공지능 스피커의 출력 방법.공개특허 10-2021-0033850-5-"}
{"patent_id": "10-2019-0115674", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 목소리 및 얼굴 안면 감정값의 산출 방법 및 이를 이용한 인공지능 스피커의 출력 방법에 관한 것이다. 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면으로부터 산출된 감정값을 기반한 인공지능 스피커의 출력 방법은 디바이스로부터 얼굴 안면 및 목소리를 수집하는 단계와, 상기 수집된 얼굴 안면 및 목소리를 이미 (뒷면에 계속)"}
{"patent_id": "10-2019-0115674", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 목소리 및 얼굴 안면 감정값의 산출 방법 및 이를 이용한 인공지능 스피커의 출력 방법에 관한 것이다. 더욱 상세하게는 사용자의 목소리 및 얼굴 안면의 감정값을 산출하고, 감정값을 반영하여 사용자의 질문에 답변하기 위한 목소리 및 얼굴 안면 감정값의 산출 방법 및 이를 이용한 인공지능 스피커의 출력 방법에 관한 것이다."}
{"patent_id": "10-2019-0115674", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근, 음악 감상이나 라디오 청취에 활용되던 스피커가 음성인식 기술과 만나 진화하고 있다. 스마트폰에서 쉽 게 만나볼 수 있는 음성인식 기술과 클라우드, 인공지능(AI) 기술을 활용해 단순하게 소리를 전달하는 도구에서 생각하고 관리하는 AI 스피커로 변신중이다. 이미 아마존, 구글, 애플 등 글로벌 IT 기업이 AI 스피커 제품을 출시하였으며, 국내에서도 SK텔레콤, KT, 네이버, 삼성전자 등도 이 시장에 관심을 보이고 있다. 기업들은 음성 기반 플랫폼이 만들어지면 스마트 홈으로 가는 사물인터넷(IoT)시장을 손쉽게 선점할 수 있다. 스마트 홈은 집 안 각종 가전제품, 수도, 전기사용량 등을 통신에 연결해 모니터링하고 제어할 수 있는 집을 말 한다. 이 핵심엔 현재 AI 스피커가 자리 잡았다. 음성 기반 플랫폼을 이용하면 손을 이용하지 않고도 편리하게 기기를 관리하거나 제어할 수 있다. AI 스피커가 등장하는 이유다. AI 스피커는 인공지능 알고리즘을 이용해 사용자와 음성으로 의사소통을 한다. AI 스피커를 이용하면 음성인식 을 통해 집안의 기기를 목소리만으로 간편하게 제어하는 식으로 손쉽게 스마트 홈 환경을 구축할 수 있다. 또한, 터치 기반과 달리 음성 기반 조작은 쉽게 배우고 사용할 수 있으며, (특허문헌 1)을 통해 AI 스피커를 구 현하고 있는 바, (특허문헌 1)을 참조하여 구체적으로 설명하면 다음과 같다. (특허문헌 1)은 인공지능 스피커 및 이의 제어 방법에 관한 것으로, 이에 따르면, 인공지능 스피커의 제어 방법 에 있어서, 마이크로부터 입력된, 사용자 음성 신호 및 상기 인공지능 스피커의 주위의 가전기기로부터 출력된 오디오 신호를 포함하는 입력 신호를 입력받는 단계와, 외부 장치로부터 상기 가전기기의 오디오 신호에 상응하 는 에코 기준 신호를 수신하는 단계와, 상기 수신된 에코 기준 신호를 기초로 상기 입력 신호로부터 상기 오디 오 신호를 제거한 전처리 신호를 출력하는 단계 및 상기 출력된 전처리 신호를 인식하여 음성인식 제어신호를 출력하는 단계를 포함하고, 상기 가전기기는 텔레비전이고, 상기 외부장치는 상기 텔레비전과 HDMI를 통해 접속 된 셋톱박스이고, 상기 인공지능 스피커는, 상기 셋톱박스와 유선 또는 무선 통신 인터페이스를 통해 접속되어 상기 셋톱박스로부터 상기 에코기준신호를 수신하는 것을 특징으로 하고 있다. 그러나, 전술한 (특허문헌 1)에 따르면, AI 스피커로부터 수집된 사용자의 데이터(예컨대 사용자의 안면 이미지 또는 목소리 음성 데이터 등)를 관리서버에 전송하게 되면, 사용자의 생체데이터는 그대로 저장되는 구조로 이 루어져 사용자의 인권침해가 빈번하게 발생하고 있고, 사용자의 생체 데이터를 수집하는데 있어 법적으로 문제 가 야기되고 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 등록특허 제10-1970731호"}
{"patent_id": "10-2019-0115674", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 사용자의 목소리 및 얼굴 안면의 감정값을 산출하고, 감정값을 반영하여 사용자의 질문에 답 변하기 위한 목소리 및 얼굴 안면 감정값의 산출 방법 및 이를 이용한 인공지능 스피커의 출력 방법을 제공하는 데 있다."}
{"patent_id": "10-2019-0115674", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 과제를 해결하기 위하여, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법은 a. 디바이스로부터 얼굴 안면 및 목소리를 수집하는 단계; b. 상기 수집된 얼굴 안면 및 목소리를 이미지화 또는 데이터화하는 단계; c. 상기 데이터화된 목소리의 감정을 인식하여 제1 분석값을 산출하는 단계; d. 상기 이미지화된 얼굴 안면의 감정을 인식하여 제2 분석값을 산출하는 단계; e. 상기 제1 분석값 및 제2 분석값에 가중치 지수테이블(matrix table)을 산정하여 최종 감정값을 산출하는 단 계; 를 포함할 수 있다. 본 발명의 일실시 예에 따른 인공지능 스피커의 출력방법은 f. 상기 산출된 최종 감정값을 서버에 제공하는 단계; g. 상기 서버에서 최종 감정값을 반영한 답변 데이터를 산출하는 단계; h. 상기 답변 데이터를 디바이스에 제공하는 단계; 및 i. 상기 디바이스에 내장된 음성합성(text to speech) 알고리즘을 통해 음성데이터로 변환하며, 변환된 음성데 이터를 출력하는 단계; 를 포함할 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 디바이스는 얼굴 안면 및 목소리를 인식할 수 있는 인식수단을 포함할 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 디바이스는 팔 또 는 다리 형상 중 적어도 어느 한 부위가 구비된 디바이스를 포함하며, 상기 음성데이터를 출력할 시, 상기 팔 또는 다리 중 적어도 어느 한 부위가 상,하,좌,우로 움직일 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 얼굴 위치 또는 목소리 의 방향을 탐지하는 방향탐색부를 포함하며, 상기 디바이스가 상기 얼굴 안면 또는 목소리의 방향이 일치하지 않는 경우, 상기 방향탐색부를 통해 인식된 방 향으로 구동될 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 a 단계는 목소리를 수집하면서 배경음악 또한 수집될 경우, 수집된 배경음악을 분석하고, 분석된 배경음악의 가수, 작곡가 또는 동 일장르음악 중 어느 하나를 선택하여 이를 추천할 수 있다.또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 c 단계는 데이터화 된 목소리를 국소 푸리에 변환(STFT) 및 멜스펙트로그램(Melspectogram) 알고리즘 방식으로 음성데이터의 주파 수를 최적화하는 단계; 상기 최적화된 주파수를 제1 신경망 및 제2 신경망에 각각 입력하는 단계; 상기 최적화된 주파수를 부동 소숫점으로 수치화하는 단계; 상기 수치화된 데이터를 기반으로 감정값을 산출하는 단계; 상기 산출된 감정값의 확신율(confidence level)이 높은 순으로 배열하고, 1순위 및 2순위로 높은 감정값을 선 택하는 단계; 상기 1순위의 감정값 및 확신율과, 상기 2순위의 감정값 및 확신율을 저장매체에 저장하는 단계; 를 포함할 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 d 단계는 상기 얼 굴 안면 이미지를 외곽선 추출 알고리즘을 통해 인간의 얼굴 안면만 추출하고, 얼굴 안면이 조정되는 단계; 상기 조정된 얼굴 안면 이미지를 제1 신경망 및 제2 신경망에 각각 입력하는 단계; 입력받은 얼굴 안면 이미지의 감정값을 산출하는 단계; 상기 산출된 감정값의 확신율(confidence level)이 높은 순으로 배열하고, 1순위 및 2순위로 높은 감정값을 선 택하는 단계; 상기 1순위의 감정값 및 확신율과, 상기 2순위의 감정값 및 확신율을 저장매체에 저장하는 단계; 를 포함할 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 최종 감정값은 서 버에 내장된 데이터베이스에 저장되고, 이를 빅데이터를 구축하며, 서버에 내장된 학습모듈에 의해 학습될 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 제1 신경망 및 제2 신경망은 다중 나선형 신경망(Multi Convolutionary Neural Network)에 포함되고, 제1 신경망 및 제2 신경망 중 어느 하나는 메인 신경망이고, 또 다른 하나는 서브 신경망인 것을 포함할 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 감정값이 사전에 설정된 기준값보다 초과된 경우, 사전에 설정된 데이터를 먼저 노출한 이후, 음성데이터를 출력할 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 감정값은 분노, 싫 음, 공포, 멸시, 슬픔, 놀람, 행복, 중립 총 8개의 감정에 대한 값을 산출할 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 인간의 얼굴 안면 만 추출된 이미지는 감정값에 도출할 것인지의 유무를 결정하는 단계;를 포함하며, 상기한 단계는 디바이스의 내장모듈에 의해 결정될 수 있다. 또한, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법에 있어서, 상기 가중치 지수테이블 은 수집된 목소리의 언어의 가중치를 산정하는 언어가중치; 및 수집된 얼굴 안면의 태생 국가를 산정하는 국가가중치;를 포함할 수 있다. 또한, 본 발명의 일실시 예에 따른 인공지능 스피커의 출력 방법에 있어서, 상기 음성합성 알고리즘은 상기 최 종 감정값을 참조하여 이에 상응하는 음성의 볼륨, 피치 및 스피드를 산출하고, 상기 산출한 값을 반영하여 음 성데이터로 변환될 수 있다. 이러한 해결 수단은 첨부된 도면에 의거한 다음의 발명의 상세한 설명으로부터 더욱 명백해질 것이다. 이에 앞서, 본 명세서 및 청구범위에 사용된 용어나 단어는 통상적이고 사전적인 의미로 해석되어서는 아니 되 며, 발명자가 그 자신의 발명을 가장 최선의 방법으로 설명하기 위해 용어의 개념을 적절하게 정의할 수 있다는 원칙에 입각하여 본 발명의 기술적 사상에 부합되는 의미와 개념으로 해석되어야만 한다."}
{"patent_id": "10-2019-0115674", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일실시 예에 따르면, 디바이스로부터 수집된 사용자의 얼굴 안면 이미지 또는 목소리 음성 데이터가 서버에 별도로 저장되지 아니하고, 알고리즘을 통해 산출된 최종 감정값이 서버에 저장되고 이를 빅데이터화되 는 구조로 이루어져 사용자의 인권 침해로부터 방지할 수 있는 효과가 있다. 또한, 본 발명의 일실시 예에 따르면, 사용자의 목소리 및 얼굴 안면의 감정값을 1차로 산출하고, 이에 따른 최 종 감정값을 2차로 산출하는 구조로 이루어져 감정값의 신뢰성을 향상시킬 수 있으며, 디바이스에서 제공되는 광고, 컨텐츠 및 UX에 대한 만족도 분석의 신뢰성 및 정확성을 향상시킬 수 있다. 또한, 본 발명의 일실시 예에 따르면, 사용자의 목소리 및 얼굴을 인식하고, 디바이스가 인식한 방향으로 움직 임으로써, 사용자 및 디바이스 간의 친밀도가 개선될 수 있다."}
{"patent_id": "10-2019-0115674", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 특이한 관점, 특정한 기술적 특징들은 첨부된 도면들과 연관되는 이하의 구체적인 내용과 일실시 예 로부터 더욱 명백해 질 것이다. 본 명세서에서 각 도면의 구성요소들에 참조 부호를 부가함에 있어, 동일한 구 성요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 발명의 일실시 예를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 발 명의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략한다. 또한, 본 발명의 구성요소를 설명하는 데 있어서, 제1, 제2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이러 한 용어는 그 구성요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성요소의 본질이나 차례 또는 순서 등이 한정되지 않는다. 어떤 구성요소가 다른 구성요소에 \"연결\", \"결합\" 또는 \"접속\"된다고 기 재된 경우, 그 구성요소는 그 다른 구성요소에 직접적으로 연결되거나 또는 접속될 수 있지만, 각 구성요소 사 이에 또 다른 구성요소가 \"연결\", \"결합\" 또는 \"접속\"될 수도 있다고 이해되어야 할 것이다.이하, 본 발명의 일실시 예를 첨부된 도면에 의거하여 상세히 설명하면 다음과 같다. 도 1에 도시된 바와 같이, 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법 및 이를 이용 한 인공지능 스피커의 출력 방법은 디바이스로부터 언굴 안면 및 목소리 중 적어도 어느 하나를 수집하는 단계 (S10)와, 수집된 정보를 이미지 또는 데이터화하는 단계(S20)와, 데이터화된 목소리의 감정을 인식하여 제1 분 석값을 산출하는 단계(S30)와, 이미지화된 얼굴 안면의 감정을 인식하여 제2 분석값을 산출하는 단계(S40)와, 제1 분석값 및 제2 분석값에 가중치를 산정하여 최종 감정값을 산출하는 단계(S50)와, 산출된 최종 감정값을 서 버에 제공하는 단계(S60)와, 서버에서 최종 감정값을 반영한 답변 데이터를 산출하는 단계(S70)와, 산출된 답변 데이터를 디바이스에 제공하는 단계(S80) 및 디바이스에서 답변 데이터를 음성데이터로 변환한 후, 음성데이터 를 출력하는 단계(S90)를 포함할 수 있다. 사용자가 디바이스에 질문을 하게 되면, 디바이스는 사용자의 얼굴 안면 및 목소리를 수집하고(S10), 수집된 사 용자의 얼굴 안면 및 목소리를 각각 데이터화 (얼굴 안면은 이미지화)한다(S20). 여기서, 디바이스는 사용자의 얼굴 안면 및 목소리를 인식할 수 있는 인식수단을 포함하며, AI(artificial intelligence) 스피커, 스마트 디바이스, 사용자의 생체 데이터(예컨대 얼굴 안면 및 목소리와 같은 데이터)를 인식할 수 있는 로봇을 포함할 수 있다. 여기서, 상기한 디바이스가 로봇(여기서 로봇은 팔 또는 다리 형상 중 적어도 어느 한 부위가 구비된 디바이스 를 지칭)일 경우, 음성데이터를 출력할 때(S90), 디바이스에 구비된 팔 또는 다리 중 적어도 어느 한 부위가 상,하,좌,우로 움직임으로써 사용자와 감정 교류를 할 수 있도록 구성될 수 있다. 예컨대 상기한 로봇이 향하는 방향이 사용자의 위치와 동일하지 않은 경우, 얼굴 위치 또는 목소리의 방향을 탐 지하는 방향탐색부를 통해 사용자의 위치를 인식하여 해당 위치의 방향으로 구동될 수 있다. 또한, 디바이스에서 사용자의 목소리를 수집하면서(S10), 배경음악과 함께 수집된 경우, 수집된 배경음악을 분 석하고, 분석된 배경음악의 가수, 작곡가, 작사가 또는 동일장르음악 중 어느 하나를 선택하여 이를 사용자에게 추천해줄 수 있으며, 추천할 때 음성데이터로 변환하여 출력(S90)할 수 있다. 한편, 데이터화된 목소리의 감정을 인식하여 제1 분석값을 산출할 수 있으며(S30), 제1 분석값을 산출 방법은 다음과 같다. 도 2에 도시된 바와 같이, 데이터화된 목소리의 감정값(제1 분석값)을 산출 방법은 데이터화된 목소리의 주파수 를 최적화시킨 이후(S31), 제1 신경망 및 제2 신경망에 각각 최적화된 주파수를 입력한다(S32). 여기서, 제1 신경망 및 제2 신경망은 다중 나선형 신경망(MCNN ; Multi Convolutionary Neural Network)에 포 함되며, 제1 신경망 및 제2 신경망 중 어느 하나는 메인 신경망이고, 또 다른 하나는 서브 신경망이며, 본 발명 에서는 제1 신경망이 메인 신경망이고, 제2 신경망이 서브 신경망인 것으로 설명하도록 한다. 즉, 최적화된 주파수를 제1 신경망 및 제2 신경망에 각각 입력된 이후(S32), 제1 신경망 및 제2 신경망에서 별 도의 알고리즘을 통해 주파수를 부동 소숫점으로 각각 수치화시킬 수 있다(S33). 수치화된 부동 소숫점을 기반으로 감정값을 산출할 수 있으며(S34), 구체적으로는 분노, 싫음, 공포, 멸시, 슬 픔, 놀람, 행복, 중립 8개의 감정에 각각 감정값을 산출하여 이를 수치화할 수 있다. 수치화된 데이터는 감정값의 확신율(confidence level)로 환산할 수 있으며, 8개의 감정 중 감정값의 확신율이 가장 높은 1순위 및 차순위로 높은 2순위를 각각 산출하게 된다. 예컨대 감정값이 분노 9%, 싫음 13%, 공포 12%, 멸시 2%, 슬픔 21%, 놀람, 23%, 행복 17%, 중립 3%로 감정값의 확신율이 환산되면, 1순위로 높은 놀람 23%가 1순위의 감정값으로 산정되고, 2순위로 높은 슬픔 21%가 2순위의 감정값으로 산정되는 구조로 이루어진다(S35). 즉, 제1 신경망 및 제2 신경망은 1순위의 감정값 및 확신율과, 2순위의 감정값 및 확신율을 산정한 이후, 저장 매체에 저장될 수 있다(S36). 상기한 저장매체는 제1 분석값에서 최종 감정값을 산출하기 위해 임시로 저장하는 메모리 역할을 수행할 수 있다.여기서, 제1 분석값은 목소리의 감정값을 지칭하며, 구체적으로는 상기한 제1 신경망에서 산출된 1순위의 감정 값 및 확신율과, 2순위의 감정값 및 확신율을 포함하고, 추가로 제2 신경망에서 산출된 1순위의 감정값 및 확신 율과, 2순위의 감정값 및 확신율을 포함할 수 있다. 즉, 제1 분석값은 4개의 감정값 및 확신율을 포함할 수 있 다. 도 3에 도시된 바와 같이, 이미지화된 얼굴 안면의 감정값(제2 분석값)을 산출 방법은 이미지화된 얼굴 안면에 서 얼굴 안면 이미지만 추출하고, 추출된 얼굴 안면 이미지를 조정한다(S41). 조정된 이미지는 후술할 감정값을 산출하는데 사용될 것인지의 유무를 판단하며(S42), 디바이스에 별도로 내장 된 모듈을 통해 판단할 수 있다. 예컨대 상기한 조정된 이미지를 사용하지 않을 경우, 이미지화된 얼굴 안면에 서 얼굴 안면 이미지만 재 추출하여 이미지를 조정한 이후, 사용 유무를 재판단한다. 사용유무를 판단한 이후, 조정된 이미지는 제1 신경망 및 제2 신경망에 각각 입력할 수 있다(S43). 여기서, 제1 신경망 및 제2 신경망은 다중 나선형 신경망(MCNN ; Multi Convolutionary Neural Network)에 포 함되며, 제1 신경망 및 제2 신경망 중 어느 하나는 메인 신경망이고, 또 다른 하나는 서브 신경망이며, 본 발명 에서는 제1 신경망이 메인 신경망이고, 제2 신경망이 서브 신경망인 것으로 설명하도록 한다. 즉, 조정된 이미지를 제1 신경망 및 제2 신경망에 각각 입력한 이후(S43), 제1 신경망 및 제2 신경망에서 별도 의 알고리즘을 통해 조정된 이미지를 부동 소숫점으로 각각 수치화 시킬 수 있다(S44). 수치화된 부동 소숫점을 기반으로 감정값을 산출할 수 있으며(S45), 구체적으로는 분노, 싫음, 공포, 멸시, 슬 픔, 놀람, 행복, 중립 8개의 감정에 각각 감정값을 산출하여 이를 수치화할 수 있다. 수치화된 데이터는 감정값의 확신율(confidence level)로 환산할 수 있으며, 8개의 감정 중 감정값의 확신율이 가장 높은 1순위 및 차순위로 높은 2순위를 각각 산출하게 된다. 예컨대 감정값이 분노 9%, 싫음 13%, 공포 12%, 멸시 2%, 슬픔 21%, 놀람, 23%, 행복 17%, 중립 3%로 감정값의 확신율이 환산되면, 1순위로 높은 놀람 23%가 1순위의 감정값으로 산정되고, 2순위로 높은 슬픔 21%가 2순위의 감정값으로 산정되는 구조로 이루어진다(S46). 즉, 제1 신경망 및 제2 신경망은 1순위의 감정값 및 확신율과, 2순위의 감정값 및 확신율을 산정한 이후, 저장 매체에 저장될 수 있다(S47). 상기한 저장매체는 제2 분석값에서 최종 감정값을 산출하기 위해 임시로 저장하는 메모리 역할을 수행할 수 있다. 여기서, 제2 분석값은 얼굴 안면의 감정값을 지칭하며, 구체적으로는 상기한 제1 신경망에서 산출된 1순위의 감 정값 및 확신율과, 2순위의 감정값 및 확신율을 포함하고, 추가로 제2 신경망에서 산출된 1순위의 감정값 및 확 신율과, 2순위의 감정값 및 확신율을 포함할 수 있다. 즉, 제2 분석값은 4개의 감정값 및 확신율을 포함할 수 있다. 도 1에 도시된 바와 같이, 상기한 방법으로 산출한 제1 분석값 및 제2 분석값에 가중치 지수테이블(matrix table)을 산정하여 최종 감정값을 산출할 수 있다(S50). 여기서, 가중치 지수테이블은 수집된 목소리의 언어의 가중치를 산정하는 언어 가중치 및 수집된 얼굴 안면의 태생 국가를 산정하는 국가가중치를 포함하며, 언어 및 국가에 대한 가중치를 산정하고, 제1 분석값에서 산출한 4개의 목소리 감정값 및 제2 분석값에서 산출한 4개의 얼굴 안면 감정값에 각각 가중치를 산정하여 최종 감정값 을 산출하는 구조로 이루어진다. 구체적으로, 제1 분석값에서 산출한 4개의 목소리 감정값에 각각 가중치를 산정하고, 추가로 제2 분석값에서 산 출한 4개의 얼굴 안면 감정값에 각각 가중치를 산정하며, 총 8개의 감정값에 확신율이 가장 높은 감정값을 최종 감정값으로 산정하는 구조로 이루어지며, 산출된 최종 감정값은 서버에 전송될 수 있다(S60). 서버는 산출된 최종 감정값을 고려하여 사용자가 답변한 질문에 관한 답변 데이터를 생성하며(S70), 생성된 답 변 데이터는 다시 디바이스에 전송된다(S80). 여기서, 답변 데이터를 디바이스에 전송한 이후, 최종 감정값은 서버 내부에 내장된 데이터베이스에 저장되어 최종 감정값에 한정하여 빅데이터를 구축할 수 있으며, 서버에 내장된 학습모듈에 의해 학습되어 인공지능을 학 습시킬 수 있다. 또한, 최종 감정값이 서버에 저장된 이후에는 디바이스로부터 수집된 생체 데이터(최초로 수집된 목소리 및 얼 굴 안면 데이터)를 삭제하여 생체 데이터를 수집된 사용자의 사생활을 보호할 수 있다. 이후, 디바이스는 답변 데이터를 음성 데이터로 변환 한 이후 이를 출력할 수 있다(S90). 여기서, 디바이스에서 음성 데이터로 변환시킬 때 음성합성(text to speech) 알고리즘을 통해 음성 데이터를 변 환시킬 수 있으며, 음성 데이터를 변환시킬 때 최종 감정값을 반영하여 음성의 볼륨(volume), 피치(pitch) 및 스피드(speed)를 조정하여 디바이스로부터 출력될 음성 데이터를 생성할 수 있다. 즉, 본 발명의 일실시 예에 따르면, 디바이스로부터 수집된 사용자의 얼굴 안면 이미지 또는 목소리 음성 데이 터가 서버에 별도로 저장되지 아니하고, 알고리즘을 통해 산출된 최종 감정값이 서버에 저장되고 이를 빅데이터 화되는 구조로 이루어져 사용자의 인권 침해로부터 방지할 수 있는 효과가 있다. 또한, 본 발명의 일실시 예에 따르면, 사용자의 목소리 및 얼굴 안면의 감정값을 1차로 산출하고, 이에 따른 최 종 감정값을 2차로 산출하는 구조로 이루어져 감정값의 신뢰성을 향상시킬 수 있으며, 디바이스에서 제공되는 광고, 컨텐츠 및 UX에 대한 만족도 분석의 신뢰성 및 정확성을 향상시킬 수 있다. 또한, 본 발명의 일실시 예에 따르면, 사용자의 목소리 및 얼굴을 인식하고, 디바이스가 인식한 방향으로 움직 임으로써, 사용자 및 디바이스 간의 친밀도가 개선될 수 있다. 이상 본 발명을 일실시 예를 통하여 상세히 설명하였으나, 이는 본 발명을 구체적으로 설명하기 위한 것으로, 본 발명에 따른 목소리 및 얼굴 안면 감정값의 산출 방법 및 이를 이용한 인공지능 스피커의 출력 방법은 이에 한정되지 않는다. 그리고 이상에서 기재된 \"포함하다\", \"구성하다\", 또는 \"가지다\", 등의 용어는 특별히 반대되 는 기재가 없는 한 해당 구성요소가 내재될 수 있음을 의미하는 것이므로, 다른 구성요소를 제외하는 것이 아니 라 다른 구성요소를 더 포함할 수 있는 것으로 해석되어야 하며, 기술적이거나 과학적인 용어를 포함한 모든 용 어들은, 다르게 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 또한, 이상의 설명은 본 발명의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자라면 본 발명의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형 가능하다. 따라서, 본 발명에 개시된 일실시 예들은 본 발명의 기술 사상을 한정하기 위한 것이 아니라 설명 하기 위한 것이고, 이러한 일실시 예에 의하여 본 발명의 기술 사상의 범위가 한정되는 것은 아니다. 본 발명의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 발 명의 권리범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3"}
{"patent_id": "10-2019-0115674", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법 및 이를 이용한 인공지능 스피커 의 출력 방법을 나타내 보인 순서도. 도 2는 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법의 목소리 음성 데이터의 감정값을 산출하기 위한 순서도. 도 3은 본 발명의 일실시 예에 따른 목소리 및 얼굴 안면 감정값의 산출 방법의 얼굴 안면 이미지의 감정값을 산출하기 위한 순서도."}
