{"patent_id": "10-2023-0129050", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0045748", "출원번호": "10-2023-0129050", "발명의 명칭": "인공지능 기반의 실시간 수화 번역 시스템 및 방법", "출원인": "김종호", "발명자": "김종호"}}
{"patent_id": "10-2023-0129050", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능 기반의 실시간 수화 번역 시스템에 있어서,수화 번역 어플리케이션을 생성하고 배포하는 서버;상기 수화 번역 어플리케이션을 통해, 비장애인인 일반인의 음성을 인식하고 인식된 음성을 수화 또는 텍스트로중 적어도 하나로 출력하는 청각장애인 단말;상기 수화 번역 어플리케이션을 통해, 청각장애인의 수화 모션을 인식하고, 인식된 수화 모션을 텍스트 또는 음성 중 적어도 하나로 출력하는 일반인 단말; 을 포함하는 인공지능 기반의 실시간 수화 번역 시스템."}
{"patent_id": "10-2023-0129050", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 수화 번역 어플리케이션; 은수화 번역 모델, 수화 생성 모델, STT(Speech to text) 모델, 음성 인식 모델을 포함하는 딥러닝 모델의 학습데이터를 수집하는 수집부;수화 모션 또는 음성을 인식하는 인식부; 및수화 모션이 인식된 경우, 상기 수화 모션을 텍스트 또는 음성으로 변환하고, 음성이 인식된 경우, 상기 음성을수화 모션 또는 텍스트로 변환하는 변환부; 를 포함하는 것을 특징으로 하는 인공지능 기반의 실시간 수화 번역시스템."}
{"patent_id": "10-2023-0129050", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 수화 번역 어플리케이션; 은청각장애인과 일반인의 안면인식을 통해, 표정을 감지하고 감지된 표정 기반으로 감정정보를 추론하는 감정인식부; 를 더 포함하는 것을 특징으로 하는 인공지능 기반의 실시간 수화 번역 시스템."}
{"patent_id": "10-2023-0129050", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서, 상기 수화 번역 어플리케이션; 은DNN(Deep Neural Network), CNN(Convolutional Neural Network) RNN(Recurrent Neural Network) 및BRDNN(Bidirectional Recurrent Deep Neural Network) 중 적어도 하나를 포함하는 딥러닝 뉴럴 네트워크를 학습 데이터로 학습시켜 수화 번역 모델, 수화 생성 모델, 음성인식 모델, 감정 예측 모델을 포함하는 딥러닝 모델을 구현하는 딥러닝부;를 포함하는 것을 특징으로 하는 인공지능 기반의 실시간 수화 번역 시스템."}
{"patent_id": "10-2023-0129050", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 수화 번역 어플리케이션; 은수화 모션을 출력하는 경우, 아바타를 생성하고 아바타에 의해 수행하는 수화 모션을 출력하는 출력부; 를 더포함하는 것을 특징으로 하는 인공지능 기반의 실시간 수화 번역 시스템. 공개특허 10-2025-0045748-3-청구항 6 인공지능 기반의 실시간 수화 번역 방법에 있어서,(A) 서버에서 수화 번역 어플리케이션을 생성하고 배포하는 단계;(B) 청각장애인 단말에서 상기 수화 번역 어플리케이션을 통해, 비장애인인 일반인의 음성을 인식하고 인식된음성을 수화 또는 텍스트로 중 적어도 하나로 출력하는 단계;(C) 일반인 단말에서 상기 수화 번역 어플리케이션을 통해, 청각장애인의 수화 모션을 인식하고, 인식된 수화모션을 텍스트 또는 음성 중 적어도 하나로 출력하는 단계; 를 포함하는 인공지능 기반의 실시간 수화 번역 방법."}
{"patent_id": "10-2023-0129050", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기(B)의 단계; 는 수집부에서 수화 번역 모델, 수화 생성 모델, STT(Speech to text) 모델, 음성 인식 모델을 포함하는 딥러닝 모델의 학습 데이터를 수집하는 단계;인식부에서 수화 모션 또는 음성을 인식하는 단계;변환부에서 수화 모션이 인식된 경우, 상기 수화 모션을 텍스트 또는 음성으로 변환하고, 음성이 인식된 경우,상기 음성을 수화 모션 또는 텍스트로 변환하는 단계; 및출력부에서 수화 모션을 출력하는 경우, 아바타를 생성하고 아바타에 의해 수행하는 수화 모션을 출력하는단계; 를 포함하는 것을 특징으로 하는 인공지능 기반의 실시간 수화 번역 방법."}
{"patent_id": "10-2023-0129050", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "실시예에 따른 인공지능 기반의 실시간 수화 번역 시스템 및 방법은 청각장애인 단말과, 일반인 단말에 수화 번 역 어플리케이션을 설치하고, 수화 번역 어플리케이션을 통해 음성과 수화를 상호 번역하여 상대방에게 전달한다. 예컨대, 청각장애인 단말은 일반인의 음성을 인식하고, STT(Speech to Text)모델, 음성인식 모델 등을 포함하는 딥러닝 모델을 통해, 언어를 텍스트로 변환하여 출력하거나, 음성을 수화 모션으로 변환하여 출력한다. 또한, 일반인 단말은 수화 모션을 인식하여 인식된 수화 모션을 텍스트나 음성으로 변환하고 이를 출력할 수 있 다. 또한, 실시예에서는 청각장애인 단말에서 음성인식 후 이를 수화 모션으로 생성하고, 아바타를 통해 수화 모 션을 출력함으로써, 청각장애인이 수화를 수단으로, 일반인과 대화할 수 있도록 한다."}
{"patent_id": "10-2023-0129050", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공지능 기반 실시간 수화 번역 시스템 및 방법에 관한 것으로 구체적으로, 음성과 수화를 상호 번 역하여 제공하는 인공지능 기반의 실시간 수화 번역 어플리케이션과 시스템에 관한 것이다."}
{"patent_id": "10-2023-0129050", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "본 명세서에서 달리 표시되지 않는 한, 이 섹션에 설명되는 내용들은 이 출원의 청구항들에 대한 종래 기술이 아니며, 이 섹션에 포함된다고 하여 종래 기술이라고 인정되는 것은 아니다. 수화는 청각 장애를 가진 사람들이 의사소통을 위해 사용하는 언어 체계이다. 수화는 주로 손과 팔을 사용하여 동작과 표현을 통해 정보를 전달한다. 수화는 각 나라나 지역에 따라 다른 형태를 가지고 있을 수 있으며, 많은 국가에서는 국립 수화를 정착시켜 수 화를 표준화하고 교육과 의료 등 다양한 분야에서 사용되도록 노력하고 있다. 예를 들어, 미국 수화(ASL; American Sign Language)는 미국에서 주로 사용되며, 한국 수화(KSL; Korean Sign Language)는 한국에서 주로 사용된다. 수화는 언어이기 때문에, 단어, 문법, 표현의 체계 등을 갖고 있다. 수화는 손 모양, 손의 위치, 움 직임, 표정, 몸의 동작 등을 통해 의사소통을 이루고, 수화는 청각 장애인뿐만 아니라 일부 비장애인들도 배우 고 사용하기도 한다. 한편, 국내 및 해외 전세계 청각장애인의 수는 전 세계에서 5억명정도로 추정하고 있고, 청각장애인과 비장애인 간의 의사소통이 절실하다. 하지만, 일반인들은 수화를 모르기 때문에 청각장애인들과 일반인들의 대화가 쉽지 가 않다. 이로 인해, 많은 청각장애인들이 제대로 누려야 할 국민의 기본권리를 충분히 누리지 못하고 있는 실 정이다. 또한, 청각 장애인은 다양한 사건 사고로부터 자유롭지 못하다. 예컨대, 청각장애인은 사건 사고 발생 시 일반인들이 하는 말을 제대로 알아들을 수 없고, 자신의 의사를 일반인에게 전달할 수 없기 때문에, 사건사 고 해결에 큰 어려움을 겪게 되는 문제가 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 1. 한국 특허등록 제10-1915088호 (2018.10.30) (특허문헌 0002) 2. 한국 특허공개 제10-2021-0026006호 (2021.03.10)"}
{"patent_id": "10-2023-0129050", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "실시예에 따른 인공지능 기반의 실시간 수화 번역 시스템 및 방법은 청각장애인 단말과, 일반인 단말에 수화 번 역 어플리케이션을 설치하고, 수화 번역 어플리케이션을 통해 음성과 수화를 상호 번역하여 상대방에게 전달한 다. 실시예에서는 수화 및 언어 딥러닝 모델을 포함하는 번역 인공신경망 모델을 이용한 언어번역 및 음성번역 그리 고 동작번역 시스템을 제공한다. 실시예에서는 번역 모델을 통해 번역을 하고, 그 번역을 텍스트로 생성하고, 생성된 텍스트를 수화 모델이 인식 하여, 각 나라의 수화로 번역하여 표현한다. 비장애인에게는 각나라의 수화를 인식하고, 인식된 수화를 사용자 가 인식할 수 있는 국가의 텍스트로 생성하여 제공한다. 예컨대, 청각장애인 단말은 일반인의 음성을 인식하고, STT(Speech to Text)모델, 음성인식 모델 등을 포함하는 딥러닝 모델을 통해, 언어를 텍스트로 변환하여 출력하거나, 음성을 수화 모션으로 변환하여 출력한다. 또한, 일반인 단말은 수화 모션을 인식하여 인식된 수화 모션을 텍스트나 음성으로 변환하고 이를 출력할 수 있다. 실시예에서는 언어를 인식하여 수화를 생성하여 보여주며 자막은 기본적으로 수화 하단에 위치한다. 실시예에서 자막은 사용자에 따라서 끄고 킬 수 있고, 실시예에서는 인공지능이 해당 자막을 음성으로 출력할 수 있다, 아 바타 또한 스킨기능을 지원하여 사용자들이 기본 아바타 외에 다양한 아바타를 고를 수 있도록 한다. 음성 또한 남성, 여성, 아동 등 다양하게 지원한다. 또한, 실시예에서는 청각장애인 단말에서 음성인식 후 이를 수화 모션으로 생성하고, 아바타를 통해 수화 모션 을 출력함으로써, 청각장애인이 수화를 수단으로, 일반인과 대화할 수 있도록 한다."}
{"patent_id": "10-2023-0129050", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "실시예에 따른 인공지능 기반의 실시간 수화 번역 시스템은 수화 번역 어플리케이션을 생성하고 배포하는 서버; 수화 번역 어플리케이션을 통해, 비장애인인 일반인의 음성을 인식하고 인식된 음성을 수화 또는 텍스트로 중 적어도 하나로 출력하는 청각장애인 단말; 수화 번역 어플리케이션을 통해, 청각장애인의 수화 모션을 인식하고, 인식된 수화 모션을 텍스트 또는 음성 중 적어도 하나로 출력하는 일반인 단말; 을 포함한다. 실시예에서는 텍스트(자막), 음성, 수화 모두가 동시에 출력이 가능하며 각 기능을 사용자의 설정에서 끄고 켤 수 있다. 또한, 실시예에서는 모든 옵션은 글을 읽지 못하는 이들을 위해 음성 및 수화지원 설명이 가능하다. 실시예에서 수화 번역 어플리케이션; 은 수화 번역 모델, 수화 생성 모델, STT(Speech to text) 모델, 음성 인 식 모델을 포함하는 딥러닝 모델의 학습 데이터를 수집하는 수집부; 수화 모션 또는 음성을 인식하는 인식부; 및 수화 모션이 인식된 경우, 상기 수화 모션을 텍스트 또는 음성으로 변환하고, 음성이 인식된 경우, 상기 음 성을 수화 모션 또는 텍스트로 변환하는 변환부; DNN(Deep Neural Network), CNN(Convolutional Neural Network) RNN(Recurrent Neural Network) 및 BRDNN(Bidirectional Recurrent Deep Neural Network) 중 적어도 하나를 포함하는 딥러닝 뉴럴 네트워크를 학습 데이터로 학습시켜 수화 번역 모델, 수화 생성 모델, 음성인식 모델, 감정 예측 모델을 포함하는 딥러닝 모델을 구현하는 딥러닝부;를 포함한다. 실시예에서는 언어 빅데이터 모델을 딥러닝 하여 학습하여 제작한 인공신경망 모델 A를 제공한다. 실시예에 따 른 인공신경망 모델은 99개 언어의 자막인식, 음성인식을 기반으로 하며 영상의 언어도 인식을 하며 원하는 언 어로 번역한 자막을 생성할 수 있다. 또한, 실시예에서는 수화 빅데이터 모델을 딥러닝 하여 학습하여 제작한 인공신경망 모델 B를 제공한다. 실시예 에서 인공신경망 모델 B는 영상 및 신체활동 중 수화에 특화되어 있으며 수화를 인식하여 문자(자막)을 생성하 거나 문자를 인식하여 수화를 생성하며 수화를 생성할 시 스테이블 디퓨전(text to image) 모델 2.0을 사용하여 수화를 생성할 수 있다. 인공신경망 모델 B는 언어를 인식하거나 번역하는 기능이 없으며 인공신경망 모델 A에 서 인식된 언어를 문자로 생성하여 인공신경망 모델 B로 전달하면 인공신경망 모델 B에서 수화로 생성할 수 있 다."}
{"patent_id": "10-2023-0129050", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상에서와 같은 인공지능 기반의 실시간 수화 번역 시스템 및 방법은 청각자애인과 일반인이 컴퓨터나, 스마트 폰을 통해 자유롭게 의사소통 할 수 있도록 하여, 청각장애인의 소통 어려움을 줄일 수 있도록 한다. 또한, 실시예에서는 각 나라의 다른 형식의 수화를 모두 번역하여, 여러 국적, 인종의 청각장애인들과 소통할 수 있도록 하고, 인공지능 기반으로 수화를 번역하여 제공함으로써, 수화 번역 정확성을 향상시킬 수 있다. 또한, 실시예에서는 음성인식 후 이를 수화 모션으로 생성하고, 아바타를 통해 수화 모션을 출력함으로써, 청각 장애인이 수화를 수단으로, 일반인과 쉽게 대화할 수 있도록 한다. 실시예에 따른 인공지능 기반의 실시간 수화 번역 시스템 및 방법은 청각장애인 단말 및 일반인 단말 은 같은 모바일 단말기를 사용하며 안드로이드 및 IOS 모두를 지원한다. 그리고 웹 버전을 지원하여 어플리케이션 설치 없이 인터넷이 연결된 피시 및 모든 기기에서 사용가능 하도록 한다."}
{"patent_id": "10-2023-0129050", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 상기한 효과로 한정되는 것은 아니며, 본 발명의 상세한 설명 또는 특허청구범위에 기재된 발 명의 구성으로부터 추론 가능한 모든 효과를 포함하는 것으로 이해되어야 한다."}
{"patent_id": "10-2023-0129050", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명 에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용 되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이 해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 본 출원에서, \"포함한다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되 어야 한다. 본 명세서에 있어서 '부(部)'란, 하드웨어에 의해 실현되는 유닛(unit), 소프트웨어에 의해 실현되는 유닛, 양 방을 이용하여 실현되는 유닛을 포함한다. 또한, 1개의 유닛이 2개 이상의 하드웨어를 이용하여 실현되어도 되 고, 2개 이상의 유닛이 1개의 하드웨어에 의해 실현되어도 된다. 본 명세서에 있어서 단말, 장치 또는 디바이스가 수행하는 것으로 기술된 동작이나 기능 중 일부는 해당 단말, 장치 또는 디바이스와 연결된 서버에서 대신 수행될 수도 있다. 이와 마찬가지로, 서버가 수행하는 것으로 기술 된 동작이나 기능 중 일부도 해당 서버와 연결된 단말, 장치 또는 디바이스에서 수행될 수도 있다. 이하, 첨부된 도면을 참고하여 본 발명을 상세히 설명하기로 한다. 도 1은 실시예에 따른 인공지능 기반의 실시간 수화 번역 시스템 구성을 나타낸 도면이다. 도 1을 참조하면, 실시예에 따른 인공지능 기반의 실시간 수화 번역 시스템은 서버, 청각장애인 단말 및 일반인 단말을 포함하여 구성될 수 있다. 서버는 수화 번역 어플리케이션을 생성하고 단말로 배포한다. 청각장애인 단말은 수화 번역 어플리케 이션을 설치하고, 수화번역 어플리케이션을 통해 일반인의 음성을 인식하고 인식된 음성을 수화 모션, 텍스트로 중 적어도 하나로 출력한다. 일반인 단말은 수화 번역 어플리케이션을 설치하고, 수화 번역 어플리케이션 을 통해, 청각장애이의 수화 모션을 인식하고, 인식된 수화 모션을 텍스트 또는 음성 중 적어도 하나로 출력한 다. 실시예에서 수화 번역 어플리케이션은 수화 모션 출력 시 아바타를 생성하고, 아바타가 수화 모션을 수행하 도록 한다. 실시예에서 수화 번역 어플리케이션은 아바타를 통해 수화 모션을 출력하여 청각장애인이 수화를 이 용하여, 일반인과 편리하게 의사소통 할 수 있도록 한다. 실시예에서 청각장애인 단말 및 일반인 단말은 네트워크를 통하여 원격지의 서버나 단말에 접속할 수 있는 컴퓨 터로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 네비게이션, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데 스크톱(Desktop), 랩톱(Laptop) 등을 포함할 수 있다. 이때, 적어도 하나의 단말은, 네트워크를 통해 원격지의 서버나 단말에 접속할 수 있는 단말로 구현될 수 있다. 적어도 하나의 단말은, 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, 네비게이션, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말, 스마트 폰(smartphone), 스마트 패드(smartpad), 태블릿 PC(Tablet PC) 등과 같은 모든 종류의 핸드헬드(Handheld) 기 반의 무선 통신 장치를 포함할 수 있다. 실시예에 따른 인공지능 기반의 실시간 수화 번역 시스템 및 방법은 청각장애인 단말과, 일반인 단말에 수화 번 역 어플리케이션을 설치하고, 수화 번역 어플리케이션을 통해 수화를 번역하여 상대방에게 전달한다. 예컨대, 청각장애인 단말은 일반인의 음성을 인식하고, STT(Speech to Text)모델, 음성인식 모델 등을 포함하는 딥러닝 모델을 통해, 언어를 텍스트로 변환하여 출력하거나, 음성을 수화 모션으로 변환하여 출력한다. 또한, 일반인 단말은 수화를 인식하여 인식된 수화를 텍스트나 음성으로 변환하고 이를 출력할 수 있다. 실시예에서는 텍스트(자막), 음성, 수화 모두가 동시에 출력이 가능하며 각 기능을 사용자의 설정에서 끄고 켤 수 있다. 또한, 실시예에서는 모든 옵션은 글을 읽지 못하는 이들을 위해 음성 및 수화지원 설명이 가능하다. 실시예에서는 언어 빅데이터 모델을 딥러닝 하여 학습하여 제작한 인공신경망 모델 A를 제공한다. 실시예에 따 른 인공신경망 모델은 99개 언어의 자막인식, 음성인식을 기반으로 하며 영상의 언어도 인식을 하며 원하는 언 어로 번역한 자막을 생성할 수 있다. 또한, 실시예에서는 수화 빅데이터 모델을 딥러닝 하여 학습하여 제작한 인공신경망 모델 B를 제공한다. 실시예 에서 인공신경망 모델 B는 영상 및 신체활동 중 수화에 특화되어 있으며 수화를 인식하여 문자(자막)을 생성하거나 문자를 인식하여 수화를 생성하며 수화를 생성할 시 스테이블 디퓨전(text to image) 모델 2.0을 사용하여 수화를 생성할 수 있다. 인공신경망 모델 B는 언어를 인식하거나 번역하는 기능이 없으며 인공신경망 모델 A에 서 인식된 언어를 문자로 생성하여 인공신경망 모델 B로 전달하면 인공신경망 모델 B에서 수화로 생성할 수 있 다. 실시예에 따른 인공지능 기반의 실시간 수화 번역 시스템 및 방법은 청각장애인 단말 및 일반인 단말 은 같 은 모바일 단말기를 사용하며 안드로이드 및 IOS 모두를 지원한다. 그리고 웹 버전을 지원하여 어플리케이션 설 치 없이 인터넷이 연결된 피시 및 모든 기기에서 사용가능 하도록 한다. 도 2는 실시예에 따른 수화 번역 어플리케이션의 데이터 처리 구성을 나타낸 도면이다. 도 2에 도시된 데이터 처리 구성은, 수화 번역 어플리케이션뿐만 아니라 수화 번역 어플리케이션을 생성하는 서버의 데이터 처리 구성 으로 해석될 수 있다. 도 2를 참조하면, 실시예에 따른 수화 번역 어플리케이션은 수집부, 전처리부, 딥러닝부, 인식 부, 변환부, 감정인식부, 출력부 및 피드백부를 포함하여 구성될 수 있다. 본 명세서 에서 사용되는 '부' 라는 용어는 용어가 사용된 문맥에 따라서, 소프트웨어, 하드웨어 또는 그 조합을 포함할 수 있는 것으로 해석되어야 한다. 예를 들어, 소프트웨어는 기계어, 펌웨어(firmware), 임베디드코드(embedded code), 및 애플리케이션 소프트웨어일 수 있다. 또 다른 예로, 하드웨어는 회로, 프로세서, 컴퓨터, 집적 회로, 집적 회로 코어, 센서, 멤스(MEMS; Micro-Electro-Mechanical System), 수동 디바이스, 또는 그 조합일 수 있 다. 수집부는 딥러닝 모델의 학습 데이터를 수집한다. 실시예에서 수집부는 수화 번역 모델, 수화 생성 모델, STT(Speech to text) 모델, 음성 인식 모델을 포함하는 딥러닝 모델의 학습 데이터를 수집한다. 수화 번 역 모델의 학습 데이터는 주로 수화 모션과 해당 수화 모션의 텍스트 번역 쌍으로 구성될 수 있다. 수화 번역 모델의 학습 데이터는 수화 사용자와 의사 소통하는 사람들이 수화 동작을 통해 표현한 내용과 해당 내용을 텍스트로 번역한 데이터이다. 또한, 여러 나라의 수화 모션과 텍스트 번역 쌍을 포함할 수 있다. 실시예에서 수집부는 미국, 일본, 중국 등 여러 나라의 수화 데이터를 수집하고 이를 수화 번역모델 및 수 화 생성모델의 학습 데이터로 이용할 수 있도록 한다. 이를 통해 실시예에서는 국가에 따라 다르게 표현되는 수 화를 모두 번역할 수 있다. 음성인식 모델의 학습 데이터는 음성 신호와 해당 음성의 텍스트 변환 쌍으로 구성될 수 있다. 음성인식 모델의 학습 데이터는 사람들이 음성으로 발화한 내용과 해당 내용을 텍스트로 변환한 데이터이다. STT(Speech to text) 모델의 학습 데이터는 음성-텍스트 변환 쌍, 말뭉치(corpus), 다국어 데이터, 환경소음 데이터 등을 포함 할 수 있다. 수화 생성 모델의 학습 데이터는 수화 동작과 해당 동작에 대한 텍스트 설명 또는 수화와 수화 간의 매핑으로 구성될 수 있다. 수화 생성 모델의 학습 데이터는 수화 사용자가 수화 동작을 수행하고 해당 동작에 대한 설명 을 텍스트로 기록한 데이터이다. 전처리부는 수집된 인공지능 학습데이터 중 편향성이나 차별성을 가진 데이터를 제거하기 위해, 수집된 학 습데이터를 전처리한다. 실시예에서 전처리부는 수집된 데이터를 전처리하여 인공지능 모델 학습에 적합한 형태로 가공한다. 예컨대, 전처리부는 노이즈 제거, 이상치 제거, 결측치 처리 등의 과정을 수행할 수 있 다. 또한, 전처리부는 데이터 전처리를 통해 데이터를 정규화하거나, 이상치를 제거하거나, 데이터의 배율 조정 등을 수행하여 모델이 불필요한 패턴을 학습하는 것을 방지할 수 있다. 실시예에서 전처리부는 데이 터 증강을 수행한다. 데이터 증강은 기존 데이터를 변형하여 학습 데이터의 다양성을 높이는 것이다. 실시예에 서 전처리부는 음성 데이터의 경우, 노이즈 추가, 속도 변화, 피치 변경 등의 방법을 사용하여 데이터 증 강을 수행할 수 있다. 또한, 전처리부는 수화 데이터에 동작 반전, 시간 왜곡 등을 적용하여 모델의 일반 화 능력을 향상시킬 수 있다. 또한, 전처리부는 데이터 분할하는 전처리 과정을 수행할 수 있다. 실시예에서 또한, 전처리부는 전 체 데이터를 학습, 검증, 테스트 세트로 분할한다. 실시예에서, 전처리부는 학습 데이터의 6080%, 검증 데 이터의 1020%, 테스트 데이터의 10~20% 비율로 분할합니다. 실시예에서는 분할된 데이터를 사용하여 모델을 학 습하고 성능을 평가할 수 있다. 딥러닝부는 딥러닝 뉴럴 네트워크를 학습 데이터로 학습시켜 딥러닝 모델을 구현한다. 실시예에서 딥러닝 모델은 수화 번역 모델, 수화 생성 모델, 음성인식 모델, 감정 예측 모델, 음성인식 모델 등을 포함할 수 있다. 수화 번역 모델은 청각장애인의 수화 모션을 인식하고, 이를 일반인이 이해할 수 있는 텍스트 또는 음성으로 변환하는 딥러닝 모델이다. 수화 생성 모델은 일반인의 음성 및 텍스트를 인식하고, 이를 수화 모션으로 변환하는 딥러닝 모델이다. 감정 예측 모델은 수화할 때의 청각장애인의 표정, 모션강도를 분석하여 청각장애인의 감정 상태를 예측하는 딥러닝 모델이다. 실시예에서 모션 강도는 청각장애인의 수화 동작에서의 움직임의 세기 또는 강도이다. 이는 손의 움직임, 몸의 동작, 표정 등의 강도를 나타낸다. 모션 강도는 청각장애인의 감정 상태를 이해하고 예측할 수 있도록 한다. 예를 들어, 수화 동작 중에 손의 움직임이 강하고 빠르다면 분노나 화남과 같 은 강한 감정을 나타낼 수 있고, 반대로, 부드럽고 느린 움직임은 평온 또는 슬픔과 같은 다른 감정을 나타낼 수 있다. 또한, 실시예에서는 딥러닝 기반의 감정 예측 모델을 통해, 수화 동작에서의 청각장애인의 표정과 모 션 강도를 분석하여, 이를 기반으로 청각장애인의 감정 상태를 예측할 수 있도록 한다. 음성인식 모델은 일반인 의 음성을 인식하여 텍스트로 변환하는 모델로, STT 모델을 포함할 수 있다. 실시예에서 딥러닝 뉴럴 네트워크는 DNN(Deep Neural Network), CNN(Convolutional Neural Network) RNN(Recurrent Neural Network) 및 BRDNN(Bidirectional Recurrent Deep Neural Network) 중 적어도 하나를 포함하고, 이에 한정하지 않는다. 인식부는 수화 모션 또는 음성을 인식한다. 실시예에서 인식부는 청각장애인 단말에서는 수화 모션을 인식하고, 일반인 단말에서는 음성을 인식한다. 실시예에서 인식부는 수화 번역 모델 및 음성인식 모델을 통해, 수화 모션과 음성을 인식할 수 있다. 실시예에서 인식부는 카메라 또는 센서를 통해, 수화 사용자가 수화 모션을 수행하면 수화 모션 정보를 생 성한다. 실시예에서 수화 모션은 손 모양, 손의 위치, 움직임, 표정, 몸의 동작 등을 사용하여 표현될 수 있다. 인식부는 수화 모션 정보를 수화 번역 모델로 전달하여, 수화 동작을 인식한다. 실시예에서 수화 번역 모 델은 입력된 수화 모션을 분석하고, 수화 동작에 해당하는 의미를 추론하거나 인식한다. 인식부는 딥스피치, LAS, Wave2Vec 등의 음성인식 모델을 통해, 일반인의 발화 음성을 인식할 수 있다. 딥 스피치(Deep Speech)는 합성곱 신경망(CNN)과 순환 신경망(RNN)을 조합한 구조를 사용한다. 딥스피치 모델은 오 디오 데이터를 직접 처리하며, 특성 추출 및 음성 인식 과정을 통합하여 효율성을 높인다. LAS(Listen, Attend, and Spell)는 RNN 기반의 인코더-디코더 구조를 사용하여 음성 데이터를 텍스트로 변환한다. 인코더는 음성 데 이터의 특성을 추출하고, 디코더는 어텐션 메커니즘을 활용해 텍스트를 생성한다. Wave2Vec는 트랜스포머 (Transformer) 기반의 구조를 사용하여 음성을 인식한다. Wave2Vec 모델은 원시 오디오 데이터를 처리하며, 라 벨이 없는 데이터를 사용한 비지도 학습 방식을 적용하여 인식 성능을 향상시킨다. 변환부는 수화 모션이 인식된 경우, 수화 모션을 텍스트 또는 음성으로 변환하고, 일반인의 음성이 인식된 경우, 음성 또는 텍스트를 수화 모션 또는 텍스트로 변환한다. 실시예에서 변환부는 수화 모션이 인식된 경우, 변환부는 해당 수화 모션을 텍스트 또는 음성으로 변 환하는 작업을 수행한다. 인식된 수화 모션에 해당하는 의미를 추론하여 텍스트로 표현하거나, 음성으로 변환한 다. 실시예에서 변환부는 일반인의 음성이 인식된 경우, 음성을 수화 모션 또는 텍스트로 변환하는 작업을 수 행한다. 이를 위해 변환부는 인식된 사용자 음성에 해당하는 의미를 추론하여 수화 모션으로 표현하거나, 텍스트로 변환한다. 실시예에서 변환부는 수화 생성 모델을 통해, 인식된 사용자 음성에 해당하는 수화 모 션을 생성할 수 있다. 실시예에서 수화 생성 모델은 Seq2Seq, 트랜스포머, GAN 등을 포함할 수 있다. Seq2Seq는 RNN 기반의 인코더-디코더 구조를 사용하여 텍스트 데이터를 수화 데이터로 변환한다. 인코더는 입력 텍스트를 의미 단위로 분석하고, 디코더는 이를 바탕으로 수화 단어 및 구를 생성한다. 트랜스포머 (Transformer)는 자연어 처리에서 뛰어난 성능을 발휘하는 모델로, 인코더-디코더 구조와 어텐션 메커니즘을 사 용하여 수화 모션을 생성한다. 트랜스포머는 텍스트를 수화로 변환하는 과정에서도 활용할 수 있으며, 병렬 처 리를 통해 학습 및 추론 속도를 향상시킬 수 있다. GAN(Generative Adversarial Network)는 수화 동작 생성을 위해 사용되는 생성적 적대 신경망이다. GAN은 생성자와 판별자 두 개의 신경망이 서로 경쟁하는 방식으로 학습 되며, 이를 통해 실제 수화와 유사한 동작을 생성한다. 감정인식부는 청각장애인의 안면인식을 통해, 표정을 감지하고 감지된 표정을 기반으로 감정정보를 추론한 다. 실시예에서 감정인식부는 감정인식 모델을 통해, 청각장애인의 감정 정보를 추론할 수 있다. 또한, 실 시예에서 감정인식부는 청각장애인의 수화 모션의 모션강도를 통해, 청각장애인의 감정 정보를 추론할 수 있다. 이때, 감정인식부는 감정 예측 모델을 이용할 수 있다. 실시예에서 감정인식부는 청각장애인의 안면을 인식하는 기술을 사용하여 수화 모니터링 영상에 포함된 안 면 이미지를 처리한다. 안면인식 기술은 컴퓨터 비전 기술과 딥러닝 알고리즘을 활용하여 안면 이미지나 수화 모니터링 영상에서 얼굴을 식별하고 주요 특징을 추출한다. 이후, 감정인식부는 안면인식 결과로부터 표정 을 감지한다. 감정인식부는 안면 이미지나 수화 모니터링 영상에서 표정을 분석하여 특정 표정, 예를 들면 웃음, 찡그림, 분노 등을 식별한다. 이를 위해 표정을 특징 집합으로 변환하거나, 미리 정의된 표정 패턴과 비 교한다. 이후, 감정인식부는 표정 감지 결과를 기반으로 감정정보를 추론한다. 감정인식부는 표정과 감정 간의 매핑을 학습한 모델이나 알고리즘을 사용하여, 특정 표정이 특정 감정과 연관되는지 분석한다. 예를 들어, 웃는 표정은 기쁨으로 추론되거나, 찡그린 표정은 분노로 추론될 수 있다. 실시예에서 수화 모니터링 영 상은 청각장애인 단말에 설치된 카메라에서 수화 번역 어플리케이션 이용 시 생성될 수 있다. 또한, 실시예에서 감정인식부는 일반인 단말에서 수집된 일반인의 표정을 분석하여 대화 중 일반인의 감정 정보를 추론하고, 이를 청각장애인에게 전달할 수 있다. 이를 위해, 감정인식부는 일반인의 안면을 인식하 는 기술을 사용하여 대화 모니터링 영상에 포함된 안면 이미지를 처리한다. 안면인식 기술은 컴퓨터 비전 기술 과 딥러닝 알고리즘을 활용하여 대화 모니터링 영상에서 얼굴을 식별하고 주요 특징을 추출한다. 이후, 감정인 식부는 안면인식 결과로부터 표정을 감지한다. 감정인식부는 안면 이미지나 대화 모니터링 영상에서 표정을 분석하여 특정 표정, 예를 들면 웃음, 찡그림, 분노 등을 식별한다. 이를 위해 표정을 특징 집합으로 변 환하거나, 미리 정의된 표정 패턴과 비교한다. 이후, 감정인식부는 표정 감지 결과를 기반으로 감정정보를 추론한다. 감정인식부는 표정과 감정 간의 매핑을 학습한 모델이나 알고리즘을 사용하여, 특정 표정이 특 정 감정과 연관되는지 분석한다. 예를 들어, 웃는 표정은 기쁨으로 추론되거나, 찡그린 표정은 분노로 추론될 수 있다. 실시예에서 대화 모니터링 영상은 일반인이 청각장애인과 대화하는 경우, 일반인의 스마트 단말에 설 치된 카메라를 통해, 생성될 수 있다. 이후, 감정인식부는 추론된 감정정보를 청각장애인과 일반인 사용자에게 제공한다. 이는 시각적인 방식으 로 표현될 수 있으며, 예를 들어 청각장애인 단말과 일반인 단말에 텍스트 또는 그래픽 인터페이스를 통해 표시 될 수 있다. 또한, 음성 또는 진동 등의 다른 방식을 활용하여 청각장애인과 일반인에게 감정 정보를 전달할 수 있다. 출력부는 수화 모션을 출력하는 경우, 아바타를 생성하고 아바타에 의해 수행하는 수화 모션을 출력한다. 실시예에서 출력부는 수화 번역 모델이 생성한 수화 모션을 기반으로 수화 동작을 출력한다. 이는 손 모양, 손의 위치, 움직임, 표정, 몸의 동작 등으로 표현되는 수화 모션이다. 출력부는 수화 모션을 디지털 형태로 표현하고, 필요한 형식으로 변환한다. 이후, 출력부는 아바타를 생성한다. 실시예에서 출력부(17 0)는 수화 모션을 기반으로 아바타를 생성한다. 아바타는 가상의 캐릭터 또는 로봇으로서 수화 동작을 수행할 수 있는 형태이다. 실시예에서 출력부에서 수행하는 아바타 생성 과정은 컴퓨터 그래픽스 기술을 사용하여 수화 동작에 대한 시각적인 표현을 생성한다. 아바타가 생성되면, 출력부는 수화 모션을 아바타에 적용하 여 실제로 수화 동작을 수행하도록 한다. 이때, 출력부는 아바타의 손과 몸의 움직임, 표정 등을 조작하여 수화 동작을 시각적으로 표현한다. 실시예에서 출력부는 아바타를 통해, 수화 모션을 출력하여 수화 동작을 시각적으로 출력함으로써, 청각 장애인이 아바타를 통해 수화 동작을 보다 쉽게 이해할 수 있도록 하고, 일반인과 수화를 통해 의사 소통할 수 있도록 한다. . 피드백부는 학습된 인공신경망 모델 및 딥러닝 모델을 평가한다. 실시예에서 피드백부는 정확도 (Accuracy), 정밀도 (Precision) 및 재현율 (Recall) 중 적어도 하나를 통해, 인공신경망 모델을 평가할 수 있 다. 정확도는 인공 신경망 모델이 예측한 결과가 실제 결과와 얼마나 일치하는지를 측정하는 지표이다. 정밀도 는 양성으로 예측한 결과 중 실제 양성인 비율을 측정하는 지표이다. 재현율은 실제 양성 중에서 모델이 양성으 로 예측한 비율을 측정하는 지표이다. 실시예에서 피드백부는 인공신경망 모델의 정확도, 정밀도 및 재현 율을 산출하고, 산출된 지표 중 적어도 하나를 기반으로 인공신경망 모델을 평가할 수 있다. 실시예에서 피드백부는 인공신경망 모델의 정확도를 평가 데이터셋을 사용하여 측정할 수 있다. 평가 데이 터셋은 모델이 학습에 사용하지 않은 데이터로 구성되어 있으며, 모델의 성능을 객관적으로 평가하는 데 사용된 다. 실시예에서 피드백부는 평가 데이터셋을 사용하여 인공신경망 모델을 실행하고, 각 입력 데이터에 대 한 인공신경망 모델의 예측 값과 해당 데이터의 실제 정답 값을 비교한다. 이후, 비교 결과를 통해, 모델이 얼 마나 정확하게 예측하는지를 측정할 수 있다. 예컨대, 피드백부에서 정확도는 전체 데이터 중에서 모델이 맞게 예측한 데이터의 비율로 계산될 수 있다. 또한, 피드백부는 정밀도와 재현율의 조화 평균으로 계산되는 지표인 정밀도와 재현율의 균형을 나타내는 지표인 F1 스코어(F1 Score)를 산출하고, 산출된 F1 스코어를 기반으로 인공신경망 모델을 평가하고, 분류 모델 의 성능을 그래프로 시각화한 지표인 AUC-ROC 곡선을 생성하고, 생성된 AUC-ROC 곡선을 기반으로 인공신경망 모 델을 평가할 수 있다. 실시예에서 피드백부는 ROC 곡선 아래 면적 (AUC)이 1에 가까울수록 모델의 성능이 좋은 것으로 평가할 수 있다. 또한, 피드백부는 인공신경망 모델의 해석 가능성을 평가할 수 있다. 실시예에서 피드백부는 SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations) 방법을 통해, 인공신경망 모델의 해석 가능성을 평가한다. SHAP (SHapley Additive exPlanations)는 모델이 예측한 결과에 대한 해석을 제공하는 라이브러리로서, 피드백부는 라이브러리에서 SHAP 값을 추출한다. 실시예에서 피드 백부는 SHAP값 추출을 통해, 모델에 입력된 특성정보가 모델 예측에 얼마나 영향을 미쳤는지를 예측할 수 있다. LIME (Local Interpretable Model-agnostic Explanations) 방법은 개별 샘플에 대한 모델의 예측을 설명하는 방법이다. 실시예에서 피드백부는 LIME 방법을 통해, 샘플을 해석 가능한 모델로 근사하여 각 특성정보의 중요도를 계산한다. 또한, 피드백부는 모델의 내부 가중치와 편향 값을 분석하여 각 특성 변수의 영향력을 추정할 수 있다. 피드백부는 인공신경망 모델의 공정성이 낮거나 차별성을 보이는 경우, 개선 작업을 수행한다. 실시예에서 피드백부는 특정 집단의 데이터가 일정수준 이상 부족한 경우 상기 특정 집단을 대표하는 데이터를 추가로 수집하고, 데이터 전처리 과정을 수행한다. 실시예에서 피드백부는 데이터 정규화, 이상치 제거, 데이터의 배율 조정을 포함하는 데이터 전처리 과정을 수행하여 모델이 불필요한 패턴을 학습하는 것을 방지한다. 또한, 실시예에서 피드백부는 모델 학습 알고리즘에 특정 조건을 추가하여 차별성을 방지하거나, 공정성을 보장 할 수 있다. 실시예에서 피드백부는 공정성 보장을 위해, 혼돈행렬(Confusion Matrix) 분석을 통해, 모델의 예측 결과 를 실제 결과와 비교하여 모델의 성능을 평가한다. 혼돈 행렬(confusion matrix)은 지도 학습에서 모델의 분류 성능을 평가하는 행렬이다. 혼돈 행렬은 모델이 예측한 결과와 실제 결과를 비교하여 분류 결과를 표시한다. 실 시예에서 피드백부는 혼돈행렬 분석을 통해, 각각의 클래스에 대한 정확도와 오분류율을 계산하여 모델의 성능을 평가할 수 있다. 또한, 실시예에서 피드백부는 학습데이터에 대한 시각화 분석을 통해 데이터의 분포를 확인할 수 있도록 한다. 예를 들어, 이미지 데이터의 경우 각 클래스에 대한 이미지 샘플을 시각화 하여 데이터의 다양성과 공정 성을 평가할 수 있다. 또한, 피드백부는 인공신경망 모델의 편향(Bias) 검증을 수행한다. 실시예에서 피드백부는 학습정보 에 대한 편향(Bias) 검증을 통해, 모델이 특정 클래스나 속성에 대해 편향되어 있는지 여부를 확인한다. 이를 위해 피드백부는 각 클래스에 대한 샘플의 수를 비교하거나, 각 클래스에 대한 분류 성능을 평가한다. 또한, 피드백부는 공정성(Fairness) 검증 및 평가 지표 계산을 통해, 학습데이터의 공정성과 다양항을 검 증하고 인공신경망 모델을 개선할 수 있도록 한다. 실시예에서 공정성 검증은 학습정보에 대해 인공신경망 모델 이 성별, 인종, 연령 등의 특정속성에 대해 차별성을 보이는지 여부를 확인하는 것이다. 실시예에서 피드백부 는 각 속성에 대한 샘플의 수를 비교하거나, 각 속성에 대한 분류 성능을 평가하여, 특정 속성에 대한 차 별성 여부를 확인할 수 있다. 또한, 피드백부는 인공신경망 모델의 성능을 평가하기 위한 다양한 지표를 계산한다. 예를 들어, 정확도, 정밀도, 재현율, F1-score 등의 지표를 계산하여 모델의 성능을 평가할 수 있다. 이때, 각 클래스에 대한 지표 를 계산하여 모델의 공정성과 다양성을 평가할 수 있다. 또한, 피드백부는 인공신경망 모델이 실제 환경에서 사용되면서 발생하는 문제점에 대한 피드백을 수집하 고, 수집된 피드백을 인공신경망 모델에 반영하여 인공신경망 모델을 지속적으로 개선한다. 이하에서는 인공지능 기반의 실시간 수화 번역 방법에 대해서 차례로 설명한다. 실시예에 따른 인공지능 기반의 실시간 수화 번역 방법의 작용(기능)은 인공지능 기반의 실시간 수화 번역 시스템의 기능과 본질적으로 같은 것 이므로 도 1 및 도 2와 중복되는 설명은 생략하도록 한다. 도 3은 실시예에 따른 인공지능 기반의 수화 번역 시스템의 신호 흐름도이다. 도 3을 참조하면, S100 단계에서는 서버에서 수화 번역 어플리케이션을 생성하고 배포한다. S200 단계에서는 청 각장애인 단말에서 수화 번역 어플리케이션을 통해, 비장애인인 일반인의 음성을 인식하고 인식된 음성을 수화 또는 텍스트로 중 적어도 하나로 출력한다. S300 단계에서는 일반인 단말에서 수화 번역 어플리케이션을 통해, 청각장애인의 수화 모션을 인식하고, 인식된 수화 모션을 텍스트 또는 음성 중 적어도 하나로 출력한다. 도 4는 실시예에 따른 수화 번역 어플리케이션의 데이터 처리 흐름을 나타낸 도면이다. 도 4를 참조하면, S410 단계에서는 수집부에서 수화 번역 모델, 수화 생성 모델, STT(Speech to text) 모델, 음 성 인식 모델을 포함하는 딥러닝 모델의 학습 데이터를 수집한다. S420 단계에서는 인식부에서 수화 모션 또는 음성을 인식한다. S430 단계에서는 변환부에서 수화 모션이 인식된 경우, 상기 수화 모션을 텍스트 또는 음성으 로 변환하고, 음성이 인식된 경우, 상기 음성을 수화 모션 또는 텍스트로 변환한다. S440 단계에서는 출력부에 서 수화 모션을 출력하는 경우, 아바타를 생성하고 아바타에 의해 수행하는 수화 모션을 출력한다. 이상에서와 같은 인공지능 기반의 실시간 수화 번역 시스템 및 방법은 청각자애인과 일반인이 컴퓨터나, 스마트 폰을 통해 자유롭게 의사소통 할 수 있도록 하여, 청각장애인의 소통 어려움을 줄일 수 있도록 한다. 또한, 실시예에서는 각 나라의 다른 형식의 수화를 모두 번역하여, 여러 국적, 인종의 청각장애인들과 소통할 수 있도록 하고, 인공지능 기반으로 수화를 번역하여 제공함으로써, 수화 번역 정확성을 향상시킬 수 있다. 또한, 실시예에서는 음성인식 후 이를 수화 모션으로 생성하고, 아바타를 통해 수화 모션을 출력함으로써, 청각 장애인이 수화를 수단으로, 일반인과 대화할 수 있도록 한다."}
{"patent_id": "10-2023-0129050", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "개시된 내용은 예시에 불과하며, 특허청구범위에서 청구하는 청구의 요지를 벗어나지 않고 당해 기술분야에서 통상의 지식을 가진 자에 의하여 다양하게 변경 실시될 수 있으므로, 개시된 내용의 보호범위는 상술한 특정의 실시예에 한정되지 않는다."}
{"patent_id": "10-2023-0129050", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 실시예에 따른 인공지능 기반의 실시간 수화 번역 시스템 구성을 나타낸 도면 도 2는 실시예에 따른 수화 번역 어플리케이션의 데이터 처리 구성을 나타낸 도면 도 3은 실시예에 따른 인공지능 기반의 수화 번역 시스템의 신호 흐름도 도 4는 실시예에 따른 수화 번역 어플리케이션의 데이터 처리 흐름을 나타낸 도면"}
