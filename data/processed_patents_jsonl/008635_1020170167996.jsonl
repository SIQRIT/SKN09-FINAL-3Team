{"patent_id": "10-2017-0167996", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0068021", "출원번호": "10-2017-0167996", "발명의 명칭": "감정 및 윤리 상태 모니터링 기반 사용자 적응형 대화 장치 및 이를 위한 방법", "출원인": "전자부품연구원", "발명자": "신사임"}}
{"patent_id": "10-2017-0167996", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자 적응형 대화 장치에 있어서,대화 상황의 사용자의 발언이 수집된 음성을 자연어 스크립트로 변환하여 대화 정보를 생성하는 음성인식부;상기 대화 상황으로부터 수집된 영상으로부터 상황을 인식하여 상기 발언이 이루어진 상황을 나타내는 상황 정보를 생성하고, 상기 발언의 의도를 나타내는 의도 정보를 생성하는 인공시각부;상기 상황 정보 및 상기 의도 정보를 자연어 스크립트로 변환하는 상황설명정보부;상기 대화 정보, 상기 의도 정보 및 상기 상황 정보에 대해 자연어 분석을 수행하는 자연언어이해부; 및상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 종합하여 상기 대화 정보의 의미를 상기 의도 정보 및 상기 상황 정보에 따라 해석한 현재 대화 상태 정보를 도출하고, 상기 현재 대화 상태 정보에 대응하는 복수의 응답을 포함하는 다음 대화 상태 정보를 결정하는 대화상태추적부;를 포함하는 것을 특징으로 하는 사용자 적응형 대화 장치."}
{"patent_id": "10-2017-0167996", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 기초로 사용자의 감정 상태를 나타내는 감정 상태 정보를생성하는 감정추적부; 및상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 기초로 상기 대화의 윤리성을 나타내는 윤리 상태 정보를생성하는 윤리성분석부;를 더 포함하는 것을 특징으로 하는 사용자 적응형 대화 장치."}
{"patent_id": "10-2017-0167996", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 감정 상태 정보 및 상기 윤리 상태 정보 중 적어도 하나에 따라 상기 복수의 응답 중 어느 하나의 응답을선택하여 선택된 응답을 포함하는 최종 다음 대화 상태 정보를 결정하는 멀티모달대화관리부;를 더 포함하는 것을 특징으로 하는 사용자 적응형 대화 장치."}
{"patent_id": "10-2017-0167996", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 최종 다음 대화 상태 정보를 자연어 스크립트인 출력 대화 스크립트로 변환하는 자연언어생성부; 및 상기 출력 대화 스크립트에 상기 감정 상태 정보, 의도 정보 및 상황 정보 중 적어도 하나에 부합하는 억양과톤을 부여한 음성 신호를 합성하는 적응형음성합성부;를 더 포함하는 것을 특징으로 하는 사용자 적응형 대화 장치."}
{"patent_id": "10-2017-0167996", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "대화 상황의 사용자의 발언이 수집된 음성을 자연어 스크립트로 변환하여 대화 정보를 생성하는 단계;상기 대화 상황으로부터 수집된 영상으로부터 상황을 인식하여 상기 발언이 이루어진 상황을 나타내는 상황 정보를 생성하고, 상기 발언의 의도를 나타내는 의도 정보를 생성하는 단계;상기 상황 정보 및 상기 의도 정보를 자연어 스크립트로 변환하는 단계;공개특허 10-2019-0068021-3-상기 대화 정보, 상기 의도 정보 및 상기 상황 정보에 대해 자연어 분석을 수행하는 단계;상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 종합하여 상기 대화 정보의 의미를 상기 의도 정보 및 상기 상황 정보에 따라 해석한 현재 대화 상태 정보를 도출하는 단계; 및상기 현재 대화 상태 정보에 대응하는 복수의 응답을 포함하는 다음 대화 상태 정보를 결정하는 단계;를 포함하는 것을 특징으로 하는 사용자 적응형 대화 방법."}
{"patent_id": "10-2017-0167996", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 기초로 사용자의 감정 상태를 나타내는 감정 상태 정보를생성하는 단계; 및상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 기초로 상기 대화의 윤리성을 나타내는 윤리 상태 정보를생성하는 단계;를 더 포함하는 것을 특징으로 하는 사용자 적응형 대화 방법."}
{"patent_id": "10-2017-0167996", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 감정 상태 정보 및 상기 윤리 상태 정보 중 적어도 하나에 따라 상기 복수의 응답 중 어느 하나의 응답을선택하여 선택된 응답을 포함하는 최종 다음 대화 상태 정보를 결정하는 단계;를 더 포함하는 것을 특징으로 하는 사용자 적응형 대화 방법."}
{"patent_id": "10-2017-0167996", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 최종 다음 대화 상태 정보를 자연어 스크립트인 출력 대화 스크립트로 변환하는 단계; 및상기 출력 대화 스크립트에 상기 감정 상태 정보, 의도 정보 및 상황 정보 중 적어도 하나에 부합하는 억양과톤을 부여한 음성 신호를 생성하는 단계;를 더 포함하는 것을 특징으로 하는 사용자 적응형 대화 방법."}
{"patent_id": "10-2017-0167996", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 감정 및 윤리 상태 모니터링 기반 사용자 적응형 대화 장치 및 이를 위한 방법에 관한 것이다. 본 발 명은 대화 상황의 사용자의 발언이 수집된 음성을 자연어 스크립트로 변환하여 대화 정보를 생성하는 음성인식부 와, 대화 상황으로부터 수집된 영상으로부터 상황을 인식하여 발언이 이루어진 상황을 나타내는 상황 정보를 생 (뒷면에 계속)"}
{"patent_id": "10-2017-0167996", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 대화 장치에 관련된 기술에 관한 것으로, 보다 상세하게는 감정 및 윤리 상태 모니터링 기반 사용자 적응형 대화 장치 및 이를 위한 방법에 관한 것이다."}
{"patent_id": "10-2017-0167996", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "지능형 에이전트는 복잡하고 변하는 환경에서 어느 정도 자율적으로 목적 달성을 시도하는 시스템으로, 자율적 응 에이전트, 소프트웨어 에이전트, 인터페이스 에이전트 등의 이름으로 전산학, 특히 인공지능분야에서 활발히 연구되어 왔다. 최근 네트워크의 급속한 발달과 고성능 개인용 컴퓨터의 보급 등으로, 한 대의 컴퓨터에서만 작업을 수행하는 것이 아니라 필요에 따라 네트워크상의 여러시스템을 옮겨 다니며 동작하는 이동성이나 사용자나 환경에 적응하 여 동작 성능이 점증적으로 향상되는 적응성 등도 지능형 에이전트 연구의 중요한 부분을 차지하고 있다. 수동으로 지능형 에이전트를 설계하는 방식은 설계자가 응용 도메인에 대한 충분한 지식을 가져야 하고 시스템 의 성능이 초기에 고정된다는 어려움이 있다. 이를 극복하기 위해 경험을 통해서 자동적으로 컴퓨터 알고리즘을 발전시키는 것에 관한 연구인 기계학습을 사용하여 지능형 에이전트를 설계하는 방식이 시도되고 있다. 사람들과 대화하면서 배워가는 기계 학습 기능을 탑재한 대화 시스템에서는 대화로 사용자와 의사소통하는 지능 형 에이전트의 문장 생성 능력을 지속적으로 향상시켜 사용자에게 다양한 유형의 답변을 제공할 수 있다. 초기 의 에이전트는 \"당신은 서울을 떠납니다. 그리고 당신은 뉴욕으로 갑니다. 언제 떠나나요?\"와 같이 어색한 표현 을 사용하지만, 기계학습 기법으로 언어 능력을 향상시켜 \"당신은 언제 서울에서 뉴욕으로 가나요?\"와 같은 세 련된 문장을 사용하게 된다. 하지만, 사람들과 대화하면서 배워가는 기계 학습 기능을 탑재한 대화 시스템은 대화 상대의 기분을 상하게 하 거나, 일반 윤리와 상식에 맞지 않는 답변을 내놓는 경우가 있다. 따라서 이에 대한 해결책이 요구된다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국공개특허 제2017-0111875호 (2017.10.12. 공개)"}
{"patent_id": "10-2017-0167996", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 사용자와의 상호 반응적 대화 시스템에서, 대화 진행 상황의 다양한 분석으로 사용자의 감정 과 윤리 상태를 모니터링하여, 이 결과에 적합한 대화를 생성하는 대화 장치 및 방법을 제공함에 있다."}
{"patent_id": "10-2017-0167996", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 바와 같은 목적을 달성하기 위한 본 발명의 바람직한 실시예에 따른 사용자 적응형 대화 장치는 대화 상 황의 사용자의 발언이 수집된 음성을 자연어 스크립트로 변환하여 대화 정보를 생성하는 음성인식부와, 상기 대 화 상황으로부터 수집된 영상으로부터 상황을 인식하여 상기 발언이 이루어진 상황을 나타내는 상황 정보를 생 성하고, 상기 발언의 의도를 나타내는 의도 정보를 생성하는 인공시각부와, 상기 상황 정보 및 상기 의도 정보 를 자연어 스크립트로 변환하는 상황설명정보부와, 상기 대화 정보, 상기 의도 정보 및 상기 상황 정보에 대해 자연어 분석을 수행하는 자연언어이해부와, 상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 종합하여 상 기 대화 정보의 의미를 상기 의도 정보 및 상기 상황 정보에 따라 해석한 현재 대화 상태 정보를 도출하고, 상 기 현재 대화 상태 정보에 대응하는 복수의 응답을 포함하는 다음 대화 상태 정보를 결정하는 대화상태추적부를 포함한다. 상기 대화 장치는 상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 기초로 사용자의 감정 상태를 나타내는 감정 상태 정보를 생성하는 감정추적부와, 상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 기초로 상기 대화의 윤리성을 나타내는 윤리 상태 정보를 생성하는 윤리성분석부를 더 포함한다. 상기 대화 장치는 상기 감정 상태 정보 및 상기 윤리 상태 정보 중 적어도 하나에 따라 상기 복수의 응답 중 어 느 하나의 응답을 선택하여 선택된 응답을 포함하는 최종 다음 대화 상태 정보를 결정하는 멀티모달대화관리부 를 더 포함한다. 상기 대화 장치는 상기 최종 다음 대화 상태 정보를 자연어 스크립트인 출력 대화 스크립트로 변환하는 자연언 어생성부와, 상기 출력 대화 스크립트에 상기 감정 상태 정보, 의도 정보 및 상황 정보 중 적어도 하나에 부합 하는 억양과 톤을 부여한 음성 신호를 합성하는 적응형음성합성부를 더 포함한다. 상술한 바와 같은 목적을 달성하기 위한 본 발명의 바람직한 실시예에 따른 대화 장치의 사용자 적응형 대화 방 법은 대화 상황의 사용자의 발언이 수집된 음성을 자연어 스크립트로 변환하여 대화 정보를 생성하는 단계와, 상기 대화 상황으로부터 수집된 영상으로부터 상황을 인식하여 상기 발언이 이루어진 상황을 나타내는 상황 정 보를 생성하고, 상기 발언의 의도를 나타내는 의도 정보를 생성하는 단계와, 상기 상황 정보 및 상기 의도 정보 를 자연어 스크립트로 변환하는 단계와, 상기 대화 정보, 상기 의도 정보 및 상기 상황 정보에 대해 자연어 분 석을 수행하는 단계와, 상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 종합하여 상기 대화 정보의 의미 를 상기 의도 정보 및 상기 상황 정보에 따라 해석한 현재 대화 상태 정보를 도출하는 단계와, 상기 현재 대화 상태 정보에 대응하는 복수의 응답을 포함하는 다음 대화 상태 정보를 결정하는 단계를 포함한다. 상기 사용자 적응형 대화 방법은 상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 기초로 사용자의 감정 상태를 나타내는 감정 상태 정보를 생성하는 단계와, 상기 대화 정보, 상기 의도 정보 및 상기 상황 정보를 기초로 상기 대화의 윤리성을 나타내는 윤리 상태 정보를 생성하는 단계를 더 포함한다. 상기 사용자 적응형 대화 방법은 상기 감정 상태 정보 및 상기 윤리 상태 정보 중 적어도 하나에 따라 상기 복 수의 응답 중 어느 하나의 응답을 선택하여 선택된 응답을 포함하는 최종 다음 대화 상태 정보를 결정하는 단계 를 더 포함한다. 상기 사용자 적응형 대화 방법은 상기 최종 다음 대화 상태 정보를 자연어 스크립트인 출력 대화 스크립트로 변 환하는 단계와, 상기 출력 대화 스크립트에 상기 감정 상태 정보, 의도 정보 및 상황 정보 중 적어도 하나에 부 합하는 억양과 톤을 부여한 음성 신호를 생성하는 단계를 더 포함한다. 상술한 바와 같은 본 발명은 사용자의 상황에 따라 동적인 감정 면화에 대응하는 대화가 가능한 대화 장치 및 방법을 제공한다. 특히, 본 발명은 비윤리적 대화 학습을 저지하는 대화 장치 및 방법을 제공한다. 더욱이, 본 발명은 감정 및 상황에 적응하면서 사용자와의 공감도를 높이는 대화 장치 및 방법을 제공한다."}
{"patent_id": "10-2017-0167996", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 현재 대화중인 사용자의 감정 상태를 인지하여 그 모니터링 결과를 대화 시스템에 적용하여 감정 상태에 적합한 대화를 유지할 수 있다. 본 발명은 현재 대화 내용의 윤리성 모니터링 결과를 대화 시스템에 적용하여 비윤리적인 대화를 방지할 수 있 다. 그리고 본 발명은 대화 사용자의 감정, 윤리, 개인화 특성을 고려한 맞춤형 대화 장치 및 방법을 제공할 수 있 다."}
{"patent_id": "10-2017-0167996", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 상세한 설명에 앞서, 이하에서 설명되는 본 명세서 및 청구범위에 사용된 용어나 단어는 통상적이거 나 사전적인 의미로 한정해서 해석되어서는 아니 되며, 발명자는 그 자신의 발명을 가장 최선의 방법으로 설명 하기 위해 용어의 개념으로 적절하게 정의할 수 있다는 원칙에 입각하여 본 발명의 기술적 사상에 부합하는 의 미와 개념으로 해석되어야만 한다. 따라서 본 명세서에 기재된 실시예와 도면에 도시된 구성은 본 발명의 가장 바람직한 실시예에 불과할 뿐, 본 발명의 기술적 사상을 모두 대변하는 것은 아니므로, 본 출원시점에 있어서 이들을 대체할 수 있는 다양한 균등물과 변형 예들이 있을 수 있음을 이해하여야 한다. 이하, 첨부된 도면을 참조하여 본 발명의 바람직한 실시예들을 상세히 설명한다. 이때, 첨부된 도면에서 동일한 구성 요소는 가능한 동일한 부호로 나타내고 있음을 유의해야 한다. 또한, 본 발명의 요지를 흐리게 할 수 있는 공지 기능 및 구성에 대한 상세한 설명은 생략할 것이다. 마찬가지의 이유로 첨부 도면에 있어서 일부 구성요소 는 과장되거나 생략되거나 또는 개략적으로 도시되었으며, 각 구성요소의 크기는 실제 크기를 전적으로 반영하 는 것이 아니다. 본 발명의 실시예에 따른 감정 및 윤리 상태 모니터링 기반 사용자 적응형 대화 장치의 구성에 대해서 설명하기 로 한다. 도 1은 본 발명의 실시예에 따른 제안하는 감정 및 윤리 상태 모니터링 기반 사용자 적응형 대화 장치 의 구성을 설명하기 위한 블록도이다. 도 1을 참조하면, 본 발명의 실시예에 따른 사용자 적응형 대화 장치 (100, 이하, '대화장치'로 축약함)는 통신부, 카메라부, 오디오부, 입력부, 표시부, 저장부 및 제어부를 포함한다. 통신부는 다른 장치와 통신하기 위한 수단이다. 통신부는 네트워크를 통해 다른 장치와 통신할 수 있 다. 통신부는 송신되는 신호의 주파수를 상승 변환 및 증폭하는 RF(Radio Frequency) 송신기(Tx) 및 수신 되는 신호를 저 잡음 증폭하고 주파수를 하강 변환하는 RF 수신기(Rx)를 포함할 수 있다. 그리고 통신부는송신되는 신호를 변조하고, 수신되는 신호를 복조하는 모뎀(Modem)을 포함할 수 있다. 통신부는 수신되는 데이터를 제어부로 전달한다. 또한, 통신부는 제어부로부터 송신하는 데이터를 전달받아 네트워 크를 통해 대화장치로 전송한다. 카메라부는 영상을 촬영하기 위한 것으로, 적어도 이미지 센서를 포함한다. 이미지 센서는 피사체에서 반 사되는 빛을 입력받아 전기신호로 변환하며, CCD(Charged Coupled Device), CMOS(Complementary Metal-Oxide Semiconductor) 등을 기반으로 구현될 수 있다. 카메라부는 아날로그-디지털 변환기(Analog to Digital Converter)를 더 포함할 수 있으며, 이미지 센서에서 출력되는 아날로그 신호를 디지털 신호로 변환하여 제어부 로 출력할 수 있다. 오디오부는 마이크(MIC) 및 스피커(SPK)를 포함한다. 오디오부는 마이크(MIC)를 통해 사용자의 음성 을 입력받고, 입력된 음성을 제어부에 제공한다. 또한, 오디오부는 제어부로부터 입력되는 음성 을 스피커(SPK)를 통해 출력한다. 입력부는 대화장치를 제어하기 위한 사용자의 키 조작을 입력받고 입력 신호를 생성하여 제어부(17 0)에 전달한다. 입력부는 대화장치를 제어하기 위한 각 종 키들을 포함할 수 있다. 입력부는 표 시부가 터치스크린으로 이루어진 경우, 각 종 키들의 기능이 표시부에서 이루어질 수 있으며, 터치스 크린만으로 모든 기능을 수행할 수 있는 경우, 입력부는 생략될 수도 있다. 표시부는 대화장치의 메뉴, 입력된 데이터, 기능 설정 정보 및 기타 다양한 정보를 사용자에게 시각 적으로 제공한다. 표시부는 대화장치의 부팅 화면, 대기 화면, 메뉴 화면, 등의 화면을 출력하는 기 능을 수행한다. 이러한 표시부는 액정표시장치(LCD, Liquid Crystal Display), 유기 발광 다이오드(OLED, Organic Light Emitting Diodes), 능동형 유기 발광 다이오드(AMOLED, Active Matrix Organic Light Emitting Diodes) 등으로 형성될 수 있다. 한편, 표시부는 터치스크린으로 구현될 수 있다. 이러한 경우, 표시부 는 터치센서를 포함한다. 터치센서는 사용자의 터치 입력을 감지한다. 터치센서는 정전용량 방식 (capacitive overlay), 압력식, 저항막 방식(resistive overlay), 적외선 감지 방식(infrared beam) 등의 터치 감지 센서로 구성되거나, 압력 감지 센서(pressure sensor)로 구성될 수도 있다. 상기 센서들 이외에도 물체의 접촉 또는 압력을 감지할 수 있는 모든 종류의 센서 기기가 본 발명의 터치센서로 이용될 수 있다. 터치센서는 사용자의 터치 입력을 감지하고, 감지 신호를 발생시켜 제어부로 전송한다. 특히, 표시부이 터치스크 린으로 이루어진 경우, 입력부 기능의 일부 또는 전부는 표시부을 통해 이루어질 수 있다. 저장부는 대화장치의 동작에 필요한 프로그램 및 데이터를 저장하는 역할을 수행한다. 특히, 저장부 는 자연어를 인식하고, 분석하기 위한 다양한 데이터를 저장할 수 있다. 저장부에 저장되는 각 종 데 이터는 사용자의 조작에 따라, 삭제, 변경, 추가될 수 있다. 제어부는 대화장치의 내부 블록들 간 신호 흐름을 제어하고, 데이터를 처리하는 데이터 처리 기능을 수행할 수 있다. 또한, 제어부는 기본적으로, 대화장치의 각 종 기능을 제어하는 역할을 수행한다. 제어부는 중앙처리장치(CPU: Central Processing Unit), 디지털신호처리기(DSP: Digital Signal Processor) 등을 예시할 수 있다. 제어부는 사용자 적응형 대화를 위한 프로세스를 수행하기 위한 세부 모 듈을 포함한다. 그러면, 이러한 제어부의 사용자 적응형 대화를 위한 세부 구성에 대해서 보다 상세하게 설명하기로 한다. 도 2는 본 발명의 실시예에 따른 제어부의 사용자 적응형 대화를 위한 세부 구성을 설명하기 위한 블록도이다. 도 2를 참조하면, 제어부는 전처리부, 음성인식부, 인공시각부, 상황설명정보부, 자 연언어이해부, 대화상태추적부, 감정추적부, 윤리성분석부, 멀티모달대화관리부, 자 연언어생성부 및 적응형음성합성부을 포함한다. 사용자의 대화 상황에서 오디오부는 대화의 음성 신호를 수집하고, 카메라부는 그 대화 상황의 영상 신호를 수집한다. 따라서 음성 신호는 대화에서 사용자의 발언이 될 수 있고, 영상 신호는 사용자가 발언할 때 의 사용자의 얼굴을 포함하는 주변을 촬영한 영상이 될 수 있다. 이때, 전처리부는 오디오부를 통해 음성 신호를 입력받고, 카메라부를 통해 영상 신호를 입력받는다. 그런 다음, 전처리부는 입력된 음 성 신호에서 노이즈 등을 제거하여 분석에 적합한 형태인 음성 특질 정보로 가공하고, 가공된 음성 특질 정보를 출력한다. 또한, 전처리부는 전처리부는 입력된 영상 신호에서 노이즈 등을 제거하여 분석에 적합한 형태인 영상 특질 정보로 가공하고, 가공된 영상 특질 정보를 출력한다. 음성인식부는 대화의 음성이 가공된 음성 특질 정보를 자연어 스크립트로 인식하여 대화 정보를 생성한다. 그런 다음, 음성인식부는 대화의 음성이 자연어 스크립트로 형태로 인식된 대화 정보를 출력한다. 인공시각부는 가공된 영상 특질 정보를 기초로 대화 중 인식 가능한 상황을 인식하여 대화 중의 상황을 나 타내는 상황 정보를 생성한다. 이러한 인공시각부는 대표적으로, 표정 인식을 포함하는 이미지 인식 기능 을 수행하는 인공신경망을 포함할 수 있다. 예를 들면, 인공시각부는 인공신경망을 통해 발언자, 즉, 사용 자의 표정 등을 인식하여 상황 정보를 생성할 수 있다. 또한, 이를 기초로 사용자 발언의 의도 및 그 문맥 등을 파악하여 사용자 발언의 의도 혹은 문맥을 나타내는 의도 정보를 생성한다. 그런 다음, 인공시각부는 상황 정보 및 의도 정보를 출력한다. 상황설명정보부는 인공시각부가 출력한 상황 정보 및 의도 정보를 자연어 스크립트 형태로 가공한다. 그런 다음, 상황설명정보부는 자연어 스크립트로 가공된 상황 정보 및 의도 정보를 출력한다. 자연언어이해부는 자연어 스크립트로 변환된 대화 정보, 의도 정보 및 상황 정보를 형태소분석, 개체명인 식, 구문분석, 의미역 인식 등을 통해 자연어 분석을 수행한다. 그런 다음, 자연어로 분석된 대화 정보, 의도 정보 및 상황 정보를 출력한다. 자연언어이해부는 시멘틱 엔진으로 구현될 수 있다. 대화상태추적부는 자연언어이해부에서 분석한 대화 정보, 의도 정보 및 상황 정보를 기초로 현재 대 화의 발언이 내포하는 실제 의미를 나타내는 현재 대화 상태 정보를 도출하고, 이에 대응하는 응답을 포함하는 다음 대화 상태 정보를 결정한다. 즉, 현재 대화 상태 정보는 대화시 사용자의 발언 그대로의 스크립트를 나타 내는 대화 정보의 의미를 의도 정보 및 상황 정보에 따라 해석한 것을 나타낸다. 여기서, 다음 대화 상태 정보 는 복수의 응답을 포함할 수 있다. 그런 다음, 대화상태추적부는 현재 대화 상태 정보 및 다음 대화 상태 정보를 출력한다. 예컨대, 사용자와 대화장치는 대화장치에서 실행되는 게임앱을 통해 게임을 하는 중에 대화장치(10 0)가 게임에서 승리하였다고 가정한다. 대화장치는 오디오부를 통해 '와, 이겼다'라고 출력했으며, 이에 대해 사용자는 '축하해.'라고 발언하였다고 가정한다. 이때, 인공시각부가 인식한 사용자의 표정은 화난 표정으로 인식되었다고 가정한다. 따라서 이러한 경우, 대화 정보는 '축하해.'이지만, 상황 정보는 게임 중 사용자가 패배하여 화난 상황이고, 의도 정보는 실제로 축하는 것이 아님을 알 수 있다. 현재 대화 상태 정보는 의도 정보 및 상황 정보를 기초로 '축하해.'라는 발언이 문언 그대로 축하는 의미인지 혹은 비꼬는 의미인지 여부를 포함하는 정보이다. 이에 따라, 다음 대화 상태 정보는 현재 대화 상태 정보에 대 한 응답으로 선택할 수 있는 복수의 후보 응답을 포함한다. 만약, 비꼬는 의미의 '축하해'라면, 다음 대화 상태 정보는 사용자의 기분을 누그러뜨리기 위한 응답, 사용자를 면박 주는 응답 등을 후보 응답으로 포함할 수 있다. 감정추적부는 앞서 분석된 대화 정보, 의도 정보 및 상황 정보를 기반으로 사용자의 감정 상태를 나타내는 감정 상태 정보를 도출할 수 있다. 또는, 감정추적부는 입력부를 통해 사용자의 감정 상태를 입력 받 고, 사용자의 감정 상태 정보를 결정할 수 있다. 윤리성분석부는 일 실시예에 따르면, 앞서 분석된 대화 정보, 의도 정보 및 상황 정보를 기반으로 현재 대 화 상황의 윤리성을 분석하여 대화의 윤리성을 나타내는 윤리 상태 정보를 생성한다. 다른 실시예에 따르면, 윤 리성분석부는 통신부를 통해 다른 장치에 앞서 분석된 대화 정보, 의도 정보 및 상황 정보를 전송하 고, 다른 장치의 사용자가 이를 기초로 윤리 상태 정보를 다른 장치를 통해 입력하면, 입력된 윤리 상태 정보를 대화장치로 전송한다. 그러면, 윤리성분석부는 통신부를 통해 윤리 상태 정보를 수신하여, 수신 된 윤리 상태 정보를 현재 대화의 윤리 상태 정보로 결정할 수 있다. 멀티모달대화관리부는 대화상태추적부가 도출한 다음 대화 상태 정보와 감정추적부가 도출한 감 정 상태 정보와 윤리성분석부가 도출한 윤리 상태 정보를 입력받고, 입력 받은 감정 상태와 윤리 상태를 고려하여 다음 대화 상태 정보의 복수의 응답 중 어느 하나의 응답을 선택하여 최종적인 다음 대화 상태 정보를 결정한다. 예컨대, 사용자가 화가 많이 난 상태라면, 사용자의 기분을 누그러뜨리는 응답을 최종적으로 최종 다 음 대화 상태 정보로 결정할 수 있다. 다른 예로, 만약, 사용자의 발언이 저장부에 기 저장된 윤리 기준에 위배되는 발언인 경우, 해당 발언에 대응하는 응답을 할 수 없거나, 그런 발언은 적절하지 못하는 내용의 응답 을 다음 대화 상태 정보로 최종 결정할 수 있다. 자연언어생성부은 멀티모달대화관리부가 최종 결정한 최종 다음 대화 상태 정보를 자연어 스크립트인 출력 대화 스크립트로 변환한다.적응형음성합성부 자연언어생성부이 생성된 출력 대화 스크립트에 앞서 도출된 감정 상태 정보, 상황 정보 및 의도 정보 중 적어도 하나에 적합한 억양과 톤을 부여한 음성 신호를 생성한다. 그런 다음, 적응형음성 합성부은 생성된 음성 신호를 오디오부를 통해 출력한다. 그러면, 본 발명의 실시예에 따른 감정 및 윤리 상태 모니터링 기반 사용자 적응형 대화 장치의 구성에 대해서 설명하기로 한다. 도 3은 본 발명의 실시예에 따른 제안하는 감정 및 윤리 상태 모니터링 기반 사용자 적응형 대화 방법을 설명하기 위한 흐름도이다. 도 3을 참조하면, 사용자의 대화 상황에서 오디오부는 대화의 음성 신호를 수집(녹음)하고, 카메라부(12 0)는 그 대화 상황의 영상 신호를 수집(녹화)한다. 따라서 음성 신호는 대화에서 사용자의 발언이 될 수 있고, 영상 신호는 사용자가 발언할 때의 사용자의 얼굴을 포함하는 주변을 촬영한 영상이 될 수 있다. 이에 따라, 전 처리부는 S110 단계에서 오디오부를 통해 음성 신호를 입력받고, 카메라부를 통해 영상 신호를 입력받는다. 그런 다음, 전처리부는 S120 단계에서 입력된 음성 신호에서 노이즈 등을 제거하여 분석에 적 합한 형태인 음성 특질 정보로 가공하고, 가공된 음성 특질 정보를 출력한다. 이와 동시에, 전처리부는 S120 단계에서 전처리부는 입력된 영상 신호에서 노이즈 등을 제거하여 분석에 적합한 형태인 영상 특질 정보로 가공하고, 가공된 영상 특질 정보를 출력한다. 음성인식부는 S130 단계에서 대화의 음성이 가공된 음성 특질 정보를 자연어 스크립트로 인식하여 대화 정 보를 생성한 후, 대화의 음성이 자연어 스크립트로 형태로 인식된 대화 정보를 출력한다. 한편, 인공시각부는 대표적으로, 표정 인식을 포함하는 이미지 인식 기능을 수행하는 인공신경망을 포함할 수 있다. 이러한 인공시각부는 S140 단계에서 앞서 가공된 영상 특질 정보를 기초로 대화 중 인식 가능한 상황을 인식하여 대화 중의 상황을 나타내는 상황 정보를 생성한다. 예를 들면, 인공시각부는 인공신경망 을 통해 발언자, 즉, 사용자의 표정 등을 인식하여 상황 정보를 생성할 수 있다. 또한, 인공시각부는 앞서 가공된 영상 특질 정보를 기초로 사용자 발언의 의도 및 그 문맥 등을 파악하여 사용자 발언의 의도 혹은 문맥 을 나타내는 의도 정보를 생성한다. 그런 다음, 인공시각부는 상황 정보 및 의도 정보를 출력한다. 그러면, 상황설명정보부는 S150 단계에서 인공시각부가 출력한 상황 정보 및 의도 정보를 자연어 스 크립트 형태로 가공하고, 자연어 스크립트로 가공된 상황 정보 및 의도 정보를 출력한다. 다음으로, 자연언어이해부는 S160 단계에서 자연어 스크립트로 변환된 대화 정보, 의도 정보 및 상황 정보 를 형태소분석, 개체명인식, 구문분석, 의미역 인식 등을 통한 자연어 분석을 수행한다. 이에 따라, 자연언어이 해부로부터 자연어로 분석된 대화 정보, 의도 정보 및 상황 정보를 출력한다. 자연언어이해부는 시멘 틱 엔진으로 구현될 수 있다. 이와 같이, 대화 정보, 의도 정보 및 상황 정보에 대한 자연어 인식(시멘틱 해석)이 완료되면, 대화상태추적부 는 S170 단계에서 대화 정보, 의도 정보 및 상황 정보를 기초로 현재 대화의 발언이 내포하는 실제 의미를 나타내는 현재 대화 상태 정보를 도출하고, 이에 대응하는 응답을 포함하는 다음 대화 상태 정보를 결정한다. 즉, 현재 대화 상태 정보는 대화시 사용자의 발언 그대로의 스크립트를 나타내는 대화 정보의 의미를 의도 정보 및 상황 정보에 따라 해석한 것을 나타낸다. 여기서, 다음 대화 상태 정보는 복수의 응답을 포함할 수 있다. 예 컨대, 사용자와 대화장치는 대화장치에서 실행되는 게임앱을 통해 게임을 하는 중에 대화장치가 게임에서 승리하였다고 가정한다. 대화장치는 오디오부를 통해 '와, 이겼다'라고 출력했으며, 이에 대해 사용자는 '축하해.'라고 발언하였다고 가정한다. 이때, 인공시각부가 인식한 사용자의 표정은 화난 표정으로 인식되었다고 가정한다. 따라서 이러한 경우, 대화 정보는 '축하해.'이지만, 상황 정보는 게임 중 사 용자가 패배하여 화난 상황이고, 의도 정보는 실제로 축하는 것이 아님을 알 수 있다. 현재 대화 상태 정보는 의도 정보 및 상황 정보를 기초로 '축하해.'라는 발언이 문언 그대로 축하는 의미인지 혹은 비꼬는 의미인지 여 부를 포함하는 정보이다. 이에 따라, 다음 대화 상태 정보는 현재 대화 상태 정보에 대한 응답으로 선택할 수 있는 복수의 후보 응답을 포함한다. 만약, 비꼬는 의미의 '축하해'라면, 다음 대화 상태 정보는 사용자의 기분 을 누그러뜨리기 위한 응답, 사용자를 면박 주는 응답 등을 후보 응답으로 포함할 수 있다. 다음으로, 감정추적부는 S180 단계에서 앞서 분석된 대화 정보, 의도 정보 및 상황 정보를 기반으로 사용 자의 감정 상태를 나타내는 감정 상태 정보를 도출할 수 있다. 또는, 다른 실시예에 따르면, S180 단계에서 감 정추적부는 입력부를 통해 사용자의 감정 상태를 입력 받고, 사용자의 감정 상태 정보를 결정할 수 있다. 이어서, 윤리성분석부는 S190 단계에서 일 실시예에 따르면, 앞서 분석된 대화 정보, 의도 정보 및 상황 정보를 기반으로 현재 대화 상황의 윤리성을 분석하여 대화의 윤리성을 나타내는 윤리 상태 정보를 생성한다. S190 단계의 다른 실시예에 따르면, 윤리성분석부는 통신부를 통해 다른 장치에 앞서 분석된 대화 정 보, 의도 정보 및 상황 정보를 전송하고, 다른 장치의 사용자가 이를 기초로 윤리 상태 정보를 다른 장치를 통 해 입력하면, 입력된 윤리 상태 정보를 대화장치로 전송한다. 그러면, 윤리성분석부는 통신부를 통해 윤리 상태 정보를 수신하여, 수신된 윤리 상태 정보를 현재 대화의 윤리 상태 정보로 결정할 수 있다. 한편, 멀티모달대화관리부는 S200 단계에서 대화상태추적부가 도출한 다음 대화 상태 정보와 감정추 적부가 도출한 감정 상태 정보와 윤리성분석부가 도출한 윤리 상태 정보를 입력받고, 입력 받은 감정 상태와 윤리 상태를 고려하여 다음 대화 상태 정보의 복수의 응답 중 어느 하나의 응답을 선택하여 최종적인 다 음 대화 상태 정보를 결정한다. 예컨대, 사용자가 화가 많이 난 상태라면, 사용자의 기분을 누그러뜨리는 응답 을 최종적으로 최종 다음 대화 상태 정보로 결정할 수 있다. 다른 예로, 만약, 사용자의 발언이 저장부에 기 저장된 윤리 기준에 위배되는 발언인 경우, 해당 발언에 대응하는 응답을 할 수 없거나, 그런 발언은 적절하 지 못하는 내용의 응답을 다음 대화 상태 정보로 최종 결정할 수 있다. 이어서, 자연언어생성부은 S210 단계에서 멀티모달대화관리부가 최종 결정한 최종 다음 대화 상태 정 보를 자연어 스크립트인 출력 대화 스크립트로 변환한다. 그런 다음, 적응형음성합성부 S220 단계에서 자연언어생성부이 생성된 출력 대화 스크립트에 앞서 도 출된 감정 상태 정보, 상황 정보 및 의도 정보 중 적어도 하나에 적합한 억양과 톤을 부여한 음성 신호를 생성 한다. 그런 다음, 적응형음성합성부은 생성된 음성 신호를 오디오부를 통해 출력한다. 전술한 바와 같은 본 발명은 현재 대화중인 사용자의 감정 상태를 인지하여 그 모니터링 결과를 대화 시스템에 적용하여 감정 상태에 적합한 대화를 유지할 수 있다. 또한, 본 발명은 현재 대화 내용의 윤리성 모니터링 결과 를 대화 시스템에 적용하여 비윤리적인 대화를 방지할 수 있다. 그리고 본 발명은 대화 사용자의 감정, 윤리, 개인화 특성을 고려한 맞춤형 대화 장치 및 방법을 제공할 수 있다. 한편, 전술한 본 발명의 실시예에 따른 방법은 다양한 컴퓨터수단을 통하여 판독 가능한 프로그램 형태로 구현 되어 컴퓨터로 판독 가능한 기록매체에 기록될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이 터구조 등을 단독으로 또는 조합하여 포함할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 예컨 대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광 기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어를 포함할 수 있다. 이러한 하드웨어 장치 는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 이상 본 발명을 몇 가지 바람직한 실시예를 사용하여 설명하였으나, 이들 실시예는 예시적인 것이며 한정적인"}
{"patent_id": "10-2017-0167996", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "것이 아니다. 이와 같이, 본 발명이 속하는 기술분야에서 통상의 지식을 지닌 자라면 본 발명의 사상과 첨부된 특허청구범위에 제시된 권리범위에서 벗어나지 않으면서 균등론에 따라 다양한 변화와 수정을 가할 수 있음을 이해할 것이다."}
{"patent_id": "10-2017-0167996", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 제안하는 감정 및 윤리 상태 모니터링 기반 사용자 적응형 대화 장치의 구성을 설명하기 위한 블록도이다. 도 2는 본 발명의 실시예에 따른 제어부의 사용자 적응형 대화를 위한 세부 구성을 설명하기 위한 블록도이다. 도 3은 본 발명의 실시예에 따른 제안하는 감정 및 윤리 상태 모니터링 기반 사용자 적응형 대화 방법을 설명하 기 위한 흐름도이다."}
