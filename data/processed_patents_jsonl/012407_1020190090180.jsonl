{"patent_id": "10-2019-0090180", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0012442", "출원번호": "10-2019-0090180", "발명의 명칭": "가상 현실 및 증강 현실을 이용한 혼합 렌더링 콘텐츠 제공을 위한 카메라 트래킹 방법 및 이", "출원인": "티팟스튜디오 주식회사", "발명자": "박민지"}}
{"patent_id": "10-2019-0090180", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "가상 현실(VR) 및 증강 현실(AR)을 이용한 혼합 렌더링 콘텐츠를 제공하기 위한 시스템에 있어서,가상 현실 장치를 이용하는 플레이어 사용자의 모션(motion) 관련 정보를 수신하고, 상기 모션 관련 정보에 기초하여 상기 플레이어 사용자의 인체 자세를 추정하도록 구성된 캐릭터 인체 모션 생성 장치; 및상기 캐릭터 인체 모션 생성 장치로부터 수신한 캐릭터의 모션 관련 정보 및 관찰자의 카메라 트래킹 정보에 기초하여 3차원 환경을 렌더링함으로써 영상 콘텐츠를 생성하도록 구성된 모바일 장치를 포함하는 혼합 렌더링 콘텐츠 제공 시스템."}
{"patent_id": "10-2019-0090180", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 모바일 장치에서 촬영한 상기 플레이어의 이미지로부터 상기 모바일 장치의 카메라의 초기 위치를 찾아낸 후에 카메라 트래킹을 수행하는 것인, 혼합 렌더링 콘텐츠 제공 시스템."}
{"patent_id": "10-2019-0090180", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 캐릭터 인체 모션 생성 장치는 기계 학습을 이용하여 상기 플레이어 사용자의 인체 자세를 추정하고, 상기 모바일 장치는 상기 추정된 상기 플레이어 사용자의 인체 자세와 상기 캐릭터의 모션 관련정보를 동기화함으로써 상기 모바일 장치의 카메라의 초기 위치를 추정하는 것인, 혼합 렌더링 콘텐츠 제공 시스템."}
{"patent_id": "10-2019-0090180", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 플레이어 사용자의 모션(motion) 관련 정보는 상기 플레이어 사용자의 인체 모션을 촬영한 카메라로부터 획득한 모션 캡쳐 정보 및 상기 플레이어 사용자가 착용한 상기 가상 현실 장치의IMU(Inertial Measurement Unit) 센서 데이터를 포함하고, 상기 캐릭터 인체 모션 생성 장치는 상기 IMU 센서데이터와 상기 모션 캡쳐 정보 사이의 상관 관계를 인공 신경망(Artificial Neural Network, ANN)을 이용하여학습하는 것인, 혼합 렌더링 콘텐츠 제공 시스템."}
{"patent_id": "10-2019-0090180", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서, 상기 모바일 장치는 주기적으로 카메라의 위치 및 방향을 조정함으로써 오차를 줄이는 것인,혼합 렌더링 콘텐츠 제공 시스템."}
{"patent_id": "10-2019-0090180", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 모바일 장치는 상기 가상 현실 장치를 이용하는 플레이어 사용자가 위치하는 공간 내에배치된 적어도 하나의 마커에 기초하여 상기 카메라의 위치 및 방향을 조정하는 것인, 혼합 렌더링 콘텐츠 제공시스템."}
{"patent_id": "10-2019-0090180", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명에 의하면, 가상 현실(VR) 및 증강 현실(AR)을 이용한 혼합 렌더링 콘텐츠를 제공하기 위한 시스템에 있 어서, 가상 현실 장치를 이용하는 플레이어 사용자의 모션(motion) 관련 정보를 수신하고, 모션 관련 정보에 기 초하여 상기 플레이어 사용자의 인체 자세를 추정하도록 구성된 캐릭터 인체 모션 생성 장치; 및 캐릭터 인체 모 션 생성 장치로부터 수신한 캐릭터의 모션 관련 정보 및 관찰자의 카메라 트래킹 정보에 기초하여 3차원 환경을 렌더링함으로써 영상 콘텐츠를 생성하도록 구성된 모바일 장치를 포함하는 혼합 렌더링 콘텐츠 제공 시스템을 제 공할 수 있다."}
{"patent_id": "10-2019-0090180", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 가상 현실(Virtual Reality, VR) 및 증강 현실(Augmented Reality, AR)을 이용한 혼합 렌더링 콘텐 츠 제공을 위한 카메라 트래킹(camera tracking) 방법, 이를 위한 시스템 및 장치에 관한 것이다. 보다 구체적 으로, 본 발명은 VR 장치 플레이어 사용자의 모션과 관련된 정보 및 모바일 기기 사용자의 카메라 트래킹 정보 에 기초하여 VR 및 AR 혼합 렌더링 콘텐츠를 생성 및 제공하기 위한 방법, 장치 및 시스템에 관한 것이다."}
{"patent_id": "10-2019-0090180", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "VR 기기의 가정 보급률이 예상보다 낮아지면서, VR 체험관 또는 VR 카페를 통해 VR을 즐기는 사용자가 늘어났으 며, 이로 인해 다수가 즐길 수 있는 VR 콘텐츠의 수요가 늘어나고 있다. HMD(Head Mounted Display)를 착용한 VR 플레이어는 몰입감 높은 게임을 즐기지만, 나머지 관찰자들은 VR 플레이어의 동작을 바라보거나 VR 플레이어 시점으로 제공되는 스크린을 수동적으로 응시하며 가상현실 환경의 특징인 몰입감을 느끼기 어렵다. 또한, VR 게임을 플레이하고 있는 VR 플레이어를 관찰하는 관찰자는 VR을 플레이하고 있는 사용자의 1인칭 시점을 스크린 을 통해 확인할 수 있으나, 화면 전환이 매우 빠르고 멀미를 유발한다는 문제점이 있다. VR 체험관에서 VR 플레이어가 아닌 관찰자들이 콘텐츠를 효과적으로 즐길 수 있기 위해서는 수동적인 화면 송출 이 아닌 능동적으로 가상 세계를 탐험할 수 있는 수단을 제공해야 한다. 또한, 과거에 비해 동영상 콘텐츠가 폭발적으로 증가하고 있으며 글이나 이미지가 아닌 동영상 콘텐츠 제작 및 송출에 대한 수요가 커지고 있으며, 특히 1인 미디어의 증가로 특별한 실시간 영상 생성을 원하고 있으며 쉽게 독특한 방송 콘텐츠를 생성하고자 하는 수요가 계속 늘어나고 있다. 이와 같은 방송 콘텐츠의 주제로 게임 분 야가 상당한 인기를 끌고 있으며, VR 게임의 대중화로 인해 VR 게임 관련 콘텐츠 제작 및 방송 시장은 크게 성 장할 것으로 예상된다. 몰입감 높은 VR 게임 영상을 제공하기 위해서는 배경을 VR 플레이어 주변에 합성하는 크로마키 기법을 사용할 수 있으나, 이는 고비용의 시설과 장비와 외산 소프트웨어가 필요하다. 특히 전문 방송국이 아닌, 유튜브 (YouTube)나 트위치(Twitch)와 같은 인터넷 방송 중계 서비스를 활용하여 콘텐츠를 제작하는 크리에이터 (creator)는 이와 같은 시설과 장비를 활용하기 어려운 상황이다. 따라서, 기존 크로마키 기법과는 반대로, 사 용자들이 쉽게 지니는 모바일 장비에 구축되어 있는 가상 환경에 VR 플레이어의 동작을 유추하여 렌더링할 수 있는 새로운 기술이 요구된다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 등록특허 제10-1824863호"}
{"patent_id": "10-2019-0090180", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 VR 장치 플레이어 사용자의 모션과 관련된 정보 및 모바일 장치 사용자의 카메라 트래킹(camera tracking) 정보에 기초하여 VR 및 AR 혼합 렌더링 콘텐츠를 생성 및 제공하기 위한 방법 및 장치를 제공하는 것 을 목적으로 한다. 또한, 본 발명은 VR 영상을 송출하는 방식이 아닌, VR 플레이어의 핵심적인 동작 정보만 전송하고 모바일 기기 에서 가상 현실 캐릭터와 가상 현실 환경이 재구축된 혼합 렌더링 콘텐츠를 제공하는 것을 목적으로 한다. 또한, 본 발명은 VR 플레이어의 HMD와 연결된 인체 모션 데이터를 생성하기 위한 컴퓨터와 관찰자의 모바일 장 치를 함께 활용하여, 연산량이 비교적 높은 작업인 모션 데이터 생성 부분은 컴퓨터에서 처리하고, 빠르게 처리 할 수 있는 동작들만 모바일 장치에서 수행함으로써 끊김 없는 VR 영상 콘텐츠를 제공하는 것을 목적으로 한다.또한, 본 발명은 전문적인 컴퓨터 그래픽(CG) 촬영 장비가 없이도 손쉽게 VR 플레이어와 VR 세계를 합성하고 영 상 스트리밍을 진행할 수 있는 방법 및 장치를 제공하는 것을 목적으로 한다. 또한, 본 발명은 VR 플레이어의 시점이 아닌 관찰자의 시점에서 VR 플레이어를 관찰할 수 있도록 가상 현실과 VR 플레이어를 합성하고, 이를 증강 현실(AR)로 관찰자가 모바일 장치를 통해서 원하는 시점으로 바라볼 수 있 는 VR 영상 콘텐츠를 제공하는 것을 목적으로 한다. 본 발명의 해결 과제들은 이상에서 언급한 내용들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0090180", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 가상 현실(VR) 및 증강 현실(AR)을 이용한 혼합 렌더링 콘텐츠를 제공하기 위한 시스템에 있어서, 가상 현실 장치를 이용하는 플레이어 사용자의 모션(motion) 관련 정보를 수신하고, 상기 모 션 관련 정보에 기초하여 상기 플레이어 사용자의 인체 자세를 추정하도록 구성된 캐릭터 인체 모션 생성 장치; 및 상기 캐릭터 인체 모션 생성 장치로부터 수신한 캐릭터의 모션 관련 정보 및 관찰자의 카메라 트래킹 정보에 기초하여 3차원 환경을 렌더링함으로써 영상 콘텐츠를 생성하도록 구성된 모바일 장치를 포함하는 혼합 렌더링 콘텐츠 제공 시스템을 제공할 수 있다. 또한, 상기 모바일 장치에서 촬영한 상기 플레이어의 이미지로부터 상기 모바일 장치의 카메라의 초기 위치를 찾아낸 후에 카메라 트래킹을 수행할 수 있다. 또한, 상기 캐릭터 인체 모션 생성 장치는 기계 학습을 이용하여 상기 플레이어 사용자의 인체 자세를 추정하고, 상기 모바일 장치는 상기 추정된 상기 플레이어 사용자의 인체 자세와 상기 캐릭터의 모션 관련 정보 를 동기화함으로써 상기 모바일 장치의 카메라의 초기 위치를 추정할 수 있다. 또한, 상기 플레이어 사용자의 모션(motion) 관련 정보는 상기 플레이어 사용자의 인체 모션을 촬영한 카메라로 부터 획득한 모션 캡쳐 정보 및 상기 플레이어 사용자가 착용한 상기 가상 현실 장치의 IMU(Inertial Measurement Unit) 센서 데이터를 포함하고, 상기 캐릭터 인체 모션 생성 장치는 상기 IMU 센서 데이터와 상기 모션 캡쳐 정보 사이의 상관 관계를 인공 신경망(Artificial Neural Network, ANN)을 이용하여 학습할 수 있다. 또한, 상기 모바일 장치는 주기적으로 카메라의 위치 및 방향을 조정함으로써 오차를 줄일 수 있다. 또한, 상기 모바일 장치는 상기 가상 현실 장치를 이용하는 플레이어 사용자가 위치하는 공간 내에 배치된 적어 도 하나의 마커에 기초하여 상기 카메라의 위치 및 방향을 조정할 수 있다."}
{"patent_id": "10-2019-0090180", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의하면, VR 장치 플레이어 사용자의 모션과 관련된 정보 및 모바일 장치 사용자의 카메라 트래킹 (camera tracking) 정보에 기초하여 VR 및 AR 혼합 렌더링 콘텐츠를 생성 및 제공하기 위한 방법 및 장치를 제 공할 수 있다. 또한, 본 발명에 의하면, VR 영상을 송출하는 방식이 아닌, VR 플레이어의 핵심적인 동작 정보만 전송하고 모바 일 기기에서 가상 현실 캐릭터와 가상 현실 환경이 재구축된 혼합 렌더링 콘텐츠를 제공할 수 있다. 또한, 본 발명에 의하면, VR 플레이어의 HMD와 연결된 인체 모션 데이터를 생성하기 위한 컴퓨터와 관찰자의 모 바일 장치를 함께 활용하여, 연산량이 비교적 높은 작업인 모션 데이터 생성 부분은 컴퓨터에서 처리하고, 빠르 게 처리할 수 있는 동작들만 모바일 장치에서 수행함으로써 끊김 없는 VR 영상 콘텐츠를 제공할 수 있다. 또한, 본 발명에 의하면, 전문적인 컴퓨터 그래픽(CG) 촬영 장비가 없이도 손쉽게 VR 플레이어와 VR 세계를 합 성하고 영상 스트리밍을 진행할 수 있는 방법 및 장치를 제공할 수 있다. 또한, 본 발명에 의하면, VR 플레이어의 시점이 아닌 관찰자의 시점에서 VR 플레이어를 관찰할 수 있도록 가상 현실과 VR 플레이어를 합성하고, 이를 증강 현실(AR)로 관찰자가 모바일 장치를 통해서 원하는 시점으로 바라볼"}
{"patent_id": "10-2019-0090180", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "수 있는 VR 영상 콘텐츠를 제공할 수 있다.본 발명의 효과들은 이상에서 언급한 내용들로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재 로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0090180", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용 이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기 에서 설명하는 실시예에 한정되지 않는다. 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 본 명세서에서 사용되는 \"포함한다(comprises)\", \"포함하는(comprising)\"은 언급된 구성 요소, 단계, 동작 및/ 또는 소자는 하나 이상의 다른 구성 요소, 단계, 동작 및/또는 소자의 존재 또는 추가를 배제하지 않는다. 또한, 본 발명에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 구성 요소들을 설명하는데 사용될 수 있지만, 구성 요소들은 용어들에 의해 한정되어서는 안 된다. 이와 같은 용어들은 하나의 구성 요소를 다른 구 성 요소로부터 구별하는 목적으로만 사용된다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구 체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 본 발명의 실시예에 나타나는 구성부들은 서로 다른 특징적인 기능들을 나타내기 위해 독립적으로 도시되 는 것으로, 각 구성부들이 분리된 하드웨어나 하나의 소프트웨어 구성단위로 이루어짐을 의미하지 않는다. 즉, 각 구성부는 설명의 편의상 각각의 구성부로 나열하여 기술되고, 각 구성부 중 적어도 두 개의 구성부가 합쳐져 하나의 구성부로 이루어지거나, 하나의 구성부가 복수 개의 구성부로 나뉘어져 기능을 수행할 수 있다. 이러한 각 구성부의 통합된 실시예 및 분리된 실시예도 본 발명의 본질에서 벗어나지 않는 한 본 발명의 권리 범위에 포함된다. 이하, 첨부된 도면을 참조하여 본 발명에 따른 실시예를 상세하게 설명한다. 본 발명의 구성 및 그에 따른 작용 효과는 이하의 상세한 설명을 통해 명확하게 이해될 것이다. 도 1은 본 발명의 일 실시예에 따른 가상 현실 및 증강 현실을 이용한 혼합 렌더링 콘텐츠 제공 시스템의 구성 을 설명하기 위한 예시도이다. 먼저, VR 게임 또는 콘텐츠를 체험하는 VR 플레이어는 머리에 착용 가능한 HMD(Head Mounted Display)와 같은 VR 장치와 양 손에 쥘 수 있는 모션 콘트롤러(motion controller) 등을 착용할 수 있으며, 이 와 같은 VR 웨어러블 장치를 통해 VR 플레이어의 머리 위치 및 움직임뿐만 아니라 양 손의 위치 및 움직임 모션 과 관련된 정보를 수신함으로써, VR 플레이어의 인체 모션 및 자세에 대한 정보를 획득할 수 있다. 또한, VR 플레이어의 동작을 관찰하면서 촬영가능한 관찰자의 모바일 장치는 카메라 이미지 및 센서 데이터 등에 기초하여 카메라 위치를 찾아내고, AR 기술로 카메라 트래킹(camera tracking)을 통해 3차원 환경을 렌더 링할 수 있다.도 1에는 도시되지 않았지만, VR 플레이어의 HMD 및 모션 콘트롤러와 연결된 인체 모션 데이터를 생성하기 위한 컴퓨터가 별도로 존재할 수 있으며, 연산량이 비교적 높은 작업인 모션 데이터 생성 부분은 모바 일 장치보다 고성능인 컴퓨터(PC)에서 처리하고, 빠르게 처리할 수 있는 동작들만 모바일 장치에서 수행함 으로써 끊김 없는 VR 영상 콘텐츠를 제공할 수 있다. 이와 같이 생성된 VR 플레이어의 전신 모션 데이터와 카메라 트래킹 정보 등을 통해 관찰자의 모바일 장치에서 VR 플레이어의 아바타로 생성된 캐릭터가 가상 현실 화면에 합성되어 렌더링되어 생성 및 디스플레이될 수 있으며, 관찰자의 모바일 장치의 위치 및 바라보는 각도에 따라 실시간으로 3차원 가 상 환경이 변화할 수 있다. 이와 같은 혼합 렌더링 콘텐츠 제공 시스템에 의하면, 종래의 크로마키 기법과 달 리 모바일 장치에 구축되어 있는 가상 환경에 플레이어 사용자의 동작 및 자세를 유추하여 렌더링함으로써 손쉽 게 관찰자나 게임 영상 크리에이터에게 VR 플레이어의 몰입감이 전해질 수 있다. 도 2는 본 발명의 일 실시예에 따른 혼합 렌더링 콘텐츠 제공 시스템의 구성을 설명하기 위한 블록도이다. 본 발명의 일 실시예에 따른 혼합 렌더링 콘텐츠 제공 시스템은 VR 플레이어가 착용하는 VR 장치, VR 장치 로부터 모션 데이터 등을 수신하여 캐릭터 인체 모션을 생성하기 위한 캐릭터 인체 모션 생성 장치, 캐릭터 인체 모션 생성 장치로부터 전신 모션 데이터 정보 등을 수신하고, 카메라 트래킹을 통해 가상 환 경을 재구축하고 실시간 렌더링을 통해 VR 플레이어 캐릭터가 포함된 VR 영상 콘텐츠를 생성하기 위한 모바일 장치로 구성될 수 있다. 먼저, VR 게임이나 콘텐츠를 직접 경험하는 VR 플레이어가 착용하는 VR 장치는 머리에 쓰는 머리에 착용 가능한 HMD(Head Mounted Display)와 양 손에 쥘 수 있는 모션 콘트롤러(motion controller) 등을 포함할 수 있으며, 이들에 제한되지 않는다. VR 장치에서 생성되는 동작 모션 관련 데이터 등으로부터 캐릭터 인체 모션 생성 장치는 플레이어의 전신 동작을 재구성(reconstruct)할 수 있으며, 이와 같이 재구성된 전신 동 작 관련 정보가 실시간으로 모바일 장치에 전송되고, 모바일 장치에서는 증강 현실 모바일 애플리케 이션 등에 의해 카메라 트래킹 기술을 통해 최종 혼합 렌더링 콘텐츠가 생성될 수 있다. VR 장치는 통신부, 처리부, 디스플레이부, 센서부 등을 포함할 수 있다. 먼저, 통신 부는 캐릭터 인체 모션 생성 장치와 네트워크를 통해 데이터를 송신 가능하도록 구성된 모듈 또는 부 품일 수 있으며, VR 장치를 착용한 플레이어의 모션, 자세 등과 관련된 데이터를 통신부를 통해 외부 장치, 예컨대 캐릭터 인체 모션 생성 장치로 전송할 수 있다. 여기서, 네트워크는 유선 또는 무선으로 연 결되는 네트워크로서, 네트워크가 무선 통신망일 경우, 셀룰러 통신 또는 근거리 통신을 포함할 수 있다. 예컨 대, 셀룰러 통신은 LTE(Long-Term Evolution), LTE-A(LTE Advanced), 5G(5th Generation), CDMA(Code Division Multiple Access), WCDMA(Wideband CDMA), UMTS(Universal Mobile Telecommunications System), WiBro(Wireless Broadband), 또는 GSM(Global System for Mobile Communications) 등 중 적어도 하나를 포함할 수 있다. 또한, 근거리 통신은 Wi-Fi(Wireless Fidelity), 블루투스(Bluetooth), 지그비(Zigbee) 또는 NFC(Near Field Communication) 등 적어도 하나를 포함할 수 있다. 그러나, 통신 방법은 이에 한정되는 것은 아니며 차후 개발되는 무선 통신의 기술도 포함될 것이다. 처리부는 VR 장치의 동작 및 디스플레이되는 정보와 관련된 다양한 데이터 처리를 담당하며, 예컨대, 처리부는 중앙 처리 장치(CPU), 어플리케이션 프로세서(AP) 등을 포함할 수 있으며, 적어도 하나의 다른 구성요소에 관계된 명령 또는 데이터를 저장할 수 있는 메모리를 내부에 포함하거나, 장치 내의 메모리부 또는 필요한 경우 외부 메모리와 통신하여 필요한 정보에 액세스할 수 있다. 디스플레이부는 VR 장치에서 처리되는 정보를 표시(출력)한다. 예를 들어, 디스플레이부는 VR 장치에서 구동되는 응용 프로그램의 실행화면 정보, 또는 이러한 실행화면 정보에 따른 UI(User Interface), GUI(Graphic User Interface) 정보를 표시할 수 있다. 또한, 상기 디스플레이부는 입체영상 을 표시하는 입체 디스플레이부로서 구성될 수 있다. 입체 디스플레이부에는 스테레오스코픽 방식(안경 방식), 오토 스테레오스코픽 방식(무안경 방식), 프로젝션 방 식(홀로그래픽 방식) 등의 3차원 디스플레이 방식이 적용될 수 있으며, 이들에 제한받지 않는 다양한 형태의 디 스플레이 기술이 활용될 수 있다. 센서부는 VR 장치의 모션 관련된 데이터를 측정할 수 있도록 구성되며, 예컨대 HMD나 모션 콘트롤러 에 내장된 IMU(Inertial Measurement Unit) 센서일 수 있다.또한, 센서부는 HMD 및 모션 콘트롤러와 별도로 플레이어 사용자에 대한 고해상도 인체 모션 데 이터를 수집하기 위해 플레이어의 인체 모션을 촬영할 수 있는 카메라를 포함할 수 있으며, 예컨대 Vicon Mocap System과 같은 고해상도 모션 캡쳐 시스템을 포함할 수 있다. 이와 같이, VR 장치에서 생성된 고해상도 인체 모션 데이터와 IMU 센서 데이터 등에 기초하여 캐릭터 인체 모션 생성 장치와 같은 별도 컴퓨터에서 사용자의 모션 및 자세 등을 추정할 수 있으며, 이와 같은 데이터 를 수집하여 기계 학습에 활용할 수 있다. 캐릭터 인체 모션 생성 장치는 VR 장치로부터 플레이어 사용자의 모션 관련 정보 등을 수신하고, 수 신한 모션 관련 정보 등에 기초하여 플레이어의 인체 자세를 추정하도록 구성될 수 있다. 캐릭터 인체 모션 생 성 장치는 예컨대, 데스크탑(desktop) 컴퓨터, 랩탑(laptop) 컴퓨터, 태블릿(tablet) 컴퓨터, 노트북, 워 크스테이션(workstation), 스마트폰(smart phone) 중 어느 하나일 수 있으며, 이들에 제한되지 않는다. 캐릭터 인체 모션 생성 장치의 통신부는 VR 장치 등으로부터 고해상도 인체 모션 데이터와 IMU 센서 데이터를 수신할 수 있도록 구성되고, VR 장치의 통신부와 마찬가지로 다양한 유선 또는 무선 네트워크를 지원하는 모듈 또는 부품일 수 있다. 캐릭터 인체 모션 생성 장치는 기계 학습 등을 통해 플레이어의 인체 자세를 추정하고, 추정된 사용자의 인체 자세를 캐릭터의 인체 구조에 맞추는 맞춤화 작업 등을 수행하도록 구성된 처리부를 포함할 수 있다. 처리부는 예컨대 중앙 처리 장치(CPU), 어플리케이션 프로세서(AP) 등을 포함할 수 있으며, 적어도 하나의 다른 구성요소에 관계된 명령 또는 데이터를 저장할 수 있는 메모리를 내부에 포함하거나, 장치 내의 메모리부 또는 필요한 경우 외부 메모리와 통신하여 필요한 정보에 액세스할 수 있다. 처리부는 VR 장치를 착용한 플레이어 사용자의 인체 자세를 추정하기 위한 인체 자세 추정부를 포함할 수 있다. 인체 자세 추정부는 VR 장치로 수신한 IMU 센서 정보와 고해상도 인체 모션 데이터 사이의 상관 관계 학습 알고리즘을 수행할 수 있으며, 예컨대 인공 신경망(Artificial Neural Networks, ANN)을 이용하여 기계 학습을 수행할 수 있다. 여기서, VR 장치를 통해 플레이되는 VR 게임 환경의 제약 조건을 고려하여 기계 학습의 계산 범위를 제한할 수 있다. 또한, 인체 자세 추정부는 시간적 연속성을 고려하여 자연스러운 자세를 추정하고, 센서 정보의 수를 다양 하게 변화시키며 학습을 진행할 수 있다. 예컨대, 학습 알고리즘 오차율 목표는 주요 인체 부위 평균 오차 0.3m 미만으로 설정될 수 있다. 처리부는 또한 인체 자세 추정부를 통해 추정된 플레이어의 인체 자세에 기초하여 캐릭터의 인체 구 조에 적합하도록 캐릭터 맞춤화 작업을 수행하도록 구성된 캐릭터 자세 맞춤화부를 포함할 수 있다. 캐릭터 자세 맞춤화부는 추정된 사용자의 인체 자세를 렌더링되는 캐릭터의 인체 구조에 맞추어 리타겟팅 (retargeting) 작업을 수행하며, 이를 통해 VR 게임 속 가상 환경과 캐릭터 모션 사이의 물리적 불일치를 최소 화하는 역할을 수행한다. 계산 시간 최적화를 통한 실시간 아바타 모션 리타겟팅을 구현할 수 있으며, 예컨대 리타겟팅 처리 시간 목표는 프레임당 33ms 이하로 설정될 수 있다. 마지막으로, 모바일 장치는 캐릭터 인체 모션 생성 장치로부터 수신한 모션 데이터들과 카메라 트래 킹을 통해 3차원 환경의 혼합 렌더링 콘텐츠를 생성하도록 구성되며, 예컨대, 스마트폰(smart phone), 태블릿 (tablet) 컴퓨터, 데스크탑(desktop) 컴퓨터, 랩탑(laptop) 컴퓨터, 노트북, 워크스테이션(workstation), PDA(Personal Digital Assistants), 포터블(portable) 컴퓨터, 무선 전화기(wireless phone), 모바일 폰 (mobile phone), e-북(e-book), PMP(portable multimedia player), 휴대용 게임기, 디지털 카메라(digital camera), 텔레비전(television), 웨어러블 디바이스(wearable device), AI(인공지능) 스피커 중 어느 하나일 수 있으며, 이들에 제한되지 않으나 휴대성이 있는 장치가 선호된다. 모바일 장치는 통신부, 카메라부, 센서부, 처리부, 디스플레이부 및 3차원 렌더 링부 등을 포함할 수 있으며, 일반적인 스마트폰, 태블릿 컴퓨터와 같은 모바일 단말이 가지고 있는 기능 들을 수행하는 구성 요소를 모두 포함할 수 있다. 통신부는 캐릭터 인체 모션 생성 장치와 네트워크를 통해 데이터를 송신 가능하도록 구성된 모듈 또 는 부품일 수 있으며, 여기서, 네트워크는 유선 또는 무선으로 연결되는 네트워크로서, 네트워크가 무선 통신망 일 경우, 셀룰러 통신 또는 근거리 통신을 포함할 수 있다. 예컨대, 셀룰러 통신은 LTE(Long-Term Evolution), LTE-A(LTE Advanced), 5G(5th Generation), CDMA(Code Division Multiple Access), WCDMA(Wideband CDMA),UMTS(Universal Mobile Telecommunications System), WiBro(Wireless Broadband), 또는 GSM(Global System for Mobile Communications) 등 중 적어도 하나를 포함할 수 있다. 또한, 근거리 통신은 Wi-Fi(Wireless Fidelity), 블루투스(Bluetooth), 지그비(Zigbee) 또는 NFC(Near Field Communication) 등 적어도 하나를 포함 할 수 있다. 그러나, 통신 방법은 이에 한정되는 것은 아니며 차후 개발되는 무선 통신의 기술도 포함될 것이다. 캐릭터 인체 모션 생성 장치에서 생성된 모션 관련 데이터들은 복수의 모바일 장치에 실시간으로 전 송될 수 있으며, 모바일 장치는 언제라도 캐릭터 인체 모션 생성 장치에 접속 또는 접속 해제가능하 게 구성될 수 있다. 예컨대, 이를 위해 웹소켓(websocket)을 이용한 서버를 구현하여 캐릭터 인체 모션 생성 장치에서 모션 데이터를 실시간으로 복수의 모바일 장치에 브로드캐스트(broadcast)할 수 있다. 또 한, 사생활 보호를 위해 내부 네트워크에서만 접근할 수 있도록 보안 기능을 함께 제공할 수 있다. 카메라부는 VR 장치를 착용한 플레이어 사용자의 동작 등을 촬영하고, 증강 현실(AR) 구현을 위해 카 메라 트래킹을 수행하기 위한 영상 데이터를 생성하도록 구성된다. 센서부는 가속도 센서 및 자이로스코프 등을 포함할 수 있고, 센서부로부터 얻은 모바일 장치의 움직임 데이터로부터 카메라 위치를 추적하고, AR을 위한 카메라 트래킹을 구현할 수 있다. 처리부는 캐릭터 인체 모션 생성 장치로부터 수신한 캐릭터의 모션 관련 정보 및 관찰자의 카메라 트 래킹 정보에 기초하여 3차원 환경을 렌더링함으로써 영상 콘텐츠를 생성하도록 구성되며, 처리부는 예컨대 중앙 처리 장치(CPU), 어플리케이션 프로세서(AP) 등을 포함할 수 있으며, 적어도 하나의 다른 구성요소에 관계 된 명령 또는 데이터를 저장할 수 있는 메모리를 내부에 포함하거나, 장치 내의 메모리부 또는 필요한 경우 외 부 메모리와 통신하여 필요한 정보에 액세스할 수 있다. 종래 기술과 같이 실시간으로 모바일 기기에 지속적으로 영상을 스트리밍하는 것을 일반적인 네트워크 환경에서 영상이 끊기거나 지연될 가능성이 높기 때문에, 이를 해결하기 위해 캐릭터 인체 모션 생성 장치가 화면의 영상을 전송하는 대신 모바일 장치에서 화면을 렌더링할 수 있는 작은 사이즈의 데이터만을 전송하도록 구 성될 수 있다. 예컨대, 모바일 장치는 캐릭터 인체 모션 생성 장치로부터 렌더링 환경, 캐릭터 모델, 캐 릭터 텍스쳐 등 정보 중 적어도 하나는 미리 수신하고, 실시간 변하는 캐릭터의 위치, 방향 및 모션 관련 정보 중 적어도 하나는 실시간으로 수신할 수 있다. 이와 같은 구성을 통해 연산량이 비교적 높은 작업인 모션 데 이터 생성 부분은 모바일 장치보다 고성능인 캐릭터 인체 모션 생성 장치의 컴퓨터(PC)에서 처리하고, 빠르게 처리할 수 있는 동작들만 모바일 장치에서 수행함으로써 끊김 없는 VR 영상 콘텐츠를 제 공할 수 있다. 처리부는 또한 AR 카메라 트래킹 기술을 구현하며, 크게 카메라 자동 조정 기술 및 AR 구현을 위한 카메라 트래킹 기술로 구분될 수 있다. 카메라 자동 조정 기술 개발은 모바일 장치에서 촬영한 이미지로부터 카 메라의 초기 위치를 자동으로 찾아내는 기술이며, 이후로는 AR기술을 활용하여 카메라를 트래킹할 수 있다. 예 컨대, 기계 학습을 통해 이미지로부터 사용자의 자세를 추적해내고, 이를 캐릭터 인체 모션 생성 장치로부 터 전달받은 모션 데이터와 동기화하여 카메라의 위치를 추정할 수 있다. 이때, 매 프레임마다 카메라 위치 추 정 동작이 수행되는 것이 아니므로 시간을 많이 들이는 대신 정확도를 보다 높일 수 있는 기계 학습 방법을 사 용할 수 있으며, 예컨대 랜덤 포레스트, 딥러닝등의 방식을 활용해 매우 높은 정확도로 카메라의 위치를 추정할 수 있다. 위의 카메라 자동 추적 및 조정 과정을 통해 VR 플레이어의 초기 위치를 보정하고 나면 예컨대 SLAM(Simultaneous Localization and Mapping) 알고리즘으로부터 카메라를 통해 얻은 이미지 데이터와 모바일 장치의 센서 데이터를 이용하여 모바일 기기의 움직임을 추측해낼 수 있다. 예컨대, 모바일 장치의 움직임과 모바일 장치의 기존 위치로부터 오일러 방법 등을 이용하면 모바일 장치의 현재 위치와 방 향을 얻을 수 있다. 또한, 주기적으로 자동 카메라 조정을 통해 카메라가 실제 위치에서 너무 벗어나는 것을 방지할 수 있다. 이와 같이, 모바일 장치는 주기적으로 카메라의 위치 및 방향을 조정함으로써 위치 오차 를 줄일 수 있다. 또한, VR 게임 등을 체험하는 공간에 복수의 마커가 배치될 수 있으며, 모바일 장치는 VR 장치를 이 용하는 플레이어 사용자가 위치하는 공간 내에 배치된 적어도 하나의 마커에 기초하여 카메라의 위치 및 방향을 조정할 수 있다. VR 체험 공간은 아주 크지 않기 때문에 공간의 각 영역에 배치된 마커를 인식함으로써 카메라 의 위치 및 방향을 용이하게 판단하고 주기적으로 조정할 수 있다.디스플레이부는 사용자에게 VR 콘텐츠 또는 혼합 렌더링 콘텐츠를 시각적으로 제공하기 위한 구성 요소이 다. 예컨대, 디스플레이부는 액정 디스플레이(LCD, liquid crystal display), 발광 다이오드(LED, light emitting diode) 디스플레이, 유기 발광 다이오드(OLED, organic LED) 디스플레이, 마이크로 LED, 마이크로 전 자기계 시스템(MEMS, micro electro mechanical systems) 디스플레이 및 전자 종이(electronic paper) 디스플 레이를 포함할 수 있으며, 이들에 제한되지 않는다. 또한, 이와 같은 디스플레이부는 터치 스크린(touch screen)의 형태로 구현될 수 있다. 마지막으로 3차원 렌더링부는 캐릭터 인체 모션 생성 장치에서 생성된 모션 데이터들과 모바일 장치 에서 트래킹 된 카메라의 위치로부터 3차원 환경을 렌더링하도록 구성된다. 모바일 장치의 성능을 감안하여 디테일 레벨(Level of Detail)을 조절하여 실시간으로 렌더링이 구현될 수 있도록 혼합 렌더링 콘텐츠 화면을 렌더링하며, 모바일 장치의 사용자가 위화감을 느끼지 않도록 네트워크 속도가 충분히 빨라야 한다. 또한, 모바일 장치는 카메라부를 통해 플레이어의 동작을 녹화하도록 구성될 수 있다. 가상 현실을 모바일 장치에서 렌더링 할 때 해당 영상을 녹화하여 콘텐츠로써 활용할 수 있으며, 동시에 VR 플레이어가 게임을 하는 현실 세계를 녹화하여 두 개의 영상 콘텐츠를 생성하고 이를 활용할 수 있도록 구현할 수 있다. 여기서, 두 개의 녹화를 동시에 진행하는 것은 모바일 장치의 성능에 부담이 될 수 있는 작업이므로, 플레 이어의 동작을 녹화한 콘텐츠를 먼저 생성한 후에, 3차원 환경을 렌더링함으로써 영상 콘텐츠를 생성하도록 구 성될 수 있다. 도 3은 본 발명의 일 실시예에 따른 VR 장치에서 수행되는 방법을 설명하기 위한 흐름도이다. VR 장치의 HMD의 디스플레이 화면을 통해 플레이어 사용자에게 VR 게임 화면을 제공할 수 있다.(S310) VR 플레이어 사용자는 VR 장치의 디스플레이 화면을 보면서 게임을 진행하거나, VR 콘텐츠를 체험하며 이에 따라 사용자의 자세 변화나 모션 동작 등이 수행된다. 이와 같은 사용자의 자세 및 모션 변화에 따라 VR 장치의 IMU 센서를 통해 플레이어의 모션 관련 데이터를 수집할 수 있다.(S320) VR 장치의 HMD나 모션 콘트롤러에 내장된 IMU 센서를 통해 플레이어 사용자의 모션 관련 데이터를 측정하고 수집할 수 있다. 또한, VR 장치는 HMD 및 모션 콘트롤러의 IMU 센서와 별도로 플레이어 사용자에 대한 고해상도 인체 모션 데이터를 수집하기 위해 플레이어의 인체 모션을 촬영할 수 있는 모션 캡쳐 카메라를 포함할 수 있으며, 이를 통해 플레이어의 인체 모션 촬영을 수행하고, 고해상도 모션 캡쳐 정보를 획득할 수 있다.(S330) 이와 같이 수집된 플레이어의 모션 관련 정보를 캐릭터 인체 모션 생성 장치로 전송할 수 있다.(S340) 도 4는 본 발명의 일 실시예에 따른 캐릭터 인체 모션 생성 장치에서 수행되는 방법을 설명하기 위한 흐름도이 다. 캐릭터 인체 모션 생성 장치는 가상 현실 장치를 이용하는 플레이어의 모션 관련 정보를 수신할 수 있 다.(S410) 모션 관련 정보는 예컨대 고해상도 인체 모션 데이터와 IMU 센서 데이터를 포함할 수 있다. 캐릭터 인체 모션 생성 장치는 모션 관련 정보에 기초하여 플레이어의 인체 자세를 추정할 수 있다.(S420) 인체 자세 추정부는 VR 장치로 수신한 IMU 센서 정보와 고해상도 인체 모션 데이터 사이의 상관 관계 학습 알고리즘을 수행할 수 있으며, 예컨대 인공 신경망(Artificial Neural Networks, ANN)을 이용하여 기계 학 습을 수행할 수 있다. 인체 자세 추정 이후에 캐릭터의 인체 구조에 적합하도록 3차원 가상 캐릭터의 자세를 맞춤화할 수 있다.(S430) 이와 같은 과정을 통해 생성된 캐릭터의 모션 관련 정보를 모바일 장치로 전송할 수 있다.(S440) 도 5는 본 발명의 일 실시예에 따른 모바일 장치에서 수행되는 방법을 설명하기 위한 흐름도이다. 먼저 모바일 장치는 캐릭터 인체 모션 생성 장치로부터 캐릭터 모션 관련 정보를 수신할 수 있 다.(S510) 여기서, 모바일 장치는 캐릭터 인체 모션 생성 장치로부터 렌더링 환경, 캐릭터 모델, 캐릭터 텍스쳐 등 정보 중 적어도 하나는 미리 수신하고, 실시간 변하는 캐릭터의 위치, 방향 및 모션 관련 정보 중 적 어도 하나는 실시간으로 수신함으로써, 실시간 전송량을 최소화할 수 있다. 모바일 장치에서 촬영한 이미지로부터 카메라의 초기 위치를 자동으로 찾아내어 추정하고, 카메라 위치 보 정을 수행한다.(S520) 여기서, 기계 학습을 통해 이미지로부터 사용자의 자세를 추적해내고, 이를 캐릭터 인체 모션 생성 장치로부터 전달받은 모션 데이터와 동기화하여 카메라의 위치를 추정할 수 있다. 다음으로, 플레이어의 초기 위치를 보정하고 나면 모바일 장치의 움직임 추정을 통해 카메라 트래킹을 수행한다.(S530) 카메라를 통해 얻은 이미지 데이터와 모바일 장치의 센서 데이터 값을 이용하여 모바일 장치의 움직임을 추정할 수 있으며, 이를 통해 모바일 장치의 현재 위치 및 방향을 얻을 수 있게 된 다. 캐릭터 인체 모션 생성 장치로부터 전달받은 모션 데이터들과 모바일 장치에서 트래킹된 카메라의 위 치 정보로부터 3차원 환경의 혼합 렌더링 콘텐츠 생성 및 디스플레이할 수 있다.(S540) 이와 같이 렌더링되는 영상을 녹화하여 VR 콘텐츠로 활용하고 공유할 수 있다. 도 6a 및 도 6b는 본 발명의 일 실시예에 따라 생성된 혼합 렌더링 콘텐츠의 화면을 보여주는 예시도이다. 도 6a를 참조하면, 관찰자 즉 혼합 렌더링 콘텐츠 생성 및 제공자는 자신의 모바일 장치를 이용하여 VR 장 치를 착용한 VR 플레이어를 촬영할 수 있으며, 이때 VR 플레이어의 시점이 아닌 관찰자의 시점에서 VR 플 레이어를 관찰할 수 있도록 가상 현실과 VR 플레이어 캐릭터를 합성한 혼합 렌더링 콘텐츠가 모바일 장치 에 디스플레이될 수 있다. 도 6b를 참조하면 VR 플레이어는 HMD로 구성된 VR 장치 외에 모션 콘트롤러를 손에 쥐고 동작을 취하 고 있으며, 이와 같은 플레이어의 자세 및 모션을 추정하여 생성된 캐릭터가 모바일 장치의 화면에 실시간 생성되어 디스플레이될 수 있다. 이와 같은 본 발명의 실시예에 따른 시스템을 통해 전문적인 CG 촬영 장비가 없이도 손쉽게 VR 플레이어와 VR 세계를 합성하고 영상 스트리밍을 진행할 수 있으며, VR 혼합 렌더링 콘텐츠영상을 쉽게 SNS 등 매체를 통해 공 유함으로써, VR 산업 저변 확대에 긍정적인 영향을 끼칠 수 있을 것이다. 이상 본 발명의 실시예에 따른 방법 및 장치를 구체적인 다양한 실시 형태로서 설명하였으나, 이는 예시에 불과 한 것으로서, 본 발명은 이에 한정되지 않는 것이며, 본 명세서에 개시된 기초 사상에 따르는 최광의 범위를 갖 는 것으로 해석되어야 한다. 당업자는 개시된 실시형태들을 조합, 치환하여 적시되지 않은 형상의 패턴을 실시 할 수 있으나, 이 역시 본 발명의 범위를 벗어나지 않는 것이다. 이외에도 당업자는 본 명세서에 기초하여 개 시된 실시형태를 용이하게 변경 또는 변형할 수 있으며, 이러한 변경 또는 변형도 본 발명의 권리범위에 속함은 명백하다."}
{"patent_id": "10-2019-0090180", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 가상 현실 및 증강 현실을 이용한 혼합 렌더링 콘텐츠 제공 시스템의 구성 을 설명하기 위한 예시도이다. 도 2는 본 발명의 일 실시예에 따른 혼합 렌더링 콘텐츠 제공 시스템의 구성을 설명하기 위한 블록도이다. 도 3은 본 발명의 일 실시예에 따른 VR 장치에서 수행되는 방법을 설명하기 위한 흐름도이다. 도 4는 본 발명의 일 실시예에 따른 캐릭터 인체 모션 생성 장치에서 수행되는 방법을 설명하기 위한 흐름도이 다. 도 5는 본 발명의 일 실시예에 따른 모바일 장치에서 수행되는 방법을 설명하기 위한 흐름도이다. 도 6a 및 도 6b는 본 발명의 일 실시예에 따라 생성된 혼합 렌더링 콘텐츠의 화면을 보여주는 예시도이다."}
