{"patent_id": "10-2019-0035039", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0119394", "출원번호": "10-2019-0035039", "발명의 명칭": "이동 로봇 및 그 제어방법", "출원인": "엘지전자 주식회사", "발명자": "이재광"}}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "본체를 이동시키는 주행부;상기 본체 외부의 지형 정보를 획득하는 라이다 센서;상기 본체 외부의 영상을 획득하는 카메라 센서; 및,상기 라이다 센서의 센싱 데이터에 기초하여 오도메트리(odometry) 정보를 생성하고, 상기 오도메트리 정보를기반으로 상기 카메라 센서로부터 입력되는 영상의 특징점 매칭을 수행하여 현재 위치를 추정하는 제어부;를 포함하는 이동 로봇."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 본체의 이동에 따른 주행 상태를 감지하는 주행 감지 센서;를 더 포함하고,상기 제어부는 상기 주행 감지 센서에서의 센싱 데이터와 상기 라이다 센서의 ICP(Iterative Closest Point) 매칭 결과를 융합하여 상기 오도메트리 정보를 생성하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제어부는, 상기 라이다 센서의 센싱 데이터를 수신하고, 상기 라이다 센서의 센싱 데이터에 기초한 지형 정보 및, 이전 위치 정보를 이용하여 위치 변위량을 판별하는 라이다 서비스 모듈, 및,상기 라이다 서비스 모듈로부터 상기 위치 변위량을 수신하고, 상기 카메라 센서로부터 영상을 수신하여, 상기위치 변위량을 기반으로 현재 영상에서 추출된 특징점과 이전 위치에서 추출된 특징점 매칭을 통해 특징점 위치를 판별하고, 판별된 특징점 위치에 기초하여 상기 현재 위치를 추정하는 비전 서비스 모듈을 포함하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 산출된 현재 위치 정보를 포함하는 노드 정보가 저장되는 저장부;를 더 포함하는 이동 로봇."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 비전 서비스 모듈은 상기 노드 정보를 상기 라이다 서비스 모듈로 송신하고,상기 라이다 서비스 모듈은 상기 노드 정보에 상기 비전 서비스 모듈이 상기 현재 위치를 산출하는 동안에 이동로봇이 이동한 위치 변위량을 반영하여 상기 이동 로봇의 현재 위치를 판별하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,상기 본체의 이동에 따른 주행 상태를 감지하는 주행 감지 센서;를 더 포함하고,상기 제어부는, 상기 주행 감지 센서의 센싱 데이터를 읽어오는 주행 서비스 모듈;을 더 포함하며,공개특허 10-2020-0119394-3-상기 주행 서비스 모듈은 상기 주행 감지 센서의 센싱 데이터를 상기 라이다 서비스 모듈로 전달하고,상기 라이다 서비스 모듈은 상기 주행 감지 센서의 센싱 데이터에 기초한 오도메트리 정보와 상기 라이다 센서의 ICP 결과를 융합하여 상기 오도메트리 정보를 생성하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 제어부는, 조도가 기준치 미만인 영역에서는 상기 라이다 센서의 센싱 데이터에 기초하여 현재 위치를 산출한 후, 상기 조도가 상기 기준치 이상인 영역으로 진입하면 루프 클로징(loop closing)을 수행하여 오차를 보정하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 제어부는, 상기 카메라 센서로부터 입력되는 영상 간의 특징점 매칭이 실패한 경우, 상기 라이다 센서의센싱 데이터에 기초하여 현재 노드와 주변 노드의 ICP(Iterative Closest Point) 매칭을 수행하여 노드간 상관관계를 추가하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "라이다 센서를 통하여 본체 외부의 지형 정보를 획득하는 단계;카메라 센서를 통하여 상기 본체 외부의 영상을 획득하는 단계; 상기 라이다 센서의 센싱 데이터에 기초하여 오도메트리(odometry) 정보를 생성하는 단계;상기 오도메트리 정보를 기반으로 상기 카메라 센서로부터 입력되는 영상의 특징점 매칭을 수행하는 단계; 및,상기 특징점 매칭 결과에 기초하여 현재 위치를 추정하는 단계;를 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 라이다 센서의 센싱 데이터에 기초한 지형 정보에 기초하여 상기 예측된 현재 위치의 불확실성(uncertanity)을 산출하는 단계;를 더 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,주행 감지 센서를 통하여 상기 본체의 이동에 따른 주행 상태를 감지하는 단계; 및,상기 라이다 센서의 센싱 데이터들을 ICP(Iterative Closest Point) 알고리즘에 따라 매칭하는 단계;를 더 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 오도메트리 정보를 생성하는 단계는, 상기 주행 감지 센서에서의 센싱 데이터와 상기 라이다 센서의ICP(Iterative Closest Point) 매칭 결과를 융합하여 상기 오도메트리 정보를 생성하는 것을 특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,상기 오도메트리 정보를 생성하는 단계는, 제어부의 라이다 서비스 모듈이, 상기 라이다 센서의 센싱 데이터를 수신하는 단계, 및공개특허 10-2020-0119394-4-상기 라이다 서비스 모듈이, 상기 지형 정보 및, 이전 위치 정보를 이용하여 위치 변위량을 판별하는 단계를 포함하는 것을 특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 특징점 매칭을 수행하는 단계는,제어부의 비전 서비스 모듈이, 상기 라이다 서비스 모듈로부터 상기 위치 변위량을 수신하는 단계, 상기 비전 서비스 모듈이, 상기 카메라 센서로부터 영상을 수신하는 단계, 및, 상기 비전 서비스 모듈이, 상기 위치 변위량을 기반으로 현재 영상에서 추출된 특징점과 이전 위치에서 추출된특징점 매칭을 통해 특징점 위치를 판별하는 단계를 포함하는 것을 특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 산출된 현재 위치 정보를 포함하는 노드 정보를 저장부에 저장하는 단계;를 더 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 비전 서비스 모듈이 상기 노드 정보를 상기 라이다 서비스 모듈로 송신하는 단계;상기 비전 서비스 모듈이 상기 현재 위치를 산출하는 동안에 이동 로봇이 이동한 위치 변위량을 상기 라이다 서비스 모듈이 산출하는 단계;상기 라이다 서비스 모듈이 상기 산출된 위치 변위량을 상기 노드 정보에 반영하여 상기 이동 로봇의 현재 위치를 판별하는 단계;를 더 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,주행 감지 센서를 통하여 상기 본체의 이동에 따른 주행 상태를 감지하는 단계;를 더 포함하고, 상기 오도메트리 정보를 생성하는 단계는, 상기 라이다 서비스 모듈이 상기 주행 감지 센서의 센싱 데이터에 기초한 오도메트리 정보와 상기 라이다 센서의 ICP 결과를 융합하여 상기 오도메트리 정보를 생성하는 것을 특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 제어부의 주행 서비스 모듈이 상기 주행 감지 센서의 센싱 데이터를 상기 라이다 서비스 모듈로 전달하는단계;를 더 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제9항에 있어서,조도가 기준치 미만인 영역에서 상기 라이다 센서의 센싱 데이터에 기초하여 현재 위치를 산출하는 단계;상기 본체가 이동하여, 상기 조도가 상기 기준치 이상인 영역으로 진입하면 루프 클로징(loop closing)을 수행하여 오차를 보정하는 단계;를 더 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제9항에 있어서,공개특허 10-2020-0119394-5-상기 카메라 센서로부터 입력되는 영상 간의 특징점 매칭이 실패한 경우, 상기 라이다 센서의 센싱 데이터에 기초하여 현재 노드와 주변 노드의 ICP(Iterative Closest Point) 매칭을 수행하여 노드간 상관관계를 추가하는단계;를 더 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2019-0035039", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 측면에 따른 이동 로봇은, 본체를 이동시키는 주행부, 본체 외부의 지형 정보를 획득하는 라이다 센서, 상기 본체 외부의 영상을 획득하는 카메라 센서, 및, 상기 라이다 센서의 센싱 데이터에 기초하여 오도메 트리(odometry) 정보를 생성하고, 상기 오도메트리 정보를 기반으로 상기 카메라 센서로부터 입력되는 영상 간의 특징점 매칭을 수행하여 현재 위치를 추정하는 제어부를 포함함으로써, 카메라 센서와 라이다 센서를 효과적으로 융합하여 위치를 정확하게 추정할 수 있다."}
{"patent_id": "10-2019-0035039", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 이동 로봇 및 그 제어 방법에 관한 것으로서, 더욱 상세하게는 이동 로봇이 맵(map)을 생성, 학습하 거나 맵 상에서 위치를 인식하는 기술에 관한 것이다."}
{"patent_id": "10-2019-0035039", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "로봇은 산업용으로 개발되어 공장 자동화의 일 부분을 담당하여 왔다. 최근에는 로봇을 응용한 분야가 더욱 확 대되어, 의료용 로봇, 우주 항공 로봇 등이 개발되고, 일반 가정에서 사용할 수 있는 가정용 로봇도 만들어지고 있다. 이러한 로봇 중에서 자력으로 주행이 가능한 것을 이동 로봇이라고 한다. 가정에서 사용되는 이동 로봇의 대표적인 예는 로봇 청소기로, 로봇 청소기는 일정 영역을 스스로 주행하면서, 주변의 먼지 또는 이물질을 흡입함으로써, 해당 영역을 청소하는 기기이다. 이동 로봇은, 스스로 이동이 가능하여 이동이 자유롭고, 주행중 장애물 등을 피하기 위한 다수의 센서가 구비되 어 장애물을 피해 주행할 수 있다. 청소 등 설정된 작업을 수행하기 위해서는 주행 구역의 맵(Map)을 정확하게 생성하고, 주행 구역 내의 어느 위 치로 이동하기 위해서 맵 상에서 이동 로봇의 현재 위치를 정확하게 파악할 수 있어야 한다. 또한, 외부적인 요인에 의해 주행 중인 이동 로봇의 위치가 강제로 변경된 경우, 이동 로봇은 직전 위치에서의 주행 정보를 근거로 미지의 현재 위치를 인식할 수 없게 된다. 일 예로, 사용자가 주행 중인 이동 로봇을 들어 서 옮긴 납치(Kidnapping) 상황이 발생한 경우가 그러하다. 이동 로봇의 현재 위치 인식을 위해, 이동 로봇의 연속적인 이동 중 자신의 직전 위치에서의 주행 정보(이동 방 향 및 이동 속도에 대한 정보 또는 연속적으로 촬영되는 바닥사진 등의 비교 등)를 바탕으로 현재 위치를 지속 적으로 파악하는 다양한 방법이 연구되어 왔다. 또한, 이동 로봇이 스스로 맵을 생성하고 학습하는 다양한 방법 에 대한 연구들이 진행되고 있다. 또한, 이동 로봇이 현재 위치에서 카메라를 통해 촬영된 영상을 이용하여 미지의 현재 위치를 인식하는 기술들 이 제안되고 있다. 선행 문헌(한국 공개특허공보 제10-2010-0104581호 공개일자 2010. 9. 29.)은, 주행 구역 내에서 촬영된 영상으 로부터 추출된 특징점들로부터 3차원 지도를 생성하고 현재 위치에서 카메라를 통해 촬영된 영상에 기초한 특징 점을 이용하여 미지의 현재 위치를 인식하는 기술을 개시한다. 상기 선행 문헌에서는, 주행 구역 내에서 촬영된 영상으로부터 추출된 특징점 들로부터 3차원 지도를 생성하고, 미지의 현재 위치에서 촬영된 영상 내의 특징점들 중 3차원 지도 내의 특징점들과 매칭되는 3쌍 이상의 특징점 쌍을 검출한다. 그 후, 현재 위치에서 촬영된 영상 내의 매칭된 3개 이상의 특징점의 2차원 좌표, 3차원 지도 상의 매칭된 3개 이상의 특징점의 3차원 좌표, 및 현재 위치에서 카메라의 초점거리 정보를 이용하여, 상기 매 칭된 3개 이상의 특징점으로부터 거리를 산출하고, 이로부터 현재 위치를 인식하는 기술이, 상기 선행 문헌에 개시되어 있다. 상기 선행 문헌과 같이, 주행 구역 상의 같은 부분을 촬영한 어느 한 영상과 인식 영상을 비교하여 특정 포인트 (point)의 특징점으로 위치를 인식하는 방식은, 주행 구역 내의 조명 온/오프(On/Off) 여부나 태양광의 입사각 이나 양에 따른 조도 변화 등 환경 변화에 의해 현재 위치의 추정 정확도가 달라질 수 있는 문제가 있다."}
{"patent_id": "10-2019-0035039", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "선행 문헌과 같이, 주행 구역 상의 같은 부분을 촬영한 어느 한 영상과 인식 영상을 비교하여 특정 포인트 (point)의 특징점으로 위치를 인식하는 방식은, 주행 구역 내의 조명 온/오프(On/Off) 여부나 태양광의 입사각 이나 양에 따른 조도 변화, 물품 위치 변경 등 환경 변화에 의해 현재 위치의 추정 정확도가 달라질 수 있는 문 제가 있다. 본 발명의 목적은 이러한 환경 변화에 강인한 위치 인식 및 맵 생성 기술을 제공함에 있다. 본 발명의 목적은 이동 로봇의 현재 위치 인식의 성공률을 높이고 더 높은 신뢰도로 현재 위치를 추정하게 하여, 효율적이고 정확한 주행 구역 내 위치 인식 기술을 제공함에 있다. 본 발명의 목적은 이종의 센서를 활용하여 획득한 이종 데이터를 상호 보완적으로 이용할 수 있는 슬램 (Simultaneous localization and mapping: SLAM) 기술을 제공함에 있다. 본 발명의 목적은, 카메라를 이용하는 비전(vision) 기반의 위치 인식과 레이저를 이용하는 라이다(Light Detection And Ranging: LiDAR) 기반의 위치 인식 기술을 효과적으로 융합하여 조도 변화, 물품 위치 변경 등 환경 변화에 강인한 슬램 기술을 제공함에 있다."}
{"patent_id": "10-2019-0035039", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇 및 그 제어 방법은, 이종의 센서를 활용하여 획득한 이종 데이터를 상호 보완적으로 이용하여 환경 변화에 강인한 맵을 생성하고 맵 상 위치를 정 확하게 인식할 수 있다. 상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇 및 그 제어 방법은, 카메라를 이용 하는 비전(vision) 기반의 위치 인식과 레이저를 이용하는 라이다(Light Detection And Ranging: LiDAR) 기반 의 위치 인식 기술을 효과적으로 융합하여 조도 변화, 물품 위치 변경 등 환경 변화에 강인한 슬램 기술을 구현 할 수 있다. 상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇 및 그 제어 방법은, 다양한 환경 변 화에 대응할 수 있는 하나의 맵에 기초하여 효율적인 주행 및 청소를 수행할 수 있다. 상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇은, 본체를 이동시키는 주행부, 상기 본체 외부의 지형 정보를 획득하는 라이다 센서, 상기 본체 외부의 영상을 획득하는 카메라 센서, 및, 상기 라 이다 센서의 센싱 데이터에 기초하여 오도메트리(odometry) 정보를 생성하고, 상기 오도메트리 정보를 기반으로 상기 카메라 센서로부터 입력되는 영상의 특징점 매칭을 수행하여 현재 위치를 추정하는 제어부를 포함함으로써, 카메라 센서와 라이다 센서를 효과적으로 융합하여 위치를 정확하게 추정할 수 있다. 또한, 본 발명의 일 측면에 따른 이동 로봇은, 본체의 이동에 따른 주행 상태를 감지하는 주행 감지 센서를 더 포함하고, 상기 제어부는 상기 주행 감지 센서에서의 센싱 데이터와 상기 라이다 센서의 ICP(Iterative Closest Point) 매칭 결과를 융합하여 상기 오도메트리 정보를 생성할 수 있다. 상기 제어부는, 상기 라이다 센서의 센싱 데이터를 수신하고, 상기 라이다 센서의 센싱 데이터에 기초한 지형 정보 및, 이전 위치 정보를 이용하여 위치 변위량을 판별하는 라이다 서비스 모듈, 및, 상기 라이다 서비스 모 듈로부터 상기 위치 변위량을 수신하고, 상기 카메라 센서로부터 영상을 수신하여, 상기 위치 변위량을 기반으 로 현재 영상에서 추출된 특징점과 이전 위치에서 추출된 특징점 매칭을 통해 특징점 위치를 판별하고, 판별된 특징점 위치에 기초하여 상기 현재 위치를 추정하는 비전 서비스 모듈을 포함할 수 있다. 또한, 본 발명의 일 측면에 따른 이동 로봇은, 상기 산출된 현재 위치 정보를 포함하는 노드 정보, 노드 정보를 포함하는 맵(map)이 저장되는 저장부를 더 포함할 수 있다. 상기 비전 서비스 모듈은 상기 노드 정보를 상기 라이다 서비스 모듈로 송신하고, 상기 라이다 서비스 모듈은 상기 노드 정보에 상기 비전 서비스 모듈이 상기 현재 위치를 산출하는 동안에 이동 로봇이 이동한 위치 변위량 을 반영하여 상기 이동 로봇의 현재 위치를 판별할 수 있다. 또한, 상기 본체의 이동에 따른 주행 상태를 감지하는 주행 감지 센서를 구비한 경우에, 상기 제어부는, 상기 주행 감지 센서의 센싱 데이터를 읽어오는 주행 서비스 모듈을 더 포함하며, 상기 주행 서비스 모듈은 상기 주 행 감지 센서의 센싱 데이터를 상기 라이다 서비스 모듈로 전달하고, 상기 라이다 서비스 모듈은 상기 주행 감 지 센서의 센싱 데이터에 기초한 오도메트리 정보와 상기 라이다 센서의 ICP 결과를 융합하여 상기 오도메트리정보를 생성할 수 있다. 본 발명의 일 측면에 따르면, 제어부가 조도가 기준치 미만인 영역에서는 상기 라이다 센서의 센싱 데이터에 기 초하여 현재 위치를 산출한 후, 상기 조도가 상기 기준치 이상인 영역으로 진입하면 루프 클로징(loop closin g)을 수행하여 오차를 보정할 수 있다. 또한, 상기 제어부는, 상기 카메라 센서로부터 입력되는 영상 간의 특징점 매칭이 실패한 경우, 상기 라이다 센 서의 센싱 데이터에 기초하여 현재 노드와 주변 노드의 ICP(Iterative Closest Point) 매칭을 수행하여 노드간 상관관계를 추가할 수 있다. 상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇의 제어 방법은, 라이다 센서를 통하 여 본체 외부의 지형 정보를 획득하는 단계, 카메라 센서를 통하여 상기 본체 외부의 영상을 획득하는 단계, 상 기 라이다 센서의 센싱 데이터에 기초하여 오도메트리(odometry) 정보를 생성하는 단계, 상기 오도메트리 정보 를 기반으로 상기 카메라 센서로부터 입력되는 영상의 특징점 매칭을 수행하는 단계, 및, 상기 특징점 매칭 결 과에 기초하여 현재 위치를 추정하는 단계를 포함할 수 있다. 또한, 상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇의 제어 방법은, 상기 라이다 센서의 센싱 데이터에 기초한 지형 정보에 기초하여 상기 예측된 현재 위치의 불확실성(uncertanity)을 산출할 수 있다. 또한, 상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇의 제어 방법은, 주행 감지 센 서를 통하여 상기 본체의 이동에 따른 주행 상태를 감지하는 단계, 및, 상기 라이다 센서의 센싱 데이터들을 ICP(Iterative Closest Point) 알고리즘에 따라 매칭하는 단계를 더 포함할 수 있다. 여기서, 오도메트리 정보를 생성하는 단계는, 상기 주행 감지 센서에서의 센싱 데이터와 상기 라이다 센서의 ICP(Iterative Closest Point) 매칭 결과를 융합하여 상기 오도메트리 정보를 생성할 수 있다. 한편, 오도메트리 정보를 생성하는 단계는, 제어부의 라이다 서비스 모듈이, 상기 라이다 센서의 센싱 데이터를 수신하는 단계, 및, 상기 라이다 서비스 모듈이, 상기 지형 정보 및, 이전 위치 정보를 이용하여 위치 변위량을 판별하는 단계를 포함할 수 있다. 여기서, 특징점 매칭을 수행하는 단계는, 제어부의 비전 서비스 모듈이, 상기 라이다 서비스 모듈로부터 상기 위치 변위량을 수신하는 단계, 상기 비전 서비스 모듈이, 상기 카메라 센서로부터 영상을 수신하는 단계, 및, 상기 비전 서비스 모듈이, 상기 위치 변위량을 기반으로 현재 영상에서 추출된 특징점과 이전 위치에서 추출된 특징점 매칭을 통해 특징점 위치를 판별하는 단계를 포함할 수 있다. 한편, 상기 산출된 현재 위치 정보를 포함하는 노드 정보를 저장부에 저장하고 맵에 등록할 수 있다. 또한, 상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇의 제어 방법은, 상기 비전 서 비스 모듈이 상기 노드 정보를 상기 라이다 서비스 모듈로 송신하는 단계, 상기 비전 서비스 모듈이 상기 현재 위치를 산출하는 동안에 이동 로봇이 이동한 위치 변위량을 상기 라이다 서비스 모듈이 산출하는 단계, 상기 라 이다 서비스 모듈이 상기 산출된 위치 변위량을 상기 노드 정보에 반영하여 상기 이동 로봇의 현재 위치를 판별 하는 단계를 더 포함할 수 있다. 또한, 주행 감지 센서를 통하여 상기 본체의 이동에 따른 주행 상태를 감지하는 경우에, 오도메트리 정보를 생 성하는 단계는, 상기 라이다 서비스 모듈이 상기 주행 감지 센서의 센싱 데이터에 기초한 오도메트리 정보와 상 기 라이다 센서의 ICP 결과를 융합하여 상기 오도메트리 정보를 생성할 수 있다. 또한, 상기 제어부의 주행 서비스 모듈은 상기 주행 감지 센서의 센싱 데이터를 상기 라이다 서비스 모듈로 전 달할 수 있다. 또한, 상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇의 제어 방법은, 조도가 기준 치 미만인 영역에서 상기 라이다 센서의 센싱 데이터에 기초하여 현재 위치를 산출하는 단계, 상기 본체가 이동 하여, 상기 조도가 상기 기준치 이상인 영역으로 진입하면 루프 클로징(loop closing)을 수행하여 오차를 보정 하는 단계를 더 포함할 수 있다. 또한, 상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇의 제어 방법은, 상기 카메라 센서로부터 입력되는 영상 간의 특징점 매칭이 실패한 경우, 상기 라이다 센서의 센싱 데이터에 기초하여 현재 노드와 주변 노드의 ICP(Iterative Closest Point) 매칭을 수행하여 노드간 상관관계를 추가하는 단계를 더 포함할 수 있다."}
{"patent_id": "10-2019-0035039", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들 중 적어도 하나에 의하면, 이종의 센서를 활용하여 획득한 이종 데이터를 융합하여 조명, 조도, 시간대, 사물 위치 변화 등의 다양한 환경 변화에 대해 강인한 맵을 생성할 수 있다. 또한, 본 발명의 실시예들 중 적어도 하나에 의하면, 이종의 센서를 활용하여 획득한 이종 데이터를 상호 보완 적으로 이용하여 다양한 환경 변화에 강인한 맵 상 이동 로봇의 위치를 정확하게 인식할 수 있다. 또한, 본 발명의 실시예들 중 적어도 하나에 의하면, 카메라를 이용하는 비전(vision) 기반의 위치 인식과 레이 저를 이용하는 라이다(LiDAR) 기반의 위치 인식 기술을 효과적으로 융합하여 조도 변화, 물품 위치 변경 등 환 경 변화에 강인한 슬램 기술을 구현할 수 있다. 또한, 본 발명의 실시예들 중 적어도 하나에 의하면, 다양한 환경 변화에 대응할 수 있는 하나의 맵과 정확한 위치 인식에 기초하여 효율적인 주행 및 청소를 수행할 수 있다. 한편, 그 외의 다양한 효과는 후술될 본 발명의 실시예에 따른 상세한 설명에서 직접적 또는 암시적으로 개시될 것이다."}
{"patent_id": "10-2019-0035039", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참조하여 본 발명의 실시예를 상세하게 설명한다. 그러나 본 발명이 이러한 실시예에 한정되는 것은 아니며 다양한 형태로 변형될 수 있음은 물론이다. 한편, 이하의 설명에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 단순히 본 명세서 작성의 용이함만 이 고려되어 부여되는 것으로서, 그 자체로 특별히 중요한 의미 또는 역할을 부여하는 것은 아니다. 따라서, 상 기 \"모듈\" 및 \"부\"는 서로 혼용되어 사용될 수도 있다. 또한, 본 명세서에서, 다양한 요소들을 설명하기 위해 제1, 제2 등의 용어가 이용될 수 있으나, 이러한 요소들 은 이러한 용어들에 의해 제한되지 아니한다. 이러한 용어들은 한 요소를 다른 요소로부터 구별하기 위해서만 이용된다. 본 발명의 일 실시예에 따른 이동 로봇은 바퀴 등을 이용하여 스스로 이동이 가능한 로봇을 의미하고, 가 정 도우미 로봇 및 로봇 청소기 등이 될 수 있다. 이하에서는, 도면들을 참조하여, 이동 로봇 중 청소 기능을 가지는 로봇 청소기를 예로 들어 설명하나, 본 발명은 이에 한정되지 않는다. 도 1은 본 발명의 일 실시예에 따른 이동 로봇 및 이동 로봇을 충전시키는 충전대를 도시한 사시도이다. 도 2는 도 1에 도시된 이동 로봇의 상면부를 도시한 도이고, 도 3은 도 1에 도시된 이동 로봇의 정면부를 도시 한 도이며, 도 4는 도 1에 도시된 이동 로봇의 저면부를 도시한 도이다. 도 5는 본 발명의 실시예에 따른 이동 로봇의 주요 구성들 간의 제어관계를 도시한 블록도이다. 도 1 내지 도 5를 참조하면, 이동 로봇은 본체를 이동시키는 주행부를 포함한다. 주행부는 본체를 이동시키는 적어도 하나의 구동 바퀴를 포함한다. 주행부는 구동 바퀴에 연결되어 구동 바퀴를 회전시키는 구동 모터(미도시)를 포함한다. 예를 들어, 구동 바퀴는 본체의 좌, 우 측에 각각 구비될 수 있으며, 이하, 각각 좌륜(136(L))과 우륜(136(R))이라고 한다. 좌륜(136(L))과 우륜(136(R))은 하나의 구동 모터에 의해 구동될 수도 있으나, 필요에 따라 좌륜(136(L))을 구 동시키는 좌륜 구동 모터와 우륜(136(R))을 구동시키는 우륜 구동 모터가 각각 구비될 수도 있다. 좌륜(136 (L))과 우륜(136(R))의 회전 속도에 차이를 두어 좌측 또는 우측으로 본체의 주행방향을 전환할 수 있다. 이동 로봇은 소정의 서비스를 제공하기 위한 서비스부를 포함한다. 도 1 내지 도 5에서는 서비스부 가 청소 작업을 수행하는 것으로 예를 들어 본 발명을 설명하나, 본 발명은 이에 한정되지 않는다. 예를 들어, 서비스부는 청소(비질, 흡입청소, 걸레질 등), 설거지, 요리, 빨래, 쓰레기 처리 등의 가사 서비스 를 사용자에게 제공하도록 구비될 수 있다. 또 다른 예로, 서비스부는 주변의 외부 침입자나 위험 상황 등 을 감지하는 보안 기능을 수행할 수 있다. 이동 로봇은 주행 구역을 이동하며 서비스부에 의해 바닥을 청소할 수 있다. 서비스부는, 이물 질을 흡입하는 흡입 장치, 비질을 수행하는 브러시(184, 185), 흡입장치나 브러시에 의해 수거된 이물질을 저장 하는 먼지통(미도시) 및/또는 걸레질을 수행하는 걸레부(미도시) 등을 포함할 수 있다. 본체의 저면부에는 공기의 흡입이 이루어지는 흡입구(150h)가 형성될 수 있으며, 본체 내에는 흡입구 (150h)를 통해 공기가 흡입될 수 있도록 흡입력을 제공하는 흡입장치(미도시)와, 흡입구(150h)를 통해 공기와 함께 흡입된 먼지를 집진하는 먼지통(미도시)이 구비될 수 있다. 본체는 이동 로봇을 구성하는 각종 부품들이 수용되는 공간을 형성하는 케이스를 포함할 수 있 다. 케이스에는 상기 먼지통의 삽입과 탈거를 위한 개구부가 형성될 수 있고, 개구부를 여닫는 먼지통 커 버가 케이스에 대해 회전 가능하게 구비될 수 있다. 흡입구(150h)를 통해 노출되는 솔들을 갖는 롤형의 메인 브러시와, 본체의 저면부 전방측에 위치하며, 방사상으로 연장된 다수개의 날개로 이루어진 솔을 갖는 보조 브러시가 구비될 수 있다. 이들 브러시(154, 155)들의 회전에 의해 주행 구역 내 바닥으로부터 먼지들이 분리되며, 이렇게 바닥으로부터 분리된 먼지들은 흡입구(150h)를 통해 흡입되어 먼지통에 모인다. 배터리는 구동 모터뿐만 아니라, 이동 로봇의 작동 전반에 필요한 전원을 공급한다. 배터리가 방전될 시, 이동 로봇은 충전을 위해 충전대로 복귀하는 주행을 실시할 수 있으며, 이러한 복귀 주행 중, 이동 로봇은 스스로 충전대의 위치를 탐지할 수 있다. 충전대는 소정의 복귀 신호를 송출하는 신호 송출부(미도시)를 포함할 수 있다. 복귀 신호는 초음파 신호 또는 적외선 신호일 수 있으나, 반드시 이에 한정되는 것은 아니다. 이동 로봇은 복귀 신호를 수신하는 신호 감지부(미도시)를 포함할 수 있다. 충전대는 신호 송출부를 통해 적외선 신호를 송출하고, 신호 감지부는 적외선 신호를 감지하는 적외선 센서를 포함할 수 있다. 이동 로 봇은 충전대로부터 송출된 적외선 신호에 따라 충전대의 위치로 이동하여 충전대와 도킹 (docking)한다. 이러한 도킹에 의해 이동 로봇의 충전 단자와 충전대의 충전 단자 간에 충 전에 이루어진다. 이동 로봇은 이동 로봇의 내/외부의 정보를 감지하는 센싱부를 포함할 수 있다. 예를 들어, 센싱부는 주행 구역에 대한 각종 정보를 감지하는 하나 이상의 센서(171, 175), 주행 구역에 대한 영상 정보를 획득하는 영상획득부를 포함할 수 있다. 실시예에 따라서, 영상획득부는 센싱부 외부에 별도로 구비될 수 있다. 이동 로봇은 센싱부가 감지한 정보를 통해, 주행 구역을 맵핑(Mapping)할 수 있다. 예를 들어, 이동 로봇은 영상획득부가 획득한 주행 구역의 천장 영상에 기초하여 비전(vision) 기반의 위치 인식 및 맵 생성을 수행할 수 있다. 또한, 이동 로봇은 레이저를 이용하는 라이다(Light Detection And Ranging:LiDAR) 센서 기반의 위치 인식 및 맵 생성을 수행할 수 있다. 더욱 바람직하게는 본 발명에 따른 이동 로봇은 카메라를 이용하는 비전 기반의 위치 인식과 레이저를 이 용하는 라이다 기반의 위치 인식 기술을 효과적으로 융합하여 조도 변화, 물품 위치 변경 등 환경 변화에 강인 한 위치 인식 및 맵 생성을 수행할 수 있다. 한편, 영상획득부는 주행 구역을 촬영하는 것으로, 본체 외부의 영상을 획득하는 하나 이상의 카메라 센서를 포함할 수 있다. 또한, 영상획득부는 카메라 모듈을 포함할 수 있다. 상기 카메라 모듈은 디지털 카메라를 포함할 수 있다. 디지털 카메라는 적어도 하나의 광학렌즈와, 광학렌즈를 통과한 광에 의해 상이 맺히는 다수개의 광다이오드 (photodiode, 예를 들어, pixel)를 포함하여 구성된 이미지센서(예를 들어, CMOS image sensor)와, 광다이오드 들로부터 출력된 신호를 바탕으로 영상을 구성하는 디지털 신호 처리기(DSP: Digital Signal Processor)를 포함 할 수 있다. 디지털 신호 처리기는 정지영상은 물론이고, 정지영상으로 구성된 프레임들로 이루어진 동영상을 생성하는 것도 가능하다. 본 실시예에서 영상획득부는, 본체 전방의 영상을 획득하도록 구비되는 전면 카메라 센서(120a)와 본 체의 상면부에 구비되어, 주행 구역 내의 천장에 대한 영상을 획득하는 상부 카메라 센서(120b)를 구비하 나, 영상획득부의 위치와 촬영범위가 반드시 이에 한정되어야 하는 것은 아니다. 예를 들어, 이동 로봇은 주행 구역 내의 천장에 대한 영상을 획득하는 상부 카메라 센서(120b)만 구비하여, 비전(vision) 기반의 위치 인식 및 주행을 수행할 수 있다. 또는, 본 발명의 일 실시예에 따른 이동 로봇의 영상획득부는, 본체의 일면에 대하여 경사지게 배치되어 전방과 상방을 함께 촬영하도록 구성된 카메라 센서(미도시)를 포함할 수 있다. 즉, 하나의 카메라 센 서로 전방과 상방을 함께 촬영할 수 있다. 이 경우에 제어부는 카메라가 촬영하여 획득한 영상에서 전방 영상과 상방 영상을 화각을 기준으로 분리할 수 있다. 분리된 전방 영상은 전면 카메라 센서(120a)에서 획득된 영상과 같이 비전(vision) 기반의 사물 인식에 사용될 수 있다. 또한, 분리된 상방 영상은 상부 카메라 센서 (120b)에서 획득된 영상과 같이 비전(vision) 기반의 위치 인식 및 주행에 사용될 수 있다. 본 발명에 따른 이동 로봇은 주변의 이미지를 이미지 기반의 기저장된 정보와 비교하거나 획득되는 이미지 들을 비교하여 현재 위치를 인식하는 비전 슬램을 수행할 수 있다. 한편, 영상획득부는 전면 카메라 센서(120a) 및/또는 상부 카메라 센서(120b)를 복수개 구비하는 것도 가 능하다. 또는 영상획득부는 전방과 상방을 함께 촬영하도록 구성된 카메라 센서(미도시)를 복수개 구비하 는 것도 가능하다. 본 실시예의 경우, 이동 로봇의 일부 부위(ex, 전방, 후방, 저면)에 카메라가 설치되어 있으며, 청소 시에 촬상 영상을 지속적으로 획득할 수 있다. 이러한 카메라는 촬영 효율을 위해 각 부위별로 여러 개가 설치될 수도 있 다. 카메라에 의해 촬상된 영상은 해당 공간에 존재하는 먼지, 머리카락, 바닥 등과 같은 물질의 종류 인식,청 소 여부, 또는 청소 시점을 확인하는데 사용할 수 있다. 전면 카메라 센서(120a)는 이동 로봇의 주행 방향 전면에 존재하는 장애물 또는 청소 영역의 상황을 촬영 할 수 있다. 본 발명의 일 실시예에 따르면, 상기 영상획득부는 본체 주변을 연속적으로 촬영하여 복수의 영상을 획득할 수 있고, 획득된 복수의 영상은 저장부에 저장될 수 있다. 이동 로봇은 복수의 영상을 이용하여 장애물 인식의 정확성을 높이거나, 복수의 영상 중 하나 이상의 영상 을 선택하여 효과적인 데이터를 사용함으로써 장애물 인식의 정확성을 높일 수 있다. 센싱부는 레이저를 이용하여 본체 외부의 지형 정보를 획득하는 라이다 센서를 포함할 수 있다. 라이다 센서는 레이저를 출력하여 레이저를 반사시킨 객체의 거리, 위치 방향, 재질 등의 정보를 제공하며 주행 구역의 지형 정보를 획득할 수 있다. 이동 로봇은 라이다 센서로 360도의 지형(Geometry) 정보 를 얻을 수 있다. 본 발명의 일 실시예에 따른 이동 로봇은 라이다 센서가 센싱한 객체들의 거리와 위치, 방향 등을 파 악하여 맵을 생성할 수 있다.본 발명의 일 실시예에 따른 이동 로봇은 외부에서 반사되어 수신되는 레이저의 시간차 또는 신호 강도 등 레이저 수신 패턴을 분석하여 주행 구역의 지형 정보를 획득할 수 있다. 또한, 이동 로봇은 라이다 센서 를 통하여 획득한 지형 정보를 이용하여 맵을 생성할 수 있다. 예를 들어, 본 발명에 따른 이동 로봇은 라이다 센서를 통하여 현재 위치에서 획득된 주변 지형 정보 를 라이다 센서 기반의 기저장된 지형 정보와 비교하거나 획득되는 지형 정보들을 비교하여 현재 위치를 인식하 는 라이다 슬램을 수행할 수 있다. 더욱 바람직하게는, 본 발명에 따른 이동 로봇은 카메라를 이용하는 비전 기반의 위치 인식과 레이저를 이 용하는 라이다 기반의 위치 인식 기술을 효과적으로 융합하여 조도 변화, 물품 위치 변경 등 환경 변화에 강인 한 위치 인식 및 맵 생성을 수행할 수 있다. 이와 같이 비전 슬램과 라이다 슬램을 융합한 슬램 기술에 대해서는 도 6 내지 도 20을 참조하여 상세히 후술할 것이다. 한편, 센싱부는 이동 로봇의 동작, 상태와 관련된 각종 데이터를 센싱하는 센서들(171, 172, 179)을 포함 할 수 있다. 예를 들어, 상기 센싱부는 전방의 장애물을 감지하는 장애물 감지센서를 포함할 수 있다. 또한, 상기 센싱부는 주행 구역 내 바닥에 낭떠러지의 존재 여부를 감지하는 낭떠러지 감지센서와, 바닥의 영상 을 획득하는 하부 카메라 센서를 더 포함할 수 있다. 도 1과 도 3을 참조하면, 상기 장애물 감지센서는 이동 로봇의 외주면에 일정 간격으로 설치되는 복 수의 센서를 포함할 수 있다. 상기 장애물 감지센서는, 적외선 센서, 초음파 센서, RF 센서, 지자기 센서, PSD(Position Sensitive Device) 센서 등을 포함할 수 있다. 한편, 상기 장애물 감지센서에 포함되는 센서의 위치와 종류는 이동 로봇의 기종에 따라 달라질 수 있고, 상기 장애물 감지센서는 더 다양한 센서를 포함할 수 있다. 상기 장애물 감지센서는 실내의 벽이나 장애물과의 거리를 감지하는 센서로, 본 발명은 그 종류에 한정되 지 않으나, 이하에서는 초음파 센서를 예시하여 설명한다. 상기 장애물 감지센서는 이동 로봇의 주행(이동) 방향에 존재하는 물체, 특히 장애물을 감지하여 장애물 정보를 제어부에 전달한다. 즉, 상기 장애물 감지센서는, 이동 로봇의 이동 경로, 전방이나 측면에 존재하는 돌출물, 집안의 집기, 가구, 벽면, 벽 모서리 등을 감지하여 그 정보를 제어 유닛에 전달할 수 있다. 이때, 제어부는 초음파 센서를 통해 수신된 적어도 2 이상의 신호에 기초하여 장애물의 위치를 감지하고, 감지된 장애물의 위치에 따라 이동 로봇의 움직임을 제어할 수 있다. 실시예에 따라서는, 케이스의 외측면에 구비되는 장애물 감지 센서는 발신부와 수신부를 포함하여 구 성될 수 있다. 예를 들어, 초음파 센서는 적어도 하나 이상의 발신부 및 적어도 2 이상의 수신부가 서로 엇갈리도록 구비될 수 있다. 이에 따라, 다양한 각도로 신호를 방사하고, 장애물에 반사된 신호를 다양한 각도에서 수신할 수 있다. 실시예에 따라서는, 장애물 감지센서에서 수신된 신호는, 증폭, 필터링 등의 신호 처리 과정을 거칠 수 있 고, 이후 장애물까지의 거리 및 방향이 산출될 수 있다. 한편, 상기 센싱부는 본체의 구동에 따른 이동 로봇의 주행 동작을 감지하고 동작 정보를 출력 하는 주행 감지 센서를 더 포함할 수 있다. 주행 감지 센서로는, 자이로 센서(Gyro Sensor), 휠 센서(Wheel Sensor), 가속도 센서(Acceleration Sensor) 등을 사용할 수 있다. 주행 감지 센서 중 적어도 하나에서 감지되 는 데이터 또는 주행 감지 센서 중 적어도 하나에서 감지되는 데이터에 기초하여 산출되는 데이터는 오도메트리 (odometry) 정보를 구성할 수 있다. 자이로 센서는, 이동 로봇이 운전 모드에 따라 움직일 때 회전 방향을 감지하고 회전각을 검출한다. 자이 로 센서는, 이동 로봇의 각속도를 검출하여 각속도에 비례하는 전압 값을 출력한다. 제어부는 자이로 센서로부터 출력되는 전압 값을 이용하여 회전 방향 및 회전각을 산출한다.휠 센서는, 좌륜(136(L))과 우륜(136(R))에 연결되어 바퀴의 회전수를 감지한다. 여기서, 휠 센서는 엔코더 (Encoder)일 수 있다. 엔코더는 좌륜(136(L))과 우륜(136(R))의 회전수를 감지하여 출력한다. 제어부는 회전수를 이용하여 좌, 우측 바퀴의 회전 속도를 연산할 수 있다. 또한, 제어부는 좌륜 (136(L))과 우륜(136(R))의 회전수 차이를 이용하여 회전각을 연산할 수 있다. 가속도 센서는, 이동 로봇의 속도 변화, 예를 들어, 출발, 정지, 방향 전환, 물체와의 충돌 등에 따른 이 동 로봇의 변화를 감지한다. 가속도 센서는 주 바퀴나 보조바퀴의 인접 위치에 부착되어, 바퀴의 미끄러짐 이나 공회전을 검출할 수 있다. 또한, 가속도 센서는 제어부에 내장되어 이동 로봇의 속도 변화를 감지할 수 있다. 즉, 가속도 센서 는 속도 변화에 따른 충격량을 검출하여 이에 대응하는 전압 값을 출력한다. 따라서, 가속도 센서는 전자식 범 퍼의 기능을 수행할 수 있다. 제어부는 주행 감지 센서로부터 출력된 동작 정보에 기초하여 이동 로봇의 위치 변화를 산출할 수 있 다. 이러한 위치는 영상 정보를 이용한 절대 위치에 대응하여 상대 위치가 된다. 이동 로봇은 이러한 상대 위치 인식을 통해 영상 정보와 장애물 정보를 이용한 위치 인식의 성능을 향상시킬 수 있다. 한편, 이동 로봇은 충전 가능한 배터리를 구비하여 로봇 청소기 내로 전원을 공급하는 전원 공급부 (미도시)를 포함할 수 있다. 상기 전원 공급부는 이동 로봇의 각 구성 요소들에 구동 전원과, 동작 전원을 공급하며, 전원 잔량이 부족 하면 충전대에서 충전 전류를 공급받아 충전될 수 있다. 이동 로봇은 배터리의 충전 상태를 감지하고, 감지 결과를 제어부에 전송하는 배터리 감지부(미 도시)를 더 포함할 수 있다. 배터리는 배터리 감지부와 연결되어 배터리 잔량 및 충전 상태가 제어부(14 0)에 전달된다. 배터리 잔량은 출력부(미도시)의 화면에 표시될 수 있다. 또한, 이동 로봇은 온/오프(On/Off) 또는 각종 명령을 입력할 수 있는 조작부를 포함한다. 조작부 를 통해 이동 로봇의 작동 전반에 필요한 각종 제어명령을 입력받을 수 있다. 또한, 이동 로봇 은 출력부(미도시)를 포함하여, 예약 정보, 배터리 상태, 동작모드, 동작상태, 에러상태 등을 표시할 수 있다. 도 5를 참조하면, 이동 로봇은 현재 위치를 인식하는 등 각종 정보를 처리하고 판단하는 제어부, 및 각종 데이터를 저장하는 저장부를 포함한다. 또한, 이동 로봇은 다른 기기와 데이터를 송수신하는 통 신부를 더 포함할 수 있다. 이동 로봇과 통신하는 기기 중 외부 단말기는 이동 로봇을 제어하기 위한 애플리케이션을 구비하고, 애플리케이션의 실행을 통해 이동 로봇이 청소할 주행 구역에 대한 맵을 표시하고, 맵 상에 특정 영역을 청소하도록 영역을 지정할 수 있다. 외부 단말기는 맵 설정을 위한 애플리케이션(application)이 탑재된 리모콘, PDA, 랩탑(laptop), 스마트 폰, 태블릿 등을 예로 들 수 있다. 외부 단말기는 이동 로봇과 통신하여, 맵과 함께 이동 로봇의 현재 위치를 표시할 수 있으며, 복수의 영역 에 대한 정보가 표시될 수 있다. 또한, 외부 단말기는 이동 로봇의 주행에 따라 그 위치를 갱신하여 표시한다. 제어부는 이동 로봇을 구성하는 센싱부, 조작부, 주행부를 제어하여, 이동 로봇(10 0)의 동작 전반을 제어한다. 저장부는 이동 로봇의 제어에 필요한 각종 정보들을 기록하는 것으로, 휘발성 또는 비휘발성 기록 매 체를 포함할 수 있다. 기록 매체는 마이크로 프로세서(micro processor)에 의해 읽힐 수 있는 데이터를 저장한 것으로, 그 종류나 구현 방식에 한정되지 않는다. 또한, 저장부에는 주행 구역에 대한 맵(Map)이 저장될 수 있다. 맵은 이동 로봇과 유선 또는 무선 통 신을 통해 정보를 교환할 수 있는 외부 단말기, 서버 등에 의해 입력된 것일 수도 있고, 이동 로봇이 스스 로 학습을 하여 생성한 것일 수도 있다. 맵에는 주행 구역 내의 방들의 위치가 표시될 수 있다. 또한, 이동 로봇의 현재 위치가 맵 상에 표시될 수 있으며, 맵 상에서의 이동 로봇의 현재의 위치는 주행 과정에서 갱신될 수 있다. 외부 단말기는 저장부 에 저장된 맵과 동일한 맵을 저장한다. 상기 저장부는 청소 이력 정보를 저장할 수 있다. 이러한 청소 이력 정보는 청소를 수행할 때마다 생성될 수 있다. 상기 저장부에 저장되는 주행 구역에 대한 맵(Map)은, 청소 중 주행에 사용되는 내비게이션 맵(Navigation map), 위치 인식에 사용되는 슬램(Simultaneous localization and mapping: SLAM) 맵, 장애물 등에 부딪히면 해당 정보를 저장하여 학습 청소시 사용하는 학습 맵, 전역 위치 인식에 사용되는 전역 위치 맵, 인식된 장애물 에 관한 정보가 기록되는 장애물 인식 맵 등일 수 있다. 한편, 상술한 바와 같이 용도별로 상기 저장부에 맵들을 구분하여 저장, 관리할 수 있지만, 맵이 용도별로 명확히 구분되지 않을 수도 있다. 예를 들어, 적어도 2 이상의 용도로 사용할 수 있도록 하나의 맵에 복수의 정 보를 저장할 수도 있다. 제어부는 주행제어모듈, 위치인식모듈, 지도생성모듈 및 장애물인식모듈을 포함할 수 있다. 도 1 내지 도 5를 참조하면, 주행제어모듈은 이동 로봇의 주행을 제어하는 것으로, 주행 설정에 따라 주행부의 구동을 제어한다. 또한, 주행제어모듈은 주행부의 동작을 바탕으로 이동 로봇의 주행경로를 파악할 수 있다. 예를 들어, 주행제어모듈은 구동 바퀴의 회전속도를 바탕으로 이동 로봇 의 현재 또는 과거의 이동속도, 주행한 거리 등을 파악할 수 있으며, 각 구동 바퀴(136(L), 136(R))의 회 전 방향에 따라 현재 또는 과거의 방향 전환 과정 또한 파악할 수 있다. 이렇게 파악된 이동 로봇의 주행 정보를 바탕으로, 맵 상에서 이동 로봇의 위치가 갱신될 수 있다. 지도생성모듈은 주행 구역의 맵을 생성할 수 있다. 지도생성모듈은 영상획득부를 통해 획득한 영상을 처리하여 맵을 작성할 수 있다. 예를 들어, 주행 구역에 대응하는 맵, 청소 영역과 대응되는 청소 맵을 작성할 수 있다. 또한, 지도생성모듈은 각 위치에서 영상획득부를 통해 획득한 영상을 처리하여 맵과 연계시켜 전역위 치를 인식할 수 있다. 또한, 지도생성모듈은 라이다 센서를 통해 획득한 정보에 기초하여 맵을 작성하고, 각 위치에서 라이 다 센서를 통해 획득한 정보에 기초하여 위치를 인식할 수 있다. 더욱 바람직하게는, 지도생성모듈은 영상획득부와 라이다 센서를 통해 획득한 정보에 기초하여 맵을 작성하고 위치 인식을 수행할 수 있다. 위치인식모듈은 현재 위치를 추정하여 인식한다. 위치인식모듈은 영상획득부의 영상 정보를 이 용하여 지도생성모듈과 연계하여 위치를 파악함으로써, 이동 로봇의 위치가 갑자기 변경되는 경우에 도 현재 위치를 추정하여 인식할 수 있다. 이동 로봇은 위치인식모듈을 통해 연속적인 주행 중에 위치 인식이 가능하고 또한, 위치인식모듈 없이 주행제어모듈, 지도생성모듈, 장애물인식모듈을 통해, 맵을 학습하고 현재 위치를 추정할 수 있다. 이동 로봇이 주행하는 중에, 영상획득부는 이동 로봇 주변의 영상들을 획득한다. 이하, 영상획 득부에 의해 획득된 영상을 '획득영상'이라고 정의한다. 획득영상에는 천장에 위치하는 조명들, 경계(edge), 코너(corner), 얼룩(blob), 굴곡(ridge) 등의 여러가지 특 징(feature)들이 포함된다. 지도생성모듈은 획득영상들 각각으로부터 특징을 검출한다. 컴퓨터 비전(Computer Vision) 기술 분야에서 영상으로부터 특징을 검출하는 다양한 방법(Feature Detection)이 잘 알려져 있다. 이들 특징의 검출에 적합한 여러 특징검출기(feature detector)들이 알려져 있다. 예를 들어, Canny, Sobel, Harris&Stephens/Plessey, SUSAN, Shi&Tomasi, Level curve curvature, FAST, Laplacian of Gaussian, Difference of Gaussians, Determinant of Hessian, MSER, PCBR, Grey-level blobs 검출기 등이 있다. 지도생성모듈은 각 특징점을 근거로 디스크립터를 산출한다. 지도생성모듈은 특징 검출을 위해 SIFT(Scale Invariant Feature Transform) 기법을 이용하여 특징점을 디스크립터(descriptor)로 변환할 수 있 다. 디스크립터는 n차원 벡터(vector)로 표기될 수 있다.SIFT는 촬영 대상의 스케일(scale), 회전, 밝기변화에 대해서 불변하는 특징을 검출할 수 있어, 같은 영역을 이 동 로봇의 자세를 달리하며 촬영하더라도 불변하는(즉, 회전 불변한(Rotation-invariant)) 특징을 검출할 수 있다. 물론, 이에 한정되지 않고 다른 다양한 기법(예를 들어, HOG: Histogram of Oriented Gradient, Haar feature, Fems, LBP:Local Binary Pattern, MCT:Modified Census Transform)들이 적용될 수도 있다. 지도생성모듈은 각 위치의 획득영상을 통해 얻은 디스크립터 정보를 바탕으로, 획득영상마다 적어도 하나 의 디스크립터를 소정 하위 분류규칙에 따라 복수의 군으로 분류하고, 소정 하위 대표규칙에 따라 같은 군에 포 함된 디스크립터들을 각각 하위 대표 디스크립터로 변환할 수 있다. 다른 예로, 실(room)과 같이 소정 구역내의 획득영상 들로부터 모인 모든 디스크립터를 소정 하위 분류규칙에 따라 복수의 군으로 분류하여 상기 소정 하위 대표규칙에 따라 같은 군에 포함된 디스크립터들을 각각 하위 대 표 디스크립터로 변환할 수도 있다. 지도생성모듈은 이 같은 과정을 거쳐, 각 위치의 특징분포를 구할 수 있다. 각 위치 특징분포는 히스토그 램 또는 n차원 벡터로 표현될 수 있다. 또 다른 예로, 지도생성모듈은 소정 하위 분류규칙 및 소정 하위 대표규칙을 거치지 않고, 각 특징점으로부터 산출된 디스크립터를 바탕으로 미지의 현재 위치를 추정할 수 있다. 또한, 위치 도약 등의 이유로 이동 로봇의 현재 위치가 미지의 상태가 된 경우에, 기 저장된 디스크립터 또는 하위 대표 디스크립터 등의 데이터를 근거로 현재 위치를 추정할 수 있다. 이동 로봇은, 미지의 현재 위치에서 영상획득부를 통해 획득영상을 획득한다. 영상을 통해 천장에 위 치하는 조명들, 경계(edge), 코너(corner), 얼룩(blob), 굴곡(ridge) 등의 여러가지 특징(feature)들이 확인된 다. 위치인식모듈은 획득영상으로부터 특징들을 검출한다. 컴퓨터 비전 기술 분야에서 영상으로부터 특징을 검 출하는 다양한 방법 및 이들 특징의 검출에 적합한 여러 특징검출기들에 대한 설명은 상기한 바와 같다. 위치인식모듈은 각 인식 특징점을 근거로 인식 디스크립터 산출단계를 거쳐 인식 디스크립터를 산출한다. 이때 인식 특징점 및 인식 디스크립터는 위치인식모듈에서 수행하는 과정을 설명하기 위한 것으로 지도생 성모듈에서 수행하는 과정을 설명하는 용어와 구분하기 위한 것이다. 다만, 이동 로봇의 외부 세계의 특징이 각각 다른 용어로 정의되는 것에 불과하다. 위치인식모듈은 본 특징 검출을 위해 SIFT(Scale Invariant Feature Transform) 기법을 이용하여 인식 특 징점을 인식 디스크립터로 변환할 수 있다. 인식 디스크립터는 n차원 벡터(vector)로 표기될 수 있다. SIFT는 앞서 설명한 바와 같이, 획득영상에서 코너점 등 식별이 용이한 특징점을 선택한 후, 각 특징점 주변의 일정한 구역에 속한 픽셀들의 밝기 구배(gradient)의 분포 특성(밝기 변화의 방향 및 변화의 급격한 정도)에 대 해, 각 방향에 대한 변화의 급격한 정도를 각 차원에 대한 수치로 하는 n차원 벡터(vector)를 구하는 영상인식 기법이다. 위치인식모듈은 미지의 현재 위치의 획득영상을 통해 얻은 적어도 하나의 인식 디스크립터 정보를 근거로, 소정 하위 변환규칙에 따라 비교대상이 되는 위치 정보(예를 들면, 각 위치의 특징분포)와 비교 가능한 정보(하 위 인식 특징분포)로 변환한다. 소정 하위 비교규칙에 따라, 각각의 위치 특징분포를 각각의 인식 특징분포와 비교하여 각각의 유사도를 산출할 수 있다. 각각의 위치에 해당하는 상기 위치 별로 유사도(확률)를 산출하고, 그 중 가장 큰 확률이 산출되는 위 치를 현재 위치로 결정할 수 있다. 이와 같이, 제어부는 주행 구역을 구분하고 복수의 영역으로 구성된 맵을 생성하거나, 기저장된 맵을 바탕 으로 본체의 현재 위치를 인식할 수 있다. 또한, 제어부는 영상획득부와 라이다 센서를 통해 획득한 정보를 융합하여 맵을 작성하고 위치 인식을 수행할 수 있다. 제어부는 맵이 생성되면, 생성된 맵을 통신부를 통해 외부 단말기, 서버 등으로 전송할 수 있다. 또 한, 제어부는 앞서 설명한 바와 같이, 외부 단말기, 서버 등으로부터 맵이 수신되면, 저장부에 저장할 수 있다. 또한 제어부는 주행 중 맵이 갱신되는 경우 갱신된 정보를 외부 단말기로 전송하여 외부 단말기와 이동 로 봇에 저장되는 맵이 동일하도록 한다. 외부 단말기와 이동 로봇에 저장된 맵이 동일하게 유지됨에 따 라 이동 단말기로부터의 청소명령에 대하여, 이동 로봇이 지정된 영역을 청소할 수 있으며, 또한, 외부 단 말기에 이동 로봇의 현재 위치가 표시될 수 있도록 하기 위함이다. 이때, 맵은 청소 영역을 복수의 영역으로 구분되고, 복수의 영역을 연결하는 연결통로가 포함하며, 영역 내의 장애물에 대한 정보를 포함할 수 있다. 제어부는 청소명령이 입력되면, 맵 상의 위치와 이동 로봇의 현재 위치가 일치하는지 여부를 판단한다. 청 소명령은 리모컨, 조작부 또는 외부 단말기로부터 입력될 수 있다. 제어부는 현재 위치가 맵 상의 위치와 일치하지 않는 경우, 또는 현재 위치를 확인할 수 없는 경우, 현재 위치를 인식하여 이동 로봇의 현재 위치를 복구한 한 후, 현재 위치를 바탕으로 지정영역으로 이동하도록 주행부를 제어할 수 있다. 현재 위치가 맵 상의 위치와 일치하지 않는 경우 또는 현재 위치를 확인 할 수 없는 경우, 위치인식모듈은 영상획득부로부터 입력되는 획득영상 및/또는 라이다 센서를 통해 획득된 지형 정보를 분석하여 맵을 바탕으로 현재 위치를 추정할 수 있다. 또한, 장애물인식모듈 또는 지도생성모듈 또한, 같은 방식으 로 현재 위치를 인식할 수 있다. 위치를 인식하여 이동 로봇의 현재 위치를 복구한 후, 주행제어모듈은 현재 위치로부터 지정영역으로 주행경로를 산출하고 주행부를 제어하여 지정영역으로 이동한다. 서버로부터 청소 패턴 정보를 수신하는 경우, 주행제어모듈은 수신한 청소 패턴 정보에 따라, 전체 주행 구역을 복수의 영역으로 나누고, 하나 이상의 영역을 지정영역으로 설정할 수 있다. 또한, 주행제어모듈은 수신한 청소 패턴 정보에 따라 주행경로를 산출하고, 주행경로를 따라 주행하며, 청 소를 수행할 수 있다. 제어부는 설정된 지정영역에 대한 청소가 완료되면, 청소기록을 저장부에 저장할 수 있다. 또한, 제어부는 통신부를 통해 이동 로봇의 동작상태 또는 청소상태를 소정 주기로 외부 단말기, 서버로 전송할 수 있다. 그에 따라 외부 단말기는 수신되는 데이터를 바탕으로, 실행중인 애플리케이션의 화면상에 맵과 함께 이동 로봇 의 위치를 표시하고, 또한 청소 상태에 대한 정보를 출력한다. 본 발명의 실시예에 따른 이동 로봇은 일방향으로 장애물이나 벽면이 감지될 때까지 이동하다가, 장애물인 식모듈이 장애물을 인식하면, 인식된 장애물의 속성에 따라 직진, 회전 등 주행 패턴을 결정할 수 있다. 예를 들어, 인식된 장애물의 속성이 넘어갈 수 있는 종류의 장애물이면, 이동 로봇은 계속 직진할 수 있다. 또는, 인식된 장애물의 속성이 넘어갈 수 없는 종류의 장애물이면, 이동 로봇은 회전하여 일정거리 이동하고, 다시 최초 이동 방향의 반대방향으로 장애물이 감지되는 거리까지 이동하여 지그재그 형태로 주행할 수 있다 본 발명의 실시예에 따른 이동 로봇은, 머신 러닝(machine learning) 기반의 사람, 사물 인식 및 회피를 수행할 수 있다. 상기 제어부는, 입력 영상에서 머신 러닝(machine learning)으로 기학습된 장애물을 인식하는 장애물인식 모듈과 상기 인식된 장애물의 속성에 기초하여, 상기 주행부의 구동을 제어하는 주행제어모듈을 포함할 수 있다. 본 발명의 실시예에 따른 이동 로봇은, 머신 러닝으로 장애물의 속성이 학습된 장애물인식모듈을 포 함할 수 있다. 머신 러닝은 컴퓨터에게 사람이 직접 로직(Logic)을 지시하지 않아도 데이터를 통해 컴퓨터가 학습을 하고 이를 통해 컴퓨터가 알아서 문제를 해결하게 하는 것을 의미한다. 딥러닝(Deep Learning)은. 인공지능을 구성하기 위한 인공신경망(Artificial Neural Networks: ANN)에 기반으 로 해 컴퓨터에게 사람의 사고방식을 가르치는 방법으로 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 상기 인공신경망(ANN)은 소프트웨어 형태로 구현되거나 칩(chip) 등 하드웨어 형태로 구현될 수 있다. 장애물인식모듈은 장애물의 속성이 학습된 소프트웨어 또는 하드웨어 형태의 인공신경망(ANN)을 포함할 수 있다. 예를 들어, 장애물인식모듈은 딥러닝(Deep Learning)으로 학습된 CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), DBN(Deep Belief Network) 등 심층신경망(Deep Neural Network: DNN)을 포함 할 수 있다. 장애물인식모듈은 상기 심층신경망(DNN)에 포함된 노드들 사이의 가중치(weight)들에 기초하여 입력되는 영상 데이터에 포함되는 장애물의 속성을 판별할 수 있다. 상기 제어부는, 상기 영상획득부, 특히 전면 카메라 센서(120a)가 획득한 영상 전체를 사용하는 것이 아니라 일부 영역만을 사용하여 이동 방향에 존재하는 장애물의 속성을 판별할 수 있다. 또한, 상기 주행제어모듈은 상기 인식된 장애물의 속성에 기초하여 상기 주행부의 구동을 제어할 수 있다. 한편, 저장부에는 장애물 속성 판별을 위한 입력 데이터, 상기 심층신경망(DNN)을 학습하기 위한 데이터가 저장될 수 있다. 저장부에는 영상획득부가 획득한 원본 영상과 소정 영역이 추출된 추출 영상들이 저장될 수 있다. 또한, 실시예에 따라서는, 저장부에는 상기 심층신경망(DNN) 구조를 이루는 웨이트(weight), 바이어스 (bias)들이 저장될 수 있다. 또는, 실시예에 따라서는, 상기 심층신경망 구조를 이루는 웨이트(weight), 바이어스(bias)들은 상기 장애물인 식모듈의 임베디드 메모리(embedded memory)에 저장될 수 있다. 한편, 상기 장애물인식모듈은 상기 영상획득부가 획득하는 영상의 일부 영역을 추출할 때마다 상기 추출된 영상을 트레이닝(training) 데이터로 사용하여 학습 과정을 수행하거나, 소정 개수 이상의 추출 영상이 획득된 후 학습 과정을 수행할 수 있다. 즉, 상기 장애물인식모듈은 장애물을 인식할 때마다 인식 결과를 추가하여 웨이트(weight) 등 심층신경망 (DNN) 구조를 업데이트(update)하거나, 소정 횟수의 트레이닝 데이터가 확보된 후에 확보된 트레이닝 데이터로 학습 과정을 수행하여 웨이트(weight) 등 심층신경망(DNN) 구조를 업데이트할 수 있다. 또는, 이동 로봇은 통신부를 통하여 상기 영상획득부가 획득한 원본 영상 또는 추출된 영상을 소정 서버로 전송하고, 상기 소정 서버로부터 머신 러닝과 관련된 데이터를 수신할 수 있다. 이 경우에, 이동 로봇은, 상기 소정 서버로부터 수신된 머신 러닝과 관련된 데이터에 기초하여 장애물인식 모듈을 업데이트(update)할 수 있다. 한편, 이동 로봇은 출력부를 더 포함하여, 소정 정보를 영상으로 표시하거나 음향으로 출력할 수 있 다. 출력부는 사용자의 명령 입력에 대응하는 정보, 사용자의 명령 입력에 대응하는 처리 결과, 동작모드, 동 작상태, 에러상태 등을 영상으로 표시하는 디스플레이(미도시)를 포함할 수 있다. 실시예에 따라서는, 상기 디스플레이는 터치패드와 상호 레이어 구조를 이루어 터치스크린으로 구성될 수 있다. 이 경우에, 터치스크린으로 구성되는 디스플레이는 출력 장치 이외에 사용자의 터치에 의한 정보의 입력이 가능 한 입력 장치로도 사용될 수 있다. 또한, 출력부는 오디오 신호를 출력하는 음향 출력부(미도시)를 포함할 수 있다. 음향 출력부는 제어부 의 제어에 따라 경고음, 동작모드, 동작상태, 에러상태 등의 알림 메시지, 사용자의 명령 입력에 대응하는 정보, 사용자의 명령 입력에 대응하는 처리 결과 등을 음향으로 출력할 수 있다. 음향 출력부는, 제어부 로부터의 전기 신호를 오디오 신호로 변환하여 출력할 수 있다. 이를 위해, 스피커 등을 구비할 수 있다. 도 6은 본 발명의 일 실시예에 따른 이동 로봇의 제어 방법을 도시한 순서도로, 맵 생성 과정을 도시한 순서도 이고, 도 7 내지 도 10은 도 6의 제어 방법에 관한 설명에 참조되는 도면이다.도 7과 도 8은, 도 6의 주행 및 정보 획득 과정(S601), 노드 생성 과정(S602), 노드 맵 생성 과정(S603), 경계 생성 과정(S604), 경계맵 생성 과정(S605) 및 디스크립터 생성 과정(S606)을 예시적으로 도시한 개념도이다. 도 7에는, 상기 과정(S601)에서 획득한 영상과 영상 내의 복수의 특징점(f1, f2, f3, f4, f5, f6, f7)이 도시 되고, 상기 과정(S606)에서 복수의 특징점(f1, f2, f3, ..., f7)에 각각 대응하는 n차원 벡터인 디스크립터 ( )를 생성시키는 도식이 도시된다. 도 7과 도 8을 참고하여, 정보 획득 과정(S601)에서, 이동 로봇이 주행하는 중에 영상획득부는 각 지 점에서 영상을 획득한다. 예를 들어, 영상획득부는 이동 로봇의 상측을 향해 촬영하여 천장 등의 영 상을 획득할 수 있다. 또한. 정보 획득 과정(S601)에서, 이동 로봇이 주행하는 중에 센싱부, 영상획득부, 그 밖에 공 지된 수단을 이용하여 주행 장애 요소를 감지할 수 있다. 이동 로봇은 각 지점에서 주행 장애 요소를 감지할 수 있다. 예를 들어, 이동 로봇은 특정 지점에서 주행 장애 요소 중 하나인 벽체의 외측면을 감지할 수 있다. 도 7과 도 8을 참고하여, 노드 생성 과정(S602)에서, 이동 로봇은 각 지점에 대응하는 노드를 생성한다. 노드(Na18, Na19, Na20)에 대응하는 좌표 정보는 이동 로봇이 측정한 주행 변위를 근거로 생성될 수 있다. 노드(Node)는, 주행 구역 내 소정 지점에 대응하는 맵 상의 어느 한 위치를 나타내는 데이터를 의미할 수 있고, 그래프(graph) 기반의 슬램에서 노드는 로봇의 포즈(pose)를 의미할 수 있다. 또한, 포즈(pose)는 좌표계상의 위치 좌표 정보(X, Y)와 방향 정보(θ)를 포함할 수 있다. 노드 정보는 상기 노드에 대응하는 각종 데이터들을 의미할 수 있다. 맵은 복수의 노드 및 이에 대응하는 노드 정보를 포함할 수 있다. 주행 변위는, 이동 로봇의 이동 방향과 이동 거리를 포함하는 개념이다. 주행구역의 바닥면을 X축 및 Y축이 직 교하는 평면 상에 있다고 가정할 때, 주행 변위는 ( )로 표현할 수 있다. 는 각각 X축 및 Y축 방 향으로의 변위를 의미하고, 는 회전 각도를 의미한다. 제어부는, 주행부의 동작을 근거로 이동 로봇의 주행 변위를 측정할 수 있다. 예를 들어, 주행 제어모듈은 구동 바퀴의 회전속도를 근거로 이동 로봇의 현재 또는 과거의 이동속도, 주행한 거 리 등을 측정할 수 있으며, 구동 바퀴의 회전 방향에 따라 현재 또는 과거의 방향 전환 과정 또한 측정할 수 있다. 또한, 제어부는 센싱부에서 센싱되는 데이터를 이용하여 상기 주행 변위를 측정하는 것도 가능하다. 예를 들어, 좌륜(136(L))과 우륜(136(R))에 연결되어 회전수를 감지하여 출력하는 엔코더(Encoder) 등 휠 센서 를 이용하여 주행 변위를 측정할 수 있다. 제어부는 회전수를 이용하여 좌, 우측 바퀴의 회전 속도를 연산할 수 있다. 또한, 제어부는 좌륜 (136(L))과 우륜(136(R))의 회전수 차이를 이용하여 회전각을 연산할 수 있다. 통상적으로 엔코더는 적분을 계속 수행해 나감에 따라 오차가 누적되는 한계가 있었다. 따라서, 더욱 바람직하 게는, 제어부는 라이다 센서의 센싱 데이터에 기초하여 주행 변위 등 오도메트리(odometry) 정보를 생성할 수 있다. 제어부는 상기 휠 센서에서 감지되는 센싱 데이터에 상기 라이다 센서의 센싱 데이터를 융합하여 더 욱 정확한 오도메트리 정보를 생성할 수 있다. 예를 들어, 상기 제어부는 상기 주행 감지 센서에서의 센싱 데이터와 상기 라이다 센서의 ICP(Iterative Closest Point) 매칭 결과를 융합하여 오도메트리 정보를 생 성할 수 있다. 이에 따라, 단순히 바퀴의 회전에만 의존하여 오도메트리 정보를 생성하는 경우에 바퀴가 헛돌거나 미끄러짐, 충돌, 구속, 납치 상황 등에서 발생할 수 있는 오류를 방지하고 누적되는 에러(error)를 최소화하여 더욱 정확한 오도메트리 정보를 생성할 수 있다. 도 7과 도 8을 참고하여, 경계 정보 생성 과정(S604)에서, 이동 로봇은 주행 장애 요소에 대응하는 경계 정보(b20)를 생성한다. 경계 정보 생성 과정(S604)에서, 이동 로봇은 각 주행 장애 요소에 각각 대응하는 각 경계 정보를 생성한다. 복수의 주행 장애 요소에 복수의 경계 정보가 일대일대응 한다. 경계 정보(b20)는 대 응하는 노드의 좌표 정보와 센싱부가 측정한 거리값을 근거로 생성될 수 있다. 도 7과 도 8을 참고하여, 노드맵 생성 과정(S603) 및 경계맵 생성 과정(S605)은 동시에 진행된다. 노드맵 생성 과정(S603)에서 복수의 노드(Na18, Na19, Na20 등)를 포함하는 노드맵이 생성된다. 경계맵 생성 과정(S605)에서 복수의 경계 정보(b20 등)를 포함하는 경계맵(Ba)이 생성된다. 노드맵 생성 과정(S603) 및 경계맵 생성 과정 (S605)에서 노드맵 및 경계맵(Ba)을 포함하는 맵(Ma)이 생성된다. 도 6에는, 노드맵 생성 과정(S603) 및 경계맵 생성 과정(S605)을 통해 생성 중인 맵(Ma)을 도시한다. 도 7에 도시된 영상에는, 천장에 위치하는 조명들, 경계(edge), 코너(corner), 얼룩(blob), 굴곡(ridge) 등에 의한 여러 가지 특징점 들이 확인된다. 이동 로봇은 영상으로부터 특징점을 추출한다. 컴퓨터 비전 (Computer Vision) 기술 분야에서 영상으로부터 특징점을 추출하는 다양한 방법(Feature Detection)이 잘 알려 져 있다. 이들 특징점의 추출에 적합한 여러 특징검출기(feature detector)들이 알려져 있다. 예를 들어, Canny, Sobel, Harris&Stephens/Plessey, SUSAN, Shi&Tomasi, Level curve curvature, FAST, Laplacian of Gaussian, Difference of Gaussians, Determinant of Hessian, MSER, PCBR, Grey-level blobs 검출기 등이 있 다. 도 7을 참고하여, 디스크립터 생성 과정(S606)에서, 획득된 영상에서 추출된 복수의 특징점(f1, f2, f3, ..., f7)을 근거로 디스크립터( )를 생성한다. 상기 디스크립터 생성 과정(S606)에서, 획득된 복수 의 영상에서 추출된 복수의 특징점(f1, f2, f3, ..., fm)을 근거로 디스크립터( )를 생성한다. (여기서, m은 자연수) 복수의 특징점(f1, f2, f3, ..., fm)에 복수의 디스크립터 ( )는 일대일대응 한다. 은 n차원 벡터를 의미한다. 의 중괄호 { } 안의 f1, f1, f1, ..., f1(n)은"}
{"patent_id": "10-2019-0035039", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "을 이루는 각 차원의 수치를 의미한다. 나머지 에 대한 표기도 이와 같은 방식이므로 설명을 생략 한다. 특징 검출을 위해 SIFT(Scale Invariant Feature Transform) 기법을 이용하여, 복수의 특징점(f1, f2, f3, ..., fm)에 대응하는 복수의 디스크립터( )를 생성할 수 있다. 예를 들면, SIFT기법을 적용하여, 영상에서 식별이 용이한 특징점(f1, f2, f3, f4, f5, f6, f7)을 선택한 후, 각 특징점(f1, f2, f3, f4, f5, f6, f7) 주변의 일정한 구역에 속한 픽셀들의 밝기 구배(gradient)의 분포 특 성(밝기 변화의 방향 및 변화의 급격한 정도)을 근거로 n차원 벡터인 디스크립터를 생성할 수 있다. 여기서, 특 징점의 각 밝기 변화의 방향을 각 차원으로 보고, 각 밝기 변화의 방향에 대한 변화의 급격한 정도를 각 차원에 대한 수치로 하는 n차원 벡터(디스크립터)를 생성할 수 있다. SIFT 기법은 촬영 대상의 스케일(scale), 회전, 밝기변화에 대해서 불변하는 특징을 검출할 수 있어, 같은 영역을 이동 로봇의 자세를 달리하며 촬영하더 라도 불변하는(즉, 회전 불변한(Rotation-invariant)) 특징을 검출할 수 있다. 물론, 이에 한정되지 않고 다른 다양한 다른 기법(예를 들어, HOG: Histogram of Oriented Gradient, Haar feature, Fems, LBP:Local Binary Pattern, MCT:Modified Census Transform)들이 적용하는 것도 가능하다. 도 9는, 이동 로봇이 이동하면서 생성하는 복수의 노드(N) 및 노드들 간의 변위(C)를 도시한 개념도이다. 도 9를 참고하여, 원점 노드(O)가 설정된 상태에서 주행 변위(C1)가 측정되어 노드(N1)의 정보가 생성된다. 이 후 측정된 주행 변위(C2)의 시점이 되는 노드(N1)의 좌표 정보에 상기 주행 변위(C2)를 더하여, 상기 주행 변위 (C2)의 종점이 되는 노드(N2)의 좌표 정보를 생성할 수 있다. 노드(N2)의 정보가 생성된 상태에서 주행 변위 (C3)가 측정되어 노드(N3)의 정보가 생성된다. 이와 같이 순차적으로 측정되는 주행 변위들(C1, C2, C3, …, C16)을 근거로 순차적으로 노드들(N1, N2, N3, …, N16)의 정보가 생성된다. 어느 하나의 주행 변위(C15)의 시점이 되는 노드(C15)를 해당 주행 변위(C15)의 종점이 되는 노드의 '기초 노드'라고 정의할 때, 루프 변위(Loop Constraint, LC)는 어느 한 노드(N15) 및 상기 어느 한 노드(N15)의 상 기 '기초 노드(N14)가 아닌' 다른 인접한 노드(N5) 사이의 변위가 측정된 값을 의미한다. 일 예로, 어느 한 노드(N15)에 대응하는 획득영상 정보와 다른 인접한 노드(N5)에 대응하는 획득영상 정보가 서 로 비교되어 두 노드(N15, N5) 사이의 루프 변위(LC)가 측정될 수 있다. 다른 예로, 어느 한 노드(N15)의 주변 환경과의 거리 정보와 다른 인접한 노드(N5)의 주변 환경과의 거리 정보가 서로 비교되어 두 노드(N15, N5) 사 이의 루프 변위(LC)가 측정될 수 있다. 도 8에는, 노드 N5와 노드 N15 사이에서 측정된 루프 변위(LC1), 및 노 드 N4와 노드 N16 사이에서 측정된 루프 변위(LC2)가 예시적으로 도시되어 있다. 상기 주행 변위를 근거로 생성된 어느 한 노드(N5)의 정보는 노드 좌표 정보 및 해당 노드에 대응하는 영상 정 보를 포함할 수 있다. 노드(N5)에 인접하는 다른 노드(N15)가 있을 때, 노드(N15)에 대응하는 영상 정보를 노드 (N5)에 대응하는 영상 정보와 비교하면 두 노드(N5, N15) 사이의 루프 변위(LC1)가 측정된다. '루프 변위(LC 1)'와 '기저장된 두 노드(N5, N15)의 좌표 정보에 따라 산출되는 변위'가 서로 다를 경우, 노드 좌표 정보에 오 차가 있는 것으로 보고, 두 노드(N5, N15)의 좌표 정보를 갱신할 수 있다. 이 경우, 두 노드(N5, N15)와 연결된 다른 노드 들(N6, N7, N8, N9, N10, N11, N12, N13, N14)의 좌표 정보도 갱신될 수 있다. 또한, 한번 갱신된 노드 좌표 정보도 계속해서 상기 과정을 거쳐 거듭 갱신될 수 있다. 보다 구체적으로 설명하면 다음과 같다. 루프 변위(LC)가 측정된 두 노드(N)를 각각 제1 루프 노드 및 제2 루프 노드라고 정의한다. 기저장된 제1 루프 노드의 노드 좌표 정보 및 제2 루프 노드의 노드 좌표 정보에 의해 산출 되는 '산출 변위( )'(좌표값의 차에 의해 산출됨)가 루프 변위(LC)( )와 차이 ( 1- )가 발생할 수 있다. 상기 차이가 발생하면, 상기 차이를 오차로 보고 노드 좌표 정보를 갱신할 수 있는데, 루프 변위(LC)가 상기 산출 변위보다 더 정확한 값이라는 가정하에 노드 좌표 정보를 갱신한다. 노드 좌표 정보를 갱신하는 경우, 상기 제1 루프 노드 및 제2 루프 노드의 노드 좌표 정보만 갱신할 수도 있으 나, 상기 오차의 발생은 주행 변위들의 오차가 누적되어 발생된 것이므로 상기 오차를 분산시켜 다른 노드들의 노드 좌표 정보도 갱신하도록 설정할 수 있다. 예를 들어, 상기 제1 루프 노드와 제2 루프 노드 사이에 상기 주 행 변위에 의해 생성된 모든 노드들에, 상기 오차 값을 분산시켜 노드 좌표 정보를 갱신할 수 있다. 도 8을 참 고하여, 루프 변위(LC1)가 측정되고 상기 오차가 산출된 경우, 제1 루프 노드(N15)와 제2 루프 노드(N5)와 사이 의 노드 들(N6 내지 N14)에 상기 오차를 분산시켜, 노드들(N5 내지 N15)의 노드 좌표 정보가 모두 조금씩 갱신 될 수 있다. 물론, 이러한 오차 분산을 확대시켜, 이외의 노드 들(N1 내지 N4)의 노드 좌표 정보를 같이 갱신할 수도 있음은 물론이다. 도 10은 제1 맵(Ma)의 일 예를 도시한 개념도로서, 생성된 노드맵을 포함하고 있는 그림이다. 도 10에는, 도 6 에 따른 맵 생성 과정을 거쳐 생성된 어느 한 맵(Ma)의 일 예가 도시된다. 상기 맵(Ma)은 노드맵 및 경계맵(B a)을 포함한다. 상기 노드맵은 복수의 제1 노드(Na1 내지 Na99)를 포함한다. 도 10을 참고하여, 어느 한 맵(Ma)은 노드맵(Na1, Na2, …, Na99) 및 경계맵(Ba)을 포함할 수 있다. 노드맵은 하나의 맵 상의 여러 가지 정보 중 복수의 노드로 이루어진 정보를 지칭하는 것이고, 경계맵은 하나의 맵 상의 여러 가지 정보 중 복수의 경계 정보로 이루어진 정보를 지칭하는 것이다. 노드맵과 경계맵은 상기 맵의 구성요 소로서, 노드맵의 생성 과정(S602, S603)과 경계맵의 생성 과정(S604, S605)은 동시에 진행된다. 예를 들어, 경 계 정보는, 특정 지점에서 떨어진 주행 장애 요소의 거리를 측정한 후, 상기 특정 지점에 대응하는 기저장된 노 드의 좌표 정보를 기준으로 생성될 수 있다. 예를 들어, 노드의 노드 좌표 정보는 특정 주행 장애 요소에서 떨 어진 특정 지점의 거리를 측정한 후, 상기 특정 장애 요소에 대응하는 기저장된 경계 정보를 기준으로 생성할 수 있다. 노드 및 경계 정보는, 기저장된 어느 하나에 대한 다른 하나의 상대적 좌표를 근거로, 다른 하나가 맵 상에 생성될 수 있다.또한, 상기 맵은 상기 과정(S606)에서 생성된 영상 정보를 포함할 수 있다. 복수의 영상 정보는 복수의 노드에 일대일대응 한다. 특정 영상 정보는 특정 노드에 대응한다. 도 11은 본 발명의 일 실시예에 따른 이동 로봇의 제어 방법을 도시한 순서도이다. 본 발명의 일 실시예에 따른 이동 로봇은, 본체 외부의 지형 정보를 획득하는 라이다 센서와 본 체 외부의 영상을 획득하는 카메라 센서(120b)를 포함할 수 있다. 이동 로봇은, 동작 중에 라이다 센서를 통하여 주행 구역의 지형 정보를 획득할 수 있다(S1110). 또한, 이동 로봇은, 동작 중에 카메라 센서(120b) 등 영상획득부를 통하여 주행 구역의 영상 정보를 획득할 수 있다(S1120). 한편, 제어부는, 상기 라이다 센서의 센싱 데이터에 기초하여 오도메트리(odometry) 정보를 생성할 수 있다(S1130). 예를 들어, 제어부는, 특정 위치에서 상기 라이다 센서를 통하여 감지되는 센싱 데이터에 기초한 주 변 지형 정보를 라이다 센서 기반의 기저장된 지형 정보와 비교하여 주행 변위 등 오도메트리 정보를 생성할 수 있다. 더욱 바람직하게 제어부는 본체의 이동에 따른 주행 상태를 감지하는 휠 센서 등 주행 감지 센서의 센싱 데이터와 상기 라이다 센서의 센싱 데이터를 융합하여 더욱 정확한 오도메트리 정보를 생성할 수 있 다. 휠 센서로는, 좌륜(136(L))과 우륜(136(R))에 연결되어 회전수를 감지하여 출력하는 엔코더(Encoder)를 이용할 수 있는데, 통상적으로 엔코더는 적분을 계속 수행해 나감에 따라 오차가 누적되는 한계가 있었다. 상기 제어부는 상기 주행 감지 센서에서의 센싱 데이터와 상기 라이다 센서의 ICP(Iterative Closest Point) 매칭 결과를 융합하여 오도메트리 정보를 생성할 수 있다. 이에 따라, 휠 센서에만 의존하는 경우보다 더욱 정확한 오도메트리 정보를 생성할 수 있고, 주행 변위를 정확 하게 산출함으로써 위치 인식의 정확도도 향상할 수 있다. 본 발명의 일 실시예에 따르면, 상기 라이다 센서의 센싱 데이터들을 ICP(Iterative Closest Point) 알고 리즘에 따라 매칭하고, 상기 오도메트리 정보를 생성하는 단계(S1130)에서 제어부는, 상기 주행 감지 센서 에서의 센싱 데이터와 상기 라이다 센서의 ICP(Iterative Closest Point) 매칭 결과를 융합하여 상기 오도 메트리 정보를 생성할 수 있다. 제어부는, 라이다 센서를 통하여 다른 시점에 획득된 정보 사이에 거리가 가장 가까운 두 지점을 검 출할 수 있다. 제어부는 검출된 두 지점을 대응점(Corresponding point)으로 설정할 수 있다. 제어부는 상기 설정된 대응점들의 위치를 동일하게 만드는 운동량을 이용하여, 이동 로봇의 주행 변 위와 관련된 오도메트리 정보를 검출할 수 있다. 제어부는 이동을 시작한 지점(이전 위치)과 관련된 위치정보와 상기 검출된 주행 변위를 이용하여 이동 로 봇의 현재 지점과 관련된 위치정보를 검출할 수 있다. 본 발명의 일 실시예에 따르면, 관련된 데이터를 정합하여 맞추는 알고리즘으로 널리 활용되고 있는 ICP(Iterative Closest Point) 알고리즘을 이용하여, 오도메트리 정보를 생성하고 이동 로봇의 위치를 추 정할 수 있다. 예를 들어, ICP 알고리즘에 따른 데이터들의 매칭 결과로, 라이다 센서에서 획득되는 데이터와 기저장된 데이터 사이에 포인트들의 거리가 가장 가깝게되는 위치로 매칭이 이루어질 수 있다. 이에 따라 이동 로봇(10 0)의 위치를 추정할 수 있다. 또한, 이전 위치를 기준으로 오도메트리 정보를 생성할 수 있다. 또는, 다른 알고리즘을 활용하여 상기 라이다 센서의 센싱 데이터에 기초한 오도메트리 정보를 생성할 수 도 있을 것이다. 한편, 제어부는, 상기 오도메트리 정보를 기반으로 상기 카메라 센서(120b)로부터 입력되는 영상의 특징점 매칭을 수행하고(S1140), 상기 특징점 매칭 결과에 기초하여 현재 위치를 추정할 수 있다(S1150). 제어부는, 상기 카메라 센서(120b)로부터 입력되는 영상으로부터, 천장에 위치하는 조명들, 경계(edge), 코너(corner), 얼룩(blob), 굴곡(ridge) 등의 여러가지 특징(feature)들을 검출한다. 도 5 내지 도 10을 참조하여 설명한 것과 같이, 제어부는, 각각의 인식 특징점을 근거로 인식 디스크립터 산출단계를 거쳐 인식 디스크립터를 산출하고, 적어도 하나의 인식 디스크립터 정보를 근거로, 소정 하위 변환 규칙에 따라 비교대상이 되는 위치 정보(예를 들면, 각 위치의 특징분포)와 비교 가능한 정보(하위 인식 특징분 포)로 변환한다. 제어부는, 상기 카메라 센서(120b)로부터 입력되는 영상들간의 특징점을 매칭하거나 현재 위치에서 상기 카메라 센서(120b)로부터 입력되는 영상에서 추출된 특징점과 맵에 등록된 영상 정보의 특징점과 매칭할 수 있 다. 소정 하위 비교규칙에 따라, 각각의 위치 특징분포를 각각의 인식 특징분포와 비교하여 각각의 유사도를 산출할 수 있다. 각각의 위치에 해당하는 상기 위치 별로 유사도(확률)를 산출하고, 그 중 가장 큰 확률이 산출되는 위 치를 현재 위치로 결정할 수 있다. 또한, 제어부는 판별된 현재 위치에 대응하는 노드를 맵에 등록할 수 있다(S1160). 맵 생성 및 갱신 과정에서 리소스 관리 측면에서 필요없는 노드를 등록하는 것은 낭비이므로, 노드 등록 여부를 소정 기준에 따라 결정할 수 있다. 예를 들어, 제어부는 노드맵에서 현재 위치에 대응하는 노드를 기준으로 기설정된 기준 거리 이내의 노드 를 확인하여 노드 등록 여부를 결정할 수 있다. 여기서, 노드맵은 센싱 정보를 이용해 계산된 로봇의 위치를 나 타내는 노드(node)를 복수개 포함하는 맵으로서 슬램(SLAM) 맵일 수 있다. 제어부는 맵 상에 의미있는 추가 정보가 필요한 경우에만 노드를 등록할 수 있도록 구성할 수 있다. 제어부는, 현재 위치 기준 소정 거리 이내의 모든 노드와 현재 노드 사이의 에지(edge, constraint)가 존 재하는 지 여부를 판별할 수 있다. 이것은 현재 노드가 주변 노드와 특징점 매칭(feature matching)되는 것이 없는지 확인하는 것일 수 있다. 예를 들어, 특징점으로 코너 지점이 있으면 이전의 코너 지점과 비교 후, 로봇 의 상대적인 좌표가 있는지 판단함으로써 상관관계 존재 여부를 판단할 수 있다. 한편, 그래프(graph) 기반의 슬램에서 노드를 이어주는 에지(edge)는 로봇의 위치 사이의 주행 변위 정보, 오도 메트리 정보로 constraint라고도 할 수 있다. 노드간 상관관계를 생성하고 추가하는 것은, 노드를 이어주는 에지를 생성하는 것이다. 일 예로, 노드간 상관관 계의 생성은 두 개의 노드 사이의 상대 위치와 상대 위치의 에러 값을 산출함을 의미할 수 있다. 즉, 에지(edge, constraint)는 노드와 로봇과의 상대적인 좌표로 노드 사이의 관계성을 나타낼 수 있다. 또한, 에지(edge, constraint)가 존재한다는 것은 노드 사이에 일정 부분 겹쳐지는 센싱 정보가 있음을 의미할 수 있 다. 제어부는, 현재 로봇 위치에 대응하는 현재 노드를 기준으로 소정 거리 이내의 후보 노드와 현재 노드를 비교하여 에지가 존재하는 여부를 체크(check)할 수 있다. 만약 에지가 존재한다면, 노드들 사이에서 공통적으로 보이는 특징(feature)이 있고, 특징점 매칭(feature matching)까지 된다는 것을 의미할 수 있다. 이후, 현재 위치에 노드에 연결된 에지(edge)와 기존 맵의 노드 정 보를 비교한다. 제어부는 기존 맵의 노드 정보를 확인하여 현재 위치에 대응하는 노드에 연결된 에지(edge) 모두가 일치 (consistent)하면, 맵에 노드를 등록하지 않고 맵에 노드를 등록할 지 여부를 확인하는 과정을 종료할 수 있다 한편, 제어부는, 상기 카메라 센서(120b)로부터 입력되는 영상 간의 특징점 매칭이 실패한 경우, 상기 라 이다 센서의 센싱 데이터에 기초하여 현재 노드와 주변 노드의 ICP(Iterative Closest Point) 매칭을 수행 하여 노드간 상관관계를 추가할 수 있다. 또한, 제어부는, 영상 특징점 매칭 성공 여부와 무관하게 맵 생성 및 업데이트 과정에서 노드간 상관관계 판별 및 생성에 상기 라이다 센서의 센싱 데이터를 이용할 수 있다. 이와 같이, 제어부는 맵을 생성하거나, 기저장된 맵을 바탕으로 이동 로봇의 현재 위치를 인식할 수 있다. 본 발명은 다양한 환경에서의 높은 위치 인식 성능을 확보하는 기술로 물리적 특성이 다른 이종의 센서를 활용 하여 위치 인식 알고리즘을 구현하였다, 본 발명에 따르면, 카메라 센서(120b)의 영상 정보와 라이다 센서의 거리 정보를 활용하여 이종의 데이터 를 상호 보완적으로 적용한다. 이를 통해 슬램에서 영상만을 사용할 경우 저조도에 취약한 단점을 보완하고 라 이다 센서만 사용할 경우 발생하는 동적 환경 대응 보완할 수 있다. 슬램 기술은 비전(Vision) 기반, 레이저(Laser) 기반, 슬램으로 나눌 수 있다. 이 중 비전(Vision) 기반 슬램은 영상에서 특징점을 추출한 후 매칭을 하여 3차원 좌표를 계산하고, 이를 토대 로 슬램을 한다. 영상에 많은 정보가 있어 환경이 밝을 경우 자기 위치 인식을 하는데 뛰어난 성능을 가지지만 어두운 곳에서는 동작이 어렵고, 작은 크기의 물체가 가까운 곳에 있는 것과 큰 크기의 물체가 먼 곳에 있는 것 을 비슷하게 인식하는 등의 스케일 드리프트(Scale Drift) 문제가 있다. 그리고 레이저(Laser) 기반 슬램은 레이저를 활용하여 각도별 거리를 측정하여 주변 환경의 지형(geometry)을 계산하는 원리로 동작을 한다. 레이저 기반 슬램은 어두운 환경에서도 동작이 잘된다. 하지만, 지형(geometry) 정보만을 가지고 위치를 인식하므로 오피스(Office) 환경같이 반복되는 영역이 많은 공간에서는 초기 위치 조건 이 없을 경우 자기 위치를 찾기 어려운 경우가 많다. 또한 가구가 옮겨지는 등 동적 환경에 대응이 어렵다. 즉, 비전(Vision) 기반 슬램의 경우 어두운 환경(불빛이 없는 환경)에서는 정확한 동작이 어렵다. 또한 레이저 (Laser) 기반 슬램의 경우 동적 환경(움직이는 물체 등)과 반복되는 환경(비슷한 패턴)에서 자기 위치 인식이 어렵고, 기존 맵과 현재 프레임의 매칭 및 루프 클로징(loop closing)의 정확도가 떨어지며, 랜드마크를 만들기 힘들기 때문에, 납치(kidnap) 문제와 같은 상황에 대처하기 힘들다. 본 발명은 카메라 센서(120b)와 라이다 센서 이종 센서의 특징들을 상호보완적으로 적용하여 슬램 성능을 획기적으로 향상할 수 있다. 예를 들어, 바퀴 엔코더 만을 사용했을 때의 누적되는 에러를 최소화하기 위해 엔코더 정보와 라이다 센서(17 5)의 ICP(Iterative Closest Point) 결과를 융합하여 오도메트리(Odometry) 정보를 생성할 수 있다. 또한, 이 오도메트리(Odometry) 정보를 기반으로 입력되는 영상간 특징점 매칭을 통해 3D 복원을 한 후, 현재 위치(로봇의 변위량)를 계산함으로써, 정확하게 현재 위치를 추정할 수 있다. 실시예에 따라서는, 추정된 현재 위치를 보정하여 최종 현재 위치를 판별할 수 있다(S1170). 예를 들어, 라이다 센서의 센싱 데이터에 기초하여 주변 지형 정보를 보고 추정된 현재 위치의 불확실성(uncertainty)을 계산 하고 불확실성 값이 최소화되도록 보정하는 방식으로 정확한 최종 현재 위치를 판별할 수 있다. 여기서, 현재 위치의 불확실성(uncertanity)은 예측된 현재 위치의 신뢰값으로 확률 또는 분산 형태로 산출될 수 있다. 예를 들어, 추정된 현재 위치의 불확실성(uncertainty)은 공분산(covariance)으로 산출될 수 있다. 또한, 최종 판별된 현재 위치에 대응하는 노드로 노드 정보를 보정하여 맵에 등록할 수 있다. 실시예에 따라서, 상기 제어부는, 상기 라이다 센서의 센싱 데이터를 수신하고, 상기 라이다 센서 의 센싱 데이터에 기초한 지형 정보 및, 이전 위치 정보를 이용하여 위치 변위량을 판별하는 라이다 서비 스 모듈(도 12의 1020 참조), 및, 상기 라이다 서비스 모듈로부터 상기 위치 변위량을 수신하고, 상기 카 메라 센서(120b)로부터 영상을 수신하여, 상기 위치 변위량을 기반으로 현재 영상에서 추출된 특징점과 이전 위 치에서 추출된 특징점 매칭을 통해 특징점 위치를 판별하고, 판별된 특징점 위치에 기초하여 상기 현재 위치를 추정하는 비전 서비스 모듈(도 12의 1030 참조)을 포함할 수 있다. 여기서, 위치 변위량은 상술한 주행 변위일 수 있다. 한편, 상기 산출된 현재 위치 정보를 포함하는 노드 정보가 저장부에 저장될 수 있다. 한편, 상기 비전 서비스 모듈은 상기 노드 정보를 상기 라이다 서비스 모듈로 송신하고, 상기 라이 다 서비스 모듈은 상기 노드 정보에 상기 비전 서비스 모듈이 상기 현재 위치를 산출하는 동안에 이동 로봇이 이동한 위치 변위량을 반영하여 상기 이동 로봇의 현재 위치를 판별할 수 있다. 즉, 현 재 위치를 보정하여 최종적인 현재 위치를 판별할 수 있다(S1170). 본 발명의 일 실시예에 따른 이동 로봇은 본체의 이동에 따른 주행 상태를 감지하는 주행 감지 센서 를 포함할 수 있다. 예를 들어, 이동 로봇은 엔코더 등의 센서를 구비할 수 있다.이 경우에, 제어부는, 상기 주행 감지 센서의 센싱 데이터를 읽어오는 주행 서비스 모듈(도 12의 1010 참 조)을 더 포함하며, 상기 주행 서비스 모듈은 상기 주행 감지 센서의 센싱 데이터를 상기 라이다 서비스 모듈로 전달하고, 상기 라이다 서비스 모듈은 상기 주행 감지 센서의 센싱 데이터에 기초한 오도메 트리 정보와 상기 라이다 센서의 ICP 결과를 융합하여 상기 오도메트리 정보를 생성할 수 있다. 이동 로봇은 그래프(Graph) 기반의 슬램(SLAM) 기술을 이용하여, 인접한 두 개의 노드의 상대 위치를 기초 로, 루프 클로징을 수행할 수 있다. 제어부는 경로 그래프를 구성하는 노드들의 상관관계들의 에러 값의 합이 최소가 되도록 각각의 노드의 위치 데이터를 보정할 수 있다. 실시예에 따라서, 제어부는, 조도가 기준치 미만인 영역에서는 상기 라이다 센서의 센싱 데이터에 기 초하여 현재 위치를 산출한 후, 상기 조도가 상기 기준치 이상인 영역으로 진입하면 루프 클로징(loop closin g)을 수행하여 오차를 보정할 수 있다. 즉, 조도가 낮은 어두운 영역에서는 라이다 기반의 위치 인식을 수행할 수 있다. 라이다 센서는 조도에 영향받지 않으므로 저조도 환경에서도 동일한 성능의 위치 인식이 가능하 다. 하지만, 라이다 기반의 슬램은 루프 클로징의 정확도가 떨어지는 단점이 있다. 따라서, 조도가 충분히 높은 영 역에 진입한 후에 루프 클로징을 수행할 수 있다. 이때 카메라 센서(120b)를 통하여 획득되는 영상 정보를 이용 하여 루프 클로징을 수행할 수 있다. 즉, 저조도 환경에서는 라이다 기반으로 슬램을 수행하고, 그렇지 않은 환 경에서는 비전 기반으로 루프 클로징 등 슬램을 수행할 수 있다. 주행 구역의 일부 영역은 어둡고, 일부 영역은 밝을 경우가 있다. 이 경우에, 본 발명의 일 실시예에 따른 이동 로봇은 어두운 영역을 지나갈 때에는 라이다 센서만을 가지고 노드(Node)를 생성하고 자기 위치를 계 산한다. 이 때 위치 오차는 누적이 될 수 있다. 따라서, 이동 로봇이 밝은 영역에 진입할 경우 루프 클로 징을 통해 비전 기반으로 생성된 노드의 노드 정보와 라이다 기반으로 생성된 노드의 노드 정보를 포함하는 전 체 노드 정보를 최적화하여 누적된 오차를 최소화 시킬 수 있다. 만약 어두운 영역이 일정 기간 지속될 경우, 이동 로봇의 속도를 늦추거나 멈춘 후 카메라 센서(120b)의 노출을 최대화하여 가능한 밝은 영상을 얻어 비전 기반 슬램을 진행할 수도 있다. 한편, 주행 서비스 모듈, 라이다 서비스 모듈, 비전 서비스 모듈은 소프트웨어적인 프로세스 또는 소프트웨어적인 프로세스를 수행하는 주체를 의미할 수 있다. 도 12와 도 13은 본 발명의 실시예에 따른 이동 로봇의 제어 방법의 소프트웨어 프로세스를 도시한 순서도로, 비전과 라이다의 융합 시퀀스를 도시한 것이다. 여기서, 주행 서비스 모듈, 라이다 서비스 모듈, 비전 서비스 모듈 등은 소프트웨어적인 프로세스일 수 있다. 도 14 내지 도 18은 본 발명의 실시예에 따른 이동 로봇의 제어 방법에 관한 설명에 참조되는 도면이다. 먼저, 도 12를 참조하면, 주행 서비스 모듈은 엔코더 등 주행 감지 센서에서의 센싱 데이터를 라이다 서 비스 모듈과 비전 서비스 모듈로 전달할 수 있다(S1210). 주행 감지 센서를 통하여 본체의 이동에 따른 주행 상태를 감지하고, 주행 서비스 모듈은 주행 감지 센서의 센싱 데이터를 라이다 서비스 모듈과 비전 서비스 모듈로 전달할 수 있다(S1210). 예를 들어, 주행 서비스 모듈은 50Hz의 속도로 바퀴의 엔코더 값을 읽어 라이다 서비스 모듈로 전 달할 수 있다. 비전 서비스 모듈은, 라이다 서비스 모듈에게 오도메트리 정보를 요청할 수 있다(S1220). 라이다 서비스 모듈은 비전 서비스 모듈의 요청에 응답하고(S1225), 오도메트리 정보를 생성할 수 있다(S1240). 예를 들어, 라이다 서비스 모듈은, 라이다 센서의 센싱 데이터를 수신하고, 수신된 라이다 센서 의 센싱 데이터에 기초한 지형 정보와 이전 위치 정보를 이용하여 이동 로봇의 위치 변위량을 판별할 수 있다. 또한, 라이다 서비스 모듈은 상기 주행 감지 센서의 센싱 데이터에 기초한 오도메트리 정보와 라이다 센 서의 ICP 결과를 융합하여 오도메트리 정보를 생성함으로써, 단순히 두 데이터를 병렬적으로 사용하는 것 이 아니라 오도메트리 정보에 정확한 산출에 사용할 수 있다.한편, 비전 서비스 모듈은, 영상획득부가 획득한 영상 정보를 불러오는 카메라 서비스 모듈에 게 영상 데이터를 요청하고(S1230), 카메라 서비스 모듈로부터 영상 데이터를 수신할 수 있다(S1235). 한편, 라이다 서비스 모듈은, 판별된 위치 변위량 정보를 비전 서비스 모듈로 송신할 수 있다 (S1245). 비전 서비스 모듈은 라이다 서비스 모듈로부터 위치 변위량 정보를 수신하고(S1245), 카메라 서비 스 모듈로부터 영상 데이터를 수신하여(S1235), 상기 위치 변위량을 기반으로 현재 영상에서 추출된 특징 점과 이전 위치에서 추출된 특징점 매칭을 통해 특징점 위치를 판별(S1250)함으로써, 라이다 기반의 오도메트리 정보 기반의 영상 특징점을 매칭할 수 있다(S1140). 한편, 제어부는, 산출된 현재 위치 정보를 포함하는 노드 정보를 맵에 등록하고, 노드 정보가 추가 또는 갱신된 맵을 저장부에 저장할 수 있다. 한편, 비전 서비스 모듈이 상기 노드 정보를 라이다 서비스 모듈로 송신할 수 있고(S1260), 라이다 서비스 모듈은 비전 서비스 모듈이 상기 현재 위치를 산출하는 동안에 이동 로봇이 이동한 위 치 변위량을 산출하여 수신한 노드 정보에 반영함으로써, 이동 로봇의 최종 현재 위치를 판별할 수 있다 (S1270). 즉, 라이다 서비스 모듈은 비전 서비스 모듈이 추정한 현재 위치를 보정하여 최종 현재 위치를 판별할 수 있다(S1170, S1270). 라이다 서비스 모듈은 판별된 최종 위치에 대응하는 노드 정보를 맵에 등록하거나 제어부 내 다른 모듈로 출력할 수 있다(S1280). 도 13은 참조하면, 슬램 서비스 모듈은 비전 기반의 슬램 뿐만 아니라 슬램 관련 프로세스를 총괄할 수 있다. 슬램 서비스 모듈은 비전 기반의 슬램에 라이다 기반의 융합하는 경우에, 비전 서비스 모듈 의 기능, 동작이 확장되어 구현될 수 있다. 도 12와 도 13을 참조하면, 슬램 서비스 모듈과 라이다 서비스 모듈은 주행 서비스 모듈로부 터 바퀴의 엔코더 값을 전달받을 수 있다. 슬램 서비스 모듈은 라이다 서비스 모듈로 라이다 데이터를 요청할 수 있고(S1310), 라이다 서비스 모듈은 라이다 데이터를 준비한다는 응답을 슬램 서비스 모듈로 송신할 수 있다(S1315). 라이다 서비스 모듈은 이동 로봇의 직전 위치와 바퀴의 엔코더 값을 기반으로 현재 위치를 예측한 후 라이다 센서로부터 입력되는 지형(geometry) 정보를 이용하여 위치 변위량 및 현재 위치를 예상하고 , 예상값을 슬램 서비스 모듈에 전달할 수 있다(S1340). 실시예에 따라서, 라이다 서비스 모듈은 위치 변위량을 포함하는 오도메트리 정보 및 공분산 형태의 불확 실성 확률값을 슬램 서비스 모듈로 송신할 수 있다. 슬램 서비스 모듈은 카메라 서비스 모듈에 영상을 요청할 수 있다(S1320). 이때 슬램 서비스 모듈 은 주행 서비스 모듈로부터 수신한 엔코더 값에 따라 대응하는 영상 정보를 요청할 수 있다. 슬램 서비스 모듈은, 카메라 서비스 모듈로부터 영상을 받고(S1325), 라이다 서비스 모듈로 부터 입력된 위치 변위량을 기반으로 현재 영상에서 추출된 특징점과 전 위치에서 추출된 특징점 매칭을 통해 3D 특징점 위치를 계산할 수 있다(S1350). 또한, 슬램 서비스 모듈은, 이렇게 계산된 3D 점들을 기반으로 이동 로봇이 이동한 변위량과 현재 위치를 계산할 수 있다(S1350). 한편, 슬램 서비스 모듈은, 이렇게 계산된 결과를 노드(Node) 형태의 노드 정보로 저장할 수 있다. 여기서 저장되는 노드 정보는 등록할 노드 인덱스(index) 정보, 전역 포즈 정보(X,Y,θ), 전역 불확실성 값 등 을 포함할 수 있다 실시예에 따라서, 슬램 서비스 모듈은 계산된 결과를 노드(Node) 형태로 저장한 후 노드 정보를 라이다 서비스 모듈에 넘겨준다(S1360). 라이다 서비스 모듈은 슬램 서비스 모듈에서 계산하는 동안 이동 로봇이 이동한 위치를 추가하여 현재 로봇 위치를 알아낼 수 있다.라이다 센서의 센싱 데이터를 이용하는 라이다 슬램은, 조도 변화에 무관하다는 장점이 있다. 도 14와 같은 환경 A에서는 라이다 슬램만으로 맵핑(mapping) 및 위치인식이 가능하고, 라이다 센서는 장 애물 감지 및 주행 방향 설정 센서로도 활용할 수 있다. 도 14의 환경 A는 장애물(1411, 1412, 1413, 1414)을 회피하면서 주행하면 로봇이 이동한 곳을 다시 되돌아 오 는 환경이므로 에러(error)가 커지지 않아 라이다 슬램만으로도 정상적인 맵 작성이 가능하다. 하지만, 라이다 슬램만 사용하는 경우에, 도 15와 같은 환경 B에서는 이동 로봇이 계속 새로운 곳을 보면 서 움직이므로 에러가 커진다. 이동 로봇이 이 상태로 첫 출발점으로 되돌아 왔을 때 이동 로봇 이 있는 곳이 첫 출발 지점인 것을 알기 어렵다. 도 15의 환경 B는 여러 장애물(1511, 1512, 1513) 중 적어도 하나가 공간 중앙에 크게 있는 환경으로 라 이다 슬램만으로는 루프 클로징(Loop closing) 문제로 맵 생성에 어려움이 있다. 도 16을 참조하면, 이동 로봇은 소정 경로로 이동한 후에 어느 한 지점(Px)이 출발 지점(Po)와 일치 하는 지 여부플 판별하고, 동일 지점으로 확인되는 경우에 에러를 최소화하는 최적화 작업을 수행하여 그래프 기반의 슬램을 수행할 수 있다. 지점(Px)이 출발 지점(Po)와 일치하면, 에러 정정 알고리즘에 따라 에러를 정정하여 정확한 경로로 경로 정보를 수정함으로써, 정확한 위치 인식 및 맵 작성이 가능하다. 따라서, 라이다 슬램을 이용하기 위해서는 정확한 루프 클로징 알고리즘과 에러 정정(Error Correction) 알고리 즘이 필요하다. 한편, 영상획득부를 통하여 획득한 영상을 이용하는 비전 슬램의 경우에, 도 14의 환경 A와 도 15의 환경 B 모두 정확한 루프 클로징을 할 수 있어 맵핑 및 위치 인식이 가능하다. 하지만, 비전 슬램은 조도에 따른 성능 변화가 있어, 조도가 낮아 영상에서 검출되는 특징의 양이 감소함에 따 라 성능에 차이가 발생할 수 있다. 특히 조도가 매우 낮으면 획득 영상으로부터 특징 추출이 불가능하여, 매핑 및 위치 인식도 불가능한 단점이 있다. 따라서, 비전 슬램을 이용하는 경우에 저조도 이하의 환경에서 동작하기 어려운 한계점을 라이다 센서를 이용한 위치 인식 기술을 통해 극복할 수 있다. 또한, 라이다 슬램의 맵을 비전 슬램의 루프 클로징과 에러 정정으로 보정함으로써, 도 15의 환경 B와 같은 공 간에서 라이다 맵핑 오류를 개선할 수 있다. 본 발명의 일 실시예에 따르면, 라이다 센서를 이용한 라이다 슬램과 영상획득부를 이용한 비전 슬램 을 상호 보완적으로 활용하여 로봇 이동 시 어두운 영역과 밝은 영역에서도 안정적인 자기 위치 인식이 가능하 다. 도 17을 참조하면, 라이다 센서를 이용한 라이다 슬램을 먼저 수행하고(S1710), 루프 클로징에 장점이 있 는 비전 슬램을 수행하여(S1720), 맵을 생성하고 저장할 수 있다(S1730). 즉, 라이다 슬램으로 맵을 생성하고(S1710), 라이다 슬램으로 생성된 맵을 비전 슬램의 루프 클로징과 에러 정 정으로 보정함으로써(S1720), 최종 맵을 생성할 수 있다(S1730). 도 18은 라이다 슬램으로 생성된 맵과 루프 클로징과 에러 정정이 수행된 맵을 예시한다. 바람직하게는, 도 1 내지 도 13을 참조하여 설명한 것과 같이, 라이다 센서의 센셍 데이터에 기초한 오도 메트리(odometry) 정보에 기초하여 비전 슬램을 수행할 수 있다. 또한, 비전 슬램 연산 과정의 소요시간 동안 이동 로봇의 이동량을 추가 반영하여 이동 로봇의 현재 위치를 판별할 수 있다. 본 발명의 일 실시예에 따르면, 영상 기반의 특징점 매칭이 잘 안되는 경우에도 주변 노드들과의 상관관계를 라 이다 센서에서 계산해줄 수 있다. 즉,영상 기반의 특징점 매칭이 잘 안되는 경우 라이다 서비스 모듈 에 정보를 알려주고 라이다 서비스 모듈에서 상관관계(Constraint)를 생성할 수 있다. 이에 따라, 더욱 풍부한 상관관계를 이용한 최적화가 가능하다는 장점이 있다. 밝은 영역에서도 영상의 특징점 매칭의 한계로 매칭이 불완전할 때가 있다. 따라서, 현재 비전 서비스 모듈 이 생각하는 위치와 주변 노드가 일정 거리 이하임에도 매칭이 잘 안될 경우, 비전 서비스 모듈이라이다 서비스 모듈로 상관관계의 추가를 요청할 수 있다. 라이다 서비스 모듈는 현재 노드와 주변 노드간 ICP 매칭(Matching)을 수행하여 Constraint를 추가할 수 있다. 이를 통해 노드 간 Constraint를 추가함으로써 더 많은 노드간 constraint를 이용하여 정확한 위치 추정 이 가능해진다. 영상 기반으로 계산한 오도메트리(Odometry) 정보의 경우 스케일 드리프트(Scale Drift)가 발생할 수 있지만 라 이다 기반의 지형 정보를 함께 고려함으로써 스케일 드리프트(Scale Drift) 최소화도 가능하다. 또한, 본 발명의 일 실시예에 따르면, 영상 기반의 비전 슬램과 라이다 기반의 라이다 슬램 간 뷰 포인트(View Point)에 대한 정밀한 교정(Calibration)이 필요 없이 퓨전(fusion) 슬램이 가능한 장점이 있다. 도 19는 본 발명의 실시예에 따른 슬램에 관한 설명에 참조되는 도면으로, 카메라 센서(120b)와 라이다 센서 에서 획득된 데이터에 기초한 상관관계인 constraints를 하나의 슬램 프레임워크(Framework)인 슬램 서비 스 모듈에서 최적화하는 실시예를 개시한다. 도 19의 슬램 서비스 모듈은 비전 기반의 슬램 뿐만 아니라 슬램 관련 프로세스를 총괄하는 것으로, 비전 서비스 모듈의 기능, 동작이 확장되어 구현될 수 있고, Visual-LiDAR 슬램 서비스로도 명명될 수 있다. 슬램 서비스 모듈은 카메라 센서(120b)로부터 영상 데이터를 수신할 수 있다. 또한, 라이다 서비스 모듈 은 라이다 센서로부터 센싱 데이터를 수신할 수 있다. 한편, 라이다 서비스 모듈과 슬램 서비스 모듈은 주행 서비스 모듈로부터 주행 서비스 모듈 에서 획득된 오도메트리 정보를 전달받을 수 있다. 예를 들어, 엔코더는 이동 로봇의 주행 중 바퀴 동작에 기초하는 오도메트리 정보를 라이다 서비스 모듈과 슬램 서비스 모듈로 전달할 수 있 다. 슬램 서비스 모듈은 라이다 서비스 모듈로 라이다 서비스 모듈에서 취득된 상관관계 정보를 요청할 수 있다(S1910). 슬램 서비스 모듈은 라이다 서비스 모듈로 직전 프레임(frame)으로부터의 상대적인 위치 정보 (relative pose)를 요청할 수 있다. 직전 프레임으로부터의 상대적인 위치 정보는 이동 로봇의 직전 위치 에서 현재 위치까지의 상대적인 위치 정보로 위치 변위량일 수 있고, ICP 매칭 결과로 나온 정보일 수 있다. 또한, 슬램 서비스 모듈은 라이다 서비스 모듈로 루프 변위(Loop Constraint)를 요청할 수 있다. 예를 들어, 매칭 대상 프레임(frame)의 인덱스(index), 로컬 맵(local map) 범위 내에서 매칭되는 루프 변위 (Loop Constraint)를 요청할 수 있다. 라이다 서비스 모듈은 슬램 서비스 모듈의 요청에 응답할 수 있다(S1920). 예를 들어, 라이다 서비 스 모듈은 슬램 서비스 모듈로 직전 프레임(frame)으로부터의 상대적인 위치 정보(relative pose), 로컬 맵(local map) 범위 내에서 매칭되는 루프 변위(Loop Constraint)를 응답할 수 있다. 한편, 슬램 서비스 모듈은 카메라 센서(120b)와 라이다 센서로부터 취득된 각각의 constraints를 통 합할 수 있다. 슬램 서비스 모듈은 라이다 서비스 모듈에서 수신한 정보에 비전 슬램 결과를 융합하여 현재 위치 를 판별할 수 있다. 슬램 서비스 모듈은 라이다 서비스 모듈에서 수신한 정보에 비전 슬램 결과를 융합하여 노드 정보 를 업데이트하고 슬램 맵을 생성할 수 있다. 슬램 서비스 모듈은, 현재 보정된 위치를 판별하고, 전체 노드의 포즈(pose)를 포함하는 포즈-그래프 (pose-graph)를 보정할 수 있다. 즉, 슬램 서비스 모듈은, 이동 로봇의 현재 위치를 판별하고, 슬램 맵의 노드 정보를 추가, 삭제, 변경하여, 슬램 맵을 생성하거나 생성된 슬램 맵을 업데이트할 수 있다. 한편, 슬램 서비스 모듈은, 이동 로봇의 현재 위치, 보정된 포즈-그래프 정보, 현재 위치에 대응하 는 로컬 맵에 해당하는 프레임 인덱스 정보, 현재 노드, 삭제된 노드 및 연결 노드 정보 등을 라이다 서비스 모 듈로 송신할 수 있다(S1930).도 20은 본 발명의 실시예에 따른 슬램에 관한 설명에 참조되는 도면으로, 비전-라이다 퓨전 슬램 서비스(200 0)의 구성 개념도이다. 도 20에서 예시된 비전-라이다 퓨전 슬램 서비스의 구성들은 소프트웨어적인 서비스이며, 비전 슬램과 라 이다 슬램은 각각 다른 쓰레드(thread)로 비동기식으로 동작할 수 있다. 이에 따라, 비전 슬램과 라이다 슬램 방식 각각의 고유의 성능은 기본적으로 확보하고, 이를 비전-라이다 퓨전 슬램 서비스에서 통합하여 더 향상된 성능을 확보할 수 있다. 도 20을 참조하면, 슬램 메인은 퓨전 슬램 서비스 내 각 서비스 모듈에서 데이터를 받아 필요한 서 비스 모듈에 전달하고, 응답을 수신하는 허브(hub)로 동작할 수 있다. Visual Odometry(VO, 2050)는, 카메라 센서(120b)로부터 획득되는 영상에서 주행 거리 등을 추정하는 비전 기반 의 오도메트리 판별을 수행할 수 있다. Visual Odometry는 카메라 센서(120b)로부터 획득되는 영상에서 특징점을 추출하고 특징점 매칭(Feature extraction matching, FEM, 2010)을 수행할 수 있다. 하지만, 저조도 환경에서 획득된 영상에서는 특징점 추출 및 매칭이 어려울 수 있다. 따라서, 라이다 서비스 모듈에서 ICP 매칭을 수행하여 오도메트리 정보를 획득하고, 이에 기반하여 퓨전 슬램을 수행하는 것이 바람직하다. 이와 같이, 로컬 맵 범위 내에서, Visual Odometry를 통하여 획득한 오도메트리 정보 및/또는 라이다 서 비스 모듈가 ICP 매칭을 수행하여 획득한 오도메트리 정보를 이용하여, 전역 위치를 판별할 수 있 다. 예를 들어, Global Pose Tracker(GPT, 2020)는 오도메트리 정보를 불러와서 전역 위치를 판별할 수 있다. 한편, Global Mapper(GM, 2030)는 로컬 맵 범위 내에서 판별된 정보를 취합하고 최적화할 수 있다. 또한, Global Mapper는 특징점 사전인 Vocabulary tree(VT, 2060)를 생성할 수 있다. 한편, Kidnap Recovery(KR, 2040)는 로컬 맵 범위 내에서 판별된 정보를 취합하고 최적화할 수 있다. 슬램 메인은 Global Pose Tracker에서 루프 변위(Loop Constraint)를 가져올 수 있다. 또한, 슬램 메인은 새 프레임과 위치변화량, 루프 변위(Loop Constraint)를 Global Mapper의 쓰레드로 전달할 수 있다. 슬램 메인은 Visual Odometry에서 새 프레임의 특징점 정보와 보정된 위치를 가져오고. 새 프레임 을 Global Mapper의 포즈-그래프 노드와 매칭하여 루프 변위(Loop Constraint)를 생성할 수 있다. Global Pose Tracker에서는 위치 추정을 수행하고, 슬램 메인은 추정된 위치 정보에 따라 포즈-그 래프의 노드 정보를 갱신할 수 있다. 슬램 메인은 현재 보정된 위치를 판별하고, 전체 노드의 포즈(pose)를 포함하는 포즈-그래프(pose- graph)를 보정할 수 있다. 슬램 메인은, 이동 로봇의 현재 위치를 판별하고, 슬램 맵의 노드 정보를 추가, 삭제, 변경하여, 슬 램 맵을 생성하거나 생성된 슬램 맵을 업데이트할 수 있다. 본 발명에 따른 이동 로봇은 상기한 바와 같이 설명된 실시예들의 구성과 방법이 한정되게 적용될 수 있는 것이 아니라, 상기 실시예들은 다양한 변형이 이루어질 수 있도록 각 실시예들의 전부 또는 일부가 선택적으로 조합 되어 구성될 수도 있다. 마찬가지로, 특정한 순서로 도면에서 동작들을 묘사하고 있지만, 이는 바람직한 결과를 얻기 위하여 도시된 그 특정한 순서나 순차적인 순서대로 그러한 동작들을 수행하여야 한다거나, 모든 도시된 동작들이 수행되어야 하 는 것으로 이해되어서는 안 된다. 특정한 경우, 멀티태스킹과 병렬 프로세싱이 유리할 수 있다. 한편, 본 발명의 실시예에 따른 이동 로봇의 제어 방법은, 프로세서가 읽을 수 있는 기록매체에 프로세서가 읽 을 수 있는 코드로서 구현하는 것이 가능하다. 프로세서가 읽을 수 있는 기록매체는 프로세서에 의해 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 또한, 인터넷을 통한 전송 등과 같은 캐리어 웨이브 의 형태로 구현되는 것도 포함한다. 또한, 프로세서가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산방식으로 프로세서가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 또한, 이상에서는 본 발명의 바람직한 실시예에 대하여 도시하고 설명하였지만, 본 발명은 상술한 특정의 실시"}
{"patent_id": "10-2019-0035039", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "예에 한정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 발명이 속하는 기술분야 에서 통상의 지식을 가진자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2019-0035039", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 이동 로봇 및 이동 로봇을 충전시키는 충전대를 도시한 사시도이다. 도 2는 도 1에 도시된 이동 로봇의 상면부를 도시한 도이다. 도 3은 도 1에 도시된 이동 로봇의 정면부를 도시한 도이다. 도 4는 도 1에 도시된 이동 로봇의 저면부를 도시한 도이다. 도 5는 본 발명의 실시예에 따른 이동 로봇의 주요 구성들 간의 제어관계를 도시한 블록도이다. 도 6은 본 발명의 일 실시예에 따른 이동 로봇의 제어 방법을 도시한 순서도이다. 도 7 내지 도 10은 도 6의 제어 방법에 관한 설명에 참조되는 도면이다. 도 11은 본 발명의 일 실시예에 따른 이동 로봇의 제어 방법을 도시한 순서도이다. 도 12와 도 13은 본 발명의 실시예에 따른 이동 로봇의 제어 방법의 소프트웨어 프로세스를 도시한 순서도이다. 도 14 내지 도 18은 본 발명의 실시예에 따른 이동 로봇의 제어 방법에 관한 설명에 참조되는 도면이다. 도 19는 본 발명의 실시예에 따른 슬램에 관한 설명에 참조되는 도면이다. 도 20은 본 발명의 실시예에 따른 슬램에 관한 설명에 참조되는 도면이다."}
