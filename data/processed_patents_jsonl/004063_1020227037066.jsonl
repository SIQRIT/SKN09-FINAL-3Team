{"patent_id": "10-2022-7037066", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0153088", "출원번호": "10-2022-7037066", "발명의 명칭": "비디오 태그 추천 모델의 트레이닝 방법 및 비디오 태그 확정 방법", "출원인": "베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니", "발명자": "지 예"}}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "처리하고자 하는 영상을 트레이닝하고자 하는 인식 모델에 입력하는 것,상기 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 상기 처리하고자 하는 영상의 적어도 한 장의 특징맵을 출력하는 것,상기 트레이닝하고자 하는 인식 모델의 헤드를 통해, 상기 적어도 한 장의 특징 맵에 따라 상기 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터, 및 상기 처리하고자 하는 영상 중의 상기 제1 목표 물체의 예측 데이터와 연관되는 제2 목표 물체의 예측 데이터를 취득하는 것, 및제1 목표 물체의 예측 데이터, 제2 목표 물체의 예측 데이터, 제1 목표 물체의 플래그 데이터 및 제2 목표 물체의 플래그 데이터에 따라, 상기 트레이닝하고자 하는 인식 모델을 최적화하여, 트레이닝된 인식 모델을 얻는 것을 포함하는인식 모델의 트레이닝 방법."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 제1 목표 물체의 예측 데이터는 상기 제1 목표 물체의 분류 예측 데이터와 상기 제1 목표 물체의 속성 예측 데이터를 포함하고, 상기 제2 목표 물체의 예측 데이터는 상기 제2 목표 물체의 예측 데이터와 상기 제2 목표 물체의 속성 예측 데이터를 포함하는인식 모델의 트레이닝 방법."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 또는 제2항 중 어느 한 항에 있어서,상기 트레이닝하고자 하는 인식 모델의 헤드를 통해, 제1 목표 물체의 예측 데이터, 및 상기 제1 목표 물체의예측 데이터와 연관되는 제2 목표 물체의 예측 데이터를 출력하는 것은,상기 특징 맵의 각 화소에 대해, 상기 제1 목표 물체의 앵커 박스 예측 데이터 및 상기 제2 목표 물체의 앵커박스 예측 데이터를 출력하는 것, 및상기 제1 목표 물체의 앵커 박스 예측 데이터 및 상기 제2 목표 물체의 앵커 박스 예측 데이터에 따라, 상기 제1 목표 물체의 예측 데이터 및 상기 제2 목표 물체의 예측 데이터를 출력하는 것을 포함하는인식 모델의 트레이닝 방법."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 내지 제3항 중 어느 한 항에 있어서,상기 제1 목표 물체는 사람의 얼굴이고,상기 제2 목표 물체는 인체인인식 모델의 트레이닝 방법."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항 내지 제4항 중 어느 한 항에 있어서,상기 특징 출력층은 백본 네트워크 및 특징 피라미드 네트워크를 포함하고,공개특허 10-2022-0153088-3-상기 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 상기 처리하고자 하는 영상의 적어도 한 장의 특징맵을 출력하는 것은,상기 백본 네트워크를 통해, 상기 처리하고자 하는 영상의 복수 장의 제1 특징 맵을 출력하는 것,상기 복수 장의 제1 특징 맵 중의 N장의 제2 특징 맵을 상기 특징 피라미드 네트워크에 입력하는 것, 여기서, N은 1 이상의 정수이고,상기 특징 피라미드 네트워크를 통해, N장의 제3 특징 맵을 출력하는 것, 및상기 N장의 제3 특징 맵을 상기 특징 맵으로 하는 것을 포함하는인식 모델의 트레이닝 방법."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "인식하고자 하는 영상을 인식 모델에 입력하여, 상기 인식하고자 하는 영상 중의 제1 목표 물체의 예측 데이터및 상기 제1 목표 물체와 연관되는 제2 목표 물체의 예측 데이터를 취득하는 것을 포함하고,상기 인식 모델은 제1항 내지 제5항 중 어느 한 항의 트레이닝된 인식 모델인인식 방법."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 인식하고자 하는 영상은 인식하고자 하는 비디오 중의 프레임 영상이고,상기 방법은,상기 제1 목표 물체의 예측 데이터 및 상기 제2 목표 물체의 예측 데이터에 따라, 상기 인식하고자 하는 비디오중의 키 프레임 영상을 취득하는 것을 더 포함하는인식 방법."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "처리하고자 하는 영상을 트레이닝하고자 하는 인식 모델에 입력하기 위한 제1 입력 모듈,상기 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 상기 처리하고자 하는 영상의 적어도 한 장의 특징맵을 출력하기 위한 특징 맵 모듈,상기 트레이닝하고자 하는 인식 모델의 헤드를 통해, 상기 적어도 한 장의 특징 맵에 따라 상기 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터, 및 상기 처리하고자 하는 영상 중의 상기 제1 목표 물체의 예측 데이터와 연관되는 제2 목표 물체의 예측 데이터를 취득하기 위한 예측 데이터 모듈, 및제1 목표 물체의 예측 데이터, 제2 목표 물체의 예측 데이터, 제1 목표 물체의 플래그 데이터 및 제2 목표 물체의 플래그 데이터에 따라, 상기 트레이닝하고자 하는 인식 모델을 최적화하여, 트레이닝된 인식 모델을 얻기 위한 트레이닝 모듈을 포함하는인식 모델의 트레이닝 장치."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 제1 목표 물체의 예측 데이터는 상기 제1 목표 물체의 분류 예측 데이터와 상기 제1 목표 물체의 속성 예측 데이터를 포함하고,상기 제2 목표 물체의 예측 데이터는 상기 제2 목표 물체의 예측 데이터와 상기 제2 목표 물체의 속성 예측 데이터를 포함하는인식 모델의 트레이닝 장치.공개특허 10-2022-0153088-4-청구항 10 제8항 또는 제9항 중 어느 한 항에 있어서,상기 예측 데이터 모듈은,상기 특징 맵의 각 화소에 대해, 상기 제1 목표 물체의 앵커 박스 예측 데이터 및 상기 제2 목표 물체의 앵커박스 예측 데이터를 출력하기 위한 제1 예측 수단, 및상기 제1 목표 물체의 앵커 박스 예측 데이터 및 상기 제2 목표 물체의 앵커 박스 예측 데이터에 따라, 상기 제1 목표 물체의 예측 데이터 및 상기 제2 목표 물체의 예측 데이터를 출력하기 위한 제2 예측 수단을 포함하는인식 모델의 트레이닝 장치."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항 내지 제10항 중 어느 한 항에 있어서,상기 제1 목표 물체는 사람의 얼굴이고,상기 제2 목표 물체는 인체인인식 모델의 트레이닝 장치."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항 내지 제11항 중 어느 한 항에 있어서,상기 특징 출력층은 백본 네트워크 및 특징 피라미드 네트워크를 포함하고,상기 특징 맵 모듈은,상기 백본 네트워크를 통해, 처리하고자 하는 영상의 복수 장의 제1 특징 맵을 출력하기 위한 제1 특징 맵수단,상기 복수 장의 제1 특징 맵 중의 N장의 제2 특징 맵을 상기 특징 피라미드 네트워크에 입력하기 위한 제1 특징맵 입력 수단, 여기서, N은 1 이상의 정수이고,상기 특징 피라미드 네트워크를 통해, N장의 제3 특징 맵을 출력하기 위한 제2 특징 맵 수단, 및상기 N장의 제3 특징 맵을 상기 특징 맵으로 하기 위한 제2 특징 맵 처리 수단을 포함하는인식 모델의 트레이닝 장치."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "인식하고자 하는 영상을 인식 모델에 입력하여, 상기 인식하고자 하는 영상 중의 제1 목표 물체의 예측 데이터및 상기 제1 목표 물체와 연관되는 제2 목표 물체의 예측 데이터를 취득하기 위한 제2 입력 모듈을 포함하고,상기 인식 모델은 제8항 내지 제12항 중 어느 한 항의 트레이닝된 인식 모델인인식 장치."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 인식하고자 하는 영상은 인식하고자 하는 비디오 중의 프레임 영상이고,상기 장치는,상기 제1 목표 물체의 예측 데이터 및 상기 제2 목표 물체의 예측 데이터에 따라, 상기 인식하고자 하는 비디오중의 키 프레임 영상을 취득하기 위한 키 프레임 영상 모듈을 더 포함하는인식 장치.공개특허 10-2022-0153088-5-청구항 15 적어도 하나의 프로세서, 및상기 적어도 하나의 프로세서와 통신가능하게 연결되는 메모리를 포함하는 전자장비로서,상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되어 있고, 상기 명령이 상기 적어도 하나의 프로세서에 의해 실행될 경우, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제7항 중 어느한 항의 방법을 실행하도록 하는 것을 특징으로 하는전자장비."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "컴퓨터 명령이 저장되어 있는 비 일시적 컴퓨터 판독가능 저장매체로서,상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제7항 중 어느 한 항의 방법을 실행하도록 하는 것을 특징으로 하는비 일시적 컴퓨터 판독가능 저장매체."}
{"patent_id": "10-2022-7037066", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "컴퓨터 프로그램을 포함한 컴퓨터 프로그램 제품으로서,상기 컴퓨터 프로그램이 프로세서에 의해 실행될 경우,제1항 내지 제7항 중 어느 한 항의 방법을 구현하는 컴퓨터 프로그램 제품."}
{"patent_id": "10-2022-7037066", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인식 모델의 트레이닝 방법, 인식 방법, 장치, 장비 및 저장매체에 관한 것으로서, 딥러닝, 컴퓨터 비 전"}
{"patent_id": "10-2022-7037066", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것이다. 구체적인 구현 방안은, 처리하고자 하는 영상을 트레이닝하고자 하는 인식 모델에 입력하는 것, 상기 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 상기 처리하고자 하는 영상의 적어도 (뒷면에 계속) 대 표 도 - 도1"}
{"patent_id": "10-2022-7037066", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2022-0153088 한 장의 특징 맵을 출력하는 것, 상기 트레이닝하고자 하는 인식 모델의 헤드를 통해, 상기 적어도 한 장의 특징 맵에 따라 상기 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터, 및 상기 처리하고자 하는 영상 중의 상기 제1 목표 물체의 예측 데이터와 연관되는 제2 목표 물체의 예측 데이터를 취득하는 것, 및 제1 목표 물체의 예측 데이터, 제2 목표 물체의 예측 데이터, 제1 목표 물체의 플래그 데이터 및 제2 목표 물체의 플래그 데이터 에 따라, 상기 트레이닝하고자 하는 인식 모델을 최적화하여, 트레이닝된 인식 모델을 얻는 것을 포함한다. 본 개시의 실시는 인식 모델의 인식 효율 및 인식 효과를 향상시킬 수 있다. CPC특허분류 G06N 3/08 (2013.01) G06V 20/46 (2022.01) G06V 40/103 (2022.01) G06V 40/168 (2022.01) G06V 40/172 (2022.01) 발명자 허웨이 왕 중국 베이징 하이디안 디스트릭트 샹디 10번가 넘 버 10, 바이두 캠퍼스 2층리 게 중국 베이징 하이디안 디스트릭트 샹디 10번가 넘 버 10, 바이두 캠퍼스 2층명 세 서 청구범위 청구항 1 처리하고자 하는 영상을 트레이닝하고자 하는 인식 모델에 입력하는 것, 상기 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 상기 처리하고자 하는 영상의 적어도 한 장의 특징 맵을 출력하는 것, 상기 트레이닝하고자 하는 인식 모델의 헤드를 통해, 상기 적어도 한 장의 특징 맵에 따라 상기 처리하고자 하 는 영상 중의 제1 목표 물체의 예측 데이터, 및 상기 처리하고자 하는 영상 중의 상기 제1 목표 물체의 예측 데 이터와 연관되는 제2 목표 물체의 예측 데이터를 취득하는 것, 및 제1 목표 물체의 예측 데이터, 제2 목표 물체의 예측 데이터, 제1 목표 물체의 플래그 데이터 및 제2 목표 물체 의 플래그 데이터에 따라, 상기 트레이닝하고자 하는 인식 모델을 최적화하여, 트레이닝된 인식 모델을 얻는 것 을 포함하는 인식 모델의 트레이닝 방법. 청구항 2 제1항에 있어서, 상기 제1 목표 물체의 예측 데이터는 상기 제1 목표 물체의 분류 예측 데이터와 상기 제1 목표 물체의 속성 예 측 데이터를 포함하고, 상기 제2 목표 물체의 예측 데이터는 상기 제2 목표 물체의 예측 데이터와 상기 제2 목 표 물체의 속성 예측 데이터를 포함하는 인식 모델의 트레이닝 방법. 청구항 3 제1항 또는 제2항 중 어느 한 항에 있어서, 상기 트레이닝하고자 하는 인식 모델의 헤드를 통해, 제1 목표 물체의 예측 데이터, 및 상기 제1 목표 물체의 예측 데이터와 연관되는 제2 목표 물체의 예측 데이터를 출력하는 것은, 상기 특징 맵의 각 화소에 대해, 상기 제1 목표 물체의 앵커 박스 예측 데이터 및 상기 제2 목표 물체의 앵커 박스 예측 데이터를 출력하는 것, 및 상기 제1 목표 물체의 앵커 박스 예측 데이터 및 상기 제2 목표 물체의 앵커 박스 예측 데이터에 따라, 상기 제 1 목표 물체의 예측 데이터 및 상기 제2 목표 물체의 예측 데이터를 출력하는 것을 포함하는 인식 모델의 트레이닝 방법. 청구항 4 제1항 내지 제3항 중 어느 한 항에 있어서, 상기 제1 목표 물체는 사람의 얼굴이고, 상기 제2 목표 물체는 인체인 인식 모델의 트레이닝 방법. 청구항 5 제1항 내지 제4항 중 어느 한 항에 있어서, 상기 특징 출력층은 백본 네트워크 및 특징 피라미드 네트워크를 포함하고,상기 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 상기 처리하고자 하는 영상의 적어도 한 장의 특징 맵을 출력하는 것은, 상기 백본 네트워크를 통해, 상기 처리하고자 하는 영상의 복수 장의 제1 특징 맵을 출력하는 것, 상기 복수 장의 제1 특징 맵 중의 N장의 제2 특징 맵을 상기 특징 피라미드 네트워크에 입력하는 것, 여기서, N 은 1 이상의 정수이고, 상기 특징 피라미드 네트워크를 통해, N장의 제3 특징 맵을 출력하는 것, 및 상기 N장의 제3 특징 맵을 상기 특징 맵으로 하는 것을 포함하는 인식 모델의 트레이닝 방법. 청구항 6 인식하고자 하는 영상을 인식 모델에 입력하여, 상기 인식하고자 하는 영상 중의 제1 목표 물체의 예측 데이터 및 상기 제1 목표 물체와 연관되는 제2 목표 물체의 예측 데이터를 취득하는 것을 포함하고, 상기 인식 모델은 제1항 내지 제5항 중 어느 한 항의 트레이닝된 인식 모델인 인식 방법. 청구항 7 제6항에 있어서, 상기 인식하고자 하는 영상은 인식하고자 하는 비디오 중의 프레임 영상이고, 상기 방법은, 상기 제1 목표 물체의 예측 데이터 및 상기 제2 목표 물체의 예측 데이터에 따라, 상기 인식하고자 하는 비디오 중의 키 프레임 영상을 취득하는 것을 더 포함하는 인식 방법. 청구항 8 처리하고자 하는 영상을 트레이닝하고자 하는 인식 모델에 입력하기 위한 제1 입력 모듈, 상기 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 상기 처리하고자 하는 영상의 적어도 한 장의 특징 맵을 출력하기 위한 특징 맵 모듈, 상기 트레이닝하고자 하는 인식 모델의 헤드를 통해, 상기 적어도 한 장의 특징 맵에 따라 상기 처리하고자 하 는 영상 중의 제1 목표 물체의 예측 데이터, 및 상기 처리하고자 하는 영상 중의 상기 제1 목표 물체의 예측 데 이터와 연관되는 제2 목표 물체의 예측 데이터를 취득하기 위한 예측 데이터 모듈, 및 제1 목표 물체의 예측 데이터, 제2 목표 물체의 예측 데이터, 제1 목표 물체의 플래그 데이터 및 제2 목표 물체 의 플래그 데이터에 따라, 상기 트레이닝하고자 하는 인식 모델을 최적화하여, 트레이닝된 인식 모델을 얻기 위 한 트레이닝 모듈을 포함하는 인식 모델의 트레이닝 장치. 청구항 9 제8항에 있어서, 상기 제1 목표 물체의 예측 데이터는 상기 제1 목표 물체의 분류 예측 데이터와 상기 제1 목표 물체의 속성 예 측 데이터를 포함하고, 상기 제2 목표 물체의 예측 데이터는 상기 제2 목표 물체의 예측 데이터와 상기 제2 목표 물체의 속성 예측 데 이터를 포함하는 인식 모델의 트레이닝 장치.청구항 10 제8항 또는 제9항 중 어느 한 항에 있어서, 상기 예측 데이터 모듈은, 상기 특징 맵의 각 화소에 대해, 상기 제1 목표 물체의 앵커 박스 예측 데이터 및 상기 제2 목표 물체의 앵커 박스 예측 데이터를 출력하기 위한 제1 예측 수단, 및 상기 제1 목표 물체의 앵커 박스 예측 데이터 및 상기 제2 목표 물체의 앵커 박스 예측 데이터에 따라, 상기 제 1 목표 물체의 예측 데이터 및 상기 제2 목표 물체의 예측 데이터를 출력하기 위한 제2 예측 수단을 포함하는 인식 모델의 트레이닝 장치. 청구항 11 제8항 내지 제10항 중 어느 한 항에 있어서, 상기 제1 목표 물체는 사람의 얼굴이고, 상기 제2 목표 물체는 인체인 인식 모델의 트레이닝 장치. 청구항 12 제8항 내지 제11항 중 어느 한 항에 있어서, 상기 특징 출력층은 백본 네트워크 및 특징 피라미드 네트워크를 포함하고, 상기 특징 맵 모듈은, 상기 백본 네트워크를 통해, 처리하고자 하는 영상의 복수 장의 제1 특징 맵을 출력하기 위한 제1 특징 맵 수단, 상기 복수 장의 제1 특징 맵 중의 N장의 제2 특징 맵을 상기 특징 피라미드 네트워크에 입력하기 위한 제1 특징 맵 입력 수단, 여기서, N은 1 이상의 정수이고, 상기 특징 피라미드 네트워크를 통해, N장의 제3 특징 맵을 출력하기 위한 제2 특징 맵 수단, 및 상기 N장의 제3 특징 맵을 상기 특징 맵으로 하기 위한 제2 특징 맵 처리 수단을 포함하는 인식 모델의 트레이닝 장치. 청구항 13 인식하고자 하는 영상을 인식 모델에 입력하여, 상기 인식하고자 하는 영상 중의 제1 목표 물체의 예측 데이터 및 상기 제1 목표 물체와 연관되는 제2 목표 물체의 예측 데이터를 취득하기 위한 제2 입력 모듈을 포함하고, 상기 인식 모델은 제8항 내지 제12항 중 어느 한 항의 트레이닝된 인식 모델인 인식 장치. 청구항 14 제13항에 있어서, 상기 인식하고자 하는 영상은 인식하고자 하는 비디오 중의 프레임 영상이고, 상기 장치는, 상기 제1 목표 물체의 예측 데이터 및 상기 제2 목표 물체의 예측 데이터에 따라, 상기 인식하고자 하는 비디오 중의 키 프레임 영상을 취득하기 위한 키 프레임 영상 모듈을 더 포함하는 인식 장치.청구항 15 적어도 하나의 프로세서, 및 상기 적어도 하나의 프로세서와 통신가능하게 연결되는 메모리를 포함하는 전자장비로서, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되어 있고, 상기 명령이 상기 적 어도 하나의 프로세서에 의해 실행될 경우, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제7항 중 어느 한 항의 방법을 실행하도록 하는 것을 특징으로 하는 전자장비. 청구항 16 컴퓨터 명령이 저장되어 있는 비 일시적 컴퓨터 판독가능 저장매체로서, 상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제7항 중 어느 한 항의 방법을 실행하도록 하는 것을 특 징으로 하는 비 일시적 컴퓨터 판독가능 저장매체. 청구항 17 컴퓨터 프로그램을 포함한 컴퓨터 프로그램 제품으로서, 상기 컴퓨터 프로그램이 프로세서에 의해 실행될 경우, 제1항 내지 제7항 중 어느 한 항의 방법을 구현하는 컴퓨터 프로그램 제품. 발명의 설명 기 술 분 야 본 출원은 2021년 05월 28일자로 중국 특허국에 제출한 출원번호가 202110591890.8이고, 발명의 명칭이 \"인식 모델의 트레이닝 방법, 인식 방법, 장치, 장비 및 저장매체\"인 중국 특허 출원의 우선권을 주장하고, 그 전부 내용을 인용함으로써 본 명세서에 포함한다. 본 개시는 인공지능 기술 분야에 관한 것으로서, 특히, 딥러닝, 컴퓨터 비전 기술 분야에 관한 것으로서, 스마 트 시티, 스마트 교통 장면에 응용될 수 있다."}
{"patent_id": "10-2022-7037066", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "목표 인식은 영상 처리의 중요한 수단 및 목적으로서, 목표 인식을 통해 비디오, 정적 화면속의 물체, 인체, 동물체 등 목표 물체를 인식할 수 있고, 인식 결과에 기초하여 신분 인증, 보안 검사 등 다양한 용도들을 실현 할 수 있다. 컴퓨터 기술의 발전에 따라, 목표 인식 기술의 응용이 필요한 다양한 장면에서는, 응용 목적의 다양화, 인식하 고자 하는 목표 물체의 다양화로 인해, 목표 물체 인식의 목적을 달성하기 위해서는 종종 다중 모델이 필요한다. 처리하고자 하는 영상을 처리하는 모델의 효율을 어떻게 향상시킬지는 개선이 필요한 문제이다."}
{"patent_id": "10-2022-7037066", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 인식 모델의 트레이닝 방법, 인식 방법, 장치, 장비 및 저장매체를 제공한다."}
{"patent_id": "10-2022-7037066", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면에 의하면, 처리하고자 하는 영상을 트레이닝하고자 하는 인식 모델에 입력하는 것, 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 처리하고자 하는 영상의 적어도 한 장의 특징 맵을 출력 하는 것, 트레이닝하고자 하는 인식 모델의 헤드를 통해, 적어도 한 장의 특징 맵에 따라 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터, 및 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터와 연관되는 제2 목표 물 체의 예측 데이터를 취득하는 것, 및 제1 목표 물체의 예측 데이터, 제2 목표 물체의 예측 데이터, 제1 목표 물체의 플래그 데이터 및 제2 목표 물체 의 플래그 데이터에 따라, 트레이닝하고자 하는 인식 모델을 최적화하여, 트레이닝된 인식 모델을 얻는 것을 포 함하는 인식 모델의 트레이닝 방법을 제공한다. 본 개시의 다른 측면에 의하면, 인식하고자 하는 영상을 인식 모델에 입력하여, 인식하고자 하는 영상 중의 제1 목표 물체의 예측 데이터 및 제 1 목표 물체와 연관되는 제2 목표 물체의 예측 데이터를 취득하는 것을 포함하고, 인식 모델은 본 개시의 임의의 일 실시예에 의해 제공되는 트레이닝된 인식 모델인 인식 방법을 제공한다. 본 개시의 다른 측면에 의하면, 처리하고자 하는 영상을 트레이닝하고자 하는 인식 모델에 입력하기 위한 제1 입력 모듈, 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 처리하고자 하는 영상의 적어도 한 장의 특징 맵을 출력 하기 위한 특징 맵 모듈, 트레이닝하고자 하는 인식 모델의 헤드를 통해, 적어도 한 장의 특징 맵에 따라 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터, 및 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터와 연관되는 제2 목표 물 체의 예측 데이터를 취득하기 위한 예측 데이터 모듈, 및 제1 목표 물체의 예측 데이터, 제2 목표 물체의 예측 데이터, 제1 목표 물체의 플래그 데이터 및 제2 목표 물체 의 플래그 데이터에 따라, 트레이닝하고자 하는 인식 모델을 최적화하여, 트레이닝된 인식 모델을 얻기 위한 트 레이닝 모듈을 포함하는 인식 모델의 트레이닝 장치를 제공한다. 본 개시의 다른 측면에 의하면, 인식하고자 하는 영상을 인식 모델에 입력하여, 인식하고자 하는 영상 중의 제1 목표 물체의 예측 데이터 및 제 1 목표 물체와 연관되는 제2 목표 물체의 예측 데이터를 취득하기 위한 제2 입력 모듈을 포함하고, 인식 모델은 본 개시의 임의의 일 실시예에 의해 제공되는 트레이닝된 인식 모델인 인식 장치를 제공한다. 본 개시의 다른 측면에 의하면, 적어도 하나의 프로세서, 및 상기 적어도 하나의 프로세서와 통신가능하게 연결되는 메모리를 포함하는 전자장비로서, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 실행 가능한 명령이 저장되어 있고, 상기 명령이 상기 적 어도 하나의 프로세서에 의해 실행될 경우, 상기 적어도 하나의 프로세서로 하여금 본 개시의 임의의 일 실시예 의 방법을 실행하도록 하는 전자장비를 제공한다. 본 개시의 다른 측면에 의하면, 컴퓨터 명령이 저장되어 있는 비 일시적 컴퓨터 판독가능 저장매체로서, 상기 컴퓨터 명령은 상기 컴퓨터로 하여금 본 개시의 임의의 일 실시예의 방법을 실행하도록 하는 비 일시적 컴 퓨터 판독가능 저장매체를 제공한다. 본 개시의 다른 측면에 의하면, 컴퓨터 프로그램을 포함한 컴퓨터 프로그램 제품으로서, 상기 컴퓨터 프로그램이 프로세서에 의해 실행될 경우, 본 개시의 임의의 일 실시예의 방법을 구현하는 컴퓨터 프로그램 제품을 제공한다. 본 명세서에 기술된 내용은 그 목적이 본 개시의 실시예의 핵심 또는 중요한 특징을 지정하기 위한 것이 아니고, 또한, 본 개시의 범위는 이에 한정되지 아니함을 이해하여야 한다. 본 개시의 다른 특징들은 하기 설명 으로부터 용이하게 이해할 수 있을 것이다."}
{"patent_id": "10-2022-7037066", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 기술에 의하면, 트레이닝하고자 하는 인식 모델을 통해 제1 목표 물체와 제2 목표 물체의 예측 데이 터를 취득하고, 예측 데이터와 플래그 데이터에 따라 트레이닝하고자 하는 인식 모델을 최적화 및 트레이닝할 수 있고, 얻은 인식 모델은 제1 목표 물체 및 제1 목표 물체와 연관되는 제2 목표 물체를 인식함으로써, 적어도 2가지 목표 물체에 대한 연관적인 인식을 실현할 수 있고, 인식하고자 하는 영상에 의해 제공되는 정보를 충분 히 활용함으로써, 적은 수량의 모델로 많은 인식 결과를 출력하여, 모델의 배치 및 인식 효율을 향상시킬 수 있 다."}
{"patent_id": "10-2022-7037066", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 개시의 예시적인 실시예들을 설명한다. 쉽게 이해할 수 있도록, 본 개시의 실시예들 의 세부사항을 포함하게 되는데, 이들은 단지 예시적인 것에 불과하다. 따라서, 당업자라면 본 개시의 범위 및 취지를 벗어나지 않으면서 본 개시의 실시예에 대해 여러가지 변경 및 수정이 이루어질 수 있음을 이해할 것이다. 또한, 명확성과 간결성을 위해 하기의 설명에 있어서, 공지된 기능 및 구성에 대한 설명은 생략한다. 본 개시의 실시예에 의하면, 우선, 도1에 도시된 바와 같이, 단계S11: 처리하고자 하는 영상을 트레이닝하고자 하는 인식 모델에 입력하는 것, 단계S12: 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 처리하고자 하는 영상의 적어도 한 장의 특징 맵을 출력하는 것, 단계S13: 트레이닝하고자 하는 인식 모델의 헤드(Head)를 통해, 적어도 한 장의 특징 맵에 따라 처리하고자 하 는 영상 중의 제1 목표 물체의 예측 데이터, 및 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터와 연 관되는 제2 목표 물체의 예측 데이터를 취득하는 것, 및 단계S14: 제1 목표 물체의 예측 데이터, 제2 목표 물체의 예측 데이터, 제1 목표 물체의 플래그 데이터 및 제2 목표 물체의 플래그 데이터에 따라, 트레이닝하고자 하는 인식 모델을 최적화하여, 트레이닝된 인식 모델을 얻 는 것, 을 포함하는 인식 모델의 트레이닝 방법을 제공한다. 본 실시예에서, 처리하고자 하는 영상은 인식하고자 하는 목표 물체를 포함하는 영상일 수 있다. 인식하고자 하 는 목표 물체는 임의의 물체일 수 있는데, 예를 들어, 인물, 사람의 얼굴, 사람의 눈, 인체, 동물, 정지 물체 등일 수 있다. 트레이닝하고자 하는 인식 모델의 특징 출력층에서 2장 이상의 특징 맵을 출력하는 경우, 2장 이상의 특징 맵의 사이즈는 서로 다르다. 본 실시예에서, 트레이닝하고자 하는 인식 모델의 헤드는 트레이닝하고자 하는 인식 모델 중의 하나의 층 구조 일 수 있다. 트레이닝하고자 하는 인식 모델의 특징 출력층에서 적어도 한 장의 특징 맵을 출력한 후, 적어도 한 장의 특징 맵을 트레이닝하고자 하는 인식 모델의 헤드에 입력하고, 트레이닝하고자 하는 인식 모델의 헤드 에 의해 제1 목표 물체의 예측 데이터 및 제2 목표 물체의 예측 데이터를 출력한다. 본 실시예에서, 제1 목표 물체와 제2 목표 물체는 인식하고자 하는 영상내의 목표 물체일 수 있다. 제1 목표 물 체는 제2 목표 물체와 기하학적 연관성 등 연관적인 관계를 갖는 목표 물체일 수 있다. 예를 들어, 제1 목표 물체가 사람의 얼굴인 경우, 제2 목표 물체는 사람의 얼굴과 연관되는 인체일 수 있다. 즉, 제1 목표 물체가 A인물의 얼굴인 경우, 제2 목표 물체는 A인물의 인체일 수 있다. 또 예를 들어, 제1 목표 물체가 사람의 눈인 경우, 제2 목표 물체는 사람의 눈과 연관되는 사람의 얼굴일 수 있다. 즉, 제1 목표 물체가 A인물의 눈인 경우, 제2 목표 물체는 A인물의 얼굴일 수 있다. 본 실시예에서는, 제1 목표 물체와 제2 목표 물체사이에 서로 포함하거나 포함되는 관계가 존재할 수 있다. 제1 목표 물체의 예측 데이터는, 제1 목표 물체가 인식하고자 하는 영상에 존재하는지, 존재하는 위치 등과 같 은 제1 목표 물체의 인식 데이터를 포함할 수 있다. 제1 목표 물체의 예측 데이터는, 제1 목표 물체의 특징, 속성, 질량 등 데이터도 포함할 수 있다. 예를 들어, 제1 목표 물체의 사이즈 등급, 완전도 등급, 외형 등급 등을 포함할 수 있다. 제1 목표 물체의 예측 데이터는 여러가지 예측 데이터를 포함할 수 있다. 본 실시에서, 제1 목표 물체의 예측 데이터와 제2 목표 물체의 예측 데이터의 종류는 동일하거나 서로 다를 수 있다. 트레이닝하고자 하는 인식 모델의 헤드를 통해, 적어도 한 장의 특징 맵에 따라 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터 및 제2 목표 물체의 예측 데이터를 취득하는 것은, 특징 맵의 각 화소에 대해, 제1 목 표 물체를 예측하기 위한 데이터 및 제2 목표 물체를 예측하기 위한 데이터를 출력하고, 모든 화소의 데이터로 부터, 제1 목표 물체의 예측 데이터 및 제2 목표 물체의 예측 데이터를 취득할 수 있다. 제1 목표 물체의 예측 데이터와 제2 목표 물체의 예측 데이터는 스마트 시티, 스마트 교통 장면에 응용될 수 있 다. 본 실시예에서는, 트레이닝하고자 하는 인식 모델을 통해 제1 목표 물체와 제2 목표 물체의 예측 데이터를 취득 하고, 예측 데이터와 플래그 데이터에 따라 트레이닝하고자 하는 인식 모델을 최적화 및 트레이닝할 수 있고, 얻은 인식 모델은 제1 목표 물체 및 제1 목표 물체와 연관되는 제2 목표 물체를 인식함으로써, 적어도 2가지 목 표 물체에 대한 연관적인 인식을 실현할 수 있고, 인식하고자 하는 영상에 의해 제공되는 정보를 충분히 활용함 으로써, 적은 수량의 모델로 많은 인식 결과를 출력하여, 모델의 배치 및 인식 효율을 향상시킬 수 있다. 일 실시형태에 의하면, 제1 목표 물체의 예측 데이터는 제1 목표 물체의 분류 예측 데이터와 제1 목표 물체의 속성 예측 데이터를 포함하고, 제2 목표 물체의 예측 데이터는 제2 목표 물체의 예측 데이터와 제2 목표 물체의 속성 예측 데이터를 포함한다. 본 실시예에서, 제1 목표 물체의 분류 예측 데이터는 인식하고자 하는 영상의 어느 영역에 대해 제1 목표 물체 인지를 판단하는데 사용될 수 있다. 제1 목표 물체의 속성 예측 데이터는 제1 목표 물체가 인식하고자 하는 영 상에서 표시되는 품질을 판단하기 위한 파라미터일 수 있다. 예를 들어, 분류 예측 데이터는, 영상내에 제1 목 표 물체가 존재하는지 판정하는 데이터, 제1 목표 물체를 둘러싸는 앵커 박스 등과 같은 제1 목표 물체의 판정데이터일 수 있다. 제2 목표 물체의 분류 예측 데이터는 제1 목표 물체의 분류 예측 데이터와 동일할 수도 있고, 제1 목표 물체의 분류 예측 데이터와 서로 다를 수도 있다. 제2 목표 물체의 속성 예측 데이터는 제1 목표 물체의 속성 예측 데 이터와 동일할 수도 있고, 제1 목표 물체의 속성 예측 데이터와 서로 다를 수도 있다. 본 실시예에서는, 제1 목표 물체의 분류 예측 데이터 및 제1 목표 물체의 속성 예측 데이터를 취득할 수 있고, 제2 목표 물체의 분류 예측 데이터 및 속성 예측 데이터도 취득할 수 있으므로, 적어도 2개의 인식이 필요한 연 관 목표 물체에 대해 연합 출력할수 있어, 소량의 모델을 통해 비교적 많은 목표 수량의 물체에 대한 인식 결과 를 취득할 수 있을 뿐만 아니라, 제1 목표 물체 및 제2 목표 물체를 인식하는 과정은 서로 융합될 수 있어, 보 다 좋은 인식 결과를 취득할 수 있다. 일 실시형태에 의하면, 도2에 도시된 바와 같이, 트레이닝하고자 하는 인식 모델의 헤드를 통해, 제1 목표 물체 의 예측 데이터, 및 제1 목표 물체의 예측 데이터와 연관되는 제2 목표 물체의 예측 데이터를 출력하는 것은, 단계S21: 특징 맵의 각 화소에 대해, 제1 목표 물체의 앵커 박스 예측 데이터 및 제2 목표 물체의 앵커 박스 예 측 데이터를 출력하는 것, 및 단계S22: 제1 목표 물체의 앵커 박스 예측 데이터 및 제2 목표 물체의 앵커 박스 예측 데이터에 따라, 제1 목표 물체의 예측 데이터 및 제2 목표 물체의 예측 데이터를 출력하는 것, 을 포함한다. 본 실시예에서, 특징 맵의 각 화소에 대해, 제1 목표 물체의 앵커 박스 예측 데이터는, 화소가 제1 목표 물체에 속하는 확률 등 데이터를 포함할 수 있다. 제2 목표 물체의 앵커 박스 예측 데이터는 제1 목표 물체의 앵커 박 스 예측 데이터와 동일할 수 있다. 제1 목표 물체의 앵커 박스 예측 데이터에 따라, 특징 맵에서 제1 목표 물체의 분계점을 확정함으로써, 제1 목 표 물체를 둘러싸는 앵커 박스를 형성하고, 앵커 박스에 따라 제1 목표 물체의 예측 데이터를 확정할 수 있다. 제2 목표 물체에 대한 예측 데이터는, 제1 목표 물체의 예측 데이터와 동일한 방식으로 생성될 수 있다. 상기 단계S21 및 단계S22는 트레이닝하고자 하는 인식 모델의 헤드를 통해 실행될 수 있다. 본 실시예에서는, 특징 맵의 각 화소에 대해 제1 목표 물체와 제2 목표 물체의 예측 데이터를 예측하기 위한 앵 커 박스 예측 데이터를 생성함으로써, 후속단계에서 목표 물체를 둘러싸는 앵커 박스를 취득할수 있으므로, 앵 커 박스 등 정보에 따라 보다 정확하게 제1 목표 물체와 제2 목표 물체의 예측 데이터를 출력할 수 있다. 일 실시형태에 의하면, 제1 목표 물체는 사람의 얼굴이고, 제2 목표 물체는 인체이다. 보안 빅 데이터 시스템 등 장면에서는, 모니터링 비디오 스트림에 나타난 자연인을 검출 및 추적하고, 그 중의 키 프레임 영상을 저장 및 기록해야 하는 경우가 종종 존재하다. 여기서, 키 프레임 영상은, 사람의 얼굴, 인체 중 적어도 하나를 포함할 수 있고, 후속단계에서 사람의 얼굴 및/또는 인체 검색에 사용될 수 있다. 본 개시의 실시예에 의하면, 제1 목표 물체를 사람의 얼굴로 설정하고, 제2 목표 물체를 인체로 설정하여, 비디오 스트림 의 스냅샷 시스템에서 인체 및 사람의 얼굴 검측에 사용될 수 있고, 동일한 자연인에 속하는 인체 및 사람의 얼 굴을 연관시키고, 연속적인 자연인의 궤적을 포함하는 비디오에서 인식에 가장 적합한 프레임 영상을 선택하여 데이터베이스에 저장함으로써, 후속단계에서의 추적, 검색, 보안 등 동작에 중요하고도 고품질의 정보를 제공할 수 있다. 본 실시예에서는, 인식하고자 하는 영상 중의 사람의 얼굴 및 사람의 얼굴과 연관되는 인체를 인식함으로써, 연 관적인 인식을 실현할 수 있다. 일 실시형태에 의하면, 특징 출력층은 백본 네트워크 및 특징 피라미드 네트워크를 포함하고, 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 처리하고자 하는 영상의 적어도 한 장의 특징 맵을 출력하는 것은, 도3 에 도시된 바와 같이, 단계S31: 백본 네트워크(Backbone)를 통해, 처리하고자 하는 영상의 복수 장의 제1 특징 맵을 출력하는 것, 단계S32: 복수 장의 제1 특징 맵 중의 N장의 제2 특징 맵을 특징 피라미드 네트워크(Feature Pyramid Network, FPN)에 입력하는 것, 여기서, N은 1 이상의 정수이고,단계S33: 특징 피라미드 네트워크를 통해, N장의 제3 특징 맵을 출력하는 것, 및 단계S34: N장의 제3 특징 맵을 특징 맵으로 하는 것, 을 포함한다. 본 실시예에서, N장의 제2 특징 맵은 제1 특징 맵 중 사이즈가 비교적 작은 N장의 제1 특징 맵으로부터 생성된 것일 수 있다. 예를 들어, 백본 네트워크는 F1, F2, F3, F4 및 F5의 5장의 제1 특징 맵을 출력하고, 여기서, F1~F5의 사이즈는 순차적으로 작아지며, 특징 피라미드 네트워크는 F3, F4, F5로부터 F6, F7 및 F8의 3장의 제2 특징 맵을 각각 출력한다. 백본 네트워크는 다층 CNN(Convolutional Neural Networks, 컨볼루션 신경망) 서브 네트워크를 포함할 수 있고, 트레이닝하고자 하는 인식 모델에 입력한 인식하고자 하는 영상에 대해 컨볼루션 동작을 실행하여, 복수 장의 제1 특징 맵을 취득할 수 있다. 특징 피라미드 네트워크는 N장의 제1 특징 맵에 대해 추가로 컨볼루션 동작 등 처리를 실행함으로써, 인식하고 자 하는 영상의 고급 시맨틱 정보를 특징 맵에 융합시켜, N장의 제2 특징 맵을 취득할 수 있다. N장의 제2 특징 맵은 사이즈가 서로 다른 특징 맵일 수 있고, 인식하고자 하는 영상에서 서로 다른 사이즈로 표 시되는 목표 물체를 인식하는데 사용될 수 있으며, 예를 들어, 비교적 작은 사이즈의 특징 맵은 비교적 큰 사이 즈의 목표 물체를 인식하는데 사용될 수 있고, 비교적 큰 사이즈의 특징 맵은 비교적 작은 사이즈의 목표 물체 를 인식하는데 사용될 수 있다. 본 실시예에서는, 트레이닝하고자 하는 인식 모델의 백본 네트워크 및 특징 피라미드 네트워크를 통해 인식하고 자 하는 영상의 특징 맵을 취득함으로써, 후속단계에서 특징 맵에 따라 제1 목표 물체 및 연관되는 제2 목표 물 체에 대한 인식 및 연관되는 데이터 예측을 실행할 수 있다. 본 개시의 실시예에 의하면, 영상 인식 방법으로서, 도4에 도시된 바와 같이, 단계S41: 인식하고자 하는 영상을 인식 모델에 입력하여, 인식하고자 하는 영상 중의 제1 목표 물체의 예측 데 이터 및 제1 목표 물체와 연관되는 제2 목표 물체의 예측 데이터를 취득하는 것, 을 포함하고, 인식 모델은 본 개시의 임의의 일 실시예에 의해 제공되는 트레이닝된 인식 모델인 영상 인식 방법을 더 제공한다. 본 실시예에서는, 트레이닝된 인식 모델을 이용하여 인식하고자 하는 영상을 인식하여, 연관되는 제1 목표 물체 와 제2 목표 물체의 예측 데이터를 취득함으로써, 비교적 적은 수량의 모델로 비교적 많은 예측 결과을 취득할 수 있다. 일 실시형태에 의하면, 인식하고자 하는 영상은 인식하고자 하는 비디오 중의 프레임 영상이고, 인식 방법은, 제1 목표 물체의 예측 데이터 및 제2 목표 물체의 예측 데이터에 따라, 인식하고자 하는 비디오 중의 키 프레임 영상을 취득하는 것을 더 포함한다. 본 실시예에서는, 제1 목표 물체의 예측 데이터 및 제2 목표 물체의 예측 데이터에 따라, 인식하고자 하는 비디 오 중 제1 목표 물체와 제2 목표 물체의 전반적인 품질이 가장 좋은 영상 프레임을 키 프레임 영상으로 확정할 수 있다. 또한, 제1 목표 물체의 예측 데이터 및 제2 목표 물체의 예측 데이터에 따라, 인식하고자 하는 비디오 중의 제1 목표 물체와 제2 목표 물체의 전반적인 품질이 가장 좋은 영상 프레임을 각각 제1 목표 물체의 키 프레임 영상 및 제2 목표 물체의 키 영상 프레임으로 확정할 수도 있다. 본 실시예에서는, 제1 목표 물체와 제2 목표 물체의 예측 데이터를 통해, 인식하고자 하는 비디오 중의 키 영상 프레임을 취득함으로써, 키 영상 프레임에 따라 안면 인식, 생체 인식, 인체 인식, 안면 추적, 인체 추적 등 동 작을 수행할 수 있으므로, 스마트 보안, 스마트 인식 등 다양한 장면과 분야에 응용될 수 있고, 보다 좋은 사용 효과를 얻을 수 있다. 본 개시의 일 실예에 의하면, 인식 모델의 트레이닝 방법은 안면 인식 및 인체 인식에 응용될 수 있으며, 도5에 도시된 단계들을 포함할 수 있다. 단계S51: 인식하고자 하는 영상을 취득한다. 구체적으로는, 모니터링 또는 다른 장면의 카메라의 실시간 비디오 스트림에 대해 영상 프레임을 추출할 수 있 고, 각 프레임을 순차적으로 추출하거나, 간격을 설정하여 추출할 수 있다. 추출된 영상 프레임은 우선 전처리 를 거쳐, 416Х416과 같은 고정된 사이즈로 변환하고, 통일된 RGB평균치(예를 들어, [104,117,123])를 빼서, 각 인식하고자 하는 영상의 사이즈 및 RGB평균치가 트레이닝하고자 하는 인식 모델의 트레이닝 과정에서 통일되도 록 함으로써, 트레이닝된 인식 모델의 완건성(Robustness)을 강화한다. 단계S52: 인식하고자 하는 영상을 인식 모델에 입력한다. 전처리를 거친 인식하고자 하는 영상은 트레이닝하고자 하는 인식 모델로 전송되어 계산을 실행할 수 있다. 단계S53: 인식하고자 하는 영상의 특징 맵을 취득한다. 트레이닝하고자 하는 인식 모델의 입력 데이터는 상기 단계S52를 거쳐 전처리된 영상일 수 있고, 백본 네트워크 의 처리를 거쳐, 심도와 척도가 서로 다른 제1 특징 맵을 얻을 수 있다. 백본 네트워크의 구성은 YOLO 통일 실 시간 목표 검측(You Only Look Once: Unified, Real-Time Object Detection)모델의 백본 네트워크와 동일할 수 있고, 구체적으로는, 컨볼루션 계산 기능을 가진 서브 네트워크를 포함할 수 있고, 서브 네트워크는 예를 들어, DarkNet, ResNet 등 네트워크일 수 있다. 백본 네트워크에 의해 출력된 제1 특징 맵 중 사이즈가 비교적 작은 N장을 특징 피라미드 네트워크에 입력한다. FPN을 통해 백본 네트워크에 의해 출력된 N장의 제1 특징 맵을 대응하는 경로에 따라 서로 융합하여, 최종적으 로 N개의 척도가 서로 다른 특징 맵을 얻는다. 이들 N개의 사이즈가 서로 다른 특징 맵은, 영상 중의 사이즈가 순차적으로 작아지는 서로 다른 척도의 목표를 감지하는데 사용될 수 있다. 단계S54: 제1 목표 물체 예측 데이터와 제2 목표 물체 예측 데이터를 취득한다. 본 실예에 의하면, 특징 피라미드 네트워크는 트레이닝하고자 하는 인식 모델의 헤드에 연결되고, 헤드는 여러 개의 컨볼루션층-활성화층-배치 프로세싱층의 조합을 포함할 수 있다. 본 실예에서는, 각 특징 맵 화소 위치에 적어도 한가지 사이즈 비례가 서로 다른 앵커 박스를 생성하고, 앵커 박스에 기초하여 하나의 결과를 리턴하도록 미리 설정할 수 있다. 각 앵커 박스는 길이가(5+N+M)인 하나의 중간 출력 데이터에 대응한다. 중간 출력 데이터의 채널 수는 (5+N+M)로서, 이는 해당 앵커 박스에 기초하여 목표 검 측박스에 대한 예측(conf,x,y,w,h,class) 및 속성에 대한 예측값을 나타낸다. conf는 해당 앵커 박스에 포함된 목표의 신뢰도를 나타내고, x, y, w, h는 정규화된 검측 박스 좌표 및 척도를 나타내고, class는 차원이 N인 벡 터이고, 목표가 특정 카테고리에 속하는 확률이 해당 카테고리 인덱스의 벡터 내에 대응하는 값을 나타내고, 속 성의 예측값은 길이가 M인 벡터이다. 인체 및 사람의 얼굴의 연관 관계에 대한 정의에 관해서는, 동일 자연인에 속하는 인체 박스와 안면 박스는 연 관 관계를 가진 한 그룹의 인체-안면 박스일 수 있다. 트레이닝 목표를 생성할 때, 각 인체 박스의 중심점에 대 응하는 앵커 포인트(화소)에 인체 박스의 라벨을 생성하고, 해당 인체 박스에 대응하는 자연인의 안면이 영상에 나타났을 경우, 동일한 앵커 포인트에 해당 인체와 연관되는 안면 박스를 생성한다. 예측 시, 전처리를 마친 영상을 네트워크에 입력하여, 영상 중의 모든 인체 박스 및 해당 인체에 대응하는 안면 박스, 그리고 인체 박스에 대응하는 속성 및 안면 박스에 대응하는 속성을 취득한다. 인체 박스에 대응하는 속 성은 절단 여부, 이상 여부, 가려진 정도 및 방향을 포함할 수 있다. 안면 박스에 대응하는 속성은 품질, 피치 각, 편항각 및 롤각을 포함할 수 있다. 인체 박스 및 안면 박스를 이용하여 연합 추적을 실행하여, 궤적으로부터 절단이 없고, 이상이 없으며, 가려진 정도가 낮은 서로 다른 방향의 인체 키 프레임을 선택하여 라이브러리에 저장하고, 품질 점수가 높고, 각도가 비교적 작은 안면 키 프레임 영상을 선택하여 라이브러리에 저장하며, 저장된 키 프레임 영상은 후속단계에서 안면 검색 등 목표 물체와 연관되는 동작을 실행하는데 사용될 수 있다. 본 개시의 실시예는 안면 및/또는 인식에 응용되는 경우, 인식하고자 하는 영상에 대해 단 한번의 딥러닝 모델 을 추출하여, 인식하고자 하는 영상 중의 모든 인체 및 안면의 검측박스, 인체 속성, 안면 속성, 및 인체와 안 면의 대응 관계를 취득할 수 있다. 단일 단계 모델을 이용하여 연관적인 검측 및 속성 결과에 대한 출력을 동시 에 실행하는 것에 비해, 본 개시의 실시예는 계산 자원 지출을 최대한 감소시키고, 또한, 모델로부터 직접 안면과 인체의 연관 관계를 출력함으로써, 안면과 인체의 연관 판단을 별도로 진행할 필요가 없다. 본 개시의 일 실예에에 의하면, 인식 모델의 구성은, 도6에 도시된 바와 같이, 백본 네트워크, 특징 피라미 드 네트워크(FPN), 및 헤드를 포함할 수 있다. 모델 트레이닝 단계에서, 헤드에 의해 출력된 데이 터로부터, 트레이닝하고자 하는 인식 모델을 최적화하기 위한 손실(Loss)을 취득할 수 있다. 백본 네트워크(6 1)을 통해, 인식하고자 하는 영상에 기초하여 복수 장의 제1 특징 맵을 출력할 수 있고, 구체적으로는, C1, C2, C3, C4, C5를 출력할 수 있으며，사이즈 관계는 C1＞C2＞C3＞C4＞C5이다. FPN을 통해, C3, C4, C5를 융합 계산하고, 예를 들어, C3에 대응하는 순서에 따라, 처리된 제1 특징 맵 중의 적어도 하나를 융합시켜, 제2 특징 맵 P3을 출력하고, C4에 대응하는 순서에 따라, 처리된 제1 특징 맵 중의 적어도 하나를 융합시켜, 제2 특징 맵 P4를 출력하고, C5에 대응하는 순서에 따라, 처리된 제1 특징 맵 중의 적어도 하나를 융합시켜, 제2 특징 맵 P5 를 출력할 수 있다. 헤드는 컨볼루션층 conv3Х3을 포함할 수 있고, 헤드의 출력 채널수는 입력 채널수의 2 배이고, 출력 데이터는 각각 안면 예측 데이터 conv3Х3C, 3(K+5+4) 및 인체 예측 데이터 conv3Х3C, 3(K+5+ 4)일 수 있다. 여기서, C는 헤드에 입력된 특징 채널수이고, k는 카테고리수이고，5는 (x,y,w,h,conf)이고，4는 안면의 세개 각도 및 품질이고, 11은 인체의 4개 속성에 대응하는 벡터, 즉, '정상 인체 여부(아니오, 예), 절단 여부(아니 오, 예), 가려진 정도(가려지지 않음, 살작 가려짐, 많이 가려짐), 방향(앞면, 뒷면, 좌측면, 우측면)'이다. 안면 예측 데이터 및 인체 예측 데이터에 따라, 안면의 품질 관련 데이터, 즉, '안면 박스(Face Box), 안면 점 수(Face Score), 안면 각도(Face Angle), 안면 품질(Face Quality)' 및 인체의 품질 관련 데이터, 즉, '인체 박스(Human Box), 인체 점수(Human Score), 인체 품질(Human Quality)'을 각각 얻을 수 있다. 예를 들어, 도7에 도시된 인식 결과에 따라, 안면 박스 및 인체 박스를 취득할 수 있고, 동시에 안면 및 인체의 품질 관련 데이터, 즉, '정상 인체, 가려지지 않음, 절단 없음, 정면'을 취득할 수 있다. 인식하고자 하는 비디오에서 연합 NMS(Non-Maximum Suppression, 비 최대 억제값) 이 가장 큰 프레임 영상을 키 프레임 영 상으로 선택할 수 있다. 본 공개의 실시예에 의하면, 도8에 도시된 바와 같이, 처리하고자 하는 영상을 트레이닝하고자 하는 인식 모델에 입력하기 위한 제1 입력 모듈, 트레이닝하고자 하는 인식 모델의 특징 출력층을 통해, 처리하고자 하는 영상의 적어도 한 장의 특징 맵을 출력 하기 위한 특징 맵 모듈, 트레이닝하고자 하는 인식 모델의 헤드를 통해, 적어도 한 장의 특징 맵에 따라 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터, 및 처리하고자 하는 영상 중의 제1 목표 물체의 예측 데이터와 연관되는 제2 목표 물 체의 예측 데이터를 취득하기 위한 예측 데이터 모듈, 및 제1 목표 물체의 예측 데이터, 제2 목표 물체의 예측 데이터, 제1 목표 물체의 플래그 데이터 및 제2 목표 물체 의 플래그 데이터에 따라, 트레이닝하고자 하는 인식 모델을 최적화하여, 트레이닝된 인식 모델을 얻기 위한 트 레이닝 모듈을 포함하는 인식 모델의 트레이닝 장치를 더 제공한다. 일 실시형태에 의하면, 제1 목표 물체의 예측 데이터는 제1 목표 물체의 분류 예측 데이터와 제1 목표 물체의 속성 예측 데이터를 포함하고, 제2 목표 물체의 예측 데이터는 제2 목표 물체의 예측 데이터와 제2 목표 물체의 속성 예측 데이터를 포함한다. 일 실시형태에 의하면, 도9에 도시된 바와 같이, 예측 데이터 모듈은, 특징 맵의 각 화소에 대해, 제1 목표 물체의 앵커 박스 예측 데이터 및 제2 목표 물체의 앵커 박스 예측 데이터 를 출력하기 위한 제1 예측 수단, 및 제1 목표 물체의 앵커 박스 예측 데이터 및 제2 목표 물체의 앵커 박스 예측 데이터에 따라, 제1 목표 물체의 예측 데이터 및 제2 목표 물체의 예측 데이터를 출력하기 위한 제2 예측 수단을 포함한다. 일 실시형태에 의하면, 제1 목표 물체는 사람의 얼굴이고, 제2 목표 물체는 인체이다. 일 실시형태에 의하면, 도10에 도시된 바와 같이, 특징 출력층은 백본 네트워크 및 특징 피라미드 네트워크를 포함하고,특징 맵 모듈은, 백본 네트워크를 통해, 처리하고자 하는 영상의 복수 장의 제1 특징 맵을 출력하기 위한 제1 특징 맵 수단 , 복수 장의 제1 특징 맵 중의 N장의 제2 특징 맵을 특징 피라미드 네트워크에 입력하기 위한 제1 특징 맵 입력 수단, 여기서, N은 1 이상의 정수이고, 특징 피라미드 네트워크를 통해, N장의 제2 특징 맵을 출력하기 위한 제2 특징 맵 수단, 및 N장의 제2 특징 맵을 특징 맵으로 하기 위한 제2 특징 맵 처리 수단을 포함한다. 본 공개의 실시예에 의하면, 도11에 도시된 바와 같이, 인식하고자 하는 영상을 인식 모델에 입력하여, 인식하고자 하는 영상 중의 제1 목표 물체의 예측 데이터 및 제 1 목표 물체와 연관되는 제2 목표 물체의 예측 데이터를 취득하기 위한 제2 입력 모듈을 포함하고, 인식 모델은 본 개시의 임의의 일 실시예에 의해 제공되는 트레이닝된 인식 모델인 영상 인식 장치를 더 제공한다. 일 실시형태에 의하면, 인식하고자 하는 영상은 인식하고자 하는 비디오 중의 프레임 영상이고, 도12에 도시된 바와 같이, 인식 장치는, 제1 목표 물체의 예측 데이터 및 제2 목표 물체의 예측 데이터에 따라, 인식하고자 하는 비디오 중의 키 프레임 영상을 취득하기 위한 키 프레임 영상 모듈을 더 포함한다. 본 개시의 실시예의 각 장치 중의 각 수단, 모듈 또는 서브 모듈의 기능은 상기의 방법 실시예에서의 대응하는 설명들을 참조할 수 있고, 여기서는 설명을 생략한다. 본 개시의 실시예에 의하면, 본 개시는 전자장비, 판독가능한 저장 매체 및 컴퓨터 프로그램 제품을 더 제공한 다. 도13은 본 개시의 실시예를 실시할 수 있는 예시적인 전자장비의 예시적인 블록도를 나타낸다. 전자장비는 예를 들어, 랩탑 컴퓨터, 데스크 탑 컴퓨터, 워크스테이션, PDA (Personal Digital Assistants), 서버, 블레이 드 서버, 메인프레임 컴퓨터, 및 기타 적절한 컴퓨터와 같은 다양한 형태의 디지털 컴퓨터를 포함할 수 있다. 전자장비는 예를 들어, PDA (Personal Digital Assistants), 셀룰러 전화기, 스마트 폰, 웨어러블 장비, 및 기 타 유사한 계산 장비와 같은 다양한 형태의 모바일 장비를 포함할 수 있다. 본 명세서에 기재된 부품, 이들의 연결 및 관계, 그리고 이들의 기능은 단지 예시적인 것에 불과하며, 본 명세서에서 설명 및/또는 요구하는 본 개시의 범위를 한정하기 위한 것이 아니다. 도13에 도시된 바와 같이, 전자장비는ROM(Read Only Memory)에 저장된 컴퓨터 프로그램 또는 저장수 단으로부터 RAM(Random Access Memory)에 로딩된 컴퓨터 프로그램에 따라 각종 적당한 동작 및 처리 를 실행할 수 있는 계산수단을 포함한다. 또한, RAM에는 전자장비의 동작에 필요한 다양한 프로 그램 및 데이터가 더 저장될 수 있다. 계산수단, ROM 및 RAM은 버스라인을 통해 서로 연결 된다. 입력/출력(I/O) 인터페이스도 버스라인에 연결된다. 전자장비내의 복수의 부품은 I/O 인터페이스에 연결되고, 상기 부품에는, 예를 들어 키보드, 마우스 등과 같은 입력수단, 예를 들어 각종 유형의 디스플레이, 스피커 등과 같은 출력수단, 예를 들어 자 기 디스크, 광 디스크 등과 같은 저장수단, 및 예를 들어 네트워크 카드, 모뎀, 무선 통신 송수신기 등과 같은 통신수단이 포함된다. 통신수단에 의해, 전자장비는 인터넷과 같은 컴퓨터 네트워크 및/또 는 각종 전자통신망을 통해 다른 장비와 정보/데이터를 교환할 수 있다. 계산수단은 처리 기능 및 계산 기능을 가진 각종 범용 및/또는 주문형 처리 어셈블리일 수 있다. 계산수단 의 일부 실예로서는, 중앙 처리 장치(CPU), 그래픽 처리 장치(GPU), 각종 주문형 인공지능(AI) 컴퓨팅 칩, 각종 머신 러닝 모델 알고리즘을 운행하는 계산수단, 디지털 신호 프로세서(DSP), 및 임의의 적합한 프로세서, 컨트롤러, 마이크로 컨트롤러 등이 포함될 수 있는데, 이에 한정되지는 않는다. 계산수단은 앞에서 설명한 각 방법 및 처리를 실행하는데, 예를 들어, 인식 모델의 트레이닝 방법을 실행한다. 예를 들어, 일부 실시예에 있어서, 인식 모델의 트레이닝 방법은 예를 들어 저장수단과 같은 기계 판독가능 매체에 포함되는 컴퓨터 소프트웨어 프로그램의 형태로 실현될 수 있다. 일부 실시예에 있어서, 컴퓨터 프로그램의 일부 또는 전부는ROM 및/또는 통신수단을 거쳐 전자장비에 로딩 및/또는 설치될 수 있다. 컴퓨터 프로그램이 RAM에 로딩되고 계산수단에 의해 실행될 경우, 앞에서 설명한 인식 모델의 트레이닝 방법의 하나 또 는 복수의 단계를 실행할 수 있다. 선택적으로, 다른 실시예에 있어서, 계산수단은 다른 임의의 적합한 방 식(예를 들어, 펌웨어)을 통해 인식 모델의 트레이닝 방법을 실행하도록 구성될 수 있다. 상기에서 설명한 시스템 및 기술의 다양한 실시 형태는 디지털 전자 회로 시스템, 집적 회로 시스템, FPGA(Field Programmable Gate Array), ASIC(Application Specific Integrated circuit), ASSP(Application Specific Standard Product), SOC(System on Chip), CPLD(Complex Programmable Logic Device), 컴퓨터 하드 웨어, 펌웨어, 소프트웨어, 및/또는 이들의 조합으로 구현될 수 있다. 이러한 다양한 실시형태는 하나 또는 복 수의 컴퓨터 프로그램을 통해 구현될 수 있고, 상기 하나 또는 복수의 컴퓨터 프로그램은 적어도 하나의 프로그 램 가능 프로세서를 포함하는 프로그램 가능 시스템에서 실행 및/또는 해석될 수 있으며, 상기 프로그램 가능 프로세서는 주문형 또는 범용 프로그램 가능 프로세서일 수 있고, 저장 시스템, 적어도 하나의 입력장치, 및 적 어도 하나의 출력장치로부터 데이터 및 명령을 수신하고, 데이터 및 명령을 저장 시스템, 적어도 하나의 입력장 치, 및 적어도 하나의 출력장치로 전송할 수 있다. 본 개시의 방법을 실시하기 위한 프로그램 코드는 하나 또는 복수의 프로그래밍 언어의 임의의 조합을 통해 프 로그래밍을 실행할 수 있다. 이러한 프로그램 코드는 범용 컴퓨터, 주문형 컴퓨터 또는 다른 프로그래밍 가능한 데이터 처리 장치의 프로세서 또는 컨트롤러에 제공되어, 프로그램 코드가 프로세서 또는 컨트롤러에 의해 실행 됨으로써, 흐름도 및/또는 블록도에서 규정한 기능/동작을 실시하도록 할 수 있다. 프로그램 코드는 전부 머신 에 의해 실행되거나 또는 부분적으로 머신에 의해 실행될 수 있고, 또는 독립적인 소프트웨어 패키지로서 부분 적으로 머신에 의해 실행됨과 동시에 부분적으로 원격 머신에 의해 실행되거나, 또는 전부 원격 머신 또는 서버 에 의해 실행될 수 있다. 본 명세서에 있어서, 기계 판독가능 매체는 실체적인 매체일 수 있고, 상기 매체에는 명령 실행 시스템, 장치 또는 장비에 의해 사용되거나 또는 명령 실행 시스템, 장치 또는 장비와 결합하여 사용되는 프로그램이 포함되 거나 저장될 수 있다. 기계 판독가능 매체는 기계 판독가능 신호 매체 또는 기계 판독가능 저장매체일 수 있다. 기계 판독가능 신호 매체는, 전자적, 자기적, 광학적, 전자기적, 적외선적 반도체 시스템, 장치 또는 장비, 또 는 이들의 임의의 적합한 조합을 포함할 수 있는데, 이에 한정되지는 않는다. 기계 판독가능 저장매체의 보다 구체적인 실예로는, 하나 또는 복수의 라인에 의해 전기적으로 연결되는 휴대용 컴퓨터 디스크, 하드 디스크, RAM, ROM, EPROM(Erasable Programming ROM), 플래시 메모리, 광 파이버, CD-ROM, 광학적 저장 장비, 자기적 저장 장비, 또는 이들의 임의의 적합한 조합일 수 있다. 사용자와의 인터랙션을 제공하기 위해서는, 컴퓨터를 통해 본 명세서에서 설명한 시스템 및 기술을 구현할 수 있는데, 상기 컴퓨터는, 사용자에게 정보를 표시하기 위한 표시 장치(예를 들어, CRT(음극선관) 또는 LCD(액정 디스플레이) 모니터), 및 사용자가 상기 컴퓨터에 입력을 제공할 수 있는 키보드 및 포인팅 디바이스(예를 들어, 마우스 또는 트랙 볼)를 포함한다. 기타 유형의 디바이스도 사용자와의 인터랙션을 제공하는데 사용될 수 있다. 예를 들어, 사용자에게 제공되는 피드백은 임의의 형태의 센싱 피드백(예를 들어, 시각 피드백, 청각 피 드백, 또는 촉각 피드백)일 수 있고, 임의의 형태(소리 입력, 음성 입력, 또는 촉각 입력을 포함)로 사용자로부 터의 입력을 수신할 수 있다. 본 명세서에서 설명한 시스템 및 기술은, 백 그라운드 부품을 포함하는 컴퓨팅 시스템(예를 들어, 데이터 서 버), 또는 미들웨어 부품을 포함하는 컴퓨팅 시스템(예를 들어, 애플리케이션 서버), 또는 프론트 앤드 부품을 포함하는 컴퓨팅 시스템(예를 들어, GUI 또는 웹 브라우저를 갖는 사용자 컴퓨터로서, 사용자는 상기 GUI 또는 상기 웹 브라우저를 통하여 본 명세서에서 설명한 상기 시스템 및 기술의 실시 형태와 인터랙션을 할 수 있음), 또는 이러한 백 그라운드 부품, 미들웨어 부품, 또는 프론트 앤드 부품의 임의의 조합을 포함하는 컴퓨팅 시스 템에서 구현될 수 있다. 시스템의 부품은 임의의 형태 또는 매체의 디지털 데이터 통신(예를 들어, 통신 네트워 크)을 통해 서로 연결될 수 있다. 통신 네트워크는 예를 들어 근거리 통신망(LAN), 광역 통신망(WAN) 및 인터넷 을 포함할 수 있다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트 및 서버는 일반적으로 서로 멀리 떨어져 있 고, 통상적으로 통신 네트워크를 통해 인터랙션을 진행한다. 클라이언트와 서버의 관계는 대응하는 컴퓨터에서 실행되고 서로 클라이언트-서버의 관계를 갖는 컴퓨터 프로그램에 의해 생성된다. 상기에서 설명한 다양한 프로세스를 사용하여 각 단계의 순서를 조정하거나, 일부 단계를 추가 또는 삭제 할 수 있다는 점을 이해하여야 한다. 예를 들어, 본 개시에 개시된 기술방안이 원하는 결과를 구현할 수 있는 한, 본개시에 기재된 다양한 단계는 병렬적으로 또는 순차적으로, 또는 서로 다른 순서로 실행될 수 있고, 본 개시는 이에 대해 특별히 한정하지 않는다. 본 개시의 보호범위는 상기 다양한 실시 형태에 의해 제한되지 않는다. 당업자라면, 설계 요구 및 기타 요소에 의해, 다양한 수정, 조합, 서브 조합 및 교체가 이루어질 수 있음을 이해할 것이다. 본 개시의 취지 및 원칙내 에서 이루어진 임의의 수정, 등가 교체 및 개선 등은 모두 본 개시의 보호범위에 속한다. 지적해두어야 할 것은, 상기 설명은 단지 본 발명의 바람직한 실시형태에 관한 것일 뿐, 본 발명이 상기 바람직 한 실시형태에 의해 한정되는 것으로 해석하여서는 안되며, 본 발명의 보호범위는 청구범위에 의해 결정된다. 당업자라면, 본 발명의 취지 및 범위를 벗어나지 않으면서 여러가지 개선 및 수정을 실시할 수 있고, 이러한 개 선 및 수정들도 본 개시의 보호범위에 포함된다."}
{"patent_id": "10-2022-7037066", "section": "도면", "subsection": "도면설명", "item": 1, "content": "첨부 도면은 본 기술방안을 보다 쉽게 이해하도록 하기 위한 것이고, 본 개시는 이에 한정되지 않는다. 도1은 본 개시의 일 실시예에 따른 인식 모델의 트레이닝 방법의 개략도이다. 도2는 본 개시의 다른 실시예에 따른 인식 모델의 트레이닝 방법의 개략도이다. 도3은 본 개시의 또 다른 실시예에 따른 인식 모델의 트레이닝 방법의 개략도이다. 도4는 본 개시의 일 실예에 따른 인식 모델의 트레이닝 방법의 개략도이다. 도5는 본 개시의 다른 실예에 따른 인식 모델의 트레이닝 방법의 개략도이다. 도 6 은 본 개시의 일 실예에 따른 데이터 처리의 개략도이다. 도 7 은 본 개시의 일 실예에 따른 인식 개략도이다. 도 8 은 본 개시의 일 실시예에 따른 인식 모델의 트레이닝 장치의 개략도이다. 도 9 는 본 개시의 다른 실시예에 따른 인식 모델의 트레이닝 장치의 개략도이다. 도 10 은 본 개시의 또 다른 실시예에 따른 인식 모델의 트레이닝 장치의 개략도이다. 도 11 은 본 개시의 일 실시예에 따른 인식 모델의 트레이닝 장치의 개략도이다. 도12는 본 개시의 일 실시예에 따른 인식 모델의 트레이닝 장치의 개략도이다. 도 13 은 본 개시의 실시예에 따른 인식 모델의 트레이닝 방법을 구현하기 위한 전자장비의 블록도이다."}
