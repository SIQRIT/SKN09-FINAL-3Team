{"patent_id": "10-2023-0121148", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0176406", "출원번호": "10-2023-0121148", "발명의 명칭": "전자 장치 및 이의 제어 방법", "출원인": "삼성전자주식회사", "발명자": "이충만"}}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,적어도 하나의 인스트럭션을 저장하는 메모리;및적어도 하나의 프로세서를 포함하며,상기 적어도 하나의 프로세서는, 상기 적어도 하나의 인스트럭션을 실행함으로써,신경망 모델에 포함된 복수의 레이어 별 가중치에 기초하여 상기 복수의 레이어 별로 제1 벡터 및 제2 벡터의곱으로 표현된 제1 양자화 행렬을 획득하며,상기 복수의 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 상기 복수의 레이어 별로 제2 양자화 행렬을 획득하며,상기 제2 양자화 행렬을 이용하여 상기 신경망 모델을 양자화하는 전자 장치."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 적어도 하나의 프로세서는,상기 신경망 모델의 가중치를 보정하기 위한 보정 데이터가 존재하면, 상기 신경망 모델에 상기 보정 데이터를입력하여 상기 복수의 레이어 별 중간 결과값을 획득하고,상기 복수의 레이어 별 중간 결과값을 통해 표본공분산행렬(Sample Covariance Matrix)을 계산하여 헤시안 행렬(Hessian matrix)을 추정하고, 상기 추정된 헤시안 행렬을 이용하여 상기 신경망 모델의 가중치를 미세 조정하는 전자 장치."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 적어도 하나의 프로세서는,상기 보정 데이터가 존재하지 않으면, 모델 인버젼(model inversion)을 수행하여 합성 데이터(synthetic data)를 생성하고, 상기 생성된 합성 데이터를 이용하여 상기 신경망 모델의 가중치를 미세 조정하는 전자 장치."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 적어도 하나의 프로세서는, 상기 복수의 레이어에 포함된 가중치와 양자화된 가중치 사이의 차이가 최소가 되도록 하는 상기 제1 벡터와 제2 벡터를 산출하여 상기 제1 양자화 행렬을 획득하는 전자 장치."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 제1 벡터는 인-채널 방향의 벡터이며,상기 제2 벡터는 아웃-채널 방향의 벡터인 전자 장치.공개특허 10-2024-0176406-3-청구항 6 제1항에 있어서,상기 적어고 하나의 프로세서는,상기 이전 레이어가 기설정된 레이어인 경우, 상기 제1 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 상기 이전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 상기 제2 양자화 행렬을 획득하며,상기 기설정된 레이어는,스케일 가변성(scale equivariance)을 만족하는 활성화 함수(activation function)를 포함하는 레이어 또는,아핀 트랜스폼(affine transform) 또는 스케일 트랜스폼(scale transform)이 포함된 레이어인 전자 장치."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 양자화는,학습 후 양자화(post-training quantization, PTQ)인 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "전자 장치의 제어 방법에 있어서,신경망 모델에 포함된 복수의 레이어 별 가중치에 기초하여 상기 복수의 레이어 별로 제1 벡터 및 제2 벡터의곱으로 표현된 제1 양자화 행렬을 획득하는 단계;상기 복수의 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 상기 복수의 레이어 별로 제2 양자화 행렬을 획득하는 단계; 및상기 제2 양자화 행렬을 이용하여 상기 신경망 모델을 양자화하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 제어 방법은,상기 신경망 모델의 가중치를 보정하기 위한 보정 데이터가 존재하면, 상기 신경망 모델에 상기 보정 데이터를입력하여 상기 복수의 레이어 별 중간 결과값을 획득하는 단계;상기 복수의 레이어 별 중간 결과값을 통해 표본공분산행렬(Sample Covariance Matrix)을 계산하여 헤시안 행렬(Hessian matrix)을 추정하는 단계; 및상기 추정된 헤시안 행렬을 이용하여 상기 신경망 모델의 가중치를 미세 조정하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 제어 방법은,상기 보정 데이터가 존재하지 않으면, 모델 인버젼(model inversion)을 수행하여 합성 데이터(synthetic data)를 생성하는 단계; 및상기 생성된 합성 데이터를 이용하여 상기 신경망 모델의 가중치를 미세 조정하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항에 있어서,상기 제1 양자화 행렬을 획득하는 단계는,상기 복수의 레이어에 포함된 가중치와 양자화된 가중치 사이의 차이가 최소가 되도록 하는 상기 제1 벡터와 제공개특허 10-2024-0176406-4-2 벡터를 산출하여 상기 제1 양자화 행렬을 획득하는 제어 방법."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항에 있어서,상기 제1 벡터는 인-채널 방향의 벡터이며,상기 제2 벡터는 아웃-채널 방향의 벡터인 제어 방법."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8항에 있어서,상기 제2 양자화 행렬을 획득하는 단계는,상기 이전 레이어가 기설정된 레이어인 경우, 상기 제1 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 상기 이전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 상기 제2 양자화 행렬을 획득하며,상기 기설정된 레이어는,스케일 가변성(scale equivariance)을 만족하는 활성화 함수(activation function)를 포함하는 레이어 또는,아핀 트랜스폼(affine transform) 또는 스케일 트랜스폼(scale transform)이 포함된 레이어인 제어 방법."}
{"patent_id": "10-2023-0121148", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8항에 있어서,상기 양자화는,학습 후 양자화(post-training quantization, PTQ)인 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치 및 이의 제어 방법이 제공된다. 본 전자 장치는 적어도 하나의 인스트럭션을 저장하는 메모리 및 적어 도 하나의 프로세서를 포함한다. 적어도 하나의 프로세서는, 적어도 하나의 인스트럭션을 실행함으로써, 신경망 모델에 포함된 복수의 레이어 별 가중치에 기초하여 복수의 레이어 별로 제1 벡터 및 제2 벡터의 곱으로 표현된 제1 양자화 행렬을 획득하며, 복수의 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이전 레이어에 대 응되는 제1 양자화 행렬의 제2 벡터와 곱하여 복수의 레이어 별로 제2 양자화 행렬을 획득하며, 제2 양자화 행렬 을 이용하여 신경망 모델을 양자화한다."}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 이의 제어 방법에 관한 것으로, 신경망 모델의 가중치를 양자화하기 위한 전자 장치 및 이의 제어 방법에 관한 것이다."}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "LLM(Large Language Model)과 같은 거대 신경망 모델을 서버 또는 엣지 디바이스(예로, TV, 모바일 단말 등)에 서 효율적으로 구동하기 위해서는 신경망 모델의 압축이 필수적이다. 신경망 모델을 압축하기 위한 방안으로 최근에는 신경망 모델의 가중치를 양자화하는 방법이 개발되고 있다. 양 자화는 높은 정밀도의 부동 소수점 값(e.g. FP32)으로 학습된 신경망 모델을 고정 소수점 방식의 신경망 모델을 변환하여 추론(inference) 시 메모리의 풋프린트(footprint) 및 레이턴시(latency)를 줄이는 모델 압축 기법이 다. 특히, 양자화 기법은 크게 양자화 인식 학습(Quantization Aware Training, QAT) 및 학습 후 양자화(Post Training Quantization, PTQ)로 분류할 수 있다. 거대 신경망 모델의 경우, 많은 양의 데이터와 컴퓨팅 자원이 필요로 하는 QAT을 적용하기 어렵기 때문에 적은 비용으로 양자화할 수 있는 PTQ가 주로 적용되고 있다. 다만, PTQ는 QAT에 비해 압축된 신경망 모델의 정확도 손실이 크다는 단점이 존재한다. 따라서, PTQ를 수행할 때, 정확도 손실이 적은 양자화 방안의 모색이 요청된다."}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따른, 전자 장치는, 적어도 하나의 인스트럭션을 저장하는 메모리;및 적어도 하나의 프 로세서를 포함하며, 상기 적어도 하나의 프로세서는, 상기 적어도 하나의 인스트럭션을 실행함으로써, 신경망 모델에 포함된 복수의 레이어 별 가중치에 기초하여 상기 복수의 레이어 별로 제1 벡터 및 제2 벡터의 곱으로표현된 제1 양자화 행렬을 획득하며, 상기 복수의 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이 전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 상기 복수의 레이어 별로 제2 양자화 행렬을 획득 하며, 상기 제2 양자화 행렬을 이용하여 상기 신경망 모델을 양자화한다. 상기 적어도 하나의 프로세서는, 상기 신경망 모델의 가중치를 보정하기 위한 보정 데이터가 존재하면, 상기 신 경망 모델에 상기 보정 데이터를 입력하여 상기 복수의 레이어 별 중간 결과값을 획득하고, 상기 복수의 레이어 별 중간 결과값을 통해 표본공분산행렬(Sample Covariance Matrix)을 계산하여 헤시안 행렬(Hessian matrix)을 추정하고, 상기 추정된 헤시안 행렬을 이용하여 상기 신경망 모델의 가중치를 미세 조정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 보정 데이터가 존재하지 않으면, 모델 인버젼(model inversion)을 수행하 여 합성 데이터(synthetic data)를 생성하고, 상기 생성된 합성 데이터를 이용하여 상기 신경망 모델의 가중치 를 미세 조정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 복수의 레이어에 포함된 가중치와 양자화된 가중치 사이의 차이가 최소가 되도록 하는 상기 제1 벡터와 제2 벡터를 산출하여 상기 제1 양자화 행렬을 획득할 수 있다. 상기 제1 벡터는 인-채널 방향의 벡터이며, 상기 제2 벡터는 아웃-채널 방향의 벡터일 수 있다. 상기 적어고 하나의 프로세서는, 상기 이전 레이어가 기설정된 레이어인 경우, 상기 제1 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 상기 이전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 상기 제2 양자화 행렬을 획득하며, 상기 기설정된 레이어는, 스케일 가변성(scale equivariance)을 만족하는 활성화 함수 (activation function)를 포함하는 레이어 또는, 아핀 트랜스폼(affine transform) 또는 스케일 트랜스폼 (scale transform)이 포함된 레이어일 수 있다. 상기 양자화는, 학습 후 양자화(post-training quantization, PTQ)일 수 있다. 본 개시의 일 실시예에 따른, 전자 장치의 제어 방법은, 신경망 모델에 포함된 복수의 레이어 별 가중치에 기초 하여 상기 복수의 레이어 별로 제1 벡터 및 제2 벡터의 곱으로 표현된 제1 양자화 행렬을 획득하는 단계; 상기 복수의 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이전 레이어에 대응되는 제1 양자화 행렬의 제 2 벡터와 곱하여 상기 복수의 레이어 별로 제2 양자화 행렬을 획득하는 단계; 및 상기 제2 양자화 행렬을 이용 하여 상기 신경망 모델을 양자화하는 단계;를 포함한다. 상기 제어 방법은, 상기 신경망 모델의 가중치를 보정하기 위한 보정 데이터가 존재하면, 상기 신경망 모델에 상기 보정 데이터를 입력하여 상기 복수의 레이어 별 중간 결과값을 획득하는 단계; 상기 복수의 레이어 별 중 간 결과값을 통해 표본공분산행렬(Sample Covariance Matrix)을 계산하여 헤시안 행렬(Hessian matrix)을 추정 하는 단계; 및 상기 추정된 헤시안 행렬을 이용하여 상기 신경망 모델의 가중치를 미세 조정하는 단계;를 포함 할 수 있다. 상기 제어 방법은, 상기 보정 데이터가 존재하지 않으면, 모델 인버젼(model inversion)을 수행하여 합성 데이 터(synthetic data)를 생성하는 단계; 및 상기 생성된 합성 데이터를 이용하여 상기 신경망 모델의 가중치를 미 세 조정하는 단계;를 포함할 수 있다. 상기 제1 양자화 행렬을 획득하는 단계는, 상기 복수의 레이어에 포함된 가중치와 양자화된 가중치 사이의 차이 가 최소가 되도록 하는 상기 제1 벡터와 제2 벡터를 산출하여 상기 제1 양자화 행렬을 획득할 수 있다. 상기 제1 벡터는 인-채널 방향의 벡터이며, 상기 제2 벡터는 아웃-채널 방향의 벡터일 수 있다. 상기 제2 양자화 행렬을 획득하는 단계는, 상기 이전 레이어가 기설정된 레이어인 경우, 상기 제1 레이어에 대 응되는 제1 양자화 행렬들의 제1 벡터 각각을 상기 이전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하 여 상기 제2 양자화 행렬을 획득하며, 상기 기설정된 레이어는, 스케일 가변성(scale equivariance)을 만족하는 활성화 함수(activation function)를 포함하는 레이어 또는, 아핀 트랜스폼(affine transform) 또는 스케일 트랜스폼(scale transform)이 포함된 레이어일 수 있다. 상기 양자화는, 학습 후 양자화(post-training quantization, PTQ)일 수 있다."}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 실시 예들은 다양한 변환을 가할 수 있고 여러 가지 실시 예를 가질 수 있는바, 특정 실시 예들을 도면에 예 시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나 이는 특정한 실시 형태에 대해 범위를 한정하려는 것 이 아니며, 본 개시의 실시 예의 다양한 변경(modifications), 균등물(equivalents), 및/또는 대체물 (alternatives)을 포함하는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유 사한 참조 부호가 사용될 수 있다. 본 개시를 설명함에 있어서, 관련된 공지 기능 혹은 구성에 대한 구체적인 설명이 본 개시의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그에 대한 상세한 설명은 생략한다. 덧붙여, 하기 실시 예는 여러 가지 다른 형태로 변형될 수 있으며, 본 개시의 기술적 사상의 범위가 하기 실시 예에 한정되는 것은 아니다. 오히려, 이들 실시 예는 본 개시를 더욱 충실하고 완전하게 하고, 당업자에게 본 개시의 기술적 사상을 완전하게 전달하기 위하여 제공되는 것이다. 본 개시에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 권리범위를 한정하려는 의도 가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. 본 개시에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\"등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 개시에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중 요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요 소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"직접 연결되어\" 있다거나 \"직 접 접속되어\" 있다고 언급된 때에는, 상기 어떤 구성요소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 개시에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for),\" \"~하는 능력을 가지는(having the capacity to),\" \"~하도록 설계된(designed to),\" \"~하도 록 변경된(adapted to),\" \"~하도록 만들어진(made to),\" 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\"것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장치가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행하도록 구성된(또는 설정된) 프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메모리 장치에 저장된 하나 이상의 소프트 웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 실시 예에 있어서 '모듈' 혹은 '부'는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 '모듈' 혹은 복수의 '부'는 특정 한 하드웨어로 구현될 필요가 있는 '모듈' 혹은 '부'를 제외하고는 적어도 하나의 모듈로 일체화되어 적어도 하 나의 프로세서로 구현될 수 있다. 한편, 도면에서의 다양한 요소와 영역은 개략적으로 그려진 것이다. 따라서, 본 발명의 기술적 사상은 첨부한 도면에 그려진 상대적인 크기나 간격에 의해 제한되지 않는다. 이하에서는 첨부한 도면을 참고하여 본 개시에 따른 실시 예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 도 1은 본 개시의 일 실시예에 따른, 전자 장치의 구성을 나타내는 도면이다. 도 1에 도시된 바와 같이, 전자 장치는 메모리 및 적어도 하나의 프로세서를 포함할 수 있다. 이때, 전자 장치는 서버일 수 있다. 다만 이는 일 실시 예에 불과하며, 전자 장치는 사용자 단말일 수도 있다. 한편, 전자 장치(10 0)의 구성이 도 1에 도시된 구성으로 한정되는 것은 아니며, 당업자에게 자명한 구성이 추가될 수도 있음은 물 론이다. 메모리는 전자 장치의 구성요소들의 전반적인 동작을 제어하기 위한 운영체제(OS: Operating System) 및 전자 장치의 구성요소와 관련된 적어도 하나의 인스터력션 또는 데이터를 저장할 수 있다. 또한, 메모 리는 전자 장치의 동작을 제어하기 위한 모듈이 각종 동작을 수행하기 위해 필요한 데이터를 저장할 수 있다. 특히, 메모리는 특정 기능을 수행하기 위한 신경망 모델에 대한 정보(예로, 가중치에 대한 정보)를 저장할 수 있다. 본 개시의 일 실시예에 따른 신경망 모델은 LLM일 수 있으나, 이는 일 실시예에 불과할 뿐, 다른 신경 망 모델(예로, 오브젝트 인식을 위한 신경망 모델 등)으로 구현될 수 있다. 전자 장치가 신경망 모델의 가중치를 양자화하는 동작을 제어하기 위한 구성은 도 2에 도시된 바와 같이, 제1 양자화 행렬 획득 모듈, 가중치 미세 조정 모듈, 제2 양자화 행렬 획득 모듈 및 양자화 모 듈를 포함할 수 있다. 한편, 메모리는 전력 공급이 중단되더라도 저장된 정보를 유지할 수 있는 비휘발성 메모리 및 저장된 정보 를 유지하기 위해서는 지속적인 전력 공급이 필요한 휘발성 메모리를 포함할 수 있다. 신경망 모델의 파라미터 를 양자화하는 동작을 제어하기 위한 구성은 비휘발성 메모리에 저장될 수 있다. 또한, 메모리는 신경망 모델을 학습하기 위한 학습 데이터, 신경망 모델의 가중치를 미세 조정하기 위한 보정 데이터(calibration data) 및 양자화 옵션에 대한 정보 등을 포함할 수 있다. 적어도 하나의 프로세서는 메모리에 저장된 적어도 하나의 인스트럭션에 따라 전자 장치를 제어 할 수 있다. 특히, 적어도 하나의 프로세서는 복수의 프로세서를 포함할 수 있다. 구체적으로, 복수의 프로세서는 CPU (Central Processing Unit), GPU (Graphics Processing Unit), APU (Accelerated Processing Unit), MIC (Many Integrated Core), DSP (Digital Signal Processor), NPU (Neural Processing Unit), 하드웨어 가속기 또는 머신 러닝 가속기 중 하나 이상을 포함할 수 있다. 하나 이상의 프로세서는 전자 장치의 다른 구성요소 중 하나 또는 임의의 조합을 제어할 수 있으며, 통신에 관한 동작 또는 데이터 처리를 수행할 수 있다. 하나 이상 의 프로세서는 메모리에 저장된 하나 이상의 프로그램 또는 명령어(instruction)을 실행할 수 있다. 예를 들어, 복수의 프로세서는 메모리에 저장된 하나 이상의 명령어를 실행함으로써, 본 개시의 일 실시 예에 따른 방법을 수행할 수 있다. 본 개시의 일 실시예에 따른 방법이 복수의 동작을 포함하는 경우, 복수의 동작은 하나의 프로세서에 의해 수행 될 수도 있고, 복수의 프로세서에 의해 수행될 수도 있다. 즉, 일 실시예에 따른 방법에 의해 제 1 동작, 제 2 동작, 제 3 동작이 수행될 때, 제 1 동작, 제 2 동작, 및 제 3 동작 모두 제 1 프로세서에 의해 수행될 수도 있 고, 제 1 동작 및 제 2 동작은 제 1 프로세서(예를 들어, 범용 프로세서)에 의해 수행되고 제 3 동작은 제 2 프 로세서(예를 들어, 인공지능 전용 프로세서)에 의해 수행될 수도 있다. 적어도 하나의 프로세서는 복수의 코어(예를 들어, 동종 멀티 코어 또는 이종 멀티 코어)를 포함하는 하나 이상의 멀티 코어 프로세서(multicore processor)로 구현될 수도 있다. 적어도 하나의 프로세서가 멀티 코 어 프로세서로 구현되는 경우, 멀티 코어 프로세서에 포함된 복수의 코어 각각은 캐시 메모리, 온 칩(On-chip) 메모리와 같은 프로세서 내부 메모리를 포함할 수 있으며, 복수의 코어에 의해 공유되는 공통 캐시가 멀티 코어 프로세서에 포함될 수 있다. 또한, 멀티 코어 프로세서에 포함된 복수의 코어 각각(또는 복수의 코어 중 일부) 은 독립적으로 본 개시의 일 실시예에 따른 방법을 구현하기 위한 프로그램 명령을 판독하여 수행할 수도 있고, 복수의 코어 전체(또는 일부)가 연계되어 본 개시의 일 실시예에 따른 방법을 구현하기 위한 프로그램 명령을 판독하여 수행할 수도 있다. 본 개시의 일 실시예에 따른 방법이 복수의 동작을 포함하는 경우, 복수의 동작은 멀티 코어 프로세서에 포함된 복수의 코어 중 하나의 코어에 의해 수행될 수도 있고, 복수의 코어에 의해 수행될 수도 있다. 예를 들어, 일 실시예에 따른 방법에 의해 제 1 동작, 제 2 동작, 및 제 3 동작이 수행될 때, 제 1 동작, 제2 동작, 및 제3 동 작 모두 멀티 코어 프로세서에 포함된 제 1 코어에 의해 수행될 수도 있고, 제 1 동작 및 제 2 동작은 멀티 코 어 프로세서에 포함된 제 1 코어에 의해 수행되고 제 3 동작은 멀티 코어 프로세서에 포함된 제 2 코어에 의해 수행될 수도 있다. 본 개시의 실시예들에서, 프로세서는 하나 이상의 프로세서 및 기타 전자 부품들이 집적된 시스템 온 칩(SoC), 단일 코어 프로세서, 멀티 코어 프로세서, 또는 단일 코어 프로세서 또는 멀티 코어 프로세서에 포함된 코어를 의미할 수 있으며, 여기서 코어는 CPU, GPU, APU, MIC, DSP, NPU, 하드웨어 가속기 또는 기계 학습 가속기 등으 로 구현될 수 있으나, 본 개시의 실시예들이 이에 한정되는 것은 아니다 특히, 적어도 하나의 프로세서는, 메모리에 저장된 적어도 하나의 인스트럭션을 실행함으로써, 신경 망 모델에 포함된 복수의 레이어 별 가중치에 기초하여 복수의 레이어 별로 제1 벡터 및 제2 벡터의 곱으로 표 현된 제1 양자화 행렬을 획득한다. 이때, 신경망 모델은 기학습된 신경망 모델로서, 신경망 모델의 양자화는 학 습 후 양자화(Post Training Quantization, PTQ)일 수 있다. 또한, 가중치는 신경망 모델에 포함된 구성으로서, 추론을 위해 입력 데이터에 적용되는 값일 수 있다. 이때, 가중치는 파라미터 등으로 불릴 수 있다. 특히, 적어도 하나의 프로세서는 복수의 레이어에 포함된 가중치와 양자화된 가중치 사이의 차이가 최소가 되도록 하는 제1 벡터와 제2 벡터를 산출하여 제1 양자화 행렬을 획득할 수 있다. 이때, 제1 벡터는 인-채널 (in-channel) 방향의 벡터이며, 제2 벡터는 아웃-채널(out-channel) 방향의 벡터일 수 있다. 적어도 하나의 프로세서는 신경망 모델의 가중치를 보정하기 위한 보정 데이터가 존재하는지 여부를 판단 할 수 있다. 신경망 모델의 가중치를 보정하기 위한 보정 데이터가 존재하면, 적어도 하나의 프로세서는 신경망 모델에 보정 데이터를 입력하여 복수의 레이어 별 중간 결과값을 획득하고, 복수의 레이어 별 중간 결과 값을 통해 표본공분산행렬(Sample Covariance Matrix)을 계산하여 헤시안 행렬(Hessian matrix)을 추정하고, 추정된 헤시안 행렬을 이용하여 신경망 모델의 가중치를 미세 조정할 수 있다. 보정 데이터가 존재하지 않으면, 적어도 하나의 프로세서는 모델 인버젼(model inversion)을 수행하여 합성 데이터(synthetic data)를 생 성하고, 생성된 합성 데이터를 이용하여 신경망 모델의 가중치를 미세 조정할 수 있다. 적어도 하나의 프로세서는 복수의 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 복수의 레이어 별로 제2 양자화 행렬을 획득한다. 이때, 적어도 하나의 프로세서는 이전 레이어가 기설정된 레이어인 경우, 제1 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 제2 양자화 행렬을 획득할 수 있다. 이때, 기설정된 레이어는 스케일 가변성(scale equivariance)을 만족하는 활성화 함수(activation function)를 포함하는 레이어 또는, 아핀 트랜스폼(affine transform) 또는 스케일 트랜스폼(scale transform)이 포함된 레이어일 수 있다. 그리고, 적어도 하나의 프로세서는 제2 양자화 행렬을 이용하여 신경망 모델을 양자화할 수 있다. 이하에서는 도 2 내지 도 5c를 참조하여 본 개시에 대해 더욱 상세히 설명하기로 한다. 도 2는 본 개시의 일 실시예에 따른, 신경망 모델의 가중치를 양자화하기 위한 구성을 도시한 블록도이다. 전자 장치는 제1 양자화 행렬 획득 모듈, 가중치 미세 조정 모듈, 제2 양자화 행렬 획득 모듈 및 양자화 모듈을 포함할 수 있다. 제1 양자화 행렬 획득 모듈은 복수의 레이어를 포함하는 신경망 모델을 획득할 수 있다. 이때, 신경망 모 델은 기학습된 신경망 모델일 수 있다. 특히, 제1 양자화 행렬 획득 모듈은 신경망 모델에 포함된 복수의 레이어 별 가중치에 기초하여 복수의 레 이어 별로 제1 벡터 및 제2 벡터의 곱으로 표현된 제1 양자화 행렬을 획득한다. 구체적으로, 기존에는 신경망 모델을 양자화하기 위하여, 하나의 벡터(예로, 아웃채널 별 스케일)를 이용하여 양자화를 수행하였다. 그러나, 본 개시의 일 실시예에 따르면, 제1 양자화 행렬은 기존의 스케일링 팩터인 α 뿐만 새로운 스케일링 팩터인 ζ를 포함할 수 있다. 이때, ζ는 인채널 방향의 제1 벡터로 표현될 수 있으며, α는 아웃 채널 방향의 제2 벡터로 표현될 수 있다. 구체적으로, 제1 양자화 행렬 획득 모듈은 신경망 모델에 포함된 가중치(W)를 이용하여 제1 벡터(Z) 및 제 2 벡터(A)의 곱으로 표현된 제1 양자화 행렬(S)을 획득하기 위하여, 아래와 같은 수학식을 이용할 수 있다. 수학식 1"}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 2"}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 3"}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이때, 및 는 각각 Hadamard division 및 Hadamard product 일 수 있다. n, p는 각각 양자화된 가중치 의 상한값과 하한값을 의미할 수 있다. 즉, 상술한 바와 같이, 양자화된 가중치(Wq)를 획득하기 위한 제1 양자화 행렬(S)이 제1 벡터(Z)와 제2 벡터 (A)의 곱으로 표현될 수 있다. 예를 들어, 도 3a에 도시된 바와 같이, 제1 레이어의 제1 가중치 행렬(W')를 양자화하여 제2 가중치 행렬 (Wq)을 획득할 때, 제1 가중치 행렬에 곱해지는 양자화 행렬(S)은 제1 스케일링 팩터(ζ0,ζ1)를 포 함하는 인 채널 방향의 제1 벡터(ζ) 및 제2 스케일링 팩터(α1, α2, α3)를 포함하는 아웃 채널 방향의 제2 벡 터(α)의 곱으로 표현될 수 있다. 기존에는 아웃 채널 방향의 제2 벡터(α)만으로 양자화 행렬을 획득하여 양자화를 수행하였으나, 본 개시에서는 아웃 채널 방향의 제2 벡터(α)뿐만 아니라 인 채널 방향의 제1 벡터(ζ)를 함께 산출함으로써, 양자화된 신경 망 모델의 정확도가 더욱 높아지는 효과가 존재한다. 특히, 제1 양자화 행렬 획득 모듈은 복수의 레이어에 포함된 가중치와 양자화된 가중치 사이의 차이가 최 소가 되도록 하는 제1 벡터와 제2 벡터를 산출하여 제1 양자화 행렬을 획득할 수 있다. 예를 들어, 제1 양자화 행렬 획득 모듈은 도 3b에 도시된 바와 같은 알고리즘을 이용하여 제1 벡터(ζ) 및 제2 벡터(α)를 산출 할 수 있다. 구체적으로, 제1 양자화 행렬 획득 모듈은 도 3b의 line 2에 도시된 바와 같이, 제1 벡터(ζ) 및 제2 벡터 (α)를 초기화한다. 이때, 제1 벡터(ζ)는 all-1 벡터로 초기화하고, 제2 벡터(α)는 아래의 수식과 같은 Min- Max 양자화를 적용하여 초기화할 수 있다. 수학식 4"}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "그리고, 제1 양자화 행렬 획득 모듈은 도 3b의 line 3-8과 같이, 최소자승법의 해(a least square solution)를 이용하여 수렴할 때(즉, 가중치와 양자화된 가중치의 차이가 최소화될 때)까지 반복적으로 제1 벡 터(ζ) 및 제2 벡터(α)를 업데이트할 수 있다. 특히, 도 3b의 line 12-14과 같이, 열별 최소 자승법(column-wise least square)이 루프 수준 병렬성(loop- level parallelism)을 제공하므로, 제1 양자화 행렬 획득 모듈은 배치 행렬-행렬 곱셈(batch matrix- matrix multiplication, 즉, bmm)을 통해 병렬 처리가 가능하다. 또 다른 예로, 제1 양자화 행렬 획득 모듈은 가중치(W)와 양자화된 가중치(Wq)간의 차이(또는 loss)가 최 소화되도록 하는 그래디언트(gradient) 기반의 최적화를 통해 제1 벡터(ζ) 및 제2 벡터(α)를 산출할 수 있다. 가중치 미세 조정 모듈은 보정 데이터를 이용하여 신경망 모델에 포함된 가중치에 대한 미세 조정(fine- tuning)을 수행할 수 있다. 이때, 가중치 미세 조정 모듈은 메모리에 보정 데이터(calibration data)가 저장되었는지 여부를 식 별할 수 있다. 메모리에 보정 데이터가 존재하지 않는 경우, 가중치 미세 조정 모듈은 신경망 모델에 대한 모델 인버젼(model inversion)을 수행하여 합성 데이터(synthetic data)를 생성할 수 있다. 그리고, 메모 리는 합성 데이터를 보정 데이터로 이용할 수 있다. 그리고, 가중치 미세 조정 모듈은 신경망 모델에 보정 데이터를 입력하여 복수의 레이어 별 중간 결과값을 획득할 수 있다. 가중치 미세 조정 모듈은 복수의 레이어 별 중간 결과값을 통해 표본공분산행렬(Sample Covariance Matrix)을 계산하여 헤시안 행렬(Hessian matrix)을 추정할 수 있다. 가중치 미세 조정 모듈 은 추정된 헤시안 행렬을 이용하여 신경망 모델의 가중치를 미세 조정할 수 있다. 예를 들어, 가중치 미세 조정 모듈은 도 4에 도시된 바와 같은 알고리즘을 이용하여 각 레이어의 가중치에 대한 미세 조정을 수행할 수 있다. 구체적으로, 도 4는 보정 데이터가 주어졌을 때, 가중치를 미세 조정하는 방 법을 설명하기 위한 알고리즘이다. 이때, 양자화된 신경망 모델이 초기화될 수 있다. 이때, 도 4의 line 2-3에 도시된 바와 같이, 손실 저하(degradation of the loss)를 최소화하기 위하여, 가중치 미세 조정 모듈은 제2 벡터(α)에 대한 헤시안 행렬(Hessian matrix)를 이용할 수 있다. 그리고, 도 4의 line 4-9에 도시된 바와 같이, 가중치 미세 조정 모듈은 OPTQ(optimized product tree quantization)를 이용하여 손실(loss)을 더 욱 최소화할 수 있다. 특히, 가중치 미세 조정 모듈은 신경망 모델의 가중치 뿐만 아니라, 제1 벡터(ζ) 및 제2 벡터(α)에 포함 된 스케일링 팩터에 대한 미세 조정을 수행할 수 있다. 예를 들어, 도 4의 line 8-9에 도시된 바와 같이, 제1 벡터(ζ)의 스케일링 팩터(ζi)에 대한 미세 조정이 수행될 수 있다. 제2 양자화 행렬 획득 모듈은 복수의 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이전 레이 어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 복수의 레이어 별로 제2 양자화 행렬을 획득할 수 있다. 이때, 복수의 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이전 레이어에 대응되는 제1 양자화 행 렬의 제2 벡터와 곱하는 동작을 폴딩 동작이라고 명명될 수 있다. 구체적으로, 도 5a에 도시된 바와 같이, 제1 벡터(ζ)의 스케일링 팩터는 인-채널의 가중치(510-1)들에 곱해지 고, 제2 벡터(α)의 스케일링 팩터는 아웃-채널의 가중치(520-2)들에 곱해질 수 있다. 이와 같이, 하나의 레이 어에 포함된 가중치에 복수의 벡터를 곱할 경우, 계산량이 증가하여 연산 성능이 떨어지는 문제점이 발생할 수 있다. 본 개시의 일 실시예에 따르면, 상술한 바와 같은 문제점을 해결하기 위하여, 인-채널의 가중치(510-1)들에 곱 해지는 제1 벡터(ζn)를 이전 레이어의 제2 벡터(αn-1)에 곱할 수 있다. 이에 의해, 제1 벡터 및 제2 벡터를 포 함하는 양자화 행렬을 통해 높은 정확도를 유지하면서, 폴딩 동작을 통해 연산량과 파라미터가 기존과 동일하여 빠른 연산 속도를 유지할 수 있게 된다. 본 개시의 일 실시예에 따르면, 제2 양자화 행렬 획득 모듈은 이전 레이어가 기설정된 레이어인 경우, 상 술한 바와 같은 폴딩 동작을 수행할 수 있다. 예를 들어, 기설정된 레이어는 스케일 가변성(scale equivariance)을 만족하는 활성화 함수(activation function)를 포함하는 레이어 또는 아핀 트랜스폼(affine transform) 또는 스케일 트랜스폼(scale transform)이 포함된 레이어일 수 있다. 일 실시예로, 도 5b에 도시된 바와 같이, 이전 레이어가 스케일 가변성(scale equivariance)을 만족하는 활성화 함수(activation function)(즉, 임의의 상수 a에 대해, f(ax)=af(x)를 만족하는 함수, 예로, ReLU (Rectified Linear Unit) 함수)를 포함하는 레이어인 경우, 제2 양자화 행렬 획득 모듈은 아래의 수학식 5와 같이, 폴 딩 동작을 수행할 수 있다. 수학식 5"}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "즉, 제2 양자화 행렬 획득 모듈는 특정 레이어(520-2)의 제1 벡터( )와 이전 레이어(520-1)의 제2 벡터( )를 곱하여 이전 레이어(520-1)의 제2 양자화 행렬( )로 획득할 수 있다. 일 실시예로, 도 5c에 도시된 바와 같이, 이전 레이어가 아핀 트랜스폼(affine transform)이 포함된 레이어 (530-1)인 경우, 제2 양자화 행렬 획득 모듈은 아래의 수학식 6과 같이, 폴딩 동작을 수행할 수 있다. 수학식 6"}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "즉, 제2 양자화 행렬 획득 모듈는 특정 레이어(530-2)의 제1 벡터( )와 이전 레이어(530-1)의 제2 벡터( )를 곱하여 이전 레이어(530-1)의 제2 양자화 행렬( )로 획득할 수 있다. 제2 양자화 행렬 획득 모듈은 상술한 바와 같은 방식을 통해 복수의 레이어에 대한 제2 양자화 행렬을 획 득할 수 있다. 양자화 모듈는 제2 양자화 행렬을 통해 신경망 모델에 대한 양자화 동작을 수행할 수 있다. 이때, 양자화 동작은 정밀도(precision) 높은 단위로 표현된 가중치를 상대적으로 낮은 정밀도의 가중치로 변환하는 과정을 말한다. 즉, 양자화 동작은 제1 비트 범위로 표현되는 가중치를 제1 비트보다 작은 제2 비트 범위로 표현되는 가중치로 변환하는 과정을 말한다. 이때, 양자화 동작은 균등 양자화(uniform quantization) 또는 비균등 양자화(non-uniform quantization)일 수 있다. 한편, 본 개시의 일 실시예에서는 전자 장치가 제2 양자화 행렬을 통해 직접 양자화를 수행하는 것으로 설 명하였으나, 이는 일 실시예에 불과할 뿐, 제2 양자화 행렬을 다른 전자 장치로 전송하고 다른 전자 장치에 의 해 신경망 모델이 양자화될 수 있음은 물론이다. 도 6은 본 개시의 일 실시예에 따른, 신경망 모델의 가중치를 양자화하기 위한 전자 장치의 제어 방법을 설명하 기 위한 흐름도이다. 우선, 전자 장치는 신경망 모델에 포함된 복수의 레이어 별 가중치에 기초하여 복수의 레이어 별로 제1 벡 터 및 제2 벡터의 곱으로 표현된 제1 양자화 행렬을 획득한다(S610). 구체적으로, 전자 장치는 복수의 레 이어에 포함된 가중치와 양자화된 가중치 사이의 차이가 최소가 되도록 하는 제1 벡터와 제2 벡터를 산출하여 제1 양자화 행렬을 획득할 수 있다. 이때, 제1 벡터는 인-채널 방향의 벡터이며, 제2 벡터는 아웃-채널 방향의 벡터일 수 있다. 전자 장치는 보정 데이터가 존재하는지 여부를 식별할 수 있다(S620). 신경망 모델의 가중치를 보정하기 위한 보정 데이터가 존재하면(S620-Y), 전자 장치는 보정 데이터를 이용 하여 신경망 모델의 가중치를 미세 조정할 수 있다(S630). 신경망 모델의 가중치를 보정하기 위한 보정 데이터 가 존재하지 않으면(S620-N), 전자 장치는 모델 인버젼(model inversion)을 수행하여 합성 데이터 (synthetic data)를 생성할 수 있다(S640). 그리고, 전자 장치 생성된 합성 데이터를 보정 데이터로 이용 하여 신경망 모델의 가중치를 미세 조정할 수 있다(S630). 전자 장치는 복수의 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 복수의 레이어 별로 제2 양자화 행렬을 획득한다(S650). 이때, 이전 레이어가 기설정된 레이어인 경우, 전자 장치는 제1 레이어에 대응되는 제1 양자화 행렬들의 제1 벡터 각각을 이전 레이어에 대응되는 제1 양자화 행렬의 제2 벡터와 곱하여 제2 양자화 행렬을 획득할 수 있다. 기설정된 레이어 는 스케일 가변성(scale equivariance)을 만족하는 활성화 함수(activation function)를 포함하는 레이어 또는, 아핀 트랜스폼(affine transform) 또는 스케일 트랜스폼(scale transform)이 포함된 레이어일 수 있다. 전자 장치는 제2 양자화 행렬을 이용하여 신경망 모델을 양자화한다(S660). 이때, 양자화는 기 학습된 신 경망 모델을 양자화하는 학습 후 양자화(post-training quantization, PTQ)일 수 있다. 상술한 바와 같이, 제1 벡터 및 제2 벡터를 산출하여 양자화를 수행함으로써, 제2 벡터만을 이용하는 기존 방식 보다 적은 오차를 가지도록 양자화를 수행할 수 있다. 이에 의해, 전자 장치는 적은 오차를 가지는 양자화 된 신경망 모델을 이용하여 더욱 정확하면서 빠른 추론(inference) 동작을 수행할 수 있게 된다. 또한, 동일한 정확도를 가지면서 더 높은 압축률을 가지는 신경망 모델을 구현할 수 있게 되므로, 거대 신경망 모델이 효율적 으로 구동될 수 있다. 또는 동일한 정확도를 가지면서 더 낮은 비트로 양자화가 가능해지므로, 더 낮은 비트 정 밀도를 사용하는 효율적인 신경망 가속기(예로, NPU)의 설계가 가능해 질 수 있게 된다. 본 개시에 따른 인공지능과 관련된 기능은 전자 장치의 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU(Central Processing Unit), GPU(Graphic Processing Unit), NPU(Neural Processing Unit) 중 적어도 하나를 포함할 수 있으나 전술한 프로세서의 예시에 한정되지 않는다. CPU는 일반 연산뿐만 아니라 인공지능 연산을 수행할 수 있는 범용 프로세서로서, 다계층 캐시(Cache) 구조를 통해 복잡한 프로그램을 효율적으로 실행할 수 있다. CPU는 순차적인 계산을 통해 이전 계산 결과와 다음 계산 결과의 유기적인 연계가 가능하도록 하는 직렬 처리 방식에 유리하다. 범용 프로세서는 전술한 CPU로 명시한 경 우를 제외하고 전술한 예에 한정되지 않는다. GPU는 그래픽 처리에 이용되는 부동 소수점 연산 등과 같은 대량 연산을 위한 프로세서로서, 코어를 대량으로 집적하여 대규모 연산을 병렬로 수행할 수 있다. 특히, GPU는 CPU에 비해 컨볼루션(Convolution) 연산 등과 같 은 병렬 처리 방식에 유리할 수 있다. 또한, GPU는 CPU의 기능을 보완하기 위한 보조 프로세서(co-processor)로 이용될 수 있다. 대량 연산을 위한 프로세서는 전술한 GPU로 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. NPU는 인공 신경망을 이용한 인공지능 연산에 특화된 프로세서로서, 인공 신경망을 구성하는 각 레이어를 하드 웨어(예로, 실리콘)로 구현할 수 있다. 이때, NPU는 업체의 요구 사양에 따라 특화되어 설계되므로, CPU나 GPU 에 비해 자유도가 낮으나, 업체가 요구하기 위한 인공지능 연산을 효율적으로 처리할 수 있다. 한편, 인공지능 연산에 특화된 프로세서로, NPU는 TPU(Tensor Processing Unit), IPU(Intelligence Processing Unit), VPU(Vision processing unit) 등과 같은 다양한 형태로 구현 될 수 있다. 인공 지능 프로세서는 전술한 NPU로 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. 또한, 하나 또는 복수의 프로세서는 SoC(System on Chip)으로 구현될 수 있다. 이때, SoC에는 하나 또는 복수의 프로세서 이외에 메모리, 및 프로세서와 메모리 사이의 데이터 통신을 위한 버스(Bus)등과 같은 네트워크 인터 페이스를 더 포함할 수 있다. 전자 장치에 포함된 SoC(System on Chip)에 복수의 프로세서가 포함된 경우, 전자 장치는 복수의 프로세서 중 일부 프로세서를 이용하여 인공지능과 관련된 연산(예를 들어, 인공지능 모델의 학습(learning)이나 추론 (inference)에 관련된 연산)을 수행할 수 있다. 예를 들어, 전자 장치는 복수의 프로세서 중 컨볼루션 연산, 행 렬 곱 연산 등과 같은 인공지능 연산에 특화된 GPU, NPU, VPU, TPU, 하드웨어 가속기 중 적어도 하나를 이용하 여 인공지능과 관련된 연산을 수행할 수 있다. 다만, 이는 일 실시예에 불과할 뿐, CPU 등과 범용 프로세서를 이용하여 인공지능과 관련된 연산을 처리할 수 있음은 물론이다. 또한, 전자 장치는 하나의 프로세서에 포함된 멀티 코어(예를 들어, 듀얼 코어, 쿼드 코어 등)를 이용하여 인공 지능과 관련된 기능에 대한 연산을 수행할 수 있다. 특히, 전자 장치는 프로세서에 포함된 멀티 코어를 이용하 여 병렬적으로 컨볼루션 연산, 행렬 곱 연산 등과 같은 인공 지능 연산을 수행할 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 기정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만들어진다는 것은, 다수의 학습 데이터들에 학습 알고리즘을 적용함으로써, 원하는 특성 의 기정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버/시스템을 통해 이루어 질 수도 있다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 적어도 하나의 레이어는 적어도 하나의 가중치 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 적어도 하나의 정의된 연산을 통해 레이 어의 연산을 수행한다. 신경망의 예로는, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 및 심층 Q-네트워크 (Deep Q-Networks), Transformer가 있으며, 본 개시에서의 신경망은 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. 학습 알고리즘은, 다수의 학습 데이터들을 이용하여 소정의 대상 기기(예컨대, 로봇)을 훈련시켜 소정의 대상 기기 스스로 결정을 내리거나 예측을 할 수 있도록 하는 방법이다. 학습 알고리즘의 예로는, 지도형 학습 (supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또 는 강화 학습(reinforcement learning)이 있으며, 본 개시에서의 학습 알고리즘은 명시한 경우를 제외하고 전술 한 예에 한정되지 않는다. 한편, 본 개시의 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 또는 두개의 사용자 장치들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또 는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 본 개시의 다양한 실시 예에 따른 방법은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine- readable storage media에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는 저장 매체로부터 저 장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실시 예들에 따른 전자 장치(예: TV)를 포함할 수 있다. 한편, 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다는 것을 의미할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구 분하지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 상기 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접 또는 상기 프로세서의 제어 하에 다른 구성요소들 을 이용하여 상기 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2023-0121148", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시가 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안 될 것이다."}
{"patent_id": "10-2023-0121148", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른, 전자 장치의 구성을 도시한 블록도, 도 2는 본 개시의 일 실시예에 따른, 신경망 모델의 가중치를 양자화하기 위한 구성을 도시한 블록도, 도 3a는 본 개시의 일 실시예에 따른, 제1 양자화 행렬을 설명하기 위한 도면, 도 3b는 본 개시의 일 실시예에 따른, 제1 벡터 및 제2 벡터의 곱으로 표현되는 제1 양자화 행렬을 획득하는 알 고리즘을 설명하기 위한 도면, 도 4는 본 개시의 일 실시예에 따른, 신경망 모델의 가중치를 미세 조정하기 위한 알고리즘을 설명하기 위한 도 면, 도 5a 내지 도 5c는 본 개시의 다양한 실시예에 따른, 제2 양자화 행렬을 획득하는 방법을 설명하기 위한 도면 들, 그리고, 도 6은 본 개시의 일 실시예에 따른, 신경망 모델의 가중치를 양자화하기 위한 전자 장치의 제어 방법을 설명하 기 위한 흐름도이다."}
