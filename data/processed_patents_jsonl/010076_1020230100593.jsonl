{"patent_id": "10-2023-0100593", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0019455", "출원번호": "10-2023-0100593", "발명의 명칭": "레시피 최적화 방법 및 장치", "출원인": "주식회사 링크디앤에스", "발명자": "김하욱"}}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서를 포함하는 컴퓨팅 장치에 의해 수행되는, 레시피 최적화 모델을 학습시키는 방법으로서,레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계를 포함하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 레시피 최적화 모델은,적어도 하나의 레시피 상태의 전이에 관련된 액션 정보, 액션에 따른 레시피 개선 정보 및 액션에 따른 보상에관련된 사용자의 평가 정보에 기초하여 주어진 레시피의 상태에 어떤 액션을 취할 것인지 최대의 보상을 받을수 있도록 결정하는 최적의 액션 정책을 찾도록 강화학습시키는 것을 특징으로 하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 레시피 최적화 모델은,액션 결정 시에는 매핑 정보에 기초하여 액션을 결정하도록 학습되고,상기 매핑 정보는 레시피의 재료 조합과 기능 변화 간의 관계를 포함하는 것을 특징으로 하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 레시피 개선 정보는,각 레시피에 대한 재료 종류, 재료 성분, 재료 기능, 재료 양, 조리 조건, 레시피 리뷰에 관한 정보 중 어느 하나 이상의 변경을 포함하고,상기 사용자 평가 정보는,사용자 행동, 사용자 관능 평가에 관한 정보 중 어느 하나 이상을 포함하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,공개특허 10-2025-0019455-3-상기 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계는,상기 레시피 최적화 모델에 기초하여, 대상 레시피의 개선 레시피를 획득하는 단계를 포함하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계는,대상 레시피에 대한 레시피의 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 상기 레시피 최적화 모델을 업데이트하는 단계를 포함하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계는,제1 대상 레시피에 대한 레시피의 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 상기 레시피최적화 모델을 업데이트하는 단계; 및업데이트시킨 레시피 최적화 모델에 기초하여 제2 대상 레시피에 대한 개선 레시피를 획득하는 단계를포함하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계는,상기 레시피 최적화 모델에 기초하여, 대상 레시피에 대하여 개선 레시피를 획득하는 단계;상기 개선 레시피에 대한 레시피의 개선 정보 및 사용자 평가 정보를 기초로 획득한 학습 세트로 상기 레시피최적화 모델을 업데이트하는 단계; 및업데이트시킨 레시피 최적화 모델에 기초하여 상기 개선 레시피에 대하여 추가 개선 레시피를 획득하는 단계;를포함하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 레시피 최적화 모델은,공개특허 10-2025-0019455-4-매핑 정보를 기초로 초기 지식을 형성하도록 사전 학습되고,상기 매핑 정보는 레시피의 재료 조합과 기능 변화 간의 관계에 관한 정보를 포함하는 것을 특징으로 하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,상기 레시피에 대한 사용자 평가 정보는,레시피의 재료 조합과 기능 변화 간의 관계를 포함하는 매핑 정보와 관련된 것을 특징으로 하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,상기 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계는,기본 레시피에 사용자 요구사항을 기초로 개선한 레시피에 대한 레시피 개선 정보 및 개선 레시피의 사용자 평가 정보를 기초로 획득한 학습 세트로 상기 레시피 최적화 모델을 학습시키는 단계를 포함하는,방법."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램으로서, 상기 컴퓨터 프로그램은 디바이스의 적어도 하나의 프로세서에서 실행되는 경우, 레시피 최적화 모델을 학습시키기 위한 동작들을 수행하며, 상기 동작들은:레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 동작을 포함하는,컴퓨터 판독가능 저장 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "컴퓨팅 장치로서,적어도 하나의 프로그램 명령을 수행하는 적어도 하나의 프로세서;를 포함하고,상기 적어도 하나의 프로세서는,레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키도록 구성되는,컴퓨팅 장치."}
{"patent_id": "10-2023-0100593", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "적어도 하나의 프로세서를 포함하는 컴퓨팅 장치에 의해 수행되는, 컴퓨팅 장치에 의해 수행되는, 레시피를 최공개특허 10-2025-0019455-5-적화하기 위한 방법으로서,레시피 최적화 모델에 기초하여, 수집된 대상 레시피에 대하여 개선 레시피를 획득하는 단계를 포함하고,상기 레시피 최적화 모델은,레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시킨 모델에 대응되는,방법."}
{"patent_id": "10-2023-0100593", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 몇몇 실시예에 따라 적어도 하나의 프로세서를 포함하는 컴퓨팅 장치에 의해 수행되는, 레시피 최적화 방법이 개시된다. 상기 방법은, 적어도 하나의 프로세서를 포함하는 컴퓨팅 장치에 의해 수행되는, 레시피 최적 화 모델을 학습시키는 방법으로서, 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피 에 대한 레시피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키 는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0100593", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 레시피 최적화 모델을 학습시키기 위한 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0100593", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "레시피 연구 개발 과정은 목적의 레시피를 도출하기 위하여 수차례 반복적인 실험을 수행하여야 하며 이러한 실 험과 평가 과정에 소요되는 시간으로 인해 시장의 니즈에 즉시 대응하기 어렵다는 문제가 있다. 오늘날 이러한 문제점을 해결하기 위하여 인공지능, 빅데이터 등을 활용한 레시피 개발 기술이 활발히 연구되고 있다. 하지만 이러한 레시피 개발을 돕는 기술들은 대부분 정적인 데이터 분석에 기반을 두고 있다. 이러한 방 식들은 과거의 데이터를 분석하여 일반적인 패턴을 찾고, 이 패턴에 기반해 사용자에게 레시피를 추천한다. 하 지만 이러한 방법들은 다이나믹한 요소들을 고려하지 못하며, 사용자의 특정 취향이나 특정 상황에 따른 맞춤형 추천을 제공하는 데에 한계가 있다. 한국 특허등록공보 101687515호에 따르면, 평가지수 정보를 바탕으로 식품의 레시피를 수정하여 판매자에게 제 공하도록 하는 평가정보에 따라 레시피를 수정하는 방법이 개시되어 있다. 한편, 본 개시는 이상에서 살핀 기술적 배경에 적어도 기초하여 도출되었으나, 본 개시의 기술적 과제 또는 목 적은, 이상에서 살핀 문제점 또는 단점을 해결하는 것에 한정되지는 않는다. 즉, 본 개시는, 이상에서 살핀 기 술적 문제 이외에도, 이하에서 설명할 내용과 관련된 다양한 기술적 문제들을 해결할 수 있다."}
{"patent_id": "10-2023-0100593", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 사용자 맞춤형 최적화 레시피를 제공하도록 레시피 최적화 모델을 제공하는 것이다. 다만, 본 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2023-0100593", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "전술한 바와 같은 과제를 실현하기 위한 본 개시의 일 실시예에 따른 적어도 하나의 프로세서를 포함하는 컴퓨 팅 장치에 의해 수행되는, 레시피 최적화 모델을 학습시키는 방법으로서, 레시피 개선에 상응하는 사용자 평가 에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학 습 세트로 레시피 최적화 모델을 학습시키는 단계를 포함할 수 있다. 일 실시예에서, 상기 레시피 최적화 모델은, 적어도 하나의 레시피 상태의 전이에 관련된 액션 정보, 액션에 따 른 레시피 개선 정보 및 액션에 따른 보상에 관련된 사용자의 평가 정보에 기초하여 주어진 레시피의 상태에 어 떤 액션을 취할 것인지 최대의 보상을 받을 수 있도록 결정하는 최적의 액션 정책을 찾도록 강화학습시키는 것 일 수 있다. 일 실시예에서, 상기 레시피 최적화 모델은, 액션 결정 시에는 매핑 정보에 기초하여 액션을 결정하도록 학습되 고, 상기 매핑 정보는 레시피의 재료 조합과 기능 변화 간의 관계를 포함할 수 있다. 일 실시예에서, 상기 레시피 개선 정보는, 각 레시피에 대한 재료 종류, 재료 성분, 재료 기능, 재료 양, 조리 조건, 레시피 리뷰에 관한 정보 중 어느 하나 이상의 변경을 포함하고,상기 사용자 평가 정보는, 사용자 행동, 사용자 관능 평가에 관한 정보 중 어느 하나 이상을 포함할 수 있다. 일 실시예에서, 상기 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시 피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계는, 상기 레시피 최적화 모델에 기초하여, 대상 레시피에 대한 개선 레시피를 획득하는 단계를 포함할 수 있다. 일 실시예에서, 상기 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시 피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계는, 대상 레시피에 대한 레시피의 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 상기 레시피 최적 화 모델을 업데이트하는 단계를 포함할 수 있다. 일 실시예에서, 상기 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시 피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계는, 제1 대상 레시피에 대한 레시피의 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 상기 레시피 최적화 모델을 업데이트하는 단계; 및 업데이트시킨 레시피 최적화 모델에 기초하여 제2 대상 레시피에 대한 개 선 레시피를 획득하는 단계를 포함할 수 있다. 일 실시예에서, 상기 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시 피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계는, 상기 레시피 최적화 모델에 기초하여, 대상 레시피에 대하여 개선 레시피를 획득하는 단계; 상기 개선 레시피에 대한 레시피의 개선 정보 및 사용자 평가 정보를 기초로 획득한 학습 세트로 상기 레시피 최적화 모델을 업데이 트하는 단계; 및 업데이트시킨 레시피 최적화 모델에 기초하여 상기 개선 레시피에 대하여 레시피 개선을 탐색 하여 상기 개선 레시피의 추가 개선 레시피를 획득하는 단계;를 포함할 수 있다. 일 실시예에서, 상기 레시피 최적화 모델은, 매핑 정보를 기초로 초기 지식을 형성하도록 사전 학습되고, 상기 매핑 정보는 레시피의 재료 조합과 기능 변화 간의 관계에 관한 정보를 포함할 수 있다. 일 실시예에서, 상기 레시피에 대한 사용자 평가 정보는, 레시피의 재료 조합과 기능 변화 간의 관계를 포함하 는 매핑 정보와 관련될 수 있다. 일 실시예에서, 상기 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시 피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계는, 기본 레시피에 사용자 요구사항을 기초로 개선한 레시피에 대한 레시피 개선 정보 및 개선 레시피의 사용자 평 가 정보를 기초로 획득한 학습 세트로 상기 레시피 최적화 모델을 학습시키는 단계를 포함할 수 있다. 전술한 바와 같은 과제를 실현하기 위한 본 개시의 일 실시예에 따른 컴퓨터 판독가능 저장 매체에 저장된 컴퓨 터 프로그램으로서, 상기 컴퓨터 프로그램은 디바이스의 적어도 하나의 프로세서에서 실행되는 경우, 레시피 최 적화하기 위한 동작들을 수행하며, 상기 동작들은: 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적 화 모델을 학습시키는 동작을 포함할 수 있다. 전술한 바와 같은 과제를 실현하기 위한 본 개시의 일 실시예에 따른 레시피 최적화 장치는, 컴퓨팅 장치로서, 적어도 하나의 프로그램 명령을 수행하는 적어도 하나의 프로세서;를 포함하고, 상기 적어도 하나의 프로세서는, 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키도록 구성될 수 있다. 전술한 바와 같은 과제를 실현하기 위한 본 개시의 일 실시예에 따른 적어도 하나의 프로세서를 포함하는 컴퓨 팅 장치에 의해 수행되는, 컴퓨팅 장치에 의해 수행되는, 레시피를 최적화하기 위한 방법으로서, 레시피 최적화 모델에 기초하여, 수집된 대상 레시피에 대하여 개선 레시피를 획득하는 단계를 포함하고, 상기 레시피 최적화 모델은, 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레시피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시킨 모델에 대응될 수 있다."}
{"patent_id": "10-2023-0100593", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 다음과 같은 효과가 있다. 본 개시의 몇몇 실시예에 따른 레시피 최적화 모델을 학습시키기 위한 방법을 통해 학습된 모델은 주어진 목표 를 달성하기 위해 가장 좋은 행동인 사용자에게 가장 적합한 레시피를 추천하는 것을 찾아 낼 수 있다. 즉, 레 시피 최적화 모델을 학습시키기 위한 방법은 사용자 경험을 향상시도록 사용자 맞춤형 레시피를 제공함으로써 기존 방식에 비해 우월한 성능을 보일 수 있다. 구체적으로 먼저 사용자의 피드백이나 행동을 통해 학습 모델이 실시간으로 학습하고 적응하므로, 사용자의 변 화하는 취향과 요구사항에 대응하는 동적 적응성이 있다. 둘째, 학습 모델은 단기적인 보상뿐만 아니라 장기적인 보상도 고려하므로 사용자가 단순히 한 번에 만족하는 레시피를 추천하는 것을 넘어 장기적인 보상이 고려 가능하며, 사용자의 건강, 다양성, 식품 재고 등 추가 요소 를 고려한 추천으로 확장 가능하게 한다. 셋째, 학습 모델은 환경과의 상호작용을 통해 학습하기 때문에, 다양한 환경 조건(예: 식재료의 가용성, 계절 등)과 그에 따른 변화를 학습하고 반영할 수 있다."}
{"patent_id": "10-2023-0100593", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "다만, 본 발명의 효과가 상술한 효과들로 제한되는 것은 아니며, 언급되지 아니한 효과들도 본 명세서 및 첨부"}
{"patent_id": "10-2023-0100593", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "된 도면으로부터 본 발명이 속하는 기술분야 에서 통상의 지식을 가진 자에게는 명확히 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0100593", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 여러 실시예들과 접근 방식들이 참조 도면을 바탕으로 다양한 구체적인 상세 사항들이 이해를 돕기 위해 제시되나, 이들 사항들이 반드시 필요하지는 않으며, 이 점은 본 기술 분야에서 일반적인 지식을 가진 사람들에 게 인식될 수 있다. 이후의 기술과 첨부된 도면들은 일련의 접근 방식들의 특정한 예제들을 상세히 기술하나, 이들 접근 방식들은 단지 예시이며, 여러 원칙들을 통해 다양한 방법들이 적용될 수 있다는 것을 명심해야 한다. 따라서, 이러한 설명들은 다양한 접근 방식들과 그에 상응하는 구성요소 내지 균등물들을 포괄하고자 하 는 의도이다. 본 명세서에서 사용되는 \"실시예\", \"예제\", \"양상\", \"예시\" 등의 용어는 특정 접근 방식이나 디자 인이 다른 것들보다 뛰어나다는 것을 의미하지 않는다. 또한, 다양한 접근 방식들과 특징들은 하나 이상의 기기, 단말, 서버, 디바이스, 컴포넌트, 모듈 등을 포함하는 시스템을 통해 설명될 것이다. 다양한 시스템들이 추가적인 기기, 단말, 서버, 디바이스, 컴포넌트, 모듈 등을 포함할 수 있으며, 또는 도면에서 논의된 모든 요소들을 포함하지 않을 수 있다는 것을 이해하고 인식해야 한다. 본 명세서에서 사용되는 \"컴퓨터 프로그램\", \"컴포넌트\", \"모듈\", \"시스템\" 등의 용어는 호환성 있게 사용되며, 컴퓨터 관련 엔티티, 하드웨어, 펌웨어, 소프트웨어, 하드웨어와 소프트웨어의 조합, 또는 소프트웨어의 실행을 의미한다. 예를 들어, 컴포넌트는 프로세서에서 실행되는 절차, 프로세서, 객체, 실행 스레드, 프로그램, 컴퓨 터 등이 될 수 있으나 이에 한정되지 않는다. 컴퓨팅 장치에서 실행되는 응용 프로그램과 컴퓨팅 장치 자체도 컴포넌트가 될 수 있다. 하나 이상의 컴포넌트는 프로세서나 실행 스레드 안에 있을 수 있으며, 일부 컴포넌트 는 하나의 컴퓨터에 위치할 수 있고, 일부는 여러 컴퓨터에 분산될 수 있다.또한, 이러한 컴포넌트들은 그 내부에 저장된 다양한 데이터 구조들을 갖는 다양한 컴퓨터 판독가능한 매체로부 터 실행할 수 있다. 컴포넌트들은 예를 들어 하나 이상의 데이터 패킷들을 갖는 신호(예를 들면, 로컬 시스템, 분산 시스템에서 다른 컴포넌트와 상호작용하는 하나의 컴포넌트로부터의 데이터 및/또는 신호를 통해 다른 시 스템과 인터넷과 같은 네트워크를 통해 전송되는 데이터)에 따라 로컬 및/또는 원격 처리들을 통해 통신할 수 있다. 이하, 도면 부호에 관계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략한다. 또한, 본 명세서에 개시된 실시예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명 이 본 명세서에 개시된 실시예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부 된 도면은 본 명세서에 개시된 실시예를 쉽게 이해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명 세서에 개시된 기술적 사상이 제한되지 않는다. 또한, 이런 요소들은 각각의 내부에 다양한 데이터 구조들을 가지고 있으며, 다양한 컴퓨터가 읽을 수 있는 매 체에서 작동할 수 있다. 이들 요소들은 예컨대 하나 또는 그 이상의 데이터 패킷들을 가진 신호를 통해(예를 들 어, 로컬 시스템 또는 분산 시스템의 한 컴포넌트로부터 다른 시스템이나 인터넷과 같은 네트워크로 전송되는 데이터 및/또는 신호) 로컬 처리 또는 원격 처리를 통해 통신할 수 있다. 다음으로, 도면 상의 기호와 관련없이 같은 또는 유사한 요소들은 같은 참조 번호를 부여받으며, 중복된 설명은 생략한다. 또한, 본 명세서에서 공개된 구현 예시를 설명하면서 관련 기술에 대한 자세한 설명이 본 명세서의 요지를 흐릴 수 있다고 판단되면 그 설명은 생략할 수 있다. 또한, 첨부된 도면은 본 명세서에서 공개된 구현 예시를 이해하기 쉽게 하기 위한 것으로, 첨부된 도면에 의해 본 명세서에서 개시된 기술 개념이 제한되지 않는 다. 본 명세서에서 사용된 용어들은 구현 예시들을 설명하는 목적으로만 사용되며, 이 공개를 제한하려는 것은 아니 다. 본 명세서에서, 단수형은 특별히 언급하지 않는 한 복수형도 포함할 수 있다. \"포함한다\" 또는 \"포함하는\" 이라는 용어는 언급된 요소 이외에 하나 이상의 다른 요소의 존재 또는 추가를 배제하지 않는다. 비록 제1, 제2 등의 용어가 다양한 부품이나 요소들을 설명하기 위해 사용되지만, 이들 부품이나 요소들이 이러 한 용어에 의해 제한되지 않는다. 이런 용어들은 단지 한 부품이나 요소를 다른 부품이나 요소와 구별하기 위해 사용된다. 따라서, 이하에서 언급되는 제1 부품이나 요소는 이 공개의 기술 개념 내에서 제2 부품이나 요소일 수도 있다. 별도의 정의가 주어지지 않는 한, 본 명세서에서 사용되는 모든 용어들(기술 및 과학적 용어 포함)은 본 개시가 속한 기술 분야에서 일반적으로 알려진 의미로 사용될 수 있다. 일반적으로 사용되는 사전에 정의된 용어들은 특별히 정의되지 않는 한 과도하게 해석되지 않는다. 또한, \"또는\"이라는 단어는 배타적인 \"또는\"이 아니라 포괄적인 \"또는\"을 의미한다. 다시 말해, 특별히 지정되 지 않거나 문맥에서 명확하지 않은 경우에, \"X는 A 또는 B를 사용한다\"는 문장은 내포적 치환 중 하나를 의미한 다. 즉, X가 A만을 사용하거나, B만을 사용하거나, 또는 A와 B를 모두 사용하는 경우, \"X는 A 또는 B를 사용한 다\"는 표현은 이들 모든 경우에 적용될 수 있다. 본 명세서에서 \"및/또는\"이라는 용어는 열거된 아이템들 중 하 나 이상의 아이템의 가능한 모든 조합을 가리킨다. 또한, 본 명세서에서 \"정보\"와 \"데이터\"라는 용어는 종종 상호 교환적으로 사용될 수 있다. 이하의 설명에서 사용되는 접미사 \"모듈\" 및 \"부\"는 문서 작성의 편의를 위해 사용되며, 이 자체로는 특별한 의 미나 역할을 가지지 않는다. 본 개시의 목적 및 효과, 그리고 그것들을 달성하기 위한 기술적 요소들은 첨부된 도면과 함께 상세하게 후술되 어 있는 구현 예시들을 참조하면 명확해질 것이다. 본 개시를 설명하는 데 있어서, 기능 또는 구성에 대한 구체 적인 설명이 본 개시의 주요 내용을 불필요하게 흐릴 수 있다고 판단되는 경우에는 그 설명을 생략할 것이다. 그리고 후술되는 용어들은 본 개시에서의 기능을 고려하여 정의되었으며, 이는 사용자, 운영자의 의도 또는 관 례 등에 따라 달라질 수 있다. 그러나 본 개시는 이하에서 개시되는 구현 예시들에 한정되는 것이 아니며, 서로 다른 다양한 형태로 구현될 수"}
{"patent_id": "10-2023-0100593", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "있다. 이러한 구현 예시들은 본 개시를 완전하게 설명하고, 본 개시가 속한 기술분야의 통상의 지식을 가진 자 에게 개시의 범위를 온전히 알리고자 제공되는 것으로, 본 개시는 청구범위에 의해 정의되는 것이 자명하다.따라서, 그 정의는 본 명세서 전체의 내용을 기반으로 판단해야 할 것이다. 본 개시의 청구범위에서의 각 단계들에 대한 권리범위는, 각 단계에서 명시된 기능 및 특징에 의해 결정되는 것으로, 각 단계의 순서가 명시되지 않는 한, 청구범위 내의 각 단계들의 기재 순서에 의해 영향을 받지 않는다. 예시로, A단계와 B단계를 포함하는 단계가 기재된 청구범위에서, A단계가 B단계보다 먼저 기재되었다 해도, A단 계가 B단계 이전에 이루어져야 한다는 것으로 권리범위가 제한되는 것은 아니다. 도 1은 본 개시의 다양한 형태가 구현될 수 있는 컴퓨팅 장치의 블록 도이다. 도 1을 참조하면, 컴퓨팅 장치는 프로세서, 통신부, 그리고 메모리를 포함할 수 있다. 그 러나, 이러한 구성 요소들은 컴퓨팅 장치의 구현에 필수적인 것은 아니어서, 컴퓨팅 장치는 위에서 열거된 구성요소들보다 많거나, 또는 적은 구성요소들을 가질 수 있다. 본 개시의 컴퓨팅 장치는 레시피 최적화 시스템을 운영하거나, 관리하기 위한 서버가 될 수 있으나 이는 한정적인 사례일 뿐이다. 컴퓨팅 장치는 예를 들어, 마이크로프로세서, 메인프레임 컴퓨터, 디지털 프로세서, 휴대용 디바이스 및 디바이스 제어기 등과 같은 임의의 유형의 컴퓨터 시스템 또는 컴퓨터 디바이스를 포함할 수 있으나 이는 한정 적인 사례일 뿐이다. 컴퓨팅 장치의 프로세서는 일반적으로 컴퓨팅 장치의 전반적인 작동을 제어한다. 프로세서(11 0)는 컴퓨팅 장치에 포함된 구성요소들을 통해 입력되거나 출력되는 신호, 데이터, 정보 등을 처리하거나 메모리에 저장된 응용 프로그램을 구동함으로써, 사용자에게 적절한 정보 또는 기능을 제공하거나 처리할 수 있다. 또한, 프로세서는 메모리에 저장된 응용 프로그램을 구동하기 위해, 컴퓨팅 장치의 구성요소들 중 적어도 일부를 제어할 수 있다. 더 나아가, 프로세서는 상기 응용 프로그램의 구동을 위해, 컴퓨팅 장 치에 포함된 구성요소들 중 적어도 둘 이상을 서로 조합하여 작동시킬 수 있다. 컴퓨팅 장치의 프로세서는 하나 이상의 코어를 포함하고 있을 수 있으며, 중앙 처리 장치(CPU: central processing unit), 범용 그래픽 처리 장치(GPGPU: general purpose graphics processing unit), 텐서 처리 장치(TPU: tensor processing unit)와 같은 데이터 분석이나 딥러닝을 수행하는 프로세서를 가지고 있을 수 있다. 프로세서는 메모리에 보관된 컴퓨터 프로그램을 읽어들여 본 개시에 따른 몇 가지 실시예를 위한 기계학습에 관한 데이터 처리를 실시할 수 있다. 본 개시의 일부 실시예에 따르면, 프로세서는 신경 망 학습에 필요한 계산을 수행할 수 있다. 프로세서는 딥러닝(DL: deep learning)에서 입력 데이터의 처리, 피처 추출, 오차 계산, 역전파(backpropagation)를 이용한 신경망의 가중치 업데이트 등을 포함하는 신경 망 학습을 위한 계산을 수행할 수 있다. 프로세서의 CPU, GPGPU, 그리고 TPU 중 적어도 하나가 네트워크 함수의 학습 처리를 담당할 수 있다. 예컨대, CPU와 GPGPU는 네트워크 함수의 학습과 데이터 분류를 함께 처리 할 수 있다. 또한, 본 개시의 일부 실시예에서는 여러 컴퓨팅 장치의 프로세서를 동시에 사용하여 네트워크 함 수의 학습과 데이터 분류를 처리할 수 있다. 본 개시에 따른 컴퓨팅 장치에서 실행되는 컴퓨터 프로그램은 CPU, GPGPU 또는 TPU에서 실행 가능한 프로그램일 수 있다. 본 개시의 몇몇 실시예에 따르면, 컴퓨팅 장치의 프로세서는 레시피 최적화 모델을 학습시켜, 최적화 레시피를 획득할 수 있다. 구체적으로, 컴퓨팅 장치의 프로세서는 적어도 하나의 레시피 각각의 개선 정보 및 사용자 평가 정보 를 획득할 수 있다. 또한, 프로세서는 레시피 개선에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 레시피에 대한 레 시피 개선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시킬 수 있다. 이를 통해 프로세서는 학습 과정에서 최적화 레시피에 관한 정보를 획득할 수 있다. 다만, 이에 한정되는 것은 아니다. 예를 들어서, 상기 레시피 개선 정보는, 각 레시피에 대한 재료 종류, 재료 성분, 재료 기능, 재료 양, 조리 조 건, 레시피 리뷰에 관한 정보 중 어느 하나 이상이 변경된 개선이나 개선된 결과를 포함하고, 상기 사용자 평가 정보는, 사용자 행동, 사용자 관능 평가에 관한 정보 중 어느 하나 이상을 포함할 수 있다, 다만, 이에 한정되 는 것은 아니다.한편, 컴퓨팅 장치의 프로세서는 최적화 레시피에 관한 정보를 사용자에게 제공할 수 있다. 예를 들 어, 프로세서는 최적화되어 개선된 레시피에 관한 정보를 사용자 단말 또는 서버로 전송하도록 통신부 를 제어할 수 있다. 여기서, 개선 레시피에 관한 레시피 개선 정보는 이전 사용자 피드백에 부합하도록 변 경된 레시피 상태 자체인 개선 레시피 상태, 변경할 재료 종류, 재료 양 등에 대한 액션 자체에 대한 정보를 포 함할 수 있다. 다만, 이에 한정되는 것은 아니다. 따라서, 본 개시의 레시피 최적화 모델은 사용자 피드백에 부합하도록 레시피를 변경하는 액션을 취하도록 학습 하며, 시간이 지남에 따라 보다 나은 레시피를 제공하게 된다. 이하, 본 개시의 컴퓨팅 장치가 레시피 최적화 모델을 학습시켜, 최적화된 개선 레시피를 획득하는 방법에 대한 설명은 본 개시의 몇몇 실시예에 관한 도 2 내지 도 3을 참조하여 후술한다. 컴퓨팅 장치의 통신부는 컴퓨팅 장치와 사용자 단말, 그리고 컴퓨팅 장치와 서버 간의 통신을 가능케 하는 하나 이상의 모듈을 가질 수 있다. 더 나아가, 이 통신부는 컴퓨팅 장치를 하나 이상의 네트워 크에 연결하는 모듈들을 포함할 수 있다. 본 개시의 특정 실시예에 따르면, 통신부는 최적화된 레시피 정보를 사용자 단말 또는 서버로 전달할 수 있다. 컴퓨팅 장치와 사용자 단말, 그리고 컴퓨팅 장치와 서버들 간의 통신을 가능케 하는 네트워크는 다양한 유 선 통신 시스템, 예를 들어 공중전화 교환망(PSTN), 다양한 DSL 기술(xDSL, RADSL, MDSL, VDSL, UADSL, HDSL), 그리고 LAN 등을 사용할 수 있다. 또한, 이러한 네트워크는 다양한 무선 통신 시스템, 예를 들어 CDMA, TDMA, FDMA, OFDMA, SCFDMA 등을 사용할 수 있다. 본 개시에 따르면, 네트워크는 유선 또는 무선과 같은 다양한 통신 형태를 가질 수 있으며, LAN, WAN 등의 다양 한 통신망으로 구성될 수 있다. 추가로, 이러한 네트워크는 인터넷(WWW)이거나, 또는 적외선(IrDA) 또는 블루투 스와 같은 단거리 무선 전송 기술을 이용할 수 있다. 본 명세서에서 언급된 기술들은 위에서 설명한 네트워크 외에도, 다른 네트워크에서도 적용될 수 있다. 컴퓨팅 장치의 메모리는 프로세서의 작동을 위한 프로그램을 저장하거나, 입/출력되는 데이터들 을 임시 또는 영구 저장할 수 있다. 이 메모리는 다양한 타입의 저장매체, 예를 들어 플래시 메모리, 하드 디스크, 멀티미디어 카드 마이크로, 카드 타입 메모리(SD 또는 XD 메모리 등), 램, SRAM, 롬, EEPROM, PROM, 자 기 메모리, 자기 디스크, 광디스크 등을 포함할 수 있다. 본 개시의 특정 실시예에 따라, 메모리는 본 개시의 학습 모델을 형성하는 신경망 또는 신경망 구조를 저 장할 수 있다. 본 개시에서는, 실시예들이 독립적인 소프트웨어 모듈들로써 구현될 수 있다는 점을 강조하고 있다. 이런 모듈 들은 각각 본 명세서에서 기술한 다양한 기능과 작동들을 실현하는데 사용될 수 있다. 프로그래밍 언어를 이용 해 소프트웨어 어플리케이션으로 작성된 소프트웨어 코드는 컴퓨팅 장치의 메모리에 보관되고, 컴퓨 팅 장치의 프로세서에 의해 실행될 수 있다. 본 명세서에서는 머신러닝 모델, 딥러닝 기반 모델, 연산 모델, 신경망, 네트워크 함수, 심층신경망, 뉴럴 네트 워크 등이 서로 바꿔 쓰일 수 있다는 점을 명시하고 있다. 신경망은 일반적으로 노드라 불리는 계산 단위들의 집합으로 구성되며, 이 노드들은 뉴런으로도 부를 수 있다. 이런 신경망은 적어도 하나 이상의 노드들로 이루어 지며, 노드들은 링크를 통해 상호 연결될 수 있다. 신경망 내에서, 링크로 연결된 노드들은 상대적으로 입력 노드와 출력 노드의 관계를 형성할 수 있다. 이 때 입 력 노드와 출력 노드의 개념은 상대적인 것이며, 같은 노드가 다른 관계에서는 입력 노드가 될 수도 있고, 출력 노드가 될 수도 있다. 링크를 통해 연결된 노드들 간의 관계에서, 출력 노드의 데이터 값은 입력 노드의 데이터 를 바탕으로 결정되며, 연결하는 링크는 가중치를 가질 수 있다. 신경망은 노드들이 링크를 통해 연결되어 입력노드와 출력 노드의 관계를 만들어낸다. 신경망의 특성은 노드들 과 링크들의 수, 그리고 노드와 링크 사이의 관계, 그리고 링크들에 부여된 가중치의 값에 따라 결정된다. 같은 수의 노드와 링크를 가지고, 링크들의 가중치 값만 다른 두 신경망은 서로 다른 신경망으로 간주될 수 있다.신경망은 하나 이상의 노드들의 집합으로 구성되며, 신경망 내의 노드들의 부분 집합은 레이어를 형성한다. 이 때 레이어는 초기 입력 노드로부터의 거리를 기준으로 결정되며, 거리는 초기 입력 노드에서 해당 노드까지 도 달하기 위해 거쳐야 하는 링크의 최소 개수로 정의된다. 그러나, 레이어의 정의는 설명의 편의를 위한 것으로, 신경망 내에서 레이어의 차수는 다른 방법으로 정의될 수 있다. 예를 들어, 노드들의 레이어는 최종 출력 노드 로부터의 거리를 기준으로 정의될 수 있다. 최초 입력 노드는 데이터가 직접 들어오는 노드나 다른 노드들과 링크를 통하지 않는 노드를 가리킨다. 반대로, 최종 출력 노드는 출력 노드를 갖지 않는 노드를 말하며, 히든 노드는 이 두 노드가 아닌 신경망의 나머지 부분 을 구성한다. 본 개시의 실시예 중 하나는, 입력 레이어와 출력 레이어의 노드 수가 같고, 히든 레이어로 넘어 가는 과정에서 노드 수가 줄어들었다가 다시 늘어나는 형태의 신경망을 포함할 수 있다. 다른 실시예에서는, 입 력 노드 수가 출력 노드 수보다 적거나, 반대로 많을 수 있으며, 각각 노드 수가 감소하거나 증가하는 패턴을 보일 수 있다. 이외에도, 여러 실시예를 조합한 형태의 신경망도 가능하다. 딥 뉴럴 네트워크는 히든 레이어를 다수 포함하며, 이를 통해 데이터의 잠재적인 구조를 이해할 수 있다. 다양 한 종류의 딥 뉴럴 네트워크가 있으며, 이들은 제한적인 예시일 뿐이며 본 개시의 범위를 벗어나지 않는다. 본 개시의 한 실시예에서 네트워크 함수는 오토 인코더를 포함할 수 있다. 오토 인코더는 입력 데이터와 비슷한 출 력 데이터를 생성하며, 홀수 개의 히든 레이어를 포함할 수 있다. 이러한 오토 인코더는 차원 축소와 대칭적인 확장을 통해 비선형 차원 감소를 실행한다. 또한, 뉴럴 네트워크는 교사 학습, 비교사 학습, 반교사학습, 또는 강화학습과 같은 방법으로 학습될 수 있다. 이는 뉴럴 네트워크가 필요한 지식을 획득하는 과정을 의미한다. 뉴럴 네트워크는 자체의 오류를 줄이는 방향으로 학습하는 시스템이다. 이 과정에서, 학습 데이터는 반복적으로 뉴럴 네트워크에 입력되고, 이를 바탕으로 네트워크의 출력과 목표치 사이의 에러가 계산된다. 이 에러는, 오류 를 축소하는 방향으로 출력 레이어에서부터 입력 레이어로 역전파되고, 이 과정에서 각 노드의 가중치가 수정된 다. 교사학습에서는 라벨링된 학습 데이터를 사용하며, 반면에 비교사 학습에서는 라벨링되지 않은 데이터를 사용한 다. 교사 학습에서는 학습 데이터에 분류 라벨이 붙어 있고, 이를 뉴럴 네트워크의 출력과 비교하여 에러를 계 산한다. 비교사 학습에서는 뉴럴 네트워크의 출력을 입력 학습 데이터와 비교하여 에러를 계산한다. 계산된 에러는 네트워크의 출력부터 입력 방향으로 역전파되며, 이 과정에서 각 노드의 가중치가 업데이트된다. 가중치의 업데이트는 학습률에 의해 결정되며, 이는 뉴럴 네트워크의 학습 사이클에서 조정된다. 뉴럴 네트워크 의 학습 초기에는 빠른 학습률을 사용해 효율성을 높이고, 학습의 후반부에는 낮은 학습률을 사용해 정확성을 향상시킬 수 있다. 뉴럴 네트워크의 학습에 사용되는 데이터는 실제 데이터의 부분 집합일 수 있으므로, 학습 데이터에 대한 오류 는 감소하지만 실제 데이터에 대한 오류는 증가할 수 있다. 이러한 현상을 과적합이라고 하며, 이는 뉴럴 네트 워크가 학습 데이터에 과도하게 학습하여 실제 데이터에 대한 성능을 저하시키는 현상이다. 과적합을 방지하기 위한 다양한 최적화 기법이 존재한다. 예를 들어, 학습 데이터의 양을 증가시키거나, 레귤라 이제이션을 적용하거나, 학습 과정에서 네트워크의 일부 노드를 비활성화하는 드롭아웃 기법을 사용하거나, 배 치 정규화 레이어를 활용하는 방법이 있다. 이러한 방법들은 모두 학습 데이터에만 과도하게 학습하는 것을 방 지하고, 실제 데이터에 대한 성능을 향상시키기 위해 사용된다. 도 3는 본 개시의 몇몇 실시예에 따른 레시피 최적화 모델을 학습시키는 방법의 일례를 설명하기 위한 흐름도이 다. 본 개시의 몇몇 실시예에서 설명된 컴퓨팅 장치의 프로세서가 수행하는 레시피 최적화 모델을 학습 시키는 방법은 상기 프로세서가 수행하는 S110, S120, S130 단계를 포함할 수 있다. 구체적으로 상기 방법은, 매핑 정보를 기초로 초기 지식을 형성하도록 사전 학습하는 단계(S100), 대상 레시피 에 대한 레시피 개선 정보 및 사용자의 평가 정보를 수집하는 단계(S110), 상기 대상 레시피에 대한 레시피 개 선 정보 및 사용자의 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 학습시키는 단계(S120), 상 기 레시피 최적화 모델에 기초하여 상기 대상 레시피에 대하여 상기 대상 레시피의 개선 레시피를 획득하는 단 계(S130)을 포함할 수 있다. 본 개시의 몇몇 실시예에서, 대상 레시피에 대한 레시피 개선 정보 및 사용자의 평가 정보를 수집하는 단계 (S110)에서 상기 레시피 개선 정보는 대상 레시피인 기본 레시피에 사용자 요구사항을 기초로 개선한 레시피에 대한 레시피 개선 정보 일 수 있다. 본 개시의 몇몇 실시예에서, 상기 대상 레시피에 대한 레시피 개선 정보 및 사용자의 평가 정보를 기초로 획득 한 학습 세트로 레시피 최적화 모델을 학습시키는 단계(S120)는, 기본 레시피에 사용자 요구사항을 기초로 개선 한 레시피에 대한 레시피 개선 정보 및 개선 레시피의 사용자 평가 정보를 기초로 획득한 학습 세트로 상기 레 시피 최적화 모델을 학습시키는 단계를 포함할 수 있다. 예를 들어서, 강화학습 알고리즘으로 학습시키는 경우, 기본 레시피와 그 개선 레시피는 최초 상태와 액션에 따른 결과 상태를 정하는 것으로 최초의 출발 지점을 의미 할 수 있다. 본 개시의 몇몇 실시예에서, 프로세서은, 상기 S130 단계와 관련하여 상기 개선 레시피에 대한 사용자 평 가 정보를 수집하는 단계(S131), 상기 개선 레시피에 대한 레시피 개선 정보 및 사용자 평가 정보를 기초로 획 득한 학습 세트로 상기 레시피 최적화 모델을 업데이트시키는 단계(S132), 업데이트시킨 레시피 최적화 모델에 기초하여 개선 레시피에 대하여 상기 개선 레시피의 추가 개선 레시피를 획득하는 단계(S133)를 수행할 수 있다. 즉, 프로세서는, 적어도 하나의 레시피에 관한 레시피 정보를 기초로 레시피 최적화 모델을 학습시킨 뒤, 개선이 필요한 대상 레시피를 레시피 최적화 모델에 입력하여 출력된 레시피 개선 정보를 기초로 개선된 레시피 의 사용자 평가 정보를 수집하고, 대상 레시피의 레시피 개선 정보와 사용자 평가 정보를 기초로 획득한 학습 세트로 레시피 최적화 모델을 업데이트하고, 다시 업데이트된 레시피 최적화 모델로 추가 개선이 필요한 대상인 현재 개선 레시피에 대하여 추가 개선 레시피를 획득하도록 할 수 있다. 여기서, 현재 레시피 최적화 모델이 찾은 개선 레시피는 아직 사용자 평가를 받지 않은 레시피로 상기 레시피 최적화 모델이 학습하여야 할 시나리오를 의미할 수 있다. 필요에 따라 현재 개선 레시피에 대한 사용자 평가 정보를 받아 이를 레시피 최적화 모델에 학습 세트로 제공하여, 현재 레시피 최적화 모델을 업데이트할 수 있고, 업데이트된 레시피 최적화 모델은 해당 사용자 피드백인 사용자 평가 정보를 반영하게 된다. 업데이트된 레시피 최적화 모델은 이전 모델에 비하여 사용자의 선호도가 반영되므로 더 최적화된 추가 개선 레시피를 제공 할 수 있다. 이러한 업데이트를 통해 현재 레시피 최적화 모델은 기존 사용자 환경과 다른 새로운 사용자 환경 에 적응할 수 있게 된다. 따라서, 업데이트된 레시피 최적화 모델을 이용하면 특정 사용자의 맛이나 향과 같은 매우 경험적이고 주관적인 사용자 관능 평가 요소가 직접 반영된 맞춤형 개선 레시피를 획득할 수 있게 된다. 본 개시의 몇몇 실시예에서, 프로세서는 상기 S130과 관련하여, S132를 단계를 수행하여 레시피 최적화 모 델을 업데이트시키는 과정은 반복되어 사용자 맞춤형 최적화를 수행할 수 있다. 이러한 업데이트 주기는, 현재 개선 레시피에 대한 사용자 평가 정보를 수집할 때마다, 현재 레시피 개선 정보 및 상기 현재 개선 레시피에 대 한 사용자 평가 정보를 기초로 레시피 최적화 모델을 업데이트시키는 단계를 수행할 수도 있고, 미리 정하여진 기준에 따라 사용자 평가 정보를 취합한 뒤 레시피 최적화 모델을 업데이트하거나, 레시피의 사용자 평가를 분 석하여 미리 정하여진 기준에 부합하면 업데이트 반복을 중단하도록 할 수 있다. 도 6은 본 개시의 일 실시예에 따른 레시피 개선 정보 및 사용자 평가 정보를 설명하기 위한 도면이다. 일 실시예에서, 상기 레시피는 특정 식품, 화장품, 화합물, 혼합물 등을 결과물로 하며, 특정 재료 종류를 일정 양으로 사용하는 재료 조합을 이용하여 특정 조건에 따라 요리, 조리 또는 제조하는 방법에 대하여 작성되는 것으로, 그 방법에 관한 사용자의 피드백을 포함하는 것을 의미할 수 있고, 텍스트, 이미지, 동영상, 특정 값 등 형식에 제한되지 아니한다. 일 실시예에서, 상기 레시피는 특정 사용자의 요구사항에 따라 제한될 수 있다. 예를 들어서, 특정 사용자가 원 하는 결과물에 대한 레시피로, 특정 재료 종류 내지 재료 성분 등을 사용자 등이 지정하여 제한된 것일 수 있다. 여기서 사용자는 인간이나 동물 등 한정하지 아니한다. 사용자가 인간인 경우, 레시피를 평가할 수 있는 자로 연구자, 고객, 기업 담당자 등 개별 인간이거나 이들로 이루어진 복수의 그룹일 수 있으나, 이에 한정하지 아니 한다. 일 실시예에서, 상기 레시피의 레시피 개선 정보에 관하여 사용자 평가 정보는 개별 레시피 별로 수집되어 있을 수 있으나, 모든 레시피 상태의 가능한 조합에 대한 사용자 평가 정보가 축적되어 있지 않을 수 있다. 일 실시예에서, 상기 레시피 개선 정보는, 각 레시피에 대한 재료 종류, 재료 성분, 재료 기능, 재료 양, 재료 가격, 조리 조건, 레시피 리뷰 등 레시피의 상태를 의미하는 속성 정보 중 어느 하나 이상을 변경하는 것 내지 변경된 레시피에 관한 정보를 의미할 수 있다. 상기 사용자 평가 정보는, 사용자 피드백에 의한 정보로 사용자 행동, 사용자 관능 평가에 관한 정보 중 어느 하나 이상을 포함할 수 있다, 다만, 이에 한정되는 것은 아니다. 예를 들어서, 재료 종류는 레시피를 구성하는 각 재료의 종류로 텍스트, 기호, 이미지, 동영상 등 등으로 표현 될 수 있으나, 이에 한정되는 것은 아니다. 재료 양은 레시피를 구성하는 각 재료의 무게, 부피, 함량, 배합비 등 그 조합을 표현하는 값일 수 있으나, 이 에 한정되는 것은 아니다. 재료 기능은 레시피의 개별 재료 종류에 매핑되어 있을 수 있다. 재료 기능은 개별 재료나 그 재료가 포함하는 성분에 의하여 재료 양에 비례하거나 비례하지 않는 기능에 관한 상관관계에 대한 정보를 의미할 수 있다. 예를 들어 재료 기능 중 관능에 미치는 기능이라면, 딸기는 색상에 붉은 영향을 미치고, 단맛, 신맛 등 에 영향을 미 치며, 딸기향이라는 특유한 향을 제공할 수 있다. 재료 가격은 개별 재료에 관한 정량, 정성적인 정보로 각 재료의 양과 종류에 따라 레시피를 구성하는 전체 재 료의 조합에 대한 가격을 산출하는데 이용될 수 있다. 재료 성분은 특정 재료 종류가 갖는 개별 성분에 관한 정보로 예를 들어 수분, 지방, 탄수화물, 단백질, 아미노 산, 비타민, 맛이나 향 관련 특정 성분에 관한 수치적인 정보일 수 있다. 조리 조건은 레시피의 조리 과정에서의 정보로 조리 시간, 조리 온도, 조리 방법, 조리 순서, 조리 도구 등일 수 있다. 조리 조건은 상술한 레시피의 다른 속성과 상관관계를 갖고, 이에 대하여 매핑될 수 있다. 예를 들어, 양파는 가열 전 매운 맛이 강하나, 가열 후에는 매운 맛은 약해지고, 단 맛이 가미될 수 있어 가열을 조리 조건 으로 하거나 가열 온도 및 시간을 조리 조건으로 하여 양파라는 특정 재료 종류 및 재료 기능과 매핑될 수 있다. 다만, 이에 한정되는 것은 아니다. 사용자 관능 평가는 사용자에게 실제 레시피가 제공된 결과물의 색, 맛, 향, 식감 등 관능으로 수행한 평가일 수 있다, 사용자 행동 평가와 달리 특정한 주관 기준이 직접 표현될 수 있고, 예를 들어서 맛인 경우, 단맛, 짠 맛, 매운맛, 쓴맛, 감미맛, 떫은 맛, 신맛 등 맛 종류와 그 강도에 대한 평가를 포함할 수 있다. 다만, 이에 한 정되는 것은 아니다. 레시피의 리뷰는 다양한 출처에서 수집되는 것으로 기존 사용자들이 레시피를 직접 사용해본 후에 그 경험에 기 반하여 작성하는 리뷰일 수 있고, 이는 사용자의 주관적인 반응을 포함하며, 다양한 요소(조리 난이도, 맛, 재 료 비용 등)에 대한 평가를 포함할 수 있다. 레시피의 리뷰는 해당 레시피에 대한 문장, 단어 등의 평가 의견이 나 객관식의 설문 답변으로 제공되는 것이 일 수 있고 텍스트 마이닝 등을 통해 다른 정보와 연계되도록 적절한 형태로 가공되어질 수 있다. 예를 들어서, 레시피의 리뷰 중 \"너무 싱거워요\"라는 텍스트 의견은 \"짠맛이 약하 다\"라는 형식으로 변환되어질 수 있다. 다만, 이에 한정되는 것은 아니다. 사용자 행동은 특정 사용자가 특정 레시피를 선호하는 정도를 나타내고 사용자의 선택, 클릭, 구매 등의 행동 데이터를 기반으로 측정될 수 있다. 다만, 이에 한정되는 것은 아니다. 예를 들어서, 구체적인 레시피 및 사용자의 관계는 다음과 같을 수 있다. 특정 사용자에게 제공된 레시피가 \"말 차 맛 우유\"를 제조하는 레시피에 관한 것인 경우를 설명한다. 해당 레시피에는 레시피 정보로 \"우유, 말차 가 루, 설탕\"이라는 재료 종류, \"우유 89g(89%), 말차 가루 1g(1%), 설탕 10g(10%)\"이라는 재료 양, \"탄수화물 10g, 단백질 10g, 수분 70g, 고형분 5g, 지방 5g\"이라는 재료 성분, \"우유 890원, 말차 가루 10원, 설탕 10 원\"이라는 재료 가격, \"우유 : 맛 없음, 흰 색 5, 말차 가루 : 떫은맛 5, 녹 색 7, 설탕: 단맛 9, 색 없음\"이 라는 재료 기능, \"우유에 말차 가루 및 설탕을 넣고 섞는다\"는 조리 조건을 가질 수 있다. 이러한 레시피에 대 하여 사용자가 \"떫은 맛이 너무 강하다\", \"설탕 1% 높여주세요\"라는 피드백을 제공할 수 있고, 또는 사용자가 해당 레시피를 선택하는 행동을 수행할 수 있다. 또는, 해당 사용자는 유사한 취향에 따라 그룹화될 수 있고 이 들에게 공통으로 제공된 레시피에 대해 사용자 피드백을 종합하여 해당 레시피에 대한 사용자 평가 정보로 삼을 수 있다. 다만, 이에 한정하지 아니한다. 이러한 레시피에 관한 정보는 실험을 통해 분석하여 수집하거나, 측정 장비를 통해 측정하거나, 제공업체나 전 문 자료를 통해 얻을 수 있거나, 사용자 인터페이스를 통해 획득될 수 있고, 적절하게 전처리 되어 정규화되거 나 이상치를 제거하는 과정을 수반할 수 있으나, 이에 한정되는 것은 아니다. 본 개시의 몇몇 실시예에 따르면, 상기 레시피 최적화 모델은, 적어도 하나의 레시피에 대한 레시피 상태에 관 련된 정보, 레시피 상태의 전이에 관련된 액션 정보, 액션에 따른 레시피 개선 정보 및 액션에 따른 보상에 관 련된 사용자의 평가 정보에 기초하여 주어진 레시피의 상태에 어떤 액션을 취할 것인지 최대의 보상을 받을 수 있도록 결정하는 최적의 액션 정책을 찾도록 강화학습시킨 학습 모델일 수 있다. 이하, 본 개시의 몇몇 실시예에 따라 레시피 최적화 모델이 강화학습을 통해 학습시킨 학습 모델인 경우, 에이 전트와 환경 간의 상호작용은 도 2을 참고하여 설명된다. 환경은 레시피 최적화의 전체적인 문맥을 의미하며, 사용자의 선호도, 특정 재료의 가용성, 계절적인 요소, 조리 요소 등을 포함할 수 있다. 다만, 이에 한정되는 것은 아니다. 환경은 각종 재료 구성, 재료 양, 재료 성분, 조리 조건, 사용자 행동, 사용자 관능 평가 등의 레시피에 관한 정보를 에이전트에게 제공하며, 레시피의 상태(state,S_t)는 레시피의 속성 정보를 기초로 하고, 보 상(reward, R_t)은 사용자 평가 정보를 기초로 할 수 있다. 이러한 레시피의 상태(S_t)에 대해서 에이전트(31 0)는 레시피를 개선시키는 행동(action, A_t)을 결정하게 된다. 결정된 행동을 환경에게 전달하면, 에이전 트는 환경으로부터 변경된 상태인 레시피 개선 정보(state,S_t+1)와 보상(reward,R_t)을 받게 된다. 구체적으로 먼저, 에이전트는 환경에서 제공하는 현재 상태를 기반으로 행동을 결정한다. 이 상태 정 보는 레시피의 현재 속성 정보(재료 구성, 재료 양, 재료 성분, 조리 조건 등)와 사용자의 평가 내지 피드백 정 보(사용자 행동, 사용자 관능 평가 등)를 포함할 수 있다. 에이전트는 이 상태 정보를 바탕으로 행동을 결 정하며, 이 행동은 레시피의 속성을 변경하거나 수정하는 것으로 예를 들어서, 재료의 양을 변경하거나, 다른 재료로 대체하는 등의 행동일 수 있다. 다음으로, 에이전트가 결정한 행동(A_t)을 환경에 적용하면, 환경은 변경된 상태인 레시피 개선 정보(S_t+1)를 다시 에이전트에게 제공할 수 있다. 또한, 에이전트의 행동에 따른 결과인 레시피 개 선 정보에 대해 사용자로부터 피드백을 받으면, 이 피드백에 관한 사용자 평가 정보에 기초하는 보상(R_t)으로 에이전트에게 전달된다. 이 보상은 에이전트가 미래의 행동(A_t+1)을 결정하는 데 중요한 역할을 한 다. 따라서, 에이전트와 환경 간의 상호작용은 상태 정보의 제공, 행동의 결정, 변경된 상태 정보의 전달, 그리고 보상의 전달을 포함하며, 이를 통해 에이전트는 레시피를 최적화하는 방향으로 행동을 결정하게 된다. 본 개시의 몇몇 실시예에서, 레시피 최적화 모델을 학습시킨 레시피 최적화 시스템에서는 주로 다음과 같이 입 력 데이터와 출력 데이터를 정의할 수 있다. 여기서 입력데이터는, 현재 레시피에 대한 레시피 속성 정보, 현재 레시피에 대한 레시피 개선 정보 및 사용자 평가 정보이다. 이는 에이전트가 행동을 결정하는 데 기반이 된다. 강화학습 모델의 학습 과정에서 레시피 사용자 평가 정보는 보상으로 취급되어 입력 데이터 중 하나가 될 수 있 고, 강화학습 모델의 이용과정에서 레시피의 속성 정보는 현재 상태를 나타내는 입력데이터이자, 에이전트가 결 정하는 행동의 결과로서 변경된 레시피의 결과인 상태를 나타내는 출력데이터에도 해당할 수 있다. 이렇게 강화 학습 과정과 그 이용 과정에서는 입력데이터와 출력데이터가 상호 작용하는 방식으로 동작하게 된다. 레시피 최적화를 수행하는 에이전트는 주어진 상태에서 어떤 행동을 취할 것인지 결정하고, 그 결과로 얻 은 보상을 바탕으로 학습을 진행한다. 이를 통해 에이전트는 보상을 취하는 방향으로 행동하도록 학습하며, 시간이 지남에 따라 보다 나은 레시피를 추천하게 된다. 예를 들어, 레시피의 재료 종류, 재료 양을 상태로 볼 수 있다. 여기서 상태는 특정 레시피에 들어가는 각 재료 들의 비율을 말할 수 있다. 예를 들어, '닭고기 250g' 등이 상태에 해당할 수 있다. 행동은 특정 재료의 배합 비율을 변화시키는 것으로 볼 수 있고, 구체적으로 특정 재료의 양을 늘리거나 줄이는 것을 의미할 수 있다. 예를 들어 행동은 '닭고기 50g 증가'일 수 있고, 이러한 행동을 통해 상태는 다른 상태 (예를 들어서, '닭고기 300g')로 전이된다. 본 개시의 몇몇 일 실시예에 따르면, 이러한 행동은 미리 정해진 기준에 의하여 제한될 수 있다. 예를 들어서, 행동은 특정 재료에 대한 배합 비율 변화에 대한 단위, 범위는 미리 정하여진 기준을 참조할 수 있다. 예를 들 어, 예를 들어서, 닭고기 증가 감소 단위는 50g로 정하는 것처럼 각 재료의 비율 범위를 사전에 정의하고, 랜덤 행동을 선택할 때 이 범위를 벗어나지 않도록 할 수 있다. 이렇게 하면 설탕이 80% 들어간 아이스크림과 같은비현실적인 레시피가 생성되는 것을 방지할 수 있다. 이러한 기준은 레시피의 속성 정보에 포함된 재료 가격, 재료 기능, 조리 조건 등을 이용하거나 식품의약품안전처 데이터베이스, 논문 기반 표준 데이터베이스 등 별도 로 수집된 데이터베이스를 이용할 수 있다. 구체적으로, 레시피 최적화 모델은 레시피에 대한 사용자 평가 정보를 기초로 하여 강화학습의 보상으로 사용할 수 있다. 이 보상은 레시피가 환경에 적합한지, 사용자에게 얼마나 만족스러운지, 또는 그 레시피를 수정함으로 써 얼마나 개선될 수 있는지를 평가하는 것을 의미할 수 있다. 여기서, 레시피의 레시피 개선 정보에 기초하는 결과는 현재 상태에 특정 행동을 취한 후의 상태로, 행동을 취 하면 예를 들어, '닭고기 300g'과 같이 행동에 따른 그 결과로 새로운 레시피 상태가 생성될 수 있다. 환경이 보상(리워드) 함수와 전이 확률 분포 함수를 알고 있으면, 이러한 강화학습은 \"모델 기반\" 강화학 습이라 부른다. 반대로, 에이전트가 환경의 보상 함수와 전이 확률 분포 함수를 모르는 경우에는 \"모 델 프리\" 강화학습이라 부른다. 에이전트는 주어진 환경에서 누적 보상 값을 최대화하는 정책(Policy)을 학습하는 것을 목표로 한다. 이러한 정책은 에이전트가 특정 상태에서 특정 행동을 취할 확률을 결정하는 규칙 집합을 의미한다. 본 개시에 따르면, 환경에는 상태와 보상을 제공할 수 있는 모델이 포함될 수 있으며, 이는 컴퓨팅 장치 내의 시뮬레이터 형태로 존재하거나, 사용자 인터페이스로부터 레시피 정보를 수신하여 상태와 보상을 산 출할 수도 있다. 에이전트가 행동을 결정하는 방법은 가치 기반, 정책 기반, 또는 가치와 정책 모두를 기반으로 한 방법 중 하나 에 근거할 수 있다. 가치 기반 방법은 Q-learning, DQN 같은 가치 함수에 기반한 방법이고, 정책 기반 방법은 Policy Gradient와 같이 가치 함수 없이, 최종 리턴과 정책 함수에 기반하여 행동을 결정하는 방법이다. 가치와 정책 모두를 기반으로 하는 방법은 정책함수가 행동을 결정하고 가치함수가 행동을 평가하여 에이전트의 행동을 결정하는 방식으로, Soft Actor-Critic 알고리즘이 이에 해당한다. 예를 들어서, Q-learning의 경우, Q-learning에서는 TD 방법을 사용해 Q-값을 업데이트하며, 이 때의 업데이트 공식은 아래와 같다. Q(s, a) ηa) + α * [r + γ * max_a' Q(s', a') - Q(s, a)] 여기서 s는 현재 상태, a는 선택한 행동, α는 학습률, r은 즉시 받은 보상, γ는 미래 보상을 얼마나 중요하게 볼 것인지를 결정하는 할인 계수,max_a' Q(s', a')는 다음 상태 s'에서 가능 한 모든 행동 a'에 대한 Q-값 중 최대값, Q(s, a)는 현재 상태와 행동에 대한 이전 Q-값을 의미한다. 상술한 강 화학습 알고리즘은 예시에 불과하며, 필요에 따라 선택되거나 조합될 수 있다. 본 개시의 몇몇 실시예에 따르면, 상기 레시피에 대한 사용자 평가 정보는, 상기 매핑 정보와 관련된 사용자 피 드백을 통해 획득될 수 있다. 예를 들어서, 레시피의 현재 상태를 나타내는 속성 정보 중 어느 하나 이상과 관 련될 수 있고, 이러한 관계는 매핑 정보로 참고된다. 예를 들어서, 사용자 평가 정보는 레시피 전반의 맛, 향, 식감 등 관능에 관한 사용자 피드백으로 획득될 수 있고, 레시피의 재료 조합에 대한 전체적인 재료 기능과 관 련된 것으로 볼 수 있다. 사용자 피드백은 특정 재료로 기존 재료를 대체하여 재료 종류를 변경하거나, 특정 재 료의 양을 조절하거나, 이들의 조합 등의 상태를 변화시키는 액션이나 액션을 수행한 상태 변화에 관한 것일 수 있다. 예를 들어서\"소금을 추가해주세요.\",\"설탕을 스테비아로 교체해주세요\", \"고추기름을 1% 줄여주세요\"와 같이 재료의 종류나 양을 직접 조절하도록 지시하는 것일 수 있다. 또는, 사용자 피드백이 맛과 같은 기능과 관 련된 경우, 재료-기능 매핑 정보는 사용자 피드백의 해석과 보상 계산에 중요한 역할을 수행하게 된다. 즉, 레 시피의 사용자 평가 정보와 레시피 정보 사이 관계는 상기 재료-기능 매핑 정보에 의하여 설명될 수 있다. 매핑 정보를 사용하여 사용자 피드백과 관련한 특정 재료를 찾아내고, 그 재료에 대한 변경(예: 양 조절)을 행동으로 하여 행동의 범위를 제한할 수 있다. 이를 통해 강화학습 에이전트가 초기 상태에서 더욱 효율적인 학습을 수행 할 수 있다. 본 개시의 몇몇 실시예에 따르면, 프로세스는 이러한 사용자 피드백을 매핑 정보를 기초로 하여 사용자 평 가 정보로 전처리하는 과정을 수행할 수 있다. 예를 들어서, \"단맛이 너무 강해요. 단맛 8입니다. 설탕을 줄여 주세요\"라는 사용자 피드백은 단맛이라는 재료 기능과 관련된 재료 종류에 대하여 재료를 대체하거나 재료의 양 을 조절하는 최적의 액션을 수행하는 사용자 평가 정보로 산출될 수 있다. 즉, 프로세서는 사용자의 피드 백을 자연어 처리(NLP) 등을 이용하여 적절한 형태로 해석할 수 있고, 예를 들어, \"단맛이 너무 강해요. 단맛 8입니다. 설탕을 줄여주세요\"라는 피드백에서 '단맛', '강하다', '8', '설탕', '줄이다' 등의 주요 정보를 추출 할 수 있다. 이러한 과정을 통해 사용자 피드백은 에이전트의 학습 및 레시피 최적화에 활용되는 보상으로 전환 될 수 있다. 본 개시의 몇몇 실시예에 따르면, 상술한 매핑 정보를 활용하여 상기 레시피 최적화 모델을 학습시킬 수 있다. 즉, 레시피 최적화 모델을 재료 조합에 따른 재료 기능에 관한 매핑 정보로 사전 학습 시킨 뒤, 매핑 정보와 관 련한 사용자 피드백을 이용하여 획득한 레시피의 사용자 평가 정보 및 레시피 개선 정보를 학습 세트로 하여 학 습시킬 수 있다. 학습된 레시피 최적화 모델은 입력된 현재 상태에 대해서 매핑 정보를 기초로 하여 액션을 결 정하여 개선 레시피를 획득하도록 할 수 있다. 이러한 매핑 정보를 활용하여 레시피 최적화 모델 학습은 재료 조합에 따라 변화하는 기능적 변화를 기초로 하여 사전학습하여 초기화하므로 이후 강화 학습 단계에서도 레시 피의 속성 정보를 이용하여 매핑된 재료 조합 및 그에 상응하는 기능적 변화와, 이에 관하여 사용자가 피드백에 관한 사용자 평가 정보를 강화학습 단계에서 직접 반영할 수 있게 된다. 본 개시의 몇몇 실시예에 따르면, 레시피 최적화 모델을 학습시키기 위한 보상을 계산하는 방법은 문제의 특성 과 목표에 따라 달라질 수 있다. 예를 들어서, 사용자가 피드백을 숫자로 제공하는 경우(예: \"맛 평점 8점\"), 이 점수를 그대로 보상으로 사용할 수 있고, 레시피에 대한 피드백 점수가 높을수록 에이전트는 그 레시피를 선 호하게 될 수 있다. 또는, 사용자가 이전 레시피와 비교하여 피드백을 제공하는 경우(예: \"이 레시피는 이전 레 시피보다 더 좋다\"), 이 정보를 상대적인 보상으로 사용할 수 있다. 예를 들어, 이전 레시피에 비해 현재 레시 피의 평점이 더 높다면 보상을 높게 설정하고, 그 반대의 경우 보상을 낮게 설정할 수 있다. 바람직하게는 사용 자 피드백을 재료-기능 매핑 정보와 결합하여 사용자가 주는 피드백이 특정 재료나 기능과 관련된 경우, 해당 재료나 기능의 변화를 반영하는 방식으로 보상을 조정할 수 있고, 이를 통해 에이전트는 사용자의 선호도를 더 정확하게 학습하고 레시피를 개선할 수 있다. 다만, 이에 한정하지 아니한다. 다시 말해서 위 와 같은 재료-기능 매핑 정보를 이용하는 강화학습 기반 레시피 최적화 학습 모델은 매핑 정보 를 이용하지 않는 방법에 비해 재료가 가지는 특성에 따른 음식의 맛, 영양성분, 조리 방법 등에 대한 정보를 더 잘 활용할 수 있어 모델이 더욱 정교한 예측과 결정을 할 수 있고, 새로운 재료나 레시피에 대한 추론 능력 이 강화되어 알려지지 않은 재료의 특성이나, 사용자 피드백이 부족한 새로운 레시피에 대한 예측에 도움이 될 수 있다. 또한, 재료-기능 매핑 정보를 이용하면, 모델은 재료 간의 상호작용이나 특성에 대한 지식을 더 빠르 게 학습할 수 있어, 전반적인 학습 효율성이 향상될 수 있다. 마찬가지로, 사용자 피드백이 상기 매핑 정보와 관련되어 있다면, 모델이 사용자의 선호나 요구를 더 정확하게 반영할 수 있게 한다. 따라서, 재료 기능 매핑 정보를 이용하는 강화학습 기반 레시피 최적화 모델은 최적화 레시피 생성, 사용자 피 드백 처리, 모델 학습 및 추론 등 다양한 과정에서 유리한 점이 있다. 본 개시의 몇몇 실시예에서, 프로세서는 상기 S110 단계와 관련하여, 매핑 정보를 기초로 초기 지식을 형 성하도록 초기화 내지 사전 학습되는 단계(S100)를 수행할 수 있다. 이러한 매핑 정보는 사전에 데이터베이스에 수집되어 있을 수 있고, 식품의약품안전처 데이터베이스, 논문 기반 표준 데이터베이스 등 외부 데이터베이스나 사용자 피드백을 반영하여 갱신되어질 수 있다. 예를 들어서, S100은 재료 종류, 재료 양, 재료 기능에 관한 정 보를 이용하여 매핑 정보를 획득하고, 이를 사전 학습 데이터로 하여 초기 지식을 형성하도록 레시피 최적화 모 델을 사전 학습시킬 수 있다. 사전 학습 방식은 지도학습, 비지도학습, 강화 학습 등을 구분하지 아니한다. 여기서, 상기 매핑 정보를 기초로 초기 지식을 형성한다는 의미는 구체적인 학습 모델의 학습 방법에 따라 다를 수 있다. 레시피 최적화 모델이 강화학습 방식을 취하는 경우, 상기 매핑 정보는 레시피에 대한 행동과 그에 대 한 보상과의 관계를 직간접적으로 나타낼 수 있다. 이를 이용하면, 매핑 정보를 사용하여 각 상태 및 액션의 초 기 가치를 추정하고, 이를 통해 에이전트가 학습을 시작할 때 더욱 효과적인 탐색을 수행하도록 할 수 있다. 예 를 들어, Q-learning의 경우, Q-value를 위 매핑 정보를 기반으로 0이 아닌 값으로 설정하는 것을 의미한다. 또는 사전 학습이 지도학습인 경우가 있을 수 있다. 사전 학습 데이터는 재료 종류 정보나, 재료 종류 및 재료 양 정보 조합을 입력 데이터로 하고, 그들의 조합에 대응하는 재료 기능 정보를 출력 데이터로 하여 지도 학습 을 수행할 수 있다. 즉, 지도학습의 사전 학습이란 매핑 정보를 기초로, 재료 종류 및 재료 양 조합과 재료 기 능 정보를 레이블한 학습 세트를 획득하고, 이를 사전 학습 것을 의미할 수 있다. 다만, 이에 한정되는 것은 아 니다. 이러한 사전 학습을 수행한 뒤 레시피 최적화 모델이 강화학습을 수행하는 경우, 사용자 피드백이 부족하더라도 미리 초기화된 지식을 강화학습의 탐색 과정에서 이용하게 되므로 레시피 최적화 모델을 구현하는데 도움이 될수 있다. 또는, 초기에는 이러한 매핑 정보를 참조하여 탐색할 수 있으나, 강화학습 에이전트가 자신의 경험을 바탕으로 학습하면서도, 일정한 확률로 탐색을 수행하여 새로운 가능성을 찾아가는 방법을 채택하는 입실론 그 리디 방법을 사용하는 것도 가능하다. 다만, 이에 한정되는 것은 아니다. 본 개시의 몇몇 실시예에 따라, 매핑 정보를 기초로 초기 지식을 형성하도록 초기화 내지 사전 학습되는 단계 (S100)를 통해 학습된 레시피 최적화 모델에 기초하여 기본 레시피에 대하여 사용자 요구사항을 기초로 개선한 레시피인 최초의 레시피 개선 정보를 획득할 수 있다. 본 개시의 몇몇 실시예에 따라, 레시피 최적화 모델이 강화학습을 통해 학습시킨 학습 모델인 경우 적어도 하나 의 레시피에 관한 레시피 개선 정보 및 사용자 평가 정보를 수집하는 단계(S110)는 강화학습을 위한 초기화 단 계(S100)를 포함할 수 있다. 이러한 초기화 단계에서, 적어도 하나의 레시피에 관한 레시피 개선 정보 및 사용자 평가 정보는 각 레시피의 상태와 행동으로 분리되고, 모든 가능한 상태와 행동에 대해 보상을 0으로 설정하여 처음에는 무작위로 행동하 면서 환경을 탐색하게 하거나, 이미 수집된 레시피에 관한 사용자 평가 정보를 바탕으로 초기 보상 값을 설정하 여 학습 초기 단계 부터 사용자 피드백이 좋은 레시피를 우선적으로 이용하도록 유도할 수도 있고, 후술하는 매 핑 정보와 같은 사전 지식을 모델로 하여 매핑 정보를 이용하여 사용자 피드백과 관련된 재료가 포함된 레시피 상태와 그 재료를 변경하는 행동으로 범위를 제한하는 일종의 모델 기반 강화학습을 하도록 할 수도 있다. 다만, 이에 한정하지 아니한다. 본 개시의 몇몇 실시예에 따라, 레시피 최적화 모델이 강화학습을 통해 학습시킨 학습 모델인 경우, 프로세서 는 상기 S130 단계와 관련하여, 레시피의 재료 조합과 재료 기능 변화 간의 관계에 관한 정보인 재료- 기 능 매핑 정보에 기초하여 액션을 결정하도록 할 수 있다. 매핑 정보는 각 재료의 종류, 재료의 양, 조리 조건 등이 전체 레시피의 맛, 향, 질감 등에 어떤 영향을 미치는지를 나타내는 정보로, 예를 들어 설탕이 어떤 중량 비율에서 특정 단맛 강도를 제공하는지와 같은 정보를 포함할 수 있다. 일 실시예에서, 레시피 최적화 모델이 현재 상태에 대한 행동을 결정하는 전략에 있어서, 새로운 시도를 하는 탐색(Exploration)과 경험 기반 선택을 하려는 이용(Exploitation)을 하는 전략 모두 매핑 정보가 적용될 수 있 다. 예를 들어, 탐색 전략이 랜덤 상태로 전이시키는 경우, 매핑 정보를 참조하여 현재 상태 중에서 이전 사용 자 피드백과 관련된 재료 기능과 매핑된 재료 종류를 가진 상태로 전이하는 액션으로 랜덤 상태를 찾을 수 있도 록 제한하거나, 이용 전략에서도 경험이 없는 상태와 행동에 대한 가상 보상으로 매핑 정보를 이용하여 초기화 할 수 있다. 본 개시의 몇몇 실시예에 따라, 레시피 최적화 모델이 강화학습을 통해 학습시킨 학습 모델인 경우, 프로세서 는 상기 S120 단계와 관련하여서도, 매핑 정보를 이용하면 에이전트가 사용자 피드백에 부합하는 액션 정 책을 찾을 때 어떤 재료가 레시피의 전반적인 기능적 특성(예: 맛, 영양성, 질감 등)에 어떤 영향을 미치는지 를 고려할 수 있다. 다시 말해서 에이전트가 어떤 재료를 어떻게 조합하고 변화시킬 것인지를 선택할지 매핑 정 보에 기초하여 결정할 수 있다. 사용자 평가 정보에 기초한 보상은 피드백에서 나타나는 사용자의 만족도를 나 타내는 지표로 볼 수 있고 예를 들어, \"단맛이 너무 강해요\"라는 피드백은 단맛의 강도가 높아 사용자의 만족도 를 떨어뜨렸음을 의미하므로, 이를 부정적인 피드백으로 액션 정책에 반영하도록, 해당 보상과 관련한 행동을 단맛이라는 재료 기능을 갖는 재료 종류를 매핑 정보를 이용하여 찾을 수 있고, 해당 재료를 대체하거나 재료의 양을 줄이도록 관련된 행동 정책을 학습시킬 수 있다. 이렇게 사용자 피드백을 받았는데 설탕의 양을 늘리는 행 동이 부정적인 보상을 얻었다면, 에이전트는 앞으로 설탕의 양을 늘리는 행동을 선호하지 않게 된다. 다만, 이 에 한정되는 것은 아니다. 이하, 프로세서가 본 개시의 몇몇 실시예에 따른 학습 방법을 사용하여 학습된 레시피 최적화 모델을 이용 하여, 최적화 레시피를 획득한 뒤, 사용자인터페이스를 이용하여 제공하는 방법을 개시한다. 프로세서는 획득한 최적화 레시피를 사용자인터페이스를 통해 사용자에게 제공하며, 해당 최적화 레시피에 관한 속성 정보를 이용하여 재료 종류, 재료 양, 재료 성분, 재료 가격, 재료 기능을 포함하여 제공할 수 있고, 이러한 정보들을 외부 데이터베이스를 이용하여 가공하여 제공할 수 있다. 예를 들어, 프로세서는 획득한 최적화 레시피의 속성 정보 및 외부 데이터베이스를 참조하여 영양성분, 환경 친화 점수, 안정 관련 정보, 재료 가격 정보를 포함한 보고서를 생성하여 사용자에게 제공할 수 있다. 이하, 프로세서가 본 개시상의 몇몇 실시예에 따른 학습 방법을 사용하여 학습된 레시피 최적화 모델을 이 용하여, 레시피 최적화를 수행하는 일 실시예가 개시된다.프로세서는 개선이 필요한 대상 레시피의 상태를 나타내는 레시피 정보를 획득하고, 레시피 최적화 모델을 기초로 새로운 레시피에 관하여 레시피 최적화를 수행할 수 있다. 여기서, 레시피 최적화 모델은, 레시피 변화 에 상응하는 사용자 평가에 최적화된 레시피를 도출하도록 적어도 하나의 기존 레시피에 관한 레시피 개선 정보 및 사용자 평가 정보를 기초로 학습시킨 모델에 대응될 수 있다. 이때, 상기 레시피 최적화 모델은 바람직하게는, 상술한 레시피의 재료 조합과 기능 변화 간의 관계에 관한 정 보로 사전 학습된 강화 학습 방법으로 구현된 것일 수 있으나, 이에 한정되지 않고 다양한 학습 방법이 사용될 수 있음은 물론이다. 본 개시의 몇몇 실시예에 따르면, 프로세스는 학습된 레시피 최적화 모델을 활용하여 새로운 레시피의 최 적화를 수행할 수 있다. 이미 학습된 모델을 이용하여 새로운 레시피를 현재 상태로 입력하고, 그 결과로 최적 의 액션을 도출하여 개선된 레시피를 획득하게 되며 여기서는 추가적인 학습은 발생하지 않으며, 학습된 모델은 단지 현재 상태에 대한 최적의 액션을 도출한다. 한편, 프로세스는 현재 레시피 최적화 모델을 테스트하도록 테스트 레시피의 속성 정보를 입력한 뒤, 모델 이 출력한 상태에 대한 사용자의 평가를 받아 학습된 모델의 성능을 평가할 수 있다. 이때, 테스트 레시피는 기 존 학습된 레시피 세트와 달리하는 것이 바람직하다. 이하, 구체적인 본 개시의 일 실시예를 설명한다. 예를 들어, 사용자에게 기본 레시피(S_1)인 닭고기 200g의 개선 레시피(S_2)로 닭고기 250g이 제공되고, 기본 레시피 상태 (S_1)에 닭고기 양을 50g 증가(행동 A_1)에 대한 사용자 피드백을 기초로한 보상(R_1)으로 강화학 습 모델을 학습시키는 과정을 반복하여, 다시 이전 레시피 상태에 대해 새로운 개선 레시피(S_t)를 획득하는 과 정을 통해 사용자에게 연속적으로 개선된 레시피(S_t+1)를 제공하는 사이클이 있을 수 있다. 상태 S_1: 닭고기 200g, 행동 A_1: 닭고기 양을 50g 증가, 사용자 피드백을 받아 보상 R_1: 긍정적인 피드백 (+1), 새로운 상태 S_2: 닭고기 250g. 상태 S_2: 닭고기 250g, 행동 A_2: 닭고기 양을 50g 증가, 사용자 피드백을 받아 보상 R_2: 중립적인 피드백 , 새로운 상태 S_3: 닭고기 300g. 상태 S_3: 닭고기 300g, 행동 A_3: 닭고기 양을 50g 감소, 사용자 피드백을 받아 보상 R_3: 긍정적인 피드백 (+1), 새로운 상태 S_4: 닭고기 250g. 상태 S_4: 닭고기 250g, 행동 A_4: 닭고기 양을 50g 감소, 사용자 피드백을 받아 보상 R_4: 부정적인 피드백 (- 1), 새로운 상태 S_5: 닭고기 200g. 상태 S_5: 닭고기 200g, 행동 A_5: 닭고기 양을 50g 증가, 사용자 피드백을 받아 보상 R_5: 긍정적인 피드백 (+1), 새로운 상태 S_6: 닭고기 250g. 위 과정을 강화학습 알고리즘 중 Q-learning으로 수행한 경우, 아래와 같은 업데이트 공식을 사용하며, α=0.5, γ=0.9로 가정하여 계산하였다. Q(s_t, a_t) ηa_t) + α * [r_t+1 + γ * max_a Q(s_t+1, a) - Q(s_t, a_t)] Q(S_1, A_1) = 0.5, Q(S_2, A_2) = 0, Q(S_3, A_3) = 0.5, Q(S_4, A_4) = -0.5, Q(S_5, A_5) = 0.5 미리 정하여진 성능이나 수렴 등 조건에 따라 위와 같은 학습 과정의 강화학습 모델의 학습을 종료한다면, 레시 피 최적화 모델에 기초하여 닭고기 250g 이라는 레시피가 최적화된 레시피라고 획득될 수 있게 된다. 본 개시가 일반적으로 컴퓨터 장치에 의해 실행될 수 있다고 서술되었으나, 당업자들은 본 개시가 하나 이상의 컴퓨터에서 작동할 수 있는 컴퓨터 실행 가능한 명령어와 다른 프로그램 모듈들과 결합되어 구현될 수 있음을 인식하고 있을 것이다. 또한, 하드웨어와 소프트웨어의 조합으로도 구현될 수 있다. 프로그램 모듈은 일반적으로 특정 작업을 수행하거나 특정 추상 데이터 유형을 구현하는 기능, 프로그램, 구성 요소, 데이터 구조 등을 포함한다. 또한, 당업자들은 본 개시에서 제시된 방법이 단일 프로세서 또는 다중 프로 세서 컴퓨터 시스템, 미니 컴퓨터, 메인프레임 컴퓨터, 개인용 컴퓨터, 휴대용 컴퓨팅 장치, 마이크로프로세서 기반 또는 프로그램 가능한 가전제품 등 다양한 컴퓨터 시스템 구성에서 실행될 수 있음을 인지하고 있을 것이다.본 개시에서 설명된 실행 예들은 또한 일부 작업들이 통신 네트워크를 통해 연결된 원격 처리 장치들에 의해 이 루어지는 분산 컴퓨팅 환경에서도 실행될 수 있다. 이 분산 컴퓨팅 환경에서 프로그램 모듈들은 로컬 메모리 저 장장치와 원격 메모리 저장장치 모두에 위치할 수 있다. 컴퓨터는 일반적으로 다양한 컴퓨터 읽을 수 있는 매체를 포함한다. 컴퓨터가 액세스 가능한 매체는 컴퓨터 읽 을 수 있는 매체로 간주될 수 있으며, 이 매체들은 휘발성 및 비휘발성 매체, 일시적 및 비일시적 매체, 이동식 및 고정식 매체를 포함한다. 예를 들어, 컴퓨터 읽을 수 있는 매체는 컴퓨터 읽을 수 있는 저장 매체와 컴퓨터 읽을 수 있는 전송 매체를 포함할 수 있다. 컴퓨터 읽을 수 있는 저장 매체는 정보를 저장하는 다양한 방법과 기술을 사용하여 구현되는 매체로, 이 매체는 RAM, ROM, EEPROM, 플래시 메모리, CD-ROM, 디지털 비디오 디스크 (DVD), 자기 카세트, 자기 테이프, 자기 디스크 저장장치 등을 포함할 수 있다. 컴퓨터 읽을 수 있는 전송 매체는 반송파 또는 다른 전송 메커니즘을 통해 컴퓨터 읽을 수 있는 명령어, 데이터 구조, 프로그램 모듈 등을 구현하는 매체로, 모든 정보 전달 매체를 포함한다. \"피변조 데이터 신호\"라는 용어 는 신호의 특성 중 하나 이상을 설정하거나 변경하여 신호에 정보를 인코딩하는 신호를 의미한다. 예를 들어, 컴퓨터 읽을 수 있는 전송 매체는 유선 네트워크, 직접 연결선, 음향, RF, 적외선, 그리고 기타 무선 매체 등을 포함한다. 이러한 매체들 중 어떠한 조합도 컴퓨터 읽을 수 있는 전송 매체의 범위 안에 포함될 수 있다. 본 개시에 따른 환경의 일례로 컴퓨터가 포함되어 있으며, 이 컴퓨터는 처리 장치, 시스템 메모리, 그리고 시스템 버스를 내장하고 있다. 여기서 시스템 버스는 처리 장치에 다 양한 시스템 구성 요소들, 특히 시스템 메모리 등을 연결하는 역할을 담당한다. 처리 장치는 다양 한 상용 프로세서 중 어느 것이라도 될 수 있으며, 듀얼 프로세서 또는 다른 멀티프로세서 아키텍처도 처리 장 치로 사용될 수 있다. 시스템 버스는 메모리 버스나 주변장치 버스, 그리고 여러 종류의 상용 버스 아키텍처 등 다양한 형태의 로컬 버스를 통해 다른 요소들과 상호 연결될 수 있다. 시스템 메모리는 판독 전용 메모리(ROM)와 랜덤 액세스 메모리(RAM)를 포함하고 있다. 비휘발성 메모리로서의 ROM, EPROM, EEPROM 등에는 BIOS가 저장되며, BIOS는 컴퓨터 구성 요소들 사이에서 정보를 교환하는 기본적인 루틴을 담당한다. RAM는 또한 고속 RAM과 같은 데이터 캐싱을 위한 정적 RAM도 포함할 수 있다. 이 컴퓨터는 내장형 하드 디스크 드라이브(HDD), 자기 플로피 디스크 드라이브(FDD), 그리고 광 디스크 드라이브 등도 포함하고 있다. 이들은 각각 하드 디스크 드라이브 인터페이스, 자기 디 스크 드라이브 인터페이스 및 광 드라이브 인터페이스를 통해 시스템 버스에 연결될 수 있다. 이들 인터페이스 중 일부는 외장형 드라이브를 위해 USB 또는 IEEE 1394 인터페이스 기술을 사용한다. 이들 드라이브와 연관된 컴퓨터 판독 가능 매체는 비휘발성 저장의 형태로 데이터, 데이터 구조, 컴퓨터 실행 가능 명령 등을 제공한다. 컴퓨터의 입장에서 보면, 이 드라이브와 매체는 모든 데이터를 적절한 디지털 형식으로 저장하는 역할을 한다. HDD, 이동식 자기 디스크, CD 또는 DVD 등의 이동식 광 매체는 본 개시의 컴퓨 터 판독 가능 매체의 예로 들 수 있지만, Zip 드라이브, 자기 카세트, 플래쉬 메모리 카드, 카트리지 등과 같은 다른 유형의 매체도 사용될 수 있다. 이들 매체는 본 개시의 방법을 실행하는 데 필요한 컴퓨터 실행 가능 명령 을 포함할 수 있다. 다양한 프로그램 모듈, 운영 체제, 하나 이상의 애플리케이션 프로그램, 그 외의 프로그램 모듈 , 프로그램 데이터 등이 드라이브와 RAM에 저장될 수 있다. 이들 중 일부 또는 전부는 RAM에 캐싱될 수 있다. 본 개시는 다양한 상업용 운영 체제나 그 조합에서 구현될 수 있다. 사용자는 키보드, 마우스 등의 입력 장치를 통해 컴퓨터에 명령이나 정보를 입력할 수 있다. 또한 마이크, IR 리모콘, 조이스틱, 게임 패드, 스타일러스 펜, 터치 스크린 등의 다른 입력 장치도 사용될 수 있다. 이들 입력 장치는 주로 입력 장치 인터페이스를 통해 시스템 버스와 연결되며, 이는 처리 장 치에 연결된다. 이 연결은 병렬 포트, IEEE 1394 직렬 포트, 게임 포트, USB 포트, IR 인터페이스 등의 다양한 인터페이스를 통해 이루어질 수 있다. 컴퓨터는 시스템 버스를 통해 비디오 어댑터와 같은 인터페이스를 통해 모니터나 다른 디스플레이 장치에 연결될 수 있다. 이외에도 스피커, 프린터 등 여러 주변 출력 장치를 내장할 수 있다. 이 컴퓨터는 원격 컴퓨터(들)와 네트워크를 구성하여 유선 또는 무선 통신을 통해 연결될 수 있다. 원격 컴퓨터(들)는 워크스테이션, 개인용 컴퓨터, 노트북, 게임 장치, 피어 장치 등 다양한 형태가 될 수 있으며, 이들은 컴퓨터의 구성요소 중 일부 또는 전부를 포함할 수 있다. 하지만 간결함을 위해, 메모리 저장 장치만을 표시했다. 이러한 연결은 LAN이나 WAN 등의 네트워크로 형성될 수 있으며, 이 는 사무실이나 회사에서 일반적으로 사용되는 네트워킹 환경이다. 이런 네트워크는 인트라넷 등의 기업 네트워 크를 구성하고, 이를 통해 전 세계적인 컴퓨터 네트워크, 예를 들어 인터넷에 연결될 수 있다. LAN에서 컴퓨터는 유선 또는 무선 통신 네트워크 인터페이스 또는 어댑터를 통해 로컬 네트워크 에 연결될 수 있다. 어댑터는 LAN으로의 유선 또는 무선 통신을 가능하게 하며, LAN은 무선 어댑터와 통신하기 위해 설치된 무선 액세스 포인트를 포함한다. WAN에서는 컴퓨터가 모뎀 을 통해, WAN상의 다른 컴퓨팅 장치에 연결되거나, 인터넷을 통해 통신을 수립한다. 이 모뎀(115 8)은 내장형이거나 외장형이며 유선 또는 무선일 수 있으며, 이는 직렬 포트 인터페이스를 통해 시스템 버스에 연결된다. 네트워크 환경에서, 컴퓨터에서 설명된 프로그램 모듈이나 부분이 원격 메모리/ 저장 장치에 저장될 수 있다. 표현된 네트워크 연결은 예시이며, 컴퓨터들 사이에 통신 링크를 구성하는 다른 방법도 가능하다. 컴퓨터는 프린터, 스캐너, 데스크톱 및/또는 휴대용 컴퓨터, PDA, 통신 위성, 무선 태그 등과 같은 다양 한 무선 장치와 통신할 수 있다. 이는 최소한 Wi-Fi와 블루투스 무선 기술을 포함한다. 따라서 통신은 표준 네 트워크 구조를 따르거나, 최소한 두 장치 간의 애드혹 통신(ad hoc communication)일 수 있다. Wi-Fi는 인터넷에 무선으로 접속할 수 있게 해주는 기술이다. 이는 컴퓨터가 실내, 실외 어디에서나 데이터를 전송 및 수신할 수 있게 해주는 셀룰러 전화와 비슷한 기술이다. Wi-Fi 네트워크는 안전하고 신뢰할 수 있으며, 빠른 무선 연결을 제공하기 위해 IEEE 802.11(a, b, g, 기타) 무선 기술을 사용한다. Wi-Fi는 컴퓨터를 서로, 인터넷, 또는 유선 네트워크(IEEE 802.3 또는 이더넷 사용)에 연결하는 데 사용될 수 있다. Wi-Fi 네트워크는 비인가된 2.4 및 5GHz 무선 대역에서 동작하며, 11Mbps(802.11a) 또는 54 Mbps(802.11b) 데이터 속도로, 또는 양 대역을 모두 지원하는 제품에서 동작할 수 있다. 본 개시의 기술 분야에서 일반적으로 알려진 바와 같이, 데이터, 지시, 명령, 정보, 신호, 비트, 심볼, 칩 등은 전압, 전류, 전자기파, 자기장, 광학장, 입자 등 다양한 형태로 표현될 수 있다. 본 개시의 기술 분야에서 통상의 지식을 가진 사람은 여기서 소개된 구현 사례들과 관련하여 여러 논리 블록들, 모듈들, 프로세서들, 수단들, 회로들 및 알고리즘 단계들이 전자 하드웨어, 다양한 형태의 프로그램(여기서는 편의상 소프트웨어로 지칭), 설계 코드 또는 이들의 조합에 의해 구현될 수 있다는 사실을 인지하게 될 것이다. 하드웨어와 소프트웨어의 이런 상호 작용성을 명확히 하기 위해, 여러 예시적인 구성 요소들, 블록들, 모듈들, 회로들 및 단계들은 그들의 기능에 대해 이전에 설명되었다. 이들 기능이 하드웨어 또는 소프트웨어로 구현되는 지는 특정 애플리케이션과 전체 시스템에 부과되는 설계 제한사항에 따라 결정된다. 본 개시의 기술 분야에서 일반적인 지식을 가진 사람은 각각의 특정 애플리케이션에 대해 설명된 기능을 다양한 방법으로 구현할 수 있지 만, 이러한 구현 결정들은 본 개시의 범위를 초과하는 것으로 해석되어서는 안 된다. 본 개시에서 제안하는 다양한 구현 방식은 방법론, 장치, 또는 표준 프로그래밍 및/또는 엔지니어링 기법을 활 용한 제품으로 구현될 수 있다. '제품'이라는 용어는 어떠한 컴퓨터-가능 저장 장치에서 접근 가능한 컴퓨터 프 로그램, 캐리어, 또는 매체를 포함한다. 예를 들면, 컴퓨터-판독가능 저장매체는 자기 저장장치(예컨대, 하드 디스크, 플로피 디스크, 자기 테이프 등), 광학 디스크(예컨대, CD, DVD 등), 스마트 카드, 그리고 플래시 메모 리 장치(예컨대, EEPROM, 카드, 스틱, 키 드라이브 등)를 포함하나, 이에 국한되지는 않는다. 또한, 여기서 언 급된 다양한 저장 매체는 정보 저장을 위한 하나 이상의 장치 및/또는 다른 기계 판독 가능한 매체를 포함한다. 제시된 과정들에서의 단계들의 특정 순서나 계층 구조는 예시적인 접근 방법의 일부일 뿐이라는 것을 알아두어 야 한다. 설계 우선 사항에 따라, 본 개시의 범위 내에서 과정들의 단계들의 특정 순서나 계층 구조는 재배열 될 수 있다는 것을 알아두어야 한다. 첨부된 방법 클레임들은 다양한 단계들의 요소들을 샘플 순서로 제공하지 만, 제시된 특정 순서나 계층 구조에 국한되는 것은 아니다. 제시된 구현 사례들에 대한 설명은 본 개시의 기술 분야에서 일반적인 지식을 가진 사람이 본 개시를 사용하거 나 실행할 수 있도록 도와준다. 이러한 구현 사례들에 대한 다양한 변형들은 본 개시의 기술 분야에서 일반적인 지식을 가진 사람에게는 분명할 것이며, 본 개시에 정의된 일반적인 원칙들은 본 개시의 범위를 넘어서 다른 구 현 사례들에도 적용될 수 있다. 따라서, 본 개시는 여기서 제시된 구현 사례들에 한정되지 않고, 여기서 제시된 원칙들과 새로운 특성들을 일관성 있게 적용한 최대한 넓은 범위로 해석되어야 할 것이다.도면 도면1 도면2 도면3"}
{"patent_id": "10-2023-0100593", "section": "도면", "subsection": "도면설명", "item": 1, "content": "아래 제시된 실시예들과 함께 구체적인 설명을 통해 본 개시에 언급된 특성들을 이해할 수 있도록 일부 실시예 들은 첨부된 도면에서 표현되며, 도면에서 동일하거나 유사한 기능을 칭하기 위해 같은 참조 번호를 사용할 수 있다. 그러나 첨부된 도면들은 이 개시의 범위를 제한하려는 것이 아니라, 단지 전형적인 실시예들을 나타내는 것뿐이며, 같은 효과를 도출할 수 있는 다른 실시예들이 존재함을 인식하도록 한다. 도 1은 본 개시의 다양한 태양이 구현될 수 있는 컴퓨팅 장치의 블록 구성도이다. 도 2는 강화학습 방법을 설명하는 에이전트와 환경에 관한 개념도이다. 도 3은 본 개시의 일 실시예에 따른 레시피 최적화 방법의 일례를 설명하기 위한 흐름도이다."}
