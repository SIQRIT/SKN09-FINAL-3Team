{"patent_id": "10-2023-0075414", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0108223", "출원번호": "10-2023-0075414", "발명의 명칭": "지식 증류 기반의 인공지능 모델에 대한 압축 장치 및 방법", "출원인": "주식회사 딥이티", "발명자": "조용범"}}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "지식 증류 기반의 인공지능 모델에 대한 압축 방법에 있어서,미리 구축된 추론 모델을 교사 모델로 설정하는 단계;타겟 모델을 상기 교사 모델에 대응하는 학생 모델로 설정하는 단계;상기 추론 모델 및 상기 타겟 모델 중 적어도 하나의 구조를 분석하여 모델 구조 정보를 도출하는 단계; 및상기 모델 구조 정보를 이용하여 상기 교사 모델을 통한 지식 증류(Knowledge Distillation)를 적용함으로써 상기 타겟 모델의 훈련을 수행하는 단계,를 포함하는, 압축 방법."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 타겟 모델을 실행하기 위한 컴퓨팅 장치를 분석하여 하드웨어 인식 정보를 도출하는 단계;상기 모델 구조 정보 및 상기 하드웨어 인식 정보 중 적어도 하나에 기초하여 상기 타겟 모델에 대하여 적용될압축 알고리즘을 결정하는 단계; 및상기 결정된 압축 알고리즘과 연계된 매개변수를 상기 모델 구조 정보 및 상기 하드웨어 인식 정보 중 적어도하나를 고려하여 설정하는 단계,를 더 포함하는 것인, 압축 방법."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 하드웨어 인식 정보를 도출하는 단계는,상기 컴퓨팅 장치 또는 상기 컴퓨팅 장치에 탑재되는 리소스의 유형별 추론 성능에 대한 테스트 결과를 기록한데이터 테이블을 이용하여 상기 하드웨어 인식 정보를 도출하는 것인, 압축 방법."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 하드웨어 인식 정보를 도출하는 단계는,상기 모델 구조 정보를 이용하여 상기 추론 모델에 대한 연산 융합을 수행하는 단계;상기 연산 융합이 적용된 연산자를 이용하여 상기 데이터 테이블을 조회하는 단계; 및상기 연산자의 수행 시간을 누적하여 상기 추론 모델과 연계된 후보 모델의 추론 성능을 산출하는 단계,를 포함하는 것인, 압축 방법."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 하드웨어 인식 정보를 도출하는 단계는,상기 조회하는 단계에서 상기 연산자가 상기 데이터 테이블에 적중되지 않으면, 상기 연산자와 연계된 상기 추론 성능을 출력하도록 미리 학습된 성능 예측 모델을 이용하여 상기 추론 성능을 도출하는 것인, 압축 방법.공개특허 10-2024-0108223-3-청구항 6 제2항에 있어서,상기 압축 알고리즘은,프루닝(Pruning), 양자화(Quantization) 및 연산자 융합 중 적어도 하나를 포함하는 것인, 압축 방법."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 추론 모델 및 상기 타겟 모델은 합성곱 신경망(Convolutional Neural Network, CNN) 모델을 포함하는 것인, 압축 방법."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 프루닝은 구조적 프루닝 및 비구조적 프루닝을 포함하고,상기 압축 알고리즘을 결정하는 단계는,상기 컴퓨팅 장치에 탑재되는 프로세싱 유닛의 유형에 따라 상기 타겟 모델의 각 브랜치에 적용되는 프루닝 기법의 유형을 선택적으로 결정하는 것인, 압축 방법."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "지식 증류 기반의 인공지능 모델에 대한 압축 장치에 있어서,미리 구축된 추론 모델을 교사 모델로 설정하고, 타겟 모델을 상기 교사 모델에 대응하는 학생 모델로 설정하는모델 설정부;상기 추론 모델 및 상기 타겟 모델 중 적어도 하나의 구조를 분석하여 모델 구조 정보를 도출하는 구조 분석부;및상기 모델 구조 정보를 이용하여 상기 교사 모델을 통한 지식 증류(Knowledge Distillation)를 적용함으로써 상기 타겟 모델의 훈련을 수행하는 지식 증류 적용부,를 포함하는, 압축 장치."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 타겟 모델을 실행하기 위한 컴퓨팅 장치를 분석하여 하드웨어 인식 정보를 도출하는 하드웨어 분석부; 및상기 모델 구조 정보 및 상기 하드웨어 인식 정보 중 적어도 하나에 기초하여 상기 타겟 모델에 대하여 적용될압축 알고리즘을 결정하고, 상기 결정된 압축 알고리즘과 연계된 매개변수를 상기 모델 구조 정보 및 상기 하드웨어 인식 정보 중 적어도 하나를 고려하여 설정하는 압축 설정부,를 더 포함하는 것인, 압축 장치."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 하드웨어 분석부는,상기 컴퓨팅 장치 또는 상기 컴퓨팅 장치에 탑재되는 리소스의 유형별 추론 성능에 대한 테스트 결과를 기록한데이터 테이블을 이용하여 상기 하드웨어 인식 정보를 도출하는 것인, 압축 장치."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,공개특허 10-2024-0108223-4-상기 하드웨어 분석부는,상기 모델 구조 정보를 이용하여 상기 추론 모델에 대한 연산 융합을 수행하고, 상기 연산 융합이 적용된 연산자를 이용하여 상기 데이터 테이블을 조회하고, 상기 연산자의 수행 시간을 누적하여 상기 추론 모델과 연계된후보 모델의 추론 성능을 산출하는 것인, 압축 장치."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 하드웨어 분석부는,상기 조회하는 단계에서 상기 연산자가 상기 데이터 테이블에 적중되지 않으면, 상기 연산자와 연계된 상기 추론 성능을 출력하도록 미리 학습된 성능 예측 모델을 이용하여 상기 추론 성능을 도출하는 것인, 압축 장치."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제10항에 있어서,상기 추론 모델 및 상기 타겟 모델은 합성곱 신경망(Convolutional Neural Network, CNN) 모델을 포함하고,상기 압축 알고리즘은,프루닝(Pruning), 양자화(Quantization) 및 연산자 융합 중 적어도 하나를 포함하는 것인, 압축 장치."}
{"patent_id": "10-2023-0075414", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 프루닝은 구조적 프루닝 및 비구조적 프루닝을 포함하고,상기 압축 설정부는,상기 컴퓨팅 장치에 탑재되는 프로세싱 유닛의 유형에 따라 상기 타겟 모델의 각 브랜치에 적용되는 프루닝 기법의 유형을 선택적으로 결정하는 것인, 압축 장치."}
{"patent_id": "10-2023-0075414", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "지식 증류 기반의 인공지능 모델에 대한 압축 장치 및 방법이 개시되며, 본원의 일 실시예에 따른 지식 증류 기 반의 인공지능 모델에 대한 압축 방법은, 미리 구축된 추론 모델을 교사 모델로 설정하는 단계, 타겟 모델을 상 기 교사 모델에 대응하는 학생 모델로 설정하는 단계, 상기 추론 모델 및 상기 타겟 모델 중 적어도 하나의 구조 를 분석하여 모델 구조 정보를 도출하는 단계 및 상기 모델 구조 정보를 이용하여 상기 교사 모델을 통한 지식 증류(Knowledge Distillation)를 적용함으로써 상기 타겟 모델의 훈련을 수행하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0075414", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본원은 지식 증류 기반의 인공지능 모델에 대한 압축 장치 및 방법에 관한 것이다. 예를 들면, 본원은 지식 증 류를 통한 합성곱 신경망(Convolutional Neural Network, CNN) 모델 압축 기법에 관한 것이다."}
{"patent_id": "10-2023-0075414", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "주로 이미지 분류, 이미지 의미론적 분할 및 이미지 객체 감지 시나리오를 다루는 오픈 소스 모델에서 자동화된 압축의 효과를 확인할 수 있으며, 자동 압축은 PyTorch 및 TensorFlow에서 생성된 추론 모델에 대하여도 지원된 다. 종래의 수동 압축 기법과 비교하여 자동 압축 기법은 딥러닝 훈련 코드 분리, 오프라인 양자화된 초매개변수 검 색, 자동 알고리즘 조합 및 하드웨어 인식모델의 네가지 측면에서 자동 압축을 구현할 수 있다. 이에 따라, 사용자가 추론 모델과 레이블이 지정되지 않은 데이터만 제공하면 양적 훈련 및 희소 훈련과 같은 훈련 프로세스에 의존하는 압축 방법을 수행할 수 있고, 자동화된 압축 기능은 지식증류(Distillation) 기법을 활용하여 추론 모델에 훈련 로직을 추가할 수 있게 된다. 본원의 배경이 되는 기술은 한국등록특허공보 제10-2500341호에 개시되어 있다."}
{"patent_id": "10-2023-0075414", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본원은 전술한 종래 기술의 문제점을 해결하기 위한 것으로서, 딥러닝 모델과 결합된 데이터 테이블을 사용하여 추론 속도에 영향을 미치는 요인을 모델링하고, 결합 알고리즘의 매개변수를 설정할 수 있는 지식 증류 기반의 인공지능 모델에 대한 압축 장치 및 방법을 제공하려는 것을 목적으로 한다. 다만, 본원의 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2023-0075414", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 방법은, 미리 구축된 추론 모델을 교사 모델로 설정하는 단계, 타겟 모델을 상기 교사 모델에 대응하는 학생 모델로 설정하는 단계, 상기 추론 모델 및 상기 타겟 모델 중 적어도 하나의 구조를 분석하여 모 델 구조 정보를 도출하는 단계 및 상기 모델 구조 정보를 이용하여 상기 교사 모델을 통한 지식 증류(Knowledge Distillation)를 적용함으로써 상기 타겟 모델의 훈련을 수행하는 단계를 포함할 수 있다. 또한, 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 방법은, 상기 타겟 모델을 실행하 기 위한 컴퓨팅 장치를 분석하여 하드웨어 인식 정보를 도출하는 단계, 상기 모델 구조 정보 및 상기 하드웨어 인식 정보 중 적어도 하나에 기초하여 상기 타겟 모델에 대하여 적용될 압축 알고리즘을 결정하는 단계 및 상기 결정된 압축 알고리즘과 연계된 매개변수를 상기 모델 구조 정보 및 상기 하드웨어 인식 정보 중 적어도 하나를 고려하여 설정하는 단계를 포함할 수 있다. 또한, 상기 하드웨어 인식 정보를 도출하는 단계는, 상기 컴퓨팅 장치 또는 상기 컴퓨팅 장치에 탑재되는 리소 스의 유형별 추론 성능에 대한 테스트 결과를 기록한 데이터 테이블을 이용하여 상기 하드웨어 인식 정보를 도 출할 수 있다. 또한, 상기 하드웨어 인식 정보를 도출하는 단계는, 상기 모델 구조 정보를 이용하여 상기 추론 모델에 대한 연 산 융합을 수행하는 단계, 상기 연산 융합이 적용된 연산자를 이용하여 상기 데이터 테이블을 조회하는 단계 및 상기 연산자의 수행 시간을 누적하여 상기 추론 모델과 연계된 후보 모델의 추론 성능을 산출하는 단계를 포함 할 수 있다. 또한, 상기 하드웨어 인식 정보를 도출하는 단계는, 상기 조회하는 단계에서 상기 연산자가 상기 데이터 테이블 에 적중되지 않으면, 상기 연산자와 연계된 상기 추론 성능을 출력하도록 미리 학습된 성능 예측 모델을 이용하 여 상기 추론 성능을 도출할 수 있다. 또한, 상기 압축 알고리즘은, 프루닝(Pruning), 양자화(Quantization) 및 연산자 융합 중 적어도 하나를 포함할 수 있다. 또한, 상기 추론 모델 및 상기 타겟 모델은 합성곱 신경망(Convolutional Neural Network, CNN) 모델을 포함할 수 있다. 또한, 상기 프루닝은 구조적 프루닝 및 비구조적 프루닝을 포함할 수 있다. 또한, 상기 압축 알고리즘을 결정하는 단계는, 상기 컴퓨팅 장치에 탑재되는 프로세싱 유닛의 유형에 따라 상기 타겟 모델의 각 브랜치에 적용되는 프루닝 기법의 유형을 선택적으로 결정할 수 있다. 한편, 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 장치는, 미리 구축된 추론 모델을 교사 모델로 설정하고, 타겟 모델을 상기 교사 모델에 대응하는 학생 모델로 설정하는 모델 설정부, 상기 추론 모델 및 상기 타겟 모델 중 적어도 하나의 구조를 분석하여 모델 구조 정보를 도출하는 구조 분석부 및 상기 모 델 구조 정보를 이용하여 상기 교사 모델을 통한 지식 증류(Knowledge Distillation)를 적용함으로써 상기 타겟 모델의 훈련을 수행하는 지식 증류 적용부를 포함할 수 있다. 또한, 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 장치는, 상기 타겟 모델을 실행하 기 위한 컴퓨팅 장치를 분석하여 하드웨어 인식 정보를 도출하는 하드웨어 분석부 및 상기 모델 구조 정보 및 상기 하드웨어 인식 정보 중 적어도 하나에 기초하여 상기 타겟 모델에 대하여 적용될 압축 알고리즘을 결정하 고, 상기 결정된 압축 알고리즘과 연계된 매개변수를 상기 모델 구조 정보 및 상기 하드웨어 인식 정보 중 적어 도 하나를 고려하여 설정하는 압축 설정부를 포함할 수 있다. 또한, 상기 하드웨어 분석부는, 상기 컴퓨팅 장치 또는 상기 컴퓨팅 장치에 탑재되는 리소스의 유형별 추론 성 능에 대한 테스트 결과를 기록한 데이터 테이블을 이용하여 상기 하드웨어 인식 정보를 도출할 수 있다.또한, 상기 하드웨어 분석부는, 상기 모델 구조 정보를 이용하여 상기 추론 모델에 대한 연산 융합을 수행하고, 상기 연산 융합이 적용된 연산자를 이용하여 상기 데이터 테이블을 조회하고, 상기 연산자의 수행 시간을 누적 하여 상기 추론 모델과 연계된 후보 모델의 추론 성능을 산출할 수 있다. 또한, 상기 하드웨어 분석부는, 상기 조회하는 단계에서 상기 연산자가 상기 데이터 테이블에 적중되지 않으면, 상기 연산자와 연계된 상기 추론 성능을 출력하도록 미리 학습된 성능 예측 모델을 이용하여 상기 추론 성능을 도출할 수 있다. 또한, 상기 압축 설정부는, 상기 컴퓨팅 장치에 탑재되는 프로세싱 유닛의 유형에 따라 상기 타겟 모델의 각 브 랜치에 적용되는 프루닝 기법의 유형을 선택적으로 결정할 수 있다. 상술한 과제 해결 수단은 단지 예시적인 것으로서, 본원을 제한하려는 의도로 해석되지 않아야 한다. 상술한 예 시적인 실시예 외에도, 도면 및 발명의 상세한 설명에 추가적인 실시예가 존재할 수 있다."}
{"patent_id": "10-2023-0075414", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본원의 과제 해결 수단에 의하면, 딥러닝 모델과 결합된 데이터 테이블을 사용하여 추론 속도에 영향을 미치는 요인을 모델링하고, 결합 알고리즘의 매개변수를 설정할 수 있는 지식 증류 기반의 인공지능 모델에 대 한 압축 장치 및 방법을 제공할 수 있다. 다만, 본원에서 얻을 수 있는 효과는 상기된 바와 같은 효과들로 한정되지 않으며, 또 다른 효과들이 존재할 수 있다."}
{"patent_id": "10-2023-0075414", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본원이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본원의 실시예를 상세히 설명한다. 그러나 본원은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본원을 명확하게 설명하기 위해서 설명과 관계없는 부분 은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본원 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\" 또는 \"간접적으로 연결\"되어 있는 경우 도 포함한다. 본원 명세서 전체에서, 어떤 부재가 다른 부재 \"상에\", \"상부에\", \"상단에\", \"하에\", \"하부에\", \"하단에\" 위치 하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재가 존 재하는 경우도 포함한다. 본원 명세서 전체에서, 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 본원은 지식 증류 기반의 인공지능 모델에 대한 압축 장치 및 방법에 관한 것이다. 예를 들면, 본원은 지식 증 류를 통한 합성곱 신경망(Convolutional Neural Network, CNN) 모델 압축 기법에 관한 것이다. 도 1은 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 장치를 포함하는 인공지능 기반 의 연산 시스템의 개략적인 구성도이다. 도 1을 참조하면, 본원의 일 실시예에 따른 인공지능 기반의 연산 시스템은 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 장치(이하, '압축 장치'라 한다.) 및 컴퓨팅 장치를 포 함할 수 있다. 압축 장치 및 컴퓨팅 장치 상호간은 네트워크를 통해 통신할 수 있다. 네트워크는 단말들 및 서버들과 같은 각각의 노드 상호간에 정보 교환이 가능한 연결 구조를 의미하는 것으로, 이러한 네트워크의 일 예에는, 3GPP(3rd Generation Partnership Project) 네트워크, LTE(Long Term Evolution) 네트워크, 5G 네 트워크, WIMAX(World Interoperability for Microwave Access) 네트워크, 인터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), wifi 네트워크, 블루투스(Bluetooth) 네트워크, 위성 방송 네트워크, 아날로그 방송 네트워크, DMB(Digital Multimedia Broadcasting) 네트워크 등이 포함되나 이에 한정되지는 않는다. 컴퓨팅 장치는 예를 들면, 스마트폰(Smartphone), 스마트패드(SmartPad), 태블릿 PC등과 PCS(Personal Communication System), GSM(Global System for Mobile communication), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말기 같은 모든 종류의 무선 통신 장치일 수 있다. 한편, 도 1에는 압축 장치가 컴퓨팅 장치와 독립적으로 구비되는 것으로 도시되어 있으나, 이에만 한 정되는 것은 아니고, 본원의 구현예에 따라서 압축 장치가 컴퓨팅 장치의 하위 구성(모듈)로서 탑재 되어 컴퓨팅 장치에 구비되는 프로세싱 유닛(연산 유닛)을 이용한 타겟 모델의 경량 훈련 및 모델 압축을 적용하기 위하여 후술하는 지식 증류(Knowledge Distillation) 기법을 적용하는 형태로 본원에서 개시하는 인공 지능 기반의 연산 시스템이 설계되는 것일 수 있다. 또한, 도 1을 참조하면, 컴퓨팅 장치는 제1연산 유닛 및 제2연산 유닛을 구비할 수 있다. 보다 구 체적으로 제1연산 유닛은 CPU(Central Processing Unit)를 포함하고, 제2연산 유닛은 FPGA(Field Programmable Gate Array)를 포함하는 것일 수 있으나, 이에만 한정되는 것은 아니고, 본원의 구현예에 따라 제 1연산 유닛 및 제2연산 유닛 각각은 인공지능 모델의 학습/추론 과정에서 필요한 연산을 처리하기 위한 특성(예를 들면, 병렬 작업에 대한 적합도 등)이 상호 구분되는 다양한 프로세서, 연산 모듈 등을 폭넓게 포함 할 수 있다. 이하에서는 제1연산 유닛이 CPU에 해당하고, 제2연산 유닛이 FPGA 에 해당하는 것으로 가 정하여 본원의 실시예를 설명하도록 한다. 또한, 본원의 실시예에 관한 설명에서 인공지능 모델은 예시적으로, 합성곱 신경망(컨볼루션 뉴럴 네트워크, convolutional neural network, CNN)일 수 있으나, 이에만 한정되는 것은 아니다. 다른 예로, 인공지능 모델은 순환 신경망(RNN, Recurrent Neural Network), 심층 신뢰 신경망(DBN, Deep Belief Network), GAN(Generative Adversarial Network. 생성 대립 신경망), 관계형 신경망 네트워크(RL, Relation Networks), 심층 신경망(Deep Neural Network, DNN), 딥러닝 네트워크 등 종래에 이미 공지되었거나 향후 개발되는 다양한 인공지능 기반의 모델을 폭넓게 포함할 수 있다. 도 2는 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 장치의 동작 프로세스를 설명하 기 위한 개념도이다. 도 2를 참조하면, 압축 장치는 타겟 모델(예를 들면, 딥러닝 모델 등)의 훈련 코드를 분리하고, 오프라인 양자화된 하이퍼 파라미터 검색, 자동 알고리즘(압축 알고리즘) 조합, 하드웨어 인식 모델 등을 적용하여 사용 자가 훈련시키고자 하는 타겟 모델을 자동적으로 맞춤형 압축하도록 동작할 수 있다. 특히, 도 2를 참조하면, 압축 장치에 의해 제공되는 자동화된 압축 기능은 지식 증류(Knowledge Distillation) 기법을 통해 추론 모델에 훈련 로직을 추가함으로써 달성될 수 있다. 보다 구체적으로 본원의 일 실시예에 따르면, 압축 장치는 먼저, 사용자가 지정한 추론 모델과 연계된 파 일을 불러오고, 지식 증류(Knowledge Distillation) 기법의 적용을 위한 교사 모델(Teacher)로 메모리에 추론 모델을 복사할 수 있다. 또한, 지식 증류(Knowledge Distillation) 기법의 적용을 위한 학생 모델로서 원본 모 델에 해당하는 타겟 모델을 복사한 다음, 모델 구조를 자동으로 분석하여 증류 손실을 추가하기에 적합한 레이 어를 탐색할 수 있다. 또한, 압축 장치는 학습 가능한 매개변수가 존재하는 레이어를 탐색하고, 교사 모델 이 증류 손실을 통해 원본 모델(타겟 모델)의 양자화 훈련을 감독하는 방식으로 타겟 모델을 최적화 할 수있다. 또한, 본원의 일 실시예에 따르면, 압축을 위한 검색 시나리오에서 많은 모델과 빠른 반복 속도를 가진 오프라 인 양자화가 해당 시나리오에 가장 적합한 압축 방법으로 적용될 수 있으며, 이에 따라 압축 장치는 다양 한 오프라인 양자화 알고리즘을 구현할 수 있고, 본원의 구현예에 따라서 서로 다른 오프라인 양자화 알고리즘 을 개별적으로 적용하거나, 복수의 오프라인 양자화 알고리즘을 조합하여 타겟 모델에 대하여 적용할 수도 있다. 한편, 도 3은 지식 증류(Knowledge Distillation) 기반의 모델 훈련 방식을 설명하기 위한 개념도이다. 도 3을 참조하면, 지식 증류 기법은 더 많은 데이터로 학습한 딥러닝 모델을 교사 모델로 하여 해당 교사 모델 로부터 전달되는 정보를 활용해 학생 모델로 설정된 타겟 모델을 학습하는 기법으로서, 압축 장치는 미리 구축된 추론 모델(사용자 지정 모델)을 교사 모델로 설정할 수 있다. 또한, 압축 장치는 최적화 하고자 하 는 타겟 모델을 교사 모델에 대응하는 학생 모델로 설정할 수 있다. 또한, 압축 장치는 추론 모델 및 타겟 모델 중 적어도 하나의 구조를 분석하여 모델 구조 정보를 도출할 수 있다. 또한, 압축 장치는 압축 장치에 의해 도출된 모델 구조 정보를 이용하여 교사 모델을 통한 지식 증류 (Knowledge Distillation)를 적용함으로써 타겟 모델의 훈련을 수행할 수 있다. 도 4는 하드웨어 인식 정보 도출 프로세스를 설명하기 위한 개념도이다. 도 4를 참조하면, 압축 장치는 타겟 모델을 실행하기 위한 컴퓨팅 장치를 분석하여 하드웨어 인식 정 보를 도출할 수 있다. 구체적으로 압축 장치는 컴퓨팅 장치 또는 컴퓨팅 장치에 탑재되는 리소스의 유형별 추론 성능 에 대한 테스트 결과를 기록한 데이터 테이블을 이용하여 하드웨어 인식 정보를 도출할 수 있다. 이와 관련하여 본원의 일 실시예에 따르면, 압축 장치는 모델 구조 정보를 이용하여 추론 모델에 대한 연 산 융합을 수행할 수 있다. 또한, 압축 장치는 연산 융합이 적용된 연산자를 이용하여 데이터 테이블을 조 회할 수 있다. 또한, 압축 장치는 연산자의 수행 시간을 누적하여 추론 모델과 연계된 후보 모델 각각의 추론 성능을 산 출할 수 있다. 이와 달리, 연산자가 데이터 테이블에 적중되지 않으면, 압축 장치는 해당 연산자와 연계된 추론 성능을 출력하도록 미리 학습된 성능 예측 모델을 이용하여 추론 모델과 연계된 후보 모델 각각의 추론 성능을 도출할 수 있다. 또한, 압축 장치는 모델 구조 정보 및 하드웨어 인식 정보 중 적어도 하나에 기초하여 타겟 모델에 대하여 적용될 압축 알고리즘을 결정할 수 있다. 예시적으로, 압축 알고리즘은 프루닝(Pruning), 양자화(Quantization) 및 연산자 융합 중 적어도 하나를 포함할 수 있으나, 이에만 한정되는 것은 아니다. 예시적으로 압축 장치는 컴퓨팅 장치에 탑재되는 프로세싱 유닛의 유형에 따라 타겟 모델의 각 브랜 치에 적용되는 프루닝 기법의 유형을 구조적 프루닝 또는 비구조적 프루닝으로 선택적으로 결정할 수 있다. 또한, 압축 장치는 결정된 압축 알고리즘과 연계된 매개변수를 모델 구조 정보 및 하드웨어 인식 정보 중 적어도 하나를 고려하여 설정할 수 있다. 이와 관련하여 압축 장치에 의해 제공되는 자동 압축 기능은 모델 구조를 분석하여 사용자가 지정한 모델 구조 특성 및 배포 환경에 따라 적절한 조합 알고리즘을 자동으로 선택함으로써 달성될 수 있으며, 결합 압축 알고리즘을 선택한 후에도 각 압축 알고리즘의 매개 변수를 결정하는 것 역시 어려운 작업에 해당한다. 이러한 압축 알고리즘의 매개변수 설정은 배포 환경과 밀접한 관련이 있으며, 칩 특성 및 추론 라이브러리의 최적화 정 도 등 다양한 요소를 고려해야 하기 때문에, 본원에서 개시하는 압축 장치는 전술한 바와 같이 하드웨어 인식 모듈을 이용하여 전개 환경의 에이전트로서 전개 환경의 특성을 모델링 및 학습하고, 파라미터 설정을 위 한 성능 조회 서비스를 제공할 수 있다. 한편, 추론 라이브러리의 연산자 융합과 같은 최적화에 의해 제약을 받는 압축 매개변수와 추론 속도 간의 관계 는 선형이 아닌데, 희소성을 예로 들면 추론 라이브러리는 희소성이 75%보다 큰 행렬 곱셈 연산을 지원할 수 있다. 즉, 60% 희소성과 10% 희소성은 추론 가속 효과가 없으며, 따라서 희소성을 60%로 설정하는 것은 큰 의미가 없게 되며, 또한, 희소의 가속 효과는 행렬 곱셈 연산자의 입력 형태에 의해서도 영향을 받고, 이에 따라 결론 적으로 다양한 모델 구조와 배포 환경의 맥락에서 사람의 경험이나 간단한 공식에 의존하여 압축 매개변수와 추 론 속도 간의 관계를 정확하게 평가하는 것은 불가능하다. 따라서, 전술한 바와 같이 압축 장치에 의한 컴 퓨팅 장치의 하드웨어 지연을 추정하는 기능이 요구된다. 이러한 압축 장치의 하드웨어 지연 추정 기능은 타겟 모델(예를 들면, 딥러닝 모델 등)과 결합된 데이터 테이블을 사용하여 추론 속도에 영향을 미치는 요인을 모델링하고 결합 알고리즘의 매개변수 설정에 대한 안내 정보를 제공할 수 있으며, 이를 위하여 압축 장치는 하드웨어 지연 추정 기능의 두 가지 핵심 모듈로서 지 연 추정 테이블과 추정기(예측자)를 구비할 수 있다. 구체적으로 추정 테이블은 전개 환경별로 복수의 사업자 각각의 추론성능을 샘플링 하여 테스트하여 표 형태로 기록한 데이터 테이블이며, 이러한 데이터 테이블의 각 행에는 연산자 유형, 연산자 자체의 매개변수(예시적으 로, 입력 모양, 보폭, 패딩 등), 희소성, 양자화 등이 포함될 수 있다. 또한, 추정 테이블을 이용하여 압축 장 치는 적중된 연산자의 정보를 정확하게 추정할 수 있지만 연산자의 가능한 모든 매개 변수를 다루기는 어 려운 측면이 존재한다. 한편, 예측자(추정기)의 경우, 추정 테이블의 데이터를 사용하여 각 유형의 연산자에 대한 예측자(성능 추정 모 델)를 훈련시켜 추론 성능을 예측할 수 있는데, 이러한 예측자의 정확도는 전술한 추정 테이블만큼 높게 나타나 지는 않지만, 일반화 능력이 우수하고, 상대적으로 더 많은 연산자에 대한 매개변수 값을 획득(포함)할 수 있다 는 이점이 있다. 도 5는 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 장치의 개략적인 구성도이다. 도 5를 참조하면, 압축 장치는 모델 설정부, 구조 분석부, 지식 증류 적용부, 하드웨어 분 석부 및 압축 설정부를 포함할 수 있다. 모델 설정부는 미리 구축된 추론 모델을 교사 모델로 설정할 수 있다. 또한, 모델 설정부는 타겟 모 델을 교사 모델에 대응하는 학생 모델로 설정할 수 있다. 구조 분석부는 추론 모델 및 타겟 모델 중 적어도 하나의 구조를 분석하여 모델 구조 정보를 도출할 수 있 다. 지식 증류 적용부는 구조 분석부에 의해 도출된 모델 구조 정보를 이용하여 교사 모델을 통한 지식 증류(Knowledge Distillation)를 적용함으로써 타겟 모델의 훈련을 수행할 수 있다. 하드웨어 분석부는 타겟 모델을 실행하기 위한 컴퓨팅 장치를 분석하여 하드웨어 인식 정보를 도출할 수 있다. 구체적으로 하드웨어 분석부는 컴퓨팅 장치 또는 컴퓨팅 장치에 탑재되는 리소스의 유형별 추론 성능에 대한 테스트 결과를 기록한 데이터 테이블을 이용하여 하드웨어 인식 정보를 도출할 수 있다. 이와 관련하여 본원의 일 실시예에 따르면, 하드웨어 분석부는 모델 구조 정보를 이용하여 추론 모델에 대 한 연산 융합을 수행할 수 있다. 또한, 하드웨어 분석부는 연산 융합이 적용된 연산자를 이용하여 데이터 테이블을 조회할 수 있다. 또한, 하드웨어 분석부는 연산자의 수행 시간을 누적하여 추론 모델과 연계된 후보 모델 각각의 추론 성능 을 산출할 수 있다. 이와 달리, 연산자가 데이터 테이블에 적중되지 않으면, 하드웨어 분석부는 해당 연산자와 연계된 추론 성 능을 출력하도록 미리 학습된 성능 예측 모델을 이용하여 추론 모델과 연계된 후보 모델 각각의 추론 성능을 도 출할 수 있다. 압축 설정부는 모델 구조 정보 및 하드웨어 인식 정보 중 적어도 하나에 기초하여 타겟 모델에 대하여 적 용될 압축 알고리즘을 결정할 수 있다. 예시적으로, 압축 알고리즘은 프루닝(Pruning), 양자화(Quantization) 및 연산자 융합 중 적어도 하나를 포함할 수 있으나, 이에만 한정되는 것은 아니다. 예시적으로 압축 설정부는 컴퓨팅 장치에 탑재되는 프로세싱 유닛의 유형에 따라 타겟 모델의 각 브 랜치에 적용되는 프루닝 기법의 유형을 구조적 프루닝 또는 비구조적 프루닝으로 선택적으로 결정할 수 있다.또한, 압축 설정부는 결정된 압축 알고리즘과 연계된 매개변수를 모델 구조 정보 및 하드웨어 인식 정보 중 적어도 하나를 고려하여 설정할 수 있다. 이하에서는 상기에 자세히 설명된 내용을 기반으로, 본원의 동작 흐름을 간단히 살펴보기로 한다. 도 6은 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 방법에 대한 동작 흐름도이다. 도 6에 도시된 지식 증류 기반의 인공지능 모델에 대한 압축 방법은 앞서 설명된 압축 장치에 의하여 수행 될 수 있다. 따라서, 이하 생략된 내용이라고 하더라도 압축 장치에 대하여 설명된 내용은 지식 증류 기반 의 인공지능 모델에 대한 압축 방법에 대한 설명에도 동일하게 적용될 수 있다. 도 6을 참조하면, 단계 S11에서 모델 설정부는 미리 구축된 추론 모델을 교사 모델로 설정할 수 있다. 다음으로, 단계 S12에서 모델 설정부는 타겟 모델을 교사 모델에 대응하는 학생 모델로 설정할 수 있다. 다음으로, 단계 S13에서 구조 분석부는 추론 모델 및 타겟 모델 중 적어도 하나의 구조를 분석하여 모델 구조 정보를 도출할 수 있다. 다음으로, 단계 S14에서 지식 증류 적용부는 모델 구조 정보를 이용하여 교사 모델을 통한 지식 증류 (Knowledge Distillation)를 적용함으로써 타겟 모델의 훈련을 수행할 수 있다. 다음으로, 단계 S15에서 하드웨어 분석부는 타겟 모델을 실행하기 위한 컴퓨팅 장치를 분석하여 하드 웨어 인식 정보를 도출할 수 있다. 구체적으로 단계 S15에서 하드웨어 분석부는 컴퓨팅 장치 또는 컴퓨팅 장치에 탑재되는 리소스 의 유형별 추론 성능에 대한 테스트 결과를 기록한 데이터 테이블을 이용하여 하드웨어 인식 정보를 도출할 수 있다. 보다 구체적으로 단계 S15에서 하드웨어 분석부는 모델 구조 정보를 이용하여 추론 모델에 대한 연산 융합 을 수행할 수 있다. 또한, 단계 S15에서 하드웨어 분석부는 연산 융합이 적용된 연산자를 이용하여 데이터 테이블을 조회할 수 있다. 또한, 단계 S15에서 하드웨어 분석부는 연산자의 수행 시간을 누적하여 추론 모델과 연계된 후보 모델 각 각의 추론 성능을 산출할 수 있다. 한편, 단계 S15에서 하드웨어 분석부는 연산자가 데이터 테이블에 적중되지 않으면, 연산자와 연계된 추론 성능을 출력하도록 미리 학습된 성능 예측 모델을 이용하여 추론 모델과 연계된 후보 모델 각각의 추론 성능을 도출할 수 있다. 다음으로, 단계 S16에서 압축 설정부는 모델 구조 정보 및 하드웨어 인식 정보 중 적어도 하나에 기초하여 타겟 모델에 대하여 적용될 압축 알고리즘을 결정할 수 있다. 예시적으로, 압축 알고리즘은 프루닝(Pruning), 양자화(Quantization) 및 연산자 융합 중 적어도 하나를 포함할 수 있으나, 이에만 한정되는 것은 아니다. 구체적으로 단계 S16에서 압축 설정부는 컴퓨팅 장치에 탑재되는 프로세싱 유닛의 유형에 따라 타겟 모델의 각 브랜치에 적용되는 프루닝 기법의 유형을 구조적 프루닝 또는 비구조적 프루닝으로 선택적으로 결정 할 수 있다. 다음으로, 단계 S17에서 압축 설정부는 단계 S16에서 결정된 압축 알고리즘과 연계된 매개변수를 모델 구 조 정보 및 하드웨어 인식 정보 중 적어도 하나를 고려하여 설정할 수 있다. 상술한 설명에서, 단계 S11 내지 S17은 본원의 구현예에 따라서, 추가적인 단계들로 더 분할되거나, 더 적은 단 계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있다. 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 방법은 다양한 컴퓨터 수단을 통하여 수 행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예 에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해 서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 또한, 전술한 지식 증류 기반의 인공지능 모델에 대한 압축 방법은 기록 매체에 저장되는 컴퓨터에 의해 실행되 는 컴퓨터 프로그램 또는 애플리케이션의 형태로도 구현될 수 있다. 지금까지 상술한 본원의 일 실시예에 따른 인공지능 기반의 연산 시스템에 대한 설명은, 본원의 구현예에 따라서, 하기에서 서술하는 본원의 다른 실시예에 따른 지식증류를 통한 합성곱 신경망 모델 압축 장치에 대한 설명을 통해서 이해될 수 있다. 따라서, 이하, 생략된 내용이라고 하더라도 상술한 본원의 일 실시예에 따른 인 공지능 기반의 연산 시스템에 대하여 설명된 내용은 하기의 본원의 다른 실시예에 따른 지식증류를 통한 합 성곱 신경망 모델 압축 장치에도 동일하게 적용될 수 있다. 본원에서 개시하는 지식증류를 통한 합성곱 신경망 모델 압축 장치(미도시)(이하, '모델 압축 장치(미도시)'라 한다.)는 합성곱 신경망 기반의 모델의 구조를 분석하고, 해당 모델에 대해 연산자 융합을 수행할 수 있다. 또한, 본원에서 개시하는 모델 압축 장치(미도시)는 해당 모델과 연계된 모든 연산자에 대하여 추정 테이블을 확인할 수 있다. 또한, 모델 압축 장치(미도시)는 모든 연산자의 수행 시간을 누적하여 해당 모델의 추론 성능을 획득할 수 있다. 모델 압축 장치(미도시)는 먼저 사용자가 지정한 추론 모델 파일을 불러오고, 지식 증류의 교사(Teacher) 모델 로 메모리에 추론 모델을 복사하고, 학생(Student) 모델로 원본 모델을 복사한 다음 모델 구조를 자동으로 분석 하여 증류 손실을 추가하기에 적합한 레이어를 탐색하고, 마지막으로 학습 가능한 매개변수가 있는 레이어를 탐 색할 수 있다. 또한, 모델 압축 장치(미도시)는 교사 모델을 이용하여 증류 손실을 통해 원본 모델의 양자화 훈 련을 감독할 수 있다. 한편, 검색 시나리오에서 많은 모델과 빠른 반복 속도를 가진 오프라인 양자화가 이 시나리오에 가장 적합한 압 축 방법으로 다양한 오프라인 양자화 알고리즘으로 구현되며, 서로 다른 오프라인 양자화 알고리즘이 각각 또는 조합하여 사용할 수도 있다. 자동 압축 기능은 모델 구조를 분석하여 사용자가 지정한 모델 구조 특성 및 배포 환경에 따라 적절한 조합 알 고리즘을 자동으로 선택하는데, 결합 압축 알고리즘을 선택한 후 각 압축 알고리즘의 매개 변수를 결정하는 방 법은 또 다른 어려움이 있다. 압축 알고리즘의 매개변수 설정은 배포 환경과 밀접한 관련이 있으며 칩 특성 및 추론 라이브러리의 최적화 정 도 등 다양한 요소를 고려해야 하는데, 하드웨어 인식 모듈은 전개 환경의 에이전트로서 전개 환경의 특성을 모 델링 및 학습하고, 파라미터 설정을 위한 성능 조회 서비스를 제공하게 된다. 또한, 추론 라이브러리 연산자 융합과 같은 최적화에 의해 제약을 받는 압축 매개변수와 추론 속도 간의 관계는 선형이 아닌데, 희소성을 예로 들면 추론 라이브러리는 희소성이 75%보다 큰 행렬 곱셈 연산을 지원할 수 있다. 즉, 60% 희소성과 10% 희소성은 추론 가속 효과가 없으며, 따라서 희소성을 60%로 설정하는 것은 전혀 의미가 없게 된다. 또한, 희소의 가속 효과는 행렬 곱셈 연산자의 입력 형태에 의해서도 영향을 받는데, 결론적으로 다양한 모델 구조와 배포 환경의 맥락에서 사람의 경험이나 간단한 공식에 의존하여 압축 매개변수와 추론 속도 간의 관계를 정확하게 평가하는 것은 불가능하다. 이를 위해 하드웨어 지연 추정 기능을 필요로 하며, 이 기능은 딥러닝 모델과 결합된 데이터 테이블을 사용하여 추론 속도에 영향을 미치는 요인을 모델링하고 결합 알고리즘의 매개변수 설정에 대한 안내 정보를 제공하기 위 한 기능으로서, 이러한 하드웨어 지연 추정을 위한 두 가지 핵심 모듈은 지연 추정 테이블(Estimation Table)과 추정기(예측기)이다. 추정 테이블(Estimation Table)은 전개환경별로 다수의 사업자의 추론성능을 샘플링하여 테스트하여 데이터표에 기록한 것으로서, 데이터 테이블의 각 행에는 연산자 유형, 연산자 자체의 매개변수(예를 들면, 입력 모양 보폭, 패딩 등), 희소성, 양자화 및 기타 정보가 포함될 수 있다. 한편, 추정 테이블은 적중 연산자의 정보를 정확하게 추정할 수 있지만 연산자의 가능한 모든 매개 변수를 다루 기는 어려운 측면이 존재한다. 추정기(예측기)는 추정 테이블의 데이터를 사용하여 각 유형의 연산자에 대한 예측자를 훈련하여 추론 성능을 예측하는데, 추정기(예측기)의 정확도는 추정 테이블만큼 우수하지는 않지만, 일반화 능력이 더 강하고 더 많은 연산자 매개변수 값을 포함할 수 있다는 이점이 있다. 또한, 모델 압축 장치(미도시)는 첫 번째 단계로서, 모델 구조를 분석하고 추론 모델에 대해 배포 중에 최종적 으로 실행되는 OP를 얻기 위한 연산자(OP) 융합을 수행할 수 있다. 또한, 모델 압축 장치(미도시)는 두 번째 단계로서, 첫 번째 단계에서 생성된 추론 모델의 모든 연산자(OP)에 대해 추정 테이블을 차례로 확인하고, 추정 테이블을 적중하지 않으면 추정기(예측기)를 확인할 수 있다. 또한, 모델 압축 장치(미도시)는 세 번째 단계로서, 모든 연산자(OP)의 시간(수행 시간)을 누적하여 후보 모델 의 최종 추론 성능을 얻을 수 있다. 이렇듯, 본원에서 개시하는 모델 압축 장치(미도시)는 전술한 하드웨어 지연 추정 기능을 지원하여 다양한 압축 매개변수에서 모델 추론 성능을 빠르게 얻은 다음 특정 하드웨어에서 사용자가 지정한 추론 가속 배수에 따라 소수의 후보 모델을 잠글 수 있으며, 후보 모델의 정확도를 하나씩 검증할 수 있다. 또한, 모델 압축 장치(미도시)는 이러한 검증 후 얻어지는 후보 모델에 대하여 CNN의 CPU-FPGA Co-Design 방식 으로 병렬 연산을 하기 위해 프루닝(pruning, 가지치기) 기법으로 CNN 브랜치(Branch)를 최적화할 수 있다. 브 랜치(Branch)는 CPU와 FPGA 두가지 프로세서에 맞게 구조적 프루닝(structured pruning)과 비구조적 프루닝 (unstructured pruning)으로 각각 최적화될 수 있으며, CPU 프로세서는 임의로 CNN 가중치를 제거하는 구조화 되지 않은 가지치기에 초점을 맞출 수 있다. 한편, 구조화 되지 않은 프루닝은 평균적으로 90%가 되는 높은 희소성을 달성할 수 있는 기술로 CNN 연산이 이 루어질 때 온 칩 스토리지 요구에 맞게 리소스를 감소하는데 도움이 되나, 높은 희소성으로 인한 추가 인코딩 및 인덱싱 오버헤드, 작업 부하 불균형, 열악한 데이터 지역성 등으로 인해 반드시 고성능 속도 향상으로 이어 지지는 않는 측면이 있다. 특히, 희소성 분포가 심하게 치우칠 경우 성능이 저하되는 것으로 나타나는데, 최근 에는 특정 희소성 패턴에 따라 네트워크를 가지치기하는 것을 목표로 하는 구조적 가지치기에서 상당한 진전이 이루어졌다. 특정 수학적 속성을 따르는 엄격한 희소성 패턴으로 시작하여 필요한 수학적 변환을 지원하도록 하드웨어를 설 계하는데, 이러한 기술은 높은 규칙성과 계산 효율성을 달성할 수 있어 FPGA 하드웨어 설계에 적합하다. 또한, 컨볼루션의 일반적인 계산은 다차원 텐서(특성 맵, 가중치)를 순회하여 덧셈과 곱셈을 수행하는 계산 집 약적인 커널인데, CNN 아키텍처의 병렬성을 증가시키기 위해 CNN branch 가중치를 블록으로 재구성해야 하며, 여러 branch가 계산 중에 텐서에 동시에 각 프로세서로 액세스하여 메모리 병목 현상을 해결하고 병렬 컴퓨팅 성능 저항 문제에 도움을 줄 수 있다."}
{"patent_id": "10-2023-0075414", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본원의 설명은 예시를 위한 것이며, 본원이 속하는 기술분야의 통상의 지식을 가진 자는 본원의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본원의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본원의 범위에 포함되는 것으로 해 석되어야 한다."}
{"patent_id": "10-2023-0075414", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 장치를 포함하는 인공지능 기반 의 연산 시스템의 개략적인 구성도이다. 도 2는 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 장치의 동작 프로세스를 설명하 기 위한 개념도이다. 도 3은 지식 증류(Knowledge Distillation) 기반의 모델 훈련 방식을 설명하기 위한 개념도이다. 도 4는 하드웨어 인식 정보 도출 프로세스를 설명하기 위한 개념도이다. 도 5는 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 장치의 개략적인 구성도이다. 도 6은 본원의 일 실시예에 따른 지식 증류 기반의 인공지능 모델에 대한 압축 방법에 대한 동작 흐름도이다."}
