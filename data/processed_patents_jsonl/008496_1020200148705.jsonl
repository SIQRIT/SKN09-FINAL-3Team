{"patent_id": "10-2020-0148705", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0063803", "출원번호": "10-2020-0148705", "발명의 명칭": "사용자가 특정하는 대상을 추정하는 방법 및 장치", "출원인": "신라대학교 산학협력단", "발명자": "윤상석"}}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇 장치가 사용자가 응시 혹은 지시하는 객체를 특정하는 방법에 있어서,상기 사용자에 대한 이미지를 획득하는 단계;상기 획득된 이미지를 이용하여 상기 사용자가 특정하는 지점을 식별하는 단계;상기 식별된 지점을 상기 로봇 장치가 모션행동하도록 구동하는 단계; 및상기 식별된 지점에 위치하는 객체를 식별하는 단계; 를 포함하는 방법."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 방법은 상기 식별된 객체에 대한 오디오 컨텐츠 또는 비디오 컨텐츠를 상기 사용자에게 출력하는 단계; 를 더 포함하는 방법."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 방법은미리 설정된 모션 검출 모델을 이용하여 상기 획득된 이미지로부터 상기 사용자의 신체 특징 영역 이미지를 식별하는 단계; 및상기 식별된 신체 특징영역 이미지로부터 신체 특징점을 식별하는 단계; 및상기 식별된 신체 특징점에 기초하여 상기 사용자가 특정하는 지점을 식별하는 단계; 를 더 포함하는 방법."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 사용자에 대한 이미지를 획득하는 단계는상기 로봇 장치가 상기 로봇 장치 내 RGB 혹은 RGBD 카메라를 이용하여 상기 사용자를 촬영함으로써 RGB 혹은Depth 이미지를 획득하는 단계; 를 포함하는 방법."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서, 상기 사용자가 특정하는 지점을 식별하는 단계는상기 식별된 신체 특징점을 맵핑하는 단계;상기 맵핑된 신체 특징점에 기초하여 상기 사용자가 상기 객체를 응시 혹은 지시하는 사용자 신체 특징 벡터를식별하는 단계;상기 사용자를 응시하는 상기 로봇의 로봇 시선 벡터를 식별하는 단계; 및상기 식별된 사용자 신체 특징 벡터 및 상기 로봇 시선 벡터에 기초하여 상기 사용자가 특정하는 지점을 식별하는 단계; 를 포함하는 방법."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 사용자가 특정하는 지점을 식별하는 단계는상기 사용자 신체 특징 벡터 및 상기 로봇 시선 벡터에 대해 기하학적 연산을 수행함으로서 상기 사용자가 특정하는 지점을 식별하는 단계; 를 포함하는 방법.공개특허 10-2022-0063803-3-청구항 7 제1항에 있어서, 상기 구동하는 단계는상기 식별된 상기 사용자가 특정하는 지점 및 상기 로봇 장치의 위치 정보에 기초하여 구동 각도를 결정하는 단계; 및상기 결정된 구동 각도에 기초하여 구동하는 단계; 를 포함하는 방법."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 구동 각도를 결정하는 단계는상기 로봇 장치가 상기 사용자를 정면으로 응시하는 초기 상태를 식별하는 단계;상기 로봇 장치 내 카메라 장치로부터 상기 사용자의 특정정보가 위치하는 상대 정보의 차이 값에 대한 각도 정보를 연산하는 단계;상기 연산된 각도 정보에 기초하여 3차원 공간에서 상기 사용자가 특정하는 객체의 위치를 추정하는 단계; 및 상기 로봇 장치의 위치 정보를 상기 객체와 일치하는 방향 벡터를 추정하는 연산 모델로부터 상기 구동 각도를결정하는 단계; 를 포함하는 방법."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "사용자가 특정하는 객체를 식별하는 로봇 장치에 있어서,통신부;적어도 하나의 카메라;기 설정된 구동 축에 따라 상기 로봇 장치를 구동하는 구동부;하나 이상의 인스트럭션을 저장하는 메모리; 및상기 하나 이상의 인스트럭션을 실행하는 적어도 하나의 프로세서; 를 포함하고,상기 프로세서는 상기 하나 이상의 인스트럭션을 실행함으로써,상기 사용자에 대한 이미지를 획득하고,상기 획득된 이미지를 이용하여 상기 사용자가 특정하는 지점을 식별하고,상기 식별된 지점을 상기 로봇 장치가 모션행동하도록 구동하고,상기 식별된 지점에 위치하는 객체를 식별하는 로봇 장치."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 로봇 장치는디스플레이; 및스피커; 를 더 포함하고,상기 적어도 하나의 프로세서는 상기 디스플레이 및 상기 스피커를 제어함으로써, 상기 식별된 객체에 대한 오디오 컨텐츠 또는 비디오 컨텐츠를 상기 사용자에게 출력하는 것을 특징으로 하는 로봇 장치."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 상기 적어도 하나의 프로세서는미리 설정된 신체 검출 모델을 이용하여 상기 획득된 이미지로부터 상기 사용자의 신체 특징 영역 이미지를 식별하고,상기 식별된 특징 영역 이미지로부터 신체 특징점을 식별하고,공개특허 10-2022-0063803-4-상기 식별된 신체 특징점에 기초하여 상기 사용자가 특정하는 지점을 식별하는 로봇 장치."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서, 상기 적어도 하나의 카메라는 RGB 혹은 RGBD 카메라를 포함하고,상기 적어도 하나의 프로세서는상기 RGB 카메라를 이용하여 상기 사용자를 촬영함으로써 RGB 혹은 RGBD 이미지를 획득하는 로봇 장치."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10항에 있어서, 상기 적어도 하나의 프로세서는상기 식별된 신체 특징점을 맵핑하고,상기 맵핑된 신체 특징점에 기초하여 상기 사용자가 상기 객체를 특정하는 사용자 특징 벡터를 식별하고,상기 사용자를 응시하는 상기 로봇의 로봇 시선 벡터를 식별하고,상기 식별된 사용자 시선 벡터 및 상기 로봇 시선 벡터에 기초하여 상기 사용자가 응시하는 지점을 식별하는 로봇 장치."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 적어도 하나의 프로세서는상기 사용자 신체 특징 벡터 및 상기 로봇 시선 벡터에 대해 기하학적 연산을 수행함으로써 상기 사용자가 특정하는 지점을 식별하는 로봇 장치."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서, 상기 적어도 하나의 프로세서는상기 식별된 상기 사용자가 특정하는 지점 및 상기 로봇 장치의 위치 정보에 기초하여 구동 각도를 결정하고,상기 결정된 구동 각도에 기초하여 상기 구동부를 제어하는 로봇 장치."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서, 상기 적어도 하나의 프로세서는상기 로봇 장치가 상기 사용자를 정면으로 응시하는 초기 상태를 식별하고,상기 로봇 장치 내 카메라 장치로부터 상기 사용자의 특정한 신체정보가 위치하는 상대 정보의 차이 값에 대한각도 정보를 연산하고, 상기 연산된 각도 정보에 기초하여 3차원 공간에서 상기 사용자가 특정하는 객체의 위치를 추정하고, 상기 로봇 장치의 위치 정보를 상기 객체와 일치하는 방향 벡터를 추정하는 연산 모델로부터 상기 구동 각도를결정하는 로봇 장치."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "로봇 장치가 사용자가 특정하는 객체를 식별하는 방법에 있어서,상기 사용자에 대한 이미지를 획득하는 단계;상기 획득된 이미지를 이용하여 상기 사용자가 특정하는 지점을 식별하는 단계;상기 식별된 지점을 상기 로봇 장치가 모션행동하도록 구동하는 단계; 및상기 식별된 지점에 위치하는 객체를 식별하는 단계; 를 포함하는, 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록 매체."}
{"patent_id": "10-2020-0148705", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "공개특허 10-2022-0063803-5-제2항에 있어서, 상기 방법은상기 사용자가 상기 지점을 특정한 시간 또는 상기 특정한 시간으로부터 상기 지점에 대한 응시 또는 지시가 유지되는 시간을 식별하는 단계; 및상기 식별된 시간이 기 설정된 임계치 이상인 경우, 상기 오디오 컨텐츠 또는 상기 비디오 컨텐츠를 상기 사용자에게 출력하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일실시예는 로봇 장치가 사용자가 응시 혹은 지시하는 객체를 특정하는 방법 및 로봇 장치에 관한 것 이다. 로봇 장치가 사용자가 특정하는 객체를 식별하는 방법은 상기 사용자에 대한 이미지를 획득하는 단계; 상 기 획득된 이미지를 이용하여 상기 사용자가 특정하는 지점을 식별하는 단계; 상기 식별된 지점을 상기 로봇 장 치가 모션행동하도록 구동하는 단계; 및 상기 식별된 지점에 위치하는 객체를 식별하는 단계;를 포함하는 것을 특징으로 하는 로봇 장치가 사용자가 응시하는 객체를 식별하는 방법을 제공한다."}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 사용자 모션(지시, 응시 등)의 특징정보로부터 지시 혹은 응시방향을 추정하고, 추정된 사용자 방위 정보로부터 3차원 공간에서 사용자가 특정하는 대상을 연산하는 방법 및 그 장치에 관한 것이다."}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "눈맞춤(Eye-contact)과 공동 관심(Joint attention)은 물건이나 사건에 대한 인식을 타인과 공유하는 상호 작용 의 한 능력으로, 미국 국립질병통제예방센터(CDC)에서는 이러한 공동 관심의 결여가 자폐 범주성 장애(Autistic spectrum disorder) 아동에게 나타나는 특성 중 하나로, 다른 사람이 가리키는 것을 쳐다보지 않거나 눈맞춤을 회피하려는 경향이 있다. 치료프로토콜의 일환인 행동중재에 있어서 이러한 자폐 범주성 장애 아동의 공동 관심 및 사회적 상호작용 개선 을 위해 자폐 범주성 장애 아동의 시선을 추적하고 머무르는 시간에 대해 분석한 연구나 단순한 기능과 친근한 외형으로 자폐 범주성 장애 아동의 관심을 끌만한 로봇을 개발하여 상호 작용 개선 및 돌봄 서비스를 시도한 연 구들이 다수 진행되고 있다. 자폐 범주성 장애 아동의 중재 치료 중 공동 관심 수행 정도를 판단하기 위해서 다수의 연구에서는 카메라를 이 용한 이미지 정보와 3차원 거리(Depth)정보로부터 대상자의 헤드 포즈 추정 기술들이 개발되고 있으나, 대상자 의 시선 혹은 헤드 포즈에 기반한 방위 정보의 정확도에만 국한하는한계가 있으며, 추정된 헤드 포즈에 기초하 여 사용자가 바라보거나 지시하는 물체를 추정하거나 특정하기 위한 기술에 대한 개발이 요구되고 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제10-2016-0031183 호"}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 이루고자 하는 기술적 과제는 로봇 장치가 사용자가 응시하는 객체를 식별하는 방법 및 이를 수행하 는 로봇 장치가 제공될 수 있다. 또한, 일 실시예에 의하면 사용자가 응시하는 객체를 식별하고, 사용자 입력에 기초하여 컨텐츠를 제공하도록 하는 객체를 특정하는 방법 및 장치를 제공하는 것이다. 본 발명이 이루고자 하는 기술적 과제는 이상에서 언급한 기술적 과제로 제한되지 않으며, 언급되지 않은 또 다 른 기술적 과제들은 아래의 기재로부터 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 기술적 과제를 달성하기 위하여, 본 발명의 일 실시예는 로봇 장치가 사용자가 응시하는 객체를 식별하는 방법은 상기 사용자에 대한 이미지 혹은 거리정보를 획득하는 단계; 상기 획득된 이미지 혹은 거리정보를 이용 하여 상기 사용자가 특정하는 지점을 식별하는 단계; 상기 식별된 지점을 상기 로봇 장치가 응시하도록 구동하는 단계; 및 상기 식별된 지점에 위치하는 객체를 식별하는 단계; 를 포함할 수 있다. 또 다른 실시예에 의하면, 사용자가 응시하는 객체를 식별하는 로봇 장치는 통신부; 적어도 하나의 카메라; 기 설정된 구동 축에 따라 상기 로봇 장치를 구동하는 구동부; 하나 이상의 인스트럭션(instruction, 훈련)을 저장 하는 메모리; 및 상기 하나 이상의 인스트럭션을 실행하는 적어도 하나의 프로세서; 를 포함하고, 상기 프로세 서는 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 사용자에 대한 이미지 혹은 거리정보를 획득하고, 상 기 획득된 이미지 혹은 거리정보를 이용하여 상기 사용자가 응시 혹은 지시하는 지점을 특정하고, 상기 특정된 지점을 상기 로봇 장치가 응시하도록 구동하고, 상기 식별된 지점에 위치하는 객체를 식별할 수 있다. 또한, 또 다른 실시예에 의하면 로봇 장치가 사용자가 응시 혹은 지시하는 객체를 식별하는 방법에 있어서, 상 기 사용자에 대한 이미지 혹은 거리정보를 획득하는 단계; 상기 획득된 이미지 혹은 거리정보를 이용하여 상기 사용자가 응시하는 지점을 식별하는 단계; 상기 식별된 지점을 상기 로봇 장치가 응시하도록 구동하는 단계; 및 상기 식별된 지점에 위치하는 객체를 특정하는 단계; 를 포함하는, 방법을 컴퓨터에서 실행시키기 위한 프로그 램을 기록한 컴퓨터로 읽을 수 있는 기록 매체가 제공될 수 있다."}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예에 따르면, 사용자의 응시 혹은 지시방향 특정하여 사용자가 원하는 컨텐츠를 효과적으로 제공 할 수 있다."}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 상기한 효과로 한정되는 것은 아니며, 본 발명의 상세한 설명 또는 특허청구범위에 기재된 발 명의 구성으로부터 추론 가능한 모든 효과를 포함하는 것으로 이해되어야 한다."}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참조하여 본 발명을 설명하기로 한다. 그러나 본 발명은 여러 가지 상이한 형태로 구 현될 수 있으며, 따라서 여기에서 설명하는 실시예로 한정되는 것은 아니다. 그리고 도면에서 본 발명을 명확하 게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사 한 도면 부호를 붙였다. 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결(접속, 접촉, 결합)\"되어 있다고 할 때, 이는 \"직접적으로 연 결\"되어 있는 경우뿐 아니라, 그 중간에 다른 부재를 사이에 두고 \"간접적으로 연결\"되어 있는 경우도 포함한다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 구비할 수 있다는 것을 의미한다. 본 명세서에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도 가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들 을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요 소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 이하 첨부된 도면을 참고하여 본 발명의 실시예를 상세히 설명하기로 한다. 도 1은 일 실시예에 따라 로봇 장치가 사용자가 응시 혹은 지시하는 객체를 특정하는 방법의 흐름도이다. 일 실시 예에 의하면 후술하는 도 9에 따른 로봇 장치는 로봇 장치의 사용자를 모니터링함으로써 사용자가 응시 혹은 지시하는 지점을 결정할 수 있다. 예를 들어, 로봇 장치는 사용자의 얼굴의 자세(POSE)를 추정하고, 추정 된 자세에 기초하여 사용자가 응시하는 방위정보 혹은 3차원 지점을 결정할 수 있다. 본 개시에 따른 로봇 장치 는 사용자가 응시 혹은 지시하는 지점을 특정하고, 해당 지점을 모니터링함으로써 3차원 공간내 특정된 지점에 위치하는 객체를 식별할 수 있다. 또 다른 실시 예에 의하면 로봇 장치는 사용자가 응시 혹은 지시하는 지점을 특정하고, 특정된 지점에 따른 임의 차원의 공간 내 위치하는 객체를 식별할 수도 있다. 일 실시 예에 의하면 임의 차원의 공간은 로봇 장치가 위치하는 현실 공간 또는 컴퓨터 데이터 공간 내 임의 차원의 공간을 포함할 수도 있다. 본 개시의 일 실시 예에 따른 로봇 장치가 사용자의 지시 방향을 특정하는 동작은 사용자 입력에 따른 사용자 시선, 사용자의 바디가 향하는 방향, 사용자가 특정하고자 하는 객체를 지시를 식별하는 동작을 포함할 수 있다. 일 실시 예에 의하면 로봇 장치가 사용자의 지시 방향을 특정하는 동작은, 사용자의 지시 방향을 결정하 는 동작 또는 사용자의 지시 방향을 식별하는 동작을 포함할 수 있다. 또한, 본 개시의 일 실시 예에 의하면 로봇 장치가 사용자 응시 혹은 지시 지점을 식별하는 동작은 로봇 장치가 사용자 특정 정보를 식별하는 동작에 대응될 수 있다. 로봇 장치는 해당 객체를 특정한 결과를 사용자에게 제공 할 수 있다. 일 실시예에 의하면 로봇 장치는 사용자 음성 입력을 획득하고, 사용자 음성 입력에 기초하여 객체 식별 결과를 사용자에게 제공할 수 있다. 예를 들어 로봇 장치는 카메라를 이용하여 RGB 혹은 Depth 이미지를 획득하고, RGB 혹은 Depth 이미지로부터 신체의 특징정보를 추출하고 그 상관관계의 연산으로부터 헤드 혹은 지 시형 제스처의 포즈를 추출하며, 기하학 공간 정보로부터 사용자가 특정하는 방위 정보의 공간 내 목표 대상 추 정할 수 있는 연산 모델을 포함할 수 있다. 이하에서는 도 1을 참조하여 로봇 장치가 사용자가 응시 혹은 지시 하는 객체를 특정하는 방법을 설명하기로 한다. S110에서, 로봇 장치는 사용자에 대한 이미지 혹은 거리정보를 획득한다. 예를 들어, 로봇 장치는 카메라를 이 용하여 사용자를 촬영함으로써 사용자에 대한 이미지를 획득할 수도 있다. 또 다른 실시 예에 의하면, 로봇 장 치는 사용자에 대한 이미지에 더하여 거리정보를 더 획득할 수도 있다. 예를 들어, 로봇 장치는 RGB 혹은 Depth 카메라를 이용하여 사용자를 촬영함으로써 사용자에 대한 RGB 혹은 Depth 이미지를 획득할 수 있다. S120에서, 로봇 장치는 획득된 이미지를 이용하여 사용자가 응시 혹은 지시하는 지점을 특정할 수 있다. 또 다 른 실시 예에 의하면, 로봇 장치는 획득된 이미지를 이용하여 사용자가 특정하는 지점을 식별할 수 있다. 예를 들어, 로봇 장치는 이미지 내 사용자의 신체 영역을 식별하고, 식별된 신체 특징 영역 내 신체 특징점으로부터 벡터 연산을 포함하는 기하학적 연산을 수행함으로써 3차원 공간에서 사용자가 응시 혹은 지시하는 지점을 특정 할 수 있다. 예를 들어, 로봇 장치는 맵핑된 얼굴 특징점에 기초하여 사용자가 객체를 응시하는 사용자 시선 벡 터를 식별할 수 있다. 또한 로봇 장치는 사용자를 응시하는 로봇의 시선 벡터를 식별하고, 상기 사용자 시선 벡 터 및 로봇 시선 벡터에 기초하여 사용자가 응시하는 지점을 식별할 수도 있다. S130에서 로봇 장치는 S120에서 식별된 사용자가 응시하는 지점을 로봇 장치 내 장착된 카메라가 추적 응시하도 록 구동할 수 있다. 예를 들어, 로봇 장치는 로봇 장치 또는 로봇 장치 내 카메라를 구동하기 위한 적어도 하나 의 구동부를 포함할 수 있고, 구동부를 이용하여 로봇 장치가 사용자가 응시하는 지점을 이동하거나 바라보도록 제어할 수 있다. 예를 들어 로봇 장치는 사용자가 응시 혹은 지시하는 지점 및 상기 로봇 장치의 위치 정보에 기초하여 구동 위 치와 각도를 결정하고, 결정된 구동 위치와 각도에 기초하여 작동할 수 있다. 예를 들어, 로봇 장치는 로봇 장 치가 상기 사용자를 정면으로 응시하는 초기 상태를 식별하고, 상기 로봇 장치 내 카메라 장치로부터 상기 사용 자의 얼굴이 위치하는 상대 정보의 차이 값에 대한 각도 정보를 연산하며, 상기 연산된 각도 정보에 기초하여 3 차원 공간에서 상기 사용자가 응시하는 객체의 위치를 추정할 수 있다. 로봇 장치는 로봇 장치의 위치 정보를 상기 객체와 일치하는 방향 벡터를 추정하는 연산 모델로부터 상기 구동 각도를 결정할 수 있다. 로봇 장치는 상기 결정된 구동 각도에 따라 구동함으로써 사용자가 응시하는 대상 객체를 함께 응시할 수 있다.일 실시 예에 의하면 로봇 장치가 구동부를 이용하여 구동하는 동작은 로봇 장치가 모션행동하는 동작을 포함할 수 있다. 로봇 장치가 모션행동 하는 동작은 로봇 장치가 사용자 특정 정보에 따른 지점을 바라보는 동작, 로봇 장치가 해당 지점으로 이동하는 동작, 로봇 장치를 해당 지점을 바라보도록 회전하는 동작 중 적어도 하나를 포 함할 수 있다. 그러나, 이에 한정되는 것은 아니며, 기타 로봇 장치의 모션에 관련된 기타 동작들을 더 포함할 수도 있다. S140에서, 로봇 장치는 로봇 장치의 구동에 따라 로봇 장치가 사용자가 응시 혹은 지시하는 지점을 바라보도록 하고, 해당 지점에 위치하는 객체를 특정할 수 있다. 예를 들어, 로봇 장치는 객체 식별을 위한 이미지 또는 영 상 처리에 적합한 인공 지능 모델을 포함할 수 있고, 인공 지능 모델을 이용하여 사용자가 응시하는 지점에 위 치하는 객체를 식별할 수 있다. 도 1에는 도시되지 않았지만, 로봇 장치는 식별된 객체에 대한 오디오 컨텐츠 또는 비디오 컨텐츠를 사용자에게 출력할 수도 있다. 예를 들어, 상술한 바와 같이 로봇 장치는 사용자가 특정하는 지점을 식별하고, 식별된 지점 에 위치하는 객체를 식별하며, 식별된 객체에 대한 정보를 오디오 컨텐츠 또는 비디오 컨텐츠로 출력할 수 있다. 일 실시 예에 의하면, 로봇 장치는 사용자가 특정 지점을 응시하기 시작하는 시간을 미리 저장하고, 미리 저장된 시간으로부터 사용자가 특정 지점을 응시하는데 걸리는 시간을 측정할 수 있다. 로봇 장치는 사용 자가 특정 지점을 응시하기 시작한 시점부터, 특정 지점에 대한 응시를 유지하는 시간이 기 설정된 임계치 이상 인 경우, 해당 지점에 위치하는 객체에 대한 정보를 컨텐츠로 출력할 수도 있다. 보다 상세하게는, 로봇 장치 는 사용자가 특정 지점을 응시하는 시간이 기 설정된 임계치 이상인 경우 트리거 정보를 생성하고, 생성 된 트리거 정보에 따라 객체에 대한 컨텐츠를 출력할 수도 있다. 예를 들어, 로봇 장치는 상기 사용자가 상기 지점을 특정한 시간 또는 상기 특정한 시간으로부터 상기 지 점에 대한 응시 또는 지시가 유지되는 시간을 식별하고, 상기 식별된 시간이 기 설정된 임계치 이상인 경우, 상 기 오디오 컨텐츠 또는 상기 비디오 컨텐츠를 상기 사용자에게 출력할 수도 있다. 또는 로봇 장치는 상기 사용자가 상기 지점을 특정한 시간 및 상기 특정한 시간으로부터 상기 지점에 대한 응시 또는 지시가 유지되는 시간을 식별하고, 상기 식별된 유지되는 시간이 기 설정된 임계치 이상인 경우 오디오 컨텐츠 또는 비디오 컨텐 츠 중 적어도 하나를 사용자에게 출력할 수도 있다. 또 다른 실시 예에 의하면, 로봇 장치는 사용자의 음성 입력을 획득하고, 획득된 음성 입력에 기초하여 사용자가 특정하는 지점을 식별하고, 식별된 지점에 위치하는 객체를 식별하며, 식별된 객체에 대한 정보를 컨 텐츠로 제공할 수 있다. 예를 들어, 로봇 장치는 사용자로부터 \"왼쪽에 저 물건은 뭐야?\"라는 음성을 획 득하고, 획득된 음성을 분석한 결과, 사용자로부터 왼쪽 방향에 대응되는 특정 지점에 대한 정보를 식별할 수 있다. 로봇 장치는 사용자로부터 왼쪽 방향에 대응되는 특정 지점에 대한 정보에 기초하여 모션행동한 후, 해당 지점에 위치하는 객체에 대한 정보를 출력할 수도 있다. 또 다른 실시 예에 의하면 로봇 장치는 이미지를 분석함으로써 획득되는 특정 지점에 대한 정보 및 사용 자 음성 입력 분석 결과 모두를 이용하여 사용자가 특정하는 지점 또는 사용자 특정하는 지점에 위치하는 객체 를 식별하고, 식별된 객체에 대한 컨텐츠를 제공할 수도 있다. 예를 들어, 로봇 장치는 사용자로부터 \"왼 쪽에 저 물건은 뭐야?\"라는 음성 입력이 인식되고, 사용자가 특정 지점을 응시하는 시간이 기 설정된 임계치 이 상인 경우에만, 해당 지점에 위치하는 객체에 대한 컨텐츠를 출력할 수도 있다. 또 다른 실시 예에 의하면 로봇 장치는 사용자가 로봇 장치에 대하여 전송하는 기타 사용자 입력에 기초하여 오 디오 컨텐츠 또는 비디오 컨텐츠를 출력할 수도 있다. 또 다른 실시 예에 의하면, 로봇 장치는 \"이거 뭐야?, \"내 이름이 뭐야?\" 또는 \"이름을 불러 줄래?\"와 같 은 음성을 출력하고, 출력된 음성에 기초하여 사용자로부터 획득되는 사용자 음성 입력에 대응되는 컨텐츠를 출 력할 수도 있다. 예를 들어, 로봇 장치는 \"내 이름을 불러 줄래\"라는 오디오 음성을 스피커를 통해 출력 하고, 미리 설정된 시간 동안, 기 저장된 로봇 호칭에 관한 사용자의 음성 입력이 식별되는지 여부를 분석할 수 있다. 로봇 장치는 기 설정된 시간 동안 사용자 음성 입력이 획득되지 않거나, 로봇 호칭에 매칭되지 않 는 사용자의 음성 입력이 식별되는 경우 \"이름을 불러 줄래?\"와 같은 오디오 신호를 다시 출력할 수도 있다. 로봇 장치는 \"이름을 불러 줄래?\"와 같은 출력 오디오 신호에 대한 응답으로, 사용자로부터 로봇 호칭에 관한 사용자 음성이 식별되는 경우 \"참 잘했어\"와 같은 칭찬에 관한 오디오 신호를 출력하거나, 또 다른 컨텐 츠 학습을 시작하기 위한 일련의 컨텐츠들을 출력할 수 있다. 본 개시에 따른 로봇 장치가 사용자에게 선컨텐츠를 제공한 후, 제공된 컨텐츠에 대해 매칭되는 사용자 입력을 획득하는 동작은 조인트 어텐션(joint attention) 알고리즘에 의해 수행될 수 있다. 본 개시에 따른 로봇 장치는 사용자로부터 음성 입력이 획득되기 전 \"이거 뭐야?\" \"내 이름이 뭐야?\" \"넌 누구니?\"와 같은 오디오 컨텐츠를 출력한 후, 출력된 오디오 신호에 대하여 매칭되는 사용자 음성 입력에 기초 하여 오디오 컨텐츠를 출력할 수도 있지만, 또 다른 실시 예에 의하면, 로봇 장치는 사용자 음성 입력 및 사용자로부터 획득되는 생체 정보를 더 이용함으로써 사용자의 모션을 모니터링하고, 모니터링 결과에 기초하여 소정의 컨텐츠들을 출력할 수도 있다. 예를 들어, 로봇 장치는 \"이거 뭐야?\"라는 오디오를 출력 한 후, 출력된 오디오에 대한 응답으로 \"강아지 야\" 라는 사용자 음성 입력을 획득함과 함께 사용자가 착용한 웨어러블 디바이스로부터 사용자 생체 정보들을 획득할 수도 있다. 일 실시 예에 의하면 로봇 장치가 네트워크 인터페이스를 통하여 사용자 웨어러블 디 바이스로부터 획득하는 생체 정보는 근전도 정보, 심박수 정보 등을 포함할 수도 있다. 로봇 장치는 \"강아지야\"라는 사용자 음성 입력이 식별되는 경우, \"강아지야\"라는 사용자 음성 입력에 대 한 피드백으로 \"참 잘했어\" 와 같은 오디오 컨텐츠를 출력할 수 있다. 그러나 또 다른 실시 예에 따른 로봇 장 치는 \"강아지야\"라는 사용자 음성 입력이 식별되고, 획득된 생체 정보에 따른 스트레스 지수를 심박수, 피부 전도도 등의 생체 신호가 소정의 임계치 이상인 경우에만 \"참 잘했어\"와 같은 칭찬에 관한 컨텐츠를 출력 하거나 이미 출력된 컨텐츠를 변환할 수도 있다. 일 실시 예에 의하면, 로봇 장치는 사용자가 응시하는 지점의 객체를 식별하고, 식별된 결과를 시각적 또는 청 각적 정보로서 제공할 수 있다. 따라서, 로봇 장치는 사용자가 ADHD, 자폐아동과 같은 장애를 가진 경우, 사용 자와의 사회적 상호 작용을 통하여 사용자의 행동장애를 중재하거나 완화시킬 수 있다. 또한, 로봇 장치는 사용 자의 음성 입력을 획득하고, 획득된 음성 입력에 음성 분석을 수행함으로써 사용자 스스로 로봇 장치를 이용하 여 치료 프로그램을 수행하도록 할 수 있다. 또 다른 실시 예에 의하면, 로봇 장치는 사용자에게 스트레스를 주기 위한 스트레스에 관한 컨텐츠들을 저장해두고, 저장된 스트레스에 관한 컨텐츠를 제공함으로써 사용자에게 스트레스를 유발할 수 있다. 로봇 장치 는 스트레스에 관한 컨텐츠를 사용자에게 제공한 후, 사용자의 생체 정보를 모니터링함으로써 사용자의 신체 변화를 모니터링하고, 모니터링 결과에 따른 신체 변화 정보를 시간의 흐름에 따라 누적하여 저장할 수 있 다. 로봇 장치는 시간의 흐름에 따라 누적하여 저장된 사용자의 신체 변화 정보를 의료 기관의 서버 장치 또는 의료 전문 인력이 사용하는 단말로 전송할 수 있다. 로봇 장치가 시간의 흐름에 따라 누적하여 저장 한 사용자의 신체 변화 정보는 자폐아동 또는 행동 장애를 가진 사용자 치료에 대한 정량적인 평가의 지표로 활 용될 수도 있다. 도 2는 일 실시예에 따라 로봇 장치가 사용자가 응시 혹은 지시하는 지점을 특정하는 구체적인 방법의 흐름도이 다. S210에서, 로봇 장치는 미리 설정된 인간의 신체 검출 모델을 이용하여 획득된 이미지 혹은 거리정보로부터 사 용자의 특정 신체 영역을 식별할 수 있다. 예를 들어, 로봇 장치는 RGB 혹은 Depth 이미지를 획득한 후, Open CV(Open Source Computer Vision Library) 와 Dlib을 이용하여 사용자의 얼굴 영역을 검출할 수 있다. 일 실시 예에 의하면 로봇 장치가 이용하는 Open CV(Open Source Computer Vision Library) 및 Dlib는 컴퓨터 비전 알 고리즘에 관한 프로그래밍 라이브러리로 얼굴 검출 모델은 프로그래밍 라이브러리 내 인스트럭션을 수행함으로 써 생성되는 모델일 수 있다. 또한, Openface 등의 이미지로부터 얼굴 특징정보를 생성할 수도 있다. S220에서, 로봇 장치는 식별된 인간의 신체 영역 이미지로부터 특징점을 식별할 수 있다. 예를 들어, 로봇 장 치는 미리 설정된 특정영역 혹은 모션 검출 모델을 이용하여 이미지 내 픽셀의 특징정보 혹은 변화량을 식별하 고, 식별된 픽셀 정보에 기초하여 신체 영역 이미지로부터 신체 방위정보와 같은 특징점을 추출할 수 있다. S230에서, 로봇 장치는 특정된 인간의 신체 특징점에 기초하여 사용자가 응시 혹은 지시하는 지점을 식별할 수 있다. 또는 로봇 장치는 신체 특징점에 기초하여 사용자가 특정하는 지점에 대한 특징 정보(예컨대 방위각)를 획득할 수 있다. 예를 들어, 로봇 장치는 얼굴 이미지로부터 식별된 신체 특징점에 기초하여 사용자의 헤드 움 직임에 대한 기하학적(translation vector, rotation vector)정보를 통해 사용자의 목표 응시 혹은 지시 지점 을 연산할 수 있다. 또 다른 실시 예에 의하면 로봇 장치는 신체 특징점에 기초하여 사용자의 신체 움직임에 대 한 기하학적 정보를 통해 사용자의 바디가 향하는 지점을 연산할 수도 있다. 로봇 장치는 사용자의 목표 지점이 연산되면 로봇 장치의 구동부를 제어함으로써 사용자의 목표 지점을 동시에 바라보거나 이동하도록 제어할 수 있다. 또는 로봇 장치는 사용자가 특정하는 지점이 식별되면 로봇 장치의 구동부를 이용하여 모션 행동할 수 있다. 도 3은 일 실시예에 따라 로봇 장치가 사용자가 응시 혹은 지시하는 지점을 식별하는 구체적인 방법의 흐름도 이다. S310에서, 로봇 장치는 신체의 특정 영역 이미지로부터 신체 특징점을 추출하고, 추출된 특징점을 맵핑할 수 있다. 일 실시예에 의하면 로봇 장치는 추출된 특징점들의 기하학적 관계에 기초하여 벡터를 생성하기 위해 적 어도 하나의 특징점들을 맵핑할 수 있다. 일 실시 예에 의하면 특징점들을 맵핑하는 과정은 기하학적 관계에 따 라 정의되는 3차원 공간 맵에 특징점들을 배치하는 과정을 포함할 수 있다. S320에서, 로봇 장치는 맵핑된 얼굴 특징점에 기초하여 사용자가 객체를 특정하는 사용자 시선 벡터를 식별할 수 있다. S330에서, 로봇 장치는 사용자가 특정하는 로봇의 로봇 시선 벡터를 식별할 수 있다. 로봇 장치가 사 용자 신체 특징 벡터 및 로봇 시선 벡터를 식별하는 과정은 후술하는 도 5를 참조하여 구체적으로 설명하기로 한다. S340에서, 로봇 장치는 식별된 사용자 신체 특징 벡터 및 로봇 시선 벡터에 기초하여 사용자가 특정하는 지점 을 식별할 수 있다. 예를 들어, 로봇 장치는 기하학적 수학 모델에 기초하여 사용자 신체 특징 벡터 및 로봇 시 선 벡터에 대해 벡터 연산 또는 기하학적 연산을 수행함으로써 사용자가 특정하는 지점을 식별할 수 있다. 도 4는 일 실시예에 따라 좌표 변환에 따른 사용자가 특정하는 지점의 연산 결과 값을 3차원 공간에 나타낸 도 면이다. 로봇 장치는 사용자 얼굴에 대한 이미지 혹은 거리정보로부터 신체 특징영역을 검출하고, 특징 영역에서 추출 된 신체 특징점들을 공간상에 맵핑함으로써 목표점의 연산 결과 값을 3차원 공간에 나타낼 수 있다. 예를 들어 로봇 장치는 카메라 장치, 사용자 및 대상 객체를 3차원 공간상에 링크로 표시하고, 링크를 기준으로 연산 결과 값들을 벡터로 표시할 수 있다. 도 5는 일 실시 예에 따라 사용자가 응시 혹은 지시하는 하는 타겟 지점에 대한 기하학적 관계를 나타내는 도면 이다. 도 5를 참조하면 로봇 장치 내 카메라 장치의 사용자에 대한 로봇 시선 벡터 W , 대상 객체에 대한 사용자의 사용자 시선 벡터 uㅇS , 로봇 장치 내 카메라 장치의 대상 객체에 대한 시선 추정 벡 터 v 및 카메라 장치, 사용자 및 대상 객체를 포함하는 평면에 수직한 수직 벡터이 도시된 다. 일 실시 예에 의하면 사용자 시선 벡터는 사용자의 신체 특징 벡터에 대응될 수도 있다. 일 실시 예에 의하면, 카메라 장치는 2차원 카메라, 3차원 카메라, RGB-Depth를 포함하는 기타 카메라 장치일 수 있다. 로봇 장치는 도 5에 도시된 바와 같이 기하학적 수학 모델을 생성하고, 상기 생성된 수학 모델에 하기의 수학 식을 적용함으로써 사용자의 응시 지점을 식별할 수 있다. 수학식 1"}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기에서 n은 상술한 수직 벡터 이고, w는 사용자에 대한 로봇 시선 벡터이며, u 및 S는 대상 객체에 대한 사용자의 사용자 시선 벡터를 구성하는 벡터 요소이다. 수학식 2"}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기에서 V는 카메라 장치의 대상 객체에 대한 시선 추정 벡터 이고, w 및 uㅇS는 상술한 바와 같다. 도 6은 일 실시 예에 따라 로봇 장치 주변의 실험 환경의 구성을 나타내는 도면이다. 일 실시예에 의하면 로봇 장치 주변의 환경은 로봇 장치, 사용자, 특정 객체 A, 특정 객체 B 및 특정 객체 C를 포함할 수 있다. 도 6에 도시된 로봇 장치 주변의 환경 구성을 참조하여 로봇 장 치가 사용자가 특정하는 타겟을 추적하는 메커니즘을 설명하기로 한다. 사용자가 특정하는 대상 객체의 좌표와 로봇 장치의 좌표를 하기의 수학식에 대입하여 로봇 장치의 구동 각도 를 결정한다. 일 실시 예에 의하면 구동 각도는 Pan-tilt 각도를 포함할 수 있다. 수학식 3"}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기에서 v1은 로봇 장치가 정면으로 사용자를 응시하는 초기 설정 상태 벡터를 나타낸다. 수학식 4"}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기에서 v2는 로봇 장치가 대상 객체 A를 특정할 경우 로봇 장치의 로봇 시선 벡터를 나타낸다. 수학식 5"}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기에서 은 로봇 장치 내 카메라로부터 사용자가 특정하는 위치하는 상대 정보의 차이 값에 대한 각도를 나타낸다. v1 및 v2는 상기 수학식 3 내지 4에서 상술한 바와 같다. 로봇 장치는 수학식 5를 v1 및 v2 벡터에 적용함으로써 pan의 각도를 결정할 수 있다. 로봇 장치는 카메라 장치와 pan-tilt 모듈 간에 d 만큼의 위치 오차에 따라 하기 도 7과 같은 기하학적 연산 모델을 적용함으로써 타겟 대상 객체의 위치 응시를 수행할 수 있다. 수학식 6"}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "수학식 7"}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "로봇 장치는 하기 도 7과 같은 기하학적 연산 모델에 기초하여, 상기 수학식 6 내지 7을 적용함으로써 tilt 각 도를 결정할 수 있다.수학식 8"}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기에서 는 로봇 장치 내 카메라로부터 사용자가 특정정보로부터 위치하는 상대 정보의 차이 값에 대한 각 도를 나타낸다. 또한 로봇 장치가 결정한 pan-tilt 각도 정보는 상기 수학식 8을 통해 회전 방향을 결정할 수 있다. 즉, 로봇 장치는 로봇 장치 내 카메라로부터 사용자 특정정보로부터 위치하는 상대 정보의 차이 값에 대한 각도 정보 및 를 연산하여 3차원 공간에서의 기하학적 타겟의 위치 좌표를 추정할 수 있다. 그런 다음 로봇 장치는 pan-tilt구동 가능한 로봇의 헤드 위치 정보를 타겟과 일치하는 방향 벡터를 추정하는 연산 모델로부터 로봇의 pan-tilt 값을 결정할 수 있다. 도 7은 일 실시 예에 따라 로봇 장치가 사용자 응시 지점을 추적하는 구성을 나타내는 도면이다. 도 7을 참조하면 로봇 장치가 응시 지점에 위치하는 타겟을 추적하는 기하학적 구성을 관측할 수 있다. 로봇 장 치는 x,y 좌표상에 위치하는 대상 객체(x, y), 로봇의 팬 틸트 각도(x,y) 및 eye pose를 공간상에 맵핑함으로써 기하학적 연산 모델을 생성하고, 상기 수학식 6 내지 7을 적용함으로써 tilt 각도 또는 pan 각도 중 적어도 하 나를 결정할 수 있다. 도 8은 일 실시 예에 따라 로봇 장치가 추정된 사용자 특정정보에 기초하여 객체를 응시하는 동작을 설명하기 위한 도면이다. 일 실시예에 따라 로봇 장치의 동작 예를 설명한다. 로봇 장치는 사용자의 특정 지점을 식별하고, 식 별된 사용자의 특정 지점에 위치하는 대상 객체(812, 814, 816)을 응시할 수 있다. 로봇 장치는 사용자가 특정하는 지점을 응시 한 후, 카메라를 통하여 획득되는 영상정보 내 대상 객체의 종류를 식별할 수 있다. 로봇 장치는 식별되는 대상 객체의 종류에 대한 정보를 출력할 수 있다. 도 9는 일 실시 예에 따른 로봇 장치의 블록도이다. 일 실시예에 의하면, 로봇 장치는 프로세서, 디스플레이, 스피커, 구동부, 카메라 및 메모리를 포함할 수 있다. 그러나 또 다른 실시 예에 의하면 로봇 장치는 상술한 구성에 더하여 네트워크 인터페이스 및 센싱부를 더 포함할 수도 있다. 예를 들어, 프로세서는 메모리 에 저장된 인스트럭션을 실행함으로써 사용자에 대한 이미지 혹은 거리정보를 획득하고, 상기 획득된 이미지 혹 은 거리정보를 이용하여 상기 사용자가 특정하는 지점을 식별하고, 상기 식별된 지점을 상기 로봇 장치가 응시 하도록 구동하고, 상기 식별된 지점에 위치하는 객체를 식별할 수 있다. 일 실시예에 의하면 프로세서는 디스플레이 및 상기 스피커를 제어함으로써, 상기 식별된 객체에 대한 오 디오 컨텐츠 또는 비디오 컨텐츠를 상기 사용자에게 출력할 수 있다. 일 실시예에 의하면 프로세서는 미리 설정된 얼굴 검출 모델을 이용하여 상기 획득된 이미지로부터 상기 사용자의 얼굴 영역 이미지를 식별하고, 상기 식별된 얼굴 영역 이미지로부터 얼굴 특징점을 식별하고, 상기 식 별된 얼굴 특징점에 기초하여 상기 사용자가 특정하는 지점을 식별할 수 있다. 일 실시예에 의하면 프로세서는 RGB 카메라를 이용하여 상기 사용자를 촬영함으로써 RGB 혹은 Depth 이미 지를 획득할 수 있다. 일 실시예에 의하면 프로세서는 식별된 신체 특징점을 맵핑하고, 상기 맵핑된 신체 특징점에 기초하여 상 기 사용자가 상기 객체를 특정하는 사용자 벡터를 식별하고, 상기 사용자를 응시하는 상기 로봇의 로봇 시선 벡 터를 식별하고, 상기 식별된 사용자 시선 벡터 및 상기 로봇 시선 벡터에 기초하여 상기 사용자가 응시하는 지 점을 식별할 수 있다. 일 실시예에 의하면 프로세서는 상기 사용자 특징 벡터 및 상기 로봇 시선 벡터에 대해 기하학적 연산을 수행함으로써 상기 사용자가 특정하는 지점을 식별할 수 있다.일 실시예에 의하면 프로세서는 상기 식별된 상기 사용자가 특정하는 지점 및 상기 로봇 장치의 위치 정보 에 기초하여 구동 각도를 결정하고, 상기 결정된 구동 각도에 기초하여 상기 구동부를 제어할 수 있다. 일 실시예에 의하면 프로세서는 상기 사용자를 정면으로 응시하는 초기 상태를 식별하고, 상기 로봇 장치 내 카메라 장치로부터 상기 사용자의 특정정보가 위치하는 상대 정보의 차이 값에 대한 각도 정보를 연산하고, 상기 연산된 각도 정보에 기초하여 3차원 공간에서 상기 사용자가 특정하는 객체의 위치를 추정하고, 상기 로봇 장치의 위치 정보를 상기 객체와 일치하는 방향 벡터를 추정하는 연산 모델로부터 상기 구동 각도를 결정할 수 있다. 일 실시 예에 의하면, 구동부는 로봇 장치의 모션, 이동, 회전을 위한 적어도 하나 이상의 구동 모듈을 포 함할 수 있다. 구동부는 로봇 장치가 결정한 각도 정보에 기초하여 결정된 특정 지점을 향하도록 로봇 장 치가 모션행동하도록 할 수 있다. 일 실시 예에 의하면 센싱부는 적어도 하나의 센서를 포함할 수 있다. 일 실시 예에 의하면, 센싱부 는 사용자의 생체 신호를 측정하기 위한 생체 신호 측정 센서를 포함할 수 있다. 일 실시 예에 의하면 생체 신 호 측정 센서(미도시)는 피부전도도 센서, 근전도 센서, 심전도 센서를 포함할 수 있다. 또한, 일 실시 예에 의하면 센싱부는 생체 신호 측정 센서에 더하여 지자기 센서(Magnetic sensor), 가속 도 센서(Acceleration sensor), 온/습도 센서, 적외선 센서, 자이로스코프 센서, 위치 센서(예컨대, GPS), 기압 센서, 근접 센서, 및 RGB 센서(illuminance sensor) 중 적어도 하나를 포함할 수 있으나, 이에 한정되는 것은 아니다. 각 센서들의 기능은 그 명칭으로부터 당업자가 직관적으로 추론할 수 있으므로, 구체적인 설명은 생략 하기로 한다. 본 개시에 따른 로봇 장치는 생체 신호 측정 센서로부터 획득되는 센서 값을 분석함으로써 사용자의 생체 정보를 식별하고, 식별된 생체 정보를 더 이용하여 사용자가 특정하는 지점을 식별할 수도 있다. 네트워크 인터페이스는 로봇 장치가 다른 장치(미도시) 또는 사용자가 착용하고 있는 웨어러블 디바 이스, 서버 장치 또는 IOT 디바이스들과 통신을 하게 하는 하나 이상의 구성 요소를 포함할 수 있다. 다른 장치 (미도시)는 로봇 장치내 프로세스와 같은 기능을 수행할 수 있는 컴퓨팅 장치이거나, 센싱 장치일 수 있 으나, 이에 제한되지 않는다. 예를 들어, 네트워크 인터페이스는, 근거리 통신부, 이동 통신부, 방송 수신 부를 포함할 수 있다. 일 실시 예에 의하면 네트워크 인터페이스는 사용자가 착용하고 있는 웨어러블 디바이스로부터 생체 신호 에 대한 정보를 획득할 수도 있고, 획득된 생체 신호에 대한 정보를 프로세서로 전달할 수 있다. 일 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 개시를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 또한, 상기 일 실시 예에 다른 방법을 수행하도록 하는 프로그램이 저장된 기록매체를 포함하는 컴퓨터 프로그램 장치가 제 공될 수 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들 어지는 것과 같은 기계어 코드 뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언 어 코드를 포함한다."}
{"patent_id": "10-2020-0148705", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "전술한 본 발명의 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자는 본 발명 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 발명의 범위는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으로 해석되어야 한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9"}
{"patent_id": "10-2020-0148705", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따라 로봇 장치가 사용자가 응시 혹은 지시하는 객체를 특정하는 방법의 흐름도이다. 도 2는 일 실시예에 따라 로봇 장치가 사용자가 응시 혹은 지시하는 지점을 특정하는 구체적인 방법의 흐름도이 다. 도 3은 일 실시예에 따라 로봇 장치가 사용자가 응시 혹은 지시하는 지점을 특정하는 구체적인 방법의 흐름도이 다. 도 4는 일 실시예에 따라 좌표 변환에 따른 사용자가 응시 혹은 지시하는 지점의 연산 결과 값을 3차원 공간에 나타낸 도면이다. 도 5는 일 실시 예에 따라 사용자가 응시 혹은 지시하는 타겟 지점에 대한 기하학적 관계를 나타내는 도면이다. 도 6은 일 실시 예에 따라 로봇 장치 주변의 실험 환경의 구성을 나타내는 도면이다. 도 7은 일 실시 예에 따라 로봇 장치가 사용자 응시 혹은 지시 지점을 추적하는 구성을 나타내는 도면이다. 도 8은 일 실시 예에 따라 로봇 장치가 추정된 사용자가 특정하는 정보에 기초하여 객체를 응시 혹은 대응하는 동작을 설명하기 위한 도면이다. 도 9는 일 실시 예에 따른 로봇 장치의 블록도이다."}
