{"patent_id": "10-2023-0032520", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0138802", "출원번호": "10-2023-0032520", "발명의 명칭": "안전보호구 착용 감지 방법 및 이를 위한 영상 분석 장치", "출원인": "주식회사 엘지유플러스", "발명자": "주진선"}}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "영상 분석을 위한 영상을 입력 받는 단계;상기 영상 내에서 사람 객체와 안전보호구 객체를 감지하는 단계;상기 사람 객체에 대응되는 제 1 객체 블랍을 포함하는 제 1 피처 영상과 상기 안전보호구 관련 객체에 대응되는 제 2 객체 블랍을 포함하는 제 2 피처 영상을 설정하는 단계; 및제 1 객체 블랍과 제 2 객체 블랍 간의 IoU(Intersection over Union) 값에 기반하여 안전보호구 착용 여부를판단하는 단계;를 포함하는 안전보호구 착용 감지 방법."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 IoU 값이 소정 기준 값 이상인 경우 상기 안전보호구의 착용 상태로 판단하는 단계;를 포함하는 것을 특징으로 하는 안전보호구 착용 감지 방법."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서, 상기 기준 값은 상기 안전보호구의 종류에 따라서 다른 것을 특징으로 하는 안전보호구 착용 감지 방법."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서, 제 1 객체 블랍과 제 2 객체 블랍을 이진 OR 연산하여 제 1 객체 블랍과 제 2 객체 블랍의 전체 영역을 구하는단계;제 1 객체 블랍과 제 2 객체 블랍을 이진 AND 연산하여 제 1 객체 블랍과 제 2 객체 블랍의 겹침 영역을 구하는단계; 및상기 전체 영역과 상기 겹침 영역에 기반하여 상기 IoU 값을 계산하는 단계;를 더욱 포함하는 것을 특징으로 하는 안전보호구 착용 감지 방법."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서, 상기 안전보호구 객체의 종류 및 상기 사람 객체의 신체 부위에 따라서 제 1 객체 블랍을 조정하는 단계;를 포함하는 것을 특징으로 하는 안전보호구 착용 감지 방법."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서, 상기 안전보호구 객체의 종류에 따라서 제 2 객체 블랍을 조정하는 단계;를 포함하는 것을 특징으로 하는 안전보호구 착용 감지 방법."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항에 있어서, 상기 영상 내에서 제 1 소정 크기 이상 또는 제 2 소정 크기 이하의 객체를 필터링 하는 단계;를 포함하는 것을공개특허 10-2024-0138802-3-특징으로 하는 안전보호구 착용 감지 방법."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서, 인공지능 모델의 추론을 통해 상기 영상 내에서 사람 객체와 안전보호구 객체가 감지되는 것을 특징으로 하는안전보호구 착용 감지 방법."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8 항에 있어서, 상기 인공지능 모델은 Yolo V3 네트워크를 포함하는 것을 특징으로 하는 안전보호구 착용 감지 방법."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서, 상기 Yolo V3 네트워크는 기본 레이어에서 9개 레이어가 가지치기(pruning)된 것을 특징으로 하는 안전보호구착용 감지 방법."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "카메라로부터 입력 받은 영상 내에서 사람 객체와 안전보호구 객체를 감지하고,상기 사람 객체에 대응되는 제 1 객체 블랍을 포함하는 제 1 피처 영상과 상기 안전보호구 관련 객체에 대응되는 제 2 객체 블랍을 포함하는 제 2 피처 영상을 설정하고,제 1 객체 블랍과 제 2 객체 블랍 간의 IoU(Intersection over Union) 값에 기반하여 안전보호구 착용 여부를판단하도록 제어하는 제어부;를 포함하는 영상 분석 장치."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서, 상기 제어부는,상기 IoU 값이 소정 기준 값 이상인 경우 상기 안전보호구의 착용 상태로 판단하도록 제어하는 것을 특징으로하는 영상 분석 장치."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12 항에 있어서, 상기 기준 값은 상기 안전보호구의 종류에 따라서 다른 것을 특징으로 하는 영상 분석 장치."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 11 항에 있어서, 상기 제어부는,제 1 객체 블랍과 제 2 객체 블랍을 이진 OR 연산하여 제 1 객체 블랍과 제 2 객체 블랍의 전체 영역을 구하고,제 1 객체 블랍과 제 2 객체 블랍을 이진 AND 연산하여 제 1 객체 블랍과 제 2 객체 블랍의 겹침 영역을구하고,상기 전체 영역과 상기 겹침 영역에 기반하여 상기 IoU 값을 계산하도록 제어하는 것을 특징으로 하는 영상 분석 장치."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서, 상기 제어부는,상기 안전보호구 객체의 종류 및 상기 사람 객체의 신체 부위에 따라서 제 1 객체 블랍을 조정하도록 제어하는것을 특징으로 하는 영상 분석 장치.공개특허 10-2024-0138802-4-청구항 16 제 15 항에 있어서, 상기 제어부는, 상기 안전보호구 객체의 종류에 따라서 제 2 객체 블랍을 조정하도록 제어하는 것을 특징으로 하는 영상 분석장치."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 11 항에 있어서, 상기 제어부는,상기 영상 내에서 제 1 소정 크기 이상 또는 제 2 소정 크기 이하의 객체를 필터링하도록 제어하는 것을 특징으로 하는 영상 분석 장치."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 11 항에 있어서, 상기 제어부는,인공지능 모델의 추론을 통해 상기 영상 내에서 사람 객체와 안전보호구 객체를 감지하도록 제어하는 것을 특징으로 하는 영상 분석 장치."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 18 항에 있어서, 상기 인공지능 모델은 Yolo V3 네트워크를 포함하는 것을 특징으로 하는 영상 분석 장치."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 19 항에 있어서, 상기 Yolo V3 네트워크는 기본 레이어에서 9개 레이어가 가지치기(pruning)된 것을 특징으로 하는 영상 분석 장치."}
{"patent_id": "10-2023-0032520", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "영상 분석을 위한 영상을 입력 받기 위한 명령어;상기 영상 내에서 사람 객체와 안전보호구 객체를 감지하기 위한 명령어;상기 사람 객체에 대응되는 제 1 객체 블랍을 포함하는 제 1 피처 영상과 상기 안전보호구 관련 객체에 대응되는 제 2 객체 블랍을 포함하는 제 2 피처 영상을 설정하기 위한 명령어; 및제 1 객체 블랍과 제 2 객체 블랍 간의 IoU(Intersection over Union) 값에 기반하여 안전보호구 착용 여부를판단하기 위한 명령어;를 포함하는, 안전보호구 착용 감지를 위해 컴퓨터에 의해 실행되도록 구성된 프로그램을저장하는 기록 매체."}
{"patent_id": "10-2023-0032520", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은, 안전보호구를 검출하기 위한 딥러닝 객체 검출 모델을 생성하고 상기 딥러닝 객체 검출 모델을 통해 검출된 각 객체의 위치 관계를 활용하여 작업자의 안전보호구 착용 상태를 인식하는 지능형 영상분석 기술을 이 용한 안전보호구 착용 감지 방법 및 이를 위한 영상 분석 장치에 관한 것이다. 본 발명은, 카메라로부터 입력 받은 영상 내에서 사람 객체와 안전보호구 객체를 감지하고, 상기 사람 객체에 대응되는 제 1 객체 블랍을 포함 하는 제 1 피처 영상과 상기 안전보호구 관련 객체에 대응되는 제 2 객체 블랍을 포함하는 제 2 피처 영상을 설 정하고, 제 1 객체 블랍과 제 2 객체 블랍 간의 IoU(Intersection over Union) 값에 기반하여 안전보호구 착용 여부를 판단하도록 제어하는 제어부를 포함하는 영상 분석 장치 등을 제공할 수 있다."}
{"patent_id": "10-2023-0032520", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 지능형 영상 분석을 통해 카메라 영상과 같은 입력 영상에서 작업자가 안전보호구(예를 들면, 안전모, 안전복, 안전화, 안전띠 등)를 착용하기 있는지 여부를 모니터링하기 위한 안전보호구 착용 감지 방법 및 이를 위한 영상 분석 장치에 관한 것이다."}
{"patent_id": "10-2023-0032520", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "사용자 단말기는 이동 가능여부에 따라 이동 단말기(mobile/portable terminal) 및 고정 단말기(stationary terminal)으로 나뉠 수 있다. 다시 이동 단말기는 사용자의 직접 휴대 가능 여부에 따라 휴대(형) 단말기 (handheld terminal) 및 거치형 단말기(vehicle mounted terminal)로 나뉠 수 있다. 사용자 단말기의 기능은 다양화 되고 있다. 예를 들면, 데이터와 음성통신, 카메라를 통한 사진촬영 및 비디오 촬영, 음성녹음, 스피커 시스템을 통한 음악파일 재생 그리고 디스플레이부에 이미지나 비디오를 출력하는 기능 이 있다. 일부 단말기는 전자게임 플레이 기능이 추가되거나, 멀티미디어 플레이어 기능을 수행한다. 특히 최근 의 이동 단말기는 방송과 비디오나 텔레비전 프로그램과 같은 시각적 컨텐츠를 제공하는 멀티캐스트 신호를 수 신할 수 있다. 이와 같은 사용자 단말기는 기능이 다양화됨에 따라 예를 들어, 사진이나 동영상의 촬영, 음악이나 동영상 파일 의 재생, 게임, 방송의 수신 등의 복합적인 기능들을 갖춘 멀티미디어 기기(Multimedia player) 형태로 구현되 고 있다. 최근 CCTV(Closed Circuit Television)의 설치가 증가하면서 효율적인 모니터링을 위해, 지능형 영상분석 기술 (Intelligent video analytics)에 대한 관심이 높아지고 있다. 상기 사용자 단말기는 안전보호구 착용 상태를 감지하기 위한 지능형 영상 분석 장치로도 활용될 수 있다. 기존의 경우 작업자의 안전보호구 착용 상태를 사람이 눈으로 직접 확인하거나, 작업장에 설치된 CCTV 또는 작 업 시 촬영한 사진을 사람이 사용자 단말기를 통해 직접 모니터링하며 작업자의 안전보호구 착용상태를 확인하 였다. 디지털 전환(Digital Transformation) 시대가 도래함에 따라 지능형 영상분석 기술 또는 스마트 헬멧 (RFID, 근 접센서 등) 등을 도입하여 자동으로 착용상태를 감지하는 시도가 이루어지고 있다. 접촉식 방법인 스마트 안전보호구 (스마트 헬멧 등)의 경우 별도의 센서가 부착된 보호구가 필요하므로 초기 도 입비용이 과도하게 발생하며 센서 간 간섭에 의해 민감한 장소 (반도체 공정 등)에서는 사용이 어렵고 세탁/세 척이 용이하지 못한 단점이 있다. 비접촉식 방법인 지능형 영상분석 기술은 영상의 정보를 분석하여 사전에 정의된 이벤트를 감지하여 자동으로 관리자에게 경보를 전송하는 기술이다. 지능형 영상 분석에서 검출 혹은 탐지하는 이벤트로는, 공사장 또는 공 장과 같은 작업장에서 작업자가 안전보호구를 착용하고 있는지 여부가 있을 수 있다. 지능형 영상 분석은, 예를 들어, 배경 영역 분리, 객체 검출, 객체 추적 및 이벤트 탐지 과정으로 이루어지며, 이러한 과정을 통해서 사전에 정의된 이벤트를 탐지할 수 있다. 여기서, 배경 영역 분리는, 입력된 영상에서 관심 있는 전경 영역과 배경 영역을 구분하는 과정이고, 객체 검출 은, 전경 영역에서 객체의 위치, 크기, 모양 등을 검출하는 과정이고, 객체 추적은 연속되는 영상에서 검출된 객체의 이동 경로를 찾는 과정이며, 이벤트 탐지는 객체의 특징 정보와 이동 정보를 바탕으로 사전에 정의된 규 칙을 위반하는지 혹은 만족하는 지를 판단하는 과정이다. 기존의 지능형 영상분석 기술의 경우, 안전복과 안전모 등과 같은 안전보호구의 색상을 인식하는 방법이 주로 사용되었으며, 안전보호구의 색상을 인식하기 위해 사전에 안전보호구의 색상을 지정하여 착용하도록 한다. 하 지만 현장의 조명상태가 변경되거나 사전에 정의되지 않은 색상의 안전보호구는 인식할 수 없다는 단점이 있다. 이러한 단점을 해결하기 위해 착용할 수 있는 보호구의 색상을 늘리거나 색상 인식 엔진에서 인식할 수 있는 색 상의 분포를 넓게 사용하기도 하는데 정의된 색상이 많고 분포가 넓을수록 오감지가 많아지는 단점이 있다. 예 를 들면, 염색된 머리 색상 또는 현장의 조명 색상에 의해 안전모 미착용 상태를 착용 상태로 인지될 수 있다는 문제점이 있었다."}
{"patent_id": "10-2023-0032520", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은, 안전보호구를 검출하기 위한 딥러닝 객체 검출 모델을 생성하고 상기 딥러닝 객체 검출 모델을 통해 검출된 각 객체의 위치 관계를 활용하여 작업자의 안전보호구 착용 상태를 인식하는 지능형 영상분석 기술을 이 용한 안전보호구 착용 감지 방법 및 이를 위한 영상 분석 장치를 제공하는 것을 그 목적으로 한다."}
{"patent_id": "10-2023-0032520", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기와 같은 목적을 달성하기 위해 본 발명의 일 측면에 따르면, 카메라로부터 입력 받은 영상 내에서 사람 객 체와 안전보호구 객체를 감지하고, 상기 사람 객체에 대응되는 제 1 객체 블랍을 포함하는 제 1 피처 영상과 상 기 안전보호구 관련 객체에 대응되는 제 2 객체 블랍을 포함하는 제 2 피처 영상을 설정하고, 제 1 객체 블랍과 제 2 객체 블랍 간의 IoU(Intersection over Union) 값에 기반하여 안전보호구 착용 여부를 판단하도록 제어하는 제어부를 포함하는 영상 분석 장치를 제공할 수 있다. 상기 제어부는, 상기 IoU 값이 소정 기준 값 이상인 경우 상기 안전보호구의 착용 상태로 판단하도록 제어할 수 있다. 상기 기준 값은 상기 안전보호구의 종류에 따라서 다를 수 있다. 상기 제어부는, 제 1 객체 블랍과 제 2 객체 블랍을 이진 OR 연산하여 제 1 객체 블랍과 제 2 객체 블랍의 전체 영역을 구하고, 제 1 객체 블랍과 제 2 객체 블랍을 이진 AND 연산하여 제 1 객체 블랍과 제 2 객체 블랍의 겹 침 영역을 구하고, 상기 전체 영역과 상기 겹침 영역에 기반하여 상기 IoU 값을 계산하도록 제어할 수 있다. 상기 제어부는, 상기 안전보호구 객체의 종류 및 상기 사람 객체의 신체 부위에 따라서 제 1 객체 블랍을 조정 하도록 제어할 수 있다. 상기 제어부는, 상기 안전보호구 객체의 종류에 따라서 제 2 객체 블랍을 조정하도록 제어할 수 있다. 상기 제어부는, 상기 영상 내에서 제 1 소정 크기 이상 또는 제 2 소정 크기 이하의 객체를 필터링하도록 제어 할 수 있다. 상기 제어부는, 인공지능 모델의 추론을 통해 상기 영상 내에서 사람 객체와 안전보호구 객체를 감지하도록 제 어할 수 있다. 상기 인공지능 모델은 Yolo V3 네트워크를 포함할 수 있다. 상기 Yolo V3 네트워크는 기본 레이어에서 9개 레이어가 가지치기(pruning)된 것일 수 있다. 또한, 본 발명의 일 측면에 따르면, 영상 분석을 위한 영상을 입력 받는 단계, 상기 영상 내에서 사람 객체와 안전보호구 객체를 감지하는 단계, 상기 사람 객체에 대응되는 제 1 객체 블랍을 포함하는 제 1 피처 영상과 상 기 안전보호구 관련 객체에 대응되는 제 2 객체 블랍을 포함하는 제 2 피처 영상을 설정하는 단계, 및 제 1 객 체 블랍과 제 2 객체 블랍 간의 IoU(Intersection over Union) 값에 기반하여 안전보호구 착용 여부를 판단하는 단계;를 포함하는 안전보호구 착용 감지 방법을 제공할 수 있다. 또한, 본 발명의 일 측면에 따르면, 영상 분석을 위한 영상을 입력 받기 위한 명령어, 상기 영상 내에서 사람 객체와 안전보호구 객체를 감지하기 위한 명령어, 상기 사람 객체에 대응되는 제 1 객체 블랍을 포함하는 제 1 피처 영상과 상기 안전보호구 관련 객체에 대응되는 제 2 객체 블랍을 포함하는 제 2 피처 영상을 설정하기 위 한 명령어, 및 제 1 객체 블랍과 제 2 객체 블랍 간의 IoU(Intersection over Union) 값에 기반하여 안전보호 구 착용 여부를 판단하기 위한 명령어를 포함하는, 안전보호구 착용 감지를 위해 컴퓨터에 의해 실행되도록 구 성된 프로그램을 저장하는 기록 매체를 제공할 수 있다."}
{"patent_id": "10-2023-0032520", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따른 안전보호구 착용 감지 방법 및 이를 위한 영상 분석 장치의 효과에 대해 설명하면 다음과 같다. 본 발명의 실시 예들 중 적어도 하나에 의하면, 관리자(즉, 사람)의 수동 작업을 최소한으로 하고 조명의 변화 에도 강건하게 작업자의 안전보호구 착용 상태에 관한 이벤트를 탐지할 수 있다는 장점이 있다."}
{"patent_id": "10-2023-0032520", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명 에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용 되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이 해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 이들 각 구성요소는 별도의 개별 하드웨어 모듈로 구성되거나 둘 이상의 하드웨어 모듈로 구현될 수도 있고, 둘 이상의 구성요소들이 하나의 하드웨어 모듈로 구현될 수도 있으며, 경우에 따라서는 소프트웨어로도 구현될 수 있음은 물론이다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함한다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되 어야 한다. 본 명세서에서 설명되는 영상 분석 장치 또는 단말기에는 휴대폰, 스마트 폰(smart phone), 노트북 컴퓨터 (laptop computer), 디지털방송용 단말기, PDA(personal digital assistants), PMP(portable multimedia player), 네비게이션, 슬레이트 PC(slate PC), 태블릿 PC(tablet PC), 울트라북(ultrabook) 등이 포함될 수 있 다. 도 1을 참조하면, 도 1은 본 발명과 관련된 영상 분석 장치를 설명하기 위한 블록도이다. 상기 영상 분석 장치는 무선 통신부, 입력부, 러닝 프로세서, 출력부, 인터페이스부 , 메모리, 제어부, 및 전원 공급부 등을 포함할 수 있다. 도 1에 도시된 모든 구성요소들 이 사용자 단말기를 구현하는데 있어서 필수적인 것은 아니어서, 본 명세서 상에서 설명되는 사용자 단말기는 위에서 열거된 구성요소들 보다 많거나, 또는 적은 구성요소들을 가질 수 있다. 도 1의 각 구성요소는 실제 구 현되는 영상 분석 장치의 사양에 따라 통합, 추가, 또는 생략될 수 있다. 즉, 필요에 따라 2 이상의 구성 요소가 하나의 구성요소로 합쳐지거나, 혹은 하나의 구성요소가 2 이상의 구성요소로 세분되어 구성될 수 있다. 보다 구체적으로, 상기 구성요소들 중 무선 통신부는, 영상 분석 장치와 무선 통신 시스템 사이, 영 상 분석 장치와 다른 영상 분석 장치 사이, 또는 영상 분석 장치와 외부서버 사이의 무선 통신 을 가능하게 하는 하나 이상의 모듈을 포함할 수 있다. 또한, 상기 무선 통신부는, 영상 분석 장치를 하나 이상의 네트워크에 연결하는 하나 이상의 모듈을 포함할 수 있다. 상기 무선 통신부는, 방송 채널을 통하여 외부의 방송 관리 서버로부터 방송 신호 및/또는 방송 관련된 정보를 수신하기 위한 방송 수신 모듈, 이동통신을 위한 기술표준들 또는 통신방식(예를 들어, LTE(Long Term Evolution), LTE-A(Long Term Evolution-Advanced), 5G 등)에 따라 구축된 이동 통신망 상에서 기지국, 외부의 단말, 서버 중 적어도 하나와 무선 신호를 송수신하기 위한 이동통신 모듈, 무선 인터넷 접속을 위한 무선 인터 넷 모듈, 근거리 통신(예를 들어, 블루투스(Bluetooth™), 적외선 통신(Infrared Data Association; IrDA), NFC(Near Field Communication), Wi-Fi(Wireless-Fidelity) 등)을 위한 근거리 통신 모듈, 및 영상 분석 장치 의 위치(또는 현재 위치)를 획득하기 위한 모듈 위치정보 모듈 (예를 들면, GPS(Global Positioning System) 모듈) 중 적어도 하나를 포함할 수 있다. 입력부는, 영상 신호 입력을 위한 카메라 또는 영상 입력부, 오디오 신호 입력을 위한 마이크로폰 (microphone, 122) 또는 오디오 입력부, 사용자로부터 정보를 입력받기 위한 사용자 입력부(123, 예를 들어, 터 치키(touch key), 푸시키(mechanical key) 등)를 포함할 수 있다. 입력부에서 수집한 음성 데이터나 이미 지 데이터는 분석되어 사용자의 제어명령으로 처리될 수 있다. 러닝 프로세서는 학습 데이터를 이용하여 인공 신경망으로 구성된 모델을 학습시킬 수 있다. 여기서, 학습 된 인공 신경망을 학습 모델이라 칭할 수 있다. 학습 모델은 학습 데이터가 아닌 새로운 입력 데이터에 대하여 결과 값을 추론해 내는데 사용될 수 있고, 추론된 값은 어떠한 동작을 수행하기 위한 판단의 기초로 이용될 수 있다. 출력부는 시각, 청각 등과 관련된 출력을 발생시키기 위한 것으로, 디스플레이부, 음향 출력부 중 적어도 하나를 포함할 수 있다. 디스플레이부는 터치 센서와 상호 레이어 구조를 이루거나 일체형으로 형성됨으로써, 터치 스크린을 구현할 수 있다. 이러한 터치 스크린은, 영상 분석 장치와 사용자 사이의 입 력 인터페이스를 제공하는 사용자 입력부로서 기능함과 동시에, 영상 분석 장치와 사용자 사이의 출 력 인터페이스를 제공할 수 있다. 인터페이스부는 영상 분석 장치에 연결되는 다양한 종류의 외부 기기와의 통로 역할을 수행한다. 이 러한 인터페이스부는, 유/무선 헤드셋 포트(port), 외부 충전기 포트(port), 유/무선 데이터 포트(port), 메모리 카드(memory card) 포트, 식별 모듈이 구비된 장치를 연결하는 포트(port), 오디오 I/O(Input/Output) 포트(port), 비디오 I/O(Input/Output) 포트(port), 이어폰 포트(port) 중 적어도 하나를 포함할 수 있다. 영 상 분석 장치에서는, 상기 인터페이스부에 외부 기기가 연결되는 것에 대응하여, 연결된 외부 기기와 관련된 적절할 제어를 수행할 수 있다. 또한, 메모리는 영상 분석 장치의 다양한 기능을 지원하는 데이터를 저장한다. 메모리는 영상 분석 장치에서 구동되는 다수의 응용 프로그램(application program 또는 애플리케이션(application)), 영상 분석 장치의 동작을 위한 데이터들, 명령어들을 저장할 수 있다. 메모리는 모델 저장부를 포함할 수 있다. 모델 저장부는 러닝 프로세서을 통하여 학습 중 인 또는 학습된 모델(또는 인공 신경망, 231a)을 저장할 수 있다. 제어부는 상기 응용 프로그램과 관련된 동작 외에도, 통상적으로 영상 분석 장치의 전반적인 동작을 제어한다. 제어부는 위에서 살펴본 구성요소들을 통해 입력 또는 출력되는 신호, 데이터, 정보 등을 처리 하거나 메모리에 저장된 응용 프로그램을 구동함으로써, 사용자에게 적절한 정보 또는 기능을 제공 또는 처리할 수 있다. 또한, 제어부는 메모리에 저장된 응용 프로그램을 구동하기 위하여, 도 1와 함께 살펴본 구성요소들 중 적어도 일부를 제어할 수 있다. 나아가, 제어부는 상기 응용 프로그램의 구동을 위하여, 영상 분석 장 치에 포함된 구성요소들 중 적어도 둘 이상을 서로 조합하여 동작시킬 수 있다. 전원공급부는 제어부의 제어 하에서, 외부의 전원, 내부의 전원을 인가 받아 영상 분석 장치에 포함된 각 구성요소들에 전원을 공급한다. 이러한 전원공급부는 배터리를 포함하며, 상기 배터리는 내장형 배터리 또는 교체가능한 형태의 배터리가 될 수 있다. 상기 각 구성요소들 중 적어도 일부는, 이하에서 설명되는 다양한 실시 예들에 따른 영상 분석 장치의 동작, 제 어, 또는 제어방법을 구현하기 위하여 서로 협력하여 동작할 수 있다. 또한, 상기 영상 분석 장치의 동작, 제어, 또는 제어방법은 상기 메모리에 저장된 적어도 하나의 응용 프로그램의 구동에 의하여 영상 분석 장 치 상에서 구현될 수 있다. 상기 영상 분석 장치는 서버로 구현될 수도 있다. 한편, 이하에서 다양한 실시 예는 예를 들어, 소프트웨어, 하드웨어 또는 이들의 조합된 것을 이용하여 컴퓨터 또는 이와 유사한 장치로 읽을 수 있는 기록매체 내에서 구현될 수 있다. 본 명세서에 따른 인공지능과 관련된 기능을 위해 상기 제어부는 하나 또는 복수의 프로세서를 포함할 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU(Central Processing Unit), AP(Application Processor), DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU(Graphic Processing Unit), VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU(Neural Processing Unit)와 같은 인공지능 전용 프로 세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인공지능 모델에 따 라, 입력 데이터를 처리하도록 제어할 수 있다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 인공 지능은 인공적인 지능 또는 이를 만들 수 있는 방법론을 연구하는 분야를 의미하며, 머신 러닝(기계 학습, Machine Learning)은 인공 지능 분야에서 다루는 다양한 문제를 정의하고 그것을 해결하는 방법론을 연구하는 분야를 의미한다. 머신 러닝은 어떠한 작업에 대하여 꾸준한 경험을 통해 그 작업에 대한 성능을 높이는 알고리 즘으로 정의하기도 한다. 인공 신경망(ANN: Artificial Neural Network)은 머신 러닝에서 사용되는 모델로서, 시냅스의 결합으로 네트워 크를 형성한 인공 뉴런(노드)들로 구성되는, 문제 해결 능력을 가지는 모델 전반을 의미할 수 있다. 인공 신경 망은 다른 레이어의 뉴런들 사이의 연결 패턴, 모델 파라미터를 갱신하는 학습 과정, 출력값을 생성하는 활성화 함수(Activation Function)에 의해 정의될 수 있다. 인공 신경망은 입력층(Input Layer), 출력층(Output Layer), 그리고 선택적으로 하나 이상의 은닉층(Hidden Layer)를 포함할 수 있다. 각 층은 하나 이상의 뉴런을 포함하고, 인공 신경망은 뉴런과 뉴런을 연결하는 시냅 스를 포함할 수 있다. 인공 신경망에서 각 뉴런은 시냅스를 통해 입력되는 입력 신호들, 가중치, 편향에 대한 활성 함수의 함숫값을 출력할 수 있다. 모델 파라미터는 학습을 통해 결정되는 파라미터를 의미하며, 시냅스 연결의 가중치와 뉴런의 편향 등이 포함된 다. 그리고, 하이퍼파라미터는 머신 러닝 알고리즘에서 학습 전에 설정되어야 하는 파라미터를 의미하며, 학습 률(Learning Rate), 반복 횟수, 미니 배치 크기, 초기화 함수 등이 포함된다. 인공 신경망의 학습의 목적은 손실 함수를 최소화하는 모델 파라미터를 결정하는 것으로 볼 수 있다. 손실 함수 는 인공 신경망의 학습 과정에서 최적의 모델 파라미터를 결정하기 위한 지표로 이용될 수 있다. 머신 러닝은 학습 방식에 따라 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning)으로 분류할 수 있다. 지도 학습은 학습 데이터에 대한 레이블(label)이 주어진 상태에서 인공 신경망을 학습시키는 방법을 의미하며, 레이블이란 학습 데이터가 인공 신경망에 입력되는 경우 인공 신경망이 추론해 내야 하는 정답(또는 결과 값)을 의미할 수 있다. 비지도 학습은 학습 데이터에 대한 레이블이 주어지지 않는 상태에서 인공 신경망을 학습시키 는 방법을 의미할 수 있다. 강화 학습은 어떤 환경 안에서 정의된 에이전트가 각 상태에서 누적 보상을 최대화 하는 행동 혹은 행동 순서를 선택하도록 학습시키는 학습 방법을 의미할 수 있다. 인공 신경망 중에서 복수의 은닉층을 포함하는 심층 신경망(DNN: Deep Neural Network)으로 구현되는 머신 러닝 을 딥 러닝(심층 학습, Deep Learning)이라 부르기도 하며, 딥 러닝은 머신 러닝의 일부이다. 이하에서, 머신 러닝은 딥 러닝을 포함하는 의미로 사용된다. 기계 학습을 이용한 객체 감지 모델은 단일 단계 방식의 YOLO(you Only Look Once) 모델, 이단계 방식의 Faster R-CNN(Regions with Convolution Neural Networks) 모델 등이 있다. YOLO(you Only Look Once) 모델은 이미지 내에 존재하는 객체와 해당 객체의 위치가 이미지를 한번만 보고 예측 할 수 있는 모델이다. YOLO(you Only Look Once) 모델은 원본 이미지를 동일한 크기의 그리드(grid)로 나눈다. 그리고, 각 그리드에 대해 그리드 중앙을 중심으로 미리 정의된 형태로 지정된 경계 박스의 개수를 예측하고 이를 기반으로 신뢰도가 계산된다. 그 후, 이미지에 객체가 포함되어 있는지, 또는 배경만 단독으로 있는지에 대한 여부가 포함되며, 높은 객체 신 뢰도를 가진 위치가 선택되어 객체 카테고리가 파악될 수 있다. Faster R-CNN(Regions with Convolution Neural Networks) 모델은 RCNN 모델보다 더 빨리 객체를 감지할 수 있 는 모델이다. Faster R-CNN(Regions with Convolution Neural Networks) 모델에 대해 좀더 구체적으로 살펴보면, 먼저 CNN(Convolution Neural Network) 모델을 통해 이미지로부터 특징 맵이 추출된다. 추출된 특징 맵에 기초하여, 복수의 관심 영역(Region of Interest, RoI)들이 추출된다. 각 관심 영역에 대해 RoI 풀링이 수행된다. RoI 풀링은 관심 영역이 투사된 피쳐 맵을 미리 정해 놓은 H x W 크기에 맞게 끔 그리드를 설정하고, 각 그리드 에 포함된 칸 별로, 가장 큰 값을 추출하여, H x W 크기를 갖는 피쳐 맵을 추출하는 과정이다. H x W 크기를 갖는 피쳐 맵로부터 특징 벡터가 추출되고, 특징 벡터로부터 객체의 식별 정보가 얻어질 수 있다. 이하, 도 2를 참조하여, 본 발명의 일실시예에 따른 안전보호구 착용 상태의 검출하기 위한 지능형 영상분석 기 술에 사용될 수 있는 인공지능 모델을 학습하기 위한 학습 데이터의 예시에 대해 살펴보겠다. 도 2는 본 발명의 일실시예에 따른 안전보호구 착용 상태의 검출하기 위한 지능형 영상분석 기술에 사용될 수 있는 인공지능 모델 을 학습하기 위한 학습 데이터의 예시를 도시한다. 도 2에 도시된 바와 같이, 상기 인공지능 모델의 학습을 위해 다양한 각도로 다양한 색상의 안전보호구를 착용 하고 있는 작업자에 관한 복수의 이미지 데이터(1100, 1200, 1300, 1400, 1500, 1600, 1700)가 학습 데이터로 서 생성하거나 수집될 수 있다. 제 1 이미지 및 제 2 이미지는 마스크와 함께 안전모를 착용한 작업자를 정면을 촬영한 이미지의 예시이다. 제 3 이미지는 안전화를 착용한 작업자의 하체를 촬영한 이미지의 예시이다. 제 2 이미지, 제 4 이미지, 및 제 5 이미지는 노란색이 아닌 다른 색상의 안전모를 착용한 작업자를 촬영한 이미지의 예시이다. 제 5 이미지 및 제 7 이미지는 안전모를 착용한 작업자를 정면 이외의 다른 각도로 촬영한 이미지 의 예시이다. 제 6 이미지는 마스크 없이 안전모를 착용한 작업자를 정면을 촬영한 이미지의 예시이다. 본 발명의 경우, 직접 수집한 안전보호구를 착용한 작업자의 이미지 데이터 이외에, Generative AI인 DALL-E, BitSleep 등의 API에 “man / woman / person / people / full body / upper body / lower body / wear / wearing / safety helmet / safety clothes / safety shoes” 등의 텍스트 키워드를 입력하여 생성된 이미지 데이터도 함께 사용하였다. 이와 같은 수집된 이미지 데이터 각각은 상기 인공지능 모델의 학습 및 테스트를 위해 레이블링될 수 있다. 여 기서 레이블링이라 함은, 학습 데이터가 인공지능 모델에 입력되는 경우 인공지능 모델이 추론해 내야 하는 정 답(똔느 결과 값)에 해당하는 레이블을 상기 학습 데이터에 부여하는 작업을 의미할 수 있다. 상기 레이블링에 대해 도 3을 더욱 참조하여 설명하겠다. 도 3은 도 2의 제 1 이미지에 대한 레이블링의 예시이다. 도 3의 (3-1)에 도시된 바와 같이, 제 1 이미지는 작업자 및 안전보호구 관련한 복수의 객체를 포함할 수 있다. 예를 들면, 즉 제 1 이미지는 작업자의 머리(얼굴 포함) 객체, 안전모 객체, 얼굴 객 체, 마스크 객체를 포함할 수 있다. 그러면, 도 3의 (3-2)에 도시된 바와 같이, 제 1 이미지는 상기 복수의 객체를 포함하는 것으로 라벨링될 수 있다. 상기 레이블링에 대해 도 4를 더욱 참조하여 설명하겠다. 도 4은 도 2의 제 2 이미지에 대한 레이블링의 예시이 다. 도 4의 (4-1)에 도시된 바와 같이, 제 2 이미지는 작업자 및 안전보호구 관련한 복수의 객체를 포함할 수 있다. 예를 들면, 즉 제 2 이미지는 작업자의 머리(얼굴 포함) 객체, 안전모 객체, 얼굴 객 체, 마스크 객체, 제 1 안전모 액세서리 객체, 및 제 1 안전모 액세서리 객체를 포함 할 수 있다. 그러면, 도 4의 (4-2)에 도시된 바와 같이, 제 4 이미지는 상기 복수의 객체를 포함하는 것으로 라벨링될 수 있다.이와 같이 준비된 학습데이터를 기반으로 본 발명의 일실시예에 따라 안전보호구 객체 검출을 위한 인공지능 모 델을 생성하고 학습시키는 것에 대해 도 5를 참조하여 설명하겠다. 도 5는 본 발명의 일실시에에 따라 안전보호 구 객체 검출을 위한 인공지능 모델을 생성하고 학습시키는 순서도이다. 본 발명의 일실시예에 따라 안전보호구 객체 검출을 위한 인공지능 모델은 상기 영상 분석 장치를 통해 학 습될 수도 있고, 또는 외부 장치 또는 서버를 통해 학습된 다음 상기 영상 분석 장치에 다운로드될 수도 있다. 상기 인공지능 모델의 학습은 오프라인 단계(off-line phase)로 이해될 수 있다. 상기 인공지능 모델의 학습을 위해 학습 데이터가 수집될 필요가 있음은 전술한 바와 같다. 예를 들면, 학습 데 이터로서 JPG 타입의 이미지 파일이 준비될 수 있다. 상기 이미지 파일들이 클래스 별로 소팅될 수 있다[S51]. 상기 클래스는 상기 레이블(label)로 이해될 수 있다. 그 다음 상기 이미지 파일들이 데이터세트 별로 소팅될 수 있다[S52]. 예를 들어, 어떤 클래스의 이미지 파일은 충분히 많은데 반하여 어떤 다른 클래스의 이미지 파일은 이에 비해 너무 적을 수 있다. 이 경우 상기 인공지능 모델의 학습이 제대로 이루어지지 않을 수 있다. 따라서, 클래스 별로 적정 개수의 이미지 파일을 갖는 데이터 세트가 준비될 필요가 있다. 상기 이미지 파일들에 대해 해당 레이블에 따라 주석달기(annotation)가 행해질 수 있다[S53]. 즉 상기 이미지 파일들에 대해 레이블링(labelling)이 행해질 수 있다. 상기 이미지 파일들이 사전 설정된 비율의 훈련(train)용 데이터와 테스트(test)용 데이터로 구분될 수 있다 [S54]. 경우에 따라서는 상기 이미지 파일들이 사전 설정된 비율의 학습(train)용 데이터, 테스트(test)용 데이 터, 및 검증(verification)용 데이터로 구분될 수 있다. 인공지능 모델의 각 출력 값과 이에 해당하는 클래스를 매칭시키는 클래스 네임 파일들(claim file names)의 리 스트를 생성할 수 있다[S55]. 인공지능 모델의 입력으로서 적절한 형태가 되도록 상기 학습데이터를 처리 또는 변환할 수 있다[S56]. 예를 들 면, 본 발명에서는 인공지능 모델로서 YOLO 네트워크(예를 들면, Yolo V3 네트워크)가 사용되었다. 따라서, 상 기 학습 데이터가 상기 YOLO 네트워크에 적합한 형태의 파일(예를 들면, data 파일)로 변환할 수 있다. 그 다음, 상기 YOLO 네트워크의 백본 네트워크(예를 들면, darknet53 모델)의 레이어 구조를 정의하는 파일(cfg 파일)을 미세조정(fine tuning)할 수 있다[S57]. 그 다음, 상기 인공지능 모델의 학습을 위한 상기 영상 분석 장치 또는 상기 외부 장치 또는 서버는 상기 YOLO 네트워크의 백본 네트워크를 다운로드 받을 수 있다[S58]. 학습을 위해 상기 영상 분석 장치 또는 상기 외부 장치 또는 서버의 GPU(Graphics Processing Unit)를 활 성화시키고, 학습을 수행할 수 있다[S59, S60]. 학습의 결과로서, 상기 학습 데이터에 기반하여 안전보호구 객체 검출을 위해 학습된 상기 인공지능 모델의 레 이어의 구조를 정의하기 위한 파일(cfg 파일)과 가중치 정보를 갖는 파일(weights 파일)이 생성될 수 있다. 이와 같이 학습된 상기 인공지능 모델의 특성에 대해 도 6 및 도 7을 참조하여 살펴보겠다. 도 6은 본 발명의 일실시예에 따라 학습된 인공지능 모델의 레이어 구조를 도시한다. 도 7은 본 발명의 일실시예에 따라 학습된 여러 인공지능 모델들의 성능 비교에 관한 그래프를 도시한다. 본 발명의 일실시예에서는 상기 준비된 학습 데이터에 기반하여 초당 더 많은 이미지를 처리할 수 있도록 딥러 닝 네트워크의 레이어를 가지치기(pruning)하여 감소시키고 그에 맞게 파라미터를 최적화한 다크넷 기반의 Yolo V3 네트워크로서 안전보호구 검출 모델을 생성함에 있어서, Yolo V3의 기본 레이어 갯수보다 9개의 레이어를 더 줄였다. 이는 Yolo V3의 기본 레이어를 통해 탐지 가능한 객체의 종류보다 안전 보호구 및 착용자와 관련된 객 체가 비교적 적다는 점을 고려한 것으로서 이를 통해 초당 이미지 처리 속도를 더 높일 수 있었다. 반드시 Yolo V3 네트워크만이 사용되어야 하는 것은 아니고, 객체 검출의 결과가 블랍(Blob)(또는 바운딩 박스(bounding box))와 클래스라면 다른 네트워크가 사용될 수도 있다. 예를 들면, AdaBoost, MLP, 또는 Yolo의 다른 버전 등 이 사용될 수 있다. Yolo V3 네트워크, Yolo V4 네트워크, 및 Yolo V5 네트워크 간의 성능 실험 비교에 대해 도 7을 더욱 참조하여 설명하겠다. 도 7에 도시된 바와 같이, 동일한 학습 데이터에 대해, 도 6의 Yolo V3 네트워크는 약 13000회의반복 학습(iterations)에서 99.38%의 최고 정확성을 달성하였고, Yolo V4 네트워크는 약 18000회의 반복 학습 (iterations)에서 99.13%의 최고 정확성을 달성하였고, Yolo V5 네트워크는 약 49225회의 반복 학습 (iterations)에서 98.65%의 최고 정확성을 달성하였다. 즉, 도 6의 Yolo V3 네트워크가 더 적은 횟수의 반복 학습으로 더 높은 정확성을 보였다. 이하, 도 8을 참조하여, 상기 인공지능 모델이 탑재된 영상 분석 장치의 안전보호구 착용 감지 동작에 대해 설 명하겠다. 도 8은 본 발명의 일실시예에 따른 영상 분석 장치의 안전보호구 착용 감지 동작에 관한 순서도이다. 상기 영상 분석 장치에는 앞서 설명한 상기 인공지능 모델이 탑재될 수 있다. 상기 영상 분석 장치의 상기 제어부는 상기 카메라를 통해 영상 분석을 위한 영상을 실시간 입 력 받을 수 있도록 제어할 수 있다[S81]. 또는, 상기 제어부는 상기 무선통신부 또는 상기 인터페이 스부를 통해 영상 분석을 위한 영상을 외부 기기 또는 외부 카메라(미도시)로부터 실시간 입력 받을 수 있도록 제어할 수 있다. 상기 영상는 실시간으로 입력되는 정지 영상 또는 동영상이거나, 촬영 또는 녹화된 후 재생되는 정지 영상 또는 동영상일 수 있다. 상기 영상이 실시간으로 입력된다는 측면에서 상기 영상 분석 장치의 상기 안전보호구 착용 감지 동작은 온라인 단계(on-line phase)로 이해될 수 있다. 상기 제어부는, 도 5의 (5-2)에 도시된 바와 같이, 상기 인공지능 모델을 이용한 인공지능 추론을 통해 상 기 영상 내의 적어도 하나의 객체를 감지할 수 있다[S82]. 상기 감지되는 객체는 작업자 및 안전보호구 중 적어 도 하나와 관련한 객체일 수 있다. 상기 제어부는 상기 인공지능 추론을 통해 상기 적어도 하나의 객체를 그 대상 기준으로 필터링할 수 있다 [S83]. 상기 필터링을 통해 상기 적어도 하나의 객체 중에서 그 크기가 너무 작거나 큰 것은 무시될 수 있다. 또는 상기 필터링을 통해 신뢰도가 상기 적어도 하나의 객체 중에서 일정 값 미만인 객체는 무시될 수 있다. 상기 제어부는 상기 정지 영상 또는 동영상에서 상기 필터링되지 않는 (즉, 무시되지 않는) 객체에 대해 바운딩 블록(bounding block)(즉 경계 상자) 설정을 하면서 그 예측 클래스를 추론할 수 있다. 상기 제어부는 각 객체에 따른 피처(feature) 영상을 설정(또는 생성)할 수 있다[S84]. 상기 피처 영상은 상기 객체의 내부 및 외부를 구분하기 위해 내부와 외부를 서로 다른 단색으로 채워 넣은 영상을 의미할 수 있 다. 즉 상기 제어부는 상기 동영상의 한 프레임 또는 정지 영상에서 작업자와 안전보호구가 모두 감지된 경우 상기 작업자에 대한 제 1 피처 영상과 상기 안정보호구에 대한 제 2 피처 영상을 설정할 수 있다. 상기 제어부는 제 1 피처 영상 및 제 2 피처 영상에 대해 이진 연산(bitwise operation)을 통해 IoU(Intersection over Union) 값을 계산할 수 있다[S85]. 상기 제어부는 상기 IoU 값에 기반하여 상기 작업자가 상기 안전보호구를 착용하고 있는지 여부를 추론할 수 있다. 상기 IoU 값에 대해서는 나중에 다시 설 명하겠다. 이하, 도 9를 참조하여, 본 발명의 일실시예에 따른 영상 분석 장치의 추론 영상 및 이로부터 설정될 수 있는 피처 영상의 예시에 대해 설명하겠다. 도 9는 본 발명의 일실시예에 따른 영상 분석 장치의 추론 영상 및 이에 따라 출력되는 피처 영상의 예시이다. 이하에서는 설명의 간편함을 위해 상기 안전보호구가 작업자의 머리에 착용되는 안전모인 것으로 가정하고 설명 하겠다. 그러나, 이하의 설명이 작업자의 다른 신체 부위에 착용될 수 있는 다른 안전보호구에도 적용될 수 있 음은 물론이다. 상기 영상 분석 장치의 상기 제어부는 실시간으로 입력되는 영상에 대해 도 9의 (9-1)에 도시된 바와 같은 추론 영상을 출력할 수 있다. 상기 추론 영상은 작업자의 머리(얼굴 포함) 객체, 안전모 객체 , 얼굴 객체, 마스크 객체를 포함할 수 있다. 도 9의 (9-2)에 도시된 바와 같이, 상기 제어부는 상기 추론 영상으로부터 상기 머리 객체에 해당하 는 제 1 피처 영상과, 상기 안전모 객체에 해당하는 제 2 피처 영상을 설정할 수 있다. 제 1 피처 영상은 상기 머리 객체에 해당하는 제 1 객체 블랍을 포함하고, 제 2 피처 영상 은 상기 안전모 객체에 해당하는 제 2 객체 블랍을 포함할 수 있다. 상기 제어부는 실시간 인 식 단계에서는 안전보호구 검출 모델을 적용하여 실시간으로 객체를 검출한 뒤 신뢰도(Confidence)가 일정 임계 치 이상인 결과에 대해서만 피처 영상을 생성할 수 있다. 본 발명에서는 실험을 통해 신뢰도 임계치를 0.71로지정하였다. 제 1 피처 영상 및 제 2 피처 영상로부터 상기 IoU 값을 계산하는 예시에 대해 도 10을 더욱 참조 하여 설명하겠다. 도 10은 도 9의 피처 영상에 따른 안전보호구 착용 여부에 대한 판단 예시를 도시한다. 도 10의 (10-1)에 도시된 바와 같이, 상기 제어부는 작업자의 안전모 착용 상태를 인식하기 위해 제 1 피 처 영상 및 제 2 피처 영상를 상호 오버랩하여 이들 간의 이진 연산(bitwise operation)을 수행할 수 있다. 예를 들면, 상기 제어부는 제 1 피처 영상의 제 1 객체 블랍과 제 2 피처 영상의 제 2 객체 블랍을 이진 OR 연산하여 제 1 객체 블랍와 제 2 객체 블랍의 전체 영역(Area of Union)을 구할 수 있다. 그리고, 상기 제어부는 제 1 피처 영상의 제 1 객체 블랍과 제 2 피처 영상의 제 2 객체 블랍을 이진 AND 연산하여 제 1 객체 블랍와 제 2 객체 블랍 의 공통 영역(또는 겹침 영역)(Area of Overlap)을 구할 수 있다. 그 다음, 상기 제어부는 도 10의 (10-2)의 수식에 따라 IoU 값을 계산할 수 있다. 상기 제어부는 상기 IoU 값이 소정 값(예를 들면, 50%) 이상인 경우 상기 작업자가 상기 안전모를 착용한 것으로 출력할 수 있다. 상기 제어부는 상기 작업자의 안전보호구 착용 여부를 출력할 때, 상기 안전보호구의 종류, 상기 안전보호 구의 착용 신체 부위, 및 IoU 값 중 적어도 하나를 함께 출력할 수 있다. 딥러닝 객체 검출은 학습된 데이터에 의해 성능이 좌우될 수 있다. 많은 데이터를 학습하더라도 실제 환경에서 사용하게 되면 계절, 날씨, 촬영 시간 등 다양한 요소에 의해 오 검출이나 미 검출이 발생할 수 있다. 이를 해 결하기 위해서는 데이터를 재수집 하고 다시 학습하여 모델을 재생성해야 한다. 하지만 본 발명에서는 재학습 하지 않고 사람의 신체나 안전보호구 영역을 제대로 검출하지 못했을 때에도 안전보호구의 착용상태를 인식할 수 있도록 딥러닝 객체 검출 클래스를 여러 개로 정의하였으므로 특정 객체가 미 검출 되더라도 다른 객체 검출 결과를 대체하여 착용 상태를 인식할 수 있다. 각 피처 영상 내의 객체 블랍의 설정 기준이 작업장 환경 및 작업자 중 적어도 하나에 따라 다양하게 정의될 수 있다. 이에 대해 도 11을 더욱 참조하여 설명하겠다. 도 11은 도 9의 피처 영상의 설정 기준에 대한 예시를 도 시한다. 먼저 상기 추론 영상이 상기 안전보호구 객체로서 안전모 객체를 포함하는 경우에 대해 살펴보겠다. 상기 제어부는 작업자의 머리(얼굴 포함) 전체 검출 영역을 제 1 피처 영상의 제 1 객체 블랍으로 설정할 수 있다. 도 9에서 설명한 바와 같다. 또는, 제어부는 작업자의 얼굴 검출 영역을 그 높이의 2/3 배만큼 위로 확장한 영역을 제 1 피처 영상의 제 1 객체 블랍으로 설정할 수 있다. 이는 작업자의 얼굴은 검출되었으나 작업자의 머리 전체는 검출되지 않는 경우에 활용될 수 있다. 또는, 상기 제어부는 작업자의 마스크 검출 영역을 그 높이의 2배만큼 위로 확장한 영역을 제 1 객체 블랍 으로 설정할 수 있다. 이는 작업자의 마스크는 검출되었으나 작업자의 머리 전체 또는 얼굴이 검출되지 않는 경 우에 활용될 수 있다. 또는, 상기 제어부는 작업자의 전신 검출 영역을 상하로 8등분하였을 때 최상단의 첫 번째 및 두 번째 등 분 영역을 제 1 객체 블랍으로 설정할 수 있다. 이는 작업자의 전신은 검출되었으나 작업자의 머리 전체,얼굴 또는 마스크가 검출되지 않는 경우에 활용될 수 있다. 또는, 상기 제어부는 작업자의 상반신 토르소 검출 영역을 그 높이의 2/3 배만큼 위로 확장한 영역을 상하 로 4등분하였을 때 최상단의 첫 번째 및 두 번째 등분 영역을 제 1 객체 블랍으로 설정할 수 있다. 이는 작업자 의 상반신 토르소는 검출되었으나 작업자의 머리 전체, 얼굴 또는 마스크가 검출되지 않는 경우에 활용될 수 있 다. 한편, 상기 제어부는 안전모 객체 영역을 제 2 객체 블랍으로 설정할 수 있다. 또는, 상기 제어부는 안전모 액세서리 영역을 그 높이의 1.5배 만큼 확장한 영역을 제 2 객체 블랍으로 설 정할 수 있다. 이는 작업자의 안전모 액세서리를 검출되었으나 안전모는 검출되지 않은 경우에 활용될 수 있다. 상기 제어부는 상기 안전모 객체 관련하여 제 1 객체 블랍 및 제 2 객체 블랍 간의 IoU 값이 50% 이상인 경우에 상기 작업자가 상기 안전모를 착용한 것으로 추론할 수 있다. 이하, 상기 추론 영상이 상기 안전보호구 객체로서 안전화 객체를 포함하는 경우에 대해 살펴보겠다. 상기 제어부는 작업자의 전신 검출 영역을 상하로 8등분하였을 때 최하단의 첫 번째 등분 영역을 제 1 객 체 블랍으로 설정할 수 있다. 또는 상기 제어부는 상기 제어부는 작업자의 하반신 검출 영역을 상하로 4등분하였을 때 최하단의 첫 번째 등분 영역을 제 1 객체 블랍으로 설정할 수 있다. 상기 제어부는 안전화 객체 영역을 제 2 객체 블랍으로 설정할 수 있다. 상기 제어부는 상기 안전화 객체 관련하여 제 1 객체 블랍 및 제 2 객체 블랍 간의 IoU 값이 30% 이상인 경우에 상기 작업자가 상기 안전화를 착용한 것으로 추론할 수 있다. 이하, 상기 추론 영상이 상기 안전보호구 객체로서 안전복(또는 안전띠) 객체를 포함하는 경우에 대해 살펴보겠 다. 상기 제어부는 작업자의 상반신 토르소 검출 영역을 제 1 객체 블랍으로 설정할 수 있다. 또는, 상기 제어부는 작업자의 전신 검출 영역을 상하로 8등분하였을 때 최상단의 첫 번째 내지 세 번째 등분 영역을 제 1 객체 블랍으로 설정할 수 있다. 이는 작업자의 전신은 검출되었으나 작업자의 상반신 토르소 가 검출되지 않는 경우에 활용될 수 있다. 또는, 상기 제어부는 작업자의 하반신 검출 영역을 그 높이의 2/3 배만큼 위로 확장한 영역을 상하로 6등 분하였을 때 최상단의 첫 번째 내지 세 번째 등분 영역을 제 1 객체 블랍으로 설정할 수 있다. 이는 작업자의 하반신 토르소는 검출되었으나 작업자의 상반신 또는 전신이 검출되지 않는 경우에 활용될 수 있다. 상기 제어부는 안전복(또는 안전띠) 객체 영역을 제 2 객체 블랍으로 설정할 수 있다. 상기 제어부는 상기 안전복(또는 안전띠) 객체 관련하여 제 1 객체 블랍 및 제 2 객체 블랍 간의 IoU 값이 70% 이상인 경우에 상기 작업자가 상기 안전화를 착용한 것으로 추론할 수 있다. 이상에서 설명된 관련된 수치는 예시적인 것으로서, 이와는 다르게 적용될 수도 있음은 물론이다. 많은 학습 데이터를 통해 인공지능 모델을 학습시키더라도, 상기 학습된 인공지능 모델이 실제 추론에 활용되는 경우 학습 데이터와 상이한 입력 데이터가 입력되면 추론 결과의 정확성은 떨어질 수 있다. 이것을 상쇄하기 위 해 앞서 설명된 것처럼 각 피처 영상 내의 객체 블랍의 설정 기준이 작업장 환경 및 작업자 중 적어도 하나에 따라 다양하게 정의될 수 있다. 각각의 해당 객체 블랍이 미검출되는 경우 다른 블랍으로 대체하여 IoU 값을 계산 할 수 있다. 예를 들면, 머리 객체가 미검출되는 경우, 얼굴, 마스크, 전신, 상반신 객체 블랍의 크기를 재구성하여 머리 객체의 블랍 대신 활용할 수 있다. 또는 전신 객체가 미 검출되는 경우 상반신 또는 하반신 검출 블랍의 크기를 확장 또는 분할 축소하여 전신 객체 블랍 대신하여 활용할 수 있다. 전술한 본 발명은, 프로그램이 기록된 매체에 컴퓨터가 읽을 수 있는 코드로서 구현하는 것이 가능하다. 컴퓨터 가 읽을 수 있는 매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 컴퓨터가 읽을 수 있는 매체의 예로는, HDD(Hard Disk Drive), SSD(Solid State Disk), SDD(Silicon Disk Drive), ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있으며, 또한 캐리어 웨이브(예를 들어, 인터넷을 통한 전송)의 형태로 구현되는 것도 포함한다. 또한, 상기 컴퓨터는 상기 제어부 를 포함할 수도 있다. 따라서, 상기의 상세한 설명은 모든 면에서 제한적으로 해석되어서는 아니되고 예시 적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적 해석에 의해 결정되어야 하고, 본 발 명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다."}
{"patent_id": "10-2023-0032520", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른 영상 분석 장치의 개략적 블록도이다. 도 2는 본 발명의 일실시예에 따른 안전보호구 착용 상태의 검출하기 위한 지능형 영상분석 기술에 사용될 수 있는 인공지능 모델을 학습하기 위한 학습 데이터의 예시를 도시한다. 도 3은 도 2의 제 1 이미지에 대한 레이블링의 예시이다. 도 4은 도 2의 제 2 이미지에 대한 레이블링의 예시이다. 도 5는 본 발명의 일실시에에 따라 안전보호구 객체 검출을 위한 인공지능 모델을 생성하고 학습시키는 순서도 이다. 도 6은 본 발명의 일실시예에 따라 학습된 인공지능 모델의 레이어 구조를 도시한다. 도 7은 본 발명의 일실시예에 따라 학습된 여러 인공지능 모델들의 성능 비교에 관한 그래프를 도시한다.도 8은 본 발명의 일실시예에 따른 영상 분석 장치의 안전보호구 착용 감지 동작에 관한 순서도이다. 도 9는 본 발명의 일실시예에 따른 영상 분석 장치의 추론 영상 및 이에 따라 출력되는 피처 영상의 예시이다. 도 10은 도 9의 피처 영상에 따른 안전보호구 착용 여부에 대한 판단 예시를 도시한다. 도 11은 도 9의 피처 영상의 설정 기준에 대한 예시를 도시한다."}
