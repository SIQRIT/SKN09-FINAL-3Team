{"patent_id": "10-2019-0030660", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0052804", "출원번호": "10-2019-0030660", "발명의 명칭": "전자 장치 및 전자 장치의 제어 방법", "출원인": "삼성전자주식회사", "발명자": "김광윤"}}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,마이크;통신부; 컴퓨터 실행가능 명령어(computer executable instructions)를 저장하는 메모리; 및상기 컴퓨터 실행가능 명령어를 실행함으로써, 기 설정된 트리거 워드를 포함한 사용자의 음성을 상기 마이크를 통해 획득하고,상기 사용자 음성에 포함된 트리거 워드를 바탕으로 상기 전자 장치의 음성 인식 기능을 활성화하고,상기 음성 인식 기능이 활성화된 동안 상기 사용자가 이동하는 이벤트를 감지하며,상기 감지된 이벤트를 바탕으로 타 전자 장치의 음성 인식 기능을 활성화시키기 위한 제어 신호를 상기 통신부를 통해 상기 타 전자 장치로 전송하도록 제어하는 프로세서; 를 포함하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 프로세서는,상기 음성 인식 기능이 활성화된 이후 상기 마이크를 통해 획득된 상기 사용자의 음성의 신호에 기초하여 상기사용자가 이동하는 이벤트를 감지하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 메모리는 음성 수신이 가능한 복수의 타 전자 장치에 대한 정보를 저장하며,상기 프로세서는,상기 사용자가 이동하는 이벤트가 감지되면, 상기 사용자의 이동 정보를 획득하고, 상기 사용자의 이동 정보에기초하여 상기 복수의 타 전자 장치 중 상기 사용자와 가장 가까운 타 전자 장치를 식별하고, 상기 식별된 타전자 장치로 상기 제어 신호를 전송하도록 상기 통신부를 제어하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 프로세서는,상기 마이크를 통해 획득된 상기 사용자의 음성에 대해 음성 인식을 수행하여 제1 음성 인식 정보를 획득하고,상기 제어 신호를 수신한 타 전자 장치로부터 제2 음성 인식 정보를 상기 통신부를 통해 수신하고, 상기 제1 음성 인식 정보와 상기 제2 음성 인식 정보를 바탕으로 최종 인식 결과를 획득하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 프로세서는,상기 제어 신호를 상기 타 전자 장치에 전송한 시간에 대한 정보를 획득하고, 상기 획득한 시간에 대한 정보를공개특허 10-2020-0052804-3-기초로 상기 제1 음성 인식 정보와 상기 제2 음성 인식 정보를 정합하여 상기 최종 인식 결과를 획득하는 전자장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 획득한 시간에 대한 정보는,상기 제어 신호를 전송한 절대적 시간에 대한 정보 또는 상기 전자 장치의 음성 인식 기능이 활성화된 시점을기준으로 상기 제어 신호를 상기 타 전자 장치에 전송한 시점의 상대적 시간에 대한 정보를 포함하는 전자장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4항에 있어서,상기 프로세서는,상기 타 전자 장치로부터 수신된 상기 제2 음성 인식 정보가 음향 모델이 적용되고 언어 모델이 미적용된 정보인 경우, 상기 제2 음성 인식 정보에 상기 전자 장치에 기 저장된 언어 모델을 적용하여 상기 최종 인식 결과를획득하고,상기 타 전자 장치로부터 수신된 상기 제2 음성 인식 정보가 음향 모델 및 언어 모델이 미 적용된 정보인 경우,상기 제2 음성 인식 정보에 상기 전자 장치에 기 저장된 음향 모델과 언어 모델을 적용하여 상기 최종 인식 결과를 획득하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제4항에 있어서,상기 프로세서는,상기 최종 인식 결과에 대한 피드백이 상기 타 전자 장치에서 제공되도록 하는 제어 신호를 상기 타 전자 장치로 전송하도록 상기 통신부를 제어하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 프로세서는,타 전자 장치로부터 음성 인식 기능을 활성화하도록 하는 제2 제어 신호가 상기 통신부를 통해 수신되면 상기전자 장치의 음성 인식 기능을 활성화하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 프로세서는,상기 타 전자 장치로부터 사용자 정보를 상기 통신부를 통해 수신하고, 상기 제2 제어 신호에 의해 음성 인식 기능이 활성화된 후 복수의 사용자의 음성이 상기 마이크를 통해 수신되면, 상기 복수의 사용자의 음성 중 상기 타 전자 장치로부터 수신한 사용자 정보에 대응하는 사용자의 음성을식별하고, 식별된 사용자 음성에 대한 음성 인식을 수행하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 프로세서는,상기 제2 제어 신호에 의해 음성 인식 기능이 활성화된 후 사용자의 발화가 종료될 때까지 상기 마이크를 통해공개특허 10-2020-0052804-4-수신한 음성에 대해 음성 인식을 수행하여 음성 인식 정보를 획득하고, 획득한 음성 인식 정보를 상기 타 전자장치로 전송하도록 상기 통신부를 제어하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,상기 프로세서는,상기 기 설정된 트리거 워드를 포함한 제1 사용자의 음성이 상기 마이크를 통해 수신되어 음성 인식 기능이 활성화된 상태에서 상기 제2 제어 신호 및 제2 사용자에 대한 정보가 상기 타 전자 장치로부터 수신되면, 상기 마이크를 통해 획득된 상기 제1 사용자의 음성과 상기 제2 사용자의 음성을 각각 처리하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "전자 장치의 제어방법에 있어서,기 설정된 트리거 워드를 포함한 사용자의 음성을 상기 전자 장치의 마이크를 통해 획득하는 단계;상기 사용자 음성에 포함된 트리거 워드를 바탕으로 상기 전자 장치의 음성 인식 기능을 활성화하는 단계;상기 음성 인식 기능이 활성화된 동안 상기 사용자가 이동하는 이벤트를 감지하는 단계; 및상기 감지된 이벤트를 바탕으로 타 전자 장치의 음성 인식 기능을 활성화시키기 위한 제어 신호를 상기 타 전자장치로 전송하는 단계; 를 포함하는 제어방법."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 사용자가 이동하는 이벤트를 감지하는 단계는,상기 음성 인식 기능이 활성화된 이후 상기 마이크를 통해 획득된 상기 사용자의 음성의 신호에 기초하여 상기사용자가 이동하는 이벤트를 감지하는 제어방법."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13항에 있어서,상기 전자 장치는 음성 수신이 가능한 복수의 타 전자 장치에 대한 정보를 저장하며,상기 제어 신호를 상기 타 전자 장치로 전송하는 단계는,상기 사용자가 이동하는 이벤트가 감지되면, 상기 사용자의 이동 정보를 획득하고, 상기 사용자의 이동 정보에기초하여 상기 복수의 타 전자 장치 중 상기 사용자와 가장 가까운 타 전자 장치를 식별하고, 상기 식별된 타전자 장치로 상기 제어 신호를 전송하는 제어방법."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제13항에 있어서,상기 마이크를 통해 획득된 상기 사용자의 음성에 대해 음성 인식을 수행하여 제1 음성 인식 정보를 획득하고,상기 제어 신호를 수신한 타 전자 장치로부터 제2 음성 인식 정보를 상기 통신부를 통해 수신하고, 상기 제1 음성 인식 정보와 상기 제2 음성 인식 정보를 바탕으로 최종 인식 결과를 획득하 단계; 를 더 포함하는 제어방법."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 최종 인식 결과를 획득하는 단계는,상기 제어 신호를 상기 타 전자 장치에 전송한 시간에 대한 정보를 획득하고, 상기 획득한 시간에 대한 정보를기초로 상기 제1 음성 인식 정보와 상기 제2 음성 인식 정보를 정합하여 상기 최종 인식 결과를 획득하는 제어방법.공개특허 10-2020-0052804-5-청구항 18 제17항에 있어서,상기 획득한 시간에 대한 정보는,상기 제어 신호를 전송한 절대적 시간에 대한 정보 또는 상기 전자 장치의 음성 인식 기능이 활성화된 시점을기준으로 상기 제어 신호를 상기 타 전자 장치에 전송한 시점의 상대적 시간에 대한 정보를 포함하는 제어방법."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제16항에 있어서,상기 최종 인식 결과를 획득하는 단계는,상기 타 전자 장치로부터 수신된 상기 제2 음성 인식 정보가 음향 모델이 적용되고 언어 모델이 미적용된 정보인 경우, 상기 제2 음성 인식 정보에 상기 전자 장치에 기 저장된 언어 모델을 적용하여 상기 최종 인식 결과를획득하고,상기 타 전자 장치로부터 수신된 상기 제2 음성 인식 정보가 음향 모델 및 언어 모델이 미 적용된 정보인 경우,상기 제2 음성 인식 정보에 상기 전자 장치에 기 저장된 음향 모델과 언어 모델을 적용하여 상기 최종 인식 결과를 획득하는 제어방법."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "전자 장치에 있어서,회로를 포함하는 통신부;적어도 하나의 명령을 포함하는 메모리; 및상기 적어도 하나의 명령을 실행하는 프로세서; 를 포함하고,상기 프로세서는, 사용자의 음성에 따른 제1 오디오 신호를 상기 통신부를 통해 제1 외부 장치로부터 수신하고, 상기 수신된 제1 오디오 신호에 포함된 정보를 바탕으로 사용자의 이동이 감지되면, 상기 사용자의 이동 방향에 위치하는 제2 외부 장치로부터 상기 사용자의 음성에 따른 제2 오디오 신호를 수신하기 위한 제어 신호를 상기 제2 외부 장치로 전송하도록 상기 통신부를 제어하며, 상기 제2 오디오 신호를 상기 통신부를 통해 제2 외부 장치로부터 수신하고, 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합하여 상기 사용자의 음성에 대한 음성 인식을 수행하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제20 항에 있어서,상기 프로세서는,상기 제1 오디오 신호가 수신된 시간 및 상기 제2 오디오 신호가 수신된 시간이 대응되도록 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정렬하고, 상기 정렬된 제1 오디오 신호 및 상기 제2 오디오를 비교하여 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제21 항에 있어서,상기 프로세서는,상기 제2 오디오 신호가 수신된 시점을 바탕으로 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합하는 전자 장치.공개특허 10-2020-0052804-6-청구항 23 제21 항에 있어서,상기 프로세서는, 상기 수신된 제1 오디오 신호의 파워 및 상기 수신된 제1 오디오 신호의 신호 대 잡음비(Signal-to-NoiseRatio, SNR) 중 적어도 하나를 바탕으로 상기 수신된 제1 오디오 신호의 품질 및 상기 수신된 제2 오디오 신호의 품질을 식별하고, 상기 식별된 제1 오디오 신호의 품질 및 상기 식별된 제2 오디오 신호의 품질을 바탕으로 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제21 항에 있어서,상기 프로세서는, 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 학습된 인공지능 모델에 입력하여 상기 수신된 제1 오디오 신호의 음성 인식 결과에 대한 제1 확률 정보 및 상기 수신된 제2 오디오 신호의 음성 인식 결과에 대한 제2 확률 정보를 획득하며, 상기 획득된 제1 확률 정보 및 상기 획득된 제2 확률 정보를 바탕으로 상기 수신된 제1 오디오 신호 및 상기수신된 제2 오디오 신호를 정합하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제20 항에 있어서, 상기 프로세서는, 상기 수신된 제1 오디오 신호의 파워 및 상기 수신된 제1 오디오 신호의 신호 대 잡음비 중 적어도 하나를 바탕으로 상기 사용자의 이동을 감지하는 전자 장치."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "전자 장치의 제어 방법에 있어서,사용자의 음성에 따른 제1 오디오 신호를 상기 통신부를 통해 제1 외부 장치로부터 수신하는 단계;상기 수신된 제1 오디오 신호에 포함된 정보를 바탕으로 사용자의 이동이 감지되면, 상기 사용자의 이동 방향에위치하는 제2 외부 장치로부터 상기 사용자의 음성에 따른 제2 오디오 신호를 수신하기 위한 제어 신호를 상기제2 외부 장치로 전송하는 단계;상기 제2 오디오 신호를 상기 통신부를 통해 제2 외부 장치로부터 수신하는 단계; 및상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합하여 상기 사용자의 음성에 대한 음성 인식을 수행하는 단계; 를 포함하는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제26 항에 있어서,상기 제어 방법은,상기 제1 오디오 신호가 수신된 시간 및 상기 제2 오디오 신호가 수신된 시간이 대응되도록 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정렬하는 단계; 를 더 포함하고,상기 음성 인식을 수행하는 단계는,상기 정렬된 제1 오디오 신호 및 상기 제2 오디오를 비교하여 상기 수신된 제1 오디오 신호 및 상기 수신된 제2오디오 신호를 정합하는 전자 장치의 제어 방법.공개특허 10-2020-0052804-7-청구항 28 제27 항에 있어서,상기 음성 인식을 수행하는 단계는,상기 제2 오디오 신호가 수신된 시점을 바탕으로 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합하는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_29", "content": "제27 항에 있어서,상기 음성 인식을 수행하는 단계는,상기 수신된 제1 오디오 신호의 파워 및 상기 수신된 제1 오디오 신호의 신호 대 잡음비(Signal-to-NoiseRatio, SNR) 중 적어도 하나를 바탕으로 상기 수신된 제1 오디오 신호의 품질 및 상기 수신된 제2 오디오 신호의 품질을 식별하는 단계; 및 상기 식별된 제1 오디오 신호의 품질 및 상기 식별된 제2 오디오 신호의 품질을 바탕으로 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합하는 단계; 를 포함하는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_30", "content": "제27 항에 있어서,상기 음성 인식을 수행하는 단계는,상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 학습된 인공지능 모델에 입력하여 상기 수신된제1 오디오 신호의 음성 인식 결과에 대한 제1 확률 정보 및 상기 수신된 제2 오디오 신호의 음성 인식 결과에대한 제2 확률 정보를 획득하는 단계; 및상기 획득된 제1 확률 정보 및 상기 획득된 제2 확률 정보를 바탕으로 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합하는 단계; 를 포함하는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0030660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_31", "content": "제26 항에 있어서, 상기 사용자의 이동은 상기 수신된 제1 오디오 신호의 파워 및 상기 수신된 제1 오디오 신호의 신호 대 잡음비중 적어도 하나를 바탕으로 감지되는 전자 장치의 제어 방법."}
{"patent_id": "10-2019-0030660", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 본 전자 장치는, 마이크, 통신부, 컴퓨터 실행가능 명령어(computer executable instructions)를 저장하는 메모리 및 컴퓨터 실행가능 명령어를 실행함으로써, 기 설정된 트리거 워드를 포함한 사용자의 음성을 마이크를 통해 획득하고, 사용자 음성에 포함된 트리거 워드를 바탕으로 전자 장치의 음성 인식 기능을 활성화하고, 음성 인식 기능이 활성화된 동안 사용자가 이동하는 이벤트를 감지하며, 감지된 이벤트를 바 탕으로 타 전자 장치의 음성 인식 기능을 활성화시키기 위한 제어 신호를 타 전자 장치로 전송하도록 통신부를 제어하는 프로세서를 포함한다."}
{"patent_id": "10-2019-0030660", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 이의 제어 방법에 관한 것으로, 더욱 상세하게는 타 전자 장치에서 음성 인식 작업이 이어져서 수행될 수 있도록 제어하며 개별 전자 장치에서 수행된 음성 인식 정보를 결합하여 최종 음성 인식 결 과를 획득할 수 있는 전자 장치 및 그의 제어방법에 대한 것이다."}
{"patent_id": "10-2019-0030660", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "근래에는 인공 지능 시스템이 다양한 분야에서 이용되고 있다. 인공 지능 시스템은 기존의 룰(rule) 기반 스마 트 시스템과 달리 기계가 스스로 학습하고 판단하며 똑똑해지는 시스템이다. 인공 지능 시스템은 사용할수록 인 식률이 향상되고 사용자 취향을 보다 정확하게 이해할 수 있게 되어, 기존 룰 기반 스마트 시스템은 점차 딥러 닝 기반 인공 지능 시스템으로 대체되고 있다. 인공 지능 기술은 기계학습(예로, 딥러닝) 및 기계학습을 활용한 요소 기술들로 구성된다. 기계학습은 입력 데이터들의 특징을 스스로 분류/학습하는 알고리즘 기술이며, 요소기술은 딥러닝 등의 기계학 습 알고리즘을 활용하여 인간 두뇌의 인지, 판단 등의 기능을 모사하는 기술로서, 언어적 이해, 시각적 이해, 추론/예측, 지식 표현, 동작 제어 등의 기술 분야로 구성된다. 인공 지능 기술이 응용되는 다양한 분야는 다음과 같다. 언어적 이해는 인간의 언어/문자를 인식하고 응용/처리 하는 기술로서, 자연어 처리, 기계 번역, 대화시스템, 질의 응답, 음성 인식/합성 등을 포함한다. 시각적 이해 는 사물을 인간의 시각처럼 인식하여 처리하는 기술로서, 오브젝트 인식, 오브젝트 추적, 영상 검색, 사람 인식, 장면 이해, 공간 이해, 영상 개선 등을 포함한다. 추론 예측은 정보를 판단하여 논리적으로 추론하고 예 측하는 기술로서, 지식/확률 기반 추론, 최적화 예측, 선호 기반 계획, 추천 등을 포함한다. 지식 표현은 인간 의 경험정보를 지식데이터로 자동화 처리하는 기술로서, 지식 구축(데이터 생성/분류), 지식 관리(데이터 활용) 등을 포함한다. 동작 제어는 차량의 자율 주행, 로봇의 움직임을 제어하는 기술로서, 움직임 제어(항법, 충돌, 주행), 조작 제어(행동 제어) 등을 포함한다. 한편, 근래에는 사용자 음성 문의에 대한 답변을 제공하는 인공지능 에이전트(예로, 빅스비TM, 어시스턴트TM 알렉 사TM 등)를 탑재한 음성 인식 기기를 이용한 다양한 서비스들이 제공되고 있다. 사용자는 트리거 워드를 통해 음 성 인식 기기의 음성 인식 기능을 활성화시킬 수 있다. 다만, 사용자의 이동이나 음성 인식 기기의 상태 변경 (전원 off 등)이 일어나면 음성 인식이 이어질 수 없다는 한계가 있었다. 즉, 사용자는 음성 인식 기능을 활성 화시킨 하나의 음성 인식 기기와만 인터렉션을 해야해야 하므로 불편함이 있었다."}
{"patent_id": "10-2019-0030660", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 문제점을 해결하기 위해 안출된 것으로, 본 개시의 목적은 타 전자 장치에서 음성 인식 작업 이 이어져서 수행될 수 있도록 제어하며 개별 전자 장치에서 수행된 음성 인식 정보를 결합하여 최종 음성 인식 결과를 획득할 수 있는 전자 장치 및 그의 제어방법을 제공함에 있다."}
{"patent_id": "10-2019-0030660", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 전자 장치는, 마이크, 통신부, 컴퓨터 실행가능 명령어(computer executable instructions)를 저장하는 메모리 및 상기 컴퓨터 실행가능 명령어를 실행함으로써, 기 설정된 트리거 워드를 포함한 사용자의 음성을 상기 마이크를 통해 획득하고, 상기 사용자 음성에 포함된 트리거 워드를 바탕으로 상 기 전자 장치의 음성 인식 기능을 활성화하고, 상기 음성 인식 기능이 활성화된 동안 상기 사용자가 이동하는 이벤트를 감지하며, 상기 감지된 이벤트를 바탕으로 타 전자 장치의 음성 인식 기능을 활성화시키기 위한 제어 신호를 상기 타 전자 장치로 전송하도록 상기 통신부를 제어하는 프로세서를 포함한다. 이 경우, 상기 프로세서는, 상기 음성 인식 기능이 활성화된 이후 상기 마이크를 통해 획득된 상기 사용자의 음 성의 신호에 기초하여 상기 사용자가 이동하는 이벤트를 감지할 수 있다. 한편, 상기 메모리는 음성 수신이 가능한 복수의 타 전자 장치에 대한 정보를 저장하며, 상기 프로세서는, 상기 사용자가 이동하는 이벤트가 감지되면, 상기 사용자의 이동 정보를 획득하고, 상기 사용자의 이동 정보에 기초 하여 상기 복수의 타 전자 장치 중 상기 사용자와 가장 가까운 타 전자 장치를 식별하고, 상기 식별된 타 전자 장치로 상기 제어 신호를 전송하도록 상기 통신부를 제어할 수 있다. 한편, 상기 프로세서는, 상기 마이크를 통해 획득된 상기 사용자의 음성에 대해 음성 인식을 수행하여 제1 음성 인식 정보를 획득하고, 상기 제어 신호를 수신한 타 전자 장치로부터 제2 음성 인식 정보를 상기 통신부를 통해 수신하고, 상기 제1 음성 인식 정보와 상기 제2 음성 인식 정보를 바탕으로 최종 인식 결과를 획득할 수 있다. 이 경우, 상기 프로세서는, 상기 제어 신호를 상기 타 전자 장치에 전송한 시간에 대한 정보를 획득하고, 상기 획득한 시간에 대한 정보를 기초로 상기 제1 음성 인식 정보와 상기 제2 음성 인식 정보를 정합하여 상기 최종 인식 결과를 획득할 수 있다. 이 경우, 상기 획득한 시간에 대한 정보는, 상기 제어 신호를 전송한 절대적 시간에 대한 정보 또는 상기 전자 장치의 음성 인식 기능이 활성화된 시점을 기준으로 상기 제어 신호를 상기 타 전자 장치에 전송한 시점의 상대 적 시간에 대한 정보를 포함할 수 있다. 한편, 상기 프로세서는, 상기 타 전자 장치로부터 수신된 상기 제2 음성 인식 정보가 음향 모델이 적용되고 언 어 모델이 미적용된 정보인 경우, 상기 제2 음성 인식 정보에 상기 전자 장치에 기 저장된 언어 모델을 적용하여 상기 최종 인식 결과를 획득하고, 상기 타 전자 장치로부터 수신된 상기 제2 음성 인식 정보가 음향 모델 및 언어 모델이 미 적용된 정보인 경우, 상기 제2 음성 인식 정보에 상기 전자 장치에 기 저장된 음향 모델과 언어 모델을 적용하여 상기 최종 인식 결과를 획득할 수 있다. 한편, 상기 프로세서는, 상기 최종 인식 결과에 대한 피드백이 상기 타 전자 장치에서 제공되도록 하는 제어 신 호를 상기 타 전자 장치로 전송하도록 상기 통신부를 제어할 수 있다. 한편, 상기 프로세서는, 타 전자 장치로부터 음성 인식 기능을 활성화하도록 하는 제2 제어 신호가 상기 통신부 를 통해 수신되면 상기 전자 장치의 음성 인식 기능을 활성화할 수 있다. 이 경우, 상기 프로세서는, 상기 타 전자 장치로부터 사용자 정보를 상기 통신부를 통해 수신하고, 상기 제2 제 어 신호에 의해 음성 인식 기능이 활성화된 후 복수의 사용자의 음성이 상기 마이크를 통해 수신되면, 상기 복 수의 사용자의 음성 중 상기 타 전자 장치로부터 수신한 사용자 정보에 대응하는 사용자의 음성을 식별하고, 식 별된 사용자 음성에 대한 음성 인식을 수행할 수 있다. 한편, 상기 프로세서는, 상기 제2 제어 신호에 의해 음성 인식 기능이 활성화된 후 사용자의 발화가 종료될 때 까지 상기 마이크를 통해 수신한 음성에 대해 음성 인식을 수행하여 음성 인식 정보를 획득하고, 획득한 음성 인식 정보를 상기 타 전자 장치로 전송하도록 상기 통신부를 제어할 수 있다. 한편, 상기 프로세서는, 상기 기 설정된 트리거 워드를 포함한 제1 사용자의 음성이 상기 마이크를 통해 수신되 어 음성 인식 기능이 활성화된 상태에서 상기 제2 제어 신호 및 제2 사용자에 대한 정보가 상기 타 전자 장치로 부터 수신되면, 상기 마이크를 통해 획득된 상기 제1 사용자의 음성과 상기 제2 사용자의 음성을 각각 처리할 수 있다. 한편, 본 개시의 일 실시 예에 따른 전자 장치의 제어방법은, 기 설정된 트리거 워드를 포함한 사용자의 음성을 상기 전자 장치의 마이크를 통해 획득하는 단계, 상기 사용자 음성에 포함된 트리거 워드를 바탕으로 상기 전자 장치의 음성 인식 기능을 활성화하는 단계, 상기 음성 인식 기능이 활성화된 동안 상기 사용자가 이동하는 이벤 트를 감지하는 단계 및 상기 감지된 이벤트를 바탕으로 타 전자 장치의 음성 인식 기능을 활성화시키기 위한 제 어 신호를 상기 타 전자 장치로 전송하는 단계를 포함한다. 이 경우, 상기 사용자가 이동하는 이벤트를 감지하는 단계는, 상기 음성 인식 기능이 활성화된 이후 상기 마이 크를 통해 획득된 상기 사용자의 음성의 신호에 기초하여 상기 사용자가 이동하는 이벤트를 감지할 수 있다. 한편, 상기 전자 장치는 음성 수신이 가능한 복수의 타 전자 장치에 대한 정보를 저장하며, 상기 제어 신호를 상기 타 전자 장치로 전송하는 단계는, 상기 사용자가 이동하는 이벤트가 감지되면, 상기 사용자의 이동 정보를 획득하고, 상기 사용자의 이동 정보에 기초하여 상기 복수의 타 전자 장치 중 상기 사용자와 가장 가까운 타 전 자 장치를 식별하고, 상기 식별된 타 전자 장치로 상기 제어 신호를 전송할 수 있다. 한편, 본 실시 예에 따른 전자 장치의 제어방법은, 상기 마이크를 통해 획득된 상기 사용자의 음성에 대해 음성 인식을 수행하여 제1 음성 인식 정보를 획득하고, 상기 제어 신호를 수신한 타 전자 장치로부터 제2 음성 인식 정보를 상기 통신부를 통해 수신하고, 상기 제1 음성 인식 정보와 상기 제2 음성 인식 정보를 바탕으로 최종 인 식 결과를 획득하는 단계를 더 포함할 수 있다. 이 경우, 상기 최종 인식 결과를 획득하는 단계는, 상기 제어 신호를 상기 타 전자 장치에 전송한 시간에 대한 정보를 획득하고, 상기 획득한 시간에 대한 정보를 기초로 상기 제1 음성 인식 정보와 상기 제2 음성 인식 정보 를 정합하여 상기 최종 인식 결과를 획득할 수 있다. 이 경우, 상기 획득한 시간에 대한 정보는, 상기 제어 신호를 전송한 절대적 시간에 대한 정보 또는 상기 전자 장치의 음성 인식 기능이 활성화된 시점을 기준으로 상기 제어 신호를 상기 타 전자 장치에 전송한 시점의 상대 적 시간에 대한 정보를 포함할 수 있다. 한편, 상기 최종 인식 결과를 획득하는 단계는, 상기 타 전자 장치로부터 수신된 상기 제2 음성 인식 정보가 음 향 모델이 적용되고 언어 모델이 미적용된 정보인 경우, 상기 제2 음성 인식 정보에 상기 전자 장치에 기 저장 된 언어 모델을 적용하여 상기 최종 인식 결과를 획득하고, 상기 타 전자 장치로부터 수신된 상기 제2 음성 인 식 정보가 음향 모델 및 언어 모델이 미 적용된 정보인 경우, 상기 제2 음성 인식 정보에 상기 전자 장치에 기 저장된 음향 모델과 언어 모델을 적용하여 상기 최종 인식 결과를 획득할 수 있다. 한편 본 개시의 일 실시 예에 따른 서버는, 통신부, 컴퓨터 실행가능 명령어(computer executable instructions)를 저장하는 메모리 및 상기 컴퓨터 실행가능 명령어를 실행함으로써, 기 설정된 트리거 워드를 포함한 사용자의 음성을 바탕으로 음성 인식 기능이 활성화된 제1 전자 장치로부터, 상기 통신부를 통해, 제1 음성 인식 정보를 수신하고, 상기 제1 전자 장치를 사용하는 사용자가 이동하는 이벤트를 감지하며, 상기 감지 된 이벤트를 바탕으로 제2 전자 장치의 음성 인식 기능을 활성화시키기 위한 제어 신호를 상기 제2 전자 장치로 전송하도록 상기 통신부를 제어하며, 상기 제2 전자 장치로부터 상기 통신부를 통해 제2 음성 인식 정보를 수신 하며, 상기 제1 전자 장치로부터 수신된 상기 제1 음성 인식 정보 및 상기 제2 전자 장치로부터 수신된 상기 제 2 음성 인식 정보를 정합하여 최종 인식 결과를 획득하는 프로세서를 포함할 수 있다. 한편, 본 개시의 일 실시 예에 따른 전자 장치는 회로를 포함하는 통신부, 적어도 하나의 명령을 포함하는 메모 리 및 상기 적어도 하나의 명령을 실행하는 프로세서를 포함하고, 상기 프로세서는 사용자의 음성에 따른 제1 오디오 신호를 상기 통신부를 통해 제1 외부 장치로부터 수신하고, 상기 수신된 제1 오디오 신호에 포함된 정보 를 바탕으로 사용자의 이동이 감지되면, 상기 사용자의 이동 방향에 위치하는 제2 외부 장치로부터 상기 사용자 의 음성에 따른 제2 오디오 신호를 수신하기 위한 제어 신호를 상기 제2 외부 장치로 전송하도록 상기 통신부를 제어하며, 상기 제2 오디오 신호를 상기 통신부를 통해 제2 외부 장치로부터 수신하고, 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합하여 상기 사용자의 음성에 대한 음성 인식을 수행한다. 여기서, 상기 프로세서는 상기 제1 오디오 신호가 수신된 시간 및 상기 제2 오디오 신호가 수신된 시간이 대응 되도록 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정렬하고, 상기 정렬된 제1 오디오 신 호 및 상기 제2 오디오를 비교하여 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합할 수 있다. 여기서, 상기 프로세서는 상기 제2 오디오 신호가 수신된 시점을 바탕으로 상기 수신된 제1 오디오 신호 및 상 기 수신된 제2 오디오 신호를 정합할 수 있다. 한편, 상기 프로세서는 상기 수신된 제1 오디오 신호의 파워 및 상기 수신된 제1 오디오 신호의 신호 대 잡음비 (Signal-to-Noise Ratio, SNR) 중 적어도 하나를 바탕으로 상기 수신된 제1 오디오 신호의 품질 및 상기 수신된 제2 오디오 신호의 품질을 식별하고, 상기 식별된 제1 오디오 신호의 품질 및 상기 식별된 제2 오디오 신호의 품질을 바탕으로 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합할 수 있다. 한편, 상기 프로세서는 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 학습된 인공지능 모델 에 입력하여 상기 수신된 제1 오디오 신호의 음성 인식 결과에 대한 제1 확률 정보 및 상기 수신된 제2 오디오 신호의 음성 인식 결과에 대한 제2 확률 정보를 획득하며, 상기 획득된 제1 확률 정보 및 상기 획득된 제2 확률 정보를 바탕으로 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합할 수 있다. 한편, 상기 프로세서는 상기 수신된 제1 오디오 신호의 파워 및 상기 수신된 제1 오디오 신호의 신호 대 잡음비 중 적어도 하나를 바탕으로 상기 사용자의 이동을 감지할 수 있다. 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법은 사용자의 음성에 따른 제1 오디오 신호를 상기 통신부를 통해 제1 외부 장치로부터 수신하는 단계, 상기 수신된 제1 오디오 신호에 포함된 정보를 바탕으로 사용자의 이 동이 감지되면, 상기 사용자의 이동 방향에 위치하는 제2 외부 장치로부터 상기 사용자의 음성에 따른 제2 오디 오 신호를 수신하기 위한 제어 신호를 상기 제2 외부 장치로 전송하는 단계, 상기 제2 오디오 신호를 상기 통신 부를 통해 제2 외부 장치로부터 수신하는 단계 및 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신 호를 정합하여 상기 사용자의 음성에 대한 음성 인식을 수행하는 단계; 를 포함한다. 여기서, 상기 제어 방법은 상기 제1 오디오 신호가 수신된 시간 및 상기 제2 오디오 신호가 수신된 시간이 대응 되도록 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정렬하는 단계를 더 포함하고, 상기 음 성 인식을 수행하는 단계는 상기 정렬된 제1 오디오 신호 및 상기 제2 오디오를 비교하여 상기 수신된 제1 오디 오 신호 및 상기 수신된 제2 오디오 신호를 정합하는 것일 수 있다. 한편, 상기 음성 인식을 수행하는 단계는 상기 제2 오디오 신호가 수신된 시점을 바탕으로 상기 수신된 제1 오 디오 신호 및 상기 수신된 제2 오디오 신호를 정합하는 것일 수 있다. 한편, 상기 음성 인식을 수행하는 단계는 상기 수신된 제1 오디오 신호의 파워 및 상기 수신된 제1 오디오 신호 의 신호 대 잡음비(Signal-to-Noise Ratio, SNR) 중 적어도 하나를 바탕으로 상기 수신된 제1 오디오 신호의 품 질 및 상기 수신된 제2 오디오 신호의 품질을 식별하는 단계 및 상기 식별된 제1 오디오 신호의 품질 및 상기 식별된 제2 오디오 신호의 품질을 바탕으로 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정합하는 단계를 포함할 수 있다. 한편, 상기 음성 인식을 수행하는 단계는 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 학습 된 인공지능 모델에 입력하여 상기 수신된 제1 오디오 신호의 음성 인식 결과에 대한 제1 확률 정보 및 상기 수 신된 제2 오디오 신호의 음성 인식 결과에 대한 제2 확률 정보를 획득하는 단계 및 상기 획득된 제1 확률 정보 및 상기 획득된 제2 확률 정보를 바탕으로 상기 수신된 제1 오디오 신호 및 상기 수신된 제2 오디오 신호를 정 합하는 단계를 포함할 수 있다. 한편, 상기 사용자의 이동은 상기 수신된 제1 오디오 신호의 파워 및 상기 수신된 제1 오디오 신호의 신호 대 잡음비 중 적어도 하나를 바탕으로 감지될 수 있다."}
{"patent_id": "10-2019-0030660", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 다양한 실시 예가 기재된다. 그러나, 이는 본 개시의 기술을 특정한 실시 형태에 대해 한정하 려는 것이 아니며, 본 개시의 실시 예들의 다양한 변경(modifications), 균등물(equivalents), 및/또는 대체물 (alternatives)을 포함하는 것으로 이해되어야 한다. 본 문서에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. 본 문서에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\"등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 문서에서 사용된 \"제 1,\" \"제 2,\" \"첫째,\" 또는 \"둘째,\" 등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들 을 한정하지 않는다. 예를 들면, 제 1 사용자 기기와 제 2 사용자 기기는, 순서 또는 중요도와 무관하게, 서로 다른 사용자 기기를 나타낼 수 있다. 예를 들면, 본 문서에 기재된 권리 범위를 벗어나지 않으면서 제 1 구성요 소는 제 2 구성요소로 명명될 수 있고, 유사하게 제 2 구성요소도 제 1 구성요소로 바꾸어 명명될 수 있다. 본 문서에서 사용된 \"모듈\", \"유닛\", \"부(part)\" 등과 같은 용어는 적어도 하나의 기능이나 동작을 수행하는 구 성요소를 지칭하기 위한 용어이며, 이러한 구성요소는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어 및 소 프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\", \"유닛\", \"부(part)\" 등은 각각이 개별적인 특정한 하드웨어로 구현될 필요가 있는 경우를 제외하고는, 적어도 하나의 모듈이나 칩으로 일체화되어 적어도 하나의 프로세서로 구현될 수 있다. 어떤 구성요소(예: 제 1 구성요소)가 다른 구성요소(예: 제 2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결 되어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제 3 구성요소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제 1 구성요소)가 다 른 구성요소(예: 제 2 구성요소)에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 상기 어 떤 구성요소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제 3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 문서에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for),\" \"~하는 능력을 가지는(having the capacity to),\" \"~하도록 설계된(designed to),\" \"~하도 록 변경된(adapted to),\" \"~하도록 만들어진(made to),\" 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장치가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행하도록 구성된(또는 설정된) 프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메 모리 장치에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 본 문서에서 사용된 용어들은 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 다른 실시예의 범위를 한정 하려는 의도가 아닐 수 있다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 문서에 기재된 기술 분야에서 통상 의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 문서에 사용된 용어들 중 일반적인 사전에 정의된 용어들은, 관련 기술의 문맥상 가지는 의미와 동일 또는 유사한 의미로 해석될 수 있으 며, 본 문서에서 명백하게 정의되지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 경우에 따라서, 본 문서에서 정의된 용어일지라도 본 문서의 실시예들을 배제하도록 해석될 수 없다. 이하에서는 도면을 참조하여 본 개시에 대해 더욱 상세히 설명하도록 한다. 다만, 본 개시를 설명함에 있어서, 관련된 공지 기능 혹은 구성에 대한 구체적인 설명이 본 개시의 요지를 불필요하게 흐릴 수 있다고 판단되는 경 우 그에 대한 상세한 설명은 생략한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조 부호가 사용될 수 있다. 도 1은 여러 대의 전자 장치를 통해 음성 인식을 수행하는 본 개시의 일 실시예를 설명하기 위한 도면이다. 도 1을 참조하면, 가정 내에 여러 대의 전자 장치(100-1, 100-2, 100-N)(모두 전자 장치로 지칭될 수 있다)가 비치될 수 있다. 전자 장치는 마이크를 포함하거나 마이크와 전기적으로 연결되고, 마이크를 통해 사용자의 음성을 획득할 수 있다. 본 개시의 일 실시 예에 따른 전자 장치는 트리거 워드(또는 웨이크업 워드)를 포함하는 사용자 음성에 의 해 음성 인식 대기 상태에서 음성 인식 기능이 활성화될 수 있다. 트리거 워드뿐만 아니라 전자 장치에 마 련된 특정 버튼을 누르면 음성 인식 기능이 활성화될 수 있다. 여기서 음성 인식 대기 상태는 마이크가 활성화되어 있고, 트리거 워드를 인식할 수 있는 모드이다. 음성 인식 대기 상태에서는 트리거 워드 이외의 것에 대한 인식은 수행되지 않을 수 있다. 따라서 적은 연산으로 음성 인 식을 하는 상태이다. 음성 인식 대기 상태는 음성 인식 대기 모드로 명명될 수도 있다. 본 개시의 일 실시 예에 따르면, 음성 인식 대기 상태에서 트리거 워드가 인식되면, 전자 장치의 음성 인 식 기능이 활성화될 수 있다. 음성 인식 기능이 활성화되면 마이크를 통해 입력되는 음성에 대해 음성 인식을 수행할 수 있다. 음성을 인식하기 위해서는 일련의 과정들이 포함될 수 있다. 예컨대, 음성을 녹음하여 오디오 신호를 획득하는 과정, 오디오 신호로부터 특징 정보를 획득하는 과정, 획득된 특징 정보와 음향 모델을 바탕으로 발음 (pronunciation) 정보, 음소(phoneme), 또는 문자열 정보를 획득하는 과정, 획득된 발음(pronunciation)정보, 음소(phoneme), 또는 문자열 정보에 대해 언어 모델을 바탕으로 텍스트 데이터를 획득하는 과정을 포함할 수 있 다. 구체적으로 전자 장치는 오디오 신호에 특징 추출 기술을 적용하여 입력된 음성 데이터에서 특징 정보를 획 득할 수 있다. 일 실시 예에서, 전자 장치는 오디오 신호에 켑스트럼(Cepstrum), 선형 예측 코딩(Linear Predictive Coefficient, LPC), 멜프리퀀시 켑스트럼(Mel Frequency Cepstral Coefficient, MFCC) 및 필터 뱅 크 에너지(Filter Bank Energy)를 포함하는 특징 추출 기술 중 어느 하나를 사용하여 입력된 오디오 신호의 특 징을 추출할 수 있다. 전술한 특징 획득 기술은 예시일 뿐이고, 본 개시에서 사용되는 특징 획득 기술이 전술 한 예시로 한정되는 것은 아니다. 본 개시의 일 실시 예에 따르면, 전자 장치는 음성 인식을 위한 일련의 과정들 전부를 수행할 수 있다. 본 개시의 또 다른 실시 예에 따르면, 전자 장치는 음성 인식을 위한 일련의 과정들 중 일부만을 수행하여 중 간 결과물을 획득하고, 중간 결과물을 외부 장치로 전달해서 나머지 과정들은 외부 장치에서 수행되도록 할 수 있다. 예컨대, 전자 장치는 음성 녹음만 수행해서 획득한 오디오 신호를 외부 장치로 전송하여 외부 장치 에서 나머지 음성 인식 과정이 진행될 수도 있다. 또 다른 예로, 전자 장치는 음향 모델을 바탕으로 발음 정보, 음소 정보 또는 문자열 정보를 획득하는 과정까지만 수행하고, 발음 정보, 혹은 음소 정보, 문자열 정보 를 외부 장치로 전송하여, 외부 장치에서 언어 모델을 바탕으로 텍스트 데이터를 획득하는 과정이 수행될 수 있 다. 본 개시에서 전자 장치가 음성 인식을 수행한다고 하는 것의 의미는, 전자 장치가 음성 인식을 위한 일련의 과정을 전부 수행하는 경우뿐만 아니라 전자 장치가 음성 인식을 위한 일련의 과정들 중 일부만을 수행하여 중간 결과물을 획득하고 중간 결과물을 외부 장치로 전달하는 경우도 포함한다. 그리고 본 개시에서 전자 장치가 음성 인식을 수행한 결과로서 획득한 음성 인식 정보는, 음성 인식을 위한 일련의 과정을 전 부 수행하여 얻어진 최종 결과물(ex. 텍스트 데이터), 또는 음성 인식을 위한 일련의 과정들 중 일부만을 수행 하여 얻어진 중간 결과물(ex. 오디오 신호, 특징 정보, 발음 정보, 음소 정보, 문자열 정보 등)을 의미할 수 있 다. 한편, 트리거 워드는 미리 정해진 단어, 문장일 수 있다. 예컨대 '하이 빅스비' 등이 사용될 수 있다. 예를 들 어, 사용자가 \"하이 빅스비, 오늘 날씨 알려줘\"라고 발화하면, 전자 장치는 음성 인식 대기 상태에서 \"하 이 빅스비\"를 인식하여 음성 인식 기능을 활성화시킬 수 있고, \"오늘 날씨 알려줘\"에 대한 음성 인식을 수행할 수 있다. 또한, 본 개시의 일 실시 예에 따른 전자 장치는 다른 전자 장치에서 음성 인식이 더 잘 수행될 수 있는 상황이라면, 그 다른 전자 장치에게 음성 인식 작업(speech recognition job)을 핸드 오버할 수 있다. 음성 인 식 작업을 핸드 오버한다는 것은, 기존에 음성 인식을 하고 있던 전자 장치와는 다른 전자 장치에게 음성 인식 을 이어서 수행하도록 제어하는 것을 의미한다. 예컨대, 음성 인식 작업을 핸드 오버하는 것은 음성 인식 기능을 활성화하도록 하는 제어 신호를 다른 전자 장치에게 전송하는 것을 포함한다. 또한, 음성 인식 작업을 핸드 오버하는 것은 전자 장치가 자신의 식별 정보(ex. 기기 ID) 및 입력된 음성에 대응하는 사용자 정보를 다른 전 자 장치에게 전송하는 것을 포함할 수 있다. 전송된 사용자 정보는 타 전자 장치에서 음성 인식을 이어할 사용 자의 음성을 식별하는데 이용될 수 있다. 타 전자 장치에는 사용자들에 대한 정보가 미리 등록되어 있어, 미리 등록된 사용자 정보와 수신된 사용자 정보를 비교할 수 있다. 또는, 등록되지 않은 사용자의 경우, 수신한 사용 자 정보와 현재 입력되는 음성으로부터 획득한 사용자 정보를 비교할 수도 있다. 도 1을 참조하면, 침실에 있는 제1 전자 장치(100-1)는 음성 인식 대기 상태에서 사용자로부터 트리거 워드(ex. 하이 빅스비)를 포함한 음성을 수신하면 음성 인식 기능을 활성화시킬 수 있다. 사용자는 트리거 워드를 말한 뒤에 침실에서 거실로 이동하면서 \"오늘 아침에 뭐 특별한 뉴스 있어?\"를 발화한다. 침실에 있는 제1 전자 장치 (100-1)는 사용자의 이동을 감지할 수 있다. 예컨대 사용자가 침실에서 거실로 나가면서 점점 제1 전자 장치 (100-1)로 입력되는 음성에 대응하는 오디오 신호 세기가 약해짐에 따라 제1 전자 장치(100-1)는 사용자의 이동 을 감지할 수 있다. 이와 같이 거실에 있는 제2 전자 장치(100-2)에서 음성 인식이 더 잘 될 수 있는 상황에서, 제1 전자 장치(100- 1)는 음성 인식 작업을 거실에 있는 제2 전자 장치(100-2)로 핸드 오버할 수 있다. 구체적으로, 제1 전자 장치 (100-1)로 입력되는 음성에 대응하는 오디오 신호 세기가 약해짐에 따라 제1 전자 장치는(100-1)는 사용자가 이 동하고 있음을 감지하고, 사용자의 이동 방향에 있는 제2 전자장치를 찾기 위한 정보를 브로드캐스팅 할 수 있 다. 그리고 제2 전자장치에서 사용자의 음성에 대응되는 오디오가 감지되었고 해당 신호에 대한 정보를 제 1 전 자장치에 송신할 수 있다. 이 경우, 제1 전자 장치(100-1)는 제2 전자 장치(100-2)로 음성 인식 기능을 활성화 시키기 위한 제어 신호를 전송할 수 있다. 즉, 이에 따라 제2 전자 장치(100-2)는 음성 인식 대기 상태에서 음 성 인식 기능을 활성화하고 사용자의 음성에 대한 인식을 수행할 수 있다. 이는 일 실시예에 불과할 뿐, 마이크 로 획득된 오디오 이외에도 카메라와 같은 다양한 센서를 획득된 정보를 이용하여 사용자의 이동을 감지하여, 이동방향에 있는 타 전자 장치를 식별하여 식별된 타 전자장치에 음성 인식 기능을 활성화시키기 위한 제어 신 호를 전송할 수 있다. 전자 장치가 사용자의 이동 방향에 있는 타 전자 장치를 식별하는 방법과 관련하여 선 도 18 내지 도 20을 참고하여 좀 더 설명하도록 한다. 제1 전자 장치(100-1)와 제2 전자 장치(100-2)에서 각각 획득된 음성 인식 정보가 정합(coordination)되어 전체 문장, 즉 \"오늘 아침에 뭐 특별한 뉴스 있어?\"가 인식될 수 있고, 이에 대한 응답(피드백)이 제1 전자 장치 (100-1)또는 제2 전자 장치(100-2)에서 제공될 수 있다. 사용자의 마지막 위치와 가까이 있는 제2 전자 장치 (100-2)에서 응답(피드백)이 제공되는 것이 바람직하다. 예컨대, \"오늘 아침의 특별한 뉴스는~~\"과 같은 음성 응답이 제2 전자 장치(100-2)의 스피커를 통해 제공될 수 있다. 타 전자 장치로 음성 인식 정보를 전송할 때, 전자 장치는 음성 인식 정보 정합에 사용될 추가 정보를 함 께 전송할 수 있다. 추가 정보는 시간 정보, 녹음 특성에 대한 정보, 음성 인식 진행 상황에 대한 정보 중 적어 도 하나를 포함할 수 있다. 여기서 시간 정보는 음성이 언제 입력되었는지를 알 수 있게 하는 정보이다. 시간 정보는 절대 시간 또는 상대 시간에 대한 정보일 수 있다. 최종 정합을 하는 장치에서 이와 같은 시간 정보를 바탕으로 시간 순서대로 음성 인식 정보를 정합할 수 있다. 녹음 특성에 대한 정보는 녹음을 수행한 장치의 마이크 특성, 주변 상황(주변 노이즈 등)을 알 수 있게 하는 정 보이다. 최종 정합을 하는 장치에서는 이러한 녹음 특성에 대한 정보를 바탕으로 적합한 노이즈 처리를 수행하 거나 적합한 음향 모델 또는 언어 모델을 적용할 수 있다. 음성 인식 진행 상황에 대한 정보는 음성 인식을 위한 일련의 과정들 중 어느 과정까지 수행하였는지를 알 수 있게 하는 정보이다. 예컨대, 음성 인식 진행 상황에 대한 정보는 오디오 신호만 전달하는지, 오디오 신호로부 터 특징 정보(특징 벡터)까지 추출하여 전달하는지, 특징 정보를 기초로 음향 모델 또는 언어 모델을 적용하여 전달하는지, 음향 모델, 언어 모델을 적용하여 획득한 텍스트 데이터를 전달하는지 등을 알 수 있게 하는 정보 를 포함할 수 있다. 이와 같이 최초에 음성 인식을 시작한 전자 장치가 자신을 도울 다른 전자 장치를 능동적으로 선택하여 음성 인 식 작업을 핸드 오버할 수 있고, 전자 장치들에서 각각 획득한 음성들이 정합되어 인식이 수행될 수 있다. 즉, 여러 대의 전자 장치를 통해 끊김 없이 음성 인식을 수행할 수 있다. 전자 장치는 예를 들면, 스마트폰(smartphone), 태블릿 PC(tablet personal computer), 이동 전화기 (mobile phone), 영상 전화기, 인공지능 스피커, 스피커(적어도 하나의 마이크를 구비한 스피커(인공지능 기능 이 탑재되지 않음)) 전자책 리더기(e-book reader), 데스크탑 PC(desktop personal computer), 랩탑 PC(laptop personal computer), 넷북 컴퓨터(netbook computer), 워크스테이션(workstation), 서버, PDA(personal digital assistant), PMP(portable multimedia player), MP3 플레이어, 모바일 의료기기, 카메라(camera), 또 는 웨어러블 장치(wearable device)로 구현될 수 있다. 웨어러블 장치는 액세서리형(예: 시계, 반지, 팔찌, 발 찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head-mounted-device(HMD)), 직물 또는 의류 일체형(예: 전자 의복), 신체 부착형(예: 스킨 패드(skin pad) 또는 문신), 또는 생체 이식형(예: implantable circuit) 중 적어도 하나를 포함할 수 있다. 어떤 실시 예들에서, 전자 장치는 가전 제품(home appliance)일 수 있다. 가전 제품은, 예를 들면, 텔레비 전, DVD(digital video disk) 플레이어, 오디오, 냉장고, 에어컨, 청소기, 오븐, 전자레인지, 세탁기, 공기 청 정기, 로봇청소기, 셋톱 박스(set-top box), 홈 오토매이션 컨트롤 패널(home automation control panel), 도 어락, 보안 컨트롤 패널(security control panel), TV 박스(예: 삼성 HomeSync쪠, 애플TV쪠, 또는 구글 TV쪠), 게임 콘솔(예: Xbox쪠, PlayStation쪠), 전자 사전, 전자 키, 캠코더(camcorder), 전자 액자 중 적어도 하나를 포함할 수 있다. 전자 장치는 사물 인터넷 장치(Internet of things)로 구현될 수 있다. 전자 장치는 전술한 다양한 장치들 중 하나 또는 그 이상의 조합일 수 있다. 또한, 전자 장치는 전술 한 기기들에 한정되지 않으며, 기술 발전에 따른 새로운 전자 장치를 포함할 수 있다. 여러 대의 전자 장치 중 적어도 일부는 같은 타입일 수 있고, 또는 각각의 전자 장치는 서로 다른 타 입일 수 있다. 상술한 바와 같이 음성 인식 작업을 핸드 오버하는 기능 및 여러 전자 장치에서 각각 획득된 음성들을 정 합하는 기능은 본 개시의 다양한 실시 예에 따른 음성 인식 시스템에서 구현될 수 있다. 음성 인식 시스템은 전 자 장치들을 포함할 수 있고, 추가로 전자 장치들과 연결될 수 있는 허브 장치, 서버 등을 더 포함할 수 있다. 이하에서 도 2 내지 도 5a를 참조하여 본 개시의 다양한 실시 예에 따른 음성 인식 시스템을 설명하도록 한다. 도 2를 참조하면, 음성 인식 시스템은 전자 장치들(100-1 ~ 100-N)(모두 전자 장치로 지칭될 수 있 다)을 포함할 수 있다. 전자 장치들은 무선 또는 유선 통신 방식으로 외부 장치와 통신할 수 있다. 예컨대, 전자 장치들은 무선 라우터 등과 같은 무선 엑세스 포인트에 연결되어 무선 엑세스 포인트를 통해 외부 장치와 통신할 수 있다. 또는, 전자 장치들은 와이파이 다이렉트(Wi-Fi Direct), 블루투스, ZigBee, Z-Wave 등의 근거리 무 선 통신방식으로 외부 장치와 통신할 수 있다. 전자 장치들 중 제1 전자 장치(100-1)에서 최초로 음성 인식 기능이 활성화되고, 음성 인식 기능이 활성화 된 동안 다른 장치에서 음성 인식을 이어서 수행해야 하는 상황과 관련된 특정 이벤트, 예컨대 사용자가 이동하 는 이벤트, 전자 장치(100-1) 주변의 노이즈가 기 설정된 정도 이상 발생하는 이벤트, 전자 장치(100-1)의 전원 이 곧 오프될 것이 예상되는 이벤트 등이 감지되면 제1 전자 장치(100-1)는 음성 인식 작업을 이어 받을 타 전 자 장치를 결정할 수 있다. 예컨대, 음성 인식 수행 작업을 이어 받을 전자 장치는 기 등록된 전자 장치(예컨대 사용자가 항상 휴대하는 모바일 기기) 또는 사용자가 이동하는 방향에 있는 전자 장치로 결정될 수 있다. 음성 인식 작업을 이어 받을 전자 장치를 결정하는 것과 관련해선 이하에서 좀 더 설명하도록 한다. 음성 인식 작업을 이어 받을 장치로서 제2 전자 장치(100-2)가 결정된 경우, 제1 전자 장치(100-1)는 제2 전자 장치(100-2)로 음성 인식 작업을 핸드 오버할 수 있다. 이 경우, 제1 전자 장치(100-1)는 제2 전자 장치(100- 2)로 음성 인식 기능을 활성화시키는 제어 신호를 전송할 수 있다. 제1 전자 장치(100-1)로부터 제어 신호를 수신한 제2 전자 장치(100-2)는 음성 인식 기능을 활성화할 수 있다. 그리고 사용자가 다시 다른 전자 장치로 이동하면 제2 전자 장치(100-2)는 제1 전자 장치(100-1)가 했던 것과 마찬가지 방식으로 음성 인식 작업을 다른 전자 장치로 핸드 오버할 수 있다. 또는, 제2 전자 장치(100-2)는 사용자의 발화 종료가 감지되면, 음성 인식 기능이 활성화된 시점부터 사용자의 발화 종료까지 획득한 사용자 음성에 대해 음성 인식을 수행하고, 음성 인식을 수행한 결과로서 음성 인식 정보 를 제1 전자 장치(100-1)로 전송할 수 있다. 제1 전자 장치(100-1)는 제1 전자 장치(100-1)에서 획득한 사용자 음성에 대해 음성 인식을 수행한 결과인 음성 인식 정보와, 제2 전자 장치(100-2)로부터 수신한 음성 인식 정보에 대해 시간 순서대로 정합을 수행할 수 있고, 정합하여 최종적으로 얻어진 음성 인식 결과를 기초로 태스크를 수행할 수 있다. 예컨대 제1 전자 장치 (100-1)와 제2 전자 장치(100-2)를 통해 획득한 사용자의 전체 음성이 \"오늘 날씨 어때?\"인 경우, 제1 전자 장 치(100-1)는 제2 전자 장치(100-2)로 하여금 \"오늘 날씨는 오후에 비가 올 예정입니다\"라는 음성 응답을 출력하 도록 하는 제어 신호를 제2 전자 장치(100-2)에 전송하는 태스크를 수행할 수 있다. 한편, 상술한 예에선 최초로 음성 인식 기능을 활성화한 제1 전자 장치(100-1)에서 정합을 수행하는 것으로 설 명하였으나, 전자 장치들(100-1 ~ 100-N) 중 어떤 전자 장치에서라도 최종 정합은 수행될 수 있다. 일 실시 예 에 따르면, 전자 장치들(100-1 ~ 100-N) 중 어떤 장치에서 정합을 수행할 것인지는 사용자의 발화가 시작되기 전에 미리 정해져 있을 수 있다. 예컨대, 사용자 발화의 종료를 감지한 전자 장치가 정합을 수행하기로 미리 정 해져 있는 경우, 사용자 발화의 종료를 감지한 전자 장치가 앞선 전자 장치들에게 음성 인식 정보, 사용자 정보, 정합을 위한 추가 정보 등을 요청할 수 있다. 또는 전자 장치가 음성 인식 작업을 타 전자 장치로 핸드오 버할 때부터 음성 인식 정보, 사용자 정보, 정합을 위한 추가 정보 등을 함께 전송할 수 있다. 또한 상기 전자 장치에서 정합을 수행한 후 음성 인식 작업을 수행하였으나, 음성 인식에 대한 스코어가 낮은 경우 타 전자장치 에서 음성 인식 작업을 재 수행하여 음성 인식 작업을 수행할 수 있다. 이 경우 음성을 인식하기 위한 일련의 과정 전부를 재 수행할 수도 있다. 또는 음성 인식을 위한 일련의 과정들 중 일부만을 재 수행할 수도 있다. 구 체적으로는 음향 모델에 대해서 적용된 음성 인식 정보에 대해서 언어모델을 적용하여 음성 인식을 위한 일련의 과정들 중 일부를 재 수행할 수도 있다.도 3은 본 개시의 또 다른 실시 예에 따른 음성 인식 시스템을 설 명하기 위한 도면이다. 음성 인식 시스템은 전자 장치들(100-1 ~ 100-N)(모두 전자 장치로 지칭될 수 있다) 및 서버를 포함할 수 있다. 전자 장치들은 적어도 하나의 네트워크를 통해 서버와 통신할 수 있다. 적어도 하나의 네트워크는 셀 룰러 네트워크, 무선 네트워크, 근거리 통신망 (LAN), 광역 통신망(WAN), 개인 영역 네트워크(PAN), 인터넷 등 과 같은 다수의 상이한 유형의 네트워크 중 임의의 하나 또는 조합을 포함할 수 있다. 전자 장치들은 무선 라우터와 같은 엑세스 포인트에 연결될 수 있다. 전자 장치들은 서버를 통하거나, D2D(Device-to-Device) 또는 P2P (Peer-to-Peer) 연결을 사용하여 서로 통신할 수 있다. 서버는 전자 장치들을 관리, 제어할 수 있다. 서버는 클라우드 서버로 구현될 수 있다. 서버는 하나의 서버로 구성되거나 복수개의 클라우드 서버로 구현될 수 있다. 서버는 앞서 도 2에서 전자 장치가 수행하는 것으로 설명한 음성 인식 작업을 핸드 오버하는 기능, 음성 인식 정보에 대한 정합 기능, 테스크 수행 기능 중 적어도 하나를 수행할 수 있다. 일 실시 예에 따르면, 전자 장치들 중 제1 전자 장치(100-1)에서 최초로 음성 인식 기능이 활성화되고, 제 1 전자 장치(100-1)는 음성 인식 기능이 활성화된 동안 획득한 사용자 음성에 대한 음성 인식을 수행해서 음성 인식 정보를 서버로 전송할 수 있다. 이후, 제1 전자 장치(100-1)에서 음성 인식 기능이 활성화된 동안 다 른 장치에서 음성 인식을 이어서 수행해야 하는 상황과 관련된 특정 이벤트가 감지되면 제1 전자 장치(100-1)는 음성 인식 수행을 이어 받을 전자 장치를 결정할 수 있다. 음성 인식 수행을 이어 받을 장치로서 제2 전자 장치(100-2)가 결정된 경우, 제1 전자 장치(100-1)는 제2 전자 장치(100-2)로 음성 인식 작업을 핸드 오버할 수 있다. 이 경우, 제1 전자 장치(100-1)는 제2 전자 장치(100- 2)로 음성 인식 기능을 활성화시키는 제어 신호를 전송할 수 있다. 제1 전자 장치(100-1)로부터 제어 신호를 수신한 제2 전자 장치(100-2)는 음성 인식 기능을 활성화할 수 있다. 제2 전자 장치(100-2)는 자신이 획득한 사용자 음성에 대한 인식 정보를 서버로 전송할 수 있다. 그리고 사용자가 다시 다른 전자 장치로 이동하면 제2 전자 장치(100-2)는 마찬가지 방식으로 음성 인식 작업을 다른 전자 장치로 핸드 오버할 수 있다. 사용자 음성이 마지막으로 입력된 n번째 전자 장치(100-N)에서 음성 인식 정보가 수신되면 서버는 전자 장 치들(100-1 ~ 100-N)로부터 수신한 음성 인식 정보들에 대해 시간 순서대로 정합을 수행할 수 있고, 정합하여 최종적으로 얻어진 음성 인식 결과를 기초로 태스크를 수행할 수 있다. 예컨대 전자 장치들(100-1 ~ 100-N)을 통해 획득한 사용자의 전체 음성이 \"오늘 날씨 어때?\"인 경우, 서버는 n번째 전자 장치(100-N)로 하여금 \"오늘 날씨는 오후에 비가 올 예정입니다\"라는 음성 응답을 출력하도록 하는 제어 신호를 n번째 전자 장치(100-N)에 전송하는 태스크를 수행할 수 있다. 도 4는 본 개시의 또 다른 실시 예에 따른 음성 인식 시스템을 설명하기 위한 도면이다. 음성 인식 시스템은 전자 장치들(100-1 ~ 100-N)(모두 전자 장치로 지칭될 수 있다) 및 허브 장치 를 포함할 수 있다. 허브 장치는 하나의 허브 장치로 구성될 수도 있고, 복수 개의 허브 장치로 구성 되어 각각의 전자 장치와 연결되어 구성될 수 있다. 예를 들면 5개의 전자장치와 2개의 허브 장치가 있는 경우, 3개의 전자장치는 제 1 허브 장치에 연결되고 2개의 전자장치는 제 2 허브 장치에 연결될 수 있다. 또한 5개의 전자장치가 제 1허브 장치에 연결되고 2개의 전자장치는 제 2허브 장치에 연결될 수 있다. 이러한 연결 여부는 사용자가 설정한 방식에 따라서 다양한 연결 방법으로 구성될 수 있다. 전자 장치들과 허브 장치는 무선 또는 유선 통신 방식으로 외부 장치와 통신할 수 있다. 예컨대, 전 자 장치들과 허브 장치는 무선 라우터 등과 같은 무선 엑세스 포인트에 연결되어 무선 엑세스 포인트 를 통해 외부 장치와 통신할 수 있다. 또는, 전자 장치들과 허브 장치는 와이파이 다이렉트(Wi-Fi Direct), 블루투스, ZigBee, Z-Wave 등의 근거리 무선 통신방식으로 외부 장치와 통신할 수 있다. 본 음성 인식 시스템에서의 통신은 허브 장치를 중심으로 하는 중심 방식(centralized fashion)으로 이루어질 수 있다. 예컨대, 전자 장치들은 허브 장치를 통해 외부 장치와 통신할 수 있다. 물론 허브 장치를 통하지 않고도 전자 장치들(100-1 ~ 100-N)이 외부 장치와 통신하는 것도 가능하다. 허브 장치는 전자 장치들을 관리, 제어할 수 있다. 허브 장치는 홈 게이트 웨이일 수 있다. 허 브 장치는 다양한 타입의 장치로 구현될 수 있다. 허브 장치는 예를 들면, 스마트폰(smartphone), 태블릿 PC(tablet personal computer), 이동 전화기 (mobile phone), 영상 전화기, 인공지능 스피커, 전자책 리더기(e-book reader), 데스크탑 PC(desktop personal computer), 랩탑 PC(laptop personal computer), 넷북 컴퓨터(netbook computer), 워크스테이션 (workstation), 서버, PDA(personal digital assistant), PMP(portable multimedia player), MP3 플레이어, 모바일 의료기기, 카메라(camera), 또는 웨어러블 장치(wearable device)로 구현될 수 있다. 웨어러블 장치는 액세서리형(예: 시계, 반지, 팔찌, 발찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head-mounted- device(HMD)), 직물 또는 의류 일체형(예: 전자 의복), 신체 부착형(예: 스킨 패드(skin pad) 또는 문신), 또는 생체 이식형(예: implantable circuit) 중 적어도 하나를 포함할 수 있다. 어떤 실시 예들에서, 허브 장치는 가전 제품(home appliance)일 수 있다. 가전 제품은, 예를 들면, 텔레비 전, DVD(digital video disk) 플레이어, 오디오, 냉장고, 에어컨, 청소기, 오븐, 전자레인지, 세탁기, 공기 청 정기, 로봇청소기, 셋톱 박스(set-top box), 홈 오토매이션 컨트롤 패널(home automation control panel), 도 어락, 보안 컨트롤 패널(security control panel), TV 박스(예: 삼성 HomeSync쪠, 애플TV쪠, 또는 구글 TV쪠), 게임 콘솔(예: Xbox쪠, PlayStation쪠), 전자 사전, 전자 키, 캠코더(camcorder), 전자 액자 중 적어도 하나를 포함할 수 있다. 허브 장치는 사물 인터넷 장치(Internet of things), 엣지 컴퓨팅 장치(Edge Computing Device)로 구현될 수 있다. 는 허브 장치는 앞서 도 2에서 전자 장치가 수행하는 것으로 설명한 음성 인식 작업을 핸드 오버하는 기 능, 음성 인식 정보에 대한 정합 기능, 태스크 수행 기능 중 적어도 하나를 수행할 수 있다. 일 실시 예에 따르면, 전자 장치들 중 제1 전자 장치(100-1)에서 최초로 음성 인식 기능이 활성화되고, 제 1 전자 장치(100-1)는 음성 인식 기능이 활성화된 동안 획득한 사용자 음성에 대한 음성 인식을 수행해서 음성 인식 정보를 허브 장치로 전송할 수 있다. 이후, 제1 전자 장치(100-1)에서 음성 인식 기능이 활성화된 동 안 다른 장치에서 음성 인식을 이어서 수행해야 하는 상황과 관련된 특정 이벤트가 감지되면 허브 장치는 음성 인식 수행을 이어 받을 전자 장치를 결정할 수 있다. 예컨대, 허브 장치는 전자 장치들로부터 수신한 음성 신호의 SNR(signal-to-noise ratio) 및/또는 신호 크기(ex. amplitude)를 기반으로 사용자의 이동을 감지할 수 있다. 예컨대, 사용자가 제1 전자 장치(100- 1)에서 제2 전자 장치(100-2)로 이동하면서 말하는 경우, 제1 전자 장치(100-1)로부터 수신한 음성 신호의 SNR 및/또는 크기는 점점 작아지고 제2 전자 장치(100-2)로부터 수신한 음성 신호 SNR 및/또는 크기는 점점 커질 것 이다. SNR과 신호 크기 중 적어도 하나를 바탕으로 허브 장치는 사용자가 제1 전자 장치(100-1)에서 제2 전자 장치(100-2)로 이동함을 감지할 수 있고, 이에 따라 허브 장치는 음성 인식 수행을 이어 받을 전자 장치로서 제2 전자 장치(100-2)를 결정할 수 있다. 한편, SNR은 음성 신호의 질에 대한 일 예시로써, SPL(SoundPressure Level) 등 다른 음성 신호에 대한 질을 평가하는 파라메터가 이용될 수도 있다. 음성 인식 수행을 이어 받을 장치로서 제2 전자 장치(100-2)가 결정된 경우, 허브 장치는 제2 전자 장치 (100-2)로 음성 인식 작업을 핸드 오버할 수 있다. 이 경우, 허브 장치는 제2 전자 장치(100-2)로 음성 인 식 기능을 활성화시키는 제어 신호를 전송할 수 있다. 허브 장치로부터 제어 신호를 수신한 제2 전자 장치(100-2)는 음성 인식 기능을 활성화할 수 있다. 제2 전 자 장치(100-2)는 자신이 획득한 사용자 음성에 대한 인식 정보를 허브 장치로 전송할 수 있다. 그리고 사 용자가 다시 다른 전자 장치로 이동하면 허브 장치는 마찬가지 방식으로 음성 인식 작업을 다른 전자 장치 로 핸드 오버할 수 있다. 사용자 음성이 마지막으로 입력된 n번째 전자 장치(100-N)에서 음성 인식 정보가 수신되면 허브 장치는 전 자 장치들(100-1 ~ 100-N)로부터 수신한 음성 인식 정보들에 대해 시간 순서대로 정합을 수행할 수 있고, 정합 하여 최종적으로 얻어진 음성 인식 결과를 기초로 태스크를 수행할 수 있다. 예컨대 전자 장치들(100-1 ~ 100- N)을 통해 획득한 사용자의 전체 음성이 \"오늘 날씨 어때?\"인 경우, 허브 장치는 n번째 전자 장치(100- N)로 하여금 \"오늘 날씨는 오후에 비가 올 예정입니다\"라는 음성 응답을 출력하도록 하는 제어 신호를 n번째 전 자 장치(100-N)에 전송하는 태스크를 수행할 수 있다. 태스크의 예시들로서, 기기제어에 대한 음성 명령을 사용자가 내린 경우, 예컨대, 사용자 음성 명령이 \"에어컨 켜줘\"이면, 태스크 수행을 위해 에어컨 켜줘에 대한 제어 명령을 생성하여 에어컨에 전달하고 에어컨에서 \"에어 컨이 켜졌습니다.\" 라고 음성 응답이 제공될 수 있다. 또 다른 예로, 사용자 음성 명령이 \"에어컨 켜줘\"이고, 태스크 수행을 위해 \"에어컨 켜줘\"에 대해 제어 명령을 생성하여 에어컨에 전달하고, 에어컨이 아닌 타 전자 장 치 (사용자와 가까운 장치 혹은 음성이 마지막으로 입력된 전자장치 혹은 다른 전자장치) 에서 \"에어컨이 켜졌 습니다\"라고 음성 응답 제공될 수 있다. 도 5a는 본 개시의 또 다른 실시 예에 따른 음성 인식 시스템을 설명하기 위한 도면이다. 음성 인식 시스템은 전자 장치들(100-1 ~ 100-N)(모두 전자 장치로 지칭될 수 있다), 허브 장치 및 서버를 포함할 수 있다. 전자 장치들, 허브 장치 및 서버는 무선 또는 유선 통신 방식으로 외부 장치와 통신할 수 있다. 예컨대, 전자 장치들은 허브 장치를 통해 서버와 통신할 수 있다. 도 4의 음성 인식 시스템과 비교하여 도 5a의 음성 인식 시스템은 허브 장치의 역할 중 일부 를 서버가 담당할 수 있다. 예컨대, 도 4의 음성 인식 시스템의 허브 장치가 수행하는 음성 인 식 정보에 대한 정합 기능, 태스크 수행 기능 중 적어도 하나를 서버가 수행할 수 있다. 도 5b는 본 개시의 또 다른 실시 예에 따른 음성 인식 시스템을 설명하기 위한 도면이다. 도 5b의 음성 인식 시스템은 도 2의 음성 인식 시스템, 도 3의 음성 인식 시스템 및 도 4의 음성 인식 시스템이 하이브리드되어 연결된 형태이다. 예컨대 도 5b를 참고하면, 음성 인식 시스템 내의 전자 장치들 중 일부 전자 장치(100-1)는 음성 인식을 위한 일련의 과정들 전부를 수행할 수 있고, 다른 일부 전자 장치(100-2)는 음성 인식을 위한 일련의 과정들 중 일부만을 수행하고 그 결과물을 허브 장치로 전송해서 허브 장치에서 나머지 일부가 수행되도록 할 수 있고, 또 다른 일부 전자 장치(100-3)는 음성 인식을 위한 일련의 과정들 중 일부만을 수행하고 그 결과물을 서버로 전송해서 서버에서 나머지 일부가 수행되도록 할 수 있다. 본 음성 인식 시스템의 전자 장치, 허브 및 서버 중 적어도 하나는 앞서 설명한 음성 인 식 작업을 핸드 오버하는 기능, 음성 인식 정보에 대한 정합 기능, 태스크 수행 기능 중 적어도 하나를 수행할 수 있다. 이하에서는 도 6 내지 도 9를 참조하여 상술한 다양한 실시 예들에 따른 음성 인식 시스템(1000 내지 4000)에서 의 음성 인식 방법을 설명하도록 한다. 도 6은 본 개시의 일 실시 예에 따른 음성 인식 시스템에서의 음성 인식 방법을 설명하기 위한 흐름도이 다. 도 6을 참고하면, 음성 인식 시스템은 제1 전자 장치(100-1)와 제2 전자 장치(100-2)를 포함할 수 있다. 먼저, 제1 전자 장치(100-1)와 제2 전자 장치(100-2)는 음성 인식 대기 상태에 있다(S610). 이후 사용자가 제1 전자 장치(100-1) 근처에서 트리거 워드를 포함하는 음성을 발화하는 경우, 제1 전자 장치(100-1)는 사용자 음 성에 포함된 트리거 워드를 인식할 수 있다(S620). 제1 전자 장치(100-1)는 사용자 음성에 포함된 트리거 워드 를 인식하면 음성 인식 기능을 활성화할 수 있다(S630). 그리고 제1 전자 장치(100-1)는 음성 인식 기능이 활성 화된 상태에서 입력되는 사용자 음성에 대한 음성 인식을 수행할 수 있다(S640). 트리거 워드 인식 이외에도, 사용자의 수동 조작에 의해 음성 인식 기능이 활성화될 수 있다. 예컨대, 제1 전자 장치(100-1)에 마련된 특정 버튼이 선택되면 음성 인식 기능이 활성화될 수 있다. 제1 전자 장치(100-1)는 음성 인식 기능이 활성화된 상태에서 다른 장치에서 음성 인식을 이어서 수행해야 하는 상황과 관련된 특정 이벤트를 감지할 수 있다(S650). 일 실시 예에 따르면, 제1 전자 장치(100-1)는 복수의 마 이크를 구비할 수 있고, 복수의 마이크를 통해 입력되는 사용자 음성의 크기 차이를 통해 사용자가 이동하는 이 벤트를 감지할 수 있다. 또 다른 실시 예에 따르면, 제1 전자 장치(100-1)는 카메라를 포함할 수 있고, 카메라 를 통해 획득한 영상을 바탕으로 사용자가 이동하는 이벤트를 감지할 수 있다. 제1 전자 장치(100-1)는 다른 장치에서 음성 인식을 이어서 수행해야 하는 상황과 관련된 특정 이벤트가 감지되 면 사용자의 이동 방향에 있는 제2 전자 장치(100-2)로 음성 인식 작업을 핸드 오버하기 위해 음성 인식 기능을 활성화시키는 제어신호를 전송할 수 있다(S660). 이 때, 제1 전자 장치(100-1)는 정합에 사용될 추가 정보, 사 용자 정보 등도 함께 전송할 수 있다. 사용자 정보는 화자가 어떤 사용자인지를 알 수 있게 하는 정보로서, 예컨대 사용자 식별정보(id), 사용자 이름, 사용자 계정 정보, 화자 인식을 위한 음성으로부터 획득된 특징정보 등 다양한 사용자 정보를 포함할 수 있다. 이러한 사용자 정보는, 동일 사용자의 음성 정보를 구별하여 정합하기 위해 사용될 수 있다. 예컨대, 제2 전자 장치(100-2)는 제1 전자 장치(100-1)로부터 수신한 사용자 정보와 현재 입력되는 음성으로부터 획득한 사 용자 정보를 비교해서 사용자가 동일한지 여부를 판단하고, 동일 사용자의 음성 정보를 정합할 수 있다. 제어 신호를 수신한 제2 전자 장치(100-2)는 음성 인식 기능을 활성화할 수 있다(S670). 그리고 제2 전자 장치(100- 2)는 음성 인식 기능이 활성화된 상태에서 입력되는 사용자 음성에 대한 음성 인식을 수행할 수 있다(S680). 이 경우, 제2 전자 장치(100-2)는 음성을 인식하기 위한 일련의 과정들 전부를 수행하거나, 또는 음성을 인식하 기 위한 일련의 과정들 중 일부만을 수행할 수 있다. 예컨대, 제2 전자 장치(100-2)는 음성 녹음을 수행해서 오 디오 신호를 획득하는 과정만을 수행할 수 있다. 또 다른 예로, 제2 전자 장치(100-2)는 음향 모델을 바탕으로 발음 정보, 음소 정보 또는 문자열 정보를 획득하는 과정까지 수행할 수 있다. 최종 정합을 수행할 장치로 제1 전자 장치(100-1)가 결정된 경우, 제2 전자 장치(100-2)는 음성 인식 수행에 따 라 획득된 음성 인식 정보를 제1 전자 장치에 전송할 수 있다(S690). 이 때, S680 단계에서 음성을 인식하 기 위한 일련의 과정들 전부를 수행한 경우에는 음성 인식 정보는 최종 결과물(ex. 사용자 음성에 대응하는 텍 스트 데이터)을 포함할 수 있다. S680 단계에서 음성을 인식하기 위한 일련의 과정들 중 일부만을 수행한 경우 에는 음성 인식 정보는 중간 결과물(ex. 오디오 신호, 오디오 신호로부터 획득된 특징 정보, 발음 정보, 음소 정보, 문자열 정보 등)을 포함할 수 있다. 그리고 제2 전자 장치(100-2)는 정합에 사용될 추가 정보, 사용자 정 보 등도 함께 제1 전자 장치(100-1)로 전송할 수 있다. 제1 전자 장치(100-1)는 S640 단계에서 수행한 음성 인식을 통해 얻은 음성 인식 정보와 제2 전자 장치(100- 2)로부터 수신한 음성 인식 정보를 시간 순서로 정합할 수 있다(S693). 이 때, 제2 전자 장치(100-2)로부터 수 신한 음성 인식 정보가 중간 결과물인 경우에는 제1 전자 장치(100-1)는 음성 인식을 위한 나머지 과정을 수행 하여 최종 결과물을 획득하고 최종 결과물을 바탕으로 정합을 수행할 수 있다. 그리고 제1 전자 장치(100-1)는 정합 결과로 획득된 전체 사용자 음성에 대한 최종 인식 결과를 바탕으로 태스 크를 수행할 수 있다(S695). 한편, 도 6의 흐름도에서는 제1 전자 장치(100-1)에서 정합이 수행되는 것으로 설명하였으나, 제2 전자 장치 (100-2)에서 정합이 수행되는 것도 가능하다. 예컨대, 제2 전자 장치(100-2)가 S690 단계에서 음성 인식 정보를 제1 전자 장치(100-1)로 전송하는 대신, 제2 전자 장치(100-2)는 음성 인식 정보를 전송해줄 것을 요청하는 신 호를 제1 전자 장치(100-1)로 전송할 수 있다. 그러면 제1 전자 장치(100-1)는 S640 단계에서 수행한 음성 인식 을 통해 얻은 음성 인식 정보를 제2 전자 장치(100-2)로 전송할 수 있다. 또는 제1 전자 장치(100-1)가 S660 단 계에서 음성 인식 정보를 제2 전자 장치(100-2)로 전송하는 것도 가능하다. 제2 전자 장치(100-2)는 제1 전자 장치(100-1)로부터 수신한 음성 인식 정보와 S680 단계에서 수행한 음성 인식을 통해 얻은 음성 인식 정보를 시간 순서로 정합할 수 있다. 그리고 제2 전자 장치(100-2)가 태스크를 수행할 수 있다. 한편, 상술한 실시 예에선 동일 전자 장치에서 정합과 태스크 수행이 이루어지는 것으로 설명하였으나, 정합을 수행한 전자 장치가 정합 결과로 획득된 전체 사용자 음성에 대한 최종 인식 결과에 대한 정보를 타 전자 장치 로 제공할 수 있고, 타 전자 장치가 최종 인식 결과에 대한 정보를 바탕으로 태스크를 수행하는 것도 가능하다. 도 7은 본 개시의 또 다른 실시 예에 따른 음성 인식 시스템에서의 음성 인식 방법을 설명하기 위한 흐름 도이다. 도 7을 참고하면, 음성 인식 시스템은 제1 전자 장치(100-1), 제2 전자 장치(100-2) 및 서버를 포함 할 수 있다. 먼저, 제1 전자 장치(100-1)와 제2 전자 장치(100-2)는 음성 인식 대기 상태에 있다(S710). 이후 사용자가 제1 전자 장치(100-1) 근처에서 트리거 워드를 포함하는 음성을 발화하는 경우, 제1 전자 장치(100-1)는 사용자 음 성에 포함된 트리거 워드를 인식할 수 있다(S720). 제1 전자 장치(100-1)는 사용자 음성에 포함된 트리거 워드를 인식하면 음성 인식 기능을 활성화할 수 있다 (S730). 트리거 워드 인식 이외에도, 사용자의 수동 조작에 의해 음성 인식 기능이 활성화될 수 있다. 예컨대, 제1 전자 장치(100-1)에 마련된 특정 버튼이 선택되면 음성 인식 기능이 활성화될 수 있다. 그리고 제1 전자 장치(100-1)는 음성 인식 기능이 활성화된 상태에서 입력되는 사용자 음성에 대한 음성 인식을 수행할 수 있다(S740). 이 경우, 제1 전자 장치(100-1)는 음성을 인식하기 위한 일련의 과정들 전부를 수행하거 나, 또는 음성을 인식하기 위한 일련의 과정들 중 일부만을 수행할 수 있다. 예컨대, 제1 전자 장치(100-1)는 음성 녹음을 수행해서 오디오 신호를 획득하는 과정만을 수행할 수 있다. 또 다른 예로, 제1 전자 장치(100- 1)는 음향 모델을 바탕으로 발음 정보, 음소 정보 또는 문자열 정보를 획득하는 과정까지 수행할 수 있다. 그리고 제1 전자 장치(100-1)는 음성 인식 수행에 따라 획득된 음성 인식 정보를 서버에 전송할 수 있다 (S745). 이 때, S740 단계에서 음성을 인식하기 위한 일련의 과정들 전부를 수행한 경우에는 음성 인식 정보는 최종 결과물(ex. 오디오 신호에 대응하는 텍스트 데이터)을 포함할 수 있다. S740 단계에서 음성을 인식하기 위 한 일련의 과정들 중 일부만을 수행한 경우에는 음성 인식 정보는 중간 결과물을 포함할 수 있다. 예컨대 중간 결과물은, 음성 녹음만 수행해서 획득한 오디오 신호일 수 있다. 또는, 중간 결과물은 오디오 신호로부터 획득 된 특징 정보일 수 있다. 또는, 중간 결과물은 음향 모델을 바탕으로 획득한 발음 정보, 음소 정보 또는 문자열 정보일 수 있다. 이 때, 제1 전자 장치(100-1)는 정합에 사용될 추가 정보, 사용자 정보 등도 함께 전송할 수 있다. 이와 같이 전자 장치와 서버는 음성 인식을 위한 과정들을 서로 나누어 수행할 수 있는바, 전자 장치 측에서의 연산 부담이 덜어질 수 있다. 그리고 제1 전자 장치(100-1)는 음성 인식 기능이 활성화된 상태에서 다른 장치에서 음성 인식을 이어서 수행해 야 하는 상황과 관련된 특정 이벤트를 감지할 수 있다(S750). 예컨대, 제1 전자 장치(100-1)는 사용자가 이동하 는 이벤트가 감지되면 사용자의 이동 방향에 있는 제2 전자 장치(100-2)로 음성 인식 작업을 핸드 오버하기 위 해 음성 인식 기능을 활성화시키는 제어신호를 전송할 수 있다(S755). 한편, 본 개시의 또 다른 실시 예에 따르면, 제1 전자 장치(100-1) 대신에 서버가 음성 인식 작업을 핸드 오버하는 동작을 수행할 수 있다. 예컨대, 서버는 사용자의 이동을 감지할 수 있는 장치(ex. 전자 장치들 또는 그 밖의 다른 장치(ex. 센서 등))로부터 사용자 이동과 관련한 신호를 수신할 수 있고, 이를 바탕으 로 사용자가 이동하는 이벤트를 감지할 수 있고, 제2 전자 장치(100-2)로 음성 인식 작업을 핸드 오버하기 위해 음성 인식 기능을 활성화시키는 제어신호를 전송할 수 있다. 제어 신호를 수신한 제2 전자 장치(100-2)는 음성 인식 기능을 활성화할 수 있다(S760). 그리고 제2 전자 장치 (100-2)는 음성 인식 기능이 활성화된 상태에서 입력되는 사용자 음성에 대한 음성 인식을 수행할 수 있다 (S765). 이 경우, 제2 전자 장치(100-2)는 음성을 인식하기 위한 일련의 과정들 전부를 수행하거나, 또는 음성 을 인식하기 위한 일련의 과정들 중 일부만을 수행할 수 있다. 그리고 제2 전자 장치(100-2)는 음성 인식 수행에 따라 획득된 음성 인식 정보를 서버에 전송할 수 있다 (S770). 이 때, S765 단계에서 음성을 인식하기 위한 일련의 과정들 전부를 수행한 경우에는 음성 인식 정보는최종 결과물(ex. 오디오 신호에 대응하는 텍스트 데이터)을 포함할 수 있다. S765 단계에서 음성을 인식하기 위 한 일련의 과정들 중 일부만을 수행한 경우에는 음성 인식 정보는 중간 결과물(ex. 오디오 신호, 오디오 신호로 부터 획득된 특징 정보, 발음 정보, 음소 정보, 문자열 정보 등)을 포함할 수 있다. 이 때, 제2 전자 장치(100- 2)는 정합에 사용될 추가 정보, 사용자 정보 등도 함께 전송할 수 있다. 서버는 제1 전자 장치(100-1)로부터 수신한 음성 인식 정보와 제2 전자 장치(100-2)로부터 수신한 음성 인 식 정보를 시간 순서로 정합할 수 있다(S780). 이 때, 제1 전자 장치(100-1)로부터 수신한 음성 인식 정보와 제 2 전자 장치(100-2)로부터 수신한 음성 인식 정보 중 적어도 하나가 중간 결과물인 경우에는 서버는 음성 인식을 위한 나머지 과정들을 수행해서 최종 결과물을 획득하고 최종 결과물을 바탕으로 정합을 수행할 수 있다. 그리고 서버는 정합 결과로 획득된 전체 사용자 음성에 대한 최종 인식 결과를 바탕으로 태스크를 수행할 수 있다(S790). 예컨대 제1 전자 장치(100-1)와 제2 전자 장치(100-2)를 통해 획득한 사용자의 전체 음성이 \"오늘 날씨 어때?\" 인 경우, 서버는 제2 전자 장치(100-2)로 하여금 \"오늘 날씨는 오후에 비가 올 예정입니다\"라는 음성 응답 을 출력하도록 하는 제어 신호를 제2 전자 장치(100-2)로 전송하는 태스크를 수행할 수 있다. 한편, 도 7의 흐름도에서는 서버에서 정합이 수행되는 것으로 설명하였으나, 제1 전자 장치(100-1) 또는 제2 전자 장치(100-2)에서 정합이 수행되는 것도 가능하다. 예컨대, 서버는 제1 전자 장치(100-1) 및 제2 전자 장치(100-2) 각각에서 획득한 사용자 음성에 대해 인식을 수행하여 음성 인식 정보들을 획득하고 획득된 음성 인식 정보들을 제1 전자 장치(100-1) 또는 제2 전자 장치(100-2)로 전송할 수 있고, 제1 전자 장치(100-1) 또는 제2 전자 장치(100-2)에서 음성 인식 정보들에 대한 정합이 수행되는 것이 가능하다. 그리고 제1 전자 장 치(100-1) 또는 제2 전자 장치(100-2)에서 태스크를 수행할 수 있다. 도 8a는 본 개시의 또 다른 실시 예에 따른 음성 인식 시스템에서의 음성 인식 방법을 설명하기 위한 흐 름도이다. 도 8a를 참고하면, 음성 인식 시스템은 제1 전자 장치(100-1), 제2 전자 장치(100-2) 및 허브 장치(20 0)를 포함할 수 있다. 먼저, 제1 전자 장치(100-1)와 제2 전자 장치(100-2)는 음성 인식 대기 상태에 있다(S810). 이후 사용자가 제1 전자 장치(100-1) 근처에서 트리거 워드를 포함하는 음성을 발화하는 경우, 제1 전자 장치(100-1)는 사용자 음 성에 포함된 트리거 워드를 인식할 수 있다(S820). 트리거 워드 인식 이외에도, 사용자의 수동 조작에 의해 음 성 인식 기능이 활성화될 수 있다. 예컨대, 제1 전자 장치(100-1)에 마련된 특정 버튼이 선택되면 음성 인식 기 능이 활성화될 수 있다. 제1 전자 장치(100-1)는 사용자 음성에 포함된 트리거 워드를 인식하면 음성 인식 기능을 활성화할 수 있다 (S830). 그리고 제1 전자 장치(100-1)는 음성 인식 기능이 활성화된 상태에서 입력되는 사용자 음성에 대한 음 성 인식을 수행할 수 있다(S840). 이 경우, 제1 전자 장치(100-1)는 음성을 인식하기 위한 일련의 과정들 전부 를 수행하거나, 또는 음성을 인식하기 위한 일련의 과정들 중 일부만을 수행할 수 있다. 예컨대, 제1 전자 장치 (100-1)는 음성 녹음을 수행해서 오디오 신호를 획득하는 과정만을 수행할 수 있다. 또 다른 예로, 또 다른 예 로, 제1 전자 장치(100-1)는 음향 모델을 바탕으로 발음 정보, 음소 정보 또는 문자열 정보를 획득하는 과정까 지 수행할 수 있다. 그리고 제1 전자 장치(100-1)는 음성 인식 수행에 따라 획득된 음성 인식 정보를 허브 장치에 전송할 수 있다(S845). 이 때, S840 단계에서 음성을 인식하기 위한 일련의 과정들 전부를 수행한 경우에는 음성 인식 정 보는 최종 결과물(ex. 오디오 신호에 대응하는 텍스트 데이터)을 포함할 수 있다. S840 단계에서 음성을 인식하 기 위한 일련의 과정들 중 일부만을 수행한 경우에는 음성 인식 정보는 중간 결과물(ex. 오디오 신호, 오디오 신호로부터 획득된 특징 정보, 발음 정보, 음소 정보, 문자열 정보 등)을 포함할 수 있다. 이 때, 제1 전자 장 치(100-1)는 정합에 사용될 추가 정보, 사용자 정보 등도 함께 전송할 수 있다. 이와 같이 전자 장치와 허브 장치는 음성 인식을 위한 과정들을 서로 나누어 수행할 수 있는바, 전자 장치 측에서의 연산 부담이 덜어질 수 있다. 제1 전자 장치(100-1)의 음성 인식 기능이 활성화된 동안 허브 장치는 다른 장치에서 음성 인식을 이어서 수행해야 하는 상황과 관련된 특정 이벤트를 감지할 수 있다(S850). 예컨대, 허브 장치는 사용자가 이동하는 이벤트가 감지되면, 사용자의 이동 방향에 있는 제2 전자 장치(100-2)로 음성 인식 작업을 핸드 오버하기 위 해 음성 인식 기능을 활성화시키는 제어신호를 전송할 수 있다(S855). 예컨대 허브 장치는 사용자의 이동 을 감지할 수 있는 장치(ex. 전자 장치들 또는 그 밖의 다른 장치(ex. 센서 등))로부터 사용자 이동과 관 련한 신호를 수신할 수 있고, 이를 바탕으로 사용자가 이동하는 이벤트를 감지할 수 있고, 제2 전자 장치(100- 2)로 음성 인식 작업을 핸드 오버하기 위해 음성 인식 기능을 활성화시키는 제어신호를 전송할 수 있다. 한편, 본 개시의 또 다른 실시 예에 따르면, 허브 장치 대신에 제1 전자 장치(100-1)가 음성 인식 작업을 핸드 오버하는 동작을 수행할 수 있다. 예컨대, 제1 전자 장치(100-1)는 복수의 마이크, 카메라를 통해 사용자 의 이동을 감지할 수 있고, 이를 바탕으로 사용자가 이동하는 이벤트를 감지할 수 있고, 제2 전자 장치(100- 2)로 음성 인식 작업을 핸드 오버하기 위해 음성 인식 기능을 활성화시키는 제어신호를 전송할 수 있다. 제어 신호를 수신한 제2 전자 장치(100-2)는 음성 인식 기능을 활성화할 수 있다(S860). 그리고 제2 전자 장치 (100-2)는 음성 인식 기능이 활성화된 상태에서 입력되는 사용자 음성에 대한 음성 인식을 수행할 수 있다 (S865). 이 경우, 제2 전자 장치(100-2)는 음성을 인식하기 위한 일련의 과정들 전부를 수행하거나, 또는 음성 을 인식하기 위한 일련의 과정들 중 일부만을 수행할 수 있다. 그리고 제2 전자 장치(100-2)는 음성 인식 수행에 따라 획득된 음성 인식 정보를 허브 장치에 전송할 수 있다(S870). 이 때, S865 단계에서 음성을 인식하기 위한 일련의 과정들 전부를 수행한 경우에는 음성 인식 정 보는 최종 결과물(ex. 오디오 신호에 대응하는 텍스트 데이터)을 포함할 수 있다. S865 단계에서 음성을 인식하 기 위한 일련의 과정들 중 일부만을 수행한 경우에는 음성 인식 정보는 중간 결과물(ex. 오디오 신호, 오디오 신호로부터 획득된 특징 정보, 발음 정보, 음소 정보, 문자열 정보 등)을 포함할 수 있다. 이 때, 제2 전자 장 치(100-2)는 정합에 사용될 추가 정보, 사용자 정보 등도 함께 전송할 수 있다. 허브 장치는 제1 전자 장치(100-1)로부터 수신한 음성 인식 정보와 제2 전자 장치(100-2)로부터 수신한 음 성 인식 정보를 시간 순서로 정합할 수 있다(S880). 이 때, 제1 전자 장치(100-1)로부터 수신한 음성 인식 정보 와 제2 전자 장치(100-2)로부터 수신한 음성 인식 정보 중 적어도 하나가 중간 결과물인 경우에는 허브 장치 는 음성 인식을 위한 나머지 과정들을 수행해서 최종 결과물을 획득하고 최종 결과물을 바탕으로 정합을 수행할 수 있다. 그리고 허브 장치는 정합 결과로 획득된 전체 사용자 음성에 대한 최종 인식 결과를 바탕으로 태스크를 수 행할 수 있다(S890). 예컨대 제1 전자 장치(100-1)와 제2 전자 장치(100-2)를 통해 획득한 사용자의 전체 음성이 \"오늘 날씨 어때?\" 인 경우, 허브 장치는 제2 전자 장치(100-2)로 하여금 \"오늘 날씨는 오후에 비가올 예정입니다\"라는 음성 응답을 출력하도록 하는 제어 신호를 제2 전자 장치(100-2)로 전송하는 태스크를 수행할 수 있다. 한편, 도 8a의 흐름도에서는 허브 장치에서 정합이 수행되는 것으로 설명하였으나, 제1 전자 장치(100-1) 또는 제2 전자 장치(100-2)에서 정합이 수행되는 것도 가능하다. 예컨대, 허브 장치는 제1 전자 장치(100- 1) 및 제2 전자 장치(100-2) 각각에서 획득한 사용자 음성에 대해 인식을 수행하여 음성 인식 정보들을 획득하 고 획득된 음성 인식 정보들을 제1 전자 장치(100-1) 또는 제2 전자 장치(100-2)로 전송할 수 있고, 제1 전자 장치(100-1) 또는 제2 전자 장치(100-2)에서 음성 인식 정보들에 대한 정합이 수행되는 것이 가능하다. 그리고 제1 전자 장치(100-1) 또는 제2 전자 장치(100-2)에서 태스크를 수행할 수 있다. 또한 전자 장치 하나에 태스크 수행에 대한 정보를 전달 할수도 있으나 둘 이상의 전자 장치에 태스크 수행에 대한 정보를 전달 할 수도 있다. 예컨대, \"에어컨 켜줘\" 라는 음성 명령 인 경우 \"에어컨 켜줘\"에 대해서 태스 크를 수행할 것을 에어컨에 제어명령으로 전달하고, 태스크를 수행했음에 대한 피드백은 사용자 근처의 전자장 치로 전달 할 수 있다. 한편, 상술한 실시 예들에서는 전자 장치에서 트리거 워드가 인식 가능한 것으로 설명하였으나, 또 다른 실시 예에 따르면 전자 장치는 트리거 워드의 인식이 불가능하고 단지 사용자 음성으로부터 획득된 오디오 데이터를 외부로 전송하고, 외부에서 음성 인식을 위한 처리가 수행되는 것도 가능하다. 이와 관련해선 도 8b 내지 도 8c를 참고하여 설명하도록 한다. 도 8b는 본 개시의 또 다른 실시 예에 따른 음성 인식 시스템에서의 음성 인식 방법을 설명하기 위한 흐름도이 다. 도 8b를 참고하면, 먼저, 제1 전자 장치(100-1)와 제2 전자 장치(100-2)는 음성 인식 대기 상태에 있다(S822). 제 1 전자 장치(100-1)에서 사람의 목소리가 감지되면(S824), 곧바로 녹음(또는 획득)한 오디오 신호를 허브 장 치로 전달할 수 있다(S826). 음성구간 검출(VAD, Voice Activity Detection)은 일반적으로 음성 인식 분 야에서 일반적으로 사용되는 기술로, 소리의 크기나 주파수 영역에서의 에너지 분포 등에 기반한 통계적 모델, 딥러닝 모델 등으로 사람의 목소리를 감지하는 기술이다. 또한 음성 인식에서 주로 사용되는 기술인 음성 끝점 검출(EPD, End Point Detection) 이용하여 사람의 목소리를 감지할 수 있다. 또 다른 실시 예에 따르면, 제1 전자 장치(100-1)는 음성구간 검출 기능을 구비하지 않을 수 있고, 이 경우에는 대기 상태에서 계속 오디오 신호를 허브 장치로 전달할 수 있다. 허브 장치가 제1 전자 장치(100-1)로부터 수신한 오디오 신호에서 트리거 워드를 인식하면(S828), 음성 인 식 기능을 활성화하고(S832), 음성 인식을 수행할 수 있다(S834). 그리고 허브 장치는 제1 전자 장치(100-1)가 아닌 다른 전자 장치에서 음성 인식을 이어서 수행해야 하는 상황과 관련된 특정 이벤트를 감지할 수 있다(S836). 예컨대 허브 장치는 사용자가 이동하는 이벤트를 감 지할 수 있다. 이 경우, 허브 장치는 음성 인식 작업을 핸드 오버하기 위해 음성 인식 기능을 활성화시키 는 제어신호를 전송할 수 있다(S838). 활성화 제어 신호를 수신한 제2 전자 장치(100-2)는 녹음을 개시하여 오디오 신호를 허브 장치로 전달할 수 있다. 그리고 허브 장치는 제2 전자 장치(100-2)로부터 수신한 오디오 신호에 대해 음성 인식을 수행할 수 있다. 허브 장치는 제1 전자 장치(100-1)로부터 수신한 오디오 신호에 대한 음성 인식 결과와 제2 전자 장치 (100-2)로부터 수신한 오디오 신호에 대한 음성 인식 결과를 정합해서 최종 인식 결과를 획득할 수 있다. 제1 전자 장치(100-1)로부터 수신한 오디오 신호와 제2 전자 장치(100-2)로부터 수신한 오디오 신호가 다른 사 용자의 것인 경우에는 허브 장치는 다른 세션으로 새로 트리거 워드 인식 및 음성 인식 등을 수행할 수 있 다. 즉, 허브 장치는 병렬적으로 여러 사용자의 음성에 대해 처리할 수 있다. 또는 허브 장치는 한 번에 하나의 사용자의 음성만 처리하도록 구성될 수도 있다. 도 8b를 참고하여 설명한 실시 예에서 허브 장치이 기능은 서버에서 구현될 수도 있다. 한편, 도 8c는 본 개시의 또 다른 실시 예에 따른 음성 인식 시스템에서의 음성 인식 방법을 설명하기 위한 흐 름도이다. 도 8b와 비교하여, 허브 장치가 음성 인식 작업을 핸드오버 하기 위한 제어 신호를 제2 전자 장 치(100-2)로 전달하지 않더라도, 제2 전자 장치(100-2)도 제1 전자 장치(100-1)처럼 음성구간 검출(Voice Activity Detection) 기능을 구비해, 사람의 목소리가 감지되면 곧바로 허브 장치로 오디오 신호를 전송할 수 있다. 또는, 제2 전자 장치(100-2)는 음성구간 검출 기능을 구비하지 않을 수 있고, 이 경우에는 대기 상태 에서 계속 오디오 신호를 허브 장치로 전달할 수 있다. 구체적으로 도 8c를 참고하면, 먼저, 제1 전자 장치(100-1)와 제2 전자 장치(100-2)는 음성 인식 대기 상태에 있다(S852). 제 1 전자 장치(100-1)에서 사람의 목소리가 감지되면(S854), 곧바로 녹음한 오디오 신호를 허브 장치로 전달할 수 있다(S856). 또 다른 실시 예에 따르면, 제1 전자 장치(100-1)는 음성구간 검출 기능을 구비하지 않 을 수 있고, 이 경우에는 대기 상태에서 계속 오디오 신호를 허브 장치로 전달할 수 있다. 허브 장치가 제1 전자 장치(100-1)로부터 수신한 오디오 신호에서 트리거 워드를 인식하면(S858), 음성 인 식 기능을 활성화하고(S862), 음성 인식을 수행할 수 있다(S864). 제2 전자 장치(100-2)는 사람의 목소리가 감지되면(S866), 곧바로 녹음한 오디오 신호를 허브 장치로 전달 할 수 있다(S868). 또 다른 실시 예에 따르면, 제2 전자 장치(100-2)는 음성구간 검출 기능을 구비하지 않을 수 있고, 이 경우에는 대기 상태에서 계속 오디오 신호를 허브 장치로 전달할 수 있다. 그리고 허브 장치 는 제2 전자 장치(100-2)로부터 수신한 오디오 신호에 대해 음성 인식을 수행할 수 있다. 허브 장치는 제1 전자 장치(100-1)로부터 수신한 오디오 신호에 대한 음성 인식 결과와 제2 전자 장치 (100-2)로부터 수신한 오디오 신호에 대한 음성 인식 결과를 정합해서 최종 인식 결과를 획득할 수 있다. 허브 장치는 이미 동일 사용자의 발화로 음성 인식이 진행 중인 경우에는 신호의 질 등을 판단해서 전환 또는 유지 또는 결합을 할 수 있다. 다른 사용자의 것인 경우에는 허브 장치는 다른 세션으로 새로 트리거워드 인식 및 음성 인식 등을 수행할 수 있다. 도 8c를 참고하여 설명한 실시 예에서 허브 장치이 기능은 서버에서 구현될 수도 있다. 도 9는 본 개시의 또 다른 실시 예에 따른 음성 인식 시스템에서의 음성 인식 방법을 설명하기 위한 흐름 도이다. 도 9를 참고하면, 음성 인식 시스템은 제1 전자 장치(100-1), 제2 전자 장치(100-2), 허브 장치 및 서버를 포함할 수 있다. 먼저, 제1 전자 장치(100-1)와 제2 전자 장치(100-2)는 음성 인식 대기 상태에 있다(S910). 이후 사용자가 제1 전자 장치(100-1) 근처에서 트리거 워드를 포함하는 음성을 발화하는 경우, 제1 전자 장치(100-1)는 사용자 음 성에 포함된 트리거 워드를 인식할 수 있다(S920). 트리거 워드 인식 이외에도, 사용자의 수동 조작에 의해 음 성 인식 기능이 활성화될 수 있다. 예컨대, 제1 전자 장치(100-1)에 마련된 특정 버튼이 선택되면 음성 인식 기 능이 활성화될 수 있다. 제1 전자 장치(100-1)는 사용자 음성에 포함된 트리거 워드를 인식하면 음성 인식 기능을 활성화할 수 있다 (S930). 그리고 제1 전자 장치(100-1)는 음성 인식 기능이 활성화된 상태에서 입력되는 사용자 음성에 대한 음 성 인식을 수행할 수 있다(S940). 이 경우, 제1 전자 장치(100-1)는 음성을 인식하기 위한 일련의 과정들 전부 를 수행하거나, 또는 음성을 인식하기 위한 일련의 과정들 중 일부만을 수행할 수 있다. 예컨대, 제1 전자 장치 (100-1)는 음성 녹음을 수행해서 오디오 신호를 획득하는 과정만을 수행할 수 있다. 또 다른 예로, 또 다른 예 로, 제1 전자 장치(100-1)는 음향 모델을 바탕으로 발음 정보, 음소 정보 또는 문자열 정보를 획득하는 과정까 지 수행할 수 있다. 그리고 제1 전자 장치(100-1)는 음성 인식 수행에 따라 획득된 음성 인식 정보를 허브 장치에 전송할 수 있다(S945). 이 때, S940 단계에서 음성을 인식하기 위한 일련의 과정들 전부를 수행한 경우에는 음성 인식 정 보는 최종 결과물(ex. 오디오 신호에 대응하는 텍스트 데이터)을 포함할 수 있다. S940 단계에서 음성을 인식하 기 위한 일련의 과정들 중 일부만을 수행한 경우에는 음성 인식 정보는 중간 결과물(ex. 오디오 신호로부터 획 득된 특징 정보, 발음 정보, 음소 정보, 문자열 정보 등)을 포함할 수 있다. 이 때, 제1 전자 장치(100-1)는 정 합에 사용될 추가 정보, 사용자 정보 등도 함께 전송할 수 있다. 이와 같이 전자 장치와 허브 장치는 음성 인식을 위한 과정들을 서로 나누어 수행할 수 있는바, 전자 장치 측에서의 연산 부담이 덜어질 수 있다. 한편, 제1 전자 장치(100-1)는 음성 인식 정보를 허브 장치로 보내는 대신 서버로 보낼 수도 있다. 제1 전자 장치(100-1)의 음성 인식 기능이 활성화된 동안 허브 장치는 다른 장치에서 음성 인식을 이어서 수행해야 하는 상황과 관련된 특정 이벤트를 감지할 수 있다(S950). 예컨대, 허브 장치는 사용자가 이동하는 이벤트가 감지되면, 사용자의 이동 방향에 있는 제2 전자 장치 (100-2)로 음성 인식 작업을 핸드 오버하기 위해 음성 인식 기능을 활성화시키는 제어신호를 전송할 수 있다 (S955). 예컨대 허브 장치는 사용자의 이동을 감지할 수 있는 장치(ex. 전자 장치들 또는 그 밖의 다 른 장치(ex. 센서 등))로부터 사용자 이동과 관련한 신호를 수신할 수 있고, 이를 바탕으로 사용자가 이동하는 이벤트를 감지할 수 있고, 제2 전자 장치(100-2)로 음성 인식 작업을 핸드 오버하기 위해 음성 인식 기능을 활 성화시키는 제어신호를 전송할 수 있다. 한편, 본 개시의 또 다른 실시 예에 따르면, 허브 장치 대신에 서버가 음성 인식 작업을 핸드 오버하 는 동작을 수행할 수 있다. 예컨대, 서버는 사용자의 이동을 감지할 수 있는 장치(ex. 전자 장치들 또는 그 밖의 다른 장치(ex. 센서 등))로부터 사용자 이동과 관련한 신호를 수신할 수 있고, 이를 바탕으로 사 용자가 이동하는 이벤트를 감지할 수 있고, 제2 전자 장치(100-2)로 음성 인식 작업을 핸드 오버하기 위해 음성 인식 기능을 활성화시키는 제어신호를 전송할 수 있다. 한편, 본 개시의 또 다른 실시 예에 따르면, 제1 전자 장치(100-1)가 음성 인식 작업을 핸드 오버하는 동작을 수행할 수 있다. 예컨대, 제1 전자 장치(100-1)는 복수의 마이크, 카메라를 통해 사용자의 이동을 감지할 수 있 고, 이를 바탕으로 사용자가 이동하는 이벤트를 감지할 수 있고, 제2 전자 장치(100-2)로 음성 인식 작업을 핸 드 오버하기 위해 음성 인식 기능을 활성화시키는 제어신호를 전송할 수 있다. 제어 신호를 수신한 제2 전자 장치(100-2)는 음성 인식 기능을 활성화할 수 있다(S960). 그리고 제2 전자 장치 (100-2)는 음성 인식 기능이 활성화된 상태에서 입력되는 사용자 음성에 대한 음성 인식을 수행할 수 있다 (S965). 이 경우, 제2 전자 장치(100-2)는 음성을 인식하기 위한 일련의 과정들 전부를 수행하거나, 또는 음성 을 인식하기 위한 일련의 과정들 중 일부만을 수행할 수 있다. 그리고 제2 전자 장치(100-2)는 음성 인식 수행에 따라 획득된 음성 인식 정보를 허브 장치에 전송할 수 있다(S970). 이 때, S965 단계에서 음성을 인식하기 위한 일련의 과정들 전부를 수행한 경우에는 음성 인식 정 보는 최종 결과물(ex. 오디오 신호에 대응하는 텍스트 데이터)을 포함할 수 있다. S965 단계에서 음성을 인식하 기 위한 일련의 과정들 중 일부만을 수행한 경우에는 음성 인식 정보는 중간 결과물(ex. 오디오 신호, 오디오 신호로부터 획득된 특징 정보, 발음 정보, 음소 정보, 문자열 정보 등)을 포함할 수 있다. 이 때, 제2 전자 장 치(100-2)는 정합에 사용될 추가 정보, 사용자 정보 등도 함께 전송할 수 있다. 한편, 제2 전자 장치(100-2)는 음성 인식 정보를 허브 장치로 보내는 대신 서버로 보낼 수도 있다. 허브 장치는 제1 전자 장치(100-1)로부터 수신한 음성 인식 정보와 제2 전자 장치(100-2)로부터 수신한 음 성 인식 정보를 서버로 전송할 수 있다(S980). 이 때, 허브 장치는 제1 전자 장치(100-1)로부터 수신 한 음성 인식 정보와 제2 전자 장치(100-2)로부터 수신한 음성 인식 정보를 그대로 서버로 전송할 수 있다. 또는, 제1 전자 장치(100-1)로부터 수신한 음성 인식 정보와 제2 전자 장치(100-2)로부터 수신한 음성 인 식 정보가 중간 결과물인 경우 허브 장치는 음성 인식을 위한 나머지 과정을 수행해서 최종 결과물을 서버 로 전송할 수 있다. 또는, 제1 전자 장치(100-1)와 제2 전자 장치(100-2)로부터 오디오 신호 자체를 수신 한 경우 허브 장치는 오디오 신호 자체를 서버로 전송할 수 있다. 그리고 서버는 수신한 음성 인식 정보들을 시간 순서로 정합할 수 있다(S990). 수신된 음성 인식 정보들이 중간 결과물인 경우 서버는 음성 인식을 위한 나머지 과정들을 수행하여 최종 결과물을 획득하고, 최종 결 과물을 기초로 정합을 수행할 수 있다. 그리고 허브 장치는 정합 결과로 획득된 전체 사용자 음성에 대한 최종 인식 결과를 바탕으로 태스크를 수행할 수 있다(S995). 예컨대 제1 전자 장치(100-1)와 제2 전자 장치(100-2)를 통해 획득한 사용자의 전체 음성이 \"오늘 날씨 어때?\" 인 경우, 서버는 제2 전자 장치(100-2)로 하여금 \"오늘 날씨는 오후에 비가올 예정입니다\"라는 음성 응답 을 출력하도록 하는 제어 신호를 제2 전자 장치(100-2)로 전송하는 태스크를 수행할 수 있다. 한편, 도 9의 흐름도에서는 서버에서 정합이 수행되는 것으로 설명하였으나, 제1 전자 장치(100-1), 제2 전자 장치(100-2) 또는 허브 장치에서 정합이 수행되는 것도 가능하다. 그리고 제1 전자 장치(100-1), 제2 전자 장치(100-2) 또는 허브 장치에서 태스크를 수행할 수 있다. 상술한 바와 같이 본 개시의 다양한 실시 예에 따른 음성 인식 시스템은 사용자의 이동에 따라 자동으로 사용자 근처에 있는 전자 장치에게 음성 인식 작업이 핸드오버될 수 있고, 각 전자 장치들에서 획득된 음성 인식 정보가 정합될 수 있고, 정합된 결과를 바탕으로 태스크가 수행될 수 있다. 또한, 음성 인식을 위한 일련 의 과정들이 여러 장치에 분배되어서 수행될 수 있어서 장치 내에서 연산 부담을 덜 수 있다는 장점이 있다. 이하에선 도 10 내지 도 13을 참고하여 상술한 전자 장치, 허브 장치 및 서버의 구성에 대해 설 명하도록 한다. 도 10은 본 개시의 일 실시 예에 따른 전자 장치의 구성을 설명하기 위한 도면이다. 도 10을 참고하면, 전자 장치는 프로세서, 메모리, 통신부 및 마이크를 포함한다. 프로세서는 전자 장치의 전반적인 동작을 제어하기 위한 구성이다. 예를 들면, 프로세서는 운영 체제, 애플리케이션을 구동하여 프로세서에 연결된 다수의 하드웨어 또는 소프트웨어 구성요소들을 제어할 수 있고, 각종 데이터 처리 및 연산을 수행할 수 있다. 프로세서는 CPU(central processing unit) 또는 GPU(graphics-processing unit)이거나 둘 다일 수 있다. 프로세서는 적어도 하나의 범용 프로세서 (general processor), 디지털 신호 프로세서(digital signal processor), ASIC(Application specific integrated circuit), SoC(system on chip), MICOM(Microcomputer) 등으로 구현될 수 있다. 메모리는 내장 메모리 또는 외장 메모리를 포함할 수 있다. 메모리는 프로세서에 의해 액세스되 며, 프로세서에 의한 데이터의 독취/기록/수정/삭제/갱신 등이 수행될 수 있다. 메모리는 하나 이상의 모듈로서 구성된 소프트웨어 및/또는 펌웨어를 포함할 수 있다. 모듈은 컴퓨터 실행 가능 명령어의 집합에 대응될 수 있다. 메모리는 트리거 워드 모듈, 음성 인식 모듈, 핸드 오버 모듈, 정합 모듈, 태스크 모 듈를 포함할 수 있다. 모듈들(121, 122, 123, 124, 125)은 다양한 기능 수행을 위해 프로세서에 의 해 실행될 수 있다. 트리거 워드 모듈은 오디오 신호 내에서 기 설정된 트리거 워드 또는 문구를 인식할 수 있다. 예컨대 트리 거 워드 모듈은 마이크를 통해 획득된 사용자 음성에 포함된 트리거 워드를 인식할 수 있다. 트리거 워드가 인식되면, 전자 장치의 음성 인식 기능을 활성화시킬 수 있다. 예컨대, 트리거 워드 모듈은 마이크의 녹음 기능을 활성화 시키고 음성 인식 모듈을 활성화(또는 구동)시킬 수 있다. 전자 장치에 트리거 워드 모듈이 구비되지 않을 수 있고, 이 경우에는 전자 장치는 사용자 수동 조작에 의해 음성 인식 기능을 활성화시킬 수 있다. 예컨대 전자 장치에 마련된 특정 버튼의 선택으로 음 성 인식 기능이 활성화될 수 있다. 또 다른 예로, 전자 장치는 녹음만을 수행해서 오디오 신호를 외부 장 치, 예컨대 다른 전자 장치 또는 허브 장치, 서버 등에 전송할 수 있고, 이러한 외부 장치에서 트리거 워드를 인식해서 전자 장치에서 음성 인식 기능이 활성화되도록 제어하는 것도 가능하다. 음성 인식 모듈은 음성 인식을 수행할 수 있다. 음성 인식 모듈은 자동 음성 인식(automatic speech recognition)(ASR)기술을 이용할 수 있다. 음성 인식 모듈은 사용자 음성에 대응하는 오디오 신호를 텍스트 데이터로 변환할 수 있다. 음성 인식 모 듈의 구체적인 기능을 도 13을 참고하여 설명하도록 한다. 도 13을 참고하면, 음성 인식 모듈은 특징 추출부와 디코더를 포함할 수 있다. 특징 추출부는 오디오 신호 로부터 특징 정보(특징 벡터)를 추출할 수 있다. 그리고 디코더는 음향 모델과 언어 모델을 바탕으로 특징 정보 들에 대응되는 음성 인식 정보를 획득할 수 있다. 음성 인식 정보는 음향 모델을 기초로 획득된 특징 정보들에 대응하는 발음 정보, 음소 정보, 문자열 정보와, 언어 모델을 기초로 상기 획득된 발음 정보들에 대응되는 텍스 트 데이터를 포함할 수 있다. 또 다른 실시 예에 따르면, 음성 인식 모듈은 음향 모델과 언어 모델 중 어느 하나만 포함하거나, 또는 둘 다 포함하고 있더라도 둘 중 어느 하나만을 사용할 수 있다. 이 경우 음성 인식 모듈은 음향 모델과 언어 모델 중 어느 하나만 적용하여 음성 인식 정보를 획득할 수 있다. 예컨대, 음성 인식 정보는 음향 모델을 기초 로 획득된 발음 정보, 음소 정보, 문자열 정보 또는 언어 모델을 기초로 획득된 텍스트 정보를 포함할 수 있다. 즉, 음성 인식 정보는 최종 결과물인 텍스트 데이터가 아니라 중간 결과물인 발음 정보, 음소 정보 또는 문자열 정보를 포함할 수 있다. 이 경우, 음성 인식 정보는 다른 장치로 전송되고, 적용되지 않은 음향 모델 또는 언어 모델을 다른 장치에서 적용해서 최종적으로 텍스트 데이터를 획득할 수 있다. 또 다른 실시 예에 따르면, 음성 인식 모듈은 음향 모델과 언어 모델 둘 다를 포함하고 있지 않을 수 있거 나, 또는 둘 중 적어도 하나를 포함하고 있더라도 그것을 사용하지 않을 수 있다. 이 경우, 음성 인식 모듈 은 특징 정보 추출 동작까지만 수행해서 특징 정보를 포함한 음성 인식 정보를 출력할 수 있다. 음성 인식 정보는 다른 장치로 전송되고, 다른 장치에서 음향 모델 및 언어 모델을 적용해서 최종적으로 텍스트 데이터를 획득할 수 있다. 또 다른 실시 예에 따르면 전자 장치는 음성 인식 모듈 자체를 포함하지 않을 수 있거나, 포함하고 있더라 도 이를 사용하지 않을 수 있다. 이 경우, 전자 장치는 마이크를 통해 획득된 오디오 신호를 다른 장 치로 전송하고, 다른 장치에서 음성 인식이 수행될 수 있다. 이와 같이 음성 인식 모듈의 기능들의 선택적 사용은 전자 장치 내에서의 연산 부담을 줄여줄 수 있 다. 또한, 음성 인식 모듈은 외부 장치로부터 중간 결과물인 음성 인식 정보가 수신되는 경우, 음성 인식 정보 에 대해 나머지 음성 인식 처리를 수행할 수 있다. 예컨대, 음성 인식 모듈은 외부 장치로부터 수신된 음 성 인식 정보가 음향 모델이 적용되고 언어 모델이 미적용된 정보인 경우 수신된 음성 인식 정보에 언어 모델을 적용하여 최종 인식 결과를 획득할 수 있다. 또 다른 예로, 음성 인식 모듈은 외부 장치로부터 수신된 음 성 인식 정보가 특징 정보만을 포함한 경우, 음성 인식 정보에 언어 모델과 음향 모델을 적용해서 최종 인식 결 과를 획득할 수 있다. 음성 인식 모듈을 통해 오디오 신호로부터 텍스트 데이터가 획득되면, 텍스트 데이터는 태스크 모듈 로 전달될 수 있다. 핸드 오버 모듈은 다른 장치에서 음성 인식을 이어서 수행해야 하는 상황과 관련된 특정 이벤트를 감지하 는 기능, 음성 인식 작업을 핸드 오버할 다른 장치를 선택하는 기능, 다른 장치의 음성 인식 기능을 활성화시키 는 제어신호를 전송하는 기능, 음성 인식 정보, 정합에 사용될 추가 정보 및/또는 사용자 정보를 전송하는 기능 을 수행할 수 있다. 전자 장치는 사용자 이동 감지를 위한 복수의 마이크를 구비할 수 있고, 핸드 오버 모듈은 복수의 마 이크를 통해 입력되는 사용자 음성의 크기 차이를 통해 사용자가 이동하는 이벤트를 감지할 수 있다. 또 다른 실시 예에 따르면, 제1 전자 장치(100-1)는 카메라를 포함할 수 있고, 핸드 오버 모듈은 카메라를 통해 획 득한 영상을 바탕으로 사용자가 이동하는 이벤트를 감지할 수 있다. 핸드 오버 모듈은 메모리에 기 저장된 음성 수신이 가능한 타 전자 장치들에 대한 정보를 이용해서 음성 인식 작업을 핸드 오버할 타 전자 장치를 선택할 수 있다. 예컨대, 핸드 오버 모듈은 사용자가 이동하는 이벤트가 감지되면 사용자의 이동 정보를 획득하고, 메모리 저장된 타 전자 장치들에 대한 정보에 기초해 서 상기 타 전자 장치들 중 현재 사용자와 가장 가까운 타 전자 장치를 선택할 수 있다. 그리고 선택된 타 전자 장치로 음성 인식 기능을 활성화시키는 제어 신호를 통신부를 통해 전송할 수 있다. 핸드 오버 모듈을 이용해 전자 장치가 사용자의 이동 방향에 있는 타 전자 장치를 식별하는 방법과 관련하여선 도 18 내지 도 20을 참고하여 좀 더 설명하도록 한다. 전자 장치는 사용자 이동 방향에 어떤 전자 장치가 있는지를 식별하기 위해, 동일 네트워크(ex. 홈 네트워 크) 내 전자 장치들의 위치 정보를 생성할 수 있다. 신규 전자 장치가 네트워크에 들어오면 신규 전자 장치에 대해서도 위치 정보를 생성할 수 있다. 전자 장치들에 대한 위치 정보 생성 방법의 일 예를 도 18을 참고하여 설명하도록 한다. 도 18을 참고하면, 위쪽 화살표가 나타내는 흐름은 위치 정보를 생성하는 과정이고, 아래쪽 화살표가 나타내는 흐름은 생성된 위치 정보를 이용해서 사용자 이동 방향의 다른 전자 장치의 음성 인식 기능을 활성화시키는 과 정이다. 위치 정보를 학습하는 과정을 보면, 1) 먼저 침실 내의 제1 전자 장치(100-1)가 음성 인식 기능이 활성화되어서 음성 인식을 시작한다. 2-1) 그리고 제1 전자 장치(100-1)는 복수의 마이크를 통해 입력되는 사용자의 음성을 기반으로 사용자의 이동 방향에 대한 정보를 획득할 수 있다. 3-1) 그리고 제1 전자 장치(100-1)는 동일 네트워 크 내 음성 수신이 가능한 모든 전자 장치에 음성 인식 기능을 활성화 시키는 제어신호를 전송할 수 있다. 4) 그리고 사용자가 이동한 거실의 제2 전자 장치(100-2)에서 사용자의 음성이 수신되면 제2 전자 장치(100-2)는 이를 제1 전자 장치(100-1)에 알린다. 5) 그러면 제1 전자 장치(100-1)는 사용자 이동 방향에 대한 정보와 제2 전자 장치(100-2)를 매칭시킴으로써 제2 전자 장치(100-2)에 대한 위치 정보를 생성할 수 있다. 이와 같은 방식 을 여러번 거침으로써 동일 네트워크 내 전자 장치들 각각에 대한 위치 정보가 생성될 수 있고, 이러한 위치 정 보는 전자 장치들 간에 공유될 수 있다. 예컨대, 전자 장치들이 서로에게 위치 정보를 전송하거나 허브 또는 서버에 위치 정보를 전송할 수 있다. 이와 같이 위치 정보가 생성되고 나면, 제1 전자 장치(100-1)는 사용자의 이동 방향에 있는 특정 기기만 활성화 시킬 수 있다. 즉, 도 18의 아래쪽 화살표가 나타내는 흐름을 보면, 1)먼저 침실 내의 제1 전자 장치(100-1)가 음성 인식 기능이 활성화되어서 음성 인식을 시작한다. 2-2) 그리고 제1 전자 장치(100-1)는 복수의 마이크를 통해 입력되는 사용자의 음성을 기반으로 사용자의 이동 방향에 대한 정보를 획득할 수 있다. 3-2) 그리고 제1 전자 장치(100-1)는 사용자가 이동한 방향이 거실의 제2 전자 장치(100-2)의 위치에 매칭된다는 것을 이미 알고 있으므로, 제2 전자 장치(100-2)에 음성 인식 기능을 활성화 시키는 제어 신호를 전송할 수 있다. 도 19는 본 개시의 일 실시 예에 따른 전자 장치의 사용자의 이동 방향을 감지하는 방법을 설명하기 위한 도면이다. 도 19를 참고하면, 전자 장치는 스피커로 구현될 수 있고, 복수의 마이크(151, 153)를 포함할 수 있다. 전 자 장치는 복수의 마이크(151, 153)로부터 수신된 사용자 음성을 이용하여 사용자의 음성이 발생된 방향을 알아낼 수 있다. 구체적으로, 음원으로부터 각 복수의 마이크(151, 153)까지의 거리는 서로 상이할 수 있다. 따라서, 특정 지점 에서 발생된 음성이 각각의 복수의 마이크(151, 153)까지 전달되는데 까지 걸리는 시간과 전달되는 소리의 크기 는 서로 다를 수 있다. 복수의 마이크(151, 153) 각각에서 동일한 음성이 감지되는 시차 또는 소리의 크기를 이용하여 전자 장치는 음성명령이 발생된 방향을 감지할 수 있다. 즉, 사람의 양쪽 귀를 통하여 소리의 방향 을 감지하는 것과 동일한 원리이다. 사용자의 이동 방향을 음성을 인용하여 판별할 수 있는 구체적인 기술로는 GCC-PHAT, SRP-PHAT 등이 있다 복수의 마이크(151, 153)의 개수는 2개로 제한되는 것은 아니고, 더 많은 개수의 마이크를 이용하여 정밀하게 방향을 감지할 수도 있다. 일 실시 예에 따르면, 전자 장치는 전자 장치 내에 구비된 복수의 마이크 중 서로 다른 조합의 두 개 의 마이크의 쌍을 선택하고, 각각의 마이크의 쌍에 대응하여, 음향 신호의 시간 지연들을 산출할 수 있다. 전자 장치는 산출된 시간 지연들 및 복수의 마이크의 위치에 기초하여 음원의 3 차원 위치를 산출할 수 있다. 이러한 방법의 대표적인 알고리즘으로 GCC-PHAT(Generalized cross-correlation with the phase transform)이 있다. 또한, 예를 들어, 전자 장치는 복수의 마이크를 이용하여 음향 신호를 수신하고, 복수의 마이크 각각에 대 응하여 음향 신호를 음향 데이터로 변경할 수 있다. 전자 장치는수신된 음향 신호가 특정 방향에서 수신되 었다는 가정하에, 복수의 마이크 각각에 대하여 시간 지연을 산출하고, 복수의 마이크 각각에 대응하는 음향 데 이터를 이에 대응하는 시간 지연만큼 이동 시킨 후 더하여 빔포밍 파워 출력(beamforming power output)을 산출 할 수 있다. 이 경우, 가정된 방향이 실제 음원의 방향일 때 음향 데이터를 더한 값이 최대가 되므로, 전자 장 치는 모든 가능한 방향에 대하여 빔포밍 파워 출력을 산출하고, 빔포밍 파워 출력이 최대가 되는 방향을 음원의 방향으로 결정할 수 있다. 이러한 방법의 대표적인 알고리즘으로 SRP-PHAT(Steered Response Power with the phase transform)이 있다. 도 20은 본 개시의 또 다른 실시 예에 따른 전자 장치가 카메라를 이용해서 사용자의 이동 방향을 감지하 는 방법을 설명하기 위한 도면이다. 도 20에 도시된 전자 장치는 복수의 마이크 및 카메라를 포함할 수 있다. 전자 장치는 복수의 마이크를 이용해 사용자의 이동 여부를 판단할 수 있다. 사용자의 움직임이 있다고 판단이 되면, 전자 장치 는 전자 장치에 구비된 카메라를 구동 하거나, 주변 다른 장치에 카메라가 있을 경우 다른 장치 의 카메라를 구동시키도록 제어할 수 있다. 그리고 전자 장치는 자신에 구비된 카메라에서 생성된 영 상을 획득하거나 또는 다른 장치에 구비된 카메라에서 생성된 영상을 수신하고, 영상을 바탕으로 사용자의 이동 방향과 노이즈 방향(ex. 열린 창문으로부터 들어오는 소리, TV 소리 등)을 식별할 수 있다. 즉, 카메라에서 생 성된 영상을 바탕으로 사용자의 방향을 감지하고, 그 밖의 다른 방향으로부터의 노이즈는 제거 또는 감쇄함으로 써 사용자의 방향을 좀 더 정확히 감지할 수 있는 것이다. 전자 장치는 음성 인식 작업을 핸드 오버 받는 타 전자 장치에게 노이즈 방향에 대한 정보도 전달할 수 있다. 구체적인 예로, 전자 장치는 통계적 학습 알고리즘에 의해 원음들을 분리해내는 독립 성분 분석 (Independent Component Analysis) 기술을 이용해서 오디오 신호에서 음원들을 분리해낼 수 있다. 그리고 카메 라를 통해 획득한 노이즈 방향에 대한 정보를 기초로, 분리된 음원들 중에서 사용자의 음성을 식별할 수 있다. 이 경우, 전자 장치는 2 이상의 마이크를 구비해서 2 이상의 마이크로 들어오는 각 음원들의 방향을 식별하고, 각 음원들의 방향과 카메라를 통해 획득한 방향 정보를 비교해서 음원들 중에서 사용자 음성에 대응하는 음원을 식별할 수 있다. 사용자에 대응하는 음원이 식별되면 나머지 음원들에 대해선 감쇄를 수행할 수 있다. 또한, 음성을 인식할 때, 수신된 오디오 신호들에서 사용자 방향에 대해서는 빔포밍하여 음원을 키우고, 노이즈 방향에 대해서는 소음 감쇄를 할 수 있다. 정합 모듈은 각기 다른 전자 장치들에서 수집된 사용자 음성에 대한 인식 정보를 시간 순서로 정합할 수 있다. 일 실시 예에 따르면, 마이크를 통해 획득된 사용자의 음성에 대해 음성 인식을 수행하여 획득한 제1 음성 인식 정보와, 음성 인식 작업을 핸드오버 받은 타 전자 장치로부터 수신한 제2 음성 인식 정보를 바탕으로 최종 인식 결과를 획득할 수 있다. 이 때, 정합 모듈은 음성 인식 기능을 활성화시키는 제어 신호를 타 전자 장치에 전송한 시간에 대한 정보 를 획득하고, 획득한 시간에 대한 정보를 기초로 상기 제1 음성 인식 정보와 상기 제2 음성 인식 정보를 정합하 여 최종 인식 결과를 획득할 수 있다. 이 경우, 상기 획득한 시간에 대한 정보는 제어 신호를 전송한 절대 시간에 대한 정보를 포함할 수 있다. 즉, 절대 시간 정보를 바탕으로 정합이 수행될 수 있다. 또는, 상기 획득한 시 간에 대한 정보는 전자 장치의 음성 인식 기능이 활성화된 이후 제어 신호를 상기 타 전자 장치에 전송할 때까지 소요된 시간에 대한 정보를 포함할 수 있다. 즉, 상대 시간 정보를 바탕으로 정합이 수행될 수 있다. 정합 모듈을 이용해 전자 장치가 음성 인식 정보들을 정합하는 예시들과 관련해선 도 21 내지 도 25 를 참고하여 설명하도록 한다. 도 21은 본 개시의 일 실시 예에 따른 전자 장치의 음성 인식 정보 정합 방법을 설명하기 위한 도면이다. 도 21을 참고하면, 사용자가 제1 전자 장치(100-1)에서부터 시작하여 제2 전자 장치(100-2), 제3 전자 장치 (100-3)로 이동하면서 발화를 하면 전자 장치들(100-1, 100-2, 100-3)이 순차적으로 음성 인식 기능을 활성화할 수 있다. 그리고 전자 장치들(100-1, 100-2, 100-3) 중 어느 한 장치 또는 허브 장치 또는 서버에서 전자 장치들(100-1, 100-2, 100-3)에서 생성된 음성 인식 정보들 정합하여 최종 결과를 생성할 수 있다. 이때, 음성 인식 정보들 정합하기 위해서는 정합에 이용될 추가 정보로서 시간 정보가 필요하다. 도 22는 절대적 시간에 정보를 바탕으로 음성 인식 정보들을 정합하는 예를 설명하기 위한 도면이다. 도 22를 참고하면, 전자 장치들(100-1, 100-2, 100-2)이 음성 인식 기능이 활성화된 시간을 절대 시간 형태로 기록을 하고, 그 축에서 정합을 할 수 있다. 이 때 스코어(음성 인식이 정확히 된 정도(진한 글자가 스코어가 높은 부분))를 기반으로 정합을 할 수 있다. 여기서 스코어는, SNR 크기 및/또는 ASR 스코어를 기초로 결정될 수 있다. ASR 스코어는 음향 모델(AM) 적용 결과의 정확도를 나타낸 AM 스코어, 언어 모델(LM) 적용 결과의 정 확도를 나타낸 LM 스코어 중 적어도 하나를 포함할 수 있다. 예컨대, 제1 전자 장치(100-1), 제2 전자 장치(100-2), 제3 전자 장치(100-3) 각각이 자신의 음성 인식 기능이 활성화된 절대적 시간(날짜, 시, 분, 초)에 대한 정보를 획득하고, 최종 정합을 수행할 장치로 시간 정보를 전 송할 수 있다. 최종 정합을 수행할 장치가 예컨대 제3 전자 장치(100-3)인 경우, 제1 전자 장치(100-1), 제2 전 자 장치(100-2)는 자신이 생성한 음성 인식 정보와, 시간 정보를 제3 전자 장치(100-3)로 전송할 수 있다. 이 경우, 제1 전자 장치(100-1)와 제2 전자 장치(100-2)가 직접적으로 제3 전자 장치(100-3)로 시간 정보를 전송할 수도 있고, 또는 제1 전자 장치(100-1)가 제2 전자 장치(100-2)에게 시간 정보를 전달하고, 제2 전자 장치(100- 2)는 자신이 획득한 시간에 대한 정보와 제1 전자 장치(100-1)로부터 수신한 시간 정보를 취합해서 제3 전자 장 치(100-3)에게 전달할 수 있다. 즉, 사슬처럼 시간 정보가 전달될 수 있다. 음성 인식 정보도 마찬가지로 개별 적으로 전달되거나 사슬처럼 전달될 수 있다. 그러면 제3 전자 장치(100-3)는 수신한 시간에 대한 정보들과 음 성 인식 정보들을 바탕으로 정합을 수행하여 최종 인식 결과를 획득하고, 최종 인식 결과를 기초로 태스크를 수 행할 수 있다. 도 23은 상대적 시간 정보를 바탕으로 음성 인식 정보들을 정합하는 예를 설명하기 위한 도면이다. 도 23을 참고하면, 최초로 사용자에 의해 음성 인식 기능이 활성화된 전자 장치(100-1)부터 최종 전자 장치 (100-3)까지 이어지는 음성 인식에서, 전자 장치들(100-1, 100-2, 100-3) 각각은 최초로 사용자에 의해 음성 인 식이 활성화된 시점을 기준으로 자신의 음성 인식 기능이 활성화된 시점의 상대적인 시간을 기록하고 다음번 전 자 장치로 전달할 수 있고, 이와 같은 상대적인 시간 정보가 정합에 사용될 수 있다. 예컨대, 제1 전자 장치(100-1), 제2 전자 장치(100-2), 제3 전자 장치(100-3) 각각이 자신의 음성 인식 기능이 활성화된 시간을 기준으로 타 장치를 활성화 시킨 시간의 상대적 시간에 대한 정보를 획득하고, 다음번 장치로 시간 정보를 전송할 수 있다. 최종 정합을 수행할 장치가 예컨대 제3 전자 장치(100-3)인 경우, 먼저 제1 전자 장치(100-1)는 자신이 획득한 상대적 시간 정보를 제2 전자 장치(100-2)로 전송하고, 제2 전자 장치(100-2)는 제1 전자 장치(100-1)로부터 수신한 시간 정보에 자신이 획득한 상대적 시간 정보를 더하고, 더한 결과 및 제1 전자 장치(100-1)로부터 수신한 시간 정보를 취합해서 제3 전자 장치(100-3)로 전송할 수 있다. 즉, 사슬처럼 시간 정보가 전달될 수 있다. 음성 인식 정보도 마찬가지로 사슬처럼 전달될 수 있다. 그러면 제3 전자 장치 (100-3)는 수신한 시간 정보를 바탕으로 음성 인식 정보들을 나열하여 정합을 수행하고, 최종 인식 결과를 획득 할 수 있다. 그리고 제3 전자 장치(100-3)는 최종 인식 결과를 바탕으로 태스크를 수행할 수 있다. 도 24는 상대 시간 정보를 바탕으로 음성 인식 정보들을 정합하는 또 다른 예를 설명하기 위한 도면이다. 도 24를 참고하면, 최초로 음성 인식 기능이 활성화된 전자 장치(100-1)부터 최종 전자 장치(100-3)까지 이어지 는 음성 인식에서, 전자 장치들(100-1, 100-2, 100-3) 각각은 타 전자 장치(100-3)의 음성 인식 기능을 활성화 시키면서 자신이 활성화된 시점과의 상대적인 시간을 기록하고 다음번 전자 장치로 전달할 수 있고, 이와 같은상대적인 시간 정보가 정합에 사용될 수 있다. 최종 정합을 수행하는 장치로 시간 정보들이 수신되면 최종 정합 을 수행하는 장치가 음성 인식 정보들을 시간 정보들을 바탕으로 나열하고 정합해서 최종 인식 결과를 획득할 수 있다. 도 25는 프래임 단위로 음성 인식 정보들을 정합하는 또 다른 예를 설명하기 위한 도면이다. 프래임은 오디오 신호로부터 특징 정보를 추출하는 구간 단위로서, 음성 인식의 단위를 의미한다. 슬라이딩 윈 도우 단위를 의미할 수 있다. 하나의 프래임은 예컨대 25ms, 10ms일 수 있다. 도 25의 실시 예는 도 23과 동일 한 방식이되, 시간 대신 프래임 단위를 이용한 것이다. 다시 도 10을 참고하면, 태스크 모듈은 음성 인식 모듈로부터 전달 받은 텍스트 데이터를 분석해서 의미를 분석하고 의미에 맞는 태스크를 수행할 수 있다. 태스크 모듈은 자연어 이해 처리(Natural Language Processing)(NLP) 기술을 이용할 수 있다. 태스크 모듈은 분석된 의미에 기초해서 수행할 태스크를 식별할 수 있다. 다양한 타입의 태스크들, 예컨대 음악 재생, 스케줄 설정, 전화 걸기, 문의에 대한 응답 등이 수행될 수 있다. 문의에 응답을 제공하는 태스크는 예컨대 \"오늘 날씨 어때?\"라는 사용자 음성에 대응하여 태스크 모듈은 \"오늘 날씨는 비가올 예정입니다\"라 는 응답을 출력하도록 전자 장치 또는 다른 장치를 제어하는 태스크일 수 있다. 본 개시의 일 실시 예에 따르면, 태스크를 수행하기 위한 인공지능 에이전트(Artificial intelligence agent) 프로그램이 전자 장치에 저장되어 있을 수 있다. 인공지능 에이전트 프로그램은 AI(Artificial Intelligence) 기반의 서비스(예를 들어, 음성 인식 서비스, 비서 서비스, 번역 서비스, 검색 서비스 등)를 제공하기 위한 전용 프로그램으로서, 기존의 범용 프로세서(예를 들어, CPU) 또는 별도의 AI 전용 프로세서(예를 들어, GPU 등)에 의해 실행될 수 있다. 프로세서는 범용 프로세서 및 AI 전용 프로세서 중 적어도 하나를 포함할 수 있다. 특히, 인공지능 에이전트 프로그램은 사용자 문의 및 응답을 자연어로 처리할 수 있는 대화 시스템을 포함할 수 있다. 대화 시스템은 음성 인식 모듈 및 태스크 모듈을 포함하여 구성될 수 있다. 도 14는 본 개시의 일 실시 예에 다른 대화 시스템을 설명하기 위한 블럭도이다. 도 14에 도시된 대화 시스템은 가상의 인공지능 에이전트와 자연어를 통해 대화를 수행하기 위한 구성으 로서, 본 개시의 일 실시예 따르면, 대화 시스템은 전자 장치의 메모리 내에 저장될 수 있다. 그러나, 이는 일 실시예에 불과할 뿐, 대화 시스템에 포함된 적어도 하나는 외부의 적어도 하나의 서버에 포함될 수 있다. 대화 시스템은 도 14에 도시된 바와 같이, 자동 음성 인식(automatic speech recognition)(ASR) 모듈 , 자연어 이해(natural language understanding)(NLU) 모듈, 대화 매니저(dialogue manager)(DM) 모듈, 자연어 생성(natural language generator)(NLG) 모듈 및 텍스트 음성 변환(text to speech)(TTS) 모듈을 포함할 수 있다. 그 밖에 대화 시스템은 패스 플래너(path planner) 모듈 또 는 액션 플래너(action planner) 모듈을 더 포함할 수 있다. 자동 음성 인식(automatic speech recognition)(ASR) 모듈은 전자 장치로부터 수신된 사용자 입력 (특히, 사용자 문의)을 텍스트 데이터로 변환할 수 있다. 예를 들어, 자동 음성 인식 모듈은 발화 인식 모듈을 포함할 수 있다. 상기 발화 인식 모듈은 음향(acoustic) 모델 및 언어(language) 모델을 포함할 수 있다. 예를 들어, 음향 모델은 발성에 관련된 정보를 포함할 수 있고, 언어 모델은 단위 음소 정보 및 단위 음 소 정보의 조합에 대한 정보를 포함할 수 있다. 발화 인식 모듈은 발성에 관련된 정보 및 단위 음소 정보에 대 한 정보를 이용하여 사용자 발화를 텍스트 데이터로 변환할 수 있다. 음향 모델 및 언어 모델에 대한 정보는, 예를 들어, 자동 음성 인식 데이터베이스(automatic speech recognition database)(ASR DB)에 저장될 수 있다. 자연어 이해 모듈은 문법적 분석(syntactic analyze) 또는 의미적 분석(semantic analyze)을 수행하여 사용자 의도를 파악할 수 있다. 문법적 분석은 사용자 입력을 문법적 단위(예: 단어, 구, 형태소 등)로 나누고, 나누어진 단위가 어떤 문법적인 요소를 갖는지 파악할 수 있다. 의미적 분석은 의미(semantic) 매칭, 룰(rule) 매칭, 포뮬러(formula) 매칭 등을 이용하여 수행할 수 있다. 이에 따라, 자연어 이해 모듈은 사용자 입력 이 어느 도메인(domain), 의도(intent) 또는 의도를 표현하는데 필요한 파라미터(parameter)(또는, 슬롯(slot))를 얻을 수 있다. 자연어 이해 모듈은 도메인(domain), 의도(intend) 및 의도를 파악하는데 필요한 파라미터(parameter)(또 는, 슬롯(slot))로 나누어진 매칭 규칙을 이용하여 사용자의 의도 및 파라미터를 결정할 수 있다. 예를 들어, 상기 하나의 도메인(예: 알람)은 복수의 의도(예: 알람 설정, 알람 해제 등)를 포함할 수 있고, 하나의 의도는 복수의 파라미터(예: 시간, 반복 횟수, 알람음 등)을 포함할 수 있다. 복수의 룰은, 예를 들어, 하나 이상의 필 수 요소 파라미터를 포함할 수 있다. 매칭 규칙은 자연어 인식 데이터베이스(natural language understanding database)(NLU DB)에 저장될 수 있다. 자연어 이해 모듈은 형태소, 구 등의 언어적 특징(예: 문법적 요소)을 이용하여 사용자 입력으로부터 추 출된 단어의 의미를 파악하고, 파악된 단어의 의미를 도메인 및 의도에 매칭시켜 사용자의 의도를 결정할 수 있 다. 예를 들어, 자연어 이해 모듈은 각각의 도메인 및 의도에 사용자 입력에서 추출된 단어가 얼마나 포 함되어 있는 지를 계산하여 사용자 의도를 결정할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 의 도를 파악하는데 기초가 된 단어를 이용하여 사용자 입력의 파라미터를 결정할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈은 사용자 입력의 의도를 파악하기 위한 언어적 특징이 저장된 자연어 인식 데이터베이 스를 이용하여 사용자의 의도를 결정할 수 있다. 자연어 이해 모듈은 개인 지식 데이터베이스(Private knowledge DB)를 이용하여 사용자 문의를 이 해할 수 있다. 개인 지식 데이터베이스는 전자 장치에 입력된 사용자 인터렉션, 사용자의 검색 히스 토리, 전자 장치가 센싱한 센싱 정보, 외부 장치로부터 수신된 사용자 정보 중 적어도 하나를 바탕으로 지 식 정보들의 관계가 학습될 수 있다. 이때, 개인 지식 데이터베이스는 지식 정보들 사이의 관계를 온톨로 지(ontology) 형태로 저장할 수 있다. 개인 지식 데이터베이스는 새로운 지식 정보가 추가된 경우, 새로운 지식 정보의 추가 정보를 외부 서버 로부터 수신하여 지식 정보와 추가 정보를 온톨로지 형태로 저장할 수 있다. 한편, 개인 지식 데이터베이스 가 온톨로지 형태로 지식 정보를 저장하는 것은 일 실시예에 불과할 뿐, 데이터셋 형태로 정보를 저장할 수 있다. 자연어 이해 모듈은 개인 지식 데이터베이스를 이용하여 사용자의 의도를 결정할 수 있다. 예를 들 어, 자연어 이해 모듈은 사용자 정보(예: 선호 문구, 선호 컨텐츠, 연락처 리스트, 음악 리스트 등)를 이 용하여 사용자의 의도를 결정할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈뿐만 아니라 자동 음성 인식 모듈도 개인 지식 데이터베이스를 참고하여 사용자의 음성을 인식할 수 있다. 자연어 이해 모듈은 사용자 입력의 의도 및 파라미터에 기초하여 패스 룰을 생성할 수 있다. 예를 들어, 자연어 이해 모듈은 사용자 입력의 의도에 기초하여 실행될 앱을 선택하고, 선택된 앱에서 수행될 동작을 결정할 수 있다. 자연어 이해 모듈은 결정된 동작에 대응되는 파라미터를 결정하여 패스 룰을 생성할 수 있다. 일 실시 예에 따르면, 자연어 이해 모듈에 의해 생성된 패스 룰은 실행될 앱, 앱에서 실행될 동작 및 상기 동작을 실행하는데 필요한 파라미터에 대한 정보를 포함할 수 있다. 자연어 이해 모듈은 사용자 입력의 의도 및 파라미터를 기반으로 하나의 패스 룰, 또는 복수의 패스 룰을 생성할 수 있다. 예를 들어, 자연어 이해 모듈은 패스 플래너 모듈로부터 전자 장치에 대응되는 패 스 룰 셋을 수신하고, 사용자 입력의 의도 및 파라미터를 수신된 패스 룰 셋에 맵핑하여 패스 룰을 결정할 수 있다. 이때, 패스 룰은 앱의 기능을 수행하기 위한 동작(또는 오퍼레이션(operation))에 대한 정보 또는 동작을 실행하기 위해 필요한 파라미터에 대한 정보를 포함할 수 있다. 또한, 패스 룰은 앱의 동작 순서를 포함할 수 있다. 전자 장치는 패스 룰을 수신하고, 패스 룰에 따라 앱을 선택하고, 선택된 앱에서 패스 룰에 포함된 동작 을 실행시킬 수 있다. 자연어 이해 모듈은 사용자 입력의 의도 및 파라미터에 기초하여 실행될 앱, 앱에서 실행될 동작 및 상기 동작을 실행하는데 필요한 파라미터를 결정하여 하나의 패스 룰, 또는 복수의 패스 룰을 생성할 수 있다. 예를 들어, 자연어 이해 모듈은 전자 장치의 정보를 이용하여 실행될 앱 및 상기 앱에서 실행될 동작을 사용자 입력의 의도에 따라 온톨로지(ontology) 또는 그래프 모델(graph model) 형태로 배열하여 패스 룰을 생 성할 수 있다. 상기 생성된 패스 룰은, 예를 들어, 패스 플래너 모듈을 통해 패스 룰 데이터베이스(path rule database)에 저장될 수 있다. 상기 생성된 패스 룰은 데이터베이스의 패스 룰 셋에 추가될 수 있다. 자연어 이해 모듈은 생성된 복수의 패스 룰 중 적어도 하나의 패스 룰을 선택할 수 있다. 예를 들어, 자 연어 이해 모듈은 복수의 패스 룰 최적의 패스 룰을 선택할 수 있다. 다른 예를 들어, 자연어 이해 모듈은 사용자 발화에 기초하여 일부 동작만이 특정된 경우 복수의 패스 룰을 선택할 수 있다. 자연어 이해 모듈은 사용자의 추가 입력에 의해 복수의 패스 룰 중 하나의 패스 룰을 결정할 수 있다. 대화 매니저 모듈은 자연어 이해 모듈에 의해 파악된 사용자의 의도가 명확한지 여부를 판단할 수 있다. 예를 들어, 대화 매니저 모듈은 파라미터의 정보가 충분하지 여부에 기초하여 사용자의 의도가 명 확한지 여부를 판단할 수 있다. 대화 매니저 모듈는 자연어 이해 모듈에서 파악된 파라미터가 태스 크를 수행하는데 충분한지 여부를 판단할 수 있다. 일 실시 예에 따르면, 대화 매니저 모듈는 사용자의 의도가 명확하지 않은 경우 사용자에게 필요한 정보를 요청하는 피드백을 수행할 수 있다. 예를 들어, 대화 매 니저 모듈는 사용자의 의도를 파악하기 위한 파라미터에 대한 정보를 요청하는 피드백을 수행할 수 있다. 일 실시 예에 따르면, 대화 매니저 모듈은 컨텐츠 제공(content provider) 모듈을 포함할 수 있다. 컨텐 츠 제공 모듈은 자연어 이해 모듈에서 파악된 의도 및 파라미터에 기초하여 동작을 수행할 수 있는 경우, 사용자 입력에 대응되는 태스크를 수행한 결과를 생성할 수 있다. 다른 실시 예에 따르면, 대화 매니저 모듈은 지식 데이터베이스를 이용하여 사용자 문의에 대한 응 답을 제공할 수 있다. 이때, 지식 데이터베이스는 전자 장치 내에 포함될 수 있으나, 이는 일 실시 예에 불과할 뿐, 외부 서버에 포함될 수 있다. 자연어 생성 모듈은 지정된 정보를 텍스트 형태로 변경할 수 있다. 상기 텍스트 형태로 변경된 정보는 자 연어 발화의 형태일 수 있다. 상기 지정된 정보는, 예를 들어, 추가 입력에 대한 정보, 사용자 입력에 대응되는 동작의 완료를 안내하는 정보 또는 사용자의 추가 입력을 안내하는 정보(예: 사용자 입력에 대한 피드백 정보) 일 수 있다. 상기 텍스트 형태로 변경된 정보는 전자 장치의 디스플레이에 표시되거나, 텍스트 음성 변환 모듈에 의해 음성 형태로 변경될 수 있다. 텍스트 음성 변환 모듈은 텍스트 형태의 정보를 음성 형태의 정보로 변경할 수 있다. 텍스트 음성 변환 모듈은 자연어 생성 모듈로부터 텍스트 형태의 정보를 수신하고, 텍스트 형태의 정보를 음성 형태 의 정보로 변경하여 스피커로 출력할 수 있다. 자동 음성 인식 모듈은 도 10의 음성 인식 모듈로 구현될 수 있고, 자연어 이해 모듈, 대화 매니저 모듈, 자연어 생성 모듈 및 텍스트 음성 변환 모듈이 도 10의 태스크 모듈로 구 현될 수 있다. 한편, 도 10의 모듈들(121, 122, 123, 124) 중 적어도 하나는 전자 장치에 없고 외부 장치에 마련될 수 있 다. 이 경우, 전자 장치는 다른 장치에게 해당 모듈의 기능을 수행할 것을 요청할 수 있다. 통신부는 예를 들면 무선 통신 또는 유선 통신을 통해서 네트워크에 연결되어 외부 장치와 통신할 수 있다. 무선 통신은, 예를 들면, 셀룰러 통신 프로토콜로서, 예를 들면, LTE(long-term evolution), LTE-A(LTE Advance), CDMA(code division multiple access), WCDMA(wideband CDMA), UMTS(universal mobile telecommunications system), WiBro(Wireless Broadband), 또는 GSM(Global System for Mobile Communications) 등 중 적어도 하나를 사용할 수 있다. 또한, 무선 통신은, 예를 들면, 근거리 통신을 포함할 수 있다. 근거리 통신은, 예를 들면, WiFi 다이렉트(wireless fidelity direct), 블루투스(Bluetooth), NFC(near field communication), 직비(Zigbee) 중 적어도 하나를 포함할 수 있다. 유선 통신은, 예를 들면, USB(universal serial bus), HDMI(high definition multimedia interface), RS-232(recommended standard232), 또는 POTS(plain old telephone service) 등 중 적어도 하나를 포함할 수 있다. 네트워크는 통 신 네트워크(telecommunications network), 예를 들면, 컴퓨터 네트워크(computer network)(예: LAN 또는 WAN), 인터넷, 또는 전화 망(telephone network) 중 적어도 하나를 포함할 수 있다. 마이크는 사운드를 수신하기 위한 구성이다. 마이크는 수신된 사운드를 전기적 신호로 변환할 수 있 다. 마이크는 전자 장치와 일체형으로 구현되거나 또는 분리될 수 있다. 분리된 마이크는 전자 장치와 전기적으로 연결될 수 있다. 마이크는 복수 개로 마련될 수 있다. 복수 개의 마이크를 통해 사용자의 이동 방향을 감지할 수 있다. 프로세서는 메모리에 저장된 컴퓨터 실행 가능 명령어(모듈)을 실행함으로써 다양한 기능을 수행할 수 있다. 일 실시 예에 따르면, 프로세서는 메모리에 저장된 컴퓨터 실행가능 명령어를 실행함으로써, 기 설정 된 트리거 워드를 포함한 사용자의 음성을 마이크를 통해 획득하고, 사용자 음성에 포함된 트리거 워드를바탕으로 전자 장치의 음성 인식 기능을 활성화시키고, 음성 인식 기능이 활성화된 동안 사용자가 이동하 는 이벤트를 감지하며, 감지된 이벤트를 바탕으로 타 전자 장치의 음성 인식 기능을 활성화시키기 위한 제어 신 호를 타 전자 장치로 전송하도록 통신부를 제어할 수 있다. 한편, 도시되진 않았지만 전자 장치는 사용자 입력 수신부를 포함할 수 있다. 사용자 입력 수신부는 다양 한 사용자 입력, 예컨대 터치 입력, 모션 입력, 버튼 조작 등을 수신할 수 있다. 예컨대 사용자 입력 수신부는 버튼, 터치 패널 등을 포함할 수 있다. 또한 전자 장치는 다양한 정보를 표시하기 위한 디스플레이를 더 포함할 수 있다. 디스플레이는 예컨대 발광 다이오드(light-emitting diode(LED)), 액정 디스플레이(liquid crystal display(LCD)) 등을 포함할 수 있다. 또한 전자 장치는 카메라를 더 포함할 수 있다. 카메라를 통 해 촬영된 영상은 사용자의 이동 방향을 판단하거나 마이크로 들어오는 노이즈를 감쇄시키는데 사용될 수 있다. 또한 전자 장치는 스피커를 더 포함할 수 있다. 스피커를 통해 사용자 문의에 대한 응답 피드백이 제공될 수 있다. 도 11은 본 개시의 일 실시 예에 따른 허브 장치의 구성을 설명하기 위한 도면이다. 도 11을 참고하면, 허브 장치는 프로세서, 메모리, 통신부를 포함한다. 프로세서는 허브 장치의 전반적인 동작을 제어하기 위한 구성이다. 예를 들면, 프로세서는 운영 체제, 애플리케이션을 구동하여 프로세서에 연결된 다수의 하드웨어 또는 소프트웨어 구성요소들을 제어할 수 있고, 각종 데이터 처리 및 연산을 수행할 수 있다. 프로세서는 CPU(central processing unit) 또는 GPU(graphics-processing unit)이거나 둘 다일 수 있다. 프로세서는 적어도 하나의 범용 프로세서 (general processor), 디지털 신호 프로세서(digital signal processor), ASIC(Application specific integrated circuit), SoC(system on chip), MICOM(Microcomputer) 등으로 구현될 수 있다. 메모리는 내장 메모리 또는 외장 메모리를 포함할 수 있다. 메모리는 프로세서에 의해 액세스되 며, 프로세서에 의한 데이터의 독취/기록/수정/삭제/갱신 등이 수행될 수 있다. 메모리는 하나 이상의 모듈로서 구성된 소프트웨어 및/또는 펌웨어를 포함할 수 있다. 모듈은 컴퓨터 실행 가능 명령어의 집합에 대응될 수 있다. 메모리는 음성 인식 모듈, 핸드 오버 모듈, 정합 모듈, 태스크 모듈를 포함할 수 있 다. 모듈들(221, 222, 223, 224)은 다양한 기능 수행을 위해 프로세서에 의해 실행될 수 있다. 음성 인식 모듈은 앞서 설명한 음성 인식 모듈의 기능과 동일한 기능을 수행할 수 있다. 특히, 전자 장치로부터 언어 모델 또는 음향 모델이 미적용된 음성 인식 정보가 통신부를 통해 수신 되는 경우, 음성 인식 모듈은 수신된 음성 인식 정보에 미적용된 언어 모델 또는 음향 모델을 적용해서 최 종 인식 결과를 획득할 수 있다. 핸드 오버 모듈은 앞서 설명한 핸드 오버 모듈의 기능과 동일한 기능을 수행할 수 있다. 특히, 핸드 오버 모듈은 사용자가 이동하는 이벤트를 감지하여, 전자 장치들 중 어떤 장치로 음성 인 식 작업을 핸드오버할 지를 결정할 수 있다. 예컨대, 핸드 오버 모듈은 전자 장치들 또는 다른 장치 (ex. 센서)에서 획득한 사용자 이동 정보를 수신할 수 있고, 수신된 사용자 이동 정보를 바탕으로 사용자가 이 동하는 이벤트를 감지할 수 있다. 또는, 허브 장치는 사용자의 이동을 감지할 수 있는 구성(ex. 복수의 마 이크, 카메라 등)을 자체적으로 구비할 수도 있다. 정합 모듈은 앞서 설명한 정합 모듈의 기능과 동일한 기능을 수행할 수 있다. 예컨대, 정합 모듈 은 전자 장치들로부터 수신한 음성 인식 정보를 시간 순서로 정합해서 최종 인식 결과를 획득할 수 있다. 태스크 모듈은 앞서 설명한 태스크 모듈의 기능과 동일한 기능을 수행할 수 있다. 예컨대 태스크 모 듈는 최종 인식 결과를 바탕으로 특정 제어 신호를 전자 장치들 중 적어도 하나에 전송하는 태스크를 수행할 수 있다. 예컨대, 최종 인식 결과가 \"오늘 날씨 어때?\"인 경우, 태스크 모듈은 \"오늘 날씨는 맑음 입니다\"라는 음성 응답을 출력하도록 하는 제어 신호를 전자 장치로 전송하는 태스크를 수행할 수 있다. 한편, 허브 장치는 도 10에서 설명한 것과 같은 트리거 워드 모듈을 더 포함할 수 있다. 예컨대, 허 브 장치는 전자 장치들로부터 사용자 음성에 대응되는 오디오 신호를 수신해서 오디오 신호에서 트리 거 워드를 검출할 수 있고, 트리거 워드가 검출되면 해당 오디오 신호를 전송한 전자 장치에게 음성 인식기능을 활성화시키는 제어 신호를 전송할 수 있다. 한편, 도 11의 모듈들(221, 222, 223, 224) 중 적어도 하나는 허브 장치에 없고 외부 장치에 마련될 수 있 다. 이 경우, 허브 장치는 다른 장치에게 해당 모듈의 기능을 수행할 것을 요청할 수 있다. 통신부는 예를 들면 무선 통신 또는 유선 통신을 통해서 네트워크에 연결되어 외부 장치와 통신할 수 있다. 무선 통신은, 예를 들면, 셀룰러 통신 프로토콜로서, 예를 들면, LTE(long-term evolution), LTE-A(LTE Advance), CDMA(code division multiple access), WCDMA(wideband CDMA), UMTS(universal mobile telecommunications system), WiBro(Wireless Broadband), 또는 GSM(Global System for Mobile Communications) 등 중 적어도 하나를 사용할 수 있다. 또한, 무선 통신은, 예를 들면, 근거리 통신을 포함할 수 있다. 근거리 통신은, 예를 들면, WiFi 다이렉트(wireless fidelity direct), 블루투스(Bluetooth), NFC(near field communication), 직비(Zigbee) 중 적어도 하나를 포함할 수 있다. 유선 통신은, 예를 들면, USB(universal serial bus), HDMI(high definition multimedia interface), RS-232(recommended standard232), 또는 POTS(plain old telephone service) 등 중 적어도 하나를 포함할 수 있다. 네트워크는 통 신 네트워크(telecommunications network), 예를 들면, 컴퓨터 네트워크(computer network)(예: LAN 또는 WAN), 인터넷, 또는 전화 망(telephone network) 중 적어도 하나를 포함할 수 있다. 프로세서는 메모리에 저장된 컴퓨터 실행 가능 명령어(모듈)을 실행함으로써 다양한 기능을 수행할 수 있다. 일 실시 예에 따르면, 프로세서는 메모리에 저장된 컴퓨터 실행가능 명령어를 실행함으로써, 기 설정 된 트리거 워드를 포함한 사용자의 음성을 바탕으로 음성 인식 기능이 활성화된 제1 전자 장치로부터, 통신부 를 통해, 음성 인식 정보를 수신하고, 제1 전자 장치를 사용하는 사용자가 이동하는 이벤트를 감지하며, 감지된 이벤트를 바탕으로 제2 전자 장치의 음성 인식 기능을 활성화시키기 위한 제어 신호를 제2 전자 장치로 전송하도록 통신부를 제어하며, 제2 전자 장치로부터 통신부를 통해 음성 인식 정보를 수신하며, 제1 및 제2 전자 장치로부터 수신된 음성 인식 정보를 바탕으로 제어 신호를 제1 및 상기 제2 전자 장치 중 하나로 전송하도록 통신부를 제어할 수 있다. 한편, 도시되진 않았지만 허브 장치는 사용자 입력 수신부를 포함할 수 있다. 사용자 입력 수신부는 다양 한 사용자 입력, 예컨대 터치 입력, 모션 입력, 버튼 조작 등을 수신할 수 있다. 예컨대 사용자 입력 수신부는 버튼, 터치 패널 등을 포함할 수 있다. 또한 허브 장치는 다양한 정보를 표시하기 위한 디스플레이를 더 포함할 수 있다. 디스플레이는 예컨대 발광 다이오드(light-emitting diode(LED)), 액정 디스플레이(liquid crystal display(LCD)) 등을 포함할 수 있다. 또한 허브 장치는 적어도 하나의 마이크를 포함할 수 있다. 허브 장치는 마이크를 통해 수신되는 사용자 음성을 인식해서 그에 대응되는 동작을 수행할 수 있고, 복수 의 마이크를 이용해 사용자의 이동 방향을 감지할 수 있다. 또한 허브 장치는 카메라를 더 포함할 수 있다. 카메라에서 촬영된 영상은 사용자의 이동 방향을 판단하거나 노이즈를 감쇄시키는데 사용될 수 있다. 또 한 허브 장치는 스피커를 더 포함할 수 있다. 스피커를 통해 사용자 문의에 대한 응답 피드백이 제공될 수 있다. 도 12는 본 개시의 일 실시 예에 따른 서버의 구성을 설명하기 위한 블럭도이다. 도 12를 참고하면, 서버는 프로세서, 메모리, 통신부를 포함한다. 프로세서는 서버의 전반적인 동작을 제어하기 위한 구성이다. 예를 들면, 프로세서는 운영 체제, 애플리케이션을 구동하여 프로세서에 연결된 다수의 하드웨어 또는 소프트웨어 구성요소들을 제어할 수 있고, 각종 데이터 처리 및 연산을 수행할 수 있다. 프로세서는 CPU(central processing unit) 또는 GPU(graphics-processing unit)이거나 둘 다일 수 있다. 프로세서는 적어도 하나의 범용 프로세서 (general processor), 디지털 신호 프로세서(digital signal processor), ASIC(Application specific integrated circuit), SoC(system on chip), MICOM(Microcomputer) 등으로 구현될 수 있다. 메모리는 내장 메모리 또는 외장 메모리를 포함할 수 있다. 메모리는 프로세서에 의해 액세스되 며, 프로세서에 의한 데이터의 독취/기록/수정/삭제/갱신 등이 수행될 수 있다. 메모리는 하나 이상의 모듈로서 구성된 소프트웨어 및/또는 펌웨어를 포함할 수 있다. 모듈은 컴퓨터 실행 가능 명령어의 집합에 대응될 수 있다. 메모리는 음성 인식 모듈, 정합 모듈, 태스크 모듈를 포함할 수 있다. 모듈들(321, 322, 323)은 다양한 기능 수행을 위해 프로세서에 의해 실행될 수 있다. 음성 인식 모듈은 앞서 설명한 음성 인식 모듈의 기능과 동일한 기능을 수행할 수 있다. 특히, 전자 장치 또는 허브 장치로부터 언어 모델 또는 음향 모델이 미적용된 음성 인식 정보가 통신 부를 통해 수신되는 경우, 음성 인식 모듈은 수신된 음성 인식 정보에 미적용된 언어 모델 또는 음향 모델을 적용해서 최종 인식 결과를 획득할 수 있다. 태스크 모듈은 앞서 설명한 태스크 모듈의 기능과 동일한 기능을 수행할 수 있다. 예컨대 태스크 모 듈은 최종 인식 결과를 바탕으로 특정 제어 신호를 전자 장치들 중 적어도 하나에 전송하는 태스크를 수행할 수 있다. 예컨대, 최종 인식 결과가 \"오늘 날씨 어때?\"인 경우, 태스크 모듈은 \"오늘 날씨는 맑음 입니다\"라는 음성 응답을 출력하도록 하는 제어 신호를 전자 장치로 전송하는 태스크를 수행할 수 있다. 한편, 서버는 도 10에서 설명한 것과 같은 트리거 워드 모듈을 더 포함할 수 있다. 예컨대, 서버 는 전자 장치들로부터 사용자 음성에 대응되는 오디오 신호를 수신하고 트리거 워드 모듈을 이 용해서 오디오 신호에서 트리거 워드를 검출할 수 있고, 트리거 워드가 검출되면 해당 오디오 신호를 전송한 전 자 장치에게 음성 인식 기능을 활성화시키는 제어 신호를 전송할 수 있다. 한편, 서버는 도 10에서 설명한 것과 같은 핸드 오버 모듈을 더 포함할 수 있다. 예컨대, 서버 는 핸드 오버 모듈을 이용해서 전자 장치들 또는 다른 장치(ex. 센서 등)에서 획득한 사용자 이동 정 보를 수신할 수 있고, 사용자 이동 정보에 기초해서 음성 인식 작업을 사용자 이동 방향에 있는 전자 장치(10 0)에 핸드오버할 수 있다. 한편, 도 12의 모듈들(321, 322, 323) 중 적어도 하나는 서버에 없고 외부 장치에 마련될 수 있다. 이 경 우, 서버는 다른 장치에게 해당 모듈의 기능을 수행할 것을 요청할 수 있다. 통신부는 예를 들면 무선 통신 또는 유선 통신을 통해서 네트워크에 연결되어 외부 장치와 통신할 수 있다. 무선 통신은, 예를 들면, 셀룰러 통신 프로토콜로서, 예를 들면, LTE(long-term evolution), LTE-A(LTE Advance), CDMA(code division multiple access), WCDMA(wideband CDMA), UMTS(universal mobile telecommunications system), WiBro(Wireless Broadband), 또는 GSM(Global System for Mobile Communications) 등 중 적어도 하나를 사용할 수 있다. 또한, 무선 통신은, 예를 들면, 근거리 통신을 포함할 수 있다. 근거리 통신은, 예를 들면, WiFi 다이렉트(wireless fidelity direct), 블루투스(Bluetooth), NFC(near field communication), 직비(Zigbee) 중 적어도 하나를 포함할 수 있다. 유선 통신은, 예를 들면, USB(universal serial bus), HDMI(high definition multimedia interface), RS-232(recommended standard232), 또는 POTS(plain old telephone service) 등 중 적어도 하나를 포함할 수 있다. 네트워크는 통 신 네트워크(telecommunications network), 예를 들면, 컴퓨터 네트워크(computer network)(예: LAN 또는 WAN), 인터넷, 또는 전화 망(telephone network) 중 적어도 하나를 포함할 수 있다. 프로세서는 메모리에 저장된 컴퓨터 실행 가능 명령어(모듈)을 실행함으로써 다양한 기능을 수행할 수 있다. 일 실시 예에 따르면, 프로세서는 메모리에 저장된 컴퓨터 실행가능 명령어를 실행함으로써, 기 설정 된 트리거 워드를 포함한 사용자의 음성을 바탕으로 음성 인식 기능이 활성화된 제1 전자 장치로부터, 통신부 를 통해, 음성 인식 정보를 수신하고, 제1 전자 장치를 사용하는 사용자가 이동하는 이벤트를 감지하며, 감지된 이벤트를 바탕으로 제2 전자 장치의 음성 인식 기능을 활성화시키기 위한 제어 신호를 제2 전자 장치로 전송하도록 통신부를 제어하며, 제2 전자 장치로부터 통신부를 통해 음성 인식 정보를 수신하며, 제1 및 제2 전자 장치로부터 수신된 음성 인식 정보를 바탕으로 제어 신호를 제1 및 상기 제2 전자 장치 중 하나로 전송하도록 통신부를 제어할 수 있다. 도 15 내지 도 17은 음성 인식 시스템에서 음향 모델 및 언어 모델의 사용과 관련한 다양한 실시 예를 설명하기 위한 도면이다. 도 15는 본 개시의 일 실시 예에 따른 음성 인식 시스템에서 음향 모델 및 언어 모델의 사용과 관련한 다 양한 실시 예를 설명하기 위한 도면이다. 음성 인식 시스템은 복수의 전자 장치 및 서버를 포함할 수 있다. \"O\", \"X\"는 음향 모델(AM)과 언어 모델(LM)의 구비 여부를 나타낸 것이다. 실시 예 1.1 에 따르면, 전자 장치는 음향 모델과 언어 모델을 포함하지 않는다. 따라서 음향 모델과 언어 모델이 적용되지 않은 음성 인식 정보, 즉 오디오 신호 자체 또는 오디오 신호로부터 추출된 특징 정보를 서버 로 전송할 수 있고, 서버에서 음향 모델과 언어 모델을 적용해서 음성 인식을 수행할 수 있다. 본 실 시 예에 따르면 전자 장치 내에서의 음성 인식과 관련한 연산 부담이 줄어들 수 있는 장점이 있다. 실시 예 1.2에 따르면, 전자 장치는 음향 모델을 적용한 음성 인식 정보를 서버로 전송할 수 있다. 그리고 서버에서 언어 모델을 적용하여 음성 인식을 수행할 수 있다. 실시 예 1.4에 따르면, 전자 장치는 타 장치에서 음향 모델은 적용된 음성 인식 정보를 수신해서 언어 모 델을 적용할 수 있다. 그리고 서버에서 전자 장치에서 적용되지 않은 언어 모델 및/또는 음향 모델을 음성 인식 정보에 적 용해서 음성 인식을 수행할 수 있다. 본 실시 예들에 따르면, 전자 장치는 음성 인식을 위한 과정 중 일부 만 수행해도 되므로 연산 부담이 줄어들 수 있는 장점이 있다. 또한, 실시 예 1.1 과 실시 예 1.2 또는 실시 예 1.4를 비교해 보면, 실시 예 1.1 은 전자 장치들이 단순히 오디오 신호 또는 특징 정보만 서버로 전 송하기 때문에 전자 장치들의 녹음 특성 차이가 무시된 채로 서버에서는 단순히 이어 붙여서 음성 인 식이 진행되기 때문에 음성 인식 성능이 나쁠 수 있다. 예컨대 전자 장치가 냉장고인 경우, 냉장고 자체에 서 발생하는 노이즈를 고려하여 학습된 음향 모델과 언어 모델을 사용하는 게 바람직하다. 이와 같이 자신의 상 황에 특화된 음향 모델과 언어 모델이 전자 장치들마다 저장되어 있을 수 있고, 따라서 전자 장치가 음향 모델 또는 언어 모델 중 적어도 하나를 적용해서 음성 인식 정보를 생성해서 서버로 전달하면, 서버 에서의 최종 음성 인식 성능이 나아질 수 있다. 한편, 실시 예 1.1의 경우도, 예컨대 냉장고로 구현된 전자 장치가 오디오 신호 또는 특징 정보를 서버 로 전송할 때, 서버에서 냉장고에 맞는 언어 모델과 음향 모델을 적용할 것을 요청하는 정보도 함께 전송할 수 있다. 실시 예 1.3은 전자 장치에 음향 모델과 언어 모델이 모두 포함되어 있는 경우로서, 상술한 실시 예들처럼 음향 모델 및 언어 모델 중 적어도 하나를 적용해서 서버로 전달하는 것이 가능하다. 도 16는 본 개시의 일 실시 예에 따른 음성 인식 시스템에서 음향 모델 및 언어 모델의 사용과 관련한 다 양한 실시 예를 설명하기 위한 도면이다. 음성 인식 시스템은 복수의 전자 장치 및 허브 장치 를 포함할 수 있다. 도 16의 실시 예들 2.1 내지 2.4는 앞서 도 15를 참고하여 설명한 실시 예들 1.1 내지 1.4와 유사한 경우이므로 중복 설명은 생략하도록 한다. 도 17은 본 개시의 일 실시 예에 따른 음성 인식 시스템에서 음향 모델 및 언어 모델의 사용과 관련한 다 양한 실시 예를 설명하기 위한 도면이다. 음성 인식 시스템은 복수의 전자 장치, 허브 장치 및 서버를 포함할 수 있다. 도 17의 실시 예들 3.1 내지 3.16는 허브 장치가 중간에 더 포함되어 있을 뿐, 앞서 도 15를 참고하여 설 명한 실시 예들 1.1 내지 1.4와 유사한 경우이다. 도 17의 실시 예들에 따르면, 전자 장치들에서 허브 장치로, 허브 장치에서 서버로 음성 인식 정보가 전달될 수 있는데, 각 장치들의 능력에 따라 음향 모델 및 언어 모델이 적용되거나 적용되지 않은 채로 다음 장치로 넘길 수 있다. 특히, 실시 예 3.5 내지 3.12의 경우는 전자 장치들에서의 음성 인식 시 연산 부담을 줄이면서도 전자 장치들의 녹음 특성에 특화된 음향 모델 또는 언어 모델이 적용될 수 있는바 음성 인식 성능이 향상될 수 있는 장점이 있다. 한편, 전자 장치들을 포함하는 음성 인식 시스템에서도 어떤 장치는 음향 모델만 사용하고 어떤 장 치는 언어 모델만 사용하는 경우가 있을 수 있다. 이 경우, 특정 전자 장치가 음향 모델만 적용된 음성 인식 정 보를 다른 전자 장치로 전송하면, 다른 전자 장치에서 언어 모델을 적용해서 최종 인식 결과를 얻을 수 있다. 전자 장치들, 허브 장치가 다른 장치로 음성 인식 정보를 전달할 때, 음성 인식 정보가 어디까지 음 성 인식 과정을 수행한 것인지에 대한 정보도 함께 전달할 수 있다. 즉, 예컨대 전자 장치가 허브 장치 로 음성 인식 정보를 전달할 때, 해당 음성 인식 정보는 음향 모델까지만 적용된 것임을 알리는 정보도 함 께 전달할 수 있다.상술한 실시 예들에서는 한 명의 사용자가 전자 장치들 주변을 돌아다니면서 음성을 발화하는 경우에 대해 서 설명하였다. 복수의 사용자가 동시에 전자 장치들 주변을 돌아다니며 음성을 발화하는 경우에도 마찬가 지로 상술한 실시 예들에 따라 음성 인식이 이루어질 수 있다. 다만, 이 경우, 하나의 전자 장치로 동시에 복수의 사용자의 음성이 입력될 수 있으므로, 복수의 사용자의 음성을 구분하기 위해선, 음성 인식 작업을 핸드 오버 할 때, 사용자 정보도 함께 전달될 필요가 있다. 본 개시의 일 실시 예에 따르면, 제1 전자 장치(100-1)는 트리거 워드를 포함하는 사용자 음성에 의해 음성 인 식 기능을 활성화할 수 있고, 사용자의 음성의 특징을 바탕으로 사용자 정보를 획득할 수 있다. 이 때 사용자 정보는 음성을 바탕으로 사용자를 식별할 수 있게 하는 정보로서, 음성 특징 정보일 수 있다. 구체적으로 사용 자 정보는 수신된 음성 신호에 근거하여 음성 신호를 발화한 사용자의 고유 특성을 분석하여, 수신된 음성 신호 의 발화자가 누구인지를 자동적으로 판단하는 동작을 의미할 수 있다. 이 동작은 화자 인식(speaker recognition)으로 음성 인식을 이용한 화자 식별로써, 본인확인을 위해서 사용자 정보로써 이용 될 수 있다. 그 리고 제1 전자 장치(100-1)는 음성 인식 기능이 활성화된 상태에서 사용자 이동 이벤트가 감지되면 사용자의 이 동 방향에 있는 제2 전자 장치(100-2)에게 음성 인식 기능 활성화를 위한 제어신호뿐만 아니라 상기 획득한 사 용자 정보를 전송할 수 있다. 이 때 전송되는 사용자 정보는 음성 신호로부터 획득된 특징 정보 또는 사용자의 ID, 사용자의 이름 등과 같은 프로파일 정보일 수 있다. 그리고 제2 전자 장치(100-2)는 제1 전자 장치(100- 1)로부터 수신한 제어 신호에 의해 음성 인식 기능이 활성화된 후 복수의 사용자의 음성이 마이크를 통해 수신되면, 복수의 사용자의 음성 중 제1 전자 장치(100-1)로부터 수신한 사용자 정보에 대응하는 사용자의 음성 을 식별하고, 식별된 사용자 음성에 대한 음성 인식을 수행할 수 있다. 또한, 제2 전자 장치(100-2)는 제3 전자 장치(100-3)로부터 수신한 사용자 정보에 대응하는 사용자 음성을 식별하고, 식별된 사용자 음성에 대한 음성인 식을 수행할 수 있다. 즉, 복수의 사용자의 음성에 대해 각각 처리할 수 있다. 이와 관련해선 도 26을 참고하여 설명하도록 한다. 도 26은 복수의 사용자가 있을 때 복수의 전자 장치들을 통해 음성 인식을 수행하는 본 개시의 일 실시 예 를 설명하기 위한 도면이다. 도 26을 참고하면, 사용자 A는 침실에서 거실로 이동하면서 \"오늘 저녁 축구 몇시에 시작하더라\"를 발화하고, 사용자 B는 주방에서 거실로 이동하면서 \"어제 주문한 것 택배 언제 도착해\"를 발화하는 상황이다. 사용자 A에 의해 침실의 제1 전자 장치(100-1)가 음성 인식 기능을 활성화하고 사용자 A가 거실로 이동함에 따 라 제1 전자 장치(100-1)가 거실에 있는 제2 전자 장치(100-2)를 활성화시키면서 사용자 A에 대한 정보도 함께 전달한다. 사용자 B에 의해 주방의 제3 전자 장치(100-3)가 음성 인식 기능을 활성화하고 사용자 B가 거실로 이동함에 따 라 제3 전자 장치(100-1)가 거실에 있는 제2 전자 장치(100-2)를 활성화시키면서 사용자 B에 대한 정보도 함께 전달한다. 거실에 있는 제2 전자 장치(100-2)에서는 사용자 A와 사용자 B로부터 동시에 음성을 입력받는 상황이 발생한다. 이 때, 제2 전자 장치(100-2)는 제1 전자 장치(100-1)와 제3 전자 장치(100-3)로부터 수신한 사용자 A에 대한 정보와 사용자 B에 대한 정보를 바탕으로 사용자 A의 음성을 식별하고 사용자 B의 음성을 식별할 수 있다. 예컨 대, 제2 전자 장치(100-2)는 오디오 신호에서 사용자 A에 대한 정보에 대응하는 오디오 신호와 사용자 B에 대한 정보에 대응하는 오디오 신호를 분리할 수 있다. 예컨대, 전자 장치(100-2)는 독립 성분 분석(Independent Component Analysis) 기술을 이용하여 오디오 신호에 서 사용자 A에 대응하는 음원과 사용자 B에 대응하는 음원을 분리해낼 수 있다. 이 경우, 사용자 A와 사용자 B 의 방향을 이용하여 음원을 분리해낼 수 있다. 각 사용자의 방향은 제2 전자 장치(100-2)에 구비된 2 이상의 마 이크를 이용하거나 또는 제2 전자 장치(100-2)에 구비된 카메라를 이용해 식별될 수 있다. 따라서 제2 전자 장치(100-2)는 각 사용자에 대해 개별적으로 인식을 수행할 수 있는 것이다. 따라서 제2 전자 장치(100-2)는 사용자 A에 대한 음성 인식 정보와 사용자 B에 대한 음성 인식 정보를 각각 생 성할 수 있다. 그리고 제2 전자 장치(100-2)는 제1 전자 장치(100-1)로부터 사용자 A의 음성 인식 정보를 수신 하고, 자신이 생성한 사용자 A에 대한 음성 인식 정보와 수신된 사용자 A의 음성 인식 정보를 정합해서 최종 인 식 결과를 획득할 수 있다. 또한 제2 전자 장치(100-2)는 제3 전자 장치(100-1)로부터 사용자 B의 음성 인식 정 보를 수신하고, 자신이 생성한 사용자 A에 대한 음성 인식 정보와 수신된 사용자 B의 음성 인식 정보를 정합해 서 최종 인식 결과를 획득할 수 있다. 제2 전자 장치(100-2)는 사용자 A에 대한 최종 인식 결과를 바탕으로 피드백을 제공할 수 있다. 예컨대 제2 전 자 장치(100-2)는 \"오늘 저녁 축구는 9시에 시작합니다\"라는 음성 응답을 스피커를 통해 출력할 수 있다. 또한 제2 전자 장치(100-2)는 사용자 B에 대한 최종 인식 결과를 바탕으로 피드백을 제공할 수 있다. 예컨대 제2 전 자 장치(100-2)는 \"어제 주문한 택배는 내일 배송될 예정입니다\"라는 음성 응답을 스피커를 통해 출력할 수 있 다. 본 실시 예에 따르면 여러 명의 사용자가 돌아다니면서 음성을 발화하는 상황에서도 각 사용자의 음성을 분리해 서 처리할 수 있게된다. 한편, 이와 같이 복수의 사용자가 동시에 발화하는 실시 예에서도 도 15 내지 도 17을 참고하여 설명한 실시 예 들처럼 음향 모델과 언어 모델이 각 장치들에서 선택적으로 사용될 수 있다. 한편, 상기에선 사용자가 질문하고 바로 답을 받는 싱글턴(single-turn) 시나리오에 대해서만 설명하였으나, 본 개시의 실시 예들은 멀티턴(multi-turn) 시나리오에도 적용될 수 있다. 예를 들어, 도 27을 참고하면, 사용자가 제1 전자 장치(100-1)에서 발화(ex. 오늘 날씨 어때?)를 종료하고 나면 제1 전자 장치(100-1)는 응답(ex. 오늘 날씨는 맑음입니다.)를 제공할 수 있다. 그리고 사용자가 상기 발화 종 료 이후 특정 시간 내에 제2 전자 장치(100-2)에서 발화(ex. 내일은?)를 한 경우, 제2 전자 장치(100-2)는 제1 전자 장치(100-1)에서 입력된 발화(오늘 날씨 어때?)를 참조하여 응답(ex. 내일 날씨는 비가올 예정입니다)을 제공할 수 있다. 이를 위해, 제1 전자 장치(100-1)는 제2 전자 장치(100-2)로 음성 인식 작업을 핸드 오버할 때 사용자 정보 및 사용자 음성의 컨텍스트 정보도 함께 전달할 수 있다. 컨텍스트 정보는 대화의 주제, 카테고리에 대한 정보를 의미한다. 예컨대, '오늘 날씨 어때?'에 대한 컨텍스트 정보는 '날씨', '문의'와 같은 정보를 포함할 수 있다. 제1 전자 장치(100-1)로부터 수신된 사용자 정보와 컨텍스트 정보를 참조해서 대화가 이어짐을 판단할 수 있고 이전 대화가 어떤 상황이었는지를 판단해서 적절한 다음 응답을 제공할 수 있다. 예컨대, 제2 전자 장치(100- 2)는 제1 전자 장치(100-1)에서 발화 종료가된 시점부터 기 설정된 시간 내에 제1 전자 장치(100-1)로부터 수신 한 사용자 정보에 대응하는 음성이 들어오는지를 확인할 수 있고, 대응되는 음성이 수신되면 해당 음성에 대해 컨텍스트 정보를 바탕으로 응답을 제공할 수 있다. 한편, 이와 같은 멀티 턴 실시 예에서도 도 15 내지 도 17을 참고하여 설명한 실시 예들처럼 음향 모델과 언어 모델이 각 장치들에서 선택적으로 사용될 수 있다. 한편, 상술한 실시 예들에선 사용자가 움직이는 이벤트가 감지되었을 때 음성 인식 작업이 다른 장치로 핸드 오 버되는 것으로 설명하였으나, 사용자가 움직이는 이벤트 이외에도 다른 장치에서 음성 인식을 이어 가는 것이 적절한 상황이라고 판단되면 음성 인식 작업이 핸드오버 될 수 있다. 일 실시 예에 따르면, 전자 장치는 음성 인식 기능이 활성화된 상태에서 입력되는 소리에 노이즈가 기 설 정된 정도 이상 포함되어 있다고 판단되면, 타 전자 장치로 음성 인식 기능을 활성화시키기 위한 제어 신호를 전송할 수 있다. 본 실시 예와 관련해선 도 28을 참고하여 설명하도록 한다. 도 28은 본 개시의 또 다른 실시 예에 따른 음성 인식 작업을 핸드오버 하는 방법을 설명하기 위한 도면이다. 도 28을 참고하면, 사용자가 트리거 워드를 발화하여 제1 전자 장치(100-1)가 음성 인식 기능이 활성화된 동안, 옆에 있던 TV가 켜지는 상황이다. 제1 전자 장치(100-1)가 TV에서 나오는 소리로 인해 사용자의 말을 잘 인식하 지 못할 것 같음을 감지하면, 사용자가 있는 방향 또는 주변 기기 중에 음성 인식 작업을 핸드 오버할 기기를 선택한다. 음성 인식 작업을 핸드 오버할 장치로 제2 전자 장치(100-2)가 선택되면 제1 전자 장치(100-1)는 제2 전자 장치(100-2)로 음성 인식 기능을 활성화시키는 제어신호를 전송할 수 있다. 그러면 제2 전자 장치(100- 2)에서 이어서 음성 인식이 수행될 수 있다. 도 29는 본 개시의 또 다른 실시 예에 따른 음성 인식 작업을 핸드오버 하는 방법을 설명하기 위한 도면이다. 본 개시에 따른 전자 장치들이 네비게이션(100-1)과 모바일 기기(100-2)으로 구현된 것을 도시한 것이다. 사용자가 주차를 하면서 네비게이션(100-1)에, \"빅스비, 오늘 다섯시에 양재역 화포식당에 6명 자리 예약하라고 홍길동한테 문자 보내줘\" 라고 말하는 도중에 시동을 끄고 하차하는 상황에서, 네비게이션(100-1)에서 \"빅스 비\"를 인식해서 음성 인식을 시작한다. 네비게이션(100-1)은 차의 시동이 꺼지면 전원이 함께 꺼지는 불안정한 기기이므로, 네비게이션(100-1)은 시동일 꺼질 수 있는 상황을 감지하고 음성 인식 작업을 모바일 기기(100-2)로 핸드오버할 수 있다. 예컨대, 네비게이션(100-1)은 차량의 속도가 기 설정된 속도 이하이고 후진 주행이 감지되면 주차를 하는 상황으로 감지하여 모바일 기기(100-2)로 음성 인식 작업을 핸드오버할 수 있다. 이 때, 네비게이션(100-1)은 음성 인식 정보를 모바일 기기(100-2)로 전송할 수 있다. 이 때, 사용자 정보 및 정합을 위한 추가 정보(시간 정보, 녹음 특성, 음성 인식 진행 상황 등에 대한 정보)도 함께 전송할 수 있다. 따라서 사용자가 차에 내려서도 계속해서 음성 인식이 진행되고, 발화가 끝나면 모바일 기기(100-2)는 네비게이 션(100-1)에서 받은 음성 인식 정보와 자신에서 생성된 음성 인식 정보와 정합하여 최종 음성 인식 결과를 획득 할 수 있고, 이를 바탕으로 태스크를 수행할 수 있다. 한편, 본 개시의 또 다른 실시 예에 따르면, 장치의 배터리 상태도 고려해서 주변 장치로 음성 인식 작업을 핸 드 오버하는 것도 가능하다. 즉, 장치의 배터리가 기 설정된 함량 이하로 남아 있는 경우에 다른 장치로 음성 인식 작업을 핸드오버 할 수 있다. 도 30은 본 개시의 일 실시 예에 따른 전자 장치를 제어하는 방법에 대한 흐름도이다. 도 30에 도시된 흐름도는 본 명세서에서 설명되는 전자 장치에서 처리되는 동작들로 구성될 수 있다. 따라서, 이하에서 생략된 내용 이라 하더라도 전자 장치에 관하여 기술된 내용은 도 30 도시된 흐름도에도 적용될 수 있다. 도 30을 참고하면, 전자 장치는 기 설정된 트리거 워드를 포함한 사용자의 음성을 전자 장치의 마이크를 통해 획득할 수 있다(S3010). 그리고 전자 장치는 사용자 음성에 포함된 트리거 워드를 바탕으로 전자 장 치의 음성 인식 기능을 활성화할 수 있다(S3020). 그리고 전자 장치는 음성 인식 기능이 활성화된 동안 사 용자가 이동하는 이벤트를 감지할 수 있다(S3030). 그리고 전자 장치는 감지된 이벤트를 바탕으로 타 전자 장치의 음성 인식 기능을 활성화시키기 위한 제어 신호를 타 전자 장치로 전송할 수 있다(S3040). S3030 단계에서, 전자 장치는 음성 인식 기능이 활성화된 이후 마이크를 통해 획득된 사용자의 음성의 신 호에 기초하여 사용자가 이동하는 이벤트를 감지할 수 있다. 한편, 마이크를 통해 획득된 음성의 신호에 기초 하여 이벤트를 감지할 수 있으나, 그외에도 카메라 등 다른 센서를 이용해서도 가능하다. S3040에서, 전자 장치는 기 저장된, 음성 수신이 가능한 복수의 타 전자 장치에 대한 정보를 이용하여 제 어 신호를 전송할 수 있다. 예컨대, 전자 장치는 사용자가 이동하는 이벤트가 감지되면, 사용자의 이동 정 보를 획득하고, 사용자의 이동 정보에 기초하여 복수의 타 전자 장치 중 사용자와 가장 가까운 타 전자 장치를 식별하고, 식별된 타 전자 장치로 제어신호를 전송할 수 있다. 한편, 전자 장치를 제어하는 방법은 마이크를 통해 획득된 상기 사용자의 음성에 대해 음성 인식을 수행하 여 제1 음성 인식 정보를 획득하고, 제어 신호를 수신한 타 전자 장치로부터 제2 음성 인식 정보를 수신하고, 제1 음성 인식 정보와 제2 음성 인식 정보를 바탕으로 최종 인식 결과를 획득하는 단계를 더 포함할 수 있다. 이 경우, 제어 신호를 상기 타 전자 장치에 전송한 시간에 대한 정보를 획득하고, 상기 획득한 시간에 대한 정 보를 기초로 상기 제1 음성 인식 정보와 상기 제2 음성 인식 정보를 정합하여 최종 인식 결과를 획득할 수 있다. 이 때, 획득한 시간에 대한 정보는, 제어 신호를 전송한 절대적 시간에 대한 정보 또는 전자 장치의 음성 인식 기능이 활성화된 시점을 기준으로 제어 신호를 상기 타 전자 장치에 전송한 시점의 상대적 시간에 대 한 정보를 포함할 수 있다. 한편, 전자 장치는 상기 타 전자 장치로부터 수신된 상기 제2 음성 인식 정보가 음향 모델이 적용되고 언 어 모델이 미적용된 정보인 경우, 상기 제2 음성 인식 정보에 상기 전자 장치에 기 저장된 언어 모델을 적용하 여 상기 최종 인식 결과를 획득하고, 타 전자 장치로부터 수신된 상기 제2 음성 인식 정보가 음향 모델 및 언어 모델이 미 적용된 정보인 경우, 상기 제2 음성 인식 정보에 상기 전자 장치에 기 저장된 음향 모델과 언어 모델 을 적용하여 최종 인식 결과를 획득할 수 있다. 한편, 전자 장치는 최종 인식 결과에 대한 피드백이 상기 타 전자 장치에서 제공되도록 하는 제어 신호를 상기 타 전자 장치로 전송할 수 있다. 한편, 전자 장치는 타 전자 장치로부터 음성 인식 기능을 활성화하도록 하는 제2 제어 신호가 수신되면 전 자 장치의 음성 인식 기능을 활성화할 수 있다. 이 경우, 전자 장치는 타 전자 장치로부터 사용자 정보를 수신하고, 상기 제2 제어 신호에 의해 음성 인식 기능이 활성화된 후 복수의 사용자의 음성이 상기 마이크를 통해 수신되면, 상기 복수의 사용자의 음성 중 상기 타 전자 장치로부터 수신한 사용자 정보에 대응하는 사용자의 음성을 식별하고, 식별된 사용자 음성에 대한 음성 인식을 수행할 수 있다. 한편, 전자 장치는 상기 제2 제어 신호에 의해 음성 인식 기능이 활성화된 후 사용자의 발화가 종료될 때 까지 상기 마이크를 통해 수신한 음성에 대해 음성 인식을 수행하여 음성 인식 정보를 획득하고, 획득한 음성 인식 정보를 상기 타 전자 장치로 전송할 수 있다. 한편, 전자 장치는 상기 기 설정된 트리거 워드를 포함한 제1 사용자의 음성이 상기 마이크를 통해 수신되 어 음성 인식 기능이 활성화된 상태에서 상기 제2 제어 신호 및 제2 사용자에 대한 정보가 상기 타 전자 장치로 부터 수신되면, 상기 마이크를 통해 획득된 상기 제1 사용자의 음성과 상기 제2 사용자의 음성을 각각 처리할 수 있다. 상술한 실시 예들에 따르면, 사용자가 이동하거나 상황이 변경되어 음성 인식의 질이 떨어질 것으로 예상되면, 현재 기기가 해당 방향 또는 거리를 추정하여 그 쪽의 다른 기기에 음성 인식을 이어받을 수 있도록 신호를 보 내서, 음성 인식이 이어질 수 있도록 한다. 이와 같이 각자의 기기에서 음성 인식을 하고, 그 결과를 결합하여 현재 사용자의 상황에 맞는 기기에서 최종 처리 응답을 진행할 수 있다. 또한, 음성 인식을 이어받도록 할 때에 녹음 특성 등에 대한 정보도 함께 전달하여 최종 결과를 생성하는데 도움을 줄 수 있다. 상술한 실시 예들에 따르면 기기가 능동적으로 음성 인식을 이어받을 타 기기를 선택할 수 있는바, 녹음의 질이 떨어질 것 같은 상황이거나, 녹음이 불가능한 상황 또는 사용자가 이동하는 상황에서도 여러 기기가 협동하여 음성 인식이 매끄럽게 이어질 수 있도록 할 수 있는 효과가 있다. 이하에서는 도 31 내지 도 37을 참고하여, 사용자가 이동하는 상황에서 여러 장치들로부터 수집된 음성을 처리 하는 추가적인 실시 예들에 대해 설명한다. 본 개시의 일 실시 예에 따르면, 엣지 컴퓨팅 장치는 제1 오디오 수신 장치(10-1) 및 제2 오디오 수신 장치 (10-2) 각각으로부터 수신된 오디오 신호를 정합하여 음성 인식을 수행할 수 있다. 구체적으로, 엣지 컴퓨팅 장치는 사용자의 음성에 따른 오디오 신호를 제1 오디오 신호 수집 장치(10-1)로 부터 수신할 수 있다. 그리고, 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치(10-1)로부터 수신된 오디오 신호에 포함된 정보를 바탕으로 사용자의 이동이 감지되면, 사용자의 이동 방향에 위치하는 제2 오디오 신호 수 집 장치(10-2)로부터 사용자의 음성에 따른 오디오 신호를 수신하기 위한 제어 신호를 제2 오디오 신호 수집 장 치(10-2)로 전송할 수 있다. 그리고, 엣지 컴퓨팅 장치는 오디오 신호를 제2 오디오 신호 수집 장치(10- 2)로부터 수신하고, 제1 오디오 신호 수집 장치(10-1)로부터 수신된 오디오 신호 및 제2 오디오 신호 수집 장치 (10-2)로부터 수신된 오디오 신호를 정합하여 사용자의 음성에 대한 음성 인식을 수행할 수 있다. 이하에서는 도 31 내지 37을 참조하여, 보다 구체적으로 설명한다. 도 31은 사용자의 이동에 따라 복수의 오디오 신호 수집 장치들로부터 수신된 오디오 신호를 정합하는 과정에 대해 설명하기 위한 도면이다. 댁 내에서는 도 31에 도시된 바와 같이 오디오 신호 수집 장치들(10-1 내지 10-7) 및 엣지 컴퓨팅 장치들(20-1, 20-2)이 배치될 수 있다. 이하에서, 오디오 신호 수집 장치들(10-1 내지 10-7)은 앞서 설명한 실시 예들에서 전 자 장치들(100-1, 100-2, 쪋 100-N)에 해당할 수 있다. 따라서 이하에서 생략된 내용이라 하더라도 전자 장치들 (100-1, 100-2, 쪋 100-N)에 관하여 기술된 내용은 오디오 신호 수집 장치들(10-1 내지 10-7)에도 적용될 수 있 다. 또한, 엣지 컴퓨팅 장치들(20-1, 20-2)은 앞서 설명한 실시 예들에서 허브 장치에 해당할 수 있다. 따 라서 이하에서 생략된 내용이라 하더라도 허브 장치에 관하여 기술된 내용은 엣지 컴퓨팅 장치들(20-1, 20-2)에도 적용될 수 있다. 엣지 컴퓨팅 장치들(20-1 내지 20-2)은 엣지 컴퓨팅 기술이 구현된 장치로서, 엣지 컴퓨팅 기술은 기존의 서버 에 의한 클라우드 컴퓨팅의 한계를 보완하기 위한 기술이다. 특히, IoT 기기들의 데이터 양이 많아지고, 실시간 처리가 중요해지면서 서버에 의한 클라우드 컴퓨팅이 한계에 부딪히게 됐는데, 엣지 컴퓨팅 기술에서는 종래에 서버에서 이루어지던 컴퓨팅 작업의 일부 또는 경우에 따라서는 전부를 수행할 수 있다. 엣지 컴퓨팅은 IoT 기 기들과 가까운 주변이나, IoT 기기 자체에서 데이터를 분산 처리하는 기술로서, 기존 클라우드 컴퓨팅 기술보다 데이터 처리를 빠르게 할 수 있다. 따라서 이러한 엣지 컴퓨팅 기술이 구현된 엣지 컴퓨팅 장치들(20-1 내지 20-2)은 오디오 신호 수집 장치들(10-1 내지 10-7)로부터 수신되는 데이터를 로컬에서 보다 효율적으로 처리할 수 있다. 오디오 신호 수집 장치들(10-1 내지 10-7)(이하, 통합적으로 오디오 신호 수집 장치로 설명함) 및 엣지 컴 퓨팅 장치들(20-1 내지 20-2)(이하, 통합적으로 엣지 컴퓨팅 장치로 설명함)은 컴퓨팅 능력이 있는 전자 장 치로서, 컴퓨터 실행 가능한 인스트럭션들을 저장하는 메모리와, 상기 인스트럭션들을 실행하여 특정 기능을 수 행할 수 있는 프로세서를 포함한다. 따라서 이하 설명되는 오디오 신호 수집 장치의 기능은 오디오 신호 수 집 장치의 프로세서에 의해 구현되며, 엣지 컴퓨팅 장치의 기능은 엣지 컴퓨팅 장치의 프로세서에 의해 구현될 수 있다. 오디오 신호 수집 장치들은 마이크를 구비하며, 구비된 마이크를 통해 사용자의 음성을 수신할 수 있으며, 수신한 음성에 대응하는 오디오 신호를 엣지 컴퓨팅 장치들 중 적어도 하나로 전송할 수 있다. 그리고 엣지 컴퓨팅 장치들은 여러 오디오 신호 수집 장치들로부터 수신한 오디오 신호들에 대한 음성 인식을 수행 할 수 있다. 일 실시 예에 따르면, 사용자가 댁 내에서 이동하면서 음성을 발화하면 오디오 신호 수집 장치들은 음성을 수신할 수 있고, 수신된 음성에 대응되는 오디오 신호를 엣지 컴퓨팅 장치들 중 적어도 하나로 전송할 수 있고, 엣지 컴퓨팅 장치들 중 적어도 하나는 각 오디오 신호 수집 장치들(10-1 내지 10-7)로부터 수신한 오 디오 신호들을 발화 순서에 따라 잇는 정합 처리를 하여 최종 음성 인식 결과를 획득할 수 있다. 오디오 신호 수집 장치는 마이크를 통해 획득된 오디오 신호에서 음성을 감지할 수 잇다. 예컨대, VAD(Voice Activity Detection) 및/또는 EPD(End Point Detection) 기술을 통하여 음성구간, 잡음구간 및 배경 잡음을 분리할 수 있다. VAD 기술은 사운드의 크기나 주파수 영역에서의 에너지 분포 등에 기반한 통계적 모델, 딥러닝 모델 등으로 사람의 목소리를 감지할 수 있는 기술이고, EPD 기술은 사운드에서 사람의 음성의 끝점을 검출하는 기술이다. 오디오 신호 수집 장치는 오디오 신호 수집 장치에 포함된 마이크를 통해 획득한 오디오 신호에서 음성 이 감지되면, 오디오 신호를 엣지 컴퓨팅 장치들 중 적어도 하나로 전송할 수 있다. 즉, 획득한 오디오 신 호를 항상 엣지 컴퓨팅 장치로 전송하는 경우 전송 부담이 발생할 수 있으므로, 음성 인식이 필요한 경우에 만 오디오 신호를 전송할 수 있다. 한편, 본 개시의 또 다른 실시 예에 따르면, 오디오 신호 수집 장치는 마이크를 통해 획득한 오디오 신호에 서 음성 구간이 검출되지 않았더라도, 엣지 컴퓨팅 장치의 제어에 의해 오디오 신호 전송 동작을 활성화할 수 있다. 예컨대, 엣지 컴퓨팅 장치는 특정 오디오 신호 수집 장치(10-1)로부터 수신되는 오디오 신호의 품 질, 예컨대 파워(power) 및/또는 신호 대 잡음비(SNR)가 낮아지는 것을 기초로 사용자가 이동함을 판단할 수 있 고, 이 경우, 사용자와 더 가까이 있어 더 좋은 품질의 오디오 신호를 줄 수 있는 오디오 신호 수집 장치(10- 2)에게 오디오 신호를 전송해줄 것을 요청할 수 있다. 오디오 신호 수집 장치는 선택적으로 카메라를 더 포함할 수 있다. 오디오 신호 수집 장치는 카메라를 통해 획득한 영상을 분석하여 사용자의 이동을 감지할 수 있다. 예컨대, 오디오 신호 수집 장치는 카메라를 통해 획득된 영상 내에서 객체를 인식하고, 인식된 객체를 트레킹하여 사용자의 이동을 감지할 수 있다. 또 다른 실시 예에 따르면, 오디오 신호 수집 장치는 카메라를 통해 획득된 영상을 엣지 컴퓨팅 장치들 중 적어도 하나로 전송할 수 있다. 이 경우 엣지 컴퓨팅 장치들 중 적어도 하나에서 영상을 분석하여 객체 인식하고 트래킹하여 사용자의 이동을 감지할 수 있다. 엣지 컴퓨팅 장치들도 카메라를 구비할 수 있어, 자 체적으로 획득한 영상을 바탕으로 사용자의 이동을 감지할 수 있다. 또한, 엣지 컴퓨팅 장치는 복수의 마이크를 통해 입력되는 사용자 음성의 크기 차이를 통해 사용자가 이동 하는 것을 감지할 수 있다. 예컨대, 엣지 컴퓨팅 장치는 복수의 마이크를 구비한 오디오 신호 수집 장치 로부터 수신되는 오디오 신호들에서 사용자 음성의 크기 차이를 기반으로 사용자의 이동을 감지할 수 있다. 또는 엣지 컴퓨팅 장치가 자체적으로 복수의 마이크를 구비할 수 있어, 이를 통해 수신되는 사용자 음성의 크기 차이를 기반으로 사용자의 이동을 감지할 수도 있다. 한편, 엣지 컴퓨팅 장치는 오디오 신호 수집 장치에 포함된 복수의 마이크를 통해 입력되는 사용자 음 성 신호의 방향 정보를 획득하고, 획득된 방향 정보를 바탕으로 사용자의 이동을 감지할 수도 있다. 구체적으로, 복수의 마이크는 복수의 마이크가 등간격 또는 비 등간격으로 정렬된 마이크 어레이(microphone array)로 구현될 수 있다. 그리고, 사용자 음성 신호의 방향 정보는 마이크 어레이를 이용한 DOA(Direction of Arrival) 기법 등에 의해 획득될 수 있다. 여기서, DOA 기법이란 마이크 어레이에 포함되는 복수의 마이크 중 각각의 마이크를 통해 들어오는 음성 신호 간의 상관관계를 이용하여 음성 신호에 대한 방향 정보를 획득하는 기법을 말한다. 구체적으로, DOA 기법에 따르면, 음성 신호가 복수의 마이크에 특정 입사각으로 수신되는 경우, 엣지 컴퓨팅 장치는 복수의 마이크에 포함되는 각각의 마이크에 음성 신호가 도착하는 거리의 차이에 따른 지연 거리 및 지연 시간을 바탕으로 음성 신호의 입사각을 획득하고, 획득된 입사각을 바탕으로 수신된 음성 신호에 대한 방향 정보를 획득할 수 있다. 오디오 신호 수집 장치는 선택적으로 스피커를 더 포함할 수 있고, 스피커를 통해 사용자 음성에 대한 응답 을 출력할 수 있다. 예컨대, 엣지 컴퓨팅 장치는 음성 인식 결과에 대응하는 응답 음성을 오디오 신호 수집 장치로 전송할 수 있고, 오디오 신호 수집 장치에서 이 응답 음성이 출력될 수 있다. 엣지 컴퓨팅 장치는 트리거 워드 모듈과 음성 인식 모듈을 포함할 수 있다. 트리거 워드 모듈은 오디오 신 호 내에서 기 설정된 트리거 워드 또는 문구를 인식할 수 있고, 트리거 워드가 인식되면, 음성 인식 모듈을 활 성화(또는 구동)시킬 수 있다. 트리거 워드는 미리 정해진 단어, 문장일 수 있다. 예컨대 '하이 빅스비' 등이 사용될 수 있다. 본 개시의 일 실시 예에 따르면, 엣지 컴퓨팅 장치는 여러 대의 오디오 신호 수집 장치로부터 수신된 오디오 신호로부터 트리거 워드를 인식할 수 있다. 도 32는 본 개시의 일 실시 예에 따른 엣지 컴퓨팅 장치가 여러 대의 오디오 신호 수집 장치로부터 수신된 오디오 신호로부터 트리거 워드를 인식하는 방식을 설명하기 위한 도면이다. 엣지 컴퓨팅 장치는 메모리 내에 소프트웨어 모듈인 트리거 워드 모듈과 음성 인식 모듈을 포 함할 수 있다. 엣지 컴퓨팅 장치의 프로세서는 이러한 소프트웨어 모듈을 실행시켜 트리거 워드 인식, 음성 인식 기능을 수행할 수 있다. 도 32를 참고하면 사용자가 제1 오디오 신호 수집 장치(10-1)에서 제2 오디오 신호 수집 장치(10-2)로 이동하면 서 \"하이 빅스비\"를 말한 경우, 제1 및 제2 오디오 신호 수집 장치(10-1, 10-2)가 음성을 감지하여 엣지 컴퓨팅 장치에 오디오 신호를 전송할 수 있다. 오디오 신호는 프레임 단위(ex. 20ms)로 구성될 수 있다. 이 경우, 트리거 워드 모듈은 제1 방법 및 제2 방법 중 하나로 트리거 워드를 인식할 수 있다. 다만, 본 개시에서 트리거 워드 모듈과 음성 인식 모듈을 구별하여 설명하는 것은 본 개시에 따른 엣지 컴퓨 팅 장치의 동작을 보다 명확하게 설명하기 위한 것일 뿐이며, 따라서 이하에서 설명하는 제1 방법 및 제2 방법이 음성 인식 모듈에서 수행될 수도 있음은 물론이다. 제1 방법에 따르면, 엣지 컴퓨팅 장치의 트리거 워드 모듈은 엣지 컴퓨팅 장치와 연결된 각 오디 오 신호 수집 장치들(10-1, 10-2)부터 오디오 신호를 수신할 수 있다. 트리거 워드 모듈은 각 오디오 신 호 수집 장치들(10-1, 10-2)로부터 수신된 오디오 신호를 구성하는 프레임들 각각의 신호의 파워(power) 및/또 는 신호 대 잡음비(SNR)를 비교하여 음성 인식에 더 좋은 프레임, 즉 파워 및/또는 신호 대 잡음비가 더 큰 프 레임을 식별할 수 있다. 예컨대, \"하이\"부분에 해당하는 프레임들은 제1 오디오 신호 수집 장치(10-1)로부터 수 신한 것이 좋고, \"빅스비\"부분에 해당하는 프레임들은 제2 오디오 신호 수집 장치(10-2)로부터 수신 것이 좋다 면, 트리거 워드 모듈은 제1 오디오 신호 수집 장치(10-1)로부터 수신한 \"하이\"부분에 해당하는 프레임들 을 음성 인식에 사용하고, 제2 오디오 신호 수집 장치(10-2)로부터 수신한 \"빅스비\"부분에 해당하는 프레임들을 음성 인식에 사용할 수 있다. 구체적으로, 제1 오디오 신호 수집 장치(10-1)로부터 수신한 오디오 신호에서 음 성 인식에 좋은 프레임들을 식별하고, 제2 오디오 신호 수집 장치(10-2)로부터 수신한 오디오 신호에서 음성 인 식에 좋은 프레임들을 식별하여, 식별된 프레임들을 시간 순으로 정합하고, 정합된 프레임들을 바탕으로 트리거 워드를 인식할 수 있다. 예를 들어, 도 32의 그래프를 참조하면, 트리거 워드 모듈은 파워 또는 신 호 대 잡음비가 소정의 값 이상인 프레임을 식별하고, 식별된 프레임을 정합하고, 정합된 프레임들을 바 탕으로 트리거 워드를 인식할 수 있다. 구체적으로, 오디오 신호 수집 장치(10-1, 10-2)로부터 오디오 신호가 수신되면, 트리거 워드 모듈은 제1 오디오 신호 수집 장치(10-1)로부터 수신한 오디오 신호의 프레임과 제2 오디오 신호 수집 장치(10-2)로부터 수 집한 프레임의 파워 및/또는 신호대 잡음비(SNR)를 비교하여 인식에 더 좋은 프레임을 식별하고, 식별된 프레임 을 시간 순으로 정합하며, 정합된 프레임들을 바탕으로 트리거 워드를 인식할 수 있다. 오디오 신호 수집 장치로부터 수신한 오디오 프레임을 비교하는 단위는 한 프레임 단위의 파워 및/또는 신호대 잡음비(SNR)가 될 수 있으며, 설정에 따라 N 개 프레임 단위의 파워 및/또는 신호대 잡음비(SNR)가 될 수도 있 다. 제2 방법에 따르면, 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치(10-1)로부터 수신된 오디오 신호에 서 음성이 작아지는 것으로 판단되면, 제2 오디오 신호 수집 장치(10-2)를 활성화시킬 수 있다. 활성화시킨다는의미는 오디오 신호를 전송하도록 요청하는 것이다. 구체적으로, 트리거 워드 모듈은 상술한 바와 같은 제1 방법과는 다르게, 특정 시점에 제2 오디오 신호 수집 장치(10-2)로부터 수신되는 오디오 신호의 프레임이 제1 오디오 신호 수집 장치(10-1)로부터 수신되는 오 디오 신호의 프레임보다 더 좋은 것으로 판단되면 그 특정 시점 이전까지 제1 오디오 신호 수집 장치(10-1)로부 터 수신되는 오디오 신호의 프레임들과, 특정 시점 이후에 제2 오디오 신호 수집 장치(10-2)로부터 수신되는 오 디오 신호의 프레임들을 정합하고, 정합된 프레임들을 바탕으로 트리거 워드를 인식할 수 있다. 예를 들어, 도 32의 그래프를 참조하면, 트리거 워드 모듈은 파워 또는 신호 대 잡음비가 소정의 값 이상인 프레임을 식별하고, 식별된 프레임을 정합하고, 정합된 프레임들을 바탕으로 트리거 워드를 인식할 수 있다. 상술한 실시 예들에 따르면, 사용자가 이동하면서 트리거 워드를 발화하더라도 정확하게 이를 인식할 수 있는 장점이 있다. 트리거 워드 모듈이 이와 같이 트리거 워드를 인식하면, 트리거 워드 모듈은 음성 인식 모듈(322 0)을 활성화(또는 구동)시킬 수 있다. 음성 인식 모듈은 트리거 워드 모듈에 의해 활성화되면 음성 인식을 수행할 수 있다. 음성 인식 모 듈은 사용자 음성에 대응하는 오디오 신호를 단어 열(텍스트)로 변환할 수 있다. 도 33을 참고하면, 음성 인식 모듈은 크게 음성의 특징되는 부분을 추출하는 과정, 추출된 특징 정보를 음향 모델(Acoustic mode(AM))에 통과시키는 과정 및 선택적으로, 음향 모델을 통과한 정보를 언어 모델에 통과 시키는 과정을 포함할 수 있다. 구체적으로, 음성 인식 모듈은 오디오 신호로부터 특징 정보를 추출(획득)한다. 예컨대 오디오 신호로부 터 켑스트럼(Cepstrum), 선형 예측 코딩(Linear Predictive Coefficient, LPC), 멜프리퀀시 켑스트럼(Mel Frequency Cepstral Coefficient, MFCC) 및 필터 뱅크 에너지(Filter Bank Energy) 중 적어도 하나를 포함하는 특징 정보를 추출할 수 있다. 음성 인식 모듈은 특징 정보를 음향 모델(Acoustic mode(AM))에 통과시켜 발음열, 문자열, 단어열을 획득 할 수 있다. 음성 인식 모듈는 선택적으로 언어 모델(language model(LM))을 더 포함할 수 있다. 언어 모델은 음향 모 델을 통과하여 얻은 정보를 보완하기 위해 사용될 수 있다. 예컨대, \"너무 더워 에어컨 온도 낮춰줘\"라고 말했 을 때, 음향모델에만 의존할 경우 \"온도\"가 \"운동\"으로 잘못 인식될 수 있다. 언어 모델은 단어들 간의 관계를 확률로 나타내어 서로 관계가 높은 단어들이 결합할 가능성을 높이는 역할을 하므로, 이러한 잘못 인식되는 문 제를 차단할 수 있다. 한편, 전술한 바와 같이, 엣지 컴퓨팅 장치는 특정 오디오 신호 수집 장치(10-1)로부터 수신되는 오디오 신 호의 품질, 즉 파워 및/또는 신호 대 잡음비가 낮아지면 사용자가 이동한 것으로 감지하여, 사용자 이동 방향에 있는 다른 오디오 신호 수집 장치(10-2)를 활성화시켜, 즉 오디오 신호를 전송하도록 요청하여, 그 오디오 신호 수집 장치(10-2)로부터 오디오 신호를 수신할 수 있다. 한편, 엣지 컴퓨팅 장치는 인공지능 모델을 이용하여 사용자의 이동 방향에 어떤 오디오 신호 수집 장치가 있는지 판단할 수 있다. 그리고, 본 개시에 따른 인공지능 모델은 적어도 하나의 인공 신경망을 포함하고, 딥러 닝(deep learning)에 의해 학습될 수 있다. 구체적으로, 인공지능 모델은 심층 신경망(Deep Neural Network, DNN), 합성곱 신경망(Convolution Neural Network, CNN), 순환 신경망(Recurrent Neural Network, RNN) 및 생 성적 적대 신경망(Generative Adversarial Networks, GAN) 중 적어도 하나의 인공 신경망을 포함할 수 있다. 다만, 본 개시에 따른 인공지능 모델에 포함되는 구체적인 인공 신경망 모델이 상술한 예에 국한되는 것은 아니 다. 예컨대, 최초에는 엣지 컴퓨팅 장치 입장에서, 제1 오디오 신호 수집 장치(10-1)로부터 수신되는 오디오 신 호의 품질이 낮아질 때, 어떤 다른 오디오 신호 수집 장치를 활성화시켜야 하는지를 판단할 수 있게 하는 정보 가 없으므로, 모든 오디오 신호 수집 장치를 활성화시킬 수 있다. 그리고 나서 모든 오디오 신호 수집 장치들로 부터 수신되는 오디오 신호들 중에서 제2 오디오 신호 수집 장치(10-2)로부터 수신된 오디오 신호에 동일 사용 자의 음성이 있고, 신호의 품질이 더 좋은 경우에, 사용자가 제1 오디오 신호 수집 장치(10-1)에서 제2 오디오 신호 수집 장치(10-2)로 이동하였음을 알 수 있다. 이러한 상황들을 여러 번 학습할 수 있다. 엣지 컴퓨팅 장치 는 이와 같은 학습 이후에는, 동일한 상황이 발생하였을 때, 모든 오디오 신호 수집 장치를 활성화시키는대신 특정 오디오 신호 수집 장치만을 활성화시킬 수 있는 것이다. 사용자의 이동을 감지하는 방법으로는, 앞서 설명한 것과 같이 오디오 신호의 품질, 복수 개의 마이크를 통해 획득된 오디오 신호에서의 크기 차이, 카메라를 통해 획득한 영상을 이용하는 방법 등이 있을 수 있다. 엣지 컴퓨팅 장치는 각기 다른 오디오 신호 수집 장치들로부터 수신한 오디오 신호에 대해 동일 사용자 음성을 시간 순서대로 정합 처리를 할 수 있다. 구체적으로, 엣지 컴퓨팅 장치는 각기 다른 오디오 신호 수 집 장치들로부터 수신한 오디오 신호들에서 음성 인식 정확도가 높을 것으로 예상되는 것들을 취합해서 이 어 붙일 수 있다. 이 때, 프레임 단위(혹은 일정 시간 단위)로 끊어서 이어 붙이는 것도 가능하고, 프레임 혹은 특정 시간 단위에 구애되지 않고, 음향 모델 또는 언어 모델을 통과한 결과(발음열, 문자열, 단어열)의 스코어 기반으로, 일정 수준 이상의 스코어를 가진 것들을 이어 붙이는 것도 가능하다. 여기서 스코어란 음향 모델 또 는 언어 모델을 통과한 결과가 어느 정도의 확률로 정확한지를 나타내는 확률 값이다. 예컨대 스코어가 0.8이면 80 %확률로 정확하다는 것을 의미한다. 엣지 컴퓨팅 장치에서 수행되는 구체적인 정합 방법은 도 34 내지 도 37을 참조하여 설명한다. 도 34 내지 도 37은 본 개시의 다양한 실시 예에 따른 엣지 컴퓨팅 장치에서의 정합 방법을 설명하기 위한 도면이다. 이하에서는 엣지 컴퓨팅 장치가 제1 오디오 신호 수집 장치(10-1) 및 제2 오디오 신호 수집 장치(10-2) 으 로부터 오디오 신호를 각각 수신하고, 수신된 오디오 신호를 정렬하며, 정렬된 오디오 신호를 비교하는 것을 전 제로 본 개시에 따른 정합 방법을 설명한다. 여기서, 수신된 오디오 신호를 정렬하기 위한 기준은 오디오 신호 가 수신된 시간일 수 있으며, 또한 발음열 또는 문자열의 유사성 등이 기준이 될 수도 있다. 다만, 이하 도 34 내지 도 36을 설명함에 있어서는 먼저 오디오 신호가 수신된 시간을 기준으로 수신된 오디오 신호를 정렬하는 것을 전제로 상술하고, 발음열 또는 문자열의 유사성 등을 기준으로 수신된 오디오 신호를 정렬하는 실시 예에 대해서는 도 37을 참조하여 후술한다. 한편, 이하에서는 엣지 컴퓨팅 장치가 제1 오디오 신호 수집 장치(10-1)로부터 오디오 신호를 수신하다가, 사용자의 이동이 감지된 이후부터 제1 오디오 신호 수집 장치(10-1)뿐만 아니라 제2 오디오 신호 수집 장치(10- 2)로부터도 오디오 신호를 수신하는 경우를 전제로 본 개시에 따른 정합 방법을 설명한다. 따라서, 이하에서 설 명하는 정합 방법은 감지된 사용자의 이동을 바탕으로 엣지 컴퓨팅 장치가 제2 오디오 신호 수집 장치(10- 2)가 활성화시킨 시점 이후부터 문제되는 것이라는 점은 자명하다. 도 34는 오디오 신호 획득 시점을 기준으로 복수의 오디오 신호를 정합하는 실시 예를 설명하기 위한 도면이다. 도 34를 참고하면, 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치(10-1)로부터 사용자 음성에 따른 오디오 신호를 수신하다가, 사용자의 이동이 감지되면 제2 오디오 신호 수집 장치(10-2)를 활성화시킬 수 있다. 그리고, 엣지 컴퓨팅 장치는 제2 오디오 신호 수집 장치(10-2)가 활성화된 시점 이전에 제1 오디오 신호 수집 장치(10-1)로부터 수신된 오디오 신호와, 제2 오디오 신호 수집 장치(10-2)가 활성화된 시점 이후부터 제2 오디오 신호 수집 장치(10-2)로부터 수신된 오디오 신호를 정합할 수 있다. 이 때, 프레임 단위 (혹은 일정 시간 단위)로 정합할 수 있다. 구체적으로, 도 34에 도시된 바와 같이, 사용자의 발화에 따른 음성이 \"너무 더워, 에어컨 온도 낮춰줘\" 인 경우, 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치(10-1)로부터 \"너무 더\"라는 사용자 음성에 대응되는 오디오 신호를 수신하고, 제2 오디오 신호 수집 장치(10-2)가 활성화된 시점 이후 제2 오디오 신호 수집 장치(10-2)로부터 \"워, 에어컨 온도 낮춰줘\"라는 사용자 음성에 대응되는 오디오 신호를 수신 할 수 있다. 그리고, 엣지 컴퓨팅 장치는 제2 오디오 신호 수집 장치(10-2)가 활성화된 시점 이전에 제1 오디오 신호 수집 장치(10-1)로부터 수신된 \"너무 더\"에 대응되는 오디오 신호와 제2 오디오 신호 수 집 장치(10-2)로부터 수신된 \"워, 에어컨 온도 낮춰줘\"에 대응되는 오디오 신호를 정합할 수 있다. 한편, 이상에서는 제2 오디오 수집 장치가 활성화된 시점을 기준으로 복수의 오디오 신호를 정합하는 실 시 예에 대해 설명하였으나, 보다 명확하게는 제2 오디오 수집 장치가 활성화된 이후 엣지 컴퓨팅 장치가 제2 오디오 수집 장치로부터 사용자 음성에 따른 오디오 신호를 수신한 시점이 복수의 오디오 신호 정합의 기준 이 될 수 있다. 다만, 본 개시를 설명함에 있어서는 편의 상, 제2 오디오 수집 장치가 활성화된 시점과 엣지 컴퓨팅 장치가 제2 오디오 수집 장치로부터 사용자 음성에 따른 오디오 신호를 수신한 시점을 모두 제 2 오디오 수집 장치가 활성화된 시점으로 지칭한다. 도 35는 수신된 오디오 신호의 품질을 기준으로 복수의 오디오 신호를 정합하는 실시 예를 설명하기 위한 도면 이다. 본 개시의 실시 예에 따르면, 단순히 제2 오디오 신호 수집 장치(10-2)의 활성화 시점 이전에 수신된 오 디오 신호와 활성화 시점 이후에 수신된 오디오 신호를 정합하는 것이 아니라, 수신된 복수의 오디오 신호의 품 질을 바탕으로 수신된 복수의 오디오 신호를 정합하는 것도 가능하다. 구체적으로, 엣지 컴퓨팅 장치는 수신된 복수의 오디오 신호의 파워 및/또는 신호 대 잡음비를 바탕으로 각 오디오 신호 수집 장치들(10-1, 10-2)로부터 수신된 오디오 신호를 구성하는 프레임들 중 각각의 신호의 품질이 일정 수준 이상인 프레임을 식별하고, 식별된 프레임을 정합할 수 있다. 예컨대, 도 35에 도시된 바와 같이, 사용자의 발화에 따른 음성이 \"너무 더워, 에어컨 온도 낮춰줘\"인 경 우, 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치(10-1)로부터 \"너무 더\"라는 사용자 음성에 대응 되는 오디오 신호를 수신하고, 제2 오디오 신호 수집 장치(10-2)가 활성화된 시점 이후 제2 오디오 신호 수집 장치(10-2)로부터 \"더워, 에어컨 온도 낮춰줘\"라는 사용자 음성에 대응되는 오디오 신호를 수신할 수 있다. 그리고, 엣지 컴퓨팅 장치는 수신된 복수의 오디오 신호의 파워 및/또는 신호 대 잡음비를 바탕으로, 제1 오디오 신호 수집 장치(10-1)와 제2 오디오 신호 수집 장치(10-2) 모두로부터 수신된 \"더\"라는 사용자 음성에 대응되는 각 오디오 신호 중 상대적으로 오디오 신호의 품질이 좋은 오디오 신호를 식별할 수 있다. 예를 들어, \"더\"라는 사용자 음성에 대응되는 각 오디오 신호 중 제2 오디오 신호 수집 장치(10-2)로부터 수신된 오디오 신 호가 제1 오디오 신호 수집 장치(10-1)로부터 수신된 오디오 신호에 비해 상대적으로 신호 품질이 좋은 경우, 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치(10-1)로부터 수신된 \"너무\"에 대응되는 오디오 신호와 제2 오디오 제2 오디오 신호 수집 장치(10-2)로부터 수신된 \"더워, 에어컨 온도 낮춰줘\"에 대응되는 오디오 신호를 정합할 수 있다. 도 36은 언어 모델 또는 음향 모델을 통해 획득된 스코어를 바탕으로 복수의 오디오 신호를 정합하는 실시 예를 설명하기 위한 도면이다. 엣지 컴퓨팅 장치는 수신된 복수의 오디오 신호를 음향 모델 또는 언어 모델에 입력하고, 음향 모델 또는 언어 모델을 통해 발음열, 문자열 또는 단어열에 대한 스코어를 획득하여, 획득된 스코어를 바탕으로 복수의 오 디오 신호를 정합할 수 있다. 여기서 스코어란 오디오 신호의 음성 인식 결과에 대한 확률 정보로서, 구체적으 로는 음향 모델 또는 언어 모델을 통해 획득된 결과가 어느 정도 정확한지를 나타내는 확률 값을 말한다. 도 36에 도시된 바와 같이, 사용자의 발화에 따른 음성이 \"너무 더워, 에어컨 온도 낮춰줘\"인 경우, 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치(10-1)로부터 \"너무 더\"라는 사용자 음성에 대응되는 오디 오 신호를 수신하고, 제2 오디오 신호 수집 장치(10-2)가 활성화된 시점 이후 제2 오디오 신호 수집 장치 (10-2)로부터 \"더워, 에어컨 온도 낮춰줘\"라는 사용자 음성에 대응되는 오디오 신호를 수신할 수 있다. 한편, 도 36에는 도 34 및 도 35와 달리, 음향 모델 또는 언어 모델을 통해 획득된 스코어(3650, 3660, 3670)가 추가로 도시되어 있다. 구체적으로, 엣지 컴퓨팅 장치는 제1 오디오 수집 장치(10-1)로부터 수신된 오디오 신호에 대한 스코어 및 제2 오디오 수집 장치(10-2)로부터 수신된 오디오 신호에 대한 스코어를 획 득할 수 있다. 그리고, 만약 0.6 이상인 스코어에 대응되는 프레임만을 정합에 사용하는 것으로 기 설정된 경우 라면, 정합에 사용되는 오디오 신호에 대응되는 스코어는 도 36에 도시된 바와 같이 결정될 수 있다. 그리고, 이 경우, 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치로부터 수신된 오디오 신호 중 \"너무 더워, 에어\"라는 사용자 음성에 대응되는 프레임과 제2 오디오 신호 수집 장치로부터 수신된 오디오 신호 중 \" 컨 온도 낮춰줘\"라는 사용자 음성에 대응되는 프레임을 사용하여 오디오 신호를 정합할 수 있다. 한편, 복수의 오디오 신호 수집 장치들로부터의 수신된 오디오 신호에 대해 빠른 음성 인식을 수행하기 위 해 엣지 컴퓨팅 장치는 복수의 음성 인식 모듈을 포함할 수 있다. 그리고, 이 경우 복수의 음성 인식 모듈 에서 병렬적으로 음성 인식이 수행되는 것이 가능하다. 구체적으로, 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치(10-1)로부터 수신된 오디오 신호와 제2 오디오 신호 수집 장치(10-2)로부터 수신된 오디오 신호를 엣지 컴퓨팅 장치에 포함된 복수의 음성 인식 모듈 각각 에 입력하고, 복수의 음성 인식 모듈에서 병렬적으로 획득된 스코어를 실시간으로 비교하여, 복수의 오디오 신 호 수집 장치로부터 수신된 오디오 신호 중 스코어가 높은 음성 인식 결과를 정합할 수 있다. 한편, 이상에서는 하나의 엣지 컴퓨팅 장치에서 음성 인식을 수행하는 경우에 대해 상술하였으나, 본 개시 의 일 실시 예에 따르면, 음성 인식의 속도와 효율성을 위해 복수의 엣지 컴퓨팅 장치의 음성 인식 모듈을 종합적으로 활용하여 음성 인식을 수행할 수 있다. 구체적으로, 복수의 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치(10-1) 및 제2 오디오 신호 수집 장치 (10-2)로부터 각각 오디오 신호를 수신할 수 있다. 그리고, 복수의 엣지 컴퓨팅 장치는 수신된 오디오 신호 를 각각의 엣지 컴퓨팅 장치에 포함된 음성 인식 모듈에 입력하고, 그에 따라 획득된 스코어를 복수의 엣지 컴퓨팅 장치 중 하나의 엣지 컴퓨팅 장치로 전송할 수 있다. 복수의 엣지 컴퓨팅 장치로부터 스코어를 수신 한 하나의 엣지 컴퓨팅 장치는 복수의 엣지 컴퓨팅 장치에서 병렬적으로 획득된 스코어를 실시간으로 비교하고, 복수의 오디오 신호 수집 장치로부터 수신된 오디오 신호 중 스코어가 높은 음성 인식 결과를 정합하 여 음성 인식 결과를 획득할 수 있다. 또 한편, 엣지 컴퓨팅 장치에서 음성 인식 처리한 것의 결과가 정확하지 않을 경우, 컴퓨팅 능력이 더 뛰어 난 외부 서버, 예컨대 앞서 설명한 실시 예들의 서버에서 한번 더 처리하는 것도 가능하다. 구체적으로, 엣지 컴퓨팅 장치에서 수신한 음성신호의 품질이 낮은 경우, 혹은 음성 모델 또는 언어 모델을 통하여 오디 오신호에 대해 처리한 음성인식 스코어가 낮은 경우에는 음성 인식 처리 결과가 정확하지 않다고 판단하여, 컴 퓨팅 능력이 더 뛰어난 외부 서버에서 한번 더 처리하는 것도 가능하다. 이 경우, 오디오 신호 수집 장치가 엣지 컴퓨팅 장치를 거치지 않고 서버에 바로 오디오 신호를 전송하는 것도 가능하다. 또 다른 실시 예에 따르면, 엣지 컴퓨팅 장치 자체도 마이크를 구비할 수 있고, 자체적으로 획득한 오디오 신호에 대한 음성 인식을 수행하는 것이 가능하고, 더 나은 음성 인식 능력을 가진 다른 엣지 컴퓨팅 장치에 오 디오 신호를 전송하는 것도 가능하다. 정합 이후 음성 인식이 완료되면, 엣지 컴퓨팅 장치는 음성 인식 결과에 대한 특정 태스크를 수행할 수 있 다. 예컨대, \"너무 더워 에어컨 온도 낮춰줘＂라는 음성이 인식되었을 때, 엣지 컴퓨팅 장치는 에어컨에 온 도 제어 명령을 전송할 수 있고, \"에어컨 온도가 xx도로 낮추었습니다\"라는 음성 응답을 제공할 수 있다. 한편, 이러한 음성 응답은 제어 명령을 받은 장치, 예컨대 에어컨에서 제공될 수 있다. 또는, 현재 사용자와 가 장 가까운 장치에서 제공될 수 있다. 또는 마지막으로 음성이 입력된 오디오 신호 수집 장치에서 제공될 수 있 다. 이 경우, 음성의 SNR, 음성의 크기(Sound Pressure Level), 음성을 발화한 사용자와의 거리 등 다양한 음성 신호의 질에 대한 파라메터 기반으로 음성을 제공할 장치를 결정할 수 있다. 도 37은 발음열 또는 문자열의 유사성 등을 기준으로 복수의 오디오 신호를 정렬하고, 정렬된 복수의 오디오 신 호를 정합하는 실시 예에 대해 설명하기 위한 도면이다. 즉, 이상에서는 복수의 오디오 신호가 수신된 시간을 기준으로 복수의 오디오 신호를 정렬하고, 정렬된 복수의 오디오 신호를 비교하여 정합하는 실시 예에 대해 상술하였으나, 본 개시의 또 다른 실시 예에 따르면, 복수의 오디오 신호 상호 간 발음열 또는 문자열의 유사성 등을 기준으로 복수의 오디오 신호를 정렬하고, 정렬된 복수 의 오디오 신호를 비교하여 정합할 수도 있다. 한편, 이상에서는 복수의 오디오 신호를 정합함에 있어서, 복수의 오디오 신호에 포함된 프레임 중 정합의 대상 이 되는 프레임을 식별하고 식별된 프레임을 정합하는 것을 전제로 설명하였으나, 본 개시가 이에 국한되는 것 은 아니다. 즉, 도 37을 참조하여 설명하는 바와 같은 본 개시의 일 실시 예와 같이 복수의 오디오 신호 상호 간 발음열 또는 문자열의 유사성 등을 기준으로 복수의 오디오 신호를 정렬하는 경우에는 발음열 또는 문자열 단위로 정합의 대상이 되는 오디오 신호의 단위를 식별할 수도 있다. 그리고, 여기서 발음열 또는 문자열의 단 위가 일정한 길이에 국한되는 것은 아니다. 도 37에 도시된 바와 같이, 사용자의 발화에 따른 음성이 \"너무 더워, 에어컨 온도 낮춰줘\"인 경우, 엣지 컴퓨 팅 장치는 제1 오디오 신호 수집 장치(10-1)로부터 \"너무 더\"라는 사용자 음성에 대응되는 오디오 신호를 수신하고, 제2 오디오 신호 수집 장치(10-2)가 활성화된 시점 이후 제2 오디오 신호 수집 장치(10-2)로부 터 \"더워, 에어컨 온도 낮춰줘\"라는 사용자 음성에 대응되는 오디오 신호를 수신할 수 있다. 한편, 도 37에는 도 34 및 도 35와 달리, 수신된 복수의 오디오 신호 상호간 발음열(3750, 3760)이 유사한 부분 을 기준으로 식별된 정렬 축들(3710, 3720, 3730, 3740)이 도시되어 있다. 구체적으로, 엣지 컴퓨팅 장치는 오디오 신호 수집 장치(10-1, 10-2)들로부터 각각 오디오 신호가 수신되면, 도 37에 도시된 바와 같이, 수신된 복수의 오디오 신호 상호간 발음열(3750, 3760)이 유사한 부분을 기준으로 적어도 하나의 정렬 축들(3710, 3720, 3730, 3740)을 식별할 수 있다. 그리고, 엣지 컴퓨팅 장치는 수신된 복수의 오디오 신호를 식별된 정 렬 축들(3710, 3720, 3730, 3740)을 기준으로 정렬하고, 정렬된 복수의 오디오 신호를 비교하여 정합할 수있다. 여기서, 복수의 오디오 신호를 비교하여 정합함에 있어서는 전술한 바와 같은 본 개시에 따른 다양한 정합 방법 이 적용될 수 있으며, 다만, 도 37에서는 도 36을 참조하여 설명한 것과 마찬가지로, 음향 모델 또는 언어 모델 을 통해 획득된 스코어를 바탕으로 복수의 오디오 신호를 정합하는 경우의 예를 도시하였다. 즉, 도 37에는 음 향 모델 또는 언어 모델을 통해 획득된 스코어(3770, 3780, 3790)가 도시되어 있다. 구체적으로, 엣지 컴퓨팅 장치는 제1 오디오 수집 장치(10-1)로부터 수신된 오디오 신호에 대한 스코어 및 제2 오디오 수집 장 치(10-2)로부터 수신된 오디오 신호에 대한 스코어를 획득할 수 있다. 한편, 도 36에 대한 설명에서는 0.6 이상인 스코어에 대응되는 프레임만을 사용하는 것으로 기 설정된 경우에 대해 설명하였으나, 본 개시가 이에 국한되는 것은 아니다. 즉, 본 개시의 또 다른 실시 예에 따르면, 엣지 컴 퓨팅 장치는 제1 오디오 신호 수집 장치(10-1)로부터 수신된 오디오 신호를 음향 모델 또는 언어 모델에 입 력하여 획득한 스코어와 제2 오디오 신호 수집 장치(10-2)로부터 수신된 오디오 신호를 음향 모델 또는 언어 모 델에 입력하여 획득한 스코어를 비교하여, 더 높은 스코어에 대응되는 프레임들을 식별할 수 있다. 구체적으로, 도 37에 도시된 바와 같이, 제1 오디오 신호 수집 장치(10-1)와 제2 오디오 신호 수집 장치(10-2) 모두로부터 오디오 신호가 수신된 구간은 사용자가 발화한 음성 중 \"워, 에어\"에 대응되는 구간일 수 있다. 그 리고, 이 경우 \"워\"에 대응되는 구간과 \"에\"에 대응되는 구간은 제1 오디오 신호 수집 장치(10-1)로부터 수신된 오디오 신호에 대한 스코어가 더 높으며(스코어: 0.7 및 0.6), \"어\"에 대응되는 구간은 제2 오디오 신호 수집 장치(10-2)로부터 수신된 오디오 신호에 대한 스코어가 더 높을 수 있다(스코어: 0.9). 따라서, 이 경우, 엣지 컴퓨팅 장치는 제1 오디오 신호 수집 장치로부터 수신된 오디오 신호 중 \"너무 더워, 에\"라는 사용자 음성에 대응되는 프레임과 제2 오디오 신호 수집 장치로부터 수신된 오디오 신호 중 \"어 컨 온도 낮춰줘\"라는 사용자 음성에 대응되는 프레임을 사용하여 오디오 신호를 정합할 수 있다. 이상에서 설명된 다양한 실시 예들은 소프트웨어(software), 하드웨어(hardware) 또는 이들의 조합으로 구현될 수 있다. 하드웨어적인 구현에 의하면, 본 개시에서 설명되는 실시 예들은 ASICs(Application Specific Integrated Circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 프로세서(processors), 제어기 (controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서(microprocessors), 기타 기능 수행 을 위한 전기적인 유닛(unit) 중 적어도 하나를 이용하여 구현될 수 있다. 특히, 이상에서 설명된 다양한 실시 예들은 전자 장치의 프로세서, 허브 장치의 프로세서 또는 서버의 프로세서에 의해 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들 은 별도의 소프트웨어 모듈들로 구현될 수 있다. 상기 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 작동을 수행할 수 있다. 본 개시의 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media)에 저장될 수 있는 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기(machine)는, 저장 매 체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실시 예들의 서버 를 포함할 수 있다. 이러한 명령어가 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 상기 프로세서의 제어 하에 다른 구성요 소들을 이용하여 명령어에 해당하는 기능을 수행할 수 있다. 명령어는 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 예컨대, 저장매체에 저장된 명령어가 프로세서에 의해 실행됨으로써, 상 술한 전자 장치, 허브 장치 또는 서버를 제어하는 방법 이 실행될 수 있다. 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일 시적'은 저장매체가 신호(signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체 에 반영구적 또는 임시적으로 저장됨을 구분하지 않는다. 일 실시 예에 따르면, 본 문서에 개시된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래 될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD- ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어쪠, 앱스토어쪠)를 통해 온라인으로 배포될 수 있 다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다.다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구성될 수 있으 며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로그램)은 하나 의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행 할 수 있다. 다양한 실시 예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2019-0030660", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2019-0030660", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른, 여러 대의 전자 장치를 통해 음성 인식을 수행하는 본 개시의 일 실시예를 설명하기 위한 도면, 도 2 내지 도 5b는 본 개시의 다양한 실시 예들에 따른 음성 인식 시스템을 설명하기 위한 도면, 도 6 내지 도 9는 음성 인식 시스템에서 음성 인식을 수행하는 본 개시의 다양한 실시 예들을 설명하기 위한 시 퀀스도, 도 10은 본 개시의 일 실시예에 따른 전자 장치의 구성을 설명하기 위한 블럭도, 도 11은 본 개시의 일 실시예에 따른 허브 장치의 구성을 설명하기 위한 블럭도, 도 12는 본 개시의 일 실시예에 따른 서버의 구성을 설명하기 위한 블럭도, 도 13은 본 개시의 일 실시 예에 따른 음성 인식 모듈을 설명하기 위한 블럭도, 도 14는 본 개시의 일 실시 예에 따른 인공지능 에이전트 시스템의 대화 시스템을 도시한 블록도, 도 15 내지 도 17은 음성 인식 시스템에서 음향 모델 및 언어 모델의 사용과 관련한 다양한 실시 예를 설명하기 위한 도면, 도 18은 전자 장치들에 대한 위치 정보 생성 방법의 일 예를 설명하기 위한 도면, 도 19는 복수의 마이크를 구비한 본 개시의 일 실시 예에 따른 전자 장치를 설명하기 위한 도면, 도 20은 본 개시의 또 다른 실시 예에 따른 전자 장치가 카메라를 이용해서 사용자의 이동 방향을 감지하는 방 법을 설명하기 위한 도면, 도 21 내지 도 25는 전자 장치가 음성 인식 정보들을 정합하는 다양한 실시 예들을 설명하기 위한 도면, 도 26은 복수의 사용자가 음성을 발화하는 상황에서 복수의 전자 장치들을 통해 음성 인식을 수행하는 본 개시 의 일 실시 예를 설명하기 위한 도면, 도 27은 멀티 턴 방식의 응답 제공과 관련한 본 개시의 일 실시 예를 설명하기 위한 도면, 도 28 내지 도 29는 본 개시의 다양한 실시 예들에 따른, 음성 인식의 핸드오버가 일어나는 상황을 설명하기 위 한 도면, 도 30은 본 개시의 일 실시 예에 따른 전자 장치의 제어방법을 설명하기 위한 흐름도, 도 31은 사용자의 이동에 따라 복수의 오디오 신호 수집 장치들로부터 수신된 오디오 신호를 정합하는 과정에 대해 설명하기 위한 도면, 도 32는 본 개시의 일 실시 예에 따른 엣지 컴퓨팅 장치가 여러 대의 오디오 신호 수집 장치로부터 수신된 오디오 신호로부터 트리거 워드를 인식하는 방식을 설명하기 위한 도면, 도 33은 음성 인식 모듈에서 오디오 신호를 단어 열로 변환하는 과정의 일 실시 예를 설명하기 위한 도면, 그리고 도 34 내지 도 37은 본 개시의 다양한 실시 예에 따른 엣지 컴퓨팅 장치에서의 정합 방법을 설명하기 위한 도면이다."}
