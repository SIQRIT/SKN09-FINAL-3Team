{"patent_id": "10-2021-0155706", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0069569", "출원번호": "10-2021-0155706", "발명의 명칭": "과채류 세그먼테이션 및 자세추정 방법", "출원인": "충북대학교 산학협력단", "발명자": "황영배"}}
{"patent_id": "10-2021-0155706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "에 있어서,자세 추정 단계는,과채류의 3D 데이터를 이용하여 과채류에 매칭되는 가상 실린더를 생성한 후, 가상 실린더의 중심축 방향으로부터 과채류의 자세를 계산하는 것을 특징으로 하는 과채류 세그먼테이션 및 자세추정 방법."}
{"patent_id": "10-2021-0155706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,과채류 영상 생성 단계는,입력되는 컬러 영상을 분석하여 과채류를 세그먼테이션 하도록 학습된 딥러닝 모델인 세그먼테이션 모델을 통해수행되는 것을 특징으로 하는 과채류 세그먼테이션 및 자세추정 방법."}
{"patent_id": "10-2021-0155706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서,세그먼테이션 모델은,과채류만 나타난 전경 영상들과 과채류 식물의 줄기와 잎들이 나타낸 배경 영상들을 각기 다른 방식으로 합성하여 생성한 합성 영상들로 학습되는 것을 특징으로 하는 과채류 세그먼테이션 및 자세추정 방법."}
{"patent_id": "10-2021-0155706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 3에 있어서,합성 영상들에서,하나의 배경 영상에 합성되는 전경 영상의 개수는 설정된 최대 개수 이내에서 랜덤하게 결정되는 것을 특징으로하는 과채류 세그먼테이션 및 자세추정 방법."}
{"patent_id": "10-2021-0155706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서,배경 영상에 합성될 전경 영상의 크기, 각도 및 밝기는 정해진 범위 내에서 랜덤하게 결정되는 것을 특징으로하는 과채류 세그먼테이션 및 자세추정 방법.공개특허 10-2023-0069569-3-청구항 6"}
{"patent_id": "10-2021-0155706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서,과채류에 매칭되는 가상 실린더는,내부에 과채류가 들어갈 수 있는 실린더들 중 반지름이 최소인 실린더인 것을 특징으로 하는 과채류 세그먼테이션 및 자세추정 방법."}
{"patent_id": "10-2021-0155706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 7에 있어서,실린더의 반지름은,반지름이 과채류의 단축 반지름과 동일한 것을 특징으로 하는 과채류 세그먼테이션 및 자세추정 방법."}
{"patent_id": "10-2021-0155706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 9에 있어서,과채류에 매칭되는 가상 실린더는,최소 자승법을 통해 계산하는 것을 특징으로 하는 과채류 세그먼테이션 및 자세추정 방법."}
{"patent_id": "10-2021-0155706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 1에 있어서,과채류의 위치와 자세에 대한 정보를 과채류 수확 로봇으로 전달하는 단계;를 더 포함하는 것을 특징으로 하는과채류 세그먼테이션 및 자세추정 방법."}
{"patent_id": "10-2021-0155706", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "과채류를 촬영하여 컬러 영상과 뎁스 영상을 획득하는 카메라;획득한 컬러 영상에서 과채류를 세그먼테이션 하여 과채류 컬러 영상을 생성하는 세그먼테이션 모델;과채류 컬러 영상을 뎁스 영상에 마스킹 하여, 과채류 뎁스 영상을 생성하는 마스킹 부;과채류 컬러 영상과 과채류 뎁스 영상을 이용하여, 과채류의 3D 데이터를 생성하는 복원부;과채류의 3D 데이터를 이용하여, 과채류의 위치를 계산하는 계산부; 및과채류의 3D 데이터를 이용하여, 과채류의 자세를 추정하는 추정부;를 포함하는 것을 특징으로 하는 과채류 세그먼테이션 및 자세추정 시스템.공개특허 10-2023-0069569-4-"}
{"patent_id": "10-2021-0155706", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "과채류 세그먼테이션 및 자세추정 방법이 제공된다. 본 발명의 실시예에 따른 과채류 세그먼테이션 및 자세추정 방법은, 과채류를 촬영한 컬러 영상과 뎁스 영상을 획득하고, 획득한 컬러 영상에서 과채류를 세그먼테이션 하여 과채류 컬러 영상을 생성하며, 과채류 컬러 영상을 뎁스 영상에 마스킹 하여 과채류 뎁스 영상을 생성하고, 과채 류 컬러 영상과 과채류 뎁스 영상을 이용하여 과채류의 3D 데이터를 생성하며, 과채류의 3D 데이터를 이용하여 과채류의 위치를 계산하고, 과채류의 3D 데이터를 이용하여 과채류의 자세를 추정한다. 본 발명의 실시예들에서 는 과채류 세그먼테이션을 수행하는 딥러닝 모델의 학습 데이터를 합성 영상 생성을 통해 충분하게 그리고 라벨 링 작업 없이 간편하게 확보하고, 자세추정의 정확도를 향상시킬 수 있게 된다."}
{"patent_id": "10-2021-0155706", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 스마트 농장 관련 기술에 관한 것으로, 더욱 상세하게는 인공지능 모델을 이용하여 영상에서 과채류 를 세그먼테이션 하고, 세그먼테이션 된 과채류의 자세를 추정하는 방법에 관한 것이다."}
{"patent_id": "10-2021-0155706", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "딥러닝 기반 디지털 영상 처리 기술의 빠른 발전으로 인해, 최근 농업 분야에까지 심층 컨볼루션 신경망을 이용 하여 과채류의 익은 정도 판별, 질병 인식 등의 작업이 수행되고 있다. 딥러닝 모델을 이용한 객체 인식을 위한 학습에는 많은 양의 영상 데이터가 필요하다. 하지만 농업 분야의 경우, 과거 딥러닝 기술 활용이 많지 않았기 때문에 데이터가 많이 축적되어 있지 않았다는 문제가 있다. 또한, 데이터가 많이 있다 하더라도, 딥러닝 모델 학습을 위해서는 라벨링 작업이 필요한데, 아주 많은 노력과 시간이 소요되는 고된 작업이다. 딥러닝 기술과 더불어 농업 분야에서 연구가 적극적으로 이루어지고 있는 것 중 하나가 많은 노동력을 요구하는 수확 작업의 자동화를 위한 과채류 수확 로봇 기술이다. 과채류 수확 로봇을 이용하기 위해서는 과채류에 대한 정확한 객체 인식 외에도 과채류의 자세를 정확하게 추정 하는 것이 필요하다. 그렇지 않으면 로봇팔이 과채류를 수확하는 과정에서 잦은 충돌로 인해 과채류의 손상이 많이 발생하여 상품 가치가 떨어지는 문제가 있다."}
{"patent_id": "10-2021-0155706", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기와 같은 문제점을 해결하기 위하여 안출된 것으로서, 본 발명의 목적은, 데이터 증대를 통해 확 보한 영상들로 학습시킨 딥러닝 모델을 이용하여 과채류를 세그먼테이션하고, 세그먼테이션 된 과채류의 자세를 정확하게 추정하는 방법을 제공함에 있다."}
{"patent_id": "10-2021-0155706", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른, 과채류 세그먼테이션 및 자세추정 방법은, 과채류를 촬영한 컬러 영상과 뎁스 영상을 획득하는 단계; 획득한 컬러 영상에서 과채류를 세그먼테이션 하여 과채류 컬 러 영상을 생성하는 단계; 과채류 컬러 영상을 뎁스 영상에 마스킹 하여, 과채류 뎁스 영상을 생성하는 단계; 과채류 컬러 영상과 과채류 뎁스 영상을 이용하여, 과채류의 3D 데이터를 생성하는 단계; 과채류의 3D 데이터를 이용하여, 과채류의 위치를 계산하는 단계; 및 과채류의 3D 데이터를 이용하여, 과채류의 자세를 추정하는 단계;를 포함한다. 과채류 영상 생성 단계는, 입력되는 컬러 영상을 분석하여 과채류를 세그먼테이션 하도록 학습된 딥러닝 모델인 세그먼테이션 모델을 통해 수행될 수 있다. 세그먼테이션 모델은, 과채류만 나타난 전경 영상들과 과채류 식물의 줄기와 잎들이 나타낸 배경 영상들을 각기 다른 방식으로 합성하여 생성한 합성 영상들로 학습될 수 있다. 합성 영상들에서, 하나의 배경 영상에 합성되는 전경 영상의 개수는 설정된 최대 개수 이내에서 랜덤하게 결정 될 수 있다. 배경 영상에 합성될 전경 영상의 크기, 각도 및 밝기는 정해진 범위 내에서 랜덤하게 결정될 수 있다. 자세 추정 단계는, 과채류의 3D 데이터를 이용하여 과채류에 매칭되는 가상 실린더를 생성한 후, 가상 실린더의 중심축 방향으로부터 과채류의 자세를 계산할 수 있다.과채류에 매칭되는 가상 실린더는, 내부에 과채류가 들어갈 수 있는 실린더들 중 반지름이 최소인 실린더일 수 있다. 실린더의 반지름은, 반지름이 과채류의 단축 반지름과 동일할 수 있다. 과채류에 매칭되는 가상 실린더는, 최소 자승법을 통해 계산할 수 있다. 본 발명의 실시예에 따른 과채류 세그먼테이션 및 자세추정 방법은, 과채류의 위치와 자세에 대한 정보를 과채 류 수확 로봇으로 전달하는 단계;를 더 포함할 수 있다. 한편, 본 발명의 다른 실시예에 따른, 과채류 세그먼테이션 및 자세추정 시스템은, 과채류를 촬영하여 컬러 영 상과 뎁스 영상을 획득하는 카메라; 획득한 컬러 영상에서 과채류를 세그먼테이션 하여 과채류 컬러 영상을 생 성하는 세그먼테이션 모델; 과채류 컬러 영상을 뎁스 영상에 마스킹 하여, 과채류 뎁스 영상을 생성하는 마스킹 부; 과채류 컬러 영상과 과채류 뎁스 영상을 이용하여, 과채류의 3D 데이터를 생성하는 복원부; 과채류의 3D 데 이터를 이용하여, 과채류의 위치를 계산하는 계산부; 및 과채류의 3D 데이터를 이용하여, 과채류의 자세를 추정 하는 추정부;를 포함한다."}
{"patent_id": "10-2021-0155706", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상 설명한 바와 같이, 본 발명의 실시예들에 따르면, 과채류 세그먼테이션을 수행하는 딥러닝 모델의 학습 데 이터를 합성 영상 생성을 통해 충분하게 그리고 라벨링 작업 없이 간편하게 확보할 수 있게 된다. 또한, 본 발명의 실시예들에 따르면, 세그먼테이션 된 과채류에 매칭되는 가상 실린더를 생성하여, 가상 실린더 의 중심축 방향으로부터 과채류의 자세를 추정함으로써, 자세추정의 정확도를 향상시킬 수 있게 된다."}
{"patent_id": "10-2021-0155706", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 도면을 참조하여 본 발명을 보다 상세하게 설명한다. 도 1은 본 발명의 일 실시예에 따른 과채류 세그먼테이션/자세추정 시스템의 블럭도이다. 본 발명의 실시예에 따른 과채류 세그먼테이션/자세추정 시스템은, 도시된 바와 같이, RGB-카메라, 뎁스-카메라, 세그먼 테이션 모델, 마스킹 부, 3D 데이터 복원부, 위치 계산부 및 자세 추정부를 포함하여 구성된다. RGB-카메라는 과채류를 촬영하여 컬러 영상을 획득하는 카메라이고, 뎁스-카메라는 과채류의 뎁스 영 상을 생성하는 카메라이다. 도 2의 좌측에는 RGB-카메라에서 획득되는 과채류의 일종인 토마토의 컬러 영 상을 예시하였고, 우측에는 뎁스-카메라에서 생성되는 토마토의 뎁스 영상을 예시하였다. 세그먼테이션 모델은 RGB-카메라에서 획득된 컬러 영상에서 과채류를 세그먼테이션 하여 과채류 컬러 영상을 생성한다. 과채류 컬러 영상은 컬러 영상에서 과채류만 세그먼테이션 된 영상이다. 세그먼테이션 모델은 입력되는 컬러 영상을 분석하여 과채류를 세그먼테이션 하도록 학습된 딥러닝 모델로 써, Mask R-CNN을 학습시켜 구현할 수 있다. 세그먼테이션 모델을 학습시키기 위해, 도 3에 도시된 바와 같이, '토마토만 나타난 전경 영상들'과 '토마 토를 제외한 토마토 식물의 줄기와 잎들이 나타낸 배경 영상들'을 각기 다른 다양한 방식으로 합성하여 합성 영 상들을 생성한다. 이와 같은 데이터 증대(data augmentation)로 생성된 합성 영상들은 세그먼테이션 모델 을 학습시키는데 이용된다. 합성 영상 생성시, 하나의 배경 영상에 합성되는 전경 영상의 개수는 설정된 최대 개수 이내에서 랜덤하게 결정 된다. 이를 테면, 하나의 배경 영상에 3개 이하의 전경 영상이 합성되도록 하여 합성 영상에 3개 이하의 토마토 가 나타나도록 제한하거나, 하나의 배경 영상에 5개 이하의 전경 영상이 합성되도록 하여 합성 영상에 5개 이하 의 토마토가 나타나도록 제한할 수 있다. 나아가, 배경 영상에 합성될 전경 영상의 크기, 각도 및 밝기도 정해진 범위 내에서 랜덤하게 결정되도록 할 수 있다. 도 4의 표에서 첫 번째와 두 번째 데이터는, 도 5에 제시된 바와 같은 실제 영상 63개를 이용하여 Mask-RCNN을 epoch 50, 100으로 학습시킨 결과이다. 이 경우 최종 train loss는 도 6에 도시된 바와 같이 0.048로 나타났다. 그리고, 도 4의 표에서 세 번째와 네 번째 데이터는 전경 영상 134개와 배경 영상 74개로 최대 전경 영상 개수 (max_foreground)를 3으로 전경 영상 각도 범위를 360°로 하여 랜덤으로 설정하여 만든 합성 영상 10,000장으 로 학습시킨 결과이며, 다섯 번째와 여섯 번째는 동일한 조건에서 최대 전경 영상 개수만 5로 변경하여 만든 합 성 영상 10,000장으로 학습시킨 결과이다. 가장 좋은 AP를 보인 최대 전경 영상 개수를 3으로 제한한 경우의 최종 train loss는 도 7에 도시된 바와 같이 0.148로 양호하게 나타났다. 합성 영상에 의한 학습의 경우가 실제 영상에 의한 최종 train loss 보다 높았지만, 이는 실제 영상의 개수가 적음(실제 영상 63장 vs 합성 영상 10,000장)으로 인해 비롯되 것이므로, 이를 감안한다면 합성 영상에 의한 학 습시 최종 train loss가 높다고 할 수 없을 것이다. 또한, 합성 영상에 의한 학습시의 AP가 실제 영상에 의한 학습시의 AP 보다 훨씬 더 높으며, 실제 영상으로 학 습된 Mask-RCNN에 대한 세그먼테이션 테스트 결과(도 8)와 합성 영상으로 학습된 Mask-RCNN에 대한 세그먼테이 션 테스트 결과(도 9)를 비교해 보면, 합성 영상으로 학습된 경우가 더 좋은 결과를 보이고 있음을 확인할 수 있다. 게다가, 실제 영상의 경우 라벨링 작업이 필요한 반면, 합성 영상의 경우 합성시에 라벨링을 자동 생성할 수 있 으므로, 합성 영상을 이용한 학습의 편리함은 실제 영상을 이용한 학습 보다 훨씬 우월하다고 할 수 있다. 다시, 도 1을 참조하여 설명한다. 마스킹 부는 뎁스-카메라에서 생성된 뎁스 영상에 세그먼테이션 모델에서 생성된 과채류 컬러 영상(과채류 세그먼테이션 결과)을 마스킹 하여, 과채류 뎁스 영상을 생성한다. 과채류 뎁스 영상은 뎁스 영상 에서 과채류 부분만 세그먼테이션 된 영상이다. 3D 데이터 복원부는 세그먼테이션 모델에서 생성된 과채류 컬러 영상과 마스킹 부에서 생성된 과채류 뎁스 영상을 이용하여, 과채류의 3D 데이터인 3D 공간 상의 포인트 클라우드 데이터를 생성한다. 도 2에 제시된 RGB 영상과 뎁스 영상으로부터 생성한 과채류 컬러 영상과 과채류 뎁스 영상을 align 해서 생성 한 포인트 클라우드 데이터를 도 10에 나타내었다. 위치 계산부는 3D 데이터 복원부에 의해 생성된 과채류의 포인트 클라우드 데이터로부터 과채류의 3D 공간 상 위치를 계산한다. 자세 추정부는 3D 데이터 복원부에 의해 생성된 과채류의 포인트 클라우드 데이터로부터 과채류에 매 칭되는 가상 실린더를 생성한다. 과채류에 매칭되는 가상 실린더는 내부에 과채류가 들어갈 수 있는 실린더들 중 반지름이 최소인 실린더이다. 이 조건을 만족하기 위해서는, 실린더의 반지름이 과채류의 단축 반지름(과채류를 타원형이라고 가정할 때 짧은 축의 반지름)과 동일하여야 한다. 이와 같은 과채류에 매칭되는 가상 실린더는 최소 자승법을 통해 계산할 수 있다. 도 11에는 과채류에 매칭되는 가상 실린더를 생성한 결과를 예시하였고, 도 12에는 과채류와 가상 실린더 를 보다 자세히 나타내었다. 자세 추정부는 생성된 가상 실린더의 중심축의 방향으로부터 과채류의 자세를 계산한다. 가상 실린더의 중 심축과 과채류의 중심축(과채류의 꼭지와 밑동을 지나는 축)이 일치하기 때문에, 가상 실린더의 중심축의 방향 으로로부터 과채류의 자세(방향)이 추정가능한 것이다. 위치 계산부에 의해 계산된 과채류의 위치 정보와 자세 추정부에 의해 추정된 과채류의 자세 정보는 과채류 수확 로봇(미도시)으로 전달되어, 과채류 수확 로봇의 제어에 이용된다. 정확한 위치와 자세 정보가 제 공되므로, 수확 로봇의 로봇팔에 의한 과채류 손상은 최소화 된다. 도 13은 본 발명의 다른 실시예에 따른 과채류 세그먼테이션/자세추정 방법의 설명에 제공되는 흐름도이다. 도시된 바와 같이, 먼저 RGB-카메라가 과채류의 컬러 영상을 획득하고(S210), 뎁스-카메라가 과채류 의 뎁스 영상을 생성한다(S220). 그러면 세그먼테이션 모델은 S210단계에서 생성된 컬러 영상에서 과채류를 세그먼테이션 하고(S230), 마스 킹 부는 S220단계에서 생성된 뎁스 영상에 S230단계에서의 과채류 세그먼테이션 결과를 마스킹 한다 (S240). S230단계에서 세그먼테이션을 수행하는 세그먼테이션 모델은 과채류의 전경 영상들과 배경 영상들을 각기 다른 다양한 방식으로 합성하여 생성한 합성 영상들로 학습된 딥러닝 모델이다. 다음 3D 데이터 복원부는 S230단계에서의 세그먼테이션 결과와 S240단계에서의 마스킹 결과를 이용하여, 과채류의 포인트 클라우드 데이터를 생성한다(S250). 그러면 위치 계산부는 S250단계에서 생성된 포인트 클라우드 데이터로부터 과채류의 3D 공간 상 위치를 계 산하고(S260), 자세 추정부는 S250단계에서 생성된 포인트 클라우드 데이터로부터 과채류의 자세를 추정한 다(S270). S270단계에서는 자세 추정을 위해 과채류에 매칭되는 가상 실린더를 생성하고, 생성된 가상 실린더의 중심축의 방향으로부터 과채류의 자세를 계산한다. 지금까지, 과채류 세그먼테이션 및 자세추정 방법 및 시스템에 대해 바람직한 실시예를 들어 상세히 설명하였다. 한편, 본 실시예에 따른 장치와 방법의 기능을 수행하게 하는 컴퓨터 프로그램을 수록한 컴퓨터로 읽을 수 있는 기록매체에도 본 발명의 기술적 사상이 적용될 수 있음은 물론이다. 또한, 본 발명의 다양한 실시예에 따른 기 술적 사상은 컴퓨터로 읽을 수 있는 기록매체에 기록된 컴퓨터로 읽을 수 있는 코드 형태로 구현될 수도 있다. 컴퓨터로 읽을 수 있는 기록매체는 컴퓨터에 의해 읽을 수 있고 데이터를 저장할 수 있는 어떤 데이터 저장 장 치이더라도 가능하다. 예를 들어, 컴퓨터로 읽을 수 있는 기록매체는 ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광디스크, 하드 디스크 드라이브, 등이 될 수 있음은 물론이다. 또한, 컴퓨터로 읽을 수 있는 기록매체 에 저장된 컴퓨터로 읽을 수 있는 코드 또는 프로그램은 컴퓨터간에 연결된 네트워크를 통해 전송될 수도 있다. 또한, 이상에서는 본 발명의 바람직한 실시예에 대하여 도시하고 설명하였지만, 본 발명은 상술한 특정의 실시"}
{"patent_id": "10-2021-0155706", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "예에 한정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 발명이 속하는 기술분야 에서 통상의 지식을 가진자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2021-0155706", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 과채류 세그먼테이션/자세추정 시스템의 블럭도, 도 2는 토마토의 컬러 영상과 뎁스 영상을 예시헌 도면, 도 3은 데이터 증대를 위한 합성 영상 생성 방법을 나타낸 도면, 도 4는 Mask-RCNN의 학습 결과를 나타낸 표, 도 5는 실제 영상을 예시한 도면, 도 6은 실제 영상으로 학습된 Mask-RCNN의 train loss를 나타낸 그래프, 도 7은 합성 영상으로 학습된 Mask-RCNN의 train loss를 나타낸 그래프, 도 8은 실제 영상으로 학습된 Mask-RCNN에 대한 세그먼테이션 테스트 결과, 도 9는 합성 영상으로 학습된 Mask-RCNN에 대한 세그먼테이션 테스트 결과, 도 10은 도 2에 제시된 RGB 영상과 뎁스 영상으로부터 생성한 포인트 클라우드 데이터, 도 11은 과채류에 매칭되는 가상 실린더를 생성한 결과를 예시한 도면, 도 12는 과채류와 가상 실린더를 보다 자세히 나타낸 도면, 도 13은 본 발명의 다른 실시예에 따른 과채류 세그먼테이션/자세추정 방법의 설명에 제공되는 흐름도이다."}
