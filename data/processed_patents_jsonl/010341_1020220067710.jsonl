{"patent_id": "10-2022-0067710", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0083987", "출원번호": "10-2022-0067710", "발명의 명칭": "음성 합성 방법, 장치, 전자 기기 및 저장 매체", "출원인": "베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니", "발명자": "쟝, 쥔텅"}}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 합성 방법에 있어서, 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득하는 단계; 상기 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하는 단계; 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발음 정보에 대해 특징 추출을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 단계; 및 상기 타겟 텍스트의 언어학 특징 및 상기 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는단계;를 포함하는,것을 특징으로 하는 음성 합성 방법."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발음 정보에 대해 특징 추출을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 단계는, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 발음 정보에 따라, 상기 적어도 하나의 캐릭터에 포함된 음소, 및 상기 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 단계;상기 타겟 텍스트가 속하는 타겟 언어 유형에 따라, 상기 음소에 접미사를 추가하고, 상기 음조의 음조 인코딩을 결정하는 단계; 및 상기 접미사가 추가된 상기 음소, 상기 음조 인코딩, 및 음절에서의 상기 음소의 위치 및 단어에서의 상기 음절의 위치 중의 적어도 하나에 따라, 상기 언어학 특징 중 대응되는 특징 항목을 생성하는 단계;를 포함하는, 것을 특징으로 하는 음성 합성 방법."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 발음 정보에 따라, 상기 적어도 하나의 캐릭터에 포함된 음소, 및 상기 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 단계는,상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터에 대해, 상기 캐릭터의 발음 정보 중의 성조, 악센트 및 얼화음 중의 하나 또는 복수의 조합에 따라, 상기 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 단계를 포함하는, 것을 특징으로 하는 음성 합성 방법."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서, 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발음 정보에 대해 특징 추출을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 단계는, 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트를 단어로 나누고, 각 분사 어휘에 대응되는 운율을 결정하는 단계; 및 각 상기 분사 어휘에 대응되는 운율에 따라, 상기 언어학 특징 중 대응되는 특징 항목을 생성하는 단계;를 더공개특허 10-2022-0083987-3-포함하는, 것을 특징으로 하는 음성 합성 방법."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 타겟 텍스트의 언어학 특징 및 상기 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는단계는, 상기 타겟 텍스트의 언어학 특징을 음성 합성 모델의 제1 인코더에 입력하여, 특징 인코딩을 획득하는 단계;상기 스피커의 식별자를 상기 음성 합성 모델의 제2 인코더에 입력하여, 상기 스피커의 음색 인코딩을 획득하는단계;상기 언어학 특징 및 상기 스피커의 식별자를 상기 음성 합성 모델의 스타일 네트워크에 입력하여 상기 타겟 텍스트 및 상기 스피커에 대응되는 스타일 인코딩을 획득하는 단계;상기 스타일 인코딩, 상기 특징 인코딩 및 상기 음색 인코딩을 융합하여, 융합 인코딩을 획득하는 단계; 및 상기 음성 합성 모델의 디코더를 사용하여 상기 융합 인코딩을 디코딩하여, 상기 타겟 음성의 음향 스펙트럼을획득하는 단계;를 포함하는, 것을 특징으로 하는 음성 합성 방법."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 타겟 텍스트의 언어학 특징을 음성 합성 모델의 제1 인코더에 입력하여, 특징 인코딩을 획득하기 전에, 상기 음성 합성 모델의 상기 제1 인코더, 상기 제2 인코더, 상기 디코더 및 참조 네트워크에 따라, 트레이닝 모델을 생성하는 단계 - 상기 제1 인코더, 상기 제2 인코더 및 상기 참조 네트워크의 출력은 상기 디코더의 입력에 연결됨 -;트레이닝 데이터를 사용하여, 상기 트레이닝 모델 및 상기 스타일 네트워크에 대해 트레이닝하는 단계; 및트레이닝된 상기 트레이닝 모델의 상기 제1 인코더, 상기 제2 인코더 및 상기 디코더, 및 트레이닝된 상기 스타일 네트워크에 따라, 상기 음성 합성 모델을 생성하는 단계;를 더 포함하는, 것을 특징으로 하는 음성 합성 방법."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 트레이닝 데이터는 텍스트 샘플의 언어학 특징, 및 상기 텍스트 샘플에 대응되는 음성 샘플 및 상기 음성샘플의 스피커 식별자를 포함하며;상기 트레이닝 데이터를 사용하여, 상기 트레이닝 모델 및 상기 스타일 네트워크에 대해 트레이닝하는 단계는, 상기 텍스트 샘플의 언어학 특징을 상기 트레이닝 모델의 상기 제1 인코더에 입력하고, 상기 음성 샘플의 스피커 식별자를 상기 트레이닝 모델의 상기 제2 인코더에 입력하는 단계;상기 음성 샘플을 상기 트레이닝 모델의 참조 네트워크에 입력하는 단계;상기 참조 네트워크의 출력, 상기 제1 인코더의 출력 및 상기 제2 인코더의 출력을 융합하고, 상기 트레이닝 모델의 상기 디코더를 사용하여 디코딩하여, 예측 음향 스펙트럼을 획득하는 단계;상기 예측 음향 스펙트럼과 상기 음성 샘플의 음향 스펙트럼 사이의 차이에 따라, 상기 트레이닝 모델에 대해모델 파라미터 조정을 수행하는 단계;상기 텍스트 샘플의 언어학 특징 및 상기 음성 샘플의 스피커 식별자를 상기 스타일 네트워크에 입력하는 단계;및 공개특허 10-2022-0083987-4-상기 스타일 네트워크의 출력과 상기 참조 네트워크의 출력 사이의 차이에 따라, 상기 스타일 네트워크에 대해모델 파라미터 조정을 수행하는 단계;를 포함하는,것을 특징으로 하는 음성 합성 방법."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "음성 합성 장치에 있어서, 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득하는 제1 획득 모듈; 상기 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하는 제2 획득 모듈; 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발음 정보에 대해 특징 추출을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 추출 모듈; 및 상기 타겟 텍스트의 언어학 특징 및 상기 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는합성 모듈;을 포함하는,것을 특징으로 하는 음성 합성 장치."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 추출 모듈은, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 발음 정보에 따라, 상기 적어도 하나의 캐릭터에 포함된 음소, 및 상기 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 제1 결정 유닛;상기 타겟 텍스트가 속하는 타겟 언어 유형에 따라, 상기 음소에 접미사를 추가하고, 상기 음조의 음조 인코딩을 결정하는 제2 결정 유닛; 및 상기 접미사가 추가된 상기 음소, 상기 음조 인코딩, 및 음절에서의 상기 음소의 위치 및 단어에서의 상기 음절의 위치 중의 적어도 하나에 따라, 상기 언어학 특징 중 대응되는 특징 항목을 생성하는 제1 생성 유닛;을 포함하는, 것을 특징으로 하는 음성 합성 장치."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 제1 결정 유닛은,상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터에 대해, 상기 캐릭터의 발음 정보 중의 성조, 악센트 및 얼화음 중의 하나 또는 복수의 조합에 따라, 상기 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 결정서브유닛을 포함하는, 것을 특징으로 하는 음성 합성 장치."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 상기 추출 모듈은, 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트를 단어로 나누고, 각 분사 어휘에 대응되는 운율을 결정하는 제3 결정 유닛; 및 각 상기 분사 어휘에 대응되는 운율에 따라, 상기 언어학 특징 중 대응되는 특징 항목을 생성하는 제2 생성 유닛;을 더 포함하는, 것을 특징으로 하는 음성 합성 장치.공개특허 10-2022-0083987-5-청구항 12 제8항 내지 제11항 중 어느 한 항에 있어서, 상기 합성 모듈은, 상기 타겟 텍스트의 언어학 특징을 음성 합성 모델의 제1 인코더에 입력하여, 특징 인코딩을 획득하는 제1 인코딩 유닛;상기 스피커의 식별자를 상기 음성 합성 모델의 제2 인코더에 입력하여, 상기 스피커의 음색 인코딩을 획득하는제2 인코딩 유닛;상기 언어학 특징 및 상기 스피커의 식별자를 상기 음성 합성 모델의 스타일 네트워크에 입력하여 상기 타겟 텍스트 및 상기 스피커에 대응되는 스타일 인코딩을 획득하는 제3 인코딩 유닛;상기 스타일 인코딩, 상기 특징 인코딩 및 상기 음색 인코딩을 융합하여, 융합 인코딩을 획득하는 융합 유닛;및 상기 음성 합성 모델의 디코더를 사용하여 상기 융합 인코딩을 디코딩하여, 상기 타겟 음성의 음향 스펙트럼을획득하는 디코딩 유닛;을 포함하는, 것을 특징으로 하는 음성 합성 장치."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 음성 합성 모델의 상기 제1 인코더, 상기 제2 인코더, 상기 디코더 및 참조 네트워크에 따라, 트레이닝 모델을 생성하는 제1 생성 모듈 - 상기 제1 인코더, 상기 제2 인코더 및 상기 참조 네트워크의 출력은 상기 디코더의 입력에 연결됨 -;트레이닝 데이터를 사용하여, 상기 트레이닝 모델 및 상기 스타일 네트워크에 대해 트레이닝하는 트레이닝모듈; 및트레이닝된 상기 트레이닝 모델의 상기 제1 인코더, 상기 제2 인코더 및 상기 디코더, 및 트레이닝된 상기 스타일 네트워크에 따라, 상기 음성 합성 모델을 생성하는 제2 생성 모듈;을 더 포함하는, 것을 특징으로 하는 음성 합성 장치."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 트레이닝 데이터는 텍스트 샘플의 언어학 특징, 및 상기 텍스트 샘플에 대응되는 음성 샘플 및 상기 음성샘플의 스피커 식별자를 포함하며;상기 트레이닝 모듈은, 상기 텍스트 샘플의 언어학 특징을 상기 트레이닝 모델의 상기 제1 인코더에 입력하고, 상기 음성 샘플의 스피커 식별자를 상기 트레이닝 모델의 상기 제2 인코더에 입력하는 제1 처리 유닛;상기 음성 샘플을 상기 트레이닝 모델의 참조 네트워크에 입력하는 제2 처리 유닛;상기 참조 네트워크의 출력, 상기 제1 인코더의 출력 및 상기 제2 인코더의 출력을 융합하고, 상기 트레이닝 모델의 상기 디코더를 사용하여 디코딩하여, 예측 음향 스펙트럼을 획득하는 제3 처리 유닛;상기 예측 음향 스펙트럼과 상기 음성 샘플의 음향 스펙트럼 사이의 차이에 따라, 상기 트레이닝 모델에 대해모델 파라미터 조정을 수행하는 제1 조정 유닛;상기 텍스트 샘플의 언어학 특징 및 상기 음성 샘플의 스피커 식별자를 상기 스타일 네트워크에 입력하는 제4처리 유닛; 및 상기 스타일 네트워크의 출력과 상기 참조 네트워크의 출력 사이의 차이에 따라, 상기 스타일 네트워크에 대해공개특허 10-2022-0083987-6-모델 파라미터 조정을 수행하는 제2 조정 유닛;을 포함하는,것을 특징으로 하는 음성 합성 장치."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "전자 기기에 있어서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 가능하게 연결되는 메모리;를 포함하고, 상기 메모리에 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 저장되어 있고, 상기 명령은 상기 적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서가 제1항 내지 제7항 중 어느 한 항에 따른방법을 구현하는,것을 특징으로 하는 전자 기기."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터가 제1항 내지 제7항 중 어느 한 항에 따른 방법을 수행하는데 사용되는, 것을 특징으로 하는 비일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2022-0067710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "컴퓨터 판독 가능 저장 매체에 저장되어 있는 컴퓨터 프로그램에 있어서, 상기 컴퓨터 프로그램이 프로세서에 의해 수행되어, 제1항 내지 제7항 중 어느 한 항에 따른 방법을 구현하는,것을 특징으로 하는 컴퓨터 프로그램."}
{"patent_id": "10-2022-0067710", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 음성 합성 방법, 장치, 전자 기기 및 저장 매체를 제공하며, 컴퓨터"}
{"patent_id": "10-2022-0067710", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것으로, 특히 딥러닝, 음성 기술과 같은 인공지능 기술 분야에 관한 것이다. 구체적인 구현 수단은 합성하고자 하는 타겟 텍스 트, 및 스피커의 식별자를 획득하는 단계; 상기 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하는 단계; 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발 음 정보에 대해 특징 추출을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 단계; 및 상기 타겟 텍스트 의 언어학 특징 및 상기 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는 단계;를 포함한다. 따라서, 한 가지 언어의 스피커에 대해, 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있다. 대 표 도 - 도1"}
{"patent_id": "10-2022-0067710", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2022-0083987 CPC특허분류 G10L 13/10 (2013.01) G10L 19/02 (2013.01) 발명자 쑨, 타오 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 10, 바이두 캠퍼스 2층지아, 레이 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 10, 바이두 캠퍼스 2층명 세 서 청구범위 청구항 1 음성 합성 방법에 있어서, 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득하는 단계; 상기 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하는 단계; 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발음 정 보에 대해 특징 추출을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 단계; 및 상기 타겟 텍스트의 언어학 특징 및 상기 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는 단계;를 포함하는, 것을 특징으로 하는 음성 합성 방법. 청구항 2 제1항에 있어서, 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발음 정 보에 대해 특징 추출을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 단계는, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 발음 정보에 따라, 상기 적어도 하나의 캐릭터에 포함된 음 소, 및 상기 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 단계; 상기 타겟 텍스트가 속하는 타겟 언어 유형에 따라, 상기 음소에 접미사를 추가하고, 상기 음조의 음조 인코딩 을 결정하는 단계; 및 상기 접미사가 추가된 상기 음소, 상기 음조 인코딩, 및 음절에서의 상기 음소의 위치 및 단어에서의 상기 음절 의 위치 중의 적어도 하나에 따라, 상기 언어학 특징 중 대응되는 특징 항목을 생성하는 단계;를 포함하는, 것을 특징으로 하는 음성 합성 방법. 청구항 3 제2항에 있어서, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 발음 정보에 따라, 상기 적어도 하나의 캐릭터에 포함된 음 소, 및 상기 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 단계는, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터에 대해, 상기 캐릭터의 발음 정보 중의 성조, 악센트 및 얼화 음 중의 하나 또는 복수의 조합에 따라, 상기 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 단계 를 포함하는, 것을 특징으로 하는 음성 합성 방법. 청구항 4 제2항에 있어서, 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발음 정 보에 대해 특징 추출을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 단계는, 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트를 단어로 나누고, 각 분사 어휘에 대응되는 운 율을 결정하는 단계; 및 각 상기 분사 어휘에 대응되는 운율에 따라, 상기 언어학 특징 중 대응되는 특징 항목을 생성하는 단계;를 더포함하는, 것을 특징으로 하는 음성 합성 방법. 청구항 5 제1항에 있어서, 상기 타겟 텍스트의 언어학 특징 및 상기 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는 단계는, 상기 타겟 텍스트의 언어학 특징을 음성 합성 모델의 제1 인코더에 입력하여, 특징 인코딩을 획득하는 단계; 상기 스피커의 식별자를 상기 음성 합성 모델의 제2 인코더에 입력하여, 상기 스피커의 음색 인코딩을 획득하는 단계; 상기 언어학 특징 및 상기 스피커의 식별자를 상기 음성 합성 모델의 스타일 네트워크에 입력하여 상기 타겟 텍 스트 및 상기 스피커에 대응되는 스타일 인코딩을 획득하는 단계; 상기 스타일 인코딩, 상기 특징 인코딩 및 상기 음색 인코딩을 융합하여, 융합 인코딩을 획득하는 단계; 및 상기 음성 합성 모델의 디코더를 사용하여 상기 융합 인코딩을 디코딩하여, 상기 타겟 음성의 음향 스펙트럼을 획득하는 단계;를 포함하는, 것을 특징으로 하는 음성 합성 방법. 청구항 6 제5항에 있어서, 상기 타겟 텍스트의 언어학 특징을 음성 합성 모델의 제1 인코더에 입력하여, 특징 인코딩을 획득하기 전에, 상기 음성 합성 모델의 상기 제1 인코더, 상기 제2 인코더, 상기 디코더 및 참조 네트워크에 따라, 트레이닝 모 델을 생성하는 단계 - 상기 제1 인코더, 상기 제2 인코더 및 상기 참조 네트워크의 출력은 상기 디코더의 입력 에 연결됨 -; 트레이닝 데이터를 사용하여, 상기 트레이닝 모델 및 상기 스타일 네트워크에 대해 트레이닝하는 단계; 및 트레이닝된 상기 트레이닝 모델의 상기 제1 인코더, 상기 제2 인코더 및 상기 디코더, 및 트레이닝된 상기 스타 일 네트워크에 따라, 상기 음성 합성 모델을 생성하는 단계;를 더 포함하는, 것을 특징으로 하는 음성 합성 방법. 청구항 7 제6항에 있어서, 상기 트레이닝 데이터는 텍스트 샘플의 언어학 특징, 및 상기 텍스트 샘플에 대응되는 음성 샘플 및 상기 음성 샘플의 스피커 식별자를 포함하며; 상기 트레이닝 데이터를 사용하여, 상기 트레이닝 모델 및 상기 스타일 네트워크에 대해 트레이닝하는 단계는, 상기 텍스트 샘플의 언어학 특징을 상기 트레이닝 모델의 상기 제1 인코더에 입력하고, 상기 음성 샘플의 스피 커 식별자를 상기 트레이닝 모델의 상기 제2 인코더에 입력하는 단계; 상기 음성 샘플을 상기 트레이닝 모델의 참조 네트워크에 입력하는 단계; 상기 참조 네트워크의 출력, 상기 제1 인코더의 출력 및 상기 제2 인코더의 출력을 융합하고, 상기 트레이닝 모 델의 상기 디코더를 사용하여 디코딩하여, 예측 음향 스펙트럼을 획득하는 단계; 상기 예측 음향 스펙트럼과 상기 음성 샘플의 음향 스펙트럼 사이의 차이에 따라, 상기 트레이닝 모델에 대해 모델 파라미터 조정을 수행하는 단계; 상기 텍스트 샘플의 언어학 특징 및 상기 음성 샘플의 스피커 식별자를 상기 스타일 네트워크에 입력하는 단계; 및 상기 스타일 네트워크의 출력과 상기 참조 네트워크의 출력 사이의 차이에 따라, 상기 스타일 네트워크에 대해 모델 파라미터 조정을 수행하는 단계;를 포함하는, 것을 특징으로 하는 음성 합성 방법. 청구항 8 음성 합성 장치에 있어서, 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득하는 제1 획득 모듈; 상기 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하는 제2 획득 모듈; 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발음 정 보에 대해 특징 추출을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 추출 모듈; 및 상기 타겟 텍스트의 언어학 특징 및 상기 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는 합성 모듈;을 포함하는, 것을 특징으로 하는 음성 합성 장치. 청구항 9 제8항에 있어서, 상기 추출 모듈은, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 발음 정보에 따라, 상기 적어도 하나의 캐릭터에 포함된 음 소, 및 상기 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 제1 결정 유닛; 상기 타겟 텍스트가 속하는 타겟 언어 유형에 따라, 상기 음소에 접미사를 추가하고, 상기 음조의 음조 인코딩 을 결정하는 제2 결정 유닛; 및 상기 접미사가 추가된 상기 음소, 상기 음조 인코딩, 및 음절에서의 상기 음소의 위치 및 단어에서의 상기 음절 의 위치 중의 적어도 하나에 따라, 상기 언어학 특징 중 대응되는 특징 항목을 생성하는 제1 생성 유닛;을 포함 하는, 것을 특징으로 하는 음성 합성 장치. 청구항 10 제9항에 있어서, 상기 제1 결정 유닛은, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터에 대해, 상기 캐릭터의 발음 정보 중의 성조, 악센트 및 얼화 음 중의 하나 또는 복수의 조합에 따라, 상기 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 결정 서브유닛을 포함하는, 것을 특징으로 하는 음성 합성 장치. 청구항 11 제9항에 있어서, 상기 추출 모듈은, 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트를 단어로 나누고, 각 분사 어휘에 대응되는 운 율을 결정하는 제3 결정 유닛; 및 각 상기 분사 어휘에 대응되는 운율에 따라, 상기 언어학 특징 중 대응되는 특징 항목을 생성하는 제2 생성 유 닛;을 더 포함하는, 것을 특징으로 하는 음성 합성 장치.청구항 12 제8항 내지 제11항 중 어느 한 항에 있어서, 상기 합성 모듈은, 상기 타겟 텍스트의 언어학 특징을 음성 합성 모델의 제1 인코더에 입력하여, 특징 인코딩을 획득하는 제1 인코 딩 유닛; 상기 스피커의 식별자를 상기 음성 합성 모델의 제2 인코더에 입력하여, 상기 스피커의 음색 인코딩을 획득하는 제2 인코딩 유닛; 상기 언어학 특징 및 상기 스피커의 식별자를 상기 음성 합성 모델의 스타일 네트워크에 입력하여 상기 타겟 텍 스트 및 상기 스피커에 대응되는 스타일 인코딩을 획득하는 제3 인코딩 유닛; 상기 스타일 인코딩, 상기 특징 인코딩 및 상기 음색 인코딩을 융합하여, 융합 인코딩을 획득하는 융합 유닛; 및 상기 음성 합성 모델의 디코더를 사용하여 상기 융합 인코딩을 디코딩하여, 상기 타겟 음성의 음향 스펙트럼을 획득하는 디코딩 유닛;을 포함하는, 것을 특징으로 하는 음성 합성 장치. 청구항 13 제12항에 있어서, 상기 음성 합성 모델의 상기 제1 인코더, 상기 제2 인코더, 상기 디코더 및 참조 네트워크에 따라, 트레이닝 모 델을 생성하는 제1 생성 모듈 - 상기 제1 인코더, 상기 제2 인코더 및 상기 참조 네트워크의 출력은 상기 디코 더의 입력에 연결됨 -; 트레이닝 데이터를 사용하여, 상기 트레이닝 모델 및 상기 스타일 네트워크에 대해 트레이닝하는 트레이닝 모듈; 및 트레이닝된 상기 트레이닝 모델의 상기 제1 인코더, 상기 제2 인코더 및 상기 디코더, 및 트레이닝된 상기 스타 일 네트워크에 따라, 상기 음성 합성 모델을 생성하는 제2 생성 모듈;을 더 포함하는, 것을 특징으로 하는 음성 합성 장치. 청구항 14 제13항에 있어서, 상기 트레이닝 데이터는 텍스트 샘플의 언어학 특징, 및 상기 텍스트 샘플에 대응되는 음성 샘플 및 상기 음성 샘플의 스피커 식별자를 포함하며; 상기 트레이닝 모듈은, 상기 텍스트 샘플의 언어학 특징을 상기 트레이닝 모델의 상기 제1 인코더에 입력하고, 상기 음성 샘플의 스피 커 식별자를 상기 트레이닝 모델의 상기 제2 인코더에 입력하는 제1 처리 유닛; 상기 음성 샘플을 상기 트레이닝 모델의 참조 네트워크에 입력하는 제2 처리 유닛; 상기 참조 네트워크의 출력, 상기 제1 인코더의 출력 및 상기 제2 인코더의 출력을 융합하고, 상기 트레이닝 모 델의 상기 디코더를 사용하여 디코딩하여, 예측 음향 스펙트럼을 획득하는 제3 처리 유닛; 상기 예측 음향 스펙트럼과 상기 음성 샘플의 음향 스펙트럼 사이의 차이에 따라, 상기 트레이닝 모델에 대해 모델 파라미터 조정을 수행하는 제1 조정 유닛; 상기 텍스트 샘플의 언어학 특징 및 상기 음성 샘플의 스피커 식별자를 상기 스타일 네트워크에 입력하는 제4 처리 유닛; 및 상기 스타일 네트워크의 출력과 상기 참조 네트워크의 출력 사이의 차이에 따라, 상기 스타일 네트워크에 대해모델 파라미터 조정을 수행하는 제2 조정 유닛;을 포함하는, 것을 특징으로 하는 음성 합성 장치. 청구항 15 전자 기기에 있어서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 가능하게 연결되는 메모리;를 포함하고, 상기 메모리에 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 저장되어 있고, 상기 명령은 상기 적어 도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서가 제1항 내지 제7항 중 어느 한 항에 따른 방법을 구현하는, 것을 특징으로 하는 전자 기기. 청구항 16 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터가 제1항 내지 제7항 중 어느 한 항에 따른 방법을 수행하는데 사용되는, 것을 특징으로 하는 비일시적 컴퓨터 판독 가능 저장 매체. 청구항 17 컴퓨터 판독 가능 저장 매체에 저장되어 있는 컴퓨터 프로그램에 있어서, 상기 컴퓨터 프로그램이 프로세서에 의해 수행되어, 제1항 내지 제7항 중 어느 한 항에 따른 방법을 구현하는, 것을 특징으로 하는 컴퓨터 프로그램. 발명의 설명 기 술 분 야 본 개시는 컴퓨터 기술 분야에 관한 것으로, 특히 딥러닝, 음성 기술 등 인공지능 기술 분야에 관한 것으로, 더 욱이 음성 합성 방법, 장치, 전자 기기 및 저장 매체에 관한 것이다."}
{"patent_id": "10-2022-0067710", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "음성 합성 기술은 텍스트 정보를 알아들을 수 있는 자연스럽고, 의인화된 음성 정보로 전환하는 기술이며, 뉴스 방송, 차량 탑재 네비게이션, 스마트 스피커 등 분야에서 널리 사용되고 있다. 음성 합성 기술의 적용 시나리오가 증가함에 따라 여러가지 언어의 음성 합성에 대한 요구가 증가하고 있다. 그 러나, 일반적으로 한 사람이 한 가지 언어만 할 수 있기 때문에, 일 대 여러가지 언어 말뭉치를 획득하기가 어 렵고, 따라서 관련 기술에서 음성 합성 기술은 일반적으로 일 대 일 언어의 음성 합성만을 지원한다. 일 대 여 러가지 언어의 음성 합성을 구현하는 방법이 음성 합성의 적용 시나리오를 확장하는데 매우 중요하다. 본 개시는 음성 합성 방법, 장치, 전자 기기 및 저장 매체를 제공한다. 본 개시의 일 측면에 따르면, 음성 합성 방법을 제공하며, 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득하는 단계; 상기 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하는 단계; 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발음 정보에 대해 특징 추출 을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 단계; 및 상기 타겟 텍스트의 언어학 특징 및 상기 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는 단계;를 포함한다. 본 개시의 다른 측면에 따르면, 음성 합성 장치를 제공하며, 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자 를 획득하는 제1 획득 모듈; 상기 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하는 제2 획득모듈; 상기 타겟 텍스트가 속하는 타겟 언어에 따라, 상기 타겟 텍스트 중 상기 적어도 하나의 캐릭터의 상기 발음 정보에 대해 특징 추출을 수행하여, 상기 타겟 텍스트의 언어학 특징을 생성하는 추출 모듈; 및 상기 타겟 텍스트의 언어학 특징 및 상기 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는 합성 모듈;을 포함한다. 본 개시의 다른 측면에 따르면, 전자 기기를 제공하며, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세 서와 통신 가능하게 연결되는 메모리;를 포함하고, 상기 메모리에 상기 적어도 하나의 프로세서에 의해 수행 가 능한 명령이 저장되어 있고, 상기 명령은 상기 적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서가 전술한 음성 합성 방법을 구현할 수 있도록 한다. 본 개시의 다른 측면에 따르면, 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장 매체를 제공하며, 상기 컴퓨터 명령은 상기 컴퓨터가 전술한 음성 합성 방법을 수행하는데 사용된다. 본 개시의 다른 측면에 따르면, 컴퓨터 판독 가능 저장 매체에 저장되어 있는 컴퓨터 프로그램을 제공하며, 상 기 컴퓨터 프로그램이 프로세서에 의해 수행되어, 전술한 음성 합성 방법의 단계를 구현한다. 이해 가능한 바로는, 본 부분에서 설명된 내용은 본 개시의 실시예의 핵심 또는 중요한 특징을 식별하기 위한 것이 아니며, 본 개시의 범위를 한정하지도 않는다. 본 개시의 기타 특징들은 하기의 명세서에 의해 쉽게 이해 될 것이다."}
{"patent_id": "10-2022-0067710", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 결합하여 본 개시의 예시적인 실시예에 대해 설명하며, 여기에는 이해를 돕기 위해 본 개 시의 실시예의 다양한 세부 사항을 포함하므로, 이는 단지 예시적인 것으로 이해해야 한다. 따라서, 당업자는 본 개시의 범위 및 사상을 벗어나지 않는 한 여기에 설명된 실시예에 대해 다양한 변경 및 수정이 이루어질 수 있음을 인식해야 한다. 마찬가지로, 명확성과 간결성을 위해, 하기의 설명에서는 공지된 기능 및 구조에 대한 설명을 생략한다. 설명해야 하는 바로는, 본 개시의 기술적 수단에서 관련된 사용자 개인정보의 획득, 저장 및 적용 등은 모두 관 계법령을 준수하고, 공서양속에 반하지 않는다. 음성 합성 기술의 적용 시나리오가 증가함에 따라 여러가지 언어의 음성 합성에 대한 요구가 증가하고 있다. 그 러나, 일반적으로 한 사람이 한 가지 언어만 할 수 있기 때문에, 일 대 여러가지 언어 말뭉치를 획득하기가 어 렵고, 따라서 관련 기술에서 음성 합성 기술은 일반적으로 일 대 일 언어의 음성 합성만을 지원한다. 일 대 여 러가지 언어의 음성 합성을 구현하는 방법이 음성 합성의 적용 시나리오를 확장하는데 매우 중요하다. 본 개시는 일 대 여러가지 언어의 음성 합성을 구현할 수 있는 방법을 제공하며, 당해 방법에서 먼저 합성하고 자 하는 타겟 텍스트, 및 스피커의 식별자를 획득하고, 다음 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보 를 획득하여, 타겟 텍스트가 속하는 타겟 언어에 따라, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 대 해 특징 추출을 수행하여, 타겟 텍스트의 언어학 특징을 생성하고, 나아가 타겟 텍스트의 언어학 특징 및 스피 커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득한다. 따라서, 합성하고자 하는 타겟 텍스트의 언 어학 특징 및 스피커의 식별자에 따라 언어 합성을 수행하여, 한 가지 언어의 스피커에 대해, 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있다. 이하, 첨부된 도면을 참조하여 본 개시의 실시예의 음성 합성 방법, 장치, 전자 기기, 비일시적 컴퓨터 판독 가 능 저장 매체 및 컴퓨터 프로그램을 설명한다. 먼저, 도1을 결합하여 본 개시에서 제공되는 음성 합성 방법에 대해 상세히 설명한다. 도1은 본 개시의 제1 실시예에 따른 음성 합성 방법의 개략적인 흐름도이다. 설명해야 하는 바로는, 본 개시의 실시예에서 제공되는 음성 합성 방법의 수행 주체는 음성 합성 장치이다. 당해 음성 합성 장치는 구체적으로 전 자 기기일 수도 있고, 전자 기기에 구성된 소프트웨어 등일 수도 있으며, 한 가지 언어의 스피커에 대해, 여러 가지 언어의 텍스트의 음성 합성을 구현할 수 있도록 한다. 본 개시의 실시예에서는 음성 합성 장치가 전자 기 기에 구성된 것을 예로 들어 설명한다. 전자 기기는 데이터 처리가 가능한 임의의 정적 또는 모바일 컴퓨팅 기기일 수 있으며, 예컨대 랩톱 컴퓨터, 스 마트폰, 웨어러블 기기와 같은 모바일 컴퓨팅 기기, 또는 데스크톱 컴퓨터와 같은 정적 컴퓨팅 기기, 또는 서버, 또는 기타 유형의 컴퓨팅 기기 등일 수 있으나, 본 개시에서 이에 대해 한정하지 않는다. 도1에 도시된 바와 같이, 음성 합성 방법은 단계 101 내지 단계 104를 포함할 수 있다. 단계 101, 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득한다. 본 개시의 실시예에서, 합성하고자 하는 텍스트는 임의의 언어의 임의의 텍스트일 수 있다. 언어는 예를 들어 중국어, 영어, 일본어 등이다. 텍스트는 예를 들어 뉴스 텍스트, 오락 텍스트, 채팅 텍스트 등이다. 설명해야 하는 바로는, 합성하고자 하는 타겟 텍스트는 한 가지 언어의 텍스트일 수도 있고, 여러가지 언어의 텍스트일 수도 있으나, 본 개시에서 이에 대해 한정하지 않는다. 스피커의 식별자는 스피커를 유일하게 식별하기 위한 것이다. 스피커는 타겟 텍스트에 따라 합성된 타겟 음성에 속하는 스피커를 의미한다. 예를 들어, 합성하고자 하는 타겟 텍스트에 따라 스피커 A의 음성을 합성할 경우, 스피커는 스피커 A이고; 합성하고자 하는 타겟 텍스트에 따라 스피커 B의 음성을 합성할 경우, 스피커는 스피커 B이다. 설명해야 하는 바로는, 본 개시의 실시예에서 음성 합성 장치는 다양한 공개, 법률 준수 방식으로 합성하고자 하는 타겟 텍스트를 획득할 수 있다. 예를 들어, 음성 합성 장치는 채팅 텍스트에 속하는 채팅 사용자의 승인을 받은 후, 채팅 사용자의 채팅 텍스트를 획득하여 합성하고자 하는 타겟 텍스트로 사용할 수 있다. 단계 102, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득한다. 발음 정보는 음소, 음절, 단어, 성조, 악센트, 얼화음 등 정보를 포함할 수 있다. 음소는 음성의 자연 속성에 따라 구분되는 최소의 음성 단위이고; 음절은 음소로 발음을 구성하는 음성 단위이고; 성조는 소리의 고저를 나 타내며, 예컨대, 중국어에서 성조는 일성, 이성, 삼성, 사성, 경성을 포함할 수 있되, 일본어에서 성조는 고음 과 저음을 포함할 수 있다. 악센트는 악센트 강도를 나타내며, 스피커가 강조하려는 논리적 중점 또는 정서적 중점을 나타낼 수 있으며, 예컨대 영어에서 악센트는 악센트 없음에서 강한 악센트까지 3단계의 악센트 강도를 포함할 수 있고; 얼화음은 중국어에서 개별 한자의 운모가 혀를 말리는 동작으로 인한 성조 변환 현상이며, 운 모 뒤에 r이 붙는 것이 특징이다. 구체적으로, 타겟 텍스트가 속하는 타겟 언어에 따라, 타겟 텍스트에 포함된 적어도 하나의 캐릭터의 발음 정보를 조회할 수 있다. \" (그들은 모두 사냥을 매우 좋아한다)\"라는 중국어 텍스트를 예로 들어, 중국어 텍스트 중 각 캐릭터의 발음 정보를 획득할 수 있다. 이 중에서, 각 캐릭터의 발음 정보는 “ta1 men5 ne5 dou1 fei1 chang2 xi3 huan1 da3 lie4”를 포함할 수 있다. “t”, “a”, “m”, “en”, “n”, “e” 등은 음소이고; “ta”, “men”, “ne”, “dou” 등은 음절이며, 음절 사이는 공백으로 이격되고; 숫자는 중 국어 성조를 나타내는 바, “1”은 일성을 나타내고, “2”는 이성을 나타내고, “3”은 삼성을 나타내고, “4”는 사성을 나타내고, “5”는 경성을 나타낸다. 단계 103, 타겟 텍스트가 속하는 타겟 언어에 따라, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 대해 특징 추출을 수행하여, 타겟 텍스트의 언어학 특징을 생성한다. 언어학 특징은 타겟 텍스트의 음조 변경, 운율 등을 나타낼 수 있는 특징이다. 상이한 언어의 텍스트가 상이한 음조 변경, 운율 등 특점을 구비하기 때문에, 본 개시의 실시예에서, 타겟 텍스 트가 속하는 타겟 언어에 따라, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 대해 특징 추출을 수행하 여, 타겟 텍스트의 언어학 특징을 생성할 수 있다. 구체적인 특징 추출 방법은 하기의 실시예에서 설명될 것이 며, 여기서 반복하지 않는다. 단계 104, 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득한다. 예시적인 실시예에서, 먼저 트레이닝하여 음성 합성 모델을 획득하고, 음성 합성 모델의 입력은 텍스트의 언어 학 특징 및 스피커의 식별자이고, 출력은 합성된 음성이다. 따러서, 타겟 텍스트의 언어학 특징 및 스피커의 식 별자를 트레이닝된 음성 합성 모델에 입력하여, 음성 합성을 수행하여, 타겟 음성을 획득할 수 있다. 임의의 언어의 타겟 텍스트에 대해, 모두 타겟 텍스트가 속하는 타겟 언어에 따라, 타겟 텍스트 중 적어도 하나 의 캐릭터의 발음 정보에 대해 특징 추출을 수행하여, 타겟 텍스트의 언어학 특징을 생성하고, 나아가 타겟 텍 스트의 언어학 특징 및 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득할 수 있기 때문에, 한 가지 언어의 스피커에 대해, 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있다. 예를 들어, 중국어를 하는 스피커 A에 대해, 스피커 A의 식별자 및 영어의 타겟 텍스트의 언어학 특징에 따라 음성 합성을 수행하여 스피 커 A가 영어로 타겟 텍스트를 진술하는 타겟 음성을 획득할 수 있고, 또는, 스피커 A의 식별자 및 일본어의 타 겟 텍스트의 언어학 특징에 따라 음성 합성을 수행하여 스피커 A가 일본어로 타겟 텍스트를 진술하는 타겟 음성 을 획득할 수 있다. 본 개시의 실시예에서 제공되는 음성 합성 방법은 먼저 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획 득하고, 다음 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하여, 타겟 텍스트가 속하는 타겟 언어 에 따라, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 대해 특징 추출을 수행하여, 타겟 텍스트의 언어 학 특징을 생성하고, 나아가 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라, 음성 합성을 수행하여 타 겟 음성을 획득한다. 따라서, 합성하고자 하는 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라 언어 합 성을 수행하여, 한 가지 언어의 스피커에 대해, 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있다. 상기 분석으로부터 알 수 있는 바, 본 개시의 실시예에서, 타겟 텍스트가 속하는 타겟 언어에 따라, 타겟 텍스 트 중 적어도 하나의 캐릭터의 발음 정보에 대해 특징 추출을 수행하여, 타겟 텍스트의 언어학 특징을 생성하고, 나아가 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라, 음성 합성을 수행할 수 있다. 이하, 도 2를 결합하여, 본 개시에서 제공되는 음성 합성 방법 중 타겟 텍스트의 언어학 특징을 생성하는 과정에 대해 더 설명한다. 도2는 본 개시의 제2 실시예에 따른 음성 합성 방법의 개략적인 흐름도이다. 도2에 도시된 바와 같이, 음성 합 성 방법은 단계 201 내지 단계 206을 포함할 수 있다. 단계 201, 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득한다. 단계 201의 구체적인 구현 과정 및 원리는 상기 실시예의 설명을 참조할 수 있으므로, 여기서 반복하지 않는다. 단계 202, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득한다. 단계 203, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 따라, 적어도 하나의 캐릭터에 포함된 음소, 및 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정한다. 발음 정보는 음소, 음절, 단어, 성조, 악센트, 얼화음 등 정보를 포함할 수 있으나, 타겟 텍스트 중 적어도 하 나의 캐릭터의 발음 정보에 따라, 적어도 하나의 캐릭터에 포함된 음소, 및 각 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정할 수 있다. 타겟 텍스트 중 적어도 하나의 캐릭터에 대해, 캐릭터의 발음 정보의 성조, 악센트 및 얼화음 중의 하나 또는 복수의 조합에 따라, 각 음소로 조합된 음절 또는 단어에 대응되는 음조를 결 정할 수 있으므로, 결정된 각 음조의 정확성을 향상시킨다. 예시적인 실시예에서, 중국어 텍스트에 대해, 적어도 하나의 캐릭터의 발음 정보에 따라, 적어도 하나의 캐릭터 에 포함된 음소를 결정하고, 적어도 하나의 캐릭터의 발음 정보의 성조, 얼화음 중의 하나 또는 2개의 조합에 따라, 각 음소로 조합된 음절에 대응되는 음조를 결정할 수 있다. 일본어 텍스트에 대해, 적어도 하나의 캐릭터의 발음 정보에 따라, 적어도 하나의 캐릭터에 포함된 음소를 결정 하고, 적어도 하나의 캐릭터의 발음 정보의 성조에 따라, 각 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정할 수 있다. 영어 텍스트에 대해, 적어도 하나의 캐릭터의 발음 정보에 따라, 적어도 하나의 캐릭터에 포함된 음소를 결정하 고, 적어도 하나의 캐릭터의 발음 정보의 악센트에 따라, 각 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정할 수 있다. \" (그들은 모두 사냥을 매우 좋아한다)\"라는 중국어 텍스트를 예로 들어, 중국어 텍스트 중 각 캐릭터의 발음 정보를 획득할 수 있다. 이 중에서, 각 캐릭터의 발음 정보는 “ta1 men5 ne5 dou1 fei1 chang2 xi3 huan1 da3 lie4”를 포함할 수 있다. “t”, “a”, “m”, “en”, “n”, “e” 등은 음소이고; “ta”, “men”, “ne”, “dou” 등은 음절이며, 음절 사이는 공백으로 이격되고; 숫자는 중 국어 성조를 나타내는 바, “1”은 일성을 나타내고, “2”는 이성을 나타내고, “3”은 삼성을 나타내고, “4 ”는 사성을 나타내고, “5”는 경성을 나타낸다. 상기 중국어 텍스트에 포함된 각 캐릭터의 발음 정보에 따라, 각 캐릭터에 포함된 “t”, “a”, “m”, “en”, “n”, “e” 등 음소, 및 음절 “ta”에 대응되는 성조 “일성”, 음절 “men”에 대응되는 성조 “경 성”, 음절 “ne”에 대응되는 성조 “경성”, 음절 “dou”에 대응되는 성조 “일성”, 음절 “fei”에 대응되 는 성조 “일성”, 음절 “chang”에 대응되는 성조 “이성”, 음절 “xi”에 대응되는 성조 “삼성”, 음절 “ huan”에 대응되는 성조 “일성”, 음절 “da”에 대응되는 성조 “삼성”, 음절 “lie”에 대응되는 성조 “사 성”을 결정하여, 각 음절에 대응되는 성조를 각 음절에 대응되는 음조로 사용할 수 있다. 단계 204, 타겟 텍스트가 속하는 타겟 언어 유형에 따라, 음소에 접미사를 추가하고, 음조의 음조 인코딩을 결 정한다. 이해 가능한 바로는, 상이한 언어 유형의 텍스트에서, 적어도 하나의 캐릭터에 포함된 음소가 중첩되는 경우가 있으며, 예를 들어 중국어 텍스트 및 영어 텍스트에 모두 음소 \"sh\"가 포함되며, 본 개시의 실시예에서, 상이한 언어 유형의 각 음소를 구별하여, 상이한 언어 유형의 각 음소의 에일리어싱을 방지하기 위해, 각 음소에 접미 사를 추가할 수 있다. 예시적인 실시예에서, 상이한 타겟 언어 유형에 대해, 상이한 접미사를 추가할 수 있다. 예를 들어, 중국어의 경우, 각 음소에 접미사를 추가할 필요가 없으므로, 예컨대 음소 “t”, “a”, “m”, “en”의 경우 접미사를 추가하기 전과 후에 각 음소는 변경되지 않는다. 일본어의 경우, 각 음소에 접미사 \"j\"를 추가할 수 있으므로, 예컨대 음소 \"yo\",\"i\",\"yu\"의 경우, 접미사가 추가된 각 음소는 \"yoj\",\"ij\",\"yuj\"이다. 영어의 경우, 각 음소에 접미사 \"l\"를 추가할 수 있으므로, 예컨대 음소 \"sh\",\"iy\",\"hh\",\"ae\"의 경우, 접미사가 추가된 각 음소는 \"shl\",\"iyl\",\"hhl\", \"ael\"이다. 예시적인 실시예에서, 음조의 음조 인코딩 방식은 필요에 따라 결정될 수 있다. 예를 들어, 중국어 텍스트의 경우, 성조 \"일성\", \"이성\", \"삼성\", \"사성\", \"경성\"을 1, 2, 3, 4, 5로 각각 인 코딩할 수 있고, 얼화음을 1로 인코딩할 수 있고, 비얼화음을 0으로 인코딩할 수 있다. 일본어 텍스트의 경우, 고음을 1로 인코딩할 수 있고, 저음을 0으로 인코딩할 수 있다. 영어 텍스트의 경우, 악센트 없음, 중등 악센트, 강한 악센트의 3단계 악센트 강도를 0, 1, 2로 각각 인코딩할 수 있다. 이에 따라, 타겟 텍스트가 속하 는 타겟 언어 유형에 따라, 각 언어 유형에서의 각 음조의 음조 인코딩 방식으로 각 음조의 음조 인코딩을 결정 할 수 있다. 도3을 참조하면, 일본어 텍스트의 성조 유형은 다양한 성조 유형을 포함하지만, 도3에서는 유형 0 내지 유형 4 를 예로 들어 예시한다. 도3에서 영문 소문자는 음절을 나타내고, 영문 대문자 \"L\"는 저음을 나타내고, 영문 대 문자 \"H\"는 고음을 나타낸다. 도 3에 도시된 바와 같이, 유형 0의 경우, 첫번째 음절은 저음이고, 후속은 계속 고음이다. 유형 1의 경우, 첫번째 음절은 고음이고, 후속은 계속 저음이다. 유형 2의 경우, 첫번째 음절은 저음 이고, 두번째 음절은 고음이고, 후속은 계속 저음이다. 유형 3의 경우, 첫번째 음절은 저음이고, 두번째 내지 세번째 음절은 고음이고, 후속은 계속 저음이다. 유형 4의 경우, 첫번째 음절은 저음이고, 두번째 내지 네번째음절은 고음이고, 후속은 계속 저음이다. 기타 성조 유형은 이와 같이 유추된다. 도 3에 나타낸 각 성조 유형의 일본어 텍스트에 대해, 모두 고음을 1로 인코딩하고, 저음을 0으로 인코딩할 수 있다. 단계 205, 접미사가 추가된 음소 및 음조 인코딩, 및 음절에서의 음소의 위치 및 단어에서의 음절의 위치 중의 적어도 하나에 따라, 언어학 특징 중 대응되는 특징 항목을 생성한다. 예시적인 실시예에서, 중국어 텍스트의 경우, 접미사가 추가된 각 음소 및 각 음조 인코딩, 및 음절에서의 각 음소의 위치를, 언어학 특징 중 대응되는 특징 항목으로 사용할 수 있고; 일본어 텍스트 및 영어 텍스트의 경우, 접미사가 추가된 각 음소 및 각 음조 인코딩, 및 음절에서의 각 음소의 위치 및 단어에서의 각 음절의 위 치를 언어학 특징 중 대응되는 특징 항목으로 사용할 수 있다. 언어학 특징 중의 각 특징 항목은 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 특징을 나타낼 수 있다. 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 따라, 적어도 하나의 캐릭터에 포함된 음소 및 음소로 조 합된 음절 또는 단어에 대응되는 음조를 결정하고, 타겟 텍스트가 속하는 타겟 언어 유형에 따라 음소에 접미사 를 추가하고 음조의 음조 인코딩을 결정하며, 접미사가 추가된 음소 및 음조 인코딩, 및 음절에서의 음소의 위 치 및 단어에서의 음절의 위치 중의 적어도 하나에 따라, 언어학 특징 중 대응되는 특징 항목을 생성하며, 이에 따라, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보로부터 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 특징을 나타내는 각 특징을 추출하여, 후속에서 언어학 특징 생성 및 언어학 특징 기반 음성 합성의 구현에 대 해 기반을 마련하였다. 예시적인 실시예에서, 언어학 특징 중의 특징 항목은 타겟 텍스트 중의 각 분사 어휘에 대응되는 운율을 더 포 함할 수 있으며, 운율은 각 분사 어휘의 중단 시간을 나타낸다. 상응하게, 상기 단계 202 이후, 상기 음성 합성 방법은, 타겟 텍스트가 속하는 타겟 언어에 따라, 타겟 텍스트를 단어로 나누고, 각 분사 어휘에 대응되는 운율을 결정 하는 단계; 및 각 분사 어휘에 대응되는 운율에 따라, 언어학 특징 중 대응되는 특징 항목을 생성하는 단계;를 더 포함할 수 있다. 예시적인 실시예에서, 사전 트레이닝된 운율 예측 모델을 통해 각 분사 어휘에 대응되는 운율을 결정할 수 있다. 운율 예측 모델의 입력은 스피커의 식별자 및 타겟 텍스트이고, 출력은 타겟 텍스트의 각 분사 어휘에 대 응되는 운율이다. 운율 예측 모델의 구조 및 운율 예측 모델에 의해 각 분사 어휘에 대응되는 운율을 결정하는 과정은 관련 기술을 참조할 수 있으나, 여기서 반복하지 않는다. 예시적인 실시예에서, 중국어 텍스트의 경우, 운율을 4단계로 나눌 수 있으며, 각 단계로 중단 길이를 나타내는 바, #1, #2, #3, #4로 각각 표시한다. 운율 단어 내부는 0이고; #1로 운율 단어의 경계를 나타내며, 기본적으로 중단이 없고; #2로 운율 구의 경계를 나타내며, 짧은 중단을 감지할 수 있고; #3으로 어조 구의 경계를 나타내 며, 긴 중단을 감지할 수 있고; #4로 문장의 끝을 나타낸다. 일본어 텍스트의 경우, 중국어와 유사하게 운율을 4단계로 나눌 수 있다. 영어 텍스트의 경우, 운율을 4단계로 나눌 수 있으며, 각 단계로 중단 길이를 나타내는 바, \"-\", \" \", \"/\", \"%\"로 각각 표시한다. \"-\"는 연속을 나타내고; \" \"는 싱글 단어의 경계를 나타내며, 기본적 으로 중단이 없고; \"/\"는 운율 구의 경계, 짧은 중단를 나타내고; \"%\"는 어조 구의 경계 또는 문장의 끝, 긴 중 단을 나타낸다. 도 4를 참조하여, 중국어의 타겟 텍스트, 일본어의 타겟 텍스트 및 영어의 타겟 텍스트에 대해, 도 4에 도시된 타겟 텍스트 중의 각 분사 어휘에 대응되는 운율, 및 각 캐릭터의 발음 정보를 각각 획득할 수 있다. 도 4에서, \"#1\", \"#2\", \"#3\", \"#4\"는 각각 중국어 텍스트 및 일본어 텍스트 중의 각 분사 어휘에 대응되는 운율 등급을 나 타내고; \"-\", \" \", \"/\", \"%\"는 영어 텍스트 중의 각 분사 어휘에 대응되는 운율 등급을 나타낸다. 도 4에 도시 된 중국어의 타겟 텍스트 중의 각 캐릭터에 대한 발음 정보에서, 음절 사이는 공백으로 이격되고, 0-5의 숫자는 중국어의 성조를 각각 나타내며; 일본어의 타겟 텍스트 중의 각 캐릭터에 대한 발음 정보에서, 음소 사이는 공 백으로 이격되고, 음절 사이는 \". \"로 이격되고, 단어 사이는 \"/\"로 이격되며, 0, 1의 숫자는 일본어의 성조를 각각 나타내고, \":\"는 장음(일본어의 장음으로서 모음을 2개의 음절로 연장하므로, 장음에 대해 표시하여 독립 적인 일본어 음소로 사용함)을 나타내며; 영어의 타겟 텍스트 중의 각 캐릭터에 대한 발음 정보에서, 음소 사이 는 공백으로 이격되고, 음절 사이는 \".\"로 이격되고, 단어 사이는 \"/\"로 이격되며, 0, 1, 2의 숫자는 영어 악센 트를 각각 나타낸다. 나아가, 타겟 텍스트 중 각 캐릭터의 발음 정보에 따라, 각 캐릭터에 포함된 음소, 음절에서의 각 음소의 위치 및 단어에서의 각 음절의 위치 중의 적어도 하나, 및 각 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정할 수 있고, 타겟 텍스트가 속하는 타겟 언어 유형에 따라, 각 음소에 접미사를 추가하며, 예를 들어 일본어 텍 스트의 각 캐릭터에 포함된 음소의 경우 접미사 \"j\"를 추가하고, 영어 텍스트의 각 캐릭터에 포함된 음소의 경 우 접미사 \"l\"를 추가하며, 및 각 음조의 음조 인코딩, 즉 도 4에 나타낸 각 숫자를 결정한다. 또한, 타겟 텍스 트의 각 분사 어휘에 대응되는 운율, 즉 도 4에 나타낸 \"#1\", \"#4\" 등을 결정할 수 있다. 나아가, 접미사가 추 가된 각 음소 및 각 음조 인코딩, 음절에서의 각 음소의 위치, 단어에서의 각 음절의 위치, 및 각 분사 어휘에 대응되는 운율에 따라, 언어학 특징 중 대응되는 특징 항목을 생성할 수 있다. 따라서, 생성된 언어학 특징 중 대응되는 특징 항목이 더 풍부하고, 후속에서 언어학 특징을 기반으로 음성 합성을 수행할 경우, 합성 효과가 더 우수하다. 예시적인 실시예에서, 생성된 언어학 특징 중 대응되는 특징 항목은 예를 들어 도5에 나타낸 바와 같다. 영어 악센트 특징 항목에 대해, 타겟 텍스트가 영어인 경우, 당해 특징 항목은 0-2일 수 있고, 타겟 텍스트가 중국어 또는 일본어인 경우, 당해 특징 항목은 0일 수 있다. 얼화음 특징 항목에 대해, 타겟 텍스트가 중국어인 경우, 당해 특징 항목은 0 또는 1(얼화음은 1이고, 비얼화음은 0임)일 수 있고, 타겟 텍스트가 영어 또는 일본어인 경 우, 당해 특징 항목은 0일 수 있다. 단어에서의 음절의 위치라는 특징 항목에 대해, 타겟 텍스트가 중국어인 경 우, 당해 특징 항목은 0일 수 있다. 예시적인 실시예에서, 언어학 특징 중 대응되는 특징 항목을 생성한 후, 각 특징 항목에 대해 예를 들어 onehot 인코딩을 수행하여, 타겟 텍스트의 언어학 특징을 생성할 수 있다. 접미사가 추가된 각 음소를 예로 들어, 독립 적인 접미사가 추가된 각 음소를 음소 리스트에 추가하여, 음소 리스트에 따라 각 음소의 위치 색인을 획득하고, 위치 색인에 따라 접미사가 추가된 각 음소를 onehot 인코딩으로 전환할 수 있다. 구체적으로, onehot 인코딩을 수행하는 과정은 관련 기술을 참조할 수 있으나, 여기서 반복하지 않는다. 단계 206, 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득한다. 본 개시의 실시예의 음성 합성 방법은 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득하고, 타겟 텍 스트에 포함된 적어도 하나의 캐릭터에 대해, 적어도 하나의 캐릭터의 발음 정보를 획득하고, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 따라, 적어도 하나의 캐릭터에 포함된 음소, 및 음소로 조합된 음절 또는 캐릭터에 대응되는 음조를 결정하고, 타겟 텍스트가 속하는 타겟 언어 유형에 따라, 음소에 접미사를 추가하고, 음조의 음조 인코딩을 결정하며, 접미사가 추가된 음소 및 음조 인코딩, 및 음절에서의 음소의 위치 및 단어에 서의 음절의 위치 중의 적어도 하나에 따라, 언어학 특징 중 대응되는 특징 항목을 생성하고, 타겟 텍스트의 언 어학 특징 및 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하며, 한 가지 언어의 스피커에 대해, 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있다. 상기 분석으로부터 알 수 있는 바, 본 개시의 실시예에서, 음성 합성 모델을 사용하여, 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라 음성 합성을 수행하여 타겟 음성을 획득할 수 있다. 이하, 도 6을 결합하여, 본 개시에서 제공되는 음성 합성 방법 중 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는 과정에 대해 더 설명한다. 도 6은 본 개시의 제3 실시예에 따른 음성 합성 방법의 개략적인 흐름도이다. 도 6에 도시된 바와 같이, 음성 합성 방법은 단계 601 내지 단계 608을 포함할 수 있다. 단계 601, 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득한다. 단계 602, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득한다. 단계 603, 타겟 텍스트가 속하는 타겟 언어에 따라, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 대해 특징 추출을 수행하여, 타겟 텍스트의 언어학 특징을 생성한다. 상기 단계 601-603의 구체적인 구현 과정 및 원리는 상기 실시예의 설명을 참조할 수 있으므로, 여기서 반복하 지 않는다. 단계 604, 타겟 텍스트의 언어학 특징을 음성 합성 모델의 제1 인코더에 입력하여, 특징 인코딩을 획득한다. 특징 인코딩은 타겟 텍스트의 언어학 특징을 묘사할 수 있다. 단계 605, 스피커의 식별자를 음성 합성 모델의 제2 인코더에 입력하여, 스피커의 음색 인코딩을 획득한다. 본 개시의 실시예에서, 스피커는 대응되는 음색 특징을 구비하며, 상이한 스피커는 상이한 음색 특징을 구비하 고, 음색 인코딩은 스피커의 음색 특징을 묘사할 수 있다. 단계 606, 언어학 특징 및 스피커의 식별자를 음성 합성 모델의 스타일 네트워크에 입력하여 타겟 텍스트 및 스 피커에 대응되는 스타일 인코딩을 획득한다. 스타일 네트워크는 스피커가 타겟 텍스트를 강술할 때의 운율 정보, 즉 스피커가 타겟 텍스트를 강술할 때의 소 리의 고저기복과 리듬을 예측하는데 사용되며, 기본 주파수, 지속 시간 및 능력에 대한 거시적인 반영이다. 스 타일 인코딩은 스피커가 타겟 텍스트를 강술할 때의 운율 정보를 묘사할 수 있다. 단계 607, 스타일 인코딩, 특징 인코딩 및 음색 인코딩을 융합하여, 융합 인코딩을 획득한다. 단계 608, 음성 합성 모델의 디코더를 사용하여 융합 인코딩을 디코딩하여, 타겟 음성의 음향 스펙트럼을 획득 한다. 예시적인 실시예에서, 음성 합성 모델의 구조는 도 7에 나타낸다. 음성 합성 모델은 제1 인코더(Text Encoder), 제2 인코더(Speaker Encoder), 스타일 네트워크(TP Net), 디코더(Decoder)를 포함한다. 제1 인코더, 제2 인코 더 및 스타일 네트워크의 출력은 디코더의 입력에 연결된다. 음성 합성 모델의 입력은 텍스트의 언어학 특징 및 스피커 식별자일 수 있고, 출력은 음성의 음향 스펙트럼일 수 있다. 음향 스펙트럼은 예를 들어 멜(Mel) 스펙트 럼일 수 있다. 타겟 텍스트의 언어학 특징을 제1 인코더에 입력하여 타겟 텍스트의 특징 인코딩(Text Encoding)을 획득할 수 있고; 스피커의 식별자를 제2 인코더에 입력하여 스피커의 음색 인코딩(Speaker Encoding)을 획득할 수 있다. 스타일 네트워크는 스타일 인코더(Style Encoder) + 제1 컨볼루션 레이어(First Conv Layers) + 제2 컨볼루션 레이어(Second Conv Layers)일 수 있으며, 스피커의 식별자를 스타일 인코더에 입력하여, 스피커에 대응되는 스 타일 특징(Style Feature)을 획득할 수 있고, 타겟 텍스트의 언어학 특징을 제2 컨볼루션 레이어에 입력하여, 타겟 텍스트에 대응되는 언어학 특징 인코딩(TP Text Encoding)을 획득할 수 있다. 나아가, 스피커에 대응되는 스타일 특징 및 타겟 텍스트에 대응되는 언어학 특징 인코딩을 융합한 후, 융합된 인코딩을 제1 컨볼루션 레이 어에 입력하여, 타겟 텍스트 및 스피커에 대응되는 스타일 인코딩을 획득할 수 있다. 도 7에서, \" \"는 특징에 대해 융합 처리를 수행하는 것을 나타낸다. 스타일 인코딩, 특징 인코딩 및 음색 인코딩을 융합하여, 융합 인코딩을 획득할 수 있으며, 나아가 디코더를 사 용하여 융합 인코딩을 디코딩하여, 타겟 음성의 음향 스펙트럼을 획득할 수 있다. 본 개시의 실시예에서, 음성 합성 모델은 세밀한 입도의 운율을 기반한 음향 모델이며, 음성 합성 모델의 제1 인코더, 제2 인코더, 스타일 네트워크의 사용을 통해, 운율 정보, 텍스트의 언어학 특징 및 스피커의 음색 특징 을 각각 결합하여 음성을 합성하므로, 음성 합성을 수행할 경우, 운율 정보는 스피커 및 텍스트에 커플링되지 않고, 독특한 특징으로 사용되어, 스피커와 언어 사이의 커플링 정도를 낮추고, 한 가지 언어를 사용하는 스피 커에 대해, 여러가지 언어의 텍스트의 음성 합성을 수행하는 시나리오에서, 한 가지 운율 정보 만을 결합할 수 있고, 음성 합성을 위해 두 가지 운율 정보를 동시에 결합하는 것을 방지할 수 있으며, 음성 합성 효과를 향상 시키고, 합성된 타겟 음성의 복원 정도를 향상시킨다. 예시적인 실시예에서, 음성 합성 모델을 사용하여, 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라, 음 성 합성을 수행하기 전에, 사전 트레이닝을 통해 음성 합성 모델을 획득할 수 있다. 음성 합성 모델을 트레이닝 할 경우, 참조 네트워크를 설정하여, 음성 합성 모델의 제1 인코더, 제2 인코더, 디코더 및 참조 네트워크에 따 라 트레이닝 모델을 생성할 수 있다. 제1 인코더, 제2 인코더 및 참조 네트워크의 출력은 디코더의 입력에 연결 되어, 트레이닝 데이터를 사용하여 트레이닝 모델 및 스타일 네트워크를 트레이닝한 후, 트레이닝된 트레이닝 모델의 제1 인코더, 제2 인코더 및 디코더, 및 트레이닝된 스타일 네트워크에 따라, 음성 합성 모델을 생성한다. 참조 네트워크의 구조는 도 8을 참조할 수 있다. 도 8에 도시된 바와 같이, 참조 네트워크는 참조 인코더 (Reference Encoder) + 어텐션 메커니즘 모듈(Reference Attention)을 포함할 수 있다. 참조 인코더는 음성으 로부터 추출된 음향 스펙트럼을 인코딩하여 음향의 특징 인코딩을 획득할 수 있고, 음향의 특징 인코딩은 어텐 션 메커니즘 모듈에 입력되어, 어텐션 메커니즘 모듈을 통해 제1 인코더에 입력된 언어학 특징과 정렬되어, 운 율 정보를 획득할 수 있다. 트레이닝 데이터는 텍스트 샘플의 언어학 특징, 및 텍스트 샘플에 대응되는 음성 샘플 및 음성 샘플의 스피커 식별자를 포함할 수 있다. 설명해야 하는 바로는, 생성된 음성 합성 모델이 한 가지 언어의 스피커에 대해 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있도록, 트레이닝 데이터는 여러가지 언어의 텍스트 샘플 및 대응되는 음성 샘플을 포함해야 한다. 예를 들어, 생성된 음성 합성 모델이 중국어를 하는 스피커에 대해, 중국어, 영어 및 일본어 세 가지 언 어의 텍스트의 음성 합성을 구현할 수 있도록, 트레이닝 데이터는 중국어, 영어 및 일본어 세 가지 언어의 텍스 트 샘플 및 대응되는 음성 샘플을 포함해야 하며, 각 언어의 음성 샘플의 스피커 식별자는 상이할 수 있으며, 즉 트레이닝 데이터는 일 대 여러가지 언어의 트레이닝 말뭉치를 요구하지 않는다. 또한, 모델의 트레이닝 효과 를 향상시키기 위해, 각 언어의 음성 샘플의 스피커 수량은 사전 설정된 임계값 예컨대 5보다 클 수 있다. 또한, 일 대 여러가지 언어의 음성 합성을 구현하기 위해, 본 개시의 실시예에서 각 언어의 텍스트 샘플의 언어 학 특징에 대해 통일된 설계 및 인코딩을 수행한다. 트레이닝 데이터의 텍스트 샘플은 도 4에 도시된 형식으로 수동 라벨링될 수 있다. 예시적인 실시예에서, 트레이닝 데이터를 사용하여 트레이닝 모델 및 스타일 네트워크에 대해 트레이닝할 경우, 트레이닝 모델 및 스타일 네트워크가 동기적으로 트레이닝되는 방식을 사용할 수 있다. 구체적인 트레이닝 과정 은 다음과 같을 수 있다. 텍스트 샘플의 언어학 특징을 트레이닝 모델의 제1 인코더에 입력하고, 음성 샘플의 스피커 식별자를 트레이닝 모델의 제2 인코더에 입력하고; 음성 샘플을 트레이닝 모델의 참조 네트워크에 입력하며; 참조 네트워크의 출력, 제1 인코더의 출력 및 제2 인코더의 출력을 융합하고, 트레이닝 모델의 디코더를 사용하여 디코딩하여, 예측 음향 스펙트럼을 획득하며; 예측 음향 스펙트럼과 음성 샘플의 음향 스펙트럼 사이의 차이에 따라, 트레이 닝 모델에 대해 모델 파라미터 조정을 수행하고; 텍스트 샘플의 언어학 특징 및 음성 샘플의 스피커 식별자를 스타일 네트워크에 입력하며; 스타일 네트워크의 출력과 참조 네트워크의 출력 사이의 차이에 따라, 스타일 네 트워크에 대해 모델 파라미터 조정을 수행한다. 구체적으로, 하나 또는 복수의 텍스트 샘플의 언어학 특징, 텍스트 샘플에 대응되는 언어 샘플 및 음성 샘플의 스피커 식별자에 대해, 텍스트 샘플의 언어학 특징을 트레이닝 모델의 제1 인코더에 입력하여, 텍스트 샘플의 언어학 특징에 대응되는 특징 인코딩을 획득하고, 음성 샘플의 스피커 식별자를 트레이닝 모델의 제2 인코더에 입력하여, 스피커에 대응되는 음색 인코딩을 획득하고, 음성 샘플을 트레이닝 모델의 참조 네트워크에 입력하여, 음성 샘플의 운율 정보를 획득하며, 나아가 참조 네트워크에서 출력된 운율 정보, 제1 인코더에서 출 력된 특징 인코딩 및 제2 인코더에서 출력된 음색 인코딩을 융합하고, 디코더를 사용하여 융합된 특징에 대해 디코딩하여, 예측 음향 스펙트럼을 획득할 수 있다. 나아가, 예측 음향 스펙트럼 및 음성 샘플의 음향 스펙트럼 사이의 차이를 결합하여, 트레이닝 모델에 대해 모델 파라미터 조정을 수행한다. 텍스트 샘플의 언어학 특징을 트레이닝 모델의 제1 인코더에 입력하고, 음성 샘플의 스피커 식별자를 트레이닝 모델의 제2 인코더에 입력하는 동시에, 텍스트 샘플의 언어학 특징 및 음성 샘플의 스피커 식별자를 스타일 네트워크에 입력하여, 스타일 네트 워크에서 출력되는 스타일 인코딩을 획득하고, 스타일 네트워크에서 출력된 스타일 인코딩 및 참조 네트워크에 서 출력된 운율 정보 사이의 차이에 따라, 스타일 네트워크에 대해 모델 파라미터 조정을 수행할 수 있다. 따라서, 트레이닝 샘플에 포함된 복수의 텍스트 샘플의 언어학 특징, 텍스트 샘플에 대응되는 음성 샘플 및 음 성 샘플의 스피커 식별자에 따라, 트레이닝 모델 및 스타일 네트워크의 모델 파라미터를 지속적으로 조정하여, 트레이닝 모델 및 스타일 네트워크에 대해 질대하여, 트레이닝 모델 및 스타일 네트워크의 출력 결과의 정확성 이 사전 설정된 임계값을 만족할 때까지 트레이닝하며, 트레이닝이 종료된 후, 트레이닝된 트레이닝 모델 및 스 타일 네트워크를 획득한다. 트레이닝 모델 및 스타일 네트워크에 대해 트레이닝한 후, 트레이닝된 트레이닝 모 델의 제1 인코더, 제2 인코더, 디코더 및 트레이닝된 스타일 네트워크에 따라 음성 합성 모델을 생성할 수 있다. 제1 인코더, 제2 인코더, 디코더, 참조 네트워크로 구성된 트레이닝 모델 및 스타일 네트워크를 동기적으로 트 레이닝하는 것을 통해, 트레이닝이 종료된 후, 제1 인코더, 제2 인코더, 디코더 및 스타일 네트워크에 따라 음 성 합성 모델을 생성하며, 즉 모델 트레이닝을 수행할 경우, 입력으로서 음성 샘플인 참조 네트워크를 결합하여 트레이닝하되, 트레이닝 후 참조 네트워크를 더 이상 필요하지 않고, 트레이닝된 음성 합성 모델을 사용하여 음 성 합성을 수행할 경우, 음성 입력에 대해 의존할 필요가 없으므로, 트레이닝 모델 및 스타일 네트워크를 동기 적으로 트레이닝하는 방식으로 모델의 트레이닝 효율을 향상시킬 수 있다. 종합하면, 본 개시의 실시예의 음성 합성 방법은, 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득하 고, 타겟 텍스트에 포함된 적어도 하나의 캐릭터에 대해, 적어도 하나의 캐릭터의 발음 정보를 획득하며, 타겟텍스트가 속하는 타겟 언어에 따라, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 대해 특징 추출을 수 행하여, 타겟 텍스트의 언어학 특징을 생성하며, 타겟 텍스트의 언어학 특징을 음성 합성 모델의 제1 인코더에 입력하여, 특징 인코딩을 획득하고, 스피커의 식별자를 음성 합성 모델의 제2 인코더에 입력하여, 스피커의 음 색 인코딩을 획득하고, 언어학 특징 및 스피커의 식별자를 음성 합성 모델의 스타일 네트워크에 입력하여 타겟 텍스트 및 스피커에 대응되는 스타일 인코딩을 획득하며, 스타일 인코딩, 특징 인코딩 및 음색 인코딩을 융합하 여, 융합 인코딩을 획득하고, 음성 합성 모델의 디코더를 사용하여 융합 인코딩을 디코딩하여, 타겟 음성의 음 향 스펙트럼을 획득하며, 한 가지 언어의 스피커에 대해, 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있 고, 음성 합성 효과를 향상시키고, 합성된 타겟 음성의 복원 정도를 향상시킨다. 이하, 도 9를 결합하여 본 개시에서 제공되는 음성 합성 장치에 대해 설명한다. 도 9는 본 개시의 제4 실시예에 따른 음성 합성 장치의 개략적인 구조도이다. 도 9에 도시된 바와 같이, 본 개시에서 제공되는 음성 합성 장치는 제1 획득 모듈, 제2 획득 모듈 , 추출 모듈 및 합성 모듈을 포함한다. 제1 획득 모듈은 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획득하는데 사용되고; 제2 획득 모듈은 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하는데 사용되고; 추출 모듈은 타겟 텍스트가 속하는 타겟 언어에 따라, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보 에 대해 특징 추출을 수행하여, 타겟 텍스트의 언어학 특징을 생성하는데 사용되고; 합성 모듈은 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라, 음성 합성을 수행하여 타겟 음성을 획득하는데 사용된다. 설명해야 하는 바로는, 본 실시예에서 제공되는 음성 합성 장치는 전술한 실시예의 음성 합성 방법을 수행할 수 있다. 음성 합성 장치는 전자 기기일 수 있고, 전자 기기에 구성된 소프트웨어일 수도 있으며, 한 가지 언어의 스피커에 대해, 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있다. 전자 기기는 데이터 처리가 가능한 임의의 정적 또는 모바일 컴퓨팅 기기일 수 있으며, 예컨대 랩톱 컴퓨터, 스 마트폰, 웨어러블 기기와 같은 모바일 컴퓨팅 기기, 또는 데스크톱 컴퓨터와 같은 정적 컴퓨팅 기기, 또는 서버, 또는 기타 유형의 컴퓨팅 기기 등일 수 있으나, 본 개시에서 이에 대해 한정하지 않는다. 설명해야 하는 바로는, 전술한 음성 합성 방법의 실시예에 대한 설명은 본 개시에서 제공되는 음성 합성 장치에 도 적용되므로, 여기서 반복하지 않는다. 본 개시의 실시예에서 제공되는 음성 합성 장치는 먼저 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획 득하고, 다음 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하여, 타겟 텍스트가 속하는 타겟 언어 에 따라, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 대해 특징 추출을 수행하여, 타겟 텍스트의 언어 학 특징을 생성하고, 나아가 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라, 음성 합성을 수행하여 타 겟 음성을 획득한다. 따라서, 합성하고자 하는 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라 언어 합 성을 수행하여, 한 가지 언어의 스피커에 대해, 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있다. 이하, 도 10을 결합하여 본 개시에서 제공되는 음성 합성 장치에 대해 설명한다. 도 10은 본 개시의 제5 실시예에 따른 음성 합성 장치의 개략적인 구조도이다. 도 10에 도시된 바와 같이, 음성 합성 장치는 구체적으로 제1 획득 모듈, 제2 획득 모듈 ), 추출 모듈 및 합성 모듈을 포함할 수 있다. 도 10에서 도시된 제1 획득 모듈, 제2 획득 모듈 , 추출 모듈 및 합성 모듈은 도 9에서 도시된 제1 획득 모듈, 제2 획득 모듈, 추 출 모듈 및 합성 모듈과 같은 기능과 구조를 구비한다. 예시적인 실시예에서, 추출 모듈은, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 따라, 적어도 하나의 캐릭터에 포함된 음소, 및 음소로 조 합된 음절 또는 단어에 대응되는 음조를 결정하는 제1 결정 유닛; 타겟 텍스트가 속하는 타겟 언어 유형에 따라, 음소에 접미사를 추가하고, 음조의 음조 인코딩을 결정하는 제2 결정 유닛; 및접미사가 추가된 음소 및 음조 인코딩, 및 음절에서의 음소의 위치 및 단어에서의 음절의 위치 중의 적어도 하 나에 따라, 언어학 특징 중 대응되는 특징 항목을 생성하는 제1 생성 유닛;을 포함한다. 예시적인 실시예에서, 제1 결정 유닛은, 타겟 텍스트 중 적어도 하나의 캐릭터에 대해, 캐릭터의 발음 정보 중의 성조, 악센트 및 얼화음 중의 하나 또 는 복수의 조합에 따라, 음소로 조합된 음절 또는 단어에 대응되는 음조를 결정하는 결정 서브유닛을 포함한다. 예시적인 실시예에서, 추출 모듈은, 타겟 텍스트가 속하는 타겟 언어에 따라, 타겟 텍스트를 단어로 나누고, 각 분사 어휘에 대응되는 운율을 결정 하는 제3 결정 유닛; 및 각 분사 어휘에 대응되는 운율에 따라, 언어학 특징 중 대응되는 특징 항목을 생성하는 제2 생성 유닛; 을 포함한다. 예시적인 실시예에서, 합성 모듈은, 타겟 텍스트의 언어학 특징을 음성 합성 모델의 제1 인코더에 입력하여, 특징 인코딩을 획득하는 제1 인코딩 유 닛; 스피커의 식별자를 음성 합성 모델의 제2 인코더에 입력하여, 스피커의 음색 인코딩을 획득하는 제2 인코딩 유 닛; 언어학 특징 및 스피커의 식별자를 음성 합성 모델의 스타일 네트워크에 입력하여 타겟 텍스트 및 스피커에 대 응되는 스타일 인코딩을 획득하는 제3 인코딩 유닛; 스타일 인코딩, 특징 인코딩 및 음색 인코딩을 융합하여, 융합 인코딩을 획득하는 융합 유닛; 및 음성 합성 모델의 디코더를 사용하여 융합 인코딩을 디코딩하여, 타겟 음성의 음향 스펙트럼을 획득하는 디코딩 유닛;을 포함한다. 예시적인 실시예에서, 음성 합성 장치는, 음성 합성 모델의 제1 인코더, 제2 인코더, 디코더 및 참조 네트워크에 따라, 트레이닝 모델을 생성하는 제1 생 성 모듈 - 제1 인코더, 제2 인코더 및 참조 네트워크의 출력은 디코더의 입력에 연결됨 -; 트레이닝 데이터를 사용하여, 트레이닝 모델 및 스타일 네트워크에 대해 트레이닝하는 트레이닝 모듈; 및 트레이닝된 트레이닝 모델의 제1 인코더, 제2 인코더 및 디코더, 및 트레이닝된 스타일 네트워크에 따라, 음성 합성 모델을 생성하는 제2 생성 모듈;을 더 포함할 수 있다. 예시적인 실시예에서, 트레이닝 데이터는 텍스트 샘플의 언어학 특징, 및 텍스트 샘플에 대응되는 음성 샘플 및 음성 샘플의 스피커 식별자를 포함하며; 트레이닝 모듈은, 텍스트 샘플의 언어학 특징을 트레이닝 모델의 제1 인코더에 입력하고, 음성 샘플의 스피커 식별자를 트레이닝 모델의 제2 인코더에 입력하는 제1 처리 유닛; 음성 샘플을 트레이닝 모델의 참조 네트워크에 입력하는 제2 처리 유닛; 참조 네트워크의 출력, 제1 인코더의 출력 및 제2 인코더의 출력을 융합하고, 트레이닝 모델의 디코더를 사용하 여 디코딩하여, 예측 음향 스펙트럼을 획득하는 제3 처리 유닛; 예측 음향 스펙트럼과 음성 샘플의 음향 스펙트럼 사이의 차이에 따라, 트레이닝 모델에 대해 모델 파라미터 조 정을 수행하는 제1 조정 유닛; 텍스트 샘플의 언어학 특징 및 음성 샘플의 스피커 식별자를 스타일 네트워크에 입력하는 제4 처리 유닛; 및 스타일 네트워크의 출력과 참조 네트워크의 출력 사이의 차이에 따라, 스타일 네트워크에 대해 모델 파라미터 조정을 수행하는 제2 조정 유닛;을 포함한다. 설명해야 하는 바로는, 전술한 음성 합성 방법의 실시예에 대한 설명은 본 개시에서 제공되는 음성 합성 장치에 도 적용되므로, 여기서 반복하지 않는다. 본 개시의 실시예에서 제공되는 음성 합성 장치는 먼저 합성하고자 하는 타겟 텍스트, 및 스피커의 식별자를 획 득하고, 다음 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보를 획득하여, 타겟 텍스트가 속하는 타겟 언어 에 따라, 타겟 텍스트 중 적어도 하나의 캐릭터의 발음 정보에 대해 특징 추출을 수행하여, 타겟 텍스트의 언어 학 특징을 생성하고, 나아가 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라, 음성 합성을 수행하여 타 겟 음성을 획득한다. 따라서, 합성하고자 하는 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따라 언어 합 성을 수행하여, 한 가지 언어의 스피커에 대해, 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있다. 본 개시의 실시예에 따르면, 본 개시는 또한 전자 기기, 판독 가능 저장 매체 및 컴퓨터 프로그램을 제공한다. 도 11은 본 개시의 실시예를 실시하기 위한 예시적인 전자 기기의 개략적인 블록도이다. 전자 기기는 랩 톱 컴퓨터, 데스크톱 컴퓨터, 워크 스테이션, 개인용 디지털 비서, 서버, 블레이드 서버, 메인 프레임워크 컴퓨 터 및 기타 적합한 컴퓨터와 같은 다양한 형태의 디지털 컴퓨터를 나타내기 위한 것이다. 전자 기기는 또한 개 인용 디지털 처리, 셀룰러 폰, 스마트 폰, 웨어러블 기기 및 기타 유사한 컴퓨팅 장치와 같은 다양한 형태의 모 바일 장치를 나타낼 수도 있다. 본 명세서에서 제시된 구성 요소, 이들의 연결 및 관계, 또한 이들의 기능은 단 지 예일 뿐이며 본문에서 설명되거나 및/또는 요구되는 본 개시의 구현을 제한하려는 의도가 아니다. 도 11에 도시된 바와 같이, 기기는 컴퓨팅 유닛을 포함하며, 읽기 전용 메모리(ROM)에 저장 된 컴퓨터 프로그램에 의해 또는 저장 유닛으로부터 랜덤 액세스 메모리(RAM)에 로딩된 컴퓨터 프 로그램에 의해 수행되어 각종 적절한 동작 및 처리를 수행할 수 있다. RAM에, 또한 기기가 오퍼레 이션을 수행하기 위해 필요한 각종 프로그램 및 데이터가 저장되어 있다. 컴퓨팅 유닛, ROM 및 RAM은 버스를 통해 서로 연결되어 있다. 입력/출력(I/O) 인터페이스도 버스에 연결되 어 있다. 키보드, 마우스 등과 같은 입력 유닛; 각종 유형의 모니터, 스피커 등과 같은 출력 유닛; 자기 디 스크, 광 디스크 등과 같은 저장 유닛; 및 네트워크 카드, 모뎀, 무선 통신 트랜시버 등과 같은 통신 유 닛을 포함하는 기기 중의 복수의 부품은 I/O 인터페이스에 연결된다. 통신 유닛은 장 치가 인터넷과 같은 컴퓨터 네트워크 및/또는 다양한 통신 네트워크를 통해 다른 기기와 정보/데이터를 교환하는 것을 허락한다. 컴퓨팅 유닛은 프로세싱 및 컴퓨팅 능력을 구비한 다양한 범용 및/또는 전용 프로세싱 컴포넌트일 수 있 다. 컴퓨팅 유닛의 일부 예시는 중앙 처리 유닛(CPU), 그래픽 처리 유닛(GPU), 다양한 전용 인공 지능 (AI) 컴퓨팅 칩, 기계 러닝 모델 알고리즘을 수행하는 다양한 컴퓨팅 유닛, 디지털 신호 처리기(DSP), 및 임의 의 적절한 프로세서, 컨트롤러, 마이크로 컨트롤러 등을 포함하지만, 이에 제한되지 않는다. 컴퓨팅 유닛(110 1)은 예를 들어 음성 합성 방법과 같은 윗글에서 설명된 각각의 방법 및 처리를 수행한다. 예를 들어, 일부 실 시예에서, 음성 합성 방법은 저장 유닛과 같은 기계 판독 가능 매체에 유형적으로 포함되어 있는 컴퓨터 소프트웨어 프로그램으로 구현될 수 있다. 일부 실시예에서, 컴퓨터 프로그램의 일부 또는 전부는 ROM 및 /또는 통신 유닛을 통해 기기에 로드 및/또는 설치될 수 있다. 컴퓨터 프로그램이 RAM에 로 딩되고 컴퓨팅 유닛에 의해 수행되는 경우, 전술한 음성 합성 방법의 하나 또는 복수의 단계를 수행할 수 있다. 대안적으로, 다른 실시예에서, 컴퓨팅 유닛은 임의의 다른 적절한 방식을 통해(예를 들어, 펌웨어 에 의해) 구성되어 음성 합성 방법을 수행하도록 한다. 여기서 설명되는 시스템 및 기술의 다양한 실시 방식은 디지털 전자 회로 시스템, 집적 회로 시스템, 필드 프로 그래머블 게이트 어레이(FPGA), 주문형 집적 회로(ASIC), 특정 용도 표준 제품(ASSP), 시스템온칩(SOC), 복합 프로그래머블 논리 소자(CPLD), 컴퓨터 하드웨어, 펌웨어, 소프트웨어 및 이들의 조합 중의 적어도 하나로 구현 될 수 있다. 이러한 다양한 실시 방식은 하나 또는 복수의 컴퓨터 프로그램에서의 구현을 포함할 수 있으며, 당 해 하나 또는 복수의 컴퓨터 프로그램은 적어도 하나의 프로그램 가능 프로세서를 포함하는 프로그램 가능 시스 템에서 수행 및/또는 해석될 수 있고, 당해 프로그램 가능 프로세서는 전용 또는 일반용일 수 있고, 저장 시스 템, 적어도 하나의 입력 장치 및 적어도 하나의 출력 장치로부터 데이터 및 명령을 수신하고 또한 데이터 및 명 령을 당해 저장 시스템, 당해 적어도 하나의 입력 장치 및 당해 적어도 하나의 출력 장치에 전송할 수 있다. 본 개시의 방법을 구현하기 위해 사용되는 프로그램 코드는 하나 또는 복수의 프로그래밍 언어의 임의의 조합으 로 작성될 수 있다. 이러한 프로그램 코드는 범용 컴퓨터, 전용 컴퓨터 또는 기타 프로그래머블 데이터 처리 장 치의 프로세서 또는 컨트롤러에 제공될 수 있으므로, 프로그램 코드가 프로세서 또는 컨트롤러에 의해 수행되는경우, 흐름도 및/또는 블록도에서 규정한 기능/조작을 구현하도록 한다. 프로그램 코드는 전체적으로 기계에서 수행되거나, 부분적으로 기계에서 수행되거나, 독립 소프트웨어 패키지로서 부분적으로 기계에서 수행되고 부분 적으로 원격 기계에서 수행되거나 또는 전체적으로 원격 기계 또는 서버에서 수행될 수 있다. 본 개시의 문맥에서, 기계 판독 가능 매체는 자연어 수행 시스템, 장치 또는 기기에 의해 사용되거나 자연어 수 행 시스템, 장치 또는 기기와 결합하여 사용되는 프로그램을 포함하거나 저장할 수 있는 유형의 매체일 수 있다. 기계 판독 가능 매체는 기계 판독 가능 신호 매체 또는 기계 판독 가능 저장 매체일 수 있다. 기계 판독 가능 매체는 전자, 자기, 광학, 전자기, 적외선 또는 반도체 시스템, 장치 또는 기기, 또는 상기 내용의 임의의 적절한 조합을 포함할 수 있지만 이에 제한되지 않는다. 기계 판독 가능 저장 매체의 더 구체적인 예시는 하나 또는 복수의 전선을 기반하는 전기 연결, 휴대용 컴퓨터 디스크, 하드 디스크, 랜덤 액세스 메모리(RAM), 읽기 전용 메모리(ROM), 지울 수 있는 프로그래머블 읽기 전용 메모리(EPROM 또는 플래시 메모리), 광섬유, 휴대용 컴팩트 디스크 읽기 전용 메모리(CD-ROM), 광학 저장 기기, 자기 저장 기기 또는 상기 내용의 임의의 적절한 조 합을 포함할 수 있지만 이에 제한되지 않는다. 사용자와의 인터랙션을 제공하기 위해 여기에 설명된 시스템 및 기술은 컴퓨터에서 실시될 수 있다. 당해 컴퓨 터는 사용자에게 정보를 디스플레이하기 위한 디스플레이 장치(예를 들어, CRT(음극선관) 또는 LCD(액정 디스플 레이) 모니터); 및 키보드 및 포인팅 장치(예를 들어, 마우스 또는 트랙볼)를 구비하며, 사용자는 당해 키보드 및 당해 포인팅 장치를 통해 컴퓨터에 입력을 제공할 수 있다. 다른 유형의 장치를 사용하여 사용자와의 인터랙 션을 제공할 수도 있으며, 예를 들어, 사용자에게 제공되는 피드백은 임의의 형태의 감지 피드백(예를 들어, 시 각적 피드백, 청각적 피드백 또는 촉각적 피드백)일 수 있고; 임의의 형태(소리 입력, 음성 입력 또는 촉각 입 력을 포함)로 사용자로부터의 입력을 수신할 수 있다. 여기서 설명된 시스템 및 기술은 백엔드 부품을 포함하는 컴퓨팅 시스템(예를 들어, 데이터 서버로서), 또는 미 들웨어 부품을 포함하는 컴퓨팅 시스템(예를 들어, 응용 서버), 또는 프런트 엔드 부품을 포함하는 컴퓨팅 시스 템(예를 들어, 그래픽 사용자 인터페이스 또는 네트워크 브라우저를 구비하는 사용자 컴퓨터인 바, 사용자는 당 해 그래픽 사용자 인터페이스 또는 네트워크 브라우저를 통해 여기서 설명된 시스템 및 기술의 실시 방식과 인 터랙션할 수 있음), 또는 이러한 백엔드 부품, 미들웨어 부품 또는 프런트 엔드 부품의 임의의 조합을 포한하는 컴퓨팅 시스템에서 실시될 수 있다. 시스템의 부품은 임의의 형태 또는 매체의 디지털 데이터 통신(예를 들어, 통신 네트워크)을 통해 서로 연결될 수 있다. 통신 네트워크의 예시는 근거리 통신망(LAN), 광역 통신망(WAN), 인터넷 및 블록체인 네트워크를 포함한다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트 및 서버는 일반적으로 서로 멀리 떨어져 있 고, 통신 네트워크를 통해 인터랙션한다. 서로 클라이언트-서버 관계를 가지는 컴퓨터 프로그램을 대응되는 컴 퓨터에서 수행하여 클라이언트와 서버 간의 관계를 생성한다. 서버는 클라우드 컴퓨팅 서버 또는 클라우드 호스 트라고도 하는 클라우드 서버일 수 있고, 클라우드 컴퓨팅 서비스 시스템 중의 일종의 호스트 제품이고, 기존의 물리적 호스트 및 VPS 서비스(Virtual Private Server, 또는 VPS로 약칭함)에 존재하고 있는 관리가 어렵고 비 즈니스 확장이 약한 결점을 해결하기 위한 것이다. 서버는 또한 분산 시스템의 서버, 또는 블록체인을 결합한 서버일 수 있다. 본 개시는 컴퓨터 기술 분야에 관한 것으로, 특히 딥러닝, 음성 기술과 같은 인공지능 기술 분야에 관한 것이다. 설명해야 하는 바로는, 인공지능은 인간의 특정 사유 과정 및 지능 행위(예컨대, 러닝, 추리, 사고, 계획 등)를 컴퓨터로 시뮬레이션하기 위해 연구하는 학과이며, 하드웨어 층면의 기술 뿐만 아니라 소프트웨어 층면의 기술 도 포함한다. 인공지능 하드웨어 기술은 일반적으로 센서, 전용 인공지능 칩, 클라우드 컴퓨팅, 분산 저장, 빅 데이터 처리 등과 같은 기술을 포함하고; 인공지능 소프트웨어 기술은 주로 컴퓨터 시각, 음성 합성 기술, 자연 어 처리 기술 및 기계 학습/딥러닝, 빅데이터 처리 기술, 지식 그래프 기술 등 몇 가지 주요 방향을 포함한다. 본 개시의 실시예의 기술 수단에 따르면, 합성하고자 하는 타겟 텍스트의 언어학 특징 및 스피커의 식별자에 따 라 언어 합성을 수행하여, 한 가지 언어의 스피커에 대해, 여러가지 언어의 텍스트의 음성 합성을 구현할 수 있 다. 이해 가능한 바로는, 전술한 다양한 형식의 프로세스에 있어서 단계 재정렬, 추가 또는 삭제를 할 수 있다. 예 를 들어, 본 개시에 개시된 기술 솔루션이 이루고자 하는 결과를 구현할 수 있는 한, 본 개시에 기재된 각 단계 들은 병렬로, 순차적으로 또는 다른 순서로 수행될 수 있으나, 본 명세서에서 이에 대해 한정하지 않는다. 전술한 구체적인 실시 방식들은 본 개시의 보호 범위에 대한 한정을 구성하지 않는다. 당업자라면 본 개시의 설 계 요건 및 기타 요인에 따라 다양한 수정, 조합, 서비스 조합 및 대체가 이루어질 수 있음을 이해해야 한다. 본 개시의 정신과 원칙 내에서 이루어진 모든 수정, 동등한 대체 및 개선은 본 개시의 보호 범위에 포함된다."}
{"patent_id": "10-2022-0067710", "section": "도면", "subsection": "도면설명", "item": 1, "content": "첨부된 도면은 본 개시의 수단을 더 잘 이해하기 위한 것으로, 본 개시에 대한 한정이 구성되지 않는다. 도1은 본 개시의 제1 실시예에 따른 음성 합성 방법의 개략적인 흐름도이다. 도2는 본 개시의 제2 실시예에 따른 음성 합성 방법의 개략적인 흐름도이다. 도3은 본 개시의 제2 실시예에 따른 일본어 텍스트의 각 성조 유형의 예시도이다. 도4는 본 개시의 제2 실시예에 따른 타겟 텍스트 중 각 캐릭터의 발음 정보 및 각 분사 어휘에 대응되는 운율의 예시도이다. 도5는 본 개시의 제2 실시예에 따른 언어학 특징 중 대응되는 특징 항목의 예시도이다. 도6은 본 개시의 제3 실시예에 따른 음성 합성 방법의 개략적인 흐름도이다. 도7은 본 개시의 제3 실시예에 따른 음성 합성 모델의 개략적인 구조도이다. 도8은 본 개시의 제3 실시예에 따른 트레이닝 모델의 스타일 네트워크의 개략적인 구조도이다. 도9는 본 개시의 제4 실시예에 따른 음성 합성 장치의 개략적인 구조도이다. 도10은 본 개시의 제5 실시예에 따른 음성 합성 장치의 개략적인 구조도이다. 도11은 본 개시의 실시예의 음성 합성 방법을 구현하기 위한 전자 기기의 블록도이다."}
