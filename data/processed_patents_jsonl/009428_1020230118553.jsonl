{"patent_id": "10-2023-0118553", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0130588", "출원번호": "10-2023-0118553", "발명의 명칭": "강화학습 기반으로 개인화된 추천을 제공하는 방법 및 서버", "출원인": "삼성전자주식회사", "발명자": "제라시 제임스 러셀"}}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "강화학습 기반으로 개인화된 추천 세션을 제공하는 방법에 있어서,사용자 데이터를 획득하는 단계;상기 사용자 데이터를 기초로, 추천을 제공 받는 실제 사용자에 대응하는 가상의 사용자를 나타내는 시뮬레이션된 사용자를 생성하는 단계;상기 시뮬레이션된 사용자의 상태에 기초하여 액션을 결정하되, 상기 액션은 상기 사용자에게 제공될 상기 추천세션에 포함되는 추천 요소를 결정하는 것인, 단계;상기 시뮬레이션된 사용자의 상태를 업데이트하는 단계;상기 시뮬레이션된 사용자의 업데이트된 상태 및 상기 시뮬레이션된 사용자로부터 출력되는 상기 추천 요소에대한 리워드를 식별하는 단계;상기 시뮬레이션된 사용자의 상기 업데이트된 상태 및 상기 리워드에 기초하여 상기 추천 요소의 결정을 반복하여 개인화된 추천 세션을 생성하는 단계; 및상기 개인화된 추천 세션을 출력하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 시뮬레이션된 사용자의 상태는,사용자 설명(description), 사용자 선호도, 사용자 내부 현황, 및 추천 이력 중 적어도 하나를 포함하는 것인,방법."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 내지 제2항 중 어느 한 항에 있어서,상기 시뮬레이션된 사용자를 생성하는 단계는,제1 사용자로부터 상기 사용자 데이터를 수신하는 단계;상기 사용자 데이터를 기초로 제2 사용자들을 클러스터하는 단계; 및상기 제2 사용자들의 클러스터에 대응하는 시뮬레이션된 사용자들에 기초하여, 상기 제1 사용자의 시뮬레이션된사용자를 생성하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 시뮬레이션된 사용자를 생성하는 단계는,상기 제2 사용자들의 수를 식별하는 단계; 및상기 제2 사용자들의 수가 기 설정된 값 미만인 것에 기초하여, 상기 사용자 데이터를 기초로 상기 제1 사용자의 시뮬레이션된 사용자에 포함되는 파라미터들을 임의로 설정하는 단계를 포함하는 방법.공개특허 10-2024-0130588-3-청구항 5 제1항 내지 제2항 중 어느 한 항에 있어서,상기 방법은,상기 개인화된 추천 세션에 대한 사용자 피드백을 획득하는 단계; 및상기 사용자 피드백에 기초하여, 상기 시뮬레이션된 사용자를 재훈련시키는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 방법은,상기 개인화된 추천 세션의 생성을 반복하여 복수의 개인화된 추천 세션들로 구성되는 추천 세션 그룹을 생성하는 단계를 더 포함하고,상기 시뮬레이션된 사용자를 재훈련시키는 단계는, 상기 복수의 개인화된 추천 세션들 각각이 생성될 때마다 수행되는 것인, 방법."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 내지 제2항 중 어느 한 항에 있어서,상기 시뮬레이션된 사용자를 생성하는 단계는,미리 훈련된 생성형 인공지능인 사용자 시뮬레이터를 이용하여 상기 시뮬레이션된 사용자를 생성하는 것인, 방법."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 사용자 시뮬레이터는, 실제 사용자들의 피드백 데이터에 기초하여 실제 사용자들과 유사한 시뮬레이션된피드백을 생성하도록 훈련된 것인, 방법."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 개인화된 추천 세션을 제공하는 강화학습 모델은, 상기 시뮬레이션된 피드백을 이용하여 미리 훈련된것인, 방법."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,상기 개인화된 추천 세션은, 운동에 관련된 무브먼트 추천 요소들을 포함하는 운동 추천 세션인 것인, 방법."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2024-0130588-4-강화학습 기반으로 개인화된 추천 세션을 제공하는 서버에 있어서,통신 인터페이스;하나 이상의 인스트럭션을 저장하는 메모리;상기 메모리에 저장된 상기 하나 이상의 인스트럭션을 실행하는 하나 이상의 프로세서를 포함하고,상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써,사용자 데이터를 획득하고,상기 사용자 데이터를 기초로, 추천을 제공 받는 실제 사용자에 대응하는 가상의 사용자를 나타내는 시뮬레이션된 사용자를 생성하고,상기 시뮬레이션된 사용자의 상태에 기초하여 액션을 결정하되, 상기 액션은 상기 사용자에게 제공될 상기 추천세션에 포함되는 추천 요소를 결정하는 것이고,상기 시뮬레이션된 사용자의 상태를 업데이트하고,상기 시뮬레이션된 사용자의 업데이트된 상태 및 상기 시뮬레이션된 사용자로부터 출력되는 상기 추천 요소에대한 리워드를 식별하고,상기 시뮬레이션된 사용자의 상기 업데이트된 상태 및 상기 리워드에 기초하여 상기 추천 요소의 결정을 반복하여 개인화된 추천 세션을 생성하고,상기 개인화된 추천 세션을 출력하는, 서버."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 시뮬레이션된 사용자의 상태는,사용자 설명(Description), 사용자 선호도, 사용자 내부 현황, 및 추천 이력 중 적어도 하나를 포함하는 것인,서버."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항 내지 제12항 중 어느 한 항에 있어서,상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써,제1 사용자로부터 상기 사용자 데이터를 수신하고,상기 사용자 데이터를 기초로 제2 사용자들을 클러스터하고,상기 제2 사용자들의 클러스터에 대응하는 시뮬레이션된 사용자들에 기초하여, 상기 제1 사용자의 시뮬레이션된사용자를 생성하는, 서버."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써,상기 제2 사용자들의 수를 식별하고,상기 제2 사용자들의 수가 기 설정된 값 미만인 것에 기초하여, 상기 사용자 데이터를 기초로 상기 제1 사용자의 시뮬레이션된 사용자에 포함되는 파라미터들을 임의로 설정하는, 서버.공개특허 10-2024-0130588-5-청구항 15 제11항 내지 제12항 중 어느 한 항에 있어서,상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써,상기 개인화된 추천 세션에 대한 사용자 피드백을 획득하고,상기 사용자 피드백에 기초하여, 상기 시뮬레이션된 사용자를 재훈련시키는, 서버."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써,상기 개인화된 추천 세션의 생성을 반복하여 복수의 개인화된 추천 세션들로 구성되는 추천 세션 그룹을 생성하되, 상기 시뮬레이션된 사용자의 재훈련이 상기 복수의 개인화된 추천 세션들 각각이 생성될 때마다 수행되는것인, 서버."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항 내지 제12항 중 어느 한 항에 있어서,상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써,미리 훈련된 생성형 인공지능인 사용자 시뮬레이터를 이용하여 상기 시뮬레이션된 사용자를 생성하는, 서버."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 사용자 시뮬레이터는, 실제 사용자들의 피드백 데이터에 기초하여 실제 사용자들과 유사한 시뮬레이션된피드백을 생성하도록 훈련된 것인, 서버."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 개인화된 추천 세션을 제공하는 강화학습 모델은, 상기 시뮬레이션된 피드백을 이용하여 미리 훈련된것인, 서버."}
{"patent_id": "10-2023-0118553", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제1항 내지 제10항 중 어느 한 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수있는 기록매체."}
{"patent_id": "10-2023-0118553", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "강화학습 기반으로 개인화된 추천을 제공하는 방법이 제공된다. 상기 방법은, 사용자 데이터를 획득하는 단계; 상기 사용자 데이터를 기초로, 추천을 제공 받는 실제 사용자에 대응하는 가상의 사용자를 나타내는 시뮬레이션 된 사용자를 생성하는 단계; 상기 시뮬레이션된 사용자의 상태에 기초하여 액션을 결정하되, 상기 액션은 상기 사용자에게 제공될 상기 추천 세션에 포함되는 추천 요소를 결정하는 것인, 단계; 상기 시뮬레이션된 사용자의 상태를 업데이트하는 단계; 상기 시뮬레이션된 사용자의 업데이트된 상태 및 상기 시뮬레이션된 사용자로부터 출 력되는 상기 추천 요소에 대한 리워드를 식별하는 단계; 상기 시뮬레이션된 사용자의 상기 업데이트된 상태 및 상기 리워드에 기초하여 상기 추천 요소의 결정을 반복하여 개인화된 추천 세션을 생성하는 단계; 및 상기 개인 화된 추천 세션을 출력하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0118553", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는, 사용자 시뮬레이터를 이용하는 강화학습을 기반으로 사용자에게 개인화된 추천을 제공하는 방법 및 서버, 전자 장치에 관한 것이다."}
{"patent_id": "10-2023-0118553", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "강화학습은 에이전트가 환경과 상호작용하며 보상을 최대화하는 액션을 학습하는 머신 러닝 분야이다. 강화학습 을 추천 시스템에 적용하면, 강화학습 모델의 에이전트가 사용자와 상호작용하며 행동하는 과정에서 얻는 리워 드를 최대화하는 추천 전략을 학습할 수 있다. 한편, 일반적인 머신 러닝 기법들과 마찬가지로 강화학습에서도 데이터 부족 문제는 중요한 고려사항 중 하나이다. 일부 강화학습 알고리즘에서는, 데이터 부족 문제를 해결하 기 위해 전이 학습(Transfer Learning)이나 사전 학습(Pre-training)과 같은 기법이 활용되기도 한다. 본 개시 는 상기한 것에 착안하여, 강화학습의 데이터 부족 문제를 극복하면서도 정밀하게 강화학습을 훈련시키고 활용 하는 기법을 제시한다."}
{"patent_id": "10-2023-0118553", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면에 따르면, 서버가 강화학습 기반으로 개인화된 추천 세션을 제공하는 방법이 제공될 수 있다. 상기 방법은, 사용자 데이터를 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 사용자 데이터를 기 초로, 추천을 제공 받는 실제 사용자에 대응하는 가상의 사용자를 나타내는 시뮬레이션된 사용자를 생성하는 단 계를 포함할 수 있다. 상기 방법은, 상기 시뮬레이션된 사용자의 상태에 기초하여 액션을 결정하되, 상기 액션 은 상기 사용자에게 제공될 상기 추천 세션에 포함되는 추천 요소를 결정하는 것인, 단계를 포함할 수 있다. 상 기 방법은, 상기 시뮬레이션된 사용자의 상태를 업데이트하는 단계를 포함할 수 있다. 상기 방법은, 상기 시뮬 레이션된 사용자의 업데이트된 상태 및 상기 시뮬레이션된 사용자로부터 출력되는 상기 추천 요소에 대한 리워 드를 식별하는 단계를 포함할 수 있다. 상기 방법은, 상기 시뮬레이션된 사용자의 상기 업데이트된 상태 및 상 기 리워드에 기초하여 상기 추천 요소의 결정을 반복하여 개인화된 추천 세션을 생성하는 단계를 포함할 수 있 다. 상기 방법은, 상기 개인화된 추천 세션을 출력하는 단계를 포함할 수 있다. 본 개시의 일 측면에 따르면, 강화학습 기반으로 개인화된 추천 세션을 제공하는 서버가 제공될 수 있다. 상기 서버는, 통신 인터페이스; 하나 이상의 인스트럭션을 저장하는 메모리; 상기 메모리에 저장된 상기 하나 이상의 인스트럭션을 실행하는 하나 이상의 프로세서를 포함할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상 의 인스트럭션을 실행함으로써, 사용자 데이터를 획득할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이 상의 인스트럭션을 실행함으로써, 상기 사용자 데이터를 기초로, 추천을 제공 받는 실제 사용자에 대응하는 가 상의 사용자를 나타내는 시뮬레이션된 사용자를 생성할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상 의 인스트럭션을 실행함으로써, 상기 시뮬레이션된 사용자의 상태에 기초하여 액션을 결정하되, 상기 액션은 상 기 사용자에게 제공될 상기 추천 세션에 포함되는 추천 요소를 결정하는 것일 수 있다. 상기 하나 이상의 프로 세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 시뮬레이션된 사용자의 상태를 업데이트할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 시뮬레이션된 사용자 의 업데이트된 상태 및 상기 시뮬레이션된 사용자로부터 출력되는 상기 추천 요소에 대한 리워드를 식별할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 시뮬레이션된 사용자 의 상기 업데이트된 상태 및 상기 리워드에 기초하여 상기 추천 요소의 결정을 반복하여 개인화된 추천 세션을 생성할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 개인화된 추천 세션을 출력할 수 있다. 본 개시의 일 측면에 따르면, 서버가 강화학습 기반으로 개인화된 추천 세션을 제공하는, 전술 및 후술하는 방 법들 중 어느 하나를 실행시키기 위한 프로그램이 기록된 컴퓨터 판독 가능 기록매체를 제공할 수 있다."}
{"patent_id": "10-2023-0118553", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 개시에 대해 구체적으로 설명하기로 한다. 본 개시에 서, \"a, b 또는 c 중 적어도 하나\" 표현은 \" a\", \" b\", \" c\", \"a 및 b\", \"a 및 c\", \"b 및 c\", \"a, b 및 c 모두\", 혹은 그 변형들을 지칭할 수 있다. 본 개시에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의 미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 또한, 본 명세서에서 사용되는 '제1' 또는 '제2' 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용할 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만 사용된다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소 프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해서 설 명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 이하 첨부된 도면을 참고하여 본 개시를 상세히 설명하기로 한다. 도 1은 본 개시의 일 실시예에 따른 서버가 강화학습 기반으로 개인화된 추천 세션을 제공하는 것을 개략적으로 도시한 도면이다. 도 1을 참조하면, 일 실시예에 따른 서버는 강화학습 모델을 이용하여 사용자에게 개인화된 추천을 제공할 수 있다. 일반적인 강화학습 시스템에서는, 에이전트가 환경과 상호작용하면서 액션을 결정하고 수행한다. 강화학습의 에 이전트는 현재 상태(State)를 인식하고, 환경(Environment)을 관측하여 리워드를 최대화하기 위해 액션을 선택 하도록 훈련된다. 일 실시예에서, 강화학습 모델은 추천 생성자 및 사용자 시뮬레이터를 포함할 수 있다. 본 개시 의 강화학습 모델에서는, 추천 생성자가 에이전트로서 사용자에게 추천 요소를 결정하는 액션을 수행 한다. 또한, 추천 요소를 제공받는 실제 사용자(또는, 추천 제공 애플리케이션)가 환경에 대응되어 에이전트와 상호작용 한다. 일 실시예에서, 강화학습 모델은 사용자 시뮬레이터를 이용하여 시뮬레이션된 사용자를 생성할 수 있 다. 시뮬레이션된 사용자는 실제 사용자에 대응하는 가상의 사용자를 나타내는 합성 데이터를 지칭한다. 또한, 사용자 시뮬레이터는 생성형 인공지능일 수 있다. 사용자 시뮬레이터는 미리 훈련된 것일 수 있으며, 시뮬레이션된 사용자가 실제 사용자와 유사한 가상의 피드백인 \"시뮬레이션된 피드백\"을 제공하도록 훈련된 것 일 수 있다. 본 개시의 실시예는 시뮬레이션된 사용자를 이용하여 강화학습 모델을 훈련시켜서, 적은 양의 실제 사용자 데이터만으로도 강화학습 모델을 훈련시킬 수 있는 이점을 제공할 수 있다. 일 실시예에서, 추천 생성자는 추천 요소들(m1~mN)로 구성되는 후보 추천 요소들 중에서 어느 한 추천 요소 를 결정하는 액션을 수행하고, 사용자 시뮬레이터는 실제 사용자를 시뮬레이션한 것과 같은 시뮬레이션된 사용자를 생성하며, 시뮬레이션된 사용자가 추천 생성자와 상호작용한다. 추천 생성자는 예를 들어, 시뮬레이션된 사용자의 상태를 관측하고, 시뮬레이션된 사용자의 상태에 기초하여 추천 요소들(m1~mN) 중 어느 하나를 결정한다. 사용자 시뮬레이터에 의해 생성된 시뮬레이션된 사용자는, 추천 생성자로부터 제공 받은 추천 요소에 대하여 리워드를 출력하며, 강화학습 모델은 리워드를 최대화하기 위해 추천 요소를 선 택하는 액션을 결정하는 과정을 최적화하도록 훈련된다. 본 개시에서의 강화학습 모델은, 사용자가 추천을 제공 받도록 미리 훈련된 모델로, 훈련된 강화학습 모델, 배포된 강화학습 모델이라고 지칭될 수 있으며, 수식 어를 생략하고 단순히 강화학습 모델이라고 지칭될 수도 있다. 한편, 강화학습 모델은, 사용자가 강화학습 모델을 사용함에 따라 수집되는 데이터에 기초하여 재훈련될 수도 있다. 일 실시예에서, 서버는 강화학습 모델을 이용하여 사용자에게 적어도 하나의 추천 세션을 제공할 수 있다. 예를 들어, 서버는 세션 1, 세션 2, ..., 세션 K를 포함하는 총 K개의 추천 세션 들을 생성할 수 있다. 본 개시에서, 서버가 제공하는 복수개의 추천 세션들은, 추천 세션 그룹이라 고 지칭될 수 있다. 일 실시예에서, 서버는 사용자에게 다양한 카테고리에 대하여 추천 세션 그룹을 제공할 수 있다. 예 를 들어, 서버는 운동 추천, 식단 추천, 미디어 콘텐츠 추천 등 다양한 카테고리에 대응하는 추천 세션 그룹을 사용자에게 제공할 수 있다. 구체적으로 예를 들면, 서버는 운동 추천 세션의 반복적인 생성을 통해, 복수의 운동 추천 세션들을 포함 하는 운동 추천 세션 그룹을 생성할 수 있다. 이 경우, 운동 추천 세션 그룹은 전체 운동(Workout)에 대응되고, 전체 운동은 복수의 운동 추천 세션들로 구성되며, 복수의 운동 추천 세션들 각각은 무브먼트(Movement) 추천 요소들로 구성된다. 여기서, 무브먼트란 운동의 특정 동작(예를 들어, 스쿼트 등)을 지칭한다. 서버는 사용자에게 운동 추천 세션을 제공하고, 사용자는 서버로부터 제공 받은 N개의 무브먼트들을 따라하면서 하 나의 운동 세션을 수행할 수 있다. 이하에서, 별도의 언급이 없는 한 본 개시의 서버가 사용자에게 개인화된 추천 세션을 제공하는 예시가 운동 추천 시나리오임을 가정하여 설명할 것이다. 다만, 이는 설명의 편의를 위한 예시일 뿐이며, 본 개시의 서 버가 제공하는 개인화된 추천은, 추천 카테고리가 운동인 것 외에 다른 것인 경우에 대해서도 동일/유사 한 방식으로 적용될 수 있다. 즉, 본 개시는, 사용자에게 개인화된 추천 세션이 제공 가능한 다양한 분야에 적 용될 수 있으며, 강화학습을 기반으로 시뮬레이션된 사용자를 이용하여 사용자에게 개인화된 추천 세션을 제공 한다. 서버가 시뮬레이션된 사용자를 사용하는 강화학습을 이용하여 사용자에게 개인화된 추천을 제공하는 동작 을 후술하는 도면들을 참조하여 더 상세하게 기술하기로 한다. 도 2는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 세션을 제공하는 동작을 설명하기 위한 흐름도이다. 도 2를 참조하여, 본 개시의 서버의 전반적인 동작을 기술할 것이다. 또한, 이후의 도면들을 참조하여 서 버의 동작의 구체적인 세부사항들을 기술할 것이다. 동작 S210에서, 서버는 사용자 데이터를 획득한다. 사용자 데이터는 서버에 저장된 것일 수 있고, 사용자의 전자 장치(모바일 폰 등)로부터 수신되는 것일 수 있다. 서버는 사용자로부터 추천 서비스 제공 요청을 수신할 수 있다. 일 실시예에서, 사용자 데이터는 추천 세션을 제공 받는 사용자에 의해 입력되는 것일 수 잇다. 사용자 데이터 는 사용자의 성향을 식별하기 위한 기본적인 설명(Basic description)을 포함할 수 있다. 예를 들어, 사용자 데 이터는 성별, 나이 등 사용자와 관련된 개인 정보를 포함할 수 있다. 또한 예를 들어, 사용자 데이터는 추천 카 테고리와 관련된 사용자 입력 정보를 포함할 수 있다. 구체적으로, 운동(Workout) 추천 시나리오에서는, 운동 스타일, 운동 기간, 집중 근육 부위, 운동 숙련도 등이 사용자 데이터에 포함될 수 있다. 또는, 체중 감량을 위 한 식단 추천 시나리오에서는, 음식 알러지 정보, 지역, 예산, 체중, 혈당, 식사 장소, 조리 접근성 등이 사용 자 데이터에 포함될 수 있다. 또는, 장기간의 콘텐츠 추천 시나리오에서는, 장르 선호도, 언어 선호도, 지역, 흥미, 취미, 혼인 상태, 자녀 유무 등이 사용자 데이터에 포함될 수 있다. 일 실시예에 따른 서버는 특정 카테고리의 추천을 사용자에게 제공하기 위해, 추천 카테고리에 대응하는 사용자 데이터를 획득할 수 있다. 이 경우, 사용자 데이터는 사용자로부터 입력되는 것일 수 있다. 예를 들어, 서버는 기 설정된 추천 카테고리에 대응하는 사용자 데이터를 획득할 수 있다. 또는, 서버는 사용 자 데이터를 획득하고, 사용자 데이터에 포함되는 데이터 요소들에 기초하여 추천 카테고리를 식별할 수 있다. 서버는 사용자 데이터에 기초하여, 복수개의 추천 요소들로 구성되는 추천 세션을 생성하기 위한 아래의 동작들을 수행할 수 있다. 동작 S220에서, 서버는 사용자 데이터를 기초로, 추천을 제공 받는 실제 사용자에 대응하는 가상의 사용 자를 나타내는 시뮬레이션된 사용자를 생성한다. 일 실시예에서, 서버는 미리 훈련된 생성형 인공지능인 사용자 시뮬레이터를 이용하여 시뮬레이션된 사용 자를 생성할 수 있다. 본 개시의 강화학습 시스템에서는, 추천을 제공 받는 실제 사용자(또는, 추천 제공 애플 리케이션)이 환경에 대응된다. 또한, 추천 요소를 결정하고 제공하는 생성자가 에이전트에 대응된다. 구체적으 로 예를 들면, 시뮬레이션된 사용자는, 운동 추천을 세션을 제공 받는 실제 사용자를 시뮬레이션한 것일 수 있 다. 시뮬레이션된 사용자는 실제 사용자의 내부 현황을 추적하고, 실제 사용자에게 제공된 추천 이력을 관리하 는 데 이용될 수 있다. 또한, 시뮬레이션된 사용자는 강화학습 모델을 훈련시키는데 이용되는 시뮬레이션된 피 드백을 생성할 수 있다. 이 경우, 사용자 시뮬레이터는, 실제 사용자들의 피드백 데이터에 기초하여 실제 사용 자들과 유사한 시뮬레이션된 피드백을 생성하도록 훈련된 것일 수 있다. 동작 S230에서, 서버는 시뮬레이션된 사용자의 상태에 기초하여 액션을 결정한다. 여기서 액션은, 추천 요소 후보들 중에서 추천 요소를 결정하는 것으로, 에이전트인 생성자를 이용하여 수행될 수 있다. 일 실시예에서, 서버는 운동 추천 세션을 제공하기 위해, 시뮬레이션된 사용자의 상태에 기초하여 운동에 관련된 무브먼트 추천 요소들인 무브먼트들을 결정할 수 있다. 운동 추천 세션에 포함되는 무브먼트 추천 요소 들이란, 하나의 운동 세션 내 포함되는 기본적인 운동 동작을 말한다. 예를 들어, 무브먼트 추천 요소는, 스쿼 트, 버피 등일 수 있으나, 이에 한정되는 것은 아니다.사용자의 상태는 다양한 세그먼트들로 구분될 수 있다(s = Concat(s1, s2, s3, ..., sn)). 사용자의 상태의 세그 먼트들은 예를 들어, 사용자 설명(Description) d, 사용자 선호도 p, 사용자 내부 현황 i, 및 추천 이력 h 중 적어도 하나를 포함할 수 있다. 운동 추천 시나리오를 예로 들면, 사용자 설명은 사용자와 관련된 개인 정보 및 추천 카테고리와 관련된 사용자 입력 정보(예를 들어, 운동 스타일, 운동 기간, 집중 근육 부위, 운동 숙련도 등)를 포함할 수 있다. 또한, 사용자 선호도는 무브먼트 선호도, 무브먼트 난이도 선호도, 다양성 선호도, 피드백 선호도 등을 포함할 수 있다. 또한, 사용자 내부 현황은 사용자가 추천을 제공받음에 따라 영향을 받는 요소들을 말한다. 예를 들어, 체력 향 상, 실시간 심박수, 근육 피로도 등이 사용자 내부 현황에 포함될 수 있다. 또한, 추천 이력은 사용자에게 추천된 추천 요소들 및 추천 요소에 대한 피드백들의 이력을 포함할 수 있다. 예 를 들어, 운동에 포함되는 추천 요소들인 무브먼트 및 무브먼트에 대한 피드백 등이 추천 이력에 포함될 수 있 다. 동작 S240에서, 서버는 시뮬레이션된 사용자의 상태를 업데이트한다. 이는, 상태 전이 함수 T(s, a, s') 에 의해 액션에 따라 상태가 전이되는 것이므로 상태가 자연적으로 업데이트 되는 것이라고 표현될 수도 있다. 시뮬레이션된 사용자는 실제 환경(사용자)을 모사한 가상 환경(가상의 사용자)이다. 시뮬레이션된 사용자의 상 태가 상태 전이 함수에 기초하여 업데이트되며, 서버는 에이전트인 생성자와 환경인 시뮬레이션된 사용자 의 상호작용을 관리한다. 할 수 있다. 상태 전이 함수 는, 에이전트가 환경과 상호작용하는 과정에서 현재 상태 s와 에이전트가 선택한 액션 a에 따라 다음 상태 s'로 어떻게 전이되는지를 정의하는 함수를 말한다. 즉, 상태 전이 함수는 특정 액션에 따라 현재 상태에서 다음 상태로 전이되는 확률을 포함하며, 강화학습 모델의 훈련 과 정에서 모델링 된 것일 수 있다. 일 실시예에서, 서버가 무브먼트 후보들 중에서 어느 한 무브먼트를 추천 요소로 선택하는 경우, 시뮬레 이션된 사용자의 상태에 포함되는 사용자 설명 d, 사용자 선호도 p, 사용자 내부 현황 i, 및 추천 이력 h 등이 업데이트될 수 있다. 동작 S250에서, 서버는 시뮬레이션된 사용자의 업데이트된 상태 및 시뮬레이션된 사용자로부터 출력되는 리워드를 식별한다. 리워드 함수는 R(s, a, s') 상태 s가 다음 상태 s'로 전이됨에 따라 제공되는 수치적인 값을 정의하는 함수를 말한다. 본 개시에서 리워드는, 에이전트가 현재 상태를 관측하고 추천 요소를 결정하는 액션에 대응하도록 정 의된다. 즉, 본 개시에서 리워드는 환경이 어떤 상태로 전이되는지와 상관없으므로, 리워드 함수가 다음 상태 s'에 영향을 받지 않는 형태인 R(s, a)로 정의된다. 에이전트는 환경과 상호작용하면서, 리워드를 기초로 액션 을 결정하는 정책을 학습함으로써, 향후에 추천 세션의 요청이 반복될 때마다 에이전트가 더 나은 최적의 액션 을 찾도록 할 수 있다. 리워드는 부정적인 리워드 및 긍정적인 리워드를 포함하도록 설계될 수 있다. 예를 들어, 부정적인 리워드는 추 천 요소를 사용자가 좋아하지 않는 경우를 포함할 수 있고, 긍정적인 리워드는 추천 요소를 사용자가 좋아하는 경우를 포함할 수 있다. 일 실시예에서, 서버는 운동 추천 세션을 제공하기 위해 추천 세션에 포함되는 추천 요소인 무브먼트를 제공하고, 추천 요소를 제공한 것에 대한 부정적인 리워드 또는 긍정적인 리워드를 식별할 수 있다. 예를 들어, 부정적인 리워드는, 사용자가 추천된 무브먼트를 좋아하지 않거나, 사용자가 추천된 무브먼트가 너 무 쉽거나 어렵다고 하거나, 추천된 무브먼트가 과도하게 반복적인 경우, 추천된 무브먼트가 사용자의 운동 스 타일에 부합하지 않는 경우 등을 포함할 수 있다. 단, 부정적인 리워드가 전술한 예시로 한정되는 것은 아니다. 예를 들어, 긍정적인 리워드는, 사용자가 추천된 무브먼트를 좋아하거나, 추천된 무브먼트가 사용자 선호 운동 난이도에 부합하거나, 추천된 무브먼트가 운동 추천 세션 내 무브먼트들의 다양성을 개선하는 경우, 추천된 무 브먼트가 사용자 데이터의 집중 근육 부위에 부합하는 경우 등을 포함할 수 있다. 단, 긍정적인 리워드가 전술 한 예시로 한정되는 것은 아니다. 동작 S260에서, 서버는 시뮬레이션된 사용자의 업데이트된 상태 및 리워드에 기초하여 추천 요소의 결정 을 반복하여 개인화된 추천 세션을 생성한다.강화학습에서, 에이전트는 강화학습의 단일 에피소드(액션들의 시퀀스)가 끝날 때까지 환경과 상호작용하고 액 션을 결정하는 동작을 반복한다. 일 실시예에서, 서버는 단일 에피소드의 종료 조건에 기초하여 추천 요소의 결정을 반복할 수 있다. 예를 들어, 서버는 추천 요소의 결정을 N회 수행하여 N개의 추천 요소를 포함하는 추천 세션을 생성할 수 있다. 구체적으로, 서버는 N개의 무브먼트 추천 요소들을 포함하는 운동 추천 세션을 생성할 수 있다. 일 실시예에서, 서버는 추천 세션의 생성을 반복하여, 복수의 추천 세션들을 포함하는 추천 세션 그룹을 생성할 수 있다. 예를 들어, 서버는 운동 추천 세션의 생성을 반복하여, 복수의 운동 추천 세션들을 포함 하는 운동 추천 세션 그룹을 생성할 수 있다. 이 경우, 운동 추천 세션 그룹은 전체 운동(Workout)에 대응되고, 전체 운동은 복수의 운동 추천 세션들로 구성되며, 복수의 운동 추천 세션들 각각은 무브먼트 추천 요소들로 구 성된다. 일 실시예에서, 서버는 추천 세션을 사용자에게 제공하고, 사용자로부터 피드백을 획득할 수 있다. 서버 는 사용자의 피드백에 기초하여, 시뮬레이션된 사용자를 재훈련시킬 수 있다. 서버는 추천 세션이 생성될 때마다 시뮬레이션된 사용자를 재훈련시켜 시뮬레이션된 사용자가 실제 사용자의 행동을 정밀하게 모방 가능하도록 함으로써, 사용자에게 점진적으로 개인화된 추천 세션을 제공할 수 있다. 동작 S270에서, 서버는 개인화된 추천 세션을 출력한다. 서버는 개인화된 추천 세션을 사용자의 전 자 장치(예를 들어, 모바일 폰 등)로 전송할 수 있다. 도 3은 본 개시의 일 실시예에 따른 운동 추천을 제공하는 강화학습 모델의 전반적인 구조를 도시한 도면이다. 도 3을 참조하면, 운동 추천을 제공 받는 실제 사용자는, 애플리케이션(예를 들어, 모바일 애플리케이션, 웹 에플리케이션 등)을 이용하여 강화학습 모델과 상호작용 할 수 있다. 사용자는 강화학습 모델로부 터 운동 추천 요소들(액션들)을 수신하고, 추천 요소들에 대한 피드백 및 운동 이력(사용자 이력)을 강화학습 모델에게 제공할 수 있다. 일 실시예에서, 운동 추천을 제공하는 강화학습 모델은 강화학습 모델의 환경을 나타내기 위해 구축 된 가상 환경(RL Gym 이라고도 지칭됨)을 포함할 수 있다. 또한, 강화학습 모델은 실제 환경인 사용 자 또는 가상 환경과 상호작용하는 에이전트(Generator 라고도 지칭됨)를 포함할 수 있다. 일 실시예에서, 가상 환경은 합성 데이터로 상호작용을 생성하는 기능을 하는 사용자 시뮬레이터를 포함할 수 있다. 사용자 시뮬레이터는 합성 데이터인 시뮬레이션된 사용자를 생성하고, 시뮬레이션된 피드 백을 생성하여 에이전트에게 제공할 수 있다. 즉, 사용자 시뮬레이터는 합성 데이터를 생성함으로써, 제한된 실제 데이터만으로도 에이전트가 훈련되도록 할 수 있다. 일 실시예에서, 강화학습 모델은 순차적인 의사결정 문제를 확률적으로 모델링하는 수학적 모델인 마르코 프 결정 프로세스(Markov Decision Process; MDP)를 이용하는 것일 수 있다. 강화학습 모델의 에이전트 는 각 상태에서 특정 액션을 선택할 수 있으며, 현재 상태에서 액션이 선택되면 상태 전이 함수에 의해 다 음 상태가 결정된다. 각 상태에서 얻는 리워드는 액션에 의해 결정되며, 리워드는 에이전트의 최종 목표(예를 들어, 개인화된 운동 세션 추천을 제공)을 반영하여 정의된다. 가상 환경은 환경의 현재 상태를 에이전트 에게 전송하고, 에이전트로부터 액션을 받은 다음에, 리워드 및 새로운 상태/관측을 에이전트에 게 제공한다. 예를 들어, 가상 환경은 시뮬레이션된 사용자의 현재 상태를 에이전트에게 전송하고, 에이전트로부터 운동 추천 요소를 제공 받은 다음에, 시뮬레이션된 사용자의 상태를 업데이트하고, 리워드 및 새로운 상태/관측을 에이전트에게 제공할 수 있다. 이하에서, 본 개시의 강화학습 모델의 동작을 설명하기 위한 상태, 액션, 리워드, 피드백 등에 대하여 상 세하게 기술한다. 일 실시예에서, 가상 환경은 상태를 에이전트에게 전달할 수 있다. 즉, 에이전트는 가상 환경 을 관측하여 상태를 식별할 수 있다. 상태는 주어진 시점의 리워드와 후속 상태로의 전이를 결정하는 데 필요한 모든 정보가 포함될 수 있다. 상태는 다양한 세그먼트들로 구분될 수 있다(s = Concat(s1, s2, s3, ..., sn)). 운동 추천 시나리오에서, 상태의 세그먼트들 중 일부를 예로 들면, 상태는 사용자 설명(Description) d, 사용자 선호도 p, 사용자 내부 현황 i, 및 추천 이력 h 중 적어도 하나를 포함할 수 있으나, 이에 한정되는 것은 아니다. 상태에 포함되는 사용자 설명은, 사용자와 관련된 개인 정보(예를 들어, 나이, 성별 등) 및 추천 카테고리인 운 동과 관련된 사용자 입력 정보(예를 들어, 운동 목표, 운동 스타일, 운동 기간, 집중 근육 부위, 운동 숙련도, 체력 수준 등)를 포함할 수 있으나, 이에 한정되는 것은 아니다. 상태에 포함되는 사용자 선호도는, 무브먼트 선호도, 무브먼트 난이도 선호도, 다양성 선호도, 심박수 선호도, 피드백 선호도(특정 타입의 피드백 제공 확률) 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 상태에 포함되는 사용자 내부 현황은, 사용자가 추천을 제공받음에 따라 영향을 받는 요소들을 포함할 수 있다. 예를 들어, 체력 향상, 실시간 심박수, 근육 피로도 등이 사용자 내부 현황에 포함될 수 있으나, 이에 한정되는 것은 아니다. 상태에 포함되는 추천 이력은, 사용자에게 추천된 추천 요소들 및 추천 요소에 대한 피드백들의 이력을 포함할 수 있다. 예를 들어, 운동에 포함되는 추천 요소들인 무브먼트 및 무브먼트에 대한 피드백 등이 추천 이력에 포 함될 수 있다. 일 실시예에서, 상태에 포함되는 세그먼트들은 \"관측 가능한 것\" 또는 \"관측되지 않는 것(또는, 부분적으로 관 측 가능한 것)\" 으로 분류될 수 있다. 관측 가능한 상태는 에이전트가 환경의 정확한 상태를 직접 관측할 수 있 는 상태를 말한다. 예를 들어, 사용자 설명(운동 목표, 초기 체력 수준, 운동 스타일 등) 및 추천 이력이 관측 가능한 것으로 분류될 수 있다. 관측되지 않는 또는 부분적으로 관측 가능한 상태는 에이전트가 환경의 정확한 상태를 직접 관측하지 못하고, 간접적으로 관측하거나 추론을 통해 정보를 획득해야 하는 상태를 말한다. 예를 들어, 사용자 선호도, 사용자 내부 현황이 관측되지 않는 것 또는 부분적으로 관측 가능한 것으로 분류될 수 있 다. 이 경우, 강화학습 모델의 마르코프 의사 결정 프로세스는, 부분적으로 관측 가능한 마르코프 의사 결정 프로세스(Partially observable Markov Decision Process; POMDP)일 수 있다. 일 실시예에서, 상태 전이 함수는 에이전트가 특정 상태에서 특정 액션을 취할 때, 현재 상태가 다음 상태 로 어떻게 전이되는지를 정의한다. 즉, 상태 전이 함수 T(s, a, s')는, 에이전트가 가상 환경 또는 실제 사용자와 상호작용하는 과정에서 현재 상태 s와 에이전트가 선택한 액션 a에 따라 다음 상태 s'로 어 떻게 전이되는지를 정의하는 함수를 말한다. 일 실시예에서, 상태에 포함되는 세그먼트들은 \"정적인 것\" 또는 \"동적인 것\" 으로 분류될 수 있다. 예를 들어, 상태에 포함되는 사용자 설명, 사용자 선호도의 경우, 정적인 것으로 분류될 수 있다. 상태 전이 함 수에서, 정적인 세그먼트는 다음 상태로 전이되더라도 그대로 유지되도록 정의될 수 있다. 또한, 상태에 포함되는 사용자 내부 현황, 추천 이력의 경우, 동적인 것으로 분류될 수 있다. 상태 전이 함수에 서, 동적인 세그먼트는 다음 상태로 전이될 때 변화하도록 정의될 수 있다. 이 경우, 동적인 세그먼트는 다시 \"결정적인 것(Deterministic, p=1)\" 또는 \"비결정적인 것(Non-deterministic, p≠1)\"으로 분류될 수 있다. 예 를 들어, 추천 이력은 동적인 세그먼트이지만, 현재 액션으로 결정된 추천 요소를 추천 세션의 리스트의 끝에 새로 부가하는 것이므로 결정적인 것으로 분류될 수 있다. 또는, 추천 요소에 대한 좋음/싫음과 같은 사용자 피 드백은, 확률에 의해 정의될 수 있으므로 비결정적인 것으로 분류될 수 있다. 마찬가지로, 사용자 내부 현황의 상태 전이도 비결정적인 것으로 분류될 수 있다. 사용자 내부 현황은 관측 불가능하거나 부분적으로 관측 가능 한 상태이므로, 사용자의 내부 현황을 추적 및 예측 가능한 내부 모델(Internal model)에 의해 정의되고 제어될 수 있다. 일 실시예에서, 에이전트는 액션을 선택함에 따른 결과로 리워드를 획득할 수 있다. 리워드 함수는 R(s, a, s') 상태 s가 다음 상태 s'로 전이됨에 따라 제공되는 수치적인 값을 정의하는 함수를 말한다. 본 개시에서 리워드는, 에이전트가 현재 상태를 관측하고 추천 요소를 결정하는 액션에 대응하도록 정의된다. 즉, 본 개시에서 리워드는 환경이 어떤 상태로 전이되는지와 상관없으므로, 리워드 함수가 다음 상태 s'에 영향을 받지 않는 형태인 R(s, a)로 정의된다. 에이전트는 환경(예를 들어, 가상 환경)과 상호작용하면서, 리워드 를 기초로 액션을 결정하는 정책을 학습함으로써, 향후에 추천 세션의 요청이 반복될 때마다 에이전트가 더 나 은 최적의 액션을 찾도록 할 수 있다. 리워드는 부정적인 리워드 및 긍정적인 리워드를 포함하도록 설계될 수 있다. 예를 들어, 부정적인 리워드는 추 천 요소를 사용자가 좋아하지 않는 경우를 포함할 수 있고, 긍정적인 리워드는 추천 요소를 사용자가 좋아하는 경우를 포함할 수 있다.구체적 예를 들면, 부정적인 리워드는, 사용자가 추천된 무브먼트를 좋아하지 않거나, 사용자가 추천된 무브먼 트가 너무 쉽거나 어렵다고 하거나, 추천된 무브먼트가 과도하게 반복적인 경우, 추천된 무브먼트가 사용자의 운동 스타일에 부합하지 않는 경우 등을 포함할 수 있다. 단, 부정적인 리워드가 전술한 예시로 한정되는 것은 아니다. 구체적 예를 들면, 긍정적인 리워드는, 사용자가 추천된 무브먼트를 좋아하거나, 추천된 무브먼트가 사용자 선 호 운동 난이도에 부합하거나, 추천된 무브먼트가 운동 추천 세션 내 무브먼트들의 다양성을 개선하는 경우, 추 천된 무브먼트가 사용자 데이터의 집중 근육 부위에 부합하는 경우 등을 포함할 수 있다. 단, 긍정적인 리워드 가 전술한 예시로 한정되는 것은 아니다. 일 실시예에서, 가상 환경은 에이전트의 훈련 프로세스 속도를 높이고 특정 액션에 제약을 가하기 위 한 액션 마스크를 에이전트에게 전달할 수 있다. 예를 들어, 액션 마스크는 특정 액션을 1 또는 0으로 마 스크한 것일 수 있다. 이 경우, 1로 표시된 액션은 선택 가능하고, 0으로 표시된 액션은 선택 불가능한 것으로 되며, 에이전트는 선택 가능한 액션만 고려하여 액션을 결정할 수 있다. 즉, 액션 마스크는 에이전트(31 6)가 불필요한 액션을 수행하지 않도록 하여 훈련 시간을 줄이고, 효율적인 훈련이 되도록 할 수 있다. 예를 들 어, 액션 마스크에 의해 에이전트의 액션이 제한될 수 있다. 즉, 액션 마스크에 의해 사용자가 과거에 싫 어했던 운동이나 사용자가 수행할 수 없다고 지정한 운동이 에이전트에 의해 추천 되는 것이 제한될 수 있 다. 도 4는 본 개시의 일 실시예에 따른 서버가 시뮬레이션된 사용자를 생성하는 동작을 설명하기 위한 도면이다. 동작 S410에서, 서버는 제1 사용자로부터 사용자 데이터를 수신한다. 제1 사용자는 개인화된 추천 세션 제공 받고자 하는 사용자를 말하며, 예를 들어, 개인화된 추천 제공 서비스를 이용하기 위해 전자 장치(예를 들 어, 모바일 폰 등)를 이용하여 서버에 접속한 신규 사용자를 포함할 수 있다. 서버는 제1 사용자가 접속하면, 사용자 데이터를 입력 받기 위한 사용자 인터페이스(예를 들어, 그래픽 사용자 인터페이스(GUI), 음 성 인터페이스 등)를 제공할 수 있다. 예를 들어, 서버는 서비스를 최초로 이용하는 사용자에게, 나이, 성별 등과 같은 개인 정보를 입력할 수 있는 사용자 인터페이스를 제공할 수 있다. 또는, 서버는 사용자 에게 운동 목표, 운동 스타일, 운동 기간, 집중 근육 부위, 운동 숙련도, 체력 수준 등과 같이 추천 카테고리인 운동과 관련된 정보를 입력할 수 있는 사용자 인터페이스를 제공할 수 있다. 예를 들어, 서버는 운동과 관련된 정보를 획득하기 위한 설문을 제공하고, 설문에 대한 사용자 응답에 기초하여 사용자 데이터를 생성할 수 있다. 동작 S420에서, 서버는 사용자 데이터를 기초로 제2 사용자들을 클러스터한다. 제2 사용자는 제1 사용자 외에 또다른 사용자들을 말하며, 예를 들어, 개인화된 추천 제공 서비스를 이용하는 기존 사용자들을 포함할 수 있다. 이 경우, 제2 사용자들의 사용자 데이터가 존재할 수 있다. 서버는 제1 사용자의 사용자 데이터 및 제2 사용자들의 사용자 데이터를 기초로, 제1 사용자와 유사한 특성을 갖는 제2 사 용자의 클러스터를 획득할 수 있다. 예를 들어, 서버는 제1 사용자 및 제2 사용자들의 사용자 데이터로부 터 주요한 특징을 선택하거나 추출하고, 유사도를 계산할 수 있다. 이 경우, 유사도 측정을 위한 다양한 알고리 즘(예를 들어, 유클리드 거리 측정)이 이용될 수 있다. 서버는 클러스터링 알고리즘(예를 들어, 계층적 클러스터링, K-Means 등)을 이용하여 사용자들을 클러스터로 그룹화 할 수 있다. 서버는 제1 사용자의 사 용자 데이터를 제2 사용자들의 클러스터들과 비교하여 제1 사용자가 속하는 클러스터를 결정할 수 있다. 동작 S430에서, 서버는 제2 사용자들의 클러스터에 대응하는 시뮬레이션된 사용자들에 기초하여, 제1 사 용자의 시뮬레이션된 사용자를 생성한다. 일 실시예에서, 제2 사용자들의 클러스터 내 각각의 사용자들은, 기존에 개인화된 추천 제공 서비스를 이용하던 사용자들이므로 제2 사용자들 각각에 대응하는 시뮬레이션된 사용자들이 존재할 수 있다. 서버는 제2 사 용자들의 클러스터에 대응하는 시뮬레이션된 사용자들에 포함되는 파라미터들에 기초하여, 제1 사용자의 시뮬레 이션된 사용자에 포함될 파라미터들을 설정할 수 있다. 예를 들어, 서버는 제1 사용자의 시뮬레이션된 사 용자의 사용자 설명(Description) d, 사용자 선호도 p, 등을 나타내는 파라미터들을 설정할 수 있다. 일 실시예에서, 서버가 제1 사용자의 시뮬레이션된 사용자를 생성할 때, 제2 사용자들의 데이터가 충분하 지 않을 수 있다. 서버는 제2 사용자들의 수를 식별하고, 제2 사용자들의 수가 기 설정된 값 이상인 경우 에 제2 사용자들을 클러스터할 수 있다. 또한, 서버는 제2 사용자들의 수가 기 설정된 값 미만인 경우, 제1 사용자의 사용자 데이터를 기초로 제1 사용자의 시뮬레이션된 사용자에 포함될 파라미터들을 임의로 설정할수 있다. 서버가 사용자 시뮬레이터를 이용하여 생성한 시뮬레이션된 사용자의 동작에 대해서는 도 7a 내지 도 7b 를 참조하여 보다 상세하게 기술한다. 도 5는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 세션을 생성하는 동작을 설명하기 위한 도면이다. 도 5를 참조하면, 동작 S510 내지 S530은 도 2의 동작 S260에 대응할 수 있다. 또한, 강화학습에 관점에서, 생 성자가 에이전트에 대응하고, 시뮬레이션된 사용자가 환경에 대응한다. 또한, 도 5에 도시된 동작들은 서버 에 의해 수행된다. 예를 들어, 서버가 강화학습 모델의 사용자 시뮬레이터를 이용하거나, 생성자를 이용하여 데이터 처리를 수행하는 것이다. 동작 S510에서, 서버는 추천 요소를 출력한다. 추천 요소를 결정하는 것은 에이전트의 액션에 대응한다. 즉, 강화학습 모델의 에이전트인 생성자가 추천 요소를 출력하며, 출력된 추천 요소는 리스트에 저장된다. 서버는 추천 요소 데이터베이스에 저장된 추천 요소 후보들 중에서 하나를 선택할 수 있다. 생성자 는 시뮬레이션된 사용자를 관측하고 시뮬레이션된 사용자의 상태에 기초하여 추천 요소를 선택하는 액션을 수행 한다. 이 경우, 추천 요소는 다양한 속성을 포함할 수 있다. 예를 들어, 운동(Workout) 추천 시나리오에서 추천 요소인 무브먼트는, 기본 무브먼트(Base movement), 난이도(Difficulty level), 집중 근육 부위(Focused muscle), 유산소/무산소(Cardio/non-cardio) 등의 속성을 포함할 수 있으나, 이에 한정되는 것은 아니다. 기본 무브먼트는 특정 유형의 운동을 이루는 기본적인 동작을 말한다. 기본 무브먼트의 다양한 변형 또는 보조 동작을 통해 다양한 방식으로 운동이 수행될 수 있다. 예를 들어, 기본 무브먼트는 스쿼트일 수 있고, 이 경우, 스쿼트에는 평행 스쿼트, 풀 스쿼트, 하프 스쿼트 등이 포함될 수 있다. 난이도는 기 설정된 범위의 값을 가질 수 있다. 난이도의 값이 높을수록 무브먼트의 난이도가 높은 것을 나타낸 다. 예를 들어, 난이도는 1 에서 4 사이의 값으로 정의될 수 있고, 평행 스쿼트의 난이도는 1.4로 정의될 수 있 다. 집중 근육 부위는 무브먼트 수행 시 관여하는 근육을 나타낸다. 예를 들어, 스쿼트의 경우 집중 근육 부위는 대 퇴사두근(Quadriceps), 햄스트링(Hamstrings) 등을 포함할 수 있다. 유산소/무산소는 무브먼트가 심장 및 폐 기능을 향상시키기 위한 유산소 운동인지 여부 또는 근육 강화를 위한 무산소 운동인지 여부를 포함할 수 있다. 서버는 강화학습 모델을 이용하여, 시뮬레이션된 사용자의 상태 및 추천 요소의 속성에 기초하여 결정된 추천 요소를 출력할 수 있다. 즉, 강화학습 모델의 생성자에 의해 추천 요소를 선택하는 액션이 수행될 수 있다. 동작 S520에서, 시뮬레이션된 사용자는 추천 요소에 기초하여 현재 상태를 다음 상태로 업데이트한다. 즉, 시뮬 레이션된 사용자는 상태 전이 함수 T(s, a, s')에 기초하여 다음 상태로 전이된다. 예를 들어, 시뮬레이션된 사 용자가 무브먼트 추천을 제공 받음에 따라, 시뮬레이션된 사용자의 사용자 내부 현황 i, 추천 이력 h와 같은 동 적인 상태들이 업데이트될 수 있다. 동작 S530에서, 시뮬레이션된 사용자는 업데이트된 상태 및 리워드를 출력한다. 업데이트된 상태 및 리워드는 에이전트로 전달되어, 에이전트가 다음 액션을 결정하도록 할 수 있다. 예를 들어, 시뮬레이션된 사용자의 업데 이트된 상태 및 리워드는, 에이전트가 이전에 추천된 무브먼트를 사용자가 수행한 다음에 수행할 또 다른 무브 먼트를 결정하는 데 이용될 수 있다. 서버는 시뮬레이션된 사용자의 이전 상태 및 다음 상태, 액션 및 리 워드를 트레이닝 데이터베이스에 저장할 수 있다. 트레이닝 데이터베이스에 저장된 데이터는 강화학 습 모델을 재훈련시키는 데 이용될 수 있다. 일 실시예에서, 서버는 동작 S510 내지 동작 S530을 기 설정된 횟수만큼 반복할 수 있다. 예를 들어, 서 버는 동작 S510 내지 S530을 기 설정된 횟수 N만큼 반복할 수 있다. 이 경우, 추천 요소 리스트에는 N개 의 추천 요소들이 포함될 수 있다. 서버가 동작 S510 내지 동작 S530을 반복하여 N개의 추천 요소를 출력 한 것은 하나의 에피소드가 수행되었다고 지칭될 수 있다. 동작 S540에서, 서버는 완성된 추천 세션을 사용자에게 출력한다. 예를 들어, 서버는 N개의 무브먼 트들을 포함하는 운동 세션을 사용자에게 출력할 수 있다. 이 경우, 사용자는 서버로부터 운동 세션을 제 공 받고, 운동 세션 내 무브먼트들을 순차적으로 수행할 수 있다. 일 실시예에서, 서버는 추천 세션을 제공할 때, 추천 세션과 관련된 정보를 사용자에게 제공할 수 있다. 예를 들어, 서버가 운동 추천 세션을 제공하는 경우, 서버는 운동 세션에 포함되는 무브먼트들 각각을 사용자가 따라할 수 있도록 이미지, 비 디오, 오디오 등의 정보를 사용자에게 제공될 수 있다. 서버가 제공하는 추천 세션과 관련된 정보는 사용 자의 전자 장치(예를 들어, 모바일 폰)에서 출력될 수 있다. 일 실시예에서, 서버는 추천 세션을 생성하는 동작(S540)을 기 설정된 횟수 M만큼 반복할 수 있다. 서버 는 추천 세션의 생성을 반복하여 M개의 추천 세션들로 구성되는 추천 세션 그룹을 생성할 수 있다. 예를 들어, 서버는 운동 추천 세션의 생성을 반복하여, M개의 추천 세션들로 구성되는 운동 추천 세션 그룹을 생성할 수 있다. 도 6은 본 개시의 일 실시예에 따른 서버가 시뮬레이션된 사용자를 재훈련시키는 동작을 설명하기 위한 도면이다. 동작 S610에서, 서버는 하나 이상의 추천 세션을 출력한다. 동작 S610은 도 2의 동작 S270 또는 도 5의 동작 S540에 대응할 수 있다. 예를 들어, 서버는 복수의 추천 요소들로 구성되는 하나의 추천 세션을 생 성하여 출력할 수 있고, 추천 세션을 생성하는 동작을 반복하여 복수의 추천 세션들을 출력할 수 있다. 구체적 으로, 서버는 운동 추천 세션 1, 운동 추천 세션 2, ..., 운동 추천 세션 M으로 구성되는 운동 추천 세션 그룹을 생성할 수 있다. 서버가 하나 이상의 추천 세션을 생성하는 동작들은 전술하였으므로, 반복되는 설명은 생략한다. 동작 S620에서, 서버는 추천 세션에 대한 피드백을 사용자로부터 수신한다. 예를 들어, 서버는 추 천 세션을 제공하는 동안 사용자로부터 피드백을 수신할 수 있다. 구체적으로, 사용자는 운동 추천 세션을 제공 받고, 운동 추천 세션을 따라 각각의 무브먼트들을 실행하면서, 즉각적인 피드백을 입력할 수 있다. 또는, 사용 자는 운동 추천 세션을 제공 받고, 운동 추천 세션을 따라 각각의 무브먼트들을 모두 실행한 후에, 운동 추천 세션에 대한 피드백을 입력할 수도 있다. 일 실시예에서, 피드백의 유형은 다양하게 정의될 수 있다. 운동 추천 세션에 대한 피드백을 예로 들면, 무브먼 트를 완료하는 것, 무브먼트를 스킵하는 것, 무브먼트를 더 어렵게 변경하거나 쉽게 변경하는 것, 무브먼트에 대한 좋음/싫음 응답, 운동 세션의 강도에 대한 스코어 평가, 운동 세션의 만족도에 대한 스코어 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 동작 S630에서, 서버는 사용자로부터의 피드백에 기초하여 시뮬레이션된 사용자를 재훈련시킬 수 있다. 일 실시예에서, 강화학습 모델은 미리 훈련된 생성형 인공지능인 사용자 시뮬레이터를 포함할 수 있다. 사용자 시뮬레이터는 실제 사용자에 대응하는 가상의 사용자를 나타내는 시뮬레이션된 사용자를 생성할 수 있다. 또한, 사용자 시뮬레이터의 시뮬레이션된 사용자는 실제 사용자들과 유사한 시뮬레이션된 피드백을 생성하도록 훈련된 것일 수 있다. 서버는 동작 S620에서 획득된 실제 사용자의 피드백에 기초하여, 시뮬레이션된 사용자의 시뮬레이션된 피드백의 정밀도를 향상시키기 위한 재훈련을 수행할 수 있다. 일 실시예에서, 서버는 개인화된 추천 세션의 생성을 반복하여 복수의 개인화된 추천 세션들로 구성되는 추천 세션 그룹을 생성할 수 있다. 이 경우, 서버는 복수의 추천 세션들 각각이 생성될 때마다 시뮬레이 션된 사용자를 재훈련시킬 수 있다. 즉, 서버는 하나의 추천 세션을 제공한 후에, 다음의 추천 세션을 제 공할 때는 재훈련된 사용자 시뮬레이터를 이용하여 추천 세션을 제공할 수 있다. 일 실시예에서, 강화학습 모델은 시뮬레이션된 사용자로부터 출력되는 시뮬레이션된 피드백을 이용하여 훈련될 수 있다. 강화학습 모델은 시뮬레이션된 사용자에게 추천 세션을 제공하고 시뮬레이션된 사용자와 상호작용하면 서, 각각의 액션 및 액션에 대한 피드백으로 구성되는 추천 이력 h를 수집할 수 있다. 구체적으로, 강화학습 모 델은 운동 추천 세션을 시뮬레이션된 사용자에게 제공하고, 운동 추천 세션에 포함되는 각각의 무브먼트 및 무 브먼트에 대한 피드백으로 구성되는 추천 이력 h를 수집할 수 있다. 추천 이력은 상태에 포함되는 세그먼트 중 하나이므로, 시뮬레이션된 피드백은 강화학습 모델이 시뮬레이션된 사용자와 상호작용하면서 에이전트가 각 상 태에서 최적의 액션을 선택하도록 훈련되는 데 이용될 수 있다. 시뮬레이션된 사용자 및 시뮬레이션된 피드백에 대해 도 7a 내지 도 7b를 참조하여 보다 상세하게 기술한다. 도 7a는 본 개시의 일 실시예에 따른 사용자 시뮬레이터의 동작을 설명하기 위한 도면이다. 일 실시예에서, 서버는 사용자 시뮬레이터에 의해 생성되는 시뮬레이션된 사용자가 실제 사용자의 피드백 f와 유사한 시뮬레이션된 피드백 f′를 출력하도록 사용자 시뮬레이터를 훈련시킬 수 있다. 서버는 사용자 시뮬레이터의 동작을 개선하기 위해, 사용자 시뮬레이터의 하이퍼파라미터 값들을 조정할 수 있다. 하이퍼파 라미터는 사용자 시뮬레이터의 동작을 제어하는 데 사용되는 변수들로, 하이퍼파라미터가 조정됨으로써 사용자 시뮬레이터의 시뮬레이션된 피드백 f′가 실제 사용자의 피드백 f와 유사해지도록 할 수 있다. 일 실시예에서, 상태는 관측 가능한 상태 관측되지 않는 상태(또는, 부분적으로 관측 가능한 상태)로 분류될 수 있다. 예를 들어, 운동 추천 시나리오에서는 사용자 설명 d, 추천 이력 h가 관측 가능한 상태로 분류 되고, 사용자 선호 p 및 사용자 내부 현황 i가 관측되지 않는 상태로 분류될 수 있다. 추천 이력 h는 액션 a 및 피드백 f를 포함할 수 있다. 예를 들어, 추천 이력 h = [(a1, f1), (a2, f2), ..., (an, fn)] 일 수 있다. 관측 가능한 상태는 직접적으로 관측 가능하며 알려진 상태이지만, 관측되지 않는 상태는 추론이 필요하다. 서 버는 사용자 시뮬레이터를 이용하여 관측되지 않는 상태를 추론하고, 관측 가능한 상태(d, h) 및 관측되 지 않는 상태(p, i)에 기초하여 시뮬레이션된 피드백 f′가 생성되도록 할 수 있다. 일 실시예에서, 사용자 시뮬레이터는 관측되지 않는 상태를 추론하기 위한 인코더를 포함할 수 있다. 인코 더는 사용자 설명 d, 액션 a 및 피드백 f를 입력으로 받아, 사용자 선호 p 및 사용 자 내부 현황 i를 출력하는 신경망 모델일 수 있다. 사용자 내부 현황 i는 이전의 상태에 의존하여 결정되므로, 인코더는 시계열적인 정보를 처리하기 위한 LSTM(Long Short Term Memory) 모델로 구현될 수 있다. 다만, 인코더의 구현 방식은 이에 한정되는 것은 아니다. 도 7b를 참조하여, 서버가 시뮬레이션된 피드백을 출력하기 위해 사용자 시뮬레이터를 훈련시키는 동작을 더 기술한다. 도 7b는 본 개시의 일 실시예에 따른 사용자 시뮬레이터의 동작을 설명하기 위한 도면이다. 일 실시예에서, 인코딩은 피드백 f로부터 관측되지 않는 상태로의 매핑(Mapping)을 얻기 위한 과정이다(즉, (p, i) = Encode(f, o, a)). 서버는 사용자 시뮬레이터를 이용하여, 실제 사용자들의 데이 터에 포함되는 관측 가능한 상태를 인코딩하여, 관측되지 않는 상태를 추론할 수 있다. 즉, 사용자 시뮬레이터 는 인코더를 이용하여, 관측 가능한 상태인 사용자 설명 d, 액션 a 및 피드백 f를 인코딩 하여 관측되지 않는 상태인 사용자 선호 p 및 사용자 내부 현황 i를 추론할 수 있다. 인코더는 훈련을 통해 조정될 수 있는 복수의 파라미터들을 포함할 수 있다. 일 실시예에서, 디코딩은 관측 가능한 상태 관측되지 않는 상태에 기반하여 시뮬레이션된 피드백을 생성하기 위 한 과정이다(즉, f′=Decode(z, d, a)). 서버는 사용자 시뮬레이터를 이용하여, 관측 가능한 상태 및 관 측되지 않는 상태를 디코딩하여 시뮬레이션된 사용자의 피드백을 나타내는 시뮬레이션된 피드백을 생성할 수 있 다. 즉, 사용자 시뮬레이터는 피드백 생성자를 이용하여, 사용자 설명 d, 액션 a, 사용자 선호 p 및 사용자 내부 현황 i를 디코딩하여 시뮬레이션된 피드백을 생성할 수 있다. 여기서, 피드백 생성 자가 디코더에 해당된다. 피드백 생성자는 훈련을 통해 조정될 수 있는 복수의 파라미터들을 포함할 수 있다. 서버는 시뮬레이션된 피드백 f′를 생성하고, 시뮬레이션된 피드백 f′와 사용자의 피드백 f를 비교 할 수 있다. 서버는 시뮬레이션된 피드백 f′와 사용자의 피드백 f의 차이가 최소화되도록 함으로써, 시뮬레이션된 사용자가 실제 사용자와 유사한 피드백을 출력하도록 할 수 있다. 즉, 사용자 시뮬레이 터는 시뮬레이션된 피드백 f′와 사용자의 피드백 f의 차이가 최소화되도록 하는 하이퍼파라미터를 찾도록 훈련된다. 일 실시예에서, 서버는 실제 사용자들의 데이터로 구성되는 데이터셋 Dr을 획득할 수 있다. 데이터셋 Dr 은, 서로 다른 복수의 정책들(예를 들어, 정책 A, 정책 B 등)에 의해 생성된 추천 이력(액션 a 및 피드백 f) 및 추천을 제공 받은 사용자 정보(사용자 설명 d)를 포함할 수 있다. 예를 들어, 데이터셋 Dr은 복수의 사용자들 각각에 대응하는 사용자 설명 d, 및 추천 이력 h = [(a1, f1), (a2, f2), ..., (an, fn)]이 포함될 수 있다. 서버는 데이터셋 Dr 에 포함되는 복수의 사용자들에 대하여, 각 사용자들마다 대응하는 데이터를 이용하여 사용자 시뮬레이터를 훈련시킬 수 있다. 즉, 서버는 데이터셋 Dr 내 각각의 사용자의 데이터에 대해 인코 딩 및 디코딩을 수행하여 사용자 시뮬레이터의 하이퍼파라미터를 튜닝할 수 있다. 이는 행동 패턴, 선호도 등의 특성이 사용자들마다 상이하기 때문에 각 사용자의 고유한 상태 특성을 반영하기 위함이다. 예를 들어, 서버는 복수의 사용자들 중 사용자 A에 대응하는 데이터를 이용하여 사용자 시뮬레이터를 훈련시킬 수 있다. 서버는 서로 다른 사용자들로 구성되는 복수의 데이터셋을 이용하여 사용자 시뮬레이터를 훈련시킬 수 있 다. 예를 들어, 서버는 제1 집단의 사용자들로 구성되는 데이터셋 Dr1, 제2 집단의 사용자들로 구성되는 데이터셋 Dr2 등을 이용하여 다양한 사용자 집단의 특성이 반영되도록 사용자 시뮬레이터를 훈련시킬 수 있다. 도 8은 본 개시의 일 실시예에 따른 서버가 사용자 시뮬레이터를 훈련시키는 동작을 설명하기 위한 도면이다. 일 실시예에서, 서버는 사용자 시뮬레이터는 시뮬레이션된 사용자를 생성할 수 있다. 또한, 사용자 시뮬 레이터는 실제 사용자를 모방하여 사용자의 행동을 전반적으로 추적, 관리할 수 있다. 예를 들어, 사용자가 특 정 운동 카테고리를 좋아하거나 싫어하는지, 운동 강도가 사용자의 만족도에 어떤 영향을 미치는지 등을 추적하 고, 사용자의 피트니스 레벨에 따른 운동 강도의 변화 속도 등을 조절할 수 있다. 사용자 시뮬레이터는 하이퍼 파라미터를 포함할 수 있으며, 하이퍼파라미터를 이용하여 전술한 동작들을 수행할 수 있다. 서버는 사용자 시뮬레이터가 실제 사용자의 행동을 모사할 수 있도록 하이퍼파라미터의 값들을 튜닝 하여 하이퍼파라미터를 최적화할 수 있다. 예를 들어, 서버는, 시뮬레이션된 사용자의 시뮬레이션된 피드백이 실제 사용자의 피드백과 유사해지도록 하기 위해, 하이퍼파라미터의 적어도 일부를 조 정하여 시뮬레이션된 사용자의 선호도 등이 실제 사용자의 선호도와 유사해지도록 할 수 있다. 일 실시예에서, 서버는 하이퍼파라미터의 초기값이 설정된 사용자 시뮬레이터를 이용하여 강화학습 모델의 정책을 훈련시킬 수 있다. 이하에서, 강화학습 모델 및 사용자 시뮬레이터의 훈련 과정을 기술한다. 서버는 강화학습 모델의 정책에 기초하여 실제 사용자에게 추천 요소를 제공하고, 실제 사용자의 데이터 를 실제 사용자 데이터셋 Dr에 축적할 수 있다. 또한, 서버는 강화학습 모델의 정책에 기초하여 시뮬레이 션된 사용자에게 추천 요소를 제공하고, 시뮬레이션된 사용자의 데이터를 시뮬레이션된 사용자 데이터셋 Ds 에 축적할 수 있다. 실제 사용자 데이터셋 Dr 및 시뮬레이션된 사용자 데이터셋 Ds 에는, 각각 사용자 설명, 추천 이 력(액션, 피드백)이 포함될 수 있다. 서버는 시뮬레이션된 사용자 데이터셋 Ds 을 이용하여 인코더 및 디코더(피드백 생성자, 미도시)를 훈련시킬 수 있다. 인코더는 관측되지 않는 상태(시뮬레이션된 사용자의 선호도를 포함)를 추론하도 록 훈련되며, 디코더는 시뮬레이션된 피드백을 생성하도록 훈련된다. 서버는 훈련된 인코더 및 훈련된 디코더를 이용하여 실제 사용자의 시뮬레이션된 피드백을 생 성할 수 있다. 서버는 시뮬레이션된 피드백과 실제 사용자 피드백 사이의 차이를 최소화하는, 하이퍼파라미터 최적화를 수행할 수 있다. 하이퍼파라미터의 조정 결과, 인코더는 실제 사용자 의 예측된 선호도를 추론할 때 실제 사용자의 수동 라벨링된 선호도에 근접하도록 선호도를 추론하고, 디코더는 실제 사용자의 시뮬레이션된 피드백을 생성할 때 실제 사용자의 피드백에 근접하 도록 피드백을 생성할 수 있다. 일 실시예에서, 서버는 사용자 시뮬레이터를 점진적으로 훈련시킬 수 있다. 예를 들어, 사용자의 수가 증 가함에 따라, 사용자 데이터 수도 증가할 수 있다. 서버는 사용자 수가 기 설정된 기준만큼 증가할 때마 다, 전술한 사용자 시뮬레이터의 훈련 동작을 반복하여 수행할 수 있다. 예를 들어, 서버는 복수의 사용 자들로 구성되는 데이터셋 Dr1 을 이용하여 사용자 시뮬레이터를 훈련시키고, 시간이 흐름에 따라 사용자 데이터 수가 증가하면, 증가된 사용자들을 포함하는 데이터셋 Dr2 을 이용하여 사용자 시뮬레이터를 재훈련시킬 수 있다. 또는, 서버는 다양한 조건에 기초하여 사용자 시뮬레이터를 재훈련시킬 수 있다. 예를 들어, 서버 는, 기 설정된 시간이 경과하거나, 사용자가 추천 제공 서비스를 이용하여 새로운 데이터가 획득되는 경 우 등에 사용자 시뮬레이터를 재훈련시킬 수 있다. 단, 서버가 사용자 시뮬레이터를 재훈련시키는 조건은 전술한 예시에 한정되는 것은 아니다. 일 실시예에서, 서버는 실제 사용자 피드백에 포함되는 피드백들 중에서 일부만을 훈련 데이터로 사 용할 수 있다. 서버는 기설정된 기준에 기초하여 실제 사용자 피드백에 포함되는 피드백들 중에서 일부를 선택할 수 있다. 예를 들어, 피드백은 긍정적, 부정적, 중립적인 피드백의 세 가지 유형으로 나뉘어질 수 있다. 구체적으로, 사용자가 무브먼트를 좋다고 하는 경우 긍정적 피드백에 해당하고, 사용자가 무브먼트를 더 어려운 것으로 바꾸는 것은 부정적 피드백에 해당할 수 있으며, 사용자가 무브먼트를 완료한 경우는 중립적인 피드백에 해당할 수 있다. 한편, 복수의 피드백 유형들 중에서, 중립적인 피드백은 추천 요소에 대한 사용자의 선호가 정확하게 반영되지 않을 수 있다. 예를 들어, 사용자가 이전에 이미 추천 요소에 대하여 긍정적/부정적인 피드백을 준 적이 있거나, 추천 요소를 극도로 좋아하거나 싫어하는 경우에도 중립적인 피드백을 남기는 경우가 있을 수 있다. 따 라서, 서버는 실제 사용자 피드백의 유형을 식별하고, 피드백 유형 중에서 특정 유형의 피드백은 제 외할 수 있다. 예를 들어, 서버는 중립적 피드백을 제외한 후에 남은 데이터를 이용하여 사용자 시뮬레이 터의 훈련 과정을 수행할 수 있다. 도 9는 본 개시의 일 실시예에 따른 서버가 강화학습 모델을 훈련시키기 위한 데이터를 생성하는 동작을 설명하 기 위한 도면이다. 도 9를 참조하면, 도 9에 도시된 동작들은 서버에 의해 수행된다. 예를 들어, 서버가 강화학습 모 델의 사용자 시뮬레이터를 이용하거나, 생성자를 이용하여 데이터 처리를 수행하는 것이다. 동작 S910에서, 모든 시뮬레이션된 사용자들이 생성자(에이전트)에게 상태를 전송한다. 일 실시예에서, 사용자 시뮬레이터에 의해 복수의 시뮬레이션된 사용자들이 생성되어 있을 수 있다. 이 경우, 서버는 모든 시뮬 레이션된 사용자들(또는, 모든 시뮬레이션된 사용자들 중 일부일 수도 있다.)에게 동작할 것을 요청할 수 있다. 서버가 모든 시뮬레이션된 사용자들에게 동작할 것을 요청한 것에 응답하여, 모든 시뮬레이션된 사용자들 각각은 추천 세션을 제공받기 위해 생성자에게 현재 상태를 전송할 수 있다. 예를 들어, 운동 추천 시나리오에 서는 사용자 설명(Description) d, 사용자 선호도 p, 사용자 내부 현황 i, 및 추천 이력 h 중 적어도 하나를 포 함하는 상태가 생성자로 전달될 수 있다. 동작 S920에서, 생성자는 시뮬레이션된 사용자들 각각에게 추천 요소를 출력하고 추천 요소 리스트에 저장한다. 생성자는 시뮬레이션된 사용자들 각각의 상태를 관측하고 시뮬레이션된 사용자들 각각의 상태에 기초하여 추천 요소를 선택하는 액션을 수행한다. 생성자는 추천 요소 데이터베이스에 저장된 추천 요소 후보들 중에서 하나를 선택하여 각 시뮬레이션된 사용자에게 제공할 수 있다. 각각의 시뮬레이션된 사용자들에게 제공되는 추 천 요소는 같거나 상이할 수 있다. 동작 S930에서, 시뮬레이션된 사용자들 각각은 추천 요소에 기초하여 현재 상태를 업데이트한다. 예를 들어, 시 뮬레이션된 사용자들 각각이 무브먼트 추천을 제공 받음에 따라, 시뮬레이션된 사용자들 각각의 사용자 내부 현 황 i, 추천 이력 h와 같은 동적인 상태들이 업데이트될 수 있다. 동작 S940에서, 시뮬레이션된 사용자들 각각은 업데이트된 상태 및 리워드를 출력한다. 업데이트된 상태 및 리 워드는 생성자로 전달되어, 생성자가 다음 액션을 결정하도록 할 수 있다. 예를 들어, 시뮬레이션된 사용자들 각각의 업데이트된 상태 및 리워드는, 생성자가 이전에 추천된 무브먼트를 사용자가 수행한 다음에 수행할 또 다른 무브먼트를 결정하는 데 이용될 수 있다. 서버는 시뮬레이션된 사용자들의 이전 상태 및 다음 상태, 액션 및 리워드를 트레이닝 데이터베이스에 저장할 수 있다. 트레이닝 데이터베이스에 저장된 데이터 는 강화학습 모델을 훈련시키기 위한 데이터로 이용될 수 있다. 도 10a는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스를 제공하기 위해 사용자 데이터를 획 득하는 동작을 설명하기 위한 도면이다. 일 실시예에서, 서버는 개인화된 추천 제공 서비스를 사용자에게 제공할 수 있다. 예를 들어, 서버(200 0)는 개인화된 추천 제공 서비스를 애플리케이션을 이용하여 제공할 수 있고, 사용자는 사용자 개인의 전 자 장치(예를 들어, 모바일 폰 등)를 이용하여 애플리케이션을 실행함으로써 서버에 접속하고 서비 스를 제공받을 수 있다. 일 실시예에서, 서버는 애플리케이션을 통하여 사용자로부터 사용자 데이터를 획득할 수 있다. 서 버는 사용자가 애플리케이션을 이용하여 사용자 데이터를 입력 할 수 있도록 하기 위한 다양한 데 이터 및 기능을 제공할 수 있다. 예를 들어, 서버는 사용자가 데이터를 간편하게 입력할 수 있도록 미리 정해진 템플릿의 그래픽 인터페이스를 제공하고, 사용자로부터 사용자 입력을 수신할 수 있다. 일 실시예에서, 애플리케이션은 운동 추천 서비스를 제공하는 애플리케이션일 수 있다. 이 경우, 서버 는 운동 추천을 제공하기 위해 운동과 관련된 사용자 데이터를 획득할 수 있다. 예를 들어, 사용자 데이 터 입력을 위한, 애플리케이션의 제1 화면을 참조하면, 서버는 사용자 전자 장치의 애플리케 이션을 통해 성별을 입력하는 사용자 입력을 수신할 수 있다. 또는 예를 들어, 사용자 데이터 입력을 위한, 애플리케이션의 제2 화면을 참조하면, 서버는 사용자 전자 장치의 애플리케이션을 통해 현재 신장 및 체중을 입력하는 사용자 입력을 수신할 수 있다. 일 실시예에서, 서버는 사용자 데이터를 획득하기 위해 추천 요소와 관련된 질문을 생성할 수 있다. 예를 들어, 서버는 사용자의 운동 능력에 관한 정보를 얻기 위해, 운동 능력에 대한 질문을 생성할 수 있다. 구체적으로, 사용자 데이터 입력을 위한, 애플리케이션의 제3 화면을 참조하면, 서버는 사용 자 전자 장치의 애플리케이션을 통해 출력되는 질문(예를 들어, 한 번에 \"Push Up\"을 10개 이상 할 수 있습니까?)에 대해 응답하는 사용자 입력을 수신할 수 있다. 한편, 서버가 획득하는 사용자 데이터는 전술한 예시에 한정되는 것은 아니다. 예를 들어, 서버는 운동 추천 시나리오에 대하여, 사용자와 관련된 개인 정보(예를 들어, 나이, 성별 등) 및 추천 카테고리인 운동 과 관련된 사용자 입력 정보(예를 들어, 운동 목표, 운동 스타일, 운동 기간, 집중 근육 부위, 운동 숙련도, 체 력 수준 등)를 포함하는 사용자 데이터를 획득할 수 있다. 도 10b는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스를 제공하기 위해 사용자 데이터를 획 득하는 동작을 설명하기 위한 도면이다. 일 실시예에서, 서버는 추천 카테고리와 관련된 사용자 데이터를 획득할 수 있다. 예를 들어, 추천 카테 고리가 운동일 때, 서버는 운동 추천을 제공하기 위해 운동과 관련된 사용자 데이터 입력을 위한 양식을 생성할 수 있다. 예를 들어, 서버는 \"운동 계획 세우기\"와 같은 데이터 입력 양식을 생성할 수 있다. 구체적으로, 사용자 데이터 입력을 위한, 애플리케이션의 제4 화면을 참조하면, 서버는 운동 계획 세우기 양식을 생성하고, 사용자로부터 운동 시작일, 운동 스타일 등을 선택하는 사용자 입력을 수신할 수 있다. 운동 스타일은 예를 들어, 고강도 인터벌 트레이닝(High Intensity Interval Training; HIIT)을 포함할 수 있다. 고강도 인터벌 트레이닝의 유형으로 써킷 트레이닝, 주니가 요법(Zuniga regimen), 타바타 요법 (Tabata regimen), 기발라 요법(Gibala regimen) 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 또는, 사용자 데이터 입력을 위한, 애플리케이션의 제5 화면을 참조하면, 서버는 운동 계획 세우기 양식을 생성하고, 사용자로부터 집중 근육 부위를 선택하는 사용자 입력을 선택할 수 있다. 집중 근육 부위는 예를 들어, 복근, 등, 이두근, 가슴, 엉덩이, 뒤 허벅지, 앞 허벅지, 어깨, 삼두근 등을 포함할 수 있으 나, 이에 한정되는 것은 아니다. 한편, 서버가 제공하는 개인화된 추천의 추천 카테고리는 운동에 한정되는 것은 아니다. 예를 들어, 식단 추천, 미디어 콘텐츠 추천 등 다양한 카테고리에 대한 추천이 제공될 수 있다. 서버는 각각의 추천 카테 고리에 대응하는 사용자 데이터를 획득할 수 있는 양식을 사용자에게 제공하여, 카테고리별로 최적화된 추천을 제공할 수 있다. 일 실시예에서, 서버는 사용자 데이터를 입력 받으면, 사용자 데이터를 기초로 추천을 제공 받는 실제 사 용자에 대응하는 가상의 사용자를 나타내는 시뮬레이션된 사용자를 생성할 수 있다. 시뮬레이션 된 사용자는 강 화학습 시스템에 환경에 대응하며, 상태에 포함되는 정보 중 적어도 일부는 사용자로부터 획득된 사용자 데이터 에 기초하여 설정될 수 있다. 예를 들어, , 사용자와 관련된 개인 정보(예를 들어, 나이, 성별 등) 및 추천 카 테고리인 운동과 관련된 사용자 입력 정보(예를 들어, 운동 목표, 운동 스타일, 운동 기간, 집중 근육 부위, 운 동 숙련도, 체력 수준 등)를 포함하는 사용자 설명 d가 사용자 데이터에 기초하여 설정될 수 있다. 또는, 무브 먼트 선호도, 무브먼트 난이도 선호도, 다양성 선호도, 심박수 선호도, 피드백 선호도(특정 타입의 피드백 제공 확률) 등을 포함하는 사용자 선호도 p 등이 사용자 데이터에 기초하여 설정될 수 있다. 도 11a는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스를 제공하는 동작을 설명하기 위한 도 면이다. 일 실시예에서, 서버는 사용자로부터 추천 서비스 제공 요청을 수신하면, 사용자 시뮬레이터를 이용하여 시뮬레이션된 사용자를 생성하고, 시뮬레이션된 사용자와 추천 생성자의 상호작용을 통해 추천 요소의 결정을 반복하여 추천 세션을 생성할 수 있다. 서버가 추천 세션을 생성하는 동작은 전술하였으므로, 반복되는 설명은 생략한다. 일 실시예에서, 서버는 개인화된 추천 제공 서비스를 애플리케이션을 이용하여 제공할 수 있다. 이 경우, 사용자는 사용자 개인의 전자 장치(예를 들어, 모바일 폰 등)를 이용하여 애플리케이션을 실행함으로써 서버에 접속하고 추천 세션을 제공받을 수 있다. 하나의 추천 세션은, 복수 개의 추천 요소들을 포함할 수 있다. 예를 들어, 추천 세션은 기 설정된 개수 N개의 추천 요소들로 구성될 수 있다. 구체적으로, 추천 세션을 제공하는 애플리케이션의 제1 화면을 참조하면, 서버는 16개의 무브먼트들로 구성되는 운동 추천 세션을 사용자에게 제공할 수 있다. 제1 화면에는 추천 요소들의 리스트, 추천 요소 항목, 추천 요소와 관련된 콘텐츠(예를 들어, 동영상) 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 구체적으로, 제1 화면에는 무브먼트들의 리스트, 무브먼트의 항목, 수행 지속 시간, 무브먼트를 따라할 수 있는 동영상 등이 포함될 수 있다. 또한, 제1 화면에는 사용자로 부터 추가 데이터를 획득하기 위한 가이드가 포함될 수 있다. 구체적으로, \"심박수를 측정하기 위해 스마트 워 치를 연결하세요\"와 같은 가이드가 제1 화면에 포함될 수 있다. 이 경우, 서버는 사용자의 또다른 전자 장치로부터 센서 데이터를 수신하고, 센서 데이터를 추가적으로 이용하여 사용자에게 추천 운동 세션을 제 공할 수 있다. 예를 들어, 서버는 사용자의 스마트 워치로부터 심박수 데이터를 수신하고, 심박수 데이터 를 이용하여 사용자에게 추천 무브먼트들로 구성되는 추천 운동 세션을 제공할 수 있다. 도 11b는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스에 대한 피드백을 획득하는 동작을 설 명하기 위한 도면이다. 일 실시예에서, 서버는 사용자가 추천 세션을 제공받는 동안 사용자로부터 피드백을 획득할 수 있다. 서 버는 추천 세션을 제공받는 도중에 입력되는 사용자 피드백에 기초하여, 현재 추천 세션에 포함되는 추천 요소들 중 적어도 일부를 변경할 수 있다. 또는, 서버는 추천 세션을 제공받는 도중에 입력되는 사용자 피드백에 기초하여, 다음 추천 세션에 포함되는 추천 요소들 중 적어도 일부를 변경할 수 있다. 일 실시예에서, 사용자 피드백은 추천 요소(또는, 추천 요소의 속성)를 변경하기 위한 피드백일 수 있다. 예를 들어, 추천 세션을 제공하는 애플리케이션의 제2 화면을 참조하면, 서버는 현재 추천되는 추천 요 소와 함께, 현재 추천 요소를 대체할 수 있는 적어도 하나의 다른 추천 요소를 사용자에게 제공할 수 있다. 구 체적으로, 제2 화면에는 현재 제공되는 추천 무브먼트인 \"힐 탭(Heel Taps)\"이 표시될 수 있고, \"힐 탭\" 을 대체할 수 있는 다른 추천 무브먼트인 \"할로우 홀드(Hollow Hold)\", \"크런치(Crunch)\" 등이 표시될 수 있다. 또한, 서버는 대체 추천 요소를 제공할 때, 대체 추천 요소에 관련된 정보를 사용자에게 제공할 수 있다. 구체적으로, 제2 화면에는 대체 추천 무브먼트인 \"할로우 홀드\"에 대하여 난이도 \"쉬움\"이 함께 표시될 수 있고, 대체 추천 무브먼트인 \"크런치\"에 대하여 난이도 \"어려움\"이 함께 표시될 수 있다. 서버는 사용 자가 대체 추천 요소를 선택하는 경우, 현재 추천 세션 또는 다음 추천 세션의 추천 요소를 선택된 대체 추천 요소로 변경할 수 있다. 또는, 제2 화면에는 도시되지 않았으나, 사용자의 피드백에 기초하여 추천 요소의 속성이 변경될 수 있다. 예를 들어, 사용자가 난이도를 \"어려움\"으로 조정하는 경우, 서버는 전술한 예시처럼 무브먼트를 변경할 수도 있으나, 무브먼트를 변경하지 않더라도 무브먼트의 속성(예를 들어, 수행 횟수, 수행 시간 등)을 변경하여 무브먼트의 난이도를 증가시킬 수도 있다. 일 실시예에서, 서버가 사용자 피드백을 반영하여 추천 세션을 변경하는 것은, 강화학습 모델을 재훈련시 켜 업데이트하는 과정을 포함할 수 있다. 도 11c는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스에 대한 피드백을 획득하는 동작을 설 명하기 위한 도면이다. 일 실시예에서, 서버는 사용자가 추천 세션을 제공받은 후에 사용자로부터 피드백을 획득할 수 있다. 서 버는 추천 세션을 제공받은 후에 입력되는 사용자 피드백에 기초하여, 다음 추천 세션에 포함되는 추천 요소들 중 적어도 일부를 변경할 수 있다. 일 실시예에서, 사용자 피드백은 추천 요소(또는, 추천 요소의 속성)를 변경하기 위한 피드백일 수 있다. 예를 들어, 추천 세션을 제공하는 애플리케이션의 제3 화면을 참조하면, 서버는 추천 세션을 제공한 이 후에, 추천 세션에 포함되는 추천 요소들에 대한 피드백 메뉴를 사용자에게 제공할 수 있다. 추천 요소들에 대 한 피드백 메뉴는, 추천 요소들 각각에 대한 개별적인 피드백을 제공할 수 있도록 구성될 수 있다. 구체적으로, 제3 화면에는 최근 제공된 운동 추천 세션에 포함되는 추천 무브먼트들인 \"리버스 런지(Reverse Lunge)\", \"어시스트 스쿼트(Assisted Squat)\", \"리버스 플랭크(Reverse Plank)\", 및 \"사이드 플랭크(Side Plank)\" 등에 대하여 피드백을 할 수 있는 메뉴가 표시될 수 있다. 서버는 사용자가 추천 요소에 대한 피드백을 제공하 는 경우, 추천 요소에 대한 피드백에 기초하여 추천 요소를 변경할 수 있다. 예를 들어, 제3 화면을 참조하면, 사용자가 \"어시스트 스쿼트\"의 난이도를 낮추도록 피드백하고, \"리버스 플랭크\"의 난이도는 높이도록 피 드백했을 수 있다. 서버는 이 경우, 다음 추천 세션을 제공할 때 사용자의 피드백이 반영하여 추천 요소 를 변경하거나 추천 요소의 속성을 조정할 수 있다. 일 실시예에서, 서버가 사용자 피드백을 반영하여 추천 세션을 변경하는 것은, 강화학습 모델을 재훈련시 켜 업데이트하는 과정을 포함할 수 있다. 도 11d는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스에 대한 피드백을 획득하는 동작을 설 명하기 위한 도면이다. 도 11d는 서버가 사용자로부터 피드백을 획득하기 위해 제공하는 그래픽 인터페이스의 다양한 방법 중 일 부를 예시로서 도시한다. 예를 들어, 추천 세션을 제공하는 애플리케이션의 제4 화면을 참조하면, 서버는 추천 세션에 대한 간편 피드백 메뉴를 제공할 수 있다. 간편 피드백은, 추천 세션에 포함되는 추천 요소들 각각에 대한 피드백을 요청하는 것이 아닌, 추천 세션에 대해 전반적인 피드백 요청하는 것일 수 있으나, 이에 한정되는 것은 아니다. 구체적으로, 서버는 운동 추천 세션을 제공한 뒤에, 운동 추천 세션에 대한 평가를 요청하는 질문을 생성 할 수 있다. 예를 들어, 서버는 \"운동 강도는 적당합니까?\", \"추천된 운동들이 만족스럽습니까?\"와 같은 질문을 제공하고, 질문에 응답하는 사용자 피드백을 획득할 수 있다. 서버는 사용자 피드백이 획득되면, 피드백에 기초하여 추천 세션의 적어도 일부를 변경할 수 있다. 서버는 간편 피드백 메뉴를 제공함으로써 사용자가 추천 세션 내 추천 요소에 대하여 일일이 평가하는 번 거로움 없이, 개인화된 추천 세션을 간편하게 최적화하도록 할 수 있다. 또다른 예를 들어, 추천 세션을 제공하는 애플리케이션의 제5 화면을 참조하면, 서버는 추천 세션 의 변경을 가이드하는 피드백 메뉴를 제공할 수 있다. 추천 세션의 변경 가이드는, 사용자에게 변경 가능한 추 천 요소를 가이드하고, 변경 여부에 대한 피드백을 요청하는 것일 수 있으나, 이에 한정되는 것은 아니다. 구체 적으로, 서버는 더 어려운 동작의 잠금 해제가 가능함을 가이드할 수 있다. 예를 들어, 서버는 현 재 완료한 무브먼트가 \"마운틴 클라이머(Mountain Climber)\" 이고, 다음 난이도의 무브먼트인 \"트위스트 마운틴 클라이머(Twisting Mountain Climber)\"를 해제할 수 있음을 가이드하고, 해제 여부를 응답하는 사용자 피드백을 획득할 수 있다. 서버는 사용자 피드백이 획득되면, 피드백에 기초하여 추천 세션의 적어도 일부를 변경 할 수 있다. 서버는 가이드 피드백 메뉴를 제공함으로써, 사용자가 추천 요소를 변경하고 싶더라도 무엇으로 변경해야 할지는 잘 모르는 경우에도 가이드에 기초하여 추천 요소를 변경할 수 있도록 할 수 있다. 이에 따라, 서버 는 사용자가, 개인화된 추천 세션을 손쉽게 최적화하도록 할 수 있다. 일 실시예에서, 서버가 사용자 피드백을 반영하여 추천 세션을 변경하는 것은, 강화학습 모델을 재훈련시 켜 업데이트하는 과정을 포함할 수 있다. 도 12는 본 개시의 일 실시예에 따른 서버가 제공된 추천 세션과 관련된 정보를 추가로 제공하는 동작을 설명하 기 위한 도면이다. 일 실시예에서, 서버는 개인화된 추천 세션을 사용자에게 제공하면서, 개인화된 추천 세션과 관련된 정보 를 사용자에게 추가로 제공할 수 있다. 개인화된 추천 세션과 관련된 정보는 예를 들어, 제공된 추천 세션의 요 약 정보, 추천 세션을 제공받은 사용자의 정보 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 추천 세션과 관련된 정보는 추천 세션의 카테고리마다 상이할 수 있다. 이하, 추천 카테고리가 운동인 경우를 예시로 설명한다."}
{"patent_id": "10-2023-0118553", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "추천 세션을 제공하는 애플리케이션의 제1 화면을 참조하면, 제1 화면에는 운동 결과를 요약한 정"}
{"patent_id": "10-2023-0118553", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "보들이 포함될 수 있다. 운동 결과 요약 정보에는 운동 시간, 운동 목표, 심박수 결과, 운동 추천 세션에 포함 되는 무브먼트들의 정보 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 추천 세션을 제공하는 애플리케이션의 제2 화면을 참조하면, 제2 화면에는 운동을 제공받은 사용자 의 정보인 운동 성취도를 나타내는 정보들이 포함될 수 있다. 운동 성취도 정보에는 사용자가 수행한 무브먼트 들, 각각의 무브먼트들의 성취도, 각각의 무브먼트들의 난이도 등이 포함될 수 있으나, 이에 한정되는 것은 아 니다.추천 세션을 제공하는 애플리케이션의 제3 화면을 참조하면, 제3 화면에는 운동을 제공받은 사용자 의 정보인 현재 운동 수준을 나타내는 무브먼트 트리가 포함될 수 있다. 무브먼트 트리에는 사용자의 성취도에 따라 잠금이 해제된 무브먼트들 및 각 무브먼트들에 관련된 정보들이 포함될 수 있으나, 이에 한정되는 것은 아 니다. 일 실시예에 따른 서버는 개인화된 추천 세션과 관련된 정보를 추가적으로 제공함으로써, 개인화 효과를 강화할 수 있다. 구체적으로, 도 12에 도시된 것과 같이, 서버는 운동 추천 세션을 제공하면서, 운동 결 과 분석에 기초한 관련 정보(예를 들어, 운동 결과, 성취도, 다음 운동을 제안하는 무브먼트 트리 등)를 제공함 으로써, 사용자가 개인화된 추천을 제공받는 경험을 강화할 수 있다. 도 13은 본 개시의 일 실시예에 따른 서버가 개인화된 추천 세션을 제공하는 예시를 설명하기 위한 도면이다. 서버는 다양한 카테고리에 대해 개인화된 추천 세션을 제공할 수 있다. 도 13에서는 개인화된 식단 추천 의 예시를 설명한다. 단, 이는 본 개시의 기술적 사상이 다양한 추천 카테고리에 적용 가능함을 설명하기 위한 예시일 뿐, 추천 카테고리를 한정하기 위한 것은 아니다. 일 실시예에서, 특정 추천 카테고리에 대하여 사용자에게 개인화된 추천 세션을 제공하는 것은 여러 제약 사항 을 충족해야 한다. 제약 사항의 예를 들면, 식단 추천의 경우, 사용자에게 주간 식사 계획의 제공이 필요하고, 계획된 식단을 여러 주 동안 참여시킬 필요가 있으며, 사용자의 선호도, 알러지, 이용 가능한 식품/재료를 반영 하는 다양한 식사를 계획하는 것이 필요하다. 또한, 식재료의 다양성이 음식 낭비를 유발하지 않아야 하고, 식 단을 지키는 데 필요한 비용이 예산 내에서 유지되어야 한다. 서버는 사용자 시뮬레이터를 포함하는 강화 학습 모델을 이용함으로써, 여러 제약사항들을 충족하면서도 사용자에게 개인화된 식단 추천 세션을 제공할 수 있다. 일 실시예에서, 서버는 강화학습 모델을 이용하여 사용자에게 식단 계획을 제공할 수 있다. 식단 계획은 복수의 식단 추천 세션들을 포함할 수 있다. 예를 들어, 도 13에 도시된 것과 같이, 1주차 식단인 Week 1, 2주차 식단인 Week 2, ..., K주차 식단인 Week K이 식단 계획에 포함될 수 있다. 일 실시예에서, 강화학습 모델은 식단 추천을 생성하는 액션을 결정하는 식단 추천 생성자 및, 합성 데이 터를 생성하여 식단 추천 생성자와 상호작용을 하는 사용자 시뮬레이터를 포함할 수 있다. 식단 추천 시나리오에서, 사용자 시뮬레이터는 식단 추천을 제공 받는 가상의 사용자를 나타내는 시뮬레 이션된 사용자를 생성할 수 있다. 시뮬레이션된 사용자는 식단 추천 생성자에게 상태를 전달하고, 식단 추천 생성자는 추천 요소인 식사(Meal)를 결정한다. 상태는 주어진 시점의 리워드와 후속 상태로의 전이 를 결정하는 데 필요한 모든 정보가 포함될 수 있다. 상태는 다양한 세그먼트들로 구분될 수 있다(s = Concat(s1, s2, s3, ..., sn)). 식단 추천 시나리오에서, 상태는 사용자 설명(Description) d, 사용자 선호도 p, 사용자 내부 현황 i, 및 추천 이력 h 중 적어도 하나를 포함할 수 있다. 상태에 포함되는 사용자 설명은, 사용자와 관련된 개인 정보(예를 들어, 나이, 성별 등) 및 추천 카테고리인 식 단과 관련된 사용자 입력 정보(예를 들어, 음식 알러지, 지역별 식재료 제약, 사용자 예산, 사용자 식사 장소, 냉장 보관 여부, 조리 장소 접근 가능 여부, 초기 체중, 혈당 등)를 포함할 수 있으나, 이에 한정되는 것은 아 니다. 상태에 포함되는 사용자 선호도는, 음식 카테고리 선호도, 음식 조리 시간 선호도, 사용할 수 있는 재료 선호도, 레시피 복잡도, 음식/성분 다양성 선호도, 피드백 선호도(예를 들어, 사용자가 음식/재료를 스킵/대체 하는 등의 피드백 제공 확률)를 포함할 수 있으나, 이에 한정되는 것은 아니다. 상태에 포함되는 사용자 내부 현황은, 사용자가 추천을 제공받음에 따라 영향을 받는 요소들을 포함할 수 있다. 예를 들어, 사용자의 체중, 혈당, 싫어하는 재료 또는 복잡한 레시피가 추천될 때 재료를 건너뛸 가능성 등이 사용자 내부 현황에 포함될 수 있으나, 이에 한정되는 것은 아니다. 상태에 포함되는 추천 이력은, 사용자에게 추천된 추천 요소들 및 추천 요소에 대한 피드백들의 이력을 포함할 수 있다. 예를 들어, 식단 추천 세션에 포함되는 추천 요소들인 음식 및 음식에 대한 피드백 등이 추천 이력에 포함될 수 있다.한편, 상태에 포함되는 사용자 내부 현황, 추천 이력의 경우, 동적인 것으로 분류될 수 있다. 상태 전이 함수에 서, 동적인 세그먼트는 다음 상태로 전이될 때 변화하도록 정의될 수 있다. 이 경우, 동적인 세그먼트는 다시 \"결정적인 것(Deterministic, p=1)\" 또는 \"비결정적인 것(Non-deterministic, p≠1)\"으로 분류될 수 있다. 예 를 들어, 추천 이력은 동적인 세그먼트이지만, 현재 액션으로 결정된 추천 요소를 추천 세션의 리스트의 끝에 새로 부가하는 것이므로 결정적인 것으로 분류될 수 있다. 또는, 추천 요소에 대한 좋음/싫음과 같은 사용자 피 드백은, 확률에 의해 정의될 수 있으므로 비결정적인 것으로 분류될 수 있다. 마찬가지로, 사용자 내부 현황의 상태 전이도 비결정적인 것으로 분류될 수 있다. 사용자 내부 현황은 관측 불가능하거나 부분적으로 관측 가능 한 상태이므로, 사용자의 내부 현황을 추적 및 예측 가능한 내부 모델(Internal model)에 의해 정의되고 제어될 수 있다. 리워드는 부정적인 리워드 및 긍정적인 리워드를 포함하도록 설계될 수 있다. 예를 들어, 부정적인 리워드는 추 천 요소를 사용자가 좋아하지 않는 경우를 포함할 수 있고, 긍정적인 리워드는 추천 요소를 사용자가 좋아하는 경우를 포함할 수 있다. 리워드는 예를 들어, 추천된 음식의 식사 간의 다양성, 음식 재료 비용, 음식 영양 성 분 다양성, 음식에 대한 좋음/싫음 피드백, 체중/혈당 목표 달성 여부 등을 반영하여 설계될 수 있다. 한편, 전술한 것과 같은 사용자 시뮬레이터를 이용하는 강화학습 시스템의 구성 요소들 외에, 강화학습 시스템 에서 에이전트와 환경 사이의 상호작용을 포함하는 구체적인 동작들에 대해서는, 이전의 도면들에서 전술하였으 므로, 반복되는 설명은 생략한다. 도 14는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 세션을 제공하는 예시를 설명하기 위한 도면이다. 서버는 다양한 카테고리에 대해 개인화된 추천 세션을 제공할 수 있다. 도 14에서는 개인화된 미디어 콘 텐츠 추천의 예시를 설명한다. 단, 이는 본 개시의 기술적 사상이 다양한 추천 카테고리에 적용 가능함을 설명 하기 위한 예시일 뿐, 추천 카테고리를 한정하기 위한 것은 아니다. 일 실시예에서, 특정 추천 카테고리에 대하여 사용자에게 개인화된 추천 세션 제공을 제공하는 것은 여러 제약 사항을 충족해야 한다. 제약 사항의 예를 들면, 미디어 콘텐츠 추천의 경우, 사용자에게 오늘의 콘텐츠를 추천 하고, 사용자가 장기적으로 시스템에 참여할 필요가 있으며, 사용자의 선호도(장르, 재생시간, 시청 시간대)를 반영하여야 하고, 다양한 콘텐츠를 다루어야 한다. 또한, 매일의 일정한 콘텐츠 시청 스케줄 제공이 필요하고, 다양한 콘텐츠 플랫폼(예를 들어, N 콘텐츠 플랫폼, Y 콘텐츠 플랫폼 등)에서 제공되는 콘텐츠를 통합하여 추천 하는 것이 필요할 수 있고, 이전 콘텐츠 시청 기록에 기초하여 다음 콘텐츠가 추천 되어야 한다. 서버는 사용자 시뮬레이터를 포함하는 강화학습 모델을 이용함으로써, 여러 제약사항들을 충족하면서도 사용자에게 개 인화된 미디어 콘텐츠 추천 세션을 제공할 수 있다. 일 실시예에서, 서버는 강화학습 모델을 이용하여 사용자에게 미디어 콘텐츠 추천을 제공할 수 있 다. 미디어 콘텐츠 추천은 복수의 미디어 콘텐츠 추천 세션들을 포함할 수 있다. 예를 들어, 도 4에 도시 된 것과 같이, 1일차 미디어 콘텐츠인 Day 1, 2일차 미디어 콘텐츠 Day 2, ..., K일차 미디어 콘 텐츠인 Day K이 미디어 콘텐츠 추천에 포함될 수 있다. 일 실시예에서, 강화학습 모델은 미디어 콘텐츠 추천을 생성하는 액션을 결정하는 미디어 콘텐츠 추천 생성자 및, 합성 데이터를 생성하여 미디어 콘텐츠 추천 생성자와 상호작용을 하는 사용자 시뮬레이터 를 포함할 수 있다. 미디어 콘텐츠 추천 시나리오에서, 사용자 시뮬레이터는 미디어 콘텐츠 추천을 제공 받는 가상의 사용자 를 나타내는 시뮬레이션된 사용자를 생성할 수 있다. 시뮬레이션된 사용자는 미디어 콘텐츠 추천 생성자 에게 상태를 전달하고, 미디어 콘텐츠 추천 생성자는 추천 요소인 콘텐트를 결정한다. 상태는 주어진 시 점의 리워드와 후속 상태로의 전이를 결정하는 데 필요한 모든 정보가 포함될 수 있다. 상태는 다양한 세그먼트 들로 구분될 수 있다(s = Concat(s1, s2, s3, ..., sn)). 미디어 콘텐츠 추천 시나리오에서, 상태는 사용자 설명(Description) d, 사용자 선호도 p, 사용자 내부 현황 i, 및 추천 이력 h 중 적어도 하나를 포함할 수 있다. 상태에 포함되는 사용자 설명은, 사용자와 관련된 개인 정보(예를 들어, 나이, 성별, 혼인 상태, 자녀 유무, 관 심사 등) 및 추천 카테고리인 미디어 콘텐츠와 관련된 사용자 입력 정보(예를 들어, 콘텐츠 장르, 콘텐츠 언어, 배우, 콘텐츠 지역, 이전 거주 지역 이력 등)를 포함할 수 있으나, 이에 한정되는 것은 아니다. 상태에 포함되는 사용자 선호도는, 콘텐츠 장로 선호도, 배우 선호도, 지역 선호도, 상영 시간 선호도, 콘텐츠 다양성 선호도, 피드백 선호도(예를 들어, 사용자가 콘텐츠를 스킵/대체하는 등의 피드백 제공 확률)를 포함할 수 있으나, 이에 한정되는 것은 아니다. 상태에 포함되는 사용자 내부 현황은, 사용자가 추천을 제공받음에 따라 영향을 받는 요소들을 포함할 수 있다. 예를 들어, 사용자의 시청 시간 추적, 서비스 재접속률, 콘텐츠 추천 적중/실패율, 콘텐츠 결정 시간, 전체 콘 텐츠 중에서 시청한 비율, 콘텐츠 평가 등이 사용자 내부 현황에 포함될 수 있으나, 이에 한정되는 것은 아니다. 상태에 포함되는 추천 이력은, 사용자에게 추천된 추천 요소들 및 추천 요소에 대한 피드백들의 이력을 포함할 수 있다. 예를 들어, 미디어 콘텐츠 추천 세션에 포함되는 추천 요소들인 콘텐트 및 콘텐트에 대한 피드백 등이 추천 이력에 포함될 수 있다. 한편, 상태에 포함되는 사용자 내부 현황, 추천 이력의 경우, 동적인 것으로 분류될 수 있다. 상태 전이 함수에 서, 동적인 세그먼트는 다음 상태로 전이될 때 변화하도록 정의될 수 있다. 이 경우, 동적인 세그먼트는 다시 \"결정적인 것(Deterministic, p=1)\" 또는 \"비결정적인 것(Non-deterministic, p≠1)\"으로 분류될 수 있다. 예 를 들어, 추천 이력은 동적인 세그먼트이지만, 현재 액션으로 결정된 추천 요소를 추천 세션의 리스트의 끝에 새로 부가하는 것이므로 결정적인 것으로 분류될 수 있다. 또는, 추천 요소에 대한 좋음/싫음과 같은 사용자 피 드백은, 확률에 의해 정의될 수 있으므로 비결정적인 것으로 분류될 수 있다. 마찬가지로, 사용자 내부 현황의 상태 전이도 비결정적인 것으로 분류될 수 있다. 사용자 내부 현황은 관측 불가능하거나 부분적으로 관측 가능 한 상태이므로, 사용자의 내부 현황을 추적 및 예측 가능한 내부 모델(Internal model)에 의해 정의되고 제어될 수 있다. 리워드는 부정적인 리워드 및 긍정적인 리워드를 포함하도록 설계될 수 있다. 예를 들어, 부정적인 리워드는 추 천 요소를 사용자가 좋아하지 않는 경우를 포함할 수 있고, 긍정적인 리워드는 추천 요소를 사용자가 좋아하는 경우를 포함할 수 있다. 리워드는 예를 들어, 하루에 제공된 콘텐츠 사이의 다양성, 일자별로 제공된 콘텐츠 사 이의 다양성, 시청한 콘텐츠 수, 콘텐츠에 대한 좋음/싫음 피드백, 추천에 대한 콘텐츠 선택 여부 등을 반영하 여 설계될 수 있다. 한편, 전술한 것과 같은 사용자 시뮬레이터를 이용하는 강화학습 시스템의 구성 요소들 외에, 강화학습 시스템 에서 에이전트와 환경 사이의 상호작용을 포함하는 구체적인 동작들에 대해서는, 이전의 도면들에서 전술하였으 므로, 반복되는 설명은 생략한다. 도 15는 본 개시의 일 실시예에 따른 서버의 구성을 도시한 블록도이다. 일 실시예에서, 서버는 통신 인터페이스, 메모리 및 프로세서를 포함할 수 있다. 통신 인터페이스는 프로세서의 제어에 의해 다른 전자 장치들과 데이터 통신을 수행할 수 있다. 통신 인터페이스는 예를 들어, 유선 랜, 무선 랜(Wireless LAN), 와이파이(Wi-Fi), 블루투스 (Bluetooth), 지그비(ZigBee), WFD(Wi-Fi Direct), 적외선 통신(IrDA, infrared Data Association), BLE (Bluetooth Low Energy), NFC(Near Field Communication), 와이브로(Wireless Broadband Internet, Wibro), 와이맥스(World Interoperability for Microwave Access, WiMAX), SWAP(Shared Wireless Access Protocol), 와이기그(Wireless Gigabit Alliances, WiGig) 및 RF 통신을 포함하는 데이터 통신 방식 중 적어도 하나를 이 용하여, 서버와 다른 디바이스들 간의 데이터 통신을 수행할 수 있는, 통신 회로를 포함할 수 있다. 통신 인터페이스는 개인화된 추천 서비스를 제공하기 위한 데이터를 외부 디바이스와 송수신할 수 있다. 예를 들어, 통신 인터페이스는 사용자의 전자 장치(예를 들어, 모바일 폰 등)로부터 사용자 데이터를 수 신할 수 있고, 사용자의 전자 장치로 개인화된 추천 세션을 전송할 수 있으며, 사용자의 전자 장치로부터 사용자 피드백을 수신할 수도 있다. 메모리는 프로세서가 판독할 수 있는 명령어들, 데이터 구조, 및 프로그램 코드(program code)가 저장될 수 있다. 프로세서가 수행하는 동작들은 메모리 저장된 프로그램의 명령어들 또는 코드들을 실행함으로써 구현될 수 있다. 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등)를 포함할 수 있으며, 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나를 포함하는 비 휘발성 메모리 및 램(RAM, Random Access Memory) 또는 SRAM(Static Random Access Memory)과 같은 휘발성 메모 리를 포함할 수 있다. 메모리는 서버가 개인화된 추천 세션 서비스를 제공하기 위해 동작하도록 하는 하나 이상의 인스트 럭션 및/또는 프로그램을 저장할 수 있다. 예를 들어, 메모리에는 강화학습 모델 및 추천 세션 관 리 모듈이 저장될 수 있다. 강화학습 모델은 사용자 시뮬레이터 및 생성자를 포함할 수 있다. 프로세서는 서버의 전반적인 동작들을 제어할 수 있다. 예를 들어, 프로세서는 메모리(220 0)에 저장된 프로그램의 하나 이상의 명령어들(instructions)을 실행함으로써, 서버가 개인화된 추천 세 션을 생성하기 위한 전반적인 동작들을 제어할 수 있다. 프로세서는 하나 이상일 수 있다. 프로세서는 예를 들어, 중앙 처리 장치(Central Processing Unit), 마이크로 프로세서(microprocessor), 그래픽 처리 장치(Graphic Processing Unit), ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 애플리케이션 프로세서(Application Processor), 신경망 처리 장치(Neural Processing Unit) 또는 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계된 인공지능 전 용 프로세서 중 적어도 하나로 구성될 수 있으나, 이에 제한되는 것은 아니다. 일 실시예에서, 프로세서는 강화학습 모델을 이용하여, 개인화된 추천 세션을 생성할 수 있다. 강 화학습 모델은 사용자 시뮬레이터 및 생성자를 포함할 수 있다. 사용자 시뮬레이터는 추천을 제공 받는 실제 사용자에 대응하는 가상의 사용자를 나타내는 시뮬레이션된 사용자를 생성할 수 있다. 생성자는 사용자에게 제공될 추천 세션에 포함되는 추천 요소를 생성할 수 있다. 본 개시의 강화학습 시 스템에서, 추천을 제공 받는 실제 사용자(또는, 추천 제공 애플리케이션)이 환경에 대응된다. 또한, 추천 요소 를 결정하고 제공하는 생성자가 에이전트에 대응된다. 이 경우, 시뮬레이션된 사용자는 실제 사용자를 시 뮬레이션한 것이므로 가상 환경이라고 지칭될 수도 있다. 시뮬레이션된 사용자는 실제 사용자의 내부 현황을 추 적하고, 실제 사용자에게 제공된 추천 이력을 관리하는 데 이용될 수 있다. 또한, 시뮬레이션된 사용자는 강화 학습 모델을 훈련시키는데 이용되는 시뮬레이션된 피드백을 생성할 수 있다. 이 경우, 사용자 시뮬레이터(221 2)는, 실제 사용자들의 피드백 데이터에 기초하여 실제 사용자들과 유사한 시뮬레이션된 피드백을 생성하도록 훈련된 것일 수 있다. 개인화된 추천 세션은, 사용자 시뮬레이터에 의해 생성된 시뮬레이션된 사용자와, 생성자의 상호작용을 통해 생성될 수 있다. 프로세서는 강화학습 모델을 이용하여 사용자에게 개인화된 추천 세션(또는 추천 세션 그룹)을 제 공할 수 있다. 프로세서는 사용자의 피드백 및/또는 사용자 시뮬레이터에 의해 생성되는 합성 데이 터를 이용하여 강화학습 모델을 훈련시킬 수 있다. 일 실시예에서, 프로세서는 추천 세션 관리 모듈을 이용하여, 사용자에게 제공되는 추천 세션을 관 리할 수 있다. 예를 들어, 프로세서는 추천 세션 관리 모듈을 이용하여 사용자에게 제공된 추천 세"}
{"patent_id": "10-2023-0118553", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "션의 이력을 관리하며, 추천 세션과 관련된 정보(예를 들어, 추천 세션 제공 결과, 요약 정보, 추천 세션을 제 공받은 사용자의 정보 등)를 생성 및 관리하고, 사용자에게 제공할 수 있다. 강화학습 모델 및 추천 세션 관리 모듈의 동작들과 관련된 설명은 이전의 도면들에서 기술한 설명 에 이미 포함되므로, 반복되는 설명은 생략한다. 한편, 전술한 메모리에 저장된 모듈 및 모델은, 설명의 편의를 위한 것이며 반드시 이에 한정되는 것은 아니다. 전술한 실시예를 구현하기 위해 다른 모듈이 추가될 수 있으며, 전술한 모듈들 중 일부의 모듈들은 하 나의 모듈로 구현될 수도 있다. 본 개시의 일 실시예에 따른 방법이 복수의 동작을 포함하는 경우, 복수의 동작은 하나의 프로세서에 의해 수행 될 수도 있고, 복수의 프로세서에 의해 수행될 수도 있다. 예를 들어, 일 실시예에 따른 방법에 의해 제1 동작, 제2 동작, 제3 동작이 수행될 때, 제1 동작, 제2 동작, 및 제3 동작 모두 제1 프로세서에 의해 수행될 수도 있 고, 제1 동작 및 제2 동작은 제1 프로세서(예를 들어, 범용 프로세서)에 의해 수행되고 제3 동작은 제2 프로세 서(예를 들어, 인공지능 전용 프로세서)에 의해 수행될 수도 있다. 여기서, 제2 프로세서의 예시인 인공지능 전 용 프로세서는, 인공지능 모델의 훈련/추론을 위한 연산들이 수행될 수도 있다. 그러나, 본 개시의 실시예가 이 에 한정되는 것은 아니다. 본 개시에 따른 하나 이상의 프로세서는 싱글 코어 프로세서(single-core processor)로 구현될 수도 있고, 멀티 코어 프로세서(multi-core processor)로 구현될 수도 있다. 본 개시의 일 실시예에 따른 방법이 복수의 동작을 포함하는 경우, 복수의 동작은 하나의 코어에 의해 수행될 수도 있고, 하나 이상의 프로세서에 포함된 복수의 코어에 의해 수행될 수도 있다. 도 16은 본 개시의 일 실시예에 따른 전자 장치의 구성을 도시한 블록도이다. 일 실시예에서, 전술한 서버의 동작들은, 전자 장치에서 수행될 수도 있다. 전자 장치는 예 를 들어, 사용자의 전자 장치를 지칭하는 것일 수 있으며, 모바일 폰, 태블릿 등 강화학습 모델을 운용 가능한 다양한 형태의 컴퓨팅 장치로 구현될 수 있다. 즉, 본 개시의 동작들은 하나의 디바이스에 의해 구현 가능한 온 디바이스 서비스일 수 있다. 이에 따라, 사용자는 자신의 전자 장치만을 이용하여 개인화된 추천 세션을 제공 받을 수도 있다. 전자 장치는 통신 인터페이스, 메모리 및 프로세서를 포함할 수 있고, 메모리에 는 강화학습 모델, 추천 세션 관리 모듈이 저장될 수 있다. 강화학습 모델은 사용자 시뮬레 이터 및 생성자를 포함할 수 있다. 전자 장치의 통신 인터페이스, 메모리 및 프로세서의 동작에 대한 설명은, 도 15의 서 버의 통신 인터페이스, 메모리 및 프로세서의 동작과 동일/유사하므로, 반복되는 설명 은 생략한다. 본 개시는, 강화학습 기반으로 사용자에게 정밀한 개인화된 추천을 제공하는 방법에 관한 것이다. 또한, 사용자 시뮬레이터를 이용하여 생성되는 합성 데이터를 이용하여, 적은 양의 실제 사용자 데이터만으로도 고성능의 강 화학습 모델을 훈련시키는 방법에 관한 것이다. 본 개시에서 이루고자 하는 기술적 과제는, 이상에서 언급한 것으로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 본 명세서의 기재로부터 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다. 본 개시의 일 측면에 따르면, 서버가 강화학습 기반으로 개인화된 추천 세션을 제공하는 방법이 제공될 수 있다. 상기 방법은, 사용자 데이터를 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 사용자 데이터를 기초로, 추천을 제공 받는 실제 사용자에 대응하는 가상의 사용자를 나타내 는 시뮬레이션된 사용자를 생성하는 단계를 포함할 수 있다. 상기 방법은, 상기 시뮬레이션된 사용자의 상태에 기초하여 액션을 결정하는 단계를 포함할 수 있다. 상기 액션은 상기 사용자에게 제공될 상기 추천 세션에 포함되는 추천 요소를 결정하는 것일 수 있다. 상기 방법은, 상기 시뮬레이션된 사용자의 상태를 업데이트하는 단계를 포함할 수 있다. 상기 방법은, 상기 시뮬레이션된 사용자의 업데이트된 상태 및 상기 시뮬레이션된 사용자로부터 출력되는 상기 추천 요소에 대한 리워드를 식별하는 단계를 포함할 수 있다. 상기 방법은, 상기 시뮬레이션된 사용자의 상기 업데이트된 상태 및 상기 리워드에 기초하여 상기 추천 요소의 결정을 반복하여 개인화된 추천 세션을 생성하는 단계를 포함할 수 있다. 상기 방법은, 상기 개인화된 추천 세션을 출력하는 단계를 포함할 수 있다. 상기 시뮬레이션된 사용자의 상태는, 사용자 설명(Description), 사용자 선호도, 사용자 내부 현황, 및 추천 이 력 중 적어도 하나를 포함하는 것일 수 있다. 상기 시뮬레이션된 사용자를 생성하는 단계는, 제1 사용자로부터 상기 사용자 데이터를 수신하는 단계를 포함할 수 있다. 상기 시뮬레이션된 사용자를 생성하는 단계는, 상기 사용자 데이터를 기초로 제2 사용자들을 클러스터하는 단계 를 포함할 수 있다. 상기 시뮬레이션된 사용자를 생성하는 단계는, 상기 제2 사용자들의 클러스터에 대응하는 시뮬레이션된 사용자 들에 기초하여, 상기 제1 사용자의 시뮬레이션된 사용자를 생성하는 단계를 포함할 수 있다.상기 시뮬레이션된 사용자를 생성하는 단계는, 상기 제2 사용자들의 수를 식별하는 단계를 포함할 수 있다. 상기 시뮬레이션된 사용자를 생성하는 단계는, 상기 제2 사용자들의 수가 기 설정된 값 미만인 것에 기초하여, 상기 사용자 데이터를 기초로 상기 제1 사용자의 시뮬레이션된 사용자에 포함되는 파라미터들을 임의로 설정하 는 단계를 포함할 수 있다. 상기 방법은, 상기 개인화된 추천 세션에 대한 사용자 피드백을 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 사용자 피드백에 기초하여, 상기 시뮬레이션된 사용자를 재훈련시키는 단계를 포함할 수 있 다. 상기 방법은, 상기 개인화된 추천 세션의 생성을 반복하여 복수의 개인화된 추천 세션들로 구성되는 추천 세션 그룹을 생성하는 단계를 포함할 수 있다. 상기 시뮬레이션된 사용자를 재훈련시키는 단계는, 상기 복수의 개인화된 추천 세션들 각각이 생성될 때마다 수 행되는 것일 수 있다. 상기 시뮬레이션된 사용자를 생성하는 단계는, 미리 훈련된 생성형 인공지능인 사용자 시뮬레이터를 이용하여 상기 시뮬레이션된 사용자를 생성하는 것일 수 있다. 상기 사용자 시뮬레이터는, 실제 사용자들의 피드백 데이터에 기초하여 실제 사용자들과 유사한 시뮬레이션된 피드백을 생성하도록 훈련된 것일 수 있다. 상기 개인화된 추천 세션을 제공하는 강화학습 모델은, 상기 시뮬레이션된 피드백을 이용하여 미리 훈련된 것일 수 있다. 상기 개인화된 추천 세션은, 운동에 관련된 무브먼트 추천 요소들을 포함하는 운동 추천 세션인 것일 수 있다. 본 개시의 일 측면에 따르면, 강화학습 기반으로 개인화된 추천 세션을 제공하는 서버가 제공될 수 있다. 상기 서버는, 통신 인터페이스; 하나 이상의 인스트럭션을 저장하는 메모리; 상기 메모리에 저장된 상기 하나 이상의 인스트럭션을 실행하는 하나 이상의 프로세서를 포함할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 사용자 데이터를 획득할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 사용자 데이터를 기초로, 추 천을 제공 받는 실제 사용자에 대응하는 가상의 사용자를 나타내는 시뮬레이션된 사용자를 생성할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 시뮬레이션된 사용자의 상태 에 기초하여 액션을 결정할 수 있다. 상기 액션은 상기 사용자에게 제공될 상기 추천 세션에 포함되는 추천 요소를 결정하는 것일 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 시뮬레이션된 사용자의 상태 를 업데이트할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 시뮬레이션된 사용자의 업데 이트된 상태 및 상기 시뮬레이션된 사용자로부터 출력되는 상기 추천 요소에 대한 리워드를 식별할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 시뮬레이션된 사용자의 상기 업데이트된 상태 및 상기 리워드에 기초하여 상기 추천 요소의 결정을 반복하여 개인화된 추천 세션을 생성할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 개인화된 추천 세션을 출력 할 수 있다. 상기 시뮬레이션된 사용자의 상태는, 사용자 설명(Description), 사용자 선호도, 사용자 내부 현황, 및 추천 이 력 중 적어도 하나를 포함하는 것일 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 제1 사용자로부터 상기 사용자 데 이터를 수신할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 사용자 데이터를 기초로 제2 사용자들을 클러스터 할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제2 사용자들의 클러스터에 대응하는 시뮬레이션된 사용자들에 기초하여, 상기 제1 사용자의 시뮬레이션된 사용자를 생성할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제2 사용자들의 수를 식별할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제2 사용자들의 수가 기 설 정된 값 미만인 것에 기초하여, 상기 사용자 데이터를 기초로 상기 제1 사용자의 시뮬레이션된 사용자에 포함되 는 파라미터들을 임의로 설정할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 개인화된 추천 세션에 대한 사용자 피드백을 획득할 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 사용자 피드백에 기초하여, 상기 시뮬레이션된 사용자를 재훈련시킬 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 개인화된 추천 세션의 생성 을 반복하여 복수의 개인화된 추천 세션들로 구성되는 추천 세션 그룹을 생성할 수 있다. 상기 시뮬레이션된 사용자의 재훈련은 상기 복수의 개인화된 추천 세션들 각각이 생성될 때마다 수행되는 것일 수 있다. 상기 하나 이상의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 미리 훈련된 생성형 인공지능인 사용자 시뮬레이터를 이용하여 상기 시뮬레이션된 사용자를 생성할 수 있다. 상기 사용자 시뮬레이터는, 실제 사용자들의 피드백 데이터에 기초하여 실제 사용자들과 유사한 시뮬레이션된 피드백을 생성하도록 훈련된 것일 수 있다. 상기 개인화된 추천 세션을 제공하는 강화학습 모델은, 상기 시뮬레이션된 피드백을 이용하여 미리 훈련된 것일 수 있다. 상기 개인화된 추천 세션은, 운동에 관련된 무브먼트 추천 요소들을 포함하는 운동 추천 세션인 것일 수 있다. 한편, 본 개시의 실시예들은 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행 가능한 명령어 를 포함하는 기록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스 될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨 터 판독 가능 매체는 컴퓨터 저장 매체 및 통신 매체를 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독 가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구 현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독 가 능 명령어, 데이터 구조, 또는 프로그램 모듈과 같은 변조된 데이터 신호의 기타 데이터를 포함할 수 있다. 또한, 컴퓨터에 의해 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다 는 것을 의미할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경 우를 구분하지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두개의 사용자 장치들(예: 모바일 폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운 로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다."}
{"patent_id": "10-2023-0118553", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "전술한 본 개시의 설명은 예시를 위한 것이며, 본 개시가 속하는 기술분야의 통상의 지식을 가진 자는 본 개시 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 개시의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 개시의 범위에 포함되는 것으 로 해석되어야 한다."}
{"patent_id": "10-2023-0118553", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 서버가 강화학습 기반으로 개인화된 추천 세션을 제공하는 것을 개략적으로 도시한 도면이다. 도 2는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 세션을 제공하는 동작을 설명하기 위한 흐름도이다. 도 3은 본 개시의 일 실시예에 따른 운동 추천을 제공하는 강화학습 모델의 전반적인 구조를 도시한 도면이다. 도 4는 본 개시의 일 실시예에 따른 서버가 시뮬레이션된 사용자를 생성하는 동작을 설명하기 위한 도면이다. 도 5는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 세션을 생성하는 동작을 설명하기 위한 도면이다.도 6은 본 개시의 일 실시예에 따른 서버가 시뮬레이션된 사용자를 재훈련시키는 동작을 설명하기 위한 도면이다. 도 7a는 본 개시의 일 실시예에 따른 사용자 시뮬레이터의 동작을 설명하기 위한 도면이다. 도 7b는 본 개시의 일 실시예에 따른 사용자 시뮬레이터의 동작을 설명하기 위한 도면이다. 도 8은 본 개시의 일 실시예에 따른 서버가 사용자 시뮬레이터를 훈련시키는 동작을 설명하기 위한 도면이다. 도 9는 본 개시의 일 실시예에 따른 서버가 강화학습 모델을 훈련시키기 위한 데이터를 생성하는 동작을 설명하 기 위한 도면이다. 도 10a는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스를 제공하기 위해 사용자 데이터를 획 득하는 동작을 설명하기 위한 도면이다. 도 10b는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스를 제공하기 위해 사용자 데이터를 획 득하는 동작을 설명하기 위한 도면이다. 도 11a는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스를 제공하는 동작을 설명하기 위한 도 면이다. 도 11b는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스에 대한 피드백을 획득하는 동작을 설 명하기 위한 도면이다. 도 11c는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스에 대한 피드백을 획득하는 동작을 설 명하기 위한 도면이다. 도 11d는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 제공 서비스에 대한 피드백을 획득하는 동작을 설 명하기 위한 도면이다. 도 12는 본 개시의 일 실시예에 따른 서버가 제공된 추천 세션과 관련된 정보를 추가로 제공하는 동작을 설명하 기 위한 도면이다. 도 13은 본 개시의 일 실시예에 따른 서버가 개인화된 추천 세션을 제공하는 예시를 설명하기 위한 도면이다. 도 14는 본 개시의 일 실시예에 따른 서버가 개인화된 추천 세션을 제공하는 예시를 설명하기 위한 도면이다. 도 15는 본 개시의 일 실시예에 따른 서버의 구성을 도시한 블록도이다. 도 16은 본 개시의 일 실시예에 따른 전자 장치의 구성을 도시한 블록도이다."}
