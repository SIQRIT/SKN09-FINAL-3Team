{"patent_id": "10-2018-0169909", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0080047", "출원번호": "10-2018-0169909", "발명의 명칭": "진정 사용자의 손을 식별하는 방법 및 이를 위한 웨어러블 기기", "출원인": "삼성전자주식회사", "발명자": "구본곤"}}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "웨어러블 기기가 상기 웨어러블 기기를 착용한 진정 사용자의 손을 식별하는 방법에 있어서, 상기 웨어러블 기기에 포함된 센서를 이용하여, 상기 센서의 감지 영역 내에 위치하는 손을 인식하는 단계;상기 손의 방향과 상기 손에 연결된 적어도 하나의 인체 부위의 위치 관계에 기반하여, 상기 손과 연결된 어깨의 위치를 추정하는 단계; 및상기 추정된 위치에서 상기 진정 사용자의 어깨가 존재할 확률 정보를 이용하여, 상기 인식된 손이 상기 진정사용자의 손인지 판단하는 단계를 포함하는, 방법."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 인식된 손이 상기 진정 사용자의 손인지 판단하는 단계는, 상기 진정 사용자의 어깨 존재 확률 정보를 획득하는 단계;상기 진정 사용자의 어깨 존재 확률 정보로부터 상기 추정된 위치에서의 확률 값을 획득하는 단계; 및상기 확률 값이 임계 값보다 큰 경우, 상기 인식된 손이 상기 진정 사용자의 손이라고 판단하는 단계를 포함하는, 방법."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서, 상기 방법은,상기 인식된 손이 상기 진정 사용자의 손이라고 판단되는 경우, 상기 인식된 손의 제스처를 식별하는 단계; 및상기 식별된 제스처에 대응하는 명령을 수행하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서, 상기 방법은,상기 인식된 손이 상기 진정 사용자의 손이 아니라고 판단되는 경우, 상기 인식된 손을 무시하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서, 상기 방법은, 상기 인식된 손이 상기 진정 사용자의 손이라고 판단되는 경우, 상기 진정 사용자의 손이 상기 감지 영역 내에위치하는지 여부를 나타내는 알림 정보를 제공하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서, 상기 알림 정보를 제공하는 단계는, 상기 진정 사용자의 손이 상기 감지 영역 내로 진입하는 경우, 상기 웨어러블 기기가 제공하는 디스플레이 영역에 제 1 식별 영상을 표시하는 단계; 및상기 진정 사용자의 손이 상기 감지 영역을 벗어나는 경우, 상기 디스플레이 영역에 제 2 식별 영상을 표시하는단계를 포함하는, 방법."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항에 있어서, 상기 방법은,공개특허 10-2020-0080047-3-상기 인식된 손이 상기 진정 사용자의 손이라고 판단되는 경우, 상기 손의 제 1 특징 정보를 획득하는 단계; 및상기 손의 제 1 특징 정보에 기초하여, 상기 감지 영역 내에 위치하는 적어도 하나의 손이 상기 진정 사용자의손인지 판단하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서, 상기 방법은, 상기 손의 제 1 특징 정보와 상기 감지 영역 내에 위치하는 적어도 하나의 손의 제 2 특징 정보를 비교하는 단계;상기 제 1 특징 정보와 상기 제 2 특징 정보의 유사도가 임계 값보다 작은 경우, 상기 적어도 하나의 손에 연결된 적어도 하나의 어깨의 위치를 추정하는 단계; 및상기 추정된 위치에서 상기 진정 사용자의 어깨가 존재할 확률 정보를 이용하여, 상기 적어도 하나의 손이 상기진정 사용자의 손인지 판단하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항에 있어서, 상기 손에 연결된 어깨의 위치를 추정하는 단계는, 상기 감지 영역 내에 위치하는 손을 포함하는 이미지를 획득하는 단계; 및상기 획득된 이미지를 분석하여, 상기 손에 연결된 하박부(下膊部), 상기 하박부에 연결된 상박부(上膊部) 및상기 하박부에 연결된 어깨 중 적어도 하나를 검출하는 단계를 포함하는, 방법."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "출력부;적어도 하나의 센서;하나 이상의 인스트럭션을 저장하는 메모리; 및상기 메모리에 연결된 프로세서를 포함하고,상기 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 적어도 하나의 센서를 이용하여, 상기 적어도 하나의 센서의 감지 영역 내에 위치하는 손을 인식하는동작;상기 손의 방향과 상기 손에 연결된 적어도 하나의 인체 부위의 위치 관계에 기반하여, 상기 손에 연결된 어깨의 위치를 추정하는 동작; 및상기 추정된 위치에서 웨어러블 기기를 착용한 진정 사용자의 어깨가 존재할 확률 정보를 이용하여, 상기 인식된 손이 상기 진정 사용자의 손인지 판단하는 동작을 수행하는, 웨어러블 기기."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10 항에 있어서, 상기 프로세서는, 상기 진정 사용자의 어깨 존재 확률 정보를 획득하는 동작;상기 진정 사용자의 어깨 존재 확률 정보로부터 상기 추정된 위치에서의 확률 값을 획득하는 동작; 및상기 확률 값이 임계 값보다 큰 경우, 상기 인식된 손이 상기 진정 사용자의 손이라고 판단하는 동작을 수행하는, 웨어러블 기기."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 10 항에 있어서, 상기 프로세서는,상기 인식된 손이 상기 진정 사용자의 손이라고 판단되는 경우, 상기 인식된 손의 제스처를 식별하는 동작; 및공개특허 10-2020-0080047-4-상기 식별된 제스처에 대응하는 명령을 수행하는 동작을 더 수행하는, 웨어러블 기기."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 10 항에 있어서, 상기 프로세서는,상기 인식된 손이 상기 진정 사용자의 손이 아니라고 판단되는 경우, 상기 인식된 손을 무시하는 동작을 더 수행하는, 웨어러블 기기."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 10 항에 있어서, 상기 프로세서는, 상기 인식된 손이 상기 진정 사용자의 손이라고 판단되는 경우, 상기 진정 사용자의 손이 상기 감지 영역 내에위치하는지 여부를 나타내는 알림 정보를 상기 출력부를 통해 제공하는 동작을 더 수행하는, 웨어러블 기기."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서, 상기 프로세서는, 상기 진정 사용자의 손이 상기 감지 영역 내로 진입하는 경우, 상기 출력부를 통해 제 1 식별 영상을 표시하는동작; 및상기 진정 사용자의 손이 상기 감지 영역을 벗어나는 경우, 상기 출력부를 통해 제 2 식별 영상을 표시하는 동작을 수행하는, 웨어러블 기기."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 10 항에 있어서, 상기 프로세서는,상기 인식된 손이 상기 진정 사용자의 손이라고 판단되는 경우, 상기 인식된 손의 제 1 특징 정보를 획득하는동작; 및상기 손의 제 1 특징 정보에 기초하여, 상기 감지 영역 내에 위치하는 적어도 하나의 손이 상기 진정 사용자의손인지 판단하는 동작을 더 수행하는, 웨어러블 기기."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16 항에 있어서, 상기 프로세서는, 상기 손의 제 1 특징 정보와 상기 감지 영역 내에 위치하는 적어도 하나의 손의 제 2 특징 정보를 비교하는 동작;상기 제 1 특징 정보와 상기 제 2 특징 정보의 유사도가 임계 값보다 작은 경우, 상기 적어도 하나의 손에 연결된 적어도 하나의 어깨의 위치를 추정하는 동작; 및상기 추정된 위치에서 상기 진정 사용자의 어깨가 존재할 확률 정보를 이용하여, 상기 적어도 하나의 손이 상기진정 사용자의 손인지 판단하는 동작을 더 수행하는, 웨어러블 기기."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "웨어러블 기기가 상기 웨어러블 기기를 착용한 진정 사용자의 손을 식별하는 방법에 있어서, 상기 웨어러블 기기에 포함된 센서를 이용하여, 상기 센서의 감지 영역 내에 위치하는 손을 인식하는 단계;상기 감지 영역의 위치 별로 상기 진정 사용자의 손이 감지될 확률에 관한 정보에 기반하여, 상기 인식된 손이상기 진정 사용자의 손인지 판단하는 단계; 상기 인식된 손이 상기 진정 사용자의 손이라고 판단되는 경우, 상기 인식된 손의 제스처를 식별하는 단계; 및상기 식별된 제스처에 대응하는 명령을 수행하는 단계를 포함하는, 방법."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "공개특허 10-2020-0080047-5-출력부;적어도 하나의 센서;하나 이상의 인스트럭션을 저장하는 메모리; 및상기 메모리에 연결된 프로세서를 포함하고,상기 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 적어도 하나의 센서를 이용하여, 상기 적어도 하나의 센서의 감지 영역 내에 위치하는 손을 인식하는동작;상기 감지 영역의 위치 별로 웨어러블 기기를 착용한 진정 사용자의 손이 감지될 확률에 관한 정보에 기반하여,상기 인식된 손이 상기 진정 사용자의 손인지 판단하는 동작; 상기 인식된 손이 상기 진정 사용자의 손이라고 판단되는 경우, 상기 인식된 손의 제스처를 식별하는 동작; 및상기 식별된 제스처에 대응하는 명령을 수행하는 동작을 수행하는, 웨어러블 기기."}
{"patent_id": "10-2018-0169909", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "웨어러블 기기에 포함된 센서를 이용하여, 상기 센서의 감지 영역 내에 위치하는 손을 인식하는 동작;상기 손의 방향과 상기 손에 연결된 적어도 하나의 인체 부위의 위치 관계에 기반하여, 상기 손에 연결된 어깨의 위치를 추정하는 동작; 및상기 추정된 위치에서 상기 진정 사용자의 어깨가 존재할 확률 정보를 이용하여, 상기 인식된 손이 상기 진정사용자의 손인지 판단하는 동작을 수행하도록 하는 프로그램이 저장된 기록매체를 포함하는 컴퓨터 프로그램 제품."}
{"patent_id": "10-2018-0169909", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "웨어러블 기기를 착용한 진정 사용자의 손을 식별하는 방법이 개시된다. 일 실시예에 의하면, 웨어러블 기기에 포함된 센서를 이용하여, 센서의 감지 영역 내에 위치하는 손을 인식하는 단계; 인식된 손의 방향과 인식된 손에 연결된 적어도 하나의 인체 부위의 위치 관계에 기반하여, 인식된 손과 연결된 어깨의 위치를 추정하는 단계; 및 추정된 위치에서 진정 사용자의 어깨가 존재할 확률 정보를 이용하여, 인식된 손이 진정 사용자의 손인지 판단하 는 단계를 포함하는 방법이 개시된다."}
{"patent_id": "10-2018-0169909", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명의 일 실시예는 웨어러블 기기를 착용한 진정 사용자의 손을 식별하는 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2018-0169909", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "증강현실 연구는 1960년대 van Surtherland가 최초의 see-through HMD를 개발한 것에서 시작되었으며, 1990년 대초 보잉사가 ‘Augmented Reality’라는 신조어를 등장시키면서 본격화되었다. 증강현실 플랫폼은 데스크탑 컴퓨터에서 출발하여 스마트폰으로 진화하고 있다. 초기 PC를 중심으로 이루어진 증강현실은 2000년 이후 PDA, UMPC, 모바일폰 등의 보급으로 모바일 증강현실의 플랫폼이 다양화되기 시작했고, 스마트폰의 보급이 본격화된 2009년 이후 모바일 증강현실의 현실 응용 가능성을 보여줬다. 스마트폰이 증강현 실 플랫폼으로 주목받는 이유는 카메라 이외도 소형화와 경량화된 GPS, 나침반, 자기센서, 가속도센서, 터치센 서, 근접센서, 조도센서, 와이파이, RFID 등 다양한 센서들이 내장되어 사용자의 위치 외에도 명령이나 의도를 파악할 수 있는 추가적인 정보를 실시간으로 제공하고 있기 때문이다. 증강현실의 플랫폼은 정보를 보기 위해 손에 스마트폰을 들고 있어야 하는 단점을 극복하기 위해 안경형으로 계 속 진화하고 있다. 동시에 사물인터넷(IoT), 컴퓨터 비전, 실감콘텐츠, 인공지능 등 핵심기술의 진보로 다양한 활용 가능성에 대한 기대를 높여가고 있다. 일 실시예에 의하면, 웨어러블 기기가 웨어러블 기기를 착용한 진정 사용자의 손을 타인의 손과 구별함으로써, 제 3자에 의한 우연한 또는 의도된 조작을 방지하는데 목적이 있다. 다른 실시예에 의하면, 웨어러블 기기를 착용한 진정 사용자의 손이 웨어러블 기기의 감지 범위 내에 있는지 여 부를 나타내는 알림 정보를 제공함으로써, 진정 사용자가 웨어러블 기기의 조작 범위를 쉽게 인지할 수 있도록 하는데 목적이 있다. 일 실시예에 따른 웨어러블 기기를 착용한 진정 사용자의 손을 식별하는 방법은, 웨어러블 기기에 포함된 센서 를 이용하여, 센서의 감지 영역 내에 위치하는 손을 인식하는 단계; 손의 방향과 손에 연결된 적어도 하나의 인 체 부위의 위치 관계에 기반하여, 손과 연결된 어깨의 위치를 추정하는 단계; 및 추정된 위치에서 진정 사용자 의 어깨가 존재할 확률 정보를 이용하여, 인식된 손이 진정 사용자의 손인지 판단하는 단계를 포함할 수 있다. 일 실시예에 따른 웨어러블 기기는, 출력부; 적어도 하나의 센서; 하나 이상의 인스트럭션을 저장하는 메모리; 및 메모리에 연결된 프로세서를 포함할 수 있으며, 프로세서는, 하나 이상의 인스트럭션을 실행함으로써, 적어 도 하나의 센서를 이용하여, 적어도 하나의 센서의 감지 영역 내에 위치하는 손을 인식하는 동작; 적어도 하나 의 센서의 감지 영역으로 진입한 손의 방향과 손에 연결된 적어도 하나의 인체 부위의 위치 관계에 기반하여, 손에 연결된 어깨의 위치를 추정하는 동작; 및 추정된 위치에서 웨어러블 기기를 착용한 진정 사용자의 어깨가 존재할 확률 정보를 이용하여, 인식된 손이 진정 사용자의 손인지 판단하는 동작을 수행할 수 있다. 일 실시예에 따른 웨어러블 기기를 착용한 진정 사용자의 손을 식별하는 방법은, 웨어러블 기기에 포함된 센서 를 이용하여, 센서의 감지 영역 내에 위치하는 손을 인식하는 단계; 감지 영역의 위치 별로 진정 사용자의 손이 감지될 확률에 관한 정보에 기반하여, 인식된 손이 진정 사용자의 손인지 판단하는 단계; 인식된 손이 진정 사 용자의 손이라고 판단되는 경우, 인식된 손의 제스처를 식별하는 단계; 및 식별된 제스처에 대응하는 명령을 수 행하는 단계를 포함할 수 있다. 일 실시예에 따른 웨어러블 기기는, 출력부; 적어도 하나의 센서; 하나 이상의 인스트럭션을 저장하는 메모리; 및 메모리에 연결된 프로세서를 포함할 수 있고, 프로세서는, 하나 이상의 인스트럭션을 실행함으로써, 적어도 하나의 센서를 이용하여, 적어도 하나의 센서의 감지 영역 내에 위치하는 손을 인식하는 동작; 감지 영역의 위 치 별로 웨어러블 기기를 착용한 진정 사용자의 손이 감지될 확률에 관한 정보에 기반하여, 인식된 손이 진정 사용자의 손인지 판단하는 동작; 인식된 손이 진정 사용자의 손이라고 판단되는 경우, 인식된 손의 제스처를 식 별하는 동작; 및 식별된 제스처에 대응하는 명령을 수행하는 동작을 수행할 수 있다. 일 실시예에 따른 컴퓨터 프로그램 제품은, 웨어러블 기기에 포함된 센서를 이용하여, 센서의 감지 영역 내에 위치하는 손을 인식하는 동작; 손의 방향과 손에 연결된 적어도 하나의 인체 부위의 위치 관계에 기반하여, 손 에 연결된 어깨의 위치를 추정하는 동작; 및 추정된 위치에서 진정 사용자의 어깨가 존재할 확률 정보를 이용하 여, 인식된 손이 진정 사용자의 손인지 판단하는 동작을 수행하도록 하는 프로그램이 저장된 기록매체를 포함할 수 있다."}
{"patent_id": "10-2018-0169909", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 발명에 대해 구체적으로 설명하기로 한다. 본 발명에서 사용되는 용어는 본 발명에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상세 히 그 의미를 기재할 것이다. 따라서 본 발명에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지 는 의미와 본 발명의 전반에 걸친 내용을 토대로 정의되어야 한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"...부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 아래에서는 첨부한 도면을 참고하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설 명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 도 1은 일 실시예에 따른, 웨어러블 기기를 착용한 진정 사용자의 손을 식별하는 시스템을 설명하기 위한 도면 이다. 일 실시예에 의하면, 진정 사용자의 손을 식별하는 시스템(이하, 시스템)은 웨어러블 기기를 포함할 수 있다. 그러나 도시된 구성요소보다 많은 구성요소에 의해 시스템이 구현될 수도 있다. 예를 들어, 시스템은 외 부 장치(예컨대, 외부의 서버 장치, 호스트 단말 등)를 더 포함할 수 있다. 시스템이 외부 장치를 더 포함하는 실시예에 대해서는 도 18을 참조하여 후에 자세히 살펴보기로 한다. 일 실시예에 따른 웨어러블 기기는 머리 부분에 장착 가능한 헤드 마운트 디스플레이(HMD: Head mounted display)의 일종일 수 있다. 예를 들어, 웨어러블 기기는 증강현실(Augmented Reality; AR) 기기, 가상 현실(Virtual Reality; VR) 기기를 포함할 수 있으나, 이에 한정되는 것은 아니다. AR 기기는 실제 환경에 컴퓨 터 모델링을 통해 생성한 가상의 오브젝트를 겹쳐서 보이게 하여 공간과 상황에 대한 가상 정보를 제공하는 기 기일 수 있다. VR 기기는 사용자의 시야각 전체를 가상 영상으로 채울 수 있는 HMD일 수 있다. 한편, 본 명세서에서는 설명의 편의상 웨어러블 기기가 안경 형태인 경우를 예로 들어 설명하나, 이에 한 정되는 것은 아니다. 예를 들어, 웨어러블 기기는 헬멧 형태, 모자 형태, 안대 형태일 수도 있으며, 한쪽눈만 가리는 형태일 수도 있다. 일 실시예에 의하며, 웨어러블 기기는, 손의 제스처를 인식하는 사용자 인터페이스를 포함할 수 있다. 예 를 들어, 웨어러블 기기는, 손의 제스처에 따라 다양한 명령을 수행할 수 있다. 예컨대, 웨어러블 기기 는, 손의 제스처에 따라, 애플리케이션을 실행하거나, 컨텐츠를 재생하거나, 가상의 오브젝트를 이동, 회 전, 삭제, 복사하거나, 컨텐츠를 외부 장치로 전송하는 등의 명령을 수행할 수 있다. 한편, 일 실시예에 의하면, 웨어러블 기기는, 웨어러블 기기를 착용한 진정 사용자(예컨대, 제 1 사용자)의 손과 타인의 손을 구별할 수 있다. 예를 들어, 도 1에 도시된 바와 같이, 제 1 사용자가 웨 어러블 기기를 착용하고, 제 2 사용자와 함께 작업 하는 경우, 제 1 사용자의 웨어러블 기기는, 웨어러블 기기의 감지 영역 내로 들어오는 손을 감지할 수 있다. 이때, 제 1 사용자와 제 2 사 용자가 함께 작업하고 있으므로, 웨어러블 기기의 감지 영역 내로 들어오는 손은 제 1 사용자 의 손일 수도 있고, 제 2 사용자의 손일 수도 있다. 이때, 웨어러블 기기는, 감지 영역 내로 들어오는 손이 제 1 사용자의 손인지 아닌지 여부를 판단하고, 감지 영역 내로 들어오 는 손이 제 1 사용자의 손인 경우에만 손의 제스처를 인식함으로써, 웨어러블 기기가 오작동하는 것을 방지할 수 있다. 예를 들어, 감지 영역 내로 들어오는 손이 제 2 사용자의 손인 경우, 웨어 러블 기기는, 제 2 사용자의 손의 제스처는 무시할 수 있다. 본 명세서에서 '진정 사용자'란 웨어러블 기기를 착용한 해당 사용자를 의미할 수 있다. 또한, 본 명세서 에서 '감지 영역'은 웨어러블 기기에 장착된 이미지 센서(예컨대, 일반 영상 장치, 깊이 카메라, 초 음파 카메라, 적외선 카메라, 동적 비전 센서 등)의 시야각(FOV) 범위 또는 상기 이미지 센서를 통해 획득되는 이미지의 범위를 의미할 수 있다. 이하에서는, 웨어러블 기기가 진정 사용자의 손을 식별함으로써, 오작동을 방지하는 방법에 대해서 자세 히 살펴보기로 한다. 도 2는 일 실시예에 따른, 웨어러블 디바이스의 진정 사용자의 손을 식별하는 방법을 설명하기 위한 순서도이다. 단계 S210에서, 웨어러블 기기는, 웨어러블 기기에 포함된 센서를 이용하여, 센서의 감지 영역 내에 위치하는 손을 인식할 수 있다. 일 실시예에 의하면, 웨어러블 기기가 손을 인식한다는 것은 감지 영역을 촬영한 이미지를 획득하고, 획득된 이미지에서 손을 검출하는 것을 포함할 수 있다. 이때, 획득된 이미지는, 웨어러블 기기 를 착용하고 있는 진정 사용자의 관점에서의 영상일 수 있으나, 이에 한정되는 것은 아니다. 예를 들어, 웨어러블 기기는, 센서(예컨대, 이미지 센서)를 이용하여, 센서의 감지 영역 내에 위치 하는 손을 포함하는 이미지를 획득(예컨대, 캡쳐)할 수 있다. 일 실시예에 의하면, 이미지는 컬러 이미지(RGB 이미지)일 수도 있고, 흑백 이미지일 수도 있다. 또한, 이미지는 3차원 이미지이거나 2차원 이미지일 수 있으며, 동영상일 수도 있고, 정지 영상일 수도 있으나, 이에 한정되는 것은 아니다. 예를 들어, 이미지는 초음 파 이미지, 적외선 이미지, 동적 비전 센서 이미지 중 적어도 하나일 수도 있다. 일 실시예에 의하면, 웨어러블 기기는 기 정의된 손의 템플릿 이미지를 이용하여, 감지 영역 내에 위치하는 손을 인식할 수 있다. 예를 들어, 웨어러블 기기는, 기 정의된 손의 템플릿 이미지와 감지 영역 을 촬영하여 획득한 이미지를 비교함으로써, 감지 영역 내에 손이 위치하는지 감지할 수 있다. 또한, 웨어러블 기기는, 기 정의된 손의 템플릿 이미지를 이용하여, 센서의 감지 영역 내에 위치하는 손이 오른손인지 왼손인지 판단할 수 있다. 일 실시예에 의하면, 웨어러블 기기는 기계 학습 모델(인공 지능 모델이라고 표현될 수도 있음)을 이용하 여 감지 영역 내에 위치하는 손을 인식할 수 있다. 예를 들어, 웨어러블 기기는 다양한 손의 이미지 를 학습한 DNN(Deep Neural Network) 모델을 이용하여 감지 영역 내에 위치하는 손을 인식할 수 있다. 웨 어러블 기기가 기계 학습 모델(예컨대, DNN)에 감지 영역을 촬영한 이미지를 입력하는 경우, 기계 학습 모델은 입력된 이미지에서 손의 형상을 검출할 수 있다. 단계 S220에서, 웨어러블 기기는, 인식된 손의 방향과 인식된 손에 연결된 적어도 하나의 인체 부위의 위 치 관계에 기반하여, 인식된 손과 연결된 어깨의 위치를 추정할 수 있다. 일 실시예에 의하면, 웨어러블 기기는 감지 영역을 촬영한 이미지로부터 손을 검출하고, 검출된 손 에 연결된 적어도 하나의 인체 부위를 감지 영역을 촬영한 이미지로부터 더 검출할 수 있다. 손에 연결된 적어도 하나의 인체 부위는, 팔목, 하박, 팔꿈치, 상박 중 적어도 하나를 포함할 수 있으나, 이에 한정되는 것 은 아니다. 이때, 웨어러블 기기는 이미지로부터 검출된 손 및 손에 연결된 적어도 하나의 인체 부위의 위치 관계를 이용하여, 검출된 손에 연결된 어깨의 위치를 최종적으로 추정할 수 있다. 예를 들어, 도 3을 참조하면, 웨어러블 기기는 이미지 센서(예컨대, 제스처 센서)를 이용하여 감지 영역 의 이미지를 획득할 수 있다(S300). 예컨대, 웨어러블 기기는, 제 1 이미지와 제 2 이미지 를 획득할 수 있다. 이 경우, 웨어러블 기기는 손 및 팔 검출기를 이용하여, 제 1 이미지 및 제 2 이미지 각각으로부터 손 (예컨대, 손의 방향) 또는 손에 연결된 적어도 하나의 인체 부위를 검출 할 수 있다. 예를 들어, 단계 S310에서 웨어러블 기기는 기계 학습 모델(예컨대, DNN)을 이용하여 제 1 이미지 및 제 2 이미지 각각으로부터 손을 검출하는 동작을 수행할 수 있다. 이때, 웨어러블 기기 는, 손의 방향 또는 검출된 손이 오른손인지 왼손인지 여부를 결정할 수도 있다. 예를 들어, 웨어러블 기 기는, 제 1 이미지 및 제 2 이미지에서 손의 방향은 북북서 방향이라고 결정하고, 검출된 손은 오른손이라고 결정할 수 있다. 단계 S320에서 웨어러블 기기는 기계 학습 모델(예컨대, DNN)을 이용하여 제 1 이미지 및 제 2 이미지 각각으로부터 손에 연결된 하박(forearm)을 검출하는 동작을 수행할 수 있다. 단계 S330에서, 웨어러블 기기는 기계 학습 모델(예컨대, DNN)을 이용하여 제 1 이미지 및 제 2 이미지 각각으로부터 하박에 연결된 상박을 검출하는 동작을 수행할 수 있다. 단계 S340에서 웨어러블 기기는 기계 학습 모델(예컨대, DNN)을 이용하여 제 1 이미지 및 제 2 이미지 각각으로부터 상 박에 연결된 어깨를 검출하는 동작을 수행할 수 있다. 도 3의 단계 S310 내지 단계 S342은, 순차적으로 수행될 수도 있고, 동시에 수행될 수도 있다. 도 4를 참조하면, 웨어러블 기기가 단계 S310 내지 단계 S340을 수행한 결과, 제 1 이미지로부터 손 , 상박을 검출할 수 있고, 제 2 이미지로부터 손, 상박, 하박을 검출할 수 있 다. 이때, 웨어러블 기기는, 검출된 손의 방향 및 검출된 인체 부위들의 위치 관계에 기반하여 어깨의 위 치를 추정할 수 있다. 예를 들어, 웨어러블 기기는, 제 1 이미지에서 검출된 손의 방향 및 손 과 상박의 위치 관계에 기반하여, 상박에 연결된 하박의 위치 및 어깨의 위치를 추정할 수 있다. 또한, 웨어러블 기기는, 제 2 이미지에서 검출된 손, 상박 및 하박의 위치 관 계에 기반하여, 하박에 연결된 어깨의 위치를 추정할 수 있다. 도 5를 참조하면, 웨어러블 기기는, 기계 학습 모델을 이용하여, 제 1 이미지 또는 제 2 이미지 에서 검출된 손에 연결된 어깨의 위치를 추정할 수 있다. 예를 들어, 웨어러블 기기는, Generative DNN 모델 또는 Discriminative DNN 모델을 이용하여 손에 연결된 어깨의 위치를 추정할 수 있다. Generative DNN 모델은 입력 데이터로부터 특정한 수식 또는 규칙에 따라 결과 값을 찾는 기계 학습 모델 을 의미할 수 있으며, Discriminative DNN 모델는 입력 데이터를 기존 데이터베이스와 비교하여 결과 값을 찾는 기계 학습 모델을 의미할 수 있다. 일 실시예에 의하면, 웨어러블 기기가 제 1 이미지를 Generative DNN 모델에 입력하는 경우, Generative DNN 모델은 제 1 이미지에서 검출된 손과 하박 사이의 각도와 상박의 위치 간 의 규칙을 이용하여, 상박의 위치를 추정할 수 있다. 그리고 Generative DNN 모델은 제 1 이미지에서 검출된 손, 하박 및 추정된 위치에서의 상박 사이의 각도와 어깨의 위치 간의 규칙을 이용하여, 어깨 의 위치를 추정할 수 있다. 일 실시예에 의하면, 웨어러블 기기가 제 2 이미지를 Discriminative DNN 모델에 입력하는 경 우, Discriminative DNN 모델은 제 2 이미지에서 검출된 손, 하박 및 상박의 위치와 유사한 데이터를 가지는 이미지를 데이터베이스에 있는 이미지들 중에서 찾음으로써, 어깨의 위치를 추정할 수 있다. 한편, 일 실시예에 의하면, 이미지 센서를 통해 획득된 감지 영역에 대한 이미지에 어깨가 포함되어 있는 경우, 웨어러블 기기는 감지 영역에 대한 이미지를 분석함으로써, 어깨의 위치를 바로 추정할 수 있 다. 이하에서는, 설명의 편의상, 감지 영역에 대한 이미지에 어깨가 포함되지 않은 경우를 예로 들어 설명 하기로 한다. 단계 S230에서, 웨어러블 기기는, 추정된 어깨의 위치에서 진정 사용자의 어깨가 존재할 확률 정보를 이 용하여, 인식된 손이 진정 사용자의 손인지 판단할 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 진정 사용자의 어깨 존재 확률 정보를 획득할 수 있다. 진정 사용 자의 어깨 존재 확률 정보는, 웨어러블 기기(또는 진정 사용자의 머리)를 기준으로, 3차원 공간 좌표 상 에서, 진정 사용자의 어깨가 실제로 존재할 확률 값들을 포함할 수 있다. 진정 사용자의 어깨 존재 확률 정보는 확률 값들의 세트로 표현될 수도 있고, 확률 분포도로 표현될 수도 있으나, 이에 한정되는 것은 아니다. 한편, 진정 사용자의 어깨 존재 확률 정보는 왼쪽 어깨의 존재 확률 정보 및 오른쪽 어깨의 존재 확률 정보를 포함할 수 있다. 예를 들어, 도 6을 참조하면, 웨어러블 기기의 중심 또는 진정 사용자의 머리의 중심을 기준 점으로 하여, 진정 사용자의 왼쪽 어깨가 존재할 확률 분포도는 제 1 그래프와 같이 나타 날 수 있고, 진정 사용자의 오른쪽 어깨가 존재할 확률 분포도는 제 2 그래프와 같이 나타날 수 있다. 이때, 진정 사용자의 어깨 존재 확률 정보는 제 1 그래프와 제 2 그래프를 포함하는 것일 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 진정 사용자의 어깨 존재 확률 정보로부터 추정된 어깨의 위치에 서의 확률 값을 획득할 수 있다. 웨어러블 기기는, 추정된 어깨의 위치에서의 확률 값이 임계 값(예컨대, 95%)보다 큰 경우, 감지 영역에서 인식된 손이 진정 사용자의 손이라고 판단할 수 있다. 예를 들어, 도 7을 참조하면, 웨어러블 기기는, 감지 영역에 대한 이미지에서 손을 검출하고, 기계 학습 모델(예컨대, Generative DNN 모델 또는 Discriminative DNN 모델)을 이용하여, 검출된 손에 연결된 하박의 위치, 하박에 연결된 상박의 위치, 상박에 연결된 어깨의 위치를 추정할 수 있다. 이때, 웨어러블 기기는, 진정 사용자의 어깨 존재 확률 정보와 추정된 어깨의 위치를 비교함으 로써, 추정된 어깨의 위치에서 진정 사용자의 어깨가 존재할 확률 값을 획득할 수 있다. 일 실시예에 의하 면, 검출된 손이 오른손이므로, 웨어러블 기기는 추정된 위치에서 진정 사용자의 오른쪽 어깨 가 존재할 확률 값을 획득할 수 있다. 추정된 위치에서 진정 사용자의 오른쪽 어깨가 존재할 확률 값이 임 계 값(예컨대, 90%)이상인 경우, 웨어러블 기기는 검출된 손이 진정 사용자의 손이라고 판단할 수 있다. 한편, 추정된 위치에서 진정 사용자의 어깨가 존재할 확률 값이 임계 값(예컨대, 90%)보다 작은 경우, 웨어러블 기기는 검출된 손이 타인의 손이라고 판단할 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 진정 사용자의 어깨 존재 확률 정보와 추정된 어깨의 위치 를 비교함으로써, 추정된 어깨의 위치에서 진정 사용자의 어깨가 존재할 확률 분포(또는 패턴)를 획 득할 수 있다. 예를 들어, 검출된 손이 오른손이므로, 웨어러블 기기는 추정된 위치에서 진정 사용자의 오른쪽 어깨가 존재할 확률 분포(또는 패턴)를 획득할 수 있다. 웨어러블 기기는, 추정된 위치 에서 진정 사용자의 오른쪽 어깨가 존재할 확률 분포(또는 패턴)를 이용하여, 검출된 손이 진정 사용 자의 손이라고 판단할 수 있다. 예를 들어, 추정된 위치에서 진정 사용자의 오른쪽 어깨가 존재할 확률 분 포(또는 패턴)가 기준 분포(또는 패턴) 이상인 경우, 웨어러블 기기는 검출된 손이 진정 사용자의 손이라고 판단할 수 있다. 도 8은 일 실시예에 따른, 진정 사용자의 손의 제스처를 식별하는 방법을 설명하기 위한 순서도이다. 단계 S810 및 단계 S820에서, 웨어러블 기기는, 인식된 손이 진정 사용자의 손이라고 판단되는 경우, 인 식된 손의 제스처를 식별할 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 감지 영역의 이미지에서 검출된 손의 동작과 데이터베이스에 기 정의된 손의 동작을 비교함으로써, 인식된 손의 제스처를 식별할 수 있다. 예를 들어, 전체적인 손의 형태, 접은 손가락의 수 또는 종류, 손이 움직이는 방향, 편 손가락이 가리키는 방향 등에 따라 서로 다른 제스처로 식별될 수 있다. 예컨대, 웨어러블 기기는, 스와이프 제스처, 드래그 제스처, 핀치 제스처, 터치 제스처, 탭 제스처, 더블 클릭 제스처, 주먹 제스처 등을 식별할 수 있으나, 이에 한정되는 것은 아니다. 일 실시예에 의하면, 웨어러블 기기는 기계 학습 모델(예컨대, DNN)을 이용하여 감지 영역의 이미지 에 포함된 손의 제스처를 식별할 수도 있다. 예를 들어, 웨어러블 기기가 기계 학습 모델에 감지 영역 의 이미지를 입력하는 경우, 기계 학습 모델은 감지 영역의 이미지에 포함된 손의 제스처에 대응하는 명령어(instruction)를 결과 값으로 출력할 수 있다. 단계 S830에서, 웨어러블 기기는, 식별된 제스처에 대응하는 명령을 수행할 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 제스처와 명령을 맵핑한 테이블에 기초하여, 제스처에 대응하는 명령을 수행할 수 있다. 예를 들어, 제스처에 대응하는 명령은 특정 컨텐트를 선택하는 명령, 특정 애플리케이 션을 실행하는 명령, 가상의 객체를 조작하는 명령(예컨대, 가상의 객체를 확대 또는 축소시키는 명령, 가상의 개체를 이동시키는 명령, 가상의 객체를 복사하는 명령, 가상의 객체를 삭제하는 명령 등), 웨어러블 기기 의 전원을 제어하는 명령, 외부 장치로 데이터를 전송하거나 외부 장치로부터 데이터를 수신하는 명령 등 을 포함할 수 있으나, 이에 한정되는 것은 아니다. 단계 S810 및 단계 S840에서, 웨어러블 기기는, 인식된 손이 진정 사용자의 손이 아니라고 판단되는 경우, 인식된 손을 무시할 수 있다. 예를 들어, 웨어러블 기기는, 인식된 손이 진정 사용자의 손이 아니라고 판단되는 경우, 인식된 손의 제 스처를 식별하지 않을 수 있다. 한편, 도 8에서는, 웨어러블 기기가 진정 사용자의 제스처에 따라 동작하는 경우를 예로 들어 설명하였으 나, 이에 한정되는 것은 아니다. 일 실시예에 의하면, 웨어러블 기기는, 진정 사용자의 음성 명령에 따라 서, 음성 명령에 대응하는 동작을 수행할 수도 있다. 예를 들어, 웨어러블 기기는, 진정 사용자의 음성의 특징을 식별하고, 수신된 음성의 특징이 진정 사용자의 음성의 특징과 일치한다고 판단되는 경우에만 음성 명령 을 수행할 수도 있다. 도 9는 일 실시예에 따른, 웨어러블 기기가 타인의 손을 무시하는 동작을 설명하기 위한 도면이다. 도 9를 참조하면, 웨어러블 기기는, 감지 영역 내의 제 1 손과 제 2 손을 인식할 수 있다. 예를 들어, 웨어러블 기기는 제 1 손과 제 2 손을 포함하는 감지 영역의 이미지 를 획득할 수 있다. 웨어러블 기기는 이미지 내에서 제 1 손을 검출하고, 제 1 손에 연결된 제 1 어깨의 제 1 위치를 추정할 수 있다. 또한, 웨어러블 기기는 이미지 내에서 제 2 손을 검출하고, 제 2 손에 연결된 제 2 어깨의 제 2 위치를 추정할 수 있다. 이때, 제 1 손 및 제 2 손 모두 오른손이므로, 웨어러블 기기는, 웨어러블 기기를 착용 한 진정 사용자의 머리를 기준점으로 하여, 진정 사용자의 오른쪽 어깨가 존재할 확률 정보를 획득할 수 있다. 웨어러블 기기는, 진정 사용자의 오른쪽 어깨가 존재할 확률 정보로부터 제 1 위치에서의 제 1 확률 값과 제 2 위치에서의 확률 값을 추출할 수 있다. 예를 들어, 제 1 위치에서 진정 사용자의 오른쪽 어깨가 존재할 확률 값은 99%이고, 제 2 위치에서 진정 사용자의 오른쪽 어깨가 존재할 확률 값은 0%일 수 있다. 이 경우, 웨어러블 기기는 제 1 손을 진정 사용자의 손으로 판단하고, 제 2 손은 진정 사용자의 손이 아니라고 판단할 수 있다. 따라서, 웨어러블 기기는, 제 1 손의 제스처에 대해서만 반응하고, 제 2 손의 제스처는 무시할 수 있다. 도 10은 일 실시예에 따른, 웨어러블 기기가 타인의 손을 검출하는 동작을 설명하기 위한 도면이다. 도 10의 제 1 실시예를 참조하면, 진정 사용자의 왼쪽에 타인이 서 있는 상태에서, 웨어러블 기기 의 감지 영역 내로 타인의 제 1 오른손이 들어올 수 있다. 이때, 웨어러블 기기는 감지 영역의 제 1 이미지를 획득하고, 제 1 이미지로부터 제 1 오른손을 검출할 수 있다. 웨어러블 기기 는 제 1 오른손에 연결된 제 1 어깨의 위치를 추정하고, 제 1 어깨의 추정된 위치와 진정 사용자의 어깨 존재 확률 정보를 비교함으로써, 추정된 위치에서 진정 사용자의 오른쪽 어깨가 존재할 제 1 확률 값을 식별할 수 있다. 이때, 제 1 확률 값은 0.00001%이므로, 웨어러블 기기는 제 1 오른손이 타인의 손 이라고 판단하고, 제 1 오른손의 제스처를 무시할 수 있다. 도 10의 제 2 실시예를 참조하면, 진정 사용자의 왼쪽에 타인이 서 있는 상태에서, 웨어러블 기기 의 감지 영역 내로 타인의 제 2 왼손이 들어올 수 있다. 이때, 웨어러블 기기는 감지 영역의 제 2 이미지를 획득하고, 제 2 이미지로부터 제 2 왼손을 검출할 수 있다. 웨어러블 기기는 제 2 왼손에 연결된 제 2 어깨의 위치를 추정하고, 제 2 어깨의 추정된 위치와 진정 사용자의 어깨 존재 확률 정보를 비교함으로써, 추정된 위치에서 진정 사용자의 왼쪽 어깨가 존재할 제 2 확률 값을 식 별할 수 있다. 이때, 제 2 확률 값은 0%이므로, 웨어러블 기기는 제 2 왼손이 타인의 손이라고 판단하고, 제 2 왼손의 제스처를 무시할 수 있다. 도 10의 제 3 실시예를 참조하면, 진정 사용자의 어깨 위쪽에서 타인이 팔을 뻗는 경우, 웨어러블 기기 의 감지 영역 내로 타인의 제 3 오른손이 들어올 수 있다. 이때, 웨어러블 기기는 감지 영역 의 제 3 이미지를 획득하고, 제 3 이미지로부터 제 3 오른손을 검출할 수 있다. 웨어러블 기 기는 제 3 오른손에 연결된 제 3 어깨의 위치를 추정하고, 제 3 어깨의 추정된 위치와 진정 사용자의 어깨 존재 확률 정보를 비교함으로써, 추정된 위치에서 진정 사용자의 오른쪽 어깨가 존재할 제 3 확률 값을 식별할 수 있다. 제 3 어깨의 추정된 위치가 기준점(예컨대, 진정 사용자의 머리 중심)과 같은 높이에 존재하므로, 제 3 확률 값은 0%일 수 있다. 따라서, 웨어러블 기기는 제 3 오른손이 타인의 손이라고 판단하고, 제 3 오른손의 제스처를 무시할 수 있다. 도 11은 일 실시예에 따른, 진정 사용자의 손이 감지 영역 내에 위치하는지 여부를 나타내는 알림 정보를 제공 하는 방법을 설명하기 위한 순서도이다. 단계 S1110 및 단계 S1120에서, 웨어러블 기기는, 인식된 손이 진정 사용자의 손이라고 판단되는 경우, 진정 사용자의 손이 감지 영역 내에 위치하는지 여부를 나타내는 알림 정보를 제공할 수 있다. 일 실시예에 의 하면, 웨어러블 기기는, 시각적 신호, 청각적 신호 및 진동 신호 중 적어도 하나를 이용하여 알림 정보를 제공할 수 있다. 예를 들어, 웨어러블 기기는, 진정 사용자의 손이 감지 영역 내로 진입하는 경우, 웨어러블 기기 가 제공하는 디스플레이 영역에 제 1 식별 영상을 표시함으로써, 진정 사용자의 손이 감지 영역 내 에 있음을 진정 사용자에게 알릴 수 있다. 이 경우, 진정 사용자는 자신의 손이 감지 영역 내에 있는 것을 알 수 있으므로, 손의 제스처를 이용하여 웨어러블 기기를 자유롭게 조작할 수 있다. 또한, 웨어러블 기기는, 진정 사용자의 손이 감지 영역을 벗어나는 경우, 웨어러블 기기의 디 스플레이 영역에 제 2 식별 영상을 표시함으로써, 진정 사용자의 손이 감지 영역에서 벗어났음을 진정 사 용자에게 알릴 수 있다. 이 경우, 진정 사용자가 자신의 손이 감지 영역 내에 있는 것으로 착각하는 것을 방지할 수 있으며, 진정 사용자가 손의 제스처로 웨어러블 기기를 조작하기 위해서 다시 감지 영역 내로 손을 이동시키도록 유도할 수 있다. 일 실시예에 의하면, 제 1 식별 영상과 제 2 식별 영상은 사용자 또는 시스템에 의해 기 정의된 영상일 수 있으 며, 사용자의 조작에 의해 변경될 수도 있다. 예를 들어, 제 1 식별 영상은 디스플레이 영역의 가장자리가 파랑 색으로 물드는 제 1 애니메이션일 수 있으며, 제 2 식별 영상은 디스플레이 영역의 가장자리가 붉은 색으로 물 드는 제 2 애니메이션일 수 있다. 또한, 제 1 식별 영상은 동그라미 안에 손 아이콘이 그려진 영상일 수 있고, 제 2 식별 영상은 손 아이콘 위에 X 표시가 중첩된 영상일 수도 있다. 제 1 식별 영상과 제 2 식별 영상의 종류 는 다양할 수 있다. 한편, 웨어러블 기기는, 진정 사용자의 손이 감지 영역 내로 진입하는 경우, 제 1 사운드(예컨대, 제 1 음악, '손이 감지됩니다'라는 제 1 음성 코멘트 등)를 출력함으로써, 진정 사용자의 손이 감지 영역 내에 있음을 진정 사용자에게 알릴 수 있다. 반면, 웨어러블 기기는, 진정 사용자의 손이 감지 영역(10 0)을 벗어나는 경우, 제 2 사운드(예컨대, 제 2 음악, '손이 감지되지 않습니다'라는 제 2 음성 코멘트 등)를 출력함으로써, 진정 사용자의 손이 감지 영역에서 벗어났음을 진정 사용자에게 알릴 수 있다. 단계 S1110 및 단계 S1130에서, 웨어러블 기기는, 인식된 손이 진정 사용자의 손이 아니라고 판단되는 경 우, 인식된 손을 무시할 수 있다. 단계 S1130은 도 8의 단계 S840에 대응되므로, 구체적인 설명은 생략하기로 한다. 도 12는 일 실시예에 따른, 알림 정보를 설명하기 위한 도면이다. 도 12에서는 감지 영역 내의 손이 진정 사용자의 손으로 판단된 경우를 예로 들어 설명하기로 한다. 도 12의 제 1 실시예를 참조하면, 웨어러블 기기를 착용한 진정 사용자의 손이 웨어러블 기기 의 감지 영역 내로 들어올 수 있다. 이때, 웨어러블 기기는 디스플레이 영역에 진정 사 용자의 손이 감지 영역 내로 들어왔음을 나타내는 제 1 식별 영상을 표시할 수 있다. 예를 들어, 웨어러블 기기는 디스플레이 영역 중에서 진정 사용자의 손이 나타난 오른쪽 아래 부분에 반투명한 파랑색 영상 을 표시할 수 있다. 도 12의 제 2 실시예를 참조하면, 웨어러블 기기를 착용한 진정 사용자의 손이 웨어러블 기기 의 감지 영역을 벗어날 수 있다. 이때, 웨어러블 기기는 디스플레이 영역에 진정 사용 자의 손이 감지 영역 내로 들어왔음을 나타내는 제 2 식별 영상을 표시할 수 있다. 예를 들어, 웨어러블 기기는 디스플레이 영역 중에서 진정 사용자의 손이 사라진 오른쪽 아래 부분에 반투명한 붉은색영상을 표시할 수 있다. 도 13은 일 실시예에 따른, 감지 영역의 위치 별로 진정 사용자의 손이 감지될 확률에 관한 정보에 기반하여, 진정 사용자의 손을 식별하는 방법을 설명하기 위한 순서도이다. 단계 S1310에서, 웨어러블 기기는, 웨어러블 기기에 포함된 센서를 이용하여, 센서의 감지 영역 내에 위치하는 손을 인식할 수 있다. 일 실시예에 의하면, 웨어러블 기기가 손을 인식한다는 것은 감지 영역을 촬영한 이미지를 획득하 고, 획득된 이미지에서 손을 검출하는 것을 포함할 수 있다. 이때, 획득된 이미지는, 웨어러블 기기를 착 용하고 있는 진정 사용자의 관점에서의 영상일 수 있으나, 이에 한정되는 것은 아니다. 단계 S1310은 도 2의 단계 S210에 대응하므로, 구체적인 설명은 생략하기로 한다. 단계 S1320에서, 웨어러블 기기는, 감지 영역의 위치 별로 진정 사용자의 손이 감지될 확률에 관한 정보에 기반하여, 감지 영역 내에서 인식된 손이 진정 사용자의 손인지 판단할 수 있다. 일 실시예에 의하면, 진정 사용자의 손이 감지될 확률에 관한 정보는, 감지 영역 상에서 진정 사용자의 손 이 나타날 수 있는 방향에 관한 확률 정보를 포함할 수도 있다. 예를 들어, 도 14를 참조하면, 감지 영역 을 네 영역으로 구분했을 때, 진정 사용자의 오른손은 제 4 사분면(④)에서 제 1 사분면(①) 방향으로 나타날 확률이 높을 수 있다. 반면, 진정 사용자의 왼손은 제 3 사분면(③)에서 제 2 사분면(②) 방향으로 나타날 확률 이 높을 수 있다. 따라서, 감지 영역 내에서 제 1 오른손이 제 4 사분면(④)에서 제 1 사분면(①) 방향으로 나타나는 경우, 웨어러블 기기는, 제 1 오른손을 진정 사용자의 오른손으로 판단하고, 감지 영역내의 제 1 오른손의 명령에 따를 수 있다. 반면, 감지 영역 내에서 제 2 오른손이 제 2 사분면 (②)에서 제 4 사분면(④) 방향으로 나타나는 경우, 웨어러블 기기는, 제 2 오른손을 진정 사용자 의 오른손이 아니라고 판단하고, 감지 영역내의 제 2 오른손의 제스처를 무시할 수 있다. 일 실시예에 의하면, 진정 사용자의 손이 감지될 확률에 관한 정보는, 감지 영역 내의 손을 기준으로 손에 연결된 인체 부위들이 존재할 수 있는 위치에 관한 확률 정보를 포함할 수도 있다. 예를 들어, 도 15의 감지 영역을 촬영한 제 1 이미지를 참조하면, 제 1 이미지에서 제 1 손 및 제 1 손에 연결된 제 1 하박이 검출된 경우, 웨어러블 기기는, 제 1 손의 위치를 기준으로, 제 1 이미지 내에서 진정 사용자의 손이 존재할 수 있는 위치에 관한 제 1 확률 분포 및 제 1 이미지 내에서 진정 사용자의 손에 연결된 진정 사용자의 하박이 존재할 수 있는 위치에 관한 제 2 확률 분포를 획득할 수 있다. 웨어러블 기기는 제 1 확률 분포 및 제 2 확률 분포 를 이용하여, 제 1 손이 진정 사용자의 손인지 판단할 수 있다. 예를 들어, 제 1 손에 연결 된 제 1 하박의 위치와 제 2 확률 분포를 비교한 결과, 제 1 하박의 위치에 진정 사용자의 하박이 위치할 확률이 99%인 경우, 웨어러블 기기는 제 1 손을 진정 사용자의 손으로 판단할 수 있 다. 또한, 도 15의 감지 영역을 촬영한 제 2 이미지를 참조하면, 제 2 이미지에서 제 2 손 , 제 2 손에 연결된 제 2 하박 및 제 2 하박에 연결된 제 2 상박이 검출된 경 우, 웨어러블 기기는, 제 2 손의 위치를 기준으로, 제 2 이미지 내에서 진정 사용자의 손이 존재할 수 있는 위치에 관한 제 1 확률 분포, 제 2 이미지 내에서 진정 사용자의 손에 연결된 진정 사용자의 하박이 존재할 수 있는 위치에 관한 제 2 확률 분포 및 진정 사용자의 하박에 연결된 진정 사용 자의 상박이 존재할 수 있는 위치에 관한 제 3 확률 분포를 획득할 수 있다. 웨어러블 기기는 제 1 확률 분포, 제 2 확률 분포, 제 3 확률 분포를 이용하여, 제 2 손이 진정 사용자의 손인지 판단할 수 있다. 예를 들어, 제 2 손에 연결된 제 2 하박의 위치와 제 2 확률 분포를 비교한 결과, 제 2 하박의 위치에 진정 사용자의 하박이 위치할 확률이 99%인 경우, 웨어러블 기기(100 0)는 제 2 손을 진정 사용자의 손으로 판단할 수 있다. 또한, 제 2 하박에 연결된 제 2 상박(152 3)의 위치와 제 3 확률 분포를 비교한 결과, 제 2 상박의 위치에 진정 사용자의 상박이 위치할 확 률이 99%인 경우, 웨어러블 기기는 제 2 손을 진정 사용자의 손으로 판단할 수 있다. 예를 들어, 도 15의 감지 영역을 촬영한 제 3 이미지를 참조하면, 제 3 이미지에서 제 3 손 및 제 3 손에 연결된 제 3 하박이 검출된 경우, 웨어러블 기기는, 제 3 손의위치를 기준으로, 제 3 이미지 내에서 진정 사용자의 손이 존재할 수 있는 위치에 관한 제 1 확률 분포 및 제 3 이미지 내에서 진정 사용자의 손에 연결된 진정 사용자의 하박이 존재할 수 있는 위치에 관한 제 2 확률 분포를 획득할 수 있다. 웨어러블 기기는 제 1 확률 분포 및 제 2 확률 분포 를 이용하여, 제 3 손이 진정 사용자의 손인지 판단할 수 있다. 예를 들어, 제 3 손에 연결 된 제 3 하박의 위치와 제 2 확률 분포를 비교한 결과, 제 3 하박의 위치에 진정 사용자의 하박이 위치할 확률이 0.00001%인 경우, 웨어러블 기기는 제 3 손을 타인의 손으로 판단할 수 있다. 다시 도 13으로 돌아오면, 단계 S1330 및 단계 S1340에서, 웨어러블 기기는, 인식된 손이 진정 사용자의 손이라고 판단되는 경우, 인식된 손의 제스처를 식별할 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 감지 영역의 이미지에서 검출된 손의 동작과 데이터베이스에 기 정의된 손의 동작을 비교함으로써, 인식된 손의 제스처를 식별할 수 있다. 또한, 일 실시예에 의하면, 웨어 러블 기기는 기계 학습 모델(예컨대, DNN)을 이용하여 감지 영역의 이미지에 포함된 손의 제스처를 식별할 수도 있다. 단계 S1340은 도 8의 단계 S820에 대응되므로, 구체적인 설명은 생략하기로 한다. 단계 S1350에서, 웨어러블 기기는, 식별된 제스처에 대응하는 명령을 수행할 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 제스처와 명령을 맵핑한 테이블에 기초하여, 제스처에 대응하는 명령을 수행할 수 있다. 예를 들어, 제스처에 대응하는 명령은, 특정 컨텐트를 선택하는 명령, 특정 애플리케이 션을 실행하는 명령, 가상의 객체를 조작하는 명령(예컨대, 가상의 객체를 확대 또는 축소시키는 명령, 가상의 개체를 이동시키는 명령, 가상의 객체를 복사하는 명령, 가상의 객체를 삭제하는 명령 등), 웨어러블 기기 의 전원을 제어하는 명령, 외부 장치로 데이터를 전송하거나 외부 장치로부터 데이터를 수신하는 명령 등 을 포함할 수 있으나, 이에 한정되는 것은 아니다. 단계 S1330 및 단계 S1360에서, 웨어러블 기기는, 인식된 손이 진정 사용자의 손이 아니라고 판단되는 경 우, 인식된 손을 무시할 수 있다. 예를 들어, 웨어러블 기기는, 인식된 손이 타인의 손이라고 판단되는 경우, 인식된 손의 제스처를 식별하지 않을 수 있다. 도 16은 일 실시예에 따른, 인식된 손의 특징 정보에 기초하여, 진정 사용자의 손을 식별하는 방법을 설명하기 위한 순서도이다. 단계 S1610에서, 웨어러블 기기는, 웨어러블 기기에 포함된 센서를 이용하여, 센서의 감지 영역 내 에 위치하는 손을 인식할 수 있다. 단계 S1620에서, 웨어러블 기기는, 인식된 손과 인식된 손에 연결된 적어도 하나의 인체 부위의 위치 관 계에 기반하여, 인식된 손에 연결된 어깨의 위치를 추정할 수 있다. 예를 들어, 웨어러블 기기는, 인식된 손의 방향 및 인식된 손에 연결된 적어도 하나의 인체 부위의 위치 관계에 기반하여, 인식된 손에 연결된 어깨 의 위치를 추정할 수 있다. 단계 S1630에서, 웨어러블 기기는, 추정된 위치에서 진정 사용자의 어깨가 존재할 확률 정보를 이용하여, 인식된 손이 진정 사용자의 손인지 판단할 수 있다. 단계 S1610 내지 단계 S1630은 도 2의 단계 S210 내지 단계 S230에 대응되므로, 구체적인 설명은 생략하기로 한 다. 단계 S1640 및 단계 S1650에서, 웨어러블 기기는, 인식된 손이 진정 사용자의 손이라고 판단되는 경우, 인식된 손의 제 1 특징 정보를 획득할 수 있다. 그리고, 웨어러블 기기는, 손의 제 1 특징 정보에 기초하 여, 감지 영역 내에 위치하는 적어도 하나의 손이 진정 사용자의 손인지 판단할 수 있다. 일 실시예에 의하면, 손의 제 1 특징 정보는, 손의 크기, 손의 색상, 장갑 착용 유무, 매니큐어 색상, 손톱의 길이 등에 관한 정보를 포함할 수 있다. 또한, 손의 제 1 특징 정보는 소매의 길이 소매의 색상, 소매의 형태 등에 관한 정보를 포함할 수도 있으나, 이에 한정되는 것은 아니다. 예를 들어, 단계 S1660에서, 웨어러블 기기는, 손의 제 1 특징 정보와 감지 영역 내에 위치하는 적 어도 하나의 손의 제 2 특징 정보를 비교할 수 있다. 단계 S1670 및 단계 S1680에서, 웨어러블 기기는, 제 1 특징 정보와 제 2 특징 정보의 유사도가 임계 값 (예컨대, 98%) 이상인 경우, 적어도 하나의 손이 진정 사용자의 손이라고 판단할 수 있다. 한편, 단계 S1670에서, 웨어러블 기기는, 제 1 특징 정보와 제 2 특징 정보의 유사도가 임계 값(예컨대, 98%)보다 작은 경우, 단계 S1620으로 돌아갈 수 있다. 예를 들어, 웨어러블 기기는, 제 1 특징 정보와 제 2 특징 정보의 유사도가 임계 값보다 작은 경우, 적어도 하나의 손에 연결된 적어도 하나의 어깨의 위치를 추정 할 수 있다. 그리고 웨어러블 기기는, 추정된 위치에서 진정 사용자의 어깨가 존재할 확률 정보를 이용하 여, 적어도 하나의 손이 진정 사용자의 손인지 판단할 수 있다. 예컨대, 진정 사용자가 반지를 착용하거나, 옷을 갈아입거나, 매니큐어 색상을 변경하거나 하는 등에 의해 진정 사용자의 손의 특징 정보가 변경되는 경우, 웨어러블 기기는, 감지 영역 내에 위치하는 적어도 하낭 늬 손 중에서 제 1 특징 정보와 동일 또는 유사한 특징 정보를 갖는 손을 인식할 수 없으므로, 적어도 하나의 손에 대해서 어깨의 위치를 다시 추정하게 된다. 단계 S1640 및 단계 S1690에서, 웨어러블 기기는 인식된 손이 진정 사용자의 손이 아니라고 판단되는 경 우, 인식된 손을 무시할 수 있다. 예를 들어, 웨어러블 기기는, 인식된 손이 타인의 손이라고 판단되는 경우, 인식된 손의 제스처를 식별하지 않을 수 있다. 예를 들어, 웨어러블 기기는, 인식된 손이 타인의 손이라고 판단되는 경우, 인식된 손의 제스처를 식별하지 않을 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 추정된 어깨의 위치에 기반하여 인식된 손이 진정 사용자의 손이 라고 판단되는 경우, 인식된 손의 특징 정보를 스캔하고, 감지 영역 내에서 인식되는 손의 특징 정보가 변 하지 않는 이상 계속 인식된 손이 진정 사용자의 손이라고 판단할 수 있다. 따라서, 일 실시예에 의하면, 웨어 러블 기기는, 진정 사용자의 손의 특징 정보를 이용함으로써, 감지 영역에 손이 나타날 때마다 어깨 의 위치를 추정하고, 추정된 위치에서의 확률 값에 기초하여 진정 사용자의 손인지 판단하는 알고리즘 구현하는 데 드는 컴퓨팅 리소스를 줄일 수 있다. 도 17는 일 실시예에 따른, 웨어러블 기기가 인식된 손의 특징 정보에 기초하여, 진정 사용자의 손을 식별하는 동작을 설명하기 위한 도면이다. 도 17의 제 1 이미지를 참조하면, 웨어러블 기기는, 진정 사용자의 어깨 존재 확률 정보와 추정된 어깨의 위치를 비교함으로써, 추정된 어깨의 위치에서 진정 사용자의 어깨가 존재할 확률 값을 획득할 수 있다. 일 실시예에 의하면, 감지 영역에서 검출된 손이 오른손이므로, 웨어러블 기 기는 추정된 위치에서 진정 사용자의 오른쪽 어깨가 존재할 확률 값을 획득할 수 있다. 예를 들어, 추정된 위치에서 진정 사용자의 오른쪽 어깨가 존재할 확률 값이 95%인 경우, 웨어러블 기기는 감 지 영역에서 검출된 손이 진정 사용자의 손이라고 판단할 수 있다. 도 17의 제 2 이미지를 참조하면, 웨어러블 기기는, 감지 영역에서 검출된 손이 진정 사용자의 손이라고 판단되었으므로, 감지 영역에서 검출된 손을 스캔함으로써, 감지 영역에서 검출된 손과 관련된 제 1 특징 정보(예컨대, 손의 색상, 손가락의 반지 착용 유무, 손톱의 길이, 소매 패 턴 등)를 추출할 수 있다. 그리고 웨어러블 기기는 제 1 특징 정보를 이용하여 감지 영역에서 이후 검출되는 손들이 진정 사용자의 손인지 판단할 수 있다. 도 17의 제 3 이미지를 참조하면, 웨어러블 기기는, 감지 영역에서 검출되는 손의 특징 정보와 제 1 특징 정보를 비교할 수 있다. 감지 영역에서 검출되는 손의 특징 정보와 제 1 특징 정 보가 일치하므로, 웨어러블 기기는, 감지 영역에서 검출되는 손이 진정 사용자의 손이라고 판 단할 수 있다. 반면, 도 17의 제 4 이미지를 참조하면, 진정 사용자가 반지를 착용하여, 진정 사용자의 손과 관련된 특 징 정보가 변경될 수 있다. 즉, 감지 영역에서 검출되는 손의 특징 정보와 제 1 특징 정보가 상이해 질 수 있다. 이 경우, 웨어러블 기기는, 더 이상 제 1 특징 정보를 이용하여 감지 영역에서 검출되 는 손이 진정 사용자의 손인지 판단할 수 없다. 따라서, 웨어러블 기기는 감지 영역에서 검출 된 손에 연결된 어깨의 위치를 추정하고, 추정된 위치에서의 진정 사용자의 오른쪽 어깨가 존재할 확률 값을 이용하여, 검출된 손이 진정 사용자의 손인지 판단할 수 있다. 예를 들어, 추정된 위치에서 진정 사 용자의 오른쪽 어깨가 존재할 확률 값이 98%인 경우, 웨어러블 기기는 감지 영역에서 검출된 손 이 진정 사용자의 손이라고 판단할 수 있다. 이때, 웨어러블 기기는, 감지 영역에서 검출된 손이 진정 사용자의 손이라고 판단되었으므로, 감지 영역에서 검출된 손을 스캔함으로써, 감 지 영역에서 검출된 손과 관련된 제 2 특징 정보(예컨대, 손의 색상, 손가락의 반지 착용 유무, 손 톱의 길이, 소매 패턴 등)를 추출할 수 있다. 그리고 웨어러블 기기는 제 2 특징 정보를 이용하여 감지영역에서 이후 검출되는 손들이 진정 사용자의 손인지 판단할 수 있다. 도 18은 일 실시예에 따른, 웨어러블 기기가 외부 장치와 연동하여, 진정 사용자의 손을 식별하는 방법을 설명 하기 위한 순서도이다. 단계 S1810에서, 웨어러블 기기는 센서의 감지 영역 내에 위치하는 손을 포함하는 이미지를 획득할 수 있다. 이때, 획득된 이미지는, 웨어러블 기기를 착용하고 있는 진정 사용자의 관점에서의 영상일 수 있다. 예를 들어, 웨어러블 기기는, 센서(예컨대, 이미지 센서)를 이용하여, 센서의 감지 영역 내에 위치 하는 손을 포함하는 이미지를 획득(예컨대, 캡쳐)할 수 있다. 일 실시예에 의하면, 이미지는 컬러 이미지(RGB 이미지)일 수도 있고, 흑백 이미지일 수도 있다. 또한, 이미지는 3차원 이미지이거나 2차원 이미지일 수 있으며, 동영상일 수도 있고, 정지 영상일 수도 있으나, 이에 한정되는 것은 아니다. 예를 들어, 이미지는 초음 파 이미지, 적외선 이미지, 동적 비전 센서 이미지 중 적어도 하나일 수도 있다. 단계 S1820에서, 웨어러블 기기는, 손을 포함하는 이미지를 외부 장치로 전송할 수 있다. 이때, 웨 어러블 기기는 이미지에서 검출되는 손이 웨어러블 기기를 착용한 진정 사용자의 손인지 여부를 외 부 장치에 문의할 수 있다. 일 실시예에 의하면, 외부 장치는 웨어러블 기기에 연결된 서버 장치일 수도 있고, 웨어러블 기기 에 연결된 호스트 장치(예컨대, 모바일 단말 등)일 수도 있으나, 이에 한정되는 것은 아니다. 일 실시예 에 의하면, 웨어러블 기기는, 근거리 무선 통신망(예컨대, Bluetooth TM, Wi-Fi 등) 또는 이동 통신망을 이용하여, 외부 장치와 연결될 수 있다. 단계 S1830에서, 외부 장치는, 수신된 이미지를 분석함으로써, 손과 손에 연결된 적어도 하나의 인체 부 위의 위치 관계에 기반하여, 이미지에 포함된 손에 연결된 어깨의 위치를 추정할 수 있다. 일 실시예에 의하면, 외부 장치는, 기 정의된 손의 템플릿 이미지를 이용하여, 감지 영역 내에 위치 하는 손을 인식할 수 있다. 예를 들어, 외부 장치는, 기 정의된 손의 템플릿 이미지와 수신된 이미지를 비교함으로써, 수신된 이미지로부터 손을 검출할 수 있다. 또한, 외부 장치는, 기 정의된 손의 템플릿 이 미지를 이용하여, 수신된 이미지에서 검출된 손이 오른손인지 왼손인지 판단할 수 있다. 외부 장치는, 기 정의된 손의 템플릿 이미지를 이용하여, 수신된 이미지에서 검출된 손의 방향을 판단할 수도 있다. 일 실시예에 의하면, 외부 장치는, 기계 학습 모델(인공 지능 모델이라고 표현될 수도 있음)을 이용하여 수신된 이미지에서 손을 검출할 수 있다. 예를 들어, 외부 장치는, 다양한 손의 이미지를 학습한 DNN(Deep Neural Network) 모델을 이용하여 수신된 이미지에서 손을 인식할 수 있다. 외부 장치는, 기계 학습 모델(예컨대, DNN)에 수신된 이미지를 입력하는 경우, 기계 학습 모델은 입력된 이미지에서 손의 형상을 검출할 수 있다. 일 실시예에 의하면, 외부 장치는, 수신된 이미지로부터 손을 검출하고, 검출된 손에 연결된 적어도 하나 의 인체 부위를 수신된 이미지로부터 더 검출할 수 있다. 손에 연결된 적어도 하나의 인체 부위는, 팔목, 하박, 팔꿈치, 상박 중 적어도 하나를 포함할 수 있으나, 이에 한정되는 것은 아니다. 이때, 외부 장치는 수신 된 이미지로부터 검출된 손 및 손에 연결된 적어도 하나의 인체 부위의 위치 관계를 이용하여, 검출된 손에 연 결된 어깨의 위치를 최종적으로 추정할 수 있다. 외부 장치는, 기계 학습 모델을 이용하여, 수신된 이미 지에서 검출된 손에 연결된 어깨의 위치를 추정할 수 있다. 예를 들어, 외부 장치는, Generative DNN 모 델 또는 Discriminative DNN 모델을 이용하여 손에 연결된 어깨의 위치를 추정할 수 있다. 한편, 일 실시예에 의하면, 수신된 이미지에 어깨가 포함되어 있는 경우, 외부 장치는, 수신된 이미지를 분석함으로써, 손에 연결된 어깨의 위치를 바로 추정할 수 있다. 단계 S1830은 도 2의 단계 S220에 대응하므로, 구체적인 설명은 생략하기로 한다. 단계 S1840에서, 외부 장치는, 추정된 위치에서 진정 사용자의 어깨가 존재할 확률 정보를 이용하여, 이 미지에 포함된 손이 진정 사용자의 손인지 판단할 수 있다. 일 실시예에 의하면, 외부 장치는, 진정 사용자의 어깨 존재 확률 정보를 획득할 수 있다. 진정 사용자의 어깨 존재 확률 정보는, 웨어러블 기기(또는 진정 사용자의 머리)를 기준으로, 3차원 공간 좌표 상에서, 진정 사용자의 어깨가 실제로 존재할 확률 값들을 포함할 수 있다. 진정 사용자의 어깨 존재 확률 정보는 확률값들의 세트로 표현될 수도 있고, 확률 분포도로 표현될 수도 있으나, 이에 한정되는 것은 아니다. 한편, 진정 사용자의 어깨 존재 확률 정보는 왼쪽 어깨의 존재 확률 정보 및 오른쪽 어깨의 존재 확률 정보를 포함할 수 있 다. 일 실시예에 의하면, 외부 장치는, 진정 사용자의 어깨 존재 확률 정보와 추정된 어깨의 위치를 비교함으 로써, 추정된 어깨의 위치에서 진정 사용자의 어깨가 존재할 확률 값을 획득할 수 있다. 예를 들어, 수신된 이 미지에서 검출된 손이 오른손인 경우, 외부 장치는, 추정된 위치에서 진정 사용자의 오른쪽 어깨가 존재 할 확률 값을 획득할 수 있다. 추정된 위치에서 진정 사용자의 오른쪽 어깨가 존재할 확률 값이 임계 값(예컨대, 90%)이상인 경우, 외부 장치는, 수신된 이미지에서 검출된 손이 진정 사용자의 손이라고 판단 할 수 있다. 반면에, 추정된 위치에서 진정 사용자의 오른쪽 어깨가 존재할 확률 값이 임계 값(예컨대, 90%)보 다 작은 경우, 외부 장치는, 수신된 이미지에서 검출된 손이 진정 사용자의 손이 아니라고 판단할 수 있 다. 단계 S1850에서, 외부 장치는, 이미지에 포함된 손이 진정 사용자의 손인지 판단한 결과를 웨어러블 기기 로 전송할 수 있다. 일 실시예에 의하면, 외부 장치는 근거리 무선 통신망 또는 이동 통신망을 통해서 이미지에 포함된 손이 진정 사용자의 손인지 판단한 결과를 웨어러블 기기로 전송할 수 있다. 단계 S1860 및 단계 S1870에서, 웨어러블 기기는, 이미지에 포함된 손이 진정 사용자의 손이라고 판단되 는 경우, 감지 영역 내의 손의 제스처를 식별할 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 감지 영역의 이미지에서 검출된 손의 동작과 데이터베이스에 기 정의된 손의 동작을 비교함으로써, 인식된 손의 제스처를 식별할 수 있다. 일 실시예에 의하면, 웨어러블 기 기는 기계 학습 모델(예컨대, DNN)을 이용하여 감지 영역의 이미지에 포함된 손의 제스처를 식별할 수도 있다. 단계 S1880에서, 웨어러블 기기는, 식별된 제스처에 대응하는 명령을 수행할 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 제스처와 명령을 맵핑한 테이블에 기초하여, 제스처에 대응하는 명령을 수행할 수 있다. 예를 들어, 제스처에 대응하는 명령은 특정 컨텐트를 선택하는 명령, 특정 애플리케이 션을 실행하는 명령, 가상의 객체를 조작하는 명령(예컨대, 가상의 객체를 확대 또는 축소시키는 명령, 가상의 개체를 이동시키는 명령, 가상의 객체를 복사하는 명령, 가상의 객체를 삭제하는 명령 등), 웨어러블 기기 의 전원을 제어하는 명령, 외부 장치로 데이터를 전송하거나 외부 장치로부터 데이터를 수신하는 명령 등 을 포함할 수 있으나, 이에 한정되는 것은 아니다. 단계 S1860 및 단계 S1890에서, 웨어러블 기기는, 인식된 손이 진정 사용자의 손이 아니라고 판단되는 경 우, 인식된 손을 무시할 수 있다. 일 실시예에 의하면, 웨어러블 기기는, 인식된 손이 타인의 손이라고 판단되는 경우, 인식된 손의 제스처를 무시할 수 있다. 도 19 및 20은 일 실시예에 따른, 손 또는 팔 정보를 이용하여 아바타 이미지를 제공하는 동작을 설명하기 위한 도면이다. 도 19를 참조하면, 웨어러블 기기는, 웨어러블 기기를 착용한 진정 사용자에 대응하는 상반신의 제 1 아바타 이미지를 타인의 장치(예컨대, 타인의 웨어러블 기기)로 전달할 수 있다. 이때, 웨어러블 기기 는, 제 1 아바타 이미지에 진정 사용자의 손 또는 팔 정보를 적용하여, 사실적인 아바타 이 미지를 생성할 수 있다. 예를 들어, 웨어러블 기기는 감지 영역 내에 위치하는 손의 형태(예컨대, 주먹 쥐고 있는 상태)를 식별하고, 식별된 손의 형태를 이용하여 사실적인 제 2 아바타 이미지를 생성할 수 있다. 그리고 웨어러 블 기기는 사실적인 제 2 아바타 이미지를 타인의 장치(예컨대, 타인의 웨어러블 기기)로 전달할 수 있다. 도 20을 참조하면, 웨어러블 기기는 진정 사용자의 손을 감지하고, 진정 사용자의 손 동작에 대응하는 아 바타 이미지를 생성할 수 있다. 제 1 사용자와 제 2 사용자가 각각 한 손을 흔드는 경우를 예로 들 어 설명하기로 한다. 제 2 사용자가 착용하고 있는 웨어러블 기기의 감지 영역 내에 제 1 사 용자의 제 1 손과 제 2 사용자의 제 2 손이 모두 감지될 수 있다. 이때, 웨어러블 기 기가 제 1 손과 제 2 손 모두의 동작을 이용하여 제 2 사용자에 대응하는 제 1 아바타이미지를 생성하는 경우, 제 1 아바타 이미지는 제 2 사용자가 양손을 흔들고 있는 것처럼 나타날 수 있다. 하지만, 일 실시예에 의하면, 웨어러블 기기는, 제 1 손과 제 2 손 각각에 연결된 어깨의 위 치를 추정하고, 어깨의 추정된 위치를 이용하여, 제 2 손만 제 2 사용자의 손이고, 제 1 손 은 제 2 사용자의 손이 아니라고 판단할 수 있다. 이 경우, 웨어러블 기기는 제 2 손의 동작 을 이용하여, 제 2 사용자에 대응하는 제 2 아바타 이미지를 생성할 수 있다. 이때, 제 2 아바타 이미지는 제 2 사용자처럼 한 손만 흔드는 것으로 나타날 수 있다. 도 21 및 도 22는 일 실시예에 따른, 웨어러블 기기의 구성을 설명하기 위한 블록 구성도이다. 도 21에 도시된 바와 같이, 일 실시예에 따른 웨어러블 기기는, 출력부, 적어도 하나의 센서 , 프로세서 및 메모리를 포함할 수 있다. 그러나 도시된 구성요소 모두가 필수구성요소인 것 은 아니다. 도시된 구성요소보다 많은 구성요소에 의해 웨어러블 기기가 구현될 수도 있고, 그보다 적은 구성요소에 의해서도 웨어러블 기기는 구현될 수 있다. 예를 들어, 도 22에 도시된 바와 같이, 일 실시예 에 따른 웨어러블 기기는, 출력부, 적어도 하나의 센서, 프로세서, 메모리 이외 에 통신부, 사용자 입력부를 더 포함할 수도 있다. 이하 상기 구성요소들에 대해 차례로 살펴본다. 출력부는, 오디오 신호 또는 비디오 신호 또는 진동 신호의 출력을 위한 것으로, 이에는 디스플레이부 와 음향 출력부, 진동 모터 등이 포함될 수 있다. 디스플레이부는 웨어러블 기기에서 처리되는 정보를 표시 출력한다. 예를 들어, 디스플레이부 는, 통화 모드인 경우 통화와 관련된 UI(User Interface) 또는 GUI(Graphic User Interface)를 표시할 수 있고, 입력 모드인 경우, 가상의 입력 인터페이스를 표시할 수 있고, 게임 실행 모드인 경우, 해당 게임과 관련된 컨텐츠를 표시할 수 있다. 일 실시예에 의하면, 디스플레이부는, 투명 디스플레이일 수도 있고, 불투명 디스플레이일 수도 있다. 투 명 디스플레이란 정보를 표시하는 스크린의 뒷면이 비치는 형태의 정보 표시 장치를 말한다. 투명 디스플레이는 투명 소자로 구성되고, 투명 소자에 대한 광 투과율을 조절하여 투명도를 조절하거나 각 픽셀의 RGB값을 조절하 여 투명도를 조절할 수 있다. 한편, 디스플레이부와 터치패드가 레이어 구조를 이루어 터치 스크린으로 구성되는 경우, 디스플레이부 는 출력 장치 이외에 입력 장치로도 사용될 수 있다. 터치 스크린은 사용자의 터치 스크린 상의 터치 제 스처를 감지하고, 터치 제스처에 관한 정보를 제어부로 전달할 수 있다. 사용자의 터치 제스처에는 탭, 터치&홀드, 더블 탭, 드래그, 패닝, 플릭, 드래그 앤드 드롭, 스와이프 등이 있을 수 있다. 디스플레이부는, 액정 디스플레이(liquid crystal display), 박막 트랜지스터 액정 디스플레이(thin film transistor-liquid crystal display), 유기 발광 다이오드(organic light-emitting diode), 플렉시블 디 스플레이(flexible display), 3차원 디스플레이(3D display), 전기영동 디스플레이(electrophoretic display) 중에서 적어도 하나를 포함할 수 있다. 그리고 웨어러블 기기의 구현 형태에 따라 웨어러블 기기는 디스플레이부를 2개 이상 포함할 수도 있다. 음향 출력부는 통신부로부터 수신되거나 메모리에 저장된 오디오 데이터를 출력할 수 있다. 또한, 음향 출력부는 웨어러블 기기에서 수행되는 기능(예를 들어, 진정 사용자의 손 식별, 애플리 케이션 실행, 호신호 수신음, 메시지 수신음 등)과 관련된 음향 신호를 출력한다. 이러한 음향 출력부에 는 스피커(speaker), 버저(Buzzer) 등이 포함될 수 있다. 진동 모터는 진동 신호를 출력할 수 있다. 예를 들어, 진동 모터는 오디오 데이터 또는 비디오 데 이터(예컨대, 호신호 수신음, 메시지 수신음 등)의 출력에 대응하는 진동 신호를 출력할 수 있다. 또한, 진동 모터는, 가상의 입력 인터페이스를 통한 입력이 발생되는 경우, 진동 신호를 출력할 수도 있다. 적어도 하나의 센서는, 웨어러블 기기의 상태 또는 웨어러블 기기 주변의 상태를 감지하고, 감지된 정보를 제어부로 전달할 수 있다. 일 실시예에 의하면, 적어도 하나의 센서는, 지자기 센서, 초음파 센서, IMU 센서, 적 외선 센서, 자이로스코프 센서, 위치 센서, 깊이 센서, 근접 센서, 광 센서, 카메라(이미지 센서), 마이크로폰을 포함할 수 있으나, 이에 한정되는 것은 아니다. 각 센 서들의 기능은 그 명칭으로부터 당업자가 직관적으로 추론할 수 있으므로, 구체적인 설명은 생략하기로 한다. 프로세서는, 통상적으로 웨어러블 기기의 전반적인 동작을 제어한다. 예를 들어, 프로세서는, 메모리에 저장된 프로그램들을 실행함으로써, 출력부, 적어도 하나의 센서 , 통신부, 사용자 입력부, 메모리 등을 전반적으로 제어할 수 있다. 일 실시예에 의하면, 프로세서는 기계 학습 모델을 생성하기 위한 AI 프로세서를 포함할 수 있으나, 이에 한정되는 것은 아니다. 일 실시예에 의하면, AI 프로세서는 프로세서와 별도의 칩으로 구현될 수도 있다. 일 실시예에 의하면, 프로세서는 적어도 하나의 센서를 이용하여, 적어도 하나의 센서의 감 지 영역 내에 위치하는 손을 인식할 수 있다. 예를 들어, 프로세서는, 감지 영역 내에서 손이 감지되는 경우, 감지 영역을 촬영한 이미지를 카메라(이미지 센서)로부터 획득할 수 있다. 일 실시예에 의하면, 프로세서는 감지 영역에서 인식된 손과 인식된 손에 연결된 적어도 하나의 인 체 부위의 위치 관계에 기반하여, 인식된 손에 연결된 어깨의 위치를 추정할 수 있다. 일 실시예에 의하면, 프로세서는 추정된 위치에서 웨어러블 기기를 착용한 진정 사용자의 어깨가 존재할 확률 정보를 이용하여, 인식된 손이 진정 사용자의 손인지 판단할 수 있다. 예를 들어, 프로세서 는 진정 사용자의 어깨 존재 확률 정보를 획득하고, 진정 사용자의 어깨 존재 확률 정보로부터 추정된 위치에서 의 확률 값을 획득할 수 있다. 그리고 프로세서는, 추정된 위치에서의 확률 값이 임계 값보다 큰 경우, 인식된 손이 진정 사용자의 손이라고 판단할 수 있다. 반면, 프로세서는, 추정된 위치에서의 확률 값이 임계 값 이하인 경우, 인식된 손이 타인의 손이라고 판단할 수 있다. 프로세서는, 인식된 손이 진정 사용자의 손이라고 판단되는 경우, 인식된 손의 제스처를 식별할 수 있고, 식별된 제스처에 대응하는 명령을 수행할 수 있다. 반면에, 프로세서는, 인식된 손이 진정 사용자의 손이 아니라고 판단되는 경우, 인식된 손을 무시할 수 있다. 한편, 프로세서는, 인식된 손이 진정 사용자의 손이라고 판단되는 경우, 진정 사용자의 손이 감지 영역 내에 위치하는지 여부를 나타내는 알림 정보를 출력부를 통해 제공할 수 있다. 예를 들어, 프로세 서는, 진정 사용자의 손이 감지 영역 내로 진입하는 경우, 출력부를 통해 제 1 식별 영상을 표시고, 진정 사용자의 손이 감지 영역을 벗어나는 경우, 출력부를 통해 제 2 식별 영상을 표시할 수 있다. 일 실시예에 의하면, 프로세서는, 인식된 손이 진정 사용자의 손이라고 판단되는 경우, 인식된 손의 제 1 특징 정보를 획득할 수 있다. 프로세서는, 손의 제 1 특징 정보에 기초하여, 감지 영역 내에 위치하 는 적어도 하나의 손이 진정 사용자의 손인지 판단할 수 있다. 예를 들어, 프로세서는, 손의 제 1 특징 정보와 감지 영역 내에 위치하는 적어도 하나의 손의 제 2 특징 정보를 비교할 수 있다. 프로세서는, 제 1 특징 정보와 제 2 특징 정보의 유사도가 임계 값 이상인 경우, 적어도 하나의 손을 진 정 사용자의 손으로 판단할 수 있다. 반면에, 제 1 특징 정보와 상기 제 2 특징 정보의 유사도가 임계 값보다 작은 경우, 프로세서는, 적어도 하나의 손이 진정 사용자의 손인지 정확히 판단하기 위해, 적어도 하나의 손에 연결된 적어도 하나의 어깨의 위치를 추정할 수 있다. 이때, 프로세서는, 추정된 위치에서 진정 사 용자의 어깨가 존재할 확률 정보를 이용하여, 적어도 하나의 손이 진정 사용자의 손인지 판단할 수 있다. 일 실시예에 의하면, 프로세서는, 적어도 하나의 센서를 이용하여, 적어도 하나의 센서의 감 지 영역 내에 위치하는 손을 인식하고, 감지 영역의 위치 별로 웨어러블 기기를 착용한 진정 사용자의 손이 감지될 확률에 관한 정보에 기반하여, 인식된 손이 진정 사용자의 손인지 판단할 수 있다. 통신부는, 웨어러블 기기와 외부 장치(예컨대, 서버 장치, 모바일 단말 등) 간의 통신을 하 게 하는 하나 이상의 구성요소를 포함할 수 있다. 예를 들어, 통신부는, 근거리 통신부, 이동 통신 부 및 방송 수신부를 포함할 수 있다. 근거리 통신부(short-range wireless communication unit)는, 블루투스 통신부, BLE(Bluetooth Low Energy) 통신부, 근거리 무선 통신부(Near Field Communication unit), WLAN(와이파이) 통신부, 지그비 (Zigbee) 통신부, 적외선(IrDA, infrared Data Association) 통신부, WFD(Wi-Fi Direct) 통신부, UWB(ultra wideband) 통신부, Ant+ 통신부 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 이동 통신부는, 이동 통신망 상에서 기지국, 외부의 단말, 서버 중 적어도 하나와 무선 신호를 송수신한 다. 여기에서, 무선 신호는, 음성 호 신호, 화상 통화 호 신호 또는 문자/멀티미디어 메시지 송수신에 따른 다 양한 형태의 데이터를 포함할 수 있다. 방송 수신부는, 방송 채널을 통하여 외부로부터 방송 신호 및/또는 방송 관련된 정보를 수신한다. 방송 채널은 위성 채널, 지상파 채널을 포함할 수 있다. 구현 예에 따라서 웨어러블 기기는 방송 수신부(141 3)를 포함하지 않을 수도 있다. 사용자 입력부는, 사용자가 웨어러블 기기를 제어하기 위한 데이터를 입력하는 수단을 의미한다. 예를 들어, 사용자 입력부에는 키 패드(key pad), 돔 스위치 (dome switch), 터치 패드(접촉식 정전 용량 방식, 압력식 저항막 방식, 적외선 감지 방식, 표면 초음파 전도 방식, 적분식 장력 측정 방식, 피에조 효과 방 식 등), 조그 휠, 조그 스위치 등이 있을 수 있으나, 이에 한정되는 것은 아니다. 일 실시예에 의하면, 사용자 입력부는 제스처 검출부를 포함할 수 있다. 제스처 검출부는 사용자의 손 제 스처를 식별하고, 사용자의 손 제스처에 대응하는 명령어를 검출할 수 있다. 또한, 일 실시예에 의하면, 사용자 입력부는 마이크로폰을 포함할 수 있다. 이 경우, 사용자 입력부는 마이크로폰을 통해 사용자의 음성 명령을 수신할 수 있다. 메모리는, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수도 있고, 입/출력되는 데이터들 (예컨대, 카메라 이미지, 애플리케이션, 컨텐츠 등)을 저장할 수도 있고, 하나 이상의 인스트럭션을 저장할 수 도 있다. 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 메모리에 저장된 프로그램들은 그 기능에 따라 복수 개의 모듈들로 분류할 수 있다. 일 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD- ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 일부 실시예는 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행가능한 명령어를 포함하는 기 록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매 체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독가능 매 체는 컴퓨터 저장 매체 및 통신 매체를 모두 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데 이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구현된 휘발 성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈, 또는 반송파와 같은 변조된 데이터 신호의 기타 데이터, 또는 기타 전송 메커니즘 을 포함하며, 임의의 정보 전달 매체를 포함한다. 또한, 일부 실시예는 컴퓨터에 의해 실행되는 컴퓨터 프로 그램과 같은 컴퓨터에 의해 실행가능한 명령어를 포함하는 컴퓨터 프로그램 또는 컴퓨터 프로그램 제품 (computer program product)으로도 구현될 수 있다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17 도면18 도면19 도면20 도면21 도면22"}
{"patent_id": "10-2018-0169909", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른, 웨어러블 기기를 착용한 진정 사용자의 손을 식별하는 시스템을 설명하기 위한 도면 이다. 도 2는 일 실시예에 따른, 웨어러블 디바이스의 진정 사용자의 손을 식별하는 방법을 설명하기 위한 순서도이다. 도 3은 일 실시예예 따른, 손 및 손에 연결된 적어도 하나의 인체 부위를 검출하는 방법을 설명하기 위한 순서 도이다. 도 4는 일 실시예에 따른, 이미지로부터 검출된 결과를 설명하기 위한 도면이다. 도 5는 일 실시예에 따른, 어깨 위치를 추정하기 위한 기계 학습 모델을 설명하기 위한 도면이다. 도 6은 일 실시예에 따른, 진정 사용자의 어깨 존재 확률 정보를 설명하기 위한 도면이다. 도 7은 일 실시예에 따른, 웨어러블 기기가 진정 사용자의 손인지 판단하는 동작을 설명하기 위한 도면이다. 도 8은 일 실시예에 따른, 진정 사용자의 손의 제스처를 식별하는 방법을 설명하기 위한 순서도이다. 도 9는 일 실시예에 따른, 웨어러블 기기가 타인의 손을 무시하는 동작을 설명하기 위한 도면이다. 도 10은 일 실시예에 따른, 웨어러블 기기가 타인의 손을 검출하는 동작을 설명하기 위한 도면이다. 도 11은 일 실시예에 따른, 진정 사용자의 손이 감지 영역 내에 위치하는지 여부를 나타내는 알림 정보를 제공 하는 방법을 설명하기 위한 순서도이다. 도 12는 일 실시예에 따른, 알림 정보를 설명하기 위한 도면이다. 도 13은 일 실시예에 따른, 감지 영역의 위치 별로 진정 사용자의 손이 감지될 확률에 관한 정보에 기반하여, 진정 사용자의 손을 식별하는 방법을 설명하기 위한 순서도이다. 도 14는 일 실시예에 따른, 감지 영역의 위치 별로 진정 사용자의 손이 감지될 확률에 관한 정보를 설명하기 위 한 도면이다. 도 15는 일 실시예에 따른, 웨어러블 기기가 진정 사용자의 손과 타인의 손을 구별하는 동작을 설명하기 위한 도면이다. 도 16은 일 실시예에 따른, 인식된 손의 특징 정보에 기초하여, 진정 사용자의 손을 식별하는 방법을 설명하기 위한 순서도이다. 도 17는 일 실시예에 따른, 웨어러블 기기가 인식된 손의 특징 정보에 기초하여, 진정 사용자의 손을 식별하는 동작을 설명하기 위한 도면이다. 도 18은 일 실시예에 따른, 웨어러블 기기가 외부 장치와 연동하여, 진정 사용자의 손을 식별하는 방법을 설명 하기 위한 순서도이다. 도 19 및 20은 일 실시예에 따른, 손 또는 팔 정보를 이용하여 아바타를 제공하는 동작을 설명하기 위한 도면이 다. 도 21 및 도 22는 일 실시예에 따른, 웨어러블 기기의 구성을 설명하기 위한 블록 구성도이다."}
