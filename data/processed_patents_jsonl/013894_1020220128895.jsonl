{"patent_id": "10-2022-0128895", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0156635", "출원번호": "10-2022-0128895", "발명의 명칭": "과제 특화 어댑터를 학습하는 방법 및 장치", "출원인": "주식회사 자이냅스", "발명자": "주동원"}}
{"patent_id": "10-2022-0128895", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "기 학습된 트랜스포머(Transformer) 구조를 가지는 제1 문장 인코더 모델에 과제 특화 어댑터를 부가함으로써제2 문장 인코더 모델을 생성하는 단계;기 설정된 학습 데이터베이스로부터 획득되는 입력 문장을 상기 제2 문장 인코더 모델에 입력하고, 상기 입력문장에 대응하는 정답 레이블을 획득하는 단계; 상기 입력 문장에 기초하여 상기 제2 문장 인코더 모델의 상위 레이어에서 하위 레이어 방향으로 순전파를 진행하고, 예측 레이블을 획득하는 단계;제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손실함수 및 제2 가중치에 기초한 제2 함수를 합산하여어댑터 학습 손실함수를 설정하는 단계; 및상기 순전파로 획득한 상기 예측 레이블, 상기 정답 레이블 및 상기 어댑터 학습 손실함수를 바탕으로 역전파를진행하여 상기 어댑터의 파라미터들을 교정하는 단계;를 포함하되, 상기 제1 손실함수는 교차 엔트로피 오차에 기초하여 설정된 것이고,상기 제2 손실함수는 메트릭 학습 기법에 기반하여 설정된 것인, 과제 특화 어댑터를 학습하는 방법."}
{"patent_id": "10-2022-0128895", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 제1 문장 인코더 모델은, 사전학습 언어모델을 대조 학습 기법에 기반하여 조정된 문장 인코더 모델을 교사 모델로 하여, 지식 증류 기법을 기반으로 학습된 학생 모델인, 방법."}
{"patent_id": "10-2022-0128895", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 지식 증류 기법은, 코사인 유사도를 이용하는 대조 학습 기법에 기반하여 설정된 제3 손실함수 및 릴레이션 헤드(Relation head)로 나눈 모델의 쿼리(Query), 키(Key), 벨류(Value) 벡터의 어텐션 분포에 기초하여 설정된 제4 손실함수 중 적어도 어느 하나를 이용하는 기법인, 방법."}
{"patent_id": "10-2022-0128895", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서, 상기 어댑터의 파라미터들을 교정하는 단계는,상기 교정된 파라미터들에 기초하여, 상기 순전파 및 상기 역전파를 수행하는 단계;를 더 포함하는, 방법."}
{"patent_id": "10-2022-0128895", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서,상기 어댑터의 파라미터들을 교정하는 단계는,상기 예측 레이블, 상기 정답 레이블 및 상기 어댑터 학습 손실함수를 바탕으로 역전파를 진행하여 상기 어댑터학습 손실함수의 상기 제1 가중치 및 상기 제2 가중치를 교정하는 단계;를 더 포함하는, 방법.공개특허 10-2023-0156635-3-청구항 6 적어도 하나의 프로그램이 저장된 메모리; 및상기 적어도 하나의 프로그램을 실행함으로써 동작하는 프로세서;를 포함하고,상기 프로세서는, 기 학습된 트랜스포머(Transformer) 구조를 가지는 제1 문장 인코더 모델에 과제 특화 어댑터를 부가함으로써 제2 문장 인코더 모델을 생성하고,기 설정된 학습 데이터베이스로부터 획득되는 입력 문장을 상기 제2 문장 인코더 모델에 입력하고, 상기 입력문장에 대응하는 정답 레이블을 획득하고,상기 입력 문장에 기초하여 상기 제2 문장 인코더 모델의 상위 레이어에서 하위 레이어 방향으로 순전파를 진행하고, 예측 레이블을 획득하고,제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손실함수 및 제2 가중치에 기초한 제2 함수를 합산하여어댑터 학습 손실함수를 설정하고,상기 순전파로 획득한 상기 예측 레이블, 상기 정답 레이블 및 상기 어댑터 학습 손실함수를 바탕으로 역전파를진행하여 상기 어댑터의 파라미터들을 교정하되,상기 제1 손실함수는 교차 엔트로피 오차에 기초하여 설정된 것이고,상기 제2 손실함수는 메트릭 학습 기법에 기반하여 설정된 것인, 과제 특화 어댑터를 학습하는 장치."}
{"patent_id": "10-2022-0128895", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "기 학습된 트랜스포머(Transformer) 구조를 가지는 제1 문장 인코더 모델에 과제 특화 어댑터를 부가함으로써 제 2 문장 인코더 모델을 생성하는 단계; 기 설정된 학습 데이터베이스로부터 획득되는 입력 문장을 상기 제2 문장 인코더 모델에 입력하고, 상기 입력 문장에 대응하는 정답 레이블을 획득하는 단계; 상기 입력 문장에 기초하여 (뒷면에 계속)"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 과제 특화 어댑터를 학습하는 방법 및 장치에 관한 것이다. 보다 자세하게는, 기 학습된 문장 인코더 모델에 어댑터를 부가하고, 서로 다른 손실함수 및 가중치를 이용하여 어댑터를 학습하는 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 딥러닝을 활용한 자연어 처리 분야에서 대규모 사전학습 언어모델 (Large-scale pretrained language model, LPLM)이 발전함에 따라 이를 미세 조정(Fine-tuning)한 의도 분류 모델의 성능도 개선되었다. 의도 분류(Intent classification)는 목적 지향 대화 시스템(Task-oriented dialog system)의 주요 단계 중 하 나로, 사용자 발화의 의도를 예측하는 작업이다. 많은 연구들이 대규모 사전학습 언어모델을 미세 조정하여 의도 분류 모델을 개발하려 하였으나, 실시간 응답을 요하는 대화 시스템에서 대규모 모델을 미세 조정하는 방법은 많은 운영 비용을 필요로 한다는 문제점이 존재하 였다. 이에, 사용자가 원하는 대화 흐름으로 이어지도록 정확하면서도, 저성능 자원에서 다량의 에이전트를 운영할 수 있는 의도 분류 모델 경량화 방법이 필요한 실정이다."}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 과제 특화 어댑터를 학습하는 방법 및 장치를 제공한다. 본 개시가 해결하고자 하는 과제는 이상에서 언급한 과제에 한정되지 않으며, 언급되지 않은 본 개시의 다른 과제 및 장점들은 하기의 설명에 의해서 이해될 수 있고, 본 개시의 실시 예에 의해보다 분명하게 이해될 것이다. 또한, 본 개시가 해결하고자 하는 과제 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 알 수 있을 것이다."}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본 개시의 제1 측면은, 기 학습된 트랜스포머 (Transformer) 구조를 가지는 제1 문장 인코더 모델에 과제 특화 어댑터를 부가함으로써 제2 문장 인코더 모델 을 생성하는 단계; 기 설정된 학습 데이터베이스로부터 획득되는 입력 문장을 상기 제2 문장 인코더 모델에 입 력하고, 상기 입력 문장에 대응하는 정답 레이블을 획득하는 단계; 상기 입력 문장에 기초하여 상기 제2 문장 인코더 모델의 상위 레이어에서 하위 레이어 방향으로 순전파를 진행하여, 예측 레이블을 획득하는 단계; 제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손실함수 및 제2 가중치에 기초한 제2 함수를 합산하여 어댑 터 학습 손실함수를 설정하는 단계; 및 상기 순전파로 획득한 상기 예측 레이블, 상기 정답 레이블 및 상기 어 댑터 학습 손실함수를 바탕으로 역전파를 진행하여 상기 어댑터의 파라미터들을 교정하는 단계;를 포함하되, 상 기 제1 손실함수는 교차 엔트로피 오차에 기초하여 설정된 것이고, 상기 제2 손실함수는 메트릭 학습 기법에 기 반하여 설정된 것인, 과제 특화 어댑터를 학습하는 방법을 제공할 수 있다. 본 개시의 제2 측면은, 적어도 하나의 프로그램이 저장된 메모리; 및 상기 적어도 하나의 프로그램을 실행함으 로써 동작하는 프로세서;를 포함하고, 상기 프로세서는, 기 학습된 트랜스포머(Transformer) 구조를 가지는 제1 문장 인코더 모델에 과제 특화 어댑터를 부가함으로써 제2 문장 인코더 모델을 생성하고, 기 설정된 학습 데이 터베이스로부터 획득되는 입력 문장을 상기 제2 문장 인코더 모델에 입력하고, 상기 입력 문장에 대응하는 정답 레이블을 획득하고, 상기 입력 문장에 기초하여 상기 제2 문장 인코더 모델의 상위 레이어에서 하위 레이어 방 향으로 순전파를 진행하여, 예측 레이블을 획득하고, 제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손 실함수 및 제2 가중치에 기초한 제2 함수를 합산하여 어댑터 학습 손실함수를 설정하고, 상기 순전파로 획득한 상기 예측 레이블, 상기 정답 레이블 및 상기 어댑터 학습 손실함수를 바탕으로 역전파를 진행하여 상기 어댑터 의 파라미터들을 교정하되, 상기 제1 손실함수는 교차 엔트로피 오차에 기초하여 설정된 것이고, 상기 제2 손실 함수는 메트릭 학습 기법에 기반하여 설정된 것인, 과제 특화 어댑터를 학습하는 장치를 제공할 수 있다. 본 개시의 제3 측면은, 본 개시의 제1 측면의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체를 제공할 수 있다. 전술한 것 외의 다른 측면, 특징, 이점이 이하의 도면, 특허청구범위 및 발명의 상세한 설명으로부터 명확해질 것이다."}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본 개시의 과제 해결 수단에 의하면, 목적 지향 시스템에서 다량의 에이전트를 운영하는 의도 분류 모델 이 필요로 하는 자원 규모를 축소시켜, 대규모 모델을 사용할 경우 발생하는 GPU 등의 고성능 자원을 필요로 한 다는 문제점을 해결할 수 있다. 또한, 본 개시의 다른 과제 해결 수단에 의하면, 미세 조정을 수행하는 경우 발생하는 모델의 모든 파라미터를 업데이트함으로써 운영하는 에이전트 수가 증가할수록 필요 자원이 비례하여 증가한다는 문제점을 해결할 수 있 다. 또한, 본 개시의 다른 과제 해결 수단에 의하면, 필요 자원 규모를 축소함으로써 문장의 의도를 예측하는데 필 요한 응답 속도를 높일 수 있다. 또한, 본 개시의 다른 과제 해결 수단에 의하면, 필요 자원 규모를 축소함에도 불구하고 높은 정확도를 얻을 수 있다."}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시는 다양하게 변환하여 실시할 수 있고, 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 기재한다. 본 개시의 효과 및 특징, 그리고 그것들을 달성하는 방법은 도면과 함께 상 세하게 후술되어 있는 실시예들을 참조해 명확해질 것이다. 그러나 본 개시는 이하에서 개시되는 실시예들에 한정되는 것이 아니라 다양한 형태로 구현될 수 있다. 이하의 실시예에서, 제1, 제2 등의 용어는 한정적인 의미가 아니라 하나의 구성요소를 다른 구성요소와 구별하는 목적 으로 사용되었다. 예컨대, 본 개시의 권리 범위를 벗어나지 않으면서 먼저 서술한 제1 구성요소는 그 뒤에 제2 구성요소로 서술되는 경우가 있을 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 서술될 수 있다. 또한, 단 수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 또한, 포함하다 또는 가지다 등 의 용어는 명세서상에 기재된 특징 또는 구성요소가 존재함을 의미하는 것이고, 하나 이상의 다른 특징 또는 구 성요소가 부가될 가능성을 미리 배제하는 것은 아니다. 또한, 도면에서는 설명의 편의를 위하여 구성 요소들이 그 크기가 과장 또는 축소될 수 있다. 예컨대, 도면에서 나타난 각 구성의 크기 및 두께는 편의를 위하여 임의로 나타낸 것으로, 본 개시는 반드시 도시된 바에 한정되 지 않는다. 이하, 첨부된 도면을 참조하여 본 개시의 실시예들을 상세히 설명하기로 하며, 도면을 참조하여 설명할 때, 동 일하거나 대응하는 구성 요소는 동일한 도면 부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 본 개시는 자연어 처리분야에 이용될 수 있으며, 자연어 처리는 인간의 언어 현상을 기계를 이용하여 묘사할 수 있도록 구현하는 것을 목적으로 한다. 본 개시에 따르면, 목적 지향 대화 시스템(Task-oriented dialog system)에서, 저성능 자원에서도 다량의 에이 전트를 운영할 수 있도록 경량화된 의도 분류 모델을 제공할 수 있다. 목적 지향 대화 시스템은 주어진 도메인에서 사용자가 원하는 업무를 수행하는 것을 돕는 시스템이며, 주로 의 도 분류, 엔티티 추출, 대화 관리, 응답 생성 등의 모듈로 구성된다. 의도 분류(Intent classification)는 목적 지향 대화 시스템의 주요 단계로서, 사용자 발화의 의도를 예측하는 작업이다. 의도 분류 문제에 사용되는 대규모 사전학습 언어모델은 BERT, RoBERTa, GPT-3 등 트랜스포머(Transformer) 기 반의 모델을, 대규모 코퍼스, 오픈 도메인 대화 데이터 및/또는 자연어 추론 데이터 등을 이용해 학습한 모델일 수 있다. 본 개시의 일 실시예에 서술되는 사전학습 언어모델은 상기 모델일 수 있다. 의도 분류 문제는 입력 문장의 길이가 짧아 데이터가 적다는 점, 분류하여야 할 클래스의 수가 많다는 점, 클래 스당 학습할 수 있는 데이터를 늘리는 것은 많은 비용을 초래한다는 점 때문에, 잘 학습된 사전학습 모델을 사 용하는 것이 바람직하며, 의도 분류 문제에 사전학습 언어 모델을 미세조정(Fine-tuning)하여 이용할 수 있다. 다만, 미세조정은 모델의 모든 파라미터를 업데이트 하여야 하므로, 미세조정을 수행하는 과정에서 많은 자원이 필요하게 되고, 과제가 증가할수록 필요 자원이 비례하여 증가한다는 문제점이 있다. 미세조정된 모델을 기초로 하여, 본 개시에 따른 문장 인코더의 경량화 또는 본 개시에 따른 과제 특화 어댑터의 학습을 이용하여, 상기 문제점을 극복할 수 있다. 본 개시의 일 실시예에 따르면, 경량화된 문장 인코더를 학습하는 단계는 과제 독립적(Task-agnostic)인 단계일 수 있고, 경량화된 문장 인코더에 어댑터를 부가하여 의도 분류 모델을 학습하는 단계는 과제 특화적(Task- specific)인 단계일 수 있다. 이하 첨부된 도면을 참고하여 본 개시를 상세히 설명하기로 한다. 도 1은 일 실시예에 따른 의도 분류 장치를 이용하여 서비스를 제공하는 방법을 설명하기 위한 도면이다. 도 1을 참조하면, 목적 지향 대화 시스템에서 본 개시를 이용하는 의도 분류 장치가 제공할 수 있는 서비스의 일 실시예가 개시된다. 일 실시예에 따르면, 사용자가 사용자의 입력 문장을 입력하고, 의도 분류 장치의 일부 구성인 의도 분류 모델이 입력 문장을 처리하여, 사용자에게 사용자의 목적 내지 의도에 부합하는 응답을 제공할 수 있다. 일 실시예에 따르면, 사용자의 입력 문장은 사용자 단말을 이용하여 입력될 수 있고, 그 과정에서 네트워 크를 이용할 수 있으며, 이는 하기의 도 2에 관한 설명에서 보다 상세히 설명될 것이다. 일 실시예에 따르면, 의도 분류 모델은 CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), LSTM(Long Short-Term Memory) 및 트랜스포머(Transformer) 알고리즘 중 적어도 하나를 이용한 인공 지능 신경망 모델일 수 있으나, 이에 한정되지 않는다. 일 실시예에 따르면, 의도 분류 모델은 입력 문장을 전처리하여 대응되는 벡터를 생성하고, 생성된 벡터를 의도 분류 모델의 파라미터들로 연산하며, 적어도 하나의 은닉 레이어를 거쳐 응답을 생성하 여 제공할 수 있다. 한편, 일 실시예에 따라 의도 분류 모델을 학습하는 과정도 상술한 과정과 유사하게 수행될 수 있다. 예를 들어, 의도 분류 모델을 학습하는 경우, 학습을 위한 입력 문장은 데이터베이스로부터 제공되고, 사용자에게 응답을 제공하는 단계 대신, 연산된 예측 값과 정답 값을 비교하여 의도 분류 모델의 파라미터 를 교정하는 단계가 더 수행될 수 있다. 도 2는 일 실시예에 따른 의도 분류 장치를 포함하는 시스템도이다. 일 실시예에 따른 시스템은 사용자 단말, 의도 분류 장치를 포함할 수 있다. 사용자 단말 은, 스마트폰, 태블릿 PC, PC, 스마트 TV, 휴대폰, PDA(personal digital assistant), 랩톱, 미디어 플레이어, 마이크로 서버, 전자책 단말기, 디지털방송용 단말기, 네비게이션, 키오스크, MP3 플레이어, 디지털 카메라, 가 전기기, 카메라가 탑재된 디바이스 및 기타 모바일 또는 비모바일 컴퓨팅 장치일 수 있다. 또한, 사용자 단말 은 통신 기능 및 데이터 프로세싱 기능을 구비한 시계, 안경, 헤어 밴드 및 반지 등의 웨어러블 디바이스 일 수 있다. 하지만 사용자 단말는 이에 한정되지 않는다. 의도 분류 장치는, 문장 인코더 경량화에 이용되는 손실을 계산하는 장치, 문장 인코더를 경량화하는 장치, 경량화된 문장 인코더를 이용하여 입력 문장의 의도를 파악하는 장치, 과제 특화 어댑터 학습 과정에 이 용되는 손실을 계산하는 장치, 과제 특화 어댑터를 학습하는 장치 및/또는 과제 특화 어댑터를 이용하여 입력 문장의 의도를 파악하는 장치 중 하나에 포함될 수 있다. 의도 분류 장치는, 의도 분류를 이용하는 목적 지향 대화 서비스를 제공할 수 있는 웹 및/또는 앱을 관리 하는 모든 종류의 서버를 의미할 수 있으며 각종 관리 플랫폼의 홈페이지 등을 관리하는 서버를 의미할 수 있다. 그러나, 이에 제한되지 않는다. 학습 데이터베이스 저장 매체(미도시)는 의도 분류 장치의 일부로서 포함되거나, 의도 분류 장치의 외부에서 의도 분류 장치와 네트워크를 이용하여 통신을 수행할 수 있 다. 학습 데이터베이스 저장 매체(미도시)는 의도 분류 장치가 네트워크를 이용하여 접근 가능한 클라우드 서버일 수 있다. 학습 데이터베이스 저장 매체(미도시)는 DRAM(dynamic random access memory), SRAM(static random access memory) 등과 같은 RAM(random access memory), ROM(read-only memory), EEPROM(electrically erasable programmable read-only memory), CD-ROM, 블루레이 또는 다른 광학 디스크 스토리지, HDD(hard disk drive), SSD(solid state drive), 또는 플래시 메모리를 포함할 수 있다. 학습 데이터베이스 저장 매체(미도시)에 저장된 데이터베이스는 인공지능 신경망 모델을 학습시키는 학습데이터 를 포함할 수 있다. 데이터베이스에 포함되는 학습데이터는 같은 의미를 공유하는 두개의 문장들로 구성된 데이"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "터를 포함할 수 있으며, 두개의 문장은 질문과 문단, 본문과 요약, 이전 문장과 다음 문장에 해당할 수 있으나, 이에 제한되지 않는다. 질문과 문단으로 구성된 데이터는, 어떤 주제의 내용을 담고 있는 문단, 해당 문단에 기초하여 할 수 있는 질문 및 그에 대한 답변으로 이루어진 데이터셋에 있어서, 질문과 그 질문이 기초하는 문단으로 구성된 데이터를 의 미할 수 있다."}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "본문과 요약으로 구성된 데이터는, 본문과 그 본문을 요약하는 요약문으로 구성된 데이터를 의미할 수 있다. 이전 문장과 다음 문장으로 구성된 데이터는, 사람간 대화 상황에서 주고받는 문장들에 있어서, 제1 인물의 발 화 문장과 이에 이어지는 제2 인물의 발화 문장으로 구성된 데이터를 의미할 수 있다. 학습 데이터베이스 저장 매체(미도시)는, 코퍼스(Corpus) 데이터베이스를 저장하는 매체를 포함할 수 있으며, 코퍼스 데이터베이스는 개인, 회사 및 국가가 수집한 데이터를 포함할 수 있다. 데이터를 수집하는 회사는 검색 엔진을 이용하여 서비스를 제공하는 회사를 포함할 수 있으나, 이에 제한되지 않는다. 한편, 의도 분류 장치는 네트워크를 통해 사용자 단말과 통신을 수행할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 사용자 단말과 네트워크를 통한 통신을 수행함으로써 경량화된 문장 인코 더 모델 또는 과제 특화 어댑터가 부가된 문장 인코더 모델에 입력하는 입력 문장을 획득할 수 있다. 또한, 일 실시예에 따르면, 의도 분류 장치는 학습 데이터베이스 저장 매체(미도시)와 네트워크를 통한 통신을 수행 함으로써 트랜스포머(Transformer) 구조를 가지는 교사 모델, 학생 모델 및 과제 특화 어댑터가 부가된 문장 인 코더 모델에 입력하는 입력 문장을 획득할 수 있다. 사용자 단말 및 의도 분류 장치는 네트워크를 이용하여 통신을 수행할 수 있다. 예를 들어, 네트워크 는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN), 부가가치 통신망(Value Added Network; VAN), 이동 통신망(mobile radio communication network), 위성 통신망 및 이들의 상호 조합을 포함할 수 있다. 네트워크는 도 2에 도시된 각 구성들(210, 220)이 서로 원활하게 통신을 할 수 있도록 하는 포 괄적인 의미의 데이터 통신망이며, 유선 인터넷, 무선 인터넷 및 모바일 무선 통신망을 포함할 수 있다. 또한, 무선 통신은 예를 들어, 무선 랜(Wi-Fi), 블루투스, 블루투스 저 에너지(Bluetooth low energy), 지그비, WFD(Wi-Fi Direct), UWB(ultra-wideband), 적외선 통신(IrDA, infrared Data Association), NFC(Near Field Communication) 등이 있을 수 있으나, 이에 한정되는 것은 아니다. 도 3은 일 실시예에 따른 의도 분류 장치의 블록도이다. 도 3을 참조하면, 의도 분류 장치는 프로세서, 메모리를 포함할 수 있으며, 통신 모듈을 더 포함할 수 있다. 도 3의 의도 분류 장치에는 실시예와 관련된 구성요소들 만이 도시되어 있으므로, 도"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "3에 도시된 구성요소들 외에 다른 범용적인 구성요소들이 더 포함될 수 있음을 당해 기술분야의 통상의 기술자 라면 이해할 수 있다. 또한, 도 3의 의도 분류 장치는 도 2의 의도 분류 장치와 대응될 수 있다. 프로세서는 의도 분류 장치의 전반적인 동작을 제어한다. 예를 들어, 프로세서는 메모리에 저장된 프로그램들을 실행함으로써, 입력부(미도시), 디스플레이(미도시), 메모리, 통신 모듈 등을 전반적으로 제어할 수 있고, 의도 분류 장치의 동작을 제어할 수 있다. 프로세서는 도 1 내지 도 16 에서 설명되는 의도 분류 장치의 동작 중 적어도 일부를 제어할 수 있다. 프로세서는 ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 제어기(controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서 (microprocessors), 기타 기능 수행을 위한 전기적 유닛 중 적어도 하나를 이용하여 구현될 수 있다. 메모리는 의도 분류 장치 내에서 처리되는 각종 데이터들을 저장하는 하드웨어로서, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수 있다. 메모리는 DRAM(dynamic random access memory), SRAM(static random access memory) 등과 같은 RAM(random access memory), ROM(read-only memory), EEPROM(electrically erasable programmable read-only memory), CD-ROM, 블루레이 또는 다른 광학 디스크 스토리지, HDD(hard disk drive), SSD(solid state drive), 또는 플래시 메모리를 포함할 수 있다. 통신 모듈은 외부 서버 또는 외부 장치와 유선/무선 통신을 하게 하는 하나 이상의 구성 요소를 포함할 수 있다. 예를 들어, 통신 모듈은, 근거리 통신부(미도시), 이동 통신부(미도시) 및 방송 수신부(미도시) 중 적어도 하나를 포함할 수 있다. 또한, 본 개시에 따른 실시 예는 컴퓨터 상에서 다양한 구성요소를 통하여 실행될 수 있는 컴퓨터 프로그램의 형태로 구현될 수 있으며, 이와 같은 컴퓨터 프로그램은 컴퓨터로 판독 가능한 매체에 기록될 수 있다. 이때, 매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플 롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등과 같은, 프로그램 명령어를 저장하고 실행하도록 특별히 구성된 하드웨어 장치를 포함할 수 있다. 한편, 상기 컴퓨터 프로그램은 본 개시를 위하여 특별히 설계되고 구성된 것이거나 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수 있다. 컴퓨터 프로그램의 예에는, 컴파일러에 의하여 만들어지는 것 과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용하여 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함될 수 있다. 일 실시예에 따르면, 본 개시의 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 또는 두 개의 사용자 장치들 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 이하, 도 4 내지 도 16을 참조하여, 의도 분류 장치가 동작하는 구체적인 예들을 설명한다. 도 4는 일 실시예에 따른 문장 인코더 경량화에 이용되는 손실을 계산하는 방법의 흐름도이다. 도 4를 참조하면, 단계 410에서, 의도 분류 장치는 기 설정된 학습 데이터베이스로부터 획득되는 입력 문 장을, 트랜스포머(Transformer) 구조를 가지는 교사 모델과 학생 모델에 입력할 수 있다. 지식 증류 기법은 인공지능 모델을 경량화하는 기법 중 하나로서, 모델의 크기를 줄이되 중요한 부분을 남기는 기법이며, 크기가 큰 모델의 지식을 상대적으로 크기가 작은 모델에 전달하는 방식이다. 이때, 크기가 큰 모델 을 교사 모델로, 크기가 작은 모델을 학생 모델로 지칭한다. 일 실시예에 따르면, 교사 모델은 사전학습 언어모델이 대조 학습(Contrastive learning) 기법에 기반하여 조정 된 문장 인코더 모델일 수 있다. 예를 들어, 교사 모델은 대규모 사전학습 언어모델을 대조 학습 기법에 기반하 여 미세조정(Fine-tuning)한 문장 인코더 모델일 수 있다. 한편, 학생 모델은 사전학습이 되지 않은 문장 인코더 모델일 수 있다. 학생 모델은 교사 모델(또는 교사 모델 의 앙상블)을 모방하도록 학습될 크기가 작은 모델일 수 있다. 대조 학습 기법은, 샘플간 유사성과 비유사성을 학습하는 기법으로, 레이블이 지정되지 않은 데이터 요소가 서 로 병치되며 어떤 점이 유사한지 학습한다. 미세조정은, 기 학습된 인공지능 모델을 기반으로 모델의 아키텍쳐를 새로운 목적에 맞게 변형하고, 이미 학습 된 모델의 파라미터를 미세하게 조정하여 학습시키는 방법이다. 기 학습된 모델은 사전 학습 과정에서 획득한 지식을, 미세조정 과정에서 활용한다. 한편, 문장 인코더 모델은 입력되는 문장을 정해진 크기의 벡터로 변환하는 모델로서, 임베딩 공간 상에서 유사 한 의미의 문장을 가깝게, 다른 의미의 문장을 멀리 배치하는 것을 목표로 한다. 일 실시예에 따르면, 의도 분류 장치가 교사 모델로 이용하는 문장 인코더 모델은, 사전학습 언어모델이 하기의 수학식 1을 이용하여 설정된 손실함수에 기초하여 대조 학습 기법으로 조정된 모델일 수 있다.수학식 1"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수학식 1의"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "은 기계 학습에서의 표현(representation)쌍일 수 있고, 질문과 문단, 본문과 요약, 이전 문장과 다음 문장 등의 문장쌍에 대응하는 표현쌍일 수 있다. 한편, 은 하기의 수학식 2에 의해 정 의될 수 있다. 수학식 2"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "수학식 2의 은 에 대한 코사인 유사도를 나타낼 수 있다. 는 온도 하이퍼파라미터 (temperature hyperparameter)로서, 미리 설정된 파라미터일 수 있다. 일 실시예에 따르면, 기 설정된 학습 데이터베이스는 교사 모델의 학습 데이터베이스에 포함될 수 있다. 일 실시예에 따르면, 사전학습 모델인 교사 모델의 학습 데이터베이스와, 교사 모델을 지식 증류 기법으로 경량 화하는 과정에서 사용되는 학습 데이터베이스는 동일할 수 있다. 일 실시예에 따르면, 학습 데이터베이스에 포함되는 학습데이터는 같은 의미를 공유하는 두개의 문장들로 구성"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "된 데이터를 포함할 수 있으며, 두개의 문장은 질문과 문단, 본문과 요약, 이전 문장과 다음 문장 등에 해당할 수 있다. 학습 데이터베이스는, 코퍼스(Corpus) 데이터베이스를 포함할 수 있으며, 코퍼스 데이터베이스는 개인, 회사 및 국가가 수집한 데이터를 포함할 수 있다. 데이터를 수집하는 회사는 검색 엔진을 이용하여 서비 스를 제공하는 회사를 포함할 수 있다. 단계 420에서, 의도 분류 장치는 입력 문장에 기초하여 교사 모델과 학생 모델의 최종 레이어들을 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장을 전처리하여 임베딩 벡터를 생성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 임베딩 벡터를 학생 모델의 파라미터들로 연산하여 학생 모델의 최 종 레이어를 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 임베딩 벡터를 교사 모델의 파라미터들로 연산하여 교사 모델의 최 종 레이어를 획득할 수 있다. 이상의 단계 420에 관한 실시예는 하기의 도 5에 관한 설명에서 보다 상세히 설명될 것이다. 단계 430에서, 의도 분류 장치는 서로 다른 제1 손실함수 및 제2 손실함수를 설정할 수 있다. 손실함수는 인공지능 신경망 모델을 통해 추론된 예측 값이 타겟 값(정답 값)과 얼마나 멀고 가까운지를 비교하 는 함수로서, 기계 학습의 방향은 손실함수로 계산된 손실 값을 줄이는 방향으로 설정된다. 일 실시예에 따르면, 제1 손실함수는, 코사인 유사도를 이용하는 대조 학습 기법에 기반하여 설정될 수 있다. 예를 들어, 제1 손실함수는 하기의 수학식 3에 의해 정의될 수 있다. 하기의 수학식 3을 이용하여 설정되는 손 실함수는 문장 인코더를 경량화하기 위해 대조 학습 기법을 지식 증류에 응용한 손실함수일 수 있다.수학식 3"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "수학식 3의 은 입력 문장에 대한 교사 모델의 표현, 은 입력 문장에 대한 학생 모델의 표현일 수 있다. 는 학습 가능한 가중치 행렬로, 학생 모델과 교사 모델의 은닉 차원(Hidden dimension)을 일치시키기 위한 것으로서, 미리 설정된 행렬일 수 있으나, 이에 한정되지 않는다. 일 실시예에 따르면, 제2 손실함수는, 릴레이션 헤드(Relation head)로 나눈 학생 모델 및 교사 모델의 쿼리 (Query), 키(Key), 벨류(Value) 벡터의 어텐션 분포에 기초하여 설정될 수 있다. 예를 들어, 제2 손실함수는 하기의 수학식 4에 의해 정의될 수 있다. 하기의 수학식 4를 이용하여 설정되는 손 실함수는 교사 모델의 크기와 무관하게 학생 모델의 크기를 정할 수 있는 트랜스포머 지식 증류 기법에 이용되 는 손실함수일 수 있으며, 이러한 기법은 릴레이션 헤드로 나눈 쿼리, 키, 벨류 벡터의 어텐션 분포를 산출한 후, 학생 모델과 교사 모델의 어텐션 분포 차이를 줄이는 방향으로 지식 증류를 진행하는 기법일 수 있다. 한편, 멀티헤드 셀프-어텐션(multihead self-attention)은 트랜스포머 모델에서 사용될 수 있다. 멀티헤드 셀프 -어텐션은 특징벡터를 모델의 헤드 수로 나눈 뒤, 독립적으로 셀프-어텐션을 수행하고, 다시 셀프-어텐션의 결 과를 합치는 방식으로 어텐션 분포를 획득하는 것을 의미한다. 트랜스포머 구조를 가지는 교사 모델 및 학생 모델에서, 멀티헤드 셀프-어텐션을 수행하는 경우, 교사 모델과 학생 모델의 헤드 수가 달라지면 어텐션 분포의 개수가 달라져 손실함수 계산이 불가능하다는 문제가 발생한다. 릴레이션 헤드는, 상기 문제점을 해결하기 위해 이용되는 것으로서, 교사 모델 및 학생 모델의 동일한 헤드 수 에 해당하는 것으로 설정될 수 있다. 예를 들어, 릴레이션 헤드가 8로 정해지는 경우, 교사 모델 및 학생 모델 은 모두 8개의 헤드로 멀티헤드 셀프-어텐션을 수행하고, 각각 8개의 어텐션 분포가 생성되므로, 어텐션 분포를 이용하는 손실함수(예를 들어, 제2 손실함수)의 계산이 가능할 수 있다. 수학식 4"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "수학식 4의 는 및 의 쿨백-라이블러 발산(Kullback-Leibler divergence)으로, 두 확률분포 의 차이를 계산하는데 사용되는 함수이다. 및 는 소프트맥스(softmax) 함수로 얻은 교사 모델 및 학생 모델의 쿼리에 대한 셀프-어텐션 분포일 수 있다. 소프트맥스 함수는 셀프-어텐션 분포를 정규화하기 위하여 사 용될 수 있다. 한편, 및 은 하기의 수학식 5에 의해 정의될 수 있다. 수학식 5"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "수학식 5의 는 교사 모델의 쿼리 벡터일 수 있다. 는 의 행과 열을 바꾼 전치(Transpose) 벡터일 수 있다. 은 교사 모델의 릴레이션 헤드 차원 수일 수 있다. 예를 들어, 멀티헤드 셀프-어텐션 수행 과정에서, 교사 모델이 768차원 벡터를 8개의 릴레이션 헤드로 나눈다면, 는 96이 될 수 있다. 한편, 상기 수학식 5를 이용하여 학생 모델의 셀프-어텐션 분포를 추출하기 위해, 교사 모델의 쿼리 벡터 대신 학생 모델의 쿼리 벡터를 대입하고, 교사 모델의 릴레이션 헤드 차원 수 대신 학생 모델의 릴레이션 차원 수를 대입할 수 있다. 상기 수학식 4는 셀프-어텐션 분포를 이용하는 손실함수 중에서도, 모델의 쿼리에 의해 추출된 셀프-어텐션 분 포를 이용하여 손실함수를 정의하는 것일 수 있다. 상기 수학식 4에 쿼리에 대한 셀프-어텐션 분포 대신, 키 또 는 벨류에 대한 셀프-어텐션 분포를 대입하고, 상기 수학식 5에 쿼리 벡터 대신 키 또는 벨류 벡터를 대입함으 로써, 모델의 키 또는 벨류에 의해 추출된 셀프-어텐션 분포를 이용하여 손실함수를 정의할 수 있다. 단계 440에서, 의도 분류 장치는 제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손실함수 및 제2 가중치에 기초한 제2 함수를 합산하여 경량화 손실함수를 설정할 수 있다. 예를 들어, 의도 분류 장치는 하기의 수학식 6에 따라 경량화 손실함수를 설정할 수 있다. 구체적으로, 수 학식 3을 이용하여 설정되는 손실함수를 제1 손실함수로, 수학식 4를 이용하여 설정되는 손실함수를 제2 손실함 수로 하고, 각각에 제1 가중치 및 제2 가중치를 곱하여 그 합을 경량화 손실함수로 설정할 수 있다. 수학식 6"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "단계 450에서, 의도 분류 장치는 최종 레이어들 및 경량화 손실함수에 기초하여 손실을 계산할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장에 대응하는 교사 모델의 최종 레이어로 타겟 값을 결정 하고, 학생 모델의 최종 레이어로 예측 값을 결정함으로써, 제1 손실함수로 상기 값들을 계산한 제1 손실 및 제 2 손실함수로 상기 값들을 계산한 제2 손실에 각각의 가중치를 연산한 값을 합하여 경량화 손실을 계산할 수 있 다. 도 5는 도 4의 단계 420의 일 예를 설명하기 위한 흐름도이다. 도 5를 참조하면, 단계 510에서, 의도 분류 장치는 입력 문장을 전처리하여 임베딩 벡터를 생성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장에서 불필요한 부분을 제거하고, 불용어를 제거하고, 대 소문자를 통일하고, 표제어를 추출하고, 어간을 추출하는 등으로 입력 문장을 전처리할 수 있고, 전처리된 입력 문장을 토큰화 할 수 있다. 단계 520에서, 의도 분류 장치는 임베딩 벡터를 학생 모델의 파라미터들로 연산하여 학생 모델의 최종 레 이어를 획득할 수 있다. 단계 530에서, 의도 분류 장치는 임베딩 벡터를 교사 모델의 파라미터들로 연산하여 교사 모델의 최종 레 이어를 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 인공지능 신경망 모델인 학생 모델 및 교사 모델의 입력 레이어에 임베딩 벡터를 입력하고, 입력 레이어의 파라미터들로 임베딩 벡터를 연산하여 다음 레이어의 노드를 거치며, 이와 유사하게 모든 은닉 레이어의 파라미터들로 연산하는 과정을 거친 후, 최종 레이어를 출력할 수 있다. 도 6은 일 실시예에 따른 문장 인코더를 경량화하는 방법의 흐름도이다. 도 6을 참조하면, 단계 610에서, 의도 분류 장치는 기 설정된 학습 데이터베이스로부터 획득되는 입력 문 장을, 트랜스포머(Transformer) 구조를 가지는 교사 모델과 학생 모델에 입력할 수 있다. 여기에서, 단계 610의 구체적인 내용은 도 4에 도시된 단계 410과 동일하다. 따라서, 이하에서는 단계 610에 대한 구체적인 설명은 생 략한다. 단계 620에서, 의도 분류 장치는 입력 문장에 기초하여 교사 모델의 최종 레이어를 획득할 수 있다. 여기 에서, 단계 620의 구체적인 내용은 도 4에 도시된 단계 420과 동일하다. 따라서, 이하에서는 단계 620에 대한 구체적인 설명은 생략한다. 단계 630에서, 의도 분류 장치는 입력 문장에 기초하여 학생 모델의 상위 레이어에서 하위 레이어 방향으 로 순전파를 진행할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 학생 모델의 입력 레이어에 입력 문장에 대응되는 임베딩 벡터를 입력하고, 입력 레이어의 파라미터들로 임베딩 벡터를 연산하여 다음 레이어의 노드를 거치며, 이와 유사하게 모든 은닉 레이어의 파라미터들로 연산하는 과정을 거친 후, 최종 레이어를 출력할 수 있다. 단계 640에서, 의도 분류 장치는 제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손실함수 및 제2 가중치에 기초한 제2 함수를 합산하여 경량화 손실함수를 설정할 수 있다. 일 실시예에 따르면, 제1 손실함수는, 코사인 유사도를 이용하는 대조 학습 기법에 기반하여 설정될 수 있다. 여기에서, 단계 640의 제1 손실함수는 도 4에 도시된 단계 430의 제1 손실함수와 동일하다. 따라서, 이하에서는 단계 640의 제1 손실함수에 대한 구체적인 설명은 생략한다. 일 실시예에 따르면, 제2 손실함수는, 릴레이션 헤드(Relation head)로 나눈 학생 모델 및 교사 모델의 쿼리 (Query), 키(Key), 벨류(Value) 벡터의 어텐션 분포에 기초하여 설정될 수 있다. 여기에서, 단계 640의 제2 손 실함수는 도 4에 도시된 단계 430의 제2 손실함수와 동일하다. 따라서, 이하에서는 단계 640의 제2 손실함수에 대한 구체적인 설명은 생략한다. 단계 650에서, 의도 분류 장치는 순전파로 획득한 학생 모델의 최종 레이어, 교사 모델의 최종 레이어 및 경량화 손실함수를 바탕으로 역전파를 진행하여 학생 모델의 파라미터들을 교정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 역전파를 진행하여 학생 모델과 교사 모델의 최종 레이어를 경량화 손실함수로 계산한 손실 값을 줄이는 방향으로 학생 모델의 파라미터들을 교정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 교정된 파라미터들에 기초하여, 순전파 및 역전파를 수행할 수 있 다. 일 실시예에 따르면, 의도 분류 장치는 기 설정된 횟수만큼 순전파 및 역전파를 반복할 수 있으며, 또는 계산된 손실 값이 기 설정된 오차범위보다 작아질 때까지 순전파 및 역전파를 반복할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 순전파로 획득한 학생 모델의 최종 레이어, 교사 모델의 최종 레이 어 및 경량화 손실함수를 바탕으로 역전파를 진행하여 제1 가중치 및 제2 가중치를 교정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 경량화 손실함수를 구성하는 제1 손실함수 및 제2 손실함수에 연산 되는 제1 가중치 및 제2 가중치를 손실 값을 크게 하는 방향으로 교정함으로써, 손실 값에 대한 제1 손실함수 및 제2 손실함수의 기여도를 조정할 수 있다. 예를 들어, 첫번째 순전파에서, 제1 손실함수로 계산된 제1 손실 값이 제2 손실함수로 계산된 제2 손실 값보다 큰 경우, 의도 분류 장치는 제1 가중치의 값을 기 설정된 비율만큼 크게 교정할 수 있고, 제2 가중치의 값 을 교정된 제1 가중치의 값에 대응되도록 작게 교정할 수 있다. 교정 후, 제1 가중치가 교정 전 제1 가중치보다 커지는 것으로 인하여, 두번째 순전파에 의한 손실 계산에서 제1 손실함수의 기여도가 더 커지게 되고, 이후 역 전파 과정에서는 학습되는 모델이 제1 손실함수를 더 크게 고려하여 모델의 파라미터를 교정하는 효과를 얻게 된다. 따라서, 제1 손실함수 및 제2 손실함수의 기여도를 조정함으로써 학습되는 모델은 경량화 손실함수로서 제1 손 실함수만을 이용하는 경우 또는 제2 손실함수만을 이용하는 경우 기대할 수 있는 학습효과보다 뛰어난 효과를 기대할 수 있다. 또한, 경량화 손실함수로서 고정된 가중치들을 제1 손실함수와 제2 손실함수에 곱해 이용하는 경우에 기대할 수 있는 학습효과보다 뛰어난 학습효과를 기대할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 파라미터들이 교정된 학생 모델에 과제 특화 어댑터를 부가할 수 있다. 예를 들어, 과제 특화 어댑터는 적어도 하나의 파라미터를 가질 수 있다. 과제 특화 어댑터는 미세조정에서 발생하는 필요 자원 증가의 문제점을 해결하기 위하여 인공지능 신경망 모델 에 부가될 수 있다. 과제 특화 어댑터는, 인공지능 신경망 모델의 파라미터를 고정시킨 다음, 인공지능 모델의 레이어 사이에 과제의 종류에 따라 어댑터를 부가하고, 해당 과제에 관련된 기계 학습 과정에서 부가된 어댑터 만의 파라미터를 교정함으로써, 인공지능 신경망 모델이 해당 과제에 특화된 지식을 학습할 수 있도록 한다. 도 7은 일 실시예에 따른 경량화된 문장 인코더를 이용하여 입력 문장의 의도를 파악하는 방법의 흐름도이다. 도 7을 참조하면, 단계 710에서, 의도 분류 장치는 제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손실함수 및 제2 가중치에 기초한 제2 함수를 합산하여 경량화 손실함수를 설정할 수 있다. 여기에서, 단계 710 의 구체적인 내용은 도 6에 도시된 단계 640과 동일하다. 따라서, 이하에서는 단계 710에 대한 구체적인 설명은 생략한다. 단계 720에서, 의도 분류 장치는 기 학습된 트랜스포머(Transformer) 구조를 가지는 문장 인코더 모델 및 경량화 손실함수를 이용하여, 지식 증류 기법에 기반한 학습으로 문장 인코더 모델을 경량화 할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 경량화된 문장 인코더 모델에 과제 특화 어댑터를 부가할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 교차 엔트로피 오차에 기초하여 설정된 제3 손실함수 및 메트릭 학 습 기법에 기반하여 설정된 제4 손실함수 중 적어도 하나에 기초하여 과제 특화 어댑터를 학습할 수 있다. 이상의 단계 720에 관한 실시예는 하기의 도 8에 관한 설명에서 보다 상세히 설명될 것이다. 일 실시예에 따르면, 기 학습된 문장 인코더 모델은, 사전학습 언어모델이 대조 학습 기법에 기반하여 조정된 문장 인코더 모델일 수 있다. 여기에서, 단계 720의 기 학습된 문장 인코더 모델은 도 4에 도시된 단계 410의 사전학습 언어모델이 대조 학습 기법에 기반하여 조정된 문장 인코더 모델과 동일하다. 따라서, 이하에서는 단 계 720의 기 학습된 문장 인코더 모델에 대한 구체적인 설명은 생략한다. 일 실시예에 따르면, 의도 분류 장치는 기 학습된 문장 인코더 모델 및 경량화 손실함수에 기초하여, 제1 가중치 및 제2 가중치를 교정할 수 있다. 여기에서, 본 실시예의 내용은 도 6에 도시된 단계 650의 일 실시예와 동일하다. 따라서, 이하에서는 본 실시예에 대한 구체적인 설명은 생략한다. 단계 730에서, 의도 분류 장치는 사용자로부터 획득된 입력 문장을 경량화된 문장 인코더 모델에 입력할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 사용자가 사용자 단말에 입력한 입력 문장을 네트워크를 이용한 통 신으로 획득할 수 있다. 단계 740에서, 의도 분류 장치는 입력 문장 및 경량화된 문장 인코더 모델에 기초하여 기 설정된 의도 집 합에 포함되는 하나의 의도를 결정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장 및 경량화된 문장 인코더 모델에 기초하여, 임베딩 벡터 를 생성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 임베딩 벡터에 기초하여, 기 설정된 의도 집합 중 하나의 의도를 선택할 수 있다. 이상의 단계 740에 관한 실시예는 하기의 도 9에 관한 설명에서 보다 상세히 설명될 것이다. 도 8은 도 7의 단계 720의 일 예를 설명하기 위한 흐름도이다. 도 8을 참조하면, 단계 810에서, 의도 분류 장치는 경량화된 문장 인코더 모델에 과제 특화 어댑터를 부가 할 수 있다. 과제 특화 어댑터는 미세조정에서 발생하는 필요 자원 증가의 문제점을 해결하기 위하여 인공지능 신경망 모델 에 부가될 수 있다. 과제 특화 어댑터는, 인공지능 신경망 모델의 파라미터를 고정시킨 다음, 인공지능 모델의 레이어 사이에 과제의 종류에 따라 어댑터를 부가하고, 해당 과제에 관련된 기계 학습 과정에서 부가된 어댑터 만의 파라미터를 교정함으로써, 인공지능 신경망 모델이 해당 과제에 특화된 지식을 학습할 수 있도록 한다. 단계 820에서, 의도 분류 장치는 교차 엔트로피 오차에 기초하여 설정된 제3 손실함수 및 메트릭 학습 기 법에 기반하여 설정된 제4 손실함수 중 적어도 하나에 기초하여 과제 특화 어댑터를 학습할 수 있다. 여기에서, 단계 820의 구체적인 내용은 도 12에 도시된 과제 특화 어댑터를 학습하는 방법과 동일하다. 따라서, 이하에서 는 단계 820에 대한 구체적인 설명은 생략한다. 도 9는 도 7의 단계 740의 일 예를 설명하기 위한 흐름도이다. 도 9를 참조하면, 단계 910에서, 의도 분류 장치는 입력 문장 및 경량화된 문장 인코더 모델에 기초하여, 임베딩 벡터를 생성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 인공지능 신경망 모델인 경량화된 문장 인코더 모델의 입력 레이어 에 임베딩 벡터를 입력하고, 입력 레이어의 파라미터들로 임베딩 벡터를 연산하여 다음 레이어의 노드를 거치며, 이와 유사하게 모든 은닉 레이어의 파라미터들로 연산하는 과정을 거친 후, 출력 값을 출력할 수 있다. 단계 920에서, 의도 분류 장치는 임베딩 벡터에 기초하여, 기 설정된 의도 집합 중 하나의 의도를 선택할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 기 설정된 의도 집합에서 어떤 의도를 선택할지 여부를 인공지능 신경망 모델인 경량화된 문장 인코더 모델의 출력 값으로부터 결정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 선택된 의도에 따라 출력 문장을 생성하는 단계를 더 수행할 수 있 다. 일 실시예에 따르면, 의도 분류 장치는 선택된 의도에 따라 네트워크를 이용하여 외부와 통신하는 단계 및 선택된 의도에 적합한 메시지, 요청 등을 외부에 송신하는 단계를 더 수행할 수 있다. 도 10은 일 실시예에 따른 과제 특화 어댑터 학습 과정에 이용되는 손실을 계산하는 방법의 흐름도이다. 도 10을 참조하면, 단계 1010에서, 의도 분류 장치는 기 학습된 트랜스포머(Transformer) 구조를 가지는 제1 문장 인코더 모델에 과제 특화 어댑터를 부가함으로써 제2 문장 인코더 모델을 생성할 수 있다. 과제 특화 어댑터는 미세조정에서 발생하는 필요 자원 증가의 문제점을 해결하기 위하여 인공지능 신경망 모델 에 부가될 수 있다. 과제 특화 어댑터는, 인공지능 신경망 모델의 파라미터를 고정시킨 다음, 인공지능 모델의 레이어 사이에 과제의 종류에 따라 어댑터를 부가하고, 해당 과제에 관련된 기계 학습 과정에서 부가된 어댑터 만의 파라미터를 교정함으로써, 인공지능 신경망 모델이 해당 과제에 특화된 지식을 학습할 수 있도록 한다. 일 실시예에 따르면, 제1 문장 인코더 모델은, 사전학습 언어모델이 대조 학습 기법에 기반하여 조정된 문장 인 코더 모델을 교사 모델로 하여, 지식 증류 기법을 기반으로 학습된 학생 모델일 수 있다. 여기에서, 단계 1010 의 제1 문장 인코더 모델은 도 4에 도시된 단계 410의 사전학습 언어모델이 대조 학습 기법에 기반하여 조정된 문장 인코더 모델과 동일하다. 따라서, 이하에서는 단계 1010의 제1 문장 인코더 모델에 대한 구체적인 설명은 생략한다. 일 실시예에 따르면, 지식 증류 기법은, 코사인 유사도를 이용하는 대조 학습 기법에 기반하여 설정된 제3 손실 함수 및 릴레이션 헤드(Relation head)로 나눈 모델의 쿼리(Query), 키(Key), 벨류(Value) 벡터의 어텐션 분포 에 기초하여 설정된 제4 손실함수 중 적어도 하나를 이용하는 기법일 수 있다. 단계 1020에서, 의도 분류 장치는 기 설정된 학습 데이터베이스로부터 획득되는 입력 문장을 제2 문장 인 코더 모델에 입력할 수 있다. 일 실시예에 따르면, 기 설정된 학습 데이터베이스에 포함되는 학습데이터는 같은 의미를 공유하는 두개의 문장"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "들로 구성된 데이터를 포함할 수 있으며, 두개의 문장은 질문과 문단, 본문과 요약, 이전 문장과 다음 문장 등 에 해당할 수 있다. 기 설정된 학습 데이터베이스는, 코퍼스(Corpus) 데이터베이스를 포함할 수 있으며, 코퍼스 데이터베이스는 개인, 회사 및 국가가 수집한 데이터를 포함할 수 있다. 데이터를 수집하는 회사는 검색 엔진을 이용하여 서비스를 제공하는 회사를 포함할 수 있다. 단계 1030에서, 의도 분류 장치는 입력 문장 및 제2 문장 인코더 모델에 기초하여 정답 레이블 및 예측 레 이블을 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장을 전처리하여 임베딩 벡터를 생성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 임베딩 벡터를 제2 문장 인코더 모델의 파라미터들로 연산하여 예 측 레이블을 획득할 수 있다. 이상의 단계 1030에 관한 실시예는 하기의 도 11에 관한 설명에서 보다 상세히 설명될 것이다. 단계 1040에서, 의도 분류 장치는 서로 다른 제1 손실함수 및 제2 손실함수를 설정할 수 있다. 일 실시예에 따르면, 제1 손실함수는, 교차 엔트로피 오차에 기초하여 설정될 수 있다. 일 실시예에 따르면, 제1 손실함수는 하기의 수학식 7에 의해 정의될 수 있다. 하기의 수학식 7을 이용하여 설 정되는 손실함수는 딥러닝 분류 문제에서 이용될 수 있는 교차 엔트로피 손실함수일 수 있다.수학식 7"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "수학식 7의 는 원-핫 인코딩(One-Hot Encoding)으로 표현된 입력 문장에 대한 정답 레이블일 수 있다. 는 입력 문장에 대한 제2 문장 인코더 모델의 출력일 수 있으며, 예측 레이블일 수 있다. 일 실시예에 따르면, 제2 손실함수는, 메트릭 학습(Metric learning) 기법에 기반하여 설정될 수 있다. 일 실시예에 따르면, 제2 손실함수는 하기의 수학식 8에 의해 정의될 수 있다. 하기의 수학식 8을 이용하여 설 정되는 손실함수는 적은 학습데이터에 의해 발생하는 과적합을 방지하는 목적으로 이용되는 메트릭 학습 기법에 이용되는 손실함수일 수 있으며, 그 중에서도 학습 정도에 따라 유연하게 오차가 적용될 수 있는 Circle 손실함 수일 수 있다. 메트릭 학습 기법은 분류가 아닌, 샘플의 같고 다름을 나타내는 거리를 계산하는 메트릭을 이용 하는 학습 기법이다. 수학식 8"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "수학식 8의 는 한 클래스 내부에서 두 샘플의 유사도 스코어의 개수, 은 상이한 클래스간 유사도 스코어의 개 수일 수 있다. 는 스케일을 조정하는 스케일 팩터(scale factor)일 수 있다. 은 상이한 클래스간 유사도 스 코어, 는 한 클래스 내부에서 두 샘플의 유사도 스코어 일 수 있다. 은 유사도 분리 정도를 최적화하기 위 하여 설정되는 오차범위일 수 있다. 단계 1050에서, 의도 분류 장치는 제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손실함수 및 제2 가중치에 기초한 제2 함수를 합산하여 어댑터 학습 손실함수를 설정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 하기의 수학식 9에 따라 어댑터 학습 손실함수를 설정할 수 있다. 구체적으로, 수학식 7을 이용하여 설정되는 손실함수를 제1 손실함수로, 수학식 8을 이용하여 설정되는 손실함 수를 제2 손실함수로 하고, 각각에 제1 가중치 및 제2 가중치를 곱하여 그 합을 어댑터 학습 손실함수로 설정할 수 있다. 수학식 9"}
{"patent_id": "10-2022-0128895", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "단계 1060에서, 의도 분류 장치는 정답 레이블, 예측 레이블 및 어댑터 학습 손실함수에 기초하여 손실을 계산할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장에 대응하는 정답 레이블로 타겟 값을 결정하고, 제2 문 장 인코더 모델의 예측 레이블로 예측 값을 결정함으로써, 제1 손실함수로 상기 값들을 계산한 제1 손실 및 제2 손실함수로 상기 값들을 계산한 제2 손실에 각각의 가중치를 연산한 값을 합하여 어댑터 학습 손실을 계산할 수 있다. 도 11은 일 실시예에 따른 도 10의 단계 1030의 일 예를 설명하기 위한 흐름도이다. 도 11을 참조하면, 단계 1110에서, 의도 분류 장치는 입력 문장을 전처리 하여 임베딩 벡터를 생성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장에서 불필요한 부분을 제거하고, 불용어를 제거하고, 대 소문자를 통일하고, 표제어를 추출하고, 어간을 추출하는 등으로 입력 문장을 전처리할 수 있고, 전처리된 입력 문장을 토큰화 할 수 있다. 단계 1120에서, 의도 분류 장치는 임베딩 벡터를 제2 문장 인코더 모델의 파라미터들로 연산하여 예측 레 이블을 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 인공지능 신경망 모델인 제2 문장 인코더 모델에 임베딩 벡터를 입 력하고, 입력 레이어의 파라미터들로 임베딩 벡터를 연산하여 다음 레이어의 노드를 거치며, 이와 유사하게 모 든 은닉 레이어의 파라미터들로 연산하는 과정을 거친 후, 예측 레이블을 출력할 수 있다. 도 12는 일 실시예에 따른 과제 특화 어댑터를 학습하는 방법의 흐름도이다. 도 12를 참조하면, 단계 1210에서, 의도 분류 장치는 기 학습된 트랜스포머(Transformer) 구조를 가지는 제1 문장 인코더 모델에 과제 특화 어댑터를 부가함으로써 제2 문장 인코더 모델을 생성할 수 있다. 여기에서, 단계 1210의 구체적인 내용은 도 10에 도시된 단계 1010과 동일하다. 따라서, 이하에서는 단계 1210에 대한 구 체적인 설명은 생략한다. 단계 1220에서, 의도 분류 장치는 기 설정된 학습 데이터베이스로부터 획득되는 입력 문장을 제2 문장 인 코더 모델에 입력하고, 입력 문장에 대응하는 정답 레이블을 획득할 수 있다. 단계 1230에서, 의도 분류 장치 는 입력 문장에 기초하여 제2 문장 인코더 모델의 상위 레이어에서 하위 레이어 방향으로 순전파를 진행하 고, 예측 레이블을 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 제2 문장 인코더의 입력 레이어에 입력 문장에 대응되는 임베딩 벡 터를 입력하고, 입력 레이어의 파라미터들로 임베딩 벡터를 연산하여 다음 레이어의 노드를 거치며, 이와 유사 하게 모든 은닉 레이어의 파라미터들로 연산하는 과정을 거친 후, 예측 레이블을 출력할 수 있다. 단계 1240에서, 의도 분류 장치는 제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손실함수 및 제2 가중치에 기초한 제2 함수를 합산하여 어댑터 학습 손실함수를 설정할 수 있다. 여기에서, 단계 1240의 구체적 인 내용은 도 10에 도시된 단계 1050과 동일하다. 따라서, 이하에서는 단계 1240에 대한 구체적인 설명은 생략 한다. 단계 1250에서, 의도 분류 장치는 순전파로 획득한 예측 레이블, 정답 레이블 및 어댑터 학습 손실함수를 바탕으로 역전파를 진행하여 어댑터의 파라미터들을 교정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 역전파를 진행하여 예측 레이블과 정답 레이블로 계산한 손실 값을 줄이는 방향으로 과제 특화 어댑터의 파라미터들을 교정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 교정된 파라미터들에 기초하여, 순전파 및 역전파를 수행할 수 있 다. 일 실시예에 따르면, 의도 분류 장치는 기 설정된 횟수만큼 순전파 및 역전파를 반복할 수 있으며, 또는 계산된 손실 값이 기 설정된 오차범위보다 작아질 때까지 순전파 및 역전파를 반복할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 예측 레이블, 정답 레이블 및 어댑터 학습 손실함수를 바탕으로 역 전파를 진행하여 어댑터 학습 손실함수의 제1 가중치 및 제2 가중치를 교정할 수 있다. 여기에서, 본 실시예는 도 6에 도시된 단계 650의 일 실시예와 동일하다. 따라서, 이하에서는 본 실시예에 대한 구체적인 설명은 생략 한다. 도 13은 일 실시예에 따른 과제 특화 어댑터를 이용하여 입력 문장의 의도를 파악하는 방법의 흐름도이다. 도 13을 참조하면, 단계 1310에서, 의도 분류 장치는 기 학습된 트랜스포머(Transformer) 구조를 가지는 제1 문장 인코더 모델에 과제 특화 어댑터를 부가함으로써 제2 문장 인코더 모델을 생성할 수 있다. 여기에서, 단계 1310의 구체적인 내용은 도 10에 도시된 단계 1010과 동일하다. 따라서, 이하에서는 단계 1310에 대한 구 체적인 설명은 생략한다. 단계 1320에서, 의도 분류 장치는 제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손실함수 및 제2 가중치에 기초한 제2 함수를 합산하여 어댑터 학습 손실함수를 설정할 수 있다. 여기에서, 단계 1320의 구체적인 내용은 도 10에 도시된 단계 1050과 동일하다. 따라서, 이하에서는 단계 1320에 대한 구체적인 설명은 생략 한다. 단계 1330에서, 의도 분류 장치는 기 설정된 의도 분류 학습 데이터베이스 및 어댑터 학습 손실함수에 기 초하여, 과제 특화 어댑터를 학습할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 기 설정된 의도 분류 학습 데이터베이스 및 어댑터 학습 손실함수에 기초하여, 제1 가중치 및 제2 가중치를 교정할 수 있다. 여기에 서, 본 실시예는 도 6에 도시된 단계 650의 일 실시예와 동일하다. 따라서, 이하에서는 본 실시예에 대한 구체 적인 설명은 생략한다. 단계 1340에서, 의도 분류 장치는 사용자로부터 획득된 입력 문장을 제2 문장 인코더 모델에 입력할 수 있 다. 일 실시예에 따르면, 의도 분류 장치는 사용자가 사용자 단말에 입력한 입력 문장을 네트워크를 이용한 통 신으로 획득할 수 있다. 단계 1350에서, 의도 분류 장치는 입력 문장 및 제2 문장 인코더 모델에 기초하여 기 설정된 의도 집합에 포함되는 하나의 의도를 결정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장 및 제2 문장 인코더 모델에 기초하여, 임베딩 벡터를 생 성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 임베딩 벡터에 기초하여, 기 설정된 의도 집합 중 하나의 의도를 설정할 수 있다. 이상의 단계 1350에 관한 실시예는 하기의 도 14에 관한 설명에서 보다 상세히 설명될 것이다. 도 14는 일 실시예에 따른 도 13의 단계 1350의 일 예를 설명하기 위한 흐름도이다. 도 14를 참조하면, 단계 1410에서, 의도 분류 장치는 입력 문장 및 제2 문장 인코더 모델에 기초하여, 임 베딩 벡터를 생성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장에서 불필요한 부분을 제거하고, 불용어를 제거하고, 대 소문자를 통일하고, 표제어를 추출하고, 어간을 추출하는 등으로 입력 문장을 전처리할 수 있고, 전처리된 입력 문장을 토큰화 할 수 있다. 단계 1420에서, 의도 분류 장치는 임베딩 벡터에 기초하여, 기 설정된 의도 집합 중 하나의 의도를 설정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 인공지능 신경망 모델인 제2 문장 인코더 모델에 임베딩 벡터를 입 력하고, 입력 레이어의 파라미터들로 임베딩 벡터를 연산하여 다음 레이어의 노드를 거치며, 이와 유사하게 모 든 은닉 레이어의 파라미터들로 연산하는 과정을 거친 후, 출력 값을 출력할 수 있다. 도 15는 일 실시예에 따른 대규모 문장 인코더를 경량화하고 과제 특화 어댑터를 학습하여 사용자 발화 의도를 예측하는 방법의 순서도이다. 도 15를 참조하면, 단계 1510에서, 일 실시예에 따라 의도 분류 장치는 경량화 손실함수를 설정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 서로 다른 제1 손실함수 및 제2 손실함수를 설정할 수 있다. 일 실시예에 따르면, 제1 손실함수는, 코사인 유사도를 이용하는 대조 학습 기법에 기반하여 설정될 수 있다. 일 실시예에 따르면, 제2 손실함수는, 릴레이션 헤드(Relation head)로 나눈 학생 모델 및 교사 모델의 쿼리 (Query), 키(Key), 벨류(Value) 벡터의 어텐션 분포에 기초하여 설정될 수 있다. 일 실시예에 따르면, 의도 분류 장치는 제1 손실함수 및 제1 가중치에 기초한 제1 함수와 제2 손실함수 및 제2 가중치에 기초한 제2 함수를 합산하여 경량화 손실함수를 설정할 수 있다. 단계 1520에서, 일 실시예에 따라 의도 분류 장치는 순전파로 획득한 학생 모델의 최종 레이어, 교사 모델 의 최종 레이어 및 경량화 손실함수를 바탕으로 역전파를 진행하여 학생 모델의 파라미터들을 교정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 기 설정된 학습 데이터베이스로부터 획득되는 입력 문장을, 트랜스 포머(Transformer) 구조를 가지는 교사 모델과 학생 모델에 입력할 수 있다. 일 실시예에 따르면, 교사 모델은 사전학습 언어모델이 대조 학습 기법에 기반하여 조정된 문장 인코더 모델일 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장에 기초하여 교사 모델과 학생 모델의 최종 레이어들을 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장에 기초하여 학생 모델의 상위 레이어에서 하위 레이어 방향으로 순전파를 진행할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장을 전처리하여 임베딩 벡터를 생성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 임베딩 벡터를 학생 모델의 파라미터들로 연산하여 학생 모델의 최 종 레이어를 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 임베딩 벡터를 교사 모델의 파라미터들로 연산하여 교사 모델의 최 종 레이어를 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 최종 레이어들 및 경량화 손실함수에 기초하여 손실을 계산할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 순전파로 획득한 학생 모델의 최종 레이어, 교사 모델의 최종 레이 어 및 경량화 손실함수를 바탕으로 역전파를 진행하여 학생 모델의 파라미터들을 교정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 순전파로 획득한 학생 모델의 최종 레이어, 교사 모델의 최종 레이 어 및 경량화 손실함수를 바탕으로 역전파를 진행하여 제1 가중치 및 제2 가중치를 교정할 수 있다. 단계 1530에서, 일 실시예에 따라 의도 분류 장치는 문장 인코더 경량화가 완료되었는지 여부를 판단할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 교정된 파라미터들에 기초하여, 순전파 및 역전파를 수행할 수 있 다. 단계 1540에서, 일 실시예에 따라 의도 분류 장치는 경량화된 제1 문장 인코더 모델에 과제 특화 어댑터를 부가함으로써 제2 문장 인코더 모델을 생성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 기 학습된 트랜스포머(Transformer) 구조를 가지는 제1 문장 인코 더 모델에 과제 특화 어댑터를 부가함으로써 제2 문장 인코더 모델을 생성할 수 있다. 일 실시예에 따르면, 제1 문장 인코더 모델은, 사전학습 언어모델이 대조 학습 기법에 기반하여 조정된 문장 인 코더 모델을 교사 모델로 하여, 지식 증류 기법을 기반으로 학습된 학생 모델일 수 있다. 일 실시예에 따르면, 지식 증류 기법은, 코사인 유사도를 이용하는 대조 학습 기법에 기반하여 설정된 제1 손실 함수 및 릴레이션 헤드(Relation head)로 나눈 모델의 쿼리(Query), 키(Key), 벨류(Value) 벡터의 어텐션 분포 에 기초하여 설정된 제2 손실함수 중 적어도 하나를 이용하는 기법일 수 있다. 일 실시예에 따르면, 과제 특화 어댑터는 적어도 하나의 파라미터를 가질 수 있다. 단계 1550에서, 일 실시예에 따라 의도 분류 장치는 어댑터 학습 손실함수를 설정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 서로 다른 제3 손실함수 및 제4 손실함수를 설정할 수 있다. 일 실시예에 따르면, 제3 손실함수는, 교차 엔트로피 오차에 기초하여 설정될 수 있다. 일 실시예에 따르면, 제4 손실함수는, 메트릭 학습 기법에 기반하여 설정될 수 있다. 일 실시예에 따르면, 의도 분류 장치는 제3 손실함수 및 제3 가중치에 기초한 제3 함수와 제4 손실함수 및 제4 가중치에 기초한 제4 함수를 합산하여 어댑터 학습 손실함수를 설정할 수 있다. 단계 1560에서, 일 실시예에 따라 의도 분류 장치는 순전파로 획득한 제2 문장 인코더의 예측 레이블, 정 답 레이블 및 어댑터 학습 손실함수를 바탕으로 역전파를 진행하여 어댑터의 파라미터들을 교정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 기 설정된 학습 데이터베이스로부터 획득되는 입력 문장을 제2 문 장 인코더 모델에 입력할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장 및 제2 문장 인코더 모델에 기초하여 정답 레이블 및 예 측 레이블을 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장에 기초하여 제2 문장 인코더 모델의 상위 레이어에서 하 위 레이어 방향으로 순전파를 진행하고, 예측 레이블을 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장을 전처리하여 임베딩 벡터를 생성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 임베딩 벡터를 제2 문장 인코더 모델의 파라미터들로 연산하여 예 측 레이블을 획득할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 정답 레이블, 예측 레이블 및 어댑터 학습 손실함수에 기초하여 손 실을 계산할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 순전파로 획득한 예측 레이블, 정답 레이블 및 어댑터 학습 손실함 수를 바탕으로 역전파를 진행하여 어댑터의 파라미터들을 교정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 예측 레이블, 정답 레이블 및 어댑터 학습 손실함수를 바탕으로 역 전파를 진행하여 어댑터 학습 손실함수의 제3 가중치 및 제4 가중치를 교정할 수 있다. 단계 1570에서, 일 실시예에 따라 의도 분류 장치는 어댑터 학습이 완료되었는지 여부를 판단할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 교정된 파라미터들에 기초하여, 순전파 및 역전파를 수행할 수 있 다. 단계 1580에서, 일 실시예에 따라 의도 분류 장치는 입력 문장과 제2 문장 인코더 모델에 기초하여 입력 문장의 의도를 결정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 사용자로부터 획득된 입력 문장을 제2 문장 인코더 모델에 입력할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장 및 제2 문장 인코더 모델에 기초하여 기 설정된 의도 집 합에 포함되는 하나의 의도를 결정할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 입력 문장 및 제2 문장 인코더 모델에 기초하여, 임베딩 벡터를 생 성할 수 있다. 일 실시예에 따르면, 의도 분류 장치는 임베딩 벡터에 기초하여, 기 설정된 의도 집합 중 하나의 의도를 설정할 수 있다. 도 16은 일 실시예에 따른 대규모 사전학습 언어모델로부터 의도 분류 모델을 생성하는 과정을 설명하기 위한 도면이다. 도 16을 참조하면, 일 실시예에 따라 의도 분류 장치는 대규모 사전학습 언어모델이 대조 학습 기법 으로 미세조정된 대규모 문장 인코더 모델을 지식 증류 기법으로 경량화하고, 어댑터를 부가한 다음, 어 댑터의 파라미터를 조정함으로써 의도 분류 모델을 학습할 수 있다. 일 실시예에 따른 대규모 사전학습 언어모델은 BERT, RoBERTa, GPT-3 등 트랜스포머(Transformer) 기반 의 모델을, 대규모 코퍼스, 오픈 도메인 대화 데이터 및/또는 자연어 추론 데이터 등을 이용해 학습한 모델일 수 있다. 일 실시예에 따르면, 대규모 문장 인코더 모델은 대규모 사전학습 언어모델이 대조 학습으로 미세 조정된 모델일 수 있다. 일 실시예에 따르면, 의도 분류 장치는 도 4에 관한 설명의 수학식 1을 이용하여 설정된 손실함수에 기초하는 대조 학습 기법으로 조정된 사전학습 언어모델을 이용할 수 있다. 일 실시예에 따르면, 대규모 문장 인코더 모델은 입력된 문장을 정해진 크기의 벡터로 변환하는 모델일 수 있다. 일 실시예에 따르면, 경량화된 문장 인코더 모델은 대규모 문장 인코더 모델을, 필요 자원을 축소 시키기 위하여 지식 증류 기법에 기반하여 경량화한 모델일 수 있다.일 실시예에 따르면, 지식 증류 기법에 이용되는 손실함수는 코사인 유사도를 이용하는 대조 학습 기법에 기반 하여 설정된 손실함수와, 릴레이션 헤드(Relation head)로 나눈 상기 학생 모델 및 상기 교사 모델의 쿼리 (Query), 키(Key), 벨류(Value) 벡터의 어텐션 분포에 기초하여 설정된 손실함수 중 적어도 하나에 기초하여 설 정된 손실함수일 수 있다. 일 실시예에 따르면, 코사인 유사도를 이용하는 대조 학습 기법에 기반하여 설정된 손실함수는 도 4에 관한 설명의 수학식 3을 이용하여 설정된 손실함수이고, 릴레이션 헤드(Relation head)로 나 눈 상기 학생 모델 및 상기 교사 모델의 쿼리(Query), 키(Key), 벨류(Value) 벡터의 어텐션 분포에 기초하여 설 정된 손실함수는 도 4에 관한 설명의 수학식 4를 이용하여 설정된 손실함수일 수 있다. 일 실시예에 따르면, 어댑터가 부가된 문장 인코더 모델은 경량화된 문장 인코더 모델에 어댑터를 부가한 모델일 수 있다. 일 실시예에 따르면, 어댑터는 과제에 특화되도록 학습되는 과제 특화 어댑터일 수 있으며, 이를 부가함으로써 다량의 에이전트를 효율적으로 운영할 수 있다. 일 실시예에 따르면, 어댑터가 학습된 의도 분류 모델은 어댑터가 부가된 문장 인코더 모델에서 어 댑터의 파라미터를 교정함으로써 학습된 인공지능 신경망 모델일 수 있다. 일 실시예에 따르면, 어댑터 학습에 이용되는 손실함수는 교차 엔트로피 오차에 기초하여 설정된 손실함수와, 메트릭 학습 기법에 기반하여 설정된 손실함수 중 적어도 하나에 기초하여 설정된 손실함수일 수 있다. 일 실시 예에 따르면, 교차 엔트로피 오차에 기초하여 설정된 손실함수는 도 10에 관한 설명의 수학식 7을 이용하여 설 정된 손실함수이고, 메트릭 학습 기법에 기반하여 설정된 손실함수는 도 10에 관한 설명의 수학식 8를 이용하여 설정된 손실함수일 수 있다. 한편, 본 개시에 따른 방법을 구성하는 단계들에 대하여 명백하게 순서를 기재하거나 반하는 기재가 없다면, 상 기 단계들은 적당한 순서로 행해질 수 있다. 반드시 상기 단계들의 기재 순서에 따라 본 개시가 한정되는 것은 아니다. 본 개시에서 모든 예들 또는 예시적인 용어(예들 들어, 등등)의 사용은 단순히 본 개시를 상세히 설명 하기 위한 것으로서 특허청구범위에 의해 한정되지 않는 이상 상기 예들 또는 예시적인 용어로 인해 본 개시의 범위가 한정되는 것은 아니다. 또한, 당업자는 다양한 수정, 조합 및 변경이 부가된 특허청구범위 또는 그 균등 물의 범주 내에서 설계 조건 및 팩터에 따라 구성될 수 있음을 알 수 있다. 따라서, 본 개시의 사상은 상기 설명된 실시 예에 국한되어 정해져서는 아니 되며, 후술하는 특허청구범위 뿐만 아니라 이 특허청구범위와 균등한 또는 이로부터 등가적으로 변경된 모든 범위는 본 개시의 사상의 범주에 속한 다고 할 것이다."}
{"patent_id": "10-2022-0128895", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 의도 분류 장치를 이용하여 서비스를 제공하는 방법을 설명하기 위한 도면이다. 도 2는 일 실시예에 따른 의도 분류 장치를 포함하는 시스템도이다. 도 3은 일 실시예에 따른 의도 분류 장치의 블록도이다. 도 4는 일 실시예에 따른 문장 인코더 경량화에 이용되는 손실을 계산하는 방법의 흐름도이다. 도 5는 일 실시예에 따른 도 4의 단계 420의 일 예를 설명하기 위한 흐름도이다.도 6은 일 실시예에 따른 문장 인코더를 경량화하는 방법의 흐름도이다. 도 7은 일 실시예에 따른 경량화된 문장 인코더를 이용하여 입력 문장의 의도를 파악하는 방법의 흐름도이다. 도 8은 일 실시예에 따른 도 7의 단계 720의 일 예를 설명하기 위한 흐름도이다. 도 9는 일 실시예에 따른 도 7의 단계 740의 일 예를 설명하기 위한 흐름도이다. 도 10은 일 실시예에 따른 과제 특화 어댑터 학습 과정에 이용되는 손실을 계산하는 방법의 흐름도이다. 도 11은 일 실시예에 따른 도 10의 단계 1030의 일 예를 설명하기 위한 흐름도이다. 도 12는 일 실시예에 따른 과제 특화 어댑터를 학습하는 방법의 흐름도이다. 도 13은 일 실시예에 따른 과제 특화 어댑터를 이용하여 입력 문장의 의도를 파악하는 방법의 흐름도이다. 도 14는 일 실시예에 따른 도 13의 단계 1350의 일 예를 설명하기 위한 흐름도이다. 도 15는 일 실시예에 따른 대규모 문장 인코더를 경량화하고 과제 특화 어댑터를 학습하여 사용자 발화 의도를 예측하는 방법의 순서도이다. 도 16은 일 실시예에 따른 대규모 사전학습 언어모델로부터 의도 분류 모델을 생성하는 과정을 설명하기 위한 도면이다."}
