{"patent_id": "10-2020-0096735", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0016614", "출원번호": "10-2020-0096735", "발명의 명칭": "심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템", "출원인": "주식회사 인포웍스", "발명자": "박현주"}}
{"patent_id": "10-2020-0096735", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "EO(Electro Optical) 및 IR(Infrared Radiation) 센서 정보를 통해 융합 이미지를 생성하는 제1 영상융합부와;라이다(LiDAR) 및 레이다(RADAR) 정보를 융합하여 거리정보가 포함된 포인트 클라우드를 생성하는 제2 영상 융합부와;상기 제1 및 제2 영상 융합부를 통해 수신되는 융합 이미지 및 포인트 클라우드를 융합하여 객체를 검출하기 위한 영상 재융합/검출부;를 포함하는 것을 특징으로 하는 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템."}
{"patent_id": "10-2020-0096735", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 제1 영상 융합부는,EO 센서의 ISO 값에 따른 합성비 기준으로 제1 컨볼루션 레이어를 통해 가시광선 영상의 특성을 추출하는 EO 정보 분석모듈 및 IR 센서 정보를 제2 컨볼루션 레이어로 통과시켜 적외선 영상의 특성을 추출하는 IR 정보 분석모듈로 이루어진 제1 영상 인코더와,상기 제1 영상 인코더로부터 추출되는 가시광선 및 적외선 영상의 특성을 융합하기 위한 제1 영상특성 융합 레이어와,상기 제1 영상특성 융합 레이어를 거쳐 융합된 영상을 제3 컨볼루션 레이어를 거쳐 융합 이미지를 재구축하기위한 제1 영상 디코더를 포함하는 것을 특징으로 하는 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합객체 인식 시스템."}
{"patent_id": "10-2020-0096735", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서,상기 제2 영상 융합부는,상기 라이다 및 레이다 정보에 대해 GPS에 기초한 거리정보에 따른 비율을 기준으로 각각 제4 및 제5 컨볼루션레이어를 통해 라이다 영상 및 레이다 영상의 특성을 추출하는 라이다 정보 분석모듈 및 레이다 정보 분석모듈로 이루어진 제2 영상 인코더와,상기 제2 영상 인코더로부터 추출되는 라이다 영상 및 레이다 영상의 특성을 융합하기 위한 제2 영상특성 융합레이어와,상기 제2 영상특성 융합 레이어를 거쳐 융합된 영상을 제6 컨볼루션 레이어를 거쳐 포인트 클라우드로 재구축하기 위한 제2 영상 디코더를 포함하는 것을 특징으로 하는 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템."}
{"patent_id": "10-2020-0096735", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 1에 있어서,공개특허 10-2022-0016614-3-상기 영상 재융합/검출부는,상기 제1 영상 융합부가 생성한 융합 이미지로부터 객체를 검출하기 위한 객체 검출모듈과,상기 제1 영상 융합부가 생성한 융합 이미지로부터 버드 아이 뷰(Bird's eye view) 이미지를 생성하기 위한 버드아이뷰 이미지 생성모듈과,상기 버드 아이 뷰 이미지로부터 세그먼테이션 알고리즘으로 객체 및 배경을 분리한 제1 이미지 세그먼테이션을생성하기 위한 제1 세그먼테이션 생성모듈과,상기 제2 영상 융합부가 생성한 포인트 클라우드로부터 세그먼테이션 알고리즘으로 객체 및 배경을 분리한 제2이미지 세그먼테이션을 생성하기 위한 제2 세그먼테이션 생성모듈과,상기 제1 및 제2 이미지 세그먼테이션에 기초하여 객체 검출모듈에 의해 검출된 객체의 중첩도를 확인하기 위한객체 중첩도 계산모듈과,상기 제2 영상 융합부가 생성한 포인트 클라우드 중 객체 중첩도 계산모듈에 의해 확인된 객체 중첩도가 일정수준 이상인 객체에 대해 뎁스맵을 생성하기 위한 뎁스맵 생성모듈을 포함하는 것을 특징으로 하는 심층 신경망알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템."}
{"patent_id": "10-2020-0096735", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 1에 있어서,상기 제1, 제2 영상 융합부 및 영상 재융합/검출부의 학습 데이터를 생성하기 위한 학습부를 더 포함하는 것을특징으로 하는 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템."}
{"patent_id": "10-2020-0096735", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 5에 있어서,상기 학습부는 제1 영상 융합부에서 생성되는 융합 이미지의 정확도를 측정하기 위한 손실함수 L을 하기 수학식1 내지 5를 통해 산출하고, 손실함수 L을 최소화하도록 학습을 진행하는 것을 특징으로 하는 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템.수학식 1: LIR = λLssim,IR + Lp,IR수학식 2: LEO = λLssim,EO + Lp,EO수학식 3: Lp = ∥Output - Input∥2수학식 4: Lssim = 1 - SSIM(Output - Input)수학식 5: L = LIR + LEO(LEO, LIR은 각각 가시광선 및 적외선 영상의 손실함수이고, Output는 융합 이미지를 의미하고, Input는 적외선또는 가시광선 영상을 의미하며, SSIM 함수는 이미지간 구조적 유사성 연산을 나타냄)"}
{"patent_id": "10-2020-0096735", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 5에 있어서,상기 학습부는 제2 영상 융합부에서 생성되는 포인트 클라우드의 거리정보 정확도를 측정하기 위한 손실함수 L를 하기 수학식 6 내지 10을 통해 산출하고, 손실함수 L를 최소화하도록 학습을 진행하는 것을 특징으로 하는심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템.공개특허 10-2022-0016614-4-수학식 6: Lx = Groundtruthx - Inputx수학식 7: Ly = Groundtruthy - Inputy수학식 8: Lz = Groundtruthz - Inputz수학식 9: Ld = ∥Groundtruth - Input∥수학식 10: L = Ld + Lx + Ly + Lz(Lx, Ly, Lz은 X, Y, Z 좌표의 손실함수이고, Ld는 거리정보에 대한 손실함수이고, Inputx, Inputy, Inputz는RADAR 또는 LiDAR 를 통해 검출된 물체의 X, Y, Z 좌표값이고, Input는 RADAR 또는 LiDAR 를 통해 검출된 물체의 유클리드 거리이며, Groundtruthx, Groundtruthy, Groundtruthz는 본 발명에 따른 객체 인식 시스템으로부터물체까지의 실제 X, Y, Z 좌표값이며 Groundtruth는 객체 인식 시스템으로부터 물체까지의 실제 유클리드 거리임)"}
{"patent_id": "10-2020-0096735", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 객체 인식 시스템에 관한 것으로서, 보다 구체적으로는 적외선 센서 정보, 가시 영상 센서 정보, 라이 다 정보, 레이다 정보를 신경망 기술을 통해 통합적으로 융합하여 날씨, 조도 변화 등에도 강건하게 객체를 인식 할 수 있는 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템에 관한 것이다. 이를 위해 본 발명은, EO(Electro Optical) 및 IR(Infrared Radiation) 센서 정보를 통해 융합 이미지를 생성하 는 제1 영상 융합부와; 라이다(LiDAR) 및 레이다(RADAR) 정보를 융합하여 거리정보가 포함된 포인트 클라우드를 생성하는 제2 영상 융합부와; 상기 제1 및 제2 영상 융합부를 통해 수신되는 융합 이미지 및 포인트 클라우드를 융합하여 객체를 검출하기 위한 영상 재융합/검출부;를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2020-0096735", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 객체 인식 시스템에 관한 것으로서, 보다 구체적으로는 적외선 센서 정보, 가시 영상 센서 정보, 라 이다 정보, 레이다 정보를 신경망 기술을 통해 통합적으로 융합하여 날씨, 조도 변화 등에도 강건하게 객체를 인식할 수 있는 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템에 관한 것이다."}
{"patent_id": "10-2020-0096735", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일반적으로 가시 카메라 센서에 의해 생성된 가시 영상은 조도에 취약점을 갖고, 적외선 카메라 센서에 의해 생 성된 적외선 영상은 질감 정보 수집이 불가능한 취약점을 갖게 된다. 이와 같은 2가지 센서의 취약점을 상호 보완하기 위해서 최근에 들어 센서 융합과 관련된 기술이 많이 발전되고 있으나, 아직까지는 밝기 정보 기반의 통계정보에 따라 합성 비율을 조정하는 방식이 주류를 이루고 있다. 또 한편으로, 라이다 센서는 짧은 주파수로 작은 물체도 감지 가능한 장점을 갖지만 날씨나 외부 환경에 민감한 단점을 갖는 반면, 레이다 센서는 라이다 센서보다 더 긴 작동거리를 갖고 외부환경에도 영향을 덜 받지만 물체 의 정밀한 이미지를 제공하지 못한다는 단점을 갖고 있기 때문에 라이다 및 레이다 센서의 취약점을 상호 보완 하기 위한 알고리즘들도 활발히 개발되고 있다. 그런데, 상술된 바와 같은 가시 카메라 센서 및 적외선 카메라 센서 2가지의 융합이나, 라이다 및 레이다 센서 2가지의 융합이나, 가시 카메라 센서 및 라이다센서와의 융합정보를 이용하는 기술은 많이 알려져 있으나, 가시 카메라 센서/적외선 카메라 센서/라이다 센서/레이다 센서 4가지 모두를 융합하여 영상의 안정성을 더욱 보완하 려는 시도는 아직까지 구체적으로 공지된 바가 없는 이유로, 이에 대한 연구가 필요한 실정이다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 등록특허공보 제10-1961177호 (특허문헌 0002) 대한민국 공개특허공보 제10-2016-0053612호"}
{"patent_id": "10-2020-0096735", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기한 종래 기술의 문제점을 해결하기 위해 안출된 것으로서, 적외선 센서 정보, 가시 영상 센서 정보, 라이다 정보, 레이다 정보를 신경망 기술을 통해 통합적으로 융합하여 날씨, 조도 변화 등에도 강건하게 객체를 인식할 수 있는 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템을 제공하는데 그 목적이 있다."}
{"patent_id": "10-2020-0096735", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위하여, 본 발명은, EO(Electro Optical) 및 IR(Infrared Radiation) 센서 정보를 통해 융합 이미지를 생성하는 제1 영상 융합부와; 라이다(LiDAR) 및 레이다(RADAR) 정보를 융합하여 거리정보가 포함 된 포인트 클라우드를 생성하는 제2 영상 융합부와; 상기 제1 및 제2 영상 융합부를 통해 수신되는 융합 이미지 및 포인트 클라우드를 융합하여 객체를 검출하기 위한 영상 재융합/검출부;를 포함하는 것을 특징으로 한다. 여기서, 상기 제1 영상 융합부는, EO 센서의 ISO 값에 따른 합성비 기준으로 제1 컨볼루션 레이어를 통해 가시 광선 영상의 특성을 추출하는 EO 정보 분석모듈 및 IR 센서 정보를 제2 컨볼루션 레이어로 통과시켜 적외선 영 상의 특성을 추출하는 IR 정보 분석모듈로 이루어진 제1 영상 인코더와, 상기 제1 영상 인코더로부터 추출되는 가시광선 및 적외선 영상의 특성을 융합하기 위한 제1 영상특성 융합 레이어와, 상기 제1 영상특성 융합 레이어 를 거쳐 융합된 영상을 제3 컨볼루션 레이어를 거쳐 융합 이미지를 재구축하기 위한 제1 영상 디코더를 포함하 는 것을 특징으로 한다. 또한, 상기 제2 영상 융합부는, 상기 라이다 및 레이다 정보에 대해 GPS에 기초한 거리정보에 따른 비율을 기준 으로 각각 제4 및 제5 컨볼루션 레이어를 통해 라이다 영상 및 레이다 영상의 특성을 추출하는 라이다 정보 분 석모듈 및 레이다 정보 분석모듈로 이루어진 제2 영상 인코더와, 상기 제2 영상 인코더로부터 추출되는 라이다 영상 및 레이다 영상의 특성을 융합하기 위한 제2 영상특성 융합 레이어와, 상기 제2 영상특성 융합 레이어를 거쳐 융합된 영상을 제6 컨볼루션 레이어를 거쳐 포인트 클라우드로 재구축하기 위한 제2 영상 디코더를 포함하 는 것을 특징으로 한다. 또한, 상기 영상 재융합/검출부는, 상기 제1 영상 융합부가 생성한 융합 이미지로부터 객체를 검출하기 위한 객 체 검출모듈과, 상기 제1 영상 융합부가 생성한 융합 이미지로부터 버드 아이 뷰(Bird's eye view) 이미지를 생 성하기 위한 버드아이뷰 이미지 생성모듈과, 상기 버드 아이 뷰 이미지로부터 세그먼테이션 알고리즘으로 객체 및 배경을 분리한 제1 이미지 세그먼테이션을 생성하기 위한 제1 세그먼테이션 생성모듈과, 상기 제2 영상 융합 부가 생성한 포인트 클라우드로부터 세그먼테이션 알고리즘으로 객체 및 배경을 분리한 제2 이미지 세그먼테이 션을 생성하기 위한 제2 세그먼테이션 생성모듈과, 상기 제1 및 제2 이미지 세그먼테이션에 기초하여 객체 검출 모듈에 의해 검출된 객체의 중첩도를 확인하기 위한 객체 중첩도 계산모듈과, 상기 제2 영상 융합부가 생성한 포인트 클라우드 중 객체 중첩도 계산모듈에 의해 확인된 객체 중첩도가 일정 수준 이상인 객체에 대해 뎁스맵 을 생성하기 위한 뎁스맵 생성모듈을 포함하는 것을 특징으로 한다. 더욱이, 본 발명은, 상기 제1, 제2 영상 융합부 및 영상 재융합/검출부의 학습 데이터를 생성하기 위한 학습부 를 더 포함하는 것을 특징으로 한다.여기서, 상기 학습부는 제1 영상 융합부에서 생성되는 융합 이미지의 정확도를 측정하기 위한 손실함수 L을 하 기 수학식 1 내지 5를 통해 산출하고, 손실함수 L을 최소화하도록 학습을 진행하는 것을 특징으로 한다. 수학식 1: LIR = λLssim,IR + Lp,IR 수학식 2: LEO = λLssim,EO + Lp,EO 수학식 3: Lp = ∥Output - Input∥2 수학식 4: Lssim = 1 - SSIM(Output - Input) 수학식 5: L = LIR + LEO (LEO, LIR은 각각 가시광선 및 적외선 영상의 손실함수이고, Output는 융합 이미지를 의미하고, Input는 적외선 또는 가시광선 영상을 의미하며, SSIM 함수는 이미지간 구조적 유사성 연산을 나타냄) 또한, 상기 학습부는 제2 영상 융합부에서 생성되는 포인트 클라우드의 거리정보 정확도를 측정하기 위한 손실 함수 L를 하기 수학식 6 내지 10을 통해 산출하고, 손실함수 L를 최소화하도록 학습을 진행하는 것을 특징으로 한다. 수학식 6: Lx = Groundtruthx - Inputx 수학식 7: Ly = Groundtruthy - Inputy 수학식 8: Lz = Groundtruthz - Inputz 수학식 9: Ld = ∥Groundtruth - Input∥ 수학식 10: L = Ld + Lx + Ly + Lz (Lx, Ly, Lz은 X, Y, Z 좌표의 손실함수이고, Ld는 거리정보에 대한 손실함수이고, Inputx, Inputy, Inputz는 RADAR 또는 LiDAR 를 통해 검출된 물체의 X, Y, Z 좌표값이고, Input는 RADAR 또는 LiDAR 를 통해 검출된 물체 의 유클리드 거리이며, Groundtruthx, Groundtruthy, Groundtruthz는 본 발명에 따른 객체 인식 시스템으로부터 물체까지의 실제 X, Y, Z 좌표값이며 Groundtruth는 객체 인식 시스템으로부터 물체까지의 실제 유클리드 거리 임)"}
{"patent_id": "10-2020-0096735", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상과 같은 본 발명에 따른 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템은 다음 과 같은 효과를 나타낼 수 있다. 1. EO 센서 정보, IR 센서 정보, 라이다 센서 정보, 레이다 정보를 딥러닝 기반 알고리즘으로 함께 융합하여 날 씨, 조도 변화 등에도 강건하게 객체를 인식할 수 있음"}
{"patent_id": "10-2020-0096735", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "2. 가시 삼차원 공간 내에 객체인식 성능 향상을 위해 가시 카메라 센서, 적외선 카메라 센서, 라이다 센서, 레 이다 센서의 4개 센서 데이터를 융합하여 객체를 인식할 수 있는 인공지능 모델을 제시함"}
{"patent_id": "10-2020-0096735", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서 및 청구범위에 사용된 용어나 단어는 통상적이거나 사전적인 의미로 한정해서 해석되어서는 아니 되 며, 발명자는 그 자신의 발명을 가장 최선의 방법으로 설명하기 위해 용어의 개념을 적절하게 정의할 수 있다는 원칙에 입각하여 본 발명의 기술적 사상에 부합하는 의미와 개념으로 해석되어야만 한다. 따라서, 본 명세서에 기재된 실시예와 도면에 도시된 구성은 본 발명의 가장 바람직한 일 실시예에 불과할 뿐이 고, 본 발명의 기술적 사상을 모두 대변하는 것은 아니므로, 본 출원시점에 있어서 이들을 대체할 수 있는 다양 한 균등물과 변형예들이 있을 수 있음을 이해하여야 한다. 이하, 도면을 참조로 하여 본 발명에 따른 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템을 설명하기로 한다. 도 1은 본 발명에 따른 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템을 개략적으로 도시한 도면이다. 본 발명은 기본적으로 제1 영상 융합부, 제2 영상 융합부 및 영상 재융합/검출부를 포함하여 구성된다. 보다 구체적으로, 본 발명은 EO(Electro Optical) 및 IR(Infrared Radiation) 센서 정보를 통해 융합 이미지를 생성하는 제1 영상 융합부와, 라이다(LiDAR) 및 레이다(RADAR) 정보를 융합하여 거리정보가 포함된 포인트 클라 우드를 생성하는 제2 영상 융합부와, 제1 및 제2 영상 융합부를 통해 수신되는 융합 이미지 및 포인트 클라우드 를 재융합하여 객체를 검출하기 위한 영상 재융합/검출부를 포함하여 이루어진다. 여기서, 상기 제1 영상 융합부는 EO 센서의 ISO 값에 따른 합성비 기준으로 제1 컨볼루션 레이어를 통해 가시광 선 영상의 특성을 추출하는 EO 정보 분석모듈 및 IR 센서 정보를 제2 컨볼루션 레이어로 통과시켜 적외선 영상 의 특성을 추출하는 IR 정보 분석모듈로 이루어진 제1 영상 인코더와, 제1 영상 인코더로부터 추출되는 가시광 선 및 적외선 영상의 특성을 융합하기 위한 제1 영상특성 융합 레이어와, 제1 영상특성 융합 레이어를 거쳐 융 합된 영상을 제3 컨볼루션 레이어를 거쳐 융합 이미지를 재구축하기 위한 제1 영상 디코더를 포함하게 된다. 상기 제1 영상 인코더는 EO 센서에 의해 전송되는 EO 정보(가시광선 영상)과 IR에 의해 전송되는 IR 정보(적외 선 영상)로부터 컨볼루션 연산에 의해 각 영상에 대한 특성을 추출하도록 기능한다. 상기 제1 영상 인코더의 EO 정보 분석모듈은 EO 정보를 ISP(Image Signal Processor)로 통과시키면서 ISO별로 적용되는 노이즈 감쇠와 같은 튜닝 값에 의해 ISO마다 다른 특성의 이미지를 출력하도록 하기 때문에 해당 특성 에 부합하는 컨볼루션 필터를 구비하는 것이 바람직하다.또한, 상기 EO 정보 분석모듈은 VGGNET에 의한 제1 컨볼루션 레이어의 반복적인 컨볼루션 연산을 통해 EO 정보 의 특성을 추출하게 되고, 마찬가지로 IR 정보 분석모듈은 VGGNET에 의한 제2 컨볼루션 레이어의 컨볼루션 연산 을 통해 IR 정보의 특성을 추출하게 된다. 상기 제1 영상특성 융합 레이어는 영상 인코더로부터 추출되는 가시광선 및 적외선 영상의 특성을 융합하는 과 정에서 제1 영상 인코더에서 사용된 ISO 값에 따른 합성비를 기준으로 추출된 가시광선 및 적외선 영상의 특성 을 융합하게 된다. 상기 제1 영상 디코더는 제1 영상특성 융합 레이어를 거쳐 융합된 영상을 제3 컨볼루션 레이어를 거쳐 재구축하 여 가시광선 및 적외선 영상 특성이 융합된 최종 융합 이미지를 출력하게 된다. 상기 제2 영상 융합부는 라이다 및 레이다 정보에 대해 GPS에 기초한 거리정보에 따른 비율을 기준으로 각각 제 4 및 제5 컨볼루션 레이어를 통해 라이다 영상 및 레이다 영상의 특성을 추출하는 라이다 정보 분석모듈 및 레 이다 정보 분석모듈로 이루어진 제2 영상 인코더와, 제2 영상 인코더로부터 추출되는 라이다 영상 및 레이다 영 상의 특성을 융합하기 위한 제2 영상특성 융합 레이어와, 제2 영상특성 융합 레이어를 거쳐 융합된 영상을 제6 컨볼루션 레이어를 거쳐 포인트 클라우드로 재구축하기 위한 제2 영상 디코더를 포함하여 이루어진다. 상기 제2 영상 인코더는 라이다 센서에 의해 전송되는 라이다 정보(라이다 영상)과 레이다 센서에 의해 전송되 는 레이다 정보(레이다 영상)로부터 컨볼루션 연산에 의해 각 영상에 대한 특성을 추출하도록 기능한다. 상기 제2 영상 인코더의 라이다 정보 분석모듈 및 레이다 정보 분석모듈은 각각 라이다 정보 및 레이다 정보에 거리정보에 따른 비율이 반영되도록 한 상태에서 제4 및 제5 컨볼루션 레이어의 반복적인 컨볼루션 연산을 통해 라이다 영상 및 레이다 영상의 특성을 추출하게 된다. 상기 제2 영상특성 융합 레이어는 제2 영상 인코더로부터 추출되는 라이다 및 레이다 영상의 특성을 융합하게 된다. 상기 제2 영상 디코더는 제2 영상특성 융합 레이어를 거쳐 융합된 영상을 제6 컨볼루션 레이어를 거쳐 재구축하 여 라이다 및 레이다 영상 특성이 융합된 포인트 클라우드를 출력하게 된다. 참고로, CNN은 사람의 시신경 구조를 모방한 구조로, 컨볼루션 연산을 사용하는 인공 신경망의 한 종류인데, 일 반적인 신경망과의 가장 큰 차이점은 CNN에서 입력 데이터가 이미지라는 것이고, 여기서의 이미지는 행렬 형태 로 이루어지게 되며, Width x Height x Depth 형태를 갖게 된다. 이러한 CNN의 가장 큰 특징은 일반적인 신경망 앞에 여러 컨볼루션 계층을 붙여 특징 추출/분류를 가능하게 한다는 것이고, 이에 따라 컨볼루션 레이어들을 통 해 입력 이미지에 대한 특성을 추출하게 되어 추출된 특성을 기반으로 기존의 신경망을 이용하여 분류 과정이 진행되도록 한다. 컨볼루션 레이어는 입력 데이터로부터 특징을 추출하는 역할을 수행하고, 이를 위해 특성을 추출하는 필터 및 필터의 값을 비선형 값으로 바꾸어주는 활성화 함수로 이루어진다. 필터는 추출하려는 이미지 의 특징이 대상 데이터에 있는지 없는지를 검출해주는 함수라 할 수 있고 행렬로 정의되어져, 입력받은 데이터 에서 해당 특성을 가지고 있다면 결과 값이 크게 나오고, 특성을 가지고 있지 않거나 비슷하지 않을 경우에 결 과값이 0에 가까운 값이 나오게 되어 입력 데이터가 그 특성을 가지고 있는지 없는지의 여부를 확인할 수 있게 해준다. 이렇게 필터들을 통해서 특징 맵이 추출되면 이 특징 맵에 활성화 함수를 적용하여 값을 활성화시키고, 이 값이 정량적으로 나오기 때문에 이러한 값들을 \"곡선 특징을 가지고 있다\" 와 \"없다\"로 바꾸어 주는 과정이필요하며, 이러한 과정은 활성화 함수에 의해 수행된다. 이렇게 추출된 특성들을 필요에 따라 서브 샘플링 과정 을 거치게 되고, 컨볼루션 레이어를 통해 특성이 추출되면 모든 특성을 고려할 필요가 없이 추출된 특성맵을 줄 이는 작업이 따르게 되며, 추출된 특성값을 기존의 신경망에 적용하여 분류하는 과정을 거치게 된다. 한편으로, 상기 영상 재융합/검출부는 제1 영상 융합부가 생성한 융합 이미지로부터 객체를 검출하기 위한 객체 검출모듈과, 제1 영상 융합부가 생성한 융합 이미지로부터 버드 아이 뷰(Bird's eye view) 이미지를 생성하기 위한 버드아이뷰 이미지 생성모듈과, 버드 아이 뷰 이미지로부터 세그먼테이션 알고리즘으로 객체 및 배경을 분 리한 제1 이미지 세그먼테이션을 생성하기 위한 제1 세그먼테이션 생성모듈과, 제2 영상 융합부가 생성한 포인 트 클라우드로부터 세그먼테이션 알고리즘으로 객체 및 배경을 분리한 제2 이미지 세그먼테이션을 생성하기 위 한 제2 세그먼테이션 생성모듈과, 제1 및 제2 이미지 세그먼테이션에 기초하여 객체 검출모듈에 의해 검출된 객 체의 중첩도를 확인하기 위한 객체 중첩도 계산모듈과, 제2 영상 융합부가 생성한 포인트 클라우드 중 객체 중 첩도 계산모듈에 의해 확인된 객체 중첩도가 일정 수준 이상인 객체에 대해 뎁스맵을 생성하기 위한 뎁스맵 생 성모듈을 포함하여 이루어진다. 더욱이, 심층 신경망 알고리즘을 수행하기 위해서 본 발명에 따른 시스템이 수행되기 전에 학습 데이터를 생성 해 둬야 하기 때문에, 본 발명은 상기 제1, 제2 영상 융합부 및 영상 재융합/검출부의 학습 데이터를 생성하기 위한 학습부를 더 포함하게 된다. 상기 제1 영상 융합부를 위한 학습 데이터는 제1 영상 인코더, 제1 영상 디코더, 제1 및 제2 컨볼루션 레이어 필터 정보, ISO 값에 따른 제1 영상특성 융합 레이어의 합성비 정보를 포함하게 된다. 여기서, 상기 학습부는 학습의 정확도를 높이기 위해 제1 영상 융합부에서 생성되는 융합 이미지의 정확도를 측 정하기 위한 손실함수 L을 하기 수학식 1 내지 5를 통해 산출하고, 손실함수 L을 최소화하도록 학습을 진행하게 된다. 수학식 1: LIR = λLssim,IR + Lp,IR 수학식 2: LEO = λLssim,EO + Lp,EO 수학식 3: Lp = ∥Output - Input∥2 수학식 4: Lssim = 1 - SSIM(Output - Input) 수학식 5: L = LIR + LEO (LEO, LIR은 각각 가시광선 및 적외선 영상의 손실함수이고, Output는 융합 이미지를 의미하고, Input는 적외선 또는 가시광선 영상을 의미하며, SSIM 함수는 이미지간 구조적 유사성 연산을 나타냄) 참고로, SSIM 함수는 두 이미지의 구조적 유사성을 나타내게 되고, 훈련단계에서 픽셀 손실과 SSIM 손실 사이에 는 3단계 차이가 있기 때문에 1, 100, 1000으로 각각 설정될 수 있다. 이와 같은 학습 방식에 따르면, 두개의 서로 다른 특성의 입력 영상이 출력 영상의 특성과 차이가 최소화되도록 컨볼루션 뉴런 네트워크를 학습시켜 가시적으로 중요한 정보가 손실되지 않도록 학습 방향을 설정하게 된다. 상기 제2 영상 융합부를 위한 학습 데이터는 제2 영상 인코더, 제2 영상 디코더, 제4 및 제5 컨볼루션 레이어 필터 정보, 거리정보 값에 따른 제2 영상특성 융합 레이어의 합성비 정보를 포함하게 된다. 또한, 상기 학습부는 학습의 정확도를 높이기 위해 센서 융합으로 생성된 거리정보가 실제와 얼마나 차이 나는 지 확인하게 되고, 이를 위해 제2 영상 융합부에서 생성되는 포인트 클라우드의 거리정보 정확도를 측정하기 위 한 손실함수 L를 하기 수학식 6 내지 10을 통해 산출하고, 손실함수 L를 최소화하도록 학습을 진행하게 된다. 수학식 6: Lx = Groundtruthx - Inputx 수학식 7: Ly = Groundtruthy - Inputy 수학식 8: Lz = Groundtruthz - Inputz 수학식 9: Ld = ∥Groundtruth - Input∥ 수학식 10: L = Ld + Lx + Ly + Lz (Lx, Ly, Lz은 X, Y, Z 좌표의 손실함수이고, Ld는 거리정보에 대한 손실함수이고, Inputx, Inputy, Inputz는 RADAR 또는 LiDAR 를 통해 검출된 물체의 X, Y, Z 좌표값이고, Input는 RADAR 또는 LiDAR 를 통해 검출된 물체 의 유클리드 거리이며, Groundtruthx, Groundtruthy, Groundtruthz는 본 발명에 따른 객체 인식 시스템으로부터 물체까지의 실제 X, Y, Z 좌표값이며 Groundtruth는 객체 인식 시스템으로부터 물체까지의 실제 유클리드 거리 임) 이와 같은 학습 방식에 따르면, 두개의 서로 다른 특성의 센서 출력 데이터를 거리에 따라 합성비를 달리하여 컨볼루션 뉴런 네트워크를 학습시켜 거리와 관련하여 더욱 정확한 정보를 출력하는 센서 정보를 융합할 수 있도 록 학습 방향을 설정하게 된다. 상기 영상 재융합/검출부를 위한 학습 데이터는 객체 검출 및 세그먼테이션을 위한 보편적인 알고리즘에 따를 수 있는 한편, 객체 중첩도 계산모듈에서는 제1 및 제2 세그먼테이션 생성모듈에서 추출된 객체의 중첩도가 90% 이상인 경우 검출된 해당 객체에 대한 객체 인식/탐지를 수행하게 된다. 이상에서 첨부된 도면을 참조하여 본 발명을 설명함에 있어 특정형상 및 방향을 위주로 설명하였으나, 본 발명 은 당업자에 의하여 다양한 변형 및 변경이 가능하고, 이러한 변형 및 변경은 본 발명의 권리범위에 포함되는 것으로 해석되어야 한다.도면 도면1 도면2 도면3 도면4"}
{"patent_id": "10-2020-0096735", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템을 개략적으로 도시한 도면 도 2는 본 발명에 따른 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템 중 제1 영상 융합부를 나타낸 도면 도 3은 본 발명에 따른 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템 중 제2 영상 융합부를 나타낸 도면 도 4는 본 발명에 따른 심층 신경망 알고리즘 기반 EO IR RADAR LiDAR 센서융합 객체 인식 시스템 중 영상 재융 합/검출부를 나타낸 도면"}
