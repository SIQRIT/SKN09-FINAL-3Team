{"patent_id": "10-2023-0012797", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0120283", "출원번호": "10-2023-0012797", "발명의 명칭": "언어 모델을 학습시키는 방법, 언어 모델을 이용한 제어된 이야기 생성 방법 및 이를 이용한", "출원인": "성균관대학교산학협력단", "발명자": "정윤경"}}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서를 구비하는 처리 장치가 언어 모델을 학습하는 방법에 있어서,말뭉치를 상기 언어 모델에 입력해 상기 언어 모델을 학습시키되, 제1 손실 함수를 최소화하는 방향으로 상기언어 모델을 사전 학습시키는 단계; 그리고,사전 학습된 언어 모델에 상기 말뭉치와 상기 말뭉치에 레이블된 메타 정보를 학습용 데이터셋으로 입력해 상기사전 학습된 언어 모델을 보강 학습시키되, 제2 손실 함수를 최소화하는 방향으로 상기 언어 모델을 보강 학습시키는 단계;를 포함하는 언어 모델의 학습 방법."}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 손실 함수(Lnll)와 상기 제2 손실함수(LScl)의 합(Ltotal)은 상기 제1 손실함수와 제2 손실함수가 트레이드오프(tradeoff)의 상관관계를 갖는 언어 모델의 학습 방법."}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 제1 손실 함수는, 아래의 수식 1과 같이 정의된 언어 모델의 학습 방법.[수식 1]D: 언어모델이 한번에 보는 데이터들x(i): i번째 데이터(이야기)에 대한 각 단어들t: 왼쪽부터 그 순서를 번호로g(i):i 번째 데이터에 대한 장르(code)"}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 제2 손실 함수는, 아래의 수식 2와 같이 정의된 언어 모델의 학습 방법.[수식 2]P(i): i번째 데이터를 볼 때 주변 데이터 여러 개를 같이 봄을 정의하며, i번째 데이터가 해당하는 장르와 같은장르인 데이터들을 같이 봄exp: exponential 함수z: 유사도 벡터공개특허 10-2024-0120283-3-tau (τ): 훈련 강도를 조절하는 값"}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 제1 손실함수와 상기 제2 손실함수의 합(Ltotal)은 아래의 수식 3과 같이 정의된 언어 모델의 학습 방법.[수식 3]"}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "학습된 언어 모델을 갖는 처리 장치가 제어된 이야기를 생성하는 방법에 있어서,상기 학습된 언어 모델이 이야기 생성을 위한 기초 데이터를 입력받는 단계; 및상기 학습된 언어 모델이 입력된 기초 데이터에 기초해 속성이 제어된 이야기를 출력하는 단계;를 포함하고,상기 학습된 언어 모델은,말뭉치를 상기 언어 모델에 입력해 상기 언어 모델을 학습시키되, 제1 손실 함수를 최소화하는 방향으로 상기언어 모델을 사전 학습시키고, 사전 학습된 언어 모델에 상기 말뭉치와 상기 말뭉치에 레이블된 메타 정보를 학습용 데이터셋으로 입력해 상기 사전 학습된 언어 모델을 보강 학습시키되, 제2 손실 함수를 최소화하는 방향으로 상기 언어 모델을 보강 학습시키는 과정을 포함하는 제어된 이야기의 생성 방법."}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 제1 손실 함수(Lnll)와 상기 제2 손실함수(LScl)의 합(Ltotal)은 상기 제1 손실함수와 제2 손실함수가 트레이드오프(tradeoff)의 상관관계를 갖는, 제어된 이야기의 생성 방법."}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 제1 손실 함수는, 아래의 수식 4와 같이 정의된 제어된 이야기의 생성 방법.[수식 4]D: 언어모델이 한번에 보는 데이터들x(i): i번째 데이터(이야기)에 대한 각 단어들t: 왼쪽부터 그 순서를 번호로g(i):i 번째 데이터에 대한 장르(code)"}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 제2 손실 함수는, 아래의 수식 5와 같이 정의된 제어된 이야기의 생성 방법.[수식 5]P(i): i번째 데이터를 볼 때 주변 데이터 여러 개를 같이 봄을 정의하며, i번째 데이터가 해당하는 장르와 같은공개특허 10-2024-0120283-4-장르인 데이터들을 같이 봄exp: exponential 함수z: 유사도 벡터tau (τ): 훈련 강도를 조절하는 값"}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 제1 손실함수와 상기 제2 손실함수의 합(Ltotal)은 아래의 수식 6과 같이 정의된 제어된 이야기의 생성방법.[수식 6]"}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 1 항 내지 제 10 항 중에 어느 한 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2023-0012797", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "속성이 제어된 이야기를 예측하는 언어 모델을 저장하는 메모리; 및상기 언어 모델을 언어 모델을 학습하는 프로그램을 실행하는 프로세서;를 포함하고,상기 프로세서에 의해 실행되는 프로그램은,말뭉치를 상기 언어 모델에 입력해 상기 언어 모델을 학습시키되, 제1 손실 함수를 최소화하는 방향으로 상기언어 모델을 사전 학습시키고, 사전 학습된 언어 모델에 상기 말뭉치와 상기 말뭉치에 레이블된 메타 정보를 학습용 데이터셋으로 입력해 상기 사전 학습된 언어 모델을 보강 학습시키되, 제2 손실 함수를 최소화하는 방향으로 상기 언어 모델을 보강 학습시키는 과정을 포함하고, 상기 제1 손실 함수(Lnll)와 상기 제2 손실함수(LScl)의합(Ltotal)은 상기 제1 손실함수와 제2 손실함수가 트레이드오프(tradeoff)의 상관관계를 갖는, 처리 장치."}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 언어 모델의 학습 방법은, 적어도 하나의 프로세서를 구비하는 처리 장치가 언어 모 델을 학습하는 방법에 있어서, 말뭉치를 상기 언어 모델에 입력해 상기 언어 모델을 학습시키되, 제1 손실 함수 를 최소화하는 방향으로 상기 언어 모델을 사전 학습시키는 단계, 사전 학습된 언어 모델에 상기 말뭉치와 상기 말뭉치에 레이블된 메타 정보를 학습용 데이터셋으로 입력해 상기 사전 학습된 언어 모델을 보강 학습시키되, 제 2 손실 함수를 최소화하는 방향으로 상기 언어 모델을 보강 학습시키는 단계를 포함하고, 상기 제1 손실 함수 (Lnll)와 상기 제2 손실함수(LScl)의 합(Ltotal)은 상기 제1 손실함수와 제2 손실함수가 트레이드오프 (tradeoff)의 상관관계를 갖다."}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 사용자가 요구하는 이야기의 속성을 기초로 제어된 이야기를 생성해 내는 인공지능에 관한 것으로, 보다 상세히는 사용자가 제시한 속성에 맞춰 이야기를 만들 수 있도록 언어 모델을 학습시키고 추론하는 방법과 이을 이용한 장치에 관한 것이다."}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "장르, 배경, 문체 등의 일관성은 독자의 이야기에 대한 기대와 흥미를 유지하는데 있어 매우 중요하다. 그러나 지금까지 제안된 기술들은 전반적인 이야기의 구조에 맞게 이야기를 만드는 것일 뿐, 사용자가 제시한 속성, 즉 장르, 배경, 문체 등에 기반해 이야기를 만드는 시스템은 존재하지 않다. 일 예로, 공개번호 KR 2022-0071372는 데이터베이스에 다수의 이야기들이 학습데이터로 저장되어 있으며, 데이 터베이스에 저장된 학습데이터를 이용하여 머신러닝 또는 딥러닝 등을 통해 학습을 수행하는 기술을 개시하나, 이는 어디까지나 이야기를 학습하기 위한 것이지, 사용자가 제시한 속성에 맞춰 이야기를 생성하는 구성은 설명 하고 있지 못한다. 또한, 본 발명의 출원 전부터 Text Style Transfer, Conditional Generation 등의 이름으로 텍스트 생성에 관 한 많은 연구가 진행되었으며, 최근에는 대용량 사전학습 언어 모델을 활용해 CTRL(Keskar, 2019), PPLM(Dathatri, 2020), GeDi(Krause, 2021), CoCon(Chan, 2021) 등과 같은 다양한 연구가 진행중이다. 다만, 현재까지의 연구들은 명시적으로 제어력(Controllability)을 높이는 훈련 목표를 설정하지 않았다. 이로 인하여 주어진 target code(사용자가 입력한 속성)와 text prompt(사용자가 입력한 글의 앞부분)중에, text prompt에 더 의존하여 텍스트를 생성하는 문제가 있었다. 예를 들어서, “조 바이든 대통령” 이라는 프롬프트 를 주고 속성을 “스포츠”라고 할 시, 이미 인공지능은 조 바이든 대통령에 대해 정치와 관련된 내용으로 학습 을 대부분 하여 우리가 원하는 스포츠라는 속성이 아니라 정치 관련된 이야기를 생성해낼 가능성이 높다. 만약 속성 제어가 가능하다면, 언어 모델을 이용해 토픽에 맞는 뉴스를 작성하거나, 소설 장르에 맞춰 이야기를 만들 수 있거나, 또는 텍스트이 전반적인 어투를 제어해 일상 대화, 학술 대화, 토론 대화 등을 만들어 낼 수 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국특허공개공보 제2022-0071372호, \" 인공지능 기반의 이야기 구성방법 및 이를 위한 이야기 구성시스템 \""}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 기술적 과제는, 사용자가 원하는 속성(장르, 핵심 단어, 등)에 따라 이야기를 만들 어주는 인공지능을 개발하고자 한다."}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 기술적 과제를 해결하기 위하여, 본 발명의 일 실시예에 따른 언어 모델의 학습 방법은, 적어도 하나의 프 로세서를 구비하는 처리 장치가 언어 모델을 학습하는 방법에 있어서, 말뭉치를 상기 언어 모델에 입력해 상기 언어 모델을 학습시키되, 제1 손실 함수를 최소화하는 방향으로 상기 언어 모델을 사전 학습시키는 단계, 사전 학습된 언어 모델에 상기 말뭉치와 상기 말뭉치에 레이블된 메타 정보를 학습용 데이터셋으로 입력해 상기 사전 학습된 언어 모델을 보강 학습시키되, 제2 손실 함수를 최소화하는 방향으로 상기 언어 모델을 보강 학습시키는 단계를 포함하고, 상기 제1 손실 함수(Lnll)와 상기 제2 손실함수(LScl)의 합(Ltotal)은 상기 제1 손실함수와 제2 손실함수가 트레이드오프(tradeoff)의 상관관계를 갖다. 본 발명의 다른 실시예에 따른 제어된 이야기를 생성하는 방법은, 학습된 언어 모델을 갖는 처리 장치가 제어된 이야기를 생성하는 방법에 있어서, 상기 학습된 언어 모델이 이야기 생성을 위한 기초 데이터를 입력받는 단계, 상기 학습된 언어 모델이 입력된 기초 데이터에 기초해 속성이 제어된 이야기를 출력하는 단계를 포함하고, 상 기 학습된 언어 모델은, 말뭉치를 상기 언어 모델에 입력해 상기 언어 모델을 학습시키되, 제1 손실 함수를 최 소화하는 방향으로 상기 언어 모델을 사전 학습시키고, 사전 학습된 언어 모델에 상기 말뭉치와 상기 말뭉치에 레이블된 메타 정보를 학습용 데이터셋으로 입력해 상기 사전 학습된 언어 모델을 보강 학습시키되, 제2 손실 함수를 최소화하는 방향으로 상기 언어 모델을 보강 학습시키는 과정을 포함하고, 상기 제1 손실 함수(Lnll)와 상기 제2 손실함수(LScl)의 합(Ltotal)은 상기 제1 손실함수와 제2 손실함수가 트레이드오프(tradeoff)의 상관관계 를 갖는다. 본 발명의 또 다른 실시예에 따른 제어된 이야기를 생성하는 처리 장치는 속성이 제어된 이야기를 예측하는 언 어 모델을 저장하는 메모리, 상기 언어 모델을 언어 모델을 학습하는 프로그램을 실행하는 프로세서를 포함하고, 상기 프로세서에 의해 실행되는 프로그램은, 말뭉치를 상기 언어 모델에 입력해 상기 언어 모델을 학 습시키되, 제1 손실 함수를 최소화하는 방향으로 상기 언어 모델을 사전 학습시키고, 사전 학습된 언어 모델에 상기 말뭉치와 상기 말뭉치에 레이블된 메타 정보를 학습용 데이터셋으로 입력해 상기 사전 학습된 언어 모델을 보강 학습시키되, 제2 손실 함수를 최소화하는 방향으로 상기 언어 모델을 보강 학습시키는 과정을 포함하고,상기 제1 손실 함수(Lnll)와 상기 제2 손실함수(LScl)의 합(Ltotal)은 상기 제1 손실함수와 제2 손실함수가 트레이 드오프(tradeoff)의 상관관계를 갖다."}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들은, 언어 모델을 메타 정보가 레이블링된 데이터셋을 이용해 학습시켜 사용자가 제시하는 속 성에 맞춰 이야기를 만들 수 있다. 이를 통해 신인 작가들은 시스템을 새로운 이야기에 대한 아이디어를 얻는 용도로 활용할 수 있다. 또한 일반 작가들은 이야기에 대한 개괄적인 아이디어만 있다면 이 시스템을 스토리 생 성 보조 작가로서 활용하여 작성 과정의 시간을 단축할 수 있다."}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 도면을 참조하여 본 발명의 실시예들을 구체적으로 설명하도록 한다. 다만, 하기의 설명 및 첨부된 도면에서 본 발명의 요지를 흐릴 수 있는 공지 기능 또는 구성에 대한 상세한 설명은 생략한다. 덧붙여, 명세서 전체에서, 어떤 구성 요소를 '포함'한다는 것은, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것 이 아니라, 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 제 1, 제 2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로 사용될 수 있다. 예를 들어, 본 발명의 권리 범위로부터 이탈되지 않은 채 제 1 구성 요소는 제 2 구성 요소 로 명명될 수 있고, 유사하게 제 2 구성 요소도 제 1 구성 요소로 명명될 수 있다. 본 발명에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"구비하다\" 등의 용어는 설시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또 는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 특별히 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본"}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "발명이 속하는 기술분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미이다. 일반적 으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미인 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해 석되지 않는다. 도 1은 본 발명의 일 실시예에 따라 언어 모델을 학습시키는 방법을 설명하는 흐름도이다. 도 1에서와 같이, 본 발명의 일 실시예에 따른 언어 모델을 학습시키는 방법은, 언어 모델을 학습시키기 위한 학습용 말뭉치를 준비하는 단계(S100), 준비된 말뭉치를 이용해 언어 모델을 사전 학습시키는 단계(S200), 말뭉 치에 레이블된 메타 정보를 학습용 데이터셋으로 구성하는 단계(S300), 준비된 학습용 데이터셋을 이용해 사전 학습된 언어모델을 보강학습하는 단계(S400)를 포함한다. S100 단계는, 언어 모델을 학습시키기 위한 말뭉치를 준비하는 단계이다. 말뭉치는 언어 모델을 학습시키기 위 한 데이터이다. 본 발명의 일 예에서, 말뭉치는 영화 줄거리와 관련된 65,851개의 데이터를 수집해 구성하였다. 수집된 말뭉치는 토큰(token) 형태로 변환되어 언어 모델에 입력되고, 사전 학습된다. S200 단계는, 언어 모델을 사전 학습시키는 과정이다. 일 예에서, 언어 모델은 트랜스포머 디코더 기반의 언어 모델과 문장의 정보를 이해하는 인코더로 구성될 수 있다. 이 언어 모델은 학습 시엔 언어 모델과 인코더 모두사용하나 추론 시엔 언어 모델만을 사용한다. 이 언어 모델에 대해서는 추후 자세히 설명한다. 본 발명에서 언어 모델의 학습은 사전 학습과 보강 학습의 2단계로 진행된다. 사전 학습은 언어 모델에게 전반 적인 언어에 대한 이해를 학습시키기 위한 과정이고, 보강 학습은 사전 학습된 언어 모델이 속성을 가르키는 데 이터인 메타 정보를 제어하기 위해서 그에 맞는 추가적인 미세조정 과정을 거치는 과정이다. 언어 모델을 학습 시키는 구성은 2022년 4월 Web Conference 2022 학회에 발표된“Genre-Controllable Story Generation via Supervised Contrastive Learning”의 방식을 따르되, 메타 정보에는 장르뿐만 아니라 다양한 속성을 나타내는 데이터들이 활용될 수 있다. S300 단계는, 언어 모델을 2차적으로 보강 학습시키는 데이터 셋을 구성하는 과정이다. S200 단계가 언어 모델 이 언어 모델에게 전반적인 언어에 대한 이해를 학습시키기 위한 과정이라고 하면, 보강 학습은 입력된 속성에 기초해서 언어를 이해하는 미세 조정 과정이다. 언어 모델의 미세 조정을 위해서, 본 발명에서는 말뭉치에 메타정보가 레이블된 데이터셋을 이용한다. 여기서 메타정보는 이야기의 속성을 나태는 것으로, 이 속성은 이야기의 장르, 이야기의 시대적 배경처럼 이야기의 특 징을 알 수 있는 데이터를 포괄적으로 말한다. S400 단계에서, 메타정보가 레이블된 데이터셋은 언어 모델에 입력되고, 출력 y를 만든다. 출력된 y는 네가티브 -y와 함께 다시 언어 모델에 입력되고 출력을 반복함으로써 보강 학습이 이뤄진다. 본 발명에 따른 언어 모델의 아키텍쳐는 도 2와 같다. 본 발명에서 언어 모델은 트랜스포머 디코더 언어 모델을 기반으로 설계될 수 있다. 이 언어 모델은 입력으로 받은 메타 정보를 이용하여 해당 정보에 맞게 제어된 이야기를 생성하며, 또한 제어된 이야기를 추론할 수 있도 록 훈련된다. 언어 모델은 casual language modeling 기반 사전학습을 통해 시계열의 형태로 구성된 언어에 대한 전반적인 이 해를 진행하며, 사전학습은 negative log likelihood 손실 함수((Lnll)(이하, 제1 손실 함수)를 최소화하는 방향 으로 진행된다. 여기서, 제1 손실 함수는 아래의 [수식 1]과 같이 정의될 수 있다. [수식 1]"}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "D: 언어모델이 한번에 보는 데이터들 x(i): i번째 데이터(이야기)에 대한 각 단어들 t: 왼쪽부터 그 순서를 번호로 g(i):i 번째 데이터에 대한 장르(code) 이 제1 손실 함수는 i번째 데이터의 문장 내 단어 x를 왼쪽부터 하나하나 보면서, 지금까지 나온 단어들 x<t를 가지고 다음에 나올 x_t를 잘 예측하도록 한다. 본 발명에서, 인코더는 학습 과정에서만 사용되고, 추론 과정에서는 사용되지 않는다. 도 2에서, training only로 표시된 점선 블록은 언어 모델을 학습하는 과정에서의 아키텍쳐이다. 먼저 인코더를 통해 입력 문장과 이에 대해 증강된 데이터(z+), 그리고 입력 문장과 의미적으로 반대되는 문장()에 고차원적인 표현을 만든다. 그 뒤 supervised constrastive loss 손실 함수(Lscl)(이하, 제2 손실함수)를 최소화하는 방향으로 학습한다. 여기서, 제2 손실 함수는 아래의 [수식 2]와 같이 정의될 수 있다. [수식 2]"}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "P(i): i번째 데이터를 볼 때 주변 데이터 여러 개를 같이 봄을 정의하며, i번째 데이터가 해당하는 장르와 같은 장르인 데이터들을 같이 봄 exp: exponential 함수 z: 유사도 벡터 tau (τ): 훈련 강도를 조절하는 값 이 제2 손실 함수는 \"I\"라는 집합내 존재하는 데이터들 중, i번째 데이터와 같은 장르인 해당 데이터들이 i번째 데이터와 유사하도록, 그리고 i번째 데이터 자기자신을 증강해서 2개로 만들었는데 그 둘이 미리 설정해둔 타 장르 데이터와는 서로 다르도록 학습시킨다. 여기서, 데이터 간 유사하거나 다르다는 것은 z 벡터를 가지고 표 현된다. 이 제1 손실함수 및 제2 손실함수의 합은 [수식 3]과 같이 정의된다. [수식 3]"}
{"patent_id": "10-2023-0012797", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수식 3에서, \"λ\"는 하이퍼패러미터로 제1 손실함수와 제2 손실함수의 상관성이 트레이트오프(tradeoff)의 관계 를 가짐을 나타낸다. 즉, 언어 모델을 훈련시킬 때 제1 손실함수의 가중치를 높이면 언어 모델의 전반적인 언어 에 대한 이해도는 높일 수 있으나, 상대적으로 제1 손실함수의 가중치는 낮아져 메타 정보에 담긴 속성에 맞는 이야기에 대한 이해도는 낮아진다. 반대로 제1 손실함수의 가중치를 낮추면 속성에 맞는 이야기에 대한 추론은 높아지나 전반적인 언어에 대한 이해도는 낮아진다. 방법론적으로, 언어 모델이 학습되는 과정에 대해서 도 2를 가지고 간단히 설명하면 다음과 같다. 말뭉치와 이 말뭉치에 레이블된 메타정보를 갖는 데이터셋과 또한 비교학습을 위한 네가티브 데이터셋 역시도 디코더에 입력된다. 여기서 네가티브 데이터셋은 네가티브 말뭉치, 네가티브 말뭉치에 레이블된 메 타 정보, Pad 토큰을 포함한다. 여기서 네가티브 말뭉치는 데이터셋의 준비과정에서 같이 만들어지며, 네가티브 말뭉치에 레이블되는 메타 정보는 데이터셋과 대비해 유사하지만 다른 속성을 갖는다. 또한, 메타정보는 각 이 야기에 대한 속성을 표현한 것이고, 네가티브 데이터셋은 특정 속성의 관점에서 반대되는 속성을 지닌 이야기로 구성된 데이터셋이다. 트랜스포머 디코더에서는 \"h\" 라는 768 차원의 벡터를 만들며, 하나의 데이터 셋 입력으로부터 증강된 h 2 개가 출력된다. 벡터 \"h\"들은 인코더를 통해 z 벡터들로 변환되며. h는 단순히 텍스트 형태의 문장을 인공지능이 언어로서 이해하기 위해 만들어진 표현이었다면, z는 “장르” 또는 다른 메타정보에 대한 표현이다. 이처럼 학습된 모델의 성능은 자체적으로 평가한 Label fidelity 평가 지표에서 타 방식 대비 0.4 이상 높은 f1-score, (p value < 0.01)를 기록했다. 그 외에도 Amazon Mechanical Turk를 활용한 사람 평가에서도 타 방 식보다 통계적으로 유의한 성능의 개선이 이루어졌다. (Label fidelity 지표, kappa 평가자 간 신뢰도 기준) 도 3은 소설에서 장르가 제어된 채로 이야기를 생성한 화면을 예시한다. 도 3에서와 같이, 메타 정보로 \"멜로로맨스\"가 주어지는 경우에, 학습된 언어 모델은 학습된 바에 따라서 이야 기를 멜로로맨스라는 속성에 맞춰 생성한다. 그리고, 메타 정보로 \"스릴러\"가 주어진 경우에, 학습된 언어 모델 은 이야기를 스릴러라는 속성에 맞춰 생성할 수 있다. 도 3에서, (A)는 멜로로맨스 장르에 맞춰 이야기가 만들어진 경우를, (B)는 스릴러 장르로 이야기가 만들어진 경우를 보여준다.도 4는 본 발명의 다른 실시예에 따른 언어 모델에 기반해 제어된 이야기를 만드는 과정을 도시한 흐름도이다 언어 모델에 기반해 제어된 이야기를 만들기 위한 일련의 데이터 과정은 이하에서 기술되는 동작을 수행하는 명 령을 포함하는 프로그램으로 구현되어 적어도 하나의 프로세서를 구비하는 연산 장치를 통해 실행될 수 있다. S1000 단계에서, 언어 모델은 메타 정보가 레이블된 데이터셋을 이용해서 학습된다. 이러한 언어 모델은 말뭉치 와 말뭉치에 레이블된 메타 정보로 구성된 입력 데이터셋을 병렬 데이터로서 입력받고, 학습된다. 출력은 제1 손실함수와 제2 손실함수의 합인 총 손실값이 최소가 되는 방향으로 손실값이 산출되고, 그 결과는 언어 모델은 다시 학습시키기 위한 데이터셋으로 사용된다. S2000 단계에서, 학습된 언어 모델을 갖는 처리 장치는 일련의 데이터들을 사용자로부터 입력받는데, 입력 데이 터는 적어도 만들어지는 이야기의 속성을 포함한다. 예를 들어서, 사용자가 판타지 소설을 만들고자 한다면, 적 어도 입력 데이터는 판타지라는 속성을 포함한다. S3000 단계에서, 처리 장치는 입력된 기초 텍스트 데이터로부터 미리 학습된 언어 모델을 이용하여 입력된 기초 텍스트 데이터를 근거로 이야기를 추론한다. 도 5는 제어된 이야기를 추론하고 학습하는 처리 장치를 도시한 블록도로서, 도 1의 일련의 처리 과정을 하드웨어 구성의 관점에서 재구성한 것이다. 따라서, 여기서는 설명의 중복을 피하고자 각 구성의 기능 및 동작 을 중심으로 그 개요만을 약술하도록 한다. 메모리는 속성에 따라 제어된 이야기를 만드는 언어 모델을 저장하는 구성이다. 프로세서는 상기 언어 모델을 이용하여 상기 언어 모델을 학습하는 프로그램을 실행하는 구성이 다. 여기서, 프로세서에 의해 실행되는 프로그램은, 말뭉치를 상기 언어 모델에 입력해 상기 언어 모델을 학습시키되, 제1 손실 함수를 최소화하는 방향으로 상기 언어 모델을 사전 학습시키고, 사전 학습된 언어 모델 에 상기 말뭉치와 상기 말뭉치에 레이블된 메타 정보를 학습용 데이터셋으로 입력해 상기 사전 학습된 언어 모 델을 보강 학습시키되, 제2 손실 함수를 최소화하는 방향으로 상기 언어 모델을 보강 학습시키는 과정을 포함하 며, 제1 손실 함수(Lnll)와 상기 제2 손실함수(LScl)의 합(Ltotal)은 상기 제1 손실함수와 제2 손실함수가 트레이드 오프(tradeoff)의 상관관계를 갖도록 언어 모델을 학습시키는 명령을 포함한다. 한편, 본 발명의 실시예들은 컴퓨터로 읽을 수 있는 기록 매체에 컴퓨터가 읽을 수 있는 코드로 구현하는 것이 가능하다. 컴퓨터가 읽을 수 있는 기록 매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록 장치를 포함한다. 컴퓨터가 읽을 수 있는 기록 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 데이터 저장장치 등을 포함한다. 또한, 컴퓨터가 읽을 수 있는 기록 매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산 방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 그리고 본 발명을 구현하기 위한 기능적인 (functional) 프로그램, 코드 및 코드 세그먼트들은 본 발명이 속하는 기술 분야의 프로그래머들에 의하여 용이 하게 추론될 수 있다. 이상에서 본 발명에 대하여 그 다양한 실시예들을 중심으로 살펴보았다. 본 발명에 속하는 기술 분야에서 통상 의 지식을 가진 자는 본 발명이 본 발명의 본질적인 특성에서 벗어나지 않는 범위에서 변형된 형태로 구현될 수 있음을 이해할 수 있을 것이다. 그러므로 개시된 실시예들은 한정적인 관점이 아니라 설명적인 관점에서 고려되 어야 한다. 본 발명의 범위는 전술한 설명이 아니라 특허청구범위에 나타나 있으며, 그와 동등한 범위 내에 있 는 모든 차이점은 본 발명에 포함된 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5"}
{"patent_id": "10-2023-0012797", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따라 언어 모델을 학습시키는 방법을 설명하는 흐름도이다. 도 2는 언어 모델의 아키텍쳐를 보여준다. 도 3은 소설에서 장르가 제어된 채로 이야기를 생성한 화면을 예시한다. 도 4는 본 발명의 다른 실시예에 따른 언어 모델에 기반해 제어된 이야기를 만드는 과정을 도시한 흐름도이다. 도 5는 제어된 이야기를 추론하고 학습하는 처리 장치를 도시한 블록도이다."}
