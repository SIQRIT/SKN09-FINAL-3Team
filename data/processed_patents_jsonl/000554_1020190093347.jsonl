{"patent_id": "10-2019-0093347", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0015064", "출원번호": "10-2019-0093347", "발명의 명칭": "전자장치와 그의 제어방법, 및 기록매체", "출원인": "삼성전자주식회사", "발명자": "김형래"}}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자장치에 있어서,제1컨텐츠의 제1언어 가사를 번역한 제2언어 가사의 정보를 획득하고,상기 제2언어 가사를 발화하는 제1음성신호를 획득하고,상기 제1언어 가사에 포함된 제1구절에 대응하는 상기 제2언어 가사의 제2구절을 식별하고,상기 제1컨텐츠에서 상기 제1구절의 구간과, 상기 제1음성신호에서 상기 제2구절의 구간 간의 시간을 매칭시켜,상기 제1음성신호로부터 변환된 제2음성신호를 획득하고,상기 제2음성신호에 기초하여 상기 제1컨텐츠의 제1언어 노래를 제2언어 노래로 대체한 제2컨텐츠를 획득하는프로세서를 포함하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 프로세서는,상기 제1구간의 전체 시간을 상기 제2구간의 총 음절 수로 나눈 시간을 상기 제2구간의 각 음절의 시간을 형성하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 프로세서는,상기 제2구간의 각 음절에 대응하는 시간을 상기 제1구간의 각 음절의 시간을 반영하여 형성하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 제1구간 및 제2구간은 문장, 절, 또는 구 중 적어도 하나를 포함하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 프로세서는 딥 뉴럴 네트워크(DNN, Deep neural networks)의 생성적 적대 신경망(GAN, GenerativeAdversarial Network)을 이용하여 제2언어가사 기반의 제2음성신호를 생성하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 프로세서는,상기 제1컨텐츠를 원곡음성신호와 원곡음악신호로 분리하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 프로세서는,공개특허 10-2021-0015064-3-상기 원곡음성신호로부터 상기 제1언어가사의 구절의 제1구간에 대한 음성의 특징을 추출하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 음성의 특징은 음의 크기, 음의 피치, 또는 음의 톤 중 적어도 하나를 포함하며,상기 제2음성신호는 상기 원곡음성신호의 음성의 특징을 반영하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서,상기 프로세서는,상기 분리된 원곡음악신호의 선명도와 해상도를 보정하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 프로세서는,상기 제2음성신호와 상기 보정된 원곡음악신호를 합성하여 상기 제2컨텐츠를 획득하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제5항에 있어서,상기 프로세서는,상기 제2음성신호를 MR음악에 합성하여 제2컨텐츠를 획득하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,외부장치로부터 상기 MR음악를 수신하는 신호입출력부를 더 포함하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제6항에 있어서,상기 제1언어 가사는,상기 음성분리기에 의해 상기 원곡음성신호로부터 분리되는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제5항에 있어서,외부장치로부터 상기 제1언어 가사를 수신하는 신호입출력부를 더 포함하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1항에 있어서,외부장치로부터 상기 제1컨텐츠를 수신하는 신호입출력부를 더 포함하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제1항에 있어서,상기 제1컨텐츠가 입력되는 마이크를 더 포함하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "공개특허 10-2021-0015064-4-제1항에 있어서,상기 제2컨텐츠를 출력하는 음성출력부를 더 포함하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제1항에 있어서,상기 제2언어 가사를 표시하는 디스플레이를 더 포함하는 전자장치."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "전자장치의 제어방법에 있어서,제1컨텐츠의 제1언어 가사를 번역한 제2언어 가사의 정보를 획득하는 단계와;상기 제2언어 가사를 발화하는 제1음성신호를 획득하는 단계와;상기 제1언어 가사에 포함된 제1구절에 대응하는 상기 제2언어 가사의 제2구절을 식별하는 단계와;상기 제1컨텐츠에서 상기 제1구절의 구간과, 상기 제1음성신호에서 상기 제2구절의 구간 간의 시간을 매칭시켜,상기 제1음성신호로부터 변환된 제2음성신호를 획득하는 단계와;상기 제2음성신호에 기초하여 상기 제1컨텐츠의 제1언어 노래를 제2언어 노래로 대체한 제2컨텐츠를 획득하는단계를 포함하는 전자장치의 제어방법."}
{"patent_id": "10-2019-0093347", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "컴퓨터에 의해 실행되는 컴퓨터 프로그램이 저장되는, 컴퓨터 판독 가능 기록매체에 있어서,상기 컴퓨터 프로그램은,제1컨텐츠의 제1언어 가사를 번역한 제2언어 가사의 정보를 획득하고,상기 제2언어 가사를 발화하는 제1음성신호를 획득하고,상기 제1언어 가사에 포함된 제1구절에 대응하는 상기 제2언어 가사의 제2구절을 식별하고,상기 제1컨텐츠에서 상기 제1구절의 구간과, 상기 제1음성신호에서 상기 제2구절의 구간 간의 시간을 매칭시켜,상기 제1음성신호로부터 변환된 제2음성신호를 획득하고,상기 제2음성신호에 기초하여 상기 제1컨텐츠의 제1언어 노래를 제2언어 노래로 대체한 제2컨텐츠를 획득하는동작을 수행하는 컴퓨터 판독가능 기록매체."}
{"patent_id": "10-2019-0093347", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "제1컨텐츠를 다른 언어 기반의 제2컨텐츠로 변환할 수 있는 전자장치가 개시된다. 전자장치는 제1컨텐츠의 제1언 어 가사를 번역한 제2언어 가사의 정보를 획득하고, 상기 제2언어 가사를 발화하는 제1음성신호를 획득하고, 상 기 제1언어 가사에 포함된 제1구절에 대응하는 상기 제2언어 가사의 제2구절을 식별하고, 상기 제1컨텐츠에서 상 기 제1구절의 구간과, 상기 제1음성신호에서 상기 제2구절의 구간 간의 시간을 매칭시켜, 상기 제1음성신호로부 터 변환된 제2음성신호를 획득하고, 상기 제2음성신호에 기초하여 상기 제1컨텐츠의 제1언어 노래를 제2언어 노 래로 대체한 제2컨텐츠를 획득하는 프로세서를 포함한다."}
{"patent_id": "10-2019-0093347", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 원곡의 가수가 부른 노래와 유사한 느낌을 갖는 다른 언어의 노래를 생성할 수 있는 전자장치와 그의 제어방법, 및 저장매체에 관한 것이다."}
{"patent_id": "10-2019-0093347", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "유명한 노래는 다른 언어의 가사의 번역곡으로 다양한 나라에 보급되고 있다. 이러한 번역곡은 오리지널 노래의 가수가 다른 언어로 번역된 번역가사로 불러야 오리지널 노래 느낌이 날 수 있다. 그러나, 오리지널 노래의 가수가 이미 세상에 없거나, 부르기를 거부하는 경우, 다른 가수가 번역가사로 부른 커버곡 형태의 노래는 오리지널 노래의 느낌이 나지 않는다."}
{"patent_id": "10-2019-0093347", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은, 오리지널 노래 가수가 부른 노래와 유사한 다른 언어 기반의 번역곡을 생성할 수 있는 전자 장치와 그의 제어방법, 및 컴퓨터 프로그램이 저장된 기록매체를 제공하는 데에 있다."}
{"patent_id": "10-2019-0093347", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 전자장치가 제공된다. 전자장치는, 제1컨텐츠의 제1언 어 가사를 번역한 제2언어 가사의 정보를 획득하고, 상기 제2언어 가사를 발화하는 제1음성신호를 획득하고, 상 기 제1언어 가사에 포함된 제1구절에 대응하는 상기 제2언어 가사의 제2구절을 식별하고, 상기 제1컨텐츠에서 상기 제1구절의 구간과, 상기 제1음성신호에서 상기 제2구절의 구간 간의 시간을 매칭시켜, 상기 제1음성신호로 부터 변환된 제2음성신호를 획득하고, 상기 제2음성신호에 기초하여 상기 제1컨텐츠의 제1언어 노래를 제2언어 노래로 대체한 제2컨텐츠를 획득하는 프로세서를 포함한다. 상기 프로세서는, 상기 제1구간의 전체 시간을 상기 제2구간의 총 음절 수로 나눈 시간을 상기 제2구간의 각 음 절의 시간을 형성할 수 있다. 상기 프로세서는, 상기 제2구간의 각 음절에 대응하는 시간을 상기 제1구간의 각 음절의 시간을 반영하여 형성 할 수 있다. 상기 제1구간 및 제2구간은 문장, 절, 또는 구 중 적어도 하나를 포함할 수 있다. 상기 프로세서는 딥 뉴럴 네트워크(DNN, Deep neural networks)의 생성적 적대 신경망(GAN, Generative Adversarial Network)을 이용하여 제2언어가사 기반의 제2음성신호를 생성할 수 있다. 상기 프로세서는, 상기 제1컨텐츠를 원곡음성신호와 원곡음악신호로 분리할 수 있다. 상기 프로세서는, 상기 원곡음성신호로부터 상기 제1언어가사의 구절의 제1구간에 대한 음성의 특징을 추출할 수 있다. 상기 음성의 특징은 음의 크기, 음의 피치, 또는 음의 톤 중 적어도 하나를 포함하며, 상기 제2음성신호는 상기 원곡음성신호의 음성의 특징을 반영할 수 있다. 상기 프로세서는, 상기 분리된 원곡음악신호의 선명도와 해상도를 보정할 수 있다. 상기 프로세서는, 상기 제2음성신호와 상기 보정된 원곡음악신호를 합성하여 상기 제2컨텐츠를 획득할 수 있다. 상기 프로세서는, 상기 제2음성신호를 MR음악에 합성하여 제2컨텐츠를 획득할 수 있다. 전자장치는 외부장치로부터 상기 MR음악를 수신하는 신호입출력부를 더 포함할 수 있다. 상기 제1언어 가사는, 상기 음성분리기에 의해 상기 원곡음성신호로부터 분리될 수 있다. 전자장치는 외부장치로부터 상기 제1언어 가사를 수신하는 신호입출력부를 더 포함할 수 있다. 전자장치는 외부장치로부터 상기 제1컨텐츠를 수신하는 신호입출력부를 더 포함할 수 있다. 전자장치는 상기 제1컨텐츠가 입력되는 마이크를 더 포함할 수 있다. 전자장치는 상기 제2컨텐츠를 출력하는 음성출력부를 더 포함할 수 있다. 전자장치는 상기 제2언어 가사를 표시하는 디스플레이를 더 포함할 수 있다. 본 발명의 실시예에 따른 전자장치의 제어방법은, 제1컨텐츠의 제1언어 가사를 번역한 제2언어 가사의 정보를 획득하는 단계와, 상기 제2언어 가사를 발화하는 제1음성신호를 획득하는 단계와, 상기 제1언어 가사에 포함된 제1구절에 대응하는 상기 제2언어 가사의 제2구절을 식별하는 단계와, 상기 제1컨텐츠에서 상기 제1구절의 구간 과, 상기 제1음성신호에서 상기 제2구절의 구간 간의 시간을 매칭시켜, 상기 제1음성신호로부터 변환된 제2음성 신호를 획득하는 단계와, 상기 제2음성신호에 기초하여 상기 제1컨텐츠의 제1언어 노래를 제2언어 노래로 대체 한 제2컨텐츠를 획득하는 단계를 포함할 수 있다. 본 발명의 실시예에 따른, 컴퓨터에 의해 실행되는 컴퓨터 프로그램이 저장되는, 컴퓨터 판독 가능 기록매체에 있어서, 상기 컴퓨터 프로그램은 제1컨텐츠의 제1언어 가사를 번역한 제2언어 가사의 정보를 획득하고, 상기 제 2언어 가사를 발화하는 제1음성신호를 획득하고, 상기 제1언어 가사에 포함된 제1구절에 대응하는 상기 제2언어 가사의 제2구절을 식별하고, 상기 제1컨텐츠에서 상기 제1구절의 구간과, 상기 제1음성신호에서 상기 제2구절의 구간 간의 시간을 매칭시켜, 상기 제1음성신호로부터 변환된 제2음성신호를 획득하고, 상기 제2음성신호에 기초하여 상기 제1컨텐츠의 제1언어 노래를 제2언어 노래로 대체한 제2컨텐츠를 획득하는 동작을 수행할 수 있다."}
{"patent_id": "10-2019-0093347", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의한 전자장치는 오리지널 노래의 가사를 다른 언어로 번역한 가사를 오리지널 가수가 부르지 않더라 도 오리지널 가수의 음성에 가까운 느낌이 나는 번역곡으로 만들 수 있다. 또한, 전자장치는 오리지널 노래의 가사를 다양한 언어로 번역한 가사를 기초로 오리지널 가수가 부른 느낌이 나는 다른 언어 기반의 노래들을 기 계적으로 쉽게 만들 수 있다. 이와 같이 기계적으로 만든 다른 언어 기반의 노래들은 오리지널 가수가 부르지 않더라도 세계 각국에 널리 보급할 수 있다."}
{"patent_id": "10-2019-0093347", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부 도면을 참조하여 본 발명의 실시예들을 상세히 설명한다. 도면에서 동일한 참조번호 또는 부호 는 실질적으로 동일한 기능을 수행하는 구성요소를 지칭하며, 도면에서 각 구성요소의 크기는 설명의 명료성과 편의를 위해 과장되어 있을 수 있다. 다만, 본 발명의 기술적 사상과 그 핵심 구성 및 작용이 이하의 실시예에 설명된 구성 또는 작용으로만 한정되지는 않는다. 본 발명을 설명함에 있어서 본 발명과 관련된 공지 기술 또는 구성에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명 을 생략하기로 한다. 본 문서에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. 본 문서에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\"등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 발명의 실시예에서, 제1, 제2 등과 같이 서수를 포함하는 용어는 하나의 구성요소를 다른 구성요소로부터 구 별하는 목적으로만 사용되며, 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 또한, 본 발명의 실시예에서 '상부', '하부', '좌측', '우측', '내측', '외측', '내면', '외면', '전방', '후방' 등의 용어는 도면을 기준으로 정의한 것이며, 이에 의해 각 구성요소의 형상이나 위치가 제한되는 것은 아니다. 본 문서에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적 합한(suitable for),\" \"~하는 능력을 가지는(having the capacity to),\" \"~하도록 설계된(designed to),\" \"~하 도록 변경된(adapted to),\" \"~하도록 만들어진(made to),\" 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장 치가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행 하도록 구성된(또는 설정된) 서브 프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세 서), 또는 메모리 장치에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 본 문서의 다양한 실시예들에 따른 전자 장치는, 다양한 종류의 컨텐츠를 공급받는 전자기기, 예를 들면, 스마트폰, 태블릿 PC, 데스크탑 PC, 랩탑 PC, 넷북 컴퓨터, 워크스테이션, 서버, PDA, PMP(portable multimedia player), MP3 플레이어, 또는 웨어러블 장치 중 적어도 하나를 포함할 수 있다. 어떤 실시예들에서, 전자 장치는, 예를 들면, 노래방기기, 텔레비전, DVD(digital video disk) 플레이어, 오디오, 셋톱 박스, 또는 전자 액자 중 적어도 하나를 포함할 수 있다. 본 문서에서, 사용자라는 용어는 전자장치를 사용하는 사람 또는 전자장치를 사용하는 장치(예: 인공지 능 전자 장치)를 지칭할 수 있다. 도 1은 본 발명의 제1실시예에 따른 전자장치를 도시한다. 전자장치는 특정 컨텐츠 공급자로부터 컨텐 츠, 예를 들면 제1컨텐츠를 공급받을 수 있다. 제1컨텐츠는 노래, 뮤직비디오, 영화, 드라마 등을 포함할 수 있 다. 예를 들면 전자장치는 셋탑박스와 같은 컨텐츠공급장치로부터 또는 네트워크를 통해 서버로부터 스 트리밍으로 제1컨텐츠를 공급받고, 리모컨으로부터 수신된 리모컨신호에 의해 제어될 수 있는 텔레비전(T V)으로 구현될 수 있다. 물론, 전자장치는 텔레비전으로만 한정되지 않고, 컨텐츠 공급자들이 제공하는 다 양한 종류의 컨텐츠를 사용하는 다양한 전자기기로 구현될 수 있다. 도 1에 도시된 바와 같이, 전자장치는 컨텐츠 공급장치나 서버를 통해 또는 사용자로부터 마이크를 통 해 직접적으로 제1컨텐츠를 수신할 수 있다. 전자장치는 제1컨텐츠의 제1언어 가사를 외부장치로부터 수신하거나 제1컨텐츠에서 직접 추출할 수 있다. 전자장치는 제1언어 가사를 제2언어 가사로 번역하거나 이미 번역된 상태의 제2언어 가사를 수신할 수 있다. 전자장치는 번역된 제2언어가사를 기초로 제1컨텐츠의 오리지널 가수가 부른 음성과 유사한 느낌이 나는 음 성신호를 생성할 수 있다. 음성신호의 생성 방법은 나중에 상세하게 설명한다. 전자장치는 번역된 제2언어가사의 음성신호와 MR(Music Recorded)을 합성하여 제2언어 기반의 제2컨텐츠를 생성할 수 있다. MR은 제1컨텐츠로부터 추출되거나 별도로 제작된 상태로 저장된 또는 외부장치로부터 수신할 수 있다. 컨텐츠공급장치는 요청에 따라 컨텐츠공급자가 제공하는 제1컨텐츠, 영상컨텐츠 및/또는 EPG UI를 전자장치 에 전송할 수 있다. 컨텐츠공급장치는 각 컨텐츠공급자가 제공하는 셋탑박스, 방송신호를 송출하는 방 송국, 케이블을 통해 컨텐츠를 공급하는 케이블방송국, 인터넷을 통해 미디어를 공급하는 미디어 서버 등을 포 함할 수 있다. 서버는 제1컨텐츠, 제1컨텐츠의 제1언어 가사, 제1컨텐츠의 전역된 제2언어 가사, MR 등을 전자장치에 제공할 수 있다. 서버는 하나 이상의 서버로 구현될 수 있다. 도 2는 도 1의 전자장치의 구성을 나타내는 블록도이고, 도 3은 도 1의 메모리의 구성을 나타내는 블록 도이이다. 전자장치는 제1컨텐츠를 다른 언어 기반의 제2컨텐츠로 생성할 수 있는 프로세서를 포함한다. 또한, 전 자장치는 신호입출력부, 마이크, 메모리, 음성인식부, 디스플레이, 및 스피커를 포함할 수 있다. 신호입출력부는 신호수신부와 리모컨신호 송수신부를 포함할 수 있다. 신호수신부는 공중파 방송국, 케이블방송국, 미디어방송국 등으로부터 컨텐츠신호를 수신할 수 있다. 신호 수신부는 셋탑박스와 같은 전용의 컨텐츠공급장치 또는 스마트폰과 같은 모바일 단말기로부터 컨텐츠 신호를 수신할 수 있다. 신호수신부가 수신하는 컨텐츠신호는 유선신호 또는 무선신호일 수 있고, 디지털 신호 또는 아날로그신호일 수도 있다. 컨텐츠신호는 공중파 신호, 케이블신호, 위성신호 또는 네트워크신호일수도 있다. 신호수신부는 USB 메모리의 접속을 위한 USB 포트 등을 추가적으로 포함할 수 있다. 신호수신 부는 영상/음성 신호를 동시에 수신할 수 있는 포트인 HDMI, DP, 썬더볼트 등으로 구현될 수 있다. 물론, 신호수신부는 영상/음성 신호를 수신하는 입력포트를 포함할 수도 있다. 또한, 영상과 음성 신호는 함께 또는 독립적으로 수신될 수도 있다. 신호수신부는 프로세서의 제어에 따라 복수의 채널 중 어느 하나의 채널신호를 수신할 수 있다. 채널 신호에는 컨텐츠 공급자가 제공하는 AV컨텐츠 및/또는 EPG UI가 실려 있다. AV컨텐츠는 드라마, 영화, 뉴스, 스 포츠, 음악, VOD 등 다양한 방송 프로그램을 포함하며, 그 내용의 제한은 없다. 신호수신부는 컨텐츠공급장치, 서버, 또는 그 밖의 다른 장치들과 네트워크 통신을 수행할 수 있 다. 신호수신부는 무선 통신을 수행하기 위해 RF(Radio Frequency)신호를 송/수신하는 RF회로를 포함할 수 있으며, Wi-fi, 블루투스, 지그비(Zigbee), UWB(Ultra-Wide Band), Wireless USB, NFC(Near Field Communication) 중 하나 이상의 통신을 수행하도록 구성될 수 있다. 신호수신부는 유선 LAN(Local Area Network)을 통해 유선 통신을 수행할 수 있다. 유선 접속을 위한 커넥터 또는 단자를 포함하는 접속부 이외에도 다양한 다른 통신 방식으로 구현될 수 있다. 리모컨신호송수신부는 리모컨으로부터 리모컨신호, 예를 들면 IR신호, 블루투스신호, 와이파이신호 등 을 수신한다. 또한, 리모컨신호송수신부는 컨텐츠공급장치와 같은 외부장치를 제어하기 위한 명령정보 를 포함하는 IR신호, 블루투스신호, 와이파이신호 등을 전송할 수 있다. 전자장치는 컨텐츠공급장치, 서버, 리모컨 각각에 대해 전용으로 통신을 수행하는 전용통신모 듈을 포함할 수 있다. 예를 들면 컨텐츠공급장치는 HDMI모듈, 서버는 이더넷 모뎀이나 와이파이 모듈, 리모컨은 블루투스 모듈이나 IR모듈을 통해 통신을 수행할 수 있다. 전자장치는 컨텐츠공급장치, 서버, 리모컨 모두와 통신을 수행하는 공용통신모듈 등을 포함할 수 있다. 예를 들면 컨텐츠공급장치, 서버, 리모컨은 와이파이모듈을 통해 통신을 수행할 수 있다. 전자장치는 컨텐츠신호 수신부외에, 컨텐츠신호를 외부로 출력하는 컨텐츠신호 출력부를 더 포함할 수 있다. 이때, 컨텐츠신호 수신부와 컨텐츠신호 출력부는 하나의 모듈로 통합되거나 별도의 모듈로 구현될 수도 있다. 마이크는 사용자의 음성, 예를 들면 노래를 수신할 수 있다. 사용자의 음성은 마이크 이외의 다른 경로 를 통해서도 수신될 수 있다. 예컨대, 사용자의 음성은, 마이크를 구비한 리모컨이나, 스마트폰과 같은 사 용자의 다른 단말기기 등을 통해 수신될 수 있으나, 이에 제한되는 것은 아니다. 리모컨 혹은 다른 단말기 기 등에 의해 수신된 사용자의 음성은, 앞서 설명된 바와 같은, 노래 또는 전자장치의 제어를 위한 다양한 음성 명령을 포함할 수 있다. 메모리는 컴퓨터에 의해 판독 가능한 기록매체로서, 한정되지 않은 데이터가 저장된다. 메모리는 프로 세서에 의해 액세스 되며, 이들에 의한 데이터의 독취, 기록, 수정, 삭제, 갱신 등이 수행된다. 메모리(1 3)에 저장되는 데이터는, 예를 들면 제1컨텐츠, 영상컨텐츠, MR컨텐츠, 제1컨텐츠의 가사, 제1컨텐츠의 번역가 사, 제1컨텐츠의 다른 언어기반인 제2컨텐츠 등을 포함할 수 있다. 메모리는 도 3에 나타낸 바와 같이, 프로세서가 실행할 수 있는, 제1컨텐츠로부터 원곡음악신호와 원곡 음성신호로 분리할 수 있는 음성분리모듈, 분리된 원곡음악신호의 선명도와 해상도를 보정하는 음악보정모 듈, 분리된 원곡음성신호로부터 음성의 특징, 예를 들면 음의 크기, 음의 피치, 음의 톤 등을 구절의 구간 별로 추출하는 음성특징추출모듈, 분리된 원곡음성신호의 제1언어가사를 다른 제2언어가사로 번역하는 번 역모듈, 원곡음성신호의 음성의 특징을 번역된 제2언어가사에 적용하여 신규음성신호를 생성하는 음성생성 모듈, 및 신규음성신호를 보정된 원곡음악신호에 합성하여 제2컨텐츠를 생성하는 노래합성모듈을 포 함할 수 있다. 메모리는 수신된 음성을 인식하는 음성인식모듈(음성인식엔진)을 포함할 수 있다. 물론, 메모리는 운영 체제, 운영체제 상에서 실행 가능한 다양한 애플리케이션, 영상데이터, 부가데이터 등을 포함할 수 있다. 메모리는 제어프로그램이 설치되는 비휘발성의 메모리, 설치된 제어프로그램의 적어도 일부가 로드되는 휘 발성의 메모리를 포함한다. 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마 이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM,Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory) 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 음성인식부는 메모리에 저장된 음성인식모듈(음성인식엔진)을 실행하여 마이크, 리모컨 등을 통해 수신된 사용자의 음성을 인식할 수 있다. 음성인식부는 컨텐츠 또는 전자장치를 제어하기 위한 음 성명령 등을 인식한다. 컨텐츠는 예컨대, 사용자가 직접 부른 컨텐츠 또는 오리지널 노래 가수를 포함한 제3자 가 부른 컨텐츠일 수 있다. 음성명령은, 예컨대, 전자장치의 온 또는 오프, 채널 전환, 볼륨 조절 등의 명 령을 포함할 수 있다. 또한, 제어명령은, 예컨대, 전자장치에 연결된 컨텐츠공급장치가 제공하는 UI를 표시할 것을 요청하는 명령일 수 있다. 리모컨 마이크로부터 수신된 아날로그 음성신호는 디지털신호로 변환되어 예를 들면 블루투스 등을 통해 전 자장치로 전송될 수 있다. 또는 전자장치 자체에 내장된 마이크로 수신된 음성신호는 아날로그 음 성신호를 디지털신호로 변환하여 전자장치의 프로세서로 전송될 수 있다. 이와 같이, 수신된 음성신호 는 전자장치의 내장된 음성인식부를 통해 텍스트로 변환할 수 있다. 음성인식부에 의해 변환된 텍 스트는 컨텐츠의 가사일 수 있다. 음성인식부는 전자장치에서 배제될 수 있다. 이때, 수신된 음성신호는 서버(음성인식서버)로 전송 될 수 있다. 서버(음성인식서버)는 음성신호 관련 데이터를 적절한 텍스트로 변환하는 기능만을 가진 STT(Speech To Text)서버이거나 STT서버 기능도 함께 수행하는 메인 서버일 수도 있다. STT서버는 처리된 데이터를 전자장치에 다시 전송하거나 다른 서버로 바로 전송할 수도 있다. 상술한 바와 같이, 전자장치의 프로세서는 서버에 전송된 텍스트나 전자장치의 음성인식부에서 자체적으로 변환한 텍스트를 이용하여 특정 기능을 수행할 수 있다. 이때, 특정 기능의 수행은 변환된 텍스트를 별도 서버(STT서버와 다른 서버 또는 STT서버 역할도 하는 서버)에 전송하여 데이터 처리한 후 전자장치로 전송한 정보/데이터를 기반으로 이루어질 수도 있다 디스플레이는 신호 처리된 영상신호에 기초하여 영상을 표시한다. 디스플레이는 메모리에 저장된 또는 신호입출력부를 통해 컨텐츠 공급장치 또는 서버로부터 수신한 디지털 컨텐츠, 예를 들면 원 곡의 제1언어가사 및/또는 번역된 제2언어가사를 표시할 수 있다. 디스플레이의 구현 방식은 한정되지 않는 바, 액정(liquid crystal), 플라즈마(plasma), 발광 다이오드 (light-emitting diode), 유기발광 다이오드(organic light-emitting diode), 면전도 전자총(surface- conduction electron-emitter), 탄소 나노 튜브(carbon nano-tube), 나노 크리스탈(nano-crystal) 등의 다양한 디스플레이 패널로 구현될 수 있다. 디스플레이는 구현 방식에 따라서 부가적인 구성을 추가적으로 포함할 수 있다. 예를 들면, 디스플레이(1 5)는 LCD 패널, LCD패널을 구동시키는 LCD패널구동부, 및 LCD 패널에 광을 공급하는 백라이트 유닛을 포함할 수 있다. 스피커는 재생되는 제1컨텐츠나 제2컨텐츠의 사운드를 출력할 수 있다. 프로세서는 전자장치의 각 구성 부품을 제어할 수 있다. 프로세서는, 예컨대, 수신된 또는 저장된 제1컨텐츠를 다른 언어기반의 제2컨텐츠로 대체할 수 있다. 프로세서는 음성분리부는 메모리에 저장된 음성분리모듈을 실행하여 저장된 또는 수신된 제1 컨텐츠를 원곡음악신호와 원곡음성신호로 분리할 수 있다. 원곡음악신호는 제1컨텐츠에서 가수의 음성을 배제한 배경음악을 의미한다. 원곡음성신호는 제1컨텐츠에서 배경음악을 배제한 가수의 음성을 의미한다. 음성분리부 는 원곡음성신호로부터 STT(Speech To Text)를 수행하여 제1언어가사(원곡가사)를 획득하는 기능을 추가로 구비할 수 있다. 또는, 프로세서는 원곡음성신호로부터 제1언어가사를 획득하는 대신에, 외부로부터 제1언 어가사를 수신하여 획득할 수도 있다. 프로세서는 메모리에 저장된 음악보정모듈을 실행하여 분리된 원곡음악신호의 선명도 및 해상도를 보정할 수 있다. 프로세서는 메모리에 저장된 음성특징추출모듈을 실행하여 원곡음성신호로부터 가수의 음성이 가 진 음성의 특징, 예컨대 음의 크기, 음의 피치, 음의 톤을 구절의 구간(문장, 절, 또는 구) 별로 추출할 수 있 다. 프로세서는 메모리에 저장된 번역모듈을 실행하여 제1언어가사로부터 다른 언어의 제2언어가사를 획득할 수 있다. 여기서, 프로세서는 직접 번역하는 대신에 외부장치, 예컨대 번역서버 또는 번역된 가사를 저장하는 컨텐츠서버에 제2언어가사를 요청하여 획득할 수도 있다. 프로세서는 메모리에 저장된 음성생성모듈을 실행하여 제2언어가사를 기초로 제1음성신호를 획득 (생성)하고, 제1음성신호에 추출한 원곡음성신호의 특징, 예컨대 음의 크기, 음의 피치, 음의 톤을 적용하고, 제1언어가사에 포함된 제1구절에 대응하는 제2언어가사의 제2구절을 식별하고, 제1음성신호에서 제1구절의 구간 과 제2구절이 구간 간의 시간을 매칭시켜, 제1음성신호로부터 변환된 제2음성신호를 획득할 수 있다. 특히, 프로세서 는 제2언어가사 기반의 제2음성신호를 생성하기 위하여 상기한 적어도 하나의 동작을 수행 함에 있어서, 데이터 분석, 처리, 및 결과 정보 생성 중 적어도 일부를 규칙 기반 또는 인공지능(Artificial Intelligence) 알고리즘으로서 기계학습, 신경망 네트워크(neural network), 또는 딥러닝 알고리즘 중 적어도 하나를 이용하여 수행할 수 있다. 일 예로, 프로세서는 학습부 및 인식부의 기능을 수행할 수 있다. 학습부는, 예를 들면, 학습된 신경망 네 트워크를 생성하는 기능을 수행하고, 인식부는 학습된 신경망 네트워크를 이용하여 데이터를 인식(또는, 추론, 예측, 추정, 판단)하는 기능을 수행할 수 있다. 학습부는 신경망 네트워크를 생성하거나 갱신할 수 있다. 학습 부는 신경망 네트워크를 생성하기 위해서 학습 데이터를 획득할 수 있다. 예를 들면, 학습부는 학습 데이터를 메모리 또는 외부로부터 획득할 수 있다. 학습 데이터는, 신경망 네트워크의 학습을 위해 이용되는 데이터 일 수 있으며, 예를 들면, 원곡음성신호의 특징을 적용하거나, 제1언어가사에 포함된 제1구절에 대응하는 제2언 어가사의 제2구절을 식별하거나, 제1음성신호에서 제1구절의 구간과 제2구절이 구간 간의 시간을 매칭하는 동작 등을 학습 데이터로 이용하여 신경망 네트워크를 학습시킬 수 있다. 학습부는 학습 데이터를 이용하여 신경망 네트워크를 학습시키기 전에, 획득된 학습 데이터에 대하여 전처리 작 업을 수행하거나, 또는 복수 개의 학습 데이터들 중에서 학습에 이용될 데이터를 선별할 수 있다. 예를 들면, 학습부는 학습 데이터를 기 설정된 포맷으로 가공하거나, 필터링하거나, 또는 노이즈를 추가/제거하여 학습에 적절한 데이터의 형태로 가공할 수 있다. 학습부는 전처리된 학습 데이터를 이용하여 원곡음성신호의 특징을 적 용하는 동작 등을 수행하도록 설정된 신경망 네트워크를 생성할 수 있다. 학습된 신경망 네트워크는, 복수의 신경망 네트워크(또는, 레이어)들로 구성될 수 있다. 복수의 신경망 네트워 크의 노드들은 가중치를 가지며, 복수의 신경망 네트워크들은 일 신경망 네트워크의 출력 값이 다른 신경망 네 트워크의 입력 값으로 이용되도록 서로 연결될 수 있다. 신경망 네트워크의 예로는, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 및 심층 Q-네트 워크 (Deep Q-Networks)과 같은 모델을 포함할 수 있다. 한편 인식부는 원곡음성신호의 특징을 적용하는 동작 등을 수행하기 위해, 타겟 데이터를 획득할 수 있다. 타겟 데이터는 메모리 또는 외부로부터 획득된 것일 수 있다. 타겟 데이터는, 신경망 네트워크의 인식 대상이 되 는 데이터일 수 있다. 인식부는 타겟 데이터를 학습된 신경망 네트워크에 적용하기 전에, 획득된 타겟 데이터에 대하여 전처리 작업을 수행하거나, 또는 복수 개의 타겟 데이터들 중에서 인식에 이용될 데이터를 선별할 수 있 다. 예를 들면, 인식부는 타겟 데이터를 기 설정된 포맷으로 가공하거나, 필터링 하거나, 또는 노이즈를 추가/ 제거하여 인식에 적절한 데이터의 형태로 가공할 수 있다. 인식부는 전처리된 타겟 데이터를 신경망 네트워크에 적용함으로써, 신경망 네트워크로부터 출력되는 출력값을 획득할 수 있다. 다양한 실시예에 따르면, 인식부는 출력값과 함께 학률값(또는, 신뢰도값)을 함께 획득할 수 있다. 프로세서는 메모리에 저장된 노래합성모듈을 실행하여 보정된 원곡음악신호와 생성된 제2음성신호 를 합성하여 새로운 제2언어가사 기반의 제2컨텐츠를 획득할 수 있다. 프로세서는 제어프로그램이 설치된 비휘발성의 메모리로부터 제어프로그램의 적어도 일부를 휘발성의 메모 리로 로드하고, 로드된 제어프로그램을 실행하는 적어도 하나의 범용 프로세서를 포함하며, 예를 들면 CPU(Central Processing Unit), AP(application processor), 또는 마이크로프로세서(microprocessor)로 구현될 수 있다. 프로세서는 싱글 코어, 듀얼 코어, 트리플 코어, 쿼드 코어 및 그 배수의 코어를 포함할 수 있다. 프로세서 는 복수 개 마련될 수 있다. 프로세서는 예를 들어, 메인 프로세서(main processor) 및 슬립 모드 (sleep mode, 예를 들어, 대기 전원만 공급되는 모드)에서 동작하는 서브 프로세서(sub processor)를 포함할 수 있다. 또한, 프로세서, 롬 및 램은 내부 버스(bus)를 통해 상호 연결된다. 프로세서는 전자장치에 내장되는 PCB 상에 실장되는 메인 SoC(Main SoC)에 포함되는 형태로서 구현 가 능하다. 다른 실시예에서 메인 SoC는 영상처리부를 더 포함할 수 있다. 제어프로그램은, BIOS, 디바이스드라이버, 운영체계, 펌웨어, 플랫폼 및 응용프로그램(어플리케이션) 중 적어도 하나의 형태로 구현되는 프로그램(들)을 포함할 수 있다. 응용프로그램은, 전자장치의 제조 시에 미리 설치 또는 저장되거나, 혹은 추후 사용 시에 외부로부터 응용프로그램의 데이터를 수신하여 수신된 데이터에 기초하 여 설치될 수 있다. 응용 프로그램의 데이터는, 예컨대, 어플리케이션 마켓과 같은 외부 서버로부터 전자장치 로 다운로드될 수도 있다. 이와 같은 외부 서버는, 컴퓨터프로그램제품의 일례이나, 이에 한정되는 것은 아 니다. 리모컨은 IR신호만을 기반으로 2비트 제어정보를 전송하는 IR리모컨, 또는 예를 들면 버튼, 음성, 터치, 드 래그 등으로 입력된 사용자입력 정보를 IR신호, 블루투수신호, 와이파이신호 등으로 전송하는 통합리모컨(MBR), 또는 리모컨 앱(app)이 설치된 스마트폰 등의 모바일단말기 등으로 구현될 수 있다. 리모컨은 사용자입력부 , 리모컨 마이크, 리모컨 통신부 및 리모컨 프로세서를 포함할 수 있다. 사용자입력부는 각종 기능키 버튼을 통한 버튼입력, 터치센서를 통한 터치 또는 드래그 입력, 리모컨마이크 을 통한 음성입력, 동작감지센서를 통한 모션입력 등을 수신할 수 있다. 리모컨 마이크는 사용자의 음성을 수신할 수 있다. 이와 같이, 수신된 아날로그 음성입력은 디지털신호로 변환되어 리모컨통신부, 예를 들면 블루투스통신 모듈, 와이파이통신 모듈, 적외선통신 모듈 등을 통해 제 어대상, 예를 들면 전자장치로 전송될 수 있다. 만일 리모컨이 음성인식기능을 가진 스마트폰과 같은 모바일단말기로 구현되는 경우, 입력된 음성입력은 음성인식을 통해 인식된 음성신호 등의 형태로 전자장치(1 0)에 전송될 수도 있다. 사용자의 음성입력은 사용자가 부른 노래나 전자장치의 전원 온/오프제어 명령, 채 널제어 명령, 볼륨제어 명령, 컨텐츠공급자의 홈 또는 가이드영상 요청 명령 등을 포함할 수 있다. 리모컨통신부는 사용자입력부로부터 입력된 컨텐츠, 제어명령, 아날로그 음성신호로부터 변환 디지털음 성신호 등의 데이터를 전자장치의 신호입출력부에 전송할 수 있다. 리모컨통신부는 무선 통신을 수행하기 위해, IR, RF(Radio Frequency), Wi-fi, 블루투스, 지그비(Zigbee), UWB(Ultra-Wide Band), Wireless USB, NFC(Near Field Communication) 중 하나 이상의 통신을 수행하도록 구성 될 수 있다. 리모컨프로세서는 리모컨의 각 구성 부품을 제어할 수 있다. 리모컨프로세서는 버튼입력, 터치입력, 드래그입력, 모션입력에 대응한 제어명령을 리모컨통신부를 통해 전자장치로 전송할 수 있다. 리모컨프로세서는 리모컨마이크를 통해 입력된 아날로그 음성신호를 디지털 음성신호로 변환하여 리모 컨통신부를 통해 전자장치로 전송할 수 있다. 리모컨프로세서는 리모컨이 음성인식 기능을 가 진 경우에 입력된 음성신호를 인식하여 인식된 내용, 컨텐츠 또는 제어명령을 리모컨통신부를 통해 전자장 치로 전송할 수 있다. 도 4는 본 발명의 제2실시예에 따른 전자장치의 구성을 나타내는 블록도이다. 제2실시예에 따른 전자장치 는 제1컨텐츠로부터 다른 언어 기반의 제2컨텐츠를 생성하여 별도의 외부출력장치로 출력할 수 있다. 예를 들면, 전자장치는 디스플레이 장치로 영상을, 오디오 장치로 오디오를 출력해 줄 수 있다. 물론, 제2실시예에 따른 전자장치는 간단한 알림, 제어 메뉴 등을 표시하기 위한 디스플레이를 포함할 수 도 있다. 제2실시예에 따른 전자장치는 신호입출력부, 마이크, 메모리, 음성인식부, 스피커, 프 로세서, 및 AV인터페이스를 포함할 수 있다. 이하, 도 2와 동일한 구성은 설명을 생략하고 다른 구성에 대해서만 설명한다. 제2실시예에 따른 전자장치는 제1실시예에 따른 전자장치와 다르게, 생성된 제2컨텐츠를 AV인터페이스 에 연결된 외부출력장치에 전송할 수 있다.AV인터페이스는 전자장치에서 처리된 영상/음성 신호를 동시에 전송할 수 있는 포트인 HDMI, DP, 썬더 볼트 등으로 구현될 수 있다. 물론, 영상인터페이스는 영상/음성 신호를 각각 인식하여 출력할 수 있는 포 트로 구현될 수도 있다. 이하, 전자장치의 제2컨텐츠 획득 방법을 상세하게 설명한다. 도 5는 본 발명의 제1실시예에 따른 제2컨텐츠의 획득 방법을 설명하는 순서도이고, 도 6 및 7은 각각 제1언어 가사와 제2언어가사의 대응 구절의 구간을 시간 매칭 전후를 나타내는 도면이고, 도 8 및 9는 각각 제1언어가사 와 제2언어가사의 상호 대응 구절의 구간에서 음절 시간을 할당하는 방법들을 설명하기 위한 도이다. 단계 S11에서, 프로세서는 메모리에 저장된 음성분리모듈을 실행하여 저장된 또는 수신된 제1컨텐 츠를 원곡음악신호와 원곡음성신호로 분리할 수 있다. 원곡음악신호는 제1컨텐츠에서 가수의 음성을 배제한 배 경음악, 예를 들면 MR(Music Recorded)를 의미한다. 원곡음성신호는 제1컨텐츠에서 배경음악을 배제한 가수의 음성을 의미한다. 음성분리부는 원곡음성신호로부터 STT(Speech To Text)를 수행하여 제1언어가사(원곡가 사)를 획득할 수 있다. 제1언어가사(원곡가사)는 STT(Speech To Text)에 의해 인식하더라도 정확성이 떨어질 수 있으므로, 분리 대신에 외부장치(서버) 또는 메모리에 저장된 가사를 활용할 수도 있다. 이하, 설명의 편의를 위해 제1컨텐츠로 \"봄날\", 제1언어가사로서 한국어 가사로 하여, 그 가사 중에 \"보고 싶다 이렇게 말하니까 더 보고 싶다\" 부분을 예로 들어 설명하기로 한다. 단계 S12에서, 프로세서는 메모리에 저장된 음악보정모듈을 실행하여 원곡음악신호의 선명도 및 해상도를 보정할 수 있다. 원곡음악신호는 제1컨텐츠에서 MR만을 분리한 것으로 선명도 및 해상도를 보정하더라 도 완벽한 보정이 어려울 수 있다. 따라서, 프로세서는 제1컨텐츠에 대해 가수의 음성이 배제하여 마련된 MR을 활용할 수 있다. 프로세서는 외부장치(서버)에 요청하여 MR을 수신할 수 있다. 단계 S13에서, 프로세서는 메모리에 저장된 음성특징추출모듈을 실행하여 원곡음성신호로부터 가 수의 음성이 가진 특징, 예컨대 음의 크기, 음의 피치, 음의 톤을 구절의 구간(문장, 절, 또는 구) 별로 추출할 수 있다. 단계 S14에서, 프로세서는 메모리에 저장된 번역모듈을 실행하여 제1언어가사를 다른 제2언어가사 를 획득할 수 있다. 여기서, 프로세서는 직접 번역하는 대신에 외부장치, 예컨대 번역서버 또는 번역된 가 사를 저장하는 컨텐츠서버에 번역된 제2언어가사를 요청하여 획득할 수도 있다. 예를 들면, 제1컨텐츠의 한국어 가사, \"보고 싶다. 이렇게 말하니까 더 보고 싶다.\"는 영어가사, \"I miss you When I say that I miss you more \"로 번역될 수 있다. 이때, 번역되는 제2언어가사는 제1언어가사의 구절의 구간 별, 문장, 절, 또는 구에 대응하도록 번역될 수 있다. 도 6에 나타낸 바와 같이, 제1언어가사의 제1구절의 제1구간인 \"보고싶 다\"는 제2언어가사의 제2구절의 제1구간인 \"I miss you\"에 대응하고, 제1구절의 제2구간 인 \"이렇게말하니까\"는 제2구절의 제2구간인 \"when I sat that\"에 대응하고, 제1구절의 제3구 간인 \"더보고싶다\"는 제2언어가사의 제2구절의 제3구간인 \"I miss you more\"에 대응한다. 이와 같이, 번역된 제2언어가사는 제1언어가사의 제1구절의 각 구간(101,102,103)에 대응하는 제2구절의 각 구간(201,202,203)으로 식별될 수 있다. 단계 S15에서, 프로세서는 메모리에 저장된 음성생성모듈을 실행하여 번역된 제2언어가사를 기초 로 제1음성신호를 획득(생성)할 수 있다(S15-1). 예컨대, 프로세서는 제2언어가사의 텍스트를 적절한 음성 으로 변환할 수 있다. 도 6에 나타낸 바와 같이, 제2언어가사를 기반으로 생성한 제1음성신호의 제2구절의 각 구간(201,202,203) 길이(시간)는 450ms, 500ms, 550ms로서, 대응하는 제1언어가사의 제1구절의 각 구 간(101,102,103) 길이(시간) 400ms, 550ms, 450ms와 다를 수 있다. 이는 번역된 제2언어가사의 텍스트를 제1음 성신호로 변환할 때에 설정하는 시간에 따라 결정될 수 있기 때문이다. 즉, 번역된 제2언어가사의 텍스트를 제1 음성신호로 변환할 때 제2구절의 구간(201,202,203) 길이는 제1언어가사의 제1구절의 구간(101,102,103)을 고려하지 않고 설정될 수 있다. 프로세서는 제1음성신호의 제2구절의 구간(201,202,203) 길이(시간)를 제1음성신호를 기초로 제2음성 신호를 생성할 수 있다(S15-2). 도 7에 나타낸 바와 같이, 제2음성신호는 제2구절의 각 구간(201,202,203) 길이(시간)를 제1언어가사의 제1구절의 각 구간(101,102,103)에 매칭시켜 설정할 수 있다. 제2언어가사의 제1구절 각 구간(201,202,203) 시간을 제1언어가사의 제1구절의 각 구간 (101,102,103) 시간과 동일하게 매칭시킨 후, 제2구절의 각 구간(201,202,203)에 포함된 각 음절의 시간 (엔벨로프)을 조정할 수 있다.한국어가사와 영어가사의 음절 개념이 다르다. 따라서, 번역된 영어가사는 원곡의 언어인 한국어의 음절 개념을 적용하여 다음 표 1과 같이 분리할 수 있다. 만일 원곡의 언어가 영어이면 영어의 음절을 기준으로 한국어 가사 를 분리할 수 있다. 이때, 제1언어가사와 제2언어가사는 음절의 수가 서로 다르다. 표 1 제1언어 /보고 싶다(4음절)/이렇게 말하니까(7음절)/더 보고 싶다(5음절)/ 제2언어/아 이 미 스 유(5음절)/ 웬 아 이 세 이 댓(6음절)/ 아 이 미 스 유 모 어(7음절)/ 물론, 제2언어가사의 제2구절의 구간에 포함된 각 음절의 구분은 물리적인 음성신호에서 각 음절의 엔벨로프로 이루어질 수도 있다.도 8에 나타낸 바와 같이, 제1언어가사의 제1구절의 제1 내지 제3구절(101,102,103)은 제2언어가사의 제2구절의 제1 내지 제3구절(201,202,203)에 대응한다. 제2언어가사의 제2구절의 제1 내지 제3구간(201,202,203) 전체 시간은 각각 제1언어가사의 제1구절의 제1 내지 제3구간(101,102,103)의 전체 시간과 상호 일치시킬 수 있다. 또한, 제2구절의 제1 내지 제3구간 (201,202,203) 내의 각 음절의 시간은 제1구절의 제 1내지 제3구간(101,102,103) 각각의 전체 시간을 각 구간 내 총 음절 수로 나누어 균등하게 배분할 수 있다. 예를 들면, 제1언어가사의 구절에서, 전체 400ms인 제1구간의 4개 음절(보고싶다)은 각각 100ms이고, 전체 550ms인 제2구간의 7개음절(이렇게말하니까)은 각각 100ms, 50ms, 50ms, 100ms, 100ms, 100ms이고, 전체 450ms인 제3구간의 5개 음절(더보고싶다)은 50ms, 100ms, 100ms, 100ms, 100ms이다. 제2언어가사의 구절에서, 제1구간은 전체 400ms, 5개의 음절(아 이 미 스 유)은 각각 80ms로 할당될 수 있다. 제2구간은 전체 550ms, 6개음절(웬 아 이 세 이 댓) 중 첫째에서 다섯째 음절은 각각 91.6ms, 마지막 음절 (댓)은 소수점 절삭한 0.4ms를 가산하여 92.0ms로 할당할 수 있다. 제3구간은 전체 450ms, 7개 음절(아 이 미 스 유 모 어) 중 첫째~여섯째 음절(아 이 미 스 유 모)은 각각 64.2ms, 마지막 음절(어)은 소수점 절삭한 0.6ms를 가산하여 64.8ms로 할당할 수 있다. 이 경우는 계산이 간단하여 가벼운 하드웨어로 구현할 수 있는 장점을 가지나, 노래 자체는 밋밋할 수 있다. 도 9에 나타낸 바와 같이, 제1구절의 구간 각각의 전체 시간과 대응하는 제2구절의 구간 각각의 전체 시간을 동일하게 설정한 후에, 제2구절의 각 구간 내의 각 음절의 시간에 제1구절의 각 구간 내의 각 음절 시간을 반영하여 배분할 수 있다. 제1언어가사의 제1구절에서, 제1구간은 전체 400ms, 4개의 음절은 각각 100ms이고, 제2구간은 전체 550ms, 7개음절은 각각 100ms, 50ms, 50ms, 100ms, 100ms, 100ms이고, 제3구간은 전체 450ms, 5개 음절 각각은 50ms, 100ms, 100ms, 100ms, 100ms이다. 제2언어가사의 제2구절의 제1 내지 제3구간(201,202,203)은 다음과 같이 배분될 수 있다. 제1구절의 제1구간에서 전체 시간은 400ms이고, 4개 음절(보고싶다)의 시간은 모두 동일하게 100ms이 므로, 제2구절의 제1구간에서 전체 시간은 400ms이고, 5개의 음절(아 이 미 스 유)의 시간은 모두 균 등하게 80ms로 배분될 수 있다. 제1구절의 제2구간에서 전체 시간은 550ms이고, 7개 음절(이렇게말하니까)의 시간은 각각 100ms, 50ms, 50ms, 100ms, 100ms, 100ms이다. 제2구절의 제2구간에서 전체 시간은 550ms이다. 6개의 음절(웬 아 이 세 이 댓) 중 첫째 음절(웬)은 제1구절의 제2구간에서 100ms인 첫째 음절(이) 전체와 50ms인 둘째 음절(렇)의 0.17에 대응하므로, 100ms+(50ms×0.17)=108.5ms로 배분될 수 있다. 6개의 음절(웬 아 이 세 이 댓) 중 둘째 음절(아)은 제1구절의 제2구간에서 50ms인 둘째 음절(렇)의 0.83와 50ms인 셋째 음절(게)의 0.34에 대응하므로, (50ms×0.83)+(50ms×0.34)=58.5ms로 배분될 수 있다. 6개의 음절(웬 아 이 세 이 댓) 중 셋째 음절(이)은 제1구절의 제2구간에서 100ms인 셋째 음절(게)의 0.66와 100ms인 넷째 음절(말)의 0.51에 대응하므로, (50ms×0.66)+(100ms×0.51)=84ms로 배분될 수 있다.6개의 음절(웬 아 이 세 이 댓) 중 넷째 음절(세)은 제1구절의 제2구간에서 100ms인 넷째 음절(말)의 0.49와 100ms인 다섯째 음절(하)의 0.68에 대응하므로, (100ms×0.34)+(100ms×0.68)=117ms로 배분될 수 있다. 6개의 음절(웬 아 이 세 이 댓) 중 다섯째 음절(이)은 제1구절의 제2구간에서 100ms인 다섯째 음절 (하)의 0.32와 100ms인 여섯째 음절(니)의 0.85에 대응하므로, (100ms×0.32)+(100ms×0.85)=117ms로 배분될 수 있다. 6개의 음절(웬 아 이 세 이 댓) 중 여섯째 음절(댓)은 제1구절의 제2구간에서 100ms인 여섯째 음절 (니)의 0.15와 50ms인 일곱째 음절(까)의 전체에 대응하므로, (100ms×0.15)+50ms=65ms로 배분될 수 있다. 제1구절의 제3구간에서 전체 시간은 450ms이고, 5개 음절(더보고싶다)의 시간은 각각 50ms, 100ms, 100ms, 100ms, 100ms이다. 제2구절의 제2구간에서 전체 시간은 450ms이다. 7개의 음절(아 이 미 스 유 모 어) 중 첫째 음절(아)은 제1구절의 제3구간에서 50ms인 첫째 음절 (더)의 0.71에 대응하므로, 50ms×0.71)=35.5ms로 배분될 수 있다. 7개의 음절(아 이 미 스 유 모 어) 중 둘째 음절(이)은 제1구절의 제3구간에서 50ms인 첫째 음절 (더)의 0.29와 100ms인 둘째 음절(보)의 0.42에 대응하므로, (50ms×0.29)+(100ms×0.42)=56.5ms로 배분될 수 있다. 7개의 음절(아 이 미 스 유 모 어) 중 셋째 음절(미)은 제1구절의 제3구간에서 100ms인 둘째 음절(보)의 0.58와 100ms인 셋째 음절(고)의 0.13에 대응하므로, (100ms×0.58)+(100ms×0.13)=71ms로 배분될 수 있다. 7개의 음절(아 이 미 스 유 모 어) 중 넷째 음절(스)은 제1구절의 제3구간에서 100ms인 셋째 음절(고)의 0.71에 대응하므로, (100ms×0.71)=71ms로 배분될 수 있다. 7개의 음절(아 이 미 스 유 모 어) 중 다섯째 음절(유)은 제1구절의 제3구간에서 100ms인 셋째 음절 (고)의 0.16와 100ms인 넷째 음절(싶)의 0.55에 대응하므로, (100ms×0.16)+(100ms×0.55)=71ms로 배분될 수 있다. 7개의 음절(아 이 미 스 유 모 어) 중 여섯째 음절(모)은 제1구절의 제3구간에서 100ms인 넷째 음절 (싶)의 0.45와 100ms인 다섯째 음절(다)의 0.26에 대응하므로, (100ms×0.45)+(100ms×0.26)=71ms로 배분될 수 있다. 7개의 음절(아 이 미 스 유 모 어) 중 일곱째 음절(어)은 제1구절의 제3구간에서 100ms인 다섯째 음 절(다)의 0.71에 대응하므로, (100ms×0.74)=71ms로 배분될 수 있다. 그러나, 제1구절의 제3구간 음 절과 제2구절의 제3구간 음절을 매칭 시킬 때, 제1구절 음절 수를 제2구절 음절 수로 나눈 값 은 71.428571이다. 이때, 소수점 이하를 절삭하고 남은 부분 3을, 마지막 일곱째 음절(어)에 가산하여 74ms로 할당하였다. 제2음성신호는 단계 S13에서 원곡음성신호로부터 추출된 가수의 음성이 가진 특징, 예컨대 구절의 구간(문장, 절, 또는 구) 별 또는 각 음절 별 음의 크기, 음의 피치, 음의 톤을 적용할 수 있다. 이상과 같이, 제2음성신호는 제1언어가사의 제1구절의 각 구간과 제2언어가사의 제2구절의 각 구간 간의 시간을 매칭시킴으로써, 원곡음성신호에 유사한 느낌이 나도록 할 수 있다. 단계 S16에서, 프로세서는 메모리에 저장된 노래합성모듈을 실행하여 보정된 원곡음악신호와 제2 음성신호를 합성하여 새로운 제2언어가사 기반의 제2컨텐츠를 획득할 수 있다. 도 10은 본 발명의 다른 실시예에 따른 제2컨텐츠의 획득 방법을 설명하는 순서도이다. 단계 S21에서, 프로세서는 메모리에 저장된 음성특징모듈을 실행하여 제1컨텐츠의 제1언어음성신 호로부터 제1구절(문장, 절, 구) 별로 음성의 특징을 추출할 수 있다. 음성의 특징은 음의 크기, 음의 피치, 또 는 음의 톤을 포함할 수 있다. 제1컨텐츠의 제1언어음성신호는 마이크, 리모컨으로부터 입력되거나 외 부장치(컨텐츠공급장치, 서버)로부터 MR이 배제된 상태로 수신될 수 있다. 단계 S22에서, 프로세서는 메모리에 저장된 번역모듈을 실행하여 제1컨텐츠의 제1언가사를 다른 언어로 번역한 제2언어가사를 획득할 수 있다. 물론, 제2언어가사는 직접 번역할 수도 있고, 외부장치, 예컨대 번역서버, 컨텐츠공급장치에 요청하여 획득할 수도 있다. 프로세서는 제1언어가사의 제1구절, 예를 들면 문장, 절, 또는 구의 구간으로 분리시키고, 번역된 제2언어 가사를 제1언어가사의 제1구절의 구간에 대응하는 제2구절의 구간을 식별한다. 단계 S23에서, 프로세서는 메모리에 저장된 음성생성모듈을 실행하여 번역된 제2언어가사를 기초 로 제1음성신호를 획득(생성)할 수 있다. 프로세서는 제1음성신호에서 제1구절의 구간과 제2구절이 구간 간의 시간을 매칭시키고, 매칭된 각 구간의 음절들의 시간을 할당하여 제1음성신호로부터 변환된 제2음성신호를 생성할 수 있다. 프로세서는 제2언어가 사를 기초로 만들어진 기계음, 예를 들면 텍스트를 적절한 음성으로 변환한 제1음성신호에 원곡음성신호에서 추 출한 음의 특징, 예컨대 음의 크기, 음의 피치, 음의 톤을 대응하는 구절 별로 적용하여 제2음성신호를 생성할 수 있다. 단계 S24에서, 프로세서는 메모리에 저장된 노래합성모듈을 실행하여 MR과 제2음성신호를 합성하 여 새로운 제2언어가사 기반의 제2컨텐츠를 획득할 수 있다. MR은 외부장치, 예컨대 컨텐츠서버 또는 컨텐츠공 급장치에 요청하여 획득할 수도 있다. 이상과 같이, 사용자가 마이크 또는 리모컨의 마이크를 통해 제1언어로 노래를 부르면, 프로세서는 사용자 가 부르는 노래의 느낌이 나는 제2언어기반의 음성신호로 변환한 후, 이를 MR과 합성하여 제2언어 기반의 노래 로 대체하여, 스피커를 통해 출력할 수 있다. 본 발명의 실시예에 따른, 제1컨텐츠로부터 원곡음악신호와 원곡음성신호로 분리할 수 있는 음성분리모듈, 분리된 원곡음악신호의 선명도와 해상도를 보정하는 음악보정모듈, 분리된 원곡음성신호로부터 음성의 특 징, 예를 들면 음의 크기, 음의 피치, 음의 톤 등을 구절의 구간별로 추출하는 음성특징추출모듈, 분리된 원곡음성신호의 제1언어가사를 다른 제2언어가사로 번역하는 번역모듈, 원곡음성신호의 음성의 특징을 번 역된 제2언어가사에 적용하여 신규음성신호를 생성하는 음성생성모듈, 또는 신규음성신호를 보정된 원곡음 악신호에 합성하여 제2컨텐츠를 생성하는 노래합성모듈 중 적어도 하나는 컴퓨터 판독 가능 기록매체로서 메모리에 저장된 컴퓨터프로그램제품 또는 네트워크통신으로 송수신되는 컴퓨터프로그램 제품으로 구현될 수 있다. 또한, 상술한 각 모듈들은 단독 또는 통합되어 컴퓨터프로그램으로 구현될 수 있다. 본 발명의 실시예에 따른 컴퓨터 프로그램은 사용자가 마이크를 통해 부르는 제1컨텐츠를 다른 언어기반의 제2 컨텐츠로 대체하는 동작을 수행할 수 있다."}
{"patent_id": "10-2019-0093347", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 제1실시예에 따른 전자장치를 나타내는 도면이다. 도 2는 도 1의 전자장치의 구성을 나타내는 블록도이다. 도 3은 도 1의 메모리의 구성을 나타내는 블록도이다. 도 4는 본 발명의 제2실시예에 따른 전자장치의 구성을 나타내는 블록도이다. 도 5는 본 발명의 제1실시예에 따른 제2컨텐츠의 획득 방법을 설명하는 순서도이다. 도 6은 제1언어가사와 제2언어가사의 대응 구절의 각 구간을 나타내는 도이다. 도 7은 도 6에서 제1언어가사와 제2언어가사의 대응 구절의 각 구간 시간을 동일하게 매칭시킨 도이다. 도 8은 제1언어가사와 제2언어가사의 상호 대응 구절의 구간에서 음절 시간을 할당하는 방법들을 설명하기 위한 도이다. 도 9는 제1언어가사와 제2언어가사의 상호 대응 구절의 구간에서 음절 시간을 할당하는 다른 방법들을 설명하기 위한 도이다. 도 10은 본 발명의 다른 실시예에 따른 제2컨텐츠의 획득 방법을 설명하는 순서도이다."}
