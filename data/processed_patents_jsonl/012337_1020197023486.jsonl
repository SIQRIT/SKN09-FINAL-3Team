{"patent_id": "10-2019-7023486", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0137943", "출원번호": "10-2019-7023486", "발명의 명칭": "멀티 센서를 이용하여 위치를 추정하는 방법 및 이를구현하는 로봇", "출원인": "엘지전자 주식회사", "발명자": "어규호"}}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇을 이동시키는 이동부; 상기 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 제1라이다 프레임을 생성하는 라이다 센서;상기 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성하는 카메라 센서; 상기 제1라이다 프레임과 비교 가능한 다수의 라이다 프레임들을 포함하는 라이다 브랜치를 저장하고, 상기 제1비주얼 프레임과 비교 가능한 다수의 비주얼 프레임들을 포함하는 비주얼 브랜치를 저장하며, 상기 저장된 라이다 프레임 또는 상기 저장된 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프를 저장하며, 상기 프레임 노드 사이의 오도메트리 정보를 저장하는 맵 저장부; 및상기 포즈 그래프의 프레임 노드에 등록된 라이다 프레임과 상기 제1라이다 프레임을 비교하며, 상기 포즈 그래프의 프레임 노드에 등록된 비주얼 프레임과 상기 제1비주얼 프레임을 비교하여 상기 제1라이다 프레임의 비교결과의 정확도를 판단하여 로봇의 현재 위치를 계산하는 제어부를 포함하는, 멀티 센서를 이용하여 위치를 추정하는 로봇."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 제어부는 상기 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산하는, 멀티 센서를 이용하여 위치를추정하는 로봇."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 제어부는 상기 제1라이다 프레임과 유사한 제2라이다 프레임과 제3라이다 프레임을 상기 포즈 그래프의 프레임 노드로부터 추출하고, 상기 제어부는 상기 제1비주얼 프레임과 유사한 제2비주얼 프레임을 상기 포즈 그래프의 프레임 노드로부터 추출하고, 상기 제어부는 상기 제2비주얼 프레임을 이용하여 상기 제2라이다 프레임 또는 제3라이다 프레임 중 어느 하나를 선택하고 선택한 라이다 프레임을 이용하여 로봇의 현재 위치로 계산하는, 멀티 센서를 이용하여 위치를 추정하는 로봇."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 제어부는 상기 제1비주얼 프레임과 상기 제2비주얼 프레임의 유사도가 설정된 기준값 보다 높을 경우, 상기 제2비주얼 프레임의 위치 정보에 대응하여 상기 제1라이다 프레임을 상기 맵 저장부에 저장하는, 멀티 센서를 이용하여 위치를 추정하는 로봇."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 제어부는 상기 제1라이다 프레임을 이용하여 제1위치 정보 및 제2위치 정보를 검색하며, 상기 제어부는 상기 제1비주얼 프레임을 이용하여 제3위치 정보 및 제4위치 정보를 검색하며, 상기 제어부는 상기 제1위치 정보, 상기 제2위치 정보, 상기 제3위치 정보, 상기 제4위치 정보를 이용하여 상기로봇의 위치 정보를 산출하는, 멀티 센서를 이용하여 위치를 추정하는 로봇.공개특허 10-2020-0137943-3-청구항 6 제5항에 있어서, 상기 제어부는 상기 제1위치 정보와 상기 제3위치 정보의 거리, 상기 제2위치 정보와 상기 제3위치 정보의거리, 상기 제1위치 정보와 상기 제4위치 정보의 거리, 상기 제2위치 정보와 상기 제4위치 정보의 거리를 비교하여 가장 근접한 거리의 위치 정보들을 이용하여 로봇의 위치 정보를 계산하는, 멀티 센서를 이용하여 위치를추정하는 로봇."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 제어부는 제1위치 정보 및 상기 제3위치 정보의 거리가 가장 근접한 것으로 판단한 경우, 상기 제1라이다 프레임과 상기 제1위치 정보의 라이다 프레임의 유사도 및 상기 제2비주얼 프레임과 상기 제3위치 정보의 비주얼 프레임의 유사도를 비교하여 상기 로봇의 위치 정보를 계산하는, 멀티 센서를 이용하여 위치를 추정하는 로봇."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "이동부가 로봇을 이동시키는 과정에서 상기 로봇의 라이다 센서가 상기 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 제1라이다 프레임을 생성하는 단계; 상기 로봇의 카메라 센서가 상기 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성하는 단계; 및상기 제어부가 상기 로봇의 맵 저장부에 저장된 라이다 프레임과 상기 제1라이다 프레임을 비교하며, 상기 포즈그래프의 프레임 노드에 등록된 비주얼 프레임과 상기 제1비주얼 프레임을 비교하여 상기 제1라이다 프레임의비교 결과의 정확도를 판단하여 로봇의 현재 위치를 계산하는 단계를 포함하는, 멀티 센서를 이용하여 위치를추정하는 방법."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 맵 저장부는 상기 제1라이다 프레임과 비교 가능한 다수의 라이다 프레임들을 포함하는 라이다 브랜치를 저장하고, 상기 제1비주얼 프레임과 비교 가능한 다수의 비주얼 프레임들을 포함하는 비주얼 브랜치를 저장하며, 상기 저장된 라이다 프레임 또는 상기 저장된 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프를 저장하며, 상기 프레임 노드 사이의 오도메트리 정보를 저장하는, 멀티 센서를 이용하여 위치를 추정하는 방법."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서, 상기 제어부는 상기 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산하는 단계를 더 포함하는, 멀티 센서를 이용하여 위치를 추정하는 방법."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 상기 제어부는 상기 제1라이다 프레임과 유사한 제2라이다 프레임과 제3라이다 프레임을 상기 포즈 그래프의 프레임 노드로부터 추출하는 단계; 상기 제어부는 상기 제1비주얼 프레임과 유사한 제2비주얼 프레임을 상기 포즈 그래프의 프레임 노드로부터 추출하는 단계; 및상기 제어부는 상기 제2비주얼 프레임을 이용하여 상기 제2라이다 프레임 또는 제3라이다 프레임 중 어느 하나를 선택하고 선택한 라이다 프레임을 이용하여 로봇의 현재 위치로 계산하는 단계를 더 포함하는, 멀티 센서를공개특허 10-2020-0137943-4-이용하여 위치를 추정하는 방법."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 제어부는 상기 제1비주얼 프레임과 상기 제2비주얼 프레임의 유사도가 설정된 기준값 보다 높을 경우, 상기 제2비주얼 프레임의 위치 정보에 대응하여 상기 제1라이다 프레임을 상기 맵 저장부에 저장하는 단계를 더포함하는, 멀티 센서를 이용하여 위치를 추정하는 방법."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8항에 있어서, 상기 제어부는 상기 제1라이다 프레임을 이용하여 제1위치 정보 및 제2위치 정보를 검색하는 단계; 상기 제어부는 상기 제1비주얼 프레임을 이용하여 제3위치 정보 및 제4위치 정보를 검색하는 단계; 및상기 제어부는 상기 제1위치 정보, 상기 제2위치 정보, 상기 제3위치 정보, 상기 제4위치 정보를 이용하여 상기로봇의 위치 정보를 산출하는 단계를 더 포함하는, 멀티 센서를 이용하여 위치를 추정하는 방법."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 제어부는 상기 제1위치 정보와 상기 제3위치 정보의 거리, 상기 제2위치 정보와 상기 제3위치 정보의거리, 상기 제1위치 정보와 상기 제4위치 정보의 거리, 상기 제2위치 정보와 상기 제4위치 정보의 거리를 비교하여 가장 근접한 거리의 위치 정보들을 이용하여 로봇의 위치 정보를 계산하는 단계를 더 포함하는, 멀티 센서를 이용하여 위치를 추정하는 방법."}
{"patent_id": "10-2019-7023486", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 상기 제어부는 제1위치 정보 및 상기 제3위치 정보의 거리가 가장 근접한 것으로 판단한 경우, 상기 제1라이다 프레임과 상기 제1위치 정보의 라이다 프레임의 유사도 및 상기 제2비주얼 프레임과 상기 제3위치 정보의 비주얼 프레임의 유사도를 비교하여 상기 로봇의 위치 정보를 계산하는 단계를 더 포함하는, 멀티 센서를 이용하여 위치를 추정하는 방법."}
{"patent_id": "10-2019-7023486", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 멀티 센서를 이용하여 위치를 추정하는 방법 및 이를 구현하는 로봇에 관한 것으로, 발명의 일 실시예 에 의한 멀티 센서를 이용하여 위치를 추정하는 방법은 이동부가 로봇을 이동시키는 과정에서 로봇의 라이다 센 서가 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 제1라이다 프레임을 생성하는 단계와, 로봇의 카메라 센서가 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성하는 단계와, 제어부가 로봇의 맵 저장부에 저장된 라이다 프레임과 제1라이다 프레임을 비교하며, 포즈 그래프의 프레임 노드에 등록된 비주얼 프 레임과 제1비주얼 프레임을 비교하여 제1라이다 프레임의 비교 결과의 정확도를 판단하여 로봇의 현재 위치를 계 산하는 단계를 포함한다. 공개특허10-2020-0137943 CPC특허분류 B25J 19/023 (2013.01) B25J 9/1694 (2013.01) 발명자 김형록 서울특별시 서초구 양재대로11길 19 LG전자 특허 센터 노동기 서울특별시 서초구 양재대로11길 19 LG전자 특허 센터박중태 서울특별시 서초구 양재대로11길 19 LG전자 특허 센터명 세 서 청구범위 청구항 1 로봇을 이동시키는 이동부; 상기 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 제1라이다 프레임을 생성하는 라이다 센서; 상기 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성하는 카메라 센서; 상기 제1라이다 프레임과 비교 가능한 다수의 라이다 프레임들을 포함하는 라이다 브랜치를 저장하고, 상기 제1 비주얼 프레임과 비교 가능한 다수의 비주얼 프레임들을 포함하는 비주얼 브랜치를 저장하며, 상기 저장된 라이 다 프레임 또는 상기 저장된 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본 으로 구성된 포즈 그래프를 저장하며, 상기 프레임 노드 사이의 오도메트리 정보를 저장하는 맵 저장부; 및 상기 포즈 그래프의 프레임 노드에 등록된 라이다 프레임과 상기 제1라이다 프레임을 비교하며, 상기 포즈 그래 프의 프레임 노드에 등록된 비주얼 프레임과 상기 제1비주얼 프레임을 비교하여 상기 제1라이다 프레임의 비교 결과의 정확도를 판단하여 로봇의 현재 위치를 계산하는 제어부를 포함하는, 멀티 센서를 이용하여 위치를 추정 하는 로봇. 청구항 2 제1항에 있어서, 상기 제어부는 상기 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산하는, 멀티 센서를 이용하여 위치를 추정하는 로봇. 청구항 3 제1항에 있어서, 상기 제어부는 상기 제1라이다 프레임과 유사한 제2라이다 프레임과 제3라이다 프레임을 상기 포즈 그래프의 프 레임 노드로부터 추출하고, 상기 제어부는 상기 제1비주얼 프레임과 유사한 제2비주얼 프레임을 상기 포즈 그래프의 프레임 노드로부터 추 출하고, 상기 제어부는 상기 제2비주얼 프레임을 이용하여 상기 제2라이다 프레임 또는 제3라이다 프레임 중 어느 하나 를 선택하고 선택한 라이다 프레임을 이용하여 로봇의 현재 위치로 계산하는, 멀티 센서를 이용하여 위치를 추 정하는 로봇. 청구항 4 제3항에 있어서, 상기 제어부는 상기 제1비주얼 프레임과 상기 제2비주얼 프레임의 유사도가 설정된 기준값 보다 높을 경우, 상 기 제2비주얼 프레임의 위치 정보에 대응하여 상기 제1라이다 프레임을 상기 맵 저장부에 저장하는, 멀티 센서 를 이용하여 위치를 추정하는 로봇. 청구항 5 제1항에 있어서, 상기 제어부는 상기 제1라이다 프레임을 이용하여 제1위치 정보 및 제2위치 정보를 검색하며, 상기 제어부는 상기 제1비주얼 프레임을 이용하여 제3위치 정보 및 제4위치 정보를 검색하며, 상기 제어부는 상기 제1위치 정보, 상기 제2위치 정보, 상기 제3위치 정보, 상기 제4위치 정보를 이용하여 상기 로봇의 위치 정보를 산출하는, 멀티 센서를 이용하여 위치를 추정하는 로봇.청구항 6 제5항에 있어서, 상기 제어부는 상기 제1위치 정보와 상기 제3위치 정보의 거리, 상기 제2위치 정보와 상기 제3위치 정보의 거리, 상기 제1위치 정보와 상기 제4위치 정보의 거리, 상기 제2위치 정보와 상기 제4위치 정보의 거리를 비교 하여 가장 근접한 거리의 위치 정보들을 이용하여 로봇의 위치 정보를 계산하는, 멀티 센서를 이용하여 위치를 추정하는 로봇. 청구항 7 제6항에 있어서, 상기 제어부는 제1위치 정보 및 상기 제3위치 정보의 거리가 가장 근접한 것으로 판단한 경우, 상기 제1라이다 프레임과 상기 제1위치 정보의 라이다 프레임의 유사도 및 상기 제2비주얼 프레임과 상기 제3위 치 정보의 비주얼 프레임의 유사도를 비교하여 상기 로봇의 위치 정보를 계산하는, 멀티 센서를 이용하여 위치 를 추정하는 로봇. 청구항 8 이동부가 로봇을 이동시키는 과정에서 상기 로봇의 라이다 센서가 상기 로봇의 외부에 배치된 사물과 로봇 사이 의 거리를 센싱하여 제1라이다 프레임을 생성하는 단계; 상기 로봇의 카메라 센서가 상기 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성하는 단계; 및 상기 제어부가 상기 로봇의 맵 저장부에 저장된 라이다 프레임과 상기 제1라이다 프레임을 비교하며, 상기 포즈 그래프의 프레임 노드에 등록된 비주얼 프레임과 상기 제1비주얼 프레임을 비교하여 상기 제1라이다 프레임의 비교 결과의 정확도를 판단하여 로봇의 현재 위치를 계산하는 단계를 포함하는, 멀티 센서를 이용하여 위치를 추정하는 방법. 청구항 9 제8항에 있어서, 상기 맵 저장부는 상기 제1라이다 프레임과 비교 가능한 다수의 라이다 프레임들을 포함하는 라이다 브랜치를 저장하고, 상기 제1 비주얼 프레임과 비교 가능한 다수의 비주얼 프레임들을 포함하는 비주얼 브랜치를 저장하며, 상기 저장된 라이 다 프레임 또는 상기 저장된 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본 으로 구성된 포즈 그래프를 저장하며, 상기 프레임 노드 사이의 오도메트리 정보를 저장하는, 멀티 센서를 이용 하여 위치를 추정하는 방법. 청구항 10 제8항에 있어서, 상기 제어부는 상기 오도메트리 정보를 이용하여 로봇의 현재 위치를 계산하는 단계를 더 포함하는, 멀티 센서 를 이용하여 위치를 추정하는 방법. 청구항 11 제9항에 있어서, 상기 제어부는 상기 제1라이다 프레임과 유사한 제2라이다 프레임과 제3라이다 프레임을 상기 포즈 그래프의 프 레임 노드로부터 추출하는 단계; 상기 제어부는 상기 제1비주얼 프레임과 유사한 제2비주얼 프레임을 상기 포즈 그래프의 프레임 노드로부터 추 출하는 단계; 및 상기 제어부는 상기 제2비주얼 프레임을 이용하여 상기 제2라이다 프레임 또는 제3라이다 프레임 중 어느 하나 를 선택하고 선택한 라이다 프레임을 이용하여 로봇의 현재 위치로 계산하는 단계를 더 포함하는, 멀티 센서를이용하여 위치를 추정하는 방법. 청구항 12 제11항에 있어서, 상기 제어부는 상기 제1비주얼 프레임과 상기 제2비주얼 프레임의 유사도가 설정된 기준값 보다 높을 경우, 상 기 제2비주얼 프레임의 위치 정보에 대응하여 상기 제1라이다 프레임을 상기 맵 저장부에 저장하는 단계를 더 포함하는, 멀티 센서를 이용하여 위치를 추정하는 방법. 청구항 13 제8항에 있어서, 상기 제어부는 상기 제1라이다 프레임을 이용하여 제1위치 정보 및 제2위치 정보를 검색하는 단계; 상기 제어부는 상기 제1비주얼 프레임을 이용하여 제3위치 정보 및 제4위치 정보를 검색하는 단계; 및 상기 제어부는 상기 제1위치 정보, 상기 제2위치 정보, 상기 제3위치 정보, 상기 제4위치 정보를 이용하여 상기 로봇의 위치 정보를 산출하는 단계를 더 포함하는, 멀티 센서를 이용하여 위치를 추정하는 방법. 청구항 14 제13항에 있어서, 상기 제어부는 상기 제1위치 정보와 상기 제3위치 정보의 거리, 상기 제2위치 정보와 상기 제3위치 정보의 거리, 상기 제1위치 정보와 상기 제4위치 정보의 거리, 상기 제2위치 정보와 상기 제4위치 정보의 거리를 비교 하여 가장 근접한 거리의 위치 정보들을 이용하여 로봇의 위치 정보를 계산하는 단계를 더 포함하는, 멀티 센서 를 이용하여 위치를 추정하는 방법. 청구항 15 제14항에 있어서, 상기 제어부는 제1위치 정보 및 상기 제3위치 정보의 거리가 가장 근접한 것으로 판단한 경우, 상기 제1라이다 프레임과 상기 제1위치 정보의 라이다 프레임의 유사도 및 상기 제2비주얼 프레임과 상기 제3위 치 정보의 비주얼 프레임의 유사도를 비교하여 상기 로봇의 위치 정보를 계산하는 단계를 더 포함하는, 멀티 센 서를 이용하여 위치를 추정하는 방법. 발명의 설명"}
{"patent_id": "10-2019-7023486", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 멀티 센서를 이용하여 위치를 추정하는 방법 및 이를 이용하여 주행하는 로봇에 관한 기술이다."}
{"patent_id": "10-2019-7023486", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "대형 마트, 백화점, 공항, 골프장 등 인적, 물적 교류가 활발하게 발생하는 공간에는 사람들에게 정보를 제공하 기 위해, 또는 사람들에게 편의를 제공하기 위해 로봇이 배치될 수 있다. 전술한 로봇의 종류로는 안내로봇, 보안로봇, 청소로봇 등이 있으며, 이들 다양한 로봇들은 공간 내에서 자신의 위치를 확인하며 이동한다. 한편, 로봇들이 자신의 위치를 확인하고 장애물을 회피하며 이동하기 위해서는 공간에 대한 정보와 로봇의 현재 위치 또는 이전에 로봇이 이동한 경로 등에 대한 정보를 로봇이 유지해야 한다. 로봇이 공간을 확인하고 이동하기 위해 로봇은 맵을 보유할 수 있다. 그런데, 맵을 생성하기 위해서 로봇은 다 양한 센서들을 이용하여 맵을 작성할 수 있으며, 맵 내의 정보의 다양한 정보들을 일치시켜 저장하는 것이 필요 하다. 또한, 맵을 저장한 후 이를 이용하여 로봇의 위치를 추정하기 위해서는 맵의 정보와 로봇이 주행 과정에서 획득 한 정보를 비교하는 것이 필요하다. 즉, 로봇은 맵의 정보와 주행 과정에서 획득한 정보를 비교한 결과 로봇의 위치를 추정하는 로컬라이제이션(localization)을 수행할 수 있다. 한편, 로봇의 위치 추정의 정확도를 높이기 위해서는 센서의 수와 이에 대응하는 맵의 정보의 수가 증가시키는 것이 필요하며 이를 이용한 로봇의 위치 추정 기술을 구현하는 것이 요청된다."}
{"patent_id": "10-2019-7023486", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 명세서에서는 전술한 문제점을 해결하기 위한 것으로, 로봇이 다양한 종류의 센서들에 기반하여 로봇의 위치 를 추정하고자 한다. 또한, 본 명세서에서는 각각의 센서들이 생성한 정보에 기반하여 로봇의 후보 위치를 산출하고 이들을 이용하여 로봇의 최종 위치 정보를 추정하고자 한다. 또한, 본 명세서에서는 로봇의 센서들 중 어느 하나의 정확도가 낮거나 어느 하나의 센서가 획득한 정보에 대응 하는 위치가 다수 검출된 경우에 로봇이 또다른 센서를 이용하여 위치 추정의 정확도를 높일 수 있도록 한다. 본 발명의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 발명의 다른 목적 및 장점들 은 하기의 설명에 의해서 이해될 수 있고, 본 발명의 실시예에 의해 보다 분명하게 이해될 것이다. 또한, 본 발 명의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것 이다."}
{"patent_id": "10-2019-7023486", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "발명의 일 실시예에 의한 멀티 센서 및 인공지능을 이용하여 위치를 추정하는 로봇은 로봇의 외부에 배치된 사 물과 로봇 사이의 거리를 센싱하여 라이다 프레임을 생성하는 라이다 센서와 로봇의 외부에 배치된 사물을 촬영 하여 비주얼 프레임을 생성하는 카메라 센서를 포함한다. 발명의 일 실시예에 의한 멀티 센서 및 인공지능을 이용하여 위치를 추정하는 로봇은 맵 저장부에 저장된 라이 다 프레임과 라이다 센서가 산출한 라이다 프레임을 비교하며, 맵 저장부에 저장된 비주얼 프레임과 카메라 센 서가 산출한 비주얼 프레임을 비교하여 라이다 센서에 기반한 위치 정보의 정확도를 판단하여 로봇의 현재 위치 를 계산하는 제어부를 포함한다. 발명의 일 실시예에 의한 멀티 센서 및 인공지능을 이용하여 위치를 추정하는 로봇은 오도메트리 정보를 이용하 여 로봇의 현재 위치를 계산한다. 발명의 일 실시예에 의한 멀티 센서 및 인공지능을 이용하여 위치를 추정하는 로봇의 제어부는 라이다 센서가 산출한 제1라이다 프레임과 유사한 제2라이다 프레임과 제3라이다 프레임을 맵 저장부로부터 추출하고, 제어부 는 카메라 센서가 산출한 제1비주얼 프레임과 유사한 제2비주얼 프레임을 맵 저장부로부터 추출한다. 발명의 일 실시예에 의한 멀티 센서 및 인공지능을 이용하여 위치를 추정하는 로봇의 제어부는 맵 저장부에서 추출한 제2비주얼 프레임을 이용하여 맵 저장부에서 추출한 제2라이다 프레임 또는 제3라이다 프레임 중 어느 하나를 선택하고 선택한 라이다 프레임을 이용하여 로봇의 현재 위치로 계산한다. 발명의 일 실시예에 의한 멀티 센서 및 인공지능을 이용하여 위치를 추정하는 로봇의 제어부는 카메라 센서가 산출한 제1비주얼 프레임과 제2비주얼 프레임의 유사도가 설정된 기준값 보다 높을 경우, 제2비주얼 프레임의 위치 정보에 대응하여 라이다 센서가 산출한 제1라이다 프레임을 맵 저장부에 저장한다. 발명의 일 실시예에 의한 멀티 센서 및 인공지능을 이용하여 위치를 추정하는 방법은 이동부가 로봇을 이동시키 는 과정에서 로봇의 라이다 센서가 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 제1라이다 프레 임을 생성하는 단계와, 로봇의 카메라 센서가 로봇의 외부에 배치된 사물을 촬영하여 제1비주얼 프레임을 생성 하는 단계와, 제어부가 로봇의 맵 저장부에 저장된 라이다 프레임과 제1라이다 프레임을 비교하며, 포즈 그래프 의 프레임 노드에 등록된 비주얼 프레임과 제1비주얼 프레임을 비교하여 제1라이다 프레임의 비교 결과의 정확 도를 판단하여 로봇의 현재 위치를 계산하는 단계를 포함한다."}
{"patent_id": "10-2019-7023486", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들을 적용할 경우, 로봇이 로봇이 다양한 종류의 센서들에 기반하여 로봇의 위치를 추정할 수 있다. 또한, 본 발명의 실시예들을 적용할 경우, 각각의 센서들이 생성한 정보에 기반하여 로봇의 후보 위치를 산출하 고 이들을 이용하여 로봇의 최종 위치 정보를 추정할 수 있다. 또한, 본 발명의 실시예들을 적용할 경우, 로봇의 센서들 중 어느 하나의 정확도가 낮거나 어느 하나의 센서가 획득한 정보에 대응하는 위치가 다수 검출된 경우에 로봇이 또다른 센서를 이용하여 위치 추정의 정확도를 높일 수 있다."}
{"patent_id": "10-2019-7023486", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 전술한 효과에 한정되지 않으며, 본 발명의 당업자들은 본 발명의 구성에서 본 발명의 다양 한 효과를 쉽게 도출할 수 있다."}
{"patent_id": "10-2019-7023486", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 동일 또는 유사한 구성요소에 대해서는 동일한 참조 부호를 붙이도록 한다. 또한, 본 발명의 일부 실시예들을 예시적인 도 면을 참조하여 상세하게 설명한다. 각 도면의 구성요소들에 참조부호를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 부호를 가질 수 있다. 또한, 본 발명을 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우에 는 그 상세한 설명은 생략할 수 있다. 본 발명의 구성 요소를 설명하는 데 있어서, 제 1, 제 2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이러한 용어는 그 구성 요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성 요소의 본질, 차례, 순서 또는 개수 등이 한정되지 않는다. 어떤 구성 요소가 다른 구성요소에 \"연결\", \"결합\" 또는 \"접속\"된다고 기재된 경우, 그 구성 요소는 그 다른 구성요소에 직접적으로 연결되거나 또는 접속될 수 있지만, 각 구성 요소 사이에 다른 구성 요소가 \"개재\"되거나, 각 구성 요소가 다른 구성 요소를 통해 \"연결\", \"결합\" 또는 \"접 속\"될 수도 있다고 이해되어야 할 것이다. 또한, 본 발명을 구현함에 있어서 설명의 편의를 위하여 구성요소를 세분화하여 설명할 수 있으나, 이들 구성요 소가 하나의 장치 또는 모듈 내에 구현될 수도 있고, 혹은 하나의 구성요소가 다수의 장치 또는 모듈들에 나뉘 어져서 구현될 수도 있다. 이하, 본 명세서에서 로봇은 특정한 목적(청소, 보안, 모니터링, 안내 등)을 가지거나 혹은 로봇이 이동하는 공 간의 특성에 따른 기능을 제공하며 이동하는 장치를 포함한다. 따라서, 본 명세서에서의 로봇은 소정의 정보와 센서를 이용하여 이동할 수 있는 이동수단을 보유하며 소정의 기능을 제공하는 장치를 통칭한다. 본 명세서에서 로봇은 맵을 보유하면서 이동할 수 있다. 맵은 공간에서 이동하지 않는 것으로 확인된 고정된 벽, 계단 등 고정 객체에 대한 정보를 의미한다. 또한, 주기적으로 배치되는 이동 장애물, 즉 동적인 객체들에 대한 정보도 맵 상에 저장될 수 있다. 일 실시예로 로봇의 진행 방향을 기준으로 일정한 범위 내에 배치된 장애물들에 대한 정보도 맵 상에 저장될 수 있다. 이 경우, 전술한 고정 객체가 저장되는 맵과 달리 임시적으로 장애물들의 정보가 맵에 등록되고 이후 로 봇이 이동한 후 맵에서 제거될 수 있다. 또한, 본 명세서에서 로봇은 다양한 센서들을 이용하여 외부의 동적 객체를 확인할 수 있다. 외부의 동적 객체 를 확인하면, 보행자로 붐비는 환경에서 로봇이 목적지까지 주행할 때, 목적지까지 거쳐가야 하는 경유 지점 (Waypoint)의 장애물에 의한 점유 상황을 확인할 수 있다. 또한 로봇은 경유 지점의 방향 변경 정도에 따라 유연하게 경유 지점을 도착한 것으로 판단하고 다음 경유 지점 으로 넘어 가도록 하여 목적지까지 성공적으로 주행할 수 있다. 도 1은 본 발명의 일 실시예에 의한 로봇의 외관을 보여준다. 도 1은 예시적인 외관에 해당하며, 도 1의 외관 외에도 다양한 외관으로 본 발명의 로봇을 구현할 수 있다. 특히, 각각의 구성요소는 로봇의 형상에 따라 상하 좌우 전후 등에서 다른 위치에 배치될 수 있다. 본체는 상하 방향으로 길이가 길게 형성되며, 전체적으로 하부에서 상부 방향으로 올라갈수록 슬림해지는 오뚝이 형상을 가질 수 있다. 본체는 로봇의 외관을 형성하는 케이스를 포함할 수 있다. 케이스는 상측에 배치되는 탑 커버 , 탑 커버의 하측에 배치되는 제1 미들 커버, 제1 미들 커버의 하측에 배치되는 제2 미들 커버 및 제2 미들 커버의 하측에 배치되는 바텀 커버를 포함할 수 있다. 여기서 제1 미들 커버와 제2 미들 커버는 하나의 미들 커버로 이루어질 수 있다. 탑 커버는 로봇의 최상단에 위치되며, 반구 또는 돔 형상을 가질 수 있다. 탑 커버는 사용자로부터 명령을 용이하게 입력 받기 위하여 성인의 키보다 낮은 높이에 위치될 수 있다. 그리고 탑 커버는 소정각도 회전 가능하도록 구성될 수 있다. 한편, 로봇은 그 내부에 제어모듈을 더 포함할 수 있다. 제어모듈은 일종의 컴퓨터 또는 프로세서 와 같이 로봇을 제어한다. 따라서 제어모듈은 로봇내에 배치되어 메인 프로세서와 유사한 기능을 수 행하며, 사용자와의 인터랙션(interaction)을 담당할 수 있다. 로봇의 이동과 주변의 사물을 감지하여 로봇을 제어하기 위해 제어모듈이 로봇 내부에 탑재된다. 로봇 제어모듈은 소프트웨어 모듈 또는 이를 하드웨어로 구현한 칩 등으로 구현 가능하다. 탑 커버는 전면 일측에 사용자로부터 명령을 입력받거나 정보를 출력하는 디스플레이부(31a)과 카메라 (31b), 마이크(31c)를 일 실시예로 하는 센서가 배치될 수 있다. 또한, 탑 커버의 디스플레이부(31a) 외에도 미들 커버의 일측에도 디스플레이부가 배치된다. 로봇의 기능에 따라 두 개의 디스플레이부(31a, 20) 모두 정보를 출력하거나 혹은 어느 한쪽에서만 정보가 출력 될 수 있다. 한편, 로봇의 일측면 또는 하단부 전체에는 35a, 35b와 같이 다양한 장애물 센서(도 2의 220)들이 배치된다. 장애물 센서들은 TOF(Time of Flight) 센서, 초음파 센서, 적외선 센서, 뎁스 센서, 레이저 센서, 라이다 센서등을 일 실시예로 한다. 센서들은 다양한 방식으로 로봇 외부의 장애물을 감지한다. 또한, 도 1의 로봇은 하단부에 로봇을 이동시키는 구성요소인 이동부를 더 포함한다. 이동부는 일종의 바퀴와 같이 로봇을 이동시키는 구성요소이다. 도 1의 로봇의 형상은 예시적이며, 본 발명이 이에 한정되는 것은 아니다. 또한, 로봇의 다양한 카메라들과 센 서들 역시 로봇의 다양한 위치에 배치될 수 있다. 도 1의 로봇은 사용자에게 정보를 안내하고 특정 지점까지 이동하여 사용자를 안내하는 안내 로봇을 일 실시예로 한다. 이외에도 청소, 보안 또는 기능을 제공하는 로봇 역시 도 1의 로봇의 범위에 포함된다. 다양한 기능을 제공할 수 있으나, 본 명세서에서는 설명의 편의를 위해 안내 로봇을 중심으로 설명한다. 도 1를 일 실시예로 하는 로봇이 서비스 공간 내에 다수 배치된 상태에서 로봇이 특정한 기능(안내, 청소, 보안 등)을 수행한다. 이 과정에서 로봇은 자신의 위치를 저장하며, 로봇은 전체 공간에서 자신의 현재 위치를 확인하고, 목표 지점으로 이동하는데 필요한 경로를 생성할 수 있다. 도 2는 본 발명의 일 실시예에 의한 로봇의 제어모듈의 구성 요소를 보여준다. 로봇은 맵을 생성하는 기능과 맵을 이용하여 로봇의 위치를 추정하는 기능 둘 다 수행할 수 있다. 또는 로봇은 맵을 생성하는 기능만 제공할 수 있다. 또는 로봇은 맵을 이용하여 로봇의 위치를 추정하는 기능만 제공할 수 있다. 이하, 본 발명의 로봇은 맵을 이용 하여 로봇의 위치를 추정하는 기능을 주로 제공한다. 추가적으로 로봇은 맵을 생성하거나 수정하는 기능을 제공 할 수 있다. 라이다 센서(LiDAR Sensor)는 2차원 또는 3차원으로 주변의 사물들을 센싱할 수 있다. 2차원 라이다 센서 의 경우 로봇을 중심으로 360도 범위의 사물의 위치를 센싱할 수 있다. 특정 위치에서 센싱한 라이다 정보는 하 나의 라이다 프레임을 구성할 수 있다. 즉, 라이다 센서는 로봇의 외부에 배치된 사물과 로봇 사이의 거리 를 센싱하여 라이다 프레임을 생성한다. 카메라 센서는 일반 카메라를 일 실시예로 한다. 시야각의 제약을 해결하기 위해 둘 이상의 카메라 센서 를 사용할 수 있다. 특정 위치에서 촬영한 영상은 비전 정보를 구성한다. 즉, 카메라 센서는 로봇의 외부에 배치된 사물을 촬영하여 비전 정보를 포함하는 비주얼 프레임을 생성한다. 이하 본 발명을 적용하는 로봇은 라이다 센서와 카메라 센서를 이용한 퓨젼-SLAM(Fusion- simultaneous localization and mapping)을 수행한다. 퓨전 SLAM은 라이다 정보와 비전 정보를 결합하여 사용할 수도 있다. 이들 라이다 정보와 비전 정보는 맵으로 구성할 수 있다. 로봇이 퓨전 SLAM을 사용할 경우, 하나의 센서만을 사용하는 방식(LiDAR-only SLAM, visual-only SLAM)와 비교 하여 위치 추정의 정확도가 높다. 즉, 라이다 정보와 비전 정보를 결합하여 퓨전 SLAM을 수행하면 맵 품질(map quality) 측면에서 더 좋은 맵을 획득할 수 있다. 여기서, 맵 품질이란 비전 정보들로 구성된 비전 맵(vision map)과 라이다 정보로 구성된 라이다 맵(lidar map) 양쪽에 모두 해당하는 기준이다. 퓨전 SLAM 시 각각의 맵 품질이 좋아지는데 이는 각각의 센서가 획득하지 못하 거나 부족한 정보를 센서들이 서로 이용할 수 있기 때문이다. 또한, 하나의 맵에서 라이다 정보 또는 비전 정보만을 추출하여 사용할 수 있다. 예를 들어, 로봇이 보유하는 메모리의 양이나 연산 프로세서의 연산 능력 등에 적합하게 라이다 정보만을 이용하거나, 비전 정보만을 이용하 거나 또는 두 정보 모두를 로봇의 위치 추정(localization)에 적용할 수 있다. 인터페이스부는 사용자로부터 정보를 입력받는다. 터치 입력, 음성 입력 등 다양한 정보를 사용자로부터 입력받고, 이에 대한 결과를 출력한다. 또한 인터페이스부는 로봇이 저장하는 맵을 출력하거나, 로봇이 이동하는 과정을 맵과 오버랩 하여 출력할 수 있다. 또한, 인터페이스부는 사용자에게 소정의 정보를 제공할 수 있다. 제어부는 후술할 도 4와 같은 맵을 생성하고 이 맵을 기반으로 로봇이 이동하는 과정에서 로봇의 위치를 추정한다. 통신부는 로봇이 다른 로봇 또는 외부의 서버와 통신하여 정보를 송수신할 수 있도록 한다. 로봇은 각각의 센서들(라이다 센서, 카메라 센서)을 이용하여 각각의 맵을 생성할 수 있다. 또는 로봇은 이들 센서들을 이용하여 하나의 맵을 만든 후 맵에서 다시 이들로부터 특정 센서에 해당하는 내용만을 추출하는 맵을 생성할 수 있다. 또한, 본 발명의 맵은 바퀴의 회전에 기반한 오도메트리(odometry) 정보를 포함할 수 있다. 오도메트리 정보는 로봇의 바퀴 회전 횟수나 양 바퀴의 회전 횟수의 차이 등을 이용하여 로봇이 이동한 거리를 산출한 정보이다. 센서들을 이용한 정보 외에도 오도메트리 정보를 이용하여 로봇이 어디까지 이동했는지 로봇이 계산할 수 있다. 도 2의 제어부는 인공지능 작업 및 처리를 위한 인공지능부를 더 포함할 수 있다. 로봇의 라이다 센서 및 카메라 센서는 외부의 물체를 식별하기 위해 로봇의 외부에 다수 배치할 수 있다. 도 2에 제시된 라이다 센서 및 카메라 센서 외에도 로봇의 외부에는 다양한 종류의 센서들(라이다 센서, 적외선 센서, 초음파 센서, 뎁스 센서, 이미지 센서, 마이크 등)이 배치된다. 제어부는 센서들이 센 싱한 정보를 취합 및 처리한다. 인공지능부는 라이다 센서 및 카메라 센서, 그리고 그 외의 센서들이 처리한 정보 또는 로봇 이 이동 과정에서 누적 저장한 정보 등을 입력하여 제어부가 외부 상황을 판단하거나, 정보를 처리하거 나, 이동 경로를 생성하는데 필요한 결과물을 출력할 수 있다. 일 실시예로, 로봇은 로봇이 이동하는 공간에 배치된 다양한 사물들의 위치 정보를 맵으로 저장할 수 있다. 사물들은 벽, 문 등의 고정 사물들과 화분, 책상 등 이동 가능한 사물들을 포함한다. 인공지능부는 맵 정 보와 라이다 센서 및 카메라 센서, 그리고 그 외의 센서들이 제공하는 정보들을 이용하여 로봇이 이 동할 경로, 혹은 로봇이 작업시 커버해야 할 범위 등에 대한 데이터를 출력할 수 있다. 또한 인공지능부는 라이다 센서 및 카메라 센서, 그리고 그 외의 센서들이 제공하는 정보들을 이용하여 로봇 주변에 배치된 사물을 인식할 수 있다. 인공지능부는 이미지를 입력받아 이미지에 관한 메 타 정보를 출력할 수 있다. 메타 정보란 이미지 내의 사물의 명칭, 사물과 로봇과의 거리, 사물의 종류, 사물이 맵 상에 위치하는지 여부 등을 포함한다. 라이다 센서 및 카메라 센서, 그리고 그 외의 센서들이 제공하는 정보는 인공지능부의 딥러닝 네트워크의 입력 노드로 입력된 후, 인공지능부의 딥러닝 네트워크의 히든 레이어의 정보 처리를 통해 인 공지능부의 출력 노드에서 결과가 출력된다. 제어부는 인공지능부가 산출한 데이터 또는 다양한 센서들이 처리한 데이터를 이용하여 로봇의 이동 경로를 산출할 수 있다. 도 2의 로봇은 전술한 바와 같이 맵을 생성하는 기능과 맵을 이용하여 주행하며 위치를 추정하는 기능 중 어느 하나 이상을 포함할 수 있다. 도 2의 로봇이 맵을 생성하거나 또는 맵을 이용하여 위치를 추정하는 공간의 예시 로 도 3을 살펴볼 수 있다. 도 3은 로봇이 공간에서 이동하는 과정을 보여준다. 공간 내에서 로봇은 41이 지시하는 선을 따라 이동하며 라이다 센서를 이용하여 특정 지점에서 라이다 센서가 센싱한 정보를 맵 저장부에 저장할 수 있다. 공간의 기초 형상은 로컬 맵(local map)으로 저장될 수 있다. 마찬가지로 로봇은 공간을 이동하면서 카메라 센서를 이용하여 특정 지점에서 카메라 센서가 센싱한 정보를 맵 저장부에 저장할 수 있다. 또는, 도 3의 공간에서 로봇은 이동하며, 앞서 맵 저장부에 저장된 정보와 비교하여 로봇의 현재 위치를 확인할 수 있다. 도 4는 본 발명의 일 실시예에 의한 맵의 다중 구조를 보여준다. 도 4는 백본(backbone)을 제1레이어(first layer)로 하며 라이다 브랜치(LiDAR branch) 및 비주얼 브랜치(Visual branch)를 각각 제2레이어(second layer)로 하는 2중층의 구성을 보여준다. 도 4와 같은 구조를 구조적으로 탄력적인 포즈 그래프 기반 슬램 (structurally elastic pose graph-based SLAM)이라 명명한다. 백본은 로봇의 궤적(trajectory)을 추적한 정보이다. 또한 백본은 궤적에 대응하는 하나 이상의 프레임 노드들 을 포함한다. 그리고 이들 프레임 노드들은 다른 프레임 노드와의 관계에서 제약 정보(constraint)를 더 포함한 다. 노드 사이의 에지는 제약 정보를 나타낸다. 에지는 오도메트리 제약 정보(odometry constraint) 또는 루프 제약 정보(loop constraint)를 의미한다. 또한, 제2레이어의 라이다 브랜치는 라이다 프레임(LiDAR Frame)들로 구성된다. 라이드 프레임은 로봇의 이동 과정에서 센싱한 라이다 센싱 값을 포함한다. 이들 라이다 프레임들 중에서 적어도 하나 이상은 라이다 키프레 임(LiDAR Keyframe)으로 설정된다. 라이다 키프레임은 백본의 노드와 대응관계를 가진다. 도 4에서 백본의 노드들 v1 내지 v5 중에서 v1, v2, v4, v5가 라이다 키프레임을 지시한다. 마찬가지로, 제2레이어의 비주얼 브랜치는 비주얼 키 프레임(Visual Keyframe)들로 구성된다. 비주얼 키프레임 은 로봇의 이동 과정에서 센싱한 카메라 센싱 값(즉, 카메라로 촬영한 영상)인 비주얼 피쳐 노드(visual feature node)들을 하나 이상 지시한다. 로봇에 배치된 카메라 센서의 수에 따라 로봇은 다수의 비주얼 피쳐 노 드를 생성할 수 있다. 즉, 도 4의 맵 구조에서는 백본의 프레임 노드에 라이다 키프레임이 연결되거나 또는 비주얼 키프레임이 연결되 는 구성이다. 물론 라이다/비주얼 키프레임 모두 하나의 프레임 노드에 연결될 수 있다(v1, v4, v5). 각 프레임 노드와 연결된 라이다 또는 비주얼 키프레임의 로봇의 포즈는 같다. 다만, 라이다 센서 또는 카메라 센서가 로봇에 부착된 위치에 따라 외부 파라미터(extrinsic parameter)가 키프레임 별로 추가될 수 있다. 외부 파라미터란 로봇 중심으로부터의 센서가 부착된 상대적인 위치 정보를 의미한다. 비주얼 키프레임은 백본의 노드와 대응관계를 가진다. 도 4에서 백본의 노드들 v1 내지 v5 중에서 v1, v3, v4, v5가 비주얼 키프레임을 지시한다. 도 2에서 비주얼 피쳐 노드(비주얼 프레임)들은 두 개가 한 쌍으로 구성되는 데, 이는 로봇이 두 개의 카메라 센서를 포함하여 영상을 촬영함을 의미한다. 카메라 센서의 증감 에 따라 각각의 위치에서 비주얼 피쳐 노드의 수 역시 증감한다. 제1레이어의 백본을 구성하는 노드들(v1~v5) 사이에는 에지(edge)가 표시되어 있다. e12, e23, e34, e45는 인접 노드들 사이의 에지이며, e13, e35, e25는 인접하지 않은 노드들 사이의 에지이다. 오도메트리 제약 정보 또는 줄여서 오도메트리 정보는 e12, e23, e34, e45와 같이 인접한 프레임 노드 사이의 제약 조건을 의미한다. 루프 제약 정보 또는 줄여서 루프 정보는 e13, e25, e35와 같이 인접하지 않은 프레임 사이의 제약 조건을 의미한다. 백본은 다수의 키프레임들로 구성된다. 다수의 키프레임을 백본으로 추가하기 위해 제어부는 초기 매핑 과 정(initial mapping process)를 수행할 수 있다. 초기 매핑은 키프레임 기반으로 라이다 키프레임과 비주얼 키 프레임을 추가한다. 도 4의 구조를 정리하면 다음과 같다. 라이다 브랜치는 하나 이상의 라이다 프레임을 포함한다. 비주얼 브랜치 는 하나 이상의 비주얼 프레임을 포함한다. 그리고 백본은 라이다 프레임 또는 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함한다. 이때, 프레임 노드에 등록되는 라이다 프레임 또는 비주얼 프레임을 키프레임이라 지칭한다. 그리고 포즈 그래프는 이들 라이다 브랜치, 비주얼 브랜치, 백본을 포함한다. 뿐만 아니라, 포즈 그래프는 프레임 노드 사이의 오도메트리 정보 및 루프 정보 등을 포함한다. 오도메트리 정 보는 로봇이 프레임 노드 사이를 이동하여 생성한 휠의 회전이나 방향 등을 포함한다. 루프 정보는 라이다 센서 의 최대 센싱 거리 내에서 특정한 프레임 노드를 중심으로 비주얼 키프레임 사이에 특정한 제약 조건으로 연결된 프레임 노드 셋에 기반한다. 도 4의 포즈 그래프는 제어부가 생성한다. 제어부는 라이다 브랜치, 비주얼 브랜치, 백본, 프레임 노 드 사이의 오도메트리 정보 및 이를 포함하는 포즈 그래프를 맵 저장부에 저장한다. 전술한 바와 같이, 도 4와 같은 포즈 그래프는 맵 생성 기능을 제공하는 로봇에 의해 생성되어 주행 기능을 제 공하는 모든 로봇의 맵 저장부에 저장될 수 있다. 특히, 도 4의 포즈 그래프는 카메라 센서와 라이다 센서 중 어느 하나만 작동 가능한 경우에도 로봇 이 위치 추정에 이용할 수 있다. 또는 로봇의 센서들 중 어느 하나의 정확도가 낮거나 어느 하나의 센서가획득한 정보에 대응하는 위치가 다수 검출된 경우에 로봇은 또다른 센서를 이용하여 위치 추정의 정확도를 높일 수 있다. 예를 들어, 하나 이상의 카메라 센서와 하나 이상의 라이다 센서를 모두 포함하는 로봇이 퓨전 SLAM(Fusion-SLAM)으로 멀티 센서가 획득한 정보를 이용하여 위치를 추정할 수 있다. 각각의 센서를 이용하여 추정한 위치 추정 결과 중 하나라도 참인 경우가 존재하면 로봇은 위치 추정이 가능하다. 또는 로봇의 각 센서 중 일부 센서만 동작 가능하거나 어느 하나의 센서만을 포함하는 경우에도 맵 저장부 에 저장된 각 센서 별 저장된 정보를 이용하여 위치 추정이 가능하다. 하나의 라이다 센서를 사용할 경우 라이다 센서는 360도를 커버할 수 있다. 도 5는 본 발명의 일 실시예에 의한 다양한 센서를 이용하여 위치를 추정하는 과정을 보여준다. 로봇은 제1센서 및 제2센서와 같이 두 종류의 센서가 장착되어 위치 추정을 수행할 수 있다. 도 2에 도시된 바와 같이 제1센서는 라이다 센서를 일 실시예로 하고, 제2센서는 카메라 센서를 일 실시예로 한다. 또는 그 반대의 경우에도 적용가능하다. 제어부는 제1센서가 획득한 정보와 맵 저장부에 저장된 정보를 비교한다(S41). 일 실시예로 제어부는 맵 저장부에 저장된 포즈 그래프의 프레임 노드에 등록된 정보와 제1센서가 획득한 정보를 비교할 수 있다. 그리고 제어부는 제2센서가 획득한 정보가 있는지 확인한다(S42). 제2센서가 획득한 정보가 있는 경우, 제 어부는 제2센서가 획득한 정보에 기반하여 제1센서가 획득한 정보의 정확도를 검증한다(S43). 그리고 제어부는 가장 높은 정확도의 정보를 이용하여 위치 추정을 수행한다(S44). 한편, S42에서 제2센서가 획득한 정보가 없을 경우, 제어부는 제1센서가 획득한 정보를 이용하여 위치 추 정을 수행한다(S45). 이때, 제어부는 휠의 회전이나 방향, 이동 거리 등에 기반한 오도메트리 정보를 추가 적으로 이용하여 위치 추정을 수행할 수 있다. 도 6은 본 발명의 일 실시예에 의한 라이다 센서와 카메라 센서를 이용하여 로봇의 위치를 추정하는 과정을 보 여준다. 먼저, 카메라 센서만 사용가능하거나, 라이다 센서가 유의미한 정보를 획득할 수 없는 경우를 살펴본 다. 이는 도 5의 제1센서가 카메라 센서이고, 제2센서가 라이다 센서이며, S41-S42-S45의 단계로 위 치 추정을 수행하는 과정에 해당한다. 제어부는 카메라 센서가 획득한 정보(Image, 51)와 로봇의 이동부의 휠에서 산출된 휠 오도메트리 (Wheel Odometry, 52)를 이용하여 맵저장부(210a)의 포즈 그래프에 등록된 비주얼 프레임들과 비교한다. 이때, 제어부는 휠 오도메트리 정보를 이용하여 현재 로봇이 저장한 전체 비주얼 프레임들 중 일부만을 선별 하여 비교한다. 그 결과 비주얼 프레임의 비교에 필요한 시간과 연산 자원을 줄일 수 있다. 제어부는 51과 210a 내의 비주얼 프레임을 비교한 결과 현재 로봇의 위치를 추정할 수 있다. 이는 라이다 센서의 센싱 값을 이용하지 않는 실시예이다. 제어부는 S51(비주얼 프레임 추출) 및 S53(위치 추정) 단계를 수행하여 비주얼 SLAM(Visual SLAM)을 수행하여 로봇의 위치를 추정할 수 있다. 특히 제어부는 휠 오도메트리를 이용하므로 정확도를 높이고 연산 시간을 줄일 수 있다. 다음으로, 라이다 센서만 사용가능하거나, 카메라 센서가 유의미한 정보를 획득할 수 없는 경우를 살 펴본다. 이는 도 5의 제1센서가 라이다 센서이고, 제2센서가 카메라 센서이며, S41-S42-S45의 단계로 위치 추정을 수행하는 과정에 해당한다. 제어부는 라이다 센서가 획득한 정보(LiDAR scan data, 53)와 로봇의 이동부의 휠에서 산출된 휠 오도메트리(Wheel Odometry, 52)를 이용하여 맵저장부(210a)의 포즈 그래프에 등록된 라이다 프레임들과 비교한 다. 이때, 제어부는 휠 오도메트리 정보를 이용하여 현재 로봇이 저장한 전체 라이다 프레임들 중 일부 만을 선별하여 비교한다. 그 결과 라이다 프레임의 비교에 필요한 시간과 연산 자원을 줄일 수 있다. 제어부는 53과 210a 내의 라이다 프레임을 비교한 결과 55와 같이 현재 로봇의 위치를 추정할 수 있다. 이 는 카메라 센서의 센싱 값을 이용하지 않는 실시예이다. 제어부는 S51(라이다 프레임 추출) 및 S52 (라이다 프레임과 비교를 통한 추정) 단계를 수행하여 라이다 SLAM(LiDAR SLAM)을 수행하여 로봇의 위치를 추정할 수 있다. 특히 제어부는 휠 오도메트리를 이용하므로 정확도를 높이고 연산 시간을 줄일 수 있다. 한편, 두 개의 센서를 모두 이용할 수 있는 경우의 실시예를 살펴본다. 다음으로, 라이다 센서 및 카메라 센서가 유의미한 정보를 획득할 수 있는 경우를 살펴본다. 이는 도 5의 제1센서가 라이다 센서이고, 제2센서가 카메라 센서인 경우 또는 도 5의 제2센서가 라이다 센서이고, 제2센서가 카메라 센서(23 0)인 경우 모두에 적용할 수 있다. 그리고 도 5의 S41-S42-S43-S44의 단계로 위치 추정을 수행하는 과정에 해당 한다. 제어부는 라이다 센서가 획득한 정보(LiDAR scan data, 53)와 로봇의 이동부의 휠에서 산출된 휠 오도메트리(Wheel Odometry, 52)를 이용하여 맵저장부(210a)의 포즈 그래프에 등록된 라이다 프레임들과 비교한 다. 이때, 제어부는 카메라 센서가 획득한 이미지를 이용하여 포즈 그래프에 등록된 비주얼 프레 임들과 비교한다. 이 과정에서 이때, 제어부는 휠 오도메트리 정보를 이용하여 현재 로봇이 저장한 전체 라이다 프레임들 중 일부만을 선별하여 비교한다. 마찬가지로 제어부는 휠 오도메트리 정보를 이용하여 현재 로봇이 저 장한 전체 비주얼 프레임들 중 일부만을 선별하여 비교한다. 그 결과 라이다 프레임의 비교 및 비주얼 프레임의 비교에 필요한 시간과 연산 자원을 줄일 수 있다. 제어부는 53과 210a 내의 라이다 프레임을 비교한 결과 55와 같이 현재 로봇의 위치를 추정할 수 있다. 이 는 카메라 센서의 센싱 값을 이용하지 않는 실시예이다. 제어부는 S51(라이다 프레임 추출) 및 S52 (라이다 프레임과 비교를 통한 추정) 단계를 수행하여 라이다 SLAM(LiDAR SLAM)을 수행하여 로봇의 위치를 추정 할 수 있다. 특히 제어부는 휠 오도메트리를 이용하므로 정확도를 높이고 연산 시간을 줄일 수 있다. 제어부는 카메라 센서의 획득 정보를 이용하여 후보가 되는 프레임 노드를 맵 저장부(210a)로부 터 추출한다. 마찬가지로 제어부는 라이다 센서의 획득 정보를 이용하여 후보가 되는 프레임 노 드를 맵 저장부(210a)로부터 추출한다. 그 결과 제어부는 각각 추출된 프레임 노드를 비교하여 어느 중복하는 프레임 노드를 이용하여 로봇의 위 치를 추정할 수 있다. 또는 제어부는 어느 하나의 센서가 획득한 정보를 우선으로 하고, 이들 정보가 다수인 경우에 다른 센서가 획득한 정보를 이용하여 정확도를 높일 수 있다. 예를 들어, 제어부는 맵 저장부(210a)로부터 라이다 센서가 획득한 라이다 프레임과 비교 가능한 라 이다 프레임을 검색 및 다수 추출한 경우를 가정한다. 제어부는 맵 저장부(210a)로부터 카메라 센서 가 획득한 비주얼 프레임과 비교 가능한 비주얼 프레임을 검색하여 하나 이상 추출한다. 제어부는 맵 저장부(210a)로부터 추출된 비주얼 프레임이 등록된 프레임 노드 또는 이와 인접한 프레임 노 드에 등록된 라이다 프레임을 확인하여, 앞서 다수 추출한 라이다 프레임 중에서 정확도가 높은 것을 선택한다. 도 7은 본 발명의 일 실시예에 의한 라이다 센서와 카메라 센서를 이용하여 로봇의 위치를 추정하는 세부 과정 을 보여준다. 도 7은 라이다 센서가 획득한 정보에 기반하여 위치 추정 후보지가 다수인 경우 카메라 센서 를 이용하여 특정 위치 추정 후보지를 선택하는 실시예이다. 라이다 센서가 획득한 라이다 스캔 데이터을 이용하여 제어부가 맵 저장부(210a)로부터 유사한 것으로 추출한 라이다 프레임은 LF1 및 LF2이다. 그런데 LF1는 v1이라는 프레임 노드에 등록되었고, LF2는 v4라 는 프레임 노드에 등록되었으며 둘의 위치가 상이하다. 정확도를 위해 제어부는 카메라 센서가 획득한 비주얼 프레임을 이용한다. 제어부는 맵 저 장부(210a)로부터 51과 유사한 것으로 비주얼 프레임(VF1)을 추출한다. 그 결과 제어부는 VF1은 v4 라는 프레임 노드에 등록된 것을 확인한다. 또한 해당 프레임 노드인 v4에 LF2 가 등록되었다. 따라서 제어부는 LF1 보다 LF2가 더 정확도가 높은 것으로 판단하고, LF2를 기준으로 로봇 의 위치를 추정한다. 도 8은 본 발명의 일 실시예에 의한 라이다 센서와 카메라 센서를 이용하여 로봇의 위치를 추정하는 세부 과정 을 보여준다. 도 8은 카메라 센서가 획득한 정보에 기반하여 위치 추정 후보지가 다수인 경우 라이다 센서 를 이용하여 특정 위치 추정 후보지를 선택하는 실시예이다.카메라 센서가 획득한 카메라 이미지을 이용하여 제어부가 맵 저장부(210a)로부터 유사한 것으로 추출한 비주얼 프레임은 VF1 및 VF2이다. 그런데 VF1는 v1이라는 프레임 노드에 등록되었고, VF2는 v4라는 프레 임 노드에 등록되었으며 둘의 위치가 상이하다. 정확도를 위해 제어부는 라이다 센서가 획득한 라이다 스캔 데이터을 이용한다. 제어부는 맵 저장부(210a)로부터 53과 유사한 것으로 라이다 프레임(LF1)을 추출한다. 그 결과 제어부는 LF1은 v1 이라는 프레임 노드에 등록된 것을 확인한다. 또한 해당 프레임 노드인 v1에는 VF1가 등록된 것을 확인한다. 따라서 제어부는 VF2 보다 VF1가 더 정확도가 높은 것으로 판단하고, VF1을 기준으로 로봇의 위치를 추정한다. 도 6 내지 도 8의 실시예에서 두 종류의 센서를 모두 사용할 수 있는 경우에 위치 추정의 정확도를 높일 수 있 다. 특히 각각의 센서들은 각각 위치 추정한 결과를 산출할 수 있으므로, 제어부는 이들 두 개의 센서들이 산출한 위치 추정 결과를 취합하여 정확도가 높은 위치 추정 결과를 산출할 수 있다. 정리하면, 맵 저장부는 라이다 스캔 데이터를 라이다 프레임으로 저장하고, 이미지를 비주얼 피쳐 노드로 저장한다. 그리고 라이다 프레임 및 비주얼 피쳐 노드 중 일부 또는 전부를 키프레임으로 하여 포즈 그래프의 특정 프레임 노드에 등록한다. 이후 로봇은 맵 저장부에 등록된 라이다 프레임/라이다 키프레임 또는 비주얼 피쳐 노드/비주얼 프레임 과 각각의 센서들이 센싱한 정보를 비교한다. 각각의 센서 둘 다 동일한 프레임 노드의 저장된 정보와 대응하거 나 유사한 경우 해당 위치를 로봇의 현재 위치로 설정한다. 이 과정에서 틀린 위치에 대응하는 센싱한 정보들 또는 맵 저장부에 저장된 정보들은 주행 과정에서 비교 대상에서 제거된다. 전술한 실시예는 각각의 센서가 획득한 정보와 맵 저장부에 저장된 정보를 비교하는 과정에서 제어부는 후 보가 되는 위치를 하나 이상 확인할 수 있다. 여기서 센서는 반드시 카메라 센서와 라이다 센서에 한정되지 않 는다. 예를 들어, IMU(Inertial Measurement Unit) 센서, GPS 센서, 소나(Sonar) 센서, 레이다(Radar) 등 여러 종류 의 센서에서 획득된 후보 위치를 동일한 방식으로 제어부가 획득할 수 있다. 또한 제어부는 이를 통 해 라이다 센서가 획득한 정보에 기반한 위치 정보들의 유효성을 검증할 수 있다. 제어부는 후보 위치에 해당하는 라이다 프레임을 다시 샘플링(Resampling)하여 로봇의 최종 위치를 수렴 및 확정할 수 있다. 따라서, 제어부는 퓨전 SLAM에서 센서를 조합하지 않고 각각에 대한 위치 추정 결과 산출된 후보 위치 (candidate pose)에서 현재 로봇의 위치 추정의 정확도를 높이는 후보를 선택할 수 있다. 전술한 실시예를 정리하면 다음과 같다. 도 2의 맵 저장부는 전술한 포즈 그래프를 저장한다. 맵 저장부 는 제1라이다 프레임과 비교 가능한 다수의 라이다 프레임들을 포함하는 라이다 브랜치를 저장한다. 또한 맵 저장부는 제1비주얼 프레임과 비교 가능한 다수의 비주얼 프레임들을 포함하는 비주얼 브랜치를 저장한 다. 맵 저장부는 저장된 라이다 프레임 또는 저장된 비주얼 프레임 중 어느 하나 이상과 등록된 프레임 노드를 둘 이상 포함하는 백본으로 구성된 포즈 그래프를 저장한다. 또한 맵 저장부는 프레임 노드 사이의 오도메 트리 정보를 저장한다. 한편, 제어부는 포즈 그래프의 프레임 노드에 등록된 라이다 프레임과 라이다 센서가 주행 과정에서 획득한 제1라이다 프레임을 비교한다. 그리고 제어부는 포즈 그래프의 프레임 노드에 등록된 비주얼 프레 임과 카메라센서가 주행 과정에서 획득한 제1비주얼 프레임을 비교한다. 제어부는 제1라이다 프레임에 대응하는 위치를 확인하기 위해 맵 저장부에 저장된 라이다 프레임을 검색한다. 그리고 검색 결과 제어부는 로봇의 후보 위치를 하나 이상 검색하여 포즈 그래프에서 확인할 수 있다. 또한 제어부는 후보 위치 중 어느 하나를 선택하기 위해, 제1비주얼 프레임에 대응하는 위치를 확인할 수 있다. 이 과정에서 제어부는 맵 저장부에 저장된 비주얼 프레임을 검색한다. 검색 결과 제어부는 로봇의 후보 위치를 하나 이상 검색하여 포즈 그래프에서 확인할 수 있다. 제어부는 라이다 프레임에 기반하여 검색된 후보 위치들의 정확도를 비주얼 프레임에 기반하여 검색된 후 보 위치들을 이용하여 판단할 수 있다. 도 9는 본 발명의 일 실시예에 의한 위치 추정 과정을 보여준다. 로봇은 A, B, C, D 위치를 이동한다. 각 위치 에서 로봇의 카메라 센서가 촬영한 정보는 각각 CapVF1~CapVF4 이다. 각 위치에서 로봇의 라이다 센 서가 획득한 스캔 데이터는 각각 CapLF1~CapLF4이다. 한편, 맵 저장부(210b)는 포즈 그래프 및 각각의 라이다 프레임들, 비전 프레임들을 저장한 상태이다. 로봇이 A의 위치에서 B의 위치로 이동할 때, 제어부는 오도메트리 정보를 이용한다. 즉, 제어부는 A 위치에 대응하는 프레임 노드(v1)에서 출발해서 이동한 방향이나 거리 정보를 오도메트리 정보를 이용하여 확 인할 수 있다. 따라서, 로봇이 B의 위치에 도달했을 경우, 제어부는 오도메트리 정보를 이용하여 로봇의 현재 위치가 v1 에서 v3 사이라는 것을 판단할 수 있다. 보다 정확하게 로봇은 B의 위치에 도달했을 경우, 오도메트리 정 보에 기반하여 v2 위치라는 것을 판단하거나 최소한 v2 주변에 있는 것으로 판단할 수 있다. 그 결과 제어부는 획득한 라이다 스캔 데이터, 즉 라이다 센서가 획득한 라이다 프레임인 CapLF2와 v2 주변의 라이다 프레임들(LF12, LF13, LF2, LF21)을 비교할 수 있다. 그 결과 제어부가 어느 하나의 라이 다 프레임(예를 들어 LF2)와 CapLF2가 유사한 것으로 판단할 수 있다. 이 경우, 제어부는 현재 로봇의 위치 를 v2의 위치로 판단할 수 있다. 물론, 이 과정에서 v2의 위치인지 보다 명확하게 판단하기 위해, 제어부는 카메라 센서가 촬영한 이 미지인 비주얼 프레임 CapVF2와 VF11~VF52를 비교할 수 있다. 또는 제어부는 v1~v3 사이의 이동으로 판단하 였으므로, 제어부는 비주얼 프레임 CapVF2와 VF21~VF32를 비교할 수 있다. 다른 실시예로, 제어부가 다른 두 개의 라이다 프레임(예를 들어 LF13과 LF21)와 CapLF2가 유사한 것으로 판 단할 수 있다. 여기서 제어부는 두 개의 라이다 프레임 중에서 정확도가 높은 것을 선택해야 한다. 제어부는 카메라 센서가 촬영한 이미지인 비주얼 프레임 CapVF2와 맵 저장부(210b)에 저장된 비주얼 프레임들을 비교한다. 전체 비교를 할 수도 있지만, 검색 효율을 위해 제어부는 오도메트리 정보를 이용하 여 비교할 비주얼 프레임의 범위를 줄일 수 있다. 또는 제어부는 검색되었던 두 개의 라이다 프레임(예를 들어 LF13과 LF21)에 대응하는 비주얼 프레임을 검 색 범위로 포함할 수 있다. 검색 결과 제어부 CapVF2와 유사한 하나의 비주얼 프레임(예를 들어 VF31)가 유사한 것으로 판단할 수 있다. 이때, 제어부는 앞서 검색된 LF13과 LF21의 위치 정보와 VF31의 위치 정보를 이용하여, 로봇의 위치가 LF21 위치라는 것을 확인한다. 정리하면, 다음과 같다. 제어부는 라이다 센서가 획득한 라이다 프레임인 CapLF2와 유사한 라이다 프 레임인 LF13과 LF21를 맵 저장부(210b)의 포즈 그래프의 프레임 노드로부터 추출한다. 그리고 제어부는 카메라 센서가 촬영한 이미지인 비주얼 프레임 CapVF2와 유사한 비주얼 프레임인 VF31을 맵 저장부(210b)의 포즈 그래프의 프레임 노드로부터 추출한다. 그리고 제어부는 VF31을 이용하여 LF13과 LF21 중 어느 어느 하나를 선택하고, 제어부는 선택한 라이다 프레임을 이용하여 로봇의 현재 위치를 계산한다. 이 과정에서 VF31가 v2 노드에 등록된 경우, 제어부는 v2 노드에 등록된 라이다 프레임이 LF13과 LF21 중 어 느 하나인지를 판단할 수 있다. 또는 등록되지 않은 경우에, 제어부는 오도메트리 정보를 이용하여 가장 최근에 위치가 확인되었던 프레임 노드를 기준으로 위치를 산출할 수 있다. 도 10 및 도 11은 본 발명의 일 실시예에 의한 맵 저장부의 정보를 업데이트 하는 과정을 보여준다. 로봇의 제어부는 이동 과정에서 카메라 센서가 획득한 비주얼 프레임과 맵저장부에 저장된 비주얼 프레임을 비교한다(S61). 그리고 비교 결과 두 비주얼 프레임의 유사도가 설정된 기준값 이상인 경우 (S62), 로봇의 위치는 저장된 비주얼 프레임의 위치 정보에 매우 근접하였음을 의미한다. 따라서, S62에서 두 프레임 간의 유사도가 기준값 보다 높은 경우 제어부는 맵 저장부에 저장된 비주 얼 프레임의 위치 정보를 이용하여 라이다 센서가 획득한 라이다 프레임을 맵 저장부에 저장한다(S63). 그리고 제어부는 로봇의 위치 추정을 수행한다(S64). 도 9에서 만약, CapVF3가 VF41가 동일하거나 매우 유사한 경우를 가정한다. 그렇다면, C의 위치는 v3이라는 프레 임 노드의 위치와 매우 유사하다. 그런데 현재 v3 은 라이다 프레임이 등록되지 않은 상태이다. 따라서, 제어부는 CapLF3을 v3에 등록시킬 수 있다. 그 결과는 도 11과 같다. 도 11은 도 9의 맵 저장부(210b)에 새로운 라이다 프레임(CapLF3)이 추가된 구성을 보여준다. 이는 로봇이 C의 위치에서 획득한 비주얼 프레임(CapVF3)과 VF41와의 유사도가 일정 기준 이상(예를 들어 95% 이상)이면, 제어부 는 VF41 의 위치에 로봇이 도달한 것으로 판단할 수 있다. 또한 VF41 가 등록된 프레임 노드인 v3에는 라이다 프레임이 등록되지 않은 상태이다. 따라서, 맵 정확도를 높이 기 위해 제어부는 C의 위치에서 라이다 센서가 스캔하여 생성한 라이다 프레임인 CapLF3을 v3에 등록 시킬 수 있다. 전술한 실시예들을 적용할 경우, 카메라 센서(비전 센서) 및 라이다 센서를 이용하여 로봇의 독립적인 위치 추 정을 수행할 수 있다. 여기서 독립적이라는 것은 각 센서들이 하나씩만 있어도 로봇의 위치 추정이 가능하다는 의미이다. 따라서, 각각의 센서를 이용하여 로봇은 비전 한정 위치추정(vision-only localization)과 라이다 한정 위치 추정(lidar-only localization)을 수행할 수 있다. 또한 각각의 센서가 다른 센서에게 영향을 미치지 않음을 의 미하며, 오히려 각 센서의 정확도를 다른 센서를 통해 검증할 수 있어 위치 추정의 정확도를 높일 수 있다. 특히, 환경적 요인 등으로 인해 특정 센서가 정확하게 위치추정을 하는데 실패할 경우에도 로봇은 다른 종류 의 센서를 이용하여 퓨전 SLAM을 수행할 수 있다. 예를 들어, 카메라 센서를 하나만 사용하는 경우 렌즈가 다른 사물에 의해 가려지는 상황(occlusion)에서 는 카메라 센서의 시야가 제한되어 위치 추정이 실패할 수 있다. 그러나 둘 이상의 카메라 센서를 이 용하면 다른 방향의 이미지를 이용하여 위치 추정을 수행할 수 있다. 예를 들어, 도 9 및 11에서 VF11/VF12가 한 쌍의 카메라 센서가 동일한 위치에서 촬영한 이미지일 경우 카 메라 센서 중 어느 하나가 가려진 상태라도 다른 카메라 센서로 이미지를 촬영하여 VF11/VF12와 비교할 수 있다. 또한, 카메라 센서의 경우 빛의 변화에 따른 환경 변화에 취약할 수 있는데, 제어부는 라이다 센서 가 획득한 라이다 스캔 데이터를 이용하여 맵 저장부에 저장된 라이다 프레임과 비교하여 위치 추정 을 수행할 수 있도록 한다. 반대로, 라이다 센서가 센싱하는 영역에 반사 물질의 배치나 기하학적 구조의 변경이 발생하여도 제어부 는 카메라 센서가 획득한 이미지 데이터를 이용하여 맵 저장부에 저장된 비주얼 프레임과 비교 하여 위치 추정을 수행할 수 있도록 한다. 따라서, 어느 하나의 센서가 사용할 수 없는 경우 혹은 정확도가 낮은 경우에도 다른 센서가 측정한 값을 이용 하므로, 위치 추정의 정확도 및 강인성(robustness)을 높일 수 있다. 그리고 두 종류의 센서 모두 사용 가능한 시점 제어부는 각 센서들이 획득한 정보를 이용하여 맵 저장부 내의 정보 검색의 속도를 높이거나, 위치 추정의 정확도를 높일 수 있다. 예를 들어 도 11에서 로봇이 획득한 라이다 프레임이 CapLF1인 경우, 제어부는 이를 통해 제1위치 정보 (Pos1) 및 제2위치 정보(Pos2)를 후보 위치로 산출한다. 마찬가지로, 로봇이 획득한 비주얼 프레임이 CapVF1인 경우 제어부는 이를 통해 제3위치 정보(Pos3) 및 제 4위치 정보(Pos4)를 후보 위치로 산출한다. 그리고 제어부는 Pos1과 Pos3/Pos4를 비교하고 Pos2와 Pos3/Pos4를 비교하여 로봇의 위치를 산출할 수 있 다. 또는 제어부는 Pos3과 Pos1/Pos2를 비교하고 Pos4와 Pos1/Pos2를 비교하여 로봇의 위치를 산출할 수 있다. 비교 방식은 맵 저장부에서 프레임을 검색하는데 소요되는 시간, 센서의 정확도 등에 따라 다양하게 설정 될 수 있다. 도 12 및 도 13은 본발명의 일 실시예에 의한 두 종류의 센서를 이용하여 로봇의 후보 위치를 표시하고 최종 위 치를 계산하는 과정을 보여준다. 도 12의 70은 로봇이 이동하는 공간을 그리드로 표시한 것이다. 70과 같이 20x20 크기의 그리드가 맵에 저 장된다. 로봇이 이동하는 과정에서 특정 위치에서 로봇이 획득한 라이다 프레임이 CapLF인 경우, 제어부는 이와 유사한 라이다 프레임을 맵 저장부에서 검색한다. 검색한 결과 그리드 상의 특정 위치에 해당하는 라이다 프레임들이 검색된다. 이들 라이다 프레임의 위치들은 각각 L1, L2, L3가 표시된다. 한편, 로봇이 획득한 주변 이미지 정보인 비주얼 프레임이 CapVF인 경우, 제어부는 이와 유사한 비주얼 프 레임을 맵 저장부에서 검색한다. 검색한 결과 그리드 상의 특정 위치에 해당하는 비주얼 프레임들이 검색 된다. 이들 비주얼 프레임의 위치들은 각각 V1, V2, V3가 표시된다. 제어부는 이들 후보위치들의 거리를 비교한다. 예를 들어 제어부는 다음 두 지점의 거리를 산출한다. L1 - V1의 거리: L1V1 // L1 - V2의 거리: L1V2 // L1 - V3의 거리: L1V3 L2 - V1의 거리: L2V1 // L2 - V2의 거리: L2V2 // L2 - V3의 거리: L2V3 L3 - V1의 거리: L3V1 // L3 - V2의 거리: L3V2 // L3 - V3의 거리: L3V3 그리고 제어부는 산출된 거리들 중에서 가장 짧은 거리의 지점을 산출한다. 도 12에서는 L2V1이 가장 가깝 다. 제어부는 L2와 V1을 기준으로 로봇의 위치를 산출할 수 있다. 제어부는 두 지점의 중간 지점으로 할 수 있다. 또는 제어부는 CapLF와 L2와의 유사도를 반영하여 로봇의 위치 정보를 산출할 수 있다. 또는 제어부는 CapVF와 V1과의 유사도를 반영하여 로봇의 위치를 산출할 수 있다. 즉 제어부는 CapLF와 L2의 유사도와 CapVF와 V1의 유사도를 비교하여 로봇의 위치를 계산할 수 있다. 일 실시예로, CapLF와 L2와의 유사도가 90%이고, CapVF와 V1의 유사도가 95%인 경우, 제어부는 로봇의 위 치가 V1에 더 가까운 것으로 판단할 수 있다. 다만 L2와의 유사도도 90%이므로, V1과 L2 사이의 거리의 중간 지 점에서 V1에 가까운 위치로 로봇의 위치를 설정할 수 있다. 이는 유사도의 산술적인 비례에 따라 혹은 센서의 정확도에 따라 다양하게 선택될 수 있다. 라이다 센서의 정확도가 매우 높다면, 제어부는 L2에 근접하여 로봇의 위치를 설정한다. 일 실시예로 유사도의 기준값이 80%인 경우 제어부는 CapLF와 L2와의 유사도 90%를 10(90-80의 계산결과) 으로 환원할 수 있다. 마찬가지로 제어부는 CapVF와 V1의 유사도 95%를 15(95-80의 계산결과)으로 환원할 수 있다. 그리고 이는 10:15의 차이이므로 2:3의 비율로 환원할 수 있다. 제어부는 V1의 위치 정보(9, 16)와 L2의 위치 정보(8, 17) 사이에서 2:3의 위치를 산출하여 로봇의 위치를 설정한다. 도 13에 도시된 바와 같이, 제어부는 V1의 중심점과 L2의 중심점을 기준으로 2:3인 지점을 로 봇의 위치로 계산한다. 한편, 로봇은 SLAM을 수행하는 과정에서 위치 추정의 정확도가 높은 영역 또는 낮은 영역에서 각각의 센서가 정보를 획득하고 이를 저장할 수 있다. 그리고 저장된 정보들을 인공지능 모듈을 이용하여 학습하여 반복적으로 위치 추정의 정확도가 낮은 영역이나 높은 영역의 획득 정보를 포즈 그래프에 반영할 수 있다. 이를 위해, 제어부의 인공지능부는 일종의 러닝 프로세서(learning processor)이며, 로봇이 누적 하여 저장된 위치 정보 및 센서가 획득한 정보, 그리고 위치 추정의 정확도에 대한 수치값을 처리하여 포즈 그 래프를 업데이트할 수 있다. 인공 지능은 인공적인 지능 또는 이를 만들 수 있는 방법론을 연구하는 분야를 의미하며, 머신 러닝(기계 학습, Machine Learning)은 인공 지능 분야에서 다루는 다양한 문제를 정의하고 그것을 해결하는 방법론을 연구하는 분야를 의미한다. 머신 러닝은 어떠한 작업에 대하여 꾸준한 경험을 통해 그 작업에 대한 성능을 높이는 알고리 즘으로 정의하기도 한다. 인공 신경망(ANN: Artificial Neural Network)은 머신 러닝에서 사용되는 모델로써, 시냅스의 결합으로 네트워 크를 형성한 인공 뉴런(노드)들로 구성되는, 문제 해결 능력을 가지는 모델 전반을 의미할 수 있다. 인공 신경 망은 다른 레이어의 뉴런들 사이의 연결 패턴, 모델 파라미터를 갱신하는 학습 과정, 출력값을 생성하는 활성화 함수(Activation Function)에 의해 정의될 수 있다. 인공 신경망은 입력층(Input Layer), 출력층(Output Layer), 그리고 선택적으로 하나 이상의 은닉층(Hidden Layer)를 포함할 수 있다. 각 층은 하나 이상의 뉴런을 포함하고, 인공 신경망은 뉴런과 뉴런을 연결하는 시냅 스를 포함할 수 있다. 인공 신경망에서 각 뉴런은 시냅스를 통해 입력되는 입력 신호들, 가중치, 편향에 대한 활성 함수의 함숫값을 출력할 수 있다. 모델 파라미터는 학습을 통해 결정되는 파라미터를 의미하며, 시냅스 연결의 가중치와 뉴런의 편향 등이 포함된 다. 그리고, 하이퍼파라미터는 머신 러닝 알고리즘에서 학습 전에 설정되어야 하는 파라미터를 의미하며, 학습 률(Learning Rate), 반복 횟수, 미니 배치 크기, 초기화 함수 등이 포함된다. 인공 신경망의 학습의 목적은 손실 함수를 최소화하는 모델 파라미터를 결정하는 것으로 볼 수 있다. 손실 함수 는 인공 신경망의 학습 과정에서 최적의 모델 파라미터를 결정하기 위한 지표로 이용될 수 있다. 머신 러닝은 학습 방식에 따라 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning)으로 분류할 수 있다. 지도 학습은 학습 데이터에 대한 레이블(label)이 주어진 상태에서 인공 신경망을 학습시키는 방법을 의미하며, 레이블이란 학습 데이터가 인공 신경망에 입력되는 경우 인공 신경망이 추론해 내야 하는 정답(또는 결과 값)을 의미할 수 있다. 비지도 학습은 학습 데이터에 대한 레이블이 주어지지 않는 상태에서 인공 신경망을 학습시키 는 방법을 의미할 수 있다. 강화 학습은 어떤 환경 안에서 정의된 에이전트가 각 상태에서 누적 보상을 최대화 하는 행동 혹은 행동 순서를 선택하도록 학습시키는 학습 방법을 의미할 수 있다. 인공 신경망 중에서 복수의 은닉층을 포함하는 심층 신경망(DNN: Deep Neural Network)으로 구현되는 머신 러닝 을 딥 러닝(심층 학습, Deep Learning)이라 부르기도 하며, 딥 러닝은 머신 러닝의 일부이다. 이하에서, 머신 러닝은 딥 러닝을 포함하는 의미로 사용된다. 로봇은 도 2에서 살펴보았던 인공지능부가 인공지능 기능을 수행할 수 있다. 이 경우, 로봇의 통신부는 유무선 통신 기술을 이용하여 다른 AI 기능을 제공하는 로봇이나 또는 도 9 에서 살펴볼 AI 서버 등의 외부 장치들과 데이터를 송수신할 수 있다. 예컨대, 통신부는 외부 장치들 과 센서 정보, 사용자 입력, 학습 모델, 제어 신호 등을 송수신할 수 있다. 이때, 통신부가 이용하는 통신 기술에는 GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), LTE(Long Term Evolution), 5G, WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), 블 루투스(BluetoothTM), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), ZigBee, NFC(Near Field Communication) 등이 있다. 인터페이스부는 다양한 종류의 데이터를 획득할 수 있다. 이때, 인터페이스부는 영상 신호 입력을 위한 카메라, 오디오 신호를 수신하기 위한 마이크로폰, 사용자로 부터 정보를 입력 받기 위한 사용자 입력부 등을 포함할 수 있다. 여기서, 라이다 센서, 카메라 센서 또는 마이크로폰이 획득한 정보들은 센싱 데이터 또는 센서 정보등 지칭한다. 인터페이스부 및 각종 센서들(220, 230)과 휠 인코더 등은 모델 학습을 위한 학습 데이터 및 학습 모 델을 이용하여 출력을 획득할 때 사용될 입력 데이터 등을 획득할 수 있다. 전술한 구성요소들은 가공되지 않은 입력 데이터를 획득할 수도 있으며, 이 경우 제어부 또는 인공지능부는 입력 데이터에 대하여 전처리 로써 입력 특징점(input feature)을 추출할 수 있다. 인공지능부는 학습 데이터를 이용하여 인공 신경망으로 구성된 모델을 학습시킬 수 있다. 여기서, 학습된 인공 신경망을 학습 모델이라 칭할 수 있다. 학습 모델은 학습 데이터가 아닌 새로운 입력 데이터에 대하여 결 과 값을 추론해 내는데 사용될 수 있고, 추론된 값은 로봇이 어떠한 동작을 수행하기 위한 판단의 기초로 이 용될 수 있다. 이때, 인공지능부는 AI 서버의 러닝 프로세서과 함께 AI 프로세싱을 수행할 수 있다. 이때, 인공지능부는 로봇에 통합되거나 구현된 메모리를 포함할 수 있다. 또는, 인공지능부는 별 도의 메모리 또는 로봇에 결합된 외부 메모리 또는 외부 장치에서 유지되는 메모리를 사용하여 구현될 수도 있다. 로봇은 다양한 센서들을 이용하여 로봇의 내부 정보, 로봇의 주변 환경 정보 및 사용자 정보 중 적어 도 하나를 획득할 수 있다. 이때, 로봇에 포함되는 센서에는 근접 센서, 조도 센서, 가속도 센서, 자기 센서, 자이로 센서, 관성 센서, RGB 센서, IR 센서, 지문 인식 센서, 초음파 센서, 광 센서, 마이크로폰, 라이다 센서, 카메라 센서, 레이더 등이 있다. 또한, 앞서 살펴본 인터페이스부는 시각, 청각 또는 촉각 등과 관련된 출력을 발생시킬 수 있다. 이때, 인터페이스부는 시각 정보를 출력하는 디스플레이부, 청각 정보를 출력하는 스피커, 촉각 정보를 출 력하는 햅틱 모듈 등이 포함될 수 있다. 로봇에 내장된 메모리는 로봇의 다양한 기능을 지원하는 데이터를 저장할 수 있다. 예컨대, 로봇에 내장된 각종 센서들이나 인터페이스부 등이 획득한 입력 데이터, 학습 데이터, 학습 모델, 학습 히스토리 등을 저장할 수 있다. 제어부는 데이터 분석 알고리즘 또는 머신 러닝 알고리즘을 사용하여 결정되거나 생성된 정보에 기초하여, 로봇의 적어도 하나의 실행 가능한 동작을 결정할 수 있다. 그리고, 제어부는 로봇의 구성 요소들을 제어하여 결정된 동작을 수행할 수 있다. 이를 위해, 제어부는 인공지능부 또는 메모리의 데이터를 요청, 검색, 수신 또는 활용할 수 있고, 상 기 적어도 하나의 실행 가능한 동작 중 예측되는 동작이나, 바람직한 것으로 판단되는 동작을 실행하도록 로봇 의 구성 요소들을 제어할 수 있다. 이때, 제어부는 결정된 동작을 수행하기 위하여 외부 장치의 연계가 필요한 경우, 해당 외부 장치를 제어 하기 위한 제어 신호를 생성하고, 생성한 제어 신호를 해당 외부 장치에 전송할 수 있다. 제어부는 사용자 입력에 대하여 의도 정보를 획득하고, 획득한 의도 정보에 기초하여 사용자의 요구 사항 을 결정할 수 있다. 이때, 제어부는 음성 입력을 문자열로 변환하기 위한 STT(Speech To Text) 엔진 또는 자연어의 의도 정보 를 획득하기 위한 자연어 처리(NLP: Natural Language Processing) 엔진 중에서 적어도 하나 이상을 이용하여, 사용자 입력에 상응하는 의도 정보를 획득할 수 있다. 이때, STT 엔진 또는 NLP 엔진 중에서 적어도 하나 이상은 적어도 일부가 머신 러닝 알고리즘에 따라 학습된 인 공 신경망으로 구성될 수 있다. 그리고, STT 엔진 또는 NLP 엔진 중에서 적어도 하나 이상은 인공지능부에 의해 학습된 것이나, AI 서버의 러닝 프로세서에 의해 학습된 것이거나, 또는 이들의 분산 처리에 의 해 학습된 것일 수 있다. 제어부는 로봇의 동작 내용이나 동작에 대한 사용자의 피드백 등을 포함하는 이력 정보를 수집하여 메 모리 또는 인공지능부에 저장하거나, AI 서버 등의 외부 장치에 전송할 수 있다. 수집된 이력 정보는 학습 모델을 갱신하는데 이용될 수 있다. 제어부는 메모리에 저장된 응용 프로그램을 구동하기 위하여, 로봇의 구성 요소들 중 적어도 일부 를 제어할 수 있다. 나아가, 제어부는 는 상기 응용 프로그램의 구동을 위하여, 로봇에 포함된 구성 요 소들 중 둘 이상을 서로 조합하여 동작시킬 수 있다. 또는 로봇과 통신하는 별도의 인공지능 서버(AI server)가 배치되고 로봇이 제공하는 정보를 처리할 수 있다. 도 14는 본 발명의 일 실시예에 의한 AI 서버의 구성을 보여준다. 인공 지능 서버, 즉 AI 서버는 머신 러닝 알고리즘을 이용하여 인공 신경망을 학습시키거나 학습된 인공 신경망을 이용하는 장치를 의미할 수 있다. 여기서, AI 서버는 복수의 서버들로 구성되어 분산 처리를 수 행할 수도 있고, 5G 네트워크로 정의될 수 있다. 이때, AI 서버는 AI 장치의 일부의 구성으로 포함되 어, AI 프로세싱 중 적어도 일부를 함께 수행할 수도 있다. AI 서버는 통신부, 메모리, 러닝 프로세서 및 프로세서 등을 포함할 수 있다. 통신부는 로봇등의 외부 장치와 데이터를 송수신할 수 있다. 메모리는 모델 저장부를 포함할 수 있다. 모델 저장부는 러닝 프로세서을 통하여 학습 중 인 또는 학습된 모델(또는 인공 신경망, 231a)을 저장할 수 있다. 러닝 프로세서는 학습 데이터를 이용하여 인공 신경망(331a)을 학습시킬 수 있다. 학습 모델은 인공 신경 망의 AI 서버에 탑재된 상태에서 이용되거나, 로봇등의 외부 장치에 탑재되어 이용될 수도 있다. 학습 모델은 하드웨어, 소프트웨어 또는 하드웨어와 소프트웨어의 조합으로 구현될 수 있다. 학습 모델의 일부 또는 전부가 소프트웨어로 구현되는 경우 학습 모델을 구성하는 하나 이상의 명령어(instruction)는 메모리 에 저장될 수 있다. 프로세서는 학습 모델을 이용하여 새로운 입력 데이터에 대하여 결과 값을 추론하고, 추론한 결과 값에 기 초한 응답이나 제어 명령을 생성할 수 있다. 도 15는 본 발명의 일 실시예에 의한 로봇이 SLAM 과정에서 인공지능에 기반하여 포즈 그래프를 업데이트하는 과정을 보여준다. 로봇은 SLAM을 수행하는 과정에서 위치 추정의 정확도가 높은 영역 또는 낮은 영역에 대한 정보를 획득한다. 즉, 라이다 센서 또는 카메라 센서가 센싱한 정보와, 이 정보를 이용하여 위치 추정을 수행한 경우의 정확도, 그리고 위치 정보를 인공지능부 또는 AI 서버에게 제공한다(S91). 인공지능부 또는 인공지능서버는 제공된 정보를 이용하여 위치 정보에 따라 각 센서가 획득한 정보의 정확도를 비교한다. 인공지능부 또는 인공지능서버는 정확도가 높거나 낮은 영역에서 획득된 센서 정 보와 기존의 맵 저장부에 저장된 정보를 비교하여 맵 업데이트 여부를 결정한다(S92). 이후, 제어부는 맵 업데이트 결정에 따라 센싱한 정보를 이용하여 맵을 업데이트한다(S93). 일 실시예로 포즈그래프에 등록된 라이다 프레임/비주얼 프레임을 업데이트할 수 있다. 도 15의 프로세스를 적용할 경우, 로봇은 SLAM을 수행하는 과정에서 위치 추정의 정확도가 높은 영역 또는 낮은 영역에서 각각의 센서가 정보를 획득하고 이를 저장할 수 있다. 그리고 저장된 정보들을 인공지능 모듈을 이용하여 학습하여 반복적으로 위치 추정의 정확도가 낮은 영역이나 높은 영역의 획득 정보를 포즈 그래프에 반 영할 수 있다. 로봇은 AI 기술이 적용되어, 안내 로봇, 운반 로봇, 청소 로봇, 웨어러블 로봇, 엔터테인먼트 로봇, 펫 로봇, 무인 비행 로봇 등으로 구현될 수 있다. 로봇은 동작을 제어하기 위한 로봇 제어 모듈을 포함할 수 있고, 로봇 제어 모듈은 소프트웨어 모듈 또는 이 를 하드웨어로 구현한 칩을 의미할 수 있다. 로봇은 다양한 종류의 센서들로부터 획득한 센서 정보를 이용하여 로봇의 상태 정보를 획득하거나, 주변 환경 및 객체를 검출(인식)하거나, 맵 데이터를 생성하거나, 이동 경로 및 주행 계획을 결정하거나, 사용자 상 호작용에 대한 응답을 결정하거나, 동작을 결정할 수 있다. 여기서, 로봇은 이동 경로 및 주행 계획을 결정하기 위하여, 라이다, 레이더, 카메라 중에서 적어도 하나 이 상의 센서에서 획득한 센서 정보를 이용할 수 있다.로봇은 적어도 하나 이상의 인공 신경망으로 구성된 학습 모델을 이용하여 상기한 동작들을 수행할 수 있다. 예컨대, 로봇은 학습 모델을 이용하여 주변 환경 및 객체를 인식할 수 있고, 인식된 주변 환경 정보 또는 객 체 정보를 이용하여 동작을 결정할 수 있다. 여기서, 학습 모델은 로봇에서 직접 학습되거나, AI 서버 등의 외부 장치에서 학습된 것일 수 있다. 이때, 로봇은 직접 학습 모델을 이용하여 결과를 생성하여 동작을 수행할 수도 있지만, AI 서버 등의 외부 장치에 센서 정보를 전송하고 그에 따라 생성된 결과를 수신하여 동작을 수행할 수도 있다. 로봇은 맵 데이터, 센서 정보로부터 검출한 객체 정보 또는 외부 장치로부터 획득한 객체 정보 중에서 적어 도 하나 이상을 이용하여 이동 경로와 주행 계획을 결정하고, 구동부를 제어하여 결정된 이동 경로와 주행 계획 에 따라 로봇을 주행시킬 수 있다. 맵 데이터에는 로봇이 이동하는 공간에 배치된 다양한 객체들에 대한 객체 식별 정보가 포함될 수 있다. 예 컨대, 맵 데이터에는 벽, 문 등의 고정 객체들과 화분, 책상 등의 이동 가능한 객체들에 대한 객체 식별 정보가 포함될 수 있다. 그리고, 객체 식별 정보에는 명칭, 종류, 거리, 위치 등이 포함될 수 있다. 또한, 로봇은 사용자의 제어/상호작용에 기초하여 구동부를 제어함으로써, 동작을 수행하거나 주행할 수 있 다. 이때, 로봇은 사용자의 동작이나 음성 발화에 따른 상호작용의 의도 정보를 획득하고, 획득한 의도 정보 에 기초하여 응답을 결정하여 동작을 수행할 수 있다. 본 발명의 실시예에 의한 제어부는 인공지능 모듈을 탑재할 수 있다. 이 경우, 제어부는 맵 저장부 에 저장된 라이다 프레임 중에서 현재 위치에서 획득된 정보와 유사한 것을 검색하는데 인공지능 모듈을 탑재할 수 있다. 예를 들어 이미지 검색에 딥러닝 네트워크를 이용할 수 있으며, 딥러닝 네트워크를 포함한 제 어부는 이미지 검색 속도를 높일 수 있다. 본 발명의 실시예를 구성하는 모든 구성 요소들이 하나로 결합되거나 결합되어 동작하는 것으로 설명되었다고 해서, 본 발명이 반드시 이러한 실시예에 한정되는 것은 아니며, 본 발명의 목적 범위 내에서 모든 구성 요소들 이 하나 이상으로 선택적으로 결합하여 동작할 수도 있다. 또한, 그 모든 구성 요소들이 각각 하나의 독립적인 하드웨어로 구현될 수 있지만, 각 구성 요소들의 그 일부 또는 전부가 선택적으로 조합되어 하나 또는 복수 개 의 하드웨어에서 조합된 일부 또는 전부의 기능을 수행하는 프로그램 모듈을 갖는 컴퓨터 프로그램으로서 구현 될 수도 있다. 그 컴퓨터 프로그램을 구성하는 코드들 및 코드 세그먼트들은 본 발명의 기술 분야의 당업자에 의해 용이하게 추론될 수 있을 것이다. 이러한 컴퓨터 프로그램은 컴퓨터가 읽을 수 있는 저장매체(Computer Readable Media)에 저장되어 컴퓨터에 의하여 읽혀지고 실행됨으로써, 본 발명의 실시예를 구현할 수 있다. 컴 퓨터 프로그램의 저장매체로서는 자기 기록매체, 광 기록매체, 반도체 기록소자를 포함하는 저장매체를 포함한 다. 또한 본 발명의 실시예를 구현하는 컴퓨터 프로그램은 외부의 장치를 통하여 실시간으로 전송되는 프로그램 모듈을 포함한다. 이상에서는 본 발명의 실시예를 중심으로 설명하였지만, 통상의 기술자의 수준에서 다양한 변경이나 변형을 가 할 수 있다. 따라서, 이러한 변경과 변형이 본 발명의 범위를 벗어나지 않는 한 본 발명의 범주 내에 포함되는 것으로 이해할 수 있을 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15"}
{"patent_id": "10-2019-7023486", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 의한 로봇의 외관을 보여준다. 도 2는 본 발명의 일 실시예에 의한 로봇의 제어모듈의 구성 요소를 보여준다. 도 3은 로봇이 공간에서 이동하는 과정을 보여준다. 도 4는 본 발명의 일 실시예에 의한 맵의 다중 구조를 보여준다. 도 5는 본 발명의 일 실시예에 의한 다양한 센서를 이용하여 위치를 추정하는 과정을 보여준다. 도 6은 본 발명의 일 실시예에 의한 라이다 센서와 카메라 센서를 이용하여 로봇의 위치를 추정하는 과정을 보 여준다. 도 7은 본 발명의 일 실시예에 의한 라이다 센서와 카메라 센서를 이용하여 로봇의 위치를 추정하는 세부 과정 을 보여준다. 도 8은 본 발명의 일 실시예에 의한 라이다 센서와 카메라 센서를 이용하여 로봇의 위치를 추정하는 세부 과정 을 보여준다. 도 9는 본 발명의 일 실시예에 의한 위치 추정 과정을 보여준다. 도 10 및 도 11은 본 발명의 일 실시예에 의한 맵 저장부의 정보를 업데이트 하는 과정을 보여준다. 도 12 및 도 13은 본발명의 일 실시예에 의한 두 종류의 센서를 이용하여 로봇의 후보 위치를 표시하고 최종 위 치를 계산하는 과정을 보여준다. 도 14는 본 발명의 일 실시예에 의한 AI 서버의 구성을 보여준다. 도 15는 본 발명의 일 실시예에 의한 로봇이 SLAM 과정에서 인공지능에 기반하여 포즈 그래프를 업데이트하는 과정을 보여준다."}
