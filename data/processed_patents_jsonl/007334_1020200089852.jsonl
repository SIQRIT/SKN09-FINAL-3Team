{"patent_id": "10-2020-0089852", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0070169", "출원번호": "10-2020-0089852", "발명의 명칭": "음성 신호에서 헤드 모델 애니메이션을 생성하는 방법 및 이를 구현하는 전자 장치", "출원인": "삼성전자주식회사", "발명자": "글라지스토브 이반 빅토로비취"}}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법에 있어서, 상기 음성 신호로부터 상기 음성 신호의 특성 정보를 획득하는 단계;인공지능 모델을 이용하여, 상기 특성 정보로부터 상기 음성 신호에 대응하는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐(viseme) 스트림을 획득하는 단계; 상기 인공지능 모델을 이용하여, 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득하는 단계;상기 음소 스트림 및 상기 비짐 스트림을 병합하는 단계; 및 상기 애니메이션 곡선을 상기 병합된 음소 및 비짐 스트림의 비짐들에 적용하여 헤드 모델 애니메이션을 생성하는 단계를 포함하는, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 인공지능 모델은, 상기 특성 정보로부터 상기 음성 신호에 대응하는 음소 스트림 및 상기음소 스트림에 대응하는 비짐 스트림을 획득하는 제1 인공지능 모델 및 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득하는 제2 인공지능 모델을 포함하는, 음성 신호로부터 헤드 모델 애니메이션을 생성하는방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 인공지능 모델은 기계학습, 신경망, 유전자, 딥러닝, 분류 알고리즘 중 적어도 하나를 이용하여 학습된 인공지능 모델인, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 인공지능 모델은 음성 신호, 상기 음성 신호에 대한 텍스트, 및 상기 음성 신호에 대응하는 비디오 신호만을 이용하여 학습된 인공지능 모델인, 음성 신호로부터 헤드 모델 애니메이션을 생성하는방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 음성 신호로부터 상기 음성 신호의 특성 정보를 획득하는 단계는, Mel-FrequencyCepstral Coefficients (MFCC) 방법 또는 다른 인공지능 모델 중 하나를 이용하여 수행되는, 음성 신호로부터헤드 모델 애니메이션을 생성하는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 특성 정보로부터 상기 음성 신호에 대응하는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐 스트림을 획득하는 단계는,상기 특성 정보로부터 상기 음성 신호의 특성을 추출하는 단계;상기 음성 신호의 특성으로부터 상기 음소 스트림을 획득하는 단계; 및상기 음소 스트림에 포함되는 각 음소에 대응되는 비짐을 선택하여, 상기 비짐 스트림을 획득하는 단계;를 포함하는, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "공개특허 10-2021-0070169-3-제6항에 있어서, 상기 특성 정보로부터 상기 음성 신호의 특징을 추출하는 단계는, 컨볼루션 신경망(convolutional neural network) 또는 순환 신경망(recurrent neural network) 중 적어도 하나를 이용하여 수행되는, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서, 상기 음성 신호의 특성으로부터 상기 음소 스트림을 획득하는 단계는, 상기 인공지능 모델의음소 스트림 형성 함수에 기초하여 수행되고,상기 음소 스트림 형성 함수는, 임의의 음성 신호 및 상기 임의의 음성 신호에 대응하는 텍스트를 포함하는 학습 데이터 세트에 의해 학습되는, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서, 상기 음소 스트림에 포함되는 각 음소에 대응되는 비짐을 선택하여, 상기 비짐 스트림을 획득하는 단계는, 상기 인공지능 모델의 비짐 스트림 형성 함수에 기초하여 수행되고,상기 비짐 스트림 형성 함수는, 임의의 음성 신호 및 상기 임의의 음성 신호에 대응되는 비디오 신호를 포함하는 학습 데이터 세트에 의해 학습되는, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서, 상기 특성 정보로부터 상기 음성 신호에 대응하는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐 스트림을 획득하는 단계는, 상기 인공지능 모델의 음소 정규화 함수를 이용하여 음소에 부분적으로 대응되는 복수의 비짐들 중 하나의 비짐을 선택하는 단계를 포함하는, 음성 신호로부터 헤드 모델 애니메이션을생성하는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서, 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득하는 단계는, 얼굴 움직임 부호화시스템(Facial Action Coding System, FACS)을 사용하여 수행되는, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "음성 신호로부터 헤드 모델 애니메이션을 생성하기 위한 인공지능 모델을 학습시키는 방법에 있어서,음성 신호, 상기 음성 신호에 대응하는 텍스트, 및 상기 음성 신호에 대응하는 비디오 신호를 포함하는 학습 데이터 세트를 획득하는 단계;상기 음성 신호를 상기 인공지능 모델에 입력하여, 상기 인공지능 모델로부터 출력되는 제1 음소 스트림, 상기제1 음소 스트림에 대응되는 비짐 스트림, 및 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득하는단계;상기 제1 음소 스트림과 상기 음성 신호의 텍스트를 이용하여, 상기 인공지능 모델을 위한 음소 스트림 형성 함수를 계산하는 단계;상기 비짐 스트림, 상기 애니메이션 곡선 및 상기 비디오 신호를 이용하여, 상기 인공지능 모델을 위한 비짐 스트림 형성 함수 및 애니메이션 곡선 형성 함수를 계산하는 단계;상기 제1 음소 스트림, 상기 비짐 스트림, 및 상기 애니메이션 곡선에 기초하여, 상기 인공지능 모델을 위한 음소 정규화 함수를 계산하는 단계; 및상기 음소 스트림 형성 함수, 상기 비짐 스트림 형성 함수, 상기 애니메이션 곡선 형성 함수, 및 상기 음소 정규화 함수를 이용하여, 상기 인공지능 모델을 갱신하는 단계;를 포함하는, 음성 신호로부터 애니메이션 헤드 모델을 생성하기 위한 인공지능 모델을 학습시키는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "공개특허 10-2021-0070169-4-제12항에 있어서, 상기 제1 음소 스트림, 상기 제1 음소 스트림에 대응되는 비짐 스트림, 및 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득하는 단계는,상기 음성 신호로부터 상기 음성 신호의 특성 정보를 획득하는 단계;상기 특성 정보를 상기 인공지능 모델에 입력하여, 상기 인공지능 모델로부터 출력되는 상기 제1 음소 스트림및 상기 제1 음소 스트림에 대응되는 비짐 스트림을 획득하는 단계; 및상기 비짐 스트림을 상기 인공지능 모델에 입력하여, 상기 인공지능 모델로부터 출력되는 상기 비짐 스트림에포함된 비짐들의 애니메이션 곡선을 획득하는 단계;를 포함하는, 음성 신호로부터 애니메이션 헤드 모델을 생성하기 위한 인공지능 모델을 학습시키는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서, 상기 음소 스트림 형성 함수를 계산하는 단계는, 상기 음성 신호에 대응되는 텍스트로부터 제2 음소 스트림을 획득하는 단계; 및상기 제1 음소 스트림과 상기 제2 음소 스트림을 비교하여, 음소 스트림 형성 함수를 계산하는 단계; 를 포함하는, 음성 신호로부터 애니메이션 헤드 모델을 생성하기 위한 인공지능 모델을 학습시키는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12항에 있어서, 상기 비짐 스트림 형성 함수 및 애니메이션 곡선 형성 함수를 계산하는 단계는,상기 애니메이션 곡선을 3D 템플릿 모델에 적용하여 비짐 애니메이션을 생성하는 단계;상기 비짐 애니메이션에서 얼굴 랜드마크의 움직임 패턴을 검출하여, 제1 움직임 패턴을 획득하는 단계; 상기 비디오 신호에서 얼굴 랜드마크의 움직임 패턴을 검출하여, 제2 움직임 패턴을 획득하는 단계; 및상기 제1 움직임 패턴과 상기 제2 움직임 패턴을 비교하여, 비짐 스트림 형성 함수 및 애니메이션 곡선 형성 함수를 계산하는 단계; 를 포함하는, 음성 신호로부터 애니메이션 헤드 모델을 생성하기 위한 인공지능 모델을 학습시키는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서, 상기 제1 움직임 패턴을 획득하는 단계는,상기 3D 템플릿 모델의 얼굴 랜드마크를 2D 평면에 투영하는 단계; 및상기 2D 평면에 투영된 얼굴 랜드마크의 움직임에 기초하여 제1 움직임 패턴을 획득하는 단계;를 포함하고, 상기 제2 움직임 패턴을 획득하는 단계는,상기 비디오 신호에서 얼굴 랜드마크를 검출하는 단계;상기 비디오 신호의 얼굴 랜드마크를 중립 얼굴에 정렬하는 단계; 및상기 중립 얼굴에 정렬된 얼굴 랜드마크의 움직임에 기초하여 제2 움직임 패턴을 획득하는 단계;를 포함하는, 전자 장치가 음성 신호로부터 애니메이션 헤드 모델을 생성하기 위한 인공지능 모델을 학습시키는 방법."}
{"patent_id": "10-2020-0089852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "전자 장치로서, 하나 이상의 인스트럭션을 저장하는 메모리; 및공개특허 10-2021-0070169-5-적어도 하나의 프로세서; 를 포함하고,상기 적어도 하나의 프로세서는 상기 하나 이상의 인스트럭션을 실행함으로써, 제1항 내지 제11항 중 어느 하나에 따른 음성 신호로부터 애니메이션 헤드 모델을 생성하는 방법을 수행하는, 전자 장치."}
{"patent_id": "10-2020-0089852", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공지능 모델을 이용하여 음성 신호에서 헤드 모델 애니메이션을 생성하는 방법 및 이 방법을 구현하는 전자 장 치가 개시된다. 개시되는 전자 장치가 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법은, 음성 신호로 부터 상기 음성 신호의 특성 정보를 획득하는 단계; 인공지능 모델을 이용하여, 상기 특성 정보로부터 상기 음성 신호에 대응하는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐(viseme) 스트림을 획득하는 단계; 상기 인공 지능 모델을 이용하여, 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득하는 단계; 상기 음소 스트 림 및 상기 비짐 스트림을 병합하는 단계; 및 상기 애니메이션 곡선을 상기 병합된 음소 및 비짐 스트림의 비짐 들에 적용하여 헤드 모델 애니메이션을 생성하는 단계를 포함한다."}
{"patent_id": "10-2020-0089852", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 일반적으로 컴퓨터 그래픽을 생성하는 방법, 보다 구체적으로는, 인공지능 모델을 이용하여 음성 신 호로부터 헤드 모델 애니메이션을 생성하는 방법 및 그 방법을 구현하는 전자 장치에 관한 것이다."}
{"patent_id": "10-2020-0089852", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "오늘날, 사용자의 아바타(avatar)에 해당하는 다양한 캐릭터들을 애니메이션화함으로써 실제 인물이 존재하는 것과 유사한 효과를 얻을 수 있는 증강 및 가상 현실이 점점 더 많이 사용되고 있다. 예를 들어, 개인화된 3차 원 (3D) 헤드 모델을 생성하고 전화 통화 또는 가상 채팅에서 이를 사용하거나, 다른 언어로 음성을 더빙할 때 헤드 모델을 표시하는 등의 작업을 수행할 수 있다. 이를 위해서는, 음성 신호로부터 헤드 모델 애니메이션을 생성하기 위한 기술적 솔루션이 필요하다. 이러한 솔루션은 실시간으로 고품질의 애니메이션을 제공하고, 음성 신호의 수신과 헤드 모델의 움직임 사이의 지연 시 간을 줄일 수 있어야 한다. 또한, 이러한 작업에 요구되는 컴퓨팅 리소스 소비를 줄일 수 있어야 한다. 한편, 이러한 솔루션은 인공지능 모델을 이용하여 제공될 수 있다. 일반적으로, 종래의 헤드 모델 애니메이션 기술에는 다음과 같은 문제가 있다. - 인공지능 모델을 학습시키기 위해서는 일반적으로 많은 계산 또는 획득하기 어려운 대량의 데이터가 필요하다. - 2차원 랜드마크에 기초하여 얼굴 움직임을 묘사하는 방법들은 일반적으로 3차원 정보의 부족으로 인해 매우 평면적인 애니메이션 결과를 제공한다. - 사람의 얼굴 움직임에 기초하여 가상 캐릭터의 고품질 애니메이션을 획득하기 위해서는 얼굴 형태의 차이로 인해 많은 계산이 요구된다. - 특정 인물이 아닌 임의의 사용자 음성에 대하여 애니메이션 데이터를 일반화하기 어렵다. - 고품질 이미지의 애니메이션 모델은 지연 시간이 길다. 따라서, 상기한 문제점들을 해결하면서 아래에 기술되는 이점들 중 적어도 하나 이상을 제공하는 기술이 요구되 고 있다."}
{"patent_id": "10-2020-0089852", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 목적은, 음성 신호로부터 헤드 모델 애니메이션을 낮은 지연 시간 및 고품질로 실시간 제공할 수 있 는, 인공지능 모델을 이용하여 음성 신호에서 헤드 모델 애니메이션을 생성하는 방법 및 이 방법을 구현하는 전 자 장치를 제공하는데 있다. 또한, 본 개시의 일부 실시예는, 상기 인공지능 모델의 학습을 위해 널리 이용 가능한 데이터를 사용하는 방법 및 이 방법을 구현하는 전자 장치를 제공할 수 있다. 또한, 본 개시의 일부 실시예는, 임의의 목소리에 대한 헤드 모델 애니메이션 또는 임의의 캐릭터의 헤드 모델 애니메이션을 생성하는, 인공지능 모델을 이용하여 음성 신호에서 헤드 모델 애니메이션을 생성하는 방법 및 이 방법을 구현하는 전자 장치를 제공할 수 있다."}
{"patent_id": "10-2020-0089852", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본 개시의 제1 측면은 음성 신호로부터 헤드 모델 애 니메이션을 생성하는 방법을 제안할 수 있다. 상기 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법은, 상기 음성 신호로부터 상기 음성 신호의 특성 정보를 획득하는 단계; 인공지능 모델을 이용하여, 상기 특성 정 보로부터 상기 음성 신호에 대응하는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐(viseme) 스트림을 획득 하는 단계; 상기 인공지능 모델을 이용하여, 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득하는 단계; 상기 음소 스트림 및 상기 비짐 스트림을 병합하는 단계; 및 상기 애니메이션 곡선을 상기 병합된 음소 및 비짐 스트림의 비짐들에 적용하여 헤드 모델 애니메이션을 생성하는 단계를 포함할 수 있다. 또한, 본 개시의 제2 측면은 음성 신호로부터 헤드 모델 애니메이션을 생성하기 위한 인공지능 모델을 학습시키 는 방법을 제안할 수 있다. 상기 음성 신호로부터 헤드 모델 애니메이션을 생성하기 위한 인공지능 모델을 학 습시키는 방법은, 음성 신호, 상기 음성 신호에 대응하는 텍스트, 및 상기 음성 신호에 대응하는 비디오 신호를 포함하는 학습 데이터 세트를 획득하는 단계; 상기 음성 신호를 상기 인공지능 모델에 입력하여, 상기 인공지능 모델로부터 출력되는 제1 음소 스트림, 상기 제1 음소 스트림에 대응되는 비짐 스트림, 및 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득하는 단계; 상기 제1 음소 스트림과 상기 음성 신호의 텍스트를 이용하 여, 상기 인공지능 모델을 위한 음소 스트림 형성 함수를 계산하는 단계; 상기 비짐 스트림, 상기 애니메이션 곡선 및 상기 비디오 신호를 이용하여, 상기 인공지능 모델을 위한 비짐 스트림 형성 함수 및 애니메이션 곡선 형성 함수를 계산하는 단계; 상기 제1 음소 스트림, 상기 비짐 스트림, 및 상기 애니메이션 곡선에 기초하여, 상기 인공지능 모델을 위한 음소 정규화 함수를 계산하는 단계; 및 상기 음소 스트림 형성 함수, 상기 비짐 스 트림 형성 함수, 상기 애니메이션 곡선 형성 함수, 및 상기 음소 정규화 함수를 이용하여, 상기 인공지능 모델 을 갱신하는 단계;를 포함할 수 있다. 또한, 본 개시의 제3 측면은 하나 이상의 인스트럭션을 저장하는 메모리 및 적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는 상기 하나 이상의 인스트럭션을 실행함으로써, 음성 신호로부터 애니 메이션 헤드 모델을 생성하는 방법을 수행하는 전자 장치를 제공할 수 있다. 또한, 본 개시의 제4 측면은 하나 이상의 인스트럭션을 저장하는 메모리 및 적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는 상기 하나 이상의 인스트럭션을 실행함으로써, 음성 신호로부터 헤드 모델 애니메이션을 생성하기 위한 인공지능 모델을 학습시키는 방법을 수행하는 전자 장치를 제공할 수 있다."}
{"patent_id": "10-2020-0089852", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 개시의 실시예를 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 이해를 용이하게 하기 위하여 이하의 설명은 다양한 구체적 세부사항들을 포함하지만, 이들 세부사항들은 단지 예시적인 것으로 간주되어야 한다. 따라서, 당업자는 본 개시의 범위를 벗어나지 않고 이하에 설명된 다양한 실시예에 대하여 다양한 변경 및 수정이 적용될 수 있다는 것을 알 것이다. 또한, 공지된 기능 및 구조에 대한 설명은 명확성 및 간결성을 위해 생략될 수 있다. 본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 개시에 대해 구체적으로 설명하기로 한다. 본 개시에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상 세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"… 부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프 트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 본 개시에 따른 인공지능과 관련된 기능은 프로세서와 메모리를 통해 동작될 수 있다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동 작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어할 수 있다. 또는, 하나 또는 복수의 프로 세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드 웨어 구조로 설계될 수 있다. 본 개시에 따른 음성 신호로부터 애니메이션 헤드 모델을 생성하는 방법에 있어서, 음성 신호에 대응되는 헤드 모델 애니메이션을 추론 또는 예측하기 위하여 인공지능 모델을 이용할 수 있다. 프로세서는 상기 음성 신호 데이터에 대해 전처리 과정을 수행하여 인공지능 모델의 입력으로 사용하는 데에 적합한 형태로 변환할 수 있다. 인공지능 모델은 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계된 인공지능 전용 프로세서에 의해 처리 될 수 있다. 인공지능 모델은 학습을 통해 만들어 질 수 있다. 여기서, 학습을 통해 만들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미한다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경 망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의 해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 추론 예측은 정보를 판단하여 논리적으로 추론하고 예측하는 기술로서, 지식/확률 기반 추론(Knowledge based Reasoning), 최적화 예측(Optimization Prediction), 선호 기반 계획(Preference-based Planning), 추천 (Recommendation) 등을 포함한다. 음소(phoneme)는 단어를 다른 단어와 구분하게 하는, 사용자가 인식하는 소리의 최소 단위이다. 비짐(viseme) 은 하나 이상의 음소와 연관된, 다른 것과 구분하여 식별 가능한 입술의 형상을 나타내는 단위이다. 일반적으 로 음소와 비짐은 1대1 대응되지 않으며, 이는 서로 다른 음성 신호들이 동일한 얼굴 형상에 대응될 수 있음을의미한다. 이하, 첨부된 도면들을 참조하여 본 개시의 다양한 실시예들을 보다 상세하게 설명한다. 도 1은 다양한 실시예들에 따른, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 시스템의 개요도이다. 도 1을 참조하면, 상기 음성 신호로부터 헤드 모델 애니메이션을 생성하는 시스템은 전자 장치를 포함할 수 있다. 전자 장치는 인공지능 모델을 이용하여 음성 신호로부터 헤드 모델 애니메이션을 생성하기 위한 장치 일 수 있다. 전자 장치는 학습 데이터 세트를 이용하여 인공지능 모델을 학습시키기 위한 장치 일 수 있다. 다양한 실시예들에 따르면, 전자 장치는 인공지능 모델, 애니메이션 생성부 및 인 공지능 모델을 학습시키는 학습부를 포함할 수 있다. 전자 장치는 음성 신호를 수신하여 인공지능 모델의 입력으로 전달할 수 있다. 상기 음성 신호는 인 터넷, TV 또는 라디오 방송, 스마트폰, 휴대전화, 보이스 레코더, 데스크톱 컴퓨터, 랩톱 등과 같은 사용 가능 한 모든 소스로부터 수신할 수 있다. 일 실시예에서, 상기 음성 신호는 전자 장치에 포함된 마이크로폰 등의 입력부(미도시)에 의해 실시간으로 수신될 수 있다. 다른 실시예에서, 상기 음성 신호는 전자 장치 에 포함된 통신부(미도시)에 의해 네트워크를 통하여 외부 전자 장치로부터 수신될 수 있다. 다른 실시예에서, 상기 음성 신호는 전자 장치의 메모리 또는 저장 장치에 저장된 오디오 데이터로부터 획득될 수 있다. 전자 장치는 학습 데이터 세트를 수신하여 학습부의 입력으로 전달할 수 있다. 학습 데이터 세 트는 음성 신호, 상기 음성 신호에 대응되는 텍스트, 및 상기 음성 신호에 대응되는 비디오 신호로 구성될 수 있다. 상기 음성 신호, 상기 텍스트, 및 상기 비디오 신호는 다른 얼굴 형태를 가진 다양한 인물의 기록을 포함할 수 있다. 학습 데이터 세트는 학습부가 인공지능 모델을 학습시키기 위하여 학습부 에 제공될 수 있다. 학습 데이터 세트는 전자 장치 내의 메모리 또는 저장 장치에 저장되어 있 을 수 있다. 또는, 학습 데이터 세트는 전자 장치 외부의 저장 장치에 저장되어 있을 수 있다. 인공지능 모델은 음성 신호로부터 헤드 모델 애니메이션을 생성하기 위한 파라미터들을 도출할 수 있다. 인공지능 모델은 음성 신호를 전처리하여 음성 신호의 특성을 나타내는 특성 정보로 변환할 수 있다. 다 양한 실시예들에서, 인공지능 모델은 음성 신호로부터 상기 음성 신호의 특성을 나타내는 특성 계수들을 획득할 수 있다. 인공지능 모델은 음성 신호로부터 추출된 음성 신호의 특성 정보를 입력받아, 상기 음성 신호에 대응하는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐(viseme) 스트림을 출력할 수 있다. 인공지능 모델은 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 출력할 수 있다. 애니메이션 곡 선은 헤드 모델의 움직임에 관련된 애니메이션 파라미터의 시간적 변화를 나타낸다. 일 실시예에서, 애니메이 션 곡선은 각 비짐 애니메이션에서의 얼굴 랜드마크의 움직임 및 비짐 애니메이션의 지속 시간을 지정할 수 있 다. 다양한 실시예들에서, 인공지능 모델은 상기 계수들로부터 상기 음성 신호에 대응하는 음소 스트림 및 상 기 음소 스트림에 대응하는 비짐 스트림을 획득하는 제1 인공지능 모델 및 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득하는 제2 인공지능 모델을 포함할 수 있다. 다양한 실시예들에서, 인공지능 모델은 음성 신호로부터 음소 스트림, 비짐 스트림, 및 애니메이션 곡선을 도출하기 위한 하나 이상의 수치 파라미터 및 함수를 포함할 수 있다. 상기 수치 파라미터는, 인공지능 모델 을 구성하는 복수의 신경망 레이어들 각각의 가중치일 수 있다. 다양한 실시예들에서, 상기 수치 파라미 터 및 함수는 인공지능 모델이 학습하는 데이터에 기초하여 결정되거나 갱신될 수 있다. 일 실시예에서, 인공지능 모델은 음소 스트림 형성 함수에 기초하여 음성 신호로부터 음소 스트림을 예측 할 수 있다. 인공지능 모델은 비짐 스트림 형성 함수에 기초하여 음성 신호로부터 비짐 스트림을 예측할 수 있다. 인공지능 모델은 음소 정규화 함수에 기초하여 음소에 대응되는 복수의 비짐들 중 하나의 비짐 을 선택할 수 있다. 인공지능 모델은 애니메이션 곡선 형성 함수에 기초하여 비짐 스트림의 비짐들의 애 니메이션 곡선을 도출할 수 있다. 인공지능 모델은 상기 음소 스트림 및 상기 비짐 스트림을 후처리할 수 있다. 일 실시예에서, 인공지능 모델은 상기 음소 스트림 및 비짐 스트림을 상기 애니메이션 곡선을 고려하여 오버레이함으로써 병합할 수 있다. 인공지능 모델은 인터넷, 데스크탑 컴퓨터, 랩탑 등과 같은 임의의 이용 가능한 소스로부터 획득될 수 있 고, 전자 장치의 메모리에 저장될 수 있다. 일 실시예에서, 인공지능 모델은 학습 데이터 세트(20 0)에 포함된 데이터들의 적어도 일부를 이용하여 미리 학습된 것일 수 있다. 인공지능 모델은 학습부 의 학습 알고리즘에 따라 학습 데이터 세트를 이용하여 갱신될 수 있다. 애니메이션 생성부는 인공지능 모델로부터 획득한 파라미터들을 헤드 모델에 적용하여, 음성 신호에 대응되는 헤드 모델 애니메이션을 생성할 수 있다. 애니메이션 생성부는 인공지능 모델로부터 병합된 음소 및 비짐 스트림 및 애니메이션 곡선을 획득할 수 있다. 애니메이션 생성부는 애니메이션 곡선을 상기 병합된 음소 및 비짐 스트림에 포함된 비짐들에 적용하여 헤드 모델 애니메이션을 생성할 수 있다. 다양한 실시예들에서, 애니메이션 생성부는 미리 정의된 헤드 모델에 기초하여 헤드 모델 애니메이션을 생 성할 수 있다. 일 실시예에서, 상기 미리 정의된 헤드 모델은 얼굴 움직임 부호화 시스템(Facial Action Coding System, FACS)에 기초한 임의의 3D 캐릭터 모델일 수 있다. FACS는 인간의 얼굴 움직임을 분류하는 시 스템이다. FACS를 사용하여, 임의의 얼굴 표현은 특정한 행동 단위 및 그들의 시간적 분할로 분해하여 부호화 될 수 있다. 예를 들어, 상기 미리 정의된 헤드 모델에서 각 비짐은 FACS 계수로 정의될 수 있다. 일 실시예에서, 애니메이션 생성부는 상기 병합된 음소 및 비짐 스트림에 기초하여, 미리 정의된 헤드 모 델의 비짐 세트를 결정할 수 있다. 애니메이션 생성부는 상기 미리 정의된 헤드 모델의 비짐 세트에 상기 애니메이션 곡선을 적용하여 헤드 모델 애니메이션을 생성할 수 있다. 학습부는 학습 데이터 세트를 이용하여 인공지능 모델을 학습시킬 수 있다. 학습부는 음성 신호, 상기 음성 신호에 대응하는 텍스트, 및 상기 음성 신호에 대응하는 비디오 신호를 포 함하는 학습 데이터 세트를 획득할 수 있다. 학습부는 학습 데이터 세트를 인공지능 모델(11 0)에 입력하여, 인공지능 모델로부터 출력되는 음소 스트림, 비짐 스트림, 및 애니메이션 곡선을 획득할 수 있다. 학습부는 인공지능 모델에 의해 학습 데이터 세트의 음성 신호로부터 생성된 음소 스트림을 학 습 데이터 세트의 텍스트와 비교하여 평가하고, 평가에 기초하여 인공지능 모델을 갱신할 수 있다. 다양한 실시예들에서, 학습부는 상기 제1 음소 스트림과 상기 텍스트를 이용하여, 인공지능 모델을 위한 음소 스트림 형성 함수를 계산할 수 있다. 학습부는 인공지능 모델에 의해 학습 데이터 세트의 음성 신호로부터 생성된 3D 헤드 모델 애니 메이션을 학습 데이터 세트의 비디오 신호와 비교하여 평가하고, 평가에 기초하여 인공지능 모델을 갱신할 수 있다. 다양한 실시예들에서, 학습부는 상기 비짐 스트림, 상기 애니메이션 곡선 및 상기 비디 오 신호를 이용하여, 인공지능 모델을 위한 비짐 스트림 형성 함수 및 애니메이션 곡선 형성 함수를 계산 할 수 있다. 다양한 실시예들에서, 학습부는 상기 제1 음소 스트림, 상기 비짐 스트림, 및 상기 애니메이 션 곡선에 기초하여, 인공지능 모델의 음소 정규화 함수를 계산할 수 있다. 다양한 실시예들에서, 학습부는 상기 음소 스트림 형성 함수, 상기 비짐 스트림 형성 함수, 상기 애니메이 션 곡선 형성 함수, 및 상기 음소 정규화 함수를 이용하여, 상기 인공지능 모델을 갱신할 수 있다. 전자 장치는 스마트폰, 태블릿 PC, PC, 스마트 TV, 휴대폰, PDA(personal digital assistant), 랩톱, 미 디어 플레이어, 마이크로 서버, GPS(global positioning system) 장치, 전자책 단말기, 디지털방송용 단말기, 네비게이션, 키오스크, MP3 플레이어, 디지털 카메라, 가전기기 및 기타 모바일 또는 비모바일 컴퓨팅 장치일 수 있으나, 이에 제한되지 않는다. 예시적으로 전자 장치가 하나의 장치로써 도 1에 도시되어 있으나, 반드시 이에 한정되는 것은 아니다. 전자 장치는 기능적으로 연결되어 상술한 동작들을 수행하는 하나 이상의 물리적으로 분리된 장치들의 집 합일 수 있다. 도 2는 다양한 실시예들에 따른, 음성 신호로부터 헤드 모델 애니메이션을 생성하기 위한 인공지능 모델의 개요 도이다. 도 2를 참조하면, 인공지능 모델은 전처리부, 제1 인공지능 모델, 제2 인공지능 모델, 및 후처리부를 포함할 수 있다. 전처리부는 헤드 모델 애니메이션의 생성을 위해 음성 신호가 이용될 수 있도록 음성 신호를 전처리할 수 있다. 전처리부는 제1 인공지능 모델이 헤드 모델 애니메이션을 생성하기 위하여 획득된 음성 신호 를 이용할 수 있도록, 획득된 음성 신호를 기 설정된 포맷으로 가공할 수 있다. 다양한 실시예들에서, 전처리부는 음성 신호를 전처리하여 음성 신호의 특성을 나타내는 특성 계수들로 변 환할 수 있다. 상기 특성 계수들은 제1 인공지능 모델에 입력되어 음성 신호에 대응되는 음소 스트림 및 비짐 스트림을 예측하기 위하여 사용될 수 있다. 일 실시예에서, 전처리부는 MFCC(Mel-Frequency Cepstral Coefficients) 방법에 의하여 음성 계수를 변환 하여 상기 특성 계수들을 획득할 수 있다. MFCC는 소리의 단구간 스펙트럼을 분석하여 특징을 추출하는 기법으 로써, 특성 계수들은 로그 파워 스펙트럼을 주파수의 비선형 Mel 스케일로 선형 코사인 변환하여 획득될 수 있 다. MFCC 방법은 발화자 및 녹음 조건에 따른 변동성에 크게 영향을 받지 않으며, 별도의 학습 과정을 필요로"}
{"patent_id": "10-2020-0089852", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "하지 않고 계산 속도가 빠르다. MFCC 방법은 기술분야에서 알려져 있으므로, 이에 대한 상세한 설명은 생략한 다. 다른 실시예에서, 상기 특성 계수들은 다른 음성 특성 추출 방법, 예를 들어 지각적 선형 예측 (Perceptual Linear Prediction) 또는 Body Linear Predictive Codes 등의 방법을 사용하여 획득될 수 있다. 다른 실시예에서, 전처리부는 다른 사전 학습된 인공지능 모델에 상기 음성 신호를 입력하여 상기 특성 계수들을 획득할 수 있다. 상기 추가 사전 학습된 인공지능 수단은 순환 신경망, 장단기 메모리 (Long Short- Term Memory, LSTM), 게이트 순환 유닛 (gated recurrent unit, GRU), 이들의 변형, 및 이들의 임의의 조합 중 적어도 하나일 수 있다. 제1 인공지능 모델은 전처리부로부터 제공받은 전처리된 음성 신호로부터, 상기 음성 신호에 대응하 는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐(viseme) 스트림을 출력할 수 있다. 일 실시예에서, 제1 인공지능 모델은 컨볼루션 신경망, 순환 신경망, 장단기 메모리 (Long Short-Term Memory, LSTM), 게이트 순환 유닛 (gated recurrent unit, GRU), 이들의 변형, 또는 이들의 임의의 조합 중 적어도 하나일 수 있다. 다양한 실시예들에서, 제1 인공지능 모델은 음소 스트림 형성 함수에 기초하여 음성 신호의 특성 계수로부 터 음소 스트림을 예측할 수 있다. 다양한 실시예들에서, 제1 인공지능 모델은 비짐 스트림 형성 함수에 기초하여 음성 신호의 특성 계수로부터 음소 스트림에 대응되는 비짐 스트림을 예측할 수 있다. 다양한 실시예 들에서, 제1 인공지능 모델은 음소 정규화 함수에 기초하여 음소에 대응되는 복수의 비짐들 중 하나의 비 짐을 선택할 수 있다. 제2 인공지능 모델은 제1 인공지능 모델에 의해 생성된 비짐 스트림을 입력받아, 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 출력할 수 있다. 애니메이션 곡선은 헤드 모델의 움직임에 관련된 애니메 이션 파라미터의 시간적 변화를 나타낸다. 일 실시예에서, 애니메이션 곡선은 각 비짐 애니메이션에서의 얼굴 랜드마크의 움직임 및 비짐 애니메이션의 지속 시간을 지정할 수 있다. 상기 애니메이션 곡선은 애니메이션 생 성부에 입력되어 헤드 모델에 적용됨으로써 헤드 모델 애니메이션을 생성하기 위하여 사용될 수 있다. 일 실시예에서, 제2 인공지능 모델은 컨볼루션 신경망(Convolutional Neural Network, CNN), 순환 신경망 (Recurrent Neural Network, RNN), 장단기 메모리 (Long Short-Term Memory, LSTM), 게이트 순환 유닛 (gated recurrent unit, GRU), 이들의 변형, 또는 이들의 임의의 조합 중 적어도 하나일 수 있다. 다양한 실시예들에서, 제2 인공지능 모델은 애니메이션 곡선 형성 함수에 기초하여 비짐 스트림의 비짐들 의 애니메이션 곡선을 도출할 수 있다. 일 실시예에서, 상기 애니메이션 곡선은 얼굴 움직임 부호화 시스템 (Facial Action Coding System, FACS)을 사용하여 계산될 수 있다. FACS를 이용하여 계산된 애니메이션 곡선 은 임의의 FACS 기반 헤드 모델에 적용되어 헤드 모델 애니메이션을 생성할 수 있다. 후처리부는 음소 스트림 및 비짐 스트림을 후처리하여 병합할 수 있다. 후처리부에서 출력된 병합된 음소 및 비짐 스트림은 애니메이션 생성부에 입력되어 헤드 모델 애니메이션을 생성하기 위하여 사용될 수 있다. 다양한 실시예들에서, 후처리부는 상기 음소 스트림 및 비짐 스트림을 상기 애니메이션 곡선을 고려하여 오버레이함으로써 병합할 수 있다. 병합된 음소 및 비짐 스트림에서, 각 음소는 대응되는 비짐과 연관될 수 있 다. 병합된 음소 및 비짐 스트림의 각 음소 및 연관되는 대응 비짐의 지속시간은 상기 비짐의 애니메이션 곡선에 의해 지정될 수 있다. 일 실시예에서, 후처리부는 병합을 위하여 두 개의 입력을 받아 하나의 출력을 반환하는 임의의 함수를 사용할 수 있다. 한편, 인공지능 모델에 포함된 전처리부, 제1 인공지능 모델, 제2 인공지능 모델, 및 후처 리부 중 적어도 하나는, 적어도 하나의 하드웨어 칩 형태로 제작되어 전자 장치에 탑재될 수 있다. 예를 들어, 전처리부, 제1 인공지능 모델, 제2 인공지능 모델, 및 후처리부 중 적어도 하나는 인공 지능(AI; artificial intelligence)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또는 기존의 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 전술한 각종 전자 장치에 탑재될 수도 있다. 또한, 전처리부, 제1 인공지능 모델, 제2 인공지능 모델, 및 후처리부는 하나의 전자 장치 에 탑재될 수도 있으며, 또는 별개의 전자 장치들에 각각 탑재될 수도 있다. 예를 들어, 전처리부, 제1 인공지능 모델, 제2 인공지능 모델, 및 후처리부 중 일부는 전자 장치에 포함되고, 나 머지 일부는 서버에 포함될 수 있다. 또한, 전처리부, 제1 인공지능 모델, 제2 인공지능 모델, 및 후처리부 중 적어도 하나는 소프트웨어 모듈로 구현될 수 있다. 전처리부, 제1 인공지능 모델, 제2 인공지능 모델, 및 후 처리부 중 적어도 하나가 소프트웨어 모듈(또는, 인스터력션(instruction) 포함하는 프로그램 모듈)로 구 현되는 경우, 소프트웨어 모듈은 컴퓨터로 읽을 수 있는 판독 가능한 비일시적 판독 가능 기록매체(non- transitory computer readable media)에 저장될 수 있다. 또한, 이 경우, 적어도 하나의 소프트웨어 모듈은 OS(Operating System)에 의해 제공되거나, 소정의 애플리케이션에 의해 제공될 수 있다. 또는, 적어도 하나의 소프트웨어 모듈 중 일부는 OS(Operating System)에 의해 제공되고, 나머지 일부는 소정의 애플리케이션에 의해 제공될 수 있다. 도 3은 다양한 실시예들에 따른, 제1 인공지능 모델 및 제2 인공지능 모델의 블록도이다. 도 3을 참조하면, 제1 인공지능 모델은 공간적 특성 추출 레이어, 시간적 특성 추출 레이어, 음 소 예측 레이어, 음소 스트림 형성 함수, 비짐 예측 레이어, 비짐 스트림 형성 함수, 및 음소 정규화 함수를 포함할 수 있다. 제2 인공지능 모델은 애니메이션 곡선 예측 레이어 및 애 니메이션 곡선 형성 함수를 포함할 수 있다. 공간적 특성 추출 레이어, 시간적 특성 추출 레이어, 음소 예측 레이어, 비짐 예측 레이어 , 및 애니메이션 곡선 예측 레이어는 특정 기능을 수행하는 신경망(neural network)의 적어도 일부일 수 있다. 음소 스트림 형성 함수, 비짐 스트림 형성 함수, 음소 정규화 함수 및 애니메이션 곡 선 형성 함수는 인공지능 모델에 포함된 하나 이상의 레이어에서 결과를 도출하거나 도출된 결과를 평가하 기 위하여 사용되는 함수일 수 있다. 공간적 특성 추출 레이어 및 시간적 추출 레이어는 입력된 음성 신호의 특성 정보로부터 상기 음성 신호의 특성을 추출할 수 있다. 상기 음성 신호의 특성 정보는 전처리부에서 출력된 음성 신호의 특성 계 수일 수 있다. 공간적 특성 추출 레이어는 입력된 음성 신호의 특성 정보를 처리하여, 공간적(spatial) 특성을 추출할 수 있다. 일 실시예에서, 공간적 특성 추출 레이어에는 완전히 연결된(fully connected) 레이어들과 비선형 성을 가진 컨볼루션 신경망(Convolutional Neural Network, CNN) 또는 순환 신경망(Recurrent Neural Network, RNN)이 사용될 수 있다. 예를 들어, 도 4a에 도시된 것과 같은 레이어 구조가 사용될 수 있다. 그러나 이에 한정되지 않고, 임의의 미분 가능한(differentiable) 레이어가 추가될 수 있다. 일 실시예에서, 공간적 특성 추출 레이어는 미리 학습된 것일 수 있다. 시간적 특성 추출 레이어는 상기 추출된 공간적 특성을 처리하여, 시간적(temporal) 특성을 추출할 수 있 다. 일 실시예에서, 시간적 특성 추출 레이어에는 완전히 연결된 레이어들과 비선형성을 가진 순환 신경 망(RNN)이 사용될 수 있다. 예를 들어, 드롭아웃(dropout)이 있는 3단계 장단기 메모리 (Long Short-Term Memory, LSTM)가 사용될 수 있다. 공간적 특성 추출 레이어 및 시간적 추출 레이어를 거쳐 추출된 음성 신호의 특성에 기초하여, 두 개 의 독립적인 스트림들, 음소 스트림과 비짐 스트림이 예측될 수 있다. 음소 예측 레이어는 추출된 음성 신호의 특성으로부터 음성 신호에 대응되는 음소 스트림을 도출할 수 있다. 비짐 예측 레이어는 상기 음 소 스트림에 포함되는 각 음소에 대응되는 비짐을 선택하여, 상기 음성 신호에 대응되는 비짐 스트림을 도출할수 있다. 다양한 실시예들에서, 음소 예측 레이어는 음소 스트림 형성 함수에 기초하여 음성 신호의 특성으로 부터 음소 스트림을 예측할 수 있다. 일 실시예에서, 음소 스트림 형성 함수는 제1 인공지능 모델이 예측한 음소 스트림이 실제의 올바른 값과 얼마나 유사한지를 측정하는 손실 함수(loss function)로부터 계산될 수 있다. 일 실시예에서, 음소 스트림 형성 함수는 임의의 음성 신호 및 상기 임의의 음성 신호에 대응하 는 텍스트를 포함하는 학습 데이터 세트에 의해 학습된 것일 수 있다. 다양한 실시예들에서, 비짐 예측 레이어는 비짐 스트림 형성 함수에 기초하여 음성 신호의 특성으로 부터 음소 스트림에 대응되는 비짐 스트림을 예측할 수 있다. 일 실시예에서, 비짐 스트림 형성 함수는 제1 인공지능 모델이 예측한 비짐 스트림이 실제의 올바른 값과 얼마나 유사한지를 측정하는 손실 함수로 부터 계산될 수 있다. 일 실시예에서, 비짐 스트림 형성 함수는 임의의 음성 신호 및 상기 임의의 음성 신호에 대응되는 비디오 신호를 포함하는 학습 데이터 세트에 의해 학습된 것일 수 있다. 하나의 음소에 부분적으로 대응할 수 있는 복수의 비짐들이 존재하는 경우, 비짐 예측 레이어는 그 중 적 합한 비짐을 선택할 수 있다. 다양한 실시예들에서, 비짐 예측 레이어는 음소 정규화 함수에 기초하 여 음소에 대응되는 복수의 비짐들 중 하나의 비짐을 선택할 수 있다. 일 실시예에서, 상기 음소 정규화 함수 는 음소의 확률 분포를 예측하고, 사용될 가능성이 낮은 음소에 대응되는 기본 형태에 페널티를 주는 함수 일 수 있다. 일 실시예에서, 상기 음소 정규화 함수는 임의의 음성 신호를 포함하는 학습 데이터 세트에 의해 정규화(regularization) 방법에 의해 계산된 것일 수 있다. 일 실시예에서, 음소 예측 레이어 및 비짐 예측 레이어에는 선형 레이어, 비선형성 레이어, 및 다른 미분 가능한 레이어들의 스택을 포함한 임의의 가능한 레이어 구조가 사용될 수 있다. 예를 들어, 도 4b에 도 시된 것과 같은, 정류된 선형 유닛(Rectified Linear Unit, ReLU)과 완전히 연결된 (fully connected) 2개의 선형 레이어들이 예측자로써 사용될 수 있다. 일 실시예에서, 음소 예측 레이어 및 비짐 예측 레이어는 시간적 추출 레이어와 가중치를 공유 할 수 있다. 두 스트림이 모두 이전 레이어와 가중치를 공유함에 따라, 예측되는 파라미터들의 특성에 의하여 모델을 정규화하는 효과를 얻을 수 있다. 애니메이션 곡선 예측 레이어는 비짐 스트림으로부터 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 예측할 수 있다. 애니메이션 곡선은 헤드 모델의 움직임에 관련된 애니메이션 파라미터의 시간적 변화를 나타 낸다. 일 실시예에서, 애니메이션 곡선은 각 비짐 애니메이션에서의 얼굴 랜드마크의 움직임 및 비짐 애니메이 션의 지속 시간을 지정할 수 있다. 다양한 실시예들에서, 애니메이션 곡선 예측 레이어는 애니메이션 곡선 형성 함수에 기초하여 비짐 스트림의 비짐들의 애니메이션 곡선을 도출할 수 있다. 일 실시예에서, 애니메이션 곡선 형성 함수는 제2 인공지능 모델이 도출한 애니메이션 곡선이 나타내는 움직임이 실제 인물의 움직임과 얼마나 유사한지를 측정하는 손실 함수로부터 계산될 수 있다. 일 실시예에서, 애니메이션 곡선 형성 함수는 임의의 음성 신 호 및 상기 임의의 음성 신호에 대응되는 비디오 신호를 포함하는 학습 데이터 세트에 의해 학습된 것일 수 있 다. 일 실시예에서, 상기 애니메이션 곡선은 얼굴 움직임 부호화 시스템(Facial Action Coding System, FACS)을 사 용하여 계산될 수 있다. 애니메이션 곡선 예측 레이어는 각 비짐에 대하여, FACS 계수를 이용하여 정의된 애니메이션 곡선을 도출할 수 있다. FACS 계수를 사용하여 정의된 애니메이션 곡선은 임의의 FACS 기반 헤드 모델에 적용될 수 있다. 그러나, 애니메이션 곡선은 반드시 FACS를 사용하여 계산되는 것에만 한정되지 않으며, 애니메이션 곡선을 계산하기 위하여 임의의 적절한 방법을 사용할 수 있다. 도 5는 다양한 실시예들에 따른, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법의 흐름도이다. 도 5 의 각 동작들은 도 1 에 도시된 전자 장치, 또는 도 9에 도시된 전자 장치 또는 전자 장치의 프 로세서에 의해 수행될 수 있다. 도 5를 참조하면, 동작 S510에서, 전자 장치는 음성 신호로부터 음성 신호의 특성 정보를 획득할 수 있다. 다양한 실시예들에서, 전자 장치는 음성 신호를 전처리하여 음성 신호의 특성을 나타내는 특성 정보, 예를 들어 특성 계수로 변환할 수 있다. 일 실시예에서, 전자 장치는 MFCC(Mel-Frequency Cepstral Coefficients) 방법에 의하여 음성 계수를 변환하여 상기 특성 계수들을 획득할 수 있다. 다른 실시예에서, 전 자 장치는 다른 사전 학습된 인공지능 모델에 상기 음성 신호를 입력하여 상기 특성 계수들을 획득할 수있다. 동작 S520에서, 전자 장치는 인공지능 모델을 이용하여, 상기 특성 정보로부터 상기 음성 신호에 대응하는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐 스트림을 획득할 수 있다. 다양한 실시예들에서, 전자 장치는 음소 스트림 형성 함수에 기초하여 음성 신호의 특성 계수로부터 음소 스트림을 예측할 수 있다. 상기 음소 스트림 형성 함수는, 임의의 음성 신호 및 상기 임의의 음성 신호에 대응 하는 텍스트를 포함하는 학습 데이터 세트에 의해 학습되는 것일 수 있다. 다양한 실시예들에서, 전자 장치는 비짐 스트림 형성 함수에 기초하여 음성 신호의 특성 계수로부터 음소 스트림에 대응되는 비짐 스트림을 예측할 수 있다. 상기 비짐 스트림 형성 함수는, 임의의 음성 신호 및 상기 임의의 음성 신호에 대응되는 비디오 신호를 포함하는 학습 데이터 세트에 의해 학습되는 것일 수 있다. 다양한 실시예들에서, 전자 장치는 음소 정규화 함수에 기초하여 음소에 대응되는 복수의 비짐들 중 하나 의 비짐을 선택할 수 있다. 동작 S530에서, 전자 장치는 인공지능 모델을 이용하여, 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득할 수 있다. 다양한 실시예들에서, 전자 장치는 애니메이션 곡선 형성 함수에 기초하여 비짐 스트림의 비짐들의 애니메이션 곡선을 도출할 수 있다. 일 실시예에서, 상기 애니메이션 곡선은 얼굴 움직임 부호화 시스템(Facial Action Coding System, FACS)을 사용하여 계산될 수 있다. 동작 S540에서, 전자 장치는 상기 음소 스트림 및 상기 비짐 스트림을 병합할 수 있다. 일 실시예에서, 인공지능 모델은 상기 음소 스트림 및 비짐 스트림을 상기 애니메이션 곡선을 고려하여 오버레이함으로써 병합할 수 있다. 동작 S550에서, 전자 장치는 상기 애니메이션 곡선을 상기 병합된 음소 및 비짐 스트림의 비짐들에 적용하 여 헤드 모델 애니메이션을 생성할 수 있다. 다양한 실시예들에서, 전자 장치는 미리 정의된 헤드 모델에 기초하여 헤드 모델 애니메이션을 생성할 수 있다. 일 실시예에서, 상기 미리 정의된 헤드 모델은 얼굴 움직임 부호화 시스템(FACS)에 기초한 임의의 3D 캐릭터 모델일 수 있다. 일 실시예에서, 전자 장치는 상기 병합된 음소 및 비짐 스트림에 기초하여, 미리 정의된 헤드 모델의 비짐 세트를 결정하고, 상기 비짐 세트에 상기 애니메이션 곡선을 적용하여 헤드 모델 애니메이션을 생성할 수 있다. 도 6은 다양한 실시예들에 따른, 음성 신호로부터 애니메이션 헤드 모델을 생성하기 위한 인공지능 모델을 학습 시키는 학습부의 개요도이다. 도 6을 참조하면, 음성 신호로부터 애니메이션 헤드 모델을 생성하기 위한 인공지능 모델을 학습시키는 학 습부는, 음소 검출부, 음소 스트림 형성 함수 계산부 , 애니메이션 생성부, 제1 움직임 패 턴 검출부 , 제2 움직임 패턴 검출부 , 비짐 스트림 형성 함수 계산부 , 애니메이션 곡선 형성 함수 계산부 , 및 음소 정규화 함수 계산부 를 포함할 수 있다. 학습부는 음성 신호, 상기 음성 신호에 대응하는 텍스트, 및 상기 음성 신호에 대응하는 비디오 신호를 포 함하는 학습 데이터 세트를 획득할 수 있다. 상기 음성 신호, 상기 텍스트, 및 상기 비디오 신호는 다른 얼굴 형태를 가진 다양한 인물의 기록을 포함할 수 있다. 상기 음성 신호, 상기 텍스트, 및 상기 비디오 신호 는 다중 목표 학습을 위하여 각각 별도로 처리될 수 있다. 상기 음성 신호의 처리, 상기 텍스트의 처리, 및 상 기 비디오 신호의 처리는 전자 장치의 구성 및 그 계산 능력에 따라 병렬적으로 수행되거나 또는 순차적으 로 수행될 수 있다. 인공지능 모델은 학습 데이터 세트의 음성 신호를 입력받아, 상기 음성 신호에 대응되는 제1 음소 스 트림, 상기 제1 음소 스트림에 대응되는 비짐 스트림, 및 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선 을 획득할 수 있다. 다양한 실시예들에서, 인공지능 모델은 전술된 것과 동일한 과정에 의하여 학습 데이터 세트의 음성 신호로부터 제1 음소 스트림, 비짐 스트림, 및 애니메이션 곡선을 획득할 수 있다. 예를 들어, 인공지능 모델 은 음성 신호를 전처리하여 음성 신호의 특성을 나타내는 특성 정보를 획득할 수 있다. 인공지능 모델 은 상기 특성 정보를 입력 받아, 상기 음성 신호에 대응하는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐(viseme) 스트림을 출력할 수 있다. 인공지능 모델은 상기 비짐 스트림에 포함된 비짐들의 애니메이 션 곡선을 출력할 수 있다.음소 검출부는 학습 데이터 세트의 음성 신호에 대응되는 텍스트를 입력받아, 제2 음소 스트림을 검 출할 수 있다. 텍스트로부터 음소를 검출하는 동작은 알려진 임의의 방법으로 실행될 수 있다. 음소 스트림 형성 함수 계산부는 상기 제1 음소 스트림과 상기 제2 음소 스트림을 비교하여, 인공지능 모 델에서 음소 스트림을 예측하는 데 사용하는 음소 스트림 형성 함수를 계산할 수 있다. 일 실시예에 서, 상기 음소 스트림 형성 함수는 상기 제1 음소 스트림과 상기 제2 음소 스트림을 비교하여 손실 함수 (loss function)를 이용하여 계산될 수 있다. 애니메이션 생성부는 인공지능 모델로부터 출력된 애니메이션 곡선을 3D 템플릿 모델에 적용하여 비 짐 애니메이션을 생성할 수 있다. 다양한 실시예들에서, 애니메이션 생성부는 애니메이션 생성부와 동일한 방법으로, 비짐 스트림에 포함된 비짐들에 애니메이션 곡선을 적용하여 비짐 애니메이션을 획득할 수 있 다. 다양한 실시예들에서, 애니메이션 생성부는 미리 정의된 헤드 모델에 기초하여 비짐 애니메이션을 획득할 수 있다. 예를 들어, 애니메이션 생성부는 3D 템플릿 헤드 모델에 애니메이션 곡선을 적용하여 각 비짐의 비짐 애니메이션을 획득할 수 있다. 제1 움직임 패턴 검출부는 획득된 비짐 애니메이션에서 얼굴 랜드마크의 움직임 패턴을 검출하여, 제1 움 직임 패턴을 획득할 수 있다. 일 실시예에서, 얼굴 랜드마크는 상기 미리 정의된 헤드 모델에 미리 정의되어 있을 수 있다. 애니메이션 곡선이 나타내는 움직임 파라미터는 상기 정의된 얼굴 랜드마크의 움직임 패턴을 지 정할 수 있다. 제2 움직임 패턴 검출부는 학습 데이터 세트의 음성 신호에 대응되는 비디오 신호를 입력받아, 비디 오 신호에서 얼굴 랜드마크를 검출할 수 있다. 일 실시예에서, 얼굴 랜드마크는 랜드마크 검출기에 의해 검출 될 수 있다. 상기 랜드마크 검출기는 임의의 종래의 랜드마크 검출 방법을 수행할 수 있다. 제2 움직임 패턴 검출부는 검출된 얼굴 랜드마크의 움직임 변위를 측정하여 제2 움직임 패턴을 획득할 수 있다. 일 실시예에서, 상기 움직임 변위는 학습 데이터 세트의 평균 또는 학습 중 선택한 특정한 얼굴을 기준으로 측정할 수 있다. 일 실시예에서, 얼굴 모양에 독립적으로 인공지능 모델을 학습시키기 위하여, 제2 움직임 패턴 검출부는 비디오 신호의 첫 프레임으로부터 획득한 얼굴 모양 또는 추정한 얼굴 모양에 기초하여 얼굴 랜드마크의 움직임 변위를 측정할 수 있다. 비짐 스트림 형성 함수 계산부는 상기 제1 움직임 패턴과 상기 제2 움직임 패턴을 비교하여, 인공지능 모 델에서 비짐 스트림을 예측하는 데 사용하는 비짐 스트림 형성 함수를 계산할 수 있다. 일 실시예에 서, 상기 비짐 스트림 형성 함수는 상기 제1 움직임 패턴과 상기 제2 움직임 패턴을 비교하여 손실 함수 (loss function)를 이용하여 계산될 수 있다. 애니메이션 곡선 형성 함수 계산부는 상기 제1 움직임 패턴과 상기 제2 움직임 패턴을 비교하여, 인공지능 모델에서 애니메이션 곡선을 예측하는 데 사용하는 애니메이션 곡선 형성 함수를 계산할 수 있다. 일 실시예에서, 상기 애니메이션 곡선 형성 함수는 상기 제1 움직임 패턴과 상기 제2 움직임 패턴을 비교 하여 손실 함수(loss function)를 이용하여 계산될 수 있다. 음소 정규화 함수 계산부는 인공지능 모델로부터 출력된 제1 음소 스트림, 비짐 스트림, 및 애니메이 션 곡선에 기초하여, 인공지능 모델에서 음소에 대응되는 복수의 비짐들 중 하나의 비짐을 선택하기 위해 사용되는 음소 정규화 함수를 계산할 수 있다. 일 실시예에서, 음소 정규화 함수 계산부는 정규화 (regularization) 방법에 의해 음소 정규화 함수를 계산할 수 있다. 학습부는 계산된 음소 스트림 형성 함수, 비짐 스트림 형성 함수, 애니메이션 곡선 형성 함수, 및 음소 정 규화 함수를 이용하여, 인공지능 모델을 갱신할 수 있다. 도 7은 다양한 실시예들에 따른, 음성 신호 및 음성 신호에 대응되는 비디오 신호로부터 비짐 스트림 형성 함수 및 애니메이션 곡선 형성 함수를 계산하는 개요도이다. 학습 데이터 세트는 얼굴 모양이 다른 다양한 사람들의 기록을 포함하며, 또한 실제 인물의 얼굴 형태는 인공적으로 제작된 애니메이션 캐릭터 모델과 차이가 있다. 따라서, 인공지능 모델에 의해 생성된 3D 헤 드 모델 애니메이션과, 비디오 신호에서 나타나는 얼굴 움직임을 직접적으로 비교할 수 없다. 그러므로 인공지 능 모델이 발화되는 음성 신호에 따른 임의의 얼굴 모양의 움직임을 학습하기 위해서는, 얼굴 모양의 차이와 관련된 오류를 제거할 필요가 있다. 도 7을 참조하면, 제1 움직임 패턴 검출부는 3D 템플릿 모델의 비짐 애니메이션을 획득할 수 있다. 상기 비짐 애니메이션은 인공지능 모델로부터 출력된 애니메이션 곡선에 기초하여 생성된 것이다. 일 실시예에서, 제1 움직임 패턴 검출부는 3D 헤드 모델 애니메이션의 움직임과 비디오 신호로부터 검출된 얼굴 움직임을 비교하기 위하여, 상기 비짐 애니메이션의 얼굴 랜드마크를 2D 평면에 투영할 수 있다. 제1 움 직임 패턴 검출부는 상기 투영된 얼굴 랜드마크의 움직임을 2D 평면 상에서 계산하여, 제1 움직임 패턴을 획득할 수 있다. 제2 움직임 패턴 검출부는 학습 데이터 세트의 비디오 신호로부터 얼굴 랜드마크를 검출할 수 있다. 일 실시예에서, 제2 움직임 패턴 검출부는 비디오 신호로부터 검출된 임의의 얼굴 움직임과 3D 헤드 모델 애니메이션의 움직임을 비교하기 위하여, 상기 비디오 신호로부터 검출된 얼굴 랜드마크를 미리 정의된 중립 얼 굴에 오버레이하여 정렬할 수 있다. 일 실시예에서, 얼굴 랜드마크의 정렬은 Procruste 분석 또는 아핀 (Affine) 변환을 사용하여 수행될 수 있다. 일 실시예에서, 최적의 변환 행렬을 찾기 위해 Kabsh 알고리즘이 사용될 수 있다. 일 실시예에서, 제2 움직임 패턴 검출부는 상기 정렬된 랜드마크의 움직임을 계산하여 제2 움직임 패턴을 획득할 수 있다. 일 실시예에서, 제2 움직임 패턴 검출부는 상기 미리 정의된 중립 얼굴을 기준으로 정렬 된 얼굴 랜드마크의 움직임 변위를 측정하여 제2 움직임 패턴을 획득할 수 있다. 학습부는 상기 획득된 제1 움직임 패턴 및 제2 움직임 패턴에 기초하여 비짐 스트림 형성 함수 및 애니메 이션 곡선 형성 함수를 계산할 수 있다. 일 실시예에서, 비짐 스트림 형성 함수 및 애니메이션 곡선 형성 함수 는 상기 제1 움직임 패턴과 제2 움직임 패턴의 차이를 나타내는 손실 함수에 기초하여 계산될 수 있다. 상기 손실 함수에 의하여, 인공지능 모델이 예측한 움직임이 실제 얼굴의 움직임과 얼마나 유사한지가 측정될 수 있다. 상술한 방법에 따르면, 얼굴 형태의 차이를 제외한 상대적인 움직임만을 학습에 사용하므로, 3D 헤드 모델과 다 른 임의의 얼굴 형태를 학습 데이터로 이용하여 인공지능 모델을 학습시킬 수 있고, 2D 움직임에 기초하여 인공지능 모델을 학습시킬 수 있다. 따라서, 쉽게 구할 수 있는 비디오 데이터를 학습 데이터로 이용할 수 있다. 도 8은 다양한 실시예들에 따른, 음성 신호로부터 애니메이션 헤드 모델을 생성하기 위한 인공지능 모델을 학습 시키는 방법의 흐름도이다. 도 8의 각 동작들은 도 1 에 도시된 전자 장치, 또는 도 9에 도시된 전자 장 치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 도 8을 참조하면, 동작 S810에서, 전자 장치는 음성 신호, 상기 음성 신호에 대응하는 텍스트, 및 상기 음 성 신호에 대응하는 비디오 신호를 포함하는 학습 데이터 세트를 획득할 수 있다. 상기 음성 신호, 상기 텍스 트, 및 상기 비디오 신호는 다른 얼굴 형태를 가진 다양한 인물의 기록을 포함할 수 있다. 동작 S820에서, 전자 장치는 상기 음성 신호를 인공지능 모델에 입력하여, 상기 인공지능 모델로부터 출력되는 제1 음소 스트림, 상기 제1 음소 스트림에 대응되는 비짐 스트림, 및 상기 비짐 스트림에 포함된 비짐 들의 애니메이션 곡선을 획득할 수 있다. 다양한 실시예들에서, 인공지능 모델은 음성 신호를 전처리하여 음성 신호의 특성을 나타내는 특성 정보를 획득할 수 있다. 인공지능 모델은 상기 특성 정보를 입력 받아, 상기 음성 신호에 대응하는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐(viseme) 스트림을 출력할 수 있다. 인공지능 모델은 상기 비짐 스 트림에 포함된 비짐들의 애니메이션 곡선을 출력할 수 있다. 동작 S830에서, 전자 장치는 상기 제1 음소 스트림과 상기 음성 신호의 텍스트를 이용하여, 상기 인공지능 모델을 위한 음소 스트림 형성 함수를 계산할 수 있다. 다양한 실시예들에서, 전자 장치는 학습 데이터 세트의 음성 신호에 대응되는 텍스트를 입력받아, 제 2 음소 스트림을 검출할 수 있다. 전자 장치는 상기 제1 음소 스트림과 상기 제2 음소 스트림을 비교하여, 인공지능 모델에서 음소 스트림을 예측하는 데 사용하는 음소 스트림 형성 함수를 계산할 수 있다.동작 S840에서, 전자 장치는 상기 비짐 스트림, 상기 애니메이션 곡선 및 상기 비디오 신호를 이용하여, 상기 인공지능 모델을 위한 비짐 스트림 형성 함수 및 애니메이션 곡선 형성 함수를 계산할 수 있다. 다양한 실시예들에서, 전자 장치는 상기 애니메이션 곡선을 3D 템플릿 모델에 적용하여 비짐 애니메이션을 생성할 수 있다. 전자 장치는 상기 비짐 애니메이션에서 얼굴 랜드마크의 움직임 패턴을 검출하여, 제1 움직임 패턴을 획득할 수 있다. 일 실시예에서, 전자 장치는 상기 3D 템플릿 모델의 얼굴 랜드마크를 2D 평면에 투영할 수 있다. 전자 장치는 상기 2D 평면에 투영된 얼굴 랜드마크의 움직임에 기초하여 제1 움 직임 패턴을 획득할 수 있다. 다양한 실시예들에서, 전자 장치는 상기 비디오 신호에서 얼굴 랜드마크의 움직임 패턴을 검출하여, 제2 움직임 패턴을 획득할 수 있다. 일 실시예에서, 전자 장치는 상기 비디오 신호에서 얼굴 랜드마크를 검출 할 수 있다. 전자 장치는 상기 비디오 신호의 얼굴 랜드마크를 중립 얼굴에 정렬할 수 있다. 전자 장치 는 상기 중립 얼굴에 정렬된 얼굴 랜드마크의 움직임에 기초하여 제2 움직임 패턴을 획득할 수 있다. 다양한 실시예들에서, 전자 장치는 상기 제1 움직임 패턴과 상기 제2 움직임 패턴을 비교하여, 비짐 스트 림 형성 함수 및 애니메이션 곡선 형성 함수를 계산할 수 있다. 동작 S850에서, 전자 장치는 상기 제1 음소 스트림, 상기 비짐 스트림, 및 상기 애니메이션 곡선에 기초하 여, 상기 인공지능 모델을 위한 음소 정규화 함수를 계산할 수 있다. 동작 S850에서, 전자 장치는 상기 음소 스트림 형성 함수, 상기 비짐 스트림 형성 함수, 상기 애니메이션 곡선 형성 함수, 및 상기 음소 정규화 함수를 이용하여, 상기 인공지능 모델을 갱신할 수 있다. 도 9는 다양한 실시예들에 따른, 음성 신호로부터 헤드 모델을 애니메이션 생성하도록 구성된 전자 장치의 블록 도이다. 도 9를 참조하면, 전자 장치는 적어도 하나의 프로세서 및 메모리를 포함할 수 있다. 메모리는, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수 있고, 전자 장치로 입력되거 나 전자 장치로부터 출력되는 데이터를 저장할 수 있다. 다양한 실시예들에서, 메모리는 적어도 하 나의 학습된 인공지능 모델을 위한 수치 파라미터들 및 함수들을 저장할 수 있다. 다양한 실시예들에서, 메모 리는 적어도 하나의 인공지능 모델을 학습시키기 위한 학습 데이터를 저장할 수 있다. 다양한 실시예들에서, 메모리는 적어도 하나의 프로세서에 의해 실행될 때, 적어도 하나의 프로세서 가 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법을 실행하게 하는 인스트럭션을 저장할 수 있다. 다양한 실시예들에서, 메모리는 적어도 하나의 프로세서에 의해 실행될 때, 적어도 하나의 프 로세서가 음성 신호로부터 헤드 모델 애니메이션을 생성하기 위한 인공지능 모델을 학습시키는 방법을 실 행하게 하는 인스트럭션을 저장할 수 있다. 프로세서는, 통상적으로 전자 장치의 전반적인 동작을 제어한다. 예를 들어, 프로세서는, 메모 리에 저장된 프로그램들을 실행함으로써, 메모리, 통신부(미도시), 입력부(미도시), 출력부 (미도시) 등을 전반적으로 제어할 수 있다. 프로세서는, 메모리, 통신부(미도시), 입력부(미도시), 출력부 (미도시) 등을 제어함으로써, 본 개시에서의 전자 장치의 동작을 제어할 수 있다. 구체적으로, 프로세서는, 메모리, 통신부(미도시), 또는 입력부(미도시) 등을 통하여, 음성 신호를 획득할 수 있다. 프로세서는 음성 신호로부터 상기 음성 신호의 특성 정보를 획득할 수 있다. 프로세서는 인공지능 모델을 이용하여, 상기 특성 정보로부터 상기 음성 신호에 대응하는 음소 스트림 및 상기 음소 스트림에 대응하는 비짐 스트림을 획득할 수 있다. 일 실시예에서, 프로세서는 음소 스트림 형 성 함수에 기초하여 음성 신호로부터 음소 스트림을 예측할 수 있다. 프로세서는 비짐 스트림 형성 함수 에 기초하여 음성 신호로부터 비짐 스트림을 예측할 수 있다. 프로세서는 인공지능 모델을 이용하여, 상 기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득할 수 있다. 일 실시예에서, 프로세서는 애니 메이션 곡선 형성 함수에 기초하여 비짐 스트림의 비짐들의 애니메이션 곡선을 도출할 수 있다. 프로세서는 상기 음소 스트림 및 상기 비짐 스트림을 병합할 수 있다. 프로세서는 상기 애니메이션 곡선을 상기 병합된 음소 및 비짐 스트림의 비짐들에 적용하여 헤드 모델 애니메이션을 생성할 수 있다. 한편, 프로세서는, 메모리, 통신부(미도시), 또는 입력부(미도시) 등을 통하여, 음성 신호, 상기 음 성 신호에 대응하는 텍스트, 및 상기 음성 신호에 대응하는 비디오 신호를 포함하는 학습 데이터 세트를 획득할수 있다. 프로세서는 상기 음성 신호를 상기 인공지능 모델에 입력하여, 상기 인공지능 모델로부터 출력되는 제1 음 소 스트림, 상기 제1 음소 스트림에 대응되는 비짐 스트림, 및 상기 비짐 스트림에 포함된 비짐들의 애니메이션 곡선을 획득할 수 있다. 프로세서는 상기 제1 음소 스트림과 상기 음성 신호의 텍스트를 이용하여, 상기 인공지능 모델을 위한 음 소 스트림 형성 함수를 계산할 수 있다. 프로세서는 상기 비짐 스트림, 상기 애니메이션 곡선 및 상기 비 디오 신호를 이용하여, 상기 인공지능 모델을 위한 비짐 스트림 형성 함수 및 애니메이션 곡선 형성 함수를 계 산할 수 있다. 프로세서는 상기 제1 음소 스트림, 상기 비짐 스트림, 및 상기 애니메이션 곡선에 기초하 여, 상기 인공지능 모델을 위한 음소 정규화 함수를 계산할 수 있다. 프로세서는 상기 음소 스트림 형성 함수, 상기 비짐 스트림 형성 함수, 상기 애니메이션 곡선 형성 함수, 및 상기 음소 정규화 함수를 이용하여, 상기 인공지능 모델을 갱신함으로써, 인공지능 모델을 학습시킬 수 있다. 본 개시의 일 양태는 음성 신호로부터 애니메이션 헤드 모델을 생성하는 방법을 제공하며, 상기 방법은 하나 이 상의 프로세서에 의해 실행되며, 음성 신호를 수신하는 단계; 상기 음성 신호를 음성 신호 특징 세트로 변환하 는 단계; 상기 음성 신호 특징 세트로부터 음성 신호 특징들을 추출하는 단계; 학습된 인공지능 수단으로 상기 음성 신호 특징들을 처리함으로써 음소 스트림 및 상기 음소 스트림의 음소들에 대응되는 비짐 스트림을 도출하 는 단계; 상기 학습된 인공지능 수단에 의해, 상기 도출된 비짐 스트림의 비짐에 대한 애니메이션 곡선을 대응 되는 음소들에 기초하여 계산하는 단계; 상기 계산된 애니메이션 곡선을 고려하여 상기 도출된 음소 스트림 및 상기 도출된 비짐 스트림을 서로 오버레이함으로써 상기 도출된 음소 스트림 및 상기 도출된 비짐 스트림을 병 합하는 단계; 및 상기 계산된 애니메이션 곡선을 사용하여 상기 병합된 음소 및 비짐 스트림의 비짐을 애니메이 팅함으로써 헤드 모델의 애니메이션을 형성하는 단계를 포함한다. 추가적인 양태에서, 인공지능 수단의 학습은, 음성 신호, 상기 음성 신호에 대한 대본, 및 상기 음성 신호에 대 응하는 비디오 신호를 포함하는 학습 데이터 세트를 수신하는 단계; 상기 음성 신호에 대한 대본으로부터 음소 스트림을 도출하는 단계; 상기 음성 신호를 음성 신호 특징 세트로 변환하는 단계; 상기 음성 신호 특징 세트로 부터 음성 신호 특징들을 추출하는 단계; 상기 음성 신호 특징들에 기초하여 음소 스트림 및 상기 음소 스트림 의 음소들에 대응하는 비짐 스트림을 도출하는 단계; 상기 음성 신호에 대한 대본으로부터 도출된 음소 스트림 과 상기 음성 신호 특징들에 기초하여 도출된 음소 스트림을 비교함으로써 상기 음소 스트림을 형성하는 함수를 계산하는 단계; 상기 음성 신호 특징들에 기초하여 도출된 비짐 스트림의 비짐에 대한 애니메이션 곡선을 계산 하는 단계; 상기 계산된 애니메이션 곡선을 미리 정해진 비짐 세트에 적용하는 단계; 상기 계산된 애니메이션 곡선을 적용한 상기 미리 정해진 비짐 세트에서 얼굴 랜드마크의 움직임 패턴을 결정하는 단계; 상기 음성 신호 에 대응하는 상기 비디오 신호에서 얼굴 랜드마크의 움직임 패턴을 결정하는 단계; 상기 음성 신호에 대응하는 상기 비디오 신호에서의 얼굴 랜드마크의 움직임 패턴을 미리 결정된 중립 얼굴에 오버레이하는 단계; 상기 미 리 결정된 중립 얼굴에 오버레이된 상기 음성 신호에 대응하는 상기 비디오 신호에서의 얼굴 랜드마크의 움직임 패턴과, 상기 미리 정해진 비짐 세트에서 결정된 상기 얼굴 랜드마크의 움직임 패턴을 비교함으로써, 상기 비짐 스트림을 형성하는 함수 및 애니메이션 곡선들을 계산하는 함수를 계산하는 단계; 및 상기 음성 신호 특징들에 기초하여 도출된 음소 스트림, 상기 음성 신호 특징들에 기초하여 도출된 비짐 스트림, 및 상기 계산된 애니메 이션 곡선에 기초하여 비짐을 선택하는 함수를 계산하는 단계를 포함한다. 다른 추가적인 양태에서, 상기 음성 신호를 음성 신호 특징 세트로 변환하는 단계 및 상기 음성 신호 특징 세트 로부터 음성 신호 특징들을 추출하는 단계는 Mel-Frequency Cepstral Coefficients (MFCC) 방법 또는 추가 사 전 학습된 인공지능 수단 중 하나에 의해 수행된다. 또 다른 추가적인 양태에서, 상기 추가 사전 학습된 인공지능 수단은 순환 신경망, 장단기 메모리 (Long Short- Term Memory, LSTM), 게이트 순환 유닛 (gated recurrent unit, GRU), 이들의 변형, 또는 이들의 조합 중 적어 도 하나이다. 또 다른 추가적인 양태에서, 상기 학습된 인공지능 수단은 적어도 2개의 블록들을 포함하고, 상기 학습된 인공 지능 수단의 적어도 2개의 블록들 중 제1 블록은 상기 음성 신호 특징들을 처리함으로써 음소 스트림 및 상기 음소 스트림의 음소들에 대응하는 비짐 스트림을 도출하는 단계를 수행하고, 상기 학습된 인공지능 수단의 적어 도 2개의 블록들 중 제2 블록은 상기 학습된 인공지능 수단에 의해 상기 도출된 비짐 스트림의 비짐을 위한 애니메이션 곡선을 대응되는 음소에 기초하여 계산하는 단계를 수행한다. 또 다른 추가적인 양태에서, 상기 학습된 인공지능 수단의 적어도 2개의 블록들 중 제1 블록은 순환 신경망, 장 단기 메모리 (Long Short-Term Memory, LSTM), 게이트 순환 유닛 (gated recurrent unit, GRU), 이들의 변형, 또는 이들의 조합 중 적어도 하나이다. 또 다른 추가적인 양태에서, 상기 학습된 인공지능 수단의 적어도 2개의 블록들 중 제2 블록은 순환 신경망, 장 단기 메모리 (Long Short-Term Memory, LSTM), 게이트 순환 유닛 (gated recurrent unit, GRU), 이들의 변형, 또는 이들의 조합 중 적어도 하나이다. 또 다른 추가적인 양태에서, 상기 음성 신호 특징들에 기초하여 도출된 비짐 스트림에서 비짐에 대한 애니메이 션 곡선들을 계산하는 단계는 얼굴 움직임 부호화 시스템(Facial Action Coding System, FACS)을 사용하여 수행 된다. 본 개시의 다른 양태는 전자 컴퓨팅 장치를 제공하는데, 상기 전자 컴퓨팅 장치는 적어도 하나의 프로세서; 및 적어도 하나의 학습된 인공지능 수단의 수치 파라미터 및, 적어도 하나의 프로세서에 의해 실행될 때, 적어도 하나의 프로세서로 하여금 음성 신호로부터 애니메이션 헤드 모델을 생성하는 방법을 수행하게 하는 인스트럭션 을 저장하는 메모리를 포함한다. 본 개시의 다양한 실시예들은 기기(machine)(예: 전자 장치) 의해 읽을 수 있는 저장 매체(storage medium)(예: 메모리)에 저장된 하나 이상의 명령어들을 포함하는 소프트웨어로서 구현될 수 있다. 예를 들면, 기기(예: 전자 장치)의 프로세서(예: 프로세서)는, 저장 매체로부터 저장된 하나 이상의 명령 어들 중 적어도 하나의 명령을 호출하고, 그것을 실행할 수 있다. 이것은 기기가 상기 호출된 적어도 하나의 명령어에 따라 적어도 하나의 기능을 수행하도록 운영되는 것을 가능하게 한다. 상기 하나 이상의 명령어들은 컴파일러에 의해 생성된 코드 또는 인터프리터에 의해 실행될 수 있는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장매 체가 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다는 것을 의미할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구분하지 않는다. 일 실시예에 따르면, 본 개시에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD- ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 또는 두 개의 사용자 장치 들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 다양한 실시예들에 따르면, 상기 기술한 구성요소들의 각각의 구성요소(예: 모듈 또는 프로그램)는 단수 또는 복수의 개체를 포함할 수 있다. 다양한 실시예들에 따르면, 전술한 해당 구성요소들 중 하나 이상의 구성요소 들 또는 동작들이 생략되거나, 또는 하나 이상의 다른 구성요소들 또는 동작들이 추가될 수 있다. 대체적으로 또는 추가적으로, 복수의 구성요소들(예: 모듈 또는 프로그램)은 하나의 구성요소로 통합될 수 있다. 이런 경 우, 통합된 구성요소는 상기 복수의 구성요소들 각각의 구성요소의 하나 이상의 기능들을 상기 통합 이전에 상 기 복수의 구성요소들 중 해당 구성요소에 의해 수행되는 것과 동일 또는 유사하게 수행할 수 있다. 다양한 실 시예들에 따르면, 모듈, 프로그램 또는 다른 구성요소에 의해 수행되는 동작들은 순차적으로, 병렬적으로, 반복 적으로, 또는 휴리스틱하게 실행되거나, 상기 동작들 중 하나 이상이 다른 순서로 실행되거나, 생략되거나, 또 는 하나 이상의 다른 동작들이 추가될 수 있다.도면 도면1 도면2 도면3 도면4a 도면4b 도면5 도면6 도면7 도면8 도면9"}
{"patent_id": "10-2020-0089852", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 다양한 실시예들에 따른, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 시스템의 개요도이다. 도 2는 다양한 실시예들에 따른, 음성 신호로부터 헤드 모델 애니메이션을 생성하기 위한 인공지능 모델의 개요 도이다. 도 3은 다양한 실시예들에 따른, 제1 인공지능 모델 및 제2 인공지능 모델의 블록도이다. 도 4a는 일 실시예에 따른 공간적 특성 추출 레이어의 구조를 도시한다. 도 4b는 일 실시예에 따른 음소 예측 레이어 및 비짐 예측 레이어의 구조를 도시한다. 도 5는 다양한 실시예들에 따른, 음성 신호로부터 헤드 모델 애니메이션을 생성하는 방법의 흐름도이다. 도 6은 다양한 실시예들에 따른, 음성 신호로부터 애니메이션 헤드 모델을 생성하기 위한 인공지능 모델을 학습 시키는 학습부의 개요도이다. 도 7은 다양한 실시예들에 따른, 음성 신호 및 음성 신호에 대응되는 비디오 신호로부터 비짐 스트림 형성 함수 및 애니메이션 곡선 형성 함수를 계산하는 개요도이다. 도 8은 다양한 실시예들에 따른, 음성 신호로부터 애니메이션 헤드 모델을 생성하기 위한 인공지능 모델을 학습 시키는 방법의 흐름도이다. 도 9는 다양한 실시예들에 따른, 음성 신호로부터 헤드 모델을 애니메이션 생성하도록 구성된 전자 장치의 블록 도이다."}
