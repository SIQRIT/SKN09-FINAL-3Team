{"patent_id": "10-2022-7030038", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0158225", "출원번호": "10-2022-7030038", "발명의 명칭": "원하는 속성을 가진 화학적 화합물을 생성하는 학습 시스템 및 방법", "출원인": "99앤드비욘드 인코포레이티드", "발명자": "사타로브, 보리스"}}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "분자의 자동화된 설계를 위한 시스템으로서,화학 반응 예측 모듈 및 스코어링 함수 모듈을 포함하는 인공 지능 환경을 특징으로 하되,상기 인공 지능 환경은 적어도 하나의 반응물을 수반하는 적어도 하나의 반응에 기초하여 가능한 반응 생성물세트를 예측하고, 상기 인공 지능 환경은 원하는 메트릭에 기초하여 상기 가능한 반응 생성물 세트를 스코어링하는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 근사화 모듈을 더 포함하되,상기 근사화 모듈은 양립가능한 메트릭 공간에서의 거리에 기초하여 모든 이용가능한 반응물 세트로부터 가장가까운 반응물 세트를 식별하는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 컴퓨터 구현 에이전트를 더 포함하되,상기 컴퓨터 구현 에이전트는 강화 학습(reinforcement learning) 프로세스에 따라 동작하고, 적어도 하나의 액터 모듈(actor module)을 포함하고, 상기 컴퓨터 구현 에이전트는 반응물들의 공간에서 반응 및/또는 액션을 시뮬레이션하기 위해 적어도 하나의 반응물을 수반하는 상기 적어도 하나의 반응을 인공 지능 환경에 제공함으로써 상기 강화 학습 프로세스를 통해상기 인공 지능 환경과 인터페이싱하는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 컴퓨터 구현 에이전트는 상기 적어도 하나의 액터 모듈의 출력을 평가하는 데 사용되는적어도 하나의 크리틱 모듈(critic modul)을 더 포함하는, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서, 상기 근사화 모듈은 미분가능하고, 컴퓨터 구현 에이전트의 일부이어서, 상기 근사화 모듈은상기 근사화 모듈을 통해 그래디언트를 전파함으로써 상기 크리틱 네트워크의 출력에 기초하여 액터 네트워크및 크리틱 네트워크 중 적어도 하나를 업데이트할 수 있는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3항에 있어서, 초기 반응물이 무작위로 샘플링되거나, 통계적 메트릭을 사용함으로써 샘플링되거나, 또는 크리틱 모듈에 의해 출력이 평가되는 네트워크를 사용함으로써 샘플링되는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 적어도 하나의 반응물을 수반하는 적어도 하나의 반응은 유전 알고리즘(geneticalgorithm)에 의해 생성된 프로토-액션(proto-action)을 통해 선택되는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 적어도 하나의 반응물을 수반하는 적어도 하나의 반응은 유전 알고리즘의 출력을 모방하도록 훈련된 강화 학습 모델에 의해 선택되는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "공개특허 10-2022-0158225-3-제7항에 있어서, 적어도 하나의 액터 및/또는 적어도 하나의 메트릭 모듈은 상기 유전 알고리즘의 출력에 기초하여 훈련되는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서, 계획 방법, 또는 계획 방법을 모방하도록 훈련된 강화 학습 모듈이 매 시간 단계에서 적어도하나의 액션을 컴퓨트하기 위해 채용되는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서, 상기 인공 지능 환경은 또한, 상기 가능한 반응 생성물 세트를 예측하는 데 적어도 하나의 반응 조건을 사용하는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항에 있어서, 상기 가능한 반응 생성물 세트는 후속 반응의 적어도 하나의 반응물로서 작용하는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항에 있어서, 상기 적어도 하나의 반응물은 모든 이용가능한 반응물 세트의 특징들에 의해 정의되는 공간 내의 텐서를 포함하는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제3항에 있어서, 크리틱 모듈이 반응물을 선택하기 위해 상기 적어도 하나의 액터 모듈의 출력을 평가하는것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1항에 있어서, 상기 화학 반응 예측 모듈은 규칙 기반 알고리즘, 물리학 기반 알고리즘, 양자 역학 알고리즘,기계 학습 알고리즘, 및 하이브리드 양자 기계 학습 알고리즘 중 적어도 하나에 기초하여 적어도 하나의 가능한반응 생성물을 예측하는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제1항에 있어서, 상기 화학 반응 예측 모듈은 N-성분 변환에 기초하여 상기 적어도 하나의 가능한 반응 생성물세트를 예측하는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제1항에 있어서, 상기 스코어링 함수 모듈은 상기 가능한 생성물 세트의 적어도 하나의 예측 또는 실험적 속성에 따라 보상을 결정하는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제1항에 있어서, 상기 인공 지능 환경은 합성 프로세스를 평가하기 위해 규칙 기반 알고리즘, 물리학 기반 알고리즘, 양자 역학 알고리즘, 기계 학습 알고리즘, 및 하이브리드 양자 기계 학습 알고리즘 중 적어도 하나에 기초한 역합성 예측 모듈을 사용하는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제17항에 있어서, 상기 인공 지능 환경은 규칙 기반 알고리즘, 물리학 기반 알고리즘, 양자 역학 알고리즘, 기계 학습 알고리즘, 및 하이브리드 양자 기계 학습 알고리즘 중의 적어도 하나에 의해 결정되는 것인, 시스템."}
{"patent_id": "10-2022-7030038", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "분자의 자동화된 설계를 위한 방법으로서,컴퓨터 구현 에이전트를 사용하여 적어도 하나의 반응물을 수반하는 적어도 하나의 반응을 발생시키는 단계;공개특허 10-2022-0158225-4-상기 컴퓨터 구현 에이전트에 의해, 상기 적어도 하나의 반응물을 수반하는 적어도 하나의 반응을 인공 지능 환경에 제공하는 단계;상기 인공 지능 환경에서, 상기 적어도 하나의 반응물을 수반하는 적어도 하나의 반응을 시뮬레이션하여 적어도하나의 가능한 반응 생성물 세트를 생성하는 단계;원하는 속성에 따라 상기 적어도 하나의 가능한 반응 생성물 세트를 스코어링하는 단계; 및상기 적어도 하나의 가능한 반응 생성물 세트로부터 선택된 최적의 반응 생성물 세트를 생성하고, 새로운 반응물 세트로서 작용하도록 상기 최적의 반응 생성물 세트를 상기 컴퓨터 구현 에이전트에 전달하는 단계를 특징으로 하되,상기 방법은 상기 최적의 반응 생성물 세트가 원하는 최종 생성물을 함유할 때 종결되는 것인, 방법."}
{"patent_id": "10-2022-7030038", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "유전 알고리즘, 전문가 반복 알고리즘, 계획 방법, 강화 학습 및 기계 학습 알고리즘을 포함하지만 이에 제한되 지 않는 여러 알고리즘에 의해 동력을 갖출 수 있는 반응 기반 메커니즘을 제형화함으로써 원하는 속성 및 특정 속성을 갖는 화학적 화합물의 라이브러리를 생성하기 위한 시스템 및 방법이 제공된다. 본 시스템 및 방법은 또 한 이러한 최적화된 생성물 S'가 반응물 R1, R2로부터 합성될 수 있고 또한 합성적으로 접근가능한 화학적 공간 의 신속하고 효율적인 탐색을 가능하게 하는 프로세스 단계들을 제공할 수 있다."}
{"patent_id": "10-2022-7030038", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "관련 출원 교차 참조 본 출원은 “SYSTEM AND METHOD FOR LEARNING TO GENERATE CHEMICAL COMPOUNDS WITH DESIRED PROPERTIES”라 는 명칭으로 2020년 1월 30일에 출원된 미국 특허 가출원 제62/967,898호의 우선권을 주장하며, 이의 전문이 본 원에 원용된다. 본 출원은 “SYSTEM AND METHOD FOR LEARNING TO GENERATE CHEMICAL COMPOUNDS WITH DESIRED PROPERTIES”라 는 명칭으로 2020년 9월 9일 수요일에 출원된 미국 특허 가출원 제63/076,151호의 우선권을 주장하며, 이의 전 문이 본원에 원용된다."}
{"patent_id": "10-2022-7030038", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "기술분야 본 발명은 신규한 제약 약물, 물질, 화장품, 살충제 또는 다른 화학적 화합물의 생성을 위한 화학 및 알고리즘 또는 기계 학습 화학 반응 예측 분야에 관한 것이다."}
{"patent_id": "10-2022-7030038", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "기계 학습 기반의 드 노보 약물 설계(de novo drug design)를 수행하기 위한 전략들은 구조 생성 방식과 반응 기반 방식의 두 그룹으로 나누어질 수 있다. 구조 생성 방식은 화학적 화합물 합성에 사용될 수 있는 화학 반응에 대한 명시적인 개념 없이 화학적 화합물을 생성하도록 훈련된 기계 학습 모델이다. 구조 생성 방식은 인코더/디코더 기반 생성 시스템뿐만 아니라 강화 학 습 시스템을 활용할 수 있다. 그러나, 이러한 구조 생성 방식은 보통 제조가 불가능하거나 실현 불가능한 분자 를 초래할 수 있다. 이를 보완하기 위해, 일반적으로 방식의 스코어링 함수 모듈에 데이터 중심 추정을 도입하 여 합성 접근성/타당성 점수를 계산한다. 또한, 전형적으로 역합성 분석이 최종 화학적 화합물이 생성된 후에 수행된다. 구조 생성 방식에는 두 가지 중요한 단점이 있다. 첫째, 생성 모델은 종종 판별자 모델에서의 데이터 중심 아티 팩트를 악용하는 경향이 있어, 생성을 잘못된 방향으로 가이드하고 생성된 구조의 실제 합성 접근성을 손상시킬 수 있다. 둘째, 합성 접근성/타당성 스코어를 예측하는 임의의 데이터 중심 모델은 모델을 훈련하는 데 사용된 훈련 세트에 따라 적용이 제한될 것이다. 이러한 단점들로 인해 계산 시간이 길어지고, 유용성이 거의 없는 결 과들이 더 많이 생성된다. 이러한 단점들은 신규 화학적 화합물의 생성을 위한 반응 기반 모델의 사용을 통해 극복될 수 있다. 알려져 있 는 반응 및 시중에서 또는 합성해서 이용가능한 반응물에 대한 알고리즘의 화학적 공간 탐색을 기반으로 함으로 써, 스코어링 함수 모듈의 효율성이 증가할 수 있고, 생성 방식의 전반적인 생산성과 효율성이 향상될 수 있다. 그러나, 기존 반응 기반 모델들에는 문제가 있다. DINGOS 또는 PathFinder와 같은 예들은 두 가지 주요 방식으 로 제한된다. 첫째, 두 시스템 모두 적용가능하기 위해 관심 있는 생물학적 표적에 대해 알려진 템플릿 리드 화 합물이 필요하다. 둘째, 이들 시스템들은 종단간 방식으로 훈련되지 않는다. 예를 들어, PathFinder의 경우, 먼 저 반응을 사용하여 화합물을 생성한 다음, 분리되어 있는 스코어링 함수 모듈을 사용하여 일부 생성물만 선택한다. DINGOS의 경우, 이차 반응이 일어날 가능성을 예측하는 부분만 실제로 훈련하고, 이 훈련은 반응 데이터 를 사용하는 감독 방식으로만 수행된다. 본원에서 설명되는 방법 및 프로세스는 이러한 한계점들을 극복하여, 이용가능한 반응물 세트를 사용함으로써 합성적으로 접근가능한 화학적 공간의 종단간 반응 기반 탐색을 가능하게 한다. 더 이상 하나 이상의 템플릿 화 합물이 필요하지 않다. 본원에서 화학적 화합물의 라이브러리를 생성하기 위한 시스템 및 방법이 설명된다. 시스템 및 방법은 강화 학 습 또는 전문가 반복 알고리즘, 유전 알고리즘, 및/또는 계획 방법을 포함하지만 이에 제한되지 않는 기계 학습 에 의해 가이드되는 반응 기반 방식을 활용하고, 원하는 속성, 특성 및/또는 거동을 보이는 화학적 후보들을 생 성하기 위한 스코어링 함수 모듈을 포함한다. 이러한 프로세스를 통해, 본 시스템 및 방법은 또한 이들 화학적 후보들이 합성되고/되거나 제조될 수 있는 대응하는 방법들을 생성하고 디스플레이한다. 이에 따라, 비교적 짧 은 시간 프레임에서 합성적으로 접근가능한 화학적 공간의 대부분을 효율적으로 탐색하는 것이 가능해진다."}
{"patent_id": "10-2022-7030038", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 양태들은 본 발명의 특정 실시예들에 관한 다음의 설명 및 관련 도면들로 개시된다. 대안적인 실시예 들이 본 발명의 사상 또는 범위로부터 벗어나지 않고 고안될 수 있다. 또한, 본 발명의 예시적인 실시예들의 주 지되어 있는 요소들은 본 발명의 관련 세부사항들을 모호하게 하지 않도록 상세히 설명되지 않거나 생략될 것이다. 또한, 본원에서 사용되는 여러 용어들에 대한 설명에 대한 이해를 용이하게 하기 위한 것이다. 본원에서 사용될 때, \"예시적인\"이라는 용어는 \"예, 사례, 또는 예시로서의 역할을 하는\"을 의미한다. 본원에서 설명되는 실시예들은 제한적인 것이 아니라, 단지 예시적인 것일 뿐이다. 설명된 실시예들은 반드시 다른 실시 예들에 비해 바람직하거나 유리한 것으로 해석되어서는 안 된다는 것이 이해되어야 한다. 또한, \"본 발명의 실 시예들\", \"실시예들\" 또는 \"발명\"이라는 용어들은 본 발명의 모든 실시예들이 논의된 특징, 이점 또는 동작 모 드를 포함할 것을 요구하지 않는다. 또한, 많은 실시예들은 예를 들어, 컴퓨팅 디바이스의 요소들에 의해 수행될 일련의 액션들의 관점에서 설명된 다. 본원에서 설명되는 다양한 액션들은 특정 회로들(예를 들어, 주문형 집적 회로(ASIC)들)에 의해, 하나 이상 의 고전적 또는 양자 프로세서에 의해 실행되는 프로그램 명령어들에 의해, 또는 양자의 조합에 의해 수행될 수 있다는 것이 인식될 것이다. 또한, 본원에서 설명되는 일련의 액션들은 실행 시에 연관된 프로세서로 하여금 본 원에서 설명된 기능을 수행하게 할 대응하는 컴퓨터 명령어 세트가 저장된 임의의 형태의 컴퓨터 판독가능 저장 매체 내에 전적으로 구현되는 것으로 고려될 수 있다. 이에 따라, 본 발명의 다양한 양태들은 다수의 상이한 형태들로 구현될 수 있으며, 이들 모두가 청구된 요지의 범위 내인 것으로 고려된다. 또한, 본원에서 설명되는 실 시예들 각각에 대해, 임의의 이러한 실시예들의 대응하는 형태는 본원에서 예를 들어, 설명된 액션을 수행하도 록 \"구성된 로직\"으로서 설명될 수 있다. 강화 학습은 알고리즘이 전체 보상 메트릭을 최대화하도록 환경을 입안하고 결정을 내리는 기계 학습 패러다임 이다. 강화 학습 방식을 구현하는 한 가지 방법은 마르코프 결정 프로세스(Markov Decision Process, MDP)를 통 한 것이다. 마르코프 결정 프로세스는 기초가 되는 강화 학습 작업을 설명하기 위한 수학적 프레임워크이다. 이 수학적 프레임워크는 매 시간 단계에서 상태 S, 액션 A, 전이 함수 P, 보상 M, 및 선택적으로 디스카운트 팩터 감마에 의해 특징지어진다. 전이 함수 P는 임의의 시간 단계 t에서 상태 S의 액션 A이 시간 단계 t+1에서 상태 S'로 이어질 확률을 나타낸다. 마르코프 결정 프로세스 프레임워크의 목적은 각 시간 단계에서 보상 M의 일부 함수를 최대화할 에이전트에 대한 정책을 찾는 것이다. 보상들 M은 선택적으로 디스카운트 팩터 γ에 의해 스케 일링될 수 있다. 마르코프 결정 프로세스는 바람직하게는 종단간 강화 학습 워크플로우로서 구조화된다. 이러한 종단간 구조는 강화 학습 워크플로우가 특정 작업에 관한 액션들만을 학습하는 것이 아니라, 다른 함수들로부터 독립적으로 개 발하기 어려울 수 있는 보다 상위 레벨 함수들까지 그리고 이들을 포함하는 전체 프로세스를 학습하는 것을 가 능하게 한다. 이는 에이전트가 보다 정교하게 훈련될 수 있게 하고, 보다 광범위한 반응들 및 필요한 경우 대응 하는 조건들이 고려될 수 있게 한다. 강화 학습 알고리즘들은 많은 상이한 유형들: 모델 기반(model-based), 모델 프리(model-free), 온 정책(on- policy), 및 오프 정책(off-policy)으로 분류될 수 있다. 이들 알고리즘들은 또한 값 기반 또는 정책 그래디언 트 방법들과 같은 업데이트 규칙에 기초하여 분류될 수 있다. 정책 그래디언트 방법들은 이산 액션 공간들 또는 연속 액션 공간들을 처리할 수 있다. 이산 액션 공간 알고리즘들의 예들은 REINFORCE, 액터-크리틱(actor- critic), 어드밴티지 액터-크리틱(advantage actor-critic), 신뢰 영역 정책 최적화(trust region policy optimization), ACKTR, 및 근접 정책 최적화(proximal policy optimization)를 포함한다. 그러나, 화학적 화합물 생성의 경우에 이산 액션 공간이 매우 크기 때문에, 연속 액션 공간에 적응되거나 이에 대응하는 알고리즘들이 바람직하다. 연속 액션 공간에서 동작하는 알고리즘들의 예들은 결정론적 정책 그래디언 트(deterministic policy gradient), DDPG(deep deterministic policy gradient), D4PG(distributed distributional deep deterministic policy gradient), TD3(twin delayed deep deterministic policy gradient), 및 SAC(soft actor critic)을 포함한다. 큰 이산 액션 공간을 관리하기 위한 본 발명에 따른 다른 방법은 연속 공간에서 액션을 예측하고, 그 후 k-최근 접 이웃(kNN) 알고리즘을 사용하여 연속 공간 액션을 하나 이상의 유효한 이산 액션에 매핑하는 것을 수반한다. 이를 실현하기 위해, 주어진 입력에 \"더 가까운\" 이웃들이 입력으로부터 \"더 먼\" 이웃들보다 더 많이 기여하는 거리 메트릭이 도입된다. \"근접도\" 메트릭을 평가하기 위해 임의의 거리 또는 발산 메트릭이 사용될 수 있다. k-최근접 이웃과 유사한 속성들을 갖는 알고리즘들이 또한 사용될 수 있다. 이제 도 1을 참조하면, MDP 워크플로우의 예시적인 실시예가 디스플레이되며, 이는 적어도 하나의 액터 모 듈 및 적어도 하나의 크리틱 모듈을 갖는 에이전트를 특징으로 한다. 에이전트는 kNN 알고 리즘에 의해 이산 공간으로 맵핑되고 스코어링 함수에 따라 스코어링되는 반응 예측기에 의해 기술되는 환경과 상호작용한다. 환경은 또한 상술한 바와 같이 크리틱 모듈을 복제하는 카피 크 리틱 모듈(120b)을 선택적으로 포함할 수 있다. 매 시간 단계 t에서, 반응물 R2(t)는 기존의 분자 또는 반응물 R1(t)과 반응하여 생성물 R1(t+1)을 생성한다. 또한, R1(t)는 상태 S(t)로 표현될 수 있고, R2(d)는 또한 액션 A(t)로 표현될 수 있으며, 생성물 R1(t+1)은 다 음의 시간 단계에 대해 상태 S'(t+1)로서 표현될 수 있다. 초기 시간 단계에서, 초기 분자 R1(t=0)은 모든 이용가능한 반응물의 목록으로부터 샘플링된다. 이러한 샘플링 은 무작위적이거나, 통계적으로 구동되거나, 스코어링 함수 모듈에 기초하여 선택되거나, 또는 본원에서 설명된 방식과 유사한 종단간 방식으로 훈련된 신경 네트워크 모듈에 따라 선택될 수 있다. 잠재적인 액션 공간이 매우 크기 때문에, 액션 공간의 크기를 감소시킬 수 있는 중간 액션 A1(t)을 도입하는 것이 바람직하다. 중간 액션 A1(t)은 액션 공간에 대한 필터로서 작용하는 반응의 형태를 띨 수 있다. 이 액션은 바람직하게는 메트릭으로서 반응물 R1(t) 및 R2(t) 중 하나 또는 양자의 활성 부위들에 기초하여 액션 공간을 필터링한다. 반응은 바람직하 게는 액션 A(t) 및/또는 반응물 R2(t)를 필터링한다. 또한, 임의의 반응물 및 반응에 대해 추가 필터링을 수행하는 것이 바람직할 수 있다. 반응물은 다양한 포맷들로 인코딩될 수 있다. 반응물 R이 분자 구조들의 도메인 특정 벡터 표현으로 코딩되는 경우, 이는 관련 네트워크들을 통해 직접 전달될 수 있다. 그러나, 반응물 R이 그래프 포맷으로 인코딩되는 경 우, 이는 반응물의 간결한 표현을 얻기 위해 학습가능한 또는 사전 훈련된 그래프 컨벌루션 또는 임의의 다른 유형의 층을 통해 전달될 수 있다. 반응물 R이 다른 상이하지만, 마찬가지로 호환불가능한 포맷으로 인코딩되는 경우, 반응물은 적절하고 호환가능하며 간결한 표현으로 변환되기에 적절한 학습가능한 층을 통해 전달된다. 이 러한 간결한 표현은 원하는 도메인 특정 벡터 표현과 동일할 수 있거나, 또한 이의 기능적 등가물일 수 있다. 이제 도 2를 참조하면, 강화 학습 워크플로우의 액터 모듈의 예시적인 실시예가 제시된다. 액터 모듈(11 0)을 구성하는 네트워크에서는 F 및 PI의 두 개의 학습 가능한 네트워크들이 사용될 수 있지만, 임의의 유형 및 임의의 활성화 유닛을 갖는 임의의 수의 신경 네트워크 계층 및 이 사용될 수 있다. 학습가능한 파라미터들을 갖는 임의의 학습가능한 모듈들이 사용될 수 있다. 선택적으로, F 네트워크의 출력은 템플릿 마스크와 요소별로 곱해질 수 있다. 이 템플릿 마스크는 주 어진 반응물에 대해 1의 값들이 유효 템플릿을 나타내고 0의 값들이 무효 템플릿을 나타내는 이진 벡터 또는 텐 서이다. 그 후, 이 곱셈의 출력은 최상의 반응 T를 나타내는 1-핫 벡터/텐서 Tij를 얻기 위해 검블-소프트맥스 (Gumbel-Softmax) 층을 통해 전달될 수 있다. 이 반응 T와 함께 반응물 R1(t)은 프로토 액션을 컴퓨트하기 위해 PI 네트워크에 대한 입력들로서 사용된다. 이에 따라, 이러한 프로토 액션은 연속 공간 ― 이는 전형적으 로 모든 액션들 A의 임베딩에 의해 정의되는 공간임 ― 내에 반응물 R2를 가질 수 있다. 이제 도 3를 참조하면, 강화 학습 워크플로우의 예시적인 크리틱 모듈이 도시된다. 강화 학습 프레임워크 와 관련해서, 크리틱 모듈은 액터 모듈의 출력을 평가한다. 크리틱 모듈에 대한 입력들은 전형적으로 상태 S(t), 반응물 R1(t), 및/또는 반응 T(t)뿐만 아니라 액션 A(t)이다. 액션 A(t)는 프로토 액션 및/또는 반 응물 R2(t)로서 크리틱 모듈에 입력될 수 있다. 크리틱 모듈의 목적은 액션의 \"양호도(goodness)\" Q(S, A)를 계산하거나 평가하는 것이다. 하나 초과의 액터 및/또는 크리틱 모듈들(110, 120)을 활용하는 워크플로우 들이 가능할 수 있다. 선택적으로, 크리틱 모듈은 또한 k-최근접 이웃 모듈의 출력에 의해 제공되는 선택들 중에서 최상의 반응물 또는 반응물들 R을 선택하는 데 사용될 수 있다. 이러한 선택들은 R1(t)를 고려하거나 고려하지 않고, 반응에 의해 좌우되는 모든 유효한 반응물들로부터 도출될 수 있다. 이제 도 4를 참조하면, 강화 학습 워크플로우에서 사용되는 환경의 예시적인 상세한 워크플로우가 설명된 다. 환경은 프로토 액션, 최상의 반응 T, 및/또는 현재 액션 A를 취한다. 그 후, 환경은 다음 상태 (들) S(t+1), 다음 상태(들) S(t+1)에 대한 대응하는 보상, 에피소드의 종료, 및/또는 적용가능한 경우 다음 상 태(들) S(t+1) 각각의 확률들을 예측한다. 환경은 또한 k-최근접 이웃 모듈, 반응 예측기, 스코 어링 함수 모듈, 최대 및/또는 arg-최대 연산자, 및/또는 에이전트의 크리릭 모듈(120b)의 카피를 포 함한다. 이 프로세스 동안, 환경은 프로토 액션, 최상의 반응 T, 및/또는 모든 반응물들 R2(t)의 나머지의 벡터/세 기 표현들을 입력들로서 k-최근접 이웃 모듈에 전달하여, 모든 반응물들 R2(t)의 최상의 반응(T)에 맞는 프로토 액션에 대한 k-최근접 이웃들을 얻는다. 그 후, 이들 k-유효 반응물들 R2K(t)는 대응하는 k개의 생성물 SK(t+1)을 얻기 위해 반응물 R1(t) 및 최상의 반응 T와 함께 반응 예측기 모듈을 통해 전달되며, 그 후 대 응하는 k개의 보상을 얻기 위해 스코어링 함수 모듈에 의해 평가된다. 그 후, max 및/또는 arg-max 연산자 들에 의해 결정되는 바와 같이, 최대 보상에 대응하는 생성물이 선택된다. 선택적으로, 환경에 제공된 크리틱 모듈은 모든 k개의 유효 반응물들 R2(t)의 양호도 Q(S,A) 값들을 평가하고, 주어진 상태 S에 대한 최상의 양호도 값 Q(S,A)에 대응하는 반응물 R2(t)를 반응물 R1(t) 또는 최상 의 반응 T, 및 액션 A(t) 또는 제공된 k개의 유효 반응물들 R2(t)로부터 선택된 반응물 R2(t)로서 선택하는 데 사용될 수 있다. 그 후, 이 최상의 반응물 R2(t)는 반응물 R1(t) 및 최상의 반응 T와 함께 생성물(들) 및/또는 각 생성물의 대응하는 확률들을 얻기 위해 반응 예측기 모듈을 통해 전달된다. 그 후, 얻어진 생성물(들) 은 보상을 컴퓨트하는 스코어링 함수 모듈에 대한 입력으로서 사용된다. 선택적으로, PI 네트워크 출력은 미분가능한 k-최근접 이웃 모듈을 통해 전달될 수 있다. 그 후, 크리틱 네트워크는 k개의 선택된 반응물들로부터 최상의 제2 반응물 R2(t)을 선택하는 데 사용될 수 있다. 환경은이의 스코어링 함수 모듈을 사용하여, 최상의 제2 반응물과 연관된 보상을 계산할 수 있다. 환경의 스코어링 함수 모듈은 규칙 기반 및/또는 물리 기반 방법에 따라 기능할 수 있다. 스코어링 함수들은 또한 기계 학습 기반 방법들을 활용할 수 있다. 스코어링 함수 모듈의 목표는 반응들에 수반되는 화합물들의 물리적, 화학적, 기능적, 전기적, 양자 기계적, 구조적, 생물물리적 및/또는 생화학적 속성들을 예 측하고/하거나 결정하는 것이다. 생화학적 속성들은 예를 들어, 세포, 조직 또는 심지어 전체 유기체와 연관된 수용체, 효소 등과 같은 단일 또는 다중 생물학적 표적에 대한 활성을 기술할 수 있다. 반응 예측 모듈들은 제공된 반응물들 및 반응 및 필요한 경우 대응하는 조건들에 기초하여 화학 반응의 결 과를 예측하는 데 활용될 수 있다. 예측 모델들은 또한 N-성분 변환을 이용할 수 있으며, 이는 제2 반응물 이 불필요하고; 단지 단일 반응물, 반응 및 필요한 경우 대응하는 조건이 필요한 반응의 유형을 나타낸다. 예측 모듈은 SMARTS 또는 다른 포맷들 및 표현들을 사용하는 반응을 수용하도록 구조화될 수 있다. 환경의 반응 예측 모듈은 또한 에피소드를 구성할 수 있는 단일 또는 다단계 가상 합성 경로의 종료 를 결정하기 위해 전술한 방법들을 활용하도록 제공된다. \"에피소드\"는 최종 또는 중간 생성물의 합성을 구성하는 프로세스로서 정의된다. 에피소드들은 적어도 하나의 단계로 구성되며, 단계는 프로세스의 단계를 수행하는 것과 관련된 정보를 포함한다. 단계는 예를 들어, 사용되 는 반응물, 반응을 촉진하는 데 필요할 수 있는 임의의 환경적 팩터들, 및/또는 달성에 필요한 임의의 촉매 또 는 다른 비반응 성분을 포함할 수 있다. 이러한 방식으로, 에피소드는 최종 또는 중간 생성물이 합성될 수 있는 일종의 방안 또는 수단을 사용자에게 제공하는 것으로 의미된다. 환경의 보상 및/또는 스코어링 함수 모듈은 원하는 및/또는 특정 속성들에 대한 예측된 및/또는 실험 적인 물리적, 화학적, 기능적, 전기적, 양자 기계적, 구조적, 생물물리학적 그리고/또는 생화학적 속성들에 따 라 반응물들 및/또는 생성물들을 스코어링한다. 이들 화학적 속성들은 반응물들 및/또는 생성물들과 함께, 기계 판독가능한 포맷으로 저장될 수 있다. 이러한 기계 판독가능한 포맷은 다양한 스테이지들에서, 인간 판독가능한 포맷들과 기계 학습 워크플로우에 바람직한 포맷들 사이에서 변환될 수 있다. 스코어링 함수 모듈은 입력으로서 화학적 화합물을 취할 수 있고, 차례로, 화합물의 하나 이상의 속성, 거 동 및/또는 특성과 연관된 대응하는 값을 출력한다. ㅁ스코어링 함수 모듈은 기계 학습 모델 및/또는 모델 들의 앙상블, 분자 또는 양자 역학 시뮬레이션, 및/또는 실험적 값들을 활용하는 것에 제한되지 않는다. 스코어 링 함수 모듈은 가중 팩터들을 활용함으로써 이들 속성들 중 하나 이상을 이들 방법들 중 하나 이상과 조 합할 수 있다. 도 5를 참조하면, 강화 학습 워크플로우의 일부로서 이중 액터-크리틱 워크플로우를 설명하는 예시적인 워 크플로우가 도시되어 있다. 강화 학습 워크플로우는 단일 액터-크리틱 워크플로우 대신에 이중 액터 및/또는 크 리틱 워크플로우를 포함할 수 있다. 이러한 방식으로, 미니-액터 및 미니-크리틱은 자신들의 액 터-크리틱 대응물들(110, 120)을 동반한다. 하나 초과의 미니-액터 모듈들 및/또는 미니-크리틱 모듈 들을 포함하는 워크플로우들이 또한 가능할 수 있다. 액터-크리틱 모듈들(110, 120)에 추가하여 하나 이상의 미 니-액터 모듈만을 포함하는 워크플로우들, 또는 액터-크리틱 모듈들(110, 120)에 추가하여 하나 이상의 미 니-크리틱 모듈만을 포함하는 워크플로우들이 가능할 수 있다. 미니-액터 모듈은 입력으로서 적어도 하나의 반응물이 주어지면 선택될 반응의 확률들을 나타내는 벡터/텐 서를 출력할 수 있다. 미니-크리틱 모듈은 필요한 경우, 미니-액터 모듈의 출력을 평가할 수 있다. 이중 액터-크리틱 워크플로우의 다른 실시예, 즉, 다중 액터-크리틱 워크플로가 도 6에 도시되어 있 다. 이중 액터-크리틱 워크플로우와 같이, 다중 액터-크리틱 워크플로우도 필요에 따라 임의의 수의 액터, 크리틱, 미니-액터, 또는 미니-크리틱 모듈을 활용할 수 있다. 피라미드형 액터-크리틱 워크플로우와 동의어로 설명되는 이중 액터-크리틱 워크플로우의 다른 실시예는 다음과 같이 공식화될 수 있고, 도 7에 도시되어 있다. 결정론적 전이 함수의 가정 하에서, 다음 상태의 값 함 수 V(s^')는 현재 상태 S 및 액션 A 쌍의 Q(S,A) 값과 정확히 동일하다. 이러한 가정은 \"크리틱\"이 크리틱 내부 의 두 개의 모듈들, 즉 곱 예측기 및 값 함수 예측기에 의해 정의되는 2-단계 프로세스로 나뉠 수 있게 한다. 값 함수 예측기는 예를 들어, 생성물의 다음 상태 s^'의 값 함수 V(s^')를 예측한다. 생성물 예측기는 두 개의 상이한 네트워크들, 즉 단분자 반응(uni-molecular reaction)을 처리하기 위한 U-net 및 이분자 반응(bi-molecular reaction)을 처리하기 위한 B-net을 갖는다. U-net은 R 및 반응 T, 또는 이들의 임의의 표현들을 입력들로서 취하고, 다음 식에 따라 가설적 생성물의 표 현을 컴퓨트한다: B-net은 R, R 및 반응 T, 또는 이들의 임의의 표현들을 입력들로서 취하고, 다음 식에 따라 가설적 생성 물의 표현을 컴퓨트한다: 그 후, 두 개의 가설적 생성물들을 조합하여, 다음 식에 따라 반응이 단분자인지 또는 이분자인지에 따라 적절 한 R마스크를 사용하여 화학 반응의 가설적 최종 생성물 P를 컴퓨트한다: 이들 가설적 생성물 예측기 모듈들로부터 얻어진 이 최종 가설적 생성물은 Q(S,A)를 얻기 위해 학습가능한 값 함수 모듈 V를 통해 전달된다. 피라미드형 액터-크리틱 워크플로우의 다른 실시예는 다음과 같이 공식화될 수 있고, 도 8에 도시되어 있다. 정 책 네트워크 PI가 L개의 계층들 ― 여기서 이 PI 네트워크의 파라미터들을 나타냄 ―, 뿐만 아니라 PI 네트워 크 내의 다양한 서브네트워크들 ― 여기서 가 정책 네트워크(PI) 내의 계층 0 내지 I를 나타냄 ― 을 갖는 다고 고려한다. 이들 층들은 L개의 이러한 가능한 미니-정책 네트워크들 중 하나를 구성한다. 다음으로, 현재 상태에서 정책 모델 PI의 계층 I의 출력을 취하는 다른 신경 네트워크 을 고려한다. 이 또한 네트워크의 출력으로서 보여질 수 있다. 신경 네트워크 은 가설적 다음 상태를 예측한다. 의 출력은 다음 상태들의 공 간 ― 이는 임의의 표현 공간일 수 있음 ― 에 있을 필요는 없다. 이러한 하나의 가설적 다음 상태 예측 모듈(HyNeSP)은 미니-정책 모듈들 각각에 부여될 수 있다. 이들 상 태들은 의 출력일 수 있는 실제 액션 A 없이는 진정한 다음 상태를 예측하지 못할 수 있기 때문에 가설적인 것일 뿐이다. 이제, 결정론적 전이 함수 ― 다시 말해, 현재 상태 S 및 액션 A가 주어지면 하나의 다음 상태 s'만이 가능함 ― 를 이용한 환경을 고려한다. 이러한 경우들에서, 현재 상태 S 및 액션 A의 Q(S,A) 함수는 다음 상태 S'의 값 함수, 또는 와 동일하다. 여기서 알고리즘들은 전이 함수가 비결정적이거나, 또는 일 때에도 사용될 수 있다. 가설적 다음 상태 h'가 HyNeSP들 중 하나를 사용하여 얻어지면, Q(S,A)를 컴퓨트하는 것은 가설적 다음 상태의 값 함수를 컴퓨트하는 것과 동등해진다. 이에 따라, 가설적 다음 상태 h'를 입력으로서 취하고 이의 값 함수를 예측하는 새로운 네트워크 V가 도입될 수 있다. m개의 상이한 HyNeSP들로부터 선택될 가설적 곱은 고정된 또는 학습가능한 확률 텐서로부터 샘플링하고 이를 1- 핫 텐셔서로 변환함으로써 결정되어, HyNeSP 마스크 Mh를 유도한다. 훈련을 위한 다른 방법은 크로스 엔트로피(cross-entropy) 방법들, 또는 보다 광범위하게는, 모델 예측 제어를 활용하는 것을 수반한다. 액터의 출력들에 잡음이 추가될 수 있고, 보상에 기초하여, 최적의 잡음 분포가 결정 되고/되거나 컴퓨트될 수 있다. 이 잡음 분포는 임의의 확률 분포를 사용하여 초기에 모델링될 수 있다. 이 프 로세스는 사전 훈련된 액터 네트워크들 상에서 그리고/또는 훈련 단계 동안 사용될 수 있다. 선택적으로, 잡음 은 네트워크들의 파라미터들에 직접 추가될 수 있다. 액터 네트워크들을 훈련하기 위한 다른 방법은 전문가 에이전트, 유사 전문가 시연 및/또는 MCTS(Monte Carlo Tree Searc) 시뮬레이션을 통해 감독 학습을 활용할 수 있다. 연속적인 액션 공간들에서 MCTS를 처리하기 위한 신규한 훈련 전략들이 도입될 수 있다. 정책 손실 또는 액터 손실은 출력 정책 분포와 표적 정책 분포 사이의 편차를 최소화하는 것을 목표로 한다. 도 9를 참조하면, 초기 반응물(들)을 선택하는 것을 학습하는 강화 학습 워크플로우가 도시되어 있다. 본 발명 의 하나의 잠재적인 실시예는 새로운 목적 함수를 기존의 강화 학습 프레임워크에 도입하는 것을 포함한다. 기존의 접근법들은 유한 또는 무한 에피소드 설정에서 가변하는 수의 시간 단계들로, 디스카운트 되거나 디스카운 트되지 않는 전체 보상, 또는 디스카운트 되거나 디스카운트되지 않는 보상의 함수를 최적화하는 것에 주로 집 중되지만, 본 발명은 그 대신에 전체 에피소드에서 달성되는 최대 보상을 최적화할 수 있다. 본 발명은 새로운 목적을 최적화하기 위해 임의의 강화 학습 설정에서 사용되는 Q-함수 및 다른 함수들/변수들에 대한 새로운 벨 만(Bellman) 방정식을 활용할 수 있다. Q-함수는 다음과 같이 정의될 수 있다:"}
{"patent_id": "10-2022-7030038", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "그리고, 상응하여, 벨만 방정식은 다음과 같은 형태를 띨 수 있다:"}
{"patent_id": "10-2022-7030038", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "에피소드의 반환은 다음과 같이 정의될 수 있다: . 이는 선택적으로 디스카운트 팩터 감마에 의 해 스케일링될 수 있고, 반환은 로서 정의될 수 있다. 초기 상태가 고정되거나, 주어지거나, 또는 랜덤하게 샘플링되는 기존의 방법들과는 대조적으로, 본 발명은 초 기 상태를 선택하는 것을 학습할 수 있다. 무작위 잡음이 샘플링되고, 그 출력이 반응물들의 임의의 바람직한 표현의 공간에 있는 발생기 네트워크 G를 통해 전달될 수 있다. 발생기 네트워크 G의 출력은 k-최근접 이웃 알 고리즘들을 사용하여 유효한 초기 반응물에 매핑될 수 있다. 달성된 보상을 손상시키지 않으면서 생성된 분자의 다양성을 촉진하기 위해, 발생기 네트워크 G를 단일 지점 또는 단일 영역으로 접철시키는 것을 피하기 위한 기 술이 채용될 수 있다. 이를 위해 사용될 수 있는 기술들의 예들은 일반화(regularization), 발생기 네트워크의 출력들의 클러스터간 거리를 최대화하는 것에 의한 소프트 k-평균 클러스터링(soft k-means clustering), 보상 다양성을 위해 환경으로부터의 보상을 수정하는 것, 또는 다수의 발생기들을 사용하는 것을 포함하지만, 이에 제한되지 않는다. 선택적으로, 추가적인 크리틱 네트워크가 발생기 네트워크 G의 출력을 평가하고 발생기 네트 워크 G의 파라미터들을 액터-크리틱 방식으로 업데이트하기 위해 이용될 수 있다. 대안적으로, 상이한 정책 그 래디언트 알고리즘이 임의의 미분가능한 버전의 k-최근접 이웃과 같은 k-최근접 이웃 대신에 사용될 수 있다. 순방향 합성 프레임워크를 훈련하기 위한 다른 방법은 도 10에 도시된 바와 같은 강화 학습 에이전트들에 대한 대안으로서 유전 알고리즘 에이전트들을 이용할 수 있다. 유전 알고리즘들에서, 개체가 소유하는 유전자 또는 특징부 세트는 염색체로 불린다. 모집단 내의 각 개체 염색체는 강화 학습 기반 구현예와 유사한 반응물 들의 공간에서의 일련의 프로토 액션들로서 표현될 수 있다. 염색체의 제1 부분은 초기 반응물 R1의 선택을 담 당하는 반응물의 특징부들의 공간에서의 다차원 프로토 액션으로 시작한다. 선택적으로, 이 단계는 스킵될 수 있고, 초기 반응물 R1을 선택하기 위한 임의의 다른 방법이 사용될 수 있다. 염색체의 후속 부분들은 순방향 합 성의 각 단계에서 상태 분자가 반응할 제2 반응물들의 선택을 담당하는 일련의 프로토 액션들이다. 그 후, 각 개체 염색체가 환경에 의해 평가된다. 환경은 염색체를 취하고, k-최근접 이웃 알고리즘을 인코딩된 프로토 액션들에 적용하여, 정의된 특징부 공간에서 프로토 액션에 가장 가까운 유효한 초기 반응물 및 후속 제 2 반응물들을 선택한다. 그 후, 환경은 다단계 순방향 합성을 수행하여 각 단계에서 화학적 화합물들을 생성하 고 각 분자의 보상 값을 컴퓨트한다. 다단계 순방향 합성 프로세스 동안 달성된 최적화된 보상 값들은 각 개체 의 염색체에서 인코딩되고, 이의 적합도 값으로서 반환된다. 초기 모집단 세대에서, 개체 염색체들은 무작위로 초기화된다. 초기 모집단이 평가되면, 초기 모집단은 교차 를 거친다. 교차 동안, 특징부들은 두 개의 개체 염색체들 사이에서 무작위로 교환되어 자손 세트를 생성 한다. 교차는 단일 개체의 염색체에서의 특징 값들의 무작위 변형인 돌연변이가 이어진다. 돌연변이 이후, 최상의 개체들이 스코어링 함수 모듈을 통해 결정되고 새로운 모집단 또는 세대를 형성하는 데 사 용되는 자신들의 보상에 따라 선택된다. 교차 및 돌연변이 사건들은 미리 정의된 확률을 갖는 특정 개체들에 대 해 촉발된다. 선택적으로, 하나 이상의 신경 네트워크가 유전 알고리즘에 의해 생성된 샘플들 또는 프로토 액션들에 대해 훈 련될 수 있다. 또한, 이들 신경 네트워크들로부터의 샘플들은 추가 교차 및 돌연변이 조작들이 수행될 수 있도록 하는 초기 염색체로서 사용될 수 있다. 이들 두 프로세스들은 서로 개선하기 위해 동시에, 순차적으로, 동기 적으로, 그리고/또는 비동기적으로 실행될 수 있다. 선택적으로, 하나 이상의 신경 네트워크, 액터 및/또는 크리틱 네트워크가 유전 알고리즘에 의해 생성된 바와 같은, 반응물들, 프로토 액션, 생성물들 및 보상들의 투플들과 같은 샘플들로부터 샘플링함으로써 강화 학습 프 레임워크에서 훈련될 수 있다. 이들 신경 네트워크들로부터의 샘플들은 초기 염색체로서 사용될 수 있고, 추가 교차 및 돌연변이 조작들이 이들 염색체들에 대해 수행될 수 있다. 이들 두 프로세스들은 서로 개선하기 위해 동시에, 순차적으로, 동기적으로, 그리고/또는 비동기적으로 실행될 수 있다. 선택적으로, 하나 이상의 신경 네트워크, 액터 및/또는 크리틱 네트워크가 상술된 임의의 유전 알고리즘들 또는 계획 방법들을 모방하거나, 흉내내거나, 복제하기 위해 강화 학습 프레임워크에서 훈련될 수 있다. 전술한 설명 및 첨부 도면은 본 발명의 원리, 바람직한 실시예 및 동작 모드를 예시한다. 그러나, 본 발명은 전 술한 특정 실시예에 한정되는 것으로 해석되어서는 안 된다. 위에서 논의된 실시예들의 추가적인 변형들이 당업 자에 의해 인식될 것이다(예를 들어, 본 발명의 특정 구성들과 연관된 특징들은 대신에, 원하는 바와 같이, 발 명의 임의의 다른 구성들과 연관될 수 있다). 이에 따라, 전술한 실시예들은 제한적인 것이 아니라 예시적인 것으로 간주되어야 한다. 따라서, 이러한 실시예 들에 대한 변형들은 다음의 청구항들에 의해 정의된 바와 같은 본 발명의 범위로부터 벗어나지 않고 당업자에 의해 이루어질 수 있다는 것이 인식되어야 한다."}
{"patent_id": "10-2022-7030038", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 발명의 실시예들의 이점들은 본 발명의 예시적인 실시예들에 대한 다음의 상세한 설명으로부터 명백해질 것 이며, 이러한 설명은 같은 도면 부호들이 같은 요소들을 나타내는 첨부 도면들과 함께 고려되어야 하며, 이러한 첨부 도면들에서: 도 1은 강화 학습 워크플로우의 예시적인 실시예이다. 도 2는 강화 학습 워크플로우의 액터 모듈의 예시적인 실시예이다. 도 3은 강화 학습 워크플로우의 크리틱 모듈의 예시적인 실시예이다. 도 4는 강화 학습 워크플로우의 환경의 예시적인 실시예이다. 도 5는 이중 액터 및 크리틱 워크플로우를 갖는 강화 학습 워크플로우의 예시적인 실시예이다. 도 6은 다중 액터 및 크리틱 워크플로우를 갖는 강화 학습 워크플로우의 예시적인 실시예이다. 도 7은 다중 액터 및 크리틱을 이용한 강화 학습 워크플로우의 예시적인 실시예이다. 도 8은 미분가능한 k-최근접 이웃 모듈을 활용한 강화 학습 워크플로우의 예시적인 실시예이다. 도 9는 초기 반응물(들)을 선택하는 것을 학습하는 강화 학습 워크플로우의 예시적인 실시예이다. 도 10은 유전 알고리즘 워크플로우의 예시적인 실시예이다."}
