{"patent_id": "10-2023-0055653", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0143601", "출원번호": "10-2023-0055653", "발명의 명칭": "미디어 콘텐트를 제공하는 방법 및 서버", "출원인": "삼성전자주식회사", "발명자": "천재민"}}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "서버가 미디어 콘텐트를 제공하는 방법에 있어서,비디오 데이터 및 오디오 데이터를 포함하는 미디어 콘텐트를 획득하는 단계(S210);상기 비디오 데이터를 분석하여 비디오와 관련된 제1 콘텍스트 데이터를 획득하는 단계(S220);상기 오디오 데이터를 분석하여 오디오와 관련된 제2 콘텍스트 데이터를 획득하는 단계(S230);상기 제1 콘텍스트 데이터 및 상기 제2 콘텍스트 데이터에 기초하여, 상기 미디어 콘텐트의 비디오 프레임들에대응하는 장면 콘텍스트 데이터를 생성하는 단계(S240);사용자 입력에 기초하여, 상기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정하는 단계(S250);상기 장면 콘텍스트 데이터에 기초하여, 상기 사용자 의도에 대응하는 적어도 하나의 비디오 프레임을 식별하는단계(S260); 및상기 식별된 적어도 하나의 비디오 프레임을 출력하는 단계(S270)를 포함하는, 방법."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 미디어 콘텐트는 텍스트 데이터를 포함하고,상기 방법은, 상기 텍스트 데이터를 분석하여 오디오와 관련된 제3 콘텍스트 데이터를 획득하는 단계를 더 포함하고,상기 장면 콘텍스트 데이터를 생성하는 단계는,상기 제3 콘텍스트 데이터에 더 기초하여, 상기 장면 콘텍스트 데이터를 생성하는 것인, 방법."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 제1 콘텍스트 데이터를 획득하는 단계는,상기 비디오 데이터의 비디오 프레임들 중 적어도 일부에 객체 인식을 적용하여 장면 정보를 획득하는 단계;상기 장면 정보에 기초하여 적어도 하나의 비디오 프레임에 대응하는 적어도 하나의 장면 그래프를 생성하는 단계; 및상기 적어도 하나의 장면 그래프에 기초하여, 비디오의 콘텍스트를 나타내는 상기 제1 콘텍스트 데이터를 획득하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 제2 콘텍스트 데이터를 획득하는 단계는,상기 오디오 데이터에 음성 인식, 사운드 이벤트 검출, 사운드 이벤트 분류 중 적어도 하나를 적용하여, 장면-공개특허 10-2024-0143601-3-사운드 정보를 획득하는 단계; 및상기 장면-사운드 정보에 기초하여, 오디오의 콘텍스트를 나타내는 상기 제2 콘텍스트 데이터를 획득하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 제3 콘텍스트 데이터를 획득하는 단계는,상기 텍스트 데이터에 자연어 처리를 적용하여, 장면-텍스트 정보를 획득하는 단계; 및상기 장면-텍스트 정보에 기초하여, 텍스트의 콘텍스트를 나타내는 상기 제3 콘텍스트 데이터를 획득하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제5항 중 어느 한 항에 있어서,상기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정하는 단계는,상기 사용자 입력이 발화인 것에 기초하여, 자동 음성 인식(Automatic Speech Recognition; ASR)을 수행하는단계; 및상기 자동 음성 인식 결과에 자연어 이해(Natural Language Understanding; NLU) 알고리즘을 적용하여, 상기사용자 의도를 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 방법은,출력된 상기 적어도 하나의 비디오 프레임 중 하나를 선택하는 사용자 입력에 기초하여, 상기 선택된 비디오 프레임부터 상기 미디어 콘텐트가 재생되도록 하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 사용자 입력은, 상기 미디어 콘텐트를 편집하기 위한 적어도 하나의 키워드를 포함하고,상기 적어도 하나의 비디오 프레임을 식별하는 단계는,상기 장면 콘텍스트 데이터에 기초하여, 상기 적어도 하나의 키워드에 대응하는 비디오 프레임을 식별하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 방법은,공개특허 10-2024-0143601-4-상기 미디어 콘텐트를 편집하기 위한 사용자 인터페이스를 제공하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 방법은,상기 미디어 콘텐트의 편집 결과 요약을 제공하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "미디어 콘텐트를 제공하는 서버(2000)에 있어서,통신 인터페이스(2100);하나 이상의 인스트럭션들을 저장하는 메모리(2200); 및상기 하나 이상의 인스트럭션들을 실행하는 적어도 하나의 프로세서(2300)를 포함하고,상기 적어도 하나의 프로세서(2300)는, 상기 하나 이상의 인스트럭션들을 실행함으로써,비디오 데이터 및 오디오 데이터를 포함하는 미디어 콘텐트를 획득하고,상기 비디오 데이터를 분석하여 비디오와 관련된 제1 콘텍스트 데이터를 획득하고,상기 오디오 데이터를 분석하여 오디오와 관련된 제2 콘텍스트 데이터를 획득하고,상기 제1 콘텍스트 데이터 및 상기 제2 콘텍스트 데이터에 기초하여, 상기 미디어 콘텐트의 비디오 프레임들에대응하는 장면 콘텍스트 데이터를 생성하고,사용자 입력에 기초하여, 상기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정하고,상기 장면 콘텍스트 데이터에 기초하여, 상기 사용자 의도에 대응하는 적어도 하나의 비디오 프레임을식별하고,상기 식별된 적어도 하나의 비디오 프레임을 출력하는, 서버."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 미디어 콘텐트는 텍스트 데이터를 포함하고,상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써,상기 텍스트 데이터를 분석하여 오디오와 관련된 제3 콘텍스트 데이터를 획득하고,상기 제3 콘텍스트 데이터에 더 기초하여, 상기 장면 콘텍스트 데이터를 생성하는, 서버."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써,상기 비디오 데이터의 비디오 프레임들 중 적어도 일부에 객체 인식을 적용하여 장면 정보를 획득하고,상기 장면 정보에 기초하여 적어도 하나의 비디오 프레임에 대응하는 적어도 하나의 장면 그래프를 생성하고,상기 적어도 하나의 장면 그래프에 기초하여, 비디오의 콘텍스트를 나타내는 상기 제1 콘텍스트 데이터를 획득공개특허 10-2024-0143601-5-하는, 서버."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써,상기 오디오 데이터에 음성 인식, 사운드 이벤트 검출, 사운드 이벤트 분류 중 적어도 하나를 적용하여, 장면-사운드 정보를 획득하고,상기 장면-사운드 정보에 기초하여, 오디오의 콘텍스트를 나타내는 상기 제2 콘텍스트 데이터를 획득하는,서버."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써,상기 텍스트 데이터에 자연어 처리를 적용하여, 장면-텍스트 정보를 획득하고,상기 장면-텍스트 정보에 기초하여, 텍스트의 콘텍스트를 나타내는 상기 제3 콘텍스트 데이터를 획득하는,서버."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항 내지 제 15항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써,상기 사용자 입력이 발화인 것에 기초하여, 자동 음성 인식(Automatic Speech Recognition; ASR)을 수행하고,상기 자동 음성 인식 결과에 자연어 이해(Natural Language Understanding; NLU) 알고리즘을 적용하여, 상기사용자 의도를 결정하는, 서버."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항에 있어서,상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써,출력된 상기 적어도 하나의 비디오 프레임 중 하나를 선택하는 사용자 입력에 기초하여, 상기 선택된 비디오 프레임부터 상기 미디어 콘텐트가 재생되도록 하는, 서버."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항에 있어서,상기 사용자 입력은, 상기 미디어 콘텐트를 편집하기 위한 적어도 하나의 키워드를 포함하고,상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써,상기 장면 콘텍스트 데이터에 기초하여, 상기 적어도 하나의 키워드에 대응하는 비디오 프레임을 식별하는, 서버.공개특허 10-2024-0143601-6-청구항 19 제11항에 있어서,상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써,상기 미디어 콘텐트를 편집하기 위한 사용자 인터페이스를 제공하는, 서버."}
{"patent_id": "10-2023-0055653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제1항 내지 제10항 중 어느 한 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수있는 기록매체."}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "서버가 미디어 콘텐트를 제공하는 방법이 제공된다. 상기 방법은, 비디오 데이터 및 오디오 데이터를 포함하는 미디어 콘텐트를 획득하는 단계; 상기 비디오 데이터를 분석하여 비디오와 관련된 제1 콘텍스트 데이터를 획득하 는 단계; 상기 오디오 데이터를 분석하여 오디오와 관련된 제2 콘텍스트 데이터를 획득하는 단계; 상기 제1 콘텍 스트 데이터 및 상기 제2 콘텍스트 데이터에 기초하여, 상기 미디어 콘텐트의 비디오 프레임들에 대응하는 장면 콘텍스트 데이터를 생성하는 단계; 사용자 입력에 기초하여, 상기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정하는 단계; 상기 장면 콘텍스트 데이터에 기초하여, 상기 사용자 의도에 대응하는 적어도 하나의 비디오 프 레임을 식별하는 단계; 및 상기 식별된 적어도 하나의 비디오 프레임을 출력하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "미디어 콘텐트를 제공하는 서버, 미디어 콘텐트를 제공하는 시스템 및, 미디어 콘텐트를 분석하여 미디어 콘텐 트의 탐색 및 편집을 제공하는 방법이 제공된다."}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "다양한 유형의 멀티미디어 콘텐트가 다양한 형태의 미디어를 통해 사용자에게 공급되고 있다. 사용자는 멀티미 디어 콘텐트를 사용자의 클라이언트 디바이스를 통해 제공 받을 수 있다. 한편, 멀티미디어 콘텐트의 탐색과 같은 원격 제어 인터랙션은, 리모컨, 키보드, 마우스, 마이크 등과 같은 제 어 장치를 통해 수행된다. 사용자가 멀티미디어 콘텐트를 되감거나, 빨리감기 하고자 하는 경우, 멀티미디어 콘 텐트의 탐색은 기 설정된 시간 구간(예를 들어, 10초 앞으로)만큼 이동하는 것을 통해 이루어지거나, 멀티미디 어 콘텐트의 프로바이더에 의해 정해진 기 설정된 장면으로 이동하는 것을 통해 이루어진다. 사용자가 멀티미디어 콘텐트를 탐색할 때, 특정 시간 간격 또는 특정 타임 스탬프에 대응되는 시점으로 이동하 는 것이 아닌, 사용자의 자연어 입력에 따라 자유로운 시점으로 정확하고 편리하게 장면 이동을 제공할 수 있는 방법이 필요하다."}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면에 따르면, 서버가 미디어 콘텐트를 제공하는 방법이 제공될 수 있다. 상기 방법은, 비디오 데이터 및 오디오 데이터를 포함하는 미디어 콘텐트를 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 비 디오 데이터를 분석하여 비디오와 관련된 제1 콘텍스트 데이터를 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 오디오 데이터를 분석하여 오디오와 관련된 제2 콘텍스트 데이터를 획득하는 단계를 포함할 수 있 다. 상기 방법은, 상기 제1 콘텍스트 데이터 및 상기 제2 콘텍스트 데이터에 기초하여, 상기 미디어 콘텐트의 비디오 프레임들에 대응하는 장면 콘텍스트 데이터를 생성하는 단계를 포함할 수 있다. 상기 방법은, 사용자 입 력에 기초하여, 상기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정하는 단계를 포함할 수 있다. 상기 방 법은, 상기 장면 콘텍스트 데이터에 기초하여, 상기 사용자 의도에 대응하는 적어도 하나의 비디오 프레임을 식 별하는 단계를 포함할 수 있다. 상기 방법은, 상기 식별된 적어도 하나의 비디오 프레임을 출력하는 단계를 포 함할 수 있다. 본 개시의 일 측면에 따르면, 미디어 콘텐트를 제공하는 서버가 제공될 수 있다. 상기 서버는, 통신 인터페이스; 하나 이상의 인스트럭션들을 저장하는 메모리; 및 상기 하나 이상의 인스트럭션들을 실행하는 적어 도 하나의 프로세서를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행 함으로써, 비디오 데이터 및 오디오 데이터를 포함하는 미디어 콘텐트를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 비디오 데이터를 분석하여 비디오와 관련된 제1 콘텍스트 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 오디오 데이터를 분석하여 오디오와 관련된 제2 콘텍스트 데이터를 획득할 수 있다. 상기 적 어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 제1 콘텍스트 데이터 및 상기 제2 콘텍스트 데이터에 기초하여, 상기 미디어 콘텐트의 비디오 프레임들에 대응하는 장면 콘텍스트 데이터를 생성할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 사용자 입력 에 기초하여, 상기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정할 수 있다. 상기 적어도 하나의 프로세 서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 장면 콘텍스트 데이터에 기초하여, 상기 사용자 의 도에 대응하는 적어도 하나의 비디오 프레임을 식별할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이 상의 인스트럭션들을 실행함으로써, 상기 식별된 적어도 하나의 비디오 프레임을 출력할 수 있다. 본 개시의 일 측면에 따르면, 미디어 콘텐트를 제공하는 디스플레이 장치가 제공될 수 있다. 상기 디스플레이 장치는, 통신 인터페이스; 디스플레이; 하나 이상의 인스트럭션들을 저장하는 메모리; 및 상기 하나 이상의 인 스트럭션들을 실행하는 적어도 하나의 프로세서를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 비디오 데이터 및 오디오 데이터를 포함하는 미디어 콘텐트를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 비디오 데이터를 분석하여 비디오와 관련된 제1 콘텍스트 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 오디오 데이터를 분석하여 오디오와 관련된 제2 콘텍스트 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 제1 콘 텍스트 데이터 및 상기 제2 콘텍스트 데이터에 기초하여, 상기 미디어 콘텐트의 비디오 프레임들에 대응하는 장 면 콘텍스트 데이터를 생성할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행 함으로써, 사용자 입력에 기초하여, 상기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 장면 콘텍스트 데이터에 기초 하여, 상기 사용자 의도에 대응하는 적어도 하나의 비디오 프레임을 식별할 수 있다. 상기 적어도 하나의 프로 세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 식별된 적어도 하나의 비디오 프레임이 상기 디 스플레이의 화면에 출력되도록 할 수 있다. 본 개시의 일 측면에 따르면, 서버 또는 디스플레이 장치가 미디어 콘텐트를 제공하는, 전술 및 후술하는 방법 들 중 어느 하나를 실행시키기 위한 프로그램이 기록된 컴퓨터 판독 가능 기록매체를 제공할 수 있다."}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에서, \"a, b 또는 c 중 적어도 하나\" 표현은 \" a\", \" b\", \" c\", \"a 및 b\", \"a 및 c\", \"b 및 c\", \"a, b 및 c 모두\", 혹은 그 변형들을 지칭할 수 있다. 본 개시에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의 미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 또한, 본 명세서에서 사용되는 '제1' 또는 '제2' 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용할 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만 사용된다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소 프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해서 설 명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 또한, 각각의 도면에서 사용된 도면 부호는 각각의 도면을 설명하기 위한 것일 뿐, 상이한 도면들 각각 에서 사용된 상이한 도면 부호가 상이한 요소를 나타내기 위한 것은 아니다. 이하 첨부된 도면을 참고하여 본 개시를 상세히 설명하기로 한다. 도 1은 본 개시의 일 실시예에 따른 서버가 미디어 콘텐트 서비스를 제공하는 것을 개략적으로 도시한 도면이다. 도 1을 참조하면, 서버는 미디어 콘텐트에 관련된 데이터를 처리할 수 있다. 일 실시예에서, 서버는 미디어 콘텐트를 스트리밍 가능한 콘텐트 프로바이더 서버일 수 있다. 이 경우, 서버는 미디어 콘텐트를 클라이언트 디바이스인 디스플레이 장치로 제공하면서 미디어 콘텐트의 분석을 수행하고, 본 개시의 실시예에 따라, 사용자가 디스플레이 장치를 이용하여 미디어 콘텐트의 탐색/편집 기능을 제공받도록 할 수 있다. 일 실시예에서, 서버는 미디어 콘텐트를 스트리밍하는 콘텐트 프로바이더 서버와 별개로 마련된 서버일 수 있다. 이 경우, 서버는 미디어 콘텐트를 획득하여 미디어 콘텐트의 분석을 수행하고, 본 개시의 실시 예에 따라, 사용자가 디스플레이 장치를 이용하여 미디어 콘텐트의 탐색/편집 기능을 제공받도록 할 수 있다. 일 실시예에서, 서버는 미디어 콘텐트를 시청하는 사용자로부터의 사용자 입력을 처리할 수 있다. 사용자 입력은 발화 입력일 수 있으나, 이에 한정되지 않고 텍스트 등의 입력일 수도 있다. 또한, 사용자 입력은 자연어 문장 형태일 수 있으나, 이에 한정되지 않고 키워드 등의 형태일 수도 있다. 예를 들어, 사용자 가 제1 화면에 표시된 것과 같이 미디어 콘텐트를 시청하던 도중에, 이전의 장면을 탐색하고자 하는 경우,사용자는 발화 형태의 사용자 입력, 예를 들어, \"OOO 장면 찾아 줘\"와 같은 문장을 입력할 수 있다. 서버 는 이 경우, 사용자의 자연어 입력을 처리하여 사용자의 의도를 식별하고, 사용자의 의도에 대응하는 장 면을 검색하여 사용자에게 제공할 수 있다. 일 실시예에서, 미디어 콘텐트에는 비디오 데이터, 오디오 데이터, 텍스트 데이터 등이 포함될 수 있다. 서버 는 사용자의 의도에 대응하는 장면을 검색하기 위해, 미디어 콘텐트를 분석하여 장면 콘텍스트 데이터를 생성할 수 있다. 미디어 콘텐트의 분석은, 비디오 분석, 오디오 분석, 텍스트 분석 각각 및 이들의 조합을 통해 수행될 수 있다. 서버는 사용자의 자연어 발화를 처리하고 장면 콘텍스트 데이터에 기초하여 대응되는 장면을 검색함으로써, 미 디어 콘텐트 내의 사용자가 원하는 탐색 시점을 정확하고 편리하게 제공할 수 있다. 서버가 미디어 콘텐트를 분석하고 처리하여 콘텐트 내비게이션 및/또는 콘텐트 편집을 제공하는 구체적인 동작들에 대하여, 후술하는 도면들과 그에 대한 설명을 통해 더 상세하게 기술하기로 한다. 도 2는 본 개시의 일 실시예에 따른 서버가 미디어 콘텐트 서비스를 제공하는 동작을 설명하기 위한 흐름도이다. 단계 S210에서, 서버는 비디오 데이터 및 오디오 데이터를 포함하는 미디어 콘텐트를 획득한다. 본 개시에서, 미디어 콘텐트는 영화, TV 프로그램, 다큐멘터리, 기타 비디오 콘텐트 등을 포함하는 다양한 미디 어 콘텐트를 지칭하며, 멀티미디어 콘텐트라고 지칭될 수도 있다. 일 실시예에서, 미디어 콘텐트는 미디어 데이터를 패키징하는 표준화된 방식을 이용하여 제작된 디지털 파일 포 맷일 수 있다. 예를 들어, 미디어 콘텐트는 MP4, AVI, MKV, MOV, WMV 등의 미디어 컨테이너 포맷으로 제작된 것 일 수 있으나, 미디어 컨테이너 포맷은 이에 한정되는 것은 아니다. 미디어 콘텐트는 다양한 타입의 미디어 데이터를 포함할 수 있다. 예를 들어, 미디어 콘텐트는 비디오 데이터, 오디오 데이터, 텍스트 데이터(예를 들어, 자막)를 포함할 수 있다. 또한, 미디어 데이터는 미디어 콘텐트에 관 한 상세 정보를 나타내는 메타데이터를 포함할 수 있다. 메타데이터는 예를 들어, 타이틀, 제작자, 재생 시간 (duration), 비트율, 해상도, 비디오 코덱, 오디오 코덱, 챕터 정보, 커버 아트 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 서버는 사용자(콘텐트 뷰어)가 미디어 콘텐트를 탐색 및/또는 편집할 수 있도록 한다. 미디어 콘텐트는 사용자의 디스플레이 장치에서 재생된다. 단계 S220에서, 서버는 비디오 데이터를 분석하여 비디오와 관련된 제1 콘텍스트 데이터를 획득한다. 제1 콘텍스트 데이터는 비디오의 콘텍스트를 나타내는 것으로, 비디오 콘텍스트 데이터 지칭될 수도 있다. 서버는 다양한 방식으로 비디오 데이터를 분석할 수 있다. 서버는, 비디오 데이터를 구성하는 비디오 프레임들 중에서 분석 대상이 되는 비디오 프레임들을 선택하 는, 다운샘플링을 작업을 할 수 있다. 예를 들어, 60fps 비디오의 경우, 1초당 60개의 비디오 프레임들이 포함 될 수 있다. 이 경우, 서버는 1초당 1개의 비디오 프레임만을 추출하여 분석 대상 프레임으로 할 수 있다. 서버는 비디오 프레임 내에서 적어도 하나의 객체를 검출할 수 있다. 그리고, 서버는 비디오 프레 임 내에서 검출된 적어도 하나의 객체의 카테고리를 인식할 수 있다. 그리고, 서버는 인식된 객체들 간 관계를 검출할 수 있다. 서버는 객체 검출, 객체 인식, 객체 관계 검출을 위해, 하나 이상의 인공지능 모 델을 이용할 수 있다. 예를 들어, 서버는 인공지능 모델들인 객체 검출 모델, 객체 인식 모델, 객체 관계 검출 모델을 이용할 수 있다. 서버는 객체 검출, 객체 인식, 객체 관계 검출의 결과 데이터에 기초하여, 장면 그래프를 생성할 수 있다. 또한, 서버는 장면 그래프에 기초하여 비디오 콘텍스트를 생성할 수 있다. 서버가 비디오 데 이터를 분석하는 동작은, 도 4를 참조하여 더 기술한다. 단계 S230에서, 서버는 오디오 데이터를 분석하여 오디오와 관련된 제2 콘텍스트 데이터를 획득한다. 제2 콘텍스트 데이터는 오디오의 콘텍스트를 나타내는 것으로, 오디오 콘텍스트 데이터 지칭될 수도 있다. 서버는 다양한 방식으로 오디오 데이터를 분석하여 장면-사운드 정보를 획득할 수 있다. 장면-사운드 정 보란, 미디어 콘텐트 내에서 어떤 장면(비디오 프레임)에 대응되는 사운드에 관련된 정보들을 나타낸다. 서버는 자동 음성 인식(Automatic Speech Recognition; ASR) 또는 음성 인식을 이용하여 오디오 데이터 로부터 대화 등을 나타내는 텍스트 추출할 수 있다. 서버는 자동 음성 인식을 위해, 자연어 처리(Natural Language Processing; NLP) 모델을 이용할 수 있다. 자연어 처리 모델은 발화된 단어를 포함하는 오디오를 입력 받아, 오디오를 전사(transcribe)한 텍스트를 출력하는 인공지능 모델일 수 있다. 서버는 오디오 데이터에서 사운드 이벤트를 검출 및/또는 분류할 수 있다. 서버는 사운드 이벤트 분류를 위해, 인공지능 모델인 사운드 이벤트 분류 모델을 이용할 수 있다. 서버는 오디오 분석을 통해 획득된 장면-사운드 정보에 기초하여, 오디오 콘텍스트를 생성할 수 있다. 서 버가 오디오 데이터를 분석하는 동작은, 도 5를 참조하여 더 기술한다. 단계 S240에서, 서버는 제1 콘텍스트 데이터 및 제2 콘텍스트 데이터에 기초하여, 미디어 콘텐트의 비디 오 프레임들에 대응하는 장면 콘텍스트 데이터를 생성한다. 제1 콘텍스트 데이터는 비디오 콘텍스트 데이터로 지칭될 수 있고, 제2 콘텍스트 데이터는 오디오 콘텍스트 데이터라고 지칭될 수 있다. 서버는 미디어 콘텐트에 포함되는 비디오 프레임들에 대하여, 비디오 프레임들 각각에 대응하는 장면 콘 텍스트 데이터를 생성할 수 있다. 일 실시예에서, 장면 콘텍스트 데이터는 시각적 장면을 이해하고 해석하는데 사용될 수 있는 데이터 포맷으로 구성된 데이터를 말한다. 장면 콘텍스트 데이터는 장면 식별 번호, 장면 내 존재하는 객체의 카테고리, 객체의 위치, 객체 간 공간적 관계, 속성 및 객체 간의 상호 작용에 대한 정보 및 기타 장면을 나타내는 정보 등을 포 함할 수 있으나, 이에 한정되는 것은 아니다. 장면 콘텍스트 데이터의 구체적 예를 들면, 장면 내 객체는 \"사람\", 및 \"자동차\"일 수 있다. 각각의 객체의 위 치 정보(바운딩 박스)는 \"사람\"은 [x1, y1, x2, y2], \"자동차\"는 [x3, y3, x4, y4]일 수 있다. \"사람\" 및 \"자 동차\"의 공간적 관계는 \"옆에(next to)\"일 수 있다. 기타 장면을 나타내는 정보는 장면의 타입 \"실외\", 장면의 날씨가 \"맑음\"이고, 장면의 시간대는 \"밤\" 등일 수 있으나, 이에 한정되는 것은 아니다. 한편, 서버는 미디어 콘텐트에 텍스트 데이터가 포함되는 경우, 텍스트 콘텍스트 데이터를 획득할 수 있 다. 서버는 장면 콘텍스트 데이터를 생성할 때, 전술한 예시에 더하여 텍스트 콘텍스트 데이터를 더 이용 하여 장면 콘텍스트 데이터를 생성할 수 있다. 서버가 텍스트 데이터를 분석하는 동작은, 도 6을 참조하 여 더 기술한다. 단계 S250에서, 서버는 사용자 입력에 기초하여, 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정한다. 일 실시예에서, 사용자 입력은 자연어 발화 입력일 수 있다. 서버는 사용자의 발화 입력을 수신하면, 자 연어 처리(Natural Language Processing; NLP) 알고리즘을 이용하여 사용자 의도를 결정할 수 있다. 서버 는 사용자 발화에 대하여 자동 음성 인식을 수행하고, 자동 음성 인식 결과에 자연어 이해 알고리즘을 적 용하여, 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정할 수 있다. 미디어 콘텐트를 탐색하기 위한 사용자 의도는 예를 들어, \"특정 장면 검색\", \"앞으로 되돌리기\", \"뒤로 건너뛰기\" 등일 수 있으나, 이에 한정되는 것 은 아니다. 구체적인 예를 들면, 사용자 발화는 \"아까 폭발 장면 보여줘\"인 경우, 미디어 콘텐트를 탐색하기 위 한 사용자 의도는 \"폭발 장면 검색\"일 수 있다. 한편, 사용자 입력은 자연어 발화에 한정되는 것은 아니다. 예를 들어, 사용자 입력은 \"아까 폭발 장면 보여 줘\"와 같은 텍스트 입력일 수도 있다. 단계 S260에서, 서버는 장면 콘텍스트 데이터에 기초하여, 사용자 의도에 대응하는 적어도 하나의 비디오 프레임을 식별한다. 서버는 사용자 의도가 결정되면 사용자 의도에 대응되는 장면 콘텍스트 데이터를 검색할 수 있다. 단계 S250에서 전술한 예시를 계속 설명하면, 사용자가 폭발 장면을 보여달라 발화한 것에 기초하여, 사용자 의도가 \"폭발 장면 검색\"으로 결정되었을 수 있다. 서버는 장면 콘텍스트 데이터를 이용하여, 미디어 콘텐트 내 에서 사용자 의도와 대응되는 장면을 검색할 수 있다. 예를 들어, 미디어 콘텐트 내 포함되는 하나 이상의 폭발 장면 \"폭발 장면 A\", \"폭발 장면 B\", \"폭발 장면 C\" 등이 검색될 수 있다. 서버는 이 경우, \"폭발 장면 A\", \"폭발 장면 B\", \"폭발 장면 C\"에 대응되는 적어도 하나의 비디오 프레임을 식별할 수 있다.단계 S270에서, 서버는 식별된 적어도 하나의 비디오 프레임을 출력한다. 서버는 식별된 적어도 하나의 비디오 프레임을 미디어 콘텐트가 재생되는 디스플레이 장치로 송출할 수 있다. 이 경우, 서버는 식별된 적어도 하나의 비디오 프레임에 관련된 정보(예를 들어, 비디오 프레임의 타임 스탬프)를 제공할 수 있다. 이 경우, 디스플레이 장치에서는 식별된 하나 이상의 비디오 프레임이 표시될 수 있다. 예를 들어, \"폭발 장면 A\", \"폭발 장면 B\", \"폭발 장면 C\"에 각각 대응되는 프레임이 표시될 수 있다. 디스플레이 장치는 사용자 입력에 기초하여 미디어 콘텐트의 탐색을 수행할 수 있다. 예를 들어, 디스플레이 장 치는 사용자가 디스플레이 장치에 표시된 \"폭발 장면 A\"를 나타내는 비디오 프레임을 선택하는 것에 기초하여, 비디오 타임라인 내에서 \"폭발 장면 A\"의 시간대로 이동한 후, \"폭발 장면 A\"부터 비디오가 재생되도록 할 수 있다. 도 3은 본 개시의 일 실시예에 따른 서버가 미디어 콘텐트로부터 장면 콘텍스트 데이터를 획득하는 것을 개략적 으로 도시한 도면이다. 일 실시예에서, 서버는 장면 분석 모듈을 이용하여 미디어 콘텐트를 분석할 수 있다. 장면 분 석 모듈은 비디오 분석 모듈, 오디오 분석 모듈 및 텍스트 분석 모듈을 포함할 수 있다. 비디오 분석 모듈은 비디오 데이터를 분석하여 비디오 콘텍스트 데이터를 획득할 수 있다. 서버 는 비디오 분석 모듈을 이용하여, 비디오 프레임들 중 적어도 일부에 객체 검출 및/또는 객체 인식 을 적용하고, 장면 정보를 획득할 수 있다. 서버는 장면 정보에 기초하여 적어도 하나의 비디오 프레임에 대응하는 장면 그래프를 생성할 수 있다. 예를 들어, 서버는 \"장면 A\"에 대응하는 \"장면 그래프 A\"를 생 성하고, \"장면 B\"에 대응하는 \"장면 그래프 B\"를 생성할 수 있다. 서버는 장면 그래프에 기초하여 비디오 의 콘텍스트를 나타내는, 비디오 콘텍스트 데이터를 획득할 수 있다. 일부 실시예에서, 서버는 장면 그래프를 비디오 콘텍스트 데이터로 획득할 수 있다. 오디오 분석 모듈은 오디오 데이터를 분석하여 오디오 콘텍스트 데이터를 획득할 수 있다. 서버 는 오디오 분석 모듈을 이용하여, 오디오 데이터에 음성 인식, 사운드 이벤트 검출, 사운드 이벤트 분류 중 적어도 하나를 적용하고, 장면-사운드 정보를 획득할 수 있다. 장면-사운드 정보는 장면에 대응하는 사 운드로부터 획득되는, 오디오 컨텍스트와 관련된 정보를 말한다. 서버는 장면-사운드 정보에 기초하여 오 디오의 콘텍스트를 나타내는, 오디오 콘텍스트 데이터를 획득할 수 있다. 텍스트 분석 모듈은 텍스트 데이터를 분석하여 텍스트 콘텍스트 데이터를 획득할 수 있다. 서버 는 텍스트 문석 모듈을 이용하여, 텍스트 데이터에 자연어 처리 알고리즘을 적용하고, 장면-텍스트 정보를 획득할 수 있다. 장면-텍스트 정보는 장면에 대응하는 텍스트로부터 획득되는, 텍스트 컨텍스트와 관련 된 정보를 말한다. 서버는 장면-텍스트 정보에 기초하여 텍스트의 콘텍스트를 나타내는, 텍스트 콘텍스트 데이터를 획득할 수 있다. 장면 분석 모듈은 비디오 콘텍스트 데이터, 오디오 콘텍스트 데이터 및 텍스트 콘텍스트 데이터 중 적어도 하나에 기초하여, 장면 콘텍스트 데이터를 획득할 수 있다. 장면 콘텍스트 데이터는 하나 이상의 비디오 프레임들에 대응할 수 있다. 예를 들어, \"장면 A\"는 하나 이상의 비디오 프레임들로 구성될 수 있다. 또한, \"장면 A\"에 대응하는 하나 이상의 비디오 프레임들 대하여, 비디오 콘텍스트 데이터, 오디오 콘텍스트 데이터 및 텍스트 콘텍스트 데이터가 획득되어 있을 수 있다. 이 경우, 서버는 \"장면 A\"에 대응하는 장면 콘텍스트로 \"장면 콘텍스트 A\"를 생성할 수 있다. 장면 분석 모듈의 비디오 분석 모듈, 오디오 분석 모듈 및 텍스트 분석 모듈 각각의 구체 적인 동작을 도 4 내지 도 6을 참조하여 더 기술한다. 도 4는 본 개시의 일 실시예에 따른 서버가 비디오 데이터를 분석하는 동작을 설명하기 위한 도면이다. 도 4를 참조하면, 서버는 비디오 분석 모듈을 이용하여 비디오 분석을 수행할 수 있다. 비디오 분석 모듈은 비디오 프레임으로부터 장면 정보를 추출할 수 있다. 비디오 분석 모듈은, 비디오 분석을 위한 다양한 알고리즘을 이용하도록 구성될 수 있다. 또한, 비디오 분석 모듈은, 하나 이상의 인공 지능 모델들을 포함할 수 있다. 일 실시예에서, 서버는 비디오 분석 모듈을 이용하여 비디오 프레임 내에서 적어도 하나의 객 체를 검출할 수 있다. 서버는 객체 검출을 위해, 인공지능 모델인 객체 검출 모델을 이용할 수 있다. 객 체 검출 모델은 이미지를 입력 받아 검출된 객체들을 나타내는 정보를 출력하는 심층 신경망 모델일 수 있다.예를 들어, 객체 검출 모델은 이미지를 입력 받아 검출된 객체들을 나타내는 바운딩 박스를 출력할 수 있다. 객 체 검출 모델은 알려진 다양한 심층 신경망 아키텍처 및 알고리즘을 이용하거나, 알려진 다양한 심층 신경망 아 키텍처 및 알고리즘의 변형을 통해 구현될 수 있다. 객체 검출 모델은 예를 들어, Convolutional neural networks (CNNs)을 기반으로 하는 Faster R-CNN, Mask R-CNN, You Only Look Once(YOLO), Single Shot Detector(SSD) 등으로 구현될 수 있으나, 이에 한정되는 것은 아니다. 일 실시예에서, 서버는 비디오 분석 모듈을 이용하여 비디오 프레임 내에서 검출된 적어도 하 나의 객체의 카테고리를 인식할 수 있다. 서버는 객체 인식을 위해, 인공지능 모델인 객체 인식 모델을 이용할 수 있다. 객체 인식 모델은 이미지를 입력 받아 객체 클래스 레이블(들)을 나타내는 정보를 출력하는 심 층 신경망 모델일 수 있다. 예를 들어, 객체 인식 모델은, 객체를 잘라낸 이미지를 입력 받아 하나 이상의 객체 클래스 레이블(예를 들어, \"자동차\", \"사람\" 등) 및 신뢰도 스코어를 출력할 수 있다. 객체 인식 모델은 알려진 다양한 심층 신경망 아키텍처 및 알고리즘을 이용하거나, 알려진 다양한 심층 신경망 아키텍처 및 알고리즘의 변형을 통해 구현될 수 있다. 객체 인식 모델은 예를 들어, Convolutional neural networks (CNNs)을 기반으로 하는 ResNet, Inception Networks, VGG Networks, DenseNet 등으로 구현될 수 있으나, 이에 한정되는 것은 아 니다. 일 실시예에서, 서버는 비디오 분석 모듈을 이용하여 인식된 객체들 간 관계를 검출할 수 있다. 서 버는 객체들 간 관계 검출을 위해, 인공지능 모델인 객체 관계 검출 모델을 이용할 수 있다. 객체 관계 검출 모델은 검출된 객체들에 관한 정보를 입력 받아 객체들 간 관계를 나타내는 정보를 출력하는 심층 신경망 모델일 수 있다. 예를 들어, 객체 관계 검출 모델은 검출된 객체 \"지붕\" 및 \"사람\"에 관한 정보를 입력 받아, 사람이 지붕 위에 있음을 나타내는 두 객체 간 관계 \"위에(on top of)\"를 출력하는 모델일 수 있다. 객체 관계 검출 모델은 알려진 다양한 심층 신경망 아키텍처 및 알고리즘을 이용하거나, 알려진 다양한 심층 신경망 아키 텍처 및 알고리즘의 변형을 통해 구현될 수 있다. 객체 관계 검출 모델은 예를 들어, Graph Neural Networks (GNNs)을 기반으로 하는 Graph R-CNN, Neural Motifs 등으로 구현될 수 있으나, 이에 한정되는 것은 아니다. 서버는 전술한 예시들을 통해 획득된 데이터를 이용하여 장면 정보를 생성할 수 있다. 예를 들어, 서버는 비디오 프레임에 대한 캡션을 생성할 수 있다. 예를 들어, 서버는 비디오 프레임 에 대한 장면 그래프를 생성할 수 있다. 장면 그래프는 하나 이상의 노드들 및 하나 이상의 에 지들을 포함할 수 있다. 여기서, 장면 그래프의 하나 이상의 노드들은 하나 이상의 객체들을 나타내고, 하 나 이상의 에지들은 하나 이상의 객체들 간 관계들을 나타낸다. 한편, 서버가 비디오 분석을 통해 획득한 장면 정보는, 전술한 예시들에 한정되는 것은 아니다. 서버는 비디오 분석을 통해 추출 가능한, 장 면과 관련된 다양한 정보를 추출할 수 있다. 서버는 장면 정보에 기초하여 비디오 콘텍스트 데이터를 생성할 수 있다. 예를 들어, 서버는 장면 정보 내 요소들인 장면 캡션, 장면 그래프 중 적어도 하나를 가공하거나, 데이터 요소들을 선별하고 패키징하여, 비디오의 콘텍스트를 나타내는 비디오 콘텍스트 데이터를 생성할 수 있다. 도 5는 본 개시의 일 실시예에 따른 서버가 오디오 데이터를 분석하는 동작을 설명하기 위한 도면이다. 도 5를 참조하면, 서버는 오디오 분석 모듈을 이용하여 오디오와 관련된 다양한 특징들(예를 들어, 사운드의 크기, 음의 높낮이, 박자, 지속 시간 등)을 추출하고 오디오 분석을 수행할 수 있다. 오디오 분석 모 듈은 비디오 프레임에 대응하는 오디오로부터 장면-오디오 정보를 추출할 수 있다. 비디오 프레 임에 대응하는 오디오는 예를 들어, 대화, 사운드 이벤트 등을 포함할 수 있으나, 이에 한정되 는 것은 아니다. 오디오 분석 모듈은, 오디오 분석을 위한 다양한 알고리즘을 이용하도록 구성될 수 있다. 또한, 오디오 분석 모듈은, 하나 이상의 인공지능 모델들을 포함할 수 있다. 일 실시예에서, 서버는 오디오 분석 모듈을 이용하여 대화를 인식할 수 있다. 서버는 대화 인 식을 위해 자연어 처리 모델을 이용할 수 있다. 자연어 처리 모델은 인간의 언어를 처리 및 분석하는 알고리즘 을 말한다. 자연어 처리는, 자동 음성 인식을 포함할 수 있다. 자동 음성 인식은 발화된 언어를 쓰여진 텍스트 로 전사하는 것을 말한다. 자동 음성 인식 모델은 예를 들어, Hidden Markov Models (HMMs), Deep Neural Networks (DNNs), Recurrent Neural Networks (RNNs), Transformer-based Models 등으로 구현될 수 있으나, 이 에 한정되는 것은 아니다. 서버는 자연어 처리 모델을 이용하여, 발화된 언어를 텍스트로 전사하고, 텍스"}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "트를 처리할 수 있다. 예를 들어, 서버는 텍스트 분류, 번역, 요약 등과 같은 출력을 생성할 수 있다. 일 실시예에서, 서버는 오디오 분석 모듈을 이용하여 오디오 데이터에서 사운드 이벤트를 검출 및/ 또는 분류할 수 있다. 예를 들어, 서버는 오디오 데이터의 스펙트로그램을 획득할 수 있다. 서버는 사운드 이벤트 분류를 위해, 사운드 이벤트 분류 모델을 이용할 수 있다. 사운드 이벤트 분류 모델은, 스펙트로 그램을 입력 받아 사운드 이벤트의 클래스 레이블(들)을 출력하는 심층 신경망 모델일 수 있다. 이용하여 음성, 음악 또는 소음(예를 들어, \"개 짖음\", \"자동차 경적\")과 같은 특정 사운드 이벤트를 식별할 수 있다. 사운드 이벤트 분류 모델은 알려진 다양한 심층 신경망 아키텍처 및 알고리즘을 이용하거나, 알려진 다양한 심층 신경 망 아키텍처 및 알고리즘의 변형을 통해 구현될 수 있다. 사운드 이벤트 검출 모델은 예를 들어, Convolutional Neural Networks (CNNs), Convolutional-Recurrent Neural Networks (CRNNs), 및 Attention-based models 등 으로 구현될 수 있으나, 이에 한정되는 것은 아니다. 서버는 전술한 예시들을 통해 획득된 데이터를 이용하여, 장면-오디오 정보를 생성할 수 있다. 장면 -오디오 정보는, 특정 장면에 대응하는 오디오로부터 추출/생성되는 정보를 말한다. 예를 들어, 서버 는 적어도 하나의 비디오 프레임에 대응되는, 대화 음성 인식 결과를 생성할 수 있다. 예를 들어, 서버는 적어도 하나의 비디오 프레임에 대응되는, 오디오 스펙트로그램을 생성할 수 있다. 예를 들 어, 서버는 적어도 하나의 비디오 프레임에 대응되는, 사운드 이벤트 검출 결과를 생성할 수 있다. 한편, 서버가 오디오 분석을 통해 획득한 장면-오디오 정보는, 전술한 예시들에 한정되는 것은 아니 다. 서버는 오디오 분석을 통해 추출 가능한, 장면의 오디오와 관련된 다양한 정보를 추출할 수 있다. 예를 들어, 서버는 오디오 데이터를 복수의 오디오 소스들로 분리할 수 있다. 서버는 오디오 데이 터를 \"대사(speech)\", \"음악(music)\", \"효과음(sound effects)\" 등의 오디오 소스들로 분리하고, 각각의 오디 오 소스들을 분석할 수 있다. 예를 들어, 서버는 대사를 분석하여 인물의 발화 내용 및 감정을 포함하는 장면-오디오 정보를 생성 할 수 있다. 예를 들어, 서버는 음악을 분석하여 음악의 분위기, 악기 정보 등을 식별하거나, 음악의 정 보(제목, 아티스트)를 나타내는 장면-오디오 정보를 생성할 수 있다. 예를 들어, 서버는 효과음을 분석하여 장면의 분위기(예를 들어, 긴장감, 안도감, 환희 등)를 나타내는 장면-오디오 정보를 생성할 수 있다. 서버는 장면-오디오 정보에 기초하여 오디오 콘텍스트 데이터를 생성할 수 있다. 예를 들어, 서버 는 장면-오디오 정보 내 요소들인 대화 음성 인식 결과, 오디오 스펙트로그램, 사운드 이 벤트 검출 결과 중 적어도 하나를 가공하거나, 데이터 요소들을 선별하고 패키징하여 오디오의 콘텍스트를 나타내는 오디오 콘텍스트 데이터를 생성할 수 있다. 도 6은 본 개시의 일 실시예에 따른 서버가 텍스트 데이터를 분석하는 동작을 설명하기 위한 도면이다. 도 6을 참조하면, 서버는 텍스트 분석 모듈을 이용하여 텍스트 분석을 수행할 수 있다. 텍스트 분석 모듈은 비디오 프레임에 대응하는 텍스트로부터 장면-텍스트 정보를 추출할 수 있다. 장면-텍스 트 정보는 특정 장면에 대응하는 텍스트로부터 추출/생성되는 정보를 말한다. 그리고, 비디오 프레임(61 0)에 대응하는 텍스트는 예를 들어, 자막(subtitle 또는 caption), 미디어 콘텐트의 메타데이터(예를 들어, 출연자 이름, 챕터 이름 등) 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 텍스트 분석 모듈(60 0)은, 텍스트 분석을 위한 다양한 알고리즘을 이용하도록 구성될 수 있다. 또한, 텍스트 분석 모듈은, 하 나 이상의 인공지능 모델들을 포함할 수 있다. 일 실시예에서, 자막은 미디어 콘텐트 내 인물 간 대화를 나타내는 텍스트를 포함할 수 있다. 또한 일부 실시예에서, 자막은 청각이 불편한 시청자를 위해, 미디어 콘텐트의 상황, 특징 등을 묘사하는 텍스트를 포함할 수도 있다. 예를 들어, 자막에는 배경음, 상황 또는 효과음을 나타내는 텍스트 \"잔잔한 음악이 흐른다\", \"자동차 경적 소리\", \"웃음 소리\" 등의 텍스트가 포함될 수 있다. 또한, 미디어 콘텐트의 메타데이터에는 예를 들어, 출연자 이름, 챕터 이름 등을 나타내는 텍스트가 포함될 수 있다. 서버는 비디오 프레임 , 자막, 메타데이터 등에 포함되는 텍스트에 대하여, 인공지능 모델(예를 들어, 자연어 처리 모"}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "델, 텍스트 검출 모델)을 이용하여, 텍스트 분류, 번역, 요약, 검출 등을 수행하고 장면-텍스트 정보를 생 성할 수 있다. 텍스트 처리를 위한 인공지능 모델은 알려진 다양한 신경망 아키텍처의 채택 또는 변형을 통해 구현될 수 있으므로, 구체적인 설명은 생략한다. 장면-텍스트 정보에는 예를 들어, 해당 장면에서의 인물 간 대화 텍스트가 포함될 수 있다. 그리고, 장면-텍스트 정보에는 상황 묘사 텍스트(예를 들어, \"주인공 두 명이 다툰다\")가 포함될 수 있다. 그 리고, 장면-텍스트 정보에는 사운드 효과 묘사 텍스트(예를 들어, \"웃음 소리\", \"폭발음\" 등)가 포함될 수있다. 그리고, 장면-텍스트 정보에는, 검출된 텍스트가 포함될 수 있다. 검출된 텍스트는 비디 오 프레임 내에 텍스트가 포함되는 경우 텍스트 검출 모델을 이용하여 획득될 수 있다. 한편, 서버가 텍스트 분석을 통해 획득한 장면-텍스트 정보는, 전술한 예시들에 한정되는 것은 아니 다. 서버는 텍스트 분석을 통해 추출 가능한, 장면의 텍스트와 관련된 다양한 정보를 추출할 수 있다. 서버는 장면-텍스트 정보에 기초하여 텍스트 콘텍스트 데이터를 생성할 수 있다. 예를 들어, 서버 는 장면-텍스트 정보 내 요소들인 대화 텍스트, 상황 묘사 텍스트, 사운드 효과 묘사 텍 스트, 검출된 텍스트 중 적어도 하나를 가공하거나, 데이터 요소들을 선별하고 패키징하여 텍스트의 콘텍스트를 나타내는 텍스트 콘텍스트 데이터를 생성할 수 있다. 도 7은 본 개시의 일 실시예에 따른 서버가 생성한 장면 콘텍스트 데이터를 설명하기 위한 도면이다. 일 실시예에서, 서버는 비디오 콘텍스트 데이터, 오디오 콘텍스트 데이터 및 텍스트 콘텍스트 데이터 중 적어도 하나에 기초하여, 장면 콘텍스트 데이터를 획득할 수 있다. 그 결과로, 미디어 콘텐트에 포함되는 비디 오 프레임들은 비디오 프레임들에 대응하는 장면 콘텍스트 데이터를 갖는다. 도 7을 참조하면, 32번째 비디오 프레임은 자동차 경주 시작 장면일 수 있다. 이 경우, 서버가 전술 한 실시예에 따라 장면 콘텍스트 데이터를 생성하면, 자동차 경주 시작 장면의 장면 콘텍스트 데이터가 획 득될 수 있다. 자동차 경주 시작 장면의 장면 콘텍스트 데이터는, 32번째 비디오 프레임에 대응한다. 이 경우, \"자동차 대수\", \"자동차 위치\", \"어두운 배경\", \"밤 시간대\", \"자동차 경주 시작\" 등, 32번째 비디오 프레임의 장면 관련된 콘텍스트를 나타내는 데이터들이 자동차 경주 시작 장면의 장면 콘텍스트 데이터 에 포함될 수 있다. 이와 마찬가지로, 또다른 장면에는 또다른 장면에 대응하는 장면 콘텍스트 데이터가 생성될 수 있다. 예를 들어, 1200번째 비디오 프레임 내지 1260번째 비디오 프레임은 전투 액션 장면 중 일부일 수 있다. 이 경우, 서 버가 전술한 실시예에 따라 장면 콘텍스트 데이터를 생성하면, 전투 액션 장면의 장면 콘텍스트 데이터 가 획득될 수 있다. 일 실시예에서, 장면 전환이 되지 않은 경우, 동일한 장면으로 분류된 비디오 프레임 들에는 동일한 장면 콘텍스트 데이터가 대응될 수 있다. 예를 들어, 전투 액션 장면 중 일부인 것으로 분류된 1200번째 비디오 프레임 내지 1260번째 비디오 프레임에는 모두, 전투 액션 장면의 장면 콘텍스트 데이터 가 대응될 수 있다. 일 실시예에서, 서버는 미디어 콘텐트 내 장면을 탐색하기 위한 사용자 입력을 수신하고, 장면 콘텍스트 데이터를 이용하여 사용자가 탐색하고자 하는 장면을 검색할 수 있다. 예를 들어, 서버는 미디어 콘텐트가 스트리밍 되는 도중에, 미디어 콘텐트 내 장면을 탐색하기 위한 사용 자 입력을 수신할 수 있다. 서버는 사용자 입력이 수신되면, 미디어 콘텐트를 분석하여 장면 콘텍스트를 획득하고, 사용자 입력에 대응되는 장면 콘텍스트를 검색할 수 있다. 미디어 콘텐트의 분석은, 전술한 비디오 분석, 오디오 분석, 텍스트 분석이 포함될 수 있다. 이에 대한 구체적인 설명을 도 8을 참조하여 더 기술한다. 예를 들어, 서버는 미리 저장된(예를 들어, 다운로드 된 미디어 콘텐트)미디어 콘텐트를 획득하고, 미디 어 콘텐트를 분석하여 장면 콘텍스트를 미리 획득할 수 있다. 서버는 미디어 콘텐트가 재생되는 동안 사 용자 입력이 수신되면, 사용자 입력에 대응되는 장면 콘텍스트를 검색할 수 있다. 이에 대한 구체적인 설명을 도 9를 참조하여 더 기술한다. 도 8은 본 개시의 일 실시예에 따른 서버가 사용자 입력 및 장면 콘텍스트에 기초하여 미디어 콘텐트를 탐색하 는 동작을 설명하기 위한 흐름도이다. 단계 S810에서, 서버는 사용자 발화를 인식한다. 서버는 사용자 입력을 수신할 수 있다. 서버는 사용자 입력이 발화인 것에 기초하여, 자동 음성 인 식을 수행하여 발화된 언어를 쓰여진 텍스트로 변화할 수 있다. 단계 S820에서, 서버는 사용자 의도를 결정한다. 서버는 자동 음성 인식 결과인 텍스트에 자연어 이해 알고리즘을 적용하여 사용자 의도를 결정할 수 있다. 이 경우, 자연어 이해 모델이 이용될 수 있다. 자연어 이해 모델은 예를 들어, 텍스트를 문장, 구절과 같 은 개별 단위로 분리하는 \"토큰화\", 명사, 동사, 형용사와 같은 품사를 식별하고 태그를 지정하는 \"품사 태깅\", 이름, 날짜, 위치 등 사전에 명명된 개체(entity)를 식별하고 분류하는 \"개체 인식\", 문장에서 단어 간 문법적관계를 식별하는 \"종속성 구문 분석\", 문장에서 긍정, 부정, 중립 등으로 분류되는 텍스트의 감정적인 어조를 결정하는 \"감정 분석\", 텍스트의 의도를 식별하는 \"의도 인식\"등의 프로세스를 포함할 수 있으나, 이에 한정되 는 것은 아니다. 예를 들어, 서버는 사용자의 발화 \"방금 전 자동차 경주 시작부터 다시 볼래\"에 기초하여, 사용자 의도가 자동차 경주 장면으로 탐색하는 것임을 결정할 수 있다. 단계 S830에서, 서버는 장면 콘텍스트를 추출한다. 서버는 미디어 콘텐트가 스트리밍되는 동안 사용자로부터의 발화가 인식되고 사용자의 미디어 콘텐트 탐 색 의도가 추출되면, 실시간으로 현재 재생 중인 미디어 콘텐트에 대한 장면 콘텍스트 추출 작업을 수행할 수 있다. 예를 들어, 서버는, 자동차 경주 장면의 장면 후보들을 식별할 수 있다. 일 실시예에서, 서버가 실시간으로 장면 콘텍스트 추출 작업을 수행하는 경우, 서버는 기 설정된 타임 구간만큼 미디어 콘텐트를 분석할 수 있다. 예를 들어, 서버는 현재 시점으로부터 전후 30초 시간대 의 미디어 콘텐트를 분석할 수 있다. 서버는 비디오 분석 모듈을 이용하여, 비디오 분석을 수행하고 비디오 콘텍스트 데이터를 획득할 수 있다. 서버는 비디오 콘텍스트 데이터에 기초하여, 사용자 의도에 대응하는 하나 이상의 장면들을 식별한 결과인 비디오 분석 결과 후보 장면군을 결정할 수 있다. 예를 들어, 비디오 분석 결과 후보 장면군 에는, \"비디오 프레임 A\", \"비디오 프레임 B\", 및 \"비디오 프레임 C\"가 포함될 수 있다. 서버는 오디오 분석 모듈을 이용하여, 오디오 분석을 수행하고 오디오 콘텍스트 데이터를 획득할 수 있다. 서버는 오디오 콘텍스트 데이터에 기초하여, 사용자 의도에 대응하는 하나 이상의 장면들을 식별한 결과인 오디오 분석 결과 후보 장면군을 결정할 수 있다. 예를 들어, 오디오 분석 결과 후보 장면군, \"비디오 프레임 B\", \"비디오 프레임 D\", 및 \"비디오 프레임 F\"가 포함될 수 있다. 마찬가지로, 서버는 텍스트 분석 모듈을 이용하여, 텍스트 콘텍스트 데이터를 획득하고, 사용자 의 도에 대응하는 하나 이상의 장면들을 식별한 결과인 텍스트 분석 결과 후보 장면군을 결정할 수 있다. 예 를 들어, 텍스트 분석 결과 후보 장면군, \"비디오 프레임 B\", \"비디오 프레임 C\", 및 \"비디오 프레임 E\"가 포함될 수 있다. 서버는 비디오 분석, 오디오 분석, 텍스트 분석 결과를 종합하여 종합 장면 후보를 결정할 수 있다. 전술한 예시에서, 비디오 분석, 오디오 분석, 텍스트 분석에서 공통되는 비디오 프레임은 \"비디오 프레임 B\"이 다. 따라서, 자동차 경주 장면일 가능성이 높은 \"비디오 프레임 B\"가 종합 장면 후보로 결정될 수 있다. 단계 S840에서, 서버는 사용자 의도에 기초하여 장면을 검색한다. 서버는 식별된 종합 장면 후보에 기초하여, 사용자가 장면을 탐색할 수 있도록 장면과 관련된 정보 를 사용자의 디스플레이 장치로 전송할 수 있다. 예를 들어, 서버는 자동차 경주 장면을 검색하고, 자동 차 경주 장면 시작 부분을 나타내는 \"비디오 프레임 B\"가 사용자의 디스플레이 장치에서 표시되도록 할 수 있다. 이 경우, 사용자는 디스플레이 장치에서 표시되는 \"비디오 프레임 B\"를 선택하여, 자동차 경주 장면을 다 시 시청할 수 있다. 서버는 미디어 콘텐트 분석 과정에서 획득되는 장면 콘텍스트를 데이터베이스에 저장할 수 있다. 일 실시예에서, 서버는 사용자 발화로부터 획득된 사용자 의도에 기초하여, 미디어 콘텐트의 분석 범위를 결정할 수 있다. 예를 들어, 사용자 발화가 \"방금 전 자동차 경주 시작부터 다시 볼래\"인 경우, 사용자 의도는 자동차 경주 장면 탐색이지만, 사용자 발화에 방금 전 이라는 단어가 포함되어 있으므로, 서버는 현재 재 생 중인 비디오 프레임으로부터 이전의 타임라인에 포함되는 장면들에 대해서만 장면 콘텍스트를 획득할 수 있 다. 예를 들어, 사용자 발화가 \"이 장면은 좀 지루하네\"인 경우, 서버는 사용자 의도는 현재 장면의 건너 뛰기라고 식별할 수 있다. 따라서, 서버는 현재 재생 중인 비디오 프레임으로부터 이후의 타임라인에 포 함되는 장면들에 대해서만 장면 콘텍스트를 획득하여, 사용자가 현재 장면을 건너뛰도록 하게 할 수 있다. 서버 는 사용자 의도를 먼저 식별하고, 사용자 의도에 기초하여 미디어 콘텐트의 분석을 수행하고, 사용자 의 도에 대응하는 장면 컨텍스트가 추출되도록 함으로써, 일괄적으로 장면 콘텍스트만을 먼저 추출하는 것보다 연 산 효율을 높일 수 있다. 한편, 도 8에서는 사용자 발화가 입력되고 인식되면 미디어 분석 및 장면 콘텍스트 추출 작업이 시작되는 것을 예시로 하였으나, 이는 설명의 편의를 위한 것이다. 예를 들어, 미디어 콘텐트의 스트리밍이 시작되면, 서버 는 스트리밍 중인 미디어 콘텐트를 실시간으로 분석하여 장면 콘텍스트를 획득하고, 데이터베이스에 저장할 수도 있다. 도 9는 본 개시의 일 실시예에 따른 서버가 사용자 입력 및 장면 콘텍스트에 기초하여 미디어 콘텐트를 탐색하 는 동작을 설명하기 위한 흐름도이다. 단계 S910에서, 서버는 장면 콘텍스트를 추출한다. 미리 저장된(예를 들어, 다운로드 된 미디어 콘텐트) 미디어 콘텐트를 획득하고, 미디어 콘텐트를 분석하여 장면 콘텍스트를 미리 획득할 수 있다. 획득된 장면 콘텍 스트는 데이터베이스에 저장될 수 있다. 단계 S920에서, 서버는 사용자 발화를 인식하고, 단계 S930에서, 서버는 사용자 의도를 결정한다. 단계 S920 내지 S930은 도 8의 단계 S810 내지 S820에 대응될 수 있으므로, 간결함을 위해 반복되는 설명은 생 략한다. 단계 S940에서, 서버는 사용자 의도에 기초하여 장면을 검색한다. 서버는 데이터베이스에 저장된 장면 콘텍스트를 이용할 수 있다. 예를 들어, 사용자가 \"이 장면은 좀 지루하네\"라고 발화한 경우, 서버는 현재 장면을 건너뛰고자 하는 사용자 의도를 식별하고, 장면 컨텍 스트에 기초하여 지루하지 않은 장면(예를 들어, 액션 장면)을 검색할 수 있다. 도 10은 본 개시의 일 실시예에 따른, 사용자가 장면을 탐색하는 것을 도시한 도면이다. 도 10을 참조하면, 사용자는 디스플레이 장치를 통해 미디어 콘텐트를 시청할 수 있다. 디스플레이 장치의 제1 화면은, 자동차들이 경주 중인 액션 장면을 도시한 것이다. 또한, 디스플레이 장치의 제2 화면 자 동차들의 경주가 시작되는 장면을 도시한 것이다. 일 실시예에서, 서버는 사용자 입력을 획득할 수 있다. 예를 들어, 사용자 입력은 발화된 언어 \"방 금 전 자동차 경주 시작부터 다시 볼래\"일 수 있다. 서버는 자연어 처리를 통해 사용자의 의도를 식별하 고, 식별된 사용자 의도에 대응되는 장면을 검색할 수 있다. 예를 들어, 서버는 \"방금 전\"이라는 단어에 기초하여 현재보다 이전의 타임 구간의 장면을 찾고자 하는 것임을 식별할 수 있으며, \"자동차 경주\" 및 \"경주 시작\"에 기초하여 사용자가 자동차 경주가 시작되려는 장면을 찾고자 하는 것임을 식별할 수 있다. 서버 는 미디어 콘텐트 분석을 통해 획득된 장면 콘텍스트 데이터에 기초하여, 사용자 의도에 대응하는 적어도 하나 의 비디오 프레임을 식별할 수 있다. 예를 들어, 서버는 \"비디오 프레임 32\"의 장면 콘텍스트 데이터 가 사용자 의도에 대응하는 장면임을 식별하고, \"비디오 프레임 34\"를 검색 결과로 출력할 수 있다. 일 실시예에 따른 서버는 비디오 분석, 오디오 분석 및 텍스트 분석을 포함하는 미디어 콘텐트 분석을 통 해, 미디어 콘텐트의 장면들에 대한 세부적인 장면 콘텍스트들을 획득할 수 있다. 또한 서버는, 자연어 처리를 통해 사용자의 미디어 콘텐트 탐색을 위한 의도를 정밀하게 식별할 수 있다. 예를 들어, 사용자가 \"주인공이 왜 화가 난 거야?\" 라고 발화한 경우, 서버는 사용자 의도를 식별하고, 주인공이 화가 나게 된 계기인, 주인공이 다른 사람과 말다툼하는 장면을 검색할 수 있다. 예를 들어, 사용자가 \"저 사람 누구더라?\" 라고 발화한 경우, 서버는 사용자 의도를 식별하고, 화면에 있 는 사람이 처음으로 등장하는 장면을 검색할 수 있다. 예를 들어, 사용자가 \"이 부분은 너무 지루해!\"라고 발화한 경우, 서버는 사용자 의도를 식별하고, 사용 자가 지루한 부분이라고 응답한 장면의 다음 장면을 검색할 수 있다. 예를 들어, 사용자가 \"아까 검은색 밴이 다리 위에서 폭발하던 장면으로 돌아가줘\"라고 발화한 경우, 서버 는 사용자 의도를 식별하고, 검은색 밴이 다리 위에서 폭발했던 장면을 검색할 수 있다. 도 11은 본 개시의 일 실시예에 따른, 사용자가 장면을 탐색하는 것을 도시한 도면이다. 도 11을 참조하면, 사용자는 디스플레이 장치를 통해 미디어 콘텐트를 시청할 수 있다. 디스플레이 장치의 제1 화면은, 미디어 콘텐트의 재생이 끝나고 엔딩 크레딧이 표시되는 것을 도시한 것이다. 또한, 디스플레이 장치의 제2 화면 사용자 입력에 기초하여 디스플레이 장치가 장면 담색을 처리 중임을 도시한 것이다. 또 한, 디스플레이 장치의 제3 화면은, 사용자 의도에 대응하는 적어도 하나의 비디오 프레임을 식별한 결과 를 도시한 것이다.일 실시예에서, 사용자 입력은 \"주인공 두 명이 싸우던 그 장면 다시 보고 싶어\"의 자연어 입력일 수 있 다. 서버는 자연어 처리를 통해 사용자 의도를 식별하고, 사용자 의도에 대응되는 장면을 검색할 수 있다. 예를 들어, 서버는 주인공 A와 주인공 B가 다투는 장면, 주인공 C와 조연 D가 다투는 장면, 조연 E와 조연 F가 다투는 장면 등을 검색할 수 있다. 서버는 장면 검색 결과를 디스플레이 장치로 제공할 수 있다. 디스플레이 장치는 서버로부터 수신되는 정보에 기초하여, 사용자 의도에 대응되는 검색 결과 를 표시할 수 있다. 일 실시예에서, 서버는 미디어 콘텐트가 재생되는 동안 미디어 콘텐트 분석을 병렬적으로 수행하고, 장면 콘텍스트 데이터를 획득하여 저장할 수 있다. 이 경우, 미디어 콘텐트가 종료된 후에 사용자 입력이 입력 되면, 서버는 저장된 장면 콘텍스트 데이터에 기초하여 사용자 의도에 대응하는 비디오 프레임을 식별할 수 있다. 일 실시예에서, 서버는 사용자 입력이 수신되는 것에 기초하여 미디어 콘텐트 분석을 시작할 수 있다. 서 버는 미디어 콘텐트 분석을 통해 장면 콘텍스트 데이터를 획득하고, 획득된 장면 콘텍스트 데이터에 기초 하여 사용자 의도에 대응하는 비디오 프레임을 식별할 수 있다. 도 12는 본 개시의 일 실시예에 따른, 사용자가 미디어 콘텐트를 편집하는 동작을 개략적으로 도시한 도면이다. 일 실시예에서, 서버는 사용자가 원하는 장면들을 검색하여 미디어 콘텐트를 편집할 수 있도록 할 수 있 다. 도 12를 내지 도 14를 설명함에 있어서, 사용자가 미디어 콘텐트를 편집하는 예시로, 아동을 위해 유해 장 면들을 편집하는 실시예를 설명한다. 다만, 이는 설명의 편의를 위한 예시일 뿐, 미디어 콘텐트의 편집이 아동 을 위해 유해 장면을 편집하는 것에 한정되는 것은 아니다. 단계 S1210에서, 서버는 영상 내에 키워드에 해당하는 장면이 있는지 판단한다. 예를 들어, 아동이 디스 플레이 장치를 이용하여 미디어 콘텐트를 시청하고자 하는 경우, 서버는 기 설정된 키워드에 기초 하여 미디어 콘텐트 내에서 장면을 검색할 수 있다. 기 설정된 키워드는 미디어 콘텐트를 편집하기 위한 사용자 입력에 의해 설정될 수 있다. 기 설정된 키워드는 자연어 문장, 구절, 단어 등으로 구성될 수 있다. 기 설정된 키워드는 예를 들어, 성적 콘텐트, 폭력적 콘텐트, 약물, 음주, 인종 차별 등을 포함할 수 있으나, 이에 한정되 는 것은 아니다. 서버는 미디어 콘텐트에 대하여 비디오 분석, 오디오 분석, 텍스트 분석 중 적어도 하나를 수행하고, 각 각의 장면들에 대해 장면 콘텍스트 데이터를 획득할 수 있다. 단계 S1220에서, 서버는 키워드에 해당하는 장면을 영상 클립으로 미리보기를 제공할 수 있다. 예를 들어, 서버는 미디어 콘텐트를 편집하기 위한 사용자 인터페이스를 제공할 수 있다. 미디어 콘텐트를 편 집하기 위한 사용자 인터페이스는, 사용자의 전자 장치(예를 들어, 스마트폰 등)에서 표시될 수 있다. 서 버 미디어 콘텐트를 편집하기 위한 사용자 인터페이스와 관련된 데이터를 사용자의 전자 장치로 전 송할 수 있다. 미디어 콘텐트를 편집하기 위한 사용자 인터페이스에는 미디어 콘텐트에 대한 정보, 영상 클립 미리보기, 키워 드 검색 결과, 편집 여부 등이 표시될 수 있으나, 이에 한정되는 것은 아니다. 미디어 콘텐트를 편집하기 위한 사용자 인터페이스는 도 14를 참조하여 더 기술한다. 단계 S1230에서, 서버는 유해 장면을 제거한 영상을 제공한다. 서버는 사용자의 전자 장치로 부터 미디어 콘텐트를 편집하는 입력을 수신하고, 수신된 입력에 기초하여 미디어 콘텐트를 편집할 수 있다. 도 12의 예시에서는 유해 장면을 제거하는 것이 미디어 콘텐트를 편집하는 것을 의미한다. 단계 S1240에서, 서버는 유해 장면을 제거한 영상이 재생되도록 한다. 유해 장면을 제거한 영상은 디스플 레이 장치에서 재생될 수 있다. 도 13은 본 개시의 일 실시예에 따른 서버가 미디어 콘텐트 편집을 제공하는 동작을 도시한 흐름도이다. 도 13을 참조하여, 도 12의 실시예를 보다 구체적으로 설명한다. 일 실시예로, 디스플레이 장치는 아동이 미디어 콘텐트를 시청하고자 하는 디바이스이고, 전자 장치는 유해 장면을 편집하고자 하는 부모/보호자 의 디바이스임을 가정하여 설명할 것이다. 디스플레이 장치는 미디어 콘텐트의 재생 시도가 있는지를 식별한다(S1302). 디스플레이 장치는 미 디어 콘텐트의 재생 시도가 있는 경우, 미디어 콘텐트의 재생 요청이 있음을 전자 장치 및/또는 서버 로 전송할 수 있다. 전자 장치는 재생 시도된 미디어 콘텐트의 정보를 획득한다(S1304). 전자 장치는 미디어 콘텐트의 정보를 디스플레이 장치 및/또는 서버로부터 수신할 수 있다. 일 실시예에서, 아동이 미디어 콘텐트를 시청하기 위해 미디어 콘텐트의 편집이 필요하지만, 미디어 콘텐트의 편집을 위한 기 설정된 키워드 필터가 존재하지 않을 수 있다. 이 경우, 전자 장치는 유해 장면 편집을 위한 키워드를 추천할 수 있다. 예를 들어, 전자 장치는 유해성 카테고리와 레벨을 선택하도록 요청할 수 있다(S1306). 그리고, 전자 장치는 선택된 유해성 카테고리 및 레벨을 기초로, 추천 키워드를 선택하도록 요청할 수 있다(S1308). 일 실시예에서, 미디어 콘텐트에 대하여, 보호자에 의해 기 설정된 키워드 필터가 있을 수 있다. 기 설정된 키 워드는 사용자 입력에 의해 설정된 것일 수 있다. 기 설정된 키워드는 예를 들어, 성적 콘텐트, 폭력적 콘텐트, 약물, 음주, 인종 차별 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 또한, 기 설정된 키워드는 단어 뿐 만 아니라 자연어 입력도 포함할 수 있다. 전자 장치는 영상 내 장면들 중 키워드에 대응하는 장면들을 표시한다(S1310). 영상 내 키워드에 대응하 는 장면들에 관한 정보는, 서버로부터 수신될 수 있다. 서버 후술되는 단계 S1312 내지 S1320을 통 해 미디어 콘텐트 내에서 키워드에 대응하는 장면들에 관한 정보를 전자 장치로 전송할 수 있다. 서버는 사용자 입력 내용 중 핵심 키워드를 도출한다(S1312). 예를 들어, 사용자 입력이 \"피가 나오는 장 면은 삭제하고 싶어\"인 경우, 서버는 자연어 처리를 통해 \"피\", \"폭력\", \"전투\", \"살인\" 등의 핵심 키워 드를 도출할 수 있다. 또는, 사용자 입력이 \"피\"와 같은 키워드인 경우, 서버는 \"피\" 및 \"피\"와 관련된 키워드들인 \"폭력\", \"전투\", \"살인\" 등을 핵심 키워드로 도출할 수 있다. 서버는 다수의 키워드를 조합한 다(S1314). 서버는 비디오 내 객체 분석을 수행한다(S1316). 서버가 비디오를 분석하여 비디오 콘텍스트 데이 터를 획득하는 것은 전술하였으므로, 반복되는 설명은 생략한다. 서버는 오디오 내 대사, 음량, 폭발음 등의 분석을 수행한다(S1318). 서버가 오디오를 분석하여 오 디오 콘텍스트 데이터를 획득하는 것은 전술하였으므로, 반복되는 설명은 생략한다. 서버는 장면 묘사 키워드를 도출한다(S1320). 서버는 비디오 콘텍스트 데이터 및 오디오 콘텍스트 데이터에 기초하여, 장면 콘텍스트 데이터를 획득할 수 있다. 이는, 전술하였으므로, 반복되는 설명은 생략한다. 한편, 미디어 콘텐트에 텍스트 데이터가 포함되는 경우, 서버는 텍스트 분석을 수행하여 텍스트 콘텍스트 데이터를 생성하고, 장면 콘텍스트 데이터를 생성하는 데에 텍스트 콘텍스트 데이터를 더 이용할 수 있다. 서버는 사용자 입력에 기초하여 획득된 다수 키워드의 조합과, 장면 묘사 키워드를 비교하여, 미디어 콘 텐트 내 키워드에 대응되는 장면들을 식별할 수 있다. 전자 장치는 표시된 장면들에 대하여, 시청 허용 여부를 선택한다(S1322). 시청 허용 여부란, 전자 장치 의 사용자가 시청을 허용하는 경우 검색된 장면들이 미디어 콘텐트 내에서 유지되고, 전자 장치의 사용자가 시청을 허용하지 않는 경우, 검색된 장면들이 미디어 콘텐트 내에서 삭제됨을 의미할 수 있다. 시청 허용을 하지 않은 장면이 없는 경우, 디스플레이 장치에서 원본 영상 콘텐트가 재생된다(S1324). 시청 허용을 하지 않은 장면이 존재하는 경우, 전자 장치는 유해 장면을 제거한 타임라인을 설정한다 (S1326). 전자 장치는 사용자가 유해 장면으로 판단한 타임 구간을 서버로 전송하고, 서버는 설정된 타임 구간에 대응되는 장면들을 삭제할 수 있다. 이 경우, 디스플레이 장치에서 설정한 타임라인 에 따라 편집된 영상 콘텐트가 재생된다(S1328). 도 14는 사용자의 전자 장치에서 표시되는 미디어 콘텐트 편집 인터페이스를 도시한 도면이다. 일 실시예에서, 서버는 미디어 콘텐트를 편집하기 위한 사용자 인터페이스를 전자 장치에게 제공할 수 있다. 전자 장치는 미디어 콘텐트를 편집하기 위한 사용자 인터페이스를 화면에 표시할 수 있다. 사용자 전자 장치의 제1 화면을 참조하면, 제1 화면에는 미디어 콘텐트에 대한 정보, 설정된 키워드, 장면 검색 결과, 영상 클립 미리보기, 키워드 검색 결과, 편집 여부 등이 표시될 수 있으나, 이 에 한정되는 것은 아니다. 제1 화면을 참조하면, 키워드에 대응하는 장면을 검색한 결과, 제1 장면 검색 결과, 제2 장면 검색 결과 및 제3 장면 검색 결과가 표시될 수 있다. 예를 들어, 제3 장면 검색 결과 를 참조하면, 검색된 장면의 썸네일, 장면의 타임 구간, 장면의 키워드, 편집 버튼 등이 포함될 수 있다. 사용자는 검색된 장면의 동영상을 미리보기를 할 수 있다. 사용자는 검색된 장면의 썸네일을 선택하여, 검색된 장면이 구체적으로 어떤 장면에 관한 것인지 직접 확인할 수 있다. 예를 들어, 전자 장치의 제2 화면 을 참조하면, 사용자가 동영상 미리보기를 선택하는 경우, 화면 내 화면을 통해 동영상 미리보기가 제공될 수 있다."}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "일 실시예에서, 서버는 미디어 콘텐트 편집 결과의 요약을 전자 장치에게 제공할 수 있다. 전자 장"}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "치의 제3 화면을 참조하면, 전자 장치는 미디어 콘텐트 편집 결과의 요약을 나타내는 편집"}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "결과 요약 창을 화면에 표시할 수 있다. 도 15는 본 개시의 일 실시예에 따른 서버의 구성을 도시한 블록도이다. 일 실시예에서, 서버는 통신 인터페이스, 메모리 및 프로세서를 포함할 수 있다. 통신 인터페이스는 통신 회로를 포함할 수 있다. 통신 인터페이스는 예를 들어, 유선 랜, 무선 랜 (Wireless LAN), 와이파이(Wi-Fi), LTE(Long-Term Evolution), 5G, 위성 통신, 라디오 통신을 포함하는 원거리 데이터 통신 방식 중 적어도 하나를 이용하여, 서버와 다른 디바이스들 간의 데이터 통신을 수행할 수 있 는, 통신 회로를 포함할 수 있다. 예를 들어, 서버는 디스플레이 장치 및/또는 사용자의 전자 장치 와 데이터 통신을 수행할 수 있다. 메모리는 ROM(Read-only memory)(예를 들어, PROM(Programmable read-only memory), EPROM(Erasable programmable read-only memory), EEPROM(Electrically erasable programmable read-only memory)), 플래시 메모리(Flash memory)(예를 들어, 메모리 카드, SSD(Solid-state drive)) 및 아날로그 기록 타입(예를 들어, HDD(Hard disk drive), 자기테이프, 광학 디스크)와 같은 비휘발성 메모리 및, RAM(random-access memory)(예 를 들어, DRAM(Dynamic random-access memory), SRAM(Static random-access memory))과 같은 휘발성 메모리를 포함할 수 있다. 프로세서는 서버의 전반적인 동작들을 제어할 수 있다. 예를 들어, 프로세서는 메모리(220 0)에 저장된 프로그램의 하나 이상의 명령어들(instructions)을 실행함으로써, 디스플레이 장치가 사용자 단말 간 비공개 연결을 설정하기 위한 전반적인 동작들을 제어할 수 있다. 프로세서는 하나 이상일 수 있 다. 메모리는 서버가 미디어 콘텐트를 처리하기 위해 동작하도록 하는 하나 이상의 인스트럭션 및 프로 그램을 저장할 수 있다. 예를 들어, 메모리에는 비디오 분석 모듈, 오디오 분석 모듈, 텍스 트 분석 모듈 및 콘텐트 탐색 모듈이 저장될 수 있다. 프로세서는 서버의 전반적인 동작들을 제어할 수 있다. 예를 들어, 프로세서는 메모리(220 0)에 저장된 프로그램의 하나 이상의 명령어들(instructions)을 실행함으로써, 서버가 미디어 콘텐트를 분석하고 탐색하도록 하는 전반적인 동작들을 제어할 수 있다. 프로세서는 하나 이상일 수 있다. 하나 이상의 프로세서는 CPU (Central Processing Unit), GPU (Graphics Processing Unit), APU (Accelerated Processing Unit), MIC (Many Integrated Core), DSP (Digital Signal Processor), 및 NPU (Neural Processing Unit) 중 적어도 하나를 포함할 수 있다. 하나 이상의 프로세서는, 하나 이상의 전자 부품을 포함하는 집적된 시스템 온 칩(SoC) 형태로 구현될 수 있다. 하나 이상의 프로세서 각각은 별개의 하드 웨어(H/W)로 구현될 수도 있다. 프로세서는 비디오 분석 모듈을 실행하여 비디오 분석을 수행할 수 있다. 비디오 분석 모듈 의 동작들과 관련된 설명은 전술한 도면들에서 이미 설명하였으므로, 간결함을 위해 반복되는 설명은 생략한다. 프로세서는 오디오 분석 모듈을 실행하여 오디오 분석을 수행할 수 있다. 오디오 분석 모듈 의 동작들과 관련된 설명은 전술한 도면들에서 이미 설명하였으므로, 간결함을 위해 반복되는 설명은 생략한다. 프로세서는 텍스트 분석 모듈을 실행하여 텍스트 분석을 수행할 수 있다. 텍스트 분석 모듈 의 동작들과 관련된 설명은 전술한 도면들에서 이미 설명하였으므로, 간결함을 위해 반복되는 설명은 생략한다.프로세서는 콘텐트 탐색 모듈을 이용하여, 미디어 콘텐트를 탐색하기 위한 사용자 의도에 대응하는 비디오 프레임 및/또는 장면을 검색할 수 있다. 프로세서는 비디오 분석, 오디오 분석, 텍스트 분석 중 적어도 하나에 기초하여 생성된 장면 콘텍스트 데이터를 이용할 수 있다. 콘텐트 탐색 모듈의 동작들과 관련된 설명은 전술한 도면들에서 이미 설명하였으므로, 간결함을 위해 반복되는 설명은 생략한다. 한편, 전술한 메모리에 저장되어 프로세서에 의해 실행되는 모듈들은, 설명의 편의를 위한 것이며 반드시 이에 한정되는 것은 아니다. 전술한 실시예들을 구현하기 위해 다른 모듈이 추가될 수 있으며, 하나의 모듈이 세부적인 기능들에 따라 구별되는 복수의 모듈들로 분할될 수 있고, 전술한 모듈들 중 일부의 모듈들이 합쳐져 하나의 모듈로 구현될 수도 있다. 본 개시의 일 실시예에 따른 방법이 복수의 동작을 포함하는 경우, 복수의 동작은 하나의 프로세서에 의해 수행 될 수도 있고, 복수의 프로세서에 의해 수행될 수도 있다. 예를 들어, 일 실시예에 따른 방법에 의해 제1 동작, 제2 동작, 제3 동작이 수행될 때, 제1 동작, 제2 동작, 및 제3 동작 모두 제1 프로세서에 의해 수행될 수도 있 고, 제1 동작 및 제2 동작은 제1 프로세서(예를 들어, 범용 프로세서)에 의해 수행되고 제3 동작은 제2 프로세 서(예를 들어, 인공지능 전용 프로세서)에 의해 수행될 수도 있다. 여기서, 제2 프로세서의 예시인 인공지능 전 용 프로세서는, 인공지능 모델의 훈련/추론을 위한 연산들이 수행될 수도 있다. 그러나, 본 개시의 실시예들이 이에 한정되는 것은 아니다. 본 개시에 따른 하나 이상의 프로세서는 싱글 코어 프로세서(single-core processor)로 구현될 수도 있고, 멀티 코어 프로세서(multi-core processor)로 구현될 수도 있다. 본 개시의 일 실시예에 따른 방법이 복수의 동작을 포함하는 경우, 복수의 동작은 하나의 코어에 의해 수행될 수도 있고, 하나 이상의 프로세서에 포함된 복수의 코어에 의해 수행될 수도 있다. 도 16은 본 개시의 일 실시예에 따른 디스플레이 장치의 구성을 도시한 블록도이다. 일 실시예에서, 디스플레이 장치는 통신 인터페이스, 디스플레이, 메모리 및 프로세서 를 포함할 수 있다. 디스플레이 장치는 디스플레이를 포함하는, TV, 스마트 모니터, 태블릿 PC, 랩 톱, 디지털 사이니지(Digital Signage), 대형 디스플레이, 360도 프로젝터 등을 포함할 수 있으나, 이에 한정되 는 것은 아니다. 디스플레이 장치의 통신 인터페이스, 메모리 및 프로세서는, 도 15의 서버의 통신 인터페이스, 메모리 및 프로세서와 각각 대응되어 동일하거나 유사한 동 작을 수행하므로, 반복되는 설명은 생략한다. 디스플레이는 프로세서의 제어에 의해 디스플레이 장치의 화면에 영상 신호를 출력할 수 있 다. 예를 들어, 디스플레이 장치는 미디어 콘텐트를 디스플레이를 통해 출력할 수 있다. 일 실시예에서, 전술한 서버의 동작들은 디스플레이 장치에서 수행될 수 있다. 디스플레이 장치 는 미디어 콘텐트를 획득하고, 미디어 콘텐트에 대하여 비디오 분석, 오디오 분석, 텍스트 분석 중 적어 도 하나를 수행할 수 있다. 디스플레이 장치는 비디오 콘텍스트 데이터, 오디오 콘텍스트 데이터, 텍스트 콘텍스트 데이터 중 적어도 하나에 기초하여, 미디어 콘텐트의 비디오 프레임들에 대응하는 장면 콘텍스트 데이터를 생성할 수 있다. 디스플레이 장치는 사용자 입력(예를 들어, 자연어 발화)을 수신하고, 미디어 콘텐트를 탐색하기 위한 사 용자 의도를 식별할 수 있다. 디스플레이 장치는 장면 콘텍스트 데이터를 이용하여, 사용자 의도에 대응하는 비디오 프레임을 식별하고, 식별된 비디오 프레임을 출력할 수 있다. 디스플레이 장치가 미디어 콘텐트를 분석하고 탐색하기 위한 기타의 추가적이고 상세한 동작들은, 이전의 도면들에서 설명한 서버의 동작들이 동일하게 적용될 수 있다. 따라서, 간결함을 위해 반복되는 설명은 생략한다. 도 17은 본 개시의 일 실시예에 따른 전자 장치의 구성을 도시한 블록도이다. 일 실시예에서, 전자 장치는 통신 인터페이스, 디스플레이, 메모리 및 프로세서(440 0)를 포함할 수 있다. 전자 장치는, 전술한 도면들에서 설명한 사용자의 전자 장치를 말한다. 전자 장치 는 예를 들어, 데스크톱, 랩톱, 스마트폰, 태블릿 등일 수 있으나, 이에 한정되는 것은 아니다.일 실시예에서, 사용자는 전자 장치를 이용하여 미디어 콘텐트를 편집할 수 있다. 전자 장치는 서 버 및/또는 디스플레이 장치로부터 미디어 콘텐트를 편집하기 위한 사용자 인터페이스와 관련된 정 보를 수신할 수 있다. 전자 장치는 미디어 콘텐트를 편집하기 위한 사용자 인터페이스를 표시하고, 사용 자로부터 미디어 콘텐트를 편집하는 사용자 입력을 수신할 수 있다. 전자 장치는 미디어 콘텐트 편집에 관련된 정보를 서버 및/또는 디스플레이 장치로 전송할 수 있다."}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "일 실시예에서, 사용자는 전자 장치를 통해 미디어 콘텐트를 편집한 결과의 요약을 제공받을 수 있다. 서"}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "버는 미디어 콘텐트 편집 결과의 요약을 전자 장치에게 제공할 수 있다. 본 개시는, 미디어 콘텐트를 시청하는 사용자에게, 자연어를 입력 받아 정밀하게 장면을 탐색하는 방법을 제공 한다. 본 개시에서 이루고자 하는 기술적 과제는, 이상에서 언급한 것으로 제한되지 않으며, 언급되지 않은 또"}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "다른 기술적 과제들은 본 명세서의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확 하게 이해될 수 있을 것이다. 본 개시의 일 측면에 따르면, 서버가 미디어 콘텐트를 제공하는 방법이 제공될 수 있다. 상기 방법은, 비디오 데이터 및 오디오 데이터를 포함하는 미디어 콘텐트를 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 비디오 데이터를 분석하여 비디오와 관련된 제1 콘텍스트 데이터를 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 오디오 데이터를 분석하여 오디오와 관련된 제2 콘텍스트 데이터를 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 제1 콘텍스트 데이터 및 상기 제2 콘텍스트 데이터에 기초하여, 상기 미디어 콘텐트의 비디 오 프레임들에 대응하는 장면 콘텍스트 데이터를 생성하는 단계를 포함할 수 있다. 상기 방법은, 사용자 입력에 기초하여, 상기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정하는 단계를 포 함할 수 있다. 상기 방법은, 상기 장면 콘텍스트 데이터에 기초하여, 상기 사용자 의도에 대응하는 적어도 하나의 비디오 프레 임을 식별하는 단계를 포함할 수 있다. 상기 방법은, 상기 식별된 적어도 하나의 비디오 프레임을 출력하는 단계를 포함할 수 있다. 상기 미디어 콘텐트는 텍스트 데이터를 포함할 수 있다. 상기 방법은, 상기 텍스트 데이터를 분석하여 오디오와 관련된 제3 콘텍스트 데이터를 획득하는 단계를 포함할 수 있다. 상기 장면 콘텍스트 데이터를 생성하는 단계는, 상기 제3 콘텍스트 데이터에 더 기초하여, 상기 장면 콘텍스트 데이터를 생성하는 것일 수 있다. 상기 제1 콘텍스트 데이터를 획득하는 단계는, 상기 비디오 데이터의 비디오 프레임들 중 적어도 일부에 객체 인식을 적용하여 장면 정보를 획득하는 단계를 포함할 수 있다. 상기 제1 콘텍스트 데이터를 획득하는 단계는, 상기 장면 정보에 기초하여 적어도 하나의 비디오 프레임에 대응 하는 적어도 하나의 장면 그래프를 생성하는 단계를 포함할 수 있다. 상기 제1 콘텍스트 데이터를 획득하는 단계는, 상기 적어도 하나의 장면 그래프에 기초하여, 비디오의 콘텍스트 를 나타내는 상기 제1 콘텍스트 데이터를 획득하는 단계를 포함할 수 있다. 상기 제2 콘텍스트 데이터를 획득하는 단계는, 상기 오디오 데이터에 음성 인식, 사운드 이벤트 검출, 사운드 이벤트 분류 중 적어도 하나를 적용하여, 장면-사운드 정보를 획득하는 단계를 포함할 수 있다. 상기 제2 콘텍스트 데이터를 획득하는 단계는, 상기 장면-사운드 정보에 기초하여, 오디오의 콘텍스트를 나타내 는 상기 제2 콘텍스트 데이터를 획득하는 단계를 포함할 수 있다. 상기 제3 콘텍스트 데이터를 획득하는 단계는, 상기 텍스트 데이터에 자연어 처리를 적용하여, 장면-텍스트 정 보를 획득하는 단계를 포함할 수 있다. 상기 제3 콘텍스트 데이터를 획득하는 단계는, 상기 장면-텍스트 정보에 기초하여, 텍스트의 콘텍스트를 나타내 는 상기 제3 콘텍스트 데이터를 획득하는 단계를 포함할 수 있다. 상기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정하는 단계는, 상기 사용자 입력이 발화인 것에 기초하 여, 자동 음성 인식(Automatic Speech Recognition; ASR)을 수행하는 단계를 포함할 수 있다. 상기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정하는 단계는, 상기 자동 음성 인식 결과에 자연어 이해 (Natural Language Understanding; NLU) 알고리즘을 적용하여, 상기 사용자 의도를 결정하는 단계를 포함할 수 있다. 상기 방법은, 출력된 상기 적어도 하나의 비디오 프레임 중 하나를 선택하는 사용자 입력에 기초하여, 상기 선 택된 비디오 프레임부터 상기 미디어 콘텐트가 재생되도록 하는 단계를 포함할 수 있다. 상기 사용자 입력은, 상기 미디어 콘텐트를 편집하기 위한 적어도 하나의 키워드를 포함할 수 있다. 상기 적어도 하나의 비디오 프레임을 식별하는 단계는, 상기 장면 콘텍스트 데이터에 기초하여, 상기 적어도 하 나의 키워드에 대응하는 비디오 프레임을 식별하는 단계를 포함할 수 있다. 상기 방법은, 상기 미디어 콘텐트를 편집하기 위한 사용자 인터페이스를 제공하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "상기 방법은, 상기 미디어 콘텐트의 편집 결과 요약을 제공하는 단계를 포함할 수 있다. 본 개시의 일 측면에 따르면, 미디어 콘텐트를 제공하는 서버가 제공될 수 있다. 상기 서버는, 통신 인터페이스; 하나 이상의 인스트럭션들을 저장하는 메모리; 및 상기 하나 이상의 인스트럭션 들을 실행하는 적어도 하나의 프로세서를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 비디오 데이터 및 오디오 데 이터를 포함하는 미디어 콘텐트를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 비디오 데이터를 분석하 여 비디오와 관련된 제1 콘텍스트 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 오디오 데이터를 분석하 여 오디오와 관련된 제2 콘텍스트 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 제1 콘텍스트 데이터 및 상기 제2 콘텍스트 데이터에 기초하여, 상기 미디어 콘텐트의 비디오 프레임들에 대응하는 장면 콘텍스트 데이 터를 생성할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 사용자 입력에 기초하여, 상 기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 장면 콘텍스트 데이터에 기초하여, 상기 사용자 의도에 대응하는 적어도 하나의 비디오 프레임을 식별할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 식별된 적어도 하나의 비디오 프레임을 출력 할 수 있다. 상기 미디어 콘텐트는 텍스트 데이터를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 텍스트 데이터를 분석하 여 오디오와 관련된 제3 콘텍스트 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 제3 콘텍스트 데이터에 더 기초하여, 상기 장면 콘텍스트 데이터를 생성할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 비디오 데이터의 비디오 프레임들 중 적어도 일부에 객체 인식을 적용하여 장면 정보를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 장면 정보에 기초하여 적어도 하나의 비디오 프레임에 대응하는 적어도 하나의 장면 그래프를 생성할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 적어도 하나의 장면 그 래프에 기초하여, 비디오의 콘텍스트를 나타내는 상기 제1 콘텍스트 데이터를 획득할 수 있다.상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 오디오 데이터에 음성 인식, 사운드 이벤트 검출, 사운드 이벤트 분류 중 적어도 하나를 적용하여, 장면-사운드 정보를 획득할 수 있 다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 장면-사운드 정보에 기 초하여, 오디오의 콘텍스트를 나타내는 상기 제2 콘텍스트 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 텍스트 데이터에 자연어 처리를 적용하여, 장면-텍스트 정보를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 장면-텍스트 정보에 기 초하여, 텍스트의 콘텍스트를 나타내는 상기 제3 콘텍스트 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 사용자 입력이 발화인 것에 기초하여, 자동 음성 인식(Automatic Speech Recognition; ASR)을 수행할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 자동 음성 인식 결과에 자연어 이해(Natural Language Understanding; NLU) 알고리즘을 적용하여, 상기 사용자 의도를 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 출력된 상기 적어도 하나의 비디오 프레임 중 하나를 선택하는 사용자 입력에 기초하여, 상기 선택된 비디오 프레임부터 상기 미디어 콘텐 트가 재생되도록 할 수 있다. 상기 사용자 입력은, 상기 미디어 콘텐트를 편집하기 위한 적어도 하나의 키워드를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 장면 콘텍스트 데이터에 기초하여, 상기 적어도 하나의 키워드에 대응하는 비디오 프레임을 식별할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 미디어 콘텐트를 편집하 기 위한 사용자 인터페이스를 제공할 수 있다. 본 개시의 일 측면에 따르면, 미디어 콘텐트를 제공하는 디스플레이 장치가 제공될 수 있다. 상기 디스플레이 장치는, 통신 인터페이스; 디스플레이; 하나 이상의 인스트럭션들을 저장하는 메모리; 및 상기 하나 이상의 인스트럭션들을 실행하는 적어도 하나의 프로세서를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 비디오 데이터 및 오디오 데 이터를 포함하는 미디어 콘텐트를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 비디오 데이터를 분석하 여 비디오와 관련된 제1 콘텍스트 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 오디오 데이터를 분석하 여 오디오와 관련된 제2 콘텍스트 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 제1 콘텍스트 데이터 및 상기 제2 콘텍스트 데이터에 기초하여, 상기 미디어 콘텐트의 비디오 프레임들에 대응하는 장면 콘텍스트 데이 터를 생성할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 사용자 입력에 기초하여, 상 기 미디어 콘텐트를 탐색하기 위한 사용자 의도를 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 장면 콘텍스트 데이터에 기초하여, 상기 사용자 의도에 대응하는 적어도 하나의 비디오 프레임을 식별할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션들을 실행함으로써, 상기 식별된 적어도 하나의 비디오 프레임이 상기 디스플레이의 화면에 출력되도록 할 수 있다. 한편, 본 개시의 실시예들은 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행 가능한 명령어 를 포함하는 기록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스 될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독 가능 매체는 컴퓨터 저장 매체 및 통신 매체를 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독 가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구 현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독 가 능 명령어, 데이터 구조, 또는 프로그램 모듈과 같은 변조된 데이터 신호의 기타 데이터를 포함할 수 있다. 또한, 컴퓨터에 의해 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다 는 것을 의미할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경 우를 구분하지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두개의 사용자 장치들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운 로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다."}
{"patent_id": "10-2023-0055653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "전술한 본 개시의 설명은 예시를 위한 것이며, 본 개시가 속하는 기술분야의 통상의 지식을 가진 자는 본 개시 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 개시의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 개시의 범위에 포함되는 것으 로 해석되어야 한다."}
{"patent_id": "10-2023-0055653", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 서버가 미디어 콘텐트 서비스를 제공하는 것을 개략적으로 도시한 도면이다. 도 2는 본 개시의 일 실시예에 따른 서버가 미디어 콘텐트 서비스를 제공하는 동작을 설명하기 위한 흐름도이다. 도 3은 본 개시의 일 실시예에 따른 서버가 미디어 콘텐트로부터 장면 콘텍스트 데이터를 획득하는 것을 개략적 으로 도시한 도면이다. 도 4는 본 개시의 일 실시예에 따른 서버가 비디오 데이터를 분석하는 동작을 설명하기 위한 도면이다. 도 5는 본 개시의 일 실시예에 따른 서버가 오디오 데이터를 분석하는 동작을 설명하기 위한 도면이다. 도 6은 본 개시의 일 실시예에 따른 서버가 텍스트 데이터를 분석하는 동작을 설명하기 위한 도면이다. 도 7은 본 개시의 일 실시예에 따른 서버가 생성한 장면 콘텍스트 데이터를 설명하기 위한 도면이다. 도 8은 본 개시의 일 실시예에 따른 서버가 사용자 입력 및 장면 콘텍스트에 기초하여 미디어 콘텐트를 탐색하 는 동작을 설명하기 위한 흐름도이다. 도 9는 본 개시의 일 실시예에 따른 서버가 사용자 입력 및 장면 콘텍스트에 기초하여 미디어 콘텐트를 탐색하 는 동작을 설명하기 위한 흐름도이다. 도 10은 본 개시의 일 실시예에 따른, 사용자가 장면을 탐색하는 것을 도시한 도면이다. 도 11은 본 개시의 일 실시예에 따른, 사용자가 장면을 탐색하는 것을 도시한 도면이다. 도 12는 본 개시의 일 실시예에 따른, 사용자가 미디어 콘텐트를 편집하는 동작을 개략적으로 도시한 도면이다.도 13은 본 개시의 일 실시예에 따른 서버가 미디어 콘텐트 편집을 제공하는 동작을 도시한 흐름도이다. 도 14는 사용자의 전자 장치에서 표시되는 미디어 콘텐트 편집 인터페이스를 도시한 도면이다. 도 15는 본 개시의 일 실시예에 따른 서버의 구성을 도시한 블록도이다. 도 16은 본 개시의 일 실시예에 따른 디스플레이 장치의 구성을 도시한 블록도이다. 도 17은 본 개시의 일 실시예에 따른 전자 장치의 구성을 도시한 블록도이다."}
