{"patent_id": "10-2023-0087076", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0007222", "출원번호": "10-2023-0087076", "발명의 명칭": "자세 추정을 위한 방법 및 장치", "출원인": "에스케이텔레콤 주식회사", "발명자": "백정렬"}}
{"patent_id": "10-2023-0087076", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "비디오 영상 내 객체의 자세를 추정하기 위한 방법으로서,복수의 프레임들의 시퀀스를 포함하는 비디오 영상을 수신하는 과정;상기 시퀀스의 제1 키프레임에 대해, 제1 자세추정 모델을 이용하여, 상기 제1 키프레임 내에서 객체가 포함된영역을 크롭핑한 영상으로부터 상기 객체의 제1 관절정보를 추정하는 과정;상기 제1 키프레임에 후속하는 키프레임 이외의 후속 프레임들 각각에 대해, 제2 자세추정 모델을 이용하여, 상기 후속 프레임 내에서 상기 객체가 포함된 영역을 크롭핑한 영상과 상기 제1 관절정보로부터 상기 객체의 제1관절정보의 변위를 추정하는 과정; 및상기 제1 관절정보의 변위와 상기 제1 관절정보로부터 상기 후속 프레임 내의 상기 객체의 제2 관절정보를 추정하는 과정을 포함하는 방법."}
{"patent_id": "10-2023-0087076", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 제1 키프레임은,상기 시퀀스에서 처음 프레임부터 시작하여 기설정된 프레임 간격마다 위치하는 키프레임들 각각인, 방법."}
{"patent_id": "10-2023-0087076", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 제1 자세추정 모델은,객체의 관절정보를 나타내는 주석들(annotations)이 포함된 영상들로 구성되는 훈련 데이터셋을 이용하여, 입력된 영상에 대응하는 상기 객체의 관절정보를 예측하도록 훈련된 신경망 모델인, 방법."}
{"patent_id": "10-2023-0087076", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 제2 자세추정 모델은,현재 프레임에서 객체를 포함하는 영상과 이전 프레임에서 상기 객체의 관절정보로부터 상기 객체의 관절정보의변위를 예측하는 과정;예측된 관절정보의 변위와 GT(Ground Truth) 관절정보의 변위를 비교하여 손실(Loss)을 산출하는 과정; 및산출된 손실을 최소화하도록 상기 제2 자세추정 모델의 파라미터들을 갱신하는 과정을 통해 훈련된 신경망 모델인, 방법."}
{"patent_id": "10-2023-0087076", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서,상기 이전 프레임에서 상기 객체의 관절정보는,상기 이전 프레임에서 상기 객체가 포함된 영상의 GT 관절정보 또는 상기 이전 프레임에서 상기 객체가 포함된영상으로부터 상기 제1 자세추정 모델을 이용하여 추정된 상기 객체의 관절정보인, 방법.공개특허 10-2025-0007222-3-청구항 6 제4 항에 있어서,상기 손실은 회귀 손실 함수인, 방법."}
{"patent_id": "10-2023-0087076", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 제2 자세추정 모델은,특징 추출을 위한 트랜스포머(transformer)와 관절정보의 변위 예측을 위한 디코더(decoder)를 포함하는, 방법."}
{"patent_id": "10-2023-0087076", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "비디오 영상 내 객체의 자세를 추정하기 위한 장치에 있어서,명령어들을 저장하는 메모리; 및적어도 하나의 프로세서를 포함하되,상기 적어도 하나의 프로세서는 상기 명령어들을 실행함으로써, 복수의 프레임들의 시퀀스를 포함하는 비디오 영상을 수신하고,상기 시퀀스의 제1 키프레임에 대해, 제1 자세추정 모델을 이용하여, 상기 제1 키프레임 내에서 객체가 포함된영역을 크롭핑한 영상으로부터 상기 객체의 제1 관절정보를 추정하고,상기 제1 키프레임에 후속하는 키프레임 이외의 후속 프레임들 각각에 대해, 제2 자세추정 모델을 이용하여, 상기 후속 프레임 내에서 상기 객체가 포함된 영역을 크롭핑한 영상과 상기 제1 관절정보로부터 상기 객체의 제1관절정보의 변위를 추정하고,상기 제1 관절정보의 변위와 상기 제1 관절정보로부터 상기 후속 프레임 내의 상기 객체의 제2 관절정보를 추정하는, 장치."}
{"patent_id": "10-2023-0087076", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "명령어가 저장된, 컴퓨터로 읽을 수 있는 기록매체로서, 상기 명령어는 상기 컴퓨터에 의해 실행될 때 상기 컴퓨터로 하여금,복수의 프레임들의 시퀀스를 포함하는 비디오 영상을 수신하는 과정;상기 시퀀스의 제1 키프레임에 대해, 제1 자세추정 모델을 이용하여, 상기 제1 키프레임 내에서 객체가 포함된영역을 크롭핑한 영상으로부터 상기 객체의 제1 관절정보를 추정하는 과정;상기 제1 키프레임에 후속하는 키프레임 이외의 후속 프레임들 각각에 대해, 제2 자세추정 모델을 이용하여, 상기 후속 프레임 내에서 상기 객체가 포함된 영역을 크롭핑한 영상과 상기 제1 관절정보로부터 상기 객체의 제1관절정보의 변위를 추정하는 과정; 및상기 제1 관절정보의 변위와 상기 제1 관절정보로부터 상기 후속 프레임 내의 상기 객체의 제2 관절정보를 추정하는 과정을 실행하도록 하는 것을 특징으로 하는 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2023-0087076", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "비디오 영상 내 등장하는 객체의 관절정보를 효율적으로 추정하기 위한 방법 및 장치를 개시한다. 본 개시의 일 측면에 의하면, 비디오 영상 내 객체의 자세를 추정하기 위한 방법으로서, 복수의 프레임들의 시퀀 스를 포함하는 비디오 영상을 수신하는 과정; 상기 시퀀스의 제1 키프레임에 대해, 제1 자세추정 모델을 이용하 여, 상기 제1 키프레임 내에서 객체가 포함된 영역을 크롭핑한 영상으로부터 상기 객체의 제1 관절정보를 추정하 는 과정; 상기 제1 키프레임에 후속하는 키프레임이 아닌 후속 프레임들 각각에 대해, 제2 자세추정 모델을 이용 하여, 상기 후속 프레임 내에서 상기 객체가 포함된 영역을 크롭핑한 영상과 상기 제1 관절정보로부터 상기 객체 의 제1 관절정보의 변위를 추정하는 과정; 및 상기 제1 관절정보의 변위와 상기 제1 관절정보로부터 상기 후속 프레임 내의 상기 객체의 제2 관절정보를 추정하는 과정을 포함하는 방법을 제공한다."}
{"patent_id": "10-2023-0087076", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 자세 추정을 위한 방법 및 장치에 관한 것이다. 보다 상세하게는, 비디오 영상 내 등장하는 객체의 관절정보를 효율적으로 추정하기 위한 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0087076", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이하에 기술되는 내용은 단순히 본 실시예와 관련되는 배경 정보만을 제공할 뿐 종래기술을 구성하는 것이 아니 다. 자세 추정(Pose Estimation)은 이미지 내 사람의 관절정보를 추론하는 기술로 비전 분야에서 많은 주목을 받아 왔으며, 최근 인공지능 기반의 딥러닝 도입에 의해 성능이 크게 향상되고 있다. 자세 추정은 감시 시스템, 자율 주행 등 다양한 분야에 이용되며, 행동 및 제스처 인식에 도움을 준다. 자세 추정 방법은 대표적으로 하향식 접근법(Top-Down Approach)과 상향식 접근법(Bottom-Up Approach)으로 나 눌 수 있다. 하향식 접근법은 전체 영상에서 사람 검출기(Person Detector)를 통해 사람을 바운딩 박스 형태로 검출한다. 검출된 사람 객체를 전체 영상에서 별도로 분할하여, 별도의 자세 추정 모델을 통해 사람의 관절정보 를 얻는다. 상향식 접근법은 사람 검출 과정 없이 먼저 전체 영상에서 등장하는 모든 사람의 관절 위치를 추정 한 후, 별도의 후처리 작업을 통해 사람 별로 추정된 관절을 매핑한다. 비디오 자세 추정(Video Pose Estimation)은 비디오 입력을 받아 시간 축으로 사람의 관절정보를 추정하는 기법 이다. 대부분의 상용 기술의 경우 비디오 입력을 프레임 별 이미지로 변환한 후, 인공지능 네트워크(AI Network)의 입력으로 전달하여 사람의 관절정보를 추정하고, 후처리를 통해 관절의 정보를 시간 축으로 분석한 다. 다른 방식으로는 이미지 단위의 추론 방식이 아닌, 과거의 특징(feature) 정보를 현재 추론 중인 네트워크 의 별도 특징으로 전달하는 방식(예컨대, LSTM 등)이 있다. 하지만 효용성 등의 이유로 널리 사용되고 있지 못 하다. 한편, 프레임 별 이미지 단위로 관절정보를 추정하고 후처리를 통해 시간 축으로 분석하는 방식은, 이전 프레임 의 정보 없이 한 이미지의 정보만으로 관절정보를 예측하므로, 정밀한 정확도를 위해 모든 프레임에 대해 무거 운 모델을 사용할 수밖에 없는 문제점이 있다."}
{"patent_id": "10-2023-0087076", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 실시예에 따르면, 비디오 영상 내 등장하는 객체의 관절정보를 효율적으로 추정하기 위하여 프레임 별로 서로 다른 모델을 이용할 수 있는 방법 및 장치를 제공한다. 본 개시의 실시예에 따르면, 현재 프레임의 영상 정보뿐만 아니라 이전 프레임의 관절정보를 함께 이용하여 객 체의 관절정보를 추정할 수 있다. 본 발명이 해결하고자 하는 과제들은 이상에서 언급한 과제들로 제한되지 않으며, 언급되지 않은 또 다른 과제 들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0087076", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 의하면, 비디오 영상 내 객체의 자세를 추정하기 위한 방법으로서, 복수의 프레임들의 시퀀스를 포함하는 비디오 영상을 수신하는 과정; 상기 시퀀스의 제1 키프레임에 대해, 제1 자세추정 모델을 이 용하여, 상기 제1 키프레임 내에서 객체가 포함된 영역을 크롭핑한 영상으로부터 상기 객체의 제1 관절정보를 추정하는 과정; 상기 제1 키프레임에 후속하는 키프레임이 아닌 후속 프레임들 각각에 대해, 제2 자세추정 모델 을 이용하여, 상기 후속 프레임 내에서 상기 객체가 포함된 영역을 크롭핑한 영상과 상기 제1 관절정보로부터 상기 객체의 제1 관절정보의 변위를 추정하는 과정; 및 상기 제1 관절정보의 변위와 상기 제1 관절정보로부터 상기 후속 프레임 내의 상기 객체의 제2 관절정보를 추정하는 과정을 포함하는 방법을 제공한다. 상기 제1 키프레임은, 상기 시퀀스에서 처음 프레임부터 시작하여 기설정된 프레임 간격마다 위치하는 키프레임 들 각각을 포함한다. 본 개시의 다른 실시예에 의하면, 비디오 영상 내 객체의 자세를 추정하기 위한 장치에 있어서, 명령어들을 저 장하는 메모리; 및 적어도 하나의 프로세서를 포함하되, 상기 적어도 하나의 프로세서는 상기 명령어들을 실행 함으로써, 복수의 프레임들의 시퀀스를 포함하는 비디오 영상을 수신하고, 상기 시퀀스의 제1 키프레임에 대해, 제1 자세추정 모델을 이용하여, 상기 제1 키프레임 내에서 객체가 포함된 영역을 크롭핑한 영상으로부터 상기 객체의 제1 관절정보를 추정하고, 상기 제1 키프레임에 후속하는 키프레임이 아닌 후속 프레임들 각각에 대해, 제2 자세추정 모델을 이용하여, 상기 후속 프레임 내에서 상기 객체가 포함된 영역을 크롭핑한 영상과 상기 제1 관절정보로부터 상기 객체의 제1 관절정보의 변위를 추정하고, 상기 제1 관절정보의 변위와 상기 제1 관절정보로부터 상기 후속 프레임 내의 상기 객체의 제2 관절정보를 추정하는, 장치를 제공한다. 본 개시의 또 다른 실시예에 의하면, 명령어가 저장된, 컴퓨터로 읽을 수 있는 기록매체로서, 상기 명령어는 상 기 컴퓨터에 의해 실행될 때 상기 컴퓨터로 하여금, 복수의 프레임들의 시퀀스를 포함하는 비디오 영상을 수신 하는 과정; 상기 시퀀스의 제1 키프레임에 대해, 제1 자세추정 모델을 이용하여, 상기 제1 키프레임 내에서 객 체가 포함된 영역을 크롭핑한 영상으로부터 상기 객체의 제1 관절정보를 추정하는 과정; 상기 제1 키프레임에 후속하는 키프레임이 아닌 후속 프레임들 각각에 대해, 제2 자세추정 모델을 이용하여, 상기 후속 프레임 내에 서 상기 객체가 포함된 영역을 크롭핑한 영상과 상기 제1 관절정보로부터 상기 객체의 제1 관절정보의 변위를 추정하는 과정; 및 상기 제1 관절정보의 변위와 상기 제1 관절정보로부터 상기 후속 프레임 내의 상기 객체의 제2 관절정보를 추정하는 과정을 실행하도록 하는 것을 특징으로 하는 컴퓨터로 읽을 수 있는 기록매체를 제공 한다."}
{"patent_id": "10-2023-0087076", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 실시예에 따르면, 자세 추정을 위해 요구되는 연산량을 대폭 감소시켜, 비용 절감의 효과를 얻을 수 있다. 본 개시의 실시예에 따르면, 현재 프레임의 영상 정보뿐만 아니라 이전 프레임의 관절정보를 함께 이용함으로써, 자세 추정의 정확도를 향상시킬 수 있다."}
{"patent_id": "10-2023-0087076", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 일부 실시예들을 예시적인 도면을 이용해 상세하게 설명한다. 각 도면의 구성 요소들에 참조 부호를 부가함에 있어서, 동일한 구성 요소들에 대해서는 비록 다른 도면 상에 표시되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 개시를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략한다. 본 개시에 따른 실시예의 구성요소를 설명하는 데 있어서, 제1, 제2, i), ii), a), b) 등의 부호를 사용할 수 있다. 이러한 부호는 그 구성요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 부호에 의해 해당 구성요소의 본질 또는 차례나 순서 등이 한정되지 않는다. 명세서에서 어떤 부분이 어떤 구성요소를 '포함' 또는 '구비'한 다고 할 때, 이는 명시적으로 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한 명세서에 기재된 '부', '모듈' 등의 용어는 적어도 하나의 기능이나 동 작을 처리하는 단위를 의미하며, 이는 하드웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 첨부된 도면과 함께 이하에 개시될 발명의 설명은 본 발명의 예시적인 실시 형태를 설명하고자 하는 것이며, 본 발명이 실시될 수 있는 유일한 실시 형태를 나타내고자 하는 것이 아니다. 본 명세서에서, 용어 '제1 모델'은 '제1 자세추정 모델'과 혼용될 수 있고, 용어 '제2 모델'은 '제2 자세추정 모델'과 혼용될 수 있다. 용어 '자세'는 '키포인트(keypoint) 정보', '관절(joint) 정보' 또는 '골격 (skeleton) 정보'와 혼용될 수 있다. 본 명세서에서 언급되는 '객체'는 사람, 동물 등을 포함할 수 있다. 비디오 영상은 일반적으로 30 fps(frame per second), 25 fps 등과 같이 초당 약 30장 내외의 이미지들로 구성 될 수 있다. 하지만, 사람의 움직임은 1/30 초 동안 크게 변하지 않는 시간적 연속성(temporal continuity)을 갖는다. 따라서 이전 프레임의 관절정보와 현재 프레임의 관절정보의 변화량은 적다. 이러한 점에 착안하여 본 개시는 이전 프레임의 관절정보와 현재 프레임의 영상 정보를 입력으로 하여 이전 프 레임의 관절정보와 현재 프레임의 관절정보의 차이를 학습하는 네트워크를 제시한다.구체적으로, 비디오 영상 내 등장하는 객체의 자세를 효율적으로 추정하기 위해, 키프레임으로 선정된 프레임에 대해서는 제1 모델을 이용하여 정밀한 관절정보를 추정하고, 키프레임이 아닌 프레임에 대해서는 제2 모델을 이 용하여 관절정보를 추정한다. 여기서, 제2 모델은 제1 모델에 비해서 상대적으로 가벼운 모델 즉, 연산에 필요 한 자원이 상대적으로 적게 요구되는 모델을 말한다. 이때 제2 모델이 일반적으로 학습된 경우, 정밀한 관절정 보를 얻기 어렵다. 따라서, 이전 프레임에서 제1 모델을 이용하여 추론된 이전 프레임의 관절정보를 제2 모델의 추가적인 입력으로 하여, 좀 더 정밀한 관절정보를 추정할 수 있도록 한다. 본 개시의 실시예에 따르면, 모든 프레임들에 대해 무거운 네트워크를 사용하여 추론하는 것이 아니라, 상대적 으로 무거운 제1 모델과 상대적으로 가벼운 제2 모델을 혼용해서 사용함으로써, 연산량을 감소시키는 효과를 얻 을 수 있다. 도 1은 본 개시의 일 실시예에 따른 비디오 영상 내 등장하는 객체의 자세를 추정하기 위해 프레임 별로 서로 다른 모델을 이용하는 방식을 설명하기 위한 예시도이다. 먼저, 본 개시의 자세 추정 방법은 하향식 접근법을 따른다. 즉, 프레임 별 이미지에서 객체 검출기와 추적기 (tracker)를 이용하여 비디오 영상 내 등장하는 객체의 위치(예를 들어, 바운딩 박스 정보 등)와 객체 아이디 (ID)를 알 수 있다. 객체의 위치를 기준으로 객체를 포함하는 영역을 잘라낸(cropped) 영상을 제1 모델 또는 제 2 모델의 입력으로 사용한다. 도 1을 참조하면, 처리할 프레임이 키프레임인지 여부에 따라 서로 다른 자세추정 모델을 이용하는 방식이 도시 되어 있다. 여기서, 키프레임은 객체의 관절정보를 추정하기 위해 제1 모델을 이용하도록 설정된 프레임을 말한 다. 예를 들어, 현재 프레임이 키프레임인 경우, 제1 모델을 이용하여 현재 프레임의 영상 정보로부터 객체의 관절 정보를 예측한다. 예를 들어, 현재 프레임이 키프레임이 아닌 경우, 제2 모델을 이용하여 이전 키프레임에서 예측된 관절정보와 현재 프레임의 영상 정보로부터 객체의 관절정보의 변위 또는 잔차 키포인트(residual key point)를 획득한다. 이후, 획득된 관절정보의 변위와 이전 키프레임에서 예측된 관절정보를 이용하여 현재 프레임 내 객체의 관절정 보를 예측한다. 비디오 영상의 처음 프레임만 제1 모델을 사용하면 시간이 흘러가면서 오류가 누적되어 미래 관절정보에 전파될 수 있으므로, 일정 프레임 간격 또는 일정 시간 간격으로 키프레임을 설정한다. 예를 들어, 비디오 영상에서 처 음 프레임부터 시작하여 일정 간격마다 위치하는 프레임들을 키프레임들로 설정할 수 있다. 일정 프레임 간격 또는 일정 시간 간격은 비디오 영상 내 객체의 움직임의 변화 정도에 따라 달리 결정될 수 있다. 이하 제1 모델 및 제2 모델에 대해 상세히 설명한다. 제1 모델은 객체의 관절정보를 나타내는 주석들(annotations)이 포함된 영상들로 구성되는 훈련 데이터셋을 이 용하여, 입력된 영상에 대응하는 객체의 관절정보를 예측하도록 훈련된 신경망 모델이다. 제1 모델은 일반적으 로 활용되는 자세 추정 네트워크를 이용하여 구현될 수 있다. 예를 들어, 비전 트랜스포머(ViT, Vision Transformer) 기반의 HRFormer(High-Resolution Transformer), ViTPose(Simple Vision Transformer Baselines for Human Pose Estimation) 등을 이용할 수 있으나, 이에 한정되지 않는다. 도 2는 본 개시의 일 실시예에 따른 제2 모델의 구조를 설명하기 위한 예시도이다. 제2 모델은 이전 키프레임에서 예측된 관절정보와 현재 프레임의 영상 정보로부터 객체의 관절정보의 변위를 예 측하도록 훈련된 신경망 모델이다. 도 2를 참조하면, 먼저, 현재 프레임의 영상과 이전 키프레임에서 예측된 관절정보 즉, 2D 자세를 비전 트랜스 포머의 입력 토큰으로 사용하기 위한 임베딩이 수행된다. 현재 프레임의 영상에 대해 패치 임베딩(patch embedding)을 수행하여 이미지 토큰(image token)을 획득한다. 예를 들어, 현재 프레임의 영상은 16x16 크기의 패치들로 분할되고 각 패치는 1x768 크기로 임베딩된다. 패치들 의 개수가 576개인 경우, 이미지 토큰은 총 576개가 된다. 이전 키프레임의 관절정보에 대해 2D 포즈 임베딩(2D pose embedding)을 수행하여 포즈 토큰(pose token)을 획 득한다. 예를 들어, 이전 키프레임의 관절정보는 제1 모델의 결과 값이므로 관절 별로 예측된 x 좌표 값, y 좌 표 값 및 신뢰도(confidence) 값을 포함할 수 있다. 또한, 사람의 관절 정보는 총 N개(예를 들어, 17개, 21개등)로 표현될 수 있으므로, 이전 키프레임의 관절정보를 N*3의 크기를 갖는 벡터로 표현할 수 있다. 이 벡터를 이미지 토큰과 동일한 1x768 크기로 투영(projection)한 후 비전 트랜스포머의 입력 토큰으로 활용한다 또한, 이미지 토큰과 포즈 토큰을 구별하기 위해, 각 토큰 앞에 학습 가능한 클래스 토큰(Learnable Class Token)을 각각 입력으로 추가한 후, 이미지 토큰 및 포즈 토큰을 비전 트랜스포머의 입력 토큰으로 사용한다. 비전 트랜스포머의 결과 토큰에 대해 전역평균풀링(GAP, Global Average Pooling)을 수행하고, 피드포워드 네트 워크(FFN, Feed Forward Network)을 거쳐 이전 프레임에서 관절 위치와 현재 프레임에서 관절 위치의 차이인 객 체의 관절정보의 변위의 예측값이 출력된다. 즉, GAP 레이어와 FFN으로 구성된 디코더(decoder)를 통해 관절정 보의 변위가 예측될 수 있다. 예측된 관절정보의 변위와 이전 키프레임에서 예측된 관절정보를 이용하여 현재 프레임에서 객체의 관절정보를 추정하게 된다. 즉, 이전 키프레임에서 제1 모델을 이용하여 예측된 관절정보를 제2 모델을 이용하여 예측된 관 절정보의 변위만큼 보정하여 현재 프레임에서 객체의 관절정보를 추정할 수 있다 도 3은 본 개시의 일 실시예에 따른 제2 모델을 훈련하는 방법을 설명하기 위한 예시도이다. 도 3을 참조하면, 먼저, 복수의 (현재 프레임의 영상, 이전 프레임의 관절정보, 현재 프레임의 GT 관절정보)로 이루어진 훈련 데이터셋을 준비한다. 여기서, 이전 프레임의 관절정보는 이전 프레임에서 객체가 포함된 영상의 GT 관절정보를 활용하거나, 이전 키프레임에서 객체가 포함된 영상으로부터 제1 모델을 이용하여 예측된 관절정 보를 활용할 수 있다. 일반적으로 둘 다 훈련에 활용하는 것이 제2 모델의 성능 향상에 도움이 된다. 제2 모델은 현재 프레임의 영상 정보와 이전 프레임의 관절정보를 둘 다 입력으로 사용한다. 제2 모델의 출력인 예측된 관절정보의 변위는 (관절 개수*2)의 크기를 갖는 벡터이며, 이 벡터는 이전 프레임에 서 관절 위치와 현재 프레임에서 관절 위치의 차이를 표현한다. 다시 말해, 각 관절 별로 이전 프레임의 관절 위치 대비 현재 프레임의 관절 위치가 x축 및 y축으로 각각 얼마만큼 이동해야 하는지 변위 정보가 담겨 있다. 예측된 관절정보의 변위와 GT 관절정보의 변위를 비교하여 손실(Loss)을 산출하고, 산출된 손실을 최소화하도록 제2 모델의 파라미터들을 역전파 방식으로 갱신하면서 제2 모델을 훈련할 수 있다. 여기서, GT 관절정보의 변위 는 현재 프레임의 GT 관절정보와 이전 프레임의 관절정보 간의 차이로 구할 수 있다. 손실(Loss)은 회귀 손실 함수 중 하나를 사용할 수 있다. 예를 들어, MSE(Mean Squared Error), RMSE(Root Mean Square Error), RMSLE(Rooted Mean Squared Logarithmic Error), MAE(Mean Absolute Error), MASE(Mean Absolute Scaled Error) 등을 사용할 수 있으나, 이에 한정되지 않는다. 도 4는 본 개시의 일 실시예에 따른 비디오 영상 내 객체의 자세를 추정하기 위한 방법의 순서도이다. 도 4를 참조하면, 복수의 프레임들의 시퀀스를 포함하는 비디오 영상을 수신한다(S410). 예를 들어, 카메라를 통해 실시간 촬영되고 있는 비디오 영상을 내외부 통신 인터페이스를 통해 수신할 수 있다. 또는, 생성되어 메 모리 등과 같은 저장장치에 저장된 비디오 영상을 수신할 수도 있다. 수신된 시퀀스의 제1 키프레임에 대해, 제1 자세추정 모델을 이용하여, 제1 키프레임 내에서 객체가 포함된 영 역을 크롭핑한 영상으로부터 상기 객체의 제1 관절정보를 추정한다(S420). 예를 들어, 객체 검출기와 추적기를 이용하여 제1 키프레임 내 등장하는 객체의 위치를 획득하고, 획득된 객체의 위치를 기준으로 객체를 포함하는 영역을 잘라낸(cropped) 영상을 제1 자세추정 모델에 입력할 수 있다. 제1 키프레임에 후속하는 키프레임이 아닌 후속 프레임들 각각에 대해, 제2 자세추정 모델을 이용하여, 후속 프 레임 내에서 객체가 포함된 영역을 크롭핑한 영상과 S420 과정에서 추정된 제1 관절정보로부터 객체의 제1 관절 정보의 변위를 추정한다(S430). S430 과정에서 추정된 제1 관절정보의 변위와 S420 과정에서 추정된 제1 관절정보로부터 후속 프레임 내의 객체 의 제2 관절정보를 추정한다(S440). 전술한 과정을 통해 비디오 영상에 포함된 모든 프레임들에 대해 프레임 별 추정된 관절정보를 이용하여 객체의 행동 인식, 제스처 인식 등을 할 수 있다. 본 발명에 따른 장치 또는 방법의 각 구성요소는 하드웨어 또는 소프트웨어로 구현되거나, 하드웨어 및 소프트 웨어의 결합으로 구현될 수 있다. 또한, 각 구성요소의 기능이 소프트웨어로 구현되고 마이크로프로세서가 각 구성요소에 대응하는 소프트웨어의 기능을 실행하도록 구현될 수도 있다.본 명세서에 설명되는 시스템 및 기법들의 다양한 구현예들은, 디지털 전자 회로, 집적회로, FPGA(field programmable gate array), ASIC(application specific integrated circuit), 컴퓨터 하드웨어, 펌웨어, 소프 트웨어, 및/또는 이들의 조합으로 실현될 수 있다. 이러한 다양한 구현예들은 프로그래밍가능 시스템 상에서 실 행 가능한 하나 이상의 컴퓨터 프로그램들로 구현되는 것을 포함할 수 있다. 프로그래밍가능 시스템은, 저장 시 스템, 적어도 하나의 입력 디바이스, 그리고 적어도 하나의 출력 디바이스로부터 데이터 및 명령들을 수신하고 이들에게 데이터 및 명령들을 전송하도록 결합되는 적어도 하나의 프로그래밍가능 프로세서(이것은 특수 목적 프로세서일 수 있거나 혹은 범용 프로세서일 수 있음)를 포함한다. 컴퓨터 프로그램들(이것은 또한 프로그램들, 소프트웨어, 소프트웨어 애플리케이션들 혹은 코드로서 알려져 있음)은 프로그래밍가능 프로세서에 대한 명령어 들을 포함하며 \"컴퓨터가 읽을 수 있는 기록매체\"에 저장된다. 컴퓨터가 읽을 수 있는 기록매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기 록장치를 포함한다. 이러한 컴퓨터가 읽을 수 있는 기록매체는 ROM, CD-ROM, 자기 테이프, 플로피디스크, 메모 리 카드, 하드 디스크, 광자기 디스크, 스토리지 디바이스 등의 비휘발성(non-volatile) 또는 비일시적인(non- transitory) 매체일 수 있으며, 또한 데이터 전송 매체(data transmission medium)와 같은 일시적인 (transitory) 매체를 더 포함할 수도 있다. 또한, 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수도 있다. 본 명세서의 순서도에서는 각 과정들을 순차적으로 실행하는 것으로 기재하고 있으나, 이는 본 개시의 일 실시 예의 기술 사상을 예시적으로 설명한 것에 불과한 것이다. 다시 말해, 본 개시의 일 실시예가 속하는 기술 분야 에서 통상의 지식을 가진 자라면 본 개시의 일 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 본 명세서 의 순서도에 기재된 순서를 변경하여 실행하거나 각 과정들 중 하나 이상의 과정을 병렬적으로 실행하는 것으로 다양하게 수정 및 변형하여 적용 가능할 것이므로, 본 명세서의 순서도는 시계열적인 순서로 한정되는 것은 아 니다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4"}
{"patent_id": "10-2023-0087076", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 비디오 영상 내 등장하는 객체의 자세를 추정하기 위해 프레임 별로 서로 다른 모델을 이용하는 방식을 설명하기 위한 예시도이다. 도 2는 본 개시의 일 실시예에 따른 제2 모델의 구조를 설명하기 위한 예시도이다. 도 3은 본 개시의 일 실시예에 따른 제2 모델을 훈련하는 방법을 설명하기 위한 예시도이다. 도 4는 본 개시의 일 실시예에 따른 비디오 영상 내 객체의 자세를 추정하기 위한 방법의 순서도이다."}
