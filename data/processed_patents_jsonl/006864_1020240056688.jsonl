{"patent_id": "10-2024-0056688", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0035425", "출원번호": "10-2024-0056688", "발명의 명칭": "번역 모델을 학습시키는 서버 장치, 학습된 번역 모델을 사용하는 전자 장치 및 그 방법들", "출원인": "삼성전자주식회사", "발명자": "정정호"}}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "서버 장치에 있어서, 제1 언어로 발화된 실제 음성, 상기 실제 음성에 대응되는 제1 텍스트, 상기 제1 텍스트에 대응되는 제2 언어인제2 텍스트, 및 번역 모델을 저장하는 메모리; 및프로세서;를 포함하며,상기 프로세서는,상기 제1 텍스트에 대응되는 합성 음성을 생성하고,상기 실제 음성, 상기 제1 텍스트, 상기 합성 음성 및 상기 제2 텍스트를 이용하여 상기 번역 모델을 학습하는,서버 장치."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 번역 모델은, 음성의 특징 정보를 포함하는 VQ 코드북(Vector Quantization codebook)을 포함하며,상기 프로세서는, 상기 실제 음성에 대응되는 특징 정보와, 상기 합성 음성에 대응되는 특징 정보에 기초하여,상기 실제 음성 및 상기 합성 음성의 특징 차이가 기 설정된 임계치 미만이 되도록 조정된 특징 정보를 생성하여, 상기 VQ 코드북(codebook)을 갱신하는, 서버 장치."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 메모리는, 상기 실제 음성 및 상기 합성 음성의 차이를 구별하기 위한 판별자 모듈(discriminator module)을 저장하며,상기 프로세서는,상기 실제 음성을 입력 받은 상기 번역 모델의 제1 출력값과, 상기 합성 음성을 입력받은 상기 번역 모델의 제2출력값을 상기 판별자 모듈에 입력하고, 상기 판별자 모듈의 출력값이 상기 임계치 미만이 될 때까지, 상기 판별자 모듈의 출력값과 상기 임계치를 비교하여 상기 번역 모델에 포함된 상기 VQ 코드북(codebook)을 재 갱신하는, 서버 장치."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 메모리는, 상기 특징 정보를 상기 제1 언어의 텍스트 별로 구분하여 학습하기 위한 정규화 모듈(regularizer module)을 저장하며, 상기 프로세서는, 상기 제1 언어의 각 텍스트 별로 획득되는 상기 제1 출력값과 상기 제2 출력값을 상기 정규화 모듈에 입력하고,상기 정규화 모듈의 출력 값에 기초하여 각 텍스트 별로 서로 다른 특징 정보를 가지도록 상기 VQ 코드북(codebook)을 갱신하는, 서버 장치."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2025-0035425-3-제4항에 있어서,상기 메모리는, 실제 음성 신호를 샘플링하는 제1 서브 샘플러;상기 제1 서브 샘플러에서 샘플링된 실제 음성을 인코딩하는 제1 인코더;상기 제1 텍스트를 TTS 모듈을 이용하여 변환한 합성 음성 신호를 샘플링하는 제2 서브 샘플러;상기 제2 서브 샘플러에서 샘플링된 합성 음성을 인코딩하는 제2 인코더;를 저장하며,상기 프로세서는, 상기 제1 인코더에서 인코딩된 상기 실제 음성에 대한 상기 번역 모델의 출력값과, 상기 제2 인코더에서 인코딩된 상기 합성 음성에 대한 상기 번역 모델의 출력값 사이의 유사도가 기 설정된 임계치 이상이 될 때까지 상기번역 모델을 반복적으로 학습시키는, 서버 장치."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 번역 모델은, 상기 VQ 코드북(codebook)에 포함된 특징 정보에 기초하여 음성을 인코딩하는 공유 인코더; 및상기 사전 데이터에 기초하여, 상기 공유 인코더에서 출력되는 특징 벡터를 디코딩하여 제2 언어의 텍스트를 추출하는 디코더;를 더 포함하며,상기 프로세서는, 상기 VQ 코드북(codebook)의 갱신이 완료된 상태에서, 상기 실제 음성 및 상기 합성 음성을 이용하여 상기 번역모델을 학습하는, 서버 장치."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,통신부를 더 포함하며,상기 프로세서는,상기 VQ 코드북(codebook)을 포함하는 번역 모델이 설치된 적어도 하나의 전자 장치로부터 인덱스 정보가 상기통신부를 통해 수신되면, 상기 메모리에 저장된 상기 VQ 코드북(codebook)으로부터 상기 인덱스 정보에 대응되는 특징 정보를 추출하고,상기 공유 인코더 및 상기 디코더를 이용하여 추출된 상기 특징 정보에 대응되는 제2 언어의 텍스트를생성하여, 생성된 상기 텍스트를 상기 통신부를 통해 상기 적어도 하나의 전자 장치로 전송하는, 서버 장치."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "전자 장치에 있어서, 마이크;통신부;디스플레이;실제 음성 및 합성 음성에 기초하여 학습된 VQ 코드북(codebook)이 저장된 메모리; 및프로세서;를 포함하며, 상기 프로세서는,상기 마이크를 통해 제1 언어의 음성 신호가 입력되면, 상기 VQ 코드북(codebook)에 기록된 특징 정보들 중에서상기 음성 신호에 대응되는 특징 정보의 인덱스 정보를 추출하여, 상기 인덱스 정보를 상기 통신부를 통해 서버공개특허 10-2025-0035425-4-장치로 전송하고, 상기 인덱스 정보에 대응되는 제2 언어의 텍스트에 대한 정보가 상기 서버 장치로부터 전송되면, 상기 제2 언어의 텍스트를 표시하도록 상기 디스플레이를 제어하는, 전자 장치."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "번역 모델을 학습하기 위한 서버 장치의 학습 방법에 있어서, 제1 언어로 발화된 실제 음성에 대응되는 제1 텍스트에 기초하여 합성 음성을 생성하는 단계; 및상기 실제 음성, 상기 제1 텍스트, 상기 합성 음성과, 상기 제1 텍스트에 대응되는 타겟 언어인 제2 텍스트를이용하여 번역 모델을 학습시키는 단계;를 포함하는, 학습 방법."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 번역 모델은, 음성의 특징 정보를 포함하는 VQ 코드북(Vector Quantization codebook)을 포함하며,상기 학습시키는 단계는,상기 실제 음성에 대응되는 특징 정보와, 상기 합성 음성에 대응되는 특징 정보에 기초하여, 상기 실제 음성 및상기 합성 음성의 특징 차이가 기 설정된 임계치 미만이 되도록 조정된 특징 정보를 생성하여, 상기 VQ 코드북(codebook)을 갱신하는 단계;를 포함하는, 학습 방법."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 학습시키는 단계는, 상기 실제 음성을 입력받은 상기 번역 모델의 제1 출력값과, 상기 합성 음성을 입력받은 상기 번역 모델의 제2출력값을 각각 획득하는 단계;상기 실제 음성 및 상기 합성 음성의 차이를 구별하기 위한 판별자 모듈(discriminator module)로 상기 제1 출력값 및 상기 제2 출력값을 입력하는 단계; 상기 판별자 모듈의 출력값이 상기 임계치 미만이 될 때까지, 상기 판별자 모듈의 출력값과 상기 임계치를 비교하여 상기 번역 모델에 포함된 상기 VQ 코드북(codebook)을 재 갱신하는 단계;를 포함하는, 학습 방법."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 학습시키는 단계는,상기 제1 언어의 각 텍스트 별로 획득되는 상기 제1 출력값과 상기 제2 출력값을, 정규화 모듈에 입력하고, 상기 정규화 모듈의 출력 값에 기초하여 각 텍스트 별로 서로 다른 특징 정보를 가지도록 상기 VQ 코드북(codebook)을 갱신하는 단계;를 더 포함하는, 학습 방법."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 학습시키는 단계는, 실제 음성 신호를 샘플링하는 단계;상기 샘플링된 실제 음성을 제1 인코더를 이용하여 인코딩하는 단계;상기 제1 텍스트를 TTS 모듈을 이용하여 변환한 상기 합성 음성을 샘플링하는 단계;샘플링된 상기 합성 음성을 제2 인코더를 이용하여 인코딩하는 단계 및 상기 제1 인코더에서 인코딩된 상기 실제 음성에 대한 상기 번역 모델의 출력값과, 상기 제2 인코더에서 인코딩된 상기 합성 음성에 대한 상기 번역 모델의 출력값 사이의 유사도가 기 설정된 임계치 이상이 될 때까지 상기공개특허 10-2025-0035425-5-번역 모델을 반복적으로 학습시키는 단계;를 포함하는, 학습 방법."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 번역 모델은, 상기 VQ 코드북(codebook)에 포함된 특징 정보에 기초하여 음성을 인코딩하는 공유 인코더; 및상기 사전 데이터에 기초하여, 상기 공유 인코더에서 출력되는 특징 벡터를 디코딩하여 제2 언어의 텍스트를 추출하는 디코더;를 더 포함하며,상기 학습시키는 단계는, 상기 VQ 코드북(codebook)의 갱신이 완료된 상태에서, 상기 실제 음성 및 상기 합성 음성을 이용하여 상기 번역모델을 학습하는 단계;를 포함하는, 학습 방법."}
{"patent_id": "10-2024-0056688", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "번역 모델을 학습시키는 서버 장치의 학습 방법을 수행하기 위한 프로그램이 저장된 비일시적 판독 가능 기록매체에 있어서,상기 학습 방법은, 제1 언어로 발화된 실제 음성에 대응되는 제1 텍스트에 기초하여 합성 음성을 생성하는 단계; 및상기 실제 음성, 상기 제1 텍스트, 상기 합성 음성과, 상기 제1 텍스트에 대응되는 타겟 언어인 제2 텍스트를이용하여 번역 모델을 학습시키는 단계;를 포함하는, 비일시적 판독 가능 기록 매체."}
{"patent_id": "10-2024-0056688", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "번역 모델을 학습시키는 서버 장치, 학습된 번역 모델을 사용하는 전자 장치 및 그 방법들이 개시된다. 본 전자 장치는 제1 언어로 발화된 실제 음성, 상기 실제 음성에 대응되는 제1 텍스트, 상기 제1 텍스트에 대응되는 타겟 언어인 제2 텍스트, 통신부 및 번역 모델을 저장하는 메모리 및 프로세서를 포함하고, 프로세서는, 상기 제1 텍 스트에 대응되는 합성 음성을 생성하고, 상기 실제 음성, 상기 제1 텍스트, 상기 합성 음성 및 상기 제2 텍스트 를 이용하여 상기 번역 모델을 학습할 수 있다."}
{"patent_id": "10-2024-0056688", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은, 이종 언어 간 번역을 수행하는 번역 모델을 학습시키는 서버 장치와 그 번역 모델을 사용하는 전자 장치 및 그 방법들에 대한 것이다."}
{"patent_id": "10-2024-0056688", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "다양한 언어를 가진 사용자들의 소통을 위하여, 서로 다른 언어 간 번역을 수행하는 번역 모델의 사용이 늘고 있다. 특히, 두 개 이상의 언어들에 대한 번역을 수행하는 다중 언어 모델에 대한 관심도 커지고 있다. 종래의 번역 모델은 단계별 방법(cascaded 방법)을 이용하였다. cascaded 방법에 의하면 2개의 번역 모델이 개 입되기 때문에, 오류 전파 (error propagation)문제와 지연 문제(latency)가 발생하였다. 따라서, 번역의 오류 와 정확성의 문제를 해결하기 위해 end-to-end 모델의 필요성이 대두되었다. 다만, end-to-end 모델을 사용하는 경우에도 실제 발화 음성의 데이터와 소스 언어의 텍스트 데이터의 쌍이 희 소하다는 단점이 있었다. 이에 따라 번역 모델에 대한 학습이 어려워, 이에 대한 해결책이 필요한 실정이다."}
{"patent_id": "10-2024-0056688", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 적어도 하나의 실시 예에 따른 서버 장치는, 프로세서, 메모리를 포함한다. 상기 프로세서는, 제1 언 어로 발화된 실제 음성의 텍스트에 대응되는 합성 음성을 생성하고, 제1 언어로 발화된 실제 음성, 상기 실제 음성에 대응되는 제1 텍스트, 제1 언어로 발화된 실제 음성에 대응되는 제1 텍스트에 대응되는 합성 음성 및 제 1 텍스트에 대응되는 타겟 언어인 제2 텍스트를 이용하여 상기 번역 모델을 학습할 수 있다. 본 개시의 또 다른 실시예에 따른, 전자 장치는, 마이크, 통신부, 디스플레이, 실제 음성 및 합성 음성에 기초 하여 학습된 VQ 코드북(codebook)이 저장된 메모리, 및 프로세서를 포함한다. 상기 프로세서는, 상기 마이크를통해 제1 언어의 음성 신호가 입력되면, 상기 VQ 코드북(codebook)에 기록된 특징 정보들 중에서 상기 음성 신 호에 대응되는 특징 정보의 인덱스 정보를 추출하여, 상기 인덱스 정보를 상기 통신부를 통해 서버 장치로 전송 하고, 상기 인덱스 정보에 대응되는 제2 언어의 텍스트에 대한 정보가 상기 서버 장치로부터 전송되면, 상기 제 2 언어의 텍스트를 표시하도록 상기 디스플레이를 제어할 수 있다. 본 개시의 또 다른 실시예에 따른, 서버 장치의 번역 모델 학습 방법은, 제1 언어로 발화된 실제 음성에 대응되 는 제1 텍스트에 기초하여 합성 음성을 생성하는 단계, 및 상기 실제 음성, 상기 제1 텍스트, 상기 합성 음성과, 상기 제1 텍스트에 대응되는 타겟 언어인 제2 텍스트를 이용하여 번역 모델을 학습시키는 단계를 포함 할 수 있다."}
{"patent_id": "10-2024-0056688", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 실시 예들은 다양한 변환을 가할 수 있고 여러 가지 실시 예를 가질 수 있는바, 특정 실시 예들을 도면에 예 시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나 이는 특정한 실시 형태에 대해 범위를 한정하려는 것 이 아니며, 본 개시의 실시 예의 다양한 변경(modifications), 균등물(equivalents), 및/또는 대체물 (alternatives)을 포함하는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유 사한 참조 부호가 사용될 수 있다. 본 개시를 설명함에 있어서, 관련된 공지 기능 혹은 구성에 대한 구체적인 설명이 본 개시의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그에 대한 상세한 설명은 생략한다. 덧붙여, 하기 실시 예는 여러 가지 다른 형태로 변형될 수 있으며, 본 개시의 기술적 사상의 범위가 하기 실시 예에 한정되는 것은 아니다. 오히려, 이들 실시 예는 본 개시를 더욱 충실하고 완전하게 하고, 당업자에게 본 개시의 기술적 사상을 완전하게 전달하기 위하여 제공되는 것이다. 본 개시에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 권리범위를 한정하려는 의도 가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. 본 개시에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\"등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 개시에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중 요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요 소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"직접 연결되어\" 있다거나 \"직 접 접속되어\" 있다고 언급된 때에는, 상기 어떤 구성요소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 개시에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for),\" \"~하는 능력을 가지는(having the capacity to),\" \"~하도록 설계된(designed to),\" \"~하도 록 변경된(adapted to),\" \"~하도록 만들어진(made to),\" 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장치가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행하도록 구성된(또는 설정된) 프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메모리 장치에 저장된 하나 이상의 소프트 웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 실시 예에 있어서 '모듈' 혹은 '부'는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 '모듈' 혹은 복수의 '부'는 특정 한 하드웨어로 구현될 필요가 있는 '모듈' 혹은 '부'를 제외하고는 적어도 하나의 모듈로 일체화되어 적어도 하 나의 프로세서로 구현될 수 있다. 한편, 도면에서의 다양한 요소와 영역은 개략적으로 그려진 것이다. 따라서, 본 발명의 기술적 사상은 첨부한 도면에 그려진 상대적인 크기나 간격에 의해 제한되지 않는다. 이하에서는 첨부한 도면을 참고하여 본 개시에 따른 실시 예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 도 1은 본 개시의 적어도 하나의 실시 예에 따른 번역 방법을 설명하기 위한 도면이다. 도 1에서는 서버 장치 및 전자 장치가 서로 통신을 수행하는 상태를 나타낸다. 전자 장치는 번역 모델을 이용하여 다른 언어간 번역을 수행할 수 있다. 도 1에서는 사용자가 한국어(즉, 소스 언어)로 발화하면, 그 내용을 영어(즉, 타겟 언어)로 번역하는 경우를 나타낸다. 구체적으로는, 도 1에서 사용자가 한국어로 \"안녕하세요\"라는 음성(30-1)을 발화하면, 전자 장치가 마 이크를 통해 발화 음성을 입력 받고, 타겟 언어인 영어의 번역 텍스트인 \"Hello\"(30-2)를 전자 장치의 디 스플레이에 표시하는 실시예를 나타낸다. 실시예에서는 사용자가 발화한 소스 언어가 한국어이고, 타겟 언어가 영어인 경우를 도시하였으나, 소스 언 어 및 타겟 언어는 다양하게 달라질 수 있다. 사용자의 발화한 언어와 사용자의 발화한 언어가 번역된 언어, 즉 소스 언어와 타겟 언어는 특정 유형의 언어에 한정되지 않는다. 일 예로, 사용자의 발화된 언어, 즉 소스 언어가 러시아어이며, 타겟 언어가 독일어인 경우라도 무방하다. 소스 언어와 타겟 언어의 종류는 사용 자의 설정에 따라 달라질 수도 있다. 다만 번역 모델이 제대로 번역을 하기 위해서는 다양한 언어들 간의 번역 에 대한 학습이 이루어져야 하며, 이를 위하여 본 개시의 다양한 실시 예들에서는 소스 언어에 대한 실제 음성 뿐만 아니라 합성 음성까지 함께 사용하여 학습 데이터의 양을 늘리게 된다. 설명의 편의상, 본 개시에서는 소스 언어를 제1 언어, 타겟 언어를 제2 언어라 기재할 수도 있다. 또한, 도 1에서 전자 장치는 휴대폰으로 표시하였으나, 이는 설명의 편의를 위한 것이다. 따라서 전자 장 치는 휴대폰에 한정되는 것은 아니며, PC, 랩탑 PC, 태블릿 PC, TV, 키오스크, 무선 스피커, 로봇 등 다양 한 장치로 구현될 수 있다. 전자 장치는 사용자가 발화한 제1 언어의 음성이 마이크를 통해 입력되면, 번역 모델을 이용하여 제2 언어의 텍스트를 획득할 수 있다. 전자 장치는 획득된 텍스트를 도 1과 같이 그대로 디스플레이할 수도 있 고, 전자 음성 형태로 스피커를 통해 출력할 수도 있다. 본 개시의 다양한 실시 예에 따르면, 번역 모델은 실제 음성 뿐만 아니라 합성 음성까지 함께 학습 데이터로 사 용하는 방식으로 학습된 모델이 될 수 있다. 구체적인 학습 방법에 대해서는 후술하는 부분에서 구체적으로 설 명한다. 한편, 학습된 번역 모델은 전자 장치에 탑재될 수도 있고, 외부 장치에 탑재될 수도 있다. 또는, 전자 장 치에 번역 모델을 구성하는 소프트웨어 모듈 중 일부가 탑재되고, 외부 장치에 또 다른 소프트웨어 모듈이 탑재될 수도 있다. 도 1에서는, 서버 장치에 번역 모델이 탑재된 경우를 나타낸다. 일 예로, 사용자는 전자 장치를 근처에 둔 상태로 \"안녕하세요\"(30-1)라는 문장을 발화할 수 있다. 전 자 장치는 사용자가 발화한 음성 데이터를 서버 장치로 전송한다. 서버 장치는 수신된 음성 데 이터를 번역 모델에 입력하여, 제2 언어의 텍스트를 획득할 수 있다. 타겟 언어가 영어로 설정되어 있다면, 서버 장치는 \"Hello\"(30-2)라는 텍스트를 전자 장치로 전송할 수 있다. 타겟 언어가 프랑스어로 설정된 경우에는, 서버 장치는 \"Bonjour\"라는 텍스트를 전송할 수 있다. 번역 모델이 다중 언어 모델로 구현된 경우, 제1 언어가 한국어가 아닌 다른 나라의 언어로 입력되더라도, 제2 언어가 프랑스로 설정되어 있다면, 서버 장치는 \"Bonjour\"라는 텍스트의 인덱스를 전송할 수 있다. 한편, 전자 장치가 번역 모델의 소프트웨어 모듈 중 적어도 일부를 포함하고 있는 경우에는, 전자 장치 는 사용자가 발화한 음성 데이터 자체를 서버 장치로 전송하지 않고, 소프트웨어 모듈에 의해 처리된 결과값을 전송할 수도 있다. 일 예로, 실제 음성 및 합성 음성으로 학습된 VQ 코드북이 저장된 경우라면, 전자 장치는 사용자가 발화한 음성에 대응되는 특징 벡터를 VQ 코드북에서 검색하고, 검색된 특징 벡터에 대한 인덱스를 서버 장치로 전송하여 줄 수 있다. 서버 장치는 수신된 인덱스에 대응되는 제2 언어의 텍스 트를 전송하여 줄 수 있고, 전자 장치는 전송된 제2 언어의 텍스트를 표시할 수 있다. VQ 코드북(Vector Quantization Codebook)이란, 사용자가 발화한 음성 신호 내에 포함된 복수의 음성 세그먼트 의 특징 벡터 추출 단계 이후에 사용되는 것으로 벡터 양자화(vector quantization)된 특징 정보(즉, 음성 특징 벡터)의 데이터베이스가 될 수 있다. 한편, 다른 예와 같이, 전자 장치가 자체적으로 번역 모델을 저장하고 있다면, 전자 장치는 서버 장 치와의 통신이 연결되지 않은 상태에서도, 번역 모델을 이용하여 상술한 번역 동작을 수행할 수도 있다. 도 1에서는, 번역된 텍스트가 전자 장치의 디스플레이에 표시되는 실시예를 표시하였지만, 이는 하나의 실 시예일 뿐, 전자 장치는 TTS(Text to Sound)모듈을 이용하여, 번역된 텍스트를 음성으로 출력할 수도 있다. 도 2는 본 개시의 적어도 하나의 실시 예에 따른 서버 장치의 구성을 나타내기 위한 블록도이다. 도 2에 따르면, 서버 장치는 통신부, 메모리 및 프로세서를 포함할 수 있다. 통신부는 각종 외부 장치와 통신을 수행하기 위한 구성이다. 통신부는 적어도 하나의 무선 통신 모듈, 적어도 하나의 유선 통신 모듈 등을 포함할 수 있다. 각 통신 모듈은 적어도 하나의 하드웨어 칩 형태로 구현될 수 있다. 일 예로, 무선 통신 모듈은 와이파이 모듈, 블루투스 모듈, 적외선 통신 모듈 또는 기타 통신 모듈 중 적어도 하나의 모듈을 포함할 수 있다. 이 밖에, 통신부는 지그비(zigbee), 3G(3rd Generation), 3GPP(3rd Generation Partnership Project), LTE(Long Term Evolution), LTE-A(LTE Advanced), 4G(4th Generation), 5G(5th Generation)등과 같은 다양한 무선 통신 규격에 따라 통신을 수행하는 적어도 하나의 통신 칩을 포함할 수 있다. 유선 통신 모듈은 일 예로, LAN(Local Area Network) 모듈, 이더넷 모듈, 페어 케이블, 동축 케이블, 광섬유 케이블 또는 UWB(Ultra Wide-Band) 모듈 중 적어도 하나를 포함할 수 있다. 메모리는 서버 장치 의 구성요소들의 전반적인 동작을 제어하기 위한 운영체제(OS: Operating System) 및 서버 장치 의 구성요소와 관련된 인스트럭션 또는 데이터를 저장할 수 있다. 메모리는 서버 장치에서 번역 학습 모델의 동작을 위한 각종 프로그램 및 데이터 등을 저장할 수도 있다. 구체적으로는, 메모리는 서브샘플러, VQ 코드북(codebook)과 같은 특징 벡터의 데이터베이스, 인코 더, 판별자 모듈, 정규화 모듈, 디코더 등과 같은 다양한 소프트웨어 모듈을 저장할 수 있다. 또는, 메모리 는 합성 음성을 생성하기 위한 합성 음성 생성 모듈, 번역 모델을 학습하기 위한 학습 모듈 등을 저장할 수도 있다. 메모리는 휘발성 메모리(예: DRAM(dynamic RAM), SRAM(static RAM), 또는 SDRAM(synchronous dynamic RAM) 등), 비휘발성 메모리(non-volatile Memory)(예: OTPROM(one time programmable ROM),PROM(programmable ROM), EPROM(erasable and programmable ROM), EEPROM(electrically erasable and programmable ROM), mask ROM, flash ROM, 플래시 메모리(예: NAND flash 또는 NOR flash 등), 하드 드라이브, 또는 솔리드 스테이트 드라이브(solid state drive(SSD)) 등 다양한 형태로 구현될 수 있다. 또한, 도 2의 각 구성은 하나씩 도시 되었으나, 서버 장치의 종류나 설계 사양에 따라서 통신부, 프 로세서, 메모리중 적어도 하나는 복수 개로 구현될 수도 있다. 또한, 일부 메모리는 프로세서 에 내장된 형태가 될 수도 있다. 프로세서는 통신부 및 메모리와 전기적으로 연결되어, 메모리에 저장된 각종 명령어 또는 프로그램을 이용하여 서버 장치의 동작을 전반적으로 제어한다. 일 실시 예에 따르면, 프로세서는 번 역 모델을 학습할 수 있다. 번역 모델의 학습을 위해, 프로세서는 제1 언어로 발화된 실제 음성과, 실제 음성에 대응되는 텍스트, 제1 텍스트에 대응되는 제2 언어, 즉, 타겟 언어의 제2 텍스트에 대한 데이터를 제공받을 수 있다. 설명의 편의 상, 실제 음성에 대응되는 텍스트를 제1 텍스트, 타겟 언어의 텍스트를 제2 텍스트라고 할 수 있다. 일 예로, 제1 텍스트가 \"안녕하세요\"라면, 제2 텍스트는 \"Hello\"가 될 수 있다. 프로세서는 마이크나 키보드, 마우스, 조이스틱 등 다양한 입력 수단을 통해서 실제 음성과, 제1 텍스트, 제2 텍스트 등을 직접 입력 받아, 메모리에 저장할 수도 있다. 또는, 실제 음성과, 제1 텍스트, 제2 텍스 트 등의 데이터를 외부 장치 등으로부터 통신부를 통해 수신할 수도 있다. 프로세서는 제1 텍스트에 대응되는 합성 음성을 생성할 수 있다. 합성 음성의 생성은 다양한 방식으로 수 행될 수 있다. 일 예로, 프로세서는 waveform 합성, 통계적 매개변수 생성, 음성 합성 시스템(ASR-TTS) 및 딥 러닝 기반 음성 변환 등의 방법을 이용하여 합성 음성을 생성할 수 있다. 이러한 생성 방법은 일 예일 뿐, 이에 한정되지 않는다. 합성 음성의 생성 방법은 공지된 기술이므로 생성 방법에 대해서는 생략한다. 프로세서는 실제 음성, 제1 텍스트, 합성 음성 및 제2 텍스트를 이용하여 번역 모델을 학습한다. 이 경우, 학습을 위한 목적은, 실제 음성에 대한 번역 모델의 출력 값과, 합성 음성에 대한 번역 모델의 출력 값의 차이 가 기 설정된 임계치 미만이 되도록 하는 제1 목적, 실제 음성에 따른 특징 정보와, 합성 음성에 따른 특징 정 보가, 제1 언어의 텍스트 별로 구분되도록 하는 제2 목적 등으로 설정될 수 있다. 프로세서는 이러한 목적 을 달성하기 위한 제1 및 제2 손실 함수에 기초하여, 각 손실 함수가 작아지는 방향으로 번역 모델을 학습시킨 다. 도 3은 본 개시의 적어도 하나의 실시 예에 따른 서버 장치의 번역 모델 학습 방법을 설명하기 위한 구성이다. 도 3에 따르면, 메모리는 복수의 인코더 및 서브 샘플러(subsampler, 310-1, 300-1, 310-2, 300-2), VQ 코드북(VQ codebook, 320), 정규화 모듈(Regularizer, 330), 판별자 모듈(Discriminator, 340), 공유 인코더 (shared encoder, 350), 디코더(decoder, 360) 등과 같은 다양한 소프트웨어 모듈을 저장한다. 하나의 인코더 및 서브 샘플러(310-1, 300-1)는 실제 음성을 위한 소프트웨어 모듈이고, 다른 하나의 인코더 및 서브 샘플러(310-2, 300-2)는 합성 음성을 위한 소프트웨어 모듈이 될 수 있다. 합성 음성을 위한 인코더 및 서브 샘플러(310-2, 300-2)와, 정규화 모듈(Regularizer, 330), 판별자 모듈 (Discriminator, 340)은 번역 모델의 학습을 위하여 마련된 소프트웨어 모듈들이 될 수 있다. 설명의 편의를 위 하여, 실제 음성에 대한 인코더 및 서브 샘플러(310-1, 300-1)는 제1 인코더 및 제1 서브 샘플러라고 하고, 합 성 음성에 대한 인코더 및 서브 샘플러(310-2, 300-2)는 제2 인코더 및 제2 서브 샘플러라고 한다. 실제 음성을 위한 인코더 및 서브 샘플러(310-1, 300-1), VQ 코드북, 공유 인코더, 디코더는 번역 모델을 구성할 수 있다. 도 3에서는, 제1 언어를 제2 언어로 번역하는 경우를 기준으로 도시하였으나, 번역할 언어의 종류가 늘어나면, 인코더 및 서브 샘플러의 개수도 늘어날 수 있다. 예를 들어, 한국어에서 영어로 번역하는 모델의 경우 실제 한 국어로 발화된 음성의 처리를 위한 인코더 및 서브 샘플러 각각 1개, 합성 음성의 처리를 위한 인코더 및 서브 샘플러 각각 1개가 요구된다. 다만, 프랑스어에서 한국어로 번역하는 모델도 추가되는 경우, 실제 프랑스어로 발화된 음성의 처리를 위한 인코더 및 서브 샘플러 각각 1개, 프랑스어로 된 텍스트의 합성 음성의 처리를 위한 인코더 및 서브 샘플러 각각 1개가 추가로 요구된다. 프로세서는 제1 서브 샘플러(300-1)를 이용하여 실제 음성 신호를 샘플링하고, 제1 서브 샘플러(300-1)에 서 샘플링된 실제 음성을 제1 인코더(310-1)를 이용하여 인코딩한다. 또한, 프로세서는 제2 서브 샘플러(300-2)를 이용하여 합성 음성 신호를 샘플링하고, 제2 서브 샘플러 (300-2)에서 샘플링된 합성 음성을 제2 인코더(310-2)를 이용하여 인코딩한다. 합성 음성 신호는 TTS 모듈(미도 시)을 이용하여 제1 텍스트로부터 직접 생성할 수도 있고, 외부 장치로부터 제공받을 수도 있다. 프로세서는, 제1 인코더(310-1)에서 인코딩된 실제 음성에 대한 번역 모델의 출력값과, 제2 인코더(310- 2)에서 인코딩된 합성 음성에 대한 번역 모델의 출력값 사이의 유사도가 기 설정된 임계치 이상이 될 때까지 번 역 모델을 반복적으로 학습한다. 즉, 번역 모델의 학습에 있어서 실제 발화된 음성만으로의 학습은 데이터가 부족하여 현실적으로 어려우므로, 합성 음성을 생성하여 번역 모델을 학습함으로서 번역 모델의 효율적인 학습을 도모할 수 있다. 번역 모델의 학습을 위해서는, 제1 언어로 발화된 실제 음성, 실제 음성에 대응되는 제1 텍스트, 제1 텍스트에 대응되는 타겟 언어인 제2 텍스트가 요구된다. 이 때, 합성 음성의 생성은 Tacotron2 또는 Fastspeech2 등의 모 델에 의해 생성될 수 있다. 다만, 해당 모델은 실시예 중 일부일 뿐이며, 프로세서는 이 밖에 다양한 방법 을 이용하여 합성 음성을 생성할 수 있다. 획득된 제1 언어로 발화된 실제 음성과 제1 텍스트의 합성 음성은 아날로그 신호의 형태이므로 디지털 신호로의 처리가 요구된다. 실제 발화된 음성은 퓨리에 변환을 통해 주파수 단위로 변환하는 과정으로서 이루어질 수 있 으며, 해당 변환은 서브샘플러와 인코더(310-1, 300-1, 310-2, 300-2)를 통해 이루어질 수 있다. 서브샘플러 (300-1, 300-2)는 다운 샘플링을 할 수 있다. 음성 신호의 경우 각각의 샘플이 일반적으로 초당 수천 개에서 수 십만 개로 구성되므로, 음성 신호의 처리 및 저장에 있어서 많은 자원을 요구할 수 있다. 따라서, 효율적인 처 리를 위해서는 음성 신호를 다운샘플링하는 것이 요구될 수 있다. 프로세서는 복수의 서브샘플러(300-1, 300-2)를 이용하여 실제 음성 및 합성 음성을 샘플링 한 후 디지털 신호로 변환하고, 인코더(310-1, 310-2)를 이용하여 각각의 디지털 신호에 대한 특징 정보를 획득한다. 특징 정 보는 특징 벡터 또는 그 벡터의 리스트를 포함하는 형태가 될 수 있다. 구체적으로, 프로세서는 번역될 실제 음성 및 학습 음성이 획득된 경우, 음성 액티비티 검출(voice activity detection; VAD) 기술을 이용하여 각 음성을 적어도 하나의 음성 세그먼트로 분할될 수 있다. 프로세 서는 인코더(310-1, 310-2)를 이용하여 각 음성으로부터 분할된 음성 세그먼트를 인코딩하여, 그 특징 정 보를 획득할 수 있다. 도 3의 인코드(310-1, 310-2)의 인코딩 결과는 인코더의 출력으로 지칭될 수 있다. 인코 더의 출력은 히든(hidden) 레이어 벡터로 표현될 수 있다. 프로세서는 실제 음성 뿐만 아니라 합성 음성에 대해서도 동일한 방식으로 특징 정보(즉, 특징 벡터)를 추출할 수 있다. 다만 실제 음성과 합성 음성 각각의 인코딩 결과값인 특징 벡터는 자유도가 매우 높은 상태이므로, 이에 대해 불연속(discrete) 형태로 바꿔야 할 필요성이 있다. 즉, 변경된 불연속(discrete) 형태의 표현을 위하여 VQ 코 드북(Vector Quantization codebook)이 요구될 수 있다. 음성 인식 과정에서, 벡터 양자화는(Vector Quantization)은 인식된 음성의 특징 벡터를 코드북 내 가장 거리가 가까운 벡터로 대응시킴으로써 데이터를 압축시키는 기능과 음성 특징 벡터가 속해야 할 그룹을 찾아내는 탐색 기능을 가능하게 하는 방법이 될 수 있다. 즉, 실제 발화된 음성과 합성 음성 각각의 인코딩된 결과인 특징 벡터는 VQ 코드북(codebook) 내의 코드워드 (code word)로 대응될 수 있다. 이 때, 코드워드(code word)란, VQ 코드북(codebook)내의 적어도 하나의 벡터 를 의미한다. 프로세서는 실제 발화된 음성과 합성 음성으로부터 추출된 특징 벡터를 VQ 코드북(codebook) 내의 코드워 드로 갱신할 수 있다. 이 경우, 실제 음성과 합성 음성에 기초한 번역 결과가 유사해질 수 있도록, 프로세서 는 상술한 제1 손실 함수 및 제2 손실 함수에 기초하여 학습을 진행한다. 프로세서는 적대적 생성 신경망 (Generative Adversarial Networks, GAN)기반 방식에 기초하여, 실제 음 성과 합성 음성의 특징 벡터가 서로 비슷한 코드워드(code word)로 대응되도록 VQ 코드북을 갱신하여 번역 모델 을 학습할 수 있다. 적대적 생성 신경망 기반 방식에 기초하는 경우, 프로세서는 생성자(Generator) 모듈(미도시)과 판별자 (Discriminator) 모듈을 사용할 수 있다. 판별자 모듈은 입력된 음성이 실제 발화된 음성인지 합성 음성인지 구별하도록 학습되고, 생성자 모듈은 판별자 모듈이 합성 음성이라고 판별할 수 없을 만큼 실제 음성과 합성 음성의 특징 정보를 유사하게 만들도록 학습된 소프트웨어 모듈들이다. 즉, 판별자 모듈은, 음성의 입력에 따라, 입력된 음성이 실제 발화된 음성인지 합성 음성인지 판단하고, 입력된 음성이 실제 발화된 음성이 아니면, 현재 학습 결과를 불일치(mismatch)로 판별하고, 입력된 음성이 실제 발화 된 음성이면, 현재 학습 결과를 일치(match)로 판별하도록 학습된다. 도 3에서, 프로세서는 실제 음성으로부터 추출한 특징 벡터에 대응되는 코드워드를 VQ 코드북로부터 추출하고, 합성 음성으로부터 추출한 특징 벡터에 대응되는 코드워드를 VQ 코드북로부터 추출한다. 특징 벡터뿐 아니라 코드워드도 상술한 특징 정보에 포함될 수 있다. 프로세서는, 실제 음성에 대응되는 특징 정보와, 합성 음성에 대응되는 특징 정보에 기초하여, 실제 음성 및 상기 합성 음성의 특징 차이가 기 설정된 임계치 미만이 되도록 조정된 특징 정보를 생성하여, VQ 코드북 (codebook)을 갱신할 수 있다. 이 경우, 판별자 모듈이 사용될 수도 있다. 프로세서는 실제 음성을 입력 받은 번역 모델의 제1 출력값과, 합성 음성을 입력받은 번역 모델의 제2 출 력값을 판별자 모듈에 입력할 수 있다. 판별자 모듈은 제1 및 제2 출력값의 차이에 대응되는 값을 출 력할 수 있다. 프로세서는 판별자 모듈의 출력값이 임계치 미만이 될 때까지, 판별자 모듈의 출 력값과 임계치를 비교하여 번역 모델에 포함된 VQ 코드북(codebook)을 재 갱신할 수 있다. 이러한 학습에 의해, 실제 음성에 대응되는 특징 정보와 합성 음성에 대응되는 특징 정보의 차이가 기 설정된 임계치 미만으로 낮추어져 서로 유사해질 수 있다. 프로세서는 서로 대립되는 동작을 수행하는 판별자 모듈과 생성자 모듈을 이용하여 상술한 비교 및 갱신 동작을 반복적으로 수행하여, 실제 음성 및 합성 음성 각각의 번역 결과의 유사도를 높이게 된다. 실제 음성에 대한 번역 모델의 출력 값과, 합성 음성에 대한 번역 모델의 출력 값의 차이가 기 설정된 임계치 미만이 되도록 하는 제1 손실 함수의 일 예는 아래와 같이 설정될 수 있다. <수학식 1>"}
{"patent_id": "10-2024-0056688", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "제1 손실 함수 LGAN (즉 GAN 손실함수)에서 D는 판별자 모듈의 출력값, 는 합성 음성의 인코더 출력값, 는 실 제 음성의 인코더 출력값, 는 실제 음성, 는 합성 음성을 의미한다. 프로세서는 수학식 1과 같은 제1 손실 함수의 값을 기 설정된 값 이하가 될 때까지 반복적으로 학습하여, 실제 발화된 음성과 합성 음성의 특징 벡터가 유사한 값을 갖도록 학습할 수 있다. 학습 과정을 N번 반복함에 따라, 생성자 모듈은 실제 발화된 음성과 유사한 합성 음성을 생성 출력하도록 학습 되어 가고, 판별자 모듈은 실제 발화된 음성과 합성 음성을 구별하는 정확도가 높아지는 방향으로 학습된다. 한편, GAN 방식으로만 학습하는 경우, 서로 다른 텍스트에 대한 합성 음성의 특징 정보가 동일해질 가능성이 있 다. 이에 따라, 프로세서는 정규화 모듈을 이용하여 서로 다른 텍스트에 대해서는 서로 다른 특징 정 보를 가질 수 있도록 VQ 코드북을 갱신시킬 수 있다. 정규화 모듈은 특징 정보를 제1 언어의 텍스트 별로 구분하여 학습하기 위한 모듈이다. 프로세서는 제1 언어의 각 텍스트 별 실제 음성에 의해 획득되는 제1 출력값과 합성 음성에 의해 획득되는 제2 출력값을 정규화 모듈에 입력할 수 있다. 프로세서는 정규화 모듈의 출력 값에 기초하여 각 텍스트 별로 서로 다른 특징 정보를 가지도록 VQ 코드북(codebook)을 갱신할 수 있다. 구체적으로, 정규화 모듈은 다음과 같은 제2 손실 함수가 작아지는 방향으로 동작할 수 있다. <수학식 2>"}
{"patent_id": "10-2024-0056688", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "S는 합성 음성의 특징 벡터와 실제 발화된 음성의 특징 벡터 사이에서 코사인 유사도를 계산하는 함수를 의미하 며, V는 VQ 코드북 내의 코드워드를 의미한다. 구체적으로, 정규화 모듈은 LContrastive의 값을 기설정된 값 이하가 되도록 학습하여, 모델 붕괴(Model Collapse)를 방지할 수 있다. 모델 붕괴(Model Collapse)란 GAN 방 식으로 학습이 이루어지는 경우, 다른 내용의 텍스트가 입력된 경우임에도 VQ 코드북(codebook)의 같은 코 드워드에 대응되는 경우를 의미한다. 예를 들어, 다른 내용의 텍스트 즉, \"안녕하세요\"와 \"벌레의 세계\"를 입력 했음에도 VQ 코드북에서 같은 코드워드로 대응되는 경우를 의미한다. 제2 손실 함수에 의한 학습은 다음과 같다. 일 예로, 실제 발화된 음성의 텍스트가 \"안녕\"이며, 합성 음성의 텍스트가 \"만나서 반가워\" 인 경우, \"안 녕\"과 \"만나서 반가워\"는 다른 의미의 텍스트이나, 의미의 유사도 측면에서 아예 다른 의미의 텍스트라고 보기 는 어렵다. 따라서, 실제 발화된 음성의 텍스트와 합성 음성의 텍스트가 코드워드에 대응될 때, 멀지 않은 위치 에서 대응되도록 하는 학습이다. 상술한 제2 손실 함수에 의한 학습 결과, 고른 코드워드(code word)의 분포를 가질 수 있다. 이 때, 제1 손실 함수의 목적은 각각 실제 발화된 음성과 합성 음성의 구별을 어렵도록 하는 것이고, 제2 손실 함수의 목적은 실제 음성에 따른 특징 정보와 합성 음성에 따른 특징 정보가 제1 언어의 텍스트 별로 구분되도 록 하는 것이므로, 프로세서는 제1 및 제2 손실 함수에 기초한 학습을 임의의 순서로 수행할 수 있다. 즉, 제1 손실 함수에 의해 학습이 이루어진 후, 제2 손실 함수에 의해 학습될 수 있으나, 제2 손실 함수에 의해 학습이 이루어진 후 제1 손실 함수에 의해 학습이 될 수 있다. 또는 제1 손실 함수에 의한 학습과 제2 손실 함 수에 의한 학습이 순차적으로 이루어질 수 있다. 또한, 제1 손실 함수에 의한 학습에서 생성자 모듈의 학습이 이루어지고, 판별자 모듈의 학습이 이루어진 후, 생성자 모듈과 판별자 모듈의 학습이 번갈아 가며 이루어질 수 있다. 다음 수식은 VQ 코드북(codebook)의 최적화를 위한 제3 손실 함수의 일 예를 나타낸다. <수학식 3>"}
{"patent_id": "10-2024-0056688", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수학식 3에 따르면, VQ 코드북(codebook)의 최적화를 위해 Gumbel-softmax 손실 함수(Lgumbel-softmax)와 다양성 손 실 함수(Ldiversity) 가 추가될 수 있다. Gumbel-softmax 손실 함수를 사용하는 경우, VQ (vector quantization) 모듈에 의해, discrete한 표현( )으로 변환된 결과값이 one-hot 벡터가 될 수 있다. One-hot 벡터란 해당하 는 항목의 위치에만 1을 가지고 나머지 항목은 0으로 채워진 벡터를 의미한다. 일 예로 \"사과\"는 [1,0,0,0] 이 며, \"딸기\"는 [0,1,0,0]로 표현될 수 있다. 또한, 다양성 손실 함수는 VQ 코드북(codebook)에서 특정 인덱스만 집중적으로 선택되는 것을 방지할 수 있다. 다양성 손실 함수란, VQ 코드북(codebook)에 대응될 때, 코드워드(code word) 사이의 거리를 최대화하는 방향으 로 학습되도록 하는 함수이다. 상술한 학습 방법은 신조어에 대한 학습을 하는 경우에도 동일하다. 일 예로, 신조어를 새롭게 번역 모델에 추 가하고 싶더라도, 모든 신조어에 대해 실제 사람의 발화 음성으로 학습시키는 것은 현실적으로 불가능할 수 있 다. 따라서, 프로세서는 신조어의 텍스트에 대한 합성 음성을 TTS 모듈을 이용하여 생성할 수 있다. 프로 세서는 생성된 합성 음성의 특징 정보를 상술한 방법으로 추출하여, VQ 코드북(codebook)에 추가하는 등의 갱신을 수행할 수 있다. 신조어에 대하여 VQ 코드북(codebook)이 학습된 경우, 사용자가 전자 장치의 마이크 근처에서 신조어를 발화하여 입력한 경우, 합성 음성에 의해 VQ 코드북(codebook)에 저장된 코드워드가 추출될 수 있다. 따라서, 매 신조어가 생길 때마다 직접 발화하여 다시 학습하지 않더라도, 그 합성 음성에 기 초하여 학습을 진행할 수 있으므로, 신조어에 대한 번역 서비스를 빠르게 제공할 수 있다. 이상과 같이 프로세서는 실제 음성 및 합성 음성을 이용하여 VQ 코드북을 갱신하여, VQ 코드북 을 포함하는 번역 모델을 학습시킬 수 있다.VQ 코드북에 대한 학습이 완료되면, 프로세서는 공유 인코더, 디코더 등을 포함하는 전체 번역 모델에 대한 학습을 수행할 수도 있다. 즉, 상술한 바와 같이, 번역 모델은, 제1 서브 샘플러(300-1), 제1 인코더(310-1), VQ 코드북, 공유 인코 더, 디코더를 포함하는 형태로 구현될 수 있다. 공유 인코더는 VQ 코드북(codebook)에 포함된 특징 정보에 기초하여 음성을 인코딩할 수 있다. 디코더는 공유 인코더에서 출력되는 특징 벡터를 디코딩하여 제2 언어의 텍스트를 추출할 수 있다. 디코더는 별도로 마련된 사전 데이터를 이용하여, 특징 벡터에 대응되는 텍스트를 추출할 수 있다. 한편, 디코더라고 기재하였으나, 공유 디코더가 될 수도 있다. 프로세서는, VQ 코드북(codebook)의 갱신이 완료된 상태에서, 공유 인코더 및 디코더를 포 함하는 전체 번역 모델에 대하여, 실제 음성, 합성 음성, 제1 언어의 텍스트, 제2 언어의 텍스트 등을 포함하는 학습 데이터를 입력하여, 학습을 수행할 수 있다. 이상과 같이 학습된 번역 모델은 다중 언어 간의 번역에 사용될 수 있다. 이러한 번역 모델은 서버 장치에 탑재된 상태로 사용될 수 있으나, 반드시 이에 한정되는 것은 아니며, 전자 장치에 직접 탑재될 수도 있고, 학습을 수행한 서버 장치 이외에 다른 종류의 서버 장치에 탑재되어 사용될 수도 있다. 이 경우, 프 로세서는 학습이 완료된 번역 모델을 통신부를 통해서 전자 장치나 기타 외부 장치로 전송하여 줄 수 있다. 또는, 번역 모델 내의 일부 소프트웨어 모듈이나 VQ 코드북만이 전자 장치에 탑재되어, 전자 장치 와 서버 장치의 연동에 의해 번역이 수행될 수도 있다. 이 경우, 프로세서는 통신부를 통 해 전자 장치와 다양한 데이터 및 신호를 송수신함으로써, 번역 서비스를 지원할 수 있다. 구체적으로, VQ 코드북(codebook)을 포함하는 번역 모델이 설치된 적어도 하나의 전자 장치가 존재한다고 가정 하면, 프로세서는 그 전자 장치가 전송하는 인덱스 정보를 통신부를 통해 수신할 수 있다. 인덱스 정 보에 대해서는 후술하는 부분에서 구체적으로 설명한다. 프로세서는 인덱스 정보가 수신되면, 메모리에 저장된 VQ 코드북(codebook)으로부터 인덱스 정 보에 대응되는 특징 정보를 추출하고, 공유 인코더 및 디코더를 이용하여 추출된 특징 정보에 대응되 는 제2 언어의 텍스트를 획득한다. 프로세서는 획득된 텍스트를 통신부를 통해 적어도 하나의 전자 장치로 전송할 수 있다. 텍스트를 수신한 전자 장치는 이를 디스플레이하거나 스피커를 통해 전자 음성으로 출 력할 수 있다. 도 4는 본 개시의 적어도 하나의 실시 예에 따른 전자 장치의 구성을 나타내는 블록도이다. 상술한 바와 같이, 전자 장치는 휴대폰이나 PC, 랩탑 PC, 태블릿 PC, TV, 키오스크, 무선 스피커, 로봇 등 다양한 장치 로 구현될 수 있다. 도 4에 따르면, 전자 장치는 통신부, 마이크, 디스플레이, 메모리, 스피커 및 프로세서를 포함한다. 다만, 이에 한정되는 것은 아니며, 전자 장치는 일부 구성이 제외된 형태로 구 현될 수도 있고, 다른 구성이 더 포함된 형태로 구현될 수도 있다. 통신부, 프로세서, 메모리의 구체적인 예시에 대해서는 도 2에서 기재한 바 있으므로 중복 설명은 생략한다. 마이크는 사용자가 실제 음성을 발화한 경우, 발화한 음성을 입력 받을 수 있다. 마이크는 프로 세서의 제어에 의해 활성화되어, 사용자가 발화하는 음성 신호 및 기타 다양한 오디오 신호를 입력 받 을 수 있다. 여기서 활성화란, 마이크로 전원을 인가하고, 마이크 기능을 수행하기 위한 소프트웨어를 메 모리에 로딩하여 실행하는 전반적인 동작을 포함할 수 있다. 실제 음성이라는 것은 반드시 사용자가 전자 장치 부근에서 발화한 음성으로 국한되는 것은 아니며, 스피커나 전화 등 기타 장치에서 출력된 음성 신호가 될 수도 있다. 또는, 가사가 포함된 음악 등이 될 수도 있 다. 일 예로, 사용자는 전자 장치의 마이크에 \"안녕하세요\"라는 음성을 발화할 수 있다. 마이크는 사 용자가 발화한 \"안녕하세요\"라는 음성을 입력할 수 있다. 메모리에는 제1 서브샘플러(300-1), 제1 인코더(310-1), VQ 코드북를 포함하는 번역 모델과, 기타 소 프트웨어 및 데이터가 저장될 수 있다. 프로세서는 제1 서브샘플러(300-1), 제1 인코더(310-1)를 이용하여 사용자의 음성으로부터 특징 정보 를 추출할 수 있다. 프로세서는 추출된 특징 정보에 대응되는 코드워드의 인덱스 정보를 VQ 코드북에 기초하여 획득한 후, 획득한 인덱스 정보를 통신부를 통해서 서버 장치로 전송할 수 있다. 인덱스 정 보는, 코드워드 그 자체가 될 수도 있고, 코드워드의 위치 등과 같이 코드워드를 식별할 수 있는 고유의 인덱스 값 등이 될 수도 있다. 일 예로, 사용자가 발화한 제1 언어의 텍스트가 '벌레의 세계'인 경우를 상정할 수 있다. 타겟 언어가 영어 인 경우, 타겟 텍스트는 'world of worms'가 될 수 있다. 프로세서는 '벌레의 세계'에 대응되는 인덱스 정 보를 획득할 수 있다. 즉, VQ 코드북에서 'wor'에 대응되는 코드워드의 인덱스 값이 1, 'ld'에 대응되는 코드워드의 인덱스 값이 2, 'of' 에 대응되는 코드워드의 인덱스 값이 3, 'ms' 에 대응되는 코드워드의 인덱스 값이 4라면, 프로세서는'world of worms'이라는 음성으로부터 [1,2,3,1,4]라는 리스트 형태의 인덱스 정보 를 생성할 수 있다. 프로세서는 생성한 인덱스 정보를 통신부를 통해서 서버 장치로 전송한다. 서버 장치는 전 송된 인덱스 정보에 대응되는 제2 언어의 텍스트에 대한 데이터를 전송할 수 있다. 프로세서는 제2 언어의 텍스트에 대한 데이터가 통신부를 통해 수신되면, 수신된 데이터에 기초하여 제2 언어의 텍스트를 표시하도록 디스플레이를 제어할 수 있다. 또는, 프로세서는 수신된 데이터를 TTS 모듈을 이용하여 전자 음성 신호로 변환한 후, 변환된 전자 음성 신호를 스피커를 통해 출력할 수도 있다. 스피커는 프로세서의 제어에 따라 다양한 오디오 신호를 출력하기 위한 구성이다. 일 예로, 사용자가 \"안녕하세요\"라는 음성을 발화한 경우, 타겟 언어가 영어라면, 프로세서는 \"Hell o\"라는 텍스트를 통신부를 통해 수신할 수 있다. 프로세서는 스피커를 제어하여, \"Hello\"라는 음성을 출력하거나, 디스플레이를 제어하여 표시할 수 있다. 이상에서는, 프로세서가 VQ 코드북(codebook)에서 획득된 인덱스 값을 포함하는 인덱스 정보를 서버 장치 로 전송한 후, 서버 장치로부터 그 인덱스 정보에 대응되는 제2 언어의 텍스트를 직접 수신하는 경우 를 설명하였으나, 반드시 이에 한정되는 것은 아니다. 즉, 프로세서는 통신부를 통해서, 서버 장치로부터 제2 언어의 텍스트에 대응되는 인덱스 정보 를 수신받을 수도 있다.또는, 전체 번역 모델이 메모리에 저장되어 있는 경우라면, 서버 장치와의 통신 없 이 프로세서는 메모리에 저장된 VQ 코드북을 통하여 직접 번역을 마무리할 수도 있다. 이 중, 서버 장치의 VQ 코드북(codebook)에 기초한 인덱스 정보를 수신하여 번역하는 과정은 다음과 같이 구현될 수 있다. 일 예로, 서버 장치로부터 수신되는 인덱스 정보 중 하나의 인덱스 값이 1인 경우를 상정 할 수 있다. VQ 코드북(codebook)의 벡터가 코드북(codebook) 벡터1: [0.1, 0.2, 0.3], 코드북(codebook) 벡터 2: [0.5, 0.6, 0.7], 코드북(codebook) 벡터3: [0.8, 0.9, 0.4]이라고 가정하면, 프로세서는 수신된 인덱 스 값 1에 대응되는 벡터 1을 획득할 수 있다. 프로세서는 획득된 벡터1: [0.1, 0.2, 0.3]를 기초로 인코딩 및 디코딩을 수행하여, 제2 언어의 텍스트를 획득할 수 있다. 이상에서는 다양한 실시 예들에 대해서 각각 설명하였으나, 각 실시 예들은 반드시 개별적으로만 구현되는 것은 아니며, 적어도 하나의 다른 실시 예들과 전체적으로 또는 부분적으로 결합되어 하나의 제품에 함께 구현될 수 도 있다. 도 5는 본 개시의 적어도 하나의 실시 예에 따른 서버 장치의 번역 모델 학습 방법을 설명하기 위한 흐름도이다. 본 개시의 일 실시예에 따르면 서버 장치의 번역 모델 학습을 위해서는 제1 텍스트에 기초하여 합성 음성을 생 성할 수 있다.(S510) 합성 음성의 생성은 상술한 바와 같이 Fastspeech2 등과 같은 모델을 사용할 수 있다. 일 예로, 실제 발화된 음성이 \"안녕하세요\"이고, 그에 대응되는 제1 텍스트가 저장되어 있다면, 서버 장치는 제1 텍스트에 대응되는 합성 음성을 Fastspeech2 등의 모델을 사용하여 생성할 수 있다. 합성 음성이 생성된 후, 실제 음성, 제1 텍스트, 합성 음성, 제2 텍스트로 번역 모델을 학습할 수 있다.(S520) 즉, 사용자에 의해 \"안녕하세요\"라고 실제 발화된 음성, \"안녕하세요\"라는 제1 텍스트, \"안녕하세요\"라는 텍스트가 합성된 음성, 타겟 언어가 영어인 경우 \"Hello\"라는 제2 텍스트를 사용하여 번역 모델을 학습시킬 수 있다. 학습 방법은 도 3에서 자세히 상술한 바, 학습 방법에 대한 설명은 생략한다. 도 6은 본 개시의 적어도 하나의 실시 예에 따른 서버 장치에서 번역하는 과정을 설명하기 위한 흐름도이다. 서버 장치는 전자 장치로부터 VQ 코드북의 인덱스 정보를 수신할 수 있다. (S610) 일 예로 \"안녕하세요\"라 는 음성에 대한 인덱스 정보가 전자 장치로부터 수신되었다면, 서버 장치는 수신한 인덱스에 대응되는 특징 벡터를 서버 장치의 VQ 코드북으로부터 획득할 수 있다.(S620) 즉, 서버 장치와 전자 장치에 탑재된 VQ 코드북은 동일하므로, 전자 장치로부터 인덱스 정보만 수신한 경 우라도 서버 장치에서는 수신한 인덱스 정보를 기초로 하여 특징 벡터를 획득할 수 있다. 서버 장치는, 획득한 특징 벡터를 공유 인코더 및 디코더를 이용하여 처리함으로써, 타겟 언어의 텍스트를 획득 할 수 있다(S640, 650). 이러한 과정에 대해서는 상술한 부분에서 구체적으로 설명하였으므로 중복 설명은 생략 한다. 타겟 언어의 텍스트가 획득된 경우, 서버 장치는 타겟 언어의 텍스트 또는 그 텍스트에 대응되는 인덱스 정보를 전자 장치로 송신할 수 있다.(S650) 도 6 및 이상에서는, S610 내지 S640의 계산 과정은 서버 장치에서 이 루어지는 것으로 기재하였으나, 이에 한정되는 것은 아니고 전자 장치에 디코더 모듈과 번역 모듈을 탑재 하여 서버 장치와의 통신 없이 타겟 언어로 번역된 텍스트를 획득하는 경우도 포함될 수 있다. 도 7은 본 개시의 적어도 하나의 실시 예에 따른 전자 장치에서 번역하는 과정을 설명하기 위한 흐름도이다. 전자 장치에서는 실제 음성을 마이크를 통해 획득할 수 있다.(S710) 전자 장치는, 실제 음성에 대응되는 VQ 코드북의 인덱스를 추출할 수 있다.(S720) 일 예로, 사용자가 전자 장치의 마이크 부분에 \"안녕하세요\"라고 발화한 경우 그 음성은 VQ 코드북(codebook)에서는 [0.2, 0.3, 0.4]로 대응될 수 있다. VQ 코드북(codebook)의 벡터가 코드북(codebook) 벡터1: [0.1, 0.2, 0.3], 코드북(codebook) 벡터2: [0.5, 0.6, 0.7], 코드북 (codebook) 벡터3: [0.8, 0.9, 0.4]과 같다면, [0.2, 0.3, 0.4]는 코드북(codebook) 벡터1과 가장 가까우므로 코드북(codebook) 벡터 1에 대응될 수 있다. 즉, [0.2, 0.3, 0.4]은 VQ 코드북(codebook) 벡터에서 인덱스 값 이 1이 될 수 있다. 즉, 실제 음성인 \"안녕하세요\"에 대응되는 VQ 코드북(codebook)의 인덱스 정보는 1이 될 수 있다. 전자 장치는 VQ 코드북의 인덱스를 서버 장치로 전송할 수 있다. (S730) 서버 장치는 전자 장치와 동일한 VQ 코드북(codebook)이 탑재되어 있으므로, 전자 장치로부터 수신되 는 인덱스 정보에 기초하여, VQ 코드북으로부터 특징 벡터를 획득할 수 있다. 특징 벡터를 획득한 후, 서버 장 치는 인코딩 및 디코딩 과정을 거쳐 타겟 언어의 텍스트를 획득할 수 있다. 이에 따라, 서버 장치가 텍스트 또 는 그에 대한 인덱스 정보를 전송하면, 전자 장치는 이를 수신할 수 있다. (S740) 전자 장치는 타겟 언어로 번역된 텍스트 또는 그 인덱스 정보에 기초하여, 타겟 언어의 텍스트를 디스플레이를 통해 표시하거나 스피커를 통해 출력할 수 있다. (S750) 이때, 타겟 언어가 영어인 경우에는 수신된 텍스트가 \"Hello\"가 될 수 있다. 타겟 언어가 불어인 경우에는 수신된 텍스트가 \"Bonjour\"이며, 언어의 종류에 한정이 없 음은 물론이다. 도 5, 6, 7에서 설명한 다양한 방법들은 상술한 도 2의 서버 장치 및 도 4의 전자 장치에 의해 수행될 수 있으 나, 반드시 이에 한정되는 것은 아니며, 다양한 구성의 장치에 의해 수행될 수도 있다. 한편, 이상에서 설명한 바와 같이, 번역 모델의 학습 및 그 번역 모델을 이용하는 번역은 서버 장치의 프로세서 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 이러한 프로세서(120, 220)는, 하나 또는 복수의 프로세서로 구성될 수 있다. 각 프로세서(120, 220)는 CPU(Central Processing Unit), GPU(Graphic Processing Unit), NPU(Neural Processing Unit) 중 적어도 하나 를 포함할 수 있으나 전술한 프로세서의 예시에 한정되지 않는다. CPU는 일반 연산뿐만 아니라 인공지능 연산을 수행할 수 있는 범용 프로세서로서, 다계층 캐시(Cache) 구조를 통해 복잡한 프로그램을 효율적으로 실행할 수 있다. CPU는 순차적인 계산을 통해 이전 계산 결과와 다음 계산 결과의 유기적인 연계가 가능하도록 하는 직렬 처리 방식에 유리하다. 범용 프로세서는 전술한 CPU로 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. GPU는 그래픽 처리에 이용되는 부동 소수점 연산 등과 같은 대량 연산을 위한 프로세서로서, 코어를 대량으로 집적하여 대규모 연산을 병렬로 수행할 수 있다. 특히, GPU는 CPU에 비해 컨볼루션(Convolution) 연산 등과 같 은 병렬 처리 방식에 유리할 수 있다. 또한, GPU는 CPU의 기능을 보완하기 위한 보조 프로세서(co-processor)로 이용될 수 있다. 대량 연산을 위한 프로세서는 전술한 GPU로 명시한 경우를 제외하고 전술한 예에 한정되지 않 는다. NPU는 인공 신경망을 이용한 인공지능 연산에 특화된 프로세서로서, 인공 신경망을 구성하는 각 레이어를 하드 웨어(예로, 실리콘)로 구현할 수 있다. 이때, NPU는 업체의 요구 사양에 따라 특화되어 설계되므로, CPU나 GPU 에 비해 자유도가 낮으나, 업체가 요구하기 위한 인공지능 연산을 효율적으로 처리할 수 있다. 한편, 인공지능 연산에 특화된 프로세서로, NPU는 TPU(Tensor Processing Unit), IPU(Intelligence Processing Unit), VPU(Vision processing unit) 등과 같은 다양한 형태로 구현 될 수 있다. 인공 지능 프로세서는 전술한 NPU로 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. 또한, 각 프로세서(120, 220)는 SoC(System on Chip)으로 구현될 수 있다. 이때, SoC에는 하나 또는 복수의 프 로세서 이외에 메모리, 및 프로세서와 메모리 사이의 데이터 통신을 위한 버스(Bus) 등과 같은 네트워크 인터페 이스를 더 포함할 수 있다. 전자 장치에 포함된 SoC(System on Chip)에 복수의 프로세서가 포함된 경우, 전자 장치는 복수의 프 로세서 중 일부 프로세서를 이용하여 인공지능과 관련된 연산(일 예로, 인공지능 모델의 학습(learning)이나 추 론(inference)에 관련된 연산)을 수행할 수 있다. 일 예로, 전자 장치는 복수의 프로세서 중 컨볼루션 연 산, 행렬 곱 연산 등과 같은 인공지능 연산에 특화된 GPU, NPU, VPU, TPU, 하드웨어 가속기 중 적어도 하나를 이용하여 인공지능과 관련된 연산을 수행할 수 있다. 다만, 이는 일 실시예에 불과할 뿐, CPU 등과 범용 프로세 서를 이용하여 인공지능과 관련된 연산을 처리할 수 있음은 물론이다. 또한, 서버 장치의 프로세서는 하나의 프로세서에 포함된 멀티 코어(일 예로, 듀얼 코어, 쿼드 코어 등)로 구현될 수 있다. 이에 따라, 상술한 바와 같인 번역 모델의 학습이나, 이를 이용한 번역 등과 같은 다양한 작업 을 수행할 수 있다. 특히, 서버 장치는 프로세서에 포함된 멀티 코어를 이용하여 병렬적으로 컨볼루 션 연산, 행렬 곱 연산 등과 같은 인공 지능 연산을 수행할 수 있다. 본 개시의 다양한 실시 예에서 사용하는 번역 모델은 end-to-end 모델이 될 수 있다. end-to-end 모델은 cascaded 방법과 다르게 1개의 모델을 기초로 번역이 이루어지므로, 번역의 속도가 향상될 수 있으며, cascaded 방법의 문제인 오류 전파(error propagation)을 줄일 수 있다. 본 개시의 다양한 실시 예들에 따르면, 번역 모 델의 성능 개선에 필수적인 학습을 수행하기 위하여, 실제 음성 뿐만 아니라 합성음성까지 학습 데이터로 사용 할 수 있다. 이에 따라, 번역 모델을 효과적으로 학습시킬 수 있으며, 결과적으로 번역의 정확도가 향상될 수 있다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 적어도 하나의 레이어는 적어도 하나의 가중치 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 적어도 하나의 정의된 연산을 통해 레이 어의 연산을 수행한다. 신경망의 예로는, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 및 심층 Q-네트워크 (Deep Q-Networks), Transformer가 있으며, 본 개시에서의 신경망은 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. 학습 알고리즘은, 다수의 학습 데이터들을 이용하여 소정의 대상 기기을 훈련시켜 소정의 대상 기기 스스로 결 정을 내리거나 예측을 할 수 있도록 하는 방법이다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습 (reinforcement learning)이 있으며, 본 개시에서의 학습 알고리즘은 명시한 경우를 제외하고 전술한 예에 한정 되지 않는다. 또한, 본 개시의 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 또는 두개의 사용자 장치들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램제품(예: 다운로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또 는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 본 개시의 다양한 실시 예에 따른 방법은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine- readable storage media에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는 저장 매체로부터 저 장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실시 예들에 따른 서버 장치 또는 전자 장치를 포함할 수 있다. 한편, 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 판독 가능 기록 매체의 형태로 제공될 수 있 다. 여기서, '비일시적 판독 가능 기록 매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다는 것을 의미할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적 으로 저장되는 경우를 구분하지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포 함할 수 있다. 상기 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접 또는 상기 프로세서의 제어 하에 다른 구성요소들 을 이용하여 상기 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2024-0056688", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시가 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형 실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안 될 것이다."}
{"patent_id": "10-2024-0056688", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 적어도 하나의 전자 장치에서 번역 모델을 이용하여 번역하는 동작을 설명하기 위한 도면이다. 도 2는 본 개시의 적어도 하나의 실시 예에 따른, 서버 장치의 구성을 나타내기 위한 블록도이다. 도 3은 서버 장치에서 번역 모델의 학습시키는 방법의 일 예를 설명하기 위한 도면이다. 도 4는 본 개시의 적어도 하나의 실시 예에 따른 전자 장치의 구성을 나타내는 블록도이다. 도 5는 본 개시의 적어도 하나의 실시 예에 따른 서버 장치의 번역 모델 학습 방법을 설명하기 위한 흐름도이다. 도 6은 본 개시의 적어도 하나의 실시 예에 따른 서버 장치에서 번역하는 과정을 설명하기 위한 흐름도이다. 도 7은 본 개시의 적어도 하나의 실시 예에 따른 전자 장치에서 번역하는 과정을 설명하기 위한 흐름도이다."}
