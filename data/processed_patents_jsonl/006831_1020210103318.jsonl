{"patent_id": "10-2021-0103318", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0167727", "출원번호": "10-2021-0103318", "발명의 명칭": "연출 연기 영상 분석 평가 시스템 및 방법", "출원인": "이현준", "발명자": "이현준"}}
{"patent_id": "10-2021-0103318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "드라마나 영화에 따라 제작된 영상을 입력하는 영상 입력부;상기 영상 입력부를 통해 입력된 영상을 시나리오(scenario)에 따라 배우(Actor) 관련 데이터로 이루어진 배우소스층(Actor Source Layer), 장소 관련 데이터로 이루어진 스페이스 로케이션층(Space Location Layer), 시간관련 데이터로 이루어진 타임 소스층(Time Source Layer)을 생성하는 필름 슈팅부(Film Shooting Part);상기 영상 입력부를 통해 입력된 영상에 대하여 상기 시나리오에 근거하여 필름 슈팅에 따른 항목으로분류하고, 분류된 각 항목의 데이터를 딥러닝 학습하여 분석 평가 모델을 생성하는 인공지능 학습부;상기 필름 슈팅부를 통해 생성된 상기 배우 소스층, 상기 스페이스 로케이션층 및 상기 타임 소스층을 각각 상기 분석 평가 모델에 투입하여 상기 시나리오에 따른 배우들(Actors)을 분류하고, 분류된 각 배우들에 대하여장면(Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 감정및 행동을 분석하여 평가를 실행하는 분석 평가부; 및상기 도출된 각 배우의 감정 및 행동에 대한 평가 결과를 화면이나 음향으로 출력하는 출력부;를 포함하는 연출 연기 영상 분석 평가 시스템."}
{"patent_id": "10-2021-0103318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 배우 소스층, 상기 스페이스 로케이션층 및 상기 타임 소스층은, 서로 독립된 개체이고, 상기 시나리오에근거하여 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유하고,상기 배우 소스층은, 배우(Actors), 노트(Note)를 포함하고,상기 스페이스 로케이션층은, 실내(IN)실외(OUT), 장소(Location), 실제주소, 로케이션 사이즈(x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate)를 포함하고,상기 타임 소스층은, 타임코드(Timecode), 낮(Day)밤(Night), 촬영시간(Time), 소요시간(Time Frame)을 포함하는, 연출 연기 영상 분석 평가 시스템."}
{"patent_id": "10-2021-0103318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 필름 슈팅에 따른 항목은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 타임코드(Timecode), 낮(Day)밤(Night), 실내(IN)실외(OUT), 장소(Location), 실제주소, 배우(Actors), 노트(Note), 로케이션 사이즈(x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate), 촬영시간(Time), 소요시간(Time Frame)을 포함하는, 연출 연기 영상 분석 평가 시스템."}
{"patent_id": "10-2021-0103318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,상기 분석 평가부는, 상기 분류된 각 배우들에 대하여 상기 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 감정에 대하여 화남(Angry), 혐오(Disgust), 공포(Fear), 행복(Happy), 슬픔(Sad), 놀람(Surprise), 중립(Neutral), 감정의 격함(Arousal), 밸공개특허 10-2022-0167727-3-런스(Valence: 감정의 부정, 긍정). 집중력(Attention)의 항목에 따라 % 단위로 평가하고, 상기 감정의 격함(Arousal) 및 상기 밸런스(Valence: 감정의 부정, 긍정)에 대한 제로(0) 값을 50%로 설정하는, 연출 연기 영상분석 평가 시스템."}
{"patent_id": "10-2021-0103318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서,상기 분석 평가부는, 상기 분류된 각 배우들에 대하여, 상기 시나리오에 근거하여 장면(Scene), 샷(shot) 수,대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 행동을 상기 대사와 맞물려 전달하는지에 따라 평가하되, 상기 분석 평가부는, 상기 도출된 각 배우의 행동에 근거하여, 발음의 정확성에 대해 0점 내지 5점 중 하나로평가하고, 상기 대사의 전달력에 대해 0점 내지 5점 중 하나로 평가하고, 행동과 대사가 상황과 감정에 부합하는가에 따른 표현력에 대해 0점 내지 5점 중 하나로 평가하는, 연출 연기 영상 분석 평가 시스템."}
{"patent_id": "10-2021-0103318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "(a) 영상 입력부에서 드라마나 영화에 따라 제작된 영상을 입력받는 단계;(b) 필름 슈팅부에서 상기 입력받은 영상을 시나리오(scenario)에 따라 배우 소스층, 스페이스 로케이션층, 및타임 소스층을 생성하는 단계;(c) 인공지능 학습부에서 상기 입력받은 영상에 대하여 상기 시나리오에 근거하여 필름 슈팅에 따른 항목으로분류하고, 분류된 각 항목의 데이터를 딥러닝 학습하여 분석 평가 모델을 생성하는 단계;(d) 분석 평가부에서 상기 배우 소스층, 상기 스페이스 로케이션층 및 상기 타임 소스층을 각각 상기 분석 평가모델에 투입하여 상기 시나리오에 따른 배우들(Actors)을 분류하고, (e) 상기 분석 평가부에서 상기 분류된 각 배우들에 대하여 장면(Scene), 샷(shot) 수, 대사(Dialogue), 배우의감정, 행동(Action)을 도출하고, 도출된 각 배우의 감정 및 행동을 분석하여 평가를 실행하는 단계; 및(f) 상기 분석 평가부에서 상기 실행된 평가 결과를 출력하는 단계;를 포함하는 연출 연기 영상 분석 평가 방법."}
{"patent_id": "10-2021-0103318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 배우 소스층은 배우(Actor) 관련 데이터를 포함하고, 상기 스페이스 로케이션층은 장소 관련 데이터를 포함하고, 상기 타임 소스층은 시간 관련 데이터를 포함하고, 상기 배우 소스층, 상기 스페이스 로케이션층 및 상기 타임 소스층은, 서로 독립된 개체이고, 상기 시나리오에근거하여 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유하는, 연출 연기 영상 분석평가 방법."}
{"patent_id": "10-2021-0103318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서,상기 배우 소스층은, 배우(Actors), 노트(Note)를 포함하고,상기 스페이스 로케이션층은, 실내(IN)실외(OUT), 장소(Location), 실제주소, 로케이션 사이즈(x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate)를 포함하고,공개특허 10-2022-0167727-4-상기 타임 소스층은, 타임코드(Timecode), 낮(Day)밤(Night), 촬영시간(Time), 소요시간(Time Frame)을 포함하는, 연출 연기 영상 분석 평가 방법."}
{"patent_id": "10-2021-0103318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 6 항에 있어서,상기 (e) 단계에서 상기 분석 평가부는, 상기 분류된 각 배우들에 대하여 상기 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 감정에 대하여 화남(Angry), 혐오(Disgust), 공포(Fear), 행복(Happy), 슬픔(Sad), 놀람(Surprise), 중립(Neutral), 감정의 격함(Arousal), 밸런스(Valence: 감정의 부정, 긍정). 집중력(Attention)의 항목에 따라 % 단위로 평가하고, 상기 감정의 격함(Arousal) 및 상기 밸런스(Valence: 감정의 부정, 긍정)에 대한 제로(0) 값을 50%로 설정하는, 연출 연기 영상 분석 평가 방법."}
{"patent_id": "10-2021-0103318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 6 항에 있어서,상기 (e) 단계에서 상기 분석 평가부는, 상기 분류된 각 배우들에 대하여, 상기 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 행동을 상기 대사와 맞물려 전달하는지에 따라 평가하되, 상기 분석 평가부는, 상기 도출된 각 배우의 행동에 근거하여, 발음의 정확성에 대해 0점 내지 5점 중 하나로평가하고, 상기 대사의 전달력에 대해 0점 내지 5점 중 하나로 평가하고, 행동과 대사가 상황과 감정에 부합하는가에 따른 표현력에 대해 0점 내지 5점 중 하나로 평가하는, 연출 연기 영상 분석 평가 방법."}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 드라마나 영화 등의 작품 영상에서 장소와 더불어 연기 시간 등을 확인하고, 배우의 연기에 대하여 대 사, 표정, 행동 등을 분석하여, 발음이나 전달력, 표현력 등을 종합적으로 평가할 수 있도록 하는, 연출 연기 영 상 분석 평가 시스템 및 방법에 대하여 개시한다. 본 발명에 따른 연출 연기 영상 분석 평가 시스템은, 입력된 영상을 시나리오(scenario)에 따라 분류하여 배우 소스층(Actor Source Layer), 스페이스 로케이션층(Space Location Layer), 타임 소스층(Time Source Layer)으 로 생성하고, 생성된 각 층을 분석 평가 모델에 투입하여 시나리오에 따른 배우들(Actors)을 분류하고, 분류된 각 배우들에 대하여 장면(Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정(emotion), 행동(Action)을 도출하 고, 도출된 각 배우의 감정 및 행동을 분석하여 평가를 실행하는 것을 특징으로 한다."}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 연출 연기 영상 분석 평가 시스템 및 방법에 관한 것으로서, 보다 상세하게는 드라마나 영화 등의 작 품 영상에서 장소와 더불어 연기 시간 등을 확인하고, 배우의 연기에 대하여 대사, 표정, 행동 등을 분석하여, 발음이나 전달력, 표현력 등을 종합적으로 평가할 수 있도록 하는, 연출 연기 영상 분석 평가 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 한류문화 등으로 국내 드라마에 대한 인기가 전세계적으로 확산되고 있다. 이러한 드라마를 제작하기 위해 서는 연기하는 배우뿐만 아니라 다양한 장소의 배경, 소품, 조명, 카메라 등의 전문 장비도 필요하다. 한편, 영화에서도 '기생충'과 같이 국제적으로 권위 있는 상을 수상함에 따라 국내의 영화 수준이 높아지고 그 에 따라 세계인들의 관심이 집중되고 있다. 이러한 영화 한 편을 제작하기 위해서는 시나리오에 따라 장소와 더불어 연기 시간 등을 검토하고, 배우들과 소 품, 조명, 카메라, 음악 등이 필요함에 따라 인건비, 교통비 등과 같은 다양한 제작 비용이 소요된다. 그런데, 영화나 드라마 등을 제작할 때, 동일한 장르나 동일한 소재의 작품을 제작하게 되는 경우에, 이전에 제 작했던 작품에 대한 구체적인 정보들이 자료화 되어 있지 않아 처음부터 다시 시작해야 한다. 따라서, 작품에 소요되는 소품이나 장소, 배우들의 연기 등의 정보가 축적되어 있지 않음에 따라 새로운 작품을 제작할 때 여러가지 시행착오를 겪게 되고, 그에 따른 비용도 많이 소모되는 문제점이 있다. 선행기술문헌특허문헌 (특허문헌 0001) 관련 선행 특허 문헌으로는 대한민국 공개특허공보 제10-1994437호(2019.06.24. 등록)가 있으 며, 상기 문헌에는 스마트 연기영상 제작 시스템 및 그 방법이 기재되어 있다."}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 드라마나 영화 등의 작품 영상에서 장소와 더불어 연기 시간 등을 확인하고, 배우의 연기에 대하여 대사, 표정, 행동 등을 분석하여, 발음이나 전달력, 표현력 등을 종합적으로 평가할 수 있도록 하는, 연 출 연기 영상 분석 평가 시스템 및 방법을 제공하는 것이다."}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "전술한 목적을 달성하기 위한 본 발명의 실시예에 따른 연출 연기 영상 분석 평가 시스템은, 드라마나 영화에 따라 제작된 영상을 입력하는 영상 입력부; 상기 영상 입력부를 통해 입력된 영상을 시나리오(scenario)에 따라 배우(Actor) 관련 데이터로 이루어진 배우 소스층(Actor Source Layer), 장소 관련 데이터로 이루어진 스페이스 로케이션층(Space Location Layer), 시간 관련 데이터로 이루어진 타임 소스층(Time Source Layer)을 생성하는 필름 슈팅부(Film Shooting Part); 상기 영상 입력부를 통해 입력된 영상에 대하여 상기 시나리오에 근거하여 필름 슈팅에 따른 항목으로 분류하고, 분류된 각 항목의 데이터를 딥러닝 학습하여 분석 평가 모델을 생성하는 인공지능 학습부; 상기 필름 슈팅부를 통해 생성된 상기 배우 소스층, 상기 스페이스 로케이션층 및 상기 타임 소스층을 각각 상기 분석 평가 모델에 투입하여 상기 시나리오에 따른 배우들(Actors)을 분류하고, 분류된 각 배우들에 대하여 장면(Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 감정 및 행동을 분석하여 평가를 실행하는 분석 평가부; 및 상기 도출된 각 배우의 감정 및 행동에 대한 평가 결과를 화면이나 음향으로 출력하는 출력부를 포함할 수 있다. 상기 배우 소스층, 상기 스페이스 로케이션층 및 상기 타임 소스층은, 서로 독립된 개체이고, 상기 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유할 수 있다. 상기 배우 소스층은 기저층에 위치하고, 상기 배우 소스층 위에 상기 타임 소스층이 위치하고, 상기 타임 소스 층 위에 상기 스페이스 로케이션층이 위치할 수 있다. 상기 배우 소스층, 상기 스페이스 로케이션층 및 상기 타임 소스층은 관통 홀(Through Hole)을 통하여 장면 (Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유할 수 있다. 상기 배우 소스층은, 배우(Actors), 노트(Note)를 포함 할 수 있다. 상기 스페이스 로케이션층은, 실내(IN)실외(OUT), 장소(Location), 실제주소, 로케이션 사이즈(x, y), 동선 좌 표(Actor Motion), 배우위치(x, y Coordinate)를 포함 할 수 있다. 상기 타임 소스층은, 타임코드(Timecode), 낮(Day)밤(Night), 촬영시간(Time), 소요시간(Time Frame)을 포함 할 수 있다. 상기 필름 슈팅에 따른 항목은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 타임코드(Timecode), 낮 (Day)밤(Night), 실내(IN)실외(OUT), 장소(Location), 실제주소, 배우(Actors), 노트(Note), 로케이션 사이즈 (x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate), 촬영시간(Time), 소요시간(Time Frame)을 포함 할 수 있다. 또한, 드라마나 영화에 따라 제작된 영상에 대한 각 시나리오에 따라 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 타임코드(Timecode), 낮(Day)밤(Night), 실내(IN)실외(OUT), 장소(Location), 실제주소, 배우 (Actors), 노트(Note), 로케이션 사이즈(x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate), 촬영시 간(Time), 소요시간(Time Frame), 배우 대사, 배우의 감정, 배우 행동, 감정 평가, 행동 평가, 발음평가, 전달 력, 표현력, 감정분류, 감정조합, 출연빈도, 종합평가를 포함하는 정보를 저장하고 있는 연기분석 데이터베이스 를 더 포함할 수 있다.상기 분석 평가부는, 상기 분류된 각 배우들에 대하여 상기 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 대 사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 감정에 대하여 화남(Angry), 혐오 (Disgust), 공포(Fear), 행복(Happy), 슬픔(Sad), 놀람(Surprise), 중립(Neutral), 감정의 격함(Arousal), 밸 런스(Valence: 감정의 부정, 긍정). 집중력(Attention)의 항목에 따라 % 단위로 평가할 수 있다. 상기 분석 평가부는, 상기 감정의 격함(Arousal) 및 상기 밸런스(Valence: 감정의 부정, 긍정)에 대한 제로 값을 50%로 설정 할 수 있다. 상기 분석 평가부는, 상기 도출된 각 배우의 감정을 1 단계 2 단계 3 단계 - 4단계로 구분하여, 각각 굳어짐- 분개-분노-광분으로 분류하고, 멸시-반감-혐오-극혐오로 분류하고, 걱정-불안-두려움-경악으로 분류하고, 만족- 즐김-기쁨-폭소로 분류하고, 낙담-우울-슬픔-비통으로 분류하고, 경계-으아함-놀람-충격으로 분류하여 평가 할 수 있다. 상기 분석 평가부는, 상기 도출된 각 배우의 감정에서 두려움과 기쁨을 조합하여 필사적임으로 평가하고, 두려 움과 슬픔을 조합하여 처참함으로 평가하고, 두려움과 놀람을 조합하여 겁먹음으로 평가하고, 기쁨과 슬픔을 조 합하여 희미한 희망으로 평가하고, 기쁨과 놀람을 조합하여 경이로움으로 평가 할 수 있다. 상기 분석 평가부는, 상기 도출된 각 배우의 감정에서, 고통스러움, 괴로움, 구역질남, 귀찮음, 근심, 끔찍함, 몸서리치는, 무정함, 미움, 부담감, 서운함, 싫음, 싫증, 쌀쌀함, 야속함, 얄미움, 억울함, 원망감, 죄스럼, 죄 책감, 증오감, 지겨움, 짜증남, 차가움, 황량감, 반감, 격변, 냉소, 경멸에 대해서 역겨움으로 분류 할 수 있다. 상기 분석 평가부는, 가혹함, 고통스러움, 골치아픔, 괘씸함, 구역질남, 기분상함, 꼴사나움, 끓어오름, 나쁜, 노여움, 떫음, 모욕감, 무서움, 배반감, 복수심, 북받침, 분개함, 분노, 불만감, 불쾌감, 섬??함, 소름끼침, 속 상함, 숨막힘, 실망감, 쓰라림, 씁쓸함, 약오름, 원한, 적의, 신경질, 호됨, 신랄함, 매서움, 적개심에 대해서 분노로 분류 할 수 있다. 상기 분석 평가부는, 가슴아픔, 걱정, 고단함, 고독감, 고민감, 공포감, 공허감, 괴로움, 구슬픔, 권태감, 근심, 기분나쁨, 낙담, 두려움, 마음이무거운, 멍한, 뭉클함, 미어짐, 부끄러움, 불쌍한, 불안한, 불편한, 비참 한, 비탄함, 서글픔, 암담한, 앞이깜깜한, 애석함, 애처로움, 애태움, 애통함, 언짢음, 염려감, 낙담, 의기소침 에 대해서 슬픔으로 분류 할 수 있다. 상기 분석 평가부는, 불안, 우려, 신경 과민, 깜짝놀람, 경악, 의심, 의혹에 대해서 공포로 분류 할 수 있다. 상기 분석 평가부는, 충격, 놀람, 경이, 감탄에 대해서 놀람으로 분류 할 수 있다. 상기 분석 평가부는, 간절, 갈망, 기대, 바람, 소망, 애끓음, 절박감, 찝찝함, 초라함, 초조함, 호기심, 후회감, 희망감에 대해서 바램으로 분류 할 수 있다. 상기 분석 평가부는, 감격, 감동, 감사, 고무됨, 기쁨, 고전적, 날아갈듯한, 들뜬, 가벼운, 눈물겨운, 든든한, 만족스러운, 뭉클한, 반가움, 벅참, 뿌듯함, 살맛남, 시원함, 싱그러움, 좋음, 짜릿함, 쾌적함, 통쾌감, 포근함, 푸근함, 행복감, 환상적임, 후련함, 흐뭇함, 흔쾌함, 흥분됨, 유쾌함, 자신감, 편안함, 활기, 활발한, 희망참, 경쾌함, 구원, 만족, 자존심에 대해서 기쁨으로 분류 할 수 있다. 상기 분석 평가부는, 수용, 우정, 믿음, 친절, 친근감, 헌신, 동경, 열중, 감미로움, 그리움, 다정함, 따사로움, 묘함, 뿌듯함, 사랑스러움, 순수함, 애틋함, 열렬함, 열망감, 친숙함, 포근함, 호감에 대해서 사랑으 로 분류하여 평가 할 수 있다. 상기 분석 평가부는, 상기 분류된 각 배우들에 대하여, 상기 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 행동을 상기 대사와 맞물려 전달하 는지에 따라 평가 할 수 있다. 상기 분석 평가부는, 상기 도출된 각 배우의 행동에 근거하여, 발음의 정확성에 대해 0점 내지 5점 중 하나로 평가하고, 상기 대사의 전달력에 대해 0점 내지 5점 중 하나로 평가하고, 행동과 대사가 상황과 감정에 부합하 는가에 따른 표현력에 대해 0점 내지 5점 중 하나로 평가 할 수 있다. 한편, 전술한 목적을 달성하기 위한 본 발명의 실시예에 따른 연출 연기 영상 분석 방법은, (a) 인공지능 학습 부가 연기분석 데이터베이스에 저장되어 있는 영상에 대한 정보를 딥러닝 학습하여 분석평가 모델을 생성하는 단계; (b) 영상 입력부가 드라마나 영화에 따라 제작된 영상을 입력받는 단계; (c) 분석 평가부가 상기 입력받은 영상에 대하여, 상기 분석평가 모델에 근거하여 장면, 행동, 대사, 표정에 따라 배우의 발음, 전달력, 표현 력을 분석하는 단계; 및 (d) 분석 평가부가 상기 분석된 결과에 근거하여 각 배우의 대사 키워드, 평균 감정 수 치, 주된 감정, 잘 표현된 감정, 발음, 전달력, 표현력 및 출연빈도를 포함하는 종합 평가를 산출하여 출력하는 단계를 포함 할 수 있다. 한편, 전술한 목적을 달성하기 위한 본 발명의 실시예에 따른 연출 연기 영상 분석 평가 방법은, (a) 영상 입력 부에서 드라마나 영화에 따라 제작된 영상을 입력받는 단계; (b) 필름 슈팅부에서 상기 입력받은 영상을 시나리 오(scenario)에 따라 배우 소스층, 스페이스 로케이션층, 및 타임 소스층을 생성하는 단계; (c) 인공지능 학습 부에서 상기 입력받은 영상에 대하여 상기 시나리오에 근거하여 필름 슈팅에 따른 항목으로 분류하고, 분류된 각 항목의 데이터를 딥러닝 학습하여 분석 평가 모델을 생성하는 단계; (d) 분석 평가부에서 상기 배우 소스층, 상기 스페이스 로케이션층 및 상기 타임 소스층을 각각 상기 분석 평가 모델에 투입하여 상기 시나리오에 따른 배우들(Actors)을 분류하는 단계; (e) 상기 분석 평가부에서 상기 분류된 각 배우들에 대하여 장면(Scene), 샷 (shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 감정 및 행동을 분석하 여 평가를 실행하는 단계; 및 (f) 상기 분석 평가부에서 상기 실행된 평가 결과를 출력하는 단계를 포함할 수 있다. 상기 배우 소스층은 배우(Actor) 관련 데이터를 포함하고, 상기 스페이스 로케이션층은 장소 관련 데이터를 포 함하고, 상기 타임 소스층은 시간 관련 데이터를 포함 할 수 있다. 상기 배우 소스층, 상기 스페이스 로케이션층 및 상기 타임 소스층은, 서로 독립된 개체이고, 상기 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유 할 수 있다. 상기 배우 소스층은 기저층에 위치하고, 상기 배우 소스층 위에 상기 타임 소스층이 위치하고, 상기 타임 소스 층 위에 상기 스페이스 로케이션층이 위치 할 수 있다. 상기 배우 소스층, 상기 스페이스 로케이션층 및 상기 타임 소스층은 관통 홀(Through Hole)을 통하여 장면 (Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유 할 수 있다. 상기 배우 소스층은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 배우(Actors), 노트(Note)를 포함 할 수 있다. 상기 스페이스 로케이션층은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 실내(IN)실외(OUT), 장소 (Location), 실제주소, 로케이션 사이즈(x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate)를 포함 할 수 있다. 상기 타임 소스층은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 타임코드(Timecode), 낮(Day)밤 (Night), 촬영시간(Time), 소요시간(Time Frame)을 포함 할 수 있다. 상기 필름 슈팅에 따른 항목은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 타임코드(Timecode), 낮 (Day)밤(Night), 실내(IN)실외(OUT), 장소(Location), 실제주소, 배우(Actors), 노트(Note), 로케이션 사이즈 (x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate), 촬영시간(Time), 소요시간(Time Frame)을 포함 할 수 있다. 상기 (e) 단계에서 상기 분석 평가부는, 상기 분류된 각 배우들에 대하여 상기 시나리오에 근거하여 장면 (Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 감정에 대 하여 화남(Angry), 혐오(Disgust), 공포(Fear), 행복(Happy), 슬픔(Sad), 놀람(Surprise), 중립(Neutral), 감 정의 격함(Arousal), 밸런스(Valence: 감정의 부정, 긍정). 집중력(Attention)의 항목에 따라 % 단위로 평가 할 수 있다. 상기 (e) 단계에서 상기 분석 평가부는, 상기 감정의 격함(Arousal) 및 상기 밸런스(Valence: 감정의 부정, 긍 정)에 대한 제로 값을 50%로 설정 할 수 있다. 상기 (e) 단계에서 상기 분석 평가부는, 상기 도출된 각 배우의 감정을 1 단계 2 단계 3 단계 - 4단계로 구분 하여, 각각 굳어짐-분개-분노-광분으로 분류하고, 멸시-반감-혐오-극혐오로 분류하고, 걱정-불안-두려움-경악으 로 분류하고, 만족-즐김-기쁨-폭소로 분류하고, 낙담-우울-슬픔-비통으로 분류하고, 경계-으아함-놀람-충격으로 분류하여 평가 할 수 있다. 상기 (e) 단계에서 상기 분석 평가부는, 상기 도출된 각 배우의 감정에서 두려움과 기쁨을 조합하여 필사적임으 로 평가하고, 두려움과 슬픔을 조합하여 처참함으로 평가하고, 두려움과 놀람을 조합하여 겁먹음으로 평가하고, 기쁨과 슬픔을 조합하여 희미한 희망으로 평가하고, 기쁨과 놀람을 조합하여 경이로움으로 평가 할 수 있다. 상기 (e) 단계에서 상기 분석 평가부는, 상기 분류된 각 배우들에 대하여, 상기 시나리오에 근거하여 장면 (Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 행동을 상 기 대사와 맞물려 전달하는지에 따라 평가 할 수 있다. 상기 (e) 단계에서 상기 분석 평가부는, 상기 도출된 각 배우의 행동에 근거하여, 발음의 정확성에 대해 0점 내 지 5점 중 하나로 평가하고, 상기 대사의 전달력에 대해 0점 내지 5점 중 하나로 평가하고, 행동과 대사가 상황 과 감정에 부합하는가에 따른 표현력에 대해 0점 내지 5점 중 하나로 평가할 수 있다."}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의하면, 드라마나 영화에 출연한 배우들의 연기력 평가를 참고하여 새로운 작품을 제작할 때 용이하 게 적용할 수 있다. 또한, 본 발명에 의하면, 작품에 소요되는 소품이나 장소, 배우들의 연기 등의 정보가 축적됨에 따라 새로운 작 품을 제작할 때 여러가지 시행착오를 겪지 않아도 되고, 그에 따른 비용도 많이 절약할 수 있다."}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "그리고, 본 발명의 효과는 상술된 것에 국한되지 않고 후술하는 본 발명의 구성으로부터 도출될 수 있는 다른"}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "효과도 본 발명의 효과에 포함된다."}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예를 참조하면 명확해질 것이다. 그러나, 본 발명은 이하에서 개시되는 실시예에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예는 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구 항의 범주에 의해 정의될 뿐이다. 따라서, 몇몇 실시예에서, 잘 알려진 공정 단계들, 잘 알려진 소자 구조 및 잘 알려진 기술들은 본 발명이 모호하게 해석되는 것을 피하기 위하여 구체적으로 설명되지 않는다. 명세서 전 체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 도면에서 여러 층 및 영역을 명확하게 표현하기 위하여 두께를 확대하여 나타내었다. 명세서 전체를 통하여 유 사한 부분에 대해서는 동일한 도면 부호를 붙였다. 층, 막, 영역, 판 등의 부분이 다른 부분 \"위에\" 있다고 할 때, 이는 다른 부분 \"바로 위에\" 있는 경우뿐 아니라 그 중간에 또 다른 부분이 있는 경우도 포함한다. 반대로 어떤 부분이 다른 부분 \"바로 위에\" 있다고 할 때에는 중간에 다른 부분이 없는 것을 뜻한다. 또한, 층, 막, 영 역, 판 등의 부분이 다른 부분 \"아래에\" 있다고 할 때, 이는 다른 부분 \"바로 아래에\" 있는 경우뿐 아니라 그 중간에 또 다른 부분이 있는 경우도 포함한다. 반대로 어떤 부분이 다른 부분 \"바로 아래에\" 있다고 할 때에는 중간에 다른 부분이 없는 것을 뜻한다. 공간적으로 상대적인 용어인 \"아래(below)\", \"아래(beneath)\", \"하부(lower)\", \"위(above)\", \"상부(upper)\" 등 은 도면에 도시되어 있는 바와 같이 하나의 소자 또는 구성 요소들과 다른 소자 또는 구성 요소들과의 상관관계 를 용이하게 기술하기 위해 사용될 수 있다. 공간적으로 상대적인 용어는 도면에 도시되어 있는 방향에 더하여 사용시 또는 동작시 소자의 서로 다른 방향을 포함하는 용어로 이해되어야 한다. 예를 들면, 도면에 도시되어 있는 소자를 뒤집을 경우, 다른 소자의 \"아래(below)\"또는 \"아래(beneath)\"로 기술된 소자는 다른 소자의 \"위 (above)\"에 놓여질 수 있다. 따라서, 예시적인 용어인 \"아래\"는 아래와 위의 방향을 모두 포함할 수 있다. 소자 는 다른 방향으로도 배향될 수 있고, 이에 따라 공간적으로 상대적인 용어들은 배향에 따라 해석될 수 있다. 본 명세서에서 어떤 부분이 다른 부분과 연결되어 있다고 할 때, 이는 직접적으로 연결되어있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 전기적으로 연결되어 있는 경우도 포함한다. 또한, 어떤 부분이 어떤 구성 요소를 포함한다고 할 때, 이는 특별히 그에 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 본 명세서에서 제 1, 제 2, 제 3 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 이러한 구성 요소들은 상기 용어들에 의해 한정되는 것은 아니다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소들로부 터 구별하는 목적으로 사용된다. 예를 들어, 본 발명의 권리 범위로부터 벗어나지 않고, 제 1 구성 요소가 제 2 또는 제 3 구성 요소 등으로 명명될 수 있으며, 유사하게 제 2 또는 제 3 구성 요소도 교호적으로 명명될 수 있 다. 다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어(기술 및 과학적 용어를 포함)는 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 사용될 수 있을 것이다. 또 일반적으로 사용되는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않은 한 이상적으로 또는 과도하게 해 석되지 않는다. 이하 첨부된 도면을 참조하여 본 발명의 바람직한 실시예에 따른 연출 연기 영상 평가 분석 시스템에 관하여 상 세히 설명하면 다음과 같다. 도 1은 본 발명의 실시예에 따른 연출 연기 영상 평가 분석 시스템의 구성을 개략적으로 나타낸 구성도이다. 도 1을 참조하면, 본 발명의 실시예에 따른 연출 연기 영상 분석 평가 시스템은, 영상 입력부, 필름 슈팅부(Film Shooting Part), 인공지능(AI) 학습부, 분석 평가부, 및 출력부 등을 포함할 수 있다. 여기서, 연기분석 데이터베이스(DB)를 더 포함할 수 있다. 연기분석 DB는 드라마나 영화에 따라 제작된 작품 영 상에 대한 시나리오, 대본, 샷(shot), 샷 시간(shot length), 타임코드(Timecode), 밤(Day)/낮(Night), 실내 (IN)/실외(OUT), 장소(Location), 실제주소, 배우(Actors), 로케이션 사이즈(x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate), 촬영시간(Time), 소요시간(Time Frame), 배우 대사, 배우의 감정, 배우 행동, 감 정 평가, 행동 평가, 발음평가, 전달력, 표현력, 감정분류, 감정조합, 출연빈도, 종합평가를 포함하는 정보를 저장하고 있다. 영상 입력부는 드라마나 영화에 따라 제작된 작품 영상을 입력한다. 필름 슈팅부는 영상 입력부를 통해 입력된 영상을 시나리오(scenario)에 따라 배우(Actor) 관련 데이 터로 이루어진 배우 소스층(Actor Source Layer), 장소 관련 데이터로 이루어진 스페이스 로케이션층 (Space Location Layer), 시간 관련 데이터로 이루어진 타임 소스층(Time Source Layer)을 생성한다. 즉, 필름 슈팅부는 입력된 영상을 시나리오에 근거하여 영상 데이터, 음향 데이터, 대사, 배우, 장소, 시간, 행동 등으로 분류하여 배우 소스층, 스페이스 로케이션층 및 타임 소스층으로 생성 하는 것이다. 여기서, 배우 소스층, 스페이스 로케이션층 및 타임 소스층은, 서로 독립된 개체이지만 장면 (Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유하여, 서로 유기적으로 연결될 수 있다. 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유함에 따라, 배우 소스층은 배우 (Actors), 노트(Note)를 포함하고, 스페이스 로케이션층은 실내(IN)실외(OUT), 장소(Location), 실제주소, 로케이션 사이즈(x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate)를 포함하고, 타임 소 스층은 타임코드(Timecode), 낮(Day)밤(Night), 촬영시간(Time), 소요시간(Time Frame)을 포함 할 수 있 다. AI 학습부는 연기분석 DB에 저장된 정보를 딥러닝 학습하여 분석평가 모델을 생성한다. 즉, AI 학습부 는 영상 입력부를 통해 입력된 영상에 대하여 시나리오에 근거하여 필름 슈팅에 따른 항목으로 분류 하고, 분류된 각 항목의 데이터를 딥러닝 학습하여 분석 평가 모델을 생성한다. 필름 슈팅에 따른 항목은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 타임코드(Timecode), 낮(Day)밤 (Night), 실내(IN)실외(OUT), 장소(Location), 실제주소, 배우(Actors), 노트(Note), 로케이션 사이즈(x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate), 촬영시간(Time), 소요시간(Time Frame)을 포함 할 수 있다. 여기서, 필름 슈팅에 따른 각 항목은 독립된 오브젝트(Object)로 존재할 수 있고, 육면체 또는 구 등의 형상을 가질 수 있다. 각 오브젝트는 블록체인(Block Chain)으로 서로 연결될 수 있다. 분석 평가부는 영상 입력부를 통해 입력된 작품 영상에 대하여, 장면, 행동, 대사, 표정에 따라 배우 의 발음, 전달력, 표현력을 분석하여 평가한다. 즉, 분석 평가부는 필름 슈팅부를 통해 생성된 배우 소스층, 스페이스 로케이션층 및 타임 소스층을 각각 분석 평가 모델에 투입하여 시나리오에 따른 배우들(Actors)을 분류하고, 분류된 각 배우들 에 대하여 장면(Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배 우의 감정 및 행동을 분석하여 평가를 실행하는 것이다. 출력부는 분석 평가부를 통해 분석된 결과를 화면이나 음성 등으로 출력할 수 있다. 즉, 출력부(15 0)는 도출된 각 배우의 감정 및 행동에 대한 평가 결과를 화면이나 음향으로 출력하는 것이다. 도 2는 본 발명의 실시예에 따른 필름 슈팅부의 구성을 개략적으로 나타낸 구성도이다. 도 2를 참조하면, 본 발명의 실시예에 따른 필름 슈팅부는, 배우 소스층, 스페이스 로케이션층 및 타임 소스층을 포함한다. 배우 소스층은 배우(Actor) 관련 데이터를 포함하고, 스페이스 로케이션층은 장소 관련 데이터를 포 함하고, 타임 소스층은 시간 관련 데이터를 포함한다. 배우 소스층은 기저층에 위치하고, 배우 소스층 위에 스페이스 로케이션층이 위치하고, 스페이 스 로케이션층 위에 타임 소스층이 위치 할 수 있다. 또한, 배우 소스층은 기저층에 위치하고, 배우 소스층 위에 타임 소스층이 위치하고, 타임 소스 층 위에 스페이스 로케이션층이 위치 할 수 있다. 배우 소스층, 스페이스 로케이션층 및 타임 소스층은, 서로 독립된 개체이고, 시나리오에 근거 하여 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유할 수 있다. 즉, 배우 소스층 , 스페이스 로케이션층 및 타임 소스층은, 관통 홀(Through Hole)을 통하여 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유할 수 있다. 여기서, 관통 홀은 배우 소스층 , 스페이스 로케이션층 및 타임 소스층을 관통하여 각 층의 데이터를 서로 주고 받을 수 있는 공유 통로이다.여기서, 서로 공유되는 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)에 대한 데이터는 각 각 오브젝트로 존재할 수 있으며, 각 오브젝트는 육면체 또는 구 형상을 가질 수 있다. 배우 소스층, 스페이스 로케이션층 및 타임 소스층은, 서로 공유되는 오브젝트를 통하여 서로 연결되거나, 별도의 블록체인(Block Chain)을 통하여 서로 연결될 수 있다. 이때, 블록체인은 배우 소스층 , 스페이스 로케이션층 및 타임 소스층에 관련된 연관어로 이루어진 오브젝트일 수 있다. 배우 소스층은, 배우(Actors), 노트(Note)를 포함할 수 있다. 스페이스 로케이션층은, 실내(IN)실외(OUT), 장소(Location), 실제주소, 로케이션 사이즈(x, y), 동선 좌 표(Actor Motion), 배우위치(x, y Coordinate)를 포함할 수 있다. 타임 소스층은, 타임코드(Timecode), 낮(Day)밤(Night), 촬영시간(Time), 소요시간(Time Frame)을 포함할 수 있다. 도 3은 본 발명의 실시예에 따른 분석 평가부에서 필름 슈팅에 따라 영화 장면을 분석한 예를 나타낸 도면이다. 도 3을 참조하면, 본 발명에 따른 분석 평가부는, 예를 들면, 2019년에 개봉된 '기생충' 영화 장면에서 161개 시나리오 씬(scenario Scenes)에 대하여, 각 씬 별로 샷(shot), 샷 길이(shot lenth)(초), 타임코드 (TimeCode)(초), 밤/낮(D/N), 실외/실내(IN/OUT), 장소(Location), 실제 주소, 배우(Actors), 노트(Note), 로 케이션 사이즈(Location Size)(x, y), 동선 움직임 좌표(Actor Motion), 배우 위치(x, y coordinate), 촬영 시 간(Time), 소요 시간(Time Frame) 등으로 분석할 수 있다. 예를 들어, 시나리오 2번 씬의 1 샷(shot)은 분석 평가부에서 샷 길이(shot lenth): 42.9, 타임코드: 42.9, 밤/낮(D/N): 낮(D), 실외/실내(IN/OUT): 실내(IN), 장소: 반지하, 실제 주소: 고양 아쿠아 스튜디오, 배 우: 기우, 소요 시간은 1 Frame 등으로 분석할 수 있다. 여기서, 배우(Actors)의 경우는 보조 출연은 제외하고 감독의 의도하에 배치된 인물들은 포함할 수 있다. 로케이션 관련 정보는 예컨대, 기택의 반지하 앞 골목길 세트를 길게 만들어서 촬영하거나, 박사장의 부잣집 세 트에서 현관과 집 진입로는 따로 만들어진 것 등이 될 수 있다. Aspect Ratio는 화면 규격, 화면비 등으로, 예를 들어, 1:1.33, 1:1.37, 1:1.66, 1:1.75, 1:1.85, 1:2.35 등이 될 수 있다. Axis Line은 대화축으로서, 예를 들어, 평면도에서 대화를 나누고 있는 가상적인 선을 대화 축이라고 할 수 있 다. Action Axis는 행동축 선을 나타내고, Imaginary Line은 영상축 선을 나타낸다. Turn around는 나눠 찍기로서, 한쪽 방향에서 필요한 세부 장면을 모두 촬영하고 카메라와 조명을 옮겨서 촬영 을 진행하여 만든 영상을 의미할 수 있다. 또한, 슬로우 모션(Slow motion), 타임 랩스(Time lapse) 등이 있다. Pre-visualization은 사전 시각화로서, 이것에 관련된 자료들을 최대한 많이 수집하는 것을 의미할 수 있다. Time Code는 그 장면의 길이, 시간 등을 나타낸다. KeyKode는 배우가 서있어야 하는 위치를 표시하는 마커이다. 그리고 배우별로 각각 다른 색의 마커를 사용하면 각 영상 내에서 배우의 식별 혼란을 피할 수 있다. 촬영 장소로서, 예를 들면, 망원동 '이모네 기사식당', 자하문 터널입구, 자하문 터널, 성북동 주택가, 고양 아 쿠아 스튜디오, 충정로역 골목 '돼지 쌀 슈퍼', 노량진 스카이피자, 박사장 집 2층 = 안성 DIMA 종합촬영소, 송 파구 방이동 올가홀푸드 방이점 등을 인식할 수 있다. 도 4는 본 발명의 실시예에 따른 AI 학습부의 내부 구성을 개략적으로 나타낸 구성도이다. 도 4를 참조하면, 본 발명의 실시예에 따른 AI 학습부는, 데이터베이스(DB), 입력부, 인공 지능부 및 출력부를 포함할 수 있다. 데이터베이스(DB)는 도 1에서 설명한 바와 같이 작품 영상에 대한 시나리오, 대본, 샷(shot), 샷 시간, 타임코 드, 밤낮, 실내실외, 장소, 실제주소, 배우, 로케이션 사이즈, 동선 좌표, 배우위치, 촬영시간, 소요시간(Frame), 배우 대사, 배우의 감정, 배우 행동, 감정 평가, 행동 평가, 발음평가, 전달력, 표현력, 감정분류, 감 정조합, 출연빈도, 종합평가를 포함하는 정보를 저장할 수 있다. 입력부는 네트워크로 연결되어 있는 영상 제작 장치 또는 케이블로 연결되어 있는 영상 제작 장치로부터 영상(이미지), 음향, 데이터 등을 입력받는다. 예를 들면, 입력부는 영상 장치로부터 '기생충' 영화에 관 한 영상 데이터를 입력받을 수 있다. 인공 지능부는 데이터베이스(DB)에 저장되어 있는 시나리오, 대본, 샷(shot), 샷 시간, 타임코드, 밤낮, 실내실외, 장소, 실제주소, 배우, 로케이션 사이즈, 동선 좌표, 배우위치, 촬영시간, 소요시간(Frame), 배우 대 사, 배우의 감정, 배우 행동, 감정 평가, 행동 평가, 발음평가, 전달력, 표현력, 감정분류, 감정조합, 출연빈도, 종합평가 등을 딥 러닝(Deep Learning) 학습하고, 이에 근거하여 분석평가 모델을 생성하고, 영상 제 작 장치 또는 영상 장치로부터 입력받은 영화 데이터 또는 드라마 데이터를 분석평가 모델에 입력하여 각 배우 별로 종합 평가 결과를 출력한다. 출력부는 인공 지능부에서 생성된 배우 별 종합 평가 결과들을 화면이나 음향 등으로 출력할 수 있다. 도 5는 본 발명의 실시예에 따른 인공 지능부의 기능 블록을 나타낸 구성도이다. 도 5를 참조하면, 본 발명의 실시예에 따른 인공 지능부는, 요소 도출기, 학습 엔진 및 조치기 를 포함할 수 있다. 즉, 본 발명에서는, 구조화된 데이터 및 비구조화된 데이터를 요소 도출기에서 처리하여 요소를 도출하고, 학습 엔진에서 요소를 이용해 자가 적응 학습을 할 수 있으며, 학습 결과를 이용하는 조치기를 포함하 여, 상황 이해 및 스케줄링, 의사결정 및 예측, 추천 및 상황 조치 등을 할 수 있는 시스템을 모듈식으로 제공 할 수 있고, 다양한 상황에 맞는 시스템을 맞춤식으로 제공할 수 있다. 요소 도출기는 입력 데이터를 처리하여 요소(Elements)를 도출할 수 있다. 즉, 요소 도출기는 구조화된 데이터 및 비구조화된 데이터를 포함하는 입력 데이터로부터 학습 엔진의 입력 정보인 요소를 도출할 수 있 다. 요소 도출기는, 텍스트 변환 모듈, 정보 추출 모듈 및 요소 도출 모듈을 포함하여 구성될 수 있다. 텍스트 변환 모듈은, 입력 데이터 중 텍스트를 제외한 비구조화된 데이터를 텍스트 데이터로 변환할 수 있 다(Text Conversion). 특히, 텍스트 변환 모듈은, 텍스트를 제외한 이미지, 영상, 음성을 포함하는 비구조 화된 데이터를 텍스트 데이터로 변환할 수 있다. 정보 추출 모듈은, 텍스트 변환 모듈에서 변환된 텍스트 데이터로부터 정보를 추출할 수 있다 (Information Extraction). 또한, 정보 추출 모듈은, 텍스트 변환 모듈에서의 변환 대상이 아닌, 텍스 트 형태의 입력 데이터로부터도 필요한 정보를 추출할 수 있다. 요소 도출 모듈은, 추출된 정보로부터 학습 엔진에 입력될 요소를 도출할 수 있다(Element Identification & Elicitation). 학습 엔진은, 요소 도출기에서 도출된 요소를 이용하여 DNA 미션(DNA Mission)을 자가 조직하고, 자가 조직된 DNA 미션을 이용하여 딥 러닝 기반의 인공 신경망 DNA 모델(DNA Model)을 자가 구성하며, 자가 구성된 DNA 모델을 학습시킬 수 있다. 본 발명은, 자가 적응 기술과 딥 러닝 기반의 학습 기술을 결합하여, DNA 미션을 자가 조직하고 DNA 모델을 자가 구성하는 학습 엔진을 포함함으로써, 상황을 이해해서 스스로 미션을 파악 하고 모델을 만들어 상황을 해결하는 인간 두뇌 메커니즘을 효과적으로 구현할 수 있다. 도 1에 도시된 바와 같 이, 학습 엔진은, 조직 모듈, 구성 모듈 및 학습 모듈을 포함하여 구성될 수 있다. 조직 모듈은, 요소 도출기에서 도출된 요소를 이용하여 집단별 DNA 미션을 자가 조직할 수 있다(Self- Organization of DNA Mission). 보다 구체적으로는, 조직 모듈은, 시간의 흐름에 따라 입력되는 요소와 미 리 정의된 집단별 조직의 미션 내 요소를 비교 및 평가하여, 시간의 흐름에 따라 변화하는 DNA 미션을 스스로 조직해서 생성할 수 있다. 여기에서, 미션은 미리 정의된 조직의 미션이고, DNA 미션은 본 발명의 조직 모듈 이 자가 조직하는 미션으로 서로 상이하다. 한편, 조직 모듈이 조직하는 집단별 DNA 미션은, 집단별 조직의 블록(Blocks of Organization)과 체인 (Chains)의 콤비네이션일 수 있다. 즉, 조직 모듈은, 뉴로블록체인 콤비네이션(Block Chain Combination)기술을 이용하여, 조직의 블록과 체인을 조합하여 DNA 미션을 조직할 수 있다. 또한, 실시예에 따라서는, DNA 미션은, 체인(Chains)의 콤비네이션으로 구성되는 특수 DNA 미션(Special DNA Mission)을 포함할 수 있다. 또한, DNA 미션은, 미션 모듈의 합으로 구성될 수 있으며, 미션 모듈은 요소 도출기로부터 전달받은 요소와 조직 집단명의 포지션(Positions of Organization)의 함수일 수 있다. 이때, 조직 구성원의 포지션은 미리 정해 질 수 있다. 본 발명에서는, 구현된 인공 지능부가 인간의 임무(미션)를 지원하도록 하기 위하여, 연기 영상에 대하여 배우의 대사, 감정, 행동, 발음, 전달력, 표현력의 포지션을 고려할 수 있다. 이러한 조직은 예를 들면, 계층형 트리 구조의 형태를 띠거나 병렬형 구조를 띨 수 있으며, 계층형 트리 구조에서는 노드와 노드 간 연결(즉, 집 단 내에서의 포지션과 포지션 간 연결(체인))이 되어 있고, 이를 조직 전체로 확장해 보면 일정 트리 구조(블록)로 구성된 그룹(조직 내 지침) 간의 연결 즉, 체인으로 볼 수 있다. 조직 모듈에서 뉴로블록체인 콤비네이션 기술을 이용할 때, 체인이란 작품 내에서 배우의 장소와 연기 간 연결 그리고 연기 내에서 행동과 감정 간 연결이고, 조직의 블록이란 배우의 연기 내에서의 포지션과 포지션이 서로 연결되어 모여 있는 그룹으로 하나의 장면 연기가 하나의 블록으로 구성되거나 또는 하나의 장면이 여러 개의 블록으로 구성될 수 있다. 도 6은 본 발명의 실시예에 따른 뉴로블록체인 콤비네이션을 이용하여, 조직 모듈에서 DNA 미션을 자가 조직하 는 방법을 예를 들어 도시한 도면이다. 본 발명의 실시예에 따른 딥 러닝 기반의 학습 엔진의 조직 모듈은, 도 6에 도시된 바와 같은 조직의 블록(Block i, Block j, Block k 등)과 체인(Chain l, Chain m, Chain n 등)을 조합하여 DNA 미션을 구성할 수 있다. 구성된 DNA 미션은 단원 내에서 서로 관련된 주요 지점별 진단 문항과 진단 문항으로 이루어진 과목으로 표현될 수 있다. 또한, DNA 미션은, 체인(Chains)의 콤비네이션으로 구성되는 특수 DNA 미션(Special DNA Mission)을 포함할 수 있다. 즉, 실시예에 따라서는, 조직의 블록 없이 체인들만의 조합으로 DNA 미션을 구성할 수도 있다. 구성 모듈은, 자가 조직된 DNA 미션을 이용하여, 딥 러닝 기반의 인공 신경망 DNA 모델을 자가 구성할 수 있다(Self-Composition of DNA Model). 즉, 구성 모듈은, 조직 모듈로부터 DNA 미션을 전달받아, 딥 러닝 기반으로 학습할 수 있는 인공 신경망 DNA 모델을 스스로 구성해서 만들 수 있다. 구성 모듈에 의해 자가 구성되는 DNA 모델은, 시간의 흐름에 따라 입력되는 요소에 의해 자가 조직된 DNA 미션을 이용해 구성되기 때문에, 입력 데이터에 따라 유연하게 변화하는 모델일 수 있다. DNA 모델은, 기능 블록(Blocks of Function)과 체인(Chains)의 콤비네이션일 수 있다. 즉, 구성 모듈은, 뉴로블록체인 콤비네이션(Block Chain Combination) 기술을 이용하여, 기능의 블록과 체인을 조합하여 DNA 모델 을 조직할 수 있다. 또한, 자가 구성 모듈은, 기능적 하위 모델(Functional Submodel)의 합으로 구성되는 DNA 모델을 자가 구성 할 수 있다. 여기에서, 기능 블록은, 인간 뇌의 상황 판단 방식을 모방하여 인공 신경망 모델에서 학습이 가능하도록 하는 상황에 대한 기능별 집합으로서, DNA 미션에서의 하나의 조직의 블록은 DNA 모델에서 하나의 기능 블록으로 구 성될 수 있다. 단순한 상황은 인간이 한 번의 생각만으로도 판단이 가능하겠지만, 복잡한 상황은 한 번의 생각 이 아니라 여러 번의 생각에 의해 판단이 가능하다는 가정을 할 수 있다. 미션을 해결하기 위한 모델을 구성하 는 과정에서는, 이와 같은 개념을 이용하여, 복잡한 상황을 기능별로 구분하고 판단을 위해 그룹화하는 방식으 로 기능 블록과 체인을 조합하여 DNA 모델을 자가 조직할 수 있다. 도 7은 본 발명의 실시예에 따른 뉴로블록체인 콤비네이션을 이용하여, 구성 모듈에서 DNA 모델을 자가 구성하 는 방법을 예를 들어 도시한 도면이다. 본 발명의 실시예에 따른 딥 러닝 기반의 학습 엔진의 구성 모듈은, 도 7에 도시된 바와 같은, 기능 블 록(Block)과 체인(Chain)들을 뉴로블록체인 콤비네이션 기술을 통해 조합하여 기능적 하위 모델(Functional Submodel i, Functional Submodel j, Functional Submodel k, Functional Submodel m, Functional Submodel n 등)을 구성하고, 기능적 하위 모델의 합으로 DNA 모델을 구성할 수 있다. 한편, 도 5에서, 학습 모듈은, 자가 구성된 DNA 모델을 자가 학습할 수 있다(Self-Learning of DNA Model). 즉, 학습 모듈은, 구성 모듈에서 구성된 DNA 모델을 학습시키는 구성으로서, 인공 신경망 기술 을 통해 학습을 할 수 있으며, 학습 결과를 조치기에 전달할 수 있다. 조치기는, 이해 및 스케줄링 모듈, 판단 및 예측 모듈, 및 추천 및 조치 모듈을 통해 학습 엔 진의 학습 결과에 따른 기능을 수행할 수 있다. 도 8에 도시된 바와 같이, 조치기는 이해 및 스케줄링 모듈, 판단 및 예측 모듈, 추천 및 조치 모듈을 포함하여 구성될 수 있으며, 각 모듈이 공통 소프 트웨어가 될 수 있다. 본 발명에서는 조치기의 구조적인 측면에 대하여 설명하도록 한다. 이해 및 스케줄링 모듈은, 주어진 상황을 이해하거나 의도를 파악하고, 상황 이해 또는 의도 파악 결과를 이용해 의사 결정권자에게 스케줄링을 제공할 수 있다(Understanding & Scheduling). 판단 및 예측 모듈은, 주어진 상황에 대한 판단 및 분석 결과를 제공하고, 발생 가능한 상황을 예측하여 제 공할 수 있다(Decision & Prediction). 예를 들면, 판단 및 예측 모듈은, 영상 장치로부터 입력받은 장면 영상을 분석평가 모델에 입력하여 배우의 발음, 전달력, 표현력 등을 획득하고 이에 근거해 특정 영화 작품에 대한 배우의 연기력을 산출할 수 있다. 추천 및 조치 모듈은, 분석 결과 및 예측 결과를 이용하여, 주어진 상황에 대한 의사결정을 추천하고 이에 따른 조치를 제공할 수 있다(Recommendation & Action). 이를 위해, 추천 및 조치 모듈은, 판단 및 예측 모듈로부터 분석 결과 및 예측 결과를 전달받을 수 있다. 예를 들면, 추천 및 조치 모듈은, 판단 및 예 측 모듈에 의해 산출된 배우의 발음 평가, 전달력, 표현력 등의 연기 평가에 근거해 특정 영화 작품에 적합 한 특정 배역을 추천할 수 있다. 도 8은 본 발명의 실시예에 따른 딥 러닝 기반의 자가 적응 학습 기술을 이용한 인공 지능부에서 DNA 툴을 더 포함하는 구성을 도시한 도면이다. 도 8에 도시된 바와 같이, 본 발명의 실시예에 따른 딥 러닝 기반의 자가 적응 학습 기술을 이용한 인공 지능부 는, DNA 툴을 더 포함하여 구성될 수 있다. DNA 툴은, 요소 도출기, 학습 엔진 및 조치기에 복수의 툴을 제공할 수 있다. 즉, DNA 툴(9 0)은, 요소 도출기, 학습 엔진 및 조치기가 각각의 기능을 수행하는 데에 도움을 주는 툴을 제공할 수 있다. 보다 구체적으로는, DNA 툴은, 비구조화된 데이터를 텍스트로 변환시키는 변환 툴(Conversion Tool), 정보를 추출하는 추출 툴(Extraction Tool), 및 DNA 미션의 자가 조직 및 DNA 모델의 자가 구성에 필요한 블록과 체인을 연결하는 콤비네이션 툴(Combination Tool)을 포함할 수 있다. 또한, 조직 모듈, 구성 모듈 및 학습 모듈이, DNA 미션을 자가 조직, DNA 모델을 자가 구성 및 자 가 학습할 수 있도록 도움을 주는 자가 적응 툴(Self-Adapted Tool)을 더 포함하여 구성될 수 있다. 도 9는 본 발명의 실시예에 따른 연출 연기 영상 분석 평가 방법을 나타낸 동작 흐름도이다. 도 9를 참조하면, 본 발명의 실시예에 따른 연출 연기 영상 분석 평가 시스템은, AI 학습부가 연기분 석 DB에 저장된 정보를 딥러닝 학습하여 분석 평가 모델을 생성한다(S910). 이어, 영상 입력부가 드라마나 영화에 따라 제작된 작품 영상을 입력한다(S920). 이어, 분석 평가부가 영상 입력부를 통해 입력받은 작품 영상에 대하여, 분석평가 모델에 근거하여 장면, 행동, 대사, 표정에 따라 배우의 발음, 전달력, 표현력을 분석한다(S930). 이어, 분석 평가부가 분석된 결과에 근거하여 각 배우의 대사 키워드, 평균 감정 수치, 주된 감정, 잘 표 현된 감정, 발음, 전달력, 표현력 및 출연빈도를 포함하는 종합 평가를 산출하여 출력한다(S940). 도 10은 본 발명의 다른 실시예에 따른 연출 연기 영상 분석 평가 방법을 나타낸 동작 흐름도이다. 도 10을 참조하면, 본 발명의 다른 실시예에 따른 연출 연기 영상 분석 평가 시스템은, 영상 입력부 에서 드라마나 영화에 따라 제작된 영상을 입력받는다(S1010). 이어, 필름 슈팅부가 입력받은 영상을 시나리오(scenario)에 따라 배우 소스층, 스페이스 로케이션층 , 및 타임 소스층을 생성한다(S1020). 이때, 배우 소스층은 배우(Actor) 관련 데이터를 포함하고, 스페이스 로케이션층은 장소 관련 데이터 를 포함하고, 타임 소스층은 시간 관련 데이터를 포함할 수 있다. 배우 소스층은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 배우(Actors), 노트(Note)를 포함 할 수 있다. 스페이스 로케이션층은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 실내(IN)실외(OUT), 장소 (Location), 실제주소, 로케이션 사이즈(x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate)를 포함 할 수 있다. 타임 소스층은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 타임코드(Timecode), 낮(Day)밤 (Night), 촬영시간(Time), 소요시간(Time Frame)을 포함할 수 있다. 또한, 배우 소스층, 스페이스 로케이션층 및 타임 소스층은 각각 직육면체 형상을 가질 수 있다. 이어, 인공지능 학습부가 입력받은 영상에 대하여, 시나리오에 근거하여 필름 슈팅에 따른 항목으로 분류 하고, 분류된 각 항목의 데이터를 딥러닝 학습하여 분석 평가 모델을 생성한다(S1030). 이때, 필름 슈팅에 따른 항목은, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 타임코드(Timecode), 낮 (Day)밤(Night), 실내(IN)실외(OUT), 장소(Location), 실제주소, 배우(Actors), 노트(Note), 로케이션 사이즈 (x, y), 동선 좌표(Actor Motion), 배우위치(x, y Coordinate), 촬영시간(Time), 소요시간(Time Frame)을 포함 할 수 있다. 이어, 분석 평가부에서 배우 소스층, 스페이스 로케이션층 및 타임 소스층을 각각 분석 평 가 모델에 투입하여 시나리오에 따른 배우들(Actors)을 분류한다(S1040). 이때, 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 배우(Actors)은, 서로 독립된 개체이고, 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)를 공유할 수 있다. 즉, 배우 소스층, 스페이스 로케이션층 및 타임 소스층 각각은 데이터를 서로 주고 받은 이동 통로로서 관통 홀을 가질 수 있고, 이 관통 홀을 통하여 장면(Scene), 샷(shot) 수, 샷 길이(shot length), 배우(Actors) 등에 관한 오브젝트를 서로 공유할 수 있다. 배우 소스층은 기저층에 위치하고, 배우 소스층 위에 스페이스 로케이션층이 위치하고, 스페이 스 로케이션층 위에 타임 소스층이 위치할 수 있다. 또한, 이에 한정되지 않고 배우 소스층은 기저층에 위치하고, 배우 소스층 위에 타임 소스층이 위치하고, 타임 소스층 위에 스페이스 로 케이션층이 위치 할 수 있다. 배우 소스층, 스페이스 로케이션층 및 타임 소스층 각각은 동일한 위치에 관통 홀을 가질 수 있고, 수직(상하) 방향으로 일직선 상에 관통 홀이 위치하게 되면, 이 관통 홀을 통하여 장면 (Scene), 샷(shot) 수, 샷 길이(shot length), 및 배우(Actor)에 관한 오브젝트를 서로 공유 할 수 있다. 이어, 분석 평가부가 분류된 각 배우들에 대하여 장면(Scene), 샷(shot) 수, 대사(Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 감정 및 행동을 분석하여 평가를 실행한다(S1050). 즉, 분석 평가부는, 분류된 각 배우들에 대하여 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 대사 (Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 감정에 대하여 화남(Angry), 혐오 (Disgust), 공포(Fear), 행복(Happy), 슬픔(Sad), 놀람(Surprise), 중립(Neutral), 감정의 격함(Arousal), 밸 런스(Valence: 감정의 부정, 긍정). 집중력(Attention)의 항목에 따라 % 단위로 평가 할 수 있다. 이때, 분석 평가부는, 감정의 격함(Arousal) 및 밸런스(Valence: 감정의 부정, 긍정)에 대한 제로 값을 50%로 설정 할 수 있다. 또한, 분석 평가부는 도출된 각 배우의 감정을 1 단계 2 단계 3 단계 - 4단계로 구분하여, 각각 굳어짐- 분개-분노-광분으로 분류하고, 멸시-반감-혐오-극혐오로 분류하고, 걱정-불안-두려움-경악으로 분류하고, 만족- 즐김-기쁨-폭소로 분류하고, 낙담-우울-슬픔-비통으로 분류하고, 경계-으아함-놀람-충격으로 분류하여 평가 할 수 있다. 또한, 분석 평가부는 도출된 각 배우의 감정에서 두려움과 기쁨을 조합하여 필사적임으로 평가하고, 두려 움과 슬픔을 조합하여 처참함으로 평가하고, 두려움과 놀람을 조합하여 겁먹음으로 평가하고, 기쁨과 슬픔을 조합하여 희미한 희망으로 평가하고, 기쁨과 놀람을 조합하여 경이로움으로 평가 할 수 있다. 또한, 분석 평가부는 분류된 각 배우들에 대하여, 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 대사 (Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 행동을 대사와 맞물려 전달하는지에 따 라 평가 할 수 있다. 또한, 분석 평가부는 도출된 각 배우의 행동에 근거하여, 발음의 정확성에 대해 0점 내지 5점 중 하나로 평가하고, 대사의 전달력에 대해 0점 내지 5점 중 하나로 평가하고, 행동과 대사가 상황과 감정에 부합하는가에 따른 표현력에 대해 0점 내지 5점 중 하나로 평가할 수 있다. 이어, 분석 평가부는 실행된 평가 결과를 출력한다(S1060). 즉, 분석 평가부는 각 배우에 대해 평균 감정 수치, 주된 감정, 잘 표현된 감정, 부족한 감정을 화면 상에 출력하고, 각 배우의 연기 평점에 대하여 발음 평점, 전달력 평점, 표현력 평점, 그리고 출연 빈도수 등을 화면 상에 출력할 수 있다. 도 11은 본 발명의 실시예에 따른 분석 평가부에서 각 장면에 대한 배우의 대사, 감정 및 평가를 분석한 예를 나타낸 도면이다. 도 11을 참조하면, 본 발명의 실시예에 따른 분석 평가부는, 가이드 라인에 따라 예를 들어, 시나리오 2번 씬의 1 샷(shot)에서 배우 '기우'의 대사, 배우의 감정, 감정 값에 대한 평가를 분석할 수 있다. 여기서, 가이드라인은 다음과 같다. 1. ~~하는 행동이 ~~하는 표정을 설득적으로 전달한다. 2. ~~하는 행동이 ~~ 하는 표정과 합쳐져 ~~의 감정을 전달한다. 3. ~~하는 행동이 ~~하는 표정과 합쳐지며, ~~의 대사를 함으로써 '인물의 어떤 의도'나 '연출적으로 어떤 의도'를 전달한다. 이때, 배우의 감정은, 7가지로서, 화남(Angry), 혐오(Disgust), 공포(Fear), 행복(Happy), 슬픔(Sad), 놀람 (Surprise), 중립(Neutral), 감정의 격함(Arousal), 밸런스(Valence: 감정의 부정, 긍정). 집중력(Attention) 등의 항목을 포함할 수 있다. 따라서, 분석 평가부는 배우의 감정에 대하여 각 항목에 따라 % 단위로 환산 하여 평가할 수 있고, 감정의 격함(Arousal) 및 밸런스(Valence: 감정의 부정, 긍정)에 대한 제로 값을 50% 로 설정할 수 있다. 도 12는 본 발명의 실시예에 따른 분석 평가부에서 각 배우의 대사에 대한 행동, 행동 평가, 발음, 전달력, 표 현력 등을 분석한 예를 나타낸 도면이다. 도 12를 참조하면, 본 발명의 실시예에 따른 분석 평가부는, 가이드 라인에 따라 예를 들어, 시나리오 2번 씬의 1 샷(shot)에서 배우 '기우'의 대사에 따른 행동에 대하여, 행동 평가, 발음 평가, 전달력, 표현력 등을 분석할 수 있다. 발음평가는 배우의 발음이 얼마나 정확한가, 전달력은 대사를 관객에게 효과적으로 전달하는가, 표현력은 행동 과 대사가 상황과 감정에 부합하는가 등이다. 분석 평가부는, 분류된 각 배우들에 대하여, 시나리오에 근거하여 장면(Scene), 샷(shot) 수, 대사 (Dialogue), 배우의 감정, 행동(Action)을 도출하고, 도출된 각 배우의 행동을 상기 대사와 맞물려 전달하는지 에 따라 평가 할 수 있다. 예를 들면, 분석 평가부는 '기우'가 연교, 거실 창문으로 걸어와 날씨를 보는 행동을 하는 것에 대하여, 대사와 맞물려 어떤 것을 전달하는지 등을 분석하여 평가할 수 있다. 또한, 분석 평가부는 도출된 각 배우의 행동에 근거하여, 발음의 정확성에 대해 0점 내지 5점 중 하나로 평가하고, 상기 대사의 전달력에 대해 0점 내지 5점 중 하나로 평가하고, 행동과 대사가 상황과 감정에 부합하 는가에 따른 표현력에 대해 0점 내지 5점 중 하나로 평가할 수 있다. 예를 들면, 발음 평가에 대하여 연교 3점 을 부여하거나, 전달력에 대하여 연교 4점을 부여하거나, 표현력에 대하여 연교 3점을 부여할 수 있다. 도 13은 본 발명의 실시예에 따른 분석 평가부에서 각 배우의 감정을 평가하기 위해 이용하는 감정 지도의 예를 나타낸 도면이다.도 13을 참조하면, 본 발명의 실시예에 따른 분석 평가부는, 배우의 감정이 고통스러움, 괴로움, 구역질남, 귀찮음, 근심, 끔찍함, 몸서리치는, 무정함, 미움, 부담감, 서운함, 싫음, 싫증, 쌀쌀함, 야속함, 얄 미움, 억울함, 원망감, 죄스럼, 죄책감, 증오감, 지겨움, 짜증남, 차가움, 황량감, 반감, 격변, 냉소, 경멸인 경우에는 역겨움으로 분류한다. 또한, 분석 평가부는, 배우의 감정이 가혹함, 고통스러움, 골치아픔, 괘씸함, 구역질남, 기분상함, 꼴사나 움, 끓어오름, 나쁜, 노여움, 떫음, 모욕감, 무서움, 배반감, 복수심, 북받침, 분개함, 분노, 불만감, 불쾌감, 섬??함, 소름끼침, 속상함, 숨막힘, 실망감, 쓰라림, 씁쓸함, 약오름, 원한, 적의, 신경질, 호됨, 신랄함, 매서 움, 적개심인 경우에는 분노나 슬픔으로 분류한다. 또한, 분석 평가부는, 배우의 감정이 불안, 우려, 신경 과민, 깜짝놀람, 경악, 의심, 의혹인 경우에 공포 로 분류한다. 표 1"}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "또한, 분석 평가부는, 배우의 감정이 충격, 놀람, 경이, 감탄인 경우에 놀람으로 분류한다. 또한, 분석 평가부는, 배우의 감정이 간절, 갈망, 기대, 바람, 소망, 애끓음, 절박감, 찝찝함, 초라함, 초 조함, 호기심, 후회감, 희망감인 경우에 바램으로 분류한다. 또한, 분석 평가부는, 배우의 감정이 감격, 감동, 감사, 고무됨, 기쁨, 고전적, 날아갈듯한, 들뜬, 가벼운, 눈물겨운, 든든한, 만족스러운, 뭉클한, 반가움, 벅참, 뿌듯함, 살맛남, 시원함, 싱그러움, 좋음, 짜릿 함, 쾌적함, 통쾌감, 포근함, 푸근함, 행복감, 환상적임, 후련함, 흐뭇함, 흔쾌함, 흥분됨, 유쾌함, 자신감, 편 안함, 활기, 활발한, 희망참, 경쾌함, 구원, 만족, 자존심인 경우에 기쁨으로 분류한다.또한, 분석 평가부는, 배우의 감정이 수용, 우정, 믿음, 친절, 친근감, 헌신, 동경, 열중, 감미로움, 그리 움, 다정함, 따사로움, 묘함, 뿌듯함, 사랑스러움, 순수함, 애틋함, 열렬함, 열망감, 친숙함, 포근함, 호감인 경우에 사랑으로 분류한다. 또한, 분석 평가부는, 도출된 각 배우의 감정을 아래 표 2와 같이, 1 단계 2 단계 3 단계 - 4단계로 구 분하여 평가할 수 있다. 표 2"}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "즉, 분석 평가부는, 각각 굳어짐-분개-분노-광분으로 분류하고, 멸시-반감-혐오-극혐오로 분류하고, 걱정- 불안-두려움-경악으로 분류하고, 만족-즐김-기쁨-폭소로 분류하고, 낙담-우울-슬픔-비통으로 분류하고, 경계-으 아함-놀람-충격으로 분류하여 평가할 수 있다. 또한, 분석 평가부는, 도출된 각 배우의 감정에서 아래 표 3과 같이, 두 감정을 조합하여 평가할 수 있다. 표 3"}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "즉, 분석 평가부는, 각 배우의 감정에서 두려움과 기쁨을 조합하여 필사적임으로 평가하고, 두려움과 슬픔 을 조합하여 처참함으로 평가하고, 두려움과 놀람을 조합하여 겁먹음으로 평가하고, 기쁨과 슬픔을 조합하여 희 미한 희망으로 평가하고, 기쁨과 놀람을 조합하여 경이로움으로 평가할 수 있다. 도 14는 본 발명의 실시예에 따른 분석 평가부에서 등장인물의 감정을 분석 및 평가한 예를 나타낸 도면이다. 도 14를 참조하면, 본 발명의 실시예에 따른 분석 평가부는, 분류된 각 배우들에 대하여 대사 키워드를 설 정하고, 그에 대응되는 평균 감정 수치, 주된 감정, 잘 표현된 감정, 부족한 감정 등을 분석하여 평가할 수 있 다. 즉, 분석 평가부는 영화 '기생충'에 등장하는 인물(배우)에 대하여, 기우, 기택, 기정, 충숙, 연교, 동익, 문광, 근세, 피자가게 사장, 민혁, 다혜, 다송, 윤 기사, 의사 등으로 분류하고, 분류된 각 배우에 대하여 배우 대사 키워드를 설정하여 분석할 수 있다. 또한, 분석 평가부는, 등장인물 기우, 기택, 기정, 충숙, 연교, 동익, 문광, 근세, 피자가게 사장, 민혁, 다혜, 다송, 윤 기사, 의사 등에 대하여, 각각 평균 감정 수치, 주된 감정, 잘 표현된 감정, 부족한 감정 등을 분석할 수 있다. 도 15는 본 발명의 실시예에 따른 분석 평가부에서 배우 연기 평점 예를 나타낸 도면이다. 도 15를 참조하면, 본 발명의 실시예에 따른 분석 평가부는, 분류된 각 배우에 대하여 대사, 행동 등에 근 거하여 배우 연기 평점을 산출하여 평가할 수 있다. 즉, 분석 평가부는 영화 '기생충'에 등장하는 인물인 기우, 기택, 기정, 충숙, 연교, 동익, 문광, 근세, 피자가게 사장, 민혁, 다혜, 다송, 윤 기사, 의사 등에 대하여, 발음 평가, 전달력, 표현력, 출연 빈도 등을 분 석하여 배우 연기 평점을 산출할 수 있다. 예를 들면, 분석 평가부는, 전술한 과정으로 각 등장인물을 분석한 후, 예를 들어, 기우에 대하여, 발음 평가 3.31, 전달력 3.53, 표현력 3.34, 출연 빈도 29, 38, 38 등으로 평가할 수 있다. 또한, 분석 평가부는, 등장인물 중 피자가게 사장에 대하여, 발음 평가 4, 전달력 3, 표현력 3, 출연 빈도 5, 8, 8 등으로 평가할 수 있다. 또한, 분석 평가부는, 등장인물 중 다혜에 대하여, 발음 평가 3.25, 전달력 3.33, 표현력 3.16, 출연 빈도 4, 6, 6 등으로 평가할 수 있다. 또한, 분석 평가부는, 등장인물 중 출연빈도가 1, 1로 낮은 의사에 대하여는 전달력 3, 표현력 3만 평가할 수 있다. 전술한 바와 같이 본 발명에 의하면, 드라마나 영화 등의 작품 영상에서 장소와 더불어 연기 시간 등을 확인하 고, 배우의 연기에 대하여 대사, 표정, 행동 등을 분석하여, 발음이나 전달력, 표현력 등을 종합적으로 평가할 수 있도록 하는, 연출 연기 영상 분석 평가 시스템 및 방법을 실현할 수 있다."}
{"patent_id": "10-2021-0103318", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이상에서는 본 발명의 실시예를 중심으로 설명하였지만, 본 발명이 속하는 기술분야에서 통상의 지식을 가진 기 술자의 수준에서 다양한 변경이나 변형을 가할 수 있다. 이러한 변경과 변형은 본 발명이 제공하는 기술 사상의 범위를 벗어나지 않는 한 본 발명에 속한다고 할 수 있다. 따라서 본 발명의 권리범위는 이하에 기재되는 청구 범위에 의해 판단되어야 할 것이다."}
{"patent_id": "10-2021-0103318", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 연출 연기 영상 분석 평가 시스템의 구성을 개략적으로 나타낸 구성도이다. 도 2는 본 발명의 실시예에 따른 필름 슈팅부의 구성을 개략적으로 나타낸 구성도이다. 도 3은 본 발명의 실시예에 따른 분석 평가부에서 영화 장면을 분석한 예를 나타낸 도면이다. 도 4는 본 발명의 실시예에 따른 AI 학습부의 내부 구성을 개략적으로 나타낸 구성도이다. 도 5는 본 발명의 실시예에 따른 인공 지능부의 기능 블록을 나타낸 구성도이다. 도 6은 본 발명의 실시예에 따른 뉴로블록체인 콤비네이션을 이용하여, 조직 모듈에서 DNA 미션을 자가 조직하 는 방법을 예를 들어 도시한 도면이다. 도 7은 본 발명의 실시예에 따른 뉴로블록체인 콤비네이션을 이용하여, 구성 모듈에서 DNA 모델을 자가 구성하 는 방법을 예를 들어 도시한 도면이다. 도 8은 본 발명의 실시예에 따른 딥 러닝 기반의 자가 적응 학습 기술을 이용한 인공 지능부에서 DNA 툴을 더 포함하는 구성을 도시한 도면이다. 도 9는 본 발명의 실시예에 따른 연출 연기 영상 분석 평가 방법을 나타낸 동작 흐름도이다. 도 10은 본 발명의 다른 실시예에 따른 연출 연기 영상 분석 평가 방법을 나타낸 동작 흐름도이다. 도 11은 본 발명의 실시예에 따른 분석 평가부에서 각 장면에 대한 배우의 대사, 감정 및 평가를 분석한 예를 나타낸 도면이다. 도 12는 본 발명의 실시예에 따른 분석 평가부에서 각 배우의 대사에 대한 행동, 행동 평가, 발음, 전달력, 표 현력 등을 분석한 예를 나타낸 도면이다. 도 13은 본 발명의 실시예에 따른 분석 평가부에서 각 배우의 감정을 평가하기 위해 이용하는 감정 지도의 예를 나타낸 도면이다. 도 14는 본 발명의 실시예에 따른 분석 평가부에서 등장인물의 감정을 분석 및 평가한 예를 나타낸 도면이다. 도 15는 본 발명의 실시예에 따른 분석 평가부에서 배우 연기 평점 예를 나타낸 도면이다."}
