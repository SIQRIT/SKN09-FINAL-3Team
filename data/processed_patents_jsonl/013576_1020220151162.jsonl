{"patent_id": "10-2022-0151162", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0070738", "출원번호": "10-2022-0151162", "발명의 명칭": "벽면에 부착된 지문 인식기 살균 동작을 수행하는 협동 로봇", "출원인": "주식회사 제타뱅크", "발명자": "최동완"}}
{"patent_id": "10-2022-0151162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "바디;상기 바디가 이동하기 위한 힘을 제공하는 구동부;상기 바디에 장착되어, 벽면에 대한 제1 영상을 획득하는 제1 카메라;상기 제1 영상에서 벽면에 부착된 지문 인식기를 검출하고,상기 바디와 상기 지문 인식기와의 거리값을 산출하고, 상기 거리값에 기초하여 제1 지점에 상기 바디가 위치하도록 상기 구동부를 제어하는 제어부;를 포함하는 지문 인식기 살균 동작을 수행하는 이동 로봇."}
{"patent_id": "10-2022-0151162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 바디 상부에 장착되고, 끝단에 UV-C 램프가 구비된 암;을 더 포함하는 지문 인식기 살균 동작을 수행하는이동 로봇."}
{"patent_id": "10-2022-0151162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서,상기 제어부는,상기 제1 영상에서 상기 지문 인식기가 제1 크기이고, 상기 제1 영상에서 제1 영역에 위치하는 것으로 판단되는경우, 상기 암을 제1 자세로 전개하는 지문 인식기 살균 동작을 수행하는 이동 로봇."}
{"patent_id": "10-2022-0151162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3항에 있어서,상기 암에 장착되어, 벽면에 대한 제2 영상을 획득하는 제2 카메라;를 더 포함하는 지문 인식기 살균 동작을 수행하는 이동 로봇."}
{"patent_id": "10-2022-0151162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4항에 있어서,상기 제어부는,상기 제2 영상에서 검출된 지문 인식기가 상기 제2 영상의 중심 영역에 위치하는 것으로 판단되는 경우, 제2 자세로 암을 전개하는 지문 인식기 살균 동작을 수행하는 이동 로봇."}
{"patent_id": "10-2022-0151162", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 바디; 상기 바디가 이동하기 위한 힘을 제공하는 구동부; 상기 바디에 장착되어, 벽면에 대한 제1 영 상을 획득하는 제1 카메라; 상기 제1 영상에서 벽면에 부착된 지문 인식기를 검출하고, 상기 바디와 상기 지문 인식기와의 거리값을 산출하고, 상기 거리값에 기초하여 제1 지점에 상기 바디가 위치하도록 상기 구동부를 제어 하는 제어부;를 포함하는 지문 인식기 살균 동작을 수행하는 이동 로봇에 관한 것이다."}
{"patent_id": "10-2022-0151162", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 벽면에 부착된 지문 인식기 살균 동작을 수행하는 협동 로봇에 관한 것이다."}
{"patent_id": "10-2022-0151162", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "로봇은 최근 산업 현장뿐만 아니라 가정이나 사무실, 관공서 등 일반 건물내에서 가사일이나 사무 보조로서 활 용되고 있다. 이러한 로봇은 공간 내에서 이동을 하면서 탑재된 고유의 기능을 수행한다. 청소 로봇, 안내 로봇, 방범 로봇, 방역 로봇 등을 예로 들 수 있다. 방역 로봇의 일종으로 한국등록특허 제10-1742489호와 같이 자외선 살균램프를 구비한 이동 로봇에 대한 연구도 있으며, 특히, 이동 로봇이 이동하면서 바닥에 UV-C 광을 조사하는 기술도 개발이 이루어지고 있다. 그러나 사람들의 손이 접촉할 수밖에 없는 지문 인식기는 별도의 장비를 이용하여 살균해야 하는 불편함이 있다. 설령, 방역 로봇을 통해 지문 인식기를 살균한다고 하여도, 로봇이 정확하게 지문 인식기를 인지하고 로 봇 암이 지문 인식기에 접근해야 하기 때문에 이를 위한 기술 개발이 필요하다. 선행기술문헌 특허문헌 (특허문헌 0001) 등록번호 10-1742489"}
{"patent_id": "10-2022-0151162", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기한 문제점을 해결하기 위하여, 벽면에 부착된 지문 인식기에 대한 살균 동작을 수행하는 이동 로 봇을 제공하는데 목적이 있다. 본 발명의 과제들은 이상에서 언급한 과제들로 제한되지 않으며, 언급되지 않은 또 다른 과제들은 아래의 기재 로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0151162", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 과제를 달성하기 위하여, 본 발명의 실시예에 따른 지문 인식기 살균 동작을 수행하는 이동 로봇은 영상 처리를 통해 획득된 정보에 기초하여, 바디 및 암을 제어한다. 본 발명의 실시예에 따른 지문 인식기 살균 동작을 수행하는 이동 로봇은 바디; 상기 바디가 이동하기 위한 힘 을 제공하는 구동부; 상기 바디에 장착되어, 벽면에 대한 제1 영상을 획득하는 제1 카메라; 상기 제1 영상에서 벽면에 부착된 지문 인식기를 검출하고, 상기 바디와 상기 지문 인식기와의 거리값을 산출하고, 상기 거리값에 기초하여 제1 지점에 상기 바디가 위치하도록 상기 구동부를 제어하는 제어부;를 포함한다. 본 발명의 실시예에 따른 지문 인식기 살균 동작을 수행하는 이동 로봇은 상기 바디 상부에 장착되고, 끝단에 UV-C 램프가 구비된 암;을 더 포함한다. 상기 제어부는, 상기 제1 영상에서 상기 지문 인식기가 제1 크기이고, 상기 제1 영상에서 제1 영역에 위치하는 것으로 판단되는 경우, 상기 암을 제1 자세로 전개한다. 본 발명의 실시예에 따른 지문 인식기 살균 동작을 수행하는 이동 로봇은 상기 암에 장착되어, 벽면에 대한 제2 영상을 획득하는 제2 카메라;를 더 포함한다. 상기 제어부는, 상기 제2 영상에서 검출된 지문 인식기가 상기 제2 영상의 중심 영역에 위치하는 것으로 판단되 는 경우, 제2 자세로 암을 전개한다. 기타 실시예들의 구체적인 사항들은 상세한 설명 및 도면들에 포함되어 있다."}
{"patent_id": "10-2022-0151162", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 다음과 같은 효과가 하나 혹은 그 이상 있다. 첫째, 바닥 살균과 함께 지문 인식기에 대한 살균이 자동으로 수행되어 공공 보건에 일조하는 효과가 있다. 둘째, 바디에 장착된 제1 카메라와 암에 장착된 제2 카메라를 모두 이용하여 벽면에 부착된 지문 인식기를 검출 하고, 영상 처리를 통해 획득된 정보에 기초하여, 바디의 자세와 암의 자세를 설정함으로써 지문 인식기에 맞춰 정확한 살균을 수행하는 효과가 있다."}
{"patent_id": "10-2022-0151162", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과들은 이상에서 언급한 효과들로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 청구범위의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0151162", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명 에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용 되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이 해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함한다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되 어야 한다. 도 1은 본 발명의 실시예에 따른 협동 로봇의 외관을 도시한 도면이다. 도면을 참조하면, 이동 로봇(이하, 로봇)은, 공간에서 이동하면서, 살균 또는 소독 동작을 수행할 수 있다. 로봇은, 지문 인식기 살균 동작을 수행하는 이동 로봇으로 명명될 수 있다. 로봇은, 사람과 같은 공간에서 이동하면서 살균 또는 소독 동작을 수행할 수 있다. 로봇은, 지문 인 식기 살균 동작을 수행하는 협동 로봇으로 명명될 수 있다. 로봇은, 자율 주행할 수 있다. 로봇은, 정해진 경로로 주행할 수 있다. 로봇은, 원격 신호에 따 라 주행할 수 있다. 그에 따라 로봇은, 이동 로봇으로 명명될 수 있다. 로봇은, 단파장 자외선 광(UV-C)을 통해 살균 또는 소독 동작을 수행할 수 있다. 로봇은, 하부 UV-C 램프 장치를 포함할 수 있다. 로봇은, 바디 및 하부 UV-C 램프 장치를 포함할 수 있다. 바디는, 이동 로봇의 외관 을 형성하는 바디, 구동부(바퀴 포함), 제어부, 센서부, 통신부, HMI, 전원부 등을 포함할 수 있다. 하부 UV-C 램프 장치는, 제어부에서 제공되는 제어 신호에 기초하여, 단파장 자외선 광(UV-C)을 로봇(10 0)의 외부로 출력하여, 살균 또는 소독 동작을 수행할 수 있다. 하부 UV-C 램프 장치는, 바디의 하부에 장착될 수 있다. 하부 UV-C 램프 장치는, 바디의 하방으로 자외선 광을 조사함으로써, 로봇이 위치한 공간의 바닥 살균 또는 소독 동작을 수행할 수 있다.하부 UV-C 램프 장치가 바닥을 향해 자외선 광을 조사하는 상태에서, 로봇은, 주행할 수 있다. 이를 통해, 건물 내부의 방역을 용이하게 수행할 수 있다. 로봇은, 암 및 상부 UV-C 램프 장치를 포함할 수 있다. 암은, 바디 상부에 장착될 수 있다. 암은, 복수의 관절을 포함할 수 있다. 암의 끝단에는 상부 UV-C 램프 장치가 장착될 수 있다. 상부 UV-C 램프 장치는, 구동부에 의한 바디의 움직임 및 암 구동부에 의한 암의 움 직임에 따라 오브젝트에 근접한 상태에서 자외선 광을 출력할 수 있다. 이로 인해, 로봇은, 오브젝트에 대 한 살균 동작을 수행할 수 있다. 한편, 오브젝트는, 사람들이 손으로 직접 터치하는 지문 인식기, 엘리베이터 버튼, 문 손잡이 등일 수 있다. 상부 UV-C 램프 장치는, 차단부를 포함할 수 있다. 차단부는, 자외선 광이 오브젝트에만 조사되 도록 하고, 외부로 향하는 광을 차단할 수 있다. 차단부는, 플렉서블 재질(예를 들면, 고무)로 형성될 수 있다. 차단부는, 암에 의해 오브젝트를 둘러싼 상태에서, 벽쪽으로 힘이 가해지면 찌그러진 상태에서 오브젝트를 커버할 수 있다. 차단부는, 암이이 벽에서 이격되면 원래의 형상으로 복구될 수 있다. 암 구동부에 의한 암의 움직임에 따라 차단부가 오브젝트를 덮은 상태에서 상부 UV-C 램프 장치 는 광을 조사할 수 있다. 도 2는 본 발명의 실시예에 따른 협동 로봇의 제어 블럭도이다. 도면을 참조하면, 로봇은, 특정 공간에서 스스로 이동하면서 동작을 수행한다. 로봇은, 자율 주행 이 동 로봇으로 명명될 수 있다. 로봇은, 자율 주행할 수 있다. 로봇은, 센싱부 및 인공지능 모듈 에서 생성된 데이터에 기초하여 주행 경로를 생성하고, 생성된 경로를 따라 자율 주행할 수 있다. 로봇은, 센싱부, 인공지능 모듈, 통신부, 입력부, 메모리, 상부 UV-C 램프 장 치, 제어부, 구동부, 암 구동부, 하부 UV-C 램프 장치, 펜스 생성부, HMI 및 충전부를 포함할 수 있다. 센싱부, 인공지능 모듈, 통신부, 입력부, 메모리, 제어부, 구동부, 하부 UV-C 램프 장치, HMI 및 충전부는, 바디의 내부 또는 외부에 배치될 수 있다. 센싱부는, 로봇 내부 또는 외부의 상황을 감지할 수 있다. 센싱부는, 적어도 하나의 센서에서 생성된 데이터를 제어부에 제공할 수 있다. 제어부는, 센싱부로부터 수신된 데이터에 기초하여 동작을 수행할 수 있다. 센싱부는, 대상 공간에서 사람을 검출할 수 있다. 예를 들면, 센싱부는, 제1 카메라로 촬영된 영상 분석을 통해, 대상 공간에서 사람을 검출할 수 있다. 센싱부는, 대상 공간의 오염도를 측정할 수 있다. 예를 들면, 센싱부는, 환경 센서에서 생성된 데이 터를 통해, 대상 공간의 오염도를 측정할 수 있다. 한편, 센싱부는, 로봇이 자율 주행하는 도중에 내부 또는 외부의 상황을 감지할 수 있다. 센싱부는, 복수의 센서를 포함한다. 센싱부는, 초음파 센서, 라이다, 레이다, 적외선 센서, 제1 카메 라, 제2 카메라, 환경 센서, IMU(Inertial Measurement Unit)를 포함할 수 있다. 초음파 센서는, 초음파를 이용하여, 로봇 외부의 오브젝트를 감지할 수 있다. 초음파 센서는, 초음파 송신부, 수신부를 포함할 수 있다. 실시예에 따라, 초음파 센서는, 초음파 송신부, 수신 부와 전기적으로 연결되어, 수신되는 신호를 처리하고, 처리된 신호에 기초하여 오브젝트에 대한 데이터를 생성 하는 적어도 하나의 제어부를 더 포함할 수 있다. 초음파 센서의 제어부 기능은 제어부에서 구현될 수도 있다. 초음파 센서는, 초음파를 기초로 오브젝트를 검출하고, 검출된 오브젝트의 위치, 검출된 오브젝트와의 거리 및 상대 속도를 검출할 수 있다. 초음파 센서는, 로봇의 전방, 후방 또는 측방에 위치하는 오브젝트를 감지하기 위해 로봇 외부의 적 절한 위치에 배치될 수 있다.라이다는, 레이저 광을 이용하여, 로봇 외부의 오브젝트를 감지할 수 있다. 라이다는, 광 송신부 및 광 수신부를 포함할 수 있다. 실시예에 따라, 라이다는, 광 송신부 및 광 수신부와 전 기적으로 연결되어, 수신되는 신호를 처리하고, 처리된 신호에 기초하여 오브젝트에 대한 데이터를 생성하는 적 어도 하나의 제어부를 더 포함할 수 있다. 라이다의 제어부 기능은 제어부에서 구현될 수도 있다. 라이다는, TOF(Time of Flight) 방식 또는 페이즈 쉬프트(phase-shift) 방식으로 구현될 수 있다. 라이다는, 구동식 또는 비구동식으로 구현될 수 있다. 구동식으로 구현되는 경우, 라이다는, 모터에 의해 회전되며, 로봇 주변의 오브젝트를 검출할 수 있다. 비구동식으로 구현되는 경우, 라이다는, 광 스티어링에 의해, 로봇을 기준으로 소정 범위 내에 위치하는 오브젝트를 검출할 수 있다. 로봇은 복수의 비구동식 라이다를 포함할 수 있다. 라이다는, 레이저 광 매개로, TOF(Time of Flight) 방식 또는 페이즈 쉬프트(phase-shift) 방식에 기초하여, 오브젝트를 검출하고, 검출된 오브젝트의 위치, 검출된 오브젝트와의 거리 및 상대 속도를 검출할 수 있다. 라이다는, 로봇의 전방, 후방 또는 측방에 위치하는 오브젝트를 감지하기 위해 로봇 외부의 적절한 위치에 배치될 수 있다. 레이다는, 전자파 송신부, 수신부를 포함할 수 있다. 레이더는 전파 발사 원리상 펄스 레이더(Pulse Radar) 방 식 또는 연속파 레이더(Continuous Wave Radar) 방식으로 구현될 수 있다. 레이더는 연속파 레이더 방식 중에서 신호 파형에 따라 FMCW(Frequency Modulated Continuous Wave)방식 또는 FSK(Frequency Shift Keying) 방식으 로 구현될 수 있다. 레이더는 전자파를 매개로, TOF(Time of Flight) 방식 또는 페이즈 쉬프트(phase-shift) 방식에 기초하여, 오브 젝트를 검출하고, 검출된 오브젝트의 위치, 검출된 오브젝트와의 거리 및 상대 속도를 검출할 수 있다. 레이더는, 로봇의 전방, 후방 또는 측방에 위치하는 오브젝트를 감지하기 위해 로봇의 외부의 적절한 위치에 배치될 수 있다. 적외선 센서는, 적외선 송신부, 수신부를 포함할 수 있다. 적외선 센서는, 적외선 광을 기초로 오브젝트를 검출 하고, 검출된 오브젝트의 위치, 검출된 오브젝트와의 거리 및 상대 속도를 검출할 수 있다. 적외선 센서는, 로봇의 전방, 후방 또는 측방에 위치하는 오브젝트를 감지하기 위해 로봇의 외부의 적절한 위치에 배치될 수 있다. 제1 카메라는, 로봇 외부 영상을 촬영할 수 있다. 제1 카메라는, 영상을 이용하여 로봇 외부의 오브젝트에 대한 정보를 생성할 수 있다. 제1 카메라 는 적어도 하나의 렌즈, 적어도 하나의 이미지 센서 및 이미지 센서와 전기적으로 연결되어 수신되는 신호 를 처리하고, 처리되는 신호에 기초하여 오브젝트에 대한 데이터를 생성하는 적어도 하나의 제어부를 포함할 수 있다. 제1 카메라는, 모노 카메라, 스테레오 카메라 중 적어도 어느 하나일 수 있다. 제1 카메라는, 다양한 영상 처리 알고리즘을 이용하여, 오브젝트의 위치 정보, 오브젝트와의 거리 정보 또 는 오브젝트와의 상대 속도 정보를 획득할 수 있다. 예를 들면, 제1 카메라는, 획득된 영상에서, 시간에 따른 오브젝트 크기의 변화를 기초로, 오브젝트와의 거리 정보 및 상대 속도 정보를 획득할 수 있다. 예를 들면, 제1 카메라는, 핀홀(pin hole) 모델, 노면 프로파일링 등을 통해, 오브젝트와의 거리 정보 및 상대 속도 정보를 획득할 수 있다. 예를 들면, 제1 카메라는, 스테레오 카메라에서 획득된 스테레오 영상에서 디스패러티(disparity) 정보를 기초로 오브젝트와의 거리 정보 및 상대 속도 정보를 획득할 수 있다. 제1 카메라는, 로봇 외부를 촬영하기 위해 FOV(field of view) 확보가 가능한 위치에 장착될 수 있다. 로봇은, 복수의 제1 카메라를 포함할 수 있다. 예를 들면, 로봇은, 전방 제1 카메라, 후방 제1 카메라, 좌측방 제1 카메라, 우측방 제1 카메라를 구성된 4채널 제1 카메라를 포함할 수 있다. 제1 카메라는, 바디에 장착되어, 벽면에 대한 제1 영상을 획득할 수 있다. 제2 카메라는, 암에 장착되어, 암이 향하는 방향의 영상을 촬영할 수 있다. 제2 카메라는, 암이 향하는 방향에 위치하는 오브젝트에 대한 정보를 생성할 수 있다. 제2 카메라 는, 적어도 하나의 렌즈, 적어도 하나의 이미지 센서 및 이미지 센서와 전기적으로 연결되어 수신되는 신 호를 처리하고, 처리되는 신호에 기초하여 오브젝트에 대한 데이터를 생성하는 적어도 하나의 제어부를 포함할 수 있다. 제2 카메라는, 모노 카메라, 스테레오 카메라 중 적어도 어느 하나일 수 있다. 제2 카메라는, 다양한 영상 처리 알고리즘을 이용하여, 오브젝트의 위치 정보, 오브젝트와의 거리 정보 또 는 오브젝트와의 상대 속도 정보를 획득할 수 있다. 예를 들면 제2 카메라는, 획득된 영상에서, 시간에 따른 오브젝트 크기의 변화를 기초로, 오브젝트와의 거 리 정보 및 상대 속도 정보를 획득할 수 있다. 예를 들면 제2 카메라는, 제1 카메라는, 핀홀(pin hole) 모델, 노면 프로파일링 등을 통해, 오브젝트 와의 거리 정보 및 상대 속도 정보를 획득할 수 있다. 예를 들면 제2 카메라는, 제1 카메라는, 스테레오 카메라에서 획득된 스테레오 영상에서 디스패러티 (disparity) 정보를 기초로 오브젝트와의 거리 정보 및 상대 속도 정보를 획득할 수 있다. 제2 카메라는, 암이 향하는 방향의 영상을 촬영하기 위해 FOV(field of view) 확보가 가능한 위치에 장착될 수 있다. 예를 들면, 제2 카메라는, 차단부 안쪽에 배치될 수 있다. 이경우, 제2 카메라(11 2)는, 차단부에 의해 보호되어, 암의 움직임에 의해 발생될 수 있는 외력으로부터 보호될 수 있다. 제2 카메라는, 암에 장착되어, 벽면에 대한 제2 영상을 획득할 수 있다. 환경 센서는, 대상 공간에 대한 오염도를 센싱할 수 있다. 예를 들면, 환경 센서는, 대상 공간에서의 방역 대상 이 되는 생화학적 오염 물질의 유무, 농도 등을 센싱할 수 있다. IMU(Inertial Measurement Unit)는, 로봇의 관성을 측정할 수 있다. IMU는, 가속도계와 회전 속도계, 때 로는 자력계의 조합을 사용하여 로봇의 특정한 힘, 각도 비율 및 때로는 로봇을 둘러싼 자기장을 측 정하는 전자 장치로 설명될 수 있다. 제어부는, IMU로부터 수신되는 데이터에 기초하여 로봇의 자세 에 대한 정보를 생성할 수 있다. IMU는, 가속도 센서, 자이로 센서, 자기 센서 중 적어도 어느 하나를 포함할 수 있다. 센싱부는, 펜스 생성부에 의해 생성된 영역 주변을 감지할 수 있다. 인공지능 모듈은, 머신 러닝으로 사물, 공간, 로봇의 속성을 학습할 수 있다. 머신 러닝은 컴퓨터에 게 사람이 직접 로직(Logic)을 지시하지 않아도 데이터를 통해 컴퓨터가 학습을 하고 이를 통해 컴퓨터가 알아 서 문제를 해결하게 하는 것을 의미한다. 딥러닝(Deep Learning)은. 인공지능(artificial intelligence)을 구성하기 위한 인공신경망(Artificial Neural Networks: ANN)에 기반으로 해 컴퓨터에게 사람의 사고방식을 가르치는 방법으로 사람이 가르치지 않아도 컴퓨 터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 상기 인공신경망(ANN)은 소프트웨어 형태로 구현되거나 칩(chip) 등 하드웨어 형태로 구현될 수 있다. 인공지능 모듈은 공간의 속성, 장애물 등 사물의 속성이 학습된 소프트웨어 또는 하드웨어 형태의 인공신 경망(ANN)을 포함할 수 있다. 예를 들어, 인공지능 모듈은 딥러닝(Deep Learning)으로 학습된 CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), DBN(Deep Belief Network) 등 심층신경망(Deep Neural Network: DNN)을 포함 할 수 있다.인공지능 모듈은 상기 심층신경망(DNN)에 포함된 노드들 사이의 가중치(weight)들에 기초하여 입력되는 영 상 데이터에 포함되는 공간, 사물의 속성을 판별할 수 있다. 인공지능 모듈은, 머신 러닝(machine learning)으로 기학습된 데이터에 기초하여 상기 선택된 특정 시점 영상에 포함되는 공간, 장애물의 속성을 인식할 수 있다. 한편, 메모리에는 공간, 사물 속성 판별을 위한 입력 데이터, 상기 심층신경망(DNN)을 학습하기 위한 데이 터가 저장될 수 있다. 메모리에는 제1 카메라 및 제2 카메라가 획득한 원본 영상과 소정 영역이 추출된 추출 영상들이 저장될 수 있다. 또한, 실시예에 따라서는, 메모리에는 상기 심층신경망(DNN) 구조를 이루는 웨이트(weight), 바이어스 (bias)들이 저장될 수 있다. 또는, 실시예에 따라서는, 상기 심층신경망 구조를 이루는 웨이트(weight), 바이어스(bias)들은 인공지능 모듈 의 임베디드 메모리(embedded memory)에 저장될 수 있다. 한편, 인공지능 모듈은 센싱부를 통해 수신한 데이터를 트레이닝(training) 데이터로 사용하여 학습 과정을 수행할 수 있다. 로봇은 통신부를 통하여 상기 소정 서버로부터 머신 러닝과 관련된 데이터를 수신할 수 있다. 이 경 우에, 로봇은, 상기 소정 서버로부터 수신된 머신 러닝과 관련된 데이터에 기초하여 인공지능 모듈을 업데이트(update)할 수 있다. 로봇의 동작으로 획득되는 데이터가 서버로 전송될 수 있다. 로봇은, 서버로 공간(space), 사물 (Object), 사용(Usage) 관련 데이터(Data)를 서버로 전송할 수 있다. 여기서, 공간(space), 사물(Object) 관련 데이터는 로봇이 인식한 공간(space)과 사물(Object)의 인식 관련 데이터이거나, 제1 카메라 또는 제 2 카메라가 획득한 공간(space)과 사물(Object)에 대한 이미지 데이터일 수 있다. 또한, 사용(Usage) 관련 데이터(Data)는 소정 제품, 예를 들어, 로봇의 사용에 따라 획득되는 데이터로, 사용 이력 데이터, 센싱부 에서 획득된 센싱 데이터 등이 해당될 수 있다. 한편, 로봇의 인공지능 모듈에는 CNN(Convolutional Neural Network) 등 심층신경망 구조(DNN)가 탑 재될 수 있다. 상기 학습된 심층신경망 구조(DNN)는 인식용 입력 데이터를 입력받고, 입력 데이터에 포함된 사 물, 공간의 속성을 인식하여, 그 결과를 출력할 수 있다. 또한, 상기 학습된 심층신경망 구조(DNN)는 인식용 입 력 데이터를 입력받고, 로봇의 사용(Usage) 관련 데이터(Data)를 분석하고 학습하여 사용 패턴, 사용 환경 등을 인식할 수 있다. 한편, 공간(space), 사물(Object), 사용(Usage) 관련 데이터(Data)는 통신부를 통하여 서버로 전송될 수 있다. 서버는 학습된 웨이트(weight)들의 구성을 생성할 수 있고, 서버는 심층신경망(DNN) 구조를 트레이닝 (training) 데이터를 사용하여 학습할 수 있다. 서버는 수신한 데이터에 기초하여, 심층신경망(DNN)을 학습시킨 후, 업데이트된 심층신경망(DNN) 구조 데이터를 로봇으로 전송하여 업데이트하게 할 수 있다. 이에 따라, 로봇은 점점 더 똑똑해지고, 사용할수록 진화되는 사용자 경험(UX)을 제공할 수 있다. 인공지능 모듈은, 센싱부를 통해 수신한 데이터를 트레이닝 데이터로 사용하여 오브젝트를 검출 동작 의 학습 과정을 수행할 수 있다. 여기서, 오브젝트는, 로봇 주변의 사물, 사람, 구조물 등 로봇의 이 동에 직접적 또는 간접적으로 영향을 주는 객체로 정의할 수 있다. 인공지능 모듈은, 소프트웨어로 구현되거나, 하드웨어 형태로 구현될 수 있다. 인공지능 모듈이 프로 세서(예를 들면, GPU)로 구현되는 경우, 인공지능 모듈이 구현된 프로세서로 명명될 수 있다. 한편, 실시예에 따라, 인공지능 모듈은, 제어부의 하위 개념으로 분류될 수 있다. 이경우, 인공지능 모듈의 동작은 제어부의 동작으로 설명될 수 있다. 통신부는, 로봇 외부의 전자 장치(예를 들면, 사용자 단말기, 서버, 다른 이동 로봇, 충전 스테이션 등)와 신호를 교환할 수 있다. 통신부는, 외부의 전자 장치와 데이터를 교환할 수 있다. 통신부는, 통신을 수행하기 위해 송신 안테나, 수신 안테나, 각종 통신 프로토콜이 구현 가능한 RF(Radio Frequency) 회로 및 RF 소자 중 적어도 어느 하나를 포함할 수 있다. 입력부는, 사용자로부터 정보를 입력받기 위한 것으로, 입력부에서 수집한 데이터는, 제어부에 의해 분석되어, 사용자의 제어 명령으로 처리될 수 있다. 입력부는, 음성 입력부, 터치 입력부를 포함할 수 있다. 실시예에 따라, 제스쳐 입력부 또는 기계식 입력 부를 포함할 수 있다. 음성 입력부는, 사용자의 음성 입력을 전기적 신호로 전환할 수 있다. 전환된 전기적 신호는, 제어부에 제 공될 수 있다. 음성 입력부는, 하나 이상의 마이크로 폰을 포함할 수 있다. 터치 입력부는, 사용자의 터치 입력을 전기적 신호로 전환할 수 있다. 전환된 전기적 신호는 제어부에 제 공될 수 있다. 터치 입력부는, 사용자의 터치 입력을 감지하기 위한 터치 센서를 포함할 수 있다. 실시예에 따라, 터치 입력부는 디스플레이와 일체형으로 형성됨으로써, 터치 스크린을 구현할 수 있다. 이 러한, 터치 스크린은, 로봇과 사용자 사이의 입력 인터페이스 및 출력 인터페이스를 함께 제공할 수 있다. 제스쳐 입력부는, 사용자의 제스쳐 입력을 전기적 신호로 전환할 수 있다. 전환된 전기적 신호는 제어부에 제공될 수 있다. 제스쳐 입력부는, 사용자의 제스쳐 입력을 감지하기 위한 적외선 센서 및 이미지 센서 중 적어도 어느 하나를 포함할 수 있다. 기계식 입력부는, 버튼, 돔 스위치(dome switch), 조그 휠 및 조그 스위치 중 적어도 어느 하나를 포함할 수 있 다. 기계식 입력부에 의해 생성된 전기적 신호는, 제어부에 제공될 수 있다. 메모리는, 제어부와 전기적으로 연결된다. 메모리는 유닛에 대한 기본데이터, 유닛의 동작제어 를 위한 제어데이터, 입출력되는 데이터를 저장할 수 있다. 메모리는, 제어부에서 처리된 데이터를 저장할 수 있다. 메모리는, 하드웨어적으로, ROM, RAM, EPROM, 플래시 드라이브, 하드 드라이브 중 적어도 어느 하나로 구성될 수 있다. 메모리는 제어부의 처리 또는 제어를 위한 프로그램 등, 로봇 전 반의 동작을 위한 다양한 데이터를 저장할 수 있다. 메모리는, 제어부와 일체형으로 구현될 수 있다. 실시예에 따라, 메모리는, 제어부의 하위 구성으로 분류될 수 있다. 상부 UV-C 램프 장치는, 구동부에 의한 바디의 움직임 및 암 구동부에 의한 암의 움 직임에 따라 오브젝트에 근접한 상태에서 자외선 광을 출력할 수 있다. 이로 인해, 로봇은, 오브젝트에 대 한 살균 동작을 수행할 수 있다. 한편, 오브젝트는, 사람들이 손으로 직접 터치하는 지문 인식기, 엘리베이터 버튼, 문 손잡이 등일 수 있다. 상부 UV-C 램프 장치는, 차단부를 포함할 수 있다. 차단부는, 자외선 광이 오브젝트에만 조사되 도록 하고, 외부로 향하는 광을 차단할 수 있다. 암 구동부에 의한 암의 움직임에 따라 차단부가 오브젝트를 덮은 상태에서 상부 UV-C 램프 장치 는 광을 조사할 수 있다. 구동부는, 로봇의 이동 동력을 제공할 수 있다. 구동부는, 바디가 이동하기 위한 힘을 제 공할 수 있다. 구동부는, 동력 생성부 및 동력 전달부를 포함할 수 있다. 동력 생성부는, 전기 에너지를 힘 에너지로 전환할 수 있다. 이를 위해 동력 생성부는, 적어도 하나의 모터로 구성될 수 있다. 동력 전달부는, 동력 생성부에서 생성된 동력을 구동 바퀴에 전달할 수 있다. 동력 전달부는, 적어도 하나의 기 어 또는 적어도 하나의 벨트를 포함할 수 있다. 암 구동부는, 암을 구동할 수 있다. 암 구동부는, 암의 끝단에 장착된 상부 UV-C 램프가 오브젝트에 근접하도록 암을 구동할 수 있다. 암 구동부는, 암이 적어도 하나의 관절을 기준으로 회전하기 위한 동력을 제공할 수 있다. 암 구동부 는, 동력 생성부 및 동력 전달부를 포함할 수 있다. 동력 생성부는, 전기 에너지를 힘 에너지로 전환할 수 있다. 이를 위해 동력 생성부는, 적어도 하나의 모터로 구성될 수 있다. 동력 전달부는, 동력 생성부에서 생성된 동력을 암의 복수의 회전부에 전달할 수 있다. 동력 전달부는, 적 어도 하나의 기어 또는 적어도 하나의 벨트를 포함할 수 있다. 하부 UV-C 램프 장치는, 대상 공간에 대한 방역 동작을 수행할 수 있다. 하부 UV-C 램프 장치는, 광출력부를 포함할 수 있다. 광출력부는, 단파장 자외선 광을 외부로 출력할 수 있다. 광출력부는 UV-C LED를 포함할 수 있다. 광출력부는, 제어부에서 제공되는 신호에 따라 전자식으로 제어될 수 있다. 제어부는, 광출력부에 제어 신호를 제 공하여 광 출력 여부, 광출력 양 등을 제어할 수 있다. 광출력부는, 복수의 UV-C LED 모듈을 포함할 수 있다. 로봇은 4개의 캐스터휠을 포함할 수 있다. 복수의 UV-C LED 모듈은, 4개의 캐스터휠 사이사이에 배치될 수 있다. 하부 UV-C 램프 장치는, 바디에 장착될 수 있다. 하부 UV-C 램프 장치는, 바닥으로 단파장 자외선 광을 출력할 수 있다. 펜스 생성부는, 적어도 하나의 광출력부를 포함할 수 있다. 펜스 생성부는, 광을 바닥에 출력하여 바디를 둘러싸는 영역을 생성할 수 있다. 예를 들면, 펜스 생 성부는, 로봇의 전면, 후면, 좌측면, 우측면 각각에 부착되는 4개의 광출력부를 포함하고, 각각의 광 출력부가 선광(line light)를 바닥에 조사함으로써 영역을 생성할 수 있다. 한편, 바닥에 출력되는 광 또는 광 에 의해 생성되는 광 패턴은 안전 펜스로 명명될 수 있다. HMI(Human Machine Interface)는, 대상 공간에 위치한 사람과 인터렉션을 수행할 수 있다. 이를 위해 HMI는, 출력 장치로 디스플레이 및 스피커를 포함할 수 있다. 상술한 입력부는 HMI의 하위 개념으로 분류될 수 있다. 디스플레이는, 다양한 정보에 대응되는 그래픽 객체를 표시할 수 있다. 디스플레이는 액정 디스플레이(liquid crystal display, LCD), 박막 트랜지스터 액정 디스플레이(thin film transistor-liquid crystal display, TFT LCD), 유기 발광 다이오드(organic light-emitting diode, OLED), 플렉서블 디스플레이(flexible display), 3차원 디스플레이(3D display), 전자잉크 디스플레이(e-ink display) 중에서 적어도 하나를 포함할 수 있다. 디스플레이는 터치 입력부와 상호 레이어 구조를 이루거나 일체형으로 형성됨으로써, 터치 스크린을 구현 할 수 있다. 디스플레이는 복수로 구성될 수 있다. 예를 들면, 디스플레이는, 터치 스크린으로 구성되어 터치 입 력을 수신할 수 있는 제1 디스플레이 및 정보 출력을 위한 제2 디스플레이로 구성될 수 있다. 스피커는, 제어부로부터 제공되는 전기 신호를 오디오 신호로 변환하여 출력한다. 이를 위해, 스피커 는, 하나 이상의 스피커를 포함할 수 있다. 충전부는, 충전 스테이션으로부터 전기 에너지를 공급받을 수 있다. 충전부는. 공급받은 전기 에너지 를 배터리에 저장할 수 있다. 제어부는, 로봇의 각 유닛의 전반적인 동작을 제어할 수 있다. 제어부는 ECU(Electronic Control Unit)로 명명될 수 있다. 제어부는, 로봇의 각 유닛과 전기적으로 연결된다. 제어부는, ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 프로세서(processors), 제어기(controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서(microprocessors), 기타 기능 수행을 위한 전기적 유닛 중 적어도 하나를 이용하여 구현될 수 있다. 제어부는, 인공 지능 모듈에서 생성한 스케줄에 기초하여, 구동부, 하부 UV-C 램프 장치, HMI 및 충전부 중 적어도 어느 하나를 제어할 수 있다. 제어부는, 센싱부로부터 센싱 데이터를 수신할 수 있다. 제어부는, 센싱부에서 생성된 데이터에 기초하여, 영역으로 진입하는 인체가 감지되는 경우, 안전 동 작을 위한 제어 신호를 생성할 수 있다. 제어부는, 영역으로 진입하는 인체가 감지되는 경우, 펜스 생성부에서 출력되는 광의 색을 제1 색에 서 제2 색으로 전환시키기 위한 제1 제어 신호를 생성할 수 있다. 제어부는, 제1 제어 신호를 펜스 생성부 에 제공할 수 있다. 펜스 생성부는, 제1 제어 신호에 기초하여, 출력되는 광의 색을 제1 색에서 제2 색으로 전환할 수 있다. 제어부는, 영역으로 진입하는 인체가 감지되는 경우, 알람 출력을 위한 제2 제어 신호를 생성할 수 있다. 제어부는, 제2 제어 신호를 스피커에 제공할 수 있다. 스피커는, 제2 제어 신호에 기초하여, 알 람을 출력할 수 있다. 제어부는, 영역으로 진입하는 인체가 감지되는 경우, 암의 움직임이 정지되도록 제3 제어 신호를 생성할 수 있다. 제어부는, 제3 제어 신호를 암 구동부에 제공할 수 있다. 암 구동부는, 제3 제어 신호 에 기초하여, 암의 움직임을 정지시킬 수 있다. 제어부는, 로봇의 동작에 따라 안전 펜스의 형상을 변경할 수 있다. 제어부는, 구동부에 의해 바디가 이동하는 경우, 바디의 이동 궤적에 기초하여 안전 펜스 의 형상을 변경할 수 있다. 제어부는, 암이 구동되는 경우, 암의 움직임 궤적에 기초하여 안전 펜스의 형상을 변경할 수 있다. 제어부는, 펜스 생성부를 제어하여, 복수의 안전 펜스를 생성할 수 있다. 예를 들면, 제어부는, 제1 안전 펜스 및 제2 안전 펜스를 생성할 수 있다. 제1 안전 펜스는, 로봇을 기준으로 제2 안전 펜스보다 더 바깥쪽에 생성될 수 있다. 제어부는, 제1 안전 펜스 내에 인체가 검출되는 경우, 알람이 출력되도록 제어 신호를 생성할 수 있다. 제어부는, 제2 안전 펜스 내에 인체가 검출되는 경우, 암의 움직임이 정지되도록 제어 신호를 생성할 수 있다. 제어부는, 암의 움직임의 궤적을 따라 안전 펜스의 형상을 변경할 수 있다. 암이 제1 방향을 향해 길게 위치하는 경우, 제어부는, 위에서 볼때, 안전 펜스가 암을 감싸도록 제1 방향으로 길게 형성되도록 펜스 생성부를 제어할 수 있다. 암이 제2 방향을 향해 길에 위치하도록 움직이는 경우, 제어부는, 암의 움직임을 따라, 안전 펜 스의 형상이 변경되도록 펜스 생성부를 제어할 수 있다. 제어부는, 위에서 볼 때 움직이는 암을 감싸도록 안전 펜스는 암의 움직임과 동기화된 채 움직 이도록 펜스 생성부를 제어할 수 있다. 암이 움직여서, 제2 방향을 향해 길게 위치하는 경우, 제어부는, 위에서 볼때, 안전 펜스가 암 을 감싸도록 제2 방향으로 길게 형성되도록 펜스 생성부를 제어할 수 있다. 제어부는, 복수의 안전 펜스가 생성되도록 펜스 생성부를 제어할 수 있다. 복수의 안전 펜스를 생성 할 수 있도록 펜스 생성부는, 복수의 안전 펜스에 대응되는 복수의 광출력부를 포함할 수 있다. 제어부는, 제1 안전 펜스 및 제2 안전 펜스가 생성되도록 펜스 생성부를 제어할 수 있다. 제1 안전 펜스와 제2 안전 펜스 사이의 영역은 제1 영역으로 정의되고, 제2 안전 펜스 안쪽의 영역은 제2 영역 으로 정의될 수 있다. 제어부는, 제1 영역으로 진입하는 인체가 검출되는 경우, 제1 안전 동작을 위한 제1 제어 신호를 생성할 수 있다. 제어부는, 제2 영역으로 진입하는 인체가 검출되는 경우, 제1 안전 동작과 다른 제2 안전 동작을 위한 제2 제어 신호를 생성할 수 있다. 예를 들면, 제어부는, 제1 영역으로 진입하는 인체의 적어도 일부가 검출되는 경우, 알람이 출력되도록 제 1 제어 신호를 생성하고, 제2 영역으로 진입하는 인체의 적어도 일부가 검출되는 경우, 암의 움직임이 멈 추도록 제2 제어 신호를 생성할 수 있다. 제어부는, 바디의 이동을 따라 안전 펜스의 형상을 변경할 수 있다. 제어부는, 바디가 정지한 경우, 제1 형상의 안전 펜스(F11)를 생성할 수 있다. 제어부는, 바디가 제1 방향으로 이동하는 경우, 제1 형상과는 다른 제2 형상의 안전 펜스를 생 성할 수 있다. 제2 형상은, 제1 형상에 비해 바디가 이동하는 방향으로 더 길게 형성될 수 있다. 제어부는, 바디의 이동 속도에 따라 안전 펜스의 형상을 결정할 수 있다. 제어부는, 바디가 제1 속도로 제1 방향으로 이동하는 경우, 제2 형상의 안전 펜스를 생성할 수 있다. 제어부는, 바디가 제2 속도로 제1 방향으로 이동하는 경우, 제3 형상의 안전 펜스를 생성할 수 있다. 제2 속도는, 제1 속도보다 더 빠른 속도일 수 있다. 제3 형상은, 제2 형상에 비해 바디가 이동하는 방향으로 더 길게 형성될 수 있다. 이와 같이, 로봇의 이동에 따라 안전 펜스의 형상을 변경함으로써, 주변 사람과의 충돌 확률을 낮출 수 있 다. 제어부는, 벽면에 부착된 오브젝트를 인식하고, 상부 UV-C 램프 장치를 이용해 오브젝트를 살균할 수 있다. 이를 위해 제어부는, 도 3의 제어 동작을 수행할 수 있다. 도 3은 본 발명의 실시예에 따른 협동 로봇의 플로우 차트이다. 도면을 참조하면, 제어부는, 바디가 제1 지점에 위치하도록 구동부를 제어할 수 있다(S310). 제 1 지점은, 로봇이 벽면에 부착된 오브젝트에 대한 살균 동작을 수행하기 위한 준비 지점으로 설명될 수 있 다. 제1 지점은, 오브젝트와의 거리에 기초하여 결정될 수 있다. 로봇은, 제1 카메라에서 획득한 제1 영상에서 벽면에 부착된 오브젝트를 검출할 수 있다. 여기서, 오 브젝트는, 지문 인식기, 엘리베이터 버튼, 문 손잡이 등, 사람들이 손으로 직접 터치하는 것일 수 있다. 로봇은, 바디와 오브젝트와의 거리값을 산출하고, 거리값에 기초하여 제1 지점에 바디가 위치하 도록 구동부를 제어할 수 있다. 바디가 제1 지점에 위치한 상태에서, 제어부는, 제1 영상에서의 오브젝트의 크기 조건 및 위치 조건 이 만족되는지 판단할 수 있다(S320). 예를 들면, 제어부는, 제1 영상에서 오브젝트가 제1 크기이고, 제1 영역에 위치하는지 여부를 판단할 수 있다. 제어부는, 영상에서의 오브젝트의 크기와 위치하는 영역에 기초하여, 바디를 기준으로 오브젝 트의 상대 위치를 파악할 수 있다. 제어부는, 제1 영상에서 오브젝트가 제1 크기이고, 제1 영상에서 제1 영역에 위치하는 것으로 판단되는 경 우, 암을 제1 자세로 전개할 수 있다(S330). 예를 들면, 제어부는, 오브젝트의 대각선 길이에 기초하여 크기가 만족되는지 판단할 수 있다. 예를 들면, 제어부는, 영상을 기 설정 개수의 복수 영역으로 분할하고, 기 설정된 영역에 위치하는지 여부 로 특정 영역에 위치하는 것으로 판단할 수 있다. 한편, 암의 제1 자세는, 암에 부착된 제2 카메라를 이용해 벽면을 촬영하기 위한 자세로 설명될 수 있다. 제어부는, 암이 제1 자세인 상태로 바디가 이동하면서 제2 영상을 획득하도록 구동부를 제 어할 수 있다(S340). 제어부는, 오브젝트가 제2 영상에 중심 영역에 위치하는지 판단할 수 있다(S350). 제어부는, 제2 영상에서 오브젝트를 검출할 수 있다. 제어부는, 검출된 오브젝트가 제2 영상의 중심 영역에 위치하는지 판단할 수 있다. 제어부는, 제2 영상에서 검출된 오브젝트가 제2 영상의 중심 영역에 위치하는 것으로 판단되는 경우, 제2 자세로 암을 전개할 수 있다(S360). 한편, 암의 제2 자세는, 암에 부착된 상부 UV-C 램프 장치를 통해 오브젝트를 살균하기 위한 자 세로 설명될 수 있다. 제어부는, 바디가 이동하는 중에, 제2 영상에서 검출된 오브젝트가 제2 영상의 중심 영역에 위치하는 것으로 판단되는 경우, 바디의 이동을 멈추고 제2 자세로 암을 전개할 수 있다. 제어부는, 제2 자세로 암을 전개하는 중에 제2 영상에서 오브젝트가 제2 크기가 되는지 판단할 수 있 다(S370). 제어부는, 제2 영상에서의 오브젝트의 크기에 기초하여, 상부 UV-C 램프 장치와 오브젝트 사이의 거 리값을 결정할 수 있다. 제어부는, 제2 영상에서의 오브젝트가 제2 크기인 경우, 상부 UV-C 램프 장치 는, 오브젝트에 매우 근접한 상태로 이해될 수 있다. 제어부는, 오브젝트의 크기가 제2 크기인 것으로 판단되는 경우, 암의 전개 동작을 멈추고, 상부 UV- C 램프 장치를 구동할 수 있다(S380). 기 설정 시간 경과된 후, 제어부는, 암을 폴딩하고 다른 동작을 수행하기 위해 이동하도록 구동부 를 제어할 수 있다. 도 4 내지 도 9는 본 발명의 실시예에 따른 협동 로봇의 동작을 설명하는데 참조되는 도면이다. 도 4 내지 도 9 는, 로봇 및 지문 인식기를 위에서 본 모습을 대략적으로 도시한다. 도 4를 참조하면, 제어부는, 바디가 제1 지점으로 이동하여 멈추도록 구동부를 제어할 수 있다. 제1 지점은, 암에 부착된 제2 카메라를 통해, 벽에 부착된 지문 인식기를 센싱하기 위한 지점으로 설 명될 수 있다. 제어부는, 제1 영상(IM1)에서 지문 인식기(410i1)를 검출할 수 있다. 제어부는, 지문 인식기(410i1) 의 대각선 길이를 측정할 수 있다. 제어부는, 지문 인식기(410i1)의 대각선 길이가 기 설정 범 위 이내인지 판단할 수 있다. 제어부는, 제1 영상(IM1)에서 지문 인식기(410i1)가 제1 영역(R1)에 위치하는지 판단할 수 있다. 제1 영역 (R1)은, 영상을 4개의 동일 면적으로 분할할 때, 오른쪽 위에 위치하는 영역일 수 있다. 제어부는, 대각선 길이가 기 설정 범위 이내이고, 지문 인식기(410i1)가 제1 영역(R1)에 위치하는 것으로 판단되는 경우, 바디가 제1 지점에 위치한 것으로 판단할 수 있다. 이경우, 제어부는, 바디 가 멈추도록 구동부를 제어할 수 있다. 한편, 제어부는, 대각선 길이와 지문 인식기(410i1)가 제1 영역(R1)에 위치하는지 여부에 기초하여, 바디와 지문 인식기 사이의 거리(L1, L2)를 추정할 수 있다. 도 5를 참조하면, 바디가 제1 지점에 위치한 상태에서, 제어부는, 암이 제1 상태로 전개되도록 암 구동부를 제어할 수 있다. 제1 상태는, 암에 장착된 제2 카메라가 벽면에 대한 영상을 획득 할 수 있는 상태로 설명될 수 있다. 암이 제1 상태로 전개된 상태에서, 제어부는, 바디가 벽면과 평행한 방향으로 이동하도록 구동 부를 제어할 수 있다. 제어부는, 바디가 이동하는 중 제2 카메라를 통해 제2 영상(IM2)을 지속적으로 획득할 수 있다. 제어부는, 제2 영상(IM2)에서 지문 인식기(410i2)를 검출할 수 있다. 도 6을 참조하면, 제어부는, 제2 영상(IM2)에서 지문 인식기(410i2)가 중심 영역에 위치하는지 판단할 수 있다. 예를 들면, 제어부는, 제2 영상(IM2)을 9등분한 상태에서, 지문 인식기(410i2)가 중심 영역(CR)에 위치하는지 판단할 수 있다. 제어부는, 지문 인식기(410i2)가 중심 영역(CR)에 위치하는 것으로 판단되는 경우, 암이 제2 상태로 전개되도록 암 구동부를 제어할 수 있다. 제2 상태는, 암에 장착된 상부 UV-C 램프 장치로 지문 인식기를 살균할 수 있는 상태로 설명될 수 있다. 또는, 제2 상태는, 차단부가 지문 인식기를 덮은 상태로 설명될 수 있다.도 7을 참조하면, 제어부는, 암이 제2 상태로 전개되도록 암 구동부를 제어하는 상태에서, 제2 영상(IM2)에서 지문 인식기(410i2)의 대각선 길이가 기준 범위 이내인지 판단할 수 있다. 제어부는, 대각선 길이가 기준 범위 이내인 것으로 판단되는 경우, 암의 전개가 중지되도록 암 구동부를 제어할 수 있다. 제어부는, 지문 인식기(410i2)의 대각선 길이에 기초하여, 상부 UV-C 램프 장치와 지문 인식기 사이의 거리를 추정할 수 있다. 도 8을 참조하면, 암의 전개가 중지된 상태에서, 제어부는, 상부 UV-C 램프가 구동되도록 제어 신호를 제공할 수 있다. 이때, 제어부는, 제2 카메라를 턴 오프 시킬 수 있다. 도 9를 참조하면, 기 설정 시간 동안 UV-C 램프를 구동한 후, 제어부는, 암을 폴딩한 후, 이동 할 수 있다. 이후, 제어부는, 로봇의 다른 동작을 수행할 수 있다. 전술한 본 발명은, 프로그램이 기록된 매체에 컴퓨터가 읽을 수 있는 코드로서 구현하는 것이 가능하다. 컴퓨터 가 읽을 수 있는 매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 컴퓨터가 읽을 수 있는 매체의 예로는, HDD(Hard Disk Drive), SSD(Solid State Disk), SDD(Silicon Disk Drive), ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있다. 또한, 상기 컴 퓨터는 프로세서 또는 제어부를 포함할 수도 있다. 따라서, 상기의 상세한 설명은 모든 면에서 제한적으로 해석 되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적 해석에 의해 결정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다."}
{"patent_id": "10-2022-0151162", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 협동 로봇의 외관을 도시한 도면이다. 도 2는 본 발명의 실시예에 따른 협동 로봇의 제어 블럭도이다. 도 3은 본 발명의 실시예에 따른 협동 로봇의 플로우 차트이다. 도 4 내지 도 9는 본 발명의 실시예에 따른 협동 로봇의 동작을 설명하는데 참조되는 도면이다."}
