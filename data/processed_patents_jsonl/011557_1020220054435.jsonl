{"patent_id": "10-2022-0054435", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0154700", "출원번호": "10-2022-0054435", "발명의 명칭": "사물까지의 거리를 산출하는 로봇 및 방법", "출원인": "삼성전자주식회사", "발명자": "하창호"}}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇에 있어서, 2D 카메라;1D 거리 센서;상기 로봇을 이동시키는 주행 모듈;하나 이상의 인스트럭션을 저장하는 적어도 하나의 메모리; 및적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는 상기 메모리에 저장된 상기 하나 이상의인스트럭션을 실행함으로써,상기 2D 카메라를 제어하여 2D 이미지를 획득하고, 상기 획득된 2D 이미지에 기초하여 상기 2D 이미지 내의 픽셀들이 나타내는 실제 영역들의 상대적 깊이들을 산출하고,상기 1D 거리 센서로부터 출력된 레이저가 조사된 지점까지의 기준 거리를 획득하고, 상기 2D 이미지 내의 픽셀들 중 상기 레이저가 조사된 지점에 대응하는 기준점의 상대적 깊이와 상기 획득된 기준 거리에 기초하여 상기 로봇으로부터 상기 2D 이미지 내의 사물까지의 거리를 결정하고, 상기 결정된 사물까지의 거리에 기초하여 주행하도록 상기 주행 모듈을 제어하는, 로봇."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 적어도 하나의 프로세서는,상기 2D 이미지 내에서 상기 사물의 영역을 식별하고,상기 식별된 사물의 영역 내의 픽셀들의 상대적 깊이들, 상기 획득된 기준 거리 및 상기 기준점의 상대적 깊이에 기초하여 상기 로봇으로부터 상기 사물까지의 거리를 결정하는, 로봇."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 레이저는 상기 1D 거리 센서로부터 상기 2D 카메라의 센서 평면과 수직으로 조사되는, 로봇."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,상기 1D 거리 센서는 상기 2D 카메라와 함께 상기 로봇의 전방을 향해 배치되는, 로봇."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2023-0154700-3-제 1 항에 있어서,상기 1D 거리 센서는 PSD(Position Sensitive Device) 센서 또는 TOF(Time of Flight) 센서 중 적어도 하나를포함하고,상기 2D 카메라는 컬러 영상 생성 카메라를 포함하는, 로봇."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1 항에 있어서,상기 로봇은 두 개 이상의 1D 거리 센서를 포함하고,상기 적어도 하나의 프로세서는, 상기 두 개 이상의 1D 거리 센서로부터 출력된 레이저가 조사된 지점들까지의기준 거리들을 획득하고, 상기 두 개 이상의 1D 거리 센서에 대응하는 두 개 이상의 기준점들의 상대적 깊이들과 상기 획득된 기준 거리들에 기초하여 상기 로봇으로부터 상기 2D 이미지 내의 사물까지의 거리를 결정하는,로봇."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항에 있어서,상기 적어도 하나의 프로세서는,상기 사물의 종류를 식별하고,상기 결정된 사물까지의 거리 및 상기 식별된 사물의 종류에 기초하여, 상기 사물의 위치 및 종류를 나타내는지도를 생성하고,상기 생성된 지도에 기초하여 주행하는, 로봇."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서,상기 로봇은 움직임 센서를 더 포함하고,상기 적어도 하나의 프로세서는 상기 움직임 센서의 센서값에 기초하여, 상기 2D 이미지 내에서 상기 기준점의위치를 보정하는, 로봇."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항에 있어서,상기 적어도 하나의 프로세서는, 상기 2D 이미지를 인공 지능 모델의 입력 데이터로써 입력하고, 상기 인공 지능 모델을 실행함으로써 상기 2D 이미지 내의 픽셀들에 대응하는 상대적 깊이들을 산출하는, 로봇."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "로봇이 사물까지의 거리를 결정하는 방법에 있어서, 2D 카메라를 제어하여 2D 이미지를 획득하는 단계;상기 획득된 2D 이미지에 기초하여 상기 2D 이미지 내의 픽셀들이 나타내는 실제 영역들의 상대적 깊이들을 산출하는 단계;공개특허 10-2023-0154700-4-1D 거리 센서로부터 출력된 레이저가 조사된 지점까지의 기준 거리를 획득하는 단계;상기 2D 이미지 내의 픽셀들 중 상기 레이저가 조사된 지점에 대응하는 기준점의 상대적 깊이와 상기 획득된 기준 거리에 기초하여 상기 로봇으로부터 상기 2D 이미지 내의 사물까지의 거리를 결정하는 단계; 및상기 결정된 사물까지의 거리에 기초하여 주행하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10 항에 있어서,상기 2D 이미지 내의 사물까지의 거리를 결정하는 단계는,상기 2D 이미지 내에서 상기 사물의 영역을 식별하는 단계; 및상기 식별된 사물의 영역 내의 픽셀들의 상대적 깊이들, 상기 획득된 기준 거리 및 상기 기준점의 상대적 깊이에 기초하여 상기 로봇으로부터 상기 사물까지의 거리를 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 10 항에 있어서,상기 레이저는 상기 1D 거리 센서로부터 상기 2D 카메라의 센서 평면과 수직으로 조사되는, 방법."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 10 항에 있어서,상기 1D 거리 센서는 상기 2D 카메라와 함께 상기 로봇의 전방을 향해 배치되는, 방법."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 10 항에 있어서,상기 1D 거리 센서는 PSD(Position Sensitive Device) 센서 또는 TOF(Time of Flight) 센서 중 적어도 하나를포함하고,상기 2D 카메라는 컬러 영상 생성 카메라를 포함하는, 방법."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 10 항에 있어서,상기 1D 거리 센서는 두 개 이상의 1D 거리 센서를 포함하고, 상기 기준 거리를 획득하는 단계는, 상기 두 개 이상의 1D 거리 센서로부터 출력된 레이저들이 조사된 지점까지의 기준 거리들을 획득하는 단계를 포함하고, 상기 2D 이미지 내의 사물까지의 거리를 결정하는 단계는, 상기 두 개 이상의 1D 거리 센서에 대응하는 두 개이상의 기준점들의 상대적 깊이들과 상기 획득된 기준 거리들에 기초하여 상기 로봇으로부터 상기 2D 이미지 내의 사물까지의 거리를 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "공개특허 10-2023-0154700-5-제 10 항에 있어서,상기 사물까지의 거리를 결정하는 방법은,상기 사물의 종류를 식별하는 단계;상기 결정된 사물까지의 거리 및 상기 식별된 사물의 종류에 기초하여, 상기 사물의 위치 및 종류를 나타내는지도를 생성하는 단계; 및상기 생성된 지도에 기초하여 주행하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 10 항에 있어서,상기 기준점의 상대적 깊이에 기초하여 상기 로봇으로부터 상기 2D 이미지 내의 사물까지의 거리를 결정하는 단계는,움직임 센서의 센서값에 기초하여, 상기 2D 이미지 내에서 상기 기준점의 위치를 보정하는 단계; 및상기 보정된 기준점의 상대적 깊이에 기초하여 상기 로봇으로부터 상기 2D 이미지 내의 사물까지의 거리를 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0054435", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 10 항에 있어서,상기 2D 이미지 내의 픽셀들이 나타내는 실제 영역들의 상대적 깊이들을 산출하는 단계는,상기 2D 이미지를 인공 지능 모델의 입력 데이터로써 입력하고, 상기 인공 지능 모델을 실행함으로써 상기 2D이미지 내의 픽셀들에 대응하는 상대적 깊이들을 산출하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-0054435", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "사물까지의 거리를 결정하는 로봇 및 결정 방법이 제공된다. 본 개시의 일 실시예에 따른 로봇은, 2D 카메라, 1D 거리 센서, 로봇을 이동시키는 주행 모듈, 하나 이상의 인스트럭션을 저장하는 적어도 하나의 메모리 및 적어도 하나의 프로세서를 포함하고, 적어도 하나의 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로 써, 2D 카메라를 제어하여 2D 이미지를 획득하고, 획득된 2D 이미지에 기초하여 2D 이미지 내의 픽셀들이 나타내 는 실제 영역들의 상대적 깊이들을 산출하고, 1D 거리 센서로부터 출력된 레이저가 조사된 지점까지의 기준 거리 를 획득하고, 2D 이미지 내의 픽셀들 중 레이저가 조사된 지점에 대응하는 기준점의 상대적 깊이와 획득된 기준 거리에 기초하여 로봇으로부터 2D 이미지 내의 사물까지의 거리를 결정하고, 결정된 사물까지의 거리에 기초하여 주행할 수 있다."}
{"patent_id": "10-2022-0054435", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 로봇에 관한 것이다. 보다 구체적으로, 장애물까지의 거리를 산출하는 로봇에 관한 것이다."}
{"patent_id": "10-2022-0054435", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "로봇의 자율 주행에 있어서, 주변의 사물을 인식하고, 사물을 회피하여 주행하는 것이 중요하다. 사물까지의 거리가 부정확할 경우 로봇이 사물과 부딪히거나, 의도치 않은 방향으로 주행이 발생할 수 있다. 현재의 로봇은 사물 인식 센서를 통해 주변의 사물을 인식할 수 있다. 그러나, 사물 인식 센서만으로는 사물까 지의 거리를 정확하게 알 수 없는 문제점이 있다. 또한, 3D 센서를 탑재하여, 사물까지의 거리를 산출하는 로봇이 등장하고 있으나, 대부분의 3D 센서는 고가이기 때문에, 로봇의 가격이 비싸지는 문제점이 있다. 따라서, 고가의 3D 센서를 사용하지 않고, 주변 사물까지의 거리를 정확하게 검출할 수 있는 방법이 필요하다."}
{"patent_id": "10-2022-0054435", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 일 실시예들은 3D 센서를 사용하지 않고, 주변 사물까지의 거리를 결정하는 로봇 및 그 제어 방법을 제공하기 위한 것이다."}
{"patent_id": "10-2022-0054435", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예의 제 1 측면은, 사물까지의 거리를 결정하는 로봇에 있어서, 2D 카메라, 1D거리 센서, 로 봇을 이동시키는 주행 모듈, 하나 이상의 인스트럭션을 저장하는 적어도 하나의 메모리 및 적어도 하나의 프로 세서를 포함하고, 적어도 하나의 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 2D 카메 라를 제어하여 2D 이미지를 획득하고, 획득된 2D 이미지에 기초하여 2D 이미지 내의 픽셀들이 나타내는 실제 영 역들의 상대적 깊이들을 산출하고, 1D 거리 센서로부터 출력된 레이저가 조사된 지점까지의 기준 거리를 획득하 고, 2D 이미지 내의 픽셀들 중 레이저가 조사된 지점에 대응하는 기준점의 상대적 깊이와 획득된 기준 거리에 기초하여 로봇으로부터 2D 이미지 내의 사물까지의 거리를 결정하고, 결정된 사물까지의 거리에 기초하여 주행 하도록 주행 모듈을 제어하는 로봇을 제공할 수 있다. 또한, 본 개시의 일 실시예의 제 2 측면은, 로봇이 사물까지의 거리를 결정하는 방법에 있어서, 2D 카메라를 제 어하여 2D 이미지를 획득하는 단계, 획득된 2D 이미지에 기초하여 2D 이미지 내의 픽셀들이 나타내는 실제 영역 들의 상대적 깊이들을 산출하는 단계, 1D 거리 센서로부터 출력된 레이저가 조사된 지점까지의 기준 거리를 획 득하는 단계, 2D 이미지 내의 픽셀들 중 레이저가 조사된 지점에 대응하는 기준점의 상대적 깊이와 획득된 기준 거리에 기초하여 로봇으로부터 2D 이미지 내의 사물까지의 거리를 결정하는 단계 및 결정된 사물까지의 거리에 기초하여 주행하는 단계를 포함하는, 방법을 제공할 수 있다."}
{"patent_id": "10-2022-0054435", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에서, \"a, b 또는 c 중 적어도 하나\" 표현은 \" a\", \" b\", \" c\", \"a 및 b\", \"a 및 c\", \"b 및 c\", \"a, b 및 c 모두\", 혹은 그 변형들을 지칭할 수 있다. 아래에서는 첨부한 도면을 참조하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 개시의 실시 예를 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 또한, 도면에서 본 개시를 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 개시에서 사용되는 용어는, 본 개시에서 언급되는 기능을 고려하여 현재 사용되는 일반적인 용어로 기재되었 으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 다양한 다른 용어를 의미할 수 있다. 따라서 본 개시에서 사용되는 용어는 용어의 명칭만으로 해석되어서는 안되며, 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 해석되어야 한다. 또한, 제1, 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 구성 요소들은 이 용어들에 의해 한정되어서는 안 된다. 이 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으로 사용된 다. 또한, 본 개시에서 사용된 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것이며, 본 개시를 한정하려는 의도로 사용되는 것이 아니다. 단수의 표현은 문맥상 명백하게 단수를 뜻하지 않는 한, 복수의 의미를 포함한다. 또한, 명세서 전체에서, 어떤 부분이 다른 부분과 '연결'되어 있다고 할 때, 이는 '직접적으로 연결'되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 '전기적으로 연결'되어 있는 경우도 포함한 다. 또한 어떤 부분이 어떤 구성 요소를 '포함'한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 본 명세서에서 다양한 곳에 등장하는 '일부 실시 예에서' 또는 '일 실시 예에서' 등의 어구는 반드시 모두 동일 한 실시 예를 가리키는 것은 아니다. 도 1은 본 개시의 일 실시예에 따른, 로봇이 사물까지의 거리를 결정하는 방법을 도시한다. 도 1을 참조하면, 로봇은 적어도 하나의 2D 카메라 및 1D 거리 센서를 이용하여 로봇 주변의 사물들까지의 거리를 결정할 수 있다. 일 실시예에 따른, 로봇은 2D 카메라를 이용하여 획득한 2D 이미지에 기초하여 2D 이미지 내의 사 물들의 상대적 깊이를 산출하고, 1D 거리 센서를 이용하여 2D 이미지 내의 하나의 지점까지의 실제 거리 를 획득하고, 획득된 실제 거리에 기초하여, 산출된 사물들의 상대적 깊이를 거리로 변환함으로써 로봇으 로부터 2D 이미지 내의 사물들까지의 실제 거리를 추정할 수 있다. 예를 들어, 로봇은, 2D 카메라를 제어하여 로봇 전방에 위치한 사물들의 2D 이미지를 획득할 수 있 다. 로봇은 인공 지능 모델을 이용하여 획득된 2D 이미지 내의 픽셀들에 대응하는 실제 영역들의 상대적 깊이를 산출할 수 있다. 또한, 로봇은 1D 거리 센서를 제어하여 레이저 조사 지점까지의 기준 거리 를 획득할 수 있다. 로봇은 2D 이미지 내의 픽셀들 중 레이저 조사 지점에 대응하는 기준점의 상대 적 깊이를 획득할 수 있다. 로봇은 획득된 기준 거리, 기준점의 상대적 깊이 및 2D 이미지 내의 사물들 간의 상대적 깊이에 기초하여 로봇으로부터 2D 이미지 내의 사물들까지의 거리를 산출할 수 있다. 로봇은 주행 기능을 가질 수 있다. 예를 들어, 로봇은 전방에 위치한 장애물을 인식하고, 장애물을 회피하여 주행할 수 있다. 이 경우, 로봇은 2D 이미지 내의 사물의 종류를 인식하고, 인식된 사물의 종류 가 장애물로써 결정된 경우, 사물까지의 거리에 기초하여 장애물을 회피하여 주행할 수 있다. 또한, 로봇은 지도 내의 목적지의 위치 및 지도 내의 자신의 위치에 기초하여 목적지까지 주행할 수 있다. 또한, 로봇은 지도 내의 주행 경로 및 지도 내의 자신의 위치에 기초하여 주행 경로를 따라 주행할 수 있다. 로봇은 주행 기능을 가진 다양한 형태의 로봇에 대응될 수 있다. 로봇은 예를 들면, 진공 청소기 로봇, 물걸레 청소기 로봇, 서빙 로봇, 공기 청정기 로봇, 육아 로봇, 가사 로 봇 및 산업용 로봇을 포함할 수 있으나 이에 제한되지 않는다. 2D 카메라는 RGB 카메라를 포함할 수 있으나, 이에 제한되지 않는다. 또한, 1D 거리 센서는 PSD 센 서 및 TOF 센서를 포함할 수 있으나, 이에 제한되지 않는다. 도 2는 본 개시의 일 실시예에 따른, 로봇이 인공 지능 모델을 이용하여 2D 이미지 내의 픽셀들에 대한 상대적 깊이를 결정하는 방법을 도시한다. 도 2를 참조하면, 로봇은 하나의 2D 이미지 내의 각각의 픽셀에 대응하여 픽셀이 나타내는 실제 영역의 상대적 깊이를 결정할 수 있다. 상대적 깊이란, 2D 카메라의 RGB 센서 평면과 평행한 하나의 기준 평면으로부터 픽셀이 나타내는 실제 영역까지의 수직 거리를 의미할 수 있다. 기준 평면은 2D 카메라의 RGB 센서 평면 과 평행한 하나의 가상 평면일 수 있다. 기준 평면은 기 결정되어 로봇 내에 저장될 수 있으며, 2D 카메 라의 RGB 센서 평면일 수도 있다. 인공 지능 모델은, 예를 들어, 딥러닝 모델인 Pix2Pix 모델을 포함할 수 있으나 이에 제한되지 않는다. 인공 지능 모델은 미리 학습될 수 있다. 예를 들어, 2D 이미지를 훈련 데이터의 입력 데이터로 입력하고, 2D 이 미지 내의 각각의 픽셀이 나타내는 실제 지점의 측정된 깊이를 훈련 데이터의 출력 데이터로 입력하여 인공 지 능 모델을 훈련 시킬 수 있다. 인공 지능 모델은 2D 이미지 내의 각각의 픽셀에 대응하여, 각각의 픽셀이 나타내는 실제 지점의 기준 평면으로 부터의 거리가 산출되도록 학습될 수 있다. 많은 수의 2D 이미지에 대해 인공 지능 모델을 훈련 시킴으로써 인 공 지능 모델은 보다 정확한 깊이를 산출할 수 있다. 인공 지능 모델은 소프트웨어로 구현되어 로봇의 메모리에 저장될 수 있으며, 별도의 프로세서로 구현되 어 로봇에 구비될 수도 있다. 또한, 인공 지능 모델은 별도의 서버(미도시)에 저장될 수도 있다. 도 2를 참조하면, 로봇은 2D 카메라를 제어하여 로봇의 전방의 사물들을 나타내는 2D 이미지 를 획득할 수 있다. 또한, 로봇은 획득된 2D 이미지를 입력 데이터로 입력하고 인공 지능 모델 을 실행함으로써, 획득한 2D 이미지 내의 각각의 픽셀에 대하여, 픽셀이 나타내는 실제 지점의 상대적 깊 이를 산출할 수 있다. 2D 이미지 내의 각각의 픽셀에 대하여, 픽셀의 상대적 깊이를 색으로 변환하는 경우, 도 2의 깊이 이미지 가 생성될 수 있다. 예를 들어, 깊이 값이 클수록 빛의 파장이 긴 노란색, 빨간색 순서로 표현될 수 있으 며, 깊이 값이 작을수록 파장인 짧은 파란색, 보라색 순서로 표현될 수 있다. 또한, 실시예에 따라, 로봇은 각각의 픽셀의 상대적 깊이에 기초하여 깊이 이미지를 3D 이미지로 변 환할 수도 있다. 도 3는 본 개시의 일 실시예에 따른, 로봇이 인공 지능 모델을 이용하여 2D 이미지 내의 사물들의 상대적 깊이 및 사물들을 인식하는 방법을 도시한다. 도 3를 참조하면, 로봇은 2D 카메라를 제어하여 2D 이미지를 획득할 수 있다. 또한, 로봇 은 제 1 인공 지능 모델을 이용하여, 획득한 2D 이미지의 각각의 픽셀에 대응하는 상대적 깊이를 산 출하고, 산출된 상대적 깊이를 나타내는 깊이 이미지를 생성할 수 있다. 제 1 인공 지능 모델은 Pix2Pix 모델(도 2의 200)일 수 있다. 또한, 로봇은 제 2 인공 지능 모델을 이용하여, 획득한 2D 이미지 내의 사물들의 영역 및 종류 를 결정할 수 있다. 제 2 인공 지능 모델의 입력은 하나의 이미지일 수 있으며, 출력은 사물의 종류 (object class) 및 사물의 영역일 수 있다. 2D 이미지 내의 사물들의 영역은 바운딩 박스들(bonding boxes)로 표현될 수 있다. 제 2 인공 지능 모델은, 예를 들어, CNN(Convolutional Neural Network), R-CNN, YOLO(You Only Look Once), SSD(Single Shot MultiBox Detector) 등을 포함할 수 있으나, 이에 제한되지 않는다. 로봇은 제 2 인공 지능 모델을 이용하여 2D 이미지 내의 사물들의 영역을 이미지 내의 좌표로써 결 정할 수 있다. 또한, 로봇은 인식된 사물의 종류를 결정할 수 있다. 예를 들어, 로봇은 2D 이미지 내의 옷의 영역에 대하여 {105, 221, 187, 07, cloths}({x 좌표, y 좌표, 너비, 높이, 사물의 종류})로써 사물의 영역 및 종류를 결정할 수 있다. 또한, 로봇은, 각각의 픽셀에 대응하는 상대적 깊이 및 사물의 영역에 기초하여 사물들의 상대적 깊이를 결정할 수 있다. 예를 들어, 로봇은 사물의 영역 내의 픽셀들에 대응하는 상대적 깊이들 중 가장 작은 깊 이를 사물의 상대적 깊이로써 결정할 수 있다. 예를 들어, 도 3을 참조하면, 옷의 영역 내의 픽셀들의 상 대적 깊이들 중 가장 작은 깊이를 옷의 상대적 깊이로써 결정할 수 있다. 또한, 로봇은, 1D 거리 센서를 제어하여, 레이저 조사 지점까지의 거리를 획득할 수 있다. 예를 들 어, 1D 거리 센서로부터 출력된 레이저가 사물에 의해 반사되어 수신됨에 따라, 로봇은 레이저가 출력되어 다시 수신될때까지의 시간 및 레이저의 전파 속도에 기초하여 레이저가 조사된 지점까지의 거리를 획득 할 수 있다. 레이저 조사 지점은 2D 이미지 내의 기 결정된 기준점에 대응될 수 있다. 2D 이미지 내에서의 기준점의 위 치는 로봇이 판매되기 전에 미리 캘리브레이션되어 로봇에 저장될 수 있다. 2D 이미지 내에서 기준 점의 위치는 1D 거리 센서의 레이저가 조사된 지점과 2D 카메라를 통해 획득된 2D 이미지를 매칭함 으로써 결정될 수 있다. 예를 들어, 2D 이미지 내에서 1D 거리 센서의 레이저가 조사된 실제 지점을 나타 내는 픽셀의 위치가 기준점으로 결정될 수 있다. 로봇 내에서 1D 거리 센서 및 2D 카메라의 위치 및 각도는 변하지 않으므로 2D 이미지 내에서 기준점의 위치 또한 고정될 수 있다. 또한, 1D 거리 센서의 레이저는 조사 각도가 기준 평면(예를 들어, 2D 카메라의 RGB 센서 평면)에 수직이 되도록 미리 조정될 수 있다. 로봇은 레이저 조사 지점까지의 거리, 기준점의 상대적 깊이 및 사물들간의 상대적 깊이에 기초하여 사물들까지의 거리를 산출할 수 있다. 도 4는 본 개시의 일 실시예에 따라, 로봇이 기준 거리에 기초하여 주변 사물까지의 거리를 결정하는 방법을 도 시한다. 도 4를 참조하면, 로봇은 1D 거리 센서를 제어하여 레이저가 조사된 지점까지의 기준 거리 를 검출할 수 있다. 또한, 로봇은 2D 카메라를 제어하여 로봇 전방의 제 1 사물, 제 2 사물 및 제 3 사물을 포함하는 2D 이미지를 획득할 수 있다. 도 4의 상단은 로봇과 사물 들(410, 420 및 430)의 탑뷰를 나타낸 것이고, 도 4의 하단은 로봇과 사물들(410, 420 및 430)의 측면뷰 를 나타낸 것이다. 로봇은 획득한 2D 이미지를 입력값으로 입력하고, 인공 지능 모델들을 실행함으로써, 획득된 2D 이미지 내의 사물들(410, 420 및 430)간의 상대적 깊이를 산출할 수 있다. 사물들(410, 420 및 430)간의 상대적 깊이는 기준 평면으로부터 사물들(410, 420 및 430)까지의 수직 거리일 수 있다. 기준 평면은 로봇으 로부터의 거리에 상관없이 2D 카메라의 센서 평면과 평행한 평면들 중 하나로써 정의될 수 있으나, 도 4 에서는 설명의 편의를 위해, 2D 카메라의 센서 평면과 평행한 평면들 중 제 2 사물과 맞닿은 평면이 기준 평면으로써 설명된다. 로봇은 레이저가 조사된 지점까지의 기준 거리, 2D 이미지 내의 상대적 깊이들 중 레이저가 조 사된 지점에 대응하는 기준점의 상대적 깊이 및 사물들(410, 420 및 430)간의 상대적 깊이에 기초하여 로 봇으로부터 사물들(410, 420 및 430)까지의 거리를 결정할 수 있다. 예를 들어, 로봇은 기준점의 상대적 깊이와 제 1 사물의 상대적 깊이 간의 차이에 기준 거리를 더한 값을 제 1 사물의 거리로써 결정할 수 있다. 도 4를 참조하면, 레이저가 조사된 지점에 대응하는 기준점의 상대적 깊이가 0인 경우, 로봇은 조사된 지점까지의 거리에 제 1 사물의 깊이를 더 한 값을 로봇으로부터 제 1 사물까지의 거리로써 결정할 수 있다. 실시예에 따라, 기준 평면 이 변경되어 제 1 사물의 상대적 깊이가 0이 아닌 값으로 변경되더라도, 기준점의 상대적 깊이와 제 1 사 물의 상대적 깊이 간의 차이는 변하지 않으므로, 기준점의 상대적 깊이와 제 1 사물의 상대적 깊이 간의 차이에 기준 거리를 더함으로써 제 1 사물까지의 거리를 산출할 수 있다. 다른 예를 들어, 로봇은 기준 거리에 제 3 사물의 상대적 깊이와 기준점의 상대적 깊이 (도 4에서는 0) 간의 차이를 더한 값을 직각 삼각형의 밑변으로써 결정하고, 조사된 지점과 제 3 사 물간의 기준 평면 내에서의 거리를 직각 삼각형의 높이로써 결정하고, 결정된 밑변과 높이 를 갖는 직각삼각형의 빗변 거리를 로봇으로부터 제 1 사물까지의 거리로써 결정할 수도 있다. 도 5는 본 개시의 일 실시예에 따른, 로봇의 블록도를 도시한다. 도 5를 참조하면, 로봇은 프로세서, 메모리, 주행 모듈, 1D 거리 센서 및 2D 카메라를 포함할 수 있다. 1D 거리 센서는 TOF 센서 또는 PDS 센서를 포함할 수 있다. TOF 센서는 짧은 빛 펄스를 조사한 후, 조사된 빛 펄스가 물체에 반사되어 돌아오는 에코 펄스를 검출하고, 빛 펄스를 조사한 시간과 에코 펄스를 검출한 시간의 차이 및 빛의 속도에 기초하여, TOF 센서로부터 물체까지의 거리를 검출하는 센서일 수 있다. TOF 센서는 빛 펄스를 방출하는 Emitter와 에코 펄스를 검출하는 Sensor를 포 함할 수 있으며, Emitter는 기 결정된 각도로 하나의 지점에 대하여 레이저를 조사할 수 있다. TOF 센서는 1 차 원 direct TOF 센서로 언급될 수 있다. 2D 카메라는 물체로부터 반사되는 자외선, 가시광선 또는 적외선을 검출하는 센서를 포함할 수 있다. 2D 카메라가 가시광선을 검출하는 센서를 포함하는 경우, 로봇은 2D 카메라로부터 RGB 이미지를 획득할 수 있다. 또한, 2D 카메라가 적외선을 검출하는 센서를 포함하는 경우, 로봇은 2D 카메라 로부터 적외선 이미지를 획득할 수도 있다. 주행 모듈은 로봇의 본체를 이동시킨다. 주행 모듈은 프로세서에 의해 설정된 제어 신 호에 따라 구동된다. 프로세서는 주행 경로를 설정하고, 주행 경로에 따라 로봇이 이동하도록 제어 신호를 생성하여 주행 모듈로 출력한다. 주행 모듈은 로봇의 바퀴들을 회전 구동시키는 모터 및 후방의 바퀴에서 발생되는 동력을 전방의 바퀴로 전달할 수 있도록 설치된 타이밍 벨트 등을 포함할 수 있다. 프로세서는 통상적으로 로봇의 전반적인 동작을 제어할 수 있다. 프로세서는 메모리에 저장된 프로그램들을 실행함으로써, 2D 카메라, 1D 거리 센서 및 주행 모듈을 제어할 수 있 다. 메모리는 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수 있다. 메모리는 인공 지능 모델들을 구현한 소프트웨어 모듈들을 저장할 수도 있다. 프로세서는 메모리에 저장된 소프트웨어 모듈들을 실행함으로써 전방의 사물들까지의 거리를 결정할 수 있다. 또한, 다른 실시예로써, 인공 지능 모델들을 구현한 소프트웨어 모듈은 서버(미도시)에 저장될 수 있다. 이 경 우, 프로세서는 1D 거리 센서를 이용하여 획득한 기준 거리 및 2D 카메라를 이용하여 획득한 2D 이미지를 서버(미도시)에게 전송하고, 로봇으로부터 2D 이미지 내의 사물들까지의 거리에 관한 정보를 서버(미도시)로부터 수신할 수 있다. 프로세서는 2D 카메라를 제어하여 2D 이미지를 획득할 수 있다. 또한, 프로세서는 획득된 2D 이미지에 기초하여 2D 이미지 내의 픽셀들이 나타내는 실제 영역들의 상대적 깊이들을 산출할 수 있다. 또한, 프로세서는 1D 거리 센서를 제어하여, 레이저가 조사된 지점까지의 기준 거리를 획득할 수 있다. 또한, 프로세서는 획득된 기준 거리, 2D 이미지 내의 픽셀들 중 레이저가 조사된 지점에 대응하는 기준점의 상대적 깊이에 기초하여 로봇으로부터 2D 이미지 내의 사물까지의 거리를 결정할 수 있다. 또한, 프로세서는 결정된 사물까지의 거리에 기초하여 주행 모듈을 제어함으로써 주행할 수 있다. 예를 들어, 인식된 사물의 종류가 컵 또는 견변과 같은 일정 거리 이내로 접근해서는 안되는 사물인 경우, 프로 세서는 결정된 사물까지의 거리가 기 설정된 거리 이내인지 여부를 판단할 수 있으며, 기 설정된 거리 이 내로 접근하지 않도록 주행 경로를 변경할 수 있다. 일정 거리 이내로 접근해서는 안되는 사물의 종류는 메모리 또는 서버(미도시)에 저장될 수 있으며, 사물의 종류마다 사물의 종류에 대응되는 접근 가능 거리가 미 리 저장되어 있을 수 있다. 도 6은 본 개시의 일 실시예에 따른, 로봇이 사물까지의 거리를 결정하는 방법의 흐름도이다. 단계 S610에서, 로봇은, 2D 카메라를 제어하여 2D 이미지를 획득할 수 있다. 2D 카메라는 RGB 카메라를 포함할 수 있다. 단계 S620에서, 로봇은, 획득된 2D 이미지에 기초하여 2D 이미지 내의 픽셀들이 나타내는 실제 영역들의 상대적 깊이들을 산출할 수 있다. 단계 S630에서, 로봇은, 1D 거리 센서로부터 출력된 레이저가 조사된 지점까지의 기준 거리를 획득 할 수 있다.1D 거리 센서는 PSD 센서 및 TOF 센서 중 적어도 하나를 포함할 수 있다. 또한, 1D 거리 센서는 2D 카메라와 함께 로봇의 전방을 향해 배치될 수 있다. 또한, 로봇은 두 개 이상의 1D 거리 센서를 포함할 수 있다. 또한, 레이저는 1D 거리 센서로부터 2D 카메라의 RGB 센서 평면과 수직으로 조사될 수 있다. 단계 S640에서, 로봇은, 2D 이미지 내의 픽셀들 중 레이저가 조사된 지점에 대응하는 기준점의 상대적 깊 이와 획득된 기준 거리에 기초하여 로봇으로부터 2D 이미지 내의 사물까지의 거리를 결정할 수 있다. 일 실시예에 따른, 로봇은, 2D 이미지 내에서 사물의 영역을 식별할 수 있다. 또한, 로봇은, 식별 된 사물의 영역 내의 픽셀들의 상대적 깊이들, 획득된 기준 거리 및 기준점의 상대적 깊이에 기초하여 로봇 으로부터 사물까지의 거리를 결정할 수 있다. 또한, 일 실시예에 따른, 로봇은, 사물의 종류를 식별할 수 있다. 사물의 종류는 옷, 책상, 의자, 가구, 가전 제품, 매트, 책, 전선, 컵, 동물의 변, 흘린 음식물 및 쏟아진 액체일 수 있으나, 이에 제한되지 않는다. 또한, 로봇은, 결정된 사물까지의 거리 및 식별된 사물의 종류에 기초하여, 사물의 위치 및 종류를 나타 내는 지도를 생성할 수 있다. 또한, 로봇은, 생성된 지도에 기초하여 주행할 수 있다. 또한, 일 실시예에 따른, 로봇은, 움직임 센서의 센서값에 기초하여, 2D 이미지 내에서 기준점의 위치를 보정할 수 있다. 또한, 로봇은, 보정된 기준점의 상대적 깊이에 기초하여 로봇으로부터 2D 이미지 내의 사물까지의 거리를 결정할 수 있다. 또한, 일 실시예에 따른, 로봇은, 2D 이미지를 인공 지능 모델의 입력 데이터로써 입력하고, 인공 지능 모델을 실행함으로써 2D 이미지 내의 픽셀들에 대응하는 상대적 깊이들을 산출할 수 있다. 단계 S650에서, 로봇은, 결정된 사물까지의 거리에 기초하여 주행할 수 있다. 도 7은 본 개시의 다른 실시예에 따른, 로봇의 블록도를 도시한다. 도 7을 참조하면, 로봇은 프로세서, 메모리, 주행 모듈, 1D 거리 센서 및 2D 카메라뿐만 아니라, IMU(Inertial Measurement Unit, 1930), Timer 및 AI(Artificial Intelligence) 프로세서를 포함할 수 있다. IMU는 로봇의 속도 및 방향, 가속도를 검출할 수 있다. IMU는 3축 가속도계 센서 및 3축 각 속도계 센서를 포함할 수 있으며, 가속도 및 각속도를 적분하여 로봇의 속도 및 자세각을 산출할 수 있다. Timer는 시간을 시, 분 및 초로써 카운트할 수 있다. AI 프로세서는 인공 지능을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 기존의 범용 프로세서 또는 그래픽 전용 프로세서(예를 들어, GPU)의 일부로 제작될 수도 있다. 다른 예로서, AI 프로세서는 NPU(Neural Processing Unit)에 대응될 수 있다. AI 프로세서는 2D 이미지를 입력 데이터로써 입력했을 때, 2D 이미지 내의 픽셀에 대응하여 픽셀이 나타 내는 실제 영역의 깊이를 출력 데이터로써 출력하는 인공 지능 모델이 구현되도록 설계될 수 있다. 또한, AI 프로세서는 2D 이미지를 입력 데이터로써 입력했을 때, 2D 이미지 내의 사물들의 영역 및 종류를 식별하 는 인공 지능 모델이 구현되도록 설계될 수 있다. 프로세서는, AI 프로세서를 제어하여 2D 이미지 내의 사물들의 영역 및 종류를 식별하고, 2D 이미 지 내의 픽셀이 나타내는 실제 영역의 깊이를 획득할 수 있다. 프로세서는, Timer가 카운트하는 시각에 기초하여, 2D 이미지를 획득할 때의 시각을 저장할 수 있 다. 또한, 프로세서는, Timer가 카운트하는 시각에 기초하여, 기준 거리를 획득할 때의 시각을 저장할 수 있다. 프로세서는 동일한 시각에 획득된 2D 이미지 및 기준 거리에 기초하여 사물들까지의 거리를 산출할 수 있 다. 또한, 프로세서는, IMU를 제어하여 로봇의 이동 거리 및 움직인 각도를 주기적으로 검출할 수 있다. 또한, 프로세서는 검출된 이동 거리 및 움직인 각도에 대응하여 검출된 시각을 저장할 수 있다. 또한, 프로세서는, 2D 이미지가 획득된 시각과 기준 거리가 획득된 시각의 차이 동안 로봇이 이동 한 거리 및 움직인 각도에 기초하여 2D 이미지 내의 기준점의 위치를 보정할 수 있다. 기준점의 위치를 보정하 는 방법은 도 8에서 후술된다. 프로세서는 보정된 기준점의 상대적 깊이에 기초하여 로봇으로부터 2D 이미지 내의 사물까지의 거 리를 결정할 수 있다. 도 8은 본 개시의 일 실시예에 따른, 로봇의 움직임에 기초하여 기준점을 보정하는 방법의 흐름도를 도시한다. 단계 S810에서, 로봇은, 2D 이미지를 획득할 때의 시각을 저장할 수 있다. 로봇은 Timer를 이용하여 2D 이미지를 획득할 때의 시각을 저장할 수 있다. 로봇은 기 결정 된 주기로 2D 이미지를 획득할 수 있으며, 주기적으로 획득된 2D 이미지 각각에 대하여 획득된 시간을 저장할 수 있다. 단계 S820에서, 로봇은, 기준 거리를 획득할 때의 시각을 저장할 수 있다. 로봇은 Timer를 이용하여 기준 거리를 획득할 때의 시각을 저장할 수 있다. 로봇은 2D 이미 지와 기준 거리를 동일한 시각에 검출할 수 있으며, 서로 다른 시각에 검출할 수도 있다. 단계 S830에서, 로봇은, 2D 이미지가 획득된 시각, 기준 거리가 획득된 시각 및 로봇의 움직임에 기초하여 레이저가 조사된 지점에 대응하는 2D 이미지 내의 기준점을 보정할 수 있다. 로봇은 IMU를 이용하여 로봇의 이동 거리 및 움직인 각도를 주기적으로 검출할 수 있다. 또 한, 로봇은 검출된 이동 거리 및 움직인 각도에 대응하여 검출된 시각을 저장할 수 있다. 로봇은 2D 이미지가 획득된 시각과 기준 거리가 획득된 시각의 사이에 로봇이 이동한 거리 및 움직 인 각도에 기초하여 2D 이미지 내의 기준점의 위치를 보정할 수 있다. 로봇이 이동하더라도 2D 이미지 내 의 기준점의 위치는 변경되지 않으나, 기준 거리를 획득한 시각과 2D 이미지를 획득한 시각의 차이가 기준 시간 이상인 경우, 기준 거리가 획득된 지점과 2D 이미지 내의 기준점이 나타내는 실제 지점이 상이할 수 있다. 따라 서, 기준 거리를 획득한 시각과 2D 이미지를 획득한 시각의 차이가 기준 시간 이상인 것으로 결정됨에 따라, 로 봇은 로봇이 이동한 거리 및 움직인 각도에 기초하여 2D 이미지 내의 기준점의 위치를 보정할 수 있다. 예를 들어, 2D 이미지가 획득된 후 기준 거리가 획득될 때까지 로봇이 왼쪽으로 이동한 경우, 이동 한 거리에 기초하여 기준점의 위치를 왼쪽으로 이동시킬 수 있다. 단계 S840에서, 로봇은, 보정된 기준점의 상대적 깊이에 기초하여 로봇으로부터 2D 이미지 내의 사 물까지의 거리를 결정할 수 있다. 이에 따라, 로봇이 1D 거리 센서 및 2D 카메라를 제어할 때 로봇이 움직이더라도 사물 까지의 거리를 보다 정확하게 산출할 수 있다. 도 9는 본 개시의 일 실시예에 따른, 로봇이 복수의 1D 거리 센서를 이용하여 사물까지의 거리를 결정하는 방법 을 도시한다. 도 9를 참조하면, 로봇은 복수의 1D 거리 센서를 포함할 수 있다. 예를 들어, 로봇은 2D 카 메라를 중심으로 제 1 1D 거리 센서 (1910a) 및 제 2 1D 거리 센서(1910b)를 포함할 수 있다. 로봇은 2D 카메라를 제어하여 2D 이미지를 획득할 수 있다. 또한, 로봇은 제 1 1D 거리 센서 (1910a) 및 제 2 1D 거리 센서(1910b)를 제어하여 제 1 거리 및 제 2 거리를 검출할 수 있다. 로봇은 획득한 2D 이미지 내의 픽셀들에 대응하여 픽셀이 나타내는 실제 영역의 상대적 깊이를 산출할 수 있다. 또한, 로봇은 2D 이미지 내에서 제 1 1D 거리 센서(1910a)의 조사 지점에 대응하는 제 1 기준점에 서의 제 1 상대적 깊이와 제 1 거리 및 제 2 1D 거리 센서(1910b)의 조사 지점에 대응하는 제 2 기준점에서의제 2 상대적 깊이와 제 2 거리에 기초하여 로봇으로부터 2D 이미지 내의 사물들까지의 거리를 보다 정확 하게 결정할 수 있다. 예를 들어, 로봇은 제 1 거리와 제 2 거리 간의 차이와 제 1 상대적 깊이와 제 2 상대적 깊이 간의 차이 의 비율에 기초하여 2D 이미지 내의 픽셀들의 상대적 깊이를 보정할 수 있다. 픽셀들의 상대적 깊이를 보정한 후, 로봇은 보정된 픽셀들의 상대적 깊이에 기초하여 로봇으로부터 2D 이미지 내의 사물들까지의 거리를 결정할 수 있다. 또한, 실시예에 따라, 로봇은 3 개 이상의 1D 거리 센서를 포함할 수 있다. 도 10은 본 개시의 일 실시예에 따른, 로봇이 지도를 생성하는 방법을 도시한다. 도 10을 참조하면, 로봇은 사물까지의 거리 및 사물의 식별 정보에 기초하여 사물의 위치를 나타내는 지 도를 생성할 수 있다. 로봇은 소정의 주행 영역에서 주행할 수 있다. 주행 영역은 예를 들면, 댁내 영역, 사무실, 상점 등의 공간에 대응될 수 있다. 로봇은 주행 영역에 대한 지도를 생성하고, 지도 상에서 주행 경로를 생성할 수 있다. 로봇은 주행 영역 내에서의 자신의 위치, 자세각 및 사물까지의 거리에 기초하여 주행 영역에서의 사물의 위치를 산출할 수 있다. 또한, 로봇은 2D 이미지 내의 사물의 종류를 식별할 수 있다. 로봇은 산출된 사물의 위치 및 식별된 사물의 종류에 기초하여 주행 영역의 지도상에 사물의 위치 및 종 류를 나타내는 지도를 생성할 수 있다. 또한, 로봇은 주행 중에 실시간으로 인식된 사물의 위치 및 종류 에 기초하여 지도를 업데이트할 수 있다. 일 실시예에 따르면, 로봇은 디바이스와 통신하고, 디바이스는 로봇의 동작에 관련된 정보, GUI(Graphic User Interface) 등을 제공할 수 있다. 디바이스는 로봇에 대한 정보를 제공하 고 로봇을 제어하기 위한 소정의 애플리케이션을 저장하고 실행할 수 있다. 디바이스는 서버(300 0)를 통해 로봇과 통신하거나, 근거리 통신을 통해 로봇과 직접 통신할 수 있다. 디바이스는 예를 들면, 통신 단말, 모바일 장치, 스마트폰, 태블릿 PC, 또는 웨어러블 장치 등에 대응될 수 있다. 도 10에 도시된 바와 같이, 로봇은, 주행 영역의 바닥에 놓인 사물의 종류 및 위치를 디바이스에게 전송할 수 있다. 로봇은 주행 중에 실시간으로 사물의 종류 및 위치를 인식하고, 인식된 사물의 종류 및 위치를 디바이스로 전송할 수 있다. 다른 예로서, 로봇은 청소를 완료한 후에, 인식된 사물의 종류 및 위치를 디바이스로 전송할 수 있다. 로봇으로부터 사물의 종류 및 위치를 수신함에 따라, 디바 이스는 주행 영역의 지도 상에, 사물의 종류 및 위치를 디스플레이할 수 있다. 실시예에 따라, 로봇은 주행 영역의 바닥에 놓인 사물들 중 사용자에 의해 치워져야 하는 사물의 위치 및 종류를 디바이스에게 전송할 수 있다. 치워져야 하는 사물의 종류는 로봇에 기 저장되어 있을 수 있다. 예를 들어, 치워져야 하는 사물의 종류는 견변, 컵, 옷, 책, 음식 등을 포함할 수 있다. 로봇이 2D 이미지 내의 사물을 견변으로 식별함에 따라, 로봇은 견변의 위치 및 식별 정보를 저장하고, 견변의 위치 및 식별 정보를 나타내는 지도를 디바이스에게 전송할 수 있다. 또한, 다른 실시예에 따라, 치워져야 하는 사물인지 여부는 서버에서 결정될 수도 있으며 디바이스(200 0)에서 결정될 수도 있다. 서버는 스마트 싱스 서버 또는 AI 서버일 수 있으나, 이에 제한되지 않는다. 또한, 도 10에 도시된 바와 같이, 디바이스는 치워져야 하는 사물의 위치 및 종류를 수신함에 따라, 치워 져야 하는 사물에 대한 GUI 가이드를 디스플레이할 수 있다. 예를 들어, 주행 영역을 나타내는 지도 상에 치워 져야 하는 사물로써 옷, 견변, 컵의 위치 및 식별 정보를 디스플레이할 수 있다. 또한, 디바이스는'장애물 구역 재청소'버튼을 누르는 사용자 입력을 수신함에 따라, 로봇에게 치워 져야 하는 사물의 위치를 다시 청소할 것을 요청하는 장애물 구역 재청소 요청을 전송할 수 있다. 로봇은 장애물 구역 재청소 요청을 수신함에 따라, 치워져야 하는 사물의 위치 정보 및 사물이 존재하는지 여부에 기초 하여 사물의 영역을 다시 청소할 수 있다. 도 11은 본 개시의 다른 실시예에 따른, 로봇의 블록도를 도시한다. 도 11을 참조하면, 로봇은 마이크로폰, 통신 모듈, 메모리, 입력 인터페이스, 출력 모듈, 센서, 주행 모듈, 프로세서 및 1D 거리 센서 및 2D 카메라를 포함할 수 있다. 도 5에 도시된 구성과 동일한 구성에 대해서는 동일한 참조 번호가 사용되었다. 도시된 구성 요소 모두가 로봇의 필수 구성 요소인 것은 아니다. 도 11에 도시된 구성 요소보다 많은 구 성 요소에 의해 로봇이 구현될 수도 있고, 도 11에 도시된 구성 요소보다 적은 구성 요소에 의해 로봇 이 구현될 수도 있다. 출력 모듈은, 음향 출력 모듈 및 디스플레이를 포함할 수 있다. 음향 출력 모듈는 음향 신호를 로봇의 외부로 출력할 수 있다. 음향 출력 모듈은, 예를 들면, 스피커 또는 리시버를 포함할 수 있다. 스피커는 멀티미디어 재생 또는 녹음 재생과 같이 일반적인 용도 로 사용될 수 있다. 디스플레이는 프로세서의 제어에 따라, 영상 처리부(미도시)에서 이미지 처리된 이미지 데이터를 디스플레이 패널(미도시)을 통해 출력할 수 있다. 디스플레이 패널(미도시)은 액정 디스플레이(liquid crystal display), 박막 트랜지스터 액정 디스플레이(thin film transistor-liquid crystal display), 유기 발광 다이 오드(organic light-emitting diode), 플렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display), 전기영동 디스플레이(electrophoretic display) 중에서 적어도 하나를 포함할 수 있다. 입력 인터페이스는 로봇을 제어하기 위한 사용자 입력을 수신할 수 있다. 입력 인터페이스는 사용자 입력을 수신하여 프로세서로 전달한다. 입력 인터페이스는 사용자의 터치를 감지하는 터치 패널, 사용자의 푸시 조작을 수신하는 버튼, 사용자의 회전 조작을 수신하는 휠, 키보드(key board), 및 돔 스위치 (dome switch) 등을 포함하는 사용자 입력 전자 장 치를 포함할 수 있으나 이에 제한되지 않는다. 또한, 입력 인터페이스는 음성 인식을 위한 음성 인식 장치를 포함할 수 있다. 예를 들어, 음성 인식 장 치는 마이크로폰이 될 수 있으며, 음성 인식 장치는 사용자의 음성 명령 또는 음성 요청을 수신할 수 있 다. 그에 따라서, 프로세서는 음성 명령 또는 음성 요청에 대응되는 동작이 수행되도록 제어할 수 있다. 메모리는 로봇의 동작에 필요한 다양한 정보, 데이터, 명령어, 프로그램 등을 저장한다. 메모리 는 휘발성 메모리 또는 비휘발성 메모리 중 적어도 하나 또는 이들의 조합을 포함할 수 있다. 메모리 는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크 로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 또한, 로봇은 인터넷(internet) 상에서 저장 기능을 수행하는 웹 스토리지(web storage) 또는 클라우드 서버를 운영할 수도 있다. 통신 모듈은 프로세서의 제어에 따라 외부 장치 또는 외부 서버와 프로토콜에 따라 정보를 송수할 수 있다. 통신 모듈는 외부 장치(미도시)와 데이터를 송수신하는 적어도 하나의 통신 모듈 및 적어도 하 나의 포트를 포함할 수 있다. 또한, 통신 모듈은 적어도 하나의 유선 또는 무선 통신 네트워크를 통해서 외부 장치와 통신을 수행할 수 있다. 통신 모듈은 근거리 통신 모듈 또는 원거리 통신 모듈 중 적어도 하나 또는 이들의 조 합을 포함할 수 있다. 통신 모듈은 다른 장치와 무선으로 통신하기 위한 적어도 하나의 안테나를 포함할 수 있다. 근거리 통신 모듈은 블루투스, 와이파이, BLE(Bluetooth Low Energy), NFC/RFID, 와이파이 다이렉트 (Wifi Direct), UWB, 또는 ZIGBEE 등의 통신 규격에 따른 통신을 수행하는 적어도 하나의 통신 모듈(미도시)를 포함할 수 있다. 또한, 이동 통신 모듈는 인터넷 통신을 위한 네트워크를 통하여 통신을 수행하는 통신 모듈(미도시)를 포함할 수 있다. 또한, 원거리 통신 모듈은 3G, 4G, 5G, 및/또는 6G 등의 통신 규격에 따 른 통신을 수행하는 이동 통신 모듈을 포함할 수 있다. 또한, 통신 모듈는 근거리에 위치하는 원격 제어 장치(remote controller)(미도시)로부터 제어 명령을 수 신할 수 있는 통신 모듈, 예를 들어, IR(infrared) 통신 모듈 등을 포함할 수 있다. 센서는 다양한 종류의 센서를 포함할 수 있다. 센서는 로봇 주변 환경에 관한 정보를 감지하도록 구성되는 다수의 센서들을 포함할 수 있다. 예를 들어, 센서는 추락 방지 센서(미도시), 초음파 센서(미도시), 움직임 센서(미도시) 및 주행거리 검출 센 서(미도시) 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 주행거리 검출 센서는 바퀴의 회전수를 계산하 는 회전검출 센서를 포함할 수 있다. 예를 들면, 회전 검출센서는 모터의 회전수를 검출하도록 설치된 엔코더가 있을 수 있다. 각 센서들의 기능은 그 명칭으로부터 당업자가 직관적으로 추론할 수 있으므로, 구체적인 설명은 생략하기로 한다. 프로세서는 로봇의 전반적인 동작을 제어한다. 프로세서는 메모리에 저장된 프로그램 을 실행하여, 로봇의 구성 요소들을 제어할 수 있다. 실시예에 따라, 프로세서는 기계학습 모델의 동작을 수행하는 별도의 NPU를 포함할 수 있다. 또한, 프로 세서는 중앙 처리부(CPU), 그래픽 전용 프로세서(GPU; Graphic Processing Unit) 등을 포함할 수 있다. 가전 기능 모듈은 가전 기능을 수행하는 장치들의 동작에 이용되는 구성들을 포함할 수 있다. 가전 기능 모듈은 흡입부 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 흡입부는, 공기를 흡입하면서 바닥의 먼지를 집진하는 기능을 하는데, 회전브러쉬 또는 빗자루, 회전브러쉬 모터, 공기흡입구, 필터, 집진실, 공기배 출구 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 흡입부는, 부가적으로 구석 먼지를 쓸어낼 수 있는 솔이 회전되는 구조로 장착될 수도 있다. 주행 모듈은 로봇의 본체를 이동시킨다. 주행 모듈은 프로세서에 의해 설정된 제어 신 호에 따라 구동된다. 프로세서는 주행 경로를 설정하고, 주행 경로에 따라 로봇이 이동하도록 제어 신호를 생성하여 주행 모듈로 출력한다. 주행 모듈은 전방의 양측에 설치된 두 개의 바퀴와 후방의 양측에 설치된 두 개의 바퀴, 후방의 두 개의 바퀴를 각각 회전 구동시키는 모터 및 후방의 바퀴에서 발생되는 동력을 전방의 바퀴로 전달할 수 있도록 설치된 타이밍 벨트 등이 포함될 수 있으나, 이에 한정되는 것은 아니 다. 1D 거리 센서는 TOF 센서 또는 PDS 센서를 포함할 수 있다. 1D 거리 센서 및 2D 카메라는 도 5를 참조하여 설명될 수 있다. 프로세서는 2D 카메라를 제어하여 2D 이미지를 획득할 수 있다. 또한, 프로세서는 획득된 2D 이미지에 기초하여 2D 이미지 내의 픽셀들이 나타내는 실제 영역들의 상대적 깊이들을 산출할 수 있다. 또한, 프로세서는 1D 거리 센서를 제어하여, 레이저가 조사된 지점까지의 기준 거리를 획득할 수 있다. 또한, 프로세서는 획득된 기준 거리, 2D 이미지 내의 픽셀들 중 레이저가 조사된 지점에 대응하는 기준점의 상대적 깊이에 기초하여 로봇으로부터 2D 이미지 내의 사물까지의 거리를 결정할 수 있다. 또한, 프로세서는 결정된 사물까지의 거리에 기초하여 주행 모듈을 제어함으로써 주행할 수 있다. 프로세서는 2D 이미지 내에서 사물의 영역을 식별할 수 있다. 또한, 프로세서는 식별된 사물의 영 역 내의 픽셀들의 상대적 깊이들, 획득된 기준 거리 및 기준점의 상대적 깊이에 기초하여 로봇으로부터 사물까지의 거리를 결정할 수 있다. 1D 거리 센서의 레이저는 1D 거리 센서로부터 2D 카메라의 RGB 센서 평면과 수직으로 조사되 도록 구비될 수 있다. 1D 거리 센서는 2D 카메라와 함께 로봇의 전방을 향해 배치될 수 있다. 1D 거리 센서는 PSD 센서(미도시) 및 TOF 센서(미도시) 중 적어도 하나를 포함할 수 있다. 2D 카메라는 RGB 카메라(미도시)를 포함할 수 있다. 로봇은 두 개 이상의 1D 거리 센서를 포 함할 수 있다. 프로세서는 사물의 영역 및 사물의 종류를 식별할 수 있다. 또한, 프로세서는 결정된 사물까지의 거리 및 식별된 사물의 종류에 기초하여, 사물의 위치 및 종류를 나타내는 지도를 생성할 수 있다. 프로세서 는 생성된 지도에 기초하여 주행 모듈을 제어함으로써 주행할 수 있다. 프로세서는 움직임 센서(미도시)의 센서값에 기초하여, 2D 이미지 내에서 기준점의 위치를 보정할 수 있 다. 프로세서는 2D 이미지를 인공 지능 모델의 입력 데이터로써 입력하고, 인공 지능 모델을 실행함으로 써 2D 이미지 내의 픽셀들에 대응하는 상대적 깊이들을 산출할 수 있다. 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, ‘비 일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다는 것을 의미 할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구분하 지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두개의 사용자 장치들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운 로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다."}
{"patent_id": "10-2022-0054435", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른, 로봇이 사물까지의 거리를 결정하는 방법을 도시한다. 도 2는 본 개시의 일 실시예에 따른, 로봇이 인공 지능 모델을 이용하여 2D 이미지 내의 픽셀들에 대한 상대적 깊이를 결정하는 방법을 도시한다. 도 3는 본 개시의 일 실시예에 따른, 로봇이 인공 지능 모델을 이용하여 2D 이미지 내의 사물들의 상대적 깊이 및 사물들을 인식하는 방법을 도시한다. 도 4는 본 개시의 일 실시예에 따른, 로봇이 기준 거리에 기초하여 주변 사물까지의 거리를 결정하는 방법을 도 시한다. 도 5는 본 개시의 일 실시예에 따른, 로봇의 블록도를 도시한다. 도 6은 본 개시의 일 실시예에 따른, 로봇이 사물까지의 거리를 결정하는 방법의 흐름도이다. 도 7은 본 개시의 다른 실시예에 따른, 로봇의 블록도를 도시한다. 도 8은 본 개시의 일 실시예에 따른, 로봇의 움직임에 기초하여 기준점을 보정하는 방법의 흐름도를 도시한다. 도 9는 본 개시의 일 실시예에 따른, 로봇이 복수의 1D 거리 센서를 이용하여 사물까지의 거리를 결정하는 방법 을 도시한다. 도 10은 본 개시의 일 실시예에 따른, 로봇이 지도를 생성하는 방법을 도시한다. 도 11은 본 개시의 다른 실시예에 따른, 로봇의 블록도를 도시한다."}
