{"patent_id": "10-2024-0021374", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0030382", "출원번호": "10-2024-0021374", "발명의 명칭": "클럭 분할을 이용한 NPU 피크 파워 제어 기술", "출원인": "주식회사 딥엑스", "발명자": "김녹원"}}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트(PE)를 포함하는 복수의 PE 그룹들을 위해 배치된, 제1 회로;클럭 소스로부터의 원 클럭 신호를 기반으로, 위상이 서로 다른 복수의 클럭 신호를 분할 생성하여 상기 복수의PE 그룹들에게 제공하는 클럭 분배기를 위하여 배치된, 제2 회로를 포함하고,상기 복수의 클럭 신호들 중 제1 클럭 신호는 상기 복수의 PE 그룹들 중에서 제1 PE 그룹에게 제공되고, 상기복수의 클럭 신호들 중 제2 클럭 신호는 상기 복수의 PE 그룹들 중에서 제2 PE 그룹에게 제공되는, 신경 프로세싱 유닛."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 복수의 클럭 신호들 중 적어도 하나는 피크 파워 저감을 고려하여 배치된 적어도 하나의 출력 버퍼를 지나서 상기 복수의 PE그룹들 중 하나에 공급되도록 구성된, 신경 프로세싱 유닛."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 복수의 클럭 신호들은 상기 원 클럭 신호 대비 주파수는 더 낮은, 신경 프로세싱 유닛."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 복수의 클럭 신호들의 주파수는, 상기 복수의 PE 그룹의 개수에 기초하여 결정되는, 신경 프로세싱 유닛."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 복수의 클럭 신호들의 주파수는, 상기 원 클럭 신호 대비 상기 복수의 PE 그룹의 개수 배수 만큼 낮은, 신경 프로세싱 유닛."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,공개특허 10-2025-0030382-3-상기 제2 클럭 신호는 상기 제1 클럭 신호에 비하여 위상이 더 늦은, 신경 프로세싱 유닛."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서, 상기 클럭 분배기를 위하여 배치된 제2 회로는:상기 원 클럭 신호를 지연시키기 위한 복수의 플리-플롭과;상기 복수의 플리-플롭과 병렬로 연결된 다중화기와; 그리고 상기 다중화기로부터의 출력 신호의 주파수를 분할하는 분할기를 포함하는, 신경 프로세싱 유닛."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항에 있어서, 상기 클럭 분배기를 위하여 배치된 제2 회로는:복수의 지연 셀과;상기 복수의 지연셀과 병렬로 연결된 다중화기를 포함하는, 신경 프로세싱 유닛"}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 항에 있어서, 상기 클럭 분배기를 위하여 배치된 제2 회로는: 원 클럭 신호와 연결되어 원 클럭 신호의 주파수를 분할하는 분할기와; 그리고상기 분할기의 출력단과 연결된 제1 D형 플리-플롭을 포함하고;상기 분할기 및 상기 제1 D형 플리-플롭의 출력단 각각은 상기 원 클럭 소스의 클럭 주파수 가 연결되는, 신경 프로세싱 유닛."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "반도체(semi-conductor) 기판;ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트(PE)를 포함하는 복수의 신경 프로세싱 유닛(NPU)들을 위하여, 상기 반도체 기판 상에 배치된, 제1 회로;클럭 소스로부터의 원 클럭 신호를 기반으로, 위상이 서로 다른 복수의 클럭 신호를 분할 생성하여 상기 복수의NPU들에게 제공하는 클럭 분배기를 위하여, 상기 반도체 기판 상에 배치된, 제2 회로를 포함하고,상기 복수의 클럭 신호들 중 제1 클럭 신호는 상기 복수의 NPU들 중에서 제1 NPU 에게 제공되고, 상기 복수의클럭 신호들 중 제2 클럭 신호는 상기 복수의 NPU들 중에서 제2 NPU에게 제공되는, 시스템-온-칩.공개특허 10-2025-0030382-4-청구항 11 제10 항에 있어서,상기 복수의 클럭 신호들 중 적어도 하나는 피크 파워 저감을 고려하여 배치된 적어도 하나의 출력 버퍼를 지나서 상기 복수의 NPU 중 하나에 공급되도록 구성된, 시스템-온-칩."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10 항에 있어서,상기 복수의 클럭 신호들은 상기 원 클럭 신호 대비 주파수는 더 낮은, 시스템-온-칩."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10 항에 있어서,상기 복수의 클럭 신호들의 주파수는, 상기 복수의 NPU들의 개수에 기초하여 결정되는, 시스템-온-칩."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제10 항에 있어서,상기 복수의 클럭 신호들의 주파수는, 상기 원 클럭 신호 대비 상기 복수의 NPU들의 개수 배수 만큼 낮은, 시스템-온-칩."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제10 항에 있어서,상기 제2 클럭 신호는 상기 제1 클럭 신호에 비하여 위상이 더 늦은, 시스템-온-칩."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "인쇄 회로 기판;ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트(PE)를 포함하는 복수의 신경 프로세싱 유닛(NPU)들을 위하여, 상기 인쇄 회로 기판 상에 배치된, 제1 회로;클럭 소스로부터의 원 클럭 신호를 기반으로, 위상이 서로 다른 복수의 클럭 신호를 분할 생성하여 상기 복수의NPU들에게 제공하는 클럭 분배기를 위하여, 상기 인쇄 회로 기판 상에 배치된, 제2 회로를 포함하고,상기 복수의 클럭 신호들 중 제1 클럭 신호는 상기 복수의 NPU들 중에서 제1 NPU 에게 제공되고, 상기 복수의클럭 신호들 중 제2 클럭 신호는 상기 복수의 NPU들 중에서 제2 NPU에게 제공되는, 전자 장치.공개특허 10-2025-0030382-5-청구항 17 제16 항에 있어서,상기 복수의 클럭 신호들 중 적어도 하나는 피크 파워 저감을 고려하여 배치된 적어도 하나의 출력 버퍼를 지나서 상기 복수의 NPU 중 하나에 공급되도록 구성된, 전자 장치."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16 항에 있어서,상기 복수의 클럭 신호들은 상기 원 클럭 신호 대비 주파수는 더 낮은, 전자 장치."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제16 항에 있어서,상기 복수의 클럭 신호들의 주파수는, 상기 원 클럭 신호 대비 상기 복수의 NPU들의 개수 배수 만큼 낮은, 전자 장치."}
{"patent_id": "10-2024-0021374", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제16 항에 있어서,상기 제2 클럭 신호는 상기 제1 클럭 신호에 비하여 위상이 더 늦은, 전자 장치."}
{"patent_id": "10-2024-0021374", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 예시는 신경 프로세싱 유닛을 제공한다. 상기 신경 프로세싱 유닛은 ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 PE 그룹들을 위해 배치된, 제1 회로와; 클럭 소스로부터의 원 클럭 신호를 기반으로, 위상이 서로 다른 복수의 클럭 신호를 분할 생성하여 상기 복수의 PE 그룹들에게 제공하는 클럭 분배기를 위하여 배치된, 제2 회로를 포 함할 수 있다. 상기 복수의 클럭 신호들 중 제1 클럭 신호는 상기 복수의 PE 그룹들 중에서 제1 PE 그룹에게 제 공되고, 상기 복수의 클럭 신호들 중 제2 클럭 신호는 상기 복수의 PE 그룹들 중에서 제2 PE 그룹에게 제공될 수 있다."}
{"patent_id": "10-2024-0021374", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 신경 프로세싱 유닛의 피크 파워를 분산하는 기술에 관한 한 것이다."}
{"patent_id": "10-2024-0021374", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(artificial intelligence: AI)도 점차 발전하고 있다. AI는 인간의 지능, 즉 인식(Recognition), 분 류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정(Control/Decision making) 등을 할 수 있는 지능을 인공적으로 모방하는 것을 의미한다. 또한, 최근에는 인공지능(AI)을 위한 연산 속도를 가속하기 위하여, 신경 프로세싱 유닛 (Neural processing unit; NPU)가 개발되고 있다. 인공지능 서비스의 목적에 따라 여러 타입의 인공신경망(ANN) 모델이 존재할 수 있다. 예를 들어, 입력되는 데 이터가 이미지 또는 영상인 경우, 인공지능 서비스를 위하여 이미지/영상 내의 객체 분류, 객체 검출, 객체 추 적 등을 위한 CNN 타입의 인공신경망모델이 사용될 수 있다. 일반적으로, 인공신경망모델은 레이어 마다 연산량이 다를 수 있다. 특히, 특정 레이어에서 연산량이 크게 증가하면, 전력 소모량이 순간적으로 증가할 수 있다."}
{"patent_id": "10-2024-0021374", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2024-0021374", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "인공신경망 연산은 데이터 인텐시브(intensive) 한 특성을 가진다. 특히, 인공신경망 연산은 병렬 처리 연산이 필요하다. 즉, 인공신경망 연산은 동시에 방대한 데이터를 빠른 속도로 병렬로 처리하지 못하면 처리 속도가 저 하되는 특성이 있다. 이에, 본 개시의 발명자들은 인공신경망 연산에 특화된 신경 프로세싱 유닛을 개발하였다. 본 개시의 발명자들 은 신경 프로세싱 유닛의 복수의 프로세싱 엘리먼트의 개수를 증가시켜서 신경 프로세싱 유닛의 병렬 처리 성능 을 향상시키고자 하였다. 또한 본 개시의 발명자들은 저전력 동작이 가능한 신경 프로세싱 유닛을 개발하고자 하였다. 한편, 엣지 디바이스에 설치되는 저전력 동작에 특화된 신경 프로세싱 유닛의 전원 공급부는 서버나 개인용 컴 퓨터(PC)에서 동작하는 그래픽 처리 장치(GPU)의 전원 공급부에 비해서 상대적으로 전원 공급 능력이 낮을 수 있다. 또한, 엣지 디바이스에 설치되는 저전력 동작에 특화된 신경 프로세싱 유닛의 전원 공급 부의 커패시턴스 의 용량이 순간적인 전원 공급을 감당하기에 부족할 수 있다. 하지만, 저전력 동작에 특화된 신경 프로세싱 유닛의 프로세싱 엘리먼트의 개수가 증가할수록, 순간적으로 공급 전압이 흔들리는 정도가 증가할 수 있다는 사실을 본 개시의 발명자들은 인식하였다. 부연 설명하면, 요구되는 신경 프로세싱 유닛의 순간 전원 공급량은 동작하는 프로세싱 엘리먼트의 개수에 비례할 수 있다. 또한, 인공신경망모델의 연산량은 각 레이어별로 편차가 상당하다. 따라서 인공신경망모델의 레이어의 연산량에 따라서 병렬로 동작하는 프로세싱 엘리먼트의 개수는 상이할 수 있다. 즉, 동시에 많은 프로세싱 엘리먼트들이 동작할 경우, 순간적으로 신경 프로세싱 유닛의 전원 공급부의 전압이 흔들리거나 또는 강하될 수 있다. 또한, 인공신경망모델의 특정 레이어의 연산량은 매우 작을 수 있다. 이러한 경우 신경 프로세싱 유닛의 구동 주파수를 증가시키더라도, 신경 프로세싱 유닛의 공급 전압 안정성이 보장될 수 있다는 것을 본 개시의 발명자 들이 인식하게 되었다. 또한, 공급 전압이 순간적으로 흔들리거나 또는 강하될 경우, 시스템 안전성을 위해서 공급 전압(VDD)을 올려야 할 경우가 발생될 수도 있다. 따라서 시스템 안전성을 확보하지 못하면 불필요하게 공급 전압이 상승되는 문제 가 발생될 수 있다. 공급 전압이 상승될 경우, 신경 프로세싱 유닛의 소비 전력이 급격하게 상승될 수 있는 문 제가 발생될 수 있다. 또한 소비 전력이 증가될 경우, 신경 프로세싱 유닛의 발열이 증가하는 문제가 발생될 수 있다. 이러한 경우, 신경 프로세싱 유닛의 구동 주파수를 저감 시켜 신경 프로세시 유닛의 공급 전원부의 전압 을 안정시킬 수 있다는 사실을 본 개시의 발명자들이 인식하였다. 이와 반대로, 공급 전압이 안정될수록, 공급 전압(VDD)을 저감 시킬 수 있다. 따라서 신경 프로세싱 유닛의 공 급 전압의 안전성을 확보하면 공급 전압을 저감 시킬 수 있으며, 결과적으로 신경 프로세싱 유닛의 소비 전력을 저감할 수 있다. 이에, 본 개시의 발명자들은 프로세싱 엘리먼트의 개수가 증가하여 피크파워가 증가하더라도 인공신경망 연산에 특화된 신경 프로세싱 유닛의 공급 전압의 안정성을 개선시키는 것이 필요하다고 인식하였다. 따라서, 본 개시의 개시들은 특정 연산 단계에서 과도한 파워에 기인한 공급 전원 부의 공급 전압의 떨림을 안 정화시키는 기술적 방안들을 제시하는 것을 목적으로 한다."}
{"patent_id": "10-2024-0021374", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "전술한 목적을 달성하기 위하여, 본 개시의 일 예시는 신경 프로세싱 유닛을 제공한다. 상기 신경 프로세싱 유 닛은 ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리 먼트 (PE)를 포함하는 복수의 PE 그룹들을 위해 배치된, 제1 회로와; 클럭 소스로부터의 원 클럭 신호를 기반으 로, 위상이 서로 다른 복수의 클럭 신호를 분할 생성하여 상기 복수의 PE 그룹들에게 제공하는 클럭 분배기를 위하여 배치된, 제2 회로를 포함할 수 있다. 상기 복수의 클럭 신호들 중 제1 클럭 신호는 상기 복수의 PE 그 룹들 중에서 제1 PE 그룹에게 제공되고, 상기 복수의 클럭 신호들 중 제2 클럭 신호는 상기 복수의 PE 그룹들 중에서 제2 PE 그룹에게 제공될 수 있다. 본 개시의 일 예시는 시스템-온-칩을 제공한다. 상기 시스템-온 칩은 반도체(semi-conductor) 기판; ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트(PE)를 포함하는 복수의 신경 프로세싱 유닛(NPU)들을 위하여, 상기 반도체 기판 상에 배치된, 제1 회로; 그리 고 클럭 소스로부터의 원 클럭 신호를 기반으로, 위상이 서로 다른 복수의 클럭 신호를 분할 생성하여 상기 복 수의 NPU들에게 제공하는 클럭 분배기를 위하여, 상기 반도체 기판 상에 배치된, 제2 회로를 포함할 수 있다. 상기 복수의 클럭 신호들 중 제1 클럭 신호는 상기 복수의 NPU들 중에서 제1 NPU 에게 제공되고, 상기 복수의 클럭 신호들 중 제2 클럭 신호는 상기 복수의 NPU들 중에서 제2 NPU에게 제공될 수 있다. 본 개시의 일 예시는 전자 장치를 제공한다. 상기 전자 장치는: 인쇄 회로 기판; ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 신경 프로세싱 유닛(NPU)들을 위하여, 상기 인쇄 회로 기판 상에 배치된, 제1 회로; 그리고 클럭 소스로부터의 원 클럭 신호를 기반으로, 위상이 서로 다른 복수의 클럭 신호를 분할 생성하여 상기 복수의 NPU들에게 제공하 는 클럭 분배기를 위하여, 상기 인쇄 회로 기판 상에 배치된, 제2 회로를 포함할 수 있다. 상기 복수의 클럭 신 호들 중 제1 클럭 신호는 상기 복수의 NPU들 중에서 제1 NPU 에게 제공되고, 상기 복수의 클럭 신호들 중 제2 클럭 신호는 상기 복수의 NPU들 중에서 제2 NPU에게 제공될 수 있다."}
{"patent_id": "10-2024-0021374", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 예시들에 따르면, 인공신경망 연산이 복수의 클럭 신호에 따라 분산 동작되게 되어, 특정 연산 단계 의 피크 파워를 낮출 수 있는 효과가 있다. 본 개시의 예시들에 따르면, 인공신경망 연산이 복수의 클럭 신호에 따라 분산 동작되게 되어, 신경 프로세싱 유닛에 공급되는 공급 전압의 안전성을 개선할 수 있는 효과가 있다. 본 개시의 예시들에 따르면, 인공신경망 연산량이 동일하더라도, 순간적인 피크 파워가 감소할 수 있기 때문에, 동일한 연산량 기준 상대적으로 더 낮은 피크 파워를 제공할 수 있는 효과가 있다."}
{"patent_id": "10-2024-0021374", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시의 예시들의 특정한 구조적 내지 단계적 설명들은 단지 본 개시의 개념에 따른 예시를 설명하기 위한 것 이다. 따라서 본 개시의 개념에 따른 예시들은 다양한 형태로 실시될 수 있다. 본 개시의 개념에 따른 예시들은 다양한 형태로 실시될 수 있다. 본 개시는 본 개시의 예시들에 한정되는 것으로 해석되어서는 아니 된다. 본 개시의 개념에 따른 예시에 다양한 변경을 가할 수 있고 여러 가지 형태를 가질 수 있다. 이에, 특정 예시들 을 도면에 예시하고 본 개시 또는 출원에 대해서 상세하게 설명하고자 한다. 그러나, 이는 본 개시의 개념에 따 른 예시를 특정한 개시 형태에 대해 한정하려는 것이 아니다. 본 개시의 개념에 따른 여시는 본 개시의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1 및/또는 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만 사용될 수 있다. 상기 용어들은 본 개시의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제1 구성요소 는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있다. 하지만 복수의 구성요소들 중간에 다른 구성요소가 존재할 수도 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거 나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃하는\"과 \"~ 에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 본 개시에서 사용한 용어는 단지 특정한 예시를 설명하기 위해 사용된 것으로, 본 개시를 한정하려는 의도가 아 니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"포함하 다\" 또는 \"가지다\" 등의 용어는 서술된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존 재함을 지정하려는 것이다. 따라서 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미 를 가지는 것으로 해석되어야 한다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 본 개시에 서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 예시를 설명함에 있어서 본 개시가 속하는 기술 분야에 익히 알려져 있고 본 개시와 직접적으로 관련이 없는 기 술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 개시의 요지를 흐리지 않고 더욱 명확히 전달하기 위함이다. <용어의 정의> 이하, 본 개시에서 제시되는 개시들의 이해를 돕고자, 본 개시에서 사용되는 용어들에 대하여 간략하게 정리하 기로 한다. NPU: 신경 프로세싱 유닛(Neural Processing Unit)의 약어로서, CPU(Central processing unit)과 별개로 인공 신경망모델의 연산을 위해 특화된 프로세서를 의미할 수 있다. ANN: 인공신경망(artificial neural network)의 약어로서, 인간의 지능을 모방하기 위하여, 인간 뇌 속의 뉴런 들(Neurons)이 시냅스(Synapse)를 통하여 연결되는 것을 모방하여, 노드들을 레이어(Layer: 계층) 구조로 연결 시킨, 네트워크를 의미할 수 있다. DNN: 심층 신경망(Deep Neural Network)의 약어로서, 보다 높은 인공 지능을 구현하기 위하여, 인공신경망의 은 닉 레이어의 개수를 늘린 것을 의미할 수 있다. CNN: 컨볼루션 신경망(Convolutional Neural Network)의 약어로서, 인간 뇌의 시각 피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망이다. 컨볼루션 신경망은 영상처리에 적합한 것으로 알려져 있으며, 입력 데이 터의 특징들을 추출하고, 특징들의 패턴을 파악하기에 용이한 것으로 알려져 있다. 이하, 첨부한 도면을 참조하여 본 개시의 바람직한 예시를 설명함으로써, 본 개시를 상세히 설명한다. 이하, 본 개시의 예시를 첨부된 도면을 참조하여 상세하게 설명한다. <인공지능> 인간은 인식(Recognition), 분류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정 (Control/Decision making) 등을 할 수 있는 지능을 갖추고 있다. 인공지능(artificial intelligence: AI)은 인간의 지능을 인공적으로 모방하는 것을 의미한다. 인간의 뇌는 뉴런(Neuron)이라는 수많은 신경세포로 이루어져 있다. 각각의 뉴런은 시냅스(Synapse)라고 불리는 연결부위를 통해 수백에서 수천 개의 다른 뉴런들과 연결되어 있다. 인간의 지능을 모방하기 위하여, 생물학적 뉴런의 동작원리와 뉴런 간의 연결 관계를 모델링한 것을, 인공신경망모델이라고 한다. 즉, 인공신경망은 뉴런 들을 모방한 노드들을 레이어(Layer: 계층) 구조로 연결시킨, 시스템이다. 이러한 인공신경망모델은 레이어 수에 따라 '단층 신경망'과 '다층 신경망'으로 구분한다. 일반적인 다층신경망 은 입력 레이어와 은닉 레이어, 출력 레이어로 구성된다. 입력 레이어(input layer)은 외부의 자료들을 받 아들이는 레이어로서, 입력 레이어의 뉴런 수는 입력되는 변수의 수와 동일하다. 은닉 레이어(hidden layer)은 입력 레이어와 출력 레이어 사이에 위치하며 입력 레이어로부터 신호를 받아 특성을 추출하여 출력층 으로 전달한다. 출력 레이어(output layer)은 은닉 레이어로부터 신호를 받아 외부로 출력한다. 뉴런 간의 입력신호는 0에서 1 사이의 값을 갖는 각각의 연결강도와 곱해진 후 합산된다. 이 합이 뉴런의 임계치보다 크면 뉴런이 활성화되어 활성화 함수를 통하여 출력 값으로 구현된다. 한편, 보다 높은 인공 지능을 구현하기 위하여, 인공신경망의 은닉 레이어의 개수를 늘린 것을 심층 신경망 (Deep Neural Network, DNN)이라고 한다. DNN은 다양한 구조로 개발되고 있다. 예를 들면, DNN의 일 예시인 합성곱 신경망(convolutional neural network, CNN)은 입력 값 (영상 또는 이미지)의 특징들을 추출하고, 추출된 출력 값의 패턴을 파악하기에 용이 한 것으로 알려져 있다. CNN은 합성곱 연산, 활성화 함수 연산, 풀링(pooling) 연산 등이 특정 순서로 처리되는 형태로 구성될 수 있다. 예를 들면, DNN의 레이어 각각에서, 파라미터(i.e., 입력 값, 출력 값, 가중치 또는 커널 등)는 복수의 채널로 구성된 행렬일 수 있다. 파라미터는 합성곱 또는 행렬 곱셈으로 NPU에서 처리될 수 있다. 각 레이어에서 연산이 처리된 출력 값이 생성된다. 예를 들면, 트랜스포머(transformer)는 어텐션(attention) 기술에 기반한 DNN이다. 트랜스포머는 행렬 곱셈 (matrix multiplication) 연산을 다수 활용한다. 트랜스포머는 입력 값과 쿼리(query; Q), 키(key; K), 및 값 (value; V) 등의 파라미터를 사용하여 출력 값인 어텐션(Q,K,V)를 획득할 수 있다. 트랜스포머는 출력 값 (즉, 어텐션(Q,K,V))에 기초하여 다양한 추론 연산을 처리할 수 있다. 트랜스포머는 CNN 보다 더 우수한 추론 성능을 보여주는 경향이 있다. 도 1은 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 이하 신경 프로세싱 유닛에서 작동될 수 있는 예시적인 인공신경망모델(110a)의 연산에 대하여 설명한다. 도 1의 예시적인 인공신경망모델(110a)은 객체 인식, 음성 인식 등 다양한 추론 기능을 수행하도록 학습된 인공 신경망일 수 있다. 인공신경망모델(110a)은 심층 신경망(DNN, Deep Neural Network)일 수 있다. 단, 본 개시의 예시들에 따른 인공신경망모델(110a)은 심층 신경망에 제한되지 않는다. 예를 들어, 인공신경망모델(110a)은 LLM, Generative Adversarial Networks (GAN), Florence, DaViT, MobileViT, Swin-Transformer, Transformer, YOLO, CNN, PIDNet, BiseNet, RCNN, VGG, VGG16, DenseNet, SegNet, DeconvNet, DeepLAB V3+, U-net, SqueezeNet, Alexnet, ResNet18, MobileNet-v2, GoogLeNet, Resnet- v2, Resnet50, Resnet101, Inception-v3 등의 모델로 구현될 수 있다. 단, 본 개시는 상술한 모델들에 제한되 지 않는다. 또한 인공신경망모델(110a)은 적어도 두 개의 서로 다른 모델들에 기초한 앙상블 모델일 수도 있다. 이하 예시적인 인공신경망모델(110a)에 의해서 수행되는 추론 과정에 대해서 설명하기로 한다. 인공신경망모델(110a)은 입력 레이어(110a-1), 제1 연결망(110a-2), 제1 은닉 레이어(110a-3), 제2 연결망 (110a-4), 제2 은닉 레이어(110a-5), 제3 연결망(110a-6), 및 출력 레이어(110a-7)을 포함하는 예시적인 심층 신경망 모델이다. 단, 본 개시는 도 1에 도시된 인공신경망모델에만 제한되는 것은 아니다. 제1 은닉 레이어 (110a-3) 및 제2 은닉 레이어(110a-5)는 복수의 은닉 레이어로 지칭되는 것도 가능하다. 입력 레이어(110a-1)는 예시적으로, x1 및 x2 입력 노드를 포함할 수 있다. 즉, 입력 레이어(110a-1)는 2개의 입력 값에 대한 정보를 포함할 수 있다. 제1 연결망(110a-2)은 예시적으로, 입력 레이어(110a-1)의 각각의 노드를 제1 은닉 레이어(110a-3)의 각각의 노 드로 연결시키기 위한 6개의 가중치 값에 대한 정보를 포함할 수 있다. 각각의 가중치 값은 입력 노드 값과 곱 해지고, 곱해진 값들의 누산된 값이 제1 은닉 레이어(110a-3)에 저장된다. 가중치 값과 입력 노드 값은 인공신 경망모델의 파라미터로 지칭될 수 있다. 제1 은닉 레이어(110a-3)는 예시적으로 a1, a2, 및 a3 노드를 포함할 수 있다. 즉, 제1 은닉 레이어(110a-3)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 도 1의 제1 프로세싱 엘리먼트(PE1)는 a1 노드의 연산을 처리할 수 있다. 도 1의 제2 프로세싱 엘리먼트(PE2)는 a2 노드의 연산을 처리할 수 있다. 도 1의 제3 프로세싱 엘리먼트(PE3)는 a3 노드의 연산을 처리할 수 있다. 제2 연결망(110a-4)은 예시적으로, 제 1 은닉 레이어(110a-3)의 각각의 노드를 제2 은닉 레이어(110a-5)의 각각의 노드로 연결시키기 위한 9개의 가중 치 값에 대한 정보를 포함할 수 있다. 제2 연결망(110a-4)의 가중치 값은 제1 은닉 레이어(110a-3)로부터 입력되는 노드 값과 각각 곱해지고, 곱해진 값들의 누산된 값이 제2 은닉 레이어(110a-5)에 저장된다. 제2 은닉 레이어(110a-5)는 예시적으로 b1, b2, 및 b3 노드를 포함할 수 있다. 즉, 제2 은닉 레이어(110a-5)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 도 1의 제4 프로세싱 엘리먼트(PE4)는 b1 노드의 연산을 처리할 수 있다. 도 1의 제5 프로세싱 엘리먼트(PE5)는 b2 노드의 연산을 처리할 수 있다. 도 1의 제6 프로세싱 엘리먼트(PE6)는 b3 노드의 연산을 처리할 수 있다. 제3 연결망(110a-6)은 예시적으로, 제2 은닉 레이어(110a-5)의 각각의 노드와 출력 레이어(110a-7)의 각각의 노 드를 연결하는 6개의 가중치 값에 대한 정보를 포함할 수 있다. 제3 연결망(110a-6)의 가중치 값은 제2 은닉 레 이어(110a-5)로부터 입력되는 노드 값과 각각 곱해지고, 곱해진 값들의 누산된 값이 출력 레이어(110a-7)에 저 장된다. 출력 레이어(110a-7)는 예시적으로 y1, 및 y2 노드를 포함할 수 있다. 즉, 출력 레이어(110a-7)는 2개의 노드 값에 대한 정보를 포함할 수 있다. 도 1의 제7 프로세싱 엘리먼트(PE7)는 y1 노드의 연산을 처리할 수 있다. 도 1의 제8 프로세싱 엘리먼트(PE8)는 y2 노드의 연산을 처리할 수 있다. 각각의 노드는 특징 값에 대응될 수 있으며, 특징 값은 특징맵에 대응될 수 있다. 도 2a은 컨볼루션 신경망(CNN)의 기본 구조를 설명하기 위한 도면이다. 도 2a을 참조하면, 입력 이미지는 특정 사이즈(size)의 행과 특정 사이즈의 열로 구성된 2차원적 행렬로 표시될 수 있다. 입력 이미지는 복수의 채널을 가질 수 있는데, 여기서 채널은 입력 데이터 이미지의 컬러 성분의 수를 나타낼 수 있다. 컨볼루션 과정은 입력 이미지를 지정된 간격으로 순회하면서 커널과 합성곱 연산을 수행하는 것을 의미한다. 컨볼루션 신경망은 현재 레이어의 출력 값(합성곱 또는 행렬 곱셈)을 다음 레이어의 입력 값으로 전달하는 구조 를 가질 수 있다. 예를 들면, 합성곱(컨볼루션)은, 두 개의 주요 파라미터(입력 특징맵 및 커널)에 의해 정의된다. 파라미터는 입 력 특징맵, 출력 특징맵, 활성화 맵, 가중치, 커널, 및 어텐션(Q,K,V) 등을 포함할 수 있다, 합성곱(컨볼루션)은 입력 특징맵 위로 커널 윈도우를 슬라이딩 한다. 커널이 입력 특징맵을 슬라이딩 하는 단차 사이즈를 보폭(stride)이라고 한다. 합성곱 이후에는 풀링(pooling)이 적용될 수 있다. 또한, 컨볼루션 신경망의 끝단에는 FC (fully-connected)레 이어가 배치될 수 있다. 도 2b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 2b을 참조하면, 예시적으로 입력 이미지가 6 x 6 크기를 갖는 2차원적 행렬인 것으로 나타나 있다. 또한, 도 2b에는 예시적으로 3개의 노드, 즉 채널 1, 채널 2, 채널 3이 사용되는 것으로 나타내었다. 먼저, 합성곱 동작에 대해서 설명하기로 한다. 입력 이미지(도 2b에서는 예시적으로 6 x 6 크기인 것으로 나타내어짐)는 첫 번째 노드에서 채널 1을 위한 커널 1(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)과 합성곱되고, 그 결과로서 특징맵1(도 2b에서는 예시적으로 4 x 4 크기인 것으로 나타내어짐)이 출력된다. 또한, 상기 입력 이미지(도 2b에서는 예시적으로 6 x 6 크기인 것으로 나타내어짐)는 두 번째 노드에서 채널 2를 위한 커널 2(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)와 합성곱되고 그 결과로서 특징맵 2(도 2b에서는 예시적으로 4 x 4 크기인 것으로 나타내어 짐)가 출력된다. 또한, 상기 입력 이미지는 세 번째 노드에서 채널 3을 위한 커널 3(도 2b에서는 예시적으로 3 x 3 크기인 것으로 나타내어짐)과 합성곱되고, 그 결과로서 특징맵3(도 2b에서는 예시적으로 4 x 4 크기인 것으로 나타내어짐)이 출력된다. 각각의 합성곱을 처리하기 위해서 신경 프로세싱 유닛의 프로세싱 엘리먼트들(PE1 to PE12)은 MAC 연산을 수행하도록 구성된다. 다음으로, 활성화 함수의 동작에 대해서 설명하기로 한다. 합성곱 동작으로부터 출력되는 특징맵1, 특징맵2 그리고 특징맵3(도 2b에서는 각각의 크기는 예시적으로 4 x 4 인 것으로 나타내어짐)에 대해서 활성화 함수가 적용될 수 있다. 활성화 함수가 적용되고 난 이후의 출력은 예 시적으로 4 x 4의 크기일 수 있다. 다음으로, 폴링(pooling) 동작에 대해서 설명하기로 한다. 상기 활성화 함수로부터 출력되는 특징맵1, 특징맵2, 특징맵3(도 2b에서는 각각의 크기는 예시적으로 4 x 4인 것으로 나타내어짐)은 3개의 노드로 입력된다. 활성화 함수로부터 출력되는 특징맵들을 입력으로 받아서 폴링 (pooling)을 수행할 수 있다. 상기 폴링이라 함은 크기를 줄이거나 행렬 내의 특정 값을 강조할 수 있다. 폴링 방식으로는 최대값 폴링과 평균 폴링, 최소값 폴링이 있다. 최대값 폴링은 행렬의 특정 영역 안에 값의 최댓값 을 모으기 위해서 사용되고, 평균 폴링은 특정 영역내의 평균을 구하기 위해서 사용될 수 있다. 도 2b의 예시에서는 4 x 4 크기의 특징맵이 폴링에 의하여 2 x 2 크기로 줄어지는 것으로 나타내었다. 구체적으로, 첫 번째 노드는 채널 1을 위한 특징맵1을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출력한다. 두 번째 노드는 채널 2을 위한 특징맵2을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출 력한다. 세 번째 노드는 채널 3을 위한 특징맵3을 입력으로 받아 폴링을 수행한 후, 예컨대 2 x 2 행렬로 출력 한다. 전술한 합성곱, 활성화 함수과 폴링이 반복되고 최종적으로는, 도 8a과 같이 fully connected로 출력될 수 있다. 해당 출력은 다시 이미지 인식을 위한 인공신경망으로 입력될 수 있다. 단, 본 개시는 특징맵, 커널의 크 기에 제한되지 않는다. 지금까지 설명한 CNN은 다양한 심층신경망(DNN) 방법 중에서도 컴퓨터 비전(Vision) 분야에서 가장 많이 쓰이는 방법이다. 특히, CNN은 이미지 분류(image classification) 및 객체 검출(objection detection)과 같은 다양한 작업을 수행하는 다양한 연구 영역에서 놀라운 성능을 보였다. <ANN의 연산을 위해 필요한 하드웨어 자원> 도 3은 본 개시의 일 예시에 따른 신경 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 3에 도시된 신경 프로세싱 유닛(neural processing unit, NPU)은 인공신경망을 위한 동작을 수행하도록 특화된 프로세서이다. 인공신경망은 여러 입력 또는 자극이 들어오면 각각 가중치를 곱해 더해주고, 추가적으로 편차를 더한 값을 활 성화 함수를 통해 변형하여 전달하는 인공 뉴런들이 모인 네트워크를 의미한다. 이렇게 학습된 인공신경망은 입 력 데이터로부터 추론(inference) 결과를 출력하는데 사용될 수 있다. 신경 프로세싱 유닛는 전기/전자 회로로 구현된 반도체일 수 있다. 전기/전자 회로라 함은 수많은 전자 소자, (예컨대 트렌지스터, 커패시터)를 포함하는 것을 의미할 수 있다. Transformer 및/또는 CNN 기반의 인공신경망모델인 경우, 신경 프로세싱 유닛는 행렬 곱셈 연산, 합성곱 연산, 등을 인공신경망의 구조(architecture)에 따라 선별하여, 처리할 수 있다. 예를 들어, 합성곱 신경망(CNN)의 레이어 각각에서, 입력 데이터에 해당하는 입력 특징맵(Input feature map)과 가중치(Weight)에 해당하는 커널(kernel)은 복수의 채널로 구성된 텐서(Tensor) 또는 행렬일 수 있다. 입력 특 징맵과 커널의 합성곱 연산이 수행되며, 각 채널에서 합성곱 연산과 풀링 출력 특징맵(output feature map)이 생성된다. 출력 특징맵에 활성화 함수를 적용하여 해당 채널의 활성화맵(activation map)이 생성된다. 이후, 활 성화맵에 대한 풀링이 적용될 수 있다. 여기서 포괄적으로 활성화맵은 출력 특징맵으로 지칭될 수 있다. 이하 설명의 편의를 위해 활성화맵은 출력 특징맵으로 지칭하여 설명한다. 단, 본 개시의 예시들은 이에 제한되지 않으며, 출력 특징맵은 행렬 곱셈 연산 또는 합성곱 연산등이 적용된 것 을 의미한다. 부연 설명하면, 본 개시의 예시들에 따른 출력 특징맵은 포괄적인 의미로 해석되어야 한다. 예를 들면, 출력 특 징맵은 행렬 곱셈 연산 또는 합성곱 연산 결과값일 수 있다. 이에, 복수의 프로세싱 엘리먼트는 추가 알고 리즘을 위한 처리 회로부를 더 포함하도록 변형 실시되는 것도 가능하다. 즉, 후술할 SFU의 일부 회로 유 닛들이 복수의 프로세싱 엘리먼트에 포함되도록 구성되는 것도 가능하다. 신경 프로세싱 유닛는 상술한 인공신경망 연산에 필요한 합성곱 및 행렬 곱셈을 처리하기 위한 복수의 프 로세싱 엘리먼트를 포함하도록 구성될 수 있다. 신경 프로세싱 유닛는 상술한 인공신경망 연산에 필요한 행렬 곱셈 연산, 합성곱 연산, 활성화 함수 연산, 풀링 연산, 스트라이드 연산, 배치 정규화 연산, 스킵 커넥션 연산, 접합 연산, 양자화 연산, 클리핑 연산, 패 딩 연산에 최적화된 각각의 처리 회로를 포함하도록 구성될 수 있다. 예를 들면, 신경 프로세싱 유닛는 상술한 알고리즘들 중 활성화 함수 연산, 풀링 연산, 스트라이드 연산, 배치 정규화 연산, 스킵 커넥션 연산, 접합 연산, 양자화 연산, 클리핑 연산, 패딩 연산 중 적어도 하나를 처리 하기 위한 SFU를 포함하도록 구성될 수 있다. 구체적으로, 신경 프로세싱 유닛는 복수의 프로세싱 엘리먼트(processing element: PE) , SFU, NPU 내부 메모리, NPU 컨트롤러, 및 NPU 인터페이스를 포함할 수 있다. 복수의 프로세싱 엘리먼 트, SFU, NPU 내부 메모리, NPU 컨트롤러, 및 NPU 인터페이스 각각은 수많은 트렌지 스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있 고, 동작에 의해서만 식별될 수 있다. 예컨대, 임의 회로는 복수의 프로세싱 엘리먼트으로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. NPU 컨트롤러는 신경 프로세싱 유닛의 인공신경망 추론 동작을 제어하도록 구성된 제어부 의 기능을 수행하도록 구성될 수 있다. 신경 프로세싱 유닛는 복수의 프로세싱 엘리먼트 및 SFU에서 추론될 수 있는 인공신경망모델의 파라미터를 저장하도록 구성된 NPU 내부 메모리, 및 복수의 프로세싱 엘리먼트, SFU, 및 NPU 내 부 메모리의 연산 스케줄을 제어하도록 구성된 스케줄러를 포함하는 NPU 컨트롤러를 포함할 수 있다. 신경 프로세싱 유닛는 SVC(scalable video coding) 또는 SFC(scalable feature-map coding)를 이용한 인 코딩 및 디코딩 방식에 대응되어 특징맵을 처리하도록 구성될 수 있다. 상기 방식들은 통신 채널 또는 통신 버 스의 실효 대역폭 및 신호대잡음비(signal to noise ratio; SNR)에 따라 가변적으로 데이터 전송량을 가변 하는 기술이다. 즉, 신경 프로세싱 유닛은 인코더 및 디코더를 더 포함하도록 구성되는 것도 가능하다. 복수의 프로세싱 엘리먼트는 인공신경망을 위한 동작의 일부를 수행할 수 있다. SFU는 인공신경망을 위한 동작의 다른 일부를 수행할 수 있다. 신경 프로세싱 유닛는 복수의 프로세싱 엘리먼트와 SFU를 사용하여 인공신경망모델의 연산을 하 드웨어적으로 가속하도록 구성될 수 있다. NPU 인터페이스는 시스템 버스를 통해서 신경 프로세싱 유닛와 연결된 다양한 구성요소들, 예컨대 메 모리와 통신할 수 있다. NPU 컨트롤러는 신경 프로세싱 유닛의 추론 연산을 위한 복수의 프로세싱 엘리먼트의 연산, SFU의 연산 및 NPU 내부 메모리의 읽기 및 쓰기 순서를 제어하도록 구성된 스케줄러를 포함할 수 있 다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 복수의 프로세싱 엘리먼트, SFU, 및 NPU 내부 메모리를 제어하도록 구성될 수 있다. NPU 컨트롤러 내의 스케줄러는 복수의 프로세싱 엘리먼트 및 SFU에서 작동할 인공신경망모델의 구조를 분석하거나 또는 이미 분석된 정보를 제공받을 수 있다. 분석된 정보는 컴파일러에 의해서 생성된 정보 일 수 있다. 예를 들면, 인공신경망모델이 포함할 수 있는 인공신경망의 데이터는 각각의 레이어의 노드 데이터 (즉, 특징맵), 레이어들의 배치 데이터, 지역성 정보 또는 구조에 대한 정보, 각각의 레이어의 노드를 연결하는 연결망 각각의 가중치 데이터 (즉, 가중치 커널) 중 적어도 일부를 포함할 수 있다. 인공신경망의 데이터는 NPU컨트롤러 내부에 제공되는 메모리 또는 NPU 내부 메모리에 저장될 수 있다. 단, 이에 제한되지 않으 며 인공신경망의 데이터는 NPU 또는 NPU를 포함하는 SoC에 구비된 별도의 캐시 메모리 또는 레지스터 파일에 저 장될 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 신경 프로세싱 유닛이 수행할 인공신경망모델의 연산 순서를 스케줄링 할 수 있다. NPU 컨트롤러 내의 스케줄러는 컴파일 된 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 신경 프로세싱 유닛이 수행할 인공신경망모델의 연산 순서의 스케줄링 정보를 제공받을 수 있다. 예를 들면, 상기 스케줄링 정보는 컴파일러에서 생성된 정보일 수 있다. 컴파일러에서 생성된 스케줄링 정보는 머신 코드(machine code) 또는 이진화 코드(binary code) 등으로 지칭될 수 있다. 즉 NPU 컨트롤러에서 활용되는 스케줄링 정보는 인공신경망모델의 데이터 지역성 정보 또는 구조에 기초하 여 컴파일러에 의해서 생성된 정보일 수 있다. 부연 설명하면, 컴파일러는 인공신경망모델이 가지는 고유한 특성인 인공신경망 데이터 지역성을 얼마나 잘 이 해하고 재구성하는지에 따라 NPU의 스케줄링을 효율적으로 할 수 있다. 부연 설명하면, 컴파일러는 신경 프로세싱 유닛의 하드웨어 구조와 성능을 얼마나 잘 이해하는지에 따라 NPU의 스케줄링을 효율적으로 할 수 있다. 부연 설명하면, 컴파일러에 의해서 인공신경망모델이 신경 프로세싱 유닛에서 실행되도록 컴파일 될 때, 인공신경망 데이터 지역성이 재구성될 수 있다. 인공신경망 데이터 지역성은 인공신경망모델에 적용된 알고리즘 들, 및 프로세서의 동작 특성에 따라서 재구성될 수 있다. 부연 설명하면, 인공신경망 데이터 지역성은 신경 프로세싱 유닛이 해당 인공신경망모델을 처리하는 방식, 예를 들면, 특징맵 타일링, 프로세싱 엘리먼트의 스테이셔너리(Stationary) 방식에 따라 재구성될 수 있다. 부연 설명하면, 인공신경망 데이터 지역성은 신경 프로세싱 유닛의 프로세싱 엘리먼트의 개수, 내부 메모 리의 용량에 따라서 재구성될 수 있다. 부연 설명하면, 인공신경망 데이터 지역성은 신경 프로세싱 유닛과 통신하는 메모리의 대역폭에 따라서 재 구성될 수 있다. 왜냐하면, 상술한 각 요인들에 의해서 동일한 인공신경망모델을 연산 처리하더라도 신경 프로세싱 유닛이 클럭 단위로 매 순간 필요한 데이터의 순서를 상이하게 결정할 수 있기 때문이다. 컴파일러는 인공신경망모델의 연산에 필요한 데이터의 순서는 인공신경망의 레이어, 단위 합성곱 및/또는 행렬 곱의 연산 순서에 기초하여 데이터 지역성을 결정하고, 컴파일 된 머신 코드를 생성할 수 있다. 스케줄러는 머신 코드에 포함된 스케줄링 정보를 활용하도록 구성될 수 있다. NPU 컨트롤러 내의 스케줄러는 스케줄링 정보에 기초하여 인공신경망모델의 레이어의 특징맵 및 가중치 데 이터가 저장된 메모리 어드레스 값을 획득할 수 있다. 예를 들면, NPU 컨트롤러 내의 스케줄러는 메모리에 저장된 인공신경망모델의 레이어의 특징맵 및 가중치 데이터가 저장된 메모리 어드레스 값을 획득할 수 있다. 따라서 NPU 컨트롤러 내의 스케줄러는 구동할 인 공신경망모델의 레이어의 특징맵 및 가중치 데이터를 메인 메모리에서 가져와서 NPU 내부 메모리에 저장할 수 있다. 각각의 레이어의 특징맵은 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. 각각의 가중치 데이터는 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보, 예를 들면, 인공신경망모델의 인공신경망의 레이어들의 배치 데이터, 지역성 정보 또는 구조에 대한 정보에 기초해서 복수 의 프로세싱 엘리먼트의 연산 순서에 대한 스케줄링 정보를 제공받을 수 있다. 스케줄링 정보는 컴파일 단 계에서 생성될 수 있다. NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 스케줄링 된 정보를 기초로 동작 하기 때문에, 일반적인 CPU의 스케줄링 개념과 다르게 동작할 수 있다. 일반적인 CPU의 스케줄링은 공평성, 효율성, 안정성, 반응 시간 등을 고려하여, 최상의 효율을 낼 수 있도록 동작한다. 즉, 우선 순위, 연산 시간 등을 고려해서 동일 시간내에 가장 많은 프로세싱을 수행하도록 스케줄링 한다. 종래의 CPU는 각 프로세싱의 우선 순서, 연산 처리 시간 등의 데이터를 고려하여 작업을 스케줄링 하는 알고리 즘을 사용하였다. 이와 다르게 NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 에 기초하여 결정된 신경 프로세싱 유닛의 프로세싱 순서대로 신경 프로세싱 유닛를 제어할 수 있다. 더 나아가면, NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및/또는 사용하려는 신경 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 결정된 프로세싱 순서대로 NPU를 구동할 수 있다. 단, 본 개시는 신경 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보에 제한되지 않는다. NPU 컨트롤러 내의 스케줄러는 인공신경망의 데이터 지역성 정보 또는 구조에 대한 정보를 저장하도록 구 성될 수 있다. 즉, NPU 컨트롤러 내의 스케줄러는 적어도 인공신경망모델의 인공신경망의 데이터 지역성 정보 또는 구조 에 대한 정보만 활용하더라도 프로세싱 순서를 결정할 수 있다. 더 나아가서, NPU 컨트롤러 내의 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및 신경 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보를 고려하여 신경 프로세싱 유닛(10 0)의 프로세싱 순서를 결정할 수 있다. 또한, 결정된 프로세싱 순서대로 신경 프로세싱 유닛의 프로세싱 최적화도 가능하다. 즉, NPU 컨트롤러는 컴파일러로부터 컴파일 된 머신 코드에 기초하여 동작하도록 구성될 수 있으나, 다른 예에서는 NPU 컨트롤러는 임베디드 컴파일러를 내장하도록 구성되는 것도 가능하다. 상술한 구성에 따르면, 신경 프로세싱 유닛은 다양한 AI 소프트웨어의 프레임워크의 형식의 파일을 입력받아 머신 코드를 생성하도록 구성될 수 있다. 예를 들면, AI 소프트웨어의 프레임워크는 TensorFlow, PyTorch, Keras, XGBoost, mxnet, DARKNET, ONNX 등이 있다. 복수의 프로세싱 엘리먼트는 인공신경망의 특징맵과 가중치 데이터를 연산하도록 구성된 복수의 프로세싱 엘리먼트들(PE1 to PE12)이 배치된 구성을 의미한다. 각각의 프로세싱 엘리먼트는 MAC (multiply and accumulate) 연산기 및/또는 ALU (Arithmetic Logic Unit) 연산기를 포함할 수 있다. 단, 본 개시에 따른 예시 들은 이에 제한되지 않는다. 각각의 프로세싱 엘리먼트는 추가적인 특수 기능을 처리하기 위해 추가적인 특수 기능 유닛을 선택적으로 더 포 함하도록 구성될 수 있다. 예를 들면, 프로세싱 엘리먼트(PE)는 배치-정규화 유닛, 활성화 함수 유닛, 인터폴레이션 유닛 등을 더 포함하 도록 변형 실시되는 것도 가능하다. SFU는 활성화 함수 연산, 풀링(pooling) 연산, 스트라이드(stride) 연산, 배치 정규화(batch- normalization) 연산, 스킵 커넥션(skip-connection) 연산, 접합(concatenation) 연산, 양자화(quantization) 연산, 클리핑(clipping) 연산, 패딩(padding) 연산 등을 인공신경망의 구조(architecture)에 따라 선별하여, 처리하도록 구성된 회로부를 포함할 수 있다. 즉, SFU는 복수의 특수 기능 연산 처리 회로 유닛들을 포함 할 수 있다. 도 3에서는 예시적으로 복수의 프로세싱 엘리먼트들이 도시되었지만, 하나의 프로세싱 엘리먼트 내부에 MAC을 대체하여, 복수의 곱셈기(multiplier) 및 가산기 트리(adder tree)로 구현된 연산기들이 병렬로 배치되어 구성 되는 것도 가능하다. 이러한 경우, 복수의 프로세싱 엘리먼트는 복수의 연산기를 포함하는 적어도 하나의 프로세싱 엘리먼트로 지칭되는 것도 가능하다. 복수의 프로세싱 엘리먼트는 복수의 프로세싱 엘리먼트들(PE1 to PE12)을 포함하도록 구성된다. 도 3에 도 시된 복수의 프로세싱 엘리먼트들(PE1 to PE12)은 단지 설명의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼 트들(PE1 to PE12)의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12)의 개수에 의해서 복수 의 프로세싱 엘리먼트의 사이즈 또는 개수가 결정될 수 있다. 복수의 프로세싱 엘리먼트의 사이즈는N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 복수의 프로세싱 엘리먼트는 N x M 개의 프로세싱 엘리먼트를 포함할 수 있다. 즉, 프로세싱 엘리먼트는 1개 이상일 수 있다. 복수의 프로세싱 엘리먼트의 사이즈는 신경 프로세싱 유닛이 작동하는 인공신경망모델의 특성을 고려 하여 설계할 수 있다. 복수의 프로세싱 엘리먼트는 인공신경망 연산에 필요한 덧셈, 곱셈, 누산 등의 기능을 수행하도록 구성된 다. 다르게 설명하면, 복수의 프로세싱 엘리먼트는 MAC(multiplication and accumulation) 연산을 수행하 도록 구성될 수 있다. 이하 복수의 프로세싱 엘리먼트 중 제1 프로세싱 엘리먼트(PE1)를 예를 들어 설명한다. 도 4a는 본 개시의 일 예시에 적용될 수 있는 복수의 프로세싱 엘리먼트 중 하나의 프로세싱 엘리먼트를 설명하 는 개략적인 개념도이다. 본 개시의 일 예시에 따른 신경 프로세싱 유닛은 복수의 프로세싱 엘리먼트, 복수의 프로세싱 엘리먼 트에서 추론될 수 있는 인공신경망모델을 저장하도록 구성된 NPU 내부 메모리 및 복수의 프로세싱 엘 리먼트 및 NPU 내부 메모리를 제어하도록 구성된 NPU 컨트롤러를 포함하고, 복수의 프로세싱 엘 리먼트는 MAC 연산을 수행하도록 구성되고, 복수의 프로세싱 엘리먼트는 MAC 연산 결과를 양자화해서 출력하도록 구성될 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. NPU 내부 메모리은 메모리 사이즈와 인공신경망모델의 데이터 사이즈에 따라 인공신경망모델의 전부 또는 일부를 저장할 수 있다. 제1 프로세싱 엘리먼트(PE1)는 곱셈기, 가산기, 누산기, 및 비트 양자화 유닛을 포함할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않으며, 복수의 프로세싱 엘리먼트는 인공신경망의 연 산 특성을 고려하여 변형 실시될 수도 있다. 곱셈기는 입력 받은 (N)bit 데이터와 (M)bit 데이터를 곱한다. 곱셈기의 연산 값은 (N+M)bit 데이터 로 출력된다. 곱셈기는 하나의 변수와 하나의 상수를 입력 받도록 구성될 수 있다. 누산기는 (L)loops 횟수만큼 가산기를 사용하여 곱셈기의 연산 값과 누산기의 연산 값을 누산 한다. 따라서 누산기의 출력부와 입력부의 데이터의 비트 폭은 (N+M+log2(L))bit로 출력될 수 있다. 여기서 L은 0보다 큰 정수이다. 누산기는 누산이 종료되면, 초기화 신호(initialization reset)를 입력 받아서 누산기 내부에 저장된 데이터를 0으로 초기화 할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 비트 양자화 유닛은 누산기에서 출력되는 데이터의 비트 폭을 저감할 수 있다. 비트 양자화 유닛 은 NPU 컨트롤러에 의해서 제어될 수 있다. 양자화된 데이터의 비트 폭은 (X)bit로 출력될 수 있다. 여기서 X는 0보다 큰 정수이다. 상술한 구성에 따르면, 복수의 프로세싱 엘리먼트는 MAC 연산을 수행하도 록 구성되고, 복수의 프로세싱 엘리먼트는 MAC 연산 결과를 양자화해서 출력할 수 있는 효과가 있다. 특히 이러한 양자화는 (L)loops가 증가할수록 소비 전력을 더 절감할 수 있는 효과가 있다. 또한 소비 전력이 저감되 면 발열도 저감할 수 있는 효과가 있다. 특히 발열을 저감하면 신경 프로세싱 유닛의 고온에 의한 오동작 발생 가능성을 저감할 수 있는 효과가 있다. 비트 양자화 유닛의 출력 데이터(X)bit은 다음 레이어의 노드 데이터 또는 합성곱의 입력 데이터가 될 수 있다. 만약 인공신경망모델이 양자화되었다면, 비트 양자화 유닛은 양자화된 정보를 인공신경망모델에서 제공받도록 구성될 수 있다. 단, 이에 제한되지 않으며, NPU 컨트롤러는 인공신경망모델을 분석하여 양자 화된 정보를 추출하도록 구성되는 것도 가능하다. 따라서 양자화된 데이터 사이즈에 대응되도록, 출력 데이터 (X)bit를 양자화 된 비트 폭으로 변환하여 출력될 수 있다. 비트 양자화 유닛의 출력 데이터(X)bit는 양자 화된 비트 폭으로 NPU 내부 메모리에 저장될 수 있다. 본 개시의 일 예시에 따른 신경 프로세싱 유닛의 복수의 프로세싱 엘리먼트는 곱셈기, 가산기 , 및 누산기를 포함한다. 비트 양자화 유닛은 양자화 적용 여부에 따라 취사 선택될 수 있다.다른 예시에서는, 비트 양자화 유닛은 SFU에 포함되도록 구성되는 것도 가능하다. 도 4b는 본 개시의 일 예시에 적용될 수 있는 SFU를 설명하는 개략적인 개념도이다. 도 4b를 참고하면 SFU는 여러 기능 유닛을 포함한다. 각각의 기능 유닛은 선택적으로 동작될 수 있다. 각 각의 기능 유닛은 선택적으로 턴-온되거나 턴-오프될 수 있다. 즉, 각각의 기능 유닛은 설정이 가능하다. 다시 말해서, SFU는 인공신경망 추론 연산에 필요한 다양한 회로 유닛들을 포함할 수 있다. 예를 들면, SFU의 회로 유닛들은 건너뛰고 연결하기(skip-connection) 동작을 위한 기능 유닛, 활성화 함 수(activation function) 동작을 위한 기능 유닛, 풀링(pooling) 동작을 위한 기능 유닛, 양자화 (quantization) 동작을 위한 기능 유닛, NMS(non-maximum suppression) 동작을 위한 기능 유닛, 정수 및 부동 소수점 변환(INT to FP32) 동작을 위한 기능 유닛, 배치 정규화(batch-normalization) 동작을 위한 기능 유닛, 보간법(interpolation) 동작을 위한 기능 유닛, 연접(concatenation) 동작을 위한 기능 유닛, 및 바이아스 (bias) 동작을 위한 기능 유닛 등을 포함할 수 있다. SFU의 기능 유닛들은 인공신경망모델의 데이터 지역성 정보에 의해서 선택적으로 턴-온되거나 혹은 턴-오 프될 수 있다. 인공신경망모델의 데이터 지역성 정보는 특정 레이어를 위한 연산이 수행될 때, 해당 기능 유닛 의 턴-오프 혹은 턴-오프와 관련된 제어 정보를 포함할 수 있다. SFU의 기능 유닛들 중 활성화된 유닛은 턴-온 될 수 있다. 이와 같이 SFU의 일부 기능 유닛을 선택적 으로 턴-오프하는 경우, 신경 프로세싱 유닛의 소비 전력을 절감할 수 있다. 한편, 일부 기능 유닛을 턴- 오프하기 위하여, 파워 게이팅(power gating)을 이용할 수 있다. 또는, 일부 기능 유닛을 턴-오프하기 위하여, 클럭 게이팅(clock gating)을 수행할 수도 있다. 도 5는 도 3에 도시된 신경 프로세싱 유닛의 변형예를 나타낸 예시도이다. 도 5에 도시된 신경 프로세싱 유닛은 도 3에 예시적으로 도시된 프로세싱 유닛과 비교하면, 복수의 프로세싱 엘리먼트를 제외하곤 실질적으로 동일하기 때문에, 이하 단지 설명의 편의를 위해서 중복 설명은 생략할 수 있다. 도 5에 예시적으로 도시된 복수의 프로세싱 엘리먼트는 복수의 프로세싱 엘리먼트들(PE1 to PE12) 외에, 각각의 프로세싱 엘리먼트들(PE1 to PE12)에 대응되는 각각의 레지스터 파일들(RF1 to RF12)을 더 포함할 수 있 다. 도 5에 도시된 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12)은 단지 설명 의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12) 의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12)의 개수에 의해서 복수의 프 로세싱 엘리먼트의 사이즈 또는 개수가 결정될 수 있다. 복수의 프로세싱 엘리먼트 및 복수의 레지스 터 파일들(RF1 to RF12)의 사이즈는 N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 복수의 프로세싱 엘리먼트의 어레이 사이즈는 신경 프로세싱 유닛이 작동하는 인공신경망모델의 특 성을 고려하여 설계할 수 있다. 부연 설명하면, 레지스터 파일의 메모리 사이즈는 작동할 인공신경망모델의 데 이터 사이즈, 요구되는 동작 속도, 요구되는 소비 전력 등을 고려하여 결정될 수 있다. 신경 프로세싱 유닛의 레지스터 파일들(RF1 to RF12)은 프로세싱 엘리먼트들(PE1 to PE12)과 직접 연결된 정적 메모리 유닛이다. 레지스터 파일들(RF1 to RF12)은 예를 들면, 플립플롭, 및/또는 래치 등으로 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 대응되는 프로세싱 엘리먼트들(PE1 to PE12)의 MAC 연산 값을 저장하도 록 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 NPU 내부 메모리와 가중치 데이터 및/또는 노드 데이 터를 제공하거나 제공받도록 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 MAC 연산 시 누산기의 임시 메모리의 기능을 수행하도록 구성되는 것도 가능하 다.<본 개시의 발명자에 의해 찾아진 기술적 난관> 인공지능 서비스의 목적에 따라 여러 타입의 인공신경망(ANN) 모델이 존재할 수 있다. 예를 들어, 입력되는 데 이터가 이미지 또는 영상인 경우, 인공지능 서비스를 위하여 이미지/영상 내의 객체 분류, 객체 검출, 객체 추 적 등을 위한 CNN 타입의 인공신경망모델이 사용될 수 있다. 일반적으로, 인공신경망모델은 레이어 마다 연산량이 다를 수 있다. 이를 도 6을 참조하여 설명하기로 한다. 도 6a은 예시적인 인공신경망모델 내의 각 레이어 별 데이터의 크기를 나타낸 예시도이다. 도 6b는 도 6a에 도 시된 예시적 인공신경망모델에서 각 레이어 별 데이터 사이즈 등을 나타낸 예시적인 테이블이다. 도 6a에 도시된 예시적인 인공신경망모델은 Mobilenet V1인 것으로 나타나 있다. 도 6a에 도시된 가로축은 예시 적인 인공신경망모델에서 레이어들을 순차적으로 나타내고, 세로축은 데이터의 크기를 나타낸다. 도 6a에 도시된 레이어 1을 참고하면, 입력 특징맵의 크기(IFMAP_SIZE) 보다 출력 특징맵의 크기(OFMAP_SIZE)가 더 큰 것을 알 수 있다. 레이어 1의 출력 특징맵은 레이어 2로 전달되어, 상기 레이어 2의 입력 특징맵이 된다. 상기 레이어 2의 연산이 마쳐지면, 출력 특징맵이 출력된다. 상기 레이어 2의 출력 특징맵은 다시 레이어 3로 전달되어, 상기 레이어 3 의 입력 특징맵이 된다. 이처럼 각 레이어에 입력되는 입력 데이터의 크기와 각 레이어에서 출력되는 출력 특징맵의 크기는 상이할 수 있다. 이에 따라 임의 레이어에서는 연산량이 작을 수도 있지만 다른 레이어에서는 연산량이 매우 클 수 있다. 이처럼 레이어 별 연산량이 큰 폭으로 변화됨에 따라, 전력 제어가 쉽지 않게 되는 문제가 발생한다. 각 레이어는 신경 프로세싱 유닛의 프로세싱 엘리먼트의 개수 및 NPU 내부 메모리의 용량 한계에 의해서 복수의 연산 단계로 나누어져서 처리될 수 있다. 이에, 신경 프로세싱 유닛은 각 레이어를 복수의 타일로 나누어 복수의 연산 단계를 처리하도록 스케줄링 할 수 있다. 상기 스케줄링은 컴파일러에서 수행될 수 있다. 예를 들면, 하나의 레이어는 4개의 타일로 분할될 수 있다. 각각의 타일은 신경 프로세싱 유닛에서 순차적으로 처리될 수 있다. 컴파일 된 인공신경망모델은 컴파일 될 때 결정된 연산 순서 정보를 스케줄러에 저장할 수 있 다. 이때 각 연산 단계 마다 PE 이용률이 정보가 제공될 수 있다 각 레이어의 연산량은 MAC으로 알 수 있다. 각 레이어별 로 연산량은 최대 227배 차이가 나는 것을 알 수 있다. 신경 프로세싱 유닛은 각 레이어의 연산 단계마다 MAC 연산량에 비례하여 복수의 프로세싱 엘리먼트 중 이용되는 프로세싱 엘리먼트의 개수를 결정할 수 있다. 그리고 이용되는 프로세싱 엘리먼트의 개수에 비례하 여 전력 소비량이 증가할 수 있다. 여기서, 이용되는 프로세싱 엘리먼트의 개수와 프로세싱 엘리먼트의 총 개수를 알면 신경 프로세싱 유닛의 이용률(%)을 계산할 수 있다. 따라서, 신경 프로세싱 유닛에서 처리되는 인공신경망모델의 각 레이어의 각 연산 단계마다 PE 이용률이 계산될 수 있다. 또한 신경 프로세싱 유닛이 처리하는 인공신경망모델의 구조가 변경되지 않는 한, 각 연 산 단계 별 PE 이용률은 반복적으로 정확히 예측될 수 있다. 즉, 신경 프로세싱 유닛은 특정 인공신경망모 델을 반복적으로 추론할 수 있다. 이러한 경우, 신경 프로세싱 유닛은 동일한 가중치 파라미터와 동일한 네트워크 레이어 구조를 반복 사용할 수 있다는 것을 본 개시의 발명자들이 인식하였다. 또한, 인접한 레이어들의 MAC 연산량 차이가 클수록, 인접한 레이어들 사이의 피크 파워의 편차가 증가할 수 있 다. 그리고 인접한 레이어들의 피크 파워의 편차가 클수록 공급 전압(VDD)이 더 흔들릴 수 있다. 이때, 공급 전 압(VDD)의 안정성을 고려하여 구동 주파수를 가변 할 수 있다는 사실을 본 개시의 발명자들이 인식하였다. 특히, 특정 레이어의 연산 단계에서 연산량이 크게 증가하면, 순간 전력 소모량이 증가함으로써, 시스템 안정성 이 저하되는 문제가 발생할 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. 예를 들어, 특정 레이어의 연산 단계의 연산을 위해서 동시에 많은 프로세싱 엘리먼트들이 가동될 수 있다. 각 각의 프로세싱 엘리먼트 구동을 위해서 소정의 전원이 필요하며, 상당한 개수의 프로세싱 엘리먼트들이 동시에구동을 하면 순간적으로 필요한 파워가 급증할 수 있다. 만약 신경 프로세싱 유닛이 저전력 동작에 특화되어 설 계될 경우, 서버용 신경 프로세싱 유닛보다 파워 공급 능력이 상대적으로 부족할 수 있다. 따라서, 이러한 엣지 디바이스용 신경 프로세싱 유닛은 순간 파워 공급 이슈에 상대적으로 더 취약할 수 있으며, 파워 공급량이 폭증 할 경우, 공급 전압(VDD)이 흔들릴 수 있다. 특히 공급 전압(VDD)이 트랜지스터의 임계 전압 이하로 떨어질 경 우, 트랜지스터에 저장된 데이터가 손실될 수 있다. 다르게 설명하면, 공급 전압(VDD)이 낮아지는 경우 트랜지 스터의 동작 속도가 저하되어 setup/hold violation 문제가 발생해 오동작이 일어날 수 있다. 특히 이러한 현상 은 반도체 파운더리의 공정이 3nm, 4nm, 5nm, 7nm 와 같이 낮아질수록 더 심화될 수 있다. 다른 예를 들어, 인공신경망을 위한 연산, 즉 예컨대 가산(add), 곱셈(multiply), 누산(accumulate)을 수행하는 PE들이 전력 소모를 순간적으로 많이 사용함으로써, 신경 프로세싱 유닛 내의 다른 컴포넌트, 예컨대 내부 메모리에는 충분한 전력이 공급되지 못할 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. 구체적 으로 내부 메모리에 충분한 전력이 공급되지 못하면, 저장되어 있는 데이터 비트가 손상(compromise)될 가 능성도 배제할 수 없는 문제점이 있을 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. 더 나아가서, 특정 연산 단계에서 PE들의 이용률이 낮을 경우, 신경 프로세싱 유닛의 구동 주파수를 증가시키더 라도, 신경 프로세싱 유닛의 전압 안정성이 보장될 수 있다는 것을 본 개시의 발명자들이 인식하게 되었다. <본 특허의 개시들> 전술한 문제점은, 인공신경망모델의 연산이 하드웨어적으로 하나의 클럭 신호를 기준으로 수행되기 때문에 발생 하는 것임을, 본 개시의 발명자들이 인식하게 되었다. 따라서, 본 특허의 발명자는, NPU 내의 상기 복수의 PE들은 제1 그룹의 PE들과 제2 그룹의 PE들로 분할한 후, 서로 다른 클럭 신호에 따라 동작되도록 함으로써, 피크 파워를 낮추는 기법을 발명하게 되었다. 본 개시의 발명자들이 발명한 기법이 구현되는 여러 예시들을 이하 도면을 참고하여 설명하기로 한다. 도 7a는 본 개시의 제1 예시에 따른 NPU의 구조를 나타낸 예시도이다. 도 7a에서는 NPU가 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 내부 메모리, SFU, 클럭 소 스, 그리고 클럭 분배기를 포함하는 것으로 도시되어 있다. 몇몇 예시에서는, 상기 NPU는 도 3 또는 도 5에 도시된 바와 같이 NPU 컨트롤러 그리고 NPU 인터페이 스를 더 포함할 수 있다. 도 7a에 도시된 제1 그룹의 PE(110-1)와 제2 그룹의 PE(110-2)는 도 3 또는 도 5에 도시된 복수의 PE들이 나뉜 것으로 이해될 수 있다. 각 그룹에 속한 PE들의 개수는 도 7a에서는 예시적으로 8개인 것으로 나타나 있지 만, 각 그룹에 속한 PE들의 개수는 달라질 수 있는 것은 자명할 것이다. 또한, 각 그룹에 속한 PE들의 개수는 반-고정적으로 혹은 동적으로 변경될 수 있다. 예를 들어, 제1 그룹의 PE들(110-1)은 10개의 PE들을 포함하고, 제2 그룹의 PE들(110-2)은 6개의 PE들을 포함하도록 설정될 수 있다. 이러한 변경은 NPU 컨트롤러의 제어 에 따라 수행될 수 있다. 각 PE 또는 PE들의 각 그룹은 NPU 코어(core), NPU 엔진(engine), NPU 쓰레드(thread) 등으로 지칭되는 것도 가능하다. 상기 NPU 코어, NPU 엔진, NPU 쓰레드 등은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 상기 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-02), 상기 내부 메모리, NPU 컨트롤러, 상기 NPU 인 터페이스, SFU, 상기 클럭 소스, 그리고 상기 클럭 분배기 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 예를 들어, 제1 그룹의 PE(110-1) 및 제2 그룹의 PE(110-2)들은 제1 회로로 지칭될 수 있다. 예를 들어, 내부 메모리는 제2 회로로 지칭될 수 있다. 예를 들어, SFU는 제3 회로로 지칭될 수 있다. 예를 들어, 클럭 소스는 적어도 하나의 클럭 신호들을 출력하도록 설정된 제4 회로로 지칭될 수 있다. 예를 들어, 클럭 분배기는 상기 적어도 하나의 클럭 신호를 변환하여 복수개의 클럭 신호들로 출력하도록 설정된 제5 회로로 지칭될 수 있다. 상기 클럭 분배기는 상기 클럭 소스에 의해서 출력되는 하나의 클럭 신호의 위상을 교정(calibrate) 혹은 조정(adjust)하여, 복수 개의 클럭 신호를 출력할 수 있다. 단 본 개시의 예시들은 이에 제한되지 않으며, 임의 회로는 복수의 PE들으로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. 상기 클럭 소스가 원 클럭 신호를 생성하여 출력하면, 상기 클럭 분배기는 원 클럭 신호를 그대로 (bypass)하여 출력하기도 하고 또는 상기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)하여 출력할 수 있다. 예를 들어, 상기 클럭 분배기는 상기 클럭 소스로부터의 상기 원 클럭 신호를 그대로(bypass) 상기 제1 그룹의 PE들(110-1)에게 전달할 수 있다. 또는 상기 클럭 분배기는 상기 클럭 소스 로부터의 상 기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)시킨 제1 지연 클럭 신호를 상기 제1 그룹의 PE들 (110-1)에게 전달할 수 있다. 또한, 상기 클럭 분배기는 상기 클럭 소스 로부터의 상기 원 클럭 신호 의 위상을 지연(delay) 또는 쉬프트(shift)시킨 제2 지연 클럭 신호를 제2 그룹의 PE들(110-2)에게 전달할 수 있다. 내부 메모리는 클럭 분배기로부터 원 클럭 신호를 그대로(bypass) 제공받도록 구성되는 것도 가능하다. 한편, 도 7a에서 도시되지는 않았으나, 상기 SFU는 상기 NPU 내부 메모리에 직접 연결될 수 있다. 한편, 도 7a에 도시된 상기 내부 메모리와 상기 SFU도 상기 클럭 분배기로부터 출력되는 클럭 신호(예를 들어, 원 클럭 신호 또는 위상 지연된 클럭 신호)에 따라 동작할 수 있다. 다른 한편, 도 7a에 도시된 상기 제1 그룹의 PE들(110-1)과 상기 제2 그룹의 PE들(110-2)이 인공신경망모델의 임의 레이어에 대한 연산을 수행 완료하면, 상기 클럭 분배기로부터 각기 제공받은 클럭 신호에 따라 상기 제1 그룹의 PE들(110-1)과 상기 제2 그룹의 PE들(110-2)은 각기 출력 특징맵을 출력할 수 있다. 구체적으로, 상 기 제1 그룹의 PE들(110-1)은 제1 출력 특징맵을 상기 클럭 분배기로부터 제공받은 임의의 클럭 신호(예를 들어 상기 원 클럭 신호 또는 상기 제1 지연 클럭 신호)에 따라 출력할 수 있다. 마찬가지로 상기 제2 그룹의 PE들(110-2)은 제2 출력 특징맵을 상기 클럭 분배기로부터 제공받은 임의의 클럭 신호(예를 들어 제2 지연 클럭 신호)에 따라 출력할 수 있다. 상기 제1 그룹의 PE들(110-1)에 의해서 출력되는 제1 출력 특징맵과 상기 제2 그룹의 PE들(110-2)에 의해서 출 력되는 제2 출력 특징맵이 서로 의존적이지 않고 독립적인 경우, 상기 SFU는 FIFO(First Input First Output) 방식으로 먼저 전달받은 출력 특징맵을 먼저 처리할 수 있다. 한편, 도 7a에 도시된 상기 제1 그룹의 PE들(110-1)과 상기 제2 그룹의 PE들(110-2)이 인공신경망모델의 임의 레이어에 대한 연산을 수행 완료하면, 상기 제1 그룹의 PE들(110-1)과 상기 제2 그룹의 PE들(110-2)은 연산 결 과, 즉 출력 특징맵을 상기 클럭의 제1 부분 및 제2 부분 중에서 어느 하나의 부분에 맞추어서 출력할 수 있다. 이를 위하여, 상기 제1 그룹의 PE들(110-1)과 상기 제2 그룹의 PE들(110-2) 중 적어도 하나 이상은 시간 지연 버퍼(예컨대, 쉬프트 레지스터)를 더 포함할 수 있다. 상기 시간 지연 버퍼(예컨대, 쉬프트 레지스터)는 상기 출력이 상기 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 더 늦은 부분에 맞추어서 수행되게끔, 시간 지연 을 수행할 수 있다. 예를 들어 상기 제2 그룹의 PE들(110-2)의 출력 포트는 시간 지연 버퍼(예컨대, 쉬프트 레지스터)에 연결될 수 있다. 대안적으로, 도 7a에 도시된 상기 제1 그룹의 PE들(110-1)은 제1 출력 특징맵을 임의 클럭의 제1 부분에 맞추어 서 출력하고, 상기 제2 그룹의 PE들(110-2)은 제2 출력 특징맵을 상기 임의 클럭의 제2 부분에 맞추어서 출력할 수 있다. 이 경우, 상기 SFU가 시간 지연 버퍼(예컨대, 쉬프트 레지스터)를 포함할 수 있다. 상기 SFU는 상기 임의 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 더 빠른 부분에 맞추어서 전달받은 출력 특징맵을 시간 지연시킴으로써, 상기 임의 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 늦은 부분에 맞추어 서 전달받게 될 출력 특징맵과 시간이 동기 되도록 할 수 있다. 대안적으로, 상기 제1 그룹의 PE들(110-1)에 의해서 출력되는 제1 출력 특징맵과 상기 제2 그룹의 PE들(110- 2)에 의해서 출력되는 제2 출력 특징맵이 서로 의존적이지 않고 독립적인 경우, 상기 SFU는 상기 클럭의 제1 부분 및 제2 부분 중에서 시간적으로 더 빠른 부분에 맞추어서 전달받은 출력 특징맵을 먼저 처리할 수 있다. 즉, 상기 제1 그룹의 PE들(110-1)로부터의 제1 출력과, 상기 제2 그룹의 PE들(110-2)로부터의 제2 출력은 상기 임의 클럭의 제1 부분을 기준으로 전달될 수 있다. 상기 제2 그룹의 PE들(110-2)을 위한 기준 위상은 상기 클럭의 제2 부분에서 제1 부분으로 변환될 수 있다. 도 7a에 도시된 상기 NPU 내의 각 엘리먼트에는 공급 전압(VDD)이 입력될 수 있다. 도 7a에서는 공통된 공급 전압(VDD)이 입력되는 것으로 도시 되었으나, 본 개시의 예시들은 이에 제한되지 않는 다. 몇몇 예시들에서는, 상기 NPU 내의 각 엘리먼트에는 독립되거나 혹은 분리된 공급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 다른 예시들에서는, 상기 NPU 내의 각 엘리먼트에는 독립되거나 혹은 분리된 공급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 또 다른 예시들에서는, 상기 NPU 내의 특정 엘리먼트에 제1 공급 전압과 상이한 제2 공급 전압이 입력되도 록 구성되는 것도 가능하다. 이상에서 설명한 바와 같이, 본 개시의 일 예시는 도 3 또는 도 5에 도시된 복수의 PE들을 도 7a에 도시된 바와 같이 제1 그룹의 PE(110-1)와 제2 그룹의 PE(110-2)로 나눈 후, 상기 제1 그룹의 PE(110-1)와 상기 제2 그룹의 PE(110-2)들을 서로 다른 클럭 신호에 따라 분산 동작하게 함으로써, 복수의 PE 그룹들(110-1, 110-2)의 피크 파워를 낮출 수 있도록 한다. 또한, 본 개시의 일 예시는 위상이 서로 다른 클럭 신호들을 복수개로 제공하기 위해서, 클럭 소스를 복수 개로 두지 않고, 단순히 위상 변환기만을 추가함으로써, 제조 단가를 낮출 수 있도록 한다. 도 7b는 본 개시의 제1 예시의 제1 변형예에 따른 NPU의 구조를 나타낸 예시도이다. 도 7b에서는 NPU가 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 제3 그룹의 PE(110-3), 내부 메모리 , SFU, 클럭 소스, 그리고 클럭 분배기를 포함하는 것으로 도시되어 있다. 몇몇 예시에서는, 상기 NPU는 도 3 또는 도 5에 도시된 바와 같이 NPU 컨트롤러 그리고 NPU 인터페이 스를 더 포함할 수 있다. 도 7b에 도시된 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 제3 그룹의 PE(110-3)는 도 3 또는 도 5에 도시 된 복수의 PE들이 나뉜 것으로 이해될 수 있다. 각 그룹에 속한 PE들의 개수는 도 7b에서는 예시적으로 8 개인 것으로 나타나 있지만, 각 그룹에 속한 PE들의 개수는 달라질 수 있는 것은 자명할 것이다. 또한, 각 그룹 에 속한 PE들의 개수는 반-고정적으로 혹은 동적으로 변경될 수 있다. 예를 들어, 제1 그룹의 PE들(110-1)은 10 개의 PE들을 포함하고, 제2 그룹의 PE들(110-2)은 8개의 PE들을 포함하고, 제3 그룹의 PE(110-3)은 6개의 PE들 을 포함하도록 설정될 수 있다. 이러한 변경은 NPU 컨트롤러의 제어에 따라 수행될 수 있다. 각 PE 또는 PE들의 각 그룹은 NPU 코어(core), NPU 엔진(engine), NPU 쓰레드(thread) 등으로 지칭되는 것도 가능하다. 상기 NPU 코어, NPU 엔진, NPU 쓰레드 등은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 상기 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-02), 제3 그룹의 PE(110-3), 상기 내부 메모리, NPU 컨트 롤러, 상기 NPU 인터페이스, SFU, 상기 클럭 소스, 그리고 상기 클럭 분배기 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 예를 들어, 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 제3 그룹의 PE(110-3)는 제1 회로로 지칭될 수 있다. 예를 들어, 내부 메모리는 제2 회로로 지칭될 수 있다. 예를 들어, SFU는 제3 회로로 지칭될 수 있다. 예를 들어, 클럭 소스는 적어도 하나의 클럭 신호들을 출력하도록 설정된 제4 회로로 지칭될 수 있다. 예를 들어, 클럭 분배기는 상기 적어도 하나의 클럭 신호를 변환하여 복수개의 클럭 신호들로 출력하도록 설정된 제5 회로로 지칭될 수 있다. 상기 클럭 분배기는 상기 클럭 소스에 의해서 출력되는 하나의 클럭 신호의 위상을 교정(calibrate) 혹은 조정(adjust)하여, 복수 개의 클럭 신호를 출력할 수 있다. 단 본 개시의 예시들은 이에 제한되지 않으며, 임의 회로는 복수의 PE들으로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. 상기 클럭 소스가 원 클럭 신호를 생성하여 출력하면, 상기 클럭 분배기는 원 클럭 신호를 그대로 (bypass)하여 출력하기도 하고 또는 상기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)하여 출력할 수 있다. 예를 들어, 상기 클럭 분배기는 상기 클럭 소스로부터의 상기 원 클럭 신호를 그대로(bypass) 상기 제1 그룹의 PE들(110-1)에게 전달할 수 있다. 또는 상기 클럭 분배기는 상기 클럭 소스 로부터의 상 기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)시킨 제1 지연 클럭 신호를 상기 제1 그룹의 PE들 (110-1)에게 전달할 수 있다. 또한, 상기 클럭 분배기는 상기 클럭 소스 로부터의 상기 원 클럭 신호 의 위상을 지연(delay) 또는 쉬프트(shift)시킨 제2 지연 클럭 신호를 제2 그룹의 PE들(110-2)에게 전달할 수 있다. 또한, 상기 클럭 분배기는 상기 클럭 소스로부터의 상기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)시킨 제3 지연 클럭 신호를 제3 그룹의 PE들(110-3)에게 전달할 수 있다. 내부 메모리 는 클럭 분배기로부터 원 클럭 신호를 그대로(bypass) 제공받도록 구성되는 것도 가능하다. 한편, 도 7b에서 도시되지는 않았으나, 상기 SFU는 상기 NPU 내부 메모리에 직접 연결될 수 있다. 한편, 도 7b에 도시된 상기 내부 메모리와 상기 SFU도 상기 클럭 분배기로부터 출력되는 클럭 신호(예를 들어, 원 클럭 신호 또는 위상 지연된 클럭 신호)에 따라 동작할 수 있다. 다른 한편, 도 7b에 도시된 상기 제1 그룹의 PE들(110-1), 상기 제2 그룹의 PE들(110-2), 제3 그룹의 PE들 (110-3)이 인공신경망모델의 임의 레이어에 대한 연산을 수행 완료하면, 상기 클럭 분배기로부터 각기 제 공받은 클럭 신호에 따라 상기 제1 그룹의 PE들(110-1), 상기 제2 그룹의 PE들(110-2), 제3 그룹의 PE들(110- 3)은 각기 출력 특징맵을 출력할 수 있다. 구체적으로, 상기 제1 그룹의 PE들(110-1)은 제1 출력 특징맵을 상기 클럭 분배기로부터 제공받은 임의의 클럭 신호(예를 들어 상기 원 클럭 신호 또는 상기 제1 지연 클럭 신 호)에 따라 출력할 수 있다. 마찬가지로 상기 제2 그룹의 PE들(110-2)은 제2 출력 특징맵을 상기 클럭 분배기 로부터 제공받은 임의의 클럭 신호(예를 들어 제2 지연 클럭 신호)에 따라 출력할 수 있다. 또한, 상기 제 3 그룹의 PE들(110-3)은 제3 출력 특징맵을 상기 클럭 분배기로부터 제공받은 임의의 클럭 신호(예를 들어 제3 지연 클럭 신호)에 따라 출력할 수 있다. 상기 제1 그룹의 PE들(110-1)에 의해서 출력되는 제1 출력 특징맵, 상기 제2 그룹의 PE들(110-2)에 의해서 출력 되는 제2 출력 특징맵 그리고 상기 제3 그룹의 PE들(110-3)에 의해서 출력되는 제3 출력 특징맵이 서로 의존적 이지 않고 독립적인 경우, 상기 SFU는 FIFO(First Input First Output) 방식으로 먼저 전달받은 출력 특 징맵을 먼저 처리할 수 있다. 한편, 도 7b에 도시된 상기 제1 그룹의 PE들(110-1), 상기 제2 그룹의 PE들(110-2), 상기 제3 그룹의 PE들 (110-3)이 인공신경망모델의 임의 레이어에 대한 연산을 수행 완료하면, 상기 제1 그룹의 PE들(110-1), 상기 제 2 그룹의 PE들(110-2), 상기 제3 그룹의 PE들(110-3)은 연산 결과, 즉 출력 특징맵을 상기 클럭의 제1 부분, 제 2 부분 그리고 제3 부분 중에서 어느 하나의 부분에 맞추어서 출력할 수 있다. 이를 위하여, 상기 제1 그룹의 PE들(110-1), 상기 제2 그룹의 PE들(110-2) 그리고 상기 제3 그룹의 PE들(110-3)중 적어도 하나 이상은 시간 지 연 버퍼(예컨대, 쉬프트 레지스터)를 더 포함할 수 있다. 상기 시간 지연 버퍼(예컨대, 쉬프트 레지스터)는 상 기 출력이 상기 클럭의 제1 부분, 제2 부분 그리고 제3 부분중에서 시간적으로 더 늦은 부분에 맞추어서 수행되 게끔, 시간 지연을 수행할 수 있다. 예를 들어 상기 제2 그룹의 PE들(110-2)의 출력 포트는 시간 지연 버퍼(예컨대, 쉬프트 레지스터)에 연결될 수 있다. 마찬가지로, 상기 제3 그룹의 PE들(110-3)의 출력 포트는 시간 지연 버퍼(예컨대, 쉬프트 레지스터)에 연 결될 수 있다 대안적으로, 도 7b에 도시된 상기 제1 그룹의 PE들(110-1)은 제1 출력 특징맵을 임의 클럭의 제1 부분에 맞추어 서 출력하고, 상기 제2 그룹의 PE들(110-2)은 제2 출력 특징맵을 상기 임의 클럭의 제2 부분에 맞추어서 출력할 수 있다. 또한, 상기 제3 그룹의 PE들(110-3)은 제3 출력 특징맵을 상기 임의 클럭의 제3 부분에 맞추어서 출력 할 수 있다. 이 경우, 상기 SFU가 시간 지연 버퍼(예컨대, 쉬프트 레지스터)를 포함할 수 있다. 상기 SFU는 상기 임의 클럭의 제1 부분, 제2 부분 그리고 제3 부분중에서 시간적으로 더 빠른 부분에 맞추어서전달받은 출력 특징맵을 시간 지연시킴으로써, 상기 임의 클럭의 제1 부분, 제2 부분 그리고 제3 부분 중에서 시간적으로 늦은 부분에 맞추어서 전달받게 될 출력 특징맵과 시간이 동기 되도록 할 수 있다. 대안적으로, 상기 제1 그룹의 PE들(110-1)에 의해서 출력되는 제1 출력 특징맵과 상기 제2 그룹의 PE들(110- 2)에 의해서 출력되는 제2 출력 특징맵 그리고 상기 제3 그룹의 PE들(110-3)에 의해서 출력되는 제3 출력 특징 맵이 서로 의존적이지 않고 독립적인 경우, 상기 SFU는 상기 클럭의 제1 부분, 제2 부분 그리고 제3 부분 중에서 시간적으로 더 빠른 부분에 맞추어서 전달받은 출력 특징맵을 먼저 처리할 수 있다. 즉, 상기 제1 그룹의 PE들(110-1)로부터의 제1 출력과, 상기 제2 그룹의 PE들(110-2)로부터의 제2 출력 그리고 상기 제3 그룹의 PE들(110-3)로부터의 제3 출력은 상기 임의 클럭의 제1 부분을 기준으로 전달될 수 있다. 상기 제2 그룹의 PE들(110-2)을 위한 기준 위상은 상기 클럭의 제2 부분에서 제1 부분으로 변환될 수 있다. 마 찬가지로, 상기 제3 그룹의 PE들(110-3)을 위한 기준 위상은 상기 클럭의 제3 부분에서 제1 부분으로 변환될 수 있다. 도 7b에 도시된 상기 NPU 내의 각 엘리먼트에는 공급 전압(VDD)이 입력될 수 있다. 도 7b에서는 공통된 공급 전압(VDD)이 입력되는 것으로 도시 되었으나, 본 개시의 예시들은 이에 제한되지 않는 다. 몇몇 예시들에서는, 상기 NPU 내의 각 엘리먼트에는 독립되거나 혹은 분리된 공급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 다른 예시들에서는, 상기 NPU 내의 각 엘리먼트에는 독립되거나 혹은 분리된 공급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 또 다른 예시들에서는, 상기 NPU 내의 특정 엘리먼트에 제1 공급 전압과 상이한 제2 공급 전압이 입력되도 록 구성되는 것도 가능하다. 이상에서 설명한 바와 같이, 본 개시의 일 예시는 도 3 또는 도 5에 도시된 복수의 PE들을 도 7b에 도시된 바와 같이 제1 그룹의 PE(110-1)와 제2 그룹의 PE(110-2) 그리고 제3 그룹의 PE(110-3)로 나눈 후, 그룹들을 서로 다른 클럭 신호에 따라 분산 동작하게 함으로써, 피크 파워를 낮출 수 있도록 한다. 또한, 본 개시의 일 예시는 위상이 서로 다른 클럭 신호들을 복수개로 제공하기 위해서, 클럭 소스를 복수 개로 두지 않고, 단순히 클럭 분배기만을 추가함으로써, 제조 단가를 낮출 수 있도록 한다. 도 7c는 본 개시의 제1 예시의 제2 변형예에 따른 NPU의 구조를 나타낸 예시도이다. 도 7c에서는 NPU가 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 제3 그룹의 PE(110-3), 제4 그룹의 PE(110-4), 내부 메모리, SFU, 클럭 소스, 그리고 클럭 분배기를 포함하는 것으로 도시되 어 있다. 몇몇 예시에서는, 상기 NPU는 도 3 또는 도 5에 도시된 바와 같이 NPU 컨트롤러 그리고 NPU 인터페이 스를 더 포함할 수 있다. 도 7b에 도시된 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 제3 그룹의 PE(110-3), 제4 그룹의 PE(110-4)는 도 3 또는 도 5에 도시된 복수의 PE들이 나뉜 것으로 이해될 수 있다. 각 그룹에 속한 PE들의 개수는 도 7c에서는 예시적으로 8개인 것으로 나타나 있지만, 각 그룹에 속한 PE들의 개수는 달라질 수 있는 것은 자명할 것이다. 또한, 각 그룹에 속한 PE들의 개수는 반-고정적으로 혹은 동적으로 변경될 수 있다. 예를 들어, 제1 그 룹의 PE들(110-1)은 14개의 PE들을 포함하고, 제2 그룹의 PE들(110-2)은 8개의 PE들을 포함하고, 제3 그룹의 PE(110-3)은 6개의 PE들을 포함하고, 제4 그룹의 PE(110-4)들은 4개의 PE들을 포함하도록 설정될 수 있다. 이러 한 변경은 NPU 컨트롤러의 제어에 따라 수행될 수 있다. 각 PE 또는 PE들의 각 그룹은 NPU 코어(core), NPU 엔진(engine), NPU 쓰레드(thread) 등으로 지칭되는 것도 가능하다. 상기 NPU 코어, NPU 엔진, NPU 쓰레드 등은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 상기 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-02), 제3 그룹의 PE(110-3), 제4 그룹의 PE(110-4), 상기 내부 메모리, NPU 컨트롤러, 상기 NPU 인터페이스, SFU, 상기 클럭 소스, 그리고 상기 클 럭 분배기 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으 로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 예를 들어, 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 제3 그룹의 PE(110-3), 제4 그룹의 PE(110-4)는 제1 회로로 지칭될 수 있다. 예를 들어, 내부 메모리는 제2 회로로 지칭될 수 있다. 예를 들어, SFU는 제3 회로로 지칭될 수 있다. 예를 들어, 클럭 소스는 적어도 하나의 클럭 신호들을 출력하도록 설정된 제4 회로로 지칭될 수 있다. 예를 들어, 클럭 분배기는 상기 적어도 하나의 클럭 신호를 변환하여 복수개의 클럭 신호들로 출력하도록 설정된 제5 회로로 지칭될 수 있다. 상기 클럭 분배기는 상기 클럭 소스에 의해서 출력되는 하나의 클럭 신호의 위상을 교정(calibrate) 혹은 조정(adjust)하여, 복수 개의 클럭 신호를 출력할 수 있다. 단 본 개시의 예시들은 이에 제한되지 않으며, 임의 회로는 복수의 PE들으로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. 상기 클럭 소스가 원 클럭 신호를 생성하여 출력하면, 상기 클럭 분배기는 원 클럭 신호를 그대로 (bypass)하여 출력하기도 하고 또는 상기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)하여 출력할 수 있다. 예를 들어, 상기 클럭 분배기는 상기 클럭 소스로부터의 상기 원 클럭 신호를 그대로(bypass) 상기 제1 그룹의 PE들(110-1)에게 전달할 수 있다. 또는 상기 클럭 분배기는 상기 클럭 소스 로부터의 상 기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)시킨 제1 지연 클럭 신호를 상기 제1 그룹의 PE들 (110-1)에게 전달할 수 있다. 또한, 상기 클럭 분배기는 상기 클럭 소스 로부터의 상기 원 클럭 신호 의 위상을 지연(delay) 또는 쉬프트(shift)시킨 제2 지연 클럭 신호를 제2 그룹의 PE들(110-2)에게 전달할 수 있다. 또한, 상기 클럭 분배기는 상기 클럭 소스로부터의 상기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)시킨 제3 지연 클럭 신호를 제3 그룹의 PE들(110-3)에게 전달할 수 있다. 또한, 상기 클럭 분배기는 상기 클럭 소스로부터의 상기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)시킨 제4 지연 클럭 신호를 제4 그룹의 PE들(110-4)에게 전달할 수 있다. 내부 메모리는 클럭 분배기로부 터 원 클럭 신호를 그대로(bypass) 제공받도록 구성되는 것도 가능하다. 한편, 도 7c에서 도시되지는 않았으나, 상기 SFU는 상기 NPU 내부 메모리에 직접 연결될 수 있다. 한편, 도 7c에 도시된 상기 내부 메모리와 상기 SFU도 상기 클럭 분배기로부터 출력되는 클럭 신호(예를 들어, 원 클럭 신호 또는 위상 지연된 클럭 신호)에 따라 동작할 수 있다. 다른 한편, 도 7c에 도시된 상기 제1 그룹의 PE들(110-1), 상기 제2 그룹의 PE들(110-2), 제3 그룹의 PE들 (110-3), 제4 그룹의 PE들(110-4)가 인공신경망모델의 임의 레이어에 대한 연산을 수행 완료하면, 상기 클럭 분 배기로부터 각기 제공받은 클럭 신호에 따라 상기 제1 그룹의 PE들(110-1), 상기 제2 그룹의 PE들(110-2), 제3 그룹의 PE들(110-3), 제4 그룹의 PE들(110-4)은 각기 출력 특징맵을 출력할 수 있다. 구체적으로, 상기 제1 그룹의 PE들(110-1)은 제1 출력 특징맵을 상기 클럭 분배기로부터 제공받은 임의의 클럭 신호(예를 들어 상기 원 클럭 신호 또는 상기 제1 지연 클럭 신호)에 따라 출력할 수 있다. 마찬가지로 상기 제2 그룹의 PE들 (110-2)은 제2 출력 특징맵을 상기 클럭 분배기로부터 제공받은 임의의 클럭 신호(예를 들어 제2 지연 클 럭 신호)에 따라 출력할 수 있다. 또한, 상기 제3 그룹의 PE들(110-3)은 제3 출력 특징맵을 상기 클럭 분배기 로부터 제공받은 임의의 클럭 신호(예를 들어 제3 지연 클럭 신호)에 따라 출력할 수 있다. 또한, 상기 제 4 그룹의 PE들(110-4)은 제4 출력 특징맵을 상기 클럭 분배기로부터 제공받은 임의의 클럭 신호(예를 들어 제4 지연 클럭 신호)에 따라 출력할 수 있다. 상기 제1 그룹의 PE들(110-1)에 의해서 출력되는 제1 출력 특징맵, 상기 제2 그룹의 PE들(110-2)에 의해서 출력 되는 제2 출력 특징맵, 상기 제3 그룹의 PE들(110-3)에 의해서 출력되는 제3 출력 특징맵 그리고 상기 제4 그룹 의 PE들(110-4)에 의해서 출력되는 제4 출력 특징맵이 서로 의존적이지 않고 독립적인 경우, 상기 SFU는 FIFO(First Input First Output) 방식으로 먼저 전달받은 출력 특징맵을 먼저 처리할 수 있다. 한편, 도 7b에 도시된 상기 제1 그룹의 PE들(110-1), 상기 제2 그룹의 PE들(110-2), 상기 제3 그룹의 PE들 (110-3), 상기 제4 그룹의 PE들(110-3)이 인공신경망모델의 임의 레이어에 대한 연산을 수행 완료하면, 상기 제 1 그룹의 PE들(110-1), 상기 제2 그룹의 PE들(110-2), 상기 제3 그룹의 PE들(110-3), 상기 제4 그룹의 PE들 (110-4)은 연산 결과, 즉 출력 특징맵을 상기 클럭의 제1 부분, 제2 부분, 제3 부분 그리고 제4 부분 중에서 어 느 하나의 부분에 맞추어서 출력할 수 있다. 이를 위하여, 상기 제1 그룹의 PE들(110-1), 상기 제2 그룹의 PE들(110-2), 상기 제3 그룹의 PE들(110-3) 그리고 상기 제4 그룹의 PE들(110-4) 중 적어도 하나 이상은 시간 지연 버퍼(예컨대, 쉬프트 레지스터)를 더 포함할 수 있다. 상기 시간 지연 버퍼(예컨대, 쉬프트 레지스터)는 상기 출력이 상기 클럭의 제1 부분, 제2 부분, 제3 부분 그리고 제4 부분 중에서 시간적으로 더 늦은 부분에 맞추어 서 수행되게끔, 시간 지연을 수행할 수 있다. 예를 들어 상기 제2 그룹의 PE들(110-2)의 출력 포트는 시간 지연 버퍼(예컨대, 쉬프트 레지스터)에 연결될 수 있다. 마찬가지로, 상기 제3 그룹의 PE들(110-3)의 출력 포트는 시간 지연 버퍼(예컨대, 쉬프트 레지스터)에 연 결될 수 있다. 그리고 상기 제4 그룹의 PE들(110-4)의 출력 포트도 시간 지연 버퍼(예컨대, 쉬프트 레지스터)에 연결될 수 있다 대안적으로, 도 7c에 도시된 상기 제1 그룹의 PE들(110-1)은 제1 출력 특징맵을 임의 클럭의 제1 부분에 맞추어 서 출력하고, 상기 제2 그룹의 PE들(110-2)은 제2 출력 특징맵을 상기 임의 클럭의 제2 부분에 맞추어서 출력할 수 있다. 또한, 상기 제3 그룹의 PE들(110-3)은 제3 출력 특징맵을 상기 임의 클럭의 제3 부분에 맞추어서 출력 할 수 있다. 그리고 상기 제4 그룹의 PE들(110-4)은 제4 출력 특징맵을 상기 임의 클럭의 제4 부분에 맞추어서 출력할 수 있다. 이 경우, 상기 SFU가 시간 지연 버퍼(예컨대, 쉬프트 레지스터)를 포함할 수 있다. 상기 SFU는 상기 임의 클럭의 제1 부분, 제2 부분, 제3 부분 그리고 제4 부분 중에서 시간적으로 더 빠른 부분 에 맞추어서 전달받은 출력 특징맵을 시간 지연시킴으로써, 상기 임의 클럭의 제1 부분, 제2 부분, 제3 부분 그 리고 제4 부분 중에서 시간적으로 늦은 부분에 맞추어서 전달받게 될 출력 특징맵과 시간이 동기 되도록 할 수 있다. 대안적으로, 상기 제1 그룹의 PE들(110-1)에 의해서 출력되는 제1 출력 특징맵과 상기 제2 그룹의 PE들(110- 2)에 의해서 출력되는 제2 출력 특징맵, 상기 제2 그룹의 PE들(110-3)에 의해서 출력되는 제3 출력 특징맵 그리 고 상기 제4 그룹의 PE들(110-4)에 의해서 출력되는 제4 출력 특징맵이 서로 의존적이지 않고 독립적인 경우, 상기 SFU는 상기 클럭의 제1 부분, 제2 부분, 제3 부분 그리고 제4 부분 중에서 시간적으로 더 빠른 부분 에 맞추어서 전달받은 출력 특징맵을 먼저 처리할 수 있다. 즉, 상기 제1 그룹의 PE들(110-1)로부터의 제1 출력과, 상기 제2 그룹의 PE들(110-2)로부터의 제2 출력, 상기 제3 그룹의 PE들(110-3)로부터의 제3 출력 그리고 상기 제4 그룹의 PE들(110-4)로부터의 제4 출력은 상기 임의 클럭의 제1 부분을 기준으로 전달될 수 있다. 상기 제2 그룹의 PE들(110-2)을 위한 기준 위상은 상기 클럭의 제2 부분에서 제1 부분으로 변환될 수 있다. 마 찬가지로, 상기 제3 그룹의 PE들(110-3)을 위한 기준 위상은 상기 클럭의 제3 부분에서 제1 부분으로 변환될 수 있다. 그리고 상기 제4 그룹의 PE들(110-4)을 위한 기준 위상은 상기 클럭의 제4 부분에서 제1 부분으로 변환될 수 있다. 도 7c에 도시된 상기 NPU 내의 각 엘리먼트에는 공급 전압(VDD)이 입력될 수 있다. 도 7c에서는 공통된 공급 전압(VDD)이 입력되는 것으로 도시 되었으나, 본 개시의 예시들은 이에 제한되지 않는 다. 몇몇 예시들에서는, 상기 NPU 내의 각 엘리먼트에는 독립되거나 혹은 분리된 공급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 다른 예시들에서는, 상기 NPU 내의 각 엘리먼트에는 독립되거나 혹은 분리된 공급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 또 다른 예시들에서는, 상기 NPU 내의 특정 엘리먼트에 제1 공급 전압과 상이한 제2 공급 전압이 입력되도 록 구성되는 것도 가능하다. 이상에서 설명한 바와 같이, 본 개시의 일 예시는 도 3 또는 도 5에 도시된 복수의 PE들을 도 7c에 도시된 바와 같이 제1 그룹의 PE들(110-1), 제2 그룹의 PE들(110-2), 제3 그룹의 PE들(110-3) 그리고 제4 그룹의 PE들 (110-4)로 나눈 후, 그룹들을 서로 다른 클럭 신호에 따라 분산 동작하게 함으로써, 피크 파워를 낮출 수 있도 록 한다. 또한, 본 개시의 일 예시는 위상이 서로 다른 클럭 신호들을 복수개로 제공하기 위해서, 클럭 소스를 복수 개로 두지 않고, 단순히 클럭 분배기만을 추가함으로써, 제조 단가를 낮출 수 있도록 한다. 도 8a는 본 개시의 제2 예시에 따른 NPU의 구조를 타낸 예시도이다. 도 8a에서는 NPU가 제1 그룹의 PE(110-1)와 제2 그룹의 PE(110-2)와 내부 메모리 그리고 SFU를 포함하는 것으로 도시되어 있으나, 상기 NPU는 그 외에도 도 3 또는 도 5에 도시된 바와 같이 NPU 컨트롤 러 그리고 NPU 인터페이스를 더 포함할 수 있다. 상기 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-02), 상기 내부 메모리, NPU 컨트롤러, 상기 NPU 인 터페이스 그리고 SFU 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 도 8a를 참조하면, 상기 클럭 소스 그리고 상기 클럭 분배기는 NPU 내부가 아니라 외부에 위치 하는 것으로 도시되어 있다. 그 외에는 도 7a에 도시된 것과 동일하므로, 도 8a에 대해서는 별도로 설명하지 않 기로 하고, 도 7a를 참조하여 설명한 내용을 그대로 원용하기로 한다. 도 8b는 본 개시의 제2 예시의 제1 변형예에 따른 NPU의 구조를 타낸 예시도이다. 도 8b에서는 NPU가 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 제3 그룹의 PE(110-3), 내부 메모리 그리고 SFU를 포함하는 것으로 도시되어 있으나, 상기 NPU는 그 외에도 도 3 또는 도 5에 도시 된 바와 같이 NPU 컨트롤러 그리고 NPU 인터페이스를 더 포함할 수 있다. 상기 제1 그룹의 PE(110-1), 상기 제2 그룹의 PE(110-2), 상기 제3 그룹의 PE(110-3), 상기 내부 메모리, NPU 컨트롤러, 상기 NPU 인터페이스 그리고 SFU 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별 될 수 있다. 도 8b를 참조하면, 상기 클럭 소스 그리고 상기 클럭 분배기는 NPU 내부가 아니라 외부에 위치 하는 것으로 도시되어 있다. 그 외에는 도 7b에 도시된 것과 동일하므로, 도 8b에 대해서는 별도로 설명하지 않 기로 하고, 도 7b를 참조하여 설명한 내용을 그대로 원용하기로 한다. 도 8c는 본 개시의 제2 예시의 제2 변형예에 따른 NPU의 구조를 타낸 예시도이다. 도 8c에서는 NPU가 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 제3 그룹의 PE(110-3), 제4 그룹의 PE(110-4), 내부 메모리 그리고 SFU를 포함하는 것으로 도시되어 있으나, 상기 NPU는 그 외에도 도 3 또는 도 5에 도시된 바와 같이 NPU 컨트롤러 그리고 NPU 인터페이스를 더 포함할 수 있다. 상기 제1 그룹의 PE(110-1), 상기 제2 그룹의 PE(110-2), 상기 제3 그룹의 PE(110-3), 상기 제4 그룹의 PE(110-4), 상기 내부 메모리, NPU 컨트롤러, 상기 NPU 인터페이스 그리고 SFU 각각은 수 많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어 려울 수 있고, 동작에 의해서만 식별될 수 있다. 도 8c를 참조하면, 상기 클럭 소스 그리고 상기 클럭 분배기는 NPU 내부가 아니라 외부에 위치 하는 것으로 도시되어 있다. 그 외에는 도 7c에 도시된 것과 동일하므로, 도 8c에 대해서는 별도로 설명하지 않 기로 하고, 도 7c를 참조하여 설명한 내용을 그대로 원용하기로 한다. 지금까지는 NPU를 설명하였으나, 상기 NPU는 SoC 형태로 구현될 수도 있다. 도 9a는 본 개시의 제3 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 9a를 참조하면, 예시적인 SoC은 복수의 NPU들과 복수의 CPU(central processing unit)들, 복수의 메 모리들을 포함할 수 있다. 상기 복수의 NPU들은 예컨대 제1 NPU(100-1)과 제2 NPU(100-2)를 포함할 수 있다. 그 리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200-1)과 제2 CPU(200-2)를 포함할 수 있다. 상기 복수의 메모리들 은 제1 메모리(300-1)와 제2 메모리(300-2)를 포함할 수 있다. 상기 복수의 NPU들 각각(예컨대 제1 NPU(100-1)과 제2 NPU(100-2))은 도 8a 내지 도 8c에 도시된 바와 같이 복 수 그룹의 PE들을 포함할 수 있다. 도 9a에서는 상기 복수의 NPU들, 복수의 CPU들 그리고 복수의 메모리들의 개수가 각기 2개인 것으로 도시되었지 만, 이에 한정되는 것은 아니고 개수는 4개, 6개, 8개 등으로 다양하게 변형될 수 있다. 상기 예시적인 SoC는 메모리 컨트롤러와, 클럭 소스와, 상기 클럭 분배기와, 시스템 버스 와 그리고 I/O(input output) 인터페이스를 포함할 수 있다.상기 시스템 버스는 반도체 다이(die) 위에 형성된 전기 전도성 패턴(electrically conductive pattern) 에 의해서 구현될 수 있다. 상기 시스템 버스는 고속 통신을 가능하게 한다. 예를 들어, 상기 복수의 NPU들, 상 기 복수의 CPU들, 상기 복수의 메모리들 그리고 상기 메모리 컨트롤러은 상기 시스템 버스를 통하여 서로 통신할 수 있다. 상기 복수의 NPU들과 그리고 상기 복수의 CPU들은 상기 시스템 버스를 통하여 상기 메모리 컨트롤러 에 요청한다. 이에 메모리 컨트롤러는 상기 복수의 메모리들 중 적어도 하나 이상으로부터 데이터를 읽어 내거나 혹은 기록할 수 있다. 도 9a에 도시된 상기 클럭 소스와 그리고 상기 클럭 분배기는 도 7a 내지 도 8c에 도시된 것과 동일 하다. 따라서, 상기 클럭 분배기는 상기 클럭 소스로부터의 상기 원 클럭 신호를 상기 제1 NPU(100-1)에 제 공할 수 있고, 아울러 상기 원 클럭 신호의 위상이 지연된 제1 위상 지연 클럭 신호를 상기 제2 NPU(100-2)에게 제공할 수 있다. 또는, 상기 클럭 분배기는 상기 클럭 소스로부터 제공된 상기 원 클럭 신호의 위상 을 지연시킨 제1 위상 지연 클럭 신호를 상기 제1 NPU(100-1)에 제공할 수 있고, 아울러 제2 위상 지연 클럭 신 호를 상기 제2 NPU(100-2)에게 제공할 수 있다. 도 9b는 본 개시의 제3 예시의 제1 변형예에 따른 SoC의 구조를 나타낸 예시도이다. 도 9b를 참조하면, 예시적인 SoC은 복수의 NPU들과 복수의 CPU(central processing unit)들, 복수의 메 모리들을 포함할 수 있다. 상기 복수의 NPU들은 예컨대 제1 NPU(100-1), 제2 NPU(100-2), 제3 NPU(100-3)를 포 함할 수 있다. 그리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200-1)과 제2 CPU(200-2)를 포함할 수 있다. 상기 복수의 메모리들은 제1 메모리(300-1)와 제2 메모리(300-2)를 포함할 수 있다. 상기 복수의 NPU들 각각(예컨대 100-1, 100-2, 100-3)은 도 3 또는 도 5에 도시된 복수의 PE들(PEs), NPU 내부 메모리, NPU 컨트롤러, NPU 인터페이스 그리고 SFU를 포함할 수 있다. 상기 클럭 소스가 원 클럭 신호를 생성하여 출력하면, 상기 상기 클럭 분배기는 원 클럭 신호를 그대 로(bypass)하여 출력하기도 하고 또는 상기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)하여 생성한 복수의 위상 지연 신호들을 출력할 수 있다. 예를 들어, 상기 클럭 분배기는 상기 클럭 소스로부터의 상기 원 클럭 신호를 상기 제1 NPU(100-1)에 제공할 수 있고, 아울러 상기 원 클럭 신호의 위상이 지연된 제1 위상 지연 클럭 신호를 상기 제2 NPU(100-2)에 게 제공할 수 있다. 그리고, 상기 클럭 분배기는 제2 위상 지연 클럭 신호를 상기 제3 NPU(100-3)에게 제 공할 수 있다. 또는, 상기 클럭 분배기는 상기 클럭 소스로부터의 상기 원 클럭 신호의 위상이 지연된 제1 위상 지 연 클럭 신호를 상기 제1 NPU(100-1)에 제공할 수 있고, 아울러 제2 위상 지연 클럭 신호를 상기 제2 NPU(100- 2)에게 제공할 수 있다. 그리고, 상기 클럭 분배기는 제3 위상 지연 클럭 신호를 상기 제3 NPU(100-3)에게 제공할 수 있다. 도 9c는 본 개시의 제3 예시의 제2 변형예에 따른 SoC의 구조를 나타낸 예시도이다. 도 9c를 참조하면, 예시적인 SoC은 복수의 NPU들과 복수의 CPU(central processing unit)들, 복수의 메 모리들을 포함할 수 있다. 상기 복수의 NPU들은 예컨대 제1 NPU(100-1), 제2 NPU(100-2), 제3 NPU(100-3), 제4 NPU(100-4)를 포함할 수 있다. 그리고, 상기 복수의 CPU들은 예컨대 제1 CPU(200-1)과 제2 CPU(200-2)를 포함 할 수 있다. 상기 복수의 메모리들은 제1 메모리(300-1)와 제2 메모리(300-2)를 포함할 수 있다. 상기 복수의 NPU들 각각(예컨대 100-1, 100-2, 100-3, 100-4)은 도 3 또는 도 5에 도시된 복수의 PE들 (PEs), NPU 내부 메모리, NPU 컨트롤러, NPU 인터페이스 그리고 SFU를 포함할 수 있 다. 상기 클럭 소스가 원 클럭 신호를 생성하여 출력하면, 상기 클럭 분배기는 원 클럭 신호를 그대로 (bypass)하여 출력하기도 하고 또는 상기 원 클럭 신호의 위상을 지연(delay) 또는 쉬프트(shift)하여 생성한 복수의 위상 지연 신호들을 출력할 수 있다. 예를 들어, 상기 클럭 분배기는 상기 클럭 소스로부터의 상기 원 클럭 신호를 상기 제1 NPU(100-1)에 제공할 수 있고, 아울러 제1 위상 지연 클럭 신호를 상기 제2 NPU(100-2)에게 제공할 수 있다. 그리고, 상기 클럭 분배기는 제2 위상 지연 클럭 신호를 상기 제3 NPU(100-3)에게 제공하고, 제3 위상 지연 클럭 신호를 상기 제4 NPU(100-4)에게 제공할 수 있다. 또는, 상기 클럭 분배기는 제1 위상 지연 클럭 신호를 상기 제1 NPU(100-1)에 제공할 수 있고, 아울러 제2 위상 지연 클럭 신호를 상기 제2 NPU(100-2)에게 제공할 수 있다. 그리고, 상기 클럭 분배기는 제3 위상 지연 클럭 신호를 상기 제3 NPU(100-3)에게 제공하고, 제4 위상 지연 클럭 신호를 상기 제4 NPU(100-4)에게 제 공할 수 있다. 도 9a 내지 도 9c에 도시된 상기 SoC 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에는 공급 전압(VDD)이 입력될 수 있다. 도 9a 내지 도 9c에서는 공통된 공급 전압(VDD)이 입력되는 것으로 도시 되었으나, 본 개시의 예시들은 이에 제 한되지 않는다. 몇몇 예시들에서는, 상기 SoC 내의 각 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 공급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 다른 예시들에서는, 상기 SoC 내의 일부 엘리먼트(즉, NPU, 메모리 및 CPU)에 독립되거나 혹은 분리된 공 급 전압(VDD)이 입력되도록 구성되는 것도 가능하다. 또 다른 예시들에서는, 상기 SoC 내의 특정 엘리먼트에 제1 공급 전압과 상이한 제2 공급 전압이 입력되 도록 구성되는 것도 가능하다. 이상에서 설명한 바와 같이, 본 개시의 제3 예시는 복수의 NPU들이 서로 다른 클럭 신호에 따라 분산 동작하게 함으로써, 피크 파워를 낮출 수 있도록 한다. 또한, 본 개시의 일 예시는 위상이 서로 다른 클럭 신호들을 복수개로 제공하기 위해서, 클럭 소스를 복수 개로 두지 않고, 단순히 클럭 분배기만을 추가함으로써, 제조 단가를 낮출 수 있도록 한다. 도 10a는 본 개시의 제4 예시에 따른 시스템의 구조를 나타낸 예시도이다. 이하, 도 9a의 설명과 다른 내용만을 설명하기로 하고, 동일한 내용은 전술한 내용을 원용하기로 한다. 먼저, 도 10a에서는 SoC가 아닌 보드 기판(board substrate) 상에 시스템 버스 역할을 하는 전기 전도성 패턴(electrically conductive pattern)이 형성될 수 있다. 그리고, 보드 기판(board substrate) 상에 제1 NPU 을 위해 마련된 제1 반도체 칩(100-1), 제2 NPU을 위해 마련된 제2 반도체 칩(100-2)가 장착될 수 있다. 또한, 상기 보드 기판 상에 제1 메모리(300-1)를 위한 반도체 칩과, 제2 메모리(300-2)를 위한 반도체 칩이 장착될 수 있다. 또한, 상기 보드 기판 상에, 메모리 컨트롤러가 장착될 수 있다. 또한, 상기 보드 기판 상에 제1 CPU(200-1)를 위한 반도체 칩과 제2 CPU(200-2)를 위한 반도체 칩이 장착될 수 있다. 또한, 상기 보드 기판 상 에 I/O 인터페이스를 위한 반도체 칩이 장착될 수 있다. 상기 보드 기판은 PCB(Printed circuit board)일 수 있다. 도 10a에 도시된 상기 클럭 소스와 상기 클럭 분배기는 도 7a 내지 도 8c에 도시된 것과 동일하다. 도 10b는 본 개시의 제4 예시의 제1 변형예에 따른 시스템의 구조를 나타낸 예시도이다. 이하, 도 9b의 설명과 다른 내용만을 설명하기로 하고, 동일한 내용은 전술한 내용을 원용하기로 한다. 먼저, 도 10b에서는 SoC가 아닌 보드 기판(board substrate) 상에 시스템 버스 역할을 하는 전기 전도성 패턴(electrically conductive pattern)이 형성될 수 있다. 그리고, 보드 기판(board substrate) 상에 제1 NPU 을 위해 마련된 제1 반도체 칩(100-1), 제2 NPU을 위해 마련된 제2 반도체 칩(100-2) 그리고 제3 NPU을 위해 마 련된 제3 반도체 칩(100-3)이 장착될 수 있다. 또한, 상기 보드 기판 상에 제1 메모리(300-1)를 위한 반도체 칩 과, 제2 메모리(300-2)를 위한 반도체 칩이 장착될 수 있다. 또한, 상기 보드 기판 상에, 메모리 컨트롤러(35 0)가 장착될 수 있다. 또한, 상기 보드 기판 상에 제1 CPU(200-1)를 위한 반도체 칩과 제2 CPU(200-2)를 위한 반도체 칩이 장착될 수 있다. 또한, 상기 보드 기판 상에 I/O 인터페이스를 위한 반도체 칩이 장착될 수 있다. 상기 보드 기판은 PCB(Printed circuit board)일 수 있다. 도 10b에 도시된 상기 클럭 소스와 상기 클럭 분배기는 도 7a 내지 도 8c에 도시된 것과 동일하다. 도 10c는 본 개시의 제4 예시의 제2 변형예에 따른 시스템의 구조를 나타낸 예시도이다. 이하, 도 9c의 설명과 다른 내용만을 설명하기로 하고, 동일한 내용은 전술한 내용을 원용하기로 한다. 먼저, 도 10c에서는 SoC가 아닌 보드 기판(board substrate) 상에 시스템 버스 역할을 하는 전기 전도성 패턴(electrically conductive pattern)이 형성될 수 있다. 그리고, 보드 기판(board substrate) 상에 제1 NPU 을 위해 마련된 제1 반도체 칩(100-1), 제2 NPU을 위해 마련된 제2 반도체 칩(100-2), 제3 NPU을 위해 마련된 제3 반도체 칩(100-3) 그리고 제4 NPU을 위해 마련된 제4 반도체 칩(100-4)이 장착될 수 있다. 또한, 상기 보드 기판 상에 제1 메모리(300-1)를 위한 반도체 칩과, 제2 메모리(300-2)를 위한 반도체 칩이 장착될 수 있다. 또 한, 상기 보드 기판 상에, 메모리 컨트롤러가 장착될 수 있다. 또한, 상기 보드 기판 상에 제1 CPU(200- 1)를 위한 반도체 칩과 제2 CPU(200-2)를 위한 반도체 칩이 장착될 수 있다. 또한, 상기 보드 기판 상에 I/O 인 터페이스를 위한 반도체 칩이 장착될 수 있다. 상기 보드 기판은 PCB(Printed circuit board)일 수 있다. 도 10c에 도시된 상기 클럭 소스와 상기 클럭 분배기는 도 7a 내지 도 8c에 도시된 것과 동일하다. 도 11a은 클럭 분배기의 제1 예시이다. 도 11a를 참고하면, 클럭 소스와 연결된 클럭 분배기는 분배 회로와 복수의 출력 회로(192a, 192b, 192c)를 포함할 수 있다. 상기 클럭 분배기는 상기 클럭 소스로부터 원 클럭 신호를 제공받아, 상기 원 클럭 신호를 NPU 또는 PE들의 그룹의 개수에 대응되는 복수개의 클럭 신호를 생성하여 출력할 수 있다. 상기 복수의 NPU 또는 PE들의 그룹들(NPU_0, NPU_1, NPU_2) 각각은 상기 클럭 분배기로부터 서로 다른 위상을 가진 클럭 신호를 제공받 을 수 있다. 상기 클럭 분배기는 후술될 분할 회로(미도시)를 더 포함할 수 있다. 따라서 각각의 복수의 NPU 또는 PE들의 그룹들(NPU_0, NPU_1, NPU_2)은 서로 다른 위상의 클럭을 기초로 동작함에 따라, 복수의 NPU 또는 PE들의 그룹들(NPU_0, NPU_1, NPU_2)을 포함하는 반도체칩 및/또는 상기 반도체칩을 포함하는 시스템의 피 크 파워가 저감될 수 있다. 도 11b는 클럭 분배기의 제2 예시이다. 도 11b를 참고하면, 클럭 소스와 연결된 클럭 분배기는 복수의 플리-플롭(193a, 193b)과 다중화기 와 그리고 분할 회로(예컨대, Div3)를 포함할 수 있다. 도 11b에서는 상기 복수의 플리-플롭(193a, 193b)은 예시적으로 2개인 것으로 도시되었으나, NPU의 개수 또는 PE 그룹의 개수에 따라 달라질 수 있다. 예를 들어, NPU의 개수가 2개일 경우 또는 PE 그룹의 개수가 2개 일 경 우, 상기 플리-플롭의 개수는 1개일 수 있다. 다른 예를 들어, NPU의 개수가 4개일 경우 또는 PE 그룹의 개수가 4개 일 경우, 상기 플리-플롭의 개수는 3개일 수 있다. 상기 복수의 플리-플롭(193a, 193b)은 도 11b에서는 예시적으로 D형 플리-플롭인 것으로 도시되어 있다. 상기 D 형 플리-플롭은 지연 소자로서 역할을 수행한다. D형 플리-플롭은 입력 신호를 다음 활성 클럭이 나타낼 때까지 지연(예컨대, 위상이 180도 지연됨)시킨 후 출력한다. 따라서, D형 플리-플롭은 지연 버퍼로서 사용될 수 있다. 구체적으로, 상기 복수의 플리-플롭들(193a, 193b) 중에서 제1 플리-플롭(193a)은 상기 클럭 소스로부터의 원 클릭 신호를 제공받으면, 상기 원 클럭 신호의 위상을 지연(예컨대, 위상이 180도 지연됨)시킨 후, 출력할 수 있다. 상기 제1 플리-플롭(193a)의 출력은 상기 다중화기와 제2 플리-플롭(193b)로 전달된다. 상기 제2 플리-플롭(193b)은 상기 제1 플리-플롭(193a)과 연결되어, 상기 제1 플리-플롭(193a)으로부터 출력되 는 클럭 신호의 위상을 지연(예컨대, 위상이 180도 지연됨)시킨 후, 출력할 수 있다. 상기 제2 플리-플롭(193 b)의 출력은 상기 다중화기로 전달된다. 상기 다중화기는 상기 클럭 소스로부터의 원 클럭 소스 그리고 상기 복수의 플리-플롭들(193a, 193b) 각각의 출력을 다중화한 후, 상기 분할 회로(예컨대, Div3)로 전달할 수 있다. 상기 분할 회로는 도 11b에서는 3 분할을 위한 Div3로 도시되어 있으나, 이에 한정되는 것은 아니며, NPU 의 개수 또는 PE 그룹의 개수에 따라 달라질 수 있다. 예를 들어, NPU의 개수가 2개일 경우 또는 PE 그룹의 개 수가 2개 일 경우, 상기 분할 회로는 3 분할을 위한 Div2일 수 있다. 다른 예를 들어, NPU의 개수가 4개일 경우 또는 PE 그룹의 개수가 4개 일 경우, 상기 분할 회로는 4 분할을 위한 Div4일 수 있다. 상기 분할 회로의 동작을 설명하기 위하여, 도 11f를 잠시 참조하여 설명하면, 상기 분할 회로는 상 기 다중화기로부터의 출력 신호에서 NPU_0 신호, NPU_1 신호, NPU_2 신호를 분할해낸 후, 각각의 NPU 또는각 PE 그룹에게 전달할 수 있다. 도 11b에 도시된 클럭 분배기의 구조에 따른 장점은 전문가에 의한 별도의 세팅이 필요치 않고 항상 정확 하게 위상을 분리할 수 있다는 점이다. 반면에 단점은 클럭 소스 내의 PLL(Phase Locked Loop; 위상동기회 로)에서 높은 주파수의 출력이 필요하다는 점이다. 도 11c는 클럭 분배기의 제3 예시이다. 도 11c를 참고하여 알 수 있는 바와 같이, 클럭 소스와 연결된 클럭 분배기는 복수의 지연셀들(196a, 196b, 196c,쪋 196n)과 다중화기를 포함할 수 있다. 상기 복수의 지연셀들(196a, 196b, 196c,쪋 196n)은 서로 체인 형태로 직렬로 연결되어 있고, 아울러 상기 다중 화기와는 병렬로 연결되어 있다. 상기 복수의 지연셀(196a, 196b, 196c,쪋 196n) 각각은 클럭 신호의 위상 을 미리 정해진 오프셋 만큼 지연시킨 후 출력할 수 있다. 상기 다중화기는 상기 클럭 소스로부터의 출력단과 상기 복수의 지연셀들(196a, 196b, 196c,쪋 196 n)의 출력단들과 연결되어 있다. 따라서, 상기 다중화기는 상기 클럭 소스로부터의 원 클럭 신호 그 리고 상기 복수의 지연셀들(196a, 196b, 196c,쪋 196n)로부터 출력 되는 클럭 신호를 다중화하여 출력할 수 있 다. 본 개시의 예시들에서 상기 지연셀은 딜레이 버퍼로 지칭되는 것도 가능하다. 도 11c에 도시된 클럭 분배기의 구조에 따른 장점은 위상 지연의 자유도가 높고, 클럭 소스 내의 PLL 에서 낮은 주파수로 출력해도 된다는 점이다. 반면에 단점은 주파수 마다 적절한 지연 값을 선택해야 한다는 점 이다. 도 11d는 클럭 분배기의 제3 예시이다. 도 11d를 참고하여 알 수 있는 바와 같이, 클럭 소스와 연결된 클럭 분배기는 복수의 플리-플롭 (193a, 193b)과 분할 회로(예컨대, Div3)를 포함할 수 있다. 클럭 분배기는 복수의 클럭 버퍼(197a, 197b, 197c) 중 적어도 하나를 선택적으로 더 포함할 수 있다. 도 11a에서는 상기 복수의 플리-플롭(193a, 193b)은 예시적으로 2개인 것으로 도시되었으나, NPU의 개수 또는 PE 그룹의 개수에 따라 달라질 수 있다. 예를 들어, NPU의 개수가 2개일 경우 또는 PE 그룹의 개수가 2개 일 경 우, 상기 플리-플롭의 개수는 1개일 수 있다. 다른 예를 들어, NPU의 개수가 4개일 경우 또는 PE 그룹의 개수가 4개 일 경우, 상기 플리-플롭의 개수는 3개일 수 있다. 상기 복수의 플리-플롭(193a, 193b)은 도 11d에서는 예시적으로 D형 플리-플롭인 것으로 도시되어 있다. 상기 D 형 플리-플롭은 지연 소자로서 역할을 수행한다. D형 플리-플롭은 입력 신호를 다음 활성 클럭이 나타낼 때까지 지연(예컨대, 위상이 180도 지연됨)시킨 후 출력한다. 따라서, D형 플리-플롭은 지연 버퍼로서 사용될 수 있다. 상기 분할 회로는 도 11d에서는 3 분할을 위한 Div3로 도시되어 있으나, 이에 한정되는 것은 아니며, NPU 의 개수 또는 PE 그룹의 개수에 따라 달라질 수 있다. 예를 들어, NPU의 개수가 2개일 경우 또는 PE 그룹의 개 수가 2개 일 경우, 상기 분할 회로는 2 분할을 위한 Div2일 수 있다. 다른 예를 들어, NPU의 개수가 4개일 경우 또는 PE 그룹의 개수가 4개 일 경우, 상기 분할 회로는 4 분할을 위한 Div4일 수 있다. 상기 클럭 분배기는 선택적으로 클럭 버퍼를 더 포함하도록 구성될 수 있다. 단, 본 개시의 예시는 클럭 버퍼에 제한되지 않는다. 도 11d에서는 복수의 클럭 버퍼(197a, 197b, 197c)가 도시되어 있으나, 클럭 버퍼는 선택적으로 더 추가될 수 있으며, 제거되는 것도 가능하다. 클럭 버퍼는 하나의 클럭 신호의 위상을 특정 위상으로 딜레이 시키도록 구성 될 수 있다. 상기 복수의 클럭 버퍼(197a, 197b, 197c)는 도 11d에서는 예시적으로 3개인 것으로 도시되었으나, NPU의 개수 또는 PE 그룹의 개수에 따라 달라질 수 있다. 예를 들어, NPU의 개수가 2개일 경우 또는 PE 그룹의 개수가 2개 일 경우, 상기 복수의 클럭 버퍼의 개수는 2개일 수 있다. 다른 예를 들어, NPU의 개수가 4개일 경우 또는 PE 그룹의 개수가 4개 일 경우, 상기 복수의 클럭 버퍼의 개수는 4개일 수 있다. 예를 들면, 도 11d에 도시된 클럭 분배기에서 각각의 클럭 신호가 120°의 위상차를 가지는 3개의 클럭 신 호가 출력되도록 구성된다. 하지만, 각각의 클럭 신호를 입력 받는 각각의 NPU 또는 PE 그룹과 클럭 분배기 사이의 물리적인 거리는 상이할 수 있다. 만약 특정 NPU 또는 PE 그룹의 물리적 거리가 다른 NPU 또는 다 른 PE 그룹 대비 특별히 먼 경우, 클럭 분배기에서 120°의 위상차가 있도록 출력되는 각각의 클럭 신호들 중 일부의 위상이 실질적으로 비슷해 지거나 또는 겹칠 수 있다. 이러한 경우 클럭 버퍼가 특정 클럭 신호가 공 급되는 신호 라인에 배치되어, 물리적인 거리에 따른 위상 변화를 보상하도록 구성될 수 있다. 즉, 복수의 클럭 신호들 중 적어도 하나는 피크 파워 저감을 고려하여 배치된 적어도 하나의 출력 버퍼를 지나서 상기 복수의 PE 그룹들 중 하나에 공급되도록 구성될 수 있다. 즉, 적어도 하나의 클럭 버퍼는 설계되는 반도체 칩의 물리적인 레이아웃 또는 신호 딜레이를 고려하여 적절히 설정될 수 있다. 도 11e는 클럭 분배기로부터의 예시적인 신호 파형을 나타낸 예시도이다. 도 11e를 참조하여 알 수 있는 바와 같이, 각 NPU 또는 각 PE 그룹에 전달되는 신호는 원 클럭 신호에 비하여 위상이 지연되어 있을 뿐만 아니라, 주파수도 더 낮은 것을 알 수 있다. 도 11e에서는, 각 NPU 또는 각 PE 그룹 에 전달되는 신호는 원 클럭 신호를 3 분할함으로써 생성된 것으로 도시되어 있다. 따라서, 각 NPU 또는 각 PE 그룹에 전달되는 신호는 원 클럭 신호 대비하여 주파수가 1/3 배로 낮다. 도 12는 본 개시의 일 예시에 따른 NPU의 동작 방법을 나타낸 예시도이다. 도 12를 참조하면, 복수의 PE들을 구동하기 위한 복수의 클럭 신호가 생성될 수 있다(S1210). 그리고, 상기 복수의 PE들은 제1 그룹의 PE들과 제2 그룹의 PE들로 분할될 수 있다(S1220). 각 그룹에 속한 PE 들의 개수는 달라질 수 있는 것은 자명할 것이다. 또한, 각 그룹에 속한 PE들의 개수는 반-고정적으로 혹은 동 적으로 변경될 수 있다. 이러한 변경은 NPU 컨트롤러의 제어에 따라 수행될 수 있다. 이어서, 제1 클럭 신호를 기준으로 상기 제1 그룹의 PE들이 구동될 수 있다(S1030). 또한, 제2 클럭 신호를 기 준으로 상기 제2 그룹의 PE들이 구동될 수 있다(S1040). 도 13은 서로 다른 위상의 클럭 신호를 NPU의 복수의 PE들에게 입력하지 않는 비교 예시를 도시한다. 도 13을 참조하면, 제1 클럭(CLK0)은 제1 그룹의 PE(110-1) 및 제2 그룹의 PE(110-2)에 입력될 수 있다. 여기 서 각각의 그룹의 PE들(110-1, 110-2)은 동일한 위상의 클럭 신호를 입력 받도록 구성된다. 도 13의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 13의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 13의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 13을 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워가 증가하고, 공급 전압(VDD)이 저하되는 경향이 도시되어 있다. 부연 설명하면, 각 클럭 사이클 마다 동작하는 PE들의 개수는 인공신경망모델을 컴파일하는 컴파일러의 성능에 따라 달라질 수 있다. 즉, 컴파일러의 성능이 우수할수록 단위 클럭마다 더 많은 PE들을 동작 시킬 수 있다. 여 기서 전체 PE들 중 동작하는 PE의 비율을 PE의 이용률(utilization rate %)로 지칭할 수 있다. 반대로, 컴파일 러의 성능이 우수할수록 NPU의 피크 파워가 더 증가할 수 있다. 즉, 피크 파워는 PE의 이용률과 정비례할 수 있 다. 따라서 컴파일러의 알고리즘이 정교해짐에 따라, NPU의 피크 파워는 더 증가될 수 있는 문제가 있다. 다시 도 13을 참조하면, 제1 클럭(CLK0)을 기준으로 동작하는 제1 그룹의 PE(110-1) 및 제2 그룹의 PE(110-2)의 피크 파워는 각 클럭당 동작하는 PE의 개수에 따라서 달라질 수 있다. 즉, 하나의 클럭에 동작하는 PE의 개수에 비례하여 피크 파워가 증가될 수 있다. 부연 설명하면, 저전력으로 개발된 NPU는 엣지 디바이스에 적용될 때 M.2 인터페이스 또는 PCI Express 인터페 이스에 대응되도록 개발될 수 있다. 특히 M.2 인터페이스의 경우 PCI Express 인터페이스에 비해서 최대 파워가 상대적으로 더 낮을 수 있다. 예를 들면 M.2 인터페이스의 경우 3A의 전류와 10W의 파워 제한을 가질 수 있다. 만약 비교예의 NPU의 피크 파워가 특정 클럭에서 10W를 초과할 경우, NPU에 공급되는 공급 전압(VDD)이 흔들릴 수 있다. 이때, PE의 이용률이 올라갈수록 NPU의 피크 파워는 더 증가되고, 피크 파워가 파워 제한을 초과할수록 공급 전압(VDD)의 저하 정도는 초과한 정보에 비례하여 더 커질 수 있다. 만약 공급 전압(VDD)이 IR-drop margin 이하로 떨어질 경우, NPU가 연산 중인 데이터에 오류가 발생할 수 있다. 부연 설명하면, 금속 상호 연결의 저항(R)과 이를 통해 흐르는 전류(I)로 인해 발생하며 옴의 법칙에 따라 전압 강하(V=IR)가 발생될 때 IR-drop이 발생할 수 있다. NPU의 복수의 PE와 관련된 많은 수의 회로가 동시에 스위칭 하면 NPU가 상당한 양의 전류를 소모할 수 있다. 이 높은 전류는 NPU에서 더 큰 전압 강하를 발생시켜 SoC의 다 른 부분에서 공급 전압을 감소시킬 수 있다. 도 13의 비교예를 정리하면, NPU의 안정적인 작동을 보장하고 잠재적인 오류를 방지하려면 충분한 IR-drop margin을 확보해야 한다. IR-drop margin은 공급 전압(VDD)과 피크 파워 조건 동안 NPU의 모든 지점에서 허용 가능한 최저 전압 간의 차이를 의미할 수 있다. 도 14a는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 NPU의 복수의 PE들에게 입력하는 예시를 도시한다. 프로세싱 엘리먼트의 이용률이 증가할수록 파워는 비례하여 증가할 수 있다는 점을 주목하여야 한다. 이에, 각 연산 단계별 PE들의 이용률 정보와 구동 주파수 정보를 기초로 각 연산 단계별 파워를 계산할 수 있다. 상기 파 워 계산은 특정 인공신경망모델의 스케줄링 정보를 기초로 계산될 수 있다. 여기서 파워(Watt)는 해당 연산 단 계에 소비되는 에너지를 단위 시간으로 나눈 값(i.e., 1 watt (W) = 1 joule per second (J/s))으로 계산될 수 있다. 따라서 각 연산 단계의 파워를 스케줄링 정보에 기초하여 계산할 수 있다. 도 14a를 참조하면, 제1 클럭(CLK1)은 제1 그룹의 PE(110-1)에 입력되고, 제2 클럭(CLK2)은 제2 그룹의 PE(110-2)에 입력될 수 있다. 여기서 각 그룹의 PE들(110-1, 110-2)은 서로 상이한 위상의 클럭 신호를 입력 받 도록 구성된다. 도 14a의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 14a의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 14a의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 14a를 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워 가 증가하고, 공급 전압(VDD)이 저하되는 경향이 도시되어 있다. 다만, 위상이 서로 다른 신호 클럭이 각 그룹 의 PE들에 입력됨에 따라 전체 피크 파워가 저감되고 및 공급 전압(VDD)의 출렁임도 저감되는 경향이 도시되어 있다. 여기서, 도 14a의 일 예시에 따른 평균 파워와 도 13의 비교예에 따른 평균 파워는 실질적으로 동일하다. 하지 만, 도 14a의 일 예시에 따른 피크 파워는 도 13의 비교예에 따른 피크 파워보다 작게 된다. 따라서, 도 14a의 일 예시에 따르면 도 13의 비교예보다, 피크 파워와 평균 파워의 비율이 개선될 수 있다. 부연 설명하면, 각 클럭 마다 동작하는 PE들의 개수는 인공신경망모델을 컴파일하는 컴파일러의 성능에 따라 달 라질 수 있다. 즉, 컴파일러의 성능이 우수할수록 단위 클럭마다 더 많은 PE들을 동작 시킬 수 있다. 여기서 전 체 PE들 중 동작하는 PE의 비율을 PE의 이용률(utilization rate %)로 지칭할 수 있다. 반대로, 컴파일러의 성 능이 우수할수록 NPU의 피크 파워가 더 증가할 수 있다. 즉, 피크 파워는 PE의 이용률과 정비례할 수 있다. 따 라서 컴파일러의 알고리즘이 정교해짐에 따라, NPU의 피크 파워는 더 증가될 수 있는 문제가 있다. 다시 도 14a를 참조하면, 제1 클럭(CLK1)에 대응하는 제1 그룹의 PE(110-1) 및 제2 그룹의 PE(110-2)의 피크 파워는 각 클럭당 동작하는 PE의 개수에 따라서 달라질 수 있다. 즉, 하나의 클럭에 동작하는 PE의 개수에 비례 하여 피크 파워가 증가될 수 있다. 부연 설명하면, 저전력으로 개발된 NPU는 엣지 디바이스에 적용될 때 M.2 인터페이스 또는 PCI Express 인터페 이스에 대응되도록 개발될 수 있다. 특히 M.2 인터페이스의 경우 PCI Express 인터페이스에 비해서 최대 파워가 상대적으로 더 낮을 수 있다. 예를 들면 M.2 인터페이스의 경우 3A의 전류와 10W의 파워 제한을 가질 수 있다. 만약 비교예의 NPU의 피크 파워가 특정 클럭에서 10W를 초과할 경우, NPU에 공급되는 공급 전압(VDD)이 흔들릴수 있다. 이때, PE의 이용률이 올라갈수록 NPU의 피크 파워는 더 증가되고, 피크 파워가 파워 제한을 증가할수 록 공급 전압(VDD)의 저하 정도는 더 커질 수 있다. 만약 공급 전압(VDD)이 IR-drop margin 이하로 떨어질 경우, NPU가 연산 중인 데이터에 오류가 발생될 수 있다. 한편, 제1 그룹의 PE(110-1)에는 제1 클럭(CLK1)이 인가되고, 제2 그룹의 PE(110-2)에는 제1 클럭(CLK1)과 위 상이 다른 제2 클럭(CLK2)이 인가될 수 있다. 따라서 각각의 그룹의 PE들의 피크 파워는 분산될 수 있으며, NPU 의 피크 파워 관점에서 피크 파워가 도 13의 비교예 대비 반으로 저감될 수 있다. 이러한 경우 공급 전압(VDD) 의 전압 안정성이 도 13에 도시된 비교예와 비교할 때 상대적으로 더 안정적일 수 있다. 즉, 본 개시의 일 예시에 따르면, NPU의 피크 파워가 저감될 수 있으며, NPU를 포함하는 SoC의 피크 파워 또한 저감될 수 있다. 또한 M.2와 같은 저전력 인터페이스의 경우, 파워 제한이 낮을 수 있기 때문에, 이러한 저전력 인터페이스에서 더욱 더 효과적일 수 있다. 하지만, 본 개시는 특정 인터페이스에 제한되지 않는다. 도 14b는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 NPU의 복수의 PE들에게 입력하는 예시를 도시한다. 프로세싱 엘리먼트의 이용률이 증가할수록 파워는 비례하여 증가할 수 있다는 점을 주목하여야 한다. 이에, 각 연산 단계별 PE들의 이용률 정보와 구동 주파수 정보를 기초로 각 연산 단계별 파워를 계산할 수 있다. 상기 파 워 계산은 특정 인공신경망모델의 스케줄링 정보를 기초로 계산될 수 있다. 여기서 파워(Watt)는 해당 연산 단 계에 소비되는 에너지를 단위 시간으로 나눈 값(i.e., 1 watt (W) = 1 joule per second (J/s))으로 계산될 수 있다. 따라서 각 연산 단계의 파워를 스케줄링 정보에 기초하여 계산할 수 있다. 도 14b를 참조하면, 제1 클럭(CLK1)은 제1 그룹의 PE(110-1)에 입력되고, 제2 클럭(CLK2)은 제2 그룹의 PE(110-2)에 입력되고, 제3 클럭(CLK3)은 제3 그룹의 PE(110-3)에 입력될 수 있다. 여기서 각 그룹의 PE들 (110-1, 110-2, 110-3)은 서로 상이한 위상의 클럭 신호를 입력 받도록 구성된다. 도 14b의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 14b의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 14b의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 14b를 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워 가 증가하고, 공급 전압(VDD)이 저하되는 경향이 도시되어 있다. 다만, 위상이 서로 다른 신호 클럭이 각 그룹 의 PE들에 입력됨에 따라 전체 피크 파워가 저감되고 및 공급 전압(VDD)의 출렁임도 저감되는 경향이 도시되어 있다. 여기서, 도 14b의 일 예시에 따른 평균 파워와 도 13의 비교예에 따른 평균 파워는 실질적으로 동일하다. 하지 만, 도 14b의 일 예시에 따른 피크 파워는 도 13의 비교예에 따른 피크 파워보다 작게 된다. 따라서, 도 14b의 일 예시에 따르면 도 13의 비교예보다, 피크 파워와 평균 파워의 비율이 개선될 수 있다. 부연 설명하면, 각 클럭 마다 동작하는 PE들의 개수는 인공신경망모델을 컴파일하는 컴파일러의 성능에 따라 달 라질 수 있다. 즉, 컴파일러의 성능이 우수할수록 단위 클럭마다 더 많은 PE들을 동작 시킬 수 있다. 여기서 전 체 PE들 중 동작하는 PE의 비율을 PE의 이용률(utilization rate %)로 지칭할 수 있다. 반대로, 컴파일러의 성 능이 우수할수록 NPU의 피크 파워가 더 증가할 수 있다. 즉, 피크 파워는 PE의 이용률과 정비례할 수 있다. 따 라서 컴파일러의 알고리즘이 정교해짐에 따라, NPU의 피크 파워는 더 증가될 수 있는 문제가 있다. 다시 도 14b를 참조하면, 제1 클럭(CLK1)에 대응하는 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2) 및 제3 그 룹의 PE(110-3)의 피크 파워는 각 클럭당 동작하는 PE의 개수에 따라서 달라질 수 있다. 즉, 하나의 클럭에 동 작하는 PE의 개수에 비례하여 피크 파워가 증가될 수 있다. 부연 설명하면, 저전력으로 개발된 NPU는 엣지 디바이스에 적용될 때 M.2 인터페이스 또는 PCI Express 인터페 이스에 대응되도록 개발될 수 있다. 특히 M.2 인터페이스의 경우 PCI Express 인터페이스에 비해서 최대 파워가 상대적으로 더 낮을 수 있다. 예를 들면 M.2 인터페이스의 경우 3A의 전류와 10W의 파워 제한을 가질 수 있다. 만약 비교예의 NPU의 피크 파워가 특정 클럭에서 10W를 초과할 경우, NPU에 공급되는 공급 전압(VDD)이 흔들릴 수 있다. 이때, PE의 이용률이 올라갈수록 NPU의 피크 파워는 더 증가되고, 피크 파워가 파워 제한을 증가할수 록 공급 전압(VDD)의 저하 정도는 더 커질 수 있다. 만약 공급 전압(VDD)이 IR-drop margin 이하로 떨어질 경우, NPU가 연산 중인 데이터에 오류가 발생될 수 있다. 한편, 제1 그룹의 PE(110-1)에는 제1 클럭(CLK1)이 인가되고, 제2 그룹의 PE(110-2)에는 제1 클럭(CLK1)과 위 상이 다른 제2 클럭(CLK2)이 인가되고, 제3 그룹의 PE(110-3)에는 제1 클럭(CLK1) 및 제2 클럭(CLK2)과 위상이 다른 제3 클럭(CLK3)이 인가될 수 있다. 따라서 각각의 그룹의 PE들의 피크 파워는 분산될 수 있으며, 도 14a의 예시 대비 더 많은 그룹의 PE들이 동작하더라도 도 14a의 예시와 도 14b의 예시의 피크 파워는 동일할 수 있다. 즉, 본 개시의 일 예시에 따르면, NPU의 피크 파워가 저감될 수 있으며, NPU를 포함하는 SoC의 피크 파워 또한 저감될 수 있다. 또한 M.2와 같은 저전력 인터페이스의 경우, 파워 제한이 낮을 수 있기 때문에, 이러한 저전력 인터페이스에서 더욱 더 효과적일 수 있다. 하지만, 본 개시는 특정 인터페이스에 제한되지 않는다. 도 14c는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 NPU의 복수의 PE들에게 입력하는 예시를 도시한다. 프로세싱 엘리먼트의 이용률이 증가할수록 파워는 비례하여 증가할 수 있다는 점을 주목하여야 한다. 이에, 각 연산 단계별 PE들의 이용률 정보와 구동 주파수 정보를 기초로 각 연산 단계별 파워를 계산할 수 있다. 상기 파 워 계산은 특정 인공신경망모델의 스케줄링 정보를 기초로 계산될 수 있다. 여기서 파워(Watt)는 해당 연산 단 계에 소비되는 에너지를 단위 시간으로 나눈 값(i.e., 1 watt (W) = 1 joule per second (J/s))으로 계산될 수 있다. 따라서 각 연산 단계의 파워를 스케줄링 정보에 기초하여 계산할 수 있다. 도 14c를 참조하면, 제1 클럭(CLK1)은 제1 그룹의 PE(110-1)에 입력되고, 제2 클럭(CLK2)은 제2 그룹의 PE(110-2)에 입력되고, 제3 클럭(CLK3)은 제3 그룹의 PE(110-3)에 입력되고, 제4 클럭(CLK4)은 제4 그룹의 PE(110-4)에 입력될 수 있다. 여기서 각 그룹의 PE들(110-1, 110-2, 110-3, 110-4)은 서로 상이한 위상의 클럭 신호를 입력 받도록 구성된다. 도 14c의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 14c의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 14c의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 공급 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 14c를 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워 가 증가하고, 공급 전압(VDD)이 저하되는 경향이 도시되어 있다. 다만, 위상이 서로 다른 신호 클럭이 각 그룹 의 PE들에 입력됨에 따라 전체 피크 파워가 저감되고 및 공급 전압(VDD)의 출렁임도 저감되는 경향이 도시되어 있다. 여기서, 도 14c의 일 예시에 따른 평균 파워와 도 13의 비교예에 따른 평균 파워는 실질적으로 동일하다. 하지 만, 도 14c의 일 예시에 따른 피크 파워는 도 13의 비교예에 따른 피크 파워보다 작게 된다. 따라서, 도 14c의 일 예시에 따르면 도 13의 비교예보다, 피크 파워와 평균 파워의 비율이 개선될 수 있다. 부연 설명하면, 각 클럭 마다 동작하는 PE들의 개수는 인공신경망모델을 컴파일하는 컴파일러의 성능에 따라 달 라질 수 있다. 즉, 컴파일러의 성능이 우수할수록 단위 클럭마다 더 많은 PE들을 동작 시킬 수 있다. 여기서 전 체 PE들 중 동작하는 PE의 비율을 PE의 이용률(utilization rate %)로 지칭할 수 있다. 반대로, 컴파일러의 성 능이 우수할수록 NPU의 피크 파워가 더 증가할 수 있다. 즉, 피크 파워는 PE의 이용률과 정비례할 수 있다. 따 라서 컴파일러의 알고리즘이 정교해짐에 따라, NPU의 피크 파워는 더 증가될 수 있는 문제가 있다.다시 도 14c를 참조하면, 제1 클럭(CLK1)에 대응하는 제1 그룹의 PE(110-1), 제2 그룹의 PE(110-2), 제3 그룹 의 PE(110-3) 및 제4 그룹의 PE(110-4)의 피크 파워는 각 클럭당 동작하는 PE의 개수에 따라서 달라질 수 있다. 즉, 하나의 클럭에 동작하는 PE의 개수에 비례하여 피크 파워가 증가될 수 있다. 부연 설명하면, 저전력으로 개발된 NPU는 엣지 디바이스에 적용될 때 M.2 인터페이스 또는 PCI Express 인터페 이스에 대응되도록 개발될 수 있다. 특히 M.2 인터페이스의 경우 PCI Express 인터페이스에 비해서 최대 파워가 상대적으로 더 낮을 수 있다. 예를 들면 M.2 인터페이스의 경우 3A의 전류와 10W의 파워 제한을 가질 수 있다. 만약 비교예의 NPU의 피크 파워가 특정 클럭에서 10W를 초과할 경우, NPU에 공급되는 공급 전압(VDD)이 흔들릴 수 있다. 이때, PE의 이용률이 올라갈수록 NPU의 피크 파워는 더 증가되고, 피크 파워가 파워 제한을 증가할수 록 공급 전압(VDD)의 저하 정도는 더 커질 수 있다. 만약 공급 전압(VDD)이 IR-drop margin 이하로 떨어질 경우, NPU가 연산 중인 데이터에 오류가 발생될 수 있다. 한편, 제1 그룹의 PE(110-1)에는 제1 클럭(CLK1)이 인가되고, 제2 그룹의 PE(110-2)에는 제1 클럭(CLK1)과 위 상이 다른 제2 클럭(CLK2)이 인가되고, 제3 그룹의 PE(110-3)에는 제1 클럭(CLK1) 및 제2 클럭(CLK2)과 위상이 다른 제3 클럭(CLK3)이 인가되고, 제4 그룹의 PE(110-4)에는 제1 클럭(CLK1), 제2 클럭(CLK2), 및 제3 클럭 (CLK3)과 위상이 다른 제4 클럭(CLK4)이 인가될 수 있다. 따라서 각각의 그룹의 PE들의 피크 파워는 분산될 수 있으며, 도 14b의 예시 대비 더 많은 그룹의 PE들이 동작하더라도 도 14b의 예시와 도 14c의 예시의 피크 파워 는 동일할 수 있다. 즉, 본 개시의 일 예시에 따르면, NPU의 피크 파워가 저감될 수 있으며, NPU를 포함하는 SoC의 피크 파워 또한 저감될 수 있다. 또한 M.2와 같은 저전력 인터페이스의 경우, 파워 제한이 낮을 수 있기 때문에, 이러한 저전력 인터페이스에서 더욱 더 효과적일 수 있다. 하지만, 본 개시는 특정 인터페이스에 제한되지 않는다. 도 15는 서로 다른 위상의 클럭 신호를 복수의 NPU들에게 입력하지 않는 비교 예시를 도시한다. 도 15를 참조하면, 제1 클럭(CLK0)은 복수의 신경 프로세싱 유닛(100-1, 100-2, 100-3, 100-4)에 입력될 수 있 다. 여기서 복수의 신경 프로세싱 유닛(100-1, 100-2, 100-3, 100-4)은 동일한 위상의 클럭 신호를 입력 받도록 구성된다. 상기 복수의 신경 프로세싱 유닛은 상기 제1 NPU(100-1), 상기 제2 NPU(100-2), 상기 제3 NPU(100- 3), 상기 제4 NPU(100-4)를 포함할 수 있다. 도 15의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 15의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 15의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압(VDD) 변화 특성을 예시적으로 도시한다. 도 15를 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워가 증가하고, 구동 전압(VDD)이 저하되는 경향이 도시되어 있다. 부연 설명하면, 각 클럭 사이클 마다 동작하는 PE들의 개수는 인공신경망모델을 컴파일하는 컴파일러의 성능에 따라 달라질 수 있다. 즉, 컴파일러의 성능이 우수할수록 단위 클럭마다 더 많은 PE들을 동작 시킬 수 있다. 여 기서 전체 PE들 중 동작하는 PE의 비율을 PE의 가동률(utilization rate %)로 지칭할 수 있다. 반대로, 컴파일 러의 성능이 우수할수록 NPU의 피크 파워가 더 증가할 수 있다. 즉, 피크 파워는 PE의 가동률과 정비례할 수 있 다. 따라서 컴파일러의 알고리즘이 정교해짐에 따라, NPU의 피크 파워는 더 증가될 수 있는 문제가 있다. 다시 도 15를 참조하면, 제1 클럭(CLK0)을 기준으로 동작하는 상기 제1 NPU(100-1), 상기 제2 NPU(100-2), 상 기 제3 NPU(100-3), 상기 제4 NPU(100-4)의 피크 파워는 각 클럭당 동작하는 PE의 개수에 따라서 달라질 수 있 다. 즉, 하나의 클럭에 동작하는 PE의 개수에 비례하여 피크 파워가 증가될 수 있다. 부연 설명하면, 저전력으로 개발된 NPU는 엣지 디바이스에 적용될 때 M.2 인터페이스 또는 PCI Express 인터페 이스에 대응되도록 개발될 수 있다. 특히 M.2 인터페이스의 경우 PCI Express 인터페이스에 비해서 최대 파워가 상대적으로 더 낮을 수 있다. 예를 들면 M.2 인터페이스의 경우 3A의 전류와 10W의 파워 제한을 가질 수 있다. 만약 비교예의 NPU의 피크 파워가 특정 클럭에서 10W를 초과할 경우, NPU에 공급되는 공급 전압(VDD)이 흔들릴수 있다. 이때, PE의 가동률이 올라갈수록 NPU의 피크 파워는 더 증가되고, 피크 파워가 파워 제한을 초과할수 록 공급 전압(VDD)의 저하 정도는 초과한 정보에 비례하여 더 커질 수 있다. 만약 공급 전압(VDD)이 IR-drop margin 이하로 떨어질 경우, NPU가 연산 중인 데이터에 오류가 발생할 수 있다. 부연 설명하면, 금속 상호 연결의 저항(R)과 이를 통해 흐르는 전류(I)로 인해 발생하며 옴의 법칙에 따라 전압 강하(V=IR)가 발생될 때 IR-drop이 발생할 수 있다. NPU의 복수의 PE와 관련된 많은 수의 회로가 동시에 스위칭 하면 NPU가 상당한 양의 전류를 소모할 수 있다. 이 높은 전류는 NPU에서 더 큰 전압 강하를 발생시켜 SoC의 다 른 부분에서 공급 전압을 감소시킬 수 있다. 도 15의 비교예를 정리하면, NPU의 안정적인 작동을 보장하고 잠재적인 오류를 방지하려면 충분한 IR-drop margin을 확보해야 한다. IR-drop margin은 공급 전압(VDD)과 피크 파워 조건 동안 NPU의 모든 지점에서 허용 가능한 최저 전압 간의 차이를 의미할 수 있다. 도 16a는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 2개의 NPU들에게 입력 하는 예시를 도시한다. 도 16a를 참조하면, 제1 클럭(CLK1)은 상기 제1 NPU(100-1)에 입력되고, 제2 클럭(CLK2)은 상기 제2 NPU(100- 2)에 입력될 수 있다. 여기서 상기 제1 NPU(100-1) 및 상기 제2 NPU(100-2)는 서로 상이한 위상의 클럭 신호를 입력 받도록 구성된다. 도 16a의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 16a의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 16a의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 16a를 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워 가 증가하고, 구동 전압(VDD)이 저하되는 경향이 도시되어 있다. 다만, 위상이 서로 다른 신호 클럭이 복수의 NPU들에 입력됨에 따라 전체 피크 파워가 저감되고 및 공급 전압(VDD)의 출렁임도 저감되는 경향이 도시되어 있 다. 부연 설명하면, 각 클럭 마다 동작하는 PE들의 개수는 인공신경망모델을 컴파일하는 컴파일러의 성능에 따라 달 라질 수 있다. 즉, 컴파일러의 성능이 우수할수록 단위 클럭마다 더 많은 PE들을 동작 시킬 수 있다. 여기서 전 체 PE들 중 동작하는 PE의 비율을 PE의 가동률(utilization rate %)로 지칭할 수 있다. 반대로, 컴파일러의 성 능이 우수할수록 NPU의 피크 파워가 더 증가할 수 있다. 즉, 피크 파워는 PE의 가동률과 정비례할 수 있다. 따 라서 컴파일러의 알고리즘이 정교해짐에 따라, NPU의 피크 파워는 더 증가될 수 있는 문제가 있다. 제1 클럭(CLK1)에 대응하는 상기 제1 NPU(100-1)의 피크 파워는 각 클럭당 동작하는 PE의 개수에 따라서 달라질 수 있다. 마찬가지로, 제2 클럭(CLK2)에 대응하는 상기 제2 NPU(100-2)의 피크 파워는 각 클럭당 동작하는 PE의 개수에 따라서 달라질 수 있다. 즉, 하나의 클럭에 동작하는 PE의 개수에 비례하여 피크 파워가 증가될 수 있다. 부연 설명하면, 저전력으로 개발된 NPU는 엣지 디바이스에 적용될 때 M.2 인터페이스 또는 PCI Express 인터페 이스에 대응되도록 개발될 수 있다. 특히 M.2 인터페이스의 경우 PCI Express 인터페이스에 비해서 최대 파워가 상대적으로 더 낮을 수 있다. 예를 들면 M.2 인터페이스의 경우 3A의 전류와 10W의 파워 제한을 가질 수 있다. 만약 비교예의 NPU의 피크 파워가 특정 클럭에서 10W를 초과할 경우, NPU에 공급되는 공급 전압(VDD)이 흔들릴 수 있다. 이때, PE의 가동률이 올라갈수록 NPU의 피크 파워는 더 증가되고, 피크 파워가 파워 제한을 증가할수 록 공급 전압(VDD)의 저하 정도는 더 커질 수 있다. 만약 공급 전압(VDD)이 IR-drop margin 이하로 떨어질 경우, NPU가 연산 중인 데이터에 오류가 발생될 수 있다. 한편, 상기 제1 NPU(100-1)에는 제1 클럭(CLK1)이 인가되고, 상기 제2 NPU(100-2)에는 제1 클럭(CLK1)과 위상 이 다른 제2 클럭(CLK2)이 인가될 수 있다. 따라서 상기 제1 NPU(100-1) 및 상기 제2 NPU(100-2)의 피크 파워 는 분산될 수 있으며, NPU의 피크 파워 관점에서 피크 파워가 반으로 저감될 수 있다. 이러한 경우 공급 전압 (VDD)의 전압 안정성이 도 15에 도시된 비교예와 비교할 때 상대적으로 더 안정적일 수 있다.즉, 본 개시의 일 예시에 따르면, NPU의 피크 파워가 저감될 수 있으며, NPU를 포함하는 SoC의 피크 파워 또한 저감될 수 있다. 또한 M.2와 같은 저전력 인터페이스의 경우, 파워 제한이 낮을 수 있기 때문에, 이러한 저전력 인터페이스에서 더욱 더 효과적일 수 있다. 하지만, 본 개시는 특정 인터페이스에 제한되지 않는다. 도 16b는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 3개의 NPU들에게 입력 하는 예시를 도시한다. 도 16b를 참조하면, 제1 클럭(CLK1)은 상기 제1 NPU(100-1)에 입력되고, 제2 클럭(CLK2)은 상기 제2 NPU(100- 2)에 입력되고, 제3 클럭(CLK3)은 상기 제3 NPU(100-3)에 입력될 수 있다. 여기서 상기 제1 NPU(100-1), 상기 제2 NPU(100-2), 상기 제3 NPU(100-3)는 서로 상이한 위상의 클럭 신호를 입력 받도록 구성된다. 도 16b의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 16b의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 16b의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 16b를 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워 가 증가하고, 구동 전압(VDD)이 저하되는 경향이 도시되어 있다. 다만, 위상이 서로 다른 신호 클럭이 복수의 NPU들에 입력됨에 따라 최대 피크 파워가 저감되고 공급 전압(VDD)의 출렁임도 저감되는 경향이 도시되어 있다. 상기 제1 NPU(100-1)에는 제1 클럭(CLK1)이 인가되고, 상기 제2 NPU(100-2)에는 제2 클럭(CLK2)이 인가되고, 상기 제3 NPU(100-3)에는 제3 클럭(CLK3)이 인가될 수 있다. 도 16b에 도시된 바와 같이, 각 NPU의 피크 파워가 발생하는 구간은 시간 축 상에서 분산될 수 있으며, 최대 피 크 파워는 크게 저감될 수 있다. 이러한 경우 공급 전압(VDD)의 전압 안정성이 도 15에 도시된 비교예와 비교할 때 상대적으로 더 안정적일 수 있다. 도 16c는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 4개의 NPU들에게 입력 하는 예시를 도시한다. 도 16c를 참조하면, 제1 클럭(CLK1)은 상기 제1 NPU(100-1)에 입력되고, 제2 클럭(CLK2)은 상기 제2 NPU(100- 2)에 입력되고, 제3 클럭(CLK3)은 상기 제3 NPU(100-3)에 입력되고, 그리고 제4 클럭(CLK4)은 상기 제4 NPU(100-4)에 입력될 수 있다. 여기서 상기 제1 NPU(100-1), 상기 제2 NPU(100-2), 상기 제3 NPU(100-3), 상기 제4 NPU(100-4)는 서로 상이한 위상의 클럭 신호를 입력 받도록 구성된다. 도 16c의 100x of PE utilization은 일백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 16c의 200x of PE utilization은 이백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 16c의 500x of PE utilization은 오백 개의 프로세싱 엘리먼트가 활성화될 때의 피크 파워 및 구동 전압 (VDD) 변화 특성을 예시적으로 도시한다. 도 16c를 참조하면, 신경 프로세싱 유닛의 프로세싱 엘리먼트의 활성화되는 개수가 증가할수록 피크 파워 가 증가하고, 구동 전압(VDD)이 저하되는 경향이 도시되어 있다. 다만, 위상이 서로 다른 신호 클럭이 복수의 NPU들에 입력됨에 따라 전체 피크 파워가 저감되고 및 공급 전압(VDD)의 출렁임도 저감되는 경향이 도시되어 있 다. 상기 제1 NPU(100-1)에는 제1 클럭(CLK1)이 인가되고, 상기 제2 NPU(100-2)에는 제1 클럭(CLK1)과 위상이 다른 제2 클럭(CLK2)이 인가될 수 있다. 상기 제3 NPU(100-3)에는 제3 클럭(CLK3)이 인가되고, 상기 제4 NPU(100- 4)에는 제4 클럭(CLK4)이 인가될 수 있다. 따라서 상기 제1 NPU(100-1), 상기 제2 NPU(100-2), 상기 제3 NPU(100-3), 상기 제4 NPU(100-4)의 피크 파워가 발생하는 구간이 시간 축 상에서 분산될 수 있다. 도 16c에 도시된 바와 같이, 각 NPU의 피크 파워가 발생하는 구간은 시간 축 상에서 분산될 수 있으며, 최대 피 크 파워는 크게 저감될 수 있다. 이러한 경우 공급 전압(VDD)의 전압 안정성이 도 15에 도시된 비교예와 비교할 때 상대적으로 더 안정적일 수 있다. . <본 개시의 예시들의 간략 정리> 지금까지 설명한 본 개시의 내용들을 정리하여 설명하면 다음과 같다. 본 개시의 일 예시는 신경 프로세싱 유닛을 제공한다. 상기 신경 프로세싱 유닛은 ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 PE 그룹들을 위해 배치된, 제1 회로와; 클럭 소스로부터의 원 클럭 신호를 기반으로, 위상이 서로 다른 복수의 클럭 신호를 분할 생성하여 상기 복수의 PE 그룹들에게 제공하는 클럭 분배기를 위하여 배치된, 제2 회로를 포 함할 수 있다. 상기 복수의 클럭 신호들 중 제1 클럭 신호는 상기 복수의 PE 그룹들 중에서 제1 PE 그룹에게 제 공되고, 상기 복수의 클럭 신호들 중 제2 클럭 신호는 상기 복수의 PE 그룹들 중에서 제2 PE 그룹에게 제공될 수 있다. 상기 복수의 클럭 신호들 중 적어도 하나는 피크 파워 저감을 고려하여 배치된 적어도 하나의 출력 버퍼를 지나 서 상기 복수의 PE그룹들 중 하나에 공급되도록 구성될 수 있다. 상기 복수의 클럭 신호들은 상기 원 클럭 신호 대비 주파수는 더 낮을 수 있다. 상기 복수의 클럭 신호들의 주파수는, 상기 복수의 PE 그룹의 개수에 기초하여 결정될 수 있다. 상기 복수의 클럭 신호들의 주파수는, 상기 원 클럭 신호 대비 상기 복수의 PE 그룹의 개수 배수 만큼 낮을 수 있다. 상기 제2 클럭 신호는 상기 제1 클럭 신호에 비하여 위상이 더 늦을 수 있다. 상기 클럭 분배기를 위하여 배치된 제2 회로는: 상기 원 클럭 신호를 지연시키기 위한 복수의 플리 플롭과; 상 기 복수의 플리-플롭과 병렬로 연결된 다중화기와; 그리고 상기 다중화기로부터의 출력 신호의 주파수를 분할 하는 분할기를 포함할 수 있다. 상기 클럭 분배기를 위하여 배치된 제2 회로는: 복수의 지연 셀과; 상기 복수의 지연셀과 병렬로 연결된 다중화 기를 포함할 수 있다. 상기 클럭 분배기를 위하여 배치된 제2 회로는: 원 클럭 신호와 연결되어 원 클럭 신호의 주파수를 분할하는 분할기와; 그리고 상기 분할기의 출력단과 연결된 제1 D형 플리-플롭을 포함할 수 있다. 상기 분할기 및 상기 제1 D형 플리-플롭의 출력단 각각은 상기 원 클럭 소스의 클럭 주파수가 연결될 수 있다. 본 개시의 일 예시는 시스템-온-칩을 제공한다. 상기 시스템-온 칩은 반도체(semi-conductor) 기판; ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 신경 프로세싱 유닛(NPU)들을 위하여, 상기 반도체 기판 상에 배치된, 제1 회로; 그리 고 클럭 소스로부터의 원 클럭 신호를 기반으로, 위상이 서로 다른 복수의 클럭 신호를 분할 생성하여 상기 복 수의 NPU들에게 제공하는 클럭 분배기를 위하여, 상기 반도체 기판 상에 배치된, 제2 회로를 포함할 수 있다. 상기 복수의 클럭 신호들 중 제1 클럭 신호는 상기 복수의 NPU들 중에서 제1 NPU 에게 제공되고, 상기 복수의 클럭 신호들 중 제2 클럭 신호는 상기 복수의 NPU들 중에서 제2 NPU에게 제공될 수 있다. 상기 복수의 클럭 신호들 중 적어도 하나는 피크 파워 저감을 고려하여 배치된 적어도 하나의 출력 버퍼를 지나 서 상기 복수의 NPU 중 하나에 공급되도록 구성될 수 있다. 상기 복수의 클럭 신호들은 상기 원 클럭 신호 대비 주파수는 더 낮을 수 있다. 상기 복수의 클럭 신호들의 주파수는, 상기 복수의 NPU들의 개수에 기초하여 결정될 수 있다. 상기 복수의 클럭 신호들의 주파수는, 상기 원 클럭 신호 대비 상기 복수의 NPU들의 개수 배수 만큼 낮을 수 있 다. 상기 제2 클럭 신호는 상기 제1 클럭 신호에 비하여 위상이 더 늦을 수 있다. 본 개시의 일 예시는 전자 장치를 제공한다. 상기 전자 장치는: 인쇄 회로 기판; ANN(artificial neural network) 모델을 위한 연산들을 수행할 수 있도록 설정되고, 복수의 프로세싱 엘리먼트 (PE)를 포함하는 복수의 신경 프로세싱 유닛(NPU)들을 위하여, 상기 인쇄 회로 기판 상에 배치된, 제1 회로; 그리고 클럭 소스로부터의 원 클럭 신호를 기반으로, 위상이 서로 다른 복수의 클럭 신호를 분할 생성하여 상기 복수의 NPU들에게 제공하 는 클럭 분배기를 위하여, 상기 인쇄 회로 기판 상에 배치된, 제2 회로를 포함할 수 있다. 상기 복수의 클럭 신 호들 중 제1 클럭 신호는 상기 복수의 NPU들 중에서 제1 NPU 에게 제공되고, 상기 복수의 클럭 신호들 중 제2 클럭 신호는 상기 복수의 NPU들 중에서 제2 NPU에게 제공될 수 있다. 상기 복수의 클럭 신호들 중 적어도 하나는 피크 파워 저감을 고려하여 배치된 적어도 하나의 출력 버퍼를 지나 서 상기 복수의 NPU 중 하나에 공급되도록 구성될 수 있다. 상기 복수의 클럭 신호들은 상기 원 클럭 신호 대비 주파수는 더 낮을 수 있다. 상기 복수의 클럭 신호들의 주파수는, 상기 복수의 NPU들의 개수에 기초하여 결정될 수 있다. 상기 복수의 클럭 신호들의 주파수는, 상기 원 클럭 신호 대비 상기 복수의 NPU들의 개수 배수 만큼 낮을 수 있다. 상기 제2 클럭 신호는 상기 제1 클럭 신호에 비하여 위상이 더 늦을 수 있다. 본 명세서와 도면에 나타난 본 개시의 예시들은 본 개시의 기술 내용을 쉽게 설명하고 본 개시의 이해를 돕기 위해 특정 예를 제시한 것뿐이며, 본 명의 범위를 한정하고자 하는 것은 아니다. 지금까지 설명한 예시들 이외 에도 다른 변형 예들이 실시 가능하다는 것은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 자명 한 것이다."}
{"patent_id": "10-2024-0021374", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 도 2a은 컨볼루션 신경망(CNN)의 기본 구조를 설명하기 위한 도면이다. 도 2b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 3은 본 개시의 일 예시에 따른 신경 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 4a는 본 개시의 일 예시에 적용될 수 있는 복수의 프로세싱 엘리먼트 중 하나의 프로세싱 엘리먼트를 설명하 는 개략적인 개념도이다. 도 4b는 본 개시의 일 예시에 적용될 수 있는 SFU를 설명하는 개략적인 개념도이다. 도 5는 도 3에 도시된 신경 프로세싱 유닛의 변형예를 나타낸 예시도이다. 도 6a은 예시적인 인공신경망모델 내의 각 레이어 별 데이터의 크기를 나타낸 예시도이고, 도 6b는 도 6a에 도 시된 예시적 인공신경망모델에서 각 레이어 별 데이터 사이즈 등을 나타낸 예시적인 테이블이다. 도 7a는 본 개시의 제1 예시에 따른 NPU의 구조를 나타낸 예시도이다. 도 7b는 본 개시의 제1 예시의 제1 변형예에 따른 NPU의 구조를 나타낸 예시도이다. 도 7c는 본 개시의 제1 예시의 제2 변형예에 따른 NPU의 구조를 나타낸 예시도이다. 도 8a는 본 개시의 제2 예시에 따른 NPU의 구조를 타낸 예시도이다. 도 8b는 본 개시의 제2 예시의 제1 변형예에 따른 NPU의 구조를 타낸 예시도이다. 도 8c는 본 개시의 제2 예시의 제2 변형예에 따른 NPU의 구조를 타낸 예시도이다. 도 9a는 본 개시의 제3 예시에 따른 SoC의 구조를 나타낸 예시도이다. 도 9b는 본 개시의 제3 예시의 제1 변형예에 따른 SoC의 구조를 나타낸 예시도이다. 도 9c는 본 개시의 제3 예시의 제2 변형예에 따른 SoC의 구조를 나타낸 예시도이다. 도 10a는 본 개시의 제4 예시에 따른 시스템의 구조를 나타낸 예시도이다.도 10b는 본 개시의 제4 예시의 제1 변형예에 따른 시스템의 구조를 나타낸 예시도이다. 도 10c는 본 개시의 제4 예시의 제2 변형예에 따른 시스템의 구조를 나타낸 예시도이다. 도 11a은 클럭 분배기의 제1 예시이다. 도 11b는 클럭 분배기의 제2 예시이다. 도 11c는 클럭 분배기의 제3 예시이다. 도 11d는 클럭 분배기의 제3 예시이다. 도 11e는 클럭 분배기로부터의 예시적인 신호 파형을 나타낸 예시도이다. 도 12는 본 개시의 일 예시에 따른 NPU의 동작 방법을 나타낸 예시도이다. 도 13은 서로 다른 위상의 클럭 신호를 NPU의 복수의 PE들에게 입력하지 않는 비교 예시를 도시한다. 도 14a는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 NPU의 2개의 그룹의 복 수의 PE들에게 입력하는 예시를 도시한다. 도 14b는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 NPU의 3개의 그룹의 복 수의 PE들에게 입력하는 예시를 도시한다. 도 14c는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 NPU의 4개의 그룹의 복 수의 PE들에게 입력하는 예시를 도시한다. 도 15는 서로 다른 위상의 클럭 신호를 복수의 NPU들에게 입력하지 않는 비교 예시를 도시한다. 도 16a는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 2개의 NPU들에게 입력 하는 예시를 도시한다. 도 16b는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 3개의 NPU들에게 입력 하는 예시를 도시한다. 도 16c는 본 개시의 일 예시에 따라 피크파워를 고려하여 서로 다른 위상의 클럭 신호를 4개의 NPU들에게 입력 하는 예시를 도시한다."}
