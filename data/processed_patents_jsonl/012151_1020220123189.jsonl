{"patent_id": "10-2022-0123189", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0044005", "출원번호": "10-2022-0123189", "발명의 명칭": "사전 훈련된 언어 모델의 추론 성능 향상을 위한 전자장치, 그 방법 및 기록매체", "출원인": "아주대학교산학협력단", "발명자": "정태선"}}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사전 훈련된 언어 모델의 추론 성능 향상을 위한 전자장치에 있어서,입력데이터를 사전 훈련된 언어 모델의 복수의 트랜스포머 계층에 순차적으로 통과시키며 출력데이터를 획득하는 프로세서를 포함하고,상기 프로세서는,상기 복수의 트랜스포머 계층의 각 후단에 연결된 복수의 중간 계층의 각각에서 각 트랜스포머 계층으로부터 수신한 예측 결과에 대한 확률 분포를 연산하고,상기 확률 분포를 이용하여 연산한 엔트로피 값에 기초하여 상기 예측 결과의 신뢰 수준을 측정하고,상기 복수의 중간 계층 중 기 정의된 수만큼 연속된 중간 계층들에서 기 정의된 값 미만의 신뢰 수준이 측정되는 경우, 상기 연속된 중간 계층들 중 마지막 제1중간 계층을 출구로 하여 상기 제1중간 계층에서의 예측 결과를 상기 출력데이터로 출력하는 전자장치."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 프로세서는,상기 복수의 중간 계층 각각에서 측정된 신뢰 수준이 기 정의된 값 미만인 상기 연속된 중간 계층의 수를 카운트하는 전자장치."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 프로세서는,상기 복수의 트랜스포머 계층의 각 계층을 통과할 때마다 상기 입력데이터에 대한 예측 결과를 식별하는 전자장치."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 프로세서는,상기 연속된 중간 계층들의 기 정의된 수 및 상기 신뢰 수준의 기 정의된 값 중 적어도 하나를 조정하는 전자장치."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 프로세서는,제2중간 계층에서 기 정의된 값 이상의 신뢰 수준이 측정되는 경우, 제2중간 계층에 대응하는 트랜스포머 계층의 다음 트랜스포머 계층에서 상기 예측 결과를 수정하는 전자장치."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "전자장치에 의해 수행되는 사전 훈련된 언어 모델의 추론 성능 향상을 위한 방법에 있어서,공개특허 10-2024-0044005-3-입력데이터를 사전 훈련된 언어 모델의 복수의 트랜스포머 계층에 순차적으로 통과시키며 출력데이터를 획득하는 단계를 포함하고,상기 출력데이터를 획득하는 단계는, 상기 복수의 트랜스포머 계층의 각 후단에 연결된 복수의 중간 계층의 각각에서 각 트랜스포머 계층으로부터 수신한 예측 결과에 대한 확률 분포를 연산하는 단계;상기 확률 분포를 이용하여 연산한 엔트로피 값에 기초하여 상기 예측 결과의 신뢰 수준을 측정하는 단계;상기 복수의 중간 계층 중 기 정의된 수만큼 연속된 중간 계층들에서 기 정의된 값 미만의 신뢰 수준이 측정되는 경우, 상기 연속된 중간 계층들 중 마지막 제1중간 계층을 출구로 하여 상기 제1중간 계층에서의 예측 결과를 상기 출력데이터로 출력하는 단계를 포함하는 방법."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 출력데이터를 획득하는 단계는, 상기 복수의 중간 계층 각각에서 측정된 신뢰 수준이 기 정의된 값 미만인 상기 연속된 중간 계층의 수를 카운트하는 단계를 포함하는 방법."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 확률 분포를 연산하는 단계는,상기 복수의 트랜스포머 계층의 각 계층을 통과할 때마다 상기 입력데이터에 대한 예측 결과를 식별하는 단계를포함하는 방법."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서,상기 연속된 중간 계층들의 기 정의된 수 및 상기 신뢰 수준의 기 정의된 값 중 적어도 하나를 조정하는 단계를더 포함하는 방법."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제6항에 있어서,상기 신뢰 수준을 측정하는 단계는,제2중간 계층에서 기 정의된 값 이상의 신뢰 수준이 측정되는 경우, 제2중간 계층에 대응하는 트랜스포머 계층의 다음 트랜스포머 계층에서 상기 예측 결과를 수정하는 단계를 포함하는 방법."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "컴퓨터가 읽을 수 있는 코드로서, 사전 훈련된 언어 모델의 추론 성능 향상을 위한 방법을 수행하는 코드를 포함하는 컴퓨터 프로그램이 저장된 기록매체에 있어서, 상기 방법은, 입력데이터를 사전 훈련된 언어 모델의 복수의 트랜스포머 계층에 순차적으로 통과시키며 출력데이터를 획득하는 단계를 포함하고,상기 출력데이터를 획득하는 단계는, 상기 복수의 트랜스포머 계층의 각 후단에 연결된 복수의 중간 계층의 각각에서 각 트랜스포머 계층으로부터 수신한 예측 결과에 대한 확률 분포를 연산하는 단계;상기 확률 분포를 이용하여 연산한 엔트로피 값에 기초하여 상기 예측 결과의 신뢰 수준을 측정하는 단계;상기 복수의 중간 계층 중 기 정의된 수만큼 연속된 중간 계층들에서 기 정의된 값 미만의 신뢰 수준이 측정되공개특허 10-2024-0044005-4-는 경우, 상기 연속된 중간 계층들 중 마지막 제1중간 계층을 출구로 하여 상기 제1중간 계층에서의 예측 결과를 상기 출력데이터로 출력하는 단계를 포함하는 것을 특징으로 하는 컴퓨터가 읽을 수 있는 프로그램이 기록된기록매체."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 출력데이터를 획득하는 단계는, 상기 복수의 중간 계층 각각에서 측정된 신뢰 수준이 기 정의된 값 미만인 상기 연속된 중간 계층의 수를 카운트하는 단계를 포함하는 것을 특징으로 하는 프로그램이 기록된 기록매체."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 확률 분포를 연산하는 단계는,상기 복수의 트랜스포머 계층의 각 계층을 통과할 때마다 상기 입력데이터에 대한 예측 결과를 식별하는 단계를포함하는 것을 특징으로 하는 프로그램이 기록된 기록매체."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 연속된 중간 계층들의 기 정의된 수 및 상기 신뢰 수준의 기 정의된 값 중 적어도 하나를 조정하는 단계를더 포함하는 것을 특징으로 하는 프로그램이 기록된 기록매체."}
{"patent_id": "10-2022-0123189", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 신뢰 수준을 측정하는 단계는,제2중간 계층에서 기 정의된 값 이상의 신뢰 수준이 측정되는 경우, 제2중간 계층에 대응하는 트랜스포머 계층의 다음 트랜스포머 계층에서 상기 예측 결과를 수정하는 단계를 포함하는 것을 특징으로 하는 프로그램이 기록된 기록매체."}
{"patent_id": "10-2022-0123189", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 사전 훈련된 언어 모델의 추론 성능 향상을 위한 전자장치에 있어서, 입력데이터를 사전 훈련된 언어 모델의 복수의 트랜스포머 계층에 순차적으로 통과시키며 출력데이터를 획득하는 프로세서를 포함하고, 상기 프로세서는, 상기 복수의 트랜스포머 계층의 각 후단에 연결된 복수의 중간 계층의 각각에서 각 트랜스포머 계층으로부터 수신한 예측 결과에 대한 확률 분포를 연산하고, 상기 확률 분포를 이용하여 연산한 엔 트로피 값에 기초하여 상기 예측 결과의 신뢰 수준을 측정하고, 상기 복수의 중간 계층 중 기 정의된 수만큼 연 속된 중간 계층들에서 기 정의된 값 미만의 신뢰 수준이 측정되는 경우, 상기 연속된 중간 계층들 중 마지막 제1 중간 계층을 출구로 하여 상기 제1중간 계층에서의 예측 결과를 상기 출력데이터로 출력한다."}
{"patent_id": "10-2022-0123189", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 사전 훈련된 언어 모델의 추론 성능 향상을 위한 전자장치, 그 방법 및 기록매체에 관한 것이다."}
{"patent_id": "10-2022-0123189", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "BERT(Bidirectional Encoder Representations from Transformers)가 출시된 이후로 GPT, XLNet, ALBERT 등 많 은 사전 훈련된 언어 모델(PLM, Pre-training language Model)은 자연어 처리(NLP, Natural Language Processing)를 위한 최고 수준(SOTA, state of the art)의 모델이 되었다. 이러한 BERT 스타일 모델은 레이블이 지정되지 않은 말뭉치에 대한 사전 훈련과 텍스트 분류, 자연어 추론(NLI) 및 시퀀스 레이블 지정과 같은 레이블이 지정된 작업에 대한 미세 조정을 통해 많은 자연어 처리 작업에서 상당 한 개선을 달성했다. 그러나, 우수한 성능에도 불구하고 현 PLM은 과도하게 사고하는 문제가 있는 바, 문장 분류 작업에서 계층들이 일부 샘플에 대해 너무 깊을 수 있다. 이는 높은 대기 시간이라는 다른 문제를 불러올 수 있으며, 소비자 쿼리가 많아지는 경우, 예를 들어, 온라인 의료 상담을 구현하는 PLM에 있어서 독감 시즌인 경우, 높은 대기 시간은 사용자의 불편을 초래할 수 있다. 언어 모델의 과도한 사고 및 대기 시간의 증가를 방지하기 위해 대기 시간을 동적으로 조정하는 것이 중요하다. 이를 위해, 대기 시간을 동적으로 조정할 수 있는 적응형 추론이 대두되었다. 가장 중요한 적응 추론 방법 중 하나인 조기 종료(Early Exiting)는 언어 모델의 각 트랜스포머 계층의 후단에 중간 계층(분류기)을 설치하고 추론 결과가 일정 조건을 만족하면 조기에 추론을 종료하는 방법이다. 다만, 기존 조기 종료 방법은 속도 향상 비율을 조정하는 데 융통성이 없다. 즉, 조기 종료를 위한 매개 변수들 이 고정되면, 고정된 속도 향상 비율만 달성할 수 있으므로, 실제 산업 시나리오에서 사용하기 불편하다. 따라서, 속도 향상 비율을 유연하게 조정할 수 있고, 성능-속도 간 트레이드 오프를 보완할 수 있는 새롭고 효 율적인 추론 방법에 대한 연구가 필요한 실정이다."}
{"patent_id": "10-2022-0123189", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 사전 훈련된 언어 모델의 추론 속도를 적응적으로 조절 가능한 전자장치, 그 방법 및 기록매 체를 제공하는 것이다. 본 발명의 목적은 사전 훈련된 언어 모델의 추론 속도를 향상시키면서도 정확한 예측 결과를 출력하는 전자장치, 그 방법 및 기록매체를 제공하는 것이다."}
{"patent_id": "10-2022-0123189", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 사전 훈련된 언어 모델의 추론 성능 향상을 위한 전자장치에 있어서, 입력데이터를 사전 훈련된 언어 모델의 복수의 트랜스포머 계층에 순차적으로 통과시키며 출력데이터를 획득하는 프로세서를 포함하고, 상기 프로세서는, 상기 복수의 트랜스포머 계층의 각 후단에 연결된 복수의 중간 계층의 각각에서 각 트랜스포머 계층으로부터 수신한 예측 결과에 대한 확률 분포를 연산하고, 상기 확률 분포를 이용하여 연산한 엔트로피 값에 기초하여 상기 예측 결과의 신뢰 수준을 측정하고, 상기 복수의 중간 계층 중 기 정의된 수만큼 연속된 중간 계층들에서 기 정의된 값 미만의 신뢰 수준이 측정되는 경우, 상기 연속된 중간 계층들 중 마지막 제1중간 계층을 출구로 하여 상기 제1중간 계층에서의 예측 결과를 상기 출력데이터로 출력한다. 상기 프로세서는, 상기 복수의 중간 계층 각각에서 측정된 신뢰 수준이 기 정의된 값 미만인 상기 연속된 중간 계층의 수를 카운트할 수 있다. 상기 프로세서는, 상기 복수의 트랜스포머 계층의 각 계층을 통과할 때마다 상기 입력데이터에 대한 예측 결과 를 식별할 수 있다. 상기 프로세서는, 상기 연속된 중간 계층들의 기 정의된 수 및 상기 신뢰 수준의 기 정의된 값 중 적어도 하나 를 조정할 수 있다. 상기 프로세서는, 제2중간 계층에서 기 정의된 값 이상의 신뢰 수준이 측정되는 경우, 제2중간 계층에 대응하는 트랜스포머 계층의 다음 트랜스포머 계층에서 상기 예측 결과를 수정할 수 있다. 본 발명의 일 실시예에 따른 전자장치에 의해 수행되는 사전 훈련된 언어 모델의 추론 성능 향상을 위한 방법에 있어서, 입력데이터를 사전 훈련된 언어 모델의 복수의 트랜스포머 계층에 순차적으로 통과시키며 출력데이터를 획득하는 단계를 포함하고, 상기 출력데이터를 획득하는 단계는, 상기 복수의 트랜스포머 계층의 각 후단에 연 결된 복수의 중간 계층의 각각에서 각 트랜스포머 계층으로부터 수신한 예측 결과에 대한 확률 분포를 연산하는 단계; 상기 확률 분포를 이용하여 연산한 엔트로피 값에 기초하여 상기 예측 결과의 신뢰 수준을 측정하는 단계; 상기 복수의 중간 계층 중 기 정의된 수만큼 연속된 중간 계층들에서 기 정의된 값 미만의 신뢰 수준이 측정되는 경우, 상기 연속된 중간 계층들 중 마지막 제1중간 계층을 출구로 하여 상기 제1중간 계층에서의 예측 결과를 상기 출력데이터로 출력하는 단계를 포함한다. 상기 출력데이터를 획득하는 단계는, 상기 복수의 중간 계층 각각에서 측정된 신뢰 수준이 기 정의된 값 미만인 상기 연속된 중간 계층의 수를 카운트하는 단계를 포함할 수 있다. 상기 확률 분포를 연산하는 단계는, 상기 복수의 트랜스포머 계층의 각 계층을 통과할 때마다 상기 입력데이터 에 대한 예측 결과를 식별하는 단계를 포함할 수 있다. 상기 방법은, 상기 연속된 중간 계층들의 기 정의된 수 및 상기 신뢰 수준의 기 정의된 값 중 적어도 하나를 조 정하는 단계를 더 포함할 수 있다.상기 신뢰 수준을 측정하는 단계는, 제2중간 계층에서 기 정의된 값 이상의 신뢰 수준이 측정되는 경우, 제2중 간 계층에 대응하는 트랜스포머 계층의 다음 트랜스포머 계층에서 상기 예측 결과를 수정하는 단계를 포함할 수 있다. 컴퓨터가 읽을 수 있는 코드로서, 사전 훈련된 언어 모델의 추론 성능 향상을 위한 방법을 수행하는 코드를 포 함하는 컴퓨터 프로그램이 저장된 기록매체에 있어서, 상기 방법은, 입력데이터를 사전 훈련된 언어 모델의 복 수의 트랜스포머 계층에 순차적으로 통과시키며 출력데이터를 획득하는 단계를 포함하고, 상기 출력데이터를 획 득하는 단계는, 상기 복수의 트랜스포머 계층의 각 후단에 연결된 복수의 중간 계층의 각각에서 각 트랜스포머 계층으로부터 수신한 예측 결과에 대한 확률 분포를 연산하는 단계; 상기 확률 분포를 이용하여 연산한 엔트로 피 값에 기초하여 상기 예측 결과의 신뢰 수준을 측정하는 단계; 상기 복수의 중간 계층 중 기 정의된 수만큼 연속된 중간 계층들에서 기 정의된 값 미만의 신뢰 수준이 측정되는 경우, 상기 연속된 중간 계층들 중 마지막 제1중간 계층을 출구로 하여 상기 제1중간 계층에서의 예측 결과를 상기 출력데이터로 출력하는 단계를 포함하 는 것을 특징으로 한다."}
{"patent_id": "10-2022-0123189", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 조기 종료는 연속된 중간 계층이 예측 결과에 대해 확신할 때 발생하므로 조기 종료 결정을 보다 안정적으로 만든다. 본 발명의 일 실시예에 따르면, 연속된 중간 계층들의 기 정의된 수 및 신뢰 수준의 기 정의된 값 중 적어도 하 나를 편리하게 조정할 수 있는 바, 속도 향상 비율을 제어할 수 있으므로 더 유연하다. 본 발명의 일 실시예에 따르면, 다양한 백본 모델에 적용할 수 있으며, 모델 압축 방법과 함께 작동하여 추론 속도를 높일 수 있다."}
{"patent_id": "10-2022-0123189", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명에 따른 바람직한 실시 형태를 첨부된 도면을 참조하여 상세하게 설명한다. 첨부된 도면과 함께 이하에 개시될 상세한 설명은 본 발명의 예시적인 실시형태를 설명하고자 하는 것이며, 본 발명이 실시될 수 있 는 유일한 실시형태를 나타내고자 하는 것이 아니다. 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관 계없는 부분은 생략할 수 있고, 명세서 전체를 통하여 동일 또는 유사한 구성 요소에 대해서는 동일한 참조 부 호를 사용할 수 있다. 도 1은 본 발명의 일 실시예에 따른 사전 훈련된 언어 모델을 도시한 개략도이다. 도 1에는 본 발명의 일 실시예에 따른 사전 훈련된 언어 모델(이하, 언어 모델이라고도 한다.)의 기본 구 조가 도시된다. 본 발명의 일 실시예에 따른 언어 모델은 복수의 트랜스포머 계층과 복수의 트랜스포머 계층의 각 후단에 복수의 중간 계층이 연결된 구조를 가진다. 이때, 중간 계층은 추론이 조기 종료 될 수 있는 출구로써, 도 1과 같이 언어 모델은 예측 결과에 따라 3번째 출구에서 추론을 조기 종료할 수 있 다. 본 발명의 일 실시예에 따른 언어 모델은 BERT를 백본 모델로 채택할 수 있다. BERT는 다층 트랜스포머 (Transformer)를 가지는 네트워크로, 대규모 코퍼스(corpus)에서 자기 지도(self-supervised) 방식으로 사전 훈련된다. 다만, 이에 한정되지 않으며 ALBERT 및 TinyBERT6와 같은 다른 유형의 사전 훈련된 백본 모델에도 적 용이 가능하다. 앞서 서술한 바와 같이, 언어 모델의 과도한 사고 및 대기 시간의 증가를 방지하기 위해, 언어 모델은 추론 속도를 동적으로 조정하는 조기 종료 방식을 채택한다. 이때, 조기 종료의 방법에는 예산 종료 모드(Budget Exiting mode)와 동적 종료 모드(Dynamic early exiting) 가 있다. 예산 종료 모드는 모든 쿼리에 대해 고정된 출구로 예측을 수행하고, 쿼리가 많은 경우 더 얕은 출구 를 지정하여 처리한다. 동적 종료 모드는 이전 및 현재 계층에서 획득한 예측 결과에 대해 각 계층에서 종료할 지 여부를 결정하도록 설계된다. 동적 종료 모드에서는 서로 다른 샘플이 서로 다른 깊이에서 종료할 수 있다. 본 발명의 일 실시예에 따른 언어 모델은 인내 및 신뢰 기반 조기 종료인, 이른바 PCEE-BERT(Patient and Confident Early Exiting-BERT)를 채택한다. 본 발명의 일 실시예에 따르면, 언어 모델에 입력데이터가 임베딩(embedding)되면, 언어 모델의 복수의 트랜스포머 계층에 순차적으로 통과시키며 출력데이터를 획득한다. 언어 모델은 예측 결과에 대해 확신할 수 있는 연속된 중간 계층의 수가 충분하면 조기 종료한다. 이때, 언 어 모델은 확률 분포를 이용하여 연산한 엔트로피를 신뢰 수준(confidence level)으로 사용한다. 언어 모델 은 단지 몇 개의 중간 계층이 확신을 가지는 것만으로는 조기 종료 하지 않고, 다음 계층에서 예측을 수행할 수도 있다. 이러한 방식으로, 언어 모델은 유연성을 유지하면서 더 높은 정확도로 종료할 수 있고, 특히 속도 향상 비율 이 큰 경우 보다 성능이 우수하다. 언어 모델은 전자장치에 장착되어 구동되는 바, 이하 도면들을 참조하여 본 발명의 일 실시예에 따른 전자장 치의 구성 및 동작에 대해 구체적으로 설명한다. 도 2는 본 발명의 일 실시예에 따른 전자장치의 구성을 도시한 블럭도이다. 본 발명의 일 실시예에 따른 전자장치는 사전 훈련된 언어 모델의 추론 성능 향상을 위한 장치로써, 컴 퓨터, 서버 등으로 구현될 수 있다. 본 발명의 일 실시예에 따른 전자장치는 입력부, 통신부, 표시부, 메모리 및 프로세 서를 포함한다. 입력부는 전자장치의 사용자 입력에 대응하여 입력데이터를 발생시킨다. 예를 들어, 사용자 입력은 전자장치의 동작을 시작하게 하는 사용자 입력, 입력데이터를 임베딩하는 입력일 수 있으며, 이 외에도 추 론 성능 향상된 언어 모델을 사용 및 출력데이터를 획득하기 위해 필요한 사용자 입력인 경우 제한하지 않고 적용 가능하다. 입력부는 적어도 하나의 입력수단을 포함한다. 입력부는 키보드(key board), 키패드(key pad), 돔 스 위치(dome switch), 터치패널(touch panel), 터치 키(touch key), 마우스(mouse), 메뉴 버튼(menu button) 등 을 포함할 수 있다. 통신부는 입력데이터, 사전 훈련된 언어모델, 출력데이터 등을 송수신하기 위해 서버 등 외부장치와의 통 신을 수행한다. 이를 위해, 통신부는 5G(5th generation communication), LTE-A(long term evolution- advanced), LTE(long term evolution), Wi-Fi(wireless fidelity) 등의 무선 통신 혹은 LAN(local area network), WAN(Wide Area Network), 전력선 통신 등의 유선 통신을 수행할 수 있다. 표시부는 전자장치의 동작에 따른 표시 데이터를 표시한다. 표시부는 사용자입력을 수신하는 화 면, 출력데이터를 표시하는 화면 등을 표시할 수 있다. 표시부는 액정 디스플레이(LCD; liquid crystal display), 발광 다이오드(LED; light emitting diode) 디 스플레이, 유기 발광 다이오드(OLED; organic LED) 디스플레이, 마이크로 전자기계 시스템(MEMS; micro electro mechanical systems) 디스플레이 및 전자 종이(electronic paper) 디스플레이를 포함한다. 표시부 는 입력부와 결합되어 터치 스크린(touch screen)으로 구현될 수 있다. 메모리는 전자장치의 동작 프로그램들을 저장한다. 메모리는 전원의 제공 유무와 무관하게 데이 터(정보)를 보존할 수 있는 비휘발성 속성의 스토리지(storage)와, 프로세서에 의해 처리되기 위한 데이터 가 로딩되며 전원이 제공되지 않으면 데이터를 보존할 수 없는 휘발성 속성의 메모리(memory)를 포함한다. 스토 리지에는 플래시메모리(flash-memory), HDD(hard-disc drive), SSD(solid-state drive) ROM(Read Only Memory) 등이 있으며, 메모리에는 버퍼(buffer), 램(RAM; Random Access Memory) 등이 있다. 메모리는 언어 모델을 저장할 수 있다. 메모리는 확률 분포의 연산, 엔트로피 값 연산, 중간 계층 의 수를 카운트 등을 수행하는 과정에서 필요한 연산 프로그램 등을 저장할 수 있다. 프로세서는 프로그램 등 소프트웨어를 실행하여 전자장치의 적어도 하나의 다른 구성요소(예: 하드웨 어 또는 소프트웨어 구성요소)를 제어할 수 있고, 다양한 데이터 처리 또는 연산을 수행할 수 있다. 본 발명의 일 실시예에 따른 프로세서는 입력데이터를 사전 훈련된 언어 모델의 복수의 트랜스포머 계층에 순차적으로 통과시키며 출력데이터를 획득할 수 있다. 본 발명의 일 실시예에 따른 프로세서는 복수의 트 랜스포머 계층의 각 후단에 연결된 복수의 중간 계층의 각각에서 각 트랜스포머 계층으로부터 수신한 예측 결과 에 대한 확률 분포를 연산하고, 확률 분포를 이용하여 연산한 엔트로피 값에 기초하여 예측 결과의 신뢰 수준을 측정하고, 복수의 중간 계층 중 기 정의된 수만큼 연속된 중간 계층들에서 기 정의된 값 미만의 신뢰 수준이 측 정되는 경우, 연속된 중간 계층들 중 마지막 제1중간 계층을 출구로 하여 제1중간 계층에서의 예측 결과를 출력 데이터로 출력할 수 있다. 이때, 프로세서가 언어 모델을 학습하거나, 기 학습되어 생성된 언어 모델을 외부로부터 수신 및 저 장하여 이용할 수 있으며 어느 하나에 한정되는 것은 아니다. 한편, 프로세서는 상기 동작들을 수행하기 위한 데이터 분석, 처리, 및 결과 정보 생성 중 적어도 일부를 규칙 기반 또는 인공지능(Artificial Intelligence) 알고리즘으로서 기계학습, 신경망 네트워크(neural network), 또는 딥러닝 알고리즘 중 적어도 하나를 이용하여 수행할 수 있다. 신경망 네트워크의 예로는, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network)과 같은 모델 을 포함할 수 있다. 도 3은 본 발명의 일 실시예에 따른 전자장치의 동작 흐름도를 도시한 도면이다. 본 발명의 일 실시예에 따르면, 프로세서는 복수의 트랜스포머 계층의 각 후단에 연결된 복수의 중간 계층 의 각각에서 각 트랜스포머 계층으로부터 수신한 예측 결과에 대한 확률 분포를 연산한다(S10). 도 1에 도시된 바와 같이, 언어 모델은 각 트랜스포머 계층에 출구가 있는 네트워크 구조를 가진다. 프로세서는 복수의 트랜스포머 계층의 각 계층을 통과할 때마다 입력데이터에 대한 예측 결과를 식별할 수 있다. 식별된 예측 결과는 해당 트랜스포머 계층의 후단에 연결된 중간 계층으로 전달된다. 입력데이터 x를 예측하기 위한 피드 포워드 프로세스가 계층 1, ..., m-1을 거치고, 이제 계층 m에 있다고 가정 한다. 입력데이터가 트랜스포머 계층 m을 통과한 후, 트랜스포머 계층 m의 후단에 연결된 중간 계층 f(m)(x; θ (m))에서 프로세서는 트랜스포머 계층 m의 예측 결과에 대한 확률 분포 p(m)(x; θ(m))를 연산한다. 이때, 트 랜스포머 계층 및 중간 계층의 모든 매개변수는 θ로 표시된다. 훈련 단계에서 모든 출구는 합산된 손실 함수로 공동으로 최적화된다. 손실 함수는 수학식 1과 같이 주어진 교 차 엔트로피(CE) 손실의 가중 평균이다. 수학식 1"}
{"patent_id": "10-2022-0123189", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 L(m) = CE(y, p(m)(x; θ(m)))는 m번째 출구의 엔트로피 교차손실을 나타낸다. 가중치 m은 출구 m의 상대적 추론 비용에 해당한다. 본 발명의 일 실시예에 따르면, 프로세서는 확률 분포를 이용하여 연산한 엔트로피 값에 기초하여 예측 결 과의 신뢰 수준을 측정한다(S20). 트랜스포머 계층 m의 신뢰 수준(C(m))은 수학식 2와 같이 확률 분포 p(m)(x; θ(m))의 엔트로피 값으로 측정된다. 수학식 2"}
{"patent_id": "10-2022-0123189", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 은 k번째 클래스 레이블에 대한 확률 질량이다. 신뢰 수준(C(m))이 기 정의된 값 τ보다 작으면 트 랜스포머 계층 m의 예측이 신뢰할 수 있는 것으로 본다. 만일 신뢰 수준(C(m))이 기 정의된 값 τ보다 크면 트랜 스포머 계층 m의 예측을 신뢰할 수 없는 것으로 본다. 본 발명의 일 실시예에 따르면, 프로세서는 복수의 중간 계층 중 기 정의된 수만큼 연속된 중간 계층들에 서 기 정의된 값 미만의 신뢰 수준이 측정되는 경우, 연속된 중간 계층들 중 마지막 제1중간 계층을 출구로 하 여 제1중간 계층에서의 예측 결과를 출력데이터로 출력한다(S30). 프로세서는 복수의 중간 계층 각각에서 측정된 신뢰 수준이 기 정의된 값 미만인 연속된 중간 계층의 수를 카운트할 수 있다. 이때, 프로세서는 인내 카운터(pct, patience counter)를 사용하여 예측이 연속된 중간 계층에서 확신을 유지하는 횟수를 저장한다. 이와 관련하여 구체적인 내용은 도 4를 참조하여 서술한다. 트랜스포머 계층 m에서의 인내 카운트 수(pct(m))가 기 정의된 수(정수) t에 도달하면, 프로세서는 트랜스 포머 계층 m에서 추론을 조기에 중지한다. 즉, 프로세서는 예측 결과에 대한 확률 분포에 대해 확신할 수 있는 연속된 중간 계층의 수가 충분하면 추 론을 조기 종료한다. 이 조건이 충족되지 않으면, 프로세서는 예측을 위해 최종 분류기 M을 사용한다. 이러한 방식으로 언어 모 델은 예측을 위해 모든 계층을 거치지 않고 조기 종료할 수 있다. 본 발명의 일 실시예에 따르면, 프로세서는 언어 모델의 추론이 다른 속도 향상 비율에 도달할 수 있도록 연속된 중간 계층들의 기 정의된 수 및 신뢰 수준의 기 정의된 값 중 적어도 하나를 조정할 수 있다. 또한, 본 발명의 일 실시예에 따른 언어 모델의 각 트랜스포머 계층에 예측 모듈이 있으므로 순방향 패스가 이 미 통과한 계층에 걸쳐 모델 앙상블을 수행할 수 있다. 크로스 레이어 앙상블(cross-layer ensemble)은 속도 향 상 비율이 크면 성능 저하로 이어지는 반면, 낮은 속도 향상 비율이 적용될 때에는 성능 향상을 가져온다. 즉, 계층의 깊이에 따라 크로스 레이어 앙상블 채택 여부를 결정할 수 있다. 본 발명에서는, 추론 속도 향상 비율을 조정할 수 있으므로 상황에 따라 크로스 레이어 앙상블 채택여부를 결정할 수 있다. 본 발명의 일 실시예에 따르면, 조기 종료는 연속된 중간 계층이 예측 결과에 대해 확신할 때 발생하므로 조기 종료 결정을 보다 안정적으로 만든다. 본 발명의 일 실시예에 따르면, 연속된 중간 계층들의 기 정의된 수 및 신뢰 수준의 기 정의된 값 중 적어도 하 나를 편리하게 조정할 수 있는 바, 속도 향상 비율을 제어할 수 있으므로 더 유연하다. 도 4는 본 발명의 일 실시예에 따른 전자장치의 동작 흐름도를 도시한 도면이다. 도 4에서는 도 3의 S30에서 설명한 바와 같이 조기 종료를 결정하는 과정에 대해 구체적으로 살펴본다. 이때, 도 3에서 서술한 내용과 중복되는 내용에 대해서는 구체적인 설명은 생략한다. 먼저, 프로세서는 트랜스포머 계층 m을 통과한 후 입력데이터에 대한 예측 결과를 식별한다(S410). 이때, m-1번째까지는 조기 종료가 결정되지 않은 것으로 가정한다. 프로세서는 예측 결과의 신뢰 수준이 기 정의된 값 미만인지 식별한다(S420). 신뢰 수준이 기 정의된 값 미만인 경우(S420의 Yes), 프로세서는 트랜스포머 계층 m-1에서 카운트 된 인내 카운트 수에서 하나를 추 가한다(S430). 신뢰 수준이 기 정의된 값 이상인 경우(S420의 No), 프로세서는 연속된 중간 계층의 수를 0으로 카운트할 수 있다(S440). 즉, 수식적으로 트랜스포머 계층 m에서 인내 카운트 수(pct(m))는 다음 수학식 3과 같이 계산될 수 있다. 수학식 3"}
{"patent_id": "10-2022-0123189", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "신뢰 수준이 기 정의된 값 미만이어서 연속된 중간 계층의 수가 0으로 카운트 된 경우(S440), 프로세서는 트랜스포머 계층 m에서 트랜스포머 계층 m+1로 이동하여 다시 S410을 수행한다(S450). 연속된 중간 계층의 수가 추가된 경우(S430), 프로세서는 연속된 중간 계층의 수(pct(m))가 기 정의된 수인 지 식별한다(S460). 만일, 기 정의된 수보다 작으면(S460의 No), 프로세서는 S450을 통해 트랜스포머 계층 m+1로 이동하여 S410을 수행한다. 프로세서는 연속된 중간 계층의 수(pct(m))가 기 정의된 수에 도달하는 경우(S460의 Yes), 트랜스포머 계층 m의 중간 계층을 출구로 하여 예측 결과를 출력데이터로 출력한다(S470). 이때, 트랜스포머 계층 m의 중간 계층 이란 트랜스포머 계층 m의 후단에 연결된 중간 계층을 의미한다. 본 발명의 일 실시예에 따르면, 언어 모델은 신뢰 수준과 신뢰 수준을 만족하는 연속된 중간 계층의 수를 조 정함으로써 평균 추론 계층을 쉽게 제어하고 모든 속도 향상 비율을 달성할 수 있기 때문에 유연하다. 따라서, PCEE-BERT는 최대 확률 질량과 같은 다양한 신뢰 측정을 채택할 수 있고, 추론 성능을 개선한 본 발명 은 다양한 백본 모델에서 일관되게 잘 수행되며 모델 압축 방법과 함께 작동하여 추론 속도를 높일 수 있다. 또 한, PCEE-BERT는 컴퓨터 비전 작업에서도 잘 수행되는 이점이 있다."}
{"patent_id": "10-2022-0123189", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 사전 훈련된 언어 모델을 도시한 개략도이다. 도 2는 본 발명의 일 실시예에 따른 전자장치의 구성을 도시한 블럭도이다. 도 3은 본 발명의 일 실시예에 따른 전자장치의 동작 흐름도를 도시한 도면이다. 도 4는 본 발명의 일 실시예에 따른 전자장치의 동작 흐름도를 도시한 도면이다."}
