{"patent_id": "10-2021-0087893", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0011078", "출원번호": "10-2021-0087893", "발명의 명칭": "능동적 인터랙션 방법, 장치, 전자 기기 및 판독 가능 기록 매체", "출원인": "베이징 바이두 넷컴 사이언스 앤 테크놀로지 코.,", "발명자": "쑤에, 양"}}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "능동적 인터랙션 방법에 있어서,실시간으로 촬영한 비디오를 취득하는 단계;상기 비디오의 각각의 이미지 프레임 내에서 시각 목표(visual target)를 추출하고, 각각의 시각 목표의 제1 특징 벡터(first feature vector)를 생성하는 단계;상기 비디오의 각각의 이미지 프레임에 대해, 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보(identification information)를 융합하여, 각각의 시각 목표의 제2 특징 벡터(second featurevector)를 생성하는 단계;동일한 식별 정보를 갖는 제2 특징 벡터를 각각 취합하여(aggregating), 각각의 이미지 프레임에 대응하는 제3특징 벡터(third feature vector)를 생성하는 단계; 및기 결정된 이미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행하는 것으로 결정한 후, 능동적인터랙션을 개시하는 단계;를 포함하는,능동적 인터랙션 방법."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하는 단계는:상기 비디오의 각각의 이미지 프레임 내에서 특정 목표를 추출하여 시각 목표로 설정하는 단계;를 포함하는,능동적 인터랙션 방법."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시각 목표의 제1 특징 벡터를 생성하는 단계는:이미지 프레임의 특징 맵(feature map)에 기반하여 시각 목표를 식별하는 단계;특징 맵 중에서 상기 시각 목표에 대응하는 특징 맵 서브 영역을 추출하고, 각각의 특징 맵 서브 영역을 크기가동일한 서브 특징 맵(sub-feature maps)으로 변환하는 단계; 및각 서브 특징 맵에 대해 전역 평균 풀링(global average pooling)을 실행한 후, 각각의 시각 목표의 제1 특징벡터를 얻는 단계;를 포함하는능동적 인터랙션 방법."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 각각의 시각 목표의 제1 특징 벡터를 얻은 후, 이미지 프레임의 중심을 원점으로 하는 2차원 좌표계에서,각각의 시각 목표의 이미지 프레임 내에서의 좌상단(upper left corner)의 좌표 및 우하단(lower rightcorner)의 좌표를 결정하는 단계;공개특허 10-2022-0011078-3-이미지 프레임 내의 각각의 시각 목표에 대응하는 좌표 범위 내에서 각각 복수의 점을 선택한 후, 각각의 시각목표의 2차원 평면에서의 위치 표현을 구축하는 단계; 및구축한 위치 표현을 소정의 차원의 위치 특징 벡터로 타일링(tiling)한 후, 각각의 시각 목표의 제1 특징 벡터와 스티칭(stitching)하는 단계;를 더 포함하는,능동적 인터랙션 방법."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 비디오의 각각의 이미지 프레임에 대해， 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 융합하여， 각각의 시각 목표의 제2 특징 벡터를 생성하는 단계는:비디오 중의 각각의 이미지 프레임에 대해, 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별정보를 사전에 구축한 신경망 모델(neural network model)에 입력하는 단계; 및상기 신경망 모델의 출력 결과를 시각 목표의 제2 특징 벡터로 설정하는 단계;를 포함하며,상기 신경망 모델은 복수의 디코더 블록을 포함하고, 각각의 디코더 블록은 자기주의(self-attention) 계층 및 피드포워드(feed forward) 계층을 포함하는,능동적 인터랙션 방법."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 능동적 인터랙션을 개시하는 단계는:각 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 취득하는 단계;기 결정된 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터에 기반하여, 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정하는 단계; 및결정된 다중 모드 인터랙션 방식을 이용하여 능동적 인터랙션을 실행하는 단계;를 포함하는,능동적 인터랙션 방법."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 취득하는 단계는:사전 트레이닝된 언어 모델을 사용하여 각각의 인터랙션 문구를 나타내는 시맨틱 벡터(semantic vector)를 취득하는 단계;각 인터랙션 표정 및 각각의 인터랙션 동작을 각각 나타내는 원-핫 코드(one-hot code)를 취득하는 단계;서로 다른 인터랙션 언어, 인터랙션 표정 및 인터랙션 동작을 사용하여 서로 다른 다중 모드 인터랙션 방식을구축하는 단계; 및각 다중 모드 인터랙션 방식에 대응하는 시맨틱 벡터 및 원-핫 코드를 스티칭한 후, 완전 연결 네트워크에 입력하여 출력 결과를 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터로 설정하는 단계;를 포함하는,능동적 인터랙션 방법."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,공개특허 10-2022-0011078-4-상기 기 설정된 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터에 기반하여, 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정하는 단계는:기 결정된 이미지 프레임의 제3 특징 벡터와 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 각각 곱한 후, 사전 트레이닝을 통해 얻은 제2 판별 모델에 입력하는 단계; 및상기 제2 판별 모델의 출력 결과에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을결정하는 단계;를 포함하는,능동적 인터랙션 방법."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "능동적 인터랙션 장치에 있어서,실시간으로 촬영한 비디오를 취득하기 위한 취득 유닛;상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시각 목표의 제1 특징 벡터를 생성하기 위한 제1 생성 유닛;상기 비디오의 각각의 이미지 프레임에 대해 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 융합하여 각각의 시각 목표의 제2 특징 벡터를 생성하기 위한 제2 생성 유닛;동일한 식별 정보를 갖는 제2 특징 벡터를 각각 취합하여 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 생성하기 위한 제3 생성 유닛; 및기 결정된 이미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행하는 것으로 결정한 후, 능동적인터랙션을 개시하기 위한 인터랙션 유닛을 구비하는,능동적 인터랙션 장치."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 취득 유닛은 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출할 때,상기 비디오의 각각의 이미지 프레임 내에서 특정 목표를 추출하여 시각 목표로 설정하는,능동적 인터랙션 장치."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 제1 생성 유닛은 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시각 목표의제1 특징 벡터를 생성할 때,이미지 프레임의 특징 맵에 기반하여 시각 목표를 식별하고;특징 맵 중에서 상기 시각 목표에 대응하는 특징 맵 서브 영역을 추출하고, 각각의 특징 맵 서브 영역을 크기가동일한 서브 특징 맵으로 변환하며;각 서브 특징 맵에 대해 전역 평균 풀링을 실행한 후, 각각의 시각 목표의 제1 특징 벡터를 얻는,능동적 인터랙션 장치."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,상기 제1 생성 유닛은:각 시각 목표의 제1 특징 벡터를 얻은 후, 이미지 프레임의 중심을 원점으로 하는 2차원 좌표계에서, 각각의 시공개특허 10-2022-0011078-5-각 목표의 이미지 프레임 내에서의 좌상단의 좌표 및 우하단의 좌표를 결정하는 것;이미지 프레임 내의 각각의 시각 목표에 대응하는 좌표 범위 내에서 각각 복수의 점을 선택한 후, 각각의 시각목표의 2차원 평면에서의 위치 표현을 구축하는 것; 및구축한 위치 표현을 소정의 차원의 위치 특징 벡터로 타일링한 후, 각각의 시각 목표의 제1 특징 벡터와 스티칭하는 것을 수행하는,능동적 인터랙션 장치."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,상기 제2 생성 유닛은 상기 비디오의 각각의 이미지 프레임에 대해 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 융합하여 각각의 시각 목표의 제2 특징 벡터를 생성할 때,비디오 중의 각각의 이미지 프레임에 대해, 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별정보를 사전에 구축한 신경망 모델에 입력하고;상기 신경망 모델의 출력 결과를 시각 목표의 제2 특징 벡터로 설정하며,여기서, 상기 신경망 모델은 복수의 디코더 블록을 포함하고, 각각의 디코더 블록은 자기주의 계층 및 피드포워드 계층을 포함하는,능동적 인터랙션 장치."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제9항에 있어서,상기 인터랙션 유닛은 능동적 인터랙션을 개시할 때,각 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 취득하고;기 결정된 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정하며,결정된 다중 모드 인터랙션 방식을 이용하여 능동적 인터랙션을 실행하는,능동적 인터랙션 장치."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 인터랙션 유닛은 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 취득할 때,사전 트레이닝 된 언어 모델을 사용하여 각각의 인터랙션 문구를 나타내는 시맨틱 벡터를 취득하고;각 인터랙션 표정 및 각각의 인터랙션 동작을 각각 나타내는 원-핫 코드를 취득하며;서로 다른 인터랙션 언어, 인터랙션 표정 및 인터랙션 동작을 사용하여 서로 다른 다중 모드 인터랙션 방식을구축하고;각 다중 모드 인터랙션 방식에 대응하는 시맨틱 벡터 및 원-핫 코드를 스티칭한 후, 완전 연결 네트워크에 입력하여 출력 결과를 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터로 설정하는,능동적 인터랙션 장치."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서,상기 인터랙션 유닛은 기 결정된 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정할 때 구체적공개특허 10-2022-0011078-6-으로,기 결정된 이미지 프레임의 제3 특징 벡터와 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 각각 곱한 후, 사전 트레이닝을 통해 얻은 제2 판별 모델에 입력하고;상기 제2 판별 모델의 출력 결과에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을결정하는,능동적 인터랙션 장치."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "전자 기기에 있어서,적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며,상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기록되어 있으며, 상기 명령이 상기적어도 하나의 프로세서에 의해 수행되어 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제8항 중 어느 한항에 기재된 방법을 수행하도록 하는,전자 기기."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "컴퓨터 명령이 기록되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서,상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제8항 중 어느 한 항에 기재된 방법을 수행하도록 하는,기록 매체."}
{"patent_id": "10-2021-0087893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "비 일시적 컴퓨터 판독 가능 기록 매체에 기록되어 있는 프로그램에 있어서,상기 프로그램은 상기 컴퓨터로 하여금 제1항 내지 제8항 중 어느 한 항에 기재된 방법을 수행하도록 하는,비 일시적 컴퓨터 판독 가능 기록 매체에 기록되어 있는 프로그램."}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 능동적 인터랙션 방법, 장치, 전자 기기 및 판독 가능 기록 매체를 개시하였는 바, 심층 학습 및 이미 지 처리"}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것이다. 본 발명의 능동적 인터랙션을 실행할 때 채용하는 실현 방안은, 실시간으로 촬영한 비디오를 취득하는 단계; 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시 (뒷면에 계속) 대 표 도 - 도1"}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2022-0011078 각 목표의 제1 특징 벡터를 생성하는 단계; 상기 비디오의 각각의 이미지 프레임에 대해 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 융합하여 각각의 시각 목표의 제2 특징 벡터를 생성하 는 단계; 동일한 식별 정보를 갖는 제2 특징 벡터를 각각 취합하여 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 생성하는 단계; 및 소정의 이미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행하는 것으 로 결정한 후, 능동적 인터랙션을 개시하는 것을 포함한다. 본 발명은 능동적 인터랙션의 정확성 및 지능성을 향 상시킬 수 있다.명 세 서 청구범위 청구항 1 능동적 인터랙션 방법에 있어서, 실시간으로 촬영한 비디오를 취득하는 단계; 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표(visual target)를 추출하고, 각각의 시각 목표의 제1 특 징 벡터(first feature vector)를 생성하는 단계; 상기 비디오의 각각의 이미지 프레임에 대해, 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레 임의 식별 정보(identification information)를 융합하여, 각각의 시각 목표의 제2 특징 벡터(second feature vector)를 생성하는 단계; 동일한 식별 정보를 갖는 제2 특징 벡터를 각각 취합하여(aggregating), 각각의 이미지 프레임에 대응하는 제3 특징 벡터(third feature vector)를 생성하는 단계; 및 기 결정된 이미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행하는 것으로 결정한 후, 능동적 인터랙션을 개시하는 단계; 를 포함하는, 능동적 인터랙션 방법. 청구항 2 제1항에 있어서, 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하는 단계는: 상기 비디오의 각각의 이미지 프레임 내에서 특정 목표를 추출하여 시각 목표로 설정하는 단계;를 포함하는, 능동적 인터랙션 방법. 청구항 3 제1항에 있어서, 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시각 목표의 제1 특징 벡터를 생성 하는 단계는: 이미지 프레임의 특징 맵(feature map)에 기반하여 시각 목표를 식별하는 단계; 특징 맵 중에서 상기 시각 목표에 대응하는 특징 맵 서브 영역을 추출하고, 각각의 특징 맵 서브 영역을 크기가 동일한 서브 특징 맵(sub-feature maps)으로 변환하는 단계; 및 각 서브 특징 맵에 대해 전역 평균 풀링(global average pooling)을 실행한 후, 각각의 시각 목표의 제1 특징 벡터를 얻는 단계;를 포함하는 능동적 인터랙션 방법. 청구항 4 제3항에 있어서, 상기 각각의 시각 목표의 제1 특징 벡터를 얻은 후, 이미지 프레임의 중심을 원점으로 하는 2차원 좌표계에서, 각각의 시각 목표의 이미지 프레임 내에서의 좌상단(upper left corner)의 좌표 및 우하단(lower right corner)의 좌표를 결정하는 단계;이미지 프레임 내의 각각의 시각 목표에 대응하는 좌표 범위 내에서 각각 복수의 점을 선택한 후, 각각의 시각 목표의 2차원 평면에서의 위치 표현을 구축하는 단계; 및 구축한 위치 표현을 소정의 차원의 위치 특징 벡터로 타일링(tiling)한 후, 각각의 시각 목표의 제1 특징 벡터 와 스티칭(stitching)하는 단계; 를 더 포함하는, 능동적 인터랙션 방법. 청구항 5 제1항에 있어서, 상기 비디오의 각각의 이미지 프레임에 대해， 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프 레임의 식별 정보를 융합하여， 각각의 시각 목표의 제2 특징 벡터를 생성하는 단계는: 비디오 중의 각각의 이미지 프레임에 대해, 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 사전에 구축한 신경망 모델(neural network model)에 입력하는 단계; 및 상기 신경망 모델의 출력 결과를 시각 목표의 제2 특징 벡터로 설정하는 단계;를 포함하며, 상기 신경망 모델은 복수의 디코더 블록을 포함하고, 각각의 디코더 블록은 자기주의(self-attention) 계층 및 피드포워드(feed forward) 계층을 포함하는, 능동적 인터랙션 방법. 청구항 6 제1항에 있어서, 상기 능동적 인터랙션을 개시하는 단계는: 각 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 취득하는 단계; 기 결정된 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터에 기반하 여, 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정하는 단계; 및 결정된 다중 모드 인터랙션 방식을 이용하여 능동적 인터랙션을 실행하는 단계;를 포함하는, 능동적 인터랙션 방법. 청구항 7 제6항에 있어서, 상기 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 취득하는 단계는: 사전 트레이닝된 언어 모델을 사용하여 각각의 인터랙션 문구를 나타내는 시맨틱 벡터(semantic vector)를 취득 하는 단계; 각 인터랙션 표정 및 각각의 인터랙션 동작을 각각 나타내는 원-핫 코드(one-hot code)를 취득하는 단계; 서로 다른 인터랙션 언어, 인터랙션 표정 및 인터랙션 동작을 사용하여 서로 다른 다중 모드 인터랙션 방식을 구축하는 단계; 및 각 다중 모드 인터랙션 방식에 대응하는 시맨틱 벡터 및 원-핫 코드를 스티칭한 후, 완전 연결 네트워크에 입력 하여 출력 결과를 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터로 설정하는 단계;를 포함하는, 능동적 인터랙션 방법. 청구항 8 제6항에 있어서,상기 기 설정된 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터에 기 반하여, 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정하는 단계는: 기 결정된 이미지 프레임의 제3 특징 벡터와 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 각각 곱 한 후, 사전 트레이닝을 통해 얻은 제2 판별 모델에 입력하는 단계; 및 상기 제2 판별 모델의 출력 결과에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정하는 단계;를 포함하는, 능동적 인터랙션 방법. 청구항 9 능동적 인터랙션 장치에 있어서, 실시간으로 촬영한 비디오를 취득하기 위한 취득 유닛; 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시각 목표의 제1 특징 벡터를 생성 하기 위한 제1 생성 유닛; 상기 비디오의 각각의 이미지 프레임에 대해 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레 임의 식별 정보를 융합하여 각각의 시각 목표의 제2 특징 벡터를 생성하기 위한 제2 생성 유닛; 동일한 식별 정보를 갖는 제2 특징 벡터를 각각 취합하여 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 생 성하기 위한 제3 생성 유닛; 및 기 결정된 이미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행하는 것으로 결정한 후, 능동적 인터랙션을 개시하기 위한 인터랙션 유닛 을 구비하는, 능동적 인터랙션 장치. 청구항 10 제9항에 있어서, 상기 취득 유닛은 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출할 때, 상기 비디오의 각각의 이미지 프레임 내에서 특정 목표를 추출하여 시각 목표로 설정하는, 능동적 인터랙션 장치. 청구항 11 제9항에 있어서, 상기 제1 생성 유닛은 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시각 목표의 제1 특징 벡터를 생성할 때, 이미지 프레임의 특징 맵에 기반하여 시각 목표를 식별하고; 특징 맵 중에서 상기 시각 목표에 대응하는 특징 맵 서브 영역을 추출하고, 각각의 특징 맵 서브 영역을 크기가 동일한 서브 특징 맵으로 변환하며; 각 서브 특징 맵에 대해 전역 평균 풀링을 실행한 후, 각각의 시각 목표의 제1 특징 벡터를 얻는, 능동적 인터랙션 장치. 청구항 12 제9항에 있어서, 상기 제1 생성 유닛은: 각 시각 목표의 제1 특징 벡터를 얻은 후, 이미지 프레임의 중심을 원점으로 하는 2차원 좌표계에서, 각각의 시각 목표의 이미지 프레임 내에서의 좌상단의 좌표 및 우하단의 좌표를 결정하는 것; 이미지 프레임 내의 각각의 시각 목표에 대응하는 좌표 범위 내에서 각각 복수의 점을 선택한 후, 각각의 시각 목표의 2차원 평면에서의 위치 표현을 구축하는 것; 및 구축한 위치 표현을 소정의 차원의 위치 특징 벡터로 타일링한 후, 각각의 시각 목표의 제1 특징 벡터와 스티칭 하는 것을 수행하는, 능동적 인터랙션 장치. 청구항 13 제9항에 있어서, 상기 제2 생성 유닛은 상기 비디오의 각각의 이미지 프레임에 대해 각각의 시각 목표의 제1 특징 벡터 및 자신 이 속하는 이미지 프레임의 식별 정보를 융합하여 각각의 시각 목표의 제2 특징 벡터를 생성할 때, 비디오 중의 각각의 이미지 프레임에 대해, 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 사전에 구축한 신경망 모델에 입력하고; 상기 신경망 모델의 출력 결과를 시각 목표의 제2 특징 벡터로 설정하며, 여기서, 상기 신경망 모델은 복수의 디코더 블록을 포함하고, 각각의 디코더 블록은 자기주의 계층 및 피드포워 드 계층을 포함하는, 능동적 인터랙션 장치. 청구항 14 제9항에 있어서, 상기 인터랙션 유닛은 능동적 인터랙션을 개시할 때, 각 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 취득하고; 기 결정된 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터에 기반하 여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정하며, 결정된 다중 모드 인터랙션 방식을 이용하여 능동적 인터랙션을 실행하는, 능동적 인터랙션 장치. 청구항 15 제14항에 있어서, 상기 인터랙션 유닛은 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 취득할 때, 사전 트레이닝 된 언어 모델을 사용하여 각각의 인터랙션 문구를 나타내는 시맨틱 벡터를 취득하고; 각 인터랙션 표정 및 각각의 인터랙션 동작을 각각 나타내는 원-핫 코드를 취득하며; 서로 다른 인터랙션 언어, 인터랙션 표정 및 인터랙션 동작을 사용하여 서로 다른 다중 모드 인터랙션 방식을 구축하고; 각 다중 모드 인터랙션 방식에 대응하는 시맨틱 벡터 및 원-핫 코드를 스티칭한 후, 완전 연결 네트워크에 입력 하여 출력 결과를 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터로 설정하는, 능동적 인터랙션 장치. 청구항 16 제14항에 있어서, 상기 인터랙션 유닛은 기 결정된 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하 는 특징 벡터에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정할 때 구체적으로, 기 결정된 이미지 프레임의 제3 특징 벡터와 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 각각 곱 한 후, 사전 트레이닝을 통해 얻은 제2 판별 모델에 입력하고; 상기 제2 판별 모델의 출력 결과에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정하는, 능동적 인터랙션 장치. 청구항 17 전자 기기에 있어서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기록되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 수행되어 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제8항 중 어느 한 항에 기재된 방법을 수행하도록 하는, 전자 기기. 청구항 18 컴퓨터 명령이 기록되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제8항 중 어느 한 항에 기재된 방법을 수행하도록 하는, 기록 매체. 청구항 19 비 일시적 컴퓨터 판독 가능 기록 매체에 기록되어 있는 프로그램에 있어서, 상기 프로그램은 상기 컴퓨터로 하여금 제1항 내지 제8항 중 어느 한 항에 기재된 방법을 수행하도록 하는, 비 일시적 컴퓨터 판독 가능 기록 매체에 기록되어 있는 프로그램. 발명의 설명 기 술 분 야 본 발명은 인공 지능 기술(artificial intelligence technology) 분야에 관한 것으로, 특히 이미지 처리 기술 (image processing technology) 분야의 방법, 장치, 전자 기기 및 판독 가능 기록 매체에 관한 것이다."}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "기존의 능동적 인터랙션의 종래의 해결안은 인체 검출, 얼굴 검출 등 방법을 사용하여, 소정의 규칙에 따라 간 단한 인터랙션 논리를 트리거 하여 능동적 인터랙션을 실현한다. 종래의 해결안은 일부 사회학 중의 사람과 사람 사이의 인터랙션의 기준을 융합하여 규칙으로 설정하지만, 감지 한 신호의 단일한 차원에 국한되어 복잡한 규칙을 설정하기 어렵기에, 간단한 인사만 가능할 뿐, 실제의 복잡한 시나리오의 복수의 종류의 인터랙션 의도에 대해 피드백을 제공할 수 없다. 특히 여러 사람의 시나리오에 대해, 규칙에 의해 구동되는 방법은 제일 중요한 인터랙션 가능 대상을 발견하기 매우 어려우며, 보행자를 방해하는 정황 등을 초래할 수 있으며, 따라서 능동적 인터랙션의 정확성(accuracy) 및 지능성(intelligence)이 모두 비 교적 낮다."}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 기술 문제를 해결하기 위하여 채용하는 기술안은 능동적 인터랙션 방법을 제공한다."}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "당해 방법은, 실시간으로 촬영한 비디오를 취득하는 단계; 상기 비디오의 각각의 이미지 프레임 내에서 시각 목 표를 추출하고, 각각의 시각 목표의 제1 특징 벡터를 생성하는 단계; 상기 비디오의 각각의 이미지 프레임에 대 해 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 융합하여 각각의 시각 목 표의 제2 특징 벡터를 생성하는 단계; 동일한 식별 정보를 갖는 제2 특징 벡터를 각각 취합하여 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 생성하는 단계; 및 기 결정된 이미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행하는 것으로 결정한 후, 능동적 인터랙션을 개시하는 것을 포함한다. 본 발명이 기술 문제를 해결하기 위하여 채용하는 기술안은 능동적 인터랙션 장치를 제공하는 바, 당해 장치는, 실시간으로 촬영한 비디오를 취득하기 위한 취득 유닛; 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표 를 추출하고, 각각의 시각 목표의 제1 특징 벡터를 생성하기 위한 제1 생성 유닛; 상기 비디오의 각각의 이미지 프레임에 대해 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 융합하여 각 각의 시각 목표의 제2 특징 벡터를 생성하기 위한 제2 생성 유닛; 동일한 식별 정보를 갖는 제2 특징 벡터를 각 각 취합하여 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 생성하기 위한 제3 생성 유닛; 및 기 결정된 이 미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행하는 것으로 결정한 후, 능동적 인터랙션을 개 시하기 위한 인터랙션 유닛을 구비한다. 전자 기기에 있어서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하 며, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기록되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 수행되어 상기 적어도 하나의 프로세서로 하여금 상기 방법을 실행하도록 한다. 컴퓨터 명령이 기록되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨 터로 하여금 상기 방법을 실행하도록 한다."}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예는 아래의 이점 또는 유익한 효과를 가진다. 본 발명은 능동적 인터랙션의 정확성 및 지능 성을 향상시킬 수 있다. 인터랙션 대상의 시각 정보를 분석하고, 인터랙션 대상에 대해 시공간 모델을 구축하여 인터랙션 대상의 인터랙션 의도를 분석하는 기술 수단을 채용하였기에, 종래 기술에서 간단한 인터랙션 논리에 만 기반하여 능동적 인터랙션을 트리거 함으로써 초래하는 정확성 및 지능성이 낮아지는 기술 문제를 극복하였 으며, 능동적 인터랙션의 정확성 및 지능성을 향상시키는 기술 효과를 실현하였다. 상기 선택적인 방식이 가지는 기타 효과는 아래에서 구체적인 실시예를 참조하여 설명하기로 한다."}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 발명의 시범적인 실시예를 설명하는 바, 본 발명에 대한 이해를 돕기 위해 여기에는 본 발명 실시예의 다양한 세부 사항이 포함되며, 이러한 세부 사항을 단지 시범적인 것으로 간주해야 할 것이다. 따라서, 당업자는 본 발명의 범위 및 정신을 벗어나지 않는 전제 하에서, 여기서 설명되는 실시예에 대 해 다양한 변경 및 수정을 수행할 수 있음을 인식해야 한다. 마찬가지로, 명확성 및 간결성을 위하여 이하의 설명에서는 잘 알려진 기능 및 구조의 설명을 생략하였다. 도 1은 본 발명에 따른 제1 실시예의 모식도이다. 도 1에 나타낸 바와 같이, 본 실시예의 능동적 인터랙션 방법 은 구체적으로 아래의 것을 포함할 수 있다. S101에 있어서, 실시간으로 촬영한 비디오를 취득한다. S102에 있어서, 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표(visual target)를 추출하고, 각각의 시 각 목표의 제1 특징 벡터(first feature vector)를 생성한다. S103에 있어서, 상기 비디오의 각각의 이미지 프레임에 대해 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하 는 이미지 프레임의 식별 정보를 융합하여 각각의 시각 목표의 제2 특징 벡터(second feature vector)를 생성한 다. S104에 있어서, 동일한 식별 정보를 갖는 제2 특징 벡터를 각각 취합하여 각각의 이미지 프레임에 대응하는 제3 특징 벡터(third feature vector)를 생성한다. S105에 있어서, 소정의 이미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행하는 것으로 결정한 후, 능동적 인터랙션을 개시한다. 본 실시예의 능동적 인터랙션 방법의 수행 주체는 지능 로봇이며, 인터랙션 대상의 시각 정보를 분석하고, 인터 랙션 대상에 대해 시공간 모델을 구축(time-space modeling)함으로써, 인터랙션 대상의 인터랙션 의도를 충분히 분석할 수 있고, 능동적 인터랙션의 지능성 및 정확성을 향상시킬 수 있다. 본 실시예는 S101을 수행하여 실시간으로 촬영한 비디오를 취득할 때, 지능 로봇을 통해 자신이 휴대한 촬영 장 치를 이용하여 실시간으로 촬영한 비디오를 취득할 수 있으며, 또한 지능 로봇을 통해 기타 촬영 장치가 실시간 으로 촬영한 비디오를 취득할 수 있다. 본 실시예는 S101을 수행하여 실시간으로 촬영한 비디오를 취득한 후, S102를 수행하여 비디오에 포함된 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시각 목표에 대응하는 제1 특징 벡터를 생성한다. 여기서, 본 실 시예에 있어서, S102를 수행하여 이미지 프레임 내에서 추출한 시각 목표는 이미지 프레임 내의 특정 목표이며, 이미지 프레임 내의 사람, 배낭, 여행 가방 및 핸드폰과 같은 4개의 유형의 목표이다. 이는 실제 접수원이 사람의 배낭 유형, 여행 가방이 있는지 여부 등의 시각 정보에 기반하여 당해 사람이 건물 내의 직원인지 외부 방문객인지를 판단하고, 또한 핸드폰은 종종 사람의 상태를 반영할 수 있기 때문이다. 예를 들면 서둘러 도킹하는 사람과 연계하거나, 또는 로봇을 촬영할 수 있다. 따라서, 본 실시예에 있어서, 상기 능 동적 인터랙션에 대해 영향을 줄 수 있는 목표를 특정 목표로 설정함으로써, S102를 통해 이미지 프레임 내의 특정 목표만을 추출함으로써, 무관한 시각 목표(irrelevant visual target)의 추출을 회피할 수 있으며, 지능 로봇 능동적 인터랙션의 정확성을 더 한층 향상시킬 수 있다. 계산 과정의 복잡도를 줄이기 위하여, 본 실시예는 S102를 수행하여 비디오의 각각의 이미지 프레임 내에서 시 각 목표를 추출할 때, 추출하는 시각 목표의 수량을 설정할 수 있는 바, 예를 들면 각각의 이미지 프레임 내에 서 2개의 시각 목표를 추출함을 이해할 수 있다. 구체적으로, 본 실시예는 S102를 수행하여 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시각 목표의 제1 특징 벡터를 생성할 때 채용할 수 있는 선택적인 실현 방식은, 이미지 프레임의 특징 맵에 기반하여 시각 목표를 식별하는 바, 예를 들면 컨볼루션 신경망(convolutional neural networks) CSPDarknet53을 사용하여 크기가 13×13×512인 특징 맵을 얻은 후, 다시 SPP+YOLO Head 구조에 기반한 목표 검출 모델을 사용하여 이미지 프레임 내의 시각 목표의 경계 박스를 식별하고; 특징 맵 중에서 시각 목표에 대 응하는 특징 맵 서브 영역을 추출하며, 각각의 특징 맵 서브 영역을 크기가 동일한 서브 특징 맵으로 변환하는 바, 예를 들면 ROI Align 기술을 사용하여 특징 맵 중에서 대응하는 수용 필드 내의 특징 서브 맵 영역을 추출 한 후, 보간 또는 다운 샘플링을 통해 크기가 서로 다른 특징 맵 서브 영역을 크기가 5×5×512인 서브 특징 맵 으로 변환하며; 각각의 서브 특징 맵에 대해 전역 평균 풀링(GAP, Global Average Polling)을 실행한 후, 각각 의 시각 목표에 대응하는 제1 특징 벡터를 얻는 바, 예를 들면 표현 시각 목표 정보의 512 차원의 특징 벡터를 얻는다. 그러나, 시각 목표의 그림 영역을 특징 서브 맵으로 표현한 후, 시각 목표의 이미지 프레임 내의 위치 및 크기 정보가 분실된다. 때문에 상기 정보의 분실을 회피하기 위하여, 본 실시예는 S102를 수행하여 각각의 시각 목표의 제1 특징 벡터를 생성할 때, 또한 이하의 내용을 포함할 수 있는 바, 즉, 이미지 프레임의 중심을 원점으로 하는 2차원 좌표계에서, 각각의 시각 목표의 이미지 프레임 내에서의 좌상단의 좌표 (Xmin, Ymin) 및 우하단의 좌표 (Xmax, Ymax)를 결정하는 단계; 이미지 프레임 내의 각각의 시각 목표에 대응하는 [Xmin, Xmax] 및 [Ymin, Ymax] 범위 내에서 복수의 점을 각각 선택한 후, 각각의 시각 목표의 2차원 평면에서의 위치 표현을 구축하는 바, 예를 들면 각각의 범위 내에서 5개의 점 (시각 목표의 서브 특징 맵의 크기에 대응함)을 균일하게 추출하고, 2차원 평면 5×5×2의 위치 행렬을 구축하는 단계; 구축한 위치 표현을 소정의 차원의 위치 특징 벡 터로 타일링(tiling)한 후, 예를 들면 Flatten 함수를 사용하여 위치 표현을 50 차원의 특징 벡터로 타일링하여, 시각 목표의 이미지 프레임 내의 위치 표현으로 설정하며, 당해 위치 표현은 시각 특징의 이미지 프레임 중심에 대한 위치 및 크기 정보를 나타내기 위한 단계; 및 얻은 위치 특징 벡터와 시각 목표의 제1 특징 벡터를 스티칭(stitching)하는 바, 예를 들면 특징 맵 중에서 얻은 512 차원의 제1 특징 벡터와 50 차원의 위치 특징 벡터를 스티칭하여, 시각 목표에 대응하는 562 차원의 특징 벡터를 얻는 것을 포함한다. 본 실시예는 S102를 수행하여 각각의 시각 목표의 제1 특징 벡터를 생성한 후, S103을 수행하여 비디오 중의 각 각의 이미지 프레임에 대해, 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 융합하여 각각의 시각 목표의 제2 특징 벡터를 생성한다. 여기서, 본 실시예에 있어서, 자신이 속하는 이미지 프레임의 식별 정보는 각각의 시각 목표가 속하는 이미지 프레임 및 프레임 사이의 상대 위치(relative position)를 기록하기 위하여 사용된다. 능동적 인터랙션을 개시할 때, 흥미를 갖는 인터랙션 대상을 발견할 필요가 있는 것 외에, 인터랙션 대상의 과 거의 일정한 시간 내의 행위에 기반하여 능동적 인터랙션을 개시할지 여부를 결정할 필요가 있으며, 따라서 동 일한 인터랙션 대상의 과거의 일정한 시간의 행위 특징을 추출할 필요가 있다. 본 실시예는 S102를 수행할 때, 자기주의(self-attention) 메커니즘을 도입하여 이미지 프레임 내의 시각 목표 에 대해 시공간 모델을 구축할 수 있으며, 따라서 시각 목표의 시간 정보와 공간 정보를 융합하여 시각 목표에 대응하는 제2 특징 벡터를 얻으며, 얻은 당해 제2 특징 벡터는 시각 목표의 과거 행위 특징(historical behavior feature)을 포함한다. 본 실시예에 있어서, S103을 수행하여 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식 별 정보를 융합하여 각각의 시각 목표의 제2 특징 벡터를 생성할 때 채용할 수 있는 선택적인 실현 방식은, 비 디오 중의 각각의 이미지 프레임에 대해, 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 사전에 구축한 신경망 모델(neural network model)에 입력하고, 당해 신경망 모델은 복수의 디코더 블록 (Decoder Block)을 포함하고, 각각의 디코더 블록은 자기주의 계층 및 피드포워드 계층을 포함하며; 상기 신경 망 모델의 출력 결과를 시각 목표의 제2 특징 벡터로 설정한다. 여기서, 본 실시예에 있어서, 자기주의 계층은 아래의 계산식을 사용하여 주의력 값을 계산한다."}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "계산식에 있어서, 는 계산하여 얻은 주의력 값을 나타내고; 는 서로 동일한 바, 입력 된 제1 특징 벡터 및 이미지 프레임의 식별 정보로 구성된 벡터이며; 는 입력된 벡터의 차원을 나타낸다. 따 라서, 본 실시예에 있어서, 디코더 블록 중의 자기주의 계층은 상기 계산식을 통해 입력된 벡터의 주의력 값을 얻은 후, 피드포워드 계층을 통해 다음의 디코더 블록 중의 자기주의 계층에 전달하여 계산을 실행하며, 이렇게 지속적으로 실행하여, 마지막 디코더 블록 중의 피드포워드 계층의 출력을 시각 목표의 제2 특징 벡터로 설정한 다. 본 실시예는 S103을 수행하여 각각의 시각 목표의 제2 특징 벡터를 생성한 후, S104를 수행하여 동일한 식별 정 보를 갖는 제2 특징 벡터를 각각 취합하여 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 생성한다. 다시 말하면, 본 실시예에 있어서, S104를 수행하여 동일한 이미지 프레임의 시각 목표에 속하는 제2 특징 벡터 를 1 그룹으로 설정하고, 최대 풀링(max pooling)의 방식을 통해 1 그룹 중에 포함된 복수의 제2 특징 벡터를 취합함으로써(aggregating), 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 얻는다. 본 실시예는 S104를 수행하여 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 생성한 후, S105를 수행하여 소정의 이미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행할지 여부를 결정하고, 능동적 인터 랙션을 실행하는 것으로 결정한 후, 능동적 인터랙션을 개시한다. 본 실시예에 있어서, S105에 언급된 소정의 이미지 프레임은 취득한 비디오 중의 임의의 하나의 프레임 이미지 일 수 있으며; 슬라이딩 윈도우를 설정하고, 슬라이딩 윈도우 내에 위치한 마지막 하나의 프레임 이미지를 소정 의 이미지 프레임으로 설정할 수 있다. 예를 들면 슬라이딩 윈도우 내에 20개의 프레임 이미지가 포함되어 있으 면, 소정의 이미지 프레임은 당해 슬라이딩 윈도우 내의 마지막 하나의 프레임 이미지이다. 본 실시예는 S105를 수행하여 소정의 이미지 프레임의 제3 특징 벡터에 기반하여 결정 능동적 인터랙션을 실행 할 때 채용할 수 있는 선택적인 실현 방식은, 소정의 이미지 프레임의 제3 특징 벡터를 사전 트레이닝을 통해 얻은 제1 판별 모델에 입력하고; 제1 판별 모델의 출력 결과에 기반하여 능동적 인터랙션을 실행할지 여부를 결 정하는 바, 예를 들면 판별 모델이 “예”를 출력하면, 능동적 인터랙션을 실행하는 것으로 결정하고, “아니오 ”를 출력하면 능동적 인터랙션을 실행하지 않는다. 본 실시예에 있어서, S105에서 사용한 제1 판별 모델은 입력된 특징 벡터에 기반하여 능동적 인터랙션을 실행하 는지 여부의 판별 결과를 출력할 수 있음을 이해할 수 있다. 본 실시예에 의해 제공되는 상기 방법을 채용하면, 지능 로봇이 인터랙션 대상의 시각 정보를 분석하고, 인터랙 션 대상에 대해 시공간 모델을 구축함으로써, 인터랙션 대상의 인터랙션 의도를 충분히 분석할 수 있고, 따라서 능동적 인터랙션의 지능성 및 정확성을 향상시킬 수 있다. 도 2는 본 발명에 따른 제2 실시예의 모식도이다. 도 2에 나타낸 바와 같이, 본 실시예는 S105를 수행하여 능동 적 인터랙션을 개시할 때 구체적으로 아래의 단계를 포함할 수 있다. S201에 있어서, 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 취득한다. S202에 있어서, 소정의 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡 터에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정한다. S203에 있어서, 결정된 다중 모드 인터랙션 방식을 이용하여 능동적 인터랙션을 실행한다. 본 실시예에 언급된 다중 모드 인터랙션 방식은, 지능 로봇과 인터랙션 대상이 인터랙션을 실행할 때 사용한 인 터랙션 언어, 인터랙션 표정 및 인터랙션 동작인 바, 서로 다른 다중 모드 인터랙션 방식은 서로 다른 인터랙션 언어(different interactive languages), 인터랙션 표정(interactive expressions) 또는 인터랙션 동작 (interactive actions)을 포함한다. 본 실시예에 있어서, S201을 수행하여 취득한 각각의 다중 모드 인터랙션 동작에 대응하는 특징 벡터는 아래의 방식을 통해 얻은 거인 바, 즉 사전 트레이닝 된 언어 모델을 사용하여 각각의 인터랙션 문구를 나타내는 시맨 틱 벡터를 취득하는 바, 예를 들면 ERNIE(Enhanced Representation through kNowledge IntEgration)를 사용하 여 언어 모델을 사전 트레이닝 하는 단계; 각각의 인터랙션 표정 및 각각의 인터랙션 동작을 각각 나타내는 원- 핫 코드(one-hot code)를 취득하는 단계; 서로 다른 인터랙션 언어, 인터랙션 표정 및 인터랙션 동작을 사용하 여 서로 다른 다중 모드 인터랙션 방식을 구축하는 단계; 및 각각의 다중 모드 인터랙션 방식에 대응하는 시맨 틱 벡터 및 원-핫 코드를 스티칭한 후, 완전 연결 네트워크에 입력하여 출력 결과를 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터로 설정하는 것을 포함한다. 본 실시예에 있어서, S202를 수행하여 소정의 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방 식에 대응하는 특징 벡터에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정할 때 채용할 수 있는 선택적인 실현 방식은, 소정의 이미지 프레임의 제3 특징 벡터와 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 각각 곱한 후, 사전 트레이닝을 통해 얻은 제2 판별 모델에 입력하는 단계; 및 제 2 판별 모델의 출력 결과에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정하 는 바, 즉 제2 판별 모델은 입력에 기반하여 현재 시나리오에서의 각각의 다중 모드 인터랙션 방식의 점수를 출 력할 수 있으며, 따라서 점수가 높은 하나의 다중 모드 인터랙션 방식을 선택하여 능동적 인터랙션을 실행하는 것을 포함한다. 본 실시예에 있어서, S202에서 사용한 제2 판별 모델은 입력된 특징 벡터에 기반하여 각각의 다중 모드 인터랙 션 방식에 대응하는 점수를 출력한다. 본 실시예에 의해 제공되는 상기 방법을 채용하면, 지능 로봇이 능동적 인터랙션을 개시할 때, 또한 현재 시나 리오에 부합되는 다중 모드 인터랙션 방식을 선택함으로써, 개시하는 능동적 인터랙션이 더욱 섬세하도록 할 수 있으며, 사람과 사람 사이의 사회적 인터랙션에 더 가깝게 만들며, 따라서 능동적 인터랙션의 지능성을 향상시 킨다. 도 3은 본 발명에 따른 제3 실시예의 모식도이다. 도 3에 나타낸 바와 같이, 본 실시예의 능동적 인터랙션 장치 는 지능 로봇 내에 위치한다. 당해 장치는, 실시간으로 촬영한 비디오를 취득하기 위한 취득 유닛; 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시각 목표의 제1 특징 벡터를 생성 하기 위한 제1 생성 유닛; 상기 비디오의 각각의 이미지 프레임에 대해 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레 임의 식별 정보를 융합하여 각각의 시각 목표의 제2 특징 벡터를 생성하기 위한 제2 생성 유닛; 동일한 식별 정보를 갖는 제2 특징 벡터를 각각 취합하여 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 생 성하기 위한 제3 생성 유닛; 및 소정의 이미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행하는 것으로 결정한 후, 능동적 인터 랙션을 개시하기 위한 인터랙션 유닛을 구비한다. 취득 유닛은 실시간으로 촬영한 비디오를 취득할 때, 지능 로봇을 통해 자신이 휴대한 촬영 장치를 이용하 여 실시간으로 촬영한 비디오를 취득할 수 있으며, 또한 지능 로봇을 통해 기타 촬영 장치가 실시간으로 촬영한 비디오를 취득할 수 있다. 취득 유닛이 실시간으로 촬영한 비디오를 취득한 후, 제1 생성 유닛이 비디오에 포함된 이미지 프레 임 내에서 시각 목표를 추출하고, 각각의 시각 목표에 대응하는 제1 특징 벡터를 생성한다. 여기서, 제1 생성 유닛이미지 프레임 내에서 추출한 시각 목표는 이미지 프레임 내의 특정 목표이며, 포함 이미지 프레임 내 의 사람, 배낭, 여행 가방 및 핸드폰과 같은 4개의 유형의 목표이다. 계산 과정의 복잡도를 줄이기 위하여, 제1 생성 유닛은 비디오의 각각의 이미지 프레임 내에서 시각 목표 를 추출할 때, 추출하는 시각 목표의 수량을 설정할 수 있는 바, 예를 들면 각각의 이미지 프레임 내에서 2개의 시각 목표를 추출함을 이해할 수 있다. 구체적으로, 제1 생성 유닛이 상기 비디오의 각각의 이미지 프레임 내에서 시각 목표를 추출하고, 각각의 시각 목표의 제1 특징 벡터를 생성할 때 채용할 수 있는 선택적인 실현 방식은, 이미지 프레임의 특징 맵에 기 반하여 시각 목표를 식별하는 단계; 특징 맵 중에서 시각 목표에 대응하는 특징 맵 서브 영역을 추출하며, 각각 의 특징 맵 서브 영역을 크기가 동일한 서브 특징 맵으로 변환하는 단계; 및 각각의 서브 특징 맵에 대해 전역 평균 풀링을 실행한 후, 각각의 시각 목표에 대응하는 제1 특징 벡터를 얻는 것을 포함한다. 시각 목표의 그림 영역을 특징 서브 맵으로 표현한 후, 시각 목표의 이미지 프레임 내의 위치 및 크기 정보가 분실된다. 때문에 상기 정보의 분실을 회피하기 위하여, 제1 생성 유닛은 각각의 시각 목표의 제1 특징 벡 터를 생성한 후, 또한 이하의 내용을 포함할 수 있는 바, 즉, 이미지 프레임의 중심을 원점으로 하는 2차원 좌 표계에서, 각각의 시각 목표의 이미지 프레임 내에서의 좌상단의 좌표 (Xmin, Ymin)및 우하단의 좌표 (Xmax, Ymax)를 결정하는 단계; 이미지 프레임 내의 각각의 시각 목표에 대응하는 [Xmin, Xmax] 및 [Ymin, Ymax] 범위 내에서 복수의 점을 각각 선택한 후, 각각의 시각 목표의 2차원 평면에서의 위치 표현을 구축하는 단계; 및 구 축한 위치 표현을 소정의 차원의 위치 특징 벡터로 타일링한 후, 시각 목표의 제1 특징 벡터와 스티칭하는 것을 포함한다. 제1 생성 유닛이 각각의 시각 목표의 제1 특징 벡터를 생성한 후, 제2 생성 유닛이 비디오 중의 각각 의 이미지 프레임에 대해, 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 융합하여 각각의 시각 목표의 제2 특징 벡터를 생성한다. 여기서, 제2 생성 유닛에 속하는 이미지 프레임 의 식별 정보는 각각의 시각 목표가 속하는 이미지 프레임 및 프레임 사이의 상대 위치를 기록하기 위하여 사용 된다. 능동적 인터랙션을 개시할 때, 흥미를 갖는 인터랙션 대상을 발견해야 하는 것 외에, 인터랙션 대상의 과거의 일정한 시간 내의 행위에 기반하여 능동적 인터랙션을 개시할지 여부를 결정할 필요가 있으며, 따라서 동일한 인터랙션 대상의 과거의 일정한 시간의 행위 특징을 추출할 필요가 있다.제2 생성 유닛은 자기주의(self-attention) 메커니즘을 도입하여 이미지 프레임 내의 시각 목표에 대해 시 공간 모델을 구축할 수 있으며, 따라서 시각 목표의 시간 정보와 공간 정보를 융합하여 시각 목표에 대응하는 제2 특징 벡터를 얻으며, 얻은 당해 제2 특징 벡터는 시각 목표의 과거 행위 특징을 포함한다. 제2 생성 유닛이 각각의 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 융합 하여 각각의 시각 목표의 제2 특징 벡터를 생성할 때 채용할 수 있는 선택적인 실현 방식은, 비디오 중의 각각 의 이미지 프레임에 대해, 시각 목표의 제1 특징 벡터 및 자신이 속하는 이미지 프레임의 식별 정보를 사전에 구축한 신경망 모델에 입력하는 것이다. 당해 신경망 모델은 복수의 디코더 블록을 포함하고, 각각의 디코더 블 록은 자기주의 계층 및 피드포워드 계층을 포함하며; 상기 신경망 모델의 출력 결과를 시각 목표의 제2 특징 벡 터로 설정한다. 여기서, 제2 생성 유닛에서 자기주의 계층은 아래의 계산식을 사용하여 주의력 값을 계산한다."}
{"patent_id": "10-2021-0087893", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "계산식에 있어서, 는 계산하여 얻은 주의력 값을 나타내고; 는 서로 동일한 바, 입력 된 제1 특징 벡터 및 이미지 프레임의 식별 정보로 구성된 벡터이며; 는 입력된 벡터의 차원을 나타낸다. 따 라서, 본 실시예에 있어서, 디코더 블록 중의 자기주의 계층은 상기 계산식을 통해 입력된 벡터의 주의력 값을 얻은 후, 피드포워드 계층을 통해 다음의 디코더 블록 중의 자기주의 계층에 전달하여 계산을 실행하며, 이렇게 지속적으로 실행하여, 마지막 디코더 블록 중의 피드포워드 계층의 출력을 시각 목표의 제2 특징 벡터로 설정한 다. 제2 생성 유닛이 각각의 시각 목표의 제2 특징 벡터를 생성한 후, 제3 생성 유닛이 동일한 식별 정보 를 갖는 제2 특징 벡터를 각각 취합하여 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 생성한다. 다시 말하면, 제3 생성 유닛은 동일한 이미지 프레임의 시각 목표에 속하는 제2 특징 벡터를 1 그룹으로 설정하고, 최대 풀링(max pooling)의 방식을 통해 1 그룹 중에 포함된 복수의 제2 특징 벡터를 취합함으로써, 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 얻는다. 제3 생성 유닛이 각각의 이미지 프레임에 대응하는 제3 특징 벡터를 생성한 후, 인터랙션 유닛이 소 정의 이미지 프레임의 제3 특징 벡터에 기반하여 능동적 인터랙션을 실행할지 여부를 결정하고, 능동적 인터랙 션을 실행하는 것으로 결정한 후, 능동적 인터랙션을 개시한다. 인터랙션 유닛에서 언급된 소정의 이미지 프레임은 취득한 비디오 중의 임의의 하나의 프레임 이미지일 수 있으며; 슬라이딩 윈도우를 설정하고, 슬라이딩 윈도우 내에 위치한 마지막 하나의 프레임 이미지를 소정의 이 미지 프레임으로 설정할 수 있다. 예를 들면 슬라이딩 윈도우 내에 20개의 프레임 이미지가 포함되어 있으면, 소정의 이미지 프레임은 당해 슬라이딩 윈도우 내의 마지막 하나의 프레임 이미지이다. 인터랙션 유닛이 소정의 이미지 프레임의 제3 특징 벡터에 기반하여 결정 능동적 인터랙션을 실행할 때 채 용할 수 있는 선택적인 실현 방식은, 소정의 이미지 프레임의 제3 특징 벡터를 사전 트레이닝을 통해 얻은 제1 판별 모델에 입력하고; 제1 판별 모델의 출력 결과에 기반하여 능동적 인터랙션을 실행할지 여부를 결정하는 바, 예를 들면 판별 모델이 “예”를 출력하면, 능동적 인터랙션을 실행하는 것으로 결정하고, “아니오”를 출 력하면 능동적 인터랙션을 실행하지 않는다. 인터랙션 유닛에서 사용한 제1 판별 모델은 입력된 특징 벡터에 기반하여 능동적 인터랙션을 실행하는지 여부의 판별 결과를 출력할 수 있음을 이해할 수 있다. 인터랙션 유닛이 능동적 인터랙션을 실행할 때 채용할 수 있는 선택적인 실현 방식은, 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터를 취득하는 단계; 소정의 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙 션 방식을 결정하는 단계; 및 결정된 다중 모드 인터랙션 방식을 이용하여 능동적 인터랙션을 실행하는 것을 포 함한다. 인터랙션 유닛이 각각의 다중 모드 인터랙션 동작에 대응하는 특징 벡터를 취득할 때 채용할 수 있는 선택 적인 실현 방식은, 사전 트레이닝 된 언어 모델을 사용하여 각각의 인터랙션 문구를 나타내는 시맨틱 벡터를 취 득하는 단계; 각각의 인터랙션 표정 및 각각의 인터랙션 동작을 각각 나타내는 원-핫 코드를 취득하는 단계; 서 로 다른 인터랙션 언어, 인터랙션 표정 및 인터랙션 동작을 사용하여 서로 다른 다중 모드 인터랙션 방식을 구 축하는 단계; 및 각각의 다중 모드 인터랙션 방식에 대응하는 시맨틱 벡터 및 원-핫 코드를 스티칭한 후, 완전 연결 네트워크에 입력하여 출력 결과를 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터로 설정하는 것을 포함한다. 인터랙션 유닛이 소정의 이미지 프레임의 제3 특징 벡터 및 각각의 다중 모드 인터랙션 방식에 대응하는 특징 벡터에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정할 때 채용할 수 있는 선택적인 실현 방식은, 소정의 이미지 프레임의 제3 특징 벡터와 각각의 다중 모드 인터랙션 방식에 대응 하는 특징 벡터를 각각 곱한 후, 사전 트레이닝을 통해 얻은 제2 판별 모델에 입력하는 단계; 및 제2 판별 모델 의 출력 결과에 기반하여 능동적 인터랙션을 개시할 때 채용하는 다중 모드 인터랙션 방식을 결정하는 단계를 포함한다. 즉 제2 판별 모델은 입력에 기반하여 현재 시나리오에서의 각각의 다중 모드 인터랙션 방식의 점수를 출력할 수 있으며, 따라서 점수가 높은 하나의 다중 모드 인터랙션 방식을 선택하여 능동적 인터랙션을 실행한 다. 인터랙션 유닛에서 사용한 제2 판별 모델은 입력된 특징 벡터에 기반하여 각각의 다중 모드 인터랙션 방식 에 대응하는 점수를 출력할 수 있음을 이해할 수 있다. 본 발명의 실시예에 따르면, 본 발명은 전자 기기 및 컴퓨터 판독 가능 기록 매체를 더 제공한다. 도 4는 본 발명에 따른 실시예의 능동적 인터랙션 방법을 실현하는 전자 기기의 블럭도이다. 전자 기기는 예를 들면 랩톱 컴퓨터, 데스크톱 컴퓨터, 워크 스테이션, 개인 디지털 보조기, 서버, 블레이드 서버, 대형 컴퓨터 및 기타 적합한 컴퓨터와 같은 다양한 형태의 디지털 컴퓨터를 나타낸다. 전자 기기는 또한 예를 들면 개인 디 지털 처리기, 셀폰, 스마트 전화, 웨어러블 기기 및 기타 유사한 계산 장치와 같은 다양한 형태의 모바일 장치 를 나타낼 수 있다. 본 명세서에 나타낸 구성 요소, 이들의 연결과 관계 및 이들의 기능은 단지 예일 뿐이며, 본 명세서에서 설명하거나 및/또는 요구하는 본 발명의 실현을 한정하려는 것이 아니다. 도 4에 나타낸 바와 같이, 당해 전자 기기는 하나 또는 복수의 프로세서, 메모리 및 각각의 구성 요 소를 연결하기 위한 인터페이스를 구비하며, 당해 인터페이스는 고속 인터페이스 및 저속 인터페이스를 포함한 다. 각각의 구성 요소는 서로 다른 버스를 통해 상호 연결되며, 공통 마더 보드에 설치되거나 또는 수요에 따라 기타 방식으로 설치된다. 프로세서 전자 기기 내에서 수행되는 명령에 대해 처리를 실행할 수 있으며, 메모리 내에 기억되어 외부 입력/출력 장치 (예를 들면 인터페이스에 연결된 디스플레이 기기) 상에 GUI의 그래픽 정보 를 표시하기 위한 명령을 포함한다. 기타 실시 방식에 있어서, 필요할 경우, 복수의 프로세서 및/또는 복수의 버스와 복수의 메모리를 함께 사용할 수 있다. 마찬가지로, 복수의 전자 기기를 연결할 수 있으며, 각각의 기기 는 부분적인 필요한 조작 (예를 들면, 서버 어레이, 일 그룹의 블레이드 서버, 또는 다중 프로세서 시스템)을 제공한다. 도 4에서는 하나의 프로세서의 예를 들었다. 메모리는 본 발명에 의해 제공되는 비 일시적 컴퓨터 판독 가능 기억 매체이다. 여기서, 상기 메모리에는 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 적어도 하나의 프로세서로 하여금 본 발명에 의해 제공되는 능동적 인터랙션 방법을 수행하도록 한다. 본 발명의 비 일시적 컴퓨터 판독 가능 기 억 매체는 컴퓨터 명령을 기억하며, 당해 컴퓨터 명령은 컴퓨터로 하여금 본 발명에 의해 제공되는 능동적 인터 랙션 방법을 수행하도록 한다. 메모리는 일종의 비 일시적 컴퓨터 판독 가능 기억 매체로서, 비 일시적 소프트웨어 프로그램을 기억하는 데 사용될 수 있는 바, 예를 들면 비 일시적 컴퓨터 수행 가능 프로그램 및 모듈, 본 발명 실시예 중의 능동적 인터랙션 방법 대응하는 프로그램 명령/모듈 (예를 들면, 도 3에 나타낸 취득 유닛, 제1 생성 유닛, 제2 생성 유닛, 제3 생성 유닛 및 인터랙션 유닛)을 기억하는데 사용될 수 있다. 프로세서(40 1)는 메모리 내에 기억된 비 일시적 소프트웨어 프로그램, 명령 및 모듈을 운행함으로써, 서버의 다양한 기능 응용 및 데이터 처리를 수행하는 바, 즉 상술한 방법 실시예 중의 능동적 인터랙션 방법을 실현한다. 메모리는 프로그램 기억 영역 및 데이터 기억 영역을 포함할 수 있으며, 여기서, 프로그램 기억 영역은 운 영 체제 및 적어도 하나의 기능에 필요한 응용 프로그램을 기억할 수 있고, 데이터 기억 영역은 능동적 인터랙 션 방법을 실현하는 전자 기기의 사용을 통해 생성된 데이터 등을 기억할 수 있다. 또한, 메모리는 고속 랜덤 액세스 메모리를 포함할 수 있고, 비 일시적 메모리를 더 포함할 수 있는 바, 예를 들면 적어도 하나의 자기 디스크 저장 장치, 플래시 장치, 또는 기타 비 일시적 고체 저장 장치를 포함할 수 있다. 일부 실시예에 있 어서, 메모리는 선택적으로 프로세서에 대해 원격 설치한 메모리를 포함할 수 있으며, 이러한 원격 메모리는 네트워크를 통해 능동적 인터랙션 방법을 실현하는 전자 기기에 연결될 수 있다. 상술한 네트워크의 실예는 인터넷, 기업 인트라 넷, 근거리 통신망, 이동 통신 네트워크 및 이들의 조합을 포함하나 이에 한정되지 않는다. 능동적 인터랙션 방법을 실현하는 전자 기기는 입력 장치 및 출력 장치를 더 포함할 수 있다. 프로세 서, 메모리, 입력 장치 및 출력 장치는 버스 또는 기타 방식을 통해 연결될 수 있으며, 도 4에서는 버스를 통해 연결하는 예를 들었다. 입력 장치는 입력된 디지털 또는 문자 정보를 수신하고, 또한 능동적 인터랙션 방법을 실현하는 전자 기기 의 사용자 설정 및 기능 제어에 관한 키 신호 입력을 생성할 수 있다. 예를 들면 터치 스크린, 키패드, 마우스, 트랙 패드, 터치 패드, 포인팅 스틱, 하나 또는 복수의 마우스 버튼, 트랙볼, 조이스틱 등 입력 장치를 포함할 수 있다. 출력 장치는 디스플레이 기기, 보조 조명 장치(예를 들면 LED) 및 촉각 피드백 장치(예를 들면 진동 모터) 등을 포함할 수 있다. 당해 디스플레이 기기는 액정 디스플레이(LCD), 발광 다이오드(LED) 디스플레 이 및 등 플라즈마 디스플레이를 포함할 수 있으나 이에 한정되지 않는다. 일부 실시 방식에 있어서, 디스플레 이 기기는 터치 스크린일 수 있다. 여기서 설명하는 시스템 및 기술의 다양한 실시 방식은 디지털 전자 회로 시스템, 집적 회로 시스템, 전용 ASIC(전용 집적 회로), 컴퓨터 하드웨어, 펌웨어, 소프트웨어 및/또는 이들의 조합에서 실현될 수 있다. 이러한 다양한 실시 예는 하나 또는 복수의 컴퓨터 프로그램에서 실시되고, 당해 하나 또는 복수의 컴퓨터 프로그램은 적어도 하나의 프로그램 가능 프로세서를 포함하는 프로그램 가능 시스템 상에서 수행 및/또는 해석될 수 있으 며, 당해 프로그램 가능 프로세서는 전용 또는 일반 프로그램 가능 프로세서일 수 있고, 기억 시스템, 적어도 하나의 입력 장치 및 적어도 하나의 출력 장치로부터 데이터 및 명령을 수신할 수 있으며, 또한 데이터 및 명령 을 당해 기억 시스템, 당해 적어도 하나의 입력 장치 및 당해 적어도 하나의 출력 장치에 전송할 수 있다. 이러한 계산 프로그램 (프로그램, 소프트웨어, 소프트웨어 응용 또는 코드로도 불림)은 프로그램 가능 프로세서 의 기계 명령을 포함하며, 또한 고급 과정 및/또는 객체 지향 프로그래밍 언어 및/또는 어셈블리/기계 언어를 이용하여 이러한 계산 프로그램을 실시할 수 있다. 본 명세서에서 사용되는 “기계 판독 가능 매체” 및 “컴퓨 터 판독 가능 매체”와 같은 용어는, 기계 명령 및/또는 데이터를 프로그램 가능 프로세서의 임의의 컴퓨터 프 로그램 제품, 기기 및/또는 장치 (예를 들면, 자기 디스크, 광 디스크, 메모리, 프로그램 가능 논리 장치(PL D))에 제공하기 위한 것을 의미하며, 기계 판독 가능 신호로서의 기계 명령을 수신하는 기계 판독 가능 매체를 포함한다. “기계 판독 가능 신호”와 같은 용어는 기계 명령 및/또는 데이터를 프로그램 가능 프로세서에 제공 하기 위한 임의의 신호를 의미한다. 유저와의 대화를 제공하기 위하여, 컴퓨터 상에서 여기서 설명하는 시스템 및 기술을 실시할 수 있으며, 당해 컴퓨터는 유저에게 정보를 표시하기 위한 디스플레이 장치(예를 들면 CRT(음극선관) 또는 LCD(액정 디스플레이) 모니터) 및 키보드와 포인팅 장치(예를 들면, 마우스 또는 트랙볼)를 구비할 수 있으며, 유저는 당해 키보드 및 당해 포인팅 장치를 통해 입력을 컴퓨터에 제공할 수 있다. 기타 유형의 장치는 또한 유저와의 대화를 제공하는 데 사용될 수 있다. 예를 들면, 유저에 제공하는 피드백은 임의의 형태의 감각 피드백(예를 들면, 시각적 피드 백, 청각적 피드백, 또는 촉각 피드백)일 수 있으며, 또한 임의의 형태(음향 입력, 음성 입력 또는 촉각 입력을 포함함)를 통해 유저로부터의 입력을 수신할 수 있다. 여기서 설명하는 시스템 및 기술을 백엔드 구성 요소를 포함하는 계산 시스템(예를 들면 데이터 서버), 또는 미 들웨어 구성 요소를 포함하는 계산 시스템(예를 들면 응용 서버), 또는 프런트 엔드 구성 요소를 포함하는 계산 시스템(예를 들면 그래픽 유저 인터페이스 또는 웹 브라우저를 구비하는 유저 컴퓨터인 바, 유저는 당해 그래픽 유저 인터페이스 또는 당해 웹 브라우저를 통해 여기서 설명하는 시스템 및 기술의 실시 방식과 대화함), 또는 이러한 백엔드 구성 요소, 미들웨어 구성 요소, 또는 프런트 엔드 구성 요소의 임의의 조합을 포함하는 계산 시 스템에서 실시할 수 있다. 임의의 형태 또는 매체의 디지털 데이터 통신 (예를 들면, 통신 네트워크)을 통해 시 스템의 구성 요소를 상호 연결할 수 있다. 통신 네트워크의 예는 근거리 통신망(LAN), 광역 통신망(WAN) 및 인 터넷을 포함한다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트와 서버는 일반적으로 서로 멀리 떨어져 있 고, 또한 일반적으로 통신 네트워크를 통해 대화를 실행한다. 해당되는 컴퓨터 상에서 운행되고, 또한 클라이언 트 - 서버 관계를 갖는 컴퓨터 프로그램을 통해 클라이언트와 서버의 관계를 발생시킬 수 있다.본 발명의 실시예의 기술안에 따르면, 인터랙션 대상의 시각 정보를 분석하고, 인터랙션 대상에 대해 시공간 모 델을 구축함으로써, 인터랙션 대상의 인터랙션 의도를 충분히 분석할 수 있고, 따라서 능동적 인터랙션의 지능 성 및 정확성을 향상시킬 수 있다. 상기에 나타낸 다양한 형태의 흐름을 이용하여 것을 재정렬, 증가 또는 삭제할 수 있음을 이해해야 한다. 예를 들면, 본 발명에 기재된 각각의 것은 병렬로 수행되거나 또는 차례로 수행되거나 또는 다른 순서로 수행될 수 있으며, 본 발명이 개시하는 기술안이 원하는 결과를 실현할 수 있는 한, 본 명세서는 이에 대해 한정하지 않는 다. 상술한 구체적인 실시 방식은 본 발명의 보호 범위를 한정하지 않는다. 당업자는 설계 요건 및 기타 요인에 따 라 다양한 수정, 조합, 서브 조합 및 대체를 실행할 수 있음을 이해해야 한다. 본 발명의 정신 및 원칙 내에서 이루어진 임의의 수정 동등한 대체 및 개선 등은 모두 본 발명의 보호 범위 내에 포함되어야 한다."}
{"patent_id": "10-2021-0087893", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도면은 본 방안을 더 잘 이해하도록 하기 위한 것이며, 본 발명에 대한 한정을 이루지 않는다. 도 1은 본 발명에 따른 제1 실시예의 모식도이다. 도 2는 본 발명에 따른 제2 실시예의 모식도이다. 도 3은 본 발명에 따른 제3 실시예의 모식도이다. 도 4는 본 발명은 실시예를 실현하기 위한 능동적 인터랙션 방법의 전자 기기의 블럭도이다."}
