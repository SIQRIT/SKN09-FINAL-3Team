{"patent_id": "10-2022-7012657", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0062400", "출원번호": "10-2022-7012657", "발명의 명칭": "투사 방법 및 시스템", "출원인": "후아웨이 테크놀러지 컴퍼니 리미티드", "발명자": "마 루이"}}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "투사 방법으로서, 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치의 디스플레이 화면 방향을 판정하는 단계 및상기 제1 장치의 상기 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정되는 경우, 상기 제1 장치의화면 콘텐츠를 목표 디스플레이 화면에 표시하는 단계 또는 상기 제1 장치의 상기 디스플레이 화면 방향이 사전설정 방향을 충족한다고 판정되는 경우, 목표 디스플레이 화면에 화면 콘텐츠를 표시하도록 상기 제1 장치에 통지하는 단계를 포함하는,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서, 상기 제1 장치의 상기 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정되는 경우, 목표 디스플레이화면에 화면 콘텐츠를 표시하도록 상기 제1 장치에 통지하는 단계는 구체적으로,상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정되는 경우, 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는 단계인,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 제1 장치의 상기 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정되는 경우, 상기 제1 장치의화면 콘텐츠를 목표 디스플레이 화면에 표시하는 단계는,상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향하고 상기 제1 장치의 인증 정보가 획득된다고 판정되는 경우, 상기 제1 장치의 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하는 단계를 포함하는,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1항에 있어서,상기 투사 방법은,감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여 상기 목표 디스플레이 화면에 접근하는 손의 동작이 있다고 판정하는 단계 및상기 목표 디스플레이 화면에 접근하는 상기 손의 상기 동작이 있고 상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정되는 경우, 상기 목표 디스플레이 화면에 상기 화면 콘텐츠를표시하도록 상기 제1 장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는 단계를 더 포함하는,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1항에 있어서,공개특허 10-2022-0062400-3-상기 투사 방법은,감지 장치에 의해 획득되고 손으로부터 상기 감지 장치까지의 거리에 대한 정보에 기초하여 상기 손으로부터 상기 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 판정하는 단계 및상기 손으로부터 상기 목표 디스플레이 화면까지의 상기 거리가 상기 사전 설정 값보다 작고 상기 제1 장치의상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정되는 경우, 상기 화면 콘텐츠를 상기목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는 단계를더 포함하는,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1항에 있어서,상기 투사 방법은 감지 장치에 의해 획득된 이미지 정보에 기초하여 상기 제1 장치가 들려 있다고(held) 판정하는 단계 및상기 제1 장치가 들려 있고 상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정되는 경우, 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는 단계를 더 포함하는,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1항에 있어서,상기 투사 방법은 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여 상기 목표 디스플레이 화면에 접근하는 손의 동작이 있다고 판정하는 단계,감지 장치에 의해 획득된 이미지 정보에 기초하여 상기 제1 장치가 들려 있다고 판정하는 단계 및상기 목표 디스플레이 화면에 접근하는 상기 손의 상기 동작이 있고 상기 제1 장치가 들려 있고 상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정되는 경우, 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는 단계를 더 포함하는,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서,상기 투사 방법은 감지 장치에 의해 획득되고 손으로부터 상기 감지 장치까지의 거리에 대한 정보에 기초하여상기 손으로부터 상기 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 판정하는 단계,감지 장치에 의해 획득된 이미지 정보에 기초하여 상기 제1 장치가 들려 있다고 판정하는 단계 및상기 손으로부터 상기 목표 디스플레이 화면까지의 상기 거리가 상기 사전 설정 값보다 작고 상기 제1 장치가들려 있고 상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정되는경우, 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하기 위해 상기 제1장치로 메시지를 전송하는 단계를 더 포함하는,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 6항 내지 제 8항 중 어느 한 항에 있어서,상기 감지 장치에 의해 획득된 상기 이미지 정보에 기초하여 상기 제1 장치가 들려 있다고 판정하는 단계는,공개특허 10-2022-0062400-4-상기 감지 장치에 의해 획득된 상기 이미지 정보에서 상기 손과 상기 제1 장치 사이의 통합대 중첩비(intersection over union)에 따라서 상기 제1 장치가 들려 있다고 판정하는 단계를 포함하는,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1항 내지 제 9항에 있어서,상기 방법은,상기 감지 장치에 의해 획득된 이미지 정보에 기초하여 상기 제1 장치의 자세(posture)를 판정하는 단계 및상기 제1 장치의 상기 자세에 기초하여 상기 목표 디스플레이 화면의 투사 디스플레이 모드를 판정하는 단계를포함하는,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 6항 내지 제 8항 중 어느 한 항에 있어서,상기 제1 장치가 들려 있고 상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정되는 것은 구체적으로,상기 감지 장치에 의해 획득된 상기 이미지 정보에 기초하고 동일한 신경 네트워크 모델에 기초하여 상기 사용자가 상기 제1 장치를 들고 있는지 여부 및 상기 제1 장치의 상기 디스플레이 화면 방향을 판정하는 것인,투사 방법."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "목표 디스플레이 화면 및 적어도 하나의 프로세서를 포함하는 투사 시스템으로서,상기 적어도 하나의 프로세서는, 감지 장치에 의해 획득된 이미지 정보에 기초하여, 제1 장치의 디스플레이 화면 방향을 판정하고,상기 적어도 하나의 프로세서가 상기 제1 장치의 상기 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정하는 경우 상기 적어도 하나의 프로세서는 상기 제1 장치의 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하거나, 상기 적어도 하나의 프로세서가 상기 제1 장치의 상기 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정하는 경우 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하는,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12항에 있어서,상기 적어도 하나의 프로세서가 상기 제1 장치의 상기 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정하는 경우 상기 적어도 하나의 프로세서는 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는 것은 구체적으로,상기 적어도 하나의 프로세서가 상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정하는 경우 상기 적어도 하나의 프로세서는 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는 것인,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 12항에 있어서,상기 적어도 하나의 프로세서가 상기 제1 장치의 상기 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정하는 경우 상기 적어도 하나의 프로세서는 상기 제1 장치의 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시공개특허 10-2022-0062400-5-하는 것은,상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향하고 상기 제1 장치의 인증 정보가 획득된다다고 판정되는 경우, 상기 적어도 하나의 프로세서가 상기 제1 장치의 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하는 것을 포함하는,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 12항에 있어서,상기 적어도 하나의 프로세서는 또한 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지에 기초하여 상기 목표 디스플레이 화면에 접근하는 손의 동작이 있다고 판정하고,상기 적어도 하나의 프로세서가 상기 목표 디스플레이 화면에 접근하는 상기 손의 상기 동작이 있고 및 상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정하는 경우, 상기 적어도 하나의 프로세서는 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하기 위해상기 제1 장치로 메시지를 전송하는,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 12항에 있어서,상기 적어도 하나의 프로세서는 또한 감지 장치에 의해 획득되고 손으로부터 상기 감지 장치까지의 거리에 대한정보에 기초하여 상기 손으로부터 상기 목표 디스플레이 화면까지의 거리는 사전 설정 값보다 작다고 판정하고,상기 적어도 하나의 프로세서가 상기 손으로부터 상기 목표 디스플레이 화면까지의 상기 거리는 상기 사전 설정값보다 작고 상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정하는 경우, 상기 적어도 하나의 프로세서는 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 12항에 있어서,상기 적어도 하나의 프로세서는 또한 감지 장치에 의해 획득된 이미지 정보에 기초하여 상기 제1 장치가 들려있다고 판정하고,상기 적어도 하나의 프로세서가 상기 제1 장치가 들려 있고 상기 제1 장치의 상기 디스플레이 화면 방향이 상기목표 디스플레이 화면을 향한다고 또한 판정하는 경우, 상기 적어도 하나의 프로세서는 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 12항에 있어서,상기 적어도 하나의 프로세서는 또한 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여 상기 목표 디스플레이 화면에 접근하는 손의 동작이 있다고 판정하고,상기 적어도 하나의 프로세서는 감지 장치에 의해 획득된 이미지 정보에 기초하여 상기 제1 장치가 들려 있다고판정하며,상기 적어도 하나의 프로세서가 상기 목표 디스플레이 화면에 접근하는 상기 손의 상기 동작이 있고, 상기 제1장치가 들려 있고, 상기 제1 장치의 상기 디스플레이 화면 방향이 상기 목표 디스플레이 화면을 향한다고 판정하는 경우, 상기 적어도 하나의 프로세서는 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기공개특허 10-2022-0062400-6-제1 장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 12항에 있어서,상기 적어도 하나의 프로세서는 또한 감지 장치에 의해 획득되고 손으로부터 상기 감지 장치까지의 거리에 대한정보에 기초하여 상기 손으로부터 상기 목표 디스플레이 화면까지의 거리는 사전 설정 값보다 작다고 판정하고,상기 적어도 하나의 프로세서는 감지 장치에 의해 획득된 이미지 정보에 기초하여 상기 제1 장치가 들려 있다고판정하며,상기 적어도 하나의 프로세서가 상기 손으로부터 상기 목표 디스플레이 화면까지의 상기 거리가상기 사전 설정값보다 작고, 상기 제1 장치가 들려 있고, 상기 제1 장치의 상기 디스플레이 화면 방향은 상기 목표 디스플레이화면을 향한다고 판정하는 경우, 상기 적어도 하나의 프로세서는 상기 화면 콘텐츠를 상기 목표 디스플레이 화면에 표시하도록 상기 제1 장치에 통지하기 위해 상기 제1 장치로 메시지를 전송하는,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 17항 내지 제 19항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서가 상기 감지 장치에 의해 획득된 상기 이미지 정보에 기초하여 상기 제1 장치가들려 있다고 판정하는 것은,상기 적어도 하나의 프로세서가 상기 감지 장치에 의해 획득된 상기 이미지 정보에서의 상기 손과 상기 제1 장치 사이의 통합대 중첩비에 따라 상기 제1 장치가 들려 있다고 판정하는 것을 포함하는,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제 12항 내지 제 19항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서는 또한 감지 장치에 의해 획득된 이미지 정보에 기초하여 상기 제1 장치의 자세를판정하고,상기 적어도 하나의 프로세서는 상기 제1 장치의 상기 자세에 기초하여 상기 목표 디스플레이 화면의 투사 디스플레이 모드를 판정하는,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제 17항 내지 제 19항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서가 상기 제1 장치가 들려 있고 상기 제1 장치의 상기 디스플레이 화면 방향이 상기목표 디스플레이 화면을 향한다고 판정하는 것은 구체적으로,상기 적어도 하나의 프로세서가 감지 장치에 의해 획득된 이미지 정보에 기초하고 신경 네트워크 모델에 기초하여 상기 사용자가 상기 제1 장치를 들고 있는지 여부 및 상기 제1 장치의 상기 디스플레이 화면 방향을 판정하는 것인,투사 시스템."}
{"patent_id": "10-2022-7012657", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 출원은 인공 지능 분야와 관련되고, 투사 방법 또는 시스템을 제공한다. 투사 방법은, 감지 장치에 의해 획득 된 이미지 정보에 기초하여 휴대용 장치의 디스플레이 화면 방향을 판정하는 단계 및 휴대용 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정되는 경우, 휴대용 장치의 화면 콘텐츠를 목표 디스플레이 화면에 (뒷면에 계속)"}
{"patent_id": "10-2022-7012657", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 사용자 행위 식별 기술 분야에 관한 것으로, 특히, 사용자 행위를 분석하여 구현되는 보다 효율적이 고 신속한 투사 (projection) 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2022-7012657", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "휴대용(handheld) 스마트 단말 투사(projection) 기술의 적용이 매우 일반적이라는 것은 잘 알려져 있다. 휴대 용 스마트 단말 투사기술은 주로 특정 동작을 통해 휴대용 스마트 단말 장치의 화면 관련 정보를 표시할 수 있 는 텔레비전과 같은 비디오 재생 장치에 적용되고, 휴대용 스마트 단말의 현재 화면 표시 내용 또는 단말의 메 뉴 인터페이스가 일반적으로 표시된다. 최근, 자율 주행 기술의 발달과 함께, 차량 내(in-vehicle) 스마트 환경을 이용하여 운전중인 사용자에게 보다 지능적인 서비스를 제공하도록 차량 내 스마트 조종석(cockpit)과 관련된 기술 연구가 발달되었다. 휴대용 장치 의 화면 디스플레이를 차량 내 디스플레이 장치에 더 잘 투사하는 방법은 또한 차량을 타거나 운전하는 동안 사 용자의 중요한 요구사항 중 하나이다. 기존의 차량 내 휴대용 단말 투사 기술은 크게 플러그인 모드(plug in mode)와 무선 모드(wireless mode) 두 가 지 모드를 포함한다. 기존의 차량 내 투사 해결방안은 차량 내 엔터테인먼트 및 통신 분야에서 사용자 경험을 매우 풍부하게 하지만, 휴대용 단말 투사의 특정 시작 수단에 대하여, 두개의 시작 모드 각각은 동작이 복잡하 고 투사 콘텐츠가 실시간으로 동기화되지 않는 문제점을 가지고 있다. 플러그인 차량 내 투사 시작 모드에서는, 플러그인 후 사용자는 대응하는 투사 버튼을 검색하고 버튼을 탭(ta p)하고, 사용자의 탭 동작에 기초하여 투사를 시작한다. 주요 단점은 휴대용 단말이 플러그인 모드에서 차량 내 인포테인먼트(infotainment)에 연결되고, 사용자가 사용하기에 불편한 USB 케이블을 휴대해야 한다는 것이다. 일반적으로 무선 차량 내 투사 시작 모드에서는, 휴대용 단말이 블루투스를 통해 차량 내 인포테인먼트와 연결 된 후, 사용자는 대응하는 투사 버튼을 검색하고, 사용자의 탭 동작에 기초하여 투사를 시작한다. 이 해결방안 에서, 휴대용 단말이 무선 방식으로 차량 내 인포테인먼트에 연결되지만, 사용자는 여전히 중앙 제어 화면상의 대응하는 시작 버튼을 찾아야 하고, 깊은 인터페이스 계층 구조로 인해 단계가 복잡하다. 휴대용 장치의 투사 동작을 보다 편리하게 구현하고, 사용자가 투사를 수행해야 할 때 사용자의 투사 의도를 정 확하게 판정하며, 동작의 편리성이 향상되면서도 오작동(misoperation)과 오인(misidentification)의 확률을 줄 이고, 사용자의 투사 경험을 향상시키는 방법이 이 해결방안에서 주안점(focus)이다."}
{"patent_id": "10-2022-7012657", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 출원의 실시예는 운전, 스마트 홈, 스마트 텔레비전 및 스마트 대형 스크린과 같은 복수의 애플리케이션 시 나리오에 적용될 수 있는 투사 방법 및 시스템을 제공한다. 정확하고 편리한 투사 동작을 구현하도록 사용자 행 위 의도와 휴대용 단말의 현재 상태가 분석된다. 이러한 방식으로, 사용자가 투사를 수행하려고 하는 경우, 사 용자는 더 이상 복잡한 탭 동작을 수행할 필요가 없다. 또한, 사용자 의도는 종합적으로 사용자 행위를 분석하 여 고려될 수 있으므로, 동작의 단순화에 의해 오작동의 확률이 높아지는 것을 피할 수 있어 사용자 의도를 판 정하는 것의 정확성을 확보할 수 있다."}
{"patent_id": "10-2022-7012657", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "제1 양태에 따르면, 본 발명의 실시예는 투사 방법을 제공한다. 그 방법은, 감지 장치(sensing apparatus)에 의 해 획득된 이미지 정보에 기초하여 제1 장치의 디스플레이 화면 방향(display screen orientation)을 판정하는 단계, 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족하는 경우 제1 장치의 화면 콘텐츠를 목표 디 스플레이 화면에 표시하는 단계 또는 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정되 는 경우 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하는 단계를 포함한다. 투사를 수행할지 여부는 제1 장치의 화면 방향을 판정함으로써 결정된다. 이는 사용자의 투사 동작을 단순화한 다. 특정 구현에서, 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정되는 경우 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하는 것은, 제1 장치의 디스플레이 화면 방향이 목표 디스 플레이 화면을 향한다고 판정되는 경우 화면 콘텐츠를 목표 디스플레이 스크린에 표시하도록 제1 장치에 통지하 기 위해 제1 장치로 메시지를 전송하는 것일 수 있다.디스플레이 화면이 목표 디스플레이 화면을 향하고 있다는 것은 투사를 수행할지 여부를 판정하기 위한 트리거 (trigger) 조건으로 사용된다. 이것은 투사 동작이 단순화되면서 오작동을 효율적으로 피할 수 있다. 이는 일반 적인 사용 상태에서 휴대용 장치의 디스플레이 화면이 주로 사용자를 향하기 때문이다. 특정 구현에서, 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정되는 경우 목표 장치 화 면상에 제1 장치의 화면 콘텐츠를 표시하는 것은, 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향하고 제1 장치의 인증 정보가 획득되었다고 판정되는 경우 제1 장치의 화면 콘텐츠를 목표 디스플레이 화면상 에 표시하는 것일 수 있다. 장치가 인증되었는지 여부가 판정되고, 장치가 인증된 경우, 투사 동작이 수행된다. 투사 동작을 인증없이 수행 하는 것을 방지하도록, 투사 동작이 단순화되면서 사용자가 투사 동작을 허용할 수 있다. 예를 들어, 허가없이 화면 내용을 휴대용 장치에 투사하기 위해 다른 사용자가 효율적으로 사용자의 휴대용 장치를 사용하는 것이 방 지될 수 있다. 제1 양태에서 투사 방법은 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여 목표 디 스플레이 화면에 접근하는 손의 동작이 있다고 판정하는 단계, 목표 디스플레이 화면에 접근하는 손의 동작이 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하는 경우, 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송하는 단계를 더 포함할 수 있다. 제1 양태에서 투사 방법은, 감지 장치에 의해 획득되고 손으로부터 감지 장치까지의 거리에 관한 정보에 기초하 여 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 판정하는 단계, 및 손으로부터 목 표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다는 것 및 제1 장치의 디스플레이 화면 방향이 목표 디 스플레이 화면을 향한다고 판정되는 경우, 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지 하기 위해 제1 장치로 메시지를 전송하는 단계를 더 포함할 수 있다. 제1 양태에서 투사 방법은, 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정하는 단계, 및 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정되는 경우, 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송 하는 단계를 더 포함할 수 있다. 제1 양태에서 투사 방법은 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지에 기초하여 목표 디스플레 이 화면에 접근하는 손의 동작이 있다고 판정하는 단계, 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정하는 단계, 및 목표 디스플레이 화면에 접근하는 손의 동작이 있다는 것, 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정되는 경우 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송하는 단계를 더 포함할 수 있다. 전술한 판정 동작의 순서는 제한되지 않는다. 제1 양태에서 투사 방법은, 감지 장치에 의해 획득되고 손으로부터 감지 장치까지의 거리에 대한 정보에 기초하 여 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 판정하는 단계, 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정하는 단계, 및 손으로부터 목표 디스플레이 화면까 지의 거리가 사전 설정 값보다 작다는 것, 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스 플레이 화면을 향한다고 판정되는 경우 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송하는 단계를 더 포함할 수 있다. 사용자의 투사 의도를 공동으로 판단하기 위해 다른 판정 조건이 추가된다. 이것은 오작동의 위험을 더욱 줄일 수 있다. 또한, 손으로 접근하여 장치가 들려 있는지 여부를 판정하는 것은 또한 단순하며 동작 중 사용자에 의 해 수행될 수 있는 쉬운 작업이다. 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정하는 것은 감지 장치에 의해 획 득된 이미지 정보에서 손과 제1 장치간의 통합대 중첩비(intersection over union)에 따라 제1 장치가 들려 있 다고 판정하는 것을 포함한다. 휴대용 장치가 들려 있는지 여부는 통합대 중첩비 사용에 의해 판정되어, 사용자가 휴대용 장치를 들고 있는지 여부가 효과적으로 식별될 수 있다. 제1 양태에서 투사 방법은 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치의 자세(posture)를 평가 하는 단계, 제1 장치의 자세에 기초하여 목표 디스플레이 스크린의 투사 디스플레이 모드를 판정하는 단계를 더 포함할 수 있다. 투사 모드는 자세 판정을 추가함으로써 판정되어, 투사 동작은 단순화 되면서 투사 기능은 보다 풍부해진다. 이 는 다양한 사용자 요구 사항을 충족시킨다. 제1 양태의 투사 방법의 특정 구현에서, 하나의 신경 네트워크(neural network)는 두개의 판정 결과 결론 (determining result conclusion)을 출력하도록 학습될 수 있다. 즉, 제1 측면의 투사 방법에서, 특정 구현에 서, 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정되는 것은, 감지 장치에 의해 획득된 이미지 정보에 기초하고 동일한 신경 네트워크 모델에 기초하여 사용자가 제1 장치를 들고 있는지 여부 및 제1 장치의 화면 방향을 판정하는 것일 수 있다. 제2 양태에 따르면, 본 발명의 실시예는 투사 시스템을 더 개시한다. 시스템은 목표 디스플레이 화면 및 적어도 하나의 프로세서를 포함하는데, 적어도 하나의 프로세서는 감지 장치에 의해 획득된 이미지 정보에 기초하여 제 1 장치의 디스플레이 화면 방향을 판정하고, 적어도 하나의 프로세서가 제1 장치의 디스플레이 화면 방향이 사 전 설정 방향을 충족한다고 판정하는 경우 적어도 하나의 프로세서는 목표 디스플레이 화면상의 제1 장치의 스 크린 콘텐츠를 표시하거나 적어도 하나의 프로세서가 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 충 족한다고 판정하는 경우 스크린 콘텐츠를 목표 디스플레이 스크린에 표시하도록 제1 장치에 통지한다. 특정 구현에서, 적어도 하나의 프로세서가 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을충족한다고 판 정하는 경우, 적어도 하나의 프로세서는 화면 콘텐츠를 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시 지를 전송한다. 구체적으로, 적어도 하나의 프로세서가 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화 면을 향한다고 판정하는 경우, 적어도 하나의 프로세서는 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제 1 장치에 통지하기 위해 제1 장치로 메시지를 전송한다. 특정 구현에서, 적어도 하나의 프로세서가 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을충족한다고 판 정하는 경우 적어도 하나의 프로세서가 제1 장치의 콘텐츠를 목표 디스플레이 화면에 표시한다는 것은 제1 장치 의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다는 것 및 제1 장치의 인증 정보가 획득된다고 판정되 는 경우 적어도 하나의 프로세서가 제1 장치의 화면 콘텐츠를 목표 디스플레이 화면에 표시하는 것일 수 있다. 적어도 하나의 프로세서는 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여 목표 디 스플레이 화면에 접근하는 손의 동작이 있다고 더 판정할 수 있다. 적어도 하나의 프로세서가 목표 디스플레이 화면에 접근하는 손의 동작이 있다는 것 및 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하는 경우, 적어도 하나의 프로세서는 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송한다. 적어도 하나의 프로세서는 감지 장치에 의해 획득되었고 손으로부터 감지장치까지의 거리에 대한 정보에 기초하 여 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 더 판정할 수 있다. 적어도 하나의 프로세서가 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다는 것 및 제1 장치의 디스플 레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하는 경우, 적어도 하나의 프로세서는 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송한다. 적어도 하나의 프로세서는 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 더 판정할 수 있다. 적어도 하나의 프로세서가 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 더 판정하는 경우, 적어도 하나의 프로세서는 화면 콘텐츠를 목표 디스플레이 화면에 표시하도 록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송한다. 적어도 하나의 프로세서는 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여 목표 디 스플레이 화면에 접근하는 손의 동작이 있다고 더 판정할 수 있다. 적어도 하나의 프로세서는 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정한다. 적어도 하나의 프로세서가 목표 디스플레이 화면에 접근하는 손의 동작이 있다는 것, 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스 플레이 화면을 향한다고 판정하는 경우, 적어도 하나의 프로세서는 화면 콘텐츠를 목표 디스플레이 화면에 표시 하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송한다. 적어도 하나의 프로세서는 감지 장치에 의해 획득되고 손으로부터 감지 장치까지의 거리에 대한 정보에 기초하 여 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 더 판정할 수 있다. 적어도 하나의 프로세서는 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정한다. 적어도 하나의 프로세서가 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다는 것, 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하는 경우, 적어도 하나의 프로세서 는 목표 디스플레이 화면에 화면 콘텐츠를 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송한다. 특정 구현에서, 적어도 하나의 프로세서가 제1 장치가 들려 있다고 감지 장치에 의해 획득된 이미지 정보에 기 초하여 판정하는 것은, 적어도 하나의 하나의 프로세서가 감지 장치에 의해 획득된 이미지 정보에서 제1 장치가 들려 있다고 손과 제1 장치 사이의 통합대 중첩비에 따라 판정하는 것일 수 있다. 제2 양태에서 투사 시스템은 적어도 하나의 프로세서가 제1 장치의 자세를 감지 장치에 의해 획득된 이미지 정 보에 기초하여 판정하는 것 및 적어도 하나의 프로세서가 목표 디스플레이 화면의 투사 디스플레이 모드를 제1 장치의 자세에 기초하여 판정하는 것을 포함할 수 있다. 적어도 하나의 프로세서가 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하는 것은 구체적으로, 적어도 하나의 프로세서가 감지 장치에 의해 획득된 이미지 정보에 기초하 고 신경 네트워크 모델에 기초하여 사용자가 제1 장치를 들고 있는지 여부 및 제1 장치의 디스플레이 화면 방향 을 판정하는 것일 수 있다. 제3 양태에 따르면, 본 발명의 시스템은 투사 시스템을 더 개시한다. 그 시스템은 이미지 수집 유닛(collection unit), 디스플레이 유닛(display unit) 및 판정 유닛(determining unit)을 더 포함한다. 이미지 수집 유닛은 이미지 정보를 획득하고 이미지 정보를 판정 유닛에 전송하도록 구성된다. 판정 유닛은 이미지 수집 유닛에 의 해 획득된 이미지 정보에 기초하여 제1 장치의 디스플레이 스크린 방향을 판정하도록 및 판정 유닛이 제1 장치 의 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정하는 경우 투사를 수행하도록 지시하기 위해 구성 된다. 디스플레이 유닛은 제1 장치의 화면 콘텐츠를 표시한다. 특정 구현에서, 판정 유닛이 투사를 수행하도록 지시하는 것은 제1 장치에 통지하여, 제1 장치가 화면 콘텐츠를 목표 디스플레이 화면에 전송하는 것일 수 있다. 특정 구현에서, 판정 유닛이 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정하는 경우, 판정 유닛은 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 보낸다. 구체적으로, 판정 유닛이 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하 는 경우, 판정 유닛은 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 보낸다. 특정 구현에서, 판정 유닛이 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정하는 경우, 디스플레이 유닛은 제1 장치의 화면 콘텐츠를 목표 디스플레이 화면에 표시한다. 구체적으로, 판정 유닛이 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향하고 제1 장치의 인증 정보가 획득된다고 판정하는 경우, 디스플레이 유닛은 제1 장치의 화면 콘텐츠를 목표 디스플레이 화면에 표시한다. 판정 유닛은 이미지 수집 유닛에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여 목표 디스플레 이 화면에 접근하는 손의 동작이 있다고 더 판정한다. 판정 유닛이 목표 디스플레이 화면에 접근하는 손의 동작 이 있다는 것 및 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하는 경우, 판정 유 닛은 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송한 다. 투사 시스템은 거리 획득 유닛(distance obtaining unit)을 더 포함할 수 있고, 판정 유닛은 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 거리 획득 유닛에 의해 획득되고 손으로부터 감지 장치 까지의 거리에 대한 정보에 기초하여 더 판정할 수 있다. 판정 유닛이 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다는 것 및 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하는 경우, 판정 유닛은 화면 콘텐츠를 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송한 다. 판정 유닛은 이미지 수집 유닛에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 더 판정할 수 있 다. 판정 유닛이 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하는 경우, 판정 유닛은 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1장치로 메시지를 전송한다. 판정 유닛은 목표 디스플레이 화면에 접근하는 손의 동작이 있다는 것을 이미지 수집 유닛에 의해 획득된 비디 오 또는 멀티 프레임 이미지 정보에 기초하여 더 판정할 수 있다. 판정 유닛은 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정한다. 판정 유닛이 목표 디스플레이 화면에 접근하는 손의 동작이 있다는 것, 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하 는 경우, 판정 유닛은 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송한다. 판정 유닛은 이미지 수집 유닛에 의해 획득되고 손으로부터 감지 장치까지의 거리에 대한 정보에 기초하여 손으 로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 더 판정할 수 있다. 판정 유닛은 감지 장 치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정한다. 판정 유닛이 손으로부터 목표 디 스플레이 화면까지의 거리가 사전 설정 값보다 작다는 것, 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하는 경우, 판정 유닛은 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송한다. 특정 구현에서, 판정 유닛이 이미지 수집 유닛에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정하는 것은 판정 유닛이 제1 장치가 들려 있다고 감지 장치에 의해 획득된 이미지 정보에서 손과 제1 장치 사이의 통합대 중첩비에 따라서 판정하는 것일 수 있다. 제2 양태에서 투사 시스템은 판정 유닛이 이미지 수집 유닛에 의해 획득된 이미지 정보에 기초하여 제1 장치의 자세를 더 판정하는 것 및 판정 유닛이 제1 장치의 자세에 기초하여 목표 디스플레이 화면의 투사 디스플레이 모드를 판정하는 것을 더 포함할 수 있다. 판정 유닛이 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정 하는 것은 구체적으로, 판정 유닛이 감지 장치에 의해 획득된 이미지 정보에 기초하고 신경 네트워크 모델에 기 초하여 사용자가 제1 장치를 들고 있는지 여부 및 제1 장치의 디스플레이 화면 방향을 판정하는 것일 수 있다. 전술한 상이한 조건에서 이미지 수집 유닛에 의해 수집된 이미지 정보는 동일할 수도 있고 상이할 수도 있다. 이미지 수집 유닛은 복수의 수집 서브유닛을 포함할 수 있고, 상이한 이미지 정보는 동일한 이미지 수집 서브유 닛 또는 상이한 이미지 수집 서브유닛으로부터의 것일 수 있다. 제4 양태에 따르면, 본 출원의 실시예는 투사 방법을 제공한다. 방법은 감지 장치에 의해 획득된정보에 기초하 여 목표 디스플레이 화면에 접근하는 손의 동작이 있는지 판정하는 단계, 목표 디스플레이 화면에 접근하는 손 의 동작이 있다고 판정된 경우, 감지 장치에 의해 획득된 이미지 정보에 기초하여 투사 대상 장치(to-be- projected device)가 들려 있는지 여부를 판정하는 단계, 투사 대상 장치가 들려 있다고 판정되는 경우, 감지 장치에 의해 획득된 이미지 정보에 기초하여 투사 대상 장치의 디스플레이 화면 방향을 판정하는 단계, 투사 대 상 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족하는 경우, 투사 대상 장치의 화면 내용을 목표 디스 플레이 화면에 표시하는 단계 또는 투사 대상 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족하는 경우, 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하는 단계를 포함한다. 전술한 방법에서, 사용자 행위 정보에 기초하여 사용자 행위, 투사 대상 장치가 들려 있는지 여부 및 투사 대상 장치의 방향이 판정된다. 그 다음 사용자 의도를 종합적으로 판정하여 투사 동작이 수행된다. 투사 단계가 단순 화되어 사용자는 한 손을 사용하여 투사를 수행할 수 있다. 또한, 투사 의도 식별의 정확성을 보장하도록 행위 탐지나 이미지 식별과 같은 종합 분석이 수행된다. 선택적으로, 제4 양태에서 감지 장치에 의해 획득된 정보에 기초하여 목표 디스플레이 화면에 접근하는 손의 동 작이 있는지 여부가 판정된다. 감지 장치에 의해 획득된 정보는 이미지 정보 또는 거리 정보일 수 있다. 따라서, 전술한 감지 장치는 카메라 또는 깊이 카메라(depth camera)일 수 있고 또는 레이더일 수 있다. 제4 양태에서, 목표 디스플레이 화면에 접근하는 손의 동작이 있는지 여부는 신경 네트워크 모델, 예를 들어, 사용자 행위를 식별하여 사용자의 행위 경향을 식별하기 위한 3D 컨볼루션 신경 네트워크 모델을 사용하여 판정 된다. 제4 양태에서 행위 식별 모델의 입력은 시간 순서로 있는 복수의 프레임의 비디오 또는 이미지이고, 출력 결과 는 사용자의 손의 동작이 목표 디스플레이 화면에 접근하는 것을 식별하는 것의 확률이거나, 출력 결과는 사용자의 손의 동작이 목표 디스플레이 화면에 접근하는지 여부를 식별하는 것의 결과이다. 전술한 방법에서, 마지막 사용자 의도의 행위 분석의 결론의 정확성을 보장하도록 사용자 행위는 신경 네트워크 모델을 사용하여 정확하게 식별될 수 있다. 또한, 제4 양태의 투사 방법은 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치의 자세를 더 판정할 수 있고, 제1 장치의 자세에 기초하여 목표 디스플레이 화면의 투사 디스플레이 모드를 판정할 수 있다. 사용자가 원하는 투사 모드는 자세를 판정하여 식별된다. 전술한 방식에서, 사용자는 투사 디스플레이 모드를 수동으로 선택하거나 설정할 필요가 없다. 제5 양태에 따르면, 본 출원의 실시예는 적어도 하나의 프로세서를 포함하는 투사 시스템을 제공한다. 적어도 하나의 프로세서는 감지 장치에 의해 획득된 정보에 기초하여 목표 디스플레이 화면에 접근하는 손의 동작이 있 는지 여부를 판정하고, 목표 디스플레이 화면에 접근하는 손의 동작이 있다고 판정되는 경우, 감지 장치에 의해 획득된 이미지 정보에 기초하여 투사 대상 장치가 들려 있는지 여부를 판정하고, 투사 대상 장치가 들려 있다고 판정되는 경우, 감지 장치에 의해 획득된 이미지 정보에 기초하여 투사 대상 장치의 디스플레이 화면 방향을 판 정하고, 투사 대상 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족하는 경우, 투사 대상 장치의 화면 콘 텐츠를 목표 디스플레이 화면에 표시하거나 투사 대상 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족하 는 경우, 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지한다. 제6 양태에 따르면, 본 출원의 실시예는 적어도 하나의 판정 유닛 또는 디스플레이 유닛을 포함하는 투사 시스 템을 제공한다. 적어도 하나의 판정 유닛은 감지 장치에 의해 획득된 정보에 기초하여 목표 디스플레이 화면에 접근하는 손의 동작이 있는지 여부를 판정하고, 목표 디스플레이 화면에 접근하는 손의 동작이 있다고 판정되는 경우, 감지 장치에 의해 획득된 이미지 정보에 기초하여 투사 대상 장치가 들려 있는지 여부를 판정하고, 투사 대상 장치가 들려 있다고 판정되는 경우, 감지 장치에 의해 획득된 이미지 정보에 기초하여 투사 대상 장치의 디스플레이 화면 방향을 판정하고, 투사 대상 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족하는 경우, 투사 대상 장치의 화면 콘텐츠를 목표 디스플레이 화면에 표시하거나 투사 대상 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족하는 경우, 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지한다. 제7 양태에 따르면, 본 출원의 실시예는 컴퓨터 프로그램 제품을 더 제공한다. 컴퓨터 프로그램 제품은 본 발명 의 실시예에서 제1 양태 또는 제4 양태의 방법을 수행하도록 구성될 수 있다. 제8 양태에 따르면, 본 출원의 실시예는 컴퓨터 판독가능한(computer-readable) 저장 매체를 더 제공한다. 컴퓨 터 판독가능 저장 매체는 컴퓨터 명령어를 저장하고, 컴퓨터 명령어를 실행함으로써 본 발명의 실시예의 제1 양 태 및 제4 양태의 방법을 방법을 수행한다. 본 출원의 실시예에서의 기술적 해결방안을 구현함으로써, 현재 투사 동작의 복잡성 및 불편함의 단점을 피할 수 있음을 알 수 있다. 기존의 차량 내 환경 및 스마트 텔레비전의 디스플레이 장치의 감지 장치를 충분히 사용 할 수 있다. 투사를 수행할지 여부를 결정하기 위해 투사 동작을 시작해야 하는지 여부를 판정하도록 사용자 행 위 정보가 획득되고 스마트 사용자 행위 분석은 수행된다. 이는 정확하고 편리한 투사 동작을 구현하고, 동작이 단순화되면서 오작동을 증가시킬 확률을 효과적으로 방지한다. 또한, 다양한 가로 및 세로 화면에 적응하는 투 사 로직을 제안하고, 가로 및 세로 투사는 휴대용 단말기의 상태를 식별하여 정확하게 판정된다. 본 발명의 투 사 해결방안은 신속한 투사를 차량 내 디스플레이 화면에서 구현할 뿐만 아니라, 다른 스마트 카메라와 같은 감 지 장치를 갖는 전자 장치, 예를 들어, 텔레비전, 노트북 컴퓨터, 태블릿 컴퓨터 또는 디스플레이로 확장될 수 있다."}
{"patent_id": "10-2022-7012657", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "다음은 예로서 본 출원의 실시예에서 첨부 도면을 사용하여 본 출원의 특정 구현을 설명한다. 그러나, 본 출원 의 구현은 본 출원의 사상 또는 범위를 벗어나지 않고 예를 들어, 다른 실시예를 사용하고 구조적 변경을 수행 하는 것과 같이 이러한 실시예를 결합하는 것을 더 포함할 수 있다. 따라서, 이하의 실시예의 상세한 설명은 제 한적인 의미로 이해되어서는 안 된다. 본 출원의 실시예에서 사용된 용어는 단지 본 출원의 특정 실시예를 설명 하기 위해 사용된 것으로, 본 출원을 한정하려는 의도가 아니다. 본 출원의 특정 실시예에서 언급된 기능, 모듈, 특징, 유닛 등의 하나 이상의 구조는 임의의 물리적 또는 유형 의 구성요소(예를 들어, 컴퓨터 장치(예를 들어, 프로세서 또는 칩에 의해 구현되는 논리 기능(logic function))에 의해 실행되는 소프트웨어 또는 하드웨어 및/또는 이들의 임의의 조합)에 의해 임의의 방식으로 구현되는 것으로 이해될 수 있다. 일부 실시예에서, 첨부 도면에 도시된 상이한 모듈 또는 유닛으로의 다양한 장치의 분할은 실제 구현에서 대응하는 상이한 물리적인 또는 유형의 구성요소의 사용을 반영할 수 있다. 선택 적으로, 본 출원의 실시예의 첨부 도면에서의 단일 모듈은 복수의 실제 물리적 구성요소에 의해 대안적으로 구 현될 수 있다. 유사하게, 첨부 도면에 도시된 2개 이상의 모듈은 또한 단일 실제 물리적 구성요소에 의해 수행 되는 상이한 기능을 반영할 수 있다. 본 출원의 실시예의 방법 흐름도에서, 일부 동작은 특정 순서로 수행되는 상이한 단계로서 설명된다. 이러한 흐 름도는 예시적이며 비제한적이다. 본 명세서에서 설명되는 일부 단계는 그룹화되어 단일 작업으로 수행될 수 있 고, 일부 단계는 복수의 하위 단계로 분할될 수 있으며, 일부 단계는 본 명세서에서 설명된 것과 다른 순서로 수행될 수 있다. 본 흐름에 도시된 단계는 임의의 회로 구조 및/또는 물리적 메커니즘(예를 들어, 컴퓨터 장치 (예를 들어, 프로세서 또는 칩에 의해 구현되는 논리적 기능)에서 실행되는 소프트웨어 또는 하드웨어) 및/또는 이들의 조합에 의해 임의의 방식으로 구현될 수 있다. 다음 설명에서, 하나 이상의 기능은 “선택 사항(optional)”으로 표시될 수 있다. 이러한 유형의 선언은 선택 사항으로 간주될 수 있는 특징의 완전한 표시로 설명되어서는 안 된다. 즉, 본 명세서에 명시적으로 식별되어 있지는 않지만, 다른 기능은 선택 사항으로 간주될 수 있다. 또한, 단일 개체의 설명은 복수의 그러한 개체의 사용을 배제하려는 것은 아니다. 유사하게, 복수의 개체의 설명은 단일 개체의 사용을 배제하려는 것이 아니다. 마지막으로, “예를 들어”라는 용어는 잠재적 구현 중 하나를 나타낸다. 본 출원의 실시예는 주로 투사 방법을 설명한다. 사용자가 투사를 시작할 동기가 있는지 여부를 판정하도록 휴 대용 단말의 현재 상태 및/또는 사용자 행위 의도가 분석된다. 정확하고 편리한 투사 동작을 구현하도록 투사 동작을 수행할지 여부는 사용자 투사 동기(motivation)에 기초하여 판정된다. 따라서 사용자가 투사를 수행하고 자 하는 경우, 사용자는 예를 들어 탭, 선택 또는 인터페이스 스위칭(interface switching)과 같은 복잡한 동작 을 수행할 필요가 없다. 본 발명의 해결방안은 차량 내 중앙 제어 화면의 신속하고 정확한 투사를 구현할 수 있을 뿐만 아니라, 카메라 와 같은 감지 장치, 예를 들어, 텔레비전, 노트북 컴퓨터, 태플릿 컴퓨터 또는 디스플레이를 갖는 다른 스마트전자 장치로 확장할 수 있다. 도 1은 본 발명의 일 실시예에 따른 예시적인 적용 시나리오, 차량 내 투사 시나리오를 도시한다. 감지 장치는 차량에 내장된 감지 장치 또는 외부 센서 장치인데, 사용자가 투사 동작을 수행하는 것을 의도하는지 여부를 판 정하도록 차량 내 프로세서 장치(도 1에 도시되지 않음)는 사용자 휴대용 장치 및/또는 사용자 행위의 상 태를 현재 사용자 휴대용 장치의 상태 정보 및/또는 감지 장치에 의해 획득된 사용자 손 행위 정보에 기초 하여 식별한다. 감지 장치는 사용자 휴대용 장치의 상태 정보 및/또는 사용자 손 행위 정보를 획득한다. 프로세서 장치와 같은 차량 내의 차량 내 제어 시스템은 사용자 의도를 사용자 휴대용 장치의 상태 정보 및/또는 감지 장치(10 3)에 의해 획득되는 사용자 손 행위 정보에 기초하여 판정한다. 예를 들어, 손이 차량 내 디스플레이 화면(10 2)에 접근하는지 여부 - 휴대용 장치의 상태는 행동 정보 내에서 식별된다 -, 사용자가 휴대용 장치 를 들고 있는지 여부, 휴대용 장치의 화면 방향이 차량 내 디스플레이 화면을 향하는지 여부, 표시를 위한 차량에서 차량 내 디스플레이 장치상의 사용자 휴대용 장치의 화면 콘텐츠에 대하여 투사 동작을 수 행할지 여부와 같은 식별된 정보의 하나 이상의 유형에 기초하여 행위 정보에서 식별되는 휴대용 장치의 상태를 판정한다. 보다 구체적인 시나리오에서, 차량을 사용하는 동안, 휴대용 장치의 화면상에서 내비게이션 정보를 보는 것은 불편하지만 사용자는 목적지를 검색하고 내비게이션을 시작하도록 휴대용 장치를 사용하거나, 사용자는 휴대용 장치상에서 차량 내 디스플레이 화면을 통해 비디오 또는 이미지를 보는 것을 예상하거나, 사용자는 동작을 위 해 휴대용 장치의 앱을 차량 내 디스플레이 장치 상에 투사할 것을 예상한다. 이 경우, 사용자는 하 나 이상의 합의된 조건에 기초하여 투사 동작을 수행할 수 있고, 조건은 사용자 휴대용 장치의 상태 정보일 수 있고/있거나 시스템이 사용자가 투사 의도를 갖는다는 것을 식별하는 것을 트리거하도록 사용되는 사용자 손 행 위 정보일 수 있다. 예를 들어, 사용자가 투사 의도를 이 순간에 갖고 있다고 판정하도록 사용자는 휴대용 장치 의 상태가 차량 내 디스플레이 장치를 향한다는 것을 감지 장치에 의해 획득되는 이미지 정보에 기초 하여 식별할 수 있다. 그런 다음 시스템은 투사 동작을 트리거한다. 일부 적용 시나리오에서, 동작이 단순화되지만 오작동이 증가할 확률을 피하고, 사용자 의도를 판정하는 것의 정확성을 보장하도록, 사용자 의도를 고려하기 위해 사용자 행위에 대한 종합적 분석이 더 수행될 수 있다. 이 경우에, 조건은 복수의 조건의 조합일 수 있다. 예를 들어, 합의된 조건은 사용자 손이 차량 내 디스플레이 장 치에 접근하고 있다는 것, 사용자가 휴대용 장치를 들고 있다는 것, 및 휴대용 장치의 상태가 차량 내 디스플레이 장치를 향한다는 것을 식별한다는 것이다. 세가지 조건이 모두 충족되면, 분석을 통해 사용자 의도가 투사를 시작하는 것임을 식별하여, 투사를 트리거하도록 투사 명령어가 전송된다. 가로 모드 또 는 세로 모드에서, 휴대용 장치의 디스플레이 화면상의 현재 목적지 내비게이션 인터페이스는 네비게이팅 (navigate)하거나 비디오를 재생하도록 사용자의 동작 지시에 따라 차량 내 디스플레이 화면상에 투사된다. 구 현에서 세가지 조건 중 임의의 두가지가 투사를 시작하도록 대안적으로 충족될 수 있다. 일부 적용 시나리오에서, 복수의 조건에 기초하여 사용자 의도에 대한 연속적인 탐지를 피하고 시스템의 계산량 을 줄이도록 복수의 조건의 종합 판정의 해결방안이 구현될 때, 복수의 조건은 연속적인 트리거 관계에 있다는 것이 선택될 수 있다. 복수의 조건 중 하나가 충족되지 않으면, 제1 조건은 처음부터 탐지 및 판정을 위해 반환 된다. 복수의 조건이 모두 연속적으로 충족되면, 투사 동작이 수행된다. 예를 들어, 세가지 조건은 사용자 손이 차량 내 디스플레이 장치에 접근한다는 것, 사용자가 휴대용 장치를 들고 있다는 것 및 휴대용 장치 의 상태가 차량 내 디스플레이 장치를 향한다는 것을 식별하는 순서이다. 구체적으로, 사용자 손이 차량 내 디스플레이 장치로 접근하는지 여부가 판정된다. 판정 결과가 아니오이면, 사용자 손이 차량 내 디스플레이 장치에 접근하는지 여부는 계속 판정된다. 판정결과가 예이면, 사용자가 휴대용 장치를 들고 있는지 여부가 더 식별된다. 유사하게, 판정결과가 아니오이면, 사용자 손이 차량 내 디스플레이 장치 로 접근하는지 여부를 판정하도록 수행되는 제1 단계로 복귀된다. 판정 결과가 예이면, 휴대용 장치 의 상태가 차량 내 디스플레이 장치를 향하는지 여부는 계속해서 판정된다. 판정 결과가 아니오이면, 사용 자 손이 차량 내 디스플레이 장치로 접근하는지 여부를 판정하도록 수행되는 제1 단계로 복귀된다. 판정 결과가 예이면, 투사가 트리거된 것으로 판정된다. 전술한 시퀀스 트리거 조건의 배열 시퀀스는 예시일 뿐, 그 대안으로 다른 배열 시퀀스일 수 있다. 마찬가지로, 판정 조건은 대안적으로 시퀀스 트리거 조건 중 임의의 두 가지일 수 있다. 또한, 전술한 시나리오에서, 사용자가 복수의 화면 디스플레이 모드를 필요로 할 수 있기 때문에, 현재 휴대용 장치의 화면 콘텐츠가 가로 모드 또는 세로 모드로 표시되는지 여부는 휴대용 장치의 장치가 수평 또 는 수직인지를 판정함으로써 더 판정될 수 있다. 따라서 투사 동작이 실행되기 전에 언제든지 자세가 판정된다. 전술한 차량 내 투사 시나리오에 더하여, 본 발명 내의 투사 해결방안은 감지 장치가 내장된 다양한 디스플레이 장치 또는 감지 장치에 연결될 수 있는 다양한 디스플레이 장치에 더 적용될 수 있다. 디스플레이 장치의 다른 유형의 적용 시나리오는 도 2에 도시되어 있고, 가정 또는 사무실 시나리오에서의 투사 동작에 적용될 수 있다. 본 발명에서의 투사 해결방안이 적용될 수 있는 장치는 디스플레이, 텔레비전, 노트북 컴퓨터 또는 카메라를 구 비하여 투사 동작이 수행될 수 있는 PAD(패드)를 더 포함할 수 있다. 전술한 해결방안의 특정 구현 세부사항은 다음의 실시예에서 자세히 설명된다. 본 발명에서 언급된 휴대용 장치는 휴대용 스마트 디바이스, 예를 들어, 휴대용 단말, 패드 또는 웨어러블 (wearable) 장치일 수 있다. 이후의 예에서, 휴대용 장치만이 예시로서 사용되지만, 휴대용 장치는 임의의 다른 휴대용 스마트 장치로 대체될 수 있다는 것이 이해될 수 있다. 차량 내 감지 장치는 카메라 또는 레이더와 같은 감지 장치일 수 있고, 사용자 행위 정보 또는 사용자 거리 정보를 감지하도록 구성된다. 본 발명에서 언급된 카 메라는 사용자 이미지 정보를 획득할 수 있는 임의의 감지 장치를 의미한다. 이미지 정보는 비디오 정보 또는 사진 정보일 수 있으며, 카메라는 기존의 카메라, 단안 카메라(monocular camera), 쌍안 카메라(binocular camera), 어안 카메라(fisheye camera) 또는 깊이 카메라(depth camera) 등 일수 있다. 본 발명에서 언급된 레 이더는 전자기파를 이용하여 목표를 탐지하는 전자 장치이고, 구체적으로 레이저 거리계(laser rangefinder), 적외선 거리계(infrared rangefinder) 및 초음파 거리계(ultrasonic rangefinder)를 포함할 수 있다. 사용자 행위 정보는 카메라에 의해 획득된 이미지 정보 및/또는 사용자의 레이더에 의해 획득된 특정 목표 부분의 거리 정보일 수 있다. 본 발명에서 언급되는 비디오 정보는 시간적으로 연속되는 복수의 프레임의 이미지 정보를 의 미할 수 있다. 도 1에 도시된 차량 내 감지 장치는 하나 이상의 동일 유형의 감지 장치일 수 있거나, 상이 한 유형의 감지 장치의 상이한 수량의 조합일 수 있다. 본 발명에서 언급된 차량 내 디스플레이 화면은 차량 내 에 배치되고 디스플레이에 사용될 수 있는 임의의 디스플레이 화면일 수 있다. 본 명세서에서 언급되는 중앙 제 어 화면은 차량 내 디스플레이 화면의 특정 예일 뿐이고, 특별히 한정되는 것은 아니다. 사용자 행위 및 휴대용 장치의 상태는 카메라를 사용하여 획득된다. 행위 식별 및 이미지 식별은 딥 러닝 방법 을 사용하여, 예를 들어, 컨볼루션 신경 네트워크 모델, 심층 신경 네트워크 모델 또는 행위 식별 또는 이미지 식별에 사용할 수 있는 기타 모델을 사용하여 수행될 수 있다. 획득된 사용자 행위에 기초하여, 사용자가 투사 를 수행하고자 한다는 것을 나타낼 수 있는 동작 및 상태가 있는지 여부가 분석된다. 예를 들어, 사용자가 투사 동작을 수행하는지 여부는 투사 동작을 수행하도록 판정하기 위하여 사용자 손 동작이 차량 내 디스플레이 화면 에 접근하는지 여부, 이미지에 휴대용 장치가 있는지 여부 또는 사용자가 휴대용 장치를 들고 있는지 여부, 및 휴대용 장치의 디스플레이 화면 방향이 차량 내 디스플레이 화면을 향하는지 여부를 판정하여 결정된다. 또는, 다른 사전 설정 동작이 사용될 수 있으며, 사용자가 사전 설정 동작을 수행하는 것으로 식별된다면 투사 동작이 시작된다. 특정 구현이 본 발명의 실시예에서 설명되는 경우, 이해의 편의를 위해, 특정 장치 이름 또는 기술적 수단을 사 용하여 해결방안을 설명한다. 그러나, 언급된 특정 장치 및 기술적 수단은 설명된 구현을 제한하도록 의도되지 않는다. 해결방안은 장치 또는 수단과 유사한 기능을 갖는 다른 장치 또는 수단을 사용하여 여전히 구현될 수 있다는 것이 이해될 수 있다. 도 3은 본 발명에 따른 투사 방법의 특정 구현이다 본 발명의 특정 구현은 차량이 예시로 사용되는 적용 시나리 오를 설명하지만, 차량은 해결방안의 적용 가능한 시나리오에 대한 제한으로 사용되지 않는다. 본 발명의 기술 적 해결방안은 또한 차량 내 디스플레이 화면이 스마트 텔레비전과 같은 디스플레이 장치로 대체되고 차량 내 카메라가 스마트 텔레비전의 내장형 카메라 또는 외장형 카메라로 대체되는 경우로 적용 가능하다. S101: 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치의 디스플레이 화면 방향을 판정. 감지 장치는 차량 카메라 또는 외부 카메라일 수 있다. 이미지 정보는 차량 내에서 감지 장치가획득한 현재 사 용자의 모습이며 비디오 또는 사진일 수 있다. 제1 장치는 휴대용 장치일 수 있고, 차량에서 심층 신경 네트워 크를 기초로 휴대용 장치 방향을 판정하기 위한 구현을 포함하는 현재 사용자의 모습에 기초하여 휴대용 장치 방향을 판정하기 위한 복수의 구현이 있을 수 있다. 휴대용 장치 방향 탐지 네트워크는 휴대용 장치 이미지 세트 X = {x0, x1, x2, ..., xn} 를 탐지한다. 대응하는 라벨 {0} 또는 {1}은 이진 분류 네트워크(binary classification network)를 학습시키도록 각각 휴대용 장치가정방을 향한다는 것 또는 휴대용 장치가 후방을 향한다는 것을 나타낸다. 이진 분류 네트워크는 입력 이미지 정 보에 기초하여 휴대용 장치의 방향 정보를 획득하는데 사용된다. 휴대용 장치의 방향 분류는 심층 신경 네트워크에 기초하여 구현된다. 분류의 원리도가 도 9에 도시된다. 휴대 용 장치 방향 탐지 네트워크의 입력은 제3 사용자 행위 정보이다. 휴대용 장치의 전면(Front)과 후면(Back)의 두 가지 범주를 예측해야 한다. 심층 신경 네트워크 모델을 기반으로 전면과 후면을 예측하는 구체적인 원리는 다음과 같다. 컨볼루션 연산이 입력 이미지에 연속적으로 적용되어 특징 정도의 크기는 지속적으로 줄어들고, 휴대용 장치 방향을 나타내는 특징 벡터가 획득되며, 그리고 최종적으로 휴대용 장치 방향의 예측 값은 완전히 연결된 층에 의해 직접 출력된다. S102: 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정되면, 제1 장치로 메시지를 전송, 제1 장치의 화면 콘텐츠를 목표 디스플레이 화면상에 표시 또는 목표 디스플레이 화면에 화면 콘텐츠를 표시하 도록 제1 장치에 통지. 사전 설정 방향은 휴대용 장치의 디스플레이 화면이 목표 디스플레이 화면을 향한다는 것 또는 디스플레이 화면 이 다른 방향을 향한다는 것일 수 있다. 분석 탐지 결과가 전술한 조건이 충족된다는 것인 경우, 투사 동작이 개시되는 것으로 판정된다. 여기서 목표 디스플레이 화면은 휴대용 장치의 화면 콘텐츠를 표시하도록 구성된 디스플레이 화면이고, 중앙 제 어 화면일 수 있거나 다른 차량 내 디스플레이 화면일 수 있거나 또는 차량이 아닌 환경에서 투사 콘텐츠가 표 시되는 임의의 다른 디스플레이 화면일 수 있다. 도 11은 본 발명에 따른 투사 방법의 다른 실시예이다. S201: 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여 목표 디스플레이 화면에 접근 하는 손의 동작이 있다고 판정, 또는 감지 장치에 의해 획득되고 손으로부터 감지 장치까지의 거리에 대한 정보 에 기초하여 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 판정. 사용자 행위 정보는 카메라, 레이더 등을 이용하여 획득된다. 사용자 행위 정보는 이미지 정보(이미지 정보는 비디오 또는 사진일 수 있음) 또는 거리 정보일 수 있고, 사용자 행위는 사전 설정 행위가 있는지 여부를 판정 하도록 획득된 행위 정보에 기초하여 탐지된다. 사전 설정 동작은 합의된 특별 동작일 수 있거나 시스템에 의해 미리 설정되어 사용자가 특별히 투사 의도를 가지고 있음을 나타내는 동작일 수 있다. 대안적으로, 장애물이 목 표 디스플레이 화면으로 접근한다고 판정되는 경우, 본 발명에서는, 예를 들어, 목표 디스플레이 화면(목표 디 스플레이 화면은 차량 내 중앙 제어 화면 또는 다른 차량 내 디스플레이 장치, 또는 다른 디스플레이 장치일 수 있음)에 접근하는 손의 동작이 투사 의도를 갖는 사용자의 동작 중 하나이다. 예를 들어, 감지 장치는 카메라이고, 사용자 행위 데이터는 구체적으로 시간 관계를 갖는 복수의 프레임의 비디 오 정보 또는 그림 정보일 수 있는 이미지 데이터이며, 이미지 정보에 기초하여 사용자 행위는 점차 목표 디스 플레이 화면으로 접근하는 손이라는 것이 식별되는 경우, 사용자는 투사 동작을 수행할 의도가 있는 것으로 간 주된다(즉, 사전 설정 동작은 중앙 제어 화면에 점차 접근하는 사용자 손임). 보다 구체적인 구현은 다음과 같 을 수 있는데, 차량 내 카메라 및 프로세서가 활성화된 상태에 있을 때, 차량 내 카메라는 연속적으로 사용자의 이미지 데이터를 포착하고 이미지 데이터를 프로세서 유닛에 전송하는 것일 수 있다. 이미지 데이터를 수신한 후, 프로세서 유닛은 차량 내 디스플레이 화면에 접근하는 사용자 손의 동작이 있는지 여부를 판정하도록 행위 탐지 동작을 수행한다. 일반적으로 이 기능을 구현하는 두가지 일반적인 구현 방식이 있고 두가지 일반적인 방 식이 각각 설명된다. ·3DCNN 기반 행위 식별 네트워크 행위 식별은 일련의 시간 관련 동작을 포함하기 때문에, 3DCNN은 중앙 제어 화면에 접근하는 손의 동작이 있는 지 여부를 판정하도록 시간 정보를 1차원의 입력으로 직접 사용하도록 사용될 수 있다. 구체적인 단계는 도 5에 도시되어 있다. (a)비디오 스트림 획득: 차량 내 디스플레이 화면에 접근하는 사용자 손의 동작이 있는지 여부를 탐지하도록, 멀티 프레임 이미지가 판정에 필요하다. 따라서, 카메라에 의해 포착된 사용자 조종석 비디오 스트림 데이터는 후속 행위 탐지 및 분석에 사용된다. (b)행위 식별 모델: 이 모델은 사용자가 획득된 비디오 스트림 데이터에 기초하여 특정 행위를 갖는지 여부를 분석하고 판정하는데 사용된다. 현재 주류 방법은 사용자 정보를 포함하는 비디오 스트림(멀티 프레임 이미지)을 분석하도록 일부 딥러닝 방법을 사용하는 것이고, 최종적으로 특정 행위 분류 결과를 획득한다. 중앙 처리 장치(central processing unit)에서, 일부 컨볼루션 네트워크(예를 들어, 3DCNN)는 중앙 제어 화면에 접근하는 사용자 손의 동작이 있는지 여부를 분석하도록 실행될 수 있다. 3DCNN 기반 행위 식별 네트워크의 학습 과정은 다음과 같다. 학습: 비디오 스트림 이 있고, 일 때 는 카메라에 의해 획득되는 비디오 스트림 프레임이라 고 가정한다. 분석 데이터를 선택하는 동안, 연속적인 비디오 스트림 프레임이 선택될 수 있다. 획득된 비디오 스트림 프레임이 과도하게 유사한 것을 방지하기 위해, 하나의 프레임은 미리 판정된 시간 간격 finterval에서 취 해진다는 것 또는 하나의 프레임은 미리 판정된 프레임의 양 j의 간격에서 취해지고, t 프레임은 누적적으로 취 해지며, j와 t는 양의 정수이고, t-프레임 이미지 또는 은 학습 샘플의 입력으로 사용된다. t-프레임 이미지 x의 레이블(label) y는 {0,1} 또는 {1,0}이며, 각각은 중앙 제어 화면에 접근하지 않는 동작 또는 중앙 제어 화면에 접근하는 동작을 나타낸다. 또는, 다른 레이블 설정 방 식이 있을 수 있다. 이는 여기서의 예시일 뿐이다. 복수의 비디오 세그먼트는 전체 트레이닝 샘플 세트 X= 를 획득하도록 사용되며, m은 샘플 세트에서 샘플의 양을 나타낸다. 대응하는 학습 레이블은 Y= 이다. 학습 샘플 세트는 입력으로 사용되고 학습 레이블은 3DCNN 모델을 학습하도록 알려진 출력 결 과로 사용되어 행위 식별 모델이 획득된다. 행위 식별 프로세스: 비디오 스트림의 한 세그먼트의 한 프레임은 사전 판정된 시간 간격 finterval에서 또는 프레 임 j의 사전 판정된 양의 간격에서 획득되고 t 프레임은 누적적으로 획득된다. 전술한 방식은 학습하는 동안 샘 플 획득의 방식에 의존하고, 일반적으로 샘플 획득의 방식과 동일하다. 예를 들어, 샘플은 동일한 간격으로 t- 프레임 이미지를 획득하는 것에 의해 획득된다. 따라서, 행위 식별 프로세스에서, t-프레임 이미지는 또한 분석 이 필요한 비디오 스트림에 대해 동일한 간격으로 획득되고, t-프레임 이미지는 예측 값{p1, p2}를 획득하도록 네트워크의 입력으로 사용된다. 만약 p1>p2라면, 중앙 제어 화면에 접근하는 동작이 있음을 나타내고, 그렇지 않으면, 중앙 제어 화면에 접근하는 동작이 없음을 나타낸다. 대안적으로, 다른 판정 규칙이 사용될 수 있다. 예를 들어, p1>0.8이면, 중앙 제어 화면에 접근하는 동작이 있음을 나타내고, 그렇지 않으면, 중앙 제어 화면에 접근하는 동작이 없음을 나타낸다. 이는 여기에서 제한되지 않는다. ·2DCNN 기반 행위 식별 네트워크 도 6에 도시된 바와 같이, 3DCNN외에, 본 발명에서 손이 중앙 제어 화면에 접근할 확률을 획득하기 위해, 2DCNN 은 특징 추출 및 판정을 수행하도록 더 사용될 수 있다. 손이 중앙 제어 화면에 접근하는지 여부의 최종 결과를 획득하기 위해, 연속된 프레임의 확률 값은 규칙 알고리즘에 따라 최종적으로 수집된다. 학습: 비디오 스트림 { }가 있다고 가정하고, Fi는 카메라에 의해 획득된 비디오 스트림 프레 임인데, 여기서 , 이고 비디오 스트림에 대응하는 레이블 { }이 있다. 네트워크의 입력은 비 디오 스트림 프레임 Fi이다. 행위 식별 네트워에 의한 식별 이후에, 손이 중앙 제어 화면 앞에 있을 확률이 획득 된다. 행위 식별 프로세스: 손이 중앙 제어 화면 앞에 있을 확률 { }을 획득하도록, 비디오 스트림 { }의 하나의 세그먼트가 입력된다. 시간 윈도우의 크기는 t 프레임으로 선택되고, t-프레임 이미지의 확률이 수집된다. 확률이 오름차순이고 최대 확률 값이 임계값보다 크면, 중앙 제어 화면에 접근하는 동작이 있는 것으로 간주되고, 그렇지 않으면, 중앙 제어 화면에 접근하는 동작이 없는 것으로 간주된다. 예를 들어, 감지 장치는 깊이 카메라 또는 레이더이고, 사용자 행위 데이터는 사용자로부터 감지 장치까지의 거 리이다. 사용자로부터 감지 장치까지의 거리에 대한 정보가 깊이 카메라 및/또는 레이더를 사용하여 식별되는 경우, 사용자 손으로부터 감지 장치까지의 거리에 대한 정보는 깊이 이미지의 분석에 기초하여 획득될 수 있고, 그 다음, 사용자 손으로부터 목표 디스플레이 화면(차량 내 디스플레이 화면)까지의 거리에 대한 정보는 사용자 손으로부터 감지 장치까지의 거리에 대한 정보 및 차량 내 구조 정보에 기초하여 추정된다. 추정된 거리 정보가 사전 설정 값보다 작은 경우, 사용자가 투사 동작을 수행할 의도를 가지고 있다고 판정된다(즉, 사전 설정 조건 은 사용자 손으로부터 목표 디스플레이 화면(차량 내 디스플레이 화면)까지의 거리에 대한 정보가 사전 설정 값보다 작다는 것이다). 보다 구체적인 구현은, 차량 내 깊이 카메라 및/또는 레이더 그리고 프로세서가 비활성화 된 상태에 있는 경우, 차량 내 깊이 카메라 및/또는 레이더는 연속적으로 사용자의 깊이 이미지 데이터를 포착 하고, 깊이 이미지 데이터를 프로세서 유닛에 전송한다. 깊이 이미지 데이터를 수신한 이후에, 프로세서 유닛은 사용자 손 및 목표 디스플레이 화면(차량 내 디스플레이 화면) 사이의 거리에 대한 정보를 추정하도록 분석 및 프로세싱을 수행한다. S202: 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있는지 판정. S201 및 S202에서 감지 장치는 동일한 감지 장치일 수 있고, 상이한 감지 장치 일 수 있는데, 예를 들어, 각각 상이한 유형의 카메라 또는 다른 위치에 배치된 동일한 유형의 카메라일 수 있거나, 각각 레이더 또는 카메라일 수 있다. 이미지 정보가 S201에서 획득되는 경우, S201에서 획득된 이미지 정보는 S202에서 획득된 이미지 정보 와 동일하거나 상이할 수 있다. S202에서의 이미지 정보는 S201에서 이미지 정보와 동일한 비디오 정보일 수 있 거나, S201에서 비디오 또는 멀티 프레임 그림 정보로부터 추출된 그림 정보일 수 있고, 추가적으로 이미지 정 보를 획득할 필요는 없으며, 감지 장치에 의해 획득되고 S201에서의 이미지 정보와 동일하거나 상이한 자원을 갖는 새로운 이미지 정보일 수 있다. 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정하기 위한 복수의 구현이 있는데, 다중 물체 탐지 네트워크에 기초하여 판정 방식을 보유하는 투사 대상 장치와 단일 물체 탐지 네트워크 에 기초하여 판정 방식을 보유하는 휴대용 장치를 포함한다. 아래에서 방식을 자세히 설명한다. ·다중 물체 탐지 네트워크에 기초한 판정을 보유하는 투사 대상 장치 다중 물체 탐지 네트워크의 탐지 방법에 기초하여, 사용자가 휴대용 장치를 들고 있는지 여부는 먼저 휴대용 장 치의 프레임 및 이미지에서 손의 프레임을 탐지하여 판정될 수 있다. 본 발명에서, 딥 러닝 방법은 탐지 결과를 획득하기 위해, 제2 사용자 행위 이미지를 분석하는데 사용된다. 즉, 프로세서는 투사 대상 장치 및 손의 프레 임 위치를 제공하도록 물체 탐지 네트워크(딥 러닝 모델)를 실행한다. 사용자가 투사 대상 장치를 들고 있는지 여부가 판정된다. 손의 프레임 및 투사 대상 장치의 프레임의 위치 정보를 획득한 이후에, 손의 프레임과 투사 대상 장치의 프레임 사이의 통합대 중첩비(intersection over union, IOU)가 계산된다."}
{"patent_id": "10-2022-7012657", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "IOUhand_handheld terminal>TIOU인 경우, 사용자가 투사 대상 장치를 들고 있는 것으로 간주될 수 있고, 그렇지 않으면, 사용자가 투사 대상 장치를 들고 있지 않는 것으로 간주될 수 있다. TIOU값은 0과 1사이의 값이다. 여기서, 값은 0.5 또는 다른 값일 수 있다. 도 7은 투사 대상 장치는 휴대용 단말인 예를 사용하여 이 해결방안에서의 식별 방법의 예를 도시한다. 물체 탐 지 네트워크는 동시에 손과 휴대용 단말과 같은 휴대용 장치를 포함하는 두 개의 물체를 감지할 수 있다. 그런 다음, 손과 휴대용 단말(또는 다른 휴대용 장치) 사이의 통합대 중첩비(IOU)가 계산되고, IOU가 지정된 임계값 보다 큰지 여부가 판정된다. IOU가 임계값보다 크면, 휴대용 장치는 들려 있고, 그렇지 않으면, 휴대용 장치는 들려 있지 않다. 학습: 샘플 이미지 { }와 단순한 이미지에 대응하는 레이블 { }이 있다고 가 정한다. 여기서 레이블 yi는 구조를 나타내고, yi = {phand, xhand, yhand, 너비hand, 높이thand, pphone, xphone, yphone, widthphone, heightphone}이다. phand은 물체가 손일 확률을 나타내고, xhand, yhand, 너비hand 및 높이hand는 손의 위치 정보를 나타낸다. Pphone는 물체가 휴대용 단말일 확률을 나타내고, xphone, yphone, 너비phone 및 높이phone는 휴대용 단말의 위치 정보를 나타낸다. 이 데이터의 배치(batch)는 물체 탐지 네트워크를 학습시키는데 사용된다. 최종 적으로, 물체 탐지 네트워크가 획득되고, 입력 이미지에 기초하여 이미지에 손 과 휴대용 장치가 있는지 여부 및 휴대용 장치와 사용자 손의 위치를 출력하도록 사용된다. 그런 다음, IOU는 전술한 입력 데이터에 기초하여 계산되고, 사용자가 휴대용 장치를 보유하고 있는지 여부가 판정된다. 전술한 위치 정보는 예시일 뿐이다. 너비와 높이 이외에, 휴대용 장치 및 손의 왼쪽 위 꼭짓점 P1(xleft, ytop) 및 오른쪽 아래 꼭짓점 P2(xright, ybottom) 그리고 또한 학습을 통해 획득된다. 휴대용 장치의 꼭짓점 위치의 개략 도는 도 10에 도시되어 있다. 그리고, 휴대용 장치 및 손의 높이 및 너비는 왼쪽 위 꼭짓점 P1(xleft, ytop) 및오른쪽 아래 꼭짓점 P2(xright, ybottom)에 기초하여 각각 획득되고, IOU가 계산된다. 선택적으로, 손 및 휴대용 장치가 이미지에 있는지 여부만을 판정함으로써 휴대용 장치는 들려 있는 것으로 간 주되고, 추가 판정이 수행된다. 특정 구현은 다음과 같을 수 있다. 학습: 샘플 이미지 { }와 샘플 이미지에 대응하는 레이블 { }이 있다고 가정 한다. 레이블 yi는 여기서 구조를 나타내며 yi = {p손, p전화}이다. 전술한 구현과 동일하게, p손은 물체가 손일 확 률을 나타내고, p전화는 물체가 휴대용 장치일 확률을 나타낸다. 이 데이터의 배치(batch)는 물체 탐지 네트워크 를 학습시키는데 사용된다. 최종적으로, 물체 탐지 네트워크는 획득되고, 입력 이미지에 기초하여 손과 휴대용 장치 또는 다른 휴대용 장치가 이미지에 있는지 여부를 출력하도록 사용된다. 손과 휴대용 장치 또는 다른 휴대 용 장치가 이미지에 있는 경우, 사용자가 휴대용 장치를 들고 있는 것으로 간주된다. ·단일 물체 탐지 네트워크에 기초한 판정을 보유하는 휴대용 단말 손은 일반적으로 휴대용 단말을 들기 때문에, 손과 휴대용 장치를 탐지하는 네트워크를 학습할 필요가 없다. 이 방법에서, 단일 물체 탐지 네트워크는 학습된다. 네트워크는 휴대용 장치만을 감지한다. 도 8에 도시된 바와 같 이, 네트워크가 휴대용 장치를 탐지한다면, 휴대용 장치가 들려 있는 것으로 간주되고, 그렇지 않다면, 휴대용 장치가 들려 있지 않은 것으로 간주된다. 샘플 이미지 { } 와 샘플 이미지에 대응하는 레이블 { }이 있다고 가정한다. 여기서, 레이블 yi는 {0,1} 또는{1,0}이며, 각각은 휴대용 장치가 이미지에 있음을 또는 이미지에 휴대용 장치 가 없음을 표시한다. 대안적으로, 다른 레이블 설정 방식이 있을 수 있다. 이것은 여기에서 예시일 뿐이다. 이 데이터의 배치(batch)는 물체 탐지 네트워크를 학습시키는데 사용된다. 최종적으로, 단일 물체 탐지 네트워크 (딥 러닝 모델)은 획득되고 휴대용 장치가 이미지에 있는지 여부를 탐지하는데 사용될 수 있다. 휴대용 장치가 이미지에 있는 경우, S201의 판정 결과를 참조하면, 이때의 상태는 사용자가 들고 있는 휴대용 장치가 목표 디 스플레이 화면에 접근하는 것으로 유추될 수 있다. S203: 감지 장치에 의해 획득된 이미지 또는 비디오 정보에 기초하여 제1 장치의 디스플레이 화면 방향을 판정. S203 및 다른 두 단계의 감지 장치는 동일한 감지 장치이거나 상이한 감지 장치일 수 있는데, 예를 들어, 각각 상이한 유형의 카메라 또는 서로 다른 위치에 배치된 동일한 유형의 카메라일 수 있거나, 각각 레이더 및 카메 라일 수 있다. 이미지 정보가 S201 및 S202에서 획득되는 경우, 세개의 단계에서 이미지 정보는 동일한 정보 또 는 상이한 정보일 수 있다. S203 및 S202에서 이미지 정보는 S201에서의 이미지 정보와 동일한 비디오 정보일 수 있거나, S201에서의 비디오 또는 멀티 프레임 그림 정보로부터 추출된 그림 정보일 수 있고, 추가적으로 이 미지 정보를 획득할 필요가 없거나 감지 장치에 의해 획득되고 S201에서의 이미지 정보와 같이 동일한 소스 또 는 상이한 소스를 갖는 새로운 이미지 정보일 수 있다. 대안적으로, S203에서 이미지 정보는 S202에서 이미지 정보와 동일할 수 있거나, 감지 장치에 의해 획득되고 S202에서의 이미지 정보와 같이 동일한 소스 또는 상이한 소스를 가지는 감지 장치에 의해 획득되는 새로운 이미지 정보일 수 있다. 현재 사용자의 차량 내 모습에 기초하여 휴대용 장치 방향을 판정하기 위한 복수의 구현이 있을 수 있다. 이는 전술한 단계 S101에서 상세히 설명되었다. 세부 사항은 여기서 다시 설명되지 않는다. S204: 목표 디스플레이 화면에 접근하는 손의 동작이 있고, 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 향한다고 판정되는 경우, 목표 디스플레이 화면상에 화면 콘텐츠를 표시하도록 제1 장 치에 통지하기 위해 제1 장치로 메시지를 전송. 목표 디스플레이 화면에 접근하는 손의 동작이 있고, 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 향한다고 판정되는 경우, 세 개의 판정 동작은 동시에 수행될 수 있거나, 순차적으로 수행될 수 있다. 특정 구현에서, 세 개의 판정 단계 모두 수행될 수 있다. 즉, 목표 디스플레이 화면에 접근하는 손의 동작이 있는지 여부가 판정되고, 제1 장치가 들려 있는지 여부가 판정되고, 그리고 제1 장치의 디스플레이 화면 방향이 사전 설정 방향인지 여부가 판정된다. 그런 다음, 모든 결과가 예이면, 투사를 수행한다고 판정된다. 대 안적으로, 세 개의 단계는 순차적으로 그리고 연속적으로 트리거(trigger)될 수 있다. 예를 들어, 목표 디스플 레이 화면에 접근하는 손의 동작이 있는지가 판정된다. 판정 결과가 예이면, 제1 장치가 들려 있는지 여부는 계 속 판정된다. 판정 결과가 아니오이면, 목표 디스플레이 화면에 접근하는 손의 동작이 있는지 여부는 계속 탐지 된다. 유사하게, 제1 장치가 들려 있다고 판정되는 경우, 제1 장치의 디스플레이 화면 방향이 사전 설정된 방향인지 여부가 더 판정된다. 사용자가 제1 장치를 들고 있지 않다고 판정되는 경우, 목표 디스플레이 화면에 접근 하는 손의 동작이 있는지 여부를 판정하는 제1 단계로 복귀하여 수행된다. 유사하게, 제1 장치의 디스플레이 화 면 방향이 사전 설정 방향을 충족하지 않는다는 것이 탐지되는 경우, 목표 디스플레이 화면에 접근하는 손의 동 작이 있는지 여부를 판단하는 제1 단계로 복귀하여 수행되고, 제1 장치의 디스플레이 화면 방향이 사전 설정 방 향을 충족한다는 것이 탐지되는 경우, 투사 동작이 트리거된다. 판정 단계를 트리거하는 전술한 시퀀스 (sequence)는 단지 예일 뿐이며, 변경될 수 있다. 임의의 단계의 판정 결과가 아니오이면, 제1 단계로 돌아가 처음부터 탐지 및 판정을 수행한다. 다른 실시예에서, S201 내지 S203에서 임의의 두개의 판정 단계만이 수행될 수 있다. 예를 들어, 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여, 목표 디스플레이 화면 에 접근하는 손의 동작이 있다고 판정된다. 제1 장치의 디스플레이 화면 방향은 감지 장치에 의해 획득된 이미 지 또는 비디오 정보에 기초하여 판정된다. 목표 디스플레이 화면에 접근하는 손의 동작이 있고 및 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정되는 경우 화면 콘텐츠를 목표 디스플레이 화면 에 표시하도록 제1 장치에 통지하기 위해 메시지가 제1 장치로 전송된다. 감지 장치에 의해 획득되고 손으로부터 감지 장치까지의 거리에 대한 정보에 기초하여, 손으로부터 목표 디스플 레이 화면까지의 거리가 사전 설정 값보다 작다고 판정된다. 제1 장치의 디스플레이 화면 방향은 감지 장치에 의해 획득된 이미지 또는 비디오 정보에 기초하여 판정된다. 손으로부터 목표 디스플레이 화면까지의 거리가 사 전 설정 값보다 작다는 것 및 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정되는 경우 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 메시지가 제1 장치로 전송된 다. 제1 장치의 디스플레이 화면 방향은 감지 장치에 의해 획득된 이미지 또는 비디오 정보에 기초하여 판정된다. 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치가 들려 있다고 판정된다. 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정되는 경우 목표 디스플레이 화면에 화면 콘텐츠를 표시하도록 제1 장치에 통지하기 위해 메시지가 제1 장치로 전송된다. 유사하게, 전술한 두개의 판정 단계는 동시에 또는 개별적으로 수행될 수 있으며, 판정 시퀀스는 제한되지 않는 다. 대안적으로, 전술한 두개의 판정 단계는 연속적인 트리거링 관계에 있을 수 있다. 도 3 및 도 11의 실시예는 하나의 투사 대상 장치의 자세 판정의 단계 S203a을 더 포함할 수 있다. S203a: 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치의 자세를 판정 및 제1 장치의자세에 기초하 여 목표 디스플레이 화면의 투사 디스플레이 모드를 판정. S203a는 S203 이후 및 S204 이전에 수행될 수 있다. 즉, 투사 대상 장치의 화면 방향이 사전 설정 조건을 충족 한다고 판정된 후에, 투사 대상 장치의 자세가 판정되고, 그런 후에 투사 동작을 가로 모드에서 또는 세로 모드 에서 수행할지 여부는 자세에 따라 판정된다. 대안적으로, S203a는 투사 통지가 전송되기 이전에 수행될 수 있 다. 즉, 투사 대상 장치의 화면 방향이 사전 설정 조건을 충족한다고 판정된 후에, 투사 동작이 수행되도록 판 정된다. 이 경우에, 투사 대상 장치의 자세는 더 판정되고, 투사 동작을 가로 모드 또는 세로 모드에서 수행할 지 여부는 자세에 따라 판정된다. 대안적으로, S203a에서의 자세 판정은 투사 동작이 이 해결방안에서 수행되기 전 임의의 순간에 발생할 수 있다. S203a가 존재하지 않는다면, 투사는 S204에서 디폴트(default) 투사 디스플레이 모드에서 수행된다. 유사하게, 도 3에서, S203은 투사 동작이 도 3의 실시예에서 수행되기 이전에 임의의 순간에도 발생할 수 있다. 유사하게, S203a에서 감지 장치는 이미지 정보가 획득될 필요가 있는 다른 단계에서의 감지 장치와 동일하거나 또는 상이할 수 있고, 이미지 정보는 또한 이미지 정보가 획득될 필요가 있는 다른 단계에서의 이미지 정보일 수 있거나, 독립적으로 획득될 수 있다. 투사 대상 장치의 자세의 판정은 주로 투사 대상 장치의 배치 위치 정보의 획득에 의존한다. ·투사 대상 장치의 배치 자세 정보는 히스토리 판정 단계에 기초하여 획득될 수 있다. 즉, 다중 물체 탐지 네트워크에 기초한 판정을 보유하는 투사 대상 장치가 S202에서 사용되는 경우, 물체 탐지 네트워크는 입력 이미지에 기초하여 손 및 휴대용 장치가 이미지에 있는지 여부 및 휴대용 장치의 높이(높이 phone) 및 너비(너비phone)를 획득하도록 휴대용 장치 및 사용자 손의 위치 yi = {phand, xhand, yhand, widthhand,heighthand, pphone, xphone, yphone, widthphone, heightphone}를 출력하도록 사용된다. 휴대용 장치의 높이와 너비가 비 교된다. 너비가 높이보다 크다면, 장치는 가로 모드인 것으로 간주되고, 가로 투사가 수행된다. 높이가 너비보 다 크면, 화면이 세로 모드인 것으로 간주되고, 세로 투사가 수행된다. S202에서, 휴대용 장치의 좌측 위 꼭짓점 P1(xleft, ytop) 및 우측 아래 꼭짓점 P2(xright, ybottom)이 획득되고, 휴 대용 단말의 높이들(높이) 및 너비들(너비)는 다음 공식을 사용하여 계산할 수 있다."}
{"patent_id": "10-2022-7012657", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "공식와 공식의 계산 후, 휴대용 단말의 길이 및 너비를 비교한다. 너비가 높이보다 크다면, 장치는 가로 모드로 간주되고, 가로 투사가 수행된다. 유사하게, 높이가 너비보다 크다면, 장치가 세로 모드인 것으로 간주되고, 세로 투사가 수행된다. ·투사 대상 장치의 배치 자세 정보는 전용 자세 탐지 네트워크를 기초로 획득될 수 있다. 휴대용 장치의 위치 좌표 정보 또는 길이 및 너비 정보 획득이 어떠한 이력 단계에 포함되지 않는 경우, 예를 들어, 휴대용 장치 및/또는 손이 S202에 존재하는지 여부만을 판정하는 경우, 휴대용 장치의 자세는 S203a에서 자세 탐지 네트워크에 기초하여 획득될 필요가 있다. 예를 들어, 샘플 이미지 { } 및 샘플 이미지에 대응하는 레이블 { }이 있다 고 가정한다. 여기서 레이블 yi는 구조를 나타낼 수 있고, yi = {pphone, xphone, yphone, 너비phone, 높이phone}이다. Pphone는 물체가 휴대용 단말일 확률을 나타내고, xphone, yphone, widthphone, and heightphone는 휴대용 단말의 위치 정보를 나타낸다. 대안적으로, yi = {pphone, xphone, yphone, 너비phone, 높이phone}이다. P1 (xleft, ytop)는 휴대용 장 치의 왼쪽 위 모서리에 있는 꼭짓점 위치를 나타내고 P2 (xright, ybottom)는 휴대용 장치의 오른쪽 아래 모서리에 있는 꼭짓점 위치를 나타낸다. 이를 기반으로, 자세 탐지 네트워크는 자세 식별 네트워크를 획득하도록 학습된 다. 획득된 물체 탐지 네트워크는 입력 이미지에 기초하여 휴대용 장치의 위치 좌표 정보 또는 길이 및 너비 정 보를 출력하도록 사용된다. 그 후, 휴대용 장치의 자세는 전술한 출력 데이터에 기초하여 판정되고, 투사 디스 플레이 모드가 판정된다. 유사하게, 도 3에서 휴대용 장치의 자세는 S202에서 언급된 다중 물체 탐지 네트워크에 기초하여 판정될 수 있 거나 전용 자세 탐지 네트워크에 기초하여 획득될 수 있다. 도 12는 본 발명에 따른 투사 방법의 다른 구현이다. 세부사항은 다음과 같다. S301: 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여 목표 디스플레이 화면에 접근 하는 손의 동작이 있는지 판정, 또는 감지 장치에 의해 획득되고 손으로부터 감지 장치까지의 거리에 대한 정보 에 기초하여 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 판정. 단계 S301의 구현은 단계 S201의 구현과 동일하다. 구체적으로, 사용자 행위 정보는 차량 내 카메라 또는 레이 더와 같은 감지 장치 사용에 의해 획득된다. 사용자 행위 정보는 비디오 정보 또는 거리 정보일 수 있고, 사용 자 행위는 사전 설정 행위가 있는지 여부를 판정하도록 획득된 이미지 정보 또는 획득된 거리 정보에 기초하여 탐지된다. 구체적인 행위 정보 획득 방식 및 사용자 행위 탐지 판정 방식에 관하여는 S201의 관련 설명을 참조 한다. 세부사항은 여기서 다시 설명되지 않는다. S302: 감지 장치에 의해 획득된 이미지 정보에 기초하여 사용자가 제1 장치를 들고 있는지 여부 및 제1 장치의 디스플레이 화면 방향을 판정. 구체적으로, 감지 장치에 의해 획득된 이미지 정보에 기초하고 신경 네트워크 모델에 기초하여 사용자가 제1 장 치를 들고 있는지 여부 및 제1 장치의 디스플레이 화면 방향이 판정된다. 전술한 실시예와 동일하게, S301 및 S302는 동시에 또는 별도로 수행될 수 있으며, 또한 트리거링 관계에 있을 수도 있다. 예를 들어, 후속 판정 단계 S302를 계속할 것인지 여부는 S301의 판정 결과에 기초하여 판정될 수 있다. 중앙 제어 화면에 접근하는 손의 동작이 있다고 판단되면, S302가 수행된다. 휴대용 장치를 들고 있는 사 용자의 동작이 있는지 여부는 더 판정되고, 투사 대상 장치의 디스플레이 화면 방향이 판정된다. 중앙 제어 화면에 접근하는 손의 동작이 없다면, 중앙 제어 화면에 접근하는 손의 동작이 있는지 여부는 계속 탐지된다. S301의 카메라 사용에 의해 이미지 정보가 획득되는 경우, S302의 이미지 정보는 S301의 이미지정보와 완전히 동일하거나 부분적으로 동일한 정보일 수 있거나, 동일한 카메라에 의해 획득된 상이한 정보일 수 있거나, 상이 한 카메라에 의해 획득된 상이한 정보일 수 있다. S302에서 감지 장치에 의해 획득된 이미지 정보에 기초하여 다중 물체 탐지 네트워크에 기초하여 투사 대상 장 치가 판정 방식을 보유하는 것 및 단일 물체 탐지 네트워크에 기초하여 휴대용 장치가 판정 방식을 보유하는 것 을 포함하여, 사용자가 투사 대상 장치를 들고 있는지 여부 및 디스플레이 화면 방향 판정을 위한 복수의 구현 이 있을 수 있다. 다음은 그 방식에 대해 자세히 설명한다. ·투사 대상 장치가 다중 물체 탐지 네트워크에 기초한 판정을 보유. 다중 물체 탐지 네트워크의 탐지 방법에 기초하여, 사용자가 휴대용 장치를 들고 있는지 여부는 먼저 휴대용 장 치의 프레임 및 이미지에서 손의 프레임을 탐지하여 판정될 수 있다. 본 발명에서는, 딥러닝 방법은 탐지 결과 를 얻도록, 제2 사용자 행위 이미지를 분석하도록 사용된다. 즉, 프로세서는 투사 대상 장치 및 손의 프레임 위 치를 제공하도록 물체 탐지 네트워크(딥 러닝 모델)를 실행한다 사용자가 투사 대상 장치를 들고 있는지 여부가 판정된다. 물체 탐지기를 사용하여 손의 프레임 및 투사 대상 장치의 프레임의 위치 정보를 획득한 이후, 손의 프레임 및 투사 대상 장치의 프레임 사이의 통합대 중첩비(intersection over union, IOU)가 계산된다."}
{"patent_id": "10-2022-7012657", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "IOUhand_handheld terminal>TIOU일 때, 사용자가 투사 대상 장치를 들고 있는 것으로 간주될 수 있고, 그렇지 않다면, 사 용자가 투사 대상 장치를 들고 있지 않는 것으로 간주될 수 있다. TIOU의 값은 0과 1사이이다. 여기서, 값은 0.5 또는 다른 값일 수 있다. 도 15는 투사 대상 장치가 휴대용 단말인 예시를 사용하여 이 해결방안에서 휴대용 장치가 들려 있는지 여부를 판정하기 위한 식별 방법의 예를 도시한다. 물체 탐지 네트워크는 동시에 손과 휴대용 단말과 같은 휴대용 장치 를 포함하는 두개의 물체 및 휴대용 단말의 방향을 탐지할 수 있다. 그런 다음, 손과 휴대용 단말(또는 다른 휴 대용 장치) 사이의 통합대 중첩비(IOU)가 계산되고, IOU가 지정된 임계값 보다 큰지 여부가 판정된다. IOU가 임 계값보다 큰 경우, 휴대용 장치는 들려 있고, 그렇지 않으면, 휴대용 장치는 들려 있지 않다. 학습: 샘플 이미지 { } 및 단순한 이미지에 대응하는 레이블 { }이 있다고 가정한다. 여기에서 레이블 yi는 구조를 나타내며, yi = {phand, xhand, yhand, 너비hand, 높이hand, pphone, xphone, yphone, 너비phone, 높이phone, p}이다. phand은 물체가 손일 확률을 나타내고, xhand, yhand, 너비hand, 높이hand은 손의 위치 정보를 나타낸다. 유사하게, pphone는 물체가 휴대용 단말일 확률을 나타내며, xphone, yphone, 너비phone, 높이 phone는 휴대용 단말의 위치 정보를 나타낸다. P는 목표 디스플레이 화면을 향할 확률을 나타낸다. 이 데이터의 배치(batch)는 물체 탐지 네트워크를 학습시키는데 사용된다. 마지막으로, 물체 탐지 네트워크는 획득되고, 입 력 이미지에 기초하여 이미지에 손 및 휴대용 장치가 있는지 여부, 휴대용 장치 및 사용자 손의 위치 및 목표 디스플레이 화면의 방향을 출력하도록 사용된다. 그런 다음, IOU는 전술한 출력 데이터에 기초하여 계산되고, 사용자가 휴대용 장치를 들고 있는지 여부가 판정된다. 전술한 위치 정보는 단지 예시일 뿐이다. 너비와 높이 외에, 휴대용 장치의 왼쪽 위 꼭짓점 P1(xleft, ytop) 및 오른쪽 아래 꼭짓점 P2(xright, ybottom) 및 손은 또한 학습을 통해 획득될 수 있다. 휴대용 장치의 꼭짓점 위치의 개략도가 도 10에 도시되어 있다. 그런 다음, 왼쪽 위 꼭짓점 P1(xleft, ytop) 및 오른쪽 아래 꼭짓점 P2(xright, ybottom)에 기초하여 휴대용 장치 및 손의 높이들(높이) 및 너비들(너비)이 각각 획득되고, IOU가 계산된다. 선택적으로, 손 및 휴대용 장치가 이미지에 있는지를 판정함으로써만 휴대용 장치가 들려 있는 것으로 간주되고, 화면 방향이 판정된다. 특정 구현은 다음과 같을 수 있다. 학습: 샘플 이미지 { }이 있고 샘플 이미지에 대응하는 레이블 { }이 있다고 가정한다. 여기에서 레이블 yi는 구조를 나타내며, yi = {p손, p전화, p}이다. 전술한 구현과 동일하게, phand은 물체가 손일 확률을 나타내고 pphone는 물체가 휴대용 장치일 확률을 나타낸다. p는 목표 디스플레이 화면을 향할 확률을 나타낸다. 이 데이터의 배치(batch)는 물체 탐지 네트워크를 학습시키는데 사용된다. 마지막으로 물체 탐지 네트워크가 획득되고, 입력 이미지에 기초하여 이미지에 손 및 휴대용 단말이 있는지 여부 및 휴대용 디스 플레이 화면의 방향을 출력하는데 사용된다. ·단일 물체 탐지 네트워크에 기초한 판정을 보유하는 휴대용 단말. 손이 휴대용 단말을 들기 때문에, 손과 휴대용 장치 모두 탐지하는 네트워크를 학습시킬 필요가 없다. 이 방법 에서, 단일 물체 탐지 네트워크가 학습된다. 네트워크는 휴대용 장치만을 탐지한다. 네트워크가 휴대용 장치를 탐지하면, 휴대용 장치가 들려 있는 것으로 간주되고, 그렇지 않으면, 휴대용 장치가 들려 있지 않다. 샘플 이미지 { }이 있고 샘플 이미지에 대응하는 레이블 { }이 있다고 가정 한다. 여기에서 레이블 yi는 {pphone, p} 인데, pphone는 물체가 휴대용 장치일 확률을 나타내고, p는 목표 디스플 레이 화면을 향할 확률을 나타낸다. 대안적으로, 다른 레이블 세팅 방식이 있을 수 있다. 이것은 여기에서 예시 일 뿐이다. 이 데이터의 배치(batch)는 물체 탐지 네트워크를 학습시키도록 사용된다. 마지막으로, 단일 물체 탐지 네트워크(딥 러닝 모델)는 획득되고 휴대용 장치가 이미지에 있는지 및 휴대용 장치의 화면 방향을 탐지하 는데 사용될 수 있다. 이미지에 휴대용 장치가 있는 경우, S101의 판정 결과를 참조하면, 이때의 상태는 사용자 에 의해 들려 있는 휴대용 장치가 목표 디스플레이 화면에 접근한다는 것이 유추될 수 있고, 투사 대상 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향하는지 여부가 유추될 수 있다. 전술한 두가지 조건이 본 명세서의 다른 실시예에 포함되는 경우, 두가지 조건이 상호 트리거(mutually triggered)되지 않는다면, 종합적 모델에 기초한 통합 판정의 전술한 구현은 사용될 수 있거나, 두가지 독립적 인 모델에 기초한 개별 판정의 구현이 사용될 수 있다. S303: 목표 디스플레이 화면에 접근하는 손의 동작이 있다는 것, 제1 장치가 들려 있고 제1 장치의 디스플레이 화면 방향이 사전 설정 방향을 향한다고 판정되는 경우, 화면 콘텐츠를 목표 디스플레이 화면에 표시하도록 제1 장치에 통지하기 위해 제1 장치로 메시지를 전송. 투사 동작이 수행되기 전에, 하나의 투사 대상 장치의 자세를 판정하는 단계 S303a가 더 포함될 수 있다. S302a: 감지 장치에 의해 획득된 이미지 정보에 기초하여 제1 장치의 자세를 판정 및 제1 장치의자세에 기초하 여 목표 디스플레이 화면의 투사 디스플레이 모드를 판정. S302a는 S302 이후 및 S303 이전에 수행될 수 있다. 즉, 사용자가 투사 대상 장치를 들고 있는지 여부 및 투사 대상 장치의 화면 방향이 판정된 후, 투사 대상 장치의 자세가 판정된다. 그런 다음, 사용자가 투사 대상 장치 를 들고 있는지 여부에 기초하고 투사 대상 장치의 화면 방향에 기초하여, 투사 대상 장치의 화면 콘텐츠가 목 표 디스플레이 화면에 표시된다고 판정된 이후에, 투사 동작을 가로 모드 또는 세로 모드로 수행할지 여부가 자 세에 따라 판정된다. 대안적으로, S302a는 S303과 동시에 발생할 수 있다. 즉, 투사 동작이 수행된다고 판정된 다. 이 경우에, 투사 대상 장치의 자세는 더 판정되고, 그런 후에 투사 동작을 가로 모드 또는 세로 모드로 수 행할지 여부는 자세에 기초하여 판정된다. 대안적으로, S302a에서의 자세 판정은 투사 동작이 이 해결방안에서 수행되기 이전에 임의의 순간에 발생할 수 있다. S302a가 존재하지 않으면, 투사는 s303의 디폴트 투사 디스플레이 모드에서 수행된다. 유사하게, S302a의 감지 장치는 이미지 정보가 획득되는 것이 필요한 다른 단계의 감지 장치와 동일하거나 상이 할 수 있고, S302a의 이미지 정보는 이미지 정보가 획득되는 것이 필요하거나 독립적으로 획득될 수 있는 다른 단계의 이미지 정보일 수 있다. 투사 대상 장치의 자세 판정은 S203a에서의 그것과 동일하며 이력 판정 단계에 기초하여 획득될 수 있거나 전용 자세 탐지 네트워크에 기초하여 획득될 수 있다. 전술한 해결방안에서, 차량 내 감지 장치는 사용자 투사 관련 행위 동작이 명확하게 관찰될 수 있는 위치에 배 치될 수 있다. 예를 들어, 본 발명은 감지 장치의 여러 선택적 배치 위치, 예를 들어, 도 4에서 카메라 아이콘 에 의해 표시되는 위지를 제공한다. 하나 이상의 감지 장치는 설치될 수 있고, 감지 장치의 하나 이상의 상이한 유형은 설치될 수 있다. 예를 들어, 레이더와 일반적인 카메라 모두가 설치될 수 있다. 레이더에 의해 획득된 정보에 기초하여, 목표 디스플레이 화면에 접근하는 사용자 손의 동작이 있는지 여부가 분석된다. 일반적인 카 메라에 의해 획득된 비디오 또는 이미지에 기초하여, 사용자가 휴대용 장치를 들고 있는지 여부, 휴대용 장치의디스플레이 화면 방향 및 휴대용 장치의 자세가 수평 또는 수직인지 여부가 판정된다. 배치 원리는 사용자 손 행위가 탐지될 수 있다는 것이다. 특정 투사 모드는 전술한 투사 방법에 제한되지 않는다. 휴대용 장치는 미리 차량 내 인포테인먼트 (infotainment) 시스템에 무선으로 연결될 수 있다. 연결 방식은 블루투스 또는 복수의 무선 연결 방식 중 Wi- Fi일 수 있다. 무선 연결은 미리 설정되거나 투사가 실행될 필요가 있을 때 설정될 수 있다. 투사가 수행된다고 판정된 경우, 휴대용 단말은 휴대용 장치의 화면 인터페이스를 차량 내 인포테인먼트 프로세서에 의해 전송된 알림에 따라 목표 차량 내 디스플레이 화면에 표시한다. 표시된 인터페이스는 현재 휴대용 단말의 디스플레이 인터페이스, 메뉴 인터페이스 또는 다른 미리 합의된 인터페이스일 수 있다. 특정 투사 모드는 다음과 같을 수 있다. 투사가 수행될 필요가 있다고 판정할 경우, 차량 내 인포테인먼트 프로세서는 투사 동작을 수행할 수 있도록 휴 대용 장치에 통지한다. 통지 행위는 복수의 메시지 형태일 수 있다. 통지 메시지를 수신한 후, 휴대용 장치는 사용자에게 프롬프트(prompt) 인터페이스를 표시하고, 사용자에게 투사를 확인할지 여부를 묻는다. 사용자가 투 사를 확인한 후, 휴대용 장치는 디스플레이를 위해 화면 정보를 목표 디스플레이 화면에 전송한다. 대안적으로, 휴대용 장치와 차량 내 인포테인먼트 사이에 인증 관계가 설립될 수 있고, 차량 내 인포테인먼트는 휴대용 장치의 화면 정보를 획득하도록 인가된다. 투사가 수행될 필요가 있다고 판정하는 경우, 차량 내 인포테 인먼트 프로세서가 투사 동작을 수행하도록 휴대용 장치에 통지한다. 통지 메시지를 수신한 이후, 휴대용 장치 는 디스플레이를 위해 목표 디스플레이 화면에 휴대용 장치의 화면 정보를 전송한다. 투사 동작은 다음을 더 포함할 수 있다. 사용자 프라이버시의 보호를 위해, 차량 내 인포테인먼트가 휴대용 장 치에 의해 전송된 화면 정보를 수신하는 경우, 차량 내 인포테인먼트는 휴대용 장치의 인증 정보가 획득되었다 고 판정하고 그 이후 휴대용 장치의 화면 정보를 목표 디스플레이 화면에 표시한다. 휴대용 장치의 인증 정보를 획득하는 것은 사용자의 이력 인증 정보가 있는지 여부 또는 이 보호를 위해 사용자의 확인 정보가 있는지 여부 를 질문(querying)함으로써 수행될 수 있다. 특정 연결 프로세스나 투사의 프로세스를 전송하는 화면 콘텐츠는 본 발명의 실시예에 제한되지 않으며, 임의의 하나의 전술한 방식일 수 있거나, 다른 방식일 수 있다. 또한, 요구 사항(requirement)에 기초하여, 본 발명의 이 실시예에서의 투사 방법의 구현에서, 화면이 잠금 상 태 또는 비활성 상태에 있는 경우 투사 의도 또한 탐지될 수 있다. 대안적으로, 화면이 잠금 상태인지 여부가 먼저 식별되고, 화면이 잠금 상태에 있다면, 화면 상태는 계속해서 모니터링 된다. 화면 상태가 불활성 상태에 있다고 판정되면, 투사 의도가 탐지된다. 전술한 투사 탐지 의도가 어떠한 시간과 조건에서 수행되는지는 본 발 명의 실시예에 제한되지 않고, 해결방안은 특정 구현에서의 특정 요구 사항에 기초하여 유연하게 설계될 수 있 다. 도 13은 본 출원의 도 3, 도11 및 도 12에 대응하는 투사 방법에 대응하는 투사 시스템의 개략적인 구조도이다. 도 13에 도시된 바와 같이, 본 발명에서의 투사 시스템은 디스플레이 유닛와 판정 유닛을 포 함할 수 있고, 선택적으로 이미지 수집 유닛 또는 거리 획득 유닛으로 구성되거나 필요에 따라 이 미지 수집 유닛 및 거리 수집 유닛 모두로 구성된다. 예를 들어, 투사 시스템이 거리 정보에 기초하여 손이 목 표 디스플레이 화면에 접근하는지 여부를 판정할 필요가 있는 경우, 투사 시스템은 거리 획득 유닛을 구 비할 필요가 있다. 그러나, 투사 시스템이 이미지 정보에 기초하여 사용자가 투사 의도를 갖는지 여부를 판정하 는 경우, 투사 시스템은 거리 획득 유닛을 구비하는 것을 필요로 하지 않을 수 있다. 유사하게, 거리 정보에 기초하여 투사를 트리거하도록 투사 시스템이 손이 목표 디스플레이 화면에 접근하는지 여부의 판정만을 필요로 하는 경우, 투사 시스템은 이미지 수집 유닛을 구비할 필요가 없을 수 있다. 이미지 수집 유닛은 이미지 정보를 수집하도록 구성된다. 거리 획득 유닛은 거리 정보를 획득하도록 구성된다. 판정 유닛은 이미지 수집 유닛에 의해 수집된 이미지 정보 및/또는 거리 획득 유닛에 의해 획득된 거리 정보를 분석 및 식별하도록 구성되고, 분석 및 식별 결과에 기초하여 투사 동작을 수행할지 여부를 판정한다. 판정 유닛이 투사 동작이 수행될 필요가 있다고 판정한 이후에 디스플레이 유닛은 투사 대상 장치 의 화면 콘텐츠를 표시하도록 구성된다. 통상의 기술자는 본 출원의 실시예의 투사 시스템이 도면에 도시된 것보다 더 많거나 또는 더 적은 구성요소를 포함할 수 있다는 것, 또는 일부 구성요소는 결합될 수 있다는 것, 또는 구성요소는 상이한 방식으로 배열될 수 있다는 것을 이해할 수 있다. 이미지 수집 유닛 및 거리 획득 유닛 각각은 복수의 서브유닛 및 이미지 정보를 더 포함할 수 있으며, 상이한 판정 조건에서 이미지 정보는 동일하거나 상이할 수 있다. 이미지 정보가 상이한 경우, 이미지 정보는 동일한 이미지 수집 서브유닛으로부터의 것일 수 있고, 상이한 이미지 수집 서브유닛으로 부터의 것일 수 있다. 투사 시스템이 도 3, 도 11 및 도 12에 대응하는 투사 방법을 수행하도록 구성된 경우, 판정 유닛 은 하나 이상의 조건에 기초하여 후속 투사 동작을 수행할지 여부를 판정할 수 있다. 복수의 조건이 있는 경우, 해결방안의 구체적인 설계에 따르면, 판정 유닛은 복수의 판정 조건에 대한 탐지를 순차적으로 수행할 수 있거나, 복수의 조건에 대해 탐지를 병렬로 수행하거나, 연속적으로 트리거링하는 방식으로 복수의 조건에 대해 탐지를 수행할 수 있다. 본 출원에서 도 14는 도 3, 도 11 및 도 12에 대응하는 투사 방법에 대응하는 투사 시스템의 개략적인 구조도이 다. 도 14에 도시된 바와 같이, 본 발명에서 투사 시스템은 하나 이상의 프로세서, 하나 이상의 메모리 , 하나 이상의 감지 장치, 하나 이상의 디스플레이 화면 및 통신 모듈을 포함할 수 있 다. 프로세서는 버스(bus)를 통해 메모리, 감지 장치, 디스플레이 화면 및 통신 모듈 와 같은 구성요소에 별도로 연결될 수 있다. 설명은 다음과 같다. 메모리는 고속 랜덤 액세스 메모리를 포함할 수 있거나, 비휘발성 메모리, 예를 들어, 적어도 하나의 자 기 디스크 저장 장치, 플래시 저장 장치 또는 다른 휘발성 고체 상태 저장 장치를 포함할 수 있다. 따라서, 메 모리는 메모리에 프로세서의 액세스를 제공하도록 메모리 컨트롤러를 더 포함할 수 있다. 메 모리는 구체적으로 소프트웨어 프로그램(명령어), 및 감지 장치에 의해 수집된 이미지 정보 및/또는 거리 정보를 저장하도록 구성될 수 있다. 감지 장치은 이미지 정보 및/또는 거리 정보를 수집하도록 구성된다. 특정 구현 과정에서, 감지 장치 는 복수의 유형의 센서, 예를 들어, 카메라나 레이더와 같이 이미지 정보를 촬영하고 거리를 측정하도록 구성되는 감지 장치를 포함할 수 있다. 하나 이상의 유형의 감지 장치가 있을 수 있거나 동일한 유형의 복수의 감지 장치가 배치될 수 있다. 프로세서가 투사 동작이 수행될 필요가 있다고 판정한 후에 디스플레이 화면은 투사 대상 장치의 화면 콘텐츠를 표시하도록 구성된다. 통신 모듈은 무선 또는 유선 통신 기술, 예를 들어, 셀룰러 이동 통신 기술(cellular mobile communications technology), WLAN 또는 블루투스(Bluetooth)를 사용하여 투사 대상 장치로의 통신 연결을 수 행하고 필요할 때 투사 대상 장치의 화면 데이터를 수신된 명령어에 따라 전송하도록 구성된다. 프로세서는 사용자 행위 정보를 분석 및 식별하고 투사 동작이 수행될 필요가 있는 경우 투사 명령어를 전달하도록 구성된다. 가능한 실시예에서, 프로세서는 하나 이상의 프로세싱 코어(processing core)를 더 포함할 수 있다. 프로세서는 프로그램 명령어를 실행하여 사용자 행위 정보를 분석 및 식별할 수 있다. 프로세서는 특수 목적 프로세서(special-purpose processor) 또는 범용 프로세서(general-purpose processor)일 수 있다. 프로세서가 범용 프로세서인 경우, 프로세서는 메모리에 저장된 소프 트웨어 프로그램(명령어) 및/또는 모듈을 실행(run or execute)한다. 프로세서는 관련 프로그램을 실행하 도록 하나 이상의 집적 회로를 더 사용할 수 있다. 프로세서는 집적 회로 칩(integrated circuit chip)일 수 있고 신호 프로세싱 능력을 갖는다. 구현 프로세스에서, 본 출원에서의 투사 방법에서 사용자 의도 분석의 단계는 프로세서에서 하드웨어 집적 논리 회로 또는 소프트웨어 형태의 명령어를 사용하여 완료될 수 있 다. 통상의 기술자는 본 출원의 이 실시예에서 투사 시스템은 다이어그램에 도시된 것보다 더 많거나 더 적은 구성 요소를 포함할 수 있다는 것, 또는 일부 구성요소가 결합될 수 있다는 것, 또는 구성요소가 다른 방식으로 배열 될 수 있다는 것을 이해할 수 있다. 예를 들어, 투사 장치는 확성기, 마이크 등을 더 포함할 수 있다. 세부사항 은 여기서 다시 설명되지 않는다.투사 시스템이 도 3에 대응하는 방법을 수행하도록 구성되는 경우, 다음이 구체적으로 포함된다. 감지 장치은 이미지 정보를 획득하도록 구성된다. 감지 장치는 차량 내 카메라, 레이더 등 중에서 하나 이상일 수 있다. 프로세서는 감지 장치에 의해 획득된 이미지 또는 비디오 정보에 기초하여 휴대용 장치의 디스플레이 화 면 방향을 판정한다. 적어도 하나의 프로세서가 휴대용 장치의 디스플레이 화면이 사전 설정 방향을 충족한다고 판정하는 경우 적어도 하나의 프로세서는 제1 장치의 화면 콘텐츠를 목표 디스플레이 화면에 표시하거나 적어도 하나의 프로세서가 휴대용 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족한다고 판정하는 경우 화면 콘 텐츠를 목표 디스플레이 화면에 표시하도록 휴대용 장치에 통지한다. 전술한 판정 단계의 특정 구현에 대하여는 도 3의 상세한 설명을 참조하라. 복수의 화면 디스플레이 모드가 필요한 경우, 프로세서는 휴대용 장치의 자세가 수평인지수직인지 여부, 현재 휴대용 장치의 화면 콘텐츠의 투사 디스플레이 모드가 가로 모드 또는 세로 모드인지 여부를 판단함으로써 더 판정할 수 있다. 따라서, 프로세서는 투사 동작이 구현되기 전에 언제든지 휴대용 장치의 자세를 판정 한다. 휴대용 장치의 자세를 판정하는 방식에 대하여는, 단계 203a의 특정 구현 세부사항을 참조하라. 투사 시스템이 도 11에 대응하는 방법을 수행하도록 구현되는 경우, 다음이 구체적으로 포함된다. 감지 장치는 해결방안에 설정된 요구 사항에 기초하여 이미지 정보 및/또는 거리 정보를 획득하도록 구성 된다. 감지 장치는 차량 내 카메라, 레이더 등 중에서 하나 이상일 수 있다. 어떠한 유형의 감지 장치가 사용되는지 및 동일한 감지 장치 또는 상이한 감지 장치가 사용되는지 여부는 획득 대상 정보(to-be-obtained information)의 콘텐츠에 따라 달라진다. 예를 들어, 상이한 판정 조건 하에서 획득될 필요가 있는 이미지의 각 도가 다르거나 상이한 위치에서의 이미지가 획득될 필요가 있는 경우, 상이한 카메라가 사용될 필요가 있다. 프로세서는 감지 장치에 의해 획득된 비디오 또는 다중 프레임 이미지 정보에 기초하여 목표 디스플레이 화면에 접근하는 손의 동작이 있다고 판정하거나, 감지 장치에 의해 획득되었고 손으로부터 감지 장치까지의 거 리에 대한 정보에 기초하여 손으로부터 감지 장치까지의 거리가 사전 설정 값보다 작다고 판정한다. 프로세서는 감지 장치에 의해 획득된 이미지 정보에 기초하여 휴대용 장치가 들려있다고 판정한다. 프로세서는 감지 장치에 의해 획득된 이미지 또는 비디오 정보에 기초하여 휴대용 장치의 디스플레이 화 면 방향을 판정한다. 전술한 판정 단계의 특정 구현에 대해서는, 도 11의 상세한 설명을 참조하라. 프로세서가 목표 디스플레이 화면에 접근하는 손의 동작이 있다는 것, 휴대용 장치가 들려 있고 휴대용 장치의 디스플레이 화면 방향이 사전 설정 방향을 향한다고 판정하는 경우 프로세서는 화면 콘텐츠를 목 표 디스플레이 화면에 표시하도록 휴대용 장치에 통지하기 위해 휴대용 장치로 메시지를 전송한다. 차량 내 시스템은 하나 이상의 프로세서를 가질 수 있다. 프로세서가 목표 디스플레이 화면에 접근 하는 손의 동작이 있다는 것, 휴대용 장치가 들려 있고 휴대용 장치의 디스플레이 화면 방향이 목표 디스플레이 화면을 향한다고 판정하는 경우, 세가지 판정 단계는 동시에 수행될 수 있거나 순차적으로 수행될 수 있다. 특 정 구현에서는 세가지 판정 단계가 모두 수행될 수 있다. 즉, 프로세서는 목표 디스플레이 화면에 접근하 는 손의 동작이 있는지 여부를 판정하고, 휴대용 장치가 들려 있는지 여부를 판정하며 휴대용 장치의 디스플레 이 화면 방향이 사전 설정 방향인지 여부를 판정한다. 그 후, 모든 결과가 예인 경우, 프로세서은 투사를 수행하라고 판정한다. 대안적으로, 프로세서는 전술한 판정 단계를 순차적으로 수행할 수 있고, 전술한 판정 프로세서는 연속적으로 트리거된다. 예를 들어, 프로세서는 목표 디스플레이 화면에 접근하는 손의 동작이 있는지 여부를 판정한다. 판정 결과가 예인 경우, 프로세서는 휴대용 장치가 들려 있는지 여부를 계속해서 판정한다. 판정 결과가 아니오인 경우, 프로세서는 목표 디스플레이 화면에 접근하는 손의 동작 이 있는지 여부를 계속해서 탐지한다. 유사하게, 휴대용 장치가 들려 있다고 판정되는 경우, 프로세서는 휴대용 장치의 디스플레이 화면 방향이 사전 설정 방향인지 여부를 더 판정한다. 사용자가 휴대용 장치를 들고 있지 않다고 판정되는 경우, 목표 디스플레이 화면에 접근하는 손의 동작이 있는지 여부를 탐지하는 제1 단계로 돌아가 수행된다. 유사하게, 휴대용 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족시키지 않는다는 것 이 탐지되는 경우, 목표 디스플레이 화면에 접근하는 손의 동작이 있는지 여부를 탐지하는 제1 단계로 돌아가 수행된다. 휴대용 장치의 디스플레이 화면 방향이 사전 설정 방향을 충족한다는 것이 탐지되는 경우, 프로세서 는 투사 동작을 트리거한다. 프로세서에 의해 수행되는 전술한 판정 단계를 트리거하는 시퀀스는단지 예일 뿐이다. 해결방안의 구현에서, 전술한 판정 단계의 시퀀스도 변경될 수 있다. 임의의 판정 결과가 아 니오인 경우, 프로세서는 더 이상 후속 판정을 수행하지 않지만, 탐지 판정을 다시 수행하도록 제1 판정 으로 돌아간다. 다른 구현에서, 투사 시스템은 대안적으로 전술한 판정 단계 중 임의의 두 단계만을 수행할 수 있다. 유사하게, 복수의 화면 디스플레이 모드가 요구될 때, 프로세서는 휴대용 장치의 자세가수평인지 수직인 지 여부를 판정함으로써 현재 휴대용 장치의 화면 콘텐츠의 투사 디스플레이 모드가 가로 모드인지 또는 세로 모드인지 여부를 더 판정할 수 있다. 따라서, 프로세서는 투사 통작이 구현되기 전에 언제든지 휴대용 장 치의 자세를 판정한다. 휴대용 장치의 자세를 판정하는 방식에 관하여는, 단계 203a의 특정 구현의 세부사항을 참조하라. 투사 시스템이 도 12에 대응하는 방법을 수행하도록 구성되는 경우, 다음이 구체적으로 포함된다. 감지 장치는 해결방안에 설정된 요구 사항에 기초하여 이미지 정보 및/또는 거리 정보를 획득하도록 구성 된다. 감지 장치는 차량 내 카메라, 레이더 등 중에서 하나 이상일 수 있다. 어떤 종류의 감지 장치가 사 용되는지 및 동일한 감지 장치 또는 상이한 감지 장치가 사용되는지 여부는 획득 대상 정보의 콘텐츠에 따라 달 라진다. 예를 들어, 상이한 판정 조건하에서 획득될 필요가 있는 이미지의 각도가 상이하거나 상이한 위치에서 의 이미지는 획득될 필요가 있는 경우, 상이한 카메라가 사용될 필요가 있다. 프로세서는 감지 장치에 의해 획득된 비디오 또는 멀티 프레임 이미지 정보에 기초하여 목표 디스플레이 화면에 접근하는 손의 동작이 있다고 판정하거나, 감지 장치에 의해 획득되고 손으로부터 감지 장치까지의 거리 에 대한 정보에 기초하여 손으로부터 목표 디스플레이 화면까지의 거리가 사전 설정 값보다 작다고 판정한다. 프로세서는 감지 장치에 의해 획득된 이미지 정보에 기초하고 종합적인 학습의 신경 네트워크 모델에 기 초하여 사용자가 제1 장치를 들고 있는지 여부 및 제1 장치의 디스플레이 화면 방향을 판정한다. 전술한 판정 단계의 특정 구현에 대해서는, 도 12의 상세한 설명을 참조하라. 전술한 두가지 조건의 판정이 투사 시스템의 다른 실시예에 포함되는 경우, 두가지 조건이 서로 트리거되지 않 는다면, 프로세서는 종합적인 모델에 기초하여 통합된 판정 또한 수행할 수 있거나, 두개의 독립적인 모 델을 기초로 개별적으로 판정을 수행할 수 있다. 유사하게, 복수의 화면 디스플레이 모드가 요구되는 경우, 프로세서는 휴대용 장치의 자세가 수평 또는 수직인지 여부를 판정함으로써 휴대용 장치의 화면 콘텐츠의 투사 디스플레이 모드가 가로 모드 또는 세로 모드 인지 여부를 더 판정할 수 있다. 따라서, 프로세서는 언제든지 투사 동작이 구현되기 이전에 휴대용 장치 의 자세를 판정한다. 휴대용 장치의 자세를 판정하는 방식에 대해서는, 단계 203a의 특정 구현 세부사항을 참조 하라. 전술한 실시예에서, 조건 판정 단계 및 투사 판정 동작은 소프트웨어, 하드웨어, 펌웨어 또는 이들의 임의의 조 합을 사용하여 전부 또는 부분적으로 구현될 수 있다. 소프트웨어가 조건 판정 단계 및 투사 판정 동작을 구현 하도록 사용되는 경우, 조건 판정 단계 및 투사 판정 동작은 컴퓨터 프로그램 제품의 형태로 완전히 또는 부분 적으로 구현될 수 있다. 컴퓨터 프로그램 제품은 하나 이상의 컴퓨터 명령어를 포함한다. 컴퓨터 명령어가 컴퓨 터에서 로드되고 실행될 때, 절차 또는 기능의 전부 또는 일부는 본 출원의 실시예에 따라 생성된다. 프로세서 는 범용 프로세서 또는 특수 목적 프로세서일 수 있다. 컴퓨터 명령어는 컴퓨터 판독가능 저장 매체(computer- readable storage medium)에 저장될 수 있거나 컴퓨터 판독가능 저장 매체로부터 다른 컴퓨터 판독가능 저장 매 체로 전송될 수 있다. 예를 들어, 명령어는 컴퓨터 웹사이트, 컴퓨터, 서버 또는 데이터 센터로부터 다른 웹사 이트, 컴퓨터, 서버 또는 데이터 센터로 유선(예를 들어, 동축 케이블, 광섬유 또는 디지털 가입자 회선 (digital subscriber line)) 또는 무선 방식(예를 들어, 적외선, 마이크로파 등)으로 전송될 수 있다. 컴퓨터 판독가능 저장 매체는 컴퓨터에 의해 액세스 가능한 임의의 사용가능한 매체 또는 하나 이상의 사용 가능한 매 체를 통합하는 데이터 저장 장치(예를 들어, 서버 또는 데이터 센터)일 수 있다. 사용 가능한 매체는 자기 매체 (예를 들어, 플로피 디스크, 하드 디스크 또는 자기 테이프), 광학 매체(예를 들어, DVD), 반도체 매체(예를 들 어, 솔리드 스테이트 드라이브(solid-state drive)) 또는 이와 유사한 것일 수 있다. 예를 들어, 본 출원의 실시예의 해결방안에서, 선택적으로, 실행 본체는 ASIC, FPGA, CPU, GPU 또는 이와 유사 한 것일 수 있고, 하드웨어 또는 소프트웨어를 사용하여 구현될 수 있다. 선택적으로, 메모리는 휘발성 또는 비 휘발성 저장 장치(예를 들어, DDR, SRAM, HDD 또는 SSD)일 수 있다. ASIC 및 FPGA는 하드웨어 구현에 속한다.구체적으로, 본 출원에서의 방법은 하드웨어 설계시 하드웨어 기술 언어(hardware description language)를 사 용하여 구현된다. 구체적으로, 본 출원의 방법은 소프트웨어 설계시 소프트웨어 프로그램 코드 방식으로 구현된 다. 전술한 실시예에서, 각 실시예의 설명은 각각 주안점을 갖는다. 실시예에서 구체적으로 설명되지 않은 부분은, 다른 실시예의 관련 설명을 참조하라."}
{"patent_id": "10-2022-7012657", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 출원의 실시예 또는 배경기술에서 기술적 해결방안을 보다 명확하게 설명하기 위해, 이하에서는 본 출원의"}
{"patent_id": "10-2022-7012657", "section": "도면", "subsection": "도면설명", "item": 2, "content": "실시예 또는 배경기술을 설명하기 위한 첨부 도면을 간단하게 설명한다. 도 1은 본 발명의 실시예에 따른 예시적인 적용 시나리오를 도시한다. 도 2는 본 발명의 실시예에 따른 다른 적용 시나리오를 도시한다. 도 3은 본 발명의 일 실시예에 따른 투사 방법의 구현을 도시한다. 도 4는 본 발명의 일 실시예에 따른 감지 장치의 예시적인 전개도를 도시한다.도 5는 본 발명의 일 실시예에 따른 3DCNN에 기초하는 손 행위 식별의 구현을 도시한다. 도 6은 본 발명의 일 실시예에 따른 2DCNN에 기초하는 손 행위 식별의 구현을 도시한다. 도 7은 본 출원의 일 실시예에 따른 손 행위 식별의 구현을 도시한다. 도 8은 본 출원의 일 실시예에 따른 손 행위 식별의 구현을 도시한다. 도 9는 본 출원의 일 실시예에 따른 휴대용 장치의 화면 방향의 구현을 도시한다. 도 10은 본 출원의 일 실시예에 따른 휴대용 장치의 자세(posture) 식별의 구현을 도시한다. 도 11은 본 출원의 일 실시예에 따른 투사 방법의 구현을 도시한다. 도 12는 본 발명의 일 실시예에 따른 투사 방법의 구현을 도시한다. 도 13은 본 발명의 일 실시예에 따른 투사 시스템의 개략적인 구조도이다. 도 14는 본 발명의 일 실시예에 따른 투사 시스템의 개략적인 구조도이다. 도 15는 본 발명의 일 실시예에 따른 손 행위 식별 및 화면 방향 식별의 구현을 도시한다."}
