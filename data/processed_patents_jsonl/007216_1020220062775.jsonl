{"patent_id": "10-2022-0062775", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0163113", "출원번호": "10-2022-0062775", "발명의 명칭": "시선 기반 증강형 자동통역 방법 및 시스템", "출원인": "한국전자통신연구원", "발명자": "이민규"}}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자의 시야 내의 영상 정보와 음성 정보를 수집하고, 상기 수집된 영상 정보에서 발화자의 얼굴 영역을 검출하는 단계;상기 검출된 얼굴 영역을 기준으로 통역 대상이 되는 적어도 하나의 발화자를 선정하고, 상기 수집된 음성 정보에서 상기 발화자의 음성 정보를 개별적으로 추출하는 단계; 및상기 발화자의 음성 정보를 기초로, 기 설정된 목적 언어에 따른 통역 결과를 생성하는 단계;를 포함하는 시선 기반 증강형 자동통역 방법."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 수집된 영상 정보에 상기 통역 결과를 부가하여 상기 사용자의 단말기 화면에 표시하는 단계;를 더 포함하고,상기 단말기 화면에 표시하는 단계에서,상기 수집된 영상 정보 상에서 상기 통역 결과가 부가되는 위치는 상기 검출된 얼굴 영역을 기초로 정해지는 것인 시선 기반 증강형 자동통역 방법."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 발화자의 음성 정보에서 발화구간을 검출하여 상기 발화자의 발화구간의 음성 정보를 추출하는 단계를 더포함하고,상기 통역 결과를 생성하는 단계는,상기 발화구간의 음성 정보를 기초로 음성인식 모델과 기계번역 모델을 이용하여 상기 통역 결과를 생성하는 것인 시선 기반 증강형 자동통역 방법."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 발화자의 음성 정보를 추출하는 단계는,상기 검출된 얼굴 영역을 기준으로, 상기 수집된 영상 정보에서 상기 발화자의 영상 정보를 추출하는 것을 더포함하고,상기 통역 결과를 생성하는 단계는,상기 발화자의 음성 정보와 상기 발화자의 영상 정보를 기초로 상기 통역 결과를 생성하는 것인 시선 기반 증강형 자동통역 방법."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서, 상기 발화자의 음성 정보를 추출하는 단계는,상기 검출된 얼굴 영역을 기준으로, 상기 수집된 영상 정보에서 상기 발화자의 영상 정보를 추출하는 것을 더포함하고,상기 발화구간의 음성 정보를 추출하는 단계는,공개특허 10-2023-0163113-3-상기 발화자의 영상 정보를 기초로 상기 발화자의 음성 정보에서 발화구간을 검출하여 상기 발화자의 발화구간의 음성 정보를 추출하는 것인인 시선 기반 증강형 자동통역 방법."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 발화자의 음성 정보를 추출하는 단계는,상기 검출된 얼굴 영역에 해당하는 영상 정보를 기초로 사용자 정보 데이터베이스에서 상기 발화자의 등록 음성정보를 추출하고,상기 등록 음성 정보를 활용하여 상기 수집된 음성 정보에서 상기 발화자의 음성 정보를 추출하는 것인 시선 기반 증강형 자동통역 방법."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 수집된 영상 정보를 기초로 상황을 추론하는 단계;를 더 포함하고,상기 통역 결과를 생성하는 단계는,상기 상황 및 상기 발화자의 음성 정보를 기초로, 상기 목적 언어에 따른 통역 결과를 생성하는 것인 시선 기반 증강형 자동통역 방법."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 상황을 추론하는 단계는,상기 상황을 기초로 상황-단어 데이터베이스에서 상기 상황과 관련된 단어를 추출하는 것을 더 포함하고,상기 통역 결과를 생성하는 단계는,상기 상황과 관련된 단어 및 상기 발화자의 음성 정보를 기초로, 상기 목적 언어에 따른 통역 결과를 생성하는것인 시선 기반 증강형 자동통역 방법."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서, 상기 수집된 영상 정보에 포함된 텍스트를 인식하는 단계;를 더 포함하고,상기 통역 결과를 생성하는 단계는,상기 텍스트 및 상기 발화자의 음성 정보를 기초로, 상기 목적 언어에 따른 통역 결과를 생성하는 것인 시선 기반 증강형 자동통역 방법."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "사용자의 단말기에서 사용자 시야 내의 영상 정보와 음성 정보를 수신하고, 상기 수신된 영상 정보에서 검출된얼굴 영역을 기준으로 통역 대상이 되는 적어도 하나의 발화자를 선정하며, 상기 수신된 음성 정보에서 상기 발화자의 음성 정보를 개별적으로 추출하는 분석 모듈; 및상기 발화자의 음성 정보를 기초로, 기 설정된 목적 언어에 따른 통역 결과를 생성하여 상기 단말기에 전송하는통역 모듈;을 포함하는 통역 서버."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2023-0163113-4-제10항에 있어서,상기 수신된 영상 정보에서 상기 검출된 얼굴 영역에 해당하는 영상을 기초로 사용자 정보 데이터베이스에서 상기 통역 대상이 되는 발화자의 등록 음성 정보를 추출하는 사용자 정보 관리 모듈을 더 포함하고,상기 분석 모듈은,상기 수신된 음성 정보와 상기 등록 음성 정보를 기초로 상기 발화자의 음성 정보를 개별적으로 추출하는 것인 통역 서버."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,상기 수신된 영상 정보에서 상기 검출된 얼굴 영역에 해당하는 영상을 기초로 사용자 정보 데이터베이스에서 상기 통역 대상이 되는 발화자의 등록 음성 정보를 추출하는 사용자 정보 관리 모듈을 더 포함하고,상기 통역 모듈은,상기 등록 음성 정보와 상기 통역 결과를 기초로 상기 발화자의 음색에 부합하는 합성음을 생성하여 상기 단말기에 전송하는 것인 통역 서버."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10항에 있어서, 상기 분석 모듈은,상기 발화자의 음성 정보에서 발화구간을 검출하여 상기 발화자의 발화구간 음성 정보를 추출하고,상기 통역 모듈은,상기 발화구간 음성 정보를 기초로 음성인식 모델과 기계번역 모델을 이용하여 상기 통역 결과를 생성하는 것인 통역 서버."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제10항에 있어서, 상기 분석 모듈은,상기 검출된 얼굴 영역을 기준으로, 상기 수신된 영상 정보에서 상기 발화자의 영상 정보를 추출하고,상기 통역 모듈은,상기 발화자의 음성 정보와 상기 발화자의 영상 정보를 기초로 상기 통역 결과를 생성하는 것인 통역 서버."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제10항에 있어서, 상기 분석 모듈은,상기 수신된 영상 정보를 기초로 상황을 추론하고,상기 통역 모듈은,상기 상황 및 상기 발화자의 음성 정보를 기초로 상기 통역 결과를 생성하는 것인 통역 서버."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "사용자의 시야의 영상 정보와 음성 정보를 수집하는 입력 모듈;상기 수집된 영상 정보에서 발화자의 얼굴 영역을 검출하고, 상기 검출된 얼굴 영역을 기준으로 통역 대상이 되는 적어도 하나의 발화자를 선정하며, 상기 수집된 음성 정보에서 상기 발화자의 음성 정보를 개별적으로 추출공개특허 10-2023-0163113-5-하여 통역 서버에 전송하는 분석 모듈; 및상기 통역 서버에서 상기 발화자의 음성 정보에 대한 통역 결과를 수신하고, 상기 통역 결과를 디스플레이 및스피커 중 적어도 어느 하나를 통해 출력하는 출력 모듈;을 포함하는 단말기."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서, 상기 단말기는상기 사용자가 착용한 스마트글래스인 것을 특징으로 하는,단말기."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항에 있어서, 상기 분석 모듈은,상기 검출된 얼굴 영역을 기준으로, 상기 수집된 영상 정보에서 상기 발화자의 영상 정보를 추출하여 상기 통역서버에 전송하는 것인 단말기."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제16항에 있어서, 상기 분석 모듈은,상기 수집된 영상 정보에서 검출된 얼굴 영역의 영상을 상기 통역 서버에 전송하고, 상기 통역 서버에서 상기검출된 얼굴 영역의 영상에 부합하는 발화자의 등록 음성 정보를 수신하며, 상기 등록 음성 정보를 활용하여 상기 수집된 음성 정보에서 상기 발화자의 음성 정보를 추출하는 것인 단말기."}
{"patent_id": "10-2022-0062775", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제16항에 있어서, 상기 분석 모듈은,상기 수집된 영상 정보를 기초로 상황을 추론하고, 상기 추론된 상황을 상기 통역 서버에 전송하는 것인 단말기."}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 스마트기기를 통해 입력되는 멀티모달(음성과 영상) 정보를 복합적으로 활용하여 사용자가 바라보는 범위(시선 범위) 내 발화자에 대한 음성만을 목적 언어로 변환하는 자동통역 방법 및 시스템에 관한 것이다. 본 발명은 스마트기기에 입력되는 음성과 영상 정보를 복합적으로 활용하여 다수 화자가 동시에 발성하는 고잡음 환경에서도 의사소통 대상 외국인과의 자동통역 성능을 대폭 향상시킬 수 있다. 또한, 본 발명은 사용자 주변에 존재하는 텍스트 정보 및 영상 정보에 기반하여 상황을 판단하고, 상황 정보를 멀티모달 정보와 함께 통역엔진에 실시간 반영할 수 있다. 또한, 본 발명은 발화자 영상 옆에 직접 통역 문장을 증강하여 표시하거나 다른 발화와 구분하여 합성음을 생성함으로써 자동통역 시스템 사용자의 편의성을 획기적으로 개선할 수 있다."}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 사용자가 바라보는 범위, 즉 사용자의 시야에 기반한 증강형 자동통역 방법 및 시스템에 관한 것이다. 구체적으로, 사용자의 시야에 존재하는 멀티모달 정보를 활용하여, 증강된 자동통역 결과를 사용자에게 전달함으로써 자동통역 성능을 높일 수 있는 자동통역 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 기술의 발달에 따라 자동통역기술이 널리 보급되고 있다. 특히 최근에는 음성신호를 입력으로 하고, 번역된 문자열을 출력으로 신경망을 이용하여 학습한 종단형(End-to-End) 음성인식, 자동통역기술까지 개발됨에 따라 과거에 비해 자동통역 성능이 대폭 향상되었다. 그러나 자동통역기술을 사용하는 과정에서는 많은 어려움이 있다. 그 중 특히 잡음을 배제하고, 통역하고 싶은 음성만을 선택하는 것은 매우 어려운 기술이다. 실제 환경에는 다양한 종류의 환경잡음과 음성잡음이 동시 다발 적으로 존재한다. 이는 음성인식 그리고 자동통역 성능을 저하시키는 큰 요인이 된다. 이를 해결하기 위해 스마 트폰의 버튼을 활용하여 음성구간을 강제하거나, 음성 정보를 이용하여 음성구간만을 자동검출 하는 기술들이 활용되고 있다. 그러나 스마트폰 버튼을 사용하는 것은 사용자에게 불편함을 발생시키며, 음성 정보만을 활용한음성구간 검출은 그 성능에 한계가 있다. 또한, 통역이 필요한 상황에서 신속하게 발화자의 음성에 대한 통역 결과를 사용자에게 제공하는 것은 매우 유 용하지만, 실제 환경에서 통역이 요구되는 상황을 예측하기 어려우므로 그 구현이 쉽지 않다는 문제가 있다."}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 자동통역이 필요한 상황에서 사용자가 주변 상황을 즉각적이고 손쉽게 파악하도록 지원하기 위하여, 영상 정보와 음성 정보를 동시에 활용하여 복수 화자들의 음성을 변환하여 사용자가 원하는 언어로 구현된 음성 을 생성하고, 사용자의 시야 내의 화자 영상에 사용자의 모국어로 이루어진 문장을 표시하거나 변환된 음성을 출력할 수 있는, 시선 기반 증강형 자동통역 방법 및 시스템을 제공하는 것을 그 목적으로 한다. 본 발명의 목적은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 또 다른 목적들은 아래의 기재로 부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 시선 기반 증강형 자동통역 방법은 사용자의 시야 내의 영상 정보와 음성 정보를 수집하고, 상기 수집된 영상 정보에서 발화자의 얼굴 영역을 검출하는 단계; 상기 검 출된 얼굴 영역을 기준으로 통역 대상이 되는 적어도 하나의 발화자를 선정하고, 상기 수집된 음성 정보에서 상 기 발화자의 음성 정보를 개별적으로 추출하는 단계; 및 상기 발화자의 음성 정보를 기초로, 기 설정된 목적 언 어에 따른 통역 결과를 생성하는 단계를 포함한다. 본 발명의 일 실시예에서, 상기 자동통역 방법은, 상기 수집된 영상 정보에 상기 통역 결과를 부가하여 상기 사 용자의 단말기 화면에 표시하는 단계를 더 포함할 수 있다. 상기 단말기 화면에 표시하는 단계에서, 상기 수집 된 영상 정보 상에서 상기 통역 결과가 부가되는 위치는 상기 검출된 얼굴 영역을 기초로 정해질 수 있다. 본 발명의 일 실시예에서, 상기 자동통역 방법은, 상기 발화자의 음성 정보에서 발화구간을 검출하여 상기 발화 자의 발화구간의 음성 정보를 추출하는 단계를 더 포함할 수 있다. 상기 통역 결과를 생성하는 단계에서, 상기 발화구간의 음성 정보를 기초로 음성인식 모델과 기계번역 모델을 이용하여 상기 통역 결과를 생성할 수 있다. 본 발명의 일 실시예에서, 상기 발화자의 음성 정보를 추출하는 단계는, 상기 검출된 얼굴 영역을 기준으로, 상 기 수집된 영상 정보에서 상기 발화자의 영상 정보를 추출하는 것을 더 포함할 수 있다. 상기 통역 결과를 생성 하는 단계에서, 상기 발화자의 음성 정보와 상기 발화자의 영상 정보를 기초로 상기 통역 결과를 생성할 수 있 다. 본 발명의 일 실시예에서, 상기 발화자의 음성 정보를 추출하는 단계는, 상기 검출된 얼굴 영역을 기준으로, 상 기 수집된 영상 정보에서 상기 발화자의 영상 정보를 추출하는 것을 더 포함할 수 있다. 상기 발화구간의 음성 정보를 추출하는 단계에서, 상기 발화자의 영상 정보를 기초로 상기 발화자의 음성 정보에서 발화구간을 검출하 여 상기 발화자의 발화구간의 음성 정보를 추출할 수 있다. 본 발명의 일 실시예에서, 상기 발화자의 음성 정보를 추출하는 단계에서, 상기 검출된 얼굴 영역에 해당하는 영상 정보를 기초로 사용자 정보 데이터베이스에서 상기 발화자의 등록 음성 정보를 추출하고, 상기 등록 음성 정보를 활용하여 상기 수집된 음성 정보에서 상기 발화자의 음성 정보를 추출할 수 있다. 본 발명의 일 실시예에서, 상기 자동통역 방법은, 상기 수집된 영상 정보를 기초로 상황을 추론하는 단계를 더 포함할 수 있다. 이 경우, 상기 통역 결과를 생성하는 단계에서, 상기 상황 및 상기 발화자의 음성 정보를 기초 로, 상기 목적 언어에 따른 통역 결과를 생성할 수 있다. 본 발명의 일 실시예에서, 상기 상황을 추론하는 단계는, 상기 상황을 기초로 상황-단어 데이터베이스에서 상기 상황과 관련된 단어를 추출하는 것을 더 포함할 수 있다. 이 경우, 상기 통역 결과를 생성하는 단계에서, 상기 상황과 관련된 단어 및 상기 발화자의 음성 정보를 기초로, 상기 목적 언어에 따른 통역 결과를 생성할 수 있다. 본 발명의 일 실시예에서, 상기 자동통역 방법은, 상기 수집된 영상 정보에 포함된 텍스트를 인식하는 단계를 더 포함할 수 있다. 이 경우, 상기 통역 결과를 생성하는 단계에서, 상기 텍스트 및 상기 발화자의 음성 정보를기초로, 상기 목적 언어에 따른 통역 결과를 생성할 수 있다. 그리고, 본 발명의 일 실시예에 따른 통역 서버는, 사용자의 단말기에서 사용자 시야 내의 영상 정보와 음성 정 보를 수신하고, 상기 수신된 영상 정보에서 검출된 얼굴 영역을 기준으로 통역 대상이 되는 적어도 하나의 발화 자를 선정하며, 상기 수신된 음성 정보에서 상기 발화자의 음성 정보를 개별적으로 추출하는 분석 모듈; 및 상 기 발화자의 음성 정보를 기초로, 기 설정된 목적 언어에 따른 통역 결과를 생성하여 상기 단말기에 전송하는 통역 모듈을 포함한다. 본 발명의 일 실시예에서, 상기 통역 서버는, 상기 수신된 영상 정보에서 상기 검출된 얼굴 영역에 해당하는 영 상을 기초로 사용자 정보 데이터베이스에서 상기 통역 대상이 되는 발화자의 등록 음성 정보를 추출하는 사용자 정보 관리 모듈을 더 포함할 수 있다. 이 경우, 상기 분석 모듈은, 상기 수신된 음성 정보와 상기 등록 음성 정 보를 기초로 상기 발화자의 음성 정보를 개별적으로 추출할 수 있다. 본 발명의 일 실시예에서, 상기 통역 서버는, 상기 수신된 영상 정보에서 상기 검출된 얼굴 영역에 해당하는 영 상을 기초로 사용자 정보 데이터베이스에서 상기 통역 대상이 되는 발화자의 등록 음성 정보를 추출하는 사용자 정보 관리 모듈을 더 포함할 수 있다. 이 경우, 상기 통역 모듈은, 상기 등록 음성 정보와 상기 통역 결과를 기 초로 상기 발화자의 음색에 부합하는 합성음을 생성하여 상기 단말기에 전송할 수 있다. 본 발명의 일 실시예에서, 상기 분석 모듈은, 상기 발화자의 음성 정보에서 발화구간을 검출하여 상기 발화자의 발화구간 음성 정보를 추출할 수 있다. 이 경우, 상기 통역 모듈은, 상기 발화구간 음성 정보를 기초로 음성인 식 모델과 기계번역 모델을 이용하여 상기 통역 결과를 생성할 수 있다. 본 발명의 일 실시예에서, 상기 분석 모듈은, 상기 검출된 얼굴 영역을 기준으로, 상기 수신된 영상 정보에서 상기 발화자의 영상 정보를 추출할 수 있다. 이 경우, 상기 통역 모듈은, 상기 발화자의 음성 정보와 상기 발화 자의 영상 정보를 기초로 상기 통역 결과를 생성할 수 있다. 본 발명의 일 실시예에서, 상기 분석 모듈은, 상기 수신된 영상 정보를 기초로 상황을 추론할 수 있다. 이 경우, 상기 통역 모듈은, 상기 상황 및 상기 발화자의 음성 정보를 기초로 상기 통역 결과를 생성할 수 있다. 그리고, 본 발명의 일 실시예에 따른 단말기는, 사용자의 시야의 영상 정보와 음성 정보를 수집하는 입력 모듈; 상기 수집된 영상 정보에서 발화자의 얼굴 영역을 검출하고, 상기 검출된 얼굴 영역을 기준으로 통역 대상이 되 는 적어도 하나의 발화자를 선정하며, 상기 수집된 음성 정보에서 상기 발화자의 음성 정보를 개별적으로 추출 하여 통역 서버에 전송하는 분석 모듈; 및 상기 통역 서버에서 상기 발화자의 음성 정보에 대한 통역 결과를 수 신하고, 상기 통역 결과를 디스플레이 및 스피커 중 적어도 어느 하나를 통해 출력하는 출력 모듈을 포함한다. 본 발명의 일 실시예에서, 상기 단말기는 상기 사용자가 착용한 스마트글래스일 수 있다. 본 발명의 일 실시예에서, 상기 분석 모듈은, 상기 검출된 얼굴 영역을 기준으로, 상기 수집된 영상 정보에서 상기 발화자의 영상 정보를 추출하여 상기 통역 서버에 전송할 수 있다. 본 발명의 일 실시예에서, 상기 분석 모듈은, 상기 수집된 영상 정보에서 검출된 얼굴 영역의 영상을 상기 통역 서버에 전송하고, 상기 통역 서버에서 상기 검출된 얼굴 영역의 영상에 부합하는 발화자의 등록 음성 정보를 수 신하며, 상기 등록 음성 정보를 활용하여 상기 수집된 음성 정보에서 상기 발화자의 음성 정보를 추출할 수 있 다. 본 발명의 일 실시예에서, 상기 분석 모듈은, 상기 수집된 영상 정보를 기초로 상황을 추론하고, 상기 추론된 상황을 상기 통역 서버에 전송할 수 있다."}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 종래 방법인 음성기반 자동통역 방법과 달리, 사용자는 주변의 복수 화자의 음성을 모두 모 국어로 변환하여 편리하게 정보습득이 가능하다는 효과가 있다. 또한, 본 발명에 따르면, 종래의 면대면(face-to-face) 방법과 융합하여 복수의 사용자가 스마트기기(스마트폰, 스마트글래스)를 착용 시 양방향 대화모드와 혼자 착용시 듣기모드를 모두 제공할 수 있는 효과가 있다. 또한, 본 발명에 따르면, 발화 겹침 구간에서 강건하게 원음을 분리할 수 있으므로, 다국어 음성을 자연스럽게 사용자의 모국어로 통역하여 증강함으로써 사용자의 여행을 효과적으로 지원할 수 있다는 효과가 있다."}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은 아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "일반적으로 자동통역은 사용자 음성만을 통역하여 상대방 단말로 송신하거나 상대방 단말기로부터 수신된 통역 결과를 합성하여 사용자에게 들려주는 방식으로 동작한다. 이러한 방식은 사용자와 상대방이 존재하는 면대면 대화를 목적으로 한다. 그러나 자동통역은, 사용자가 외국을 여행하는 경우, 외국인과 대화(관광, 식당, 쇼핑, 호텔, 비즈니스 등)를 하는 경우, 공항 안내 음성, 여행지역 TV 뉴스, 주변 외국인 음성 등을 이해하여 상황을 판단해야 하는 경우 등, 주변의 외국어 음성을 사용자가 능동적으로 수신하여 정보를 얻어야 하는 경우에 그 필요성이 높다. 즉, 사 용자가 원하는 정보를 제공하는 외국인 음성만을 통역할 수 있도록 지원할 수 있는 기술이 요구된다. 종래 기술을 살펴보면, 사용자가 원하는 음성구간을 직접 터치하여 원하는 음성 구간에 대한 자동통역을 시도하 는 방법, 사용자 주변에서 흔히 발생하는 상황으로서, 다수 화자의 음성이 혼합된 경우에 개별 음성을 분리하여 통역 결과를 들려주는 등의 방법 등이 제안되었다. 그러나, 사용자가 직접 버튼을 통해 음성 구간을 지정하는 것은 사용성 측면에서 매우 불편하고, 이를 보완하기 위하여 등장한 자동 음성 구간 검출기의 경우 음성의 정보 만을 이용하여 잡음 상황에서 의도하는 음성구간을 확보하는 방법을 취하고 있어서, 성능에 한계가 있다. 또한 다수 화자의 음성이 혼합된 소리에서 원하는 음성만을 분리 후 번역된 결과로 보여주는 방법 또한 안정적인 성 능 확보가 어렵다. 또한, 분리과정을 거쳐 생성된 음성은 원하는 음성만이 아닌 불필요한 통역 결과까지 함께 전달하여 사용자에게 혼란을 줄 수 있다는 단점이 있다. 본 발명은 종래 기술과 달리, 음성 정보와 함께 사용자가 의도가 반영된 시선입력 영상 정보도 활용하여 정보 수집 대상의 범위를 좁히고, 그 범위 내에서 존재하는 발화자들의 음성만을 통역함으로써 강인한 통역 결과를 얻을 수 있다. 즉, 본 발명은 영상 정보와 음성 정보를 함께 활용함으로써 사용자의 시선 움직임에 따라 사용자 의 의도를 반영하여 통역 결과를 출력하는 자동통역 시스템 및 방법을 제안한다. 특히 본 발명은 사용자의 조작 없이도 자연스럽게 사용자가 청취 대상으로 삼는 화자의 발화를 즉시 화면에 통역하여 표시하는 기능을 제공함 으로써 사용자가 주변에서 발생하는 음성이나 대화 상대의 발화 음성에서 빠르고 자연스럽게 정보를 획득할 수 있는 방법을 제공한다. 또한, 앞으로 널리 사용될 스마트글래스(예: 구글글래스, 애플글래스)를 그대로 활용할 수 있으므로 본 발명의 활용 범위도 매우 크다. 본 발명을 활용함으로써, 사용자는 자신의 주변에서 발생하는 다중화자의 음성을 자신의 모국어로 시각화할 수 있으므로 주변의 다국어 음성 정보를 편리하게 취득할 수 있다. 본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 한편, 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포 함한다. 명세서에서 사용되는 \"포함한다(comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성소자, 단계, 동작 및/또는 소자는 하나 이상의 다른 구성소자, 단계, 동작 및/또는 소자의 존재 또는 추가를 배제하지 않는다. 본 발명에서 사용자의 단말기('단말기')는 멀티모달 정보를 입력받을 수 있고, 서버(통역 서버 또는 사용자 정 보 관리 서버)에서 통역 결과 또는 사용자 정보(화자정보)를 수신하고, 통역 결과를 화면이나 스피커를 통해 출 력할 수 있는 기기이며, 구현 형태에 제한이 없다. 예를 들어, 단말기는 스마트폰이나 스마트글래스 등의 스마 트기기일 수 있다. 본 명세서에서 '사용자의 시야(視野, visual field)'(또는 '시선 범위')는 사용자의 가시 범위일 수 있다. '사 용자의 시야'는 사용자의 위치 및/또는 사용자의 얼굴이 향하는 방향을 기준으로 결정될 수 있다. '사용자의 시 야'는 사용자의 얼굴의 정면 방향을 중심으로 하거나 사용자의 양안(兩眼)의 법선을 기준으로 하여 수평방향과 수직방향으로 일정한 크기를 가지는 각도 범위로 표현될 수 있다. 예를 들어, 사용자의 얼굴의 정면 방향을 중 심으로 수평방향으로 -60˚~ 60˚(binocular visual field, 양안 시야(兩眼 視野)) 또는 -90˚~ 90˚ (monocular visual field, 단안 시야(單眼 視野)), 수직방향으로 -75˚(아래) ~ 55˚(위)의 범위를 '사용자의 시야'로 설정할 수 있다. 다만, 본 발명의 실시예에서 '사용자의 시야'는 영상정보 입력 수단(예: 스마트글래스)의 특성에 따라 제한되거나 확장될 수 있다. 예를 들어, '사용자의 시야'는 사용자가 착용한 스마 트글래스에 장착된 렌즈의 화각(FOV, field of view) 범위 내로 제한될 수 있다. 본 발명을 설명함에 있어서, 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 이하, 본 발명의 실시예를 첨부한 도면들을 참조하여 상세히 설명한다. 본 발명을 설명함에 있어 전체적인 이해 를 용이하게 하기 위하여 도면 번호에 상관없이 동일한 수단에 대해서는 동일한 참조 번호를 사용하기로 한다. 도 1은 본 발명의 일 실시예에 따른 시선 기반 증강형 자동통역 방법을 설명하기 위한 흐름도이다. 본 발명의 일 실시예에 따른 시선 기반 증강형 자동통역 방법은 S110 단계 내지 S160 단계를 포함한다. S110 단계는 목적 언어 설정 단계이다. 본 단계에서, 통역 결과를 구성할 목적 언어가 설정된다. 목적 언어는 사용자의 단말기를 통해 설정될 수 있는데, 본 발명에 따른 자동통역 시스템에서 해당 사용자에 대해 사전에 설 정된 언어일 수도 있고, 통역 작업이 시작될 때 사용자가 선택하는 방식으로 설정될 수도 있다. S120 단계는 영상 기반 얼굴 검출 단계이다. 사용자의 단말기(이하 '단말기')는 사용자의 시야 내에 있는 멀티 모달 정보(영상 정보 및 음성 정보)를 입력받는다. 단말기는 멀티모달 정보를 입력받을 수 있고, 서버(통역 서 버 또는 사용자 정보 관리 서버)에서 통역 결과 또는 사용자 정보(화자정보)를 수신하고, 통역 결과를 화면이나 스피커를 통해 출력할 수 있는 기기이며, 구현 형태에 제한이 없다. 예를 들어, 단말기는 스마트폰이나 스마트 글래스 등의 스마트기기일 수 있다. 멀티모달 정보에 포함되는 영상 정보의 범위는 사용자의 의도를 반영하여 조정될 수 있다. 예를 들어, 단말기에 입력되는 영상 정보는 부분적으로 확대, 축소될 수 있다(줌 인, 줌 아 웃). 본 발명에 따른 자동통역 시스템은 단말기에 입력되는 영상 정보를 기초로 사용자가 통역을 원하는 발화자 의 범위(통역 대상이 되는 발화자)를 판단한다. 단말기 또는 통역 서버는 상기 영상 정보에 포함된 발화자의 얼 굴을 검출한다. 예를 들어, 단말기가 단말기에 입력된 영상 정보를 통역 서버로 전송하면, 통역 서버가 상기 영 상 정보에 포함된 발화자의 얼굴을 검출한다. 이후, 단말기 또는 통역 서버는 음성 추출 및 발화구간 검출 작업 을 수행하는데, 검출된 얼굴을 기준으로 발화자의 수를 판단한 후, 발화자의 수만큼 병렬적으로 음성 추출(발화 자별 음성 분리) 및 발화 구간 검출 작업을 수행하게 된다."}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "S130 단계는 발화자의 영상 정보 및 음성 정보를 추출하는 단계이다. 단말기 또는 통역 서버는 발화자의 얼굴이 검출된 이후, 단말기에서 수집된 영상 및 음성에서 상기 검출된 얼굴 영역에 대응하는 각 발화자의 영상 정보와 음성 정보를 추출한다. 즉, 단말기 또는 통역 서버는 단말기에서 수집된 영상에서 각 발화자의 영상 정보를 분 리하며, 단말기에서 수집된 음성에서 각 발화자의 음성 정보를 분리한다. 예를 들어, 단말기가 단말기에 입력된음성 정보를 통역 서버로 전송하면, 통역 서버가 상기 음성 정보에서 각 발화자의 음성 정보를 추출한다. 단말기 또는 통역 서버는 수집된 영상이나 개별 발화자의 영상을 활용하여 발화자별 음성 추출 결과의 신뢰성을 높일 수 있다. 예를 들어, 단말기 또는 통역 서버는 개별 발화자의 영상 정보에 나타난 발화자의 입모양 등을 기초로 발화자별 음성 추출 결과를 검증할 수 있다. 단말기 또는 통역 서버의 통역 모듈이 파이프라인형 통역 엔진을 사용하는 경우, 각 발화자의 음성 정보에서 발 화 구간의 음성을 추출하기 위해 S130 단계 종료 후 S140 단계가 진행된다. 만약 단말기 또는 통역 서버의 통역 모듈이 종단형 통역 엔진을 사용하는 경우, 발화구간 검출 및 발화구간의 음성 정보 추출이라는 전처리 과정 (S140 단계)이 요구되지 않으므로 S130 단계 종료 후 곧바로 S150 단계가 수행된다. S140 단계는 발화구간 검출 단계이다. 단말기 또는 통역 서버는 각 발화자의 음성 정보에서 발화구간을 검출한 다. 즉, 단말기 또는 통역 서버는 각 발화자의 음성 정보에서 발화 시작점과 종료점을 검출함으로써 발화구간을 검출한다. 이 경우, 단말기나 통역 서버는 개별 발화자의 영상 정보를 활용하여 발화의 시작점 및 종료점 검출 의 신뢰성을 높일 수 있다. 예를 들어, 단말기나 통역 서버는 개별 발화자의 입모양을 기초로 발화 여부를 판단 함으로써 발화구간을 검출하거나 기 검출된 발화구간의 정확성을 검증할 수 있다. 단말기 또는 통역 서버는 검 출된 발화구간의 음성 정보를 단말기 또는 통역 서버에 내장된 통역 모듈(통역 엔진)에 전달한다. S150 단계는 통역 단계이다. 통역 모듈은 통역 대상 음성 정보를 기초로 통역 결과를 생성한다. 발화자의 음성 정보 또는 발화구간의 음성 정보가 통역 대상 음성 정보가 될 수 있다. 통역 결과는 목적 언어로 된 텍스트일 수도 있고, 목적 언어로 합성된 발화 음성일 수도 있다. 본 발명에서 통역 모듈의 통역 방식은 제한을 두지 않 는다. 예를 들어, 통역 모듈은 음성인식 모델과 기계번역 모델을 이용하여 통역 작업을 수행하는 파이프라인형 통역 엔진을 사용할 수도 있고, 하나의 통역 모델을 이용하여 통역 작업을 수행하는 종단형(end-to-end) 통역 엔진을 사용할 수도 있다. 만약, 통역 모듈은 음성인식 모델과 기계번역 모델을 이용하여 통역 작업을 수행하는 파이프라인형 통역 엔진을 사용하는 경우, 통역 모듈은 S140 단계에서 생성한 발화구간의 음성 정보를 음성인식 모델에 입력하여 목적 언 어로 된 음성 인식 결과를 생성하고, 상기 음성 인식 결과를 기계번역 모델에 입력하여 통역 결과를 생성한다. 이와 달리, 통역 모듈이 종단형(end-to-end) 통역 엔진을 사용하는 경우, 통역 모듈은 S130 단계에서 생성한 발 화자의 음성 정보를 하나의 통역 모델(예를 들어 학습된 신경망)에 입력하여 통역 결과를 생성한다. 다른 예로서, 통역 모듈은 발화자의 영상 정보와 통역 대상 음성 정보를 기초로 통역 결과를 생성할 수 있다. 구체적으로, 통역 모듈은 발화자의 영상 정보와 통역 대상 음성 정보를 학습된 딥러닝 모델(신경망), 즉 멀티모 달 종단형 통역 엔진에 입력하여 통역 결과를 생성할 수 있다. 또한, 통역 모듈은 발화자의 영상 정보 대신에 발화자 영상 정보에서 추출한 특징(예를 들어, 입모양)을 발화자 의 음성 정보와 함께 학습된 딥러닝 모델에 입력하여 통역 결과를 생성할 수도 있다. S160 단계는 통역 결과 출력 단계이다. 단말기는 통역 결과를 화면에 표시하거나 스피커로 출력한다. 통역 서버 에서 통역 결과가 생성된 경우, 단말기는 통역 서버에서 통역 결과를 전달받아야 한다. 단말기가 통역 결과를 화면에 표시할 경우, 목적 언어로 된 통역 결과 텍스트를 사용자가 보는 화면에 증강하여 출력할 수 있다. 예를 들어, 다수 발화자가 발화하는 경우, 단말기는 목적 언어로 된 통역 결과 텍스트를 개별 발화자의 얼굴 영역 영상에 증강하여 출력할 수 있다. 단말기가 통역 결과를 스피커로 출력할 경우, 단말기는 목적 언어로 이루어진 합성음을 출력(재생)한다. 이때 상기 합성음은 발화자 음색과 유사하게 합성된 음성일 수 있다. 단말기가 목적 언어로 된 텍스트를 기초로 합성 음을 생성하여 이를 출력할 수도 있고, 통역 서버에서 생성한 합성음을 단말기에서 전송받아 출력할 수도 있다."}
{"patent_id": "10-2022-0062775", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 4, "content": "도 2는 본 발명의 일 실시예에 따른 시선 기반 증강형 자동통역 방법을 설명하기 위한 흐름도이다. 도 2의 실시 예는 본 발명에 따른 자동통역 시스템의 서버(통역 서버 또는 사용자 정보 관리 서버)에 사용자 정보가 등록된 경우를 전제하며, 등록된 다수의 사용자 간의 대화에 있어서 사용자 정보를 통역에 활용하는 방법을 제시한다. 즉, 도 2의 실시예는 다수의 사용자가 동시에 멀티모달 자동통역 시스템을 사용할 경우 통역 서버에 등록된 사 용자 정보(화자 정보)를 사용하여 자동통역하는 방법에 관한 것이다. 도 2의 실시예에 따른 자동통역 방법은 S210 단계 내지 S270 단계를 포함한다.S210 단계는 사용자 정보(화자 정보)를 등록하는 단계이다. 자동통역 시스템의 개별 사용자는 최초 사용 시에 본인의 음성 정보, 얼굴 정보 및 사용 언어(목적 언어)를 서버(통역 서버 또는 사용자 정보 관리 서버)에 등록 한다. 상기 서버는 사용자 정보 데이터베이스(사용자 정보 DB)에 사용자 음성, 사용자 얼굴 및 사용자의 목적 언어에 관한 정보를 전달하며, 상기 정보는 사용자 정보 데이터베이스에 저장된다. S220 단계는 목적 언어 설정 단계이다. 본 단계에서, 통역 결과를 구성할 목적 언어가 설정된다. 목적 언어는 사용자의 단말기를 통해 설정될 수 있는데, 기본적으로 단말기의 사용자가 서버(통역 서버 또는 사용자 정보 관 리 서버)에 등록한 본인의 목적 언어로 설정될 수 있고, 사용자는 설정된 목적 언어를 다른 언어로 변경할 수 있다. S230 단계는 영상 기반 얼굴 검출 단계이다. 단말기는 사용자의 시야에 있는 멀티모달 정보(영상 정보 및 음성 정보)를 입력받는다. 멀티모달 정보에 포함되는 영상 정보의 범위는 사용자의 의도를 반영하여 조정될 수 있다. 본 발명에 따른 자동통역 시스템은 단말기에 입력되는 영상 정보를 기초로 사용자가 통역을 원하는 발화자의 범 위(통역 대상이 되는 발화자)를 판단한다. 단말기 또는 통역 서버는 상기 영상 정보에 포함된 발화자의 얼굴을 검출한다. 예를 들어, 단말기가 단말기에 입력된 영상 정보를 통역 서버로 전송하면, 통역 서버가 상기 영상 정 보에 포함된 발화자의 얼굴을 검출한다. 단말기에서 얼굴을 검출한 경우, 단말기는 검출된 얼굴 정보를 서버(통역 서버 또는 사용자 정보 관리 서버)에 전송한다. 서버(통역 서버 또는 사용자 정보 관리 서버)는 검출된 얼굴 정보와 상기 서버에 등록된 얼굴 정보(사용자 정보 DB에 저장된 얼굴 정보)의 유사성 분석을 통하여 검출된 얼굴에 대응하는 사용자(발화자)를 식별할 수 있다. 따 라서, 상기 서버는 상기 발화자 식별 결과를 기초로 해당 발화자의 등록 음성 정보(서버에 등록된 발화자의 음 성 정보를 말함)를 사용자 정보 DB에서 추출할 수 있다. 또한, 상기 서버는 상기 발화자 식별 결과를 기초로 해 당 발화자의 사용 언어를 사용자 정보 DB에서 추출할 수 있다. 발화자의 사용 언어 정보는 S240 단계 및 S260 단계에서 사용될 수 있다. 예를 들어, 다수 발화자의 혼합 음성에서 특정 발화자의 음성을 추출하는 과정이나 통역대상 음성 정보를 인식하여 목적 언어로 된 텍스트를 생성하는 과정에서 단말기 또는 통역 서버는 발화자의 사용 언어 정보를 사용할 수 있다. 단말기 또는 통역 서버는 발화자의 사용 언어 정보를 사용함으로써 혼합 음 성에서 특정 발화자의 음성을 강인하게 추출할 수 있으며, 이에 따라 통역이 자연스럽게 이루어질 수 있다. 단말기 또는 통역 서버는 이후 음성 추출 및 발화구간 검출 작업을 수행하는데, 검출된 얼굴을 기준으로 발화자 의 수를 판단한 후, 발화자의 수만큼 병렬적으로 음성 추출(발화자별 음성 분리) 및 발화 구간 검출 작업을 수 행하게 된다. S240 단계는 발화자의 영상 정보 및 음성 정보를 추출하는 단계이다. 단말기 또는 통역 서버는 발화자의 얼굴이 검출된 이후, 단말기에서 수집된 영상 및 음성에서 상기 검출된 영역에 대한 개별 발화자의 영상 정보와 음성 정보(통역 대상 영상 및 음성)를 추출한다. 즉, 단말기 또는 통역 서버는 단말기에서 수집된 영상 및 음성에서 개별 발화자의 영상과 음성을 분리한다. 예를 들어, 단말기가 단말기에 입력된 음성 정보를 통역 서버로 전송하 면, 통역 서버가 상기 음성 정보에서 각 발화자의 음성 정보를 추출한다. 단말기 또는 통역 서버는 서버(통역 서버 또는 사용자 정보 관리 서버)에 등록된 발화자의 음성 정보(발화자의 등록 음성 정보)를 기반으로 다수 발화자의 혼합된 음성에서 사용자가 통역 대상으로 삼는 개별 발화자의 음성 만을 추출할 수 있다. 본 발명의 다른 예로서, 단말기 또는 통역 서버는 상기 서버에 등록된 사용자의 음성 정보를 기반으로 혼합된 음성에서 사용자의 음성을 분리할 수 있고, 사용자의 음성을 기준으로 사용자와 대화하는 발화자가 누구인지 식 별하여, 식별된 발화자를 통역 대상 발화자로 선택하여 해당 발화자의 음성 정보를 추출할 수도 있다. 예를 들 어, 사용자의 발화가 종료한 시점과 특정 발화자의 발화가 시작된 시점이 일정 시간 범위 내에 포함되거나, 사 용자의 발화가 시작한 시점과 특정 발화자의 발화가 종료된 시점이 일정 시간 범위 내에 포함되는 경우, 단말기 또는 통역 서버는 상기 특정 발화자의 음성만을 추출할 수 있을 것이다. 단말기 또는 통역 서버는 통역 대상 발 화자가 선택된 경우, 해당 발화자가 영상 정보(화면) 상에서 이탈하더라도 해당 발화자의 음성 정보를 추출할 수 있다. 단말기 또는 통역 서버는 수집된 영상이나 개별 발화자 영상을 활용하여 화자별 음성 추출 결과의 신뢰성을 높 일 수 있다. 예를 들어, 단말기 또는 통역 서버는 개별 발화자 영상에 나타난 화자의 입모양 등을 기초로 화자 별 음성 추출 결과를 검증할 수 있다.단말기 또는 통역 서버의 통역 모듈이 파이프라인형 통역 엔진을 사용하는 경우, 통역 대상인 개별 발화자의 음 성 정보에서 발화 구간의 음성 정보를 추출하기 위해 S240 단계 종료 후 S250 단계가 진행된다. 만약 단말기 또 는 통역 서버의 통역 모듈이 종단형 통역 엔진을 사용하는 경우, 발화구간 검출 및 발화구간의 음성 정보 추출 이라는 전처리 과정(S250 단계)이 요구되지 않으므로 S240 단계 종료 후 곧바로 S260 단계가 수행된다. S250 단계는 발화구간 검출 단계이다. 단말기 또는 통역 서버는 각 발화자의 음성 정보에서 발화구간을 검출한 다. 즉, 단말기 또는 통역 서버는 각 발화자의 음성 정보에서 발화 시작점과 종료점을 검출함으로써 발화구간을 검출한다. 이 경우, 단말기나 통역 서버는 개별 발화자의 영상 정보를 활용하여 발화의 시작점 및 종료점 검출 의 신뢰성을 높일 수 있다. 예를 들어, 단말기나 통역 서버는 개별 발화자의 입모양을 기초로 발화 여부를 판단 함으로써 발화구간을 검출하거나 기 검출된 발화구간의 정확성을 검증할 수 있다. 단말기 또는 통역 서버는 검 출된 발화구간의 음성 정보를 단말기 또는 통역 서버에 내장된 통역 모듈(통역 엔진)에 전달한다. S260 단계는 통역 단계이다. 통역 모듈은 통역대상 음성 정보를 기초로 통역 결과를 생성한다. 발화자의 음성 정보 또는 발화구간의 음성 정보가 통역 대상 음성 정보가 될 수 있다. 통역 결과는 목적 언어로 된 텍스트일 수도 있고, 목적 언어로 합성된 발화 음성일 수도 있다. 본 발명에서 통역 모듈의 통역 방식은 제한을 두지 않 는다. 예를 들어, 통역 모듈은 음성인식 모델과 기계번역 모델을 이용하여 통역 작업을 수행하는 파이프라인형 통역 엔진을 사용할 수도 있고, 하나의 통역 모델을 이용하여 통역 작업을 수행하는 종단형(end-to-end) 통역 엔진을 사용할 수도 있다. 만약, 통역 모듈은 음성인식 모델과 기계번역 모델을 이용하여 통역 작업을 수행하는 파이프라인형 통역 엔진을 사용하는 경우, 통역 모듈은 S250 단계에서 생성한 발화구간의 음성 정보를 음성인식 모델에 입력하여 목적 언 어로 된 음성 인식 결과를 생성하고, 상기 음성 인식 결과를 기계번역 모델에 입력하여 통역 결과를 생성한다. 이와 달리, 통역 모듈이 종단형(end-to-end) 통역 엔진을 사용하는 경우, 통역 모듈은 S240 단계에서 생성한 발 화자의 음성 정보를 하나의 통역 모델(예를 들어 학습된 신경망)에 입력하여 통역 결과를 생성한다. 다른 예로서, 통역 모듈은 발화자의 영상 정보와 통역 대상 음성 정보를 기초로 통역 결과를 생성할 수도 있다. 구체적으로, 통역 모듈은 발화자의 영상 정보와 통역 대상 음성 정보를 학습된 딥러닝 모델(신경망), 즉 멀티모 달 종단형 통역 엔진에 입력하여 통역 결과를 생성할 수 있다. 또한, 통역 모듈은 발화자의 영상 정보 대신에 발화자 영상 정보에서 추출한 특징(예를 들어, 입모양)을 학습 된 딥러닝 모델에 입력하여 통역 결과를 생성할 수도 있다. S270 단계는 통역 결과 출력 단계이다. 단말기는 통역 결과를 화면에 표시하거나 스피커로 출력한다. 통역 서버 에서 통역 결과가 생성된 경우, 단말기는 통역 서버에서 통역 결과를 전달받아야 한다. 단말기가 통역 결과를 화면에 표시할 경우, 목적 언어로 된 통역 결과 텍스트를 사용자가 보는 화면에 증강하여 출력할 수 있다. 예를 들어, 다수 발화자가 발화하는 경우, 단말기는 목적 언어로 된 통역 결과 텍스트를 개별 발화자의 얼굴 영역 영상에 증강하여 출력할 수 있다. 단말기가 통역 결과를 스피커로 출력할 경우, 단말기는 목적 언어로 이루어진 합성음을 출력(재생)한다. 이때 상기 합성음은 서버(통역 서버 또는 사용자 정보 관리 서버)에 등록된 발화자의 음성 정보(발화자의 등록 음성 정보)에 나타난 음색과 유사하게 합성된 음성일 수 있다. 단말기가 목적 언어로 된 텍스트를 기초로 합성음을 생성하여 이를 출력할 수도 있고, 통역 서버에서 생성한 합성음을 단말기에서 전송받아 출력할 수도 있다. 도 3은 본 발명의 일 실시예에 따른 시선 기반 증강형 자동통역 방법을 설명하기 위한 흐름도이다. 도 3의 실시예는 도 1의 실시예에 영상 기반 상황 판단 단계(S350)와 단어/어구 추출 단계(S360)를 추가한 것으 로서, 영상 분석으로 얻은 상황 정보를 이용하여 자동통역 성능을 개선하는 실시예를 나타낸 것이다. 도 3의 실 시예에 따른 자동통역 방법은 S310 단계 내지 S380 단계를 포함한다. S310 단계, S320 단계, S330 단계, S340 단계, S370 단계 및 S380 단계는, 각각 S110 단계 내지 S160 단계의 각 단계와 그 수행 내용이 기본적으로 동일 하다. 따라서 도 1의 실시예와 동일한 내용에 대해서는 설명을 생략한다. 다만, 본 실시예에서 S310 단계 수행 이후 S320 단계와 S350 단계가 병렬적으로 수행되며, S340 단계와 S360 단계 종료 이후 S370 단계가 수행된다. 도 3의 실시예에 대해서는 도 1의 실시예와의 차이점을 기준으로 하기에 설명한다. 자동통역 시스템에서 주제(예:여행, 음식, 과학 등) 또는 상황 정보(예: 목적지가 있는 방향을 문의하는 상황, 식당에서 음식을 주문하는 상황 등)는 통역 성능을 향상하는 데 있어서 매우 중요한 정보이다. 특히, 고유명사가 포함된 통역대상 음성 정보를 통역하는 경우, 주제 또는 상황 정보가 통역 성능에 미치는 영향이 크다. S350 단계 및 S360 단계는 주제 또는 상황 정보를 통역에 반영하기 위하여 도 1의 실시예에 추가된 단계이다. S350 단계는 영상 기반 상황 판단 단계이다. 단말기 또는 통역 서버는 단말기에 입력된 영상 정보를 분석하여 주제 또는 상황을 판단한다. 구체적으로, 단말기 또는 통역 서버는 영상에서 인식되는 텍스트, 객체, 복수 객체 의 분포, 사용자 주변 환경의 특징(건물의 형태, 창문의 모양과 배치, 윤곽선의 배치와 방향), 색상 등의 특징 을 기반으로 주제나 상황을 판단할 수 있다. 단말기 또는 통역 서버는 상기 영상 정보 또는 영상 정보에서 추출 한 특징을 기 학습된 딥러닝 모델에 입력하여 주제나 상황을 추론할 수 있다. 도 4는 발화자가 미국 여행 중 그 랜드 센트럴 터미널(Grand Central Terminal)에서 자유의 여신상으로 가는 방법을 문의하는 상황을 도시한 것이다. 단말기 또는 통역 서버는 영상 분석을 통하여 주제나 상황을 추론하거나 주제나 상황에 관한 임베딩 벡터 (embedding vector, latent vector, representation)를 생성할 수 있다. 예를 들어, 단말기 또는 통역 서버는 발화자의 영상 정보를 기반으로 발화자가 그랜드 센트럴 터미널에서 다른 곳으로 이동하는 상황이라는 판단을 할 수 있다. S360 단계는 상황과 관련된 단어나 어구를 추출하는 단계이다. 먼저, 단말기 또는 통역 서버는 S350 단계에서 생성된 상황 판단 결과를 기반으로 관련 단어나 어구를 추출할 수 있다. 예를 들어, 단말기 또는 통역 서버는 S350 단계 결과 도출된 주제나 상황에 관련된 단어/어구를 상황-단어 데이터베이스에서 추출할 수 있다. 이 과 정에서 단말기 또는 통역 서버는 주제나 상황에 관한 임베딩 벡터를 기초로 관련된 단어/어구를 추출할 수 있다. 도 4의 예시와 같이, 단말기 또는 통역 서버는 발화자가 그랜드 센트럴 터미널에서 다른 곳으로 이동하는 상황이라는 정보에 기초하여 데이터베이스에서 'U.S.A.', 'Grand Central (Terminal)', 'New York', 'Statue of liberty' 등의 단어/어구를 추출할 수 있다. 다른 예로, 단말기 또는 통역 서버는 영상에서 텍스트를 인식하고 상기 텍스트에서 단어나 어구를 추출할 수 있 다. 예를 들어, 단말기 또는 통역 서버는 영상에 나타난 간판에서 브랜드명이나 상호를 추출할 수 있으며, 영상 에 나타난 식당 메뉴판에서 식당이나 요리의 명칭을 추출할 수 있다. 단말기 또는 통역 서버는 영상 정보에서 직접적으로 추출한 단어나 어구도 상황과 관련된 단어/어구로 취급하여 통역에 활용할 수 있다. S370 단계는 통역 단계이다. 통역 모듈은 통역 대상 음성 정보를 기초로 통역 결과를 생성한다. S330 단계에서 생성된 발화자의 음성 정보 또는 S340 단계에서 생성된 발화구간의 음성 정보가 통역 대상 음성 정보가 될 수 있다. 통역 결과는 목적 언어로 된 텍스트일 수도 있고, 목적 언어로 합성된 발화 음성일 수도 있다. 통역 모듈은 S360 단계에서 도출된 정보/데이터를 통역 과정에 반영할 수 있다. 즉, 통역 모듈은, 주제, 상황, 주제에 관련된 임베딩 벡터, 상황에 관련된 임베딩 벡터, 주제에 관련된 단어/어구 및 상황에 관련된 단어/어구 중 적어도 어느 하나를 통역 과정에 반영할 수 있다. 예를 들어, 통역 모듈은 통역대상 음성 정보를 인식하는 과정에서 상황에 관련된 단어와 유사한 발음을 해당 단어로 인식할 수 있다. 통역 과정에 주제/상황 정보, 주제/상황에 관련된 임베딩 벡터나 단어/어구 등을 활용할 경우, 사용자 주변 환 경에 관한 정보가 통역에 반영되므로 음성 인식의 정확도를 높일 수 있어서 통역품질이 개선될 수 있다. 한편, 도 3의 실시예에서는 이해를 돕기 위해 S310 단계에서 분기되거나 S340 단계와 S360 단계의 종료 후 S370 단계가 수행되는 예를 보여주고 있으나 본 발명의 범위는 이와 같은 예에 한정되지 않는다. 본 발명에 따른 자 동통역 방법은 통역 성능의 향상을 위해 영상, 음성 및 상황정보를 통합적으로 학습한 통역 모델을 사용할 수 있다. 도 1 내지 도 4를 참조하여 전술한 시선 기반 증강형 자동통역 방법은 도면에 제시된 흐름도를 참조로 하여 설 명되었다. 간단히 설명하기 위하여 상기 방법은 일련의 블록들로 도시되고 설명되었으나, 본 발명은 상기 블록 들의 순서에 한정되지 않고, 몇몇 블록들은 다른 블록들과 본 명세서에서 도시되고 기술된 것과 상이한 순서로 또는 동시에 일어날 수도 있으며, 동일한 또는 유사한 결과를 달성하는 다양한 다른 분기, 흐름 경로, 및 블록 의 순서들이 구현될 수 있다. 또한, 본 명세서에서 기술되는 방법의 구현을 위하여 도시된 모든 블록들이 요구 되지 않을 수도 있다. 한편 도 1 내지 도 3을 참조한 설명에서, 각 단계는 본 발명의 구현예에 따라서, 추가적인 단계들로 더 분할되 거나, 더 적은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있다. 예를 들어, 도 1의 실시예에서 S110 단계 또는 S160 단계는 생략될 수 있다. 또한, 본 발명 에 따른 자동통역 시스템의 통역 모듈이 종단형 통역 엔진을 사용하는 경우, 발화구간을 검출하는 전처리 과정 인 S140 단계, S250 단계 및 S340 단계가 생략될 수 있다. 아울러, 기타 생략된 내용이라 하더라도 도 4 내지도 11의 내용은 도 1 내지 도 3의 내용에 적용될 수 있다. 또한, 도 1 내지 도 3의 내용은 도 4 내지 도 11의 내용에 적용될 수 있다. 도 5는 본 발명의 실시예에 따른 자동통역 과정을 나타낸 참고도이다. 도 5는 각자 다른 언어를 사용하는 다수 의 발화자의 통역대상 음성 정보가 혼합되는 경우를 도시하고 있다. 본 발명에 따른 자동통역 시스템을 최초로 사용할 때 개별 사용자(발화자)는 서버(통역 서버 또는 사용자 정보 관리 서버)에 본인의 얼굴과 음성 그리고 사용 언어(해당 사용자에 대하여 목적 언어가 된다)를 등록한다. 최초 등록 시점은 개별 사용자의 상황에 따라 다를 수 있다. 최초 등록 후 등록정보는 상기 서버에 저장/ 관리된다. 이후 사용자가 자동통역 시스템을 사용할 경우, 단말기 또는 통역 서버는 시야 내의 얼굴이 어떤 화자의 얼굴인지를 확인(식별)할 수 있다. 단말기 또는 통역 서버는 발화자의 등록 음성 정보를 기초로 다수 발화자의 혼합된 음성에서 개별 발화자의 음성 정보를 효 과적으로 분리함으로써 통역 품질을 향상할 수 있다. 복수 발화자의 음성 정보에 대한 통역이 완료되면, 통역 결과를 발화자별 얼굴 영상에 증강하여 표시할 수 있다. 도 6은 본 발명의 실시예에 따라 단일 화자에 대한 통역 결과를 스마트폰으로 표시하는 경우의 예시 도면이다. 도 6은 통역 대상을 한 명으로 제한하였을 때의 자동통역 시스템의 동작을 나타낸다. 사용자의 단말기(스마트기 기)는 음성 정보와 영상 정보를 수신하여 서버에 전송한 후, 서버에서 통역 결과를 전달받아 출력할 수도 있고, 내장된 통역 모듈(임베디드 연산장치)에서 통역 결과를 전달받아 출력할 수도 있다. 도 7은 본 발명의 실시예에 따라 다중 화자에 대한 통역 결과를 스마트폰으로 표시하는 경우의 예시 도면이다. 도 7은 통역 대상이 되는 발화자가 다수인 경우, 본 발명에 따른 자동통역 시스템이 단말기(스마트기기)의 시야 에 따라 발화자를 제한하여 동작하는 모습을 보여준다. 다수의 발화자가 동시에 발화하는 상황에서, 단말기는 사용자의 시야 내에 있는 발화자의 자동통역 결과만을 제공함으로써 사용자의 의도를 통역 결과에 자연스럽게 반영하고 있다. 사용자의 단말기가 스마트폰 대신 스마트글래스인 경우 상술한 효과가 극대화될 수 있다. 도 8은 본 발명의 실시예에 따라 다국어 발화 상황에서 통역 결과를 스마트글래스로 표시하는 경우의 예시 도면 이다. 도 8의 예시는 통역 대상이 여러 언어를 동시에 사용하는 경우에 있어서 사용자가 착용한 스마트글래스가 사용자의 목적 언어로 통역결과를 표시하는 모습을 나타낸다. 본 발명에 따른 통역 서버 또는 단말기(본 예시에 서는 스마트글래스)는 다국어 발화 음성에서 자동으로 각 언어를 식별한다. 그리고, 상기 단말기(스마트글래 스)는 영상 범위 내 다국어 음성에 대한 목적언어 통역결과를 발화자별로 표시한다. 도 9는 본 발명의 제1 실시예에 따른 시선 기반 증강형 자동통역 시스템의 구성을 나타낸 블록도이다. 본 발명의 제1 실시예에 따른 시선 기반 증강형 자동통역 시스템은 통역 서버와 단말기를 포 함하여 구성된다. 통역 서버는 시선 기반 증강형 자동통역 시스템의 사용자의 정보(사용자 정보)를 등록한다. 상기 사용자 정보는 사용자의 음성 정보, 사용자의 얼굴 정보 및 사용자의 언어를 포함한다. 이 중에서 사용자의 언 어는 통역 시 목적언어가 될 수 있다. 통역 서버는 단말기에서 상기 사용자 정보를 수신하여 이를 등록할 수 있다. 단말기는 통역 서버에서 사용자 정보에 포함된 사용자의 언어를 수신하여 목적 언 어를 설정하거나, 사용자의 직접 입력을 통해 목적 언어를 설정할 수 있다. 단말기는 사용자의 시야에 있 는 멀티모달 정보(영상 정보 및 음성 정보)를 입력받고, 상기 멀티모달 정보를 통역 서버에 전송한다. 통 역 서버는 상기 멀티모달 정보를 기초로 통역 결과를 생성하여 단말기에 전송한다. 단말기는 통역 결과를 디스플레이를 통해 표시하거나 스피커를 통해 재생한다. 통역 서버는 분석 모듈, 사용자 정보 관리 모듈 및 통역 모듈을 포함하며, 사용자 정 보 DB 및 대화 히스토리 DB를 더 포함할 수 있다. 또한 단말기는 입력 모듈 및 출력 모듈을 포함한다. 분석 모듈의 얼굴검출부는 입력 모듈에서 수신한 영상 정보에 포함된 발화자의 얼굴을 검출 하고, 검출된 얼굴 정보를 사용자 정보 관리 모듈에 전달한다. 사용자 정보 관리 모듈은 검출된 얼굴 정보와 사용자 정보 DB에 저장된 얼굴 정보 간의 유사성 분 석을 통하여 검출된 얼굴에 대응하는 사용자(발화자)를 식별한다. 사용자가 단말기를 통해 특별히 변경하 지 않는 한, 상기 영상 정보에서 얼굴검출부에 의해 얼굴이 검출된 발화자가 통역 대상이 된다. 사용자 정보 관리 모듈은 발화자의 식별자를 기초로 발화자의 등록 음성 정보를 사용자 정보 DB에서 추출 하여 음성추출부에 전달한다. 또한, 사용자 정보 관리 모듈은 발화자의 등록 음성 정보를 단말기에 전송할 수도 있는데, 상기 등록 음성 정보는 출력 모듈에서 음성을 합성하는 데 사용될 수 있다. 사용자 정보 관리 모듈은 얼굴이 검출된 발화자가 복수일 경우, 그에 대응하는 복수의 발화자의 등록 음 성 정보를 음성추출부에 전달하는데, 이 경우, 통역 서버는 복수의 발화자의 통역 결과를 병렬적으 로 생성하게 된다. 즉, 통역 서버는 각 발화자에 대하여 병렬적으로 음성 추출, 발화 구간 검출, 통역 작 업을 수행한다. 얼굴검출부는 입력 모듈에서 수신한 영상 정보(단말기에 입력된 영상 정보)에서 검출된 얼굴 영역에 대한 개별 발화자의 영상 정보를 추출한다. 즉, 얼굴검출부는 상기 영상 정보에서 통역 대상이 되 는 발화자의 영상 정보를 추출한다. 얼굴검출부는 개별 발화자의 영상 정보를 통역 모듈에 전달할 수 있다. 음성추출부는 입력 모듈에서 수신한 음성 정보에서 개별 발화자의 음성 정보를 추출하는데, 음성추 출부는 입력 모듈에서 수신한 음성 정보(단말기에 입력된 음성 정보)에서 발화자의 등록 음 성 정보를 기초로 개별 발화자의 음성 정보를 추출할 수 있다. 다른 예로서, 음성추출부는 입력 모듈 에서 수신한 음성 정보(단말기에 입력된 음성 정보)에서 발화자의 영상 정보를 기초로 개별 발화자 의 음성 정보를 추출할 수 있다. 음성추출부는 발화자의 등록 음성 정보와 발화자의 영상 정보를 함께 활 용하여 해당 발화자의 음성 정보를 추출할 수도 있다. 만약 통역모듈이 종단형 통역 엔진을 사용하는 경우, 음성추출부는 개별 발화자의 음성 정보를 통 역 모듈에 전달한다. 음성추출부는 발화자의 음성 정보에서 발화구간을 검출한다. 즉, 음성추출부는 발화자의 음성 정보 에서 발화구간의 음성 정보를 생성한다. 음성추출부는 개별 발화자의 영상 정보를 활용하여 발화의 시작 점 및 종료점 검출의 신뢰성을 높일 수 있다. 음성추출부는 각 발화자의 발화구간의 음성 정보를 통역 모 듈에 전달한다. 분석 모듈은 얼굴검출부 및 음성추출부 외에도 상황판단부와 단어추출부를 더 포함할 수 있다. 상황판단부는 입력 모듈에서 수신한 영상 정보(단말기에 입력된 영상 정 보)를 분석하여 주제 또는 상황을 판단한다. 상황판단부는 상기 영상 정보를 분석하여 주제나 상황에 관 한 임베딩 벡터를 생성할 수 있다. 단어추출부는 상황판단부에서 판단한 주제 또는 상황과 관련된 단어나 어구를 상황-단어 데이터베 이스에서 추출할 수 있다. 이 과정에서 단어추출부는 주제나 상황에 관한 임베딩 벡터를 기초로 관련 단 어/어구를 추출할 수 있다. 또한 단어추출부는 입력 모듈에서 수신한 영상 정보(단말기에 입 력된 영상 정보)에서 텍스트를 인식하고, 상기 텍스트에서 상황과 관련된 단어나 어구를 추출할 수 있다. 통역 모듈은 통역 대상 음성 정보를 기초로 통역 결과를 생성한다. 통역 대상 음성 정보는 발화자의 음성 정보이거나 발화구간의 음성 정보일 수 있다. 통역 결과는 목적 언어로 된 텍스트일 수도 있고, 목적 언어로 합 성된 발화 음성일 수도 있다. 통역 모듈은 발화구간 음성 정보를 기초로 음성인식 모델과 기계번역 모델 을 이용하여 통역 결과를 생성할 수도 있으며(파이프라인형), 발화자의 음성 정보를 기초로 일체화된 통역 모델 을 이용하여 통역 결과를 생성할 수도 있다. 또한, 통역 모듈은 발화자의 영상 정보와 발화자의 통역 대 상 음성 정보를 기초로 일체화된 통역 모델(예를 들어 학습된 딥러닝 모델)을 이용하여 통역 결과를 생성할 수 도 있다. 예를 들어, 통역 모듈은 발화자의 영상 정보와 발화자의 통역 대상 음성 정보를 통역 모델에 입 력하여 그 출력값인 통역 결과를 획득할 수 있다. 다른 예로서, 통역 모듈은 통역 결과와 통역 서버에 등록된 발화자 음성 정보를 기초로 목적 언어 로 이루어진 합성음을 생성할 수 있다. 상기 합성음은 발화자의 등록 음성 정보에 나타난 음색과 유사하게 합성 된 음성일 수 있다. 한편, 분석 모듈에 상황판단부와 단어추출부가 포함된 경우, 통역 모듈은 주제나 상황 에 관련된 정보를, 통역대상 음성 정보를 기초로 통역 결과를 생성하는 과정에 반영할 수 있다. 상기 주제나 상 황에 관련된 정보는, 주제, 상황, 주제에 관련된 임베딩 벡터, 상황에 관련된 임베딩 벡터, 주제에 관련된 단어 /어구 및 상황에 관련된 단어/어구 중 적어도 어느 하나일 수 있다. 통역 모듈은 통역 결과를 단말기의 출력 모듈에 전송하며, 상기 통역 결과를 대화 히스토리 DB에 저장할 수 있다. 단말기의 입력 모듈은 영상입력부와 음성입력부를 포함하는데, 영상입력부는 외 부에서 영상 정보를 수신하여 분석 모듈에 전송하며, 음성입력부는 외부에서 음성 정보를 수신하여 분석 모듈에 전송한다. 단말기의 출력 모듈은 영상증강부, 디스플레이, 음성합성부 및 스피커를 포함한다. 영상증강부는 영상 정보에 통역 결과를 추가하여 디스플레이를 통해 표시한다. 예를 들 어, 영상증강부는 영상 정보에 통역 전후의 텍스트, 즉, 발화자의 언어로 된 텍스트와 목적 언어로 된 텍 스트를 부가한 영상을 디스플레이를 통해 표시한다. 음성합성부는 통역 결과와 발화자의 등록 음성 정보를 기초로 목적 언어로 이루어진 합성음을 생성하여 스피커를 통해 재생한다. 다른 예로서, 만약 통역 모듈에서 통역 결과와 발화자의 등록 음성 정보를 기초로 목적 언어로 이루어진 합성음을 생성한 경우, 통역 모듈은 출력 모듈에 합성음 정보를 전달하고, 음성합성부는 상 기 합성음 정보를 스피커를 통해 재생한다. 도 10은 본 발명의 제2 실시예에 따른 시선 기반 증강형 자동통역 시스템의 구성을 나타낸 블록도이다. 본 발명의 제2 실시예에 따른 시선 기반 증강형 자동통역 시스템은 통역 서버와 단말기를 포 함하여 구성된다. 제2 실시예는 제1 실시예와 비교할 때 단말기에 분석 모듈이 더 포함되는 것을 특징으로 한다. 제2 실시예는 단말기에 내장된 프로세서의 연산 처리 성능이 뛰어난 경우에 구현될 수 있다. 각 모듈의 기본적인 기능은 제1 실시예와 동일하다. 즉, 사용자 정보 관리 모듈은 사용자 정보 관리 모듈 과, 통역 모듈은 통역 모듈과, 입력 모듈은 입력 모듈과, 분석 모듈은 분석 모듈과, 출력 모듈은 출력 모듈과 기능이 동일하다. 따라서 제2 실시예에 관하여는 분 석 모듈과 다른 모듈 간의 통신 내용을 중심으로 하기에 설명한다. 단말기의 입력 모듈은 영상입력부와 음성입력부를 포함하는데, 영상입력부는 영 상 정보를 외부에서 수신하여 분석 모듈에 전달하며, 음성입력부는 음성 정보를 외부에서 수신하여 분석 모듈에 전달한다. 분석 모듈의 얼굴검출부는 입력 모듈에서 수신한 영상 정보(단말기에 입력된 영상 정 보)에 포함된 발화자의 얼굴을 검출하고, 검출된 얼굴 정보를 사용자 정보 관리 모듈에 전송한다. 또한, 얼굴검출부는 입력 모듈에서 수신한 영상 정보(단말기에 입력된 영상 정보)에서 검출된 얼굴 영역에 대한 개별 발화자의 영상 정보를 추출한다. 즉, 얼굴검출부는 상기 영상 정보에서 통역 대상이 되 는 발화자의 영상 정보를 추출한다. 얼굴검출부는 개별 발화자의 영상 정보를 통역 모듈에 전달할 수 있다. 사용자 정보 관리 모듈은 검출된 얼굴 정보와 사용자 정보 DB에 저장된 얼굴 정보 간의 유사성 분 석을 통하여 검출된 얼굴에 대응하는 사용자(발화자)를 식별한다. 사용자가 단말기를 통해 특별히 변경하 지 않는 한, 상기 영상 정보에서 얼굴검출부에 의해 얼굴이 검출된 발화자가 통역 대상이 된다. 사용자 정보 관리 모듈은 발화자의 식별자를 기초로 발화자의 등록 음성 정보를 사용자 정보 DB에서 추출 하여 분석 모듈의 음성추출부에 전송한다. 음성추출부는 입력 모듈에서 수신한 음성 정보에서 개별 발화자의 음성 정보를 추출하는데, 음성추 출부는 입력 모듈에서 수신한 음성 정보(단말기에 입력된 음성 정보)에서 발화자의 등록 음 성 정보를 기초로 개별 발화자의 음성 정보를 추출할 수 있다. 다른 예로서, 음성추출부는 입력 모듈 에서 수신한 음성 정보(단말기에 입력된 음성 정보)에서 발화자의 영상 정보를 기초로 개별 발화자 의 음성 정보를 추출할 수 있다. 음성추출부는 발화자의 등록 음성 정보와 발화자의 영상 정보를 함께 활 용하여 개별 발화자의 음성 정보를 추출할 수도 있다. 만약 통역모듈이 종단형 통역 엔진을 사용하는 경우, 음성추출부는 개별 발화자의 음성 정보를 통 역 모듈에 전달한다. 음성추출부는 개별 발화자의 음성 정보에서 발화구간을 검출한다. 즉, 음성추출부는 개별 발화자의 음성 정보에서 발화구간 음성 정보를 생성한다. 음성추출부는 개별 발화자의 영상 정보를 활용하여 발화의 시작점 및 종료점 검출의 신뢰성을 높일 수 있다. 음성추출부는 각 발화자의 발화구간 음성 정보를 통 역 모듈에 전송한다. 분석 모듈이 상황판단부 및 단어추출부를 더 포함하는 경우, 분석 모듈은 상황판단부 및 단어추출부에서 생성한 주제/상황 관련 정보를 통역 모듈에 전송할 수 있다. 상기 주제 나 상황에 관련된 정보는, 주제, 상황, 주제에 관련된 임베딩 벡터, 상황에 관련된 임베딩 벡터, 주제에 관련된 단어/어구 및 상황에 관련된 단어/어구 중 적어도 어느 하나일 수 있다. 통역 모듈은 통역 대상 음성 정보를 기초로 생성한 통역 결과를 단말기의 출력 모듈에 전송 하며, 상기 통역 결과를 대화 히스토리 DB에 저장할 수 있다. 단말기의 출력 모듈은 통역 결과를 디스플레이나 스피커를 통해 사용자에게 전달한다. 도 11은 본 발명의 제3 실시예에 따른 시선 기반 증강형 자동통역 시스템의 구성을 나타낸 블록도이다. 본 발명의 제3 실시예에 따른 시선 기반 증강형 자동통역 시스템은 통역 서버와 단말기를 포 함하여 구성된다. 제3 실시예는 제2 실시예와 비교할 때 단말기에 통역 모듈이 더 포함되는 것을 특징으로 한다. 제3 실시예는 단말기에서 발화자의 얼굴 검출, 개별 발화자의 음성 정보 추출, 각 발화자의 발화구간 음성 정 보 추출, 통역 작업이 수행되므로, 단말기에 내장된 프로세서의 연산 처리 성능이 매우 뛰어난 경우에 구 현될 수 있다. 각 모듈의 기본적인 기능은 제1 실시예 및 제2 실시예와 동일하다. 즉, 사용자 정보 관리 모듈은 사용자 정보 관리 모듈(1120, 2110)과, 입력 모듈은 입력 모듈(1210, 2210)과, 분석 모듈은 분석 모듈 (1110, 2220)과, 통역 모듈은 통역 모듈(1130, 2120)과, 출력 모듈은 출력 모듈(1220, 2230)과 기 능이 동일하다. 제3 실시예에서 사용자 정보 관리 서버와 단말기 간의 통신은, 분석 모듈이 발화자의 검출된 얼굴 정보를 사용자 정보 관리 모듈에 송신하고, 사용자 정보 관리 모듈이 사용자 정보 DB에 서 추출한 발화자의 등록 음성 정보를 분석 모듈에 송신하는 과정으로 구성된다. 참고로, 본 발명의 실시예에 따른 구성 요소들은 소프트웨어 또는 DSP(digital signal processor), FPGA(Field Programmable Gate Array) 또는 ASIC(Application Specific Integrated Circuit)와 같은 하드웨어 형태로 구현 될 수 있으며, 소정의 역할들을 수행할 수 있다. 그렇지만 '구성 요소들'은 소프트웨어 또는 하드웨어에 한정되는 의미는 아니며, 각 구성 요소는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 구성 요소는 소프트웨어 구성 요소들, 객체지향 소프트웨어 구성 요소들, 클래스 구성 요소 들 및 태스크 구성 요소들과 같은 구성 요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로 그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로 코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테 이블들, 어레이들 및 변수들을 포함한다. 구성 요소들과 해당 구성 요소들 안에서 제공되는 기능은 더 작은 수의 구성 요소들로 결합되거나 추가적인 구 성 요소들로 더 분리될 수 있다. 한편, 흐름도 도면들의 각 블록과 흐름도 도면들의 조합들은 컴퓨터 프로그램 인스트럭션들에 의해 수행될 수 있음을 이해할 수 있을 것이다. 이들 컴퓨터 프로그램 인스트럭션들은 범용 컴퓨터, 특수용 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서에 탑재될 수 있으므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서를 통해 수행되는 그 인스트럭션들이 흐름도 블록(들)에서 설명된 기능들을 수 행하는 수단을 생성하게 된다. 컴퓨터 프로그램 인스트럭션들은 컴퓨터 또는 기타 프로그램 가능한 데이터 프로 세싱 장비 상에 탑재되는 것도 가능하므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에서 일 련의 동작 단계들이 수행되어 컴퓨터로 실행되는 프로세스를 생성해서 컴퓨터 또는 기타 프로그램 가능한 데이 터 프로세싱 장비를 수행하는 인스트럭션들은 흐름도 블록(들)에서 설명된 기능들을 실행하기 위한 단계들을 제 공하는 것도 가능하다. 또한, 각 블록은 특정된 논리적 기능(들)을 실행하기 위한 하나 이상의 실행 가능한 인스트럭션들을 포함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있다. 또, 몇 가지 대체 실행 예들에서는 블록들에서 언급된 기능들이 순서를 벗어나서 발생하는 것도 가능함을 주목해야 한다. 예컨대, 잇달아 도시되어 있는 두 개의 블록들 은 사실 실질적으로 동시에 수행되는 것도 가능하고 또는 그 블록들이 때때로 해당하는 기능에 따라 역순으로 수행되는 것도 가능하다. 본 실시예에서 사용되는 '~부' 또는 '~모듈'이라는 용어는 소프트웨어 또는 FPGA또는 ASIC과 같은 하드웨어 구 성요소를 의미하며, '~부' 또는 '~모듈'는 어떤 역할들을 수행한다. 그렇지만 '~부' 또는 '~모듈'는 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. '~부' 또는 '~모듈'는 어드레싱할 수 있는 저장 매체에 있도록 구성 될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 '~부' 또 는 '~모듈'는 소프트웨어 구성요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소 들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들, 및 변수 들을 포함한다. 구성요소들과 '~부' 또는 '~모듈'들 안에서 제공되는 기능은 더 작은 수의 구성요소들 및 '~부' 또는 '~모듈'들로 결합되거나 추가적인 구성요소들과 '~부' 또는 '~모듈'들로 더 분리될 수 있다. 뿐만 아니라, 구성요소들 및 '~부' 또는 '~모듈'들은 디바이스 또는 보안 멀티미디어카드 내의 하나 또는 그 이상의 CPU들을 재생시키도록 구현될 수도 있다. 상기에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특 허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다."}
{"patent_id": "10-2022-0062775", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 시선 기반 증강형 자동통역 방법을 설명하기 위한 흐름도. 도 2는 본 발명의 일 실시예에 따른 시선 기반 증강형 자동통역 방법을 설명하기 위한 흐름도. 도 3은 본 발명의 일 실시예에 따른 시선 기반 증강형 자동통역 방법을 설명하기 위한 흐름도. 도 4는 본 발명의 실시예에 따른, 영상 기반의 상황 판단 및 단어 추출의 예시 도면. 도 5는 본 발명의 실시예에 따른 자동통역 과정을 나타낸 참고도. 도 6은 본 발명의 실시예에 따라 단일 화자에 대한 통역 결과를 스마트폰으로 표시하는 경우의 예시 도면. 도 7은 본 발명의 실시예에 따라 다중 화자에 대한 통역 결과를 스마트폰으로 표시하는 경우의 예시 도면. 도 8은 본 발명의 실시예에 따라 다국어 발화 상황에서 통역 결과를 스마트글래스로 표시하는 경우의 예시 도면. 도 9는 본 발명의 제1 실시예에 따른 시선 기반 증강형 자동통역 시스템의 구성을 나타낸 블록도. 도 10은 본 발명의 제2 실시예에 따른 시선 기반 증강형 자동통역 시스템의 구성을 나타낸 블록도. 도 11은 본 발명의 제3 실시예에 따른 시선 기반 증강형 자동통역 시스템의 구성을 나타낸 블록도."}
