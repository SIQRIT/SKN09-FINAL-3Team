{"patent_id": "10-2021-7005883", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0028728", "출원번호": "10-2021-7005883", "발명의 명칭": "가상 환경에서 가상 객체를 스케줄링하기 위한 방법, 장치 및 디바이스", "출원인": "텐센트 테크놀로지", "발명자": "추, 푸하오"}}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터 디바이스에 의해 수행되는, 가상 환경에서 가상 객체들을 스케줄링하는 방법- 상기 가상 환경은 적어도2개의 가상 객체와 상기 가상 객체들에 이용가능한 가상 리소스들을 포함함 -으로서,상기 가상 환경의 응용 프로그램이 실행되는 동안 생성된 프레임 데이터를 획득하는 단계;상기 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하는단계- 상기 상태 특징은 연관된 가상 객체들 및 상기 가상 리소스들의 상태들을 포함하고, 상기 연관된 가상 객체들은 상기 대상 가상 객체 및 상기 대상 가상 객체와 이익 관계를 갖는 가상 객체를 포함함 -;N개의 스케줄링 정책에 따라 상기 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하는 단계-N은 2 이상의 양의 정수임 -;가치 네트워크 예측 모델을 호출하여 상기 N개의 후속 상태 특징을 처리함으로써, 상기 대상 가상 객체에 의해상기 N개의 스케줄링 정책을 실행하는 것의 기대 수익을 획득하는 단계; 및상기 N개의 스케줄링 정책의 기대 수익에 따라 상기 N개의 스케줄링 정책으로부터 대상 스케줄링 정책을 선택하고, 상기 대상 가상 객체를 제어하여 상기 대상 스케줄링 정책을 실행하는 단계를 포함하는 방법."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 가상 환경의 맵은 n개의 이동 영역으로 분할되고, n은 상기 맵의 픽셀 값보다 작고, n은 2 이상의 양의 정수이고;N개의 스케줄링 정책에 따라 상기 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하는단계는:상기 대상 가상 객체에 대응하는 m개의 거동을 획득하는 단계- m은 1 이상의 양의 정수임 -;상기 대상 가상 객체에 대응하는 상기 m개의 거동 및 상기 n개의 이동 영역에 따라 상기 N개의 스케줄링 정책을획득하는 단계- 상기 스케줄링 정책들은 상기 n개의 이동 영역 중 i번째 이동 영역으로 이동하여 상기 m개의 거동 중 j번째 거동을 수행하는 것을 포함하고, i와 j는 양의 정수이고, 1≤i≤n, 1≤j≤m임 -; 및상기 N개의 스케줄링 정책을 상태 특징에 적용하고, 미리 설정된 상태 천이 관계에 따라 상기 N개의 후속 상태특징을 획득하는 단계- 상기 상태 천이 관계는 상기 스케줄링 정책들의 적용 이후에 상기 상태 특징의 상태 변경들을 나타내는데 사용됨 -를 포함하는 방법."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 맵은 상기 n개의 이동 영역의 어레이로 균등하게 분할되고; 대안적으로, 상기 맵은 상기 가상 리소스들의위치들에 따라 상기 n개의 이동 영역으로 분할되는 방법."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하는단계는:상기 프레임 데이터에서 상기 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 판독하고, 상기 상태특징을 추출하는 단계를 포함하고;공개특허 10-2021-0028728-3-상기 연관된 가상 객체들의 상태들은 상기 연관된 가상 객체들의 위치들, 체력 포인트들, 공격력 값들, 방어력값들, 경험치들, 살상 카운트들, 사망 카운트들, 또는 경제적 가치들 중 적어도 하나를 포함하고,상기 가상 리소스들의 상태들은 상기 가상 리소스들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 내구성 값들, 또는 점유 상태들 중 적어도 하나를 포함하는 방법."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 적어도 2개의 가상 객체는 각각 적어도 2개의 상호 적대적 캠프에 속하고;상기 가상 리소스는 건물, 중립 생물, 군대, 운송 수단, 무기, 또는 탄약 중 적어도 하나를 포함하고;상기 점유 상태는 중립 상태, 제어된 상태, 점유된 상태, 또는 경쟁 상태를 포함하고;상기 중립 상태는 상기 가상 리소스가 어떠한 캠프에도 속하지 않는다는 것을 나타내는데 사용되고;상기 제어된 상태는 상기 가상 리소스가 위치하는 이동 영역 내에 동일한 캠프에 속하는 적어도 하나의 가상 객체가 존재한다는 것을 나타내는데 사용되고;상기 점유된 상태는 상기 가상 리소스가 속하는 캠프를 나타내는데 사용되고;상기 경쟁 상태는 상기 가상 리소스가 위치하는 이동 영역 내에 상이한 캠프들에 속하는 적어도 2개의 가상 객체가 존재한다는 것을 나타내는데 사용되는 방법."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제5항 중 어느 한 항에 있어서, 상기 가치 네트워크 예측 모델은 샘플 데이터 세트들의 트레이닝에 기초하여 획득되는 기대 수익 규칙(expected-return rule)을 나타내는데 사용되고, 상기 샘플 데이터 세트들은: 샘플 상태 특징들 및 상기 샘플상태 특징들에 대응하는 실제 수익들을 포함하고;가치 네트워크 예측 모델을 호출하여 상기 N개의 후속 상태 특징을 처리하기 전에, 상기 방법은:p개의 샘플 프레임 데이터를 획득하는 단계- p는 2 이상의 양의 정수임 -;상기 p개의 샘플 프레임 데이터에 대해 특징 추출을 수행하여 각각의 샘플 프레임 데이터의 샘플 상태 특징을획득하는 단계- 상기 샘플 상태 특징은 상기 연관된 가상 객체들의 상태들, 또는 상기 연관된 가상 객체들과 상기 가상 리소스들의 상태들을 포함함 -;상기 p개의 샘플 상태 특징 각각의 실제 수익을 계산하는 단계;상기 p개의 샘플 상태 특징을 원래의 가치 네트워크 예측 모델에 입력하여, 상기 각각의 샘플 상태 특징의 트레이닝 결과를 획득하는 단계;상기 샘플 데이터 세트들 각각에 대해, 상기 트레이닝 결과를 상기 샘플 상태 특징의 실제 수익과 비교하여, 계산 손실을 획득하는 단계- 상기 계산 손실은 상기 트레이닝 결과와 상기 샘플 상태 특징의 실제 수익 사이의 오차를 표시하는데 사용됨 -; 및적어도 하나의 샘플 데이터 세트에 대응하는 상기 각각의 계산 손실에 따라 오차 역 전파 알고리즘(error backpropagation algorithm)을 사용하여 트레이닝을 통해 상기 가치 네트워크 예측 모델을 획득하는 단계를 추가로포함하는 방법."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 p개의 샘플 프레임 데이터를 획득하는 단계는:상기 가상 환경의 응용 프로그램이 실행되는 동안 생성된 l개의 이력 프레임 데이터를 획득하는 단계- l은 p 이상의 양의 정수임 -;공개특허 10-2021-0028728-4-미리 설정된 시간 단계에 따라 상기 l개의 이력 프레임 데이터를 샘플링하여, q개의 후보 프레임 데이터를 획득하는 단계- q는 양의 정수이고, p≤q≤l임 -; 및상기 q개의 후보 프레임 데이터를 판독하고, 상기 q개의 후보 프레임 데이터에서, 공격 거동을 포함하는 후보프레임 데이터를 상기 샘플 프레임 데이터라고 판정하여, 상기 p개의 샘플 프레임 데이터를 획득하는 단계를 포함하는 방법."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항 또는 제7항에 있어서, 상기 p개의 샘플 상태 특징 각각의 실제 수익을 계산하는 단계는:상기 샘플 상태 특징에서 상기 대상 가상 객체의 거동 및 상태에 따라 상기 각각의 샘플 상태 특징의 즉각적인보상을 계산하는 단계- 상기 즉각적인 보상은 승리 조건에 대한, 상기 거동에 의해 유발되는 상태의 변화의 기여임 -; 및상기 p개의 샘플 프레임 데이터 중 i번째 샘플 프레임 데이터에 대해, 상기 i번째 샘플 프레임 데이터의 즉각적인 보상 및 (i+1)번째 샘플 프레임 데이터의 즉각적인 보상에 따라 상기 i번째 샘플 프레임 데이터의 실제 수익을 계산하는 단계- i는 p 이하의 양의 정수임 -를 포함하는 방법."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서, 상기 N개의 스케줄링 정책의 기대 수익에 따라 상기 N개의 스케줄링 정책으로부터 대상 스케줄링 정책을 선택하는 것은:상기 N개의 스케줄링 정책에서 기대 수익이 가장 높은 스케줄링 정책을 상기 대상 스케줄링 정책으로서 획득하는 것을 포함하는 방법."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "가상 환경에서 가상 객체들을 스케줄링하기 위한 장치- 상기 가상 환경은 적어도 2개의 가상 객체와 상기 가상객체들에 이용가능한 가상 리소스들을 포함함 -로서,상기 가상 환경의 응용 프로그램이 실행되는 동안 생성된 프레임 데이터를 획득하도록 구성된 데이터 획득모듈;상기 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하도록 구성된 특징 추출 모듈- 상기 상태 특징은 연관된 가상 객체들의 상태들 및 상기 가상 리소스들의 상태들을포함하고, 상기 연관된 가상 객체들은 상기 대상 가상 객체 및 상기 대상 가상 객체와 이익 관계를 갖는 가상객체를 포함함 -;N개의 스케줄링 정책에 따라 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하도록 구성된 상태 추론 모듈- N은 2 이상의 양의 정수임 -;가치 네트워크 예측 모델을 호출하여 상기 N개의 후속 상태 특징을 처리함으로써, 상기 대상 가상 객체에 의해상기 N개의 스케줄링 정책을 실행하는 것의 기대 수익을 획득하도록 구성된 가치 평가 모듈; 및상기 N개의 스케줄링 정책의 기대 수익에 따라 상기 N개의 스케줄링 정책으로부터 대상 스케줄링 정책을 선택하고, 상기 대상 가상 객체를 제어하여 상기 대상 스케줄링 정책을 실행하도록 구성된 제어 모듈을 포함하는장치."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "메모리와 프로세서를 포함하는 컴퓨터 디바이스로서, 상기 메모리는 컴퓨터 판독가능 명령어들을 저장하고, 상기 컴퓨터 판독가능 명령어들은, 상기 프로세서에 의해실행될 때, 상기 프로세서로 하여금:공개특허 10-2021-0028728-5-상기 가상 환경의 응용 프로그램이 실행되는 동안 생성된 프레임 데이터를 획득하는 동작;상기 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하는동작- 상기 상태 특징은 연관된 가상 객체들 및 상기 가상 리소스들의 상태들을 포함하고, 상기 연관된 가상 객체들은 상기 대상 가상 객체 및 상기 대상 가상 객체와 이익 관계를 갖는 가상 객체를 포함함 -;N개의 스케줄링 정책에 따라 상기 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하는 동작-N은 2 이상의 양의 정수임 -;가치 네트워크 예측 모델을 호출하여 상기 N개의 후속 상태 특징을 처리함으로써, 상기 대상 가상 객체에 의해N개의 스케줄링 정책을 실행하는 것의 기대 수익을 획득하는 동작; 및상기 N개의 스케줄링 정책의 기대 수익에 따라 상기 N개의 스케줄링 정책으로부터 대상 스케줄링 정책을 선택하고, 상기 대상 가상 객체를 제어하여 상기 대상 스케줄링 정책을 실행하는 동작을 수행하게 하는 컴퓨터 디바이스."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 가상 환경의 맵은 n개의 이동 영역으로 분할되고, n은 상기 맵의 픽셀 값보다 작고, n은 2 이상의 양의 정수이고;N개의 스케줄링 정책에 따라 상기 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하는동작은:상기 대상 가상 객체에 대응하는 m개의 거동을 획득하는 동작- m은 1 이상의 양의 정수임 -;상기 대상 가상 객체에 대응하는 상기 m개의 거동 및 상기 n개의 이동 영역에 따라 상기 N개의 스케줄링 정책을획득하는 동작- 상기 스케줄링 정책들은 상기 n개의 이동 영역 중 i번째 이동 영역으로 이동하여 상기 m개의 거동 중 j번째 거동을 수행하는 것을 포함하고, i와 j는 양의 정수이고, 1≤i≤n, 1≤j≤m임 -; 및상기 N개의 스케줄링 정책을 상기 상태 특징에 적용하고, 미리 설정된 상태 천이 관계에 따라 상기 N개의 후속상태 특징을 획득하는 동작- 상기 상태 천이 관계는 상기 스케줄링 정책들의 적용 이후에 상기 상태 특징의 상태 변경들을 나타내는데 사용됨 -을 포함하는 컴퓨터 디바이스."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 맵은 상기 n개의 이동 영역의 어레이로 균등하게 분할되고; 대안적으로, 상기 맵은 상기 가상 리소스들의위치들에 따라 상기 n개의 이동 영역으로 분할되는 컴퓨터 디바이스."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하는동작은:상기 프레임 데이터에서 상기 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 판독하고, 상기 상태특징을 추출하는 동작을 포함하고;상기 연관된 가상 객체들의 상태들은 상기 연관된 가상 객체들의 위치들, 체력 포인트들, 공격력 값들, 방어력값들, 경험치들, 살상 카운트들, 사망 카운트들, 또는 경제적 가치들 중 적어도 하나를 포함하고,상기 가상 리소스들의 상태들은 상기 가상 리소스들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 내구성 값들, 또는 점유 상태들 중 적어도 하나를 포함하는 컴퓨터 디바이스."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 공개특허 10-2021-0028728-6-상기 적어도 2개의 가상 객체는 각각 적어도 2개의 상호 적대적 캠프에 속하고;상기 가상 리소스는 건물, 중립 생물, 군대, 운송 수단, 무기, 또는 탄약 중 적어도 하나를 포함하고;상기 점유 상태는 중립 상태, 제어된 상태, 점유된 상태, 또는 경쟁 상태를 포함하고; 상기 중립 상태는 상기가상 리소스가 어떠한 캠프에도 속하지 않는다는 것을 나타내는데 사용되고; 상기 제어된 상태는 상기 가상 리소스가 위치하는 이동 영역 내에 동일한 캠프에 속하는 적어도 하나의 가상 객체가 존재한다는 것을 나타내는데사용되고; 상기 점유된 상태는 상기 가상 리소스가 속하는 캠프를 나타내는데 사용되고; 상기 경쟁 상태는 상기가상 리소스가 위치하는 이동 영역 내에 상이한 캠프들에 속하는 적어도 2개의 가상 객체가 존재한다는 것을 나타내는데 사용되는 컴퓨터 디바이스."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항 내지 제15항 중 어느 한 항에 있어서, 상기 가치 네트워크 예측 모델은 샘플 데이터 세트들의 트레이닝에 기초하여 획득되는 기대 수익 규칙을 나타내는데 사용되고, 상기 샘플 데이터 세트들은: 샘플 상태 특징들 및 상기 샘플 상태 특징들에 대응하는 실제 수익들을 포함하고;가치 네트워크 예측 모델을 호출하여 상기 N개의 후속 상태 특징을 처리하기 전에, 상기 컴퓨터 판독가능 명령어들은 상기 프로세서로 하여금:p개의 샘플 프레임 데이터를 획득하는 동작- p는 2 이상의 양의 정수임 -;상기 p개의 샘플 프레임 데이터에 대해 특징 추출을 수행하여 각각의 샘플 프레임 데이터의 샘플 상태 특징을획득하는 동작- 상기 샘플 상태 특징은 상기 연관된 가상 객체들의 상태들, 또는 상기 연관된 가상 객체들과 상기 가상 리소스들의 상태들을 포함함 -;상기 p개의 샘플 상태 특징 각각의 실제 수익을 계산하는 동작;상기 p개의 샘플 상태 특징을 원래의 가치 네트워크 예측 모델에 입력하여, 상기 각각의 샘플 상태 특징의 트레이닝 결과를 획득하는 동작;상기 샘플 데이터 세트들 각각에 대해, 상기 트레이닝 결과를 상기 샘플 상태 특징의 실제 수익과 비교하여, 계산 손실을 획득하는 동작- 상기 계산 손실은 상기 트레이닝 결과와 상기 샘플 상태 특징의 실제 수익 사이의 오차를 표시하는데 사용됨 -; 및적어도 하나의 샘플 데이터 세트에 대응하는 각각의 계산 손실에 따라 오차 역 전파 알고리즘을 사용하여 트레이닝을 통해 상기 가치 네트워크 예측 모델을 획득하는 동작을 추가로 수행하게 하는 컴퓨터 디바이스."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서, 상기 p개의 샘플 프레임 데이터를 획득하는 동작은:상기 가상 환경의 응용 프로그램이 실행되는 동안 생성된 l개의 이력 프레임 데이터를 획득하는 동작- l은 p 이상의 양의 정수임 -;미리 설정된 시간 단계에 따라 상기 l개의 이력 프레임 데이터를 샘플링하여, q개의 후보 프레임 데이터를 획득하는 동작- q는 양의 정수이고, p≤q≤l임 -; 및상기 q개의 후보 프레임 데이터를 판독하고, 상기 q개의 후보 프레임 데이터에서, 공격 거동을 포함하는 후보프레임 데이터를 상기 샘플 프레임 데이터라고 판정하여, 상기 p개의 샘플 프레임 데이터를 획득하는 동작을 포함하는 컴퓨터 디바이스."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항 또는 제17항에 있어서, 상기 p개의 샘플 상태 특징 각각의 실제 수익을 계산하는 동작은:상기 샘플 상태 특징에서 상기 대상 가상 객체의 거동 및 상태에 따라 상기 각각의 샘플 상태 특징의 즉각적인공개특허 10-2021-0028728-7-보상을 계산하는 동작- 상기 즉각적인 보상은 승리 조건에 대한, 상기 거동에 의해 유발되는 상태의 변화의 기여임 -; 및상기 p개의 샘플 프레임 데이터 중 i번째 샘플 프레임 데이터에 대해, 상기 i번째 샘플 프레임 데이터의 즉각적인 보상 및 (i+1)번째 샘플 프레임 데이터의 즉각적인 보상에 따라 상기 i번째 샘플 프레임 데이터의 실제 수익을 계산하는 동작- i는 p 이하의 양의 정수임 -을 포함하는 컴퓨터 디바이스."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항에 있어서, 상기 N개의 스케줄링 정책의 기대 수익에 따라 상기 N개의 스케줄링 정책으로부터 대상 스케줄링 정책을 선택하는 것은:상기 N개의 스케줄링 정책에서 기대 수익이 가장 높은 스케줄링 정책을 상기 대상 스케줄링 정책으로서 획득하는 것을 포함하는 컴퓨터 디바이스."}
{"patent_id": "10-2021-7005883", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "컴퓨터 판독가능 명령어들을 저장하는 하나 이상의 비휘발성 저장 매체로서, 상기 컴퓨터 판독가능 명령어들은, 하나 이상의 프로세서에 의해 실행될 때, 상기 하나 이상의 프로세서로 하여금 제1항 내지 제9항 중 어느 한 항에 따른 방법을 수행하게 하는 하나 이상의 비휘발성 저장 매체."}
{"patent_id": "10-2021-7005883", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "가상 환경에서 가상 객체를 스케줄링하고, 인공 지능에 관련되며, 컴퓨터들의"}
{"patent_id": "10-2021-7005883", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 속하는 방법, 장치, 및 디바이스가 개시된다. 본 방법은: 동작 동안 가상 환경의 응용 프로그램에 의해 생성되는 프레임 데이터를 취득하는 단계; 프레임 데이터에 대해 특징 추출을 수행하여 대상 가상 객체의 상태 특징들을 획득하는 단계; 상 태 특징들을 추론하여 N개의 타입의 후속 상태 특징을 획득하는 단계; 가치 네트워크 예측 모델을 호출하여 N개 의 타입의 후속 상태 특징을 처리함으로써, N개의 타입의 스케줄링 정책의 실행의 기대 수익을 획득하는 단계; 및 대상 가상 객체를 제어하여 기대 수익이 가장 높은 스케줄링 정책을 실행하는 단계를 포함한다. 대 표 도 - 도2"}
{"patent_id": "10-2021-7005883", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2021-0028728 발명자 취, 펑 중국 518057 광둥 선전 난산 디스트릭트 미드웨스 트 디스트릭트 오브 하이테크 파크 커지중이 로드 텐센트 빌딩 35층 리, 샤오첸 중국 518057 광둥 선전 난산 디스트릭트 미드웨스 트 디스트릭트 오브 하이테크 파크 커지중이 로드 텐센트 빌딩 35층량, 징 중국 518057 광둥 선전 난산 디스트릭트 미드웨스 트 디스트릭트 오브 하이테크 파크 커지중이 로드 텐센트 빌딩 35층명 세 서 청구범위 청구항 1 컴퓨터 디바이스에 의해 수행되는, 가상 환경에서 가상 객체들을 스케줄링하는 방법- 상기 가상 환경은 적어도 2개의 가상 객체와 상기 가상 객체들에 이용가능한 가상 리소스들을 포함함 -으로서, 상기 가상 환경의 응용 프로그램이 실행되는 동안 생성된 프레임 데이터를 획득하는 단계; 상기 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하는 단계- 상기 상태 특징은 연관된 가상 객체들 및 상기 가상 리소스들의 상태들을 포함하고, 상기 연관된 가상 객 체들은 상기 대상 가상 객체 및 상기 대상 가상 객체와 이익 관계를 갖는 가상 객체를 포함함 -; N개의 스케줄링 정책에 따라 상기 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하는 단계- N은 2 이상의 양의 정수임 -; 가치 네트워크 예측 모델을 호출하여 상기 N개의 후속 상태 특징을 처리함으로써, 상기 대상 가상 객체에 의해 상기 N개의 스케줄링 정책을 실행하는 것의 기대 수익을 획득하는 단계; 및 상기 N개의 스케줄링 정책의 기대 수익에 따라 상기 N개의 스케줄링 정책으로부터 대상 스케줄링 정책을 선택하 고, 상기 대상 가상 객체를 제어하여 상기 대상 스케줄링 정책을 실행하는 단계를 포함하는 방법. 청구항 2 제1항에 있어서, 상기 가상 환경의 맵은 n개의 이동 영역으로 분할되고, n은 상기 맵의 픽셀 값보다 작고, n은 2 이상의 양의 정 수이고; N개의 스케줄링 정책에 따라 상기 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하는 단계는: 상기 대상 가상 객체에 대응하는 m개의 거동을 획득하는 단계- m은 1 이상의 양의 정수임 -; 상기 대상 가상 객체에 대응하는 상기 m개의 거동 및 상기 n개의 이동 영역에 따라 상기 N개의 스케줄링 정책을 획득하는 단계- 상기 스케줄링 정책들은 상기 n개의 이동 영역 중 i번째 이동 영역으로 이동하여 상기 m개의 거 동 중 j번째 거동을 수행하는 것을 포함하고, i와 j는 양의 정수이고, 1≤i≤n, 1≤j≤m임 -; 및 상기 N개의 스케줄링 정책을 상태 특징에 적용하고, 미리 설정된 상태 천이 관계에 따라 상기 N개의 후속 상태 특징을 획득하는 단계- 상기 상태 천이 관계는 상기 스케줄링 정책들의 적용 이후에 상기 상태 특징의 상태 변 경들을 나타내는데 사용됨 -를 포함하는 방법. 청구항 3 제2항에 있어서, 상기 맵은 상기 n개의 이동 영역의 어레이로 균등하게 분할되고; 대안적으로, 상기 맵은 상기 가상 리소스들의 위치들에 따라 상기 n개의 이동 영역으로 분할되는 방법. 청구항 4 제3항에 있어서, 상기 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하는 단계는: 상기 프레임 데이터에서 상기 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 판독하고, 상기 상태 특징을 추출하는 단계를 포함하고;상기 연관된 가상 객체들의 상태들은 상기 연관된 가상 객체들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 경험치들, 살상 카운트들, 사망 카운트들, 또는 경제적 가치들 중 적어도 하나를 포함하고, 상기 가상 리소스들의 상태들은 상기 가상 리소스들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 내구 성 값들, 또는 점유 상태들 중 적어도 하나를 포함하는 방법. 청구항 5 제4항에 있어서, 상기 적어도 2개의 가상 객체는 각각 적어도 2개의 상호 적대적 캠프에 속하고; 상기 가상 리소스는 건물, 중립 생물, 군대, 운송 수단, 무기, 또는 탄약 중 적어도 하나를 포함하고; 상기 점유 상태는 중립 상태, 제어된 상태, 점유된 상태, 또는 경쟁 상태를 포함하고; 상기 중립 상태는 상기 가상 리소스가 어떠한 캠프에도 속하지 않는다는 것을 나타내는데 사용되고; 상기 제어된 상태는 상기 가상 리소스가 위치하는 이동 영역 내에 동일한 캠프에 속하는 적어도 하나의 가상 객 체가 존재한다는 것을 나타내는데 사용되고; 상기 점유된 상태는 상기 가상 리소스가 속하는 캠프를 나타내는데 사용되고; 상기 경쟁 상태는 상기 가상 리소스가 위치하는 이동 영역 내에 상이한 캠프들에 속하는 적어도 2개의 가상 객 체가 존재한다는 것을 나타내는데 사용되는 방법. 청구항 6 제1항 내지 제5항 중 어느 한 항에 있어서, 상기 가치 네트워크 예측 모델은 샘플 데이터 세트들의 트레이닝에 기초하여 획득되는 기대 수익 규칙 (expected-return rule)을 나타내는데 사용되고, 상기 샘플 데이터 세트들은: 샘플 상태 특징들 및 상기 샘플 상태 특징들에 대응하는 실제 수익들을 포함하고; 가치 네트워크 예측 모델을 호출하여 상기 N개의 후속 상태 특징을 처리하기 전에, 상기 방법은: p개의 샘플 프레임 데이터를 획득하는 단계- p는 2 이상의 양의 정수임 -; 상기 p개의 샘플 프레임 데이터에 대해 특징 추출을 수행하여 각각의 샘플 프레임 데이터의 샘플 상태 특징을 획득하는 단계- 상기 샘플 상태 특징은 상기 연관된 가상 객체들의 상태들, 또는 상기 연관된 가상 객체들과 상 기 가상 리소스들의 상태들을 포함함 -; 상기 p개의 샘플 상태 특징 각각의 실제 수익을 계산하는 단계; 상기 p개의 샘플 상태 특징을 원래의 가치 네트워크 예측 모델에 입력하여, 상기 각각의 샘플 상태 특징의 트레 이닝 결과를 획득하는 단계; 상기 샘플 데이터 세트들 각각에 대해, 상기 트레이닝 결과를 상기 샘플 상태 특징의 실제 수익과 비교하여, 계 산 손실을 획득하는 단계- 상기 계산 손실은 상기 트레이닝 결과와 상기 샘플 상태 특징의 실제 수익 사이의 오 차를 표시하는데 사용됨 -; 및 적어도 하나의 샘플 데이터 세트에 대응하는 상기 각각의 계산 손실에 따라 오차 역 전파 알고리즘(error back propagation algorithm)을 사용하여 트레이닝을 통해 상기 가치 네트워크 예측 모델을 획득하는 단계를 추가로 포함하는 방법. 청구항 7 제6항에 있어서, 상기 p개의 샘플 프레임 데이터를 획득하는 단계는: 상기 가상 환경의 응용 프로그램이 실행되는 동안 생성된 l개의 이력 프레임 데이터를 획득하는 단계- l은 p 이 상의 양의 정수임 -;미리 설정된 시간 단계에 따라 상기 l개의 이력 프레임 데이터를 샘플링하여, q개의 후보 프레임 데이터를 획득 하는 단계- q는 양의 정수이고, p≤q≤l임 -; 및 상기 q개의 후보 프레임 데이터를 판독하고, 상기 q개의 후보 프레임 데이터에서, 공격 거동을 포함하는 후보 프레임 데이터를 상기 샘플 프레임 데이터라고 판정하여, 상기 p개의 샘플 프레임 데이터를 획득하는 단계를 포 함하는 방법. 청구항 8 제6항 또는 제7항에 있어서, 상기 p개의 샘플 상태 특징 각각의 실제 수익을 계산하는 단계는: 상기 샘플 상태 특징에서 상기 대상 가상 객체의 거동 및 상태에 따라 상기 각각의 샘플 상태 특징의 즉각적인 보상을 계산하는 단계- 상기 즉각적인 보상은 승리 조건에 대한, 상기 거동에 의해 유발되는 상태의 변화의 기 여임 -; 및 상기 p개의 샘플 프레임 데이터 중 i번째 샘플 프레임 데이터에 대해, 상기 i번째 샘플 프레임 데이터의 즉각적 인 보상 및 (i+1)번째 샘플 프레임 데이터의 즉각적인 보상에 따라 상기 i번째 샘플 프레임 데이터의 실제 수익 을 계산하는 단계- i는 p 이하의 양의 정수임 -를 포함하는 방법. 청구항 9 제1항에 있어서, 상기 N개의 스케줄링 정책의 기대 수익에 따라 상기 N개의 스케줄링 정책으로부터 대상 스케줄링 정책을 선택하 는 것은: 상기 N개의 스케줄링 정책에서 기대 수익이 가장 높은 스케줄링 정책을 상기 대상 스케줄링 정책으로서 획득하 는 것을 포함하는 방법. 청구항 10 가상 환경에서 가상 객체들을 스케줄링하기 위한 장치- 상기 가상 환경은 적어도 2개의 가상 객체와 상기 가상 객체들에 이용가능한 가상 리소스들을 포함함 -로서, 상기 가상 환경의 응용 프로그램이 실행되는 동안 생성된 프레임 데이터를 획득하도록 구성된 데이터 획득 모듈; 상기 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하도 록 구성된 특징 추출 모듈- 상기 상태 특징은 연관된 가상 객체들의 상태들 및 상기 가상 리소스들의 상태들을 포함하고, 상기 연관된 가상 객체들은 상기 대상 가상 객체 및 상기 대상 가상 객체와 이익 관계를 갖는 가상 객체를 포함함 -; N개의 스케줄링 정책에 따라 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하도록 구성된 상 태 추론 모듈- N은 2 이상의 양의 정수임 -; 가치 네트워크 예측 모델을 호출하여 상기 N개의 후속 상태 특징을 처리함으로써, 상기 대상 가상 객체에 의해 상기 N개의 스케줄링 정책을 실행하는 것의 기대 수익을 획득하도록 구성된 가치 평가 모듈; 및 상기 N개의 스케줄링 정책의 기대 수익에 따라 상기 N개의 스케줄링 정책으로부터 대상 스케줄링 정책을 선택하 고, 상기 대상 가상 객체를 제어하여 상기 대상 스케줄링 정책을 실행하도록 구성된 제어 모듈을 포함하는 장치. 청구항 11 메모리와 프로세서를 포함하는 컴퓨터 디바이스로서, 상기 메모리는 컴퓨터 판독가능 명령어들을 저장하고, 상기 컴퓨터 판독가능 명령어들은, 상기 프로세서에 의해 실행될 때, 상기 프로세서로 하여금:상기 가상 환경의 응용 프로그램이 실행되는 동안 생성된 프레임 데이터를 획득하는 동작; 상기 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하는 동작- 상기 상태 특징은 연관된 가상 객체들 및 상기 가상 리소스들의 상태들을 포함하고, 상기 연관된 가상 객 체들은 상기 대상 가상 객체 및 상기 대상 가상 객체와 이익 관계를 갖는 가상 객체를 포함함 -; N개의 스케줄링 정책에 따라 상기 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하는 동작- N은 2 이상의 양의 정수임 -; 가치 네트워크 예측 모델을 호출하여 상기 N개의 후속 상태 특징을 처리함으로써, 상기 대상 가상 객체에 의해 N개의 스케줄링 정책을 실행하는 것의 기대 수익을 획득하는 동작; 및 상기 N개의 스케줄링 정책의 기대 수익에 따라 상기 N개의 스케줄링 정책으로부터 대상 스케줄링 정책을 선택하 고, 상기 대상 가상 객체를 제어하여 상기 대상 스케줄링 정책을 실행하는 동작을 수행하게 하는 컴퓨터 디바이 스. 청구항 12 제11항에 있어서, 상기 가상 환경의 맵은 n개의 이동 영역으로 분할되고, n은 상기 맵의 픽셀 값보다 작고, n은 2 이상의 양의 정 수이고; N개의 스케줄링 정책에 따라 상기 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하는 동작은: 상기 대상 가상 객체에 대응하는 m개의 거동을 획득하는 동작- m은 1 이상의 양의 정수임 -; 상기 대상 가상 객체에 대응하는 상기 m개의 거동 및 상기 n개의 이동 영역에 따라 상기 N개의 스케줄링 정책을 획득하는 동작- 상기 스케줄링 정책들은 상기 n개의 이동 영역 중 i번째 이동 영역으로 이동하여 상기 m개의 거 동 중 j번째 거동을 수행하는 것을 포함하고, i와 j는 양의 정수이고, 1≤i≤n, 1≤j≤m임 -; 및 상기 N개의 스케줄링 정책을 상기 상태 특징에 적용하고, 미리 설정된 상태 천이 관계에 따라 상기 N개의 후속 상태 특징을 획득하는 동작- 상기 상태 천이 관계는 상기 스케줄링 정책들의 적용 이후에 상기 상태 특징의 상 태 변경들을 나타내는데 사용됨 -을 포함하는 컴퓨터 디바이스. 청구항 13 제12항에 있어서, 상기 맵은 상기 n개의 이동 영역의 어레이로 균등하게 분할되고; 대안적으로, 상기 맵은 상기 가상 리소스들의 위치들에 따라 상기 n개의 이동 영역으로 분할되는 컴퓨터 디바이스. 청구항 14 제13항에 있어서, 상기 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하는 동작은: 상기 프레임 데이터에서 상기 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 판독하고, 상기 상태 특징을 추출하는 동작을 포함하고; 상기 연관된 가상 객체들의 상태들은 상기 연관된 가상 객체들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 경험치들, 살상 카운트들, 사망 카운트들, 또는 경제적 가치들 중 적어도 하나를 포함하고, 상기 가상 리소스들의 상태들은 상기 가상 리소스들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 내구 성 값들, 또는 점유 상태들 중 적어도 하나를 포함하는 컴퓨터 디바이스. 청구항 15 제14항에 있어서, 상기 적어도 2개의 가상 객체는 각각 적어도 2개의 상호 적대적 캠프에 속하고; 상기 가상 리소스는 건물, 중립 생물, 군대, 운송 수단, 무기, 또는 탄약 중 적어도 하나를 포함하고; 상기 점유 상태는 중립 상태, 제어된 상태, 점유된 상태, 또는 경쟁 상태를 포함하고; 상기 중립 상태는 상기 가상 리소스가 어떠한 캠프에도 속하지 않는다는 것을 나타내는데 사용되고; 상기 제어된 상태는 상기 가상 리 소스가 위치하는 이동 영역 내에 동일한 캠프에 속하는 적어도 하나의 가상 객체가 존재한다는 것을 나타내는데 사용되고; 상기 점유된 상태는 상기 가상 리소스가 속하는 캠프를 나타내는데 사용되고; 상기 경쟁 상태는 상기 가상 리소스가 위치하는 이동 영역 내에 상이한 캠프들에 속하는 적어도 2개의 가상 객체가 존재한다는 것을 나 타내는데 사용되는 컴퓨터 디바이스. 청구항 16 제11항 내지 제15항 중 어느 한 항에 있어서, 상기 가치 네트워크 예측 모델은 샘플 데이터 세트들의 트레이닝에 기초하여 획득되는 기대 수익 규칙을 나타내 는데 사용되고, 상기 샘플 데이터 세트들은: 샘플 상태 특징들 및 상기 샘플 상태 특징들에 대응하는 실제 수익 들을 포함하고; 가치 네트워크 예측 모델을 호출하여 상기 N개의 후속 상태 특징을 처리하기 전에, 상기 컴퓨터 판독가능 명령 어들은 상기 프로세서로 하여금: p개의 샘플 프레임 데이터를 획득하는 동작- p는 2 이상의 양의 정수임 -; 상기 p개의 샘플 프레임 데이터에 대해 특징 추출을 수행하여 각각의 샘플 프레임 데이터의 샘플 상태 특징을 획득하는 동작- 상기 샘플 상태 특징은 상기 연관된 가상 객체들의 상태들, 또는 상기 연관된 가상 객체들과 상 기 가상 리소스들의 상태들을 포함함 -; 상기 p개의 샘플 상태 특징 각각의 실제 수익을 계산하는 동작; 상기 p개의 샘플 상태 특징을 원래의 가치 네트워크 예측 모델에 입력하여, 상기 각각의 샘플 상태 특징의 트레 이닝 결과를 획득하는 동작; 상기 샘플 데이터 세트들 각각에 대해, 상기 트레이닝 결과를 상기 샘플 상태 특징의 실제 수익과 비교하여, 계 산 손실을 획득하는 동작- 상기 계산 손실은 상기 트레이닝 결과와 상기 샘플 상태 특징의 실제 수익 사이의 오 차를 표시하는데 사용됨 -; 및 적어도 하나의 샘플 데이터 세트에 대응하는 각각의 계산 손실에 따라 오차 역 전파 알고리즘을 사용하여 트레 이닝을 통해 상기 가치 네트워크 예측 모델을 획득하는 동작을 추가로 수행하게 하는 컴퓨터 디바이스. 청구항 17 제16항에 있어서, 상기 p개의 샘플 프레임 데이터를 획득하는 동작은: 상기 가상 환경의 응용 프로그램이 실행되는 동안 생성된 l개의 이력 프레임 데이터를 획득하는 동작- l은 p 이 상의 양의 정수임 -; 미리 설정된 시간 단계에 따라 상기 l개의 이력 프레임 데이터를 샘플링하여, q개의 후보 프레임 데이터를 획득 하는 동작- q는 양의 정수이고, p≤q≤l임 -; 및 상기 q개의 후보 프레임 데이터를 판독하고, 상기 q개의 후보 프레임 데이터에서, 공격 거동을 포함하는 후보 프레임 데이터를 상기 샘플 프레임 데이터라고 판정하여, 상기 p개의 샘플 프레임 데이터를 획득하는 동작을 포 함하는 컴퓨터 디바이스. 청구항 18 제16항 또는 제17항에 있어서, 상기 p개의 샘플 상태 특징 각각의 실제 수익을 계산하는 동작은: 상기 샘플 상태 특징에서 상기 대상 가상 객체의 거동 및 상태에 따라 상기 각각의 샘플 상태 특징의 즉각적인보상을 계산하는 동작- 상기 즉각적인 보상은 승리 조건에 대한, 상기 거동에 의해 유발되는 상태의 변화의 기 여임 -; 및 상기 p개의 샘플 프레임 데이터 중 i번째 샘플 프레임 데이터에 대해, 상기 i번째 샘플 프레임 데이터의 즉각적 인 보상 및 (i+1)번째 샘플 프레임 데이터의 즉각적인 보상에 따라 상기 i번째 샘플 프레임 데이터의 실제 수익 을 계산하는 동작- i는 p 이하의 양의 정수임 -을 포함하는 컴퓨터 디바이스. 청구항 19 제11항에 있어서, 상기 N개의 스케줄링 정책의 기대 수익에 따라 상기 N개의 스케줄링 정책으로부터 대상 스케줄링 정책을 선택하 는 것은: 상기 N개의 스케줄링 정책에서 기대 수익이 가장 높은 스케줄링 정책을 상기 대상 스케줄링 정책으로서 획득하 는 것을 포함하는 컴퓨터 디바이스. 청구항 20 컴퓨터 판독가능 명령어들을 저장하는 하나 이상의 비휘발성 저장 매체로서, 상기 컴퓨터 판독가능 명령어들은, 하나 이상의 프로세서에 의해 실행될 때, 상기 하나 이상의 프로세서로 하여 금 제1항 내지 제9항 중 어느 한 항에 따른 방법을 수행하게 하는 하나 이상의 비휘발성 저장 매체. 발명의 설명 기 술 분 야 본 출원은 2018년 11월 21일자로 중국 특허청에 출원되고 발명의 명칭이 \"METHOD, APPARATUS, AND DEVICE FOR SCHEDULING VIRTUAL OBJECT IN VIRTUAL ENVIRONMENT\"인 중국 특허 출원 제201811393388.0호에 대한 우선권을 주장하며, 그 개시내용은 그 전체가 본 명세서에 참고로 포함된다. 본 출원은 컴퓨터 기술 분야에 관한 것으로, 특히, 가상 환경에서 가상 객체들을 스케줄링하기 위한 방법, 장치, 및 디바이스에 관한 것이다."}
{"patent_id": "10-2021-7005883", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 지능(AI)은 인간 사고 방식으로 반응하고 대응하는 거동을 실행하는 컴퓨터 프로그램 시스템이다. AI는 이미지 인식, 음성 인식, 자연어 처리, 및 전문가 시스템과 같은 분야에 널리 적용된다. 전문가 시스템은 도메 인 문제를 해결하기 위해 인간 전문가를 시뮬레이션하는 AI, 예를 들어, 바둑을 플레이하기 위해 바둑 플레이어 를 시뮬레이션하는 AI 알파고(Alphago)이다. 스마트폰 및 태블릿 컴퓨터와 같은 단말기들에는, MOBA(multiplayer online battle arena) 게임 및 SLG(simulation game)와 같은 2차원 또는 3차원 가상 환경을 갖춘 많은 응용 프로그램들이 있다. 전술한 응용 프로그램에서, 사용자는 가상 객체(예를 들어, 가상 캐릭터)를 제어하여 승리하기 위해 스케줄링 정책을 실행할 수 있다. 사용자가 어떤 이유로 가상 객체를 제어할 수 없을 때, 사용자는 AI에 의존하여 대응하는 스케줄링 정책을 실행하는 것을 지원할 필요가 있다. 가상 환경에서의 상황 상태 공간은 바둑의 것보다 훨씬 더 크고, 가상 객체들의 실행가능한 거동들은 연속적이 고 다양하다. 예를 들어, MOBA 게임의 맵 크기는 60개보다 많은 상이한 타입의 동작가능한 단위들을 포함하는, 약 50,000×50,000 픽셀들이다. 각각의 동작가능한 단위는 연속성 및 다양성을 갖는다. 따라서, AI가 가상 환 경의 상태를 도출하기 어렵기 때문에, 스케줄링 정책을 결정할 때 AI의 정확도가 비교적 낮아진다."}
{"patent_id": "10-2021-7005883", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 출원에서 제공되는 다양한 실시예들에 따르면, 가상 환경에서 가상 객체들을 스케줄링하기 위한 방법, 장치, 및 디바이스가 제공된다. 일 양태에 따르면, 본 출원의 실시예는 컴퓨터 디바이스에 의해 수행되는, 가상 환경에서 가상 객체들을 스케줄 링하기 위한 방법을 제공하며, 가상 환경은 적어도 2개의 가상 객체와 가상 객체들에 이용가능한 가상 리소스들 을 포함한다. 본 방법은: 실행 동안 가상 환경의 응용 프로그램에 의해 생성되는 프레임 데이터를 획득하는 단계; 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하는 단계- 상태 특징은 연관된 가상 객체들 및 가상 리소스들의 상태들을 포함하고, 연관된 가상 객체들은 대상 가상 객체 및 대상 가상 객체와 이익 관계를 갖는 가상 객체를 포함함 -; N개의 스케줄링 정책에 따라 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하는 단계- N은 2 이상의 양의 정수임 -; 가치 네트워크 예측 모델을 호출하여 N개의 후속 상태 특징을 처리함으로써, 대상 가상 객체에 의해 N개의 스케 줄링 정책을 실행하는 것의 기대 수익을 획득하는 단계; 및 대상 가상 객체를 제어하여 N개의 스케줄링 정책에서 기대 수익이 가장 높은 스케줄링 정책을 실행하는 단계를 포함한다. 일 양태에 따르면, 본 출원의 실시예는 가상 환경에서 가상 객체들을 스케줄링하기 위한 장치를 제공하며, 가상 환경은 적어도 2개의 가상 객체와 가상 객체들에 이용가능한 가상 리소스들을 포함한다. 본 장치는: 실행 동안 가상 환경의 응용 프로그램에 의해 생성되는 프레임 데이터를 획득하도록 구성된 데이터 획득 모듈; 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득하도록 구 성된 특징 추출 모듈- 상태 특징은 연관된 가상 객체들 및 가상 리소스들의 상태들을 포함하고, 연관된 가상 객 체들은 대상 가상 객체 및 대상 가상 객체와 이익 관계를 갖는 가상 객체를 포함함 -; N개의 스케줄링 정책에 따라 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하도록 구성된 상 태 추론 모듈- N은 2 이상의 양의 정수임 -; 가치 네트워크 예측 모델을 호출하여 N개의 후속 상태 특징을 처리함으로써, 대상 가상 객체에 의해 N개의 스케 줄링 정책을 실행하는 것의 기대 수익을 획득하도록 구성된 가치 평가 모듈; 및 대상 가상 객체를 제어하여 N개의 스케줄링 정책에서 기대 수익이 가장 높은 스케줄링 정책을 실행하도록 구성 된 제어 모듈을 포함한다. 일 양태에 따르면, 본 출원의 실시예는 메모리와 프로세서를 포함하는 컴퓨터 디바이스를 제공하고, 메모리는 컴퓨터 판독가능 명령어들을 저장하고, 컴퓨터 판독가능 명령어들은, 프로세서에 의해 실행될 때, 프로세서로 하여금 가상 환경에서 가상 객체들을 스케줄링하기 위한 방법을 수행하게 한다. 일 양태에 따르면, 본 출원의 실시예는 컴퓨터 판독가능 명령어들을 저장하는 하나 이상의 비휘발성 저장 매체 를 제공하고, 컴퓨터 판독가능 명령어들은, 하나 이상의 프로세서에 의해 실행될 때, 하나 이상의 프로세서로 하여금 가상 환경에서 가상 객체들을 스케줄링하기 위한 방법을 수행하게 한다. 본 출원의 하나 이상의 실시예의 세부사항들은 첨부 도면들과 아래의 설명에서 제공된다. 본 출원의 다른 특징 들, 목적들, 및 이점들은 명세서, 첨부 도면들, 및 청구항들로부터 명백해질 것이다."}
{"patent_id": "10-2021-7005883", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 출원의 목적들, 기술적 해결책들, 및 장점들을 보다 명확하게 하기 위해, 본 출원의 구현들이 첨부 도면들을 참조하여 이하에서 상세히 추가로 설명된다. 먼저, 본 출원의 실시예들에 관련된 용어들이 소개된다: 가상 환경: 이것은 단말기 상에서 실행될 때 응용 프로그램에 의해 표시(또는 제공) 되는 가상 환경이다. 가상 환경은 현실 세계의 시뮬레이션된 환경일 수 있거나, 반-시뮬레이션된 반-가상 3차원 환경일 수 있거나, 또는 완전히 가상의 3차원 환경일 수 있다. 가상 환경은 2차원 가상 환경, 2.5차원 가상 환경, 및 3차원 가상 환경 중 어느 하나일 수 있다. 일부 실시예에서, 가상 환경은 또한 적어도 2개의 가상 역할 사이의 가상 환경 전투 를 위해 사용되고, 가상 환경에서 적어도 2개의 가상 역할에 이용할 수 있는 가상 리소스가 있다. 선택적으로, 가상 환경의 맵은 정사각형 또는 직사각형이고, 정사각형 또는 직사각형은 대칭인 좌측 하부 대각선 영역 및 우 측 상부 대각선 영역을 포함하고; 가상 환경에서 전투의 승리 조건은 적 캠프의 목표 요새들을 점유하거나 파괴 하는 것을 포함한다. 목표 요새들은 적 캠프의 모든 요새, 또는 적 캠프의 요새들 중 일부(예를 들어, 주기지 및 방어 타워)일 수 있다. 가상 객체: 가상 환경에서 이동가능한 객체를 지칭한다. 이동가능한 객체는 가상 캐릭터, 가상 동물, 및 만화 캐릭터 중 적어도 하나일 수 있다. 일부 실시예에서, 가상 환경이 3차원 가상 환경일 때, 가상 객체들은 3차원 모델들이다. 각각의 가상 객체는 3차원 가상 환경에서 형상 및 체적을 가지며, 3차원 가상 환경에서 일부 공간 을 차지한다. 선택적으로, 가상 객체는 MOBA 게임에서의 영웅, 군인, 또는 중립 생물(neutral creature)일 수 있다. 본 출원의 실시예들에서, 대상 가상 객체가 영웅인 예를 사용하여 설명된다. MOBA 게임: 이는 가상 환경에서 여러 개의 요새들이 제공되고, 사용자들이 상이한 캠프들에 속한 가상 객체들을 제어하여 가상 환경에서 전투하고, 요새들을 점령하거나 적 캠프 요새들을 파괴하는 게임이다. 예를 들어, MOBA 게임에서, 가상 객체들은 2개의 적대적 캠프(hostile camp)로 분할될 수 있고, 가상 환경에서 흩어져서 서 로 경쟁하며, 모든 적의 요새들을 파괴 또는 점령하는 것은 승리 조건으로서 사용된다. MOBA 게임은 라운드를 단위로서 사용한다. MOBA 게임의 라운드의 지속 기간은 게임이 시작하는 시점으로부터 승리 조건이 충족되는 시점까지이다. 스케줄링 정책: 이것은 가상 객체가 대상 영역으로 이동하고 대상 영역에서 대응하는 작업을 실행하는 것을 결 정하는 거동이다. 예를 들어, 더 나은 보상을 획득하기 위해, 적 캠프의 가상 객체들과의 로컬 전투를 위해 지 역 A에서 방어하는 가상 객체들을 지역 B로 대규모 전송이 수행된다. 일반적으로, 스케줄링 정책은 사용자의 전체 뷰에 대해 비교적 높은 요건을 갖는다.프레임 데이터: 이것은 실행 동안 가상 환경의 응용 프로그램에 의해 생성되는 데이터이다. 가상 환경의 사용 자 인터페이스가 프레임을 단위로서 사용하여 표시되기 때문에, 실행 동안 응용 프로그램에 의해 생성되는 데이 터도 프레임 단위이고, 따라서 프레임 데이터라고 지칭한다. 프레임 데이터는 가상 환경에서 각각의 가상 객체 에 대응하는 프레임 데이터를 포함하고, 가상 객체에 대응하는 프레임 데이터는 가상 객체에 대응하는 사용자 인터페이스를 생성하는데 사용되는 데이터이다. MOBA 게임인 응용 프로그램이 일례로서 사용된다. 가상 객체 A를 제어하는 단말기 A가 MOBA 게임을 실행하고 있을 때, 가상 객체 A에 대응하는 게임 화면(즉, 사용자 인터페 이스)의 프레임이 특정 시간 간격으로 표시되고, 게임 화면의 각각의 프레임에 대응하는 데이터는 가상 객체 A 에 대응하는 프레임 데이터이다. 보상: 이것은 승리 조건에 대한 스케줄링 정책 및/또는 가상 객체의 거동으로부터의 전체적인 기여이다. 승리 조건에 대한 가상 객체의 거동으로부터의 기여는 즉각적인 보상이고, 승리 조건에 대한 가상 객체의 스케줄링 정책으로부터의 기여는 수익이다. 예를 들어, 가상 객체는 지역 A에서 방어하고 있고, 가상 객체의 거동은 가 상 동물을 공격하고 있다. 승리 조건에 대해 가상 동물에 대한 공격에 의해 야기되는 경험치의 증가로부터의 기여는 가상 객체 A의 즉각적인 보상이다. 사용자는 지역 A로부터 지역 B로 대규모로 전달되도록 가상 객체를 제어하여, 적 캠프의 가상 객체들과 로컬 전투를 한다. 적 캠프에서 가상 객체들을 죽임으로써 승리 조건에 대 한 가상 객체로부터의 기여는 수익이다. 가치 네트워크 모델(value network)은 다음의 방식으로 획득된다: 각각의 상황 상태에서의 가상 환경의 응용 프 로그램의 상태 특징 및 전투 결과에 따라, 승리 조건에 대한 각각의 상태 특징으로부터의 기여를 계산하여, 상 태 특징의 수익을 획득함으로써, 다수의 상태 특징-수익 샘플 쌍의 데이터 세트를 획득하고, 상태 특징-수익 샘 플 쌍의 데이터 세트를 사용하여 감독된 딥 학습 모델(supervised deep learning model)을 통해, 상태 특징에 대응하는 수익을 예측하기 위한 머신 학습 모델을 획득한다. 머신 학습 모델은 서로 접속된 다수의 노드들(또는 뉴런들이라고 지칭됨)을 포함하는 동작 모델이고, 각각의 노 드는 하나의 정책 기능에 대응한다. 각각의 2개의 노드 사이의 접속은 접속을 통과하는 신호의 가중된 값을 나 타내고, 가중된 값은 가중치로서 지칭된다. 샘플이 머신 학습 모델의 노드에 입력된 후에, 출력 결과는 각각의 노드에 의해 출력되고, 출력 결과는 다음 노드에 대한 입력 샘플로서 사용된다. 머신 학습 모델은 샘플의 최종 출력 결과를 사용하여 각각의 노드의 정책 기능 및 가중치를 조정한다. 이 프로세스는 트레이닝이라고 지칭된 다. 본 출원의 실시예들에서 제공되는 방법은 가상 환경에서 가상 객체들을 스케줄링하는 것에 적용가능하다. 추론 방법은 가상 환경의 상황 상태에 대한 추론을 수행하는데 사용될 수 있다. 추론 방법은 바둑의 추론에도 적용 될 수 있다. 시뮬레이션 및 추론은 라운드가 끝날 때까지 모든 실행가능한 후속 이동 거동들에 따라 바둑의 현 재 상황 상태에 대해 수행되어, 각각의 후속 이동 거동의 승리 및 손실의 최종 확률을 획득하고나서, 최적의 이 동 정책을 정확하게 결정할 수 있다. 바둑은 19×19의 2차원 공간을 사용하여 표현될 수 있기 때문에, 바둑의 상황 상태들은 이산 유한 상태들이다. 또한, 하나의 바둑 게임에는 평균적으로 약 150개의 라운드가 있고, 상 황 상태들 사이의 천이는 이동 거동에 따라 고유하게 결정될 수 있다. 따라서, 바둑의 상황 상태의 추론은 이 산 상태 공간들과 거동 공간들, 및 제한된 수의 라운드들에 기초하여 구현될 수 있다. 예를 들어, 스마트폰 및 태블릿 컴퓨터와 같은 단말기들 상의 가상 환경의 경우, MOBA 게임 및 SLG와 같은 2차 원 또는 3차원 가상 환경을 갖는 많은 응용 프로그램들이 있다. 전술한 응용 프로그램에서, 사용자는 가상 객 체(예를 들어, 가상 캐릭터)를 제어하여 승리하기 위해 스케줄링 정책을 실행할 수 있다. 사용자가 어떤 이유 로 가상 객체를 제어할 수 없을 때, 사용자는 AI에 의존하여 대응하는 스케줄링 정책을 실행하는 것을 지원할 수 있다. AI가 스케줄링 정책을 실행하기 전에, 본 출원의 실시예들에서 제공되는 방법을 사용하여 현재 상황 상태에 대해 추론을 수행하여, 최적의 스케줄링 정책을 결정할 수 있다. AI는 디지털 컴퓨터 또는 디지털 컴퓨터에 의해 제어되는 머신을 사용하여, 인간 지능을 시뮬레이션, 연장, 및 확장하고, 환경을 인식하고, 지식을 습득하고, 지식을 사용하여 최적의 결과를 획득하는 이론, 방법, 기술, 및 응용 시스템이다. 즉, AI는 지능의 본질을 이해하려고 시도하고, 인간 지능과 유사한 방식으로 반응하는 새로 운 지능형 머신을 생산하는, 컴퓨터 과학의 포괄적인 기술이다. AI는 다양한 지능형 머신의 설계 원리 및 구현 방법을 연구하여, 머신이 인식, 추론, 및 의사 결정의 기능을 가질 수 있게 한다. AI 기술은 포괄적인 분야이며, 하드웨어 수준 및 소프트웨어 수준의 기술 양쪽 모두를 포함하는 광범위한 분야 에 관한 것이다. 기본적인 AI 기술은 일반적으로 센서들, 전용 AI 칩들, 클라우드 컴퓨팅, 분산된 스토리지, 빅 데이터 처리 기술, 운영/상호작용 시스템, 및 전기기계 통합과 같은 기술들을 포함한다. AI 소프트웨어 기술들은 주로 컴퓨터 비전 기술, 음성 처리 기술, 자연어 처리 기술, 및 머신 학습/딥 학습과 같은 몇몇 주요 방 향들을 포함한다. 컴퓨터 비전(CV)은 머신이 \"볼(see)\" 수 있게 하는 방법을 연구하는, 구체적으로, 인간 눈 대신에 카메라와 컴 퓨터를 사용함으로써 대상을 인식, 추적, 측정 등의 머신 비전을 구현하고, 그래픽 처리를 추가로 수행하여, 컴 퓨터가 대상을 관찰하기 위해 인간의 눈에 더 적합하거나, 검출하기 위해 기기에 전송하는데 더 적합한 이미지 로 처리할 수 있게 하는 과학이다. 과학 주제로서, CV는 관련 이론 및 기술을 연구하고, 이미지 또는 다차원 데이터로부터 정보를 획득할 수 있는 AI 시스템을 구축하려고 시도한다. CV 기술들은 일반적으로 이미지 처리, 이미지 인식, 이미지 시맨틱 이해, 이미지 검색, OCR(optical character recognition), 비디오 처리, 비디오 시맨틱 이해, 비디오 콘텐츠/거동 인식, 3차원 물체 재구성, 3D 기술, 가상 현실, 증강 현실, 동기 포지셔닝, 및 맵 구성과 같은 기술들을 포함하며, 일반 얼굴 인식 및 지문 인식과 같은 생물학적 특징 인식 기술들을 추가 로 포함한다. 머신 학습(ML)은 다분야 학제(multi-field interdiscipline)이고, 확률 이론, 통계, 근사 이론, 볼록 분석, 및 알고리즘 복잡성 이론과 같은 복수의 분야에 관한 것이다. 머신 학습은 컴퓨터가 새로운 지식 또는 기술들을 획득하기 위해 인간 학습 거동을 시뮬레이션 또는 구현하는 방법을 연구하고, 기존의 지식 구조를 재구성하여 컴퓨터의 성능을 지속적으로 개선하는 것에 특화되어 있다. 머신 학습은, AI의 핵심이자, 컴퓨터를 지능화하는 근본적인 방식이며, AI의 다양한 분야에 적용가능하다. 머신 학습 및 딥 학습은 일반적으로 인공 신경망, 신념 네트워크(belief network), 강화 학습, 전달 학습(transfer learning), 유도 학습(inductive learning), 및 시 연들로부터의 학습과 같은 기술들을 포함한다. AI 기술의 연구 진행에 따라, AI 기술은 일반 스마트 홈, 스마트 웨어러블 디바이스, 가상 어시스턴트, 스마트 스피커, 스마트 마케팅, 무인 운전, 자동 운전, 무인 항공기, 로봇, 스마트 의료 관리, 및 스마트 고객 서비스 와 같은 복수의 분야에서 연구되고 적용된다. 기술들의 발전에 따라, AI 기술은 더 많은 분야에 적용되고, 점 점 더 중요한 역할을 할 것으로 여겨진다. 본 출원의 실시예들에서 제공되는 해결책들은 AI의 ML과 같은 기술들에 관한 것이며, 다음의 실시예들을 사용하 여 구체적으로 설명된다. 도 1은 본 출원의 일부 예시적인 실시예들에 따른 컴퓨터 시스템의 구조 블록도이다. 네트워크 액세스 시스템 은: 제1 단말기, 제2 단말기, 및 서버를 포함한다. 가상 환경을 지원하는 응용 프로그램은 제1 단말기 상에서 설치되고 실행된다. 제1 단말기가 응용 프로그 램을 실행할 때, 응용 프로그램의 사용자 인터페이스는 제1 단말기의 화면 상에 표시된다. 응용 프로그램 은 군사 시뮬레이션 프로그램, MOBA 게임, 또는 SLG 중 어느 하나일 수 있다. 제1 단말기는 제1 사용자 에 의해 사용되는 단말기이고, 제1 사용자는 제1 단말기를 사용하여 가상 환경에 위치하는 제1 가상 객체를 제어하여 이동을 수행한다. 이동은 신체 자세 조정, 크롤링(crawling), 걷기, 달리기, 사이클링 (cycling), 점핑, 운전, 픽업(picking-up), 사격, 공격, 및 던지기 중 적어도 하나를 포함하지만, 이에 한정되 지 않는다. 예를 들어, 제1 가상 객체는 시뮬레이션된 캐릭터 역할 또는 만화 캐릭터 역과 같은 제1 가상 캐릭 터이다. 가상 환경을 지원하는 응용 프로그램은 제2 단말기 상에 설치되고 실행된다. 응용 프로그램은 군사 시뮬 레이션 프로그램, MOBA 게임, 또는 SLG 중 어느 하나일 수 있다. 제2 단말기가 응용 프로그램을 실행할 때, 응 용 프로그램의 사용자 인터페이스는 제1 단말기의 화면 상에 표시된다. 제2 단말기는 제2 사용자 에 의해 사용되는 단말기이고, 제2 사용자는 제2 단말기를 사용하여 가상 환경에 위치하는 제2 가상 객체를 제어하여 이동을 수행한다. 예를 들어, 제2 가상 객체는 시뮬레이션된 캐릭터 역할 또는 만화 캐 릭터 역할과 같은 제2 가상 캐릭터이다. 일부 실시예에서, 제1 가상 캐릭터와 제2 가상 캐릭터는 동일한 가상 환경에 위치한다. 일부 실시예에서, 제1 가상 캐릭터와 제2 가상 캐릭터는 동일한 캠프, 동일한 팀, 또는 동일한 조직에 속하거나, 친구 관계를 갖거나, 임시 통신 권한을 가질 수 있다. 선택적으로, 제1 가상 캐릭터와 제2 가상 캐릭터는 상이한 캠프들, 상이한 팀 들, 또는 상이한 조직들에 속하거나, 서로 적대적 관계를 가질 수 있다. 일부 실시예에서, 제1 단말기 및 제2 단말기 상에 설치된 응용 프로그램들은 동일하거나, 2개의 단말 상에 설치된 응용 프로그램들은 상이한 제어 시스템 플랫폼들에서 동일한 타입의 응용 프로그램들이다. 제1 단 말기는 일반적으로 복수의 단말기 중 하나를 지칭할 수 있고, 제2 단말기는 일반적으로 복수의 단말기 중 하나를 지칭할 수 있다. 이 실시예에서, 제1 단말기와 제2 단말기만이 설명을 위한 예로서 사 용된다. 제1 단말기와 제2 단말기는 동일하거나 상이한 디바이스 타입이며, 디바이스 타입은 스마트 폰, 태블릿 컴퓨터, 전자책 판독기, MP3 플레이어, MP4 플레이어, 랩톱 휴대용 컴퓨터, 및 데스크톱 컴퓨터 중 적어도 하나를 포함한다. 다른 단말기는 개발자에 대응하는 단말기일 수 있다. 가상 환경의 응용 프로그램에 대한 개발 및 편집 플 랫폼은 단말기 상에 설치된다. 개발자는 단말기 상의 응용 프로그램을 편집하고 편집된 응용 프로그 램 파일을 유선 또는 무선 네트워크를 통해 서버로 전송할 수 있다. 제1 단말기와 제2 단말기 는 서버로부터 응용 프로그램에 대응하는 업데이트 패키지를 다운로드하여 응용 프로그램을 업데이트할 수 있다. 제1 단말기, 제2 단말기, 및 다른 단말기는 무선 네트워크 또는 유선 네트워크를 통해 서버 에 접속된다. 서버는 하나의 서버, 복수의 서버, 클라우드 컴퓨팅 플랫폼, 및 가상화 센터 중 적어도 하나를 포함한다. 서버는 3차원 가상 환경을 지원하는 응용 프로그램에 대한 배경 서비스들을 제공하도록 구성된다. 일부 실시예에서, 서버는 1차 컴퓨팅 작업을 담당하고, 단말기들은 2차 컴퓨팅 작업을 담당하고; 대안적으로, 서버는 2차 컴퓨팅 작업을 담당하고, 단말기들은 1차 컴퓨팅 작업을 담당하고; 대안적으로, 협업 컴퓨팅은 서버와 단말기들 사이의 분산된 컴퓨팅 아키텍처를 사용하여 수행된다. 서버는 적어도 하나의 서버 모듈을 포함한다. 서버 모듈은 프로세서, 사용자 데이터베이 스, 응용 프로그램 데이터베이스, 사용자 지향의 입출력(I/O) 인터페이스, 및 개발자 지향의 I/O 인터페이스를 포함한다. 프로세서는 서버 모듈에 저장된 명령어들을 로딩하고, 사용자 데 이터베이스 및 응용 프로그램 데이터베이스의 데이터를 처리하도록 구성되고; 사용자 데이터베이스 는 무선 네트워크 또는 유선 네트워크를 통해 제1 단말기 및/또는 제2 단말기에 의해 업로드되 는 사용자 데이터를 저장하도록 구성되고; 응용 프로그램 데이터베이스는 가상 환경의 응용 프로그램에 데 이터를 저장하도록 구성되고; 사용자 지향의 I/O 인터페이스는 통신을 구축하고 무선 네트워크 또는 유선 네트워크를 통해 제1 단말기 및/또는 제2 단말기와 데이터를 교환하도록 구성되고; 개발자 지향의 I/O 인터페이스는 통신을 구축하고 무선 네트워크 또는 유선 네트워크를 통해 다른 단말들과 데이터 를 교환하도록 구성된다. AI가 사용자가 대상 가상 객체의 스케줄링을 제어하는 것을 지원하는 시나리오에서, 제1 사용자는 제1 단 말기를 사용하여 가상 환경에 위치하는 제1 가상 객체를 제어하여 이동을 수행한다. 사용자가 제1 가상 객체를 제어하는 것이 불편할 때, 사용자는 응용 프로그램에서 AI 보조 제어 기능을 인에이블할 수 있다. AI 보조 기능이 인에이블되었다는 신호를 수신한 후에, 서버는 제1 가상 객체를 스케줄링한다. AI가 대상 가상 객체를 제어하여 사용자와 전투하는 시나리오에서, 서버는 가상 환경에 위치하는 제1 가상 객체를 제어하여 이동을 수행하고, 제2 사용자는 제2 단말기를 사용하여 동일한 가상 환경에 위치하 는 제2 가상 객체를 제어함으로써 이동을 수행한다. AI가 가상 환경에서 적어도 하나의 가상 객체를 제어하여 전투하는 시나리오에서, 서버는 가상 환경에 위 치하는 복수의 가상 객체를 제어하여 전투한다. 이 응용 시나리오에서, 제1 단말기와 제2 단말기는 선택적인 디바이스들이다. 상기 응용 시나리오들에서, 서버는 다음과 같은 방식으로 스케줄링 정책들에 액세스한다: 실행 동안 가상 환경의 응용 프로그램에 의해 생성되는 프레임 데이터를 획득하는 방식; 프레임 데이터로부터 대상 가상 객체에 대응하는 대상 프레임 데이터를 추출하는 방식; 대상 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상 태에서 대상 가상 객체의 상태 특징을 획득하는 방식; 및 가치 네트워크 예측 모델을 호출하여 상태 특징을 처 리함으로써, 현재 상황 상태에서 대상 가상 객체에 의해 N개의 스케줄링 정책을 실행하는 것의 기대 수익을 획 득하는 방식 -N은 2 이상의 양의 정수임 -. 대상 가상 객체는 제1 가상 객체, 또는 제2 가상 객체, 또는 다른 단말기에 의해 제어되는 다른 가상 객체일 수 있다."}
{"patent_id": "10-2021-7005883", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "본 기술분야의 통상의 기술자는 더 많거나 더 적은 단말기들이 존재할 수 있다는 것을 알 수 있다. 예를 들어, 하나의 단말기만이 있을 수 있거나, 수십 또는 수백 개 이상의 단말기가 있을 수 있다. 단말기들의 수 및 디바 이스 타입들은 본 출원의 실시예들에서 한정되지 않는다. 도 2는 본 출원의 일부 예시적인 실시예들에 따른 가상 환경에서 가상 객체들을 스케줄링하는 방법의 흐름도이 다. 본 방법은 도 1에 도시된 컴퓨터 시스템에 적용가능하며, 본 방법은 다음의 단계들을 포함한다: 단계 201: 실행 동안 가상 환경의 응용 프로그램에 의해 생성되는 프레임 데이터를 획득한다. 프레임 데이터는 실행 동안 가상 환경의 응용 프로그램에 의해 생성되는 데이터이다. 프레임 데이터는 가상 환 경 내의 각각의 가상 객체에 관한 데이터와 가상 환경 내의 각각의 가상 리소스에 관한 데이터를 포함한다. 가 상 환경의 사용자 인터페이스가 프레임을 단위로서 사용하여 표시되기 때문에, 실행 동안 응용 프로그램에 의해 생성되는 데이터도 프레임들의 단위들로 되어 있고, 따라서 프레임 데이터라고 지칭된다. 서버는 미리 결정된 기간의 간격으로 프레임 데이터를 획득하거나, 새로운 프레임 데이터가 생성될 때마다, 서버는 새롭게 생성된 프레임 데이터를 획득한다. 예를 들어, 가상 객체에 관한 데이터는 가상 객체의 위치(좌표들), 속성들(경험치, 경제적 가치, 체력 포인트들, 살상 카운트, 사망 카운트, 리소스 가치, 능력치 등) 또는 거동(이동 타입 거동, 픽업 타입(pick-up- type) 거동, 공격 타입 거동 등) 중 적어도 하나를 포함하고; 가상 리소스는 건물, 중립 생물, 군대, 운송 수단, 무기, 탄약 등 중 적어도 하나를 포함하고, 가상 리소스에 관한 데이터는 가상 리소스의 위치(좌표), 속 성들(내구성 값, 체력 포인트들, 능력치, 속성 등) 또는 거동(이동 거동, 공격 거동 등) 중 적어도 하나를 포함 한다. 단계 202: 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득 한다. 서버는 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 적어도 2개의 가상 객체 중 대상 가상 객체의 상태 특징을 획득한다. 상태 특징은 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 포함한 다. 연관된 가상 객체들은 대상 가상 객체 및 대상 가상 객체와 이익 관계를 갖는 가상 객체이며, 예를 들어, 대상 가상 객체와 동일한 캠프에 속하는 가상 객체, 및 대상 가상 객체의 시야에 나타나는 적 가상 객체 또는 대상 가상 객체와 동일한 캠프에 속하는 가상 객체이다. 예를 들어, 대상 가상 객체의 상태 특징은 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 포함한다. 예를 들어, 가상 객체 A, 가상 객체 B, 및 가상 객체 C는 캠프 1에 속하고; 가상 객체 D, 가상 객체 E, 및 가상 객체 F는 캠프 2에 속하고; 가상 객체 A는 서버에 의해 제어되는 대상 가상 객체이다. 가상 객체 D 는 가상 객체 B의 시야에 있다. 가상 객체 E는 가상 객체 A의 시야에 있고, 가상 객체 A와 격투 상태에 있고, 가상 객체 A와 이익 관계를 갖는다. 가상 객체 F는 가상 객체 A, 가상 객체 B, 및 가상 객체 C의 시야에 있지 않다. 따라서, 연관된 가상 객체들은 가상 객체 A, 가상 객체 B, 가상 객체 C, 가상 객체 D, 및 가상 객체 E를 포함한다. 서버는 프레임 데이터에서 연관된 가상 객체들의 데이터 및 가상 리소스들의 데이터를 판독하고, 특징 추출을 수행하여 상태 특징을 획득한다. 예를 들어, 서버에 의해 추출되는 상태 특징은 S(S1h, S2h ..., S01t, S02t ..., S01m, S02m ...)이다. S는 현재 상황 상태에서의 대상 가상 객체의 상태 특징을 나타내고, S*h는 a*번째 가상 객 체의 상태를 나타내고, S*t는 a*번째 건물의 상태를 나타내고, S*m는 a*번째 중립 생물의 상태를 나타낸다. 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들이 상태 특징으로서 샘플링되기 때문에, 가상 환경에서 의 현재 상황에 대한 샘플링이 구현된다. 제한된 수의 연관된 가상 객체들 및 가상 리소스들이 존재하기 때문 에, 가상 환경에서의 현재 상황은 이산화된다. 예를 들어, MOBA 게임은 2개의 캠프를 포함하고, 각각의 캠프는 5개의 가상 객체를 포함하고, MOBA 게임의 맵은 경쟁되거나, 점유되거나, 또는 파괴될 수 있는 144개의 가상 리소스 위치를 포함한다. 따라서, 추출된 상태 특 징이 5개 내지 10개의 가상 객체의 상태들 및 144개의 가상 리소스의 상태들을 포함함으로써, 가상 환경에서 현 재 상황의 샘플링 및 이산화를 구현한다. 단계 203: 상태 특징에 대해 추론을 수행하여, N개의 후속 상태 특징을 획득하고, N은 2 이상의 양의 정수이다. 예를 들어, N개의 스케줄링 정책에 따라 상태 특징에 대해 추론이 수행되고, 스케줄링 정책들 각각은 개별적으 로 실행되어, 각각의 스케줄링 정책에 대응하는 후속 상태 특징을 획득할 수 있다. m개의 거동을 수행하기 위 해 n개의 액세스가능한 이동 영역으로 이동하는 스케줄링 정책들이 예로서 사용된다. 대상 가상 객체는 m개의실행가능한 거동(픽업-타입 거동들 및 공격-타입 거동들을 포함하고, 여기서 픽업-타입 거동은 아이템을 픽업하 는 것, 아이템을 장착하는 것, 아이템을 파괴하는 것, 아이템을 수정하는 것 등 중 적어도 하나를 포함하고, 공 격-타입 거동은 사격, 물리적 공격, 주문 공격, 탄약 던지기 등 중 적어도 하나를 포함함) 및 n개의 액세스가능 한 이동 영역에 대응한다. m개의 거동 및 n개의 액세스가능한 위치는 대상 가상 객체의 N개의 스케줄링 정책을 구성하고, m과 n은 양의 정수들이고, m≥1, n≥2, m*n=N이다. 서버는 현재 상황 상태의 변환에 의해 야기되는 현재 상황 상태에서의 상태 특징으로부터, 대상 가상 객체가 N개의 스케줄링 정책을 실행한 후의 N개의 후속 상 황 상태로 변환된 N개의 후속 상태 특징을 계산한다. 예를 들어, 현재 상태 특징에서, 이동 영역 1에 위치된 건물 1의 상태는 중립적이고, 대상 가상 객체의 N개의 스케줄링 정책 중 i번째 스케줄링 정책은 이동 영역 1로 이동하는 것이다. 대상 가상 객체가 이동 영역 1로 이동한 후에, 이동 영역 1에 있는 건물 1의 상태는 중립 상 태로부터 제어된 상태로 변환된다. 단계 204: 가치 네트워크 예측 모델을 호출하여 N개의 후속 상태 특징을 처리함으로써, 대상 가상 객체에 의해 N개의 스케줄링 정책을 실행하는 것의 기대 수익을 획득한다. 기대 수익은 가상 환경에서 전투의 승리 조건에 대한 스케줄링 정책들 각각으로부터의 기여를 나타내는데 사용 된다. 서버는 N개의 후속 상태 특징을 가치 네트워크 예측 모델에 입력하여, 스케줄링 정책들에 대응하는 각각 의 예측된 수익을 획득한다. 즉, 대상 가상 객체에 의해 N개의 스케줄링 정책을 실행하는 것의 기대 수익은 N 개의 스케줄링 정책 각각에 대응하는 각각의 기대 수익을 지칭한다. 예를 들어, 스케줄링 정책들은 n개의 이동 영역으로 이동하는 것이다. 대상 가상 객체가 이동할 수 있는 영역 들은 이동 영역 1, 이동 영역 2, 및 이동 영역 3이다. 상태 특징을 판독하는 것을 통해, 이동 영역 1의 상태는 중립 상태의 건물이고, 이동 영역 2의 상태는 동일한 캠프 내의 가상 객체들이 적 캠프 내의 가상 객체들과 전 투하고 있고, 이동 영역 3의 상태는 중립 상태의 중립 생물이라는 것을 알 수 있다. 가치 네트워크 예측 모델 은 대상 가상 객체를 이동 영역 1로 이동시키는 것의 기대 수익이 1인 것으로 예측하고; 대상 가상 객체를 이동 영역 2로 이동시킴으로써 영역을 점유하는 유익한 효과를 산출하고, 기대 수익은 3인 것으로 예측하고; 이동 영 역 3으로 이동할 때, 대상 가상 객체가 중립 생물을 죽여서 경험치를 획득하고, 기대 수익이 2인 것으로 예측한 다. 단계 205: 대상 가상 객체를 제어하여 N개의 스케줄링 정책에서 기대 수익이 가장 높은 스케줄링 정책을 실행한 다. 서버는 N개의 스케줄링 정책의 기대 수익 중 가장 높은 기대 수익에 대응하는 스케줄링 정책을 대상 스케줄링 정책으로서 획득하고, 대상 가상 객체를 제어하여 대상 스케줄링 정책을 실행한다. 예를 들어, 단계 204에서 설명된 바와 같이, 서버는 계산을 통해 대상 가상 객체를 이동 영역 1로 이동시키는 것의 기대 수익이 1이고, 대상 가상 객체를 이동 영역 2로 이동시키는 것의 기대 수익이 3이고, 대상 가상 객체를 이동 영역 3으로 이동 시키는 것의 기대 수익이 2라는 것을 알고, 그 후 가장 높은 기대 수익을 갖는 스케줄링 정책을 결정한다. 즉, 이동 영역 2로의 이동은 대상 스케줄링 정책이고, 대상 가상 객체는 이동 영역 2로 이동하도록 제어된다. 일부 실시예에서, 대상 스케줄링 정책은 스케줄링 정책들의 기대 수익에 따라 N개의 스케줄링 정책으로부터 선 택될 수 있고, 대상 가상 객체는 그 대상 스케줄링 정책을 실행하도록 제어된다. 예를 들어, 기대 수익이 가장 높은 전술한 스케줄링 정책은 대상 스케줄링 정책으로서 선택될 수 있거나, 또는 대상 스케줄링 정책은 다른 정 책 선택 방법과 조합하여 선택될 수 있다. 예를 들어, 사용자는 영웅의 히트 포인트들을 보장하는 우선 순위를 미리 설정할 수 있다. 따라서, 미리 설정된 순위보다 높은 기대 수익 및 영웅의 최소 히트 포인트 손실을 갖는 스케줄링 정책이 대상 스케줄링 정책으로서 선택될 수 있다. 상기에 기초하여, 본 출원의 이 실시예에서, 가상 환경의 현재 상황에서의 상태 특징이 획득된다. 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들이 상태 특징으로서 샘플링되기 때문에, 가상 환경에서의 현재 상황 에 대한 샘플링이 구현된다. 제한된 수의 연관된 가상 객체들 및 가상 리소스들이 존재하기 때문에, 가상 환경 에서의 현재 상황은 이산화된다. 따라서, N개의 후속 상태 특징은 제한된 이산화된 상태 특징들에 기초하여 N 개의 스케줄링 정책과 조합하여 획득될 수 있어, 가치 네트워크 예측 모델을 호출하여 N개의 후속 상태 특징을 처리함으로써, 대상 가상 객체에 의해 스케줄링 정책들 각각을 실행하는 것의 기대 수익을 획득하게 된다. 대 안적으로, 승리 또는 손실 조건을 충족시키는 최종 상태 특징까지 현재 상황 상태의 상태 특징에 대해 추론을 수행할 필요가 없을 수 있고, 그에 의해 추론 단계들을 단순화하여, 가상 환경의 상황 상태들에 대한 추론을 구 현하고, 스케줄링 정책을 실행하도록 가상 객체를 제어함에 있어서 AI의 정확도를 더욱 향상시킨다.또한, 본 출원의 이 실시예에서, 가상 객체에 의해 실행되는 스케줄링 정책은 현재 상황의 상태 특징 및 N개의 스케줄링 정책에 기초하여 추론을 수행하여 N개의 후속 상태 특징을 획득하고, 가치 네트워크 모델을 사용하여 N개의 상태 특징을 처리함으로써 획득된다. 따라서, 가상 환경의 응용 프로그램의 로직과는 독립적이므로, 가 상 객체의 스케줄링을 제어하는데 있어서 AI의 적응성을 향상시킨다. 가치 네트워크 예측 모델을 호출하여 상태 특징을 처리하기 전에, 서버는 가치 네트워크 예측 모델을 트레이닝 할 필요가 있다. 가치 네트워크 예측 모델의 트레이닝 데이터는 사용자에 의해 가상 환경의 응용 프로그램을 실행하는 이력 데이터에 기초한다. 다음의 실시예에서는 가치 네트워크 예측 모델의 트레이닝 프로세스가 설명 된다. 도 3은 본 출원의 일부 예시적인 실시예들에 따른 가치 네트워크 예측 모델(value network prediction model)을 트레이닝하기 위한 방법의 흐름도이다. 본 방법은 도 1에 도시된 컴퓨터 시스템에 적용가능하다. 본 방 법은 도 2의 실시예에서 단계 202 전에 수행되는 방법일 수 있다. 본 방법은 다음 단계들을 포함한다. 단계 301: p개의 샘플 프레임 데이터를 획득하고, p는 2 이상의 양의 정수이고; 선택적으로, 서버에 의해 p개의 샘플 프레임 데이터를 획득하기 위한 방법은 다음 단계들을 포함하지만, 이에 한정되지 않는다: 단계 301a: l개의 이력 프레임 데이터를 획득하고, l은 p 이상의 양의 정수이다. 서버는 실행 동안 가상 환경의 응용 프로그램에 의해 생성되는 이력 프레임 데이터를 획득하여, l개의 이력 대 상 프레임 데이터를 획득한다. 예를 들어, 승리 조건을 충족시키는 MOBA 게임의 한 라운드를 실행하는 것은 약 20,000개의 프레임의 이력 프레 임 데이터를 생성한다. 서버는 이력 프레임 데이터를 획득하기 위해, 프레임을 단위로서 사용하여, MOBA 게임 의 적어도 하나의 라운드를 실행함으로써 생성되는 이력 프레임 데이터를 획득한다. 단계 301b: 미리 설정된 시간 단계에 따라 l개의 이력 프레임 데이터를 추출하여, q개의 후보 프레임 데이터를 획득하고, q는 양의 정수이고, p≤q≤l이다. 서버는 미리 설정된 시간 단계를 단위로서 사용하여 m개의 이력 프레임 데이터에 대해 추출을 수행하여, q개의 후보 프레임 데이터를 획득한다. 예를 들어, 이력 프레임 데이터 사이의 시간 간격은 0.05초이고, 서버는 미리 설정된 시간 단계 1초를 단위로서 사용하여 이력 프레임 데이터에 대해 추출을 수행하여, 후보 프레임 데이터를 획득함으로써, 처리될 프레임 데이터의 양을 95%만큼 감소시킨다. 단계 301c: q개의 후보 프레임 데이터를 판독하고, q개의 후보 프레임 데이터에서, 공격 거동을 포함하는 후보 프레임 데이터를 샘플 프레임 데이터라고 판정하여, p개의 샘플 프레임 데이터를 획득하고, n≤q이다. 서버는 후보 프레임 데이터를 판독하고, 후보 프레임 데이터에서 공격 거동을 포함하는 후보 프레임 데이터를 샘플 프레임 데이터하고 판정하여, p개의 샘플 프레임 데이터를 획득한다. 단계 302: 각각의 샘플 프레임 데이터에 대해 특징 추출을 수행하여, p개의 샘플 프레임 데이터의 샘플 상태 특 징들을 획득한다. 서버는 샘플 프레임 데이터에 대해 특징 추출을 수행하여, p개의 샘플 프레임 데이터의 상태 특징을 획득한다. 상태 특징은 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 포함한다. 선택적으로, 가상 리소스는 건물, 중립 생물, 군대, 운송 수단, 무기, 또는 탄약 중 적어도 하나를 포함하고; 연관된 가상 객체들의 상태들은 연관된 가상 객체들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 경험 치들, 살상 카운트들, 사망 카운트들, 또는 경제적 가치들 중 적어도 하나를 포함하고; 가상 리소스들의 상태들 은 가상 리소스들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 내구성 값들, 또는 점유 상태들 중 적 어도 하나를 포함한다. 가상 리소스의 점유 상태는 중립 상태, 제어된 상태, 점유된 상태, 또는 경쟁 상태를 포함한다. 중립 상태는 가상 리소스가 어떠한 캠프에도 속하지 않는다는 것을 나타내는데 사용된다. 예를 들어, 중립 생 물 및 중립 건물의 점유 상태들은 중립 상태들이고, 임의의 캠프에서의 가상 객체들이 중립 생물을 공격함으로 써, 중립 생물이 제어된 상태에 있게 한다.제어된 상태는 가상 리소스가 위치하는 이동 영역 내의 동일한 캠프에 속하는 적어도 하나의 가상 객체가 존재 한다는 것을 나타내는데 사용된다. 예를 들어, 중립 건물 1이 이동 영역 1 내에 위치하고, 이동 영역 1 내의 캠프 1에 속하는 가상 객체 A가 존재한다면, 중립 건물 1의 점유 상태는 캠프 1에 의해 제어되는 상태이다. 점유된 상태는 가상 리소스가 임의의 캠프에 속한다는 것을 나타내는데 사용된다. 예를 들어, 방어 타워 1이 캠프 1에 속하는 건물인 경우, 다른 캠프들은 방어 타워 1을 제어할 수 없고, 다른 캠프들 내의 가상 객체들은 단지 방어 타워 1을 우회하거나 이동 중에 이를 파괴할 수만 있다. 경쟁 상태는 가상 리소스가 위치하는 이동 영역 내에 상이한 캠프들에 속하는 적어도 2개의 가상 객체가 존재한 다는 것을 나타내는데 사용된다. 예를 들어, 중립 생물 또는 중립 건물이 위치하는 이동 영역에 각각 캠프 1 및 캠프 2에 속하는 가상 객체들이 존재하면, 중립 생물 또는 중립 건물의 점유 상태는 경쟁 상태이다. 본 출원의 이 실시예에서, 가상 환경의 맵은 가상 리소스들의 위치들에 기초하여 n개의 이동 영역으로 분할되고, n은 2 이상의 양의 정수이다. 예를 들어, 도 4에 도시된 바와 같이, 가상 환경의 맵은 가상 리 소스들의 위치들에 기초하여 10개의 이동 영역(411 내지 420)로 분할된다(도 4에서, 삼각형들은 가상 건물들의 식별자들이고, 별모양은 중립 생물들의 식별자들이다). 맵은 대상 가상 객체, 대상 가상 객체 와 동일한 캠프에 속하는 가상 객체, 대상 가상 객체의 시야 내에 있고 적대적 캠프에 속하는 가상 객체, 및 가상 객체의 시야 내에 있고 적대적 캠프에 속하는 가상 객체를 나타낸다. 도 4에 화 살표들로 도시된 바와 같이, 대상 가상 객체가 액세스할 수 있는 이동 영역들은 10개의 이동 영역(대상 가상 객 체가 위치하는 이동 영역을 포함함)이다. 단계 303: p개의 샘플 상태 특징 각각의 즉각적인 보상을 계산한다. 예를 들어, 서버는 샘플 상태 특징에서 가상 객체의 거동 및 상태를 판독하고, 가상 객체에 의해 거동을 실행함 으로써 야기되는 상태의 변화를 계산하여, 상태의 변화에 기초하여 승리 조건에 대한 기여를 계산하여 즉각적인 보상을 획득한다. 예를 들어, 샘플 상태 특징 1에서의 가상 객체의 경험치는 2이고, 가상 객체의 거동은 중립 생물을 공격하는 것이다. 중립 생물을 죽인 후에, 가상 객체는 3의 경험치를 획득할 수 있다. 즉, 가상 객체 가 중립 생물을 죽인 후에 획득되는 경험치는 5이다. 서버는 승리 조건에 대해 가상 객체의 경험치의 2에서 5 로 증가한 기여를 계산하여, 샘플 상태 특징 1의 즉각적인 보상을 획득한다. 단계 304: p개의 샘플 상태 특징 중 i번째 샘플 상태 특징에 대해, i번째 샘플 상태 특징의 즉각적인 보상 및 (i+1)번째 샘플 상태 특징의 즉각적인 보상에 따라 i번째 샘플 상태 특징의 실제 수익을 계산하고, i는 p 이하 의 양의 정수이다. 각각의 상태 특징의 실제 수익은 누적 결과이다. 예를 들어, p개의 샘플 상태 특징에서의 x번째 샘플 상태 특 징 내지 y번째 샘플 상태 특징은 전투의 동일한 라운드에서의 초기 상태 특징 내지 마지막 상태 특징이다. i번 째 샘플 상태 특징은 x번째 샘플 상태 특징 내지 y번째 샘플 상태 특징 중 임의의 샘플 상태 특징이다. i번째 샘플 상태 특징의 실제 수익은 (i+1)번째 샘플 상태 특징의 즉각적인 보상으로부터 y번째 샘플 상태 특징의 즉 각적인 보상까지의 축적 값이다. 예를 들어, i번째 샘플 상태 특징의 실제 수익은 다음의 공식을 사용하여 계산될 수 있다:"}
{"patent_id": "10-2021-7005883", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 Gi는 i번째 샘플 상태 특징의 실제 수익을 나타내고, R*는 a*번째 샘플 상태 특징의 즉각적인 보상을 나 타내고, λ*-i-1는 i번째 샘플 상태 특징에 대한 *번째 샘플 상태 특징의 즉각적인 보상의 영향 계수(impact factor)를 나타내고, *번째 샘플 상태 특징으로부터 i번째 샘플 상태 특징까지의 시간이 길어질수록 더 작은 영 향 계수를 표시한다. 스케줄링 정책 θ의 실행 이후의 i번째 샘플 상태 특징의 실제 수익은 다음 공식을 사용하여 계산될 수 있다:"}
{"patent_id": "10-2021-7005883", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서 Vθ(s)는 스케줄링 정책 θ의 실행 이후의 샘플 상태 특징 S의 실제 수익을 나타내고, E[s]는 상태 특징 s 하에서의 기대 수익을 나타내고, 는 반복적 계산을 통해 획득된 파라미터를 나타낸다. 서버는 단계들 301 내지 304를 반복적으로 수행하고, 라운드를 단위로서 사용하여 샘플 프레임 데이터를 연속적 으로 획득함으로써, 샘플 상태 특징-보상(즉각적인 보상 및 실제 수익)의 복수의 샘플 데이터 세트를 추출할 수 있다. 단계 305: 원래의 가치 네트워크 예측 모델을 호출하여 각각의 샘플 상태 특징을 처리하여, 각각의 샘플 상태 특징의 트레이닝 결과를 획득한다. 서버는 각각의 샘플 상태 특징을 원래의 가치 네트워크 예측 모델에 입력하여, 각각의 샘플 상태 특징의 트레이 닝 결과를 획득한다. 원래의 가치 네트워크 예측 모델은 트레이닝되지 않은 머신 학습 모델이고, 각각의 샘플 상태 특징의 트레이닝 결과는 샘플 상태 특징 하에서 각각의 스케줄링 정책을 실행하는, 원래의 가치 네트워크 모델에 의해 예측되는, 기대 수익이다. 단계 306: 샘플 데이터 세트들 각각에 대해, 트레이닝 결과를 실제 보상과 비교하여, 계산 손실을 획득하고, 계 산 손실은 트레이닝 결과와 샘플 상태 특징의 실제 보상 사이의 오차를 표시하는데 사용된다. 예를 들어, 샘플 상태 특징 S1을 예로 들면, 스케줄링 정책 θ1을 실행하는 것의 실제 수익은 2이고, 스케줄링 정책 θ2를 실행하는 것의 실제 수익은 -1이고, 스케줄링 정책 θ3을 실행하는 것의 실제 수익은 3이고; 샘플 상 태 특징 S1의 트레이닝 결과는 다음과 같다: 스케줄링 정책 θ1을 실행하는 것의 기대 수익은 1.5이고, 스케줄링 정책 θ2를 실행하는 것의 기대 수익은 -1이고, 스케줄링 정책 θ3을 실행하는 것의 기대 수익은 2이다. 이 경 우, 계산 손실은 (0.5, 0, 1)이다. 단계 307: 적어도 하나의 샘플 데이터 세트에 대응하는 각각의 계산 손실에 따라 오차 역 전파 알고리즘(error back propagation algorithm)을 사용하여 트레이닝을 통해 가치 네트워크 예측 모델을 획득한다. 서버는 적어도 하나의 샘플 데이터 세트에 대응하는 각각의 계산 손실에 따라 오차 역 전파 알고리즘을 사용하 여 트레이닝을 통해 가치 네트워크 예측 모델을 획득한다. 가치 네트워크 예측 모델은 상태 특징에 따라, 상태 특징 하에서 각각의 스케줄링 정책을 실행하는 것의 기대 수익을 예측하는데 사용된다. 상기에 기초하여, 본 출원의 이 실시예에서, 샘플 프레임 데이터의 상태 특징은 샘플 프레임 데이터 내의 데이 터를 판독함으로써 추출되고, 여기서 상태 특징은 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 포함한다. 서버가 가상 환경 내의 모든 파라미터를 상태 특징으로서 사용하여 샘플 프레임 데이터의 상태를 표 현할 필요가 없기 때문에, 상태 특징의 결정이 단순화되고, 그에 의해 서버의 계산량을 감소시키고, 서버의 처 리 속도를 향상시킨다. 선택적으로, 본 출원의 이 실시예에서, 미리 설정된 시간 단계에 따라 이력 프레임 데이터로부터 후보 프레임 데이터가 추출되고, 후보 프레임 데이터에서, 공격 거동을 포함하는 프레임 데이터가 샘플 프레임 데이터로서 결정된다. 모든 이력 대상 프레임 데이터를 처리할 필요가 없기 때문에, 서버의 처리 효율이 향상된다. 선택적으로, 본 출원의 이 실시예에서, 샘플 프레임 데이터의 각각의 프레임의 즉각적인 보상이 계산되고; 현재 샘플 프레임 데이터의 실제 수익은 현재 샘플 프레임 데이터의 즉각적인 보상 및 샘플 프레임 데이터의 다음 프 레임의 즉각적인 보상에 따라 계산된다. 실제 수익에 대한 현재 샘플 프레임 데이터 이후의 샘플 프레임 데이 터 각각의 프레임으로부터의 영향을 누적하여 계산할 필요가 없기 때문에, 서버의 계산량이 감소되고, 그에 의 해 서버의 처리 속도를 향상시킨다. 도 5는 본 출원의 일부 예시적인 실시예들에 따른 가상 환경에서 가상 객체들을 스케줄링하는 방법의 흐름도이 다. 본 방법은 도 1에 도시된 컴퓨터 시스템에 적용가능하며, 본 방법은 다음의 단계들을 포함한다:단계 501: 실행 동안 가상 환경의 응용 프로그램에 의해 생성되는 프레임 데이터를 획득한다. 서버에 의해 프레임 데이터를 획득하기 위한 방법에 대해서는, 도 2의 실시예에서의 단계 201을 참조할 수 있고, 세부사항들은 본 명세서에서 설명되지 않는다. 단계 502: 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득 한다. 서버는 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획득한 다. 상태 특징은 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 포함한다. 선택적으로, 가상 리소스는 건물, 중립 생물, 군대, 운송 수단, 무기, 또는 탄약 중 적어도 하나를 포함하고; 연관된 가상 객체들의 상태들은 연관된 가상 객체들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 경험 치들, 살상 카운트들, 사망 카운트들, 또는 경제적 가치들 중 적어도 하나를 포함하고; 가상 리소스들의 상태들 은 가상 리소스들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 내구성 값들, 또는 점유 상태들 중 적 어도 하나를 포함하고; 가상 리소스의 점유 상태는 중립 상태, 제어된 상태, 점유된 상태, 또는 경쟁 상태를 포 함한다. 프레임 데이터에 대해 특징 추출을 수행하여 서버에 의해 현재 상황 상태에서 대상 가상 객체의 상태 특징을 획 득하기 위한 방법에 대해서는, 도 2의 실시예에서의 단계 201 및 도 3의 실시예에서의 단계들을 참조할 수 있다. 단계 503: 대상 가상 객체에 대응하는 m개의 거동을 획득하고, m은 1 이상의 양수이다. 서버는 프레임 데이터에서 대상 가상 객체의 데이터를 판독하여, 대상 가상 객체에 의해 실행가능한 m개의 거동 을 획득한다. 거동들은 픽업-타입 거동들 및 공격-타입 거동들을 포함한다. 선택적으로, 픽업-타입 거동은 아 이템을 픽업하는 것, 아이템을 장착하는 것, 아이템을 파괴하는 것, 아이템을 수정하는 것 등 중 적어도 하나를 포함하고; 공격-타입 거동은 사격, 물리적 공격, 주문 공격, 탄약 던지기 등 중 적어도 하나를 포함한다. 단계 504: 대상 가상 객체에 대응하는 m개의 거동 및 n개의 이동 영역에 따라 N개의 스케줄링 정책을 획득하고, 스케줄링 정책들은 n개의 이동 영역 중 i번째 이동 영역으로 이동하여 m개의 거동 중 j번째 거동을 수행하는 것 을 포함하고, i와 j는 양의 정수이고, 1≤i≤n, 1≤j≤m이다. 예를 들어, 서버는 m개의 거동 및 n개의 이동 영역에 따라 스케줄링 정책 공간을 형성한다. 공간은 m행과 n열 의 행렬이거나, 공간은 n행과 m열의 행렬이다. 스케줄링 정책 공간은 N개의 스케줄링 정책을 포함한다. 단계 505: N개의 스케줄링 정책을 상태 특징에 적용하고, 미리 설정된 상태 천이 관계에 따라 N개의 후속 상태 특징을 획득한다. 상태 천이 관계는 스케줄링 정책들의 적용 후에 상태 특징의 상태 변경들을 나타내는데 사용된다. 예를 들어, 상태 천이 관계의 표현 형태는 상태 천이 행렬이다. 상태 천이 행렬은 가상 객체들 및 가상 리소스 들의 상태들과 거동들 사이의 매핑 관계들을 포함한다. 서버는 스케줄링 정책 공간에 상태 천이 행렬을 곱함으 로써 N개의 후속 상태 특징을 획득한다. 예를 들어, 스케줄링 정책에서 가상 객체의 거동은 물리적 공격이고, 물리적 공격의 공격력은 5이고, 상태 특징 S에서의 공격받은 건물 1의 내구성 값은 6이고; 이 경우, 후속 상태 특징 S'에서의 건물 1의 내구성 값은 1이다. 단계 506: 가치 네트워크 예측 모델을 호출하여 N개의 후속 상태 특징을 처리함으로써, 대상 가상 객체에 의해 N개의 스케줄링 정책을 실행하는 것의 기대 수익을 획득한다. 예를 들어, 도 6에 도시된 바와 같이, 상태 특징 S에서, 대상 가상 객체는 이동 영역에 위치하고, 대 상 가상 객체는 N개의 스케줄링 정책을 실행하여 상태 특징이 후속 상태 특징 S'로 변환되게 한다. 후속 상태 특징 S'의 상태 특징 공간에서, 대상 가상 객체는 복수의 실행가능한 액션을 수행하기 위해 이동 영 역들(411 내지 420) 내에 위치할 수 있다. 서버는 가치 네트워크 예측 모델을 호출하여 후속 상태 특징 S'를 처리함으로써, 후속 상태 특징 S'에서의 각각의 서브공간의 기대 수익을 획득한다. 단계 507: 대상 가상 객체를 제어하여 N개의 스케줄링 정책에서 기대 수익이 가장 높은 스케줄링 정책을 실행한 다.서버는 N개의 스케줄링 정책의 기대 수익 중 기대 수익이 가장 높은 스케줄링 정책을 대상 스케줄링 정책으로서 획득하고, 제어 명령어를 단말기에 전송한다. 제어 명령어는 대상 가상 객체를 제어하여 대상 스케줄링 정책을 실행하도록 단말기에게 지시하는데 사용된다. 상기에 기초하여, 본 출원의 이 실시예에서는, 현재 상황 상태에서의 상태 특징이 추출된다. 상태 특징은 단지 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 포함하기 때문에, 가상 환경에서의 현재 상황에 대 한 이산화 및 샘플링 처리가 구현된다. 따라서, 현재 상황에서의 상태 특징의 N개의 후속 상태 특징은 현재 상 황 상태의 상태 특징에 기초하여 N개의 스케줄링 정책과 조합하여 획득될 수 있다. 또한, 가치 네트워크 예측 모델을 호출하여 N개의 후속 상태 특징을 처리함으로써, 대상 가상 객체에 의해 스케줄링 정책들 각각을 실행하 는 것의 기대 수익을 획득한다. 승리 또는 손실 조건을 충족시키는 최종 상태 특징까지 현재 상황 상태의 상태 특징에 대해 추론을 수행할 필요가 없고, 그에 의해 추론 단계들을 단순화하여, 가상 환경의 상황 상태들에 대 한 추론을 구현하고, 스케줄링 정책을 실행하도록 가상 객체를 제어함에 있어서 AI의 정확도를 더욱 향상시킨다. 본 출원의 실시예들의 단계들은 단계 번호들에 의해 표시된 시퀀스에 따라 반드시 수행되는 것은 아니다. 본 명세서에서 달리 명백하게 특정되지 않는 한, 단계들은 임의의 엄격한 시퀀스 제한없이 수행되고, 다른 순서로 수행될 수 있다. 또한, 실시예들에서의 단계들 중 적어도 일부는 복수의 서브-단계들 또는 복수의 스테이지를 포함할 수 있다. 서브-단계들 또는 스테이지들은 반드시 동일한 시점에서 수행되는 것은 아니고, 상이한 시점 들에서 수행될 수도 있다. 서브-단계들 또는 스테이지들은 반드시 순차적으로 수행되는 것은 아니고, 다른 단 계들 또는 적어도 일부 서브-단계들 또는 다른 단계들의 스테이지들과 차례로 또는 교대로 수행될 수 있다. 일 실시예에서, 서버가 추가로 제공된다. 단말기는 가상 환경에서 가상 객체들을 스케줄링하는 장치를 포함한 다. 가상 환경에서 가상 객체들을 스케줄링하는 장치는 모듈들을 포함하고, 각각의 모듈은 소프트웨어, 하드웨 어, 또는 이들의 조합을 사용하여 전체적으로 또는 부분적으로 구현될 수 있다. 예시적인 실시예에서, 도 7에 도시된 바와 같이, 본 출원의 이 실시예에서의 서버는 데이터 획득 모듈, 트 레이닝 모듈, 가치 평가 모듈, 및 제어 모듈을 포함한다. 가치 네트워크 트레이닝 모듈은 상태 샘플링 모듈, 특징 추출 모듈, 보상 추출 모듈, 및 머신 학습 트레이닝 모듈을 포함 한다. 가치 평가 모듈은 특징 추출 모듈, 상태 추론 모듈, 및 예측 모듈을 포함한다. 각 각의 모듈은 다음과 같이 기술된다: 1. 트레이닝 모듈: 상태 샘플링 모듈: 일반적으로, MOBA 게임의 한 라운드는 수십 분보다 더 오래 지속되고, 게임으로 변환된 프레임 데이터는 수만 프레임보다 더 많다. 그러나, 실제 게임 프로세스에서, 매크로 상황은 비교적 느리게 변 하고, 프레임 데이터의 상황 상태들은 높은 상관관계에 있다. 가상 객체의 공격 거동이 상황 상태의 변경의 주 요 인자이기 때문에, 상태 샘플링 모듈에서, MOBA 게임의 이력 프레임 데이터는 미리 설정된 시간 단계에 의해 보완되는, 코어로서 가상 객체의 공격 거동을 사용하여 샘플링되어, 샘플 데이터를 획득함으로써 공격 거 동에 기초하여 샘플 데이터를 획득한다. 이러한 방식으로, 상황 상태들과의 상관관계가 낮은 프레임 데이터는 샘플 데이터가 감소되는 동안 유지된다. 도 8에 도시된 바와 같이, 데이터 획득 모듈은 MOBA 게임의 한 라운드에서 생성된 이력 프레임 데이터 를 획득하고, 이력 프레임 데이터를 상태 샘플링 모듈에 전송하고; 상태 샘플링 모듈은 미 리 설정된 시간 단계에서 이력 대상 프레임 데이터에 대한 추출을 수행하여 후보 프레임 데이터를 획 득하고, 후보 프레임 데이터에서 공격 거동을 포함하는 프레임 데이터를 판독하여, 샘플 프레임 데이터 를 획득한다. 특징 추출 모듈은 샘플 프레임 데이터에서 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 추출하여 샘플 상태 특징을 획득한다. 보상 추출 모듈: 가치 네트워크 모델에 의해 예측되는 기대 수익은 특정 스케줄링 정책이 특정 시점 t에서 의 상황 상태, 즉 모든 후속 시점들에서의 즉각적인 보상의 누적에서 실행될 때 획득되는 수익을 나타낸다. 실 제로, 게임의 한 라운드가 종료되지 않는 한, 모든 즉각적인 보상을 명시적으로 획득함으로써 각각의 상황 상태 의 수익은 계산될 수 없다. 따라서, 벨만(Bellman) 방정식을 도입하여, 현재 상황 상태에서의 수익의 계산이 단지 다음 상황 상태에서의 수익 및 현재 상황 상태에서의 즉각적인 보상과 관련되게 할 수 있다. 즉각적인 보 상에 대한 참조 인자들은 가상 객체의 경험치, 경제적 가치, 가상 객체의 물리적 강도(체력 포인트들), 살상 카운트, 사망 카운트, 및 건물의 내구성 값을 포함하지만, 이들로 한정되지 않는다. 본 출원의 이 실시예에서, 도 7에 도시된 바와 같이, 보상 추출 모듈은 각각의 샘플 상태 특징의 즉각적인 보상을 계산하고, 복 수의 샘플 상태 특징 각각의 즉각적인 보상 및 다음 프레임 샘플 상태 특징의 즉각적인 보상에 따라, 각각 의 샘플 상태 특징의 실제 수익을 계산한다. 머신 학습 트레이닝 모듈은 MOBA 게임에서 사용자의 이력 전투 프레임 데이터로부터 획득된 상황 상태 특 징들 및 실제 수익을 포함하는 샘플 데이터 세트들을 사용하여, 다층 콘볼루션 신경망과 같은 머신 학습 모델을 통해 다량의 반복 트레이닝을 수행하여, 트레이닝을 통해 가치 네트워크 예측 모델을 획득한다. 본 출원의 이 실시예에서, 서버는 각각의 샘플 상태 특징을 원래의 가치 네트워크 예측 모델에 입력하여 트레이닝 결과 를 획득하고, 트레이닝 결과와 실제 수익 사이의 차이에 따라 계산 손실을 획득함으로써, 원래 의 가치 네트워크 예측 모델을 트레이닝하여, 가치 네트워크 예측 모델을 획득한다. 2. 가치 평가 모듈들: 특징 추출 모듈은 MOBA 게임에서 생성된 프레임 데이터에 대해 상태 추출을 수행하여, 각각의 프레임 데이 터의 상태 특징을 획득한다. 예를 들어, 도 9에 도시된 바와 같이, 데이터 획득 모듈은 현재 상태의 프레임 데이터를 획득하고, 프레임 데이터를 특징 추출 모듈에 전송하고; 특징 추출 모듈은 프레임 데이터에서 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 판독하여, 상태 특징을 획득하고, 상태 특징을 상태 추론 모듈에 전송한다. 상태 추론 모듈은 N개의 스케줄링 정책에 따라 상태 특징에 대한 추론을 수행하여, 대상 가상 객체가 N개의 스케줄링 정책을 실행한 후에 상태 특징으로부터 변환된 후속 상태 특징들(9041, 9042 및 9043)을 획득하고, 이 후속 상태 특징들을 예측 모듈에 전송한다. 예측 모듈은 트레이닝된 가치 네트워크 모델을 사용하여 상태 특징들(9041, 9042 및 9043)에 대한 예측을 수행하고, 상태 특징들(9041, 9042 및 9043)에 각각 대응하는 기대 수익(9051, 9052 및 9053)을 출력한다. 3. 제어 모듈 제어 모듈은 예측 모듈에 의해 출력된 기대 수익(9051, 9052, 및 9053)에 따라, 기대 수익이 가장 높 은 스케줄링 정책을 대상 스케줄링 정책으로서 사용하고, 대상 가상 객체를 제어하여 대상 스케줄링 정책을 실 행한다. 예시적인 실시예에서, 본 출원의 예시적인 스케줄링 흐름도인 도 10에 도시된 바와 같이, 프로세스는 다음의 단 계들을 포함한다: 단계 S1001에서, 데이터 획득 모듈은 프레임 데이터를 획득하고, 여기서 프레임 데이터는 단계 S1001에서의 글 로벌 맵에 대응하는 데이터일 수 있고; 단계 S1002에서, 특징 추출 모듈은 프레임 데이터에서 연관된 가상 객체 들의 상태들(즉, 홈 캠프의 영웅의 상태 및 홈 캠프의 영웅의 시야 내의 적 캠프의 영웅의 상태) 및 가상 리소 스들의 상태들(중립 생물들의 상태들 및 양측의 요새들의 상태를 포함함)을 판독하고, 현재 상황 상태의 상태 특징 S를 추출하고; 단계 S1003에서, 상태 추론 모듈은 상태 특징 S에 대해 추론을 수행하여 복수의 후속 상태 특징에 의해 형성된 상태 특징 공간 S'을 획득하고; 단계 S1004에서, 예측 모듈이 기대 수익이 가장 높은 스케 줄링 정책을 결정한 후에, 제어 모듈은 대상 가상 객체를 제어하여 수익이 가장 높은 스케줄링 정책을 실행한다 (대상 가상 객체는 홈 캠프 내의 임의의 영웅일 수 있고, 도 10의 단계 S1004에서의 화살표 방향은 결정된 스케 줄링 정책이다). 도 11은 본 출원의 일부 예시적인 실시예들에 따른 가상 환경에서 가상 객체들을 스케줄링하기 위한 장치의 구 조 블록도이다. 본 장치는 소프트웨어, 하드웨어, 또는 이들의 조합을 사용함으로써 도 1의 실시예에서 서버 로서 구현될 수 있다. 본 장치는 데이터 획득 모듈, 특징 추출 모듈, 상태 추론 모듈, 예측 모듈, 제어 모듈, 보상 추출 모듈, 머신 학습 트레이닝 모듈, 및 상태 샘플링 모 듈을 포함한다. 보상 추출 모듈, 머신 학습 트레이닝 모듈, 및 상태 샘플링 모듈은 선택적인 모듈들이다. 데이터 획득 모듈은 실행 동안 가상 환경의 응용 프로그램에 의해 발생되는 프레임 데이터를 획득하도록 구성되고;특징 추출 모듈은 프레임 데이터에 대해 특징 추출을 수행하여 현재 상황 상태에서 적어도 2개의 가상 객 체 중 대상 가상 객체의 상태 특징을 획득하도록 구성되며, 상태 특징은 연관된 가상 객체들의 상태들 및 가상 리소스들의 상태들을 포함한다. 상태 추론 모듈은 N개의 스케줄링 정책에 따라 상태 특징에 대한 추론을 수행하여, N개의 후속 상태 특징 을 획득하도록 구성되며, N은 2 이상의 양의 정수이다. 예측 모듈은 가치 네트워크 예측 모델을 호출하여 N개의 후속 상태 특징을 처리함으로써, 대상 가상 객체 에 의해 N개의 스케줄링 정책을 실행하는 것의 기대 수익을 획득하도록 구성된다. 제어 모듈은 대상 가상 객체를 제어하여 N개의 스케줄링 정책에서 기대 수익이 가장 높은 스케줄링 정책 을 실행하도록 구성된다. 선택적인 실시예에서, 가상 환경의 맵은 n개의 이동 영역으로 분할되고, n은 맵의 픽셀 값보다 작고, n은 2 이 상의 양의 정수이다. 상태 추론 모듈은: 대상 가상 객체에 대응하는 m개의 거동을 획득하고- m은 1 이상의 양수임 -; 대상 가 상 객체에 대응하는 m개의 거동 및 n개의 이동 영역에 따라 N개의 스케줄링 정책을 획득하고- 스케줄링 정책들 은 n개의 이동 영역 중 i번째 이동 영역으로 이동하여 m개의 거동 중 j번째 거동을 수행하는 것을 포함하고, i 와 j는 양의 정수이고, 1≤i≤n, 1≤j≤m임 -; N개의 스케줄링 정책을 상태 특징에 적용하여, 미리 설정된 상태 천이 관계에 따라 N개의 후속 상태 특징을 획득하도록 추가로 구성된다. 선택적인 실시예에서, 가상 환경의 맵은 n개의 이동 영역의 어레이로 균등하게 분할되고; 대안적으로, 맵은 가 상 리소스들의 위치들에 따라 n개의 이동 영역으로 분할된다. 선택적인 실시예에서, 특징 추출 모듈은 프레임 데이터에서 연관된 가상 객체들의 상태들 및 가상 리소스 들의 상태들을 판독하고 상태 특징을 추출하도록 추가로 구성된다. 연관된 가상 객체들의 상태들은 연관된 가 상 객체들의 위치들, 체력 포인트들, 공격력 값들, 방어력 값들, 경험치들, 살상 카운트들, 사망 카운트들, 또 는 경제적 가치들 중 적어도 하나를 포함하고; 가상 리소스들의 상태들은 가상 리소스들의 위치들, 체력 포인트 들, 공격력 값들, 방어력 값들, 내구성 값들, 또는 점유 상태들 중 적어도 하나를 포함한다. 선택적인 실시예에서, 적어도 2개의 가상 객체는 적어도 2개의 상호 적대적 캠프에 각각 속하고; 가상 리소스는 건물, 중립 생물, 군대, 운송 수단, 무기, 또는 탄약 중 적어도 하나를 포함하고; 점유 상태는 중립 상태, 제어 된 상태, 점유된 상태, 또는 경쟁 상태를 포함하고; 중립 상태는 가상 리소스가 어떠한 캠프에도 속하지 않는다 는 것을 나타내는데 사용되고; 제어된 상태는 가상 리소스가 위치하는 이동 영역 내에 동일한 캠프에 속하는 적 어도 하나의 가상 객체가 존재한다는 것을 나타내는데 사용되고; 점유된 상태는 가상 리소스가 속하는 캠프를 나타내는데 사용되고; 경쟁 상태는 가상 리소스가 위치하는 이동 영역 내에 상이한 캠프들에 속하는 적어도 2개 의 가상 객체가 존재한다는 것을 나타내는데 사용된다. 선택적인 실시예에서, 가치 네트워크 예측 모델은 샘플 데이터 세트들의 트레이닝에 기초하여 획득되는 기대 수 익 규칙(expected-return rule)을 나타내는데 사용되고, 샘플 데이터 세트들은: 샘플 상태 특징들 및 샘플 상태 특징들에 대응하는 실제 수익을 포함한다. 데이터 획득 모듈은 p개의 샘플 프레임 데이터를 획득하도록 추가로 구성되고, p는 2 이상의 양의 정수이 다. 특징 추출 모듈은 p개의 샘플 프레임 데이터에 대해 특징 추출을 수행하여 각각의 샘플 프레임 데이터의 샘플 상태 특징을 획득하도록 추가로 구성된다. 샘플 상태 특징은 연관된 가상 객체들의 상태들 및 가상 리소 스들의 상태들을 포함한다. 보상 추출 모듈은 p개의 샘플 상태 특징 각각의 실제 수익을 계산하도록 구성된다. 머신 학습 트레이닝 모듈은: p개의 샘플 상태 특징을 원래의 가치 네트워크 예측 모델에 입력하여 샘플 상태 특징들 각각의 트레이닝 결과를 획득하고; 샘플 데이터 세트 각각에 대해, 트레이닝 결과를 샘플 상태 특 징의 실제 수익과 비교하여, 계산 손실을 획득하고- 계산 손실은 트레이닝 결과와 샘플 상태 특징의 실제 수익 사이의 오차를 표시하는데 사용됨 -; 적어도 하나의 샘플 데이터 세트에 대응하는 각각의 계산 손실에 따라 오 차 역 전파 알고리즘을 사용하여 트레이닝을 통해 가치 네트워크 예측 모델을 획득하도록 구성된다. 선택적인 실시예에서, 데이터 획득 모듈은 실행 동안 가상 환경의 응용 프로그램에 의해 생성되는 l개의 이력 프레임 데이터를 획득하도록 추가로 구성되고, l은 p 이상의 양의 정수이다. 상태 샘플링 모듈은 미리 설정된 시간 단계에 따라 l개의 이력 프레임 데이터를 추출하여, q개의 후보 프 레임 데이터를 획득하고- q는 양의 정수이고, p≤q≤l임 -; q개의 후보 프레임 데이터를 판독하고, q개의 후보 프레임 데이터에서, 공격 거동을 포함하는 후보 프레임 데이터를 샘플 프레임 데이터라고 판정하여, p개의 샘플 프레임 데이터를 획득하도록 구성된다. 보상 추출 모듈은 샘플 상태 특징에서 대상 가상 객체의 거동 및 상태에 따라 각각의 샘플 상태 특징의 즉각적인 보상을 계산하고; p개의 샘플 프레임 데이터 중 i번째 샘플 프레임 데이터에 대해, i번째 샘플 프레임 데이터의 즉각적인 보상 및 (i+1)번째 샘플 프레임 데이터의 즉각적인 보상에 따라 i번째 샘플 프레임 데이터의 실제 수익을 계산하도록 추가로 구성되고, i는 p 이하의 양의 정수이다. 도 12는 본 출원의 일부 예시적인 실시예들에 따른 컴퓨터 디바이스의 개략적인 구조도이다. 컴퓨터 디바이스 는 도 1의 실시예에서 서버일 수 있다. 구체적으로, 컴퓨터 디바이스는 중앙 처리 유닛 (CPU), 랜덤 액세스 메모리(RAM) 및 판독 전용 메모리(ROM)를 포함하는 시스템 메모리 , 및 시스템 메모리와 CPU를 접속하는 시스템 버스를 포함한다. 컴퓨터 디바이스 는 컴퓨터 내의 컴포넌트들 사이에서 정보를 송신하는 것을 돕는 기본 입출력(I/O) 시스템과, 운영 체제, 응용 프로그램, 및 다른 프로그램 모듈을 저장하도록 구성된 대용량 저장 디바이스 를 추가로 포함한다. 기본 I/O 시스템은 정보를 표시하도록 구성된 디스플레이와, 마우스 또는 키보드와 같은, 정보를 입력하기 위해 사용자에 의해 사용되는 입력 디바이스를 포함한다. 디스플레이와 입력 디바이스 는 모두 시스템 버스에 접속된 입출력 제어기를 사용하여 CPU에 접속된다. 기본 I/O 시스템은 키보드, 마우스, 또는 전자 스타일러스와 같은, 복수의 다른 디바이스로부터 입력을 수신하고 처리하기 위한 입출력 제어기를 추가로 포함할 수 있다. 유사하게, 입출력 제어기는 디스플레이 화면, 프린터, 또는 다른 타입의 출력 디바이스에 출력을 추가로 제공한다. 대용량 저장 디바이스는 시스템 버스에 접속된 대용량 저장 제어기(도시되지 않음)를 사용하여 CPU에 접속된다. 대용량 저장 디바이스 및 그의 연관된 컴퓨터 판독가능 저장 매체는 컴퓨터 디바 이스에 대한 비휘발성 스토리지를 제공한다. 즉, 대용량 저장 디바이스는 하드 디스크 또는 CD- ROM 드라이브와 같은, 컴퓨터 판독가능 저장 매체(도시되지 않음)를 포함할 수 있다. 일반성을 잃지 않고, 컴퓨터 판독가능 저장 매체는 컴퓨터 저장 매체 및 통신 매체를 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈, 또는 기타 데이터와 같은 정보를 저장하기 위해 사용되는 임의의 방법 또는 기술을 사용하여 구현되는 휘발성 및 비휘발성 매체, 및 이동식 및 비이동식 매체를 포함한다. 컴퓨터 저장 매체는 RAM, ROM, EPROM, EEPROM, 플래시 메모리, 또는 다른 고체 상태 저장 기 술, CD-ROM, DVD, 또는 다른 광학 스토리지, 자기 카세트, 자기 테이프, 자기 디스크 스토리지, 또는 다른 자기 저장 디바이스를 포함한다. 명백히, 컴퓨터 저장 매체가 전술한 몇몇 타입들에 제한되지 않는다는 것이 본 기 술분야의 통상의 기술자에 의해 알려질 수 있다. 시스템 메모리와 대용량 저장 디바이스는 총괄하 여 메모리라고 지칭될 수 있다. 메모리는 하나 이상의 프로그램을 저장한다. 하나 이상의 프로그램은 하나 이상의 CPU에 의해 실행되도 록 구성되고, 가상 환경에서 가상 객체들을 스케줄링하는 전술한 방법을 구현하기 위한 명령어들을 포함한다. CPU는 전술한 방법 실시예들에서 제공되는 가상 환경에서 가상 객체들을 스케줄링하는 방법을 구현하기 위해 하나 이상의 프로그램을 실행한다. 본 출원의 다양한 실시예들에 따르면, 컴퓨터 디바이스는 인터넷과 같은 네트워크를 통해, 실행 동안 네 트워크 상의 원격 컴퓨터에 추가로 접속될 수 있다. 즉, 컴퓨터 디바이스는 시스템 버스에 접속된 네트워크 인터페이스 유닛을 사용하여 네트워크에 접속될 수 있거나, 또는 네트워크 인터페이스 유 닛을 사용하여 다른 타입의 네트워크 또는 원격 컴퓨터 시스템(도시되지 않음)에 접속될 수 있다. 메모리는 하나 이상의 프로그램을 추가로 포함한다. 하나 이상의 프로그램은 메모리에 저장되고, 본 개시내용 의 실시예들에서 제공되는 가상 환경에서 가상 객체들을 스케줄링하는 방법을 구현하기 위해 컴퓨터 디바이스에 의해 수행되는 단계들을 포함한다. 본 출원의 실시예는 컴퓨터 판독가능 저장 매체를 추가로 제공하고, 저장 매체는 적어도 하나의 명령어, 적어도 하나의 프로그램, 코드 세트, 또는 명령어 세트를 저장하고, 적어도 하나의 명령어, 적어도 하나의 프로그램,코드 세트 또는 명령어 세트는 전술한 실시예들 중 어느 하나에 따른 가상 환경에서 가상 객체들을 스케줄링하 기 위한 방법을 구현하기 위해 프로세서에 의해 로딩되고 실행된다. 본 출원은 컴퓨터 프로그램 제품을 추가로 제공하고, 컴퓨터 프로그램 제품은 컴퓨터 상에서 실행될 때, 컴퓨터 로 하여금 전술한 방법 실시예들에 따른 가상 환경에서 가상 객체들을 스케줄링하는 방법을 수행하게 한다. 명세서에서 언급된 \"복수\"는 2개 이상을 의미한다. \"및/또는\"은 연관된 객체들을 설명하기 위한 연관 관계를 기술하고 3개의 관계가 존재할 수 있다는 것을 나타낸다. 예를 들어, A 및/또는 B는 다음 세 가지 경우를 표현 할 수 있다: A만 존재하고, A와 B가 모두 존재하고, 및 B만 존재하는 것. 문자 \"/\"는 일반적으로 연관된 객체 들 사이의 \"또는\" 관계를 표시한다. 본 출원의 전술한 실시예들의 시퀀스 번호들은 단지 설명 목적을 위한 것이고, 실시예들 사이의 선호도를 표시 하는 것은 아니다. 본 분야의 통상의 기술자라면, 전술한 실시예들의 단계들의 전부 또는 일부가 하드웨어를 사용하여 구현될 수 있거나, 또는 관련 하드웨어에게 지시하는 프로그램에 의해 구현될 수 있다는 것을 이해할 수 있을 것이다. 프 로그램은 컴퓨터 판독가능 저장 매체에 저장될 수 있다. 저장 매체는 ROM, 자기 디스크, 광학 디스크 등일 수 있다. 전술한 설명들은 본 출원의 단지 예시적인 실시예들일 뿐이며, 본 출원을 제한하려고 의도된 것은 아니다. 본 출원의 사상 및 원리 내에서 이루어진 임의의 수정, 균등한 대체, 또는 개선은, 본 출원의 보호 범위 내에 든다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12"}
{"patent_id": "10-2021-7005883", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 출원의 실시예들에서의 기술적 해결책들을 더 명확하게 설명하기 위해, 실시예들의 설명에 필요한 첨부 도면 들이 아래에 간략하게 설명된다. 명백하게, 다음 설명에서의 첨부 도면들은 일부 실시예만을 도시한다. 명확"}
{"patent_id": "10-2021-7005883", "section": "도면", "subsection": "도면설명", "item": 2, "content": "하게, 다음 설명에서의 첨부 도면들은 이 출원의 일부 실시예를 단지 도시하고, 본 기술분야의 통상의 기술자는 창조적인 노력들없이 이 첨부 도면들로부터 다른 도면들을 여전히 유도할 수도 있다. 도 1은 본 출원의 일부 예시적인 실시예들에 따른 컴퓨터 시스템의 구조 블록도이다.도 2는 본 출원의 일부 예시적인 실시예들에 따른 가상 환경에서 가상 객체들을 스케줄링하는 방법의 흐름도이 다. 도 3은 본 출원의 일부 예시적인 실시예들에 따른 가치 네트워크 예측 모델(value network prediction model)을 트레이닝하기 위한 방법의 흐름도이다. 도 4는 본 출원의 일부 예시적인 실시예들에 따른 가상 환경의 맵의 개략도이다. 도 5는 본 출원의 일부 예시적인 실시예들에 따른 가상 환경에서 가상 객체들을 스케줄링하는 방법의 흐름도이 다. 도 6은 본 출원의 일부 예시적인 실시예들에 따른 상태 특징 추론의 효과도이다. 도 7은 본 출원의 일부 예시적인 실시예들에 따른 가상 장치의 전체 아키텍처 도면이다. 도 8은 본 출원의 일부 예시적인 실시예들에 따른 트레이닝 모듈의 작업 흐름도이다. 도 9는 본 출원의 일부 예시적인 실시예들에 따른 예측 모듈의 작업 흐름도이다. 도 10은 본 출원의 일부 예시적인 실시예들에 따른 예측 모듈의 개략적인 작업도이다. 도 11은 본 출원의 일부 예시적인 실시예들에 따른 가상 환경에서 가상 객체들을 스케줄링하기 위한 장치의 블 록도이다. 도 12는 본 출원의 일부 예시적인 실시예들에 따른 컴퓨터 디바이스의 구조 블록도이다."}
