{"patent_id": "10-2017-0074424", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2018-0057489", "출원번호": "10-2017-0074424", "발명의 명칭": "감정을 생성하여 표현하는 로봇 시스템과, 그 시스템에서의 감정 생성 및 표현 방법", "출원인": "주식회사 로보러스", "발명자": "김대훈"}}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "외부의 영상으로부터 얼굴 영상을 인식하는 얼굴 인식부; 외부의 음성을 인식하는 음성 인식부;신경망 모델을 사용하여 얼굴 영상과 음성에 대응되는 사용자의 감정을 학습하고, 학습 결과를 사용하여 상기얼굴 인식부에서 인식되는 얼굴 영상과 상기 음성 인식부에 의해 인식되는 음성 인식 결과에 대응되는 사용자의감정을 인식하는 감정 인식부;퍼지 추론(Fuzzy Inference)을 사용하여 사용자의 감정 입력에 대응되는 로봇 시스템의 감정 생성을 학습하고,학습 결과를 사용하여 상기 감정 인식부에 의해 인식되는 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성하는 감정 생성부; 및상기 감정 생성부에 의해 생성되는 로봇 시스템의 감정을 외부로 표현하는 감정 표현부를 포함하는 로봇 시스템."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 감정 생성부는 사용자의 감정을 외부의 클라우드 서버 ― 여기서 클라우드 서버는 퍼지 추론 사용한 학습이 가능한 서버임 ―로 전달하여 사용자의 감정에 대응되는 로봇 시스템의 감정 생성 학습을 수행시킨 후,상기 감정 인식부로부터 입력되는 사용자의 감정을 상기 클라우드 서버로 전달하여 상기 사용자의 감정에 대응되는 로봇 시스템의 감정 생성을 요청한 후 상기 클라우드 서버로부터 전달되는 로봇 시스템의 감정 생성 결과를 수신하여 로봇 시스템의 감정으로 사용하는,로봇 시스템."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 신경망 모델은 합성곱 신경망(Convolutional Neural Network, CNN) 모델 모델인,로봇 시스템."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 또는 제2항에 있어서,상기 감정 생성부는,상기 감정 인식부에 의해 인식되는 사용자의 감정을 입력으로 상기 퍼지 시스템을 사용하여 로봇 시스템의 감정상태를 감정 이모지값으로 생성하는 감정 이모지값 생성부;상기 감정 인식부에 의해 인식되는 사용자의 감정을 입력으로 상기 퍼지 시스템을 사용하여 로봇 시스템의 감정상태를 감정 LED 컬러값으로 생성하는 감정 LED 컬러값 생성부;상기 감정 인식부에 의해 인식되는 사용자의 감정을 입력으로 상기 퍼지 시스템을 사용하여 로봇 시스템의 감정상태를 감정 음성값으로 생성하는 감정 음성값 생성부; 및상기 감정 인식부에 의해 인식되는 사용자의 감정을 입력으로 상기 퍼지 시스템을 사용하여 로봇 시스템의 감정상태를 감정 모션값으로 생성하는 감정 모션값 생성부를 포함하는, 로봇 시스템.공개특허 10-2018-0057489-3-청구항 5 제1항에 있어서,상기 감정 생성부에서의 학습 결과 정보를 저장하는 저장부를더 포함하고,상기 감정 생성부는 상기 저장부에 저장된 학습 결과 정보에 기초하여, 상기 감정 인식부로부터 입력되는 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성하는,로봇 시스템."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제2항에 있어서,상기 감정 생성부는 상기 클라우드 서버가 제공하는 API(Application Programming Interface)를 사용하여 상기클라우드 서버에 접속하여 사용자의 감정에 대응되는 감정 생성의 학습을 요청하고, 학습 결과에 기초하여 상기감정 인식부에서 출력되는 사용자의 감정에 대응되는 로봇 시스템의 감정 생성을 요청하고, 상기 클라우드 서버에서 제공되는 로봇 시스템의 감정 생성값을 수신하여 사용하는,로봇 시스템."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4항에 있어서,상기 감정 이모지값 생성부에서 생성되는 감정 이모지값을 사용하여 로봇 시스템의 감정을 표현하는디스플레이;상기 감정 LED 컬러값 생성부에서 생성되는 감정 LED 컬러값을 사용하여 로봇 시스템의 감정을 표현하는 복수의LED;상기 감정 음성값 생성부에서 생성되는 감정 음성값을 사용하여 로봇 시스템의 감정을 표현하는 음성 출력부;및상기 감정 모션값 생성부에서 생성되는 감정 모션값을 사용하여 로봇 시스템의 모션을 제어하여 로봇 시스템의감정을 표현하는 모션 출력부를 더 포함하는 로봇 시스템."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "로봇 시스템이 감정을 생성하여 표현하는 방법에 있어서,얼굴 영상에 대응되는 사용자의 감정을 신경망 모델을 사용하여 학습하고, 얼굴 영상과 음성에 대응되는 로봇시스템의 감정 생성을 퍼지 추론을 사용하여 학습하는 단계;실시간으로 입력되는 사용자의 촬영 영상과 음성으로부터 사용자의 얼굴과 음성을 인식하는 단계; 상기 학습의 정보에 기초하여, 인식되는 얼굴 영상과 음성 인식 결과에 대응되는 사용자의 감정을 인식하는 단계; 및상기 학습의 정보에 기초하여, 인식되는 상기 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성하여 표현하는 단계를 포함하는 로봇 시스템의 감정 생성 및 표현 방법."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,외부의 클라우드 서버 ― 여기서 클라우드 서버는 퍼지 추론 사용한 학습이 가능한 서버임 ―를 사용하여 사용공개특허 10-2018-0057489-4-자의 감정에 대응되는 로봇 시스템의 감정 생성 학습을 수행시킨 후,상기 인식되는 사용자의 감정을 상기 클라우드 서버로 전달하여 상기 사용자의 감정에 대응되는 로봇 시스템의감정을 생성하는,로봇 시스템의 감정 생성 및 표현 방법."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항 또는 제9항에 있어서,상기 신경망 모델은 합성곱 신경망(Convolutional Neural Network, CNN) 모델 모델이며,상기 합성곱 신경망 모델은 입력되는 얼굴 영상과 상기 음성 인식 결과에 대해 분노(Anger), 행복(Happiness),놀람(Surprise), 혐오(Disgust), 슬픔(Sadness), 공포(Fear) 및 무표정(Neutral) 중 하나를 사용자의 감정으로분류하는,로봇 시스템의 감정 생성 및 표현 방법."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항 또는 제9항에 있어서,상기 로봇 시스템의 감정을 생성하여 표현하는 단계에서,로봇 시스템의 감정을 디스플레이를 통해 표현하는 데 사용되는 감정 이모지값, 로봇 시스템의 감정을 복수의LED를 통해 표현하는 데 사용되는 감정 LED 컬러값, 로봇 시스템의 감정을 음성 출력을 통해 표현하는 데 사용되는 감정 음성값, 및 로봇 시스템의 감정을 상기 로봇 시스템의 모션을 통해 표현하는 데 사용되는 감정 모션값을 생성하는,로봇 시스템의 감정 생성 및 표현 방법."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "감정을 생성하여 표현하는 로봇 시스템으로서,입력기, 출력기, 메모리 및 프로세서를 포함하며,상기 입력기는 사용자의 영상 및 음성을 입력받고,상기 출력기는 상기 로봇 시스템의 감정을 외부로 표현하며,상기 메모리는 코드의 집합을 저장하도록 구성되고,상기 코드는,상기 입력기로부터 입력되는 사용자의 영상으로부터 사용자의 얼굴을 인식하고, 사용자의 음성을 인식하는동작;신경망 모델을 통해 얼굴 인식 결과와 음성 인식 결과에 대응되는 사용자의 감정을 인식하는 동작;퍼지 추론을 통해 사용자의 감정에 대응되는 로봇 시스템의 감정 생성을 학습하는 동작; 및학습된 정보에 기초하여 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성하여 표현하는 동작을 실행하도록 상기 프로세서를 제어하는 데 사용되는, 로봇 시스템."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 프로세서는, 상기 입력기를 통해 입력되는 얼굴 영상과 음성에 대응되는 사용자의 감정에 대해 상기 신경망 모델을 사용하여공개특허 10-2018-0057489-5-학습하고, 학습 결과를 사용하여 상기 입력기를 통해 실시간으로 입력되는 얼굴 영상과 음성에 대응되는 사용자의 감정을 인식하는 동작을 더 실행하는,로봇 시스템."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서,상기 프로세서는,외부의 클라우드 서버 ― 여기서 클라우드 서버는 퍼지 추론 사용한 학습이 가능한 서버임 ―를 사용하여 사용자의 감정에 대응되는 로봇 시스템의 감정 생성 학습을 수행시키는 동작; 및상기 인식되는 사용자의 감정을 상기 클라우드 서버로 전달하여 상기 사용자의 감정에 대응되는 로봇 시스템의감정을 생성하는 동작을 더 실행하는, 로봇 시스템."}
{"patent_id": "10-2017-0074424", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12항에 있어서,상기 프로세서는 로봇 시스템의 감정을 상기 출력기를 통해 이미지로 표현하는 데 사용되는 감정 이모지값, 로봇 시스템의 감정을 상기 출력기를 통해 표현하는 데 사용되는 감정 LED 컬러값, 로봇 시스템의 감정을 음성 출력을 통해 표현하는 데 사용되는 감정 음성값, 및 로봇 시스템의 감정을 모션을 통해 표현하는 데 사용되는 감정 모션값을 생성하는 동작을 더 실행하는, 로봇 시스템."}
{"patent_id": "10-2017-0074424", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "감정을 생성하여 표현하는 로봇 시스템과, 그 시스템에서의 감정 생성 및 표현 방법이 개시된다. 이 시스템에서, 얼굴 인식부는 외부의 영상으로부터 얼굴 영상을 인식하고, 음성 인식부는 외부의 음성을 인식한 다. 감정 인식부는 신경망 모델을 사용하여 얼굴 영상과 음성에 대응되는 사용자의 감정을 학습하고, 학습 결과 를 사용하여 상기 얼굴 인식부에서 인식되는 얼굴 영상과 상기 음성 인식부에 의해 인식되는 음성 인식 결과에 대응되는 사용자의 감정을 인식한다. 감정 생성부는 퍼지 추론(Fuzzy Inference)을 사용하여 사용자의 감정 입 력에 대응되는 로봇 시스템의 감정 생성을 학습하고, 학습 결과를 사용하여 상기 감정 인식부에 의해 인식되는 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성한다. 감정 표현부는 상기 감정 생성부에 의해 생성되는 로봇 시스템의 감정을 외부로 표현한다."}
{"patent_id": "10-2017-0074424", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 감정을 생성하여 표현하는 로봇 시스템과, 그 시스템에서의 감정 생성 및 표현 방법에 관한 것이다."}
{"patent_id": "10-2017-0074424", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "컨시어지(concierge)란 본래 중세 시대에 성을 관리하는 '집사'를 가리키는 말이었으나, 지금은 호텔이나 백화 점 등의 업계에서 VIP 고객과 일대일로 대면하며 고객의 편의를 살피는 개념으로 사용되고 있다. 한편, 최근에는 정보 통신 기술 및 로봇 기술의 발전으로, 사람 대신에 로봇을 사용한 컨시어지 서비스는 물론 다양한 형태의 대면 서비스가 가능해졌다. 특히, 최근 유통업계에선 로봇을 이용한 서비스시 인공 지능 (Artifictial Intelligence, AI) 기술을 접목한 사례가 늘고 있다. 그러나, 로봇 기술에 인공 지능을 접목한 서비스의 경우 기술적인의 한계로 인해 현재 제공할 수 있는 서비스가 상품 선택 정도 등으로 다양한 서비스를 제공하기에는 부족한 현실이다. 따라서, 로봇을 이용한 컨시어지 서비스 등의 대면 서비스시 보다 다양한 기능을 제공할 수 있는 기술이 요구되 고 있으며, 특히 사용자의 얼굴이나 음성 인식에 기초하여 사용자의 감정을 인식하고, 인식되는 사용자의 감정 에 대응되는 로봇 시스템의 감정을 생성하여 표시할 수 있는 기술이 요구된다."}
{"patent_id": "10-2017-0074424", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 이루고자 하는 기술적 과제는 사용자의 감정 상태에 대응되는 감정을 생성하여 표현할 수 있는 로봇 시스템과, 그 시스템에서의 감정 생성 및 표현 방법을 제공한다."}
{"patent_id": "10-2017-0074424", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 한 특징에 따른 로봇 시스템은, 외부의 영상으로부터 얼굴 영상을 인식하는 얼굴 인식부; 외부의 음성을 인식하는 음성 인식부; 신경망 모델을 사용하여 얼굴 영상과 음성에 대응되는 사용자의 감정을 학습하고, 학습 결과를 사용하여 상기 얼굴 인식부에서 인식되는 얼굴 영상과 상기 음성 인식부에 의해 인식되는 음성 인식 결과에 대응되는 사용자의 감정을 인식하는 감정 인식부; 퍼지 추론(Fuzzy Inference)을 사용하여 사용자의 감정 입력에 대응되는 로봇 시스템의 감정 생성 을 학습하고, 학습 결과를 사용하여 상기 감정 인식부에 의해 인식되는 사용자의 감정에 대응되는 로봇 시스템 의 감정을 생성하는 감정 생성부; 및 상기 감정 생성부에 의해 생성되는 로봇 시스템의 감정을 외부로 표현하는 감정 표현부를 포함한다. 여기서, 상기 감정 생성부는 사용자의 감정을 외부의 클라우드 서버 ― 여기서 클라우드 서버는 퍼지 추론 사용 한 학습이 가능한 서버임 ―로 전달하여 사용자의 감정에 대응되는 로봇 시스템의 감정 생성 학습을 수행시킨 후, 상기 감정 인식부로부터 입력되는 사용자의 감정을 상기 클라우드 서버로 전달하여 상기 사용자의 감정에 대응되는 로봇 시스템의 감정 생성을 요청한 후 상기 클라우드 서버로부터 전달되는 로봇 시스템의 감정 생성 결과를 수신하여 로봇 시스템의 감정으로 사용한다. 또한, 상기 신경망 모델은 합성곱 신경망(Convolutional Neural Network, CNN) 모델 모델이다. 또한, 상기 감정 생성부는, 상기 감정 인식부에 의해 인식되는 사용자의 감정을 입력으로 상기 퍼지 시스템을 사용하여 로봇 시스템의 감정 상태를 감정 이모지값으로 생성하는 감정 이모지값 생성부; 상기 감정 인식부에 의해 인식되는 사용자의 감정을 입력으로 상기 퍼지 시스템을 사용하여 로봇 시스템의 감정 상태를 감정 LED 컬 러값으로 생성하는 감정 LED 컬러값 생성부; 상기 감정 인식부에 의해 인식되는 사용자의 감정을 입력으로 상기 퍼지 시스템을 사용하여 로봇 시스템의 감정 상태를 감정 음성값으로 생성하는 감정 음성값 생성부; 및 상기 감 정 인식부에 의해 인식되는 사용자의 감정을 입력으로 상기 퍼지 시스템을 사용하여 로봇 시스템의 감정 상태를 감정 모션값으로 생성하는 감정 모션값 생성부를 포함한다. 또한, 상기 감정 생성부에서의 학습 결과 정보를 저장하는 저장부를 더 포함하고, 상기 감정 생성부는 상기 저 장부에 저장된 학습 결과 정보에 기초하여, 상기 감정 인식부로부터 입력되는 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성한다. 또한, 상기 감정 생성부는 상기 클라우드 서버가 제공하는 API(Application Programming Interface)를 사용하 여 상기 클라우드 서버에 접속하여 사용자의 감정에 대응되는 감정 생성의 학습을 요청하고, 학습 결과에 기초 하여 상기 감정 인식부에서 출력되는 사용자의 감정에 대응되는 로봇 시스템의 감정 생성을 요청하고, 상기 클 라우드 서버에서 제공되는 로봇 시스템의 감정 생성값을 수신하여 사용한다. 또한, 상기 감정 이모지값 생성부에서 생성되는 감정 이모지값을 사용하여 로봇 시스템의 감정을 표현하는 디스 플레이; 상기 감정 LED 컬러값 생성부에서 생성되는 감정 LED 컬러값을 사용하여 로봇 시스템의 감정을 표현하 는 복수의 LED; 상기 감정 음성값 생성부에서 생성되는 감정 음성값을 사용하여 로봇 시스템의 감정을 표현하는 음성 출력부; 및 상기 감정 모션값 생성부에서 생성되는 감정 모션값을 사용하여 로봇 시스템의 모션을 제어하 여 로봇 시스템의 감정을 표현하는 모션 출력부를 더 포함한다. 본 발명의 다른 특징에 따른 로봇 시스템의 감정 생성 및 표현 방법은, 로봇 시스템이 감정을 생성하여 표현하는 방법으로서, 얼굴 영상에 대응되는 사용자의 감정을 신경망 모델을 사 용하여 학습하고, 얼굴 영상과 음성에 대응되는 로봇 시스템의 감정 생성을 퍼지 추론을 사용하여 학습하는 단 계; 실시간으로 입력되는 사용자의 촬영 영상과 음성으로부터 사용자의 얼굴과 음성을 인식하는 단계; 상기 학 습의 정보에 기초하여, 인식되는 얼굴 영상과 음성 인식 결과에 대응되는 사용자의 감정을 인식하는 단계; 및 상기 학습의 정보에 기초하여, 인식되는 상기 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성하여 표현하 는 단계를 포함한다. 여기서, 외부의 클라우드 서버 ― 여기서 클라우드 서버는 퍼지 추론 사용한 학습이 가능한 서버임 ―를 사용하 여 사용자의 감정에 대응되는 로봇 시스템의 감정 생성 학습을 수행시킨 후, 상기 인식되는 사용자의 감정을 상 기 클라우드 서버로 전달하여 상기 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성한다. 또한, 상기 신경망 모델은 합성곱 신경망(Convolutional Neural Network, CNN) 모델 모델이며, 상기 합성곱 신 경망 모델은 입력되는 얼굴 영상과 상기 음성 인식 결과에 대해 분노(Anger), 행복(Happiness), 놀람 (Surprise), 혐오(Disgust), 슬픔(Sadness), 공포(Fear) 및 무표정(Neutral) 중 하나를 사용자의 감정으로 분류한다. 또한, 상기 로봇 시스템의 감정을 생성하여 표현하는 단계에서, 로봇 시스템의 감정을 디스플레이를 통해 표현 하는 데 사용되는 감정 이모지값, 로봇 시스템의 감정을 복수의 LED를 통해 표현하는 데 사용되는 감정 LED 컬 러값, 로봇 시스템의 감정을 음성 출력을 통해 표현하는 데 사용되는 감정 음성값, 및 로봇 시스템의 감정을 상 기 로봇 시스템의 모션을 통해 표현하는 데 사용되는 감정 모션값을 생성한다. 본 발명의 또 다른 특징에 따른 로봇 시스템은, 감정을 생성하여 표현하는 로봇 시스템으로서, 입력기, 출력기, 메모리 및 프로세서를 포함하며, 상기 입력기는 사용자의 영상 및 음성을 입력받고, 상기 출력기는 상기 로봇 시스템의 감정을 외부로 표현하며, 상기 메모리는 코드의 집합을 저장하도록 구성되고, 상기 코드는, 상기 입력기로부터 입력되는 사용자의 영상으로부터 사용자 의 얼굴을 인식하고, 사용자의 음성을 인식하는 동작; 신경망 모델을 통해 얼굴 인식 결과와 음성 인식 결과에 대응되는 사용자의 감정을 인식하는 동작; 퍼지 추론을 통해 사용자의 감정에 대응되는 로봇 시스템의 감정 생 성을 학습하는 동작; 및 학습된 정보에 기초하여 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성하여 표 현하는 동작을 실행하도록 상기 프로세서를 제어하는 데 사용된다. 여기서, 상기 프로세서는, 상기 입력기를 통해 입력되는 얼굴 영상과 음성에 대응되는 사용자의 감정에 대해 상 기 신경망 모델을 사용하여 학습하고, 학습 결과를 사용하여 상기 입력기를 통해 실시간으로 입력되는 얼굴 영 상과 음성에 대응되는 사용자의 감정을 인식하는 동작을 더 실행한다. 또한, 상기 프로세서는, 외부의 클라우드 서버 ― 여기서 클라우드 서버는 퍼지 추론 사용한 학습이 가능한 서 버임 ―를 사용하여 사용자의 감정에 대응되는 로봇 시스템의 감정 생성 학습을 수행시키는 동작; 및 상기 인식 되는 사용자의 감정을 상기 클라우드 서버로 전달하여 상기 사용자의 감정에 대응되는 로봇 시스템의 감정을 생 성하는 동작을 더 실행한다. 또한, 상기 프로세서는 로봇 시스템의 감정을 상기 출력기를 통해 이미지로 표현하는 데 사용되는 감정 이모지 값, 로봇 시스템의 감정을 상기 출력기를 통해 표현하는 데 사용되는 감정 LED 컬러값, 로봇 시스템의 감정을 음성 출력을 통해 표현하는 데 사용되는 감정 음성값, 및 로봇 시스템의 감정을 모션을 통해 표현하는 데 사용 되는 감정 모션값을 생성하는 동작을 더 실행한다."}
{"patent_id": "10-2017-0074424", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 촬영되는 사용자의 영상 및 음성에 기초하여 실시간으로 사용자의 감정을 인식하고, 사용자 의 감정 상태에 대응되는 로봇의 감정을 생성할 수 있다. 따라서, 컨시어지 서비스를 제공하는 컨시어지 로봇 등에 탑재되어 사용자의 감정에 부합되어 자연스러운 대화 및 사용자 친화적인 대화가 가능하다. 또한, 사용자의 감정에 대응되는 로봇 시스템의 감정 생성을 수행하는 기능을 외부의 클라우드 서버를 사용하여 수행함으로써, 로봇 시스템의 구성이 간단해져서 컨시어지 로봇 등으로 사용이 용이하다. 이와 같이, 이미 검증되어 있고 매우 다양하고 많은 정보를 갖고 있는 클라우드 서버를 사용함으로써 로봇 시스 템에 대한 신뢰를 제공할 수 있다."}
{"patent_id": "10-2017-0074424", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위 해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 명세서 전체에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 명세서에 기재 된 \"…부\", \"…기\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드 웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 도 1은 본 발명의 실시예에 따른 감정을 생성하여 표현하는 로봇 시스템의 구성을 개략적으로 도시한 도면이다. 도 1에 도시된 바와 같이, 본 발명의 실시예에 따른 감정을 생성하여 표현하는 로봇 시스템은 영상 촬영부 , 얼굴 인식부, 사용자 인식부, 음성 입력부, 음성 인식부, 감정 인식부, 디스 플레이, 발광부, 음성 출력부, 모션 출력부, 감정 생성부, 감정 표현부, 저장 부, 및 제어부를 포함한다. 여기서, 영상 촬영부, 음성 입력부, 디스플레이, 발광부 및 음성 출력부는 사용자 인 터페이스 장치를 구성한다. 즉, 사용자로부터 정보를 획득하고, 획득한 정보에 기초하여 사용자에게 출력하는 인터페이스 기능을 한다. 영상 촬영부는 외부 영상을 촬영하여 영상에 대응되는 영상 신호를 출력한다. 영상 촬영부로는 디지 털 카메라인 CCD(Charge Coupled Device) 카메라가 사용될 수 있다. 얼굴 인식부는 영상 촬영부에서 출력되는 영상 신호에 대응되는 영상을 사용하여 영상 속 인물의 얼 굴을 인식한다. 사용자 인식부는 딥 러닝 기술의 신경망 모델, 특히 합성곱 신경망(Convolutional Neural Network, CNN) 모델을 이용하여 얼굴 인식부에서 출력되는 영상을 학습하여 기등록된 사용자에 대한 학습 정보를 생성할 수 있다. 그 후, 사용자 인식부는 기등록된 사용자에 대한 학습 정보에 기초하여, 이후 얼굴 인식부에 의해 인 식되는 얼굴 영상을 입력으로 하여 CNN 모델을 사용하여 기등록된 사용자 여부를 인식할 수 있다. 사용자 인식 부에 의해 기등록된 사용자가 아닌 것으로 인식되는 경우에는 해당 사용자에게 사용자로서의 등록 절차가 진행될 수 있다. 사용자 인식부는 동일인 및 동일인의 다른 이미지와 전혀 다른 대조군의 이미지를 통한 딥(deep) CNN 학습 을 통한 128개의 각각의 개인을 구분하는 특징점을 추출해서 숫자로 표현되는 암호화를 수행하여 학습 정보로서 저장한다.또한, 사용자 인식부는 SVM(Support Vector Machine) 알고리즘을 통해 기등록된 사용자인지의 여부를 인식 할 수 있다. 여기서, SVM은 데이터들을 분류하기 위한 최적의 분리 경계면을 제공하는 지도 학습 방법으로 최 적의 분리 경계면은 각 클래스의 데이터 사이의 중간에 위치하도록 학습된다. 음성 입력부는 외부로부터 음성을 입력받는 구성으로, 예를 들어 마이크로폰 어레이가 사용될 수 있다. 음성 인식부는 음성 입력부를 통해 입력되는 외부 음성을 인식한다. 이러한 음성 인식 기술에 대해 서는 이미 잘 알려져 있으므로 여기에서는 구체적인 설명을 생략한다. 감정 인식부는 전술한 CNN 모델을 이용하여 복수의 얼굴 영상을 학습하여 얼굴 영상이 나타내고 있는 사용 자의 감정에 대한 학습을 수행한다. 그 후, 감정 인식부는 얼굴 인식부에 의해 인식되는 얼굴 영상과 음성 인식부에 의한 음성 인식 결과를 입력으로 하여 CNN 모델을 사용하여 해당 얼굴에 대응되는 사용자 감정을 인식할 수 있다. 상기한 사용자 인식부 및 감정 인식부에서 사용되는 CNN 모델은 이미지 인식과 음성 인식 등 다양한 곳에서 사용되며, 특히 이미지 인식 분야에서 딥러닝(Deep Learning)을 활용한 기법은 거의 다 CNN을 기반으로 한다. 이러한 CNN은 합성곱 계층(Convolutional Layer)과 신경망(Neural Network)으로 구분된다. 여기서, 신경망은 완전 연결 계층(Fully Connected Layer)과 분류 계층을 포함한다. 합성곱 계층은 입력 영상으로부터 특징을 추출하는 역할을 수행한다. 신경망의 완전 연결 계층(Fully Connected Layer)과 분류 계층은 합성곱 계층에서 추출되는 특징 값을 사용하여 분류를 수행한다. 이러한 신경망은 CNN 이전의 기존의 신경망으로 이전 계층의 모든 뉴런과 결합되어 있는 형 태를 말한다. 분류 계층에서는 Softmax 함수가 사용되며, 이러한 Softmax 함수는 전술한 ReLu와 같은 활성 함 수의 일종이다. 감정 인식부는 도 2에 도시된 바와 같이, 얼굴 인식부에서 인식되는 얼굴 영상과 음성 인식부에 서의 음성 인식 결과에 대해 CNN 모델을 사용하여 사용자의 감정을 인식한다. 예를 들어, 도 2에 도시된 바와 같이, 얼굴 영상과 음성 인식 결과에 대해 학습된 CNN 모델을 통해 본 발명의 실시예에서 학습된 7가지 감정, 즉 분노, 행복, 놀람, 혐오, 슬픔, 공포 및 무표정의 감정 중에서 한 가지 감정을 인식하고, 그 결과를 출력한 다. 디스플레이는 로봇 시스템이 수행하는 동작의 정보를 표시하고, 사용자 인식부에 의해 인식되는 사용자 인식 정보를 표시하며, 감정 인식부에 의해 인식되는 사용자의 감정 상태를 표시한다. 또한, 디스플레이는 로봇 시스템의 감성 생성 정보를 표시하고, 로봇 시스템의 감정을 표시한다. 이러한 디스플레이는 LCD(Liquid Crystal Display), LED(Light Emitting Diode) 디스플레이, 박막 트 랜지스터 액정 디스플레이(Thin Film Transistor-Liquid Crystal Display, TFT LCD), 유기 발광 다이오드 (Organic Light-Emitting Diode, OLED), 플렉시블 디스플레이(Flexible Display), 전계 방출 디스플레이(Feld Emission Display, FED), 3차원 디스플레이(3D Display) 중에서 적어도 하나를 포함할 수 있다. 또한, 디스플 레이는 다수개가 구비될 수도 있다. 발광부는 로봇 시스템이 출력하는 제어 신호에 따라 구동하는 적어도 하나의 LED(Light Emitting Diode)를 포함할 수 있으며, 기본적으로 로봇 시스템의 감정을 표시하는 데 사용된다. 음성 출력부는 음성을 외부로 출력하며, 이러한 음성 출력부는 TTS(Text to Speech) 기능을 구비한 스피커일 수 있다. 모션 출력부는 로봇 시스템의 제어에 의해 구동 신호를 발생하는 액추에이터 구동부(미도시)와, 액추 에이터 구동부(미도시)의 구동 신호에 따라 구동되어 미리 설정된 각도의 범위에서 회전하거나, 미리 설정된 방 향으로 움직이는 작동 부재를 포함할 수 있다. 즉, 모션 출력부는 로봇 시스템의 모션을 제어하기 위한 구동 신호를 출력하며, 기본적으로 로봇 시스템의 감정을 표시하는 데 사용된다. 감정 생성부는 감정 인식부에 의해 인식되는 사용자의 감정 상태를 입력으로 하여 퍼지 추론 엔진 (Fuzzy Inference Engine)을 사용하는 퍼지 시스템을 사용하여 로봇 시스템의 감정 상태를 정의하여 로봇시스템의 감정을 생성한다. 한편, 감정 생성부에서 사용되는 퍼지 이론은 애매하고 불분명한 상황에서 여러 문제들을 두뇌가 판단 결 정하는 과정에 대하여 수학적으로 접근하려는 이론이다. 이러한 퍼지 이론에 따른 퍼지 시스템은 소속함수와 규칙을 기반으로 언어적 변수를 표현하기에 적절하기 때문에 비선형성이 강하고 복잡한 시스템을 비교적 쉽게 제어할 수 있는 장점을 가지고 있는 구조이다. 이러한 퍼지 시스템은 도 3에 개략적으로 도시된 바와 같이, 퍼지 규칙 베이스(Fuzzy Rule Base), 퍼 지화기(Fuzzifier), 퍼지 추론 엔진 및 비퍼지화기(Defuzzifier)를 포함한다. 퍼지 규칙 베이스는 제어 대상에 대한 지식과 제어 방법을 나타내며 퍼지 IF-THEN 규칙들로 구성되어 있다. 퍼지화기는 실제 입력 변수가 입력되면 0과 1 사이의 값으로 나타낸 퍼지 집합으로 변환하여 출력한다. 퍼지 추론 엔진은 퍼지화기를 통해 출력되는 퍼지화된 입력값과 퍼지 규칙 베이스를 기반으로 출력 값을 적절하게 추론하는 논리 연산 부분이다. 이것은 퍼지 규칙 베이스에 존재하는 퍼지 IF-THEN 규 칙과 입력에 대한 소속함수(U)의 값으로 출력에 대한 소속함수의 값을 결정한다. 비퍼지화기는 퍼지 추론 엔진을 통해 추론 과정을 거친 퍼지 집합을 실제 일반적인 값으로 변환하여 출력한다. 제어 대상의 제어 입력 값은 일반적인 집합의 값을 사용하므로 언어적으로 표현된 값을 정량적인 값 으로 변환하여 제어 입력으로 만드는 과정이다. 한편, 감정 생성부는 도 4에 도시된 감정 기준 맵에 감정 인식부에 의해 인식되는 사용자 감정 인식 결과를 적용하여 도 5에 도시된 바와 같은 감정 적용 기준 맵을 사용하여 퍼지 시스템에 의한 퍼지 추론을 수행하고, 그 결과를 로봇 시스템의 감정으로 정의할 수 있다. 감정 표현부는 감정 생성부에 의해 생성되는 로봇 시스템의 감정을 외부로 표현하기 위해 디스 플레이, 발광부, 음성 출력부 및 모션 출력부에 대한 제어를 수행한다. 저장부는 로봇 시스템이 동작하는데 필요한 데이터와 프로그램 등을 저장할 수 있다. 일 예로, 저장 부는 기등록된 사용자와 관련된 정보를 저장할 수 있다. 또한, 저장부는 로봇 시스템의 동작 수행을 위한 명령 등을 저장할 수도 있다. 또한, 저장부는 사용자 인식부와 감정 인식부에서의 학습 정보와 감정 생성부에서의 퍼지 추론 결과를 각각 저장할 수 있다. 이러한 저장부는 플래시 메모리 타입(Flash Memory Type), 하드 디스크 타입(Hard Disk Type), 멀티미디 어 카드 마이크로 타입(Multimedia Card Micro Type), 카드 타입의 메모리(예를 들면, SD 또는 XD 메모리 등), 자기 메모리, 자기 디스크, 광디스크, 램(Random Access Memory, RAM), SRAM(Static Random Access Memory), 롬(Read-Only Memory, ROM), PROM(Programmable Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory) 중 적어도 하나의 저장매체를 포함할 수 있다. 제어부는 로봇 시스템의 각 구성요소, 즉 영상 촬영부, 얼굴 인식부, 사용자 인식부, 음성 입력부, 음성 인식부, 감정 인식부, 디스플레이, 발광부, 음성 출력부, 모션 출력부, 감정 생성부, 감정 표현부 및 저장부 간에 전달되는 신호를 처리한다. 상기한 로봇 시스템은 로봇의 형태로 구현될 수 있으며, 한 예시로 도 6 및 도 7과 같다. 도 6은 본 발명의 실시예에 따른 로봇 시스템의 전면도이고, 도 7은 본 발명의 실시예에 따른 로봇 시스템 의 후면 사시도이다. 도 6을 참조하면, 사람과 유사한 로봇 형상을 가지는 본체의 머리 부분에 영상 촬영부, 음성 입력부 가 장착된다. 그리고 본체의 얼굴 부분에 이모티콘, 이모지(Emoji) 등으로 표시하기 위한 화면을 구 비하는 제1 디스플레이(107-1)가 장착된다. 본체의 몸통 전면에는 얼굴 인식 결과, 음성 인식 결과, 감정 인식 결과, 사용자 인식 결과 등을 출력하기 위한 모니터 화면을 구비하는 제2 디스플레이(107-3)가 장착된다. 도 7을 참조하면, 본체의 얼굴 부분, 몸통 부분에는 각각 LED가 장착된다. 그리고 몸통 전면을 둘러 음성 출력부가 장착된다. 본체의 후면에는 로봇 시스템을 제어하는 데 사용되는 리셋 버튼, 전원 버튼이 장착되고, 입출력 포트를 포함하는 I/O 박스가 마련되어 있다. 도 8은 도 1에 도시된 감정 생성부의 구체적인 구성을 도시한 도면이다. 도 8에 도시된 바와 같이, 감정 생성부는 감정 이모지값 생성부, 감정 LED 컬러값 생성부, 감 정 음성값 생성부 및 감정 모션값 생성부를 포함한다. 감정 이모지값 생성부는 감정 인식부에 의해 인식되는 사용자의 감정 상태를 입력으로 퍼지 시스템 을 사용하여 로봇 시스템의 감정 상태를 감정 이모지값으로 정의하여 로봇 시스템의 감정을 생성한다. 도 9는 감정 이모지값 생성부에 의해 생성되는 감정 이모지 값이 맵핑된 감정 이모지 맵의 예를 나타낸다. 따라서, 감정 표현부는 감정 생성부에 의해 생성된 도 9에 도시된 바와 같은 감정 이모지 맵을 사용 하여 LCD와 같은 디스플레이를 통해 로봇 시스템의 감정을 표현할 수 있다. 감정 LED 컬러값 생성부는 감정 인식부에 의해 인식되는 사용자의 감정 상태를 입력으로 퍼지 시스 템을 사용하여 로봇 시스템의 감정 상태를 감정 LED 컬러값으로 정의하여 로봇 시스템의 감정을 생성 한다. 도 10은 감정 LED 컬러값 생성부에 의해 생성되는 감정 LED 컬러값의 예를 나타낸다. 따라서, 감정 표현부는 감정 생성부에 의해 생성된 도 10에 도시된 바와 같은 감정 LED 컬러값을 사 용하여 LED로 구성되는 발광부를 통해 로봇 시스템의 감정을 표현할 수 있다. 감정 음성값 생성부는 감정 인식부에 의해 인식되는 사용자의 감정 상태를 입력으로 퍼지 시스템을 사용하여 로봇 시스템의 감정 상태를 감정 음성값으로 정의하여 로봇 시스템의 감정을 생성한다. 따라서, 감정 표현부는 감정 생성부에 의해 생성된 감정 음성값을 사용하여 스피커를 포함하는 음성 출력부를 통해 로봇 시스템의 감정을 표현할 수 있다. 감정 모션값 생성부는 감정 인식부에 의해 인식되는 사용자의 감정 상태를 입력으로 퍼지 시스템을 사용하여 로봇 시스템의 감정 상태를 감정 모션값으로 정의하여 로봇 시스템의 감정을 생성한다. 따라서, 감정 표현부는 감정 생성부에 의해 생성된 감정 모션값을 사용하여 로봇 시스템의 모션 을 구동하는 모션 출력부를 통해 로봇 시스템의 감정을 표현할 수 있다. 이하, 본 발명의 실시예에 따른 로봇 시스템을 사용하여 로봇의 감정을 생성하여 표현하는 방법에 대해 설 명한다. 본 발명의 실시예에 따른 로봇 시스템을 사용하여 로봇의 감정을 생성하여 표현하는 방법은 제어부의 제어를 통해 수행된다. 도 11은 본 발명의 실시예에 따른 로봇 시스템의 감정 생성 및 표현 방법의 흐름도이다. 도 11을 참조하면, 먼저, 사용자 인식부가 얼굴 인식부에서 인식되는 얼굴 영상을 사용하여 사용자를 인식하기 위한 CNN 모델을 통한 학습, 감정 인식부가 얼굴 인식부에서 인식되는 얼굴 영상과 음성 인 식부에서의 음성 인식 결과를 사용하여 사용자 감정을 인식하기 위한 CNN 모델을 통한 학습, 감정 생성부 가 감정 인식부에 의해 인식되는 사용자의 감정을 사용하여 로봇 시스템의 감정을 생성하기 위 한 퍼지 추론 엔진을 통한 학습이 먼저 수행되어 그 학습 결과가 저장부에 저장된다(S100). 그 후, 영상 촬영부를 통해 사용자의 영상을 촬영하고, 음성 입력부를 통해 사용자 음성을 입력받아 서(S110), 얼굴 인식부를 통해 촬영된 영상으로부터 얼굴을 인식하고, 음성 인식부를 통해 입력 음성 을 인식한다(S120). 그리고, 인식되는 얼굴에 대해 사용자 인식부를 통해 CNN 모델을 사용하여 기등록된 사용자의 얼굴인지를 인식한다(S130). 이 때, 기등록된 사용자의 얼굴로 인식되면 사용자 인식이 완료되고, 인식된 사용자의 정보를 저장부로부터 추출함으로써 사용자의 정보를 알 수가 있다. 그러나, 기등록된 사용자의 얼굴이 아닌 경우 신규 사용자의 얼굴이므로 신규 사용을 위해 사용자 등록을 수행하는 절차가 개시될 수 있다.다음, 감정 인식부를 통해, 얼굴 인식부에 의해 인식된 얼굴 영상과 음성 인식부에 의해 인식되 는 음성 인식 결과에 대해 CNN 모델을 사용하여 사용자의 감정을 인식한다(S140). 이 때, 인식되는 사용자 감 성에 대해 디스플레이를 통해 외부로 표시할 수 있다. 그 후, 감정 생성부는 도 4에 도시된 감정 기준 맵에 감정 인식부에 의해 인식되는 사용자 감정 인식 결과를 적용하여 도 5에 도시된 바와 같은 감정 적용 기준 맵을 생성하고(S150), 생성되는 감정 적용 기준 맵을 사용하여 퍼지 시스템에 의한 퍼지 추론을 수행한 후 그 결과를 로봇 시스템의 감정으로 생성한다 (S160). 다음, 감정 생성부에 의해 생성되는 로봇 시스템의 감정을 외부로 표현하기 위해 디스플레이, 발광부, 음성 출력부 및 모션 출력부를 통한 감정 출력을 위한 감정값을 생성한다(S170). 즉, 감정 생성부는 퍼지 추론 결과로 정의되는 감정에 대응되는 감정 이모지값, 감정 LED 컬러값, 감정 음성값 및 감정 모션값을 생성한다. 그 후, 감정 표현부는 상기 단계(S170)에서 생성되는 로봇 감정값에 기초하여 디스플레이, 발광부 , 음성 출력부 및 모션 출력부를 사용하여 로봇 시스템의 감정을 표현한다(S180). 이와 같이, 본 발명의 실시예에 따른 로봇 시스템의 감정을 생성하여 표현하는 방법에 따르면, 촬영되는 사용자의 영상과 입력되는 사용자의 음성을 인식하여 사용자의 현재 감정 상태를 판단하고, 판단되는 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성하여 표현할 수 있으며, 이러한 특징을 갖는 로봇 시스템(10 0)의 감정 생성 및 표현 방법에 따라 컨시어지 서비스를 제공하는 컨시어지 로봇 등이 사용자와의 상호작용을 위한 표시나 행동 등에 사용될 수 있다. 한편, 본 발명의 실시예에 따른 로봇 시스템의 감정 생성 및 표현 방법은 상기한 로봇 시스템의 감정 을 표현하는 단계(S180) 후에, 영상 촬영부를 통해 사용자의 얼굴을 촬영하여 상기 단계(S180)에서 표현되 는 로봇 시스템의 감정에 대응되는 사용자의 반응을 확인함으로써, 로봇 시스템의 감정 표현과 사용 자의 반응 간의 상관관계를 측정하고 분석하여 학습함으로써, 사용자의 감정에 대응되는 로봇 시스템의 감 정 생성 및 표현의 품질을 향상시킬 수 있으며, 또한 사용자에게 최신, 최적의 로봇 시스템의 감정 표현 을 제공할 수 있다. 한편, 상기에서는 촬영된 사용자의 영상으로부터 얼굴을 인식하여 사용자를 인식하는 학습을 수행하고, 학습 결 과에 기초하여 새로이 촬영되는 영상을 통해 사용자를 인식하는 사용자 인식부와, 얼굴 영상과 음성 인식 결과 통해 사용자의 감정을 학습하고, 학습 결과에 기초하여 새로이 촬영되어 인식되는 얼굴 영상과 음성 인식 결과에 대응되는 사용자의 감정을 인식하는 감정 인식부가 합성곱 신경망(CNN)을 사용하여 사용자를 인식 하고, 사용자의 감정을 인식하는 것으로 설명하였고, 감정 생성부가 감정 인식부에 의해 인식되는 사 용자의 감정에 대해 퍼지 추론을 사용하여 로봇 시스템의 감정을 인식하는 것에 대해 설명하였으나, 본 발 명의 기술적 범위는 여기에 한정되지 않는다. 예를 들어, 영상과 음성을 사용하여 외부의 인공지능 클라우드 서버를 통한 학습과, 사용자 인식, 감정 인식 및 로봇 시스템의 감정 생성을 수행하는 방식에 대해 설명한 다. 최근에는 각종 자료를 사용자의 컴퓨터나 스마트폰 등 내부 저장 공간이 아닌 외부 클라우드 서버에 저장한 뒤 에 다운로드받는 서비스인 클라우드 서비스가 각광을 받고 있으며, 이러한 클라우드 서비스는 인공지능 기술을 제공하는 클라우드 서버에 의해서도 제공될 수 있다. 예를 들어, IBM사의 인공지능 기술인 왓슨(Watson)이나 구글사의 텐서플로우(TesnorFlow) 등이 오픈 소프트웨어로서 잘 알려져 있다. 이들은 인공지능을 활용해 자연 어 질문을 알아듣고, 방대한 데이터로부터 관련 있는 사항을 분석, 종합해 적절한 답변을 제공하는 기술이다. 이들은 API(Application Programming Interface)를 통해 인공지능 기술을 사용할 수 있는 클라우드 서비스를 제공하고 있다. 따라서, 본 발명의 다른 실시예에 따른 감정을 생성하고 표현하는 로봇 시스템에서는 영상을 통한 사용자 인식 학습과 영상 및 음성을 통한 사용자 감정 인식 학습, 및 사용자 감정을 통한 로봇 시스템의 감정 생성에 관한 학습, 그리고 학습 결과에 기초한 사용자 인식, 사용자 감정 인식 및 로봇 시스템의 감정 생성을 로봇 시스템에 서 직접 수행하지 않고 API를 통해 접속되는 외부의 클라우드 서버에게 영상 정보, 음성 정보 및 사용자 감정 정보를 전달하여 이에 대응되는 사용자 인식 결과, 사용자 감정 인식 결과, 또는 로봇 시스템 감정 결과를 제공 받아서 사용한다. 이하, 본 발명의 다른 실시예에 따른 로봇 시스템에 대해 설명한다. 도 12는 본 발명의 다른 실시예에 따른 로봇 시스템의 개략적인 구성도이다. 도 12에 도시된 바와 같이, 본 발명의 다른 실시예에 따른 로봇 시스템은 영상 촬영부, 얼굴 인식부 , 사용자 인식부, 음성 입력부, 음성 인식부, 감정 인식부, 디스플레이, 발광 부, 음성 출력부, 모션 출력부, 감정 생성부, 감정 표현부, 저장부, 서버 인터 페이스 및 제어부를 포함한다. 여기서, 영상 촬영부, 얼굴 인식부, 음성 입력부, 음 성 인식부, 디스플레이, 발광부, 음성 출력부, 모션 출력부 및 감정 표현부는 도 1을 참조하여 설명한 로봇 시스템의 영상 촬영부, 얼굴 인식부, 음성 입력부, 음성 인 식부, 디스플레이, 발광부, 음성 출력부, 모션 출력부 및 감정 표현부의 기능과 동작이 동일하므로 여기에서는 이들에 대한 구체적인 설명을 생략하고, 그 기능과 동작이 상이한 구성요소들에 대해서만 설명한다. 서버 인터페이스는 외부의 클라우드 서버가 제공한 API를 사용하여 네트워크를 통해 클라우드 서버에 접속되어 정보의 송수신이 가능하도록 하는 기능을 수행한다. 여기서, 클라우드 서버는 IBM 사의 왓슨이나 구글사의 텐서플로우와 같이 딥 러닝 기술의 CNN 모델에 따른 정보 학습 및 학습 결과를 이용한 사용자 인식 및 감정 인식과, 퍼지 추론을 통한 학습 및 학습 결과를 이용한 로봇 시스템의 감정 생성이 가능한 서버를 말한다. 사용자 인식부는 얼굴 인식부에서 출력되는 영상을 서버 인터페이스를 통해 클라우드 서버(50 0)로 전달하여 사용자 인식에 대한 학습을 수행시킬 수 있다. 이와 같이, 클라우드 서버를 통한 사용자 인식에 대한 학습이 완료된 후, 사용자 인식부는 얼굴 인식 부에서 출력되는 영상을 서버 인터페이스를 통해 클라우드 서버에게 전달하고, 전달된 영상에 대응되어 클라우드 서버로부터 제공되는 사용자 인식 결과를 사용하여 사용자 인식을 수행할 수 있다. 또한, 감정 인식부는 얼굴 인식부에서 출력되는 얼굴 영상과 음성 인식부에서 출력되는 음성 인 식 결과를 서버 인터페이스를 통해 클라우드 서버로 전달하여 사용자의 감정 인식에 대한 학습을 수 행시킬 수 있다. 이와 같이, 클라우드 서버를 통한 사용자 감정 인식에 대한 학습이 완료된 후, 감정 인식부는 얼굴 인식부에서 출력되는 영상과 음성 인식부에서 출력되는 음성 인식 결과를 서버 인터페이스를 통 해 클라우드 서버에게 전달하고, 전달된 음성 인식 결과에 대응되어 클라우드 서버로부터 제공되는 감정 인식 결과를 사용하여 사용자 감정 인식을 수행할 수 있다. 또한, 감정 생성부는 감정 인식부에서 출력되는 사용자의 감정을 서버 인터페이스를 통해 클라 우드 서버로 전달하여 로봇 시스템의 감정 생성에 대한 학습을 수행시킬 수 있다. 이와 같이, 클라우드 서버를 통한 로봇 시스템의 감정 생성에 대한 학습이 완료된 후, 감정 생성부 는 감정 인식부에서 출력되는 사용자의 감정을 서버 인터페이스를 통해 클라우드 서버에게 전달하고, 전달된 사용자의 감정에 대응되어 클라우드 서버로부터 제공되는 감정 생성 결과를 사용하여 로 봇 시스템의 감정을 생성할 수 있다. 저장부는 기본적으로는 이전 실시예에서의 로봇 시스템에서의 저장부와 유사하지만, 본 발명의 다른 실시예에서는 얼굴 영상을 통한 사용자 인식, 얼굴 영상과 음성 인식 결과를 통한 사용자의 감정 인식, 및 사용자의 감정을 통한 로봇 시스템의 감정 생성에 대한 학습을 수행하지 않으므로 이러한 학습 정보를 저 장하지 않는 부분이 상이하다. 또한, 제어부도 로봇 시스템 각 구성요소, 즉 영상 촬영부, 얼굴 인식부, 사용자 인식부 , 음성 입력부, 음성 인식부, 감정 인식부, 디스플레이, 발광부, 음성 출력부 , 모션 출력부, 감정 생성부, 감정 표현부, 저장부 및 서버 인터페이스 간에 전달되는 신호를 처리하며, 이전 실시예에서의 제어부와 상이한 점은 저장부에서와 같이, 영상에 대 응되는 사용자 인식, 영상 및 음성 인식 결과에 대응되는 사용자의 감정 인식, 및 사용자의 감정에 대응되는 로 봇 시스템의 감정 생성에 대한 학습을 위한 신호 전달 제어 부분이 없다는 것이다. 한편, 본 발명의 다른 실시예에서 사용되는 외부의 클라우드 서버가 본 발명의 다른 실시예에서 얼굴 영상 에 대응되는 사용자 인식, 얼굴 영상과 음성 인식 결과에 대응되는 사용자의 감정 인식, 및 사용자의 감정에 대 응되는 로봇 시스템의 감정 생성을 위한 학습과, 그 학습 정보를 사용하여 얼굴 영상에 대응되어 실시간으로 사용자를 인식하고, 얼굴 영상과 음성 인식 결과에 대응되어 실시간으로 사용자의 감정을 인식하며, 사용자 의 감정에 대응되어 실시간으로 로봇 시스템의 감정을 생성하는 기술에 대해서는 API를 제공하여 클라우드 서버로서 동작 가능한 클라우드 서버 고유의 기능으로 이에 대해서는 잘 알려져 있으므로 여기에서는 구체적인 설명을 생략한다. 이와 같이, 본 발명의 다른 실시예에 따른 로봇 시스템에서는 얼굴 영상에 대응되는 사용자 인식, 얼굴 영 상과 음성 인식 결과에 대응되는 사용자의 감정 인식, 사용자의 감정에 대응되는 로봇 시스템의 감정 생성 에 대한 학습을 수행하는 기능을 외부의 클라우드 서버를 사용하여 수행함으로써, 로봇 시스템의 구 성이 간단해질 뿐만 아니라, 이미 검증되어 있고 매우 다양하고 많은 정보를 갖고 있는 클라우드 서버를 사용함으로써 로봇 시스템에 대한 신뢰를 제공할 수 있다. 이하, 상기한 로봇 시스템을 사용하여 로봇 시스템의 감정을 생성하여 표현하는 방법에 대해 설명한 다. 도 13은 본 발명의 다른 실시예에 따른 로봇 시스템의 감정 생성 및 표현 방법의 흐름도이다. 도 13을 참조하면, 먼저, 사용자 인식부가 얼굴 인식부에서 인식되는 얼굴 영상을 네트워크를 통해 외부의 클라우드 서버로 전달하여 CNN 모델을 통해 사용자 인식에 대한 학습을 수행하고, 감정 인식 부가 얼굴 인식부에서 인식되는 얼굴 영상과 음성 인식부에서의 음성 인식 결과를 네트워크 를 통해 외부의 클라우드 서버로 전달하여 CNN 모델을 통해 사용자 인식에 대한 학습을 수행하며, 감 정 생성부가 감정 인식부에 의해 인식되는 사용자의 감정을 네트워크를 통해 외부의 클라우드 서버로 전달하여 퍼지 추론을 통해 로봇 시스템의 감정 생성에 대한 학습을 수행한다(S200). 그 후, 영상 촬영부를 통해 사용자의 영상을 촬영하고, 음성 입력부를 통해 사용자 음성을 입력받아 서(S210), 얼굴 인식부를 통해 촬영된 영상으로부터 얼굴을 인식하고, 음성 인식부를 통해 입력 음성 을 인식한다(S220). 그리고, 상기 단계(S220)에서 인식되는 얼굴 영상을 외부의 클라우드 서버로 전달하여 대응되는 사용자 인 식을 요청한다(S230). 그 후, 클라우드 서버로부터, 전달된 얼굴 영상에 대응되어 수행된 사용자 인식 결과를 수신한다(S240). 즉, 이러한 사용자 인식 결과가 사용자 인식부에서의 사용자 인식 결과가 된다. 또한, 상기 단계(S220)에서 실시간으로 인식되는 얼굴 영상과 음성 인식 결과를 외부의 클라우드 서버로 전달하여 대응되는 사용자의 감정 인식을 요청한다(S250). 그 후, 클라우드 서버로부터, 전달된 얼굴 영상과 음성 인식 결과에 대응되어 수행된 사용자의 감정 인식 결과를 수신한다(S260). 즉, 이러한 사용자의 감정 인식 결과가 감정 인식부에서의 사용자 감정 인식 결 과가 된다. 다음, 상기 단계(S260)에서 인식된 사용자의 감정을 외부의 클라우드 서버로 전달하여 대응되는 로봇 시스 템의 감정 생성을 요청한다(S270). 마찬가지로, 클라우드 서버로부터, 전달된 사용자의 감정에 대응되는 로봇 시스템의 감정 생성 결과 를 수신한다(S280). 즉, 이러한 로봇 시스템의 감정 생성 결과가 감정 생성부에서의 로봇 시스템 의 감정 생성 결과가 된다. 다음, 감정 생성부에 의해 생성되는 로봇 시스템의 감정을 외부로 표현하기 위해 로봇 시스템의 감정값을 생성하는 단계(S290)과 상기 단계(S290)에서 생성되는 로봇 시스템의 감정값에 기초하여 디스플 레이, 발광부, 음성 출력부 및 모션 출력부를 사용하여 로봇 시스템의 감정을 표현하 는 단계(S300)는 도 11을 참조하여 설명한 단계(S170, S180)과 동일하므로 이에 대한 구체적인 설명을 생략한다. 이하, 본 발명의 또 다른 실시예에 따른 로봇 시스템에 대해 설명한다. 도 14는 본 발명의 또 다른 실시예에 따른 로봇 시스템의 개략적인 구성 블록도이다. 도 14를 참조하면, 본 발명의 또 다른 실시예에 따른 로봇 시스템은 입력기, 출력기, 메모리 , 프로세서 및 버스를 포함한다.입력기는 외부로부터 영상, 음성 등을 입력받는다. 이러한 입력기는 구체적으로, 외부의 영상을 촬영 등을 통해 입력받는 영상 입력기, 마이크 등을 통해 음성을 입력받는 음성 입력기를 포함하낟. 출력기는 외부로 영상, 음성, 문자, 행동 등을 출력한다. 이러한 출력기는 구체적으로, 영상을 출 력하는 디스플레이, 빛을 발산하는 LED, 스피커와 같이 음성을 출력하는 음성 출력기 및 로 봇 시스템의 팬, 틸트 등의 모션 제어를 통한 출력을 수행하는 모션 출력기를 포함한다. 메모리는 코드의 집합을 저장하도록 구성되고, 그 코드는 다음과 같은 동작을 실행하기 위해 프로세서 를 제어하는 데 사용된다. 이러한 동작은, 입력기를 통해 사용자의 영상과 음성을 입력받는 동작, 영상으로부터 얼굴 영상을 인식하는 동작, 음성을 인식하는 동작, 얼굴 영상과 음성 인식 결과를 학습하여 대응 되는 사용자의 감정을 학습하는 동작, 학습된 정보에 기초하여 입력되는 얼굴 영상과 음성 인식 결과에 대응되 는 사용자의 감정을 인식하는 동작, 사용자의 감정에 대응되는 로봇 시스템의 감정 생성을 학습하는 동작, 학습된 정보에 기초하여 입력되는 사용자의 감정에 대응되는 로봇 시스템의 감정을 생성하는 동작, 로봇 시스템의 감정을 표현하는 동작을 포함한다. 버스는 로봇 시스템의 모든 구성요소들, 즉 입력기, 출력기, 메모리 및 프로세 서를 결합하도록 구성된다. 한편, 메모리는 ROM(Read Only Memory)과 RAM(Random Access Memory), NVRAM(Non-Volatile Random Access Memory)을 포함할 수 있다. 또한, 프로세서는 컨트롤러(controller), 마이크로 컨트롤러 (microcontroller), 마이크로 프로세서(microprocessor), 마이크로 컴퓨터(microcomputer) 등으로도 호칭될 수 있다. 또한, 프로세서는 하드웨어(hardware) 또는 펌웨어(firmware), 소프트웨어, 또는 이들의 결합에 의해 구현될 수 있다. 이상에서 설명한 본 발명의 실시예는 장치 및 방법을 통해서만 구현이 되는 것은 아니며, 본 발명의 실시예의 구성에 대응하는 기능을 실현하는 프로그램 또는 그 프로그램이 기록된 기록 매체를 통해 구현될 수도 있다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속하는 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14"}
{"patent_id": "10-2017-0074424", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 감정을 생성하여 표현하는 로봇 시스템의 구성을 개략적으로 도시한 도면이다. 도 2는 본 발명의 실시예에 따른 로봇 시스템에서 CNN 모델을 사용하여 인식되는 사용자의 감정 분류를 도시한 도면이다. 도 3은 본 발명의 실시예에 따른 로봇 시스템이 감정을 생성하는 데 사용되는 퍼지 시스템의 개략적인 구성도이 다. 도 4는 본 발명의 실시예에 따른 로봇 시스템에서 사용되는 감정 기준 맵의 예를 도시한 도면이다. 도 5는 도 4에 도시된 감정 기준 맵에 사용자의 감정 인식 결과를 적용한 감정 적용 기준 맵의 예를 도시한 도 면이다.도 6은 본 발명의 실시예에 따른 로봇 시스템의 전면도이다. 도 7은 본 발명의 실시예에 따른 로봇 시스템의 후면 사시도이다. 도 8은 도 1에 도시된 감정 생성부의 구체적인 구성을 도시한 도면이다. 도 9는 도 8에 도시된 감정 이모지값 생성부에 의해 생성되는 감정 이모지 값이 맵핑된 감정 이모지 맵의 예를 나타낸다. 도 10은 도 8에 도시된 감정 LED 컬러값 생성부에 의해 생성되는 감정 LED 컬러값의 예를 나타낸다. 도 11은 본 발명의 실시예에 따른 로봇 시스템의 감정 생성 및 표현 방법의 흐름도이다. 도 12는 본 발명의 다른 실시예에 따른 로봇 시스템의 개략적인 구성도이다. 도 13은 본 발명의 다른 실시예에 따른 로봇 시스템의 감정 생성 및 표현 방법의 흐름도이다. 도 14는 본 발명의 또 다른 실시예에 따른 로봇 시스템의 개략적인 구성 블록도이다."}
