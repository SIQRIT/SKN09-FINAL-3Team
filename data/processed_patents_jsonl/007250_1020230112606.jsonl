{"patent_id": "10-2023-0112606", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0029532", "출원번호": "10-2023-0112606", "발명의 명칭": "컴퓨팅 가속기 상에서 데이터 변환을 사용한 매트릭스 연산을 위한 방법 및 장치", "출원인": "디-매트릭스 코포레이션", "발명자": "류보미르스키, 일리야"}}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능 가속기(AI accelerator) IC(integrated circuit)를 위한 IC로 구성된 매트릭스 컴퓨팅 장치에있어서, 제1 매트릭스 입력(matrix input)을 수신하도록 구성(configure)된 IB(input buffer) 디바이스, 여기서 상기제1 매트릭스 입력은 제1 포맷을 특징으로 하고 적어도 제1 입력 부분(input portion) 및 제2 입력 부분을가짐;상기 IB 디바이스에 결합(couple)된 컴퓨팅 디바이스(compute device), 여기서 상기 컴퓨팅 디바이스는 적어도제1 컴퓨팅 유닛 및 제2 컴퓨팅 유닛을 갖는 복수의 컴퓨팅 유닛을 포함하고, 상기 제1 컴퓨팅 유닛은 적어도상기 제1 입력 부분을 사용하여 제1 매트릭스 출력(matrix output)을 결정하도록 구성되고, 상기 제2 컴퓨팅 유닛은 적어도 상기 제2 입력 부분을 사용하여 제2 매트릭스 출력을 결정하도록 구성되고, 상기 컴퓨팅 디바이스는 상기 제1 매트릭스 출력 및 상기 제2 매트릭스 출력을 사용하여 제2 포맷으로 제1 결합 매트릭스 출력(combined matrix output)을 결정하도록 구성됨;상기 컴퓨팅 디바이스에 제공(provide)되고, 상기 제1 결합 매트릭스 출력을 사용하여 변환(convert) 출력 포맷으로 제1 변환 매트릭스 출력(converted matrix output)을 결정하도록 구성된 컴퓨팅 컨버터(computeconverter); 및상기 컴퓨팅 디바이스에 결합된 OB(Output Buffer) 디바이스, 여기서 상기 OB 디바이스는 상기 제1 변환 매트릭스 출력을 저장하도록 구성됨; 를 포함하는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 컴퓨팅 디바이스는 복수의 컴퓨팅 유닛에 결합된 정렬 디바이스(alignment device)를 포함하며, 상기 정렬 디바이스는 상기 제1 결합 매트릭스 출력을 사용하여 제3 포맷의 제1 환산 매트릭스 출력(rounded matrix output)을 결정하도록 구성되는 장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 컴퓨팅 디바이스는 상기 정렬 디바이스에 결합된 PPR(partial products reduction) 디바이스를 포함하고, 상기 PPR 디바이스는 상기 제1 환산 매트릭스 출력을 사용하여 제1 환원 매트릭스 출력(reduced matrix output)을 결정하도록 구성되고, 상기 컴퓨팅 컨버터는 상기 제1 한원 매트릭스 출력을 사용하여 상기 제1 변환 매트릭스 출력을 결정하도록 구성되는 장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 제1 포맷은 제1 블록 부동 소수점 포맷(first block floating point format)을포함하고;공개특허 10-2024-0029532-3-상기 제2 포맷은 제2 블록 부동 소수점 포맷을 포함하고;상기 제3 포맷은 제3 블록 부동 소수점 포맷을 포함하고;상기 변환 출력 포맷은 부동 소수점 포맷을 포함하는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서, 상기 제1 포맷은 BFP26-64 포맷을 포함하고;상기 제2 포맷은 BFP46-1 포맷을 포함하고;상기 제3 포맷은 BFP32-1 포맷을 포함하고;상기 변환 출력 포맷은 FP16 포맷 또는 B플로트16(Bfloat16) 포맷을 포함하고; 및상기 제1 매트릭스 출력 및 상기 제2 매트릭스 출력 각각은 64x64 바이트 타일 구성(tile configuration)을 특징으로 하는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 OB 디바이스에 결합된 SIMD(Single Instruction, Multiple Data) 디바이스를 더 포함하되, 상기 IB 디바이스, 상기 컴퓨팅 디바이스, 상기 OB 디바이스 및 상기 SIMD 디바이스는 각각 제1 컴퓨팅 경로(compute path) 내에서 제1 IB 디바이스, 제1 컴퓨팅 디바이스, 제1 OB 디바이스 및 제1 SIMD 디바이스로서 구성되고;하나 이상의 제2 컴퓨팅 경로를 더 포함하며, 상기 추가 컴퓨팅 경로 각각은 제2 IB 디바이스, 상기 제2 IB 디바이스에 결합된 제2 컴퓨팅 디바이스, 상기 제2 컴퓨팅 디바이스에 결합된 제2 OB 디바이스, 및 상기 제2 OB디바이스에 결합된 제2 SIMD 디바이스를 포함하는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 컴퓨팅 디바이스는 상기 제1 매트릭스 출력을 시프트(shift)하고 상기 시프트된 제1 매트릭스 출력을 상기 제2 매트릭스 출력에 더하여(add) 상기 제1 결합 매트릭스 출력을 결정하도록 구성되는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 제1 매트릭스 입력은 상기 제1 매트릭스 가중치 입력(matrix weight input) 및 상기 제1매트릭스 활성화 입력(matrix activation input)을 포함하고, 상기 제1 매트릭스 가중치 입력은 상기 제1 매트릭스 가중치 지수(matrix weight exponent)와 상기 제1 매트릭스 가중치 멘티사(matrix weight mantissa)를 포함하며, 상기 제1 매트릭스 가중치 기수는 MSB(most significant byte) 부분과 LSB(least significant byte)부분을 포함하고, 상기 제1 매트릭스 활성화 입력은 제1 매트릭스 활성화 지수(matrix activation exponent) 및제1 매트릭스 활성화 멘티사(matrix activation mantissa)를 포함하고;상기 제1 컴퓨팅 유닛은 상기 제1 매트릭스 가중치 멘티사의 MSB 부분을 저장(store)하고, 상기 제1 매트릭스가중치 멘티사의 MSB 부분 및 상기 제1 매트릭스 활성화 멘티사를 사용하여 상기 제1 매트릭스 출력을 결정하도공개특허 10-2024-0029532-4-록 구성되고; 및상기 제2 컴퓨팅 유닛은, 상기 제1 매트릭스 가중치 멘티사의 LSB 부분을 저장하고, 상기 제1 매트릭스 가중치멘티사의 LSB 부분 및 상기 제1 매트릭스 활성화 멘티사를 사용하여 상기 제2 매트릭스 출력을 결정하도록 구성되는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 컴퓨팅 디바이스는 상기 제1 매트릭스 출력을 시프트하고, 상기 시프트된 제1 매트릭스출력을 상기 제2 매트릭스 출력에 더하여 상기 제1 결합 매트릭스 출력을 결정하도록 구성되는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서, 상기 컴퓨팅 디바이스는 복수의 컴퓨팅 유닛에 결합된 정렬 디바이스를 포함하며, 상기 정렬디바이스는 상기 제1 결합 매트릭스 출력을 환산(round)하여 제3 포맷의 제1 환산 매트릭스 출력(roundedmatrix output)을 결정하도록 구성되는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 컴퓨팅 디바이스는 상기 정렬 디바이스에 결합된 PPR(partial products reduction) 디바이스를 포함하며, 상기 PPR 디바이스는 제1 환산 매트릭스 출력을 환원(reduce)시켜 제1 환원 매트릭스 출력을결정하도록 구성되고; 및상기 컴퓨팅 컨버터는 상기 제1 환원 매트릭스 출력을 사용하여 상기 제1 변환 매트릭스 출력을 결정하도록 구성되는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항에 있어서, 상기 복수의 컴퓨팅 유닛 각각은 정수 포맷(integer numerical format)으로 구성되고, 상기MSB 부분은 부호화된 정수(signed integer)이고, 상기 LSB 부분은 부호화되지 않은 정수(unsigned integer)인것을 특징으로 하는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항에 있어서, 상기 IB 디바이스에 결합된 입력 컨버터 디바이스를 더 포함하고, 상기 입력 컨버터 디바이스는 상기 제1 매트릭스 입력을 부동 소수점 포맷으로부터 상기 제1 포맷으로 변환하도록 구성되는장치."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "칩렛 디바이스(chiplet device)에 있어서, 복수의 타일, 여기서 각각의 타일은 복수의 슬라이스, 및 복수의 슬라이스에 결합된 CPU(central processing공개특허 10-2024-0029532-5-unit)를 포함하고;상기 복수의 슬라이스는 각각은:제1 매트릭스 입력(matrix input)을 수신하도록 구성(configure)된 IB(input buffer) 디바이스, 여기서 상기제1 매트릭스 입력은 제1 포맷을 특징으로 하고 적어도 제1 입력 부분(input portion) 및 제2 입력 부분을 가지고;상기 IB 디바이스에 결합(couple)된 컴퓨팅 디바이스(compute device), 여기서 상기 컴퓨팅 디바이스는 적어도제1 컴퓨팅 유닛 및 제2 컴퓨팅 유닛을 갖는 복수의 컴퓨팅 유닛을 포함하고, 상기 제1 컴퓨팅 유닛은 적어도상기 제1 입력 부분을 사용하여 제1 매트릭스 출력(matrix output)을 결정하도록 구성되고, 상기 제2 컴퓨팅 유닛은 적어도 상기 제2 입력 부분을 사용하여 제2 매트릭스 출력을 결정하도록 구성되고, 상기 컴퓨팅 디바이스는 상기 제1 매트릭스 출력 및 상기 제2 매트릭스 출력을 사용하여 제2 포맷으로 제1 결합 매트릭스 출력(combined matrix output)을 결정하도록 구성되고;상기 컴퓨팅 디바이스에 제공(provide)되고, 상기 제1 결합 매트릭스 출력을 사용하여 변환(convert) 출력 포맷으로 제1 변환 매트릭스 출력(converted matrix output)을 결정하도록 구성된 컴퓨팅 컨버터(computeconverter); 및상기 컴퓨팅 디바이스에 결합된 OB(Output Buffer) 디바이스, 여기서 상기 OB 디바이스는 상기제1 변환 매트릭스 출력을 저장하도록 구성되는; 를 포함하는 디바이스."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 상기 제1 매트릭스 입력은 상기 제1 매트릭스 가중치 입력(matrix weight input) 및 상기 제1 매트릭스 활성화입력(matrix activation input)을 포함하고, 상기 제1 매트릭스 가중치 입력은 상기 제1 매트릭스 가중치 지수(matrix weight exponent)와 상기 제1 매트릭스 가중치 멘티사(matrix weight mantissa)를 포함하며, 상기 제1매트릭스 가중치 기수는 MSB(most significant byte) 부분과 LSB(least significant byte) 부분을 포함하고,상기 제1 매트릭스 활성화 입력은 제1 매트릭스 활성화 지수(matrix activation exponent) 및 제1 매트릭스 활성화 멘티사(matrix activation mantissa)를 포함하고;상기 제1 컴퓨팅 유닛은 상기 제1 매트릭스 가중치 멘티사의 MSB 부분을 저장(store)하고, 상기 제1 매트릭스가중치 멘티사의 MSB 부분 및 상기 제1 매트릭스 활성화 멘티사를 사용하여 상기 제1 매트릭스 출력을 결정하도록 구성되고;상기 제2 컴퓨팅 유닛은, 상기 제1 매트릭스 가중치 멘티사의 LSB 부분을 저장하고, 상기 제1 매트릭스 가중치멘티사의 LSB 부분 및 상기 제1 매트릭스 활성화 멘티사를 사용하여 상기 제2 매트릭스 출력을 결정하도록 구성되고; 및상기 컴퓨팅 디바이스는 상기 제1 매트릭스 출력을 시프트하고, 상기 시프트된 제1 매트릭스 출력을 상기 제2매트릭스 출력에 더하여 상기 제1 결합 매트릭스 출력을 결정하도록 구성되는디바이스."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서, 상기 컴퓨팅 디바이스는 복수의 컴퓨팅 유닛에 결합된 정렬 디바이스(alignment device)를 포함하며, 상기 정렬디바이스는 상기 제1 결합 매트릭스 출력을 사용하여 제3 포맷의 제1 환산 매트릭스 출력(rounded matrixoutput)을 결정하도록 구성되고, 공개특허 10-2024-0029532-6-상기 컴퓨팅 디바이스는 상기 정렬 디바이스에 결합된 PPR(partial products reduction) 디바이스를 포함하고,상기 PPR 디바이스는 상기 제1 환산 매트릭스 출력을 사용하여 제1 환원 매트릭스 출력(reduced matrix output)을 결정하도록 구성되고, 상기 컴퓨팅 컨버터는 상기 제1 한원 매트릭스 출력을 사용하여 상기 제1 변환 매트릭스 출력을 결정하도록 구성되는디바이스."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제14항에 있어서, 상기 복수의 컴퓨팅 유닛 각각은 정수 포맷(integer numerical format)으로 구성되고, 상기 MSB 부분은 부호화된 정수(signed integer)이고, 상기 LSB 부분은 부호화되지 않은 정수(unsigned integer)인 것을 특징으로 하는디바이스."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제14항에 있어서, 상기 CPU는 상기 제1 매트릭스 입력을 부동 소수점 포맷으로부터 상기 제1 포맷으로 변환하도록 구성되는디바이스."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제14항에 있어서, 상기 복수의 슬라이스 각각은 IB 디바이스에 결합된 입력 컨버터 디바이스를 포함하며, 상기입력 컨버터 디바이스는 상기 제1 매트릭스 입력을 부동 소수점 포맷으로부터 상기 제1 포맷으로 변환하도록 구성되는디바이스."}
{"patent_id": "10-2023-0112606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "인공지능 가속기 장치에 있어서, 복수의 칩렛(chiplet), 여기서 각 칩렛은 복수의 타일을 포함하고, 상기 타일 각각은 복수의 슬라이스를 포함하고, 상기 복수의 슬라이스에 결합되는 CPU(central processing unit)를 포함하고, 상기 복수의 슬라이스 각각은:상기 CPU로부터 제1 매트릭스 입력을 수신하도록 구성된 IB(input buffer) 디바이스;상기 IB 디바이스에 결합된 DIMC(digital in-memory compute) 디바이스; 및상기 DIMC 디바이스에 결합된 OB(output buffer) 디바이스를 포함하되, 상기 OB 디바이스는 제1 변환 매트릭스출력을 저장하도록 구성되는장치."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "매트릭스 컴퓨팅 장치에서 데이터 변환을 위한 방법 및 장치가 제안된다. 해당 장치는 제1 포맷을 특징으로 하는 하나 이상의 매트릭스 입력을 수신하는 입력 버퍼(IB)와 결합된 매트릭스 출력을 결정하도록 구성된 IB 디바이스 에 결합된 컴퓨팅 장치를 포함한다. 해당 컴퓨팅 디바이스는 매트릭스 입력의 첫 번째 입력 부분을 사용하여 첫 번째 매트릭스 출력을 결정하고 두 번째 입력 부분을 사용하여 두 번째 매트릭스 출력을 결정한다. 그런 다음 컴 퓨팅 디바이스는 첫 번째 및 두 번째 매트릭스 출력을 사용하여 두 번째 포맷의 결합 매트릭스 출력을 결정한다. 컴퓨팅 디바이스 내에서 정렬 디바이스는 결합 행렬 출력으로부터 환산 행렬 출력을 결정할 수 있고, 부분 곱 환 원/감소(PPR) 디바이스는 컴퓨팅 디바이스에 결합된 출력 버퍼(OB)에 저장된 환산 행렬 출력을 사용하여 세 번째 포맷으로 환원 행렬 출력을 결정할 수 있습니다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "[1] 변환기(Transformer)는 자연어 처리(NLP; natural language processing) 분야에서 지배적인 신경망 아키 텍처였으며 그 사용은 다른 기계 학습(machine learning) 애플리케이션으로 계속 확장되고 있다. 최초 변환기 (Transformer)는 \"Attention is all you need\"(Vaswani et al., 2017) 논문에 소개되었으며, GPT(Generative Pre-trained Transformer) 및 BERT(bidirectional encoder representations from transformers) 모델들과 같 은 다양한 변환기(transformer) 모델의 개발을 촉발했다. 이러한 변환기는 재귀(recursion)를 피하고 쉬운 병렬 처리를 허용하는 자체-어텐션 메커니즘(self-attention mechanism)을 사용하여 추론 작업(inference task)에서 다른 모델보다 훨씬 뛰어난 성능을 보였다. 반면에 변환기(transformer) 워크로드는 계산 집약적이며 메모리 요 구 사항이 높으며 시간 집약적이고 비효율적이라는 문제가 있었다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "[2] 최근에 NLP 모델은 모델 크기와 컴퓨팅 요구 사항 모두에서 1,000배 성장했다. 예를 들어, 1,750억 개의 파라미터가 있는 GPT-3와 같은 모델을 훈련하는 데 1,024개의 GPU(graphics processing unit)가 약 4개월이 걸 릴 수 있다. 1조 개의 파라미터가 있는 새로운 NLP 모델이 이미 개발 중이며 수조 개의 파라미터 모델이 출시될 예정이다. 이러한 급속한 성장으로 인해 NLP 모델을 대규모로 제공하는 것이 점점 더 어려워졌다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "[3]위에서 살펴본 바와 같이, AI의 컴퓨팅 워크로드(compute workloads)를 가속화하기 위한 디바이스와 방법의 개선이 매우 바람직하다는 것을 알 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "[4] 일 실시예에 따르면, 본 발명은 매트릭스 컴퓨팅 장치(matrix compute apparatus)에서의 데이터 변환 (data conversion)에 관한 것이다. 특정 애플리케이션에서, 매트릭스 컴퓨팅 장치는 네이티브 수치 포맷 외에 다른 수치 포맷을 처리할 수 있는 기능을 갖는 것이 바람직하다. 따라서, 본 발명은 데이터를 분할(segment)하 고, 분할된 데이터 부분을 매트릭스 컴퓨팅 장치의 네이티브 포맷(native format)으로 병렬 처리함으로써, 타겟 포맷(target format)으로 매트릭스 데이터를 처리하도록 구성된 매트릭스 컴퓨팅 장치를 가능하게 하는 방법 및 장치를 제공한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "[5] 매트릭스 컴퓨팅 장치는 IB(input buffer; 입력 버퍼) 디바이스, IB 디바이스에 결합된 컴퓨팅 디바이스, 및 상기 컴퓨팅 디바이스에 결합된 OB (output buffer; 출력 버퍼) 디바이스를 포함할 수 있다. IB 디바이스는 제1 포맷을 특징으로 하고 적어도 제1 입력 부분 및 제2 입력 부분을 갖는 하나 이상의 매트릭스 입력을 수신하 도록 구성된다. 상기 컴퓨팅 디바이스는 적어도 제1 컴퓨팅 유닛 및 제2 컴퓨팅 유닛을 포함하는 복수의 컴퓨팅 유닛을 갖는다. 각 매트릭스 입력에 대해, 상기 컴퓨팅 디바이스는 제1 입력 부분 및 제2 입력 부분으로부터 각 각 제1 매트릭스 출력 및 제2 매트릭스 출력을 결정하도록 구성된다. 그런 다음, 상기 컴퓨팅 디바이스는 제1 및 제2 매트릭스 출력을 사용하여 제2 포맷으로 결합 매트릭스 출력(combined matrix output)을 결정한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "[6] 예시적인 예에서, 각 매트릭스 입력은 매트릭스 가중치(matrix weight) 및 매트릭스 활성화(matrix activation)를 포함한다. 상기 매트릭스 가중치 및 매트릭스 활성화 각각은 지수(exponent) 및 멘티사 (mantissa)를 포함할 수 있다. 상기 매트릭스 가중치 멘티사는 제1 부분과 제2 부분으로 분리될 수 있으며, 이 는 각각 제1 컴퓨팅 유닛과 제2 컴퓨팅 유닛에 저장된다. 이 경우, 상기 컴퓨팅 디바이스는 상기 매트릭스 활성 화 및 상기 매트릭스 가중치 멘티사의 제1 부분을 사용하여 내적(dot product; 도트 곱) 프로세스를 수행하여 제1 매트릭스 출력을 결정한다. 마찬가지로, 상기 컴퓨팅 디바이스는 상기 매트릭스 활성화 및 상기 매트릭스 가중치 멘티사의 제2 부분을 사용하여 내적(도트 곱) 프로세스를 수행하여 제2 매트릭스 출력을 결정한다. 상기 결합 매트릭스 출력을 결정하는 것은, 제1 매트릭스 출력을 시프트하고, 상기 제1 매트릭스 출력을 상기 제2 매트릭스 출력에 더하는 것을 포함한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 4, "content": "[7] 일예에서, 상기 컴퓨팅 디바이스는 정렬 디바이스 및 복수의 컴퓨팅 유닛에 결합된 부분 곱 환원(PPR; partial products reduction) 디바이스를 포함한다. 상기 정렬 디바이스는 결합 매트릭스 출력들 각각에 대해 환산(rounded 또는 반올림) 매트릭스 출력을 결정하도록 구성될 수 있고, 상기 PPR디바이스는 상기 환산 매트릭 스 출력들 각각에 대해 제3 포맷으로 환원된 매트릭스 출력을 결정하도록 구성될 수 있다. 상기 컴퓨팅 디바이 스는 또한 상기 컴퓨팅 디바이스로부터의 결과 매트릭스 출력(resulting matrix output)을 사용하여 각 매트릭 스 입력에 대해 변환 출력 포맷(converted output format)으로 변환 매트릭스 출력을 결정하도록 구성된 컴퓨팅 컨버터/변환기(compute converter)를 포함할 수 있다. 그 후, 상기 결과 변환 매트릭스 출력(resulting converted matrix output)은 상기 OB 디바이스에 저장된다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 5, "content": "[8] 앞의 실시예에서는 매트릭스 데이터를 두 개의 컴퓨팅 디바이스에 의해 병렬 처리되는 두 개의 부분으로 분 할하는 것에 대해서만 설명하였으나, 본 발명의 다른 실시예에서는 매트릭스 데이터를 복수의 컴퓨팅 디바이스 에 의해 병렬 처리되는 복수의 부분으로 분할할 수도 있다. 매트릭스 컴퓨팅 장치 및 관련 방법의 실시예는 또 한 칩렛 디바이스(chiplet device) 및 AI 가속기 시스템에서 구현될 수 있다. 당업자는 다른 변형, 수정 및 대 안을 인식할 것이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 6, "content": "[9] 본 매트릭스 컴퓨팅 장치 및 그 관련 방법의 실시예는 많은 이점을 제공할 수 있다. 본 방법 및 장치는 네 이티브 포맷(native format)과 호환되는 부분으로 분할될 수 있는 상이한 데이터 포맷의 매트릭스 입력을 계산 처리할 수 있게 한다. 또한, 이러한 다중 포맷 기능(multi-format capability)은 완전히 별도의 하드웨어 및 컴 퓨팅 경로를 필요로 하지 않고도 수행될 수 있다. 특히 효율적인 정수 연산을 위해 설계된 매트릭스 곱셈 유닛 (matrix multiply unit)은 IEEE FP16 또는 Bfloat16과 같은 부동 소수점 데이터(floating point data)를 처리 하는 데 사용할 수 있다. 또한 이러한 이점은 실리콘 면적의 추가 비용을 최소화하면서 IC 칩과 칩렛 장치에서 구현할 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "[24] 본 발명은 일반적으로 집적 회로(IC) 디바이스 및 인공 지능(AI) 시스템에 관한 것이다. 특히, 본 발명은 변환기-기반 신경망 모델(변환기(transformer)라고도 함)에서 컴퓨팅 워크로드(workload)를 가속화하기 위한 방 법 및 디바이스 구조에 관한 것이다. 이러한 방법과 구조는 자연어 처리(NLP: natural language processing), 컴퓨터 비전(CV: computer vision) 등과 같은 머신/딥 러닝(machine/deep learning) 애플리케이션에서 사용할 수 있다. 단지 예로서, 본 발명은 NLP에 대한 높은 처리량 동작을 수행하도록 구성된 AI 가속기 장치 및 칩렛 디바이스에 적용되었다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "[25] 현재, 대부분의 NLP 모델은 BERT(bidirectional encoder representations from transformers) 모델, BERT Large 모델, 및 GPT-2/GPT-3 등과 같은 GPT(Generative Pre-trained Transformer) 모델과 같은 변환기 모델 (transformer model)을 기반으로 한다. 그러나 이러한 변환기는 컴퓨팅 및 메모리 요구 사항이 매우 높다. 일례 에 따르면, 본 발명은 AI 애플리케이션을 위한 변환기(transformer) 계산을 가속화하도록 구성된 칩렛 디바이스 를 사용하는 장치를 제공한다. 상기 AI 가속기 장치의 예는 도 1a 및 도 1b에 도시된다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "[26] 도 1a는 두 개의 칩렛 디바이스를 갖는 단순화된 AI 가속기 장치를 도시한다. 도시된 바와 같이, 칩렛 디바이스는 하나 이상의 (D2D: die-to-die) 인터커넥트에 의해 서로 결합된다. 또한, 각 각의 칩렛 디바이스는 메모리 인터페이스(예를 들어, SRAM(Static Random Access Memory), DRAM(Dynamic Random Access Memory), SDRAM(Synchronous Dynamic RAM) 등)에 결합된다. 상기 장치는 또 한 기판 부재의 표면 영역(surface region) 상에 구성된 칩렛 디바이스에 기계적 지지(mechanical support)를 제공하는 기판 부재를 포함한다. 기판(substrate)은 실리콘 인터포저(silicon interposer), 유리 인터포저, 유기 인터포저 등과 같은 인터포저를 포함할 수 있다. 상기 칩렛은 하나 이상의 인터포저에 결 합될 수 있으며, 이는 상기 칩렛과 다른 구성요소(예를 들어, 전기 신호가 내부 및 외부 요소 사이를 통과하게 하는 브리지 또는 도관(conduit) 역할을 함) 사이의 통신을 가능하게 하도록 구성될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "[27] 도 1b는 상기 기판 부재 상의 4개의 칩렛의 2개 그룹으로 구성된 8개의 칩렛 디바이스를 갖는 단순화된 AI 가속기 장치를 도시한다. 여기서, 그룹 내의 각각의 칩렛 디바이스는 하나 이상의 D2D 인터커넥트에 의해 다른 칩렛 디바이스에 결합된다. 장치는 또한 각각의 칩렛 디바이스에 결합 된 DRAM 메모리 인터페이스를 도시한다. 상기 DRAM 메모리 인터페이스는 \"Mem\" 블록으로 표현되는 하 나 이상의 메모리 모듈에 결합될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "[28] 도시된 바와 같이, 상기 AI 가속기 장치(101, 102)는 PCIe(Peripheral Component Interconnect Express) 카드 폼 팩터로 구현되지만, 상기 AI 가속기 장치는 다른 폼 팩터로도 구성될 수 있다. 이러한 PCIe 카드 폼 팩 터는 다양한 치수(예: 전체 높이-전체 길이(FHFL: height, full length), 절반 높이-절반 길이(HHHL: half height, half length) 등) 및 기계적 크기(예: 1x, 2x, 4x, 16x)로 구성할 수 있다. 예에서, 하나 이상의 칩렛 을 각각 갖는 하나 이상의 기판 부재는 PCIe 카드에 결합된다. 당업자는 AI 가속기 장치의 이들 요소 및 구성에 대한 다른 변형, 수정 및 대안을 인식할 것이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "[29] 실시예는 다양한 AI 애플리케이션에서 성능(예를 들어, 계산 효율)을 개선하기 위해 여러 기술을 구현할 수 있다. 상기 AI 가속기 장치는 계산 기능과 메모리 패브릭(memory fabric)을 통합하기 위해 DIMC(Digital In-Memory-Compute)를 포함할 수 있다. 매퍼, 수치(numeric) 및 희소성(sparsity)에 대한 알고리즘은 컴퓨팅 패브릭(compute fabric) 내에서 최적화할 수 있다. 또한 유기 인터포저(organic interposer)에 구성된 칩렛 및 인터커넥트을 사용하면 모듈성(modularity)과 확장성(scalability)을 제공할 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "[30] 일 예에 따르면, 본 발명은 변환기(transformer)의 워크로드에 의해 요구되는 계산을 가속화하기 위해 사 용될 수 있는 인-메모리-컴퓨트(IMC: in-memory-compute) 기능을 갖는 칩렛을 구현한다. 이러한 모델을 훈련 (training)시키기 위한 계산에는 특정 AI 애플리케이션에서 원하는 결과와 관련된 확률 분포를 결정하기 위해 스케일드 내적 어텐션(scaled dot-product attention) 함수를 수행하는 것이 포함될 수 있다. NLP 모델을 교육 하는 경우 원하는 결과에는 후속 단어 예측, 문맥 단어 의미 결정, 다른 언어로 번역 등이 포함될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "[31] 칩렛 아키텍처는 변환기 계산을 병렬로 수행하기 위해 중앙 처리 장치(CPU)에 의해 제어되는 복수의 슬라 이스 디바이스(또는 슬라이스)를 포함할 수 있다. 각 슬라이스는 이러한 계산의 일부를 처리할 수 있는 모듈식 IC 디바이스이다. 복수의 슬라이스는 CPU 가 타일 내의 각각의 슬라이스에 결합된 하나 이상의 슬라이스의 갱 (gang)/타일(즉, 서브세트)로 분할될 수 있다. 이러한 타일 CPU는 타일 내의 각 슬라이스를 통해 변환기 계산을 병렬로 수행하도록 구성할 수 있다. 글로벌 CPU는 이러한 각 타일 CPU에 결합될 수 있으며 타일 CPU를 사용하여하나 이상의 칩렛의 모든 슬라이스를 통해 병렬로 변환기(transformer) 계산을 수행하도록 구성할 수 있다. 상 기 칩렛의 추가 세부 사항은 도 2a 내지 도 5b를 참조하여 설명하고 변환기(transformer)는 도 6 내지 도 9를 참조하여 설명한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "[32] 도2a는 16-슬라이스 칩렛 디바이스의 예시적인 구성을 도시하는 단순화된 블록도이다. 이 경우, 상기 칩렛은 각각 4개의 슬라이스 디바이스, CPU, 및 하드웨어 디스패치(HW DS) 디바이스를 포 함할 수 있다. 특정 예에서, 이들 타일은 대칭 방식으로 배열된다. 전술한 바와 같이, 타일의 CPU는 타일 내의 모든 슬라이스에 의해 수행되는 동작을 조정할 수 있다. 상기 HW DS는 상기 CPU에 결합되고 상기 타일에서 상기 슬라이스의 제어를 조정하도록 구성될 수 있다(예를 들어, 타일에서 어떤 슬라이스가 변환기 계산의 대상 부분을 처리하는지 결정하기 위해). 구체적인 예에서, 상기 CPU는 RISC(Reduced Instruction Set Computer) CPU 등일 수 있다. 또한, 상기 CPU는 CPU의 제 어를 조정하도록(예를 들어, 특정 CPU에 의해 처리되는 변환기(transformer) 연산의 부분을 결정하도록) 구성되 는 디스패치 엔진에 결합될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "[33] 각각의 타일의 CPU는 글로벌 CPU 인터페이스(예를 들어, 버스, 커넥터, 소켓 등)를 통해 글로벌 CPU에 결합될 수 있다. 이러한 글로벌 CPU는 각각 도 1a 및 1b의 장치(101 및 102)와 같은 AI 가속기 장 치에서 모든 칩렛 디바이스의 처리를 조정하도록 구성될 수 있다. 예에서, 글로벌 CPU는 각 타일의 HW DS 를 사용하여 각 연관된 CPU가 타일의 슬라이스에 걸쳐 변환기 계산의 다양한 부분을 수행하도록 지시할 수 있다. 또한, 상기 글로벌 CPU는 RISC 프로세서 등이 될 수 있다. 상기 칩렛은 또한 D2D 인터커넥트 및 메모리 인터페이스를 포함하고, 이들 모두는 각각의 타일에서 각각의 CPU에 결합된다. 예를 들어, 상기 D2D 인터커넥트는 단일 종단 신호(single-ended signaling)로 구성될 수 있다. 상기 메모리 인터페이스 는 하나 이상의 메모리 디바이스(예를 들어, DRAM, SRAM, SDRAM 등)에 결합된 하나 이상의 메모리 버스를 포함할 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "[34] 상기 칩렛은 각 타일의 각 CPU에 결합된 PCIe 인터페이스/버스를 포함한다. 상기 PCIe 인 터페이스는 서버 또는 다른 통신 시스템과 통신하도록 구성될 수 있다. 복수의 칩렛 디바이스의 경우, 메 인 버스 디바이스는 마스터 칩렛 디바이스(예를 들어, 마스터 칩렛 디바이스에도 결합된 메인 버스 디바이스)를 사용하여 각 칩렛 디바이스의 PCIe 버스에 결합된다. 이러한 마스터 칩렛 디바이스는 적어도 D2D 인터커넥 트를 사용하여 서로 다른 칩렛 디바이스에 결합된다. 상기 마스터 칩렛 디바이스 및 메인 버스 디바이스는 기판 부재(예를 들어, 칩렛과 동일한 기판 또는 별도의 기판) 위에 놓이도록 구성될 수 있다. 하나 이상의 칩렛 을 통합하는 장치는 또한 전원에 결합(예를 들어, 온칩 구성, 시스템에 구성 또는 외부에 결합)될 수 있고, 서 버, 네트워크 스위치 또는 주 버스 장치를 사용하는 호스트 시스템에 구성 및 작동될 수 있다. 상기 서버 장치 는 또한 데이터 센터 내의 서버 팜(server farm) 또는 다른 유사한 구성을 위해 구성된 복수의 서버 장치 중 하 나일 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "[35] 특정한 일례에서, GPT-3 용으로 구성된 AI 가속기 장치는 8개의 칩렛(도 1b의 장치과 유사함)을 통합 할 수 있다. 이러한 칩렛은 D2D 16x16Gb/s 인터커넥트, 32비트 LPDDR5 6.4Gb/s 메모리 모듈, 및 16레인 PCIe Gen 5 PHY NRZ 32Gb/s/lane 인터페이스로 구성할 수 있다. LPDDR5(16 x 16GB)는 양자화된 GPT-3과 같은 대규모 NLP 모델에 필요한 용량, 대역폭 및 저전력을 제공할 수 있다. 물론 다른 변형, 수정 및 대안이 있을 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "[36] 도 2b는 16-슬라이스 칩렛 디바이스의 예시적인 구성을 도시하는 단순화된 블록도이다. 칩렛과 유사하게, 상기 칩렛은 각각 4개의 슬라이스 디바이스 및 CPU를 포함하는 4개의 갱(또는 타일) 을 포함한다. 도시된 바와 같이, 각 갱/타일의 CPU는 각각의 슬라이스 및 다른 갱/타일의 서로 다른 CPU에 결합된다. 예에서, 상기 타일/갱은 신경 코어 역할을 하고, 상기 슬라이스는 컴퓨팅/컴퓨 트 코어 역할을 한다. 이 멀티 코어 구성을 사용하면 여러 계산을 병렬로 수행하고 실행하도록 칩렛 디바이스를 구성할 수 있다. 상기 CPU는 또한 글로벌 CPU 인터페이스, D2D 인터커넥트, 메모리 인터페이스 및 PCIe 인터페이스에 결합된다. 도 2a에 대해 설명된 바와 같이, 상기 글로벌 CPU 인터페이스(23 0)는 각 갱의 모든 CPU를 제어하는 글로벌 CPU에 연결된다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "[37] 도 3a는 칩렛의 예시적인 슬라이스 디바이스를 예시하는 단순화된 블록도이다. 상기 16-슬라이스 칩 렛 예에서, 슬라이스 디바이스는 4개의 컴퓨팅 경로를 갖는 컴퓨팅 코어를 포함하며, 각 경로는 상호 간에 결합되는 입력 버퍼(IB) 디바이스, DIMC(digital in-memory-compute) 디바이스, 출력 버 퍼(OB) 디바이스, 및 SIMD(Single Instruction, Multiple Data) 디바이스를 포함한다. 이들 경로 각각은 각 경로에 의해 수행되는 계산을 조정하기 위해 상기 타일 CPU에 의해 제어되는 슬라이스 크로스바(cross-bar)/컨트롤러에 결합된다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "[38] 예에서, 상기 DIMC는 클록에 결합되고 칩렛의 복수의 슬라이스 각각의 하나 이상의 부분 내에 구성되어 상 기 DIMC에서 제공되는 하나 이상의 매트릭스 계산(matrix computation)의 높은 쓰루풋(high throughput)을 허 용하되, 상기 높은 쓰루풋(high throughput)은 클록 주기당 512 곱셈 누산(multiply accumulates)을 특징으로 한다. 구체적인 예에서, 상기 DIMC에 결합된 클록은 대략 0.5GHz 내지 4GHz의 클록 신호를 출력하도록 구성된 제1 클록(예를 들어, 칩렛 클록 생성기, AI 가속기 장치 클록 생성기 등)에서 파생된 제2 클록이고; 제2 클록은 상기 제1클록 레이트의 대략 절반의 출력 레이트로 구성할 수 있다. 상기 DIMC는 블록 구조의 희소성(block structured sparsity)을 지원(예: 변환기(transformer)와 같은 신경망의 가중치 패턴에 구조적 제약을 부과)하 도록 구성할 수도 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "[39] 예에서, 상기 SIMD 디바이스는 DIMC의 출력에 결합된 SIMD 프로세서이다. 상기 SIMD는 벡터 프 로세스(vector process)에서 하나 이상의 비선형 연산 및 하나 이상의 선형 연산을 처리하도록 구성될 수 있다. 상기 SIMD는 프로그래머블 벡터 유닛 등일 수 있다. 상기 SIMD는 또한 데이터 RAM(Random-Access Memory) 모듈, 명령어 RAM 모듈 등과 같은 하나 이상의 RAM 모듈을 포함할 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "[40] 예에서, 상기 슬라이스 컨트롤러는 각각의 계산 경로의 모든 블록에 결합되고 또한 각각의 계산 경로에 결합된 제어/상태 레지스터(CSR: control/status register)를 포함한다. 상기 슬라이스 컨트롤러 는 또한 메모리 뱅크 및 데이터 재구성 엔진(DRE: data reshape engine)에 결합된다. 상기 슬 라이스 컨트롤러는 상기 메모리 뱅크로부터 각각의 컴퓨팅 경로의 블록으로 데이터를 공급하고 이러한 컴퓨팅/계산 경로를 조정하도록 구성될 수 있다. 특정 예에서, PIF는 각 컴퓨팅 경로의 SIMD에 결합된다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "[41] 컴퓨팅 코어에 대한 추가 세부사항은 도 3b에 도시되어 있다. 슬라이스 디바이스의 단순화된 블 록도는 입력 버퍼, DIMC 매트릭스 벡터 유닛, 출력 버퍼, 네트워크 온 칩(NoC: network on chip) 디바이스 및 SIMD 벡터 유닛를 포함한다. 상기 DIMC 유닛은, 높은 쓰루풋의 매트릭스 곱 셈 누산 동작(matrix multiply-accumulate operation)이 필요한 확률 분포(probability distribution)를 결정 하기 위해, 입력 데이터에 대한 스케일드 내적 어텐션(scaled dot-product attention) 함수를 계산하도록 구성 된 복수의 IMC(in-memory-compute) 모듈을 포함한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "[42] 이러한 IMC 모듈은 또한 상기 DIMC 결과를 상기 출력 버퍼에 출력하기 전에 추가 처리(further processing)를 위해 블록 부동 소수점 정렬 모듈(block floating point alignment module) 및 부분 곱 환 원/감소 모듈(partial products reduction module)에 결합될 수 있다. 예에서, 상기 입력 버퍼는 메 모리 뱅크(도 3a에 도시됨)로부터 입력 데이터(예를 들어, 데이터 벡터)를 수신하고 데이터를 상기 IMC 모 듈로 송신한다. 상기 IMC 모듈은 또한 메모리 뱅크로부터 명령을 수신할 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "[43] 이전에 논의된 세부사항에 더하여, 상기 SIMD는 엘리멘트 별(element-wise) 벡터 유닛으로 구성될 수 있다. 상기 SIMD는 계산 유닛(예를 들어, 더하기, 빼기, 곱하기, 최대값 등), LUT(look-up table), 및 상기 출력 버퍼로부터 하나 이상의 출력을 수신하도록 구성된 SM(state machine) 모듈 을 포함할 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "[44] 상기 NoC 디바이스는 숏컷(shortcut) 연결을 통해 피드포워드(feedforward) 루프에 구성된 상 기 출력 버퍼에 결합된다. 또한, 상기 NoC 디바이스는 각각의 슬라이스에 결합되고 멀티캐스트 및 유 니캐스트 프로세스를 위해 구성된다. 특히, 상기 NoC 디바이스는 모든 슬라이스 및 모든 타일을 연결하도 록 구성될 수 있고, 모든 슬라이스/타일에 멀티캐스트 입력 활성화일 수 있고, 특별하게 분산된 누적 (distributed accumulation)을 위해 유니캐스트될 부분 계산(partial computation)을 수집하도록 구성될 수 있 다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 22, "content": "[45] 이전의 8칩렛 AI 가속기 장치 예를 고려하면, 상기 입력 버퍼는 16뱅크로 64KB의 용량을 가질 수 있고, 상 기 출력 버퍼는 16뱅크로 128KB의 용량을 가질 수 있다. 상기 DIMC는 크기가 64x64(64x64 IMC 모듈 8개)인 8비 트 블록일 수 있으며, 상기 NoC 는 크기가 512비트일 수 있다. 상기 SIMD의 연산/컴퓨팅 블록은 8비트 및 32비 트 정수(int) 및 부호 없는 정수(uint) 계산을 위해 구성될 수 있을 뿐만 아니라 IEEE 854 float16 또는 float32과 같은 부동 소수점 연산을 위해 구성될 수 있다. 이러한 슬라이스 구성 요소는 AI 가속기 장치가 제공 할 변환기(transformer)에 따라 달라질 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 23, "content": "[46] 도 4는 예시적인 IMC 모듈을 도시하는 단순화된 블록도이다. 도시된 바와 같이, 모듈은 하나 이상의 읽기-쓰기 블록으로부터의 입력 데이터에 대해 원하는 계산을 수행하도록 구성된 하나 이상의 계산 트리 블록(computation tree blocks)을 포함한다. 이러한 읽기-쓰기 블록 각각은 하나 이상의 제1 메모리 선택 유닛(\"W\"라고도 함), 하나 이상의 제2 메모리 선택 유닛(\"I\"라고도 함), 활성화 멀티플 렉서(activation multiplexer), 및 오퍼레이터 유닛을 포함한다. 상기 제1 메모리 선택 유닛은 상기 오퍼레이터 유닛에 입력을 제공하고, 상기 제2 메모리 선택 유닛은 역시 상기 오퍼레이터 유닛 에 결합된 활성화 멀티플렉서를 제어한다. 곱셈 누산 연산의 경우, 상기 오퍼레이터 유닛은 곱 셈 유닛(multiplier unit)이고 상기 계산 트리 블록은 곱셈 덧셈 트리 블록(multiplier adder tree block)(즉, ∑x.w)이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 24, "content": "[47] 클로즈업에 도시된 바와 같이, 각각의 메모리 선택 유닛(422, 424)은 메모리 셀(예를 들어, SRAM 셀 등) 및 선택 멀티플렉서를 포함한다. 각각의 메모리 선택 유닛(422, 424)은 또한 메모리 뱅크/드 라이버 블록에 결합되는 읽기-쓰기 컨트롤러에 결합된다. 예에서, 읽기-쓰기 컨트롤러는 열 쓰 기 드라이버(column write driver) 및 열 읽기 감지 증폭기(column read sense amplifier)로 구성될 수 있는 반면, 상기 메모리 뱅크/드라이버 블록은 순차 행 선택 드라이버(sequential row select driver)로 구성 될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 25, "content": "[48] 입력 활성화 컨트롤러는 상기 읽기-쓰기 블록 각각의 활성화 멀티플렉서에 결합될 수 있다.상기 입력 활성화 컨트롤러는 정밀도 및 희소성 인식 입력 활성화 레지스터 및 드라이버(precision and sparsity aware input activation register and driver)를 포함할 수 있다. 상기 오퍼레이터 유닛은 상기 제1 메모리 선택 유닛의 출력을 수신하고 상기 제2 메모리 선택 유닛의 출력에 의해 제어되는 활성화 멀티플렉서를 통해 이 블록의 출력을 수신한다. 상기 오퍼레이터 유닛의 출력은 계산 트 리 블록으로 공급된다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 26, "content": "[49] 상기 입력 활성화 블록은 또한 클록 소스/생성기에 결합된다. 전술한 바와 같이, 상기 클록 생 성기는 약 0.5GHz 내지 4GHz의 클록 신호를 출력하도록 구성된 제1 클록으로부터 유도된 제2 클록을 생성 할 수 있고; 제2 클록은 제1 클록의 레이트의 약 절반의 출력 레이트로 구성할 수 있다. 상기 클록 생성기(46 0)는 상기 계산 트리 블록의 출력을 수신하도록 구성된 하나 이상의 부호 및 정밀도 인식 누산기 (precision aware accumulator)에 결합된다. 예에서, 상기 누산기는 2개의 계산 트리 블록의 출력을 수신하도록 구성된다. 상기 IMC의 출력 판독값 예는 도 13a 내지 도 13c에 도시된다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 27, "content": "[50] 8-칩렛 AI 가속기 장치 예를 다시 참조하면, 상기 메모리 셀은 이중 뱅크 2x6T SRAM 셀일 수 있고, 상기 선택 멀티플렉서는 8T 뱅크 선택 멀티플렉서일 수 있다. 이 경우, 상기 메모리 뱅크/드라이버 블록은 듀얼 뱅크 SRAM 뱅크를 포함한다. 또한 상기 읽기/쓰기 컨트롤러는 64바이트의 쓰기 드라이버와 64바이트의 읽기 감 지 증폭기를 포함할 수 있다. 당업자는 이들 IMC 모듈 구성요소 및 그 구성에 대한 다른 변형, 수정 및 대안을 인식할 것이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 28, "content": "[51] 도 5a는 슬라이스에서 처리되는 데이터의 예시적인 수치 포맷(numerical format)을 도시하는 단순화된 블 록 흐름도이다. 다이어그램은 GM/입력 버퍼, IMC, 출력 버퍼, SIMD 및 GM/입력 버퍼 로 피드백되는 NoC에 대한 데이터 포맷멧을 가진 루프를 보여준다. 상기 IMC 블록은 곱셈 누적 연산(∑x.w)을 표현한다. 또한, 상기 IMC로부터의 데이터에 대한 포맷도 출력 버퍼 로 흐른다. 이 예 에서 수치 포맷에는 정수(int), 부동 소수점(float) 및 가변 길이의 블록 부동(bfloat: block floating) 포인 트가 포함된다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 29, "content": "[52] 도 5b는 도 5a에 도시된 특정 포맷을 포함하여 특정 수치 포맷(numerical format)을 도시하는 단순화된 도면이다. 블록 부동 소수점 수치는 성능에 대한 특정 장벽을 해결하는 데 사용할 수 있다. 변환기의 학습/훈련 (Training)은 일반적으로 부동 소수점, 즉 32비트 부동 소수점 또는 16비트 부동 소수점에서 수행되며 추론 (inference)은 일반적으로 8비트 정수(\"int8\")에서 수행된다. 블록 부동 소수점을 사용하면 각 가수(mantissa) 가 별도의 지수(exponent)를 갖는 부동 소수점(도 5A 상단의 32 비트 float 및 16비트 float 포맷)과 달리, 지 수(exponent)는 가수 유효 값(mantissa significant value) 세트에서 공유(도 5B 하단에 있는 int8 벡터의 대 각선으로 채워진 블록 참조)된다. 추론(inference)에 블록 부동 소수점 수치 포맷을 사용하는 방법은 정수 연산 의 정확성 및 배치(deployment) 문제없이 고정 소수점의 효율성을 발휘할 수 있으며, 정확성을 유지하면서 4비 트 정수(\"int4\")와 같은 더 작은 소수점을 사용할 수도 있다. 또한 블록 부동 소수점 포맷(예: 활성화, 가중치 등에 대해) 및 희소성을 사용하여 학습/훈련 모델의 추론을 가속화하여 더 나은 성능을 얻을 수 있다. 당업자는 변환기(transformer) 워크로드(workload)를 처리하는 데 사용되는 이러한 수치 포맷(numerical format)에 대한다른 변형, 수정 및 대안을 인식할 것이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 30, "content": "[53] 도 6은 단순화된 변환기 아키텍처를 예시한다. 일반적인 변환기(transformer)는 디코더 스택 (encoder stack)으로 구성된 인코더 스택(encoder stack)을 갖는 것으로 설명될 수 있고, 각각의 이러한 스택은 하나 이상의 레이어를 가질 수 있다. 인코더 레이어 내에서, 자체 어텐션(self-attention) 레이어은 입력 데이터를 인코딩하는 동안 컨텍스트 정보(contextual information)를 결정하고 인코딩된 데이터를 피드 포 워드 신경망에 공급한다. 인코더 레이어는 입력 시퀀스를 아래에서 위로(from bottom to top) 처리하 여 출력을 어텐션 벡터 K 및 V의 세트로 변환한다. 디코더 레이어은 또한 대응하는 자체 어텐션 레이어 및 피드포워드 신경망을 포함하고, 추가 컨텍스트 프로세싱에서 디코더를 돕는 인코더 스택으로부터 의 어텐션 벡터를 사용하는 인코더-디코더 어텐션 계층을 더 포함할 수 있다. 디코더 스택은 부동 소수점 의 벡터를 출력하고(도 5b에 대해 논의된 바와 같이), 이것은 출력을 원하는 최종 결과(예를 들어, 원하는 단어 예측, 해석 또는 번역)로 투영(project)하기 위해 선형 및 소프트맥스(softmax) 레이어에 공급된다. 선형 레이어는 디코더 출력 벡터를 모든 잠재적 결과(예: 모든 잠재적 단어)와 관련된 점수를 포함하는 더 큰 벡터 (즉, 로짓 벡터)로 투영하는 완전히 연결된 신경망이고, 소프트맥스(softmax) 레이어는 이러한 점수를 확률로 바꾼다. 이 확률 출력에 기초하여, 예상된 단어 의미는 어플리케이션에 따라 가장 높은 확률 또는 다른 파생된 기준에 따라 선택될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 31, "content": "[54] 변환기 모델 변형은 디코더 스택에만 기초한 것들(예를 들어, GPT-2, GPT-3 등과 같은 변환기 언어 모델) 및 인코더 스택에만 기초한 것들(예를 들어, BERT, BERT Large와 같은 마스킹된 언어 모델)을 포함한다. 변환기 는: 시퀀스 길이(S)(즉, 토큰 수); 어텐션 헤드 수(A); 레이어 수(L); 및 임베딩 길이(embedding length)(H)의 네 가지 파라미터를 기반으로 한다. 이러한 파라미터의 변형은 오늘날 거의 모든 변환기(transformer) 기반 모 델을 구축하는 데 사용된다. 본 발명의 실시예는 임의의 유사한 모델 유형에 대해 구성될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 32, "content": "[55] 변환기는 훈련 되지 않은 상태로 시작하고 원하는 학습 애플리케이션을 위해 원하는 데이터 세트에 노출됨 으로써 사전 훈련 된다. 변환기 기반 언어 모델은 텍스트 시퀀스의 다음 단어 예측, 텍스트를 다른 언어로 번역 등과 같은 언어 처리 기능을 훈련하기 위해 대량의 텍스트(예: Wikipedia)에 노출된다. 이 훈련 프로세스에는 텍스트(예를 들어 단어 또는 단어의 일부)를 토큰 ID로 변환하고 자체 어텐션(self-attention) 레이어로 토큰의 컨텍스트를 평가하고 피드포워드 신경망으로 결과를 예측한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 33, "content": "[56] 상기 자체 어텐션 프로세스는 입력 문장에서 각 단어의 임베딩을 위한 쿼리(Q), 키(K) 및 값(V) 벡터 를 결정하는 단계, 타겟 단어에 대한 입력 문장의 각 단어에 대한 Q와 K의 내적(dot product)으로부터 점수 를 계산하는 단계, 점수를 K 차원의 제곱근으로 나누고, 결과를 소프트맥스(softmax) 연산에 전달하여 점수를 정규화(normalize)하고, 각 V에 소프트맥스 점수를 곱하고, 가중치가 부여된 V 벡터를 합산하여 출력을 생성하는 과정을 포함한다. 상기 값 매트릭스 V는 소프트맥스 어텐션 행렬(softmax attention matrix)과 매트릭스 곱셈(matrix multiplication)을 위한 가중치 행렬(weight matrix)이 되는데, 블록 부동 소수점 수치 의 맥락에서, 이는 아래에 설명된 바와 같이 V에 대한 컬럼 차단 컨버터(column blocking converter)가 필요하 다는 점에 유의한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 34, "content": "[57] 이러한 변환기(transformer) 아키텍처의 성능에 영향을 미치는 것은 여러 가지가 있다. 상기 소프트맥스 함수(softmax function)는 변환기 레이어(transformer layers)의 임계 경로(critical path)인 경향이 있다(그 리고 하드웨어에서 가속하기 어려웠다). 컴퓨팅, SIMD 작업 및 NoC 전송의 중첩 요구 사항도 성능에 영향을 미 친다. 또한, NoC, SIMD 및 메모리 대역폭 활용의 효율성도 중요하다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 35, "content": "[58] 양자화(quantization), 희소성(sparsity), 지식 추출(knowledge distillation), 효율적인 토큰화 (efficient tokenization) 및 소프트웨어 최적화와 같은 다양한 기술을 AI 가속기 장치 및 칩렛 디바이스 예시 와 함께 적용하여 성능을 향상시킬 수 있다. 가변 시퀀스 길이를 지원하면(즉, 가장 높은 시퀀스 길이까지 패딩 할 필요가 없음) 메모리 요구 사항도 줄일 수 있다. 다른 기술들에는 슬라이스 및 칩(chip)들 사이에서 자체 어 텐션(self-attention)을 분할하는 방법, 슬라이스 및 칩들 사이에서 레이어 및 텐서(tensor)를 이동하는 방법, 및 레이어 및 FC 매트릭스 사이의 데이터 이동에 대한 최적화가 포함될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 36, "content": "[59] 일 실시예에 따르면, 본 발명은 변환기 디바이스(예를 들어, BERT, BERT 대형, GPT-2, GPT-3 등)의 집합체 에 결합된 인공지능 가속기 장치(도 1A 및 1B에 도시된 바와 같이)를 제공한다. 특정 예에서, 이러한 변환기 디 바이스들의 집합체는 3개 내지 N 개의 레이어 범위의 스택으로 구성된 복수의 변환기들을 포함할 수 있으며, 여 기서 N 은 최대 128의 정수이다. [60] 일례에서, 각 변환기(transformer)는 하나 이상의 DIMC 내에 구성되며, 각 변환기는 변환기의 어텐션 레 이어에 대해 구성된 QKV 매트릭스와 3개의 완전 연결 매트릭스(FC: fully-connected matrices)를 포함하는 복수 의 매트릭스 곱셈기(matrix multiplier)를 포함하도록 구성된다. 이 구성에서 상기 DIMC는 상기 변환기를 가속 하도록 구성되며, Q KT의 도트 곱(dot product/내적)과 소프트맥스(Q KT/square root (dk))V를 추가로 포함한다. 예시적인 예에서, 상기 인공지능 가속기 장치는 또한 소프트맥스 함수의 컴퓨팅 프로세스를 가속하도 록 구성된 SIMD 디바이스(도 3a 및 3b에 도시된 바와 같이)를 포함한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 37, "content": "[61] BERT 대형과 같은 변환기를 사용하는 경우, NLP는 매우 높은 연산(예를 들어, CV보다 5배 높은 연산)을 필 요로 한다. 예를 들어, BERT Large는 변환기 레이어당 초당 5.6 기가 곱셈-누적 연산(\"GMACs: giga-multiply- accumulate operations per second \")을 필요로 한다. 따라서, NLP 추론의 과제는 가장 낮은 에너지 소비로 이 러한 성능을 제공하는 것이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 38, "content": "[62] 본 발명은 NLP 애플리케이션을 위한 BERT 대형 변환기의 맥락에서 논의되지만, 당업자는 변형, 수정 및 대 안을 인식할 것이다. 도시된 특정 실시예들은 또한 다른 변환기 기반 모델 및 다른 인공지능/머신 러닝 애플리 케이션에 적용될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 39, "content": "[63] 앞서 논의한 바와 같이, 블록 부동 소수점(BFP) 포맷은 심층 신경망 추론(deep neural network inference)에서 매트릭스 곱셈 연산을 효율적으로 하드웨어 가속화하는 데 중요하다. 매트릭스 가중치는 종종 열을 따라 차단(block)되는 반면, 활성화는 종종 행(row)을 따라 차단된다. 따라서 BFP 수치를 사용하면 넓은 동적 범위(dynamic range)를 유지하면서 매트릭스 곱셈을 효율적으로 정수 연산으로 구현할 수 있다. 매트릭스 곱셈 후 활성화 행 벡터와 가중치 열 벡터의 도트 곱(dot product/내적)은 부동 소수점(FP) 포맷(예: FP32, FP16 등)으로 누적(accumulated)되어 출력 버퍼에 매트릭스 타일(예: FP16의 64x64 타일)로 저장된다. 또한 부 분 곱의 누적(accumulation of partial product)을 위해 24비트 멘티사(2의 보수(complement))와 8비트 지수(2 의 보수(complement))가 있는 BFP32-1을 FP32와 동등한 포맷으로 사용할 수도 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 40, "content": "[64] 출력 버퍼 메모리 로드/저장은 일반적으로 행 단위(row-wise)로 구현되며, 이는 다음 매트릭스 곱셈을 위 한 활성화를 생성하기 위한 행 단위 BFP 블로킹의 일반적인 경우에 편리하다. 그러나 매트릭스 곱셈의 출력이 후속 매트릭스 곱셈의 가중치 행렬로 사용되는 경우(예: BERT 인코더 모델에서 주의 함수에 대한 값 행렬을 사 용한 매트릭스 곱셈)가 있으며, 이 경우 출력 버퍼의 데이터를 컬럼 차단/블로킹(column blocking) 구성에 저장 해야 한다. 이러한 경우, 열(column)을 가로지르는 차단/블로킹은 메모리 로드/저장이 행 단위 저장 구성을 특 징으로 하는 경우 출력 컨버터가 한 번에 한 행만 데이터를 읽을 수 있기 때문에 문제를 야기한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 41, "content": "[65] 일 실시예에 따르면, 본 발명은 행 차단 구성의 제1 포맷으로부터 컬럼 차단 구성의 제2 포맷으로 데이터 를 변환하기 위한 컬럼 차단 변환 장치 및 방법을 제공한다. 컬럼 차단 장치는 앞서 설명한 예시적인 인공지능 가속기 IC와 같은 인공지능 가속기 장치용 IC로 구성될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 42, "content": "[66] 도 7a 및 7b는 본 발명의 실시예에 따른 컬럼 차단 컨버터 장치(701/702)를 설명하기 위한 단순화된 블록 도이다. 도시된 바와 같이, 장치들(701/702)은 도 3a에 도시된 슬라이스 디바이스와 유사하다. 이들 도면 들 간에 공유되는 참조 번호는 앞서 설명한 것과 동일한 요소를 지칭한다. 여기서, 도 7a 및 도 7b는 컴퓨팅 코 어에서 2개의 컴퓨팅 경로만을 도시하지만, 애플리케이션에 따라 추가적인 컴퓨팅 경로가 있을 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 43, "content": "[67] 도 7a의 장치는 입력 버퍼(IB) 디바이스, 컴퓨팅 디바이스, 출력 버퍼(OB) 디바이스(34 0)를 갖는 컴퓨팅 경로를 포함할 수 있다. 상기 IB 디바이스는 컴퓨팅 디바이스에 결합되고, 복 수의 매트릭스 입력을 수신하도록 구성된다. 상기 컴퓨팅 디바이스는 OB 디바이스에 결합되고, 복수 의 매트릭스 입력에 대해 복수의 매트릭스 컴퓨팅을 수행하도록 구성된다. 구체적인 예에서, 상기 컴퓨팅 디바 이스는 앞서 설명한 바와 같이 소프트맥스 기능을 수행하는 디지털 인메모리 컴퓨팅(DIMC) 디바이스 일 수 있다. 이 경우, 상기 OB 디바이스는 행 단위 저장 구성을 특징으로 할 수 있고, 복수의 매트릭스 컴 퓨팅에 따른 복수의 매트릭스 출력으로부터 결과되는 복수의 매트릭스 출력을 제1 포맷으로 저장하도록 구성될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 44, "content": "[68] 상기 OB 컨버터 디바이스는 상기 컴퓨팅 디바이스와 상기 OB 디바이스 사이에 결합될 수 있다. 이러한 OB 컨버터 디바이스는 복수의 매트릭스 출력을 상기 OB 디바이스 내에 제1 포맷으로 저 장하도록 구성될 수 있다. 도시된 바와 같이, OB 컨버터 디바이스는 OB 디바이스와 별도로 구성되지 만, OB 컨버터 디바이스는 OB 디바이스 내에 구성될 수도 있다. 이러한 구성은 인라인 컬럼 차단 컨버터 장치(inline column blocking converter apparatus)로 구현될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 45, "content": "[69] 크로스바(crossbar) 디바이스는 상기 IB 디바이스, 컴퓨팅 디바이스 및 OB 디바이스(34 0)에 결합된다. 또한, 상기 크로스바 컨버터 디바이스는 상기 OB 디바이스에 결합되어 복수의 매트릭 스 출력을 복수의 매트릭스 출력 각각에 대해 결정된 최대 지수(exponent) 값과 멘티사 값을 이용하여 복수의 매트릭스 출력을 제1 포맷에서 제2 포맷으로 변환하여 복수의 변환 매트릭스 출력을 생성하도록 구성된다. 도시 된 바와 같이, 상기 크로스바 컨버터 디바이스는 연산 경로 내에 구성되지만, 상기 크로스바 컨버터 디바이스는 상기 크로스바 디바이스 내에 구성될 수도 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 46, "content": "[70] 또한, 메모리 디바이스는 상기 크로스바 디바이스에 결합된다. 이러한 메모리 디바이스는 최대 지수 값 및 멘티사 값을 사용하여 복수의 변환 매트릭스 출력을 제2 포맷 및 컬럼 차단 구성으로 저장하도록 구 성된다. 제1 포맷은 부동 소수점(FP) 포맷일 수 있고, 제2 포맷은 블록 부동 소수점(BFP) 포맷일 수 있다. 구체 적인 예로, 제1 포맷은 FP16 포맷이고 제2 포맷은 블록 크기가 64개 요소, 멘티사 비트 폭(mantissa bit widt h)이 8비트, 공유 지수(shared exponent)가 8비트인 BFP 포맷이다(BFP16-64 포맷). 이 경우, 복수의 매트릭스 출력은 64.64바이트 타일의 멘티사 및 64바이트 행의 공유 지수(shared exponents)로 특징지어질 수 있다. 본 발명의 실시예는 출력 버퍼에 저장된 64x64 요소의 FP16 타일을 열을 따라 차단된 BFP16-64 타일로 변환하기 위 한 컬럼 차단 컨버터(column blocking converter)를 위한 효율적인 알고리즘 및 하드웨어 아키텍처를 포함한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 47, "content": "[71] 일 실시예에서, 크로스바 컨버터 디바이스는 복수의 매트릭스 출력 각각에 대한 최대 지수 값을 저장 하도록 구성된 최대 지수 레지스터(max exponent register)를 포함한다. 상기 OB 디바이스와 상기 컨버터 디바이스는 복수의 매트릭스 출력 각각에 대한 최대 지수 값을 제1 행 단위 프로세스(row-by-row process)로 결정하고, 복수의 매트릭스 출력 각각에 대한 멘티사 값을 제2 행 단위 프로세스로 결정하고, 최대 지수 값과 멘티사 값을 메모리 디바이스에 저장하도록 함께 구성될 수 있다. 상기 최대 지수 레지스터는 제1 행 단위 프로세스(row-by-row process)에서 최대 지수 값을 저장하기 위해 사용될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 48, "content": "[72] 구체적인 예에서, 상기 크로스바 컨버터 디바이스는 제2 행 단위 프로세스(row-by-row process) 동안 복수의 매트릭스 출력 각각에 대해 시프팅 프로세스 및 멘티사 값에 대한 환산/반올림 프로세스를 수행하도록 구성된다. 크로스바 디바이스는 제2 행 단위 프로세스의 각 행 후에 멘티사 값을 상기 메모리 디바이스 에 기록하도록 구성될 수 있다. 또한, 상기 크로스바 디바이스는 제2 행 단위 프로세스가 끝난 후 최 대 지수 값을 상기 메모리 디바이스에 기록하도록 구성될 수 있다. 상기 크로스바 컨버터 디바이스는 피드백 구성으로 상기 OB 디바이스에 결합되어 제1 행 단위 및 제2 행 단위 프로세스를 수행하도록 구성될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 49, "content": "[73] 도 7a와 비교하여, 도 7b는 상기 OB 컨버터 디바이스가 상기 크로스바 컨버터 디바이스에 결합 되는 최대 지수 레지스터를 포함하는 컬럼 차단 컨버터 장치의 대안적인 장치 아키텍처를 나타낸다. 크로스바 컨버터 디바이스 대신에, 상기 OB 컨버터 디바이스는 제1 행 단위 프로세스(row-by-row process)에서 복수의 매트릭스 출력 각각에 대한 최대 지수 값을 결정하고, 이러한 제1 최대 지수 레지스터 에 최대 지수 값을 저장하도록 구성될 수 있다. 그런 다음, 상기 크로스바 컨버터 디바이스는 제1 최 대 지수 레지스터의 최대 지수 값을 제2 최대 지수 레지스터에 저장하고, 제2 행 단위 프로세스(row- by-row process)에서 상기 OB 디바이스로부터의 복수의 매트릭스 출력 각각에 대한 멘티사 값을 결정하도록 구 성될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 50, "content": "[74] 제1 아키텍처와 유사하게, 상기 크로스바 컨버터 디바이스는 복수의 매트릭스 출력 각각에 대해 시 프팅 프로세스 및 멘티사 값에 대한 환산/반올림 프로세스를 수행하도록 구성될 수 있다. 그리고, 크로스바 디 바이스는 제2 행 단위 프로세스(row-by-row process)를 수행한 후 최대 지수 값을 상기 메모리 디바이스 에 기록(write)하도록 구성될 수 있다. 상기 OB 컨버터 디바이스 및 크로스바 컨버터 디바이스 에 의해 수행되는 프로세스에 대한 보다 상세한 설명은 도 8a 및 도 8b를 참조하여 설명한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 51, "content": "[75] 도 8a는 본 발명의 일 실시예에 따른 컬럼 차단 컨버터 장치의 동작 방법을 설명하기 위한 단순화된 흐름도이다. 이 방법은 도 7a에 도시된 장치에 대응하는 것으로서, 상기 크로스바 컨버터 디바이스는 제1 및 제2 행 단위 프로세스(row-by-row processes)를 수행하도록 구성된다. 도시된 바와 같이, 방법은 상기 OB 컨버터에서 복수의 매트릭스 출력(NxM 매트릭스 출력의 타일; 여기서, N 및 M은 정수임)을 수신하 는 것으로 시작된다. 일례에서, 이러한 매트릭스 출력은 상기 OB 컨버터가 제1 포맷(\"D1-F1\" 내지 \"DN- F1\"로 표시됨)으로 변환하여 상기 OB 디바이스/뱅크(DN,M-F1로 표시됨)에 쓰는 매트릭스 곱셈(예를 들어, 소프트맥스 함수의 경우)의 결과(\"Data1\" 내지 \"DataN\"으로 표시된 각 행)이다. 64x64 바이트 예시를 고려하면,연속된 64개의 요소들 각각은 BFP32-1 포맷이며, 상기 OB 컨버터 디바이스는 FP16 포맷으로 변환한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 52, "content": "[76] 여기서, 상기 크로스바 컨버터 디바이스는 상기 OB 뱅크를 판독하여 각각 최대 지수 및 멘티사 값을 결정하기 위한 제1 행 및 제2 행 단위 프로세스(row-by-row process)를 수행한다. 상기 크로스바 컨버터 디바이스는 상기 OB 뱅크에 저장된 데이터의 각 행을 한 번에 한 행 씩 판독(read)하여 각 항목의 최 대 지수 값을 결정하고 최대 지수 레지스터를 업데이트한다(예: if expi < reg_exp[i], then reg_exp[i] = expi). 64x64 바이트 예시에서, 상기 컨버터 디바이스는 한 번에 64개의 FP16 요소 행을 읽는다. 모든 행 이 처리된 후, 최대 지수 레지스터는 상기 OB 뱅크에 저장된 데이터 타일의 각 열에 대한 최대 지수 (\"Exp1\" 내지 \"ExpM\"으로 표시됨)를 포함한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 53, "content": "[77] 그런 다음, 상기 컨버터 디바이스는 제2 행 단위 프로세스에서 OB 뱅크로부터 각 행을 다시 읽 어들여 멘티사 값을 결정한다. 각각의 OB 뱅크 엔트리(entry)에 대해, 상기 컨버터 디바이스는 시프팅 프 로세스 및 환산/반올림 프로세스를 수행하여 멘티사 값을 원하는 포맷(예를 들어, 정수 포맷 또는 다른 수치 포 맷)으로 변환할 수 있다. 64x64 바이트 예시에서, 시프트 및 반올림 프로세스는 멘티사 값을 8비트 정수(int8) 포맷으로 변환하는 결과를 초래할 수 있다. 멘티사 행(\"Man1\"에서 \"ManN\"으로 표시됨)을 처리한 후, 처리된 데 이터는 메모리 디바이스에 기록되도록 전송된다. 모든 행이 처리되면 컬럼 차단 구성에서 멘티사 제2 포맷 (\"DN,M-F2\"로 표시)으로 변환하는 작업이 완료된다. 이후 전송된 최대 지수 레지스터 데이터로 상기 메모리 디 바이스는 각 열이 제2 포맷인 연속적인 데이터 블록을 포함하게 된다. 64x64 바이트 매트릭스 데이터 예제 에서, 연속 블록은 65x64 바이트로 특징지어지며 각 열은 BFP16-64 포맷이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 54, "content": "[78] 도 8b는 본 발명의 일 실시예에 따른 컬럼 차단 컨버터 장치의 동작 방법을 설명하기 위한 단순화된 흐름도이다. 이 방법은 도 7b에 도시된 장치에 대응하는 것으로서, 상기 OB 컨버터 디바이스는 자체 최대 지수 레지스터를 사용하여 제1 행 단위 프로세스(row-by-row process)를 수행하도록 구성되고, 상기 크로스바 컨버터 디바이스는 제2 행 단위 프로세스를 수행하도록 구성된다. 방법 801과 동일한 표시를 사 용하여, 이러한 방법 은 상기 OB 컨버터에서 복수의 매트릭스 출력(매트릭스 출력의 타일)을 수신하 는 것으로 시작된다. 일례에서, OB 컨버터 디바이스는 출력을 제1 포맷으로 변환한다. 데이터의 각 행이 제1 포맷으로 변환된 후, 상기 OB 컨버터 디바이스는 또한 각 항목의 최대 지수 값을 결정하고 최대 지수 레지스터를 업데이트한다(예: if expi < reg_exp[i], then reg_exp[i] = expi). 출력의 모든 행이 상기 OB 컨버터 디바이스에 의해 처리된 후, 최대 지수 레지스터는 타일의 각 열에 대한 최대 지수를 포함한 다. 64x64 바이트의 예를 고려하면, 행에 있는 64개의 요소 각각은 BFP32-1 포맷(32비트 부동소수점 포맷)이며, OB 컨버터 디바이스는 이를 FP16 포맷(16비트 부동소수점 포맷)으로 변환한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 55, "content": "[79] 그런 다음, 상기 크로스바 컨버터 디바이스는 상기 OB 컨버터 레지스터로부터 자신의 최대 지수 레지스터로 최대 지수 데이터를 판독(read)한다. 상기 방법 과 유사하게, 상기 크로스바 컨버터 디바 이스는 제2 행 단위 프로세스(row-by-row process )에서 상기 OB 뱅크로부터 각 행을 판독하여 멘티 사 값을 결정한다. 상기 컨버터 디바이스는 또한 시프트 프로세스 및 환산/반올림 프로세스를 수행하여 멘 티사 값을 원하는 포맷(예: 정수 포맷 또는 다른 수치 포맷)으로 변환한다. 멘티사 행을 처리한 후, 처리된 데 이터는 메모리 디바이스에 기록되도록 전송된다. 모든 행이 처리되면 멘티사를 컬럼 차단 구성의 제2 포맷 으로 변환하는 작업이 완료된다. 이후 전송된 최대 지수 레지스터 데이터와 함께 상기 메모리 디바이스는 각 열이 제2 포맷인 연속적인 데이터 블록을 포함하게 된다. 64x64 바이트 매트릭스 데이터 예시에서, 연속 블 록은 65x64 바이트로 특징지어지고 각 열은 BFP16-64 포맷이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 56, "content": "[80] 이러한 예들은 FP 및 BFP 수치 포맷과 관련하여 논의되었지만, 컬럼 차단 변환 장치 및 그 방법은 대응하 는 지수 및 멘티사 값에 의해 결정될 수 있는 임의의 제1 포맷으로부터 임의의 제2 포맷으로의 데이터의 변환에 적용될 수 있다. 또한 공유 블록 지수(shared block exponent)를 계산하는 방법에는 다양한 변형이 있는데, 예 를 들어 최대 지수(max exponent) 대신 백분위수 값(percentile value)을 사용할 수 있다. 또한, 버퍼 메모리 로드/저장이 열 단위로 구현되는 경우, 본원에 설명된 동일한 기법을 사용하여 열 단위 스토리지 구성에서 행 단위 스토리지 구성으로 변환할 수 있다. 당업자는 이러한 블로킹 변환 방법 및 구조의 다른 변형, 수정 및 대 안을 인식할 것이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 57, "content": "[81] 도 9는 변환기와 예시적인 AI 가속기 장치 사이의 매핑 프로세스를 예시하는 단순화된 블록 흐름도이다. 도시된 바와 같이, 변환기는 복수의 변환기 레이어들을 포함하며, 각 레이어들은 어텐션 레이어(90 2)를 갖는다. 이 경우, 앞서 설명한 바와 같이 어텐션 기능(attention function)을 계산하는 16개의 어텐션 헤드(예컨대, BERT Large)가 있다. 이러한 16개의 어텐션 헤드는 타일 CPU들과 통신하는 글로벌 CPU를 통해 AI 가속기 장치(장치들(201, 202)과 유사)의 16개 슬라이스에 매핑된다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 58, "content": "[82] 일 실시예에 따르면, 본 발명은 매트릭스 컴퓨팅 장치에서 데이터 변환을 위한 방법 및 장치/디바이스를 제공한다. 특정 예에서, 매트릭스 컴퓨팅 장치는 앞서 논의된 NLP 워크로드를 포함하여 심층 신경망 추론 애플 리케이션을 가속화하는 데 사용되는 도트 곱(dot product; 내적) 및 매트릭스 곱셈 하드웨어의 핵심 빌딩 블록 역할을 하는 곱하기 및 누적(MAC; multiply and accumulate) 유닛으로 구성될 수 있다. 이러한 애플리케이션에 서는 두 가지 이상의 데이터 포맷을 처리해야 할 필요가 있을 수 있다. 예를 들어, 효율적인 MAC 구현은 고정 소수점 또는 블록 부동 소수점(BFP) 수치 포맷을 지원하는 정수 연산을 기반으로 하는 경우가 많다. 그러나, 특 정 애플리케이션에서, MAC 유닛 또는 다른 매트릭스 컴퓨팅 장치는 부동 소수점(FP) 또는 브레인 부동 소수점 (Bfloat) 수치 포맷을 처리할 수 있는 능력을 갖는 것이 바람직하다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 59, "content": "[83] 따라서, 본 발명은 데이터를 분할하고, 분할된 데이터 부분을 매트릭스 컴퓨팅 장치의 네이티브 포맷으로 병렬 처리함으로써, 타겟 포맷의 매트릭스 데이터를 처리하도록 구성된 매트릭스 컴퓨팅 장치를 가능하게 하는 방법 및 장치를 제공한다. 예를 들어, 본 발명은 네이티브 포맷을 8비트 정수(int8) 포맷으로, 타겟 포맷을 16 비트 부동소수점(FP16) 포맷으로 설명한다. 본 매트릭스 컴퓨팅 장치의 실시예는 앞서 설명한 인공지능 가속기 시스템과 같은 인공지능 가속기 IC를 위한 IC로 구성될 수 있다. 보다 상세한 내용은 도 10a 및 도 10b를 참조 하여 이하에서 설명한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 60, "content": "[84] 도 10a는 본 발명의 일 실시예에 따른 매트릭스 컴퓨팅 장치를 도시한 단순화된 사시도이다. 도시된 바와 같이, 이 장치는 입력 버퍼(IB) 디바이스, 상기 IB 디바이스에 결합된 컴퓨팅 디바이스 (예컨대, DIMC 장치), 및 상기 컴퓨팅 디바이스에 결합된 출력 버퍼(OB) 디바이스를 포함하 여 도 3B의 예시적인 슬라이스 디바이스와 유사하게 구성될 수 있다. 또한, SIMD(Single Instruction, Multiple Data) 디바이스가 상기 OB 디바이스에 결합될 수 있다. 슬라이스 디바이스와 유사하 게, 이 장치/디바이스는 인공지능 가속기 시스템(도 1A 및 도 1B의 예들 참조)의 일부인 칩렛 디바이스 (도 2A 및 도 2B의 예들 참조) 내에 구성될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 61, "content": "[85] 일례에서, 입력 버퍼(IB) 디바이스는 하나 이상의 매트릭스 입력(예를 들어, 메모리 디바이스 등으 로부터)을 수신하도록 구성된다. 이 IB 디바이스는 이전에 도시된 IB 디바이스들(예컨대, 도 3a 및 도 3b)과 유사하게 구성될 수 있다. 이러한 각 매트릭스 입력은 제1 포맷을 특징으로 하고 적어도 제1 입력 부분 및 제2 입력 부분을 구비할 수 있다. 이러한 입력 부분은 컴퓨팅 디바이스에 의해 병렬로 처리될 매트릭 스 입력의 분할된 부분이다. 실시예에 따라, 매트릭스 입력은 매트릭스 가중치 및 활성화 부분을 포함하는 복수 의 입력 부분을 가질 수 있다(도 10b 참조)."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 62, "content": "[86] 상기 IB 디바이스는 매트릭스 입력을 제1 포맷으로 변환하도록 구성된 입력 컨버터 디바이스로부터 제1 포맷의 제1 매트릭스 입력 또는 복수의 매트릭스 입력을 수신할 수 있다. 이러한 입력 컨버터 디바이스는, 예를 들어, CPU(예컨대, 도 2B에 도시된 타일 CPU), IB 디바이스에 결합된 인라인 입력 컨버터 (inline input converter)(점선으로 도시됨) 등과 같은 것일 수 있다. 앞의 예들을 참조하면, 매트릭스 입력(들)은 FP 포맷, B플로트(Bfloat) 포맷 등이 될 수 있다. 제1 포맷은 BFP 포맷, 고정 소수점 포맷 등이 될 수 있다. 제1 포맷이 원래 포맷으로부터 매트릭스 데이터의 변환된 분할을 허용하는 한, 다른 포맷도 사용될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 63, "content": "[87] 단지 예시적으로, 매트릭스 컴퓨팅 장치는 정수 수치 포맷으로 매트릭스 컴퓨팅을 수행하도록 구성될 수 있다. 이러한 경우, 컴퓨팅 장치는 정수 포맷 내에 맞을 수 있는 데이터 부분으로 매트릭스 입력을 처리하도록 구성될 수 있다. 예를 들어, 복수의 컴퓨팅 디바이스 각각은 int8 포맷의 매트릭스 컴퓨팅을 위해 구성될 수 있 으며, 매트릭스 입력은 64x64 바이트 타일 구성의 FP16 포맷이 될 수 있다. 이 경우 입력 컨버터 장치(예: 타일 CPU, 인라인 입력 컨버터 1012 등)는 FP16 매트릭스 입력을 16비트 멘티사 및 8비트 지수를 사용하여 24비트 블 록 부동 소수점(BFP24) 포맷으로 변환한다. 그런 다음, 멘티사는 컴퓨팅 디바이스에 의해 병렬로 처리될 수 있도록 두 개의 8비트 부분, 최상위 바이트(MSB) 부분 및 최하위 바이트(LSB) 부분으로 분할될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 64, "content": "[88] 일례에서, 상기 컴퓨팅 디바이스는 적어도 제1 컴퓨팅 유닛 및 제2 컴퓨팅 유닛을 갖는 복수의 컴퓨팅 유닛을 포함한다. 이 컴퓨팅 유닛 쌍(pair)은 비-네이티브 포맷(non-native format)의 매 트릭스 입력에 대한 매트릭스 컴퓨팅을 수행하도록 구성될 수 있다. 보다 구체적으로, 상기 제1 컴퓨팅 유닛 은 적어도 제1 입력 부분을 사용하여 제1 매트릭스 출력을 결정하도록 구성될 수 있고, 제2 컴퓨팅 유닛 은 적어도 제2 입력 부분을 사용하여 제2 매트릭스 출력을 결정하도록 구성될 수 있다. 그런 다음, 컴퓨팅 디바이스는 상기 제1 매트릭스 출력 및 상기 제2 매트릭스 출력을 사용하여 제2 포맷으로, 결합 매트 릭스 출력(combined matrix output)을 결정하도록 구성될 수 있다. 특정 예에서, 상기 컴퓨팅 디바이스는 상기 제1 매트릭스 출력을 시프트하고, 시프트된 제1 매트릭스 출력을 상기 제2 매트릭스 출력에 합산(add)함으 로써 상기 결합 매트릭스 출력을 결정한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 65, "content": "[89] 예시적인 예에서, 각 매트릭스 입력은 매트릭스 가중치 및 매트릭스 활성화를 포함한다. 각각의 매트릭스 가중치 입력은 매트릭스 가중치 지수(matrix weight exponent) 및 매트릭스 가중치 멘티사(matrix weight mantissa)를 포함할 수 있다. FP16의 예를 다시 참조하면, 매트릭스 가중치 지수는 8비트를 포함하고 매트릭스 가중치 멘티사는 8비트 MSB 부분과 8비트 LSB 부분으로 분리될 수 있는 16비트를 포함한다. 마찬가지로, 매트릭 스 활성화 지수(matrix activation exponent)도 8비트를 포함하고 매트릭스 활성화 멘티사도 8비트 MSB 부분과 8비트 LSB 부분으로 분리될 수 있는 16비트를 포함한다. 이 경우, 상기 컴퓨팅 디바이스는 상기 매트릭스 가중 치 멘티사의 MSB 부분 및 매트릭스 활성화을 사용하여 도트 곱(dot product) 프로세스를 수행하여 상기 제1 매 트릭스 출력을 결정한다. 유사하게, 상기 컴퓨팅 디바이스는 상기 매트릭스 가중치 멘티사의 LSB 부분 및 매트 릭스 활성화분을 사용하여 도트 곱 프로세스를 수행함으로써 상기 제2 매트릭스 출력을 결정한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 66, "content": "[90] 앞의 예에서는 매트릭스 입력 데이터를 두 부분으로 분할하는 것에 대해서만 설명했지만, 다른 예에서는 복수의 컴퓨팅 디바이스에 의해 병렬로 처리되는 복수의 데이터 부분으로 데이터를 분할할 수도 있다. 이러한 경우, 컴퓨팅 디바이스는 유사한 시프트 및 더하기 프로세스(shifting and addition processes)를 사용 하여 복수의 매트릭스 출력을 결정하고, 이러한 매트릭스 출력을 적절한 순서로 배치된 각 데이터 부분과 결합 매트릭스 출력으로 결합할 것이다. 이러한 부분은 컴퓨팅 디바이스/장치의 기본 포맷과 일치하는 세그먼트화된 부분에 저장될 수도 있다. 당업자는 데이터 포맷 및 데이터 분할의 선택에 대한 다른 변형, 수정 및 대안을 인 식할 것이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 67, "content": "[91] FP16 실시예를 고려하면, 상기 제1 입력 부분은 MSB 가중치 부분이고, 상기 제2 입력 부분은 LSB 가중치 부분이다. 상기 제1 컴퓨팅 유닛은 MSB 부분을 사용하여 제1 매트릭스 출력을 결정하도록 구성되고, 상기 제2 컴퓨팅 유닛은 LSB 부분을 사용하여 제2 매트릭스 출력을 결정하도록 구성될 수 있다. 상기 매트릭스 출력은 도 10b에 도시된 바와 같이 MSB 부분을 8비트 이동하고 LSB 부분과 더함으로써 결합된다. 이러한 결과 결합 매트릭스 출력은 38비트 멘티사(64x64 행렬의 경우)와 8비트 지수(8-bit exponent)를 가지며, 이는 BFP46- 1 포맷으로 표시될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 68, "content": "[92] 일예에서, 상기 컴퓨팅 디바이스는 복수의 컴퓨팅 유닛에 결합된 정렬 디바이스를 포함한다. 상기 정렬 디바이스는 결합 출력(combined output)을 사용하여 제3 포맷으로, 환산/반올림 매트릭스 출력 을 결정하도록 구성될 수 있다. 이러한 환산/반올림 프로세스는 후속 PPR(partial products reduction) 프로세 스를 위한 매트릭스 출력을 준비하는 데 사용될 수 있다. FP16 예제에서는 BFP46-1 포맷의 결합 매트릭스 출력 을 BFP32-1 포맷의 매트릭스 출력으로 반내림(round down)할 수 있다. 다른 예에서, BFP46-1 결합 매트릭스 출 력은 상기 정렬 디바이스 또는 정렬 디바이스에 결합된 데이터 컨버터에 의해 FP32 매트릭스 출력 으로 변환될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 69, "content": "[93] 일 실시예에서, 상기 PPR 디바이스는 상기 정렬 디바이스에 결합된다. 상기 PPR 디바이스 는 환산/반올림 매트릭스 출력을 사용하여 환원 매트릭스 출력(reduced matrix output)을 결정하도록 구 성될 수 있다. 상기 PPR 프로세스는 상기 OB 디바이스에 저장될 원래 데이터 포맷(예컨대, FP16)을 후속 적으로 변환하기 위한 매트릭스 출력을 준비하기 위해 사용될 수 있다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 70, "content": "[94] 예에서, 상기 컴퓨팅 디바이스는 또한 이전 매트릭스 출력을 사용하여 변환된 출력 포맷으로 제1 변 환 매트릭스 출력을 결정하도록 구성되는 컴퓨팅 컨버터를 포함한다. FP16 예에서, 상기 컴퓨팅 컨버터 는 BFP32-1 포맷의 환원 매트릭스 출력을 FP16 매트릭스 출력으로 변환한다. 상기 결합 매트릭스 출력이 FP32 포맷으로 변환되는 경우, 상기 컴퓨팅 컨버터는 FP32 포맷의 환원 매트릭스 출력을 FP16 매트릭스 출력으로 변환한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 71, "content": "[95] 일 실시예에서, 상기 OB 디바이스는 결과 변환 매트릭스 출력을 저장하도록 구성된다. 이 OB 디바이 스는 이전에 도시된 OB 디바이스들(예컨대, 도 3a 및 도 3b)과 유사하게 구성될 수 있다. FP16 예의 IB 디바이스에 대해 논의한 바와 같이, 상기 OB 디바이스는 64x64 바이트 타일 구성으로 매트릭스 출 력을 저장하도록 구성될 수 있다. 매트릭스 데이터 변환 및 컴퓨팅 프로세스에 대한 추가 세부 사항은 도 10b를 참조하여 논의된다.[96] 본 매트릭스 컴퓨팅 장치 및 관련 방법의 실시예는 많은 이점을 제공할 수 있다. 본 방법 및 장치는 네이 티브 포맷과 호환되는 부분으로 분할될 수 있는 상이한 데이터 포맷의 매트릭스 입력을 계산 처리할 수 있게 한 다. 또한, 이러한 다중 포맷 기능은 완전히 별도의 하드웨어 및 컴퓨팅 경로를 필요로 하지 않고도 수행될 수 있다. 또한, 이러한 이점은 실리콘 면적의 추가 비용을 최소화하면서 IC 칩 및 칩렛 디바이스에서 실현될 수 있 다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 72, "content": "[97] 도 10b는 본 발명의 일 실시예에 따른 매트릭스 컴퓨팅 장치에서 데이터 분할 및 병렬 처리를 이용 한 데이터 포맷 변환 방법을 예시하는 단순화된 도면이다. 도시된 바와 같이, 장치는 상기 IB 디바이스 및 0 내지 N으로 번호가 매겨진 복수의 연산 유닛을 갖는 컴퓨팅 디바이스, 정렬 디바이스 , PPR 디바이스 및 컴퓨팅 컨버터 를 포함한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 73, "content": "[98] 앞의 예에서 설명한 바와 같이, 각 매트릭스 입력은 매트릭스 가중치 및 매트릭스 활성화를 포함할 수 있 다. 각각의 매트릭스 가중치 입력은 매트릭스 가중치 지수(matrix weight exponent) 및 매트릭스 가중치 멘티사 (matrix weight mantissa)를 포함할 수 있다. FP16의 예를 다시 참조하면, 상기 매트릭스 가중치 지수는 8비트 를 포함하고 매트릭스 가중치 멘티사는 8비트 MSB 부분과 8비트 LSB 부분으로 분리할 수 있는 16비트를 포함한 다. 이 경우, 상기 매트릭스 활성화 지수는 또한 8비트를 포함하고 매트릭스 활성화 멘티사는 8비트 MSB 부분과 8비트 LSB 부분으로 분리될 수 있는 16비트를 포함한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 74, "content": "[99] 이 경우, 상기 매트릭스 가중치의 제1 부분(예컨대, MSB)은 제1 컴퓨팅 유닛(1022-0)(IMC0으로 도시됨)에 저장되고, 상기 매트릭스 가중치의 제2 부분(예컨대, LSB)은 제2 컴퓨팅 유닛(1022-4)(IMC4로 도시됨)에 저장된 다. 상기 컴퓨팅 디바이스는 매트릭스 활성화 및 매트릭스 가중치의 제1 부분을 사용하여 도트 곱(dot product) 프로세스를 수행함으로써 제1 매트릭스 출력을 결정하고, 매트릭스 활성화 및 매트릭스 가중치의 제2 부분을 사용하여 도트 곱 프로세스를 수행함으로써 제2 매트릭스 출력을 결정한다. 그런 다음, 상기 제1 매트릭 스 출력을 시프트(FP16 예에서는 8비트)하고 상기 제2 매트릭스 출력에 더하여 결합 매트릭스 출력을 결정한다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 75, "content": "[100] 이어서, 상기 정렬 디바이스는 결합 매트릭스 출력으로부터 환산/반올림 매트릭스 출력을 결정할 수 있고, 상기 PPR 디바이스는 환산/반올림 매트릭스 출력으로부터 상기 환원 매트릭스 출력을 결정할 수 있다. 또한, 상기 컴퓨팅 컨버터는 환원 매트릭스 출력으로부터 변환 매트릭스 출력을 결정할 수 있다. 매트릭스 출력의 흐름도는 컴퓨팅 디바이스의 컴포넌트들과 관련하여 점선들 내에 도 10b에 도시되어 있 다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 76, "content": "[101] 앞서 논의한 바와 같이, 다른 예들은 복수의 컴퓨팅 유닛들에 의해 병렬로 처리되는 복수의 데이터 부분 으로 데이터를 분할할 수 있다. 이러한 경우, 상기 컴퓨팅 디바이스는 유사한 시프트 및 더하기 프로세스 를 사용하여 복수의 매트릭스 출력을 결정하고, 이러한 매트릭스 출력을 적절한 순서로 배치된 각 데이터 부분 과 결합 매트릭스 출력으로 결합할 것이다. 이러한 부분은 컴퓨팅 디바이스의 기본 포맷과 일치하는 세그먼트화 된 부분에 저장될 수도 있다(예: FP16 매트릭스 입력을 처리하도록 구성된 int8 컴퓨팅 장치). 또한, 매트릭스 출력을 처리하기 위한 단계들은 각각의 하드웨어 구성요소와 함께 애플리케이션에 따라 추가, 제거 또는 재배열 될 수 있다. 당업자는 데이터 포맷 및 데이터 세분화의 선택에 대한 다른 변형, 수정 및 대안을 인식할 것이다."}
{"patent_id": "10-2023-0112606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 77, "content": "[102] 상기는 특정 실시예의 전체 설명이지만, 다양한 수정, 대체 구성 및 균등 구성이 사용될 수 있다. 예를 들어, AI 가속기 장치 및 칩렛 디바이스는 본 명세서의 외부 뿐만 아니라 위에서 설명한 요소들의 임의의 조합 을 포함할 수 있다. 따라서, 상기 설명 및 예시는 첨부된 청구범위에 의해 정의되는 본 발명의 범위를 제한하는 것으로 간주되어서는 안 된다. 도면 도면1a 도면1b 도면2a 도면2b 도면3a 도면3b 도면4 도면5a 도면5b 도면6 도면7a 도면7b 도면8a 도면8b 도면9 도면10a 도면10b"}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 1, "content": "[10] 도 1a 내지 도 1b는 본 발명의 예에 따른 AI 가속기 장치를 도시하는 단순화된 블록도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 2, "content": "[11] 도 2a 내지 도 2b는 본 발명의 예에 따른 16-슬라이스 칩렛 디바이스를 도시하는 단순화된 블록도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 3, "content": "[12] 도 3a 내지 도 3b는 본 발명의 예에 따른 슬라이스 디바이스를 도시하는 단순화된 블록도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 4, "content": "[13] 도 4는 본 발명의 일례에 따른 인-메모리-컴퓨트(IMC: in-memory-compute) 모듈을 도시하는 단순화된 블록 도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 5, "content": "[14] 도 5a는 본 발명의 일례에 따라 슬라이스 디바이스에서 처리되는 데이터의 수치 포맷(numerical format)을 도시하는 단순화된 블록 흐름도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 6, "content": "[15] 도 5b는 예시적인 수치 포맷을 도시하는 단순화된 다이어그램이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 7, "content": "[16] 도 6은 변환기(transformer) 아키텍처의 단순화된 블록도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 8, "content": "[17] 도 7a는 본 발명의 일 실시예에 따른 컬럼 차단 컨버터(column blocking converter) 장치를 예시하는 단 순화된 블록도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 9, "content": "[18] 도 7b는 본 발명의 일실시예에 따른 컬럼 차단 컨버터(column blocking converter) 장치를 설명하기 위한 단순화된 블록도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 10, "content": "[19] 도 8a는 본 발명의 일실시예에 따른 컬럼 차단 장치의 동작 방법을 설명하기 위한 단순화된 흐름도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 11, "content": "[20] 도 8b는 본 발명의 일 실시예에 따른 컬럼 차단 장치의 동작 방법을 예시하는 단순화된 흐름도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 12, "content": "[21] 도 9는 본 발명의 일 실시예에 따른 변환기 디바이스와 인공지능 가속기 장치 사이의 매핑 과정을 설명하 기 위한 단순화된 블록 흐름도이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 13, "content": "[22] 도 10a는 본 발명의 일 실시예에 따른 매트릭스 컴퓨팅 장치를 예시하는 단순화된 도면이다."}
{"patent_id": "10-2023-0112606", "section": "도면", "subsection": "도면설명", "item": 14, "content": "[23] 도 10b는 본 발명의 일 실시예에 따른 매트릭스 컴퓨팅 장치의 동작 방법을 설명하기 위한 단순화된 사시 도이다."}
