{"patent_id": "10-2023-0074606", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0108221", "출원번호": "10-2023-0074606", "발명의 명칭": "인공지능 모델에 대한 가속기 설계 장치 및 방법", "출원인": "주식회사 딥이티", "발명자": "조용범"}}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능 모델에 대한 가속기 설계 방법에 있어서,사용자가 설계한 타겟 네트워크에 대응하도록 생성된 네트워크 정의 파일을 획득하는 단계;상기 네트워크 정의 파일을 기초로 하여 상기 타겟 네트워크를 이루는 복수의 레이어를 분해하는 단계; 및상기 분해된 복수의 레이어 각각을 미리 설정된 템플릿에 매핑하고, 미리 구축된 인공지능 기반의 최적화 모델을 이용하여 상기 타겟 네트워크를 최적화 하는 단계,를 포함하는, 가속기 설계 방법."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 최적화 하는 단계는,상기 타겟 네트워크와 관련된 코드 정보로부터 소정의 파라미터를 추출하는 단계;상기 추출된 파라미터를 이용하여 상기 최적화 모델을 훈련시키는 단계; 및상기 훈련된 최적화 모델을 이용하여 상기 타겟 네트워크의 구현을 위한 하드웨어 디자인을 도출하는 단계,를 포함하는 것인, 가속기 설계 방법."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 최적화 모델은 장단기 메모리(Long Short Term Memory, LSTM) 기반의 인공지능 모델을 포함하는 것인, 가속기 설계 방법."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 파라미터를 추출하는 단계는,추상 구문 트리(Abstract Syntax Tree)를 이용하여 상기 코드 정보를 구조화 하는 단계,를 포함하는 것인, 가속기 설계 방법."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서,상기 파라미터는 상기 타겟 네트워크와 연계된 연산자, 변수, 병렬 구조 및 메모리 할당 사이즈 중 적어도 하나를 포함하는 것인, 가속기 설계 방법."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제2항에 있어서,상기 하드웨어 디자인을 도출하는 단계는,인터체인지(Interchange) 기법, 타일(Tile) 기법 및 펼치기(Unrolling) 기법 중 적어도 하나를 포함하는 최적화기법을 상기 타겟 네트워크에 대하여 적용하는 단계,공개특허 10-2024-0108221-3-를 포함하는 것인, 가속기 설계 방법."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 템플릿은 SDSOC(Software defined system-on-chip) 기반의 템플릿을 포함하는 것을 특징으로 하는, 가속기 설계 방법."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 타겟 네트워크는 합성곱 신경망(Convolutional Neural Network, CNN)을 포함하고,상기 복수의 레이어는 합성곱 레이어, 풀링 레이어 및 완전 연결 레이어 중 적어도 하나를 포함하는 것인, 가속기 설계 방법."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "인공지능 모델에 대한 가속기 설계 장치에 있어서,사용자가 설계한 타겟 네트워크에 대응하도록 생성된 네트워크 정의 파일을 획득하고, 상기 네트워크 정의 파일을 기초로 하여 상기 타겟 네트워크를 이루는 복수의 레이어를 분해하는 네트워크 분석부; 및상기 분해된 복수의 레이어 각각을 미리 설정된 템플릿에 매핑하고, 미리 구축된 인공지능 기반의 최적화 모델을 이용하여 상기 타겟 네트워크를 최적화 하는 자동 생성부,를 포함하는, 가속기 설계 장치."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 자동 생성부는,상기 타겟 네트워크와 관련된 코드 정보로부터 소정의 파라미터를 추출하고, 상기 추출된 파라미터를 이용하여상기 최적화 모델을 훈련시키고, 상기 훈련된 최적화 모델을 이용하여 상기 타겟 네트워크의 구현을 위한 하드웨어 디자인을 도출하는 것인, 가속기 설계 장치."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 최적화 모델은 장단기 메모리(Long Short Term Memory, LSTM) 기반의 인공지능 모델을 포함하는 것인, 가속기 설계 장치."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,상기 자동 생성부는,추상 구문 트리(Abstract Syntax Tree)를 이용하여 상기 코드 정보를 구조화 하는 것인, 가속기 설계 장치."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10항에 있어서,상기 파라미터는 상기 타겟 네트워크와 연계된 연산자, 변수, 병렬 구조 및 메모리 할당 사이즈 중 적어도 하나를 포함하는 것인, 가속기 설계 장치."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "공개특허 10-2024-0108221-4-제10항에 있어서,상기 자동 생성부는,인터체인지(Interchange) 기법, 타일(Tile) 기법 및 펼치기(Unrolling) 기법 중 적어도 하나를 포함하는 최적화기법을 상기 타겟 네트워크에 대하여 적용하는 것인, 가속기 설계 장치."}
{"patent_id": "10-2023-0074606", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서,상기 타겟 네트워크는 합성곱 신경망(Convolutional Neural Network, CNN)을 포함하고,상기 복수의 레이어는 합성곱 레이어, 풀링 레이어 및 완전 연결 레이어 중 적어도 하나를 포함하는 것인, 가속기 설계 장치."}
{"patent_id": "10-2023-0074606", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공지능 모델에 대한 가속기 설계 장치 및 방법이 개시되며, 본원의 일 실시예에 따른 인공지능 모델에 대한 가 속기 설계 방법은, 사용자가 설계한 타겟 네트워크에 대응하도록 생성된 네트워크 정의 파일을 획득하는 단계, 상기 네트워크 정의 파일을 기초로 하여 상기 타겟 네트워크를 이루는 복수의 레이어를 분해하는 단계 및 상기 분해된 복수의 레이어 각각을 미리 설정된 템플릿에 매핑하고, 미리 구축된 인공지능 기반의 최적화 모델을 이용 하여 상기 타겟 네트워크를 최적화 하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0074606", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본원은 인공지능 모델에 대한 가속기 설계 장치 및 방법에 관한 것이다. 예를 들면, 본원은 딥러닝 모델과 연계 된 FPGA 자동 생성기 설계 기법에 관한 것이다."}
{"patent_id": "10-2023-0074606", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "가속기 설계에서의 처리량을 개선하기 위하여는 연산 계산과 데이터 통신이 중요한 요소에 해당한다. 딥러닝 네 트워크는 다양한 연산자로 구성 되어 있으며, 모두 동일한 속성을 가지지 않으며, 비슷한 연산을 수행할 때에도 메모리를 많이 사용하거나 적게 사용하는 등 다양한 속성을 보일 수 있다. 연산자는 매개변수로 인해 속성을 변경하여 다양하게 계산할 수 있으며, 딥러닝 네트워크의 다양한 연산자들은 계산에 집중적인 연산자와 메모리 액세스에 집중적인 연산자로 분류할 수 있고, 계산에 집중적인 연산자는 대표 적으로 컨볼루션(Convolution) 연산과 완전 연결(Fully-Connected, FC) 연산을 들 수 있다. CNN을 추론하는데 구현되는 시간은 계산에 집중적인 연산자와의 계산과 선형 관계를 가지며, 메모리 액세스에 집중적인 연산자는 ReLU, Concat 등이 대표적이고, 이러한 연산자의 경우 CNN 추론 시간이 메모리 액세스와 선 형 관계를 가진다. 한편, 딥러닝 네트워크는 이 두가지 종류의 연산자가 모두 포함하여 구성되어 있지만, 두가지 속성의 연산자를 사용하여 하드웨어 성능을 완전히 사용하는 CNN의 설계에는 많은 어려움이 따른다. 따라서 개발자의 성능 요구에 맞게 자동으로 CNN 모델 최적화 설계를 수행할 수 있는 기법에 대한 개발이 요구 된다. 본원의 배경이 되는 기술은 한국등록특허공보 제10-2360116호에 개시되어 있다."}
{"patent_id": "10-2023-0074606", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본원은 전술한 종래 기술의 문제점을 해결하기 위한 것으로서, CNN 네트워크의 제약 조건과 FPGA 성능이 적절한 균형을 이루는 최적의 가속기 설계를 위해 공간 탐색 모델(space exploration model)을 적용하는 인공지능 모델 에 대한 가속기 설계 장치 및 방법을 제공하려는 것을 목적으로 한다. 다만, 본원의 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2023-0074606", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본원의 일 실시예에 따른 인공지능 모델에 대한 가속 기 설계 방법은, 사용자가 설계한 타겟 네트워크에 대응하도록 생성된 네트워크 정의 파일을 획득하는 단계, 상 기 네트워크 정의 파일을 기초로 하여 상기 타겟 네트워크를 이루는 복수의 레이어를 분해하는 단계 및 상기 분해된 복수의 레이어 각각을 미리 설정된 템플릿에 매핑하고, 미리 구축된 인공지능 기반의 최적화 모델을 이용 하여 상기 타겟 네트워크를 최적화 하는 단계를 포함할 수 있다. 또한, 상기 최적화 하는 단계는, 상기 타겟 네트워크와 관련된 코드 정보로부터 소정의 파라미터를 추출하는 단 계, 상기 추출된 파라미터를 이용하여 상기 최적화 모델을 훈련시키는 단계 및 상기 훈련된 최적화 모델을 이용 하여 상기 타겟 네트워크의 구현을 위한 하드웨어 디자인을 도출하는 단계를 포함할 수 있다. 또한, 상기 최적화 모델은 장단기 메모리(Long Short Term Memory, LSTM) 기반의 인공지능 모델을 포함할 수 있 다. 또한, 상기 파라미터를 추출하는 단계는, 추상 구문 트리(Abstract Syntax Tree)를 이용하여 상기 코드 정보를 구조화 하는 단계를 포함할 수 있다. 또한, 상기 파라미터는 상기 타겟 네트워크와 연계된 연산자, 변수, 병렬 구조 및 메모리 할당 사이즈 중 적어 도 하나를 포함할 수 있다. 또한, 상기 하드웨어 디자인을 도출하는 단계는, 인터체인지(Interchange) 기법, 타일(Tile) 기법 및 펼치기 (Unrolling) 기법 중 적어도 하나를 포함하는 최적화 기법을 상기 타겟 네트워크에 대하여 적용하는 단계를 포 함할 수 있다. 또한, 상기 템플릿은 SDSOC(Software defined system-on-chip) 기반의 템플릿을 포함할 수 있다. 또한, 상기 타겟 네트워크는 합성곱 신경망(Convolutional Neural Network, CNN)을 포함할 수 있다. 또한, 상기 복수의 레이어는 합성곱 레이어, 풀링 레이어 및 완전 연결 레이어 중 적어도 하나를 포함할 수 있 다. 한편, 본원의 일 실시예에 따른 인공지능 모델에 대한 가속기 설계 장치는, 사용자가 설계한 타겟 네트워크에 대응하도록 생성된 네트워크 정의 파일을 획득하고, 상기 네트워크 정의 파일을 기초로 하여 상기 타겟 네트워 크를 이루는 복수의 레이어를 분해하는 네트워크 분석부 및 상기 분해된 복수의 레이어 각각을 미리 설정된 템 플릿에 매핑하고, 미리 구축된 인공지능 기반의 최적화 모델을 이용하여 상기 타겟 네트워크를 최적화 하는 자 동 생성부를 포함할 수 있다. 또한, 상기 자동 생성부는, 상기 타겟 네트워크와 관련된 코드 정보로부터 소정의 파라미터를 추출하고, 상기 추출된 파라미터를 이용하여 상기 최적화 모델을 훈련시키고, 상기 훈련된 최적화 모델을 이용하여 상기 타겟 네트워크의 구현을 위한 하드웨어 디자인을 도출할 수 있다. 또한, 상기 자동 생성부는, 추상 구문 트리(Abstract Syntax Tree)를 이용하여 상기 코드 정보를 구조화 할 수 있다. 또한, 상기 자동 생성부는, 인터체인지(Interchange) 기법, 타일(Tile) 기법 및 펼치기(Unrolling) 기법 중 적 어도 하나를 포함하는 최적화 기법을 상기 타겟 네트워크에 대하여 적용할 수 있다. 상술한 과제 해결 수단은 단지 예시적인 것으로서, 본원을 제한하려는 의도로 해석되지 않아야 한다. 상술한 예 시적인 실시예 외에도, 도면 및 발명의 상세한 설명에 추가적인 실시예가 존재할 수 있다."}
{"patent_id": "10-2023-0074606", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본원의 과제 해결 수단에 의하면, CNN 네트워크의 제약 조건과 FPGA 성능이 적절한 균형을 이루는 최적 의 가속기 설계를 위해 공간 탐색 모델(space exploration model)을 적용하는 인공지능 모델에 대한 가속기 설 계 장치 및 방법을 제공할 수 있다. 다만, 본원에서 얻을 수 있는 효과는 상기된 바와 같은 효과들로 한정되지 않으며, 또 다른 효과들이 존재할 수 있다."}
{"patent_id": "10-2023-0074606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본원이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본원의 실시예를 상세히 설명한다. 그러나 본원은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본원을 명확하게 설명하기 위해서 설명과 관계없는 부분 은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본원 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\" 또는 \"간접적으로 연결\"되어 있는 경우 도 포함한다. 본원 명세서 전체에서, 어떤 부재가 다른 부재 \"상에\", \"상부에\", \"상단에\", \"하에\", \"하부에\", \"하단에\" 위치 하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재가 존 재하는 경우도 포함한다. 본원 명세서 전체에서, 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 본원은 인공지능 모델에 대한 가속기 설계 장치 및 방법에 관한 것이다. 예를 들면, 본원은 딥러닝 모델과 연계 된 FPGA 자동 생성기 설계 기법에 관한 것이다. 도 1은 본원의 일 실시예에 따른 인공지능 모델에 대한 가속기 설계 장치를 포함하는 인공지능 기반의 연산 시 스템의 개략적인 구성도이다. 도 1을 참조하면, 본원의 일 실시예에 따른 인공지능 기반의 연산 시스템은 본원의 일 실시예에 따른 인공 지능 모델에 대한 가속기 설계 장치(이하, '가속기 설계 장치'라 한다.) 및 컴퓨팅 장치를 포함 할 수 있다. 가속기 설계 장치 및 컴퓨팅 장치 상호간은 네트워크를 통해 통신할 수 있다. 네트워크는 단 말들 및 서버들과 같은 각각의 노드 상호간에 정보 교환이 가능한 연결 구조를 의미하는 것으로, 이러한 네트워 크의 일 예에는, 3GPP(3rd Generation Partnership Project) 네트워크, LTE(Long Term Evolution) 네트워 크, 5G 네트워크, WIMAX(World Interoperability for Microwave Access) 네트워크, 인터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), wifi 네트워크, 블루투스(Bluetooth) 네트워크, 위성 방송 네트워크, 아날로그 방송 네트워크, DMB(Digital Multimedia Broadcasting) 네트워크 등이 포함되나 이에 한정되지는 않는다. 컴퓨팅 장치는 예를 들면, 스마트폰(Smartphone), 스마트패드(SmartPad), 태블릿 PC등과 PCS(Personal Communication System), GSM(Global System for Mobile communication), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말기 같은 모든 종류의 무선 통신 장치일 수 있다. 한편, 도 1에는 가속기 설계 장치가 컴퓨팅 장치와 독립적으로 구비되는 것으로 도시되어 있으나, 이 에만 한정되는 것은 아니고, 본원의 구현예에 따라서 가속기 설계 장치가 컴퓨팅 장치의 하위 구성 (모듈)로서 탑재되어 컴퓨팅 장치에 구비되는 프로세싱 유닛(연산 유닛)을 이용한 인공지능 모델의 가속 연산을 위하여 후술하는 자동 생성기 설계 기법을 적용하는 형태로 본원에서 개시하는 인공지능 기반의 연산 시스템이 설계되는 것일 수 있다. 또한, 도 1을 참조하면, 컴퓨팅 장치는 제1연산 유닛 및 제2연산 유닛을 구비할 수 있다. 보다 구 체적으로 제1연산 유닛은 CPU(Central Processing Unit)를 포함하고, 제2연산 유닛은 FPGA(Field Programmable Gate Array)를 포함하는 것일 수 있으나, 이에만 한정되는 것은 아니고, 본원의 구현예에 따라 제 1연산 유닛 및 제2연산 유닛 각각은 타겟 네트워크의 학습/추론 과정에서 필요한 연산을 처리하기 위한 특성(예를 들면, 병렬 작업에 대한 적합도 등)이 상호 구분되는 다양한 프로세서, 연산 모듈 등을 폭넓게 포함 할 수 있다. 이하에서는 제1연산 유닛이 CPU에 해당하고, 제2연산 유닛이 FPGA에 해당하는 것으로 가정 하여 본원의 실시예를 설명하도록 한다. 또한, 본원의 실시예에 관한 설명에서 타겟 네트워크는 예시적으로, 합성곱 신경망(컨볼루션 뉴럴 네트워크, convolutional neural network, CNN)일 수 있으나, 이에만 한정되는 것은 아니다. 다른 예로, 인공지능 모델은 순환 신경망(RNN, Recurrent Neural Network), 심층 신뢰 신경망(DBN, Deep Belief Network), GAN(Generative Adversarial Network. 생성 대립 신경망), 관계형 신경망 네트워크(RL, Relation Networks), 심층 신경망(Deep Neural Network, DNN), 딥러닝 네트워크 등 종래에 이미 공지되었거나 향후 개발되는 다양한 인공지능 기반의 모델을 폭넓게 포함할 수 있다. 이하에서는 도 2 내지 도 4를 참조하여 가속기 설계 장치의 구체적인 기능 및 동작에 대하여 설명하도록 한다. 도 2는 본원의 일 실시예에 따른 인공지능 모델에 대한 가속기 설계 장치의 구조 및 동작 프로세스를 설명하기 위한 개념도이다. 도 2를 참조하면, 가속기 설계 장치는 사용자가 설계하고자 하는 타겟 네트워크(예를 들면, 합성곱 신경망 (CNN) 등)에 대한 제약 조건과 타겟 네트워크를 실제로 실행하는 컴퓨팅 장치에 탑재되는 연산 유닛(예를 들면, FPGA 등)의 성능 간의 적절한 균형을 이루는 최적의 타겟 네트워크 가속기 설계를 위한 공간 탐색 모델 (space exploration model)을 제공할 수 있다. 보다 구체적으로 도 2를 참조하면, 가속기 설계 장치는 타겟 네트워크에 대한 네트워크 분석기(Network Analyzer; 110)와 ARM C/C++, HDL 등의 소프트웨어를 이용한 타겟 네트워크의 구조(하드웨어 디자인)를 최적화 하는 자동 생성기(Auto Generator; 120)를 이용하여 타겟 네트워크의 연산을 가속화 할 수 있는 하드웨어 가속 기를 설계할 수 있다. 달리 말해, 본원에서 개시하는 가속기 설계 장치를 이용하여 타겟 네트워크를 구축하고자 하는 사용자는 가속기 설계 장치가 제공하는 딥러닝 프레임워크를 사용하여 타겟 네트워크를 설계하고, 타겟 네트워크에 대한 학습/추론을 컴퓨팅 장치 등을 이용하여 수행할 수 있다. 이를 위하여, 가속기 설계 장치는 사용자가 설계한 타겟 네트워크에 맞게 훈련(생성)된 네트워크 정의 파 일을 네트워크 분석기(Network Analyzer)로 전달하고, 네트워크 분석기(Network Analyzer)에서는 사용자 (개발자)가 설계한 타겟 네트워크에 포함된 레이어를 분해할 수 있다. 예시적으로, 타겟 네트워크로부터 분해된 레이어는 타겟 네트워크가 합성곱 신경망(CNN)인 경우를 예로 들면, 합성곱 레이어(Convolution Layer), 풀링 레이어(Pooling Layer), 완전 연결 레이어(Fully-connected(FC) Layer) 등과 같은 다양한 계층으로 구성될 수 있다. 또한, 네트워크 분석기(Network Analyzer)에 의해 분해된 레이어는 미리 구축된 템플릿(예를 들면, SDSOC(Software defined system-on-chip) 기반의 템플릿 등)에 맞게 매핑되고, 자동 생성기(Auto Generator)는 SDSOC의 추론 데이터, C/C++ 코드 등을 포함하는 코드 정보로부터 특정 값(파라미터)을 추출하여, 추출된 특정 값(파라미터)을 후술하는 최적화 모델(예를 들면, LSTM 모델 등)을 통해 훈련함으로써 타겟 네트워크에 대하여 최적화 된 가속기를 설계할 수 있다. 이에 따라, 본원에서 개시하는 가속기 설계 장치에 의해, CNN 등의 타겟 네트워크가 높은 수준의 병렬 처 리를 통한 추론이 가능하도록 자동적으로 최적화(설계)될 수 있다. 특히, 가속기 설계 장치에 의해 최적화 (설계)된 타겟 네트워크는 사용자(개발자)의 정확도 요구 사항과 속도 요구 사항이 균형있게 달성되도록 컴퓨팅 장치에 탑재되는 각 연산 유닛에 대응하도록 개별 컴파일될 수 있다. 예시적으로 도2를 참조하면, 가속 설계 장치는 CPU에 해당하는 제1연산 유닛과 FPGA에 해당하는 제2연 산 유닛 각각에서 컴파일 될 수 있도록 타겟 네트워크를 최적화 할 수 있다. 예를 들어 가속 설계 장치는 ARM Compiler를 이용하여, 타겟 네트워크의 CPU를 이용한 실행을 위한 BIN 파일을 생성하고, HDL Compiler를 이용하여, 타겟 네트워크의 FPGA를 이용한 실행을 위한 비트 스트림(FPGA Bit-stream)을 생성할 수 있으나, 이에만 한정되는 것은 아니다. 종합하면, 가속기 설계 장치는 사용자가 설계한 타겟 네트워크에 대응하도록 생성된 네트워크 정의 파일을 획득할 수 있다. 또한, 가속기 설계 장치는 획득한 네트워크 정의 파일을 기초로 하여 타겟 네트워크를 이 루는 복수의 레이어를 분해할 수 있다. 또한, 가속기 설계 장치는 분해된 복수의 레이어 각각을 미리 설정된 템플릿에 매핑하고, 미리 구축된 인 공지능 기반의 최적화 모델을 이용하여 타겟 네트워크를 최적화할 수 있다. 구체적으로 가속기 설계 장치는 타겟 네트워크와 관련된 코드 정보로부터 소정의 파라미터를 추출할 수 있 다. 이와 관련하여 본원의 일 실시예에 따르면, 가속기 설계 장치는 타겟 네트워크와 연계된 연산자, 변수, 병렬 구조 및 메모리 할당 사이즈 중 적어도 하나를 포함하는 파라미터를 코드 정보로부터 추출할 수 있 다. 이와 관련하여 가속기 설계에서의 처리량을 개선하는데 연산 계산과 데이터 통신이 중요하며, 딥러닝 네트워크 등의 형태로 구축되는 타겟 네트워크는 다양한 연산자를 포함하도록 구축될 수 있으며, 각 연산자는 모두 상이 한 속성을 가질 수 있다. 이에 따라, 유사한 연산을 수행하는 경우라 할지라도 컴퓨팅 장치의 메모리가 상 대적으로 많이 사용되거나 적게 사용될 수 있고, 각 연산자는 매개변수로 인해 속성을 변경하여 다양하게 계산 할 수 있다. 특히, 타겟 네트워크에 대하여 적용될 수 있는 연산자들은 계산에 집중적인 연산자와 메모리 액세스에 집중적인 연산자로 분류될 수 있는데, 계산에 집중적인 연산자는 대표적으로 합성곱(Convolution) 연산, 완전 연결 (Fully-connected, FC) 연산 등이 포함될 수 있다. 이와 관련하여, 합성곱 신경망 등의 타겟 네트워크를 이용한 추론 시 필요한 구현 시간은 계산에 집중적인 연산자와의 계산과 선형 관계를 가진다. 또한, 메모리 액세스에 집중적인 연산자는 ReLU 연산, 순차(Concatenation) 연산 등이 포함되며, 이러한 연산자 의 경우 타겟 네트워크를 이용한 추론 시간이 메모리 액세스와 선형 관계를 가지게 된다. 도 3은 루프 최적화를 위한 코드 구조화 방식을 설명하기 위한 개념도이다. 도 3을 참조하면, 가속기 설계 장치는 추상 구문 트리(Abstract Syntax Tree)를 이용하여 코드 정보를 구 조화 할 수 있다. 예를 들어, 가속기 설계 장치는 코드 정보에 포함된 루프(for문)를 표현하는 루프 범위를 저장하고, 루프 레벨의 변수의 행렬을 메모리 할당 패턴으로 변환할 수 있다. 또한, 가속기 설계 장치는 각 루프 레벨에서 중복 루프 사용을 피하기 위해 버퍼 차원을 정의하는 태그를 해당 루프를 다른 루프와 구별하기 위하여 추가할 수 있다. 또한, 가속기 설계 장치는 하드웨어 디자인 도 출(하드웨어 합성) 전에, 추출된 루프를 최적화하여 태그에 추가할 수 있다. 예시적으로 가속기 설계 장치(10 0)는 자동 합성곱 신경망 모델 가속기(CNN 생성기)를 설계하기 위한 루프 최적화 기법으로써 Interchange, Tile, Unrolling 등을 선택하여 적용할 수 있고, 예시적으로 SDSOC pragma를 이용하여 루프를 결정하는 특정 값 을 추출할 수 있다. 즉, 본원의 일 실시예에 따르면, 가속기 설계 장치는 인터체인지(Interchange) 기법, 타일(Tile) 기법 및 펼치기(Unrolling) 기법 중 적어도 하나를 포함하는 최적화 기법을 타겟 네트워크에 대하여 적용하는 하드웨어 디자인을 도출할 수 있다. 또한, 가속기 설계 장치는 추출된 파라미터를 이용하여 최적화 모델을 훈련시킬 수 있다. 도 4는 장단기 메모리(Long Short Term Memory, LSTM) 기반의 최적화 모델의 구조를 예시적으로 나타낸 도면이 다. 도 4를 참조하면, 본원의 실시예에 관한 설명에서 최적화 모델은 장단기 메모리(Long Short Term Memory, LSTM) 기반의 인공지능 모델을 포함할 수 있다. 한편, 전술한 바와 같이 이렇듯, 타겟 네트워크는 두 가지 종류의 연산자(계산 집중적 연산자 및 메모리 접근 집중적 연산자)를 모두 포함하여 구성되는 것이 일반적이나, 두 가지 속성의 연산자를 사용하여 하드웨어 성능을 최적화 하도록 타겟 네트워크를 설계하는 것은 어려운 작업에 해당하므로, 본원에서 개시하는 가속기 설계 장치는 사용자(개발자)의 성능 요구 사항에 부합하도록 자동으로 타겟 네트워크를 최적화 하기 위하여 후 술하는 바와 같이 장단기 메모리(LSTM) 기반의 최적화 모델을 적용할 수 있다. 이를 위하여, 장단기 메모리(LSTM) 기반 최적화 모델 설계를 위하여 타겟 네트워크와 연계된 코드 정보(예를 들 면, C/C++ 코드 등)로부터 특정 값(파라미터)을 추출해야 하고, 가속기 설계 장치는 추출한 특정 값(파라 미터)을 사용하여 장단기 메모리(LSTM) 기반 최적화 모델을 트레이닝한 후, 트레이닝된 장단기 메모리(LSTM) 기 반 최적화 모델로 SDSOC와 연계된 하드웨어 디자인을 탐색할 수 있다. 또한, 가속기 설계 장치는 훈련된 최적화 모델을 이용하여 타겟 네트워크의 구현을 위한 하드웨어 디자인 을 도출할 수 있다. 예를 들면, 가속기 설계 장치는 특정 값 추출을 통해 하드웨어 디자인을 합성하기 위 해 LSTM을 통한 훈련을 수행할 수 있다. 한편, 장단기 메모리(LSTM) 기반의 최적화 모델의 세부적인 구조는 타겟 네트워크(예를 들면, CNN 등)의 깊이, 레이어 크기, 레이어 간의 연결 활성화 함수 등을 고려해야 하며, 이를 매개 변수로 표현하여 하드웨어 디자인 (설계)을 최적화 할 수 있는데, 이 때 적용되는 매개변수의 개수에 따라 타겟 네트워크의 성능이 개선되거나 과 적합(overfitting) 될 수도 있다. 따라서, 가속기 설계 장치는 최적 성능을 갖는 장단기 메모리(LSTM) 기반의 최적화 모델을 구축하기 위해 회귀 LSTM 기반의 훈련을 적용할 수 있다. 회귀 LSTM은 속도를 향상하여 타겟 네트워크에 대한 추론을 수행할 때, 회귀 문제로 모델링을 수행하는 특징을 가진다. 또한, 본원의 일 실시예에 따르면, 가속기 설계 장치는 최적화 모델에 의해 도출된 출력값으로 타겟 네트 워크와 연계된 기존 코드를 변환하고, 하드웨어 디자인 도출 이전의 프로그램과 비교한 예상 실행 속도 정보를 사용자에게 추가로 제공하도록 동작할 수 있다. 도 5는 본원의 일 실시예에 따른 인공지능 모델에 대한 가속기 설계 장치의 개략적인 구성도이다. 도 5를 참조하면, 가속기 설계 장치는 네트워크 분석부 및 자동 생성부를 포함할 수 있다. 네트워크 분석부는 사용자가 설계한 타겟 네트워크에 대응하도록 생성된 네트워크 정의 파일을 획득할 수 있다. 또한, 네트워크 분석부는 획득한 네트워크 정의 파일을 기초로 하여 타겟 네트워크를 이루는 복수의 레이어를 분해할 수 있다. 자동 생성부는 분해된 복수의 레이어 각각을 미리 설정된 템플릿에 매핑하고, 미리 구축된 인공지능 기반 의 최적화 모델을 이용하여 타겟 네트워크를 최적화할 수 있다. 구체적으로 자동 생성부는 타겟 네트워크와 관련된 코드 정보로부터 소정의 파라미터를 추출할 수 있다. 이와 관련하여 본원의 일 실시예에 따르면, 자동 생성부는 타겟 네트워크와 연계된 연산자, 변수, 병렬 구 조 및 메모리 할당 사이즈 중 적어도 하나를 포함하는 파라미터를 코드 정보로부터 추출할 수 있다. 또한, 본원의 일 실시예에 따르면, 자동 생성부는 추상 구문 트리(Abstract Syntax Tree)를 이용하여 코드 정보를 구조화 할 수 있다. 또한, 자동 생성부는 추출된 파라미터를 이용하여 최적화 모델을 훈련시킬 수 있다. 또한, 자동 생성부는 훈련된 최적화 모델을 이용하여 타겟 네트워크의 구현을 위한 하드웨어 디자인을 도 출할 수 있다. 이와 관련하여 본원의 일 실시예에 따르면, 자동 생성부는 인터체인지(Interchange) 기법, 타일(Tile) 기 법 및 펼치기(Unrolling) 기법 중 적어도 하나를 포함하는 최적화 기법을 타겟 네트워크에 대하여 적용하는 하 드웨어 디자인을 도출할 수 있다. 이하에서는 상기에 자세히 설명된 내용을 기반으로, 본원의 동작 흐름을 간단히 살펴보기로 한다. 도 6은 본원의 일 실시예에 따른 인공지능 모델에 대한 가속기 설계 방법에 대한 동작 흐름도이다. 도 6에 도시된 인공지능 모델에 대한 가속기 설계 방법은 앞서 설명된 가속기 설계 장치에 의하여 수행될 수 있다. 따라서, 이하 생략된 내용이라고 하더라도 가속기 설계 장치에 대하여 설명된 내용은 인공지능 모델에 대한 가속기 설계 방법에 대한 설명에도 동일하게 적용될 수 있다.도 6을 참조하면, 단계 S11에서 네트워크 분석부는 사용자가 설계한 타겟 네트워크에 대응하도록 생성된 네트워크 정의 파일을 획득할 수 있다. 다음으로, 단계 S12에서 네트워크 분석부는 획득한 네트워크 정의 파일을 기초로 하여 타겟 네트워크를 이 루는 복수의 레이어를 분해할 수 있다. 다음으로, 단계 S13에서 자동 생성부는 분해된 복수의 레이어 각각을 미리 설정된 템플릿에 매핑하고, 미 리 구축된 인공지능 기반의 최적화 모델을 이용하여 타겟 네트워크를 최적화할 수 있다. 상술한 설명에서, 단계 S11 내지 S13은 본원의 구현예에 따라서, 추가적인 단계들로 더 분할되거나, 더 적은 단 계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있다. 도 7은 타겟 네트워크 최적화 프로세스에 대한 세부 동작 흐름도이다. 도 7에 도시된 타겟 네트워크 최적화 프로세스는 앞서 설명된 가속기 설계 장치에 의하여 수행될 수 있다. 따라서, 이하 생략된 내용이라고 하더라도 가속기 설계 장치에 대하여 설명된 내용은 도 7에 대한 설명에 도 동일하게 적용될 수 있다. 도 7을 참조하면, 단계 S131에서 자동 생성부는 타겟 네트워크와 관련된 코드 정보로부터 소정의 파라미터 를 추출할 수 있다. 구체적으로 단계 S131에서 자동 생성부는 타겟 네트워크와 연계된 연산자, 변수, 병렬 구조 및 메모리 할당 사이즈 중 적어도 하나를 포함하는 파라미터를 코드 정보로부터 추출할 수 있다. 또한, 본원의 일 실시예에 따르면, 단계 S131에서 자동 생성부는 추상 구문 트리(Abstract Syntax Tree)를 이용하여 코드 정보를 구조화 할 수 있다. 다음으로, 단계 S132에서 자동 생성부는 추출된 파라미터를 이용하여 최적화 모델을 훈련시킬 수 있다. 다음으로, 단계 S133에서 자동 생성부는 훈련된 최적화 모델을 이용하여 타겟 네트워크의 구현을 위한 하 드웨어 디자인을 도출할 수 있다. 본원의 일 실시예에 따르면, 단계 S133에서 자동 생성부는 인터체인지(Interchange) 기법, 타일(Tile) 기 법 및 펼치기(Unrolling) 기법 중 적어도 하나를 포함하는 최적화 기법을 타겟 네트워크에 대하여 적용하는 하 드웨어 디자인을 도출할 수 있다. 상술한 설명에서, 단계 S131 내지 S133은 본원의 구현예에 따라서, 추가적인 단계들로 더 분할되거나, 더 적은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있 다. 본원의 일 실시예에 따른 인공지능 모델에 대한 가속기 설계 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있 는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프 로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이 프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크 (floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같 은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴 파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행 될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 또한, 전술한 ~방법은 기록 매체에 저장되는 컴퓨터에 의해 실행되는 컴퓨터 프로그램 또는 애플리케이션의 형 태로도 구현될 수 있다. 지금까지 상술한 본원의 일 실시예에 따른 인공지능 기반의 연산 시스템에 대한 설명은, 본원의 구현예에 따라서, 하기에서 서술하는 본원의 다른 실시예에 따른 딥러닝 FPGA 자동 생성기 설계 장치에 대한 설명을 통해 서 이해될 수 있다. 따라서, 이하, 생략된 내용이라고 하더라도 상술한 본원의 일 실시예에 따른 인공지능 기반 의 연산 시스템에 대하여 설명된 내용은 하기의 본원의 다른 실시예에 따른 딥러닝 FPGA 자동 생성기 설계장치에 대한 설명에도 동일하게 적용될 수 있다. 본원에서 개시하는 딥러닝 FPGA 자동 생성기 설계 장치(미도시)(이하, '설계 장치(미도시)'라 함.)는 사용자가 설계한 타겟 네트워크에 부합하는 네트워크 정의 파일을 생성할 수 있다. 또한, 설계 장치(미도시)는 생성된 네트워크 정의 파일을 기초로 하여 상기 타겟 네트워크에서 레이어를 분해할 수 있다. 또한, 설계 장치(미도시)는 분해된 레이어를 미리 설정된 템플릿에 매핑하고, LSTM 기반의 최적화 모델을 통해 사용자가 설계한 타겟 네트워크를 최적화할 수 있다. 보다 구체적으로, 설계 장치(미도시)는 CNN 네트워크 분석기와 ARM C/C++, HDL 등 소프트웨어를 이용한 CNN 구 조 자동 생성을 포함하는 절차를 통해 CNN 가속기를 생성할 수 있다. 이와 관련하여, CNN 모델을 구축하고자 하는 사용자는 딥러닝 프레임워크를 사용하여 타겟 네트워크를 설계하고 학습을 수행할 수 있으며, 설계 장치(미도시)는 사용자가 설계한 타겟 네트워크에 맞게 훈련된 CNN 네트워크 정 의 파일을 생성할 수 있다. 또한, 설계 장치(미도시)는 CNN 네트워크 정의 파일을 CNN 네트워크 분석기로 전달하고, 네트워크 분석기에서는 개발자가 설계한 네트워크에서 레이어를 분해할 수 있다. 예시적으로 분해된 레이어는 Convolution Layer, Pooling Layer, FC Layer와 같이 다양한 레이어로 이루어질 수 있다. 또한, 설계 장치는 분해된 레이어를 SDSOC 템플릿에 맞게 매핑하고, SDSOC의 CNN 추론 C 또는 C++ 코드에 서 특정 값을 추출하여 특정 값을 LSTM으로 훈련하며, LSTM기반의 자동 CNN 모델 최적화 생성기를 설계할 수 있 다. 따라서, 본원에서 개시하는 설계 장치(미도시)는 사용자가 설계한 타겟 네트워크에 대응하여 CNN 추론을 높은 수준의 병렬 처리가 가능한 설계를 자동으로 생성할 수 있으며, 이러한 CNN 모델 최적화 생성기를 통해 자동으 로 생성된 CNN 모델은 사용자의 정확도와 속도 요구 사항이 균형 맞게 CPU와 FPGA 각각의 프로세서에서 컴파일 이 될 수 있도록 생성되게 된다. 또한, 본원에서 개시하는 설계 장치(미도시)는 개발자의 성능 요구에 맞게 자동으로 CNN 모델 최적화 설계 생성 을 하기 위해 LSTM을 기반으로 최적화 모델을 설계할 수 있으며, 이러한 LSTM 설계를 위해 C/C++ 코드에서 특정 값을 추출하고, 추출한 특정 값을 사용하여 LSTM을 트레이닝하고 트레이닝된 LSTM으로 SDSOC에서의 새로운 방법 을 예측할 수 있다. 설계 장치(미도시)가 SDSOC 코드에서 추출하는 특정 값은 연산자, 변수, 병렬 구조, 메모리 할당 사이즈 등을 포함할 수 있으며, 이를 추출하기 위해 설계 장치(미도시)는 추상 구문 트리(Abstract Syntax Tree)를 사용할 수 있다. 추상 구문 트리(Abstract Syntax Tree)는 특정 값을 정렬된 트리로 구조화 하여 표현할 수 있다. 먼저, 설계 장치(미도시)는 for문과 같은 루프를 표현하는데 루프의 범위를 저장하고, 그 후 루프 레벨의 변수 의 행렬을 메모리 할당 패턴으로 변환할 수 있다. 또한, 설계 장치(미도시)는 각 루프 레벨에서 중복 루프 사용을 피하기 위해 버퍼 차원을 정의하는 루프를 다른 루프와 구별하는 태그를 추가할 수 있으며, 추가로 하드웨어 합성 전, 루프를 최적화하여 태그에 추가할 수 있 다. 한편, 자동 CNN 생성기를 설계하기 위해 최적화 예시로 'Interchange', 'Tile', 'Unrolling' 등을 채택할 수 있고, SDSOC pragma에 이러한 루프를 결정하는 특정 값으로 추출할 수 있다."}
{"patent_id": "10-2023-0074606", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본원의 설명은 예시를 위한 것이며, 본원이 속하는 기술분야의 통상의 지식을 가진 자는 본원의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본원의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본원의 범위에 포함되는 것으로 해석되어야 한다."}
{"patent_id": "10-2023-0074606", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본원의 일 실시예에 따른 인공지능 모델에 대한 가속기 설계 장치를 포함하는 인공지능 기반의 연산 시 스템의 개략적인 구성도이다. 도 2는 본원의 일 실시예에 따른 인공지능 모델에 대한 가속기 설계 장치의 구조 및 동작 프로세스를 설명하기위한 개념도이다. 도 3은 루프 최적화를 위한 코드 구조화 방식을 설명하기 위한 개념도이다. 도 4는 장단기 메모리(Long Short Term Memory, LSTM) 기반의 최적화 모델의 구조를 예시적으로 나타낸 도면이 다. 도 5는 본원의 일 실시예에 따른 인공지능 모델에 대한 가속기 설계 장치의 개략적인 구성도이다. 도 6은 본원의 일 실시예에 따른 인공지능 모델에 대한 가속기 설계 방법에 대한 동작 흐름도이다. 도 7은 타겟 네트워크 최적화 프로세스에 대한 세부 동작 흐름도이다."}
