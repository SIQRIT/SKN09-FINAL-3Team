{"patent_id": "10-2024-7021349", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0115872", "출원번호": "10-2024-7021349", "발명의 명칭": "무선 네트워크에서의 인공 지능", "출원인": "후아웨이 테크놀러지 컴퍼니 리미티드", "발명자": "주, 융허"}}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 방법으로서,연합 학습에 참여하는 단말에 제1 구성 정보를 전송하는 단계- 상기 제1 구성 정보는 다음: 트레이닝 지속기간,시간-주파수 리소스, 또는 보고 순간 중 적어도 하나를 구성하기 위해 사용되고; 연합 학습에 참여하는 상이한단말들에 대해 동일한 트레이닝 지속기간, 동일한 시간-주파수 리소스, 또는 동일한 보고 순간이 구성됨 -; 및연합 학습에 참여하는 상기 단말들에 의해 보고되는 그래디언트들의 공중(over-the-air) 중첩을 통해 획득되는신호를 수신하는 단계- 상기 그래디언트들은 트레이닝 지속기간 내에 트레이닝이 완료된 AI 모델의 것인 그리고시간-주파수 리소스를 사용하여 보고 순간에 단말들에 의해 보고되는 그래디언트들임 -를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 추가로,상기 단말로부터 트레이닝 완료 표시를 수신하는 단계- 상기 트레이닝 완료 표시는 상기 AI 모델의 트레이닝이상기 트레이닝 지속기간 내에 완료될 때 상기 단말에 의해 제2 노드에 전송됨 -; 및상기 단말에 의해 전송되는 상기 트레이닝 완료 표시에 기초하여, 상기 트레이닝 지속기간 내에 상기 AI 모델의트레이닝을 완료한 단말들의 수량에 관한 통계를 수집하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 추가로,상기 AI 모델의 트레이닝을 완료한 단말들의 수량이 단말 수량 임계값 이상이면, 연합 학습에 참여하는 상기 단말들에 의해 보고되는 상기 그래디언트들에 기초하여 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 결정하는 단계; 또는 그렇지 않으면, 모델 트레이닝의 이전 라운드에서의 평균 그래디언트를 모델 트레이닝의 현재 라운드에서의 평균 그래디언트로서 사용하는 단계; 및상기 모델 트레이닝의 현재 라운드에서의 평균 그래디언트에 기초하여 상기 AI 모델의 파라미터를업데이트하고, 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 상기 단말에 전송하는 단계를 포함하는방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서, 추가로,상기 트레이닝 지속기간 내에 상기 AI 모델의 트레이닝을 완료한 단말들의 수량 및 상기 단말들에 의해 보고되는 상기 그래디언트들의 공중 중첩을 통해 획득되는 신호를, 제1 노드에, 전송하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항 내지 제4항 중 어느 한 항에 있어서, 상기 제1 구성 정보는 다음:전용 베어러 RB 리소스, 변조 스킴, 초기 AI 모델, 또는 송신 전력 중 적어도 하나를 구성하기 위해 추가로 사용되는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 송신 전력을 결정하는 프로세스는,단말의 업링크 채널 품질을 결정하기 위해, 단말로부터의 사운딩 참조 신호 SRS를 측정하는 단계; 및상기 업링크 채널 품질에 기초하여 상기 단말의 송신 전력을 결정하는 단계를 포함하는 방법.공개특허 10-2024-0115872-3-청구항 7 제1항 내지 제4항 중 어느 한 항에 있어서, 상기 제1 구성 정보는 다음:전용 베어러 RB 리소스, 변조 스킴, 초기 AI 모델, 채널 상태 정보 CSI 간격, 또는 채널 반전 파라미터 중 적어도 하나를 구성하기 위해 추가로 사용되는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항 내지 제7항 중 어느 한 항에 있어서, 추가로,상기 제1 노드로부터 제2 구성 정보를 수신하는 단계- 상기 제2 구성 정보는 다음: 연합 학습에 참여하는 단말들의 리스트, 상기 초기 AI 모델, 그룹 임시 식별자, 상기 트레이닝 지속기간, 상기 단말 수량 임계값, 수송 블록의 크기, 또는 업링크 요건 중 적어도 하나를 구성하기 위해 사용됨 -를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제8항 중 어느 한 항에 있어서, 추가로,상기 단말로부터 제1 단말 정보를 수신하고, 상기 제1 노드에 제2 단말 정보를 전송하는 단계를 포함하고,상기 제1 단말 정보는 다음: 상기 단말의 통신 능력, 상기 단말의 컴퓨팅 능력, 또는 상기 단말의 데이터 세트특징 중 적어도 하나를 포함하고; 상기 제2 단말 정보는 다음: 상기 단말의 통신 능력, 상기 단말의 컴퓨팅 능력, 상기 단말의 데이터 세트 특징, 또는 단말 임시 식별자 중 적어도 하나를 포함하고, 상기 단말 임시 식별자는 상기 제2 노드에 의해 상기 단말에 할당되는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제9항 중 어느 한 항에 있어서, 추가로,모델 트레이닝 종료 조건이 충족될 때, 상기 단말에 모델 트레이닝 종료 표시를 전송하는 단계; 또는상기 제1 노드로부터 모델 트레이닝 종료 표시를 수신하고, 상기 모델 트레이닝 종료 표시를 상기 단말에 전달하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 방법으로서,제2 노드로부터 제1 구성 정보를 수신하는 단계- 상기 제1 구성 정보는 다음: 트레이닝 지속기간, 시간-주파수리소스 및 보고 순간 중 적어도 하나를 구성하기 위해 사용되고; 연합 학습에 참여하는 상이한 단말들에 대해동일한 트레이닝 지속기간, 동일한 시간-주파수 리소스, 및 동일한 보고 순간이 구성됨 -;모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 획득하기 위해, 상기 트레이닝 지속기간 내에 AI모델을 트레이닝하는 단계; 및상기 시간-주파수 리소스를 사용하여 상기 보고 순간에 상기 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 상기 제2 노드에 보고하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 추가로,상기 트레이닝 지속기간이 종료될 때, 상기 AI 모델의 트레이닝이 완료되면, 상기 제2 노드에 트레이닝 완료 표시를 전송하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항 또는 제12항에 있어서, 추가로,상기 AI 모델의 트레이닝이 상기 트레이닝 지속기간 내에 완료되지 않으면, 상기 AI 모델의 트레이닝을 종료하는 단계를 포함하는 방법.공개특허 10-2024-0115872-4-청구항 14 제11항 내지 제13항 중 어느 한 항에 있어서, 추가로,상기 제2 노드로부터 모델 트레이닝의 이전 라운드에서의 평균 그래디언트를 수신하는 단계; 및상기 모델 트레이닝의 이전 라운드에서의 평균 그래디언트에 기초하여 상기 모델 트레이닝의 현재 라운드에서의AI 모델의 그래디언트를 업데이트하는 단계; 또는상기 모델 트레이닝의 현재 라운드에서의 평균 그래디언트에 기초하여 상기 모델 트레이닝의 현재 라운드에서의AI 모델의 파라미터 및 그래디언트를 업데이트하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항 내지 제14항 중 어느 한 항에 있어서, 상기 제1 구성 정보는 다음:전용 베어러 RB 리소스, 변조 스킴, 초기 AI 모델, 또는 송신 전력 중 적어도 하나를 구성하기 위해 추가로 사용되는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항 내지 제14항 중 어느 한 항에 있어서, 상기 제1 구성 정보는 다음:전용 베어러 RB 리소스, 변조 스킴, 초기 AI 모델, 채널 상태 정보 CSI 간격, 또는 채널 반전 파라미터 중 적어도 하나를 구성하기 위해 추가로 사용되는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서, 상기 제1 구성 정보가 상기 채널 상태 정보 CSI 간격 및 상기 채널 반전 파라미터를 구성하기위해 추가로 사용될 때, 상기 방법은 추가로,동일한 주파수 리소스가 다운링크 채널 및 업링크 채널에 대해 구성되면, 상기 다운링크 채널의 측정된 CSI에기초하여 상기 단말의 업링크 채널의 CSI를 결정하는 단계; 및상기 업링크 채널의 CSI가 상기 CSI 간격의 요건을 충족하면, 상기 채널 반전 파라미터에 기초하여 상기 송신전력을 결정하는 단계를 포함하고;상기 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 상기 제2 노드에 보고하는 단계는,상기 결정된 송신 전력에 기초하여 상기 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 상기 제2노드에 보고하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항 내지 제17항 중 어느 한 항에 있어서, 상기 제1 구성 정보는 그룹 임시 식별자를 추가로 포함하고, 상기그룹 임시 식별자는 제1 노드에 의해 상기 단말에 할당되는 그룹 임시 식별자인 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서, 추가로,상기 제2 노드로부터 스케줄링 표시를 수신하는 단계- 상기 스케줄링 표시는 그룹 임시 식별자를 포함함 -; 및상기 스케줄링 표시에 포함되는 그룹 임시 식별자가 상기 제1 노드에 의해 상기 단말에 할당되는 그룹 임시 식별자와 동일할 때, 상기 모델 트레이닝의 현재 라운드에서의 AI 모델의 트레이닝을 수행하는 단계; 또는 그렇지않으면, 상기 모델 트레이닝의 현재 라운드에서의 AI 모델의 트레이닝을 수행하는 것을 생략하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제11항 내지 제19항 중 어느 한 항에 있어서, 추가로,상기 제2 노드로부터 모델 트레이닝 종료 표시를 수신하는 단계; 및공개특허 10-2024-0115872-5-상기 모델 트레이닝 종료 표시에 기초하여 상기 AI 모델의 트레이닝을 종료하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제11항 내지 제20항 중 어느 한 항에 있어서, 추가로,상기 제2 노드에 제1 단말 정보를 전송하는 단계- 상기 제1 단말 정보는 다음: 상기 단말의 통신 능력, 상기 단말의 컴퓨팅 능력, 또는 상기 단말의 데이터 세트 특징 중 적어도 하나를 포함함 -를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 방법으로서,제2 구성 정보를 결정하는 단계- 상기 제2 구성 정보는 다음: 연합 학습에 참여하는 단말들의 리스트, 초기 AI모델, 그룹 임시 식별자, 트레이닝 지속기간, 단말 수량 임계값, 수송 블록의 크기, 또는 업링크 요건 중 적어도 하나를 구성하기 위해 사용됨 -; 및상기 제2 구성 정보를 제2 노드에 전송하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제22항에 있어서, 추가로,상기 제2 노드로부터 제2 단말 정보를 수신하는 단계- 상기 제2 단말 정보는 다음: 단말의 통신 능력, 상기 단말의 컴퓨팅 능력, 상기 단말의 데이터 세트 특징, 또는 단말 임시 식별자 중 적어도 하나를 포함하고, 상기 단말 임시 식별자는 상기 제2 노드에 의해 상기 단말에 할당됨 -; 및상기 단말 정보에 기초하여, 연합 학습에 참여하는 상기 단말들의 리스트를 결정하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제22항 또는 제23항에 있어서, 추가로,상기 제2 노드로부터, 연합 학습에 참여하는 상기 단말들에 의해 보고되는 그래디언트들의 공중 중첩을 통해 획득되는 신호, 및 상기 트레이닝 지속기간 내에 AI 모델의 트레이닝을 완료한 단말들의 수량을 수신하는 단계;상기 트레이닝 지속기간 내에 상기 모델의 트레이닝을 완료한 단말들의 수량이 상기 단말 수량 임계값이상이면, 상기 단말들에 의해 보고되는 상기 AI 모델의 그래디언트들에 기초하여 상기 모델 트레이닝의 현재라운드에서의 평균 그래디언트를 결정하는 단계; 또는 그렇지 않으면, 모델 트레이닝의 이전 라운드에서의 평균그래디언트를 모델 트레이닝의 현재 라운드에서의 평균 그래디언트로서 사용하는 단계; 및상기 모델 트레이닝의 현재 라운드에서의 평균 그래디언트에 기초하여 상기 AI 모델의 파라미터를 업데이트하는단계, 및 상기 제2 노드가 상기 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 상기 단말에 전송하는것을 가능하게 하기 위해 상기 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 상기 제2 노드에 전송하는 단계를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제22항 내지 제24항 중 어느 한 항에 있어서, 추가로,상기 제2 노드에 스케줄링 표시를 전송하는 단계- 상기 스케줄링 표시는 그룹 임시 식별자를 포함하고, 상기 모델 트레이닝의 현재 라운드에서의 상기 AI 모델의 트레이닝을 수행하기 위해, 상기 그룹 임시 식별자에 대응하는 단말을 스케줄링하기 위해 상기 스케줄링 표시가 사용됨 -를 포함하는 방법."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제22항 내지 제25항 중 어느 한 항에 있어서, 추가로,모델 트레이닝 종료 조건이 충족될 때, 상기 모델 트레이닝의 현재 라운드에서의 상기 AI 모델의 트레이닝을 종료하라고 상기 단말에 표시하기 위해, 상기 제2 노드에 모델 트레이닝 종료 표시를 전송하는 단계를 포함하는방법.공개특허 10-2024-0115872-6-청구항 27 무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 장치로서, 제1항 내지 제10항 중 어느 한 항에 따른 방법을 구현하도록 구성되는 유닛을 포함하는 장치."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 장치로서, 프로세서 및 메모리를 포함하고, 상기프로세서는 제1항 내지 제10항 중 어느 한 항에 따른 방법을 구현하도록 구성되는 장치."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_29", "content": "무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 장치로서, 제11항 내지 제21항 중 어느 한 항에따른 방법을 구현하도록 구성되는 유닛을 포함하는 장치."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_30", "content": "무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 장치로서, 프로세서 및 메모리를 포함하고, 상기프로세서는 제11항 내지 제21항 중 어느 한 항에 따른 방법을 구현하도록 구성되는 장치."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_31", "content": "무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 장치로서, 제22항 내지 제26항 중 어느 한 항에따른 방법을 구현하도록 구성되는 유닛을 포함하는 장치."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_32", "content": "무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 장치로서, 프로세서 및 메모리를 포함하고, 상기프로세서는 제22항 내지 제26항 중 어느 한 항에 따른 방법을 구현하도록 구성되는 장치."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_33", "content": "시스템으로서, 제27항 또는 제28항에 따른 장치 및 제29항 또는 제30항에 따른 장치를 포함하는 시스템."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_34", "content": "제33항에 있어서, 제31항 또는 제32항에 따른 장치를 추가로 포함하는 시스템."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_35", "content": "컴퓨터-판독가능 저장 매체로서, 상기 컴퓨터-판독가능 저장 매체는 명령어들을 저장하고; 상기 명령어들이 컴퓨터 상에서 실행될 때, 상기 컴퓨터는 제1항 내지 제10항 중 어느 한 항에 따른 방법, 제11항 내지 제21항 중어느 한 항에 따른 방법, 또는 제22항 내지 제26항 중 어느 한 항에 따른 방법을 수행하는 것이 가능하게 되는컴퓨터-판독가능 저장 매체."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_36", "content": "컴퓨터 프로그램 제품으로서, 명령어들을 포함하고, 상기 명령어들이 컴퓨터 상에서 실행될 때, 상기 컴퓨터는제1항 내지 제10항 중 어느 한 항에 따른 방법, 제11항 내지 제21항 중 어느 한 항에 따른 방법, 또는 제22항내지 제26항 중 어느 한 항에 따른 방법을 수행하는 것이 가능하게 되는 컴퓨터 프로그램 제품."}
{"patent_id": "10-2024-7021349", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_37", "content": "칩 시스템으로서, 제1항 내지 제10항 중 어느 한 항에 따른 방법, 제11항 내지 제21항 중 어느 한 항에 따른 방법, 또는 제22항 내지 제26항 중 어느 한 항에 따른 방법을 수행하도록 구성되는 프로세서 또는 회로를 포함하는 칩 시스템."}
{"patent_id": "10-2024-7021349", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 방법 및 장치가 제공된다. 이러한 방법은, 연합 학습에 참여하는 단말에 제1 구성 정보를 전송하는 단계- 제1 구성 정보는 다음: 트레이닝 지속기간, 시간-주파 수 리소스 및 보고 순간 중 적어도 하나를 구성하기 위해 사용되고; 연합 학습에 참여하는 상이한 단말들에 대해 (뒷면에 계속)"}
{"patent_id": "10-2024-7021349", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "<관련 출원들에 대한 상호-참조> 본 출원은 2021년 12월 10일자로 중국 특허청에 출원되고 발명의 명칭이 \"METHOD FOR TRAINING ARTIFICIAL INTELLIGENCE AI MODEL IN WIRELESS NETWORK AND APPARATUS\"인 중국 특허 출원 제202111505116.7호에 대한 우 선권을 주장하며, 이는 그 전체가 본 명세서에 참조로 원용된다. <기술 분야> 본 출원의 실시예들은 인공 지능(artificial intelligence, AI) 분야에, 특히, 무선 네트워크에서의 AI 모델을 트레이닝하기 위한 방법 및 장치에 관련된다."}
{"patent_id": "10-2024-7021349", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "무선 통신 네트워크에서, 예를 들어, 이동 통신 네트워크에서, 네트워크에 의해 지원되는 서비스들은 점점 더 다양화되고, 따라서 충족될 필요가 있는 요건들은 점점 더 다양화된다. 예를 들어, 네트워크는 초고속, 초저 레이턴시 및/또는 초대형 접속을 지원하는 것이 가능할 필요가 있다. 이러한 특징으로 인해, 네트워크 계획, 네트워크 구성 및/또는 리소스 스케줄링이 점점 더 복잡해지고 있다. 이러한 새로운 요건들, 시나리오들, 및 특징들은 네트워크 계획, 동작 및 유지보수, 및 효율적인 동작에 전례없는 도전과제들을 가져온다. 이러한 도 전과제들을 충족하기 위해, 인공 지능 기술이 무선 통신 네트워크에 도입되어, 네트워크 지능을 구현할 수 있다. 연합 학습은 프라이버시를 손상시키지 않고 모델 트레이닝에 참여하기 위해 에지 디바이스 상에 위치되 는 분산 데이터가 호출될 수 있는 인기있는 모델 트레이닝 아키텍처이다. 연합 학습에서, 중앙 노드 및 에지 디바이스는 인공 지능(artificial intelligence, AI) 모델의 파라미터들 또는 그래디언트들을 교환할 필요가 있 다. 무선 네트워크에서의 연합 학습을 어떻게 배포할지는 연구할 가치가 있는 문제이다. 본 출원은, 연합 학습이 무선 네트워크에 배포될 때, 연합 학습에 참여하는 에지 노드들이 상호 직교 업링크 시 간-주파수 리소스들을 사용할 필요가 있기 때문에 야기되는 과도하게 높은 시간-주파수 리소스 사용 및 큰 지연 과 같은 문제들이 해결되도록, 무선 네트워크에서의 AI 모델을 트레이닝하기 위한 방법 및 장치를 제공한다. 제1 양태에 따르면, 무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 방법이 제공된다. 이러한 방 법은 제2 노드에 의해 실행되거나, 또는 제2 노드에서 구성되는 컴포넌트(프로세서, 칩 등)에 의해 실행될 수 있거나, 또는 소프트웨어 모듈에 의해 실행될 수 있다. 이러한 방법은, 연합 학습에 참여하는 단말에 제1 구성 정보를 전송하는 단계- 제1 구성 정보는 다음: 트레이닝 지속기간, 시간-주파수 리소스 및 보고 순간 중 적어도 하나를 구성하기 위해 사용되고; 연합 학습에 참여하는 상이한 단말들에 대해 동일한 트레이닝 지속기간, 동일 한 시간-주파수 리소스, 및 동일한 보고 순간이 구성됨 -; 및 연합 학습에 참여하는 단말들에 의해 보고되는 그 래디언트들의 공중(over-the-air) 중첩을 통해 획득되는 신호를 수신하는 단계- 그래디언트들은 트레이닝 지속 기간 내에 트레이닝이 완료된 AI 모델의 것인 그리고 시간-주파수 리소스를 사용하여 보고 순간에 단말들에 의 해 보고되는 그래디언트들임 -를 포함한다. 본 출원의 설명들에서, 제2 노드는 액세스 네트워크 디바이스라고 또한 지칭될 수 있고, 제1 노드는 중앙 노드 라고 또한 지칭될 수 있는 등이라는 점이 주목되어야 한다. 전술한 설계에서, 동일한 트레이닝 지속기간, 동일 한 시간-주파수 리소스, 및 동일한 보고 순간은 연합 학습에 참여하는 단말들에 대한 액세스 네트워크 디바이스 (제2 노드라고 지칭될 수 있음) 또는 중앙 노드(제1 노드라고 지칭될 수 있음)에 의해 구성된다. 예를 들어, 연합 학습에 참여하는 단말들의 수량은 n이다. 본 출원에서, 액세스 네트워크 디바이스 또는 중앙 노드는 n개 의 단말들에 동일한 시간-주파수 리소스를 할당한다. 연합 학습에 참여하는 n개의 단말들에 n개의 직교 시간- 주파수 리소스들이 할당되는 종래의 해결책과 비교하여, 시간-주파수 리소스들의 오버헤드들이 감소될 수 있다. 또한, n개의 직교 시간-주파수 리소스들이 연합 학습에 참여하는 n개의 단말들에 할당되면, 각각의 단말은 각각 의 시간-주파수 리소스를 사용하여 AI 모델의 그래디언트를 보고한다. 액세스 네트워크 디바이스는 n개의 무선 주파수 신호들을 수신하고, n개의 무선 주파수 신호들을 개별적으로 처리하여, 각각의 단말에 의해 보고되는 그 래디언트를 복원할 수 있다. 결과적으로, 지연이 크다. 그러나, 본 출원에서, 액세스 네트워크 디바이스는 연 합 학습에 참여하는 n개의 단말들에 하나의 시간-주파수 리소스를 할당하고, n개의 단말들은 시간-주파수 리소 스 상에서 그래디언트들을 보고한다. 따라서, 무선 채널의 중첩 속성에 기초하여, 공중 송신 동안 n개의 그래 디언트들이 함께 중첩된다. 예를 들어, n의 값은 3이고, 단말 1에 의해 보고되는 그래디언트는 Y1이고, 단말 2 에 의해 보고되는 그래디언트는 Y2이고, 단말 3에 의해 보고되는 그래디언트는 Y3이다. 이러한 경우, 전술한 3개의 그래디언트들은 동일한 시간-주파수 리소스 상에서 송신되고, 전술한 3개의 그래디언트들은 함께 중첩된다. 무선 채널은 완벽한 신호 중첩을 충족한다고 가정된다(여기서, 페이딩, 간섭, 잡음 등은 무시될 수 있다). 이러한 경우, 중첩된 신호는, Y=Y1+Y2+Y3이다. 채널이 완벽한 신호 중첩을 충족하지 않을 때, 전술한 시간-주파수 리소스 상에서 전술한 신호를 수신한 후에, 액세스 네트워크 디바이스는 수신된 신호에 대해 신호 처리를 수행하여, 중첩된 신호 Y를 복원할 수 있고, 단말들은 중첩된 신호 Y를 사용하여 그래디언트 집계를 수 행할 수 있다. 그래디언트 집계의 프로세스는 산술 평균을 계산하는 프로세스일 수 있다. 예를 들어, 중첩된 신호 Y는 3으로 나누어질 수 있고, 결과는 집계 그래디언트로서 사용된다. 본 출원의 해결책에 따르면, 중첩 신호 Y는 하나의 무선 주파수 신호를 처리하는 것에 의해 획득될 수 있다. 그러나, 기존의 해결책에서는, 상이 한 슬롯들에서의 3개의 무선 주파수 신호들은 대응하는 그래디언트를 복원하기 위해 순차적으로 처리될 필요가 있고, 다음으로 집계가 추가로 수행된다. 본 출원의 해결책을 사용하여, 시간-주파수 리소스들의 사용이 어느 정도 감소될 수 있고, 그래디언트 집계의 지연이 감소될 수 있다. 단말에 대해 제2 노드(또는 액세스 네트워크 디바이스라고 지칭됨)에 의해 구성되는 트레이닝 지속기간은 단말 의 실제 트레이닝 지속기간이 아닐 수 있지만, 트레이닝의 각각의 라운드에 대해 단말에 의해 요구되는 시간의 상한이라는 점이 이해되어야 한다. 다시 말해서, 단말은 트레이닝 지속기간 내에 모델 트레이닝의 현재 라운드 를 완료하고, 트레이닝 완료 표시를 기지국에 보고한다. 그렇지 않으면, 단말은 모델 트레이닝의 현재 라운드 를 종료하고, 다음 트레이닝 지속기간이 도달하기를 기다릴 수 있다. 본 출원에서, 트레이닝 지속기간은 상이 한 단말들이 모델 트레이닝의 그래디언트들을 액세스 네트워크 디바이스에 동시에 보고할 수 있는 것을 보장하 도록 설정된다. 선택적으로, 모델 트레이닝의 것인 그리고 단말에 의해 액세스 네트워크 디바이스에 보고되는 그래디언트는 모델 트레이닝의 다른 라운드에서의 그래디언트, 예를 들어, 모델 트레이닝의 이전 라운드에서의 그래디언트 대신에, 모델 트레이닝의 현재 라운드에서의 그래디언트이다. 예를 들어, 트레이닝 지속기간은 연 합 학습에 참여하는 단말들의 컴퓨팅 능력들, 모델의 복잡도 등을 포괄적으로 고려하는 것에 의해 결정되는 글 로벌 파라미터일 수 있다. 가능한 설계에서, 이러한 방법은 추가로, 단말로부터 트레이닝 완료 표시를 수신하는 단계- 트레이닝 완료 표시 는 AI 모델의 트레이닝이 트레이닝 지속기간 내에 완료될 때 단말에 의해 제2 노드에 전송됨 -; 및 단말에 의해 전송되는 트레이닝 완료 표시에 기초하여, 트레이닝 지속기간 내에 AI 모델의 트레이닝을 완료한 단말들의 수량 에 관한 통계를 수집하는 단계를 포함한다. 가능한 설계에서, 이러한 방법은 추가로, AI 모델의 트레이닝을 완료한 단말들의 수량이 단말 수량 임계값 이상 이면, 연합 학습에 참여하는 상이한 단말들에 의해 보고되는 그래디언트들에 기초하여 모델 트레이닝의 현재 라 운드에서의 평균 그래디언트를 결정하는 단계; 또는 그렇지 않으면, 모델 트레이닝의 이전 라운드에서의 평균 그래디언트를 모델 트레이닝의 현재 라운드에서의 평균 그래디언트로서 사용하는 단계; 및 모델 트레이닝의 현 재 라운드에서의 평균 그래디언트에 기초하여 AI 모델의 파라미터를 업데이트하고, 모델 트레이닝의 현재 라운 드에서의 평균 그래디언트를 단말에 전송하는 단계를 포함한다. 전술한 설계에 따르면, 제1 노드는 단말 수량 임계값을 결정할 수 있다. 공중 계산이 연합 학습에 도입될 때, 연합 학습에 참여하는 단말들의 수량은 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 계산하는 정확도 에 영향을 미친다. 제1 노드는 단말 수량 임계값을 설정할 수 있다. 그래디언트들을 보고하는 단말들의 수량 이 단말 수량 임계값 이상일 때, 모델 트레이닝의 현재 라운드에서의 평균 그래디언트가 계산되어 단말에 전송 되거나; 또는 그렇지 않으면, 모델 트레이닝의 이전 라운드에서의 평균 그래디언트가 단말에 전송되거나, 또는 모델 트레이닝의 이전 라운드에서의 평균 그래디언트가 모델 트레이닝의 현재 라운드에서의 평균 그래디언트로 서 사용되고 단말에 전송되어, 모델 트레이닝의 현재 라운드에서의 계산된 평균 그래디언트의 정확도가 요건을 충족하는 것을 보장한다. 가능한 설계에서, 이러한 방법은 추가로, 트레이닝 지속기간 내에 AI 모델의 트레이닝을 완료한 단말들의 수량 및 단말들에 의해 보고되는 그래디언트들의 공중 중첩을 통해 획득되는 신호를, 제1 노드에, 전송하는 단계를 포함한다. 가능한 설계에서, 제1 구성 정보는 다음: 전용 베어러 RB 리소스, 변조 스킴, 초기 AI 모델, 또는 송신 전력 중 적어도 하나를 구성하기 위해 추가로 사용된다. 가능한 설계에서, 송신 전력을 결정하는 프로세스는, 단말의 업링크 채널 품질을 결정하기 위해, 단말로부터의 사운딩 참조 신호 SRS를 측정하는 단계; 및 업링크 채널 품질에 기초하여 단말의 송신 전력을 결정하는 단계를 포함한다.전술한 설계에 따르면, 제2 노드는 업링크 채널을 측정하는 것에 의해 최적의 송신 전력을 결정하고, 단말이 모 델 트레이닝의 현재 라운드에서의 그래디언트를 전송하도록 송신 전력을 구성하고, 그렇게 함으로써 공중 계산 의 정밀도를 개선하고, 그래디언트 집계의 정확도를 추가로 개선한다. 가능한 설계에서, 제1 구성 정보는 다음: 전용 베어러 RB 리소스, 변조 스킴, 초기 AI 모델, 채널 상태 정보 CSI 간격, 또는 채널 반전 파라미터 중 적어도 하나를 구성하기 위해 추가로 사용된다. 가능한 설계에서, 이러한 방법은 추가로, 제1 노드로부터 제2 구성 정보를 수신하는 단계- 제2 구성 정보는 다 음: 연합 학습에 참여하는 단말들의 리스트, 초기 AI 모델, 그룹 임시 식별자, 트레이닝 지속기간, 단말 수량 임계값, 수송 블록의 크기, 또는 업링크 요건 중 적어도 하나를 구성하기 위해 사용됨 -를 포함한다. 가능한 설계에서, 이러한 방법은 추가로, 단말로부터 제1 단말 정보를 수신하고, 제1 노드에 제2 단말 정보를 전송하는 단계를 포함하고, 제1 단말 정보는 다음: 단말의 통신 능력, 단말의 컴퓨팅 능력, 또는 단말의 데이터 세트 특징 중 적어도 하나를 포함하고; 제2 단말 정보는 다음: 단말의 통신 능력, 단말의 컴퓨팅 능력, 단말의 데이터 세트 특징, 또는 단말 임시 식별자 중 적어도 하나를 포함하고, 단말 임시 식별자는 제2 노드에 의해 단 말에 할당된다. 이러한 설계에서, 단말의 통신 능력은, 예를 들어, 단말에 의해 지원될 수 있는 최대 송신 전력, 단말의 안테나 구성 등을 포함한다. 단말의 컴퓨팅 능력은, 예를 들어, 중앙 처리 유닛(central processing unit, CPU)의 성 능, 그래픽 처리 유닛(graphics processing unit, GPU)의 성능, 저장 공간, 및 전기량을 포함한다. 단말의 데 이터 세트 특징은, 예를 들어, 데이터 세트의 크기, 데이터 세트의 분포, 데이터 세트가 완료되었는지, 및 데이 터 세트의 레이블이 완료되었는지를 포함한다. 선택적으로, 데이터 세트는 백분율에 기초하여 트레이닝 세트, 검증 세트, 및 테스트 세트로 추가로 분할될 수 있다. 예를 들어, 데이터 세트의 60%는 트레이닝 세트이고, 데 이터 세트의 20%는 검증 세트이고, 데이터 세트의 20%는 테스트 세트이다. 트레이닝 세트는 AI 모델을 트레이 닝하기 위해 사용되고, 검증 세트는 트레이닝된 AI 모델을 평가하기 위해 사용되고, 테스트 세트는 트레이닝된 AI 모델을 테스트하기 위해 사용된다는 점이 이해될 수 있다. 단말 임시 식별자: 이러한 식별자는 셀 무선 네 트워크 임시 식별자(cell radio network temporary identifier, C-RNTI), 다른 임시 식별자 등일 수 있다. 가능한 설계에서, 이러한 방법은 추가로, 모델 트레이닝 종료 조건이 충족될 때, 단말에 모델 트레이닝 종료 표 시를 전송하는 단계; 또는 제1 노드로부터 모델 트레이닝 종료 표시를 수신하고, 모델 트레이닝 종료 표시를 단 말에 전달하는 단계를 포함한다. 제2 양태에 따르면, 무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 방법이 제공된다. 이러한 방 법은 제1 양태에 대응한다. 유익한 효과들에 대해서는, 제1 양태의 설명들을 참조한다. 이러한 방법은 단말에 의해 실행되거나, 또는 단말에서 구성되는 컴포넌트(프로세서, 칩, 또는 다른 컴포넌트)에 의해 실행될 수 있거 나, 또는 소프트웨어 모듈 등에 의해 실행될 수 있다. 이러한 방법은 제2 노드로부터 제1 구성 정보를 수신하 는 단계- 제1 구성 정보는 다음: 트레이닝 지속기간, 시간-주파수 리소스 및 보고 순간 중 적어도 하나를 구성 하기 위해 사용되고; 연합 학습에 참여하는 상이한 단말들에 대해 동일한 트레이닝 지속기간, 동일한 시간-주파 수 리소스, 및 동일한 보고 순간이 구성됨 -; 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 획득 하기 위해, 트레이닝 지속기간 내에 AI 모델을 트레이닝하는 단계; 및 시간-주파수 리소스를 사용하여 보고 순 간에 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 제2 노드에 보고하는 단계를 포함한다. 가능한 설계에서, 이러한 방법은, 트레이닝 지속기간이 종료될 때, AI 모델의 트레이닝이 완료되면, 트레이닝 완료 표시를 제2 노드에 전송하는 단계를 추가로 포함한다. 가능한 설계에서, 이러한 방법은, AI 모델의 트레이닝이 트레이닝 지속기간 내에 완료되지 않으면, AI 모델의 트레이닝을 종료하는 단계를 추가로 포함한다. 가능한 설계에서, 이러한 방법은 추가로, 제2 노드로부터 모델 트레이닝의 이전 라운드에서의 평균 그래디언트 를 수신하는 단계; 및 모델 트레이닝의 이전 라운드에서의 평균 그래디언트에 기초하여 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 업데이트하는 단계; 또는 모델 트레이닝의 현재 라운드에서의 평균 그래 디언트에 기초하여 모델 트레이닝의 현재 라운드에서의 AI 모델의 파라미터 및 그래디언트를 업데이트하는 단계 를 포함한다. 가능한 설계에서, 제1 구성 정보는 다음: 전용 베어러 RB 리소스, 변조 스킴, 초기 AI 모델, 또는 송신 전력 중 적어도 하나를 구성하기 위해 추가로 사용된다.가능한 설계에서, 제1 구성 정보는 다음: 전용 베어러 RB 리소스, 변조 스킴, 초기 AI 모델, 채널 상태 정보 CSI 간격, 또는 채널 반전 파라미터 중 적어도 하나를 구성하기 위해 추가로 사용된다. 가능한 설계에서, 제1 구성 정보가 채널 상태 정보 CSI 간격 및 채널 반전 파라미터를 구성하기 위해 추가로 사 용될 때, 이러한 방법은 추가로, 동일한 주파수 리소스가 다운링크 채널 및 업링크 채널에 대해 구성되면, 다운 링크 채널의 측정된 CSI에 기초하여 단말의 업링크 채널의 CSI를 결정하는 단계; 업링크 채널의 CSI가 CSI 간격 의 요건을 충족하면, 채널 반전 파라미터에 기초하여 송신 전력을 결정하는 단계를 포함하고; 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 제2 노드에 보고하는 단계는, 결정된 송신 전력에 기초하여 모델 트 레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 제2 노드에 보고하는 단계를 포함한다. 가능한 설계에서, 제1 구성 정보는 그룹 임시 식별자를 추가로 포함하고, 그룹 임시 식별자는 제1 노드에 의해 단말에 할당되는 그룹 임시 식별자이다. 가능한 설계에서, 이러한 방법은 추가로, 제2 노드로부터 스케줄링 표시를 수신하는 단계- 스케줄링 표시는 그 룹 임시 식별자를 포함함 -; 및 스케줄링 표시에 포함되는 그룹 임시 식별자가 제1 노드에 의해 단말에 할당되 는 그룹 임시 식별자와 동일할 때, 모델 트레이닝의 현재 라운드에서의 AI 모델의 트레이닝을 수행하는 단계; 또는 그렇지 않으면, 모델 트레이닝의 현재 라운드에서의 AI 모델의 트레이닝을 수행하는 것을 생략하는 단계를 포함한다. 가능한 설계에서, 이러한 방법은 제2 노드로부터 모델 트레이닝 종료 표시를 수신하는 단계; 및 모델 트레이닝 종료 표시에 기초하여 AI 모델의 트레이닝을 종료하는 단계를 포함한다. 가능한 설계에서, 이러한 방법은 추가로, 제2 노드에 제1 단말 정보를 전송하는 단계- 제1 단말 정보는 다음: 단말의 통신 능력, 단말의 컴퓨팅 능력, 또는 단말의 데이터 세트 특징 중 적어도 하나를 포함함 -를 포함한다. 제3 양태에 따르면, 무선 네트워크에서의 인공 지능 AI 모델을 트레이닝하기 위한 방법이 제공된다. 이러한 방 법은 제1 양태에 대응한다. 유익한 효과들에 대해서는, 제1 양태의 설명들을 참조한다. 이러한 방법은 제1 노 드에 의해 실행되거나, 또는 제1 노드에서 구성되는 컴포넌트(프로세서, 칩, 또는 다른 컴포넌트)에 의해 실행 될 수 있거나, 또는 소프트웨어 모듈 등에 의해 실행될 수 있다. 이러한 방법은, 제2 구성 정보를 결정하는 단 계- 제2 구성 정보는 다음: 연합 학습에 참여하는 단말들의 리스트, 초기 AI 모델, 그룹 임시 식별자, 트레이닝 지속기간, 단말 수량 임계값, 수송 블록의 크기, 또는 업링크 요건 중 적어도 하나를 구성하기 위해 사용됨 -; 및 제2 구성 정보를 제2 노드에 전송하는 단계를 포함한다. 가능한 설계에서, 이러한 방법은 제2 노드로부터 제2 단말 정보를 수신하는 단계- 제2 단말 정보는 다음: 단말 의 통신 능력, 단말의 컴퓨팅 능력, 단말의 데이터 세트 특징, 또는 단말 임시 식별자 중 적어도 하나를 포함하 고, 단말 임시 식별자는 제2 노드에 의해 단말에 할당됨 -; 및 단말 정보에 기초하여, 연합 학습에 참여하는 단 말들의 리스트를 결정하는 단계를 포함한다. 가능한 설계에서, 이러한 방법은 추가로, 제2 노드로부터, 연합 학습에 참여하는 단말들에 의해 보고되는 그래 디언트들의 공중 중첩을 통해 획득되는 신호, 및 트레이닝 지속기간 내에 AI 모델의 트레이닝을 완료한 단말들 의 수량을 수신하는 단계; 트레이닝 지속기간 내에 모델의 트레이닝을 완료한 단말들의 수량이 단말 수량 임계 값 이상이면, AI 모델의 것인 그리고 단말들에 의해 보고되는 그래디언트들에 기초하여 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 결정하는 단계; 또는 그렇지 않으면, 모델 트레이닝의 이전 라운드에서의 평균 그래디언트를 모델 트레이닝의 현재 라운드에서의 평균 그래디언트로서 사용하는 단계; 및 모델 트레이닝의 현 재 라운드에서의 평균 그래디언트에 기초하여 AI 모델의 파라미터를 업데이트하는 단계, 및 제2 노드가 모델 트 레이닝의 현재 라운드에서의 평균 그래디언트를 단말에 전송하는 것을 가능하게 하기 위해 모델 트레이닝의 현 재 라운드에서의 평균 그래디언트를 제2 노드에 전송하는 단계를 포함한다. 가능한 설계에서, 이러한 방법은 추가로, 제2 노드에 스케줄링 표시를 전송하는 단계- 스케줄링 표시는 그룹 임 시 식별자를 포함하고, 모델 트레이닝의 현재 라운드에서의 AI 모델의 트레이닝을 수행하기 위해, 그룹 임시 식 별자에 대응하는 단말을 스케줄링하기 위해 스케줄링 표시가 사용됨 -를 포함한다. 가능한 설계에서, 이러한 방법은 추가로, 모델 트레이닝 종료 조건이 충족될 때, 모델 트레이닝의 현재 라운드 에서의 AI 모델의 트레이닝을 종료하라고 단말에 표시하기 위해, 제2 노드에 모델 트레이닝 종료 표시를 전송하 는 단계를 포함한다. 제4 양태에 따르면, 장치가 제공된다. 이러한 장치는 제1 양태, 제2 양태, 또는 제3 양태에서 설명되는 방법/ 동작들/단계들/액션들과 일-대-일 대응관계에 있는 유닛들 또는 모듈들을 포함한다. 이러한 유닛 또는 모듈은 하드웨어 회로일 수 있거나, 또는 소프트웨어일 수 있거나, 또는 소프트웨어와 조합하여 하드웨어 회로에 의해 구현될 수 있다. 제5 양태에 따르면, 통신 장치가 제공된다. 이러한 장치는 프로세서 및 메모리를 포함한다. 메모리는 컴퓨터 프로그램 또는 명령어들을 저장하도록 구성되고, 프로세서는 메모리에 연결된다. 프로세서가 컴퓨터 프로그램 또는 명령어들을 실행할 때, 이러한 장치는 제1 양태, 제2 양태, 또는 제3 양태에서의 방법을 수행하는 것이 가 능하게 된다. 제6 양태에 따르면, 장치가 제공되고, 이는 프로세서 및 인터페이스 회로를 포함한다. 프로세서는, 인터페이스 회로를 통해 다른 장치와 통신하도록, 그리고 제1 양태, 제2 양태, 또는 제3 양태 중 어느 하나에서 설명되는 방법을 수행하도록 구성된다. 하나 이상의 프로세서가 존재한다. 제7 양태에 따르면, 장치가 제공되고, 이는 메모리에 연결되는 프로세서를 포함한다. 프로세서는 메모리에 저 장된 프로그램을 실행하여, 제1 양태, 제2 양태, 또는 제3 양태 중 어느 하나에서 설명되는 방법을 수행하도록 구성된다. 메모리는 장치의 내부 또는 외부에 위치될 수 있다. 또한, 하나 이상의 프로세서가 존재할 수 있다. 제8 양태에 따르면, 칩 시스템이 제공되고, 이는 제1 양태, 제2 양태, 또는 제3 양태 중 어느 하나에서 설명되 는 방법을 수행하도록 구성되는 프로세서 또는 회로를 포함한다. 제9 양태에 따르면, 컴퓨터-판독가능 저장 매체가 제공된다. 컴퓨터-판독가능 저장 매체는 컴퓨터 프로그램 또 는 명령어들을 저장한다. 이러한 컴퓨터 프로그램 또는 명령어들이 장치에 의해 실행될 때, 장치는 제1 양태, 제2 양태, 또는 제3 양태에서의 방법을 수행하는 것이 가능하게 된다. 제10 양태에 따르면, 컴퓨터 프로그램 제품이 제공된다. 이러한 컴퓨터 프로그램 제품은 컴퓨터 프로그램 또는 명령어들을 포함한다. 이러한 컴퓨터 프로그램 또는 명령어들이 장치에 의해 실행될 때, 장치는 제1 양태, 제2 양태, 또는 제3 양태에서의 방법을 수행하는 것이 가능하게 된다. 제11 양태에 따르면, 시스템이 제공되고, 이는 제1 양태에서의 방법을 수행하기 위한 장치 및 제2 양태에서의 방법을 수행하기 위한 장치를 포함한다. 선택적으로, 이러한 장치는 제3 양태에서의 방법을 수행하기 위한 장 치를 추가로 포함할 수 있다."}
{"patent_id": "10-2024-7021349", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 1, "content": "도 1은 본 출원이 적용가능한 통신 시스템의 아키텍처의 개략도이다. 도 1에 도시되는 바와 같이, 통신 시스템은 무선 액세스 네트워크와 코어 네트워크를 포함한다. 선택적으로, 통신 시스템은 인 터넷을 추가로 포함할 수 있다. 무선 액세스 네트워크는 적어도 하나의 액세스 네트워크 디바이스 (예를 들어, 도 1에서의 110a 및 110b)를 포함할 수 있고, 적어도 하나의 단말(예를 들어, 도 1에서의 120a 내 지 120j)을 추가로 포함할 수 있다. 단말은 무선 방식으로 액세스 네트워크 디바이스에 접속되고, 액세스 네트 워크 디바이스는 무선 또는 유선 방식으로 코어 네트워크에 접속된다. 코어 네트워크 디바이스 및 액세스 네트 워크 디바이스는 서로 독립적인 상이한 물리적 디바이스들일 수 있거나, 또는 코어 네트워크 디바이스의 기능들및 액세스 네트워크 디바이스의 논리적 기능들은 동일한 물리적 디바이스에 통합될 수 있거나, 또는 코어 네트 워크 디바이스의 기능들의 일부 및 액세스 네트워크 디바이스의 기능들의 일부는 하나의 물리적 디바이스에 통 합될 수 있다. 단말들은 유선 또는 무선 방식으로 서로 접속될 수 있고, 액세스 네트워크 디바이스들은 유선 또는 무선 방식으로 서로 접속될 수 있다. 도 1은 단지 개략도이다. 통신 시스템은 다른 네트워크 디바이스를 추가로 포함할 수 있다, 예를 들어, 도 1에 도시되지 않은 무선 중계 디바이스 및 무선 백홀 디바이스를 추가로 포함할 수 있다. 액세스 네트워크 디바이스는 기지국(base station), 진화된 NodeB(evolved NodeB, eNodeB), 송신 수신 포인트 (transmission reception point, TRP), 5세대(5th generation, 5G) 이동 통신 시스템에서의 차세대 NodeB(next generation NodeB, gNB), 개방 무선 액세스 네트워크(open radio access network, O-RAN)에서의 액세스 네트워 크 디바이스, 6세대(6th generation, 6G) 이동 통신 시스템에서의 차세대 기지국, 미래의 이동 통신 시스템에서 의 기지국, 무선 충실도(wireless fidelity, Wi-Fi) 시스템에서의 액세스 노드 등일 수 있거나; 또는 모듈 또는 유닛일 수 있다, 예를 들어, 기지국의 기능들의 일부를 완료하는, 중앙 유닛(central unit, CU), 분산 유닛 (distributed unit, DU), 중앙 유닛 제어 평면(CU control plane, CU-CP) 모듈, 또는 중앙 유닛 사용자 평면 (CU user plane, CU-UP) 모듈일 수 있다. 액세스 네트워크 디바이스는 매크로 기지국(예를 들어, 도 1에서의 110a)일 수 있거나, 또는 마이크로 기지국 또는 실내 기지국(예를 들어, 도 1에서의 110b)일 수 있거나, 또는 중계 노드, 도너 노드 등일 수 있다. 액세스 네트워크 디바이스에 의해 사용되는 구체적인 기술 및 구체적인 디바이스 형태는 본 출원에서 제한되지 않는다. 본 출원에서, 액세스 네트워크 디바이스의 기능들을 구현하도록 구성되는 장치는 액세스 네트워크 디바이스일 수 있거나; 또는, 기능들을 구현함에 있어서 액세스 네트워크 디바이스를 지원할 수 있는 장치, 예를 들어, 칩 시스템, 하드웨어 회로, 소프트웨어 모듈, 또는 하드웨어 회로와 소프트웨어 모듈의 조합일 수 있고, 이러한 장 치는 액세스 네트워크 디바이스에 설치될 수 있거나 또는 사용을 위해 액세스 네트워크 디바이스와 매칭될 수 있다. 본 출원에서, 칩 시스템은 칩을 포함할 수 있거나, 또는 칩 및 다른 개별 컴포넌트를 포함할 수 있다. 설명의 용이함을 위해, 다음은 액세스 네트워크 디바이스의 기능들을 구현하도록 구성되는 장치가 액세스 네트 워크 디바이스이고 액세스 네트워크 디바이스가 기지국인 예를 사용하여 본 출원에서 제공되는 기술적 해결책들 을 설명한다. 프로토콜 레이어 구조 액세스 네트워크 디바이스와 단말 사이의 통신은 구체적인 프로토콜 레이어 구조를 준수한다. 프로토콜 레이어 구조는 제어 평면 프로토콜 레이어 구조 및 사용자 평면 프로토콜 레이어 구조를 포함할 수 있다. 예를 들어, 제어 평면 프로토콜 레이어 구조는 무선 리소스 제어(radio resource control, RRC) 레이어, 패킷 데이터 수렴 프로토콜(packet data convergence protocol, PDCP) 레이어, 무선 링크 제어(radio link control, RLC) 레이어, 매체 액세스 제어(media access control, MAC) 레이어, 및 물리적 레이어와 같은 프로토콜 레이어들의 기능들을 포함할 수 있다. 예를 들어, 사용자 평면 프로토콜 레이어 구조는 PDCP 레이어, RLC 레이어, MAC 레 이어, 및 물리적 레이어와 같은 프로토콜 레이어들의 기능들을 포함할 수 있다. 가능한 구현에서, 서비스 데이 터 적응 프로토콜(service data adaptation protocol, SDAP) 레이어가 PDCP 레이어 위에 추가로 포함될 수 있 다. 선택적으로, 액세스 네트워크 디바이스와 단말 사이의 프로토콜 레이어 구조는 AI 기능에 관련된 데이터를 송신 하도록 구성되는 인공 지능(artificial intelligence, AI) 레이어를 추가로 포함할 수 있다. 중앙 유닛(central unit, CU) 및 분산 유닛(distributed unit, DU) 액세스 디바이스는 CU 및 DU를 포함할 수 있다. 중앙집중식 방식으로 하나의 CU에 의해 복수의 DU들이 제어될 수 있다. 예를 들어, CU와 DU 사이의 인터페이스는 F1 인터페이스라고 지칭될 수 있다. 제어 평면(control plane, CP) 인터페이스는 F1-C일 수 있고, 사용자 평면(user plane, UP) 인터페이스는 F1-U일 수 있다. 각각 의 인터페이스의 구체적인 명칭은 본 출원에서 제한되지 않는다. CU 및 DU는 무선 네트워크의 프로토콜 레이어 들에 기초하여 정의될 수 있다. 예를 들어, PDCP 레이어 및 PDCP 레이어 위의 프로토콜 레이어의 기능들은 CU 상에서 설정되고, PDCP 레이어 아래의 프로토콜 레이어들(예를 들어, RLC 레이어 및 MAC 레이어)의 기능들은 DU 상에서 설정된다. 다른 예를 들어, PDCP 레이어 위의 프로토콜 레이어의 기능들은 CU 상에서 설정되고, PDCP 레이어 및 PDCP 레이어 아래의 프로토콜 레이어들의 기능들은 DU 상에서 설정된다. 이러한 것은 제한되지 않는 다.프로토콜 레이어들에 기초하는 CU 및 DU의 처리 기능들로의 전술한 분할은 단지 예이고, 대안적으로 다른 분할 일 수 있다. 예를 들어, CU 또는 DU는 더 많은 프로토콜 레이어들의 기능들을 갖도록 정의될 수 있고, 다른 예 를 들어, CU 또는 DU는 대안적으로 프로토콜 레이어들의 처리 기능들의 일부를 갖도록 정의될 수 있다. 설계에 서, RLC 레이어의 기능들 및 RLC 레이어 위의 프로토콜 레이어들의 기능들의 일부가 CU 상에서 설정되고, RLC 레이어의 나머지 기능들 및 RLC 레이어 아래의 프로토콜 레이어들의 기능들이 DU 상에서 설정된다. 다른 설계 에서, CU 또는 DU의 기능들의 분할은 서비스 타입 또는 다른 시스템 요건에 기초하여 대안적으로 수행될 수 있 다. 예를 들어, 분할은 지연에 기초하여 수행될 수 있다. 그 처리 시간이 지연 요건을 충족할 필요가 있는 기 능이 DU 상에서 설정되고, 그 처리 시간이 지연 요건을 충족할 필요가 없는 기능이 CU 상에서 설정된다. 다른 설계에서, CU는 대안적으로 코어 네트워크의 하나 이상의 기능을 가질 수 있다. 예를 들어, CU는 중앙집중식 관리를 용이하게 하기 위해 네트워크 측에 배치될 수 있다. 다른 설계에서는, DU의 무선 유닛(radio unit, R U)이 원격으로 배치된다. 선택적으로, RU는 무선 주파수 기능을 가질 수 있다. 선택적으로, DU와 RU는 물리 레이어(physical layer, PHY)에서 구별될 수 있다. 예를 들어, DU는 PHY 레이어 의 상위-레이어 기능들을 구현할 수 있고, RU는 PHY 레이어의 하위-레이어 기능들을 구현할 수 있다. PHY 레이 어가 전송을 위해 사용될 때, PHY 레이어의 기능들은 다음의 기능들: 순환 중복 체크(cyclic redundancy check, CRC) 코드의 추가, 채널 인코딩, 레이트 매칭, 스크램블링, 변조, 레이어 맵핑, 프리코딩, 리소스 맵핑, 물리 안테나 맵핑, 또는 무선 주파수 전송 중 적어도 하나를 포함할 수 있다. PHY 레이어가 수신을 위해 사용될 때, PHY 레이어의 기능들은 다음의 기능들: CRC 체크, 채널 디코딩, 디-레이트 매칭, 디스크램블링, 복조, 레이어 디맵핑, 채널 검출, 리소스 디맵핑, 물리 안테나 디맵핑, 또는 무선 주파수 수신 중 적어도 하나를 포함할 수 있다. PHY 레이어의 상위-레이어 기능들은 PHY 레이어의 기능들의 일부를 포함할 수 있다. 예를 들어, 이러한 기능들의 일부는 MAC 레이어에 더 가깝다. PHY 레이어의 하위-레이어 기능들은 PHY 레이어의 기능들의 다른 부 분을 포함할 수 있다. 예를 들어, 이러한 기능들의 일부는 무선 주파수 기능에 더 가깝다. 예를 들어, PHY 레 이어의 상위-레이어 기능들은 CRC 코드의 추가, 채널 인코딩, 레이트 매칭, 스크램블링, 변조 및 레이어 맵핑을 포함할 수 있고, PHY 레이어의 하위-레이어 기능들은 프리코딩, 리소스 맵핑, 물리 안테나 맵핑 및 무선 주파수 전송의 기능들을 포함할 수 있다. 대안적으로, PHY 레이어의 상위-레이어 기능들은 CRC 코드의 추가, 채널 인 코딩, 레이트 매칭, 스크램블링, 변조, 레이어 맵핑, 및 프리코딩을 포함할 수 있고, PHY 레이어의 하위-레이어 기능들은 리소스 맵핑, 물리 안테나 맵핑, 및 무선 주파수 전송의 기능들을 포함할 수 있다. 예를 들어, PHY 레이어의 상위-레이어 기능들은 CRC 체크, 채널 디코딩, 디-레이트 매칭, 디코딩, 복조, 및 레이어 디맵핑을 포 함할 수 있고, PHY 레이어의 하위-레이어 기능들은 채널 검출, 리소스 디맵핑, 물리 안테나 디맵핑, 및 무선 주 파수 수신의 기능들을 포함할 수 있다. 대안적으로, PHY 레이어의 상위-레이어 기능들은 CRC 체크, 채널 디코 딩, 디-레이트 매칭, 디코딩, 복조, 레이어 디맵핑 및 채널 검출을 포함할 수 있고, PHY 레이어의 하위-레이어 기능들은 리소스 디맵핑, 물리 안테나 디맵핑 및 무선 주파수 수신의 기능들을 포함할 수 있다. 예를 들어, CU의 기능들은 하나의 엔티티에 의해 구현될 수 있거나, 또는 상이한 엔티티들에 의해 구현될 수 있 다. 예를 들어, CU의 기능들이 추가로 분할될 수 있다. 구체적으로, CU의 제어 평면 및 사용자 평면은 제어 평면 CU 엔티티(즉, CU-CP 엔티티) 및 사용자 평면 CU 엔티티(즉, CU-UP 엔티티)인 상이한 엔티티들에 의해 분 리되고 구현된다. CU-CP 엔티티 및 CU-UP 엔티티는 DU에 연결될 수 있어, 액세스 네트워크 디바이스의 기능들 을 공동으로 완료한다. 선택적으로, DU, CU, CU-CP, CU-UP, 및 RU 중 어느 하나는 소프트웨어 모듈, 하드웨어 구조, 또는 소프트웨어 모듈과 하드웨어 구조의 조합일 수 있다. 이러한 것은 제한되지 않는다. 상이한 엔티티들이 상이한 형태들로 존재할 수 있으며, 이는 제한되지 않는다. 예를 들어, DU, CU, CU-CP, 및 CU-UP는 소프트웨어 모듈들이고, RU 는 하드웨어 구조이다. 이러한 모듈들 및 이러한 모듈들에 의해 수행되는 방법들은 또한 본 개시내용의 보호 범위 내에 속한다. 가능한 구현에서, 액세스 네트워크 디바이스는 CU-CP, CU-UP, DU, 및 RU를 포함한다. 예를 들어, 본 출원은 DU, 또는 DU 및 RU, 또는 CU-CP, DU, 및 RU, 또는 CU-UP, DU, 및 RU에 의해 실행된다. 이러한 것은 제한되지 않는다. 모듈들에 의해 수행되는 방법들 또한 본 출원의 보호 범위 내에 속한다. 단말은 단말 디바이스, 사용자 장비(user equipment, UE), 이동국, 이동 단말 등이라고 또한 지칭될 수 있다. 단말은, 예를 들어, 다음의 시나리오들: 디바이스-대-디바이스(device-to-device, D2D), 차량-대-사물 (vehicle-to-everything, V2X), 머신-타입 통신(machine-type communication, MTC), 사물 인터넷(internet of things, IOT), 가상 현실, 증강 현실, 산업 제어, 자율-주행, 원격의료, 스마트 그리드, 스마트 가구, 스마트 오피스, 스마트 웨어러블, 스마트 수송, 스마트 시티 등 중 적어도 하나를 포함하지만 이에 제한되지는 않는 다양한 시나리오들에서의 통신에 널리 적용된다. 단말은 이동 전화, 태블릿 컴퓨터, 무선 송수신기 기능을 갖는 컴퓨터, 웨어러블 디바이스, 차량, 무인 비행체, 헬리콥터, 비행기, 선박, 로봇, 기계 팔, 스마트 홈 디바이스 등일 수 있다. 단말에 의해 사용되는 구체적인 기술 및 구체적인 디바이스 형태는 본 출원에서 제한되지 않는 다. 본 출원에서, 단말의 기능들을 구현하도록 구성되는 장치는 단말일 수 있거나; 또는, 기능들을 구현함에 있어서 단말을 지원할 수 있는 장치, 예를 들어, 칩 시스템, 하드웨어 회로, 소프트웨어 모듈, 또는 하드웨어 회로와 소프트웨어 모듈의 조합일 수 있고, 이러한 장치는 단말에 설치될 수 있거나 또는 사용을 위해 단말과 매칭될 수 있다. 설명들의 용이함을 위해, 다음은 단말의 기능들을 구현하도록 구성되는 장치가 단말인 예를 사용하여 본 출원에서 제공되는 기술적 해결책들을 설명한다. 기지국 및 단말은 고정되거나 또는 이동가능할 수 있다. 기지국 및/또는 단말은, 실내 또는 실외 시나리오, 및 핸드헬드 또는 차량-장착형 시나리오를 포함하여, 육상에 배치될 수 있거나; 또는 수상에 배치될 수 있거나; 또 는 공기 중에 있는 비행기, 풍선, 및 인공 위성 상에 배치될 수 있다. 기지국 및 단말의 애플리케이션 시나리 오들은 본 출원에서 제한되지 않는다. 기지국 및 단말은 동일한 시나리오 또는 상이한 시나리오들로 배치될 수 있다. 예를 들어, 기지국 및 단말은 양자 모두 육상에 배치된다. 대안적으로, 기지국은 육상에 배치되고, 단 말은 수상에 배치된다. 예들이 하나씩 설명되지는 않는다. 기지국 및 단말의 역할들은 상대적일 수 있다. 예를 들어, 도 1에서의 헬리콥터 또는 무인 비행체(120i)가 이 동 기지국으로서 구성될 수 있다. 120i를 통해 무선 액세스 네트워크에 액세스하는 단말(120j)에 대해, 단말(120i)은 기지국이고; 반면 기지국(110a)에 대해, 120i는 단말이다, 다시 말해서, 110a 및 120i는 무선 에 어 인터페이스 프로토콜에 기초하여 서로 통신한다. 110a 및 120i는 대안적으로 기지국들 사이의 인터페이스 프로토콜에 기초하여 서로 통신할 수 있다. 이러한 경우, 110a에 상대적으로, 120i가 또한 기지국이다. 따라 서, 기지국 및 단말은 통신 장치들이라고 집합적으로 지칭될 수 있고, 도 1에서의 110a 및 110b는 기지국의 기 능들을 갖는 통신 장치라고 지칭될 수 있고, 도 1에서의 120a 내지 120j는 단말의 기능들을 갖는 통신 장치라고 지칭될 수 있다. 기지국과 단말 사이의, 기지국과 기지국 사이의, 또는 단말과 단말 사이의 통신은 허가 스펙트럼을 사용하여 수 행될 수 있거나, 또는 비허가 스펙트럼을 사용하여 수행될 수 있거나, 또는 허가 스펙트럼 및 비허가 스펙트럼 양자 모두를 사용하여 수행될 수 있다. 6 기가헤르츠(gigahertz, GHz) 미만의 스펙트럼을 사용하여 수행될 수 있거나, 또는 6GHz 초과의 스펙트럼을 사용하여 수행될 수 있거나, 또는 6GHz 미만의 스펙트럼 및 6GHz 초과의 스펙트럼 양자 모두를 사용하여 통신이 수행될 수 있다. 무선 통신을 위해 사용되는 스펙트럼 리소스는 본 출 원에서 제한되지 않는다. 본 출원에서, 기지국은 단말에 다운링크 신호 또는 다운링크 정보를 전송하고- 다운링크 정보는 다운링크 채널 상에서 운반됨 -; 단말은 기지국에 업링크 신호 또는 업링크 정보를 전송한다- 업링크 정보는 업링크 채널 상에 서 운반됨 -. 기지국과 통신하기 위해, 단말은 기지국에 의해 제어되는 셀과의 무선 접속을 수립할 수 있다. 단말이 무선 접속을 수립하는 셀은 단말의 서빙 셀이라고 지칭된다. 서빙 셀과 통신할 때, 단말은 이웃 셀로부 터의 신호에 의해 간섭을 받을 수 있다. 본 출원에서, AI-관련 동작을 구현하기 위해 도 1에 도시되는 통신 시스템에 독립 네트워크 엘리먼트(예를 들어, 중앙 노드, AI 네트워크 엘리먼트, 또는 AI 노드라고 지칭됨)가 도입될 수 있다. 중앙 노드는 통신 시스 템에서의 액세스 네트워크 디바이스에 직접 접속될 수 있거나, 또는 제3자 네트워크 엘리먼트를 통해 액세스 네 트워크 디바이스에 간접적으로 접속될 수 있다. 제3자 네트워크 엘리먼트는 인증 관리 기능(authentication management function, AMF) 네트워크 엘리먼트 또는 사용자 평면 기능(user plane function, UPF) 네트워크 엘 리먼트와 같은 코어 네트워크 엘리먼트일 수 있다. 대안적으로, AI-관련 동작을 구현하도록 통신 시스템에서의 다른 네트워크 엘리먼트에서 AI 기능, AI 모듈, 또는 AI 엔티티가 구성될 수 있다. 예를 들어, 다른 네트워크 엘리먼트는 액세스 네트워크 디바이스(예를 들어, gNB), 코어 네트워크 디바이스, 또는 네트워크 관리 (operation, administration and maintenance, OAM)일 수 있다. 이러한 경우, AI-관련 동작을 수행하는 네트 워크 엘리먼트는 내장된 AI 기능을 갖는 네트워크 엘리먼트이다. 본 출원에서, OAM은 코어 네트워크 디바이스 를 동작, 관리, 및/또는 유지보수하도록 구성되고, 및/또는 액세스 네트워크 디바이스를 동작, 관리, 및/또는 유지보수하도록 구성된다. 본 출원에서, AI 모델은 AI 기능을 구현하기 위한 구체적인 방법이고, AI 모델은 모델의 입력과 출력 사이의 맵 핑 관계를 표현한다. AI 모델은 신경망 또는 다른 머신 학습 모델일 수 있다. AI 모델은 줄여서 모델이라고지칭될 수 있다. AI-관련 동작은 다음: 데이터 수집, 모델 트레이닝, 모델 정보 릴리즈, 모델 추론(model inference), 추론 결과 릴리즈 등 중 적어도 하나를 포함할 수 있다. 신경망이 예로서 사용된다. 신경망은 머신 러닝 기술의 구체적인 구현 형태이다. 보편적 근사화 정리에 따르 면, 신경망은 이론적으로 임의의 연속 함수에 근사화할 수 있어서, 신경망은 임의의 맵핑을 학습하는 능력을 갖 는다. 종래의 통신 시스템에서, 통신 모듈은 풍부한 전문 지식으로 설계될 필요가 있다. 그러나, 신경망-기반 딥 러닝 통신 시스템은 대량의 데이터 세트로부터 암시적 패턴 구조를 자동으로 발견하고, 데이터 사이의 맵핑 관계를 수립하고, 종래의 모델링 방법의 성능보다 양호한 성능을 획득할 수 있다. 신경망의 아이디어는 뇌 조직의 뉴런 구조로부터의 것이다. 각각의 뉴런은 뉴런의 입력 값들에 대해 가중 합산 연산을 수행하고, 활성화 함수를 통해 가중 합산의 결과를 출력한다. 도 2a는 뉴런의 구조의 개략도이다. 뉴 런의 입력들은 이고, 입력들에 대응하는 가중치들은 각각 이고, 가중 합 산의 바이어스는 b인 것으로 가정된다. 활성화 함수의 형태들은 다양화될 수 있다. 하나의 뉴런의 활성화 함 수는 인 것으로 가정된다. 이러한 경우, 뉴런의 출력은 이다. 다른 예로서, 하나의 뉴런의 활성화 함수가 이면, 뉴런의 출력은 이다. b는 십진수, 정수(0, 양의 정수, 음의 정수 등을 포함함), 또는 복소수와 같은 임의의 가능한 값일 수 있다. 신경망에서의 상이한 뉴런들 의 활성화 함수들은 동일하거나 또는 상이할 수 있다. 신경망은 멀티-레이어 구조를 일반적으로 포함하고, 각각의 레이어는 하나 이상의 뉴런을 포함할 수 있다. 신 경망의 깊이 및/또는 폭을 증가시키는 것은 신경망의 표현 능력을 개선할 수 있고, 복잡한 시스템들에 대한 더 강력한 정보 추출 및 추상화 모델링 능력들을 제공할 수 있다. 신경망의 깊이는 신경망에 포함되는 레이어들의 수량을 지칭할 수 있고, 각각의 레이어에 포함되는 뉴런들의 수량은 레이어의 폭이라고 지칭될 수 있다. 도 2b 는 신경망의 레이어 관계의 개략도이다. 구현에서, 신경망은 입력 레이어 및 출력 레이어를 포함한다. 수신된 입력에 관해 뉴런 처리를 수행한 후에, 신경망의 입력 레이어는 결과를 출력 레이어에 이송하고, 출력 레이어는 신경망의 출력 결과를 획득한다. 다른 구현에서, 신경망은 입력 레이어, 은닉 레이어, 및 출력 레이어를 포함 한다. 수신된 입력에 대해 뉴런 처리를 수행한 이후, 신경망의 입력 레이어는 결과를 중간 은닉 레이어에 이송 하고, 다음으로 은닉 레이어는 계산 결과를 출력 레이어 또는 인접한 은닉 레이어에 이송하고, 최종적으로, 출 력 레이어는 신경망의 출력 결과를 획득한다. 하나의 신경망은 하나의 은닉 레이어 또는 순차적으로 접속되는 복수의 은닉 레이어들을 포함할 수 있다. 이러한 것은 제한되지 않는다. 신경망의 트레이닝 프로세스에서, 손 실 함수가 정의될 수 있다. 손실 함수는 신경망의 출력 값과 이상적인 타겟 값 사이의 갭 또는 차이를 설명한 다. 손실 함수의 구체적인 형태는 본 출원에서 제한되지 않는다. 신경망의 트레이닝 프로세스는, 신경망의 그 래디언트, 레이어들의 수량, 및 폭, 뉴런의 가중치, 뉴런의 활성화 함수에서의 파라미터 등과 같은, 신경망 파 라미터를 조정하여, 손실 함수의 값이 임계값보다 작거나 또는 타겟 요건을 충족하는 프로세스이다. 도 2c는 AI의 애플리케이션 프레임워크의 개략도이다. 데이터 소스(data source)는 트레이닝 데이터 및 추론 데이터를 저장하도록 구성된다. 모델 트레이닝 노드(model training host)는 데이터 소스에 의해 제공되는 트 레이닝 데이터(training data)를 분석 또는 트레이닝시켜, AI 모델을 획득하고, AI 모델을 모델 추론 노드 (model inference host)에 배치한다. 선택적으로, 모델 트레이닝 노드는 모델 추론 노드 상에 배치되는 AI 모 델을 추가로 업데이트할 수 있다. 모델 추론 노드는 배치된 모델의 관련 정보를 모델 트레이닝 노드에게 추가 로 피드백할 수 있어서, 모델 트레이닝 노드는 배치된 AI 모델을 최적화하거나 또는 업데이트하는 등이다. AI 모델은 모델의 입력과 출력 사이의 맵핑 관계를 표현한다. 모델 트레이닝 노드에 의한 학습을 통해 AI 모델 을 획득하는 것은 트레이닝 데이터를 사용하여 모델 트레이닝 노드에 의한 학습을 통해 모델의 입력과 출력 사 이의 맵핑 관계를 획득하는 것과 동등하다. 모델 추론 노드는 AI 모델을 사용하여 데이터 소스에 의해 제공된 추론 데이터에 기초하여 추론을 수행하고 추론 결과를 획득한다. 이러한 방법은 또한 다음과 같이 설명될 수 있다: 모델 추론 노드는 추론 데이터를 AI 모델에 입력하고, AI 모델을 사용하여 출력을 획득한다. 출력은 추 론 결과이다. 추론 결과는 액션의 대상에 의해 사용(동작)되는 구성 파라미터 및/또는 액션의 대상에 의해 수 행되는 동작을 표시할 수 있다. 추론 결과는 액터(actor) 엔티티에 의해 중앙집중식으로 계획될 수 있고, 액션 을 위해 액션의 하나 이상의 대상(예를 들어, 네트워크 엔티티들)에 전송될 수 있다.연합 학습(federated learning, FL)은 인기있는 AI/ML 모델 트레이닝 프레임워크이고, 사용자 프라이버시 보호, 데이터 보안, 및 정부 규제들의 요건들을 충족하면서 복수의 조직들이 데이터를 사용하고 머신 학습 모델링을 수행하는 것을 효과적으로 도울 수 있다. 분산 머신 학습 모델로서의 연합 학습은 데이터 사일로의 문제를 효 과적으로 해결할 수 있고, 참여자가 데이터를 공유하지 않고 공동 모델링을 수행할 수 있게 해주고, 그렇게 함 으로써 기술적으로 데이터 사일로를 깨고 AI 협업을 구현한다. 연합 학습은 중앙 노드 및 에지 노드를 포함한 다. 중앙 노드(예를 들어, 서버 또는 기지국)는 프라이버시를 손상시키지 않고 모델 트레이닝에 참여하기 위해 에지 디바이스(예를 들어, 스마트폰 또는 센서) 상에 위치되는 분산 디바이스를 호출할 수 있다. 연합 학습은 다음의 3개의 타입들: 수평 연합 학습, 수직 연합 학습, 및 연합 이송 학습을 포함한다. 본 출원 은 주로 수평 연합 학습의 프로세스에 관련된다. 도 3은 수평 연합 학습의 트레이닝 프로세스를 도시한다. 수 평 연합 학습은 하나의 중앙 노드 및 복수의 에지 노드들을 포함한다는 점을 알 수 있다. 원본 데이터는 에지 노드 상에 분포되고, 중앙 노드는 원본 데이터를 갖지 않으며, 에지 노드는 원본 데이터를 중앙 노드에 전송하 는 것을 허용하지 않는다. 연합 학습의 트레이닝 프로세스에서, 중앙 노드는 먼저 (초기 AI 모델이라고 지칭될 수 있는) 초기화된 AI 모델 을 각각의 에지 노드에 전송하고, 다음으로 반복 트레이닝을 시작한다. 각각의 반복 트레이닝 프로세스는 다음 과 같다: 1. 에지 노드는 로컬 데이터를 사용하여 초기 AI 모델을 트레이닝하고, 트레이닝된 AI 모델의 그래디언트를 획 득한다. 2. 각각의 에지 노드는 에지 노드에 의한 트레이닝을 통해 획득되는 그래디언트를 중앙 노드에 보고한다. 3. 에지 노드들에 의해 보고되는 그래디언트들을 수신한 후, 중앙 노드는 그래디언트들을 집계하고, 집계된 그 래디언트에 기초하여 AI 모델의 파라미터를 업데이트한다. 4. 중앙 노드는 트레이닝에 참여하는 각각의 에지 노드에 집계된 그래디언트를 전달하고, 에지 노드는 중앙 노 드에 의해 전달된 집계된 그래디언트에 기초하여, 로컬로 트레이닝되는 AI 모델의 파라미터 및 그래디언트를 업 데이트한다. 5. 중앙 노드는 파라미터를 업데이트하는 것에 의해 획득되는 AI 모델의 손실 함수를 계산하고; 손실 함수가 조 건을 충족하면, 모델 트레이닝을 종료하고; 손실 함수가 조건을 충족하지 않으면, 전술한 단계들 2 내지 4를 반 복한다. 종래의 연합 학습 해결책에서, 상호 직교 업링크 시간-주파수 리소스들은 에지 노드들에 할당될 필요가 있고, 에지 노드들은 상호 직교 업링크 시간-주파수 리소스들 상에서 그래디언트들을 송신한다. 중앙 노드는, 그래디 언트 집계를 수행하기 위해, 연합 학습 트레이닝에 참여하는 모든 단말들에 의해 보고되는 그래디언트들을 순차 적으로 복원할 필요가 있다. 이러한 것은 시간-주파수 리소스들의 큰 오버헤드들 및 큰 지연을 야기하고, 제한 된 대역폭 및/또는 높은 지연 요건을 갖는 연합 학습 시나리오에 적용가능하지 않다. 본 출원은 무선 네트워크에서의 AI 모델을 트레이닝하기 위한 방법을 제공한다. 이러한 방법에서, 연합 학습에 참여하는 단말들에 동일한 시간-주파수 리소스 및 동일한 보고 순간이 할당될 수 있다. 연합 학습에 참여하는 단말들은 동일한 시간-주파수 리소스를 사용하여 동일한 보고 순간에 트레이닝된 AI 모델의 그래디언트들을 보 고하여, 복수의 상호 직교하는 시간-주파수 리소스들이 연합 학습에 참여하는 단말들에 할당되기 때문에 야기되 는 큰 시간-주파수 리소스 오버헤드들 및 큰 지연의 전술한 문제들을 해결한다. 도 4에 도시되는 바와 같이, 무선 네트워크에서의 AI 모델을 트레이닝하기 위한 방법의 프로시저가 제공된다. 이러한 프로시저는 적어도 다음의 단계들을 포함한다. 단계 401: 기지국이 연합 학습에 참여하는 단말에 제1 구성 정보를 전송함- 제1 구성 정보는 다음: 트레이닝 지 속기간, 시간-주파수 리소스 또는 보고 순간 중 적어도 하나를 구성하기 위해 사용됨 -. 대응하여, 단말은 기지 국으로부터 제1 구성 정보를 수신한다. 단계 402: 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 획득하기 위해, 단말이 트레이닝 지속기 간 내에 AI 모델을 트레이닝함. 단계 403: 단말이, 기지국에, 시간-주파수 리소스를 사용하여 보고 순간에 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 보고함. 대응하여, 기지국은 단말들에 의해 보고되는 그래디언트들의 공중 중첩을 통해획득되는 신호를 수신한다. 본 출원에서, 연합 학습에 참여하는 단말들에 대해 기지국 또는 중앙 노드에 의해 동일한 트레이닝 지속기간, 동일한 시간-주파수 리소스, 및 동일한 보고 순간이 구성된다. 예를 들어, 연합 학습에 참여하는 단말들의 수 량은 n이다. 본 출원에서, 기지국 또는 중앙 노드는 n개의 단말들에 동일한 시간-주파수 리소스를 할당한다. 연합 학습에 참여하는 n개의 단말들에 n개의 직교 시간-주파수 리소스들이 할당되는 종래의 해결책과 비교하여, 시간-주파수 리소스들의 오버헤드들이 감소될 수 있다. 또한, n개의 직교 시간-주파수 리소스들이 연합 학습에 참여하는 n개의 단말들에 할당되면, 각각의 단말은 각각의 시간-주파수 리소스를 사용하여 AI 모델의 그래디언 트를 보고한다. 기지국은 n개의 무선 주파수 신호들을 수신하고, n개의 무선 주파수 신호들을 개별적으로 처리 하여, 각각의 단말에 의해 보고되는 그래디언트를 복원할 수 있다. 결과적으로, 지연이 크다. 그러나, 본 출 원에서, 기지국은 연합 학습에 참여하는 n개의 단말들에 하나의 시간-주파수 리소스를 할당하고, n개의 단말들 은 시간-주파수 리소스 상에서 그래디언트들을 보고한다. 따라서, 무선 채널의 중첩 속성에 기초하여, 공중 송 신 동안 n개의 그래디언트들이 함께 중첩된다. 예를 들어, n의 값은 3이고, 단말 1에 의해 보고되는 그래디언 트는 Y1이고, 단말 2에 의해 보고되는 그래디언트는 Y2이고, 단말 3에 의해 보고되는 그래디언트는 Y3이다. 이 러한 경우, 전술한 3개의 그래디언트들은 동일한 시간-주파수 리소스 상에서 송신되고, 전술한 3개의 그래디언 트들은 함께 중첩된다. 무선 채널은 완벽한 신호 중첩을 충족한다고 가정된다(여기서, 페이딩, 간섭, 잡음 등 은 무시될 수 있다). 이러한 경우, 중첩된 신호는 Y=Y1+Y2+Y3이다. 채널이 완벽한 신호 중첩을 충족하지 않을 때, 전술한 시간-주파수 리소스 상에서 전술한 신호를 수신한 후에, 기지국은 수신된 신호에 대해 신호 처리를 수행하여, 중첩된 신호 Y를 복원할 수 있고, 단말들은 중첩된 신호 Y를 사용하여 그래디언트 집계를 수행할 수 있다. 그래디언트 집계의 프로세스는 산술 평균을 계산하는 프로세스일 수 있다. 예를 들어, 중첩된 신호 Y는 3으로 나누어질 수 있고, 결과는 집계 그래디언트로서 사용된다. 본 출원의 해결책에 따르면, 중첩 신호 Y는 하나의 무선 주파수 신호를 처리하는 것에 의해 획득될 수 있다. 그러나, 기존의 해결책에서는, 상이한 슬롯들 에서의 3개의 무선 주파수 신호들은 대응하는 그래디언트를 복원하기 위해 순차적으로 처리될 필요가 있고, 다 음으로 집계가 추가로 수행된다. 본 출원의 해결책을 사용하여, 시간-주파수 리소스들의 사용이 어느 정도 감 소될 수 있고, 그래디언트 집계의 지연이 감소될 수 있다. 단말에 대해 기지국(또는 액세스 네트워크 디바이스에서의 다른 디바이스)에 의해 구성되는 트레이닝 지속기간 은 단말의 실제 트레이닝 지속기간이 아닐 수 있지만, 일반적으로 트레이닝의 각각의 라운드에 대해 단말에 의 해 요구되는 시간의 상한이다. 다시 말해서, 단말은 트레이닝 지속기간 내에 모델 트레이닝의 현재 라운드를 완료하고, 트레이닝 완료 표시를 기지국에 보고한다. 그렇지 않으면, 단말은 모델 트레이닝의 현재 라운드를 종료하고, 다음 트레이닝 지속기간이 도달하기를 기다릴 수 있다. 본 출원에서, 기지국은 상이한 단말들이 모 델 트레이닝의 그래디언트들을 기지국에 동시에 보고할 수 있는 것을 보장하도록 트레이닝 지속기간을 설정한다. 선택적으로, 모델 트레이닝의 것인 그리고 단말에 의해 기지국에 보고되는 그래디언트는 모델 트레 이닝의 다른 라운드에서의 그래디언트, 예를 들어, 모델 트레이닝의 이전 라운드에서의 그래디언트 대신에, 모 델 트레이닝의 현재 라운드에서의 그래디언트라는 점이 더 양호하게 보장될 수 있다. 예를 들어, 트레이닝 지 속기간은 연합 학습에 참여하는 단말들의 컴퓨팅 능력들, 모델의 복잡도 등을 포괄적으로 고려하는 것에 의해 결정되고, 다음으로 각각의 단말에 대해 기지국에 의해 구성되는 글로벌 파라미터일 수 있다. 본 출원에서, 연합 학습에 참여하는 단말들의 수량은 n일 수 있고, 각각의 단말은 에지 노드로서 고려될 수 있 고, 중앙 노드는 기지국일 수 있거나, 또는 OAM일 수 있거나, 또는 코어 네트워크에서의 모듈로서 독립적으로 위치 될 수 있는 등이다. 이러한 것은 제한되지 않는다. 후속 설명들에서, 중앙 노드가 기지국과 독립적인, 다시 말해서, 중앙 노드와 기지국이 2개의 디바이스들인 예가 설명을 위해 사용된다. 도 5a 및 도 5b에 도시되는 바와 같이, 무선 네트워크에서의 AI 모델을 트레이닝하기 위한 방법의 프로시저가 제공된다. 이러한 프로시저는 적어도 다음의 단계들을 포함한다. 단계 501: n개의 단말들 각각이 단말 정보를 기지국에 보고하고, 기지국이 단말 정보를 중앙집중식으로 수집하 고, 수집된 단말 정보를 중앙 노드에 보고함. 본 출원에서, 단말 정보는 다음 중 적어도 하나를 포함한다: 1. 예를 들어, 단말에 의해 지원될 수 있는 최대 송신 전력 및 단말의 안테나 구성을 포함하는, 단말의 통신 능 력. 2. 예를 들어, 중앙 처리 유닛(central processing unit, CPU)의 성능, 그래픽 처리 유닛(graphics processing unit, GPU)의 성능, 저장 공간, 및 전기량을 포함하는, 단말의 컴퓨팅 능력. 3. 예를 들어, 데이터 세트의 크기, 데이터 세트의 분포, 데이터 세트가 완료되었는지, 및 데이터 세트의 레이 블이 완료되었는지인, 단말의 데이터 세트 특징. 선택적으로, 데이터 세트는 백분율에 기초하여 트레이닝 세트, 검증 세트, 및 테스트 세트로 추가로 분할될 수 있다. 예를 들어, 데이터 세트의 60%는 트레이닝 세트이고, 데 이터 세트의 20%는 검증 세트이고, 데이터 세트의 20%는 테스트 세트이다. 트레이닝 세트는 AI 모델을 트레이 닝하기 위해 사용되고, 검증 세트는 트레이닝된 AI 모델을 평가하기 위해 사용되고, 테스트 세트는 트레이닝된 AI 모델을 테스트하기 위해 사용된다는 점이 이해될 수 있다. 선택적으로, 기지국은 단말에 단말 임시 식별자를 할당한다. 이러한 식별자는 셀 무선 네트워크 임시 식별자 (cell radio network temporary identifier, C-RNTI), 다른 임시 식별자 등일 수 있다. 선택적으로, 다른 임 시 식별자들은 코딩 방식을 사용하여, 예를 들어, 표 1 또는 표 2에서 보여지는 시퀀스 번호들을 사용하여 구별 될 수 있다. 표 1"}
{"patent_id": "10-2024-7021349", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "표 2"}
{"patent_id": "10-2024-7021349", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "선택적으로, 단계 501 전에, 프로시저는 다음을 추가로 포함할 수 있다: 기지국이, n개의 단말들에, 단말 정보 를 보고하기 위한 표시를 전송한다. n개의 단말들은 이러한 표시에 기초하여 전술한 단계 501에서의 각각의 단 말 정보를 개별적으로 보고한다. 단계 502: 중앙 노드가 제2 구성 정보를 기지국에 전송함. 선택적으로, 제2 구성 정보는 다음: 연합 학습에 참 여하는 단말들의 리스트, 초기 AI 모델, 트레이닝 지속기간, 단말 수량 임계값, 수송 블록의 크기, 업링크 요건 등 중 적어도 하나를 구성하기 위해 사용된다. 업링크 요건은 단말의 업링크 송신 동안 레이트, 비트 에러 레 이트, 지연 등을 포함할 수 있다. 수송 블록은 MAC 프로토콜 데이터 유닛(protocol data unit, PDU)을 포함하 는 데이터 블록이고, 데이터 블록은 송신 시간 간격(transmission time interval, TTI)에서 송신된다. 본 출원에서, 기지국이 단말에 의해 보고되는 단말 정보를 수신할 때, 단말 정보는 제1 단말 정보라고 지칭될 수 있다는 점이 주목되어야 한다. 기지국은 임시 식별자를 단말에 할당하고, 임시 식별자를 단말 정보에 추가 하여, 제2 단말 정보를 형성한다. 단말 임시 식별자 외에도, 제2 단말 정보는 다음: 단말의 통신 능력, 단말의 컴퓨팅 능력, 또는 단말의 데이터 세트 특징 중 적어도 하나를 추가로 포함할 수 있다. 기지국은 제2 단말 정 보를 중앙 노드에 보고한다. 구체적으로는, 중앙 노드는, 기지국에 의해 보고되는 제2 단말 정보에 기초하여, 연합 학습에 참여하는 단말들 의 리스트를 결정할 수 있다. 예를 들어, 제2 단말 정보는 단말의 통신 능력, 컴퓨팅 능력, 데이터 세트 특징, 임시 식별자 등을 포함한다. 예를 들어, 중앙 노드는 단말의 통신 능력, 컴퓨팅 능력, 데이터 세트 특징 등을 포괄적으로 고려하고, 연합 학습에 참여하는 단말들의 리스트를 생성한다. 예를 들어, 포괄적인 고려 동안, 우 선순위들은 단말의 통신 능력, 컴퓨팅 능력, 데이터 세트 특징 등에 대해 설정될 수 있다. 단말의 데이터 세트 특징의 우선순위는 단말의 통신 능력의 우선순위보다 높고, 단말의 통신 능력의 우선순위는 단말의 컴퓨팅 능력 의 우선순위보다 높다. 추가로, 대응하는 임계값들이 통신 능력, 컴퓨팅 능력, 및 단말의 데이터 세트 특징에 대해 각각 설정되고, 임계값 조건을 충족하지 않는 단말은 연합 학습에 참여하는 단말들의 리스트에 포함되는 것으로 고려되지 않는다. 예를 들어, 중앙 노드는, 통신 능력이 통신 능력 임계값 이상이고, 컴퓨팅 능력이 컴 퓨팅 능력 임계값 이상이고, 데이터 세트 특징이 데이터 세트 특징 요건을 충족하는 단말을 연합 학습에 참여하는 단말로서 사용할 수 있다. 본 출원에서, 중앙 노드는 연합 학습에 참여하는 단말에 대한 트레이닝 지속기간을 구성할 수 있다. 트레이닝 지속기간은, 연합 학습에 참여하는 모든 단말들이 트레이닝 지속기간 내에 로컬 모델 트레이닝을 완료할 수 있 도록 보장하기 위해, 트레이닝 대상 AI 모델의 계산 복잡도 및 단말들의 컴퓨팅 능력들을 고려하여 구성될 필요 가 있다. 그러나, AI 모델의 트레이닝의 전체 효율에 영향을 미치는 것을 회피하기 위해, 트레이닝 지속기간은 과도하게 길게 설정되어서는 안 된다. 본 출원에서, 중앙 노드는 단말 수량 임계값을 결정할 수 있다. 공중 계산이 연합 학습에 도입될 때, 연합 학 습에 참여하는 단말들의 수량은 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 계산하는 정확도에 영향 을 미친다. 따라서, 중앙 노드는 단말 수량 임계값을 설정할 수 있다. 그래디언트들을 보고하는 단말들의 수 량이 단말 수량 임계값 이상일 때, 모델 트레이닝의 현재 라운드에서의 평균 그래디언트가 계산되어 단말에 전 송되거나; 또는 그렇지 않으면, 모델 트레이닝의 이전 라운드에서의 평균 그래디언트가 단말에 전송되거나, 또 는 모델 트레이닝의 이전 라운드에서의 평균 그래디언트가 모델 트레이닝의 현재 라운드에서의 평균 그래디언트 로서 사용되어 단말에 전송될 수 있다. 단계 503: 기지국이 단말에 제1 구성 정보를 전송함. 본 출원에서, 제1 구성 정보는 다음: 트레이닝 지속기간, 보고 순간, 시간-주파수 리소스, 전용 베어러 무선 베 어러(radio bearer, RB) 리소스, 변조 스킴, 초기 AI 모델 등 중 적어도 하나를 구성하기 위해 사용된다. 예를 들어, 기지국은 업링크 요건에 기초하여 시간-주파수 리소스, 전용 베어러 RB 리소스, 변조 스킴 등을 결 정할 수 있다. 본 출원에서, 기지국에 의해 연합 학습에 참여하는 n개의 단말들에 동일한 시간-주파수 리소스, 동일한 보고 순간, 및 동일한 트레이닝 지속기간이 할당된다. 예를 들어, 기지국은 전용 베어러 RB 리소스를 단말에 할당할 필요가 있다. 전용 베어러 RB 리소스는 시그널링 무선 베어러(signal radio bearer, SRB) 리소스, 데이터 무선 베어러(data radio bearer, DRB) 리소스 등일 수 있고, AI 모델의 그래디언트를 송신하기 위해 사용된다. 전용 베어러 RB 리소스는 그래디언트의 송신에만 단지 사용될 수 있고, 다른 데이터의 송신에는 사용될 수 없다. 단말에 대해 기지국에 의해 구성되는 변조 스킴은 위상 시프트 키잉(phase shift keying, PSK), 직교 진폭 변조(quadrature amplitude modulation, QAM), 다른 변조 스킴 등일 수 있다. 변조를 위해 사용될 PSK 또는 QAM의 구체적인 순서는 업링크 요건, 업링크 채널 품질, 기지국의 통신 능력, 단말의 통신 능력 등에 기초하여 결정될 수 있다. 단계 504: 단말이 트레이닝 지속기간 내에 AI 모델을 트레이닝함. 본 출원에서, 제1 구성 정보를 수신할 때, 단말은 AI 모델을 트레이닝할 수 있다. 제1-라운드 트레이닝 프로세 스에서, 단말은 초기 AI 모델을 구체적으로 트레이닝하고, 초기 AI 모델은 단말에 대해 중앙 노드에 의해 구성 된다는 점이 이해될 수 있다. 후속 트레이닝 프로세스에서, 단말은 이전 트레이닝을 통해 획득되는 AI 모델을 구체적으로 트레이닝한다. 본 출원에서의 트레이닝 지속기간은 T에 의해 표현될 수 있다. 연합 학습에 참여하 는 단말에 대해, AI 모델의 트레이닝이 트레이닝 지속기간 T 내에 완료되면, 트레이닝 완료 표시가 기지국에 보 고된다. AI 모델의 트레이닝이 트레이닝 지속기간 T 내에 완료되지 않으면, 모델 트레이닝이 종료된다. 단계 505: 기지국이, 단말에 의해 보고되는 트레이닝 완료 표시에 기초하여, 트레이닝 지속기간 T 내에 AI 모델 의 트레이닝을 완료한 단말들의 수량에 관한 통계를 수집하고, 모델 트레이닝의 현재 라운드를 완료한 단말의 업링크 채널 품질을 측정함. 선택적으로, 모델 트레이닝의 현재 라운드에서의 모델 트레이닝을 완료하지 않은 단말에 대해, 기지국은 대응하는 단말의 업링크 채널 품질을 더 이상 측정하지 않는다. 대응하여, 모델 트레이 닝을 완료하지 않은 단말은 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 기지국에 더 이상 보고 하지 않는다. 예를 들어, 기지국은 카운터를 설정할 수 있다. 트레이닝의 각각의 라운드가 시작될 때, 기지국은 트레이닝 완 료 표시를 보고하는 단말들을 카운트하고, 트레이닝의 각각의 라운드가 종료될 때 카운터를 0으로 재설정한다. 트레이닝 지속기간 T가 종료될 때, 카운터의 카운트가 단말 수량 임계값 이상이면, 기지국은 단말들의 업링크 채널 품질을 측정하거나; 또는 그렇지 않으면, 기지국은 다음 라운드의 트레이닝을 수행하도록 단말들을 트리거 한다. 선택적으로, 하나의 단말에 대해, 기지국이 단말의 업링크 채널 품질을 측정하는 프로세스는 다음을 포함한다: 기지국은 단말로부터 사운딩 참조 신호(sounding reference signal, SRS)를 수신한다. 기지국은, 단말의 업링크 채널 품질을 결정하기 위해, SRS를 측정한다. 단계 506: 기지국이 단말의 업링크 채널 품질에 기초하여 대응하는 단말의 송신 전력을 결정하고; 기지국이 단 말에 제3 구성 정보를 전송함- 제3 구성 정보는 단말의 송신 전력, 보고 순간 등을 구성하기 위해 사용됨 -. 본 출원에서, 제3 구성 정보 및 제1 구성 정보는 집합적으로 하나의 구성 정보라고 지칭될 수 있다. 예를 들어, 기지국은 공중 계산의 에러 요건, 단말의 업링크 채널 품질, 단말에 의해 지원되는 최대 송신 전력, 및 총 전력과 같은 조건들을 포괄적으로 고려하는 것에 의해 단말의 송신 전력을 결정할 수 있다. 공중 계산은 노드들의 동기화에 대한 요건을 갖기 때문에, 과도하게 큰 동기화 에러는 공중 계산의 정확도에 영향을 미친다. 따라서, 기지국은 연합 학습에 참여하는 n개의 단말들에 대해 동일한 보고 순간을 구성할 수 있다. 또한, 채널 은 항상 변경되고 있고, 따라서, 채널 품질 측정을 위한 유효 기간이 존재한다. 따라서, 단말들은 보고 순간에 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트들을 동시에 보고할 필요가 있고, 보고 순간은 채널 품질 측정을 위한 유효 기간 내에 있어야 한다. 선택적으로, 기지국에 의해 단말의 송신 전력을 결정하기 위한 시간 오버헤드들을 고려하여, 채널 품질 측정 결 과의 유효 기간이 초과될 수 있고, 그렇게 함으로써 전력 할당과 업링크 채널 품질 사이의 불일치 등을 야기한 다. 설계에서, 기지국은 단말의 과거 업링크 채널 품질에 기초하여 현재 보고 순간에서의 업링크 채널 품질을 예측하고, 미리 최적 송신 전력을 최적화할 수 있다. 구체적인 예측 방식이 도 6에 도시된다. 기지국은 순간 와 순간 사이의 각각의 단말의 과거 업링크 채널 상태 품질을 수집하고, 순간 에서의 단말의 과거 업링크 채널 품질에 기초하여 순간 에서의 단말의 업링크 채널 품질을 예측하기 시작하고, 순간 에서의 단말의 최적 전력 분포 해결책을 결정하고, 순간 에서의 단말에 최적 전력 분포 해결책을 전달한다. 전력 분포 해결책을 수신한 후, 단말은 모델 트레이닝의 현재 라운드에서의 그래디언트를 즉시 보고한다. 단계 507: 단말이 모델 트레이닝의 현재 라운드에서의 그래디언트를 보고하고, 기지국이 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 계산하고, 기지국이 모델 트레이닝의 현재 라운드에서의 평균 그래디언트에 기 초하여 AI 모델의 파라미터를 업데이트하고, 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 연합 학습 에 참여하는 n개의 단말들에 전달함. 예를 들어, 연합 학습에 참여하는 n개의 단말들이 보고 순간에 트레이닝 지속기간 T 내에 모델 트레이닝을 완료 하면, 단말들은 모델 트레이닝의 현재 라운드에서의 그래디언트들을 기지국에 보고할 수 있다. 모델 트레이닝의 현재 라운드에 있고 n개의 단말들에 의해 보고되는 그래디언트들을 수신할 때, 기지국은 단말들에 의해 보고되는 그 래디언트들에 기초하여 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 계산할 수 있다. 예를 들어, n 개의 단말들이 동일한 시간-주파수 리소스 상에서 모델 트레이닝의 현재 라운드에서의 그래디언트들을 보고하기 때문에, 기지국에 의해 수신된 신호는 n개의 그래디언트들의 공중 중첩을 통해 획득되는 신호이다. 예를 들어, n개의 그래디언트들의 중첩을 통해 획득되는 그래디언트가 Y이면, 기지국은 모델 트레이닝의 현재 라운드에서의 평균 그래디언트가 Y/n과 동일한 것으로 결정할 수 있다. 선택적으로, 본 출원에서, 트레이닝 지속기간 T 내에 모델 트레이닝을 완료하지 않은 단말은 모델 트레이닝의 현재 라운드에서의 그래디언트를 기지국에 더 이상 보 고하지 않는다. 본 출원에서, 각각의 단말은 단말들의 보고 시간 에러를 가능한 한 많이 감소시키기 위해, 보고 순간이 도달할 때 가능한 한 빨리 모델 트레이닝의 현재 라운드에서의 그래디언트를 보고하기 시작해야 한다. 기지국은, 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 복원하기 위해, 공중 계산을 통해 획득되는 수신된 신호를 처 리할 필요가 있다. 예를 들어, 기지국에서의 카운터의 카운트가 단말 수량 임계값 이상이면, 기지국은 모델 트 레이닝의 현재 라운드에서의 로컬로 계산된 평균 그래디언트를 각각의 단말에 전달한다. 그렇지 않으면, 모델 트레이닝의 이전 라운드에서의 평균 그래디언트는 모델 트레이닝의 현재 라운드에서의 평균 그래디언트로서 사 용되고, 각각의 단말에 전달된다. 선택적으로, 연합 학습에 참여하는 단말들이 변경되면, 기지국은 단말에 업데이트된 모델 파라미터를 전송할 수 있다. 단계 504 내지 단계 507은 순환 프로세스라는 점이 이해될 수 있다. 모델 트레이닝의 현재 라운드에서의 평균 그래디언트가 단계 507에서 전달된 후, 단말은 모델 트레이닝의 현재 라운드에서의 평균 그래디언트에 기초하여 AI 모델의 파라미터 및 그래디언트를 업데이트하고, 모델 트레이닝이 트레이닝 지속기간 T 내에 완료되면 트레 이닝 완료 표시를 기지국에 보고할 수 있다. 머신 학습에서, 손실 함수를 감소시키기 위해, AI 모델의 파라미 터들은 그래디언트들의 네거티브 방향을 따라 하강할 필요가 있다, 즉, 그래디언트들이 하강한다는 점이 주목되 어야 한다. 본 출원에서, AI 모델의 파라미터는 모델 트레이닝의 이전 라운드에서의 평균 그래디언트에 기초하 여 모델 트레이닝의 현재 라운드에서의 먼저 업데이트될 수 있고, 다음으로 업데이트된 파라미터에 기초하여 AI 모델의 그래디언트가 업데이트된다, 즉, 그래디언트가 업데이트된다. 단계 508: 기지국이 모델 트레이닝 종료 조건을 결정하고, 단말에 모델 트레이닝 종료 표시를 전송함. 예를 들어, 기지국은 모델 트레이닝 종료 조건을 결정하고, 모델 트레이닝 종료 표시를 각각의 단말에 전송할 수 있다. 모델 트레이닝 종료 조건은 다음 중 적어도 하나일 수 있다: 모델 파라미터가 수렴함; 모델 트레이닝 의 최대 횟수가 도달됨; 모델 트레이닝을 위한 최대 시간이 도달됨; 등. 본 출원에서, 공중 계산이 도입되어, 연합 학습 프로세스에서 통신 지연, 시간-주파수 리소스의 오버헤드, 및 시그널링 오버헤드를 감소시킨다. 구성된 트레이닝 지속기간 및 구성된 보고 순간은 단말들에 의해 그래디언트 들을 보고하는 시간 동기화 에러를 감소시킬 수 있다. 그래디언트가 보고되기 전에, 기지국은 먼저 모든 참여 단말들의 업링크 채널 품질을 측정하고, 각각의 단말의 송신 전력을 최적화하여, 공중 계산의 성능을 개선하고 연합 학습의 트레이닝 효과를 추가로 개선한다. 도 7a 내지 도 7c에 도시되는 바와 같이, 본 출원은 무선 네트워크에서의 AI 모델을 트레이닝하기 위한 방법의 프로시저를 제공한다. 프로시저와 도 5a 및 도 5b에 도시되는 프로시저 사이의 주요 차이는, 프로시저에서, 단 말이 모델 트레이닝의 현재 라운드에서의 그래디언트를 보고하는 송신 전력을, 스스로, 결정하고, 송신 전력이 기지국에 의해 더 이상 구성되지 않는다는 점에 있다. 이러한 프로시저는 적어도 다음의 단계들을 포함한다. 단계 701: n개의 단말들이 단말 정보를 기지국에 보고하고, 기지국이 단말 정보를 중앙 노드에 중앙집중식으로 보고함. 구체적으로, 단말들의 단말 정보를 수신할 때, 단말은 단말에 임시 식별자를 할당하고, 단말 정보에 임시 식별 자를 추가하고, 단말 정보를 중앙 노드에 보고할 수 있다. 도 5a 및 도 5b에서의 설명을 참조한다. 단계 702: 중앙 노드가 제2 구성 정보를 기지국에 전송함- 제2 구성 정보는 다음: 연합 학습에 참여하는 단말들 의 리스트, 초기 AI 모델, 트레이닝 지속기간, 단말 수량 임계값, 채널 상태 정보(channel state information, CSI) 간격, 채널 반전 파라미터, 수송 블록의 크기, 또는 업링크 요건 중 적어도 하나를 구성하기 위해 사용됨 -. 연합 학습에서의 단말들의 리스트, 초기 AI 모델, 트레이닝 지속기간, 단말 수량 임계값, 수송 블록의 크기, 업링크 요건 등에 대해서는, 도 5a 및 도 5b에서의 설명들을 참조한다. 이러한 프로시저는 CSI 간격 및 채널 반전 파라미터에 중점을 둔다. 본 출원에서, CSI는 채널 상태 정보이고, 신호-대-잡음 비율, 도플러 주파수 시프트, 다중경로 지연 확산 등을 포함한다. CSI 간격은 신호-대-잡음 비율 간격, 최대 도플러 주파수 시프트 간격, 및 최대 지연 확산 간격을 포함한다. 신호-대-잡음 비율 간격은 이고, 및 는 각각 신호-대-잡음 비율의 하한 및 상 한이다. 최대 도플러 주파수 시프트 간격은 이고, 및 은 각각 최대 도플러 주파수 시프트 의 하한 및 상한이다. 최대 지연 확산 간격은 이고, 및 은 각각 최대 지연 확산의 하한 및 상한이다. 본 출원에서, 모델 트레이닝의 현재 라운드에서의 그래디언트는 단말의 다운링크 CSI가 CSI 간격을 충족할 때에 만 기지국에 보고될 수 있다. 그렇지 않으면, 그래디언트는 보고되지 않는다. 예를 들어, 단말에 의해 측정되 는 신호-대-잡음 비율, 최대 도플러 주파수 시프트, 및 최대 지연 확산은 각각 , , 및 이다. , , 및 이면, 단말은 보고 순간에 모델 트레이닝의 현재 라 운드에서의 그래디언트를 기지국에 보고할 수 있다. 그렇지 않으면, 단말은 모델 트레이닝의 현재 라운드에서 의 그래디언트를 보고하지 않는다.본 출원에서, 채널 반전 파라미터 는 전력 제어를 위해 사용되는 파라미터이다. 번째 단말의 최대 전력 및 채널 이득은 각각 및 이고, 인 것으로 가정된다. 이러한 경우, 이고, 그래디언트를 보고하기 위해 번째 단말에 의해 사용되는 송신 전력은 이어야 한다. 단계 703: 기지국이 업링크 요건에 기초하여 시간-주파수 리소스, 전용 베어러 RB 리소스, 스케줄링 스킴 등을 결정함. 기지국은 단말에 제1 구성 정보를 전송하고, 제1 구성 정보는 다음: 트레이닝 지속기간, 보고 순간, 시 간-주파수 리소스, 초기 AI 모델, 전용 베어러 RB 리소스, 변조 스킴 등 중 적어도 하나를 구성하기 위해 사용 된다. 본 출원에서, 전용 베어러 RB 리소스는 전용 SRB 리소스, 전용 DRB 리소스 등을 포함한다. 기지국은 업링크 요 건에 기초하여 시간-주파수 리소스, SRB/DRB 리소스, 변조 스킴 등을 결정할 수 있다. 공중 계산의 요건을 충 족하기 위해, 모든 단말들은 동일한 시간-주파수 리소스를 사용할 필요가 있다. 선택적으로, 기지국은 n개의 단말들에 동일한 시간-주파수 리소스들을 할당하고, 시간 다이버시티 또는 주파수 다이버시티를 사용하여 공중 계산의 성능을 개선할 수 있다. 기지국이 n개의 단말들에 대해 동일한 시간-주파수 리소스들을 구성할 때, n개 의 단말들은 시간-주파수 리소스들을 사용하여 구체적인 보고 순간에 모델 트레이닝의 현재 라운드에서의 그래 디언트들을 기지국에 보고한다는 점이 주목되어야 한다. 예를 들어, 기지국이 n개의 단말들에 대해 3개의 시간 -주파수 리소스를 구성하면, n개의 단말들은 3개의 시간-주파수 리소스에 있는 제1 시간-주파수 리소스, 제2 시 간-주파수 리소스, 및 제3 시간-주파수 리소스를 동시를 위해 사용하여 보고 순간에 모델 트레이닝의 현재 라운 드에서의 그래디언트들을 보고한다. 즉, 보고 순간에, n개의 단말들이 모델 트레이닝의 현재 라운드에서의 그 래디언트들을 보고할 때, 동일한 시간-주파수 리소스들이 구체적으로 사용된다. 다른 데이터로부터의 간섭을 회피하기 위해, 기지국은 모델 트레이닝의 현재 라운드에서의 그래디언트를 송신하기 위해 단말에 독립적인 SRB/DRB 리소스를 할당할 수 있다. 기지국은 단말에 대한 PSK 또는 QAM과 같은 변조 스킴을 구성할 수 있다. 변조를 위해 사용될 PSK 또는 QAM의 구체적인 순서는 다음: 업링크 요건, 업링크 채널 품질, 기지국의 통신 능 력, 단말의 통신 능력 등 중 적어도 하나에 기초하여 결정될 수 있다. 단계 704: 단말이 모델을 로컬로 트레이닝함. 본 출원에서, 단말은 제1 구성 정보를 수신할 때 모델 트레이닝을 수행하기 시작할 수 있다. 단말이 트레이닝 지속기간 T 내에 모델 트레이닝을 완료하면, 단말은 트레이닝 완료 표시를 기지국에 보고한다. 단말이 트레이 닝 지속기간 T 내에 모델 트레이닝을 완료하지 않으면, 단말은 모델 트레이닝을 종료한다. 단계 705: 단말이 다운링크 채널의 CSI를 측정하고, 단말의 송신 전력을 결정함. 본 출원에서, 단말의 업링크 채널 및 다운링크 채널에 대해 동일한 주파수 리소스가 구성될 수 있다. 채널 상 호성에 기초하여, 단말은, 업링크 채널의 CSI를 획득하기 위해, 다운링크 채널의 CSI를 측정할 수 있다. 단말 은 업링크 채널의 획득된 CSI가 CSI 간격 내에 있는지를 결정할 수 있다. 업링크 채널의 획득된 CSI가 CSI 간 격 내에 있으면, 송신 전력은 채널 반전 파라미터에 기초하여 결정된다. 전술한 설명으로부터, 채널 반전 파라 미터: 이고, 그래디언트를 보고하기 위해 번째 단말에 의해 사용되는 송신 전력은 이어야 한다는 점이 학습될 수 있다. 본 출원에서, k번째 단말에 대해, 파라미터 는 CSI를 사용하여 획득될 수 있고, k번째 단말의 송신 전력 는 채널 반전 파라미터 를 참조하여 결정될 수 있고, k는 1 이상이고 n 이하 인 양의 정수이다. 단계 706: 단말이 모델 트레이닝의 현재 라운드에서의 그래디언트를 보고하고, 기지국이 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 계산하고, 모델 트레이닝의 현재 라운드에서의 평균 그래디언트에 기초하여 AI 모델의 파라미터를 업데이트하고, 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 연합 학습에 참여하는 각각의 단말에 전송함. 단계 706의 구체적인 프로세스에 대해서는, 전술한 단계 507을 참조한다. 단계 707: 기지국이 모델 종료 조건을 결정하고, 모델 트레이닝 종료 표시를 각각의 단말에 전송함. 설계에서, 단말들이 로컬 트레이닝에서 그래디언트들을 동시에 보고하는 것을 보장하기 위해, 단말들이 구성된 트레이닝 지속기간 및 구성된 보고 순간을 사용할 수 있다는 점이 주목되어야 한다. 예를 들어, 모델 트레이닝 의 이전 라운드에 있고 기지국에 의해 전달되는 평균 그래디언트를 수신한 후에, 단말은 모델 트레이닝의 현재 라운드를 수행하기 시작하고, 모델 트레이닝의 이전 라운드에서의 평균 그래디언트를 수신한 후 T번째 초에 모 델 트레이닝의 현재 라운드에서의 로컬 트레이닝 그래디언트를 보고하도록 구성될 수 있다. T번째 초는 트레이 닝 사이클이고, 모델 트레이닝의 이전 라운드에서의 평균 그래디언트가 수신된 후의 T번째 초는 보고 순간이다. 본 출원에서, 공중 계산이 도입되어, 연합 학습 프로세스에서 통신 지연 및 대역폭 오버헤드들을 감소시키고, 구성된 트레이닝 지속기간 및 구성된 보고 순간은 단말들에 의해 그래디언트들을 보고하는 시간 에러를 감소시 킬 수 있다. 기지국은, 사전구성 방식으로, 트레이닝 지속기간, 전력 조정 해결책, 및 보고 순간과 같은 파라 미터들을 연합 학습에 참여하는 각각의 단말로 전송하고, 단말은 모델을 주기적으로 트레이닝하고 그래디언트를 보고한다. 그래디언트를 보고하기 전에, 단말은 채널 상호성을 사용하여 다운링크 채널 품질의 측정을 통해 단 말의 송신 전력을 최적화하고, 그렇게 함으로써 공중 계산의 성능을 개선하고 연합 학습의 트레이닝 효과를 추 가로 개선한다. 또한, 단말은 트레이닝 지속기간이 종료될 때 트레이닝 그래디언트를 능동적으로 보고하여, 단 말을 스케줄링하기 위한 기지국의 시그널링 오버헤드들이 감소될 수 있다. 도 8a 내지 도 8d에 도시되는 바와 같이, 무선 네트워크에서의 AI 모델을 트레이닝하기 위한 방법의 흐름도가 제공된다. 흐름도와 도 5a 및 도 5b에 도시되는 프로시저 사이의 주요 차이는 연합 학습에 참여하는 단말들이 그룹화되고, 다음으로 중앙 노드가 모델 트레이닝의 현재 라운드에서의 그래디언트들을 중앙집중식으로 보고하 기 위해 구체적인 그룹에서 단말을 스케줄링한다는 점에 있다. 이러한 흐름도는 적어도 다음의 단계들을 포함 한다. 단계 801: 단말들이 단말 정보를 기지국에 보고하고, 기지국이 단말 정보를 중앙 노드에 중앙집중식으로 보고함. 구체적인 설명들에 대해서는, 도 5a 및 도 5b에서의 설명들을 참조한다. 단계 802: 중앙 노드가 제2 구성 정보를 기지국에 전송함. 본 출원에서, 중앙 노드는 연합 학습에 참여하는 단말들의 리스트, 초기 AI 모델, 트레이닝 지속기간, 단말 수 량 임계값, 수송 블록의 크기, 업링크 요건, 그룹 임시 식별자 등을 결정할 수 있다. 제2 구성 정보는 다음: 연합 학습에 참여하는 단말들의 리스트, 초기 AI 모델, 트레이닝 지속기간, 단말 수량 임계값, 수송 블록의 크 기, 업링크 요건, 그룹 임시 식별자 등 중 적어도 하나를 구성하기 위해 사용된다. 본 출원에서, 중앙 노드는 단말의 통신 능력, 단말의 컴퓨팅 능력, 데이터 세트 특징 등을 포괄적으로 고려하고, 각각의 기지국의 서비스 범위 내에 연합 학습에 참여하는 단말들의 리스트를 결정하고, 연합 학습에 참여하는 그리고 기지국의 서비스 범위 내에 있는 단말들을 하나의 그룹으로 그룹화하고, 임시 식별자를 그룹에 할당할 수 있다. 이러한 식별자는 그룹 임시 식별자라고 지칭될 수 있다. 예를 들어, 포괄적인 고려 동안, 중 앙 노드는 단말의 통신 능력, 컴퓨팅 능력, 데이터 세트 특징 등에 대한 우선순위들을 설정할 수 있고, 데이터 세트 특징의 우선순위는 단말의 통신 능력의 우선순위보다 높고, 단말의 통신 능력의 우선순위는 단말의 컴퓨팅 능력의 우선순위보다 높다. 추가로, 대응하는 임계값들이 통신 능력, 컴퓨팅 능력, 및 단말의 데이터 세트 특 징 등에 대해 각각 설정되고, 임계값 조건을 충족하지 않는 단말은 연합 학습에 참여하는 단말들의 리스트에 포 함되는 것으로 고려되지 않는다. 본 출원에서, 연합 학습에 참여하는 그리고 하나의 기지국의 커버리지 영역 내에 있는 단말들은 하나의 그룹으로 그룹화될 수 있다. 예를 들어, 도 8a 내지 도 8d에서의 프로시저에서, 기 지국 1의 커버리지 영역 내의 m개의 단말들이 하나의 그룹으로 그룹화되고, 기지국 N의 커버리지 영역 내의 n개 의 단말들이 또 다른 그룹으로 그룹화되는 예가 사용된다. m 및 n 양자 모두는 양의 정수들이고, m 및 n의 값 들은 동일하거나 또는 상이할 수 있다. 각각의 그룹에 대해, 단말은 그룹 임시 식별자라고 지칭되는 임시 식별 자를 그룹에 할당할 수 있다. 가능한 구현에서, 2개의 그룹이 예로서 사용된다. 각각의 그룹에서의 연합 학습 에 참여하는 단말들의 리스트에 대해서는, 다음의 표 3을 참조한다.표 3"}
{"patent_id": "10-2024-7021349", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "단계 803: 기지국이 단말에 제1 구성 정보를 전송함- 제1 구성 정보는 다음: 초기 AI 모델, 그룹 임시 식별자, 트레이닝 지속기간, 시간-주파수 리소스, 전용 베어러 RB 리소스, 변조 스킴 등 중 적어도 하나를 구성하기 위 해 사용됨 -. 본 출원에서, 기지국은 제1 구성 정보에서의 업링크 요건에 기초하여 시간-주파수 리소스, 전용 베어러 RB 리소 스, 변조 스킴 등을 결정할 수 있다. 대안적으로, 시간-주파수 리소스는 중앙 노드에 의해 구성될 수 있고, 기 지국을 통해 각각의 단말로 전달될 수 있다. 중앙 노드는 동일한 그룹에서의 모든 단말들에 동일한 시간-주파 수 리소스를 할당할 수 있다. 단계 804: 중앙 노드가 스케줄링 표시를 기지국에 전송하고, 기지국이 스케줄링 표시를 단말에 전달함- 스케줄 링 표시는 그룹 임시 식별자를 포함하고, 모델 트레이닝의 현재 라운드에서의 AI 모델의 트레이닝을 수행하기 위해, 그룹 임시 식별자에 대응하는 단말을 스케줄링하기 위해 스케줄링 표시가 사용됨 -. 본 출원에서, 하나의 기지국의 커버리지 영역 내의 n개의 단말들이 하나의 그룹으로 그룹화되는 예가 사용된다. 중앙 노드가 모델 트레이닝의 현재 라운드에서의 모델 트레이닝 및 보고 그래디언트들을 수행하기 위해 구체적 인 그룹을 구체적으로 스케줄링하면, 중앙 노드는 스케줄링 표시를 그룹에 대응하는 기지국에 전송한다. 스케 줄링 표시를 수신할 때, 기지국은 기지국의 커버리지 영역에서 스케줄링 표시를 브로드캐스트할 수 있다. 스케 줄링 표시를 수신할 때, 단말은 스케줄링 표시에서 운반되는 그룹 임시 식별자를, 중앙 노드에 의해 단말에 할 당되는 그룹 임시 식별자와 비교할 수 있다. 이러한 2개가 동일하면, 단말은 모델 트레이닝의 현재 라운드에서 의 AI 모델의 트레이닝을 수행하거나; 또는 그렇지 않으면, 단말은 모델 트레이닝의 현재 라운드에서의 AI 모델 의 트레이닝을 수행하지 않는다. 도 5a 및 도 5b에 도시되는 프로시저와 유사하게, 트레이닝 지속기간이 종료될 때, 단말이 AI 모델의 트레이닝 을 완료하면, 단말은 트레이닝 완료 표시를 기지국에 보고하고; 트레이닝 지속기간이 종료될 때, 단말이 AI 모 델의 트레이닝을 완료하지 못하면, 단말은 또한 AI 모델의 트레이닝을 종료한다. 기지국은 단말에 의해 보고되 는 트레이닝 완료 표시에 기초하여, 트레이닝 지속기간 내에 모델 트레이닝을 완료한 단말들의 수량을 카운트하 고, 단말들의 수량을 중앙 노드에 보고한다. 단계 805: 기지국이 단말에 제3 구성 정보를 전송함- 제3 구성 정보는 각각의 단말의 송신 전력 및 보고 순간을 구성하기 위해 사용됨 -. 선택적으로, 기지국은 단말들에 대해 동일한 보고 순간을 구성한다. 대안적으로, 보 고 순간은 중앙 노드에 의해 구성되고 기지국에 의해 각각의 단말에 전달될 수 있다, 다시 말해서, 제2 구성 정 보는 보고 순간을 구성하기 위해 추가로 사용될 수 있다. 본 출원에서, 제3 구성 정보 및 제1 구성 정보는 하 나의 구성 정보라고 지칭될 수 있다. 예를 들어, 기지국에 의해 송신 전력을 결정하는 프로세스는 다음을 포함한다: 기지국은, 단말의 업링크 채널 품질을 결정하기 위해, 단말로부터의 SRS를 측정한다. 단말은 업링크 채널 품질에 기초하여 단말의 송신 전력 을 결정한다. 상세사항들에 대해서는, 도 5a 및 도 5b에서의 설명들을 참조한다. 상세사항들이 본 명세서에서 다시 설명되지는 않는다. 단계 806: 단말이 모델 트레이닝의 현재 라운드에서의 그래디언트를 중앙 노드에 보고하고, 중앙 노드가 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 계산하고, 모델 트레이닝의 현재 라운드에서의 평균 그래디언 트에 기초하여 AI 모델의 파라미터를 업데이트하고, 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 기 지국을 통해 현재 스케줄링 그룹에서의 단말들에 전달함. 구체적으로, 중앙 노드는 트레이닝 지속기간 내에 AI 모델의 트레이닝을 완료한 단말들의 수량과 단말 수량 임 계값 사이의 값 관계를 비교할 수 있다. 트레이닝 지속기간 내에 AI 모델의 트레이닝을 완료한 단말들의 수량 이 단말 수량 임계값보다 클 때, 중앙 노드는 모델 트레이닝의 현재 라운드에 있고 단말들에 의해 보고되는 그 래디언트도들에 기초하여 모델 트레이닝의 현재 라운드에서의 평균 그래디언트도를 계산하거나; 또는 그렇지 않으면, 중앙 노드는 모델 트레이닝의 이전 라운드에서의 평균 그래디언트를 모델 트레이닝의 현재 라운드에서의 평균 그래디언트로서 사용한다. 중앙 노드는 모델 트레이닝의 현재 라운드에서의 평균 그래디언트에 기초하여 AI 모델의 파라미터를 업데이트하고, 모델 트레이닝의 현재 라운드에서의 평균 그래디언트를 기지국을 통해 전 술한 스케줄링 그룹에서의 단말들에 전송한다. 단계 807: 중앙 노드가 모델 트레이닝 종료 조건을 결정하고, 대응하는 단말에 모델 트레이닝 종료 표시를 전송 함. 모델 트레이닝 종료 조건에 대해서는, 도 5a 및 도 5b 또는 도 7a 내지 도 7c의 프로시저의 설명을 참조한다. 도 8a 내지 도 8d의 프로시저에서, 중앙 노드는 모델 트레이닝 종료 조건이 충족되는지를 결정하고, 도 5a 및 도 5b 또는 도 7a 내지 도 7c에 도시되는 프로시저에서, 기지국은 모델 트레이닝 종료 조건이 충족되는지를 결 정한다는 점에 차이가 있다. 중앙 노드는 먼저 모델 트레이닝 종료 표시를 기지국에 전송할 수 있고, 기지국은 단말에 모델 트레이닝 종료 표시를 전달한다는 점이 이해될 수 있다. 도 8a 내지 도 8d에 도시되는 프로시저에서, 중앙 노드는 모든 그룹들에서의 단말들이 연합 학습 트레이닝에 참 여하도록 스케줄링할 수 있거나, 또는 그룹들의 일부에서의 단말들만이 연합 학습 트레이닝에 참여하도록 스케 줄링할 수 있다. 전술한 설명들에서, 스케줄링 표시를 수신할 때, 단말은 연합 학습 트레이닝을 수행할 수 있 거나; 또는 그렇지 않으면, 단말은 연합 학습 트레이닝을 수행하지 않는다. 선택적으로, 단말은 단말의 모니터 링 파라미터를 추가로 구성할 수 있고, 파라미터는 모니터링 순간 및 모니터링 지속기간을 포함한다. 모니터링 순간이 도달할 때, 구성된 모니터링 지속기간에 기초하여 스케줄링 표시가 모니터링된다. 모니터링 지속기간에 도달할 때, 단말은 슬립 모드에 머무르고, 그렇게 함으로써 단말의 전력을 절약한다. 전술한 해결책에서, 단말들이 그룹화되고, 단말들의 일부만이 연합 학습 트레이닝을 수행하도록 스케줄링될 수 있다. 모든 단말들이 연합 학습에 참여할 필요가 있는 해결책과 비교하여, 단말들의 전력 소비가 감소될 수 있 다. 전술한 방법들에서의 기능들을 구현하기 위해, 기지국, 단말, 및 중앙 노드는 이러한 기능들을 수행하기 위한 대응하는 하드웨어 구조들 및/또는 소프트웨어 모듈들을 포함한다는 점이 이해될 수 있다. 해당 기술에서의 숙 련자는, 본 출원에서 설명되는 예들에서의 유닛들 및 방법 단계들을 참조하여, 본 출원이 하드웨어 또는 하드웨 어와 컴퓨터 소프트웨어의 조합에 의해 구현될 수 있다는 점을 용이하게 인식할 것이다. 기능이 하드웨어에 의 해 수행되는지 또는 컴퓨터 소프트웨어에 의해 구동되는 하드웨어에 의해 수행되는지는 기술적 해결책들의 특정 애플리케이션 시나리오들 및 설계 제약들에 의존한다. 도 9 및 도 10은 본 출원에 따른 가능한 통신 장치들의 구조들의 개략도들이다. 이러한 통신 장치들은 전술한 방법들에서 단말, 기지국, 또는 중앙 노드의 기능들을 구현하도록 구성될 수 있고, 따라서 전술한 방법들의 유 익한 효과들을 또한 구현할 수 있다. 본 출원에서, 단말의 기능들을 구현할 때, 통신 장치는 도 1에 도시되는 단말들(120a 내지 120j) 중 하나일 수 있고; 기지국의 기능들을 구현할 때, 통신 장치는 도 1에 도시되는 기지 국(110a 또는 110b)일 수 있거나, 또는 단말 또는 기지국에서 사용되는 모듈(예를 들어, 칩)일 수 있다. 도 9에 도시되는 바와 같이, 통신 장치는 처리 유닛 및 송수신기 유닛을 포함한다. 통신 장치 는 도 4, 도 5a 및 도 5b, 도 7a 내지 도 7c, 또는 도 8a 내지 도 8d에 도시되는 방법에서의 단말, 기지국, 또는 중앙 노드의 기능들을 구현하도록 구성된다. 통신 장치가 도 4, 도 5a 및 도 5b, 도 7a 내지 도 7c, 또는 도 8a 내지 도 8d에 도시되는 방법에서의 기 지국의 기능들을 구현하도록 구성될 때, 송수신기 유닛은, 연합 학습에 참여하는 단말에 제1 구성 정보를 전송하도록- 제1 구성 정보는 다음: 트레이닝 지속기간, 시간-주파수 리소스 및 보고 순간 중 적어도 하나를 구 성하기 위해 사용되고; 연합 학습에 참여하는 상이한 단말들에 대해 동일한 트레이닝 지속기간, 동일한 시간-주 파수 리소스, 및 동일한 보고 순간이 구성됨 -; 그리고 연합 학습에 참여하는 단말들에 의해 보고되는 그래디언 트들의 공중 중첩을 통해 획득되는 신호를 수신하도록- 그래디언트들은 트레이닝 지속기간 내에 트레이닝이 완 료된 AI 모델의 것인 그리고 시간-주파수 리소스를 사용하여 보고 순간에 단말들에 의해 보고되는 그래디언트들 임 - 구성된다. 처리 유닛은, 제1 구성 정보를 생성하도록, 그리고 단말에 의해 보고되는 그래디언트를 처리하도록 구성된다. 통신 장치가 도 4, 도 5a 및 도 5b, 도 7a 내지 도 7c, 또는 도 8a 내지 도 8d에 도시되는 방법에서의 단 말의 기능들을 구현하도록 구성될 때, 송수신기 유닛은 제2 노드로부터 제1 구성 정보를 수신하도록 구성 되고- 제1 구성 정보는 다음: 트레이닝 지속기간, 시간-주파수 리소스 및 보고 순간 중 적어도 하나를 구성하기위해 사용되고; 연합 학습에 참여하는 상이한 단말들에 대해 동일한 트레이닝 지속기간, 동일한 시간-주파수 리 소스, 및 동일한 보고 순간이 구성됨 -; 처리 유닛은, 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래 디언트를 획득하기 위해, 트레이닝 지속기간 내에 AI 모델을 트레이닝하도록 구성되고; 송수신기 유닛은 시간-주파수 리소스를 사용하여 보고 순간에 모델 트레이닝의 현재 라운드에서의 AI 모델의 그래디언트를 제2 노드에 보고하도록 추가로 구성된다. 통신 장치가 도 4, 도 5a 및 도 5b, 도 7a 내지 도 7c, 또는 도 8a 내지 도 8d에 도시되는 방법에서의 중 앙 노드의 기능들을 구현하도록 구성될 때, 처리 유닛은 제2 구성 정보를 결정하도록 구성되고- 제2 구성 정보는 다음: 연합 학습에 참여하는 단말들의 리스트, 초기 AI 모델, 그룹 임시 식별자, 트레이닝 지속기간, 단 말 수량 임계값, 수송 블록의 크기, 또는 업링크 요건 중 적어도 하나를 구성하기 위해 사용됨 -; 송수신기 유 닛은 제2 구성 정보를 제2 노드에 전송하도록 구성된다. 처리 유닛 및 송수신기 유닛의 더 상세한 설명들에 대해서는, 도 4, 도 5a 및 도 5b, 도 7a 내지 도 7c, 또는 도 8a 내지 도 8d에 도시되는 방법에서의 관련 설명들을 직접 참조한다. 상세사항들이 본 명세서에서 다시 설명되지는 않는다. 도 10에 도시되는 바와 같이, 통신 장치는 프로세서 및 인터페이스 회로를 포함한다. 프로 세서 및 인터페이스 회로는 서로 연결된다. 인터페이스 회로는 송수신기 또는 입력/출력 인 터페이스일 수 있다는 점이 이해될 수 있다. 선택적으로, 통신 장치는, 프로세서에 의해 실행되는 명령어들을 저장하도록, 또는 명령어들을 실행하기 위해 프로세서에 의해 요구되는 입력 데이터를 저장하 도록, 또는 프로세서가 명령어들을 실행한 후에 생성되는 데이터를 저장하도록 구성되는 메모리를 추가로 포함할 수 있다. 통신 장치가 전술한 방법을 구현하도록 구성될 때, 프로세서는 처리 유닛의 기능들을 구현하 도록 구성되고, 인터페이스 회로는 송수신기 유닛의 기능들을 구현하도록 구성된다. 통신 장치가 단말에서 사용되는 칩일 때, 단말에서의 칩은 전술한 방법에서의 단말의 기능들을 구현한다. 단말 에서의 칩은 단말에서의 다른 모듈(예를 들어, 무선 주파수 모듈 또는 안테나)로부터 정보를 수신하거나- 이러 한 정보는 기지국에 의해 단말에 전송됨 -; 또는 단말에서의 칩은 단말에서의 다른 모듈(예를 들어, 무선 주파 수 모듈 또는 안테나)에 정보를 전송한다- 이러한 정보는 단말에 의해 기지국에 전송됨 -. 통신 장치가 기지국에서 사용되는 모듈일 때, 기지국에서의 모듈은 전술한 방법에서의 기지국의 기능들을 구현 한다. 기지국에서의 모듈은 기지국에서의 다른 모듈(예를 들어, 무선 주파수 모듈 또는 안테나)로부터 정보를 수신하거나- 이러한 정보는 단말에 의해 기지국에 전송됨 -; 또는 기지국에서의 모듈은 정보를 기지국에서의 다 른 모듈(예를 들어, 무선 주파수 모듈 또는 안테나)에 전송한다- 이러한 정보는 기지국에 의해 단말에 전송됨 -. 본 명세서에서의 기지국에서의 모듈은 기지국에서의 기저대역 칩일 수 있거나, 또는 DU 또는 다른 모듈일 수 있다. 본 명세서에서의 DU는 개방 무선 액세스 네트워크(open radio access network, O-RAN) 아키텍처에서의 DU일 수 있다. 전술한 장치가 중앙 노드에서 사용되는 모듈일 때, 중앙 노드는 전술한 방법에서의 중앙 노드의 기능들을 구현 한다. 중앙 노드는 중앙 노드에서의 다른 모듈(예를 들어, 무선 주파수 모듈 또는 안테나)로부터 정보를 수신 하거나- 이러한 정보는 기지국에 의해 중앙 노드에 전송됨 -; 또는 중앙 노드에서의 모듈은 중앙 노드에서의 다 른 모듈(예를 들어, 무선 주파수 모듈 또는 안테나)에 정보를 전송한다- 이러한 정보는 중앙 노드에 의해 기지 국에 전송됨 -. 본 명세서에서 중앙 노드에서의 모듈은 중앙 노드에서의 기저대역 칩 또는 다른 모듈일 수 있다. 본 출원에서의 프로세서는 중앙 처리 유닛(central processing unit, CPU)일 수 있거나, 또는 다른 범용 프로세 서, 디지털 신호 프로세서(digital signal processor, DSP), 또는 주문형 집적 회로(application-specific Integrated circuit, ASIC), 필드 프로그램가능 게이트 어레이(field programmable gate array, FPGA), 또는 다른 프로그램가능 로직 디바이스, 트랜지스터 로직 디바이스, 하드웨어 컴포넌트, 또는 이들의 임의의 조합일 수 있다는 점이 이해될 수 있다. 범용 프로세서는 마이크로프로세서 또는 임의의 종래의 프로세서 등일 수 있 다. 본 출원에서의 메모리는 랜덤 액세스 메모리, 플래시 메모리, 판독-전용 메모리, 프로그램가능 판독-전용 메모 리, 소거가능 프로그램가능 판독-전용 메모리, 전기적으로 소거가능 프로그램가능 판독-전용 메모리, 레지스터, 하드 디스크, 이동식 하드 디스크, CD-ROM, 또는 해당 기술에서 잘 알려진 임의의 다른 형태의 저장 매체일 수있다. 예를 들어, 저장 매체는 프로세서에 연결되어, 프로세서가 저장 매체로부터 정보를 판독하거나 또는 정보를 저 장 매체에 기입할 수 있다. 이러한 저장 매체는 대안적으로 프로세서의 컴포넌트일 수 있다. 프로세서 및 저 장 매체는 ASIC에 배치될 수 있다. 또한, ASIC는 기지국 또는 단말 내에 위치될 수 있다. 물론, 프로세서 및 저장 매체는 개별 컴포넌트들로서 기지국 또는 단말 내에 존재할 수 있다. 본 출원에서의 방법들의 일부 또는 전부는 소프트웨어, 하드웨어, 펌웨어, 또는 이들의 임의의 조합에 의해 구 현될 수 있다. 이러한 방법들을 구현하기 위해 소프트웨어가 사용될 때, 이러한 방법들의 일부 또는 전부는 컴 퓨터 프로그램 제품의 형태로 구현될 수 있다. 이러한 컴퓨터 프로그램 제품은 하나 이상의 컴퓨터 프로그램 또는 명령어를 포함한다. 이러한 컴퓨터 프로그램들 또는 명령어들이 컴퓨터 상에서 로딩되고 실행될 때, 본 출원의 실시예들에 따른 프로시저 또는 기능들이 완전히 또는 부분적으로 생성된다. 이러한 컴퓨터는 범용 컴 퓨터, 전용 컴퓨터, 컴퓨터 네트워크, 네트워크 디바이스, 사용자 장비, 코어 네트워크 디바이스, OAM, 또는 다 른 프로그램가능 장치일 수 있다. 이러한 컴퓨터 프로그램들 또는 명령어들은 컴퓨터-판독가능 저장 매체에 저 장될 수 있거나, 또는 컴퓨터-판독가능 저장 매체로부터 또 다른 컴퓨터-판독가능 저장 매체로 송신될 수 있다. 예를 들어, 이러한 컴퓨터 프로그램들 또는 명령어들은 하나의 웹사이트, 컴퓨터, 서버, 또는 데이터 센터로부 터 유선 또는 무선 방식으로 또 다른 웹사이트, 컴퓨터, 서버, 또는 데이터 센터로 송신될 수 있다. 이러한 컴 퓨터-판독가능 저장 매체는 컴퓨터에 의해 액세스가능한 임의의 사용가능한 매체, 또는 데이터 저장 디바이스, 예를 들어, 서버 또는 하나 이상의 사용가능한 매체를 통합하는 데이터 센터일 수 있다. 이러한 사용가능 매체 는 자기 매체, 예를 들어, 플로피 디스크, 하드 디스크, 또는 자기 테이프일 수 있거나; 또는 광학 매체, 예를 들어, 디지털 비디오 디스크일 수 있거나; 또는 반도체 매체, 예를 들어, 고체-상태 드라이브일 수 있다. 이러 한 컴퓨터-판독가능 저장 매체는 휘발성 또는 비-휘발성 저장 매체일 수 있거나, 또는 2개지 타입들의 저장 매 체: 휘발성 저장 매체 및 비-휘발성 저장 매체를 포함할 수 있다. 본 출원에서, 달리 표명되거나 또는 논리 충돌이 없는 한, 상이한 실시예들에서의 용어들 및/또는 설명들은 일 관되고 상호 참조될 수 있으며, 상이한 실시예들에서의 기술적 특징들은 이들의 내부 논리적 관계에 기초하여 조합되어, 새로운 실시예를 형성할 수 있다. 본 출원에서, \"적어도 하나(at least one)\"는 하나 이상을 의미하고, \"복수의(a plurality of)\"는 2개 이상을 의미한다. \"및/또는(and/or)\"은 연관된 객체들 사이의 연관 관계를 설명하고 3개의 관계들이 존재할 수 있다는 점을 표현한다. 예를 들어, A 및/또는 B는 다음의 경우들: A가 단독으로 존재함, A 및 B 양자 모두가 존재함, 및 B가 단독으로 존재함을 표현할 수 있고, A 및 B는 단수 또는 복수일 수 있다. 본 출원의 텍스트 설명들에서, 문자 \"/\"는 연관된 객체들 사이의 \"또는(or)\" 관계를 일반적으로 표시한다. 본 출원에서의 공식에 서, 문자 \"/\"는 연관된 객체들 사이의 \"나눗셈(division)\" 관계를 표시한다. \"A, B, 또는 C 중 적어도 하나를 포함함(including at least one of A, B, or C)\"은 다음을 표시할 수 있다: A를 포함함; B를 포함함; C를 포함 함; A 및 B를 포함함; A 및 C를 포함함; B 및 C를 포함함; 및 A, B 및 C를 포함함. 본 출원에서 사용되는 다양한 숫자들은 단지 설명의 용이함을 위해 구별되고, 본 출원의 범위를 제한하기 위해 사용되지 않는다는 점이 이해될 수 있다. 전술한 프로세스들의 시퀀스 번호들은 실행 시퀀스들을 의미하지는 않고, 프로세스들의 실행 시퀀스들은 프로세스들의 기능들 및 내부 로직에 기초하여 결정되어야 한다.도면 도면1 도면2a 도면2b 도면2c 도면3 도면4 도면5a 도면5b 도면6 도면7a 도면7b 도면7c 도면8a 도면8b 도면8c 도면8d 도면9 도면10"}
{"patent_id": "10-2024-7021349", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 출원에 따른 네트워크 아키텍처의 개략도이다. 도 2a 및 도 2b는 본 출원에 따른 신경망의 개략도들이다. 도 2c는 본 출원에 따른 AI 모델의 개략도이다. 도 3은 본 출원에 따른 연합 학습 트레이닝의 개략도이다. 도 4 및 도 5a 및 도 5b는 본 출원에 따른 개략적인 흐름도들이다. 도 6은 본 출원에 따른 채널 측정의 개략도이다. 도 7a 내지 도 7c 및 도 8a 내지 도 8d는 본 출원에 따른 다른 개략적인 흐름도들이다. 도 9 및 도 10은 본 출원에 따른 장치들의 개략도들이다."}
