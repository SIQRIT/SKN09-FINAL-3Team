{"patent_id": "10-2023-0058299", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0156268", "출원번호": "10-2023-0058299", "발명의 명칭": "집단 지성을 이용한 정보 처리 시스템 및 그 방법", "출원인": "유한회사 닥터다비드", "발명자": "김행철"}}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "특정 주제와 관련해서 수집된 하나 이상의 로우 데이터, 상기 로우 데이터와 관련한 메타 정보, 비교 대상영상, 상기 비교 대상 영상과 관련한 메타 정보 및 단말의 식별 정보를 전송하는 단말; 및상기 단말로부터 전송되는 단말로부터 전송되는 특정 주제와 관련한 하나 이상의 로우 데이터, 상기 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 상기 비교 대상 영상과 관련한 메타 정보 및 단말의 식별 정보를 수신하고, 상기 단말과 연동하여 상기 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하고, 상기 선택라벨링된 로우 데이터에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여 기계 학습 결과를 근거로 상기로우 데이터에 대한 분류값을 생성하고, 상기 생성된 로우 데이터에 대한 분류값, 상기 선택라벨링된 로우 데이터에 대한 정보, 상기 로우 데이터, 상기 로우 데이터와 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교대상 영상과 관련한 메타 정보를 입력값으로 하여 기계 학습을 수행하고 기계 학습 결과를 근거로 상기 로우 데이터에 대응하는 제 1 영상을 생성하고, 상기 생성된 제 1 영상을 상기 단말에 전송하는 서버를 포함하는 집단지성을 이용한 정보 처리 시스템."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 서버는,상기 단말과 연동하여 상기 제 1 영상을 대상으로 추가 선택라벨링을 수행하고, 상기 추가 선택라벨링된 제 1영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여 기계 학습 결과를 근거로 상기 제 1 영상에대한 분류값을 생성하고, 상기 생성된 제 1 영상에 대한 분류값, 상기 추가 선택라벨링된 제 1 영상에 대한 정보, 상기 제 1 영상, 상기 제 1 영상과 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 상기 제 1 영상에 대응하는제 2 영상을 생성하고, 상기 생성된 제 2 영상을 상기 단말에 전송하는 것을 특징으로 하는 집단 지성을 이용한정보 처리 시스템."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 서버는,상기 특정 주제와 관련해서, 복수의 단말로부터 제공되는 복수의 로우 데이터에 대해서, 앞선 선택라벨링 과정,분류 모델 추론 과정, 예측 모델 추론 과정, 생성된 제 1 영상에 대한 추가 선택라벨링 과정, 추가 분류 모델추론 과정 및, 추가 예측 모델 추론 과정을 각각 반복 수행하여, 상기 특정 주제와 관련해서 집단 지성화된 제2 영상을 생성하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 시스템."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "서버에 의해, 단말로부터 전송되는 특정 주제와 관련한 하나 이상의 로우 데이터, 상기 로우 데이터와 관련한메타 정보, 비교 대상 영상, 상기 비교 대상 영상과 관련한 메타 정보 및 단말의 식별 정보를 수신하는 단계;상기 서버에 의해, 상기 단말과 연동하여, 상기 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하는 단계;상기 서버에 의해, 상기 선택라벨링된 로우 데이터에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여, 기계 학습 결과를 근거로 상기 로우 데이터에 대한 분류값을 생성하는 단계;상기 서버에 의해, 상기 생성된 로우 데이터에 대한 분류값, 상기 선택라벨링된 로우 데이터에 대한 정보, 상기로우 데이터, 상기 로우 데이터와 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메공개특허 10-2023-0156268-3-타 정보를 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 상기 로우 데이터에 대응하는 제 1영상을 생성하는 단계;상기 서버에 의해, 상기 생성된 제 1 영상을 상기 단말에 전송하는 단계; 및상기 단말에 의해, 상기 서버로부터 전송되는 제 1 영상을 출력하는 단계를 포함하는 집단 지성을 이용한 정보처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서,상기 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하는 단계는,상기 단말에 표시되는 로우 데이터에 대해서 사용자 입력에 따라 상기 로우 데이터 중 하나 이상의 특정 시점및 하나 이상의 특정 구간 중 적어도 하나에서의 라벨값을 설정하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 4 항에 있어서,상기 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하는 단계는,상기 단말의 영상 표시 영역에 표시되는 로우 데이터에 대해서 상기 단말의 사용자 입력에 따라, 특정 시점 또는 특정 구간에서의 상기 로우 데이터에 포함된 객체의 움직임에 대한 잘된 행위 또는 잘못된 행위에 대해 라벨값을 설정하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 4 항에 있어서,상기 서버에 의해, 상기 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하는 단계 이전에 또는 이후에,상기 단말과 연동하여, 상기 하나 이상의 로우 데이터를 대상으로 계층라벨링을 수행하는 단계를 더 포함하는것을 특징으로 하는 집단 지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서,상기 하나 이상의 로우 데이터를 대상으로 계층라벨링을 수행하는 단계는,상기 단말에 표시되는 로우 데이터에 대해서 미리 설정된 복수의 라벨 분류를 근거로 사용자 입력에 따라 상기로우 데이터 중 다른 특정 시점 및 다른 특정 구간 중 적어도 하나에서의 라벨값을 설정하는 과정; 및상기 로우 데이터를 복수의 서브 로우 데이터로 분할하는 과정을 포함하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 4 항에 있어서,상기 기계 학습 결과를 근거로 상기 로우 데이터에 대한 분류값을 생성하는 단계는,상기 선택라벨링된 로우 데이터에 대한 정보를 미리 설정된 분류 모델의 입력값으로 하여 기계 학습을수행하고, 기계 학습 결과를 근거로 상기 로우 데이터에 대한 분류값을 생성하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 4 항에 있어서,상기 기계 학습 결과를 근거로 상기 로우 데이터에 대응하는 제 1 영상을 생성하는 단계는,공개특허 10-2023-0156268-4-상기 생성된 로우 데이터에 대한 분류값, 상기 선택라벨링된 로우 데이터에 대한 정보, 상기 로우 데이터, 상기로우 데이터와 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 미리 설정된 예측 모델의 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 상기 로우 데이터와 관련한제 1 영상을 생성하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 4 항에 있어서,상기 서버에 의해, 상기 단말과 연동하여, 상기 제 1 영상을 대상으로 추가 선택라벨링을 수행하는 단계;상기 서버에 의해, 상기 추가 선택라벨링된 제 1 영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여, 기계 학습 결과를 근거로 상기 제 1 영상에 대한 분류값을 생성하는 단계;상기 서버에 의해, 상기 생성된 제 1 영상에 대한 분류값, 상기 추가 선택라벨링된 제 1 영상에 대한 정보, 상기 제 1 영상, 상기 제 1 영상과 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 상기 제 1 영상에 대응하는 제 2영상을 생성하는 단계;상기 서버에 의해, 상기 생성된 제 2 영상을 상기 단말에 전송하는 단계;상기 단말에 의해, 상기 서버로부터 전송되는 제 2 영상을 출력하는 단계; 및상기 서버에 의해, 상기 특정 주제와 관련해서, 복수의 단말로부터 제공되는 복수의 로우 데이터에 대해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정, 생성된 제 1 영상에 대한 추가 선택라벨링 과정, 추가 분류 모델 추론 과정 및, 추가 예측 모델 추론 과정을 각각 반복 수행하여, 상기 특정 주제와 관련해서 집단 지성화된 제 2 영상을 생성하는 단계를 더 포함하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서,상기 제 1 영상을 대상으로 추가 선택라벨링을 수행하는 단계는,상기 단말에 의해, 상기 로우 데이터에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서브 로우 데이터에대한 정보를 근거로 상기 제 1 영상을 복수의 서브 영상으로 분할하는 과정;상기 단말에 의해, 상기 분할된 복수의 서브 영상에 대해서 사용자 입력에 따라 잘된 행위에 대한 라벨값 또는잘못된 행위에 대한 라벨값을 각각 입력받는 과정;상기 단말에 의해, 상기 복수의 서브 영상의 순서를 정렬하기 위해서 사용자 입력에 따라 상기 복수의 서브 영상의 순서를 나타내는 라벨값을 입력받는 과정;상기 단말에 의해, 상기 입력된 상기 복수의 서브 영상에 대한 잘된 행위와 잘못된 행위에 대한 라벨값, 상기복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값 및 상기 단말의 식별 정보를 상기 서버에 전송하는과정; 및상기 서버에 의해, 상기 제 1 영상을 대상으로 한 시계열 분할 선택라벨링 기능 수행에 따라, 상기 단말로부터전송되는 상기 복수의 서브 영상에 대한 잘된 행위와 잘못된 행위에 대한 라벨값, 상기 복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값 및 상기 단말의 식별 정보를 수신하는 과정을 포함하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 11 항에 있어서,상기 제 1 영상을 대상으로 추가 선택라벨링을 수행하는 단계는,상기 단말에 의해, 상기 로우 데이터에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서브 로우 데이터에대한 정보를 근거로 상기 제 1 영상을 복수의 서브 영상으로 분할하는 과정;공개특허 10-2023-0156268-5-상기 단말에 의해, 상기 분할된 복수의 서브 영상에 포함된 아바타의 동작 순서에 대한 라벨값을 각각 입력받는과정;상기 단말에 의해, 상기 복수의 서브 영상에 포함된 아바타의 동작에서 신체부위별로 동작 순서를 정렬하기 위해서 사용자 입력에 따라 상기 복수의 서브 영상의 순서를 나타내는 라벨값을 입력받는 과정;상기 단말에 의해, 상기 입력된 상기 복수의 서브 영상에 포함된 아바타의 동작 순서에 대한 라벨값, 상기 복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값 및 상기 단말의 식별 정보를 상기 서버에 전송하는 과정; 및상기 서버에 의해, 상기 제 1 영상을 대상으로 한 신체부위별 선택라벨링 기능 수행에 따라, 상기 단말로부터전송되는 상기 복수의 서브 영상에 포함된 아바타의 동작 순서에 대한 라벨값, 상기 복수의 서브 영상에 대한순서를 정렬하기 위한 라벨값 및 상기 단말의 식별 정보를 수신하는 과정을 포함하는 것을 특징으로 하는 집단지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "특정 주제와 관련해서 실제 인간, 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상 및, 상기 동작 관련 영상과 관련한 메타 정보를 수집하고, 상기 수집된 동작 관련 영상을 실제 로봇의 동작으로 구현하기위해서, 상기 수집된 동작 관련 영상을 로봇 동작 영상으로 재구성하고, 단말과 연동하여 상기 로봇 동작 영상을 대상으로 선택라벨링을 수행하고, 상기 선택라벨링된 로봇 동작 영상에 대한 정보를 근거로 인공지능 기반의기계 학습을 수행하여 기계 학습 결과를 근거로 상기 로봇 동작 영상에 대한 분류값을 생성하고, 상기 생성된로봇 동작 영상에 대한 분류값, 상기 선택라벨링된 로봇 동작 영상에 대한 정보, 상기 로봇 동작 영상, 상기 로봇 동작 영상과 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 근거로상기 로봇 동작 영상에 대응하는 제 1 로보틱스 영상을 생성하고, 상기 생성된 제 1 로보틱스 영상을 상기 단말에 전송하는 서버; 및상기 서버로부터 전송되는 제 1 로보틱스 영상을 출력하는 상기 단말을 포함하는 집단 지성을 이용한 정보 처리시스템."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서,상기 서버는,상기 단말과 연동하여 상기 제 1 로보틱스 영상을 대상으로 추가 선택라벨링을 수행하고, 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여 기계 학습 결과를 근거로상기 제 1 로보틱스 영상에 대한 분류값을 생성하고, 상기 생성된 제 1 로보틱스 영상에 대한 분류값, 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보, 상기 제 1 로보틱스 영상, 상기 제 1 로보틱스 영상과 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 상기 제 1 로보틱스 영상에 대응하는 제 2 로보틱스 영상을 생성하고,상기 생성된 제 2 로보틱스 영상을 상기 단말에 전송하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 시스템."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15 항에 있어서,상기 서버는,상기 특정 주제와 관련해서, 복수의 단말로부터 제공되는 복수의 실제 인간, 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상에 대해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정, 생성된 제 1 로보틱스 영상에 대한 추가 선택라벨링 과정, 추가 분류 모델 추론 과정 및, 추가 예측 모델 추론 과정을 각각 반복 수행하여, 상기 특정 주제와 관련해서 집단 지성화된 제 2 로보틱스 영상을 생성하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 시스템."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "서버에 의해, 특정 주제와 관련해서 실제 인간, 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상 및,공개특허 10-2023-0156268-6-상기 동작 관련 영상과 관련한 메타 정보를 수집하는 단계;상기 서버에 의해, 상기 수집된 동작 관련 영상을 실제 로봇의 동작으로 구현하기 위해서, 상기 수집된 동작 관련 영상을 로봇 동작 영상으로 재구성하는 단계;상기 서버에 의해, 단말과 연동하여, 상기 로봇 동작 영상을 대상으로 선택라벨링을 수행하는 단계;상기 서버에 의해, 상기 선택라벨링된 로봇 동작 영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여, 기계 학습 결과를 근거로 상기 로봇 동작 영상에 대한 분류값을 생성하는 단계;상기 서버에 의해, 상기 생성된 로봇 동작 영상에 대한 분류값, 상기 선택라벨링된 로봇 동작 영상에 대한정보, 상기 로봇 동작 영상, 상기 로봇 동작 영상과 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상영상과 관련한 메타 정보를 근거로 상기 로봇 동작 영상에 대응하는 제 1 로보틱스 영상을 생성하는 단계;상기 서버에 의해, 상기 생성된 제 1 로보틱스 영상을 상기 단말에 전송하는 단계; 및상기 단말에 의해, 상기 서버로부터 전송되는 제 1 로보틱스 영상을 출력하는 단계를 포함하는 집단 지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 17 항에 있어서,상기 서버에 의해, 상기 로봇 동작 영상을 대상으로 선택라벨링을 수행하는 단계 이전에 또는 이후에, 상기 단말과 연동하여, 상기 로봇 동작 영상을 대상으로 계층라벨링을 수행하는 단계를 더 포함하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 17 항에 있어서,상기 서버에 의해, 상기 단말과 연동하여, 상기 제 1 로보틱스 영상을 대상으로 추가 선택라벨링을 수행하는 단계;상기 서버에 의해, 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여, 기계 학습 결과를 근거로 상기 제 1 로보틱스 영상에 대한 분류값을 생성하는 단계;상기 서버에 의해, 상기 생성된 제 1 로보틱스 영상에 대한 분류값, 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보, 상기 제 1 로보틱스 영상, 상기 제 1 로보틱스 영상과 관련한 메타 정보, 상기 비교 대상 영상및 상기 비교 대상 영상과 관련한 메타 정보를 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 상기 제 1 로보틱스 영상에 대응하는 제 2 로보틱스 영상을 생성하는 단계;상기 서버에 의해, 상기 생성된 제 2 로보틱스 영상을 상기 단말에 전송하는 단계;상기 단말에 의해, 상기 서버로부터 전송되는 제 2 로보틱스 영상을 출력하는 단계; 및상기 서버에 의해, 상기 특정 주제와 관련해서, 복수의 단말로부터 제공되는 복수의 실제 인간, 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상에 대해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델추론 과정, 생성된 제 1 로보틱스 영상에 대한 추가 선택라벨링 과정, 추가 분류 모델 추론 과정 및, 추가 예측모델 추론 과정을 각각 반복 수행하여, 상기 특정 주제와 관련해서 집단 지성화된 제 2 로보틱스 영상을 생성하는 단계를 더 포함하는 것을 특징으로 하는 집단 지성을 이용한 정보 처리 방법."}
{"patent_id": "10-2023-0058299", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 집단 지성을 이용한 정보 처리 시스템 및 그 방법을 개시한다. 즉, 본 발명은 사용자로부터 제공되는 특정 콘텐츠와 관련한 하나 이상의 로우 데이터에 대해서 라벨링을 수행하고, 라벨링된 로우 데이터에 대해서 미 리 설정된 분류 모델 및 예측 모델을 통해 학습 기능을 수행하고, 예측 모델의 출력값인 제 1 영상에 대해서 추 가 라벨링을 수행하고, 추가 라벨링된 제 1 영상에 대해서 분류 모델 및 예측 모델을 통해 추가 학습 기능을 수 행하여 제 2 영상을 출력함으로써, 로우 데이터와 관련한 아바타 및/또는 아이템을 사용자에게 제공하고, 로우 데이터에 대한 라벨링을 통해 인공지능의 추론 능력을 향상시킬 수 있다."}
{"patent_id": "10-2023-0058299", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 집단 지성을 이용한 정보 처리 시스템 및 그 방법에 관한 것으로서, 특히 사용자로부터 제공되는 특 정 콘텐츠와 관련한 하나 이상의 로우 데이터에 대해서 라벨링을 수행하고, 라벨링된 로우 데이터에 대해서 미 리 설정된 분류 모델 및 예측 모델을 통해 학습 기능을 수행하고, 예측 모델의 출력값인 제 1 영상에 대해서 추 가 라벨링을 수행하고, 추가 라벨링된 제 1 영상에 대해서 분류 모델 및 예측 모델을 통해 추가 학습 기능을 수 행하여 제 2 영상을 출력하는 집단 지성을 이용한 정보 처리 시스템 및 그 방법을 제공하는 데 있다."}
{"patent_id": "10-2023-0058299", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "집단 지성은 집단 구성원들이 서로 협력하거나 경쟁하여 쌓은 지적 능력의 결과로 얻어진 지성. 또는 그러한 집 단적 능력을 나타낸다. 이러한 집단 지성은 아바타, 아이템, 로보틱스 등의 정보 데이터베이스 기술의 발전에 따라, 새로운 빅데이터 기반의 지식 서비스와의 연결에 대한 필요성이 존재한다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제10-1859198호 [제목: 집단지성 서비스 시스템 및 그 방법]"}
{"patent_id": "10-2023-0058299", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 사용자로부터 제공되는 특정 콘텐츠와 관련한 하나 이상의 로우 데이터에 대해서 라벨링을 수 행하고, 라벨링된 로우 데이터에 대해서 미리 설정된 분류 모델 및 예측 모델을 통해 학습 기능을 수행하고, 예 측 모델의 출력값인 제 1 영상에 대해서 추가 라벨링을 수행하고, 추가 라벨링된 제 1 영상에 대해서 분류 모델 및 예측 모델을 통해 추가 학습 기능을 수행하여 제 2 영상을 출력하는 집단 지성을 이용한 정보 처리 시스템 및 그 방법을 제공하는 데 있다. 본 발명의 다른 목적은 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상을 로봇 동작 영상으로 재구성 하고, 재구성된 로봇 동작 영상에 대해서 라벨링을 수행하고, 라벨링된 로봇 동작 영상에 대해서 미리 설정된 분류 모델 및 예측 모델을 통해 학습 기능을 수행하고, 학습 기능 수행 결과인 제 1 로보틱스 영상에 대해서 추 가 라벨링을 수행하고, 추가 라벨링된 제 1 로보틱스 영상에 대해서 분류 모델 및 예측 모델을 통해 추가 학습 기능을 수행하여 제 2 로보틱스 영상을 출력하는 집단 지성을 이용한 정보 처리 시스템 및 그 방법을 제공하는 데 있다."}
{"patent_id": "10-2023-0058299", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시예에 따른 집단 지성을 이용한 정보 처리 시스템은 특정 주제와 관련해서 수집된 하나 이상의 로 우 데이터, 상기 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 상기 비교 대상 영상과 관련한 메타 정보 및 단말의 식별 정보를 전송하는 단말; 및 상기 단말로부터 전송되는 단말로부터 전송되는 특정 주제와 관련한 하나 이상의 로우 데이터, 상기 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 상기 비교 대상 영상과 관련 한 메타 정보 및 단말의 식별 정보를 수신하고, 상기 단말과 연동하여 상기 하나 이상의 로우 데이터를 대상으 로 선택라벨링을 수행하고, 상기 선택라벨링된 로우 데이터에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여 기계 학습 결과를 근거로 상기 로우 데이터에 대한 분류값을 생성하고, 상기 생성된 로우 데이터에 대 한 분류값, 상기 선택라벨링된 로우 데이터에 대한 정보, 상기 로우 데이터, 상기 로우 데이터와 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 입력값으로 하여 기계 학습을 수행하 고 기계 학습 결과를 근거로 상기 로우 데이터에 대응하는 제 1 영상을 생성하고, 상기 생성된 제 1 영상을 상 기 단말에 전송하는 서버를 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 서버는, 상기 단말과 연동하여 상기 제 1 영상을 대상으로 추가 선택라벨링을 수행하고, 상기 추가 선택라벨링된 제 1 영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여 기 계 학습 결과를 근거로 상기 제 1 영상에 대한 분류값을 생성하고, 상기 생성된 제 1 영상에 대한 분류값, 상기 추가 선택라벨링된 제 1 영상에 대한 정보, 상기 제 1 영상, 상기 제 1 영상과 관련한 메타 정보, 상기 비교 대 상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과 를 근거로 상기 제 1 영상에 대응하는 제 2 영상을 생성하고, 상기 생성된 제 2 영상을 상기 단말에 전송할 수 있다. 본 발명과 관련된 일 예로서 상기 서버는, 상기 특정 주제와 관련해서, 복수의 단말로부터 제공되는 복수의 로 우 데이터에 대해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정, 생성된 제 1 영상에대한 추가 선택라벨링 과정, 추가 분류 모델 추론 과정 및, 추가 예측 모델 추론 과정을 각각 반복 수행하여, 상기 특정 주제와 관련해서 집단 지성화된 제 2 영상을 생성할 수 있다. 본 발명의 실시예에 따른 집단 지성을 이용한 정보 처리 방법은 서버에 의해, 단말로부터 전송되는 특정 주제와 관련한 하나 이상의 로우 데이터, 상기 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 상기 비교 대상 영상 과 관련한 메타 정보 및 단말의 식별 정보를 수신하는 단계; 상기 서버에 의해, 상기 단말과 연동하여, 상기 하 나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하는 단계; 상기 서버에 의해, 상기 선택라벨링된 로우 데 이터에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여, 기계 학습 결과를 근거로 상기 로우 데이터 에 대한 분류값을 생성하는 단계; 상기 서버에 의해, 상기 생성된 로우 데이터에 대한 분류값, 상기 선택라벨링 된 로우 데이터에 대한 정보, 상기 로우 데이터, 상기 로우 데이터와 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거 로 상기 로우 데이터에 대응하는 제 1 영상을 생성하는 단계; 상기 서버에 의해, 상기 생성된 제 1 영상을 상기 단말에 전송하는 단계; 및 상기 단말에 의해, 상기 서버로부터 전송되는 제 1 영상을 출력하는 단계를 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하는 단계는, 상기 단 말에 표시되는 로우 데이터에 대해서 사용자 입력에 따라 상기 로우 데이터 중 하나 이상의 특정 시점 및 하나 이상의 특정 구간 중 적어도 하나에서의 라벨값을 설정할 수 있다. 본 발명과 관련된 일 예로서 상기 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하는 단계는, 상기 단 말의 영상 표시 영역에 표시되는 로우 데이터에 대해서 상기 단말의 사용자 입력에 따라, 특정 시점 또는 특정 구간에서의 상기 로우 데이터에 포함된 객체의 움직임에 대한 잘된 행위 또는 잘못된 행위에 대해 라벨값을 설 정할 수 있다. 본 발명과 관련된 일 예로서 상기 서버에 의해, 상기 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하 는 단계 이전에 또는 이후에, 상기 단말과 연동하여, 상기 하나 이상의 로우 데이터를 대상으로 계층라벨링을 수행하는 단계를 더 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 하나 이상의 로우 데이터를 대상으로 계층라벨링을 수행하는 단계는, 상기 단 말에 표시되는 로우 데이터에 대해서 미리 설정된 복수의 라벨 분류를 근거로 사용자 입력에 따라 상기 로우 데 이터 중 다른 특정 시점 및 다른 특정 구간 중 적어도 하나에서의 라벨값을 설정하는 과정; 및 상기 로우 데이 터를 복수의 서브 로우 데이터로 분할하는 과정을 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 기계 학습 결과를 근거로 상기 로우 데이터에 대한 분류값을 생성하는 단계는, 상기 선택라벨링된 로우 데이터에 대한 정보를 미리 설정된 분류 모델의 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 상기 로우 데이터에 대한 분류값을 생성할 수 있다. 본 발명과 관련된 일 예로서 상기 기계 학습 결과를 근거로 상기 로우 데이터에 대응하는 제 1 영상을 생성하는 단계는, 상기 생성된 로우 데이터에 대한 분류값, 상기 선택라벨링된 로우 데이터에 대한 정보, 상기 로우 데이 터, 상기 로우 데이터와 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 미리 설정된 예측 모델의 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 상기 로우 데이터와 관련한 제 1 영상을 생성할 수 있다. 본 발명과 관련된 일 예로서 상기 서버에 의해, 상기 단말과 연동하여, 상기 제 1 영상을 대상으로 추가 선택라 벨링을 수행하는 단계; 상기 서버에 의해, 상기 추가 선택라벨링된 제 1 영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여, 기계 학습 결과를 근거로 상기 제 1 영상에 대한 분류값을 생성하는 단계; 상기 서버에 의해, 상기 생성된 제 1 영상에 대한 분류값, 상기 추가 선택라벨링된 제 1 영상에 대한 정보, 상기 제 1 영상, 상기 제 1 영상과 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보 를 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 상기 제 1 영상에 대응하는 제 2 영상을 생성하는 단계; 상기 서버에 의해, 상기 생성된 제 2 영상을 상기 단말에 전송하는 단계; 상기 단말에 의해, 상 기 서버로부터 전송되는 제 2 영상을 출력하는 단계; 및 상기 서버에 의해, 상기 특정 주제와 관련해서, 복수의 단말로부터 제공되는 복수의 로우 데이터에 대해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추 론 과정, 생성된 제 1 영상에 대한 추가 선택라벨링 과정, 추가 분류 모델 추론 과정 및, 추가 예측 모델 추론 과정을 각각 반복 수행하여, 상기 특정 주제와 관련해서 집단 지성화된 제 2 영상을 생성하는 단계를 더 포함할 수 있다.본 발명과 관련된 일 예로서 상기 제 1 영상을 대상으로 추가 선택라벨링을 수행하는 단계는, 상기 단말에 의해, 상기 로우 데이터에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서브 로우 데이터에 대한 정보를 근거로 상기 제 1 영상을 복수의 서브 영상으로 분할하는 과정; 상기 단말에 의해, 상기 분할된 복수의 서브 영 상에 대해서 사용자 입력에 따라 잘된 행위에 대한 라벨값 또는 잘못된 행위에 대한 라벨값을 각각 입력받는 과 정; 상기 단말에 의해, 상기 복수의 서브 영상의 순서를 정렬하기 위해서 사용자 입력에 따라 상기 복수의 서브 영상의 순서를 나타내는 라벨값을 입력받는 과정; 상기 단말에 의해, 상기 입력된 상기 복수의 서브 영상에 대 한 잘된 행위와 잘못된 행위에 대한 라벨값, 상기 복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값 및 상 기 단말의 식별 정보를 상기 서버에 전송하는 과정; 및 상기 서버에 의해, 상기 제 1 영상을 대상으로 한 시계 열 분할 선택라벨링 기능 수행에 따라, 상기 단말로부터 전송되는 상기 복수의 서브 영상에 대한 잘된 행위와 잘못된 행위에 대한 라벨값, 상기 복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값 및 상기 단말의 식별 정보를 수신하는 과정을 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 제 1 영상을 대상으로 추가 선택라벨링을 수행하는 단계는, 상기 단말에 의해, 상기 로우 데이터에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서브 로우 데이터에 대한 정보를 근거로 상기 제 1 영상을 복수의 서브 영상으로 분할하는 과정; 상기 단말에 의해, 상기 분할된 복수의 서브 영 상에 포함된 아바타의 동작 순서에 대한 라벨값을 각각 입력받는 과정; 상기 단말에 의해, 상기 복수의 서브 영 상에 포함된 아바타의 동작에서 신체부위별로 동작 순서를 정렬하기 위해서 사용자 입력에 따라 상기 복수의 서 브 영상의 순서를 나타내는 라벨값을 입력받는 과정; 상기 단말에 의해, 상기 입력된 상기 복수의 서브 영상에 포함된 아바타의 동작 순서에 대한 라벨값, 상기 복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값 및 상 기 단말의 식별 정보를 상기 서버에 전송하는 과정; 및 상기 서버에 의해, 상기 제 1 영상을 대상으로 한 신체 부위별 선택라벨링 기능 수행에 따라, 상기 단말로부터 전송되는 상기 복수의 서브 영상에 포함된 아바타의 동 작 순서에 대한 라벨값, 상기 복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값 및 상기 단말의 식별 정보 를 수신하는 과정을 포함할 수 있다. 본 발명의 실시예에 따른 집단 지성을 이용한 정보 처리 시스템은 특정 주제와 관련해서 실제 인간, 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상 및, 상기 동작 관련 영상과 관련한 메타 정보를 수집하고, 상 기 수집된 동작 관련 영상을 실제 로봇의 동작으로 구현하기 위해서, 상기 수집된 동작 관련 영상을 로봇 동작 영상으로 재구성하고, 단말과 연동하여 상기 로봇 동작 영상을 대상으로 선택라벨링을 수행하고, 상기 선택라벨 링된 로봇 동작 영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여 기계 학습 결과를 근거로 상 기 로봇 동작 영상에 대한 분류값을 생성하고, 상기 생성된 로봇 동작 영상에 대한 분류값, 상기 선택라벨링된 로봇 동작 영상에 대한 정보, 상기 로봇 동작 영상, 상기 로봇 동작 영상과 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 근거로 상기 로봇 동작 영상에 대응하는 제 1 로보틱스 영 상을 생성하고, 상기 생성된 제 1 로보틱스 영상을 상기 단말에 전송하는 서버; 및 상기 서버로부터 전송되는 제 1 로보틱스 영상을 출력하는 상기 단말을 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 서버는, 상기 단말과 연동하여 상기 제 1 로보틱스 영상을 대상으로 추가 선 택라벨링을 수행하고, 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여 기계 학습 결과를 근거로 상기 제 1 로보틱스 영상에 대한 분류값을 생성하고, 상기 생성된 제 1 로보틱스 영상에 대한 분류값, 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보, 상기 제 1 로보틱스 영상, 상기 제 1 로보틱스 영상과 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메 타 정보를 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 상기 제 1 로보틱스 영상에 대응하 는 제 2 로보틱스 영상을 생성하고, 상기 생성된 제 2 로보틱스 영상을 상기 단말에 전송할 수 있다. 본 발명과 관련된 일 예로서 상기 서버는, 상기 특정 주제와 관련해서, 복수의 단말로부터 제공되는 복수의 실 제 인간, 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상에 대해서, 앞선 선택라벨링 과정, 분류 모 델 추론 과정, 예측 모델 추론 과정, 생성된 제 1 로보틱스 영상에 대한 추가 선택라벨링 과정, 추가 분류 모델 추론 과정 및, 추가 예측 모델 추론 과정을 각각 반복 수행하여, 상기 특정 주제와 관련해서 집단 지성화된 제 2 로보틱스 영상을 생성할 수 있다. 본 발명의 실시예에 따른 집단 지성을 이용한 정보 처리 방법은 서버에 의해, 특정 주제와 관련해서 실제 인간, 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상 및, 상기 동작 관련 영상과 관련한 메타 정보를 수 집하는 단계; 상기 서버에 의해, 상기 수집된 동작 관련 영상을 실제 로봇의 동작으로 구현하기 위해서, 상기 수집된 동작 관련 영상을 로봇 동작 영상으로 재구성하는 단계; 상기 서버에 의해, 단말과 연동하여, 상기 로봇 동작 영상을 대상으로 선택라벨링을 수행하는 단계; 상기 서버에 의해, 상기 선택라벨링된 로봇 동작 영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여, 기계 학습 결과를 근거로 상기 로봇 동작 영상에 대 한 분류값을 생성하는 단계; 상기 서버에 의해, 상기 생성된 로봇 동작 영상에 대한 분류값, 상기 선택라벨링된 로봇 동작 영상에 대한 정보, 상기 로봇 동작 영상, 상기 로봇 동작 영상과 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 근거로 상기 로봇 동작 영상에 대응하는 제 1 로보틱스 영 상을 생성하는 단계; 상기 서버에 의해, 상기 생성된 제 1 로보틱스 영상을 상기 단말에 전송하는 단계; 및 상 기 단말에 의해, 상기 서버로부터 전송되는 제 1 로보틱스 영상을 출력하는 단계를 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 서버에 의해, 상기 로봇 동작 영상을 대상으로 선택라벨링을 수행하는 단계 이전에 또는 이후에, 상기 단말과 연동하여, 상기 로봇 동작 영상을 대상으로 계층라벨링을 수행하는 단계를 더 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 서버에 의해, 상기 단말과 연동하여, 상기 제 1 로보틱스 영상을 대상으로 추 가 선택라벨링을 수행하는 단계; 상기 서버에 의해, 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보를 근거로 인공지능 기반의 기계 학습을 수행하여, 기계 학습 결과를 근거로 상기 제 1 로보틱스 영상에 대한 분류 값을 생성하는 단계; 상기 서버에 의해, 상기 생성된 제 1 로보틱스 영상에 대한 분류값, 상기 추가 선택라벨링 된 제 1 로보틱스 영상에 대한 정보, 상기 제 1 로보틱스 영상, 상기 제 1 로보틱스 영상과 관련한 메타 정보, 상기 비교 대상 영상 및 상기 비교 대상 영상과 관련한 메타 정보를 입력값으로 하여 기계 학습을 수행하고, 기 계 학습 결과를 근거로 상기 제 1 로보틱스 영상에 대응하는 제 2 로보틱스 영상을 생성하는 단계; 상기 서버에 의해, 상기 생성된 제 2 로보틱스 영상을 상기 단말에 전송하는 단계; 상기 단말에 의해, 상기 서버로부터 전송 되는 제 2 로보틱스 영상을 출력하는 단계; 및 상기 서버에 의해, 상기 특정 주제와 관련해서, 복수의 단말로부 터 제공되는 복수의 실제 인간, 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상에 대해서, 앞선 선 택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정, 생성된 제 1 로보틱스 영상에 대한 추가 선택라벨 링 과정, 추가 분류 모델 추론 과정 및, 추가 예측 모델 추론 과정을 각각 반복 수행하여, 상기 특정 주제와 관 련해서 집단 지성화된 제 2 로보틱스 영상을 생성하는 단계를 더 포함할 수 있다."}
{"patent_id": "10-2023-0058299", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 사용자로부터 제공되는 특정 콘텐츠와 관련한 하나 이상의 로우 데이터에 대해서 라벨링을 수행하고, 라벨링된 로우 데이터에 대해서 미리 설정된 분류 모델 및 예측 모델을 통해 학습 기능을 수행하고, 예측 모델 의 출력값인 제 1 영상에 대해서 추가 라벨링을 수행하고, 추가 라벨링된 제 1 영상에 대해서 분류 모델 및 예 측 모델을 통해 추가 학습 기능을 수행하여 제 2 영상을 출력함으로써, 로우 데이터와 관련한 아바타 및/또는 아이템을 사용자에게 제공하고, 로우 데이터에 대한 라벨링을 통해 인공지능의 추론 능력을 향상시킬 수 있는 효과가 있다. 또한, 본 발명은 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상을 로봇 동작 영상으로 재구성하고, 재구성된 로봇 동작 영상에 대해서 라벨링을 수행하고, 라벨링된 로봇 동작 영상에 대해서 미리 설정된 분류 모 델 및 예측 모델을 통해 학습 기능을 수행하고, 학습 기능 수행 결과인 제 1 로보틱스 영상에 대해서 추가 라벨 링을 수행하고, 추가 라벨링된 제 1 로보틱스 영상에 대해서 분류 모델 및 예측 모델을 통해 추가 학습 기능을 수행하여 제 2 로보틱스 영상을 출력함으로써, 인공지능에 따른 결과물을 인공지능의 분류 모델 및 예측 모델에 반복적으로 적용하여 인공지능의 학습 능력을 향상시킬 수 있는 효과가 있다."}
{"patent_id": "10-2023-0058299", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명에서 사용되는 기술적 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려 는 의도가 아님을 유의해야 한다. 또한, 본 발명에서 사용되는 기술적 용어는 본 발명에서 특별히 다른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 의미로 해석되어야 하며, 과도하게 포괄적인 의미로 해석되거나, 과도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 발명에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에 는 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용 되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 발명에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한 복수의 표현을 포함한다. 본 발명에서 \"구성된다\" 또는 \"포함한다\" 등의 용어는 발명에 기재된 여러 구성 요소들 또는 여러 단계를 반드 시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수 도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 발명에서 사용되는 제 1, 제 2 등과 같이 서수를 포함하는 용어는 구성 요소들을 설명하는데 사용될 수 있지만, 구성 요소들은 용어들에 의해 한정되어서는 안 된다. 용어들은 하나의 구성 요소를 다른 구성 요소 로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제 1 구성 요소는 제 2 구성 요소로 명명될 수 있고, 유사하게 제 2 구성 요소도 제 1 구성 요소로 명명될 수 있다. 이하, 첨부된 도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, 도면 부호에 관계없이 동일 하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 발명의 사상을 쉽게 이해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아니 됨을 유의해야 한다. 도 1은 본 발명의 실시예에 따른 집단 지성을 이용한 정보 처리 시스템의 구성을 나타낸 블록도이다. 도 1에 도시한 바와 같이, 집단 지성을 이용한 정보 처리 시스템은 단말 및 서버로 구성된다. 도 1에 도시된 집단 지성을 이용한 정보 처리 시스템의 구성 요소 모두가 필수 구성 요소인 것은 아니며, 도 1 에 도시된 구성 요소보다 많은 구성 요소에 의해 집단 지성을 이용한 정보 처리 시스템이 구현될 수도 있고, 그보다 적은 구성 요소에 의해서도 집단 지성을 이용한 정보 처리 시스템이 구현될 수도 있다. 상기 단말은 스마트폰(Smart Phone), 휴대 단말기(Portable Terminal), 이동 단말기(Mobile Terminal), 폴더블 단말기(Foldable Terminal), 개인 정보 단말기(Personal Digital Assistant: PDA), PMP(Portable Multimedia Player) 단말기, 텔레매틱스(Telematics) 단말기, 내비게이션(Navigation) 단말기, 개인용 컴퓨터 (Personal Computer), 노트북 컴퓨터, 슬레이트 PC(Slate PC), 태블릿 PC(Tablet PC), 울트라북(ultrabook), 웨어러블 디바이스(Wearable Device, 예를 들어, 워치형 단말기(Smartwatch), 글래스형 단말기(Smart Glass), HMD(Head Mounted Display) 등 포함), 와이브로(Wibro) 단말기, IPTV(Internet Protocol Television) 단말기, 스마트 TV, 디지털방송용 단말기, AVN(Audio Video Navigation) 단말기, A/V(Audio/Video) 시스템, 플렉시블 단말기(Flexible Terminal), 디지털 사이니지 장치, VR 시뮬레이터, 로봇(robot) 등과 같은 다양한 단말기에 적 용될 수 있다. 상기 서버는 클라우드 컴퓨팅(cloud computing), 그리드 컴퓨팅(grid computing), 서버 기반 컴퓨팅 (server-based computing), 유틸리티 컴퓨팅(utility computing), 네트워크 컴퓨팅(network computing), 퀀텀 클라우드 컴퓨팅(quantum cloud computing), 웹 서버, 데이터베이스 서버, 프록시 서버 등의 형태로 구현될 수 있다. 또한, 상기 서버에는 네트워크 부하 분산 메커니즘, 내지 해당 서버가 인터넷 또는 다른 네트 워크상에서 동작할 수 있도록 하는 다양한 소프트웨어 중 하나 이상이 설치될 수 있으며, 이를 통해 컴퓨터화된 시스템으로 구현될 수 있다. 또한, 네트워크는 http 네트워크일 수 있으며, 전용 회선(private line), 인트라넷 또는 임의의 다른 네트워크일 수 있다. 나아가, 상기 단말 및 상기 서버 간의 연결은 데이터가 임의 의 해커 또는 다른 제3자에 의한 공격을 받지 않도록 보안 네트워크로 연결될 수 있다. 또한, 상기 서버는 복수의 데이터베이스 서버를 포함할 수 있으며, 이러한 데이터베이스 서버가 분산 데이터베이스 서버 아키텍처 를 비롯한 임의의 유형의 네트워크 연결을 통해 상기 서버와 별도로 연결되는 방식으로 구현될 수 있다. 상기 단말 및 상기 서버 각각은 다른 단말들과의 통신 기능을 수행하기 위한 통신부(미도시), 다양한 정보 및 프로그램(또는 애플리케이션)을 저장하기 위한 저장부(미도시), 다양한 정보 및 프로그램 실행 결과를 표시하기 위한 표시부(미도시), 상기 다양한 정보 및 프로그램 실행 결과에 대응하는 음성 정보를 출력하기 위 한 음성 출력부(미도시), 각 단말의 다양한 구성 요소 및 기능을 제어하기 위한 제어부(미도시) 등을 포함할 수 있다. 상기 단말은 상기 서버 등과 통신한다. 이때, 상기 단말은 해당 서버에서 제공하는 전용 앱을 통해 로우 데이터 수집 기능, 정보/영상에 대한 계층라벨링 기능, 정보/영상에 대한 선택라벨링 기능, 정 보/영상에 대한 시계열 분할 선택라벨링 기능, 정보/영상에 대한 신체부위별 선택라벨링 기능 등을 수행하기 위 한 사용자(또는 특정 분야의 전문가)가 소지한 단말일 수 있다. 또한, 상기 단말은 상기 서버와의 연동에 의해, 상기 서버에서 제공하는 전용 앱 및/또는 웹 사 이트를 통해 로우 데이터 수집 기능, 정보/영상에 대한 계층라벨링 기능, 정보/영상에 대한 선택라벨링 기능, 정보/영상에 대한 시계열 분할 선택라벨링 기능, 정보/영상에 대한 신체부위별 선택라벨링 기능 등을 제공받기 위한 사용자로 회원 가입하며, 개인 정보 등을 상기 서버에 등록한다. 이때, 상기 개인 정보는 아이디, 이 메일 주소, 패스워드(또는 비밀번호), 이름, 성별, 생년월일, 연락처, 주소지(또는 주소정보) 등을 포함한다. 또한, 상기 단말은 해당 단말의 사용자가 가입한 SNS 계정 정보 또는 타사이트 계정 정보 또는 모바 일 메신저 계정 정보를 이용하여 상기 서버에 사용자로 회원 가입할 수도 있다. 여기서, 상기 SNS 계정은 페이스북, 트위터, 인스타그램, 카카오 스토리, 네이버 블로그 등과 관련한 정보일 수 있다. 또한, 상기 타사이 트 계정은 유튜브, 카카오, 네이버 등과 관련한 정보일 수 있다. 또한, 상기 모바일 메신저 계정은 카카오톡 (KakaoTalk), 라인(line), 바이버(viber), 위챗(wechat), 와츠앱(whatsapp), 텔레그램(Telegram), 스냅챗 (snapchat) 등과 관련한 정보일 수 있다. 또한, 회원 가입 절차 수행 시, 상기 단말은 본인 인증 수단(예를 들어 이동 전화, 신용카드, 아이핀 등 포함)을 통한 인증 기능을 완료해야 상기 서버에 대한 회원 가입 절차를 정상적으로 완료할 수 있다. 또한, 회원 가입이 완료된 후, 상기 단말은 상기 서버에서 제공하는 서비스를 이용하기 위해서, 상기 서버로부터 제공되는 전용 앱(또는 애플리케이션/응용 프로그램/특정 앱)을 해당 단말에 설치한다. 이때, 상기 전용 앱은 네이티브 앱(Native App), 모바일 웹앱(Mobile WebApp), 반응형 웹앱(Mobile WebApp Design: RWD), 적응형 웹앱(Adaptive Web Design: AWD), 하이브리드 앱(Hybrid App) 등을 포함하며, 로우 데이 터 수집 기능, 정보/영상에 대한 계층라벨링 기능, 정보/영상에 대한 선택라벨링 기능, 정보/영상에 대한 시계 열 분할 선택라벨링 기능, 정보/영상에 대한 신체부위별 선택라벨링 기능 등을 수행하기 위한 앱일 수 있다. 또한, 회원 가입이 완료된 후, 상기 단말은 상기 서버에서 제공되는 할인 쿠폰을 해당 전용 앱을 통 해 표시할 수 있다. 이때, 상기 할인 쿠폰은 해당 서버에서 제공하는 로우 데이터 수집 기능, 정보/영상에 대한 계층라벨링 기능, 정보/영상에 대한 선택라벨링 기능, 정보/영상에 대한 시계열 분할 선택라벨링 기능, 정 보/영상에 대한 신체부위별 선택라벨링 기능 등을 이용시 일정 비율의 할인 정보를 포함하는 할인 쿠폰일 수 있 다. 또한, 상기 단말은 상기 서버에서 제공하는 기능들을 수행하기 위해서, 상기 서버 및 결제 서버 (미도시)와 연동하여, 구독 기능에 따라 결제 기능을 수행한다. 이때, 상기 서버는 카드 결제, 은행의 결 제 계좌 연동을 통한 자동 이체, 상기 서버에 회원 가입한 상기 단말의 계정에 남아 있는 현금성 포 인트나 현금을 이용한 결제, 카카오페이, 네이버페이 등을 포함하는 간편결제 등을 통해 결제 기능을 수행할 수 있다. 결제가 실패한 경우, 상기 단말은 상기 서버(또는 상기 결제 서버)로부터 전송되는 결제가 실패한 상 태임을 나타내는 정보(예를 들어 잔액 부족, 한도 초과 등 포함)를 수신하고, 상기 수신된 결제가 실패한 상태 임을 나타내는 정보를 출력(또는 표시)한다. 또한, 상기 단말은 결제 기능이 정상적으로 수행된 후, 상기 서버로부터 전송되는 결제 기능 수행 결 과를 수신한다. 여기서, 상기 결제 기능 수행 결과는 구독 기간, 결제 금액, 결제 일자 및 시각 정보 등을 포함 한다. 또한, 상기 단말은 해당 단말에 미리 설치된 전용 앱을 실행하고, 전용 앱 실행에 따른 앱 실행 결과 화면을 표시한다. 여기서, 상기 앱 실행 결과 화면은 특정 주제와 관련한 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보 등을 수집하기 위한 수집 메뉴(또는 버튼/항목), 수집된 정보나 상기 서버로부 터 제공되는 정보를 표시하기 위한 보기 메뉴, 환경 설정을 위한 설정 메뉴 등을 포함한다. 여기서, 상기 단말 은 해당 전용 앱을 제공하는 서버에 회원 가입한 상태로, 회원 가입에 따른 아이디 및 비밀번호, 상 기 아이디를 포함하는 바코드 또는 QR 코드 등을 이용해서 상기 전용 앱 실행 시 로그인 절차를 수행하여, 해당 전용 앱의 하나 이상의 기능(예를 들어 로우 데이터 수집 기능, 정보/영상에 대한 계층라벨링 기능, 정보/영상 에 대한 선택라벨링 기능, 정보/영상에 대한 시계열 분할 선택라벨링 기능, 정보/영상에 대한 신체부위별 선택 라벨링 기능 등 포함)을 수행할 수 있다. 또한, 상기 단말에 표시되는 앱 실행 결과 화면에서 미리 설정된 수집 메뉴가 선택되는 경우, 상기 단말 은 사용자 설정에 따른 하나 이상의 시각 세트 장치(미도시)로부터 특정 주제와 관련해서, 하나 이상의 로 우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 수집하기 위해서, 상기 선택된 수집 메뉴에 대응하는 수집 화면을 표시한다. 여기서, 상기 수집 화면은 사 용자 선택(또는 사용자 입력/터치/제어)에 따라 해당 단말과 연동하는 하나 이상의 시각 세트 장치를 선택 하기 위한 정보 수집 대상 선택 항목, 선택된 정보 수집 대상으로부터 수집할 정보의 종류를 선택하기 위한 수 집 정보 종류 선택 항목, 선택된 항목에 따라 해당 정보 수집 대상으로부터 정보를 수집하기 위한 수집 시작 항 목 등을 포함한다. 또한, 상기 단말은 해당 단말에 표시되는 수집 화면에서 해당 단말의 사용자 입력(또는 사용자/ 전문가 선택/터치/제어)에 따라 복수의 입력 항목에 대응하는 복수의 입력값을 수신한다. 여기서, 상기 복수의 입력값은 정보 수집 대상(또는 시각 세트 장치 정보/시각 세트 장치의 식별 정보), 수집할 정보의 종류(예를 들 어 시퀀셜 정지영상(또는 복수의 시퀀셜 정지영상), 동영상, 측정값/센서값 등 포함) 등을 포함한다. 또한, 상기 단말은 상기 수신된 복수의 입력값을 근거로 상기 하나 이상의 시각 세트 장치와 연동하여, 특 정 주제와 관련해서, 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 수집한다. 여기서, 상기 특정 주제(또는 특정 콘텐츠)는 의료 행위(예 를 들어 시술, 수술 등 포함), 댄스, 운동 종목(예를 들어 축구, 농구, 탁구 등 포함), 게임, 이-스포츠(e- sport) 등을 포함한다. 이때, 상기 단말은 해당 특정 주제와 관련해서 1명의 사용자로부터 1개의 로우 데 이터를 수집할 수도 있고, 1명의 사용자로부터 서로 다른 복수의 로우 데이터(또는 어노테이션 단계(annotation 단계) 또는 어트리뷰트(attribute) 항목의 로우 데이터/기초 영상정보)를 수집할 수도 있다. 여기서, 상기 비교 대상 영상은 저작권, 초상권 등의 지식 재산권에 저촉되지 않는 콘텐츠일 수 있다. 상기 시각 세트 장치는 상기 단말, 상기 서버 등과 통신한다. 또한, 상기 시각 세트 장치는 카메라부, 라이다, 아이트래커, 모션 캡처 및 모션트래커, 의료장비(예를 들어 CT, 스캐너, MRI, 의료용 초음파 등) 등을 포함한다. 또한, 상기 시각 세트 장치는 해당 시각 세트 장치가 구성된(또는 배치된/설치된) 장소(또는 영역)와 관련한 실 제 현실의 영상(또는 실제 현실의 영상정보)을 획득(또는 수집/촬영/측정)한다. 여기서, 상기 실제 현실의 영상 은 로우 데이터(raw data)(또는 원본 데이터/소스 데이터/시각 데이터)를 나타내며, 실제 현실에서 획득되는(또 는 수집되는/촬영되는/측정되는) 시퀀셜 정지영상(또는 복수의 시퀀셜 정지영상/속성), 동영상(또는 타깃 속 성), 측정값 등을 포함한다. 또한, 상기 측정값은 상기 라이다, 상기 아이트래커, 상기 모션 캡처 및 모션트래 커, 상기 의료장비 등을 통해 측정되는 영상 정보(또는 3차원 데이터) 등을 포함한다. 또한, 상기 획득된 하나 이상의 실제 현실의 영상은 합병(merge)하여 사용할 수 있다. 또한, 상기 단말은 지멘스 헬시니어스(Siemens Healthineers)가 마이크로소프트 홀로렌즈 2를 활용해 개발 한 의료 보조 애플리케이션 시네마틱 리얼리티와 연동하여, 실제 현실의 영상을 획득할 수도 있다. 여기서, 상 기 시네마틱 리얼리티는 의료용 CT, MRI 등으로부터 얻어지는 복셀 데이터를 렌더링하는 기능을 포함한다. 시네 마틱 리얼리티로 렌더링된 데이터는 디지털 카데바, 3D 프린팅 인공 카데바 등을 제작하기 위한 데이터 셋으로 사용된다. 이때, 상기 복셀 데이터(boxel data)는 GNN 형태의 포인트 클라우드(point cloud) 데이터와 합병하여 사용한다. 도 2는 본 발명의 실시예에 따른 로우 데이터를 나타낸 도이다. 여기서, 상기 로우 데이터는 실제 현실의 영상 (또는 실제 현실 데이터), 로봇 동작 영상(또는 로봇 동작 영상정보) 등을 포함한다. 이때, 상기 로봇 동작 영 상은 실제 로봇의 작동을 시각세트장치로 수집한 것으로, 아바타 및/또는 아이템의 로우 데이터와 동일한 방식 으로 도 1 내지 도 17, 도 22에 적용된다. 상기 도 2에 도시된 로우 데이터는 K1개의 군집(또는 시퀀셜 형태의 데이터/정지영상)을 나타낸다. 여기서, 상 기 K는 자연수(또는 양의 정수)일 수 있다. 이때, 상기 서버에 의해 생성되는 가상의 생성데이터 (Augmentation 데이터)는 해당 로우 데이터에 포함된다. 또한, 상기 도 2의 로우 데이터를 이용해서 생성되는 가상의 생성데이터는 어트리뷰(attribute) 항목(또는 어노 테이션(annotation) 단계의 복수의 데이터)으로 제공한다. 본 발명은 가상 수술 시뮬레이션 및 가상 치아 삭제 시뮬레이션에서 적은 양의 실제 수술 수집 데이터(또는 실 제 현실의 영상정보/로우 데이터를 사용하여 성능을 극대화하는데 1차 목표가 있으며, 이를 위해 생성된 가상 디지털 카데바 생성데이터를 트레이닝 및 시뮬레이션 단계에서 제공하고, 상기 가상 디지털 카데바 생성데이터 를 의사가 선택라벨링을 하는 방식으로 인공지능(또는 분류 모델/예측 모델)을 지도학습시킬 수 있다. 상기 디지털 카데바(digital cadaver)는 환자의 아바타이다. 디지털 카데바의 단점인 균일한 디지털 속성상 개 별 환자의 상이한 신체 구조(또는 변이)를 반영하기 어려운 것을 보완하기 위해서 의료 현장에서의 정보 수집 장치(또는 상기 시각 세트 장치)(예를 들어 CT, X-Ray, 초음파 장치, 구강 스캐너 등 포함)로부터 수집된 의료 정보, 전문 인력의 경험, 전문 인력의 지식 등을 활용한다. 이런 종합적인 정보를 사용하여 특정 환자의 변이가 반영된 디지털 카데바 및 인공 카데바 사용을 병행하여, 가상현실(VR: virtual reality) 및 3D 시뮬레이터(미도 시)의 가상 치료, 가상 수술 등을 진행할 수 있다.또한, 상기 단말은 상기 수집된 특정 주제와 관련한 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보, 단말의 식별 정보 등을 서버 에 전송한다. 여기서, 상기 단말의 식별 정보는 MDN(Mobile Directory Number), 모바일 IP, 모바일 MAC, Sim(subscriber identity module: 가입자 식별 모듈) 카드 고유정보, 시리얼번호 등을 포함한다. 이때, 상기 로우 데이터와 관련한 비교 대상 영상이 해당 단말에서 수집되지 않은 경우, 상기 단말은 상기 수집된 특정 주제와 관련한 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 상기 단말 의 식별 정보 등을 상기 서버에 전송한다. 또한, 상기 단말은 상기 전송에 응답하여 상기 서버로부터 전송되는 해당 로우 데이터와 관련한 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 수신하고, 상기 수신된 해당 로우 데이터와 관련한 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등과 상기 수집된 특정 주제와 관련한 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보 등을 매칭하여(또는 매핑하여/연동하여) 관리한다. 또한, 상기 단말은 상기 수집된 특정 주제와 관련해서, 하나 이상의 로우 데이터, 해당 로우 데이터와 관 련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 표시(또는 출력)한다. 이때, 상기 단말은 해당 로우 데이터 등에 가상현실, 증강현실, 확장현실, 혼합현실 등을 적용하여 표시(또는 출 력)할 수도 있다. 즉, 상기 단말에 표시되는 앱 실행 결과 화면에서 미리 설정된 보기 메뉴가 선택되는 경우, 상기 단말 은 수집된 정보나 상기 서버로부터 제공되는 정보를 표시하기 위해서, 상기 선택된 보기 메뉴에 대응 하는 보기 화면을 표시한다. 여기서, 상기 보기 화면은 상기 로우 데이터나 생성된 영상을 표시하기 위한 영상 표시 영역, 상기 비교 대상 영상을 표시하기 위한 비교 대상 영상 표시 영역, 계층라벨링을 위해 변수값(또는 라벨값)을 선택하기 위한 계층라벨 입력 메뉴, 선택라벨링을 위해 설정값을 선택하기 위한 선택라벨 입력 메뉴, 동영상에 대한 재생/일시정지/멈춤 기능 등을 제공하기 위한 재생바 등을 포함한다. 또한, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 수집된 로우 데이터를 상기 영상 표시 영역에 표시(또는 출력)하고, 상기 수집된 로우 데이터에 대응하는 비교 대상 영상(또는 상기 서버로부터 제공받은 해당 로우 데이터에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표시(또는 출력)한 다. 이때, 상기 단말은 상기 로우 데이터 및 상기 비교 대상 영상에 각각 대응하는 메타 정보를 근거로 해 당 로우 데이터 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 로우 데이터 및 비교 대상 영상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 여기서, 상기 단말 내의 상기 영상 표시 영역에 표시되는 로우 데이터 및 상기 비교 대상 영상 표시 영역에 표시되는 비교 대상 영상 중 에서 어느 하나가 일시정지 기능 또는 멈춤 기능에 의해 멈추는 경우, 상기 단말은 다른 하나도 함께 일시 정지 기능 또는 멈춤 기능에 의해 멈추도록 제어한다. 또한, 상기 단말은 상기 서버와 연동하여, 해당 단말에 표시되는 로우 데이터에 대해서, 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 로우 데이터 중 특정 시점(또는 특정 구간)에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 로우 데이터에 대해서 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 특정 시점(또는 특정 구간)에서의 해당 로우 데이터에 포함 된 객체의 움직임(또는 객체의 행위)에 대한 잘된 행위 또는 잘못된 행위에 대해 라벨(또는 라벨값)을 설정(또 는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 로우 데이터의 하나 이상의 특정 시점에서 사용자 입력 에 따라 잘된 행위에 대한 라벨값(예를 들어 미리 설정된 승인/승낙/ACCEPT 라벨) 또는 잘못된 행위에 대한 라 벨값(예를 들어 미리 설정된 거절/REJECT 라벨)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련한 로우 데이터에 대해서, 해당 특정 주제와 관련한 전문가 인 해당 단말의 사용자 입력에 따라, 하나 이상의 특정 시점(또는 특정 구간)에서 하나 이상의 선택라벨 (또는 선택라벨값)을 각각 설정(또는 수신/입력)한다. 또한, 이와 같이, 상기 단말의 사용자는 해당 단말에 표시되는(또는 출력되는) 로우 데이터에서 사용 자 자신의 전문 지식을 기초로 판단하여, 잘못된 행위와 관련된 부분이 보이면, 해당 부분에는 거절 라벨을 선택하고, 잘된 행위와 관련된 부분에는 승인 라벨을 선택한다. 또한, 상기 단말은 해당 로우 데이터에 마우스(미도시)의 드래그(drag)를 하거나 태그를 붙여서, 경계선과 경계면을 자동 인식하는 객체 인식 방법을 이용하여 2분법, 3분법, 다분법 등의 방식으로 해당 단말에 표 시되는 로우 데이터의 특정 시점(또는 특정 구간)에서 라벨을 붙일 수 있다. 여기서, 상기 선택라벨링(또는 선 택레이블링/1차 선택라벨링/제 1 선택라벨링)은 상기 로우 데이터의 특정 시점(또는 특정 구간)에서의 오류(또 는 이상) 유무에 대한 라벨(label)(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 로우 데이터 중에서 상기 선택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점(또는 구간)은 미리 설정 된 디폴트 라벨값(예를 들어 승인 라벨)이 설정될 수 있다. 또한, 상기 단말은 상기 로우 데이터 중에서 상기 승인 라벨이 붙지 않은 시점(또는 구간/속성/타깃 속성)에는 미리 설정된 not ACCEPT 라벨을 붙이고, 상기 로우 데이터 중에서 상기 거절 라벨이 붙지 않은 시점(또는 구간/속성/타깃 속성)에는 미리 설정된 not REJECT 라벨을 붙일 수도 있다. 또한, 상기 객체 인식(object recognition/object detection)을 위한 인공 신경망은 해당 단말의 사용자 가 해당 단말에 표시되는 로우 데이터에 드래그를 하거나 태그를 붙이면, 하나 이상의 잘못된 동작 부위 및 동작을 탐지하여 이미지를 분리하고 분석한다. 또한, 상기 단말은 인공 지능 추론 과정을 통해 사용자 에게 추론 결과를 제공한다. 다양한 실시예로, 로우 데이터(또는 영상 정보)는 2D 영상 정보, 3D 영상 정보, 정지 영상의 포인트 클라우드 정보 등을 포함한다. 또한, 상기 단말은 해당 단말에 표시되는 로우 데이터(예를 들어 동영상)에서 해당 재생바 내의 타임 라인 상에서 사용자 입력에 따라 마우스의 화살표(또는 마우스의 포인트)를 이동하여 특정 시점에서 정지하여 정지 영상을 캡처한 다음, 해당 시각의 정지 영상에 경계선과 경계면을 자동 인식하여 마우스의 버튼과 화살표 를 이용해서 태그를 붙인다. 또한, 상기 단말은 동영상 중에서 캡처한 복수의 3D 정지 영상에 태그를 붙이 면, 동영상 전체의 경계면과 경계선이 자동 인식되도록 제어한다. 또한, 상기 단말에 출력되는 로우 데이터(또는 영상 정보) 전체에 대해 승인 라벨 또는 거절 라벨을 붙이 고자 하는 경우, 상기 단말은 사용자 입력에 따라 해당 단말에 표시되는 승인 버튼 또는 거절 버튼을 직접 누르는 방식으로 가능하고, 세밀한 부분을 지정하여 승인 버튼 또는 거절 버튼을 눌러 라벨을 붙이고자 하 는 경우, 마우스 드래그를 이용하여 경계선(예를 들어 직선, 곡선 등 포함)이나 경계면(예를 들어 폐곡선 등 포 함)을 지정하거나 또는, 복수의 포인트를 마우스 버튼으로 지정한 다음에 승인 버튼이나 거절 버튼을 눌러 라벨 을 붙일 수 있다. 본 발명의 일 실시예에서, 객체 인식의 방법에서 객체 탐지, 위치 측정, 객체 및 인스턴스 분할, 자세 추정 등 이 적용되고, 동영상 분석을 위한 인스턴스 추적, 행동 인식, 움직임 추정 등에도 동일하게 적용된다. 또한, 동 영상 클립에 포함된 동작을 감지하기 위해 합성곱 신경망과 결합하여 사용한다. 동작 감지, 장면 추출, 다음 프 레임 예측, 객체 추적 등이 사용된다. 자동 인식된 경계선과 경계면을 기준으로 인터페이스에서 출력되는 객체 및 동작의 잘된 부분과 잘못된 부분에 각각 승인 버튼 또는 거절 버튼을 누르는 방식으로 해당 라벨을 붙인다. 본 발명의 일 실시예에서, 2D 영상 정보 및 3D 영상 정보의 복수의 포인트 클라우드에 마우스의 왼쪽 버튼을 눌 러 태그를 붙이거나 및/또는 드래그를 하여 태그를 붙이면, 경계선 및 경계면의 자동 인식이 가능할 수 있다. 또한, 3D 정지 영상 중에서 x, y, z 좌표상에 존재하는 복수의 포인트 클라우드에 마우스 왼쪽 버튼을 눌러 태 그를 붙이거나 드래그를 하여 태그를 붙이면, 잘된 정보와 잘못된 정보의 경계선이 자동 인식되고, 폐곡선에 의 해 경계면이 자동 인식될 수 있다. 또한, 상기 정보 처리 시스템은 기타 입력 장치(미도시)를 더 포함할 수 있다. 상기 기타 입력 장치는 상기 단말, 상기 서버 등과 통신한다. 또한, 상기 기타 입력 장치는 로우 데이터(또는 영상 정보)에 태그를 붙이거나 드래그를 하여 라벨을 붙일 때 사용한다. 또한, 상기 기타 입력 장치는 컨트롤러, 아이 트래커(eye tracker), 데이터 글러브(data glove), 음성인식 인터 페이스(speech recognition interface), 브레인 컴퓨터 인터페이스(Brain-Computer Interface: BCI), 손 추적 기술(hand tracking technology), 햅틱 장치(haptic device) 등을 포함한다.다음은 상기 기타 입력 장치의 사용 방법의 예를 나타낸다. 즉, 상기 기타 입력 장치의 사용 방법은 마우스의 화살표나 버튼을 음성 인식 인터페이스와 브레인 컴퓨터 인터 페이스로 작동시켜, 태그를 붙이거나 드래그를 하고 라벨을 붙이는 방법, 광선이 나오는 컨트롤러를 음성 인식 인터페이스와 브레인 컴퓨터 인터페이스로 작동시켜 태그를 붙이거나 드래그를 하고 라벨을 붙이는 방법, 아이 트래커를 음성 인식 인터페이스와 브레인 컴퓨터 인터페이스로 작동시켜 태그를 붙이거나 드래그를 하고 라벨을 붙이는 방법, 데이터 글러브를 손 추적 기술, 음성 인식 인터페이스, 브레인 컴퓨터 인터페이스로 작동시켜 태 그를 붙이거나 드래그를 하고 라벨을 붙이는 방법 등을 포함한다. 본 발명의 일 실시예에서, 음성 인식 인터페이스로 컴퓨터의 마우스의 버튼을 직접 움직인다. 컨트롤러의 광선 을 움직여서 태그를 붙이거나 드래그를 할 수도 있다. 또한, 아이 트래커를 사용하여 사용자의 응시를 감지해 시야각의 중심부에 태그를 붙이는 방법으로 분류하고자 하는 개체를 식별하여 객체에 라벨링을 수행한다. 또한, 데이터 글러브 및 손동작의 상호작용(또는 손동작 추적 기술)을 이용하여 사용자 인터페이스상의 영상에 태그를 붙이거나 객체에 경계선과 경계면을 만든다. 플랫폼 사용자(또는 각 분야 전문가 집단)가 사람의 뇌와 컴퓨터를 연결하는 브레인 컴퓨터 인터페이스 기술을 이용한다면, 플랫폼 사용자는 영상(예를 들어 정지 영상, 동영상 등 포함)을 보고, 경계선과 경계면에 마우스로 태그를 붙이거나 드래그를 한 다음 자신의 의지(또는 생각)만으로 정지 영상이나 동영상에 승인 버튼 또는 거절 버튼을 눌러 라벨을 붙일 수 있다. 더 나아가, 자신의 의지(또는 생각)만으로 정지 영상이나 동영상에서 경계선과 경계면을 만들기 위한 태그를 붙이거나 드래그를 한 다음, 구 분된 정지 영상과 동영상에서 선택라벨링을 수행한다. 본 발명의 일 실시예에서, 브레인 컴퓨터 인터페이스와 순환 신경망, 합성곱 신경망, 다층 신경망 알고리즘 등 과 로봇팔 기술을 융합하여 사용하면, 생각만으로도 해당 단말의 화면에 표시되는 승인 버튼 또는 거절 버 튼을 눌러 라벨을 붙이거나 선택라벨링을 할 수 있다. 또한, 상기 단말은 뇌 기계 인터페이스, 뉴로모핍 칩 등을 이용하여 정지 영상 정보와 동영상 정보에 라벨링을 하고, 라벨링된 정보들로 계층적 군집화를 할 수도 있다. 본 발명의 일 실시예에서, 고도화된 브레인 컴퓨터 인터페이스가 개발되면, 해당 단말에 표시되는 사용자 인터페이스(또는 화면)가 생각만으로 사용자의 머릿속에 나타나고, 사용자의 생각만으로 라벨링을 할 수도 있다. 또한, 상기 단말 또는 상기 서버는 라벨링된 정보들로 계층적 군집화를 하고, 분류 모델 및 예 측 모델에 활용할 수 있다. 본 발명의 일 실시예에서, 정지 영상에서 잘못된 부분을 지정하는 방식은 다음과 같다. 치과 의사가 환자의 구강에 식립된 교정용 미니 임플란트의 위치가 자신의 의학 지식에 근거하여 적절한 위치보 다 다소 높거나 낮다고 판단하면, 마우스 드래그를 이용하여 경계선(예를 들어 직선, 곡선 등 포함)이나 경계면 (예를 들어 폐곡선 등 포함)을 지정하거나 복수의 포인트를 마우스 버튼으로 지정하고, 거절 버튼을 누를 수 있 다. 이 부분은 거절 라벨이 붙게 된다. 본 발명의 일 실시예에서, 수술 동영상에서 잘못된 부분을 지정하고 라벨을 붙이는 방식은 다음과 같다. 먼저, 잘못된 의료 행위 및/또는 잘못된 의료 동작이 행해진 동영상 구간을 시계열(또는 해당 재생바 내의 타임 라인 상)에서 마우스의 화살표를 이용하여 한정한다. 해당 마우스의 화살표를 이동하여 선택된 시각과 시각의 사이에 존재하는 동영상 정보가 라벨을 붙일 정보로 한정된다. 본 발명의 일 실시예에서, 교정용 미니 임플란트를 식립하는 동영상에서 선택라벨링을 하고자 한다면, 동영상의 정지 화면 및/또는 동영상 화면에서 마우스 드래그나 마우스 버튼을 이용한 복수의 태그를 사용하여 경계선(예 를 들어 곡선, 직선 등 포함) 및/또는 경계면(예를 들어 폐곡선 등 포함)이 되는 포인트 클라우드들을 지정한다. 이때, 선택하고자 하는 동영상이 자동 인식되고, 그 다음 순서로 인식된 동영상에 승인 버튼을 누를 수 있다. 또한, 상기 단말은 상기 로우 데이터와 관련한 하나 이상의 특징 시점(또는 특정 구간)에서의 하나 이상의 선택라벨값, 해당 로우 데이터의 메타 정보, 해당 단말의 식별 정보 등을 상기 서버에 전송한다. 또한, 상기 단말은 상기 서버와 연동하여, 해당 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하기 이전에 또는 수행한 이후에, 해당 하나 이상의 로우 데이터를 대상으로 계층라벨링을 수행하고, 계층 라벨링 수행 전/후로 해당 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행할 수도 있다. 여기서, 상기 계층라벨링(또는 계층레이블링/1차 계층라벨링/제 1 계층라벨링)은 사용자에 의한 입력 피처 엔지니어링(inputfeature engineering)(또는 계층적 군집화 라벨링)으로, 해당 로우 데이터에 대한 특징을 나타내는 라벨(또는 라벨값)을 붙이고, 해당 로우 데이터를 특징에 따라 복수의 서브 로우 데이터로 분할(또는 분류)하는 라벨링 방 법을 나타낸다. 즉, 상기 단말은 상기 서버와 연동하여, 해당 단말에 표시되는 로우 데이터에 대해서, 해당 특 정 주제와 관련해서 미리 설정된 복수의 라벨 분류를 참조하여(또는 근거로), 해당 단말의 사용자 입력(또 는 사용자 선택/터치/제어)에 따라, 해당 로우 데이터 중 다른 특정 시점(또는 다른 특정 구간)에서의 라벨(또 는 라벨값)을 설정(또는 수신/입력)한다. 다음의 [표 1] 내지 [표 11]은 특정 분야에서의 라벨 분류(또는 라벨 분류표)의 예를 나타낸다. 상기 라벨 분류는 인공지능이 학습할 정답 데이터 셋을 나타내며, 사용자가 참조하여 계층적 군집화 라벨링을 할 수 있도록 임의의 방식 및 단계별로 계층화한 분류를 나타낸다. 즉, 상기 [표 1] 내지 상기 [표 6]은 치과 대학교수(또는 의사 인플루언서)가 임플란트 수술 또는 라미네이트 시술을 하는 과정에서의 계층별 라벨값(또는 변수값)의 예를 나타낸다. 이때, 의사 인플루언서의 라벨 분류는 다양한 동작에 대해 m1×m2×m3×n×n'×N(각 라벨분류들의 곱)개 존재한다. 사용자는 라벨 분류를 참조하여 계층적 군집화 관련 변수값(또는 라벨값)을 도 23 내지 도 28 및 도 30 내지 도 32의 입력창(s1, s2, s3)에 입력한다. 제1 계층(201, 701, 801, 901, 1001, 1101)의 변수 s1의 변수값(또는 라벨값)이 입력되고, 제2 계층(102, 702, 802, 902, 1002, 1102)의 변수 s2의 변수값(또는 라벨값)이 입력되고, 제 3계층(203, 703, 803, 903, 1003, 1103)의 변수 s3의 변수값(또는 라벨값)이 입력된다. 계층의 개수에 따라 입력창의 개수가 늘어난다. 라벨분류를 참조한 사용자가 재생바(도 23내지 도 28, 도 29 내지 도 32) 내의 타임 라인 상에서 시각을 나타내 는 표(또는 화살표)를 이동하여, 동영상을 분할하고자 하는 시점의 정지영상정보를 캡처한 후, ACCEPT 버튼을 누르거나 혹은 선택을 하면, 해당 선택시점이 동영상의 분할시점이 되며, 상기 단말(또는 상기 서버(20 0))은 상기 선택된 ACCEPT 버튼에 대응하는 선택시점(또는 분할시점)에 따라 해당 동영상을 분할한다. 제4 계층(204, 704, 804, 904, 1004, 1104), 제5 계층(905, 1005, 1105), 제6 계층 등의 동영상정보는 라벨 분류의 라벨값(k, L, f)과 같은 순서대로 분할된다. 표 1 변수값(라벨값) 특정 아바타(외과 수술 분야) 정보 형태 1 설암 수술 문서 등 2 양악 수술(BSSRQ) 문서 등 3 대장암 수술 문서 등 ... ... 문서 등 S1 치과 임플란트 수술 문서 등 ... ... 문서 등 m1 간 이식 수술 문서 등 표 2 변수값(라벨값) 특정 아바타의 특정 동작 (특정 변이를 갖는 환자들의 치과임플란트수술의 증례 혹은 특 정 변이를 갖는 디지털카데바의 증례)정보 형태 1 상악 구치부 골폭이 좁은 증례 영상 등 2 전치부 골폭이 좁고 치조골 소실이 많은 증례 영상 등 ... ... 영상 등 S2 하악 구치부 골폭이 좁은 증례 영상 등 ... ... 영상 등 m2 ... 영상 등표 3 변수값(라벨값) 특정 동작의 특정 방식 (하악 구치부골폭이 좁은 증례의 수술 방식)정보 형태 1 ridge split을 한 후... 영상 등 2 3D stent를 이용하여 드릴이 안전하게 삽입... 영상 등 ... ... 영상 등 S3 block bone을 이식한 수술 영상 등 ... ... 영상 등 m3 ... 영상 등 표 4 변수값(라벨값) 특정 방식의 특정 단계 (block bone 이식 수술 방식의 수술 단계)정보 형태 1 절개 및 피판을 형성한다. ... 영상 등 2 block bone을 공여 부위에서 채취한다. ... 영상 등 ... ... 영상 등 K 이식 부위에 block bone을 고정한다. 영상 등 ... ... 영상 등 n 봉합 및 소독을 한다. ... 영상 등 표 5 변수값(라벨값) 세부 동작 단계 1 (상악중절치 라미네이트 11번(치식) 삭제를 진행하는 30초 동 영상)정보 형태 1 치아 삭제 전에 미리 제작한 치아 삭제용 인덱스를 구강 및 치 아에 위치시킨다.영상 등 2 구강 및 치아에 위치된 인덱스를 치과 의사가 눈으로 확인하고 삭제량을 측정한다.영상 등 ... ... 영상 등 L 치아 절단부 3분의 1의 예상 삭제 깊이를 depth gage bur로 삭 제한다.영상 등 ... ... 영상 등 n' 상악 중절치 전체 치아를 핸드피스의 트리밍 bur(다듬는 칼 날)로 다듬고 미세하게 삭제한다.영상 등 표 6 변수값(라벨값) 세부 동작 단계 2 (치식) 정보 형태 1 11(오른쪽 상악 중절치에 해당됨) 영상 등 2 12 영상 등 ... ... 영상 등 f 35(좌측 하악 두 번째 작은 어금니 해당됨) 영상 등 ... ... 영상 등 N 48(40번대 치열에서 사랑이에 해당됨) 영상 등 또한, 상기 [표 7] 내지 상기 [표 11]은 댄서(또는 춤 인플루언서)가 블랙핑크의 마지막처럼의 춤 동작에서의 계층별 라벨값(또는 변수값)의 예를 나타낸다. 표 7 변수값(라벨값) 특정 아바타 (딥페이크 할 게임 캐릭터) 정보 형태 1 BTS 진 영상 등 2 BTS 슈가 영상 등... ... 영상 등 S1 블랙핑크 제니 영상 등 ... ... 영상 등 m3 블랙핑크 지수 영상 등 표 8 변수값(라벨값) 특정 아바타의 특정 동작 (제니의 댄스 동작 및 노래 종류)정보 형태 1 Shut Down(4분 10초) 영상 등 ... ... 영상 등 S2 마지막처럼(3분 14초) 영상 등 ... ... 영상 등 m3 자 오늘 밤이야(3분 55초) 영상 등 표 9 변수값(라벨값) 특정 동작의 특정 방식 (제니의 마지막처럼 방송 목록)정보 형태 1 뮤직 뱅크 2022년 3월 14일 방송 영상 등 ... ... 영상 등 S3 열린음악회 2022년 7월 8일 방송 영상 등 ... ... 영상 등 m3 콘서트 2022년 6월 3일 녹화 영상 등 표 10 변수값(라벨값) 특정 동작의 특정 단계 (열린음악회 2022년 7월 8일 방송)정보 형태 1 좌측 그루브 영상 등 ... ... 영상 등 K 앞뒤 웨이브 영상 등 ... ... 영상 등 n 상체 팝핀 및 골반 튕기기 영상 등 표 11 변수값(라벨값) 세부 동작 단계 2 (제니가 앞뒤 웨이브할 때 가장 많이 움직이는 신체 부위의 순서)정보 형태 1 왼쪽 팔을 든다. 영상 등 2 오른쪽 팔을 든다. 영상 등 3 가슴을 앞으로 내민다. 영상 등 4 배를 앞으로 내민다. 영상 등 5 골반을 앞으로 내민다. 영상 등 6 다리를 앞으로 내민다. 영상 등 이와 같이, 상기 [표 5] 및 상기 [표 10]은 사용자에 의해 동영상이 1초 ~ 3초 내외로 짧게 분할될 수 있도록 특징적인 동작으로 세부 분류된 라벨 분류이다. 상기 [표 1] 내지 [표 12]의 동일한 방식으로 실제 현실의 로봇 동작 영상이 라벨 분류로 제작될 수 있다. 또한, 상기 [표 6] 및 상기 [표11]은 아바타, 인간, 로봇 등의 신체부위에 라벨을 붙인 것으로 제 1, 2 계층라 벨링, 선택라벨링, 추가 선택라벨링, 시계열분할 선택 라벨링, 신체부위별 선택라벨링 등에 사용되며, 전문가집단이 임의로 설정한 라벨 분류이다. 신체부위별 선택은 단수 혹은 복수의 분할된 정지영상에서 신체의 각 세밀한 부위에 대한 객체 인식을 통해 신 체의 세밀한 부위의 각 영상에 라벨분류 [표 6] 및 상기 [표 11]의 라벨 순서대로 라벨값을 불이는 방식이다. 재생바(시각을 나타내는 표) 또는 신체부위별 선택을 이용하여 데이터단위 5로 동영상을 분할 할 수 있다. 신체 부위별 선택(데이터단위 5를 생성, 동영상 분할, f)을 통한 계층라벨링은 사용자에 의해 실행되는 인풋 피 쳐 엔지니어링이고, 이와 같은 신체부위별 선택은 생략될 수 있다. 서버는 신체부위별 선택(세밀한 부위 신체의 객체인식 등)에 관한 라이브러리를 호출하여 자동으로 라벨링(라벨 f)을 하고 동영상을 분할(데이터단위 5)할 수 있다. 도 11은 데이터 단위 5를 기준으로 하는 계층적 군집화를 나타내는 도면으로 사용될 수 있다. 신체부위별 선택은 계층라벨링이고, 신체부위별 선택 라벨링은 서버와 사용자의 상호작용(서버에 의한 동 영상 분할시점(라벨값)에 대한 사용자의 판단 혹은 신체부위의 동작 순서에 대한 판단)을 통해 디지털단위 5를 생성하고, 동영상을 분할하는 라벨링이다. 시계열분할 선택(데이터단위 3, 4 생성)은 계층라벨링이고, 시계열분할 선택라벨링은 사용자의 상호작용(서버에 의한 동영상 분할시점(라벨값)에 대한 사용자의 판단)을 통해 디지털단위 3, 4를 생성하고, 동영상을 분할하는 라벨링이다. 세부 동작 단계는 동영상을 분할하는 방식에 따라 세부 동작 단계 1과 세부 동작 단계 2로 분류한다. 여기서, 상기 세부 동작 단계 1은 시계열 분할 선택라벨링에 의한 동작 단계의 세부 분할이고, 상기 세부 동작 단계 2는 신체 부위별 선택라벨링에 의한 동작 단계의 세부 분할이다. 본 발명의 일 실시예에서, 라벨 분류 [표 9]의 제니의 마지막처럼(3분 14초) 노래의 열린음악회 2022년 7월 8일 방송의 댄스 동작이 사용자의 HMD(Head-mounted display)를 통해서 영상으로 출력된다. 제니의 영상은 동영상 형태이고, 분할된 형태로 사용자에게 보일 수 있다. 사용자는 라벨값을 순서대로 정지 영상을 시청할 수 있고, 분할된 동영상의 끝부분 정지 영상을 볼 수 있다. 사용자는 HMD를 통해서 출력된 제니의 영상을 참조하여, 제니의 동작과 유사하거나 동일한 동작을 VR 트레드밀 (treadmill)상에서 실행하고, 사용자의 동작 영상 정보는 상기 시각 세트 장치로 수집되어 로우 데이터(또는 기 초 영상/기초 영상 정보)로 사용된다. 사용자는 자신의 선택에 따라, 제니의 동작과 합성된 자신의 아바타를 생 성할 수도 있고, 자신의 모습과 동작이 제니의 동작과 합성되지 않고 그대로 출력되도록 할 수도 있다. 이때, 사용자는 제니의 동작에 있어서, 정지된 상태로 보이는 정지 영상과 정지 영상의 라벨값을 참조하여, 자 신의 아바타 및 타인의 아바타에 대해 계층라벨링, 선택라벨링, 시계열 분할 선택라벨링, 신체부위별 선택라벨 링 등을 수행한다. 사용자는 제니의 아바타와 인공지능에 의해 합성되어 생성된 자신의 아바타 및 타인의 아바 타의 동작을 HMD를 통해 제 3 자적 시점에서 시청할 수도 있고, 라벨 분류 [표 9]의 제니의 마지막처럼(3분 14 초) 노래의 열린음악회 2022년 7월 8일 방송의 댄스 동작과 비교하여 상기 라벨링을 수행한다. 사용자는 제니의 동작을 수차례 걸쳐 따라할 수 있고, 이러한 춤 동작인 기초 영상 정보(또는 로우 데이터)는 상기 단말에 의해 수집되어 상기 서버에 전송할 수 있다. 수차례의 춤 동작 정보는 어트리뷰트 (attribute) 항목(또는 어노테이션 단계)의 복수의 데이터(또는 복수의 로우 데이터)이다. 본 발명의 일 실시예에서, 상기 [표 1] 내지 [표 6]은 치과 대학교수(또는 의사 인플루언서)가 임플란트 수술 또는 라미네이트 시술을 하는 과정에서의 계층별 라벨값(또는 변수값)의 예를 나타내며, 치과대학 학생이나 치 과 의사들은 정답 데이터 셋인 상기 [표 1] 내지 [표 6]의 라벨 분류를 HMD를 통해 시청하면서, 치아삭제 VR 시 뮬레이터(미도시)를 이용하여, 디지털 카데바에 가상 수술, 가상 시술 등을 진행할 수 있고, 라벨링을 진행할 수 있다. 이때, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 수집된 로우 데이터를 상기 영상 표시 영역에 표시(또는 출력)하고, 상기 수집된 로우 데이터에 대응하는 비교 대상 영상(또는 상기 서버로부터 제공받은 해당 로우 데이터에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표시(또는 출력)한 다. 이때, 상기 단말은 상기 로우 데이터 및 상기 비교 대상 영상에 각각 대응하는 메타 정보를 근거로 해 당 로우 데이터 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 로우 데이터 및 비교 대상 영상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 여기서, 상기 단말 내의상기 영상 표시 영역에 표시되는 로우 데이터 및 상기 비교 대상 영상 표시 영역에 표시되는 비교 대상 영상 중 에서 어느 하나가 일시정지 기능 또는 멈춤 기능에 의해 멈추는 경우, 상기 단말은 다른 하나도 함께 일시 정지 기능 또는 멈춤 기능에 의해 멈추도록 제어한다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 로우 데이터에 대해서 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 다른 특정 시점(또는 다른 특정 구간)에서의 해당 로우 데이 터에 포함된 객체의 움직임(또는 객체의 행위)에 대한 하나 이상의 단계별 라벨(또는 라벨값)을 설정(또는 수신 /입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 로우 데이터의 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서 사용자 입력에 따라 해당 로우 데이터에 포함된 객체의 움직임(또는 객체의 행위)에 대해 계층 적으로(hierarchical) 객체의 특정 동작, 특정 동작의 특정 방식, 특정 방식의 특정 단계 등에 대해 계층라벨 (또는 계층라벨값)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련한 로우 데이터에 대해서, 해당 특정 주제와 관련한 전문가 인 해당 단말의 사용자 입력에 따라, 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서 하나 이상의 계층라벨(또는 계층라벨값)을 각각 설정(또는 수신/입력)한다. 또한, 상기 단말은 계층라벨링 과정을 수행한 전/후로, 앞서 설명한 선택라벨링 과정을 수행한다. 이와 같이, 상기 단말은 특정 아바타, 인간, 로봇 등의 동작별 및/또는 특정 방식별 및/또는 특정 단계별 및/또는 세부동작 단계별로 아바타, 인간, 로봇 등의 동작을 계층적으로 분류한 라벨 분류를 참조하여, 라벨값 을 입력하는 계층적 군집화 라벨링인 계층라벨링을 수행한다. 어떤 특정 환자와 비슷한 해부학적 구조(또는 특정 변이)를 가진 환자들의 시술 또는 수술과 관련한 동작은 특 정 아바타, 인간, 로봇 등의 특정 동작에 포함된다. 치과 시술 혹은 외과수술의 계층적 군집화에서 상기 [표 2] 내지 [표 3]의 특정 증례의 특정 방식의 라벨 분류 는 상기 [표 7] 내지 [표 9]에서 특정 아바타, 인간, 로봇 등의 특정 동작별, 특정 동작의 방식별의 라벨 분류 에 포함된다. 본 발명의 일 실시예에서, 상기 서버에서 계층라벨링의 라벨값을 학습한 인공 지능이 상기 단말의 사 용자에게 라벨값 또는 영상 정보를 반환하고, 사용자는 이에 대해 승인 라벨 또는 거절 라벨을 붙일 수 있다. 다양한 실시예에서, 사용자는 각 분야의 직업을 갖는 전문가(예를 들어 도메인 전문가, 치과의사, 의사, 축구선 수, 댄서 등 포함)일 수 있다. 도 3의 직육면체는 도 4 내지 도 6의 타깃 속성인 분할된 동영상의 동영상 정보이고, k번째 단계의 분할된 동작의 동영상 정보, L번째 단계의 분할된 동작의 동영상 정보, f번째 단계의 분할된 동작의 동영상 정보를 나타낸다. 여기서, 시작부분 yz 평면 또는 끝부분 yz 평면은 속성인 정지 영상 정보를 나타낸다. 여기서, 변수 m1, m2 및 m3는 임의의 양의 정수(또는 자연수)를 나타내고, s1, s2 및 s3는 변수를 나타내고, , , 를 나타내고, k는 변수를 나타내고, 을 나타내고, 변수 n, n' 및 N은 임의의 양의 정수(또는 자연수)를 나타내고, L과 f는 변수를 나타내고, 과 을 나 타낸다. 사용자는 제 1 속성 및 제 1 타깃 속성에 해당하는 로우 데이터(또는 동영상 정보)의 출력을 해당 단말에 표시되는 화면에서 먼저 확인하고서, 라벨 분류를 참조하여 계층적 군집화 관련 변수값(또는 라벨값)을 해당 단 말에 표시되는 화면에서 입력한다. 또한, 상기 단말은 아바타, 인간, 로봇 등의 동작 등에 관련한 영상 정보(예를 들어 속성, 타깃 속성 등 포함)를 출력한다. 또한, 상기 도 4 내지 상기 도 6에서의 속성 및 타깃 속성은 상기 서버에 의해 생성된 가상의 아바타, 아 이템, 인간, 로봇 등의 동작 관련 영상 정보일 수 있다. 또한, 상기 단말은 해당 보기 화면 내의 계층라벨 입력 메뉴에 포함된 복수의 입력창을 통해서 사용자 입 력에 따른 제 1 계층의 변수 S1의 변수값(라벨값)을 수신하고, 제 2 계층의 변수 S2의 변수값(또는 라벨값)을 수신하고, 제 3 계층의 변수 S3의 변수값(또는 라벨값)을 수신한다. 이때, 상기 계층라벨 입력 메뉴에 포함된 복수의 입력창은 설계자의 설계에 따라 계층의 개수에 따라 다양하게 설정할 수 있다. 또한, 제 4 계층, 제 5 계층 및 제 6 계층의 동영상 정보는 라벨 분류의 라벨값(예를 들어 k, L, f)과 같은 순 서대로 분할된다. 본 발명의 일 실시예에서, 사용자가 단계를 분할하는 방법은 상기 재생바 내의 타임 라인 상에서 시각을 나타내 는 표(또는 화살표)를 시간축에서 마우스로 이동시키고, 분할시키고자 하는 동영상의 시각을 확인한 다음 마우 스로 선택한다. 본 발명의 실시예에서는 상기 사용자가 상기 라벨 분류를 참조하여 특정 로우 데이터와 관련한 계층라벨링을 설 정하는 것을 주로 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 단말은 해당 단말의 사용자 입 력에 따라, 단계별로(또는 계층별로/캐스케이드 형태로) 계층라벨링값을 직접 입력받을 수도 있다. 상기 도 4는 동영상(또는 기다란 입체 도형)이 n개로 분할된 상태의 예를 나타내고, 상기 도 5는 동영상이 n'개 로 분할된 상태의 예를 나타내고, 상기 도 6은 동영상이 N개로 분할된 상태의 예를 나타낸다. 여기서, 상기 n, n' 및 N은 , 및 을 만족하고, 변수 k, L 및 f는 양의 정수(또는 자연수)를 나타 낸다. 상기 도 4 내지 상기 도 6은 아바타(또는 사람)의 동작 1회를 입체 도형으로 표현한 것이다. 첫 번째 사각형(검 정색 표시)은 동작(또는 동영상)의 시작부분 정지영상 정보(401, 501, 601)이고, 마지막 사각형(검정색 표시)은 동작(또는 동영상)의 끝부분 정지영상 정보(404, 504, 604)이고, x축은 시간이고, yz평면(사각형)은 정지영상 정보이고, 분할된 직육면체는 분할된 동영상을 나타낸다. 상기 도 4 내지 상기 도 6의 분할된 입체도형 1개는 상기 도 3의 직육면체에 해당(또는 대응)한다. 또한, 상기 도 5의 분할된 입체도형 1개는 기다란 직육면체가 n'개로 분할된 것이고, 상기 도 6의 분할된 입체 도형 1개는 기다란 직육면체가 N개로 분할된 것이다. 상기 도 4 내지 상기 도 6의 기다란 막대모양의 직육면체 는 아바타의 전체 동작 관련 동영상 정보를 입체도형으로 나타낸 것이다. 또한, 상기 도 3을 참조하면, 검정색 형태의 사각형인 끝부분 yz 평면은 속성(또는 정지영상)이고, 직육면 체는 분할된 동작의 동영상(또는 타깃 속성)이다. 다음은 소괄호의 순서대로 매칭된다. 상기 도 4 내지 상기 도 6의 (k, L, f)번째 시작부분 정지영상정보(402, 502, 602)는 상기 도 3의 시작부분 정지영상정보를 나타내고, (k, L, f)번째 끝부분 정지영상정보(403, 503, 603)는 상기 도 3의 끝부분 정지영상정보를 나타내고, (k, L, f)번째 시작부분 정지영상정보(402, 502, 602)와 (k-1, L-1, f-1)번째 끝부분 정지영상정보(403, 503, 603)는 동일하다. 상기 도 4 내지 상기 도 6 의 (k, L, f)번째 끝부분 정지영상정보(403, 503, 603)는 속성이고, 분할된 동작 동영상의 (k, L, f)번째 단계 (405, 505, 605)는 타깃 속성이다. 상기 도 3 내지 상기 도 6에서 데이터 단위 1은 아바타, 인간, 로봇 등의 1 회 동작 전체 동영상의 시작부분 정지영상정보(401, 501, 601)와 전체 동영상의 끝부분 정지영상정보(404, 504, 604)의 합을 나타내고, 데이터 단위 2는 아바타, 인간, 로봇 등의 분할된 동작 동영상의 최초 단계 정지영상정 보로부터 마지막 단계 정지영상정보까지의 합을 나타낸다. 상기 도 4에서 데이터 단위 3은 아바타, 인간, 로봇동작 등의 분할된 동작 동영상의 k번째 단계의 동영상정보와 k번째 단계의 끝부분 정지영상정보의 합을 나타낸다. 상기 도 5에서 데이터 단위 4는 아바타, 인간, 로봇동작 등의 분할된 동작 동영상의 L번째 단계의 동영상정보와 L번째 단계의 끝부분 정지영상정보의 합을 나타낸다. 상기 도 6에서 데이터 단위 4는 아바타, 인간, 로봇동작 등의 분할된 동작 동영상의 f번째 단계의 동영상정보와 f번째 단계의 끝부분 정지영상정보의 합을 나타낸다. 도 7은 데이터 단위 1, 2, 3을 기준으로 하는 계층적 군집화를 나타내고, 도 9는 데이터 단위 1, 2, 3, 4를 기 준으로 하는 계층적 군집화를 나타낸다. 상기 데이터 단위 3에서 속성은 아바타, 인간, 로봇동작 등의 분할된 동작 동영상의 k번째 단계의 끝부분 정지 영상정보이고, 상기 도 4에서의 검정색 표시의 사각형을 나타낸다.또한, 상기 데이터 단위 4에서의 속성은 아바타, 인간, 로봇동작 등의 분할된 동작 동영상의 L번째 단계의 끝부 분 정지영상정보이고, 상기 도 5에서의 검정색 표시의 사각형을 나타낸다. 또한, 상기 데이터 단위 5에서의 속성은 아바타, 인간, 로봇동작 등의 분할된 동작 동영상의 f번째 단계의 끝부 분 정지영상정보이고, 상기 도 6에서의 검정색 표시의 사각형을 나타낸다. 본 발명의 실시예에 따른 분류 모델 및 예측 모델(또는 유도 및/또는 추론 모델)에서 사용되는 데이터 단위 3, 데이터 단위 4, 데이터 단위 5 등은 상기 도 3의 분할된 직육면체일 수 있다.(디지털 단위도 동일) 상기 도 2는 실제 현실의 데이터(또는 로우 데이터)에 대한 계층적 군집화의 계통도로써, K1개의 군집을 나타낸다. 로우 데이터(실제 현실 데이터)에는 시각세트 장치로 수집한 로봇동작 영상정보가 포함된다. 로봇동 작 영상정보를 로우 데이터로 사용할 경우 도 22는 로봇트레이닝에 사용될 수 있다. 상기 도 7 또는 상기 9는 데이터 단위에 의해 동영상의 단계가 분할될 때 붙여진 라벨값에 의해 만들어진 계층 적 군집화의 계통도이다. 상기 도 7은 데이터 단위 3에 근거한 K2개의 군집을 나타내고, 상기 도 9는 데이터 단위 4에 근거한 K4개의 군 집을 나타낸다. 본 발명의 일 실시예에서, 시작부분 정지영상정보도 속성이고, 타깃 속성과의 합으로 데이터 단위가 되고, 알고 리즘의 방향성에 있어서 순방향의 동영상 생성 및 출력에 사용된다. 본 발명의 일 실시예에서, 증례, 방식, 단계별 계층적 군집화와 관련한 라벨링 방식은 다음과 같다. 앞선 [표 1] 내지 [표 6]은 의사 및 치과의사들의 의료 분야 전문지식에 근거하여 제작된 것으로, 상기 단말 에 표시되는 앱 실행 결과 화면(또는 보기 화면)에서 변수값(또는 라벨값)을 입력하기 위해 제시된 라벨 분류들의 예시이다. 상기 [표 1]은 수술 분야의 변수값(또는 라벨값)을 입력하기 위한 예시이고, 상기 [표 2]는 수술 증례의 변수값 (또는 라벨값)을 입력하기 위한 예시이고, 상기 [표 3]은 수술 방식의 변수값(또는 라벨값)을 입력하기 위한 예 시이고, 상기 [표 4]는 수술 단계의 변수값(또는 라벨값)을 입력하기 위한 예시이다. 이것은 임상적인 기준(예 를 들어 증례, 방식, 단계 등 포함)을 사용자(예를 들어 의사, 치과의사 등 포함)가 참조하여 변수값(또는 라벨 값)을 입력하는 방식이다. 상기 [표 5]는 수술 단계를 더욱 세분화하여 분류한 예시이고, 상기 [표 6]은 아바타, 인간, 로봇 등의 신체부 위에 라벨값을 지정한 분류의 예시이다. 외과 수술에 대한 증례, 방식, 단계별 분류 기준이 계층적 군집 라벨 값 입력에 의해, 상기도 7 및 도 9의 데이 터 단위를 기준으로 적용된다. 의료동영상 정보 및 기타 정보들을 증례, 방식, 단계별 계층적 군집화를 하는 것 을 기본으로 하나, 위 정보들을 임의의 방식으로 세부적으로 라벨 링하고, 동영상을 분할하여, 세분화된 계층적 군집화를 한다면 계층의 종류 및 개수(예를 들어 3개 층, 4개 층, 5개 층 등 포함)와 동영상 분할의 방식과 상 관없이, 분류 모델 및/또는 예측 모델에 적용할 수 있다. 본 발명의 일 실시 예에서, 실제 수술과 시술 등에 사용되는 환자의 신체나 장기의 영상 정보, 기타 의료정보, 디지털 카데바 등은 상기 단말에 표시되는 입 실행 결과 화면(또는 보기 화면)을 통해 라벨링되어, 상기 도 7 내지 상기 도 10에서 각각 K2, K3, K4 및 K5개의 군집이 된다. 여기서, 상기 K는 변수(또는 자연수)를 나 타낸다. 같은 군집에 속한 특정한 환자의 신체나 장기정보, 디지털 카데바 등은 그 군집에 해당되는 메타데이터 (또는 메타정보)이다. 메타데이터를 기반으로 한 디지털 카데바의 인공지능 추론 내지 반환으로 가상 수술, 가 상 시술 등을 진행한다. 인공지능 추론 및 반환으로 출력된 디지털 카데바와 인공 카데바를 활용한 가상 수술 동영상, 가상 시술 동영상 등에 대해 의사 및/또는 치과의사들은 상기 단말에 표시되는 앱 실행 결과 화면 (또는 보기 화면)을 통해 선택라벨링을 수행한다. 다양한 실시예에서, 상기 도 3 내지 상기 도 6에서 아바타(또는 사람)의 동작 1회는 어떤 특정 환자의 외과 수 술 1회이다. 동작 시작시의 최초 정지영상정보는 진단정보이며, 아바타(또는 사람)의 동작 1회 동영상 정보의 각각 k, L, f번째 동작의 단계는 어떤 특정 환자의 외과 수술 1회 동영상 정보의 각각의 k, L, f번째 수술의 단 계이다. 수술을 받는 디지털 카데바의 반응은 아바타 또는 사람의 동작과 비교하면, 일종의 수동적인 아바타의 동작이라 할 수 있다(예를 들어 디지털 카데바는 일종의 환자 아바타). 이와 같이, 상기 단말은 아바타, 아이템, 로봇 등의 영상에 대해 계층라벨링 기능, 선택라벨링 기능 등을 수행한다. 본 발명의 실시예에서는 상기 계층라벨링 기능 및 상기 선택라벨링 기능을 분리하여 설명하고 있으나, 이에 한 정되는 것은 아니며, 상기 단말은 상기 계층라벨링 기능을 상기 선택라벨링 기능에 포함시켜 수행할 수 있 으며, 또한 상기 계층라벨링과 상기 선택라벨링을 하나의 라벨링 기능으로 통합하여 수행할 수도 있다. 또한, 상기 단말은 상기 서버로부터 전송되는 제 1 영상을 수신한다. 여기서, 상기 제 1 영상은 상기 서버에서의 해당 로우 데이터를 대상으로 한 분류 모델 및 예측 모델에 의한 학습 결과로 생성된 결과물로, 상기 로우 데이터를 근거로 생성되는 아바타, 아이템, 로봇 등의 동작 관련 영상, 상기 로우 데이터 가 업데이트된 영상(예를 들어 상기 로우 데이터에 포함된 인간/사람의 동작/행위/행동이 업데이트된 영상) 등 일 수 있다. 또한, 상기 단말은 상기 수신된 제 1 영상을 상기 영상 표시 영역에 출력한다. 이때, 상기 단말은 상 기 로우 데이터, 상기 비교 대상 영상 및 상기 제 1 영상을 동기화한 상태에서 해당 단말의 화면을 분할하 여 동시에 출력할 수도 있다. 또한, 상기 단말은 상기 서버와 연동하여, 상기 제 1 영상을 대상으로 추가 선택라벨링을 수행한다. 여기서, 상기 추가 선택라벨링(또는 추가 선택레이블링/2차 선택라벨링/제 2 선택라벨링)은 상기 제 1 영상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 오류(또는 이상) 유무에 대한 라벨(또는 라벨값)을 설정하는 (또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 제 1 영상 중에서 상기 추가 선택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점(또는 구간)은 미리 설정된 디폴트 라벨값(예를 들어 승인 라벨)이 설정될 수 있 다. 또한, 상기 단말은 상기 제 1 영상 중에서 상기 승인 라벨이 붙지 않은 시점(또는 구간/속성/타깃 속 성)에는 미리 설정된 not ACCEPT 라벨을 붙이고, 상기 제 1 영상 중에서 상기 거절 라벨이 붙지 않은 시점(또는 구간/속성/타깃 속성)에는 미리 설정된 not REJECT 라벨을 붙일 수도 있다. 즉, 상기 단말은 상기 서버와 연동하여, 해당 단말에 표시되는 제 1 영상에 대해서, 해당 단말 의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 제 1 영상 중 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 오류(또는 이상) 유무에 대한 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 제 1 영상을 상기 영상 표시 영역에 표 시(또는 출력)하고, 상기 로우 데이터(또는 상기 제 1 영상)에 대응하는 비교 대상 영상(또는 상기 서버로 부터 제공받은 해당 로우 데이터/제 1 영상에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표 시(또는 출력)한다. 이때, 상기 단말은 상기 제 1 영상 및 상기 비교 대상 영상에 각각 대응하는 메타 정 보를 근거로 해당 제 1 영상 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 제 1 영상 및 비교 대상 영상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 여기서, 상기 단 말 내의 상기 영상 표시 영역에 표시되는 제 1 영상 및 상기 비교 대상 영상 표시 영역에 표시되는 비교 대상 영상 중에서 어느 하나가 일시정지 기능 또는 멈춤 기능에 의해 멈추는 경우, 상기 단말은 다른 하나 도 함께 일시정지 기능 또는 멈춤 기능에 의해 멈추도록 제어한다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 제 1 영상에 대해서 해당 단말의 사 용자 입력(또는 사용자 선택/터치/제어)에 따라, 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 해당 제 1 영상에 포함된 객체(또는 아바타)의 움직임(또는 객체/아바타의 행위)에 대한 잘된 행위 또는 잘못된 행위에 대 해 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 제 1 영상의 하나 이상의 또 다른 특정 시점에서 사용자 입력에 따라 잘된 행위에 대한 라벨값(예를 들어 미리 설정된 승인/승낙/ACCEPT 라벨) 또는 잘못된 행위에 대한 라벨값(예를 들어 미리 설정된 거절/REJECT 라벨)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련해서 생성된 제 1 영상에 대해서, 해당 특정 주제와 관련한 전문가인 해당 단말의 사용자 입력에 따라, 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 선택라벨(또는 추가 선택라벨값)을 각각 설정(또는 수신/입력)한다. 이때, 상기 단말은 해당 단말의 사용자 입력에 따라 시계열 분할 선택라벨링 기능 또는 신체부위별 선택라벨링 기능을 수행한다. 상기 단말은 다음의 과정을 통해 시계열 분할 선택라벨링 기능을 수행한다. 즉, 상기 단말은 상기 제 1 영상을 분할한 복수의 서브 영상에 대해서, 사용자 입력에 따라 각각의 서브 영상의 분할 상태가 잘된 상태(또는 잘된 행위)에 대한 라벨값(예를 들어 미리 설정된 승인/승낙/ACCEPT 라벨) 또는 잘못된 상태(또는 잘못된 행위)에 대한 라벨값(예를 들어 미리 설정된 거절/REJECT 라벨)을 각각 입력받고, 해당 복수의 서브 영상의 순서를 정렬하기 위해서 사용자 입력에 따라 해당 복수의 서브 영상의 순서 를 나타내는 라벨값(또는 분할 시점이 잘못되거나 조정이 필요한 경우 분할 시점을 조정하기 위한 라벨값)을 입 력받는다. 여기서, 상기 제 1 영상에 대한 복수의 서브 영상으로의 분할은 상기 로우 데이터에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서브 로우 데이터에 대한 정보를 근거로 상기 제 1 영상을 상기 복수의 서브 영상으로 분할한 상태이거나 또는, 상기 서버에서의 상기 로우 데이터에 대한 인공지능 기능이나 영상 분 석 기능 수행에 따라 상기 제 1 영상을 상기 복수의 서브 영상으로 분할한 상태일 수 있다. 이에 따라, 상기 단말은 상기 제 1 영상을 대상으로 해당 단말의 사용자 입력에 따라, 복수의 서브 영상의 분할 상태가 잘된 상태와 분할 상태가 잘못된 상태에 대한 라벨값을 각각 입력받고, 해당 복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값(또는 해당 복수의 서브 영상의 순서를 나타내는 라벨값/분할 시점이 잘못되거나 조정이 필요한 경우 분할 시점을 조정하기 위한 라벨값)을 각각 입력받는다. 또한, 상기 단말은 상기 입력된 상기 복수의 서브 영상의 분할 상태가 잘된 상태와 잘못된 상태에 대한 라 벨값, 상기 복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값(또는 분할 시점을 조정하기 위한 라벨값), 상기 단말의 식별 정보 등을 상기 서버에 전송한다. 또한, 상기 서버는 해당 제 1 영상을 대상으로 한 시계열 분할 선택라벨링 기능 수행에 따라, 상기 단말 로부터 전송되는 상기 복수의 서브 영상의 분할 상태가 잘된 상태와 잘못된 상태에 대한 라벨값, 상기 복 수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값(또는 분할 시점을 조정하기 위한 라벨값), 상기 단말(10 0)의 식별 정보 등을 수신한다. 또한, 상기 서버는 상기 수신된 상기 복수의 서브 영상의 분할 상태가 잘된 상태와 잘못된 상태에 대한 라 벨값, 상기 복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값(또는 분할 시점을 조정하기 위한 라벨값) 등 을 근거로 상기 제 1 영상을 분할한 복수의 서브 영상의 순서를 재정렬한다. 이와 같이, 상기 시계열 분할 선택라벨링은 상기 제 1 영상이 복수의 서브 영상으로 분할된 경우, 상기 단말 의 사용자 입력에 따라, 해당 제 1 영상의 복수의 서브 영상으로의 각각의 분할 시점(예를 들어 라벨값, 정지영상정보 등 포함)이 맞는지 또는 틀리는지에 대해 라벨링하고, 분할 시점이 잘못된 경우 분할 시점 또는 순서를 조정하기 위한 라벨값에 대해 라벨링하는 과정일 수 있다. 또한, 상기 단말은 다음의 과정을 통해 신체부위별 선택라벨링 기능을 수행한다. 즉, 상기 단말은 상기 제 1 영상을 분할한 복수의 서브 영상에 포함된 아바타(또는 객체)에 대해서, 사용자 입 력에 따라 상기 복수의 서브 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값)을 각각 입력받고, 해당 복수의 서브 영상에 포함된 아 바타, 인간, 로봇 등의 동작에서 신체부위(또는 로봇의 각 부위)별로 동작 순서를 정렬하기 위해서 사용자 입력 에 따라 해당 복수의 서브 영상의 순서를 나타내는 라벨값(또는 아바타가 포함된 서브 영상의 순서를 조정하기 위한 라벨값)을 입력받는다. 이와 같은 신체부위별 선택은 사용자에 의해 실행되거나 생략될 수 있고, 서버 에 의해 자동으로 실행될 수도 있다(계층라벨링). 여기서, 상기 제 1 영상에 대한 복수의 서브 영상으로의 분할은 상기 로우 데이터에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서브 로우 데이터에 대한 정보를 근거로 상기 제 1 영상을 상기 복수의 서브 영상으로 분할한 상태이거나 또는, 상기 서버에서의 상기 로우 데이터에 대한 인공지능 기능이나 영상 분석 기능 수행에 따라 상기 제 1 영상을 상기 복수의 서브 영상으로 분 할한 상태일 수 있다. 이에 따라, 상기 단말은 상기 제 1 영상을 대상으로 해당 단말의 사용자 입력에 따라, 복수의 서브 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타, 로봇 등의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값)을 각각 입력받고, 해당 복수의 서브 영상에 대한 순서(또는 해당 복수의 서브 영상에 포함된 아바타, 로봇 등의 동작 순서)를 정렬하기 위한 라벨값(또는 해당 복수의 서브 영상의 순서 를 나타내는 라벨값/아바타, 로봇이 포함된 서브 영상의 순서를 조정하기 위한 라벨값)을 각각 입력받는다. 또한, 상기 단말은 상기 입력된 상기 복수의 서브 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타, 로봇 등의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값), 상기 복수의 서 브 영상에 대한 순서(또는 해당 복수의 서브 영상에 포함된 아바타, 로봇 등의 동작 순서)를 정렬하기 위한 라벨값, 상기 단말의 식별 정보 등을 상기 서버에 전송한다. 또한, 상기 서버는 해당 제 1 영상을 대상으로 한 신체부위별 선택라벨링 기능 수행에 따라, 상기 단말 로부터 전송되는 상기 복수의 서브 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해 당 아바타의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값), 상기 복수의 서브 영상에 대한 순서(또 는 해당 복수의 서브 영상에 포함된 아바타의 동작 순서)를 정렬하기 위한 라벨값, 상기 단말의 식별 정보 등을 수신한다. 또한, 상기 서버는 상기 수신된 상기 복수의 서브 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값), 상기 복수의 서브 영상에 대한 순서(또는 해당 복수의 서브 영상에 포함된 아바타의 동작 순서)를 정렬하기 위한 라벨값 등을 근거로 상 기 제 1 영상을 분할한 복수의 서브 영상의 순서를 재정렬한다. 이와 같이, 상기 신체부위별 선택라벨링은 상기 제 1 영상이 복수의 서브 영상으로 분할된 경우, 상기 단말 의 사용자 입력에 따라, 해당 제 1 영상의 분할된 복수의 서브 영상에 각각 포함된 아바타(또는 객체)의 동작 순서가 맞는지 또는 틀리는지에 대해 라벨링하고, 해당 아바타의 동작 순서를 조정하기 위해서 상기 복수 의 서브 영상에 대한 순서(또는 해당 복수의 서브 영상에 포함된 아바타의 동작 순서)를 정렬하기 위한 라벨값 에 대해 라벨링하는 과정일 수 있다. 또한, 상기 신체부위별 선택라벨링 기능은 다음의 기능을 더 포함한다. 즉, 상기 서버는 상기 분할된 복수의 서브 영상에 대해서 상기 서버에서의 인공지능 기능이나 영상 분석 기능 수행에 따라 상기 복수의 서브 영상에 포함된 아바타, 로봇 등의 동작 순서에 대한 정보를 상기 단말 에 제공한다. 또한, 상기 단말은 상기 단말에서 사용자 입력에 따라 해당 복수의 서브 영상에 포함된 아바타(또는 로봇)의 동작 순서에 대해 잘된 상태 또는 잘못된 상태에 대해서 라벨링하고, 아바타(또는 로봇)의 동작 순서가 잘못되거나 조정이 필요한 경우 동작 순서 또는 아바타, 로봇이 포함된 서브 영상의 순서를 조정하기 위한 라벨 값 등을 입력받고, 입력받은 해당 복수의 서브 영상에 포함된 아바타(또는 로봇)의 동작 순서에 대해 잘된 상태 또는 잘못된 상태에 대한 라벨값, 아바타(또는 로봇)의 동작 순서가 잘못되거나 조정이 필요한 경우 동작 순서 또는 아바타(또는 로봇)가 포함된 서브 영상의 순서를 조정하기 위한 라벨값, 상기 단말의 식별 정보 등을 상기 서버에 전송한다. 또한, 상기 서버는 상기 단말로부터 전송되는 해당 복수의 서브 영상에 포함된 아바타(또는 로봇)의 동작 순서에 대해 잘된 상태 또는 잘못된 상태에 대한 라벨값, 아바타(또는 로봇)의 동작 순서가 잘못되거나 조 정이 필요한 경우 동작 순서 또는 아바타(또는 로봇)가 포함된 서브 영상의 순서를 조정하기 위한 라벨값, 상기 단말의 식별 정보 등을 수신한다. 또한, 상기 서버는 상기 수신된 해당 복수의 서브 영상에 포함된 아바타(또는 로봇)의 동작 순서에 대해 잘된 상태 또는 잘못된 상태에 대한 라벨값, 아바타(또는 로봇)의 동작 순서가 잘못되거나 조정이 필요한 경우 동작 순서 또는 아바타(또는 로봇)가 포함된 서브 영상의 순서를 조정하기 위한 라벨값 등을 근거로 상기 제 1 영상을 분할한 복수의 서브 영상의 순서를 재정렬한다. 또한, 상기 단말은 상기 제 1 영상과 관련한 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 의 하나 이상의 추가 선택라벨값, 하나 이상의 시계열 분할 선택라벨값, 하나 이상의 신체부위별 선택라벨값, 복수의 서브 영상의 순서를 정렬하기 위한 라벨값, 해당 단말의 식별 정보 등을 상기 서버에 전송한 다. 또한, 상기 단말은 상기 서버와 연동하여, 해당 제 1 영상을 대상으로 추가 선택라벨링을 수행하기 이전에 또는 수행한 이후에, 해당 하나 이상의 제 1 영상을 대상으로 추가 계층라벨링을 수행하고, 추가 계층라 벨링 수행 전/후로 해당 제 1 영상을 대상으로 추가 선택라벨링을 수행할 수도 있다. 여기서, 상기 추가 계층라 벨링(또는 추가 계층레이블링/2차 계층라벨링/제 2 계층라벨링)은 사용자에 의한 입력 피처 엔지니어링으로, 해 당 제 1 영상에 대한 특징을 나타내는 라벨(또는 라벨값)을 붙이고, 해당 제 1 영상을 특징에 따라 복수의 서브 영상으로 분할(또는 분류)하는 라벨링 방법을 나타낸다. 즉, 상기 단말은 상기 서버와 연동하여, 해당 단말에 표시되는 제 1 영상에 대해서, 해당 특정 주제와 관련해서 미리 설정된 복수의 라벨 분류를 참조하여(또는 근거로), 해당 단말의 사용자 입력(또는사용자 선택/터치/제어)에 따라, 해당 제 1 영상 중 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 추가 라 벨(또는 추가 라벨값)을 설정(또는 수신/입력)한다. 이때, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 제 1 영상을 상기 영상 표시 영역에 표 시(또는 출력)하고, 상기 제 1 영상과 관련한 비교 대상 영상(또는 상기 서버로부터 제공받은 해당 로우 데이터/제 1 영상에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표시(또는 출력)한다. 이때, 상기 단말은 상기 제 1 영상 및 상기 비교 대상 영상에 각각 대응하는 메타 정보를 근거로 해당 제 1 영상 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 제 1 영상 및 비교 대상 영상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 여기서, 상기 단말 내의 상기 영상 표시 영역에 표시되는 제 1 영상 및 상기 비교 대상 영상 표시 영역에 표시되는 비교 대상 영상 중에서 어느 하나가 일시정지 기능 또는 멈춤 기능에 의해 멈추는 경우, 상기 단말은 다른 하나도 함께 일시정지 기능 또는 멈 춤 기능에 의해 멈추도록 제어한다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 제 1 영상에 대해서 해당 단말의 사 용자 입력(또는 사용자 선택/터치/제어)에 따라, 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 해당 제 1 영상에 포함된 객체의 움직임(또는 객체의 행위)에 대한 하나 이상의 단계별 추가 라벨(또는 추가 라벨값)을 설 정(또는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 제 1 영상의 하나 이상의 또 다른 특정 시점(또는 또 다 른 특정 구간)에서 사용자 입력에 따라 해당 제 1 영상에 포함된 객체의 움직임(또는 객체의 행위)에 대해 계층 적으로 객체의 특정 동작, 특정 동작의 특정 방식, 특정 방식의 특정 단계 등에 대해 추가 계층라벨(또는 추가 계층라벨값)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련한 제 1 영상에 대해서, 해당 특정 주제와 관련한 전문가인 해당 단말의 사용자 입력에 따라, 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상 의 추가 계층라벨(또는 추가 계층라벨값)을 각각 설정(또는 수신/입력)한다. 또한, 상기 단말은 추가 계층라벨링 과정을 수행한 전/후로, 앞서 설명한 추가 선택라벨링 과정을 수행한 다. 이와 같이, 상기 단말은 상기 제 1 영상을 대상으로 추가 계층라벨링 기능, 추가 선택라벨링 기능 등을 수 행한다. 본 발명의 실시예에서는 상기 추가 계층라벨링 기능 및 상기 추가 선택라벨링 기능을 분리하여 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 단말은 상기 추가 계층라벨링 기능을 상기 추가 선택라벨링 기 능에 포함시켜 수행할 수 있으며, 또한 상기 추가 계층라벨링과 상기 추가 선택라벨링을 하나의 추가 라벨링 기 능으로 통합하여 수행할 수도 있다. 또한, 상기 단말은 상기 서버로부터 전송되는 제 2 영상을 수신한다. 여기서, 상기 제 2 영상은 상기 서버에서의 해당 제 1 영상을 대상으로 한 분류 모델 및 예측 모델에 의한 학습 결과로 생성된 결과물로, 상기 제 1 영상을 근거로 생성되는 아바타, 아이템, 로봇 등의 동작 관련 영상, 상기 제 1 영상이 업데이트된 영상 등일 수 있다. 또한, 상기 단말은 상기 수신된 제 2 영상을 상기 영상 표시 영역에 출력한다. 이때, 상기 단말은 상 기 로우 데이터, 상기 비교 대상 영상, 상기 제 1 영상 및 상기 제 2 영상을 동기화한 상태에서 해당 단말(10 0)의 화면을 분할하여 동시에 출력할 수도 있다. 또한, 상기 단말은 상기 특정 주제(또는 상기 로우 데이터)와 관련해서 최신의 집단 지성화된 제 2 영상 (또는 업데이트된 제 2 영상)을 상기 서버로부터 제공받을 수 있다. 또한, 상기 단말은 특정 주제와 관련해서, 상기 단말에서 출력되는 아바타, 아이템, 로봇 등의 동작 관련 영상(또는 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상), 해당 동작 관련 영상과 관련한 메 타 정보 등을 상기 서버에 전송한다. 여기서, 상기 특정 주제(또는 특정 콘텐츠)는 의료 행위(예를 들어 시술, 수술 등 포함), 댄스, 운동 종목(예를 들어 축구, 농구, 탁구 등 포함), 게임, 이-스포츠 등을 포함한다. 또한, 상기 아바타 및/또는 아이템의 동작 관련 영상은 해당 특정 주제와 관련한 임의의 로우 데이터를 근거로 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정 등을 통해 생성된 영상일 수 있다. 상기 로봇 영상은 실제 현실의 로봇 동작을 시각세트장치로 수집한 영상(또는 로우 데이터)이다. 또한, 상기 단말은 상기 서버와 연동하여, 해당 단말에 표시되는 로봇 동작 영상(도 29, 기초로 보틱스 영상)에 대해서, 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 로봇 동작 영상 중 특정 시점(또는 특정 구간)에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 로봇 동작 영상을 상기 영상 표시 영역 에 표시(또는 출력)하고, 상기 로봇 동작 영상에 대응하는 비교 대상 영상(또는 상기 서버로부터 제공받은 해당 로봇 동작 영상에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표시(또는 출력)한다. 이 때, 상기 단말은 상기 로봇 동작 영상 및 상기 비교 대상 영상에 각각 대응하는 메타 정보를 근거로 해당 로봇 동작 영상 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 로봇 동작 영상 및 비교 대상 영 상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 여기서, 상기 단말 내의 상기 영상 표시 영역에 표시되는 로봇 동작 영상 및 상기 비교 대상 영상 표시 영역에 표시되는 비교 대상 영상 중에서 어느 하나가 일시정지 기능 또는 멈춤 기능에 의해 멈추는 경우, 상기 단말은 다른 하나도 함 께 일시정지 기능 또는 멈춤 기능에 의해 멈추도록 제어한다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 로봇 동작 영상에 대해서 해당 단말(10 0)의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 특정 시점(또는 특정 구간)에서의 해당 로봇 동작 영상 에 포함된 객체의 움직임(또는 객체의 행위)에 대한 잘된 행위 또는 잘못된 행위에 대해 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 로봇 동작 영상의 하나 이상의 특정 시점에서 사용자 입 력에 따라 잘된 행위에 대한 라벨값(예를 들어 미리 설정된 승인/승낙/ACCEPT 라벨) 또는 잘못된 행위에 대한 라벨값(예를 들어 미리 설정된 거절/REJECT 라벨)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련한 로봇 동작 영상에 대해서, 해당 특정 주제와 관련한 전 문가인 해당 단말의 사용자 입력에 따라, 하나 이상의 특정 시점(또는 특정 구간)에서 하나 이상의 선택라 벨(또는 선택라벨값)을 각각 설정(또는 수신/입력)한다. 여기서, 상기 선택라벨링(또는 선택레이블링/1차 선택 라벨링/제 1 선택라벨링)은 상기 로봇 동작 영상의 특정 시점(또는 특정 구간)에서의 오류(또는 이상) 유무에 대한 라벨(label)(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 로봇 동작 영상 중에서 상기 선택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점(또는 구간)은 미리 설정된 디폴트 라 벨값(예를 들어 승인 라벨)이 설정될 수 있다. 또한, 상기 단말은 상기 로봇 동작 영상 중에서 상기 승인 라벨이 붙지 않은 시점(또는 구간/속성/타깃 속성)에는 미리 설정된 not ACCEPT 라벨을 붙이고, 상기 로봇 동작 영상 중에서 상기 거절 라벨이 붙지 않은 시점(또는 구간/속성/타깃 속성)에는 미리 설정된 not REJECT 라벨을 붙일 수도 있다. 또한, 상기 단말은 상기 로봇 동작 영상과 관련한 하나 이상의 특징 시점(또는 특정 구간)에서의 하나 이 상의 선택라벨값, 해당 로봇 동작 영상의 메타 정보, 해당 단말의 식별 정보 등을 상기 서버에 전송 한다. 또한, 상기 단말은 상기 서버와 연동하여, 해당 로봇 동작 영상을 대상으로 선택라벨링을 수행하기 이전에 또는 수행한 이후에, 해당 로봇 동작 영상을 대상으로 계층라벨링을 수행하고, 계층라벨링 수행 전/후로 해당 로봇 동작 영상을 대상으로 선택라벨링을 수행할 수도 있다. 여기서, 상기 계층라벨링(또는 계층레이블 링)은 사용자에 의한 입력 피처 엔지니어링으로, 해당 로봇 동작 영상에 대한 특징을 나타내는 라벨을 붙이고, 해당 로봇 동작 영상을 특징에 따라 복수의 서브 로봇 동작 영상으로 분할(또는 분류)하는 라벨링 방법을 나타 낸다. 즉, 상기 단말은 상기 서버와 연동하여, 해당 단말에 표시되는 로봇 동작 영상에 대해서, 해당 특정 주제와 관련해서 미리 설정된 복수의 라벨 분류를 참조하여(또는 근거로), 해당 단말의 사용자 입력 (또는 사용자 선택/터치/제어)에 따라, 해당 로봇 동작 영상 중 다른 특정 시점(또는 다른 특정 구간)에서의 라 벨(또는 라벨값)을 설정(또는 수신/입력)한다. 이때, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 로봇 동작 영상을 상기 영상 표시 영역 에 표시(또는 출력)하고, 상기 로봇 동작 영상에 대응하는 비교 대상 영상(또는 상기 서버로부터 제공받은해당 로봇 동작 영상에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표시(또는 출력)한다. 이 때, 상기 단말은 상기 로봇 동작 영상 및 상기 비교 대상 영상에 각각 대응하는 메타 정보를 근거로 해당 로봇 동작 영상 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 로봇 동작 영상 및 비교 대상 영 상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 여기서, 상기 단말 내의 상기 영상 표시 영역에 표시되는 로봇 동작 영상 및 상기 비교 대상 영상 표시 영역에 표시되는 비교 대상 영상 중에서 어느 하나가 일시정지 기능 또는 멈춤 기능에 의해 멈추는 경우, 상기 단말은 다른 하나도 함 께 일시정지 기능 또는 멈춤 기능에 의해 멈추도록 제어한다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 로봇 동작 영상에 대해서 해당 단말(10 0)의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 다른 특정 시점(또는 다른 특정 구간)에서의 해당 로봇 동작 영상에 포함된 객체의 움직임(또는 객체의 행위)에 대한 하나 이상의 단계별 라벨(또는 라벨값)을 설정(또 는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 로봇 동작 영상의 하나 이상의 다른 특정 시점(또는 다 른 특정 구간)에서 사용자 입력에 따라 해당 로봇 동작 영상에 포함된 객체의 움직임(또는 객체의 행위)에 대해 계층적으로 객체의 특정 동작, 특정 동작의 특정 방식, 특정 방식의 특정 단계 등에 대해 계층라벨(또는 계층라 벨값)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련한 로봇 동작 영상에 대해서, 해당 특정 주제와 관련한 전 문가인 해당 단말의 사용자 입력에 따라, 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서 하나 이 상의 계층라벨(또는 계층라벨값)을 각각 설정(또는 수신/입력)한다. 또한, 상기 단말은 계층라벨링 과정을 수행한 전/후로, 앞서 설명한 선택라벨링 과정을 수행한다. 또한, 상기 단말은 상기 서버로부터 전송되는 제 1 로보틱스 영상을 수신한다. 여기서, 상기 제 1 로 보틱스 영상은 상기 서버에서의 해당 로봇 동작 영상을 대상으로 한 분류 모델 및 예측 모델에 의한 학습 결과로 생성된 결과물로, 상기 로봇 동작 영상을 근거로 생성되는 아바타, 아이템, 로봇 등의 동작 관련 영상, 상기 로우 데이터가 업데이트된 영상(예를 들어 상기 로우 데이터에 포함된 인간/사람의 동작/행위/행동이 업데 이트된 영상) 등일 수 있다. 또한, 상기 단말은 상기 수신된 제 1 로보틱스 영상을 상기 영상 표시 영역에 출력한다. 이때, 상기 단말 은 상기 로봇 동작 영상, 상기 비교 대상 영상 및 상기 제 1 로보틱스 영상을 동기화한 상태에서 해당 단 말의 화면을 분할하여 동시에 출력할 수도 있다. 또한, 상기 단말은 상기 서버와 연동하여, 상기 제 1 로보틱스 영상을 대상으로 추가 선택라벨링을 수행한다. 여기서, 상기 추가 선택라벨링(또는 추가 선택레이블링/2차 선택라벨링/제 2 선택라벨링)은 상기 제 1 로보틱스 영상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 오류(또는 이상) 유무에 대한 라벨(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 제 1 로보틱스 영상 중에서 상기 추가 선택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점(또는 구간)은 미리 설정된 디폴트 라벨값(예를 들 어 승인 라벨)이 설정될 수 있다. 또한, 상기 단말은 상기 제 1 로보틱스 영상 중에서 상기 승인 라벨이 붙지 않은 시점(또는 구간/속성/타깃 속성)에는 미리 설정된 not ACCEPT 라벨을 붙이고, 상기 제 1 로보틱스 영 상 중에서 상기 거절 라벨이 붙지 않은 시점(또는 구간/속성/타깃 속성)에는 미리 설정된 not REJECT 라벨을 붙 일 수도 있다. 상기 제 2 선택라벨링은 도 19의 제 1 로보틱스 선택라벨링에 해당될 수 있다. 즉, 상기 단말은 상기 서버와 연동하여, 해당 단말에 표시되는 제 1 로보틱스 영상에 대해서, 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 제 1 로보틱스 영상 중 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 제 1 로보틱스 영상을 상기 영상 표시 영역에 표시(또는 출력)하고, 상기 로봇 동작 영상(또는 상기 제 1 로보틱스 영상)에 대응하는 비교 대상 영상 (또는 상기 서버로부터 제공받은 해당 로봇 동작 영상/제 1 로보틱스 영상에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표시(또는 출력)한다. 이때, 상기 단말은 상기 제 1 로보틱스 영상 및 상기 비교 대상 영상에 각각 대응하는 메타 정보를 근거로 해당 제 1 로보틱스 영상 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 제 1 로보틱스 영상 및 비교 대상 영상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다.또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 제 1 로보틱스 영상에 대해서 해당 단말 의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 해당 제 1 로보틱스 영상에 포함된 객체(또는 아바타)의 움직임(또는 객체/아바타의 행위)에 대한 잘된 행위 또 는 잘못된 행위에 대해 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 제 1 로보틱스 영상의 하나 이상의 또 다른 특정 시점에 서 사용자 입력에 따라 잘된 행위에 대한 라벨값(예를 들어 미리 설정된 승인/승낙/ACCEPT 라벨) 또는 잘못된 행위에 대한 라벨값(예를 들어 미리 설정된 거절/REJECT 라벨)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련해서 생성된 제 1 로보틱스 영상에 대해서, 해당 특정 주제 와 관련한 전문가인 해당 단말의 사용자 입력에 따라, 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 선택라벨(또는 추가 선택라벨값)을 각각 설정(또는 수신/입력)한다. 이때, 상기 단말은 해당 단말의 사용자 입력에 따라 시계열 분할 선택라벨링 기능 또는 신체부위별 선택라벨링 기능을 수행한다. 상기 단말은 다음의 과정을 통해 시계열 분할 선택라벨링 기능을 수행한다. 즉, 상기 단말은 상기 제 1 로보틱스 영상을 분할한 복수의 서브 로보틱스 영상에 대해서, 사용자 입력에 따라 각각의 서브 로보틱스 영상의 분할 상태가 잘된 상태(또는 잘된 행위)에 대한 라벨값(예를 들어 미리 설정 된 승인/승낙/ACCEPT 라벨) 또는 잘못된 상태(또는 잘못된 행위)에 대한 라벨값(예를 들어 미리 설정된 거절 /REJECT 라벨)을 각각 입력받고, 해당 복수의 서브 로보틱스 영상의 순서를 정렬하기 위해서 사용자 입력에 따 라 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값(또는 분할 시점이 잘못되거나 조정이 필요한 경 우 분할 시점을 조정하기 위한 라벨값)을 각각 입력받는다. 여기서, 상기 제 1 로보틱스 영상에 대한 복수의 서 브 로보틱스 영상으로 분할은 상기 로봇 동작 영상에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서브 로 봇 동작 영상에 대한 정보를 근거로 상기 제 1 로보틱스 영상을 상기 복수의 서브 로보틱스 영상으로 분할한 상 태이거나 또는, 상기 서버에서의 상기 로봇 동작 영상에 대한 인공지능 기능이나 영상 분석 기능 수행에 따라 상기 제 1 로보틱스 영상을 상기 복수의 서브 로보틱스 영상으로 분할한 상태일 수 있다. 이에 따라, 상기 단말은 상기 제 1 로보틱스 영상을 대상으로 해당 단말의 사용자 입력에 따라, 복수 의 서브 로보틱스 영상의 분할 상태가 잘된 상태와 분할 상태가 잘못된 상태에 대한 라벨값을 각각 입력받고, 해당 복수의 서브 로보틱스 영상에 대한 순서를 정렬하기 위한 라벨값(또는 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값/분할 시점이 잘못되거나 조정이 필요한 경우 분할 시점을 조정하기 위한 라벨값)을 각 각 입력받는다. 또한, 상기 단말은 상기 입력된 상기 복수의 서브 로보틱스 영상의 분할 상태가 잘된 상태와 잘못된 상태 에 대한 라벨값, 상기 복수의 서브 로보틱스 영상에 대한 순서를 정렬하기 위한 라벨값(또는 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값/분할 시점을 조정하기 위한 라벨값), 상기 단말의 식별 정보 등 을 상기 서버에 전송한다. 또한, 상기 서버는 해당 제 1 로보틱스 영상을 대상으로 한 시계열 분할 선택라벨링 기능 수행에 따라, 상 기 단말로부터 전송되는 상기 복수의 서브 로보틱스 영상의 분할 상태가 잘된 상태와 잘못된 상태에 대한 라벨값, 상기 복수의 서브 로보틱스 영상에 대한 순서를 정렬하기 위한 라벨값(또는 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값/분할 시점을 조정하기 위한 라벨값), 상기 단말의 식별 정보 등을 수신한 다. 또한, 상기 서버는 상기 수신된 상기 복수의 서브 로보틱스 영상의 분할 상태가 잘된 상태와 잘못된 상태 에 대한 라벨값, 상기 복수의 서브 로보틱스 영상에 대한 순서를 정렬하기 위한 라벨값(또는 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값/분할 시점을 조정하기 위한 라벨값) 등을 근거로 상기 제 1 로보틱스 영상을 분할한 복수의 서브 로보틱스 영상의 순서를 재정렬한다. 이와 같이, 상기 시계열 분할 선택라벨링은 상기 제 1 로보틱스 영상이 복수의 서브 로보틱스 영상으로 분할된 경우, 상기 단말의 사용자 입력에 따라, 해당 제 1 로보틱스 영상의 복수의 서브 로보틱스 영상으로의 각 각의 분할 시점(예를 들어 라벨값, 정지영상정보 등 포함)이 맞는지 또는 틀리는지에 대해 라벨링하고, 분할 시 점이 잘못된 경우 분할 시점 또는 순서를 조정하기 위한 라벨값에 대해 라벨링하는 과정일 수 있다. 또한, 상기 단말은 다음의 과정을 통해 신체부위별 선택라벨링 기능을 수행한다. 즉, 상기 단말은 상기 제 1 로보틱스 영상을 분할한 복수의 로보틱스 서브 영상에 포함된 아바타(또는 객체)에 대해서, 사용자 입력에 따라 상기 복수의 서브 로보틱스 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값)을 각각 입력받고, 해당 복 수의 서브 로보틱스 영상에 포함된 아바타(또는 객체)의 동작에서 신체부위별로 동작 순서를 정렬하기 위해서 사용자 입력에 따라 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값(또는 아바타가 포함된 서브 로 보틱스 영상의 순서를 조정하기 위한 라벨값)을 입력받는다. 여기서, 상기 제 1 로보틱스 영상에 대한 복수의 서브 로보틱스 영상으로의 분할은 상기 로봇 동작 영상에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서 브 로보틱스 데이터에 대한 정보를 근거로 상기 제 1 로보틱스 영상을 상기 복수의 서브 로보틱스 영상으로 분 할한 상태이거나 또는, 상기 서버에서의 상기 로봇 동작 영상에 대한 인공지능 기능이나 영상 분석 기능 수행에 따라 상기 제 1 로보틱스 영상을 상기 복수의 서브 로보틱스 영상으로 분할한 상태일 수 있다. 이에 따라, 상기 단말은 상기 제 1 로보틱스 영상을 대상으로 해당 단말의 사용자 입력에 따라, 복수 의 서브 로보틱스 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타의 동작 순서 의 잘된 상태 또는 잘못된 상태에 대한 라벨값)을 각각 입력받고, 해당 복수의 서브 로보틱스 영상에 대한 순서 (또는 해당 복수의 서브 로보틱스 영상에 포함된 아바타의 동작 순서)를 정렬하기 위한 라벨값(또는 해당 복수 의 서브 로보틱스 영상의 순서를 나타내는 라벨값/아바타가 포함된 서브 로보틱스 영상의 순서를 조정하기 위한 라벨값)을 각각 입력받는다. 또한, 상기 단말은 상기 입력된 상기 복수의 서브 로보틱스 영상에 포함된 아바타(또는 객체)의 동작 순서 에 대한 라벨값(또는 해당 아바타의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값), 상기 복수의 서 브 로보틱스 영상에 대한 순서(또는 해당 복수의 서브 로보틱스 영상에 포함된 아바타의 동작 순서)를 정렬하기 위한 라벨값(또는 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값), 상기 단말의 식별 정보 등 을 상기 서버에 전송한다. 또한, 상기 서버는 해당 제 1 로보틱스 영상을 대상으로 한 신체부위별 선택라벨링 기능 수행에 따라, 상 기 단말로부터 전송되는 상기 복수의 서브 로보틱스 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값), 상기 복수의 서브 로보틱 스 영상에 대한 순서(또는 해당 복수의 서브 로보틱스 영상에 포함된 아바타의 동작 순서)를 정렬하기 위한 라 벨값(또는 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값), 상기 단말의 식별 정보 등을 수신 한다. 또한, 상기 서버는 상기 수신된 상기 복수의 서브 로보틱스 영상에 포함된 아바타(또는 객체)의 동작 순서 에 대한 라벨값(또는 해당 아바타의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값), 상기 복수의 서 브 로보틱스 영상에 대한 순서(또는 해당 복수의 서브 로보틱스 영상에 포함된 아바타의 동작 순서)를 정렬하기 위한 라벨값(또는 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값) 등을 근거로 상기 제 1 로보틱스 영상을 분할한 복수의 서브 로보틱스 영상의 순서를 재정렬한다. 이와 같이, 상기 신체부위별 선택라벨링은 상기 제 1 로보틱스 영상이 복수의 서브 로보틱스 영상으로 분할된 경우, 상기 단말의 사용자 입력에 따라, 해당 제 1 로보틱스 영상의 분할된 복수의 서브 로보틱스 영상에 각각 포함된 아바타(또는 객체)의 동작 순서가 맞는지 또는 틀리는지에 대한 라벨링하고, 해당 아바타의 동작 순서를 조정하기 위해서 상기 복수의 서브 로보틱스 영상에 대한 순서(또는 해당 복수의 서브 로보틱스 영상에 포함된 아바타의 동작 순서)를 정렬하기 위한 라벨값에 대해 라벨링하는 과정일 수 있다. 또한, 상기 신체부위별 선택라벨링 기능은 다음의 기능을 더 포함한다. 즉, 상기 서버는 상기 분할된 복수의 서브 로보틱스 영상에 대해서 상기 서버에서의 인공지능 기능이 나 영상 분석 기능 수행에 따라 상기 복수의 서브 로보틱스 영상에 포함된 아바타의 동작 순서에 대한 정보를 상기 단말에 제공한다. 또한, 상기 단말은 상기 단말에서 사용자 입력에 따라 해당 복수의 서브 로보틱스 영상에 포함된 아 바타의 동작 순서에 대해 잘된 상태 또는 잘못된 상태에 대해서 라벨링하고, 아바타의 동작 순서가 잘못되거나 조정이 필요한 경우 동작 순서 또는 아바타(또는 인간)가 포함된 서브 로보틱스 영상의 순서를 조정하기 위한 라벨값 등을 입력받고, 입력받은 해당 복수의 서브 로보틱스 영상에 포함된 아바타의 동작 순서에 대해 잘된 상 태 또는 잘못된 상태에 대한 라벨값(예를 들어 선택, 거부 등 포함), 아바타의 동작 순서가 잘못되거나 조정이 필요한 경우 동작 순서 또는 아바타, 인간, 로봇 등이 포함된 서브 로보틱스 영상의 순서를 조정하기 위한 라벨값, 상기 단말의 식별 정보 등을 상기 서버에 전송한다. 또한, 상기 서버는 상기 단말로부터 전송되는 해당 복수의 서브 로보틱스 영상에 포함된 아바타의 동 작 순서에 대해 잘된 상태 또는 잘못된 상태에 대한 라벨값, 아바타의 동작 순서가 잘못되거나 조정이 필요한 경우 동작 순서 또는 아바타가 포함된 서브 로보틱스 영상의 순서를 조정하기 위한 라벨값, 상기 단말의 식별 정보 등을 수신한다. 또한, 상기 서버는 상기 수신된 해당 복수의 서브 로보틱스 영상에 포함된 아바타의 동작 순서에 대해 잘 된 상태 또는 잘못된 상태에 대한 라벨값, 아바타의 동작 순서가 잘못되거나 조정이 필요한 경우 동작 순서 또 는 아바타가 포함된 서브 로보틱스 영상의 순서를 조정하기 위한 라벨값 등을 근거로 상기 제 1 로보틱스 영상 을 분할한 복수의 서브 로보틱스 영상의 순서를 재정렬한다. 또한, 상기 단말은 상기 제 1 로보틱스 영상과 관련한 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값, 하나 이상의 시계열 분할 선택라벨값, 하나 이상의 신체부위별 선택 라벨값, 해당 복수의 서브 로보틱스 영상의 순서를 정렬하기 위한 라벨값, 해당 단말의 식별 정보 등을 상 기 서버에 전송한다. 또한, 상기 단말은 상기 서버와 연동하여, 해당 제 1 로보틱스 영상을 대상으로 추가 선택라벨링을 수행하기 이전에 또는 수행한 이후에, 해당 하나 이상의 제 1 로보틱스 영상을 대상으로 추가 계층라벨링을 수 행하고, 추가 계층라벨링 수행 전/후로 해당 제 1 로보틱스 영상을 대상으로 추가 선택라벨링을 수행할 수도 있 다. 여기서, 상기 추가 계층라벨링(또는 추가 계층레이블링)은 사용자에 의한 입력 피처 엔지니어링으로, 해당 제 1 로보틱스 영상에 대한 특징을 나타내는 라벨(또는 라벨값)을 붙이고, 해당 제 1 로보틱스 영상을 특징에 따라 복수의 서브 로보틱스 영상으로 분할(또는 분류)하는 라벨링 방법을 나타낸다. 즉, 상기 단말은 상기 서버와 연동하여, 해당 단말에 표시되는 제 1 로보틱스 영상에 대해서, 해당 특정 주제와 관련해서 미리 설정된 복수의 라벨 분류를 참조하여(또는 근거로), 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 제 1 로보틱스 영상 중 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 추가 라벨(또는 추가 라벨값)을 설정(또는 수신/입력)한다. 이때, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 제 1 로보틱스 영상을 상기 영상 표시 영역에 표시(또는 출력)하고, 상기 제 1 로보틱스 영상과 관련한 비교 대상 영상(또는 상기 서버로부터 제 공받은 해당 로봇 동작 영상/제 1 로보틱스 영상에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역 에 표시(또는 출력)한다. 이때, 상기 단말은 상기 제 1 로보틱스 영상 및 상기 비교 대상 영상에 각각 대 응하는 메타 정보를 근거로 해당 제 1 로보틱스 영상 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기 화된 제 1 로보틱스 영상 및 비교 대상 영상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 여기서, 상기 단말 내의 상기 영상 표시 영역에 표시되는 제 1 로보틱스 영상 및 상기 비 교 대상 영상 표시 영역에 표시되는 비교 대상 영상 중에서 어느 하나가 일시정지 기능 또는 멈춤 기능에 의해 멈추는 경우, 상기 단말은 다른 하나도 함께 일시정지 기능 또는 멈춤 기능에 의해 멈추도록 제어한다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 제 1 로보틱스 영상에 대해서 해당 단말 의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 해당 제 1 로보틱스 영상에 포함된 객체의 움직임(또는 객체의 행위)에 대한 하나 이상의 단계별 추가 라벨(또 는 추가 라벨값)을 설정(또는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 제 1 로보틱스 영상의 하나 이상의 또 다른 특정 시점 (또는 또 다른 특정 구간)에서 사용자 입력에 따라 해당 제 1 로보틱스 영상에 포함된 객체의 움직임(또는 객체 의 행위)에 대해 계층적으로 객체의 특정 동작, 특정 동작의 특정 방식, 특정 방식의 특정 단계 등에 대해 추가 계층라벨(또는 추가 계층라벨값)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련한 제 1 로보틱스 영상에 대해서, 해당 특정 주제와 관련한 전문가인 해당 단말의 사용자 입력에 따라, 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 계층라벨(또는 추가 계층라벨값)을 각각 설정(또는 수신/입력)한다. 또한, 상기 단말은 추가 계층라벨링 과정을 수행한 전/후로, 앞서 설명한 추가 선택라벨링 과정을 수행한 다.이와 같이, 상기 단말은 상기 제 1 로보틱스 영상을 대상으로 추가 계층라벨링 기능, 추가 선택라벨링 기 능 등을 수행한다. 본 발명의 실시예에서는 상기 추가 계층라벨링 기능 및 상기 추가 선택라벨링 기능을 분리하여 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 단말은 상기 추가 계층라벨링 기능을 상기 추가 선택라벨링 기 능에 포함시켜 수행할 수 있으며, 또한 상기 추가 계층라벨링과 상기 추가 선택라벨링을 하나의 추가 라벨링 기 능으로 통합하여 수행할 수도 있다. 또한, 상기 단말은 상기 서버로부터 전송되는 제 2 로보틱스 영상을 수신한다. 여기서, 상기 제 2 로 보틱스 영상은 상기 서버에서의 해당 제 1 로보틱스 영상을 대상으로 한 분류 모델 및 예측 모델에 의한 학습 결과로 생성된 결과물로, 상기 제 1 로보틱스 영상을 근거로 생성되는 아바타, 아이템, 로봇 등의 동작 관 련 영상, 상기 제 1 로보틱스 영상이 업데이트된 영상 등일 수 있다. 또한, 상기 단말은 상기 수신된 제 2 로보틱스 영상을 상기 영상 표시 영역에 출력한다. 이때, 상기 단말 은 상기 로봇 동작 영상, 상기 비교 대상 영상, 상기 제 1 로보틱스 영상 및 상기 제 2 로보틱스 영상을 동기화한 상태에서 해당 단말의 화면을 분할하여 동시에 출력할 수도 있다. 또한, 상기 단말은 상기 특정 주제(또는 상기 로우 데이터)와 관련해서 최신의 집단 지성화된 제 2 로보틱 스 영상(또는 업데이트된 제 2 로보틱스 영상)을 상기 서버로부터 제공받을 수 있다. 본 발명의 실시예에서는 상기 단말에서 전용 앱 형태로 로우 데이터 수집 기능, 정보/영상에 대한 계층라 벨링 기능, 정보/영상에 대한 선택라벨링 기능, 정보/영상에 대한 시계열 분할 선택라벨링 기능, 정보/영상에 대한 신체부위별 선택라벨링 기능 등을 수행하는 것을 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 전용 앱 이외에도 상기 서버에 제공하는 웹 사이트 등을 통해 상기 로우 데이터 수집 기능, 상기 정보/영상에 대한 계층라벨링 기능, 상기 정보/영상에 대한 선택라벨링 기능, 상기 정보/영상에 대한 시계열 분할 선택라벨 링 기능, 상기 정보/영상에 대한 신체부위별 선택라벨링 기능 등을 수행할 수도 있다. 상기 서버는 상기 단말 등과 통신한다. 또한, 상기 서버는 상기 단말 등의 사용자에 대한 회원 가입 절차 등을 수행한다. 또한, 상기 서버는 상기 단말 등의 사용자와 관련한 개인 정보를 등록한다. 이때, 상기 서버는 해당 개인 정보 등을 DB 서버(미도시)에 등록(또는 관리)할 수 있다. 또한, 상기 서버는 상기 단말 등의 사용자에 대한 회원 관리 기능을 수행한다. 또한, 상기 서버는 로우 데이터 수집 기능, 정보/영상에 대한 계층라벨링 기능, 정보/영상에 대한 선택라 벨링 기능, 정보/영상에 대한 시계열 분할 선택라벨링 기능, 정보/영상에 대한 신체부위별 선택라벨링 기능 등 을 제공하는 전용 앱 및/또는 웹 사이트를 상기 단말 등에 제공한다. 또한, 상기 서버는 공지사항, 이벤트 등을 위한 게시판 기능을 제공한다. 또한, 상기 서버는 상기 단말 및 상기 결제 서버와 연동하여, 해당 서버에서 제공하는 로우 데 이터 수집 기능, 정보/영상에 대한 계층라벨링 기능, 정보/영상에 대한 선택라벨링 기능, 정보/영상에 대한 시 계열 분할 선택라벨링 기능, 정보/영상에 대한 신체부위별 선택라벨링 기능 등에 대해서 해당 단말에서의 구독 기능 수행에 따른 결제 기능을 수행한다. 결제 기능이 실패한 경우, 상기 서버는 결제 실패 정보(예를 들어 결제일자, 결제금액, 실패 정보(예를 들 어 잔액 부족, 한도 초과 등 포함) 등 포함)(또는 결제가 실패한 상태임을 나타내는 정보)를 상기 단말로 제공한다. 또한, 상기 서버는 상기 단말과의 결제 기능이 정상적으로 수행된 후, 상기 결제 서버로부터 제공되 는 결제 기능 수행 결과를 상기 단말에 각각 전송한다. 여기서, 상기 결제 기능 수행 결과는 구독 기간, 결제 금액, 결제 일자 및 시각 정보 등을 포함한다. 또한, 상기 서버는 결제 기능 수행 결과를 해당 단말(또는 해당 단말과 관련한 계정 정보)과 매 핑하여(또는 매칭하여/연동하여) 관리(또는 저장/등록)한다. 또한, 상기 서버는 상기 구독 기능 수행에 따라, 상기 단말에서 해당 전용 앱을 통해 해당 서버(20 0)에서 제공하는 로우 데이터 수집 기능, 정보/영상에 대한 계층라벨링 기능, 정보/영상에 대한 선택라벨링 기능, 정보/영상에 대한 시계열 분할 선택라벨링 기능, 정보/영상에 대한 신체부위별 선택라벨링 기능 등을 수행 하기 위한 다양한 정보 등을 제공한다. 또한, 상기 서버는 해당 서버의 구성 요소 간 통신 기능을 제공하기 위해서 버스(미도시), 통신 인터 페이스(미도시) 등을 더 포함할 수 있다. 상기 버스는 주소 버스(address bus), 데이터 버스(data bus), 제어 버스(control bus) 등 다양한 형태의 버스 로 구현한다. 상기 통신 인터페이스는 상기 서버의 유/무선 인터넷 통신을 지원한다. 또한, 상기 서버는 컴퓨터 프로그램이 메모리에 로드될 때, 프로세서로 하여금 본 발명의 다양한 실시예에 따른 방법/기능을 수행하도록 하는 하나 이상의 인스트럭션을 포함한다. 즉, 상기 프로세서는 상기 하나 이상의 인스트럭션을 실행함으로써, 본 발명의 다양한 실시예에 따른 상기 방법/기능을 수행한다. 또한, 상기 서버는 사전에 수집된 특정 주제와 관련한 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보, 제 1 영상, 해당 제 1 영상과 관련한 메타 정 보, 제 2 영상, 해당 제 2 영상과 관련한 메타 정보, 아바타 및/또는 아이템의 동작 관련 영상, 해당 동작 관련 영상과 관련한 메타 정보, 제 1 로보틱스 영상, 해당 제 1 로보틱스 영상과 관련한 메타 정보, 제 2 로보틱스 영상, 해당 제 2 로보틱스 영상과 관련한 메타 정보 등을 지속적인 기계학습(또는 딥러닝)의 데이터로 활용한다. 여기서, 상기 기계학습을 위한 입력 데이터세트는 상기 특정 주제와 관련한 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보, 제 1 영상, 해당 제 1 영상과 관련한 메타 정보, 제 2 영상, 해당 제 2 영상과 관련한 메타 정보, 아바타 및/또는 아이템의 동작 관련 영상, 해당 동작 관련 영상과 관련한 메타 정보, 제 1 로보틱스 영상, 해당 제 1 로보틱스 영상과 관련한 메타 정보, 제 2 로보틱스 영상, 해당 제 2 로보틱스 영상과 관련한 메타 정보 등을 미리 설정된 비율(예를 들어 7:3, 8:2 등 포함)로 훈련 세트(train set)와 테스트 세트(test set)로 분할하여, 훈련 및 테스트 기능을 수행 할 수 있다. 또한, 상기 기계학습을 위한 입력 데이터세트는 추후 수집되는 특정 주제와 관련한 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보, 제 1 영상, 해당 제 1 영상과 관련한 메타 정보, 제 2 영상, 해당 제 2 영상과 관련한 메타 정보, 아바타 및/또는 아이템의 동작 관련 영상, 해당 동작 관련 영상과 관련한 메타 정보, 제 1 로보틱스 영상, 해당 제 1 로보틱스 영상과 관 련한 메타 정보, 제 2 로보틱스 영상, 해당 제 2 로보틱스 영상과 관련한 메타 정보 등을 포함한다. 또한, 상기 기계학습을 위한 출력 데이터세트는 예측하고 싶은 부분으로, 수집된 정보 등에 따라 학습하고, 추후에 이를 분 류하거나 예측하여, 해당 로우 데이터, 제 1 영상, 제 2 영상, 동작 관련 영상, 제 1 로보틱스 영상, 제 2 로보 틱스 영상 등과 관련한 라벨을 분류하고, 분류된 정보들을 근거로 생성되는 제 1 영상, 제 2 영상, 제 1 로보틱 스 영상, 제 2 로보틱스 영상 등을 포함한다. 즉, 상기 서버는 미리 설정된 학습용 데이터를 통해 분류 모델에 대해서 사전에 수집된 특정 주제와 관련 한 로우 데이터, 제 1 영상, 아바타 및/또는 아이템의 동작 관련 영상, 제 1 로보틱스 영상 등에 대해서 해당 정보들과 관련한 라벨값을 분류하기 위한 학습 기능을 수행한다. 이때, 상기 서버는 해당 정보들을 병렬 및 분산하여 저장하고, 저장된 정보들 내에 포함된 사전에 수집된 특정 주제와 관련한 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보, 제 1 영상, 해당 제 1 영상과 관련한 메타 정보, 제 2 영상, 해당 제 2 영상과 관련한 메타 정보, 아바타 및/또는 아이템의 동작 관련 영상, 해당 동작 관련 영상과 관련한 메타 정보, 제 1 로보틱스 영상, 해당 제 1 로보틱스 영상과 관련한 메타 정보, 제 2 로보틱스 영상, 해당 제 2 로보틱스 영상과 관련한 메타 정보 등을 비정형(Unstructed) 데이터, 정 형(Structured) 데이터, 반정형 데이터(Semi-structured)를 정제하고, 메타 데이터로 분류를 포함한 전처리를 실시하고, 전처리된 데이터를 데이터 마이닝(Data Mining)을 포함하는 분석을 실시하고 적어도 하나의 종류의 기계학습에 기반하여 학습, 훈련 및 테스트를 진행하여 빅데이터를 구축할 수 있다. 이때, 적어도 하나의 종류 의 기계학습은 지도 학습(Supervised Learning), 반지도 학습(Semi-Supervised Learning), 비지도 학습 (Unsupervised Learning), 강화 학습(Reinforcement Learning) 및 심층 강화 학습(Deep Reinforcement Learning) 중 어느 하나 또는 적어도 하나의 조합으로 이루어질 수 있다. 또한, 상기 서버는 미리 설정된 학습용 데이터를 통해 예측 모델에 대해 사전에 수집된 특정 주제와 관련 해서 상기 분류 모델을 통해 분류된 분류값, 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영 상, 해당 비교 대상 영상과 관련한 메타 정보, 제 1 영상, 해당 제 1 영상과 관련한 메타 정보, 제 2 영상, 해 당 제 2 영상과 관련한 메타 정보, 아바타 및/또는 아이템의 동작 관련 영상, 해당 동작 관련 영상과 관련한 메타 정보, 제 1 로보틱스 영상, 해당 제 1 로보틱스 영상과 관련한 메타 정보, 제 2 로보틱스 영상, 해당 제 2 로보틱스 영상과 관련한 메타 정보 등에 대해서 해당 정보들과 관련한 새로운 영상(예를 들어 제 1 영상, 제 2 영상 등 포함)을 생성하기 위한 학습 기능을 수행한다. 이때, 상기 서버는 해당 정보들을 병렬 및 분산하 여 저장하고, 저장된 정보들 내에 포함된 사전에 수집된 특정 주제와 관련한 상기 분류 모델을 통해 분류된 분 류값, 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메 타 정보, 제 1 영상, 해당 제 1 영상과 관련한 메타 정보, 제 2 영상, 해당 제 2 영상과 관련한 메타 정보, 아 바타 및/또는 아이템의 동작 관련 영상, 해당 동작 관련 영상과 관련한 메타 정보, 제 1 로보틱스 영상, 해당 제 1 로보틱스 영상과 관련한 메타 정보, 제 2 로보틱스 영상, 해당 제 2 로보틱스 영상과 관련한 메타 정보 등 을 비정형 데이터, 정형 데이터, 반정형 데이터를 정제하고, 메타 데이터로 분류를 포함한 전처리를 실시하고, 전처리된 데이터를 데이터 마이닝을 포함하는 분석을 실시하고 적어도 하나의 종류의 기계학습에 기반하여 학습, 훈련 및 테스트를 진행하여 빅데이터를 구축할 수 있다. 이때, 적어도 하나의 종류의 기계학습은 지도 학 습, 반지도 학습, 비지도 학습, 강화 학습 및 심층 강화 학습 중 어느 하나 또는 적어도 하나의 조합으로 이루 어질 수 있다. 이와 같이, 상기 서버는 상기 학습용 데이터 등을 통해서 뉴럴 네트워크(Neural Networks) 형태의 상기 분 류 모델, 상기 예측 모델 등에 대해서 학습 기능을 수행한다. 또한, 상기 서버는 생성적 신경망 알고리즘, 추적 신경망 네트워크 등을 사용한다. 여기서, 상기 추적 신 경망 네트워크는 시퀀셜(sequential) 형태의 입력이 들어오는 모델이면서, 객체의 영상 정보의 xyz 좌표의 상대 값을 4차원 벡터적으로 측정 및 자료 구조화하는 것이 가능한 신경망 알고리즘일 수 있다. 본 발명의 일 실시예에서, 생성적 신경망 알고리즘과 추적 신경망 네트워크로 GNN(Graph Neural Network), GAN(Generative Adversarial Network) 등을 이용한다. 인공지능 알고리즘으로 GAN과 GNN의 조합이 있을 수 있 고, GAN을 제외한 GNN 단독 적용이 있을 수 있고, GNN을 제외한 GAN 단독 적용이 있을 수 있다. GAN을 단독으로 사용하는 경우에는 'GNN 회귀모델 1형' 및 'GNN 회귀모델 2형'을 사용하지 않고, 속성과 타깃 속성의 예측값을 구할 때, 딥러닝 및 연관규칙을 사용한다. GAN은 정지영상이나 동영상의 표현, 화질의 자연스러움, 정교함 등을 보강한다. 다음 동작의 예측을 위해서 동작 패턴의 연관규칙 등으로 추론한다. 로우 데이터인 제 1 기초 영상정보는 제 1 계층라벨링에 의해 군집화된 제 1 속성 및 제 1 타깃 속 성이 된다. 상기 도 3 내지 상기 도 5를 참조하면, 어노테이션 단계의 복수의 기초 영상(또는 로우 데이터/기초 영상 정 보)에 대해 사용자가 제 1 계층라벨링을 하면, 기초 영상 정보는 계층적으로 군집화된다. 이를 제 1 계층 적 군집이라 하며, 기초 영상은 기초영상정보가 상기 단말의 보기 화면으로 출력되는 영상을 나타낸다. 또한, 상기 서버는 상기 단말로부터 전송되는 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보, 단말의 식별 정보 등을 수신한다. 이때, 상기 단말로부터 상기 로우 데이터와 관련한 비교 대상 영상이 전송되지 않은 경우, 상기 서버(20 0)는 상기 수신된 해당 특정 주제와 관련한 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보 등 을 근거로 해당 서버에서 관리 중인 복수의 비교 대상 영상 중에서 상기 로우 데이터와 관련한 비교 대상 영상을 확인(또는 검색)하고, 상기 확인된 해당 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 상기 단말에 제공한다. 또한, 상기 서버는 상기 수신된 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행한다. 여기서, 상 기 선택라벨링(또는 선택레이블링)은 상기 로우 데이터의 특정 시점(또는 특정 구간)에서의 오류(또는 이상) 유 무에 대한 라벨(label)(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 로우 데이 터 중에서 상기 선택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점(또는 구간)은 미리 설정된 디폴트 라벨값(예를 들어 승인 라벨)이 설정될 수 있다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 로우 데이터에 대해서, 해당 단 말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 로우 데이터 중 특정 시점(또는 특정 구간) 에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 서버는 상기 단말로부터 전송되는 상기 로우 데이터와 관련한 하나 이상의 특징 시점(또 는 특정 구간)에서의 하나 이상의 선택라벨값, 해당 로우 데이터의 메타 정보, 해당 단말의 식별 정보 등을 수신한다. 본 발명의 실시예에서는, 상기 단말에서 사용자 입력에 따라 해당 로우 데이터 중 하나 이상의 특정 시점 (또는 특정 구간)에서 하나 이상의 선택라벨값을 설정(또는 수신/입력)하는 것을 주로 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 서버는 해당 로우 데이터 및 해당 로우 데이터와 관련한 비교 대상 영상에 대 한 영상 분석 기능을 수행하고, 영상 분석 기능 수행 결과를 근거로 해당 로우 데이터에 대해서 하나 이상의 특 정 시점(또는 특정 구간)에서 하나 이상의 선택라벨값을 자동으로 각각 설정할 수도 있다. 또한, 상기 서버에서 해당 로우 데이터에 대해 하나 이상의 특정 시점(또는 특정 구간)에서 하나 이상의 선택라벨값을 설정한 경우, 상기 서버는 상기 설정된 해당 로우 데이터와 관련한 하나 이상의 특정 시점 (또는 특정 구간)에서의 하나 이상의 선택라벨값에 대한 정보를 상기 단말에 제공하고, 해당 단말에 서 상기 서버에서 설정된 해당 로우 데이터와 관련한 하나 이상의 특정 시점(또는 특정 구간)에서의 하나 이상의 선택라벨값에 대한 정보를 표시하고, 해당 단말의 사용자 입력에 따라 해당 하나 이상의 특정 시점 (또는 특정 구간)에서의 하나 이상의 선택라벨값에 대해 최종 승인 여부를 결정하도록 구성할 수도 있다. 이때, 해당 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하기 이전에 또는 수행한 이후에, 상기 서버 는 상기 단말과 연동하여, 해당 하나 이상의 로우 데이터를 대상으로 계층라벨링을 수행하고, 계층라 벨링 수행 전/후로 해당 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행할 수도 있다. 여기서, 상기 계 층라벨링(또는 계층레이블링)은 사용자에 의한 입력 피처 엔지니어링(input feature engineering)으로, 해당 로 우 데이터에 대한 특징을 나타내는 라벨(또는 라벨값)을 붙이고, 해당 로우 데이터를 특징에 따라 복수의 서브 로우 데이터로 분할(또는 분류)하는 라벨링 방법을 나타낸다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 로우 데이터에 대해서, 해당 특 정 주제와 관련해서 미리 설정된 복수의 라벨 분류를 참조하여(또는 근거로), 해당 단말의 사용자 입력(또 는 사용자 선택/터치/제어)에 따라, 해당 로우 데이터 중 다른 특정 시점(또는 다른 특정 구간)에서의 라벨(또 는 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 서버는 상기 로우 데이터를 복수의 서브 로우 데이터로 분할한다. 본 발명의 실시예에서는, 상기 단말에서 사용자 입력에 따라 해당 로우 데이터 중 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서 하나 이상의 계층라벨값을 설정(또는 수신/입력)하는 것을 주로 설명하고 있으 나, 이에 한정되는 것은 아니며, 상기 서버는 해당 로우 데이터 및 해당 로우 데이터와 관련한 비교 대상 영상에 대한 영상 분석 기능을 수행하고, 영상 분석 기능 수행 결과를 근거로 해당 로우 데이터에 대해서 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서 하나 이상의 계층라벨값을 자동으로 각각 설정할 수도 있다. 또한, 상기 서버에서 해당 로우 데이터에 대해 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서 하 나 이상의 계층라벨값을 설정한 경우, 상기 서버는 상기 설정된 해당 로우 데이터와 관련한 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서의 하나 이상의 계층라벨값에 대한 정보를 상기 단말에 제공하고, 해당 단말에서 상기 서버에서 설정된 해당 로우 데이터와 관련한 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서의 하나 이상의 계층라벨값에 대한 정보를 표시하고, 해당 단말의 사용자 입력에 따라 해당 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서의 하나 이상의 계층라벨값에 대해 최 종 승인 여부를 결정하도록 구성할 수도 있다. 또한, 상기 서버는 입력 피처 엔지니어링에 관한 라이브러리를 호출하여, 기초영상정보(또는 로우 데이 터)를 입력 피처 벡터(input feature vector)로 변환(conversion)한다. 사용자에 의한 계층라벨링은 기초영상정 보를 데이터 단위 3 또는 데이터 단위 4로 분할하고, 데이터 단위 3 또는 데이터 단위 4의 속성값이 복합적인 입력 피처가 되도록 예측 모델을 지도 학습한다. 상기 복합적인 입력 피처는 포인트 클라우드, RGB, JPG, 동영 상 정보, 복셀(또는 3D 이미지), 벡터 포맷 등이 결합된 기초영상정보가 입력 피처로 변환된 것을 나타낸다. 또한, 사용자에 의한 계층라벨링은 상기 서버가 입력 피처 엔지니어링에 관한 라이브러리를 호출하여 기초 영상정보를 입력 피처 벡터로 변환하는 과정에서 생략될 수 있다. 제 1 계층적 군집이 상기 서버에 의해 스스로 생성될 수 있다. 사용자에 의한 제 1 계층라벨링, 제 2 계층라벨링 등이 일부 혹은 전체를 사용자가 수행하지 않을 경우에, 인공지능이 입력 피처를 스스로 구하는 것을 계층적 군집화 라벨링이 상기 서버에 의해 수행되는 것이라 할 수 있다. 본 발명의 일 실시예에서, 계층적 군집화 라벨링 정보를 수신하는 계층라벨링 정보 수신 단계를 생략한다. 제 1 계층라벨링, 제 2 계층라벨링, 제 3 계층라벨링 등과 같은 사용자에 의한 입력 피처 엔지니어링을 생략하고, 상 기 서버가 스스로 입력 피처를 구한다. 제 1 계층라벨링, 제 2 계층라벨링, 제 3 계층라벨링 등의 반복되 는 계층라벨링과 같은 사용자에 의한 입력 피처 엔지니어링을 생략하고, 상기 서버가 스스로 입력 피처를 구한다. 상기 도 12에서 제 1 계층적군집은 제 1 기초영상정보가 제 1 계층라벨링에 의해 군집화된 것을 나 타낸다. 상기 제 1 계층적 군집은 상기 도 7의 데이터 단위 3 기준으로 계층적군집이 되거나 또는, 상기 도 9의 데이터 단위 4 기준으로 계층적군집이 된다. 본 발명의 일 실시예에서, 계층적 군집은 상기 서버가 스스로 입력 피처를 구하는 방식을 포함한다. 상기 도 12의 제 2 계층라벨링은 상기 단말의 보기 화면에서 출력되는 제 1 영상정보에 대해 계층적 군집 화 라벨링을 진행하는 것으로, 상기 제 1 영상정보에 대해 계층적 군집화 라벨값을 입력하기 위해 사용자는 앞 선 [표 1] 내지 [표 11]의 라벨 분류를 참조한다. 본 발명의 일 실시예에서, 사용자는(또는 단말/서버) 특정 단계별 및/또는 세부동작 단계별 라벨링을 하지 않는다. 상기 서버에 의해 데이터 단위 3 또는 데이터 단위 4 또는 데이터 단위 5로 동영상이 분할될 수 있다. 또한, 상기 서버는 상기 선택라벨링된 로우 데이터에 대한 정보 등을 근거로 인공지능 기반의 기계 학습을 수행하여, 기계 학습 결과를 근거로 해당 로우 데이터에 대한 분류값을 생성(또는 확인)한다. 여기서, 상기 해 당 로우 데이터에 대한 분류값(또는 해당 로우 데이터의 분류값/선택라벨링된 로우 데이터의 분류값/계층라벨링 된 로우 데이터의 분류값)은 선택라벨링값, 계층라벨링값 등을 동일 항목별로 분류한 값일 수 있다. 즉, 상기 서버는 상기 선택라벨링된 로우 데이터에 대한 정보 등을 미리 설정된 분류 모델의 입력값으로 하여 기계 학습(또는 인공지능/딥 러닝)을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거 로 해당 로우 데이터에 대한 분류값을 생성(또는 확인)한다. 다양한 실시예에서, 라벨링 단계에서 아바타, 인간, 로봇 등의 동작을 승인(ACCEPT) 또는 거절(REJECT) 분류하 는 라벨링은 지도학습형태로 진행되며, 이는 분류모델에 해당된다. 승인(ACCEPT) 및 거절(REJECT) 2진 분류는 보편적으로 사용하는 2진 분류(Binary Classification) 모델로 사용 가능하며, 수술 및 동작의 성공 실패를 5단 계의 척도로 표현시에는 각 클래스(class)의 확률값이 도출되는 멀티 분류 모델(multiple classification model)로 구현할 수 있다. 다양한 실시예에서, 앞선 단말의 앱 실행 결과 화면(또는 보기 화면)의 사용자 인터페이스에서 승인 (ACCEPT) 또는 거절(REJECT)을 선택하는 이분법으로 영상정보에 라벨을 붙일 수 있지만, 승인(ACCEPT), 노멀 (NORMAL), 거절(REJECT)로 분류하여 3단계로 영상정보에 라벨을 붙일 수도 있다. 잘된 동작과 잘못된 동작의 단 계를 정도로 나누어서 5단계, 6단계 라벨로 세분화하여 라벨링 할 수도 있다. 5단계, 6단계와 같이 라벨의 세분 화가 클 경우, 잘된 점수를 5점부터 1점까지 점수를 매긴다. 잘된 점수가 일정 이상(4점 이상)인 경우에는 승인 (ACCEPT)으로 간주하고, 잘못된 점수가 일정 이하(2점 이하)인 경우에는 거절(REJECT)로 간주하여 분류한다. 3 점은 노멀(NORMAL)로 분류한다. 또한, 상기 서버는 상기 생성된 해당 로우 데이터에 대한 분류값(또는 해당 로우 데이터의 분류값), 상기 선택라벨링된 로우 데이터에 대한 정보, 해당 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 상기 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 입력값으로 하여 기계 학습(또는 인공지능/딥 러닝) 을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거로 해당 로우 데이터에 대응하는 제 1 영상을 생성한다. 이때, 상기 제 1 영상은 상기 로우 데이터를 근거로 생성되는 아바타, 아이템, 로봇 등의 동 작 관련 영상, 상기 로우 데이터가 업데이트된 영상(예를 들어 상기 로우 데이터에 포함된 인간/사람의 동작/행 위/행동이 업데이트된 영상) 등일 수 있다. 즉, 상기 서버는 상기 생성된 해당 로우 데이터에 대한 분류값(또는 해당 로우 데이터의 분류값), 상기 선 택라벨링된 로우 데이터에 대한 정보, 해당 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 상기 비교 대 상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 미리 설정된 예측 모델의 입력값으로 하여 기계 학습 (또는 인공지능/딥 러닝)을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거로 해당 로우 데이터와 관련한 제 1 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 1 영상을 상기 단말에 전송(또는 제공)한다. 도 13을 참조하면, GNN의 구조는 다음과 같다. 동영상이나 정지영상 속의 객체는 노드(x1~x4, z1~z4)로 표현된다. 각 객체들은 서로 연관되어 있고, 그 관계가 상호영향을 주는 시계열적인 움직임 패턴이 있다. 입력층과 출력층은 복수의 층(layer)이 겹쳐 있 고, 입력층과 출력층의 중간에는 감쳐진 층(또는 히든층)이 존재한다. 입력(Input)이 들어가 면 다음 출력(output)이 예측된다. 기존의 GAN은 3D 복셀(voxel) 방식을 이용한다. 공간을 3D 복셀화하는 경우, X*, Y*, Z*, 4차원(dimension)의 정보가 수백 메가바이트에 달하므로, 매우 많은 하드웨어, GPU, 메모리 리소스를 필요로 하고, 트레이닝 시간이 매우 많이 소요되는 문제가 있다. 이러한 문제로 인해 최근에는 포인트 클라우드 방식을 주로 이용하고 있다. 포인트 클라우드 방식은 라이다(Lidar) 등을 이용하여 실물 공간 측정이 가능하고, xyz 좌표의 상대값을 물리적 으로 측정 및 자료구조화하는 것이 가능하여 3D 복셀 방식에 비하여 효과적인 측면이 있다. 하지만 포인트 (point)는 정렬되어 있지 않고, 정형화되어 있지 않아, 인공지능에 객체의 특성을 매우 일부만 표현해주는 단점 이 있어, 상대적인 값, 특징(feature)에 대한 정렬된 정보를 표현할 필요가 있다. 본 발명의 실시 예에 따른 GAN은 동영상, 포즈, 움직임의 특성에 대한 정보를 특징 내에 표현하기 위해, 포인트 와 포인트의 연결에 있어서, 관절의 특성(예를 들어 안으로만 접힐 수 있음), 각도, 거리, 랜드마크 포인트 (land mark point) 등을 추가로 표현한다. 본 발명의 일 실시예에서, 포인트 클라우드는 필수적 특징을 벗어나지 않는 범위에서 다른 자료 구조로 구체화 될 수 있다.. 즉, 본 발명의 실시 예에 따른 GAN은 관절, 구조물의 특성을 벡터(vector)적으로 표현하고, 포인트 클라우드에 서 GNN 형태로의 치환된 자료구조를 가져갈 수 있다. 또한, 3D 공간, 3D 모션(motion), 체형, 동작에 대한 GAN 활용에 있어, 공간의 정보를 포인트가 아닌 복수의 포 인트가 결합된 오브젝트(object)로 생성하며, 이를 GNN 형태로 자료구조화 하여 처리한다. GNN 형태로 처리함에 있어 부가적으로 메타(meta) 정보를 입력값(input)의 또 다른 특징(feature)으로 사용하고 있으며, 메타정보의 특징의 형태가 상이한 경우, 이를 단순하게 변경할 수 없으므로, 층(layer)을 나누어 합병 하여(merge) 사용한다. 합병하여 또 다른 입력값으로 사용하는 메타 정보는 사용자 정보 및 아이템 정보를 포함한다. 메타 정보는 비지도(unsupervised) GAN의 트레이닝에 있어, 지도(supervised) 라벨의 보완 정보로 사용되며, 조 건(Conditional) 정보로 사용된다. 해당 메타 정보는 각종 시각적 트레이닝 시 유사 정도를 GAN이 기억하게 되고, 향후 특정 속성 정보가 바뀔 때, 시각정보가 그에 맞추어 가변적으로 인위적 개입을 함에 있어 도움을 주는 입력값 정보로 활용된다. 본 발명의 일 실시예에서, 근육량 수치를 늘리거나, 연령을 낮추는 경우 생성된 가상의 아바타의 모양은 메타정보의 해당 값에 따라 달라지게 될 수 있다. GNN은 특정 파라미터 간 매핑된 데이터를 기초로 모델링된 모델링 데이터를 이용하여, 모델링 데이터 간의 유사 도와 특징점을 도출하는 방식으로 구현된 인공신경망 구조를 나타낼 수 있다. 여기에서 망라된 알고리즘 이외에 도 다른 것의 사용도 가능하고 언급된 알고리즘에 국한하지 않는다. 본 발명의 일 실시예에서, 사용자 정보는 얼굴 및 몸의 형태 및 색상, 연령, 성별, 헤어, 인종, fat 정도, 근육 질 정도, 기타 각종 카테고리 정보, 숫자(numeric) 정보, 기타 사용자 속성 정보를 포함하고, 아이템 정보는 브 랜드, 생성자ID, 광고주ID, NFT ID, 상품그룹ID, 기타 아이템 속성 정보를 포함한다. 디지털 카데바에서 활용되 는 경우는 각 부위명, 혈액형, 나이, 성별, 발병종류, 진행상태 등의 정보이다. 도 14를 참조하면, 서버는 조건부(Conditional) GAN의 조건(Condition) 메타 정보를 수정하여, 아 바타의 날씬해지기, 근육맨 등 체형특성정보를 수정한다. 도 14를 참조하면, 다양한 게임에서 메타정보를 수정할 수 있다. 본 발명의 일 실시예에서, 상기 서버는 댄싱 퍼포먼스, 가상 수술, 가상 축구게임, 가상 전투기 등을 조정 하는 아바타 등을 생성(또는 관리)한다. 본 발명의 일 실시예에서, 디지털 카데바는 치과 수술시 보철, 임플란트 등이 교체 가능한 외부 객체일 수 있으 며, 이를 교체하여 수술 전 시뮬레이션 해 볼 수 있으며, 성형에서는 성형 후 시뮬레이션으로, 일반 수술에서는 3D 입체적인 크기와 구조에 따른 물리적 결합 시뮬레이션 용도로 활용해 볼 수 있다. 이를 통해, 미리 학습된 객체에 대한 특성(예를 들어 각종 의료장비의 열리고 닫힘, 의사의 손과 발은 몸에서 떨어질 수 없고 안쪽으로 굽어질 수 있음, 의료장비 및 기구는 디지털 카데바에서 떨어질 수 있고, 붙어질 수 있음 등 포함)을 트레이닝 특징으로 활용한다. 이를 통해, 미리 학습된 객체에 대한 특성(예를 들어 치과 핸드피스의 치과용 날(버)는 돌아갈 수 있음, 수술용 칼에 의해 조직이 열림, 치아는 치아 잇몸에서 뽑힐 수 있음, 인체 장기는 대체될 수 있음 등 포함)을 트레이닝 특징으로 활용한다. 이를 통해, 미리 학습된 객체에 대한 특성(예를 들어 자동차의 바퀴는 돌아갈 수 있음, 집의 현관문은 열림, 손 과 발은 몸에서 떨어질 수 없고 안쪽으로 굽어질 수 있음, 모자는 머리에서 떨어질 수 있고, 씌워질 수 있음 등 포함)을 트레이닝 특징으로 활용한다. 이를 통해, 상기 서버는 조건부 GAN의 조건 메타 정보를 수정하여, 아바타의 일종인 디지털 카데바 의 변이, 증례에 따른 각종 질환정보를 수정한다. 다양한 실시의 예로서, 상기 단말은 다양한 형태의 VR 시뮬레이터일 수 있다. GAN 및/또는 GNN 예측모델에 의해 시각렌더링이 제공되는 VR 시뮬레이터에는 햅틱렌더링이 동시에 제공된다. VR 시뮬레이터에는 시각 세트 장치 및 다양한 형태의 햅틱 디바이스가 연결된다. VR 시뮬레이터의 형태에 따른 종류는 다음과 같다. 즉, 상기 VR 시뮬레이터는 치아삭제 VR 시뮬레이터, 수술 VR 시뮬레이터, 비히클(VEHICLE) VR 시뮬레이터, VR 트레드밀 등을 포함한다. VR 시뮬레이터의 형태는 이에 국한하지 않는다. 본 발명의 일 실시예에서, 치아삭제 VR 시뮬레이터의 장비는 HMD, 햅틱 디바이스, 치과용 체어에 사용되는 풋페 달 시스템(예를 들어 아두이노, 라즈베리파이 등 포함) 등이 필요하다. 3D 프린팅을 이용하여 디지털 카데바를 가상현실에서 만들고 HD 촉각을 구현한 인공 카데바를 만든다. VR 및 3D 시뮬레이터를 이용하여 가상의 치과치 료 및 외과수술을 진행한다. 본 발명의 일 실시예에서, 수술 VR 시뮬레이터는 다음과 같다. 환자의 병변에 대한 3D 모델을 작성하여 병변의 위치 및 상태, 영상정보를 기반으로 3D 환자 좌표계와 수술대 위에 놓인 환자의 좌표계를 정합함으로써, 보이지 않는 병변의 위치를 예측하여 수술을 수행하게 하는 방식이다. 본 발명의 일 실시예에서, 다양한 형태의 VR 시뮬레이터(VEHICLE 예시 : 잠수함, 탱크, 드론, 전투기 등)에서 아바타, 인간, 로봇 등이 VEHICLE 형 VR 시뮬레이터의 조종장치를 이용하여 운전하는 방식을 데이터화하여 아바 타를 생성할 수 있다. VR VEHICLE 시뮬레이터는 조종사 자신의 아바타 팔, 발, 다른 신체 일부 등으로 시뮬레이션한다. 좌표계는 메타 버스 월드 상에서 시작부터 종료까지 규칙에 따라 동기화된다. 높은 수준의 시각 렌더링을 구현하기 위해 라이 다 및 적외선 트래킹 및 모션트레킹을 장착하여 인체의 모션데이터 정합 알고리즘을 갖추어야 하고 메타버스 월 드에서 시뮬레이터의 위치에 대한 정합 알고리즘도 필요하다. 예를 들어, 가상 비행기 조종에 의해 획득된 시각데이터(기초 영상정보)는 도 12의 유도 및/또는 추론 알고리즘 의 초기 모델의 데이터 셋이 된다. 도 12의 유도 및/또는 추론 알고리즘은 도 15의 부분 유도 및/추론 알고리즘(제1,2 유도 및/또는 추론 알 고리즘)의 합이다. 가상 비행기 조종에 의해 획득된 시각데이터(기초 영상정보)는 인공지능이 비행 시뮬레이터를 작동시킬 수 있게 하는 기초 데이터가 된다. 인공지능의 가상 비행 조종에 있어서 오류와 오차가 많이 발생하면 사용자(비행기 조 종사)는 상기 단말의 앱 실행 결과 화면(또는 보기 화면)의 사용자 인터페이스를 통해 선택라벨링을 진행 한다. 본 발명의 일 실시예에서, VR 트레드밀을 이용한 아바타 컨트롤 시스템 (HEAD MOUNTED DISPLAY 착용)은 다음과 같은 기술이 필요하다. 사용자와 아바타의 이동, 행동, 무한 보행, 회전 등의 정합 알고리즘, 자세 제어 시스템, 바이브(VIVE) 트래커 를 활용한 모션 및 이동 제어 시스템, 라이다 및 적외선 트래킹(신발의 압력값과 적외선 센서값을 이용)을 이용 한 무한보행 및 인체 모션데이터 정합 알고리즘, 인체의 거의 모든 움직임 등이 가능하도록 설계된 VR 트레드밀본체, 메타버스 세상의 좌표기준과 환경 변이에 따라 반응 기술, 모션 데이터 동기화와 전용서버, 사용자의 네 트워크 플레이가 가능하도록 하는 동기화 시스템 등이 필요하다. 상기 도 7 내지 도 11의 총 K2개 내지 K6개의 군집 중의 하나인, 특정군집에 속한, 속성의 정지영상의 좌표값과 각종 시각 데이터에 대해 GNN을 사용하는 회귀 모델을 'GNN 회귀 모델 1형'으로 정의하고, 타깃속성에 대한 동 영상의 좌표값과 각종 시각 데이터에 대해 GNN을 사용하는 회귀모델을 'GNN 회귀모델 2형'으로 정의한다. 도 13을 참조하여, 아바타의 동작 행위에 대한 특정 시점의 상대적 영상 정보 및 상태값을 예측하는 모델이 GNN 형태로 구조화되고, 이의 각 수치를 예측한 모델을 'GNN 회귀모델 1형 및 2형'으로 정의한다. GAN 단독 사용시, 제1 연관규칙 1형 및 제1 연관규칙 2형이 제2 속성 및 제2 타깃속성 을 예측한다. 연관규칙 1형 및 2형은 GNN을 사용하지 않고, 각각 정지영상 및 동영상을 연관규칙 및 딥러 닝(GNN 회귀모델을 제외한 Sequential 형태의 input이 들어오는 모델)으로 추론하는 모델이고, GNN 형태의 구조 화를 사용하지 않는 점을 제외하고 도 13의 GNN 회귀모델 1형 및 2형과 동일한 형태의 모델이다. 본 발명의 일 실시예에서, 추적 신경망 네트워크에 사용되는 딥러닝(GNN 회귀모델을 제외한 Sequential 형태의 input이 들어오고, 객체의 x, y, z 좌표가 추적되는 모델)은 딥뉴럴네트워크가 있다. 'GNN 회귀모델 1형 및 2형' 혹은 '연관규칙 1형 및 2형'은 슬라이딩 윈도(sliding window) 기법을 사용하고. 시 퀀셜(Sequential) 형태의 입력이 들어오는 모델이다. 'GNN 회귀모델 1형 및 2형' 혹은 '연관규칙 1형 및 2형'은 도 16의 'GAN 및/또는 GNN 예측모델'이다. 도 16을 참조하면, 시각세트장치가 연결된 단말의 사용자 인터페이스에서 사용자는 선택라벨 링을 하고, 라벨링된 시각데이터는 GAN 및/또는 GNN 예측모델에서 사용된다. GAN 및/또는 GNN 예측 모델은 아바타의 동작을 생성하거나 출력하기 위해 시각데이터를 시뮬레이션 엔진에 전달한다. 도 16에서 시각데이터는 시뮬레이션 엔진 및 그래픽스 엔진 및 디스플레이 장치 및 제어알고리 즘의 순서대로 전달되어 사용자 인터페이스를 통해 출력된다. 상기 단말의 앱 실행 결과 화면 (또는 보기 화면)은 사용자 인터페이스가 단말에서 화면으로 구현된 것이다. 도 16의 GAN 및/또는 GNN 예측모델은 인터페이스 API과정을 포함한다. 다양한 실시예에서, 인터페이스 API의 예시는 다음과 같다. IoT Edge(예를 들어 아두이노, 라즈베리파이 등 포 함) 디바이스가 받아들이는 데이터는 입력 데이터 자체일 수도 있고, Edge 상에서 구동된 인공지능 추론의 결과 출력일 수도 있다. 파이썬 등으로 만들어진 인공지능 모델은 ONNX 등의 오픈소스 라이브러리를 통하여 IoT Edge 디바이스용으로 컨버팅 가능하고, 이를 통해 Edge 상에서 1차 추론된 출력 결과(output Result) 데이터 및 입력 데이터는 Server API 호출을 통해 더욱 복잡한 집단지성 모델로 재 추론된다. 디지털 단위는 인공지능과 사용자의 상호작용(예를 들어 시계열 분할 선택라벨링, 신체부위별 선택라벨링 등 포 함)에 의해 분할된 동영상 단위를 의미한다. 상기 도 7 또는 상기 도 9의 계층적 군집에서 K2 혹은 K4개의 각 군집별로 선택라벨링 된 제1 속성(122 4)을 분류하여 제1 GNN 회귀모델 1형 혹은 제1 연관규칙 1형 을 유도 및/또는 추론(ai inferenc e)한다. 상기 도 7 또는 상기 도 9의 계층적 군집에서 K2 혹은 K4 개의 각 군집화된 제1 속성과 제1 타깃속성 에서 제1 GNN 회귀모델 2형 혹은 제1 연관규칙 2형을 유도 및/또는 추론(ai inference)한다. 제1 GNN 회귀모델 1형 혹은 제1 연관규칙 1형에 정지영상정보(데이터단위 1, 2 혹은 제1 속성, 1224)의 시계열 시퀀스를 입력하면, 제1 GNN 회귀모델 1형 혹은 제1 연관규칙 1형은 상기 단말 의 앱 실행 결과 화면(또는 보기 화면)으로 제2 속성의 시계열 시퀀스를 반환한다. 제2 속성 은 제1 GNN 회귀모델 1형 혹은 제1 연관규칙 1형의 예측값(1206, 1216)이고 동작 동영상의 k번째 혹은 L번째 혹은 f번째 단계의 정지영상정보에 대한 특징벡터표현(feature vector representation)이다. 제1 GNN 회귀모델 2형 혹은 제1 연관규칙 2형에 '제2 속성의 시계열 시퀀스가 입력되면, 제1 GNN 회귀모델 2형 혹은 제1 연관규칙 2형은 상기 단말의 앱 실행 결과 화면(또는 보기 화면) 으로 제1 GNN 회귀모델 2형 혹은 제1 연관규칙 2형 의 예측값(1207, 1217)인 '제2 타깃속성 '을 생성 및 출력한다. 제2 타깃속성은 동작 동영상의 k번째 혹은 L번째 혹은 f번째 단계의 동영상정보에 대한 특징벡터표현(feature vector representation)이다. 도 12 및 도 15를 참조하면, 제1 유도 및/또는 추론(ai inference) 알고리즘은 다음과 같다. 제1 계층적 군집의 데이터는 제1 선택라벨링되어 제1 분류모델이 유도 및/또는 추론(ai inference)되고, 분류된 제1 속성 및 제1 타깃속성은 제1 GAN 및/또는 GNN 예측모델의 유도 및/또는 추론에 사용된다 또한, 상기 서버는 상기 제 1 영상을 대상으로 추가 선택라벨링을 수행한다. 여기서, 상기 추가 선택라벨 링(또는 추가 선택레이블링)은 상기 제 1 영상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 오류(또는 이상) 유무에 대한 라벨(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 제 1 영상 중에서 상기 추가 선택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점(또는 구간)은 미리 설정된 디폴 트 라벨값(예를 들어 승인 라벨)이 설정될 수 있다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 제 1 영상에 대해서, 해당 단말 의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 제 1 영상 중 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 서버는 상기 단말로부터 전송되는 상기 제 1 영상과 관련한 하나 이상의 또 다른 특정 시 점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값, 하나 이상의 시계열 분할 선택라벨값, 하나 이상의 신체부위별 선택라벨값, 복수의 서브 영상의 순서를 정렬하기 위한 라벨값, 해당 단말의 식별 정보 등을 수신한다. 본 발명의 실시예에서는, 상기 단말에서 사용자 입력에 따라 해당 제 1 영상 중 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 선택라벨값을 설정(또는 수신/입력)하는 것을 주로 설명하 고 있으나, 이에 한정되는 것은 아니며, 상기 서버는 해당 제 1 영상 및 해당 제 1 영상과 관련한 비교 대 상 영상에 대한 영상 분석 기능을 수행하고, 영상 분석 기능 수행 결과를 근거로 해당 제 1 영상에 대해서 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 선택라벨값을 자동으로 설정할 수도 있다. 또한, 상기 서버에서 해당 제 1 영상에 대해 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 선택라벨값을 설정한 경우, 상기 서버는 상기 설정된 해당 제 1 영상과 관련한 하나 이 상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값에 대한 정보를 상기 단말 에 제공하고, 해당 단말에서 상기 서버에서 설정된 해당 제 1 영상과 관련한 하나 이상의 또 다 른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값에 대한 정보를 표시하고, 해당 단말 의 사용자 입력에 따라 해당 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값에 대해 최종 승인 여부를 결정하도록 구성할 수도 있다. 이때, 해당 제 1 영상을 대상으로 추가 선택라벨링을 수행하기 이전에 또는 수행한 이후에, 상기 서버는 상기 단말과 연동하여, 해당 하나 이상의 제 1 영상을 대상으로 추가 계층라벨링을 수행하고, 추가 계층라 벨링 수행 전/후로 해당 제 1 영상을 대상으로 추가 선택라벨링을 수행할 수도 있다. 여기서, 상기 추가 계층라 벨링(또는 추가 계층레이블링)은 사용자에 의한 입력 피처 엔지니어링으로, 해당 제 1 영상에 대한 특징을 나타 내는 라벨(또는 라벨값)을 붙이고, 해당 제 1 영상을 특징에 따라 복수의 서브 영상으로 분할(또는 분류)하는 라벨링 방법을 나타낸다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 제 1 영상에 대해서, 해당 특정 주제와 관련해서 미리 설정된 복수의 라벨 분류를 참조하여(또는 근거로), 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 제 1 영상 중 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 추가 라 벨(또는 추가 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 서버는 상기 제 1 영상을 복수의 서브 영상으로 분할한다. 본 발명의 실시예에서는, 상기 단말에서 사용자 입력에 따라 해당 제 1 영상 중 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 계층라벨(또는 추가 계층라벨값)을 설정(또는 수신/입력) 하는 것을 주로 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 서버는 해당 제 1 영상 및 해당 제 1 영상과 관련한 비교 대상 영상에 대한 영상 분석 기능을 수행하고, 영상 분석 기능 수행 결과를 근거로 해당 제 1 영상에 대해서 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 계층라벨값을자동으로 각각 설정할 수도 있다. 또한, 상기 서버에서 해당 제 1 영상에 대해 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 계층라벨값을 설정한 경우, 상기 서버는 상기 설정된 해당 제 1 영상과 관련한 하나 이 상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 계층라벨값에 대한 정보를 상기 단말 에 제공하고, 해당 단말에서 상기 서버에서 설정된 해당 제 1 영상과 관련한 하나 이상의 또 다 른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 계층라벨값에 대한 정보를 표시하고, 해당 단말 의 사용자 입력에 따라 해당 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 계층라벨값에 대해 최종 승인 여부를 결정하도록 구성할 수도 있다. 상기 도 15의 제1 GNN 및/또는 GAN 예측모델의 예측값인 제1 영상정보에 대해 제2 계층라벨링 을 하면 제2 계층적 군집이 된다. 제2 기초영상정보에 대한 제1 계층라벨링이 동시에 이루어 지고 생성된 군집은 제2 계층적 군집에 포함된다. 본 발명의 일 실시예에서, 계층적 군집화 라벨링 정보를 수신하는 제 2 계층라벨링정보 수신 단계를 생략 한다. 사용자에 의한 입력 피처 엔지니어링을 생략하고 상기 서버에 의해 스스로 생성될 수 있다. 본 발명의 실시예에서, 제2 계층라벨링은 제2 선택라벨링에 포함되어 실시될 수 있다. 또한, 상기 서버는 상기 추가 선택라벨링된 제 1 영상에 대한 정보 등을 근거로 인공지능 기반의 다른 기 계 학습을 수행하여, 다른 기계 학습 결과를 근거로 해당 제 1 영상에 대한 분류값을 생성(또는 확인)한다. 여 기서, 상기 해당 제 1 영상에 대한 분류값(또는 해당 제 1 영상의 분류값)은 추가 선택라벨링값, 추가 계층라벨 링값 등을 동일 항목별로 분류한 값일 수 있다. 즉, 상기 서버는 상기 추가 선택라벨링된 제 1 영상에 대한 정보 등을 상기 미리 설정된 분류 모델의 입력 값으로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또는 다른 인 공지능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 영상에 대한 분류값을 생성(또는 확인)한다. 상기 서버는 K7(수차례) * K8(여러 명)의 사용자가 입력한 기초영상(또는 로우 데이터)에 대해 분류 모델 을 유도 및/또는 추론하는 제2 분류모델의 유도 및/또는 추론 단계를 포함한다. 제1 GAN 및/또는 GNN 예측모델에 의해 예측된 값이 제1 영상정보이다. 제1 영상정보의 정지 영상정보는 제2 속성이고 동영상정보는 제2 타깃속성이다. 제2 분류 모델은 제2 계층적군집중의 하나인, 특정군집에 속한 '제2 속성 및 제2 타깃속성 '을 분류한 것이다. 특정군집에 속한 제2 속성 및 제2 타깃속성에 대해 사용자들이 계층적 군집화 라벨값을 입력하고 '선택라벨링'을 하면 라벨링된 데이터에 대한 분류모델을 유도 및/또는 추론한 다. 상기 분류모델에서 제2 기초 영상정보의 제1 속성 및 제1 타깃속성이 하나의 모델로 학 습된다. 본 발명의 실시예에서, 계층적 군집화 라벨링 정보를 수신하는 사용자에 의한 제1 기초영상정보에 기반한 제1 영상정보의 제2 계층라벨링 정보와 제2 기초 영상정보에 기반한 제1 계층라벨링 정보의 수신 단계를 생략할 때, 계층적 군집은 상기 서버에 의해 스스로 생성된다. 또한, 상기 서버는 상기 생성된 해당 제 1 영상에 대한 분류값(또는 해당 제 1 영상의 분류값), 상기 추가 선택라벨링된 제 1 영상에 대한 정보, 해당 제 1 영상, 해당 제 1 영상과 관련한 메타 정보, 상기 비교 대상 영 상, 해당 비교 대상 영상과 관련한 메타 정보 등을 입력값으로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또는 다른 인공지능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 영상에 대응하는 제 2 영상을 생성한다. 이때, 상기 제 2 영상은 상기 제 1 영상을 근거로 생성되는 아바타, 아 이템, 로봇 등의 동작 관련 영상, 상기 제 1 영상이 업데이트된 영상 등일 수 있다. 즉, 상기 서버는 상기 생성된 해당 제 1 영상에 대한 분류값(또는 해당 제 1 영상의 분류값), 상기 추가 선택라벨링된 제 1 영상에 대한 정보, 해당 제 1 영상, 해당 제 1 영상과 관련한 메타 정보, 상기 비교 대상 영 상, 해당 비교 대상 영상과 관련한 메타 정보 등을 상기 미리 설정된 예측 모델의 입력값으로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또는 다른 인공지능 결과/다른 딥 러 닝 결과)를 근거로 해당 제 1 영상과 관련한 제 2 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 2 영상을 상기 단말에 전송(또는 제공)한다. '제1 기초 영상정보, 제2 기초 영상정보, …'는 도 12의 유도 및/또는 추론(ai inference) 알고리즘에 입 력되는 것으로, 시각세트장치를 통해 계속적으로 수집된 실제 현실의 시각데이터이다. 제2 유도 및/또는 추론 알고리즘은 다음과 같다. 제1 GAN 및/또는 GNN 예측모델의 예측값은 제2 계 층라벨링에 의해 제2 계층적 군집이 된다. 제2 계층적 군집의 데이터는 제2 선택라벨링 되어 제2 분류모델이 유도 및/또는 추론되고, 분류된 제2 속성 및 제2 타깃속성은 제2 GAN 및/또는 GNN 예측모델의 유도 및/또는 추론에 사용된다. 이하 알고리즘의 유도 및/또는 추론이 반복된다. 제1 GAN 및/또는 GNN 예측모델은 제1 GNN 회귀모델 1형 및 제1 GNN 회귀모델 2형이거나 제1 연관규칙 1형 및 제1 연관규칙 2형이다. 제2 GAN 및/또는 GNN 예측모델은 제1 기초영상정보에서 기반한 제1 영상정보의 제2 계층적 군집이 제2 선택라벨링되고, 제2 분류모델에 분류된 제2 속성과 제2 타깃속성에 의해 유도 및/또는 추론된 모델이다. 제2 계층적 군집은 제2 속성 및 제2 타깃속성의 군집이 다. 또한, 제2 기초영상정보에서 기반한 제1 속성 및 제1 타깃속성도 제2 GAN 및/또는 GNN 예측모델의 유도 및/또는 추론에 사용된다. 상기 도 15를 참조하면, 제2 유도 및/또는 추론 알고리즘에서 제1 영상정보 및 제2 기초 영상정보가 단일 모델로 학습되고, 각 군집별로 제2 영상정보가 생성된다. 제1 기초 영상정보에서 기반한 제1 영상 정보의 제2 속성 및 제2 타깃속성과 제2 기초영상정보에서 기반한 제1 속성과 제1 타깃속성이 라벨링되어 단일모델로 학습된다. 본 발명의 일 실시예에서, 잘된 동작의 결과인 동영상정보를 예측하는 모델이 제1 GNN 회귀모델 2형 혹은 제1 연관규칙 2형이다. 다양한 실시예에서, '제1 GNN 회귀모델 2형'은 연관규칙을 사용한다. 도 8, 도 10 및 도 11의, 특정군집 에 속한, 디지털단위의 정지영상정보(속성)에서 객체의 패턴, 물리적 속성값을 포함한 동영상정보(타깃속성)를 연관규칙으로 예측한다. 본 발명의 일 실시예에서, GNN 회귀모델 2형은 역방향의 연관규칙을 사용하는 방식과 순방향의 연관규칙을 사용 하는 방식과 양방향의 연관규칙을 사용하는 방식으로 나뉜다. 제2 기초 영상정보와 제1 영상정보가 동일 모델(단일모델)로 학습된다. 제1 영상정보는 제1 기초 영상정보의 라벨링 데이터이다. 상기 도 15에서 모델 관점에서 제2 기초 영상정보와 제1 유도 및/또는 추론 알고리즘의 예측값인 제1 영 상정보는 정확도나 정교함의 장단점이 서로 상이할 수 있는데, 제2 유도 및/또는 추론 알고리즘의 학습 데이터로 사용된다 1차 라벨링 된 제1 영상정보는 라벨링에 의해 2차 라벨링되고, 이 과정은 계속하여 반복된다. 이 과정을 과거 라벨 된 데이터(제1 영상정보, 1503)와 다른 새로운 데이터(제2 기초 영상정보, 1505)가 함께 반복 수행하 게 되며, 한번 학습했던 데이터 및/또는 유사한 레이블 값 또한 매 반복 학습(epoch)에 계속 등장하여, 여러 번 의 실험을 거치는 과정이 필요하다. 매 에포크(epoch)는 누적된 단위 레이블의 총 개수(batch size)만큼을 학습 연산 단위(mini batch size)로 분할하여 다양한 실험을 하게 되며 해당 과정에서 집단지성의 라벨값은, 취사 선 택 및 평균화되어 모델에 반영된다. 상기 도 15에서 '제1 유도 및/또는 추론 알고리즘, 제2 유도 및/또는 추론 알고리즘, …'은 도 12 의 유도 및/또는 추론 알고리즘이고, 전체 알고리즘을 부분 알고리즘의 합으로 표현한 것이다. 다양한 실시의 예에서, 가상공간상 3D 프린팅 시뮬레이션으로 디지털 카데바를 손쉽게 제작 및 초기화 가능하고, 이의 사용은 가상공간의 제약을 덜기 위해 가상수술 오디션 게임을 진행한다. 실제 의료기관, 가상수 술 오디션 등을 통해 수집된 수술 패턴을 군집화 및 패턴화하여 인공지능의 초기 모델을 만든다. 수술의 정교함 의 정도 및 성공 여부에 대하여는 검증된 전문의의 패턴을 별도 추출하여 지도학습한다. 위 방식으로 초기 모델 화된 각 수술별 수술 의료인공지능은 스스로 디지털 카데바 및 인공 카데바에 대해 VR 시뮬레이터로 가상수술 (예를 들어 시술, 치료 등 포함)을 수행한다. 그리고 의사가 의료 인공지능이 수행한 가상수술정보에 대해 라벨 링을 하는 것을 보상하는 방식으로 게임화한다. 의료 인공지능 수술 라벨링은 의사가 직접 가상공간상에서 수술 을 하거나, 학습된 인공지능이 하는 수술을 첨삭 보정하는 방법으로 정교화된다. 이러한 라벨링 행위의 강화는보상을 통해 게임화 한다. 본 발명의 일 실시예에서, 슬라이딩 윈도(sliding window)는 다음과 같다. 동영상 정보들에 대해서 각 윈도 크 기(windows size)의 단위를 분류(classification)한다. 위 방식으로 총 50초짜리 동영상들이 10초씩 5개로 나 뉜다고 할 때, A,B,Z,A,B 의 순서로 입력이 들어오면, 다음이 Z 라는 걸 연관규칙으로 예측한다. 본 발명의 일 실시예에서, 잘 알려진 딥러닝 알고리즘인 RNN, LSTM 등은 양방향(bidirectional) LSTM이라는 변 형 알고리즘을 통해 순방향을 역방향 및 양방향으로 확대하고 추가적인 성능 형상을 꾀할 수 있는데, 제안하는 디지털 단위 또한 양방향과 마찬가지로 역방향 및 양방향으로 확대 가능하다. 제안하는 디지털 단위는 RNN 및 LSTM과 달리 복합적인 입력 피처가 결합되어 있다는 차이점이 있다. 동영상 장면 프레임은 특징적인 패턴으로 군집화되어, A동작, B동작, C동작 형태로 군집 될 수 있다. 수술 동작 및/또는 게임 내 특정 캐릭터의 특별한 동작은 모두 학습된 동작 군집(A, B, C, …)의 일련의 순서 패턴으로 다시 연관도가 만들어질 수 있다. 이는 본 발명의 일 실시예에서 A→B→D, A→B→F 등의 시퀀셜(Sequential) 연관 패턴이 학습 데이터 내에서 순서 연관도 높게 빈발하게 관찰될 경우, 그 패턴이 순서와 함께 학습되게 된다. 특정 반복 동작은 동영상 군집의 시퀀셜 연 관 패턴으로 학습 및 재현되어 질 수 있다. 여기서 말하는 재현은 앞부분의 일부 패턴이 입력으로 입력되었을 때 뒤이을 패턴이 어떤 군집의 동작 패턴인지 연관규칙으로 유추 추론 가능하다는 의미이다. 다양한 실시예로, 시계열 역방향의 연관규칙을 설명한다. GNN 회귀모델 1형을 사용하여 정지영상정보인 출력(Output) 데이터(흔적 및 결과)를 예측하고, 출력 데이터(흔 적 및 결과)에 대한 원인이 되는 포인트 클라우드의 벡터적이 역방향의 제1 GNN 회귀모델 2형의 예측값이 다. 결과값을 분석하여 시간의 변화에 따른 GNN 프레임워크 상의 포인트 클라우드의 벡터적을 찾아내어 플랫폼 사용자에게 반환한다. 결과값이 있으면 포인트 클라우드의 벡터적이 있다는 규칙을 발견하여, GNN 회귀모델 1형 이 임의의 예측값(결과값) 및/또는 정지영상정보를 제시하면, GNN 회귀모델 2형은 시간의 변화에 따른 포인트 클라우드의 벡터적(원인값)을 반환한다. 연관규칙 추론을 위해 결과값들 간의 의미 있는 관계를 찾아내기 위하 여 결과값의 데이터 집합과 포인트 클라우드의 벡터적(원인값)을 반환하는 트랜잭션의 집합을 구축한다. 연관규 칙은 선행사건과 후행사건이 있으며, 결과값과 원인값의 집합에 각각 포함되는 것으로 이는 연관규칙 추론의 결 과로 얻게 되며, 벡터적은 복잡함(complexity)이 있는 정보이기 때문에 많은 연관규칙이 존재하게 된다. 의미 있는 연관 규칙을 찾아내는 평가기준이 필요하다. 평가척도로는 지지도, 신뢰도, 향상도 등을 사용한다. 연관 규칙 알고리즘에서 결과값과 원인 값의 각각의 집합이 디지털 단위 4 및 디지털 단위 5에서 정지영상정보의 군 집과 동영상정보 군집을 의미한다. 앞서 설명한 시계열 분할 선택라벨링 기능에 대해서 추가로 설명한다. 계층라벨링을 하지 않고도 상기 서버는 사용자에게 제1 영상정보, 제2 영상정보의 분할시점(예를 들어 정 지영상정보, 정지영상정보의 라벨값 등 포함)을 반환(정지영상정보에 대한 사용자의 시청)하고, 사용자는 반환 값(분할시점)에 대해 시계열분할 선택라벨링 혹은 신체부위별 선택라벨링을 수행한다. 시계열분할 선택라벨링 혹은 신체부위별 라벨링을 수행하면, 디지털 단위 3 혹은 디지털 단위 4 혹은 디지털 단위 5가 생성된다. 상기 영상정보는 제1 영상정보 또는 제2 영상정보이고, 상기 제2 선택라벨링 또는 제3 선택 라벨링 정보 수신 단계는 도 17의 시계열 분할 선택라벨링을 포함한다. 상기 영상정보는 GAN 및/또는 GNN 예측모델의 반복되는 예측값으로, '제1, 2, 3, …의 영상정보'이다. 시계열분할 선택라벨링을 포함한 제2 계층라벨링 또는 제3 계층라벨링 정보는 도 8 혹은 도 10의 계층적 군집과 관련된 것이다. 본 발명의 실시예에서, 계층적 군집화 라벨링이 생략되는 경우에도 시계열 분할 선택라벨링은 제2 선택라 벨링 또는 제3 선택라벨링에 포함되어 실행되기도 하고, 제2 선택라벨링 또는 제3 선택라벨링 전후 에도 실행될 수 있다. 제2 계층적 군집 또는 제3 계층적 군집은 도 8 혹은 도 10의 디지털 단위 3 혹은 디지털 단위 4를 기준으 로 처리된(또는 전산화된) 것이다. 도 8은 디지털 단위 3을 기준으로 처리된 계층적군집이고, 도 10은 디지털 단위 4를 기준으로 처리된 계층 적 군집이다. '제1, 2 GNN 회귀모델 1형의 예측값 혹은 제1, 2 연관규칙 1형의 예측값' 혹은 '제1, 2 GNN 회귀모델 2형의 예 측값 혹은 제1, 2 연관규칙 2형의 예측값'에 대해 사용자가 ACCEPT 혹은 REJECT의 라벨을 붙여 분할시점(정지영상정보 혹은 속성의 라벨값)을 선택하거나 거부하는 방식을 도 17의 '시계열분할 선택라벨링'이라고 정의 한다. 사용자가 라벨분류를 참조하여 도 17의 시계열분할 선택라벨링을 수행한 다음, 제2 분류모델을 유 도 및/또는 추론한다. 인공지능이 분할시점을 학습하여 이를 사용자에게 반환하면 사용자는 ACCEPT 버튼 또는 REJECT 버튼을 통해 선 택한다. 분류모델은 라벨링이 된 정보를 다시 분류하고, 'GNN 회귀모델 혹은 연관규칙(딥러닝 포함)'은 집단지 성화된 예측값을 반환하고, 사용자는 반복하여 도 17의 시계열분할 선택라벨링을 수행한다. 라벨분류를 참조한 사용자가 상기 단말의 앱 실행 결과 화면(또는 보기 화면)을 통해 라벨값을 입력한다. 본 발명의 일 실시예에서, 시계열분할 선택라벨링에서 사용자가 REJECT 버튼을 눌렀다면, 상기 단말(10 0)의 앱 실행 결과 화면(또는 보기 화면)의 상기 재생바 내의 타임 라인 상에서 시각을 나타내는 표(또는 화살 표)를 이동하여, 동영상을 분할하고자 하는 시점의 정지영상정보를 캡처한 후, ACCEPT 버튼을 누르는 방법으로 시계열 분할에 따라 라벨링되는 라벨값을 직접 입력한다. 도 12의 유도 및/또는 추론 알고리즘에 사용되는 디지털 단위 3, 디지털 단위 4는 도 3의 분 할된 직육면체이다. 디지털 단위 3에서 속성은 아바타, 인간, 로봇 등의 분할된 동작 동영상의 k번째 단계의 끝부분 정지영상 정보이고, 도 4 검정색 표시의 사각형이다.. 상기 도 4를 참조하면, 분할된 동작 동영상의 n번째 단계의 끝부분 정지영상정보 또한 속성에 해당되고, 도 4의 마지막 검정색 표시의 사각형이다. 디지털 단위 4에서 속성은 아바타, 인간, 로봇 등의 분할된 동작 동영상의 L번째 단계의 끝부분 정지영상 정보이고, 도 5의 검정색 표시의 사각형이다. 상기 도 4 내지 상기 도 5를 참조하면, 분할된 동작 동영상의 (k, L)번째 단계의 끝부분 정지영상정보 또한 속 성에 해당되고, 도 4 내지 도 5의 마지막 검정색 표시의 사각형이다. 상기 도 8 및 상기 도 10은 디지털 단위 3, 디지털 단위 4에 근거한 K3, K5개의 군집이다. 다양한 실시예에서, 시작부분 정지영상정보도 속성이고 타깃속성인 동영상정보와의 합으로 디지털 단위 3 혹은 디지털 단위 4가 된다. 본 발명의 일 실시예에서, 도 8 혹은 도 10은 도 7 내지 상기 단말의 앱 실행 결과 화면(또는 보기 화면) 의 입력창에 입력된 변수값에 의해 동영상의 단계가 분할될 때 붙여진 라벨값에 의해 만들어진 계층적 군집화의 계통도이다. 도 17의 시계열분할 선택라벨링은 '데이터 단위 3 혹은 데이터 단위 4'를 '디지털 단위 3 혹은 디지털 단위 4'를 기준으로 동영상을 분할한다. 본 발명의 일 실시예에서, 디지털 단위 3 혹은 디지털 단위 4는 시계열분할 선택라벨링과 데 이터 정렬을 병행하여 동영상을 분할한 것이다. 사용자는 정렬된 동영상의 순서를 정렬하기 위해 동영상 혹은 동영상의 순서를 나타내는 라벨값에 대해 ACCEPT 라벨 혹은 REJECT 라벨을 붙여 분할된 동영상의 순서를 선택하 거나 거부한다. 디지털 단위 4는 사용자가 라벨분류를 참조하여 아바타, 인간, 로봇 등의 동작을 약 0.5초 ~ 3초 내외의 특징적 인 세부 동작들로 분할하는 시계열분할 선택라벨링에 의해 분할된 동작 동영상정보이다. 디지털 단위 5는 디지털 단위 4보다 좀더 세분화된 동영상의 분할이 가능하다. 디지털 단위 3은 사용자가 라벨분류를 참조하여 아바타, 인간, 로봇 등의 동작을 약 3초 ~ 수십초 내외의 특징 적인 동작들로 분할하는 시계열분할 선택라벨링에 의해 분할된 동작 동영상정보이다. 본 발명의 실시예에서 설명하는 데이터 단위는 사용자에 의해 생성되는 복합적인 피처 벡터의 단위이고, 디지털 단위는 사용자와 인공지능의 상호작용으로 생성된 복합적인 피처 벡터의 단위이다. 본 발명의 일 실시예에서, 아바타, 인간, 로봇 등의 동작을 약 0.5초 ~ 3초 내외의 특징적인 세부 동작들로 분 류한 라벨분류는 앞선 [표 5] 혹은 [표 10]이다.본 발명의 일 실시예에서, 데이터 단위 3 및 디지털 단위 3은 수초에서부터 수십초 단위로 분할된 동영상정보의 단위일 수 있다. 3000 큐비트 이상의 양자 클라우드 컴퓨팅장치가 상용화되어 컴퓨팅 파워가 지금보다 월등히 향상될 때, 데이터 단위 3 및 디지털 단위 3은 영상정보의 생성과 출력에 사용된다. 디지털 단위 3은 데이터 단위 3과 동일한 방식으로 속성(정지영상정보)과 타깃속성(동영상정보)의 합이 처리된 것이다. 디지털 단위 4는 데이터 단위 4와 동일한 방식으로 속성(정지영상정보)과 타깃속성(동영상정보)의 합이 처리된 것이다. 본 발명의 일 실시예에서, 다수의 사용자(공군 사관생도 및/또는 전투기 조종사 등)가 영화 '탑건'의 전투기 조 종 장면 약 1분간을 가상 전투기 시뮬레이터(VEHICLE VR 시뮬레이터)를 이용하여 따라서 조종하고 라벨링을 진 행하여, 도 12의 유도 및/또는 추론 알고리즘에 사용하는 데이터 단위 및 디지털 단위별 데이터 셋을 획득한다. 전투기 조정은 조종의 세부적인 조종술마다 각 방식의 특징적인 동작이 있기 때문에 다수의 사용자가 유사한 가 상 비행을 하면 전체동영상이 1초 ~ 2초 내외의 짧은 동영상으로 분할된다. 본 발명의 일 실시예에서, 다수의 사용자가 영화 '라이언 일병 구하기'의 전투 장면 약 1분간을 VR 트레드밀을 이용하여 전동콘트롤러형 무기를 격발하고 라벨링을 진행한다. 영화 속 보병이나 공병의 움직임(소총 격발 및 수류탄 던지기 등의 연속동작)도 1초 ~ 2초 내외의 짧은 동영상으로 분할이 가능하다. 본 발명의 일 실시예에서, 디지털 단위 3의 시계열 분할 방식은 다음과 같다. 앞선 [표 1] 내지 [표 4]와 같은 라벨분류 등을 참조하여 치과의사, 의사 등의 라벨링을 하여, 도 12 의 유도 및/또는 추론 알고리즘이 고도화된다면, 'GNN 회귀모델'’은 정지영상정보를 반환하면서 플랫폼 사용자 (의사, 치과의사 등)에게 분할시점 및 라벨값 (s1, s2, s3, k)을 반환한다. 반환값에 대해 사용자는 시계열분할 선택라벨링을 수행한다. 본 발명의 일 실시예에서, 다음의 [표 12]는 상악중절치 라미네이트 11번(치식) 삭제를 진행하는 30초 동영상을 10단계로 분할하여 30초를 약 2초 ~ 4초 간격으로 분할하여 설명한다. 사용자의 시계열분할 선택라벨링을 통해 디지털 단위 4로 동영상의 분할이 가능하다. 표 12 변수값(라벨값) 상악 중절치 라미네이트 치료를 위한 11번 치아 삭제 방법(세 부동작단계)정보 형태 1 치아삭제 전에 미리 제작한 치아삭제용 인덱스를 구강 및 치아 에 위치시킨다.영상 등 2 구강 및 치아에 위치된 인덱스를 치과의사가 눈으로 확인하고 삭제량을 측정한다.영상 등 3 치과의사는 자신이 판단으로 삭제량을 정하고 치과용 핸드피스 의 depth gage bur(삭제할 깊이를 치아에 표시하는 치아 삭제 용 핸드피스 버, 칼날)를 체크하여 핸드피스에 장착한다.영상 등 4 치경부 3분의 1의 예상 삭제 깊이를 depth gage bur로 삭제한 다.영상 등 5 치아 중앙부 3분의 1의 예상 삭제 깊이를 depth gage bur로 삭 제한다.영상 등 6 치아 절단부 3분의 1의 예상 삭제 깊이를 depth gage bur로 삭 제한다.영상 등 7 치아 치경부 3분의 1를 실제 치아 삭제용 핸드피스 bur로 삭제 한다.영상 등 8 치아 중앙부 3분의 1를 실제 치아 삭제용 핸드피스 bur로 삭제 한다.영상 등 9 치아 절단부 3분의 1를 실제 치아 삭제용 핸드피스 bur로 삭제 한다.영상 등 10 상악 중절치 전체 치아를 핸드피스의 트리밍 bur(다듬는 칼 날)로 다듬고 미세하게 삭제한다.영상 등 위와 같은 10단계로 분할되는 여러 환자(아바타 및 디지털 카데바)들을 대상으로 한 동영상정보들이 도 9에서 같은 특정군집에 속해 있다고 하더라도 해당 동영상에서 세밀한 수술과 시술의 순서는 집도하는 의사의 의료기 술에 따라 다를 수가 있다. 다른 순서는 앞선 [표 5]의 라벨 순서를 기준으로 전처리하여 분류모델에 적용시킬수 있다. 또한, 단계가 순서가 다르거나 생략된 부분 및/또는 추가된 부분에 대해서 [표 12]의 라벨 순서를 기 준으로 동영상 정보를 정렬하여 군집화한다. 라벨분류([표 12])를 참조하여 치과의사가 라벨링을 수행한다면, 인공지능의 반환에 대해 치과의사는 ACCEPT 라 벨 또는 REJECT 라벨을 붙여 분할시점(정지영상정보)에 대해 시계열분할 선택라벨링을 하면, 분류모델은 라벨링 이 된 정보를 다시 분류하고, 'GNN 회귀모델'은 더욱 집단지성화된 분할시점(정지영상정보)과 라벨값을 반환하 게 된다. 인공지능의 반환에 대해 치과의사는 ACCEPT 버튼 또는 REJECT 버튼으로 선택한다. 위 방식으로 치과의 사가 라벨링을 수행한다면, 동영상정보를 분할하여 정지영상정보를 반환하는 GNN 회귀모델이 정지영상을 반환하 면서 도 12의 유도 및/또는 추론 알고리즘은 치과의사에게 분할시점(속성값) 및 라벨값 반환한다. 인공지 능의 예측값에 대해 치과의사가 ACCEPT 버튼 또는 REJECT 버튼을 눌러 라벨을 붙여 분할시점(속성 혹은 속성의 라벨값)을 선택하거나 거부하면, 분류 모델은 라벨링이 된 정보를 다시 분류하고, 'GNN 회귀모델 1형'은 더욱 집단지성화된 분할시점(정지영상정보)과 라벨값을 반환한다. 결국, 충분히 집단지성화된 디지털 단위 4가 생성된다. 앞서 설명한 신체부위별 선택라벨링 기능에 대해서 추가로 설명한다. 상기 영상정보는 제1 영상정보 또는 제2 영상정보이고, 상기 제2 선택라벨링 및 제3 선택라 벨링 정보 수신 단계는 신체부위별 선택라벨링을 포함한다. 상기 영상정보는 GAN 및/또는 GNN 예측모델의 반복되는 예측값으로 '제1, 2, 3, …의 영상정보'이다. 신체부위별 선택라벨링을 실행하여, 신체부위별 선택라벨링 정보가 포함된 제2 계층라벨링 또는 제3 계층 라벨링 정보는 계층적군집이 된다. 본 발명의 실시예에서, 계층적 군집화 라벨링이 생략되는 경우에도 신체 부위별 선택라벨링은 제2 선택라 벨링에 포함되어 실행되기도 하고 제2 선택라벨링 전후에 실행될 수 있다. 제2 계층적 군집 또는 제3 계층적 군집은 디지털 단위 5를 기준으로 전산화된 군집이다. 디지털 단위 5는 디지털 단위 4와 동일한 방식으로 속성(정지영상정보)과 타깃속성(동영상정보)의 합이 처리된 것이다. 데이터 단위 3 혹은 데이터 단위 4 혹은 디지털 단위 3 혹은 디지털 단위 4는 신체부 위별 선택라벨링에 의해 '디지털 단위 5'로 처리된다. 아바타, 인간, 로봇 등의 동작에서 신체부위별로 동작 순서를 정하기 위해 신체부위별로 순서를 정하는 라벨을 붙여 실제 동영상에서의 동작 순서를 바꾸기 위한 라벨링을 하는 것을 '신체부위별 선택'이라고 정의한다. 본 발명의 실시예에서, '신체 부위별 선택'을 한 후, 동영상 데이터 정렬에 따른 전처리작업(삭제, 추가 등)등 에 대해서 ACCEPT 라벨 또는 REJECT 라벨을 붙여 선택하거나 거부할 수 있다. 사용자가 라벨분류를 참조하여 제1, 2 영상(또는 영상정보)에 신체부위별 선택라벨링을 한 다음, 제2, 3 분류모델을 유도 및/또는 추론한다. 사용자가 라벨분류를 참조하여 제1 영상정보 또는 제2 영상정보에 신체부위별 선택라벨링을 수행하고, 이로 인해 디지털 단위 5로 동영상이 분할되는 단계이다. 디지털 단위 5에서 속성은 아바타, 인간, 로봇 등의 분할된 동작 동영상의 f번째 단계의 끝부분 정지영상 정보이고, 도 6의 검정색 표시의 사각형이다. 도 6을 참조하면, 분할된 동작 동영상의 f번째 단계의 끝부분 정지영상정보 또한 속성에 해당되고, 도 6의 마지 막 검정색 표시의 사각형이다. 도 11은 디지털 단위 5에 근거한 K6 개의 군집이다. 도 12의 유도 및/또는 추론 알고리즘에 사용되는 디지털 단위 5는 도 3의 분할된 직육면체이 다. 다양한 실시예에서, 시작부분 정지영상정보도 속성이고 타깃속성인 동영상정보와의 합으로 디지털 단위 5가 된 다. 본 발명의 일 실시예에서, 도 11은 상기 단말의 앱 실행 결과 화면(또는 보기 화면)의 입력창에 입력된 변 수값(라벨값)에 의해 동영상의 단계가 분할될 때 붙여진 라벨값에 의해 만들어진 계층적 군집화의 계통도이다. '신체부위별 선택라벨링'에 의해 디지털 단위 5로 아바타, 인간, 로봇 등의 동작 동영상이 분할된다. 본 발명의 일 실시예에서, 라미네이트 치아 11번 삭제에 대해서도 대부분의 치과의사들이 치아 삭제용 인덱스를 제작하여 치아삭제를 하지만 인덱스를 제작하지 않는 치과의사도 있고, depth gage bur를 사용하지 않는 사람도 있다. 위 차이를 기준으로 계층적 군집화를 수행하고 동영상 정보를 정렬하여 전처리한다. 치아삭제시 순서(치 경부, 중앙부, 절단부 순서 등)를 라벨분류의 기준으로 하지 않고 자신만의 순서대로 진행하는 사람도 있을 수 있다. 이런 경우, 상악 중절치 치아의 중앙부, 절단부, 치경부와 같은 신체 부위에 대한 세부적인 순서를 지정 하는 라벨링을 통해서 분할된 동영상의 순서를 지정하고 라벨링 순서에 맞는 라벨분류를 만든다. 또한, 위 라벨 링의 순서대로 동영상정보들을 정렬한다. 동일한 특정 군집(본 발명의 일 실시예에서, 11번 치아를 인덱스를 사 용하지 않고 삭제하는 방식)에 포함되었으나 치아삭제의 순서(치경부, 중앙부, 절단부 삭제 순서)가 다른 동영 상 및 정지영상 정보들에 대해 신체부위의 순서 라벨링과 동영상정보의 정렬 등의 전처리작업을 통해서 분류모 델의 오차값들을 줄이고 분류모델의 정확도를 높인다. 본 발명의 일 실시예에서, 어떤 치과의사가 치경부, 중앙부, 절단부 순으로 치아삭제를 하고 어떤 치과의사가 중앙부, 절단부, 치경부 순으로 치아를 삭제했다면 전부 치경부, 중앙부, 절단부 삭제 순서와 같은 방식으로 동 영상정보를 정렬하고 이 순서대로 분할하여 군집화한다. 또한, 치과의사가 마우스의 화살표를 이용하여 신체의 특정 부분을 가리키거나 신체의 특정 부위를 생각만 한다면 사용자가 마우스로 가리키거나 생각한 부위의 경계 선 및 경계면을 객체인식을 통해 인공지능이 반환한다. 또한, 인공지능은 치료순서에 대한 정렬된 정보를 사용 자에게 반환한다. 이에 대해 사용자는 '자신이 의도하거나 생각한 부위가 맞다, 틀리다' 및/또는 '자신이 의도 한 순서가 맞다, 틀리다' 및/또는 '순서에 대한 라벨값이 맞다, 틀리다'를 판단한다. 이와 같이 브레인 컴퓨터 인터페이스를 이용한 판단만으로 동영상 및 정지영상 등에 ACCEPT 라벨 또는 REJECT 라벨을 붙이고 정렬한다. 위 라벨링을 반복하여 도 12의 유도 및/또는 추론 알고리즘에 적용한다. 본 발명의 일 실시예에서, [표 6]에서 건강한 성인의 구강에는 약 28개의 치아가 있고 치아마다 치식(치아번 호)이 있다. 상악 우측 중절치는 11번이다. 치식 22, 21, 11, 12 치아 4개를 라미네이트 치료를 위해 삭제하는 술식을 하는 경우에, 모든 치과의사들이 라미네이트를 하기 위해 치아삭제를 할 때, 일정한 치아번호(치식)의 순서대로 진행하는 것이 아니므로, 위 동영상정보를 일정한 순서(치식)대로 정렬하고 삭제되거나 추가된 동영상 정보에 대해서도 전처리를 한다. 시계열 분할 라벨링을 수행할 경우, 계층적 군집화와 더불어 신체 부위의 구체적 시술 순서에 대한 정렬(치식순 서)을 동시에 진행하면 더욱 정확한 군집화가 가능하다(계층적 군집화 및 신체 부위별 선택). 또한, 좀 더 세부 적인 신체부위별 선택라벨링을 하고자 한다면, 상악중절치 라미네이트 치아삭제(11번 치아)의 방식과 순 서도 치과의사마다 다를 수 있으므로 라벨분류(일정한 기준)를 근거로 동영상정보를 정렬하고 생략되거나 추가 된 동영상정보에 대해서 전처리를 한다. 본 발명의 일 실시예에서, 메타버스 축구게임의 동영상에서 0.5초 이하의 순간적인 작은 데이터 크기의 디지털 단위로 분할된 동영상을 얻고자 한다면, 신체부위별 선택라벨링과 정렬을 이용하여 동영상을 세분화하여 분할한다. 손흥민이 인스텝 드리블 3스텝 롱동작을 할 때, 축구공이 미리 설정된 라벨 분류의 발가락 터치, 1 스텝 달리기, 발목 터치, 2 스텝 달리기 순서대로 축구공이 손흥민의 발에 순간적인 터치를 하였는데 이것을 재 현한 특정 사용자가 발목 터치, 1 스텝 달리기, 발가락 터치, 2 스텝 달리기 순으로 터치하고 달리기를 하였다 면, 특정 사용자의 인프런트 드리블의 터치순서 및 달리기의 순서에 대해 브레인 컴퓨터 인터페이스를 이용하여 신체부위별 선택라벨링을 수행한다. 본 발명의 일 실시예에서, [표 10]에서 열린음악회 2022년 7월 8일 방송의 제니의 k번째 동작이 앞뒤 웨이브라 고 했을 때, [표 11]에서 블랙핑크 제니의 앞뒤 웨이브 동작은 왼쪽 팔 들기, 오른쪽 팔 들기, 가슴 움직이기, 배 움직이기, 골반 움직이기, 다리 움직이기 순으로 들거나 앞뒤로 움직인다. 특정 사용자가 다리 움직이기, 골 반 움직이기, 배 움직이기, 가슴 움직이기, 오른쪽 팔 들기, 왼쪽 팔들기 순서대로 앞뒤 웨이브를 했다면, 브레 인 컴퓨터 인터페이스를 이용하여 신체부위별 선택라벨링을 수행한다. 특정사용자의 동작 동영상을 블랙 핑크 제니의 동작 순서대로 정렬한다. 또한, 총 3분 14초의 동영상은 1초 ~ 2초 내외의 동영상, 약 200개로 시 계열 분할이 가능하다. 춤 동작은 머리, 손, 발, 몸통의 움직임의 조합이 연속된 것이다. 신체부위별 선택라벨 링을 하지 않고 시계열분할 선택라벨링을 할 수도 있다. 또한, 상기 서버는 해당 특정 주제와 관련해서, 복수의 단말로부터 제공되는 복수의 로우 데이터에 대해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정, 생성된 제 1 영상에 대한 추가 선 택라벨링 과정, 추가 분류 모델 추론 과정, 추가 예측 모델 추론 과정을 각각 반복 수행하여, 해당 특정 주제와 관련해서(또는 해당 특정 주제와 관련한 비교 대상 영상과 관련해서) 집단 지성화된 제 2 영상을 생성(또는 업 데이트)한다. 이때, 상기 서버는 해당 특정 주제와 관련해서 로우 데이터를 제공한 복수의 단말에 마지막으로 업데 이트된(또는 최신으로 생성된) 제 2 영상을 실시간 또는 특정 단말의 요청에 따라 제공할 수도 있다. 이에 따라, 해당 특정 주제와 관련한 로우 데이터를 상기 서버에 제공한 모든 단말 또는 특정 단말 은 해당 특정 주제와 관련해서 최신의 집단 지성화된 제 2 영상을 제공받을 수 있다. GAN 및/또는 GNN 예측 모델에 의해 반복되어 생성되는 '제1, 2, 3, …의 영상정보'는 기초영상정보(160 1)와 단일모델로 반복적으로 학습된다. 계층라벨링, 선택라벨링이 반복적으로 실행된다. 분류 모델이 반 복적으로 추론되고, GAN 및/또는 GNN 예측 모델이 반복적으로 유도 및/또는 추론된다. 본 발명의 일 실시예에서, 시계열 분할 선택라벨링 및/또는 신체 부위별 선택라벨링이 반복적으로 실행된다. 또한, 상기 서버는 단말과 연동하여, 특정 주제와 관련해서, 상기 단말에서 출력되는(또는 관리 중인) 실제 인간(또는 실제 사람), 가상의 아바타나 아이템 등의 동작 관련 영상(또는 인간, 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상), 해당 동작 관련 영상과 관련한 메타 정보 등을 수집한다. 여기서, 상기 특정 주제(또는 특정 콘텐츠)는 의료 행위(예를 들어 시술, 수술 등 포함), 댄스, 운동 종목(예를 들어 축 구, 농구, 탁구 등 포함), 게임, 이-스포츠 등을 포함한다. 또한, 상기 인간과 관련한 동작 관련 영상(또는 기 초영상정보/로우 데이터)은 실제 인간(또는 사람/인플루언서)이 상기 특정 주제와 관련해서 수행 중인 행동(또 는 동작/행위)를 획득한(또는 촬영한) 영상일 수 있다. 또한, 상기 아바타 및/또는 아이템의 동작 관련 영상은 해당 특정 주제와 관련한 임의의 로우 데이터를 근거로 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정 등을 통해 생성된 영상일 수 있다. 본 발명의 일 실시예에서, 도 18의 아바타, 아이템, 인간 동작의 시각데이터는 아바타 또는 인간이 조작 하여 작동하는 비히클(vehicle) 움직임의 시각데이터를 포함한다. 여기서, 상기 시각데이터는 현실세계의 사용자(또는 인간)의 동작에 대한 로우 데이터를 나타낸다. 또한, 상기 서버는 상기 수집된 동작 관련 영상을 실제 로봇의 동작으로 구현하기 위해서, 상기 수집된 동 작 관련 영상(또는 상기 수집된 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상)을 로봇 동작 영상으 로 재구성한다. 여기서, 상기 로봇은 치아삭제 VR 시뮬레이터의 시각데이터를 이용하여 치아삭제 VR 시뮬레이터 에서 작동할 수 있는 형태로 제작한 로봇팔, 수술 VR 시뮬레이터의 시각데이터를 이용하여 수술 VR 시뮬레이터 에서 작동할 수 있는 형태로 제작한 로봇팔, VEHICLE VR 시뮬레이터의 시각데이터를 이용하여 VEHICLE 형태로 제작한 로봇, VR 트레드밀에서 작동할 수 있는 휴머노이드 로봇을 포함한다. 즉, 상기 서버는 상기 수집된 동작 관련 영상, 상기 동작 관련 영상과 관련한 메타 정보 등을 근거로 해당 실제 인간, 가상의 아바타나 아이템 등의 동작을 실제 로봇에 적용하기 위해서 해당 동작 관련 영상에 포함된 실제 인간, 가상의 아바타나 아이템 등과 관련한 좌표 정보를 상기 실제 로봇에 적용하기 위한 로봇 좌표 정보 로 변환하여, 해당 동작 관련 영상을 상기 로봇 동작 영상으로 재구성한다. 또한, 상기 서버는 상기 로봇 동작 영상(또는 재구성된 로봇 동작 영상), 해당 로봇 동작 영상에 대한 메 타 정보, 상기 수집된 동작 관련 영상, 상기 동작 관련 영상과 관련한 메타 정보, 상기 서버에서 관리 중 인 복수의 비교 대상 영상 중에서 상기 수집된 동작 관련 영상(또는 로봇 동작 영상)과 관련해서 검색된 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 상기 서버에 미리 등록된 복수의 단말 중 에서 선택된 특정 단말에 전송한다. 또한, 상기 특정 단말은 상기 서버로부터 전송되는 상기 로봇 동작 영상, 해당 로봇 동작 영상에 대 한 메타 정보, 상기 동작 관련 영상, 상기 동작 관련 영상과 관련한 메타 정보, 상기 동작 관련 영상(또는 로봇 동작 영상)에 대응하는 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 수신한다. 단말과 연결하여 로봇의 움직임에 대한 공간적 시간적 좌표를 정확히 계측하면서 상기 단말의 앱 실 행 결과 화면(또는 보기 화면)의 디스플레이 장치 및 사용자 인터페이스를 통해 로봇의 움직임을 평가하여 사용자가 로보틱스 선택라벨링을 하는 방식으로 고도화하여 서버상에서 작동하는 집단지성 모델로 추론된 로보틱스 프로그래밍을 '집단지성 로보틱스'라 정의한다. 기초 로보틱스 영상정보 에 대해서 제 1 로보틱스 선택라벨링 이전에 선택라벨링(기초 선택라벨링)을 실행하여 제 1 집단지성 로보틱스 를 추론 및/또는 유도한다. 기초 로보틱스 영상정보에 대해 계층라벨링 및/또는 선택라벨링이 도 12의 방 식과 동일하게 실행될 수 있다. 본 발명의 일 실시예에서, 상기 단말의 앱 실행 결과 화면(또는 보기 화면)에서 출력되는 시각데이터는 단 말이 제공하는 로봇의 가상현실, 증강현실, 혼합현실, 확장현실 등에서의 동작 화면이다. 로보틱스 영상정보는 앞선 도 3 내지 도 6의 속성 및 타깃속성에 해당되고, 집단지성 로보틱스에 의해 생성되는 시각데이터이다. 제 1 집단지성 로보틱스는 제1 기초 로보틱스 영상정보가 입력되어 프로그래밍 되고, 제1 로보틱스 영상정보를 생성한다. 도 18의 기초 로보틱스 영상정보는 메타버스 사용자의 행동 정보 및 위치정보가 가상환경의 좌표에 정합 되도록 동기화된 상태에서 단말로부터 확보된 아바타, 인간, 로봇 등의 동작 데이터가 상기 서버 에서 로봇 동작 데이터(영상정보)로 재구성된 것이다. 상기 단말로부터 확보된 아바타 동작의 시각데 이터는 도 16의 GAN 및/또는 GNN 예측 모델의 예측값이고, 도 15에서의 '제1, 2, 3, …의 영상정 보'이며, 이하 반복되는 메타버스 월드의 GAN 및/또는 GNN 예측모델의 예측값이다. 또는, 인간(또는 사용 자) 동작의 시각데이터은 현실세계에서의 사용자의 동작에 대한 로우 데이터를 의미한다. 인간(또는 사용 자) 동작의 시각데이터 상기 서버에서 로봇 동작 데이터(영상정보)로 재구성되고, 상기 인간 동작의 시각데이터가 재구성된 로봇 동작 영상정보는 기초 로보틱스 영상정보에 포함된다. 본 발명의 일 실시예에서, 상기 단말의 앱 실행 결과 화면(또는 보기 화면)에서 출력되는 시각데이터는 단 말이 제공하는 로봇의 동작 화면으로 가상현실, 증강현실, 확장현실, 혼합현실 등일 수 있다. 단말의 사용자 인터페이스상의 좌표계와 로봇 동작에서의 좌표계의 오차를 줄이기 위해 로봇 크기 기반 실제 거리 좌표계를 추정하고, 로봇 관절별 각도를 추출하며 제어한다. 본 발명의 실시예에서, 치아삭제 VR 시뮬레이터, 수술 VR 시뮬레이터, VEHICLE VR 시뮬레이터, VR 트레드밀의 시각 데이터를 이용하여, 로봇팔 형태 혹은 휴머노이드 형태 혹은 VEHICLE 형태의 로봇을 제작한다. 도 18을 참조하면, 기초 로보틱스 영상정보는 집단지성 로보틱스에 입력된다. 집단지성 로보틱스 에 포함된 GAN 및/또는 GNN 로보틱스 예측 모델은 인터페이스 API 과정을 포함하고, 예측 모델은 상기 단 말의 앱 실행 결과 화면(또는 보기 화면)에 로보틱스 영상정보를 출력한다. GAN 및/또는 GNN 로보틱 스 예측모델은 GAN 및/또는 GNN 예측모델과 동일한 방식의 로보틱스 동작에 관한 시각데이터의 모델이다. 로보틱스 영상정보는 GAN 및/또는 GNN 로보틱스 예측모델에 의해 반복적으로 출력 및/또는 생성되는 '제 1, 2, 3, …의 로보틱스 영상정보'이다. 상기 영상정보에 대해 반복적으로 로보틱스 선택라벨링이 이루어 진다. 집단지성 로보틱스에서 출력된 시각데이터는 로봇 시뮬레이션 엔진으로 전달되고, 로봇과의 API 통 신을 통해 로봇을 작동시키고, 그래픽스 엔진을 거쳐 디스플레이 장치 및 사용자 인터 페이스를 통해 출력된다. 본 발명의 일 실시예에서, 상기 서버에서의 로보틱스의 프로그래밍은 다음과 같다. ROS(Robot Operating System)와 OpenCV(Open Source Computer Vision), PCL(Point Cloud Library)을 활용하여 비전 센서를 ROS와 인 터페이스하고, OpenCV 및 PCL과 같은 라이브러리를 이용하여 프로그래밍한다. 본 발명의 일 실시예에서, 메타버스 병원 및 치과병원 게임에서 단말은 환자의 병변에 대한 3D 모델을 작 성하여 병변의 위치 및 상태, 영상정보를 기반으로 3D 환자 좌표계와 수술대 위에 놓인 환자의 좌표계를 정합되 도록 정합한다. 이와 같이, 본 발명에 의하면, 병원 게임에서 치과의사 사용자에게는 외료기기, 의료장비, 재료 등의 아이템을 디지털 카데바(환자의 아바타)의 얼굴 및 몸에 대입 및 다양한 조합으로 생성 및/또는 출력해보는 서비스를 제 공할 수 있다. 본 발명의 일 실시예에서, 의사 및/또는 치과의사에 의해 동작하는 VR 시뮬레이터를 통해 충분한 가상 수술 및 치아삭제 시술에 관한 시각데이터가 확보되면, 로보틱스 프로그래밍에 의해 VR 시뮬레이터에서 자동화 수술 및시술을 할 수 있는 인공지능 수술 및 시술 로봇 제작이 가능하다. '실제 의료기관의 데이터 수집 및 가상 치아 시뮬레이터 및 가상 수술 시뮬레이터'에서 수집된 군집의 활용에 있어서, 연관규칙과 앞에서 예측한 것이 뒤에 입력으로 가면서 반복되는 시퀀셜(sequential) 모델을 통해 '자동화 수술 및 시술할 수 있는 인공지능'의 초기 모델을 만든다. 로보틱스 선택라벨링을 통해 인공지능을 고도화한다. 인공지능이 가상수술 및 시술을 진 행하고, 이에 대해 의사가 라벨링을 진행하여 도 12의 유도 및/또는 추론 알고리즘을 적용하고 인공지능 을 고도화한다. 도 19의 제1 로보틱스 영상정보는 도 4 내지 도 6의 제2 속성 및 제2 타깃속성이다. 기초 로보틱스 영상 정보는 도 7 내지 도 11의 군집중의 하나인, 특정군집에 속하는 데이터이다. '로보틱스 영상정보 또한 같은 특정군집에 속하는 데이터가 된다. 또한, 상기 서버는 상기 로봇 동작 영상을 대상으로 선택라벨링을 수행한다. 여기서, 상기 선택라벨링(또 는 선택레이블링)은 상기 로봇 동작 영상의 특정 시점(또는 특정 구간)에서의 오류(또는 이상) 유무에 대한 라 벨(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 로봇 동작 영상 중에서 상기 선 택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점(또는 구간)은 미리 설정된 디폴트 라벨값(예를 들어 승인 라벨)이 설정될 수 있다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 로봇 동작 영상에 대해서, 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 로봇 동작 영상 중 특정 시점(또는 특정 구간)에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 서버는 상기 단말로부터 전송되는 상기 로봇 동작 영상과 관련한 하나 이상의 특징 시점 (또는 특정 구간)에서의 하나 이상의 선택라벨값, 해당 로봇 동작 영상의 메타 정보, 해당 단말의 식별 정 보 등을 수신한다. 본 발명의 실시예에서는, 상기 단말에서 사용자 입력에 따라 해당 로봇 동작 영상 중 하나 이상의 특정 시 점(또는 특정 구간)에서 하나 이상의 선택라벨값을 설정(또는 수신/입력)하는 것을 주로 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 서버는 해당 로봇 동작 영상 및 해당 로봇 동작 영상과 관련한 비교 대상 영 상에 대한 영상 분석 기능을 수행하고, 영상 분석 기능 수행 결과를 근거로 해당 로봇 동작 영상에 대해서 하나 이상의 특정 시점(또는 특정 구간)에서 하나 이상의 선택라벨값을 자동으로 각각 설정할 수도 있다. 또한, 상기 서버에서 해당 로봇 동작 영상에 대해 하나 이상의 특정 시점(또는 특정 구간)에서 하나 이상 의 선택라벨값을 설정한 경우, 상기 서버는 상기 설정된 해당 로봇 동작 영상과 관련한 하나 이상의 특정 시점(또는 특정 구간)에서의 하나 이상의 선택라벨값에 대한 정보를 상기 단말에 제공하고, 해당 단말 에서 상기 서버에서 설정된 해당 로봇 동작 영상과 관련한 하나 이상의 특정 시점(또는 특정 구간)에 서의 하나 이상의 선택라벨값에 대한 정보를 표시하고, 해당 단말의 사용자 입력에 따라 해당 하나 이상의 특정 시점(또는 특정 구간)에서의 하나 이상의 선택라벨값에 대해 최종 승인 여부를 결정하도록 구성할 수도 있 다. 이때, 해당 로봇 동작 영상을 대상으로 선택라벨링을 수행하기 이전에 또는 수행한 이후에, 상기 서버는 상기 단말과 연동하여, 해당 로봇 동작 영상을 대상으로 계층라벨링을 수행하고, 계층라벨링 수행 전/후로 해당 로봇 동작 영상을 대상으로 선택라벨링을 수행할 수도 있다. 여기서, 상기 계층라벨링(또는 계층레이블 링)은 사용자에 의한 입력 피처 엔지니어링으로, 해당 로봇 동작 영상에 대한 특징을 나타내는 라벨을 붙이고, 해당 로봇 동작 영상을 특징에 따라 복수의 서브 로봇 동작 영상으로 분할(또는 분류)하는 라벨링 방법을 나타 낸다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 로봇 동작 영상에 대해서, 해당 특정 주제와 관련해서 미리 설정된 복수의 라벨 분류를 참조하여(또는 근거로), 해당 단말의 사용자 입력 (또는 사용자 선택/터치/제어)에 따라, 해당 로봇 동작 영상 중 다른 특정 시점(또는 다른 특정 구간)에서의 라 벨(또는 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 서버는 상기 로봇 동작 영상을 복수의 서브 로봇 동작 영상으로 분할한다. 본 발명의 실시예에서는, 상기 단말에서 사용자 입력에 따라 해당 로봇 동작 영상 중 하나 이상의 다른 특 정 시점(또는 다른 특정 구간)에서 하나 이상의 계층라벨값을 설정(또는 수신/입력)하는 것을 주로 설명하고 있 으나, 이에 한정되는 것은 아니며, 상기 서버는 해당 로봇 동작 영상 및 해당 로봇 동작 영상과 관련한 비 교 대상 영상에 대한 영상 분석 기능을 수행하고, 영상 분석 기능 수행 결과를 근거로 해당 로봇 동작 영상에대해서 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서 하나 이상의 계층라벨값을 자동으로 각각 설정할 수도 있다. 또한, 상기 서버에서 해당 로봇 동작 영상에 대해 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서 하나 이상의 계층라벨값을 설정한 경우, 상기 서버는 상기 설정된 해당 로봇 동작 영상과 관련한 하나 이 상의 다른 특정 시점(또는 다른 특정 구간)에서의 하나 이상의 계층라벨값에 대한 정보를 상기 단말에 제 공하고, 해당 단말에서 상기 서버에서 설정된 해당 로봇 동작 영상과 관련한 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서의 하나 이상의 계층라벨값에 대한 정보를 표시하고, 해당 단말의 사용자 입력에 따라 해당 하나 이상의 다른 특정 시점(또는 다른 특정 구간)에서의 하나 이상의 계층라벨값에 대해 최 종 승인 여부를 결정하도록 구성할 수도 있다. 상기 영상정보인 제1 로보틱스 영상정보 에 대해 제1 로보틱스 선택라벨링 정보를 수신한다. 제1 로보틱스 분류모델 유도 및/또는 추론 방식은 도 12의 제2 분류모델의 방식과 동일하다. 제1 집단지성 로보틱스의 출력인 제1 로보틱스 영상정보에 대해 제1 로보틱스 선택라벨링을 수행한다. 제1 로보틱스 선택라벨링을 수행하여 획득된 시각데이터를 분류한 분류모델을 '제1 로보틱스 분류모델'이라 정의한다. 이하 로보틱스 분류모델은 반복된다. 로보틱스 선택라벨링은 도 16 의 선택라벨링과 동일한 방식이다. 상기 단말의 앱 실행 결과 화면(또는 보기 화면) 형태의 도 18의 사용자 인터페이스에서 출력되는 로봇의 동작에 대해 사용자가 도 18의 로보틱스 선택라벨링을 수행한다. 로봇의 동작은 로보틱스 영상정 보이다. '집단지성 로보틱스'의 초기 모델의 로봇은 동작에 오류가 많을 수 있다. 다소 부정확한 로봇의 움 직임에 대해 로보틱스 개발자는 로보틱스 선택라벨링 및 분류를 통해 지도학습을 한다. 가상 시뮬레이션 에서 생성 및 출력된 아바타와 아이템, 그리고 공간 환경, 서사 등을 집단지성 로보틱스에 제공하고, 사 용자가 로보틱스 선택라벨링을 수행하는 방식으로 인공지능에 지도학습을 한다. 도 12의 유도 및/또는 추 론 알고리즘은 집단지성 로보틱스를 고도화한다. 또한, 상기 서버는 상기 선택라벨링된 로봇 동작 영상에 대한 정보 등을 근거로 인공지능 기반의 기계 학 습을 수행하여, 기계 학습 결과를 근거로 해당 로봇 동작 영상에 대한 분류값을 생성(또는 확인)한다. 여기서, 상기 해당 로봇 동작 영상에 대한 분류값(또는 해당 로봇 동작 영상의 분류값/선택라벨링된 로봇 동작 영상의 분류값/계층라벨링된 로봇 동작 영상의 분류값)은 선택라벨링값, 계층라벨링값 등을 동일 항목별로 분류한 값일 수 있다. 즉, 상기 서버는 상기 선택라벨링된 로봇 동작 영상에 대한 정보 등을 미리 설정된 분류 모델의 입력값으 로 하여 기계 학습(또는 인공지능/딥 러닝)을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거로 해당 로봇 동작 영상에 대한 분류값을 생성(또는 확인)한다. 도 18에서 사용자 인터페이스에 출력된 영상정보는 로보틱스 선택라벨링에 라벨링되어 로보틱스 분 류모델을 통해 분류된다. 분류된 시각데이터는 집단지성 로보틱스에 전달되는 라벨링된 로보틱스 라벨정보이다. 본 발명의 실시예에서, 로보틱스 선택라벨링은 메타버스의 영상처리와 동일한 방식의 계층라벨링, 시계열 분할 선택라벨링, 신체부위별 선택라벨링 등을 포함한다. 제1 로보틱스 분류모델에 의해 분류된 정보가 제1 로보틱스 라벨정보이다. 본 발명의 일 실시예에서, 가상 시뮬레이션 게임의 각 분야 전문가에 해당되는 다수의 사용자들이 단말의 인터페이스를 통해 로보틱스 선택라벨링을 진행하여 충분한 시각데이터가 확보되면 VR 시뮬레이터 를 조작하는 인공지능 로봇이 제작된다. 로봇 관절과 팔과 다리 등을 이용하여 VR 시뮬레이터를 조작하는 집단 지성 로보틱스의 초기 모델이 개발된 경우에 사용자들의 라벨링을 통해 집단지성 로보틱스 모델의 능력을 지도학습으로 고도화한다. 고도화가 되면 실제 현실에서 동작할 수 있는 집단지성 로보틱스 초기 모델이 개발될 수 있고, 이 경우에도 사용자 및 전문가들의 로보틱스 선택라벨링을 통해 집단지성 로보틱 스를 지도학습으로 고도화한다. 도 18 및 도 19의 집단지성 로보틱스는 도 12의 유도 및/또는 추론 알고리즘을 반복 적용하고 라벨링을 반복하여 집단지성 로보틱스의 모델을 고도화한다. 집단지성 로보틱스의 고도화에 따라 로봇팔을 이용한 실제 의료 현장에서의 자동화 시술 및 수술할 수 있는 인공지능 초기 모델이 개발될 수 있고, 이 경우에도 의사들의 로보틱스 선택라벨링을 통해 인공지능의 초기모델 의 자동화 능력을 지도학습으로 고도화한다. 본 발명의 일 실시예에서, 사용자(의사)에 의하여, 평가되고 다듬어지는 집단지성화 알고리즘은 인공지능 추론 을 고도화하여 오차와 오류가 없이 가상 수술 시뮬레이션 및 치아삭제 시뮬레이션에서 자동화 수술을 하는 수준 으로 인공지능을 고도화한다. 로봇팔을 이용한 VR 시뮬레이터 상에서의 자동화 시술 및 수술할 수 있는 인공지 능 초기 모델이 개발된 경우에 의사들의 로보틱스 선택라벨링을 통해 인공지능 모델의 자동화 능력을 지 도학습으로 고도화한다. 고도화가 되면 로봇팔을 이용한 실제 의료 현장에서의 자동화 시술 및 수술할 수 있는 인공지능 초기 모델이 개발될 수 있고, 이 경우에도 의사들의 로보틱스 선택라벨링을 통해 인공지능 초기 모델의 자동화 능력을 지도학습으로 고도화한다. 본 발명의 일 실시예에서, 로봇머리, 로봇팔, 로봇다리, 로봇 몸, 로봇 관절 등을 사용하는 휴머노이드형 로봇 을 제작하고, 자율주행형 차량, 드론, 비행기 등의 비히클 로봇 및 인공지능 치과의사로봇 및 인공지능 의사로 봇을 제작한다. 또한, 상기 서버는 상기 생성된 해당 로봇 동작 영상에 대한 분류값(또는 해당 로봇 동작 영상의 분류값), 상기 선택라벨링된 로봇 동작 영상에 대한 정보, 해당 로봇 동작 영상, 해당 로봇 동작 영상과 관련한 메타 정 보, 상기 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 입력값으로 하여 기계 학습(또는 인공 지능/딥 러닝)을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거로 해당 로봇 동작 영상에 대응하는 제 1 로보틱스 영상을 생성한다. 이때, 상기 제 1 로보틱스 영상은 상기 로봇 동작 영상을 근거로 생 성되는 아바타, 아이템, 로봇 등의 동작 관련 영상, 상기 로봇 동작 영상이 업데이트된 영상 등일 수 있다. 즉, 상기 서버는 상기 생성된 해당 로봇 동작 영상에 대한 분류값(또는 해당 로봇 동작 영상의 분류값), 상기 선택라벨링된 로봇 동작 영상에 대한 정보, 해당 로봇 동작 영상, 해당 로봇 동작 영상과 관련한 메타 정 보, 상기 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 미리 설정된 예측 모델의 입력값으로 하여 기계 학습(또는 인공지능/딥 러닝)을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거 로 해당 로봇 동작 영상과 관련한 제 1 로보틱스 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 1 로보틱스 영상을 상기 단말에 전송(또는 제공)한다. 제2 로보틱스 영상정보는 제2 집단지성 로보틱스의 예측값인 영상으로, 본 발명에 있어서 반복된 유도 및/또는 추론 알고리즘의 반복된 적용으로 고도화된 예측모델의 예측값으로 정의한다. 제2 로보틱스 영상정보는 도 4 내지 도 6에서 제3 속성과 제3 타깃속성이다. 제1 로보틱스 분류모델에 의해 분류된 제1 로보틱스 라벨정보는 제2 집단지성 로보틱스에 입 력되고, 제2 기초 로보틱스 영상정보도 제 2 집단지성 로보틱스에 입력되어 단일모델로 프로그래밍 되고, 제2 로보틱스 영상정보를 생성한다. 또한, 상기 서버는 상기 제 1 로보틱스 영상을 대상으로 추가 선택라벨링을 수행한다. 여기서, 상기 추가 선택라벨링(또는 추가 선택레이블링)은 상기 제 1 로보틱스 영상의 또 다른 특정 시점(또는 또 다른 특정 구 간)에서의 오류(또는 이상) 유무에 대한 라벨(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 제 1 로보틱스 영상 중에서 상기 추가 선택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점 (또는 구간)은 미리 설정된 디폴트 라벨값(예를 들어 승인 라벨)이 설정될 수 있다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 제 1 로보틱스 영상에 대해서, 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 제 1 로보틱스 영상 중 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 서버는 상기 단말로부터 전송되는 상기 제 1 로보틱스 영상과 관련한 하나 이상의 또 다 른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값, 하나 이상의 시계열 분할 선택라벨 값, 하나 이상의 신체부위별 선택라벨값, 해당 복수의 서브 로보틱스 영상의 순서를 정렬하기 위한 라벨값, 해 당 단말의 식별 정보 등을 수신한다. 본 발명의 실시예에서는, 상기 단말에서 사용자 입력에 따라 해당 제 1 로보틱스 영상 중 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 선택라벨값을 설정(또는 수신/입력)하는 것을 주로 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 서버는 해당 제 1 로보틱스 영상 및 해당 제 1 로보틱스 영상과 관련한 비교 대상 영상에 대한 영상 분석 기능을 수행하고, 영상 분석 기능 수행 결과를 근거로 해당 제 1 로보틱스 영상에 대해서 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상 의 추가 선택라벨값을 자동으로 설정할 수도 있다. 또한, 상기 서버에서 해당 제 1 로보틱스 영상에 대해 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 선택라벨값을 설정한 경우, 상기 서버는 상기 설정된 해당 제 1 로보틱스 영 상과 관련한 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값에 대 한 정보를 상기 단말에 제공하고, 해당 단말에서 상기 서버에서 설정된 해당 제 1 로보틱스 영 상과 관련한 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값에 대 한 정보를 표시하고, 해당 단말의 사용자 입력에 따라 해당 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값에 대해 최종 승인 여부를 결정하도록 구성할 수도 있다. 이때, 해당 제 1 로보틱스 영상을 대상으로 추가 선택라벨링을 수행하기 이전에 또는 수행한 이후에, 상기 서버 는 상기 단말과 연동하여, 해당 하나 이상의 제 1 로보틱스 영상을 대상으로 추가 계층라벨링을 수행 하고, 추가 계층라벨링 수행 전/후로 해당 제 1 로보틱스 영상을 대상으로 추가 선택라벨링을 수행할 수도 있다. 여기서, 상기 추가 계층라벨링(또는 추가 계층레이블링)은 사용자에 의한 입력 피처 엔지니어링으로, 해 당 제 1 로보틱스 영상에 대한 특징을 나타내는 라벨(또는 라벨값)을 붙이고, 해당 제 1 로보틱스 영상을 특징 에 따라 복수의 서브 로보틱스 영상으로 분할(또는 분류)하는 라벨링 방법을 나타낸다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 제 1 로보틱스 영상에 대해서, 해당 특정 주제와 관련해서 미리 설정된 복수의 라벨 분류를 참조하여(또는 근거로), 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 제 1 로보틱스 영상 중 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 추가 라벨(또는 추가 라벨값)을 설정(또는 수신/입력)한다. 또한, 상기 서버는 상기 제 1 로보틱스 영상을 복수의 서브 로보틱스 영상으로 분할한다. 본 발명의 실시예에서는, 상기 단말에서 사용자 입력에 따라 해당 제 1 로보틱스 영상 중 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 계층라벨(또는 추가 계층라벨값)을 설정(또는 수신/입력)하는 것을 주로 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 서버는 해당 제 1 로보틱스 영상 및 해당 제 1 로보틱스 영상과 관련한 비교 대상 영상에 대한 영상 분석 기능을 수행하고, 영상 분석 기능 수행 결과를 근거로 해당 제 1 로보틱스 영상에 대해서 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 계층라벨값을 자동으로 각각 설정할 수도 있다. 또한, 상기 서버에서 해당 제 1 로보틱스 영상에 대해 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 계층라벨값을 설정한 경우, 상기 서버는 상기 설정된 해당 제 1 로보틱스 영 상과 관련한 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 계층라벨값에 대 한 정보를 상기 단말에 제공하고, 해당 단말에서 상기 서버에서 설정된 해당 제 1 로보틱스 영 상과 관련한 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 계층라벨값에 대 한 정보를 표시하고, 해당 단말의 사용자 입력에 따라 해당 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 계층라벨값에 대해 최종 승인 여부를 결정하도록 구성할 수도 있다. 제2 로보틱스 영상정보는 제2 집단지성 로보틱스의 예측값인 영상으로, 본 발명에 있어서 반복된 유도 및/또는 추론 알고리즘의 반복된 적용으로 고도화된 예측모델의 예측값으로 정의한다. 제2 로보틱스 영상정보는 도 4 내지 도 6에서 제2 속성과 제2 타깃속성이다. 또한, 상기 서버는 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보 등을 근거로 인공지능 기반의 다른 기계 학습을 수행하여, 다른 기계 학습 결과를 근거로 해당 제 1 로보틱스 영상에 대한 분류값을 생성(또 는 확인)한다. 여기서, 상기 해당 제 1 로보틱스 영상에 대한 분류값(또는 해당 제 1 로보틱스 영상의 분류값) 은 추가 선택라벨링값, 추가 계층라벨링값 등을 동일 항목별로 분류한 값일 수 있다. 즉, 상기 서버는 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보 등을 상기 미리 설정된 분류 모 델의 입력값으로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또 는 다른 인공지능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 로보틱스 영상에 대한 분류값을 생성(또는 확인)한다. 제2 로보틱스 영상정보는 사용자에 의해 제2 로보틱스 선택라벨링 방식으로 라벨링되고, 라벨링된 데이터에 의해 제2 로보틱스 분류모델이 유도 및/또는 추론된다. 분류된 제2 로보틱스 라벨정보는제3 집단지성 로보틱스에 입력된다. 이하 반복된다. 제1 로보틱스 라벨정보 및 제2 기초 로보틱스 영상정보는 제 2 집단지성 로보틱스의 단일 모 델로 학습된다. 제1 기초 로보틱스 영상정보 입력에 의해 제1 집단지성 로보틱스가 프로그래밍 되 고, 제2 기초 로보틱스 영상정보와 제1 로보틱스 라벨정보의 입력에 의해 제2 집단지성 로보틱스 가 프로그래밍 되고, 이하 반복된다. 또한, 상기 서버는 상기 생성된 해당 제 1 로보틱스 영상에 대한 분류값(또는 해당 제 1 로보틱스 영상의 분류값), 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보, 해당 제 1 로보틱스 영상, 해당 제 1 로보 틱스 영상과 관련한 메타 정보, 상기 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 입력값으 로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또는 다른 인공지 능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 로보틱스 영상에 대응하는 제 2 로보틱스 영상을 생성한다. 이 때, 상기 제 2 로보틱스 영상은 상기 제 1 로보틱스 영상을 근거로 생성되는 아바타, 아이템, 로봇 등의 동작 관련 영상, 상기 제 1 로보틱스 영상이 업데이트된 영상 등일 수 있다. 즉, 상기 서버는 상기 생성된 해당 제 1 로보틱스 영상에 대한 분류값(또는 해당 제 1 로보틱스 영상의 분 류값), 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보, 해당 제 1 로보틱스 영상, 해당 제 1 로보틱 스 영상과 관련한 메타 정보, 상기 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 상기 미리 설정된 예측 모델의 입력값으로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또는 다른 인공지능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 로보틱스 영상과 관련한 제 2 로보 틱스 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 2 로보틱스 영상을 상기 단말에 전송(또는 제공)한다. 도 19를 참조하면, 모델 관점에서 메타버스의 출력 및/또는 생성 데이터인 제2 기초 로보틱스 영상정보와 제1 집단지성 로보틱스의 출력 및/또는 생성 데이터인 제1 로보틱스 라벨정보의 정확도나 정교함의 장단점이 서로 상이할 수 있는데, 두 과정이 서로 다른 형태의 라벨링 과정이지만, 모델이 두 과정의 장점만을 수용하게 하기 위해서는 양 라벨 접근방법의 수정된 제2 기초 로보틱스 영상정보와 출력 및/또는 생성 데 이터인 제1 로보틱스 라벨정보는 서로 다른 모델이 아닌 동일 모델(단일모델)의 학습 데이터로 사용된다. 제1 로보틱스 선택라벨링 및 제1 로보틱스 분류모델로 1차 라벨링 된 이후 제안한 출력 및/또는 생 성 데이터인 제1 로보틱스 라벨정보는 제2 로보틱스 선택라벨링에 의해 2차 라벨링 되고, 이 과정 은 계속하여 반복된다. 과거 라벨된 데이터(제1 로보틱스 라벨정보, 1905)는 다른 라벨 데이터(제2 기초 로보틱 스 영상정보, 1907)와 함께 반복 수행하게 되며, 한번 학습했던 데이터 및/또는 유사한 레이블 값 또한 매 반복 학습(epoch)에 계속 등장하여, 여러 번의 실험을 거치는 과정이 필요하다. 매 에포크(epoch)는 누적된 단위 레 이블의 총 개수(batch size) 만큼을 학습 연산 단위(mini batch size)로 분할하여 다양한 실험을 하게 되며, 해 당 과정에서 집단지성의 레이블 값은, 취사 선택 및 평균화 되어 모델에 반영된다. 집단지성 로보틱스의 로보틱스 선택라벨링은 아바타, 인간, 로봇 등의 선택라벨링과 동일한 방식이다. 본 발명의 일 실시예에서, 움직임이나 조합의 범위가 한정돼 있는 자동 수술 및 치과시술 로봇 경우에는 계층적 군집화를 할 필요없이 로보틱스 선택라벨링을 통하여 영상정보의 잘된 부분과 잘못된 부분을 하나하나 세 밀하게 라벨링을 수행한다. 자유도 및/또는 추론이 높은 휴머노이드 로봇이나 비히클 로봇(예를 들어 댄스하는 로봇, 축구하는 로봇, 2족 보행 로봇 등 포함)의 경우에는, 계층적 군집화를 위한 도 17에서 시계열분할 라벨링 , 신체부위별 선택라벨링 등을 통하여 디지털 단위 3 및/또는 디지털 단위 4 및/또는 디지털단위 5로 로보틱스 영상정보를 분할한 후, 로보틱스 선택라벨링을 수행한다. 또한, 상기 서버는 해당 특정 주제와 관련해서, 복수의 단말로부터 수집되는 복수의 실제 인간, 가상 의 아바타나 아이템 등의 동작 관련 영상에 대해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추 론 과정, 생성된 제 1 로보틱스 영상에 대한 추가 선택라벨링 과정, 추가 분류 모델 추론 과정, 추가 예측 모델 추론 과정(예를 들어 앞선 S2910 단계 ~ S2980 단계)을 각각 반복 수행하여, 해당 특정 주제와 관련해서 집단 지성화된 제 2 로보틱스 영상을 생성(또는 업데이트)한다. 이때, 상기 서버는 해당 특정 주제와 관련해서 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상을 제공한 복수의 단말에 마지막으로 업데이트된(또는 최신으로 생성된) 제 2 로보틱스 영상을 실시간 또는 특정 단말의 요청에 따라 제공할 수도 있다.이에 따라, 해당 특정 주제와 관련한 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상을 상기 서버 에 제공한 모든 단말 또는 특정 단말은 해당 특정 주제와 관련해서(또는 해당 특정 주제와 관련 한 비교 대상 영상과 관련해서) 최신의 집단 지성화된 제 2 로보틱스 영상을 제공받을 수 있다. GAN 및/또는 GNN 로보틱스 예측모델에 의해 반복되어 출력 및/또는 생성되는 '제1, 2, 3, …의 로보틱스 영상정 보'는 기초 로보틱스 영상정보와 단일모델로 반복적으로 학습된다. 로보틱스 선택라벨링이 반복적으로 실행된다. 로보틱스 분류모델이 반복적으로 유도 및/또는 추론되고 GAN 및/또는 GNN 로보틱스 예측모델이 반복적으로 유도 및/또는 추론된다. GAN 및/또는 GNN 로보틱스 예측 모델은 집단지성 로보틱스 에 포함된다. 본 발명의 일 실시예에서, 아바타 동작의 정보처리와 동일한 방식의 계층라벨링, 시계열 분할 선택라벨링 , 신체 부위별 선택라벨링 등이 반복적으로 실행된다. 또한, 상기 서버는 블록체인 서버(미도시)와 연동하여, 상기 단말에서 제공한 로우 데이터, 아바타 및/또는 아이템의 동작 관련 영상 등을 근거로 생성되는 제 1 영상, 제 2 영상, 제 1 로보틱스 영상, 제 2 로보 틱스 영상 등을 대상으로 NFT(non-fungible token: 대체 불가 토큰)를 발행(또는 발급)한다. 상기 서버에 의해 발행되는 NFT(또는 NFT 콘텐츠)는 상기 로우 데이터, 상기 아바타 및/또는 아이템의 동 작 관련 영상을 제공한 소유권이 있는 소유자가 소지한 임의의 디지털 아트와 관련되며, 해당 디지털 아트(예를 들어 상기 제 1 영상, 상기 제 2 영상, 상기 제 1 로보틱스 영상, 상기 제 2 로보틱스 영상 등 포함)에 대응하 여 생성된 콘텐츠(또는 MR 콘텐츠/실감 콘텐츠)이며, 원본 디지털 자산에 디지털 파일을 가리키는 주소, 고유 식별 코드(예를 들어 자산 정보, 작성자, 소유자 등에 대한 정보 포함) 등이 토큰에 삽입된 상태일 수 있다. 또한, 상기 서버는 상기 발행된 NFT와 관련해서, 상기 제 1 영상, 상기 제 2 영상, 상기 제 1 로보틱스 영 상, 상기 제 2 로보틱스 영상 등이 표시되는 상기 단말의 화면의 일측에 마커가 함께 표시되도록 구성한다. 또한, 상기 제 1 영상, 상기 제 2 영상, 상기 제 1 로보틱스 영상, 상기 제 2 로보틱스 영상 등의 일측에 표시 되는 마커가 상기 단말의 사용자 터치에 따라 선택되는 경우, 상기 서버는 상기 선택된 마커에 대응 하는 NFT를 확인하고, 상기 확인된 NFT에 대한 정보(예를 들어 자산 정보, 작성자, 소유자 등에 대한 정보 등 포함)를 상기 단말의 화면 일측에(또는 상기 제 1 영상, 상기 제 2 영상, 상기 제 1 로보틱스 영상, 상기 제 2 로보틱스 영상 등이 표시되는 화면에 팝업 형태로) 표시되도록 구성할 수 있다. 이때, 상기 단말은 상기 확인된 NFT에 대한 정보를 가상현실, 증강현실, 혼합현실, 확장현실 등의 형태로 표시할 수도 있다. 또한, 상기 서버는 상기 발행된 상기 제 1 영상, 상기 제 2 영상, 상기 제 1 로보틱스 영상, 상기 제 2 로 보틱스 영상 등과 관련한 NFT에 대해서 거래 기능(또는 판매 기능/소유권 이전 기능) 등을 제공한다. 즉, 상기 영상정보(예를 들어 상기 제 1 영상, 상기 제 2 영상, 상기 제 1 로보틱스 영상, 상기 제 2 로보틱스 영상 등 포함)는 NFT가 부여된 영상정보이고 상기 NFT가 부여된 영상정보 플랫폼 제공 시스템은 사용자 및 참여 자 및 기업들이 이익을 창출하고 돈을 벌면서 재미요소를 배가하는 원순환 구조(flywheel)이다. 상기 도 20을 참조하면, GAN 및/또는 GNN을 이용한 가상 아바타 생성 및/또는 출력 플랫폼 제공 시스템은 사용 자(user), 참여자(인플루언서 또는 SNS에 자신의 캐릭터를 홍보하는 개개인), 기업(광고주 및/또는 제조 사)들이 서로서로 이익을 창출하고 돈을 벌면서 재미요소를 배가하는 플랫폼으로서의 원순환 구조(flywheel)이 다. NFT가 부여된 영상정보는 GAN 및/또는 GNN 예측모델에 의해 반복적으로 출력 및 생성되는 '제1, 2, 3, … 의 영상정보'이다. 상기 도 20을 참조하면, GNN 및/또는 GAN 예측모델은 도 1의 상기 서버에서 작동한다. GNN 및/또는 GAN 예측모델은 사용자 및 인플루언서로부터 제공받은 기초 영상정보(도 15의 제1 기초 영상정보 1501, 제 2 기초 영상정보 1505)를 활용하여 마케팅 플랫폼에 NFT 아바타 및 아이템을 생성하거나 출력한 다. 기업 및 투자자는 인플루언서의 프로필 NFT 및 상품 NFT를 소유할 수 있고 마케팅 및/또는 기업 홍보 에 활용한다. 프로필(예를 들어 동영상, 사진 등 포함)은 생성된 아바타이고, 상품은 아이템이다. 본 발명의 일 실시예에서, 사용자와 인플루언서의 딥페이크를 이용하여, 마케팅 플랫폼에 광고를 하고, 국내외 NFT 마켓에 자동등록할 수 있도록 프로그래밍된다본 발명의 일 실시예에서, 마케팅 플랫폼은 마케팅이 가능한 모든 플랫폼을 의미한다. 메타버스 상에서 NFT는 아바타와 아이템을 현실세계의 소유자, 생성자, 광고주, 실물 상품 등과의 디지털 트윈 (digital twin)을 위한 매개로 연결된다. 또한, 참여자에게는 홍보비를 제공하고, 사용자에게는 아바타 및 아이템에 대한 NFT를 발급함으로써 유일성이 부여되어 가치가 측정되고, 가치에 따른 비용이 환급됨으로써 이익을 창출한다. 도 14를 참조하면, 상기 서버는 사람 몸을 별도 객체화하고, 성별, 나이, 체형, 동양인 등의 정보를 메타 정보와 연결한다. 아이템(상품 등)을 별도 객체화하여 메타정보와 연결한다. 이때, 각 아바타 ID는 유저ID 및 아이템 ID 및 NFT ID와 연결된다. 다양한 현실의 가치 및 재화 정보는 메타 데이터 형태로 포함되어 NFT화될 수 있으며, 이는 아이템 NFT 형태로 유일성이 보장되면서 매매 및 거래가 될 수 있다. 플랫폼은 해당 NFT 소유가 현실의 가치 이용의 사용권이 될 수 있도록 보장하며, 서비스의 사용 내역 및 단계는 플랫폼 데이터베이스와 연동되어, NFT 메타 정보가 갱신되 고 참조된다. 본 발명의 일 실시예에서, NFT 소유에 대한 현실의 가치는 환자의 아바타인 디지털 카데바 사용권 등이 있다. 도 20를 참조하면, 도 1의 상기 서버는 실제 판매되는 제품을 메타버스 내 아이템으로 생성할 수 있고, 현 실에서 실제 제품을 구매할 수 있도록 인스트럭션을 제공할 수 있다. 본 발명의 일 실시예에서, 본 발명의 서비스를 이용하는 인플루언서는 본인의 아바타나 본 발명의 서비스 를 본인의 네트워크상의 SNS에 홍보할 수 있고, 상기 서버는 네트워크상의 SNS 채널에 업로드된 홍보 관련 콘텐츠를 획득할 수 있다. 상기 서버는 네트워크상의 SNS 채널을 통해 유입된 사용자를 분석할 수 있고, 분석된 결과를 기초로 네트워크상의 SNS에게 제공할 홍보비용을 정산할 수 있다. 상기 서버는 각 인플루언 서별로 상이한 링크를 생성하여 제공할 수 있고, 해당 링크를 통해 유입되는 사용자에 대한 보상을 인플 루언서에게 제공할 수 있다. 또한, 상기 서버는 사용자의 가입 여부, 아이템 구매 금액 등을 분석하 여 인플루언서에게 추가 보상을 제공할 수도 있다. 본 발명의 일 실시예에서, 인플루언서는 연예인, 배우, 운동선수 등을 포함한다. 본 발명의 일 실시예에서, 메타버스 내의 땅이나 바다, 건물을 포함하는 각각의 영역에도 NFT가 부여되어, 부동 산 등기부와 같은 역할을 수행하도록 한다. 사용자들은 NFT를 이용하여 각각의 영역을 거래한다. 본 발명의 일 실시예에서, 메타버스 게임에서 각 객체는 무늬, 색, 재질, 디자인 등의 복합 요소로 구성될 수 있고, 상기 서버는 브랜드, 상품ID, 판매자ID, 생성자ID, 광고자ID, 소유자ID 등과 메타 정보를 연동하고 NFT화한다. 또한, 상기 서버는 모자, 액세서리, 의상을 별도 객체화하고, 각 객체는 사용자, 생성자, 유일 성ID 혹은 대표객체ID의 메타 정보와 연결한다. 이때, 각 아이템ID는 NFT ID와 연결될 수 있다. 또한, 상기 서 버는 액세서리 등 사용자가 구매한 아이템에 NFT를 부여하고, 이에 기반한 거래가 메타버스 내에서 가능하 도록 구성한다. 본 발명의 일 실시예에서, 상기 서버는 메타버스 내에 치과 및 성형외과 및/또는 기타 상점 콘텐츠를 제공 하고, 원하는 시술 혹은 수술에 대한 비용 및/또는 아이템에 대한 비용을 지급하면 GAN 및/또는 GNN을 이용하여 아바타 및/또는 디지털 카데바의 일정 부분이나 전체를 변경한다. 상기 서버는 구매 완료된 아이템(예를 들어 수술장비, 수술기구, 수술 기법 등 포함)이 합성된 디지털 카데바 및 자신의 아바타 캐릭터에 NFT를 발급 한다. 사용자는 해당 디지털 카데바에 대한 NFT를 발급받을 수 있으며, 이를 판매하여 수익을 얻는 것이 가능하 다. 즉, 본 발명의 실시예에 따르면, GAN 및/또는 GNN을 통해 디지털 카데바에게 다양한 조합의 아이템을 적용 하면서, 재미요소를 제공하고, 합성이 완료된 디지털 카데바에 대해 NFT를 발급함으로써, 유일성을 제공하며, 이를 통한 수익을 얻을 수도 있다. 본 발명의 일 실시예에서, 상기 서버는 구매 완료된 아이템이 합성된 아바타에 NFT를 발급한다. 사용자는 해당 아바타에 대한 NFT를 발급받을 수 있으며, 이를 판매하여 수익을 얻는 것이 가능하다. 즉, 본 발명의 실시 예에 따르면, GAN 및/또는 GNN을 통해 아바타에게 다양한 조합의 아이템을 코디네이션 하면서, 재미요소 를 제공하고, 합성이 완료된 아바타에 대해 NFT를 발급함으로써, 유일성을 제공하며, 이를 통한 수익을 얻을 수 도 있다. 또한, 상기 서버는 액세서리 등 사용자가 구매한 아이템에 NFT를 부여하고, 이에 기반한 거래가 메타버스 내에서 가능하도록 구성한다.본 발명의 일 실시예에서, 상기 서버는 화장해보기, 옷입어 보기, 화장스타일, 패션스타일 추천받기, 연예 인의 영상에 나의 얼굴을 대입, 스타일 확인해보기 등의 서비스를 제공한다. 또한, 상기 서버는 상기 로우 데이터, 상기 제 1 영상, 상기 제 2 영상, 상기 아바타 및/또는 아이템의 동 작 관련 영상, 상기 제 1 로보틱스 영상, 상기 제 2 로보틱스 영상 등에 대한 라벨링 과정(예를 들어 선택라벨 링 과정, 계층라벨링 과정, 시계열 분할 선택라벨링 과정, 신체부위별 선택라벨링 과정 등 포함)에서의 사용자 입력에 따른 라벨값(예를 들어 잘된 것에 대응하는 승인 라벨, 잘못된 것에 대응하는 거절 라벨 등 포함)을 근 거로 사용자가 행하는 사소한 실수나 치명적인 실수에 대해 보정 및 수행 중지 경보(alert)를 제공하는 상기 영 상정보에 대한 정보 발신의 단계를 수행한다. 상기 도 16을 참조하면, 상기 서버는 사용자의 판단에 의한 잘된 것과 잘못된 것에 대해 선택라벨링(160 4)에 따라 인공지능에게 지도학습을 한다. 또한, 상기 서버는 상기 단말에서 사용자가 행하는 사소한 실수나 치명적인 실수에 대해 보정 및 수행 중지 경보로 개입한다 본 발명의 일 실시예에서, 상기 영상정보는 '제2, 3, 4, …'와 같이 반복된다. 상기 영상정보는 GAN 및/또는 GNN 예측모델의 반복되는 예측값으로 '제1, 2, 3, …의 영상정보'이다. 본 발명의 일 실시예에서, 워닝 시그널(또는 경보 시그널)(warning/alert signal)을 제공하는 방식으로 인간과 집단지성 로보틱스는 상호 작용을 한다. 환자의 생명에 영향을 주는 자동화 수술 인공지능은 그것 스스로 의사를 대체하는 것이 아닌, 의사의 수술 과정에서 정교한 수술을 돕는 로봇팔 조향 장치에 햅틱 개념으로 포함 되어, 잘못된 수술을 시술하려 하는 경우, 진동 등의 워닝 시그널을 줌으로 인하여, 의사와 상호 작용 및 개입 할 수 있다. 해당 경보 시그널을 무시하고, 수술을 집행하는 경우에는, '해당 상황에서 그렇게 행동하는 것이 올바른 정답이다'라는 별도의 라벨 데이터로 사용될 수 있으며, 이를 통해, 가상세계의 인공지능은 실존세계의 의사의 수술에 개입하고 도움을 주면서, 그 피드백에 따라 사용자가 많아 질 수록 정교함이 배가 된다. 본 발명의 일 실시예에서, not ACCEPT 라벨 혹은 REJECT 라벨이 붙여진 동영상에서의 아바타, 인간, 로봇 등의 행위에 대해서, 인공지능은 지도학습하여 경보를 보낸다. 경보는 VR 시뮬레이터에서의 가상 수술이나 가상 운전, 비행 등에도 가능하고 실제 수술이나 실제 운전, 비행 등에서도 가능하다. 본 발명의 일 실시예에서, 경보는 영상정보, 음성정보, 햅틱디바이스 등을 통해서도 가능하다. 본 발명의 일 실시예에서, 외과의사가 위암수술을 하는 경우에 실수가 있다면 동영상에 REJECT 라벨을 붙인다. 인공지능은 이에 대해 지도학습을 하게 된다. 인공지능 의사로봇이 위암수술을 보조하는 경우, 가상 수술게임 및/또는 실제의 위암수술에서 의사의 잘못된 수술 동작을 감지하여 경보를 보낸다. 본 발명의 일 실시예에서, 사용자가 가상의 전쟁게임의 전투기 조정에서 전투기를 조정하는 경우 및/또는 적기 에 의해 격추되는 경우에, 사용자가 이 동영상에 ACCEPT 혹은 REJECT 라벨을 붙인다면 인공지능은 이에 대해 지 도학습을 하게 되고 실제 전투기 조종사의 비행전투에서 잘못된 조정을 감지하여 경보를 보낸다. 예를 들면, VR 트레드밀의 가상의 경찰 게임에서 사용자(도둑 역할)가 물건을 훔치거나 범죄를 저지르는 행위에 대해 REJECT 라벨을 붙인다면, 인공지능은 이를 지도학습하게 되고, 실제 경비 시스템에서 도둑의 행위를 감지 하여 경보를 보낸다. 또한, 상기 영상정보에 대한 정보 발신의 단계는 사용자가 행하는 실수에 대해 보정 동작을 하거나 및 로봇 스 스로 자율동작을 하는 단계일 수 있다. 즉, 도 18 내지 도 19의 집단지성 로보틱스는 로보틱스 선택라벨링된 시각데이터에 대해 지도학습 하고, 인공지능 로봇은 스스로 동작한다. 단말에서 사용자가 행하는 실수에 대해 보정 동작을 하거나 및 자율동작을 수행한다. 본 발명의 일 실시예에서, 상기 로보틱스 영상정보는 '제2, 3, 4, …'와 같이 반복된다. 상기 영상정보는 GAN 및/또는 GNN 로보틱스 예측모델의 반복되는 예측값으로 '제1, 2, 3, …의 로보틱스 영상정 보'이다. 본 발명의 일 실시예에서, ACCEPT 라벨 혹은 REJECT 라벨 혹은 not ACCEPT 라벨 혹은 not REJECT 라벨이 붙은 정보는 인공지능이 사용자에게 경보를 하는데 사용되고, 발생한 문제를 해결하거나 회피하기 위해 인공지능 스 스로 동작하는데 사용된다. 집단지성 로보틱스의 자율동작은 VR 시뮬레이터에서도 가능하고 실제 현실에서도 가능하다. 본 발명의 일 실시예에서, 가상의 수술이나 각종 드론(VEHICLE)의 자율운전, 자율비행 혹은 휴머노이드 로봇의 자율동작 등에서도 가능하다. 본 발명의 일 실시예에서, 고도화된 수술 의료인공지능은 인공 카데바 및 실제 환자에 대해 로봇팔을 사용하여 수술(시술, 치료 등)을 하면서 의사가 행하는 사소한 실수나 치명적인 실수를 보정 및 수행 중지 Alert으로 개 입함으로써 현실세계의 수술에 도움을 줄 수 있다. VR 시뮬레이터를 인공지능 로봇팔이 작동하고 작동 의사가 수술정보에 대해 라벨링을 하는 것을 보상하는 방식으로 게임화한다. 추가로 라벨링 된 수술정보에 대해서는 기 존 알고리즘 모델을 추가 파인 튜닝(fine tuning)하는 방식으로 의료인공지능을 고도화한다. 궁극적으로는 인공 지능 로봇팔이 실제 인체에 대해 수술을 진행하고 이에 대해 의사가 라벨링을 할 수 있다. 본 발명의 일 실시예에서, 가상의 수술 게임에서 자동 수술 로봇은 수술 VR 시뮬레이터에서의 위암수술을 할 수 있다. 시뮬레이터 상의 가상 수술에서 의사가 선택라벨링을 하면 인공지능은 이에 대해 지도학습을 하게 되고 인공지능 의사 로봇은 점차 고도화된다. 고도화된 인공지능 의사 로봇은 실제 수술을 자동으로 할 수 있게 되고 이를 의사가 다시 한번 선택라벨링을 하여 인공지능은 더욱 고도화된다. 반복되는 알고리즘으로 집단지성 로보틱스는 자율동작하는 인공지능 의사 로봇 혹은 인공지능 치과의사 로봇이 된다. 본 발명의 일 실시예에서, 가상의 전투기 비행게임에서 비히클 로봇이 전투기를 조정하여 적기를 격추하는 동영 상에 사용자가 ACCEPT 버튼을 눌러 ACCEPT 라벨을 붙인다면, 인공지능은 이에 대해 지도학습을 하게 되고, 가상 의 혹은 실제 전투기 조종사의 비행기 조정을 학습한다. 가상 전투기 비행게임에서 혹은 실제의 전투기 비행에 서 적극적인 작동으로 회피기동이나 공격기동을 할 수 있다. 본 발명의 일 실시예에서, 가상의 자동차 조정에서 VEHICLE 로봇이 VR 시뮬레이터 상에서 자율주행을 하는 것에 대해 실제 사람이 선택라벨링을 하면, 인공지능은 이를 지도학습하게 되고, VEHICLE 로봇은 점차 고도화 된다. 고도화된 로봇은 실제 주행을 자동으로 할 수 있게 되고, 이를 실제 사람이 다시 한번 로보틱스 선택라벨 링을 하여 인공지능은 더욱 고도화된다. 본 발명의 일 실시예에서, VR 트레드밀의 가상의 댄스 경연에서 휴머노이드 로봇이 댄스 경연을 하는 동영상에 실제 댄스 전문가 혹은 도메인 전문가 혹은 로보틱스 개발자 혹은 사용자가 로보틱스 선택라벨링을 하면, 인공지능은 이에 대해 지도학습을 하게 되고 휴머노이드 로봇의 동작은 점차적으로 고도화된다. 상기 집단 지성을 이용한 정보 처리 시스템은 외부 서버(미도시)를 더 포함할 수 있다. 상기 외부 서버는 네트워크를 통해 서비스 제공 장치인 상기 서버와 연결될 수 있으며, 상기 서버가 GAN 및/또는 GNN을 이용한 가상 아바타의 생성 및/또는 출력 플랫폼 제공 방법을 수행하기 위한 각종 정보를 저 장 및 관리한다. 또한, 상기 외부 서버는 상기 서버가 GAN 및/또는 GNN을 이용한 가상 아바타의 생성 및/또는 출력 플랫폼 제공 방법을 수행함에 따라, 생성 및/또는 출력되는 각종 정보 및 데이터를 제공받아 저장한다. 본 발명의 일 실시예에서, 상기 외부 서버는 상기 서버 외부에 별도로 구비되는 저장 서버이다. 도 21은 본 발명의 실시예에 따른 GNN 및/또는 GAN을 이용한 가상 아바타 및 아이템의 생성 및/또는 출력 플랫 폼 제공 방법을 나타내는 흐름도이다. 도 21을 참조하면, 상기 서버는 사용자로부터 사용자 정보를 획득하고(S2110), 획득된 사용자 정보를 기초 로, 가상의 아바타를 GAN에 의해 생성하거나 GNN에 의해 출력하고(S2120), 상기 아바타를 메타버스 상에 제공하 고(S2130), 상기 아바타를 이용한 메타버스 게임 등을 진행한다(S2140). 본 발명의 일 실시예에서, 상기 서버는 메타버스 국가 플랫폼에서 할 수 있는 게임의 아바타를 제작한다. 재판게임, 경찰게임, 소방관게임, 예술품 창작 게임, 농업게임, 무역게임, 토지개발 게임, 건축게임, 금융투자 게임, 에너지 발전 게임, 국가기관 운영게임, 전쟁 및 전투 게임, 슈팅 게임, 전략 게임, 아케이드 게임, 스포 츠 게임, 오디션 게임 등이 메타버스 국가 내에서 일어날 수 있는 경쟁 게임의 일부이고 디지털 카데바는 아바 타의 일종이다. 이와 같이, 본 발명에 의하면, 메타버스 게임에서 사용자에게는 화장품, 패션 아이템 및 의류를 나의 얼굴 및 몸에 대입 및 다양한 조합으로 생성 및 합성해보는 서비스를 제공할 수 있다.또한, 본 발명에 의하면, 아이템을 제공하는 기업에는 마케팅 광고 플랫폼 및 온라인 구매 연결 플랫폼을 제공 할 수 있으며, 인플루언서에게는 자신의 다양한 이미지 및 영상을 SNS를 경유하여 구매로 유도하고, 이를 트래 킹하는 일련의 마케팅 활동을 수익으로 환원해 주는 플랫폼을 제공할 수 있다. 도 21에서 컴퓨터 프로그램은 사용자로부터 사용자 정보를 획득하는 단계(S2110)와, 획득된 사용자 정보를 기초 로, 가상의 아바타 및 아이템을 생성하거나 출력하는 단계(S2120)와, 상기 아바타를 메타버스 상에 제공하는 단 계(S2130)와, 상기 아바타를 이용한 가상 게임 등을 진행하는 단계(S2140)를 포함하는 가상 아바타의 생성 및/ 또는 출력에 관한 플랫폼 제공 방법을 수행하도록 하는 하나 이상의 인스트럭션을 포함한다. 도 21을 참조하면, 상기 서버는 사용자로부터 사용자 정보를 획득한다(S2110). 사용자 정보는 성별, 나이, 체형, 인종, 사용자의 얼굴 이미지 등을 포함하나, 이에 한정되는 것은 아니다. 상기 서버는 획득된 사용 자 정보를 기초로 가상의 아바타 및 아이템을 생성 혹은 출력한다(S2120). 상기 서버는 아바타를 메타버스 상에 제공하고(S2130), 게임 서버(미도시) 등과 연동하여 아바타를 이용해 각종 게임을 진행한다(S2140). 이와 같이, 사용자로부터 제공되는 특정 콘텐츠와 관련한 하나 이상의 로우 데이터에 대해서 라벨링을 수행하고, 라벨링된 로우 데이터에 대해서 미리 설정된 분류 모델 및 예측 모델을 통해 학습 기능을 수행하고, 예측 모델의 출력값인 제 1 영상에 대해서 추가 라벨링을 수행하고, 추가 라벨링된 제 1 영상에 대해서 분류 모 델 및 예측 모델을 통해 추가 학습 기능을 수행하여 제 2 영상을 출력할 수 있다. 또한, 이와 같이, 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상을 로봇 동작 영상으로 재구성하고, 재구성된 로봇 동작 영상에 대해서 라벨링을 수행하고, 라벨링된 로봇 동작 영상에 대해서 미리 설정된 분류 모 델 및 예측 모델을 통해 학습 기능을 수행하고, 학습 기능 수행 결과인 제 1 로보틱스 영상에 대해서 추가 라벨 링을 수행하고, 추가 라벨링된 제 1 로보틱스 영상에 대해서 분류 모델 및 예측 모델을 통해 추가 학습 기능을 수행하여 제 2 로보틱스 영상을 출력할 수 있다. 이하에서는, 본 발명에 따른 집단 지성을 이용한 정보 처리 방법을 도 1 내지 도 32를 참조하여 상세히 설명한 다. 도 22는 본 발명의 제 1 실시예에 따른 집단 지성을 이용한 정보 처리 방법을 나타낸 흐름도이다. 먼저, 단말은 하나 이상의 시각 세트 장치(미도시)와 연동하여, 특정 주제와 관련해서, 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 수집한다. 여기서, 상기 시각 세트 장치는 카메라부, 라이다, 아이트래커, 모션 캡처 및 모션트래커, 의료장비 (예를 들어 CT, 스캐너, MRI, 의료용 초음파 등) 등을 포함한다. 또한, 상기 특정 주제(또는 특정 콘텐츠)는 의 료 행위(예를 들어 시술, 수술 등 포함), 댄스, 운동 종목(예를 들어 축구, 농구, 탁구 등 포함), 게임, 이-스 포츠(e-sport) 등을 포함한다. 또한, 상기 로우 데이터(raw data)(또는 원본 데이터/소스 데이터/시각 데이터/ 실제 현실의 영상)는 실제 현실에서 획득되는(또는 수집되는/촬영되는/측정되는) 시퀀셜 정지영상(또는 복수의 시퀀셜 정지영상), 동영상, 측정값 등을 포함한다. 여기서, 상기 측정값은 상기 라이다, 상기 아이트래커, 상기 모션 캡처 및 모션트래커, 상기 의료장비 등을 통해 측정되는 영상 정보(또는 3차원 데이터) 등을 포함한다. 또한, 상기 단말은 상기 수집된 특정 주제와 관련한 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보, 단말의 식별 정보 등을 서버 에 전송한다. 여기서, 상기 단말의 식별 정보는 MDN, 모바일 IP, 모바일 MAC, Sim 카드 고유정보, 시리얼 번호 등을 포함한다. 일 예로, 제 1 단말은 제 1 치과 병원에 설치된 시각 세트 장치에 포함된 제 1 카메라부와 연동하여, 제 1 치과 의사의 제 1 수술(예를 들어 임플란트 수술)과 관련한 제 1 로우 데이터, 상기 제 1 로우 데이터와 관련한 메타 정보, 상기 제 1 수술과 관련한 제 1 비교 대상 영상, 상기 제 1 비교 대상 영상과 관련한 메타 정보 등을 수집한다. 또한, 상기 제 1 단말은 상기 수집된 제 1 치과 의사의 제 1 수술(예를 들어 임플란트 수술)과 관련한 제 1 로 우 데이터, 상기 제 1 로우 데이터와 관련한 메타 정보, 상기 제 1 수술과 관련한 제 1 비교 대상 영상, 상기 제 1 비교 대상 영상과 관련한 메타 정보, 상기 제 1 단말의 식별 정보 등을 상기 서버에 전송한다. 다른 일 예로, 제 2 단말은 제 2 댄스 학원에 설치된 시각 세트 장치에 포함된 제 2 카메라부와 연동하여, 홍길동이 블랙핑크의 제니 춤 동작을 따라하는 커버 댄스(cover dance)와 관련한 제 2 로우 데이터, 상기 제 2 로우 데이터와 관련한 메타 정보, 상기 커버 댄스와 관련한 제 2 비교 대상 영상, 상기 제 2 비교 대상 영상과관련한 메타 정보 등을 수집한다. 로우 데이터가 로봇 동작 영상일 경우, 홍길동은 로봇이 되고 블랙핑크의 제 니 춤을 추는 동작은 로봇 동작에 대한 정답 데이터가 된다. 로봇 동작 영상을 평가하고 라벨링을 하는 전문댄 서는 로봇 동작에 대한 평가를 내릴 수 있는 전문가(예시, 로봇 엔지니어)일 수 있다. 또한, 상기 제 2 단말은 상기 수집된 홍길동이 블랙핑크의 제니 춤 동작을 따라하는 커버 댄스와 관련한 제 2 로우 데이터, 상기 제 2 로우 데이터와 관련한 메타 정보, 상기 커버 댄스와 관련한 제 2 비교 대상 영상, 상기 제 2 비교 대상 영상과 관련한 메타 정보, 상기 제 2 단말의 식별 정보 등을 상기 서버에 전송한다 (S2210). 이후, 상기 서버는 상기 단말로부터 전송되는 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보, 단말의 식별 정보 등을 수신한다. 또한, 상기 서버는 상기 수신된 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행한다. 여기서, 상 기 선택라벨링(또는 선택레이블링)은 상기 로우 데이터의 특정 시점(또는 특정 구간)에서의 오류(또는 이상) 유 무에 대한 라벨(label)(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 로우 데이 터 중에서 상기 선택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점(또는 구간)은 미리 설정된 디폴트 라벨값(예를 들어 승인 라벨)이 설정될 수 있다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 로우 데이터에 대해서, 해당 단 말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 로우 데이터 중 특정 시점(또는 특정 구간) 에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 이때, 상기 단말은 해당 단말에 미리 설치된 전용 앱을 실행하고, 전용 앱 실행에 따른 앱 실행 결과 화면을 표시한다. 여기서, 상기 앱 실행 결과 화면은 특정 주제와 관련한 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보 등을 수집하기 위한 수집 메뉴(또는 버튼/항목), 수집된 정보나 상기 서버로부 터 제공되는 정보를 표시하기 위한 보기 메뉴, 환경 설정을 위한 설정 메뉴 등을 포함한다. 이때, 상기 단말 은 해당 전용 앱을 제공하는 상기 서버에 회원 가입한 상태로, 회원 가입에 따른 아이디 및 비밀번호, 상기 아이디를 포함하는 바코드 또는 QR 코드 등을 이용해서 상기 전용 앱 실행 시 로그인 절차를 수 행하여, 해당 전용 앱의 하나 이상의 기능(예를 들어 로우 데이터 수집 기능, 정보/영상에 대한 계층라벨링 기 능, 정보/영상에 대한 선택라벨링 기능, 정보/영상에 대한 시계열 분할 선택라벨링 기능, 정보/영상에 대한 신 체부위별 선택라벨링 기능 등 포함)을 수행할 수 있다. 또한, 상기 단말에 표시되는 앱 실행 결과 화면에서 미리 설정된 보기 메뉴가 선택되는 경우, 상기 단말 은 수집된 정보나 상기 서버로부터 제공되는 정보를 표시하기 위해서, 상기 선택된 보기 메뉴에 대응 하는 보기 화면을 표시한다. 여기서, 상기 보기 화면은 상기 로우 데이터나 생성된 영상을 표시하기 위한 영상 표시 영역, 상기 비교 대상 영상을 표시하기 위한 비교 대상 영상 표시 영역, 계층라벨링을 위해 변수값(또는 라벨값)을 선택하기 위한 계층라벨 입력 메뉴, 선택라벨링을 위해 설정값을 선택하기 위한 선택라벨 입력 메뉴, 동영상에 대한 재생/일시정지/멈춤 기능 등을 제공하기 위한 재생바 등을 포함한다. 또한, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 수집된 로우 데이터를 상기 영상 표시 영역에 표시(또는 출력)하고, 상기 수집된 로우 데이터에 대응하는 비교 대상 영상(또는 상기 서버로부터 제공받은 해당 로우 데이터에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표시(또는 출력)한 다. 이때, 상기 단말은 상기 로우 데이터 및 상기 비교 대상 영상에 각각 대응하는 메타 정보를 근거로 해 당 로우 데이터 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 로우 데이터 및 비교 대상 영상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 로우 데이터에 대해서 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 특정 시점(또는 특정 구간)에서의 해당 로우 데이터에 포함 된 객체의 움직임(또는 객체의 행위)에 대한 잘된 행위 또는 잘못된 행위에 대해 라벨(또는 라벨값)을 설정(또 는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 로우 데이터의 하나 이상의 특정 시점에서 사용자 입력 에 따라 잘된 행위에 대한 라벨값(예를 들어 미리 설정된 승인/승낙/ACCEPT 라벨) 또는 잘못된 행위에 대한 라 벨값(예를 들어 미리 설정된 거절/REJECT 라벨)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련한 로우 데이터에 대해서, 해당 특정 주제와 관련한 전문가 인 해당 단말의 사용자 입력에 따라, 하나 이상의 특정 시점(또는 특정 구간)에서 하나 이상의 선택라벨 (또는 선택라벨값)을 각각 설정(또는 수신/입력)한다. 또한, 상기 단말은 상기 로우 데이터와 관련한 하나 이상의 특징 시점(또는 특정 구간)에서의 하나 이상의 선택라벨값, 해당 로우 데이터의 메타 정보, 해당 단말의 식별 정보 등을 상기 서버에 전송한다. 또한, 상기 서버는 상기 단말로부터 전송되는 상기 로우 데이터와 관련한 하나 이상의 특징 시점(또 는 특정 구간)에서의 하나 이상의 선택라벨값, 해당 로우 데이터의 메타 정보, 해당 단말의 식별 정보 등 을 수신한다. 이때, 해당 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행하기 이전에 또는 수행한 이후에, 상기 서버 는 상기 단말과 연동하여, 해당 하나 이상의 로우 데이터를 대상으로 계층라벨링을 수행하고, 계층라 벨링 수행 전/후로 해당 하나 이상의 로우 데이터를 대상으로 선택라벨링을 수행할 수도 있다. 여기서, 상기 계 층라벨링(또는 계층레이블링)은 사용자에 의한 입력 피처 엔지니어링(input feature engineering)으로, 해당 로 우 데이터에 대한 특징을 나타내는 라벨(또는 라벨값)을 붙이고, 해당 로우 데이터를 특징에 따라 복수의 서브 로우 데이터로 분할(또는 분류)하는 라벨링 방법을 나타낸다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 로우 데이터에 대해서, 해당 특 정 주제와 관련해서 미리 설정된 복수의 라벨 분류를 참조하여(또는 근거로), 해당 단말의 사용자 입력(또 는 사용자 선택/터치/제어)에 따라, 해당 로우 데이터 중 다른 특정 시점(또는 다른 특정 구간)에서의 라벨(또 는 라벨값)을 설정(또는 수신/입력)한다. 일 예로, 상기 제 1 단말은 해당 제 1 단말에 미리 설치된 닥터다비드 앱을 실행하고, 닥터다비드 앱 실행 결과 화면을 표시한다. 이때, 상기 제 1 단말의 제 1 치과 의사는 제 1 아이디와 제 1 비밀번호를 이용해서 해당 닥 터다비드 앱에 로그인한 상태일 수 있다. 또한, 상기 닥터다비드 앱 실행 결과 화면 중에서 보기 메뉴가 선택될 때, 도 23에 도시된 바와 같이, 상기 제 1 단말은 상기 선택된 보기 메뉴에 대응하는 보기 화면을 표시한다. 또한, 상기 보기 화면 내의 재생바가 선택될 때, 도 24에 도시된 바와 같이, 상기 제 1 단말은 상 기 영상 표시 영역에 상기 수집된 제 1 로우 데이터를 출력하고, 상기 비교 대상 영상 표시 영역에 상기 수집된 제 1 비교 대상 영상을 출력한다. 이때, 상기 제 1 단말은 상기 제 1 로우 데이터와 상기 제 1 비 교 대상 영상을 동기화한 상태에서 출력한다. 또한, 상기 제 1 단말은 미리 설정된 앞선 [표 1] 내지 [표 4]에 따른 라벨 분류를 참조하여, 상기 출력되는 제 1 로우 데이터에 대해서 해당 제 1 수술(예를 들어 임플란트 수술)과 관련해서 해당 제 1 단말의 제 1 치과 의 사 입력에 따라, 상기 제 1 로우 데이터에 대한 제 1-1 계층 라벨값(예를 들어 S1에 대응하는 치과 임플란트 수 술), 제 1-2 계층 라벨값(예를 들어 S2에 대응하는 하악 구치부 골폭이 좁은 증례), 제 1-3 계층 라벨값(예를 들어 S3에 대응하는 블록 본(block bone)을 이식한 수술) 등을 각각 수신한다. 또한, 상기 제 1 단말은 상기 제 1 로우 데이터를 미리 설정된 10초 간격으로 분할한다. 또한, 상기 제 1 단말은 상기 보기 화면 내의 상기 영상 표시 영역에 출력되는 상기 제 1 로우 데 이터와 상기 비교 대상 영상 표시 영역에 출력되는 상기 제 1 비교 대상 영상에 대해서, 상기 제 1 치과 의사의 선택에 따라, 제 1-1 시점(예를 들어 1분 10초)에서의 제 1-1 Accept 라벨값, 제 1-2 구간(예를 들어 1 분 45초 ~ 1분 58초)에서의 제 1-2 Reject 라벨값, 제 1-3 시점(예를 들어 2분 20초)에서의 제 1-3 Accept 라 벨값 등을 각각 수신한다. 또한, 상기 제 1 단말은 상기 제 1 로우 데이터와 관련한 제 1-1 시점(예를 들어 1분 10초)에서의 제 1-1 Accept 라벨값, 제 1-2 구간(예를 들어 1분 45초 ~ 1분 58초)에서의 제 1-2 Reject 라벨값, 제 1-3 시점(예를 들어 2분 20초)에서의 제 1-3 Accept 라벨값, 상기 제 1 로우 데이터에 대한 제 1-1 계층 라벨값(예를 들어 S1 에 대응하는 치과 임플란트 수술), 제 1-2 계층 라벨값(예를 들어 S2에 대응하는 하악 구치부 골폭이 좁은 증례), 제 1-3 계층 라벨값(예를 들어 S3에 대응하는 블록 본을 이식한 수술), 상기 분할에 대한 정보(예를 들 어 10초 간격 분할), 상기 제 1 로우 데이터와 관련한 메타 정보, 상기 제 1 단말의 식별 정보 등을 상기 서버 에 전송한다. 또한, 상기 서버는 상기 제 1 단말로부터 전송되는 상기 제 1 로우 데이터와 관련한 제 1-1 시점(예를 들 어 1분 10초)에서의 제 1-1 Accept 라벨값, 제 1-2 구간(예를 들어 1분 45초 ~ 1분 58초)에서의 제 1-2 Reject라벨값, 제 1-3 시점(예를 들어 2분 20초)에서의 제 1-3 Accept 라벨값, 상기 제 1 로우 데이터에 대한 제 1-1 계층 라벨값(예를 들어 S1에 대응하는 치과 임플란트 수술), 제 1-2 계층 라벨값(예를 들어 S2에 대응하는 하악 구치부 골폭이 좁은 증례), 제 1-3 계층 라벨값(예를 들어 S3에 대응하는 블록 본을 이식한 수술), 상기 분할에 대한 정보(예를 들어 10초 간격 분할), 상기 제 1 로우 데이터와 관련한 메타 정보, 상기 제 1 단말의 식별 정 보 등을 수신한다. 다른 일 예로, 상기 제 2 단말은 해당 제 2 단말에 미리 설치된 닥터다비드 앱을 실행하고, 닥터다비드 앱 실행 결과 화면을 표시한다. 이때, 상기 제 2 단말의 제 2 전문 댄서는 제 2 아이디와 제 2 비밀번호를 이용해서 해 당 닥터다비드 앱에 로그인한 상태일 수 있다. 또한, 상기 닥터다비드 앱 실행 결과 화면 중에서 보기 메뉴가 선택될 때, 도 25에 도시된 바와 같이, 상기 제 2 단말은 상기 선택된 보기 메뉴에 대응하는 보기 화면을 표시한다. 또한, 상기 보기 화면 내의 재생바가 선택될 때, 도 26에 도시된 바와 같이, 상기 제 2 단말은 상 기 영상 표시 영역에 상기 수집된 제 2 로우 데이터를 출력하고, 상기 비교 대상 영상 표시 영역에 상기 수집된 제 2 비교 대상 영상을 출력한다. 이때, 상기 제 2 단말은 상기 제 2 로우 데이터와 상기 제 2 비 교 대상 영상을 동기화한 상태에서 출력한다. 또한, 상기 제 2 단말은 미리 설정된 앞선 [표 7] 내지 [표 11]에 따른 라벨 분류를 참조하여, 상기 출력되는 제 2 로우 데이터에 대해서 해당 홍길동의 커버 댄스와 관련해서 해당 제 2 단말의 제 2 전문 댄서 입력에 따라, 상기 제 2 로우 데이터에 대한 제 2-1 계층 라벨값(예를 들어 S1에 대응하는 블랙핑크 제니), 제 2-2 계 층 라벨값(예를 들어 S2에 대응하는 마지막처럼(3분 14초), 제 2-3 계층 라벨값(예를 들어 S3에 대응하는 열린 음악회 2022년 7월 8일 방송) 등을 각각 수신한다. 또한, 상기 제 2 단말은 상기 제 2 로우 데이터를 미리 설정된 3초 간격으로 분할한다. 또한, 상기 제 2 단말은 상기 보기 화면 내의 상기 영상 표시 영역에 출력되는 상기 제 2 로우 데 이터와 상기 비교 대상 영상 표시 영역에 출력되는 상기 제 2 비교 대상 영상에 대해서, 상기 제 2 전문 댄서의 선택에 따라, 제 2-1 구간(예를 들어 30초 ~ 45초)에서의 제 2-1 Reject 라벨값, 제 2-2 구간(예를 들어 1분 10초 ~ 1분 20초)에서의 제 2-2 Accept 라벨값, 제 2-3 시점(예를 들어 1분 50초)에서의 제 2-3 Accept 라 벨값 등을 각각 수신한다. 또한, 상기 제 2 단말은 상기 제 2 로우 데이터와 관련한 제 2-1 구간(예를 들어 30초 ~ 45초)에서의 제 2-1 Reject 라벨값, 제 2-2 구간(예를 들어 1분 10초 ~ 1분 20초)에서의 제 2-2 Accept 라벨값, 제 2-3 시점(예를 들어 1분 50초)에서의 제 2-3 Accept 라벨값, 상기 제 2 로우 데이터에 대한 제 2-1 계층 라벨값(예를 들어 S1 에 대응하는 블랙핑크 제니), 제 2-2 계층 라벨값(예를 들어 S2에 대응하는 마지막처럼(3분 14초), 제 2-3 계층 라벨값(예를 들어 S3에 대응하는 열린 음악회 2022년 7월 8일 방송), 상기 분할에 대한 정보(예를 들어 3초 간 격 분할), 상기 제 2 로우 데이터와 관련한 메타 정보, 상기 제 2 단말의 식별 정보 등을 상기 서버에 전 송한다. 또한, 상기 서버는 상기 제 2 단말로부터 전송되는 상기 제 2 로우 데이터와 관련한 제 2-1 구간(예를 들 어 30초 ~ 45초)에서의 제 2-1 Reject 라벨값, 제 2-2 구간(예를 들어 1분 10초 ~ 1분 20초)에서의 제 2-2 Accept 라벨값, 제 2-3 시점(예를 들어 1분 50초)에서의 제 2-3 Accept 라벨값, 상기 제 2 로우 데이터에 대한 제 2-1 계층 라벨값(예를 들어 S1에 대응하는 블랙핑크 제니), 제 2-2 계층 라벨값(예를 들어 S2에 대응하는 마 지막처럼(3분 14초), 제 2-3 계층 라벨값(예를 들어 S3에 대응하는 열린 음악회 2022년 7월 8일 방송), 상기 분 할에 대한 정보(예를 들어 3초 간격 분할), 상기 제 2 로우 데이터와 관련한 메타 정보, 상기 제 2 단말의 식별 정보 등을 수신한다(S2220). 이후, 상기 서버는 상기 선택라벨링된 로우 데이터에 대한 정보 등을 근거로 인공지능 기반의 기계 학습을 수행하여, 기계 학습 결과를 근거로 해당 로우 데이터에 대한 분류값을 생성(또는 확인)한다. 여기서, 상기 해 당 로우 데이터에 대한 분류값(또는 해당 로우 데이터의 분류값)은 선택라벨링값, 계층라벨링값 등을 동일 항목 별로 분류한 값일 수 있다. 즉, 상기 서버는 상기 선택라벨링된 로우 데이터에 대한 정보 등을 미리 설정된 분류 모델의 입력값으로 하여 기계 학습(또는 인공지능/딥 러닝)을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거 로 해당 로우 데이터에 대한 분류값을 생성(또는 확인)한다.일 예로, 상기 서버는 상기 선택라벨링된 제 1 로우 데이터에 대한 정보인 제 1-1 시점(예를 들어 1분 10 초)에서의 제 1-1 Accept 라벨값, 제 1-2 구간(예를 들어 1분 45초 ~ 1분 58초)에서의 제 1-2 Reject 라벨값, 제 1-3 시점(예를 들어 2분 20초)에서의 제 1-3 Accept 라벨값 등을 상기 분류 모델의 입력값으로 하여 기계 학 습을 수행하고, 기계 학습 결과를 근거로 해당 제 1 로우 데이터에 대해서 제 1-1 Accept 라벨값 및 제 1-3 Accept 라벨값과, 제 1-2 Reject 라벨값을 분류한다. 다른 일 예로, 상기 서버는 상기 선택라벨링된 제 2 로우 데이터에 대한 정보인 제 2-1 구간(예를 들어 30 초 ~ 45초)에서의 제 2-1 Reject 라벨값, 제 2-2 구간(예를 들어 1분 10초 ~ 1분 20초)에서의 제 2-2 Accept 라벨값, 제 2-3 시점(예를 들어 1분 50초)에서의 제 2-3 Accept 라벨값 등을 상기 분류 모델의 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 해당 제 2 로우 데이터에 대해서 제 2-2 Accept 라벨값 및 제 2-3 Accept 라벨값과, 제 2-1 Reject 라벨값을 분류한다(S2230). 이후, 상기 서버는 상기 생성된 해당 로우 데이터에 대한 분류값(또는 해당 로우 데이터의 분류값), 상기 선택라벨링된 로우 데이터에 대한 정보, 해당 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 상기 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 입력값으로 하여 기계 학습(또는 인공지능/딥 러닝) 을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거로 해당 로우 데이터에 대응하는 제 1 영상을 생성한다. 이때, 상기 제 1 영상은 상기 로우 데이터를 근거로 생성되는 아바타, 아이템, 로봇 등의 동 작 관련 영상, 상기 로우 데이터가 업데이트된 영상(예를 들어 상기 로우 데이터에 포함된 인간/사람의 동작/행 위/행동이 업데이트된 영상) 등일 수 있다. 즉, 상기 서버는 상기 생성된 해당 로우 데이터에 대한 분류값(또는 해당 로우 데이터의 분류값), 상기 선 택라벨링된 로우 데이터에 대한 정보, 해당 로우 데이터, 해당 로우 데이터와 관련한 메타 정보, 상기 비교 대 상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 미리 설정된 예측 모델의 입력값으로 하여 기계 학습 (또는 인공지능/딥 러닝)을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거로 해당 로우 데이터와 관련한 제 1 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 1 영상을 상기 단말에 전송한다. 또한, 상기 단말은 상기 서버로부터 전송되는 상기 제 1 영상을 수신하고, 상기 출력 중인 상기 로우 데이터 대신에 상기 수신된 제 1 영상을 상기 영상 표시 영역에 출력한다. 이때, 상기 단말은 상기 로우 데이터, 상기 비교 대상 영상 및 상기 제 1 영상을 동기화한 상태에서 해당 단말의 화면을 분할하여 동시 에 출력할 수도 있다. 일 예로, 상기 서버는 상기 생성된 해당 제 1 로우 데이터에 대해서 제 1-1 Accept 라벨값 및 제 1-3 Accept 라벨값과, 제 1-2 Reject 라벨값에 대한 분류값, 상기 선택라벨링된 제 1 로우 데이터에 대한 정보인 제 1-1 시점(예를 들어 1분 10초)에서의 제 1-1 Accept 라벨값, 제 1-2 구간(예를 들어 1분 45초 ~ 1분 58초)에서 의 제 1-2 Reject 라벨값, 제 1-3 시점(예를 들어 2분 20초)에서의 제 1-3 Accept 라벨값, 상기 제 1 로우 데 이터, 상기 제 1 로우 데이터와 관련한 메타 정보, 상기 제 1 비교 대상 영상, 상기 제 1 비교 대상 영상과 관 련한 메타 정보 등을 상기 예측 모델의 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 해당 제 1 로우 데이터와 관련한 제 1-1 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 1-1 영상을 상기 제 1 단말에 전송한다. 또한, 상기 제 1 단말은 상기 서버로부터 전송되는 제 1-1 영상을 수신하고, 상기 영상 표시 영역에 출력 중인 상기 제 1 로우 데이터를 대체하여, 상기 수신된 제 1-1 영상을 출력한다. 다른 일 예로, 상기 서버는 상기 생성된 해당 제 2 로우 데이터에 대해서 제 2-2 Accept 라벨값 및 제 2-3 Accept 라벨값과, 제 2-1 Reject 라벨값에 대한 분류값, 상기 선택라벨링된 제 2 로우 데이터에 대한 정보인 제 2-1 구간(예를 들어 30초 ~ 45초)에서의 제 2-1 Reject 라벨값, 제 2-2 구간(예를 들어 1분 10초 ~ 1분 20초) 에서의 제 2-2 Accept 라벨값, 제 2-3 시점(예를 들어 1분 50초)에서의 제 2-3 Accept 라벨값, 상기 제 2 로우 데이터, 상기 제 2 로우 데이터와 관련한 메타 정보, 상기 제 2 비교 대상 영상, 상기 제 2 비교 대상 영상과 관련한 메타 정보 등을 상기 예측 모델의 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 해 당 제 2 로우 데이터와 관련한 제 1-2 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 1-2 영상을 상기 제 2 단말에 전송한다. 또한, 상기 제 2 단말은 상기 서버로부터 전송되는 제 1-2 영상을 수신하고, 상기 영상 표시 영역에 출력 중인 상기 제 2 로우 데이터를 대신하여, 상기 수신된 제 1-2 영상을 출력한다(S2240). 이후, 상기 서버는 상기 제 1 영상을 대상으로 추가 선택라벨링을 수행한다. 여기서, 상기 추가 선택라벨 링(또는 추가 선택레이블링)은 상기 제 1 영상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 오류(또는 이상) 유무에 대한 라벨(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 제 1 영상 중에서 상기 추가 선택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점(또는 구간)은 미리 설정된 디폴 트 라벨값(예를 들어 승인 라벨)이 설정될 수 있다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 제 1 영상에 대해서, 해당 단말 의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 제 1 영상 중 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 이때, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 제 1 영상을 상기 영상 표시 영역에 표 시(또는 출력)하고, 상기 로우 데이터(또는 상기 제 1 영상)에 대응하는 비교 대상 영상(또는 상기 서버로 부터 제공받은 해당 로우 데이터/제 1 영상에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표 시(또는 출력)한다. 이때, 상기 단말은 상기 제 1 영상 및 상기 비교 대상 영상에 각각 대응하는 메타 정 보를 근거로 해당 제 1 영상 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 제 1 영상 및 비교 대상 영상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 제 1 영상에 대해서 해당 단말의 사 용자 입력(또는 사용자 선택/터치/제어)에 따라, 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 해당 제 1 영상에 포함된 객체(또는 아바타)의 움직임(또는 객체/아바타의 행위)에 대한 잘된 행위 또는 잘못된 행위에 대 해 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 제 1 영상의 하나 이상의 또 다른 특정 시점에서 사용자 입력에 따라 잘된 행위에 대한 라벨값(예를 들어 미리 설정된 승인/승낙/ACCEPT 라벨) 또는 잘못된 행위에 대한 라벨값(예를 들어 미리 설정된 거절/REJECT 라벨)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련해서 생성된 제 1 영상에 대해서, 해당 특정 주제와 관련한 전문가인 해당 단말의 사용자 입력에 따라, 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 선택라벨(또는 추가 선택라벨값)을 각각 설정(또는 수신/입력)한다. 이때, 상기 단말은 해당 단말의 사용자 입력에 따라 시계열 분할 선택라벨링 기능 또는 신체부위별 선택라벨링 기능을 수행한다. 상기 단말은 다음의 과정을 통해 시계열 분할 선택라벨링 기능을 수행한다. 즉, 상기 단말은 상기 제 1 영상을 분할한 복수의 서브 영상에 대해서, 사용자 입력에 따라 각각의 서브 영상의 분할 상태가 잘된 상태(또는 잘된 행위)에 대한 라벨값(예를 들어 미리 설정된 승인/승낙/ACCEPT 라벨) 또는 잘못된 상태(또는 잘못된 행위)에 대한 라벨값(예를 들어 미리 설정된 거절/REJECT 라벨)을 각각 입력받고, 해당 복수의 서브 영상의 순서를 정렬하기 위해서 사용자 입력에 따라 해당 복수의 서브 영상의 순서 를 나타내는 라벨값(또는 분할 시점이 잘못되거나 조정이 필요한 경우 분할 시점을 조정하기 위한 라벨값)을 입 력받는다. 상기 신체부위별 선택은 생략될 수 있다. 여기서, 상기 제 1 영상에 대한 복수의 서브 영상으로의 분 할은 상기 로우 데이터에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서브 로우 데이터에 대한 정보를 근 거로 상기 제 1 영상을 상기 복수의 서브 영상으로 분할한 상태이거나 또는, 상기 서버에서의 상기 로우 데이터에 대한 인공지능 기능이나 영상 분석 기능 수행에 따라 상기 제 1 영상을 상기 복수의 서브 영상으로 분 할한 상태일 수 있다. 이에 따라, 상기 단말은 상기 제 1 영상을 대상으로 해당 단말의 사용자 입력에 따라, 복수의 서브 영상의 분할 상태가 잘된 상태와 분할 상태가 잘못된 상태에 대한 라벨값을 각각 입력받고, 해당 복수의 서브 영상에 대한 순서를 정렬하기 위한 라벨값(또는 해당 복수의 서브 영상의 순서를 나타내는 라벨값/분할 시점이 잘못되거나 조정이 필요한 경우 분할 시점을 조정하기 위한 라벨값)을 각각 입력받는다. 또한, 상기 단말은 다음의 과정을 통해 신체부위별 선택라벨링 기능을 수행한다. 즉, 상기 단말은 상기 제 1 영상을 분할한 복수의 서브 영상에 포함된 아바타(또는 객체)에 대해서, 사용자 입 력에 따라 상기 복수의 서브 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타의동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값)을 각각 입력받고, 해당 복수의 서브 영상에 포함된 아 바타(또는 객체)의 동작에서 신체부위별로 동작 순서를 정렬하기 위해서 사용자 입력에 따라 해당 복수의 서브 영상의 순서를 나타내는 라벨값(또는 아바타가 포함된 서브 영상의 순서를 조정하기 위한 라벨값)을 입력받는다. 여기서, 상기 제 1 영상에 대한 복수의 서브 영상으로의 분할은 상기 로우 데이터에 대한 계층라벨 링 기능 수행에 따라 복수로 분할된 서브 로우 데이터에 대한 정보를 근거로 상기 제 1 영상을 상기 복수의 서 브 영상으로 분할한 상태이거나 또는, 상기 서버에서의 상기 로우 데이터에 대한 인공지능 기능이나 영상 분석 기능 수행에 따라 상기 제 1 영상을 상기 복수의 서브 영상으로 분할한 상태일 수 있다. 이에 따라, 상기 단말은 상기 제 1 영상을 대상으로 해당 단말의 사용자 입력에 따라, 복수의 서브 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값)을 각각 입력받고, 해당 복수의 서브 영상에 대한 순서(또는 해당 복수의 서브 영상 에 포함된 아바타의 동작 순서)를 정렬하기 위한 라벨값(또는 해당 복수의 서브 영상의 순서를 나타내는 라벨값 /아바타가 포함된 서브 영상의 순서를 조정하기 위한 라벨값)을 각각 입력받는다. 또한, 상기 단말은 상기 제 1 영상과 관련한 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 의 하나 이상의 추가 선택라벨값, 하나 이상의 시계열 분할 선택라벨값, 하나 이상의 신체부위별 선택라벨값, 복수의 서브 영상의 순서를 정렬하기 위한 라벨값, 해당 단말의 식별 정보 등을 상기 서버에 전송한 다. 또한, 상기 서버는 상기 단말로부터 전송되는 상기 제 1 영상과 관련한 하나 이상의 또 다른 특정 시 점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값, 하나 이상의 시계열 분할 선택라벨값, 하나 이상의 신체부위별 선택라벨값, 복수의 서브 영상의 순서를 정렬하기 위한 라벨값, 해당 단말의 식별 정보 등을 수신한다. 일 예로, 상기 제 1 단말의 보기 화면 내의 재생바가 선택될 때, 도 27에 도시된 바와 같이, 상기 제 1 단말은 상기 영상 표시 영역에 상기 제 1-1 영상을 출력하고, 상기 비교 대상 영상 표시 영역에 상기 제 1 비교 대상 영상을 출력한다. 이때, 상기 제 1 단말은 상기 제 1-1 영상과 상기 제 1 비교 대상 영상을 동기화한 상태에서 출력한다. 여기서, 상기 비교 대상 영상 표시 영역(2720, 2820)에 출력되는 제 1 비교 대상 영상 및 제 2 비교 대상 영상은 아바타 동작에 관한 라벨 분류로 앞선 [표 1] 내지 [표 11]가 영상으로 출력된 것이다. 또한, 상기 제 1 단말은 미리 설정된 앞선 [표 12]에 따른 라벨 분류를 참조하여, 상기 출력되는 제 1-1 영상에 대해서 해당 제 1 수술(예를 들어 임플란트 수술)에서의 세부 동작인 상막 중절치 라미네이트 치료를 위한 11번 치아 삭제 방법과 관련해서 해당 제 1 단말의 제 1 치과 의사 입력에 따라, 상기 제 1-1 영상을 2초 ~ 4초 단위 의 복수의 구간인 제 1-1-1 구간 내지 제 1-1-10 구간으로 분할하고, 상기 분할된 제 1-1-1 구간 내지 제 1-1- 10 구간 각각에 대한 제 1-1-1 라벨값 내지 제 1-1-10 라벨값을 각각 수신한다. 또한, 상기 제 1 단말은 상기 제 1-1-1 구간 내지 제 1-1-10 구간에 대해서, 해당 제 1 단말의 제 1 치과 의사 입력에 따라, 순서를 정렬하기 위한 라벨값(예를 들어 제 1-1-1 구간, 제 1-1-2 구간, 제 1-1-3 구간, 제 1-1-6 구간, 제 1-1-7 구간, 제 1-1-8 구간, 제 1-1-4 구간, 제 1-1-5 구간, 제 1-1-9 구간 및 제 1-1-10 구간으로 정렬하기 위한 라벨값)을 각각 수신한다. 또한, 상기 제 1 단말은 상기 제 1-1 영상과 관련한 상기 제 1-1-1 구간 내지 상기 제 1-1-10 구간 각각에 대한 상기 제 1-1-1 라벨값 내지 상기 제 1-1-10 라벨값, 상기 순서를 정렬하기 위한 라벨값(예를 들어 제 1-1-1 구 간, 제 1-1-2 구간, 제 1-1-3 구간, 제 1-1-6 구간, 제 1-1-7 구간, 제 1-1-8 구간, 제 1-1-4 구간, 제 1-1-5 구간, 제 1-1-9 구간 및 제 1-1-10 구간으로 정렬하기 위한 라벨값), 상기 제 1 단말의 식별 정보 등을 상기 서 버에 전송한다. 또한, 상기 서버는 상기 제 1 단말로부터 전송되는 상기 제 1-1 영상과 관련한 상기 제 1-1-1 구간 내지 상기 제 1-1-10 구간 각각에 대한 상기 제 1-1-1 라벨값 내지 상기 제 1-1-10 라벨값, 상기 순서를 정렬하기 위 한 라벨값(예를 들어 제 1-1-1 구간, 제 1-1-2 구간, 제 1-1-3 구간, 제 1-1-6 구간, 제 1-1-7 구간, 제 1-1-8 구간, 제 1-1-4 구간, 제 1-1-5 구간, 제 1-1-9 구간 및 제 1-1-10 구간으로 정렬하기 위한 라벨값), 상기 제 1 단말의 식별 정보 등을 수신한다. 다른 일 예로, 상기 제 2 단말의 보기 화면 내의 재생바가 선택될 때, 도 28에 도시된 바와 같이, 상기 제 2 단 말은 상기 영상 표시 영역에 상기 제 1-2 영상을 출력하고, 상기 비교 대상 영상 표시 영역에 상기 제 2 비교 대상 영상을 출력한다. 이때, 상기 제 2 단말은 상기 제 1-2 영상과 상기 제 2 비교 대상 영상을 동기화한 상태에서 출력한다. 또한, 상기 제 2 단말은 미리 설정된 앞선 [표 11]에 따른 라벨 분류를 참조하여, 상기 출력되는 제 1-2 영상에 대해서 해당 홍길동이 블랙핑크의 제니 춤 동작을 따라하는 커버 댄스와 관련해서, 상기 제 2 단말의 제 2 전문 댄서 입력에 따라, 상기 제 1-2 영상을 블랙핑크의 제니가 앞/뒤 웨이브할 때 가장 많이 움직이는 신체 부위의 순서에 따라 2초 ~ 4초 단위의 복수의 구간인 제 1-2-1 구간 내지 제 1-2-20 구간으로 분할하고, 상기 분할된 제 1-2-1 구간 내지 제 1-2-20 구간 각각에 대한 제 1-2-1 라벨값 내지 제 1-2-20 라벨값을 각각 수신한다. 또한, 상기 제 2 단말은 상기 제 1-2-1 구간 내지 제 1-2-20 구간에 대해서, 해당 제 2 단말의 제 2 전문 댄서 입력에 따라, 순서를 정렬하기 위한 라벨값(예를 들어 제 1-2-1 구간 내지 제 1-2-7 구간, 제 1-2-13 구간 내지 제 1-2-17 구간, 제 1-2-8 구간 내지 제 1-2-10 구간, 제 1-2-18 구간 내지 제 1-2-20 구간 및 제 1-2-11 구간 내지 제 1-2-12 구간으로 정렬하기 위한 라벨값)을 각각 수신한다. 또한, 상기 제 2 단말은 상기 제 1-2 영상과 관련한 상기 제 1-2-1 구간 내지 상기 제 1-2-20 구간 각에 대한 제 1-2-1 라벨값 내지 제 1-2-20 라벨값, 상기 순서를 정렬하기 위한 라벨값(예를 들어 제 1-2-1 구간 내지 제 1-2-7 구간, 제 1-2-13 구간 내지 제 1-2-17 구간, 제 1-2-8 구간 내지 제 1-2-10 구간, 제 1-2-18 구간 내지 제 1-2-20 구간 및 제 1-2-11 구간 내지 제 1-2-12 구간으로 정렬하기 위한 라벨값), 상기 제 2 단말의 식별 정 보 등을 상기 서버에 전송한다. 또한, 상기 서버는 상기 제 2 단말로부터 전송되는 데이터 셋인 상기 제 1-2 영상과 관련한 상기 제 1-2-1 구간 내지 상기 제 1-2-20 구간 각에 대한 제 1-2-1 라벨값 내지 제 1-2-20 라벨값, 상기 순서를 정렬하기 위한 라벨(예를 들어 제 1-2-1 구간 내지 제 1-2-7 구간, 제 1-2-13 구간 내지 제 1-2-17 구간, 제 1-2-8 구간 내지 제 1-2-10 구간, 제 1-2-18 구간 내지 제 1-2-20 구간 및 제 1-2-2 구간 내지 제 1-2-12 구간으로 정렬하기 위 한 라벨), 상기 제 2 단말의 식별 정보 등을 수신한다(S2250). 이후, 상기 서버는 상기 추가 선택라벨링된 제 1 영상에 대한 정보 등을 근거로 인공지능 기반의 다른 기 계 학습을 수행하여, 다른 기계 학습 결과를 근거로 해당 제 1 영상에 대한 분류값을 생성(또는 확인)한다. 여 기서, 상기 해당 제 1 영상에 대한 분류값(또는 해당 제 1 영상의 분류값)은 추가 선택라벨링값, 추가 계층라벨 링값 등을 동일 항목별로 분류한 값일 수 있다. 즉, 상기 서버는 상기 추가 선택라벨링된 제 1 영상에 대한 정보 등을 상기 미리 설정된 분류 모델의 입력 값으로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또는 다른 인 공지능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 영상에 대한 분류값을 생성(또는 확인)한다. 일 예로, 상기 서버는 상기 추가 선택라벨링된 제 1-1 영상에 대한 정보인 상기 제 1-1-1 구간 내지 상기 제 1-1-10 구간 각각에 대한 상기 제 1-1-1 라벨값 내지 상기 제 1-1-10 라벨값을 상기 분류 모델의 입력값으로 하여 다른 기계 학습을 수행하고, 다른 기계 학습 결과를 근거로 해당 제 1-1 영상에 대해서 Accept 라벨인 제 1-1-1 라벨값 내지 제 1-1-7 라벨값 및 제 1-1-10 라벨값과, Reject 라벨인 제 1-1-8 라벨값 내지 제 1-1-9 라 벨값을 분류한다. 다른 일 예로, 상기 서버는 상기 추가 선택라벨링된 제 1-2 영상에 대한 정보인 상기 제 1-2-1 구간 내지 상기 제 1-2-20 구간 각에 대한 제 1-2-1 라벨값 내지 제 1-2-20 라벨값을 상기 분류 모델의 입력값으로 하여 다른 기계 학습을 수행하고, 다른 기계 학습 결과를 근거로 해당 제 1-2 영상에 대해서 Accept 라벨인 제 1-2-1 라벨값 내지 제 1-2-8 라벨값, 제 1-2-12 라벨값 내지 제 1-2-20 라벨값과, Reject 라벨인 제 1-2-9 라벨값 내 지 제 1-2-11 라벨값을 분류한다(S2260). 이후, 상기 서버는 상기 생성된 해당 제 1 영상에 대한 분류값(또는 해당 제 1 영상의 분류값), 상기 추가 선택라벨링된 제 1 영상에 대한 정보, 해당 제 1 영상, 해당 제 1 영상과 관련한 메타 정보, 상기 비교 대상 영 상, 해당 비교 대상 영상과 관련한 메타 정보 등을 입력값으로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또는 다른 인공지능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 영상에 대응하는 제 2 영상을 생성한다. 이때, 상기 제 2 영상은 상기 제 1 영상을 근거로 생성되는 아바타, 아 이템, 로봇 등의 동작 관련 영상, 상기 제 1 영상이 업데이트된 영상 등일 수 있다. 즉, 상기 서버는 상기 생성된 해당 제 1 영상에 대한 분류값(또는 해당 제 1 영상의 분류값), 상기 추가 선택라벨링된 제 1 영상에 대한 정보, 해당 제 1 영상, 해당 제 1 영상과 관련한 메타 정보, 상기 비교 대상 영 상, 해당 비교 대상 영상과 관련한 메타 정보 등을 상기 미리 설정된 예측 모델의 입력값으로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또는 다른 인공지능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 영상과 관련한 제 2 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 2 영상을 상기 단말에 전송한다. 또한, 상기 단말은 상기 서버로부터 전송되는 상기 제 2 영상을 수신하고, 상기 출력 중인 상기 제 1 영상 대신에 상기 수신된 제 2 영상을 상기 영상 표시 영역에 출력한다. 이때, 상기 단말은 상기 로우 데 이터, 상기 비교 대상 영상, 상기 제 1 영상 및 상기 제 2 영상을 동기화한 상태에서 해당 단말의 화면을 분할하여 동시에 출력할 수도 있다. 일 예로, 상기 서버는 상기 생성된 해당 제 1-1 영상에 대해서 Accept 라벨인 제 1-1-1 라벨값 내지 제 1- 1-7 라벨값 및 제 1-1-10 라벨값과, Reject 라벨인 제 1-1-8 라벨값 내지 제 1-1-9 라벨값에 대한 분류값, 상기 추가 선택라벨링된 제 1-1 영상에 대한 정보인 상기 제 1-1-1 구간 내지 상기 제 1-1-10 구간 각각에 대한 상기 제 1-1-1 라벨값 내지 상기 제 1-1-10 라벨값, 상기 순서를 정렬하기 위한 라벨값(예를 들어 제 1-1-1 구간, 제 1-1-2 구간, 제 1-1-3 구간, 제 1-1-6 구간, 제 1-1-7 구간, 제 1-1-8 구간, 제 1-1-4 구간, 제 1-1-5 구간, 제 1-1-9 구간 및 제 1-1-10 구간으로 정렬하기 위한 라벨값), 상기 제 1-1 영상, 상기 제 1-1 영상과 관련한 메타 정보, 상기 제 1 비교 대상 영상, 상기 제 1 비교 대상 영상과 관련한 메타 정보 등을 상기 예측 모델의 입력값으로 하여 다른 기계 학습을 수행하고, 다른 기계 학습 결과를 근거로 해당 제 1-1 영상과 관련한 제 2-1 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 2-1 영상을 상기 제 1 단말에 전송한다. 또한, 상기 제 1 단말은 상기 서버로부터 전송되는 제 2-1 영상을 수신하고, 상기 영상 표시 영역에 출력 중인 상기 제 1-1 영상을 대신하여, 상기 수신된 제 2-1 영상을 출력한다. 다른 일 예로, 상기 서버는 상기 생성된 해당 제 1-2 영상에 대해서 Accept 라벨인 제 1-2-1 라벨값 내지 제 1-2-8 라벨값, 제 1-2-12 라벨값 내지 제 1-2-20 라벨값과, Reject 라벨인 제 1-2-9 라벨값 내지 제 1-2-11 라벨값에 대한 분류값, 상기 추가 선택라벨링된 제 1-2 영상에 대한 정보인 상기 제 1-2-1 구간 내지 상기 제 1-2-20 구간 각에 대한 제 1-2-1 라벨값 내지 제 1-2-20 라벨값, 상기 순서를 정렬하기 위한 라벨값(예를 들어 제 1-2-1 구간 내지 제 1-2-7 구간, 제 1-2-13 구간 내지 제 1-2-17 구간, 제 1-2-8 구간 내지 제 1-2-10 구간, 제 1-2-18 구간 내지 제 1-2-20 구간 및 제 1-2-11 구간 내지 제 1-2-12 구간으로 정렬하기 위한 라벨 값), 상기 제 1-2 영상, 상기 제 1-2 영상과 관련한 메타 정보, 상기 제 2 비교 대상 영상, 상기 제 2 비교 대 상 영상과 관련한 메타 정보 등을 상기 예측 모델의 입력값으로 하여 다른 기계 학습을 수행하고, 다른 기계 학 습 결과를 근거로 해당 제 1-2 영상과 관련한 제 2-2 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 2-2 영상을 상기 제 2 단말에 전송한다. 또한, 상기 제 2 단말은 상기 서버로부터 전송되는 제 2-2 영상을 수신하고, 상기 영상 표시 영역에 출력 중인 상기 제 1-2 영상을 대신하여, 상기 수신된 제 2-2 영상을 출력한다(S2270). 이후, 상기 서버는 해당 특정 주제와 관련해서, 복수의 단말로부터 제공되는 복수의 로우 데이터에 대해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정, 생성된 제 1 영상에 대한 추가 선 택라벨링 과정, 추가 분류 모델 추론 과정, 추가 예측 모델 추론 과정(예를 들어 앞선 S2210 단계 ~ S2270 단계)을 각각 반복 수행하여, 해당 특정 주제와 관련해서(또는 해당 특정 주제와 관련한 비교 대상 영상과 관련 해서) 집단 지성화된 제 2 영상을 생성(또는 업데이트)한다. 이때, 상기 서버는 해당 특정 주제와 관련해서 로우 데이터를 제공한 복수의 단말에 마지막으로 업데 이트된(또는 최신으로 생성된) 제 2 영상을 실시간 또는 특정 단말의 요청에 따라 제공할 수도 있다. 이에 따라, 해당 특정 주제와 관련한 로우 데이터를 상기 서버에 제공한 모든 단말 또는 특정 단말 은 해당 특정 주제와 관련해서 최신의 집단 지성화된 제 2 영상을 제공받을 수 있다. 일 예로, 상기 서버는 상기 제 1 단말 이외에 제 101 단말 내지 제 200 단말로부터 각각 제공되 는 상기 제 1 수술(예를 들어 임플란트 수술)과 관련한 제 101 로우 데이터 내지 제 200 로우 데이터 각각에 대 해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정, 생성된 제 1 영상에 대한 추가 선택 라벨링 과정, 추가 분류 모델 추론 과정, 추가 예측 모델 추론 과정을 각각 수행하여, 해당 제 1 수술과 관련해 서 집단 지성화된 제 2 영상을 업데이트한다(S2280). 도 29는 본 발명의 제 2 실시예에 따른 집단 지성을 이용한 정보 처리 방법을 나타낸 흐름도이다. 먼저, 서버는 단말과 연동하여, 특정 주제와 관련해서, 상기 단말에서 출력되는(또는 관리 중인) 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상(또는 인간, 아바타 및 아이템 중 적어도 하나와 관련한 동작 관련 영상), 해당 동작 관련 영상과 관련한 메타 정보 등을 수집한다. 여기서, 상기 특정 주제(또 는 특정 콘텐츠)는 의료 행위(예를 들어 시술, 수술 등 포함), 댄스, 운동 종목(예를 들어 축구, 농구, 탁구 등 포함), 게임, 이-스포츠 등을 포함한다. 또한, 상기 인간과 관련한 동작 관련 영상은 실제 인간(또는 사람/인플 루언서)이 상기 특정 주제와 관련해서 수행 중인 행동(또는 동작/행위)를 획득한(또는 촬영한) 영상일 수 있다. 또한, 상기 아바타 및/또는 아이템의 동작 관련 영상은 해당 특정 주제와 관련한 임의의 로우 데이터를 근거로 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정 등을 통해 생성된 영상일 수 있다. 일 예로, 상기 서버는 제 3 단말과 연동하여, 해당 제 3 단말에서 출력되는 제 3 아바타의 동작과 관 련한 제 3 동작 관련 영상, 상기 제 3 동작 관련 영상과 관련한 메타 정보 등을 수집한다(S2910). 이후, 상기 서버는 상기 수집된 동작 관련 영상을 실제 로봇의 동작으로 구현하기 위해서, 상기 수집된 동 작 관련 영상(또는 상기 수집된 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상)을 로봇 동작 영상으 로 재구성한다. 여기서, 상기 로봇은 치아삭제 VR 시뮬레이터의 시각데이터를 이용하여 치아삭제 VR 시뮬레이터 에서 작동할 수 있는 형태로 제작한 로봇팔, 수술 VR 시뮬레이터의 시각데이터를 이용하여 수술 VR 시뮬레이터 에서 작동할 수 있는 형태로 제작한 로봇팔, VEHICLE VR 시뮬레이터의 시각데이터를 이용하여 VEHICLE 형태로 제작한 로봇, VR 트레드밀에서 작동할 수 있는 휴머노이드 로봇을 포함한다. 즉, 상기 서버는 상기 수집된 동작 관련 영상, 상기 동작 관련 영상과 관련한 메타 정보 등을 근거로 해당 실제 인간, 가상의 아바타나 아이템 등의 동작을 실제 로봇에 적용하기 위해서 해당 동작 관련 영상에 포함된 실제 인간, 가상의 아바타나 아이템 등과 관련한 좌표 정보를 상기 실제 로봇에 적용하기 위한 로봇 좌표 정보 로 변환하여, 해당 동작 관련 영상을 상기 로봇 동작 영상(또는 기초 로보틱스 영상)으로 재구성한다. 또한, 상기 서버는 상기 로봇 동작 영상(또는 재구성된 로봇 동작 영상), 해당 로봇 동작 영상에 대한 메 타 정보, 상기 수집된 동작 관련 영상, 상기 동작 관련 영상과 관련한 메타 정보, 상기 서버에서 관리 중 인 복수의 비교 대상 영상 중에서 상기 수집된 동작 관련 영상(또는 로봇 동작 영상)과 관련해서 검색된 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 상기 서버에 미리 등록된 복수의 단말 중 에서 선택된 특정 단말에 전송한다. 또한, 상기 특정 단말은 상기 서버로부터 전송되는 상기 로봇 동작 영상, 해당 로봇 동작 영상에 대 한 메타 정보, 상기 동작 관련 영상, 상기 동작 관련 영상과 관련한 메타 정보, 상기 동작 관련 영상(또는 로봇 동작 영상)에 대응하는 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 수신한다. 일 예로, 상기 서버는 상기 수집된 제 3 아바타의 동작과 관련한 제 3 동작 관련 영상, 상기 제 3 동작 관 련 영상과 관련한 메타 정보 등을 근거로 해당 제 3 아바타의 동작을 인공관절 수술용 로봇에 적용하기 위해서 상기 제 3 동작 관련 영상을 제 3 로봇 동작 영상으로 재구성한다. 또한, 상기 서버는 상기 재구성된 제 3 로봇 동작 영상, 상기 제 3 로봇 동작 영상과 관련한 메타 정보, 상기 수집된 제 3 아바타의 동작과 관련한 제 3 동작 관련 영상, 상기 제 3 동작 관련 영상과 관련한 메타 정보, 상기 제 3 동작 관련 영상에 대응하는 제 3 비교 대상 영상, 상기 제 3 비교 대상 영상과 관련한 메타 정 보 등을 상기 서버에 미리 등록된 복수의 단말 중에서 선택된 제 4 단말에 전송한다. 또한, 상기 제 4 단말은 상기 서버로부터 전송되는 상기 제 3 로봇 동작 영상, 상기 제 3 로봇 동작 영상 과 관련한 메타 정보, 상기 제 3 아바타의 동작과 관련한 제 3 동작 관련 영상, 상기 제 3 동작 관련 영상과 관 련한 메타 정보, 상기 제 3 동작 관련 영상에 대응하는 제 3 비교 대상 영상, 상기 제 3 비교 대상 영상과 관련 한 메타 정보 등을 수신한다(S2920). 이후, 상기 서버는 상기 로봇 동작 영상을 대상으로 선택라벨링을 수행한다. 여기서, 상기 선택라벨링(또 는 선택레이블링)은 상기 로봇 동작 영상의 특정 시점(또는 특정 구간)에서의 오류(또는 이상) 유무에 대한 라 벨(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 로봇 동작 영상 중에서 상기 선 택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점(또는 구간)은 미리 설정된 디폴트 라벨값(예를 들어 승인 라벨)이 설정될 수 있다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 로봇 동작 영상에 대해서, 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 로봇 동작 영상 중 특정 시점(또는 특정구간)에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 이때, 상기 단말은 해당 단말에 미리 설치된 전용 앱을 실행하고, 전용 앱 실행에 따른 앱 실행 결과 화면을 표시한다. 여기서, 상기 앱 실행 결과 화면은 특정 주제와 관련한 하나 이상의 로우 데이터, 해당 로우 데이터와 관련한 메타 정보 등을 수집하기 위한 수집 메뉴(또는 버튼/항목), 수집된 정보나 상기 서버로부 터 제공되는 정보를 표시하기 위한 보기 메뉴, 환경 설정을 위한 설정 메뉴 등을 포함한다. 이때, 상기 단말 은 해당 전용 앱을 제공하는 상기 서버에 회원 가입한 상태로, 회원 가입에 따른 아이디 및 비밀번호, 상기 아이디를 포함하는 바코드 또는 QR 코드 등을 이용해서 상기 전용 앱 실행 시 로그인 절차를 수 행하여, 해당 전용 앱의 하나 이상의 기능(예를 들어 로우 데이터 수집 기능, 정보/영상에 대한 계층라벨링 기 능, 정보/영상에 대한 선택라벨링 기능, 정보/영상에 대한 시계열 분할 선택라벨링 기능, 정보/영상에 대한 신 체부위별 선택라벨링 기능 등 포함)을 수행할 수 있다. 또한, 상기 단말에 표시되는 앱 실행 결과 화면에서 미리 설정된 보기 메뉴가 선택되는 경우, 상기 단말 은 수집된 정보나 상기 서버로부터 제공되는 정보를 표시하기 위해서, 상기 선택된 보기 메뉴에 대응 하는 보기 화면을 표시한다. 여기서, 상기 보기 화면은 상기 로우 데이터나 생성된 영상을 표시하기 위한 영상 표시 영역, 상기 비교 대상 영상을 표시하기 위한 비교 대상 영상 표시 영역, 계층라벨링을 위해 변수값(또는 라벨값)을 선택하기 위한 계층라벨 입력 메뉴, 선택라벨링을 위해 설정값을 선택하기 위한 선택라벨 입력 메뉴, 동영상에 대한 재생/일시정지/멈춤 기능 등을 제공하기 위한 재생바 등을 포함한다. 또한, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 로봇 동작 영상을 상기 영상 표시 영역 에 표시(또는 출력)하고, 상기 로봇 동작 영상에 대응하는 비교 대상 영상(또는 상기 서버로부터 제공받은 해당 로봇 동작 영상에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표시(또는 출력)한다. 이 때, 상기 단말은 상기 로봇 동작 영상 및 상기 비교 대상 영상에 각각 대응하는 메타 정보를 근거로 해당 로봇 동작 영상 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 로봇 동작 영상 및 비교 대상 영 상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 로봇 동작 영상에 대해서 해당 단말(10 0)의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 특정 시점(또는 특정 구간)에서의 해당 로봇 동작 영상 에 포함된 객체의 움직임(또는 객체의 행위)에 대한 잘된 행위 또는 잘못된 행위에 대해 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 로봇 동작 영상의 하나 이상의 특정 시점에서 사용자 입 력에 따라 잘된 행위에 대한 라벨값(예를 들어 미리 설정된 승인/승낙/ACCEPT 라벨) 또는 잘못된 행위에 대한 라벨값(예를 들어 미리 설정된 거절/REJECT 라벨)을 각각 입력받는다. 이와 같이, 상기 단말은 해당 특정 주제와 관련한 로봇 동작 영상에 대해서, 해당 특정 주제와 관련한 전 문가인 해당 단말의 사용자 입력에 따라, 하나 이상의 특정 시점(또는 특정 구간)에서 하나 이상의 선택라 벨(또는 선택라벨값)을 각각 설정(또는 수신/입력)한다. 또한, 상기 단말은 상기 로봇 동작 영상과 관련한 하나 이상의 특징 시점(또는 특정 구간)에서의 하나 이 상의 선택라벨값, 해당 로봇 동작 영상의 메타 정보, 해당 단말의 식별 정보 등을 상기 서버에 전송 한다. 또한, 상기 서버는 상기 단말로부터 전송되는 상기 로봇 동작 영상과 관련한 하나 이상의 특징 시점 (또는 특정 구간)에서의 하나 이상의 선택라벨값, 해당 로봇 동작 영상의 메타 정보, 해당 단말의 식별 정 보 등을 수신한다. 일 예로, 상기 제 4 단말은 해당 제 4 단말에 미리 설치된 닥터다비드 앱을 실행하고, 닥터다비드 앱 실행 결과 화면을 표시한다. 이때, 상기 제 4 단말의 제 4 외과 수술 전문의사는 제 4 아이디와 제 4 비밀번호를 이용해서 해당 닥터다비드 앱에 로그인한 상태일 수 있다. 또한, 상기 닥터다비드 앱 실행 결과 화면 중에서 보기 메뉴가 선택될 때, 도 30에 도시된 바와 같이, 상기 제 4 단말은 상기 선택된 보기 메뉴에 대응하는 보기 화면을 표시한다. 또한, 상기 보기 화면 내의 재생바가 선택될 때, 도 31에 도시된 바와 같이, 상기 제 4 단말은 상 기 영상 표시 영역에 상기 제 3 로봇 동작 영상을 출력하고, 상기 비교 대상 영상 표시 영역에 상기 제 3 로봇 동작 영상과 관련한 제 3 비교 대상 영상을 출력한다. 이때, 상기 제 4 단말은 상기 제 3 로봇 동 작 영상과 상기 제 3 비교 대상 영상을 동기화한 상태에서 출력한다. 여기서, 상기 제 3 비교 대상 영상은 로봇 동작에 관한 라벨 분류로 [표 1] 내지 [표 11]과 유사한 방식으로 제작되고 로봇 동작에 대한 정답 데이터 셋이 며 영상으로 출력된다. 또한, 상기 제 4 단말은 미리 설정된 복수의 라벨 분류를 참조하여, 상기 출력되는 제 3 로봇 동작 영상에 대해 서 해당 인공관절 수술과 관련해서 해당 제 4 단말의 제 4 외과 수술 전문의사 입력에 따라, 상기 제 3 로봇 동 작 영상에 대한 제 3-1 계층 라벨값(예를 들어 S1에 대응하는 인공관절 수술), 제 3-2 계층 라벨값(예를 들어 S2에 대응하는 우측 무릎 관절), 제 3-3 계층 라벨값(예를 들어 S3에 대응하는 부분 치환술) 등을 각각 수신한 다. 또한, 상기 제 4 단말은 상기 제 3 로봇 동작 영상을 미리 설정된 5초 간격으로 분할한다. 또한, 상기 제 4 단말은 상기 보기 화면 내의 상기 영상 표시 영역에 출력되는 상기 제 3 로봇 동 작 영상과 상기 비교 대상 영상 표시 영역에 출력되는 상기 제 3 비교 대상 영상에 대해서, 상기 제 4 외 과 수술 전문의사의 선택에 따라, 제 3-1 시점(예를 들어 35초)에서의 제 3-1 Reject 라벨값, 제 3-2 구간(예를 들어 1분 10초 ~ 1분 30초)에서의 제 3-2 Accept 라벨값, 제 3-3 구간(예를 들어 1분 35초 ~ 1분 50초)에서의 제 3-3 Accept 라벨값, 제 3-4 구간(예를 들어 2분 5초 ~ 2분 25초)에서의 제 3-4 Accept 라벨값 등을 각각 수 신한다. 또한, 상기 제 4 단말은 상기 제 3 로봇 동작 영상과 관련한 제 3-1 시점(예를 들어 35초)에서의 제 3-1 Reject 라벨값, 제 3-2 구간(예를 들어 1분 10초 ~ 1분 30초)에서의 제 3-2 Accept 라벨값, 제 3-3 구간(예를 들어 1 분 35초 ~ 1분 50초)에서의 제 3-3 Accept 라벨값, 제 3-4 구간(예를 들어 2분 5초 ~ 2분 25초)에서의 제 3-4 Accept 라벨값, 상기 제 3 로봇 동작 영상에 대한 제 3-1 계층 라벨값(예를 들어 S1에 대응하는 인공관절 수 술), 제 3-2 계층 라벨값(예를 들어 S2에 대응하는 우측 무릎 관절), 제 3-3 계층 라벨값(예를 들어 S3에 대응 하는 부분 치환술), 상기 분할에 대한 정보(예를 들어 5초 간격 분할), 상기 제 4 단말의 식별 정보 등을 상기 서버에 전송한다. 또한, 상기 서버는 상기 제 4 단말로부터 전송되는 상기 제 3 로봇 동작 영상과 관련한 제 3-1 시점(예를 들어 35초)에서의 제 3-1 Reject 라벨값, 제 3-2 구간(예를 들어 1분 10초 ~ 1분 30초)에서의 제 3-2 Accept 라벨값, 제 3-3 구간(예를 들어 1분 35초 ~ 1분 50초)에서의 제 3-3 Accept 라벨값, 제 3-4 구간(예를 들어 2 분 5초 ~ 2분 25초)에서의 제 3-4 Accept 라벨값, 상기 제 3 로봇 동작 영상에 대한 제 3-1 계층 라벨값(예를 들어 S1에 대응하는 인공관절 수술), 제 3-2 계층 라벨값(예를 들어 S2에 대응하는 우측 무릎 관절), 제 3-3 계 층 라벨값(예를 들어 S3에 대응하는 부분 치환술), 상기 분할에 대한 정보(예를 들어 5초 간격 분할), 상기 제 4 단말의 식별 정보 등을 수신한다(S2930). 이후, 상기 서버는 상기 선택라벨링된 로봇 동작 영상에 대한 정보 등을 근거로 인공지능 기반의 기계 학 습을 수행하여, 기계 학습 결과를 근거로 해당 로봇 동작 영상에 대한 분류값을 생성(또는 확인)한다. 여기서, 상기 해당 로봇 동작 영상에 대한 분류값(또는 해당 로봇 동작 영상의 분류값)은 선택라벨링값, 계층라벨링값 등을 동일 항목별로 분류한 값일 수 있다. 즉, 상기 서버는 상기 선택라벨링된 로봇 동작 영상에 대한 정보 등을 미리 설정된 분류 모델의 입력값으 로 하여 기계 학습(또는 인공지능/딥 러닝)을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거로 해당 로봇 동작 영상에 대한 분류값을 생성(또는 확인)한다. 일 예로, 상기 서버는 상기 선택라벨링된 제 3 로봇 동작 영상에 대한 정보인 제 3-1 시점(예를 들어 35초)에서의 제 3-1 Reject 라벨값, 제 3-2 구간(예를 들어 1분 10초 ~ 1분 30초)에서의 제 3-2 Accept 라벨값, 제 3-3 구간(예를 들어 1분 35초 ~ 1분 50초)에서의 제 3-3 Accept 라벨값, 제 3-4 구간(예를 들어 2 분 5초 ~ 2분 25초)에서의 제 3-4 Accept 라벨값 등을 상기 분류 모델의 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 해당 제 3 로봇 동작 영상에 대해서 제 3-2 Accept 라벨값, 제 3-3 Accept 라벨값 및 제 3-4 Accept 라벨값과, 제 3-1 Reject 라벨값을 분류한다(S2940). 이후, 상기 서버는 상기 생성된 해당 로봇 동작 영상에 대한 분류값(또는 해당 로봇 동작 영상의 분류값), 상기 선택라벨링된 로봇 동작 영상에 대한 정보, 해당 로봇 동작 영상, 해당 로봇 동작 영상과 관련한 메타 정 보, 상기 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 입력값으로 하여 기계 학습(또는 인공 지능/딥 러닝)을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거로 해당 로봇 동작 영상에대응하는 제 1 로보틱스 영상을 생성한다. 이때, 상기 제 1 로보틱스 영상은 상기 로봇 동작 영상을 근거로 생 성되는 아바타, 아이템, 로봇 등의 동작 관련 영상, 상기 로봇 동작 영상이 업데이트된 영상 등일 수 있다. 즉, 상기 서버는 상기 생성된 해당 로봇 동작 영상에 대한 분류값(또는 해당 로봇 동작 영상의 분류값), 상기 선택라벨링된 로봇 동작 영상에 대한 정보, 해당 로봇 동작 영상, 해당 로봇 동작 영상과 관련한 메타 정 보, 상기 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 미리 설정된 예측 모델의 입력값으로 하여 기계 학습(또는 인공지능/딥 러닝)을 수행하고, 기계 학습 결과(또는 인공지능 결과/딥 러닝 결과)를 근거 로 해당 로봇 동작 영상과 관련한 제 1 로보틱스 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 1 로보틱스 영상을 상기 단말에 전송한다. 또한, 상기 단말은 상기 서버로부터 전송되는 상기 제 1 로보틱스 영상을 수신하고, 상기 출력 중인 상기 로봇 동작 영상 대신에 상기 수신된 제 1 로보틱스 영상을 상기 영상 표시 영역에 출력한다. 이때, 상기 단말은 상기 로봇 동작 영상, 상기 비교 대상 영상 및 상기 제 1 로보틱스 영상을 동기화한 상태에서 해당 단말의 화면을 분할하여 동시에 출력할 수도 있다. 일 예로, 상기 서버는 상기 생성된 해당 제 3 로봇 동작 영상에 대해서 제 3-2 Accept 라벨값, 제 3-3 Accept 라벨값 및 제 3-4 Accept 라벨값과, 제 3-1 Reject 라벨값에 대한 분류값, 상기 선택라벨링된 제 3 로봇 동작 영상에 대한 정보인 제 3-1 시점(예를 들어 35초)에서의 제 3-1 Reject 라벨값, 제 3-2 구간(예를 들어 1 분 10초 ~ 1분 30초)에서의 제 3-2 Accept 라벨값, 제 3-3 구간(예를 들어 1분 35초 ~ 1분 50초)에서의 제 3-3 Accept 라벨값, 제 3-4 구간(예를 들어 2분 5초 ~ 2분 25초)에서의 제 3-4 Accept 라벨값, 상기 제 3 로봇 동 작 영상, 상기 제 3 로봇 동작 영상과 관련한 메타 정보, 상기 제 3 비교 대상 영상, 상기 제 3 비교 대상 영상 과 관련한 메타 정보 등을 상기 예측 모델의 입력값으로 하여 기계 학습을 수행하고, 기계 학습 결과를 근거로 해당 제 3 로봇 동작 영상과 관련한 제 1-3 로보틱스 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 1-3 로보틱스 영상을 상기 제 4 단말에 전송한다. 또한, 상기 제 4 단말은 상기 서버로부터 전송되는 제 1-3 로보틱스 영상을 수신하고, 상기 영상 표시 영 역에 출력 중인 상기 제 3 로봇 동작 영상을 대체하여, 상기 수신된 제 1-3 로보틱스 영상을 출력한다(S2950). 이후, 상기 서버는 상기 제 1 로보틱스 영상을 대상으로 추가 선택라벨링을 수행한다. 여기서, 상기 추가 선택라벨링(또는 추가 선택레이블링)은 상기 제 1 로보틱스 영상의 또 다른 특정 시점(또는 또 다른 특정 구 간)에서의 오류(또는 이상) 유무에 대한 라벨(또는 라벨값)을 설정하는(또는 붙이는) 라벨링 방법을 나타낸다. 이때, 상기 제 1 로보틱스 영상 중에서 상기 추가 선택라벨링에 따라 라벨(또는 라벨값)이 설정되지 않은 시점 (또는 구간)은 미리 설정된 디폴트 라벨값(예를 들어 승인 라벨)이 설정될 수 있다. 즉, 상기 서버는 상기 단말과 연동하여, 해당 단말에 표시되는 제 1 로보틱스 영상에 대해서, 해당 단말의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 해당 제 1 로보틱스 영상 중 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 이때, 상기 단말에 표시되는 앱 실행 결과 화면 내의 보기 화면에 포함된 재생바가 선택되는 경우 또는 해 당 보기 화면 내의 재생 버튼이 선택되는 경우, 상기 단말은 상기 제 1 로보틱스 영상을 상기 영상 표시 영역에 표시(또는 출력)하고, 상기 로봇 동작 영상(또는 상기 제 1 로보틱스 영상)에 대응하는 비교 대상 영상 (또는 상기 서버로부터 제공받은 해당 로봇 동작 영상/제 1 로보틱스 영상에 대응하는 비교 대상 영상)을 상기 비교 대상 영상 표시 영역에 표시(또는 출력)한다. 이때, 상기 단말은 상기 제 1 로보틱스 영상 및 상기 비교 대상 영상에 각각 대응하는 메타 정보를 근거로 해당 제 1 로보틱스 영상 및 상기 비교 대상 영상에 대해 동기화를 수행하여, 동기화된 제 1 로보틱스 영상 및 비교 대상 영상을 상기 영상 표시 영역 및 상기 비교 대상 영상 표시 영역에 각각 표시할 수 있다. 또한, 상기 단말은 상기 단말의 영상 표시 영역에 표시되는 제 1 로보틱스 영상에 대해서 해당 단말 의 사용자 입력(또는 사용자 선택/터치/제어)에 따라, 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 해당 제 1 로보틱스 영상에 포함된 객체(또는 아바타)의 움직임(또는 객체/아바타의 행위)에 대한 잘된 행위 또 는 잘못된 행위에 대해 라벨(또는 라벨값)을 설정(또는 수신/입력)한다. 즉, 상기 단말은 상기 영상 표시 영역에 표시되는 제 1 로보틱스 영상의 하나 이상의 또 다른 특정 시점에 서 사용자 입력에 따라 잘된 행위에 대한 라벨값(예를 들어 미리 설정된 승인/승낙/ACCEPT 라벨) 또는 잘못된 행위에 대한 라벨값(예를 들어 미리 설정된 거절/REJECT 라벨)을 각각 입력받는다.이와 같이, 상기 단말은 해당 특정 주제와 관련해서 생성된 제 1 로보틱스 영상에 대해서, 해당 특정 주제 와 관련한 전문가인 해당 단말의 사용자 입력에 따라, 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서 하나 이상의 추가 선택라벨(또는 추가 선택라벨값)을 각각 설정(또는 수신/입력)한다. 이때, 상기 단말은 해당 단말의 사용자 입력에 따라 시계열 분할 선택라벨링 기능 또는 신체부위별 선택라벨링 기능을 수행한다. 상기 단말은 다음의 과정을 통해 시계열 분할 선택라벨링 기능을 수행한다. 즉, 상기 단말은 상기 제 1 로보틱스 영상을 분할한 복수의 서브 로보틱스 영상에 대해서, 사용자 입력에 따라 각각의 서브 로보틱스 영상의 분할 상태가 잘된 상태(또는 잘된 행위)에 대한 라벨값(예를 들어 미리 설정 된 승인/승낙/ACCEPT 라벨) 또는 잘못된 상태(또는 잘못된 행위)에 대한 라벨값(예를 들어 미리 설정된 거절 /REJECT 라벨)을 각각 입력받고, 해당 복수의 서브 로보틱스 영상의 순서를 정렬하기 위해서 사용자 입력에 따 라 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값(또는 분할 시점이 잘못되거나 조정이 필요한 경 우 분할 시점을 조정하기 위한 라벨값)을 각각 입력받는다. 여기서, 상기 제 1 로보틱스 영상에 대한 복수의 서 브 로보틱스 영상으로 분할은 상기 로봇 동작 영상에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서브 로 봇 동작 영상에 대한 정보를 근거로 상기 제 1 로보틱스 영상을 상기 복수의 서브 로보틱스 영상으로 분할한 상 태이거나 또는, 상기 서버에서의 상기 로봇 동작 영상에 대한 인공지능 기능이나 영상 분석 기능 수행에 따라 상기 제 1 로보틱스 영상을 상기 복수의 서브 로보틱스 영상으로 분할한 상태일 수 있다. 이에 따라, 상기 단말은 상기 제 1 로보틱스 영상을 대상으로 해당 단말의 사용자 입력에 따라, 복수 의 서브 로보틱스 영상의 분할 상태가 잘된 상태와 분할 상태가 잘못된 상태에 대한 라벨값을 각각 입력받고, 해당 복수의 서브 로보틱스 영상에 대한 순서를 정렬하기 위한 라벨값(또는 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값/분할 시점이 잘못되거나 조정이 필요한 경우 분할 시점을 조정하기 위한 라벨값)을 각 각 입력받는다. 또한, 상기 단말은 다음의 과정을 통해 신체부위별 선택라벨링 기능을 수행한다. 즉, 상기 단말은 상기 제 1 로보틱스 영상을 분할한 복수의 로보틱스 서브 영상에 포함된 아바타(또는 객체)에 대해서, 사용자 입력에 따라 상기 복수의 서브 로보틱스 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타의 동작 순서의 잘된 상태 또는 잘못된 상태에 대한 라벨값)을 각각 입력받고, 해당 복 수의 서브 로보틱스 영상에 포함된 아바타(또는 객체)의 동작에서 신체부위별로 동작 순서를 정렬하기 위해서 사용자 입력에 따라 해당 복수의 서브 로보틱스 영상의 순서를 나타내는 라벨값(또는 아바타가 포함된 서브 로 보틱스 영상의 순서를 조정하기 위한 라벨값)을 입력받는다. 여기서, 상기 제 1 로보틱스 영상에 대한 복수의 서브 로보틱스 영상으로의 분할은 상기 로봇 동작 영상에 대한 계층라벨링 기능 수행에 따라 복수로 분할된 서 브 로보틱스 데이터에 대한 정보를 근거로 상기 제 1 로보틱스 영상을 상기 복수의 서브 로보틱스 영상으로 분 할한 상태이거나 또는, 상기 서버에서의 상기 로봇 동작 영상에 대한 인공지능 기능이나 영상 분석 기능 수행에 따라 상기 제 1 로보틱스 영상을 상기 복수의 서브 로보틱스 영상으로 분할한 상태일 수 있다. 이에 따라, 상기 단말은 상기 제 1 로보틱스 영상을 대상으로 해당 단말의 사용자 입력에 따라, 복수 의 서브 로보틱스 영상에 포함된 아바타(또는 객체)의 동작 순서에 대한 라벨값(또는 해당 아바타의 동작 순서 의 잘된 상태 또는 잘못된 상태에 대한 라벨값)을 각각 입력받고, 해당 복수의 서브 로보틱스 영상에 대한 순서 (또는 해당 복수의 서브 로보틱스 영상에 포함된 아바타의 동작 순서)를 정렬하기 위한 라벨값(또는 해당 복수 의 서브 로보틱스 영상의 순서를 나타내는 라벨값/아바타가 포함된 서브 로보틱스 영상의 순서를 조정하기 위한 라벨값)을 각각 입력받는다. 또한, 상기 단말은 상기 제 1 로보틱스 영상과 관련한 하나 이상의 또 다른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값, 하나 이상의 시계열 분할 선택라벨값, 하나 이상의 신체부위별 선택 라벨값, 해당 복수의 서브 로보틱스 영상의 순서를 정렬하기 위한 라벨값, 해당 단말의 식별 정보 등을 상 기 서버에 전송한다. 또한, 상기 서버는 상기 단말로부터 전송되는 상기 제 1 로보틱스 영상과 관련한 하나 이상의 또 다 른 특정 시점(또는 또 다른 특정 구간)에서의 하나 이상의 추가 선택라벨값, 하나 이상의 시계열 분할 선택라벨 값, 하나 이상의 신체부위별 선택라벨값, 해당 복수의 서브 로보틱스 영상의 순서를 정렬하기 위한 라벨값, 해 당 단말의 식별 정보 등을 수신한다.일 예로, 상기 제 4 단말의 보기 화면 내의 재생바가 선택될 때, 도 32에 도시된 바와 같이, 상기 제 4 단말은 상기 영상 표시 영역에 상기 제 1-3 로보틱스 영상을 출력하고, 상기 비교 대상 영상 표시 영역에 상기 제 3 비교 대상 영상을 출력한다. 이때, 상기 제 4 단말은 상기 제 1-3 로보틱스 영상과 상기 제 3 비교 대상 영상을 동기화를 맞춘 상태에서 출력한다. 또한, 상기 제 4 단말은 미리 설정된 복수의 라벨 분류를 참조하여, 상기 출력되는 제 1-3 로보틱스 영상에 대 해서 해당 제 3 수술(예를 들어 인공관절 수술)에서의 세부 동작과 관련해서 해당 제 4 단말의 제 4 외과 수술 전문의사 입력에 따라, 상기 제 1-3 로보틱스 영상을 2초 ~ 4초 단위의 복수의 구간인 제 1-3-1 구간 내지 제 1-3-15 구간으로 분할하고, 상기 분할된 제 1-3-1 구간 내지 제 1-3-15 구간 각각에 대한 제 1-3-1 라벨값 내지 제 1-3-15 라벨값을 각각 수신한다. 또한, 상기 제 4 단말은 상기 제 1-3-1 구간 내지 제 1-3-15 구간에 대해서, 해당 제 4 단말의 제 4 외과 수술 전문의사 입력에 따라, 순서를 정렬하기 위한 라벨값(예를 들어 제 1-3-1 구간 내지 제 1-3-5 구간, 제 1-3-11 구간 내지 제 1-3-15 구간 및 제 1-3-6 구간 내지 제 1-3-10 구간으로 정렬하기 위한 라벨값)을 각각 수신한다. 또한, 상기 제 4 단말은 상기 제 1-3 로보틱스 영상과 관련한 상기 제 1-3-1 구간 내지 제 1-3-15 구간 각각에 대한 상기 제 1-3-1 라벨값 내지 제 1-3-15 라벨값, 상기 순서를 정렬하기 위한 라벨값(예를 들어 제 1-3-1 구 간 내지 제 1-3-5 구간, 제 1-3-11 구간 내지 제 1-3-15 구간 및 제 1-3-6 구간 내지 제 1-3-10 구간으로 정렬 하기 위한 라벨값), 상기 제 4 단말의 식별 정보 등을 상기 서버에 전송한다. 또한, 상기 서버는 상기 제 4 단말로부터 전송되는 상기 제 1-3 로보틱스 영상과 관련한 상기 제 1-3-1 구 간 내지 제 1-3-15 구간 각각에 대한 상기 제 1-3-1 라벨값 내지 제 1-3-15 라벨값, 상기 순서를 정렬하기 위한 라벨값(예를 들어 제 1-3-1 구간 내지 제 1-3-5 구간, 제 1-3-11 구간 내지 제 1-3-15 구간 및 제 1-3-6 구간 내지 제 1-3-10 구간으로 정렬하기 위한 라벨값), 상기 제 4 단말의 식별 정보 등을 수신한다(S2960). 이후, 상기 서버는 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보 등을 근거로 인공지능 기반의 다른 기계 학습을 수행하여, 다른 기계 학습 결과를 근거로 해당 제 1 로보틱스 영상에 대한 분류값을 생성(또 는 확인)한다. 여기서, 상기 해당 제 1 로보틱스 영상에 대한 분류값(또는 해당 제 1 로보틱스 영상의 분류값) 은 추가 선택라벨링값, 추가 계층라벨링값 등을 동일 항목별로 분류한 값일 수 있다. 즉, 상기 서버는 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보 등을 상기 미리 설정된 분류 모 델의 입력값으로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또 는 다른 인공지능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 로보틱스 영상에 대한 분류값을 생성(또는 확인)한다. 일 예로, 상기 서버는 상기 추가 선택라벨링된 제 1-3 로보틱스 영상에 대한 정보인 상기 제 1-3-1 구간 내지 제 1-3-15 구간 각각에 대한 상기 제 1-3-1 라벨값 내지 제 1-3-15 라벨값을 상기 분류 모델의 입력값으로 하여 다른 기계 학습을 수행하고, 다른 기계 학습 결과를 근거로 해당 제 1-3 로보틱스 영상에 대해서 Accept 라벨인 상기 제 1-3-1 라벨값 내지 제 1-3-5 라벨값 및 상기 제 1-3-11 라벨값 내지 제 1-3-15 라벨값과, Reject 라벨인 상기 제 1-3-6 라벨값 내지 제 1-3-10 라벨값을 분류한다(S2970). 이후, 상기 서버는 상기 생성된 해당 제 1 로보틱스 영상에 대한 분류값(또는 해당 제 1 로보틱스 영상의 분류값), 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보, 해당 제 1 로보틱스 영상, 해당 제 1 로보 틱스 영상과 관련한 메타 정보, 상기 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 입력값으 로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또는 다른 인공지 능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 로보틱스 영상에 대응하는 제 2 로보틱스 영상을 생성한다. 이 때, 상기 제 2 로보틱스 영상은 상기 제 1 로보틱스 영상을 근거로 생성되는 아바타, 아이템, 로봇 등의 동작 관련 영상, 상기 제 1 로보틱스 영상이 업데이트된 영상 등일 수 있다. 즉, 상기 서버는 상기 생성된 해당 제 1 로보틱스 영상에 대한 분류값(또는 해당 제 1 로보틱스 영상의 분 류값), 상기 추가 선택라벨링된 제 1 로보틱스 영상에 대한 정보, 해당 제 1 로보틱스 영상, 해당 제 1 로보틱 스 영상과 관련한 메타 정보, 상기 비교 대상 영상, 해당 비교 대상 영상과 관련한 메타 정보 등을 상기 미리 설정된 예측 모델의 입력값으로 하여 다른 기계 학습(또는 다른 인공지능/다른 딥 러닝)을 수행하고, 다른 기계 학습 결과(또는 다른 인공지능 결과/다른 딥 러닝 결과)를 근거로 해당 제 1 로보틱스 영상과 관련한 제 2 로보 틱스 영상을 생성한다.또한, 상기 서버는 상기 생성된 제 2 로보틱스 영상을 상기 단말에 전송한다. 또한, 상기 단말은 상기 서버로부터 전송되는 상기 제 2 로보틱스 영상을 수신하고, 상기 출력 중인 상기 제 1 로보틱스 영상 대신에 상기 수신된 제 2 로보틱스 영상을 상기 영상 표시 영역에 출력한다. 이때, 상 기 단말은 상기 동작 관련 영상, 상기 비교 대상 영상, 상기 제 1 로보틱스 영상 및 상기 제 2 로보틱스 영상을 동기화한 상태에서 해당 단말의 화면을 분할하여 동시에 출력할 수도 있다. 일 예로, 상기 서버는 상기 생성된 해당 제 1-3 로보틱스 영상에 대해서 Accept 라벨인 상기 제 1-3-1 라 벨값 내지 제 1-3-5 라벨값 및 상기 제 1-3-11 라벨값 내지 제 1-3-15 라벨값과, Reject 라벨인 상기 제 1-3-6 라벨값 내지 제 1-3-10 라벨값에 대한 분류값, 상기 추가 선택라벨링된 제 1-3 로보틱스 영상에 대한 정보인 상 기 제 1-3-1 구간 내지 제 1-3-15 구간 각각에 대한 상기 제 1-3-1 라벨값 내지 제 1-3-15 라벨값, 상기 순서를 정렬하기 위한 라벨값(예를 들어 제 1-3-1 구간 내지 제 1-3-5 구간, 제 1-3-11 구간 내지 제 1-3-15 구간 및 제 1-3-6 구간 내지 제 1-3-10 구간으로 정렬하기 위한 라벨값), 상기 제 1-3 로보틱스 영상, 상기 제 1-3 로보 틱스 영상과 관련한 메타 정보, 상기 제 3 비교 대상 영상, 상기 제 3 비교 대상 영상과 관련한 메타 정보 등을 상기 예측 모델의 입력값으로 하여 다른 기계 학습을 수행하고, 다른 기계 학습 결과를 근거로 해당 제 1-3 로 보틱스 영상과 관련한 제 2-3 로보틱스 영상을 생성한다. 또한, 상기 서버는 상기 생성된 제 2-3 로보틱스 영상을 상기 제 4 단말에 전송한다. 또한, 상기 제 4 단말은 상기 서버로부터 전송되는 제 2-3 로보틱스 영상을 수신하고, 상기 영상 표시 영 역에 출력 중인 상기 제 1-3 로보틱스 영상을 대신하여, 상기 수신된 제 2-3 로보틱스 영상을 출력한다(S2980). 이후, 상기 서버는 해당 특정 주제와 관련해서, 복수의 단말로부터 수집되는 복수의 실제 인간, 가상 의 아바타나 아이템 등의 동작 관련 영상에 대해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추 론 과정, 생성된 제 1 로보틱스 영상에 대한 추가 선택라벨링 과정, 추가 분류 모델 추론 과정, 추가 예측 모델 추론 과정(예를 들어 앞선 S2910 단계 ~ S2980 단계)을 각각 반복 수행하여, 해당 특정 주제와 관련해서 집단 지성화된 제 2 로보틱스 영상을 생성(또는 업데이트)한다. 이때, 상기 서버는 해당 특정 주제와 관련해서 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상을 제공한 복수의 단말에 마지막으로 업데이트된(또는 최신으로 생성된) 제 2 로보틱스 영상을 실시간 또는 특정 단말의 요청에 따라 제공할 수도 있다. 이에 따라, 해당 특정 주제와 관련한 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상을 상기 서버 에 제공한 모든 단말 또는 특정 단말은 해당 특정 주제와 관련해서(또는 해당 특정 주제와 관련 한 비교 대상 영상과 관련해서) 최신의 집단 지성화된 제 2 로보틱스 영상을 제공받을 수 있다. 일 예로, 상기 서버는 상기 제 4 단말 이외에 제 201 단말 내지 제 300 단말로부터 각각 제공되 는 상기 제 3 수술(예를 들어 인공관절 수술)과 관련한 제 201 동작 관련 영상 내지 제 300 동작 관련 영상 각 각에 대해서, 앞선 선택라벨링 과정, 분류 모델 추론 과정, 예측 모델 추론 과정, 생성된 제 1 로보틱스 영상에 대한 추가 선택라벨링 과정, 추가 분류 모델 추론 과정, 추가 예측 모델 추론 과정을 각각 수행하여, 해당 제 3 수술과 관련해서 집단 지성화된 제 2 로보틱스 영상을 업데이트한다(S2990). 본 발명의 실시예는 앞서 설명된 바와 같이, 사용자로부터 제공되는 특정 콘텐츠와 관련한 하나 이상의 로우 데 이터에 대해서 라벨링을 수행하고, 라벨링된 로우 데이터에 대해서 미리 설정된 분류 모델 및 예측 모델을 통해 학습 기능을 수행하고, 예측 모델의 출력값인 제 1 영상에 대해서 추가 라벨링을 수행하고, 추가 라벨링된 제 1 영상에 대해서 분류 모델 및 예측 모델을 통해 추가 학습 기능을 수행하여 제 2 영상을 출력하여, 로우 데이터 와 관련한 아바타 및/또는 아이템을 사용자에게 제공하고, 로우 데이터에 대한 라벨링을 통해 인공지능의 추론 능력을 향상시킬 수 있다. 또한, 본 발명의 실시예는 앞서 설명된 바와 같이, 실제 인간, 가상의 아바타나 아이템 등의 동작 관련 영상을 로봇 동작 영상으로 재구성하고, 재구성된 로봇 동작 영상에 대해서 라벨링을 수행하고, 라벨링된 로봇 동작 영 상에 대해서 미리 설정된 분류 모델 및 예측 모델을 통해 학습 기능을 수행하고, 학습 기능 수행 결과인 제 1 로보틱스 영상에 대해서 추가 라벨링을 수행하고, 추가 라벨링된 제 1 로보틱스 영상에 대해서 분류 모델 및 예 측 모델을 통해 추가 학습 기능을 수행하여 제 2 로보틱스 영상을 출력하여, 인공지능에 따른 결과물을 인공지 능의 분류 모델 및 예측 모델에 반복적으로 적용하여 인공지능의 학습 능력을 향상시킬 수 있다. 전술된 내용은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자라면 본 발명의 본질적인 특성에서 벗어 나지 않는 범위에서 수정 및 변형이 가능할 것이다. 따라서, 본 발명에 개시된 실시예들은 본 발명의 기술 사상 을 한정하기 위한 것이 아니라 설명하기 위한 것이고, 이러한 실시예에 의하여 본 발명의 기술 사상의 범위가 한정되는 것은 아니다. 본 발명의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 발명의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2023-0058299", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 집단 지성을 이용한 정보 처리 시스템의 구성을 나타낸 블록도이다. 도 2는 본 발명의 실시예에 따른 로우 데이터(실제 현실 데이터/로봇 동작 영상)의 계층적 군집화의 계통도이다. 도 3은 본 발명의 실시예에 따른 도 4 내지 도 6의 분할된 동작 동영상에서 입체도형의 정의를 나타낸 개념도이 다. 도 4는 본 발명의 실시예에 따른 아바타(인간) 및/또는 로보틱스의 동작에 대한 n개의 정보 수집을 입체도형으 로 나타낸 개념도이다. 도 5는 본 발명의 실시예에 따른 아바타(인간) 및/또는 로보틱스의 동작에 대한 n'개의 정보 수집을 입체도형으 로 나타낸 개념도이다.도 6은 본 발명의 실시예에 따른 아바타(인간) 및/또는 로보틱스의 동작에 대한 N개의 정보 수집을 입체도형으 로 나타낸 개념도이다. 도 7은 본 발명의 실시예에 따른 데이터 단위 3을 기준으로 처리된 계층적 군집화의 계통도이다. 도 8은 본 발명의 실시예에 따른 디지털 단위 3을 기준으로 처리된 계층적 군집화의 계통도이다. 도 9는 본 발명의 실시예에 따른 데이터 단위 4를 기준으로 처리된 계층적 군집화의 계통도이다. 도 10은 본 발명의 실시예에 따른 디지털 단위 4를 기준으로 처리된 계층적 군집화의 계통도이다. 도 11은 본 발명의 실시예에 따른 디지털 단위 5를 기준으로 처리된 계층적 군집화의 계통도이다. 도 12는 본 발명의 실시예에 따른 유도 및/또는 추론 알고리즘에 정보처리된 데이터가 어떤 방식으로 적용되는 지를 나타낸 순서도이다. 도 13은 본 발명의 실시예에 따른 GNN 회귀모델의 원리를 나타내는 도이다. 도 14는 본 발명의 실시예에 따른 GAN을 이용한 가상의 아바타 및 아이템의 생성 방법을 나타내는 도이다. 도 15는 본 발명의 실시예에 따른 기초 영상정보가 계속 수집될 경우, 기존 데이터와 같이 하나의 모델로 적용 되는 원리를 나타내는 도이다. 도 16은 본 발명의 실시예에 따른 서버에서 작동하여 단말에서 출력 및 생성되는 시각렌더링의 원리를 나타내는 도이다. 도 17은 본 발명의 실시예에 따른 라벨링에 의해 디지털 단위가 생성되는 원리를 나타내는 도이다. 도 18은 본 발명의 실시예에 따른 집단지성 로보틱스가 서버에서 작동되는 원리를 나타내는 도이다. 도 19는 본 발명의 실시예에 따른 로보틱스 라벨링에 의해 집단지성 로보틱스가 고도화되는 원리를 나타내는 도 이다. 도 20은 본 발명의 실시예에 따른 사용자 및 참여자 및 기업들이 이익을 창출하고 돈을 벌면서 재미요소를 배가 하는 플랫폼으로서의 원순환 구조를 나타내는 도이다. 도 21은 본 발명의 실시예에 따른 GAN 및/또는 GNN을 이용한 가상 아바타 및 아이템의 생성 및/또는 출력 플랫 폼 제공 방법을 나타내는 도이다. 도 22는 본 발명의 제 1 실시예에 따른 집단 지성을 이용한 정보 처리 방법을 나타낸 흐름도이다. 도 23 내지 도 28은 본 발명의 실시예에 따른 단말의 화면의 예를 나타낸 도이다. 도 29는 본 발명의 제 2 실시예에 따른 집단 지성을 이용한 정보 처리 방법을 나타낸 흐름도이다. 도 30 내지 도 32는 본 발명의 실시예에 따른 단말의 화면의 예를 나타낸 도이다."}
