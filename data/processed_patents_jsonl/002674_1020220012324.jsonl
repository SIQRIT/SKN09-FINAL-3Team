{"patent_id": "10-2022-0012324", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0115638", "출원번호": "10-2022-0012324", "발명의 명칭": "복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제", "출원인": "연세대학교 산학협력단", "발명자": "최종은"}}
{"patent_id": "10-2022-0012324", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터를 포함하는 연산 처리 수단에 의해 각 단계가 수행되는 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법으로서,연산 처리 수단에서, 상기 액추에이터 시스템의 기저장되는 복수의 단위 작업 각각에 대하여, 강화학습을 구성하는 파라미터를 설정하고, 상태 및 행동을 설정하며, 기설정된 보상 함수를 기준으로 보상을 수행하는 전처리단계(S100);정책 평가를 통해, 상기 전처리 단계(S100)에 의한 복수의 단위 작업 각각에 의한 전체 보상의 합이 최대인 행동에 해당하는 최적 정책을 도출하여, 각 단위 정책을 생성하는 저층위 정책 생성 단계(S200);정책 평가를 통해 복수의 단위 작업을 연계 작업들끼리 모듈화하고, 모듈화한 단위 작업에 대하여 가중치를 설정하고, 기설정된 보상 함수를 기준으로 상기 액추에이터 시스템의 행동에 대한 보상을 수행하되, 상기 보상의합이 최대화가 되도록 하는최적 가중치를 산출하는 고층위 정책 생성 단계(S300); 및상기 최적 가중치 및 최적 정책에 따른 제어 신호를 상기 복수의 액추에이터에 전달하여, 상기 액추에이터 시스템을 작동시키는 제어 단계(S400);를 포함하는, 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법."}
{"patent_id": "10-2022-0012324", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 액추에이터 시스템은다관절 로봇 시스템인, 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법."}
{"patent_id": "10-2022-0012324", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서,상기 상태는 상기 다관절 로봇 시스템에 포함되어 작업을 수행하는 엔드 이펙터의 기준점의 3차원 위치 좌표(x,y, z)이며,상기 행동은 상기 다관절 로봇 시스템에 포함되는 각각의 관절을 회전시키기 위한 각 모터의 회전 각도인, 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법."}
{"patent_id": "10-2022-0012324", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1항에 있어서,상기 저층위 정책 생성 단계(S200)는메모리로부터 상기 전처리 단계(S100)에 의한 복수의 단위 작업 각각에 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성하는, 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한제어 방법.공개특허 10-2023-0115638-2-청구항 5 제 4항에 있어서,상기 저층위 정책 생성 단계(S200)는상기 전처리 단계(S100)에 의한 복수의 단위 작업 중 선택되는 일부는 해당하는 각각의 단위 작업에 의한 전체보상의 합이 최대인 행동에 해당하는 최적 정책을 도출하고, 선택되는 또다른 일부는 메모리로부터 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성하는, 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법."}
{"patent_id": "10-2022-0012324", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1항에 있어서,상기 제어 단계(S400)는상기 최적 가중치 및 최적 정책에 따라, 모듈화한 단위 작업이 순차적으로 또는, 동시에 수행될 수 있도록 상기제어 신호를 생성하는, 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법."}
{"patent_id": "10-2022-0012324", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "복수의 액추에이터를 구동시키기 위해 기저장되는 복수의 단위 작업 각각에 대하여, 강화학습을 구성하는 파라미터를 설정하고, 상태 및 행동을 설정하며, 기설정된 보상 함수를 기준으로 보상을 수행하는 전처리 동작부(100);정책 평가를 통해, 상기 전처리 동작부(100)에 의한 복수의 단위 작업 각각에 의한 전체 보상의 합이 최대인 행동에 해당하는 최적 정책을 도출하여, 각 단위 정책을 생성하는 저층위 정책 생성부(200);복수의 단위 작업을 연계 작업들끼리 모듈화하는 모듈화부(300);상기 모듈화부(300)에 의한 모듈화한 단위 작업에 대하여 가중치를 설정하고, 기설정된 보상 함수를 기준으로행동에 대한 보상을 수행하는 후처리 동작부(400);상기 후처리 동작부(400)에 의한 모듈화한 단위 작업 각각에 의한 보상의 합이 최대화가 되도록 정책 평가를 통해, 최적 가중치를 산출하는 고층위 정책 생성부(500); 및최적 가중치 및 최적 정책에 따른 제어 신호를 상기 복수의 액추에이터에 전달하여, 구동을 제어하는 제어부(600);를 포함하는, 계층적 강화학습 기법을 이용한 제어가 적용된 복수의 액추에이터를 포함하는 액추에이터 시스템."}
{"patent_id": "10-2022-0012324", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7항에 있어서,상기 계층적 강화학습 기법을 이용한 제어가 적용된 복수의 액추에이터를 포함하는 액추에이터 시스템은다관절 로봇 시스템이며,상기 상태는 상기 다관절 로봇 시스템에 포함되어 작업을 수행하는 엔드 이펙터의 기준점의 3차원 위치 좌표(x,y, z)이며,상기 행동은 상기 다관절 로봇 시스템에 포함되는 각각의 관절을 회전시키기 위한 각 모터의 회전 각도인, 계층적 강화학습 기법을 이용한 제어가 적용된 복수의 액추에이터를 포함하는 액추에이터 시스템.공개특허 10-2023-0115638-3-청구항 9 제 7항에 있어서,상기 저층위 정책 생성부(200)는메모리로부터 상기 전처리 동작부(100)에 의한 복수의 단위 작업 각각에 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성하는, 계층적 강화학습 기법을 이용한 제어가 적용된 복수의 액추에이터를 포함하는 액추에이터시스템."}
{"patent_id": "10-2022-0012324", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9항에 있어서,상기 저층위 정책 생성부(200)는상기 전처리 동작부(100)에 의한 복수의 단위 작업 중 선택되는 일부는 해당하는 각각의 단위 작업에 의한 전체보상의 합이 최대인 행동에 해당하는 최적 정책을 도출하고, 선택되는 또다른 일부는 메모리로부터 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성하는, 계층적 강화학습 기법을 이용한 제어가 적용된 복수의 액추에이터를 포함하는 액추에이터 시스템."}
{"patent_id": "10-2022-0012324", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 7항에 있어서,상기 제어부(600)는상기 최적 가중치 및 최적 정책에 따라, 모듈화한 단위 작업이 순차적으로 또는, 동시에 수행될 수 있도록 상기제어 신호를 생성하는, 계층적 강화학습 기법을 이용한 제어가 적용된 복수의 액추에이터를 포함하는 액추에이터 시스템."}
{"patent_id": "10-2022-0012324", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법 및 이에 의한 액추에이터 시스템에 관한 것으로서, 복잡한 작업을 각각의 계층 구조의 조합을 통해서 구성하고, 각 계층 구조 별로 학습 및 가중치 작업을 수행함으로써, 복잡한 동작을 포함하는 로봇 작업의 학습 시간을 크게 감소시킬 수 있는 기술에 관한 것이다."}
{"patent_id": "10-2022-0012324", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법 및 이에 의한 액추에이터 시스템에 관한 것으로, 더욱 상세하게는 복잡한 작업을 소분화하고, 각각의 소분화한 작업에 대한 최적 정책을 학습 및 이에 대한 적절한 가중치를 부여한 후, 이들을 다시 조합할 수 있도록 학습함 으로써, 복잡한 작업에 대한 동작 상태를 용이하게 제어할 수 있는 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법 및 이에 의한 액추에이터 시스템에 관한 것이다."}
{"patent_id": "10-2022-0012324", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근들어 다관절 로봇 시스템 등을 의미하는 복수의 액추에이터를 포함하는 액추에이터 시스템은, 단순하게 무 거운 물건을 운반하거나 또는, 위험한 현장에서 작업을 수행하는 데에 더해서 다양한 첨단기술이 융합된 형태로 발전되고 된다. 즉, 수준 높은 복잡한 작업을 수행하도록 진화하고 있는데, 종래의 제어 방법의 경우, 각 단위 작업(pick-and place 작업으로 예를 들자면, 다가가는 모션, 잡는 모션, 잡은 상태에서 움직이는 모션 등)에 대해서 움직임 목 적 변수(end to end)에 대해서만 설정하면, 중간 과정인 이동 경로 또는, 모션 등에 대해서는 제어가 불가능했 다. 다시 말하자면, 수준 높은 복잡한 작업을 수행할 경우, 필연적으로 적절하지 않은 모션 또는, 피해야 하는 이동 경로가 존재할 수 있으나, 이에 대한 제어가 불가능한 문제점이 있다.이러한 문제점을 해결하기 위하여, 본 발명의 일 실시예에 따른 복수의 액추에이터를 포함하는 액추에이터 시스 템에 대한 계층적 강화학습 기법을 이용한 제어 방법 및 이에 의한 액추에이터 시스템은 수준 높은 복잡한 작업 을 소분화한 후, 계층적 강화학습 기법을 이용하여, 각 단위 작업의 상태 및 행동에 대한 보상을 제공한 후, 이 를 다시 모듈화하여 보상을 통한 최적 가중치를 산출함으로써, 수준 높은 복잡한 작업을 성공적으로 수행할 수 있도록 제어할 수 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 국내등록특허 제10-2233739호(등록일자 2021.03.24.)"}
{"patent_id": "10-2022-0012324", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기한 바와 같은 종래 기술의 문제점을 해결하기 위하여 안출된 것으로, 본 발명의 목적은 수행하고 자 하는 기존의 복잡한 작업을 계층 구조로 분해하고, 각각 학습 및 이에 대한 적절한 가중치를 부여한 후, 이 를 다시 조합할 수 있도록 학습함으로써, 복잡한 작업에 대한 동작 상태를 용이하게 제어할 수 있는 복수의 액 추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법 및 이에 의한 액추에 이터 시스템을 제공하는 것이다."}
{"patent_id": "10-2022-0012324", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 컴퓨터를 포함하는 연산 처리 수단에 의해 각 단계가 수행되는 복수의 액추에이터 를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법으로서, 연산 처리 수단에서, 상기 액추에이터 시스템의 기저장되는 복수의 단위 작업 각각에 대하여, 강화학습을 구성하는 파라미터를 설정 하고, 상태 및 행동을 설정하며, 기설정된 보상 함수를 기준으로 보상을 수행하는 전처리 단계(S100), 정책 평 가를 통해, 상기 전처리 단계(S100)에 의한 복수의 단위 작업 각각에 의한 전체 보상의 합이 최대인 행동에 해 당하는 최적 정책을 도출하여, 각 단위 정책을 생성하는 저층위 정책 생성 단계(S200), 복수의 단위 작업을 연 계 작업들끼리 모듈화하고, 모듈화한 단위 작업에 대하여 가중치를 설정하고, 기설정된 보상 함수를 기준으로 상기 액추에이터 시스템의 행동에 대한 보상을 수행하되, 상기 보상의 합을 목적 함수로 정의하여 정책 평가를 통해 상기 목적 함수가 최대화가 되도록 하는 최적 가중치를 산출하는 고층위 정책 생성 단계(S300) 및 상기 최 적 가중치 및 최적 정책에 따른 제어 신호를 상기 복수의 액추에이터에 전달하여, 상기 액추에이터 시스템을 작 동시키는 제어 단계(S400)를 포함하는 것이 바람직하다. 더 나아가, 상기 액추에이터 시스템은 다관절 로봇 시스템인 것이 바람직하다. 더 나아가, 상기 상태는 상기 다관절 로봇 시스템에 포함되어 작업을 수행하는 엔드 이펙터의 기준점의 3차원 위치 좌표(x, y, z)이며, 상기 행동은 상기 다관절 로봇 시스템에 포함되는 각각의 관절을 회전시키기 위한 각 모터의 회전 각도인 것이 바람직하다. 더 나아가, 상기 저층위 정책 생성 단계(S200)는 메모리로부터 상기 전처리 단계(S100)에 의한 복수의 단위 작 업 각각에 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성하는 것이 바람직하다. 더 나아가, 상기 저층위 정책 생성 단계(S200)는 상기 전처리 단계(S100)에 의한 복수의 단위 작업 중 선택되는 일부는 해당하는 각각의 단위 작업에 의한 전체 보상의 합이 최대인 행동에 해당하는 최적 정책을 도출하고, 선 택되는 또다른 일부는 메모리로부터 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성하는 것이 바람직하다. 더 나아가, 상기 제어 단계(S400)는 상기 최적 가중치 및 최적 정책에 따라, 모듈화한 단위 작업이 순차적으로 또는, 동시에 수행될 수 있도록 상기 제어 신호를 생성하는 것이 바람직하다. 본 발명의 또 다른 일 실시예에 따른 복수의 액추에이터를 포함하는 액추에이터 시스템에 있어서, 복수의 액추 에이터를 구동시키기 위해 기저장되는 복수의 단위 작업 각각에 대하여, 강화학습을 구성하는 파라미터를 설정 하고, 상태 및 행동을 설정하며, 기설정된 보상 함수를 기준으로 보상을 수행하는 전처리 동작부, 정책 평가를 통해, 상기 전처리 동작부에 의한 복수의 단위 작업 각각에 의한 전체 보상의 합이 최대인 행동에 해 당하는 최적 정책을 도출하여, 각 단위 정책을 생성하는 저층위 정책 생성부, 복수의 단위 작업을 연계 작 업들끼리 모듈화하는 모듈화부, 상기 모듈화부에 의한 모듈화한 단위 작업에 대하여 가중치를 설정하 고, 기설정된 보상 함수를 기준으로 행동에 대한 보상을 수행하는 후처리 동작부, 상기 후처리 동작부 에 의한 모듈화한 단위 작업 각각에 의한 보상의 합이 최대화가 되도록 정책을 평가하여, 최적 가중치를 산출하는 고층위 정책 생성부 및 최적 가중치 및 최적 정책에 따른 제어 신호를 상기 복수의 액추에이터에 전달하여, 구동을 제어하는 제어부를 포함하는 것이 바람직하다. 더 나아가, 상기 계층적 강화학습 기법을 이용한 제어가 적용된 복수의 액추에이터를 포함하는 액추에이터 시스 템은 다관절 로봇 시스템이며, 상기 상태는 상기 다관절 로봇 시스템에 포함되어 작업을 수행하는 엔드 이펙터 의 기준점의 3차원 위치 좌표(x, y, z)이며, 상기 행동은 상기 다관절 로봇 시스템에 포함되는 각각의 관절을 회전시키기 위한 각 모터의 회전 각도인 것이 바람직하다. 더 나아가, 상기 저층위 정책 생성부는 메모리로부터 상기 전처리 동작부에 의한 복수의 단위 작업 각각에 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성하는 것이 바람직하다. 더 나아가, 상기 저층위 정책 생성부는 상기 전처리 동작부에 의한 복수의 단위 작업 중 선택되는 일 부는 해당하는 각각의 단위 작업에 의한 전체 보상의 합이 최대인 행동에 해당하는 최적 정책을 도출하고, 선택 되는 또다른 일부는 메모리로부터 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성하는 것이 바람직하다. 더 나아가, 상기 제어부는 상기 최적 가중치 및 최적 정책에 따라, 모듈화한 단위 작업이 순차적으로 또는, 동시에 수행될 수 있도록 상기 제어 신호를 생성하는 것이 바람직하다."}
{"patent_id": "10-2022-0012324", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기와 같은 구성에 의한 본 발명의 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법 및 이에 의한 액추에이터 시스템은 강화학습 기법을 처음부터 적용할 필요 없이, 기존 에 학습된 정책들을 사용할 수 있으며, 저층위 정책에 부여한 가중치 및 보상 함수를 통해 정책의 의도, 의사 및 목적을 사용자에게 제시할 수 있어, 추후 인공지능 로봇의 안전한 상용화를 가능하게 하는 장점이 있다."}
{"patent_id": "10-2022-0012324", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 첨부한 도면들을 참조하여 본 발명의 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강 화학습 기법을 이용한 제어 방법 및 이에 의한 액추에이터 시스템을 상세히 설명한다. 다음에 소개되는 도면들 은 당업자에게 본 발명의 사상이 충분히 전달될 수 있도록 하기 위해 예로서 제공되는 것이다. 따라서, 본 발명 은 이하 제시되는 도면들에 한정되지 않고 다른 형태로 구체화될 수도 있다. 또한, 명세서 전반에 걸쳐서 동일 한 참조번호들은 동일한 구성요소들을 나타낸다. 이때, 사용되는 기술 용어 및 과학 용어에 있어서 다른 정의가 없다면, 이 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 통상적으로 이해하고 있는 의미를 가지며, 하기의 설명 및 첨부 도면에서 본 발명의 요지를 불필요하게 흐릴 수 있는 공지 기능 및 구성에 대한 설명은 생략한다.더불어, 시스템은 필요한 기능을 수행하기 위하여 조직화되고 규칙적으로 상호 작용하는 장치, 기구 및 수단 등 을 포함하는 구성 요소들의 집합을 의미한다. 본 발명의 일 실시예에 따른 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법 및 이에 의한 액추에이터 시스템은, 복잡한 작업을 해결하기 위한 강화학습 기법을 사용하여 최적 정책을 도출해 내는 상황에서, 해당 작업을 더 쉬운 단위 작업들로 소분화하여 각 단위 작업들에 대한 최 적 정책(저층위 정책)들을 각각 생성한 후, 각 저층위 정책들에 대해서 적절한 가중치를 부여하여 조합할 수 있 는 고층위 정책(meta-policy)을 학습시킴으로써, 최초 입력된 복잡한 작업을 해결할 수 있는 알고리즘에 관한 것이다. 이 때, 저층위 정책의 설정(생성)에 대한 제약 조건이 없고, 학습한 최적 정책 역시 또다른 작업의 저층위 정책 으로 적용될 수 있으므로, 기존에 학습시켰던 정책들을 DB화하여, 추후 새로운 작업을 학습시킬 때 읽어오는 형 식으로 운용할 수도 있기 때문에, 더욱 효율적인 학습이 가능한 장점이 있다. 이렇게 성공적으로 학습된 고층위 정책은 매번 저층위 정책에 합당한 가중치를 부여함으로써, 작업을 성공적으 로 해결할 수 있을 뿐 아니라, 사용자의 관점에서 저층위 정책에 부여된 가중치를 보고 정책의 의도를 알 수 있 어, 이루어진 학습에 대한 설명이 가능한 장점이 있다. 본 발명의 일 실시예에 따른 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법은 사전에 훈련된 소분화된 작업(단위 작업)이 전체 정책을 구성하기 위해 계층 구조로 조합되 는 계층적 강화학습 기법을 이용하게 된다. 상기 계층적 강화학습 기법은 복잡한 작업을 해결하기 위해 강화학습을 사용하여 최적 정책을 도출해내는 상황 에서, 해당 작업을 더 쉬운 작업들로 소분화하여 이들에 대한 최적 정책(저층위 정책)들을 도출한 후, 각각의 저층위 정책에 대해 적절한 가중치를 부여하여 조합할 수 있는 고층위 정책(meta-policy)을 학습시킴으로써, 원 래의 복잡한 작업을 해결할 수 있는 기법으로서, 본 발명의 일 실시예에 따른 복수의 액추에이터를 포함하는 액 추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법은 저층위 정책의 선정에 대한 제약조건 없으 며, 학습한 고층위 정책 역시도 더 복잡한 작업의 저층위 정책으로 적용될 수 있기 때문에, 기존에 학습시켰던 정책들을 DB화하여 추후 새로운 작업을 학습시킬 때 불러오는 형식으로 운용할 수 있기 때문에, 더욱 더 효율적 인 학습 결과를 제공할 수 있는 장점이 있다. 성공적으로 학습된 고층위 정책은 매번 저층위 정책에 합당한 가중치를 부여함으로써, 작업을 성공적으로 해결 할 수 있을 뿐 아니라, 사용자의 관점에서 저층위 정책에 부여된 가중치를 전달받아 본래 정책의 의도를 파악할 수 있는 장점이 있다. 더불어, 강화학습을 초기 단계부터 적용할 필요 없이, 기존에 학습해 두었던 정책들을 활용할 수 있기 때문에, 저층위 정책에 부여한 가중치를 통해, 정책의 의도, 의사 및 목적을 사용자에게 제시할 수 있어, 추후 인공지능 로봇의 안전한 상용화를 가능하게 하는 장점이 있다. 도 1은 본 발명의 일 실시예에 따른 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법을 나타낸 순서 예시도이다. 도 1을 참조로 하여 본 발명의 일 실시예에 따른 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법을 상세히 설명한다. 본 발명의 일 실시예에 따른 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법은 컴퓨터를 포함하는 연산 처리 수단에 의해 각 단계가 수행되는 복수의 액추에이터를 포함하 는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법으로서, 도 1에 도시된 바와 같이, 전처 리 단계(S100), 저층위 정책 생성 단계(S200), 고층위 정책 생성 단계(S300) 및 제어 단계(S400)를 포함하여 구 성되는 것이 바람직하다. 각 단계에 대해서 자세히 알아보자면, 상기 전처리 단계(S100)는 연산 처리 수단에서, 상기 액추에이터 시스템의 미리 저장되는 복수의 단위 작업 각 각에 대하여, 강화학습을 구성하는 파라미터(parameter)를 설정하고, 상태(state) 및 행동(action)을 설정하며, 미리 설정된 보상 함수를 기준으로 환경과의 지속적인 상호 작용을 통해 보상(reward)을 수행하는 것이 바람직 하다.여기서, 상기 액추에이터 시스템은 다관절 로봇 시스템으로 예를 들고 있으나, 이는 실시예에 불과하다. 또한, 상기 상태는 상기 다관절 로봇 시스템에 포함되어 작업을 수행하는 엔드 이펙터의 기준점의 3차원 위치 좌표(x, y, z)를 의미하며, 상기 행동은 상기 다관절 로봇 시스템에 포함되는 각각의 관절을 회전시키기 위한 각 모터의 회전 각도인 것이 바람직하다. 상기 미리 저장되는 복수의 단위 작업이란, 도 2에 도시된 바와 같이, Pick-and-Place 작업은 복수의 단위 작업 (picking/placing)으로 소분화되어 각각의 단위 작업으로 저장되게 된다. 또한, 각각의 picking/placing 작업 역시도, 각각 reaching/grasping과 reaching/releasing으로 소분화되어 각각의 단위 작업으로 저장되게 된다. 도 3은 미리 설정되어 저장되는 작업의 정책과 소분화 작업의 대응 예시도이다. 상기 저층위 정책 생성 단계 (S200)는 정책 평가(policy evaluation)를 통해, 상기 전처리 단계(S100)에 의한 복수의 단위 작업 각각에 의한 전체 보상의 합이 최대인 행동에 해당하는 최적 정책(optical policy)을 도출하여, 각 단위 정책을 생성하게 된 다. 이 때, 상기 저층위 정책 생성 단계(S200)는 모든 단위 작업에 대해서 각 단위 정책을 생성하는 것이 아니라, 메모리로부터 상기 전처리 단계(S100)에 의한 복수의 단위 작업 각각에 해당하는 최적 정책을 읽어들여, 각 단 위 정책을 생성할 수도 있다. 이에 따라, 상기 저층위 정책 생성 단계(S200)는 상기 전처리 단계(S100)에 의한 복수의 단위 작업 중 선택되는 일부는 해당하는 각각의 단위 작업에 의한 전체 오차가 최소인 행동에 해당하는 최적 정책을 도출하고, 선택되 는 또다른 일부는 메모리로부터 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성하는 것이 바람직하다. 여기서, 메모리로부터 읽어들이는 최적 정책은 미리 생성된 고층위 정책인 것이 바람직하다. 상기 고층위 정책 생성 단계(S300)는 복수의 단위 작업을 연계 작업들끼리 모듈화하고, 모듈화한 단위 작업에 대하여 가중치를 설정하고, 미리 설정된 보상 함수를 기준으로 상기 액추에이터 시스템의 행동에 대한 보상을 수행하게 된다. 이 때, 상기 고층위 정책 생성 단계(S300)는 상기 보상이 최대화가 되도록 하는 최적 가중치를 산출하는 것이 바람직하다. 상기 제어 단계(S400)는 상기 최적 가중치 및 최적 정책에 따른 제어 신호를 생성하고, 생성한 상기 제어 신호 를 상기 복수의 액추에이터에 전달하여 상기 액추에이터 시스템을 작동시키는 것이 바람직하다. 이 때, 상기 제어 단계(S400)는 상기 최적 가중치 및 최적 정책에 따라, 모듈화한 단위 작업이 순차적으로 또는, 동시에 수행될 수 있도록 상기 제어 신호를 생성하는 것이 바람직하다. 상기 제어 단계(S400)를 통해서 상기 최적 가중치 및 최적 정책을 도출하는 목적 함수 하기의 수학식 1과 같이 해석할 수 있다. 수학식 1"}
{"patent_id": "10-2022-0012324", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "(여기서, Π는 고층위 정책이며, J(Π)는 고층위 정책에 대한 목적 함수를 의미하며, st와 at는 각 시간 t에서의 상태 및 제어 신호를 의미하며, ρΠ는 고층위 정책 Π를 사용하였을 때 겪을 수 있는 상태 및 제어 신호의 분포를 의미하고, r(st, at)는 시간 t에서 수행한 상태와 제어 신호 st, at에 대해 부여받는 보상 함수(reward function)이며, α는 미래의 보상에 대한 감소 요인(discount factor)이며, P는 임의의 시간에서의 상태에서 어떠한 제어 신호를 받았을 때, 다음 상태에 대한 전환 확률(transition probability)이며, ω는 고층위 정책이 생성하는 가중치이고, H는 그 가중치 분포에 대한 연속 엔트로피를 의미함.) 상기 목적 함수는 액추에이터 시스템이 매 시간마다 행동을 취하고 얻는 보상과 정책의 불확실성의 합의 기댓값 을 나타내는 것으로, 최적 정책의 목표는 상기 목적 함수의 최대화에 있다. 이러한 점을 감안하여, 정책의 불확실성을 더해주어, 최대 엔트로피 강화학습에서 통상적으로 사용하는 목적 함 수를 기술하였으나, 이는 본 발명의 일 실시예에 불과하며, 목적 함수의 형태에 무관하게 보상 합의 기댓값을 최대화하는 정책의 도출이 가능한 함수는 모두 사용 가능하다. 또한, 상기 제어 신호는 하기의 수학식 2와 같이 해석할 수 있다. 수학식 2"}
{"patent_id": "10-2022-0012324", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "(여기서, 제어 신호는 a로 표기하며, aj는 벡터 a의 j번째 차원을 의미함.) 상기의 수학식 2에서 알 수 있듯이, 제어 신호(aj)는 좌항의 분포에서 표집되는데, 이 분포는 먼저, 상기 저층 위 정책 생성 단계(S200)에 의해 생성된 정책들의 제어 신호에 대한 분포를 정규 분포로 가정할 때, 이들 분포 를 상기 고층위 정책 생성 단계(S300)에 의해 산출한 가중치로 지수승하고, 모두 곱한 후 정규화한 것으로 계산 한다. 세부적으로, 인덱스 i는 i번째 저층위 정책을 의미하며, μi,j는 i번째 저층위 정책의 제어 신호 분포에서 j번째 차원의 평균값을 의미한다. 이 때, 저층위 정책이 제어 신호를 내어놓기 위해서는, 각각의 상기 상태(si)를 받아야 한다. 또한, Cj는 제어 신호의 j번째 차원에 관여하는 저층위 정책들의 집합이며, Σωc는 이들 집합에 소속하는 저증 취 정책들에 부여된 가중치의 총 합을 의미한다. 더불어, σi, j는 i번째 저층부 정책의 제어 신호 분포에서 j번째 차원의 표준편차를 의미한다. 도 4는 본 발명의 일 실시예에 따른 계층적 강화학습 기법을 이용한 제어 방법에 의한 복수의 액추에이터를 포 함하는 액추에이터 시스템을 나타낸 구성 예시도이다. 도 4를 참조로 하여 본 발명의 일 실시예에 따른 계층적 강화학습 기법을 이용한 제어 방법에 의한 복수의 액추에이터를 포함하는 액추에이터 시스템을 상세히 설명한다. 본 발명의 일 실시예에 따른 계층적 강화학습 기법을 이용한 제어 방법에 의한 복수의 액추에이터를 포함하는 액추에이터 시스템은 도 4에 도시된 바와 같이, 전처리 동작부, 저층위 생성부, 모듈화부, 후처 리 동작부, 고층위 정책 생성부 및 제어부를 포함하여 구성되는 것이 바람직하다. 각 구성에 대해서 자세히 알아보자면, 상기 전처리 동작부는 복수의 액추에이터를 구동시키기 위해 미리 저장되는 복수의 단위 작업 각각에 대하 여, 강화학습을 구성하는 파라미터(parameter)를 설정하고, 상태(state) 및 행동(action)을 설정하며, 미리 설정된 보상 함수를 기준으로 오환경과의 지속적인 상호 작용을 통해 보상(reward)을 수행하는 것이 바람직하다. 여기서, 상기 액추에이터 시스템은 다관절 로봇 시스템으로 예를 들고 있으나, 이는 실시예에 불과하다. 또한, 상기 상태는 상기 다관절 로봇 시스템에 포함되어 작업을 수행하는 엔드 이펙터의 기준점의 3차원 위치 좌표(x, y, z)를 의미하며, 상기 행동은 상기 다관절 로봇 시스템에 포함되는 각각의 관절을 회전시키기 위한 각 모터의 회전 각도인 것이 바람직하다. 상기 미리 저장되는 복수의 단위 작업이란, 도 2에 도시된 바와 같이, Pick-and-Place 작업은 복수의 단위 작업 (picking/placing)으로 소분화되어 각각의 단위 작업으로 저장되게 된다. 또한, 각각의 picking/placing 작업 역시도, 각각 reaching/grasping과 reaching/releasing으로 소분화되어 각각의 단위 작업으로 저장되게 된다. 상기 저층위 정책 생성부는 정책 평가(policy evaluation)를 통해, 상기 전처리 동작부에 의한 복수 의 단위 작업 각각에 의한 전체 보상이 최대인 행동에 해당하는 최적 정책(optical policy)을 도출하여, 각 단 위 정책을 생성하게 된다. 이 때, 상기 저층위 정책 생성부는 모든 단위 작업에 대해서 각 단위 정책을 생성하는 것이 아니라, 메모 리로부터 복수의 단위 작업 각각에 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성할 수도 있다. 이에 따라, 상기 저층위 정책 생성부는 상기 전처리 동작부에 의한 복수의 단위 작업 중 선택되는 일 부는 해당하는 각각의 단위 작업에 의한 전체 보상의 합이 최대인 행동에 해당하는 최적 정책을 도출하고, 선택 되는 또다른 일부는 메모리로부터 해당하는 최적 정책을 읽어들여, 각 단위 정책을 생성하는 것이 바람직하다. 여기서, 메모리로부터 읽어들이는 최적 정책은 미리 생성된 고층위 정책인 것이 바람직하다. 상기 모듈화부는 복수의 단위 작업을 연계 작업들끼리 모듈화하는 것이 바람직하며, 상기 후처리 동작부 는 상기 모듈화부에 의한 모듈화한 단위 작업에 대하여 가중치를 설정하고, 미리 설정된 보상 함수를 기준으로 상기 액추에이터 시스템의 행동에 대해 보상을 수행하게 된다. 상기 고층위 정책 생성부는 정책 평가를 통해, 상기 후처리 동작부에 의한 모듈화한 단위 작업 각각 에 의한 보상의 합이 최대화가 되도록 최적 가중치를 산출하는 것이 바람직하다. 이를 통해서, 상기 제어부 는 상기 최적 가중치 및 최적 정책에 따른 제어 신호를 생성하고, 생성한 상기 제어 신호를 상기 복수의 액추에이터에 전달하여 상기 액추에이터 시스템을 작동시키는 것이 바람직하다. 상기 제어부는 상기 최적 가중치 및 최적 정책에 따라, 모듈화한 단위 작업이 순차적으로 또는, 동시에 수 행될 수 있도록 상기 제어 신호를 생성하는 것이 바람직하다. 이러한 제어 신호는 하기의 수학식 2와 같이 해석 할 수 있다. 더불어, 상기 제어부를 통해서 상기 최적 가중치 및 최적 정책을 도출하는 목적 함수는 상기 의 수학식 1과 같이 해석할 수 있다. 상기 목적 함수는 액추에이터 시스템이 매 시간마다 행동을 취하고 얻는 보상과 정책의 불확실성의 합의 기댓값 을 나타내는 것으로, 최적 정책의 목표는 상기 목적 함수의 최대화에 있다. 이러한 점을 감안하여, 정책의 불확실성을 더해주어, 최대 엔트로피 강화학습에서 통상적으로 사용하는 목적 함 수를 기술하였으나, 이는 본 발명의 일 실시예에 불과하며, 목적 함수의 형태에 무관하게 보상 합의 기댓값을 최대화하는 정책의 도출이 가능한 함수는 모두 사용 가능하다. 이상과 같이 본 발명에서는 구체적인 구성 소자 등과 같은 특정 사항들과 한정된 실시예 도면에 의해 설명되었 으나 이는 본 발명의 보다 전반적인 이해를 돕기 위해서 제공된 것 일 뿐, 본 발명은 상기의 일 실시예에 한정 되는 것이 아니며, 본 발명이 속하는 분야에서 통상의 지식을 가진 자라면 이러한 기재로부터 다양한 수정 및 변형이 가능하다. 따라서, 본 발명의 사상은 설명된 실시예에 국한되어 정해져서는 아니 되며, 후술하는 특허 청구 범위뿐 아니라 이 특허 청구 범위와 균등하거나 등가적 변형이 있는 모든 것들은 본 발명 사상의 범주에 속한다고 할 것이다."}
{"patent_id": "10-2022-0012324", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법을 나타낸 순서 예시도이다. 도 2는 본 발명의 일 실시예에 따른 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법 및 이에 의한 액추에이터 시스템에서, 미리 설정되어 저장되는 작업(예를 들자면, pick-and place 작업)을 단위 작업으로 소분화한 개념 예시도이다. 도 3은 본 발명의 일 실시예에 따른 복수의 액추에이터를 포함하는 액추에이터 시스템에 대한 계층적 강화학습 기법을 이용한 제어 방법 및 이에 의한 액추에이터 시스템에서, 미리 설정되어 저장되는 작업의 정책과 소분화 작업의 대응 예시도이다. 도 4는 본 발명의 일 실시예에 따른 계층적 강화학습 기법을 이용한 제어 방법에 의한 복수의 액추에이터를 포 함하는 액추에이터 시스템을 나타낸 구성 예시도이다."}
