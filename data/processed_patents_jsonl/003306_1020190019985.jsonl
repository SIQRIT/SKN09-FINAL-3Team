{"patent_id": "10-2019-0019985", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0101735", "출원번호": "10-2019-0019985", "발명의 명칭": "임베딩 기반의 인과 관계 탐지 시스템과 방법 및 이를 실행하기 위한 프로그램이 기록된 기록", "출원인": "성균관대학교산학협력단", "발명자": "정윤경"}}
{"patent_id": "10-2019-0019985", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "입력 텍스트를 각 문장 단위로 구분하고, 상기 각 문장에 대한 프리디컷(predicate)을 추출하여, 상기 각 문장에 대해 추출된 프리디컷을 대표 표현으로 정규화하는 자연어 처리부;상기 각 문장에 대한 상기 대표 표현과 상기 대표 표현 각각에 대응되는 아이디(id)를 포함하는 룩업 테이블을생성하고, 상기 룩업 테이블 상의 각 아이디에 대해 각 아이디와 인접한 대표 표현들과의 관계를 정의하여 페어(pair)로 표현한 후, 상기 각 문장에 대한 상기 페어를 상기 각 문장에 대한 임베딩 벡터(embedding vector)로표현하는 임베딩 벡터 생성부;상기 각 문장에 대한 임베딩 벡터 값을 이용하여, 상기 각 문장 사이의 거리값을 산출하는 인과관계 추출부;를포함하는 임베딩 기반의 인과 관계 탐지 시스템."}
{"patent_id": "10-2019-0019985", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 자연어 처리부는 상기 프리디컷을 추출하기 위해, 각 문장을 토크나이즈(tokenize)하여 각 문장의 품사를분석한 후, 주어부 및 동사구를 필터링하고, 필터링된 주어부 및 동사구에 대한 프리딧컷을 추출하는 임베딩 기반의 인과 관계 탐지 시스템."}
{"patent_id": "10-2019-0019985", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,상기 임베딩 벡터 생성부는 상기 각 문장에 대한 상기 페어를 이용하여 워드 투 벡터(word2vec)의 스킵-그램(skip-gram) 방식으로 신경망을훈련을 수행하는 임베딩 기반의 인과 관계 탐지 시스템."}
{"patent_id": "10-2019-0019985", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 임베딩 벡터 생성부는 상기 각 문장에 대한 상기 페어에서 첫 번째 값을 입력으로, 두 번째를 출력으로 하여, 원 핫 인코딩(one-hotencoding) 방식을 이용한 신경망 훈련을 수행하여, 상기 임베딩 벡터로 표현하는 임베딩 기반의 인과 관계 탐지시스템."}
{"patent_id": "10-2019-0019985", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "입력 텍스트를 각 문장 단위로 구분하고, 상기 각 문장에 대한 프리디컷(predicate)을 추출하는 프리디컷 추출단계;상기 각 문장에 대해 추출된 프리디컷을 대표 표현으로 정규화하는 정규화 단계;상기 각 문장에 대한 상기 대표 표현과 상기 대표 표현 각각에 대응되는 아이디(id)를 포함하는 룩업 테이블을생성하는 룩업 테이블 생성 단계;상기 룩업 테이블 상의 각 아이디에 대해 해당 아이디에 인접한 대표 표현들과의 관계를 정의하여 페어(pair)으로 표현하는 페어 표현 단계;상기 각 문장에 대한 상기 페어를 상기 각 문장에 대한 임베딩 벡터(embedding vector)로 표현하는 벡터화단계; 및공개특허 10-2020-0101735-3-상기 각 문장에 대한 임베딩 벡터 값을 이용하여, 상기 각 문장 사이의 거리값을 산출하는 거리값 산출 단계;를포함하는 임베딩 기반의 인과 관계 탐지 방법."}
{"patent_id": "10-2019-0019985", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서,상기 프리디컷 추출 단계 이전에, 상기 프리디컷을 추출하기 위해, 각 문장을 토크나이즈(tokenize)하여 각 문장의 품사를 분석한 후, 주어부 및 동사구를 필터링하는 임베딩 기반의 인과 관계 탐지 방법."}
{"patent_id": "10-2019-0019985", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5 항에 있어서,상기 벡터화 단계는 상기 각 문장에 대한 상기 페어를 이용하여 워드 투 벡터(word2vec)의 스킵-그램(skip-gram) 방식으로 신경망을훈련을 수행하는 임베딩 기반의 인과 관계 탐지 방법."}
{"patent_id": "10-2019-0019985", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제5 항에 있어서,상기 벡터화 단계는 상기 각 문장에 대한 상기 페어에서 첫 번째 값을 입력으로, 두 번째를 출력으로 하여, 원 핫 인코딩(one-hotencoding) 방식을 이용한 신경망 훈련을 수행하여, 상기 임베딩 벡터로 표현하는 임베딩 기반의 인과 관계 탐지방법."}
{"patent_id": "10-2019-0019985", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제5 항 내지 제8항 중 어느 하나에 기재된 임베딩 기반의 인과 관계 탐지 방법을 실행하기 위한 프로그램이 기록된 기록매체."}
{"patent_id": "10-2019-0019985", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 임베딩 기반의 인과 관계 탐지 시스템과 방법 및 이를 실행하기 위한 프로그램이 기록된 기록매체에 관한 것이다. 본 발명의 일례에 따른 임베딩 기반의 인과 관계 탐지 시스템은 입력 텍스트를 각 문장 단위로 구분하고, 각 문 (뒷면에 계속)"}
{"patent_id": "10-2019-0019985", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 임베딩 기반의 인과 관계 탐지 시스템과 방법 및 이를 실행하기 위한 프로그램이 기록된 기록매체에 관한 것이다."}
{"patent_id": "10-2019-0019985", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "텍스트에서 인과 관계를 추출하는 것은 자연어 처리 문제 중에서도 어려운 문제로서 추출 성능이 높지 않다. 관련 선행 기술 문헌으로 기재된 특허 문헌에서는 cause, result from, thus, process 등의 단어에 기반한 분석 방법을 제안하고 있는데, 특정 단어 리스트를 정하는 도메인 지식에 의존하는 단점이 있다. 선행 기술 문헌으로 기재된 비특허 문헌의 및 는 CNN(Convolutional Neural Network) 딥러닝 모델을 사 용하였고, 비특허 문헌의 은 LSTM(Long Short-Term Memory) 모델에 자연어의 최소 의존 패스를 활용하여 관 계를 분류하였다. 이들 방법은 지도 기반으로서, 관계의 종류가 이미 태깅(tagging)되어 있어야 하므로 태깅에 소요되는 노력이나 시간이 필요한 문제점이 있었다. 선행기술문헌 특허문헌 (특허문헌 0001) 미국 등록번호-US9009590B2 비특허문헌(비특허문헌 0001) Relation Classification via Convolutional Deep Neural Network. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou and Jun Zhao. Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335?2344, Dublin, Ireland, August 23-29 2014. (비특허문헌 0002) Classifying Relations by Ranking with Convolutional Neural Networks. Cicero Nogueira dos Santos, Bing Xiang, Bowen Zhou. (비특허문헌 0003) Classifying Relations via Long Short Term Memory Networksalong Shortest Dependency PathsYan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, Zhi Jin."}
{"patent_id": "10-2019-0019985", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 어떤 텍스트 데이터가 주어졌을 때, 문장간의 관계를 특정 규칙을 사용하지 않고 데이터 패턴에서 추 출할 수 있는 임베딩 기반의 인과 관계 탐지 시스템과 방법 및 이를 실행하기 위한 프로그램이 기록된 기록매체 를 제공하는데 그 목적이 있다."}
{"patent_id": "10-2019-0019985", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일례에 따른 임베딩 기반의 인과 관계 탐지 시스템은 입력 텍스트를 각 문장 단위로 구분하고, 각 문 장에 대한 프리디컷(predicate)을 추출하여, 각 문장에 대해 추출된 프리디컷을 대표 표현으로 정규화하는 자연 어 처리부; 각 문장에 대한 대표 표현과 대표 표현 각각에 대응되는 아이디(id)를 포함하는 룩업 테이블을 생성 하고, 룩업 테이블 상의 각 아이디에 대해 각 아이디와 인접한 대표 표현들과의 관계를 정의하여 페어(pair)로 표현한 후, 각 문장에 대한 페어를 각 문장에 대한 임베딩 벡터(embedding vector)로 표현하는 임베딩 벡터 생 성부; 각 문장에 대한 임베딩 벡터 값을 이용하여, 각 문장 사이의 거리값을 산출하는 인과관계 추출부;를 포함 한다. 여기서, 자연어 처리부는 프리디컷을 추출하기 위해, 각 문장을 토크나이즈(tokenize)하여 각 문장의 품사를 분 석한 후, 주어부 및 동사구를 필터링하고, 필터링된 주어부 및 동사구에 대한 프리딧컷을 추출할 수 있다. 임베딩 벡터 생성부는 각 문장에 대한 페어를 이용하여 워드 투 벡터(word2vec)의 스킵-그램(skip-gram) 방식으 로 신경망을 훈련을 수행할 수 있다. 임베딩 벡터 생성부는 각 문장에 대한 페어에서 첫 번째 값을 입력으로, 두 번째를 출력으로 하여, 원 핫 인코 딩(one-hot encoding) 방식을 이용한 신경망 훈련을 수행하여, 임베딩 벡터로 표현할 수 있다. 본 발명의 일례에 따른 임베딩 기반의 인과 관계 탐지 방법은 입력 텍스트를 각 문장 단위로 구분하고, 각 문장 에 대한 프리디컷(predicate)을 추출하는 프리디컷 추출 단계; 각 문장에 대해 추출된 프리디컷을 대표 표현으 로 정규화하는 정규화 단계; 각 문장에 대한 대표 표현과 대표 표현 각각에 대응되는 아이디(id)를 포함하는 룩 업 테이블을 생성하는 룩업 테이블 생성 단계; 룩업 테이블 상의 각 아이디에 대해 해당 아이디에 인접한 대표 표현들과의 관계를 정의하여 페어(pair)으로 표현하는 페어 표현 단계; 각 문장에 대한 페어를 각 문장에 대한 임베딩 벡터(embedding vector)로 표현하는 벡터화 단계; 및 각 문장에 대한 임베딩 벡터 값을 이용하여, 각 문 장 사이의 거리값을 산출하는 거리값 산출 단계;를 포함한다. 프리디컷 추출 단계 이전에, 프리디컷을 추출하기 위해, 각 문장을 토크나이즈(tokenize)하여 각 문장의 품사를 분석한 후, 주어부 및 동사구를 필터링할 수 있다. 벡터화 단계는 각 문장에 대한 페어를 이용하여 워드 투 벡터(word2vec)의 스킵-그램(skip-gram) 방식으로 신경 망을 훈련을 수행할 수 있다. 일례로, 벡터화 단계는 각 문장에 대한 페어에서 첫 번째 값을 입력으로, 두 번째를 출력으로 하여, 원 핫 인코 딩(one-hot encoding) 방식을 이용한 신경망 훈련을 수행하여, 임베딩 벡터로 표현할 수 있다. 본 발명은 이와 같은 임베딩 기반의 인과 관계 탐지 방법을 실행하기 위한 프로그램이 기록된 기록매체를 포함 한다."}
{"patent_id": "10-2019-0019985", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 도메인 지식이나 태깅 정보가 필요 없이, 비지도 방식으로 인과 관계를 추출하는 방법을 제안한다는 점에서 기존 방법과의 차별화가 되며, 태깅 등의 사람이 직접 입력해야 하는 정보가 필요 없어 시간이 절감되는 효과가 있다. 보다 구체적으로, 본 발명은 텍스트 데이터가 주어졌을 때, 문장간의 관계를 특정 규칙을 사용하지 않고 데이터 패턴에서 추출할 수 있다."}
{"patent_id": "10-2019-0019985", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "문장이나 이벤트에서 인과 관계는 자연어를 활용한 고차원적인 임무(대화 시스템, Q&A, 문서 요약 등)를 수행할 때 중요하게 활용되는 핵심 정보이며, 인과 관계는 특히 기호 기반 인공지능 기술에서 도메인 지식을 코딩할 때 에 많이 사용되는데, 자동으로 추출하기 어렵다 보니 사람이 매뉴얼로 코딩하는 데에 많은 시간과 노력이 소요 되었지만, 본 발명은 이러한 인과 관계를 자동으로 추출하여 시간 및 비용을 절감할 수 있다."}
{"patent_id": "10-2019-0019985", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위 해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 그러면 첨부한 도면을 참고로 하여 본 발명에 대하여 설명한다. 도 1은 본 발명의 일례에 따른 임베딩 기반의 인과 관계 탐지 시스템의 개념을 설명하기 위한 구성도이고, 도 2 는 도 1에 따른 본 발명의 시스템이 동작하는 각 단계를 플로우 차트로 설명하기 위한 도이다. 도 3은 도 1에서 자연어 처리부가 대표 표현으로 룩업 테이블을 생성한 일례를 설명하기 위한 도이고, 도 4 는 도 1에서 자연어 처리부의 동작의 일례를 설명하기 위한 도이고, 도 5는 도 1에서 자연어 처리부의 동작의 다른 일례를 설명하기 위한 도이고, 도 6은 도 1에서 임베딩 벡터 생성부의 동작을 설명하기 위한 도이다. 도 1에 도시된 바와 같이, 본 발명의 일례에 따른 임베딩 기반의 인과 관계 탐지 시스템은 자연어 처리부, 임베딩 벡터 생성부, 인과관계 추출부를 포함하고, 자연어 처리부에 입력 텍스트를 제공하는 데이 터 베이스를 더 포함할 수 있다. 데이터 베이스는 텍스트 정보를 저장할 수 있으며, 자연어 처리부로 텍스트를 출력할 수 있다. 자연어 처리부는 데이터 베이스에서 텍스트를 입력받아, 입력된 텍스트를 각 문장 단위로 구분하고, 각 문장에 포함된 단어에 대한 프리디컷(predicate)을 추출하여, 각 문장에 대해 추출된 프리디컷을 대표 표현으로 정규화할 수 있다. 임베딩 벡터 생성부는 자연어 처리부에서 대표 표현으로 정규화된 각 문장에 대한 대표 표현과 대표 표 현 각각에 대응되는 아이디(id)를 포함하는 룩업 테이블을 생성하고, 룩업 테이블 상의 각 아이디에 대해 각 아 이디와 인접한 대표 표현들과의 관계를 정의하여 페어(pair)로 표현한 후, 각 문장에 대한 페어를 각 문장에 대한 임베딩 벡터(embedding vector)로 표현하여 출력할 수 있다. 인과관계 추출부는 임베딩 텍터 생성부에서 출력된 각 문장에 대한 임베딩 벡터 값을 이용하여, 각 문장 사 이의 거리값을 산출할 수 있다. 이하에서는 도 2 내지 도 5를 참조하여, 이와 같은 임베딩 기반의 인과 관계 탐지 시스템의 동작 방법을 보다 구체적으로 설명한다. 도 2에 도시된 바와 같이, 본 발명의 일례에 따른 임베딩 기반의 인과 관계 탐지 방법은 프리디컷 추출 단계 (S1), 정규화 단계(S2), 룩업 테이블 생성 단계(S3), 페어 표현 단계(S4), 벡터화 단계(S5) 및 거리값 산출 단 계(S6)를 포함할 수 있다. 여기서, 프리디컷 추출 단계(S1)와 정규화 단계(S2)는 자연어 처리부에 의해 수행될 수 있으며, 룩업 테이 블 생성 단계(S3)와 페어 표현 단계(S4) 및 벡터화 단계(S5)는 임베딩 벡터 생성부에 의해 수행될 수 있으 며, 거리값 산출 단계(S6)는 인과 관계 추출부에 의해 수행될 수 있다. 프리디컷 추출 단계(S1)는 입력 텍스트를 각 문장 단위로 구분하고, 각 문장에 대한 프리디컷(predicate)을 추 출할 수 있다. 정규화 단계(S2)는 각 문장에 대해 추출된 프리디컷을 대표 표현으로 정규화할 수 있다. 룩업 테이블 생성 단계(S3)는 각 문장에 대한 대표 표현과 대표 표현 각각에 대응되는 아이디(id)를 포함하는 룩업 테이블을 생성할 수 있다. 페어 표현 단계(S4)는 룩업 테이블 상의 각 아이디에 대해 해당 아이디에 인접한 대표 표현들과의 관계를 정의 하여 페어(pair)으로 표현할 수 있다. 벡터화 단계(S5)는 각 문장에 대한 페어를 각 문장에 대한 임베딩 벡터(embedding vector)로 표현할 수 있다. 거리값 산출 단계(S6)는 각 문장에 대한 임베딩 벡터 값을 이용하여, 각 문장 사이의 거리값을 산출할 수 있다. 일례로, 자연어 처리부는 프리디컷 추출 단계(S1)와 정규화 단계(S2)를 수행하고, 임베딩 벡터 생성부 가 룩업 테이블 생성 단계(S3)를 수행하여, 도 3의 (a)와 같이, 데이버 베이스로부터 입력된 텍스트를 도 3의 (b)에 도시된 아이디와 대표 표현을 포함하는 룩업 테이블을 생성할 수 있다. 이와 같은 각 단계에 대해 보다 구체적으로 설명하면 다음과 같다. 자연어 처리부는 데이터 베이스로부터 텍스트가 입력되면, 입력 텍스트를 각 문장 단위로 구분하고, 각 문장에 포함된 단어에 대한 프리디컷(predicate)을 추출하는 프리디컷 추출 단계(S1)를 수행할 수 있다. 이를 위해, 일례로 도 3의 (a)에 도시된 텍스트를 자연어 처리부는 프리디컷 추출 단계(S1)를 통해, 도 4의 (a) 및 도 5의 (a)에 도시된 바와 같이, 텍스트를 P1, P2, P3 등 각 문장 단위로 구분할 수 있다. 이후, 각 문장에 포함된 단어에 대한 프리디컷(predicate)을 추출하기 위해, 각 문장을 단어 단위로 토크나이즈 (tokenize)할 수 있다. 즉, 도 4의 (a) 및 도 5의 (a)의 문장 중 일례로, P1과 P2가 다음과 같은 문장을 가질 때, “P1: Karen was assigned a roommate her first year of college.” “P2: Her roommate asked her to go to a nearby city for a concert.” P1과 P2 문장은 아래와 같이, 토크나이즈(tokenize)될 수 있다. P1: [Karen, was, assigned, a, roommate, her, first, year, of, college] P2: [Her, roommate, asked, her, to, go, to, a, nearby, city, for, a, concert] 이후, 각 문장에 대하여 품사를 분석하고, 명사구나 동사구 등에 대한 덩어리(chunk) 정보를 추출할 수 있다. 이후, 각 문장의 주용 내용인 주어부 및 동사구만 필터링할 수 있다. 일례로, 문장 P1은 다음과 같이, “P1:Karen was assigned a roommate” 으로 필터링할 수 있다. 이후, 필터링된 문장의 프리디컷(predicate)을 다음과 같이, “P1: (assign, Karem, roommate)”으로 추출할 수 있다. 이와 같은 방법으로, 도 4의 (b) 및 도 5의 (b)와 같은 각 문장에 대한 프리디컷을 추출할 수 있다. 이후, 자연어 처리부는 정규화 단계(S2)를 통해, 각 문장에 대해 추출된 프리디컷은 대표 표현으로 정규화 될 수 있다. 일례로, 단수 및 복수는 단수로, 현재 시제 및 과거 시제는 대표 형태로 정규화할 수 있다. 아울러, 개체명 인식(Named Entity Recognition, NER) 방법을 수행하여, 사람 이름, 대명사를 사람(PERSON)으 로, 장소 이름은 장소(LOCATION)로, 숫자의 명칭(일례로, 1, 2, 3 등)은 숫자(NUMBER)로 정규화할 수 있다. 이에 따라, P1 문장은 “P1: (assign, PERSON, PERSON)”으로 정규화될 수 있고, 이와 같은 방법으로, 도 4의 (c) 및 도 5의 (c)에 도시된 바와 같이, 정규화될 수 있다. 이후, 임베딩 벡터 생성부가 도 4의 (d)와 같이, 상호 인접한 대표 표현들과의 관계를 정의하여, 페어 (pair)로 표현할 수 있다. 이를 위해, 도 5의 (d)와 같이, 도 5의 (d)와 같이, 각 문장에 대한 대표 표현과 대표 표현 각각에 대응되는 아 이디(id)를 포함하는 룩업 테이블을 생성할 수 있다. 구체적으로 룩업 테이블에서는 각 문장을 아이디(id) 값으로 지정하고, 각 문장에 대한 대표 표현을 아이디(i d)에 대응하는 내용이 되도록 할 수 있다. 이와 같은 도 5의 (d)와 같은 룩업 테이블을 참조하여, 룩업 테이블 상의 각 아이디에 대해 각 아이디와 인접한 대표 표현들과의 관계를 정의하여, 도 5의 (e)와 같이, 페어(pair)로 표현할 수 있다. 일례로, 임베딩 벡터 생성부는 텍스트를 아이디(id) 리스트 (p1, p2, p3…pN)로 표현하고, 각 아이디(id)에 대하여 해당 아이디(id)와 인접한 대표 표현들(context)과와의 관계를 페어로 표현할 수 있다. 이때, 대표 표현 식에 대한 아이디(id)를 사용할 수 있다. 일례로, 윈도우 사이즈를 N이라 할 때, 임베딩 벡터 생성부는 pi에 대하여 (pi, pi-N/2), (pi, pi-1), (pi, pi-2), …, (pi, pi+1), (pi, pi+2), …, (pi, pi+N/2) 쌍을 생성하여, 도 4의 (d) 및 도 5의 (e)와 같은 페어를 표현할 수 있다. 따라서, 페어의 첫 번째 값은 아이디(id)값이 기재되고, 두 번째 값은 인접한 대표 표현들(context)과와의 관계 값이 기재될 수 있다. 이후, 임베딩 벡터 생성부는 벡터화 단계(S5)에서, 각 문장에 대한 페어를 이용하여 워드 투 벡터 (word2vec)의 스킵-그램(skip-gram) 방식으로 신경망을 훈련을 수행할 수 있다. 보다 구체적으로, 임베딩 벡터 생성부는 벡터화 단계(S5)에서, 도 6에 도시된 바와 같이, 각 문장에 대한 페어에서 첫 번째 값을 입력으로, 두 번째를 출력으로 하여, 원 핫 인코딩(one-hot encoding) 방식을 이용한 신 경망 훈련을 수행하여, 임베딩 벡터(embedding vector)로 표현할 수 있다. 일례로, 도 6에서, 원 핫 인코딩(one-hot encoding) 방식을 이용한 신경망 훈련을 수행할 때, 페어에서 각 문장 에 대한 아이디 값인 첫 번째 값은 입력으로, 인접한 대표 표현들(context)과와의 관계 값인 두 번째 값은 출력 으로 설정한 상태에서 신경망 훈련을 수행하여, 임베딩 벡터(embedding vector)값을 출력 수 있다. 도 6에서 Matrix W의 i번째 행이 ID i에 해당하는 문장을 표현하는 N 차원 벡터가 될 수 있고, 두 문장 IDi, IDj간의 관련성은 두 벡터 i, j간의 거리(distance) 수치가 적을수록 관련이 높을 수 있다. 이후, 인과 관계 추출부는 거리값 산출 단계(S6)에서 각 문장에 대한 임베딩 벡터(embedding vector) 값을 이용 하여, 각 문장 사이의 거리값을 산출할 수 있다. 따라서, 이와 같이 각 문장을 임베딩 벡터(embedding vector) 값을 이용간 각 문장 사이의 거리값을 이용하여, 각 문장의 관계성을 측정할 수 있다. 예를 들어, 다음 문장 P1, P2, P3가 아래와 같이, 임베딩 벡터(embedding vector)로 표현될 때, P1: assign, Karen, roommate => [0.2, 0.2, 0.3, 0.4, 0.5, 0.3, 0.2] P2: ask, roommate, Karen, concert => [0.3, 0., 0.1, 0., 1., 0.1, 0.5] P3: agree, Karen ==> [0.1, 0.1, 0.7, 0., 0.8, 0.1, 0.5] 각 문장간의 거리는 벡터의 각 원소의 단순 차이의 절대값의 합으로 계산하면, p1과 p2간 거리는 1.9, p2와 p3 간의 거리는 1.10이 될 수 있다. 여기서, 거리가 더 작은 p2, p3가 인과 관계가 있다고 판단할 수 있다. 이와 같은 본 발명의 시스템은 문장간의 논리적 관련성을 계산하는데 이용될 수 있다. 즉, 복수의 문장이 텍스트로 입력되면, 각 문장에 해당하는 임베딩 벡터(embedding vector)를 생성하고, 이와 같은 각 문장에 대한 임베딩 벡터(embedding vector)를 페어 와이즈(pair-wise) 방법으로 벡터간의 유사도를 계 산하여, 유사도가 높으면 관련성 있는 문장으로 판별할 수 있다. 또한, 본 발명의 시스템은 어떤 텍스트가 주어지면, 그 다음에 나올 문장을 선택하는데, 이용될 수 있다 즉, 본 발명과 같이 입력된 텍스트를 임베딩 벡터(embedding vector)로 벡터화하고, 그 다음에 나올 후보 문장 이 일반적인 다음 문장 생성 알고리즘으로 생성된 경우, 각 문장들과 후보 문장을 임베딩 벡터(embedding vector)로 벡터화하고, 각 문장과 후보 문장간의 임베딩 벡터(embedding vector)의 유사도를 판단하여, 유사도 가 높은 문장을 선택하도록 할 수 있다. 일례로, 도 4의 (a)에서 E1, E2가 후보 문장이라고 할 때, E1, E2의 임베딩 벡터(embedding vector)와 P1, P2, P3의 임베딩 벡터(embedding vector) 사이의 거리 값을 산출하여, E1, E2문장 중 P1, P2, P3 문장과의 거리 값 이 가장 작은 문장을 선택하도록 할 수도 있다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속하는 것이다."}
{"patent_id": "10-2019-0019985", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일례에 따른 임베딩 기반의 인과 관계 탐지 시스템의 개념을 설명하기 위한 구성도이다. 도 2는 도 1에 따른 본 발명의 시스템이 동작하는 각 단계를 플로우 차트로 설명하기 위한 도이다. 도 3은 도 1에서 자연어 처리부가 대표 표현으로 룩업 테이블을 생성한 일례를 설명하기 위한 도이다. 도 4는 도 1에서 자연어 처리부의 동작의 일례를 설명하기 위한 도이다. 도 5는 도 1에서 자연어 처리부의 동작의 다른 일례를 설명하기 위한 도이다. 도 6은 도 1에서 임베딩 벡터 생성부의 동작을 설명하기 위한 도이다."}
