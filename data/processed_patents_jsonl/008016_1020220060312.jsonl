{"patent_id": "10-2022-0060312", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0160604", "출원번호": "10-2022-0060312", "발명의 명칭": "이중언어 학습 서비스 제공 시스템", "출원인": "주식회사 엘스아이", "발명자": "유선호"}}
{"patent_id": "10-2022-0060312", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성발화를 입력받는 웨어러블 기기;상기 웨어러블 기기와 연동되어 상기 음성발화된 음성발화 데이터를 수신하고, 상기 음성발화 데이터를 실시간으로 번역하여 이중언어 데이터로 출력하는 사용자 단말; 및상기 사용자 단말로부터 상기 사용자 단말 및 웨어러블 기기를 등록받아 매핑되도록 저장하는 저장부, 상기 웨어러블 기기에서 음성발화가 입력된 경우 상기 사용자 단말에서 음성발화 데이터를 업로드하도록 하는 음성입력부, 상기 음성발화 데이터를 STT(Speech to Text)로 변환한 후 상기 사용자 단말에서 설정한 언어로 번역되도록번역 프로그램을 이용하는 번역부, 상기 번역 프로그램에서 번역된 번역 데이터를 TTS(Text To Speech)로 변환하는 변환부, 상기 사용자 단말에서 변환된 이중언어 데이터를 음성발화되도록 출력하는 출력부를 포함하는 학습 서비스 제공 서버;를 포함하는 이중언어 학습 서비스 제공 시스템."}
{"patent_id": "10-2022-0060312", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 사용자 단말의 사용자는 적어도 하나의 양육자를 포함하고,상기 학습 서비스 제공 서버는,상기 사용자 단말로부터 적어도 하나의 양육자의 음성을 입력받아 등록하고, 화자식별(Speaker Identification)및 화자 검증(Speaker Verification)을 통하여 등록된 화자인지의 여부를 확인한 후, 등록된 화자로 식별 및 검증된 경우에만 실시간 번역을 시작하는 화자인식부;를 더 포함하는 것을 특징으로 하는 이중언어 학습 서비스 제공 시스템."}
{"patent_id": "10-2022-0060312", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 학습 서비스 제공 서버는,상기 음성발화가 상기 사용자 단말의 사용자의 음성발화인지의 여부를 확인하기 위하여, 기 설정된 키워드의 발화여부를 체크한 후, 상기 기 설정된 키워드가 발화되는 경우에만 실시간 번역을 시작하는 키워드인식부;를 더 포함하는 것을 특징으로 하는 이중언어 학습 서비스 제공 시스템."}
{"patent_id": "10-2022-0060312", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,상기 학습 서비스 제공 서버는,상기 웨어러블 기기에 적어도 하나의 종류의 입력이 발생하는 경우에만 실시간 번역을 시작하는 타이밍인식부;를 더 포함하는 것을 특징으로 하는 이중언어 학습 서비스 제공 시스템.공개특허 10-2023-0160604-3-청구항 5 제 1 항에 있어서,상기 학습 서비스 제공 서버는,상기 번역 프로그램으로 기 번역된 번역 데이터를 상기 사용자 단말에 저장하고, 상기 사용자 단말에 입력된 음성발화 데이터 중 상기 기 번역된 번역 데이터와 동일한 데이터가 존재하는 경우, 상기 기 번역된 번역 데이터를 재사용하도록 하는 재사용부;를 더 포함하는 것을 특징으로 하는 이중언어 학습 서비스 제공 시스템."}
{"patent_id": "10-2022-0060312", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1 항에 있어서,상기 학습 서비스 제공 서버는,상기 사용자 단말로부터 상기 사용자의 음성 샘플을 등록받고, 상기 TTS의 음성을 상기 음성 샘플로 학습시킨후 상기 사용자 단말에서 TTS로 출력되는 음성을 상기 사용자의 음성으로 출력되도록 설정하는 부모음성제공부;를 더 포함하는 것을 특징으로 하는 이중언어 학습 서비스 제공 시스템."}
{"patent_id": "10-2022-0060312", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항에 있어서,상기 학습 서비스 제공 서버는,상기 웨어러블 기기로부터 음성발화가 입력되기 이전에 상기 웨어러블 기기 또는 적어도 하나의 베이비 모니터로부터 입력된 영유아 음성 데이터를 수집하고, 상기 영유아 음성 데이터와 사용자의 음성발화 데이터 간을 매핑하여 저장하는 반응수집부;를 더 포함하는 것을 특징으로 하는 이중언어 학습 서비스 제공 시스템."}
{"patent_id": "10-2022-0060312", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서,상기 학습 서비스 제공 서버는,상기 영유아 음성 데이터를 스펙트로그램(Spectrogram)으로 분석하여 특징을 분석한 후, CNN(ConvolutionalNeural Network)로 상기 영유아 음성 데이터의 특징에 따른 상기 사용자의 음성발화 데이터를 학습 및검증하며, 상기 웨어러블 기기 또는 적어도 하나의 베이비 모니터로부터 영유아 음성 데이터가 질의(Query)로입력된 경우, 상기 영유아 음성 데이터에 대응하는 사용자의 음성발화 데이터가 상기 웨어러블 기기 또는 적어도 하나의 베이비 모니터로부터 출력되도록 제어하는 부모모사부;를 더 포함하는 것을 특징으로 하는 이중언어 학습 서비스 제공 시스템."}
{"patent_id": "10-2022-0060312", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항에 있어서,상기 번역 프로그램은 적어도 하나의 종류의 실시간 번역 API(Application Programming Interface)를 포함하는것을 특징으로 하는 이중언어 학습 서비스 제공 시스템.공개특허 10-2023-0160604-4-청구항 10 제 1 항에 있어서,상기 학습 서비스 제공 서버는,상기 사용자 단말에서 번역에 이용한 데이터에 대응한 과금 데이터를 생성하여 상기 사용자 단말로 결제청구를전송하는 과금부;를 더 포함하는 것을 특징으로 하는 이중언어 학습 서비스 제공 시스템."}
{"patent_id": "10-2022-0060312", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "이중언어 학습 서비스 제공 시스템이 제공되며, 음성발화를 입력받는 웨어러블 기기, 웨어러블 기기와 연동되어 음성발화된 음성발화 데이터를 수신하고, 음성발화 데이터를 실시간으로 번역하여 이중언어 데이터로 출력하는 사용자 단말 및 사용자 단말로부터 사용자 단말 및 웨어러블 기기를 등록받아 매핑되도록 저장하는 저장부, 웨 어러블 기기에서 음성발화가 입력된 경우 사용자 단말에서 음성발화 데이터를 업로드하도록 하는 음성입력부, 음 성발화 데이터를 STT(Speech to Text)로 변환한 후 사용자 단말에서 설정한 언어로 번역되도록 번역 프로그램을 이용하는 번역부, 번역 프로그램에서 번역된 번역 데이터를 TTS(Text To Speech)로 변환하는 변환부, 사용자 단 말에서 변환된 이중언어 데이터를 음성발화되도록 출력하는 출력부를 포함하는 학습 서비스 제공 서버를 포함한 다."}
{"patent_id": "10-2022-0060312", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 이중언어 학습 서비스 제공 시스템에 관한 것으로, 이중언어를 학습할 수 있도록 영유아 시기부터 양 육자의 음성발화를 실시간으로 번역하여 제공하는 시스템을 제공한다."}
{"patent_id": "10-2022-0060312", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "아동의 언어능력은 아동 내적인 요인과 외적인 요인의 상호작용을 통해 발달한다. 아동 내적인 요인은 아동 자 신에게 내재된 언어 습득 능력을 의미하며, 외적인 요인은 언어 사용 환경 및 양육 환경, 사회경제적 수준, 문 화 등과 같은 환경적 요인을 의미한다. 외적인 요인 중 언어적 사용 환경은 특히 영유아의 언어발달에 직접적 인 영향을 줄 수 있는 요인이며, 이 시기의 아동은 가정에서 부모와 대부분의 시간을 보내면서 자연스러운 상호 작용을 통해 언어에 직접적으로 노출된다. 부모를 통해 언어를 습득하는 과정을 살펴보면, 부모는 아동이 언어 를 사용하여 표현하기 이전부터 아동의 행동이나 발성에 직접적이고 긍정적인 언어적 반응을 제공하는데 이와 같은 부모의 구어적 모델링, 그리고 아동과 대화를 주고받는 식의 상호작용은 아동의 발성 산출 행동을 강화하 는 역할을 한다. 따라서 아동은 자신의 구어적, 비구어적 표현이 부모의 반응을 이끌어낼 수 있다는 사실을 이 해하게 되고, 점차 부모의 언어를 비롯하여 자신을 둘러싼 환경에서 사용되는 언어를 습득하게 된다. 이때, 실시간으로 번역 서비스를 제공하거나 실시간 번역 기반으로 커뮤니케이션을 제공하는 방법이 연구 및 개 발되었는데, 이와 관련하여, 선행기술인 한국공개특허 제2015-0090357호(2015년08월06일 공개) 및 한국등록특허 제10-2264224호(2021년06월11일 공고)에는, 사용자 단말에서 대화방에 입장하는 경우 기 설정된 언어로 실시간 통역 서비스를 제공할 때, 음성 또는 문자 기반으로 실시간 통역을 제공하는 구성과, 원격 커뮤니케이션을 진행 할 때 커뮤니케이션하는 언어를 설정받고, 번역을 해야 하는 언어가 감지되면 이를 설정받은 언어로 각각 번역 하는 구성이 각각 개시되어 있다. 다만, 전자 및 후자 모두 상호 간 소통을 위하여 번역을 해주는 구성일 뿐, 부모인 양육자가 영유아에게 언어를 노출시키는 환경을 조성하는 구성이 아니다. 이중언어 아동이 경험하는 언어 노출 환경은 단일언어 아동의 언 어 환경과 차이가 있는데, 주 양육자를 통해 하나의 언어를 입력받는 단일언어 아동과는 달리, 동시적 이중언어 아동은 주 양육자의 언어와 주 양육자가 아닌 부 또는 모의 언어가 다르기 때문에 한 가정에서 두 개의 언어에 노출되는 상황이 빈번하다. 순차적 이중언어 아동도 일정 시기까지는 한 개의 언어(L1)에 노출되다가, 일정 시 기 이후부터 사회적 언어(L2)를 접하며 이로 인해 일상에서 지속적으로 두 개의 언어에 노출된다. 즉, 이중언 어 아동의 언어 환경은 단일언어 아동의 언어 환경과 차이가 발생하게 되는데, 모국어에 대한 사용량과 노출량 은 아동의 언어발달에 매우 중요한 외적 요인으로 고려된다. 이에, 부모의 단일언어를 이중언어로 실시간으로 번역 및 변환해주는 플랫폼의 연구 및 개발이 요구된다."}
{"patent_id": "10-2022-0060312", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 실시예는, 사용자 단말에서 음성발화된 제 1 언어를 실시간으로 번역하여 제 2 언어로 변화한 후 음성발화되도록 TTS(Text To Speech)를 이용하고, 부모의 음성발화만을 번역하도록 화자식별 알고리즘을 이용하 거나 기 설정된 키워드의 음성발화로 인식 및 식별하도록 하며, 기 사용된 단어, 구, 절 및 문장의 경우 사용자 단말에 저장하여 재사용하도록 함으로써 번역 프로그램 또는 API를 이용함에 따른 과금을 줄이고, 블루투스와연동된 넥밴드(NeckBand)를 포함하는 웨어러블 기기를 이용하여 음성발화를 감지하도록 함으로써 사용자 단말의 배터리 소모를 줄일 수 있는, 이중언어 학습 서비스 제공 시스템을 제공할 수 있다. 다만, 본 실시예가 이루고 자 하는 기술적 과제는 상기된 바와 같은 기술적 과제로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2022-0060312", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본 발명의 일 실시예는, 음성발화를 입력받는 웨어러 블 기기, 웨어러블 기기와 연동되어 음성발화된 음성발화 데이터를 수신하고, 음성발화 데이터를 실시간으로 번 역하여 이중언어 데이터로 출력하는 사용자 단말 및 사용자 단말로부터 사용자 단말 및 웨어러블 기기를 등록 받아 매핑되도록 저장하는 저장부, 웨어러블 기기에서 음성발화가 입력된 경우 사용자 단말에서 음성발화 데이 터를 업로드하도록 하는 음성입력부, 음성발화 데이터를 STT(Speech to Text)로 변환한 후 사용자 단말에서 설 정한 언어로 번역되도록 번역 프로그램을 이용하는 번역부, 번역 프로그램에서 번역된 번역 데이터를 TTS(Text To Speech)로 변환하는 변환부, 사용자 단말에서 변환된 이중언어 데이터를 음성발화되도록 출력하는 출력부를 포함하는 학습 서비스 제공 서버를 포함한다."}
{"patent_id": "10-2022-0060312", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본 발명의 과제 해결 수단 중 어느 하나에 의하면, 사용자 단말에서 음성발화된 제 1 언어를 실시간으로 번역하여 제 2 언어로 변화한 후 음성발화되도록 TTS(Text To Speech)를 이용하고, 부모의 음성발화만을 번역하 도록 화자식별 알고리즘을 이용하거나 기 설정된 키워드의 음성발화로 인식 및 식별하도록 하며, 기 사용된 단 어, 구, 절 및 문장의 경우 사용자 단말에 저장하여 재사용하도록 함으로써 번역 프로그램 또는 API를 이용함에 따른 과금을 줄이고, 블루투스와 연동된 넥밴드(NeckBand)를 포함하는 웨어러블 기기를 이용하여 음성발화를 감 지하도록 함으로써 사용자 단말의 배터리 소모를 줄일 수 있다."}
{"patent_id": "10-2022-0060312", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 발명의 실시예를 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관 계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\"되어 있는 경우도 포함한다. 또한 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아 니라 다른 구성요소를 더 포함할 수 있는 것을 의미하며, 하나 또는 그 이상의 다른 특징이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해 되어야 한다. 명세서 전체에서 사용되는 정도의 용어 \"약\", \"실질적으로\" 등은 언급된 의미에 고유한 제조 및 물질 허용오차 가 제시될 때 그 수치에서 또는 그 수치에 근접한 의미로 사용되고, 본 발명의 이해를 돕기 위해 정확하거나 절 대적인 수치가 언급된 개시 내용을 비양심적인 침해자가 부당하게 이용하는 것을 방지하기 위해 사용된다. 본 발명의 명세서 전체에서 사용되는 정도의 용어 \"~(하는) 단계\" 또는 \"~의 단계\"는 \"~ 를 위한 단계\"를 의미하지 않는다. 본 명세서에 있어서 '부(部)'란, 하드웨어에 의해 실현되는 유닛(unit), 소프트웨어에 의해 실현되는 유닛, 양 방을 이용하여 실현되는 유닛을 포함한다. 또한, 1 개의 유닛이 2 개 이상의 하드웨어를 이용하여 실현되어도 되고, 2 개 이상의 유닛이 1 개의 하드웨어에 의해 실현되어도 된다. 한편, '~부'는 소프트웨어 또는 하드웨어 에 한정되는 의미는 아니며, '~부'는 어드레싱 할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 '~부'는 소프트웨어 구성요소들, 객 체 지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로코드, 회 로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들 및 변수들을 포함한다. 구성요소들과 '~부'들 안에서 제공되는 기능은 더 작은 수의 구성요소들 및 '~부'들로 결합되거나 추가적인 구성요소들과 '~부'들로 더 분리될 수 있다. 뿐만 아니라, 구성요소들 및 '~부'들은 디바이스 또는 보안 멀티미디어카드 내의 하나 또 는 그 이상의 CPU들을 재생시키도록 구현될 수도 있다. 본 명세서에 있어서 단말, 장치 또는 디바이스가 수행하는 것으로 기술된 동작이나 기능 중 일부는 해당 단말, 장치 또는 디바이스와 연결된 서버에서 대신 수행될 수도 있다. 이와 마찬가지로, 서버가 수행하는 것으로 기 술된 동작이나 기능 중 일부도 해당 서버와 연결된 단말, 장치 또는 디바이스에서 수행될 수도 있다. 본 명세서에서 있어서, 단말과 매핑(Mapping) 또는 매칭(Matching)으로 기술된 동작이나 기능 중 일부는, 단말 의 식별 정보(Identifying Data)인 단말기의 고유번호나 개인의 식별정보를 매핑 또는 매칭한다는 의미로 해석 될 수 있다. 이하 첨부된 도면을 참고하여 본 발명을 상세히 설명하기로 한다. 도 1은 본 발명의 일 실시예에 따른 이중언어 학습 서비스 제공 시스템을 설명하기 위한 도면이다. 도 1을 참 조하면, 이중언어 학습 서비스 제공 시스템은, 적어도 하나의 사용자 단말, 학습 서비스 제공 서버 , 적어도 하나의 웨어러블 기기를 포함할 수 있다. 다만, 이러한 도 1의 이중언어 학습 서비스 제공 시스템은, 본 발명의 일 실시예에 불과하므로, 도 1을 통하여 본 발명이 한정 해석되는 것은 아니다. 이때, 도 1의 각 구성요소들은 일반적으로 네트워크(Network, 200)를 통해 연결된다. 예를 들어, 도 1에 도시 된 바와 같이, 적어도 하나의 사용자 단말은 네트워크를 통하여 학습 서비스 제공 서버와 연결 될 수 있다. 그리고, 학습 서비스 제공 서버는, 네트워크를 통하여 적어도 하나의 사용자 단말 , 적어도 하나의 웨어러블 기기와 연결될 수 있다. 또한, 적어도 하나의 웨어러블 기기는, 네 트워크를 통하여 학습 서비스 제공 서버와 연결될 수 있다. 여기서, 네트워크는, 복수의 단말 및 서버들과 같은 각각의 노드 상호 간에 정보 교환이 가능한 연결 구조를 의 미하는 것으로, 이러한 네트워크의 일 예에는 근거리 통신망(LAN: Local Area Network), 광역 통신망(WAN: Wide Area Network), 인터넷(WWW: World Wide Web), 유무선 데이터 통신망, 전화망, 유무선 텔레비전 통신망 등을 포함한다. 무선 데이터 통신망의 일례에는 3G, 4G, 5G, 3GPP(3rd Generation Partnership Project), 5GPP(5th Generation Partnership Project), LTE(Long Term Evolution), WIMAX(World Interoperability for Microwave Access), 와이파이(Wi-Fi), 인터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), RF(Radio Frequency), 블루투스 (Bluetooth) 네트워크, NFC(Near-Field Communication) 네트워크, 위성 방송 네트워크, 아날로그 방송 네트워 크, DMB(Digital Multimedia Broadcasting) 네트워크 등이 포함되나 이에 한정되지는 않는다. 하기에서, 적어도 하나의 라는 용어는 단수 및 복수를 포함하는 용어로 정의되고, 적어도 하나의 라는 용어가 존재하지 않더라도 각 구성요소가 단수 또는 복수로 존재할 수 있고, 단수 또는 복수를 의미할 수 있음은 자명 하다 할 것이다. 또한, 각 구성요소가 단수 또는 복수로 구비되는 것은, 실시예에 따라 변경가능하다 할 것이다. 적어도 하나의 사용자 단말은, 이중언어 학습 서비스 관련 웹 페이지, 앱 페이지, 프로그램 또는 애플리케 이션을 이용하여 적어도 하나의 웨어러블 기기와 연동되고, 웨어러블 기기에서 입력된 음성발화를 음 성발화 데이터로 학습 서비스 제공 서버로 전송한 후, 학습 서비스 제공 서버에서 실시간으로 번역된 번역 데이터를 TTS(Text to Speech)를 통하여 음성으로 출력하는 단말일 수 있다. 이때, 실시예에 따라 웨어러 블 기기가 구비되지 않을 수 있고, 이 경우에는 사용자 단말에서 [음성발화 입력-실시간 번역된 번역 데이터 수신-음성발화 출력]의 과정을 모두 수행하는 단말일 수 있다. 여기서, 적어도 하나의 사용자 단말은, 네트워크를 통하여 원격지의 서버나 단말에 접속할 수 있는 컴퓨터 로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 네비게이션, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱(Desktop), 랩톱(Laptop) 등을 포함할 수 있다. 이때, 적어도 하나의 사용자 단말은, 네트워크를 통해 원격지의 서버나 단말에 접속할 수 있는 단말로 구현될 수 있다. 적어도 하나의 사용자 단말은, 예 를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, 네비게이션, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말, 스마트폰(Smartphone), 스마트 패드(Smartpad), 타블렛 PC(Tablet PC) 등과 같은 모든 종류의 핸드헬드(Handheld) 기반의 무선 통신 장치를 포함할 수 있다. 학습 서비스 제공 서버는, 이중언어 학습 서비스 웹 페이지, 앱 페이지, 프로그램 또는 애플리케이션을 제 공하는 서버일 수 있다. 그리고, 학습 서비스 제공 서버는, 적어도 하나의 번역 서버와 연계되어 API 사용에 따른 과금을 정산하거나, API의 이용허락을 받아 라이센싱 계약을 맺은 후 번역 프로그램을 학습 서 비스 제공 서버에서 제공하는 서버일 수 있다. 또한, 학습 서비스 제공 서버는 사용자 단말에 서 웨어러블 기기를 등록하는 경우, 웨어러블 기기와 사용자 단말에서 음성발화의 입력 또는 출 력 등을 선택적으로 출력하도록 설정기능을 제공하는 서버일 수 있다. 웨어러블 기기를 입력 인터페이스 로, 사용자 단말을 출력 인터페이스로 사용할 수도 있고 그 역도 성립하며 웨어러블 기기 및 사용자 단말 중 어느 하나의 단말이나 기기만을 사용하여 두 가지 기능을 모두 구현하도록 할 수도 있으며, 이는 실시예나 구현예 또 사용자의 설정으로 다양하게 변경가능하도록 설정할 수 있다. 여기서, 학습 서비스 제공 서버는, 네트워크를 통하여 원격지의 서버나 단말에 접속할 수 있는 컴퓨터로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 네비게이션, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스 크톱(Desktop), 랩톱(Laptop) 등을 포함할 수 있다. 적어도 하나의 웨어러블 기기는, 이중언어 학습 서비스 관련 웹 페이지, 앱 페이지, 프로그램 또는 애플리 케이션을 이용하거나 이용하지 않고 사용자 단말과 연동되고, 웨어러블 기기에 입력된 음성발화를 사 용자 단말로 전송하는 장치일 수 있다. 이때, 웨어러블 기기는 넥밴드 형태의 마이크일 수도 있고, 핸드헬드형 마이크일 수도 있고 다양한 손목밴드 등 착용형태의 웨어러블 기기로 구현될 수 있으나 어느 하나에 한정하지 않고 열거되지 않은 이유로 배제하지 않는다. 여기서, 적어도 하나의 웨어러블 기기는, 네트워크를 통하여 원격지의 서버나 단말에 접속할 수 있는 컴퓨 터로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 네비게이션, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스크톱(Desktop), 랩톱(Laptop) 등을 포함할 수 있다. 이때, 적어도 하나의 웨어러블 기기는, 네트워크 를 통해 원격지의 서버나 단말에 접속할 수 있는 단말로 구현될 수 있다. 적어도 하나의 웨어러블 기기는, 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, 네비게이션, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말, 스마트폰(Smartphone), 스마트 패드(Smartpad), 타블렛 PC(Tablet PC) 등과 같은 모든 종류의 핸드헬드(Handheld) 기반의 무선 통신 장치를 포함할 수 있다. 적어도 하나의 번역 서버는, 이중언어 학습 서비스 관련 웹 페이지, 앱 페이지, 프로그램 또는 애플리케이 션을 이용하거나 이용하지 않고 학습 서비스 제공 서버로 번역 API나 프로그램 등을 제공하는 서버일 수 있다. 이때, 과금 체계에 따라 정산을 학습 서비스 제공 서버 또는 사용자 단말로 요청하는 서버일 수 있다. 여기서, 적어도 하나의 번역 서버는, 네트워크를 통하여 원격지의 서버나 단말에 접속할 수 있는 컴퓨터로 구현될 수 있다. 여기서, 컴퓨터는 예를 들어, 네비게이션, 웹 브라우저(WEB Browser)가 탑재된 노트북, 데스 크톱(Desktop), 랩톱(Laptop) 등을 포함할 수 있다. 이때, 적어도 하나의 번역 서버는, 네트워크를 통해 원격지의 서버나 단말에 접속할 수 있는 단말로 구현될 수 있다. 적어도 하나의 번역 서버는, 예를 들어, 휴대성과 이동성이 보장되는 무선 통신 장치로서, 네비게이션, PCS(Personal Communication System), GSM(Global System for Mobile communications), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말, 스마트폰(Smartphone), 스마트 패드(Smartpad), 타블렛 PC(Tablet PC) 등과 같은 모든 종류의핸드헬드(Handheld) 기반의 무선 통신 장치를 포함할 수 있다. 도 2는 도 1의 시스템에 포함된 학습 서비스 제공 서버를 설명하기 위한 블록 구성도이고, 도 3 및 도 4는 본 발명의 일 실시예에 따른 이중언어 학습 서비스가 구현된 일 실시예를 설명하기 위한 도면이다. 도 2를 참조하면, 학습 서비스 제공 서버는, 저장부, 음성입력부, 번역부, 변환부, 출력부, 화자인식부, 키워드인식부, 타이밍인식부, 재사용부, 부모음성제공부, 반응수집부, 부모모사부 및 과금부를 포함할 수 있다. 본 발명의 일 실시예에 따른 학습 서비스 제공 서버나 연동되어 동작하는 다른 서버(미도시)가 적어도 하 나의 사용자 단말 및 적어도 하나의 웨어러블 기기로 이중언어 학습 서비스 애플리케이션, 프로그램, 앱 페이지, 웹 페이지 등을 전송하는 경우, 적어도 하나의 사용자 단말 및 적어도 하나의 웨어러블 기기 는, 이중언어 학습 서비스 애플리케이션, 프로그램, 앱 페이지, 웹 페이지 등을 설치하거나 열 수 있다. 또한, 웹 브라우저에서 실행되는 스크립트를 이용하여 서비스 프로그램이 적어도 하나의 사용자 단말 및 적어도 하나의 웨어러블 기기에서 구동될 수도 있다. 여기서, 웹 브라우저는 웹(WWW: World Wide Web) 서 비스를 이용할 수 있게 하는 프로그램으로 HTML(Hyper Text Mark-up Language)로 서술된 하이퍼 텍스트를 받아 서 보여주는 프로그램을 의미하며, 예를 들어 넷스케이프(Netscape), 익스플로러(Explorer), 크롬(Chrome) 등을 포함한다. 또한, 애플리케이션은 단말 상의 응용 프로그램(Application)을 의미하며, 예를 들어, 모바일 단말 (스마트폰)에서 실행되는 앱(App)을 포함한다. 도 2를 설명하기 이전에 본 발명의 기본개념인 이중언어에 대한 개념을 다중언어와 비교하여 설명한다. 이하에 서 설명된 내용은 도 2에서 반복하여 설명하지 않는다. <다중언어주의> 먼저, 다중언어주의 하면 떠오르는 표상은 여러 개의 언어로, 이와 관련되어서는 다언어주의(Multilinguisme)와 다중언어주의(Plurilinguisme)라는 두 개의 용어가 존재한다. 특정 지역이나 공동체 내에서 여러언어가 공존하 고 병용되는 현상이나 상태를 설명하는 개념을 다언어주의로, 개별언어 사용자가 여러 언어를 의사소통 상황과 목적에 맞게 자율적으로 사용하는 양상에 주목하여 언어 사용자의 언어목록(Repertoire Linguistique)에 가치를 부여하고 이들 목록을 구성하는 요소들 간의 상호작용을 설명하는 개념을 다중언어주의로 구분한다. 이 두 개 념의 구분은 유럽에서 추진하고 있는 언어정책과 언어교육 정책에서 보다 구체적으로 드러난다. 유럽연합은 지 리, 경제, 정치적 이유로 형성된 초국가적 연합으로 다양성을 보존하면서도 단일성을 도모해야 했기에 언어정책 추진에 있어서는 모든 회원국들의 언어를 보호하고 증진시키기 위한 다언어주의를 채택하고 있고, 언어교육 정 책에 있어서는 이러한 다언어적 상황에서 유럽 시민들 간 의사소통을 촉진시키고 상호이해를 증진시키기 위해 다중언어주의를 그 핵심 원리로 채택하여 유럽 시민이 모국어 이외의 두 개 언어를 더 경험하고 학습할 수 있도 록 제도적으로 지원하고 있다. <이중언어주의> 한편 다중언어주의는 언어교육 영역에서 자주 함께 거론되는 이중언어주의(Bilinguisme)의 개념과도 구분되어야 한다. 몇몇 연구자들은 이중언어와 다중언어주의는 교수-학습의 대상이 되는 언어의 수에서만 차이가 날 뿐이 라고 주장하지만, 이 두 개념은 결코 동일한 개념이 아니다. 80년대 본격적으로 연구되기 시작한 이중언어주의 는 언어 사용자가 두 개의 언어를 모두 원어민 수준으로 유창하게 구사하는 이상적인 모델을 지향한다. 이러한 관점에서 각 언어 시스템은 분리되어 병렬적으로 존재하는 것으로 간주되므로 학습자는 각 언어를 분리시켜 학 습한 뒤 상황에 따라 두 개 언어를 유창하게 구사할 수 있는 능력을 지닌 존재로 정의된다. 반면에 90년대 유럽연합의 건설 및 회원국의 확대로 활발히 연구되기 시작한 다중언어주의의 관점에서 두 개 혹 은 그 이상의 언어의 사용 능력은 각 언어를 모두 원어민 수준으로 구사할 수 있는 능력이 아니라, 한 언어를 다른 한 언어보다 더 높은 수준으로 구사하거나, 특정상황에서는 특정언어를 더 잘 구사하거나, 혹은 해당 언어 로 모두 듣고 말할 수 있지만 읽거나 쓰는 것은 한 언어로만 가능한 경우 등과 같이 불완전하고 부분적인 능력 모두를 일컫는다. 즉, 다중언어주의 관점에서 두 개 혹은 그 이상의 언어를 학습하는 것은 각 언어를 분리된 영역에서 매번 완전한 백지 상태로 학습하는 것이 아니라, 선행 지식이나 경험을 새로운 언어학습에 연계시킴으 로써 선행학습한 언어 지식이나 경험도 새로운 언어능력과 함께 단계적으로 발달시키는 것이다. 따라서 다중언 어주의에 입각한 교육은 학습자의 상황이나 경험, 학습 방법 및 전략에 따른 부분 능력을 인정하고 학습자 언어 목록 상의 언어와 언어학습 경험들 간의 상호작용에 큰 가치를 부여한다. 이상에서 살펴본 다언어주의, 이중언어주의, 다중언어주의의 정의에 의거할 때, 단일 언어 국가인 한국에서 여 러 언어를 모두 보호ㆍ육성하는 다언어주의나 국가 주도하에 국민 모두 완벽하게 특정 언어 구사능력을 갖추도 록 하는 이중언어주의를 정책적으로 추진하는 것은 무리가 따른다. 이러한 이유로 다중언어주의를 다언어주의 나 이중언어주의와 동일한 개념으로 잘못 해석하게 되면 지금까지 그랬던 것처럼 다중언어 교육이 우리 교육 상 황과 무관하다는 편견이나 고정관념에 사로잡히게 될 수 밖에 없다. 그러나 개별 언어 사용자, 즉 학습자에 초 점을 맞춘 이중언어주의는 개별적으로 수행가능하며 우리 학습자들이 국제무대에서 자신의 권리를 온전히 누리 는데 필요한 능력을 보다 체계적이고 효율적으로 길러줄 수 있는 언어교육 정책을 수립하는 데 있어 큰 시사점 을 줄 수 있다. 이에, 본 발명의 일 실시예는 영유아기의 언어에 대한 개념이 백지상태일 때 이중언어를 함께 접할 수 있도록 함으로써 이중언어를 모국어와 같이 구사할 수 있도록 한다. 이를 위하여 본 발명의 일 실시예에 따른 시스템 은 부모가 음성발화한 제 1 언어를 제 2 언어로 실시간으로 번역하여 출력하도록 하며, 부모의 음성을 인지한 영유아의 경우 음성발화하는 음성도 부모의 음성을 그대로 모사하여 TTS(Text To Speech)가 되도록 함으로써, 부모가 이중언어를 발화하는 것과 같은 환경을 제공해줄 수 있다. 상술한 개념을 기반으로 도 2를 참조하면, 저장부는, 사용자 단말로부터 사용자 단말 및 웨어러 블 기기를 등록받아 매핑되도록 저장할 수 있다. 웨어러블 기기는, 음성발화를 입력받을 수 있다. 이때, 화자인식을 위해서는 지속적인 전원 공급 하에서 캐패시턴스 값의 변화를 측정해 소리를 감지하는 콘덴서 형 마이크로폰이 대부분인데, 콘덴서형 마이크로폰은 낮은 민감도로 인해 증폭이 필요하고, 지속적인 전력 공급 과 증폭으로 높은 전력 소비량을 가진다. 이에 본 발명의 웨어러블 기기에는 높은 민감도의 다중 공진 주 파수 대역을 가지는 생체모방 및 유연 압전 음성센서에 의해 활성화된 기계학습 기반 화자 인식 시스템을 이용 할 수 있다. 압전 방식을 이용하여 외부 전원 공급없이 자가 구동이 가능하며, PZT를 이용하여 높은 압전 상수 로 뛰어난 민감도를 가질 수 있다. 이때, 화자인식을 위해서는 GMM 알고리즘이 이용될 수 있는데, 평균 벡터 및 공분산 행렬을 포함하는 다중 확률밀도함수을 갖는 GMM(Gaussian Mixture Model)을 이용할 수 있다. 공분산 행렬은 GMM의 과적합을 방지하기 위해 대각선으로 제한될 수 있다. GMM 알고리즘의 매개변수는 기대 최대화 (Expectation Maximization) 알고리즘을 이용하여 학습될 수 있다. 음성입력부는, 웨어러블 기기에서 음성발화가 입력된 경우 사용자 단말에서 음성발화 데이터를 업로드하도록 할 수 있다. 웨어러블 기기에서 사용자의 음성발화를 입력받아 사용자 단말로 전송하 면, 사용자 단말에서 음성입력부로 업로드하는 것이다. 번역부는, 음성발화 데이터를 STT(Speech to Text)로 변환한 후 사용자 단말에서 설정한 언어로 번역 되도록 번역 프로그램을 이용할 수 있다. 번역 프로그램은 적어도 하나의 종류의 실시간 번역 API(Application Programming Interface)를 포함할 수 있다. 이때, 번역 프로그램은 번역 서버에 도시된 종류일 수 있으나 나열된 것들로 한정되지 않고 열거되지 않은 이유로 배제되지 않는다. 변환부는, 번역 프로그램에서 번역된 번역 데이터를 TTS(Text To Speech)로 변환할 수 있다. 출력부(35 0)는, 사용자 단말에서 변환된 이중언어 데이터를 음성발화되도록 출력할 수 있다. 사용자 단말은, 웨어러블 기기와 연동되어 음성발화된 음성발화 데이터를 수신하고, 음성발화 데이터를 실시간으로 번역하 여 이중언어 데이터로 출력할 수 있다. 화자인식부는, 사용자 단말로부터 적어도 하나의 양육자의 음성을 입력받아 등록하고, 화자식별 (Speaker Identification) 및 화자 검증(Speaker Verification)을 통하여 등록된 화자인지의 여부를 확인한 후, 등록된 화자로 식별 및 검증된 경우에만 실시간 번역을 시작할 수 있다. 이때, 사용자 단말의 사용자 는 적어도 하나의 양육자를 포함할 수 있다. 사람 음성의 특징을 추출하여 인식하는 화자 인식(Speaker Recognition) 기술은 특정 문장의 사용에서의 인식에 따라 문장 종속(Text Dependent)방식과 문장 독립(Text Independent)방식으로 구분된다. 문장 종속방식은 한 사람 목소리의 고유한 개별 특성을 학습함으로써 작동하 고, 화자 등록 과정과 검증 과정 음성의 내용이 동일하기 때문에 높은 정확도를 얻을 수 있다. 반대로 문장 독 립방식은 불특정 다수 화자의 음성을 인식하도록 개발되고, 검증 과정에서의 음성의 내용이 등록한 음성의 내용 과 무관하기에 상대적으로 높은 정확도를 얻기 힘들다. 본 발명의 일 실시예에서는 음성신호를 통해 정확한 화자 인식을 위해 음성 추출 과정을 거친 음성을 딥러닝 알 고리즘 중 합성곱 신경망 알고리즘을 이용한 모델을 음성 데이터에 맞게 개선한 후 사용하여 음성 데이터를 학 습하도록 한다. 그 결과를 통해 일반적인 전처리 과정 및 화자 인식모델보다 높은 정확도를 나타낼 수 있는 방 법을 이용할 수 있다.<단시간 푸리에 변환> 단시간 푸리에 변환(Short-Time Fourier Transform, STFT)은 시간이 지남에 따라 변화하는 음성신호의 사인파 주파수와 위상 성분을 결정하는 데 사용되는 푸리에 변환이다. 고속 푸리에 변환(Fast Fourier Transform)은 단순하게 주파수 성분만을 표시하여 결함의 발생 시기 및 주파수 성분 분석에 어려움이 따르기 때문에 이를 해 결하기 위해서 제안된 STFT는 시간에 따라 바뀌는 긴 음성신호를 짧은 시간 단위로 분할한 후 푸리에 변환을 적 용하기에 각 구간마다 어떤 주파수들이 존재하는지 알 수 있다. <MFCC> MFCC(Mel Frecquency Cepstrum Coefficients)는 각 구간의 주파수를 변환하여 음성신호의 특징 파라미터 (Parameter)를 얻게 된다. STFT 알고리즘을 거쳐 음성신호를 짧은 시간 단위로 분할 후 이산 푸리에 변환 (Discrete Fourier Transform, DFT) 을 이용하여 주파수 영역으로 변환되어 인간의 청각특성을 반영하도록 한다. 이러한 과정을 거치기 때문에 음성의 스펙트럼 기반을 특징으로 하며 인간의 귀가 가지는 비선형적인 주 파수 특성을 이용하는 유효한 특징 파라미터값으로 알려져 있다. 사람의 귀의 비선형성에 기인하여 잡음이 있 는 환경에 강하며 비 균일한 스케일로 주파수를 나눌 수 있다는 장점을 갖고 있다. <CNN> CNN(Convolutional Neural Networks)은 동물의 시각피질 구조에서 영감을 얻어 만들어진 딥러닝 신경망 모델로 동물의 계층적 특징 추출과 시각인식 체계를 참조하여 만들어졌다. 합성곱 연산을 적용하여 신경망을 연산하는 것으로 이미지 및 자연어해석, 컴퓨터 비전, 영상분류 등 다양한 분야에 활용되고 있다. 합성곱 신경망은 이미 지 일부분의 특징을 추출하고 이러한 특징을 통하여 예측하여 2차원 배열의 데이터를 사용하기 때문에 일반적으 로 2D 합성곱(2D Convolution)을 사용하지만, 생체 신호 분석 및 음성 데이터와 같은 1차원 시계열 데이터를 사 용할 때에는 1D 합성곱(1D Convolution)을 사용한다. 본 발명의 일 실시예에서는 시계열 데이터 중 하나인 음 성 데이터를 주로 다룰 것이기 때문에 1D 합성곱을 이용한 모델을 이용하기로 한다. <화자인식모델> 화자를 등록하는 과정에서는 화자 다수의 동일한 내용의 음성 파일과 차별화된 음성 파일에서 전체적인 전처리 과정을 거쳐 화자의 음성 특징을 추출하여 MFCC를 생성한다. 생성한 MFCC를 음성으로 화자를 구분하여 인식할 수 있는 화자 인식 모델로 학습한다. 화자를 인식하는 과정에서는 화자 본인이 지정한 내용의 음성 파일에서 화자의 음성 특징을 추출한 후 기존에 학습이 되어있는 음성 특징과 비교하여 유사도를 통해 화자를 인식한다. <음성 특징 추출> 화자의 음성 특징을 추출하는 과정은 이하 표 1과 같을 수 있다. 화자의 음성 특징을 추출하기 위하여, 사용자 에게 음성 입력을 받게 되면 시간에 따라 변하는 음성신호의 특성을 제대로 처리하기 위해 일정한 프레임 단위 로 자른 후 Hanning window 함수를 씌우고, STFT 알고리즘을 거쳐 음성 데이터를 주파수 영역으로 변환시킨다. 그 후 멜 스케일 필터를 사용하여 STFT 알고리즘을 거친 음성과 삼각 필터뱅크(Filter-Bank) 값을 각각 곱해준 후 dB로 크기를 바꿔주면 멜 스펙트로그램(Mel Spectrogram) 값을 얻을 수 있다. 이 값에 로그값을 취해준 후 이산코사인 변환(Discrete Cosine Transform, DCT) 연산을 적용해준 후 주변 잡음에 덜 예민하게 반응하기 위해 Lifter 함수를 적용하는 작업을 해주면 최종적으로 사용할 음성 데이터의 MFCC 값을 얻을 수 있다. 표 1 진행순서(진행방향→) 음성발화 Framing WindowingSTFT Mel- SpectrogramDCT LifteringMFCCs <단시간 푸리에 변환> STFT는 상술한 바와 같이, 시간, 주파수와 음량 간의 상관관계를 손실하지 않고 특징을 추출 해내기 위해서 시 간 구간별로 푸리에 변환을 수행하는 알고리즘이다. 각 구간별로 주파수를 분할한 후 분석하고자 하는 신호에 Hanning Window 함수를 적용한 후 푸리에 변환을 수행하는 것으로 수학식 1과 같이 정리할 수 있다.수학식 1"}
{"patent_id": "10-2022-0060312", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 x(t)는 분석하고자 하는 신호이고, w(r-t)는 Window 함수이다. 이때, 예를 들어, 샘플링레이트 (Sampling Rate), Hop Length, n_mels, n_fft를 적용할 수 있는데, 샘플링 레이트는 이산적 신호를 얻기 위한 단위 시간당 샘플링 횟수를 뜻하고, Hop Length는 음성의 크기(Magnitude)를 얼마나 겹쳐서 잘라서 보여주는지 를 뜻하고, n_mels는 주파수를 나눠주는 수이고, n_fft는 음성 프레임의 길이를 결정하는 것이다. <De-Emphasize를 위한 Lifter 함수 적용> 주변 소음이 있는 환경에서도 음성의 특징을 추출하여 높은 인식률을 보이는 MFCC를 잡음이 있는 환경에서도 높 은 성능을 보이기 위해 잡음 및 주위 환경 변화에 적응할 수 있는 Lifter 함수를 적용할 수 있다. Lifter 함수 연산은 전체 켑스트럼(Cepstrum)에서 원하는 위치의 직사각형 Window를 곱해 분석할 주파수 영역을 채택하는 주 파수 영역에서의 필터링 연산이기 때문에 분석 알고리즘 자체에서 기인하는 변동성 및 개별 화자에 따른 변동성 의 영향을 줄여준다. <CNN을 이용한 화자 인식 모델> 음성 데이터를 입력을 받으면 상술한 전처리 과정을 거쳐 변환된 형태로 1D-CNN 모델에 입력이 되어 학습을 할 수 있다. CNN 모델의 구조는 크게 4번의 1D-CNN 합성곱 연산을 거친 후 Fully Connected 되어 학습을 마칠 수 있다. 시계열 데이터를 학습하기에 유용한 1D-CNN 층을 통해 값들을 합성곱 연산을 한 후 학습의 속도를 올리 기 위해 정규화를 거치게 되는데, 사용한 모델에서는 Batch 정규화를 실시할 수 있다. 그 후엔 활성화 층 (Activation Layer)을 거치게 되는데, 사용한 모델에서는 ReLU 활성화 함수를 사용할 수 있다. 그 후엔 Max Pooling 층을 통해 결과 벡터로부터 가장 큰 값을 빼낸다. 이런 과정을 총 4번 거친 후 Flatten 층과 Dense 층 을 통해 Fully Connected하여 모든 값을 학습한 후 출력값을 가질 수 있다. 물론 상술한 CNN 외에도 시계열 데이터를 다루는데 유용한 알고리즘 중 좋다고 평가되는 순환 신경망(Recurrent Neural Network) 알고리즘 종류의 장단기 메모리(Long Short-Term Memory, LSTM)를 1D 합성곱과 결합하여 사용 하게 된다면 많은 음성 데이터를 다룰 때 재귀적 학습을 통해 화자 인식 시스템의 향상된 성능을 얻을 수 있다. 또, 생산적 적대 신경망(Generative Adversarial Network)을 사용하여 주어진 학습 데이터의 확률 분포를 학습 하여 학습된 분포로부터 새로운 데이터를 생성하여 데이터 증강의 한계점을 개선하게 된다면 음성 데이터를 학 습하기 힘든 CNN 알고리즘보다 안정적으로 학습이 가능할 수 있다. 물론, 상술한 알고리즘에 한정되지 않고 다 양한 알고리즘을 이용하여 화자인식이 가능할 수 있다. 키워드인식부는, 음성발화가 사용자 단말의 사용자의 음성발화인지의 여부를 확인하기 위하여, 기 설 정된 키워드의 발화여부를 체크한 후, 기 설정된 키워드가 발화되는 경우에만 실시간 번역을 시작할 수 있다. 키워드를 미리 설정해두고, 이 키워드가 인식되는 경우에 키워드 다음부터 실시간 번역을 수행할 수 있다. 또 는 타이밍인식부는, 웨어러블 기기에 적어도 하나의 종류의 입력이 발생하는 경우에만 실시간 번역을 시작할 수 있다. 즉 버튼 등을 누르고 있는 동안에만 또는 버튼을 누른 후 음성발화를 인식한 후 음성발화가 끝날 때까지 인식을 수행할 수 있다. 재사용부는, 번역 프로그램으로 기 번역된 번역 데이터를 사용자 단말에 저장하고, 사용자 단말(10 0)에 입력된 음성발화 데이터 중 기 번역된 번역 데이터와 동일한 데이터가 존재하는 경우, 기 번역된 번역 데 이터를 재사용하도록 할 수 있다. 물론, 기 번역된 번역 데이터에 대한 저작권 또는 라이센싱에 대한 이용계약 이 가능한 경우 또는 번역 서버의 기업과 협의가 된 경우에만 가능할 수도 있다. 이때, 영유아의 상태, 예를 들어, 수면상태, 식사중, 노는중 등에 따라 출력되는 번역 데이터의 음성발화의 볼륨을 자동으로 증감되도 록 설정할 수도 있다. 노는중에는 아이도 소리를 지를 가능성이 높기 때문에 너무 작으면 들리지 않고 수면에 접어드는 경우 너무 큰 소리로 TTS가 음성발화를 하게 되면 아기가 잠에서 깰 수 있기 때문이다. 또, 반복하여 출력하고자 하는 문장이나 문구 등을 미리 등록시켜두고, 사용자가 기 설정해 둔 주기로 반복하여 제공되도록 할 수도 있다. 부모음성제공부는, 사용자 단말로부터 사용자의 음성 샘플을 등록받고, TTS의 음성을 음성 샘플로 학 습시킨 후 사용자 단말에서 TTS로 출력되는 음성을 사용자의 음성으로 출력되도록 설정할 수 있다. 최근 딥보이스(Deep Voice)라는 프로그램과 같이 음성 샘플을 제공한 후 학습을 시키면 TTS의 목소리가 음성 샘플을 제공한 사람의 목소리처럼 나오는 프로그램들이 다양하게 출시되고 있다. 또, 이하에서 설명될 영유아가 우는 패턴 또는 칭얼거리거나 소리를 지르는 음성을 학습하고, 영유아의 음성에 반응하는 부모의 음성발화를 매핑하 여 데이터셋(DataSet)을 만들어 학습 및 검증시키는 방법으로 모델링을 하는 경우, 부모가 즉각적으로 영유아에 게 가지 못하더라도, 아기가 우는 경우, 음성으로부터 특징을 추출하고, 추출된 특징에 매핑된 부모의 음성발화 를 재생시켜주면, 즉각적인 부모 모사가 가능해진다. 이때 영유아의 경우 단어를 말하는 것이 아니라 소리를 낼 뿐이므로 STT를 이용한 음성인식은 불가하다. 이에 따라, 음성신호를 스펙트로그램으로 표현한 후, [영유아 음성신호-스펙트로그램-부모 음성발화]의 관계를 CNN으로 학습하는 경우 유사한 영유아의 음성신호에 대해 즉각 적으로 부모의 음성발화를 들려줄 수 있게 된다. CNN은 이미지를 학습하는 인공지능 알고리즘이기 때문에 스펙 트로그램으로 음성신호를 변환하여 사용하는 경우 음성신호의 특징을 찾아내기 적합하다. 이에 따라, 반응수집부는, 웨어러블 기기로부터 음성발화가 입력되기 이전에 웨어러블 기기 또 는 적어도 하나의 베이비 모니터로부터 입력된 영유아 음성 데이터를 수집하고, 영유아 음성 데이터와 사용자의 음성발화 데이터 간을 매핑하여 저장할 수 있다. 부모모사부는, 영유아 음성 데이터를 스펙트로그램 (Spectrogram)으로 분석하여 특징을 분석한 후, CNN(Convolutional Neural Network)로 영유아 음성 데이터의 특징에 따른 사용자의 음성발화 데이터를 학습 및 검증하며, 웨어러블 기기 또는 적어도 하나의 베이비 모 니터로부터 영유아 음성 데이터가 질의(Query)로 입력된 경우, 영유아 음성 데이터에 대응하는 사용자의 음성발 화 데이터가 웨어러블 기기 또는 적어도 하나의 베이비 모니터로부터 출력되도록 제어할 수 있다. 과금부는, 사용자 단말에서 번역에 이용한 데이터에 대응한 과금 데이터를 생성하여 사용자 단말 로 결제청구를 전송할 수 있다. 이하, 상술한 도 2의 학습 서비스 제공 서버의 구성에 따른 동작 과정을 도 3 및 도 4를 예로 들어 상세히 설명 하기로 한다. 다만, 실시예는 본 발명의 다양한 실시예 중 어느 하나일 뿐, 이에 한정되지 않음은 자명하다 할 것이다. 도 3을 참조하면, (a) 학습 서비스 제공 서버는 사용자 단말로부터 웨어러블 기기를 등록받고 (b)와 같이 웨어러블 기기에서 음성발화가 입력되어 사용자 단말을 경유하여 업로드되는 경우, 번역 된 번역 데이터를 다시 사용자 단말로 전송하여 TTS가 음성발화를 스피커로 출력하도록 한다. (c) 이때, 음성인식시작방법은 화자인식, 키워드인식 및 버튼인식 등을 포함할 수 있고 이는 사용자가 선택적으로 또는 모 두 사용할 수도 있다. (d) 학습 서비스 제공 서버는 또 사용자 단말에서 사용한 데이터 또는 번역 서버의 과금 체계에 따라 과금을 할 수도 있고, 학습 서비스 제공 서버는 도 4의 (a)와 같이 부모 또 는 양육자의 음성을 모사하여 TTS가 그대로 발성하도록 학습 및 검증을 시킬 수도 있으며, (b) 영유아의 음성과 부모의 반응을 학습하도록 함으로써 (c) 인공지능 알고리즘이 부모의 목소리, 반응, 음성발화 등을 그대로 모사 하여 아기의 울음소리 등에 반응하도록 구현할 수도 있다. 이와 같은 도 2 내지 도 4의 이중언어 학습 서비스 제공 방법에 대해서 설명되지 아니한 사항은 앞서 도 1을 통 해 이중언어 학습 서비스 제공 방법에 대하여 설명된 내용과 동일하거나 설명된 내용으로부터 용이하게 유추 가 능하므로 이하 설명을 생략하도록 한다. 도 5는 본 발명의 일 실시예에 따른 도 1의 이중언어 학습 서비스 제공 시스템에 포함된 각 구성들 상호 간에 데이터가 송수신되는 과정을 나타낸 도면이다. 이하, 도 5를 통해 각 구성들 상호간에 데이터가 송수신되는 과 정의 일 예를 설명할 것이나, 이와 같은 실시예로 본원이 한정 해석되는 것은 아니며, 앞서 설명한 다양한 실시"}
{"patent_id": "10-2022-0060312", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "예들에 따라 도 5에 도시된 데이터가 송수신되는 과정이 변경될 수 있음은 기술분야에 속하는 당업자에게 자명 하다. 도 5를 참조하면, 학습 서비스 제공 서버는, 사용자 단말로부터 사용자 단말 및 웨어러블 기기를 등록받아 매핑 되도록 저장한다(S5100). 그리고, 학습 서비스 제공 서버는, 웨어러블 기기에서 음성발화가 입력된 경우 사용자 단말에서 음성발화 데이 터를 업로드하도록 하고(S5200), 음성발화 데이터를 STT(Speech to Text)로 변환한 후 사용자 단말에서 설정한 언어로 번역되도록 번역 프로그램을 이용한다(S5300). 또, 학습 서비스 제공 서버는, 번역 프로그램에서 번역된 번역 데이터를 TTS(Text To Speech)로 변환하고 (S5400), 사용자 단말에서 변환된 이중언어 데이터를 음성발화되도록 출력한다(S5500).상술한 단계들(S5100~S5500)간의 순서는 예시일 뿐, 이에 한정되지 않는다. 즉, 상술한 단계들(S5100~S5500)간 의 순서는 상호 변동될 수 있으며, 이중 일부 단계들은 동시에 실행되거나 삭제될 수도 있다. 이와 같은 도 5의 이중언어 학습 서비스 제공 방법에 대해서 설명되지 아니한 사항은 앞서 도 1 내지 도 4를 통 해 이중언어 학습 서비스 제공 방법에 대하여 설명된 내용과 동일하거나 설명된 내용으로부터 용이하게 유추 가 능하므로 이하 설명을 생략하도록 한다. 도 5를 통해 설명된 일 실시예에 따른 이중언어 학습 서비스 제공 방법은, 컴퓨터에 의해 실행되는 애플리케이 션이나 프로그램 모듈과 같은 컴퓨터에 의해 실행가능한 명령어를 포함하는 기록 매체의 형태로도 구현될 수 있 다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발 성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독가능 매체는 컴퓨터 저장 매체를 모두 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터 와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 전술한 본 발명의 일 실시예에 따른 이중언어 학습 서비스 제공 방법은, 단말기에 기본적으로 설치된 애플리케 이션(이는 단말기에 기본적으로 탑재된 플랫폼이나 운영체제 등에 포함된 프로그램을 포함할 수 있음)에 의해 실행될 수 있고, 사용자가 애플리케이션 스토어 서버, 애플리케이션 또는 해당 서비스와 관련된 웹 서버 등의 애플리케이션 제공 서버를 통해 마스터 단말기에 직접 설치한 애플리케이션(즉, 프로그램)에 의해 실행될 수도 있다. 이러한 의미에서, 전술한 본 발명의 일 실시예에 따른 이중언어 학습 서비스 제공 방법은 단말기에 기본 적으로 설치되거나 사용자에 의해 직접 설치된 애플리케이션(즉, 프로그램)으로 구현되고 단말기에 등의 컴퓨터 로 읽을 수 있는 기록매체에 기록될 수 있다."}
{"patent_id": "10-2022-0060312", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "전술한 본 발명의 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자는 본 발명 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 발명의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으 로 해석되어야 한다.도면 도면1 도면2 도면3 도면4 도면5"}
{"patent_id": "10-2022-0060312", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 이중언어 학습 서비스 제공 시스템을 설명하기 위한 도면이다. 도 2는 도 1의 시스템에 포함된 학습 서비스 제공 서버를 설명하기 위한 블록 구성도이다. 도 3 및 도 4는 본 발명의 일 실시예에 따른 이중언어 학습 서비스가 구현된 일 실시예를 설명하기 위한 도면이 다. 도 5는 본 발명의 일 실시예에 따른 이중언어 학습 서비스 제공 방법을 설명하기 위한 동작 흐름도이다."}
