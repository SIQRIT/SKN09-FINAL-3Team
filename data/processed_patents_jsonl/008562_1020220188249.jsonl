{"patent_id": "10-2022-0188249", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0105824", "출원번호": "10-2022-0188249", "발명의 명칭": "전이학습 기반의 에이전트 학습 시스템 및 방법", "출원인": "한국전자통신연구원", "발명자": "장수영"}}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터에 의해 수행되는 방법에 있어서,제1 환경 조건에서 기 학습된 에이전트(이하, 소스 에이전트)를 준비하는 단계;상기 소스 에이전트를 이용하여 상기 제1 환경 조건과 상이한 제2 환경 조건에서 학습할 에이전트(이하, 타겟에이전트)의 학습을 위한 학습 데이터를 획득하는 단계;상기 학습 데이터를 기반으로 상기 타겟 에이전트를 사전 학습시키는 단계; 및상기 사전 학습된 타겟 에이전트를 대상으로 상기 제2 환경 조건에서 심층 강화학습 기반의 학습을 수행하는 단계를 포함하는,전이학습 기반의 에이전트 학습 방법."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 소스 에이전트를 이용하여 상기 제1 환경 조건과 상이한 제2 환경 조건에서 타겟 에이전트의 학습을 위한학습 데이터를 획득하는 단계는,상기 제1 환경 조건을 대상으로, 기 수집되는 소스 에이전트를 위한 제1 관측값에 더하여 상기 타겟 에이전트를위한 제2 관측값을 수집하는 단계;상기 제1 환경 조건에서 상기 소스 에이전트를 동작시키는 단계; 및상기 제1 환경 조건에서의 동작 결과를 기반으로 상기 타겟 에이전트의 학습을 위한 학습 데이터를 수집하는 단계를 포함하는,전이학습 기반의 에이전트 학습 방법."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 제1 환경 조건에서 상기 소스 에이전트를 동작시키는 단계는,정책 신경망에서의 상기 제1 관측값에 대한 최적 행동값을 획득하는 단계; 및가치 신경망에서의 상기 제1 관측값에 대한 현 상태의 가치값을 획득하는 단계를 포함하는,전이학습 기반의 에이전트 학습 방법."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 제1 환경 조건에서의 동작 결과를 기반으로 상기 타겟 에이전트의 학습을 위한 학습 데이터를 수집하는 단계는,상기 제2 관측값과, 상기 제1 관측값에 대한 최적 행동값 및 가치값을 상기 학습 데이터로 수집하는 것인,공개특허 10-2024-0105824-3-전이학습 기반의 에이전트 학습 방법."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 학습 데이터를 기반으로 상기 타겟 에이전트를 사전 학습시키는 단계는,상기 타겟 에이전트의 정책 신경망 및 가치 신경망을 초기화하는 단계;상기 초기화된 정책 신경망 및 가치 신경망을 대상으로 지도학습 기반의 학습을 위한 상기 학습 데이터를 설정하는 단계; 및상기 학습 데이터를 기반으로 정책 신경망 및 가치 신경망의 각 손실함수 계산 및 가중치 갱신 과정을 반복 수행하여 학습하는 단계를 포함하는,전이학습 기반의 에이전트 학습 방법."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 학습 데이터를 기반으로 정책 신경망 및 가치 신경망의 각 손실함수 계산 및 가중치 갱신 과정을 반복 수행하여 학습을 수행하는 단계는,상기 각 손실함수 값의 감소 정도에 따른 학습 조기 종료 조건 및 사전 정의된 반복 횟수 중 적어도 하나를 포함하는 학습 종료 조건을 만족할 때까지 상기 학습을 반복 수행하는 것인,전이학습 기반의 에이전트 학습 방법."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 사전 학습된 타겟 에이전트를 대상으로 상기 제2 환경 조건에서 심층 강화학습 기반의 학습을 수행하는 단계는,상기 제2 환경 조건에 상응하는 문제 해결 성공률 및 사전 정의된 반복 횟수 중 적어도 하나를 포함하는 학습조건을 만족할 때까지 상기 심층 강화학습 기반의 학습을 반복 수행하는 것인,전이학습 기반의 에이전트 학습 방법."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 환경 조건에서 기 학습된 에이전트(이하, 소스 에이전트) 및 상기 소스 에이전트에 기반하여 상기 제1 환경조건과 상이한 제2 환경 조건에서 학습할 에이전트(이하, 타겟 에이전트)의 학습을 위한 프로그램이 저장된 메모리 및상기 메모리에 저장된 프로그램을 실행시킴에 따라, 상기 타겟 에이전트의 학습을 위한 학습 데이터를획득하고, 상기 학습 데이터를 기반으로 상기 타겟 에이전트를 사전 학습시킨 후, 상기 사전 학습된 타겟 에이전트를 대상으로 상기 제2 환경 조건에서 심층 강화학습 기반의 학습을 수행하는 프로세서를 포함하는,전이학습 기반의 에이전트 학습 시스템."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "공개특허 10-2024-0105824-4-제8항에 있어서,상기 프로세서는 상기 제1 환경 조건을 대상으로, 기 수집되는 소스 에이전트를 위한 제1 관측값에 더하여 상기타겟 에이전트를 위한 제2 관측값을 상태값에 포함하여 반환하고, 상기 제1 환경 조건에서 상기 소스 에이전트를 동작시킨 후 동작 결과를 기반으로 상기 타겟 에이전트의 학습을 위한 학습 데이터를 수집하는 것인,전이학습 기반의 에이전트 학습 시스템."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 프로세서는 정책 신경망에서의 상기 제1 관측값에 대한 최적 행동값과, 가치 신경망에서의 상기 제1 관측값에 대한 현 상태의 가치값을 각각 획득하는 것인,전이학습 기반의 에이전트 학습 시스템."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 프로세서는 상기 제2 관측값과, 상기 제1 관측값에 대한 최적 행동값 및 가치값을 상기 학습 데이터로 수집하는 것인,전이학습 기반의 에이전트 학습 시스템."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항에 있어서,상기 프로세서는 상기 타겟 에이전트의 정책 신경망 및 가치 신경망을 초기화시킨 후, 지도학습 기반으로 상기초기화된 정책 신경망 및 가치 신경망을 학습시키기 위한 상기 학습 데이터를 설정한 후, 상기 설정된 학습 데이터를 기반으로 정책 신경망 및 가치 신경망의 각 손실함수 계산 및 가중치 갱신 과정을 반복 수행하여 학습을수행하는 것인,전이학습 기반의 에이전트 학습 시스템."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 프로세서는 상기 각 손실함수 값의 감소 정도에 따른 학습 조기 종료 조건 및 사전 정의된 반복 횟수 중적어도 하나를 포함하는 학습 종료 조건을 만족할 때까지 상기 학습을 반복 수행하는 것인,전이학습 기반의 에이전트 학습 시스템."}
{"patent_id": "10-2022-0188249", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8항에 있어서,상기 프로세서는 상기 제2 환경 조건에 상응하는 문제 해결 성공률 및 사전 정의된 반복 횟수 중 적어도 하나를포함하는 학습 조건을 만족할 때까지 상기 심층 강화학습 기반의 학습을 반복 수행하는 것인,전이학습 기반의 에이전트 학습 시스템.공개특허 10-2024-0105824-5-"}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전이학습 기반의 에이전트 학습 방법이 제공된다. 상기 방법은 제1 환경 조건에서 기 학습된 에이전트(이하, 소 스 에이전트)를 준비하는 단계; 상기 소스 에이전트를 이용하여 상기 제1 환경 조건과 상이한 제2 환경 조건에서 학습할 에이전트(이하, 타겟 에이전트)의 학습을 위한 학습 데이터를 획득하는 단계; 상기 학습 데이터를 기반으 로 상기 타겟 에이전트를 사전 학습시키는 단계; 및 상기 사전 학습된 타겟 에이전트를 대상으로 상기 제2 환경 조건에서 심층 강화학습 기반의 학습을 수행하는 단계를 포함한다."}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 전이학습 기반의 에이전트 학습 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 들어 자율 로봇 제어 시스템의 구축을 위한 요소 기술들이 급격히 발전하고 있으며, 자율 로봇 제어를 위 한 심층 강화학습 기술에 대한 연구가 활발히 진행되고 있다. 하지만, 실제 자율 로봇 제어 시스템이 적용되는 분야는 다양하므로, 각 분야에 맞게 심층 신경망을 최적화 시 키기에는 많은 시간 및 비용이 소요되는 문제가 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 공개특허공보 제10-2020-0061653호 (2020.06.03)"}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는 변경된 상황에 맞는 에이전트를 학습시킬 때 기존 상황에서 학습된 에이전트 의 지식을 신규 에이전트로 전이하는 전이 학습 기반의 학습 방식을 적용하여, 변화된 상황에서 에이전트를 처 음부터 학습시킬 때 필요한 시간 대비 현저히 적은 시간으로 에이전트 학습이 가능한, 전이학습 기반의 에이전 트 학습 시스템 및 방법을 제공하는 것을 목적으로 한다. 다만, 본 발명이 해결하고자 하는 과제는 상기된 바와 같은 과제로 한정되지 않으며, 또다른 과제들이 존재할 수 있다."}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 과제를 해결하기 위한 본 발명의 제1 측면에 따른 전이학습 기반의 에이전트 학습 방법은 제1 환경 조건 에서 기 학습된 에이전트(이하, 소스 에이전트)를 준비하는 단계; 상기 소스 에이전트를 이용하여 상기 제1 환 경 조건과 상이한 제2 환경 조건에서 학습할 에이전트(이하, 타겟 에이전트)의 학습을 위한 학습 데이터를 획득 하는 단계; 상기 학습 데이터를 기반으로 상기 타겟 에이전트를 사전 학습시키는 단계; 및 상기 사전 학습된 타 겟 에이전트를 대상으로 상기 제2 환경 조건에서 심층 강화학습 기반의 학습을 수행하는 단계를 포함한다. 또한, 본 발명의 제2 측면에 따른 전이학습 기반의 에이전트 학습 시스템은 제1 환경 조건에서 기 학습된 에이 전트(이하, 소스 에이전트) 및 상기 소스 에이전트에 기반하여 상기 제1 환경 조건과 상이한 제2 환경 조건에서 학습할 에이전트(이하, 타겟 에이전트)의 학습을 위한 프로그램이 저장된 메모리 및 상기 메모리에 저장된 프로 그램을 실행시킴에 따라, 상기 타겟 에이전트의 학습을 위한 학습 데이터를 획득하고, 상기 학습 데이터를 기반 으로 상기 타겟 에이전트를 사전 학습시킨 후, 상기 사전 학습된 타겟 에이전트를 대상으로 상기 제2 환경 조건 에서 심층 강화학습 기반의 학습을 수행하는 프로세서를 포함한다. 상술한 과제를 해결하기 위한 본 발명의 다른 면에 따른 컴퓨터 프로그램은, 전이학습 기반의 에이전트 학습 방 법을 실행하며, 컴퓨터 판독가능 기록매체에 저장된다. 본 발명의 기타 구체적인 사항들은 상세한 설명 및 도면들에 포함되어 있다."}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "실제 환경에서 해결해야 하는 문제는 다양하므로, 각 문제마다 에이전트를 처음부터 별개로 학습시키기에는 많 은 학습 시간 및 컴퓨팅 자원이 소요되는 효용성 측면의 문제가 있었으나, 본 발명의 일 실시예에 의하면 기존 문제에서 학습된 에이전트의 지식을 신규 에이전트로 전이하는 방식을 적용하는 것을 통해, 심층 강화학습 기반에이전트의 실적용을 촉진시킬 수 있다. 특히, 본 발명의 일 실시예는 새로운 문제 해결을 위한 에이전트를 빠르게 학습시킬 수 있으며, 이를 통해 다양 한 문제 즉, 다양한 상황이 존재하는 실제 환경에 대한 매우 높은 적용 가능성을 기대할 수 있다. 또한, 본 발명의 일 실시예를 자율 로봇 제어 시스템에 적용시, 로봇에 탑재된 센서가 바뀌거나 로봇의 임무가 바뀌더라도 새로운 에이전트를 빠르게 학습시킬 수 있는바, 자율 로봇 제어 기술의 응용 분야를 확장시킬 수 있 다. 뿐만 아니라, 자율 로봇 제어 기술의 실적용을 촉진시킬 수 있다. 즉, 본 발명의 일 실시예는 상황 변화에 대한 빠른 대처가 가능하도록 하여 다양한 상황이 존재하는 실제 환경에서의 큰 적용상 이점을 기대할 수 있다."}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과들은 이상에서 언급된 효과로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재로 부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나, 본 발명은 이하에서 개시되는 실시예들에 제한되는 것이 아니라 서 로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하고, 본 발명이 속하는 기술 분야의 통상의 기술자에게 본 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명 세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한다 (comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성요소 외에 하나 이상의 다른 구성요소의 존재 또는 추가를 배제하지 않는다. 명세서 전체에 걸쳐 동일한 도면 부호는 동일한 구성 요소를 지칭하며, \"및/또는\"은 언급된 구성요소들의 각각 및 하나 이상의 모든 조합을 포함한다. 비록 \"제1\", \"제2\" 등이 다양한 구성요소들을 서술하기 위해서 사용되나, 이들 구성요소들은 이들 용어에 의해 제한되지 않음은 물론이다. 이들 용어들은 단 지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사용하는 것이다. 따라서, 이하에서 언급되는 제1 구성 요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있음은 물론이다. 다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어(기술 및 과학적 용어를 포함)는 본 발명이 속하는 기술 분야의 통상의 기술자에게 공통적으로 이해될 수 있는 의미로 사용될 수 있을 것이다. 또한, 일반적으로 사용되 는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 이하에서는 먼저 통상의 기술자의 이해를 돕기 위해 본 발명이 착안된 배경에 대해 설명한 후, 본 발명에 대해 상세히 설명하도록 한다.최근 들어 자율 로봇 제어 시스템의 구축을 위한 하드웨어 및 소프트웨어 요소 기술들이 급격히 발전하고 있다. 하드웨어 측면으로는, 자율 로봇 제어를 위해 필수 요소 중 하나인 인지를 담당하는 RGB-D, Lidar 등 센서들의 고성능화, 소형화, 경량화, 저전력화 및 저비용화 등 전방위적으로 센서 기술 발전이 이루어지고 있다. 또한, 자율 로봇 제어를 위한 컴퓨팅 파워를 담당할 임베디드 보드(embedded board) 또한 엔비디아 젯슨(Nvidia Jetson)의 등장으로 성능이 비약적으로 상승하고 있다. 소프트웨어 측면으로는, 센서 등을 통해 인지된 주변 상황에서 유의미한 정보 추출을 위한 컴퓨터 비전, 자연어 처리 그리고 인지된 주변 상황을 기반으로 제어 명령을 자율적으로 생성할 수 있는 강화학습 등 인공지능 기술 연구가 폭발적으로 이루어지며, 인공지능 기술이 산업 전반으로 확대되고 있다. 이러한 요소 기술들의 발전은 자율 로봇 제어 시스템의 발전을 가속화하며, 이에 따라 응용분야가 지속적으로 확대되고 있다. 특히, 자율 로봇 제어 기술을 통해 사람이 직접 또는 원격으로 조종할 필요 없이 로봇 제어 목 적을 달성할 수 있기 때문에, 재난 환경에서의 요구조자의 탐색, 산불 감지 및 긴급 통신망 구축 등의 응용분야 도 각광받고 있다. 자율 로봇 제어를 위한 다양한 인공지능 기술 중 심층 강화학습은 로봇 에이전트(이하, 에이전트라 한다)의 행 동을 결정하는 에이전트의 제어 지능인 정책 함수를 심층 신경망으로 근사한 후, 근사화된 신경망을 에이전트의 제어 목적에 맞게 최적화하는 학습 방식이다. 일반적으로 심층 강화학습은 에이전트가 주어진 환경에서 다양한 시행 착오를 하며 수집되는 경험 데이터에 기반하여 최적 정책 함수를 학습해 나가는 방식으로, 사람이 제어 규 칙을 일일이 지정할 필요 없이 에이전트 스스로 제어 규칙을 학습할 수 있지만 학습에 많은 시간과 컴퓨팅 파워 가 필요하다는 단점이 있다. 특히, 상황이 변할 때마다 변화된 상황에서 에이전트를 학습시켜야 하는데, 이 경우 많은 시간과 컴퓨팅 파워가 소요되며, 이는 결국 심층 강화학습 기반 에이전트의 실적용을 어렵게 하는 주 원인 중 하나이다. 이때, 상황이 변화는 예로는 대표적으로 센서 종류 변경, 센서 해상도 변경, 센서 전처리 기법 변경, 제어 목적(즉, 임무)의 변경이 있다. 한편, 에이전트의 학습 속도를 높이기 위해서 커리큘럼 학습(curriculum learning), 모방 학습(imitation learning) 및 오프라인 강화학습(offline reinforcement learning) 등의 기술들이 연구되고 있으나, 해당 기술 들은 상황이 변하지 않는 것을 가정한다. 즉, 상황이 변하지 않을 때 에이전트를 어떻게 학습시킬 수 있을 지가 주 목적이다. 여기에서, 커리큘럼 학습은 사람 학습 방식을 모방한 것으로 에이전트에게 최종 목표를 달성하기 위한 커리큘럼 을 제공하는 학습 방법이다. 모방 학습은 전문가가 어떻게 목표를 해결하는 지를 에이전트에게 제공하고, 에이 전트가 이를 따라하며 배울 수 있도록 하는 것이다. 이때, 전문가와 학습 대상 에이전트는 동일한 문제를 해결 하는 것을 가정한다. 또한, 오프레인 강화학습은 학습된 에이전트로부터 데이터를 획득하여 학습 대상 에이전트 에게 제공함으로써 에이전트의 학습을 촉진하는 방식이다. 본 발명의 일 실시예는 종래 기술과 달리, 변경된 상황에 맞는 에이전트를 학습시킬 때 기존 상황에서 학습된 에이전트를 이용하는 전이 학습 기반의 학습 방식을 제안한다. 이를 통해, 본 발명의 일 실시예는 변화된 상황 에서 에이전트를 처음부터 학습시킬 때 필요한 시간 대비 현저히 적은 시간으로 에이전트 학습이 가능하다는 장 점이 있다. 실제 환경에서 해결해야 하는 문제는 다양하므로, 각 문제마다 에이전트를 처음부터 별개로 학습시키기에는 많 은 학습 시간 및 컴퓨팅 자원이 소요되는 효용성 측면의 문제가 있었으나, 본 발명의 일 실시예에 의하면 기존 문제에서 학습된 에이전트의 지식을 신규 에이전트로 전이하는 방식을 적용하는 것을 통해, 심층 강화학습 기반 에이전트의 실적용을 촉진시킬 수 있다. 이하에서는 도 1을 참조하여 본 발명의 일 실시예에 따른 전이학습 기반의 에이전트 학습 시스템(100, 이하 에 이전트 학습 시스템)를 설명하도록 한다. 도 1은 본 발명의 일 실시예에 따른 에이전트 학습 시스템을 설명하기 위한 도면이다. 본 발명의 일 실시예에 따른 에이전트 학습 시스템는 입력부, 통신부, 표시부, 메모리 및 프로세서를 포함한다. 입력부는 에이전트 학습 시스템의 입력에 대응하여 입력 데이터를 발생시킨다. 이때의 입력은 사용자 입력일 수 있으며, 사용자 입력은 에이전트 학습 시스템이 처리하고자 하는 데이터에 관한 사용자 입력을 포함할 수 있다. 입력부는 적어도 하나의 입력수단을 포함한다. 입력부는 키보드(key board), 키패드 (key pad), 돔 스위치(dome switch), 터치패널(touch panel), 터치 키(touch key), 마우스(mouse), 메뉴 버튼 (menu button) 등을 포함할 수 있다. 통신부는 내부 구성 간의 데이터를 송수신하거나, 외부 서버 등 외부장치와의 통신을 수행한다. 즉, 통신 부는 기존 환경에 적용된 에이전트의 관측값, 또는 학습 대상 에이전트(이하, 타겟 에이전트)가 적용되기 위한 환경에서의 관측값을 입력받거나, 기타 필요한 데이터를 송수신할 수 있다. 이와 같은 통신부는 유선 통신 모듈 및 무선 통신 모듈을 모두 포함할 수 있다. 유선 통신 모듈은 전력선 통신 장치, 전화선 통신 장치, 케이블 홈(MoCA), 이더넷(Ethernet), IEEE1294, 통합 유선 홈 네트워크 및 RS-485 제어 장치로 구현될 수 있다. 또한, 무선 통신 모듈은 WLAN(wireless LAN), Bluetooth, HDR WPAN, UWB, ZigBee, Impulse Radio, 60GHz WPAN, Binary-CDMA, 무선 USB 기술 및 무선 HDMI 기술, 그밖에 5G(5th generation communication), LTE-A(long term evolution-advanced), LTE(long term evolution), Wi-Fi(wireless fidelity) 등의 기능을 구현하기 위한 모듈로 구성될 수 있다. 표시부는 에이전트 학습 시스템의 동작에 따른 표시 데이터를 표시한다. 표시부는 액정 디스플 레이(LCD; liquid crystal display), 발광 다이오드(LED; light emitting diode) 디스플레이, 유기 발광 다이 오드(OLED; organic LED) 디스플레이, 마이크로 전자기계 시스템(MEMS; micro electro mechanical systems) 디 스플레이 및 전자 종이(electronic paper) 디스플레이를 포함한다. 표시부는 입력부와 결합되어 터치 스크린(touch screen)으로 구현될 수 있다. 메모리는 제1 환경 조건에서 기 학습된 에이전트(이하, 소스 에이전트) 및 상기 소스 에이전트에 기반하여 상기 제1 환경 조건과 상이한 제2 환경 조건에서 학습할 에이전트(이하, 타겟 에이전트)의 학습을 위한 프로그 램들을 저장한다. 여기에서, 메모리는 전원이 공급되지 않아도 저장된 정보를 계속 유지하는 비휘발성 저 장장치 및 휘발성 저장장치를 통칭하는 것이다. 예를 들어, 메모리는 콤팩트 플래시(compact flash; CF) 카드, SD(secure digital) 카드, 메모리 스틱(memory stick), 솔리드 스테이트 드라이브(solid-state drive; SSD) 및 마이크로(micro) SD 카드 등과 같은 낸드 플래시 메모리(NAND flash memory), 하드 디스크 드라이브 (hard disk drive; HDD) 등과 같은 마그네틱 컴퓨터 기억 장치 및 CD-ROM, DVD-ROM 등과 같은 광학 디스크 드 라이브(optical disc drive) 등을 포함할 수 있다. 프로세서는 프로그램 등 소프트웨어를 실행하여 에이전트 학습 시스템의 적어도 하나의 다른 구성요 소(예: 하드웨어 또는 소프트웨어 구성요소)를 제어할 수 있고, 다양한 데이터 처리 또는 연산을 수행할 수 있 다. 프로세서는 프로그램을 실행시킴에 따라, 학습 데이터를 구성하고, 사전학습 및 심층 강화학습을 수행한다. 이하에서는 도 2 내지 도 8을 참조하여 본 발명의 일 실시예에 따른 에이전트 학습 시스템에 의해 수행되 는 전이학습 기반의 에이전트 학습 방법에 대해 설명하도록 한다. 도 2는 본 발명의 일 실시예에 따른 전이학습 기반의 에이전트 학습 방법의 순서도이다. 도 3은 본 발명의 일 실시예에 따른 전이학습 기반의 에이전트 학습 방법의 전체 흐름을 나타내는 알고리즘을 도시한 도면이다. 본 발명의 일 실시예는 제1 환경 조건에서 기 학습된 소스 에이전트를 준비하는 단계(S110)와, 상기 소스 에이 전트를 이용하여 상기 제1 환경 조건과 상이한 제2 환경 조건에서 학습할 타겟 에이전트의 학습을 위한 학습 데 이터를 획득하는 단계(S120)와, 상기 학습 데이터를 기반으로 상기 타겟 에이전트를 사전 학습시키는 단계 (S130) 및 상기 사전 학습된 타겟 에이전트를 대상으로 상기 제2 환경 조건에서 심층 강화학습 기반의 학습을 수행하는 단계(S140)를 포함하여 수행된다. 본 발명에서는 기존 문제(제1 환경 조건)는 소스(Source)로 나타내 고, 신규 문제(제2 환경 조건)는 타겟(Target)으로 나타내도록 한다. 본 발명의 일 실시예에서 심층 강화학습 기반 에이전트는 가중치 로 구성된 정책 신경망(policy network, )과 가중치 로 구성된 가치 신경망(value network, )로 구성된다. 정책 신경망은 관측값 에 대한 최적 행동값 를 결정하는 신경망이며 에이전트는 최적 행동값에 따라 행동하게 된다. 가치 신경망은 심층 강화학습 기반 에이전트의 학습에 필요한 신경망으로 현재 상태에 대한 가치 를 나타낸다.이러한 본 발명의 일 실시예에 따른 전이학습 기반의 에이전트 학습 방법은 일 예로, 드론에서 깊이(Depth) 카 메라를 이용하여 장애물을 회피하며 목표를 찾는 기존 문제(제1 환경 조건)에서 학습된 에이전트를, RGB-D 카메 라를 이용하여 장애물을 회피하며 목표를 찾는 새로운 문제(제2 환경 조건)에 적용 가능하도록 심층 강화학습하 는 것을 목적으로 한다. 기존 방식으로는 에이전트의 입력이 깊이 영상에서 RGB-D 영상으로 변경되었기 때문에 처음부터 다시 에이전트 를 학습하여야 했으나, 본 발명의 일 실시예에 따르면 깊이 카메라 기반으로 학습된 에이전트를 이용하여 RGB-D 카메라 기반으로 동작하는 에이전트를 빠르게 학습시킬 수 있다. 도 4는 본 발명의 일 실시예에서 학습 데이터를 획득하는 과정을 설명하기 위한 순서도이다. 도 5는 본 발명의 일 실시예에서 학습 데이터를 획득하는 과정을 나타내는 알고리즘을 도시한 도면이다. 일 실시예로, 본 발명은 소스 에이전트의 준비가 완료되면, 소스 에이전트를 이용하여 타겟 에이전트를 학습시 키기 위한 학습 데이터를 획득한다(S120). 이때, 본 발명의 일 실시예는 에이전트가 상호작용하는 환경 조건을 수정하여, 제1 환경 조건을 대상으로 기 수 집되는 소스 에이전트를 위한 제1 관측값( )에 더하여 타겟 에이전트를 위한 제2 관측값( )을 상태 값( )에 포함하여 반환되도록 한다(S121). 일 예로, 기존에 깊이 카메라 기반의 경우 제1 관측값으로 깊이 카메라 정보만을 반환하였으나, 이를 수정하여 RGB-D 정보도 함께 반환되도록 한다. 그 다음, 제1 환경 조건에서 소스 에이전트를 동작시킨다(S122). 즉, 소스 에이전트는 제1 환경 조건에서 를 이용하여 소스 에이전트가 학습한 기존 문제를 해결하도록 한다. 그 다음, 제1 환경 조건에서의 동작 결과를 기반으로 타겟 에이전트의 학습을 위한 학습 데이터( )를 수집한다(S123). 여기에서 는 정책 신경망에서의 제1 관측값에 대한 최적 행동값을 나타내며, 는 가치 신경망에서의 제1 관측 값에 대한 현 상태의 가치값을 나타낸다. 본 발명의 일 실시예에서 최적 행동값과 가치값은 시간에 따른 함수값 ( )으로 표현될 수 있다. 도 6은 본 발명의 일 실시예에서 사전 학습 과정을 설명하기 위한 순서도이다. 도 7은 본 발명의 일 실시예에서 사전 학습 과정을 나타내는 알고리즘을 도시한 도면이다. 일 실시예로, 본 발명은 학습 데이터의 구성이 완료되면, 학습 데이터를 기반으로 타겟 에이전트를 학습시킨다 (S130). 이를 위해, 본 발명의 일 실시예는 타겟 에이전트의 정책 신경망( )과 가치 신경망( 을 초기화시킨 다(S131). 그 다음, 초기화된 정책 신경망 및 가치 신경망을 대상으로 지도학습 기반의 학습을 위한 학습 데이터를 설정한 다(S132). 이때, 정책 신경망을 위한 학습 데이터는 ( )이고, 가치 신경망을 위한 학습 데이터는 ( )이다. 그 다음, 학습 데이터를 기반으로 정책 신경망 및 가치 신경망의 각 손실함수 계산 및 가중치 갱신 과정을 반복 수행하여 학습한다(S133). 이때, 정책 신경망의 손실함수는 식 1과 같이 나타낼 수 있으며, 가치 신경망의 손실함수는 식 2와 같이 나타낼 수 있다.[식 1]"}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "[식 2]"}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "한편, 사전 학습 과정은 각 손실함수 값의 감소 정도에 따른 학습 조기 종료 조건(Early Stopping)을 만족하는 것과 사전 정의된 반복 횟수 조건 중 적어도 하나를 포함하는 학습 종료 조건을 만족할 때까지 반복하여 수행될 수 있다. 도 8은 본 발명의 일 실시예에서 심층 강화학습 과정을 나타내는 알고리즘을 도시한 도면이다. 일 실시예로, 본 발명은 사전 학습된 타겟 에이전트를 대상으로 제2 환경 조건에서 심층 강화학습을 수행한다 (S140). 이때, 본 발명은 소정의 학습 조건( )이 만족될 때까지 강화학습 기반으로 타겟 에이전트 ( )를 학습할 수 있다. 일 예로, 소정의 학습 조건은 제2 환경 조건에 상응하는 문제 해 결 성공률 및 사전 정의된 반복 횟수 중 적어도 하나를 포함할 수 있다. 한편, 상술한 설명에서, 단계 S110 내지 단계 S140은 본 발명의 구현예에 따라서, 추가적인 단계들로 더 분할되 거나, 더 적은 단계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있다. 아울러, 기타 생략된 내용이라 하더라도 도 1에 기술된 내용과 도 2 내지 도 8에 기술된 내 용은 각각 상호 적용될 수 있다. 도 9는 종래 기술에 따른 학습 소요 시간을 나타낸 도면이다. 도 10은 본 발명의 일 실시예에서의 학습 소요 시 간을 나타낸 도면이다. 도 9를 참조하면, 종래 기술은 새로운 문제의 해결을 위해서는 처음부터 강화학습 기반으로 타겟 에이전트를 학 습해야 하며, 이 경우 총 159.188시간이 학습에 소요되었다. 도 10을 참조하면, 본 발명의 일 실시예를 적용하여 타겟 에이전트를 학습할 경우 총 4.855시간이 소요됨을 확 인할 수 있으며, 이는 종래기술 대비 약 96.95%의 학습 시간이 감소한 것을 확인할 수 있다. 이상에서 전술한 본 발명의 일 실시예에 따른 전이학습 기반의 에이전트 학습 방법은, 하드웨어인 컴퓨터와 결 합되어 실행되기 위해 프로그램(또는 어플리케이션)으로 구현되어 매체에 저장될 수 있다. 상기 전술한 프로그램은, 상기 컴퓨터가 프로그램을 읽어 들여 프로그램으로 구현된 상기 방법들을 실행시키기 위하여, 상기 컴퓨터의 프로세서(CPU)가 상기 컴퓨터의 장치 인터페이스를 통해 읽힐 수 있는 C, C++, JAVA, Ruby, 기계어 등의 컴퓨터 언어로 코드화된 코드(Code)를 포함할 수 있다. 이러한 코드는 상기 방법들을 실행하 는 필요한 기능들을 정의한 함수 등과 관련된 기능적인 코드(Functional Code)를 포함할 수 있고, 상기 기능들 을 상기 컴퓨터의 프로세서가 소정의 절차대로 실행시키는데 필요한 실행 절차 관련 제어 코드를 포함할 수 있 다. 또한, 이러한 코드는 상기 기능들을 상기 컴퓨터의 프로세서가 실행시키는데 필요한 추가 정보나 미디어가 상기 컴퓨터의 내부 또는 외부 메모리의 어느 위치(주소 번지)에서 참조되어야 하는지에 대한 메모리 참조관련 코드를 더 포함할 수 있다. 또한, 상기 컴퓨터의 프로세서가 상기 기능들을 실행시키기 위하여 원격(Remote)에 있는 어떠한 다른 컴퓨터나 서버 등과 통신이 필요한 경우, 코드는 상기 컴퓨터의 통신 모듈을 이용하여 원격에 있는 어떠한 다른 컴퓨터나 서버 등과 어떻게 통신해야 하는지, 통신 시 어떠한 정보나 미디어를 송수신해야 하 는지 등에 대한 통신 관련 코드를 더 포함할 수 있다. 상기 저장되는 매체는, 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체가 아니라 반 영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 구체적으로는, 상기 저 장되는 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 데이터 저장장치 등이 있지만, 이에 제한되지 않는다. 즉, 상기 프로그램은 상기 컴퓨터가 접속할 수 있는 다양한 서버 상의 다양한 기록매체 또는사용자의 상기 컴퓨터상의 다양한 기록매체에 저장될 수 있다. 또한, 상기 매체는 네트워크로 연결된 컴퓨터 시 스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장될 수 있다."}
{"patent_id": "10-2022-0188249", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "전술한 본 발명의 설명은 예시를 위한 것이며, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자는 본 발명 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 발명의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함되는 것으 로 해석되어야 한다."}
{"patent_id": "10-2022-0188249", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 에이전트 학습 시스템을 설명하기 위한 도면이다. 도 2는 본 발명의 일 실시예에 따른 전이학습 기반의 에이전트 학습 방법의 순서도이다. 도 3은 본 발명의 일 실시예에 따른 전이학습 기반의 에이전트 학습 방법의 전체 흐름을 나타내는 알고리즘을 도시한 도면이다. 도 4는 본 발명의 일 실시예에서 학습 데이터를 획득하는 과정을 설명하기 위한 순서도이다. 도 5는 본 발명의 일 실시예에서 학습 데이터를 획득하는 과정을 나타내는 알고리즘을 도시한 도면이다. 도 6은 본 발명의 일 실시예에서 사전 학습 과정을 설명하기 위한 순서도이다. 도 7은 본 발명의 일 실시예에서 사전 학습 과정을 나타내는 알고리즘을 도시한 도면이다. 도 8은 본 발명의 일 실시예에서 심층 강화학습 과정을 나타내는 알고리즘을 도시한 도면이다. 도 9는 종래 기술에 따른 학습 소요 시간을 나타낸 도면이다. 도 10은 본 발명의 일 실시예에서의 학습 소요 시간을 나타낸 도면이다."}
