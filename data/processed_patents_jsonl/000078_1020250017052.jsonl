{"patent_id": "10-2025-0017052", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0057702", "출원번호": "10-2025-0017052", "발명의 명칭": "모니터링 정보를 이용한 로드 밸런싱 방법 및 시스템", "출원인": "리벨리온 주식회사", "발명자": "김기정"}}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "AI(Artificial Intelligence, AI) 서비스 제공 시스템에서의 로드 밸런싱 방법에 있어서,복수의 서버들의 로드 밸런싱 서비스 연동 정보를 구성하는 단계;상기 복수의 서버들의 로드 밸런싱 모니터링 정보를 구성하는 단계;유저 디바이스로부터 AI 서비스에 대한 추론 작업 요청 메시지를 획득하는 단계;상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들 중 대상 서버를 도출하는 단계; 및상기 도출된 대상 서버에 상기 AI 서비스의 추론 작업에 대한 로드 밸런싱을 수행하는 단계를 포함하고,상기 로드 밸런싱 모니터링 정보는 상기 복수의 서버들의 추론 작업 처리 현황 정보를 기반으로 구성되는 것을특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 로드 밸런싱 모니터링 정보는 서버의 최대 병렬 처리 가능 추론 작업 수, 현재 처리 추론 작업 수 및 대기추론 작업 수를 포함하는 것을 특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들 중 상기 대상 서버를 도출하는 단계는, 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들이 모두 아이들(idle) 상태인지 여부를 판단하는 단계를 포함하고,상기 아이들 상태는 서버의 현재 처리 추론 작업 수가 0 인 상태인 것을 특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 대상 서버는 상기 복수의 서버들 중 아이들 상태인 서버들이 많아지고, 아이들 상태인 서버가 아이들 상태를 길게 유지하도록 도출되는 것을 특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서, 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들 중 상기 대상 서버를 도출하는 단계는, 상기 복수의 서버들이 모두 아이들 상태인 경우, 상기 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가가장 큰 서버를 상기 대상 서버로 도출하는 단계를 더 포함하는 것을 특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3항에 있어서, 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들 중 상기 대상 서버를 도출하는 단계는, 상기 복수의 서버들 중 적어도 하나의 서버가 아이들 상태가 아닌 경우, 동작 서버 중 처리 가능 서버가 존재하는지 판단하는 단계를 더 포함하고,공개특허 10-2025-0057702-3-상기 동작 서버는 현재 처리 추론 작업 수가 1 이상인 서버이고, 상기 처리 가능 서버는 현재 처리 가능 추론작업 수가 1 이상인 서버인 것을 특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 현재 처리 가능 추론 작업 수는 서버의 최대 병렬 처리 가능 추론 작업 수에서 현재 처리 추론 작업 수를뺀 값으로 도출되는 것을 특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서, 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들 중 상기 대상 서버를 도출하는 단계는, 상기 동작 서버 중 상기 처리 가능 서버가 존재하는 경우, 상기 처리 가능 서버에서 현재 처리 가능 추론 작업수가 가장 작은 서버를 상기 대상 서버로 도출하는 단계를 더 포함하는 것을 특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들 중 상기 대상 서버를 도출하는 단계는, 상기 동작 서버 중 상기 처리 가능 서버가 존재하지 않고 모두 혼잡 서버인 경우, 상기 복수의 서버들에서 대기추론 작업 수가 가장 작은 서버를 상기 대상 서버로 도출하는 단계를 더 포함하고,상기 혼잡 서버는 현재 처리 가능 추론 작업 수가 0인 서버인 것을 특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서, 상기 복수의 서버들의 상기 추론 작업 처리 현황 정보는 로드 밸런싱 모니터링 연동 정보의 모니터링 네트워크를 통하여 전달되는 것을 특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 로드 밸런싱 모니터링 연동 정보는 서버의 모니터링 IP 주소 및 모니터링 포트 번호를 포함하는 것을 특징으로 하는 로드 밸런싱 방법."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "로드 밸런서 및 복수의 서버들을 포함한 데이터 센터; 및상기 데이터 센터와 네트워크로 연결된 유저 디바이스를 포함하되, 상기 로드 밸런서는 상기 복수의 서버들의 로드 밸런싱 서비스 연동 정보를 구성하고, 상기 복수의 서버들의 로드 밸런싱 모니터링 정보를 구성하고, 상기 유저 디바이스로부터 AI 서비스에 대한 추론 작업 요청 메시지를 획득하고, 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들 중 대상 서버를 도출하고, 상기 도출된 대상 서버에 상기 AI 서비스의 추론 작업에 대한 로드 밸런싱을 수행하고, 상기 추론 작업이 분배된 대상 서버로부터 전달된 추론 작업 결과를 상기 유저 디바이스로 전달하고, 상기 로드 밸런싱 모니터링 정보는 상기 복수의 서버들의 추론 작업 처리 현황 정보를 기반으로 구성되는 것을특징으로 하는 AI 서비스 제공 시스템."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,공개특허 10-2025-0057702-4-상기 로드 밸런싱 모니터링 정보는 서버의 최대 병렬 처리 가능 추론 작업 수, 현재 처리 추론 작업 수 및 대기추론 작업 수를 포함하는 것을 특징으로 하는 AI 서비스 제공 시스템."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 로드 밸런서는 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들이 모두 아이들(idle) 상태인지 여부를 판단하고,상기 아이들 상태는 서버의 현재 처리 추론 작업 수가 0 인 상태인 것을 특징으로 하는 AI 서비스 제공 시스템."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 상기 로드 밸런서는 상기 복수의 서버들 중 아이들 상태인 서버들이 많아지고, 아이들 상태인 서버가 아이들 상태를 길게 유지하도록 상기 대상 서버를 도출하는 것을 특징으로 하는 AI 서비스 제공 시스템."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서, 상기 복수의 서버들이 모두 아이들 상태인 경우, 상기 로드 밸런서는 상기 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 서버를 상기 대상 서버로 도출하는 것을 특징으로 하는 AI 서비스 제공 시스템."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제14항에 있어서, 상기 복수의 서버들 중 적어도 하나의 서버가 아이들 상태가 아닌 경우, 상기 로드 밸런서는 동작 서버 중 처리가능 서버가 존재하는지 판단하고,상기 동작 서버는 현재 처리 추론 작업 수가 1 이상인 서버이고, 상기 처리 가능 서버는 현재 처리 가능 추론작업 수가 1 이상인 서버인 것을 특징으로 하는 AI 서비스 제공 시스템."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서, 상기 현재 처리 가능 추론 작업 수는 서버의 최대 병렬 처리 가능 추론 작업 수에서 현재 처리 추론 작업 수를뺀 값으로 도출되는 것을 특징으로 하는 AI 서비스 제공 시스템."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제17항에 있어서, 상기 동작 서버 중 상기 처리 가능 서버가 존재하는 경우, 상기 로드 밸런서는 상기 처리 가능 서버에서 현재처리 가능 추론 작업 수가 가장 작은 서버를 상기 대상 서버로 도출하는 것을 특징으로 하는 AI 서비스 제공 시스템."}
{"patent_id": "10-2025-0017052", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서, 상기 동작 서버 중 상기 처리 가능 서버가 존재하지 않고 모두 혼잡 서버인 경우, 상기 로드 밸런서는 상기 복수의 서버들에서 대기 추론 작업 수가 가장 작은 서버를 상기 대상 서버로 도출하고, 상기 혼잡 서버는 현재 처리 가능 추론 작업 수가 0인 서버인 것을 특징으로 하는 AI 서비스 제공 시스템."}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 명세서는 로드 밸런싱 모니터링 정보를 이용한 로드 밸런싱 방법 및 AI 서비스 제공 시스템에 관한 것이다. 본 명세서에 따른 로드 밸런싱 방법은 복수의 서버들의 로드 밸런싱 서비스 연동 정보를 구성하는 단계, 상기 복 수의 서버들의 로드 밸런싱 모니터링 정보를 구성하는 단계, 유저 디바이스로부터 AI 서비스에 대한 추론 작업 요청 메시지를 획득하는 단계, 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들 중 대상 서버를 도출하는 단계, 및 상기 도출된 대상 서버에 상기 AI 서비스의 추론 작업에 대한 로드 밸런싱을 수행하는 단계를 포함하고, 상기 로드 밸런싱 모니터링 정보는 상기 복수의 서버들의 추론 작업 처리 현황 정보를 기반으로 구성 되는 것을 특징으로 한다."}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 문서는 AI 서비스 제공 시스템에서의 로드 밸런싱 모니터링 정보를 이용한 로드 밸런싱 방법 및 시스템 등에 관한 것이다."}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 (Artificial Intelligence, AI) 기술의 발달로 이를 활용한 AI 서비스가 보편화되고 있으며, AI 서비 스를 제공하기 위한 복수의 백엔드 서버들을 포함한 데이터 센터가 구축되고 있다. 하지만, 다양한 AI 서비스를 제공하기 위하여 대규모의 백엔드 서버들을 포함하는 데이터 센터가 구축되면서 대 량의 전력 소모량이 발생할 수 있다. 이에 따라, 대규모의 서버들에서 소모되는 전력을 효율적으로 관리하여 데이터 센터의 전체적인 전력 소모 에너 지를 절감하기 위한 요구가 발생하고 있다."}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 문서의 기술적 과제는 상기와 같은 문제점을 해결하기 위한 로드 밸런싱 모니터링 정보를 이용하여 AI 가속 기의 병렬 처리 전력 소모 특성을 고려한 로드 밸런싱 방법 및 AI 서비스 제공 시스템을 제공하는 것에 있다."}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위하여, 본 문서의 일 실시예에 따른 로드 밸런싱 방법은 복수의 서버들의 로드 밸런싱 서비스 연동 정보를 구성하는 단계, 상기 복수의 서버들의 로드 밸런싱 모니터링 정보를 구성하는 단계, 유저 디바이스로부터 AI 서비스에 대한 추론 작업 요청 메시지를 획득하는 단계, 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들 중 대상 서버를 도출하는 단계, 및 상기 도출된 대상 서버에 상기 AI 서비스의 추 론 작업에 대한 로드 밸런싱을 수행하는 단계를 포함하고, 상기 로드 밸런싱 모니터링 정보는 상기 복수의 서버 들의 추론 작업 처리 현황 정보를 기반으로 구성되는 것을 특징으로 한다. 본 문서의 다른 일 실시예에 따른 AI 서비스 제공 시스템은 로드 밸런서 및 복수의 서버들을 포함한 데이터 센 터, 및 상기 데이터 센터와 네트워크로 연결된 유저 디바이스를 포함하되, 상기 로드 밸런서는 상기 복수의 서 버들의 로드 밸런싱 서비스 연동 정보를 구성하고, 상기 복수의 서버들의 로드 밸런싱 모니터링 정보를 구성하 고, 상기 유저 디바이스로부터 AI 서비스에 대한 추론 작업 요청 메시지를 획득하고, 상기 로드 밸런싱 모니터 링 정보를 기반으로 상기 복수의 서버들 중 대상 서버를 도출하고, 상기 도출된 대상 서버에 상기 AI 서비스의 추론 작업에 대한 로드 밸런싱을 수행하고, 상기 추론 작업이 분배된 대상 서버로부터 전달된 추론 작업 결과를 상기 유저 디바이스로 전달하고, 상기 로드 밸런싱 모니터링 정보는 상기 복수의 서버들의 추론 작업 처리 현황 정보를 기반으로 구성되는 것을 특징으로 한다."}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 문서의 실시예에 따르면 1개의 추론 작업을 처리하는 것과 복수의 추론 작업들을 병렬 처리하는데 소모되는 전력량이 동일하거나 거의 차이가 없는 AI 가속기의 특성을 고려하여 백엔드 서버들에 추론 작업을 분배할 수 있고, 이를 통하여, AI 서비스 제공 시스템의 전력 소모량을 줄이고 전력 효율을 향상시키는 효과를 발생시킬 수 있다. 본 문서의 실시예에 따르면 AI 서비스 제공 시스템의 복수의 서버들 중 idle 상태인 서버들이 많아지고, idle 상태인 서버가 idle 상태를 길게 유지하도록 로드 밸런싱을 수행할 수 있고, 이를 통하여, AI 서비스 제공 시스 템의 전력 소모량을 줄이고 전력 효율을 향상시키는 효과를 발생시킬 수 있다. 본 문서의 실시예에 따르면 서버들로부터 주기적으로 추론 작업 처리 현황 정보를 전달받아 로드 밸런싱을 위한 로드 밸런싱 모니터링 테이블을 자동 업데이트할 수 있고, 이를 통하여, AI 서비스 제공 시스템의 전력 소모량 을 줄이고 전력 효율을 향상시키는 효과를 발생시킬 수 있다."}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 실시를 위한 구체적인 내용을 첨부된 도면을 참조하여 상세히 설명한다. 다만, 이하의 설명에 서는 본 개시의 요지를 불필요하게 흐릴 우려가 있는 경우, 널리 알려진 기능이나 구성에 관한 구체적 설명은 생략하기로 한다. 첨부된 도면에서, 동일하거나 대응하는 구성요소에는 동일한 참조부호가 부여되어 있다. 또한, 이하의 실시예 들의 설명에 있어서, 동일하거나 대응되는 구성요소를 중복하여 기술하는 것이 생략될 수 있다. 그러나, 구성 요소에 관한 기술이 생략되어도, 그러한 구성요소가 어떤 실시예에 포함되지 않는 것으로 의도되지는 않는다. 개시된 실시예의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 후술되어 있는 실시예 들을 참조하면 명확해질 것이다. 그러나, 본 개시는 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 개시가 완전하도록 하고, 본 개시가 통상의 기술 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것일 뿐이다. 본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 개시된 실시예에 대해 구체적으로 설명하기로 한다. 본 명세서에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 관련 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상 세히 그 의미를 기재할 것이다. 따라서, 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어 가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서에서의 단수의 표현은 문맥상 명백하게 단수인 것으로 특정하지 않는 한, 복수의 표현을 포함한다. 또 한, 복수의 표현은 문맥상 명백하게 복수인 것으로 특정하지 않는 한, 단수의 표현을 포함한다. 명세서 전체에 서 어떤 부분이 어떤 구성요소를 포함한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제 외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에서 사용되는 '모듈' 또는 '부'라는 용어는 소프트웨어 또는 하드웨어 구성요소를 의미하며, '모 듈' 또는 '부'는 어떤 역할들을 수행한다. 그렇지만, '모듈' 또는 '부'는 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. '모듈' 또는 '부'는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이 상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서, '모듈' 또는 '부'는 소프트웨어 구성요 소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세 스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로 코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들 또는 변수들 중 적어도 하나를 포함할 수 있다. 구성요소들과 '모듈' 또는 '부'들은 안에서 제공되는 기능은 더 작은 수의 구성요소들 및 '모듈' 또는 '부'들로 결합되거나 추가적인 구성요소들과 '모듈' 또는 '부'들로 더 분리될 수 있다. 본 개시의 일 실시예에 따르면, '모듈' 또는 '부'는 프로세서 및 메모리로 구현될 수 있고, 회로(circuit, circuitry)로 구현될 수 있다. '회로(circuit, circuitry)'와 같은 용어는 하드웨어 상의 회로를 의미하기도 하 지만 소프트웨어 상의 회로를 의미할 수도 있다. '프로세서'는 범용 프로세서, 중앙 처리 장치(CPU), 마이크로 프로세서, 디지털 신호 프로세서(DSP), 제어기, 마이크로 제어기, 상태 머신 등을 포함하도록 넓게 해석되어야 한다. 몇몇 환경에서, '프로세서'는 주문형 반도체(ASIC), 프로그램가능 로직 디바이스(PLD), 필드 프로그램 가 능 게이트 어레이(FPGA) 등을 지칭할 수도 있다. '프로세서'는, 예를 들어, DSP와 마이크로 프로세서의 조합, 복수의 마이크로 프로세서들의 조합, DSP 코어와 결합한 하나 이상의 마이크로 프로세서들의 조합, 또는 임의의 다른 그러한 구성들의 조합과 같은 처리 디바이스들의 조합을 지칭할 수도 있다. 또한, '메모리'는 전자 정보를 저장 가능한 임의의 전자 컴포넌트를 포함하도록 넓게 해석되어야 한다. '메모리'는 임의 액세스 메모리 (RAM), 판독-전용 메모리(ROM), 비-휘발성 임의 액세스 메모리(NVRAM), 프로그램가능 판독-전용 메모리(PROM), 소거-프 로그램가능 판독 전용 메모리(EPROM), 전기적으로 소거가능 PROM(EEPROM), 플래쉬 메모리, 자기 또는 마킹 데이 터 저장 장치, 레지스터들 등과 같은 프로세서-판독가능 매체의 다양한 유형들을 지칭할 수도 있다. 프로세서가 메모리로부터 정보를 판독하고/하거나 메모리에 정보를 기록할 수 있다면 메모리는 프로세서와 전자 통신 상태 에 있다고 불린다. 프로세서에 집적된 메모리는 프로세서와 전자 통신 상태에 있다. 본 개시에서, '복수의 A의 각각' 또는 '복수의 A 각각'은 복수의 A에 포함된 모든 구성 요소의 각각을 지칭하거 나, 복수의 A에 포함된 일부 구성 요소의 각각을 지칭할 수 있다. 또한, 이하의 실시예들에서 사용되는 제1, 제2, A, B, (a), (b) 등의 용어는 어떤 구성요소를 다른 구성요소와 구별하기 위해 사용되는 것일 뿐, 그 용어에 의해 해당 구성요소의 본질이나 차례 또는 순서 등이 한정되지는 않는다. 또한, 이하의 실시예들에서, 어떤 구성요소가 다른 구성요소에 '연결', '결합' 또는 '접속'된다고 기재된 경우, 그 구성요소는 그 다른 구성요소에 직접적으로 연결되거나 또는 접속될 수 있지만, 각 구성요소 사이에 또 다른 구성요소가 '연결', '결합' 또는 '접속'될 수도 있다고 이해되어야 한다. 또한, 이하의 실시예들에서 사용되는 '포함한다(comprises)' 및/또는 '포함하는(comprising)'은 언급된 구성요 소, 단계, 동작 및/또는 소자는 하나 이상의 다른 구성요소, 단계, 동작 및/또는 소자의 존재 또는 추가를 배제 하지 않는다. 또한, 이하의 실시예들에서, '미만인지 여부를 판정' 또는 '미만인 경우' 등을 개시하고 있으나, '이하인지 여 부를 판정' 또는 '이하인 경우' 등도 해당 실시예들에 적용될 수 있다. 본 개시의 다양한 실시예들을 설명하기에 앞서, 사용되는 용어에 대하여 설명하기로 하기로 한다. 본 개시에서, '인스트럭션(instruction)'은 기능을 기준으로 묶인 일련의 컴퓨터 판독가능 명령어들로서 컴퓨터 프로그램의 구성요소이자 프로세서에 의해 실행되는 것을 지칭할 수 있다. 본 개시에서, '네트워크'는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN) 또는 부가가치 통신망(Value Added Network; VAN) 등과 같은 유선 네트워크나 이동 통신망(mobile radio communication network) 또는 위성 통신망 등과 같은 모든 종류의 무선 네트워크로 구현될 수 있다. 도 1은 본 개시의 일 실시예에 따른 복수의 서버들을 포함하는 데이터 센터를 이용한 AI 서비스 제공 시스템의 실시예를 나타낸 블록도이다. 도 1을 참조하면 AI 서비스 제공 시스템은 유저 디바이스, 인터넷 및/또 는 데이터 센터를 포함할 수 있다. 상기 데이터 센터는 로드 밸런싱 장치(load balancing device) 및 적어도 하나의 서버를 포함할 수 있다. 도 1은 상기 데이터 센터가 4개의 서버들을 포함하는 것으로 도시되어 있으나, 이에 한정되지 않으며, 상이하거나 더 많은 수의 서버들을 포함하도록 구성될 수도 있다. 상기 데이터 센터의 서버는 인공지능(Artificial Intelligence, AI) 서비스를 제공하기 위해 인공신경망을 이용하여 연 산을 수행하는 뉴럴 프로세싱 장치(Neural Processing Unit, NPU)를 포함한 AI 가속기를 포함할 수 있다. 상기 서버는 백엔드 서버(backend server)라고 불릴 수도 있다. 도 1을 참조하면 유저 디바이스는 인터넷 등의 네트워크를 통하여 로드 밸런싱 장치와 연결될 수 있다. 상기 로드 밸런싱 장치는 로드 밸런서 또는 API 게이트웨이일 수 있다. 유저 디바이스는 인터넷 을 통하여 상기 로드 밸런싱 장치로 AI 서비스에 대한 추론 작업을 요청할 수 있다. 즉, 예를 들어, 유저 디바이스는 상기 로드 밸런싱 장치로 상기 AI 서비스에 대한 추론 작업 요청 메시지를 전달할 수 있다. 예를 들어, 상기 추론 작업 요청 메시지는 URL 형식일 수 있다. 상기 URL 형식은 프로토콜(protocol) 식별자, 호스트 주소(host address) 패스(Path) 및/또는 쿼리(query)로 구성될 수 있다. 로드 밸런싱 장치는 상기 서버들로 상기 AI 서비스에 대한 추론 작업에 대한 로드 밸런싱을 수행할 수 있다. 즉, 로드 밸런싱 장치는 상기 서버들로 상기 AI 서비스에 대한 추론 작업을 분배할 수 있다. 예를 들 어, 상기 로드 밸런싱은 기설정된 로드 밸런싱 알고리즘을 기반으로 로드 밸런싱을 수행할 수 있다. 예를 들어, 상기 기설정된 로드 밸런싱 알고리즘은 라운드 로빈(round robin) 알고리즘, 스틱키 라운드 로빈(sticky round robin) 알고리즘, 가중 라운드 로빈(weighted round robin) 알고리즘, IP/URL 해쉬(IP/URL hash) 알고리즘, 최 소 커넥션(least connection) 알고리즘 또는 최소 시간(least time) 알고리즘일 수 있다. 도 2는 로드 밸런싱 알고리즘들을 세부적으로 설명하기 위한 도면이다. 도 2를 참조하면 로드 밸런싱 장치는 로 드 밸런싱 알고리즘에 따라서 로드 밸런싱을 수행할 수 있다. 예를 들어, 도 2의 (a)는 라운드 로빈 알고리즘(round robin algorithm)을 나타낸다. 라운드 로빈 알고리즘은 로드 밸런싱 장치로 전달된 요청들을 요청된 순서대로 서버들에 분배하는 방법일 수 있다. 예를 들어, 도 2의 (a)를 참조하면 로드 밸런싱 장치는 사용자 1 및 사용자 2로부터 전달된 요청 1 내지 요청 4를 서버 A, 서버 B, 서버 C 순서로 분배할 수 있다. 또한, 예를 들어, 도 2의 (b)는 스틱키 라운드 로빈 알고리즘(sticky round robin algorithm)을 나타낸다. 스 틱키 라운드 로빈 알고리즘은 로드 밸런싱 장치로 전달된 요청들을 요청된 순서대로 서버들에 분배하되, 특정 사용자의 요청이 특정 서버로 분배되면 다음 순서의 상기 특정 사용자의 요청도 상기 특정 서버로 분배하는 방 법일 수 있다. 예를 들어, 도 2의 (b)를 참조하면 로드 밸런싱 장치는 사용자 1 로부터 전달된 요청 1 내지 요 청 2를 서버 A로 분배할 수 있고, 사용자 2 로부터 전달된 요청 3 내지 요청 4를 서버 B로 분배할 수 있다. 또한, 예를 들어, 도 2의 (c)는 가중 라운드 로빈 알고리즘(weighted round robin algorithm)을 나타낸다. 가 중 라운드 로빈 알고리즘은 로드 밸런싱 장치로 전달된 요청들을 서버들에 순서대로 분배하되, 가중치들에 따라 서 분배하는 방법일 수 있다. 구체적으로, 가중 라운드 로빈 알고리즘이 적용되는 경우, 가중치가 높은 서버에 우선적으로 요청을 분배하는 방법일 수 있다. 로드 밸런싱 장치는 가중치가 높은 서버에 우선적으로 요청을 분 배하되, 전체 요청들의 개수에서 가중치의 비율만큼의 개수(예를 들어, 총 4개 * 0.8 = 3.2개)까지 상기 서버로 분배할 수 있다. 예를 들어, 도 2의 (c)를 참조하면 서버 A의 가중치는 0.8, 서버 B의 가중치는 0.1, 서버 C의 가중치는 0.1로 설정될 수 있고, 로드 밸런싱 장치는 사용자 1 및 사용자 2로부터 전달된 요청 1 내지 요청 4 중 요청 1 내지 요청 3을 포함하는 3개의 요청들을 서버 A로 분배할 수 있고, 요청 4를 포함하는 1개의 요청은 서버 B로 분배할 수 있다. 또한, 예를 들어, 도 2의 (d)는 IP/URL 해시 알고리즘(IP/URL hash algorithm)을 나타낸다. IP/URL 해시 알고 리즘은 로드 밸런싱 장치로 전달된 요청들을 사용자의 IP/URL에 대한 해시 값에 따라서 분배하는 방법일 수 있 다. 예를 들어, 도 2의 (d)를 참조하면 서버 A가 처리하는 해시 값은 0, 서버 B가 처리하는 해시 값은 1, 서버 C가 처리하는 해시 값은 2로 설정될 수 있고, 사용자 1의 IP/URL에 대한 해시 값은 0, 사용자 2의 IP/URL에 대 한 해시 값은 2로 도출될 수 있다. 이 경우, 로드 밸런싱 장치는 사용자 1 로부터 전달된 요청 1 내지 요청 2를 서버 A로 분배할 수 있고, 사용자 2 로부터 전달된 요청 3 내지 요청 4를 서버 C로 분배할 수 있다. 또한, 예를 들어, 도 2의 (e)는 최소 커넥션 알고리즘(least connections algorithm)을 나타낸다. 최소 커넥션 알고리즘은 로드 밸런싱 장치로 전달된 요청들을 서버들 중 최소 커넥션을 갖는 서버로 분배하는 방법일 수 있 다. 예를 들어, 도 2의 (e)를 참조하면 서버 A의 커넥션은 1000, 서버 B의 커넥션은 100, 서버 C의 커넥션은 10 일 수 있고, 이 경우, 로드 밸런싱 장치는 사용자 1 및 사용자 2로부터 전달된 요청 1 내지 요청 4를 가장 작은 커넥션을 갖는 서버 C로 분배할 수 있다. 또한, 예를 들어, 도 2의 (f)는 최소 시간 알고리즘(least time algorithm)을 나타낸다. 최소 시간 알고리즘은 로드 밸런싱 장치로 전달된 요청들을 서버들 중 최소 응답 시간(response time)을 갖는 서버로 분배하는 방법일 수 있다. 예를 들어, 도 2의 (f)를 참조하면 서버 A의 응답 시간은 100ms, 서버 B의 응답 시간은 10ms, 서버 C 의 응답 시간은 1ms일 수 있고, 이 경우, 로드 밸런싱 장치는 사용자 1 및 사용자 2로부터 전달된 요청 1 내지 요청 4를 가장 작은 응답 시간을 갖는 서버 C로 분배할 수 있다. 한편, 상술한 로드 밸런싱 알고리즘들과 같이, 기존의 로드 밸런싱 방법에 따르면 사용자의 AI 서비스에 대한 추론 작업들은 서버들에 균일하게 분산될 수 있다. 하지만, AI 가속기는 1개의 추론 작업을 병렬로 처리하는 경 우와 복수의 추론 작업들을 병렬로 처리하는 경우의 작업 소요 시간과 소모 전력량이 같은 특성이 있다. 상기 AI 가속기의 특성으로 인해, 기존의 로드 밸런싱 방법과 같이 추론 작업들을 서버들에 분산하는 방법 대신 서버 가 수용 가능한 범위 내에서 추론 작업들을 할당하는 방법이 AI 서비스 제공 시스템의 작업 소요 시간 및 소모전력량을 줄일 수 있다. 도 3a 내지 도 3b는 AI 가속기의 추론 작업 수행 개수에 따른 소모 전력량을 설명하기 위한 도면이다. 도 3a는 AI 가속기에 1개의 추론 작업이 요청된 경우의 소모 전력량을 나타낼 수 있다. 예를 들어, 도 3a를 참 조하면, AI 가속기에 추론 작업이 요청되기 전에는 AI 가속기에서 대기 전력이 소모될 수 있고, 상기 AI 가속기 에 t1 시점에 1개의 추론 작업이 요청된 경우, 상기 AI 가속기는 상기 추론 작업을 처리할 수 있고, 상기 추론 작업이 완료되는 t2 시점까지 상기 AI 가속기에서 액티브(active) 전력이 소모될 수 있다. 도 3b는 AI 가속기에 N개의 추론 작업들이 요청된 경우의 소모 전력량을 나타낼 수 있다. 예를 들어, AI 가속기 에서 최대로 병렬 처리 가능한 추론 작업 개수는 상기 N보다 크거나 같을 수 있다. 이 경우, 도 3b를 참조하면, AI 가속기에 추론 작업이 요청되기 전에는 AI 가속기에서 대기 전력이 소모될 수 있고, 상기 AI 가속기에 t1 시 점에 N개의 추론 작업들이 요청된 경우, 상기 AI 가속기는 상기 N개의 추론 작업들을 병렬로 처리할 수 있고, 상기 추론 작업들이 완료되는 t2 시점까지 상기 AI 가속기에서 액티브(active) 전력이 소모될 수 있다. 상기 도 3a에 도시된 1개의 추론 작업을 처리하는 동안 소모되는 액티브 전력과 상기 도 3b에 도시된 N개의 추 론 작업들을 처리하는 동안 소모되는 액티브 전력은 동일하거나 차이가 아주 작을 수 있다. AI 가속기는 병렬 처리를 고속으로 수행할 수 있는 구조를 가지고 있다. AI 가속기의 핵심 성능 지표는 속도, 처리량, 소모에너지, 발열량, 리소스 크기 등이 존재하며, 그 중에서도 처리량을 늘리기 위해 AI 가속기는 병렬 처리를 수행하기 위한 static batch, continuous batch라는 기법이 적용되고 있다. 이를 통하여, 상기 AI 가속 기의 메모리 사용량과 처리량의 효율이 극대화될 수 있고, AI 가속기는 N개의 추론 작업들을 병렬 처리하는 것 과 1개의 추론 작업을 처리하는 것에 있어서 에너지 소모량은 동일하거나 거의 차이가 없어 소모 에너지도 효율 화 할 수 있는 특징을 가지고 있다. 따라서, 복수의 사용자들로부터 요청된 N개의 추론 작업들을 N개의 서버들을 구축하여 서버들이 각각 1개의 추 론 작업을 처리하도록 할 수도 있으나, 상기 AI 가속기의 특성을 고려하여 1개의 서버가 N개의 추론 작업들을 병렬로 처리함으로써 속도, 처리량에는 영향을 주지 않지만 소모되는 에너지 효율은 극대화할 수 있다. 이에, 상대적으로 큰 비용이 드는 AI 서비스의 비용을 효율화 하기 위하여 본 문서는 AI 가속기의 특성을 고려하여 로 드 밸런싱을 수행하는 방안을 제안한다. 일 예로, 다음과 같이 본 문서의 일 실시예에 따른 로드 밸런싱을 수행하는 방법이 제안될 수 있다. 구체적으로, 본 문서는 로드 밸런서가 백엔드 서버의 모니터링 정보를 기반으로 AI 가속기가 idle 상태인 백엔 드 서버가 많아지고 idle 상태인 기간을 길게 유지하도록 추론 작업을 분배하는 방안을 제안한다. 예를 들어, 본 문서의 일 실시예에 따른 로드 밸런싱 방법은 크게 3가지의 과정들로 구성될 수 있다. 예를 들어, 일 실시예에 따른 로드 밸런싱 방법은 1) 로드 밸런서 초기 설정 과정, 2) 서버 모니터링 과정, 3) 로드 밸런서의 작업 분배 과정으로 구성될 수 있다. 첫째로, 로드 밸런서 초기 설정 과정이 수행될 수 있다. 예를 들어, 로드 밸런서는 사용자의 요청을 수신하기 위한 서비스 IP 주소와 TCP 포트 번호를 설정할 수 있고, 추론 작업을 분배할 백엔드 서버들의 IP 주소와 TCP 포트 번호를 설정할 수 있다. 상기 백엔드 서버들은 AI 가속기를 포함하여 추론 작업이 처리 가능한 서버 및 AI 가속기를 포함하지 않고 추론 작업이 처리 가능한 서버를 포함할 수 있다. 상기 사용자의 요청을 수신하기 위한 TCP 포트 번호는 서비스 포트 번호라고 나타낼 수 있고, 백엔드 서버의 IP 주소 및 TCP 포트 번호는 서버 IP 주 소 및 서버 포트 번호라고 나타낼 수 있다. 사용자의 서비스 IP 주소 및 서비스 포트 번호는 상기 사용자의 서 비스 네트워크 정보라고 나타낼 수 있고, 서버의 서버 IP 주소 및 서버 포트 번호는 상기 서버의 서버 네트워크 정보라고 나타낼 수 있다. 상기 서비스 IP 주소, 상기 서비스 포트 번호, 서버 IP 주소 및/또는 서버 포트 번호는 시스템 관리자에 의하여 로드 밸런서에 입력될 수 있다. 또는, 예를 들어, AI 서비스 제공 시스템에 의하여 서비스 IP 주소, 상기 서비 스 포트 번호, 서버 IP 주소 및/또는 서버 포트 번호가 로드 밸런서에 자동으로 입력되도록 구성될 수 있다. 로 드 밸런서는 입력 받은 서비스 IP 주소, 상기 서비스 포트 번호, 서버 IP 주소 및/또는 서버 포트 번호를 기반 으로 로드 밸런싱 서비스 연동 정보를 구성할 수 있다. 상기 로드 밸런싱 서비스 연동 정보의 일 예는 다음과 같을 수 있다. 상기 로드 밸런싱 서비스 연동 정보는 로 드 밸런싱 서비스 연동 테이블이라고 불릴 수도 있다. 한편, 후술한 표는 상기 로드 밸런싱 서비스 연동 정보의 일 예일 뿐, 이에 한정되는 것은 아니다.표 1"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "표 1에 도시된 바와 같이 상기 로드 밸런싱 서비스 연동 정보는 사용자의 서비스 IP 주소 및 서비스 포트 번호 를 포함할 수 있고, 서버의 서버 IP 주소 및 서버 포트 정보를 포함할 수 있다. 또한, 예를 들어, 로드 밸런서는 로드 밸런싱 모니터링 연동 정보를 구성할 수 있다. 예를 들어, 백엔드 서버는 상기 백엔드 서버의 현재 추론 작업 처리 현황을 제공하는 모듈 및/또는 인터페이스를 포함할 수 있다. 즉, 예 를 들어, 상기 백엔드 서버의 상기 모듈 및/또는 상기 인터페이스를 기반으로 상기 백엔드 서버의 현재 추론 작 업 처리 현황이 모니터링될 수 있다. 상기 백엔드 서버의 모니터링 인터페이스 정보는 시스템 관리자에 의하여 로드 밸런서에 입력될 수 있다. 또는, 예를 들어, AI 서비스 제공 시스템에 의하여 상기 백엔드 서버의 모니터링 인터페이스 정보가 로드 밸런서에 자 동으로 입력되도록 구성될 수 있다. 상기 백엔드 서버의 모니터링 인터페이스 정보는 모니터링 IP 주소 및 모니 터링 포트 번호를 포함할 수 있다. 상기 서버의 모니터링 IP 주소 및 모니터링 포트 번호는 상기 서버의 모니터 링 인터페이스 정보 또는 모니터링 네트워크 정보라고 나타낼 수 있다. 로드 밸런서는 입력 받은 상기 백엔드 서버의 모니터링 인터페이스 정보를 기반으로 로드 밸런싱 모니터링 연동 정보를 구성할 수 있다. 상기 로드 밸런싱 모니터링 연동 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로드 밸런싱 모 니터링 연동 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 2"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "표 2에 도시된 바와 같이 상기 로드 밸런싱 모니터링 연동 정보는 서버의 모니터링 IP 주소 및 모니터링 포트 정보를 포함할 수 있다. 도 4a 내지 4b는 본 개시의 일 실시예에 따른 복수의 서버들을 포함하는 데이터 센터를 이용한 AI 서비스 제공 시스템의 실시예를 나타낸 블록도이다. 도 4a 내지 4b를 참조하면 AI 서비스 제공 시스템은 유저 디바이스, 인터넷, 로드 밸런서, 서비스 스위치 및/또는 서버들를 포함할 수 있다. 상기 서비스 스위치는 상기 로드 밸런서와 별개의 장치로 구성될 수 있고, 또는 상기 로드 밸런서에 포함될 수도 있다. 도 4a는 서버의 서비스 IP 주소와 모니터링 IP 주소가 동일한 AI 서비스 제공 시스템의 실시예를 나타낼 수 있 다. 예를 들어, 서비스 스위치는 서버의 하나의 IP 주소로 로드 밸런서가 분배하는 추론 작업을 서버로 전달할 수 있고, 서버로부터 모니터링 정보를 획득할 수 있다. 상기 모니터링 정보는 상기 서버의 추론 작업 처 리 현황 정보라고도 나타낼 수 있다. 이 경우, 구성되는 로드 밸런싱 모니터링 연동 정보의 일 실시예는 상술한 표 2와 같을 수 있다. 도 4b는 서버의 서비스 IP 주소와 별개의 모니터링 인터페이스가 구성된 AI 서비스 제공 시스템의 실시예를 나 타낼 수 있다. 도 4b를 참조하면 AI 서비스 제공 시스템은 모니터 스위치를 포함할 수 있다. 서비스 스위치 는 서버의 서비스 IP 주소로 로드 밸런서가 분배하는 추론 작업을 서버로 전달할 수 있고, 모니터 스위 치는 상기 서버의 모니터링 IP 주소로 서버의 모니터링 정보를 로드 밸런서로 전달할 수 있다. 또한, 상기 백엔드 서버의 모니터링 인터페이스 정보는 시스템 관리자에 의하여 로드 밸런서에 입력될 수 있다. 또는, 예를 들어, AI 서비스 제공 시스템에 의하여 상기 백엔드 서버의 모니터링 인터페이스 정보가 로드 밸런 서에 자동으로 입력되도록 구성될 수 있다. 상기 백엔드 서버의 모니터링 인터페이스 정보는 모니터링 IP 주소 및 모니터링 포트 번호를 포함할 수 있다. 로드 밸런서는 입력 받은 상기 백엔드 서버의 모니터링 인터페이스 정보를 기반으로 로드 밸런싱 모니터링 연동 정보를 구성할 수 있다. 상기 로드 밸런싱 모니터링 연동 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로드 밸런싱 모 니터링 연동 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 3"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "표 3에 도시된 바와 같이 상기 로드 밸런싱 모니터링 연동 정보는 서버의 모니터링 IP 주소 및 모니터링 포트 정보를 포함할 수 있다. 도 4b을 참조하면, 서버에 모니터링을 위한 별도의 네트워크 연결이 구성될 수 있고, 따라서, 표 3에 도시된 바와 같이 로드 밸런싱 모니터링 연동 정보는 서버의 서버 IP 주소와 다른 모니터링 IP 주소를 포함할 수 있다. 도 5는 로드 밸런서 초기 설정을 수행하는 실시예를 세부적으로 설명하기 위한 도면이다. 도 5를 참조하면 시스템 관리자는 사용자의 서비스 IP 주소, 사용자의 서비스 포트 번호, 서버의 서버 IP 주소, 및/또는 서버의 서버 포트 번호를 로드 밸런서로 입력할 수 있다(S501). 또는, 예를 들어, AI 서비스 제공 시스 템에 의하여 상기 서비스 IP 주소, 상기 서비스 포트 번호, 상기 서버 IP 주소 및/또는 상기 서버 포트 번호가 로드 밸런서에 자동으로 입력되도록 구성될 수 있다. 로드 밸런서는 입력된 사용자의 서비스 IP 주소, 사용자의 서비스 포트 번호, 서버의 서버 IP 주소, 및/또는 서 버의 서버 포트 번호를 기반으로 로드 밸런싱 서비스 연동 정보를 구성할 수 있다(S502). 예를 들어, 상기 로드 밸런싱 서비스 연동 정보는 사용자의 서비스 IP 주소 및 서비스 포트 번호를 포함할 수 있고, 서버의 서버 IP 주소 및 서버 포트 정보를 포함할 수 있다. 상기 로드 밸런싱 서비스 연동 정보는 상술한 표 1과 같을 수 있다. 또한, 시스템 관리자는 서버의 모니터링을 위한 모니터링 IP 주소 및/또는 모니터링 포트 번호를 로드 밸런서로 입력할 수 있다(S503). 예를 들어, 서버에 모니터링을 위한 별도의 네트워크 연결이 구성될 수 있다. 이 경우, 상기 서버의 서버 IP 주소 및 서버 포트 번호는 상기 서버의 모니터링 IP 주소 및 모니터링 포트 번호와 다를 수 있다. 또는, 상기 서버의 서비스 IP 주소를 통하여 모니터링이 수행될 수 있다. 이 경우, 상기 서버의 서버 IP 주소와 상기 서버의 모니터링 IP 주소가 동일할 수 있다. 로드 밸런서는 입력된 서버의 모니터링 IP 주소 및/또는 모니터링 포트 번호를 기반으로 로드 밸런싱 모니터링 연동 정보를 구성할 수 있다(S504). 예를 들어, 상기 로드 밸런싱 모니터링 연동 정보는 서버의 모니터링 IP 주 소 및 모니터링 포트 정보를 포함할 수 있다. 상기 로드 밸런싱 모니터링 연동 정보는 상술한 표 2 또는 표 3과 같을 수 있다. 둘째로, 모니터링 과정이 수행될 수 있다. 예를 들어, 로드 밸런서는 백엔드 서버에서 제공하는 현재 추론 작업 의 처리 현황을 조회할 수 있는 인터페이스를 호출할 수 있고, 이를 통하여, 백엔드 서버들의 추론 작업 처리 현황을 지속적으로 모니터링 및 업데이트할 수 있다. 상기 인터페이스의 주소는 상술한 로드 밸런서 초기 설정 과정에서 생성된 로드 밸런싱 모니터링 연동 정보를 참조하여 확인될 수 있다. 즉, 로드 밸런서는 상기 로드 밸 런싱 모니터링 연동 정보를 기반으로 백엔드 서버의 추론 작업 처리 현황을 모니터링할 수 있다. 또한, 예를 들어, 상기 서버의 모니터링을 위한 통신 프로토콜로 HTTP, RPC, TCP, UDP, 또는 IP 등이 사용될 수 있다. 또한, 시스템 관리자는 추론 작업 요청의 최대 버퍼링 시간 및/또는 최대 버퍼링 개수를 로드 밸런서로 입력할 수 있다(S505). 사용자의 고객 경험을 고려하여 추론 작업 요청에 대한 최대 버퍼링 시간 및/또는 최대 버퍼링 개수를 설정할 필요가 있을 수 있다. 따라서, 시스템 관리자는 추론 작업 요청의 최대 버퍼링 시간 및/또는 최 대 버퍼링 개수를 로드 밸런서로 입력할 수 있고, 로드 밸런서는 입력된 최대 버퍼링 시간 및/또는 최대 버퍼링 개수를 기반으로 추론 작업 요청에 대한 버퍼링 임계값 정보를 구성할 수 있다(S506). 도 6은 모니터링 설정을 수행하는 실시예를 세부적으로 설명하기 위한 도면이다. 도 6을 참조하면 서버의 추론 작업 처리 현황을 모니터링하는 동작이 주기적으로 수행될 수 있다. 로드 밸런서는 서버에 추론 작업 처리 현황을 요청할 수 있다(S601). 로드 밸런서는 복수의 서버들 각각에 대하 여 추론 작업 처리 현황 전달을 요청할 수 있다. 예를 들어, 로드 밸런서는 로드 밸런싱 모니터링 연동 정보를 기반으로 서버의 모니터링 인터페이스 정보를 도출할 수 있고, 상기 모니터링 인터페이스 정보를 기반으로 상기 서버에 추론 작업 처리 현황을 요청하는 정보를 전달할 수 있다. 예를 들어, 로드 밸런서는 로드 밸런싱 모니터 링 연동 정보를 기반으로 서버의 모니터링 인터페이스 정보를 도출할 수 있고, 상기 모니터링 인터페이스 정보 를 기반으로 상기 서버에 추론 작업 처리 현황 요청 메시지를 전달할 수 있다. 서버에 추론 작업 처리 현황이 요청된 경우, 상기 서버는 추론 작업 처리 현황을 나타내는 모니터링 정보를 상 기 로드 밸런서로 전달할 수 있다(S602). 상기 서버는 상기 로드 밸런서의 상기 추론 작업 처리 현황 요청에 대 한 응답을 상기 로드 밸런서로 전달할 수 있다. 예를 들어, 상기 서버가 상기 로드 밸런서의 추론 작업 처리 현 황 요청 메시지를 수신한 경우, 상기 서버의 추론 작업 현황 정보를 상기 로드 밸런서로 전달할 수 있다. 상기 서버의 상기 추론 작업 현황 정보는 모니터링 정보라고 나타낼 수 있다. 예를 들어, 상기 모니터링 정보는 상기 서버가 최대로 병렬 처리가 가능한 추론 작업의 개수를 나타내는 최대 병렬 처리 가능 추론 작업 수 정보, 상기 서버가 현재 처리 중인 추론 작업의 개수를 나타내는 현재 처리 추론 작업 수 정보 및/또는 추론 작업 요청을 받았지만 처리를 기다리는 추론 작업의 개수를 나타내는 대기 추론 작업 수 정보를 포함할 수 있다. 즉, 상기 모니터링 정보는 상기 서버의 최대 병렬 처리 가능 추론 작업 수에 대한 정보 및/또는 현재 처리 추론 작업 수 에 대한 정보 및/또는 대기 추론 작업 수에 대한 정보를 포함할 수 있다. 상기 최대 병렬 처리 가능 추론 작업 수에 대한 정보는 max_batch_size 로 나타낼 수 있고, 상기 현재 처리 추론 작업 수에 대한 정보는 processing_job 로 나타낼 수 있고, 상기 대기 추론 작업 수에 대한 정보는 waiting_job 로 나타낼 수 있다. 또 한, 예를 들어, 상기 서버의 상기 모니터링 정보는 상기 서버의 상기 정보 이외의 정보를 더 포함할 수도 있다. 또한, 상기 모니터링 정보는 데이터베이스 테이블 형태, json 포맷, 텍스트파일, in-memory 데이터 등 다양한 형태로 구성될 수 있다. 상기 로드 밸런서는 서버들로부터 전달된 모니터링 정보를 기반으로 상기 서버들의 모니터링 정보를 생성 및 업 데이트할 수 있다(S603). 예를 들어, 상기 로드 밸런서는 주기적으로 상기 서버들 각각으로부터 모니터링 정보 를 전달받을 수 있고, 상기 서버들 각각으로부터 전달된 모니터링 정보를 기반으로 상기 서버들의 로드 밸런싱 모니터링 정보를 생성 및 업데이트할 수 있다. 상기 로드 밸런싱 모니터링 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로드 밸런싱 모니터 링 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 4"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "표 4를 참조하면 상기 로드 밸런싱 모니터링 정보는 서버의 서버 IP 주소, 서버 포트 번호, 모니터링 IP 주소, 모니터링 포트 번호, 최대 병렬 처리 가능 추론 작업 수에 대한 정보, 현재 처리 추론 작업 수에 대한 정보, 대기 추론 작업 수에 대한 정보를 포함할 수 있다. 셋째로, 작업 분배 과정이 수행될 수 있다. 예를 들어, 로드 밸런서는 상기 서버들의 로드 밸런싱 모니터링 정 보를 기반으로 사용자의 추론 작업을 상기 서버들 중 대상 서버로 분배할 수 있다. 도 7은 작업 분배 과정을 수행하는 실시예를 세부적으로 설명하기 위한 도면이다. 도 7을 참조하면 로드 밸런서 는 사용자의 추론 작업을 로드밸런싱 알고리즘을 기반으로 도출된 대상 서버로 전달할 수 있다. 사용자는 추론 작업 요청 메시지를 AI 서비스 제공 시스템의 로드 밸런서로 전달할 수 있다(S701). 사용자의 유 저 디바이스는 상기 AI 서비스 제공 시스템과 네트워크로 연결될 수 있다. 예를 들어, 상기 유저 디바이스는 상 기 AI 서비스 제공 시스템의 상기 로드 밸런서와 네트워크로 연결될 수 있다. 예를 들어, 로드 밸런서는 네트워 크를 통하여 유저 디바이스로부터 상기 AI 서비스의 추론(Inference) 작업에 대한 추론 작업 요청 메시지를 획 득할 수 있다. '네트워크'는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN) 또는 부가가치 통신망(Value Added Network; VAN) 등과 같은 유선 네트워크나 이동 통신망(mobile radio communication network) 또는 위성 통신망 등과 같은 모든 종류의 무선 네트워크로 구현될 수 있다. 로드 밸런서는 로드밸런싱 알고리즘을 수행하여 상기 추론 작업 요청 메시지의 추론 작업을 전달할 대상 서버를 도출할 수 있다(S702). 예를 들어, 로드 밸런서가 사용자로부터 추론 작업 요청 메시지를 전달받은 경우, 로드 밸런서는 상기 로드 밸 런싱 모니터링 정보를 기반으로 상기 사용자의 추론 작업을 전달할 대상 서버를 도출할 수 있다. 예를 들어, 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 모두 0인 경우, 로드 밸런서 는 상기 로드 밸런싱 모니터링 정보에서 max_batch_size 값이 가장 큰 서버를 대상 서버로 도출할 수 있고, 상 기 도출된 대상 서버에 상기 사용자의 추론 작업 요청을 전달할 수 있다. 한편, 예를 들어, 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 모두 0이고, 상기 서버들 중 가장 큰 max_batch_size 값을 갖는 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 복수의 서버들 중 특정 서버를 대상 서버로 도출할 수 있고, 상기 도출된 대상 서버에 상기 사용자의 추론 작업 요청을 전달할 수 있다. 예를 들어, 상기 로드 밸런싱 모니터링 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로드 밸 런싱 모니터링 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 5"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "표 5에 도시된 바와 같이 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 모두 0일 수 있 다. 이 경우, 로드 밸런서는 상기 로드 밸런싱 모니터링 정보에서 max_batch_size 값이 가장 큰 서버를 대상 서 버로 도출할 수 있다. 표 5를 참조하면 서버 IP 주소가 192.168.10.29 이고, 서버 포트 번호가 8443 인 서버가 max_batch_size 값이 16으로 최대 max_batch_size 값인 서버일 수 있고, 로드 밸런서는 상기 서버를 대상 서버 로 도출할 수 있다. 상술한 실시예와 같이, 백엔드 서버들이 idle한 상태에서 로드 밸런서가 사용자의 추론 작업 요청 메시지를 수 신한 경우에 max_batch_size가 가장 큰 서버로 추론 작업을 전달함으로써 추후 수신되는 사용자의 추론 요청 작 업들이 복수의 백엔드 서버들로 나누어 지는 것을 최소화할 수 있고, 이를 통하여 AI 서비스 제공 시스템의 서 버들의 전력 소비 효율을 향상시키는 효과를 얻을 수 있다. 또한, 예를 들어, 로드 밸런서가 사용자로부터 추론 작업 요청 메시지를 전달받은 경우에 상술한 조건에 해당하 지 않을 수 있다. 예를 들어, 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들 중 적어도 하 나가 0이 아닌 경우, 로드 밸런서는 processing_job 값이 0이 아닌 서버 중 상기 로드 밸런싱 모니터링 정보를기반으로 현재 처리 가능 작업수가 1이상인 서버가 존재하는지 판단할 수 있다. 서버의 현재 처리 가능 작업수 는 상기 서버의 max_batch_size - processing_job 로 도출될 수 있다. 즉, 상기 현재 처리 가능 작업수는 서버 의 최대 병렬 처리 가능 추론 작업 수에서 현재 처리 추론 작업 수를 뺀 값일 수 있다. 예를 들어, processing_job 값이 0이 아닌 서버 중 현재 처리 가능 작업수가 1이상인 서버가 존재하는 경우, 로 드 밸런서는 상기 현재 처리 가능 작업수가 1이상인 서버 중 현재 처리 가능 작업수가 가장 작은 서버를 대상 서버로 도출할 수 있고, 상기 도출된 대상 서버에 상기 사용자의 추론 작업 요청을 전달할 수 있다. 예를 들어, 상기 로드 밸런싱 모니터링 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로드 밸 런싱 모니터링 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 6"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "표 6에 도시된 바와 같이 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 중 적어도 하나 가 0이 아닐 수 있고, 현재 처리 가능 작업수가 1이상인 서버가 존재할 수 있다. 예를 들어, processing_job 값 이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버는 서버 IP 주소가 192.168.10.29 이고, 서버 포트 번호 가 8443 인 서버를 포함할 수 있다. 서버 IP 주소가 192.168.10.29 이고, 서버 포트 번호가 8443 인 서버의 현 재 처리 가능 작업수는 max_batch_size - processing_job 인 15일 수 있다. 이 경우, 로드 밸런서는 상기 로드 밸런싱 모니터링 정보에서 processing_job 값이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버 중 현재 처리 가능 작업수가 가장 작은 서버를 대상 서버로 도출할 수 있다. 표 6를 참 조하면 processing_job 값이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버 중 현재 처리 가능 작업수가 가장 작은 서버는 현재 처리 가능 작업수가 15 인 서버 IP 주소가 192.168.10.29 이고, 서버 포트 번호가 8443 인 서버일 수 있고, 로드 밸런서는 상기 서버를 대상 서버로 도출할 수 있다. 또는, 예를 들어, 상기 로드 밸런싱 모니터링 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로 드 밸런싱 모니터링 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 7"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "표 7에 도시된 바와 같이 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 중 적어도 하나 가 0이 아닐 수 있고, 현재 처리 가능 작업수가 1이상인 서버가 존재할 수 있다. 예를 들어, processing_job 값 이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버는 서버 IP 주소가 192.168.10.22 이고, 서버 포트 번호 가 8443 인 서버를 포함할 수 있다. 서버 IP 주소가 192.168.10.22 이고, 서버 포트 번호가 8443 인 서버의 현 재 처리 가능 작업수는 max_batch_size - processing_job 인 7일 수 있다. 이 경우, 로드 밸런서는 상기 로드 밸런싱 모니터링 정보에서 processing_job 값이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버 중 현재 처리 가능 작업수가 가장 작은 서버를 대상 서버로 도출할 수 있다. 표 7을 참 조하면 processing_job 값이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버 중 현재 처리 가능 작업수가가장 작은 서버는 현재 처리 가능 작업수가 7 인 서버 IP 주소가 192.168.10.22 이고, 서버 포트 번호가 8443 인 서버일 수 있고, 로드 밸런서는 상기 서버를 대상 서버로 도출할 수 있다. 또는, 예를 들어, 상기 로드 밸런싱 모니터링 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로 드 밸런싱 모니터링 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 8"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "표 8에 도시된 바와 같이 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 중 적어도 하나 가 0이 아닐 수 있고, 현재 처리 가능 작업수가 1이상인 서버가 존재할 수 있다. 예를 들어, processing_job 값 이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버는 서버 IP 주소가 192.168.10.22 이고, 서버 포트 번호 가 8443 인 서버를 포함할 수 있다. 서버 IP 주소가 192.168.10.22 이고, 서버 포트 번호가 8443 인 서버의 현 재 처리 가능 작업수는 max_batch_size - processing_job 인 2일 수 있다. 이 경우, 로드 밸런서는 상기 로드 밸런싱 모니터링 정보에서 processing_job 값이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버 중 현재 처리 가능 작업수가 가장 작은 서버를 대상 서버로 도출할 수 있다. 표 8을 참 조하면 processing_job 값이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버 중 현재 처리 가능 작업수가 가장 작은 서버는 현재 처리 가능 작업수가 2 인 서버 IP 주소가 192.168.10.22 이고, 서버 포트 번호가 8443 인 서버일 수 있고, 로드 밸런서는 상기 서버를 대상 서버로 도출할 수 있다. 또는, 예를 들어, 상기 로드 밸런싱 모니터링 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로 드 밸런싱 모니터링 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 9"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "표 9에 도시된 바와 같이 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 중 적어도 하나 가 0이 아닐 수 있고, 현재 처리 가능 작업수가 1이상인 서버가 존재할 수 있다. 예를 들어, processing_job 값 이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버는 서버 IP 주소가 192.168.10.22 이고, 서버 포트 번호 가 8443 인 제1 서버 및 서버 IP 주소가 192.168.10.29 이고, 서버 포트 번호가 8443 인 제1 서버를 포함할 수 있다. 상기 제1 서버의 현재 처리 가능 작업수는 max_batch_size - processing_job 인 1일 수 있고, 상기 제2 서버의 현재 처리 가능 작업수는 max_batch_size - processing_job 인 8일 수 있다. 이 경우, 로드 밸런서는 상기 로드 밸런싱 모니터링 정보에서 processing_job 값이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버 중 현재 처리 가능 작업수가 가장 작은 서버를 대상 서버로 도출할 수 있다. 표 8을 참 조하면 processing_job 값이 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버 중 현재 처리 가능 작업수가 가장 작은 서버는 현재 처리 가능 작업수가 1 인 상기 제1 서버일 수 있고, 로드 밸런서는 상기 제1 서버를 대 상 서버로 도출할 수 있다. 한편, 예를 들어, 상기 현재 처리 가능 작업수가 1이상인 서버 중 가장 작은 현재 처리 가능 작업수를 갖는 복 수의 서버들이 존재하는 경우, 로드 밸런서는 상기 복수의 서버들의 max_batch_size 값들을 기반으로 대상 서버 를 도출할 수 있다. 예를 들어, 상기 현재 처리 가능 작업수가 1이상인 서버 중 가장 작은 현재 처리 가능 작업 수를 갖는 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 복수의 서버들 중 max_batch_size 값이 가장 큰 서버를 대상 서버로 도출할 수 있고, 상기 도출된 대상 서버에 상기 사용자의 추론 작업 요청을 전달할 수 있다. 또한, 예를 들어, 상기 서버들 중 가장 작은 현재 처리 가능 작업수를 갖는 복수의 서버들이 존재하고, 상기 복수의 서버들 중 최대의 max_batch_size 값을 갖는 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 최대의 max_batch_size 값을 갖는 복수의 서버들 중 특정 서버를 대상 서버로 도출할 수 있고, 상기 도출된 대 상 서버에 상기 사용자의 추론 작업 요청을 전달할 수 있다. 예를 들어, 상기 로드 밸런싱 모니터링 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로드 밸 런싱 모니터링 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 10"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "표 10에 도시된 바와 같이 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 중 적어도 하 나가 0이 아닐 수 있고, 현재 처리 가능 작업수가 1이상인 서버가 존재할 수 있다. 예를 들어, 현재 처리 가능 작업수가 1이상인 서버는 서버 IP 주소가 192.168.10.22 이고, 서버 포트 번호가 8443 인 제1 서버와, 서버 IP 주소가 192.168.10.29 이고, 서버 포트 번호가 8443 인 제2 서버를 포함할 수 있다. 상기 제1 서버의 현재 처리 가능 작업수는 max_batch_size - processing_job 인 7일 수 있고, 상기 제2 서버의 현재 처리 가능 작업수는 max_batch_size - processing_job 인 7일 수 있다. 이 경우, 로드 밸런서는 상기 로드 밸런싱 모니터링 정보에서 현재 처리 가능 작업수가 1이상인 서버 중 현재 처리 가능 작업수가 가장 작은 서버를 대상 서버로 도출할 수 있다. 표 10을 참조하면 가장 작은 현재 처리 가 능 작업수인 7을 현재 처리 가능 작업수로 갖는 상기 제1 서버 및 상기 제2 서버가 존재할 수 있다. 즉, 상기 현재 처리 가능 작업수가 1이상인 서버 중 가장 작은 현재 처리 가능 작업수를 갖는 복수의 서버들이 존재하므 로, 로드 밸런서는 상기 복수의 서버들의 max_batch_size 값들을 기반으로 대상 서버를 도출할 수 있다. 예를 들어, 로드 밸런서는 상기 복수의 서버들 중 max_batch_size 값이 가장 큰 서버를 대상 서버로 도출할 수 있다. 표 10을 참조하면 상기 제1 서버의 max_batch_size 값은 8, 상기 제2 서버의 max_batch_size 값은 16이므로, 로드 밸런서는 상기 제1 서버 및 상기 제2 서버 중 상기 제2 서버를 대상 서버로 도출할 수 있다. 상술한 실시예와 같이, 로드 밸런서는 추론 작업 요청을 분산하기 보다는 이미 추론 작업을 수행중인 백엔드 서 버로 추론 작업을 전달할 수 있고, 이를 통하여 추가로 수신된 추론 작업 요청을 수용하되, AI 서비스 제공 시 스템의 서버들의 전력 소모량을 최소화하고 전력 소비 효율을 향상시키는 효과를 얻을 수 있다. 또한, 예를 들어, 로드 밸런서가 사용자로부터 추론 작업 요청 메시지를 전달받은 경우에 상술한 조건에 해당하 지 않을 수 있다. 예를 들어, 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들 중 적어도 하 나가 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버가 존재하지 않는 경우, 로드 밸런서는 상기 로드 밸런 싱 모니터링 정보를 기반으로 서버들의 처리 대기 중인 추론 작업 수를 판단할 수 있다. 예를 들어, 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들 중 적어도 하나가 0이 아니고, 현재 처리 가능 작업수가 1이상인 서버가 존재하지 않는 경우, 로드 밸런서는 처리 대기 중인 추론 작업 수가 가장 작은 서버를 대상 서버로 도출할 수 있고, 상기 도출된 대상 서버에 상기 사용자의 추론 작업 요청을 전달 할 수 있다. 한편, 예를 들어, 상기 서버들 중 가장 작은 처리 대기 중인 추론 작업 수를 갖는 복수의 서버들이 존재하는 경 우, 로드 밸런서는 상기 복수의 서버들의 max_batch_size 값들을 기반으로 대상 서버를 도출할 수 있다. 예를 들어, 상기 서버들 중 가장 작은 처리 대기 중인 추론 작업 수를 갖는 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 복수의 서버들 중 max_batch_size 값이 가장 큰 서버를 대상 서버로 도출할 수 있고, 상기 도출된 대상 서버에 상기 사용자의 추론 작업 요청을 전달할 수 있다. 또한, 예를 들어, 상기 서버들 중 가장 작은 처 리 대기 중인 추론 작업 수를 갖는 복수의 서버들이 존재하고, 상기 복수의 서버들 중 최대의 max_batch_size 값을 갖는 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 최대의 max_batch_size 값을 갖는 복수의 서버 들 중 특정 서버를 대상 서버로 도출할 수 있고, 상기 도출된 대상 서버에 상기 사용자의 추론 작업 요청을 전 달할 수 있다. 예를 들어, 상기 로드 밸런싱 모니터링 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로드 밸 런싱 모니터링 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 11"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "표 11에 도시된 바와 같이 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 중 적어도 하 나가 0이 아닐 수 있고, 현재 처리 가능 작업수가 1이상인 서버가 존재하지 않을 수 있다. 이 경우, 로드 밸런 서는 상기 로드 밸런싱 모니터링 정보를 기반으로 서버들의 처리 대기 중인 추론 작업 수를 판단할 수 있고, 표 11에 도시된 바와 같이 가장 작은 처리 대기 중인 추론 작업 수를 0으로 도출할 수 있다. 표 11을 참조하면, 가 장 작은 처리 대기 중인 추론 작업 수를 갖는 복수의 서버들, 즉, 처리 대기 중인 추론 작업 수가 0인 복수의 서버들이 존재하므로, 로드 밸런서는 상기 복수의 서버들 중 최대의 max_batch_size 값을 갖는 서버 IP 주소가 192.168.10.29 이고, 서버 포트 번호가 8443 인 서버를 대상 서버로 도출할 수 있다. 또한, 예를 들어, 상기 로드 밸런싱 모니터링 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로 드 밸런싱 모니터링 정보의 일 예일 뿐, 이에 한정되는 것은 아니다. 표 12"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "표 12에 도시된 바와 같이 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 중 적어도 하 나가 0이 아닐 수 있고, 현재 처리 가능 작업수가 1이상인 서버가 존재하지 않을 수 있다. 이 경우, 로드 밸런 서는 상기 로드 밸런싱 모니터링 정보를 기반으로 서버들의 대기 추론 작업 수들을 판단할 수 있고, 표 12에 도 시된 바와 같이 가장 작은 대기 추론 작업 수를 0으로 도출할 수 있다. 표 12를 참조하면, 가장 작은 대기 추론 작업 수를 갖는 복수의 서버들, 즉, 대기 추론 작업 수가 0인 복수의 서버들이 존재하므로, 로드 밸런서는 상기 복수의 서버들 중 최대의 max_batch_size 값을 갖는 서버 IP 주소가 192.168.10.22 이고, 서버 포트 번호가 8443 인 서버를 대상 서버로 도출할 수 있다. 또한, 예를 들어, 상기 로드 밸런싱 모니터링 정보의 일 예는 다음과 같을 수 있다. 한편, 후술한 표는 상기 로 드 밸런싱 모니터링 정보의 일 예일 뿐, 이에 한정되는 것은 아니다.표 13"}
{"patent_id": "10-2025-0017052", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "표 13에 도시된 바와 같이 상기 로드 밸런싱 모니터링 정보에서 서버들의 processing_job 값들이 중 적어도 하 나가 0이 아닐 수 있고, 현재 처리 가능 작업수가 1이상인 서버가 존재하지 않을 수 있다. 이 경우, 로드 밸런 서는 상기 로드 밸런싱 모니터링 정보를 기반으로 서버들의 대기 추론 작업 수들을 판단할 수 있고, 표 13에 도 시된 바와 같이 가장 작은 대기 추론 작업 수를 0으로 도출할 수 있다. 표 13을 참조하면, 가장 작은 대기 추론 작업 수를 갖는 복수의 서버들, 즉, 대기 추론 작업 수가 0인 복수의 서버들이 존재하므로, 로드 밸런서는 상기 복수의 서버들 중 최대의 max_batch_size 값을 갖는 서버 IP 주소가 192.168.10.29 이고, 서버 포트 번호가 8443 인 서버를 대상 서버로 도출할 수 있다. 상술한 실시예와 같이, 서버들이 모두 최대의 추론 작업 수를 처리하는 경우, 즉, 서버들이 매우 혼잡한 상황에 서 로드 밸런서는 대기 추론 작업 수가 가장 작은 백엔드 서버로 추론 작업을 전달할 수 있고, 이를 통하여, AI 서비스 제공 시스템의 추론 작업 요청에 대한 평균 응답을 최소화하면서 서버들의 전력 소모량을 최소화하고 전 력 소비 효율을 향상시키는 효과를 얻을 수 있다. 로드 밸런서는 도출된 대상 서버로 추론 작업 요청을 전달할 수 있다(S703). 로드 밸런서는 상기 도출된 대상 서버에 상기 사용자의 추론 작업에 대한 로드 밸런싱을 수행할 수 있다. 상기 추론 작업이 분배된 대상 서버는 상기 추론 작업을 수행할 수 있고(S704), 상기 추론 작업 결과를 상기 로 드 밸런서로 전달할 수 있다(S705). 로드 밸런서는 상기 추론 작업이 분배된 대상 서버로부터 전달된 상기 추론 작업 결과를 상기 사용자의 유저 디 바이스로 전달할 수 있다(S706). 유저 디바이스는 상기 추론 작업 결과를 기반으로 사용자가 요청한 AI 서비스 의 결과를 사용자에게 제공할 수 있다. 도 8은 본 문서에 따른 모니터링 정보를 기반으로 대상 서버를 도출하는 로드 밸런싱 알고리즘의 일 예를 세부 적으로 설명하기 위한 순서도이다. 로드 밸런서는 사용자로부터 추론 작업 요청 메시지를 획득한다(S801). 사용자의 유저 디바이스는 상기 AI 서비 스 제공 시스템과 네트워크로 연결될 수 있다. 예를 들어, 상기 유저 디바이스는 상기 AI 서비스 제공 시스템의 상기 로드 밸런서와 네트워크로 연결될 수 있다. 예를 들어, 로드 밸런서는 네트워크를 통하여 유저 디바이스로 부터 상기 AI 서비스의 추론(Inference) 작업에 대한 추론 작업 요청 메시지를 획득할 수 있다. '네트워크'는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN) 또는 부가가치 통신망(Value Added Network; VAN) 등과 같은 유선 네트워크나 이동 통신망(mobile radio communication network) 또는 위성 통신망 등과 같은 모든 종류의 무선 네트워크로 구현될 수 있다. 로드 밸런서는 모니터링 정보를 기반으로 서버들 각각의 최대 병렬 처리 가능 추론 작업 수, 현재 처리 추론 작 업 수, 대기 추론 작업 수를 확인한다(S802). 상술한 모니터링 과정 실시예와 같이, 로드 밸런서는 백엔드 서버에서 제공하는 현재 추론 작업의 처리 현황을 조회할 수 있는 인터페이스를 호출할 수 있고, 이를 통하여, 백엔드 서버들의 추론 작업 처리 현황을 지속적으 로 모니터링 및 업데이트할 수 있다. 예를 들어, 로드 밸런서는 서버들로부터 처리 현황 정보를 획득할 수 있고, 상기 처리 현황 정보를 기반으로 상기 서버들의 로드 밸런싱 모니터링 정보를 구성 및 업데이트할 수 있 다. 서버의 처리 현황 정보는 상기 서버의 최대 병렬 처리 가능 추론 작업 수, 현재 처리 추론 작업 수 및/또는 대기 추론 작업 수를 포함할 수 있다. 이후, 로드 밸런서는 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 추론 작업 요청 메시지를 전달할 대상 서버를 도출할 수 있다. 로드 밸런서는 서버들의 현재 처리 추론 작업 수들이 모두 0인지 판단한다(S803). 예를 들어, 로드 밸런서는 상 기 모니터링 정보를 기반으로 상기 서버들이 idle 상태인지 판단할 수 있다. 상기 idle 상태는 서버들의 현재 처리 추론 작업 수들이 모두 0인 경우를 나타낼 수 있다. 서버들의 현재 처리 작업 수들이 모두 0인 경우, 로드 밸런서는 상기 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 서버로 사용자의 추론 작업 요청 메시지를 전달한다(S804). 예를 들어, 서버들의 현재 처리 작업 수들이 모두 0인 경우, 로드 밸런서는 상기 서버들의 최대 병렬 처리 가능 추론 작업 수를 기반으로 대상 서버 를 도출할 수 있다. 예를 들어, 서버들의 현재 처리 작업 수들이 모두 0인 경우, 로드 밸런서는 상기 서버들의 최대 병렬 처리 가능 추론 작업 수가 가장 큰 서버를 대상 서버로 도출할 수 있고, 상기 대상 서버로 사용자의 추론 작업 요청 메시지를 전달할 수 있다. 즉, 예를 들어, 서버들이 idle 상태인 경우, 로드 밸런서는 상기 서버들의 최대 병렬 처리 가능 추론 작업 수를 기반으로 대상 서버를 도출할 수 있다. 예를 들어, 상기 서버들이 idle 상태인 경우, 로드 밸런서는 상기 서버 들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 서버를 대상 서버로 도출할 수 있고, 상기 대상 서버로 사 용자의 추론 작업 요청 메시지를 전달할 수 있다. 한편, 예를 들어, 상기 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 복수의 서버들 중 특정 서버를 대상 서버로 도출할 수 있고, 상기 대상 서버로 상기 추론 작업 요청 메시지를 전달할 수 있다. 서버들의 현재 처리 작업 수들 중 적어도 하나가 0이 아닌 경우, 로드 밸런서는 상기 서버들 중 현재 처리 가능 추론 작업 수가 1 이상인 서버가 존재하는지 판단한다(S805). 예를 들어, 상기 서버의 현재 처리 가능 추론 작 업 수는 상기 서버의 최대 병렬 처리 가능 추론 작업 수에서 현재 처리 추론 작업 수를 뺀 값으로 도출될 수 있 다. 예를 들어, 서버들의 현재 처리 작업 수들 중 적어도 하나가 0이 아닌 경우, 로드 밸런서는 상기 서버들의 현재 처리 가능 추론 작업 수를 도출할 수 있고, 상기 서버들 중 현재 처리 가능 추론 작업 수가 1 이상인 서버가 존 재하는지 판단할 수 있다. 즉, 예를 들어, 서버들 중 적어도 하나가 동작 상태인 경우, 로드 밸런서는 상기 서버들이 혼잡 상태인지 판단 할 수 있다. 예를 들어, 상기 동작 상태는 서버의 현재 처리 작업 수가 0이 아닌 경우를 나타낼 수 있다. 또한, 예를 들어, 상기 혼잡 상태는 서버의 현재 처리 가능 추론 작업 수가 0인 경우를 나타낼 수 있다. 현재 처리 작업 수가 0이 아닌 서버 중 현재 처리 가능 추론 작업 수가 1이상인 서버가 존재하는 경우, 로드 밸 런서는 상기 현재 처리 가능 추론 작업 수가 1이상인 서버 중 현재 처리 가능 추론 작업 수가 가장 작은 서버로 사용자의 추론 작업 요청 메시지를 전달한다(S806). 예를 들어, 현재 처리 작업 수가 0이 아닌 서버 중 현재 처리 가능 추론 작업 수가 1이상인 서버가 존재하는 경 우, 로드 밸런서는 현재 처리 가능 추론 작업 수를 기반으로 현재 처리 가능 추론 작업 수가 1이상인 서버 중 대상 서버를 도출할 수 있다. 예를 들어, 현재 처리 가능 추론 작업 수가 1이상인 서버가 존재하는 경우, 로드 밸런서는 상기 현재 처리 가능 추론 작업 수가 1이상인 서버 중 현재 처리 가능 추론 작업 수가 가장 작은 서버 를 대상 서버로 도출할 수 있고, 상기 대상 서버로 상기 추론 작업 요청 메시지를 전달할 수 있다. 즉, 예를 들어, 동작 서버들 중 처리 가능 서버가 존재하는 경우, 로드 밸런서는 현재 처리 가능 추론 작업 수 를 기반으로 대상 서버를 도출할 수 있다. 예를 들어, 동작 서버들 중 처리 가능 서버가 존재하는 경우, 로드 밸런서는 상기 처리 가능 서버 중 현재 처리 가능 추론 작업 수가 가장 작은 서버를 대상 서버로 도출할 수 있 고, 상기 대상 서버로 상기 추론 작업 요청 메시지를 전달할 수 있다. 상기 동작 서버는 현재 처리 작업 수가 0 이 아닌 서버(즉, 현재 처리 작업 수가 1 이상인 서버)를 나타낼 수 있고, 처리 가능 서버는 현재 처리 가능 추 론 작업 수가 1 이상인 서버를 나타낼 수 있다. 한편, 예를 들어, 상기 처리 가능 서버 중 현재 처리 가능 추론 작업 수가 가장 작은 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 서버를 대상 서버로 도출할 수 있고, 상기 대상 서버로 상기 추론 작업 요청 메시지를 전달할 수 있다. 한편, 예를 들어, 상기 현재 처리 가능 추론 작업 수가 가장 작은 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 복수의 서버들 중 특정 서버를 대상 서버로 도출할 수 있고, 상기 대상 서버로 상기 추론 작업 요청 메시지를 전달할 수 있다. 현재 처리 작업 수가 0이 아닌 서버 중 현재 처리 가능 추론 작업 수가 1이상인 서버가 존재하지 않는 경우, 로 드 밸런서는 상기 서버들 중 대기 추론 작업 수가 가장 작은 서버로 사용자의 추론 작업 요청 메시지를 전달한 다(S807). 예를 들어, 현재 처리 작업 수가 0이 아닌 서버 중 현재 처리 가능 추론 작업 수가 1이상인 서버가 존재하지 않 는 경우, 로드 밸런서는 상기 서버들의 대기 추론 작업 수를 기반으로 대상 서버를 도출할 수 있다. 예를 들어, 현재 처리 작업 수가 0이 아닌 서버 중 현재 처리 가능 추론 작업 수가 1이상인 서버가 존재하지 않는 경우, 로 드 밸런서는 대기 추론 작업 수가 가장 작은 서버를 대상 서버로 도출할 수 있고, 상기 대상 서버로 상기 추론 작업 요청 메시지를 전달할 수 있다. 즉, 예를 들어, 동작 서버 중 처리 가능 서버가 존재하지 않는 경우(즉, 동작 서버가 모두 혼잡 서버인 경우), 로드 밸런서는 상기 서버들의 대기 추론 작업 수를 기반으로 대상 서버를 도출할 수 있다. 예를 들어, 처리 가 능 서버가 존재하지 않는 경우(즉, 서버들이 모두 혼잡 서버인 경우), 로드 밸런서는 대기 추론 작업 수가 가장 작은 서버를 대상 서버로 도출할 수 있고, 상기 대상 서버로 상기 추론 작업 요청 메시지를 전달할 수 있다. 여 기서, 상기 혼잡 서버는 현재 처리 가능 추론 작업 수가 0 인 서버를 나타낼 수 있다. 한편, 예를 들어, 상기 서버들 중 대기 추론 작업 수가 가장 작은 복수의 서버들이 존재하는 경우, 로드 밸런서 는 상기 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 서버를 대상 서버로 도출할 수 있고, 상기 대상 서버로 상기 추론 작업 요청 메시지를 전달할 수 있다. 한편, 예를 들어, 상기 대기 추론 작업 수가 가장 작은 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 복수의 서버들 중 특정 서버를 대상 서버로 도출 할 수 있고, 상기 대상 서버로 상기 추론 작업 요청 메시지를 전달할 수 있다. 도 9는 본 개시의 일 실시예에 따른 AI 서비스 제공 시스템에서의 모니터링정보를 이용한 로드 밸런싱 방법을 세부적으로 설명하기 위한 순서도이다. 로드 밸런서는 복수의 서버들의 로드 밸런싱 모니터링 정보를 구성한다(S900). 예를 들어, AI 서비스 제공 시스 템은 로드 밸런서 및 복수의 서버들을 포함할 수 있다. 예를 들어, 로드 밸런서는 복수의 서버들로부터 전달된 추론(Inference) 작업 처리 현황 정보를 기반으로 로드 밸런싱 모니터링 정보를 구성할 수 있다. 예를 들어, 복수의 서버들은 추론 작업 처리 현황 정보를 주기적으로 상기 로드 밸런서로 전달할 수 있고, 로드 밸런서는 복수의 서버들로부터 전달된 추론 작업 처리 현황 정보를 기반으로 로드 밸런싱 모니터링 정보를 주기 적으로 업데이트할 수 있다. 또는, 예를 들어, 로드 밸런서는 복수의 서버들에게 추론 작업 처리 현황 정보를 요청할 수 있고, 복수의 서버들은 추론 작업 처리 현황 정보를 상기 로드 밸런서로 전달할 수 있다. 상기 추론 작업 처리 현황 정보는 서버의 최대 병렬 처리 가능 추론 작업 수, 현재 처리 추론 작업 수 및/또는 대기 추론 작업 수를 포함할 수 있다. 예를 들어, 상기 로드 밸런싱 모니터링 정보는 서버의 서버 IP 주소, 서버 포트 번호, 모니터링 IP 주소, 모니 터링 포트 번호, 최대 병렬 처리 가능 추론 작업 수, 현재 처리 추론 작업 수 및/또는 대기 추론 작업 수를 포 함할 수 있다. 또한, 예를 들어, 상기 서버의 추론 작업 처리 현황 정보는 로드 밸런싱 모니터링 연동 정보의 모니터링 네트워 크를 통하여 전달될 수 있다. 상기 로드 밸런싱 모니터링 연동 정보는 서버의 모니터링 네트워크 정보를 포함할 수 있다. 예를 들어, 상기 로드 밸런싱 모니터링 연동 정보는 서버의 모니터링 IP 주소 및 모니터링 포트 번호 를 포함할 수 있다. 상기 로드 밸런싱 모니터링 연동 정보는 시스템 관리자에 의하여 설정될 수 있다. 또는, 상 기 로드 밸런싱 모니터링 연동 정보는 기설정될 수 있다. 또한, 로드 밸런싱 서비스 연동 정보가 설정될 수 있다. 로드 밸런싱 서비스 연동 정보는 사용자의 서비스 네트 워크 정보 및 서버의 서버 네트워크 정보를 포함할 수 있다. 예를 들어, 로드 밸런싱 서비스 연동 정보는 사용 자의 서비스 IP 주소 및 서비스 포트 번호, 서버의 서버 IP 주소 및 서버 포트 번호를 포함할 수 있다. 상기 로 드 밸런싱 서비스 연동 정보는 시스템 관리자에 의하여 설정될 수 있다. 또는, 상기 로드 밸런싱 서비스 연동 정보는 기설정될 수 있다. 로드 밸런서는 유저 디바이스로부터 AI 서비스에 대한 추론 작업 요청 메시지를 획득한다(S910). 예를 들어, 상기 유저 디바이스는 상기 AI 서비스 제공 시스템의 상기 로드 밸런서와 서비스 네트워크로 연결될 수 있다. 예를 들어, 로드 밸런서는 서비스 네트워크를 통하여 유저 디바이스로부터 상기 AI 서비스의 추론 작 업에 대한 추론 작업 요청 메시지를 획득할 수 있다. '네트워크'는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN) 또는 부가가치 통신망(Value Added Network; VAN) 등과 같은 유선 네트 워크나 이동 통신망(mobile radio communication network) 또는 위성 통신망 등과 같은 모든 종류의 무선 네트 워크로 구현될 수 있다. 로드 밸런서는 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들 중 대상 서버를 도출한다 (S920). 예를 들어, 로드 밸런서는 상기 로드 밸런싱 모니터링 정보를 기반으로 상기 복수의 서버들이 모두 아이들 (idle) 상태인지 여부를 판단할 수 있다. 예를 들어, 로드 밸런서는 상기 복수의 서버들 중 idle 상태인 서버들 이 많아지고, idle 상태인 서버가 idle 상태를 길게 유지하도록 상기 대상 서버를 도출할 수 있다. 여기서, idle 상태는 서버의 현재 처리 추론 작업 수가 0 인 상태를 나타낼 수 있다. 예를 들어, 상기 서버들이 모두 아이들(idle) 상태인 경우, 로드 밸런서는 상기 복수의 서버들의 최대 병렬 처 리 가능 추론 작업 수들을 기반으로 상기 대상 서버를 도출할 수 있다. 예를 들어, 상기 서버들이 모두 아이들 (idle) 상태인 경우, 로드 밸런서는 상기 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 서버 를 상기 대상 서버로 도출할 수 있다. 상기 복수의 서버들의 최대 병렬 처리 가능 추론 작업 수들은 상기 로드 밸런싱 모니터링 정보를 기반으로 도출될 수 있다. 한편, 예를 들어, 상기 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 복수의 서버들이 존재하 는 경우, 로드 밸런서는 상기 복수의 서버들 중 특정 서버를 상기 대상 서버로 도출할 수 있다. 또는, 예를 들어, 상기 복수의 서버들 중 적어도 하나의 서버가 아이들 상태가 아닐 수 있다. 즉, 예를 들어, 상기 복수의 서버들 중 동작 서버가 존재할 수 있다. 여기서, 동작 서버는 아이들 상태가 아니고 동작 상태인 서버를 나타낼 수 있고, 상기 동작 상태는 서버의 현재 처리 추론 작업 수가 1 이상인 상태를 나타낼 수 있다. 즉, 상기 동작 서버는 현재 처리 추론 작업 수가 1 이상인 서버를 나타낼 수 있다. 상기 복수의 서버들의 현재 처리 추론 작업 수들은 상기 로드 밸런싱 모니터링 정보를 기반으로 도출될 수 있다. 예를 들어, 상기 복수의 서버들 중 동작 서버가 존재하는 경우(즉, 상기 복수의 서버들 중 적어도 하나의 서버 가 아이들 상태가 아닌 경우), 로드 밸런서는 상기 동작 서버 중 처리 가능 서버가 존재하는지 판단할 수 있다. 상기 처리 가능 서버는 현재 처리 가능 추론 작업 수가 1 이상인 서버를 나타낼 수 있다. 서버의 상기 현재 처 리 가능 추론 작업 수는 상기 서버의 최대 병렬 처리 가능 추론 작업 수에서 현재 처리 추론 작업 수를 뺀 값으 로 도출될 수 있다. 예를 들어, 상기 동작 서버 중 처리 가능 서버가 존재하는 경우, 로드 밸런서는 상기 처리 가능 서버의 현재 처 리 가능 추론 작업 수를 기반으로 상기 대상 서버를 도출할 수 있다. 예를 들어, 상기 동작 서버 중 처리 가능 서버가 존재하는 경우, 로드 밸런서는 상기 처리 가능 서버에서 현재 처리 가능 추론 작업 수가 가장 작은 서버 를 상기 대상 서버로 도출할 수 있다. 한편, 예를 들어, 상기 처리 가능 서버 중 현재 처리 가능 추론 작업 수가 가장 작은 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 서버를 상기 대상 서 버로 도출할 수 있다. 한편, 예를 들어, 상기 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 복수의 서버들 중 특정 서버를 상기 대상 서버로 도출할 수 있다. 또는, 예를 들어, 상기 동작 서버 중 처리 가능 서버가 존재하지 않을 수 있다. 즉, 예를 들어, 상기 동작 서버 는 모두 혼잡 서버일 수 있다. 여기서, 혼잡 서버는 혼잡 상태인 서버를 나타낼 수 있고, 상기 혼잡 상태는 서 버의 현재 처리 가능 추론 작업 수가 0인 상태를 나타낼 수 있다. 즉, 상기 혼잡 서버는 현재 처리 가능 추론 작업 수가 0인 서버를 나타낼 수 있다. 예를 들어, 상기 동작 서버 중 처리 가능 서버가 존재하지 않는 경우(상기 동작 서버가 모두 혼잡 서버인 경 우), 로드 밸런서는 상기 복수의 서버들의 대기 추론 작업 수들을 기반으로 상기 대상 서버를 도출할 수 있다. 예를 들어, 상기 동작 서버 중 처리 가능 서버가 존재하지 않는 경우, 로드 밸런서는 상기 복수의 서버들에서대기 추론 작업 수가 가장 작은 서버를 상기 대상 서버로 도출할 수 있다. 상기 복수의 서버들의 대기 추론 작 업 수들은 상기 로드 밸런싱 모니터링 정보를 기반으로 도출될 수 있다. 한편, 예를 들어, 상기 복수의 서버들 중 대기 추론 작업 수가 가장 작은 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 대기 추론 작업 수가 가장 작은 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 서버를 상기 대상 서버로 도출할 수 있다. 한편, 예를 들어, 상기 복수의 서버들 중 최대 병렬 처리 가능 추론 작업 수가 가장 큰 복수의 서버들이 존재하는 경우, 로드 밸런서는 상기 최대 병렬 처리 가능 추론 작업 수가 가장 큰 복수의 서버들 중 특정 서버를 상기 대상 서버로 도출할 수 있다. 로드 밸런서는 상기 도출된 대상 서버에 상기 AI 서비스의 추론 작업에 대한 로드 밸런싱을 수행한다(S930). 로드 밸런서는 상기 도출된 대상 서버에 상기 AI 서비스의 추론 작업을 분배할 수 있다. 상기 추론 작업이 분배 된 대상 서버는 상기 AI 서비스의 상기 추론 작업을 수행할 수 있고, 상기 추론 작업 결과를 상기 로드 밸런서 로 전달할 수 있다. 로드 밸런서는 상기 추론 작업이 분배된 대상 서버로부터 전달된 상기 추론 작업 결과를 상기 유저 디바이스로 전달할 수 있다. 이후, 유저 디바이스는 상기 추론 작업 결과를 기반으로 사용자가 요청한 AI 서비스의 결과를 사용자에게 제공할 수 있다. 이상에서 설명한 실시예들에 따른 AI 서비스 제공 시스템에서의 로드 밸런싱 방법은 1개의 추론 작업을 처리하 는 것과 복수의 추론 작업들을 병렬 처리하는데 소모되는 전력량이 동일하거나 거의 차이가 없는 AI 가속기의 특성을 고려하여 백엔드 서버들에 추론 작업을 분배할 수 있고, 이를 통하여, AI 서비스 제공 시스템의 전력 소 모량을 줄이고 전력 효율을 향상시키는 효과를 발생시킬 수 있다. 또한, AI 서비스 제공 시스템의 복수의 서버들 중 idle 상태인 서버들이 많아지고, idle 상태인 서버가 idle 상 태를 길게 유지하도록 로드 밸런싱을 수행할 수 있고, 이를 통하여, AI 서비스 제공 시스템의 전력 소모량을 줄 이고 전력 효율을 향상시키는 효과를 발생시킬 수 있다. 또한, 서버들로부터 주기적으로 추론 작업 처리 현황 정보를 전달받아 로드 밸런싱을 위한 로드 밸런싱 모니터 링 테이블을 자동 업데이트할 수 있고, 이를 통하여, AI 서비스 제공 시스템의 전력 소모량을 줄이고 전력 효율 을 향상시키는 효과를 발생시킬 수 있다. 이상에서 살펴본 본 명세서는 도면에 도시된 실시예들을 참고로 하여 설명하였으나 이는 예시적인 것에 불과하 며 당해 분야에서 통상의 지식을 가진 자라면 이로부터 다양한 변형 및 실시예의 변형이 가능하다는 점을 이해 할 것이다. 즉, 본 명세서의 권리범위는 상술된 구현예에 한정되지 않으며 다음의 청구범위에서 정의하고 있는 구현예의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또 한 구현예의 권리범위에 속하는 것이다. 따 라서, 본 명세서의 진정한 기술적 보호범위는 첨부된 청구범위의 기술적 사상에 의해서 정해져야 할 것이다.도면 도면1 도면2 도면3a 도면3b 도면4a 도면4b 도면5 도면6 도면7 도면8 도면9"}
{"patent_id": "10-2025-0017052", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 복수의 서버들을 포함하는 데이터 센터를 이용한 AI 서비스 제공 시스템의 실시예를 나타낸 블록도이다. 도 2는 로드 밸런싱 알고리즘들을 세부적으로 설명하기 위한 도면이다. 도 3a 내지 도 3b는 AI 가속기의 추론 작업 수행 개수에 따른 소모 전력량을 설명하기 위한 도면이다. 도 4a 내지 4b는 본 개시의 일 실시예에 따른 복수의 서버들을 포함하는 데이터 센터를 이용한 AI 서비스 제공 시스템의 실시예를 나타낸 블록도이다. 도 5는 로드 밸런서 초기 설정을 수행하는 실시예를 세부적으로 설명하기 위한 도면이다. 도 6은 모니터링 설정을 수행하는 실시예를 세부적으로 설명하기 위한 도면이다. 도 7은 작업 분배 과정을 수행하는 실시예를 세부적으로 설명하기 위한 도면이다. 도 8은 본 문서에 따른 모니터링 정보를 기반으로 대상 서버를 도출하는 로드 밸런싱 알고리즘의 일 예를 세부 적으로 설명하기 위한 순서도이다. 도 9는 본 개시의 일 실시예에 따른 AI 서비스 제공 시스템에서의 모니터링정보를 이용한 로드 밸런싱 방법을 세부적으로 설명하기 위한 순서도이다."}
