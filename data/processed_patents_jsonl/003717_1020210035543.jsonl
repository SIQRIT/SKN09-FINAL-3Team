{"patent_id": "10-2021-0035543", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0148872", "출원번호": "10-2021-0035543", "발명의 명칭": "복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법, 장치, 기기 및 기록 매체", "출원인": "베이징 바이두 넷컴 사이언스 앤 테크놀로지 코.,", "발명자": "리, 쩐"}}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법에 있어서,제1 단어 마스크(first word mask)를 포함하는 제1 샘플 텍스트 코퍼스(sample text corpus)를 언어 모델에 입력하고, 상기 언어 모델을 통해 상기 제1 단어 마스크의 콘텍스트 벡터(context vector)를 출력하는 단계;상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스(first word vector parametermatrix)에 기반하여 상기 제1 단어 마스크의 제1 확률 분포 매트릭스(first probability distribution)를얻고, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭스(second word vectorparameter matrix)에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스(second probabilitydistribution)를 얻는 단계 - 상기 제1 단어 벡터 매개 변수 매트릭스는 상기 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이며, 상기 제2 단어 벡터 매개 변수 매트릭스는 제2 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스임 -;상기 제1 확률 분포 매트릭스 및 상기 제2 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터(word vector)를 결정하는 단계; 및제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여상기 언어 모델을 트레이닝하는 단계;를 포함하는 것을 특징으로 하는,언어 모델 트레이닝 방법."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 단어 마스크를 포함하는 샘플 텍스트 코퍼스를 언어 모델에 입력하고, 상기 언어 모델을 통해 상기 제1 단어 마스크의 콘텍스트 벡터를 출력하는 단계 이후에,상기 방법은,상기 제1 단어 마스크의 콘텍스트 벡터 및 완전 연결 매트릭스에 기반하여 상기 제1 단어 마스크의 제3 확률 분포 매트릭스를 얻는 단계를 더 포함하며,상기 제1 확률 분포 매트릭스 및 상기 제2 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계는,상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스에 기반하여 상기제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계를 포함하는 것을 특징으로 하는,언어 모델 트레이닝 방법."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제1 확률 분포 매트릭스를 얻는 단계는, 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제1 단어 벡터 매개변수 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제1 확률 분포 매트릭스를 얻는 단계를 포함하고,및/또는,상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크공개특허 10-2021-0148872-3-의 제2 확률 분포 매트릭스를 얻는 단계는, 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제2 단어 벡터 매개변수 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻는 단계를 포함하고,및/또는,상기 제1 단어 마스크의 콘텍스트 벡터 및 완전 연결 매트릭스에 기반하여 상기 제1 단어 마스크의 제3 확률 분포 매트릭스를 얻은 단계는, 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 완전 연결 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제3 확률 분포 매트릭스를 얻는 단계를 포함하는 것을 특징으로 하는,언어 모델 트레이닝 방법."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스에 기반하여 상기제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계는,상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스를 가산하여 토탈확률 분포 매트릭스를 얻는 단계;상기 토탈 확률 분포 매트릭스 중의 확률 값에 대해 정규화 처리를 실행하여, 상기 제1 단어 마스크가 복수의단어 벡터에 대응하는 복수의 정규화 확률 값을 얻는 단계; 및상기 복수의 정규화 확률 값에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계를 포함하는 것을 특징으로 하는,언어 모델 트레이닝 방법."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서,상기 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델을 트레이닝하는 단계는,상기 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델 및 상기 완전 연결 매트릭스를 트레이닝하는 단계를 포함하는 것을 특징으로 하는,언어 모델 트레이닝 방법."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제5항 중 어느 한 항에 있어서,상기 제1 단어 마스크를 포함하는 제1 샘플 텍스트 코퍼스를 상기 언어 모델에 입력하고, 상기 언어 모델을 통해 상기 제1 단어 마스크의 콘텍스트 벡터를 출력하는 단계의 전에,상기 방법은,제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 초기화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매트릭스를 트레이닝 하여 상기 언어 모델 및 상기 제1 단어 벡터 매개 변수 매트릭스를 얻은 단계를 더 포함하는것을 특징으로 하는,언어 모델 트레이닝 방법."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 초기화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매트릭스를 트레이닝하는 단계는,사전에 코퍼스 베이스 중의 기설정된 텍스트 코퍼스를 사용하여 상기 초기화 언어 모델에 대해 사전 트레이닝을공개특허 10-2021-0148872-4-실행하는 단계;제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어를 각각 제2 단어 마스크로 교체하여 적어도 하나의 제2 단어마스크를 포함하는 제2 샘플 텍스트 코퍼스를 얻는 단계;상기 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스를 상기 초기화 언어 모델에 입력하고,상기 초기화 언어 모델을 통해 상기 적어도 하나의 제2 단어 마스크 중의 각각의 상기 제2 단어 마스크의 콘텍스트 벡터를 출력하는 단계;각각의 상기 제2 단어 마스크의 콘텍스트 벡터 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭스에 기반하여각각의 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하는 단계; 및제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 적어도 하나의 제2 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭스를 트레이닝하는 단계를 포함하는 것을 특징으로 하는,언어 모델 트레이닝 방법."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어를 각각 제2 단어 마스크로 교체하는 단계는,상기 제2 샘플 텍스트 코퍼스에 대해 단어 분할을 실행하고, 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트코퍼스 중의 적어도 하나의 단어 중의 각각의 단어를 각각 하나의 제2 단어 마스크로 교체하는 단계를 포함하는것을 특징으로 하는,언어 모델 트레이닝 방법."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제5항 중 어느 한 항에 있어서,상기 언어 모델은 지식 강화 시맨틱 표현 (ERNIE) 모델을 포함하고, 및/또는,상기 제2 언어 모델은 연속적인 배그-오프-워드 (CBOW) 모델을 포함하는 것을 특징으로 하는,언어 모델 트레이닝 방법."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 장치에 있어서,제1 단어 마스크를 포함하는 제1 샘플 텍스트 코퍼스를 입력하고, 상기 제1 단어 마스크의 콘텍스트 벡터를 출력하기 위한 언어 모델;상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제1 확률 분포 매트릭스를 얻고, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻기 위한 취득 유닛 - 상기 제1 단어 벡터매개 변수 매트릭스는 상기 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이며, 상기제2 단어 벡터 매개 변수 매트릭스는 제2 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스임 -;상기 제1 확률 분포 매트릭스 및 상기 제2 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하기 위한 제1 결정 유닛; 및제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여상기 언어 모델을 트레이닝 하기 위한 제1 트레이닝 유닛;을 구비하는 것을 특징으로 하는,언어 모델 트레이닝 장치.공개특허 10-2021-0148872-5-청구항 11 제10항에 있어서,상기 취득 유닛은 또한 상기 제1 단어 마스크의 콘텍스트 벡터 및 완전 연결 매트릭스에 기반하여 상기 제1 단어 마스크의 제3 확률 분포 매트릭스를 얻고,상기 제1 결정 유닛은 구체적으로 상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 것을 특징으로 하는,언어 모델 트레이닝 장치."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 취득 유닛은 구체적으로,상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제1 단어 벡터 매개 변수 매트릭스를 곱셈하여, 상기 제1 단어마스크의 제1 확률 분포 매트릭스를 얻고, 및/또는,상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제2 단어 벡터 매개 변수 매트릭스를 곱셈하여, 상기 제1 단어마스크의 제2 확률 분포 매트릭스를 얻으며, 및/또는,상기 제1 단어 마스크의 콘텍스트 벡터와 상기 완전 연결 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제3 확률 분포 매트릭스를 얻는 것을 특징으로 하는,언어 모델 트레이닝 장치."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스를 가산하여 토탈확률 분포 매트릭스를 얻기 위한 가산 유닛; 및상기 토탈 확률 분포 매트릭스 중의 확률 값에 대해 정규화 처리를 실행하여, 상기 제1 단어 마스크가 복수의단어 벡터에 대응하는 복수의 정규화 확률 값을 취득하기 위한 정규화 유닛;을 더 구비하며,상기 제1 결정 유닛은 구체적으로 상기 복수의 정규화 확률 값에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 것을 특징으로 하는,언어 모델 트레이닝 장치."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 제1 트레이닝 유닛은 구체적으로,상기 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델 및 상기 완전 연결 매트릭스를 트레이닝하는 것을 특징으로 하는,언어 모델 트레이닝 장치."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제10항 내지 제14항 중 어느 한 항에 있어서,제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 초기화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매트릭스를 트레이닝 하여 상기 언어 모델 및 상기 제1 단어 벡터 매개 변수 매트릭스를 얻기 위한 제2 트레이닝유닛;공개특허 10-2021-0148872-6-을 더 구비하는 것을 특징으로 하는,언어 모델 트레이닝 장치."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,사전에 코퍼스 베이스 중의 기설정된 텍스트 코퍼스를 사용하여 상기 초기화 언어 모델에 대해 사전 트레이닝을실행하기 위한 사전 트레이닝 유닛;제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어를 각각 제2 단어 마스크로 교체하여 적어도 하나의 제2 단어마스크를 포함하는 제2 샘플 텍스트 코퍼스를 얻기 위한 교체 유닛;상기 초기화 언어 모델에 입력하기 위한 교체 유닛;상기 교체 유닛에 의해 입력되는 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스에 기반하여 상기 적어도 하나의 제2 단어 마스크 중의 각각의 상기 제2 단어 마스크의 콘텍스트 벡터를 출력하기 위한상기 초기화 언어 모델; 및각 상기 제2 단어 마스크의 콘텍스트 벡터 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭스에 기반하여 각각의 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하기 위한 제2 결정 유닛;을 더 구비하며,상기 제2 트레이닝 유닛은 구체적으로 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 적어도 하나의 제2 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭스를 트레이닝하는 것을 특징으로 하는,언어 모델 트레이닝 장치."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 교체 유닛은 구체적으로,상기 제2 샘플 텍스트 코퍼스에 대해 단어 분할을 실행하고, 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트코퍼스 중의 적어도 하나의 단어 중의 각각의 단어를 각각 하나의 제2 단어 마스크로 교체하는 것을 특징으로하는,언어 모델 트레이닝 장치."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제10항 내지 제14항 중 어느 한 항에 있어서,상기 언어 모델은 지식 강화 시맨틱 표현(ERNIE) 모델을 포함하고, 및/또는,상기 제2 언어 모델은 연속적인 배그-오프-워드(CBOW) 모델을 포함하는것을 특징으로 하는,언어 모델 트레이닝 장치."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "전자 기기에 있어서,적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 구비하며,상기 메모리에는 상기 적어도 하나의 프로세서가 실행 가능한 명령이 기록되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제9항 중 어느 한 항공개특허 10-2021-0148872-7-에 기재된 방법을 실행하도록 하는 것을 특징으로 하는,전자 기기."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "컴퓨터 명령이 기록되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서,상기 컴퓨터 명령은 제1항 내지 제9항 중 어느 한 항에 기재된 방법을 컴퓨터에 실행시키기 위한 것인 것을 특징으로 하는,기록 매체."}
{"patent_id": "10-2021-0035543", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "비 일시적 컴퓨터 판독 가능 기록 매체에 기록되어 있는 컴퓨터 프로그램에 있어서,상기 컴퓨터 프로그램은 컴퓨터로 하여금 제1항 내지 제9항 중 어느 한 항에 기재된 방법을 실행하도록 하는 것을 특징으로 하는,컴퓨터 프로그램."}
{"patent_id": "10-2021-0035543", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 출원은 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법, 장치, 기기 및 매체를 개시하는 바, 인공 지능 중의 자연 언어 처리의"}
{"patent_id": "10-2021-0035543", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것이다. 구체적인 실현 방안은 제1 단어 마스크를 포함 하는 제1 샘플 텍스트 코퍼스를 언어 모델에 입력하고, 상기 언어 모델을 통해 상기 제1 단어 마스크의 콘텍스트 (뒷면에 계속) 대 표 도 - 도1"}
{"patent_id": "10-2021-0035543", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2021-0148872 벡터를 출력하는 단계; 상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제1 확률 분포 매트릭스를 얻고, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡 터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻는 단계; 및 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델을 트레이닝하는 단계를 포함한다. 복수 종류의 고 품질 단어 벡터를 결합하여 언어 모델을 트레이닝 함으로써, 언어 모델로 하여금 멀티 소스의 고품질 시맨틱 정 보를 학습하도록 하여, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰고, 언어 모델의 예측 성능을 향상 시켰으며, 글자 입도에 기반하는 학습에 의해 야기되는 정보 누설 리스크를 피할 수 있다. CPC특허분류 G06F 40/30 (2020.01) G06K 9/6277 (2013.01) G06N 3/0454 (2013.01)명 세 서 청구범위 청구항 1 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법에 있어서, 제1 단어 마스크(first word mask)를 포함하는 제1 샘플 텍스트 코퍼스(sample text corpus)를 언어 모델에 입 력하고, 상기 언어 모델을 통해 상기 제1 단어 마스크의 콘텍스트 벡터(context vector)를 출력하는 단계; 상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스(first word vector parameter matrix)에 기반하여 상기 제1 단어 마스크의 제1 확률 분포 매트릭스(first probability distribution)를 얻고, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭스(second word vector parameter matrix)에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스(second probability distribution)를 얻는 단계 - 상기 제1 단어 벡터 매개 변수 매트릭스는 상기 언어 모델에 대응하는 사전에 트 레이닝된 단어 벡터 매개 변수 매트릭스이며, 상기 제2 단어 벡터 매개 변수 매트릭스는 제2 언어 모델에 대응 하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스임 -; 상기 제1 확률 분포 매트릭스 및 상기 제2 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단 어 벡터(word vector)를 결정하는 단계; 및 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델을 트레이닝하는 단계; 를 포함하는 것을 특징으로 하는, 언어 모델 트레이닝 방법. 청구항 2 제1항에 있어서, 상기 제1 단어 마스크를 포함하는 샘플 텍스트 코퍼스를 언어 모델에 입력하고, 상기 언어 모델을 통해 상기 제 1 단어 마스크의 콘텍스트 벡터를 출력하는 단계 이후에, 상기 방법은, 상기 제1 단어 마스크의 콘텍스트 벡터 및 완전 연결 매트릭스에 기반하여 상기 제1 단어 마스크의 제3 확률 분 포 매트릭스를 얻는 단계를 더 포함하며, 상기 제1 확률 분포 매트릭스 및 상기 제2 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단 어 벡터를 결정하는 단계는, 상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계를 포함하는 것을 특징으로 하는, 언어 모델 트레이닝 방법. 청구항 3 제2항에 있어서, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크 의 제1 확률 분포 매트릭스를 얻는 단계는, 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제1 단어 벡터 매개 변수 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제1 확률 분포 매트릭스를 얻는 단계를 포함하고, 및/또는, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻는 단계는, 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제2 단어 벡터 매개 변수 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻는 단계를 포함하고, 및/또는, 상기 제1 단어 마스크의 콘텍스트 벡터 및 완전 연결 매트릭스에 기반하여 상기 제1 단어 마스크의 제3 확률 분 포 매트릭스를 얻은 단계는, 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 완전 연결 매트릭스를 곱셈하여, 상 기 제1 단어 마스크의 제3 확률 분포 매트릭스를 얻는 단계를 포함하는 것을 특징으로 하는, 언어 모델 트레이닝 방법. 청구항 4 제2항에 있어서, 상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계는, 상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스를 가산하여 토탈 확률 분포 매트릭스를 얻는 단계; 상기 토탈 확률 분포 매트릭스 중의 확률 값에 대해 정규화 처리를 실행하여, 상기 제1 단어 마스크가 복수의 단어 벡터에 대응하는 복수의 정규화 확률 값을 얻는 단계; 및 상기 복수의 정규화 확률 값에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 단계를 포함하 는 것을 특징으로 하는, 언어 모델 트레이닝 방법. 청구항 5 제2항에 있어서, 상기 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반 하여 상기 언어 모델을 트레이닝하는 단계는, 상기 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반 하여 상기 언어 모델 및 상기 완전 연결 매트릭스를 트레이닝하는 단계를 포함하는 것을 특징으로 하는, 언어 모델 트레이닝 방법. 청구항 6 제1항 내지 제5항 중 어느 한 항에 있어서, 상기 제1 단어 마스크를 포함하는 제1 샘플 텍스트 코퍼스를 상기 언어 모델에 입력하고, 상기 언어 모델을 통 해 상기 제1 단어 마스크의 콘텍스트 벡터를 출력하는 단계의 전에, 상기 방법은, 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 초기화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매 트릭스를 트레이닝 하여 상기 언어 모델 및 상기 제1 단어 벡터 매개 변수 매트릭스를 얻은 단계를 더 포함하는 것을 특징으로 하는, 언어 모델 트레이닝 방법. 청구항 7 제6항에 있어서, 상기 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 초기화 언어 모델 및 초기화 제1 단어 벡터 매 개 변수 매트릭스를 트레이닝하는 단계는, 사전에 코퍼스 베이스 중의 기설정된 텍스트 코퍼스를 사용하여 상기 초기화 언어 모델에 대해 사전 트레이닝을실행하는 단계; 제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어를 각각 제2 단어 마스크로 교체하여 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스를 얻는 단계; 상기 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스를 상기 초기화 언어 모델에 입력하고, 상기 초기화 언어 모델을 통해 상기 적어도 하나의 제2 단어 마스크 중의 각각의 상기 제2 단어 마스크의 콘텍 스트 벡터를 출력하는 단계; 각각의 상기 제2 단어 마스크의 콘텍스트 벡터 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭스에 기반하여 각각의 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하는 단계; 및 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 적어도 하나의 제2 단어 마스크에 대응하는 단어 벡 터에 기반하여 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭스를 트레이닝하는 단계 를 포함하는 것을 특징으로 하는, 언어 모델 트레이닝 방법. 청구항 8 제7항에 있어서, 상기 제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어를 각각 제2 단어 마스크로 교체하는 단계는, 상기 제2 샘플 텍스트 코퍼스에 대해 단어 분할을 실행하고, 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어 중의 각각의 단어를 각각 하나의 제2 단어 마스크로 교체하는 단계를 포함하는 것을 특징으로 하는, 언어 모델 트레이닝 방법. 청구항 9 제1항 내지 제5항 중 어느 한 항에 있어서, 상기 언어 모델은 지식 강화 시맨틱 표현 (ERNIE) 모델을 포함하고, 및/또는, 상기 제2 언어 모델은 연속적인 배그-오프-워드 (CBOW) 모델을 포함하는 것을 특징으로 하는, 언어 모델 트레이닝 방법. 청구항 10 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 장치에 있어서, 제1 단어 마스크를 포함하는 제1 샘플 텍스트 코퍼스를 입력하고, 상기 제1 단어 마스크의 콘텍스트 벡터를 출 력하기 위한 언어 모델; 상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크 의 제1 확률 분포 매트릭스를 얻고, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭 스에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻기 위한 취득 유닛 - 상기 제1 단어 벡터 매개 변수 매트릭스는 상기 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이며, 상기 제2 단어 벡터 매개 변수 매트릭스는 제2 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭 스임 -; 상기 제1 확률 분포 매트릭스 및 상기 제2 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단 어 벡터를 결정하기 위한 제1 결정 유닛; 및 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델을 트레이닝 하기 위한 제1 트레이닝 유닛; 을 구비하는 것을 특징으로 하는, 언어 모델 트레이닝 장치.청구항 11 제10항에 있어서, 상기 취득 유닛은 또한 상기 제1 단어 마스크의 콘텍스트 벡터 및 완전 연결 매트릭스에 기반하여 상기 제1 단 어 마스크의 제3 확률 분포 매트릭스를 얻고, 상기 제1 결정 유닛은 구체적으로 상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확 률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정하는 것을 특징으로 하는, 언어 모델 트레이닝 장치. 청구항 12 제11항에 있어서, 상기 취득 유닛은 구체적으로, 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제1 단어 벡터 매개 변수 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제1 확률 분포 매트릭스를 얻고, 및/또는, 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제2 단어 벡터 매개 변수 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻으며, 및/또는, 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 완전 연결 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제3 확 률 분포 매트릭스를 얻는 것을 특징으로 하는, 언어 모델 트레이닝 장치. 청구항 13 제11항에 있어서, 상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스를 가산하여 토탈 확률 분포 매트릭스를 얻기 위한 가산 유닛; 및 상기 토탈 확률 분포 매트릭스 중의 확률 값에 대해 정규화 처리를 실행하여, 상기 제1 단어 마스크가 복수의 단어 벡터에 대응하는 복수의 정규화 확률 값을 취득하기 위한 정규화 유닛; 을 더 구비하며, 상기 제1 결정 유닛은 구체적으로 상기 복수의 정규화 확률 값에 기반하여 상기 제1 단어 마스크에 대응하는 단 어 벡터를 결정하는 것을 특징으로 하는, 언어 모델 트레이닝 장치. 청구항 14 제11항에 있어서, 상기 제1 트레이닝 유닛은 구체적으로, 상기 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반 하여 상기 언어 모델 및 상기 완전 연결 매트릭스를 트레이닝하는 것을 특징으로 하는, 언어 모델 트레이닝 장치. 청구항 15 제10항 내지 제14항 중 어느 한 항에 있어서, 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 초기화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매 트릭스를 트레이닝 하여 상기 언어 모델 및 상기 제1 단어 벡터 매개 변수 매트릭스를 얻기 위한 제2 트레이닝 유닛;을 더 구비하는 것을 특징으로 하는, 언어 모델 트레이닝 장치. 청구항 16 제15항에 있어서, 사전에 코퍼스 베이스 중의 기설정된 텍스트 코퍼스를 사용하여 상기 초기화 언어 모델에 대해 사전 트레이닝을 실행하기 위한 사전 트레이닝 유닛; 제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어를 각각 제2 단어 마스크로 교체하여 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스를 얻기 위한 교체 유닛; 상기 초기화 언어 모델에 입력하기 위한 교체 유닛; 상기 교체 유닛에 의해 입력되는 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스에 기반하 여 상기 적어도 하나의 제2 단어 마스크 중의 각각의 상기 제2 단어 마스크의 콘텍스트 벡터를 출력하기 위한 상기 초기화 언어 모델; 및 각 상기 제2 단어 마스크의 콘텍스트 벡터 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭스에 기반하여 각각 의 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하기 위한 제2 결정 유닛; 을 더 구비하며, 상기 제2 트레이닝 유닛은 구체적으로 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 적어도 하나 의 제2 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 매 개 변수 매트릭스를 트레이닝하는 것을 특징으로 하는, 언어 모델 트레이닝 장치. 청구항 17 제16항에 있어서, 상기 교체 유닛은 구체적으로, 상기 제2 샘플 텍스트 코퍼스에 대해 단어 분할을 실행하고, 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어 중의 각각의 단어를 각각 하나의 제2 단어 마스크로 교체하는 것을 특징으로 하는, 언어 모델 트레이닝 장치. 청구항 18 제10항 내지 제14항 중 어느 한 항에 있어서, 상기 언어 모델은 지식 강화 시맨틱 표현(ERNIE) 모델을 포함하고, 및/또는, 상기 제2 언어 모델은 연속적인 배그-오프-워드(CBOW) 모델을 포함하는 것을 특징으로 하는, 언어 모델 트레이닝 장치. 청구항 19 전자 기기에 있어서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 구비하며, 상기 메모리에는 상기 적어도 하나의 프로세서가 실행 가능한 명령이 기록되어 있으며, 상기 명령이 상기 적어 도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제9항 중 어느 한 항에 기재된 방법을 실행하도록 하는 것을 특징으로 하는, 전자 기기. 청구항 20 컴퓨터 명령이 기록되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서, 상기 컴퓨터 명령은 제1항 내지 제9항 중 어느 한 항에 기재된 방법을 컴퓨터에 실행시키기 위한 것인 것을 특 징으로 하는, 기록 매체. 청구항 21 비 일시적 컴퓨터 판독 가능 기록 매체에 기록되어 있는 컴퓨터 프로그램에 있어서, 상기 컴퓨터 프로그램은 컴퓨터로 하여금 제1항 내지 제9항 중 어느 한 항에 기재된 방법을 실행하도록 하는 것 을 특징으로 하는, 컴퓨터 프로그램. 발명의 설명 기 술 분 야 컴퓨터 기술의 분야에 관한 것으로, 구체적으로 인공 지능에서 자연 언어 처리 기술에 관한 것이며, 특히 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법, 장치, 기기 및 기록 매체에 관한 것이다."}
{"patent_id": "10-2021-0035543", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "중국어 자연 언어 처리(Natural Language Processing, NLP)의 분야에 있어서, 대량의 비지도 학습 텍스트 (unsupervised learning text)를 사용하여 언어 모델 자가 지도 학습의 사전 트레이닝(pre-training)을 실행한 후 지도 학습 태스크 데이터를 사용하여 언어 모델에 대해 매개 변수 미세 조정(fine-tuning)을 실행하는 것은, 현재의 NLP 분야의 선진적의 언어 모델 트레이닝 기술이다. 종래의 기술에서는 언어 모델 자가 지도 학습의 사전 트레이닝 과정에서, 언어 모델의 트레이닝 효과가 토크 나 이저의 성능의 영향을 받지 않게 하기 위하여, 글자 입도에 기반하여 언어 모델의 자가 지도 학습의 사전 트레 이닝을 실행하고 있기에, 언어 모델이 더 큰 시맨틱 입도 (예를 들면, 단어)의 정보를 학습하기 어렵다. 따라서, 정보 누설 리스크가 존재할 가능성이 있고, 언어 모델의 단어 자체의 시맨틱에 대한 학습을 파괴할 가 능성이 있기에, 언어 모델의 예측 성능에 불리한 영향을 준다."}
{"patent_id": "10-2021-0035543", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 출원의 복수 양태는 글자 입도에 기반한 학습에 의해 야기되는 정보 누설 리스크를 피하고, 언어 모델의 시 맨틱 정보에 대한 학습 능력을 강화시키며, 언어 모델의 예측 성능을 향상시키기 위한, 복수 종류의 단어 벡터 에 기반하여 언어 모델을 트레이닝하는 방법, 장치, 기기 및 매체를 제공한다."}
{"patent_id": "10-2021-0035543", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "제1 양태에 따르면, 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법을 제공하는 바, 당해 방 법은, 제1 단어 마스크를 포함하는 제1 샘플 텍스트 코퍼스를 언어 모델에 입력하고, 상기 언어 모델을 통해 상기 제1 단어 마스크의 콘텍스트 벡터를 출력하는 단계; 상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크 의 제1 확률 분포 매트릭스를 얻고, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭 스에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻는 단계 - 상기 제1 단어 벡터 매개 변수매트릭스는 상기 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이며, 상기 제2 단어 벡터 매개 변수 매트릭스는 제2 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스임 -; 상기 제1 확률 분포 매트릭스 및 상기 제2 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단 어 벡터를 결정하는 단계; 및 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델을 트레이닝하는 단계를 포함한다. 제2 양태에 따르면, 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 장치를 제공하는 바, 당해 장 치는, 제1 단어 마스크를 포함하는 제1 샘플 텍스트 코퍼스를 언어 모델에 입력하고, 상기 제1 단어 마스크의 콘텍스 트 벡터를 출력하기 위한 언어 모델; 상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크 의 제1 확률 분포 매트릭스를 얻고, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭 스에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻기 위한 취득 유닛 - 상기 제1 단어 벡터 매개 변수 매트릭스는 상기 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이며, 상기 제2 단어 벡터 매개 변수 매트릭스는 제2 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭 스임 -; 상기 제1 확률 분포 매트릭스 및 상기 제2 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단 어 벡터를 결정하기 위한 제1 결정 유닛; 및 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델을 트레이닝 하기 위한 제1 트레이닝 유닛을 구비한다. 제3 양태에 따르면, 전자 기기를 제공하는 바, 당해 전자 기기는, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결되는 메모리를 구비하며, 상기 메모리에는 상기 적어도 하나의 프로세서가 실행 가능한 명령이 기록되어 있으며, 상기 명령이 상기 적어 도 하나의 프로세서에 의해 실행되어, 상기 적어도 하나의 프로세서로 하여금 상기에 기재된 양태 및 임의의 가 능한 실현 방식의 방법을 실행하도록 한다. 제4 양태에 따르면, 컴퓨터 명령이 기록되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체를 제공하는 바, 상기 컴퓨터 명령은 상기에 기재된 양태 및 임의의 가능한 실현 방식의 방법을 컴퓨터에 실행시킨다."}
{"patent_id": "10-2021-0035543", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기의 기술 방안으로부터 알 수 있듯이, 본 출원의 실시예는 제1 단어 마스크를 포함하는 제1 샘플 텍스트 코 퍼스를 언어 모델에 입력하고, 언어 모델을 통해 제1 단어 마스크의 콘텍스트 벡터를 출력하며, 당해 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 제1 확률 분포 매트릭스를 얻되, 제1 단어 벡터 매개 변수 매트릭스는 상기 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스 이며, 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 제2 확률 분포 매트 릭스를 얻되, 제2 단어 벡터 매개 변수 매트릭스는 제2 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매 개 변수 매트릭스이며, 그리고, 제1 확률 분포 매트릭스 및 제2 확률 분포 매트릭스에 기반하여 제1 단어 마스 크에 대응하는 단어 벡터를 결정하고, 또한 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 제1 단어 마 스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델을 트레이닝 함으로써, 트레이닝된 언어 모델을 얻을 수 있다. 본 출원의 실시예는 제2 언어 모델에 대응하는 제2 단어 벡터 매개 변수 매트릭스를 도입하는 동시에, 제 1 단어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 복수 종류의 고품질 단어 벡 터를 결합하여 언어 모델을 트레이닝 함으로써, 언어 모델로 하여금 멀티 소스의 고품질 시맨틱 정보를 학습하 도록 하여, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰고, 언어 모델의 예측 성능을 향상시켰다. 또한, 본 출원에 의해 제공되는 기술 방안을 이용하면, 마스크를 포함하는 샘플 텍스트 코퍼스를 사용하여 언어 모델을 트레이닝 하며, 글자 벡터와 비교하면 단어 벡터가 풍부한 시맨틱 정보 표현을 포함하기에, 단어 마스크방식을 이용해 콘텍스트에 기반하여 단어 벡터를 모델링 함으로써, 언어 모델의 시맨틱 정보에 대한 모델링을 강화시켰고, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰으며, 글자의 전체 단어 마스킹(Whole Word Masking)에 의해 야기될 가능성이 있는 정보 누설 리스크를 효과적으로 피할 수 있다. 상기의 발명의 개요에 기재된 내용은 본 개시의 실시예의 관건적인 특징 또는 중요한 특징을 나타냄을 의도하는 것이 아니며, 본 개시의 범위를 한정하려는 것을 의도하는 것이 아님을 이해해야 한다. 본 개시의 기타 특징은 이하의 명세서에 의해 용이하게 이해될 것이다."}
{"patent_id": "10-2021-0035543", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 발명의 시범적인 실시예를 설명하는 바, 본 발명에 대한 이해를 돕기 위해 여기에는 본 발명 실시예의 다양한 세부 사항이 포함되며, 이러한 세부 사항을 단지 시범적인 것으로 간주해야 할 것이다. 따라서, 당업자는 본 발명의 범위 및 정신을 벗어나지 않는 전제 하에서, 여기서 설명되는 실시예에 대 해 다양한 변경 및 수정을 수행할 수 있음을 인식해야 한다. 마찬가지로, 명확성 및 간결성을 위하여 이하의 설 명에서는 잘 알려진 기능 및 구조에 대한 설명을 생략한다. 물론, 설명되는 실시예는 본 출원의 일부 실시예 일 뿐, 전부의 실시예가 아니다. 당업자가 본 출원의 실시예에 기반하여 발명적인 노력을 하지 않는 전제 하에서 얻을 수 있는 모든 기타 실시예는 본 출원의 보호하는 범위에 속한다. 본 출원의 실시예에 관한 단말은 휴대전화, 개인용 디지털 단말기(PDA: Personal Digital Assistant), 무선 휴 대 기기, 태블릿 컴퓨터(Tablet Computer), 컴퓨터(PC: Personal Computer), MP3 플레이어, MP4 플레이어, 웨 어러블 디바이스 (예를 들면, 스마트 안경, 스마트 시계, 스마트 팔찌 등) 등을 포함할 수 있지만, 이에 한정되 지 않음을 설명할 필요가 있다. 또한, 본 명세서 중의 용어인 \"및/또는”은 단지 관련 대상의 관련 관계를 기술하는 용어인 바, 예를 들면, \"A 및/또는 B\"는 A가 단독으로 존재하는 것, A와 B가 동시에 존재하는 것, B가 단독으로 존재하는 것과 같은 세 가 지 관계의 존재를 나타낼 수 있다. 또한, 본 명세서 중의 캐릭터인 \"/\"은 일반적으로 전후의 관련대상이 \"혹은 ”의 관계인 것을 나타낸다. 종래의 기술에서는 언어 모델의 자가 지도 학습(self-supervised learning)의 사전 트레이닝에 있어서, 글자 (letter) 입도에 기반하여 언어 모델의 자가 지도 학습의 사전 트레이닝을 실행하고 있기 때문에, 언어 모델의 더 큰 시맨틱 입도 (예를 들면 단어)의 정보의 학습을 어렵게 하며, 정보 누설의 리스트가 존재할 가능성이 있 고, 언어 모델의 단어(word) 자체의 시맨틱에 대한 학습을 파괴할 가능성이 있기에, 언어 모델의 예측 성능에 불리한 영향을 준다. 예를 들면, 종래의 언어 모델 중의 지식이 강화된 시맨틱 표현 (Enhanced Representation from kNowledge IntEgration, ERNIE) 모델의 사전 트레이닝에서는, 글자의 전체 단어 마스킹 방식을 이용하여 ERNIE 모델이 실 체의 표현을 학습하도록 한다. 그러나, 글자의 전체 단어 마스킹 방식은 아직 예를 들면 단어 벡터와 같은 더욱 큰 시맨틱 입도의 정보를 명시적으로 도입하지 않고 있으며, 또한 정보 누설 리스크가 존재할 가능성이 있다.예를 들면, 텍스트인 \"하얼빈은 흑룡강의 성소재지이다”에 대하여, 각각 \"하”, \"얼”, \"빈”이라는 3개의 글 자를 3개의 마스크 (MASK)로 교체하여, \"[MASK] [MASK] [MASK]은 흑룡강의 성소재지이다”를 취득하며, ERNIE 모델에 의해 3개의 [MASK]에 대응하는 \"하”, \"얼”, \"빈” 이 3개의 글자가 학습되는 것을 기대하는 바, 예측 하려는 정보가 3개의 글자로 구성되었다는 것을 ERNIE 모델에 사전에 알리는 것과 같으므로, 이러한 정보는 모 델의 단어 자체의 시맨틱에 대한 학습을 파괴할 가능성이 있다. 상기의 문제에 대해, 본 출원은 글자 입도에 기반한 학습에 의해 야기되는 정보 누설 리스크를 피하고, 언어 모 델의 시맨틱 정보에 대한 학습 능력을 강화시키며, 언어 모델의 예측 성능을 향상시키기 위하여, 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법, 장치, 기기 및 매체를 제안한다. 도 1은 본 출원의 제1 실시예에 따른 모식도인 바, 도 1에 나타낸 바와 같다. 101에 있어서, 제1 단어 마스크를 포함하는 제1 샘플 텍스트 코퍼스를 언어 모델에 입력하고, 상기 언어 모델을 통해 상기 제1 단어 마스크의 콘텍스트 벡터를 출력한다. 102에 있어서, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제1 확률 분포 매트릭스를 얻고, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 취득한다. 여기서, 상기 제1 단어 벡터 매개 변수 매트릭스는 상기 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매 개 변수 매트릭스이며, 상기 제2 단어 벡터 매개 변수 매트릭스는 제2 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스다. 103에 있어서, 상기 제1 확률 분포 매트릭스 및 상기 제2 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크 에 대응하는 단어 벡터를 결정한다. 104에 있어서, 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡 터에 기반하여 상기 언어 모델을 트레이닝 하여 즉 상기 언어 모델 및 그 중의 매개 변수 값을 조정한다. 본 출원의 실시예에 있어서, 단어집을 이용하여 가능한 단어를 포함할 수 있으며, 제1 단어 벡터 매개 변수 매 트릭스 및 제2 단어 벡터 매개 변수 매트릭스는 각각 복수의 단어의 단어 벡터를 포함하는 매트릭스이고, 제1 단어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스 중의 단어 벡터는 단어집 중의 각각의 단 어의 단어 벡터이며, 제1 단어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스의 차원은 동일한 바, [단어 벡터의 차원, 단어집의 사이즈]로 나타낼 수 있으며, 여기서의 단어집의 사이즈는 단어집에 포함되는 단어의 수량이다. 여기서, 제1 확률 분포 매트릭스는 상기 제1 단어 마스크가 상기 제1 단어 벡터 매개 변수 매 트릭스에 기반하여 단어집 중의 각각의 단어 벡터에 각각 대응하는 확률 값을 나타내고, 제2 확률 분포 매트릭 스는 상기 제1 단어 마스크가 상기 제1 단어 벡터 매개 변수 매트릭스에 기반하여 단어집 중의 각각의 단어 벡 터에 각각 대응하는 확률 값을 나타낸다. 상기 제1 단어 벡터 매개 변수 매트릭스는 상기 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이므로, 단어집 중의 각각의 단어의 단어 벡터를 정확하게 나타낼 수 있다. 상기 제2 단어 벡터 매개 변수 매트릭스는 상기 제2 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이므로, 단어 집 중의 각각의 단어의 단어 벡터를 정확하게 나타낼 수도 있다. 상기 언어 모델로 하여금 더 많은 풍부한 시맨 틱 정보를 학습할 수 있도록 하기 위하여, 제2 언어 모델을 이용하여 트레이닝된 단어 벡터 (제2 단어 벡터 매 개 변수 매트릭스)를 도입하여 추가로 언어 모델을 트레이닝 한다. 여기서, 상기의 101∼104는 반복적으로 실행하는 과정일 수 있으며, 101∼104를 반복적으로 실행함으로써 언어 모델의 트레이닝을 실현하며, 제1 기설정된 트레이닝 완료 조건을 충족시키면 언어 모델의 트레이닝이 완료된다. 트레이닝된 언어 모델은 102∼103을 통해 하나의 텍스트 중의 제1 단어 마스크에 대응하는 단어 벡터 를 정확하게 출력할 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 상기 제1 기설정된 트레이닝 완료 조건은 실제의 수요 에 따라 설정될 수 있는 바, 예를 들면 언어 모델에 의해 출력되는 단어 벡터의 제1 샘플 텍스트 코퍼스에 대응 하는 퍼플렉서티(perplexity)가 제1 소정의 임계 값에 달하는 것, 및/또는, 언어 모델을 트레이닝 한 횟수 (즉 101∼104를 반복적으로 실행하는 횟수)가 제2 소정의 임계 값에 달하는 것을 포함할 수 있다. 여기서, 101∼104의 실행 주체의 일부 또는 전부는 로컬 단말에 위치하는 애플리케이션이나, 로컬 단말에 위치 하는 애플리케이션에 인스톨된 플러그인 또는 소프트웨어 개발 킷(SDK: Software Development Kit) 등의 기능유닛일 수 있으며, 또는 네트워크 측의 서버에 위치하는 처리 엔진일 수도 있는 바, 본 실시예에서는 이에 대해 특별히 한정하지 않음을 설명할 필요가 있다. 상기 애플리케이션은 단말에 인스톨된 로컬 프로그램(nativeApp) 또는 단말 상의 브라우저의 웹 애플리케이션 (webApp)일 수도 있는 바, 본 실시예에서는 이에 대해 한정하지 않음을 이해해야 된다. 본 실시예에 있어서, 제2 언어 모델에 대응하는 제2 단어 벡터 매개 변수 매트릭스를 도입하는 동시에, 제1 단 어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 복수 종류의 고품질 단어 벡터를 결합하여 언어 모델을 트레이닝 함으로써, 언어 모델로 하여금 멀티 소스의 고품질 시맨틱 정보를 학습하도록 하여, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰고, 언어 모델의 예측 성능을 향상시켰다. 또한, 본 출원에 의해 제공되는 기술 방안을 이용하면, 마스크를 포함하는 샘플 텍스트 코퍼스를 사용하여 언어 모델을 트레이닝 하며, 글자 벡터와 비교하면 단어 벡터가 풍부한 시맨틱 정보 표현을 포함하기에, 단어 마스크 방식을 이용해 콘텍스트에 기반하여 단어 벡터를 모델링 함으로써, 언어 모델의 시맨틱 정보에 대한 모델링을 강화시켰고, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰으며, 글자에 기반한 단어 완전 마스킹에 의 해 야기될 가능성이 있는 정보 누설 리스크를 효과적으로 피할 수 있다. 구체적인 일 실현 방식에 있어서, 단어집을 이용하여 가능한 단어를 포함할 수 있으며, 제1 단어 벡터 매개 변 수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스에는 각각 복수의 단어의 단어 벡터의 구체적인 표현이 포함 되며, 제1 단어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스 중의 단어 벡터는 단어집 중의 각각의 단어의 단어 벡터이므로, 제1 단어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스는 단 어 벡터의 세트 또는 전체 단어 벡터라고도 불린다. 각각의 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제1 단어 벡터 매개 변수 매트릭스를 곱셈하여, 각각의 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제1 단어 벡 터 매개 변수 매트릭스 중의 각각의 단어 벡터의 관련성을 얻을 수 있고, 따라서 각각의 상기 제1 단어 마스크 가 상기 제1 단어 벡터 매개 변수 매트릭스에 기반하여 단어집 중의 각각의 단어 벡터에 각각 대응하는 확률 값 을 얻되, 각각의 확률 값은 상기 제1 단어 마스크가 하나의 단어 벡터에 대응하는 확률을 나타낸다. 마찬가지로, 각각의 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제2 단어 벡터 매개 변수 매트릭스를 곱셈하 여, 각각의 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제2 단어 벡터 매개 변수 매트릭스 중의 각각의 단어 벡터의 관련성을 얻을 수 있고, 각각의 상기 제1 단어 마스크가 상기 제1 단어 벡터 매개 변수 매트릭스에 기반 하여 단어집 중의 각각의 단어 벡터에 각각 대응하는 확률 값을 얻되, 각각의 확률 값은 상기 제1 단어 마스크 가 하나의 단어 벡터에 대응하는 확률을 나타낸다. 도 2는 본 출원의 제2 실시예에 따른 모식도인 바, 도 2에 나타낸 바와 같다. 201에 있어서, 제1 단어 마스크를 포함하는 제1 샘플 텍스트 코퍼스를 언어 모델에 입력하고, 상기 언어 모델을 통해 상기 제1 단어 마스크의 콘텍스트 벡터를 출력한다. 202에 있어서, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제1 확률 분포 매트릭스를 얻고, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻으며, 상기 제1 단어 마스 크의 콘텍스트 벡터 및 완전 연결(Fully Connect, FC) 매트릭스에 기반하여 상기 제1 단어 마스크의 제3 확률 분포 매트릭스를 취득한다. 여기서, 상기 제1 단어 벡터 매개 변수 매트릭스는 상기 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매 개 변수 매트릭스이며, 상기 제2 단어 벡터 매개 변수 매트릭스는 제2 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이다. 상기 완전 연결 매트릭스는 초기화된 트레이닝 되어 있지 않은 매트릭스일 수 있다. 203에 있어서, 상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정한다. 구체적인 일 예에 있어서, 트레이닝에 참여하는 제1 단어 마스크에 대응하는 단어의 수량 (샘플 수라고도 불 림)이 batch_size이고, 각각의 단어의 단어 벡터의 차원이 embedding_size이며, 단어집의 사이즈가 vocab_size 이라고 가정하면, 상기 언어 모델에 의해 출력되는 단어 벡터의 차원은 [batch_size, embedding_size]이고, 상 기 제1 단어 벡터 매개 변수 매트릭스, 상기 제2 단어 벡터 매개 변수 매트릭스 및 완전 연결 매트릭스의 차원 은 모두 [embedding_size, vocab_size]이며, 상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및상기 제3 확률 분포 매트릭스의 차원은 모두 [batch_size, vocab_size]이다. 204에 있어서, 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡 터에 기반하여 상기 언어 모델을 트레이닝 한다. 본 실시예에 있어서, 제1 단어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스는 각각 서로 다 른 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이므로, 2개의 서로 다른 언어 모델 에 대응하는 단어 벡터 매개 변수 매트릭스 중의 단어 벡터를 더 잘 융합시키기 위하여, 하나의 FC 매트릭스를 도입하여, 2개의 서로 다른 언어 모델에 대응하는 단어 벡터 매개 변수 매트릭스를 융합한 후의 단어 벡터를 지 원하고 보충함으로써, 상기 언어 모델에 의한 2개의 서로 다른 언어 모델에 대응하는 단어 벡터 매개 변수 매트 릭스에 대응하는 단어 벡터의 학습 효과를 더 향상시킨다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 202에 있어서, 상기 제1 단어 마스크의 콘텍스트 벡터 와 상기 제1 단어 벡터 매개 변수 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제1 확률 분포 매트릭스를 얻 을 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 202에 있어서, 상기 제1 단어 마스크의 콘텍스트 벡터 와 상기 제2 단어 벡터 매개 변수 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻 을 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 202에 있어서, 상기 제1 단어 마스크의 콘텍스트 벡터 와 상기 완전 연결 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제3 확률 분포 매트릭스를 얻을 수 있다. 당해 실현 방식에 있어서, 제1 단어 마스크의 콘텍스트 벡터와, 제1 단어 벡터 매개 변수 매트릭스, 제2 단어 벡터 매개 변수 매트릭스, 완전 연결 매트릭스를 각각 매트릭스 곱셈함으로써, 제1 단어 마스크의 각각 제1 단 어 벡터 매개 변수 매트릭스, 제2 단어 벡터 매개 변수 매트릭스, 완전 연결 매트릭스에 기반한 복수의 단어 벡 터에 대응하는 확률 분포를 취득함으로써, 제1 확률 분포 매트릭스, 제2 확률 분포 매트릭스, 제3 확률 분포 매 트릭스에 기반하여 제1 단어 마스크에 대응하는 단어 벡터를 종합적으로 결정한다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 203에 있어서, 상기 제1 확률 분포 매트릭스, 상기 제 2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스를 가산하여 토탈 확률 분포 매트릭스를 얻은 후, 상기 토탈 확률 분포 매트릭스 중의 확률 값에 대해 정규화 처리를 실행한다. 예를 들면, 정규화 지수 함수(softma x)를 통해 상기 토탈 확률 분포 매트릭스 중의 확률 값에 대해 정규화 처리를 실행하여, 상기 제1 단어 마스크 가 복수의 단어 벡터에 대응하는 복수의 정규화 확률 값을 취득할 수 있으며, 또한 상기 복수의 정규화 확률 값 에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정할 수 있다. softmax를 통해 상기 토탈 확률 분 포 매트릭스 중의 확률 값에 대해 정규화 처리를 실행하기 때문에, 제1 단어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스는 softmax 매개 변수 매트릭스 또는 softmax 단어 벡터 매개 변수 매트릭스라고 도 불릴 수 있다. 당해 실현 방식에 있어서, 제1 확률 분포 매트릭스, 제2 확률 분포 매트릭스 및 제3 확률 분포 매트릭스를 가산 하여 취득한 토탈 확률 분포 매트릭스의 확률 값에 대해 정규화 처리를 실행하고, 정규화된 확률 값에 기반하여 예를 들면 확률 값이 가장 높은 단어 벡터를 제1 단어 마스크에 대응하는 단어 벡터로 선택하여, 제1 단어 마스 크에 대응하는 단어 벡터를 정확하게 결정할 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 204에 있어서, 상기 제1 기설정된 트레이닝 완료 조건 을 충족시킬 때까지, 상기 제1 단어 벡터 매개 변수 매트릭스 및 상기 제2 단어 벡터 매개 변수 매트릭스의 매 개 변수 값을 그대로 유지하고, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델 및 상 기 완전 연결 매트릭스를 트레이닝하는 바, 즉 상기 언어 모델 및 상기 완전 연결 매트릭스 중의 매개 변수 값 을 조정함으로써. 트레이닝된 언어 모델 및 완전 연결 매트릭스를 얻으며, 트레이닝된 완전 연결 매트릭스, 제1 단어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스의 세트를 최종의 단어 벡터의 세트로 이용 할 수 있다. 당해 실현 방식에 있어서, 상기 언어 모델 및 상기 완전 연결 매트릭스에 대해 공동 트레이닝을 실행함으로써, 상기 언어 모델과 단어 벡터의 수렴 속도를 가속화시키고, 트레이닝 효과를 높일 수 있다. 선택적으로, 상기의 제1 실시예 또는 제2 실시예 전에, 또한 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까 지, 초기화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매트릭스를 사전에 트레이닝 함으로써, 상기 언어 모델 및 상기 제1 단어 벡터 매개 변수 매트릭스를 얻고, 트레이닝된 언어 모델 및 제1 단어 벡터 매개 변수 매트 릭스를 얻을 수 있으며, 트레이닝된 제1 단어 벡터 매개 변수 매트릭스를 상기 언어 모델이 사용되는 단어집 중 의 단어의 단어 벡터의 세트로 이용할 수 있다. 본 실시예에 있어서, 초기화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매트릭스를 사전에 트레이닝 하여 트레이닝된 상기 언어 모델 및 상기 제1 단어 벡터 매개 변수 매트릭스를 얻은 후, 제2 언어 모델의 단어 벡터 매개 변수 매트릭스를 결합하여 상기 언어 모델을 더욱 트레이닝 함으로써, 트레이닝 속도를 가속화시키고, 트 레이닝 효과를 향상시킬 수 있다. 도 3은 본 출원의 제3 실시예에 따른 모식도인 바, 도 3에 나타낸 바와 같다. 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 초기화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매 트릭스를 트레이닝하는 단계는 이하의 방식을 통해 실현할 수 있다. 301에 있어서, 미리 코퍼스 베이스 중의 소정의 텍스트 코퍼스를 사용하여 상기 초기화 언어 모델에 대해 사전 트레이닝(pre-training)을 실행한다. 미리 코퍼스 베이스 중의 소정의 텍스트 코퍼스를 사용하여 상기 언어 모델에 대해 사전 트레이닝을 실행함으로 써, 언어 모델은 텍스트 코퍼스 중의 단어, 실체 및 실체 관계를 학습할 수 있다. 302에 있어서, 제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어를 각각 제2 단어 마스크로 교체하여 적어도 하 나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스를 얻는다. 제2 샘플 텍스트 코퍼스는 상기 제1 샘플 텍스트 코퍼스와 동일할 수도 있고, 다를 수도 있다. 또한, 제2 샘플 텍스트 코퍼스는 코퍼스 베이스 중의 소정의 텍스트 코퍼스 중의 하나의 소정의 텍스트 코퍼스일 수도 있고, 코 퍼스 베이스 중의 소정의 텍스트 코퍼스와 다른 기타 텍스트 코퍼스일 수도 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어를 각각 제2 단어 마스크로 교체할 경우, 여전히 글자에 기반하여 제2 단어 마스크의 콘텍스트를 나타낸다. 303에 있어서, 상기 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스를 상기 초기화 언어 모 델에 입력하고, 상기 초기화 언어 모델을 통해 상기 적어도 하나의 제2 단어 마스크 중의 각각의 상기 제2 단어 마스크의 콘텍스트 벡터를 출력한다. 304에 있어서, 각각의 상기 제2 단어 마스크의 콘텍스트 벡터 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭 스에 기반하여 각각의 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정한다. 305에 있어서, 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 적어도 하나의 제2 단어 마스크에 대 응하는 단어 벡터에 기반하여 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭스를 트레 이닝 한다. 여기서, 상기의 302∼305는 반복적으로 실행하는 과정일 수 있으며, 302∼305를 반복적으로 실행함으로써, 초기 화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매트릭스에 대한 트레이닝을 실현하고, 제2 기설정된 트레이 닝 완료 조건을 충족시키면 초기화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매트릭스에 대한 트레이닝이 완료된다. 예를 들면, 구체적인 일 예에 있어서, 미리 코퍼스 베이스 중의 소정의 텍스트 코퍼스를 사용하여 초기화 언어 모델에 대해 사전 트레이닝을 실행하여, \"하얼빈”은 \"흑룡강”의 성소재지이고 또한 \"하얼빈”은 얼음과 눈의 도시이다는 것을 학습하였다. 제2 샘플 텍스트 코퍼스인 \"하얼빈은 흑룡강의 성소재지이다” 중의 \"하얼빈”을 하나의 단어 마스크 (MASK)로 교체하여 언어 모델에 입력하고, 초기화 언어 모델을 통해 하나의 단어 벡터를 출 력하며, 당해 초기화 언어 모델에 의해 출력된 단어 벡터가 정확한지 여부에 기반하여 초기화 언어 모델 및 초 기화 제1 단어 벡터 매개 변수 매트릭스를 트레이닝 하여, 트레이닝이 완료된 후, 언어 모델에 \"[MASK]은 흑룡 강의 성소재지이다”를 입력할 때, 언어 모델로 하여금 \"하얼빈”이라는 단어 벡터를 정확하게 출력할 수 있도 록 한다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 304에 있어서, 상기 제2 단어 마스크의 콘텍스트 벡터 와 상기 초기화 제1 단어 벡터 매개 변수 매트릭스를 곱셈하여, 상기 제2 단어 마스크가 복수의 단어 벡터에 대 응하는 확률 값을 얻은 후, 상기 제2 단어 마스크가 복수의 단어 벡터에 대응하는 확률 값에 대해 정규화 처리 를 실행하며, 상기 제2 단어 마스크가 복수의 단어 벡터에 대응하는 복수의 정규화 확률 값을 얻고, 또한 상기복수의 정규화 확률 값에 기반하여 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정할 수 있다. 구체적으로 는 정규화 확률 값이 가장 높은 단어 벡터를 상기 제2 단어 마스크에 대응하는 단어 벡터로 결정할 수 있다. 구체적인 일 실현 방식에 있어서, 단어집을 이용하여 가능한 단어를 포함할 수 있으며, 제1 단어 벡터 매개 변 수 매트릭스는 복수의 단어 벡터를 포함하고, 당해 제1 단어 벡터는 각각 단어집 중의 각각의 단어에 대응하며, 상기 제2 단어 마스크의 콘텍스트 벡터와 상기 초기화 제1 단어 벡터 매개 변수 매트릭스를 곱셈하여, 각각의 상기 2단어 마스크의 콘텍스트 벡터와 상기 초기화 제1 단어 벡터 매개 변수 매트릭스 중의 각각의 단어 벡터의 관련성을 취득할 수 있고, 상기 제2 단어 마스크가 단어집 중의 각각의 단어 벡터에 각각 대응하는 확률 값을 취득하며, 당해 확률 값은 상기 제2 단어 마스크가 대응하는 단어 벡터인 확률을 나타낸다. 당해 실현 방식에 있어서, 제2 단어 마스크의 콘텍스트 벡터와 단어 벡터 매개 변수 매트릭스를 곱셈하여, 취득 된 확률 값에 대해 정규화 처리를 실행한다. 예를 들면, softmax를 통해, 각각의 상기 제2 단어 마스크가 복수 의 단어 벡터에 대응하는 확률 값에 대해 정규화 처리를 실행할 수 있으며, 따라서 정규화된 확률 값에 기반하 여 확률 값이 가장 높은 단어 벡터를 제2 단어 마스크에 대응하는 단어 벡터로 선택하고, 제2 단어 마스크에 대 응하는 단어 벡터를 결정할 수 있다. softmax를 통해 각각의 상기 제2 단어 마스크가 복수의 단어 벡터에 대응 하는 확률 값에 대해 정규화 처리를 실행할 때, 제1 단어 벡터 매개 변수 매트릭스는 softmax 매개 변수 매트릭 스 또는 softmax 단어 벡터 매개 변수 매트릭스라고도 불릴 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 305에 있어서, 상기 제2 기설정된 트레이닝 완료 조건 은 실제의 수요에 따라 설정될 수 있는 바, 예를 들면, 언어 모델에 의해 출력되는 단어 벡터의 샘플 텍스트 코퍼스에 대응하는 퍼플렉서티(perplexity)가 제1 소정의 임계 값에 달하는 것; 복수의 제2 샘플 텍스트 코퍼스를 이용하여 302∼304를 실행하고, 복수의 제2 샘플 텍스트 코퍼스 중에서 제2 단어 마스크로 교체된 단어는 단어집 중의 복수의 단어를 포함하여 (일부 단어 또는 모든 단어일 수 있음), 304 에서 제2 단어 마스크가 복수의 단어 벡터에 대응하는 복수의 정규화 확률 값을 취득한 후, 트레이닝에 참여한 모든 제2 단어 마스크의 정규화 확률 값이 최대화된 것; 및 상기 초기화 언어 모델 및 상기 초기화 단어 벡터 매개 변수 매트릭스에 대한 트레이닝 횟수 (즉 302∼305를 반 복적으로 실행한 횟수)가 제2 소정의 임계 값에 달하는 것 중의 임의의 하나 또는 복수를 포함할 수 있다. 본 실시예에 있어서, 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스를 언어 모델에 입력하고, 초기화 언 어 모델을 통해 상기 제2 단어 마스크의 콘텍스트 벡터를 출력한 후, 상기 제2 단어 마스크의 콘텍스트 벡터 및 초기화 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하고, 또한 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제2 단어 마스크에 대응하는 단어 벡터에 기반 하여 상기 초기화 언어 모델 및 상기 제1 단어 벡터 매개 변수 매트릭스를 트레이닝 함으로써, 트레이닝된 언어 모델 및 제1 단어 벡터 매개 변수 매트릭스를 얻을 수 있다. 트레이닝된 제1 단어 벡터 매개 변수 매트릭스를 상기 언어 모델에 대응하는 단어집 중의 각각의 단어의 단어 벡터로 이용하며, 글자 벡터와 비교하면 단어 벡터 가 풍부한 시맨틱 정보 표현을 포함하기에, 더 큰 입도의 시맨틱 정보 표현을 도입하여 단어 마스킹 방식을 통 해 콘텍스트에 기반하여 단어 벡터를 모델링 함으로써, 언어 모델의 시맨틱 정보에 대한 모델링을 강화시켰고, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰다. 또한, 본 실시예에 있어서, 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스를 사용하여 초기화 언어 모델 을 트레이닝 함으로써, 글자의 전체 단어 마스킹에 의해 야기될 가능성이 있는 정보 누설 리스크를 효과적으로 피할 수 있다. 또한, 본 실시예를 사용하면, 초기화 언어 모델과 초기화 제1 단어 벡터 매개 변수 매트릭스의 트레이닝을 결합 하여, 초기화 언어 모델과 초기화 제1 단어 벡터 매개 변수 매트릭스에 대해 공동 트레이닝을 실행함으로써, 언 어 모델과 단어 벡터의 수렴 속도를 가속화시키고, 트레이닝 효과를 높일 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 302에 있어서, 상기 제2 샘플 텍스트 코퍼스에 대해 단어 분할을 실행하고, 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어 중의 각각의 단어를 각각 하나의 제2 단어 마스크로 교체할 수 있다. 제2 마스크로 교체된 단어를 제외하고, 상기 제 2 샘플 텍스트 코퍼스에서는 여전히 글자에 기반하여 제2 단어 마스크의 콘텍스트를 나타낸다. 당해 실현 방식에 있어서, 제2 샘플 텍스트 코퍼스에 대해 단어 분할을 실행함으로써, 단어 분할 결과에 기반하 여 제2 샘플 텍스트 코퍼스 중의 단어를 정확하게 결정할 수 있고, 그 중의 하나 또는 복수의 단어 중의 각각의 단어를 각각 하나의 제2 단어 마스크로 교체함으로써, 단어 마스크를 정확하게 설치하여 초기화 언어 모델의 트 레이닝에 사용될 수 있고, 초기화 언어 모델로 하여금 콘텍스트에 기반하여 단어 벡터를 모델링 하도록 하여, 언어 모델의 시맨틱 정보에 대한 모델링을 강화시켰고, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰 다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 상기의 실시예 중의 상기 언어 모델 및 상기 제2 언어 모델은 임의의 2개의 서로 다른 종류의 언어 모델일 수도 있고, 서로 다른 코퍼스 베이스 중의 소정의 텍스트 코퍼스를 통해 트레이닝 하여 얻은 같은 종류의 서로 다른 언어 모델일 수 있는 바 본 출원의 실시예에서는 상 기 언어 모델 및 상기 제2 언어 모델이 구체적인 종류를 한정하지 않는다. 예를 들면, 그 중에 구체적인 일 실현 방식에 있어서, 예를 들면, 상기 언어 모델은 지식 강화 시맨틱 표현 (ERNIE) 모델일 수 있고, 상기 제2 언어 모델은 연속적인 배그-오프-워드(Continuous Bag of Word, CBOW) 모델 또는 ERNIE 모델 및 CBOW모델과 다른 기타 언어 모델일 수 있다. 여기서, ERNIE 모델은 대량의 데이터 중의 실체 개념 등 선험 시맨틱 지식을 모델링 함으로써, 완전한 개념의 시맨틱 표현을 학습하고, 단어 및 실체 개념 등의 시맨틱 유닛을 마스크하여 ERNIE 모델을 사전 트레이닝시킴으 로써, ERNIE 모델의 시맨틱 지식 유닛에 대한 표현이 현실 세계와 가깝도록 한다. 따라서, ERNIE 모델은 글자 특징 입력에 기반하여 모델링 하는 동시에, 직접 선험 시맨틱 지식 유닛을 모델링 하기에, 강한 시맨틱 표현 능 력을 가진다. 본 실시예에 있어서, ERNIE 모델을 언어 모델로 사용함으로써, ERNIE 모델의 강한 시맨틱 표현 능 력을 이용하여, 대량의 데이터 중의 단어, 실체 및 실체 관계를 모델링 하고, 현실 세계의 시맨틱 지식을 학습 할 수 있으며, 모델 시맨틱 표현 능력을 강화시킬 수 있다. 예를 들면, ERNIE 모델은 단어나 실체의 표현을 학 습함으로써 \"하얼빈”과 \"흑룡강”의 관계를 모델링 하여, \"하얼빈”은 \"흑룡강”의 성소재지이고, 또한 \"하얼 빈”은 얼음과 눈의 도시이다는 것을 학습할 수 있다. CBOW모델은 하나의 중간 단어의 콘텍스트에 대응하는 단어 벡터에 기반하여 당해 중간 단어의 단어 벡터를 예측 할 수 있으며, CBOW 모델이 히든 레이어를 포함하지 않기에, 트레이닝 속도가 빠르다. 또한, CBOW 모델의 각각 의 단어 벡터에 대한 계산은 슬라이드 윈도우에 의해 한정되는 콘텍스트에만 관계되므로, 트레이닝 매개 변수가 적고, 모델의 복잡도가 낮으며, 모델의 예측 정확율이 높다. CBOW 모델에 대응하는 사전에 트레이닝된 단어 벡 터 매개 변수 매트릭스 (CBOW 단어 벡터라고도 불림)와 ERNIE 모델에 대응하는 사전에 트레이닝된 단어 벡터 매 개 변수 매트릭스 (ERNIE-WORD 단어 벡터라고도 불림)을 동시에 결합하여, ERNIE 모델을 추가로 트레이닝 함으 로써, ERNIE 모델로 하여금 고품질의 CBOW 단어 벡터 및 ERNIE-WORD 단어 벡터의 시맨틱 정보를 동시에 학습하 도록 하여, ERNIE 모델의 시맨틱 정보 학습 능력을 강화시켰고, ERNIE 모델의 텍스트 중의 단어의 예측 능력을 높인다. 또한, 상기 실시예의 기초 상에서, 제1 기설정된 트레이닝 완료 조건을 충족시켜 트레이닝된 언어 모델을 취득 한 후, 지도 학습 NLP 태스크를 통해 언어 모델을 추가로 최적화시키고, 언어 모델의 NLP 태스크의 예측 성능을 더 향상시킬 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 트레이닝된 언어 모델을 이용하여 NLP 태스크를 실행 하여 처리 결과를 얻고, 소정의 조건을 충족시킬 때까지, 상기 처리 결과와 라벨링 결과 정보 사이의 차이에 기 반하여, 상기 언어 모델 중의 매개 변수 값을 미세 조정(fine-tuning)할 수 있으며, 상기 소정의 조건은 상기 처리 결과와 라벨링 결과 정보 사이의 차이가 소정의 차이보다 작은 것, 및/또는, 상기 언어 모델의 트레이닝 횟수가 소정의 횟수에 달한 것 등 일수 있다. 구체적으로 상기의 NLP 태스크는 예를 들면 분류, 매칭, 시퀀스 라벨링 등의 NLP 태스크 중의 임의의 하나 또는 복수일 수 있는 바, 본 실시예에서는 이에 대해 특히 한정하지 않는다. 이에 따라, 처리 결과는 구체적인 NLP 태스크의 처리 결과, 예를 들면 분류 결과, 매칭 결과, 시퀀스 라벨링 결과 등이다. 구체적인 실현 방식에 있어서, 트레이닝된 언어 모델을 이용하여 분류, 매칭, 시퀀스 라벨링을 실현하기 위한 기타 네트워크 모델, 예를 들면 컨볼루션 뉴럴 네트워크(convolutional neural network, CNN), 장기/단기 기억 (Long Short Term Memory, LSTM) 모델, 배그-오프-워드(Bag of Word, BOW) 모델과 결합하여, NLP 태스크를 실 행하여 처리 결과를 얻을 수도 있다. 예를 들면, 서로 다른 네트워크 모델인 분류, 매칭, 시퀀스 라벨링을 실현 하는 네트워크 모델은 언어 모델의 출력에 기반하여 분류, 매칭, 시퀀스 라벨링 등의 처리를 실행하여 해당하는 분류 결과, 매칭 결과, 시퀀스 라벨링 결과 등의 처리 결과를 얻다.본 실시예에 있어서, 단어 벡터 매개 변수 매트릭스가 필요 없기 때문에, 언어 모델의 전체의 구조를 변경하지 않고, 지도 학습 데이터 (즉 라벨링 결과 정보)의 NLP 태스크를 통해 언어 모델을 더 최적화시켰고, 언어 모델 의 예측 성능을 향상시켰으며, 각각의 NLP 태스크에 기반하여 언어 모델에 대해 최적화 반복을 실행하기 용이하 다. 여기서, 상기의 각각의 방법의 실시예에 대해, 설명의 간소화를 위하여 당해 실시예를 일련의 동작 조합으로서 표현했지만, 본 출원에 따라 일부 단계를 서로 다른 순서에 따라 또는 동시에 실행할 수 있기 때문에, 당업자는 본 출원이 기재된 동작의 순서에 제한되지 않음을 이해해야 함을 설명할 필요가 있다. 그리고, 당업자는 명세서 에 기재된 실시예는 모두 바람직한 실시예에 해당하며, 관련된 동작 및 모듈은 본 출원에 있어서 꼭 필요하는 것이 아님을 이해해야 한다. 상기의 실시예에 있어서, 각각의 실시예에 대한 기재는 각각 강조된 부분이 있으며, 어느 한 실시예에서 상세히 기재되지 않은 부분은 기타 실시예의 관련 기재를 참조할 수 있다. 도 4는 본 출원의 제4 실시예에 따른 모식도인 바, 도 4에 나타낸 바와 같다. 본 실시예에 있어서, 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 장치는 언어 모델, 취득 유닛, 제1 결정 유닛 및 제1 트레이닝 유닛을 포함할 수 있다. 여기서, 언어 모델은 제1 단어 마스크를 포함하는 제 1 샘플 텍스트 코퍼스를 입력하고, 상기 제1 단어 마스크의 콘텍스트 벡터를 출력하며, 취득 유닛은 상기 제1 단어 마스크의 콘텍스트 벡터 및 제1 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제 1 확률 분포 매트릭스를 얻고, 상기 제1 단어 마스크의 콘텍스트 벡터 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻되, 상기 제1 단어 벡터 매개 변수 매트릭스는 상기 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이고, 상기 제2 단어 벡터 매개 변 수 매트릭스는 제2 언어 모델에 대응하는 사전에 트레이닝된 단어 벡터 매개 변수 매트릭스이며, 제1 결정 유닛 은 상기 제1 확률 분포 매트릭스 및 상기 제2 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대 응하는 단어 벡터를 결정하고, 제1 트레이닝 유닛은 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델을 트레이닝 한다. 여기서, 본 실시예에 따른 언어 모델의 트레이닝 장치의 실행 주체의 일부 또는 전부는 로컬 단말에 위치하는 애플리케이션이나, 로컬 단말에 위치하는 애플리케이션에 인스톨된 플러그인 또는 소프트웨어 개발 킷(SDK: Software Development Kit) 등의 기능 유닛일 수 있으며, 또는 네트워크 측의 서버에 위치하는 처리 엔진일 수 도 있는 바, 본 실시예에서는 이에 대해 특별히 한정하지 않음을 설명할 필요가 있다. 상기 애플리케이션은 단말에 인스톨된 로컬 프로그램(nativeApp) 또는 단말 상의 브라우저의 웹 애플리케이션 (webApp)일 수도 있는 바, 본 실시예에서는 이에 대해 한정하지 않음을 이해해야 된다. 본 실시예에 있어서, 제2 언어 모델에 대응하는 제2 단어 벡터 매개 변수 매트릭스를 도입하는 동시에, 제1 단 어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 복수 종류의 고품질 단어 벡터를 결합하여 언어 모델을 트레이닝 함으로써, 언어 모델로 하여금 멀티 소스의 고품질 시맨틱 정보를 학습하도록 하여, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰고, 언어 모델의 예측 성능을 향상시켰다. 또한, 본 출원에 의해 제공되는 기술 방안을 이용하면, 단어 마스크를 포함하는 샘플 텍스트 코퍼스를 사용하여 언어 모델을 트레이닝 하며, 글자 벡터와 비교하면 단어 벡터가 풍부한 시맨틱 정보 표현을 포함하기에, 단어 마스크 방식을 이용해 콘텍스트에 기반하여 단어 벡터를 모델링 함으로써, 언어 모델의 시맨틱 정보에 대한 모 델링을 강화시켰고, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰으며, 글자에 기반한 단어 완전 마스 킹에 의해 야기될 가능성이 있는 정보 누설 리스크를 효과적으로 피할 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 상기 취득 유닛은 또한 상기 제1 단어 마스크의 콘텍스트 벡터 및 완전 연결 매트릭스에 기반하여 상기 제1 단어 마스크의 제3 확률 분포 매트릭스를 얻을 수 있다. 이에 따라, 본 실시예에 있어서, 상기 제1 결정 유닛은 구체적으로 상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스에 기반하여 상기 제1 단어 마스크에 대응하는 단 어 벡터를 결정할 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 상기 취득 유닛은 구체적으로 상기 제1 단어 마 스크의 콘텍스트 벡터와 상기 제1 단어 벡터 매개 변수 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제1 확률 분포 매트릭스를 얻고, 및/또는, 상기 제1 단어 마스크의 콘텍스트 벡터와 상기 제2 단어 벡터 매개 변수 매트 릭스를 곱셈하여, 상기 제1 단어 마스크의 제2 확률 분포 매트릭스를 얻으며, 및/또는, 상기 제1 단어 마스크의콘텍스트 벡터와 상기 완전 연결 매트릭스를 곱셈하여, 상기 제1 단어 마스크의 제3 확률 분포 매트릭스를 얻는 다. 도 5는 본 출원의 제5의 실시예에 따른 모식도인 바, 도 5에 나타낸 바와 같이, 도 4에 나타낸 실시예를 기초로, 본 실시예에서 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 장치는, 가산 유닛 과 정규화 유닛을 더 구비할 수 있다. 여기서, 가산 유닛은 상기 제1 확률 분포 매트릭스, 상기 제2 확률 분포 매트릭스 및 상기 제3 확률 분포 매트릭스를 가산하여 토탈 확률 분포 매트릭스를 얻고, 정규화 유닛은 상기 토탈 확률 분포 매트릭스 중의 확률 값에 대해 정규화 처리를 실행하여 상기 제1 단어 마스크 가 복수의 단어 벡터에 대응하는 복수의 정규화 확률 값을 얻는다. 이에 따라, 본 실시예에 있어서, 상기 제1 결정 유닛은 구체적으로 상기 복수의 정규화 확률 값에 기반하여 상기 제1 단어 마스크에 대응하는 단어 벡터를 결정할 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 상기 제1 트레이닝 유닛은 구체적으로 상기 제1 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 상기 제1 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 언어 모델 및 상기 완전 연결 매트릭스를 트레이닝 할 수 있다. 선택적으로, 다시 도 5를 참조하면 상기의 실시예에서 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝 하는 장치는, 제2 기설정된 트레이닝 완료 조건을 충족시킬 때까지, 초기화 언어 모델 및 초기화 제1 단어 벡터 매개 변수 매트릭스를 트레이닝 하기 위한 상기 언어 모델 및 상기 제1 단어 벡터 매개 변수 매트릭 스를 얻기 위한 제2 트레이닝 유닛을 더 포함할 수 있다. 선택적으로, 다시 도 5를 참조하면 상기의 실시예에서 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝 하는 장치는, 사전 트레이닝 유닛, 교체 유닛 및 제2 결정 유닛을 더 구비할 수 있다. 여 기서, 사전 트레이닝 유닛은 미리 코퍼스 베이스 중의 소정의 텍스트 코퍼스를 사용하여 상기 초기화 언어 모델에 대해 사전 트레이닝을 실행하고, 교체 유닛은 제2 샘플 텍스트 코퍼스 중의 적어도 하나의 단어를 각각 제2 단어 마스크로 교체하여 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스를 얻어, 상기 초기화 언어 모델에 입력하며, 상기 초기화 언어 모델은 상기 교체 유닛에 의해 입력되는 적어도 하나의 제2 단어 마스크를 포함하는 제2 샘플 텍스트 코퍼스에 기반하여 상기 적어도 하나의 제2 단어 마스크 중의 각 각의 상기 제2 단어 마스크의 콘텍스트 벡터를 출력하고, 제2 결정 유닛은 각각의 상기 제2 단어 마스크의 콘텍스트 벡터 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭스에 기반하여 각각의 상기 제2 단어 마스크에 대응하는 단어 벡터를 결정하며, 상기 제2 트레이닝 유닛은 구체적으로 제2 기설정된 트레이닝 완료 조건 을 충족시킬 때까지, 상기 적어도 하나의 제2 단어 마스크에 대응하는 단어 벡터에 기반하여 상기 초기화 언어 모델 및 상기 초기화 제1 단어 벡터 매개 변수 매트릭스를 트레이닝 한다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 상기 교체 유닛은 구체적으로 상기 제2 샘플 텍 스트 코퍼스에 대해 단어 분할을 실행하고, 단어 분할 결과에 기반하여 상기 제2 샘플 텍스트 코퍼스 중의 적어 도 하나의 단어 중의 각각의 단어를 각각 하나의 제2 단어 마스크로 교체할 수 있다. 선택적으로, 본 실시예의 가능한 일 실현 방식에 있어서, 상기의 실시예 중의 상기 언어 모델 및 상기 제2 언어 모델은 임의의 2개의 서로 다른 종류의 언어 모델일 수도 있고, 서로 다른 코퍼스 베이스 중의 소정의 텍스트 코퍼스를 통해 트레이닝 하여 얻을 수 있는 같은 종류의 서로 다른 언어 모델일 수도 있는 바 본 출원의 실시예 에서는 상기 언어 모델 및 상기 제2 언어 모델의 구체적인 종류를 한정하지 않는다. 예를 들면, 그 중의 구체적으로 실현 방식에 있어서, 예를 들면 상기 언어 모델은 지식 강화 시맨틱 표현 (ERNIE) 모델일 수 있고, 상기 제2 언어 모델은 CBOW 모델 또는 ERNIE 모델 및 CBOW 모델과 서로 다른 언어 모 델일 수 있다. 여기서, 도 1∼도 3에 대응하는 실시예 중의 방법은 상기의 도 4∼도 5의 실시예에 의해 제공되는 상기의 실시 예에 따른 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 장치에 의해 실현될 수 있다. 상세한 설명은 도 1∼도 3의 대응하는 실시예의 관련 내용을 참조할 수 있는 바, 여기서 다시 설명되지 않는다. 본 출원의 실시예에 따르면, 본 출원은 또한 전자 기기 및 컴퓨터 명령이 기록되어 있는 컴퓨터 판독 가능 기록 매체를 제공한다. 도 6은 본 발명에 따른 실시예의 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법을 실현하는 전자 기기의 블럭도이다. 전자 기기는 예를 들면 랩톱 컴퓨터, 데스크톱 컴퓨터, 워크 스테이션, 개인 디지털 보조기, 서버, 블레이드 서버, 대형 컴퓨터 및 기타 적합한 컴퓨터와 같은 다양한 형태의 디지털 컴퓨터를 나타낸다. 전자 기기는 또한 예를 들면 개인 디지털 처리기, 셀폰, 스마트 전화, 웨어러블 기기 및 기타 유사한 계 산 장치와 같은 다양한 형태의 모바일 장치를 나타낼 수 있다. 본 명세서에 나타낸 구성 요소, 이들의 연결과 관계 및 이들의 기능은 단지 예일 뿐이며, 본 명세서에서 설명하거나 및/또는 요구하는 본 발명의 실현을 한정 하려는 것이 아니다. 도 6에 나타낸 바와 같이, 당해 전자 기기는 하나 또는 복수의 프로세서, 메모리 및 각각의 구성 요 소를 연결하기 위한 인터페이스를 구비하며, 당해 인터페이스는 고속 인터페이스 및 저속 인터페이스를 포함한 다. 각각의 구성 요소는 서로 다른 버스를 통해 상호 연결되며, 공통 마더 보드에 설치되거나 또는 수요에 따라 기타 방식으로 설치된다. 프로세서 전자 기기 내에서 수행되는 명령에 대해 처리를 실행할 수 있으며, 메모리 내에 기억되어 외부 입력/출력 장치 （예를 들면 인터페이스에 연결된 디스플레이 기기） 상에 GUI의 그래픽 정 보를 표시하기 위한 명령을 포함한다. 기타 실시 방식에 있어서, 필요할 경우, 복수의 프로세서 및/또는 복수의 버스와 복수의 메모리를 함께 사용할 수 있다. 마찬가지로, 복수의 전자 기기를 연결할 수 있으며, 각각의 기기 는 부분적인 필요한 조작 （예를 들면, 서버 어레이, 일 그룹의 블레이드 서버, 또는 다중 프로세서 시스템）을 제공한다. 도 6에서는 하나의 프로세서의 예를 들었다. 메모리는 본 발명에 의해 제공되는 비 일시적 컴퓨터 판독 가능 기억 매체이다. 여기서, 상기 메모리에는 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 적어도 하나의 프로세서로 하여금 본 발명에 의해 제공되는 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법을 수행하도록 한다. 본 발명의 비 일시적 컴퓨터 판독 가능 기억 매체는 컴퓨터 명령을 기억하며, 당해 컴퓨터 명령은 컴퓨터 로 하여금 본 발명에 의해 제공되는 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법을 수행 하도록 한다. 메모리는 일종의 비 일시적 컴퓨터 판독 가능 기억 매체로서, 비 일시적 소프트웨어 프로그램을 기억하는 데 사용될 수 있는 바, 예를 들면 비 일시적 컴퓨터 수행 가능 프로그램 및 모듈, 본 발명 실시예 중의 복수 종 류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법 대응하는 프로그램 명령/모듈 （예를 들면, 도 5에 나타낸 관련 모듈）을 기억하는데 사용될 수 있다. 프로세서는 메모리 내에 기억된 비 일시적 소프트 웨어 프로그램, 명령 및 모듈을 운행함으로써, 서버의 다양한 기능 응용 및 데이터 처리를 수행하는 바, 즉 상 술한 방법 실시예 중의 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법을 실현한다. 메모리는 프로그램 기억 영역 및 데이터 기억 영역을 포함할 수 있으며, 여기서, 프로그램 기억 영역은 운 영 체제 및 적어도 하나의 기능에 필요한 응용 프로그램을 기억할 수 있고, 데이터 기억 영역은 복수 종류의 단 어 벡터에 기반하여 언어 모델을 트레이닝하는 방법을 실현하는 전자 기기의 사용을 통해 생성된 데이터 등을 기억할 수 있다. 또한, 메모리는 고속 랜덤 액세스 메모리를 포함할 수 있고, 비 일시적 메모리를 더 포함 할 수 있는 바, 예를 들면 적어도 하나의 자기 디스크 저장 장치, 플래시 장치, 또는 기타 비 일시적 고체 저장 장치를 포함할 수 있다. 일부 실시예에 있어서, 메모리는 선택적으로 프로세서에 대해 원격 설치한 메모리를 포함할 수 있으며, 이러한 원격 메모리는 네트워크를 통해 복수 종류의 단어 벡터에 기반하여 언어 모 델을 트레이닝하는 방법을 실현하는 전자 기기에 연결될 수 있다. 상술한 네트워크의 실시예는 인터넷, 기업 인 트라 넷, 근거리 통신망, 이동 통신 네트워크 및 이들의 조합을 포함하나 이에 한정되지 않는다. 복수 종류의 단어 벡터에 기반하여 언어 모델을 트레이닝하는 방법을 실현하는 전자 기기는 입력 장치 및 출력 장치를 더 포함할 수 있다. 프로세서, 메모리, 입력 장치 및 출력 장치는 버스 또는 기타 방식을 통해 연결될 수 있으며, 도 6에서는 버스를 통해 연결하는 예를 들었다. 입력 장치는 입력된 디지털 또는 문자 정보를 수신하고, 또한 복수 종류의 단어 벡터에 기반하여 언어 모 델을 트레이닝하는 방법을 실현하는 전자 기기의 사용자 설정 및 기능 제어에 관한 키 신호 입력을 생성할 수 있다. 예를 들면 터치 스크린, 키패드, 마우스, 트랙 패드, 터치 패드, 포인팅 스틱, 하나 또는 복수의 마우스 버튼, 트랙볼, 조이스틱 등 입력 장치를 포함할 수 있다. 출력 장치는 디스플레이 기기, 보조 조명 장치 （예를 들면 LED） 및 촉각 피드백 장치 （예를 들면 진동 모터） 등을 포함할 수 있다. 당해 디스플레이 기기 는 액정 디스플레이（LCD）, 발광 다이오드（LED） 디스플레이 및 등 플라즈마 디스플레이를 포함할 수 있으나 이에 한정되지 않는다. 일부 실시 방식에 있어서, 디스플레이 기기는 터치 스크린일 수 있다. 여기서 설명하는 시스템 및 기술의 다양한 실시 방식은 디지털 전자 회로 시스템, 집적 회로 시스템, 전용 ASIC （전용 집적 회로）, 컴퓨터 하드웨어, 펌웨어, 소프트웨어 및/또는 이들의 조합에서 실현될 수 있다. 이러한 다양한 실시예는 하나 또는 복수의 컴퓨터 프로그램에서 실시되고, 당해 하나 또는 복수의 컴퓨터 프로그램은 적어도 하나의 프로그램 가능 프로세서를 포함하는 프로그램 가능 시스템 상에서 수행 및/또는 해석될 수 있으며, 당해 프로그램 가능 프로세서는 전용 또는 일반 프로그램 가능 프로세서일 수 있고, 기억 시스템, 적어도 하나의 입력 장치 및 적어도 하나의 출력 장치로부터 데이터 및 명령을 수신할 수 있으며, 또한 데이터 및 명령 을 당해 기억 시스템, 당해 적어도 하나의 입력 장치 및 당해 적어도 하나의 출력 장치에 전송할 수 있다. 이러한 계산 프로그램 （프로그램, 소프트웨어, 소프트웨어 응용 또는 코드로도 불림）은 프로그램 가능 프로세 서의 기계 명령을 포함하며, 또한 고급 과정 및/또는 객체 지향 프로그래밍 언어 및/또는 어셈블리/기계 언어를 이용하여 이러한 계산 프로그램을 실시할 수 있다. 본 명세서에서 사용되는 \"기계 판독 가능 매체” 및 \"컴퓨터 판독 가능 매체”와 같은 용어는, 기계 명령 및/또는 데이터를 프로그램 가능 프로세서의 임의의 컴퓨터 프로그 램 제품, 기기 및/또는 장치 （예를 들면, 자기 디스크, 광 디스크, 메모리, 프로그램 가능 논리 장치（PLD）） 에 제공하기 위한 것을 의미하며, 기계 판독 가능 신호로서의 기계 명령을 수신하는 기계 판독 가능 매체를 포 함한다. \"기계 판독 가능 신호”와 같은 용어는 기계 명령 및/또는 데이터를 프로그램 가능 프로세서에 제공하 기 위한 임의의 신호를 의미한다. 유저와의 대화를 제공하기 위하여, 컴퓨터 상에서 여기서 설명하는 시스템 및 기술을 실시할 수 있으며, 당해 컴퓨터는 유저에게 정보를 표시하기 위한 디스플레이 장치 （예를 들면 CRT（음극선관） 또는 LCD（액정 디스플 레이） 모니터） 및 키보드와 포인팅 장치（예를 들면, 마우스 또는 트랙볼）를 구비할 수 있으며, 유저는 당해 키보드 및 당해 포인팅 장치를 통해 입력을 컴퓨터에 제공할 수 있다. 기타 종류의 장치는 또한 유저와의 대화 를 제공하는데 사용될 수 있다. 예를 들면, 유저에 제공하는 피드백은 임의의 형태의 감각 피드백 （예를 들면, 시각적 피드백, 청각적 피드백, 또는 촉각 피드백）일 수 있으며, 또한 임의의 형태（음향 입력, 음성 입력 또 는 촉각 입력을 포함함）를 통해 유저로부터의 입력을 수신할 수 있다. 여기서 설명하는 시스템 및 기술을 백엔드 구성 요소를 포함하는 계산 시스템 （예를 들면 데이터 서버）, 또는 미들웨어 구성 요소를 포함하는 계산 시스템 （예를 들면 응용 서버）, 또는 프런트엔드 구성 요소를 포함하는 계산 시스템 （예를 들면 그래픽 유저 인터페이스 또는 웹 브라우저를 구비하는 유저 컴퓨터인 바, 유저는 당해 그래픽 유저 인터페이스 또는 당해 웹 브라우저를 통해 여기서 설명하는 시스템 및 기술의 실시 방식과 대화함 ）, 또는 이러한 백엔드 구성 요소, 미들웨어 구성 요소, 또는 프런트엔드 구성 요소의 임의의 조합을 포함하는 계산 시스템에서 실시할 수 있다. 임의의 형태 또는 매체의 디지털 데이터 통신 （예를 들면, 통신 네트워크） 을 통해 시스템의 구성 요소를 상호 연결할 수 있다. 통신 네트워크의 예는 근거리 통신망（LAN）, 광역 통신망 （WAN） 및 인터넷을 포함한다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트와 서버는 일반적으로 서로 멀리 떨어져 있 고, 또한 일반적으로 통신 네트워크를 통해 대화를 실행한다. 해당되는 컴퓨터 상에서 운행되고, 또한 클라이언 트 - 서버 관계를 갖는 컴퓨터 프로그램을 통해 클라이언트와 서버의 관계를 발생시킬 수 있다. 본 출원의 실시예의 기술 방안에 따르면, 제2 언어 모델에 대응하는 제2 단어 벡터 매개 변수 매트릭스를 도입 하는 동시에, 제1 단어 벡터 매개 변수 매트릭스 및 제2 단어 벡터 매개 변수 매트릭스에 기반하여 복수 종류의 고품질 단어 벡터를 결합하여 언어 모델을 트레이닝 함으로써, 언어 모델로 하여금 멀티 소스의 고품질 시맨틱 정보를 학습하도록 하여, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰고, 언어 모델의 예측 성능을 향상시켰다. 또한, 본 출원에 의해 제공되는 기술 방안을 이용하면, 마스크를 포함하는 샘플 텍스트 코퍼스를 사용하여 언어 모델을 트레이닝 하며, 글자 벡터와 비교하면 단어 벡터가 풍부한 시맨틱 정보 표현을 포함하기에, 단어 마스크 방식을 이용해 콘텍스트에 기반하여 단어 벡터를 모델링 함으로써, 언어 모델의 시맨틱 정보에 대한 모델링을 강화시켰고, 언어 모델의 시맨틱 정보에 대한 학습 능력을 강화시켰으며, 글자에 기반한 단어 완전 마스킹에 의 해 야기될 가능성이 있는 정보 누설 리스크를 효과적으로 피할 수 있다. 상기에 나타낸 다양한 형태의 흐름을 이용하여 단계를 재정열, 증가 또는 삭제할 수 있음을 이해해야 한다. 예 를 들면, 본 발명에 기재된 각각의 단계는 병열로 수행되거나 또는 차례로 수행되거나 또는 다른 순서로 수행될 수 있으며, 본 발명이 개시하는 기술 방안이 원하는 결과를 실현할 수 있는 한, 본 명세서는 이에 대해 한정하 지 않는다. 상술한 구체적인 실시 방식은 본 발명의 보호 범위를 한정하지 않는다. 당업자는 설계 요건 및 기타 요인에 따 라 다양한 수정, 조합, 서브 조합 및 대체를 실행할 수 있음을 이해해야 한다. 본 발명의 정신 및 원칙 내에서 이루어진 임의의 수정 동등한 대체 및 개선 등은 모두 본 발명의 보호 범위 내에 포함되어야 한다.도면 도면1 도면2 도면3 도면4 도면5 도면6"}
{"patent_id": "10-2021-0035543", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 출원의 실시예 중의 기술 방안을 더욱 명확히 설명하기 위하여, 이하, 실시예 또는 종래 기술의 설명에 필요 한 도면을 간단히 설명한다. 물론, 이하에 설명하는 도면은 본 출원의 몇 가지의 실시예이며, 당업자는 발명적 인 노력을 하지 않는 전제 하에서 이러한 도면에 기반하여 기타의 도면을 얻을 수 있다. 도면은 본 방안을 더욱 잘 이해하게 하기 위한 것일 뿐, 본 출원의 제한을 구성하는 것이 아니다. 도 1은 본 출원의 제1 실시예에 따른 순서도이다. 도 2는 본 출원의 제2 실시예에 따른 순서도이다. 도 3은 본 출원의 제3 실시예에 따른 순서도이다. 도 4는 본 출원의 제4 실시예에 따른 블록도이다. 도 5는 본 출원의 제5의 실시예에 따른 블록도이다. 도 6은 본 출원의 실시예에 따른 언어 모델을 트레이닝하는 방법을 실현하기 위한 전자 기기의 블록도이다."}
