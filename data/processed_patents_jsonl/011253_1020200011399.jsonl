{"patent_id": "10-2020-0011399", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0097559", "출원번호": "10-2020-0011399", "발명의 명칭": "3차원 가상 공간을 구축하기 위한 방법, 컴퓨터 판독 가능한 기록 매체, 컴퓨터 프로그램 및", "출원인": "에스케이텔레콤 주식회사", "발명자": "박종한"}}
{"patent_id": "10-2020-0011399", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "서버를 이용한 3차원 가상 공간을 구축하기 위한 방법에 있어서,전자 장치로부터 하나 이상의 이미지 데이터를 수신하는 단계;상기 하나 이상의 이미지 데이터 각각에서 배경 영역과 상기 배경 영역 이외의 오브젝트를 인식하고, 상기 오브젝트를 기 설정된 카테고리에 기초하여 분류하는 단계;상기 배경 영역과 상기 분류된 오브젝트를 기초로 복수의 3차원 이미지 데이터를 생성하는 단계; 및상기 생성된 복수의 3차원 이미지 데이터를 정합하여 상기 3차원 가상 공간에 대응하는 3차원 이미지 데이터를생성하는 단계를 포함하는 3차원 가상 공간을 구축하기 위한 방법."}
{"patent_id": "10-2020-0011399", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서, 상기 배경 영역과 상기 분류된 오브젝트를 기초로 상기 복수의 3차원 이미지 데이터를 생성하는 단계는,상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트에 기초하여 상기 기 설정된 카테고리에 대응하는 3차원오브젝트 이미지 데이터를 생성하는 단계 및상기 하나 이상의 이미지 데이터 각각에서의 상기 배경 영역에 기초하여 3차원 배경 이미지 데이터를 생성하는단계를 포함하는 3차원 가상 공간을 구축하기 위한 방법."}
{"patent_id": "10-2020-0011399", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서, 상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트에 기초하여 상기 기 설정된 카테고리에 대응하는 3차원 오브젝트 이미지 데이터를 생성하는 단계는,상기 전자 장치로부터 수신된 사용자 제공 데이터, 상기 서버의 인공 지능 기반 학습 모델 및 상기 서버의 머신러닝 기반 학습 모델 중 적어도 하나에 기초하여, 상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트의 왜곡 여부를 확인하는 단계, 및상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트의 왜곡 확인 시, 상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트를, 대응되는 오브젝트로 변경하거나 이미지 처리 기술을 이용하여 이미지 보정하는 단계를포함하는 3차원 가상 공간을 구축하기 위한 방법."}
{"patent_id": "10-2020-0011399", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1항에 있어서, 상기 하나 이상의 이미지 데이터 중 적어도 하나에 기 설정된 적어도 하나의 왜곡 발생 요소가 포함되어 있는지를 확인하는 단계, 및상기 하나 이상의 이미지 데이터 중 적어도 하나에 상기 기 설정된 적어도 하나의 왜곡 발생 요소가 포함되어있으면, 상기 왜곡 발생 요소를 제거하는 단계를 포함하는 3차원 가상 공간을 구축하기 위한 방법."}
{"patent_id": "10-2020-0011399", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4항에 있어서, 상기 적어도 하나의 왜곡 발생 요소는, 그림자에 대응하는 이미지 및 빛 반사에 대응하는 이미지 중 적어도 하나를 포함하며,상기 기 설정된 적어도 하나의 왜곡 발생 요소가 포함되어 있는지를 확인하는 단계는,공개특허 10-2021-0097559-3-인공 기능 기반 또는 머신 러닝 기반 학습 모델에 기초하여, 상기 하나 이상의 이미지 데이터에 왜곡 발생 요소가 포함되어 있는지 확인하거나 또는 상기 하나 이상의 이미지 데이터 간의 비교에 기초하여 상기 하나 이상의이미지 데이터 중 적어도 하나에 왜곡 발생 요소가 포함되어 있는지를 확인하는 단계를 포함하며,상기 왜곡 발생 요소를 제거하는 단계는,상기 왜곡 발생 요소가 포함된 오브젝트를 상기 왜곡 발생 요소가 포함된 오브젝트와 대응되는 기 저장된 오브젝트로 대체하거나 또는 상기 왜곡 발생 요소가 포함된 오브젝트에 대한 이미지 처리를 수행하는 단계를 포함하는 3차원 가상 공간을 구축하기 위한 방법."}
{"patent_id": "10-2020-0011399", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1항에 있어서, 특징점들의 비교에 기초하여 상기 수신한 하나 이상의 이미지 데이터와 대응되는 3차원 이미지 데이터가 저장되어 있는지를 확인하는 단계, 및상기 하나 이상의 이미지 데이터와 대응되는 상기 3차원 이미지 데이터가 저장되어 있으면, 상기 하나 이상의이미지 데이터에 대응하는 시간 정보, 해상도, 선명도, 및 왜곡 발생 요소 중 적어도 하나에 기초하여, 상기 저장된 3차원 이미지 데이터의 업데이트 필요 여부를 확인하는 단계를 더 포함하며,상기 분류하는 단계는,상기 저장된 3차원 이미지 데이터의 업데이트가 필요한 것으로 확인된 것에 기초하여 수행되는 3차원 가상 공간을 구축하기 위한 방법."}
{"patent_id": "10-2020-0011399", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6항에 있어서, 상기 확인하는 단계에서,상기 수신한 하나 이상의 이미지 데이터의 특징점들의 좌표들과 상기 3차원 이미지 데이터의 특징점들의 좌표들간의 매칭 수가 소정의 임계 값 이상이면, 상기 수신한 하나 이상의 이미지 데이터와 상기 3차원 이미지 데이터가 대응되는 것으로 결정하는 3차원 가상 공간을 구축하기 위한 방법."}
{"patent_id": "10-2020-0011399", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서, 상기 하나 이상의 이미지 데이터에 대응하는, 상기 전자 장치가 위치하는 공간의 환경 정보 및 컨텍스트 정보중 적어도 하나의 정보를 수신하는 단계 및상기 3차원 가상 공간에 대응하는 상기 3차원 이미지 데이터와 대응되도록, 상기 수신된 적어도 하나의 정보를저장하는 단계를 더 포함하는 3차원 가상 공간을 구축하기 위한 방법."}
{"patent_id": "10-2020-0011399", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "컴퓨터 프로그램을 저장하고 있는 컴퓨터 판독 가능 기록매체로서,상기 컴퓨터 프로그램은,제 1항 내지 제 8항 중 어느 한 항에 따른 방법을 프로세서가 수행하도록 하기 위한 명령어를 포함하는, 컴퓨터판독 가능한 기록매체."}
{"patent_id": "10-2020-0011399", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "컴퓨터 판독 가능한 기록매체에 저장되어 있는 컴퓨터 프로그램으로서,상기 컴퓨터 프로그램은,제 1항 내지 제 8항 중 어느 한 항에 따른 방법을 프로세서가 수행하도록 하기 위한 명령어를 포함하는, 컴퓨터프로그램.공개특허 10-2021-0097559-4-청구항 11 3차원 가상 공간을 구축하기 위한 장치에 있어서,통신 모듈; 및상기 통신 모듈을 이용하여, 전자 장치로부터 하나 이상의 이미지 데이터를 수신하며, 상기 하나 이상의 이미지데이터 각각에서 배경 영역과 상기 배경 영역 이외의 오브젝트를 인식하고, 상기 오브젝트를 기 설정된 카테고리에 기초하여 분류하며, 상기 배경 영역과 상기 분류된 오브젝트를 기초로 복수의 3차원 이미지 데이터를 생성하며, 상기 생성된 복수의 3차원 이미지 데이터를 정합하여 상기 3차원 가상 공간에 대응하는 3차원 이미지 데이터를 생성하는 제어부를 포함하는 3차원 가상 공간을 구축하기 위한 장치."}
{"patent_id": "10-2020-0011399", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "다양한 실시 예에 따르면, 서버를 이용한 3차원 가상 공간을 구축하기 위한 방법에 있어서, 전자 장치로부터 하 나 이상의 이미지 데이터를 수신하는 단계; 상기 하나 이상의 이미지 데이터 각각에서 배경 영역과 상기 배경 영 역 이외의 오브젝트를 인식하고, 상기 오브젝트를 기 설정된 카테게로이 기초하여 분류하는 단계; 상기 배경 영 역과 상기 분류된 오브젝트를 기초로 복수의 3차원 이미지 데이터를 생성하는 단계; 및 상기 생성된 복수의 3차 원 이미지 데이터를 정합하여 상기 3차원 가상 공간에 대응하는 3차원 이미지 데이터를 생성하는 단계를 포함할 수 있다."}
{"patent_id": "10-2020-0011399", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 3차원 가상 공간을 구축하기 위한 방법, 컴퓨터 판독 가능한 기록 매체, 컴퓨터 프로그램 및 장치에 관한 것이다."}
{"patent_id": "10-2020-0011399", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근, 증강 현실(AR; augmented reality), 가상 현실(VR; virtual reality), 및/또는 혼합 현실(MR; mixed reality) 기술을 이용하여 다양한 콘텐츠를 제공하는 서비스들이 상용화되고 있다. 상기 증강 현실 기술은, 현 실의 이미지나 배경에 3차원 가상 이미지를 겹쳐서 하나의 영상으로 보여주는 기술을 의미한다. 상기 가상 현실 기술은, 컴퓨터 등을 이용해 만들어 놓은 가상 공간에서 사람이 실제와 같은 체험을 할 수 있도록 하는 기술을 의미한다. 상기 혼합 현실 기술은 현실 세계에 가상 현실이 접목되어 현실의 물리적인 객체와 가상의 객체가 상 호 작용할 수 있는 환경을 제공하는 기술을 의미한다. 상기 증강 현실, 가상 현실, 및/또는 혼합 현실 기술의 구현을 위해 종래에는 다양한 3차원 가상 공간 구축 방 식이 개발되어 있다. 예를 들어, 종래에는 카메라 장치를 이용하여 특정 공간에 대한 복수의 이미지 데이터를 획득하고, 상기 획득된 복수의 이미지 데이터를 이용하여 현실 공간에 대한 3차원 가상 공간을 구축할 수 있다. 다른 예로, 종래에는 현실 공간에 대한 보다 더 정확한 정보를 획득하기 위해, 상기의 복수의 이미지 데이터 획 득에 추가로 라이다(lidar)를 이용하여 상기 특정 공간에 포함된 특정 오브젝트(물체)에 대한 거리 측정을 통해 거리 데이터를 획득할 수 있으며, 상기 획득된 복수의 이미지 데이터 및 상기 획득된 거리 데이터를 이용하여 현실 공간에 대한 3차원 가상 공간을 구축할 수 있다. 종래의 카메라 장치만을 이용한 3차원 가상 공간 구축 방식의 경우, 주변 환경의 영향 및/또는 카메라 장치의 성능 등으로, 현실 공간 내 오브젝트의 형태 및/또는 재질에 따라, 3차원 가상 공간의 특정 영역에서 왜곡 및/ 또는 누락이 발생할 수 있다. 이에 따라, 상기 왜곡 및/또는 누락에 의해 전체적으로 메쉬(mesh) 수가 증가하면 서 용량이 커지게 되고, 이는 렌더링(rendering) 시의 부하로 작용할 수 있다. 또한, 상기 왜곡 및/또는 누락에 대한 보정을 하기 위해 많은 수작업이 동반되어야 하는 단점이 있었다. 또한, 종래에는 상기 복수의 이미지 데 이터를 획득하기 위해, 많은 사진 촬영 및 상기 많은 사진 촬영에 따른 오랜 시간이 소요되는 단점이 있었다. 또한, 종래의 카메라 장치에 추가로 라이다를 이용한 3차원 가상 공간 구축 방식의 경우, 라이다가 고비용이라 는 단점이 있었다. 또한, 종래에는 상술한 방식들 중 하나를 이용하여 3차원 가상 공간을 구축한 이후, 상기 3차원 가상 공간에, 새롭게 취득한 현실 공간에 대한 정보가 추가되도록 업데이트 하는 것에 어려움이 있어 왔다."}
{"patent_id": "10-2020-0011399", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "종래의 현실 공간에 대한 정보 취득 및 상기 취득한 정보를 이용한 3차원 가상 공간의 구축 방식의 경우, 빛 반 사 등과 같은 주변 환경의 영향 및/또는 카메라 장치의 성능 등으로 인해 정보 왜곡이 발생되어, 자동화된 고품 질의 3차원 공간 구축의 어려움이 있어 왔다. 따라서, 본 발명의 실시 예는, 종래의 단점을 보완하여 자동화된 고품질의 3차원 공간 구축을 위한, 새로운 3차 원 가상 공간을 구축하기 위한 방법, 컴퓨터 판독 가능한 기록 매체, 컴퓨터 프로그램 및 장치를 제공할 수 있 다. 예를 들어, 본 발명의 실시 예는, 범용성 있는 스마트 폰 등의 장치를 이용하여 크라우드소싱(crowd sourcing) 형태로 다양한 현실 공간 정보를 협력하는 방식(cooperative 방식)으로 획득할 수 있으며, 획득한 현실 공간 정 보의 특성을 분석하여 1차적으로 3차원 공간을 구축할 수 있다. 또한, 본 발명의 실시 예는, 상기 1차적으로 구 축된 3차원 공간 내의 왜곡 정보를, 사용자에 의해 제공된 현실 공간 추가 정보 및/또는 머신 러닝(machine learning) 기법을 기초로 자동 보정하여 자동화된 고품질 3차원 가상 공간 구축을 할 수 있다. 또한, 본 발명의 실시 예는, 크라우드소싱 형태로 사용자의 추가 현실 공간 정보를 기초로 기 구축된 현실 공간의 정보를 자동으 로 업데이트할 수 있다. 상술한 본 발명이 해결하고자 하는 과제는 이상에서 언급한 것으로 제한되지 않으며, 언급되지 않은 또 다른 해 결하고자 하는 과제는 아래의 기재로부터 본 발명이 속하는 통상의 지식을 가진 자에게 명확하게 이해될 수 있 을 것이다."}
{"patent_id": "10-2020-0011399", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시 예에 따르면, 서버를 이용한 3차원 가상 공간을 구축하기 위한 방법에 있어서, 전자 장치로부터 하나 이상의 이미지 데이터를 수신하는 단계; 상기 하나 이상의 이미지 데이터 각각에서 배경 영역과 상기 배경 영역 이외의 오브젝트를 인식하고, 상기 오브젝트를 기 설정된 카테고리에 기초하여 분류하는 단계; 상기 배경 영역 과 상기 분류된 오브젝트를 기초로 복수의 3차원 이미지 데이터를 생성하는 단계; 및 상기 생성된 복수의 3차원 이미지 데이터를 정합하여 상기 3차원 가상 공간에 대응하는 3차원 이미지 데이터를 생성하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 상기 배경 영역과 상기 분류된 오브젝트를 기초로 상기 복수의 3차원 이미지 데이터를 생 성하는 단계는, 상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트에 기초하여 상기 기 설정된 카테고리에 대응하는 3차원 오브젝트 이미지 데이터를 생성하는 단계 및 상기 하나 이상의 이미지 데이터 각각에서의 상기 배경 영역에 기초하여 3차원 배경 이미지 데이터를 생성하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트에 기초하여 상기 기 설정된 카테 고리에 대응하는 3차원 오브젝트 이미지 데이터를 생성하는 단계는, 상기 전자 장치로부터 수신된 사용자 제공 데이터, 상기 서버의 인공 지능 기반 학습 모델 및 상기 서버의 머신 러닝 기반 학습 모델 중 적어도 하나에 기 초하여, 상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트의 왜곡 여부를 확인하는 단계, 및 상기 기 설 정된 카테고리에 대응되도록 분류된 오브젝트의 왜곡 확인 시, 상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트를, 대응되는 오브젝트로 변경하거나 이미지 처리 기술을 이용하여 이미지 보정하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 상기 방법은, 상기 하나 이상의 이미지 데이터 중 적어도 하나에 기 설정된 적어도 하나 의 왜곡 발생 요소가 포함되어 있는지를 확인하는 단계, 및 상기 하나 이상의 이미지 데이터 중 적어도 하나에 상기 기 설정된 적어도 하나의 왜곡 발생 요소가 포함되어 있으면, 상기 왜곡 발생 요소를 제거하는 단계를 포 함할 수 있다. 일 실시 예에 따르면, 상기 적어도 하나의 왜곡 발생 요소는, 그림자에 대응하는 이미지 및 빛 반사에 대응하는 이미지 중 적어도 하나를 포함하며, 상기 기 설정된 적어도 하나의 왜곡 발생 요소가 포함되어 있는지를 확인하 는 단계는, 인공 기능 기반 또는 머신 러닝 기반 학습 모델에 기초하여, 상기 하나 이상의 이미지 데이터에 왜 곡 발생 요소가 포함되어 있는지 확인하거나 또는 상기 하나 이상의 이미지 데이터 간의 비교에 기초하여 상기 하나 이상의 이미지 데이터 중 적어도 하나에 왜곡 발생 요소가 포함되어 있는지를 확인하는 단계를 포함하며, 상기 왜곡 발생 요소를 제거하는 단계는, 상기 왜곡 발생 요소가 포함된 오브젝트를 상기 왜곡 발생 요소가 포 함된 오브젝트와 대응되는 기 저장된 오브젝트로 대체하거나 또는 상기 왜곡 발생 요소가 포함된 오브젝트에 대 한 이미지 처리를 수행하는 단계를 포함할 수 있다. 일 실시 예에 따르면, 상기 방법은, 특징점들의 비교에 기초하여 상기 수신한 하나 이상의 이미지 데이터와 대 응되는 3차원 이미지 데이터가 저장되어 있는지를 확인하는 단계, 및 상기 하나 이상의 이미지 데이터와 대응되 는 상기 3차원 이미지 데이터가 저장되어 있으면, 상기 하나 이상의 이미지 데이터에 대응하는 시간 정보, 해상 도, 선명도, 및 왜곡 발생 요소 중 적어도 하나에 기초하여, 상기 저장된 3차원 이미지 데이터의 업데이트 필요 여부를 확인하는 단계를 더 포함하며, 상기 분류하는 단계는, 상기 저장된 3차원 이미지 데이터의 업데이트가필요한 것으로 확인된 것에 기초하여 수행될 수 있다. 일 실시 예에 따르면, 상기 확인하는 단계에서, 상기 수신한 하나 이상의 이미지 데이터의 특징점들의 좌표들과 상기 3차원 이미지 데이터의 특징점들의 좌표들 간의 매칭 수가 소정의 임계 값 이상이면, 상기 수신한 하나 이 상의 이미지 데이터와 상기 3차원 이미지 데이터가 대응되는 것으로 결정할 수 있다. 일 실시 예에 따르면, 상기 방법은, 상기 하나 이상의 이미지 데이터에 대응하는, 상기 전자 장치가 위치하는 공간의 환경 정보 및 컨텍스트 정보 중 적어도 하나의 정보를 수신하는 단계 및 상기 3차원 가상 공간에 대응하 는 상기 3차원 이미지 데이터와 대응되도록, 상기 수신된 적어도 하나의 정보를 저장하는 단계를 더 포함할 수 있다. 일 실시 예에 따르면, 컴퓨터 프로그램을 저장하고 있는 컴퓨터 판독 가능 기록매체로서, 상기 컴퓨터 프로그램 은, 전자 장치로부터 하나 이상의 이미지 데이터를 수신하는 단계; 상기 하나 이상의 이미지 데이터 각각에서 배경 영역과 상기 배경 영역 이외의 오브젝트를 인식하고, 상기 오브젝트를 기 설정된 카테고리에 기초하여 분 류하는 단계; 상기 배경 영역과 상기 분류된 오브젝트를 기초로 복수의 3차원 이미지 데이터를 생성하는 단계; 및 상기 생성된 복수의 3차원 이미지 데이터를 정합하여 상기 3차원 가상 공간에 대응하는 3차원 이미지 데이터 를 생성하는 단계를 포함하는 방법을 프로세서가 수행하도록 하기 위한 명령어를 포함할 수 있다. 일 실시 예에 따르면, 컴퓨터 판독 가능한 기록매체에 저장되어 있는 컴퓨터 프로그램으로서, 상기 컴퓨터 프로 그램은, 전자 장치로부터 하나 이상의 이미지 데이터를 수신하는 단계; 상기 하나 이상의 이미지 데이터 각각에 서 배경 영역과 상기 배경 영역 이외의 오브젝트를 인식하고, 상기 오브젝트를 기 설정된 카테고리에 기초하여 분류하는 단계; 상기 배경 영역과 상기 분류된 오브젝트를 기초로 복수의 3차원 이미지 데이터를 생성하는 단계; 및 상기 생성된 복수의 3차원 이미지 데이터를 정합하여 상기 3차원 가상 공간에 대응하는 3차원 이미지 데이터를 생성하는 단계를 포함하는 방법을 프로세서가 수행하도록 하기 위한 명령어를 포함할 수 있다. 일 실시 예에 따르면, 3차원 가상 공간을 구축하기 위한 장치에 있어서, 통신 모듈; 및 상기 통신 모듈을 이용 하여, 전자 장치로부터 하나 이상의 이미지 데이터를 수신하며, 상기 하나 이상의 이미지 데이터 각각에서 배경 영역과 상기 배경 영역 이외의 오브젝트를 인식하고, 상기 오브젝트를 기 설정된 카테고리에 기초하여 분류하며, 상기 배경 영역과 상기 분류된 오브젝트를 기초로 복수의 3차원 이미지 데이터를 생성하며, 상기 생 성된 복수의 3차원 이미지 데이터를 정합하여 상기 3차원 가상 공간에 대응하는 3차원 이미지 데이터를 생성하 는 제어부를 포함할 수 있다."}
{"patent_id": "10-2020-0011399", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시 예에 따른 3차원 가상 공간을 구축하기 위한 방법, 컴퓨터 판독 가능한 기록 매체, 컴퓨터 프로 그램 및 장치는, 자동화된 고품질의 3차원 공간 구축을 위한 기술을 제공할 수 있다. 예를 들어, 본 발명의 실시 예는, 가상 공간 서비스의 실시간 생성(획득), 고품질 가상 공간 구축의 자동화, 및 /또는 기 구축된 가상 공간의 업데이트 자동화 프레임워크를 정의할 수 있다."}
{"patent_id": "10-2020-0011399", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "먼저, 본 발명의 장점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되는 실 시 예들을 참조하면 명확해질 것이다. 여기에서, 본 발명은 이하에서 개시되는 실시 예들에 한정되는 것이 아니 라 서로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시 예들은 본 발명의 개시가 완전하도록 하고, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 발명의 범주를 명확하게 이해할 수 있도록 하기 위해 예시 적으로 제공되는 것이므로, 본 발명의 기술적 범위는 청구항들에 의해 정의되어야 할 것이다. 아울러, 아래의 본 발명을 설명함에 있어서 공지 기능 또는 구성 등에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명을 생략할 것이다. 그리고, 후술되는 용어들은 본 발명에서의 기능을 고려하여 정의된 용어들인 것으로, 이는 사용자, 운용자 등의 의도 또는 관례 등에 따라 달 라질 수 있음은 물론이다. 그러므로, 그 정의는 본 명세서의 전반에 걸쳐 기술되는 기술사상을 토대로 이루어져 야 할 것이다. 이하, 첨부된 도면을 참조하여 본 발명의 바람직한 실시 예에 대하여 상세하게 설명한다. 도 1a는 본 발명의 일 실시 예에 따른 3차원 가상 공간 구축을 위한 시스템의 블록도이다. 도 1b는 본 발명의 일 실시 예에 따른 서버의 제어부의 블록도이다. 도 1c는 본 발명의 일 실시 예에 따른 서버의 제어부의 블록도 이다. 도 1a를 참조하면, 시스템은, 전자 장치, 서버, 서버 및/또는 서버를 포함할 수 있다. 도 1a를 참조하면, 전자 장치는 프로세서, 카메라 모듈, 표시부, 입출력 인터페이스, 통신 인터페이스, 센서 모듈 및/또는 메모리를 포함할 수 있다. 프로세서(제어부, 제어 장치 또는 제어 회로라고도 함)는 연결된 전자 장치의 적어도 하나의 다른 구성 요소(예: 하드웨어 구성 요소(예: 카메라 모듈, 표시부, 입출력 인터페이스, 통신 인터페이스, 센서 모듈 및/또는 메모리) 또는 소프트웨어 구성 요소)를 제어할 수 있고, 다양한 데이터 처리 및 연 산을 수행할 수 있다. 카메라 모듈은 정지 영상 및 동영상을 촬영할 수 있다. 예를 들어, 카메라 모듈은 하나 이상의 이미지 센서 및 하나 이상의 렌즈를 포함할 수 있다. 예를 들어, 카메라 모듈은 360도 카메라를 포함할 수 있다. 표시부는 전자 장치의 사용자에게 정보를 시각적으로 제공하기 위한 장치로서, 예를 들면, 디스플레이 (또는 터치스크린), 홀로그램 장치, 또는 프로젝터 및 해당 장치를 제어하기 위한 제어 회로를 포함할 수 있다. 입출력 인터페이스는 예를 들면, 사용자 또는 다른 외부 장치(기기)로부터 입력된 명령 또는 데이터를 장치 의 다른 구성요소(들)에 전달하거나, 또는 장치의 다른 구성요소(들)로부터 수신된 명령 또는 데이터를 사용자 또는 다른 외부 기기, 예를 들어, 서버, 서버 및/또는 서버로 출력할 수 있다. 통신 인터페이스는 전자 장치와 외부 장치, 예를 들어, 서버, 서버 및/또는 서버간의 유선 또는 무선 통신 채널의 수립, 및 수립된 통신 채널을 통한 통신 수행을 지원할 수 있다. 예를 들어, 통신 인터페이스는 무선 통신 모듈 (예: 셀룰러 통신 모듈, 와이파이 통신 모듈, 근거리 무선 통신 모듈, 또는 GNSS(global navigation satellite system) 통신 모듈) 및/또는 유선 통신 모듈 (예: LAN(local area network) 통신 모듈, 또는 전력선 통신 모듈)을 포함하고, 그 중 해당하는 통신 모듈을 이용하여 제1 네트워크(미도시) (예: 블루투스, WiFi direct 또는 IrDA(infrared data association) 같은 근거리 통신 네트워크) 또는 제2 네 트워크(미도시)(예: 셀룰러 네트워크, 인터넷, 또는 컴퓨터 네트워크(예: LAN 또는 WAN)와 같은 원거리 통신 네 트워크)를 통하여 상기 외부 장치와 통신할 수 있다. 센서 모듈은 전자 장치의 내부의 작동 상태(예: 전력 또는 온도) 또는 외부의 환경 상태에 대응하는 전 기 신호 또는 데이터 값을 생성할 수 있다. 센서 모듈은, 예를 들면, 자이로 센서, 기압 센서, 가속도 센서, 온도 센서, 습도 센서, 및/또는 조도 센서를 포함할 수 있다. 메모리는 전자 장치의 적어도 하나의 구성요소(프로세서, 카메라 모듈, 표시부, 입출력 인 터페이스, 통신 인터페이스, 센서 모듈 및/또는 메모리)에 의해 사용되는 다양한 데이터, 예를 들어, 소프트웨어(예: 프로그램) 및, 이와 관련된 명령에 대한 입력 데이터 또는 출력 데이터를 저장할 수 있다. 메모리는, 휘발성 메모리 또는 비휘발성 메모리를 포함할 수 있다. 도시하지 않았지만, 상기 전자 장치는 추가로 GPS(global positioning system) 모듈을 더 포함할 수 있다. 일 실시 예에 따르면, 상기 프로세서는 상기 카메라 모듈, 상기 입출력 인터페이스, 상기 통신 인 터페이스, 상기 센서 모듈, 및/또는 상기 GPS 모듈(미도시)를 제어하여, 상기 전자 장치가 위치하 는 공간과 관련된 정보(데이터라고도 함)를 획득할 수 있다. 예를 들어, 상기 전자 장치가 위치하는 공간과 관련된 정보는, 상기 전자 장치가 위치하는 공간의 적어 도 일부를 촬영하여 획득한 하나 이상의 이미지 데이터(2차원 이미지 데이터라고도 함), 상기 하나 이상의 이미 지 데이터 각각에 대응하는 상기 전자 장치가 위치하는 공간의 환경 정보(바람, 온도, 습도, 및/또는 일조 량 등), 상기 하나 이상의 이미지 데이터 각각에 대응하는 컨텍스트(context) 정보(상기 2차원 이미지 데이터를 획득한 시간, 상기 2차원 이미지 데이터를 획득한 장소, 상기 2차원 이미지 데이터를 획득할 때의 날씨 등) 및/ 또는 상기 전자 장치가 서버로부터 수신한 위치 정보를 포함하는 하나 이상의 이미지 데이터를 포함 할 수 있다. 예를 들어, 상기 프로세서는 상기 카메라 모듈을 제어하여 전자 장치가 위치하는 공간의 적어도 일 부를 촬영하여 하나 이상의 이미지 데이터를 획득할 수 있다. 예를 들어, 상기 프로세서는 상기 입출력 인터페이스를 이용하여 상기 하나 이상의 이미지 데이터 및/ 또는 상기 하나 이상의 이미지 데이터에 포함된 하나 이상의 오브젝트에 대한 사용자 입력 수신을 통해, 상기 하나 이상의 이미지 데이터에 대응하는 사용자 제공 데이터 및/또는 상기 하나 이상의 이미지 데이터에 포함된 상기 하나 이상의 오브젝트에 대응하는 사용자 제공 데이터를 획득할 수 있다. 예를 들어, 상기 하나 이상의 이 미지 데이터에 대응하는 사용자 제공 데이터는 상기 하나 이상의 이미지 데이터의 위치 정보(방, 사무실, 공원 등)를 포함할 수 있다. 예를 들어, 상기 하나 이상의 이미지 데이터에 포함된 상기 하나 이상의 오브젝트에 대 응하는 사용자 제공 데이터는 상기 하나 이상의 오브젝트가 무엇인지에 대한 정보(책상, 의자, 꽃, 나무 등)를 포함할 수 있다. 예를 들어, 상기 하나 이상의 이미지 데이터에 대응하는 사용자 제공 데이터 및/또는 상기 하 나 이상의 이미지 데이터에 포함된 상기 하나 이상의 오브젝트에 대응하는 사용자 제공 데이터는, 예를 들어, 공간(사무실, 방, 부엌 등), 물체(사람, 공, 꽃병 등), 재질(유리, 돌, 쇠 등), 및/또는 상표(메이커(maker)를 포함할 수 있으며, 3차원 이미지 데이터 생성 시(3차원 공간 구축 시)의 왜곡 보정을 위한 보정 정보로 이용할 수도 있다. 예를 들어, 상기 프로세서는 상기 통신 인터페이스를 이용하여 외부의 서버, 예를 들어, 서버로 부터 상기 전자 장치가 위치하는 공간, 즉, 위치의 환경 정보 및/또는 컨텍스트 정보를 수신할 수 있다. 예 를 들어, 상기 프로세서는 상기 센서 모듈을 이용하여 상기 전자 장치가 위치하는 공간의 환경 정 보, 예를 들어, 바람, 온도, 습도, 및/또는 일조량 등을 획득할 수 있다. 예를 들어, 상기 전자 장치가 위 치하는 공간의 환경 정보 및/또는 컨텍스트 정보는, 상기 하나 이상의 이미지 데이터의 획득 시 획득될 수 있다. 예를 들어, 상기 프로세서는 상기 통신 인터페이스를 이용하여, 서버, 예를 들어, 특정 웹 사이 트를 제공하는 서버 및/또는 특정 SNS(social network sevices/sites)를 제공하는 서버로부터, 하나 이상의 이 미지(사진) 데이터를 수신하여 상기 하나 이상의 이미지 데이터를 획득할 수 있다. 예를 들어, 특정 웹 사이트 를 제공하는 서버 및/또는 특정 SNS(social network sevices/sites)를 제공하는 서버로부터, 수신된 하나 이상 의 이미지 데이터에는 위치 정보가 포함(태깅)될 수 있다. 일 실시 예에 따르면, 상기 프로세서는 상기 통신 인터페이스를 이용하여 상기 획득한 전자 장치가 위치하는 공간과 관련된 정보를 서버로 전송할 수 있다. 일 실시 예에 따르면, 상기 프로세서는 카메라 모듈을 통해 이미지 데이터를 획득하는 동안, 상기 획득 하는 이미지 데이터에 대응하는 이미지 및 상기 이미지 데이터의 획득과 관련된 현황 정보를 표시부에 디스 플레이할 수 있다. 상기 프로세서는 사용자 입력에 기초하여, 상기 카메라 모듈을 통한 이미지 데이터 의 획득을 종료하며 상기 카메라 모듈을 통해 획득된 전체 이미지 데이터를 통신 인터페이스를 이용하 여 서버로 전송할 수 있다. 상기 프로세서는 상기 전체 이미지 데이터의 전송에 응답하여, 상기 통신 인터 페이스를 이용하여 상기 서버에 의해 생성된 3차원 이미지 데이터를 수신할 수 있다. 상기 프로세서는 상기 수신된 3차원 이미지 데이터에 대응하는 3차원 이미지를 상기 표시부에 디스플레이할 수 있다. 예를 들어, 상기 현황 정보는, 상기 획득하는 이미지 데이터의 사이즈, 현재까지 획득한 이미지 데이터의 개수 및 현 재까지 획득한 이미지 데이터의 전체 용량 중 적어도 하나를 포함할 수 있다. 일 실시 예에 따르면, 상기 전자 장치는 스마트폰(smartphone), 랩탑 PC(laptop personal computer), PDA(personal digital assistant), 카메라(camera), 및/또는 웨어러블 장치(wearable device)등 다양한 형태로 구현될 수 있다. 도 1a를 참조하면, 서버는, 제어부, 통신 모듈 및/또는 메모리를 포함할 수 있다. 제어부(프로세서, 제어 장치 또는 제어 회로라고도 함)는 연결된 서버의 적어도 하나의 다른 구성 요 소(예: 하드웨어 구성 요소(예: 통신 모듈 및/또는 메모리) 또는 소프트웨어 구성 요소)를 제어할 수 있고, 다양한 데이터 처리 및 연산을 수행할 수 있다. 통신 모듈은 서버와 외부 장치, 예를 들어, 전자 장치간의 유선 또는 무선 통신 채널의 수립, 및 수립된 통신 채널을 통한 통신 수행을 지원할 수 있다. 메모리(데이터베이스라고도 함)는 서버의 적어도 하나의 구성요소(제어부 및/또는 통신 모듈 )에 의해 사용되는 다양한 데이터, 예를 들어, 소프트웨어(예: 프로그램) 및, 이와 관련된 명령에 대한 입 력 데이터 또는 출력 데이터를 저장할 수 있다. 일 실시 예에 따르면, 상기 제어부는 상기 통신 모듈을 이용하여 상기 전자 장치로부터 상기 전 자 장치가 위치하는 공간과 관련된 정보를 수신하도록 제어할 수 있다. 일 실시 예에 따르면, 상기 제어부는, 상기 수신된 상기 전자 장치가 위치하는 공간과 관련된 정보에 포함된 왜곡 발생 요소(노이즈(noise)라고도 함)(조명, 그림자, 거울 내 반사된 이미지, 소음 등)을 검출할 수 있다. 상기 제어부는 상기 전자 장치가 위치하는 공간과 관련된 정보에서, 상기 검출된 왜곡 발생 요 소를 제거할 수 있다. 상기 제어부는 상기 왜곡 발생 요소가 제거된 상기 전자 장치가 위치하는 공간 과 관련된 정보에 기초하여, 3차원 공간 정보, 예를 들어 3차원 이미지 데이터를 자동으로 생성할 수 있다. 상 기 제어부는, 상기 3차원 공간 정보의 생성 시, 발생할 수 있는 왜곡을, 상기 전자 장치로부터 수신된 사용자 제공 데이터 및/또는 인공 지능(AI; artificial intelligence) 및/또는 머신 러닝(ML; machine learning) 기반 학습을 이용하여(예를 들어, 인공 지능 학습 모델 및/또는 머신 러닝 학습 모델을 이용하여) 고 품질의 3차원 공간으로 자동 보정, 즉 고품질의 3차원 이미지 데이터를 자동으로 생성할 수 있다. 상기 제어부 는, 상기 수신된 상기 전자 장치가 위치하는 공간과 관련된 정보를 기초로 상호 작용(interaction)이 가능한 증강 현실(AR; augmented reality), 가상 현실(VR; virtual reality), 및/또는 혼합 현실(MR; mixed reality) 서비스 제공을 위해, 3차원 가상 공간 내 재질, 조명, 환경(바람, 온도, 습도 등)의 정보가 대응되도 록(포함되도록)하여, 3차원 가상 공간 정보를 저장할 수 있다. 상기 제어부는 상기 수신된 상기 전자 장치 가 위치하는 공간과 관련된 정보가 기 저장된(기 구축된) 3차원 가상 공간에 대응하는 3차원 이미지 데이터 와 대응되는지를 확인할 수 있다. 상기 제어부는 상기 수신된 상기 전자 장치가 위치하는 공간과 관련 된 정보가 기 저장된(기 구축된) 3차원 가상 공간에 대응하는 3차원 이미지 데이터와 대응되는 경우, 상기 수신 된 상기 전자 장치가 위치하는 공간과 관련된 정보를 기 설정된 조건에 따라 평가(촬영 시간, 해상도, 선명 도, PSNR(peak signal to noise ratio; 최대 신호 대 잡음비) 등에 기초하여 평가)하여, 상기 기 저장된 3차원 가상 공간에 대응하는 3차원 이미지 데이터의 업데이트 필요 여부를 확인하고, 필요 시, 3차원 이미지 데이터의 업데이트 동작을 수행할 수 있다. 일 실시 예에 따르면, 상기 제어부는, 3차원 이미지 데이터의 모델링(생성), 기 모델링된(기 생성된) 3차 원 이미지 데이터의 업데이트 및/또는 3차원 이미지 데이터의 제공 시 사용자가 4차원의 경험을 할 수 있도록 상기 3차원 이미지 데이터에 대응되는 물리적 환경 제어 기능을 설정할 수 있다. 도 1b를 참조하면, 상기 제어부는, 데이터 전처리부, 데이터 보유 여부 판단부, 3D 모델링 처리 부, 3D 모델링 업데이트부 및/또는 4D 모델링부를 포함할 수 있다. 데이터 전처리부는, 상기 전자 장치로부터 수신된, 상기 전자 장치가 위치하는 공간과 관련된 정 보에 대한 전처리를 수행할 수 있다. 예를 들어, 상기 전자 장치가 위치하는 공간과 관련된 정보는, 하나 이상의 이미지 데이터 및/또는 상기 하나 이상의 이미지 데이터 각각에 대응하는 정보(상기 전자 장치가 위 치하는 공간의 환경 정보 및/또는 컨텍스트 정보)를 포함할 수 있다. 예를 들어, 상기 데이터 전처리부는, 상기 전자 장치가 위치하는 공간과 관련된 정보를 지정된 조건에 기초하여 분류할 수 있다. 상기 지정된 조건은, 위치, 시간, 및/또는 데이터 타입일 수 있다. 예를 들어, 상기 데이터 전처리부는, 상기 전자 장치가 위치하는 공간과 관련된 정보에 포함된 하나 이상의 이미지 데 이터 각각에 대해, 대응하는 정보(상기 전자 장치가 위치하는 공간의 환경 정보 및/또는 컨텍스트 정보)를 태깅(tagging) 할 수 있다. 예를 들어, 상기 하나 이상의 이미지 데이터 각각에 대한 대응하는 정보는, 이미지 데이터가 획득된 위치, 이미지 데이터가 획득된 시간, 및/또는 이미지 데이터의 데이터 타입을 포함할 수 있다. 예를 들어, 상기 데이터 전처리부는, 인공 지능(AI; artificial intelligence) 및/또는 머신 러닝(ML; machine learning) 기반 학습을 이용하여 상기 하나 이상의 이미지 데이터 각각에 포함된 오브젝트(물체)를 인식(인지)할 수 있다. 예를 들어, 상기 데이터 전처리부는, 상기 하나 이상의 이미지 데이터 각각에 기 설정된 왜곡 발생 요소 (예를 들어, 조명, 그림자, 및/또는 행인 등)가 포함되어 있는지를 확인할 수 있으며, 상기 이미지 데이터 각각 에 기 설정된 왜곡 발생 요소가 포함되어 있는 경우 상기 이미지 데이터 각각에서 상기 기설정된 왜곡 발생 요 소를 제거할 수 있다. 예를 들어, 상기 이미지 데이터 각각에 상기 기 설정된 왜곡 발생 요소가 포함되어 있는 지의 확인은, 상기 인공 지능 및/또는 머신 러닝 기반 학습을 이용한 오브젝트 인식에 기초할 수 있다. 데이터 보유 여부 판단부는, 상기 전처리부에 의해 상기 전처리된, 상기 전자 장치가 위치하는 공간과 관련된 복수의 정보를 비교 분석하여, 메모리에 상기 전처리된, 상기 전자 장치가 위치하는 공 간과 관련된 복수의 정보에 포함된, 상기 하나 이상의 이미지 데이터와 대응되는 데이터의 저장(보유) 여부를 확인할 수 있다. 예를 들어, 데이터 보유 여부 판단부는, 특징점들에 기초하여, 상기 전처리된 하나 이상 의 이미지 데이터와 유사한(매칭되는) 이미지 데이터가 메모리에 저장되어 있는지를 확인할 수 있다. 3D 모델링 처리부는, 상기 데이터 보유 여부 판단부의 동작에 따라 메모리에 상기 전처리된 하 나 이상의 이미지 데이터와 대응되는 데이터가 저장되지 않은 것으로 확인하면, 상기 전처리된 하나 이상의 이 미지 데이터를 기초로, 3차원 이미지 데이터를 모델링할 수 있다. 예를 들어, 상기 3D 모델링 처리부는, 포토그래메트리(photogrammetry) 방식 등 다양한 3차원 이미지 데이터의 모델링 방식 중 하나를 이용하여 상기 3차원 이미지 데이터의 모델링을 수행할 수 있다. 3D 모델링 업데이트부는, 상기 데이터 보유 여부 판단부의 동작에 따라 메모리에 상기 전처리된 하나 이상의 이미지 데이터와 대응되는 데이터(2차원 이미지 데이터 및/또는 3차원 이미지 데이터)가 저장된 것으로 확인되면, 기 설정된 조건에 기초하여, 상기 전처리된 하나 대응되는 데이터의 업데이트 필요 여부를 확인 할 수 있다. 예를 들어, 상기 3D 모델링 업데이트부는, 상기 전처리된 하나 이상의 이미지 데이터 및 상기 대응되는 데이터의 촬영 시간, 해상도, 선명도, 및/또는 기 설정된 왜곡 발생 요소의 비교를 통해, 상기 대응되 는 데이터의 업데이트 필요 여부를 확인할 수 있다. 예를 들어, 3D 모델링 업데이트부는, 상기 전처리된 하나 이상의 이미지 데이터의 촬영 시간이, 상기 대응 되는 데이터의 촬영 시간이 포함된 시간 간격에 포함되지 않을 경우, 상기 대응되는 데이터의 업데이트가 필요 한 것으로 결정할 수 있다. 3D 모델링 업데이트부는, 상기 전처리된 하나 이상의 이미지 데이터의 촬영 시 간이, 상기 대응되는 데이터의 촬영 시간이 포함된 시간 간격에 포함되는 경우, 상기 대응되는 데이터의 업데이 트가 필요하지 않은 것으로 결정할 수 있다. 예를 들어, 3D 모델링 업데이트부는, 상기 전처리된 하나 이상의 이미지 데이터의 해상도가, 상기 대응되 는 데이터의 해상도보다 지정된 임계 값 이상으로 높을 경우, 상기 대응되는 데이터의 업데이트가 필요한 것으 로 결정할 수 있다. 3D 모델링 업데이트부는, 상기 전처리된 하나 이상의 이미지 데이터의 해상도가, 상기 대응되는 데이터의 해상도보다 지정된 임계 값보다 낮을 경우, 상기 대응되는 데이터의 업데이트가 필요하지 않 은 것으로 결정할 수 있다. 예를 들어, 3D 모델링 업데이트부는, 상기 전처리된 하나 이상의 이미지 데이터의 선명도가, 상기 대응되 는 데이터의 선명도보다 지정된 임계 값 이상으로 높을 경우, 상기 대응되는 데이터의 업데이트가 필요한 것으 로 결정할 수 있다. 3D 모델링 업데이트부는, 상기 전처리된 하나 이상의 이미지 데이터의 선명도가, 상기 대응되는 데이터의 선명도보다 지정된 임계 값보다 낮은 경우, 상기 대응되는 데이터의 업데이트가 필요하지 않 은 것으로 결정할 수 있다. 예를 들어, 3D 모델링 업데이트부는, 상기 전처리된 하나 이상의 이미지 데이터가 상기 대응되는 데이터에 비해, 기 설정된 왜곡 발생 요소가 적게 포함된 경우, 상기 대응되는 데이터의 업데이트가 필요한 것으로 결정 할 수 있다. 3D 모델링 업데이트부는, 상기 전처리된 하나 이상의 이미지 데이터가 상기 대응되는 데이터 에 비해, 기 설정된 왜곡 발생 요소가 많이 포함된 경우, 상기 대응되는 데이터의 업데이트가 필요하지 않은 것으로 결정할 수 있다. 3D 모델링 업데이트부는, 상기 대응되는 데이터의 업데이트가 필요한 것으로 결정하면, 상기 전처리된 하 나 이상의 이미지 데이터 및 상기 하나 이상의 이미지 데이터 각각에 대응하는 정보(상기 전자 장치가 위치 하는 공간의 환경 정보 및/또는 컨텍스트 정보)를 메모리에 저장할 수 있다. 3D 모델링 업데이트부는, 상기 대응되는 데이터의 업데이트가 필요한 것으로 결정하면, 상기 전처리된 하나 이상의 이미지 데이터, 상기 하나 이상의 이미지 데이터 각각에 대응하는 정보(상기 전자 장치가 위치하는 공간의환경 정보 및/또는 컨텍스트 정보) 및 상기 대응되는 데이터에 기초하여, 3차원 이미지 데이터를 모델링하여 저 장(업데이트)할 수 있다. 예를 들어, 3D 모델링 업데이트부에 의해 저장되는 3차원 이미지 데이터는 날짜 및/또는 시간 별로 관리될 수 있다. 4D 모델링부(4D 모델링 인코딩 및 4D 렌더링부라고도 할 수 있음)는, 상기 3D 모델링 처리부 또는 상기 3D 모델링 업데이트부를 통해 모델링된 3차원 이미지 데이터에, 상기 전자 장치로부터 수신된, 상기 전자 장치가 위치하는 공간과 관련된 복수의 정보 중 상기 하나 이상의 이미지 데이터 각각에 대응하는 정보(상 기 전자 장치가 위치하는 공간의 환경 정보 및/또는 컨텍스트 정보)를 대응시켜(매칭시켜) 저장할 수 있다. 4D 모델링부는, 모델링된 3차원 이미지 데이터에 상기 하나 이상의 이미지 데이터 각각에 대응하는 정보(상 기 전자 장치가 위치하는 공간의 환경 정보 및/또는 컨텍스트 정보)를 대응시켜(매칭시켜) 저장한 것을 이 용하여, 사용자에게 상기 모델링된 3차원 이미지 데이터의 제공 시, 상기 사용자가 상기 환경 정보 및/또는 컨 텍스트 정보에 대응하는 경험을 할 수 있도록 대응하는 물리적 환경 제어 정보를 설정할 수 있다. 예를 들어, 4D 모델링부는, 상기 모델링된 3차원 이미지 데이터에 상기 환경 정보 및/또는 상기 컨텍스트 정보를 추가해, 사용자에게 4D(4 dimensional)의 경험을 제공할 수 있도록 인코딩(encoding)할 수 있다. 예를 들어, 4D 모델링부는, 구축된 4D 공간을 디코딩(decoding)하여, 상기 환경 정보 및/또는 상기 컨텍스트 정 보를 사용자 환경에 맞추어 렌더링(rendering)할 수 있다. 일 실시 예에 따르면, 상기 제어부는, 상기 통신 모듈을 통해 수신된 사용자 제공 데이터 및/또는 인 공 지능 및/또는 머신 러닝 기반 학습에 기초하여, 3차원 공간의 왜곡을 최소화(보완)할 수 있는 3차원 이미지 데이터를 생성할 수 있다. 예를 들어, 상기 사용자 제공 데이터는 하나 이상의 이미지 데이터의 위치 정보(방, 사무실, 공원 등) 및/또는 상기 하나 이상의 이미지 데이터에 포함된 상기 하나 이상의 오브젝트가 무엇인지에 대한 정보(책상, 의자, 꽃, 나무 등)를 포함할 수 있다. 도 1c를 참조하면, 상기 제어부는 이미지 세그먼테이션(segmentation)부, 3D 모델링부, 오브젝 트 검출부 및/또는 공간 데이터 정합부를 포함할 수 있다. 예를 들어, 상기 이미지 세그먼테이션부 , 3D 모델링부, 오브젝트 검출부 및/또는 공간 데이터 정합부는 도 1b의 3D 모델링 처리부 및 3D 모델링 업데이트부에 포함될 수도 있다. 이미지 세그먼테이션부는, 상기 전자 장치로부터 수신된 하나 이상의 이미지 데이터(원본 이미지 데이 터라고도 함) 각각에 포함된, 지정된(기 설정된) 하나 이상의 오브젝트 카테고리 각각에 대응하는, 하나 이상의 오브젝트를 확인하고, 상기 하나 이상의 이미지 데이터 각각에서 상기 확인된 하나 이상의 오브젝 트를 세그먼테이션(분할)할 수 있다. 상기 하나 이상의 오브젝트 카테고리는, 자연물(예: 나무, 풀 등), 정형 사물(정형 오브젝트)(예: 테이블, 냉장고 등), 및/또는 비정형 사물(비정형 오브젝트)(예: 의자, 선풍기 등)를 포함할 수 있다. 이미지 세그먼테이션부는, 상기 하나 이상의 이미지 데이터 각각에서 상기 확인된 하나 이상의 오브젝트를 세그먼테이션할 때, 상기 하나 이상의 이미지 데이터 각각에서의 상기 확인된 하 나 이상의 오브젝트의 위치 정보(영역 정보 또는 좌표 정보)를 저장(기록)할 수 있다. 예를 들어, 상기 세그먼 테이션된 하나 이상의 오브젝트 및 상기 세그먼테이션된 하나 이상의 오브젝트 각각의 위치 정보는 메모리 에 저장될 수 있다. 3D 모델링부는, 상기 이미지 세그먼테이션부에 의해 상기 세그먼테이션된 하나 이상의 오브젝트에서, 동일한 오브젝트 카테고리에 포함된(상기 하나 이상의 이미지 데이터에 대한 서로 대응되는), 세그먼테이션된 오브젝트들을 그룹화하여 3차원 이미지 데이터를 모델링할 수 있다. 예를 들어, 상기 3D 모델링부는, 상기 동일한 오브젝트 카테고리에 포함된, 세그먼테이션된 오브젝트들에 대한 왜곡 여부(빛 반사, 낮은 선명도, 및/ 또는 외곽선 왜곡(직선의 외곽선이 곡선으로 왜곡) 등)를 확인하여, 왜곡 확인 시, 왜곡 보정을 수행할 수 있다. 상기 왜곡 보정은, 상기 통신 모듈을 통해 수신된 사용자 제공 데이터 및/또는 인공 지능 및/또는 머신 러닝 기반 학습을 이용하여, 수행될 수 있다. 예를 들어, 상기 사용자 제공 데이터 및/또는 상기 인공 지 능 및/또는 머신 러닝 기반 학습을 이용하여 왜곡이 확인된 오브젝트가, 특정 오브젝트, 예를 들어, 책상으로 확인된 경우, 대응되는 왜곡 되지 않은 오브젝트로 상기 왜곡이 확인된 오브젝트를 대체하거나, 이미지 처리 기 술을 이용하여 왜곡된 부분의 왜곡을 보정할 수 있다. 3D 모델링부는, 상기 세그먼테이션된 하나 이상의 오브젝트를 제외한 하나 이상의 이미지 데이터(배경 이 미지 데이터) 각각에 대해, 포토그래메트리 방식 등 다양한 3차원 이미지 데이터의 모델링 방식 중 하나를 이용 하여 상기 3차원 이미지 데이터의 모델링을 수행할 수 있다.오브젝트 검출부는, 인공 지능 및/또는 머신 러닝 기반 학습을 이용하여, 상기 이미지 세그먼테이션부 에 의해 상기 세그먼테이션된 하나 이상의 오브젝트 각각에 대한 유형 파악을 할 수 있다. 예를 들어, 오 브젝트 검출부는, 비정형 사물로 분류되어 상기 세그먼테이션된 하나 이상의 오브젝트 각각을 의자, 또는 식탁 등으로 파악할 수 있다. 오브젝트 검출부는 상기 유형이 파악된 하나 이상의 오브젝트 각각에 대한 3 차원 공간 정보를 생성할 수 있다. 예를 들어, 오브젝트 검출부는 상기 유형이 파악된 하나 이상의 오브젝 트 각각에 대한 3차원 이미지 데이터에서의 상기 하나 이상의 오브젝트 각각에 대한 공간 정보(좌표 정보)를 설 정 및 저장할 수 있다. 상기 3차원 이미지 데이터에서의 상기 하나 이상의 오브젝트 각각에 대한 공간 정보 는 메모리에 저장될 수 있다. 공간 데이터 정합부는 상기 하나 이상의 오브젝트 각각에 대한 공간 정보에 기초하여 상기 하나 이상 의 오브젝트 각각을, 상기 세그먼테이션된 하나 이상의 오브젝트를 제외한 하나 이상의 이미지 데이터(배경 이 미지 데이터) 각각에 정합하여(포함시켜), 최종 3차원 이미지 데이터를 생성할 수 있다. 상술한 실시 예에서는, 서버가 메모리를 포함하는 것으로 설명하였으나, 다른 실시 예에 따르면, 상 기 메모리는 별도로 구성되어 서버의 통신 모듈을 통해 서버와 연결될 수 있다. 일 실시 예에 따르면, 상기 제어부는 상기 통신 모듈을 이용하여, 전자 장치로부터 하나 이상의 이미지 데이터를 수신하며, 상기 하나 이상의 이미지 데이터 각각에서 배경 영역과 상기 배경 영역 이외의 오브 젝트를 인식하고, 상기 오브젝트를 기 설정된 카테고리에 기초하여 분류(세그먼테이션이라고도 함)하며, 상기 배경 영역과 상기 분류된 오브젝트를 기초로 복수의 3차원 이미지 데이터를 생성하며, 상기 생성된 복수의 3차 원 이미지 데이터를 정합하여 상기 3차원 가상 공간에 대응하는 3차원 이미지 데이터를 생성할 수 있다. 예를 들어, 상기 제어부는, 상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트에 기초하여 상기 기 설정된 카테고리에 대응하는 3차원 오브젝트 이미지 데이터를 생성하고, 상기 하나 이상의 이미지 데이터 각각 에서의 상기 배경 영역에 기초하여 3차원 배경 이미지 데이터를 생성하여, 상기 복수의 3차원 이미지 데이터를 생성할 수 있다. 예를 들어, 상기 제어부는, 상기 전자 장치로부터 수신된 사용자 제공 데이터, 상기 서버의 인공 지능 기반 학습 모델 및 상기 서버의 머신 러닝 기반 학습 모델 중 적어도 하나에 기초하여, 상기 기 설정된 카테고 리에 대응되도록 분류된 오브젝트의 왜곡 여부를 확인할 수 있다. 상기 제어부는, 상기 기 설정된 카테고 리에 대응되도록 분류된 오브젝트의 왜곡 확인 시, 상기 기 설정된 카테고리에 대응되도록 분류된 오브젝트를, 대응되는 오브젝트로 변경하거나 이미지 처리 기술을 이용하여 이미지 보정할 수 있다. 일 실시 예에 따르면, 상기 제어부는 상기 하나 이상의 이미지 데이터 중 적어도 하나에 기 설정된 적어도 하나의 왜곡 발생 요소가 포함되어 있는지를 확인할 수 있다. 상기 제어부는 상기 하나 이상의 이미지 데 이터 중 적어도 하나에 상기 기 설정된 적어도 하나의 왜곡 발생 요소가 포함되어 있으면, 상기 왜곡 발생 요소 를 제거할 수 있다. 예를 들어, 상기 적어도 하나의 왜곡 발생 요소는, 그림자에 대응하는 이미지 및 빛 반사에 대응하는 이미지 중 적어도 하나를 포함할 수 있다. 예를 들어, 상기 제어부는 인공 기능 기반 또는 머신 러닝 기반 학습 모델 에 기초하여, 상기 하나 이상의 이미지 데이터에 왜곡 발생 요소가 포함되어 있는지 확인하거나 또는 상기 하나 이상의 이미지 데이터 간의 비교에 기초하여 상기 하나 이상의 이미지 데이터 중 적어도 하나에 왜곡 발생 요소 가 포함되어 있는지를 확인할 수 있다. 상기 제어부는 상기 왜곡 발생 요소가 포함된 오브젝트를 상기 왜 곡 발생 요소가 포함된 오브젝트와 대응되는 기 저장된 오브젝트로 대체하거나 또는 상기 왜곡 발생 요소가 포 함된 오브젝트에 대한 이미지 처리를 수행할 수 있다. 일 실시 예에 따르면, 상기 제어부는, 특징점들의 비교에 기초하여 상기 수신한 하나 이상의 이미지 데이 터와 대응되는 3차원 이미지 데이터가 저장되어 있는지를 확인할 수 있다. 상기 제어부는 상기 하나 이상 의 이미지 데이터와 대응되는 상기 3차원 이미지 데이터가 저장되어 있으면, 상기 하나 이상의 이미지 데이터에 대응하는 시간 정보, 해상도, 선명도, 및 왜곡 발생 요소 중 적어도 하나에 기초하여, 상기 저장된 3차원 이미 지 데이터의 업데이트 필요 여부를 확인할 수 있다. 상기 분류하는 동작의 경우, 상기 저장된 3차원 이미지 데 이터의 업데이트가 필요한 것으로 확인된 것에 기초하여 수행될 수 있다. 일 실시 예에 따르면, 상기 제어부는 상기 수신한 하나 이상의 이미지 데이터의 특징점들의 좌표들과 상기 3차원 이미지 데이터의 특징점들의 좌표들 간의 매칭 수가 소정의 임계 값 이상이면, 상기 수신한 하나 이상의 이미지 데이터와 상기 3차원 이미지 데이터가 대응되는 것으로 결정할 수 있다.일 실시 예에 따르면, 상기 제어부는, 상기 하나 이상의 이미지 데이터에 대응하는, 상기 전자 장치가 위 치하는 공간의 환경 정보 및 컨텍스트 정보 중 적어도 하나의 정보를 수신할 수 있으며, 상기 3차원 가상 공간 에 대응하는 상기 3차원 이미지 데이터와 대응되도록, 상기 수신된 적어도 하나의 정보를 저장할 수 있다. 도 2는 본 발명의 일 실시 예에 따른 3차원 가상 공간 구축 동작의 흐름도이다. 201 동작에서 전자 장치(전자 장치의 프로세서)는 상기 전자 장치가 위치한 공간과 관련된 정 보를 획득할 수 있다. 일 실시 예에 따르면, 상기 전자 장치는, 3차원 가상 공간 구축을 위해 현실 공간 정보 취득 현황을 AR 기 반으로 시각화하여 사용자의 효과적인 현실 공간 취득을 가이드하는 스마트폰, 이벤트 등의 이미지 획득 장치일 수 있다. 일 실시 예에 따르면, 상기 전자 장치는 카메라 모듈(예: 카메라 모듈)을 이용하여 전자 장치가 위 치한 공간과 관련된 정보, 예를 들어, 하나 이상의 이미지 데이터를 획득(촬영)할 수 있다. 일 실시 예에 따르면, 상기 전자 장치는 통신 인터페이스(예: 통신 인터페이스)를 이용하여 전자 장치 가 위치한 공간과 관련된 정보, 예를 들어, 인터넷 웹 사이트 및/또는 특정 SNS(social network services/sites)에 공개된 이미지 데이터(사진 데이터)를 획득(수신)할 수 있다. 예를 들어, 상기 이미지 데이 터에는 상기 획득된 이미지와 관련된 정보(위치, 시간 등)가 저장(태깅)되어 있을 수 있다. 일 실시 예에 따르면, 상기 전자 장치는 입출력 인터페이스(예: 입출력 인터페이스)를 이용하여, 상기 전자 장치가, 획득된 전자 장치가 위치한 공간과 관련된 정보에 대한 추가 정보 수신할 수 있다. 예를 들어, 상기 추가 정보는, 상기 획득된 공간과 관련된 정보 내의 향후 3D 공간 구축 시 왜곡 보정을 위한 추가 정보(예: 공간(사무실, 방, 부엌 등), 물체(사람, 공, 꽃병 등), 재질(유리, 돌, 쇠 등), 및/또는 메이커 (maker; 브랜드(brand))이름)를 포함할 수 있다. 일 실시 예에 따르면, 상기 전자 장치는 상기 통신 인터페이스, 상기 입출력 인터페이스, GPS 모듈 및/또는 센서 모듈(예: 센서 모듈)을 이용하여, 상기 전자 장치가 위치한 공간의 환경 정보(예: 바람, 온도, 습 도 등)를 획득할 수 있다. 203 동작에서 상기 전자 장치는 상기 전자 장치가 위치한 공간과 관련된 정보를 전송할 수 있다. 일 실시 예에 따르면, 상기 전자 장치는 상기 통신 인터페이스를 이용하여, 상기 전자 장치가 위치한 공간과 관련된 정보를 서버로 전송할 수 있다. 205 동작에서 서버(서버의 제어부)는 상기 전자 장치가 위치한 공간과 관련된 정보에 포함된 하 나 이상의 이미지 데이터에서 3차원 가상 공간 생성 시의 하나 이상의 왜곡 발생 요소를 검출할 수 있다. 일 실시 예에 따르면 서버는 통신 모듈(예: 통신 모듈)을 이용하여 상기 전자 장치가 위치한 공 간과 관련된 정보를 수신할 수 있다. 일 실시 예에 따르면, 상기 전자 장치가 위치한 공간과 관련된 정보는 복수의 이미지 데이터를 포함할 수 있으며, 상기 서버는 상기 복수의 이미지 데이터의 비교를 통해 상기 복수의 이미지 데이터 각각에 왜곡 발 생 요소(빛 반사, 낮은 선명도, 외곽선 왜곡 등)가 포함되어 있는지를 확인할 수 있다. 예를 들어, 상기 복수의 이미지 데이터 간의 서로 대응되는 영역(또는 오브젝트)의 색상(음영) 등의 비교를 통해, 상이한 정도가 지정된 임계 값 이상일 경우, 상기 복수의 이미지 데이터 중 하나 이상의 이미지 데이터의 상기 영역에 왜곡 발생 요소 가 포함된 것으로 확인할 수 있다. 다른 실시 예에 따르면, 상기 서버는 상기 하나 이상의 이미지 데이터에 기 설정된 하나 이상의 왜곡 발생 요소와 대응되는 왜곡 발생 요소가 포함되어 있는지를 확인하여 하나 이상의 왜곡 발생 요소를 검출할 수 있다. 예를 들어, 상기 기설정된 하나 이상의 왜곡 발생 요소는 조명, 그림자, 빛 반사 및/또는 행인 등을 포함할 수 있다. 예를 들어, 상기 서버는, 상기 인공 지능 및/또는 머신 러닝 기반 학습 및/또는 이미지 비교 기술을 이용한 오브젝트 인식에 기초하여, 상기 하나 이상의 이미지 데이터에 기 설정된 하나 이상의 왜곡 발생 요소와 대응되는 왜곡 발생 요소가 포함되어 있는지를 확인하여 하나 이상의 왜곡 발생 요소를 검출할 수 있다. 207 동작에서 서버는 상기 하나 이상의 이미지 데이터에서 상기 검출된 하나 이상의 왜곡 발생 요소를 제 거할 수 있다.일 실시 예에 따르면, 상기 서버는 상기 하나 이상의 이미지 데이터 각각에서, 상기 하나 이상의 이미지 데이터 각각에서 검출한 하나 이상의 왜곡 발생 요소를 제거(삭제, 보정)할 수 있다. 209 동작에서 서버는 상기 하나 이상의 왜곡 발생 요소가 제거된 상기 하나 이상의 이미지 데이터를 이용 하여, 3차원 가상 공간에 대응하는 3차원 이미지 데이터를 생성할 수 있다. 일 실시 예에 따르면, 서버는 포토그래메트리(photogrammetry) 방식 등 다양한 3차원 이미지 데이터의 모 델링 방식 중 하나를 이용하여 상기 3차원 이미지 데이터를 생성할 수 있다. 일 실시 예에 따르면, 상기 서버는 상기 생성될 3차원 이미지 데이터에 대응하는 3차원 가상 공간 상에 발 생되는(발생될) 왜곡을 최소화하기 위해, 다음의 동작들을 수행할 수 있다. 일 실시 예에 따르면, 상기 서버는 상기 하나 이상의 이미지 데이터 각각에서, 지정된 하나 이상의 오브젝 트 카테고리 각각에 대응하는 오브젝트를 세그먼테이션할 수 있다. 예를 들어, 상기 지정된 하나 이상의 오브젝 트는, 자연물, 정형 사물(정형 오브젝트), 및/또는 비정형 사물(비정형 오브젝트)을 포함할 수 있다. 예를 들어, 상기 오브젝트의 세그먼테이션 시, 상기 오브젝트의 이미지 데이터에서의 위치 정보가 상기 오브젝트에 대응되도록 저장될 수 있다. 일 실시 예에 따르면, 상기 서버는 지정된 하나 이상의 오브젝트 카테고리 각각마다 포함된 세그먼테이션 된 오브젝트에 대한 왜곡 여부를 확인하여, 왜곡 확인 시, 왜곡 보정을 수행할 수 있다. 상기 왜곡 보정은, 상 기 통신 모듈을 통해 수신된, 상기 전자 장치의 입출력 인터페이스를 통해 획득된 사용자 제공 데 이터 및/또는 인공 지능 및/또는 머신 러닝 기반 학습을 이용하여, 수행될 수 있다. 예를 들어, 상기 사용자 제 공 데이터 및/또는 상기 인공 지능 및/또는 머신 러닝 기반 학습을 이용하여 왜곡이 확인된 오브젝트가, 특정 오브젝트, 예를 들어, 책상으로 확인된 경우, 대응되는 왜곡 되지 않은 오브젝트로 상기 왜곡이 확인된 오브젝 트를 대체하거나, 이미지 처리 기술을 이용하여 왜곡된 부분의 왜곡을 보정할 수 있다. 일 실시 예에 따르면, 상기 서버는 상기 세그먼테이션된 오브젝트를 제외한 3차원 이미지 데이터를 생성하 고, 이후 상기 세그먼테이션되어 왜곡 보정된 오브젝트를, 저장된 위치 정보를 이용하여, 상기 상기 세그먼테이 션된 오브젝트를 제외하여 생성한 3차원 이미지 데이터에 정합하여, 최종 왜곡 보정된 3차원 이미지 데이터를 생성할 수 있다. 상술한 도 2의 실시 예에 추가로, 상기 서버는, 사용자에게 4D 의 경험을 제공할 수 있도록, 상기 211 동 작에 따라 상기 보정한 3차원 이미지 데이터에, 상기 전자 장치가 위치하는 공간의 환경 정보 및/또는 컨텍 스트 정보를 대응시켜 저장할 수 있다. 이에 따라, 상기 서버는, 상기 보정한 3차원 이미지 데이터의 제공 시, 상기 환경 정보 및/또는 컨텍스트 정보에 대응되는 물리적 환경 제어에 대한 정보를 제공할 수 있다. 상술한 실시 예에서 추가로, 상기 서버는 보정된 3차원 이미지 데이터가 날짜, 시간, 및/또는 위치 정보를 기초로 분류되어 관리되도록 메모리(예: 메모리)에 저장할 수 있다. 도 3a 내지 도 3c는 본 발명의 일 실시 예에 따른 전자 장치(예: 전자 장치)에 의해, 상기 전자 장치가 위 치한 공간과 관련된 정보를 획득하는 동작을 설명하기 위한 도면이다. 일 실시 예에 따르면, 상기 전자 장치의 메모리(예: 메모리)에는 상기 전자 장치가 위치한 공간과 관련된 정보를 획득할 수 있는 어플리케이션(application)(가이드 어플리케이션이라고도 함)이 저장될 수 있다. 상기 전자 장치는, 상기 전자 장치의 입출력 인터페이스(예: 입출력 인터페이스)를 이용한, 상기 어플리케 이션을 실행시키기 위한 사용자 입력의 수신 시, 도 3a와 같이 표시부 상에 아이디 및 패스워드 입력 항목 을 포함하는 로그인 화면을 디스플레이할 수 있다. 상기 전자 장치는, 상기 입출력 인터페이스를 이용한 아이디 및 패스워드의 입력에 기초하여 로그인 동작을 수 행할 수 있으며, 상기 로그인 동작의 수행에 따른 로그인 성공 시, 도 3a의 (b)와 같은 실행 대기 화면을 디스 플레이할 수 있다. 도 3a의 (b)를 참조하면, 상기 실행 대기 화면은, Start Scanning 버튼, How to Shoot 버튼, 및/또는 View My Project 버튼을 포함할 수 있다. 상기 Start Scanning 버튼은, 이미지 촬영을 시작하도록 하는 버튼일 수 있다. 예를 들어, 상기 Start Scanning 버튼에 대한 사용자 입력 시, 상기 전자 장치는, 이미지 촬영(이미지 획득)을 할 수 있는 화면을 제공할 수 있다. 상기 How to Shoot 버튼은, 이미지 촬영 가이드를 제공하기 위한 버튼일 수 있다. 예를 들어, 상기 How to Shoot 버튼에 대한 사용자 입력 시, 상기 전자 장치는, 에니메이션 형태 등으로 상기 어플리케이션을 이용 한 이미지 촬영 가이드를 제공할 수 있다. 상기 View My Project 버튼은, 상기 어플리케이션을 이용하여 획득한 복수의 이미지 데이터를 기초로 생성 된 3차원 이미지 데이터에 대응하는 3차원 공간(3차원 가상 공간이라고도 함)을 제공하기 위한 버튼일 수 있다. 예를 들어, 상기 How to Shoot 버튼에 대한 사용자 입력 시, 상기 전자 장치는, 상기 복수의 이미지 데이 터를 기초로 3차원 이미지 데이터를 생성할 수 있으며, 상기 3차원 이미지 데이터를 상기 표시부에 디스플레이 하여 사용자에게 상기 3차원 이미지 데이터에 대응하는 3차원 공간을 제공할 수 있다. 다른 예로, 상기 How to Shoot 버튼에 대한 사용자 입력 시, 상기 전자 장치는, 서버(예: 서버)로부터 수신되어 저장된 3차원 이미지 데이터를 상기 표시부에 디스플레이하여 사용자에게 상기 3차원 이미지 데이터에 대응하는 3차원 공간을 제공할 수도 있다. 일 실시 예에 따르면, 상기 Start Scanning 버튼에 대한 사용자 입력의 수신 시, 상기 전자 장치는, 도 3b 의 (a)와 같은 이미지 촬영 대기 화면을 디스플레이할 수 있다. 예를 들어, 상기 Start Scanning 버튼에 대한 사용자 입력 시, 상기 전자 장치는, 상기 전자 장치의 카메라 모듈(예: 카메라 모듈)을 활성화시킬 수 있으며, 상기 활성화된 카메라 모듈을 통해 획득되는 이미지 데이터에 대응하는 이미지를 상기 표시부에 디스플 레이할 수 있다. 상기 도 3b의 (a)의 이미지 촬영 대기 화면은 SCAN 버튼을 포함할 수 있다. 일 실시 예에 따르면, 상기 SCAN 버튼에 대한 사용자 입력의 수신 시, 상기 전자 장치가 상기 카메라 모듈 을 이용하여 지정된 조건에 따라 이미지 촬영 즉, 상기 카메라 모듈을 통해 획득되는 이미지 데이터의 저장을 할 수 있다. 예를 들어, 상기 전자 장치는, 특징점 추출 기술 등을 이용하여, 상기 획득된 이미지 데이터에 대 응하는 이미지에 하나 이상의 주요 부분(특정 코너, 음영 변화가 심한 영역 등)이 포함되어 있는지를 자동으로 확인(검출, 추출)할 수 있으며, 상기 하나 이상의 주요 부분을 포함하는 이미지의 이미지 데이터를 자동으로 저 장할 수 있다. 일 실시 예에 따르면, 상기 SCAN 버튼에 대한 사용자 입력의 수신 이후, 상기 전자 장치는, 상기 표시부 상에 디스플레이된 상기 카메라 모듈을 통해 획득된 이미지 데이터의 이미지에서, 이전에 촬영된 공간에 대응하 는 영역의 경우 음영을 제공하여 이전에 촬영된 공간임을 사용자에게 알려줄 수 있다. 일 실시 예에 따르면, 상기 SCAN 버튼에 대한 사용자 입력의 수신 이후, 상기 전자 장치는, 상기 전자 장 치가 추가로 더 촬영해야 할 공간에 대한 가이드를 제공할 수 있다. 예를 들어, 상기 전자 장치가 위치한 공간 에서, 3차원 가상 공간을 생성하기 위해 필요한, 촬영하지 않은 공간을 추가 촬영하도록 상기 표시부 상에 시각 적인 가이드를 제공할 수 있다. 일 실시 예에 따르면, 상기 SCAN 버튼에 대한 사용자 입력의 수신 이후, 도 3b의 (b)를 참조하면, 상기 전 자 장치는, 실시간으로 상기 전자 장치의 촬영 현황에 대응하는 정보를 사용자에게 제공할 수 있다. 예를 들어, 상기 전자 장치는, 도 3b의 (b)와 같이, 상기 표시부 상에 Number of images saved(저장된 이미지 개수)(현재까 지 획득한 이미지 데이터의 개수라고도 함), Size of the image currently stored(현재 저장된 이미지 사이 즈)(획득하는 이미지 데이터의 사이즈라고도 함), Amount of images stored(저장된 이미지 용량)(션재까지 획득 한 이미지 데이터의 전체 용량이라고도 함)에 대응하는 정보를 디스플레이할 수 있다. 도 3b의 (b)를 참조하면, 상기 표시부 상에 디스플레이된 화면에는 STOP 메뉴가 표시될 수 있다. 일 실시 예에 따르면, 상기 STOP 메뉴에 대한 사용자 입력의 수신 시, 상기 전자 장치는, 상기 이미지 촬 영을 종료할 수 있다. 예를 들어, 상기 STOP 메뉴에 대한 사용자 입력의 수신 시, 상기 전자 장치는, 상기 카메라 모듈이 이미지 데이터는 획득하되, 상기 획득한 이미지 데이터를 저장하지 않도록 할 수 있다. 다른 예 로, 상기 STOP 메뉴에 대한 사용자 입력의 수신 시, 상기 전자 장치는, 상기 카메라 모듈의 활성화 상태를 비활성화 상태로 변경시킬 수 있다. 일 실시 예에 따르면, 상기 STOP 메뉴에 대한 사용자 입력의 수신 시, 상기 전자 장치는, 상기 촬영된, 즉, 획득되어 저장된 하나 이상의 이미지 데이터를 서버(예: 서버)로 전송할 수 있다. 일 실시 예에 따르면, 상기 STOP 메뉴에 대한 사용자 입력의 수신 시, 도 3c의 (a)와 같은 팝업 창을 포함 하는 화면을 제공하여, 사용자에게 촬영 종료 및 촬영된 이미지 데이터들을 전송할 것인지를 재확인하도록 할 수 있다. 예를 들어, 상기 팝업 창에는 도 3c의 (a)와 같은 문구가 포함될 수 있다. 일 실시 예에 따르면, 상기 도 3c의 (a)와 같은 화면에 제공된 팝업 창의 메뉴들(Continue 및 Exit) 중 Exit에 대한 사용자 입력의 수신 시, 상기 전자 장치는 상기 촬영 종료 및 촬영된 이미지 데이터를 상기 서버로 전송하는 동작을 수행할 수 있다. 예를 들어, 상기 Exit에 대한 사용자 입력의 수신 시, 상기 촬영된 이미지 데이터의 전송에 따라, 상기 전자 장치는 도 3c의 (b)와 같은, 이미지 전송에 대한 정보(전송률 정보)를 나타내는 팝업 창을 포함하는 화면을 제공할 수 있다. 예를 들어, 상기 Continue에 대한 사용자 입력의 수신 시, 상기 전자 장 치는 상기 촬영을 위한 동작을 계속 수행할 수 있다. 도 4는 본 발명의 일 실시 예에 따른 3차원 가상 공간을 구축하기 위한 동작의 흐름도이다. 401 동작에서 서버(예: 서버 또는 서버의 제어부)는, 전자 장치(예: 전자 장치)로부터 하나 이상의 이미지 데이터를 수신할 수 있다. 403 동작에서 상기 서버는, 상기 하나 이상의 이미지 데이터 각각에서 기 설정된 카테고리들에 대응하는 오브젝 트들을 세그먼테이션(분류라고도 함)할 수 있다. 일 실시 예에 따르면, 상기 서버는, 상기 하나 이상의 이미지 데이터 중 적어도 하나에 기 설정된 적어도 하나 의 왜곡 발생 요소가 포함되어 있는지를 확인할 수 있다. 상기 서버는, 상기 하나 이상의 이미지 데이터 중 적 어도 하나에 상기 기 설정된 적어도 하나의 왜곡 발생 요소가 포함되어 있으면, 상기 왜곡 발생 요소를 제거할 수 있다. 예를 들어, 상기 서버는, 상기 왜곡 발생 요소가 제거된 상기 하나 이상의 이미지 데이터 각각에서 기 설정된 카테고리들에 대응하는 오브젝트들을 세그먼테이션할 수 있다. 예를 들어, 상기 기 설정된 카테고리들을, 오브젝트 카테고리들이라고 할 수 있으며, 상기 오브젝트 카테고리들 은, 자연물(예: 나무, 풀 등), 정형 사물(정형 오브젝트)(예: 테이블, 냉장고 등), 및/또는 비정형 사물(비정형 오브젝트)(예: 의자, 선풍기 등)을 포함할 수 있다. 405 동작에서 상기 서버는, 상기 하나 이상의 이미지 데이터 각각에서 상기 세그먼테이션된 오브젝트들 및 상기 하나 이상의 이미지 데이터 각각에서 상기 세그먼테이션된 오브젝트들이 제외된 이미지 데이터에 기초하여 복수 의 3차원 이미지 데이터를 생성할 수 있다. 일 실시 예에 따르면, 상기 복수의 3차원 이미지 데이터는 3차원 오브젝트 이미지 데이터(또는 왜곡 보정된 3차 원 오브젝트 이미지 데이터) 및/또는 3차원 배경 이미지 데이터를 포함할 수 있다. 예를 들어, 상기 서버는, 상 기 세그먼테이션된 오브젝트들을 상기 기 설정된 카테고리들 각각에 대응되도록 분류하고, 상기 기 설정된 카테 고리들 각각에 대응되도록 분류된 오브젝트들에 기초하여 상기 기 설정된 카테고리들 각각에 대응하는 3차원 오 브젝트 이미지 데이터를 생성할 수 있다. 상기 서버는, 상기 하나 이상의 이미지 데이터 각각에서 상기 세그먼 테이션된 오브젝트들이 제외된 이미지 데이터에 기초하여 3차원 배경 이미지 데이터를 생성할 수 있다. 예를 들어, 상기 서버는, 상기 전자 장치로부터 수신된 사용자 제공 데이터, 상기 서버의 인공 기능 기반 학습 모델 및 상기 서버의 머신 러닝 기반 학습 모델 중 적어도 하나에 기초하여, 상기 분류된 오브젝트들 중 적어도 하나의 오브젝트의 왜곡 여부를 확인할 수 있다. 상기 서버는, 상기 적어도 하나의 오브젝트의 왜곡 확인 시, 상기 적어도 하나의 오브젝트를 대응되는 오브젝트로 변경하거나 이미지 처리 기술을 이용하여 상기 적어도 하 나의 오브젝트를 이미지 보정하여, 상기 왜곡을 보정(또는 최소화 또는 제거)할 수 있다. 407 동작에서 상기 서버는, 상기 생성된 복수의 3차원 이미지 데이터를 정합하여 상기 3차원 가상 공간에 대응 하는 3차원 이미지 데이터를 생성할 수 있다. 상술한 도 4의 실시 예에 추가로, 상기 서버는, 상기 하나 이상의 이미지 데이터와 대응되는 3차원 이미지 데이 터가 저장되어 있는지를 확인할 수 있다. 상기 서버는, 상기 하나 이상의 이미지 데이터와 대응되는 상기 3차원 이미지 데이터가 저장되어 있으면, 상기 하나 이상의 이미지 데이터에 대응하는 시간 정보, 해상도, 선명도, 및 왜곡 발생 요소 중 적어도 하나에 기초하여, 상기 저장된 3차원 이미지 데이터의 업데이트 필요 여부를 확인할 수 있다. 상기 403 동작인 상기 세그먼테이션하는 동작은, 상기 저장된 3차원 이미지 데이터의 업데이트가 필요 한 것으로 확인된 것에 기초하여 수행될 수 있다. 또한, 상술한 도 4의 실시 예에 추가로, 상기 서버는, 상기 하나 이상의 이미지 데이터에 대응하는, 상기 전자 장치가 위치하는 공간의 환경 정보 및 컨텍스트 정보 중 적어도 하나의 정보를 수신할 수 있다. 상기 서버는, 상기 생성된 3차원 가상 공간에 대응하는 3차원 이미지 데이터와 대응되도록, 상기 수신된 적어도 하나의 정보 를 저장할 수 있다. 상술한 실시 예들에 따르면, 스마트폰 등의 전자 장치(예: 전자 장치)를 가진 사용자라면 누구나 실시간으 로 현실 공간의 정보를 취득하여 고품질의 3차원 가상 공간을 구축할 수 있다. 예를 들어, 상기 전자 장치상에, 상기 전자 장치가 위치한 공간과 관련된 정보를 획득할 수 있는 어플리케이션(application)(가이드 어플리케이션이라고도 함)을 설치할 경우, 상기 전자 장치는, 상기 어플리케이션의 실행에 따라, 현재의 전자 장치가 위치 한 공간과 관련된 정보를 자동으로 획득할 수 있다. 상기 전자 장치는 상기 획득된 상기 전자 장치가 위치한 공 간과 관련된 정보를 서버(예: 서버)로 전송할 수 있다. 상기 서버는 상기 정보를 수신하여 3차원 가상 공 간의 구축 예를 들어 3차원 이미지 데이터를 생성하여, 상기 전자 장치로 상기 생성된 3차원 이미지 데이터를 전송할 수 있다. 상기 전자 장치는 상기 생성된 3차원 이미지 데이터를 수신하여, 상기 3차원 이미지 데이터를 기반으로 증강 현실, 가상 현실, 및/또는 혼합 현실 서비스를 제공할 수 있다. 예를 들어, 상기 전자 장치는 3 차원 이미지 데이터의 시각적인 정보뿐 아니라, 상기 전자 장치가 위치하는 현실 공간의 환경 정보도 같이 서비 스로 제공하여, 몰입감을 극대화시킬 수 있다. 본 문서의 다양한 실시예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media)(예: 메모리, 메모리(내장 메모리 또는 외장 메모리))에 저장된 명령어를 포함하는 소프트웨어 (예: 프로그램)로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실시예들에 따른 전자 장치(예: 제1 전자 장치, 제2 전자 장치)를 포 함할 수 있다. 상기 명령이 제어부(예: 제어부, 제어부)(또는 프로세서)에 의해 실행될 경우, 제어부 가 직접, 또는 상기 제어부의 제어하에 다른 구성요소들을 이용하여 상기 명령에 해당하는 기능을 수행할 수 있 다. 명령은 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장매체가 신호(signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임 시적으로 저장됨을 구분하지 않는다. 일시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 이상의 설명은 본 발명의 기술사상을 예시적으로 설명한 것에 불과한 것으로서, 본 발명이 속하는 기술 분야에 서 통상의 지식을 가진 자라면 본 발명의 본질적인 특성에서 벗어나지 않는 범위 내에서 여러 가지 치환, 변형 및 변경 등이 가능함을 쉽게 알 수 있을 것이다. 즉, 본 발명에 개시된 실시 예들은 본 발명의 기술 사상을 한 정하기 위한 것이 아니라 설명하기 위한 것으로서, 이러한 실시 예에 의하여 본 발명의 기술 사상의 범위가 한 정되는 것은 아니다. 따라서, 본 발명의 보호 범위는 후술되는 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모 든 기술사상은 본 발명의 권리범위에 포함되는 것으로 해석되어야 할 것이다. 산업상 이용가능성 본 발명의 실시 예는, 증강 현실(AR; augmented reality), 가상 현실(VR; virtual reality), 및/또는 혼합 현 실(MR; mixed reality) 기술에 이용될 수 있다."}
{"patent_id": "10-2020-0011399", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1a는 본 발명의 일 실시 예에 따른 3차원 가상 공간 구축을 위한 시스템의 블록도이다. 도 1b는 본 발명의 일 실시 예에 따른 서버의 제어부의 블록도이다. 도 1c는 본 발명의 일 실시 예에 따른 서버의 제어부의 블록도이다. 도 2는 본 발명의 일 실시 예에 따른 3차원 가상 공간 구축 동작의 흐름도이다. 도 3a 내지 도 3c는 본 발명의 일 실시 예에 따른 전자 장치에 의해, 상기 전자 장치가 위치한 공간과 관련된 정보를 획득하는 동작을 설명하기 위한 도면이다. 도 4는 본 발명의 일 실시 예에 따른 3차원 가상 공간을 구축하기 위한 동작의 흐름도이다."}
