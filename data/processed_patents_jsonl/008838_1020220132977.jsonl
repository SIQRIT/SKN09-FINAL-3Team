{"patent_id": "10-2022-0132977", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0053190", "출원번호": "10-2022-0132977", "발명의 명칭": "표정 분석을 통한 감정 변화 검출 장치 및 방법", "출원인": "한국전자통신연구원", "발명자": "한병옥"}}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로그램이 기록된 메모리; 및프로그램을 실행하는 프로세서를 포함하며,프로그램은, 적어도 하나의 사람이 포함된 동영상을 획득하는 카메라 영상 획득부;.동영상으로부터 사용자의 얼굴 영상을 추출하고, 추출된 얼굴 영상을 전처리하는 전처리부; 사용자의 얼굴 영상에서 표정 벡터를 추출 및 누적 저장하는 표정 분석부; 및누적 저장된 표정 벡터 값들을 기반으로 추출된 감정 신호를 분석하여 감정이 급변하는 시간적 지점을 검출하는감정 변화 분석부를 포함하는, 표정 분석을 통한 감정 변화 검출 장치."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 전처리부는, 동영상으로부터 적어도 하나의 얼굴 영역을 추론하는 얼굴 검출부; 추론된 적어도 하나의 얼굴 영역에 해당하는 사람의 신원 분석을 통해 사용자의 얼굴을 검색하는 신원 인식부; 검색된 사용자의 얼굴 영상에서 랜드마크 위치 정보 및 자세 정보를 추출하는 얼굴 자세 추출부; 및추출된 랜드마크 위치 정보 및 자세 정보를 기반으로 얼굴 영상을 정규화하는 얼굴 정규화부를 포함하는, 표정분석을 통한 감정 변화 검출 장치."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서, 메모리는, 신원 인식 및 검증을 위해 미리 학습된 딥러닝 네트워크를 더 저장하고, 신원 인식부는,미리 획득된 사용자의 얼굴 영상을 입력으로 딥러닝 네트워크가 출력한 제1 특징 벡터를 미리 저장하고, 영상에 포함된 적어도 하나의 얼굴 영역에 해당하는 영상 각각을 입력을 딥러닝 네트워크가 출력한 적어도 하나의 제2 특징 벡터를 추출하여, 제1 특징 벡터 및 적어도 하나의 제2 특징 벡터 각각의 유사도를 점수화된 신뢰도가 소정 임계치 이상일 경우, 해당 얼굴 영역의 얼굴 영상을 사용자로 판단하는, 표정 분석을 통한 감정 변화검출 장치."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2 항에 있어서, 얼굴 정규화부는, 추출된 랜드마크의 위치를 기반으로 얼굴 영상을 변형하거나, 추출된 자세 정보를 기반으로 분석 대상이 되는얼굴 영상을 필터링하는, 표정 분석을 통한 감정 변화 검출 장치."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서, 표정 분석부는,정규화된 얼굴 영상에서 감정 카테고리 모델의 7차원 신뢰도 벡터를 추출하는 표정 분류 추론부; 및표정 차원 모델의 정서가(valence), 각성(arousal)의 차원 회귀 결과 값을 2차원 추가하는 표정 차원 추론부를공개특허 10-2024-0053190-3-포함하는, 표정 분석을 통한 감정 변화 검출 장치."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서, 감정 변화 검출부는, 얼굴 영상으로부터 추출된 다차원 표정 벡터의 누적 값으로부터 감정 신호를 추출하는 신호 추출부;추출된 감정 신호에서 노이즈 제거하는 신호 정제부; 및노이즈 제거된 감정 신호를 기반으로 감정 신호의 변화 속도 및 크기가 소정 임계치 이상인 시간적 지점을 검출하는 변화 지점 검출부를 포함하는, 표정 분석을 통한 감정 변화 검출 장치."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서, 감정 변화 분석부는, 동영상이 오프라인으로 입력될 경우, 동영상이 종료되면 동영상 전체의 감정 신호를 일정 길이의 부신호로 분할하고, 연속된 부신호들 간의 차이를 기반으로 변화 지점으로 검출하는, 표정 분석을 통한 감정 변화 검출 장치."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6 항에 있어서, 감정 변화 분석부는, 동영상이 온라인으로 실시간 입력될 경우, 사용자가 지정한 신호 길이의 동영상의 감정 신호를 미분하여 그래디언트값을 산출하고, 과거 신호의 그래디언트값과 현재 그래디언트값을 비교하여 변화 지점인지 여부를결정하는, 표정 분석을 통한 감정 변화 검출 장치."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "적어도 하나의 사람이 포함된 동영상을 획득하는 단계;동영상으로부터 사용자의 얼굴 영상을 추출하고, 추출된 얼굴 영상을 전처리하는 단계; 사용자의 얼굴 영상에서 표정 벡터를 추출 및 누적 저장하는 단계; 및누적 저장된 표정 벡터 값들을 기반으로 추출된 감정 신호를 분석하여 감정이 급변하는 시간적 지점을 검출하는단계를 포함하는, 표정 분석을 통한 감정 변화 검출 방법."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9 항에 있어서, 전처리하는 단계는, 동영상으로부터 적어도 하나의 얼굴 영역을 추론하는 단계; 추론된 적어도 하나의 얼굴 영역에 해당하는 사람의 신원 분석을 통해 사용자의 얼굴을 검색하는 단계; 검색된 사용자의 얼굴 영상에서 랜드마크 위치 정보 및 자세 정보를 추출하는 단계; 및추출된 랜드마크 위치 정보 및 자세 정보를 기반으로 얼굴 영상을 정규화하는 단계를 포함하는, 표정 분석을 통한 감정 변화 검출 방법."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10 항에 있어서, 신원 분석을 통해 사용자의 얼굴을 검색하는 단계는,미리 획득된 사용자의 얼굴 영상을 입력으로 딥러닝 네트워크가 출력한 제1 특징 벡터를 미리 저장하고, 영상에 포함된 적어도 하나의 얼굴 영역에 해당하는 영상 각각을 입력을 딥러닝 네트워크가 출력한 적어도 하나의 제2 특징 벡터를 추출하여, 제1 특징 벡터 및 적어도 하나의 제2 특징 벡터 각각의 유사도를 점수화된 신뢰도가 소정 임계치 이상일 경우, 해당 얼굴 영역의 얼굴 영상을 사용자로 판단하되, 딥러닝 네트워크는, 신원 인식 및 검증을 위해 미리 학습되는, 표정 분석을 통한 감정 변화 검출 방법. 공개특허 10-2024-0053190-4-청구항 12 제10 항에 있어서, 얼굴 영상을 정규화하는 단계는, 추출된 랜드마크의 위치를 기반으로 얼굴 영상을 변형하거나, 추출된 자세 정보를 기반으로 분석 대상이 되는얼굴 영상을 필터링하는, 표정 분석을 통한 감정 변화 검출 방법."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9 항에 있어서, 표정 벡터를 추출 및 누적 저장하는 단계는, 정규화된 얼굴 영상에서 감정 카테고리 모델의 7차원 신뢰도 벡터를 추출하는 단계; 및표정 차원 모델의 정서가(valence), 각성(arousal)의 차원 회귀 결과 값을 2차원 추가하는 단계를 포함하는, 표정 분석을 통한 감정 변화 검출 방법."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제9 항에 있어서, 감정이 급변하는 시간적 지점을 검출하는 단계는, 얼굴 영상으로부터 추출된 다차원 표정 벡터의 누적 값으로부터 감정 신호를 추출하는 단계;추출된 감정 신호에서 노이즈 제거하는 단계; 및노이즈 제거된 감정 신호를 기반으로 감정 신호의 변화 속도 및 크기가 소정 임계치 이상인 시간적 지점을 검출하는 단계를 포함하는, 표정 분석을 통한 감정 변화 검출 방법."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14 항에 있어서, 감정이 급변하는 시간적 지점을 검출하는 단계는, 동영상이 오프라인으로 입력될 경우, 동영상이 종료되면 동영상 전체의 감정 신호를 일정 길이의 부신호로 분할하고, 연속된 부신호들 간의 차이를 기반으로 변화 지점으로 검출하는, 표정 분석을 통한 감정 변화 검출 방법."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14 항에 있어서, 감정이 급변하는 시간적 지점을 검출하는 단계는, 동영상이 온라인으로 실시간 입력될 경우, 사용자가 지정한 신호 길이의 동영상의 감정 신호를 미분하여 그래디언트값을 산출하고, 과거 신호의 그래디언트값과 현재 그래디언트값을 비교하여 변화 지점인지 여부를결정하는, 표정 분석을 통한 감정 변화 검출 방법."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "적어도 하나의 사람이 포함된 동영상을 획득하는 단계;동영상으로부터 신원이 확인된 사용자의 얼굴 영상을 추출하는 단계; 사용자의 얼굴 영상에서 다차원 표정 벡터를 추출 및 누적 저장하는 단계;다차원 표정 벡터의 누적 값으로부터 감정 신호를 추출하는 단계; 및추출된 감정 신호를 기반으로 감정 신호의 변화 속도 및 크기가 소정 임계치 이상인 시간적 지점을 검출하는 단계를 포함하는, 표정 분석을 통한 감정 변화 검출 방법."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17 항에 있어서, 검색된 사용자의 얼굴 영상에서 랜드마크 위치 정보 및 자세 정보를 추출하는 단계; 및추출된 랜드마크 위치 정보 및 자세 정보를 기반으로 얼굴 영상을 정규화하는 단계를 더 포함하는, 표정 분석을통한 감정 변화 검출 방법. 공개특허 10-2024-0053190-5-청구항 19 제17 항에 있어서, 표정 벡터를 추출 및 누적 저장하는 단계는, 정규화된 얼굴 영상에서 감정 카테고리 모델의 7차원 신뢰도 벡터를 추출하는 단계; 및표정 차원 모델의 정서가(valence), 각성(arousal)의 차원 회귀 결과 값을 2차원 추가하는 단계를 포함하는, 표정 분석을 통한 감정 변화 검출 방법."}
{"patent_id": "10-2022-0132977", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제17 항에 있어서, 감정이 급변하는 시간적 지점을 검출하는 단계는, 동영상이 오프라인으로 입력될 경우, 동영상이 종료되면 동영상 전체의 감정 신호를 일정 길이의 부신호로 분할하고, 연속된 부신호들 간의 차이를 기반으로 변화 지점으로 검출하고, 동영상이 온라인으로 실시간 입력될 경우, 사용자가 지정한 신호 길이의 동영상의 감정 신호를 미분하여 그래디언트값을 산출하고, 과거 신호의 그래디언트값과 현재 그래디언트값을 비교하여 변화 지점인지 여부를결정하는, 표정 분석을 통한 감정 변화 검출 방법."}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "표정 분석을 통한 감정 변화 검출 장치 및 방법이 개시된다. 본 발명의 실시예에 따른 표정 분석을 통한 감정 변 화 검출 장치는, 적어도 하나의 프로그램이 기록된 메모리 및 프로그램을 실행하는 프로세서를 포함하며, 프로그 램은, 적어도 하나의 사람이 포함된 동영상을 획득하는 카메라 영상 획득부, 동영상으로부터 사용자의 얼굴 영상 을 추출하고, 추출된 얼굴 영상을 전처리하는 전처리부, 사용자의 얼굴 영상에서 표정 벡터를 추출 및 누적 저장 하는 표정 분석부 및 누적 저장된 표정 벡터 값들을 기반으로 추출된 감정 신호를 분석하여 감정이 급변하는 시 간적 지점을 검출하는 감정 변화 분석부를 포함할 수 있다."}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "기재된 실시예는 지능형 로봇, 자율 주행 자동차 및 자폐 스펙트럼 장애 검사 공간 등에 장착되어 있는 RGB 카 메라로부터 사용자의 얼굴을 포함하고 있는 동영상으로부터 사용자의 표정을 분석하는 기술이다."}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래의 인공 지능 기반 표정 분석 기술은 사용자의 표정을 통한 감정 상태 인식에 초점을 맞추어 개발되어 왔다. 즉, 사용자의 얼굴 RGB 영상을 입력으로 받아서 얼굴 영역을 찾아내고, 얼굴 영역의 표정을 분석하여 표 정이 표현하고 있는 감정 상태를 인식해서 사람의 감정을 이해하는 시스템을 주로 개발해왔다. 구체적으로, 인 공지능 기술이 RGB 영상 내의 얼굴 영역을 검출하고 해당 얼굴이 나타내고 있는 표정을 분류(classification) 또는 구분하여 행복, 슬픔, 역겨움, 분노, 놀람 및 두려움 등 6가지 감정 상태를 인식하는 기술이 대표적이다. 하지만, 표정 분석을 통해 적절히 사람의 감정을 이해하려면 감정 상태를 알아내는 것뿐만 아니라, 다음의 세 가지 이유로 감정 변화의 지점을 검출하는 것 또한 중요하다. 첫 번째로, 감정 변화의 지점은 사람의 감정을 이해하는 데 중요한 맥락(context) 또는 배경(background) 상황 을 이해하는 데 중요한 단초가 될 수 있다. 예를 들면, 입원해 있는 환자가 지속적인 슬픈 감정을 경험하고 있 을 때, 예상치 못한 건강에 대한 나쁜 소식을 듣게 되는 경우, 기존의 감정 상태 인식 기술은 지속적인 슬픔으 로 대상의 감정을 인식하게 된다. 하지만, 감정 변화 검출 기술을 이용하면 감정 변화의 타이밍과 감정의 변화 를 알 수 있기 때문에, 맥락 또는 배경 정보의 변화로 인한 대상의 감정 변화를 적절히 이해할 수 있다. 두 번째로, 사회적 지능을 갖추고 있는 인간-로봇 상호작용 또는 인간-컴퓨터 상호작용 시스템을 위해, 감정 변 화의 지점은 맥락 또는 배경 정보를 포함한 사용자의 포괄적인 감정 상태를 이해하기 위한 인식 지점으로 사용 될 수 있다. 예를 들어, 상기 첫 번째 이유에서 언급된 감정 변화의 원인인 맥락 정보(예상치 못한 건강에 대한 나쁜 소식)를 인공 지능 기술을 활용하여 인식하는 지점 제공이 가능하다. 세 번째로, 감정 변화의 지점은 사회적 커뮤니케이션에서 중요한 역할을 하는 다른 사람의 표정에 대한 반응성 (responsiveness) 정보 제공이 가능하다. 다른 사람의 감정에 반응하는 능력은 사회적, 인지적 관점에서 복잡한 능력이다. 특히, 자폐 스펙트럼 장애로 알려진 사회적 상호작용이 부족한 사람의 경우, \"J. Deng et al., \"RetinaFace: Single-shot mult-level face localization in the wild,\" IEEE conference on Computer Vision and Pattern Recognition (CVPR), pp. 5203-5212, 2020\"에 개시된 내용과 같이 다른 사람의 감정에 즉각적으로 또는 자동적으로 반응하는 것이 어렵다고 알려져 있다. 전술한 이유로 인해 표정 분석을 통한 사람의 감정 상태 인식뿐만 아니라, 표정 분석을 통한 감정 변화의 지점"}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "을 검출하는 기술이 필요하다. 발명의 내용"}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "기재된 실시예는 RGB 카메라로부터 입력된 사용자의 얼굴 영상의 표정 분석을 통한 감정 변화의 시간적 지점을 검출하는 데 그 목적이 있다."}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "실시예에 따른 표정 분석을 통한 감정 변화 검출 장치는, 적어도 하나의 프로그램이 기록된 메모리 및 프로그 램을 실행하는 프로세서를 포함하며, 프로그램은, 적어도 하나의 사람이 포함된 동영상을 획득하는 카메라 영상 획득부, 동영상으로부터 사용자의 얼굴 영상을 추출하고, 추출된 얼굴 영상을 전처리하는 전처리부, 사용자의 얼굴 영상에서 표정 벡터를 추출 및 누적 저장하는 표정 분석부 및 누적 저장된 표정 벡터 값들을 기반으로 추 출된 감정 신호를 분석하여 감정이 급변하는 시간적 지점을 검출하는 감정 변화 분석부를 포함할 수 있다. 이때, 전처리부는, 동영상으로부터 적어도 하나의 얼굴 영역을 추론하는 얼굴 검출부, 추론된 적어도 하나의 얼 굴 영역에 해당하는 사람의 신원 분석을 통해 사용자의 얼굴을 검색하는 신원 인식부, 검색된 사용자의 얼굴 영 상에서 랜드마크 위치 정보 및 자세 정보를 추출하는 얼굴 자세 추출부 및 추출된 랜드마크 위치 정보 및 자세 정보를 기반으로 얼굴 영상을 정규화하는 얼굴 정규화부를 포함할 수 있다. 이때, 메모리는, 신원 인식 및 검증을 위해 미리 학습된 딥러닝 네트워크를 더 저장하고, 신원 인식부는, 미리 획득된 사용자의 얼굴 영상을 입력으로 딥러닝 네트워크가 출력한 제1 특징 벡터를 미리 저장하고, 영상에 포함 된 적어도 하나의 얼굴 영역에 해당하는 영상 각각을 입력을 딥러닝 네트워크가 출력한 적어도 하나의 제2 특징 벡터를 추출하여, 제1 특징 벡터 및 적어도 하나의 제2 특징 벡터 각각의 유사도를 점수화된 신뢰도가 소정 임 계치 이상일 경우, 해당 얼굴 영역의 얼굴 영상을 사용자로 판단할 수 있다. 이때, 얼굴 정규화부는, 추출된 랜드마크의 위치를 기반으로 얼굴 영상을 변형하거나, 추출된 자세 정보를 기반 으로 분석 대상이 되는 얼굴 영상을 필터링할 수 있다. 이때, 표정 분석부는, 정규화된 얼굴 영상에서 감정 카테고리 모델의 7차원 신뢰도 벡터를 추출하는 표정 분류 추론부 및 표정 차원 모델의 정서가(valence), 각성(arousal)의 차원 회귀 결과 값을 2차원 추가하는 표정 차원 추론부를 포함할 수 있다. 이때, 감정 변화 검출부는, 얼굴 영상으로부터 추출된 다차원 표정 벡터의 누적 값으로부터 감정 신호를 추출하 는 신호 추출부, 추출된 감정 신호에서 노이즈 제거하는 신호 정제부 및 노이즈 제거된 감정 신호를 기반으로 감정 신호의 변화 속도 및 크기가 소정 임계치 이상인 시간적 지점을 검출하는 변화 지점 검출부를 포함할 수 있다. 이때, 감정 변화 분석부는, 동영상이 오프라인으로 입력될 경우, 동영상이 종료되면 동영상 전체의 감정 신호를 일정 길이의 부신호로 분할하고, 연속된 부신호들 간의 차이를 기반으로 변화 지점으로 검출할 수 있다. 이때, 감정 변화 분석부는, 동영상이 온라인으로 실시간 입력될 경우, 사용자가 지정한 신호 길이의 동영상의 감정 신호를 미분하여 그래디언트값을 산출하고, 과거 신호의 그래디언트값과 현재 그래디언트값을 비교하여 변 화 지점인지 여부를 결정할 수 있다. 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법은, 적어도 하나의 사람이 포함된 동영상을 획득하는 단계, 동영상으로부터 사용자의 얼굴 영상을 추출하고, 추출된 얼굴 영상을 전처리하는 단계, 사용자의 얼굴 영 상에서 표정 벡터를 추출 및 누적 저장하는 단계 및 누적 저장된 표정 벡터 값들을 기반으로 추출된 감정 신호 를 분석하여 감정이 급변하는 시간적 지점을 검출하는 단계를 포함할 수 있다. 이때, 전처리하는 단계는, 동영상으로부터 적어도 하나의 얼굴 영역을 추론하는 단계, 추론된 적어도 하나의 얼 굴 영역에 해당하는 사람의 신원 분석을 통해 사용자의 얼굴을 검색하는 단계, 검색된 사용자의 얼굴 영상에서 랜드마크 위치 정보 및 자세 정보를 추출하는 단계 및 추출된 랜드마크 위치 정보 및 자세 정보를 기반으로 얼 굴 영상을 정규화하는 단계를 포함할 수 있다. 이때, 신원 분석을 통해 사용자의 얼굴을 검색하는 단계는, 미리 획득된 사용자의 얼굴 영상을 입력으로 딥러닝 네트워크가 출력한 제1 특징 벡터를 미리 저장하고, 영상에 포함된 적어도 하나의 얼굴 영역에 해당하는 영상 각각을 입력을 딥러닝 네트워크가 출력한 적어도 하나의 제2 특징 벡터를 추출하여, 제1 특징 벡터 및 적어도하나의 제2 특징 벡터 각각의 유사도를 점수화된 신뢰도가 소정 임계치 이상일 경우, 해당 얼굴 영역의 얼굴 영 상을 사용자로 판단하되, 딥러닝 네트워크는, 신원 인식 및 검증을 위해 미리 학습된 것일 수 있다. 이때, 얼굴 영상을 정규화하는 단계는, 추출된 랜드마크의 위치를 기반으로 얼굴 영상을 변형하거나, 추출된 자 세 정보를 기반으로 분석 대상이 되는 얼굴 영상을 필터링할 수 있다. 이때, 표정 벡터를 추출 및 누적 저장하는 단계는, 정규화된 얼굴 영상에서 감정 카테고리 모델의 7차원 신뢰도 벡터를 추출하는 단계 및 표정 차원 모델의 정서가(valence), 각성(arousal)의 차원 회귀 결과 값을 2차원 추가 하는 단계를 포함할 수 있다. 이때, 감정이 급변하는 시간적 지점을 검출하는 단계는, 얼굴 영상으로부터 추출된 다차원 표정 벡터의 누적 값 으로부터 감정 신호를 추출하는 단계, 추출된 감정 신호에서 노이즈 제거하는 단계 및 노이즈 제거된 감정 신호 를 기반으로 감정 신호의 변화 속도 및 크기가 소정 임계치 이상인 시간적 지점을 검출하는 단계를 포함할 수 있다. 이때, 감정이 급변하는 시간적 지점을 검출하는 단계는, 동영상이 오프라인으로 입력될 경우, 동영상이 종료되 면 동영상 전체의 감정 신호를 일정 길이의 부신호로 분할하고, 연속된 부신호들 간의 차이를 기반으로 변화 지 점으로 검출할 수 있다. 이때, 감정이 급변하는 시간적 지점을 검출하는 단계는, 동영상이 온라인으로 실시간 입력될 경우, 사용자가 지 정한 신호 길이의 동영상의 감정 신호를 미분하여 그래디언트값을 산출하고, 과거 신호의 그래디언트값과 현재 그래디언트값을 비교하여 변화 지점인지 여부를 결정할 수 있다. 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법은, 적어도 하나의 사람이 포함된 동영상을 획득하는 단계, 동영상으로부터 신원이 확인된 사용자의 얼굴 영상을 추출하는 단계, 사용자의 얼굴 영상에서 다차원 표 정 벡터를 추출 및 누적 저장하는 단계, 다차원 표정 벡터의 누적 값으로부터 감정 신호를 추출하는 단계 및 추 출된 감정 신호를 기반으로 감정 신호의 변화 속도 및 크기가 소정 임계치 이상인 시간적 지점을 검출하는 단계 를 포함할 수 있다. 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법은, 검색된 사용자의 얼굴 영상에서 랜드마크 위치 정보 및 자세 정보를 추출하는 단계 및 추출된 랜드마크 위치 정보 및 자세 정보를 기반으로 얼굴 영상을 정규화하는 단계를 더 포함할 수 있다. 이때, 표정 벡터를 추출 및 누적 저장하는 단계는, 정규화된 얼굴 영상에서 감정 카테고리 모델의 7차원 신뢰도 벡터를 추출하는 단계 및 표정 차원 모델의 정서가(valence), 각성(arousal)의 차원 회귀 결과 값을 2차원 추가 하는 단계를 포함할 수 있다. 이때, 감정이 급변하는 시간적 지점을 검출하는 단계는, 동영상이 오프라인으로 입력될 경우, 동영상이 종료되 면 동영상 전체의 감정 신호를 일정 길이의 부신호로 분할하고, 연속된 부신호들 간의 차이를 기반으로 변화 지 점으로 검출하고, 동영상이 온라인으로 실시간 입력될 경우, 사용자가 지정한 신호 길이의 동영상의 감정 신호 를 미분하여 그래디언트값을 산출하고, 과거 신호의 그래디언트값과 현재 그래디언트값을 비교하여 변화 지점인 지 여부를 결정할 수 있다."}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "기재된 실시예에 따라, RGB 카메라로부터 입력된 사용자의 얼굴 영상의 표정 분석을 통한 감정 변화의 시간적 지점을 검출할 수 있다."}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 비록 \"제1\" 또는 \"제2\" 등이 다양한 구성요소를 서술하기 위해서 사용되나, 이러한 구성요소는 상기와 같은 용 어에 의해 제한되지 않는다. 상기와 같은 용어는 단지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사 용될 수 있다. 따라서, 이하에서 언급되는 제1 구성요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있다. 본 명세서에서 사용된 용어는 실시예를 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세 서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한다 (comprises)\" 또는 \"포함하는(comprising)\"은 언급된 구성요소 또는 단계가 하나 이상의 다른 구성요소 또는 단 계의 존재 또는 추가를 배제하지 않는다는 의미를 내포한다."}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어는 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 해석될 수 있다. 또한, 일반적으로 사용되는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 이하에서는, 도 1 내지 도 7를 참조하여 실시예에 따른 표정 분석을 통한 감정 변화 검출 장치 및 방법이 상세 히 설명된다. 기재된 실시예는 지능형 로봇, 자율 주행 자동차 및 자폐 스펙트럼 장애 검사 공간 등에 장착되어 있는 RGB 카 메라로부터 입력된 사용자의 얼굴을 포함하고 있는 동영상을 활용하여 사용자의 표정 변화가 상대적으로 크거나 깊은 동영상 내의 시간적 지점(temporal location)을 검출하는 기술에 관한 것이다. 도 1은 실시 예에 따른 표정 분석을 통한 감정 변화 검출 장치의 블록 구성도이고, 도 1을 참조하면, 실시 예에 따른 표정 분석을 통한 감정 변화 검출 장치는, 카메라 영상 획득부, 전 처리부, 표정 분석부 및 감정 변화 분석부을 포함할 수 있다. 카메라 영상 획득부는, 적어도 하나의 사람이 포함된 RGB 동영상 또는 실시간 RGB 영상을 획득한다. 이때, 획득되는 동영상의 실시간 획득 여부에 따라, 추후 변화 지점 검출시에 상이한 변화 지점 검출 알고리즘이 적용 될 수 있다. 또한, 이때, 카메라 영상 획득부는, 웹캠, 원격지의 경우 IP 카메라, 키넥트 카메라 등을 사 용하여 RGB 영상을 획득할 수 있다. 전처리부은, 획득된 RGB 영상으로부터 사용자의 얼굴 영상을 추출하고, 전처리한다. 이에 대한 상세한 설 명은 도 2를 참조하여 후술하기로 한다. 표정 분석부는, 추출된 사용자의 얼굴 영상에서 표정 벡터를 추출한다. 이때, 동영상이 종료되거나, 사용 자로부터 감정 변화 분석 요청이 수신될 때까지 표정 벡터를 누적 저장한다. 이에 대한 상세한 설명은 도 2 및 도 3을 참조하여 후술하기로 한다. 감정 변화 분석부는, 누적 저장된 표정 벡터 값들을 기반으로 감정 신호를 생성하고, 생성된 감정 신호를 분석하여 최종적으로 감정의 급격한 변화가 있는 지점을 검출한다. 이에 대한 상세한 설명은 도 2 및 도 4를 참 조하여 후술하기로 한다. 도 2는 실시예에 따른 표정 분석을 통한 감정 변화 검출 장치의 세부 구성 블록도이다. 도 2를 참조하면, 전처리부는, 얼굴 검출부, 신원 인식부, 얼굴 자세 추출부 및 얼굴 정규 화부를 포함할 수 있다. 얼굴 검출부는, RGB 영상을 입력받아 사람의 얼굴 영역을 추론한다. 이때, RGB 영상에 복수의 사람들이 존 재할 경우, 얼굴 영역은 복수 개가 될 수 있다. 이때, 얼굴 검출 알고리즘은, 얼굴 검출을 위해 딥러닝 네트워크를 이용하여 학습된 RetinaFace, MTCNN 및 Adaboost Classifier가 학습된 Viola-Jones 얼굴 검출기 등이 활용될 수 있다. 신원 인식부는, 추론된 적어도 하나의 얼굴 영역에 해당하는 사람의 신원 분석을 통해 사용자의 얼굴을 검 색한다. 이때, 신원 인식(identity recognition) 또는 검증(verification)을 위해 기 학습된 딥러닝 네트워크(ResNet, MobileNet 등)는 미리 저장되어 있고, 신원 인식부는, 별도의 신원 정보 입력 프로세스를 통해 미리 획득 된 사용자의 얼굴 영상을 입력으로 딥러닝 네트워크가 출력한 제1 특징 벡터를 미리 저장한다. 그러면, 신원 인식부는, 영상에 포함된 적어도 하나의 얼굴 영역에 해당하는 영상 각각을 입력을 딥러닝 네트워크가 출력한 적어도 하나의 제2 특징 벡터를 추출하여, 제1 특징 벡터 및 적어도 하나의 제2 특징 벡터 각각의 유사도(similarity)를 점수화된 신뢰도가 소정 임계치 이상일 경우, 해당 얼굴 영역의 얼굴 영상을 사용 자로 판단할 수 있다. 얼굴 자세 추출부는, 얼굴 영상의 랜드마크(눈, 코, 입 등) 위치 정보 및 자세 정보(roll, pitch, yaw)를 추출하여 분석을 수행할 얼굴 영상의 입력을 정규화하기 위한 정보를 추출한다. 이때, 미리 학습된 딥러닝 기반 의 RetinaFace, MTCNN 등의 알고리즘을 활용하여 얼굴 랜드마크를 추출할 수 있다. 얼굴 정규화부는, 얼굴 자세 추출부에서 추출된 랜드마크 위치 벙보 및 자세 정보를 기반으로 얼굴 영상을 정규화(normalization)한다. 이를 통해, 얼굴 영상을 정렬(alignment)하여 표정 분석 성능을 향상시킬 수 있다. 일 실시예에 따라, 얼굴 정규화부는, 입력된 얼굴 영상의 랜드마크 위치, 예컨대, 눈의 위치를 기반으로 영상의 눈의 높이를 맞춰서 얼굴 영상을 변형 (transform)할 수 있다. 다른 실시예에 따라, 얼굴 정규화부는, 추출된 자세 정보를 기반으로 분석 대상이 되는 얼굴 영상을 필터 링할 수 있다. 예컨대, 정면에서 많이 벗어난 얼굴 영상은 제외하여 표정 분석에 입력으로 들어갈 얼굴 영상을 정제할 수 있다. 다시 도 2를 참조하면, 표정 분석부는, 표정 분류 추론부 및 표정 차원 추론부를 포함할 수 있 다. 이러한 표정 분석부는, 얼굴 표정 영상과 표정 레이블을 입력으로 받아 표정을 분류(classification) 또는 회귀(regression)하는 딥러닝 네트워크를 학습하여 구성된다. 표정 분류 추론부는, 정규화된 얼굴 영상에서 얼굴의 카테고리 모델 (P. Ekman and D. Keltner, \"Universal facial expressions of emotion,\" nonverbal Communication: Where Nature Meets Culture, pp.27- 46, 1997)의 표정 인식 분류 결과의 신뢰도(confidence score) 벡터를 추출하여 다차원의 감정 신호를 추출한다. 이때, 다차원 감정 신호는, 예컨대, 7차원의 감정 신호로, 행복, 슬픔, 역겨움, 화남, 놀람, 두려움 및 중립 표 정을 분류할 때, 딥러닝 네트워크의 확률 값으로 나타낸 Softmax 결과값을 활용할 수 있다. 표정 차원 추론부는, 전술한 감정 카테고리 모델 외의 감정을 다른 관점으로 해석하기 위해 표정 차원 모 델 (J. A. Russell, \"A circumplex model of affect.\" Journal of Personality and Social Psychology, vol. 39, no.6, p. 1161, 1980)의 정서가(valence), 각성 (arousal) 등의 표정 회귀 결과 값을 활용하여 다차원의 표정 벡터를 추가한다. 표정 분석부는, 정규화된 얼굴 영상을 입력으로 감정 카테고리 모델의 7차원 신뢰도 벡터와 표정 차원 모 델의 정서가(valence), 각성(arousal) 등의 차원 회귀 결과 값을 2차원 추가하여 총 9차원의 감정 신호로 구성 할 수 있다. 도 3은 실시예에 따른 표정 분석부의 딥러닝 네트워크 구성 예시도이다. 도 3을 참조하면, 딥러닝 네트워크를 활용하여 7차원의 표정 분류 결과 값과 2차원의 표정 차원 추론 결과 값을 구성하여 9차원의 누적된 표정 벡터가 누적될 수 있다. 다시 도 2를 참조하면, 감정 변화 분석부는 신호 추출부, 신호 정제부 및 변화 지점 검출부 를 포함할 수 있다. 신호 추출부는, 얼굴 영상으로부터 추출된 다차원 표정 벡터의 누적 값으로부터 분석할 감정 신호를 추출 한다. 일 실시 예로, 신호 추출부는, 다차원의 시계열 표정 벡터로부터 1차원의 행복 벡터만을 추출하여 1차원 감정 신호로 구성할 수 있다. 다른 실시예로, 신호 추출부는, 모든 차원의 표정 벡터를 모두 활용하여 9차원의 감정 신호로 구성할 수 있다. 신호 정제부는, 추출된 감정 신호의 노이즈 제거를 수행하여 감정 신호의 이상치(outlier)를 제거한다. 이 때, 신호 정제부 알고리즘으로, 신호 정점의 높이 등의 모양(shape)을 지키면서 노이즈를 제거하는 사비츠키-골 레이 (Savitzky-Golay) 필터가 활용될 수 있다. 변화 지점 검출부는, 노이즈 제거된 감정 신호를 기반으로 감정 신호의 변화 속도 및 크기가 소정 임계치 이상인 시간적 지점을 검출한다. 즉, 최종적으로 감정 신호의 변화가 급격하거나 또는 큰 폭의 시간적 지점을 검출한다. 이때, 변화 지점 검출부는, 입력 영상의 유형에 따라, 오프라인 변화 지점 검출 알고리즘과 온라인 변화 지점 검출 알고리즘을 수행할 수 있다. 오프라인 변화 지점 검출 알고리즘은 동영상 전체의 감정 신호를 모두 입력 받아서 변화 지점을 찾는 것에 비해, 온라인 변화 지점 검출 알고리즘은 사용자가 지정한 신호 길이에 도달하면 과거의 신호에 기반하여 현재 입력된 표정 벡터 값이 변화 지점인지 아닌지를 결과 값으로 출력할 수 있다. 오프라인 변화 지점 검출 알고리즘은, 일 실시예로 bottom-up merge(S. Chen, P.Gopalakrishnan, \"Speaker, environment and channel change detection and clustering via the Bayesian information criterion,\" in DARPA Broadcast News Transcription and Understanding Workshop, vol. 8, 1998, pp. 127-132.) 알고리즘이 활용될 수 있다. 여기서, bottom-up merge 알고리즘은, 전체 신호를 부신호(sub-signal)로 분할하고, 연속된 부 신호 간의 차이(difference)를 기반으로 변화 지점을 검출한다. 즉, 차이가 상대적으로 크거나 사용자가 지정한 임계 값보다 큰 경우, 연속된 부신호를 나누는 지점을 변화 지점으로 검출한다. 도 4는 bottom-up merge 알고리즘의 변화 검출 예시도이다. 도 4를 참조하면, 전체 신호를 일정한 길이의 부신호(sub-signal)로 분할하고, 연속된 부신호 간의 차이를 계산 한다. 그런 후, 계산된 차이가 사용자가 지정한 임계 값보다 작으면 병합을 수행하고, 그렇지 않으면 병합을 수 행하지 않는다. 이와 같은 동작을 반복 수행하여 전체 신호에서 변화가 큰 지점을 찾아낸다. 이때, 신호 간의 차이는, L2-distance, L1-distance 등을 활용하여 계산될 수 있다. 오프라인 변화 지점 검출 알고리즘은, 다른 실시예로 binary segmentation (J. Chen and A.K. Gupta, Parametric statistical change point anlaysis: with applications to genetics, medicine, and finance. Springer 2012), sliding window (K. Karagiannaki, A. Panousopoulou, and P. Tsakalides, \"An online feature selection architecture for human activity recognition, \"in IEEE International Conference on Acoustics, Speech and Signa Processing (ICASSP), 2017, pp. 2522-2526.), dynamic programming (M. Lavielle, \"Using penalized contrasts for the change-point problem,\" Signal Processing, vol.85, no. 8, pp. 1501-1510, 2005.) 및 pruned exact linear time (R. Killick, P. Fearnhead, and I. A. Eckley, \"Optimal detection of changepoints with a linear computational cost,\" Journal of the American Statistical Association, vol. 107, no. 500, pp. 1590-1598, 2012.) 등이 사용될 수도 있다. 한편, 온라인 변화 지점 검출 알고리즘은, 일 실시예로 간단하게는 그래디언트(gradient) 기반 알고리즘을 활용 하여, 과거의 신호를 분석하여 현재 표정 벡터 값이 변화 지점인지 아닌지를 검출할 수 있다. 즉, gradient 기 반 알고리즘에서는 사용자가 지정한 길이의 과거 신호를 미분하여 gradient 값을 계산하고, 과거 신호의 모든 gradient 값들보다 상대적으로 큰 현재 gradient 값이 계산되었을 때 또는 사용자가 지정한 임계 값 이상으로 현재 gradient 값이 계산되었을 때를 변화 지점으로 검출할 수 있다. 온라인 변화 지점 검출 알고리즘은, 다른 실시예로 Bayesian 알고리즘을 기반으로 과거 신호에 비해 현재 표정 벡터 값의 변화 가능성을 확률로 계산하여 변화 지점 검출이 가능하다. 이때, 변화 지점 검출 알고리즘의 입력은 다차원의 신호로 구성될 수 있으며, 변화 지점 검출부는 최종 결 과로 다차원의 신호의 변화 지점 결과를 통합 및 분석하여 변화 지점 검출을 수행한다. 또한, 언급한 변화 지점 검출 알고리즘을 이용하여 최종 출력할 변화 지점의 개수는 사용자가 지정한 개수대로 가장 변화가 큰 지점을 찾거나 (예시: Top-5, Top-1 등), 변화의 정도를 사용자가 지정한 임계 값 이상인 지점 을 변화 지점으로 정하여 출력 가능하다. 다음으로, 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법은, 적어도 하나의 사람이 포함된 동영상을 획 득하는 단계, 동영상으로부터 사용자의 얼굴 영상을 추출하고, 추출된 얼굴 영상을 전처리하는 단계, 사용자의 얼굴 영상에서 표정 벡터를 추출 및 누적 저장하는 단계 및 누적 저장된 표정 벡터 값들을 기반으로 추출된 감 정 신호를 분석하여 감정이 급변하는 시간적 지점을 검출하는 단계를 포함할 수 있다. 이때, 획득되는 동영상의 실시간 획득 여부에 따라, 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법은 두 가지 실시예들이 있을 수 있다. 일 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법은, 미리 촬영된 동영상이 획득됨에 따라, 오프라인 변 화 지점 검출 알고리즘이 수행될 수 있다. 다른 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법은, 실시간 동영상이 획득됨에 따라, 온라인 변화 지 점 검출 알고리즘이 수행될 수 있다. 도 5는 일 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법을 설명하기 위한 순서도이다. 도 5를 참조하면, 표정 분석을 통한 감정 변화 검출 장치(이하 '장치'로 기재함)는 적어도 하나의 사람이 포함된 RGB 동영상이 입력(S510)됨에 따라, 동영상으로부터 적어도 하나의 얼굴 영역을 추론한다(S520). 그런 후, 장치는 추론된 적어도 하나의 얼굴 영역에 해당하는 사람의 신원 분석을 통해 사용자의 얼굴을 검색한다(S530). 이때, 미리 획득된 사용자의 얼굴 영상을 입력으로 딥러닝 네트워크가 출력한 제1 특징 벡터를 미리 저장되어 있을 수 있다. 그리고, 장치는, 영상에 포함된 적어도 하나의 얼굴 영역에 해당하는 영상 각각을 입력을 딥러닝 네트워크 가 출력한 적어도 하나의 제2 특징 벡터를 추출할 수 있다. 장치는, 신원 인식을 통한 신원 신뢰도가 사용자가 지정한 임계값(T_id) 이상인지를 판단한다(S540). 즉, 제1 특징 벡터 및 적어도 하나의 제2 특징 벡터 각각의 유사도를 점수화된 신뢰도가 소정 임계치 이상일 경우, 해당 얼굴 영역의 얼굴 영상을 사용자로 판단한다. S540의 판단 결과 신원 신뢰도가 사용자가 지정한 임계값(T_id) 이상이 아닐 경우, 장치는 S510으로 진행 하여 사용자의 얼굴 영상을 다시 획득한다. 반면, S540의 판단 결과 신원 신뢰도가 사용자가 지정한 임계값(T_id) 이상일 경우, 장치는 검색된 사용자 의 얼굴 영상에서 랜드마크 위치 정보 및 자세 정보를 추출하는 단계 및 추출된 랜드마크 위치 정보 및 자세 정 보를 기반으로 얼굴 영상을 정규화하는 단계를 수행할 수 있다(S550). 이때, 얼굴 영상을 정규화하는 단계는, 추출된 랜드마크의 위치를 기반으로 얼굴 영상을 변형하거나, 추출된 자 세 정보를 기반으로 분석 대상이 되는 얼굴 영상을 필터링하는 것일 수 있다. 다음으로, 장치는, 정규화된 사용자의 얼굴 영상으로부터 얼굴 표정을 분석할 수 있다(S560). 이때, 장치는, S560에서 정규화된 얼굴 영상에서 감정 카테고리 모델의 7차원 신뢰도 벡터를 추출하는 단 계 및 표정 차원 모델의 정서가(valence), 각성(arousal)의 차원 회귀 결과 값을 2차원 추가하는 단계를 수행할 수 있다. 장치는, 동영상 종료 여부를 확인(S570)하여, 동영상이 종료될 때까지 S510 내지 S560을 반복 수행한다. 장치는, 동영상이 종료되면 얼굴 영상으로부터 추출된 다차원 표정 벡터의 누적 값으로부터 감정 신호를 추출(S580)하고, 추출된 감정 신호에서 노이즈 제거(S590)하고, 노이즈 제거된 감정 신호를 기반으로 감정 신호 의 변화 속도 및 크기가 소정 임계치 이상인 시간적 지점을 검출한다(S595). 이때, 장치는, S595에서 오프라인 변화 지점 검출 알고리즘을 사용하여, 동영상 전체의 감정 신호를 일정 길이의 부신호로 분할하고, 연속된 부신호들 간의 차이를 기반으로 변화 지점으로 검출할 수 있다. 도 6은 다른 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법을 설명하기 위한 순서도이다. 도 6을 참조하면, 장치는 적어도 하나의 사람이 포함된 RGB 동영상이 실시간 입력(S610)됨에 따라, 실시간 입력된 동영상으로부터 적어도 하나의 얼굴 영역을 추론한다(S620). 그런 후, 장치는 추론된 적어도 하나의 얼굴 영역에 해당하는 사람의 신원 분석을 통해 사용자의 얼굴을 검색한다(S630). 이때, 미리 획득된 사용자의 얼굴 영상을 입력으로 딥러닝 네트워크가 출력한 제1 특징 벡터를 미리 저장되어 있을 수 있다. 그리고, 장치는, 영상에 포함된 적어도 하나의 얼굴 영역에 해당하는 영상 각각을 입력을 딥러닝 네트워크 가 출력한 적어도 하나의 제2 특징 벡터를 추출할 수 있다. 장치는, 신원 인식을 통한 신원 신뢰도가 사용자가 지정한 임계값(T_id) 이상인지를 판단한다(S640). 즉, 제1 특징 벡터 및 적어도 하나의 제2 특징 벡터 각각의 유사도를 점수화된 신뢰도가 소정 임계치 이상일 경우, 해당 얼굴 영역의 얼굴 영상을 사용자로 판단한다. S640의 판단 결과 신원 신뢰도가 사용자가 지정한 임계값(T_id) 이상이 아닐 경우, 장치는 S610으로 진행 하여 사용자의 얼굴 영상을 다시 획득한다. 반면, S640의 판단 결과 신원 신뢰도가 사용자가 지정한 임계값(T_id) 이상일 경우, 장치는 검색된 사용자 의 얼굴 영상에서 랜드마크 위치 정보 및 자세 정보를 추출하는 단계 및 추출된 랜드마크 위치 정보 및 자세 정 보를 기반으로 얼굴 영상을 정규화하는 단계를 수행할 수 있다(S650). 이때, 얼굴 영상을 정규화하는 단계는, 추출된 랜드마크의 위치를 기반으로 얼굴 영상을 변형하거나, 추출된 자 세 정보를 기반으로 분석 대상이 되는 얼굴 영상을 필터링하는 것일 수 있다. 다음으로, 장치는, 정규화된 사용자의 얼굴 영상으로부터 얼굴 표정을 분석할 수 있다(S660). 이때, 장치는, S660에서 정규화된 얼굴 영상에서 감정 카테고리 모델의 7차원 신뢰도 벡터를 추출하는 단 계 및 표정 차원 모델의 정서가(valence), 각성(arousal)의 차원 회귀 결과 값을 2차원 추가하는 단계를 수행할 수 있다. 장치는, 누적 프레임의 길이가 사용자가 지정한 길이(T_Len) 이상인지를 확인(S670)하여, 누적 프레임의 길이가 사용자가 지정한 길이(T_Len) 가 될 때까지 S610 내지 S660을 반복 수행한다. 장치는, 동영상이 종료되면 얼굴 영상으로부터 추출된 다차원 표정 벡터의 누적 값으로부터 감정 신호를 추출(S680)하고, 추출된 감정 신호에서 노이즈 제거(S690)하고, 노이즈 제거된 감정 신호를 기반으로 감정 신호 의 변화 속도 및 크기가 소정 임계치 이상인 시간적 지점을 검출한다(S695). 이때, 장치는, S695에서 온라인 변화 지점 검출 알고리즘을 활용하여, 사용자가 지정한 신호 길이의 동영 상의 감정 신호를 미분하여 그래디언트값을 산출하고, 과거 신호의 그래디언트값과 현재 그래디언트값을 비교하 여 변화 지점인지 여부를 결정할 수 있다. 다음으로, 장치는, 카메라로 촬영된 실시간 동영상의 입력이 종료(S700)될 때까지 S610 내지 S695를 반복 수행한다. 도 7은 실시예에 따른 컴퓨터 시스템 구성을 나타낸 도면이다. 실시예에 따른 실시예에 따른 표정 분석을 통한 감정 변화 검출 장치는 컴퓨터로 읽을 수 있는 기록매체와 같은 컴퓨터 시스템에서 구현될 수 있다. 컴퓨터 시스템은 버스를 통하여 서로 통신하는 하나 이상의 프로세서, 메모리, 사용자 인터페이스 입력 장치, 사용자 인터페이스 출력 장치 및 스토리지를 포함할 수 있다. 또한, 컴퓨터 시스템은 네트워크에 연결되는 네트워크 인터페이스를 더 포함할 수 있다. 프로세서 는 중앙 처리 장치 또는 메모리나 스토리지에 저장된 프로그램 또는 프로세싱 인스트럭션들 을 실행하는 반도체 장치일 수 있다. 메모리 및 스토리지는 휘발성 매체, 비휘발성 매체, 분리형 매체, 비분리형 매체, 통신 매체, 또는 정보 전달 매체 중에서 적어도 하나 이상을 포함하는 저장 매체일 수 있 다. 예를 들어, 메모리는 ROM이나 RAM을 포함할 수 있다.또한, 하나 이상의 프로세서는 하나 이상의 프로그램을 실행시킬 수 있다. 이때, 프로그램은, 카메라 영 상 획득부, 전처리부, 표정 분석부 및 감정 변화 분석부을 포함할 수 있다."}
{"patent_id": "10-2022-0132977", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이상에서 첨부된 도면을 참조하여 본 발명의 실시예들을 설명하였지만, 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자는 본 발명이 그 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 실시 될 수 있다는 것을 이해할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다."}
{"patent_id": "10-2022-0132977", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 실시 예에 따른 표정 분석을 통한 감정 변화 검출 장치의 블록 구성도이다. 도 2는 실시예에 따른 표정 분석을 통한 감정 변화 검출 장치의 세부 구성 블록도이다. 도 3은 실시예에 따른 표정 분석부의 딥러닝 네트워크 구성 예시도이다. 도 4는 bottom-up merge 알고리즘의 변화 검출 예시도이다. 도 5는 일 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법을 설명하기 위한 순서도이다. 도 6은 다른 실시예에 따른 표정 분석을 통한 감정 변화 검출 방법을 설명하기 위한 순서도이다. 도 7은 실시예에 따른 컴퓨터 시스템 구성을 나타낸 도면이다."}
