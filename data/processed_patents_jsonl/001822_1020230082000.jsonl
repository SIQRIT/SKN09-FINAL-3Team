{"patent_id": "10-2023-0082000", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0000310", "출원번호": "10-2023-0082000", "발명의 명칭": "인공지능 기반의 로봇 과수 적과 처리장치 및 방법", "출원인": "전남대학교산학협력단", "발명자": "이경환"}}
{"patent_id": "10-2023-0082000", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "작업 대상 과일나무를 촬영하여 제1 영상을 생성하고 과총 분할 모델을 통해 상기 제1 영상에서 과총 객체를 인식하는 객체 인식부;과총을 촬영하여 제2 영상을 생성하고 열매 분할 모델을 통해 상기 제2 영상에서 상기 과총 내 개별 열매 객체들을 식별하는 개별 객체 식별부;상기 개별 열매 객체들에 대해 선별 기준에 따라 남겨둘 열매 객체와 제거할 열매 객체를 선별하는 적과선별부; 및선별 결과를 기초로 적과 작업 정보를 생성하고 적과 작업 로봇에게 상기 적과 작업 정보를 제공하여 적과 작업을 진행시키는 적과 작업 처리부를 포함하는 인공지능 기반의 로봇 과수 적과 처리장치."}
{"patent_id": "10-2023-0082000", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 객체 인식부는현장 설치 멀티 RGB-D 카메라로부터 RGB 이미지 및 깊이 데이터로 구성되는 상기 제1 영상을 실시간 수신하고상기 제1 영상을 상기 과총 분할 모델에 입력하여 상기 과총 객체를 인식하는 것을 특징으로 하는 인공지능 기반의 로봇 과수 적과 처리장치."}
{"patent_id": "10-2023-0082000", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 객체 인식부는과총의 RGB 이미지를 데이터셋으로 하는 시맨틱 분할(Semantic Segmentation) 딥러닝 네트워크로 상기 과총 분할 모델을 사전 구축하는 것을 특징으로 하는 인공지능 기반의 로봇 과수 적과 처리장치."}
{"patent_id": "10-2023-0082000", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 객체 인식부는상기 제1 영상의 RGB 이미지를 상기 과총 분할 모델에 입력하여 상기 과총 객체를 분할하고 분할된 과총 객체의포인트 클라우드를 클러스터링하여 그룹화하고, 상기 제1 영상의 깊이 데이터와 맵핑을 통해 과총 별 그룹 영역의 위치 정보를 구하는 것을 특징으로 하는 인공지능 기반의 로봇 과수 적과 처리장치."}
{"patent_id": "10-2023-0082000", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 개별 객체 식별부는객체 인식된 상기 과총의 위치로 상기 적과 작업 로봇의 팔에 장착된 RGB-D 카메라를 위치시켜 해당 과총을 촬영한 RGB 이미지 및 깊이 데이터로 구성되는 상기 제2 영상을 실시간 수신하고 상기 제2 영상을 상기 열매 분할모델에 입력하여 상기 개별 열매 객체들을 식별하는 것을 특징으로 하는 인공지능 기반의 로봇 과수 적과 처리장치.공개특허 10-2025-0000310-3-청구항 6 제5항에 있어서, 상기 개별 객체 식별부는과총 내 열매들의 RGB 이미지를 데이터셋으로 하는 인스턴스 분할(Instance Segmentation) 딥러닝 네트워크로상기 열매 분할 모델을 사전 구축하는 것을 특징으로 하는 인공지능 기반의 로봇 과수 적과 처리장치."}
{"patent_id": "10-2023-0082000", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 적과 선별부는상기 개별 열매 객체들에 대해 크기 및 색상을 기준으로 발육 상태를 결정하여 상기 발육 상태를 기초로 선별하고 상기 제2 영상의 깊이 데이터를 맵핑하여 남겨둘 열매 및 제거할 열매들의 위치 및 자세 정보를 구하는 것을특징으로 하는 인공지능 기반의 로봇 과수 적과 처리장치."}
{"patent_id": "10-2023-0082000", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 적과 작업 처리부는상기 과총 내 남겨둘 열매와 제거할 열매의 위치 및 자세 정보가 포함되는 상기 적과 작업 정보를 상기 적과 작업 로봇에게 제공하여 상기 적과 작업 로봇이 상기 적과 작업 정보에 따라 과총의 위치에서 해당 과총 내 제거할 열매들만 솎아내는 적과 작업을 수행하도록 하는 것을 특징으로 하는 인공지능 기반의 로봇 과수 적과 처리장치."}
{"patent_id": "10-2023-0082000", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "작업 대상 과일나무를 촬영하여 제1 영상을 생성하고 과총 분할 모델을 통해 상기 제1 영상에서 과총 객체를 인식하는 단계;과총을 촬영하여 제2 영상을 생성하고 열매 분할 모델을 통해 상기 제2 영상에서 상기 과총 내 개별 열매 객체들을 식별하는 단계;상기 개별 열매 객체들에 대해 선별 기준에 따라 남겨둘 열매 객체와 제거할 열매 객체를 선별하는 단계; 및선별 결과를 기초로 적과 작업 정보를 생성하고 적과 작업 로봇에게 상기 적과 작업 정보를 제공하여 적과 작업을 진행시키는 단계를 포함하는 인공지능 기반의 로봇 과수 적과 처리방법."}
{"patent_id": "10-2023-0082000", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공지능 기반의 로봇 과수 적과 처리장치 및 방법에 관한 것으로, 상기 장치는 작업 대상 과일나무를 촬영하여 제1 영상을 생성하고 과총 분할 모델을 통해 상기 제1 영상에서 과총 객체를 인식하는 객체 인식부; 과 총을 촬영하여 제2 영상을 생성하고 열매 분할 모델을 통해 상기 제2 영상에서 상기 과총 내 개별 열매 객체들을 식별하는 개별 객체 식별부; 상기 개별 열매 객체들에 대해 선별 기준에 따라 남겨둘 열매 객체와 제거할 열매 객체를 선별하는 적과 선별부; 및 선별 결과를 기초로 적과 작업 정보를 생성하고 적과 작업 로봇에게 상기 적과 작업 정보를 제공하여 적과 작업을 진행시키는 적과 작업 처리부를 포함한다. 따라서, 본 발명은 적과 작업에 대해 무인 자동화 시스템을 구현할 수 있다."}
{"patent_id": "10-2023-0082000", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 로봇 과수 적과 기술에 관한 것으로, 보다 상세하게는 이미지 분할 딥러닝 모델을 통해 적과할 객체 를 인식하고 인식한 객체 정보를 로봇에게 제공하여 적과 작업이 자동으로 이루어질 있는 인공지능 기반의 로봇 과수 적과 처리장치 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0082000", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "작물을 재배하는 데 있어 적과(摘果)는 중요한 작업 중 하나이며, 적과를 제시기에 하지 않으면, 상품성 있는 고품위 과실을 얻지 못한다. 사과 재배의 경우에는 5월에 사과의 품질과 수확량을 높이기 위해 과일 솎기 작업 인 적과를 수행한다. 일반적으로 수작업으로 이뤄지는 적과 작업은 먼저 작업자가 육안으로 과총(果叢) 즉, 개 별적인 열매가 한 부위에서 여러 개 발생하여 형성한 다발을 확인하고 과총 내 발육이 좋은 1~2개의 과일을 남겨두고 나머지 과일은 전지가위를 이용하여 제거하는 방식으로 진행된다. 하지만 이러한 작업은 노동력 집약도가 높고 많은 작업 시간이 투입되기 마련이다. 적과는 정교한 농작업 기술 이 필요해 기계적 일괄 처리로 대체하기 어려워 최근에도 수작업으로 많은 노동력이 투입되어 진행되고 있으며 적과 수작업을 위한 편의 장비만 개발되고 있다. 최근 농업에 인공지능을 적용한 연구가 많아지고 있음에도 불구하고, 적과 관련 자동화 연구는 아직 미비한 상 태이다. 따라서 적과 자동화를 위해 로봇을 이용한 적과 방식에 대해 개발이 요구되었다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제10-1473102 (2014.12.09)"}
{"patent_id": "10-2023-0082000", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 실시예는 상세하게는 이미지 분할 딥러닝 모델을 통해 적과할 객체를 인식하고 인식한 객체 정보 를 로봇에게 제공하여 적과 작업이 자동으로 이루어질 있는 인공지능 기반의 로봇 과수 적과 처리장치 및 방법 을 제공하고자 한다. 본 발명의 일 실시예는 과일나무로부터 과총을 인식하여 과총 내에서 남겨둘 열매를 결정하고 남겨둘 열매의 위 치와 자세를 추정하여 로봇 적과 작업을 진행할 수 있는 인공지능 기반의 로봇 과수 적과 처리장치 및 방법을 제공하고자 한다. 본 발명의 일 실시예는 육안으로 일일이 과총을 확인하지 않고 과총 내 발육이 좋은 열매들을 남겨두고 나머지 열매들을 로봇을 이용하여 신속 정확하게 제거함으로써 적과 작업의 편의성 및 효율성을 높일 수 있는 인공지능 기반의 로봇 과수 적과 처리장치 및 방법을 제공하고자 한다."}
{"patent_id": "10-2023-0082000", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "실시예들 중에서, 인공지능 기반의 로봇 과수 적과 처리장치는 작업 대상 과일나무를 촬영하여 제1 영상을 생성 하고 과총 분할 모델을 통해 상기 제1 영상에서 과총 객체를 인식하는 객체 인식부; 과총을 촬영하여 제2 영상 을 생성하고 열매 분할 모델을 통해 상기 제2 영상에서 상기 과총 내 개별 열매 객체들을 식별하는 개별 객체 식별부; 상기 개별 열매 객체들에 대해 선별 기준에 따라 남겨둘 열매 객체와 제거할 열매 객체를 선별하는 적 과 선별부; 및 선별 결과를 기초로 적과 작업 정보를 생성하고 적과 작업 로봇에게 상기 적과 작업 정보를 제공 하여 적과 작업을 진행시키는 적과 작업 처리부를 포함한다. 상기 객체 인식부는 현장 설치 멀티 RGB-D 카메라로부터 RGB 이미지 및 깊이 데이터로 구성되는 상기 제1 영상 을 실시간 수신하고 상기 제1 영상을 상기 과총 분할 모델에 입력하여 상기 과총 객체를 인식할 수 있다. 상기 객체 인식부는 과총의 RGB 이미지를 데이터셋으로 하는 시맨틱 분할(Semantic Segmentation) 딥러닝 네트 워크로 상기 과총 분할 모델을 사전 구축할 수 있다. 상기 객체 인식부는 상기 제1 영상의 RGB 이미지를 상기 과총 분할 모델에 입력하여 상기 과총 객체를 분할하고 분할된 과총 객체의 포인트 클라우드를 클러스터링하여 그룹화하고, 상기 제1 영상의 깊이 데이터와 맵핑을 통 해 과총 별 그룹 영역의 위치 정보를 구할 수 있다. 상기 개별 객체 식별부는 객체 인식된 상기 과총의 위치로 상기 적과 작업 로봇의 팔에 장착된 RGB-D 카메라를 위치시켜 해당 과총을 촬영한 RGB 이미지 및 깊이 데이터로 구성되는 상기 제2 영상을 실시간 수신하고 상기 제 2 영상을 상기 열매 분할 모델에 입력하여 상기 개별 열매 객체들을 식별할 수 있다. 상기 개별 객체 식별부는 과총 내 열매들의 RGB 이미지를 데이터셋으로 하는 인스턴스 분할(Instance Segmentation) 딥러닝 네트워크로 상기 열매 분할 모델을 사전 구축할 수 있다. 상기 적과 선별부는 상기 개별 열매 객체들에 대해 크기 및 색상을 기준으로 발육 상태를 결정하여 상기 발육 상태를 기초로 선별하고 상기 제2 영상의 깊이 데이터를 맵핑하여 남겨둘 열매 및 제거할 열매들의 위치 및 자 세 정보를 구할 수 있다. 상기 적과 작업 처리부는 상기 과총 내 남겨둘 열매와 제거할 열매의 위치 및 자세 정보가 포함되는 상기 적과 작업 정보를 상기 적과 작업 로봇에게 제공하여 상기 적과 작업 로봇이 상기 적과 작업 정보에 따라 과총의 위 치에서 해당 과총 내 제거할 열매들만 솎아내는 적과 작업을 수행하도록 할 수 있다. 실시예들 중에서, 인공지능 기반의 로봇 과수 적과 처리방법은 작업 대상 과일나무를 촬영하여 제1 영상을 생성 하고 과총 분할 모델을 통해 상기 제1 영상에서 과총 객체를 인식하는 단계; 과총을 촬영하여 제2 영상을 생성 하고 열매 분할 모델을 통해 상기 제2 영상에서 상기 과총 내 개별 열매 객체들을 식별하는 단계; 상기 개별 열 매 객체들에 대해 선별 기준에 따라 남겨둘 열매 객체와 제거할 열매 객체를 선별하는 단계; 및 선별 결과를 기 초로 적과 작업 정보를 생성하고 적과 작업 로봇에게 상기 적과 작업 정보를 제공하여 적과 작업을 진행시키는 단계를 포함한다."}
{"patent_id": "10-2023-0082000", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "개시된 기술은 다음의 효과를 가질 수 있다. 다만, 특정 실시예가 다음의 효과를 전부 포함하여야 한다거나 다 음의 효과만을 포함하여야 한다는 의미는 아니므로, 개시된 기술의 권리범위는 이에 의하여 제한되는 것으로 이 해되어서는 아니 될 것이다. 본 발명의 일 실시예에 따른 인공지능 기반의 로봇 과수 적과 처리장치 및 방법은 이미지 분할 딥러닝 모델을 통해 적과할 객체를 인식하고 인식한 객체 정보를 로봇에게 제공하여 적과 작업이 자동으로 이루어질 있다. 본 발명의 일 실시예에 따른 인공지능 기반의 로봇 과수 적과 처리장치 및 방법은 과일나무로부터 과총을 인식 하여 과총 내에서 남겨둘 열매를 결정하고 남겨둘 열매의 위치와 자세를 추정하여 로봇 적과 작업을 진행할 수 있다. 본 발명의 일 실시예에 따른 인공지능 기반의 로봇 과수 적과 처리장치 및 방법은 육안으로 일일이 과총을 확인 하지 않고 과총 내 발육이 좋은 열매들을 남겨두고 나머지 열매들을 로봇을 이용하여 신속 정확하게 제거함으로 써 적과 작업의 편의성 및 효율성을 높일 수 있다."}
{"patent_id": "10-2023-0082000", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명에 관한 설명은 구조적 내지 기능적 설명을 위한 실시예에 불과하므로, 본 발명의 권리범위는 본문에 설 명된 실시예에 의하여 제한되는 것으로 해석되어서는 아니 된다. 즉, 실시예는 다양한 변경이 가능하고 여러 가 지 형태를 가질 수 있으므로 본 발명의 권리범위는 기술적 사상을 실현할 수 있는 균등물들을 포함하는 것으로 이해되어야 한다. 또한, 본 발명에서 제시된 목적 또는 효과는 특정 실시예가 이를 전부 포함하여야 한다거나 그러한 효과만을 포함하여야 한다는 의미는 아니므로, 본 발명의 권리범위는 이에 의하여 제한되는 것으로 이해되어서는 아니 될 것이다. 한편, 본 출원에서 서술되는 용어의 의미는 다음과 같이 이해되어야 할 것이다. \"제1\", \"제2\" 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하기 위한 것으로, 이들 용어들에 의해 권리범위가 한정되어서는 아니 된다. 예를 들어, 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\"있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결될 수 도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\"있다고 언급된 때에는 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 한편, 구성요소들 간의 관계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃 하는\"과 \"~에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한 복수의 표현을 포함하는 것으로 이해되어야 하고, \"포함 하다\"또는 \"가지다\" 등의 용어는 실시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함을 지정하려는 것이며, 하나 또는 그 이상의 다른 특징이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이 들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 각 단계들에 있어 식별부호(예를 들어, a, b, c 등)는 설명의 편의를 위하여 사용되는 것으로 식별부호는 각 단 계들의 순서를 설명하는 것이 아니며, 각 단계들은 문맥상 명백하게 특정 순서를 기재하지 않는 이상 명기된 순 서와 다르게 일어날 수 있다. 즉, 각 단계들은 명기된 순서와 동일하게 일어날 수도 있고 실질적으로 동시에 수 행될 수도 있으며 반대의 순서대로 수행될 수도 있다. 본 발명은 컴퓨터가 읽을 수 있는 기록매체에 컴퓨터가 읽을 수 있는 코드로서 구현될 수 있고, 컴퓨터가 읽을 수 있는 기록 매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록 장치를 포함 한다. 컴퓨터가 읽을 수 있는 기록 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있다. 또한, 컴퓨터가 읽을 수 있는 기록 매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산 방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 여기서 사용되는 모든 용어들은 다르게 정의되지 않는 한, 본 발명이 속하는 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적으로 사용되는 사전에 정의되어 있는 용어들은 관 련 기술의 문맥상 가지는 의미와 일치하는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한 이 상적이거나 과도하게 형식적인 의미를 지니는 것으로 해석될 수 없다. 도 1은 본 발명의 일 실시예에 따른 인공지능 기반의 로봇 과수 적과 처리 시스템을 설명하는 도면이다. 도 1을 참조하면, 로봇 과수 적과 처리 시스템은 로봇, 카메라 센서, 과수 적과 처리장치 및 데이터베이스를 포함할 수 있다. 로봇은 과수원 현장에서 적과 작업을 수행할 수 있다. 일 실시예에서, 로봇은 자율 주행하면서 팔 (Arm)로 과일나무의 과총 내 제거할 열매만을 판정한 후 선택적으로 적과 작업을 수행할 수 있다. 로봇은 팔에 복수의 손가락으로 구성되는 손이 있으며, 남겨둘 열매에 손상을 주지 않으면서 소정의 힘으로 제거할 열 매를 잡은 다음 손의 회전력에 의해 열매를 솎아낼 수 있다. 로봇의 엔드이펙터(End-effector)는 과수를 적과하기 위한 도구가 사용될 수 있다. 예를 들어, 엔드이펙터는 가위, 절단기 형태 등이 쓰일 수 있고, 적과 대상 열매를 잡기 위하여 압력 센서를 가질 수 있다. 로봇은 과수 적과 처리장치와 네트워크를 통해 연결될 수 있고, 복수의 로봇들이 과수 적과 처리장치와 동시에 또는 시간 간격을 두고 연결될 수도 있다. 카메라 센서는 과수원 현장 및 로봇팔에 각각 설치되고 작업 대상 과일나무를 촬영한 제1 영상 및 해당 과 일나무 내의 과총을 촬영한 제2 영상을 각각 생성하여 전송할 수 있다. 여기에서, 카메라 센서는 멀티 RGB-D 카메라로 구현될 수 있으며, 반드시 이에 한정되는 것은 아니다. 카메라 센서는 과일나무에 있는 과총 및 과총 내 열매를 각각 촬영하고 RGB 이미지 및 깊이 정보로 구성되는 촬영 영상을 과수 적과 처리장치 에게 제공할 수 있다. 카메라 센서는 과수 적과 처리장치와 네트워크를 통해 연결될 수 있고, 복수의 카메라 센서들이 과수 적과 처리장치와 동시에 또는 시간 간격을 두고 연결될 수도 있다. 과수 적과 처리장치는 로봇을 이용한 자동화된 적과 작업 처리를 수행하는 컴퓨터 또는 프로그램에 해당하는 서버로 구현될 수 있다. 과수 적과 처리장치는 로봇 및 카메라 센서와 유선 또는 무 선 네트워크를 통해 연결될 수 있고 상호 간에 데이터를 주고받을 수 있다. 한편, 과수 적과 처리장치는 본 발명에 따른 인공지능 기반의 로봇 과수 적과 처리를 수행하는 과정에서 다양한 외부 시스템(또는 서버)과 연동하여 동작할 수 있다. 과수 적과 처리장치는 카메라 단말로부터 촬영된 영상을 수신하고 영상 분석을 통해 실시간으로 과총 인식 및 과총 내 개별 열매를 탐지할 수 있다. 과수 적과 처리장치는 실시 간으로 탐지된 과총 내 개별 열매들에 대해 선별 알고리즘을 기초로 남겨둘 열매를 결정한 뒤 남겨둘 열매와 제 거할 열매의 위치 및 자세를 추정할 수 있고, 로봇에게 과총 내 남겨둘 열매와 제거할 열매의 위치 및 자 세 정보를 전송할 수 있다. 데이터베이스는 과수 적과 처리장치의 동작 과정에서 필요한 다양한 정보들을 저장하는 저장장치에 해당할 수 있다. 예를 들어, 데이터베이스는 카메라 센서로부터 촬영된 영상을 저장할 수 있고, 과총 및 과총 내 개별 열매의 의미론적 분할을 위한 AI 모델 구축을 위한 학습 알고리즘 및 데이터셋을 저장할 수 있 으며, 반드시 이에 한정되지 않고, 과수 적과 처리장치가 본 발명에 따른 로봇을 이용한 인공지능 기반의 과수 적과 처리작업을 수행하는 과정에서 다양한 형태로 수집 또는 가공된 정보들을 저장할 수 있다. 도 2는 도 1의 과수 적과 처리장치의 시스템 구성을 설명하는 도면이다. 도 2를 참조하면, 과수 적과 처리장치는 프로세서, 메모리, 사용자 입출력부 및 네트워크 입출력부를 포함할 수 있다. 프로세서는 과수 적과 처리장치가 동작하는 과정에서의 각 단계들을 처리하는 프로시저를 실행할 수 있고, 그 과정 전반에서 읽혀지거나 작성되는 메모리를 관리할 수 있으며, 메모리에 있는 휘발성 메 모리와 비휘발성 메모리 간의 동기화 시간을 스케줄할 수 있다. 프로세서는 과수 적과 처리장치의 동 작 전반을 제어할 수 있고, 메모리, 사용자 입출력부 및 네트워크 입출력부와 전기적으로 연결 되어 이들 간의 데이터 흐름을 제어할 수 있다. 프로세서는 과수 적과 처리장치의 CPU(Central Processing Unit)로 구현될 수 있다. 메모리는 SSD(Solid State Drive) 또는 HDD(Hard Disk Drive)와 같은 비휘발성 메모리로 구현되어 과수 적과 처리장치에 필요한 데이터 전반을 저장하는데 사용되는 보조기억장치를 포함할 수 있고, RAM(Random Access Memory)과 같은 휘발성 메모리로 구현된 주기억장치를 포함할 수 있다. 사용자 입출력부는 사용자 입력을 수신하기 위한 환경 및 사용자에게 특정 정보를 출력하기 위한 환경을 포함할 수 있다. 예를 들어, 사용자 입출력부는 터치 패드, 터치 스크린, 화상 키보드 또는 포인팅 장치와 같은 어댑터를 포함하는 입력장치 및 모니터 또는 터치스크린과 같은 어댑터를 포함하는 출력장치를 포함할 수 있다. 일 실시예에서, 사용자 입출력부는 원격 접속을 통해 접속되는 컴퓨팅 장치에 해당할 수 있고, 그러 한 경우, 과수 적과 처리장치는 독립적인 서버로서 수행될 수 있다. 네트워크 입출력부은 네트워크를 통해 외부 장치 또는 시스템과 연결하기 위한 환경을 포함하고, 예를 들 어, LAN(Local Area Network), MAN(Metropolitan Area Network), WAN(Wide Area Network) 및 VAN(Value Added Network) 등의 통신을 위한 어댑터를 포함할 수 있다. 도 3은 도 1의 과수 적과 처리장치의 기능적 구성을 설명하는 도면이다. 도 3을 참조하면, 과수 적과 처리장치는 객체 인식부, 개별 객체 식별부, 적과 선별부, 적 과 작업 처리부 및 제어부를 포함할 수 있다. 객체 인식부는 제1 영역을 촬영하여 제1 영상을 생성하고 제1 영상에서 객체를 인식할 수 있다. 여기에서, 제1 영역은 과수원 현장에 설치된 카메라 센서의 촬영 범위에 해당할 수 있다. 예컨대, 제1 영 상은 카메라 센서의 설치 현장에 있는 작업 대상 과수나무가 촬영된 영상일 수 있다. 객체 인식부는 카메라 센서에서 촬영한 제1 영역의 영상(제1 영상)을 실시간으로 수신할 수 있다. 카메라 센서는 멀티 RGB-D 카메라로 구현되고 설치 현장에 있는 작업 대상 과일나무를 촬영하여 RGB 이미 지와 깊이 정보로 구성되는 제1 영상을 과수 적과 처리장치에 제공할 수 있다. RGB-D 카메라는 RGB 카메라와 깊이 카메라로 구성되고 RGB 카메라로 영상을 촬영하며 깊이 카메라로 깊이 영상(흑백)을 획득한다. 깊이 영상은 적외선의 반사되는 정도를 측정하여 카메라와 촬영된 객체 간의 실제 거리를 표현한다. 깊이 카메라는 RGB 카메라로 획득된 영상의 모든 픽셀의 3차원 정보를 획득할 수 있다. 과수 적과 처리장치는 이미지에 깊이 정보를 추가하여 객체 인식률을 높일 수 있다. 객체 인식부는 이미지 분할 딥러닝 모델을 사용하여 제1 영상에 있는 작업 대상 과수나무로부터 과총 객체 를 인식할 수 있다. 딥러닝의 이미지 분할에는 2가지 방법으로 시맨틱 분할(Semantic segmentation)과 인스턴스 분할(Instance segmentation)이 있다. 시맨틱 분할은 객체의 분할을 수행하되 같은 클래스에 속하는 객체들은 같은 영역과 색 으로 분할된다는 것을 의미하며, 인스턴스 분할은 같은 클래스에 속하더라도 객체들을 모두 구분해 주는 것을 의미한다. 즉, 시맨틱 분할의 경우에는 과총 내 열매 각각을 구별하지 않고 열매들이 속한 과총을 하나의 큰 픽셀 덩어리로 분류할 수 있다. 인스턴스 분할의 경우에는 과총 내 열매들을 하나의 덩어리로 합치지 않고 각 열매를 구분하여 개별 객체로 분류할 수 있다. 일 실시예에서, 객체 인식부는 시맨틱 분할(semantic segmentation)로 제1 영상 내에 있는 과총 객체를 의 미 분할하여 과총 객체를 인식할 수 있다. 즉, 객체 인식부는 제1 영상의 RGB 이미지를 기 구축된 과총 분할 모델에 입력하여 과총 객체를 분할하고 분할된 과총 객체들을 클러스터링(clustering)을 이용하여 과총 별 로 그룹화할 수 있다. 여기에서, 과총 분할 모델은 과총의 RGB 정보 기반 데이터셋을 학습하여 작업 대상 과일 나무에 있는 과총 객체를 분할하도록 구현된 시맨틱 분할 딥러닝 모델에 해당할 수 있다. 객체 인식부는 과총 분할 모델을 사전학습을 통해 구축할 수 있으며, 과총 분할 모델을 기초로 작업 대상 과일나무를 촬영한 제1 영상을 구성하는 RGB 이미지 정보를 통해 과일나무에 있는 과총 객체를 인식할 수 있다. 객체 인식부(31 0)는 별도의 학습 서버와 연동하여 동작할 수 있으며, 이 경우 학습 서버에 의해 구축된 과총 분할 모델을 이용 하여 RGB 이미지로부터 과총 객체에 대한 시맨틱 분할을 수행할 수 있다. 또한, 과총 분할 모델은 과총의 RGB 정보 기반 데이터셋을 학습한 결과로서 구축될 수 있다. 여기에서, 과총의 RGB 정보는 사과, 배 등의 적과 작업이 요구되는 과일나무 별로 해당 과일나무의 과총을 포함한 영역을 촬영하 여 획득한 이미지 정보에 해당할 수 있다. 일 실시예에서, 객체 인식부는 깊이 정보와 맵핑을 통해 각 그룹 영역 3차원 위치 정보를 산출하여 작업 대상 과일나무에서 과총의 위치를 파악할 수 있다. 깊이 영상은 객체와 카메라 간의 거리를 측정하며 직관적으 로 같은 객체는 유사한 깊이를 가진다. 유사한 깊이를 가지는 영역은 같은 객체일 확률이 높다는 것을 의미한 다. 개별 객체 식별부는 제2 영역의 제2 영상에서 과총 내 개별 열매 객체를 식별할 수 있다. 여기에서, 제2 영역은 과총의 위치에 해당할 수 있으며, 제2 영상은 과총의 위치로 로봇 팔에 장착된 카메라 센서를 위치 시켜 해당 과총 내 열매들을 촬영한 영상에 해당할 수 있다. 제2 영상은 RGB 이미지와 깊이 정보로 구성될 수 있다. 일 실시예에서, 개별 객체 식별부는 기 구축된 열매 분할 모델을 통해 제2 영상의 RGB 이미지로부 터 과총 내 개별 열매들을 인식할 수 있다. 여기에서, 열매 분할 모델은 과총 열매들에 대한 개별 객체 의미론 적 분할을 위한 학습 모델에 해당할 수 있다. 일 실시예에서, 개별 객체 식별부는 열매 분할 모델이 인스턴스 분할(Instance Segmentation) 딥러닝 모델 인 경우에는 과총 내 열매들 각각에 대한 마스킹을 수행하여 개별 객체를 인식할 수 있다. 즉, 개별 객체 식별 부는 Mask-Rcnn를 활용하여 영상 내 각 객체에 마스킹 처리할 수 있다. Mask-Rcnn은 객체가 있을만한 영 역(바운딩 박스)을 탐지 후 탐지한 영역 내 어떠한 범주가 있을지 예측하고 탐지한 영역 내 픽셀이 예측한 객체 인지 아닌지 예측하는 객체 탐지 모델이다. 특히, Mask-Rcnn은 바운딩 박스로 영상 내에 있는 과총 객체를 탐 지하는 것과 더불어 인스턴스 분할로 각 객체에 마스킹까지 할 수 있는 모델이다. 적과 선별부는 식별된 열매 개별 객체에 대해 적과 여부를 결정할 수 있다. 일 실시예에서, 적과 선별부 는 선별 알고리즘에 따라 발육이 가장 좋은 열매 1~2개를 선별하여 과총 내 남겨둘 열매와 제거할 열매를 결정할 수 있다. 여기에서, 선별 알고리즘은 크기 및 색상을 기준으로 발육 상태를 결정할 수 있다. 예를 들 어, 선별 알고리즘은 열매 영상의 색깔에 대한 분포를 파악하고 해당 열매 영상의 면적을 계산하여 개별 열매의 발육 상태를 결정할 수 있다. 적과 선별부는 개별 객체의 마스킹 영역의 크기(Size)를 연산하여 열매의 크기를 구할 수 있다. 적과 선별부는 깊이 정보를 맵핑하여 보존할 열매와 제거할 열매들의 3차원 위치와 자세를 계산할 수 있다. 적과 작업 처리부는 적과 선별 결과를 기초로 적과 작업 정보를 생성하고 로봇에게 적과 작업 정보를 제공할 수 있다. 여기에서, 적과 작업 정보는 과총 내 남겨둘 열매와 제거할 열매의 위치 및 자세 정보가 포함 될 수 있다. 로봇은 적과 작업 정보에 따라 과총의 위치에서 해당 과총 내 제거할 열매들만 솎아내는 적 과 작업을 수행할 수 있다. 제어부는 과수 적과 처리장치의 전체적인 동작을 제어하고, 객체 인식부, 개별 객체 식별부 , 적과 선별부 및 적과 작업 처리부 간의 제어 흐름 또는 데이터 흐름을 관리할 수 있다. 도 4는 본 발명에 따른 인공지능 기반의 로봇 과수 적과 처리 과정의 일 실시예를 설명하는 순서도이다. 도 4를 참조하면, 과수 적과 처리장치는 객체 인식부를 통해 작업 대상 과일나무를 촬영하여 제1 영 상을 생성하고 제1 영상에서 과총 객체를 인식할 수 있다(단계 S410). 객체 인식부는 과수원 현장에 설치 된 카메라 센서에서 작업 대상 과일나무를 촬영한 제1 영상을 획득할 수 있다. 제1 영상에는 깊이 정보가 포함될 수 있다. 객체 인식부는 과총의 의미론적 분할을 위한 인공지능 학습 모델을 이용하여 제1 영상에 서 과총 객체를 인식하고 인식된 과총 객체를 깊이 정보와 맵핑하여 과총의 위치를 파악할 수 있다. 과수 적과 처리장치는 개별 객체 식별부를 통해 위치가 파악된 과총을 촬영하여 제2 영상을 생성하고 제2 영상에서 과총 내 각각의 열매 개별 객체를 식별할 수 있다(단계 S430). 개별 객체 식별부는 과총의 위치로 이동시킨 로봇의 팔에 장착된 카메라 센서에서 해당 과총을 촬영한 제2 영상을 획득할 수 있 다. 제2 영상에도 깊이 정보가 포함될 수 있다. 개별 객체 식별부는 과총 열매의 의미있는 객체의 분할 을 위한 인공지능 학습 모델을 이용하여 제2 영상에서 열매들 각각의 개별 객체를 인식하고 인식된 열매 개별 객체를 깊이 정보와 맵핑하여 열매의 위치 및 자세를 추정할 수 있다. 과수 적과 처리장치는 적과 선별부를 통해 식별된 과총 내 열매들에 대해 선별 기준에 따라 남겨둘 열매와 제거할 열매를 선별할 수 있다(단계 S450). 과수 적과 처리장치는 적과 작업 처리부를 통해 선별 결과를 기초로 적과 작업 정보를 생성하고 로봇 에게 제공하여 로봇을 이용한 자동 적과 작업을 수행할 수 있다(단계 S470). 로봇은 적과 작업 정보에 따라 과총의 위치에서 남겨둘 열매 및 제거할 열매의 위치 및 자세를 기초로 남겨둘 열매에 손상을 주지 않으면서 제거할 열매만을 솎아내는 적과 작업을 시행할 수 있다. 도 5는 본 발명에 따른 과수 적과 처리장치의 동작 흐름을 설명하는 도면이다. 도 5를 참조하면, 과수 적과 처리장치는 멀티 RGB-D 카메라 영상 분석을 통해 실시간으로 작업 대상 과일 나무로부터 과총의 객체를 인식 및 과총의 위치를 파악한 다음 해당 과총 내에서 남겨둘 열매를 결정하고 남겨 둘 열매의 위치와 자세를 추정하여 로봇 적과 작업을 효과적으로 진행할 수 있다. 보다 구체적으로, 과수 적과 처리장치는 과수원 현장에 설치된 카메라 센서로부터 작업 대상 과일나무에 대한 제1 영상을 획득할 수 있다(단계 S510). 카메라 센서는 깊이 카메라(Depth Camera)를 포함할 수 있고, 이 경우에 제1 영상에 는 과일나무에 대한 RGB 이미지 및 깊이 데이터를 포함할 수 있다. 과수 적과 처리장치는 획득된 제1 영 상을 포인트 클라우드 시맨틱 분할로 의미 분할하여 과총의 객체를 인식할 수 있다(단계 S520). 여기에서, 포 인트 클라우드(Point cloud)는 RGB-D 카메라 센서로부터 수집되는 데이터를 의미한다. RGB-D 카메라 센서는 물 체(여기서는 작업 대상 과일나무)에 빛 또는 신호를 보내서 돌아오는 시간을 기록하여 각 빛 또는 신호 당 거리 정보를 계산하고 하나의 포인트를 생성한다. 포인트 클라우드는 3차원 공간에 속한 데이터 점(포인트)들의 집 합을 의미한다. 포인트 클라우드는 2D 이미지와 다르게 깊이(z축) 정보를 가지고 있기 때문에 3차원 형상이나 객체를 표현한다. 데이터 집합의 각 점은 x,y,z 기하 좌표로 표현된다. 과수 적과 처리장치는 의미 분할된 과총 객체의 포인트 클라우드를 특정 기준으로 클러스터링하여 그룹화 할 수 있다(단계 S530). 일 실시예에서, 과수 적과 처리장치는 유클리드 거리 기반 클러스터링을 통해 포 인트 클라우드를 거리에 따라 그룹핑할 수 있다. 과수 적과 처리장치는 각 그룹 영역을 깊이(z축) 정보와 맵핑을 통해 인식된 과총의 위치 정보를 구할 수 있다(단계 S540). 과수 적과 처리장치는 과총의 위치로 로봇팔에 장착된 카메라 센서를 위치시켜 해당 위치에 있는 과총에 대한 제2 영상을 획득할 수 있다(단계 S550). 과수 적과 처리장치는 로봇의 이동을 제어하여 팔에 장착된 카메라 센서를 작업 대상과일나무에 있는 과총의 위치로 이동시킬 수 있다. 로봇의 팔에 장착된 카메라 센서는 깊이 카메라 (Depth Camera)를 포함할 수 있고, 이 경우에 제2 영상에는 과총에 대한 RGB 이미지 및 깊이 데이터를 포함할 수 있다. 과수 적과 처리장치는 획득된 제2 영상을 인스턴스 분할로 의미 있는 객체 분할하여 과총 내 열 매들의 개별 객체를 식별할 수 있다(단계 S560). 여기에서, 인스턴스 분할은 다중 객체들(과총 내 열매들) 각 각에 대한 마스킹(색칠)을 수행하여 겹쳐져 있는 객체들을 구별해내고 배경을 추출하여 개별 객체(과총 내 개별 열매들)를 인식할 수 있다. 과수 적과 처리장치는 개별 식별된 각각의 열매 객체들에 대해 선별 알고리즘에 따라 발육이 가장 좋은 열 매 1~2개를 선별하고 깊이 정보를 맵핑하여 보존할 열매와 제거할 열매들의 3차원 위치 및 자세를 구할 수 있다 (단계 S570). 과수 적과 처리장치는 크기 및 색상을 기준으로 발육 상태를 결정할 수 있다. 과수 적과 처리장치는 남겨둘 열매와 제거할 열매의 위치 및 자세 정보를 로봇에게 전송하여 적과 작업을 시행 할 수 있다(단계 S580)."}
{"patent_id": "10-2023-0082000", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 2, "content": "도 6은 본 발명에 따른 과총 분할 모델의 프레임워크를 나타내는 도면이다. 도 6을 참조하면, 과총 분할 모델은 시맨틱 분할 딥러닝 네트워크로 구성될 수 있다. 시맨틱 분할은 주로 RGB 이미지를 사용하며 깊이 정보 등을 추가하면 의미 분할의 정확도를 향상시킬 수 있다. 깊이 정보의 추가는 RGB 및 깊이 데이터를 함께 연결하고 네트워크를 사용하거나 두개의 네트워크를 사용하여 RGB 및 깊이의 특징을 추 출한 다음 네트워크 중간에서 특징 융합을 수행할 수 있다. 과총 분할 모델은 도 6과 같이, RGB 및 X 모달 입력에서 특징을 추출하는 두개의 병렬 백본, 중간 입력으로 특 징 수정을 위한 CM-FRM(Cross-Modal Feature Rectification Module) 및 FFM(Feature Fusion Module)로 구성된 다. 과총 분할 모델은 대칭 이중 경로 구조를 통해 교차 모달 전역 추론이 수행되며 혼합 채널 임베딩이 적용 될 수 있다. 즉, 과총 분할 모델은 RGB 색상 정보에 깊이 데이터를 융합할 수 있다. 도 7은 본 발명에 따른 열매 분할 모델의 프레임워크를 나타내는 도면이다. 도 7을 참조하면, 열매 분할 모델은 영상에서 과총 내 열매들의 개별 객체를 인식하기 위해 우선, 백본 (backbone)을 통해 특징을 추출한다. 여기에서, 이미지의 공간정보를 유지하면서 특징, 예를 들어 라인, 코너, 특징점 등을 추출하여 최종적으로 배열형태의 특징맵(Feature maps)을 추출한다. 백본으로 MobileNets-FPN을 사용하여 영상의 특징을 추출하고 이후 추출된 특징 계층을 가지고 영상 내에 객체가 있을 법한 위치에 바운딩 박스를 쳐주는 RPN(Region Proposal Network)를 수행한다. RPN에서 제안된 영역만을 가지고 객체가 있는 곳에 바운딩 박스를 잘 쳤는지 다시 확인하는 절차 및 해당 객체의 종류를 판별하는 과정을 수행하게 된다. 이후, FCN(Fully Convolutional Network)을 통해 영상 내에 각 객체들에 마스크가 씌어지게 된다. 일 실시예에 따른 인공지능 기반의 로봇 과수 적과 처리장치 및 방법은 인공지능 학습 모델을 통해 과일나무의 과총 및 과총 내 개별 열매를 단계적으로 인식하고 선별을 통해 최종적으로 로봇에게 열매의 위치 및 자세 정보 를 제공하여 자동화된 적과 작업을 수행할 수 있다. 이에 따라, 적과 작업에 투입되는 노동력 및 시간을 최소 화하며 적과 작업의 스마트 무인 자동화를 구현할 수 있다. 상기에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특 허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다."}
{"patent_id": "10-2023-0082000", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 인공지능 기반의 로봇 과수 적과 처리 시스템을 설명하는 도면이다. 도 2는 도 1의 과수 적과 처리장치의 시스템 구성을 설명하는 도면이다. 도 3은 도 1의 과수 적과 처리장치의 기능적 구성을 설명하는 도면이다. 도 4는 본 발명에 따른 인공지능 기반의 로봇 과수 적과 처리 과정의 일 실시예를 설명하는 순서도이다. 도 5는 본 발명에 따른 로봇 과수 적과 처리장치의 동작 흐름을 설명하는 도면이다. 도 6은 본 발명에 따른 과총 분할 모델의 프레임워크를 나타내는 도면이다. 도 7은 본 발명에 따른 열매 분할 모델의 프레임워크를 나타내는 도면이다."}
