{"patent_id": "10-2023-0130033", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0047436", "출원번호": "10-2023-0130033", "발명의 명칭": "딥러닝 기반의 교통상태 추정 시스템 및 그 방법", "출원인": "한국과학기술원", "발명자": "여화수"}}
{"patent_id": "10-2023-0130033", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "복수의 검지영역들 및 상기 복수의 검지영역들 상의 차량을 촬영하여 복수의 원본영상들을 생성하는 복수의 카메라들; 및상기 복수의 검지영역들 중 인접한 어느 두 검지영역들 사이의 영역인 비검지영역의 교통상태를 추정하기 위한딥러닝(Deep Learning) 기반의 교통상태 추정모델이 저장되고, 상기 복수의 카메라들과 통신할 수 있는 서버를포함하며,상기 복수의 검지영역들 각각은 교차로의 적어도 일부를 포함하고, 상기 비검지영역의 교통상태는 상기 비검지영역 내 차량의 수로 정의되는 교통밀도이며,상기 서버는 영상분석 알고리즘을 이용하여 상기 복수의 원본영상들 각각에서 차량을 객체로 추출하고, 상기 객체를 추적하여 차량궤적정보를 생성하며, 상기 객체의 특징을 추출하여 차량특징정보를 생성하고,상기 서버는 상기 원본영상을 탑뷰영상으로 변환하며, 상기 탑뷰영상을 이용하여 공간 내 차량의 수로 정의되는교통밀도정보를 산출하고,상기 서버는 상기 차량특징정보 중 적어도 일부를 포함하는 재인식특성을 이용하여 상기 복수의 원본영상들 중어느 하나에서 추출된 제1 객체 및 상기 복수의 원본영상들 중 다른 하나에서 추출된 제2 객체가 서로 동일한지여부를 판단하여 차량매칭정보를 생성하고,상기 교통상태 추정모델은 상기 교통밀도정보 및 상기 차량매칭정보를 이용하여 상기 비검지영역의 교통상태를추정하는 딥러닝 기반의 교통상태 추정시스템."}
{"patent_id": "10-2023-0130033", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 서버는 상기 객체에 대응되는 상기 재인식특성을 기초로 특성벡터를 생성하고, 상기 제1 객체에 대응되는제1 특성벡터 및 상기 제2 객체에 대응되는 제2 특성벡터의 유사도를 계산하며, 상기 유사도를 이용하여 상기제1 객체 및 상기 제2 객체가 서로 동일한지 여부를 판단하여 상기 차량매칭정보를 생성하는 딥러닝 기반의 교통상태 추정시스템."}
{"patent_id": "10-2023-0130033", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 서버는 상기 객체의 이동시간 및 상기 객체의 이동방향을 더 이용하여 상기 차량매칭정보를 생성하는 딥러닝 기반의 교통상태 추정시스템."}
{"patent_id": "10-2023-0130033", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 교통상태 추정모델은 인코더-디코더 구조를 가지고, CNN 및 Transformer 중 적어도 어느 하나를 이용하는딥러닝 기반의 교통상태 추정시스템."}
{"patent_id": "10-2023-0130033", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서,상기 교통상태 추정모델은 진입 교통량 및 직진신호 길이에 따른 AIMSUN 시뮬레이션 결과인 로우데이터가 전처리되어 생성된 학습데이터를 기반으로 학습되는 딥러닝 기반의 교통상태 추정시스템.공개특허 10-2025-0047436-3-청구항 6 제1 항에 있어서,상기 교통상태 추정모델은 교통신호정보를 더 이용하여 상기 비검지영역의 상기 교통상태를 추정하는 딥러닝 기반의 교통상태 추정시스템."}
{"patent_id": "10-2023-0130033", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 영상분석 알고리즘은 Yolov5 및 JDE tracker 중 적어도 어느 하나를 포함하는 딥러닝 기반의 교통상태 추정시스템."}
{"patent_id": "10-2023-0130033", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "복수의 카메라들 및 서버를 이용하는 딥 러닝 기반의 교통상태 추정방법에 있어서,상기 복수의 카메라들은 복수의 검지영역들 및 상기 복수의 검지영역 상의 차량을 촬영하여 복수의 원본영상들을 생성하고, 상기 서버에 상기 복수의 검지영역들 중 인접한 어느 두 검지영역들 사이의 영역인 비검지영역의교통상태를 추정하기 위한 딥러닝(Deep Learning) 기반의 교통상태 추정모델이 저장되고, 상기 서버는 상기 복수의 카메라들과 통신할 수 있으며,상기 서버가 영상분석 알고리즘을 이용하여 복수의 원본영상들에서 차량을 객체로 추출하고, 상기 객체를 추적하여 차량궤적정보를 생성하며, 상기 객체의 특징을 추출하여 차량특징정보를 생성하는 차량객체인식 단계;상기 서버가 상기 원본영상을 탑뷰영상으로 변환하며, 상기 탑뷰영상을 이용하여 공간 내 차량의 수로 정의되는교통밀도정보를 산출하는 교통밀도산출 단계;상기 서버가 상기 차량특징정보 중 적어도 일부를 포함하는 재인식특성을 이용하여 상기 복수의 원본영상들 중어느 하나에서 추출된 제1 객체 및 상기 복수의 원본영상들 중 다른 하나에서 추출된 제2 객체가 서로 동일한지여부를 판단하여 차량매칭정보를 생성하는 매칭정보생성 단계; 및 상기 서버가 상기 교통밀도정보 및 상기 차량매칭정보를 기초로 상기 교통상태 추정모델을 이용하여 상기 비검지영역의 상기 교통상태를 추정하는 비검지영역추정 단계를 포함하는 딥러닝 기반의 교통상태 추정방법."}
{"patent_id": "10-2023-0130033", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 매칭정보생성 단계는,상기 서버가 상기 객체에 대응되는 상기 재인식특성을 기초로 특성벡터를 생성하는 특성벡터생성 단계;상기 서버가 상기 제1 객체에 대응되는 제1 특성벡터 및 상기 제2 객체에 대응되는 제2 특성벡터의 유사도를 계산하는 유사도계산 단계; 및상기 서버가 상기 유사도, 상기 객체의 이동시간, 및 상기 객체의 이동방향을 이용하여 상기 차량매칭정보를 생성하는 매칭여부판단 단계를 포함하는 딥러닝 기반의 교통상태 추정방법."}
{"patent_id": "10-2023-0130033", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8 항에서,상기 교통상태 추정모델은 인코더-디코더 구조를 가지고 CNN 및 Transformer 중 적어도 어느 하나를 이용하고,진입 교통량 및 직진신호 길이에 따른 AIMSUN 시뮬레이션 결과인 로우데이터가 전처리되어 생성된 학습데이터를기반으로 학습되며, 교통신호정보를 더 이용하여 상기 비검지영역의 상기 교통상태를 추정하고,상기 영상분석 알고리즘은 Yolov5 및 JDE tracker 중 적어도 어느 하나를 포함하는 딥러닝 기반의 교통상태 추정방법."}
{"patent_id": "10-2023-0130033", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "교통상태 추정시스템은 복수의 카메라들 및 서버를 포함한다. 복수의 카메라들은 복수의 검지영역들 및 상기 복 수의 검지영역들 상의 차량을 촬영하여 원본영상을 생성한다. 비검지영역은 복수의 검지영역들 중 인접한 어느 두 검지영역들 사이의 영역이다. 서버에 비검지영역의 교통상태를 추정하기 위한 딥러닝(Deep Learning) 기반의 교통상태 추정모델이 저장된다. 서버는 복수의 카메라들과 통신할 수 있다. 복수의 검지영역들 각각은 교차로의 적어도 일부를 포함한다. 비검지영역의 교통상태는 비검지영역 내 차량의 수로 정의되는 교통밀도이다. 서버는 영상분석 알고리즘을 이용하여 복수의 원본영상들 각각에서 차량을 객체로 추출한다. 서버는 객체를 추적하여 차 량궤적정보를 생성한다. 서버는 객체의 특징을 추출하여 차량특징정보를 생성한다."}
{"patent_id": "10-2023-0130033", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 복수의 카메라들에서 수집된 영상들을 기초로 딥러닝 기반의 인공지능 모델을 이용하여 교통상태를 추정하는 시스템 및 방법에 관한 것이다. 구체적으로는, 복수의 카메라들에서 수집된 영상들을 기초로 영상이 수집되지 않는 영역의 교통상태를 추정하는 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0130033", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "C-ITS(Cooperative Intelligent Transportation Systems)는 협력적 지능형 교통 시스템을 의미하며, 구체적으 로 광범위한 도심 지역의 교통 시스템을 지능적으로 개선하는 기술 및 시스템을 의미한다. C-ITS는 도로의 안정 성, 효율성, 환경보호 등 다양한 측면에서 교통을 개선하는데 적용될 수 있다. 구체적으로 C-ITS를 이용하면, 교통상황 모니터링, 교통 흐름 관리, 사고 감지 및 예방, 운전자 지원, 스마트 도로 구축, 교통 혼잡 해소 등의 효과가 기대된다. 예를 들어, C-ITS를 이용하면 교통이 혼잡한 지역에서 신호 주기를 제한하고, 특정 도로로의 차량 유입을 제한함으로써 교통 효율을 개선할 수 있다. 또한, C-ITS를 통해 생성되는 교통 흐름, 차량 속도, 교통 사고 발생률 등에 관한 데이터는 교통정책 수립, 도로 개선, 교통 시뮬레 이션 등에 활용될 수 있다. 이러한 C-ITS를 구현하기 위해서는, 광범위한 도심 지역의 실시간 교통상태 데이터를 수집하거나 추정하는 기술 이 요구된다. 그러나, 현재의 교통상태 모니터링 기술로서는 영상이 수집되는 제한된 영역에 대한 교통상태만이 수집 가능하다. 따라서, C-ITS 고도화를 위해서는 영상이 수집되지 않는 영역에 대해 교통상태를 추정하는 기술 의 개발이 요구되고 있는 상황이다."}
{"patent_id": "10-2023-0130033", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상술한 종래의 문제점을 해결하기 위한 것으로, 검지영역을 촬영하여 생성된 영상을 이용하여, 비검 지영역의 교통상태를 추정할 수 있는 딥러닝 기반의 교통상태 추정 시스템 및 방법을 제공하는 것을 목적으로 한다. 또한, 본 발명은 AIMSUM 시뮬레이션 결과에 기반하여 학습된 교통상태 추정모델을 이용하는 딥러닝 기반의 교통 상태 추정 시스템 및 방법을 제공하는 것을 목적으로 한다. 또한, 본 발명은 서로 다른 카메라에서 촬영된 차량들이 서로 동일한 차량인지 판단하여 차량매칭정보를 생성하 고, 차량매칭정보를 이용하여 비검지영역의 교통상태를 추정하는 딥러닝 기반의 교통상태 추정 시스템 및 방법 을 제공하는 것을 목적으로 한다."}
{"patent_id": "10-2023-0130033", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템은 복수의 카메라들 및 서버를 포함할 수 있다. 복수의 카메라들은 복수의 검지영역들 및 상기 복수의 검지영역들 상의 차량을 촬영하여 복수의 원본영상들을 생성할 수 있다. 서버에는 상기 복수의 검지영역들 중 인접한 어느 두 검지영역들 사이의 영역인 비검지영역의 교통상태를 추정하기 위한 딥러닝(Deep Learning) 기반의 교통상태 추정모델이 저장될 수 있다. 서버는 상기 복 수의 카메라들과 통신할 수 있다. 상기 복수의 검지영역들 각각은 교차로의 적어도 일부를 포함할 수 있다. 상 기 비검지영역의 교통상태는 상기 비검지영역 내 차량의 수로 정의되는 교통밀도이다. 서버는 영상분석 알고리 즘을 이용하여 상기 복수의 원본영상들 각각에서 차량을 객체로 추출할 수 있다. 또한, 서버는 상기 객체를 추 적하여 차량궤적정보를 생성할 수 있다. 또한, 서버는 상기 객체의 특징을 추출하여 차량특징정보를 생성할 수 있다. 또한, 상기 서버는 상기 원본영상을 탑뷰영상으로 변환할 수 있다. 또한, 서버는 상기 탑뷰영상을 이용하여 공 간 내 차량의 수로 정의되는 교통밀도정보를 산출할 수 있다. 또한, 상기 서버는 상기 차량특징정보 중 적어도 일부를 포함하는 재인식특성을 이용하여 상기 복수의 원본영상 들 중 어느 하나에서 추출된 제1 객체 및 상기 복수의 원본영상들 중 다른 하나에서 추출된 제2 객체가 서로 동 일한지 여부를 판단하여 차량매칭정보를 생성할 수 있다. 상기 교통상태 추정모델은 상기 교통밀도정보 및 상기 차량매칭정보를 이용하여 상기 비검지영역의 교통상태를 추정할 수 있다. 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템의 상기 서버는 상기 객체에 대응되는 상기 재 인식특성을 기초로 특성벡터를 생성할 수 있다. 또한, 서버는 상기 제1 객체에 대응되는 제1 특성벡터 및 상기 제2 객체에 대응되는 제2 특성벡터의 유사도를 계산할 수 있다. 또한, 서버는 상기 유사도를 이용하여 상기 제1 객체 및 상기 제2 객체가 서로 동일한지 여부를 판단하여 상기 차량매칭정보를 생성할 수 있다. 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템에서 상기 서버는 상기 객체의 이동시간 및 상 기 객체의 이동방향을 더 이용하여 상기 차량매칭정보를 생성할 수 있다. 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템에서 상기 교통상태 추정모델은 인코더-디코더 구조를 가지고, CNN 및 Transformer 중 적어도 어느 하나를 이용할 수 있다. 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템에서 상기 교통상태 추정모델은 진입 교통량 및 직진신호 길이에 따른 AIMSUN 시뮬레이션 결과인 로우데이터가 전처리되어 생성된 학습데이터를 기반으로 학습 될 수 있다. 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템에서 상기 교통상태 추정모델은 교통신호정보를 더 이용하여 상기 비검지영역의 상기 교통상태를 추정할 수 있다. 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템에서 상기 영상분석 알고리즘은 Yolov5 및 JDE tracker 중 적어도 어느 하나를 포함할 수 있다. 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정방법에서 복수의 카메라들 및 서버를 이용될 수 있다. 상기 복수의 카메라들은 복수의 검지영역들 및 상기 복수의 검지영역 상의 차량을 촬영하여 복수의 원본영상들 을 생성할 수 있다. 상기 서버에 상기 복수의 검지영역들 중 인접한 어느 두 검지영역들 사이의 영역인 비검지 영역의 교통상태를 추정하기 위한 딥러닝(Deep Learning) 기반의 교통상태 추정모델이 저장될 수 있다. 상기 서 버는 상기 복수의 카메라들과 통신할 수 있다. 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정방법은 차량객체인식 단계, 교통밀도산출 단계, 매칭 정보생성 단계, 및 비검지영역추정 단계를 포함할 수 있다. 차량객체인식 단계에서 상기 서버는 영상분석 알고리즘을 이용하여 복수의 원본영상들에서 차량을 객체로 추출 할 수 있다. 또한, 상기 서버는 상기 객체를 추적하여 차량궤적정보를 생성할 수 있다. 또한, 상기 서버는 상기 객체의 특징을 추출하여 차량특징정보를 생성할 수 있다. 교통밀도산출 단계에서, 상기 서버는 상기 원본영상을 탑뷰영상으로 변환할 수 있다. 또한, 상기 서버는 상기 탑뷰영상을 이용하여 공간 내 차량의 수로 정의되는 교통밀도정보를 산출할 수 있다. 매칭정보생성 단계에서, 상기 서버는 상기 차량특징정보 중 적어도 일부를 포함하는 재인식특성을 이용하여 상 기 복수의 원본영상들 중 어느 하나에서 추출된 제1 객체 및 상기 복수의 원본영상들 중 다른 하나에서 추출된 제2 객체가 서로 동일한지 여부를 판단하여 차량매칭정보를 생성할 수 있다. 비검지영역추정 단계에서, 상기 서버는 상기 교통밀도정보 및 상기 차량매칭정보를 기초로 상기 교통상태 추정 모델을 이용하여 상기 비검지영역의 상기 교통상태를 추정할 수 있다. 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정방법의 상기 매칭정보생성 단계는 특성벡터생성 단계, 유사도계산 단계, 및 매칭여부판단 단계를 포함할 수 있다. 특성벡터생성 단계에서 상기 서버는 상기 객체에 대 응되는 상기 재인식특성을 기초로 특성벡터를 생성할 수 있다. 유사도계산 단계에서, 상기 서버는 상기 제1 객 체에 대응되는 제1 특성벡터 및 상기 제2 객체에 대응되는 제2 특성벡터의 유사도를 계산할 수 있다. 매칭여부 판단 단계에서, 상기 서버는 상기 유사도, 상기 객체의 이동시간, 및 상기 객체의 이동방향을 이용하여 상기 차 량매칭정보를 생성할 수 있다. 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정방법에서, 상기 교통상태 추정모델은 인코더-디코더 구조를 가지고 CNN 및 Transformer 중 적어도 어느 하나를 이용할 수 있다. 또한, 상기 교통상태 추정모델은 진 입 교통량 및 직진신호 길이에 따른 AIMSUN 시뮬레이션 결과인 로우데이터가 전처리되어 생성된 학습데이터를 기반으로 학습될 수 있다. 또한, 상기 교통상태 추정모델은 교통신호정보를 더 이용하여 상기 비검지영역의 상 기 교통상태를 추정할 수 있다. 상기 영상분석 알고리즘은 Yolov5 및 JDE tracker 중 적어도 어느 하나를 포함 할 수 있다."}
{"patent_id": "10-2023-0130033", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 검지영역을 촬영하여 생성된 영상을 이용하여, 비검지영역의 교통상태를 추정할 수 있는 딥러닝 기반의 교통상태 추정 시스템 및 방법을 제공할 수 있다. 본 발명의 일 실시예에 따르면, AIMSUM 시뮬레이션 결과에 기반하여 학습된 교통상태 추정모델을 이용하므로, 학습데이터 수집을 위한 비용 및 부담이 절감된 딥러닝 기반의 교통상태 추정 시스템 및 방법을 제공하는 것을 목적으로 한다. 본 발명의 일 실시예에 따르면, 서로 다른 카메라에서 촬영된 차량들이 서로 동일한 차량인지 판단하여 차량매 칭정보를 생성하고 차량매칭정보를 이용하여 비검지영역의 교통상태를 추정하므로, 더 정확하게 교통상태를 추 정할 수 있는 딥러닝 기반의 교통상태 추정 시스템 및 방법을 제공하는 것을 목적으로 한다."}
{"patent_id": "10-2023-0130033", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 첨부된 도면들을 참조하여 본 발명의 바람직한 실시예를 보다 상세하게 설명한다. 도면들에 있어서, 구성 요소들의 비율 및 치수는 기술적 내용의 효과적인 설명을 위해 과장된 것일 수 있다. \"포함하다\" 등의 용어는 명세서 상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 또한, 어떤 구성요소 \"상\"으로 기재된 경우, 해당 구성요소의 위 또는 아래를 의미하고, 반드시 중력 방향을 기 준으로 상측에 위치하는 것을 의미하는 것은 아니다. 또한, 어떤 구성요소가 다른 구성요소에 \"연결\" 또는 \"결합\"된다고 기재된 경우, 해당 구성요소가 다른 구성요 소에 직접적으로 연결 또는 결합되는 경우뿐만 아니라, 해당 구성요소가 또 다른 구성요소를 통해 간접적으로 연결 또는 결합되는 경우도 포함할 수 있다. 또한, 어떤 구성요소를 설명하는데 있어서 제1, 제2 등의 용어를 사용할 수 있지만, 이러한 용어는 해당 구성요 소를 다른 구성요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성요소의 본질이나 차례 또는 순서 등을 한정하고자 하는 것은 아니다. 도 1은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템(TSS)이 적용된 도로 및 교차로(JT)의 모습을 예시적으로 도시한 것이다. 도 2는 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템 (TSS)의 구성을 설명하기 위해 도시된 것이다. 도 1 및 도 2를 참조하면, 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템(TSS)은 복수의 카메 라들(CM) 및 서버(SV)를 포함할 수 있다. 복수의 카메라들(CM)은 복수의 검지영역들(DS) 및 복수의 검지영역들(DS) 상의 차량(VH)을 촬영하여 복수의 원 본영상들(CV)을 생성할 수 있다. 복수의 검지영역들(DS) 각각은 교차로(JT)의 적어도 일부를 포함할 수 있다. 서버(SV)에는 교통상태 추정모델(TM)이 저장될 수 있다. 교통상태 추정모델(TM)은 비검지영역의 교통상태(TS)를 추정하기 위한 것일 수 있다. 비검지영역(NS)은 복수의 검지영역들(DS) 중 인접한 어느 두 검지영역들(DS) 사이 의 영역일 수 있다. 비검지영역의 교통상태(TS)는 비검지영역(NS) 내 차량(VH)의 수로 정의되는 교통밀도이다. 비검지영역(NS) 및 검지영역(DS)의 교통상태를 이용하면, 광범위한 도심 지역의 교통혼잡 원인 분석 및 자율주 행차량의 운영전략 수립 등에 활용될 수 있다. 본 발명의 다른 실시예에서, 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템(TSS)에서 서버(SV)는 비검지영역(NS) 및 검지영역(DS)의 교통상태를 이용하여 검지 영역(DS) 및 비검지영역(NS)의 교통신호 및 가변속도 중 적어도 어느 하나를 제어할 수 있다. 도 3은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템(TSS)의 서버(SV)가 원본영상(CV)에서 차량(VH)을 객체(OB)로 추출하는 과정을 설명하기 위해 도시된 것이다. 도 4는 본 발명의 일 실시예에 따른 딥 러닝 기반의 교통상태 추정시스템(TSS)의 서버(SV)가 차량궤적정보(TD)를 생성하는 과정을 설명하기 위해 도시 된 것이다. 도 3 및 도 4를 참조하면, 서버(SV)는 영상분석 알고리즘을 이용하여 복수의 원본영상들(CV) 각각에서 차량(V H)을 객체(OB)로 추출할 수 있다. 또한, 서버(SV)는 객체(OB)를 추적하여 차량궤적정보(TD)를 생성할 수 있다. 또한, 서버(SV)는 객체(OB)의 특징을 추출하여 차량특징정보를 생성할 수 있다. 본 발명의 일 실시예에서, 영상 분석 알고리즘은 Yolov5 및 JDE tracker 중 적어도 어느 하나를 포함할 수 있다. 서버(SV)는 시점변환행렬을 이용하여 원본영상(CV)을 탑뷰영상(TV)으로 변환할 수 있다. 본 명세서에서 탑뷰영 상(TV)은 카메라(CM)가 검지영역(DS)에 수직한 방향으로 이격된 지점에서 검지영역(DS)을 내려다보며 촬영했을 때 생성되는 영상을 의미한다. 원본영상(CV)은 카메라(CM)가 검지영역(DS)을 사선으로 촬영하여 생성되는 영상 이므로, 차량(VH)의 속도를 측정하기에 적합하지 않다. 따라서, 서버(SV)는 원본영상(CV)을 탑뷰영상(TV)으로 변환하여, 차량(VH)의 속도를 측정할 수 있다. 또한, 서버(SV)는 탑뷰영상(TV)을 이용하여 공간 내 차량(VH)의 수로 정의되는 교통밀도정보를 산출할 수 있다. 서버(SV)는 차량특징정보 중 적어도 일부를 포함하는 재인식특성을 이용하여 제1 객체(OB1) 및 제2 객체(OB2)가 서로 동일한지 여부를 판단하여 차량매칭정보를 생성할 수 있다. 제1 객체(OB1)는 복수의 원본영상들(CV) 중 어 느 하나에서 추출된 객체일 수 있다. 제2 객체(OB2)는 복수의 원본영상들(CV) 중 다른 하나에서 추출된 객체일 수 있다. 차량매칭정보를 이용하면, 추정모델(TM)이 비검지영역의 교통상태(TS)를 더 정확하게 추정할 수 있다. 이에 대해서는 도 8 및 도 9에서 자세히 설명한다. 도 5는 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템(TSS)의 서버(SV)가 차량매칭정보를 생 성하는 과정을 설명하기 위해 도시된 것이다. 도 5를 참조하면, 본 발명의 일 실시예에서 서버(SV)는 객체(OB)에 대응되는 재인식특성을 기초로 특성벡터를 생성할 수 있다. 또한, 서버(SV)는 제1 객체(OB1)에 대응되는 제1 특성벡터 및 제2 객체(OB2)에 대응되는 제2 특성벡터의 유사도를 계산할 수 있다. 또한, 서버(SV)는 유사도를 이용하여 제1 객체(OB1) 및 제2 객체(OB2)가 서로 동일한지 여부를 판단하여 차량매칭정보를 생성할 수 있다. 본 발명의 일 실시예에서, 서버(SV)는 객체 (OB)의 이동시간 및 객체(OB)의 이동방향을 더 이용하여 차량매칭정보를 생성할 수 있다. 특성벡터의 유사도, 객체(OB)의 이동시간, 및 객체(OB)의 이동방향이 반영된 차량매칭정보를 이용하면, 추정모델(TM)이 비검지영역 의 교통상태(TS)를 더 정확하게 추정할 수 있다. 도 6은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템(TSS)의 교통상태 추정모델(TM)이 교통 밀도정보 및 차량매칭정보를 이용하여 비검지영역의 교통상태(TS)를 추정하는 과정을 설명하기 위해 도시된 것이다. 제1 이미지(IM1)는 교통밀도정보를 최대 교통밀도를 기준으로 정규화 한 후 시각화 한 것이다. 제1 이미지(IM 1)에서 가로축은 시간, 세로축은 공간을 의미한다. 각 픽셀의 색은 교통밀도에 대응된다. 빨강색은 교통밀도가 높음을 의미하고 파랑색 및 흰색은 교통밀도가 낮음을 의미한다. 즉, 제1 이미지(IM1)는 공간에 대한 시간별 교 통밀도를 측정하여 시각화한 것이다. 구체적으로, 도 6의 제1 이미지(IM1)에서 제1 영역(SC1) 및 제3 영역(SC3)은 검지영역(DS)에서의 시간별 교통밀 도를 의미한다. 제2 영역(SC2)은 비검지영역(NS)에서의 시간별 교통밀도를 의미한다. 비검지영역(NS)의 시간별 교통밀도는 측정이 아닌 추정의 대상이므로, 비검지영역(NS)의 모든 픽셀의 색깔은 흰색이다. 제2 이미지(IM2)는 차량매칭정보를 수치화한 후 시각화 한 것이다. 교통상태 추정모델(TM)은 교통밀도정보 및 차량매칭정보를 이용하여 비검지영역의 교통상태(TS)를 추정할 수 있 다. 본 발명의 일 실시예에서, 교통상태 추정모델(TM)은 교통신호정보를 더 이용하여 비검지영역의 교통상태 (TS)를 추정할 수 있다. 본 명세서에서, 교통신호정보란 신호등의 신호, 직진신호의 길이, 정지신호의 길이, 및 전체 신호의 주기 중 적어도 어느 하나를 포함할 수 있다. 교통신호정보가 반영되면, 교통상태 추정모델(TM)은 비검지영역의 교통상태(TS)를 더 정확하게 추정할 수 있다. 본 발명의 일 실시예에서, 교통상태 추정모델(TM)은 인코더-디코더 구조(CD)를 가질 수 있다. 인코더-디코더 구 조(CD)는 제한적인 데이터로 전체 데이터를 추정하는데 효과적이다. 또한, 교통상태 추정모델(TM)은 CNN(Convolutional Neural Network) 및 Transformer 중 적어도 어느 하나를 이용하는 것일 수 있다. 단, 본 발 명의 교통상태 추정모델(TM)이 이용하는 알고리즘이 이에 제한되는 것은 아니며, 교통상태 추정모델(TM)은 딥러 닝 알고리즘들 중 적어도 하나를 이용하는 것이면 충분하다. 도 7은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템(TSS)에서, 교통상태 추정모델(TM)의 학 습데이터가 생성되는 과정을 설명하기 위해 도시된 것이다. 본 발명의 일 실시예에서, 교통상태 추정모델(TM)은 학습데이터를 기반으로 학습될 수 있다. 학습데이터는 진입 교통량 및 직진신호 길이에 따른 AIMSUN 시뮬레이션 결과인 로우데이터(RD)가 전처리 되어 생성된 것일 수 있다. 로우데이터(RD)는 AIMSUN 시뮬레이션 결과로 생성된 차량궤적정보일 수 있다. AIMSUN은 도로 및 교통 시 스템 시뮬레이션 소프트웨어이다. AIMSUN은 도로 네트워크, 교통 흐름 및 도로 사용 패턴 등과 관련된 다양한 교통 시나리오를 모델링하고 시뮬레이션하는 데 사용될 수 있다. 전처리는 교통밀도정보를 셀 별로 수치화한 후, 수치화된 값들을 최대교통밀도를 기준으로 정규화하는 과정을 포함할 수 있다. 또한, 전처리는 차량매칭정보를 0.1n(n은 해당 셀에 포함된 차량의 대수)로 수치화하는 과정을 포함할 수 있다. 도 7을 참조하면, 로우데이터(RD)는 진입 교통량이 500veh/h, 1000veh/h, 및 1500veh/h 중 어느 하나이고 직진 신호 길이가 50초, 75초, 및 100초 중 어느 하나인 조건에서 AIMSUN 시뮬레이션 결과로 생성된 차량궤적정보일 수 있다. 본 출원에서, veh/h는 단위 시간당 진입하는 차량의 수를 의미할 수 있다. 직진신호는 신호등에서 직 진 신호가 유지되는 시간을 의미할 수 있다. 로우데이터(RD)를 참조하면, 진입 교통량이 크고 직진신호 길이가 짧을수록 차량 대기행렬이 길어지는 것을 확인할 수 있다. 도 8 및 도 9는 사용된 인공지능 모델 및 입력 데이터에 따라, 교통상태 추정모델의 성능을 평가하여 도시한 것 이다. 도 8에서 Kalman filter(traj) 항목은 차량궤적정보(TD)를 기초로 칼만필터(Kalman filter)에 기반한 교통상태 추정모델을 이용하여 교통상태를 추정한 결과의 정확도를 평가한 것이다. 도 8에서 CNN(traj) 항목은 차량궤적 정보(TD)를 기초로 CNN에 기반한 교통상태 추정모델을 이용하여 교통상태를 추정한 결과의 정확도를 평가한 것 이다. 도 8에서 CNN(traj+ID) 항목은 차량궤적정보(TD) 및 차량매칭정보를 기초로, CNN에 기반한 교통상태 추정 모델을 이용하여 교통상태를 추정한 결과의 정확도를 평가한 것이다. 도 8에서 Transformer(traj) 항목은 차량 궤적정보(TD)를 기초로, Transformer에 기반한 교통상태 추정모델을 이용하여 교통상태를 추정한 결과의 정확도 를 평가한 것이다. 도 8에서 Transformer(traj+ID) 항목은 차량궤적정보(TD) 및 차량매칭정보를 기초로, Transformer에 기반한 교통상태 추정모델을 이용하여 교통상태를 추정한 결과의 정확도를 평가한 것이다. 교통상태를 추정한 결과의 정확도를 평가하기 위해, 드론으로 취득한 실제 차량궤적정보가 이용되었다. 정확도 를 평가한 결과는 RMSE(Root Mean Square Error; 평균 제곱근 오차) 교통밀도 오차 및 MAE(Mean AbsoluteError; 평균 절대 오차) 교통밀도 오차로 수치화 되었다. 도 8을 참조하면, CNN 및 Transformer를 포함하는 딥러닝 기반의 교통상태 추정모델을 이용한 교통상태 추정 결 과가 Kalman filter 기반의 교통상태 추정모델을 이용한 교통상태 추정 결과 보다 비슷하거나 더 정확한 것을 알 수 있다. 이하, 그 이유에 대해 자세히 설명한다. 구체적으로, Kalman filter 기반의 교통상태 추정모델은 특정 셀 보다 전면에 위치한 셀에 차량이 없는 경우, 특정 셀에 포함되는 차량들이 비혼잡상태에서 자유속도로만 이동한다고 가정하기 때문에, 교통상태 추정의 정확 도가 제한되는 문제가 있다. 반면, CNN 기반의 교통상태 추정모델은 (특정 셀에 포함되는 차량들이 비혼잡상태에서 자유속도로만 이동한다는) 가정 없이 입력 데이터만을 이용하여 교통 상태를 추정하므로 상술한 문제가 발생하지 않는다. 다 만, CNN 기반의 교통상태 추정모델은 특정 셀 보다 전면에 위치한 셀에 차량이 없는 경우, 차량이 존재하는 것 처럼 추정하므로, 교통상태 추정의 정확도가 제한되는 문제가 있다. 이 때문에, Kalman filter(traj)은 차량궤적정보(TD)를 기초로 칼만필터(Kalman filter)에 기반한 교통상태 추 정모델을 이용하여 교통상태를 추정하는 경우와 CNN(traj)은 차량궤적정보(TD)를 기초로 CNN에 기반한 교통상태 추정모델을 이용하여 교통상태를 추정하는 경우의 추정 정확도가 유사하게 나타난다. 상술한 CNN 기반의 교통상태 추정모델의 문제를 해결하기 위해, 차량매칭정보가 CNN 기반의 교통상태 추정모델 의 입력 데이터로 활용된다. 차량매칭정보가 CNN 기반의 교통상태 추정모델의 입력 데이터로 활용되면, 차량이 존재하는 범위가 효과적으로 식별되므로, 상술한 CNN 기반의 교통상태 추정모델의 문제가 획기적으로 개선된다. 차량매칭정보가 Transformer 기반의 교통상태 추정모델의 입력 데이터로 활용되는 경우에도 이러한 효과가 동일 하게 나타난다. 또한, CNN(traj+ID) 및 Transformer(traj+ID)에 대응되는 정확도 평가를 참조하면, 차량매칭정보가 교통상태 추 정모델에 대한 입력 데이터로 활용되는 경우, 교통상태 추정 결과가 더 정확한 것을 알 수 있다. 또한, CNN 기반의 교통상태 추정모델을 이용한 교통상태 추정 결과 보다 Transformer 기반의 교통상태 추정모델 을 이용한 교통상태 추정 결과가 더 정확한 것을 알 수 있다. 이는 Transformer 기반의 교통상태 추정모델이 CNN 기반의 교통상태 추정모델 보다 더 먼 거리에 위치한 픽셀들 간의 연관성을 더 효율적으로 추출할 수 있기 때문이다. 도 10은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정방법의 순서도를 도시한 것이다. 도 10을 참조하면, 본 발명의 일 실시예에 따른 딥 러닝 기반의 교통상태 추정방법(TSM)은 복수의 카메라들(CM) 및 서버(SV)를 이용할 수 있다. 복수의 카메라들(CM)은 복수의 검지영역들(DS) 및 복수의 검지영역 상의 차량(VH)을 촬영하여 복수의 원본영상 들(CV)을 생성할 수 있다. 서버(SV)에 비검지영역의 교통상태(TS)를 추정하기 위한 딥러닝(Deep Learning) 기반의 교통상태 추정모델(TM) 이 저장될 수 있다. 비검지영역(NS)은 복수의 검지영역들(DS) 중 인접한 어느 두 검지영역들(DS) 사이의 영역일 수 있다. 서버(SV)는 복수의 카메라들(CM)과 통신할 수 있다. 딥 러닝 기반의 교통상태 추정방법(TSM)은 차량객체인식 단계(S10), 교통밀도산출 단계(S20), 매칭정보생성 단 계(S30), 및 비검지영역추정 단계(S40)를 포함할 수 있다. 차량객체인식 단계(S10)에서, 서버(SV)는 영상분석 알고리즘을 이용하여 복수의 원본영상들(CV)에서 차량(VH)을 객체(OB)로 추출할 수 있다. 또한, 서버(SV)는 객체(OB)를 추적하여 차량궤적정보(TD)를 생성할 수 있다. 또한, 서버(SV)는 객체(OB)의 특징을 추출하여 차량특징정보를 생성할 수 있다. 교통밀도산출 단계(S20)에서, 서버(SV)는 원본영상(CV)을 탑뷰영상(TV)으로 변환할 수 있다. 또한, 서버(SV)는 탑뷰영상(TV)을 이용하여 공간 내 차량(VH)의 수로 정의되는 교통밀도정보를 산출할 수 있다. 매칭정보생성 단계(S30)에서, 서버(SV)는 차량특징정보 중 적어도 일부를 포함하는 재인식특성을 이용하여 제1 객체(OB1) 및 제2 객체(OB2)가 서로 동일한지 여부를 판단하여 차량매칭정보를 생성할 수 있다. 제1 객체(OB1) 는 복수의 원본영상들(CV) 중 어느 하나에서 추출된 객체(OB)일 수 있다. 제2 객체(OB2)는 복수의 원본영상들 (CV) 중 다른 하나에서 추출된 객체(OB)일 수 있다.비검지영역추정 단계(S40)에서, 서버(SV)는 교통밀도정보 및 차량매칭정보를 기초로 교통상태 추정모델(TM)을 이용하여 비검지영역의 교통상태(TS)를 추정할 수 있다. 도 11은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정방법에서, 매칭정보생성 단계(S30)의 세부 순 서도를 도시한 것이다. 도 11을 참조하면, 본 발명의 일 실시예에서, 매칭정보생성 단계(S30)는 특성벡터생성 단계(S300), 유사도계산 단계(S310), 및 매칭여부판단 단계(S320)를 포함할 수 있다. 특성벡터생성 단계(S300)에서, 서버(SV)는 객체 (OB)에 대응되는 재인식특성을 기초로 특성벡터를 생성할 수 있다. 유사도계산 단계(S310)에서, 서버(SV)는 제1 객체(OB1)에 대응되는 제1 특성벡터 및 제2 객체(OB2)에 대응되는 제2 특성벡터의 유사도를 계산할 수 있다. 매 칭여부판단 단계(S320)에서, 서버(SV)는 유사도, 객체(OB)의 이동시간, 및 객체(OB)의 이동방향을 이용하여 차 량매칭정보를 생성할 수 있다. 본 발명의 일 실시예에서, 교통상태 추정모델(TM)은 인코더-디코더 구조(CD)를 가지고 CNN 및 Transformer 중 적어도 어느 하나를 이용할 수 있다. 또한, 교통상태 추정모델(TM)은 학습데이터를 기반으로 학습될 수 있다. 학습데이터는 진입 교통량 및 직진신호 길이에 따른 AIMSUN 시뮬레이션 결과인 로우데이터(RD)가 전처리되어 생 성된 것일 수 있다. 또한, 교통상태 추정모델(TM)은 교통신호정보를 더 이용하여 비검지영역의 교통상태(TS)를 추정할 수 있다. 또한, 영상분석 알고리즘은 Yolov5 및 JDE tracker 중 적어도 어느 하나를 포함할 수 있다. 그 외의 내용은 도 1 내지 9에서 설명한 것과 실질적으로 동일하므로, 생략한다. 실시 예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특허 청구의 범위에 기재된 본 발 명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다. 또한 본 발명에 개시된 실시 예는 본 발명의 기술 사상을 한정하기 위한 것이 아니고, 하기의 특허 청구의 범위 및 그와 동등한 범위 내에 있는 모든 기술 사상은 본 발명의 권리범위에 포함되는 것으로 해 석되어야 할 것이다."}
{"patent_id": "10-2023-0130033", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템이 적용된 도로 및 교차로의 모습을 예 시적으로 도시한 것이다. 도 2는 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템의 구성을 설명하기 위해 도시된 것이다. 도 3은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템의 서버가 원본영상에서 차량을 객체로 추출하는 과정을 설명하기 위해 도시된 것이다. 도 4는 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템의 서버가 차량궤적정보를 생성하는 과 정을 설명하기 위해 도시된 것이다. 도 5는 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템의 서버가 차량매칭정보를 생성하는 과 정을 설명하기 위해 도시된 것이다. 도 6은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템의 교통상태 추정모델이 교통밀도정보 및 차량매칭정보를 이용하여 비검지영역의 교통상태를 추정하는 과정을 설명하기 위해 도시된 것이다. 도 7은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정시스템에서, 교통상태 추정모델의 학습데이터 가 생성되는 과정을 설명하기 위해 도시된 것이다. 도 8 및 도 9는 사용된 인공지능 모델 및 입력 데이터에 따라, 교통상태 추정모델의 성능을 평가하여 도시한 것 이다. 도 10은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정방법의 순서도를 도시한 것이다. 도 11은 본 발명의 일 실시예에 따른 딥러닝 기반의 교통상태 추정방법에서, 매칭정보생성 단계의 세부 순서도 를 도시한 것이다."}
