{"patent_id": "10-2017-0072746", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2018-0134683", "출원번호": "10-2017-0072746", "발명의 명칭": "인공지능 이동 로봇의 제어 방법", "출원인": "엘지전자 주식회사", "발명자": "노동기"}}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "복수의 로컬(local) 맵(map)을 포함하는 내비게이션(navigation) 맵에 기반하여 본체를 이동시키는 주행부;상기 이동 중에 상기 복수의 로컬 맵에 대응하는 영역들에서 복수의 영상을 획득하는 영상획득부; 상기 영상획득부가 획득하는 복수의 영상을 저장하는 저장부; 및,상기 복수의 로컬 맵 중 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵의 속성을 인식하고, 인식 결과를상기 저장부에 저장하도록 제어하며, 상기 복수의 로컬 맵 중에서, 인접한 소정 개수의 로컬 맵으로 구성되는의미 맵을 생성하고, 상기 의미 맵에 포함되는 로컬 맵들의 속성 인식 결과들에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 인식하는 제어부;를 포함하는 이동 로봇."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제어부는,상기 로컬 맵에서 획득된 N개의 영상에 기초하여 상기 로컬 맵에 대응하는 영역을 인식하며, 상기 로컬 맵에서 획득된 N개의 영상에서, 사물에 대응하는 이미지를 추출하고, 상기 추출된 이미지에기초하여, 상기 로컬 맵에 대응하는 영역에 존재하는 사물을 인식하며,상기 로컬 맵에 대응하는 영역의 인식 결과와 사물 인식 결과를 통합하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,장애물을 감지하는 하나 이상의 센서를 구비하는 센서부;를 더 포함하고,상기 제어부는, 상기 복수의 영상 중 일부 영상을 소정 기준에 따라 필터링(filtering)하며,상기 소정 기준은, 상기 센서부의 장애물 감지 여부 조건을 포함하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,장애물을 감지하는 하나 이상의 센서를 구비하는 센서부;를 더 포함하고,상기 제어부는, 소정 기준에 따라 촬영 여부를 결정하며,상기 소정 기준은, 상기 센서부의 장애물 감지 여부 조건을 포함하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 제어부는, 상기 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵에 대응하는 영역에서는 추가 촬영을하지 않도록 상기 영상획득부를 제어하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 제어부는, 상기 복수의 영상을 획득 시 위치 정보 및 방향 정보와 연계하여 저장하도록 제어하는 것을 특징으로 하는 이동 로봇.공개특허 10-2018-0134683-3-청구항 7 제1항에 있어서,상기 다른 방향으로 촬영된 N개의 영상은,소정 위치에서 회전하며 촬영되는 N개의 영상인 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 주행부는, 제1 방향으로 지그재그 패턴으로 주행하는 1회차 주행과 상기 제1 방향에 수직하는 제2 방향으로 지그재그 패턴으로 주행하는 2회차 주행하도록 상기 본체를 이동시키는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 다른 방향으로 촬영된 N개의 영상은,상기 1회차 주행 시 획득되는 영상과 상기 2회차 주행 시 획득되는 영상을 포함하는 것을 특징으로 하는 이동로봇."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,상기 제어부는,머신 러닝(machine learning)으로 기학습된 데이터에 기초하여, 상기 로컬 맵에서 획득된 N개의 영상을 분류하여 상기 로컬 맵의 속성을 판별하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,상기 제어부는,상기 인식된 복수의 인식 결과에서, 인식 결과의 빈도수, 신뢰값, 신뢰값들의 평균값 중 적어도 하나에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 판별하는 것을 특징으로 하는 이동 로봇."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "복수의 로컬(local) 맵(map)을 포함하는 내비게이션(navigation) 맵에 기반하여 이동하는 단계;상기 이동 중에, 영상획득부를 통하여, 상기 복수의 로컬 맵에 대응하는 영역들에서 복수의 영상을 획득하는 단계; 상기 복수의 로컬 맵 중 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵의 속성을 인식하는 단계; 상기 인식 결과를 저장하는 단계;상기 복수의 로컬 맵 중에서, 인접한 소정 개수의 로컬 맵으로 구성되는 의미 맵을 생성하는 단계; 및,상기 의미 맵에 포함되는 로컬 맵들의 속성 인식 결과들에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 인식하는 단계;를 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 로컬 맵의 속성을 인식하는 단계는,상기 로컬 맵에서 획득된 N개의 영상에 기초하여 상기 로컬 맵에 대응하는 영역을 인식하는 단계;공개특허 10-2018-0134683-4-상기 로컬 맵에서 획득된 N개의 영상에서, 사물에 대응하는 이미지를 추출하는 단계;상기 추출된 이미지에 기초하여, 상기 로컬 맵에 대응하는 영역에 존재하는 사물을 인식하는 단계; 및,상기 로컬 맵에 대응하는 영역의 인식 결과와 사물 인식 결과를 통합하는 단계;를 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서,상기 복수의 영상 중 일부 영상을 소정 기준에 따라 필터링(filtering)하는 단계;를 더 포함하고,상기 소정 기준은, 센서부의 장애물 감지 여부 조건을 포함하는 것을 특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12항에 있어서,소정 기준에 따라 촬영 여부를 결정하는 단계;를 더 포함하고,상기 소정 기준은, 센서부의 장애물 감지 여부 조건을 포함하는 것을 특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제12항에 있어서,상기 복수의 영상을 획득하는 단계는, 상기 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵에 대응하는 영역에서는 추가 촬영을 하지 않는 것을특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제12항에 있어서,상기 복수의 영상을 획득 시의 위치 정보, 방향 정보, 주행 라인 정보, 로컬 맵 정보와 연계하여 저장하는단계;를 더 포함하는 이동 로봇의 제어방법."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제12항에 있어서,상기 다른 방향으로 촬영된 N개의 영상은,소정 위치에서 회전하며 촬영되는 N개의 영상인 것을 특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제12항에 있어서,상기 이동 단계는,제1 방향으로 지그재그 패턴으로 주행하는 1회차 주행과 상기 제1 방향에 수직하는 제2 방향으로 지그재그 패턴으로 주행하는 2회차 주행을 포함하는 것을 특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서,상기 다른 방향으로 촬영된 N개의 영상은,상기 1회차 주행 시 획득되는 영상과 상기 2회차 주행 시 획득되는 영상을 포함하는 것을 특징으로 하는 이동로봇의 제어방법.공개특허 10-2018-0134683-5-청구항 21 제12항에 있어서,상기 로컬 맵의 속성을 인식하는 단계는,머신 러닝(machine learning)으로 기학습된 데이터에 기초하여, 상기 로컬 맵에서 획득된 N개의 영상을 분류하여 상기 로컬 맵의 속성을 판별하는 것을 특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2017-0072746", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제12항에 있어서,상기 의미 맵에 대응하는 영역의 최종 속성을 인식하는 단계는,상기 인식된 복수의 인식 결과에서, 인식 결과의 빈도수, 신뢰값, 신뢰값들의 평균값 중 적어도 하나에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 판별하는 것을 특징으로 하는 이동 로봇의 제어방법."}
{"patent_id": "10-2017-0072746", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 측면에 따른 이동 로봇은, 복수의 로컬(local) 맵(map)을 포함하는 내비게이션(navigation) 맵에 기반하여 본체를 이동시키는 주행부, 이동 중에 복수의 로컬 맵에 대응하는 영역들에서 복수의 영상을 획득하는 영상획득부, 영상획득부가 획득하는 복수의 영상을 저장하는 저장부, 및, 복수의 로컬 맵 중 다른 방향으로 촬영 된 N개의 영상이 획득된 로컬 맵의 속성을 인식하고, 인식 결과를 저장부에 저장하도록 제어하며, 복수의 로컬 맵 중에서, 인접한 소정 개수의 로컬 맵으로 구성되는 의미 맵을 생성하고, 의미 맵에 포함되는 로컬 맵들의 속 성 인식 결과들에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 인식하는 제어부를 포함함으로써, 영 역들의 속성을 효과적으로 인식할 수 있다."}
{"patent_id": "10-2017-0072746", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 이동 로봇 및 그 제어방법에 관한 것으로서, 더욱 상세하게는 맵(map) 내 영역들의 속성을 인식할 수 있는 이동 로봇 및 그 제어 방법에 관한 것이다."}
{"patent_id": "10-2017-0072746", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "로봇은 산업용으로 개발되어 공장 자동화의 일 부분을 담당하여 왔다. 최근에는 로봇을 응용한 분야가 더욱 확 대되어, 의료용 로봇, 우주 항공 로봇 등이 개발되고, 일반 가정에서 사용할 수 있는 가정용 로봇도 만들어지고 있다. 이러한 로봇 중에서 자력으로 주행이 가능한 것을 이동 로봇이라고 한다. 가정에서 사용되는 이동 로봇의 대표적인 예는 로봇 청소기로, 로봇 청소기는 일정 영역을 스스로 주행하면서, 주변의 먼지 또는 이물질을 흡입함으로써, 해당 영역을 청소하는 기기이다. 종래의 이동 로봇은 영역을 구분하지 않고 현재 위치를 기준으로 이동하며 청소를 수행하고, 직선 주행중에 장 애물이 존재하는 경우 방향을 전환하는 방식으로 주행함에 따라 특정 영역을 반복하여 청소하거나 이동 후 초기 위치로 복귀하지 못하는 문제점이 있었다. 그에 따라, 기저장된 맵을 이용하여 이동 로봇이 특정 영역에 대한 청소를 수행하도록 하고 중복 청소를 방지하 도록 하고 있다. 그러나 맵을 생성하는데 있어서, 이동 로봇은 벽 추정(월팔로윙)을 통해 이동하며 주행구역에 대한 외곽선을 추 출할 뿐이므로 모두 연결되어 있는 복수의 실(룸)을 각각의 영역으로 구분해야할 필요성이 있다. 한편. 사용자는 가정 내 공간을 침실, 옷장, 거실, 서재 등 그 용도에 따라 속성으로 구분하는 것이 통상적이다. 따라서, 영역 구분 후 사용자가 구분된 영역을 이용하여 이동 로봇을 편리하게 제어할 수 있도록, 이동 로봇이 사용자와 유사한 방식으로 영역들의 속성을 인식하는 방안이 요구되고 있다."}
{"patent_id": "10-2017-0072746", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은, 주행 구역 내 복수의 영역들의 속성을 인식할 수 있는 이동 로봇 및 그 제어방법을 제공함에 있다. 본 발명의 목적은, 영역 속성 인식 결과를 편리하게 이용할 수 있어, 사용자의 편의성을 향상할 수 있는 이동 로봇 및 그 제어방법을 제공함에 있다. 본 발명의 목적은, 머신 러닝에 기반하여 영역들의 속성을 정확하게 인식할 수 있는 이동 로봇 및 그 제어방법 을 제공함에 있다."}
{"patent_id": "10-2017-0072746", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇은, 복수의 로컬(local) 맵(map)을 포함하는 내비게이션(navigation) 맵에 기반하여 본체를 이동시키는 주행부, 이동 중에 복수의 로컬 맵에 대응 하는 영역들에서 복수의 영상을 획득하는 영상획득부, 영상획득부가 획득하는 복수의 영상을 저장하는 저장부, 및, 복수의 로컬 맵 중 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵의 속성을 인식하고, 인식 결과를 저 장부에 저장하도록 제어하며, 복수의 로컬 맵 중에서, 인접한 소정 개수의 로컬 맵으로 구성되는 의미 맵을 생 성하고, 의미 맵에 포함되는 로컬 맵들의 속성 인식 결과들에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 인식하는 제어부를 포함함으로써, 영역들의 속성을 효과적으로 인식할 수 있다. 또한, 상기 또는 다른 목적을 달성하기 위해 본 발명의 일 측면에 따른 이동 로봇의 제어방법은, 복수의 로컬 맵을 포함하는 내비게이션 맵에 기반하여 이동하는 단계; 이동 중에, 영상획득부를 통하여, 복수의 로컬 맵에 대응하는 영역들에서 복수의 영상을 획득하는 단계, 복수의 로컬 맵 중 다른 방향으로 촬영된 N개의 영상이 획 득된 로컬 맵의 속성을 인식하는 단계, 인식 결과를 저장하는 단계, 복수의 로컬 맵 중에서, 인접한 소정 개수 의 로컬 맵으로 구성되는 의미 맵을 생성하는 단계, 및, 의미 맵에 포함되는 로컬 맵들의 속성 인식 결과들에 기초하여, 의미 맵에 대응하는 영역의 최종 속성을 인식하는 단계를 포함할 수 있다."}
{"patent_id": "10-2017-0072746", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시 예들 중 적어도 하나에 의하면, 이동 로봇이, 주행 구역 내 복수의 영역들의 속성을 인식할 수 있다. 또한, 본 발명의 실시 예들 중 적어도 하나에 의하면, 영역 속성 인식 결과를 편리하게 이용할 수 있어, 사용자 의 편의성을 향상할 수 있다. 또한, 본 발명의 실시 예들 중 적어도 하나에 의하면, 머신 러닝에 기반하여 영역들의 속성을 정확하게 인식할 수 있다. 한편, 그 외의 다양한 효과는 후술될 본 발명의 실시 예에 따른 상세한 설명에서 직접적 또는 암시적으로 개시 될 것이다."}
{"patent_id": "10-2017-0072746", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부한 도면을 참조하여 본 발명의 실시예를 상세하게 설명한다. 그러나 본 발명이 이러한 실시예에 한정되는 것은 아니며 다양한 형태로 변형될 수 있음은 물론이다. 도면에서는 본 발명을 명확하고 간략하게 설명하기 위하여 설명과 관계 없는 부분의 도시를 생략하였으며, 명세 서 전체를 통하여 동일 또는 극히 유사한 부분에 대해서는 동일한 도면 참조부호를 사용한다. 한편, 이하의 설명에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 단순히 본 명세서 작성의 용이함만 이 고려되어 부여되는 것으로서, 그 자체로 특별히 중요한 의미 또는 역할을 부여하는 것은 아니다. 따라서, 상 기 \"모듈\" 및 \"부\"는 서로 혼용되어 사용될 수도 있다. 본 발명의 일 실시예에 따른 이동 로봇은 바퀴 등을 이용하여 스스로 이동이 가능한 로봇을 의미하고, 가 정 도우미 로봇 및 로봇 청소기 등이 될 수 있다. 이하에서는, 도면들을 참조하여, 이동 로봇 중 청소 기능을 가지는 로봇 청소기를 예로 들어 설명하나, 본 발명은 이에 한정되지 않는다. 도 1은 본 발명의 일 실시예에 따른 이동 로봇 및 이동 로봇을 충전시키는 충전대를 도시한 사시도이고, 도 2는 도 1에 도시된 이동 로봇의 상면부를 도시한 도이며, 도 3은 도 1에 도시된 이동 로봇의 정면부를 도시한 도이 고, 도 4는 도 1에 도시된 이동 로봇의 저면부를 도시한 도이다. 도 5는 본 발명의 일 실시예에 따른 이동 로봇의 주요 구성들 간의 제어관계를 도시한 블록도이다. 도 1 내지 도 5를 참조하면, 이동 로봇은, 본체와, 본체 주변의 영상을 획득하는 영상획득부 를 포함한다. 이하, 본체의 각부분을 정의함에 있어서, 주행구역 내의 천장을 향하는 부분을 상면부 (도 2 참조)로 정의하고, 주행구역 내의 바닥을 향하는 부분을 저면부(도 4 참조)로 정의하며, 상면부와 저면부 사이에서 본체의 둘레를 이루는 부분 중 주행방향을 향하는 부분을 정면부(도 3 참조)라고 정의한다. 이동 로봇은 본체를 이동시키는 주행부를 포함한다. 주행부는 본체를 이동시키는 적 어도 하나의 구동 바퀴를 포함한다. 주행부는 구동 바퀴에 연결되어 구동 바퀴를 회전시키는 구 동 모터(미도시)를 포함한다. 구동 바퀴는 본체의 좌, 우 측에 각각 구비될 수 있으며, 이하, 각각 좌륜(136(L))과 우륜(136(R))이라고 한다. 좌륜(136(L))과 우륜(136(R))은 하나의 구동 모터에 의해 구동될 수도 있으나, 필요에 따라 좌륜(136(L))을 구 동시키는 좌륜 구동 모터와 우륜(136(R))을 구동시키는 우륜 구동 모터가 각각 구비될 수도 있다. 좌륜(136 (L))과 우륜(136(R))의 회전 속도에 차이를 두어 좌측 또는 우측으로 본체의 주행방향을 전환할 수 있다. 본체의 저면부에는 공기의 흡입이 이루어지는 흡입구(110h)가 형성될 수 있으며, 본체 내에는 흡입구 (110h)를 통해 공기가 흡입될 수 있도록 흡입력을 제공하는 흡입장치(미도시)와, 흡입구(110h)를 통해 공기와 함께 흡입된 먼지를 집진하는 먼지통(미도시)이 구비될 수 있다. 본체는 이동 로봇을 구성하는 각종 부품들이 수용되는 공간을 형성하는 케이스를 포함할 수 있 다. 케이스에는 먼지통의 삽입과 탈거를 위한 개구부가 형성될 수 있고, 개구부를 여닫는 먼지통 커버 가 케이스에 대해 회전 가능하게 구비될 수 있다. 흡입구(110h)를 통해 노출되는 솔들을 갖는 롤형의 메인 브러시와, 본체의 저면부 전방측에 위치하며, 방사상으로 연장된 다수개의 날개로 이루어진 솔을 갖는 보조 브러시가 구비될 수 있다. 이들 브러시(134, 135)들의 회전에 의해 주행구역 내 바닥으로부터 먼지들이 분리되며, 이렇게 바닥으로부터 분리된 먼지들은 흡입구(110h)를 통해 흡입되어 먼지통에 모인다. 배터리는 구동 모터뿐만 아니라, 이동 로봇의 작동 전반에 필요한 전원을 공급한다. 배터리가 방전될 시, 이동 로봇은 충전을 위해 충전대로 복귀하는 주행을 실시할 수 있으며, 이러한 복귀 주행 중, 이동 로봇은 스스로 충전대의 위치를 탐지할 수 있다. 충전대는 소정의 복귀 신호를 송출하는 신호 송출부(미도시)를 포함할 수 있다. 복귀 신호는 초음파 신호 또는 적외선 신호일 수 있으나, 반드시 이에 한정되어야하는 것은 아니다. 이동 로봇은 복귀 신호를 수신하는 신호 감지부(미도시)를 포함할 수 있다. 충전대는 신호 송출부를 통해 적외선 신호를 송출하고, 신호 감지부는 적외선 신호를 감지하는 적외선 센서를 포함할 수 있다. 이동 로봇은 충전대로부터 송출된 적외선 신호에 따라 충전대의 위치로 이동하여 충전대와 도킹 (docking)한다. 이러한 도킹에 의해 이동 로봇의 충전 단자와 충전대의 충전 단자 간에 충 전이 이루어진다. 영상획득부는 주행구역을 촬영하는 것으로, 디지털 카메라를 포함할 수 있다. 디지털 카메라는 적어도 하 나의 광학렌즈와, 광학렌즈를 통과한 광에 의해 상이 맺히는 다수개의 광다이오드(photodiode, 예를들어, pixel)를 포함하여 구성된 이미지센서(예를들어, CMOS image sensor)와, 광다이오드들로부터 출력된 신호를 바 탕으로 영상을 구성하는 디지털 신호 처리기(DSP: Digital Signal Processor)를 포함할 수 있다. 디지털 신호 처리기는 정지영상은 물론이고, 정지영상으로 구성된 프레임들로 이루어진 동영상을 생성하는 것도 가능하다. 바람직하게, 영상획득부는, 본체 전방의 영상을 획득하도록 구비되는 전면 카메라(120a)와 본체(11 0)의 상면부에 구비되어 주행구역 내의 천장에 대한 영상을 획득하는 상부 카메라(120b)를 구비할 수 있다. 하 지만, 영상획득부가 포함하는 카메라의 위치와 촬영범위가 반드시 이에 한정되어야 하는 것은 아니다. 본 실시예의 경우, 이동 로봇의 일부 부위(ex, 전방, 후방, 저면)에 카메라가 설치되어 있으며, 청소 시에 촬상 영상을 지속적으로 획득할 수 있다. 이러한 카메라는 촬영 효율을 위해 각 부위별로 여러 개가 설치될 수도 있 다. 카메라에 의해 촬상된 영상은 해당 공간의 속성 인식, 해당 공간에 존재하는 먼지, 머리카락, 바닥 등과 같 은 사물의 종류 인식, 청소 여부, 또는 청소 시점을 확인하는데 사용할 수 있다. 전면 카메라(120a)는 이동 로봇의 주행 방향 전면의 소정 각도 범위 내에서 영상을 촬영할 수 있고, 주행 방향 전면에 존재하는 장애물 또는 청소 영역의 상황을 촬영할 수 있다. 본 발명의 일 실시예에 따르면, 상기 영상획득부는 본체 주변을 연속적으로 촬영하여 복수의 영상을 획득할 수 있고, 획득된 복수의 영상은 저장부에 저장될 수 있다. 이동 로봇은 복수의 영상을 이용하여 영역, 사물 인식의 정확성을 높이거나, 복수의 영상 중 하나 이상의 영상을 선택하여 효과적인 데이터를 사용함으로써 인식의 정확성을 높일 수 있다. 또한, 이동 로봇은 이동 로봇의 동작, 상태와 관련된 각종 데이터를 센싱하는 센서들을 포함하는 센서부 를 포함할 수 있다. 예를 들어, 상기 센서부는 전방의 장애물을 감지하는 장애물 감지센서를 포함할 수 있다. 또한, 상기 센서부는 주행구역 내 바닥에 낭떠러지의 존재 여부를 감지하는 낭떠러지 감지센서와, 바닥의 영상을 획득하는 하부 카메라 센서를 더 포함할 수 있다. 도 1과 도 3을 참조하면, 상기 장애물 감지센서는 이동 로봇의 외주면에 일정 간격으로 설치되는 복 수의 센서를 포함할 수 있다. 예를 들어, 상기 센서부는, 상기 본체의 전면에 배치되는 제1 센서, 상기 제1 센서로부터 좌, 우로 이격되도록 배치되는 제2 센서 및 제3 센서를 포함할 수 있다. 상기 장애물 감지센서는, 적외선 센서, 초음파 센서, RF 센서, 지자기 센서, PSD(Position Sensitive Device) 센서 등을 포함할 수 있다. 한편, 상기 장애물 감지센서에 포함되는 센서의 위치와 종류는 이동 로봇의 기종에 따라 달라질 수 있고, 상기 장애물 감지센서는 더 다양한 센서를 포함할 수 있다. 상기 장애물 감지센서는 실내의 벽이나 장애물과의 거리를 감지하는 센서로, 본 발명은 그 종류에 한정되 지 않으나, 이하에서는 초음파 센서를 예시하여 설명한다. 상기 장애물 감지센서는 이동 로봇의 주행(이동) 방향에 존재하는 물체, 특히 장애물을 감지하여 장애물 정보를 제어부에 전달한다. 즉, 상기 장애물 감지센서는, 이동 로봇의 이동 경로, 전방이나 측면에 존재하는 돌출물, 집안의 집기, 가구, 벽면, 벽 모서리 등을 감지하여 그 정보를 제어 유닛에 전달할 수 있다. 이때, 제어부는 초음파 센서를 통해 수신된 적어도 2 이상의 신호에 기초하여 장애물의 위치를 감지하고, 감지된 장애물의 위치에 따라 이동 로봇의 움직임을 제어할 수 있다. 실시예에 따라서는, 케이스의 외측면에 구비되는 장애물 감지 센서는 발신부와 수신부를 포함하여 구 성될 수 있다. 예를 들어, 초음파 센서는 적어도 하나 이상의 발신부 및 적어도 2 이상의 수신부가 서로 엇갈리도록 구비될 수 있다. 이에 따라, 다양한 각도로 신호를 방사하고, 장애물에 반사된 신호를 다양한 각도에서 수신할 수 있다. 실시예에 따라서는, 장애물 감지센서에서 수신된 신호는, 증폭, 필터링 등의 신호 처리 과정을 거칠 수 있 고, 이후 장애물까지의 거리 및 방향이 산출될 수 있다. 한편, 상기 센서부는 본체의 구동에 따른 이동 로봇의 동작을 감지하고 동작 정보를 출력하는 동작 감지 센서를 더 포함할 수 있다. 동작 감지 센서로는, 자이로 센서(Gyro Sensor), 휠 센서(Wheel Sensor), 가속도 센서(Acceleration Sensor) 등을 사용할 수 있다. 동작 감지 센서에서 감지되는 데이터 또는 동작 감지 센서에서 감지되는 데이터에 기초하여 산출되는 데이터는 오도메트리(odometry) 정보를 구성할 수 있다. 자이로 센서는, 이동 로봇이 운전 모드에 따라 움직일 때 회전 방향을 감지하고 회전각을 검출한다. 자이 로 센서는, 이동 로봇의 각속도를 검출하여 각속도에 비례하는 전압 값을 출력한다. 제어부는 자이로 센서로부터 출력되는 전압 값을 이용하여 회전 방향 및 회전각을 산출한다. 휠 센서는, 좌륜(136(L))과 우륜(136(R))에 연결되어 바퀴의 회전수를 감지한다. 여기서, 휠 센서는 로터리 엔 코더(Rotary Encoder)일 수 있다. 로터리 엔코더는 좌륜(136(L))과 우륜(136(R))의 회전수를 감지하여 출력한다. 제어부는 회전수를 이용하여 좌, 우측 바퀴의 회전 속도를 연산할 수 있다. 또한, 제어부는 좌륜 (136(L))과 우륜(136(R))의 회전수 차이를 이용하여 회전각을 연산할 수 있다. 가속도 센서는, 이동 로봇의 속도 변화, 예를 들어, 출발, 정지, 방향 전환, 물체와의 충돌 등에 따른 이 동 로봇의 변화를 감지한다. 가속도 센서는 주 바퀴나 보조바퀴의 인접 위치에 부착되어, 바퀴의 미끄러짐 이나 공회전을 검출할 수 있다. 또한, 가속도 센서는 제어부에 내장되어 이동 로봇의 속도 변화를 감지할 수 있다. 즉, 가속도 센서 는 속도 변화에 따른 충격량을 검출하여 이에 대응하는 전압 값을 출력한다. 따라서, 가속도 센서는 전자식 범 퍼의 기능을 수행할 수 있다. 제어부는 동작 감지 센서로부터 출력된 동작 정보에 기초하여 이동 로봇의 위치 변화를 산출할 수 있 다. 이러한 위치는 영상 정보를 이용한 절대 위치에 대응하여 상대 위치가 된다. 이동 로봇은 이러한 상대 위치 인식을 통해 영상 정보와 장애물 정보를 이용한 위치 인식의 성능을 향상시킬 수 있다. 한편, 이동 로봇은 충전 가능한 배터리를 구비하여 로봇 청소기 내로 전원을 공급하는 전원 공급부 (미도시)를 포함할 수 있다. 상기 전원 공급부는 이동 로봇의 각 구성 요소들에 구동 전원과, 동작 전원을 공급하며, 전원 잔량이 부족 하면 충전대에서 충전 전류를 공급받아 충전될 수 있다. 이동 로봇은 배터리의 충전 상태를 감지하고, 감지 결과를 제어부에 전송하는 배터리 감지부(미 도시)를 더 포함할 수 있다. 배터리는 배터리 감지부와 연결되어 배터리 잔량 및 충전 상태가 제어부(14 0)에 전달된다. 배터리 잔량은 출력부(미도시)의 화면에 표시될 수 있다. 또한, 이동 로봇은 On/Off 또는 각종 명령을 입력할 수 있는 조작부를 포함한다. 조작부를 통해 이동 로봇의 작동 전반에 필요한 각종 제어명령을 입력받을 수 있다. 또한, 이동 로봇은 출력부(미도 시)를 포함하여, 예약 정보, 배터리 상태, 동작모드, 동작상태, 에러상태 등을 표시할 수 있다. 도 5를 참조하면, 이동 로봇은 현재 위치를 인식하는 등 각종 정보를 처리하고 판단하는 제어부, 각 종 데이터를 저장하는 저장부를 포함한다. 또한, 이동 로봇은 데이터를 송수신하는 통신부를 더 포함할 수 있다. 이동 로봇은 리모컨(미도시) 또는 단말(미도시)을 통해 동작에 대한 명령을 수신할 수 있다. 단말은 이동 로봇을 제어하기 위한 애플리케이션을 구비하고, 애플리케이션의 실행을 통해 이동 로봇이 청소할 주 행구역에 대한 맵을 표시하고, 맵 상에 특정 영역을 청소하도록 영역을 지정할 수 있다. 단말은 맵 설정을 위한 애플리케이션(application)이 탑재된 리모콘, PDA, 랩탑(laptop), 태블릿PC, 스마트 폰 등을 예로 들 수 있다. 단말은 이동 로봇과 통신하여, 맵을 수신하여 표시하고, 맵 상에 이동 로봇의 현재 위치를 표시할 수 있다. 또한, 단말은 이동 로봇의 주행에 따라 그 위치를 갱신하여 표시한다. 제어부는 이동 로봇을 구성하는 영상획득부, 조작부, 주행부를 제어하여, 이동 로봇 의 동작 전반을 제어한다. 저장부는 이동 로봇의 제어에 필요한 각종 정보들을 기록하는 것으로, 휘발성 또는 비휘발성 기록 매 체를 포함할 수 있다. 기록 매체는 마이크로 프로세서(micro processor)에 의해 읽힐 수 있는 데이터를 저장한 것으로, HDD(Hard Disk Drive), SSD(Solid State Disk), SDD(Silicon Disk Drive), ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등을 포함할 수 있다. 저장부에는 주행구역에 대한 맵(Map)이 저장될 수 있다. 또한, 저장부에는 주행 중 감지되는 장애물 에 대한 정보를 저장하고, 충전대 위치, 또는 충전대 탐색을 위해 설정되는 데이터가 저장된다. 이때, 저장부에 저장되는 맵은, 이동 로봇이 스스로 학습을 하여 생성할 수 있고, 경우에 따라 이동 로봇 과 유선 또는 무선 통신을 통해 정보를 교환할 수 있는 단말에 의해 입력된 것일 수도 있다. 맵에는 주행구역 내의 방들의 위치가 영역별로 표시될 수 있다. 또한, 이동 로봇의 현재 위치가 맵 상에 표시될 수 있으며, 맵 상에서의 이동 로봇의 현재의 위치는 주행 과정에서 갱신될 수 있다. 단말은 저장부 에 저장된 맵과 동일한 맵을 저장한다. 상기 저장부에 저장되는 주행구역에 대한 맵(Map)은, 청소 중 주행에 사용되는 내비게이션 맵(Navigation map), 위치 인식에 사용되는 SLAM(Simultaneous localization and mapping) 맵, 장애물 등에 부딪히면 해당 정 보를 저장하여 학습 청소시 사용하는 학습 맵, 전역 위치 인식에 사용되는 전역 위치 맵, 인식된 장애물에 관한 정보가 기록되는 장애물 인식 맵 등일 수 있다. 한편, 상술한 바와 같이 용도별로 상기 저장부에 맵들을 구분하여 저장, 관리할 수 있지만, 맵이 용도별로 명확히 구분되지 않을 수도 있다. 예를 들어, 적어도 2 이상의 용도로 사용할 수 있도록 하나의 맵에 복수의 정 보를 저장할 수도 있다. 제어부는 주행 중, 이동 로봇 주변의 먼지 또는 이물질을 흡수하여 청소를 수행한다. 제어부는 브러 쉬를 동작시켜 이동 로봇 주변의 먼지 또는 이물질을 흡입하기 쉬운 상태로 만들고, 흡입장치를 동작시켜 먼지 또는 이물질을 흡입한다. 제어부는 복수의 영역 중 적어도 하나의 영역에 대한 청소를 지시할 수 있고, 주행부에 의해 지정된 영역 으로 이동하여 청소를 수행하도록 한다. 이때 주행부에 의해 이동하는 중에도 청소를 수행할 수 있다. 제어부는 배터리의 충전용량을 체크하여 충전대로의 복귀 시기를 결정한다. 제어부는 충전용량 이 일정값에 도달하면, 수행중이던 동작을 중지하고, 충전대 복귀를 위해 충전대 탐색을 시작한다. 제어부(14 0)는 배터리의 충전용량에 대한 알림 및 충전대 복귀에 대한 알림을 출력할 수 있다. 제어부는 주행제어모듈, 구역구분모듈, 학습모듈 및 인식모듈을 포함할 수 있다. 주행제어모듈은 이동 로봇의 주행을 제어하는 것으로, 주행 설정에 따라 주행부의 구동을 제어 한다. 또한, 주행제어모듈은 주행부의 동작을 바탕으로 이동 로봇의 이동 경로를 파악할 수 있 다. 예를 들어, 주행제어모듈은 구동 바퀴의 회전속도를 바탕으로 이동 로봇의 현재 또는 과거 의 이동속도, 주행한 거리 등을 파악할 수 있으며, 각 구동 바퀴(136(L), 136(R))의 회전 방향에 따라 현재 또 는 과거의 방향 전환 과정 또한 파악할 수 있다. 이렇게 파악된 이동 로봇의 주행 정보를 바탕으로, 맵 상 에서 이동 로봇의 위치가 갱신될 수 있다. 구역구분모듈은 소정 기준에 따라 주행구역을 복수의 구역으로 구분할 수 있다. 주행구역은 이동 로봇 이 주행 경험이 있는 모든 평면상의 구역 및 현재 주행하고 있는 평면상의 구역을 모두 합한 범위로 정의 될 수 있다. 구역구분모듈은 주행구역을 복수의 소구역으로 영역을 구분하며, 각 소구역은 주행구역 내의 각 실(방)을 근거로 구분될 수 있다. 또한, 구역구분모듈은 주행구역을 주행능력상 서로 분리된 복수의 대구역으로 구 분할 수 있다. 예를 들면, 서로 동선상 완전히 분리된 두개의 실내공간은 각각 두개의 대구역으로 구분될 수 있 다. 다른 예로, 같은 실내 공간이라 하더라도, 상기 대구역은 주행구역 내의 각 층을 근거로 구분될 수 있다. 구역구분모듈은 초기 맵 생성시, 학습모듈을 통해, 영역이 구분되지 않은 기초맵이 생성되면, 기초맵을 복 수의 셀(로컬 맵)으로 구분하고, 각 셀에 대해서 영역을 인식한 후에 그 결과를 병합함으로써, 영역이 구분된 최종맵을 생성할 수 있다. 또는, 구역구분모듈은 초기 맵 생성시, 학습모듈을 통해, 영역이 구분되지 않은 기초맵이 생성되면, 침식 과 팽창의 원리를 이용하여 소영역을 분리한 후 대표영역을 설정한다. 구역구분모듈은 대표영역을 설정한 이후 에, 분리된 소영역을 세부영역으로써 추출하여, 어느 하나의 대표영역에 병합함으로써, 영역이 구분된 최종맵을 생성할 수 있다. 학습모듈은 주행구역의 맵을 생성할 수 있다. 또한, 학습모듈은 각 위치에서 영상획득부를 통해 획득한 영상을 처리하여 맵과 연계시켜 전역위치를 인식한다. 인식모듈은 현재 위치를 추정하여 인식한다. 인식모듈은 영상획득부의 영상 정보를 이용하여 학 습모듈과 연계하여 위치를 파악함으로써, 이동 로봇의 위치가 갑자기 변경되는 경우에도 현재 위치를 추정하여 인식할 수 있다. 이동 로봇은 구역구분모듈을 통해 연속적인 주행 중에 위치 인식이 가능하고 또한, 구역구분모듈 없이 학습모듈 및 인식모듈을 통해, 맵을 학습하고 현재 위치를 추정할 수 있다. 한편, 도 5에서는 학습모듈과 인식모듈이 제어부 내에 별도로 구비되는 예를 도시하였으나, 본 발명은 이에 한정되지 않는다. 예를 들어, 학습모듈과 인식모듈은 하나의 인식기로써 통합되어 구성될 수 있다. 이 경우에, 머신 러 닝 등의 학습 기법을 이용하여 인식기를 학습시키고, 학습된 인식기는 이후에 입력되는 데이터를 분류하여 영역, 사물 등의 속성을 인식할 수 있다. 이동 로봇이 주행하는 중에, 영상획득부는 이동 로봇 주변의 영상들을 획득한다. 이하, 영상획 득부에 의해 획득된 영상을 '획득영상'이라고 정의한다. 획득영상에는 천장에 위치하는 조명들, 경계 (edge), 코너(corner), 얼룩(blob), 굴곡(ridge) 등의 여러가지 특징(feature)들이 포함된다. 학습모듈은 획득영상들 각각으로부터 특징을 검출한다. 컴퓨터 비전(Computer Vision) 기술 분야에서 영상 으로부터 특징을 검출하는 다양한 방법(Feature Detection)이 잘 알려져 있다. 이들 특징의 검출에 적합한 여러 특징검출기(feature detector)들이 알려져 있다. 예를들어, Canny, Sobel, Harris&Stephens/Plessey, SUSAN, Shi&Tomasi, Level curve curvature, FAST, Laplacian of Gaussian, Difference of Gaussians, Determinant of Hessian, MSER, PCBR, Grey-level blobs 검출기 등이 있다. 학습모듈은 각 특징점을 근거로 디스크립터를 산출한다. 학습모듈은 특징 검출을 위해 SIFT(Scale Invariant Feature Transform) 기법을 이용하여 특징점을 디스크립터(descriptor)로 변환할 수 있다. 디스크립 터는 n차원 벡터(vector)로 표기될 수 있다. SIFT는 촬영 대상의 스케일(scale), 회전, 밝기변화에 대해서 불변하는 특징을 검출할 수 있어, 같은 영역을 이 동 로봇의 자세를 달리하며 촬영하더라도 불변하는(즉, 회전 불변한(Rotation-invariant)) 특징을 검출할 수 있다. 물론, 이에 한정되지 않고 다른 다양한 기법(예를들어, HOG: Histogram of Oriented Gradient, Haar feature, Fems, LBP:Local Binary Pattern, MCT:Modified Census Transform)들이 적용될 수도 있다. 학습모듈은 각 위치의 획득영상을 통해 얻은 디스크립터 정보를 바탕으로, 획득영상마다 적어도 하나의 디 스크립터를 소정 하위 분류규칙에 따라 복수의 군으로 분류하고, 소정 하위 대표규칙에 따라 같은 군에 포함된 디스크립터들을 각각 하위 대표 디스크립터로 변환할 수 있다. 다른 예로, 실(room)과 같이 소정 구역내의 획득영상 들로부터 모인 모든 디스크립터를 소정 하위 분류규칙에 따라 복수의 군으로 분류하여 상기 소정 하위 대표규칙에 따라 같은 군에 포함된 디스크립터들을 각각 하위 대 표 디스크립터로 변환할 수도 있다. 학습모듈은 이 같은 과정을 거쳐, 각 위치의 특징분포를 구할 수 있다. 각 위치 특징분포는 히스토그램 또 는 n차원 벡터로 표현될 수 있다. 또 다른 예로, 학습모듈은 소정 하위 분류규칙 및 소정 하위 대표규칙을 거치지 않고, 각 특징점으로부터 산출된 디스크립터를 바탕으로 미지의 현재위치를 추정할 수 있다. 또한, 위치 도약 등의 이유로 이동 로봇의 현재 위치가 미지의 상태가 된 경우에, 기 저장된 디스크립터 또는 하위 대표 디스크립터 등의 데이터를 근거로 현재 위치를 추정할 수 있다. 이동 로봇은, 미지의 현재 위치에서 영상획득부를 통해 획득영상을 획득한다. 영상을 통해 천장에 위 치하는 조명들, 경계(edge), 코너(corner), 얼룩(blob), 굴곡(ridge) 등의 여러가지 특징(feature)들이 확인된다. 인식모듈은 획득영상으로부터 특징들을 검출한다. 컴퓨터 비전 기술 분야에서 영상으로부터 특징을 검출하 는 다양한 방법 및 이들 특징의 검출에 적합한 여러 특징검출기들에 대한 설명은 상기한 바와 같다. 인식모듈은 각 인식 특징점을 근거로 인식 디스크립터 산출단계를 거쳐 인식 디스크립터를 산출한다. 이때 인식 특징점 및 인식 디스크립터는 인식모듈에서 수행하는 과정을 설명하기 위한 것으로 학습모듈에 서 수행하는 과정을 설명하는 용어와 구분하기 위한 것이다. 다만, 이동 로봇의 외부 세계의 특징이 각각 다른 용어로 정의되는 것에 불과하다. 인식모듈은 본 특징 검출을 위해 SIFT(Scale Invariant Feature Transform) 기법을 이용하여 인식 특징점 을 인식 디스크립터로 변환할 수 있다. 인식 디스크립터는 n차원 벡터(vector)로 표기될 수 있다. SIFT는 앞서 설명한 바와 같이, 획득영상에서 코너점 등 식별이 용이한 특징점을 선택한 후, 각 특징점 주변의 일정한 구역에 속한 픽셀들의 밝기 구배(gradient)의 분포 특성(밝기 변화의 방향 및 변화의 급격한 정도)에 대 해, 각 방향에 대한 변화의 급격한 정도를 각 차원에 대한 수치로 하는 n차원 벡터(vector)를 구하는 영상인식 기법이다. 인식모듈은 미지의 현재 위치의 획득영상을 통해 얻은 적어도 하나의 인식 디스크립터 정보를 근거로, 소 정 하위 변환규칙에 따라 비교대상이 되는 위치 정보(예를 들면, 각 위치의 특징분포)와 비교 가능한 정보(하위 인식 특징분포)로 변환한다. 소정 하위 비교규칙에 따라, 각각의 위치 특징분포를 각각의 인식 특징분포와 비교하여 각각의 유사도를 산출할 수 있다. 각각의 위치에 해당하는 상기 위치 별로 유사도(확률)를 산출하고, 그 중 가장 큰 확률이 산출되는 위 치를 현재위치로 결정할 수 있다. 이와 같이, 제어부는 주행구역을 구분하고 복수의 영역으로 구성된 맵을 생성한다. 또한, 제어부는 저장된 맵을 바탕으로 본체의 현재 위치를 인식할 수 있다. 제어부는 맵이 생성되면, 생성된 맵을 통신부를 통해 외부 단말기로 전송한다. 또한, 제어부는 앞서 설명한 바와 같이, 외부 단말기로부터 맵이 수신되면, 저장부에 저장할 수 있다. 또한 제어부는 주행 중 맵이 갱신되는 경우 갱신된 정보를 외부 단말기로 전송하여 외부 단말기와 이동 로 봇에 저장되는 맵이 동일하도록 한다. 외부 단말기와 이동 로봇에 저장된 맵이 동일하게 유지됨에 따 라 이동 단말기로부터의 청소명령에 대하여, 이동 로봇이 지정된 영역을 청소할 수 있으며, 또한, 외부 단 말기에 이동 로봇의 현재 위치가 표시될 수 있도록 하기 위함이다. 이때, 맵은 청소 영역을 복수의 영역으로 구분되고, 복수의 영역을 연결하는 연결통로가 포함하며, 영역 내의 장애물에 대한 정보를 포함한다. 청소 영역에 대한 구분은, 앞서 설명한 바와 같이 구역구분모듈에 의해 소영역 및 대영역으로 구분된다. 제어부는 청소명령이 입력되면, 맵 상의 위치와 이동 로봇의 현재위치가 일치하는지 여부를 판단한다. 청 소명령은 리모컨, 조작부 또는 외부 단말기로부터 입력될 수 있다. 제어부는 현재 위치가 맵 상의 위치와 일치하지 않는 경우, 또는 현재 위치를 확인할 수 없는 경우, 현재 위치를 인식하여 이동 로봇의 현재 위치를 복구한 한 후, 현재 위치를 바탕으로 지정영역으로 이동하도록 주행부를 제어한다. 현재 위치가 맵 상의 위치와 일치하지 않는 경우 또는 현재 위치를 확인할 수 없는 경우, 인식모듈은 영상 획득부로부터 입력되는 획득영상을 분석하여 맵을 바탕으로 현재 위치를 추정할 수 있다. 또한, 구역구분 모듈 또는 학습모듈 또한, 앞서 설명한 바와 같이 현재 위치를 인식할 수 있다. 위치를 인식하여 이동 로봇의 현재 위치를 복구한 후, 주행제어모듈은 현재 위치로부터 지정영역으로 이동 경로를 산출하고 주행부를 제어하여 지정영역으로 이동한다. 외부 단말기로부터 복수의 영역 중, 적어도 하나의 영역이 선택되는 경우, 주행제어모듈은 선택된 영역을 지정영역으로 설정하고, 이동 경로를 산출한다. 주행제어모듈은 지정영역 이동 후, 청소를 수행한다. 한편, 복수의 영역이 지정영역으로 선택된 경우, 주행제어모듈은 복수의 영역 중 우선영역이 설정되었는지 여부 또는 선택된 복수의 지정영역에 대한 청소순서가 설정되었는지 여부를 판단한 후, 지정영역으로 이동하여청소를 수행한다. 주행제어모듈은 복수의 지정영역 중 어느 하나가 우선영역으로 설정된 경우, 복수의 지정영역 중 우선영역 으로 이동하여, 우선영역을 제일 먼저 청소한 후 나머지 지정영역으로 이동하여 청소하도록 한다. 또한, 지정영 역에 대한 청소순서가 설정된 경우 주행제어모듈은 지정된 청소순서에 따라 지정영역을 순차적으로 이동하 면서 청소를 수행한다. 또한, 주행제어모듈은 맵 상의 복수의 영역에 대한 구분에 관계없이 새롭게 임의의 영역이 설정되는 경우, 설정된 지정영역으로 이동하여 청소를 수행하도록 한다. 제어부는 설정된 지정영역에 대한 청소가 완료되면, 청소기록을 저장부에 저장한다. 또한, 제어부는 통신부를 통해 이동 로봇의 동작상태 또는 청소상태를 소정 주기로 외부 단말기 로 전송한다. 그에 따라 단말은 수신되는 데이터를 바탕으로, 실행중인 애플리케이션의 화면상에 맵과 함께 이 동 로봇의 위치를 표시하고, 또한 청소상태에 대한 정보를 출력한다. 또한, 제어부는 충전대의 복귀신호를 통해 충전대가 감지되면, 학습부 및 인식부를 통해 이동 로봇의 현재 위치를 인식하고, 이동 로봇의 현재위치를 바탕으로 충전대의 위치를 산출하여 저장한다. 또한, 제어 부는 충전대의 위치가 맵 상에 표시되도록 설정할 수 있다. 도 6 은 본 발명의 일 실시예에 따른 이동 로봇의 영역 구분 및 그에 따른 맵 생성의 예가 도시된 도이다. 도 6의 (a)에 도시된 바와 같이, 이동 로봇은 저장된 맵이 존재하지 않는 경우, 월팔로윙을 통해 주행구역 (X1)을 주행하면서 맵을 생성할 수 있다. 구역구분모듈은 도 6의(b)에 도시된 바와 같이, 주행구역(X1)을 복수의 영역(A1' 내지 A9')으로 구분하여 도 6의 (c)와 같이 맵을 생성한다. 생성된 맵은 저장부에 저장되고, 통신부를 통해 외부 단말기로 전 송된다. 구역구분모듈은 전술한 바와 같이 주행구역(X1)에 대하여 소영역과 대영역을 구분하고 그에 따른 맵을 생성한다. 단말은 애플리케이션을 실행하고, 수신된 맵을 화면에 표시한다. 이때 구분된 복수의 영역(A1 내지 A9)을 각각 상이하게 표시한다. 맵은 복수의 영역(A1 내지 A9)이 각각 상이한 색상으로 표시되거나 또는 상이한 이름이 표 시된다. 이동 로봇과 단말은 동일한 맵을 저장하는 것을 기본으로 하나, 단말에는 사용자가 영역을 쉽게 인식할 수 있도 록 도 6의 (c)와 같이 영역을 단순화한 사용자맵이 표시되도록 하고, 이동 로봇은, 장애물에 대한 정보가 포함 된 도 6의 (b)와 같은 맵을 바탕으로 주행 및 청소를 수행한다. 도 6의 (c)의 사용자맵에도 장애물이 표시될 수 있다. 도 6의 (b)에서 예시된 맵은 SLAM 맵 이거나 SLAM 맵에 기반한 내비게이션 맵일 수 있다. 이동 로봇은 청소명령이 입력되면, 저장된 맵을 바탕으로, 현재 위치를 판단하고, 현재 위치와 맵 상의 위 치가 일치하는 경우에는 지정된 청소를 수행하고, 현재 위치가 일치하지 않는 경우, 현재 위치를 인식하여 복구 한 후 청소를 수행한다. 따라서 이동 로봇은 복수의 영역(A1 내지 A9) 중 어느 위치에 있더라도 현재 위치 를 판단 한 후 지정영역으로 이동하여 청소를 수행할 수 있다. 리모컨 또는 단말은 도시된 바와 같이, 복수의 영역(A1 내지 A9) 중 적어도 하나의 영역을 선택하여 이동 로봇 으로 청소명령을 입력할 수 있다. 또한, 이동 로봇은 리모컨 또는 단말을 통해 어느 하나의 영역의 일부를 청소영역으로 설정하거나, 복수의 영역에 대하여 영역 구분없이 터치 또는 드래그를 통해 청소영역을 설 정할 수 있다. 복수의 영역에 대하여 청소명령이 입력되는 경우, 어느 하나의 영역을 우선영역으로 설정하거나, 우선영역을 시 작한 후 근거리인 영역으로 이동하여 청소 할 수 있으며, 또는 청소순서를 설정할 수 있다. 이동 로봇은 복수의 지정영역에 대하여 청소순서가 설정된 경우 지정된 순서에 따라 이동하며 청소를 수행한다. 이동 로봇 은 복수의 청소영역에 대하여, 별도의 순서가 지정되지 않은 경우에는 현재 위치로부터 가까운 영역으로 이동하여 청소를 수행할 수 있다. 도 7은 본 발명의 일 실시예에 따른 이동 로봇의 제어 방법을 도시한 순서도이다. 도 1 내지 도 7을 참조하면, 이동 로봇은 명령 또는 설정에 따라서 이동하며 청소를 수행할 수 있다 (S710). 예를 들어, 이동 로봇은 내비게이션(navigation) 맵(map)에 기반하여 이동할 수 있다. 내비게이션 맵은, SLAM 맵에 기초하여 생성되는 맵일 수 있고, 도 6의 (b)에서 예시된 맵일 수 있다.한편, 상기 내비게이션 맵은 복수의 로컬(local) 맵을 포함할 수 있다. 상기 내비게이션 맵은 복수의 영역으로 구분되고, 각 영역은 하나 이상의 로컬 맵을 포함할 수 있다. 각 로컬 맵은 서로 겹치지 않도록 설정된다. 상기 로컬 맵은 일종의 단위 맵으로서 임의의 크기로 설정 가능하다. 예를 들어, 상기 로컬 맵은 N by N의 크기 를 가지는 정사각형 형태로 설정되거나 기타 다른 모양으로도 설정 가능하다. 한편, 구역구분모듈은 주행구역을 복수의 소구역으로 영역을 구분할 수 있다. 예를 들어, 구역구분모듈은 초기 맵 생성시, 학습모듈을 통해, 영역이 구분되지 않은 기초맵이 생성되면, 기초맵을 복수의 로컬 맵으로 구분하고, 각 로컬 맵에 대해서 영역을 인식한 후에 그 결과를 병합함으로써, 영 역이 구분된 최종맵을 생성할 수 있다. 또는, 구역구분모듈은 초기 맵 생성시, 학습모듈을 통해, 영역이 구분되지 않은 기초맵이 생성되면, 침식 과 팽창의 원리를 이용하여 소영역을 분리한 후 대표영역을 설정한다. 구역구분모듈은 대표영역을 설정한 이후 에, 분리된 소영역을 세부영역으로써 추출하여, 어느 하나의 대표영역에 병합함으로써, 영역이 구분된 최종맵을 생성할 수 있다. 이 경우에, 구역구분모듈은 모폴로지(Morphology)연산을 통해 기초맵을 침식(Erosion) 및 팽창(Dilatio n)하여 소영역을 분리함으로써 대표영역을 설정할 수 있다. 이에 따라, 기초맵 중 폭이 좁거나 면적이 작은 소 영역은 사라지거나, 일부만 남게 되고, 면접이 넓은 영역의 일부가 남도록 한다. 이렇게 남겨진 면적이 넓은 영 역에 세부영역들을 병합함으로써, 문 등이 위치하는 좁아지는 영역을 경계로 하여 실내 주행구역을 면적이 넓은 영역을 중심으로 구분할 수 있다. 한편, 상기 이동 로봇의 이동 중에, 영상획득부를 통하여, 상기 복수의 로컬 맵에 대응하는 영역들에 서 복수의 영상을 획득할 수 있다(S720). 영상획득부는, 본체 전방의 영상을 획득하도록 구비되는 전면 카메라(120a)와 본체의 상면부에 구비되는 상부 카메라(120b)를 통하여, 이동 중에 본체 주변의 영상을 획득할 수 있다. 한편, 제어부는, 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵에 대응하는 영역에서는 추가 촬영을 하지 않도록 상기 영상획득부를 제어할 수 있다. 속성 인식에 필요한 개수의 데이터가 확보되면, 더 이상 의 영상을 획득하지 않음으로써, 불필요한 연산, 처리, 인식 과정을 방지할 수 있다. 실시예에 따라서는, 제어부는, 영상획득부가 획득하는 복수의 영상 중 일부 영상을 소정 기준에 따라 필터링(filtering)할 수 있다. 예를 들어, 상기 소정 기준은, 센서부의 장애물 감지 여부 조건을 포함할 수 있다. 센서부가 장애물 을 감지했을 때, 촬영된 영상은 장애물의 인식에는 효과적이나, 경우에 따라서는 전체 영역의 속성을 인식하는 데에는 장애물이 방해가 될 수 있다. 예를 들어, 벽, 책상, 큰 물건 등의 일부만이 촬영된 영상에는 영상의 대 부분이 장애물의 일부분으로만 채워질 수 있다. 이 경우에, 장애물의 일부분으로는 해당 장애물의 인식에도 어 려운 경우가 발생할 수 있고, 전체 영역의 공간 인식에는 그 어려움이 더 클 수 있다. 또한, 상기 소정 기준은, 이동 로봇이 소정 영역 안에 있는지 여부, 이동 로봇의 자세(방향)이 소정 영역 의 중심을 향하는 지 여부 조건 등을 더 포함할 수 있다. 즉, 소정 연역의 공간 속성 인식에 적합하지 않은 영상들을 걸러내어 남은 영상들로만 속성 인식에 이용할 수 있다. 또는, 제어부는 소정 기준에 따라 촬영 여부를 결정할 수 있고, 이 경우에도, 상기 소정 기준은, 센서부 의 장애물 감지 여부 조건을 포함할 수 있다. 또한, 상기 소정 기준은, 이동 로봇이 소정 영역 안에 있는지 여부, 이동 로봇의 자세(방향)이 소정 영역의 중심을 향하는 지 여부 조건 등을 더 포함할 수 있다. 즉, 제어부는, 획득되는 영상들을 소정 기준에 따라 필터링하여 속성 인식에 사용하지 않거나, 영상획득부 가 소정 기준에 따라 촬영하지 않도록 제어함으로써, 속성 인식의 정확도를 저하시킬 수 있는 데이터를 사 용하는 것을 방지할 수 있다. 한편, 제어부는, 영상획득부를 통하여 획득된 영상을 입력 데이터로 하여 영상에 대응하는 영역 및 영상 내에 존재하는 사물을 인식할 수 있다. 더욱 바람직하게는, 제어부는, 전면 카메라(120a)에서 획득되는 영상들에 기초하여, 영역 및 사물의 속성 을 인식할 수 있다. 한편, 제어부는, 인식 결과의 정확성을 향상하기 위하여, 소정 영역에서 촬영된 복수의 영상 각각에 대하 여 영역 인식 과정을 수행하고, 그 인식 결과를 취합할 수 있다. 이를 위해, 이동 로봇은, 이동 중에, 영상획득부를 통하여 소정 기준에 따라 영상 촬영을 계속할 수 있고, 제어부는, 특정 로컬 맵에 대응하는 영역에서 촬영된 영상이 소정 기준 갯수 이상 확보되면(S730), 해당 영역의 속성을 인식할 수 있다(S740). 더욱 바람직하게는, 제어부는, 상기 복수의 로컬 맵 중 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵의 속성을 인식할 수 있다(S740). 예를 들어, 상기 N개의 기준 갯수는, 동/서/남/북의 4방위, 8방위, 16방위 등에 대응하는, 4, 8, 16개 등으로 설정될 수 있다. 실시예에 따라서는, 상기 다른 방향으로 촬영된 N개의 영상은, 이동 로봇이 소정 위치에서 회전하며 촬영 되는 N개의 영상일 수 있다. 하지만, 속성 인식을 위한 영상을 획득하기 위하여, 이동 로봇이 전체 주행 구역에 포함되는 다수의 위치 에서 회전하는 것은 비효율적이다. 또한, 사용자는 반복적으로 주행을 멈추고, 제자리에서 회전하는 이동 로봇 의 동작을 이해하지 못하여, 제품 신뢰도가 저하될 수 있다. 따라서, 청소를 위한 일반 주행에서 속성 인식을 위한 영상을 획득하는 것이 더 바람직하다. 예를 들어, 제어부는, 지그재그 패턴으로 주행하며 이동 중에, 영상획득부가 영상들을 계속해서 획득 하고, 획득된 영상들을 촬영된 위치에 따라 분류하여, 특정 로컬 맵에 대응하는 영역에서 N개까지의 데이터를 확보하도록 제어할 수 있다. 더욱 바람직하게는, 주행부는, 제어부의 제어에 따라, 제1 방향으로 지그재그 패턴으로 주행하는 1회 차 주행과 상기 제1 방향에 수직하는 제2 방향으로 지그재그 패턴으로 주행하는 2회차 주행하도록 본체를 이동시킬 수 있다. 이 경우에, 상기 다른 방향으로 촬영된 N개의 영상은, 상기 1회차 주행 시 획득되는 영상과 상기 2회차 주행 시 획득되는 영상을 포함할 수 있다. 즉, 제어부는, 다른 방향의 지그재그 패턴으로 주행하도록 제어하고, 서로 다른 방향의 지그재그 패턴 주 행 시 획득되는 영상들을 획득시 위치에 따라 분류하여 특정 로컬 맵에 대응하는 영역에서 N개까지의 데이터를 확보하도록 제어할 수 있다. 한편, 제어부는, 상기 복수의 영상을 획득 시의 위치 정보, 방향 정보, 주행 라인 정보, 로컬 맵 정보와 연계하여 저장부에 저장하도록 제어할 수 있다. 특정 로컬 맵에 대응하는 영역에서, 동/서/남/북의 4방위, 8방위, 16방위 등 다른 방향에서 촬영된 복수의 영상 이 획득되면(S730), 제어부는 다른 방향에서 촬영된 영상들을 이용하여 로컬 맵의 속성을 인식할 수 있다 (S740). 또한, 제어부는, 상기 로컬 맵의 속성 인식 결과를 저장부에 저장하도록 제어할 수 있다. 제어부는, 다른 방향에서 촬영된 N개의 영상 전부 또는 일부를 입력 데이터로 하여 머신 러닝에 기반한 속 성 인식을 수행하고 그 결과를 출력할 수 있다. 한편, 제어부는, 상기 복수의 로컬 맵 중에서, 인접한 소정 개수의 로컬 맵으로 구성되는 의미 맵을 생성 할 수 있고(S750), 상기 의미 맵에 포함되는 로컬 맵들의 속성 인식 결과들에 기초하여, 상기 의미 맵에 대응하 는 영역의 최종 속성을 인식할 수 있다(S760). 제어부는, 속성 인식이 완료된 로컬 맵 중 인접한 로컬 맵들을 묶어 의미 맵을 생성한 후에(S750), 상기 의미 맵에 포함되는 로컬 맵들의 속성 인식 결과들에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 인식할 수 있다(S760). 또는, 제어부는, 미리 인접한 로컬 맵들을 묶어 의미 맵을 생성해두고 (S750), 소정 의미 맵에 포함되는 로컬 맵들의 속성 인식이 완료되면(S740), 상기 의미 맵에 포함되는 로컬 맵들의 속성 인식 결과들에 기초하여,상기 의미 맵에 대응하는 영역의 최종 속성을 인식할 수 있다(S760). 한편, 제어부는, 상기 로컬 맵에서 획득된 N개의 영상에 기초하여 상기 로컬 맵에 대응하는 영역을 인식할 수 있다. 또한, 제어부는, 상기 로컬 맵에서 획득된 N개의 영상에서, 사물에 대응하는 이미지를 추출한 후에, 상기 추출된 이미지에 기초하여, 상기 로컬 맵에 대응하는 영역에 존재하는 사물을 인식할 수 있다. 이 경우에, 제어부는, 상기 센서부가 감지하는 장애물의 방향에 대응하여 영상획득부가 획득하 는 영상의 일부 영역을 잘라내어 추출하도록 제어할 수 있다. 실시예에 따라서는, 제어부는, 직접 영상의 일부를 추출하거나, 상기 제어부와는 별도로 구비되는 영 상처리부(미도시)가 일부 영역을 추출하거나, 영상획득부가 일부 영역을 추출하도록 제어할 수 있다. 한편, 학습모듈 및/또는 인식모듈은 영상 전체에서 머신 러닝(machine learning)으로 기학습된 데이 터에 기초하여 소정 영역의 속성을 인식할 수 있다. 또한, 학습모듈 및/또는 인식모듈은 영상 중 적어도 일부에서 머신 러닝(machine learning)으로 기학 습된 데이터에 기초하여 사물을 인식할 수 있다. 이를 위해, 학습모듈 및/또는 인식모듈은 머신 러닝으로 장애물의 종류 등 속성을 인식하도록 학습된 인공신경망을 포함할 수 있고, 기학습된 데이터에 기초하여 영역 및 사물의 속성을 인식할 수 있다. 머신 러닝은 컴퓨터에게 사람이 직접 로직(Logic)을 지시하지 않아도 데이터를 통해 컴퓨터가 학습을 하고 이를 통해 컴퓨터가 알아서 문제를 해결하게 하는 것을 의미한다. 예를 들어, 학습모듈 및/또는 인식모듈에는 딥러닝 구조 중 하나인 CNN(Convolutional Neural Network)이 탑재되고, 기학습된 CNN(Convolutional Neural Network)은 입력 데이터에 포함된 영역 및 사물의 속성을 인식하여 그 결과를 출력할 수 있다. 머신 러닝(Machine Learning)의 일종인 딥러닝(Deep Learning) 기술은 데이터를 기반으로 다단계로 깊은 수준까 지 내려가 학습하는 것이다. 딥러닝(Deep learning)은 단계를 높여갈수록 복수의 데이터들로부터 핵심적인 데이터를 추출하는 머신 러닝 (Machine Learning) 알고리즘의 집합을 나타낼 수 있다. 딥러닝 구조는 인공신경망(ANN)를 포함할 수 있으며, 예를 들어 딥러닝 구조는 CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), DBN(Deep Belief Network) 등 심층신경망(DNN)으로 구성될 수 있다 인공신경망은 각 레벨에 대응하는 학습된 레이어(layer)를 이용하여, 각 레벨에 대응하는 특징 정보를 추출할 수 있다. 인공신경망은 순차적으로 추상화하여, 가장 상위 레벨의 특징 정보를 활용하여 소정 대상을 인식할 수 있다. 예를 들어, 딥러닝에 의한 얼굴인식 과정을 살펴보면, 컴퓨터는 입력 영상으로부터, 픽셀의 밝기에 따라 밝은 픽셀과 어두운 픽셀을 구분하고, 테두리, 에지 등 단순한 형태를 구분한 후, 조금 더 복잡한 형태와 사물을 구 분할 수 있다. 최종적으로 컴퓨터는 인간의 얼굴을 규정하는 형태를 파악할 수 있다. 본 발명에 따른 딥러닝 구조는 공지된 다양한 구조를 이용할 수 있다. 예를 들어, 본 발명에 따른 딥러닝 구조 는 CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), DBN(Deep Belief Network) 등일 수 있다. RNN(Recurrent Neural Network)은, 자연어 처리 등에 많이 이용되고 있으며, 시간의 흐름에 따라 변하는 시계열 데이터(Time-series data) 처리에 효과적인 구조로 매 순간마다 레이어를 쌓아올려 인공신경망 구조를 구성할 수 있다. DBN(Deep Belief Network)은 딥러닝 기법인 RBM(Restricted Boltzman Machine)을 다층으로 쌓아 구성되는 딥러 닝 구조이다. RBM(Restricted Boltzman Machine) 학습을 반복하여, 일정 수의 레이어가 되면 해당 개수의 레이 어를 가지는 DBN(Deep Belief Network)를 구성할 수 있다. CNN(Convolutional Neural Network)은, 특히 객체 인식 분야에서 많이 사용되는 구조로써, CNN(Convolutional Neural Network)은 사람이 물체를 인식할 때 물체의 기본적인 특징들을 추출한 다음 뇌 속에 서 복잡한 계산을 거쳐 그 결과를 기반으로 물체를 인식한다는 가정을 기반으로 만들어진 사람의 뇌 기능을 모 사한 모델이다. 한편, 도 5에서는 학습모듈과 인식모듈이 제어부 내에 별도로 구비되는 예를 도시하였으나, 본 발명은 이에 한정되지 않는다. 예를 들어, 학습모듈과 인식모듈은 하나의 인식기로써 통합되어 구성될 수 있다. 이 경우에, 머신 러 닝 등의 학습 기법을 이용하여 인식기를 학습시키고, 학습된 인식기는 이후에 입력되는 데이터를 분류하여 영역, 사물 등의 속성을 인식할 수 있다. 제어부는, 머신 러닝(machine learning)으로 기학습된 데이터에 기초하여, 상기 로컬 맵에서 획득된 N개의 영상을 분류하여 상기 로컬 맵의 속성을 판별할 수 있다. 한편, 제어부는, 상기 로컬 맵에 대응하는 영역의 인식 결과와 사물 인식 결과를 통합하여, 해당 로컬 맵 의 최종 인식 결과를 생성할 수 있다. 제어부는, 상기 인식된 복수의 로컬 맵 인식 결과에서, 인식 결과의 빈도수, 신뢰값, 신뢰값들의 평균값 중 적어도 하나에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 판별할 수 있다. 예를 들어, 학습모듈 및/또는 인식모듈은, 상기 인식된 복수의 인식 결과(해당 의미 맵에 포함되는 로컬 맵들의 인식 결과들)에서, 가장 많은 빈도수를 가지는 인식 결과를 상기 의미 맵의 최종 속성으로 판별할 수 있다. 즉, 복수의 인식 결과 중 가장 많이 판정된 속성을 최종 속성으로 선정할 수 있다. 또는 학습모듈 및/또는 인식모듈은, 신뢰값이 가장 높은 인식 결과를 상기 의미 맵의 최종 속성으로 판별할 수 있다. 즉, 복수의 인식 결과 중 가장 높은 신뢰값을 보이는 인식 결과에서 판정된 속성을 최종 속성 으로 선정할 수 있다. 또는 학습모듈 및/또는 인식모듈은, 동일한 인식 결과들에 대응하는 신뢰값의 평균값들 중에서 가장 높은 평균값에 대응하는 인식 결과를 상기 의미 맵의 최종 속성으로 판별할 수 있다. 즉, 학습모듈 및/또 는 인식모듈은, 복수의 인식 결과들을 동일한 인식 결과를 가지는 것들끼리 그룹핑(grouping)하고, 신뢰값 들을 평균한 후에, 평균값이 가장 높은 그룹의 인식 결과를 최종 속성으로 판별할 수 있다. 도 8 내지 도 11은 본 발명의 실시예에 따른 이동 로봇의 제어 방법에 관한 설명에 참조되는 도면이다. 도 8을 참조하면, 본 발명의 일 실시예에 따른 이동 로봇은, 명령 또는 설정에 따라서 이동하며 청소를 수 행할 수 있다. 예를 들어, 이동 로봇은 내비게이션 맵에 기반하여 지그재그 패턴으로 이동할 수 있다. 도 9를 참조하면, 내비게이션 맵은 복수의 로컬 맵(LM1, LM2, LM3, LM4, LM5, ...)을 포함할 수 있다. 상 기 내비게이션 맵은, 복수의 영역으로 구분되고, 각 영역은 하나 이상의 로컬 맵을 포함할 수 있다. 각 로컬 맵 은 서로 겹치지 않도록 설정된다. 상기 로컬 맵들(LM1, LM2, LM3, LM4, LM5)은 일종의 단위 맵으로서 임의의 크기로 설정 가능하다. 예를 들어, 도 9와 같이, 로컬 맵들(LM1, LM2, LM3, LM4, LM5)은 벽(Wall)을 기준으로 N by N의 크기를 가지는 정사각형 형태로 설정될 수 있다. 또한, 이동 로봇은, 이동 중에, 연속되는 영상 정보와 맵 정보를 활용한 영역 정보를 획득할 수 있다. 이 동 로봇은, 가정 내 청소를 수행하면서 이동할 수 있고, 영상획득부가 이동 중에 촬영하여 복수의 영 상을 획득할 수 있다. 이 경우에, 제어부는, 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵에 대응하는 영역에서는 추가 촬 영을 하지 않도록 상기 영상획득부를 제어할 수 있다. 이에 따라, 속성 인식에 필요한 개수의 데이터가 확 보되면, 더 이상의 영상을 획득하지 않음으로써, 불필요한 연산, 처리, 인식 과정을 방지할 수 있다. 예를 들어, 제어부는, 각 로컬 맵 별 4방위 또는 8방위에서 영상을 획득하고 영상이 모두 획득되면 추가적 인 영상을 획득하지 않도록 상기 영상획득부를 제어할 수 있다. 한편, 영상 획득 시, 이동 로봇의 로컬 맵 내 위치는 큰 영향이 없고 방향이 중요하다. 상기 영상획득부 가 구비하는 카메라는 화각에 따라 소정 범위의 영역을 촬영할 수 있다, 다라서, 동일한 위치가 아니여도, 소정 범위 내에 존재하는 위치에서 다른 각도로 촬영되는 영상들은, 특정 위치에서 회전하면서 촬영하는 360도 와 유사한 범위를 커버(cover)할 수 있고, 이를 분석하면, 해당 공간의 속성을 정확하게 인식할 수 있다.한편, 제어부는, 복수의 영상들이 획득됨에 따라, 영역 인지 가능 여부를 판단할 수 있다. 제어부는, 상기 로컬 맵에서 획득된 N개의 영상에 기초하여 상기 로컬 맵에 대응하는 영역을 인식할 수 있 다. 또한, 제어부는, 상기 로컬 맵에서 획득된 N개의 영상에서, 사물에 대응하는 이미지를 추출한 후에, 상기 추출된 이미지에 기초하여, 상기 로컬 맵에 대응하는 영역에 존재하는 사물을 인식할 수 있다. 한편, 제어부는, 영상 전체에서 머신 러닝(machine learning)으로 기학습된 데이터에 기초하여 소정 영역 의 속성을 인식할 수 있다. 또한, 제어부는, 영상 중 적어도 일부에서 머신 러닝으로 기학습된 데이터에 기초하여 사물을 인식할 수 있다. 제어부는, 각 로컬 맵들의 사물 인식 결과와 영역 인식 결과들을 통합하여 최종 결론 도출하고, 도 10, 도 11과 같은 복수의 의미 맵을 포함하는 계층 맵을 구성할 수 있다. 도 10과 도 11을 참조하면, 제어부는, 로컬 맵의 개수가 연속해서 M개(ex 3개)가 모아지면, 점진적으로 의 미 맵을 생성할 수 있다. 이 경우에, 의미 맵의 경계는 겹쳐도 상관없다. 예를 들어, 제어부는, 제1 로컬 맵(LM1), 제2 로컬 맵(LM2), 제3 로컬 맵(LM3)으로 하나의 의미 맵을 구성 하고, 제2 로컬 맵(LM2), 제3 로컬 맵(LM3), 제4 로컬 맵(LM4)으로 하나의 의미 맵을 구성하고, 제3 로컬 맵 (LM3), 제4 로컬 맵(LM4), 제5 로컬 맵(LM5)으로 하나의 의미 맵을 구성할 수 있다. 제어부는, 상기 인식된 복수의 로컬 맵 인식 결과에서, 인식 결과의 빈도수, 신뢰값, 신뢰값들의 평균값 중 적어도 하나에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 판별할 수 있다. 예를 들어, 제어부는, 제1 로컬 맵(LM1), 제2 로컬 맵(LM2), 제3 로컬 맵(LM3)으로 구성된 의미 맵에서, 제1 로컬 맵(LM1), 제2 로컬 맵(LM2), 제3 로컬 맵(LM3) 각각의 인식 결과들의 빈도수, 신뢰값, 신뢰값들의 평 균값 중 적어도 하나에 기초하여, 의미 맵에 대응하는 영역의 속성을 침실로 판별할 수 있다. 또한, 제어부는, 제3 로컬 맵(LM3), 제4 로컬 맵(LM4), 제5 로컬 맵(LM5)으로 구성된 의미 맵에서, 제3 로 컬 맵(LM3), 제4 로컬 맵(LM4), 제5 로컬 맵(LM5) 각각의 인식 결과들의 빈도수, 신뢰값, 신뢰값들의 평균값 중 적어도 하나에 기초하여, 의미 맵에 대응하는 영역의 속성을 거실로 판별할 수 있다. 또한, 제어부는, 제2 로컬 맵(LM2), 제3 로컬 맵(LM3), 제4 로컬 맵(LM4)으로 구성된 의미 맵에서, 신뢰값 이 소정 문턱값(Threshold)을 넘지 못할 경우 Unknown으로 처리할 수 있다. 도 12는 본 발명의 일 실시예에 따른 이동 로봇의 제어 방법을 도시한 순서도이다. 도 12를 참조하면, 이동 로봇은 내비게이션 맵에 기반하여 이동할 수 있다(S1210). 여기서, 상기 내비게이 션 맵은 복수의 로컬 맵을 포함할 수 있다. 상기 내비게이션 맵은 복수의 영역으로 구분되고, 각 영역은 하나 이상의 로컬 맵을 포함할 수 있다. 각 로컬 맵은 서로 겹치지 않도록 설정된다. 이동 로봇은, 이동 중에, 영상획득부를 통하여, 상기 복수의 로컬 맵에 대응하는 영역들에서 복수의 영상을 획득할 수 있다(S1220). 영상획득부는, 본체 전방의 영상을 획득하도록 구비되는 전면 카메라(120a)와 본체의 상면부에 구비되는 상부 카메라(120b)를 통하여, 이동 중에 본체 주변의 영상을 획득할 수 있다. 한편, 제어부는, 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵에 대응하는 영역에서는 추가 촬영을 하지 않도록 상기 영상획득부를 제어할 수 있다. 속성 인식에 필요한 개수의 데이터가 확보되면, 더 이상 의 영상을 획득하지 않음으로써, 불필요한 연산, 처리, 인식 과정을 방지할 수 있다. 또한, 제어부는, 영상획득부가 획득하는 복수의 영상 중 일부 영상을 소정 기준에 따라 필터링 (filtering)할 수 있다(S1225). 예를 들어, 상기 소정 기준은, 센서부의 장애물 감지 여부 조건을 포함할 수 있다. 또한, 상기 소정 기준 은, 이동 로봇이 소정 영역 안에 있는지 여부, 이동 로봇의 자세(방향)이 소정 영역의 중심을 향하는 지 여부 조건 등을 더 포함할 수 있다. 즉, 소정 연역의 공간 속성 인식에 적합하지 않은 영상들을 걸러내어 남은 영상들로만 속성 인식에 이용할 수 있다. 이동 로봇은, 이동 중에, 영상획득부를 통하여 소정 기준에 따라 영상 촬영을 계속할 수 있고, 제어 부는, 특정 로컬 맵에 대응하는 영역에서 촬영된 영상이 소정 기준 갯수 이상 확보되면(S1230), 해당 영역 의 속성을 인식할 수 있다(S1240). 바람직하게는, 제어부는, 상기 복수의 로컬 맵 중 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵의 속성을 인식할 수 있다(S1240). 예를 들어, 상기 N개의 기준 갯수는, 동/서/남/북의 4방위, 8방위, 16방위 등에 대응하는, 4, 8, 16개 등으로 설정될 수 있다. 한편, 제어부는, 상기 복수의 영상을 획득 시의 위치 정보, 방향 정보, 주행 라인 정보, 로컬 맵 정보와 연계하여 저장부에 저장하도록 제어할 수 있다. 특정 로컬 맵에 대응하는 영역에서, 동/서/남/북의 4방위, 8방위, 16방위 등 다른 방향에서 촬영된 복수의 영상 이 획득되면(S1230), 제어부는 다른 방향에서 촬영된 영상들을 이용하여 로컬 맵의 속성을 인식할 수 있다 (S1240). 또한, 제어부는, 상기 로컬 맵의 속성 인식 결과를 저장부에 저장하도록 제어할 수 있다. 제어부는, 다른 방향에서 촬영된 N개의 영상 전부 또는 일부를 입력 데이터로 하여 머신 러닝에 기반한 속 성 인식을 수행하고 그 결과를 출력할 수 있다. 한편, 제어부는, 상기 복수의 로컬 맵 중에서, 인접한 소정 개수의 로컬 맵으로 구성되는 의미 맵을 생성 할 수 있고(S1250), 상기 의미 맵에 포함되는 로컬 맵들의 속성 인식 결과들에 기초하여, 상기 의미 맵에 대응 하는 영역의 최종 속성을 인식할 수 있다(S1260). 제어부는, 머신 러닝(machine learning)으로 기학습된 데이터에 기초하여 로컬 맵 및 의미 맵에 대응하는 영역의 속성을 인식할 수 있다. 도 13은 본 발명의 일 실시예에 따른 영역 속성 인식에 관한 설명에 참조되는 도면이다. 본 발명의 일 실시예에 따른 이동 로봇은, 가정 내 장소에서 다양한 방향에서 촬영한 영상들의 특성을 분 석하여 장소의 속성(Region Recognition)을 파악할 수 있다. 도 13을 참조하면, 이동 로봇의 이동 중에, 영상획득부는 소정 영역에서 이동 로봇의 자세(pose) 별 로 촬영된 복수의 영상을 획득할 수 있다. 예를 들어, 4방위의 영상을 획득하도록 설정된 경우에, 영상획득부는, 0도 내지 90도의 자세(Pose 0)에서 하나, 90도 내지 180도의 자세(Pose 1)에서 하나, 180도 내지 270도의 자세(Pose 2)에서 하나, 270도 내지 360 도의 자세(Pose 4)에서 하나의 영상을 획득할 수 있다. 도 13을 참조하면, 본 발명의 일 실시예에 따른 이동 로봇의 제어부는, 영상획득부가 획득하는 영상 데이터들을 센싱 데이터, 좌표 정보 등을 고려한 다양한 기준에 따라 필터링(Image filtering)할 수 있다. 예를 들어, 제어부는, 이동 로봇이 영역 안에 있는가?, 이동 로봇의 방향이 영역의 중심을 향하는가?, 이 동 로봇의 전방에 장애물이 없는가? 등 소정 기준에 따라 획득된 영상 중 일부 데이터를 제거할 수 있다. 또한, 도 5 등을 참조하여 설명한 것과 같이, 제어부는, 필터링 후 남은 복수의 영상에서 딥러닝(Deep larning) 기반의 영상 특성 값을 추출(DNN based feature extraction)할 수 있다. 제어부는, 상기 추출된 특성 값을 이용하여 해당 영역의 속성을 분석할 수 있다. 예를 들어, 제어부는, 해당 영역의 속성을 거실, 침실, 서재, 옷방, 부엌, 욕실, 복도, 인식 불가 (Unknown) 중 어느 하나로 판정할 수 있다. 본 발명의 일 실시에 따른 제어부에는 딥러닝(Deep larning) 기반의 영역 인식기가 탑재될 수 있다. 학습 된 영역 인식기(Region classifier)는 입력되는 데이터를 분류하여 영역, 사물 등의 속성을 인식할 수 있다. 딥러닝(Deep Learning)은. 인공지능을 구성하기 위한 인공신경망(Artificial Neural Networks: ANN)에 기반으 로 해 컴퓨터에게 사람의 사고방식을 가르치는 방법으로 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학 습할 수 있는 인공지능 기술이다. 상기 인공신경망(ANN)은 소프트웨어 형태로 구현되거나 칩(chip) 등 하드웨어 형태로 구현될 수 있다. 제어부, 예를 들어, 학습모듈 및/또는 인식모듈은, 영역의 속성이 학습된 소프트웨어 또는 하드 웨어 형태의 인공신경망(ANN)을 포함할 수 있다. 예를 들어, 학습모듈 및/또는 인식모듈은, 딥러닝(Deep Learning)으로 학습된 CNN(Convolutional Neural Network), RNN(Recurrent Neural Network), DBN(Deep Belief Network) 등 심층신경망(Deep Neural Network: DNN)을 포함할 수 있다. 학습모듈 및/또는 인식모듈은 상기 심층신경망(DNN)에 포함된 노드들 사이의 가중치(weight)들에 기 초하여 입력되는 영상 데이터에 포함되는 영역의 속성을 판별할 수 있다. 한편, 저장부에는 속성 판별을 위한 입력 데이터, 상기 심층신경망(DNN)을 학습하기 위한 데이터가 저장될 수 있다. 저장부에는 영상획득부가 획득한 원본 영상과 소정 영역이 추출된 추출 영상들이 저장될 수 있다. 또한, 실시예에 따라서는, 저장부에는 상기 심층신경망(DNN) 구조를 이루는 웨이트(weight), 바이어스 (bias)들이 저장될 수 있다. 또는, 실시예에 따라서는, 상기 심층신경망 구조를 이루는 웨이트(weight), 바이어스(bias)들은 상기 제어부 의 임베디드 메모리(embedded memory)에 저장될 수 있다. 딥러닝 기반의 영상 인식이 가능한 제어부는, 복수의 영상을 분석하여, 해당 영역의 속성을 거실, 침실, 서재, 옷방, 부엌, 욕실, 복도, 인식 불가(Unknown) 중 어느 하나로 판정할 수 있다. 본 발명의 일 실시예에 따른 이동 로봇은 맵에 포함되는 복수의 영역들의 속성을 인식한 후, 통신부 를 통하여, 맵과 인식된 속성 결과를 서버, 외부 단말 등으로 전송하거나, 인식된 속성이 등록된 맵을 서버, 외 부 단말기 등으로 전송할 수 있다. 이 경우에, 이동 로봇이, 사용자가 영역을 쉽게 인식할 수 있도록 도 6의 (c)와 같이 영역을 단순화한 사 용자 맵을 표시할 수 있는 데이터를 단말로 전송할 수 있다. 도 14는 본 발명의 일 실시예에 따른 유저 인터페이스(user interface)를 예시한 도면이다. 도 14를 참조하면, 사용자 단말의 디스플레이에는 이동 로봇을 제어할 수 있는 유저 인터페이스가 표시될 수 있다. 도 14는 영역 구분 청소 항목에 대응하는 유저 인터페이스를 예시한다. 도 14를 참조하면, 유저 인터페이스는, 단순화된 맵과, 사용자의 조작 메뉴들을 포함할 수 있다. 이 때, 맵의 각 영역에는 거실, 침실, 욕실, 서재, 옷방 등 해당 영역의 식별된 속성을 나타내는 아이콘 (1411, 1412, 1413, 1414, 1415)가 표시될 수 있다. 도 15는 본 발명의 일 실시예에 따른 이동 로봇의 제어 방법을 도시한 순서도이다. 도 15를 참조하면, 이동 로봇은 내비게이션 맵에 기반하여 이동할 수 있다(S1510). 여기서, 상기 내비게이 션 맵은 복수의 로컬 맵을 포함할 수 있다. 상기 내비게이션 맵은 복수의 영역으로 구분되고, 각 영역은 하나 이상의 로컬 맵을 포함할 수 있다. 각 로컬 맵은 서로 겹치지 않도록 설정된다. 제어부는 맵(map) 기반의 소정 기준에 따라 촬영 여부를 결정할 수 있다(S1515). 예를 들어, 제어부 는, 다른 방향으로 촬영된 N개의 영상이 획득된 로컬 맵에 대응하는 영역에서는 추가 촬영을 하지 않도록 상기 영상획득부를 제어할 수 있다. 속성 인식에 필요한 개수의 데이터가 확보되면, 더 이상의 영상을 획득하지 않음으로써, 불필요한 연산, 처리, 인식 과정을 방지할 수 있다. 또한, 상기 소정 기준은, 센서부의 장애물 감지 여부 조건을 포함할 수 있고, 상기 소정 기준은, 이동 로 봇이 소정 영역 안에 있는지 여부, 이동 로봇의 자세(방향)이 소정 영역의 중심을 향하는 지 여부 조건 등 을 더 포함할 수 있다. 즉, 제어부는, 영상획득부가 소정 기준에 따라 촬영하지 않도록 제어함으로써, 속성 인식의 정확도를 저하시킬 수 있는 데이터를 사용하는 것을 방지할 수 있다.이동 로봇은, 이동 중에, 영상획득부를 통하여, 상기 복수의 로컬 맵에 대응하는 영역들에서 복수의 영상을 획득할 수 있다(S1520). 영상획득부는, 본체 전방의 영상을 획득하도록 구비되는 전면 카메라(120a)와 본체의 상면부에 구비되는 상부 카메라(120b)를 통하여, 이동 중에 본체 주변의 영상을 획득할 수 있다. 제어부는, 영상획득부가 획득한 영상을 이용하여 로컬 맵의 속성을 인식할 수 있다(S1530). 제어부는, 영상 중 일부 영역을 추출한 후에 머신 러닝(machine learning)으로 기학습된 데이터에 기초하 여 사물을 인식할 수 있다(S1531). 또한, 제어부는 영상 전체 영역에서 머신 러닝(machine learning)으로 기학습된 데이터에 기초하여 소정 영역의 속성을 인식할 수 있다(S1532). 또한, 제어부는, 상기 로컬 맵에 대응하는 영역의 인식 결과와 사물 인식 결과를 통합하여, 해당 로컬 맵 의 최종 인식 결과를 생성할 수 있다(S1533). 한편, 제어부는, 해당 로컬 맵의 최종 인식 결과를 저장부에 저장하도록 제어할 수 있다(S1540). 이후, 제어부는, 로컬 맵들에 대한 인식 결과들이 저장됨에 따라, 영역 인지 가능 여부를 판단할 수 있다 (S1550). 제어부는, 로컬 맵의 개수가 연속해서 M개(ex 3개)가 모아지면, 점진적으로 의미 맵을 생성할 수 있다. 도 10과 도 11을 참조하여 설명한 것과 같이, 제어부는, 각 로컬 맵들의 사물 인식 결과와 영역 인식 결과 들을 통합하여 최종 결론 도출하고, 복수의 의미 맵을 포함하는 계층 맵을 구성할 수 있다. 소정 의미 맵에 포함되는 로컬 맵에 대한 인식 결과들이 충분하면(S1550), 제어부는, 상기 인식된 복수의 로컬 맵 인식 결과들을 종합하여 대응하는 영역의 속성을 판단할 수 있다(S1560). 제어부는, 복수의 인식 결과의 빈도수, 신뢰값, 신뢰값들의 평균값 중 적어도 하나에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 판별할 수 있다. 도 16 내지 도 23은 본 발명의 실시예에 따른 이동 로봇의 제어 방법에 관한 설명에 참조되는 도면이다. 종래에는, 특정 패턴을 가진 마커(Marker)를 부착하여 영역을 설정하는 등의 제한된 영역 인지 기술이 있었다. 이 경우에, 매번 고객이 직접 부가적인 마커를 탈부착해야 하고, 각 영역별로 고객이 명칭을 설정해야 하는 등 불편함이 있었다. 하지만, 본 발명에 따르면, 이동 로봇의 카메라를 활용하여 영역의 속성을 파악하고 이를 통해 영역에 맞 는 주행 패턴 생성할 수 있다. 또한, 각 영역 별로 위치 지정하거나 소정 기능을 명령할 수 있는 사용자 경험(UX)을 제공할 수 있다. 예를 들어, 거실 가서 청소해 등 사용자가 일상적으로 사용하는 영역 속성에 따른 명령이 가능하다. 또한, 장소의 맥락(context)을 파악하여 속성에 맞게 이동 로봇이 적합한 행동 시나리오를 수행할 수 있다. 도 16을 참조하면, 특정 장소에 대해서는 가상 벽(Virtual Wall)을 생성할 수 있다. 예를 들어, 현관에 대해서 가상벽이 설정되면, 이동 로봇이 설정된 현관으로 주행하여 이탈하는 것을 방지할 수 있다. 또한, 도 17을 참조하면, 이동 로봇의 영역 인지 결과에 따른 청소 출력을 조절함으로써, 사용자의 생활 공간에 최적화된 주행 및 청소를 수행할 수 있다. 실시예에 따라서는, 이동 로봇이 소정 위치에서 회전하며 촬영하는 N개의 영상으로 영역의 속성을 인지할 수 있다. 하지만, 속성 인식을 위한 영상을 획득하기 위하여, 이동 로봇이 전체 주행 구역에 포함되는 다수의 위치 에서 회전하는 것은 비효율적이다. 또한, 사용자는 반복적으로 주행을 멈추고, 제자리에서 회전하는 이동 로봇 의 동작을 이해하지 못하여, 제품 신뢰도가 저하될 수 있다. 따라서, 청소를 위한 일반 주행에서 속성 인식을 위한 영상을 획득하는 것이 더 바람직하다. 예를 들어, 제어부는, 지그재그 패턴으로 주행하며 이동 중에, 영상획득부가 영상들을 계속해서 획득 하고, 획득된 영상들을 촬영된 위치에 따라 분류하여, 특정 로컬 맵에 대응하는 영역에서 N개까지의 데이터를 확보하도록 제어할 수 있다. 도 18을 참조하면, 주행부는, 제어부의 제어에 따라, 제1 방향으로 지그재그 패턴으로 주행하는 1회 차 주행(a)과 상기 제1 방향에 수직하는 제2 방향으로 지그재그 패턴으로 주행하는 2회차 주행(b) 패턴을 조합 하여 주행하도록 본체를 이동시킬 수 있다. 즉, 영역 인지 모드에서는 총 2회의 전체 청소를 진행하면서, 영상들을 획득할 수 있다. 한편, 2회의 전체 청소는 연속 2회일 필요는 없다. 즉, 한번은 전체 주행구역에 대하여 1회차 주행(a)하며, 청 소 및 영상을 획득하고, 다음의 청소 주행시 전체 주행구역에 대하여 2회차 주행(b)하며, 청소 및 영상을 획득 할 수 있다. 이 경우에, 상기 다른 방향으로 촬영된 N개의 영상은, 상기 1회차 주행 시 획득되는 영상과 상기 2회차 주행 시 획득되는 영상을 포함할 수 있다. 즉, 제어부는, 다른 방향의 지그재그 패턴으로 주행하도록 제어하고, 서로 다른 방향의 지그재그 패턴 주 행 시 획득되는 영상들을 획득시 위치에 따라 분류하여 특정 로컬 맵에 대응하는 영역에서 N개까지의 데이터를 확보하도록 제어할 수 있다. 이에 따라, 기존 주행 패턴을 크게 훼손하는 새로운 주행 패턴을 사용하지 않으면서도, 영역 속성 인식에 필요 한 데이터를 충분히 확보할 수 있다. 왕복 시 이동 로봇은 비슷한 영역에서 영상을 촬영할 수 있다. 도 19를 참조하면, 이동 로봇은, 소정 로컬 맵(LM)에서 1회차 주행하면서, 동쪽 방향을 향하는 위치(E1)에 서 영상을 획득하고, 서쪽 방향을 향하는 위치(W1)에서 영상을 획득할 수 있다. 또한, 이동 로봇은, 상기 로컬 맵(LM)에서 2회차 주행하면서, 북쪽 방향을 향하는 위치(N1)에서 영상을 획 득하고, 남쪽 방향을 향하는 위치(S1)에서 영상을 획득할 수 있다. 따라서, 기존 청소 패턴을 훼손하지 않는 범위에서 주행 패턴만 바꾸게 되면 4방위에 대한 데이터 획득이 가능 하다. 도 20을 참조하면, 제어부는 1회차 지그재그(Zig-Zag) 주행 시 일정 간격 마다 영상 촬영하고 영역 인지용 맵(Map)을 구성할 수 있다. 도 21의 (a)를 참조하면, 영역 인지용 맵의 최소 단위 정보는, 로봇의 X 좌표, 로봇의 Y 좌표, 방향 (Direction), 주행 라인 정보(Following Line Number), 촬영 유무(Occupancy), 로컬 맵 정보 (LocalMapNumber), 영상 데이터를 포함할 수 있다. 여기서, 방향 정보는 동/서/남/북 등 방향 정보에 대응하도 록 설정된 숫자값으로 저장될 수 있다. 도 20과 도 21을 참조하면, 제어부는, 주행을 시작하면서, 최초 주행 라인(Following Line)을 정의하고, 일정 간격으로 영상을 촬영하도록 제어할 수 있다. 예를 들어, 1회차 주행의 제1 위치(P1)에서 촬영을 수행한 경우에, 획득된 영상 데이터는 도 21의 (b)와 같이, (Rx1, Ry1, 0, 1, True, 1, 영상)로 저장될 수 있다. 여기서, Rx1, Ry1,는 제1 위치(P1)의 좌표 정보, 0은 동 쪽 방향 정보, 1은 첫번째 주행 라인(L1), Ture는 영상을 촬영 했음을 의미하고, 그 다음에 이어지는 1은 해당 로컬 맵을 나타낸다. 이후, 이동 로봇은 이동하면서 일정 간격 마다 영상을 촬영할 수 있다. 예를 들어, 1회차 주행의 제4 위치(P4)에서 촬영을 수행한 경우에, 획득된 영상 데이터는 도 21의 (c)와 같이, (Rx4, Ry4, 0, 1, True, 4, 영상)로 저장될 수 있다. 여기서, Rx4, Ry4,는 제4 위치(P4)의 좌표 정보, 0은 동 쪽 방향 정보, 1은 첫번째 주행 라인(L1), Ture는 영상을 촬영 했음을 의미하고, 그 다음에 이어지는 4는 해당 로컬 맵을 나타낸다. 한편, 제어부는 회전(Turn)을 하면서 주행 라인(Following Line) 변경이 발생하면 새로운 주행 라인(L2)를 정의할 수 있다.제어부는 현재 주행 라인(L2)을 따라 가면서 이전 주행 라인(L1)에서 촬영한 좌표에 대응하는 좌표에서 촬 영을 하도록 제어할 수 있다. 이 경우에, 제어부는 새로운 주행 라인(L2)을 주행하면서 인접 주행 라인(L1)에서 촬영된 데이터 및 그에 대응하는 짝(Pair)에 해당하는 데이터가 있는지 검사하고, 대응하는 데이터가 없으면, 인접 주행 라인(L1)에서 촬영된 지점(P4)에대응하는 지점(P5)에서 영상을 촬영할 수 있다. 만약, 소정 로컬 맵에 포함되는 인접한 두 주행 라인(L1, L2)에서 짝(Pair)을 이루는 두 데이터가 있으면, 현재 주행 라인(L2)에서는 촬영하지 않는다. 도 22a를 참조하면, 이동 로봇은 충전대에서 출발하면서, 최초 주행 라인(L1)을 정의할 수 있다. 이동 로봇은 최초 주행 라인(L1)을 따라 주행하면서, 설정된 소정 거리를 이동한 후에 영상을 촬영할 수 있다. 다시 이동 로봇은 소정 거리 직진 이동 후에 현재 로봇 좌표가 영상을 촬영한 로컬 맵 안에 있는지 확인하고, 그렇다면 다시 소정 거리 직진 이동하고, 그렇지 않다면 영상을 촬영한 후에 이동하게 된다. 이와 같은 방식으로, 이동 로봇은 일정 간격(P11, P12, P13)으로 영상을 촬영할 수 있고, 동쪽 방향에서 로컬 맵 당 하나씩의 영상을 획득할 수 있다. 도 22b를 참조하면, 이동 로봇은 회전 후, 두번째 주행 라인(L2)을 정의하고, 두번째 주행 라인(L2)을 따 라 주행하면서, 설정된 소정 거리를 이동한 후에 영상을 촬영할 수 있다. 또한, 두번째 주행 라인(L2)을 따라 주행하면 저장되는 정보의 방향 정보 및 주행 라인 정보도 달라지게 된다. 한편, 두번째 주행 라인(L2)의 소정 위치(P21)는 첫번째 주행 라인(L1)의 소정 위치(P13)과 짝(pair)이 되는 위 치로, 첫번째 주행 라인(L1)의 소정 위치(P13)에서 촬영된 데이터와 짝(pair)을 이루는 데이터가 저장된 적이 없으므로, 이동 로봇은 소정 위치(P21)에서 촬영을 수행하고, 서쪽 방향으로 소정 거리만큼 직진 주행하게 된다. 상기와 같은 방식으로 두번째 주행 라인(L2)의 소정 위치들(P21, P22, P22)에서 촬영이 수행된다. 한편, 이동 로봇은, 두번째 주행 라인(L2)을 따라 주행한 후 회전하면, 세번째 주행 라인(L3)을 정의하고 세번째 주행 라인(L3)을 따라 주행하게 된다. 세번째 주행 라인(L3)이 이전의 주행 라인(L1, L2)과 동일한 로컬 맵(1,2,3)을 주행하게 된다면, 이미 짝(pai r)을 이루는 데이터 쌍이 존재하므로, 영상을 촬영하지 않고 주행하게 된다. 세번째 주행 라인(L3)이 이전의 주행 라인(L1, L2)과 동일한 로컬 맵(1,2,3)이 아닌 다른 로컬 맵(4,5,6)을 주 행하는 경우에는, 영상을 촬영하면서 주행하게 된다. 설정에 따라서는, 세번째 주행 라인(L3)이 이전의 주행 라인(L1, L2)과 동일한 로컬 맵(1,2,3)이 아닌 다른 로 컬 맵(4,5,6)을 주행하는 경우에도 소정 기준에 따라 영상을 촬영하지 않고 주행할 수도 있다. 예를 들어, 세번째 주행 라인(L3)이 로컬 맵 등의 경계 영역에 걸쳐 있는 경우에는 더욱 안정적인 위치에서 영 상을 획득하기 위하여 영상 촬영을 스킵(skip)할 수 있다. 도 22c를 참조하면, 세번째 주행 라인(L3)이 스킵되면, 네번째 주행 라인(L4)과 다섯번째 주행 라인(L5)에서 촬 영이 수행된다. 도 22a 내지 도 22c를 참조하여 설명한 것과 같이, 1회차 주행을 수행하면서, 동서 방향의 영상들을 획득할 수 있다. 이후, 2회차 주행에서 남북 방향의 영상들을 획득할 수 있다. 도 22d를 참조하면, 2회차 주행에서 첫번째 주행 라인(VL1) 내지 여섯번째 주행 라인(VL6)을 따라 주행하면서 촬영이 수행된다. 바람직하게는 2회차 주행은 제1 로컬 맵에서 최초 촬영된 지점(P11)과 그 짝(pair)을 이루는 지점(P23)의 중심 점(P31)에서부터 촬영을 시작하도록 설정되는 것이 바람직하다. 이에 따라, 동/서/남/북을 향하는 방향에서 촬 영하는 위치가 안정적으로 소정 범위 내에 위치할 수 있다. 도 23은 각 로컬 맵에 대한 속성 인식 결과들을 예시한 것이다. 도 23을 참조하면, 각 로컬 맵 당 4방위(E,W,S,N) 데이터가 있으므로 4방위에 대한 각각의 인식 결과와 신뢰값 (Confidence Value)이 생성될 수 있다. 예를 들어, 제1 로컬맵은 동쪽 방위에서 촬영된 영상은 인식 결과 침실, 신뢰값 0.9로 분석되고, 서쪽 방위에서 촬영된 영상은 인식 결과 침실, 신뢰값 0.7로 분석되며, 남쪽 방위에서 촬영된 영상은 인식 결과 침실, 신뢰값 0.7로 분석되고, 북쪽 방위에서 촬영된 영상은 침실, 신뢰값 0.9로 분석될 수 있다. 제2 로컬맵은 동쪽 방위에서 촬영된 영상은 인식 결과 침실, 신뢰값 0.8로 분석되고, 서쪽 방위에서 촬영된 영 상은 인식 결과 침실, 신뢰값 0.6로 분석되며, 남쪽 방위에서 촬영된 영상은 인식 결과 침실, 신뢰값 0.5로 분 석되고, 북쪽 방위에서 촬영된 영상은 침실, 신뢰값 0.9로 분석될 수 있다. 제3 로컬 맵은, 동쪽 방위에서 촬영된 영상은 인식 결과 침실, 신뢰값 0.7로 분석되고, 서쪽 방위에서 촬영된 영상은 인식 결과 거실, 신뢰값 0.5로 분석되며, 남쪽 방위에서 촬영된 영상은 인식 결과 침실, 신뢰값 0.4로 분석되고, 북쪽 방위에서 촬영된 영상은 침실, 신뢰값 0.5로 분석될 수 있다. 제어부는, 상기 인식된 복수의 로컬 맵 인식 결과에서, 인식 결과의 빈도수, 신뢰값, 신뢰값들의 평균값 중 적어도 하나에 기초하여, 상기 의미 맵에 대응하는 영역의 최종 속성을 판별할 수 있다. 도 10과 도 11을 참조하면, 제어부는, 제1 로컬 맵(LM1), 제2 로컬 맵(LM2), 제3 로컬 맵(LM3)으로 구성 된 의미 맵에서, 제1 로컬 맵(LM1), 제2 로컬 맵(LM2), 제3 로컬 맵(LM3) 각각의 인식 결과들의 빈도수, 신뢰값, 신뢰값들의 평균값 중 적어도 하나에 기초하여, 의미 맵에 대응하는 영역의 속성을 침실로 판별할 수 있다. 본 발명의 실시 예들 중 적어도 하나에 의하면, 이동 로봇이, 주행 구역 내 복수의 영역들의 속성을 인식할 수 있다. 또한, 본 발명의 실시 예들 중 적어도 하나에 의하면, 영역 속성 인식 결과를 편리하게 이용할 수 있어, 사용자 의 편의성을 향상할 수 있다. 또한, 본 발명의 실시 예들 중 적어도 하나에 의하면, 머신 러닝에 기반하여 영역들의 속성을 정확하게 인식할 수 있다. 본 발명의 이동 로봇 및 그 제어방법은 맵을 생성하는데 있어서, 주행구역을 복수의 영역으로 구분하고, 그 속 성을 인삭함으로써, 현재위치를 쉽게 판단할 수 있으며, 영역별 청소 지시를 입력받아 각 영역별로 영역의 형태 에 따라 청소를 수행할 수 있어 이동 및 청소에 따른 효율이 향상되는 효과가 있다. 본 발명에 따른 이동 로봇은 상기한 바와 같이 설명된 실시예들의 구성과 방법이 한정되게 적용될 수 있는 것이 아니라, 상기 실시예들은 다양한 변형이 이루어질 수 있도록 각 실시예들의 전부 또는 일부가 선택적으로 조합 되어 구성될 수도 있다. 한편, 본 발명의 실시예에 따른 이동 로봇의 제어 방법은, 프로세서가 읽을 수 있는 기록매체에 프로세서가 읽 을 수 있는 코드로서 구현하는 것이 가능하다. 프로세서가 읽을 수 있는 기록매체는 프로세서에 의해 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 프로세서가 읽을 수 있는 기록매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 데이터 저장장치 등이 있으며, 또한, 인터넷을 통한 전송 등과 같 은 캐리어 웨이브의 형태로 구현되는 것도 포함한다. 또한, 프로세서가 읽을 수 있는 기록매체는 네트워크로 연 결된 컴퓨터 시스템에 분산되어, 분산방식으로 프로세서가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 또한, 이상에서는 본 발명의 바람직한 실시예에 대하여 도시하고 설명하였지만, 본 발명은 상술한 특정의 실시"}
{"patent_id": "10-2017-0072746", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "예에 한정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 발명이 속하는 기술분야 에서 통상의 지식을 가진자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2017-0072746", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 이동 로봇 및 이동 로봇을 충전시키는 충전대를 도시한 사시도이다. 도 2는 도 1에 도시된 이동 로봇의 상면부를 도시한 도이다. 도 3은 도 1에 도시된 이동 로봇의 정면부를 도시한 도이다. 도 4는 도 1에 도시된 이동 로봇의 저면부를 도시한 도이다. 도 5는 본 발명의 일 실시예에 따른 이동 로봇의 주요 구성들 간의 제어관계를 도시한 블록도이다. 도 6 은 본 발명의 일 실시예에 따른 이동 로봇의 맵 생성의 예가 도시된 도이다. 도 7은 본 발명의 일 실시예에 따른 이동 로봇의 제어 방법을 도시한 순서도이다. 도 8 내지 도 11은 본 발명의 실시예에 따른 이동 로봇의 제어 방법에 관한 설명에 참조되는 도면이다. 도 12는 본 발명의 일 실시예에 따른 이동 로봇의 제어 방법을 도시한 순서도이다. 도 13은 본 발명의 일 실시예에 따른 영역 속성 인식에 관한 설명에 참조되는 도면이다. 도 14는 본 발명의 일 실시예에 따른 유저 인터페이스를 예시한 도면이다. 도 15는 본 발명의 일 실시예에 따른 이동 로봇의 제어 방법을 도시한 순서도이다.도 16 내지 도 23은 본 발명의 실시예에 따른 이동 로봇의 제어 방법에 관한 설명에 참조되는 도면이다."}
